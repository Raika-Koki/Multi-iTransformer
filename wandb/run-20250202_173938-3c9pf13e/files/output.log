[32m[I 2025-02-02 17:39:46,439][0m A new study created in memory with name: no-name-991acc22-7ef9-40b6-a40f-ecd3e0de1ec6[0m
[32m[I 2025-02-02 17:41:13,290][0m Trial 0 finished with value: 0.36511471463462053 and parameters: {'observation_period_num': 73, 'train_rates': 0.6515113704090687, 'learning_rate': 6.884980325172971e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8432648179458309}. Best is trial 0 with value: 0.36511471463462053.[0m
[32m[I 2025-02-02 17:43:18,147][0m Trial 1 finished with value: 0.05103677598351829 and parameters: {'observation_period_num': 8, 'train_rates': 0.9210844254667259, 'learning_rate': 1.93392787187726e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8561612078678438}. Best is trial 1 with value: 0.05103677598351829.[0m
Early stopping at epoch 60
[32m[I 2025-02-02 17:43:34,543][0m Trial 2 finished with value: 0.309844935465543 and parameters: {'observation_period_num': 73, 'train_rates': 0.8307486848120764, 'learning_rate': 6.229557042684729e-05, 'batch_size': 227, 'step_size': 1, 'gamma': 0.8086675951755116}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:44:03,454][0m Trial 3 finished with value: 0.08571182456750273 and parameters: {'observation_period_num': 34, 'train_rates': 0.7700222732786325, 'learning_rate': 3.827380648230617e-05, 'batch_size': 188, 'step_size': 14, 'gamma': 0.9466148293412858}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:44:28,219][0m Trial 4 finished with value: 1.1171074227209834 and parameters: {'observation_period_num': 112, 'train_rates': 0.6974543884382022, 'learning_rate': 1.9418300849625895e-06, 'batch_size': 211, 'step_size': 13, 'gamma': 0.839658466436986}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:47:02,682][0m Trial 5 finished with value: 0.28062162467756785 and parameters: {'observation_period_num': 147, 'train_rates': 0.7363890439225608, 'learning_rate': 9.387552454306484e-06, 'batch_size': 30, 'step_size': 6, 'gamma': 0.878689988298498}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:47:35,130][0m Trial 6 finished with value: 0.3426223320991288 and parameters: {'observation_period_num': 143, 'train_rates': 0.8039108043958849, 'learning_rate': 4.040377085982599e-05, 'batch_size': 168, 'step_size': 1, 'gamma': 0.8909426117616068}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:49:29,920][0m Trial 7 finished with value: 0.3571075856685638 and parameters: {'observation_period_num': 154, 'train_rates': 0.9216726609070225, 'learning_rate': 1.3439552636525415e-06, 'batch_size': 48, 'step_size': 8, 'gamma': 0.8426831134713186}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:49:59,373][0m Trial 8 finished with value: 0.08695014353309359 and parameters: {'observation_period_num': 144, 'train_rates': 0.9028832718561985, 'learning_rate': 0.0002103423999537197, 'batch_size': 198, 'step_size': 13, 'gamma': 0.8730223845234941}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:51:03,641][0m Trial 9 finished with value: 0.35983238835514475 and parameters: {'observation_period_num': 185, 'train_rates': 0.7316224247817549, 'learning_rate': 1.573536147393674e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.8703448018616572}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:51:59,939][0m Trial 10 finished with value: 0.07226646691560745 and parameters: {'observation_period_num': 7, 'train_rates': 0.9879554652158616, 'learning_rate': 0.0006644623475265623, 'batch_size': 114, 'step_size': 10, 'gamma': 0.7707845711433968}. Best is trial 1 with value: 0.05103677598351829.[0m
[32m[I 2025-02-02 17:52:55,498][0m Trial 11 finished with value: 0.04831642284989357 and parameters: {'observation_period_num': 5, 'train_rates': 0.9788723834045355, 'learning_rate': 0.0008601251450257633, 'batch_size': 120, 'step_size': 10, 'gamma': 0.7502620354030177}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 17:53:48,649][0m Trial 12 finished with value: 0.09946387261152267 and parameters: {'observation_period_num': 243, 'train_rates': 0.9774420966853519, 'learning_rate': 0.0001701102306326558, 'batch_size': 110, 'step_size': 10, 'gamma': 0.7504287012476063}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 17:54:28,956][0m Trial 13 finished with value: 0.05577967036515474 and parameters: {'observation_period_num': 14, 'train_rates': 0.8816137424984083, 'learning_rate': 0.0006333861467159812, 'batch_size': 145, 'step_size': 15, 'gamma': 0.9878043128472971}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 17:55:35,142][0m Trial 14 finished with value: 0.06077391563942938 and parameters: {'observation_period_num': 61, 'train_rates': 0.9431636048687339, 'learning_rate': 0.0001637348206363566, 'batch_size': 89, 'step_size': 11, 'gamma': 0.9172354714669738}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 17:56:17,908][0m Trial 15 finished with value: 0.4926496434313042 and parameters: {'observation_period_num': 39, 'train_rates': 0.8790065882291037, 'learning_rate': 4.343010740151734e-06, 'batch_size': 139, 'step_size': 4, 'gamma': 0.7946230621910125}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:00:25,918][0m Trial 16 finished with value: 0.06566670015975312 and parameters: {'observation_period_num': 103, 'train_rates': 0.849717917353241, 'learning_rate': 2.0097169893857264e-05, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8112942389761063}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:01:44,816][0m Trial 17 finished with value: 0.05460099034358801 and parameters: {'observation_period_num': 34, 'train_rates': 0.9341157424660292, 'learning_rate': 7.514034629229768e-05, 'batch_size': 74, 'step_size': 15, 'gamma': 0.9381933029813478}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:02:37,175][0m Trial 18 finished with value: 0.08496010303497314 and parameters: {'observation_period_num': 204, 'train_rates': 0.9633274303271442, 'learning_rate': 0.00036417969414702297, 'batch_size': 112, 'step_size': 8, 'gamma': 0.7780093648226394}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:03:00,392][0m Trial 19 finished with value: 0.4288931410414589 and parameters: {'observation_period_num': 94, 'train_rates': 0.8526734501705168, 'learning_rate': 2.7086115257046306e-06, 'batch_size': 247, 'step_size': 9, 'gamma': 0.9888614539627245}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:03:29,178][0m Trial 20 finished with value: 0.18708365602022886 and parameters: {'observation_period_num': 47, 'train_rates': 0.6047123449061189, 'learning_rate': 8.426924909105296e-05, 'batch_size': 169, 'step_size': 6, 'gamma': 0.9074748670273526}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:04:49,532][0m Trial 21 finished with value: 0.0541070871137453 and parameters: {'observation_period_num': 8, 'train_rates': 0.938695930317142, 'learning_rate': 0.0009381446420747388, 'batch_size': 73, 'step_size': 15, 'gamma': 0.9453493701846237}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:06:00,511][0m Trial 22 finished with value: 0.04850235296602266 and parameters: {'observation_period_num': 6, 'train_rates': 0.9065283799998148, 'learning_rate': 0.0009560094379463848, 'batch_size': 83, 'step_size': 12, 'gamma': 0.940993024997974}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:07:03,688][0m Trial 23 finished with value: 0.06375861862961878 and parameters: {'observation_period_num': 19, 'train_rates': 0.8986281896307061, 'learning_rate': 0.00031045053016936916, 'batch_size': 92, 'step_size': 12, 'gamma': 0.9639661038038843}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:08:56,212][0m Trial 24 finished with value: 0.054982882249971915 and parameters: {'observation_period_num': 57, 'train_rates': 0.9602300365878494, 'learning_rate': 0.00044486161757679066, 'batch_size': 52, 'step_size': 12, 'gamma': 0.9105238008883588}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:09:48,782][0m Trial 25 finished with value: 0.10263407975435257 and parameters: {'observation_period_num': 24, 'train_rates': 0.9896550551053962, 'learning_rate': 2.0349534933721157e-05, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8216095443789406}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:10:51,507][0m Trial 26 finished with value: 0.0717312692379465 and parameters: {'observation_period_num': 81, 'train_rates': 0.9149751249940766, 'learning_rate': 0.0009040114181074428, 'batch_size': 93, 'step_size': 14, 'gamma': 0.8552990926475209}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:13:26,070][0m Trial 27 finished with value: 0.08766398888529109 and parameters: {'observation_period_num': 51, 'train_rates': 0.8653852014585527, 'learning_rate': 1.0870521083640862e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.7508521912012412}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:14:05,038][0m Trial 28 finished with value: 0.0647841203577055 and parameters: {'observation_period_num': 27, 'train_rates': 0.8295983939860415, 'learning_rate': 0.00012541412940395732, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9669184901886021}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:15:40,000][0m Trial 29 finished with value: 0.07991488357334572 and parameters: {'observation_period_num': 7, 'train_rates': 0.7994586017397717, 'learning_rate': 4.9569573385728824e-06, 'batch_size': 55, 'step_size': 14, 'gamma': 0.928074076770348}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:17:20,741][0m Trial 30 finished with value: 0.06568172998236914 and parameters: {'observation_period_num': 83, 'train_rates': 0.9525232000212521, 'learning_rate': 0.00032170539358630063, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8875355782802554}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:18:35,532][0m Trial 31 finished with value: 0.05402453222031732 and parameters: {'observation_period_num': 7, 'train_rates': 0.9302557664693916, 'learning_rate': 0.0009041638153369329, 'batch_size': 80, 'step_size': 15, 'gamma': 0.9568568484134022}. Best is trial 11 with value: 0.04831642284989357.[0m
[32m[I 2025-02-02 18:19:34,150][0m Trial 32 finished with value: 0.04665892187160315 and parameters: {'observation_period_num': 5, 'train_rates': 0.9012938135817958, 'learning_rate': 0.0005371811390096977, 'batch_size': 99, 'step_size': 14, 'gamma': 0.968858255555839}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:20:32,496][0m Trial 33 finished with value: 0.055189080123792644 and parameters: {'observation_period_num': 29, 'train_rates': 0.8951964634438705, 'learning_rate': 0.0005075200163315509, 'batch_size': 100, 'step_size': 14, 'gamma': 0.97454362413972}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:21:17,328][0m Trial 34 finished with value: 0.060619696840375124 and parameters: {'observation_period_num': 37, 'train_rates': 0.8290547166737174, 'learning_rate': 0.00024899573943307544, 'batch_size': 127, 'step_size': 13, 'gamma': 0.9298222610111594}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:22:49,443][0m Trial 35 finished with value: 0.07091163843870163 and parameters: {'observation_period_num': 67, 'train_rates': 0.9670804208569066, 'learning_rate': 4.664194227067615e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.8262304190758558}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:24:49,165][0m Trial 36 finished with value: 0.10147169337708048 and parameters: {'observation_period_num': 117, 'train_rates': 0.7606326071033516, 'learning_rate': 2.872742337410577e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.856039667118919}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:25:25,239][0m Trial 37 finished with value: 0.06248002207359752 and parameters: {'observation_period_num': 46, 'train_rates': 0.9110671474881319, 'learning_rate': 0.0006311549713171122, 'batch_size': 168, 'step_size': 14, 'gamma': 0.7928055604124522}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:26:20,103][0m Trial 38 finished with value: 0.055967402335835345 and parameters: {'observation_period_num': 25, 'train_rates': 0.8837167251096063, 'learning_rate': 0.00011330196029875583, 'batch_size': 104, 'step_size': 13, 'gamma': 0.9558598190618736}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:30:35,604][0m Trial 39 finished with value: 0.09942373135772935 and parameters: {'observation_period_num': 20, 'train_rates': 0.859914510895696, 'learning_rate': 0.0005027083222962078, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8855956386714959}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:31:42,314][0m Trial 40 finished with value: 0.07472767337010457 and parameters: {'observation_period_num': 174, 'train_rates': 0.9206908033298846, 'learning_rate': 5.6615815943629495e-05, 'batch_size': 84, 'step_size': 8, 'gamma': 0.898981435756967}. Best is trial 32 with value: 0.04665892187160315.[0m
[32m[I 2025-02-02 18:32:54,531][0m Trial 41 finished with value: 0.04275042941878895 and parameters: {'observation_period_num': 5, 'train_rates': 0.9331365959537985, 'learning_rate': 0.0009998408535559025, 'batch_size': 82, 'step_size': 15, 'gamma': 0.956108635251356}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:33:33,641][0m Trial 42 finished with value: 0.06747560615984843 and parameters: {'observation_period_num': 5, 'train_rates': 0.6808017582666293, 'learning_rate': 0.0009995642132552128, 'batch_size': 127, 'step_size': 14, 'gamma': 0.974539289365778}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:35:00,864][0m Trial 43 finished with value: 0.04943588107133543 and parameters: {'observation_period_num': 18, 'train_rates': 0.9488180366891096, 'learning_rate': 0.0006808215976830142, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9456187163338288}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:36:28,001][0m Trial 44 finished with value: 0.04917577744738476 and parameters: {'observation_period_num': 19, 'train_rates': 0.9748749405740913, 'learning_rate': 0.0006915712053498646, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9368618802557548}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:37:19,725][0m Trial 45 finished with value: 0.06351789087057114 and parameters: {'observation_period_num': 40, 'train_rates': 0.9725400571252248, 'learning_rate': 0.0004118796026342751, 'batch_size': 119, 'step_size': 13, 'gamma': 0.9288984696810209}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:38:18,839][0m Trial 46 finished with value: 0.05164767324924469 and parameters: {'observation_period_num': 19, 'train_rates': 0.9490846561230912, 'learning_rate': 0.00023960513460530425, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9773881622401556}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:39:00,493][0m Trial 47 finished with value: 0.061373669654130936 and parameters: {'observation_period_num': 32, 'train_rates': 0.984569261664488, 'learning_rate': 0.0006730860736562783, 'batch_size': 152, 'step_size': 2, 'gamma': 0.9564142323932613}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:40:03,182][0m Trial 48 finished with value: 0.07335856215933621 and parameters: {'observation_period_num': 54, 'train_rates': 0.9186847987262673, 'learning_rate': 0.0007112226020171813, 'batch_size': 95, 'step_size': 12, 'gamma': 0.9369661135272285}. Best is trial 41 with value: 0.04275042941878895.[0m
[32m[I 2025-02-02 18:41:30,752][0m Trial 49 finished with value: 0.09092008518545251 and parameters: {'observation_period_num': 235, 'train_rates': 0.8950998082433241, 'learning_rate': 0.0005174683107773924, 'batch_size': 60, 'step_size': 15, 'gamma': 0.967080689584437}. Best is trial 41 with value: 0.04275042941878895.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_PFE_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3673 | 0.1129
Epoch 2/300, Loss: 0.1326 | 0.0888
Epoch 3/300, Loss: 0.1216 | 0.0846
Epoch 4/300, Loss: 0.1138 | 0.0836
Epoch 5/300, Loss: 0.1159 | 0.0718
Epoch 6/300, Loss: 0.1174 | 0.0748
Epoch 7/300, Loss: 0.1235 | 0.0741
Epoch 8/300, Loss: 0.0992 | 0.0597
Epoch 9/300, Loss: 0.0948 | 0.0568
Epoch 10/300, Loss: 0.1018 | 0.0574
Epoch 11/300, Loss: 0.1007 | 0.0560
Epoch 12/300, Loss: 0.1002 | 0.0739
Epoch 13/300, Loss: 0.0897 | 0.0524
Epoch 14/300, Loss: 0.0865 | 0.0547
Epoch 15/300, Loss: 0.0882 | 0.0597
Epoch 16/300, Loss: 0.0922 | 0.0685
Epoch 17/300, Loss: 0.0893 | 0.0697
Epoch 18/300, Loss: 0.0841 | 0.0611
Epoch 19/300, Loss: 0.0818 | 0.0525
Epoch 20/300, Loss: 0.0810 | 0.0504
Epoch 21/300, Loss: 0.0820 | 0.0488
Epoch 22/300, Loss: 0.0813 | 0.0478
Epoch 23/300, Loss: 0.0782 | 0.0487
Epoch 24/300, Loss: 0.0754 | 0.0478
Epoch 25/300, Loss: 0.0744 | 0.0465
Epoch 26/300, Loss: 0.0724 | 0.0449
Epoch 27/300, Loss: 0.0711 | 0.0449
Epoch 28/300, Loss: 0.0706 | 0.0511
Epoch 29/300, Loss: 0.0723 | 0.0517
Epoch 30/300, Loss: 0.0715 | 0.0532
Epoch 31/300, Loss: 0.0702 | 0.0441
Epoch 32/300, Loss: 0.0690 | 0.0450
Epoch 33/300, Loss: 0.0670 | 0.0405
Epoch 34/300, Loss: 0.0671 | 0.0413
Epoch 35/300, Loss: 0.0655 | 0.0402
Epoch 36/300, Loss: 0.0664 | 0.0417
Epoch 37/300, Loss: 0.0672 | 0.0447
Epoch 38/300, Loss: 0.0674 | 0.0529
Epoch 39/300, Loss: 0.0648 | 0.0408
Epoch 40/300, Loss: 0.0672 | 0.0450
Epoch 41/300, Loss: 0.0686 | 0.0476
Epoch 42/300, Loss: 0.0692 | 0.0459
Epoch 43/300, Loss: 0.0693 | 0.0527
Epoch 44/300, Loss: 0.0703 | 0.0457
Epoch 45/300, Loss: 0.0726 | 0.0428
Epoch 46/300, Loss: 0.0677 | 0.0455
Epoch 47/300, Loss: 0.0686 | 0.0468
Epoch 48/300, Loss: 0.0649 | 0.0441
Epoch 49/300, Loss: 0.0640 | 0.0421
Epoch 50/300, Loss: 0.0636 | 0.0413
Epoch 51/300, Loss: 0.0640 | 0.0463
Epoch 52/300, Loss: 0.0610 | 0.0397
Epoch 53/300, Loss: 0.0635 | 0.0412
Epoch 54/300, Loss: 0.0640 | 0.0535
Epoch 55/300, Loss: 0.0657 | 0.0475
Epoch 56/300, Loss: 0.0641 | 0.0436
Epoch 57/300, Loss: 0.0623 | 0.0404
Epoch 58/300, Loss: 0.0625 | 0.0471
Epoch 59/300, Loss: 0.0659 | 0.0401
Epoch 60/300, Loss: 0.0639 | 0.0421
Epoch 61/300, Loss: 0.0639 | 0.0459
Epoch 62/300, Loss: 0.0640 | 0.0458
Epoch 63/300, Loss: 0.0616 | 0.0435
Epoch 64/300, Loss: 0.0644 | 0.0510
Epoch 65/300, Loss: 0.0619 | 0.0383
Epoch 66/300, Loss: 0.0610 | 0.0422
Epoch 67/300, Loss: 0.0614 | 0.0433
Epoch 68/300, Loss: 0.0626 | 0.0465
Epoch 69/300, Loss: 0.0588 | 0.0385
Epoch 70/300, Loss: 0.0589 | 0.0400
Epoch 71/300, Loss: 0.0600 | 0.0420
Epoch 72/300, Loss: 0.0608 | 0.0416
Epoch 73/300, Loss: 0.0624 | 0.0426
Epoch 74/300, Loss: 0.0592 | 0.0407
Epoch 75/300, Loss: 0.0606 | 0.0418
Epoch 76/300, Loss: 0.0579 | 0.0408
Epoch 77/300, Loss: 0.0592 | 0.0411
Epoch 78/300, Loss: 0.0580 | 0.0422
Epoch 79/300, Loss: 0.0623 | 0.0432
Epoch 80/300, Loss: 0.0596 | 0.0399
Epoch 81/300, Loss: 0.0600 | 0.0390
Epoch 82/300, Loss: 0.0588 | 0.0414
Epoch 83/300, Loss: 0.0613 | 0.0462
Epoch 84/300, Loss: 0.0639 | 0.0435
Epoch 85/300, Loss: 0.0606 | 0.0451
Epoch 86/300, Loss: 0.0580 | 0.0397
Epoch 87/300, Loss: 0.0586 | 0.0448
Epoch 88/300, Loss: 0.0579 | 0.0452
Epoch 89/300, Loss: 0.0569 | 0.0399
Epoch 90/300, Loss: 0.0570 | 0.0455
Epoch 91/300, Loss: 0.0580 | 0.0431
Epoch 92/300, Loss: 0.0543 | 0.0468
Epoch 93/300, Loss: 0.0602 | 0.0546
Epoch 94/300, Loss: 0.0705 | 0.0448
Epoch 95/300, Loss: 0.0689 | 0.0429
Epoch 96/300, Loss: 0.0610 | 0.0409
Epoch 97/300, Loss: 0.0635 | 0.0444
Epoch 98/300, Loss: 0.0589 | 0.0420
Epoch 99/300, Loss: 0.0591 | 0.0438
Epoch 100/300, Loss: 0.0594 | 0.0514
Epoch 101/300, Loss: 0.0578 | 0.0591
Epoch 102/300, Loss: 0.0553 | 0.0434
Epoch 103/300, Loss: 0.0550 | 0.0445
Epoch 104/300, Loss: 0.0572 | 0.0567
Epoch 105/300, Loss: 0.0581 | 0.0602
Epoch 106/300, Loss: 0.0559 | 0.0462
Epoch 107/300, Loss: 0.0561 | 0.0594
Epoch 108/300, Loss: 0.0575 | 0.0641
Epoch 109/300, Loss: 0.0610 | 0.0666
Epoch 110/300, Loss: 0.0575 | 0.0609
Epoch 111/300, Loss: 0.0587 | 0.0436
Epoch 112/300, Loss: 0.0628 | 0.0623
Epoch 113/300, Loss: 0.0635 | 0.0456
Epoch 114/300, Loss: 0.0567 | 0.0453
Epoch 115/300, Loss: 0.0546 | 0.0493
Epoch 116/300, Loss: 0.0569 | 0.0519
Epoch 117/300, Loss: 0.0577 | 0.0423
Epoch 118/300, Loss: 0.0570 | 0.0429
Epoch 119/300, Loss: 0.0569 | 0.0468
Epoch 120/300, Loss: 0.0585 | 0.0463
Epoch 121/300, Loss: 0.0583 | 0.0425
Epoch 122/300, Loss: 0.0578 | 0.0501
Epoch 123/300, Loss: 0.0569 | 0.0466
Epoch 124/300, Loss: 0.0567 | 0.0535
Epoch 125/300, Loss: 0.0597 | 0.0667
Epoch 126/300, Loss: 0.0599 | 0.0527
Epoch 127/300, Loss: 0.0559 | 0.0455
Epoch 128/300, Loss: 0.0563 | 0.0500
Epoch 129/300, Loss: 0.0614 | 0.0506
Epoch 130/300, Loss: 0.0558 | 0.0497
Epoch 131/300, Loss: 0.0527 | 0.0416
Epoch 132/300, Loss: 0.0566 | 0.0480
Epoch 133/300, Loss: 0.0576 | 0.0449
Epoch 134/300, Loss: 0.0576 | 0.0452
Epoch 135/300, Loss: 0.0580 | 0.0489
Epoch 136/300, Loss: 0.0647 | 0.0563
Epoch 137/300, Loss: 0.0615 | 0.0574
Epoch 138/300, Loss: 0.0639 | 0.0563
Epoch 139/300, Loss: 0.0646 | 0.0497
Epoch 140/300, Loss: 0.0626 | 0.0569
Epoch 141/300, Loss: 0.0576 | 0.0526
Epoch 142/300, Loss: 0.0556 | 0.0485
Epoch 143/300, Loss: 0.0579 | 0.0742
Epoch 144/300, Loss: 0.0538 | 0.0493
Epoch 145/300, Loss: 0.0530 | 0.0456
Epoch 146/300, Loss: 0.0598 | 0.0525
Epoch 147/300, Loss: 0.0564 | 0.0483
Epoch 148/300, Loss: 0.0542 | 0.0440
Epoch 149/300, Loss: 0.0530 | 0.0418
Epoch 150/300, Loss: 0.0515 | 0.0438
Epoch 151/300, Loss: 0.0483 | 0.0438
Epoch 152/300, Loss: 0.0486 | 0.0420
Epoch 153/300, Loss: 0.0467 | 0.0440
Epoch 154/300, Loss: 0.0471 | 0.0477
Epoch 155/300, Loss: 0.0491 | 0.0442
Epoch 156/300, Loss: 0.0481 | 0.0450
Epoch 157/300, Loss: 0.0475 | 0.0470
Epoch 158/300, Loss: 0.0480 | 0.0475
Epoch 159/300, Loss: 0.0473 | 0.0455
Epoch 160/300, Loss: 0.0464 | 0.0455
Epoch 161/300, Loss: 0.0458 | 0.0463
Epoch 162/300, Loss: 0.0447 | 0.0513
Epoch 163/300, Loss: 0.0452 | 0.0441
Epoch 164/300, Loss: 0.0468 | 0.0481
Epoch 165/300, Loss: 0.0460 | 0.0479
Epoch 166/300, Loss: 0.0453 | 0.0471
Epoch 167/300, Loss: 0.0449 | 0.0454
Epoch 168/300, Loss: 0.0436 | 0.0487
Epoch 169/300, Loss: 0.0472 | 0.0470
Epoch 170/300, Loss: 0.0482 | 0.0445
Epoch 171/300, Loss: 0.0475 | 0.0535
Epoch 172/300, Loss: 0.0458 | 0.0537
Epoch 173/300, Loss: 0.0473 | 0.0467
Epoch 174/300, Loss: 0.0461 | 0.0485
Epoch 175/300, Loss: 0.0480 | 0.0434
Epoch 176/300, Loss: 0.0484 | 0.0435
Epoch 177/300, Loss: 0.0457 | 0.0450
Epoch 178/300, Loss: 0.0439 | 0.0435
Epoch 179/300, Loss: 0.0451 | 0.0430
Epoch 180/300, Loss: 0.0446 | 0.0426
Epoch 181/300, Loss: 0.0444 | 0.0444
Epoch 182/300, Loss: 0.0432 | 0.0460
Epoch 183/300, Loss: 0.0486 | 0.0445
Epoch 184/300, Loss: 0.0466 | 0.0432
Epoch 185/300, Loss: 0.0447 | 0.0451
Epoch 186/300, Loss: 0.0528 | 0.0443
Epoch 187/300, Loss: 0.0572 | 0.0414
Epoch 188/300, Loss: 0.0507 | 0.0431
Epoch 189/300, Loss: 0.0494 | 0.0493
Epoch 190/300, Loss: 0.0468 | 0.0434
Epoch 191/300, Loss: 0.0440 | 0.0418
Epoch 192/300, Loss: 0.0440 | 0.0451
Epoch 193/300, Loss: 0.0477 | 0.0401
Epoch 194/300, Loss: 0.0487 | 0.0402
Epoch 195/300, Loss: 0.0500 | 0.0398
Epoch 196/300, Loss: 0.0463 | 0.0398
Epoch 197/300, Loss: 0.0452 | 0.0399
Epoch 198/300, Loss: 0.0445 | 0.0402
Epoch 199/300, Loss: 0.0436 | 0.0418
Epoch 200/300, Loss: 0.0434 | 0.0401
Epoch 201/300, Loss: 0.0441 | 0.0400
Epoch 202/300, Loss: 0.0420 | 0.0394
Epoch 203/300, Loss: 0.0433 | 0.0390
Epoch 204/300, Loss: 0.0420 | 0.0394
Epoch 205/300, Loss: 0.0428 | 0.0404
Epoch 206/300, Loss: 0.0421 | 0.0416
Epoch 207/300, Loss: 0.0430 | 0.0403
Epoch 208/300, Loss: 0.0407 | 0.0398
Epoch 209/300, Loss: 0.0405 | 0.0426
Epoch 210/300, Loss: 0.0420 | 0.0424
Epoch 211/300, Loss: 0.0395 | 0.0403
Epoch 212/300, Loss: 0.0424 | 0.0407
Epoch 213/300, Loss: 0.0420 | 0.0415
Epoch 214/300, Loss: 0.0426 | 0.0416
Epoch 215/300, Loss: 0.0426 | 0.0421
Epoch 216/300, Loss: 0.0420 | 0.0425
Epoch 217/300, Loss: 0.0399 | 0.0435
Epoch 218/300, Loss: 0.0413 | 0.0427
Epoch 219/300, Loss: 0.0386 | 0.0411
Epoch 220/300, Loss: 0.0386 | 0.0414
Epoch 221/300, Loss: 0.0426 | 0.0420
Epoch 222/300, Loss: 0.0417 | 0.0426
Epoch 223/300, Loss: 0.0417 | 0.0432
Epoch 224/300, Loss: 0.0400 | 0.0426
Epoch 225/300, Loss: 0.0420 | 0.0437
Epoch 226/300, Loss: 0.0404 | 0.0425
Epoch 227/300, Loss: 0.0401 | 0.0434
Epoch 228/300, Loss: 0.0443 | 0.0462
Epoch 229/300, Loss: 0.0432 | 0.0460
Epoch 230/300, Loss: 0.0406 | 0.0431
Epoch 231/300, Loss: 0.0418 | 0.0436
Epoch 232/300, Loss: 0.0377 | 0.0458
Epoch 233/300, Loss: 0.0380 | 0.0429
Epoch 234/300, Loss: 0.0468 | 0.0441
Epoch 235/300, Loss: 0.0427 | 0.0425
Epoch 236/300, Loss: 0.0405 | 0.0425
Epoch 237/300, Loss: 0.0421 | 0.0454
Epoch 238/300, Loss: 0.0433 | 0.0424
Epoch 239/300, Loss: 0.0432 | 0.0405
Epoch 240/300, Loss: 0.0398 | 0.0400
Epoch 241/300, Loss: 0.0383 | 0.0413
Epoch 242/300, Loss: 0.0382 | 0.0421
Epoch 243/300, Loss: 0.0396 | 0.0415
Epoch 244/300, Loss: 0.0369 | 0.0418
Epoch 245/300, Loss: 0.0360 | 0.0420
Epoch 246/300, Loss: 0.0401 | 0.0426
Epoch 247/300, Loss: 0.0353 | 0.0426
Epoch 248/300, Loss: 0.0345 | 0.0424
Epoch 249/300, Loss: 0.0337 | 0.0425
Epoch 250/300, Loss: 0.0346 | 0.0422
Epoch 251/300, Loss: 0.0337 | 0.0435
Epoch 252/300, Loss: 0.0395 | 0.0415
Epoch 253/300, Loss: 0.0371 | 0.0429
Epoch 254/300, Loss: 0.0390 | 0.0431
Epoch 255/300, Loss: 0.0350 | 0.0414
Epoch 256/300, Loss: 0.0349 | 0.0420
Epoch 257/300, Loss: 0.0384 | 0.0419
Epoch 258/300, Loss: 0.0369 | 0.0414
Epoch 259/300, Loss: 0.0364 | 0.0418
Epoch 260/300, Loss: 0.0362 | 0.0439
Epoch 261/300, Loss: 0.0383 | 0.0435
Epoch 262/300, Loss: 0.0367 | 0.0445
Epoch 263/300, Loss: 0.0354 | 0.0451
Epoch 264/300, Loss: 0.0342 | 0.0442
Epoch 265/300, Loss: 0.0355 | 0.0472
Epoch 266/300, Loss: 0.0433 | 0.0625
Epoch 267/300, Loss: 0.0530 | 0.0438
Epoch 268/300, Loss: 0.0451 | 0.0512
Epoch 269/300, Loss: 0.0378 | 0.0409
Epoch 270/300, Loss: 0.0355 | 0.0413
Epoch 271/300, Loss: 0.0341 | 0.0471
Epoch 272/300, Loss: 0.0344 | 0.0429
Epoch 273/300, Loss: 0.0336 | 0.0428
Epoch 274/300, Loss: 0.0340 | 0.0583
Epoch 275/300, Loss: 0.0398 | 0.0410
Epoch 276/300, Loss: 0.0361 | 0.0420
Epoch 277/300, Loss: 0.0347 | 0.0429
Epoch 278/300, Loss: 0.0332 | 0.0449
Epoch 279/300, Loss: 0.0323 | 0.0441
Epoch 280/300, Loss: 0.0318 | 0.0457
Epoch 281/300, Loss: 0.0318 | 0.0426
Epoch 282/300, Loss: 0.0354 | 0.0414
Epoch 283/300, Loss: 0.0345 | 0.0408
Epoch 284/300, Loss: 0.0341 | 0.0414
Epoch 285/300, Loss: 0.0358 | 0.0413
Epoch 286/300, Loss: 0.0335 | 0.0420
Epoch 287/300, Loss: 0.0349 | 0.0412
Epoch 288/300, Loss: 0.0322 | 0.0409
Epoch 289/300, Loss: 0.0319 | 0.0411
Epoch 290/300, Loss: 0.0317 | 0.0410
Epoch 291/300, Loss: 0.0316 | 0.0411
Epoch 292/300, Loss: 0.0316 | 0.0416
Epoch 293/300, Loss: 0.0317 | 0.0421
Epoch 294/300, Loss: 0.0315 | 0.0419
Epoch 295/300, Loss: 0.0313 | 0.0424
Epoch 296/300, Loss: 0.0311 | 0.0422
Epoch 297/300, Loss: 0.0310 | 0.0426
Epoch 298/300, Loss: 0.0307 | 0.0428
Epoch 299/300, Loss: 0.0302 | 0.0425
Epoch 300/300, Loss: 0.0318 | 0.0426
Runtime (seconds): 215.00194716453552
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 2.3743353127829323
RMSE: 1.5408878326416016
MAE: 1.5408878326416016
R-squared: nan
[24.203503]
