ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-07 12:49:17,760][0m A new study created in memory with name: no-name-fe06ec28-2a44-478c-972b-2c7c0b338575[0m
[32m[I 2025-02-07 12:50:52,477][0m Trial 0 finished with value: 1.4058770533164062 and parameters: {'observation_period_num': 107, 'train_rates': 0.7020961606662012, 'learning_rate': 0.0007149231869352248, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9861942625572496}. Best is trial 0 with value: 1.4058770533164062.[0m
[32m[I 2025-02-07 12:53:50,482][0m Trial 1 finished with value: 0.6999594317219122 and parameters: {'observation_period_num': 152, 'train_rates': 0.9563947404076614, 'learning_rate': 7.372790901686192e-06, 'batch_size': 61, 'step_size': 13, 'gamma': 0.8742945614280437}. Best is trial 1 with value: 0.6999594317219122.[0m
[32m[I 2025-02-07 12:58:04,898][0m Trial 2 finished with value: 0.8870242523727075 and parameters: {'observation_period_num': 239, 'train_rates': 0.7023187753360782, 'learning_rate': 0.00029498874619066323, 'batch_size': 65, 'step_size': 11, 'gamma': 0.8185041822096218}. Best is trial 1 with value: 0.6999594317219122.[0m
[32m[I 2025-02-07 13:00:37,879][0m Trial 3 finished with value: 1.8304047413214952 and parameters: {'observation_period_num': 170, 'train_rates': 0.7001983647892089, 'learning_rate': 2.238649413733413e-06, 'batch_size': 250, 'step_size': 14, 'gamma': 0.867354044640018}. Best is trial 1 with value: 0.6999594317219122.[0m
[32m[I 2025-02-07 13:01:52,241][0m Trial 4 finished with value: 0.4593115880054498 and parameters: {'observation_period_num': 80, 'train_rates': 0.8342642000829233, 'learning_rate': 0.0001752450723711808, 'batch_size': 239, 'step_size': 11, 'gamma': 0.9526511362006725}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:03:21,332][0m Trial 5 finished with value: 1.2165276194638748 and parameters: {'observation_period_num': 94, 'train_rates': 0.8694691164701749, 'learning_rate': 6.4635989141723554e-06, 'batch_size': 239, 'step_size': 13, 'gamma': 0.7932077189706616}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:05:35,562][0m Trial 6 finished with value: 1.3331455392272848 and parameters: {'observation_period_num': 147, 'train_rates': 0.729358102008841, 'learning_rate': 9.74248021657604e-06, 'batch_size': 245, 'step_size': 9, 'gamma': 0.9045857132786013}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:06:00,696][0m Trial 7 finished with value: 1.6501052799663172 and parameters: {'observation_period_num': 20, 'train_rates': 0.6331216098311389, 'learning_rate': 5.860341507052784e-06, 'batch_size': 159, 'step_size': 7, 'gamma': 0.7739466359497298}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:06:51,513][0m Trial 8 finished with value: 1.2248310530317665 and parameters: {'observation_period_num': 55, 'train_rates': 0.8132581398483552, 'learning_rate': 1.040279888100436e-05, 'batch_size': 235, 'step_size': 5, 'gamma': 0.8763977201267242}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:10:24,403][0m Trial 9 finished with value: 1.3160600416380153 and parameters: {'observation_period_num': 193, 'train_rates': 0.8491349795837115, 'learning_rate': 1.7801483358498877e-06, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9835566110573112}. Best is trial 4 with value: 0.4593115880054498.[0m
[32m[I 2025-02-07 13:11:33,512][0m Trial 10 finished with value: 0.4338257610797882 and parameters: {'observation_period_num': 61, 'train_rates': 0.9834466596597438, 'learning_rate': 9.183685757259604e-05, 'batch_size': 98, 'step_size': 1, 'gamma': 0.9377360524161695}. Best is trial 10 with value: 0.4338257610797882.[0m
[32m[I 2025-02-07 13:12:42,738][0m Trial 11 finished with value: 0.2908082604408264 and parameters: {'observation_period_num': 64, 'train_rates': 0.9869108632349963, 'learning_rate': 8.325530176959068e-05, 'batch_size': 105, 'step_size': 2, 'gamma': 0.9367666753834442}. Best is trial 11 with value: 0.2908082604408264.[0m
[32m[I 2025-02-07 13:13:31,315][0m Trial 12 finished with value: 0.587243378162384 and parameters: {'observation_period_num': 18, 'train_rates': 0.9876012038001496, 'learning_rate': 5.49057830276076e-05, 'batch_size': 109, 'step_size': 1, 'gamma': 0.9290191310413213}. Best is trial 11 with value: 0.2908082604408264.[0m
[32m[I 2025-02-07 13:14:28,104][0m Trial 13 finished with value: 0.6296820561675465 and parameters: {'observation_period_num': 52, 'train_rates': 0.9182225263188313, 'learning_rate': 5.4579630651777776e-05, 'batch_size': 105, 'step_size': 1, 'gamma': 0.9349560246678231}. Best is trial 11 with value: 0.2908082604408264.[0m
[32m[I 2025-02-07 13:17:17,868][0m Trial 14 finished with value: 0.20762120130214284 and parameters: {'observation_period_num': 57, 'train_rates': 0.9191423902013525, 'learning_rate': 0.00013595024967442924, 'batch_size': 28, 'step_size': 3, 'gamma': 0.9085798825532303}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:19:32,821][0m Trial 15 finished with value: 0.557753430600332 and parameters: {'observation_period_num': 122, 'train_rates': 0.9087554464553408, 'learning_rate': 2.4648290349315214e-05, 'batch_size': 57, 'step_size': 3, 'gamma': 0.8346263630658384}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:22:48,720][0m Trial 16 finished with value: 0.9541673807508917 and parameters: {'observation_period_num': 8, 'train_rates': 0.9150400862821356, 'learning_rate': 0.000932329265868315, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9096825751829779}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:26:35,410][0m Trial 17 finished with value: 0.7505058871415389 and parameters: {'observation_period_num': 37, 'train_rates': 0.7743927113457957, 'learning_rate': 0.00024864475420230837, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9015786406496213}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:27:57,467][0m Trial 18 finished with value: 0.6237689256668091 and parameters: {'observation_period_num': 81, 'train_rates': 0.9473210025818198, 'learning_rate': 2.331093057530267e-05, 'batch_size': 197, 'step_size': 3, 'gamma': 0.9651484004981896}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:29:12,246][0m Trial 19 finished with value: 0.5130313800440894 and parameters: {'observation_period_num': 74, 'train_rates': 0.8876707555424161, 'learning_rate': 0.00010275460162753409, 'batch_size': 132, 'step_size': 3, 'gamma': 0.8491127235286615}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:31:13,450][0m Trial 20 finished with value: 0.3041115943462618 and parameters: {'observation_period_num': 113, 'train_rates': 0.9456621496600195, 'learning_rate': 0.00044793860992348443, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8955686142814763}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:33:18,437][0m Trial 21 finished with value: 0.26959499157965183 and parameters: {'observation_period_num': 114, 'train_rates': 0.9437878364300161, 'learning_rate': 0.0004606439403623943, 'batch_size': 76, 'step_size': 6, 'gamma': 0.8936148021207971}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:36:12,232][0m Trial 22 finished with value: 0.27264618483659264 and parameters: {'observation_period_num': 139, 'train_rates': 0.9623366657763149, 'learning_rate': 0.00013931726702306842, 'batch_size': 42, 'step_size': 2, 'gamma': 0.9199206414272032}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:40:08,554][0m Trial 23 finished with value: 0.27455021155917125 and parameters: {'observation_period_num': 198, 'train_rates': 0.8751236657504806, 'learning_rate': 0.00016598087415639433, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9202647173263894}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:43:01,424][0m Trial 24 finished with value: 0.393929311890661 and parameters: {'observation_period_num': 140, 'train_rates': 0.9426894450742683, 'learning_rate': 0.0004758676586288679, 'batch_size': 40, 'step_size': 4, 'gamma': 0.8931086907187584}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:45:54,567][0m Trial 25 finished with value: 0.779149347641429 and parameters: {'observation_period_num': 165, 'train_rates': 0.7813534532649742, 'learning_rate': 0.0003901378844359132, 'batch_size': 75, 'step_size': 7, 'gamma': 0.858290605748631}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:48:00,576][0m Trial 26 finished with value: 0.29499981843906903 and parameters: {'observation_period_num': 101, 'train_rates': 0.9034791257045788, 'learning_rate': 0.00018770013838150803, 'batch_size': 40, 'step_size': 2, 'gamma': 0.8854523735379487}. Best is trial 14 with value: 0.20762120130214284.[0m
[32m[I 2025-02-07 13:52:58,974][0m Trial 27 finished with value: 0.18490472321326917 and parameters: {'observation_period_num': 133, 'train_rates': 0.9631839432653752, 'learning_rate': 4.180505174341226e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9564118288220385}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 13:56:54,748][0m Trial 28 finished with value: 0.3970271374381243 and parameters: {'observation_period_num': 183, 'train_rates': 0.8490450660256659, 'learning_rate': 4.1504219707002646e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9590833264476799}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 13:59:04,517][0m Trial 29 finished with value: 0.4385819474333211 and parameters: {'observation_period_num': 117, 'train_rates': 0.9330929439406213, 'learning_rate': 1.510834415728553e-05, 'batch_size': 85, 'step_size': 6, 'gamma': 0.9882738223441142}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 13:59:38,212][0m Trial 30 finished with value: 1.283737419487594 and parameters: {'observation_period_num': 39, 'train_rates': 0.6040614728973265, 'learning_rate': 0.0006104980867816432, 'batch_size': 141, 'step_size': 9, 'gamma': 0.9701432839884053}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:02:07,935][0m Trial 31 finished with value: 0.2263357180590723 and parameters: {'observation_period_num': 128, 'train_rates': 0.9642443911825928, 'learning_rate': 0.00014065983323477467, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9509049544266098}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:04:01,695][0m Trial 32 finished with value: 0.23549004533189408 and parameters: {'observation_period_num': 93, 'train_rates': 0.9674531056384674, 'learning_rate': 6.715401653214347e-05, 'batch_size': 55, 'step_size': 4, 'gamma': 0.948281807892907}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:06:41,751][0m Trial 33 finished with value: 0.2448265366256237 and parameters: {'observation_period_num': 136, 'train_rates': 0.9660616940664398, 'learning_rate': 6.238121723956862e-05, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9500921656140286}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:09:18,926][0m Trial 34 finished with value: 0.23439515273222764 and parameters: {'observation_period_num': 97, 'train_rates': 0.9689074861633399, 'learning_rate': 3.248437633229288e-05, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9464557904646551}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:12:43,537][0m Trial 35 finished with value: 0.25699126122856303 and parameters: {'observation_period_num': 159, 'train_rates': 0.8952540916208112, 'learning_rate': 3.275367934205397e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9692247183166168}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:17:50,628][0m Trial 36 finished with value: 0.7974099957942963 and parameters: {'observation_period_num': 234, 'train_rates': 0.9266585534729731, 'learning_rate': 3.1242521166982184e-06, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9170539594714326}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:22:51,465][0m Trial 37 finished with value: 0.237413417510312 and parameters: {'observation_period_num': 96, 'train_rates': 0.9655644824919644, 'learning_rate': 1.7191901560045852e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9746257504208968}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:25:01,724][0m Trial 38 finished with value: 0.3053981525856152 and parameters: {'observation_period_num': 128, 'train_rates': 0.879120892362341, 'learning_rate': 0.00011933655979025423, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9475723421090595}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:26:27,746][0m Trial 39 finished with value: 0.758697601154022 and parameters: {'observation_period_num': 82, 'train_rates': 0.7436271694762524, 'learning_rate': 3.553055337730364e-05, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9255906125684481}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:29:29,610][0m Trial 40 finished with value: 0.6147735227048398 and parameters: {'observation_period_num': 153, 'train_rates': 0.8218151675635084, 'learning_rate': 0.0002487832361588459, 'batch_size': 33, 'step_size': 8, 'gamma': 0.9412033740155342}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:31:23,823][0m Trial 41 finished with value: 0.1939117560784022 and parameters: {'observation_period_num': 93, 'train_rates': 0.9708538846207841, 'learning_rate': 7.581699613047865e-05, 'batch_size': 52, 'step_size': 4, 'gamma': 0.9548105409644047}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:33:29,316][0m Trial 42 finished with value: 0.22222114182435548 and parameters: {'observation_period_num': 106, 'train_rates': 0.9728148865703067, 'learning_rate': 4.509060185367011e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9587572794433734}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:35:59,157][0m Trial 43 finished with value: 0.2906973495412229 and parameters: {'observation_period_num': 130, 'train_rates': 0.9289890417063454, 'learning_rate': 4.5623906569070546e-05, 'batch_size': 66, 'step_size': 5, 'gamma': 0.9579724561988433}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:36:46,559][0m Trial 44 finished with value: 0.2145031988620758 and parameters: {'observation_period_num': 44, 'train_rates': 0.9899898340122228, 'learning_rate': 9.691548858559881e-05, 'batch_size': 189, 'step_size': 3, 'gamma': 0.9813161546415385}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:37:31,050][0m Trial 45 finished with value: 0.7771128416061401 and parameters: {'observation_period_num': 42, 'train_rates': 0.9761729887567702, 'learning_rate': 8.242095332133865e-05, 'batch_size': 198, 'step_size': 3, 'gamma': 0.7551282834131839}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:38:36,071][0m Trial 46 finished with value: 0.36062514781951904 and parameters: {'observation_period_num': 63, 'train_rates': 0.9888540860852721, 'learning_rate': 2.3750543241393175e-05, 'batch_size': 196, 'step_size': 12, 'gamma': 0.9795988652321753}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:39:28,082][0m Trial 47 finished with value: 0.37841543555259705 and parameters: {'observation_period_num': 49, 'train_rates': 0.9513854358082805, 'learning_rate': 7.312212179832916e-05, 'batch_size': 178, 'step_size': 1, 'gamma': 0.9897148811018395}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:40:00,326][0m Trial 48 finished with value: 0.902929829720834 and parameters: {'observation_period_num': 31, 'train_rates': 0.8625388967411944, 'learning_rate': 4.766391970172917e-05, 'batch_size': 218, 'step_size': 2, 'gamma': 0.8069532837359653}. Best is trial 27 with value: 0.18490472321326917.[0m
[32m[I 2025-02-07 14:41:08,408][0m Trial 49 finished with value: 0.8539810568997355 and parameters: {'observation_period_num': 79, 'train_rates': 0.6901748690745108, 'learning_rate': 0.00010177651063550168, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9654768855859046}. Best is trial 27 with value: 0.18490472321326917.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-07 14:41:08,415][0m A new study created in memory with name: no-name-99aa08b7-eb66-421a-894f-d1ccf1095c4d[0m
[32m[I 2025-02-07 14:45:06,884][0m Trial 0 finished with value: 0.42355941131399977 and parameters: {'observation_period_num': 216, 'train_rates': 0.8077682588911114, 'learning_rate': 7.242067906202452e-05, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9845482788157317}. Best is trial 0 with value: 0.42355941131399977.[0m
[32m[I 2025-02-07 14:45:36,837][0m Trial 1 finished with value: 1.8860451306561608 and parameters: {'observation_period_num': 15, 'train_rates': 0.6779335930988948, 'learning_rate': 1.3248994018415883e-06, 'batch_size': 134, 'step_size': 1, 'gamma': 0.9284734805440131}. Best is trial 0 with value: 0.42355941131399977.[0m
[32m[I 2025-02-07 14:48:24,283][0m Trial 2 finished with value: 1.2521487757049758 and parameters: {'observation_period_num': 12, 'train_rates': 0.6452292887269782, 'learning_rate': 0.00041021928003399043, 'batch_size': 21, 'step_size': 1, 'gamma': 0.860405214198356}. Best is trial 0 with value: 0.42355941131399977.[0m
[32m[I 2025-02-07 14:48:49,162][0m Trial 3 finished with value: 0.271992624816248 and parameters: {'observation_period_num': 12, 'train_rates': 0.9198777244971637, 'learning_rate': 0.00026363530371735657, 'batch_size': 223, 'step_size': 13, 'gamma': 0.7655635324724032}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 14:50:23,085][0m Trial 4 finished with value: 0.28005746006965637 and parameters: {'observation_period_num': 89, 'train_rates': 0.9820343381335743, 'learning_rate': 0.0004077927455575834, 'batch_size': 156, 'step_size': 11, 'gamma': 0.9671261401866516}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 14:51:18,138][0m Trial 5 finished with value: 0.7914093732833862 and parameters: {'observation_period_num': 60, 'train_rates': 0.8405153443514789, 'learning_rate': 1.227209446332467e-05, 'batch_size': 154, 'step_size': 11, 'gamma': 0.9253132306482068}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 14:55:11,234][0m Trial 6 finished with value: 1.071664375279369 and parameters: {'observation_period_num': 219, 'train_rates': 0.7028511984533317, 'learning_rate': 1.408254931835669e-05, 'batch_size': 50, 'step_size': 2, 'gamma': 0.9403334498242997}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 14:55:49,926][0m Trial 7 finished with value: 1.6797668087198057 and parameters: {'observation_period_num': 44, 'train_rates': 0.6543161037499118, 'learning_rate': 1.8643934634378335e-05, 'batch_size': 145, 'step_size': 1, 'gamma': 0.9348149403507386}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 14:59:11,120][0m Trial 8 finished with value: 1.4219454909302034 and parameters: {'observation_period_num': 202, 'train_rates': 0.739670818387282, 'learning_rate': 7.146257889376811e-06, 'batch_size': 182, 'step_size': 13, 'gamma': 0.7796061587031754}. Best is trial 3 with value: 0.271992624816248.[0m
Early stopping at epoch 64
[32m[I 2025-02-07 15:00:24,217][0m Trial 9 finished with value: 1.1302269387703676 and parameters: {'observation_period_num': 128, 'train_rates': 0.7427264975086287, 'learning_rate': 0.0005316628935325945, 'batch_size': 134, 'step_size': 1, 'gamma': 0.8012465799690164}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 15:03:09,737][0m Trial 10 finished with value: 0.38970643281936646 and parameters: {'observation_period_num': 154, 'train_rates': 0.9241420096484464, 'learning_rate': 0.00010331648444590465, 'batch_size': 241, 'step_size': 15, 'gamma': 0.7544981697983897}. Best is trial 3 with value: 0.271992624816248.[0m
[32m[I 2025-02-07 15:04:47,891][0m Trial 11 finished with value: 0.25567272305488586 and parameters: {'observation_period_num': 96, 'train_rates': 0.9877507213379405, 'learning_rate': 0.0009255183231224971, 'batch_size': 249, 'step_size': 9, 'gamma': 0.8422475603944457}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:06:30,568][0m Trial 12 finished with value: 0.37860071485692803 and parameters: {'observation_period_num': 105, 'train_rates': 0.9037011061733919, 'learning_rate': 0.0001437384270300552, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8400148437925422}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:09:26,304][0m Trial 13 finished with value: 0.43832188844680786 and parameters: {'observation_period_num': 156, 'train_rates': 0.9666316760249295, 'learning_rate': 0.0009847110040722656, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8228091716333249}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:10:33,229][0m Trial 14 finished with value: 0.31260022972436513 and parameters: {'observation_period_num': 65, 'train_rates': 0.8823557559479828, 'learning_rate': 0.00019356906256848963, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8747437797909373}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:12:04,791][0m Trial 15 finished with value: 0.5004553198814392 and parameters: {'observation_period_num': 92, 'train_rates': 0.9412227796836952, 'learning_rate': 0.0009202977394898839, 'batch_size': 216, 'step_size': 7, 'gamma': 0.75352417989783}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:15:19,159][0m Trial 16 finished with value: 0.3893044412605406 and parameters: {'observation_period_num': 180, 'train_rates': 0.8630124443867757, 'learning_rate': 0.00021318057163099882, 'batch_size': 105, 'step_size': 15, 'gamma': 0.8869303692687885}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:15:56,508][0m Trial 17 finished with value: 0.6485841870307922 and parameters: {'observation_period_num': 34, 'train_rates': 0.9785138401394222, 'learning_rate': 5.4432486387524735e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.795442553174147}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:17:31,182][0m Trial 18 finished with value: 1.0233138922492273 and parameters: {'observation_period_num': 124, 'train_rates': 0.6040302514416529, 'learning_rate': 0.00029928741176197334, 'batch_size': 187, 'step_size': 13, 'gamma': 0.8281946998004185}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:18:39,625][0m Trial 19 finished with value: 0.6595103148258094 and parameters: {'observation_period_num': 71, 'train_rates': 0.8284945332294057, 'learning_rate': 3.4033640172651215e-05, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9004817078209066}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:23:42,505][0m Trial 20 finished with value: 0.2652930021286011 and parameters: {'observation_period_num': 241, 'train_rates': 0.9308030356420618, 'learning_rate': 0.0005576859866722598, 'batch_size': 189, 'step_size': 13, 'gamma': 0.8499942066670179}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:28:34,061][0m Trial 21 finished with value: 0.2668599486351013 and parameters: {'observation_period_num': 238, 'train_rates': 0.9416589466125423, 'learning_rate': 0.0006559100564984941, 'batch_size': 184, 'step_size': 13, 'gamma': 0.8514431712791208}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:33:36,108][0m Trial 22 finished with value: 0.3101332485675812 and parameters: {'observation_period_num': 239, 'train_rates': 0.9456233576289576, 'learning_rate': 0.0006486315198206791, 'batch_size': 187, 'step_size': 12, 'gamma': 0.8492291480232625}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:38:46,162][0m Trial 23 finished with value: 0.5712173581123352 and parameters: {'observation_period_num': 245, 'train_rates': 0.9521875076976142, 'learning_rate': 0.0009368502785505337, 'batch_size': 179, 'step_size': 14, 'gamma': 0.8177660696863274}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:42:21,802][0m Trial 24 finished with value: 1.170666217803955 and parameters: {'observation_period_num': 184, 'train_rates': 0.9899619525070332, 'learning_rate': 2.6224508300788126e-06, 'batch_size': 108, 'step_size': 10, 'gamma': 0.8713245569818585}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:47:24,014][0m Trial 25 finished with value: 0.3103096341894519 and parameters: {'observation_period_num': 252, 'train_rates': 0.8855462490056947, 'learning_rate': 0.0005265904999687252, 'batch_size': 167, 'step_size': 12, 'gamma': 0.9011060381791967}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:49:58,222][0m Trial 26 finished with value: 0.6398647805022886 and parameters: {'observation_period_num': 158, 'train_rates': 0.778827254549857, 'learning_rate': 0.0003158085769544701, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8481074028543443}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:54:09,033][0m Trial 27 finished with value: 0.5931478879150677 and parameters: {'observation_period_num': 218, 'train_rates': 0.8931488654777218, 'learning_rate': 8.901819881025936e-05, 'batch_size': 235, 'step_size': 5, 'gamma': 0.7945317863591035}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 15:57:23,488][0m Trial 28 finished with value: 0.39395729444883576 and parameters: {'observation_period_num': 183, 'train_rates': 0.8565583169591056, 'learning_rate': 0.0001285237836680757, 'batch_size': 115, 'step_size': 11, 'gamma': 0.8902976625204606}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:01:31,722][0m Trial 29 finished with value: 0.598057095854132 and parameters: {'observation_period_num': 228, 'train_rates': 0.7967217119942197, 'learning_rate': 5.78440939977795e-05, 'batch_size': 197, 'step_size': 14, 'gamma': 0.8098098723923941}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:05:30,603][0m Trial 30 finished with value: 0.3119943260859985 and parameters: {'observation_period_num': 201, 'train_rates': 0.9247608679269415, 'learning_rate': 0.0005557993646226032, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8373451765832389}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:07:14,515][0m Trial 31 finished with value: 0.3288407623767853 and parameters: {'observation_period_num': 105, 'train_rates': 0.9199855445945445, 'learning_rate': 0.00025794768496492204, 'batch_size': 237, 'step_size': 13, 'gamma': 0.776881407106855}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:07:51,629][0m Trial 32 finished with value: 0.3295210301876068 and parameters: {'observation_period_num': 34, 'train_rates': 0.957398010827414, 'learning_rate': 0.000698738383550101, 'batch_size': 225, 'step_size': 14, 'gamma': 0.8535253172951272}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:10:25,087][0m Trial 33 finished with value: 0.2661118868150209 and parameters: {'observation_period_num': 144, 'train_rates': 0.918904354500176, 'learning_rate': 0.0003759462681308282, 'batch_size': 168, 'step_size': 12, 'gamma': 0.8646170435736198}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:12:59,886][0m Trial 34 finished with value: 0.2723197638988495 and parameters: {'observation_period_num': 142, 'train_rates': 0.9492119003351484, 'learning_rate': 0.000415042119069748, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8629820134869}. Best is trial 11 with value: 0.25567272305488586.[0m
[32m[I 2025-02-07 16:16:44,318][0m Trial 35 finished with value: 0.2543867732106949 and parameters: {'observation_period_num': 196, 'train_rates': 0.9061327436538634, 'learning_rate': 0.00017563910600859023, 'batch_size': 127, 'step_size': 10, 'gamma': 0.8797862817926259}. Best is trial 35 with value: 0.2543867732106949.[0m
[32m[I 2025-02-07 16:20:35,540][0m Trial 36 finished with value: 0.2596340017087424 and parameters: {'observation_period_num': 199, 'train_rates': 0.903019959932298, 'learning_rate': 0.00016088877114188254, 'batch_size': 92, 'step_size': 10, 'gamma': 0.9116294581750762}. Best is trial 35 with value: 0.2543867732106949.[0m
[32m[I 2025-02-07 16:24:23,910][0m Trial 37 finished with value: 0.44067059829831123 and parameters: {'observation_period_num': 202, 'train_rates': 0.8604944066692964, 'learning_rate': 0.00021747901134582906, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9200641845132889}. Best is trial 35 with value: 0.2543867732106949.[0m
[32m[I 2025-02-07 16:27:43,490][0m Trial 38 finished with value: 0.24892952456706907 and parameters: {'observation_period_num': 168, 'train_rates': 0.9707249379020771, 'learning_rate': 0.00016467705629185964, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9592899763534913}. Best is trial 38 with value: 0.24892952456706907.[0m
[32m[I 2025-02-07 16:31:07,010][0m Trial 39 finished with value: 0.30365473127111475 and parameters: {'observation_period_num': 170, 'train_rates': 0.9663986702550336, 'learning_rate': 3.732473498384424e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.9897306783649565}. Best is trial 38 with value: 0.24892952456706907.[0m
[32m[I 2025-02-07 16:35:12,810][0m Trial 40 finished with value: 0.7128996663244374 and parameters: {'observation_period_num': 196, 'train_rates': 0.8196005446897665, 'learning_rate': 0.00015074126040720776, 'batch_size': 21, 'step_size': 7, 'gamma': 0.970206583821508}. Best is trial 38 with value: 0.24892952456706907.[0m
[32m[I 2025-02-07 16:39:34,244][0m Trial 41 finished with value: 0.32101092743597437 and parameters: {'observation_period_num': 217, 'train_rates': 0.9055934728588034, 'learning_rate': 6.256975269505834e-05, 'batch_size': 125, 'step_size': 8, 'gamma': 0.9641934514091077}. Best is trial 38 with value: 0.24892952456706907.[0m
[32m[I 2025-02-07 16:43:03,354][0m Trial 42 finished with value: 0.1567600816488266 and parameters: {'observation_period_num': 174, 'train_rates': 0.984514580657239, 'learning_rate': 0.00010828904694852491, 'batch_size': 65, 'step_size': 10, 'gamma': 0.946240550310895}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 16:46:28,461][0m Trial 43 finished with value: 0.17433898150920868 and parameters: {'observation_period_num': 171, 'train_rates': 0.9880750781780261, 'learning_rate': 0.0001014761603547814, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9468890650885354}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 16:50:08,381][0m Trial 44 finished with value: 0.20603987303647128 and parameters: {'observation_period_num': 175, 'train_rates': 0.9762582492354601, 'learning_rate': 9.25965784961599e-05, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9524060038960551}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 16:53:44,397][0m Trial 45 finished with value: 0.20079746382471955 and parameters: {'observation_period_num': 174, 'train_rates': 0.9712179572228995, 'learning_rate': 8.715798703569465e-05, 'batch_size': 46, 'step_size': 11, 'gamma': 0.9511317524703871}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 16:57:12,354][0m Trial 46 finished with value: 0.26192131387182027 and parameters: {'observation_period_num': 168, 'train_rates': 0.9702749796497232, 'learning_rate': 2.2353572748311185e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.9471039277350277}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 17:00:40,465][0m Trial 47 finished with value: 0.2065514624118805 and parameters: {'observation_period_num': 167, 'train_rates': 0.9859018925035863, 'learning_rate': 8.585895959564573e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.9509397852526519}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 17:03:37,382][0m Trial 48 finished with value: 0.1599382907152176 and parameters: {'observation_period_num': 139, 'train_rates': 0.9859356134916295, 'learning_rate': 8.66845249752056e-05, 'batch_size': 40, 'step_size': 11, 'gamma': 0.9790348805538939}. Best is trial 42 with value: 0.1567600816488266.[0m
[32m[I 2025-02-07 17:06:35,956][0m Trial 49 finished with value: 0.26225346284654905 and parameters: {'observation_period_num': 141, 'train_rates': 0.9623808853743366, 'learning_rate': 4.962361983264313e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9783253245455225}. Best is trial 42 with value: 0.1567600816488266.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-07 17:06:35,964][0m A new study created in memory with name: no-name-843819bc-ff10-4717-9b63-8fb224b1ab6d[0m
[32m[I 2025-02-07 17:08:46,211][0m Trial 0 finished with value: 0.27373383275601043 and parameters: {'observation_period_num': 32, 'train_rates': 0.9595620339863595, 'learning_rate': 2.3647203450785413e-05, 'batch_size': 38, 'step_size': 10, 'gamma': 0.9736335631234772}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:12:54,784][0m Trial 1 finished with value: 1.138967249607075 and parameters: {'observation_period_num': 213, 'train_rates': 0.6846716160859471, 'learning_rate': 3.485839273726641e-06, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7620580701388564}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:15:43,091][0m Trial 2 finished with value: 0.8416676621570766 and parameters: {'observation_period_num': 174, 'train_rates': 0.7691087517634314, 'learning_rate': 0.0009764556088895394, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9533955291011611}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:17:18,391][0m Trial 3 finished with value: 0.8187005866059492 and parameters: {'observation_period_num': 107, 'train_rates': 0.7329423596173353, 'learning_rate': 0.0005523559901285882, 'batch_size': 252, 'step_size': 14, 'gamma': 0.8865107455607286}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:18:48,098][0m Trial 4 finished with value: 0.8732147259869455 and parameters: {'observation_period_num': 98, 'train_rates': 0.625736354636951, 'learning_rate': 9.03590230487471e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8324886310070068}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:19:12,586][0m Trial 5 finished with value: 2.1108727779122596 and parameters: {'observation_period_num': 20, 'train_rates': 0.902520181804977, 'learning_rate': 1.0504925840809977e-06, 'batch_size': 255, 'step_size': 12, 'gamma': 0.7765958638821318}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:24:22,629][0m Trial 6 finished with value: 0.3424145961052751 and parameters: {'observation_period_num': 225, 'train_rates': 0.8664223308037631, 'learning_rate': 2.550416082220269e-05, 'batch_size': 19, 'step_size': 14, 'gamma': 0.812126728205493}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:28:40,893][0m Trial 7 finished with value: 1.793096645929185 and parameters: {'observation_period_num': 247, 'train_rates': 0.6792718444126308, 'learning_rate': 1.2748672373213934e-06, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9077818746732289}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:31:50,430][0m Trial 8 finished with value: 1.1925951428131152 and parameters: {'observation_period_num': 202, 'train_rates': 0.6503846093843283, 'learning_rate': 1.578481990446668e-05, 'batch_size': 100, 'step_size': 7, 'gamma': 0.9352834426031846}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:35:55,894][0m Trial 9 finished with value: 2.019921877845753 and parameters: {'observation_period_num': 211, 'train_rates': 0.7496547524822563, 'learning_rate': 1.119064957577374e-06, 'batch_size': 29, 'step_size': 3, 'gamma': 0.8239688456201877}. Best is trial 0 with value: 0.27373383275601043.[0m
[32m[I 2025-02-07 17:36:28,553][0m Trial 10 finished with value: 0.21423016488552094 and parameters: {'observation_period_num': 5, 'train_rates': 0.9859273355896871, 'learning_rate': 0.00011314657741325483, 'batch_size': 166, 'step_size': 7, 'gamma': 0.9800979856263933}. Best is trial 10 with value: 0.21423016488552094.[0m
[32m[I 2025-02-07 17:36:59,670][0m Trial 11 finished with value: 0.20722955465316772 and parameters: {'observation_period_num': 5, 'train_rates': 0.9885195413508866, 'learning_rate': 0.00015581003288676155, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9891500053604111}. Best is trial 11 with value: 0.20722955465316772.[0m
[32m[I 2025-02-07 17:38:05,497][0m Trial 12 finished with value: 0.20606006681919098 and parameters: {'observation_period_num': 63, 'train_rates': 0.9875769989356117, 'learning_rate': 0.00014123121767863688, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9886725301550586}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:39:04,005][0m Trial 13 finished with value: 0.41564074349621616 and parameters: {'observation_period_num': 63, 'train_rates': 0.8491114537768855, 'learning_rate': 0.00021033769735640385, 'batch_size': 197, 'step_size': 5, 'gamma': 0.988727623658356}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:40:02,023][0m Trial 14 finished with value: 0.2551648224272379 and parameters: {'observation_period_num': 60, 'train_rates': 0.915005600528783, 'learning_rate': 0.00026926543858668545, 'batch_size': 202, 'step_size': 5, 'gamma': 0.9246152174109898}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:40:54,602][0m Trial 15 finished with value: 0.3632213441225199 and parameters: {'observation_period_num': 53, 'train_rates': 0.9330026485441278, 'learning_rate': 6.402062948937896e-05, 'batch_size': 187, 'step_size': 9, 'gamma': 0.8788821230142337}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:43:13,281][0m Trial 16 finished with value: 0.4238197897091385 and parameters: {'observation_period_num': 136, 'train_rates': 0.8282809194239987, 'learning_rate': 0.0002662114446060726, 'batch_size': 123, 'step_size': 5, 'gamma': 0.9549353126423138}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:44:38,118][0m Trial 17 finished with value: 0.9402331113815308 and parameters: {'observation_period_num': 83, 'train_rates': 0.973209471510651, 'learning_rate': 8.217626453544358e-06, 'batch_size': 225, 'step_size': 8, 'gamma': 0.8549565309779931}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:47:03,714][0m Trial 18 finished with value: 0.6438963259570301 and parameters: {'observation_period_num': 138, 'train_rates': 0.8865429943918962, 'learning_rate': 5.4052925137525205e-05, 'batch_size': 155, 'step_size': 1, 'gamma': 0.9133167707283724}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:47:45,690][0m Trial 19 finished with value: 0.41498071828099803 and parameters: {'observation_period_num': 39, 'train_rates': 0.8220551118821003, 'learning_rate': 0.0001777480019057349, 'batch_size': 125, 'step_size': 4, 'gamma': 0.9531937248485398}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:48:39,398][0m Trial 20 finished with value: 0.3973809718280225 and parameters: {'observation_period_num': 9, 'train_rates': 0.9372421969991549, 'learning_rate': 0.000499310059577274, 'batch_size': 95, 'step_size': 7, 'gamma': 0.9642414253180233}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:49:10,573][0m Trial 21 finished with value: 0.2384757697582245 and parameters: {'observation_period_num': 7, 'train_rates': 0.9759236118795404, 'learning_rate': 0.00010385142376307002, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9886276973528958}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:49:52,031][0m Trial 22 finished with value: 0.27755868434906006 and parameters: {'observation_period_num': 38, 'train_rates': 0.972809588609754, 'learning_rate': 0.00012974169556408265, 'batch_size': 168, 'step_size': 6, 'gamma': 0.9375298974661856}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:51:16,297][0m Trial 23 finished with value: 0.25075963139533997 and parameters: {'observation_period_num': 81, 'train_rates': 0.986088022679766, 'learning_rate': 4.689708423223176e-05, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9792597201662845}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:51:47,473][0m Trial 24 finished with value: 0.3009812831878662 and parameters: {'observation_period_num': 27, 'train_rates': 0.9399581323432873, 'learning_rate': 0.00046682222005236433, 'batch_size': 230, 'step_size': 8, 'gamma': 0.9707869779440564}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:52:15,744][0m Trial 25 finished with value: 0.3100058287382126 and parameters: {'observation_period_num': 9, 'train_rates': 0.8772695025559474, 'learning_rate': 0.0001313709298791548, 'batch_size': 181, 'step_size': 6, 'gamma': 0.8972549148251587}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:53:06,952][0m Trial 26 finished with value: 0.516030044545161 and parameters: {'observation_period_num': 50, 'train_rates': 0.9221716858952289, 'learning_rate': 4.113791588039319e-05, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9362812426396763}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:54:30,942][0m Trial 27 finished with value: 0.31676802039146423 and parameters: {'observation_period_num': 73, 'train_rates': 0.9878080547776189, 'learning_rate': 0.0003633504509231274, 'batch_size': 135, 'step_size': 9, 'gamma': 0.9895667890455968}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:56:28,036][0m Trial 28 finished with value: 0.28434310496692927 and parameters: {'observation_period_num': 107, 'train_rates': 0.9502662639895294, 'learning_rate': 7.604287895519094e-05, 'batch_size': 108, 'step_size': 6, 'gamma': 0.9587093230397469}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:57:04,358][0m Trial 29 finished with value: 0.7399253249168396 and parameters: {'observation_period_num': 29, 'train_rates': 0.9537994431938313, 'learning_rate': 1.54953611803755e-05, 'batch_size': 163, 'step_size': 11, 'gamma': 0.8648502688471447}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:57:48,391][0m Trial 30 finished with value: 0.46902345115565913 and parameters: {'observation_period_num': 43, 'train_rates': 0.904213182956298, 'learning_rate': 0.0009905594671300027, 'batch_size': 220, 'step_size': 10, 'gamma': 0.9723896884122816}. Best is trial 12 with value: 0.20606006681919098.[0m
[32m[I 2025-02-07 17:58:19,777][0m Trial 31 finished with value: 0.19032911956310272 and parameters: {'observation_period_num': 8, 'train_rates': 0.9886463350722028, 'learning_rate': 0.00011451733459865281, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9877302632288697}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 17:58:49,644][0m Trial 32 finished with value: 0.2634586989879608 and parameters: {'observation_period_num': 23, 'train_rates': 0.9586424730236336, 'learning_rate': 0.00014486288756915267, 'batch_size': 198, 'step_size': 7, 'gamma': 0.9707954631591512}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 17:59:19,026][0m Trial 33 finished with value: 0.45815449953079224 and parameters: {'observation_period_num': 5, 'train_rates': 0.9860028622598068, 'learning_rate': 3.40015560851472e-05, 'batch_size': 182, 'step_size': 4, 'gamma': 0.944915590074951}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 17:59:56,069][0m Trial 34 finished with value: 0.2826892137527466 and parameters: {'observation_period_num': 22, 'train_rates': 0.9584733197886078, 'learning_rate': 8.565773488899009e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9709731037639531}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:02:01,423][0m Trial 35 finished with value: 0.21621841192245483 and parameters: {'observation_period_num': 120, 'train_rates': 0.989724255075876, 'learning_rate': 0.0002957304890292338, 'batch_size': 167, 'step_size': 6, 'gamma': 0.9810871288252302}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:02:34,161][0m Trial 36 finished with value: 0.28145045042037964 and parameters: {'observation_period_num': 32, 'train_rates': 0.928330426354086, 'learning_rate': 0.00018560435329185602, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9501169994298525}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:03:36,145][0m Trial 37 finished with value: 0.7669701816943976 and parameters: {'observation_period_num': 67, 'train_rates': 0.783953287745623, 'learning_rate': 0.0001025209710393783, 'batch_size': 187, 'step_size': 8, 'gamma': 0.7581983627866058}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:06:30,722][0m Trial 38 finished with value: 0.2841181294205262 and parameters: {'observation_period_num': 163, 'train_rates': 0.8960885552576486, 'learning_rate': 0.0005949756398501735, 'batch_size': 144, 'step_size': 4, 'gamma': 0.9201719096768533}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:07:47,652][0m Trial 39 finished with value: 0.8518843745404587 and parameters: {'observation_period_num': 91, 'train_rates': 0.7226709754653227, 'learning_rate': 6.296326198667962e-05, 'batch_size': 211, 'step_size': 12, 'gamma': 0.7856602367785831}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:08:25,253][0m Trial 40 finished with value: 0.5397410009588514 and parameters: {'observation_period_num': 18, 'train_rates': 0.8571494180750968, 'learning_rate': 2.3766611626139772e-05, 'batch_size': 130, 'step_size': 5, 'gamma': 0.958838722140564}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:12:04,815][0m Trial 41 finished with value: 0.19147558510303497 and parameters: {'observation_period_num': 181, 'train_rates': 0.9896997972711663, 'learning_rate': 0.0003019522030846118, 'batch_size': 171, 'step_size': 6, 'gamma': 0.9811150702677358}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:15:18,925][0m Trial 42 finished with value: 0.3944639563560486 and parameters: {'observation_period_num': 172, 'train_rates': 0.9638251958005793, 'learning_rate': 0.0006613412199644972, 'batch_size': 158, 'step_size': 7, 'gamma': 0.9806159195124664}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:18:11,034][0m Trial 43 finished with value: 0.272558331489563 and parameters: {'observation_period_num': 151, 'train_rates': 0.9524310809334987, 'learning_rate': 0.0003701725126043888, 'batch_size': 178, 'step_size': 6, 'gamma': 0.9778489710252607}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:22:06,250][0m Trial 44 finished with value: 0.30035922390719255 and parameters: {'observation_period_num': 202, 'train_rates': 0.9129150601348368, 'learning_rate': 0.00024462091985596313, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9898049011283488}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:26:53,474][0m Trial 45 finished with value: 0.27428216789707993 and parameters: {'observation_period_num': 235, 'train_rates': 0.9396187652245241, 'learning_rate': 0.0001607466511421683, 'batch_size': 148, 'step_size': 7, 'gamma': 0.9436310200146002}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:30:29,473][0m Trial 46 finished with value: 0.3066731095314026 and parameters: {'observation_period_num': 188, 'train_rates': 0.9690698901278272, 'learning_rate': 0.00011765906633069032, 'batch_size': 209, 'step_size': 5, 'gamma': 0.9635770447698447}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:31:14,447][0m Trial 47 finished with value: 1.0137711627276864 and parameters: {'observation_period_num': 54, 'train_rates': 0.6934988692170823, 'learning_rate': 0.0007469915295430756, 'batch_size': 175, 'step_size': 9, 'gamma': 0.931111096694246}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:31:45,723][0m Trial 48 finished with value: 0.9810903251171113 and parameters: {'observation_period_num': 16, 'train_rates': 0.612294646166951, 'learning_rate': 0.000352799927872654, 'batch_size': 114, 'step_size': 4, 'gamma': 0.9634343952740098}. Best is trial 31 with value: 0.19032911956310272.[0m
[32m[I 2025-02-07 18:33:06,028][0m Trial 49 finished with value: 1.1650267405943437 and parameters: {'observation_period_num': 42, 'train_rates': 0.9697942844296835, 'learning_rate': 1.5929282536539284e-06, 'batch_size': 64, 'step_size': 6, 'gamma': 0.9792638650043775}. Best is trial 31 with value: 0.19032911956310272.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-07 18:33:06,036][0m A new study created in memory with name: no-name-5740a6b9-6bb4-4ac1-af17-0a4541b7f44a[0m
[32m[I 2025-02-07 18:34:50,369][0m Trial 0 finished with value: 0.8301222031789999 and parameters: {'observation_period_num': 127, 'train_rates': 0.6707481740932421, 'learning_rate': 0.0002274573888043693, 'batch_size': 180, 'step_size': 6, 'gamma': 0.7956804847382727}. Best is trial 0 with value: 0.8301222031789999.[0m
[32m[I 2025-02-07 18:37:11,537][0m Trial 1 finished with value: 1.2431814519680693 and parameters: {'observation_period_num': 163, 'train_rates': 0.6576441454548939, 'learning_rate': 5.9131269104640915e-05, 'batch_size': 250, 'step_size': 2, 'gamma': 0.9083715870927321}. Best is trial 0 with value: 0.8301222031789999.[0m
[32m[I 2025-02-07 18:40:28,221][0m Trial 2 finished with value: 1.2756496249650517 and parameters: {'observation_period_num': 213, 'train_rates': 0.6002082939209679, 'learning_rate': 1.1877297717679758e-05, 'batch_size': 65, 'step_size': 12, 'gamma': 0.7959782366487111}. Best is trial 0 with value: 0.8301222031789999.[0m
[32m[I 2025-02-07 18:41:25,660][0m Trial 3 finished with value: 0.7158863893412551 and parameters: {'observation_period_num': 61, 'train_rates': 0.7674959252722588, 'learning_rate': 3.043982076784668e-05, 'batch_size': 98, 'step_size': 15, 'gamma': 0.7638988385809854}. Best is trial 3 with value: 0.7158863893412551.[0m
[32m[I 2025-02-07 18:43:54,662][0m Trial 4 finished with value: 0.3206515932305259 and parameters: {'observation_period_num': 28, 'train_rates': 0.8900513341930307, 'learning_rate': 0.0001722301020729049, 'batch_size': 31, 'step_size': 7, 'gamma': 0.963781107562785}. Best is trial 4 with value: 0.3206515932305259.[0m
[32m[I 2025-02-07 18:45:49,492][0m Trial 5 finished with value: 1.554446421340554 and parameters: {'observation_period_num': 127, 'train_rates': 0.7638650770031326, 'learning_rate': 5.507870894414738e-06, 'batch_size': 166, 'step_size': 4, 'gamma': 0.9176182679999854}. Best is trial 4 with value: 0.3206515932305259.[0m
[32m[I 2025-02-07 18:48:30,327][0m Trial 6 finished with value: 0.9996514631901416 and parameters: {'observation_period_num': 180, 'train_rates': 0.6499925729922509, 'learning_rate': 0.0006130865187556966, 'batch_size': 98, 'step_size': 7, 'gamma': 0.7566019164277138}. Best is trial 4 with value: 0.3206515932305259.[0m
[32m[I 2025-02-07 18:49:46,774][0m Trial 7 finished with value: 0.40308226216329285 and parameters: {'observation_period_num': 80, 'train_rates': 0.8984363616319538, 'learning_rate': 0.00011355469583161145, 'batch_size': 245, 'step_size': 11, 'gamma': 0.7687362010000143}. Best is trial 4 with value: 0.3206515932305259.[0m
[32m[I 2025-02-07 18:51:05,824][0m Trial 8 finished with value: 1.2189379711286414 and parameters: {'observation_period_num': 97, 'train_rates': 0.6733755666298621, 'learning_rate': 1.6129314071766957e-05, 'batch_size': 201, 'step_size': 13, 'gamma': 0.7846031732408505}. Best is trial 4 with value: 0.3206515932305259.[0m
[32m[I 2025-02-07 18:55:10,494][0m Trial 9 finished with value: 0.26265655240120245 and parameters: {'observation_period_num': 197, 'train_rates': 0.9380906404486258, 'learning_rate': 0.00019158159926844102, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8313160979911347}. Best is trial 9 with value: 0.26265655240120245.[0m
[32m[I 2025-02-07 19:00:32,944][0m Trial 10 finished with value: 1.4518680572509766 and parameters: {'observation_period_num': 249, 'train_rates': 0.9817806559374885, 'learning_rate': 1.5654120516608075e-06, 'batch_size': 133, 'step_size': 10, 'gamma': 0.851327877997902}. Best is trial 9 with value: 0.26265655240120245.[0m
[32m[I 2025-02-07 19:03:11,551][0m Trial 11 finished with value: 2.3628226285855445 and parameters: {'observation_period_num': 40, 'train_rates': 0.8842732457892931, 'learning_rate': 0.0008361709127896414, 'batch_size': 29, 'step_size': 8, 'gamma': 0.9713871400571829}. Best is trial 9 with value: 0.26265655240120245.[0m
[32m[I 2025-02-07 19:07:06,792][0m Trial 12 finished with value: 0.19237796783447267 and parameters: {'observation_period_num': 15, 'train_rates': 0.9149778452191653, 'learning_rate': 0.00020803661838281575, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8470553114609486}. Best is trial 12 with value: 0.19237796783447267.[0m
Early stopping at epoch 75
[32m[I 2025-02-07 19:08:03,078][0m Trial 13 finished with value: 0.6773133452122028 and parameters: {'observation_period_num': 11, 'train_rates': 0.9559134555924677, 'learning_rate': 0.00035740890407578025, 'batch_size': 68, 'step_size': 1, 'gamma': 0.840028887702565}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:11:12,234][0m Trial 14 finished with value: 0.5407127412994213 and parameters: {'observation_period_num': 173, 'train_rates': 0.8253740631898342, 'learning_rate': 7.267130530498729e-05, 'batch_size': 61, 'step_size': 4, 'gamma': 0.8301039302649686}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:16:23,017][0m Trial 15 finished with value: 0.4163788853429299 and parameters: {'observation_period_num': 223, 'train_rates': 0.9339972317603922, 'learning_rate': 0.00037801084252663995, 'batch_size': 21, 'step_size': 4, 'gamma': 0.8852984256447933}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:18:58,692][0m Trial 16 finished with value: 0.526771261560436 and parameters: {'observation_period_num': 152, 'train_rates': 0.8320766001811309, 'learning_rate': 4.638311577668383e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8190509354735256}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:21:00,906][0m Trial 17 finished with value: 0.25248672354397517 and parameters: {'observation_period_num': 108, 'train_rates': 0.928765252199008, 'learning_rate': 0.00013901432910500296, 'batch_size': 54, 'step_size': 5, 'gamma': 0.8747017797019114}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:22:39,643][0m Trial 18 finished with value: 0.6375763064200899 and parameters: {'observation_period_num': 81, 'train_rates': 0.8426992487765013, 'learning_rate': 2.3734925660786876e-05, 'batch_size': 46, 'step_size': 3, 'gamma': 0.8867579489244904}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:24:28,572][0m Trial 19 finished with value: 0.21904122829437256 and parameters: {'observation_period_num': 103, 'train_rates': 0.9854100303704707, 'learning_rate': 8.215106011905433e-05, 'batch_size': 124, 'step_size': 5, 'gamma': 0.934175607770449}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:25:06,958][0m Trial 20 finished with value: 1.4493767023086548 and parameters: {'observation_period_num': 5, 'train_rates': 0.98970503230939, 'learning_rate': 6.234310170245936e-06, 'batch_size': 134, 'step_size': 1, 'gamma': 0.9426692289715785}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:27:11,435][0m Trial 21 finished with value: 0.24380989594662444 and parameters: {'observation_period_num': 106, 'train_rates': 0.9176856461556806, 'learning_rate': 0.00012050189107298469, 'batch_size': 45, 'step_size': 5, 'gamma': 0.8650768357134255}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:31:20,756][0m Trial 22 finished with value: 0.2973789537998394 and parameters: {'observation_period_num': 59, 'train_rates': 0.8680796059444997, 'learning_rate': 8.4440151905454e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.858253810986131}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:34:05,350][0m Trial 23 finished with value: 0.2785853603482246 and parameters: {'observation_period_num': 142, 'train_rates': 0.9646814769279728, 'learning_rate': 0.00036773855606969685, 'batch_size': 82, 'step_size': 3, 'gamma': 0.909868489876432}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:35:48,237][0m Trial 24 finished with value: 0.37229217915420987 and parameters: {'observation_period_num': 104, 'train_rates': 0.9122460434338805, 'learning_rate': 5.393664391656488e-05, 'batch_size': 159, 'step_size': 6, 'gamma': 0.9427445177417905}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:36:29,171][0m Trial 25 finished with value: 0.6777665432819651 and parameters: {'observation_period_num': 43, 'train_rates': 0.7923891949661777, 'learning_rate': 0.0002824874898262674, 'batch_size': 210, 'step_size': 5, 'gamma': 0.8690767712188677}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:38:13,960][0m Trial 26 finished with value: 0.4023514466521181 and parameters: {'observation_period_num': 81, 'train_rates': 0.859276054469004, 'learning_rate': 0.00011888203825667394, 'batch_size': 44, 'step_size': 3, 'gamma': 0.9887215124923868}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:40:19,716][0m Trial 27 finished with value: 0.4087235804011182 and parameters: {'observation_period_num': 119, 'train_rates': 0.9567326116584048, 'learning_rate': 0.0006119903994544389, 'batch_size': 117, 'step_size': 8, 'gamma': 0.9304079759323192}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:41:55,311][0m Trial 28 finished with value: 0.8728656929892462 and parameters: {'observation_period_num': 56, 'train_rates': 0.7319573557747803, 'learning_rate': 3.192763124181547e-05, 'batch_size': 42, 'step_size': 2, 'gamma': 0.8933985214270804}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:44:31,503][0m Trial 29 finished with value: 0.38906194139812994 and parameters: {'observation_period_num': 141, 'train_rates': 0.9146494430566035, 'learning_rate': 8.94113496550976e-05, 'batch_size': 78, 'step_size': 6, 'gamma': 0.8134916664490763}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:45:06,604][0m Trial 30 finished with value: 0.22282223403453827 and parameters: {'observation_period_num': 22, 'train_rates': 0.972888237590541, 'learning_rate': 0.00023723630361938857, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8533091712714557}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:45:42,049][0m Trial 31 finished with value: 0.2237394154071808 and parameters: {'observation_period_num': 26, 'train_rates': 0.9894420260933504, 'learning_rate': 0.0002205966488391359, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8535675010628458}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:46:17,158][0m Trial 32 finished with value: 0.20560260117053986 and parameters: {'observation_period_num': 22, 'train_rates': 0.9894794555427313, 'learning_rate': 0.00023218882688285866, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8471766630488938}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:46:47,141][0m Trial 33 finished with value: 0.26356640458106995 and parameters: {'observation_period_num': 20, 'train_rates': 0.9599480472871841, 'learning_rate': 0.0005317765472121446, 'batch_size': 176, 'step_size': 9, 'gamma': 0.8145899423296205}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:47:33,457][0m Trial 34 finished with value: 0.30057716369628906 and parameters: {'observation_period_num': 43, 'train_rates': 0.9681000162133983, 'learning_rate': 0.0002517690747133521, 'batch_size': 194, 'step_size': 8, 'gamma': 0.8449429520043095}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:48:09,886][0m Trial 35 finished with value: 0.2925426956400814 and parameters: {'observation_period_num': 31, 'train_rates': 0.9433768304719108, 'learning_rate': 0.00016481032928136895, 'batch_size': 146, 'step_size': 7, 'gamma': 0.903399093464732}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:49:21,398][0m Trial 36 finished with value: 0.858494222164154 and parameters: {'observation_period_num': 69, 'train_rates': 0.9730379193620121, 'learning_rate': 4.641328338394268e-05, 'batch_size': 224, 'step_size': 4, 'gamma': 0.793805298849994}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:49:50,848][0m Trial 37 finished with value: 0.23741628229618073 and parameters: {'observation_period_num': 5, 'train_rates': 0.9450199761528808, 'learning_rate': 0.0004760091295953871, 'batch_size': 181, 'step_size': 6, 'gamma': 0.8785797339078207}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:50:44,115][0m Trial 38 finished with value: 0.6227969634951207 and parameters: {'observation_period_num': 52, 'train_rates': 0.9020887721893422, 'learning_rate': 0.0008590383313928274, 'batch_size': 121, 'step_size': 2, 'gamma': 0.8325707081521782}. Best is trial 12 with value: 0.19237796783447267.[0m
[32m[I 2025-02-07 19:51:22,806][0m Trial 39 finished with value: 0.1635187268257141 and parameters: {'observation_period_num': 19, 'train_rates': 0.989966340772602, 'learning_rate': 0.00026443297906503704, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9002720822461073}. Best is trial 39 with value: 0.1635187268257141.[0m
[32m[I 2025-02-07 19:52:32,032][0m Trial 40 finished with value: 0.33382561926014165 and parameters: {'observation_period_num': 69, 'train_rates': 0.8742190075089675, 'learning_rate': 0.00015928764003798594, 'batch_size': 144, 'step_size': 15, 'gamma': 0.9232827561829664}. Best is trial 39 with value: 0.1635187268257141.[0m
[32m[I 2025-02-07 19:53:03,714][0m Trial 41 finished with value: 0.15193605422973633 and parameters: {'observation_period_num': 20, 'train_rates': 0.9770243428128543, 'learning_rate': 0.00025945020984547996, 'batch_size': 178, 'step_size': 7, 'gamma': 0.8992789759004131}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:53:35,179][0m Trial 42 finished with value: 0.1670129895210266 and parameters: {'observation_period_num': 18, 'train_rates': 0.989841313837057, 'learning_rate': 0.0003051312506621142, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9072539544577297}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:54:12,990][0m Trial 43 finished with value: 0.24373771984185746 and parameters: {'observation_period_num': 34, 'train_rates': 0.9313202325495848, 'learning_rate': 0.0003465618761423515, 'batch_size': 192, 'step_size': 9, 'gamma': 0.9004783467444962}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:54:44,709][0m Trial 44 finished with value: 0.20966903865337372 and parameters: {'observation_period_num': 16, 'train_rates': 0.9524783364241288, 'learning_rate': 0.00043749260384402306, 'batch_size': 171, 'step_size': 11, 'gamma': 0.9122895830447422}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:55:32,946][0m Trial 45 finished with value: 0.23833052814006805 and parameters: {'observation_period_num': 46, 'train_rates': 0.9898080516964505, 'learning_rate': 0.0006674470749047018, 'batch_size': 219, 'step_size': 7, 'gamma': 0.8973359925183888}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:56:02,855][0m Trial 46 finished with value: 1.7101633548736572 and parameters: {'observation_period_num': 14, 'train_rates': 0.9657095515501141, 'learning_rate': 1.3100337546776213e-06, 'batch_size': 185, 'step_size': 13, 'gamma': 0.9559478837671163}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:56:36,469][0m Trial 47 finished with value: 0.687176962951561 and parameters: {'observation_period_num': 34, 'train_rates': 0.7364056349503436, 'learning_rate': 0.0002929786399215038, 'batch_size': 148, 'step_size': 10, 'gamma': 0.883685004998755}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:57:24,515][0m Trial 48 finished with value: 0.29251876104445684 and parameters: {'observation_period_num': 24, 'train_rates': 0.8926911164133219, 'learning_rate': 0.00021438714567912376, 'batch_size': 101, 'step_size': 8, 'gamma': 0.9211827140625873}. Best is trial 41 with value: 0.15193605422973633.[0m
[32m[I 2025-02-07 19:58:34,532][0m Trial 49 finished with value: 0.265131413936615 and parameters: {'observation_period_num': 70, 'train_rates': 0.9259787786346652, 'learning_rate': 0.0007292857754858836, 'batch_size': 238, 'step_size': 10, 'gamma': 0.8636150267367538}. Best is trial 41 with value: 0.15193605422973633.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-07 19:58:34,539][0m A new study created in memory with name: no-name-e709dfbf-0e1a-439e-92e5-24611f1c1a60[0m
[32m[I 2025-02-07 19:59:49,648][0m Trial 0 finished with value: 0.7817042620712622 and parameters: {'observation_period_num': 84, 'train_rates': 0.7590718222457156, 'learning_rate': 0.0004048429735802031, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8658197290409897}. Best is trial 0 with value: 0.7817042620712622.[0m
[32m[I 2025-02-07 20:02:32,320][0m Trial 1 finished with value: 0.3797847469045658 and parameters: {'observation_period_num': 151, 'train_rates': 0.894376767022945, 'learning_rate': 7.313631072403687e-05, 'batch_size': 251, 'step_size': 12, 'gamma': 0.9166791468211511}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:04:05,979][0m Trial 2 finished with value: 2.1127043729770683 and parameters: {'observation_period_num': 113, 'train_rates': 0.7064027522079919, 'learning_rate': 2.7101572135204607e-06, 'batch_size': 218, 'step_size': 9, 'gamma': 0.8081840215410485}. Best is trial 1 with value: 0.3797847469045658.[0m
Early stopping at epoch 60
[32m[I 2025-02-07 20:06:41,304][0m Trial 3 finished with value: 2.3988692155655693 and parameters: {'observation_period_num': 245, 'train_rates': 0.6990393050541521, 'learning_rate': 6.705863456227002e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.820331362050475}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:08:25,856][0m Trial 4 finished with value: 0.789165468069545 and parameters: {'observation_period_num': 93, 'train_rates': 0.840624462938154, 'learning_rate': 0.0005098821225336461, 'batch_size': 52, 'step_size': 12, 'gamma': 0.9077583427281142}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:09:35,476][0m Trial 5 finished with value: 1.5773955109253737 and parameters: {'observation_period_num': 72, 'train_rates': 0.9016213292311055, 'learning_rate': 3.5562286474890978e-06, 'batch_size': 247, 'step_size': 7, 'gamma': 0.8527138633085196}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:10:29,694][0m Trial 6 finished with value: 1.8061384980271502 and parameters: {'observation_period_num': 49, 'train_rates': 0.915585508272905, 'learning_rate': 1.0408710060335242e-06, 'batch_size': 112, 'step_size': 13, 'gamma': 0.8472423498601909}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:12:02,003][0m Trial 7 finished with value: 1.32582965133013 and parameters: {'observation_period_num': 122, 'train_rates': 0.6058833195956722, 'learning_rate': 0.0007497245635882038, 'batch_size': 222, 'step_size': 15, 'gamma': 0.8886000128518342}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:13:21,308][0m Trial 8 finished with value: 1.292044819930461 and parameters: {'observation_period_num': 96, 'train_rates': 0.6659588540859743, 'learning_rate': 0.0008328302176227769, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8253394117185265}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:15:24,132][0m Trial 9 finished with value: 1.2186410422281828 and parameters: {'observation_period_num': 141, 'train_rates': 0.6477789468898223, 'learning_rate': 0.0006533144187484292, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8864188161557061}. Best is trial 1 with value: 0.3797847469045658.[0m
[32m[I 2025-02-07 20:18:55,412][0m Trial 10 finished with value: 0.24068845808506012 and parameters: {'observation_period_num': 182, 'train_rates': 0.9898319263000741, 'learning_rate': 6.410010295072568e-05, 'batch_size': 181, 'step_size': 6, 'gamma': 0.9787115187248603}. Best is trial 10 with value: 0.24068845808506012.[0m
[32m[I 2025-02-07 20:22:40,138][0m Trial 11 finished with value: 0.20528629422187805 and parameters: {'observation_period_num': 187, 'train_rates': 0.9825565830132581, 'learning_rate': 7.356461823675044e-05, 'batch_size': 194, 'step_size': 6, 'gamma': 0.9773985139225294}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:26:33,324][0m Trial 12 finished with value: 0.2618955671787262 and parameters: {'observation_period_num': 193, 'train_rates': 0.9889885418405084, 'learning_rate': 5.730003769812903e-05, 'batch_size': 186, 'step_size': 5, 'gamma': 0.9875205829103995}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:30:16,970][0m Trial 13 finished with value: 0.3943379819393158 and parameters: {'observation_period_num': 190, 'train_rates': 0.98224468075143, 'learning_rate': 2.1047087363804633e-05, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9893457829978696}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:34:12,286][0m Trial 14 finished with value: 0.2461974024772644 and parameters: {'observation_period_num': 195, 'train_rates': 0.9484252464762578, 'learning_rate': 0.00018202517740176626, 'batch_size': 185, 'step_size': 4, 'gamma': 0.9467525541551249}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:38:49,040][0m Trial 15 finished with value: 0.9075343464439661 and parameters: {'observation_period_num': 244, 'train_rates': 0.8325548503400623, 'learning_rate': 1.8841223773168176e-05, 'batch_size': 205, 'step_size': 8, 'gamma': 0.7520186386640457}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:41:48,300][0m Trial 16 finished with value: 0.4292430559743033 and parameters: {'observation_period_num': 167, 'train_rates': 0.8604797665817365, 'learning_rate': 0.00014442039772650744, 'batch_size': 161, 'step_size': 2, 'gamma': 0.944058348265328}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:45:38,743][0m Trial 17 finished with value: 0.7206090202667569 and parameters: {'observation_period_num': 218, 'train_rates': 0.7824249836022337, 'learning_rate': 4.3657887879276e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9536353246084511}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:48:47,591][0m Trial 18 finished with value: 0.24606549739837646 and parameters: {'observation_period_num': 169, 'train_rates': 0.9497709483751653, 'learning_rate': 0.00014776308419194804, 'batch_size': 203, 'step_size': 9, 'gamma': 0.9697038548969444}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:53:17,742][0m Trial 19 finished with value: 0.8340114317834377 and parameters: {'observation_period_num': 221, 'train_rates': 0.9416455136942605, 'learning_rate': 1.126055492192841e-05, 'batch_size': 130, 'step_size': 7, 'gamma': 0.9250941340416748}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:56:20,519][0m Trial 20 finished with value: 0.20869004642912026 and parameters: {'observation_period_num': 18, 'train_rates': 0.8815022763486977, 'learning_rate': 9.288716841186648e-05, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9668247857768026}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 20:57:53,392][0m Trial 21 finished with value: 0.25835597797258364 and parameters: {'observation_period_num': 29, 'train_rates': 0.8845275778479421, 'learning_rate': 9.340723835439377e-05, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9646827469676625}. Best is trial 11 with value: 0.20528629422187805.[0m
[32m[I 2025-02-07 21:00:30,558][0m Trial 22 finished with value: 0.1650585894649093 and parameters: {'observation_period_num': 5, 'train_rates': 0.9877373558321083, 'learning_rate': 0.0002555460019121542, 'batch_size': 32, 'step_size': 4, 'gamma': 0.9312863796186431}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:05:27,821][0m Trial 23 finished with value: 0.23294847053924794 and parameters: {'observation_period_num': 16, 'train_rates': 0.9332550152122008, 'learning_rate': 0.0002696105729521284, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9338873580494663}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:09:36,685][0m Trial 24 finished with value: 0.5506623661377977 and parameters: {'observation_period_num': 6, 'train_rates': 0.8125557116271144, 'learning_rate': 0.0002556112233022616, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9624437097129622}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:10:36,221][0m Trial 25 finished with value: 0.5185695521514434 and parameters: {'observation_period_num': 51, 'train_rates': 0.8655657809021902, 'learning_rate': 3.6008088477411675e-05, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9043213793549929}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:12:47,917][0m Trial 26 finished with value: 0.2083889193394605 and parameters: {'observation_period_num': 34, 'train_rates': 0.9653076194461153, 'learning_rate': 0.0001305144003981947, 'batch_size': 38, 'step_size': 5, 'gamma': 0.936165914553814}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:14:49,295][0m Trial 27 finished with value: 0.27452159491745204 and parameters: {'observation_period_num': 45, 'train_rates': 0.961937391426452, 'learning_rate': 0.00028117779100353575, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9326323655752361}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:16:04,011][0m Trial 28 finished with value: 0.5314927123211048 and parameters: {'observation_period_num': 65, 'train_rates': 0.9254378319592867, 'learning_rate': 2.294020042102633e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8919873715074077}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:17:12,659][0m Trial 29 finished with value: 0.25882834714391956 and parameters: {'observation_period_num': 39, 'train_rates': 0.968653381717117, 'learning_rate': 0.0003873455425739697, 'batch_size': 74, 'step_size': 6, 'gamma': 0.8662892256312743}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:18:27,349][0m Trial 30 finished with value: 0.6521759298416453 and parameters: {'observation_period_num': 72, 'train_rates': 0.7733591625191396, 'learning_rate': 0.0001149125287009093, 'batch_size': 62, 'step_size': 10, 'gamma': 0.9469494121419433}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:20:51,709][0m Trial 31 finished with value: 0.27175504936460865 and parameters: {'observation_period_num': 20, 'train_rates': 0.9613711757066293, 'learning_rate': 9.710065942901969e-05, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9718985780466655}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:23:22,430][0m Trial 32 finished with value: 0.21916655856731523 and parameters: {'observation_period_num': 9, 'train_rates': 0.9047522848311201, 'learning_rate': 0.00020145858556182324, 'batch_size': 31, 'step_size': 2, 'gamma': 0.9147486785716042}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:24:35,144][0m Trial 33 finished with value: 0.7312014459926177 and parameters: {'observation_period_num': 26, 'train_rates': 0.7446193017458754, 'learning_rate': 6.564137489564558e-05, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9584702543574337}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:27:17,912][0m Trial 34 finished with value: 0.4851945186598917 and parameters: {'observation_period_num': 33, 'train_rates': 0.8784328036387644, 'learning_rate': 0.0004015140836812486, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9279517161626946}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:30:00,007][0m Trial 35 finished with value: 0.26172996576015767 and parameters: {'observation_period_num': 141, 'train_rates': 0.93088661691635, 'learning_rate': 4.455486558209637e-05, 'batch_size': 67, 'step_size': 5, 'gamma': 0.9771268812587824}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:32:06,319][0m Trial 36 finished with value: 0.36388860558563807 and parameters: {'observation_period_num': 106, 'train_rates': 0.9629221570716338, 'learning_rate': 8.700564028364286e-05, 'batch_size': 46, 'step_size': 1, 'gamma': 0.9408704495307143}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:33:10,257][0m Trial 37 finished with value: 0.2844820167842529 and parameters: {'observation_period_num': 60, 'train_rates': 0.9040440047264873, 'learning_rate': 0.0001337563546130682, 'batch_size': 91, 'step_size': 3, 'gamma': 0.9184554525640934}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:34:35,498][0m Trial 38 finished with value: 0.7049924011198466 and parameters: {'observation_period_num': 87, 'train_rates': 0.8442695481839071, 'learning_rate': 2.7603335361202567e-05, 'batch_size': 162, 'step_size': 8, 'gamma': 0.9022402243683975}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:37:42,866][0m Trial 39 finished with value: 0.42060424467502544 and parameters: {'observation_period_num': 15, 'train_rates': 0.9178077438540247, 'learning_rate': 1.2666710773666493e-05, 'batch_size': 25, 'step_size': 6, 'gamma': 0.792357936715865}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:39:45,565][0m Trial 40 finished with value: 0.25871444439131114 and parameters: {'observation_period_num': 123, 'train_rates': 0.8887605745913985, 'learning_rate': 0.0004649683088148921, 'batch_size': 232, 'step_size': 7, 'gamma': 0.9533979381788721}. Best is trial 22 with value: 0.1650585894649093.[0m
[32m[I 2025-02-07 21:42:04,444][0m Trial 41 finished with value: 0.14540720313060573 and parameters: {'observation_period_num': 15, 'train_rates': 0.9723427543788988, 'learning_rate': 0.00021117898484721565, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9346173841962877}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:44:07,410][0m Trial 42 finished with value: 0.14631802165830457 and parameters: {'observation_period_num': 5, 'train_rates': 0.9749892107655971, 'learning_rate': 0.0002084661953466724, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9819818322483258}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:46:10,298][0m Trial 43 finished with value: 0.19864100362691614 and parameters: {'observation_period_num': 7, 'train_rates': 0.9758544329308193, 'learning_rate': 0.0001979474999290285, 'batch_size': 41, 'step_size': 2, 'gamma': 0.8779711815795355}. Best is trial 41 with value: 0.14540720313060573.[0m
Early stopping at epoch 89
[32m[I 2025-02-07 21:47:13,224][0m Trial 44 finished with value: 0.6855896711349487 and parameters: {'observation_period_num': 6, 'train_rates': 0.9842358132405158, 'learning_rate': 0.0009920659978249213, 'batch_size': 74, 'step_size': 1, 'gamma': 0.8765281544498755}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:48:05,266][0m Trial 45 finished with value: 0.5213337540626526 and parameters: {'observation_period_num': 51, 'train_rates': 0.9752087656480466, 'learning_rate': 0.00021334259390594544, 'batch_size': 252, 'step_size': 2, 'gamma': 0.8353877344190057}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:49:39,218][0m Trial 46 finished with value: 0.4830162320400308 and parameters: {'observation_period_num': 25, 'train_rates': 0.9446204245780339, 'learning_rate': 0.0005823387921895241, 'batch_size': 53, 'step_size': 2, 'gamma': 0.9828212614512833}. Best is trial 41 with value: 0.14540720313060573.[0m
Early stopping at epoch 80
[32m[I 2025-02-07 21:50:46,827][0m Trial 47 finished with value: 0.3920469582080841 and parameters: {'observation_period_num': 42, 'train_rates': 0.9866185992470795, 'learning_rate': 0.0003423770178692616, 'batch_size': 62, 'step_size': 1, 'gamma': 0.8528708278356837}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:54:51,856][0m Trial 48 finished with value: 0.38717198913747614 and parameters: {'observation_period_num': 208, 'train_rates': 0.9482593206742818, 'learning_rate': 0.00018153101839466125, 'batch_size': 130, 'step_size': 4, 'gamma': 0.8057243437402674}. Best is trial 41 with value: 0.14540720313060573.[0m
[32m[I 2025-02-07 21:58:09,598][0m Trial 49 finished with value: 0.9302229965274984 and parameters: {'observation_period_num': 166, 'train_rates': 0.9212166531854661, 'learning_rate': 2.26751933537367e-06, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8773388037061048}. Best is trial 41 with value: 0.14540720313060573.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-07 21:58:09,606][0m A new study created in memory with name: no-name-c240ee82-8c00-4cec-bd33-03d6f88dbbf4[0m
[32m[I 2025-02-07 21:58:34,669][0m Trial 0 finished with value: 0.448947150430244 and parameters: {'observation_period_num': 15, 'train_rates': 0.836017599636561, 'learning_rate': 0.00017146454726984188, 'batch_size': 190, 'step_size': 12, 'gamma': 0.9036115180638953}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:01:10,234][0m Trial 1 finished with value: 0.7545876688427395 and parameters: {'observation_period_num': 142, 'train_rates': 0.9201825603344395, 'learning_rate': 2.3131363955496897e-05, 'batch_size': 97, 'step_size': 8, 'gamma': 0.7626116290186685}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:02:24,072][0m Trial 2 finished with value: 0.9411555831375801 and parameters: {'observation_period_num': 63, 'train_rates': 0.6499954732539168, 'learning_rate': 6.612796670031704e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.7693485890461389}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:03:20,414][0m Trial 3 finished with value: 1.5666761510292844 and parameters: {'observation_period_num': 62, 'train_rates': 0.8235341872036418, 'learning_rate': 1.1468516000183287e-06, 'batch_size': 166, 'step_size': 4, 'gamma': 0.9825405216415098}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:07:13,658][0m Trial 4 finished with value: 1.4581358636384452 and parameters: {'observation_period_num': 239, 'train_rates': 0.6292423563291685, 'learning_rate': 0.0007935711578084289, 'batch_size': 90, 'step_size': 5, 'gamma': 0.9166409784784814}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:09:27,922][0m Trial 5 finished with value: 1.4537503495741118 and parameters: {'observation_period_num': 151, 'train_rates': 0.6275417915959629, 'learning_rate': 7.878708701387677e-06, 'batch_size': 112, 'step_size': 12, 'gamma': 0.8922781259104963}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:11:32,326][0m Trial 6 finished with value: 0.8250762366566313 and parameters: {'observation_period_num': 63, 'train_rates': 0.7131368042611125, 'learning_rate': 0.00022112827100158455, 'batch_size': 31, 'step_size': 10, 'gamma': 0.7583661200888778}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:12:37,904][0m Trial 7 finished with value: 1.0750993696962976 and parameters: {'observation_period_num': 72, 'train_rates': 0.735127891422676, 'learning_rate': 1.700259519598581e-05, 'batch_size': 114, 'step_size': 12, 'gamma': 0.7746475146474204}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:13:39,380][0m Trial 8 finished with value: 0.7511184470307443 and parameters: {'observation_period_num': 27, 'train_rates': 0.9152925127866702, 'learning_rate': 7.6016398150487115e-06, 'batch_size': 79, 'step_size': 11, 'gamma': 0.850008485482035}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:16:35,162][0m Trial 9 finished with value: 1.574995994567871 and parameters: {'observation_period_num': 156, 'train_rates': 0.9714653296647069, 'learning_rate': 1.1149701738102821e-06, 'batch_size': 137, 'step_size': 12, 'gamma': 0.9159939853383628}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:16:54,888][0m Trial 10 finished with value: 0.47566810285902594 and parameters: {'observation_period_num': 7, 'train_rates': 0.829994953396276, 'learning_rate': 0.00011047704664277906, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8359862177924591}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:17:14,461][0m Trial 11 finished with value: 0.4958205876811858 and parameters: {'observation_period_num': 9, 'train_rates': 0.8317466494287408, 'learning_rate': 0.00014072563403411215, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8494239071040098}. Best is trial 0 with value: 0.448947150430244.[0m
[32m[I 2025-02-07 22:17:37,645][0m Trial 12 finished with value: 0.34579803381520924 and parameters: {'observation_period_num': 9, 'train_rates': 0.8629741578136597, 'learning_rate': 0.0005532441779303582, 'batch_size': 227, 'step_size': 15, 'gamma': 0.815261215396735}. Best is trial 12 with value: 0.34579803381520924.[0m
[32m[I 2025-02-07 22:19:14,477][0m Trial 13 finished with value: 0.43854376426869374 and parameters: {'observation_period_num': 100, 'train_rates': 0.8858101931698938, 'learning_rate': 0.0009879408701276935, 'batch_size': 203, 'step_size': 15, 'gamma': 0.811613388060378}. Best is trial 12 with value: 0.34579803381520924.[0m
[32m[I 2025-02-07 22:20:56,595][0m Trial 14 finished with value: 0.5955212213016524 and parameters: {'observation_period_num': 105, 'train_rates': 0.9052227625966827, 'learning_rate': 0.0008381755399382306, 'batch_size': 209, 'step_size': 2, 'gamma': 0.8042263563499383}. Best is trial 12 with value: 0.34579803381520924.[0m
[32m[I 2025-02-07 22:24:14,376][0m Trial 15 finished with value: 0.6945396515506285 and parameters: {'observation_period_num': 194, 'train_rates': 0.7655416551763529, 'learning_rate': 0.00046960143923828674, 'batch_size': 217, 'step_size': 15, 'gamma': 0.8203229109947406}. Best is trial 12 with value: 0.34579803381520924.[0m
[32m[I 2025-02-07 22:25:55,424][0m Trial 16 finished with value: 0.3349091480057742 and parameters: {'observation_period_num': 103, 'train_rates': 0.8818511714775898, 'learning_rate': 0.0004096987651847455, 'batch_size': 167, 'step_size': 9, 'gamma': 0.7982012167037247}. Best is trial 16 with value: 0.3349091480057742.[0m
[32m[I 2025-02-07 22:29:32,174][0m Trial 17 finished with value: 0.21843838691711426 and parameters: {'observation_period_num': 186, 'train_rates': 0.9822647171833558, 'learning_rate': 0.00033351906043613673, 'batch_size': 160, 'step_size': 8, 'gamma': 0.7928104260330442}. Best is trial 17 with value: 0.21843838691711426.[0m
[32m[I 2025-02-07 22:33:29,266][0m Trial 18 finished with value: 0.20280686020851135 and parameters: {'observation_period_num': 196, 'train_rates': 0.9758353278676509, 'learning_rate': 0.0003166290349744031, 'batch_size': 160, 'step_size': 8, 'gamma': 0.7899694352350037}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:37:26,216][0m Trial 19 finished with value: 0.3898772597312927 and parameters: {'observation_period_num': 193, 'train_rates': 0.9877999879943697, 'learning_rate': 4.6134735424141814e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.8696830517537899}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:42:41,243][0m Trial 20 finished with value: 0.5732662677764893 and parameters: {'observation_period_num': 250, 'train_rates': 0.9531637850571204, 'learning_rate': 0.0002679268179194448, 'batch_size': 137, 'step_size': 3, 'gamma': 0.7890215976288439}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:46:22,064][0m Trial 21 finished with value: 0.36076244711875916 and parameters: {'observation_period_num': 189, 'train_rates': 0.9401869741022396, 'learning_rate': 0.0002845726242021825, 'batch_size': 173, 'step_size': 9, 'gamma': 0.7892665896796877}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:50:49,139][0m Trial 22 finished with value: 0.23766681551933289 and parameters: {'observation_period_num': 213, 'train_rates': 0.9896673482934158, 'learning_rate': 0.00039565107021523273, 'batch_size': 151, 'step_size': 8, 'gamma': 0.7505250774476298}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:55:26,560][0m Trial 23 finished with value: 0.41127049922943115 and parameters: {'observation_period_num': 221, 'train_rates': 0.9862895005173625, 'learning_rate': 8.064162016636436e-05, 'batch_size': 145, 'step_size': 7, 'gamma': 0.751434508354521}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 22:58:41,346][0m Trial 24 finished with value: 0.39003822207450867 and parameters: {'observation_period_num': 174, 'train_rates': 0.9479499187815893, 'learning_rate': 0.0003585428253612659, 'batch_size': 187, 'step_size': 6, 'gamma': 0.7749702629742474}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:03:08,480][0m Trial 25 finished with value: 0.6696275056911116 and parameters: {'observation_period_num': 218, 'train_rates': 0.9566334189269631, 'learning_rate': 3.986246153644689e-05, 'batch_size': 117, 'step_size': 8, 'gamma': 0.8304796328594327}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:07:34,256][0m Trial 26 finished with value: 0.22514700889587402 and parameters: {'observation_period_num': 214, 'train_rates': 0.9898361490380078, 'learning_rate': 0.0001104898532790025, 'batch_size': 149, 'step_size': 10, 'gamma': 0.9591892642366433}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:10:42,235][0m Trial 27 finished with value: 0.3760413204368792 and parameters: {'observation_period_num': 169, 'train_rates': 0.9318306762707748, 'learning_rate': 9.392128774385432e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.9898006080972103}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:12:46,491][0m Trial 28 finished with value: 0.31971143285103415 and parameters: {'observation_period_num': 128, 'train_rates': 0.8935821525980678, 'learning_rate': 0.00020303426095830227, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9598359636011636}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:16:58,385][0m Trial 29 finished with value: 0.6713440697796181 and parameters: {'observation_period_num': 234, 'train_rates': 0.7806057806579529, 'learning_rate': 0.00015404700345787451, 'batch_size': 182, 'step_size': 13, 'gamma': 0.9637477245789334}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:20:43,890][0m Trial 30 finished with value: 0.582252793727403 and parameters: {'observation_period_num': 202, 'train_rates': 0.8635873801851057, 'learning_rate': 0.00015743442665640367, 'batch_size': 153, 'step_size': 1, 'gamma': 0.9396993436439182}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:25:06,667][0m Trial 31 finished with value: 0.2577624022960663 and parameters: {'observation_period_num': 215, 'train_rates': 0.970215945380948, 'learning_rate': 0.0005892951254582293, 'batch_size': 152, 'step_size': 7, 'gamma': 0.8832195041988461}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:28:35,957][0m Trial 32 finished with value: 0.3595463037490845 and parameters: {'observation_period_num': 175, 'train_rates': 0.9803426219836446, 'learning_rate': 6.638396638407517e-05, 'batch_size': 133, 'step_size': 8, 'gamma': 0.7856161545832646}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:32:40,949][0m Trial 33 finished with value: 0.3186431359953997 and parameters: {'observation_period_num': 206, 'train_rates': 0.9254366923046727, 'learning_rate': 0.00027482894277256197, 'batch_size': 99, 'step_size': 10, 'gamma': 0.7510632387188151}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:37:38,120][0m Trial 34 finished with value: 0.4229253828525543 and parameters: {'observation_period_num': 233, 'train_rates': 0.9560096130026193, 'learning_rate': 0.0004000724879496699, 'batch_size': 175, 'step_size': 6, 'gamma': 0.7739770040268961}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:41:18,048][0m Trial 35 finished with value: 0.8358244299888611 and parameters: {'observation_period_num': 183, 'train_rates': 0.9899905554318809, 'learning_rate': 2.2077995806584407e-05, 'batch_size': 75, 'step_size': 5, 'gamma': 0.7656740987307796}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:45:25,760][0m Trial 36 finished with value: 0.9934798972928418 and parameters: {'observation_period_num': 252, 'train_rates': 0.6655783181314245, 'learning_rate': 0.0006467661680669027, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9261482847384269}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:47:50,624][0m Trial 37 finished with value: 0.38503591536851456 and parameters: {'observation_period_num': 129, 'train_rates': 0.932428928856145, 'learning_rate': 0.00010396199279611487, 'batch_size': 149, 'step_size': 8, 'gamma': 0.8656481613631992}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:50:28,249][0m Trial 38 finished with value: 0.5321228960891823 and parameters: {'observation_period_num': 158, 'train_rates': 0.8074543700234795, 'learning_rate': 5.540958915486536e-05, 'batch_size': 161, 'step_size': 9, 'gamma': 0.8931249513756224}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:54:40,390][0m Trial 39 finished with value: 1.4087458848953247 and parameters: {'observation_period_num': 206, 'train_rates': 0.9696053617791813, 'learning_rate': 4.168689041935484e-06, 'batch_size': 106, 'step_size': 5, 'gamma': 0.7985103179685678}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-07 23:57:59,917][0m Trial 40 finished with value: 0.9751895478447609 and parameters: {'observation_period_num': 221, 'train_rates': 0.6038877861668772, 'learning_rate': 0.0002363225212248266, 'batch_size': 121, 'step_size': 7, 'gamma': 0.7632670230853488}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:02:22,978][0m Trial 41 finished with value: 0.39885178208351135 and parameters: {'observation_period_num': 211, 'train_rates': 0.9657580048997575, 'learning_rate': 0.000630322214123092, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8872681368740658}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:06:55,430][0m Trial 42 finished with value: 0.3366888277232647 and parameters: {'observation_period_num': 230, 'train_rates': 0.9120958085190981, 'learning_rate': 0.0003708800422171704, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9465890811631965}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:12:06,096][0m Trial 43 finished with value: 0.3820611536502838 and parameters: {'observation_period_num': 244, 'train_rates': 0.9681013263882305, 'learning_rate': 0.0006035383108851039, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8474940559908196}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:16:37,117][0m Trial 44 finished with value: 0.32364925742149353 and parameters: {'observation_period_num': 223, 'train_rates': 0.947410586723522, 'learning_rate': 0.00017696192892906692, 'batch_size': 172, 'step_size': 8, 'gamma': 0.9044282160573579}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:20:09,588][0m Trial 45 finished with value: 0.28660476207733154 and parameters: {'observation_period_num': 183, 'train_rates': 0.9681357372166907, 'learning_rate': 0.00013311440442612192, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9726803856793478}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:22:52,911][0m Trial 46 finished with value: 0.8321700360132679 and parameters: {'observation_period_num': 143, 'train_rates': 0.9222349593714306, 'learning_rate': 0.0009094554008596907, 'batch_size': 89, 'step_size': 6, 'gamma': 0.8829902810088671}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:26:27,180][0m Trial 47 finished with value: 0.346227831606354 and parameters: {'observation_period_num': 196, 'train_rates': 0.8582129394764335, 'learning_rate': 0.0003209698133382328, 'batch_size': 194, 'step_size': 9, 'gamma': 0.8610716400366298}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:29:58,094][0m Trial 48 finished with value: 0.4012669424215953 and parameters: {'observation_period_num': 161, 'train_rates': 0.9883826558695574, 'learning_rate': 0.0005267598595658185, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8368761212534117}. Best is trial 18 with value: 0.20280686020851135.[0m
[32m[I 2025-02-08 00:33:19,307][0m Trial 49 finished with value: 2.1212812389334497 and parameters: {'observation_period_num': 212, 'train_rates': 0.6813938619905122, 'learning_rate': 1.5748321977347712e-06, 'batch_size': 177, 'step_size': 4, 'gamma': 0.8234311384692788}. Best is trial 18 with value: 0.20280686020851135.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 133, 'train_rates': 0.9631839432653752, 'learning_rate': 4.180505174341226e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9564118288220385}
Epoch 1/300, trend Loss: 0.6466 | 1.1572
Epoch 2/300, trend Loss: 0.4775 | 1.0085
Epoch 3/300, trend Loss: 0.4041 | 0.9064
Epoch 4/300, trend Loss: 0.3535 | 0.8050
Epoch 5/300, trend Loss: 0.3056 | 0.7316
Epoch 6/300, trend Loss: 0.2876 | 0.6818
Epoch 7/300, trend Loss: 0.2637 | 0.6441
Epoch 8/300, trend Loss: 0.2632 | 0.6209
Epoch 9/300, trend Loss: 0.2452 | 0.5728
Epoch 10/300, trend Loss: 0.2341 | 0.5348
Epoch 11/300, trend Loss: 0.2268 | 0.5288
Epoch 12/300, trend Loss: 0.2241 | 0.5197
Epoch 13/300, trend Loss: 0.2252 | 0.4939
Epoch 14/300, trend Loss: 0.2107 | 0.4685
Epoch 15/300, trend Loss: 0.2111 | 0.4479
Epoch 16/300, trend Loss: 0.2050 | 0.4459
Epoch 17/300, trend Loss: 0.2045 | 0.4315
Epoch 18/300, trend Loss: 0.2013 | 0.4480
Epoch 19/300, trend Loss: 0.1949 | 0.3966
Epoch 20/300, trend Loss: 0.1941 | 0.3979
Epoch 21/300, trend Loss: 0.1901 | 0.4015
Epoch 22/300, trend Loss: 0.1890 | 0.3805
Epoch 23/300, trend Loss: 0.1872 | 0.4024
Epoch 24/300, trend Loss: 0.1807 | 0.3531
Epoch 25/300, trend Loss: 0.1809 | 0.3560
Epoch 26/300, trend Loss: 0.1761 | 0.3450
Epoch 27/300, trend Loss: 0.1726 | 0.3528
Epoch 28/300, trend Loss: 0.1702 | 0.3477
Epoch 29/300, trend Loss: 0.1678 | 0.3260
Epoch 30/300, trend Loss: 0.1664 | 0.3164
Epoch 31/300, trend Loss: 0.1638 | 0.3173
Epoch 32/300, trend Loss: 0.1608 | 0.3219
Epoch 33/300, trend Loss: 0.1609 | 0.3163
Epoch 34/300, trend Loss: 0.1580 | 0.3042
Epoch 35/300, trend Loss: 0.1579 | 0.2950
Epoch 36/300, trend Loss: 0.1557 | 0.2961
Epoch 37/300, trend Loss: 0.1555 | 0.2942
Epoch 38/300, trend Loss: 0.1550 | 0.3007
Epoch 39/300, trend Loss: 0.1525 | 0.2893
Epoch 40/300, trend Loss: 0.1503 | 0.2771
Epoch 41/300, trend Loss: 0.1501 | 0.2739
Epoch 42/300, trend Loss: 0.1478 | 0.2769
Epoch 43/300, trend Loss: 0.1491 | 0.2876
Epoch 44/300, trend Loss: 0.1465 | 0.2758
Epoch 45/300, trend Loss: 0.1456 | 0.2657
Epoch 46/300, trend Loss: 0.1447 | 0.2691
Epoch 47/300, trend Loss: 0.1449 | 0.2646
Epoch 48/300, trend Loss: 0.1448 | 0.2746
Epoch 49/300, trend Loss: 0.1438 | 0.2630
Epoch 50/300, trend Loss: 0.1421 | 0.2579
Epoch 51/300, trend Loss: 0.1419 | 0.2559
Epoch 52/300, trend Loss: 0.1405 | 0.2498
Epoch 53/300, trend Loss: 0.1407 | 0.2720
Epoch 54/300, trend Loss: 0.1389 | 0.2545
Epoch 55/300, trend Loss: 0.1374 | 0.2480
Epoch 56/300, trend Loss: 0.1379 | 0.2422
Epoch 57/300, trend Loss: 0.1360 | 0.2420
Epoch 58/300, trend Loss: 0.1355 | 0.2503
Epoch 59/300, trend Loss: 0.1340 | 0.2459
Epoch 60/300, trend Loss: 0.1334 | 0.2394
Epoch 61/300, trend Loss: 0.1327 | 0.2347
Epoch 62/300, trend Loss: 0.1318 | 0.2347
Epoch 63/300, trend Loss: 0.1311 | 0.2383
Epoch 64/300, trend Loss: 0.1306 | 0.2370
Epoch 65/300, trend Loss: 0.1306 | 0.2301
Epoch 66/300, trend Loss: 0.1293 | 0.2278
Epoch 67/300, trend Loss: 0.1297 | 0.2292
Epoch 68/300, trend Loss: 0.1279 | 0.2286
Epoch 69/300, trend Loss: 0.1282 | 0.2253
Epoch 70/300, trend Loss: 0.1277 | 0.2251
Epoch 71/300, trend Loss: 0.1272 | 0.2238
Epoch 72/300, trend Loss: 0.1270 | 0.2235
Epoch 73/300, trend Loss: 0.1263 | 0.2244
Epoch 74/300, trend Loss: 0.1269 | 0.2179
Epoch 75/300, trend Loss: 0.1249 | 0.2180
Epoch 76/300, trend Loss: 0.1257 | 0.2204
Epoch 77/300, trend Loss: 0.1250 | 0.2186
Epoch 78/300, trend Loss: 0.1243 | 0.2167
Epoch 79/300, trend Loss: 0.1241 | 0.2143
Epoch 80/300, trend Loss: 0.1239 | 0.2146
Epoch 81/300, trend Loss: 0.1231 | 0.2136
Epoch 82/300, trend Loss: 0.1224 | 0.2118
Epoch 83/300, trend Loss: 0.1223 | 0.2127
Epoch 84/300, trend Loss: 0.1221 | 0.2130
Epoch 85/300, trend Loss: 0.1221 | 0.2123
Epoch 86/300, trend Loss: 0.1207 | 0.2113
Epoch 87/300, trend Loss: 0.1213 | 0.2083
Epoch 88/300, trend Loss: 0.1212 | 0.2093
Epoch 89/300, trend Loss: 0.1206 | 0.2093
Epoch 90/300, trend Loss: 0.1205 | 0.2070
Epoch 91/300, trend Loss: 0.1200 | 0.2064
Epoch 92/300, trend Loss: 0.1192 | 0.2066
Epoch 93/300, trend Loss: 0.1196 | 0.2057
Epoch 94/300, trend Loss: 0.1199 | 0.2045
Epoch 95/300, trend Loss: 0.1185 | 0.2032
Epoch 96/300, trend Loss: 0.1186 | 0.2041
Epoch 97/300, trend Loss: 0.1182 | 0.2030
Epoch 98/300, trend Loss: 0.1186 | 0.2028
Epoch 99/300, trend Loss: 0.1182 | 0.2018
Epoch 100/300, trend Loss: 0.1182 | 0.2017
Epoch 101/300, trend Loss: 0.1179 | 0.2012
Epoch 102/300, trend Loss: 0.1177 | 0.2020
Epoch 103/300, trend Loss: 0.1172 | 0.1999
Epoch 104/300, trend Loss: 0.1166 | 0.2000
Epoch 105/300, trend Loss: 0.1166 | 0.2006
Epoch 106/300, trend Loss: 0.1167 | 0.2002
Epoch 107/300, trend Loss: 0.1172 | 0.1992
Epoch 108/300, trend Loss: 0.1162 | 0.1999
Epoch 109/300, trend Loss: 0.1165 | 0.1994
Epoch 110/300, trend Loss: 0.1166 | 0.1988
Epoch 111/300, trend Loss: 0.1162 | 0.1985
Epoch 112/300, trend Loss: 0.1157 | 0.1974
Epoch 113/300, trend Loss: 0.1160 | 0.1981
Epoch 114/300, trend Loss: 0.1159 | 0.1977
Epoch 115/300, trend Loss: 0.1155 | 0.1977
Epoch 116/300, trend Loss: 0.1145 | 0.1972
Epoch 117/300, trend Loss: 0.1155 | 0.1976
Epoch 118/300, trend Loss: 0.1152 | 0.1954
Epoch 119/300, trend Loss: 0.1154 | 0.1958
Epoch 120/300, trend Loss: 0.1143 | 0.1958
Epoch 121/300, trend Loss: 0.1152 | 0.1946
Epoch 122/300, trend Loss: 0.1148 | 0.1946
Epoch 123/300, trend Loss: 0.1143 | 0.1940
Epoch 124/300, trend Loss: 0.1142 | 0.1943
Epoch 125/300, trend Loss: 0.1134 | 0.1930
Epoch 126/300, trend Loss: 0.1139 | 0.1938
Epoch 127/300, trend Loss: 0.1145 | 0.1936
Epoch 128/300, trend Loss: 0.1137 | 0.1942
Epoch 129/300, trend Loss: 0.1142 | 0.1942
Epoch 130/300, trend Loss: 0.1135 | 0.1930
Epoch 131/300, trend Loss: 0.1146 | 0.1929
Epoch 132/300, trend Loss: 0.1135 | 0.1932
Epoch 133/300, trend Loss: 0.1139 | 0.1932
Epoch 134/300, trend Loss: 0.1128 | 0.1923
Epoch 135/300, trend Loss: 0.1128 | 0.1915
Epoch 136/300, trend Loss: 0.1140 | 0.1925
Epoch 137/300, trend Loss: 0.1125 | 0.1920
Epoch 138/300, trend Loss: 0.1130 | 0.1926
Epoch 139/300, trend Loss: 0.1136 | 0.1922
Epoch 140/300, trend Loss: 0.1129 | 0.1916
Epoch 141/300, trend Loss: 0.1125 | 0.1924
Epoch 142/300, trend Loss: 0.1131 | 0.1918
Epoch 143/300, trend Loss: 0.1126 | 0.1915
Epoch 144/300, trend Loss: 0.1125 | 0.1912
Epoch 145/300, trend Loss: 0.1119 | 0.1905
Epoch 146/300, trend Loss: 0.1122 | 0.1902
Epoch 147/300, trend Loss: 0.1128 | 0.1909
Epoch 148/300, trend Loss: 0.1123 | 0.1905
Epoch 149/300, trend Loss: 0.1118 | 0.1907
Epoch 150/300, trend Loss: 0.1128 | 0.1903
Epoch 151/300, trend Loss: 0.1125 | 0.1905
Epoch 152/300, trend Loss: 0.1116 | 0.1896
Epoch 153/300, trend Loss: 0.1120 | 0.1897
Epoch 154/300, trend Loss: 0.1116 | 0.1900
Epoch 155/300, trend Loss: 0.1120 | 0.1905
Epoch 156/300, trend Loss: 0.1122 | 0.1908
Epoch 157/300, trend Loss: 0.1121 | 0.1900
Epoch 158/300, trend Loss: 0.1119 | 0.1904
Epoch 159/300, trend Loss: 0.1124 | 0.1905
Epoch 160/300, trend Loss: 0.1116 | 0.1899
Epoch 161/300, trend Loss: 0.1114 | 0.1895
Epoch 162/300, trend Loss: 0.1114 | 0.1891
Epoch 163/300, trend Loss: 0.1121 | 0.1894
Epoch 164/300, trend Loss: 0.1114 | 0.1895
Epoch 165/300, trend Loss: 0.1104 | 0.1888
Epoch 166/300, trend Loss: 0.1115 | 0.1892
Epoch 167/300, trend Loss: 0.1116 | 0.1899
Epoch 168/300, trend Loss: 0.1111 | 0.1893
Epoch 169/300, trend Loss: 0.1113 | 0.1892
Epoch 170/300, trend Loss: 0.1109 | 0.1891
Epoch 171/300, trend Loss: 0.1117 | 0.1894
Epoch 172/300, trend Loss: 0.1114 | 0.1888
Epoch 173/300, trend Loss: 0.1103 | 0.1887
Epoch 174/300, trend Loss: 0.1102 | 0.1884
Epoch 175/300, trend Loss: 0.1111 | 0.1888
Epoch 176/300, trend Loss: 0.1113 | 0.1888
Epoch 177/300, trend Loss: 0.1104 | 0.1888
Epoch 178/300, trend Loss: 0.1108 | 0.1887
Epoch 179/300, trend Loss: 0.1114 | 0.1884
Epoch 180/300, trend Loss: 0.1112 | 0.1887
Epoch 181/300, trend Loss: 0.1112 | 0.1882
Epoch 182/300, trend Loss: 0.1112 | 0.1885
Epoch 183/300, trend Loss: 0.1106 | 0.1884
Epoch 184/300, trend Loss: 0.1110 | 0.1884
Epoch 185/300, trend Loss: 0.1107 | 0.1882
Epoch 186/300, trend Loss: 0.1117 | 0.1881
Epoch 187/300, trend Loss: 0.1115 | 0.1882
Epoch 188/300, trend Loss: 0.1113 | 0.1880
Epoch 189/300, trend Loss: 0.1101 | 0.1881
Epoch 190/300, trend Loss: 0.1098 | 0.1878
Epoch 191/300, trend Loss: 0.1110 | 0.1878
Epoch 192/300, trend Loss: 0.1099 | 0.1876
Epoch 193/300, trend Loss: 0.1105 | 0.1877
Epoch 194/300, trend Loss: 0.1113 | 0.1881
Epoch 195/300, trend Loss: 0.1104 | 0.1882
Epoch 196/300, trend Loss: 0.1108 | 0.1877
Epoch 197/300, trend Loss: 0.1106 | 0.1879
Epoch 198/300, trend Loss: 0.1100 | 0.1873
Epoch 199/300, trend Loss: 0.1110 | 0.1874
Epoch 200/300, trend Loss: 0.1100 | 0.1875
Epoch 201/300, trend Loss: 0.1111 | 0.1877
Epoch 202/300, trend Loss: 0.1103 | 0.1881
Epoch 203/300, trend Loss: 0.1101 | 0.1880
Epoch 204/300, trend Loss: 0.1101 | 0.1876
Epoch 205/300, trend Loss: 0.1101 | 0.1876
Epoch 206/300, trend Loss: 0.1099 | 0.1876
Epoch 207/300, trend Loss: 0.1109 | 0.1879
Epoch 208/300, trend Loss: 0.1102 | 0.1876
Epoch 209/300, trend Loss: 0.1101 | 0.1875
Epoch 210/300, trend Loss: 0.1103 | 0.1877
Epoch 211/300, trend Loss: 0.1106 | 0.1875
Epoch 212/300, trend Loss: 0.1102 | 0.1875
Epoch 213/300, trend Loss: 0.1103 | 0.1874
Epoch 214/300, trend Loss: 0.1103 | 0.1874
Epoch 215/300, trend Loss: 0.1102 | 0.1870
Epoch 216/300, trend Loss: 0.1094 | 0.1870
Epoch 217/300, trend Loss: 0.1096 | 0.1871
Epoch 218/300, trend Loss: 0.1103 | 0.1872
Epoch 219/300, trend Loss: 0.1104 | 0.1873
Epoch 220/300, trend Loss: 0.1096 | 0.1868
Epoch 221/300, trend Loss: 0.1102 | 0.1868
Epoch 222/300, trend Loss: 0.1105 | 0.1867
Epoch 223/300, trend Loss: 0.1099 | 0.1865
Epoch 224/300, trend Loss: 0.1097 | 0.1866
Epoch 225/300, trend Loss: 0.1102 | 0.1867
Epoch 226/300, trend Loss: 0.1099 | 0.1866
Epoch 227/300, trend Loss: 0.1094 | 0.1867
Epoch 228/300, trend Loss: 0.1099 | 0.1866
Epoch 229/300, trend Loss: 0.1095 | 0.1864
Epoch 230/300, trend Loss: 0.1097 | 0.1865
Epoch 231/300, trend Loss: 0.1099 | 0.1865
Epoch 232/300, trend Loss: 0.1099 | 0.1863
Epoch 233/300, trend Loss: 0.1107 | 0.1866
Epoch 234/300, trend Loss: 0.1096 | 0.1865
Epoch 235/300, trend Loss: 0.1101 | 0.1868
Epoch 236/300, trend Loss: 0.1105 | 0.1869
Epoch 237/300, trend Loss: 0.1104 | 0.1868
Epoch 238/300, trend Loss: 0.1098 | 0.1869
Epoch 239/300, trend Loss: 0.1099 | 0.1868
Epoch 240/300, trend Loss: 0.1101 | 0.1870
Epoch 241/300, trend Loss: 0.1106 | 0.1872
Epoch 242/300, trend Loss: 0.1096 | 0.1873
Epoch 243/300, trend Loss: 0.1103 | 0.1874
Epoch 244/300, trend Loss: 0.1103 | 0.1872
Epoch 245/300, trend Loss: 0.1100 | 0.1872
Epoch 246/300, trend Loss: 0.1095 | 0.1872
Epoch 247/300, trend Loss: 0.1101 | 0.1869
Epoch 248/300, trend Loss: 0.1102 | 0.1868
Epoch 249/300, trend Loss: 0.1095 | 0.1868
Epoch 250/300, trend Loss: 0.1102 | 0.1868
Epoch 251/300, trend Loss: 0.1093 | 0.1868
Epoch 252/300, trend Loss: 0.1094 | 0.1866
Epoch 253/300, trend Loss: 0.1095 | 0.1867
Epoch 254/300, trend Loss: 0.1101 | 0.1867
Epoch 255/300, trend Loss: 0.1093 | 0.1866
Epoch 256/300, trend Loss: 0.1096 | 0.1866
Epoch 257/300, trend Loss: 0.1098 | 0.1865
Epoch 258/300, trend Loss: 0.1099 | 0.1864
Epoch 259/300, trend Loss: 0.1098 | 0.1863
Epoch 260/300, trend Loss: 0.1102 | 0.1863
Epoch 261/300, trend Loss: 0.1097 | 0.1862
Epoch 262/300, trend Loss: 0.1091 | 0.1862
Epoch 263/300, trend Loss: 0.1101 | 0.1862
Epoch 264/300, trend Loss: 0.1101 | 0.1862
Epoch 265/300, trend Loss: 0.1101 | 0.1861
Epoch 266/300, trend Loss: 0.1091 | 0.1861
Epoch 267/300, trend Loss: 0.1103 | 0.1862
Epoch 268/300, trend Loss: 0.1102 | 0.1863
Epoch 269/300, trend Loss: 0.1095 | 0.1862
Epoch 270/300, trend Loss: 0.1105 | 0.1862
Epoch 271/300, trend Loss: 0.1099 | 0.1864
Epoch 272/300, trend Loss: 0.1100 | 0.1863
Epoch 273/300, trend Loss: 0.1093 | 0.1862
Epoch 274/300, trend Loss: 0.1097 | 0.1862
Epoch 275/300, trend Loss: 0.1094 | 0.1863
Epoch 276/300, trend Loss: 0.1096 | 0.1863
Epoch 277/300, trend Loss: 0.1104 | 0.1864
Epoch 278/300, trend Loss: 0.1099 | 0.1864
Epoch 279/300, trend Loss: 0.1098 | 0.1864
Epoch 280/300, trend Loss: 0.1101 | 0.1863
Epoch 281/300, trend Loss: 0.1095 | 0.1863
Epoch 282/300, trend Loss: 0.1094 | 0.1864
Epoch 283/300, trend Loss: 0.1094 | 0.1863
Epoch 284/300, trend Loss: 0.1095 | 0.1864
Epoch 285/300, trend Loss: 0.1096 | 0.1864
Epoch 286/300, trend Loss: 0.1102 | 0.1864
Epoch 287/300, trend Loss: 0.1096 | 0.1865
Epoch 288/300, trend Loss: 0.1095 | 0.1865
Epoch 289/300, trend Loss: 0.1097 | 0.1865
Epoch 290/300, trend Loss: 0.1093 | 0.1864
Epoch 291/300, trend Loss: 0.1106 | 0.1864
Epoch 292/300, trend Loss: 0.1099 | 0.1864
Epoch 293/300, trend Loss: 0.1097 | 0.1864
Epoch 294/300, trend Loss: 0.1095 | 0.1865
Epoch 295/300, trend Loss: 0.1102 | 0.1865
Epoch 296/300, trend Loss: 0.1098 | 0.1864
Epoch 297/300, trend Loss: 0.1100 | 0.1864
Epoch 298/300, trend Loss: 0.1089 | 0.1864
Epoch 299/300, trend Loss: 0.1096 | 0.1863
Epoch 300/300, trend Loss: 0.1101 | 0.1864
Training seasonal_0 component with params: {'observation_period_num': 174, 'train_rates': 0.984514580657239, 'learning_rate': 0.00010828904694852491, 'batch_size': 65, 'step_size': 10, 'gamma': 0.946240550310895}
Epoch 1/300, seasonal_0 Loss: 0.9747 | 1.4754
Epoch 2/300, seasonal_0 Loss: 0.7050 | 0.9443
Epoch 3/300, seasonal_0 Loss: 0.5863 | 0.8124
Epoch 4/300, seasonal_0 Loss: 0.5021 | 0.7643
Epoch 5/300, seasonal_0 Loss: 0.4332 | 0.6724
Epoch 6/300, seasonal_0 Loss: 0.4172 | 0.6145
Epoch 7/300, seasonal_0 Loss: 0.3531 | 0.6013
Epoch 8/300, seasonal_0 Loss: 0.3359 | 0.5228
Epoch 9/300, seasonal_0 Loss: 0.3535 | 0.5665
Epoch 10/300, seasonal_0 Loss: 0.3912 | 0.4997
Epoch 11/300, seasonal_0 Loss: 0.3562 | 0.4821
Epoch 12/300, seasonal_0 Loss: 0.3667 | 0.4988
Epoch 13/300, seasonal_0 Loss: 0.3722 | 0.4591
Epoch 14/300, seasonal_0 Loss: 0.3020 | 0.4557
Epoch 15/300, seasonal_0 Loss: 0.2750 | 0.4120
Epoch 16/300, seasonal_0 Loss: 0.2784 | 0.3985
Epoch 17/300, seasonal_0 Loss: 0.2502 | 0.3826
Epoch 18/300, seasonal_0 Loss: 0.2433 | 0.3682
Epoch 19/300, seasonal_0 Loss: 0.2392 | 0.3525
Epoch 20/300, seasonal_0 Loss: 0.2295 | 0.3493
Epoch 21/300, seasonal_0 Loss: 0.2353 | 0.3327
Epoch 22/300, seasonal_0 Loss: 0.2254 | 0.3258
Epoch 23/300, seasonal_0 Loss: 0.2247 | 0.3268
Epoch 24/300, seasonal_0 Loss: 0.2178 | 0.3134
Epoch 25/300, seasonal_0 Loss: 0.2150 | 0.3065
Epoch 26/300, seasonal_0 Loss: 0.2210 | 0.3032
Epoch 27/300, seasonal_0 Loss: 0.2119 | 0.2827
Epoch 28/300, seasonal_0 Loss: 0.2105 | 0.2938
Epoch 29/300, seasonal_0 Loss: 0.2169 | 0.2879
Epoch 30/300, seasonal_0 Loss: 0.2151 | 0.2795
Epoch 31/300, seasonal_0 Loss: 0.2094 | 0.2829
Epoch 32/300, seasonal_0 Loss: 0.2203 | 0.2755
Epoch 33/300, seasonal_0 Loss: 0.2053 | 0.2587
Epoch 34/300, seasonal_0 Loss: 0.2038 | 0.2778
Epoch 35/300, seasonal_0 Loss: 0.1987 | 0.2534
Epoch 36/300, seasonal_0 Loss: 0.2061 | 0.2563
Epoch 37/300, seasonal_0 Loss: 0.2253 | 0.3002
Epoch 38/300, seasonal_0 Loss: 0.2123 | 0.2511
Epoch 39/300, seasonal_0 Loss: 0.2221 | 0.2539
Epoch 40/300, seasonal_0 Loss: 0.2700 | 0.2646
Epoch 41/300, seasonal_0 Loss: 0.2251 | 0.2581
Epoch 42/300, seasonal_0 Loss: 0.1983 | 0.2506
Epoch 43/300, seasonal_0 Loss: 0.2006 | 0.2483
Epoch 44/300, seasonal_0 Loss: 0.1844 | 0.2389
Epoch 45/300, seasonal_0 Loss: 0.1864 | 0.2406
Epoch 46/300, seasonal_0 Loss: 0.1753 | 0.2324
Epoch 47/300, seasonal_0 Loss: 0.1774 | 0.2417
Epoch 48/300, seasonal_0 Loss: 0.1755 | 0.2294
Epoch 49/300, seasonal_0 Loss: 0.1751 | 0.2250
Epoch 50/300, seasonal_0 Loss: 0.1719 | 0.2295
Epoch 51/300, seasonal_0 Loss: 0.1719 | 0.2181
Epoch 52/300, seasonal_0 Loss: 0.1657 | 0.2192
Epoch 53/300, seasonal_0 Loss: 0.1678 | 0.2267
Epoch 54/300, seasonal_0 Loss: 0.1650 | 0.2106
Epoch 55/300, seasonal_0 Loss: 0.1637 | 0.2124
Epoch 56/300, seasonal_0 Loss: 0.1634 | 0.2234
Epoch 57/300, seasonal_0 Loss: 0.1625 | 0.2019
Epoch 58/300, seasonal_0 Loss: 0.1625 | 0.2105
Epoch 59/300, seasonal_0 Loss: 0.1661 | 0.2204
Epoch 60/300, seasonal_0 Loss: 0.1656 | 0.1993
Epoch 61/300, seasonal_0 Loss: 0.1609 | 0.2046
Epoch 62/300, seasonal_0 Loss: 0.1642 | 0.2128
Epoch 63/300, seasonal_0 Loss: 0.1634 | 0.1955
Epoch 64/300, seasonal_0 Loss: 0.1622 | 0.1963
Epoch 65/300, seasonal_0 Loss: 0.1701 | 0.2168
Epoch 66/300, seasonal_0 Loss: 0.1647 | 0.1966
Epoch 67/300, seasonal_0 Loss: 0.1711 | 0.2130
Epoch 68/300, seasonal_0 Loss: 0.1662 | 0.2038
Epoch 69/300, seasonal_0 Loss: 0.1634 | 0.2049
Epoch 70/300, seasonal_0 Loss: 0.1627 | 0.2034
Epoch 71/300, seasonal_0 Loss: 0.1656 | 0.1856
Epoch 72/300, seasonal_0 Loss: 0.1653 | 0.2048
Epoch 73/300, seasonal_0 Loss: 0.1635 | 0.2026
Epoch 74/300, seasonal_0 Loss: 0.1590 | 0.1819
Epoch 75/300, seasonal_0 Loss: 0.1614 | 0.1897
Epoch 76/300, seasonal_0 Loss: 0.1590 | 0.1948
Epoch 77/300, seasonal_0 Loss: 0.1516 | 0.1843
Epoch 78/300, seasonal_0 Loss: 0.1512 | 0.2027
Epoch 79/300, seasonal_0 Loss: 0.1482 | 0.1796
Epoch 80/300, seasonal_0 Loss: 0.1498 | 0.1855
Epoch 81/300, seasonal_0 Loss: 0.1549 | 0.2018
Epoch 82/300, seasonal_0 Loss: 0.1550 | 0.1741
Epoch 83/300, seasonal_0 Loss: 0.1512 | 0.2013
Epoch 84/300, seasonal_0 Loss: 0.1601 | 0.1864
Epoch 85/300, seasonal_0 Loss: 0.1506 | 0.1820
Epoch 86/300, seasonal_0 Loss: 0.1504 | 0.1853
Epoch 87/300, seasonal_0 Loss: 0.1484 | 0.1744
Epoch 88/300, seasonal_0 Loss: 0.1447 | 0.1918
Epoch 89/300, seasonal_0 Loss: 0.1475 | 0.1840
Epoch 90/300, seasonal_0 Loss: 0.1452 | 0.1744
Epoch 91/300, seasonal_0 Loss: 0.1482 | 0.1812
Epoch 92/300, seasonal_0 Loss: 0.1441 | 0.1723
Epoch 93/300, seasonal_0 Loss: 0.1410 | 0.1784
Epoch 94/300, seasonal_0 Loss: 0.1412 | 0.1893
Epoch 95/300, seasonal_0 Loss: 0.1423 | 0.1758
Epoch 96/300, seasonal_0 Loss: 0.1457 | 0.1677
Epoch 97/300, seasonal_0 Loss: 0.1453 | 0.1783
Epoch 98/300, seasonal_0 Loss: 0.1430 | 0.1787
Epoch 99/300, seasonal_0 Loss: 0.1395 | 0.1730
Epoch 100/300, seasonal_0 Loss: 0.1406 | 0.1661
Epoch 101/300, seasonal_0 Loss: 0.1403 | 0.1791
Epoch 102/300, seasonal_0 Loss: 0.1475 | 0.1716
Epoch 103/300, seasonal_0 Loss: 0.1377 | 0.1718
Epoch 104/300, seasonal_0 Loss: 0.1351 | 0.1695
Epoch 105/300, seasonal_0 Loss: 0.1355 | 0.1641
Epoch 106/300, seasonal_0 Loss: 0.1351 | 0.1674
Epoch 107/300, seasonal_0 Loss: 0.1340 | 0.1739
Epoch 108/300, seasonal_0 Loss: 0.1351 | 0.1698
Epoch 109/300, seasonal_0 Loss: 0.1349 | 0.1586
Epoch 110/300, seasonal_0 Loss: 0.1343 | 0.1658
Epoch 111/300, seasonal_0 Loss: 0.1413 | 0.1887
Epoch 112/300, seasonal_0 Loss: 0.1399 | 0.1652
Epoch 113/300, seasonal_0 Loss: 0.1447 | 0.1589
Epoch 114/300, seasonal_0 Loss: 0.1367 | 0.1738
Epoch 115/300, seasonal_0 Loss: 0.1323 | 0.1675
Epoch 116/300, seasonal_0 Loss: 0.1305 | 0.1622
Epoch 117/300, seasonal_0 Loss: 0.1286 | 0.1603
Epoch 118/300, seasonal_0 Loss: 0.1269 | 0.1694
Epoch 119/300, seasonal_0 Loss: 0.1271 | 0.1617
Epoch 120/300, seasonal_0 Loss: 0.1263 | 0.1628
Epoch 121/300, seasonal_0 Loss: 0.1250 | 0.1599
Epoch 122/300, seasonal_0 Loss: 0.1248 | 0.1633
Epoch 123/300, seasonal_0 Loss: 0.1250 | 0.1617
Epoch 124/300, seasonal_0 Loss: 0.1238 | 0.1602
Epoch 125/300, seasonal_0 Loss: 0.1235 | 0.1578
Epoch 126/300, seasonal_0 Loss: 0.1237 | 0.1592
Epoch 127/300, seasonal_0 Loss: 0.1243 | 0.1638
Epoch 128/300, seasonal_0 Loss: 0.1232 | 0.1588
Epoch 129/300, seasonal_0 Loss: 0.1229 | 0.1526
Epoch 130/300, seasonal_0 Loss: 0.1232 | 0.1557
Epoch 131/300, seasonal_0 Loss: 0.1211 | 0.1642
Epoch 132/300, seasonal_0 Loss: 0.1220 | 0.1586
Epoch 133/300, seasonal_0 Loss: 0.1230 | 0.1525
Epoch 134/300, seasonal_0 Loss: 0.1213 | 0.1626
Epoch 135/300, seasonal_0 Loss: 0.1225 | 0.1612
Epoch 136/300, seasonal_0 Loss: 0.1212 | 0.1627
Epoch 137/300, seasonal_0 Loss: 0.1203 | 0.1532
Epoch 138/300, seasonal_0 Loss: 0.1195 | 0.1570
Epoch 139/300, seasonal_0 Loss: 0.1201 | 0.1628
Epoch 140/300, seasonal_0 Loss: 0.1207 | 0.1595
Epoch 141/300, seasonal_0 Loss: 0.1211 | 0.1539
Epoch 142/300, seasonal_0 Loss: 0.1195 | 0.1573
Epoch 143/300, seasonal_0 Loss: 0.1203 | 0.1541
Epoch 144/300, seasonal_0 Loss: 0.1192 | 0.1638
Epoch 145/300, seasonal_0 Loss: 0.1179 | 0.1543
Epoch 146/300, seasonal_0 Loss: 0.1177 | 0.1555
Epoch 147/300, seasonal_0 Loss: 0.1167 | 0.1542
Epoch 148/300, seasonal_0 Loss: 0.1184 | 0.1603
Epoch 149/300, seasonal_0 Loss: 0.1167 | 0.1542
Epoch 150/300, seasonal_0 Loss: 0.1166 | 0.1525
Epoch 151/300, seasonal_0 Loss: 0.1160 | 0.1532
Epoch 152/300, seasonal_0 Loss: 0.1158 | 0.1641
Epoch 153/300, seasonal_0 Loss: 0.1152 | 0.1535
Epoch 154/300, seasonal_0 Loss: 0.1177 | 0.1494
Epoch 155/300, seasonal_0 Loss: 0.1161 | 0.1480
Epoch 156/300, seasonal_0 Loss: 0.1156 | 0.1604
Epoch 157/300, seasonal_0 Loss: 0.1162 | 0.1612
Epoch 158/300, seasonal_0 Loss: 0.1177 | 0.1489
Epoch 159/300, seasonal_0 Loss: 0.1182 | 0.1483
Epoch 160/300, seasonal_0 Loss: 0.1158 | 0.1586
Epoch 161/300, seasonal_0 Loss: 0.1178 | 0.1622
Epoch 162/300, seasonal_0 Loss: 0.1150 | 0.1502
Epoch 163/300, seasonal_0 Loss: 0.1176 | 0.1461
Epoch 164/300, seasonal_0 Loss: 0.1153 | 0.1565
Epoch 165/300, seasonal_0 Loss: 0.1143 | 0.1588
Epoch 166/300, seasonal_0 Loss: 0.1133 | 0.1484
Epoch 167/300, seasonal_0 Loss: 0.1134 | 0.1474
Epoch 168/300, seasonal_0 Loss: 0.1118 | 0.1547
Epoch 169/300, seasonal_0 Loss: 0.1126 | 0.1560
Epoch 170/300, seasonal_0 Loss: 0.1118 | 0.1490
Epoch 171/300, seasonal_0 Loss: 0.1114 | 0.1468
Epoch 172/300, seasonal_0 Loss: 0.1102 | 0.1542
Epoch 173/300, seasonal_0 Loss: 0.1110 | 0.1536
Epoch 174/300, seasonal_0 Loss: 0.1099 | 0.1483
Epoch 175/300, seasonal_0 Loss: 0.1106 | 0.1482
Epoch 176/300, seasonal_0 Loss: 0.1100 | 0.1545
Epoch 177/300, seasonal_0 Loss: 0.1093 | 0.1533
Epoch 178/300, seasonal_0 Loss: 0.1095 | 0.1460
Epoch 179/300, seasonal_0 Loss: 0.1098 | 0.1473
Epoch 180/300, seasonal_0 Loss: 0.1086 | 0.1548
Epoch 181/300, seasonal_0 Loss: 0.1097 | 0.1527
Epoch 182/300, seasonal_0 Loss: 0.1095 | 0.1467
Epoch 183/300, seasonal_0 Loss: 0.1085 | 0.1482
Epoch 184/300, seasonal_0 Loss: 0.1074 | 0.1530
Epoch 185/300, seasonal_0 Loss: 0.1074 | 0.1522
Epoch 186/300, seasonal_0 Loss: 0.1078 | 0.1469
Epoch 187/300, seasonal_0 Loss: 0.1061 | 0.1483
Epoch 188/300, seasonal_0 Loss: 0.1068 | 0.1503
Epoch 189/300, seasonal_0 Loss: 0.1065 | 0.1501
Epoch 190/300, seasonal_0 Loss: 0.1057 | 0.1479
Epoch 191/300, seasonal_0 Loss: 0.1060 | 0.1458
Epoch 192/300, seasonal_0 Loss: 0.1062 | 0.1508
Epoch 193/300, seasonal_0 Loss: 0.1050 | 0.1487
Epoch 194/300, seasonal_0 Loss: 0.1063 | 0.1461
Epoch 195/300, seasonal_0 Loss: 0.1067 | 0.1454
Epoch 196/300, seasonal_0 Loss: 0.1069 | 0.1514
Epoch 197/300, seasonal_0 Loss: 0.1055 | 0.1483
Epoch 198/300, seasonal_0 Loss: 0.1056 | 0.1482
Epoch 199/300, seasonal_0 Loss: 0.1061 | 0.1474
Epoch 200/300, seasonal_0 Loss: 0.1044 | 0.1476
Epoch 201/300, seasonal_0 Loss: 0.1049 | 0.1464
Epoch 202/300, seasonal_0 Loss: 0.1040 | 0.1459
Epoch 203/300, seasonal_0 Loss: 0.1037 | 0.1457
Epoch 204/300, seasonal_0 Loss: 0.1037 | 0.1476
Epoch 205/300, seasonal_0 Loss: 0.1038 | 0.1479
Epoch 206/300, seasonal_0 Loss: 0.1032 | 0.1462
Epoch 207/300, seasonal_0 Loss: 0.1037 | 0.1439
Epoch 208/300, seasonal_0 Loss: 0.1037 | 0.1496
Epoch 209/300, seasonal_0 Loss: 0.1043 | 0.1481
Epoch 210/300, seasonal_0 Loss: 0.1041 | 0.1476
Epoch 211/300, seasonal_0 Loss: 0.1047 | 0.1457
Epoch 212/300, seasonal_0 Loss: 0.1036 | 0.1481
Epoch 213/300, seasonal_0 Loss: 0.1027 | 0.1495
Epoch 214/300, seasonal_0 Loss: 0.1040 | 0.1487
Epoch 215/300, seasonal_0 Loss: 0.1038 | 0.1443
Epoch 216/300, seasonal_0 Loss: 0.1024 | 0.1500
Epoch 217/300, seasonal_0 Loss: 0.1026 | 0.1485
Epoch 218/300, seasonal_0 Loss: 0.1029 | 0.1462
Epoch 219/300, seasonal_0 Loss: 0.1028 | 0.1459
Epoch 220/300, seasonal_0 Loss: 0.1018 | 0.1466
Epoch 221/300, seasonal_0 Loss: 0.1024 | 0.1446
Epoch 222/300, seasonal_0 Loss: 0.1029 | 0.1450
Epoch 223/300, seasonal_0 Loss: 0.1016 | 0.1455
Epoch 224/300, seasonal_0 Loss: 0.1013 | 0.1453
Epoch 225/300, seasonal_0 Loss: 0.1009 | 0.1457
Epoch 226/300, seasonal_0 Loss: 0.0997 | 0.1441
Epoch 227/300, seasonal_0 Loss: 0.1004 | 0.1429
Epoch 228/300, seasonal_0 Loss: 0.1017 | 0.1449
Epoch 229/300, seasonal_0 Loss: 0.1003 | 0.1445
Epoch 230/300, seasonal_0 Loss: 0.1012 | 0.1447
Epoch 231/300, seasonal_0 Loss: 0.0999 | 0.1461
Epoch 232/300, seasonal_0 Loss: 0.0998 | 0.1469
Epoch 233/300, seasonal_0 Loss: 0.1006 | 0.1453
Epoch 234/300, seasonal_0 Loss: 0.1006 | 0.1432
Epoch 235/300, seasonal_0 Loss: 0.1004 | 0.1455
Epoch 236/300, seasonal_0 Loss: 0.1001 | 0.1459
Epoch 237/300, seasonal_0 Loss: 0.0996 | 0.1456
Epoch 238/300, seasonal_0 Loss: 0.0996 | 0.1427
Epoch 239/300, seasonal_0 Loss: 0.0990 | 0.1449
Epoch 240/300, seasonal_0 Loss: 0.0996 | 0.1458
Epoch 241/300, seasonal_0 Loss: 0.0993 | 0.1443
Epoch 242/300, seasonal_0 Loss: 0.0992 | 0.1460
Epoch 243/300, seasonal_0 Loss: 0.0988 | 0.1474
Epoch 244/300, seasonal_0 Loss: 0.0991 | 0.1439
Epoch 245/300, seasonal_0 Loss: 0.0991 | 0.1428
Epoch 246/300, seasonal_0 Loss: 0.0980 | 0.1446
Epoch 247/300, seasonal_0 Loss: 0.0974 | 0.1460
Epoch 248/300, seasonal_0 Loss: 0.0979 | 0.1450
Epoch 249/300, seasonal_0 Loss: 0.0980 | 0.1448
Epoch 250/300, seasonal_0 Loss: 0.0982 | 0.1465
Epoch 251/300, seasonal_0 Loss: 0.0984 | 0.1439
Epoch 252/300, seasonal_0 Loss: 0.0991 | 0.1428
Epoch 253/300, seasonal_0 Loss: 0.0986 | 0.1436
Epoch 254/300, seasonal_0 Loss: 0.0987 | 0.1454
Epoch 255/300, seasonal_0 Loss: 0.0981 | 0.1434
Epoch 256/300, seasonal_0 Loss: 0.0979 | 0.1436
Epoch 257/300, seasonal_0 Loss: 0.0978 | 0.1445
Epoch 258/300, seasonal_0 Loss: 0.0979 | 0.1421
Epoch 259/300, seasonal_0 Loss: 0.0972 | 0.1418
Epoch 260/300, seasonal_0 Loss: 0.0973 | 0.1433
Epoch 261/300, seasonal_0 Loss: 0.0974 | 0.1435
Epoch 262/300, seasonal_0 Loss: 0.0975 | 0.1421
Epoch 263/300, seasonal_0 Loss: 0.0964 | 0.1424
Epoch 264/300, seasonal_0 Loss: 0.0968 | 0.1422
Epoch 265/300, seasonal_0 Loss: 0.0967 | 0.1421
Epoch 266/300, seasonal_0 Loss: 0.0970 | 0.1432
Epoch 267/300, seasonal_0 Loss: 0.0970 | 0.1435
Epoch 268/300, seasonal_0 Loss: 0.0971 | 0.1423
Epoch 269/300, seasonal_0 Loss: 0.0965 | 0.1438
Epoch 270/300, seasonal_0 Loss: 0.0967 | 0.1436
Epoch 271/300, seasonal_0 Loss: 0.0966 | 0.1421
Epoch 272/300, seasonal_0 Loss: 0.0965 | 0.1415
Epoch 273/300, seasonal_0 Loss: 0.0964 | 0.1425
Epoch 274/300, seasonal_0 Loss: 0.0972 | 0.1428
Epoch 275/300, seasonal_0 Loss: 0.0958 | 0.1422
Epoch 276/300, seasonal_0 Loss: 0.0969 | 0.1431
Epoch 277/300, seasonal_0 Loss: 0.0962 | 0.1423
Epoch 278/300, seasonal_0 Loss: 0.0963 | 0.1437
Epoch 279/300, seasonal_0 Loss: 0.0965 | 0.1428
Epoch 280/300, seasonal_0 Loss: 0.0964 | 0.1437
Epoch 281/300, seasonal_0 Loss: 0.0951 | 0.1436
Epoch 282/300, seasonal_0 Loss: 0.0961 | 0.1430
Epoch 283/300, seasonal_0 Loss: 0.0948 | 0.1427
Epoch 284/300, seasonal_0 Loss: 0.0957 | 0.1430
Epoch 285/300, seasonal_0 Loss: 0.0955 | 0.1417
Epoch 286/300, seasonal_0 Loss: 0.0961 | 0.1411
Epoch 287/300, seasonal_0 Loss: 0.0956 | 0.1412
Epoch 288/300, seasonal_0 Loss: 0.0955 | 0.1423
Epoch 289/300, seasonal_0 Loss: 0.0947 | 0.1421
Epoch 290/300, seasonal_0 Loss: 0.0956 | 0.1422
Epoch 291/300, seasonal_0 Loss: 0.0959 | 0.1419
Epoch 292/300, seasonal_0 Loss: 0.0949 | 0.1422
Epoch 293/300, seasonal_0 Loss: 0.0950 | 0.1421
Epoch 294/300, seasonal_0 Loss: 0.0944 | 0.1408
Epoch 295/300, seasonal_0 Loss: 0.0960 | 0.1423
Epoch 296/300, seasonal_0 Loss: 0.0954 | 0.1416
Epoch 297/300, seasonal_0 Loss: 0.0946 | 0.1403
Epoch 298/300, seasonal_0 Loss: 0.0951 | 0.1411
Epoch 299/300, seasonal_0 Loss: 0.0953 | 0.1402
Epoch 300/300, seasonal_0 Loss: 0.0945 | 0.1401
Training seasonal_1 component with params: {'observation_period_num': 8, 'train_rates': 0.9886463350722028, 'learning_rate': 0.00011451733459865281, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9877302632288697}
Epoch 1/300, seasonal_1 Loss: 1.1440 | 1.9220
Epoch 2/300, seasonal_1 Loss: 0.7156 | 1.2657
Epoch 3/300, seasonal_1 Loss: 0.5862 | 1.0276
Epoch 4/300, seasonal_1 Loss: 0.5152 | 0.9270
Epoch 5/300, seasonal_1 Loss: 0.4705 | 0.8617
Epoch 6/300, seasonal_1 Loss: 0.4344 | 0.8149
Epoch 7/300, seasonal_1 Loss: 0.4159 | 0.7750
Epoch 8/300, seasonal_1 Loss: 0.4186 | 0.7616
Epoch 9/300, seasonal_1 Loss: 0.4229 | 0.7451
Epoch 10/300, seasonal_1 Loss: 0.4158 | 0.7204
Epoch 11/300, seasonal_1 Loss: 0.3920 | 0.6765
Epoch 12/300, seasonal_1 Loss: 0.3496 | 0.6403
Epoch 13/300, seasonal_1 Loss: 0.3191 | 0.6160
Epoch 14/300, seasonal_1 Loss: 0.3182 | 0.6037
Epoch 15/300, seasonal_1 Loss: 0.3214 | 0.5791
Epoch 16/300, seasonal_1 Loss: 0.3187 | 0.5609
Epoch 17/300, seasonal_1 Loss: 0.3442 | 0.5628
Epoch 18/300, seasonal_1 Loss: 0.3585 | 0.5498
Epoch 19/300, seasonal_1 Loss: 0.3876 | 0.6664
Epoch 20/300, seasonal_1 Loss: 0.3201 | 0.5235
Epoch 21/300, seasonal_1 Loss: 0.2871 | 0.5044
Epoch 22/300, seasonal_1 Loss: 0.2855 | 0.4929
Epoch 23/300, seasonal_1 Loss: 0.2684 | 0.4843
Epoch 24/300, seasonal_1 Loss: 0.2696 | 0.4731
Epoch 25/300, seasonal_1 Loss: 0.2647 | 0.4740
Epoch 26/300, seasonal_1 Loss: 0.2630 | 0.4559
Epoch 27/300, seasonal_1 Loss: 0.2514 | 0.4471
Epoch 28/300, seasonal_1 Loss: 0.2404 | 0.4365
Epoch 29/300, seasonal_1 Loss: 0.2406 | 0.4194
Epoch 30/300, seasonal_1 Loss: 0.2382 | 0.4223
Epoch 31/300, seasonal_1 Loss: 0.2388 | 0.4047
Epoch 32/300, seasonal_1 Loss: 0.2407 | 0.4068
Epoch 33/300, seasonal_1 Loss: 0.2384 | 0.3959
Epoch 34/300, seasonal_1 Loss: 0.2641 | 0.4014
Epoch 35/300, seasonal_1 Loss: 0.2480 | 0.3921
Epoch 36/300, seasonal_1 Loss: 0.2551 | 0.3845
Epoch 37/300, seasonal_1 Loss: 0.2432 | 0.3989
Epoch 38/300, seasonal_1 Loss: 0.2365 | 0.3716
Epoch 39/300, seasonal_1 Loss: 0.2465 | 0.3700
Epoch 40/300, seasonal_1 Loss: 0.2175 | 0.3727
Epoch 41/300, seasonal_1 Loss: 0.2106 | 0.3516
Epoch 42/300, seasonal_1 Loss: 0.2171 | 0.3541
Epoch 43/300, seasonal_1 Loss: 0.2115 | 0.3385
Epoch 44/300, seasonal_1 Loss: 0.2194 | 0.3448
Epoch 45/300, seasonal_1 Loss: 0.2058 | 0.3410
Epoch 46/300, seasonal_1 Loss: 0.1993 | 0.3305
Epoch 47/300, seasonal_1 Loss: 0.1944 | 0.3230
Epoch 48/300, seasonal_1 Loss: 0.1886 | 0.3225
Epoch 49/300, seasonal_1 Loss: 0.1867 | 0.3086
Epoch 50/300, seasonal_1 Loss: 0.1846 | 0.3170
Epoch 51/300, seasonal_1 Loss: 0.1866 | 0.3014
Epoch 52/300, seasonal_1 Loss: 0.1889 | 0.3061
Epoch 53/300, seasonal_1 Loss: 0.1813 | 0.2937
Epoch 54/300, seasonal_1 Loss: 0.1815 | 0.2945
Epoch 55/300, seasonal_1 Loss: 0.1796 | 0.2910
Epoch 56/300, seasonal_1 Loss: 0.1857 | 0.2839
Epoch 57/300, seasonal_1 Loss: 0.1990 | 0.2875
Epoch 58/300, seasonal_1 Loss: 0.1956 | 0.2779
Epoch 59/300, seasonal_1 Loss: 0.2006 | 0.2850
Epoch 60/300, seasonal_1 Loss: 0.2413 | 0.2805
Epoch 61/300, seasonal_1 Loss: 0.2378 | 0.2777
Epoch 62/300, seasonal_1 Loss: 0.2560 | 0.2925
Epoch 63/300, seasonal_1 Loss: 0.2129 | 0.2857
Epoch 64/300, seasonal_1 Loss: 0.2019 | 0.3055
Epoch 65/300, seasonal_1 Loss: 0.1921 | 0.2769
Epoch 66/300, seasonal_1 Loss: 0.1867 | 0.2905
Epoch 67/300, seasonal_1 Loss: 0.1727 | 0.2617
Epoch 68/300, seasonal_1 Loss: 0.1691 | 0.2663
Epoch 69/300, seasonal_1 Loss: 0.1666 | 0.2557
Epoch 70/300, seasonal_1 Loss: 0.1669 | 0.2581
Epoch 71/300, seasonal_1 Loss: 0.1702 | 0.2541
Epoch 72/300, seasonal_1 Loss: 0.1651 | 0.2502
Epoch 73/300, seasonal_1 Loss: 0.1635 | 0.2470
Epoch 74/300, seasonal_1 Loss: 0.1576 | 0.2444
Epoch 75/300, seasonal_1 Loss: 0.1565 | 0.2421
Epoch 76/300, seasonal_1 Loss: 0.1591 | 0.2419
Epoch 77/300, seasonal_1 Loss: 0.1620 | 0.2376
Epoch 78/300, seasonal_1 Loss: 0.1657 | 0.2467
Epoch 79/300, seasonal_1 Loss: 0.1598 | 0.2331
Epoch 80/300, seasonal_1 Loss: 0.1613 | 0.2382
Epoch 81/300, seasonal_1 Loss: 0.1787 | 0.2324
Epoch 82/300, seasonal_1 Loss: 0.1884 | 0.2313
Epoch 83/300, seasonal_1 Loss: 0.2068 | 0.2445
Epoch 84/300, seasonal_1 Loss: 0.1819 | 0.2323
Epoch 85/300, seasonal_1 Loss: 0.1743 | 0.2589
Epoch 86/300, seasonal_1 Loss: 0.1657 | 0.2281
Epoch 87/300, seasonal_1 Loss: 0.1607 | 0.2440
Epoch 88/300, seasonal_1 Loss: 0.1593 | 0.2255
Epoch 89/300, seasonal_1 Loss: 0.1593 | 0.2356
Epoch 90/300, seasonal_1 Loss: 0.1592 | 0.2256
Epoch 91/300, seasonal_1 Loss: 0.1569 | 0.2265
Epoch 92/300, seasonal_1 Loss: 0.1561 | 0.2222
Epoch 93/300, seasonal_1 Loss: 0.1511 | 0.2217
Epoch 94/300, seasonal_1 Loss: 0.1492 | 0.2152
Epoch 95/300, seasonal_1 Loss: 0.1494 | 0.2188
Epoch 96/300, seasonal_1 Loss: 0.1538 | 0.2096
Epoch 97/300, seasonal_1 Loss: 0.1657 | 0.2182
Epoch 98/300, seasonal_1 Loss: 0.1627 | 0.2078
Epoch 99/300, seasonal_1 Loss: 0.1589 | 0.2250
Epoch 100/300, seasonal_1 Loss: 0.1456 | 0.2059
Epoch 101/300, seasonal_1 Loss: 0.1397 | 0.2119
Epoch 102/300, seasonal_1 Loss: 0.1405 | 0.2036
Epoch 103/300, seasonal_1 Loss: 0.1415 | 0.2034
Epoch 104/300, seasonal_1 Loss: 0.1448 | 0.2029
Epoch 105/300, seasonal_1 Loss: 0.1433 | 0.2002
Epoch 106/300, seasonal_1 Loss: 0.1459 | 0.1993
Epoch 107/300, seasonal_1 Loss: 0.1470 | 0.2021
Epoch 108/300, seasonal_1 Loss: 0.1543 | 0.1936
Epoch 109/300, seasonal_1 Loss: 0.1758 | 0.2109
Epoch 110/300, seasonal_1 Loss: 0.1727 | 0.1963
Epoch 111/300, seasonal_1 Loss: 0.1783 | 0.2317
Epoch 112/300, seasonal_1 Loss: 0.1610 | 0.1954
Epoch 113/300, seasonal_1 Loss: 0.1513 | 0.2140
Epoch 114/300, seasonal_1 Loss: 0.1485 | 0.2012
Epoch 115/300, seasonal_1 Loss: 0.1478 | 0.2016
Epoch 116/300, seasonal_1 Loss: 0.1497 | 0.1995
Epoch 117/300, seasonal_1 Loss: 0.1446 | 0.1979
Epoch 118/300, seasonal_1 Loss: 0.1462 | 0.1909
Epoch 119/300, seasonal_1 Loss: 0.1419 | 0.1918
Epoch 120/300, seasonal_1 Loss: 0.1391 | 0.1870
Epoch 121/300, seasonal_1 Loss: 0.1343 | 0.1884
Epoch 122/300, seasonal_1 Loss: 0.1333 | 0.1838
Epoch 123/300, seasonal_1 Loss: 0.1349 | 0.1861
Epoch 124/300, seasonal_1 Loss: 0.1382 | 0.1798
Epoch 125/300, seasonal_1 Loss: 0.1484 | 0.1870
Epoch 126/300, seasonal_1 Loss: 0.1509 | 0.1819
Epoch 127/300, seasonal_1 Loss: 0.1588 | 0.1919
Epoch 128/300, seasonal_1 Loss: 0.1540 | 0.1815
Epoch 129/300, seasonal_1 Loss: 0.1565 | 0.1917
Epoch 130/300, seasonal_1 Loss: 0.1521 | 0.1833
Epoch 131/300, seasonal_1 Loss: 0.1496 | 0.1928
Epoch 132/300, seasonal_1 Loss: 0.1434 | 0.1827
Epoch 133/300, seasonal_1 Loss: 0.1374 | 0.1905
Epoch 134/300, seasonal_1 Loss: 0.1357 | 0.1779
Epoch 135/300, seasonal_1 Loss: 0.1350 | 0.1872
Epoch 136/300, seasonal_1 Loss: 0.1381 | 0.1741
Epoch 137/300, seasonal_1 Loss: 0.1401 | 0.1792
Epoch 138/300, seasonal_1 Loss: 0.1428 | 0.1757
Epoch 139/300, seasonal_1 Loss: 0.1365 | 0.1700
Epoch 140/300, seasonal_1 Loss: 0.1356 | 0.1732
Epoch 141/300, seasonal_1 Loss: 0.1324 | 0.1706
Epoch 142/300, seasonal_1 Loss: 0.1316 | 0.1696
Epoch 143/300, seasonal_1 Loss: 0.1289 | 0.1684
Epoch 144/300, seasonal_1 Loss: 0.1287 | 0.1673
Epoch 145/300, seasonal_1 Loss: 0.1262 | 0.1663
Epoch 146/300, seasonal_1 Loss: 0.1236 | 0.1678
Epoch 147/300, seasonal_1 Loss: 0.1205 | 0.1615
Epoch 148/300, seasonal_1 Loss: 0.1195 | 0.1645
Epoch 149/300, seasonal_1 Loss: 0.1190 | 0.1599
Epoch 150/300, seasonal_1 Loss: 0.1180 | 0.1647
Epoch 151/300, seasonal_1 Loss: 0.1173 | 0.1577
Epoch 152/300, seasonal_1 Loss: 0.1164 | 0.1622
Epoch 153/300, seasonal_1 Loss: 0.1162 | 0.1570
Epoch 154/300, seasonal_1 Loss: 0.1167 | 0.1592
Epoch 155/300, seasonal_1 Loss: 0.1165 | 0.1555
Epoch 156/300, seasonal_1 Loss: 0.1138 | 0.1561
Epoch 157/300, seasonal_1 Loss: 0.1134 | 0.1528
Epoch 158/300, seasonal_1 Loss: 0.1128 | 0.1551
Epoch 159/300, seasonal_1 Loss: 0.1136 | 0.1508
Epoch 160/300, seasonal_1 Loss: 0.1132 | 0.1545
Epoch 161/300, seasonal_1 Loss: 0.1124 | 0.1474
Epoch 162/300, seasonal_1 Loss: 0.1104 | 0.1530
Epoch 163/300, seasonal_1 Loss: 0.1105 | 0.1476
Epoch 164/300, seasonal_1 Loss: 0.1104 | 0.1498
Epoch 165/300, seasonal_1 Loss: 0.1100 | 0.1462
Epoch 166/300, seasonal_1 Loss: 0.1108 | 0.1476
Epoch 167/300, seasonal_1 Loss: 0.1103 | 0.1457
Epoch 168/300, seasonal_1 Loss: 0.1104 | 0.1462
Epoch 169/300, seasonal_1 Loss: 0.1102 | 0.1439
Epoch 170/300, seasonal_1 Loss: 0.1106 | 0.1474
Epoch 171/300, seasonal_1 Loss: 0.1090 | 0.1422
Epoch 172/300, seasonal_1 Loss: 0.1089 | 0.1482
Epoch 173/300, seasonal_1 Loss: 0.1091 | 0.1399
Epoch 174/300, seasonal_1 Loss: 0.1094 | 0.1479
Epoch 175/300, seasonal_1 Loss: 0.1105 | 0.1397
Epoch 176/300, seasonal_1 Loss: 0.1095 | 0.1445
Epoch 177/300, seasonal_1 Loss: 0.1108 | 0.1400
Epoch 178/300, seasonal_1 Loss: 0.1094 | 0.1415
Epoch 179/300, seasonal_1 Loss: 0.1103 | 0.1403
Epoch 180/300, seasonal_1 Loss: 0.1162 | 0.1444
Epoch 181/300, seasonal_1 Loss: 0.1220 | 0.1440
Epoch 182/300, seasonal_1 Loss: 0.1297 | 0.1550
Epoch 183/300, seasonal_1 Loss: 0.1251 | 0.1398
Epoch 184/300, seasonal_1 Loss: 0.1243 | 0.1564
Epoch 185/300, seasonal_1 Loss: 0.1194 | 0.1406
Epoch 186/300, seasonal_1 Loss: 0.1165 | 0.1425
Epoch 187/300, seasonal_1 Loss: 0.1140 | 0.1409
Epoch 188/300, seasonal_1 Loss: 0.1083 | 0.1396
Epoch 189/300, seasonal_1 Loss: 0.1053 | 0.1396
Epoch 190/300, seasonal_1 Loss: 0.1025 | 0.1389
Epoch 191/300, seasonal_1 Loss: 0.1013 | 0.1365
Epoch 192/300, seasonal_1 Loss: 0.1008 | 0.1370
Epoch 193/300, seasonal_1 Loss: 0.1030 | 0.1358
Epoch 194/300, seasonal_1 Loss: 0.1057 | 0.1369
Epoch 195/300, seasonal_1 Loss: 0.1070 | 0.1368
Epoch 196/300, seasonal_1 Loss: 0.1069 | 0.1403
Epoch 197/300, seasonal_1 Loss: 0.1039 | 0.1341
Epoch 198/300, seasonal_1 Loss: 0.1015 | 0.1380
Epoch 199/300, seasonal_1 Loss: 0.0998 | 0.1316
Epoch 200/300, seasonal_1 Loss: 0.0992 | 0.1344
Epoch 201/300, seasonal_1 Loss: 0.0991 | 0.1309
Epoch 202/300, seasonal_1 Loss: 0.0991 | 0.1334
Epoch 203/300, seasonal_1 Loss: 0.0995 | 0.1317
Epoch 204/300, seasonal_1 Loss: 0.0987 | 0.1324
Epoch 205/300, seasonal_1 Loss: 0.0983 | 0.1318
Epoch 206/300, seasonal_1 Loss: 0.0977 | 0.1292
Epoch 207/300, seasonal_1 Loss: 0.1003 | 0.1353
Epoch 208/300, seasonal_1 Loss: 0.1062 | 0.1312
Epoch 209/300, seasonal_1 Loss: 0.1122 | 0.1359
Epoch 210/300, seasonal_1 Loss: 0.1178 | 0.1378
Epoch 211/300, seasonal_1 Loss: 0.1104 | 0.1315
Epoch 212/300, seasonal_1 Loss: 0.1045 | 0.1383
Epoch 213/300, seasonal_1 Loss: 0.1014 | 0.1308
Epoch 214/300, seasonal_1 Loss: 0.1027 | 0.1316
Epoch 215/300, seasonal_1 Loss: 0.1069 | 0.1306
Epoch 216/300, seasonal_1 Loss: 0.1063 | 0.1315
Epoch 217/300, seasonal_1 Loss: 0.1064 | 0.1330
Epoch 218/300, seasonal_1 Loss: 0.1035 | 0.1322
Epoch 219/300, seasonal_1 Loss: 0.1018 | 0.1332
Epoch 220/300, seasonal_1 Loss: 0.1019 | 0.1309
Epoch 221/300, seasonal_1 Loss: 0.1010 | 0.1319
Epoch 222/300, seasonal_1 Loss: 0.0995 | 0.1297
Epoch 223/300, seasonal_1 Loss: 0.0980 | 0.1298
Epoch 224/300, seasonal_1 Loss: 0.0967 | 0.1292
Epoch 225/300, seasonal_1 Loss: 0.0983 | 0.1297
Epoch 226/300, seasonal_1 Loss: 0.0999 | 0.1308
Epoch 227/300, seasonal_1 Loss: 0.1023 | 0.1326
Epoch 228/300, seasonal_1 Loss: 0.1083 | 0.1349
Epoch 229/300, seasonal_1 Loss: 0.1086 | 0.1357
Epoch 230/300, seasonal_1 Loss: 0.1092 | 0.1408
Epoch 231/300, seasonal_1 Loss: 0.1051 | 0.1337
Epoch 232/300, seasonal_1 Loss: 0.1021 | 0.1392
Epoch 233/300, seasonal_1 Loss: 0.0999 | 0.1311
Epoch 234/300, seasonal_1 Loss: 0.0977 | 0.1347
Epoch 235/300, seasonal_1 Loss: 0.0953 | 0.1276
Epoch 236/300, seasonal_1 Loss: 0.0949 | 0.1311
Epoch 237/300, seasonal_1 Loss: 0.0944 | 0.1263
Epoch 238/300, seasonal_1 Loss: 0.0947 | 0.1272
Epoch 239/300, seasonal_1 Loss: 0.0970 | 0.1260
Epoch 240/300, seasonal_1 Loss: 0.0979 | 0.1258
Epoch 241/300, seasonal_1 Loss: 0.0986 | 0.1282
Epoch 242/300, seasonal_1 Loss: 0.0967 | 0.1260
Epoch 243/300, seasonal_1 Loss: 0.0950 | 0.1281
Epoch 244/300, seasonal_1 Loss: 0.0939 | 0.1262
Epoch 245/300, seasonal_1 Loss: 0.0927 | 0.1264
Epoch 246/300, seasonal_1 Loss: 0.0931 | 0.1243
Epoch 247/300, seasonal_1 Loss: 0.0946 | 0.1262
Epoch 248/300, seasonal_1 Loss: 0.0960 | 0.1241
Epoch 249/300, seasonal_1 Loss: 0.0984 | 0.1262
Epoch 250/300, seasonal_1 Loss: 0.0990 | 0.1250
Epoch 251/300, seasonal_1 Loss: 0.1020 | 0.1305
Epoch 252/300, seasonal_1 Loss: 0.1014 | 0.1247
Epoch 253/300, seasonal_1 Loss: 0.1051 | 0.1342
Epoch 254/300, seasonal_1 Loss: 0.1033 | 0.1298
Epoch 255/300, seasonal_1 Loss: 0.1065 | 0.1367
Epoch 256/300, seasonal_1 Loss: 0.1050 | 0.1299
Epoch 257/300, seasonal_1 Loss: 0.1071 | 0.1340
Epoch 258/300, seasonal_1 Loss: 0.1036 | 0.1280
Epoch 259/300, seasonal_1 Loss: 0.1022 | 0.1332
Epoch 260/300, seasonal_1 Loss: 0.0991 | 0.1274
Epoch 261/300, seasonal_1 Loss: 0.0953 | 0.1287
Epoch 262/300, seasonal_1 Loss: 0.0926 | 0.1251
Epoch 263/300, seasonal_1 Loss: 0.0917 | 0.1252
Epoch 264/300, seasonal_1 Loss: 0.0899 | 0.1236
Epoch 265/300, seasonal_1 Loss: 0.0883 | 0.1227
Epoch 266/300, seasonal_1 Loss: 0.0875 | 0.1236
Epoch 267/300, seasonal_1 Loss: 0.0878 | 0.1229
Epoch 268/300, seasonal_1 Loss: 0.0880 | 0.1225
Epoch 269/300, seasonal_1 Loss: 0.0886 | 0.1222
Epoch 270/300, seasonal_1 Loss: 0.0875 | 0.1227
Epoch 271/300, seasonal_1 Loss: 0.0869 | 0.1219
Epoch 272/300, seasonal_1 Loss: 0.0864 | 0.1238
Epoch 273/300, seasonal_1 Loss: 0.0867 | 0.1206
Epoch 274/300, seasonal_1 Loss: 0.0865 | 0.1222
Epoch 275/300, seasonal_1 Loss: 0.0881 | 0.1210
Epoch 276/300, seasonal_1 Loss: 0.0888 | 0.1247
Epoch 277/300, seasonal_1 Loss: 0.0907 | 0.1243
Epoch 278/300, seasonal_1 Loss: 0.0899 | 0.1263
Epoch 279/300, seasonal_1 Loss: 0.0896 | 0.1225
Epoch 280/300, seasonal_1 Loss: 0.0890 | 0.1234
Epoch 281/300, seasonal_1 Loss: 0.0890 | 0.1240
Epoch 282/300, seasonal_1 Loss: 0.0878 | 0.1243
Epoch 283/300, seasonal_1 Loss: 0.0874 | 0.1246
Epoch 284/300, seasonal_1 Loss: 0.0879 | 0.1225
Epoch 285/300, seasonal_1 Loss: 0.0887 | 0.1234
Epoch 286/300, seasonal_1 Loss: 0.0892 | 0.1220
Epoch 287/300, seasonal_1 Loss: 0.0892 | 0.1210
Epoch 288/300, seasonal_1 Loss: 0.0886 | 0.1203
Epoch 289/300, seasonal_1 Loss: 0.0896 | 0.1207
Epoch 290/300, seasonal_1 Loss: 0.0887 | 0.1196
Epoch 291/300, seasonal_1 Loss: 0.0883 | 0.1206
Epoch 292/300, seasonal_1 Loss: 0.0858 | 0.1206
Epoch 293/300, seasonal_1 Loss: 0.0844 | 0.1210
Epoch 294/300, seasonal_1 Loss: 0.0834 | 0.1212
Epoch 295/300, seasonal_1 Loss: 0.0840 | 0.1202
Epoch 296/300, seasonal_1 Loss: 0.0841 | 0.1202
Epoch 297/300, seasonal_1 Loss: 0.0856 | 0.1195
Epoch 298/300, seasonal_1 Loss: 0.0854 | 0.1189
Epoch 299/300, seasonal_1 Loss: 0.0853 | 0.1186
Epoch 300/300, seasonal_1 Loss: 0.0832 | 0.1180
Training seasonal_2 component with params: {'observation_period_num': 20, 'train_rates': 0.9770243428128543, 'learning_rate': 0.00025945020984547996, 'batch_size': 178, 'step_size': 7, 'gamma': 0.8992789759004131}
Epoch 1/300, seasonal_2 Loss: 1.1430 | 2.0758
Epoch 2/300, seasonal_2 Loss: 0.7629 | 1.2497
Epoch 3/300, seasonal_2 Loss: 0.6562 | 1.0352
Epoch 4/300, seasonal_2 Loss: 0.5663 | 0.9255
Epoch 5/300, seasonal_2 Loss: 0.5072 | 0.8612
Epoch 6/300, seasonal_2 Loss: 0.4779 | 0.8350
Epoch 7/300, seasonal_2 Loss: 0.5499 | 0.7500
Epoch 8/300, seasonal_2 Loss: 0.5542 | 0.8939
Epoch 9/300, seasonal_2 Loss: 0.4268 | 0.7008
Epoch 10/300, seasonal_2 Loss: 0.3693 | 0.6318
Epoch 11/300, seasonal_2 Loss: 0.3604 | 0.6127
Epoch 12/300, seasonal_2 Loss: 0.3388 | 0.5711
Epoch 13/300, seasonal_2 Loss: 0.3590 | 0.5531
Epoch 14/300, seasonal_2 Loss: 0.3662 | 0.5411
Epoch 15/300, seasonal_2 Loss: 0.4273 | 0.5863
Epoch 16/300, seasonal_2 Loss: 0.3202 | 0.5142
Epoch 17/300, seasonal_2 Loss: 0.2952 | 0.5008
Epoch 18/300, seasonal_2 Loss: 0.2685 | 0.4779
Epoch 19/300, seasonal_2 Loss: 0.2624 | 0.4568
Epoch 20/300, seasonal_2 Loss: 0.2710 | 0.4441
Epoch 21/300, seasonal_2 Loss: 0.2703 | 0.4357
Epoch 22/300, seasonal_2 Loss: 0.3048 | 0.4332
Epoch 23/300, seasonal_2 Loss: 0.2787 | 0.4491
Epoch 24/300, seasonal_2 Loss: 0.2758 | 0.4179
Epoch 25/300, seasonal_2 Loss: 0.2636 | 0.4079
Epoch 26/300, seasonal_2 Loss: 0.2505 | 0.3945
Epoch 27/300, seasonal_2 Loss: 0.2486 | 0.3918
Epoch 28/300, seasonal_2 Loss: 0.2387 | 0.3757
Epoch 29/300, seasonal_2 Loss: 0.2319 | 0.3785
Epoch 30/300, seasonal_2 Loss: 0.2275 | 0.3621
Epoch 31/300, seasonal_2 Loss: 0.2227 | 0.3573
Epoch 32/300, seasonal_2 Loss: 0.2196 | 0.3442
Epoch 33/300, seasonal_2 Loss: 0.2151 | 0.3400
Epoch 34/300, seasonal_2 Loss: 0.2129 | 0.3326
Epoch 35/300, seasonal_2 Loss: 0.2098 | 0.3279
Epoch 36/300, seasonal_2 Loss: 0.2077 | 0.3214
Epoch 37/300, seasonal_2 Loss: 0.2079 | 0.3172
Epoch 38/300, seasonal_2 Loss: 0.2104 | 0.3156
Epoch 39/300, seasonal_2 Loss: 0.2141 | 0.3083
Epoch 40/300, seasonal_2 Loss: 0.2087 | 0.3079
Epoch 41/300, seasonal_2 Loss: 0.2055 | 0.3015
Epoch 42/300, seasonal_2 Loss: 0.2001 | 0.2976
Epoch 43/300, seasonal_2 Loss: 0.1970 | 0.2935
Epoch 44/300, seasonal_2 Loss: 0.1954 | 0.2906
Epoch 45/300, seasonal_2 Loss: 0.1951 | 0.2881
Epoch 46/300, seasonal_2 Loss: 0.1935 | 0.2853
Epoch 47/300, seasonal_2 Loss: 0.1922 | 0.2830
Epoch 48/300, seasonal_2 Loss: 0.1909 | 0.2786
Epoch 49/300, seasonal_2 Loss: 0.1899 | 0.2765
Epoch 50/300, seasonal_2 Loss: 0.1887 | 0.2736
Epoch 51/300, seasonal_2 Loss: 0.1864 | 0.2707
Epoch 52/300, seasonal_2 Loss: 0.1863 | 0.2680
Epoch 53/300, seasonal_2 Loss: 0.1855 | 0.2668
Epoch 54/300, seasonal_2 Loss: 0.1851 | 0.2644
Epoch 55/300, seasonal_2 Loss: 0.1836 | 0.2622
Epoch 56/300, seasonal_2 Loss: 0.1840 | 0.2604
Epoch 57/300, seasonal_2 Loss: 0.1823 | 0.2582
Epoch 58/300, seasonal_2 Loss: 0.1822 | 0.2571
Epoch 59/300, seasonal_2 Loss: 0.1806 | 0.2552
Epoch 60/300, seasonal_2 Loss: 0.1807 | 0.2533
Epoch 61/300, seasonal_2 Loss: 0.1804 | 0.2515
Epoch 62/300, seasonal_2 Loss: 0.1794 | 0.2501
Epoch 63/300, seasonal_2 Loss: 0.1774 | 0.2488
Epoch 64/300, seasonal_2 Loss: 0.1777 | 0.2469
Epoch 65/300, seasonal_2 Loss: 0.1771 | 0.2455
Epoch 66/300, seasonal_2 Loss: 0.1764 | 0.2441
Epoch 67/300, seasonal_2 Loss: 0.1762 | 0.2432
Epoch 68/300, seasonal_2 Loss: 0.1752 | 0.2418
Epoch 69/300, seasonal_2 Loss: 0.1753 | 0.2412
Epoch 70/300, seasonal_2 Loss: 0.1741 | 0.2399
Epoch 71/300, seasonal_2 Loss: 0.1744 | 0.2383
Epoch 72/300, seasonal_2 Loss: 0.1745 | 0.2373
Epoch 73/300, seasonal_2 Loss: 0.1731 | 0.2366
Epoch 74/300, seasonal_2 Loss: 0.1737 | 0.2366
Epoch 75/300, seasonal_2 Loss: 0.1712 | 0.2351
Epoch 76/300, seasonal_2 Loss: 0.1723 | 0.2348
Epoch 77/300, seasonal_2 Loss: 0.1720 | 0.2330
Epoch 78/300, seasonal_2 Loss: 0.1715 | 0.2317
Epoch 79/300, seasonal_2 Loss: 0.1715 | 0.2313
Epoch 80/300, seasonal_2 Loss: 0.1708 | 0.2306
Epoch 81/300, seasonal_2 Loss: 0.1700 | 0.2303
Epoch 82/300, seasonal_2 Loss: 0.1708 | 0.2293
Epoch 83/300, seasonal_2 Loss: 0.1698 | 0.2282
Epoch 84/300, seasonal_2 Loss: 0.1693 | 0.2271
Epoch 85/300, seasonal_2 Loss: 0.1698 | 0.2267
Epoch 86/300, seasonal_2 Loss: 0.1688 | 0.2255
Epoch 87/300, seasonal_2 Loss: 0.1684 | 0.2253
Epoch 88/300, seasonal_2 Loss: 0.1681 | 0.2247
Epoch 89/300, seasonal_2 Loss: 0.1682 | 0.2240
Epoch 90/300, seasonal_2 Loss: 0.1675 | 0.2236
Epoch 91/300, seasonal_2 Loss: 0.1670 | 0.2234
Epoch 92/300, seasonal_2 Loss: 0.1672 | 0.2225
Epoch 93/300, seasonal_2 Loss: 0.1667 | 0.2218
Epoch 94/300, seasonal_2 Loss: 0.1665 | 0.2216
Epoch 95/300, seasonal_2 Loss: 0.1662 | 0.2208
Epoch 96/300, seasonal_2 Loss: 0.1662 | 0.2204
Epoch 97/300, seasonal_2 Loss: 0.1658 | 0.2197
Epoch 98/300, seasonal_2 Loss: 0.1655 | 0.2192
Epoch 99/300, seasonal_2 Loss: 0.1652 | 0.2186
Epoch 100/300, seasonal_2 Loss: 0.1654 | 0.2180
Epoch 101/300, seasonal_2 Loss: 0.1656 | 0.2179
Epoch 102/300, seasonal_2 Loss: 0.1651 | 0.2179
Epoch 103/300, seasonal_2 Loss: 0.1642 | 0.2174
Epoch 104/300, seasonal_2 Loss: 0.1649 | 0.2172
Epoch 105/300, seasonal_2 Loss: 0.1641 | 0.2165
Epoch 106/300, seasonal_2 Loss: 0.1642 | 0.2163
Epoch 107/300, seasonal_2 Loss: 0.1641 | 0.2162
Epoch 108/300, seasonal_2 Loss: 0.1638 | 0.2155
Epoch 109/300, seasonal_2 Loss: 0.1634 | 0.2150
Epoch 110/300, seasonal_2 Loss: 0.1634 | 0.2146
Epoch 111/300, seasonal_2 Loss: 0.1640 | 0.2140
Epoch 112/300, seasonal_2 Loss: 0.1632 | 0.2137
Epoch 113/300, seasonal_2 Loss: 0.1635 | 0.2136
Epoch 114/300, seasonal_2 Loss: 0.1636 | 0.2134
Epoch 115/300, seasonal_2 Loss: 0.1634 | 0.2132
Epoch 116/300, seasonal_2 Loss: 0.1628 | 0.2126
Epoch 117/300, seasonal_2 Loss: 0.1627 | 0.2124
Epoch 118/300, seasonal_2 Loss: 0.1623 | 0.2121
Epoch 119/300, seasonal_2 Loss: 0.1622 | 0.2120
Epoch 120/300, seasonal_2 Loss: 0.1623 | 0.2120
Epoch 121/300, seasonal_2 Loss: 0.1625 | 0.2117
Epoch 122/300, seasonal_2 Loss: 0.1622 | 0.2114
Epoch 123/300, seasonal_2 Loss: 0.1611 | 0.2111
Epoch 124/300, seasonal_2 Loss: 0.1613 | 0.2109
Epoch 125/300, seasonal_2 Loss: 0.1612 | 0.2106
Epoch 126/300, seasonal_2 Loss: 0.1612 | 0.2103
Epoch 127/300, seasonal_2 Loss: 0.1616 | 0.2102
Epoch 128/300, seasonal_2 Loss: 0.1611 | 0.2101
Epoch 129/300, seasonal_2 Loss: 0.1616 | 0.2098
Epoch 130/300, seasonal_2 Loss: 0.1610 | 0.2098
Epoch 131/300, seasonal_2 Loss: 0.1608 | 0.2096
Epoch 132/300, seasonal_2 Loss: 0.1610 | 0.2094
Epoch 133/300, seasonal_2 Loss: 0.1610 | 0.2094
Epoch 134/300, seasonal_2 Loss: 0.1606 | 0.2091
Epoch 135/300, seasonal_2 Loss: 0.1605 | 0.2088
Epoch 136/300, seasonal_2 Loss: 0.1610 | 0.2086
Epoch 137/300, seasonal_2 Loss: 0.1607 | 0.2086
Epoch 138/300, seasonal_2 Loss: 0.1607 | 0.2085
Epoch 139/300, seasonal_2 Loss: 0.1608 | 0.2084
Epoch 140/300, seasonal_2 Loss: 0.1601 | 0.2084
Epoch 141/300, seasonal_2 Loss: 0.1595 | 0.2084
Epoch 142/300, seasonal_2 Loss: 0.1603 | 0.2084
Epoch 143/300, seasonal_2 Loss: 0.1598 | 0.2082
Epoch 144/300, seasonal_2 Loss: 0.1607 | 0.2082
Epoch 145/300, seasonal_2 Loss: 0.1604 | 0.2079
Epoch 146/300, seasonal_2 Loss: 0.1595 | 0.2076
Epoch 147/300, seasonal_2 Loss: 0.1599 | 0.2074
Epoch 148/300, seasonal_2 Loss: 0.1601 | 0.2073
Epoch 149/300, seasonal_2 Loss: 0.1601 | 0.2071
Epoch 150/300, seasonal_2 Loss: 0.1598 | 0.2070
Epoch 151/300, seasonal_2 Loss: 0.1600 | 0.2069
Epoch 152/300, seasonal_2 Loss: 0.1591 | 0.2067
Epoch 153/300, seasonal_2 Loss: 0.1605 | 0.2068
Epoch 154/300, seasonal_2 Loss: 0.1591 | 0.2067
Epoch 155/300, seasonal_2 Loss: 0.1598 | 0.2065
Epoch 156/300, seasonal_2 Loss: 0.1588 | 0.2065
Epoch 157/300, seasonal_2 Loss: 0.1593 | 0.2064
Epoch 158/300, seasonal_2 Loss: 0.1590 | 0.2062
Epoch 159/300, seasonal_2 Loss: 0.1600 | 0.2062
Epoch 160/300, seasonal_2 Loss: 0.1600 | 0.2061
Epoch 161/300, seasonal_2 Loss: 0.1590 | 0.2060
Epoch 162/300, seasonal_2 Loss: 0.1597 | 0.2059
Epoch 163/300, seasonal_2 Loss: 0.1592 | 0.2057
Epoch 164/300, seasonal_2 Loss: 0.1593 | 0.2056
Epoch 165/300, seasonal_2 Loss: 0.1586 | 0.2056
Epoch 166/300, seasonal_2 Loss: 0.1593 | 0.2055
Epoch 167/300, seasonal_2 Loss: 0.1588 | 0.2055
Epoch 168/300, seasonal_2 Loss: 0.1594 | 0.2054
Epoch 169/300, seasonal_2 Loss: 0.1586 | 0.2053
Epoch 170/300, seasonal_2 Loss: 0.1588 | 0.2053
Epoch 171/300, seasonal_2 Loss: 0.1584 | 0.2052
Epoch 172/300, seasonal_2 Loss: 0.1588 | 0.2051
Epoch 173/300, seasonal_2 Loss: 0.1588 | 0.2050
Epoch 174/300, seasonal_2 Loss: 0.1592 | 0.2049
Epoch 175/300, seasonal_2 Loss: 0.1595 | 0.2049
Epoch 176/300, seasonal_2 Loss: 0.1588 | 0.2049
Epoch 177/300, seasonal_2 Loss: 0.1584 | 0.2049
Epoch 178/300, seasonal_2 Loss: 0.1587 | 0.2048
Epoch 179/300, seasonal_2 Loss: 0.1584 | 0.2048
Epoch 180/300, seasonal_2 Loss: 0.1586 | 0.2047
Epoch 181/300, seasonal_2 Loss: 0.1589 | 0.2047
Epoch 182/300, seasonal_2 Loss: 0.1589 | 0.2046
Epoch 183/300, seasonal_2 Loss: 0.1588 | 0.2046
Epoch 184/300, seasonal_2 Loss: 0.1585 | 0.2045
Epoch 185/300, seasonal_2 Loss: 0.1583 | 0.2045
Epoch 186/300, seasonal_2 Loss: 0.1588 | 0.2044
Epoch 187/300, seasonal_2 Loss: 0.1583 | 0.2044
Epoch 188/300, seasonal_2 Loss: 0.1591 | 0.2044
Epoch 189/300, seasonal_2 Loss: 0.1585 | 0.2044
Epoch 190/300, seasonal_2 Loss: 0.1581 | 0.2043
Epoch 191/300, seasonal_2 Loss: 0.1581 | 0.2043
Epoch 192/300, seasonal_2 Loss: 0.1587 | 0.2043
Epoch 193/300, seasonal_2 Loss: 0.1585 | 0.2043
Epoch 194/300, seasonal_2 Loss: 0.1587 | 0.2043
Epoch 195/300, seasonal_2 Loss: 0.1583 | 0.2043
Epoch 196/300, seasonal_2 Loss: 0.1584 | 0.2042
Epoch 197/300, seasonal_2 Loss: 0.1576 | 0.2042
Epoch 198/300, seasonal_2 Loss: 0.1585 | 0.2042
Epoch 199/300, seasonal_2 Loss: 0.1588 | 0.2042
Epoch 200/300, seasonal_2 Loss: 0.1583 | 0.2041
Epoch 201/300, seasonal_2 Loss: 0.1578 | 0.2041
Epoch 202/300, seasonal_2 Loss: 0.1582 | 0.2041
Epoch 203/300, seasonal_2 Loss: 0.1580 | 0.2040
Epoch 204/300, seasonal_2 Loss: 0.1585 | 0.2040
Epoch 205/300, seasonal_2 Loss: 0.1582 | 0.2040
Epoch 206/300, seasonal_2 Loss: 0.1582 | 0.2040
Epoch 207/300, seasonal_2 Loss: 0.1581 | 0.2040
Epoch 208/300, seasonal_2 Loss: 0.1582 | 0.2039
Epoch 209/300, seasonal_2 Loss: 0.1582 | 0.2039
Epoch 210/300, seasonal_2 Loss: 0.1577 | 0.2039
Epoch 211/300, seasonal_2 Loss: 0.1581 | 0.2038
Epoch 212/300, seasonal_2 Loss: 0.1583 | 0.2038
Epoch 213/300, seasonal_2 Loss: 0.1588 | 0.2038
Epoch 214/300, seasonal_2 Loss: 0.1582 | 0.2038
Epoch 215/300, seasonal_2 Loss: 0.1579 | 0.2038
Epoch 216/300, seasonal_2 Loss: 0.1579 | 0.2038
Epoch 217/300, seasonal_2 Loss: 0.1579 | 0.2038
Epoch 218/300, seasonal_2 Loss: 0.1584 | 0.2037
Epoch 219/300, seasonal_2 Loss: 0.1577 | 0.2037
Epoch 220/300, seasonal_2 Loss: 0.1580 | 0.2037
Epoch 221/300, seasonal_2 Loss: 0.1583 | 0.2036
Epoch 222/300, seasonal_2 Loss: 0.1588 | 0.2036
Epoch 223/300, seasonal_2 Loss: 0.1583 | 0.2036
Epoch 224/300, seasonal_2 Loss: 0.1584 | 0.2036
Epoch 225/300, seasonal_2 Loss: 0.1581 | 0.2035
Epoch 226/300, seasonal_2 Loss: 0.1581 | 0.2035
Epoch 227/300, seasonal_2 Loss: 0.1569 | 0.2035
Epoch 228/300, seasonal_2 Loss: 0.1577 | 0.2035
Epoch 229/300, seasonal_2 Loss: 0.1582 | 0.2035
Epoch 230/300, seasonal_2 Loss: 0.1583 | 0.2035
Epoch 231/300, seasonal_2 Loss: 0.1582 | 0.2034
Epoch 232/300, seasonal_2 Loss: 0.1583 | 0.2034
Epoch 233/300, seasonal_2 Loss: 0.1578 | 0.2034
Epoch 234/300, seasonal_2 Loss: 0.1577 | 0.2034
Epoch 235/300, seasonal_2 Loss: 0.1581 | 0.2034
Epoch 236/300, seasonal_2 Loss: 0.1583 | 0.2034
Epoch 237/300, seasonal_2 Loss: 0.1577 | 0.2033
Epoch 238/300, seasonal_2 Loss: 0.1581 | 0.2033
Epoch 239/300, seasonal_2 Loss: 0.1581 | 0.2033
Epoch 240/300, seasonal_2 Loss: 0.1572 | 0.2033
Epoch 241/300, seasonal_2 Loss: 0.1578 | 0.2034
Epoch 242/300, seasonal_2 Loss: 0.1577 | 0.2033
Epoch 243/300, seasonal_2 Loss: 0.1581 | 0.2033
Epoch 244/300, seasonal_2 Loss: 0.1580 | 0.2033
Epoch 245/300, seasonal_2 Loss: 0.1579 | 0.2033
Epoch 246/300, seasonal_2 Loss: 0.1576 | 0.2033
Epoch 247/300, seasonal_2 Loss: 0.1578 | 0.2033
Epoch 248/300, seasonal_2 Loss: 0.1589 | 0.2033
Epoch 249/300, seasonal_2 Loss: 0.1579 | 0.2033
Epoch 250/300, seasonal_2 Loss: 0.1578 | 0.2033
Epoch 251/300, seasonal_2 Loss: 0.1580 | 0.2033
Epoch 252/300, seasonal_2 Loss: 0.1583 | 0.2033
Epoch 253/300, seasonal_2 Loss: 0.1579 | 0.2033
Epoch 254/300, seasonal_2 Loss: 0.1577 | 0.2032
Epoch 255/300, seasonal_2 Loss: 0.1577 | 0.2032
Epoch 256/300, seasonal_2 Loss: 0.1574 | 0.2032
Epoch 257/300, seasonal_2 Loss: 0.1578 | 0.2032
Epoch 258/300, seasonal_2 Loss: 0.1576 | 0.2032
Epoch 259/300, seasonal_2 Loss: 0.1574 | 0.2032
Epoch 260/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 261/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 262/300, seasonal_2 Loss: 0.1578 | 0.2032
Epoch 263/300, seasonal_2 Loss: 0.1575 | 0.2032
Epoch 264/300, seasonal_2 Loss: 0.1577 | 0.2032
Epoch 265/300, seasonal_2 Loss: 0.1567 | 0.2032
Epoch 266/300, seasonal_2 Loss: 0.1582 | 0.2032
Epoch 267/300, seasonal_2 Loss: 0.1583 | 0.2032
Epoch 268/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 269/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 270/300, seasonal_2 Loss: 0.1573 | 0.2032
Epoch 271/300, seasonal_2 Loss: 0.1587 | 0.2032
Epoch 272/300, seasonal_2 Loss: 0.1581 | 0.2032
Epoch 273/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 274/300, seasonal_2 Loss: 0.1578 | 0.2032
Epoch 275/300, seasonal_2 Loss: 0.1577 | 0.2032
Epoch 276/300, seasonal_2 Loss: 0.1574 | 0.2032
Epoch 277/300, seasonal_2 Loss: 0.1574 | 0.2032
Epoch 278/300, seasonal_2 Loss: 0.1578 | 0.2032
Epoch 279/300, seasonal_2 Loss: 0.1577 | 0.2032
Epoch 280/300, seasonal_2 Loss: 0.1579 | 0.2032
Epoch 281/300, seasonal_2 Loss: 0.1576 | 0.2032
Epoch 282/300, seasonal_2 Loss: 0.1577 | 0.2031
Epoch 283/300, seasonal_2 Loss: 0.1579 | 0.2031
Epoch 284/300, seasonal_2 Loss: 0.1583 | 0.2031
Epoch 285/300, seasonal_2 Loss: 0.1578 | 0.2031
Epoch 286/300, seasonal_2 Loss: 0.1584 | 0.2031
Epoch 287/300, seasonal_2 Loss: 0.1571 | 0.2031
Epoch 288/300, seasonal_2 Loss: 0.1585 | 0.2031
Epoch 289/300, seasonal_2 Loss: 0.1582 | 0.2031
Epoch 290/300, seasonal_2 Loss: 0.1578 | 0.2031
Epoch 291/300, seasonal_2 Loss: 0.1578 | 0.2031
Epoch 292/300, seasonal_2 Loss: 0.1573 | 0.2031
Epoch 293/300, seasonal_2 Loss: 0.1584 | 0.2031
Epoch 294/300, seasonal_2 Loss: 0.1585 | 0.2031
Epoch 295/300, seasonal_2 Loss: 0.1576 | 0.2031
Epoch 296/300, seasonal_2 Loss: 0.1578 | 0.2031
Epoch 297/300, seasonal_2 Loss: 0.1583 | 0.2031
Epoch 298/300, seasonal_2 Loss: 0.1575 | 0.2031
Epoch 299/300, seasonal_2 Loss: 0.1575 | 0.2031
Epoch 300/300, seasonal_2 Loss: 0.1575 | 0.2031
Training seasonal_3 component with params: {'observation_period_num': 15, 'train_rates': 0.9723427543788988, 'learning_rate': 0.00021117898484721565, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9346173841962877}
Epoch 1/300, seasonal_3 Loss: 0.7190 | 1.0408
Epoch 2/300, seasonal_3 Loss: 0.6862 | 0.9927
Epoch 3/300, seasonal_3 Loss: 0.6995 | 0.8746
Epoch 4/300, seasonal_3 Loss: 0.6235 | 0.9030
Epoch 5/300, seasonal_3 Loss: 0.6054 | 0.7922
Epoch 6/300, seasonal_3 Loss: 0.4768 | 0.7113
Epoch 7/300, seasonal_3 Loss: 0.4088 | 0.6859
Epoch 8/300, seasonal_3 Loss: 0.3900 | 0.6280
Epoch 9/300, seasonal_3 Loss: 0.3802 | 0.6115
Epoch 10/300, seasonal_3 Loss: 0.3577 | 0.5587
Epoch 11/300, seasonal_3 Loss: 0.3303 | 0.5456
Epoch 12/300, seasonal_3 Loss: 0.2900 | 0.5105
Epoch 13/300, seasonal_3 Loss: 0.2759 | 0.4861
Epoch 14/300, seasonal_3 Loss: 0.2568 | 0.4536
Epoch 15/300, seasonal_3 Loss: 0.2474 | 0.4359
Epoch 16/300, seasonal_3 Loss: 0.2396 | 0.4179
Epoch 17/300, seasonal_3 Loss: 0.2305 | 0.3990
Epoch 18/300, seasonal_3 Loss: 0.2207 | 0.3912
Epoch 19/300, seasonal_3 Loss: 0.2146 | 0.3743
Epoch 20/300, seasonal_3 Loss: 0.2127 | 0.3603
Epoch 21/300, seasonal_3 Loss: 0.2067 | 0.3564
Epoch 22/300, seasonal_3 Loss: 0.2038 | 0.3631
Epoch 23/300, seasonal_3 Loss: 0.1992 | 0.3330
Epoch 24/300, seasonal_3 Loss: 0.2026 | 0.3234
Epoch 25/300, seasonal_3 Loss: 0.1994 | 0.3526
Epoch 26/300, seasonal_3 Loss: 0.1940 | 0.3106
Epoch 27/300, seasonal_3 Loss: 0.1907 | 0.3052
Epoch 28/300, seasonal_3 Loss: 0.1869 | 0.3095
Epoch 29/300, seasonal_3 Loss: 0.1812 | 0.2893
Epoch 30/300, seasonal_3 Loss: 0.1778 | 0.2994
Epoch 31/300, seasonal_3 Loss: 0.1752 | 0.2824
Epoch 32/300, seasonal_3 Loss: 0.1732 | 0.2792
Epoch 33/300, seasonal_3 Loss: 0.1708 | 0.2835
Epoch 34/300, seasonal_3 Loss: 0.1689 | 0.2738
Epoch 35/300, seasonal_3 Loss: 0.1670 | 0.2731
Epoch 36/300, seasonal_3 Loss: 0.1664 | 0.2694
Epoch 37/300, seasonal_3 Loss: 0.1647 | 0.2675
Epoch 38/300, seasonal_3 Loss: 0.1630 | 0.2647
Epoch 39/300, seasonal_3 Loss: 0.1615 | 0.2622
Epoch 40/300, seasonal_3 Loss: 0.1610 | 0.2601
Epoch 41/300, seasonal_3 Loss: 0.1599 | 0.2591
Epoch 42/300, seasonal_3 Loss: 0.1595 | 0.2578
Epoch 43/300, seasonal_3 Loss: 0.1587 | 0.2571
Epoch 44/300, seasonal_3 Loss: 0.1572 | 0.2560
Epoch 45/300, seasonal_3 Loss: 0.1565 | 0.2532
Epoch 46/300, seasonal_3 Loss: 0.1562 | 0.2519
Epoch 47/300, seasonal_3 Loss: 0.1559 | 0.2508
Epoch 48/300, seasonal_3 Loss: 0.1550 | 0.2504
Epoch 49/300, seasonal_3 Loss: 0.1549 | 0.2486
Epoch 50/300, seasonal_3 Loss: 0.1527 | 0.2473
Epoch 51/300, seasonal_3 Loss: 0.1526 | 0.2464
Epoch 52/300, seasonal_3 Loss: 0.1527 | 0.2467
Epoch 53/300, seasonal_3 Loss: 0.1518 | 0.2454
Epoch 54/300, seasonal_3 Loss: 0.1520 | 0.2444
Epoch 55/300, seasonal_3 Loss: 0.1516 | 0.2440
Epoch 56/300, seasonal_3 Loss: 0.1511 | 0.2429
Epoch 57/300, seasonal_3 Loss: 0.1509 | 0.2417
Epoch 58/300, seasonal_3 Loss: 0.1499 | 0.2417
Epoch 59/300, seasonal_3 Loss: 0.1493 | 0.2410
Epoch 60/300, seasonal_3 Loss: 0.1504 | 0.2400
Epoch 61/300, seasonal_3 Loss: 0.1500 | 0.2395
Epoch 62/300, seasonal_3 Loss: 0.1494 | 0.2399
Epoch 63/300, seasonal_3 Loss: 0.1490 | 0.2390
Epoch 64/300, seasonal_3 Loss: 0.1486 | 0.2391
Epoch 65/300, seasonal_3 Loss: 0.1483 | 0.2387
Epoch 66/300, seasonal_3 Loss: 0.1483 | 0.2384
Epoch 67/300, seasonal_3 Loss: 0.1482 | 0.2380
Epoch 68/300, seasonal_3 Loss: 0.1486 | 0.2380
Epoch 69/300, seasonal_3 Loss: 0.1473 | 0.2375
Epoch 70/300, seasonal_3 Loss: 0.1473 | 0.2371
Epoch 71/300, seasonal_3 Loss: 0.1473 | 0.2371
Epoch 72/300, seasonal_3 Loss: 0.1467 | 0.2364
Epoch 73/300, seasonal_3 Loss: 0.1473 | 0.2366
Epoch 74/300, seasonal_3 Loss: 0.1472 | 0.2362
Epoch 75/300, seasonal_3 Loss: 0.1470 | 0.2356
Epoch 76/300, seasonal_3 Loss: 0.1471 | 0.2354
Epoch 77/300, seasonal_3 Loss: 0.1465 | 0.2357
Epoch 78/300, seasonal_3 Loss: 0.1468 | 0.2358
Epoch 79/300, seasonal_3 Loss: 0.1464 | 0.2359
Epoch 80/300, seasonal_3 Loss: 0.1463 | 0.2356
Epoch 81/300, seasonal_3 Loss: 0.1467 | 0.2353
Epoch 82/300, seasonal_3 Loss: 0.1456 | 0.2352
Epoch 83/300, seasonal_3 Loss: 0.1459 | 0.2351
Epoch 84/300, seasonal_3 Loss: 0.1457 | 0.2350
Epoch 85/300, seasonal_3 Loss: 0.1454 | 0.2349
Epoch 86/300, seasonal_3 Loss: 0.1467 | 0.2347
Epoch 87/300, seasonal_3 Loss: 0.1468 | 0.2349
Epoch 88/300, seasonal_3 Loss: 0.1463 | 0.2347
Epoch 89/300, seasonal_3 Loss: 0.1457 | 0.2345
Epoch 90/300, seasonal_3 Loss: 0.1457 | 0.2346
Epoch 91/300, seasonal_3 Loss: 0.1452 | 0.2346
Epoch 92/300, seasonal_3 Loss: 0.1456 | 0.2345
Epoch 93/300, seasonal_3 Loss: 0.1465 | 0.2345
Epoch 94/300, seasonal_3 Loss: 0.1460 | 0.2344
Epoch 95/300, seasonal_3 Loss: 0.1452 | 0.2343
Epoch 96/300, seasonal_3 Loss: 0.1457 | 0.2343
Epoch 97/300, seasonal_3 Loss: 0.1461 | 0.2343
Epoch 98/300, seasonal_3 Loss: 0.1462 | 0.2343
Epoch 99/300, seasonal_3 Loss: 0.1446 | 0.2343
Epoch 100/300, seasonal_3 Loss: 0.1456 | 0.2341
Epoch 101/300, seasonal_3 Loss: 0.1456 | 0.2341
Epoch 102/300, seasonal_3 Loss: 0.1457 | 0.2341
Epoch 103/300, seasonal_3 Loss: 0.1444 | 0.2340
Epoch 104/300, seasonal_3 Loss: 0.1450 | 0.2340
Epoch 105/300, seasonal_3 Loss: 0.1457 | 0.2340
Epoch 106/300, seasonal_3 Loss: 0.1458 | 0.2340
Epoch 107/300, seasonal_3 Loss: 0.1452 | 0.2339
Epoch 108/300, seasonal_3 Loss: 0.1444 | 0.2338
Epoch 109/300, seasonal_3 Loss: 0.1458 | 0.2338
Epoch 110/300, seasonal_3 Loss: 0.1457 | 0.2338
Epoch 111/300, seasonal_3 Loss: 0.1448 | 0.2338
Epoch 112/300, seasonal_3 Loss: 0.1453 | 0.2338
Epoch 113/300, seasonal_3 Loss: 0.1451 | 0.2338
Epoch 114/300, seasonal_3 Loss: 0.1450 | 0.2338
Epoch 115/300, seasonal_3 Loss: 0.1448 | 0.2338
Epoch 116/300, seasonal_3 Loss: 0.1446 | 0.2337
Epoch 117/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 118/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 119/300, seasonal_3 Loss: 0.1459 | 0.2337
Epoch 120/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 121/300, seasonal_3 Loss: 0.1444 | 0.2337
Epoch 122/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 123/300, seasonal_3 Loss: 0.1451 | 0.2337
Epoch 124/300, seasonal_3 Loss: 0.1454 | 0.2337
Epoch 125/300, seasonal_3 Loss: 0.1461 | 0.2337
Epoch 126/300, seasonal_3 Loss: 0.1458 | 0.2337
Epoch 127/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 128/300, seasonal_3 Loss: 0.1460 | 0.2337
Epoch 129/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 130/300, seasonal_3 Loss: 0.1458 | 0.2337
Epoch 131/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 132/300, seasonal_3 Loss: 0.1448 | 0.2337
Epoch 133/300, seasonal_3 Loss: 0.1456 | 0.2337
Epoch 134/300, seasonal_3 Loss: 0.1452 | 0.2337
Epoch 135/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 136/300, seasonal_3 Loss: 0.1454 | 0.2337
Epoch 137/300, seasonal_3 Loss: 0.1447 | 0.2337
Epoch 138/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 139/300, seasonal_3 Loss: 0.1458 | 0.2337
Epoch 140/300, seasonal_3 Loss: 0.1446 | 0.2337
Epoch 141/300, seasonal_3 Loss: 0.1452 | 0.2337
Epoch 142/300, seasonal_3 Loss: 0.1457 | 0.2337
Epoch 143/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 144/300, seasonal_3 Loss: 0.1457 | 0.2337
Epoch 145/300, seasonal_3 Loss: 0.1447 | 0.2337
Epoch 146/300, seasonal_3 Loss: 0.1457 | 0.2337
Epoch 147/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 148/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 149/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 150/300, seasonal_3 Loss: 0.1446 | 0.2337
Epoch 151/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 152/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 153/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 154/300, seasonal_3 Loss: 0.1445 | 0.2337
Epoch 155/300, seasonal_3 Loss: 0.1452 | 0.2337
Epoch 156/300, seasonal_3 Loss: 0.1451 | 0.2337
Epoch 157/300, seasonal_3 Loss: 0.1446 | 0.2337
Epoch 158/300, seasonal_3 Loss: 0.1451 | 0.2337
Epoch 159/300, seasonal_3 Loss: 0.1447 | 0.2337
Epoch 160/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 161/300, seasonal_3 Loss: 0.1449 | 0.2337
Epoch 162/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 163/300, seasonal_3 Loss: 0.1458 | 0.2337
Epoch 164/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 165/300, seasonal_3 Loss: 0.1451 | 0.2337
Epoch 166/300, seasonal_3 Loss: 0.1442 | 0.2337
Epoch 167/300, seasonal_3 Loss: 0.1448 | 0.2337
Epoch 168/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 169/300, seasonal_3 Loss: 0.1452 | 0.2337
Epoch 170/300, seasonal_3 Loss: 0.1455 | 0.2337
Epoch 171/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 172/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 173/300, seasonal_3 Loss: 0.1449 | 0.2337
Epoch 174/300, seasonal_3 Loss: 0.1457 | 0.2337
Epoch 175/300, seasonal_3 Loss: 0.1462 | 0.2337
Epoch 176/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 177/300, seasonal_3 Loss: 0.1461 | 0.2337
Epoch 178/300, seasonal_3 Loss: 0.1449 | 0.2337
Epoch 179/300, seasonal_3 Loss: 0.1447 | 0.2337
Epoch 180/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 181/300, seasonal_3 Loss: 0.1452 | 0.2337
Epoch 182/300, seasonal_3 Loss: 0.1448 | 0.2337
Epoch 183/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 184/300, seasonal_3 Loss: 0.1457 | 0.2337
Epoch 185/300, seasonal_3 Loss: 0.1453 | 0.2337
Epoch 186/300, seasonal_3 Loss: 0.1458 | 0.2337
Epoch 187/300, seasonal_3 Loss: 0.1450 | 0.2337
Epoch 188/300, seasonal_3 Loss: 0.1461 | 0.2337
Epoch 189/300, seasonal_3 Loss: 0.1448 | 0.2337
Epoch 190/300, seasonal_3 Loss: 0.1460 | 0.2337
Epoch 191/300, seasonal_3 Loss: 0.1448 | 0.2337
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 196, 'train_rates': 0.9758353278676509, 'learning_rate': 0.0003166290349744031, 'batch_size': 160, 'step_size': 8, 'gamma': 0.7899694352350037}
Epoch 1/300, resid Loss: 1.0449 | 2.0700
Epoch 2/300, resid Loss: 0.7840 | 1.2595
Epoch 3/300, resid Loss: 0.6415 | 1.1577
Epoch 4/300, resid Loss: 0.6781 | 1.0032
Epoch 5/300, resid Loss: 0.6885 | 1.0054
Epoch 6/300, resid Loss: 0.5424 | 0.9186
Epoch 7/300, resid Loss: 0.4551 | 0.8595
Epoch 8/300, resid Loss: 0.4025 | 0.8039
Epoch 9/300, resid Loss: 0.3817 | 0.7554
Epoch 10/300, resid Loss: 0.3631 | 0.7395
Epoch 11/300, resid Loss: 0.3558 | 0.6929
Epoch 12/300, resid Loss: 0.3614 | 0.6652
Epoch 13/300, resid Loss: 0.3343 | 0.6298
Epoch 14/300, resid Loss: 0.3075 | 0.6210
Epoch 15/300, resid Loss: 0.3033 | 0.5927
Epoch 16/300, resid Loss: 0.3397 | 0.5731
Epoch 17/300, resid Loss: 0.2948 | 0.5752
Epoch 18/300, resid Loss: 0.2849 | 0.5490
Epoch 19/300, resid Loss: 0.2705 | 0.5343
Epoch 20/300, resid Loss: 0.2624 | 0.5167
Epoch 21/300, resid Loss: 0.2546 | 0.5086
Epoch 22/300, resid Loss: 0.2493 | 0.4951
Epoch 23/300, resid Loss: 0.2440 | 0.4862
Epoch 24/300, resid Loss: 0.2406 | 0.4736
Epoch 25/300, resid Loss: 0.2366 | 0.4675
Epoch 26/300, resid Loss: 0.2338 | 0.4579
Epoch 27/300, resid Loss: 0.2311 | 0.4514
Epoch 28/300, resid Loss: 0.2288 | 0.4442
Epoch 29/300, resid Loss: 0.2264 | 0.4388
Epoch 30/300, resid Loss: 0.2242 | 0.4346
Epoch 31/300, resid Loss: 0.2221 | 0.4276
Epoch 32/300, resid Loss: 0.2199 | 0.4234
Epoch 33/300, resid Loss: 0.2186 | 0.4189
Epoch 34/300, resid Loss: 0.2175 | 0.4145
Epoch 35/300, resid Loss: 0.2156 | 0.4106
Epoch 36/300, resid Loss: 0.2144 | 0.4071
Epoch 37/300, resid Loss: 0.2135 | 0.4042
Epoch 38/300, resid Loss: 0.2127 | 0.4010
Epoch 39/300, resid Loss: 0.2109 | 0.3980
Epoch 40/300, resid Loss: 0.2099 | 0.3949
Epoch 41/300, resid Loss: 0.2084 | 0.3930
Epoch 42/300, resid Loss: 0.2084 | 0.3908
Epoch 43/300, resid Loss: 0.2077 | 0.3884
Epoch 44/300, resid Loss: 0.2059 | 0.3859
Epoch 45/300, resid Loss: 0.2057 | 0.3838
Epoch 46/300, resid Loss: 0.2049 | 0.3823
Epoch 47/300, resid Loss: 0.2040 | 0.3805
Epoch 48/300, resid Loss: 0.2036 | 0.3787
Epoch 49/300, resid Loss: 0.2029 | 0.3770
Epoch 50/300, resid Loss: 0.2023 | 0.3753
Epoch 51/300, resid Loss: 0.2022 | 0.3744
Epoch 52/300, resid Loss: 0.2016 | 0.3727
Epoch 53/300, resid Loss: 0.2015 | 0.3714
Epoch 54/300, resid Loss: 0.2010 | 0.3707
Epoch 55/300, resid Loss: 0.2004 | 0.3695
Epoch 56/300, resid Loss: 0.2002 | 0.3681
Epoch 57/300, resid Loss: 0.2000 | 0.3670
Epoch 58/300, resid Loss: 0.1990 | 0.3663
Epoch 59/300, resid Loss: 0.1993 | 0.3655
Epoch 60/300, resid Loss: 0.1997 | 0.3646
Epoch 61/300, resid Loss: 0.1986 | 0.3639
Epoch 62/300, resid Loss: 0.1979 | 0.3634
Epoch 63/300, resid Loss: 0.1983 | 0.3627
Epoch 64/300, resid Loss: 0.1973 | 0.3618
Epoch 65/300, resid Loss: 0.1970 | 0.3611
Epoch 66/300, resid Loss: 0.1970 | 0.3604
Epoch 67/300, resid Loss: 0.1968 | 0.3599
Epoch 68/300, resid Loss: 0.1963 | 0.3597
Epoch 69/300, resid Loss: 0.1970 | 0.3592
Epoch 70/300, resid Loss: 0.1968 | 0.3586
Epoch 71/300, resid Loss: 0.1962 | 0.3579
Epoch 72/300, resid Loss: 0.1954 | 0.3575
Epoch 73/300, resid Loss: 0.1958 | 0.3572
Epoch 74/300, resid Loss: 0.1951 | 0.3568
Epoch 75/300, resid Loss: 0.1951 | 0.3566
Epoch 76/300, resid Loss: 0.1954 | 0.3562
Epoch 77/300, resid Loss: 0.1954 | 0.3559
Epoch 78/300, resid Loss: 0.1956 | 0.3554
Epoch 79/300, resid Loss: 0.1958 | 0.3551
Epoch 80/300, resid Loss: 0.1954 | 0.3550
Epoch 81/300, resid Loss: 0.1951 | 0.3548
Epoch 82/300, resid Loss: 0.1949 | 0.3545
Epoch 83/300, resid Loss: 0.1953 | 0.3542
Epoch 84/300, resid Loss: 0.1945 | 0.3540
Epoch 85/300, resid Loss: 0.1946 | 0.3539
Epoch 86/300, resid Loss: 0.1943 | 0.3536
Epoch 87/300, resid Loss: 0.1944 | 0.3534
Epoch 88/300, resid Loss: 0.1945 | 0.3531
Epoch 89/300, resid Loss: 0.1942 | 0.3530
Epoch 90/300, resid Loss: 0.1947 | 0.3527
Epoch 91/300, resid Loss: 0.1940 | 0.3526
Epoch 92/300, resid Loss: 0.1944 | 0.3525
Epoch 93/300, resid Loss: 0.1940 | 0.3524
Epoch 94/300, resid Loss: 0.1941 | 0.3524
Epoch 95/300, resid Loss: 0.1942 | 0.3522
Epoch 96/300, resid Loss: 0.1944 | 0.3521
Epoch 97/300, resid Loss: 0.1940 | 0.3520
Epoch 98/300, resid Loss: 0.1943 | 0.3519
Epoch 99/300, resid Loss: 0.1936 | 0.3518
Epoch 100/300, resid Loss: 0.1940 | 0.3516
Epoch 101/300, resid Loss: 0.1938 | 0.3515
Epoch 102/300, resid Loss: 0.1933 | 0.3515
Epoch 103/300, resid Loss: 0.1938 | 0.3513
Epoch 104/300, resid Loss: 0.1940 | 0.3512
Epoch 105/300, resid Loss: 0.1935 | 0.3512
Epoch 106/300, resid Loss: 0.1939 | 0.3512
Epoch 107/300, resid Loss: 0.1937 | 0.3512
Epoch 108/300, resid Loss: 0.1932 | 0.3511
Epoch 109/300, resid Loss: 0.1933 | 0.3511
Epoch 110/300, resid Loss: 0.1939 | 0.3511
Epoch 111/300, resid Loss: 0.1933 | 0.3510
Epoch 112/300, resid Loss: 0.1933 | 0.3509
Epoch 113/300, resid Loss: 0.1929 | 0.3509
Epoch 114/300, resid Loss: 0.1932 | 0.3508
Epoch 115/300, resid Loss: 0.1927 | 0.3507
Epoch 116/300, resid Loss: 0.1926 | 0.3507
Epoch 117/300, resid Loss: 0.1936 | 0.3507
Epoch 118/300, resid Loss: 0.1932 | 0.3507
Epoch 119/300, resid Loss: 0.1935 | 0.3506
Epoch 120/300, resid Loss: 0.1930 | 0.3506
Epoch 121/300, resid Loss: 0.1932 | 0.3506
Epoch 122/300, resid Loss: 0.1936 | 0.3505
Epoch 123/300, resid Loss: 0.1937 | 0.3505
Epoch 124/300, resid Loss: 0.1935 | 0.3505
Epoch 125/300, resid Loss: 0.1933 | 0.3505
Epoch 126/300, resid Loss: 0.1928 | 0.3504
Epoch 127/300, resid Loss: 0.1941 | 0.3504
Epoch 128/300, resid Loss: 0.1936 | 0.3504
Epoch 129/300, resid Loss: 0.1935 | 0.3504
Epoch 130/300, resid Loss: 0.1932 | 0.3503
Epoch 131/300, resid Loss: 0.1934 | 0.3503
Epoch 132/300, resid Loss: 0.1933 | 0.3503
Epoch 133/300, resid Loss: 0.1934 | 0.3503
Epoch 134/300, resid Loss: 0.1939 | 0.3503
Epoch 135/300, resid Loss: 0.1933 | 0.3503
Epoch 136/300, resid Loss: 0.1936 | 0.3503
Epoch 137/300, resid Loss: 0.1936 | 0.3503
Epoch 138/300, resid Loss: 0.1928 | 0.3503
Epoch 139/300, resid Loss: 0.1929 | 0.3502
Epoch 140/300, resid Loss: 0.1929 | 0.3502
Epoch 141/300, resid Loss: 0.1931 | 0.3502
Epoch 142/300, resid Loss: 0.1925 | 0.3502
Epoch 143/300, resid Loss: 0.1933 | 0.3502
Epoch 144/300, resid Loss: 0.1931 | 0.3502
Epoch 145/300, resid Loss: 0.1942 | 0.3502
Epoch 146/300, resid Loss: 0.1933 | 0.3502
Epoch 147/300, resid Loss: 0.1934 | 0.3502
Epoch 148/300, resid Loss: 0.1931 | 0.3502
Epoch 149/300, resid Loss: 0.1931 | 0.3502
Epoch 150/300, resid Loss: 0.1934 | 0.3501
Epoch 151/300, resid Loss: 0.1928 | 0.3501
Epoch 152/300, resid Loss: 0.1940 | 0.3501
Epoch 153/300, resid Loss: 0.1932 | 0.3501
Epoch 154/300, resid Loss: 0.1936 | 0.3501
Epoch 155/300, resid Loss: 0.1931 | 0.3501
Epoch 156/300, resid Loss: 0.1933 | 0.3501
Epoch 157/300, resid Loss: 0.1921 | 0.3501
Epoch 158/300, resid Loss: 0.1943 | 0.3501
Epoch 159/300, resid Loss: 0.1930 | 0.3501
Epoch 160/300, resid Loss: 0.1938 | 0.3501
Epoch 161/300, resid Loss: 0.1922 | 0.3501
Epoch 162/300, resid Loss: 0.1931 | 0.3501
Epoch 163/300, resid Loss: 0.1930 | 0.3501
Epoch 164/300, resid Loss: 0.1929 | 0.3501
Epoch 165/300, resid Loss: 0.1930 | 0.3501
Epoch 166/300, resid Loss: 0.1929 | 0.3501
Epoch 167/300, resid Loss: 0.1926 | 0.3501
Epoch 168/300, resid Loss: 0.1940 | 0.3501
Epoch 169/300, resid Loss: 0.1932 | 0.3501
Epoch 170/300, resid Loss: 0.1935 | 0.3501
Epoch 171/300, resid Loss: 0.1932 | 0.3501
Epoch 172/300, resid Loss: 0.1929 | 0.3501
Epoch 173/300, resid Loss: 0.1930 | 0.3501
Epoch 174/300, resid Loss: 0.1933 | 0.3501
Epoch 175/300, resid Loss: 0.1933 | 0.3501
Epoch 176/300, resid Loss: 0.1928 | 0.3501
Epoch 177/300, resid Loss: 0.1931 | 0.3501
Epoch 178/300, resid Loss: 0.1940 | 0.3501
Epoch 179/300, resid Loss: 0.1937 | 0.3501
Epoch 180/300, resid Loss: 0.1935 | 0.3501
Epoch 181/300, resid Loss: 0.1927 | 0.3501
Epoch 182/300, resid Loss: 0.1933 | 0.3501
Epoch 183/300, resid Loss: 0.1928 | 0.3501
Epoch 184/300, resid Loss: 0.1939 | 0.3501
Epoch 185/300, resid Loss: 0.1934 | 0.3501
Epoch 186/300, resid Loss: 0.1937 | 0.3501
Epoch 187/300, resid Loss: 0.1936 | 0.3501
Epoch 188/300, resid Loss: 0.1932 | 0.3501
Epoch 189/300, resid Loss: 0.1938 | 0.3501
Epoch 190/300, resid Loss: 0.1940 | 0.3501
Epoch 191/300, resid Loss: 0.1939 | 0.3501
Epoch 192/300, resid Loss: 0.1934 | 0.3501
Epoch 193/300, resid Loss: 0.1939 | 0.3501
Epoch 194/300, resid Loss: 0.1930 | 0.3501
Early stopping for resid
Runtime (seconds): 2441.4489641189575
4.180505174341226e-05
[141.86761]
[-1.2730484]
[-5.072345]
[6.2783]
[1.9348388]
[4.763481]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 475.14965883479454
RMSE: 21.797927856445312
MAE: 21.797927856445312
R-squared: nan
[148.49887]
