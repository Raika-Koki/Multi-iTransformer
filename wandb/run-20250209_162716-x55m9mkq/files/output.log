[32m[I 2025-02-09 16:27:17,346][0m A new study created in memory with name: no-name-3c7ac774-9cf2-4417-aa1d-9ba939ed0784[0m
[32m[I 2025-02-09 16:31:56,294][0m Trial 0 finished with value: 0.14787664715502713 and parameters: {'observation_period_num': 7, 'train_rates': 0.7529287370325064, 'learning_rate': 6.704361760143287e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8165612500054014}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:32:33,066][0m Trial 1 finished with value: 0.3156325987134224 and parameters: {'observation_period_num': 76, 'train_rates': 0.8401166323053233, 'learning_rate': 9.838950133236343e-06, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9127757152584082}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:34:05,800][0m Trial 2 finished with value: 0.4455861156339088 and parameters: {'observation_period_num': 218, 'train_rates': 0.6862614812676362, 'learning_rate': 2.5069985057800693e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.803322229854642}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:34:26,929][0m Trial 3 finished with value: 0.8468487558878056 and parameters: {'observation_period_num': 42, 'train_rates': 0.6232447028483474, 'learning_rate': 1.5179014016229318e-06, 'batch_size': 246, 'step_size': 4, 'gamma': 0.9564426119747494}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:36:59,148][0m Trial 4 finished with value: 0.40635124096210967 and parameters: {'observation_period_num': 224, 'train_rates': 0.8309550719414143, 'learning_rate': 3.0410022886784946e-06, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8590460761951593}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:37:38,532][0m Trial 5 finished with value: 0.4083815813064575 and parameters: {'observation_period_num': 104, 'train_rates': 0.9552466948021727, 'learning_rate': 1.6077062850920834e-05, 'batch_size': 155, 'step_size': 13, 'gamma': 0.7899713609045365}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:38:23,278][0m Trial 6 finished with value: 0.20148007641510662 and parameters: {'observation_period_num': 58, 'train_rates': 0.7790429280872279, 'learning_rate': 0.0006382114618186415, 'batch_size': 122, 'step_size': 9, 'gamma': 0.8654120915055272}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:39:02,902][0m Trial 7 finished with value: 0.2714153528213501 and parameters: {'observation_period_num': 81, 'train_rates': 0.9569955693900218, 'learning_rate': 0.00013244977352842713, 'batch_size': 156, 'step_size': 3, 'gamma': 0.7833285561972491}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:39:30,329][0m Trial 8 finished with value: 0.1608030866635473 and parameters: {'observation_period_num': 137, 'train_rates': 0.8808873404121781, 'learning_rate': 0.00016619173026668883, 'batch_size': 225, 'step_size': 6, 'gamma': 0.8728042070882178}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:40:10,813][0m Trial 9 finished with value: 0.7125296790133878 and parameters: {'observation_period_num': 56, 'train_rates': 0.8663430486097515, 'learning_rate': 1.0551812053963402e-05, 'batch_size': 145, 'step_size': 1, 'gamma': 0.8929212994340753}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:41:13,159][0m Trial 10 finished with value: 0.2379716252811098 and parameters: {'observation_period_num': 154, 'train_rates': 0.7304338928599509, 'learning_rate': 9.217475807194417e-05, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7565266803930661}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:41:38,508][0m Trial 11 finished with value: 0.17109295891390908 and parameters: {'observation_period_num': 156, 'train_rates': 0.8894686890286053, 'learning_rate': 0.00017657246171472153, 'batch_size': 245, 'step_size': 6, 'gamma': 0.834088672875319}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:42:05,668][0m Trial 12 finished with value: 0.23615275720470613 and parameters: {'observation_period_num': 135, 'train_rates': 0.756390295236343, 'learning_rate': 0.0007799274437017788, 'batch_size': 208, 'step_size': 7, 'gamma': 0.8305676637700895}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:43:03,639][0m Trial 13 finished with value: 0.1651492068288373 and parameters: {'observation_period_num': 183, 'train_rates': 0.9098083819986291, 'learning_rate': 5.855577799025233e-05, 'batch_size': 97, 'step_size': 11, 'gamma': 0.9305813823737482}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:43:28,734][0m Trial 14 finished with value: 0.3188163823372609 and parameters: {'observation_period_num': 109, 'train_rates': 0.7056642226366608, 'learning_rate': 0.00029123414545829604, 'batch_size': 207, 'step_size': 6, 'gamma': 0.9822833451040558}. Best is trial 0 with value: 0.14787664715502713.[0m
[32m[I 2025-02-09 16:43:59,262][0m Trial 15 finished with value: 0.07824988569110734 and parameters: {'observation_period_num': 13, 'train_rates': 0.8041414042378952, 'learning_rate': 5.726778340382843e-05, 'batch_size': 199, 'step_size': 7, 'gamma': 0.8321796043919198}. Best is trial 15 with value: 0.07824988569110734.[0m
[32m[I 2025-02-09 16:49:05,730][0m Trial 16 finished with value: 0.032951470000379894 and parameters: {'observation_period_num': 12, 'train_rates': 0.8019332243500439, 'learning_rate': 4.251700827713319e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8235800358540518}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:49:36,704][0m Trial 17 finished with value: 0.06733730886531124 and parameters: {'observation_period_num': 6, 'train_rates': 0.8072770450989287, 'learning_rate': 3.7597512095841e-05, 'batch_size': 186, 'step_size': 13, 'gamma': 0.7682879444787717}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:50:05,269][0m Trial 18 finished with value: 0.3943003048509184 and parameters: {'observation_period_num': 31, 'train_rates': 0.6571252607830262, 'learning_rate': 4.699196972063409e-06, 'batch_size': 181, 'step_size': 13, 'gamma': 0.7582800460735935}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:50:55,450][0m Trial 19 finished with value: 0.07610497979191959 and parameters: {'observation_period_num': 27, 'train_rates': 0.8015341840370255, 'learning_rate': 2.678755872039019e-05, 'batch_size': 114, 'step_size': 15, 'gamma': 0.7769295691009437}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:52:24,381][0m Trial 20 finished with value: 0.2649412751197815 and parameters: {'observation_period_num': 252, 'train_rates': 0.9871862384365611, 'learning_rate': 0.00035408057620950264, 'batch_size': 65, 'step_size': 12, 'gamma': 0.7507745543959097}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:53:12,807][0m Trial 21 finished with value: 0.06801763897652888 and parameters: {'observation_period_num': 26, 'train_rates': 0.8043333988451354, 'learning_rate': 3.205363548946053e-05, 'batch_size': 115, 'step_size': 15, 'gamma': 0.7823399813522918}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:53:46,489][0m Trial 22 finished with value: 0.06451416735426854 and parameters: {'observation_period_num': 6, 'train_rates': 0.8227583703751282, 'learning_rate': 3.8279677454033515e-05, 'batch_size': 186, 'step_size': 14, 'gamma': 0.8024455091885204}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:54:18,450][0m Trial 23 finished with value: 0.10957504744673598 and parameters: {'observation_period_num': 47, 'train_rates': 0.8431522717664393, 'learning_rate': 3.165653911223474e-05, 'batch_size': 183, 'step_size': 13, 'gamma': 0.8072321849793485}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:54:50,874][0m Trial 24 finished with value: 0.28346638962852233 and parameters: {'observation_period_num': 11, 'train_rates': 0.7793938161594265, 'learning_rate': 1.1034050043583658e-05, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8481682664890737}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:55:17,170][0m Trial 25 finished with value: 0.12463667946289125 and parameters: {'observation_period_num': 76, 'train_rates': 0.8322528689527167, 'learning_rate': 4.3137989914896023e-05, 'batch_size': 224, 'step_size': 14, 'gamma': 0.8074565325873724}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:55:48,441][0m Trial 26 finished with value: 0.21838162722212545 and parameters: {'observation_period_num': 8, 'train_rates': 0.7323293282881314, 'learning_rate': 1.806819255541834e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.7747036688021985}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:56:33,415][0m Trial 27 finished with value: 0.25185722652697723 and parameters: {'observation_period_num': 34, 'train_rates': 0.8996796582866996, 'learning_rate': 6.2569969863862736e-06, 'batch_size': 138, 'step_size': 14, 'gamma': 0.820570734296742}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 16:57:33,328][0m Trial 28 finished with value: 0.06174485708398772 and parameters: {'observation_period_num': 60, 'train_rates': 0.8619235899162602, 'learning_rate': 9.792336149293483e-05, 'batch_size': 96, 'step_size': 11, 'gamma': 0.7960951968464798}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:02:30,414][0m Trial 29 finished with value: 0.16365806829270482 and parameters: {'observation_period_num': 98, 'train_rates': 0.8661449716545656, 'learning_rate': 9.782842830608417e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.7955329036389119}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:03:41,708][0m Trial 30 finished with value: 0.06682386444728883 and parameters: {'observation_period_num': 58, 'train_rates': 0.9171009091445432, 'learning_rate': 8.390946066388665e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8455484478086264}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:04:48,146][0m Trial 31 finished with value: 0.07297481075998699 and parameters: {'observation_period_num': 59, 'train_rates': 0.9290245010831232, 'learning_rate': 8.391863217854415e-05, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8459756028768425}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:06:26,358][0m Trial 32 finished with value: 0.14623420811318733 and parameters: {'observation_period_num': 68, 'train_rates': 0.9214727613763641, 'learning_rate': 0.00024040730801829714, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8186781378691509}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:08:18,222][0m Trial 33 finished with value: 0.09814629197682974 and parameters: {'observation_period_num': 86, 'train_rates': 0.8545763434008122, 'learning_rate': 5.8505452148592916e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.8818983469232283}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:09:15,942][0m Trial 34 finished with value: 0.12795482668465746 and parameters: {'observation_period_num': 44, 'train_rates': 0.8315286455685272, 'learning_rate': 1.8732849977642462e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8467679976913645}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:14:23,985][0m Trial 35 finished with value: 0.04013767966041059 and parameters: {'observation_period_num': 17, 'train_rates': 0.9447387514144379, 'learning_rate': 8.94173201699703e-05, 'batch_size': 19, 'step_size': 14, 'gamma': 0.7977245253369699}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:19:38,531][0m Trial 36 finished with value: 0.19471997506706054 and parameters: {'observation_period_num': 23, 'train_rates': 0.7622895252377426, 'learning_rate': 0.000466070459713195, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7985594550587185}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:22:38,095][0m Trial 37 finished with value: 0.04096405522432178 and parameters: {'observation_period_num': 22, 'train_rates': 0.9410324535896326, 'learning_rate': 0.00011812276461844398, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8098433042234853}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:25:37,261][0m Trial 38 finished with value: 0.052355746179819106 and parameters: {'observation_period_num': 41, 'train_rates': 0.949585660837804, 'learning_rate': 0.0001540330223350382, 'batch_size': 33, 'step_size': 12, 'gamma': 0.8168780953659176}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:28:25,324][0m Trial 39 finished with value: 0.052256837322869724 and parameters: {'observation_period_num': 36, 'train_rates': 0.9542342078177602, 'learning_rate': 0.0001439432798766486, 'batch_size': 35, 'step_size': 13, 'gamma': 0.817993593372836}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:31:30,320][0m Trial 40 finished with value: 0.0684752873411136 and parameters: {'observation_period_num': 19, 'train_rates': 0.9813411457047932, 'learning_rate': 0.00021615409604233102, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9034281956669132}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:34:37,047][0m Trial 41 finished with value: 0.09189948121706645 and parameters: {'observation_period_num': 41, 'train_rates': 0.949465448509274, 'learning_rate': 0.0004081779140810796, 'batch_size': 32, 'step_size': 12, 'gamma': 0.8173455790028362}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:36:50,961][0m Trial 42 finished with value: 0.12049958416393824 and parameters: {'observation_period_num': 38, 'train_rates': 0.9412412664949581, 'learning_rate': 0.00013041470561658705, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8597187230569586}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:40:42,463][0m Trial 43 finished with value: 0.2840546391745831 and parameters: {'observation_period_num': 21, 'train_rates': 0.9687063259650691, 'learning_rate': 1.0764325343797774e-06, 'batch_size': 26, 'step_size': 14, 'gamma': 0.826842069027204}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:42:46,683][0m Trial 44 finished with value: 0.05647199905110062 and parameters: {'observation_period_num': 46, 'train_rates': 0.9632241860736136, 'learning_rate': 0.00014986915141202502, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8116988644618556}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:44:20,447][0m Trial 45 finished with value: 0.0978014023307173 and parameters: {'observation_period_num': 70, 'train_rates': 0.9362570071699727, 'learning_rate': 0.00011087121592013238, 'batch_size': 63, 'step_size': 12, 'gamma': 0.7906506209803819}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:46:43,268][0m Trial 46 finished with value: 0.20905917039146935 and parameters: {'observation_period_num': 116, 'train_rates': 0.8901454561476729, 'learning_rate': 0.00017366791602893333, 'batch_size': 39, 'step_size': 15, 'gamma': 0.8357852507137827}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:49:41,147][0m Trial 47 finished with value: 0.19764498437120911 and parameters: {'observation_period_num': 94, 'train_rates': 0.61425154785123, 'learning_rate': 0.00022870707111857105, 'batch_size': 24, 'step_size': 9, 'gamma': 0.7856908894292787}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:51:31,783][0m Trial 48 finished with value: 0.04981285992056825 and parameters: {'observation_period_num': 18, 'train_rates': 0.9713223800600096, 'learning_rate': 6.177248425259682e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7666070028507542}. Best is trial 16 with value: 0.032951470000379894.[0m
[32m[I 2025-02-09 17:53:31,528][0m Trial 49 finished with value: 0.046818962024355475 and parameters: {'observation_period_num': 17, 'train_rates': 0.9707249604348537, 'learning_rate': 7.042178807927744e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.7678600026241568}. Best is trial 16 with value: 0.032951470000379894.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2016 | 0.1675
Epoch 2/300, Loss: 0.1340 | 0.1200
Epoch 3/300, Loss: 0.1205 | 0.0929
Epoch 4/300, Loss: 0.1137 | 0.0802
Epoch 5/300, Loss: 0.1095 | 0.0747
Epoch 6/300, Loss: 0.1062 | 0.0733
Epoch 7/300, Loss: 0.1032 | 0.0724
Epoch 8/300, Loss: 0.1010 | 0.0742
Epoch 9/300, Loss: 0.0990 | 0.0765
Epoch 10/300, Loss: 0.0972 | 0.0783
Epoch 11/300, Loss: 0.0953 | 0.0793
Epoch 12/300, Loss: 0.0936 | 0.0793
Epoch 13/300, Loss: 0.0917 | 0.0766
Epoch 14/300, Loss: 0.0903 | 0.0759
Epoch 15/300, Loss: 0.0890 | 0.0747
Epoch 16/300, Loss: 0.0878 | 0.0733
Epoch 17/300, Loss: 0.0867 | 0.0716
Epoch 18/300, Loss: 0.0856 | 0.0699
Epoch 19/300, Loss: 0.0844 | 0.0650
Epoch 20/300, Loss: 0.0837 | 0.0637
Epoch 21/300, Loss: 0.0830 | 0.0620
Epoch 22/300, Loss: 0.0823 | 0.0604
Epoch 23/300, Loss: 0.0818 | 0.0588
Epoch 24/300, Loss: 0.0812 | 0.0574
Epoch 25/300, Loss: 0.0805 | 0.0542
Epoch 26/300, Loss: 0.0802 | 0.0533
Epoch 27/300, Loss: 0.0798 | 0.0526
Epoch 28/300, Loss: 0.0794 | 0.0519
Epoch 29/300, Loss: 0.0791 | 0.0512
Epoch 30/300, Loss: 0.0787 | 0.0506
Epoch 31/300, Loss: 0.0783 | 0.0484
Epoch 32/300, Loss: 0.0781 | 0.0480
Epoch 33/300, Loss: 0.0778 | 0.0477
Epoch 34/300, Loss: 0.0775 | 0.0473
Epoch 35/300, Loss: 0.0772 | 0.0470
Epoch 36/300, Loss: 0.0769 | 0.0467
Epoch 37/300, Loss: 0.0766 | 0.0453
Epoch 38/300, Loss: 0.0765 | 0.0451
Epoch 39/300, Loss: 0.0762 | 0.0449
Epoch 40/300, Loss: 0.0759 | 0.0447
Epoch 41/300, Loss: 0.0756 | 0.0444
Epoch 42/300, Loss: 0.0754 | 0.0442
Epoch 43/300, Loss: 0.0752 | 0.0430
Epoch 44/300, Loss: 0.0750 | 0.0428
Epoch 45/300, Loss: 0.0748 | 0.0426
Epoch 46/300, Loss: 0.0745 | 0.0424
Epoch 47/300, Loss: 0.0743 | 0.0421
Epoch 48/300, Loss: 0.0741 | 0.0419
Epoch 49/300, Loss: 0.0739 | 0.0411
Epoch 50/300, Loss: 0.0738 | 0.0409
Epoch 51/300, Loss: 0.0736 | 0.0407
Epoch 52/300, Loss: 0.0734 | 0.0405
Epoch 53/300, Loss: 0.0732 | 0.0403
Epoch 54/300, Loss: 0.0730 | 0.0401
Epoch 55/300, Loss: 0.0728 | 0.0398
Epoch 56/300, Loss: 0.0727 | 0.0396
Epoch 57/300, Loss: 0.0725 | 0.0394
Epoch 58/300, Loss: 0.0723 | 0.0393
Epoch 59/300, Loss: 0.0721 | 0.0391
Epoch 60/300, Loss: 0.0719 | 0.0389
Epoch 61/300, Loss: 0.0718 | 0.0389
Epoch 62/300, Loss: 0.0716 | 0.0388
Epoch 63/300, Loss: 0.0715 | 0.0387
Epoch 64/300, Loss: 0.0713 | 0.0385
Epoch 65/300, Loss: 0.0712 | 0.0384
Epoch 66/300, Loss: 0.0711 | 0.0383
Epoch 67/300, Loss: 0.0709 | 0.0385
Epoch 68/300, Loss: 0.0708 | 0.0384
Epoch 69/300, Loss: 0.0707 | 0.0383
Epoch 70/300, Loss: 0.0706 | 0.0382
Epoch 71/300, Loss: 0.0705 | 0.0381
Epoch 72/300, Loss: 0.0704 | 0.0380
Epoch 73/300, Loss: 0.0703 | 0.0383
Epoch 74/300, Loss: 0.0702 | 0.0382
Epoch 75/300, Loss: 0.0701 | 0.0381
Epoch 76/300, Loss: 0.0700 | 0.0380
Epoch 77/300, Loss: 0.0699 | 0.0379
Epoch 78/300, Loss: 0.0698 | 0.0379
Epoch 79/300, Loss: 0.0698 | 0.0379
Epoch 80/300, Loss: 0.0697 | 0.0379
Epoch 81/300, Loss: 0.0696 | 0.0378
Epoch 82/300, Loss: 0.0696 | 0.0377
Epoch 83/300, Loss: 0.0695 | 0.0377
Epoch 84/300, Loss: 0.0694 | 0.0376
Epoch 85/300, Loss: 0.0694 | 0.0376
Epoch 86/300, Loss: 0.0693 | 0.0375
Epoch 87/300, Loss: 0.0693 | 0.0375
Epoch 88/300, Loss: 0.0692 | 0.0374
Epoch 89/300, Loss: 0.0691 | 0.0374
Epoch 90/300, Loss: 0.0691 | 0.0373
Epoch 91/300, Loss: 0.0690 | 0.0373
Epoch 92/300, Loss: 0.0690 | 0.0373
Epoch 93/300, Loss: 0.0689 | 0.0373
Epoch 94/300, Loss: 0.0689 | 0.0372
Epoch 95/300, Loss: 0.0688 | 0.0372
Epoch 96/300, Loss: 0.0688 | 0.0372
Epoch 97/300, Loss: 0.0687 | 0.0372
Epoch 98/300, Loss: 0.0687 | 0.0371
Epoch 99/300, Loss: 0.0687 | 0.0371
Epoch 100/300, Loss: 0.0686 | 0.0371
Epoch 101/300, Loss: 0.0686 | 0.0371
Epoch 102/300, Loss: 0.0685 | 0.0371
Epoch 103/300, Loss: 0.0685 | 0.0371
Epoch 104/300, Loss: 0.0685 | 0.0371
Epoch 105/300, Loss: 0.0684 | 0.0370
Epoch 106/300, Loss: 0.0684 | 0.0370
Epoch 107/300, Loss: 0.0684 | 0.0370
Epoch 108/300, Loss: 0.0683 | 0.0370
Epoch 109/300, Loss: 0.0683 | 0.0370
Epoch 110/300, Loss: 0.0683 | 0.0370
Epoch 111/300, Loss: 0.0683 | 0.0369
Epoch 112/300, Loss: 0.0682 | 0.0369
Epoch 113/300, Loss: 0.0682 | 0.0369
Epoch 114/300, Loss: 0.0682 | 0.0369
Epoch 115/300, Loss: 0.0682 | 0.0369
Epoch 116/300, Loss: 0.0681 | 0.0369
Epoch 117/300, Loss: 0.0681 | 0.0369
Epoch 118/300, Loss: 0.0681 | 0.0368
Epoch 119/300, Loss: 0.0681 | 0.0368
Epoch 120/300, Loss: 0.0681 | 0.0368
Epoch 121/300, Loss: 0.0681 | 0.0368
Epoch 122/300, Loss: 0.0680 | 0.0368
Epoch 123/300, Loss: 0.0680 | 0.0368
Epoch 124/300, Loss: 0.0680 | 0.0368
Epoch 125/300, Loss: 0.0680 | 0.0368
Epoch 126/300, Loss: 0.0680 | 0.0368
Epoch 127/300, Loss: 0.0680 | 0.0368
Epoch 128/300, Loss: 0.0679 | 0.0368
Epoch 129/300, Loss: 0.0679 | 0.0367
Epoch 130/300, Loss: 0.0679 | 0.0367
Epoch 131/300, Loss: 0.0679 | 0.0367
Epoch 132/300, Loss: 0.0679 | 0.0367
Epoch 133/300, Loss: 0.0679 | 0.0367
Epoch 134/300, Loss: 0.0679 | 0.0367
Epoch 135/300, Loss: 0.0679 | 0.0367
Epoch 136/300, Loss: 0.0678 | 0.0367
Epoch 137/300, Loss: 0.0678 | 0.0367
Epoch 138/300, Loss: 0.0678 | 0.0367
Epoch 139/300, Loss: 0.0678 | 0.0367
Epoch 140/300, Loss: 0.0678 | 0.0367
Epoch 141/300, Loss: 0.0678 | 0.0366
Epoch 142/300, Loss: 0.0678 | 0.0366
Epoch 143/300, Loss: 0.0678 | 0.0366
Epoch 144/300, Loss: 0.0678 | 0.0366
Epoch 145/300, Loss: 0.0678 | 0.0366
Epoch 146/300, Loss: 0.0677 | 0.0366
Epoch 147/300, Loss: 0.0677 | 0.0366
Epoch 148/300, Loss: 0.0677 | 0.0366
Epoch 149/300, Loss: 0.0677 | 0.0366
Epoch 150/300, Loss: 0.0677 | 0.0366
Epoch 151/300, Loss: 0.0677 | 0.0366
Epoch 152/300, Loss: 0.0677 | 0.0366
Epoch 153/300, Loss: 0.0677 | 0.0366
Epoch 154/300, Loss: 0.0677 | 0.0366
Epoch 155/300, Loss: 0.0677 | 0.0366
Epoch 156/300, Loss: 0.0677 | 0.0366
Epoch 157/300, Loss: 0.0677 | 0.0366
Epoch 158/300, Loss: 0.0677 | 0.0366
Epoch 159/300, Loss: 0.0676 | 0.0366
Epoch 160/300, Loss: 0.0676 | 0.0366
Epoch 161/300, Loss: 0.0676 | 0.0366
Epoch 162/300, Loss: 0.0676 | 0.0366
Epoch 163/300, Loss: 0.0676 | 0.0366
Epoch 164/300, Loss: 0.0676 | 0.0366
Epoch 165/300, Loss: 0.0676 | 0.0366
Epoch 166/300, Loss: 0.0676 | 0.0366
Epoch 167/300, Loss: 0.0676 | 0.0366
Epoch 168/300, Loss: 0.0676 | 0.0366
Epoch 169/300, Loss: 0.0676 | 0.0366
Epoch 170/300, Loss: 0.0676 | 0.0366
Epoch 171/300, Loss: 0.0676 | 0.0366
Epoch 172/300, Loss: 0.0676 | 0.0366
Epoch 173/300, Loss: 0.0676 | 0.0366
Epoch 174/300, Loss: 0.0676 | 0.0366
Epoch 175/300, Loss: 0.0676 | 0.0366
Epoch 176/300, Loss: 0.0676 | 0.0366
Epoch 177/300, Loss: 0.0675 | 0.0366
Epoch 178/300, Loss: 0.0675 | 0.0366
Epoch 179/300, Loss: 0.0675 | 0.0366
Epoch 180/300, Loss: 0.0675 | 0.0365
Epoch 181/300, Loss: 0.0675 | 0.0366
Epoch 182/300, Loss: 0.0675 | 0.0366
Epoch 183/300, Loss: 0.0675 | 0.0365
Epoch 184/300, Loss: 0.0675 | 0.0365
Epoch 185/300, Loss: 0.0675 | 0.0365
Epoch 186/300, Loss: 0.0675 | 0.0365
Epoch 187/300, Loss: 0.0675 | 0.0365
Epoch 188/300, Loss: 0.0675 | 0.0365
Epoch 189/300, Loss: 0.0675 | 0.0365
Epoch 190/300, Loss: 0.0675 | 0.0365
Epoch 191/300, Loss: 0.0675 | 0.0365
Epoch 192/300, Loss: 0.0675 | 0.0365
Epoch 193/300, Loss: 0.0675 | 0.0365
Epoch 194/300, Loss: 0.0675 | 0.0365
Epoch 195/300, Loss: 0.0675 | 0.0365
Epoch 196/300, Loss: 0.0675 | 0.0365
Epoch 197/300, Loss: 0.0675 | 0.0365
Epoch 198/300, Loss: 0.0675 | 0.0365
Epoch 199/300, Loss: 0.0675 | 0.0365
Epoch 200/300, Loss: 0.0675 | 0.0365
Epoch 201/300, Loss: 0.0675 | 0.0365
Epoch 202/300, Loss: 0.0675 | 0.0365
Epoch 203/300, Loss: 0.0675 | 0.0365
Epoch 204/300, Loss: 0.0675 | 0.0365
Epoch 205/300, Loss: 0.0675 | 0.0365
Epoch 206/300, Loss: 0.0675 | 0.0365
Epoch 207/300, Loss: 0.0675 | 0.0365
Epoch 208/300, Loss: 0.0675 | 0.0365
Epoch 209/300, Loss: 0.0675 | 0.0365
Epoch 210/300, Loss: 0.0675 | 0.0365
Epoch 211/300, Loss: 0.0675 | 0.0365
Epoch 212/300, Loss: 0.0675 | 0.0365
Epoch 213/300, Loss: 0.0675 | 0.0365
Epoch 214/300, Loss: 0.0675 | 0.0365
Epoch 215/300, Loss: 0.0675 | 0.0365
Epoch 216/300, Loss: 0.0674 | 0.0365
Epoch 217/300, Loss: 0.0674 | 0.0365
Epoch 218/300, Loss: 0.0674 | 0.0365
Epoch 219/300, Loss: 0.0674 | 0.0365
Epoch 220/300, Loss: 0.0674 | 0.0365
Epoch 221/300, Loss: 0.0674 | 0.0365
Epoch 222/300, Loss: 0.0674 | 0.0365
Epoch 223/300, Loss: 0.0674 | 0.0365
Epoch 224/300, Loss: 0.0674 | 0.0365
Epoch 225/300, Loss: 0.0674 | 0.0365
Epoch 226/300, Loss: 0.0674 | 0.0365
Epoch 227/300, Loss: 0.0674 | 0.0365
Epoch 228/300, Loss: 0.0674 | 0.0365
Epoch 229/300, Loss: 0.0674 | 0.0365
Epoch 230/300, Loss: 0.0674 | 0.0365
Epoch 231/300, Loss: 0.0674 | 0.0365
Epoch 232/300, Loss: 0.0674 | 0.0365
Epoch 233/300, Loss: 0.0674 | 0.0365
Epoch 234/300, Loss: 0.0674 | 0.0365
Epoch 235/300, Loss: 0.0674 | 0.0365
Epoch 236/300, Loss: 0.0674 | 0.0365
Epoch 237/300, Loss: 0.0674 | 0.0365
Epoch 238/300, Loss: 0.0674 | 0.0365
Epoch 239/300, Loss: 0.0674 | 0.0365
Epoch 240/300, Loss: 0.0674 | 0.0365
Epoch 241/300, Loss: 0.0674 | 0.0365
Epoch 242/300, Loss: 0.0674 | 0.0365
Epoch 243/300, Loss: 0.0674 | 0.0365
Epoch 244/300, Loss: 0.0674 | 0.0365
Epoch 245/300, Loss: 0.0674 | 0.0365
Epoch 246/300, Loss: 0.0674 | 0.0365
Epoch 247/300, Loss: 0.0674 | 0.0365
Epoch 248/300, Loss: 0.0674 | 0.0365
Epoch 249/300, Loss: 0.0674 | 0.0365
Epoch 250/300, Loss: 0.0674 | 0.0365
Epoch 251/300, Loss: 0.0674 | 0.0365
Epoch 252/300, Loss: 0.0674 | 0.0365
Epoch 253/300, Loss: 0.0674 | 0.0365
Epoch 254/300, Loss: 0.0674 | 0.0365
Epoch 255/300, Loss: 0.0674 | 0.0365
Epoch 256/300, Loss: 0.0674 | 0.0365
Epoch 257/300, Loss: 0.0674 | 0.0365
Epoch 258/300, Loss: 0.0674 | 0.0365
Epoch 259/300, Loss: 0.0674 | 0.0365
Epoch 260/300, Loss: 0.0674 | 0.0365
Epoch 261/300, Loss: 0.0674 | 0.0365
Epoch 262/300, Loss: 0.0674 | 0.0365
Epoch 263/300, Loss: 0.0674 | 0.0365
Epoch 264/300, Loss: 0.0674 | 0.0365
Epoch 265/300, Loss: 0.0674 | 0.0365
Epoch 266/300, Loss: 0.0674 | 0.0365
Epoch 267/300, Loss: 0.0674 | 0.0365
Epoch 268/300, Loss: 0.0674 | 0.0365
Epoch 269/300, Loss: 0.0674 | 0.0365
Epoch 270/300, Loss: 0.0674 | 0.0365
Epoch 271/300, Loss: 0.0674 | 0.0365
Epoch 272/300, Loss: 0.0674 | 0.0365
Epoch 273/300, Loss: 0.0674 | 0.0365
Epoch 274/300, Loss: 0.0674 | 0.0365
Epoch 275/300, Loss: 0.0674 | 0.0365
Epoch 276/300, Loss: 0.0674 | 0.0365
Epoch 277/300, Loss: 0.0674 | 0.0365
Epoch 278/300, Loss: 0.0674 | 0.0365
Epoch 279/300, Loss: 0.0674 | 0.0365
Epoch 280/300, Loss: 0.0674 | 0.0365
Epoch 281/300, Loss: 0.0674 | 0.0365
Epoch 282/300, Loss: 0.0674 | 0.0365
Epoch 283/300, Loss: 0.0674 | 0.0365
Epoch 284/300, Loss: 0.0674 | 0.0365
Epoch 285/300, Loss: 0.0674 | 0.0365
Epoch 286/300, Loss: 0.0674 | 0.0365
Epoch 287/300, Loss: 0.0674 | 0.0365
Epoch 288/300, Loss: 0.0674 | 0.0365
Epoch 289/300, Loss: 0.0674 | 0.0365
Epoch 290/300, Loss: 0.0674 | 0.0365
Epoch 291/300, Loss: 0.0674 | 0.0365
Epoch 292/300, Loss: 0.0674 | 0.0365
Epoch 293/300, Loss: 0.0674 | 0.0365
Epoch 294/300, Loss: 0.0674 | 0.0365
Epoch 295/300, Loss: 0.0674 | 0.0365
Epoch 296/300, Loss: 0.0674 | 0.0365
Epoch 297/300, Loss: 0.0674 | 0.0365
Epoch 298/300, Loss: 0.0674 | 0.0365
Epoch 299/300, Loss: 0.0674 | 0.0365
Epoch 300/300, Loss: 0.0674 | 0.0365
Runtime (seconds): 920.4940040111542
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 329.14594338298775
RMSE: 18.142379760742188
MAE: 18.142379760742188
R-squared: nan
[205.28238]
