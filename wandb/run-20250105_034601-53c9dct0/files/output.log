ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 03:46:03,285][0m A new study created in memory with name: no-name-0350d651-30cb-4b88-8d89-1056f8f25872[0m
[32m[I 2025-01-05 03:46:32,190][0m Trial 0 finished with value: 0.6893674873315705 and parameters: {'observation_period_num': 247, 'train_rates': 0.675643974149218, 'learning_rate': 1.1163368242159318e-05, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9531030448020286}. Best is trial 0 with value: 0.6893674873315705.[0m
[32m[I 2025-01-05 03:47:45,641][0m Trial 1 finished with value: 0.18303549417853354 and parameters: {'observation_period_num': 228, 'train_rates': 0.8168394310943714, 'learning_rate': 2.2963039918977686e-05, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8218917130574855}. Best is trial 1 with value: 0.18303549417853354.[0m
[32m[I 2025-01-05 03:48:19,398][0m Trial 2 finished with value: 0.35688509092317133 and parameters: {'observation_period_num': 250, 'train_rates': 0.7448133236622505, 'learning_rate': 1.6887417420435947e-05, 'batch_size': 144, 'step_size': 11, 'gamma': 0.90466490556627}. Best is trial 1 with value: 0.18303549417853354.[0m
[32m[I 2025-01-05 03:51:00,076][0m Trial 3 finished with value: 0.2702080523714921 and parameters: {'observation_period_num': 93, 'train_rates': 0.7281446273927992, 'learning_rate': 6.904708822779299e-06, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8156861055369433}. Best is trial 1 with value: 0.18303549417853354.[0m
[32m[I 2025-01-05 03:51:19,178][0m Trial 4 finished with value: 0.697710510138627 and parameters: {'observation_period_num': 166, 'train_rates': 0.6086177888465875, 'learning_rate': 7.222266308294707e-06, 'batch_size': 248, 'step_size': 8, 'gamma': 0.8838346880154271}. Best is trial 1 with value: 0.18303549417853354.[0m
[32m[I 2025-01-05 03:51:58,267][0m Trial 5 finished with value: 0.6599854597678552 and parameters: {'observation_period_num': 91, 'train_rates': 0.7731534601788993, 'learning_rate': 1.7808662426006917e-06, 'batch_size': 132, 'step_size': 11, 'gamma': 0.7915414009315177}. Best is trial 1 with value: 0.18303549417853354.[0m
[32m[I 2025-01-05 03:56:30,426][0m Trial 6 finished with value: 0.0685474160667057 and parameters: {'observation_period_num': 158, 'train_rates': 0.8676183844141606, 'learning_rate': 3.0300608297906733e-05, 'batch_size': 19, 'step_size': 11, 'gamma': 0.9429640265726605}. Best is trial 6 with value: 0.0685474160667057.[0m
[32m[I 2025-01-05 03:56:56,184][0m Trial 7 finished with value: 0.08666324087352438 and parameters: {'observation_period_num': 124, 'train_rates': 0.7962804691628336, 'learning_rate': 0.0001760319712947258, 'batch_size': 223, 'step_size': 9, 'gamma': 0.8525390823765717}. Best is trial 6 with value: 0.0685474160667057.[0m
[32m[I 2025-01-05 03:57:41,474][0m Trial 8 finished with value: 0.6343207955360413 and parameters: {'observation_period_num': 184, 'train_rates': 0.982945005899608, 'learning_rate': 4.308816851366775e-06, 'batch_size': 133, 'step_size': 4, 'gamma': 0.8389455121064258}. Best is trial 6 with value: 0.0685474160667057.[0m
[32m[I 2025-01-05 03:58:24,770][0m Trial 9 finished with value: 0.13496531641909054 and parameters: {'observation_period_num': 201, 'train_rates': 0.6441935959822319, 'learning_rate': 0.0008729092425578974, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8033565154910569}. Best is trial 6 with value: 0.0685474160667057.[0m
[32m[I 2025-01-05 04:04:06,041][0m Trial 10 finished with value: 0.04400508924825302 and parameters: {'observation_period_num': 29, 'train_rates': 0.897019084122896, 'learning_rate': 7.830661847682159e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9895746663765446}. Best is trial 10 with value: 0.04400508924825302.[0m
[32m[I 2025-01-05 04:09:29,967][0m Trial 11 finished with value: 0.022607267855008127 and parameters: {'observation_period_num': 5, 'train_rates': 0.8935337267884343, 'learning_rate': 9.135917617528002e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9885733641654746}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:10:56,818][0m Trial 12 finished with value: 0.02719719925145638 and parameters: {'observation_period_num': 8, 'train_rates': 0.9183746054948582, 'learning_rate': 0.00011545928799793345, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9860029369859279}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:12:25,661][0m Trial 13 finished with value: 0.05375601579207224 and parameters: {'observation_period_num': 20, 'train_rates': 0.9515605964661601, 'learning_rate': 0.00016951048709423617, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9708379768085977}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:14:07,380][0m Trial 14 finished with value: 0.057837384658750525 and parameters: {'observation_period_num': 55, 'train_rates': 0.9080657533148679, 'learning_rate': 0.0006908768454074325, 'batch_size': 55, 'step_size': 13, 'gamma': 0.9263030224578759}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:15:04,888][0m Trial 15 finished with value: 0.04135794831045832 and parameters: {'observation_period_num': 55, 'train_rates': 0.8567682836241092, 'learning_rate': 7.737642434001237e-05, 'batch_size': 97, 'step_size': 13, 'gamma': 0.9775323995035287}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:16:59,422][0m Trial 16 finished with value: 0.02674090580489391 and parameters: {'observation_period_num': 12, 'train_rates': 0.9338736284088996, 'learning_rate': 0.000256001468994546, 'batch_size': 50, 'step_size': 13, 'gamma': 0.9149122119761914}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:17:34,038][0m Trial 17 finished with value: 0.11343397945165634 and parameters: {'observation_period_num': 52, 'train_rates': 0.9741739931286992, 'learning_rate': 0.0003818935047386304, 'batch_size': 185, 'step_size': 13, 'gamma': 0.9045206242672984}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:19:27,362][0m Trial 18 finished with value: 0.10815270737498656 and parameters: {'observation_period_num': 91, 'train_rates': 0.8481276664119901, 'learning_rate': 0.0004168058419517583, 'batch_size': 46, 'step_size': 6, 'gamma': 0.7516099915353427}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:20:27,172][0m Trial 19 finished with value: 0.05092458774126013 and parameters: {'observation_period_num': 32, 'train_rates': 0.9349395102720482, 'learning_rate': 5.0056742101133985e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8790262163493803}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:22:30,775][0m Trial 20 finished with value: 0.13307140818052499 and parameters: {'observation_period_num': 113, 'train_rates': 0.8703942014495187, 'learning_rate': 0.0002745629099689274, 'batch_size': 43, 'step_size': 5, 'gamma': 0.9408713896772404}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:23:42,715][0m Trial 21 finished with value: 0.0340468353532218 and parameters: {'observation_period_num': 8, 'train_rates': 0.9314163548498864, 'learning_rate': 0.0001306140418039082, 'batch_size': 83, 'step_size': 15, 'gamma': 0.9652623956367417}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:26:01,891][0m Trial 22 finished with value: 0.02662159395223161 and parameters: {'observation_period_num': 6, 'train_rates': 0.9042108865534019, 'learning_rate': 6.262006693383519e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.925831685986618}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:28:23,406][0m Trial 23 finished with value: 0.03922516465323364 and parameters: {'observation_period_num': 42, 'train_rates': 0.8313407502151181, 'learning_rate': 3.99249716952059e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9167860553589438}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:33:58,877][0m Trial 24 finished with value: 0.0723850987501279 and parameters: {'observation_period_num': 62, 'train_rates': 0.8898847807067394, 'learning_rate': 6.895840299847572e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9304341194715232}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:36:05,264][0m Trial 25 finished with value: 0.033819546923041344 and parameters: {'observation_period_num': 5, 'train_rates': 0.9594799145262366, 'learning_rate': 0.0002497434746407947, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8964817370675826}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:37:13,017][0m Trial 26 finished with value: 0.08903636458119371 and parameters: {'observation_period_num': 74, 'train_rates': 0.8879705865459911, 'learning_rate': 0.0005787346838383677, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8637123158931053}. Best is trial 11 with value: 0.022607267855008127.[0m
[32m[I 2025-01-05 04:40:13,452][0m Trial 27 finished with value: 0.021426303312182426 and parameters: {'observation_period_num': 27, 'train_rates': 0.9893599021095496, 'learning_rate': 0.00011341307324445398, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9570948286432536}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:41:04,342][0m Trial 28 finished with value: 0.0548207089304924 and parameters: {'observation_period_num': 33, 'train_rates': 0.9776768536112328, 'learning_rate': 5.2615812833935905e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9563001668247048}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:41:43,660][0m Trial 29 finished with value: 0.41838014125823975 and parameters: {'observation_period_num': 70, 'train_rates': 0.9469968151619782, 'learning_rate': 1.6361239990222138e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.947743957831476}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:45:01,231][0m Trial 30 finished with value: 0.039634425193071365 and parameters: {'observation_period_num': 38, 'train_rates': 0.9899336140494999, 'learning_rate': 0.00011421126190307594, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9635463062762524}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:46:31,315][0m Trial 31 finished with value: 0.028574589300521855 and parameters: {'observation_period_num': 18, 'train_rates': 0.9197565843968887, 'learning_rate': 0.00023834117233814488, 'batch_size': 63, 'step_size': 14, 'gamma': 0.9231494111797065}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:49:22,403][0m Trial 32 finished with value: 0.03647895004388278 and parameters: {'observation_period_num': 19, 'train_rates': 0.9582200394058081, 'learning_rate': 2.9766076348158284e-05, 'batch_size': 34, 'step_size': 12, 'gamma': 0.94963093005295}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:50:29,048][0m Trial 33 finished with value: 0.049944369274945485 and parameters: {'observation_period_num': 42, 'train_rates': 0.8199334756793029, 'learning_rate': 9.197217992543395e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9753012759929547}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:52:14,399][0m Trial 34 finished with value: 0.025812870305636382 and parameters: {'observation_period_num': 6, 'train_rates': 0.9082871349847846, 'learning_rate': 0.00018350565513399551, 'batch_size': 54, 'step_size': 10, 'gamma': 0.91359756726158}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:54:53,925][0m Trial 35 finished with value: 0.04685277868907876 and parameters: {'observation_period_num': 24, 'train_rates': 0.7221319939854155, 'learning_rate': 2.0817238524160795e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.9373803356073946}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:56:22,857][0m Trial 36 finished with value: 0.02636175009483186 and parameters: {'observation_period_num': 5, 'train_rates': 0.7679485474318527, 'learning_rate': 0.00017065546224128496, 'batch_size': 57, 'step_size': 10, 'gamma': 0.8947546571208445}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:57:42,279][0m Trial 37 finished with value: 0.12742209514961556 and parameters: {'observation_period_num': 222, 'train_rates': 0.7664484832772926, 'learning_rate': 0.0001606430355082353, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8950026176499961}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:58:41,704][0m Trial 38 finished with value: 0.14832879789618147 and parameters: {'observation_period_num': 150, 'train_rates': 0.7013107514787982, 'learning_rate': 3.771296982422141e-05, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8858640572303671}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 04:59:10,431][0m Trial 39 finished with value: 0.07531688750282792 and parameters: {'observation_period_num': 108, 'train_rates': 0.7875177958842554, 'learning_rate': 0.00041042786155985654, 'batch_size': 190, 'step_size': 7, 'gamma': 0.850006313383147}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:02:15,185][0m Trial 40 finished with value: 0.09163257344299461 and parameters: {'observation_period_num': 84, 'train_rates': 0.7565293108292129, 'learning_rate': 0.00017752910960634565, 'batch_size': 26, 'step_size': 10, 'gamma': 0.9590047135615885}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:04:35,402][0m Trial 41 finished with value: 0.12067403138442384 and parameters: {'observation_period_num': 5, 'train_rates': 0.7935507477245287, 'learning_rate': 1.0940796294983314e-06, 'batch_size': 37, 'step_size': 11, 'gamma': 0.9105220873931656}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:06:10,335][0m Trial 42 finished with value: 0.03545682123965687 and parameters: {'observation_period_num': 24, 'train_rates': 0.8313828997313274, 'learning_rate': 5.846345153075728e-05, 'batch_size': 56, 'step_size': 9, 'gamma': 0.8715992595992236}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:09:47,106][0m Trial 43 finished with value: 0.0433559836215013 and parameters: {'observation_period_num': 42, 'train_rates': 0.8678982647134748, 'learning_rate': 0.0001145716840166254, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9342581565089179}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:10:56,619][0m Trial 44 finished with value: 0.06452550755730316 and parameters: {'observation_period_num': 18, 'train_rates': 0.7264822737348993, 'learning_rate': 8.892642030538908e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8943986686877327}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:16:15,219][0m Trial 45 finished with value: 0.045672927748161736 and parameters: {'observation_period_num': 28, 'train_rates': 0.8118254774565808, 'learning_rate': 0.0002059049536773462, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9778152303317359}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:16:58,868][0m Trial 46 finished with value: 0.39144022293695485 and parameters: {'observation_period_num': 139, 'train_rates': 0.6806890013467055, 'learning_rate': 1.2087163687189076e-05, 'batch_size': 110, 'step_size': 8, 'gamma': 0.9051538750364614}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:19:07,337][0m Trial 47 finished with value: 0.06884018445325164 and parameters: {'observation_period_num': 14, 'train_rates': 0.9087471134963876, 'learning_rate': 3.874955225942484e-06, 'batch_size': 44, 'step_size': 15, 'gamma': 0.9874090154739814}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:20:09,664][0m Trial 48 finished with value: 0.059997545778751375 and parameters: {'observation_period_num': 47, 'train_rates': 0.8798279923667718, 'learning_rate': 0.00015241027775216755, 'batch_size': 91, 'step_size': 9, 'gamma': 0.9460692380007899}. Best is trial 27 with value: 0.021426303312182426.[0m
[32m[I 2025-01-05 05:21:54,128][0m Trial 49 finished with value: 0.07837728752030267 and parameters: {'observation_period_num': 31, 'train_rates': 0.8462829141597703, 'learning_rate': 0.0003238520220514768, 'batch_size': 52, 'step_size': 10, 'gamma': 0.9194492127701048}. Best is trial 27 with value: 0.021426303312182426.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 05:21:54,138][0m A new study created in memory with name: no-name-ca778e0f-108c-4802-9601-2e28961466ed[0m
[32m[I 2025-01-05 05:24:54,093][0m Trial 0 finished with value: 0.1681537694532876 and parameters: {'observation_period_num': 35, 'train_rates': 0.8279990732727751, 'learning_rate': 3.1633107659530305e-06, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8262948357643348}. Best is trial 0 with value: 0.1681537694532876.[0m
[32m[I 2025-01-05 05:25:15,924][0m Trial 1 finished with value: 0.1027973845866082 and parameters: {'observation_period_num': 137, 'train_rates': 0.7407594546890484, 'learning_rate': 0.0005208749082552972, 'batch_size': 232, 'step_size': 5, 'gamma': 0.8802520416258461}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:26:14,100][0m Trial 2 finished with value: 0.12938276122790374 and parameters: {'observation_period_num': 179, 'train_rates': 0.6913950566580976, 'learning_rate': 0.0003130529197038175, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9766810325478796}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:26:49,352][0m Trial 3 finished with value: 0.22893310223634428 and parameters: {'observation_period_num': 69, 'train_rates': 0.8921405915399795, 'learning_rate': 1.2113645246810785e-05, 'batch_size': 171, 'step_size': 4, 'gamma': 0.9568920756785606}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:27:32,422][0m Trial 4 finished with value: 0.1466243991041302 and parameters: {'observation_period_num': 238, 'train_rates': 0.703322200409417, 'learning_rate': 8.30700968969511e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.8412113553232993}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:28:01,868][0m Trial 5 finished with value: 0.2994555950909853 and parameters: {'observation_period_num': 167, 'train_rates': 0.7129831887876117, 'learning_rate': 1.65108618554142e-05, 'batch_size': 177, 'step_size': 9, 'gamma': 0.9260759066893002}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:28:50,933][0m Trial 6 finished with value: 0.13672370111816445 and parameters: {'observation_period_num': 251, 'train_rates': 0.8074034839121339, 'learning_rate': 0.000256503819695174, 'batch_size': 103, 'step_size': 3, 'gamma': 0.8500315512260397}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:29:13,624][0m Trial 7 finished with value: 0.3993434695487327 and parameters: {'observation_period_num': 175, 'train_rates': 0.6619975404719517, 'learning_rate': 1.1972976536481685e-05, 'batch_size': 224, 'step_size': 14, 'gamma': 0.8633323366786759}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:31:27,876][0m Trial 8 finished with value: 0.11586305132861864 and parameters: {'observation_period_num': 117, 'train_rates': 0.95856271006016, 'learning_rate': 0.00039491514426779955, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8714013711900706}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:32:04,519][0m Trial 9 finished with value: 0.39084058598030447 and parameters: {'observation_period_num': 75, 'train_rates': 0.9254325609446157, 'learning_rate': 4.3793139857699495e-06, 'batch_size': 167, 'step_size': 15, 'gamma': 0.8561193203040554}. Best is trial 1 with value: 0.1027973845866082.[0m
Early stopping at epoch 58
[32m[I 2025-01-05 05:32:16,234][0m Trial 10 finished with value: 0.3213921058177948 and parameters: {'observation_period_num': 119, 'train_rates': 0.6034242637482614, 'learning_rate': 0.0009594369255296826, 'batch_size': 255, 'step_size': 1, 'gamma': 0.7578546460521798}. Best is trial 1 with value: 0.1027973845866082.[0m
[32m[I 2025-01-05 05:37:12,270][0m Trial 11 finished with value: 0.062372259388212115 and parameters: {'observation_period_num': 122, 'train_rates': 0.9774332205224523, 'learning_rate': 0.0008528406812026509, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9050769903603767}. Best is trial 11 with value: 0.062372259388212115.[0m
[32m[I 2025-01-05 05:37:39,070][0m Trial 12 finished with value: 0.1731591600438823 and parameters: {'observation_period_num': 143, 'train_rates': 0.8610686520923517, 'learning_rate': 8.541486833379414e-05, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9142850007809705}. Best is trial 11 with value: 0.062372259388212115.[0m
[32m[I 2025-01-05 05:38:50,267][0m Trial 13 finished with value: 0.0668469064295032 and parameters: {'observation_period_num': 83, 'train_rates': 0.7606950295655418, 'learning_rate': 7.696335746905993e-05, 'batch_size': 69, 'step_size': 7, 'gamma': 0.9035532976507067}. Best is trial 11 with value: 0.062372259388212115.[0m
[32m[I 2025-01-05 05:40:21,171][0m Trial 14 finished with value: 0.032429445534944534 and parameters: {'observation_period_num': 7, 'train_rates': 0.9838023327995071, 'learning_rate': 8.832059273218294e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9132894352565387}. Best is trial 14 with value: 0.032429445534944534.[0m
[32m[I 2025-01-05 05:45:17,777][0m Trial 15 finished with value: 0.021635800882270843 and parameters: {'observation_period_num': 8, 'train_rates': 0.9889566067833049, 'learning_rate': 0.00015500514411775907, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8029146535851374}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 05:47:09,212][0m Trial 16 finished with value: 0.041525136679410934 and parameters: {'observation_period_num': 6, 'train_rates': 0.9838271927789077, 'learning_rate': 3.907012562322822e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.7911453758324416}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 05:48:11,640][0m Trial 17 finished with value: 0.6391247153282166 and parameters: {'observation_period_num': 15, 'train_rates': 0.9200621128780471, 'learning_rate': 1.2915680884595356e-06, 'batch_size': 94, 'step_size': 12, 'gamma': 0.809269507433148}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 05:48:56,086][0m Trial 18 finished with value: 0.04323392101455965 and parameters: {'observation_period_num': 48, 'train_rates': 0.8600134710569978, 'learning_rate': 0.0001626736603190684, 'batch_size': 129, 'step_size': 12, 'gamma': 0.7614063011518547}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 05:54:47,966][0m Trial 19 finished with value: 0.040446802920528824 and parameters: {'observation_period_num': 33, 'train_rates': 0.9404199979248259, 'learning_rate': 4.156145418670219e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.940931247751707}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 05:56:24,164][0m Trial 20 finished with value: 0.05661629444435007 and parameters: {'observation_period_num': 53, 'train_rates': 0.8894176921556074, 'learning_rate': 0.0001413579536747972, 'batch_size': 58, 'step_size': 13, 'gamma': 0.7927721882595882}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:02:19,485][0m Trial 21 finished with value: 0.03792296497281208 and parameters: {'observation_period_num': 16, 'train_rates': 0.9416097156548048, 'learning_rate': 4.480707773057183e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9432331900094135}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:05:02,202][0m Trial 22 finished with value: 0.04124576693364218 and parameters: {'observation_period_num': 6, 'train_rates': 0.9828956741769548, 'learning_rate': 2.3783985790746067e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9864632166050761}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:06:16,011][0m Trial 23 finished with value: 0.06575142296182143 and parameters: {'observation_period_num': 29, 'train_rates': 0.9370245456935418, 'learning_rate': 6.039875158042404e-05, 'batch_size': 79, 'step_size': 11, 'gamma': 0.9487110795248505}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:08:09,488][0m Trial 24 finished with value: 0.08509188568849357 and parameters: {'observation_period_num': 86, 'train_rates': 0.9033695316456869, 'learning_rate': 0.00016101200110066083, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8953105310312893}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:10:39,086][0m Trial 25 finished with value: 0.06469196036201098 and parameters: {'observation_period_num': 56, 'train_rates': 0.9602981716199156, 'learning_rate': 2.4737165332739545e-05, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9672784605360483}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:12:14,693][0m Trial 26 finished with value: 0.036895133554935455 and parameters: {'observation_period_num': 24, 'train_rates': 0.9888705618421854, 'learning_rate': 0.0001156335854659436, 'batch_size': 64, 'step_size': 13, 'gamma': 0.9306135678535259}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:12:54,583][0m Trial 27 finished with value: 0.08876647889139798 and parameters: {'observation_period_num': 99, 'train_rates': 0.8679959324904605, 'learning_rate': 0.00013000353443521506, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9228321294104344}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:14:01,918][0m Trial 28 finished with value: 0.046058136969804764 and parameters: {'observation_period_num': 30, 'train_rates': 0.9883650536882578, 'learning_rate': 0.00020311280659926205, 'batch_size': 91, 'step_size': 13, 'gamma': 0.8886073136995746}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:14:46,963][0m Trial 29 finished with value: 0.26885100293227215 and parameters: {'observation_period_num': 45, 'train_rates': 0.8185034192436939, 'learning_rate': 7.215760993014568e-06, 'batch_size': 123, 'step_size': 8, 'gamma': 0.8268714629340301}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:16:06,498][0m Trial 30 finished with value: 0.04370267266870304 and parameters: {'observation_period_num': 24, 'train_rates': 0.7858381521190346, 'learning_rate': 0.0005509971049653126, 'batch_size': 65, 'step_size': 14, 'gamma': 0.9289674630458612}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:19:35,151][0m Trial 31 finished with value: 0.032416108467463234 and parameters: {'observation_period_num': 16, 'train_rates': 0.9544204582313975, 'learning_rate': 5.323528339884915e-05, 'batch_size': 28, 'step_size': 11, 'gamma': 0.9394038833752035}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:22:44,605][0m Trial 32 finished with value: 0.0306384158897991 and parameters: {'observation_period_num': 7, 'train_rates': 0.9591194128614849, 'learning_rate': 0.00011244586641555192, 'batch_size': 31, 'step_size': 10, 'gamma': 0.9622224616352971}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:25:59,649][0m Trial 33 finished with value: 0.032391423423742426 and parameters: {'observation_period_num': 6, 'train_rates': 0.9607394641952656, 'learning_rate': 6.156317052358762e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.9646622057217421}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:28:48,973][0m Trial 34 finished with value: 0.05246723722666502 and parameters: {'observation_period_num': 39, 'train_rates': 0.9533557686123106, 'learning_rate': 5.500966262133839e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9625454456145799}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:32:23,417][0m Trial 35 finished with value: 0.07340128930796565 and parameters: {'observation_period_num': 65, 'train_rates': 0.9217929985920572, 'learning_rate': 0.0002649073239612613, 'batch_size': 26, 'step_size': 8, 'gamma': 0.9866500270693684}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:35:43,346][0m Trial 36 finished with value: 0.03522484304337967 and parameters: {'observation_period_num': 18, 'train_rates': 0.9022941524958014, 'learning_rate': 2.5029030058622777e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9678765516834333}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:36:47,000][0m Trial 37 finished with value: 0.1243287159711527 and parameters: {'observation_period_num': 203, 'train_rates': 0.8395997443239747, 'learning_rate': 0.00039200377969815525, 'batch_size': 80, 'step_size': 9, 'gamma': 0.9768398496773815}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:38:45,814][0m Trial 38 finished with value: 0.08092052710823494 and parameters: {'observation_period_num': 39, 'train_rates': 0.9655508559962114, 'learning_rate': 1.373580983736929e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.9537940351472067}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:41:39,392][0m Trial 39 finished with value: 0.08355151238988658 and parameters: {'observation_period_num': 197, 'train_rates': 0.8896340064838599, 'learning_rate': 5.976829148016399e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7765881684165944}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:43:39,278][0m Trial 40 finished with value: 0.060533374246315434 and parameters: {'observation_period_num': 62, 'train_rates': 0.9148292780376895, 'learning_rate': 3.1716644780138453e-05, 'batch_size': 47, 'step_size': 9, 'gamma': 0.878962162874041}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:47:16,173][0m Trial 41 finished with value: 0.032156765295733365 and parameters: {'observation_period_num': 6, 'train_rates': 0.9580482859701391, 'learning_rate': 9.688019652999784e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.9382412667186575}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:50:44,851][0m Trial 42 finished with value: 0.03480240883072838 and parameters: {'observation_period_num': 16, 'train_rates': 0.95671143991532, 'learning_rate': 0.00010029280512353679, 'batch_size': 28, 'step_size': 11, 'gamma': 0.9374436978245996}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:53:14,081][0m Trial 43 finished with value: 0.02933721358633854 and parameters: {'observation_period_num': 5, 'train_rates': 0.9405096582672541, 'learning_rate': 0.0002077429066728512, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9741632952420618}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:53:53,285][0m Trial 44 finished with value: 0.04705625195848815 and parameters: {'observation_period_num': 37, 'train_rates': 0.9357449461133437, 'learning_rate': 0.00022919957347395482, 'batch_size': 153, 'step_size': 10, 'gamma': 0.9740858505605038}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:54:26,039][0m Trial 45 finished with value: 0.04862331598997116 and parameters: {'observation_period_num': 6, 'train_rates': 0.9682148873781434, 'learning_rate': 0.0004081030557624991, 'batch_size': 193, 'step_size': 7, 'gamma': 0.9894695136149918}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:56:11,427][0m Trial 46 finished with value: 0.06757781115850062 and parameters: {'observation_period_num': 26, 'train_rates': 0.6732982889315233, 'learning_rate': 0.00019500835809061832, 'batch_size': 43, 'step_size': 8, 'gamma': 0.9571089689651361}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 06:57:52,604][0m Trial 47 finished with value: 0.13903208690161786 and parameters: {'observation_period_num': 140, 'train_rates': 0.8737693975289194, 'learning_rate': 0.0005797809303963, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8341139632242088}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 07:03:26,981][0m Trial 48 finished with value: 0.0456679193302989 and parameters: {'observation_period_num': 44, 'train_rates': 0.9445853292117679, 'learning_rate': 0.0003243534767041593, 'batch_size': 17, 'step_size': 5, 'gamma': 0.812576502954736}. Best is trial 15 with value: 0.021635800882270843.[0m
[32m[I 2025-01-05 07:04:33,050][0m Trial 49 finished with value: 0.07897648360082495 and parameters: {'observation_period_num': 100, 'train_rates': 0.7405891665761858, 'learning_rate': 7.491741242803701e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9771590074518341}. Best is trial 15 with value: 0.021635800882270843.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 07:04:33,069][0m A new study created in memory with name: no-name-467a57f6-bfd9-455f-a2e4-8956163928c3[0m
[32m[I 2025-01-05 07:05:02,364][0m Trial 0 finished with value: 0.1547059926251879 and parameters: {'observation_period_num': 248, 'train_rates': 0.666924617483964, 'learning_rate': 0.00025620091412477186, 'batch_size': 156, 'step_size': 14, 'gamma': 0.9315790752201041}. Best is trial 0 with value: 0.1547059926251879.[0m
[32m[I 2025-01-05 07:05:24,351][0m Trial 1 finished with value: 0.8210718421649684 and parameters: {'observation_period_num': 83, 'train_rates': 0.7333855131985626, 'learning_rate': 2.2307856245294255e-06, 'batch_size': 245, 'step_size': 9, 'gamma': 0.9760196790154557}. Best is trial 0 with value: 0.1547059926251879.[0m
[32m[I 2025-01-05 07:05:49,962][0m Trial 2 finished with value: 0.9733754650452446 and parameters: {'observation_period_num': 127, 'train_rates': 0.6432377750065501, 'learning_rate': 2.241227111057356e-06, 'batch_size': 189, 'step_size': 1, 'gamma': 0.9654046946411848}. Best is trial 0 with value: 0.1547059926251879.[0m
[32m[I 2025-01-05 07:06:20,188][0m Trial 3 finished with value: 0.1357361442906764 and parameters: {'observation_period_num': 112, 'train_rates': 0.6375935304804299, 'learning_rate': 0.00013295300286054863, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9635579089299144}. Best is trial 3 with value: 0.1357361442906764.[0m
[32m[I 2025-01-05 07:07:05,742][0m Trial 4 finished with value: 0.8301056731467279 and parameters: {'observation_period_num': 171, 'train_rates': 0.7858735052293977, 'learning_rate': 1.4126756196439164e-06, 'batch_size': 115, 'step_size': 10, 'gamma': 0.7945661935249864}. Best is trial 3 with value: 0.1357361442906764.[0m
[32m[I 2025-01-05 07:07:49,054][0m Trial 5 finished with value: 0.3074964798058968 and parameters: {'observation_period_num': 161, 'train_rates': 0.7041919832872843, 'learning_rate': 7.373969661009397e-05, 'batch_size': 113, 'step_size': 1, 'gamma': 0.9360614140566069}. Best is trial 3 with value: 0.1357361442906764.[0m
[32m[I 2025-01-05 07:09:06,255][0m Trial 6 finished with value: 0.16676562130250017 and parameters: {'observation_period_num': 158, 'train_rates': 0.6027152237424805, 'learning_rate': 0.0005903933771024291, 'batch_size': 53, 'step_size': 4, 'gamma': 0.7677175582747056}. Best is trial 3 with value: 0.1357361442906764.[0m
[32m[I 2025-01-05 07:13:05,103][0m Trial 7 finished with value: 0.1610224087101718 and parameters: {'observation_period_num': 233, 'train_rates': 0.9295975531484101, 'learning_rate': 0.0002930159619708153, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9589166934769816}. Best is trial 3 with value: 0.1357361442906764.[0m
[32m[I 2025-01-05 07:13:35,110][0m Trial 8 finished with value: 0.04739418195066026 and parameters: {'observation_period_num': 27, 'train_rates': 0.8626468250401613, 'learning_rate': 0.00010772544175333216, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9648536704854953}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:14:46,494][0m Trial 9 finished with value: 0.11976066325698886 and parameters: {'observation_period_num': 188, 'train_rates': 0.879886978974042, 'learning_rate': 6.63184695784448e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.7741168307618752}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:15:15,517][0m Trial 10 finished with value: 0.10135123878717422 and parameters: {'observation_period_num': 11, 'train_rates': 0.9836653475642629, 'learning_rate': 1.2973745291332233e-05, 'batch_size': 225, 'step_size': 11, 'gamma': 0.8657818159966989}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:15:44,624][0m Trial 11 finished with value: 0.10965906083583832 and parameters: {'observation_period_num': 5, 'train_rates': 0.9779959851227764, 'learning_rate': 1.2473784733339937e-05, 'batch_size': 239, 'step_size': 11, 'gamma': 0.865838581074355}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:16:14,357][0m Trial 12 finished with value: 0.11428020303321272 and parameters: {'observation_period_num': 15, 'train_rates': 0.8534082667598285, 'learning_rate': 1.7004435913244732e-05, 'batch_size': 196, 'step_size': 12, 'gamma': 0.8699115058590124}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:16:45,574][0m Trial 13 finished with value: 0.3195991814136505 and parameters: {'observation_period_num': 56, 'train_rates': 0.9878680495654216, 'learning_rate': 1.4366376184072302e-05, 'batch_size': 216, 'step_size': 5, 'gamma': 0.8301196962245289}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:17:17,124][0m Trial 14 finished with value: 0.42436968619437543 and parameters: {'observation_period_num': 46, 'train_rates': 0.8450003315912555, 'learning_rate': 5.230815550530712e-06, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9033852381618834}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:17:42,354][0m Trial 15 finished with value: 0.09807135909795761 and parameters: {'observation_period_num': 44, 'train_rates': 0.9169959801277202, 'learning_rate': 4.668497217759118e-05, 'batch_size': 255, 'step_size': 12, 'gamma': 0.8341640951583351}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:18:07,659][0m Trial 16 finished with value: 0.09301833453632537 and parameters: {'observation_period_num': 53, 'train_rates': 0.89866926625707, 'learning_rate': 4.625804534548122e-05, 'batch_size': 253, 'step_size': 15, 'gamma': 0.8206631291796089}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:18:33,864][0m Trial 17 finished with value: 0.06764324762040588 and parameters: {'observation_period_num': 86, 'train_rates': 0.7942806384584065, 'learning_rate': 0.0009988903495378029, 'batch_size': 210, 'step_size': 15, 'gamma': 0.8244156622668933}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:19:05,989][0m Trial 18 finished with value: 0.06353612809325568 and parameters: {'observation_period_num': 98, 'train_rates': 0.776062897398847, 'learning_rate': 0.0008232371878116823, 'batch_size': 161, 'step_size': 3, 'gamma': 0.9003114805633027}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:19:39,054][0m Trial 19 finished with value: 0.0697031847529614 and parameters: {'observation_period_num': 91, 'train_rates': 0.745503127608907, 'learning_rate': 0.0002837830293800997, 'batch_size': 160, 'step_size': 3, 'gamma': 0.9068438282628741}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:20:25,440][0m Trial 20 finished with value: 0.14972688017940328 and parameters: {'observation_period_num': 210, 'train_rates': 0.821066201814899, 'learning_rate': 0.000129808376173435, 'batch_size': 115, 'step_size': 2, 'gamma': 0.9028947154676598}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:20:51,077][0m Trial 21 finished with value: 0.10112027578422281 and parameters: {'observation_period_num': 89, 'train_rates': 0.7871186543996289, 'learning_rate': 0.000845754299932514, 'batch_size': 209, 'step_size': 5, 'gamma': 0.9892137751915553}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:21:21,028][0m Trial 22 finished with value: 0.08259915021035368 and parameters: {'observation_period_num': 70, 'train_rates': 0.7665406765683797, 'learning_rate': 0.0005379639036125203, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8016992958362041}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:22:01,438][0m Trial 23 finished with value: 0.06336487745118935 and parameters: {'observation_period_num': 121, 'train_rates': 0.8196619140259571, 'learning_rate': 0.0008509511393954317, 'batch_size': 134, 'step_size': 3, 'gamma': 0.8479577709803292}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:22:43,606][0m Trial 24 finished with value: 0.09556441850561116 and parameters: {'observation_period_num': 135, 'train_rates': 0.8412959643625507, 'learning_rate': 0.00014176842401772093, 'batch_size': 131, 'step_size': 3, 'gamma': 0.8851099225729938}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:23:24,050][0m Trial 25 finished with value: 0.06849562051513838 and parameters: {'observation_period_num': 113, 'train_rates': 0.8229476087370017, 'learning_rate': 0.000466089763655729, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9332908441434108}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:24:00,033][0m Trial 26 finished with value: 0.15278304434050238 and parameters: {'observation_period_num': 133, 'train_rates': 0.7032832915422218, 'learning_rate': 0.0002018799764093603, 'batch_size': 136, 'step_size': 3, 'gamma': 0.8454404918280927}. Best is trial 8 with value: 0.04739418195066026.[0m
[32m[I 2025-01-05 07:25:03,171][0m Trial 27 finished with value: 0.031265230758524526 and parameters: {'observation_period_num': 28, 'train_rates': 0.8741865378340667, 'learning_rate': 0.0004336960145411193, 'batch_size': 91, 'step_size': 6, 'gamma': 0.8853892311847404}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:26:09,424][0m Trial 28 finished with value: 0.0442069966700815 and parameters: {'observation_period_num': 31, 'train_rates': 0.9426470894595838, 'learning_rate': 0.0004110556191582718, 'batch_size': 89, 'step_size': 6, 'gamma': 0.849721442267696}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:27:25,633][0m Trial 29 finished with value: 0.05760929396130004 and parameters: {'observation_period_num': 26, 'train_rates': 0.9458250484322406, 'learning_rate': 0.00035517907241250395, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9215045861395054}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:28:28,055][0m Trial 30 finished with value: 0.04172206106218132 and parameters: {'observation_period_num': 33, 'train_rates': 0.8862862744976077, 'learning_rate': 0.00019494341175596222, 'batch_size': 90, 'step_size': 6, 'gamma': 0.750689884730033}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:29:33,519][0m Trial 31 finished with value: 0.044732800033207315 and parameters: {'observation_period_num': 23, 'train_rates': 0.8798629871284935, 'learning_rate': 0.00019503571074875936, 'batch_size': 87, 'step_size': 6, 'gamma': 0.7502907276278915}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:30:39,965][0m Trial 32 finished with value: 0.03990927831620774 and parameters: {'observation_period_num': 28, 'train_rates': 0.8851484877944782, 'learning_rate': 0.00021060566690943587, 'batch_size': 86, 'step_size': 6, 'gamma': 0.7513535361023721}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:32:29,513][0m Trial 33 finished with value: 0.06213232106887377 and parameters: {'observation_period_num': 65, 'train_rates': 0.9551272436543172, 'learning_rate': 0.00038501257235368946, 'batch_size': 54, 'step_size': 6, 'gamma': 0.750569260683719}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:33:27,818][0m Trial 34 finished with value: 0.04007819229736924 and parameters: {'observation_period_num': 38, 'train_rates': 0.9180235086249457, 'learning_rate': 0.00022863942809719297, 'batch_size': 102, 'step_size': 9, 'gamma': 0.7871028750338313}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:34:57,889][0m Trial 35 finished with value: 0.04086274607972585 and parameters: {'observation_period_num': 42, 'train_rates': 0.9078962744403244, 'learning_rate': 0.0002088490980992541, 'batch_size': 63, 'step_size': 9, 'gamma': 0.7686976989856118}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:36:33,169][0m Trial 36 finished with value: 0.056559436435576614 and parameters: {'observation_period_num': 72, 'train_rates': 0.9091986900961508, 'learning_rate': 8.48381122007227e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.7896932114728995}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:39:43,436][0m Trial 37 finished with value: 0.05046119684754293 and parameters: {'observation_period_num': 43, 'train_rates': 0.9188558338210939, 'learning_rate': 3.272729545904271e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7702643878401801}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:40:33,959][0m Trial 38 finished with value: 0.05929248308963027 and parameters: {'observation_period_num': 62, 'train_rates': 0.9584863864905253, 'learning_rate': 0.00024325572092439128, 'batch_size': 103, 'step_size': 9, 'gamma': 0.8065756186268559}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:42:47,647][0m Trial 39 finished with value: 0.03664916031917587 and parameters: {'observation_period_num': 38, 'train_rates': 0.8954085900641011, 'learning_rate': 0.0001489835984575979, 'batch_size': 38, 'step_size': 8, 'gamma': 0.7859984140027584}. Best is trial 27 with value: 0.031265230758524526.[0m
[32m[I 2025-01-05 07:45:13,815][0m Trial 40 finished with value: 0.030977545881458603 and parameters: {'observation_period_num': 16, 'train_rates': 0.8700404739420563, 'learning_rate': 9.320639022296683e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7825574255273182}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 07:47:44,804][0m Trial 41 finished with value: 0.037206643020281906 and parameters: {'observation_period_num': 20, 'train_rates': 0.8695431632959297, 'learning_rate': 0.00010421873577798055, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7855340700590064}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 07:50:17,043][0m Trial 42 finished with value: 0.03156505967490375 and parameters: {'observation_period_num': 16, 'train_rates': 0.8695288026372651, 'learning_rate': 9.027487681771019e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7785759601264731}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 07:52:44,053][0m Trial 43 finished with value: 0.0333338854241993 and parameters: {'observation_period_num': 15, 'train_rates': 0.8664028258873315, 'learning_rate': 5.531988125674617e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8089592495430566}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 07:54:42,912][0m Trial 44 finished with value: 0.03227239344388183 and parameters: {'observation_period_num': 13, 'train_rates': 0.8405807626375326, 'learning_rate': 5.420993975254801e-05, 'batch_size': 45, 'step_size': 10, 'gamma': 0.8154671484131405}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 07:59:44,416][0m Trial 45 finished with value: 0.033515406913603794 and parameters: {'observation_period_num': 5, 'train_rates': 0.8100694952092715, 'learning_rate': 2.3816212356108694e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.810936262443497}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 08:01:41,360][0m Trial 46 finished with value: 0.03180692896770298 and parameters: {'observation_period_num': 14, 'train_rates': 0.8355020032445658, 'learning_rate': 5.687015436348506e-05, 'batch_size': 46, 'step_size': 10, 'gamma': 0.811792253734352}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 08:03:38,397][0m Trial 47 finished with value: 0.03945777759234193 and parameters: {'observation_period_num': 6, 'train_rates': 0.8365969695977715, 'learning_rate': 2.876207803142356e-05, 'batch_size': 46, 'step_size': 11, 'gamma': 0.8577087591978773}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 08:05:04,918][0m Trial 48 finished with value: 0.05083438689095089 and parameters: {'observation_period_num': 52, 'train_rates': 0.8600234716919367, 'learning_rate': 3.6429691737302895e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8847573866971198}. Best is trial 40 with value: 0.030977545881458603.[0m
[32m[I 2025-01-05 08:06:19,908][0m Trial 49 finished with value: 0.03324378079321032 and parameters: {'observation_period_num': 15, 'train_rates': 0.8324767807851551, 'learning_rate': 7.26332209343769e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.8198332415404912}. Best is trial 40 with value: 0.030977545881458603.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 08:06:19,918][0m A new study created in memory with name: no-name-af2a9375-ddc2-4f66-8f99-3b05cd1577b7[0m
[32m[I 2025-01-05 08:09:40,395][0m Trial 0 finished with value: 0.4372327213236724 and parameters: {'observation_period_num': 126, 'train_rates': 0.8169341329261359, 'learning_rate': 3.4783009673878094e-06, 'batch_size': 25, 'step_size': 3, 'gamma': 0.8181875996094002}. Best is trial 0 with value: 0.4372327213236724.[0m
[32m[I 2025-01-05 08:10:21,722][0m Trial 1 finished with value: 0.6733491635814155 and parameters: {'observation_period_num': 199, 'train_rates': 0.929726003191531, 'learning_rate': 3.544205620114502e-06, 'batch_size': 141, 'step_size': 7, 'gamma': 0.7661245062911155}. Best is trial 0 with value: 0.4372327213236724.[0m
[32m[I 2025-01-05 08:15:01,144][0m Trial 2 finished with value: 0.2356312469897156 and parameters: {'observation_period_num': 126, 'train_rates': 0.8821261757276204, 'learning_rate': 2.382107340815635e-06, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8442564683244868}. Best is trial 2 with value: 0.2356312469897156.[0m
[32m[I 2025-01-05 08:15:28,390][0m Trial 3 finished with value: 0.20110165125156546 and parameters: {'observation_period_num': 125, 'train_rates': 0.9158690541776817, 'learning_rate': 1.551723661341172e-05, 'batch_size': 233, 'step_size': 12, 'gamma': 0.9742930269440262}. Best is trial 3 with value: 0.20110165125156546.[0m
[32m[I 2025-01-05 08:16:21,984][0m Trial 4 finished with value: 0.6722123446789655 and parameters: {'observation_period_num': 240, 'train_rates': 0.9028128443812399, 'learning_rate': 2.0530871233475773e-06, 'batch_size': 102, 'step_size': 9, 'gamma': 0.803929925561494}. Best is trial 3 with value: 0.20110165125156546.[0m
[32m[I 2025-01-05 08:16:59,315][0m Trial 5 finished with value: 0.09461738169193268 and parameters: {'observation_period_num': 153, 'train_rates': 0.9841376120735512, 'learning_rate': 0.00025388708277518347, 'batch_size': 172, 'step_size': 4, 'gamma': 0.8274385998998697}. Best is trial 5 with value: 0.09461738169193268.[0m
[32m[I 2025-01-05 08:17:54,536][0m Trial 6 finished with value: 0.2430097982287407 and parameters: {'observation_period_num': 191, 'train_rates': 0.7968336489980827, 'learning_rate': 1.5368173389150418e-05, 'batch_size': 93, 'step_size': 9, 'gamma': 0.8780486694200901}. Best is trial 5 with value: 0.09461738169193268.[0m
[32m[I 2025-01-05 08:18:16,237][0m Trial 7 finished with value: 1.2601818649665169 and parameters: {'observation_period_num': 157, 'train_rates': 0.7864201565110919, 'learning_rate': 1.1550102231684925e-06, 'batch_size': 247, 'step_size': 13, 'gamma': 0.8195209087507148}. Best is trial 5 with value: 0.09461738169193268.[0m
[32m[I 2025-01-05 08:19:15,884][0m Trial 8 finished with value: 0.27538432732121293 and parameters: {'observation_period_num': 240, 'train_rates': 0.627587957768672, 'learning_rate': 0.0004437502203389592, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8520564469785006}. Best is trial 5 with value: 0.09461738169193268.[0m
[32m[I 2025-01-05 08:19:42,851][0m Trial 9 finished with value: 0.39631762274300775 and parameters: {'observation_period_num': 240, 'train_rates': 0.6386206927918917, 'learning_rate': 4.010956089681903e-05, 'batch_size': 169, 'step_size': 10, 'gamma': 0.8209637440587683}. Best is trial 5 with value: 0.09461738169193268.[0m
[32m[I 2025-01-05 08:20:17,937][0m Trial 10 finished with value: 0.031756214797496796 and parameters: {'observation_period_num': 15, 'train_rates': 0.9899685818728468, 'learning_rate': 0.0009599571124825946, 'batch_size': 185, 'step_size': 2, 'gamma': 0.9088085049067876}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:20:50,735][0m Trial 11 finished with value: 0.04420420899987221 and parameters: {'observation_period_num': 34, 'train_rates': 0.9842444313731364, 'learning_rate': 0.0008184731206366486, 'batch_size': 195, 'step_size': 1, 'gamma': 0.9108366766904116}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:21:23,595][0m Trial 12 finished with value: 0.03315130993723869 and parameters: {'observation_period_num': 7, 'train_rates': 0.9888856220007524, 'learning_rate': 0.000952120015418231, 'batch_size': 200, 'step_size': 2, 'gamma': 0.9168014781961882}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:21:54,245][0m Trial 13 finished with value: 0.04960819333791733 and parameters: {'observation_period_num': 30, 'train_rates': 0.9888025680405448, 'learning_rate': 0.00013454129279903235, 'batch_size': 212, 'step_size': 6, 'gamma': 0.9310350632453817}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:22:29,417][0m Trial 14 finished with value: 0.06265589090831139 and parameters: {'observation_period_num': 69, 'train_rates': 0.7349858428188419, 'learning_rate': 0.0009729403135571128, 'batch_size': 150, 'step_size': 2, 'gamma': 0.9358799697863123}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:22:59,153][0m Trial 15 finished with value: 0.07196803951497983 and parameters: {'observation_period_num': 73, 'train_rates': 0.8502325213474132, 'learning_rate': 0.0001072661195182412, 'batch_size': 203, 'step_size': 5, 'gamma': 0.8920245038340314}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:23:20,865][0m Trial 16 finished with value: 0.04241954518518257 and parameters: {'observation_period_num': 6, 'train_rates': 0.7208104977768627, 'learning_rate': 0.0003067766665764128, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9767129974292011}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:23:55,853][0m Trial 17 finished with value: 0.09338559955358505 and parameters: {'observation_period_num': 73, 'train_rates': 0.9439751794878614, 'learning_rate': 6.334217843533334e-05, 'batch_size': 179, 'step_size': 4, 'gamma': 0.9354172685342976}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:24:23,590][0m Trial 18 finished with value: 0.03925196386751582 and parameters: {'observation_period_num': 7, 'train_rates': 0.8760420824276941, 'learning_rate': 0.000469479286479442, 'batch_size': 222, 'step_size': 3, 'gamma': 0.9031133612266817}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:25:11,567][0m Trial 19 finished with value: 0.04689026722213724 and parameters: {'observation_period_num': 46, 'train_rates': 0.9376230486713318, 'learning_rate': 0.00017384849310985494, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9497689753453844}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:25:38,614][0m Trial 20 finished with value: 0.3785434493876022 and parameters: {'observation_period_num': 88, 'train_rates': 0.679160961296182, 'learning_rate': 1.7992595638611384e-05, 'batch_size': 187, 'step_size': 5, 'gamma': 0.8712503611898839}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:26:06,198][0m Trial 21 finished with value: 0.03238620281656634 and parameters: {'observation_period_num': 7, 'train_rates': 0.8672810084563927, 'learning_rate': 0.0005646861686324976, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9080212883216521}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:26:35,632][0m Trial 22 finished with value: 0.08329705893993378 and parameters: {'observation_period_num': 24, 'train_rates': 0.957734516659638, 'learning_rate': 0.0006595849924719652, 'batch_size': 228, 'step_size': 1, 'gamma': 0.91553690087406}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:27:11,443][0m Trial 23 finished with value: 0.04291491873406755 and parameters: {'observation_period_num': 46, 'train_rates': 0.8518506243756699, 'learning_rate': 0.0003603831438979315, 'batch_size': 159, 'step_size': 3, 'gamma': 0.889455850250278}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:27:42,279][0m Trial 24 finished with value: 0.07497934997081757 and parameters: {'observation_period_num': 98, 'train_rates': 0.9590943184802206, 'learning_rate': 0.0009575273808540599, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9569237921574761}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:28:09,428][0m Trial 25 finished with value: 0.0359521433106297 and parameters: {'observation_period_num': 7, 'train_rates': 0.8955645822308215, 'learning_rate': 0.00021008270281152873, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9234028473873581}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:28:39,401][0m Trial 26 finished with value: 0.0667846923413342 and parameters: {'observation_period_num': 55, 'train_rates': 0.849000805243811, 'learning_rate': 0.0005439424247097078, 'batch_size': 192, 'step_size': 2, 'gamma': 0.8536844873414736}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:29:05,258][0m Trial 27 finished with value: 0.048619271609289895 and parameters: {'observation_period_num': 25, 'train_rates': 0.7624175170886566, 'learning_rate': 8.582406573022306e-05, 'batch_size': 216, 'step_size': 4, 'gamma': 0.9579018245374326}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:29:56,821][0m Trial 28 finished with value: 0.04517888277769089 and parameters: {'observation_period_num': 54, 'train_rates': 0.9632253001335498, 'learning_rate': 0.0005469091519742036, 'batch_size': 119, 'step_size': 7, 'gamma': 0.8961126710109024}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:30:30,730][0m Trial 29 finished with value: 0.5693150723924731 and parameters: {'observation_period_num': 103, 'train_rates': 0.8230390166625053, 'learning_rate': 6.299987516487945e-06, 'batch_size': 157, 'step_size': 3, 'gamma': 0.87918751053591}. Best is trial 10 with value: 0.031756214797496796.[0m
[32m[I 2025-01-05 08:32:34,706][0m Trial 30 finished with value: 0.031031625252217053 and parameters: {'observation_period_num': 21, 'train_rates': 0.9183894153012183, 'learning_rate': 0.00026434279305932246, 'batch_size': 46, 'step_size': 2, 'gamma': 0.9459383607195632}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:34:21,962][0m Trial 31 finished with value: 0.0311529043553225 and parameters: {'observation_period_num': 23, 'train_rates': 0.91507119613603, 'learning_rate': 0.00030852747713681146, 'batch_size': 53, 'step_size': 2, 'gamma': 0.9880456312663077}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:36:22,639][0m Trial 32 finished with value: 0.03461156818920221 and parameters: {'observation_period_num': 24, 'train_rates': 0.9203625909641345, 'learning_rate': 0.00028139698245431105, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9626969049009995}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:39:03,239][0m Trial 33 finished with value: 0.0584716832222925 and parameters: {'observation_period_num': 37, 'train_rates': 0.8794739888642509, 'learning_rate': 0.00014815969199139347, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9819223132325938}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:40:33,232][0m Trial 34 finished with value: 0.0566604478427997 and parameters: {'observation_period_num': 63, 'train_rates': 0.9191807175828219, 'learning_rate': 5.613423346676618e-05, 'batch_size': 63, 'step_size': 4, 'gamma': 0.9412954107074468}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:43:28,452][0m Trial 35 finished with value: 0.03291842025825351 and parameters: {'observation_period_num': 19, 'train_rates': 0.8975512112296076, 'learning_rate': 0.000357944298630209, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9896809213527133}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:44:48,060][0m Trial 36 finished with value: 0.038586833789478074 and parameters: {'observation_period_num': 48, 'train_rates': 0.824409760326379, 'learning_rate': 0.00020133173053151895, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9696095603610482}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:46:30,701][0m Trial 37 finished with value: 0.032885142466949145 and parameters: {'observation_period_num': 38, 'train_rates': 0.8654832272305351, 'learning_rate': 0.0006469680041555129, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9488650798898264}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:47:27,890][0m Trial 38 finished with value: 0.11144534813821631 and parameters: {'observation_period_num': 87, 'train_rates': 0.9342091344404613, 'learning_rate': 0.0003898704625326098, 'batch_size': 100, 'step_size': 8, 'gamma': 0.9065675042466963}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:53:02,112][0m Trial 39 finished with value: 0.06388136197745574 and parameters: {'observation_period_num': 135, 'train_rates': 0.9067798457469182, 'learning_rate': 2.399551580549955e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7636809646238816}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:53:43,445][0m Trial 40 finished with value: 0.10958310961723328 and parameters: {'observation_period_num': 193, 'train_rates': 0.9585595425264976, 'learning_rate': 9.477933976244169e-05, 'batch_size': 141, 'step_size': 15, 'gamma': 0.9672328446003398}. Best is trial 30 with value: 0.031031625252217053.[0m
[32m[I 2025-01-05 08:54:52,829][0m Trial 41 finished with value: 0.024509217805014208 and parameters: {'observation_period_num': 16, 'train_rates': 0.8586086135426391, 'learning_rate': 0.00071731686769784, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9487245629432022}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 08:55:54,723][0m Trial 42 finished with value: 0.03406278512972806 and parameters: {'observation_period_num': 14, 'train_rates': 0.8098044734758273, 'learning_rate': 0.0006354900091090862, 'batch_size': 85, 'step_size': 3, 'gamma': 0.78218583146433}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 08:57:00,348][0m Trial 43 finished with value: 0.02997569247548069 and parameters: {'observation_period_num': 18, 'train_rates': 0.8379287262554754, 'learning_rate': 0.0002496852401277708, 'batch_size': 83, 'step_size': 2, 'gamma': 0.9243418440050929}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 08:58:02,659][0m Trial 44 finished with value: 0.28351399405897604 and parameters: {'observation_period_num': 35, 'train_rates': 0.7829305125423932, 'learning_rate': 6.773660479369121e-06, 'batch_size': 84, 'step_size': 2, 'gamma': 0.9214908676242842}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 08:58:48,846][0m Trial 45 finished with value: 0.11837587001519215 and parameters: {'observation_period_num': 213, 'train_rates': 0.8399462544072389, 'learning_rate': 0.00025987085394950165, 'batch_size': 112, 'step_size': 1, 'gamma': 0.9464349447503637}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 09:00:49,416][0m Trial 46 finished with value: 0.03262945062839068 and parameters: {'observation_period_num': 18, 'train_rates': 0.889626933372036, 'learning_rate': 0.0007433277590837617, 'batch_size': 46, 'step_size': 4, 'gamma': 0.926228494181603}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 09:01:57,016][0m Trial 47 finished with value: 0.033554501808393075 and parameters: {'observation_period_num': 31, 'train_rates': 0.8347877641137651, 'learning_rate': 0.00023441345865297052, 'batch_size': 81, 'step_size': 2, 'gamma': 0.989872181495279}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 09:03:04,857][0m Trial 48 finished with value: 0.0412693244673438 and parameters: {'observation_period_num': 43, 'train_rates': 0.7973555248316864, 'learning_rate': 0.00012395259716883474, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9752936829076548}. Best is trial 41 with value: 0.024509217805014208.[0m
[32m[I 2025-01-05 09:04:46,057][0m Trial 49 finished with value: 0.046470638583688176 and parameters: {'observation_period_num': 58, 'train_rates': 0.9120648937235657, 'learning_rate': 0.00047073917033007895, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9337340826243236}. Best is trial 41 with value: 0.024509217805014208.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 09:04:46,067][0m A new study created in memory with name: no-name-21faf67c-8f5a-4235-b09c-66f5f81279f3[0m
[32m[I 2025-01-05 09:06:14,494][0m Trial 0 finished with value: 0.15207491005802976 and parameters: {'observation_period_num': 128, 'train_rates': 0.7744933473970383, 'learning_rate': 3.647159068984127e-05, 'batch_size': 55, 'step_size': 2, 'gamma': 0.9028978514767096}. Best is trial 0 with value: 0.15207491005802976.[0m
[32m[I 2025-01-05 09:06:49,019][0m Trial 1 finished with value: 0.14792146300201986 and parameters: {'observation_period_num': 236, 'train_rates': 0.8523389228997398, 'learning_rate': 0.0004998445393436299, 'batch_size': 162, 'step_size': 11, 'gamma': 0.975426454128304}. Best is trial 1 with value: 0.14792146300201986.[0m
[32m[I 2025-01-05 09:07:30,669][0m Trial 2 finished with value: 0.2631231604710869 and parameters: {'observation_period_num': 239, 'train_rates': 0.6188217291382401, 'learning_rate': 6.413098193435961e-05, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9222521379139824}. Best is trial 1 with value: 0.14792146300201986.[0m
[32m[I 2025-01-05 09:09:24,708][0m Trial 3 finished with value: 0.34628300545712576 and parameters: {'observation_period_num': 117, 'train_rates': 0.7328477087718998, 'learning_rate': 1.0726416428685485e-05, 'batch_size': 41, 'step_size': 4, 'gamma': 0.7905128772687877}. Best is trial 1 with value: 0.14792146300201986.[0m
[32m[I 2025-01-05 09:11:26,244][0m Trial 4 finished with value: 0.4704366808629218 and parameters: {'observation_period_num': 163, 'train_rates': 0.906174527972818, 'learning_rate': 1.3363753710744688e-06, 'batch_size': 44, 'step_size': 15, 'gamma': 0.8668109526629927}. Best is trial 1 with value: 0.14792146300201986.[0m
[32m[I 2025-01-05 09:12:02,198][0m Trial 5 finished with value: 0.14090048173228567 and parameters: {'observation_period_num': 32, 'train_rates': 0.8781590269167423, 'learning_rate': 1.630872553114006e-05, 'batch_size': 169, 'step_size': 12, 'gamma': 0.827064630190575}. Best is trial 5 with value: 0.14090048173228567.[0m
[32m[I 2025-01-05 09:12:32,310][0m Trial 6 finished with value: 0.08226965022021236 and parameters: {'observation_period_num': 71, 'train_rates': 0.6861284061680599, 'learning_rate': 0.0006942028709440015, 'batch_size': 165, 'step_size': 9, 'gamma': 0.7989160484207607}. Best is trial 6 with value: 0.08226965022021236.[0m
[32m[I 2025-01-05 09:12:57,343][0m Trial 7 finished with value: 0.05414796188359952 and parameters: {'observation_period_num': 70, 'train_rates': 0.9094822061446135, 'learning_rate': 0.0009183854516270928, 'batch_size': 251, 'step_size': 5, 'gamma': 0.877595409016265}. Best is trial 7 with value: 0.05414796188359952.[0m
[32m[I 2025-01-05 09:13:48,663][0m Trial 8 finished with value: 0.1187805963703726 and parameters: {'observation_period_num': 71, 'train_rates': 0.7809617003037346, 'learning_rate': 9.291514859247954e-05, 'batch_size': 106, 'step_size': 3, 'gamma': 0.799943377670225}. Best is trial 7 with value: 0.05414796188359952.[0m
[32m[I 2025-01-05 09:14:13,126][0m Trial 9 finished with value: 0.10416956254857815 and parameters: {'observation_period_num': 151, 'train_rates': 0.8220094363613373, 'learning_rate': 8.379468866425445e-05, 'batch_size': 234, 'step_size': 11, 'gamma': 0.951985486268693}. Best is trial 7 with value: 0.05414796188359952.[0m
[32m[I 2025-01-05 09:14:40,279][0m Trial 10 finished with value: 0.0392618402838707 and parameters: {'observation_period_num': 10, 'train_rates': 0.9784958536336408, 'learning_rate': 0.000331578800693056, 'batch_size': 255, 'step_size': 6, 'gamma': 0.8675052308463348}. Best is trial 10 with value: 0.0392618402838707.[0m
[32m[I 2025-01-05 09:15:07,515][0m Trial 11 finished with value: 0.048971205949783325 and parameters: {'observation_period_num': 21, 'train_rates': 0.9825365215949456, 'learning_rate': 0.0002721039915970202, 'batch_size': 253, 'step_size': 6, 'gamma': 0.8564719942325651}. Best is trial 10 with value: 0.0392618402838707.[0m
[32m[I 2025-01-05 09:15:38,377][0m Trial 12 finished with value: 0.03762373328208923 and parameters: {'observation_period_num': 12, 'train_rates': 0.9825613686155675, 'learning_rate': 0.00032605842163209984, 'batch_size': 216, 'step_size': 7, 'gamma': 0.8492059858240169}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:16:09,418][0m Trial 13 finished with value: 0.04270145297050476 and parameters: {'observation_period_num': 12, 'train_rates': 0.9749474994842804, 'learning_rate': 0.00020722340491782402, 'batch_size': 210, 'step_size': 7, 'gamma': 0.8374027953694332}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:16:42,536][0m Trial 14 finished with value: 0.06875123828649521 and parameters: {'observation_period_num': 47, 'train_rates': 0.9584320134002562, 'learning_rate': 0.00022096372739814217, 'batch_size': 195, 'step_size': 8, 'gamma': 0.7531291879312438}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:17:11,641][0m Trial 15 finished with value: 0.5482426661032217 and parameters: {'observation_period_num': 98, 'train_rates': 0.9244356799859029, 'learning_rate': 3.7051357072612986e-06, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9020969096206666}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:17:38,576][0m Trial 16 finished with value: 0.15041233599185944 and parameters: {'observation_period_num': 184, 'train_rates': 0.9485657146556697, 'learning_rate': 0.00015037075552594341, 'batch_size': 225, 'step_size': 6, 'gamma': 0.8387893993839337}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:18:10,285][0m Trial 17 finished with value: 0.039668369444384094 and parameters: {'observation_period_num': 46, 'train_rates': 0.8578630448571154, 'learning_rate': 0.00040847817510541204, 'batch_size': 190, 'step_size': 4, 'gamma': 0.8934188912928747}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:18:55,873][0m Trial 18 finished with value: 0.1236067963533165 and parameters: {'observation_period_num': 8, 'train_rates': 0.9179577819413853, 'learning_rate': 3.089526317689855e-05, 'batch_size': 133, 'step_size': 1, 'gamma': 0.9299055575672865}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:19:22,916][0m Trial 19 finished with value: 0.06805167347192764 and parameters: {'observation_period_num': 92, 'train_rates': 0.9861845065184367, 'learning_rate': 0.0009038868862336084, 'batch_size': 252, 'step_size': 8, 'gamma': 0.7638890389787202}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:19:58,856][0m Trial 20 finished with value: 0.17893330387012688 and parameters: {'observation_period_num': 198, 'train_rates': 0.6970483749165642, 'learning_rate': 0.00035837313995409225, 'batch_size': 135, 'step_size': 13, 'gamma': 0.8198109531916887}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:20:30,696][0m Trial 21 finished with value: 0.03857295424570136 and parameters: {'observation_period_num': 42, 'train_rates': 0.8511389568353114, 'learning_rate': 0.00038236356436210494, 'batch_size': 188, 'step_size': 4, 'gamma': 0.8868009001243152}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:21:03,535][0m Trial 22 finished with value: 0.061243459582328796 and parameters: {'observation_period_num': 48, 'train_rates': 0.9505141126937584, 'learning_rate': 0.00013097434713554525, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8651977426870185}. Best is trial 12 with value: 0.03762373328208923.[0m
[32m[I 2025-01-05 09:21:29,457][0m Trial 23 finished with value: 0.035220005979067284 and parameters: {'observation_period_num': 5, 'train_rates': 0.8162280661510584, 'learning_rate': 0.0004807403323532519, 'batch_size': 224, 'step_size': 4, 'gamma': 0.8801807474394673}. Best is trial 23 with value: 0.035220005979067284.[0m
[32m[I 2025-01-05 09:21:55,497][0m Trial 24 finished with value: 0.038252904766002564 and parameters: {'observation_period_num': 41, 'train_rates': 0.8123633257848147, 'learning_rate': 0.0006057094650432416, 'batch_size': 226, 'step_size': 4, 'gamma': 0.921825040153023}. Best is trial 23 with value: 0.035220005979067284.[0m
[32m[I 2025-01-05 09:22:21,246][0m Trial 25 finished with value: 0.03306172779040005 and parameters: {'observation_period_num': 28, 'train_rates': 0.8183590440026183, 'learning_rate': 0.0006785069007802569, 'batch_size': 223, 'step_size': 3, 'gamma': 0.9253317017056137}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:22:47,481][0m Trial 26 finished with value: 0.04658304327762032 and parameters: {'observation_period_num': 26, 'train_rates': 0.7432872442124101, 'learning_rate': 0.0009860481798450445, 'batch_size': 205, 'step_size': 1, 'gamma': 0.9541823133921493}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:23:10,406][0m Trial 27 finished with value: 0.08697982046670048 and parameters: {'observation_period_num': 64, 'train_rates': 0.658956521807512, 'learning_rate': 0.0001588601810528593, 'batch_size': 233, 'step_size': 3, 'gamma': 0.9877964817070326}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:23:41,037][0m Trial 28 finished with value: 0.1501727560548172 and parameters: {'observation_period_num': 94, 'train_rates': 0.7515427603446796, 'learning_rate': 5.83759601594905e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8491148875302704}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:24:17,431][0m Trial 29 finished with value: 0.08403381744497701 and parameters: {'observation_period_num': 7, 'train_rates': 0.8066598697983678, 'learning_rate': 2.905731872154776e-05, 'batch_size': 153, 'step_size': 2, 'gamma': 0.9101124745102841}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:25:30,555][0m Trial 30 finished with value: 0.10191988112690213 and parameters: {'observation_period_num': 128, 'train_rates': 0.8822130474740726, 'learning_rate': 0.0005651626841785549, 'batch_size': 75, 'step_size': 2, 'gamma': 0.9513758813120213}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:29:59,705][0m Trial 31 finished with value: 0.03602050142345108 and parameters: {'observation_period_num': 31, 'train_rates': 0.8191080496326939, 'learning_rate': 0.0006023212675865185, 'batch_size': 19, 'step_size': 3, 'gamma': 0.928966784844457}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:33:53,567][0m Trial 32 finished with value: 0.06014522083907454 and parameters: {'observation_period_num': 55, 'train_rates': 0.8339477790605321, 'learning_rate': 0.0005530501524302975, 'batch_size': 22, 'step_size': 3, 'gamma': 0.9310404374430327}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:34:58,346][0m Trial 33 finished with value: 0.03419238044451719 and parameters: {'observation_period_num': 29, 'train_rates': 0.7811282304061483, 'learning_rate': 0.0002622333587155914, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9085738789487456}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:36:09,781][0m Trial 34 finished with value: 0.03324160348541988 and parameters: {'observation_period_num': 29, 'train_rates': 0.7825931081678883, 'learning_rate': 0.00022038041651255725, 'batch_size': 73, 'step_size': 5, 'gamma': 0.9095052979180358}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:37:16,976][0m Trial 35 finished with value: 0.07241991369260682 and parameters: {'observation_period_num': 83, 'train_rates': 0.7682672783393444, 'learning_rate': 5.4852504987926254e-05, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9016158571566568}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:38:08,008][0m Trial 36 finished with value: 0.04480330654586427 and parameters: {'observation_period_num': 27, 'train_rates': 0.7174393261569314, 'learning_rate': 0.00011174896288464601, 'batch_size': 98, 'step_size': 5, 'gamma': 0.9170959083817548}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:38:53,495][0m Trial 37 finished with value: 0.06166898636412581 and parameters: {'observation_period_num': 108, 'train_rates': 0.7904864630471108, 'learning_rate': 0.00022294060701649358, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9385949373630355}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:40:02,753][0m Trial 38 finished with value: 0.262524944005741 and parameters: {'observation_period_num': 227, 'train_rates': 0.7623774560166291, 'learning_rate': 1.608881229876786e-05, 'batch_size': 69, 'step_size': 7, 'gamma': 0.8851332117913743}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:41:34,751][0m Trial 39 finished with value: 0.19701953647566622 and parameters: {'observation_period_num': 21, 'train_rates': 0.792083423211728, 'learning_rate': 1.8587589781923815e-06, 'batch_size': 56, 'step_size': 4, 'gamma': 0.969430500657571}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:43:38,590][0m Trial 40 finished with value: 0.08918656869680992 and parameters: {'observation_period_num': 63, 'train_rates': 0.7230064457476754, 'learning_rate': 0.00018300928799072532, 'batch_size': 38, 'step_size': 9, 'gamma': 0.9102061825833362}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:48:41,606][0m Trial 41 finished with value: 0.034084773020500815 and parameters: {'observation_period_num': 36, 'train_rates': 0.8317073399163264, 'learning_rate': 0.0005185175091371175, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9420178714568973}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:49:44,843][0m Trial 42 finished with value: 0.0332345889166814 and parameters: {'observation_period_num': 36, 'train_rates': 0.8501772688908213, 'learning_rate': 0.0004304305386607834, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9439624611200631}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:50:44,361][0m Trial 43 finished with value: 0.048252461746530244 and parameters: {'observation_period_num': 79, 'train_rates': 0.8409672297811674, 'learning_rate': 0.0007616354657684404, 'batch_size': 90, 'step_size': 1, 'gamma': 0.9690869139454412}. Best is trial 25 with value: 0.03306172779040005.[0m
[32m[I 2025-01-05 09:52:22,099][0m Trial 44 finished with value: 0.03223111216720068 and parameters: {'observation_period_num': 37, 'train_rates': 0.867330621222478, 'learning_rate': 0.00027141777063759126, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9444974498560493}. Best is trial 44 with value: 0.03223111216720068.[0m
[32m[I 2025-01-05 09:54:02,900][0m Trial 45 finished with value: 0.0394144537616266 and parameters: {'observation_period_num': 56, 'train_rates': 0.8742739130771324, 'learning_rate': 9.035901067924063e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.9411106317073815}. Best is trial 44 with value: 0.03223111216720068.[0m
[32m[I 2025-01-05 09:56:55,108][0m Trial 46 finished with value: 0.038331197721630995 and parameters: {'observation_period_num': 37, 'train_rates': 0.8913279783430197, 'learning_rate': 0.0004416410720217227, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9464593976069133}. Best is trial 44 with value: 0.03223111216720068.[0m
[32m[I 2025-01-05 09:58:27,383][0m Trial 47 finished with value: 0.11447041275103886 and parameters: {'observation_period_num': 139, 'train_rates': 0.8670137272949234, 'learning_rate': 0.00029860493414913874, 'batch_size': 58, 'step_size': 3, 'gamma': 0.9637139883683612}. Best is trial 44 with value: 0.03223111216720068.[0m
[32m[I 2025-01-05 09:59:16,883][0m Trial 48 finished with value: 0.07841988567352556 and parameters: {'observation_period_num': 61, 'train_rates': 0.8422137876696265, 'learning_rate': 0.0007702685297965843, 'batch_size': 113, 'step_size': 2, 'gamma': 0.9857194033275444}. Best is trial 44 with value: 0.03223111216720068.[0m
[32m[I 2025-01-05 10:01:12,872][0m Trial 49 finished with value: 0.12886923296710615 and parameters: {'observation_period_num': 17, 'train_rates': 0.8309525857909578, 'learning_rate': 7.31278345978685e-06, 'batch_size': 46, 'step_size': 1, 'gamma': 0.9621232085685217}. Best is trial 44 with value: 0.03223111216720068.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 10:01:12,883][0m A new study created in memory with name: no-name-2266fe22-f6ce-4d34-a60d-a5c736cbb7d9[0m
[32m[I 2025-01-05 10:02:04,868][0m Trial 0 finished with value: 0.10490080091670517 and parameters: {'observation_period_num': 213, 'train_rates': 0.8648051128118739, 'learning_rate': 7.118444449688703e-05, 'batch_size': 102, 'step_size': 14, 'gamma': 0.8778411142791243}. Best is trial 0 with value: 0.10490080091670517.[0m
[32m[I 2025-01-05 10:02:38,023][0m Trial 1 finished with value: 0.19450099766254425 and parameters: {'observation_period_num': 20, 'train_rates': 0.9619079955775752, 'learning_rate': 6.330620523476506e-05, 'batch_size': 192, 'step_size': 1, 'gamma': 0.9215008650194894}. Best is trial 0 with value: 0.10490080091670517.[0m
[32m[I 2025-01-05 10:03:11,961][0m Trial 2 finished with value: 0.2758565232338243 and parameters: {'observation_period_num': 10, 'train_rates': 0.6356531251731983, 'learning_rate': 1.457616051621382e-05, 'batch_size': 137, 'step_size': 1, 'gamma': 0.8921362928164501}. Best is trial 0 with value: 0.10490080091670517.[0m
[32m[I 2025-01-05 10:04:13,904][0m Trial 3 finished with value: 0.17752638972673704 and parameters: {'observation_period_num': 13, 'train_rates': 0.6174502814082525, 'learning_rate': 4.8125127152856125e-06, 'batch_size': 72, 'step_size': 11, 'gamma': 0.7824445886829289}. Best is trial 0 with value: 0.10490080091670517.[0m
[32m[I 2025-01-05 10:04:39,272][0m Trial 4 finished with value: 0.28701999258995053 and parameters: {'observation_period_num': 99, 'train_rates': 0.9127414087606809, 'learning_rate': 1.338786754535432e-05, 'batch_size': 241, 'step_size': 14, 'gamma': 0.7820653438439024}. Best is trial 0 with value: 0.10490080091670517.[0m
[32m[I 2025-01-05 10:06:41,054][0m Trial 5 finished with value: 0.09160517105489101 and parameters: {'observation_period_num': 185, 'train_rates': 0.8514813679073874, 'learning_rate': 0.0001968935192697932, 'batch_size': 42, 'step_size': 3, 'gamma': 0.7946071049966081}. Best is trial 5 with value: 0.09160517105489101.[0m
[32m[I 2025-01-05 10:07:56,881][0m Trial 6 finished with value: 0.6914290189743042 and parameters: {'observation_period_num': 167, 'train_rates': 0.9871030307837906, 'learning_rate': 1.2790876139252745e-06, 'batch_size': 77, 'step_size': 12, 'gamma': 0.8118305422882758}. Best is trial 5 with value: 0.09160517105489101.[0m
[32m[I 2025-01-05 10:10:10,958][0m Trial 7 finished with value: 0.5970745043523703 and parameters: {'observation_period_num': 209, 'train_rates': 0.6200547958785012, 'learning_rate': 5.207721921032705e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8581262940284844}. Best is trial 5 with value: 0.09160517105489101.[0m
[32m[I 2025-01-05 10:11:59,002][0m Trial 8 finished with value: 0.425958733115576 and parameters: {'observation_period_num': 127, 'train_rates': 0.7604550311980357, 'learning_rate': 1.1137046160976183e-05, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8140789522522472}. Best is trial 5 with value: 0.09160517105489101.[0m
[32m[I 2025-01-05 10:12:47,090][0m Trial 9 finished with value: 0.21031582410688754 and parameters: {'observation_period_num': 205, 'train_rates': 0.91188647576311, 'learning_rate': 2.0018205855899185e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.9127714400423584}. Best is trial 5 with value: 0.09160517105489101.[0m
[32m[I 2025-01-05 10:13:17,843][0m Trial 10 finished with value: 0.06707089043905697 and parameters: {'observation_period_num': 82, 'train_rates': 0.7597079526591594, 'learning_rate': 0.0006633581198443671, 'batch_size': 168, 'step_size': 5, 'gamma': 0.9848872510460726}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:13:50,419][0m Trial 11 finished with value: 0.0681922029307548 and parameters: {'observation_period_num': 77, 'train_rates': 0.7551344661617974, 'learning_rate': 0.0006059348225525331, 'batch_size': 166, 'step_size': 5, 'gamma': 0.7562786188266828}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:14:21,440][0m Trial 12 finished with value: 0.07937458844316916 and parameters: {'observation_period_num': 67, 'train_rates': 0.723364013504797, 'learning_rate': 0.0009620604896099874, 'batch_size': 169, 'step_size': 6, 'gamma': 0.9830215299162373}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:14:48,618][0m Trial 13 finished with value: 0.161366741378586 and parameters: {'observation_period_num': 68, 'train_rates': 0.7067805525743278, 'learning_rate': 0.0009156651898660469, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9802103744293654}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:15:20,574][0m Trial 14 finished with value: 0.06888404053631168 and parameters: {'observation_period_num': 72, 'train_rates': 0.7949793371028437, 'learning_rate': 0.00027330113471799646, 'batch_size': 168, 'step_size': 5, 'gamma': 0.7529349595154462}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:15:41,805][0m Trial 15 finished with value: 0.16022704541683197 and parameters: {'observation_period_num': 131, 'train_rates': 0.6907663246149193, 'learning_rate': 0.0002895056222816397, 'batch_size': 247, 'step_size': 8, 'gamma': 0.9485827864666355}. Best is trial 10 with value: 0.06707089043905697.[0m
[32m[I 2025-01-05 10:16:18,848][0m Trial 16 finished with value: 0.04193633093073034 and parameters: {'observation_period_num': 42, 'train_rates': 0.7964464687538088, 'learning_rate': 0.0003990195706728083, 'batch_size': 147, 'step_size': 4, 'gamma': 0.8447524407117967}. Best is trial 16 with value: 0.04193633093073034.[0m
[32m[I 2025-01-05 10:16:58,984][0m Trial 17 finished with value: 0.04274472050959644 and parameters: {'observation_period_num': 40, 'train_rates': 0.8228211121498498, 'learning_rate': 0.0001398902147033581, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8477741619955128}. Best is trial 16 with value: 0.04193633093073034.[0m
[32m[I 2025-01-05 10:17:41,101][0m Trial 18 finished with value: 0.03955670599090426 and parameters: {'observation_period_num': 34, 'train_rates': 0.8376206340892641, 'learning_rate': 0.000139104092843557, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8446227122081712}. Best is trial 18 with value: 0.03955670599090426.[0m
[32m[I 2025-01-05 10:18:09,964][0m Trial 19 finished with value: 0.04563694468858402 and parameters: {'observation_period_num': 41, 'train_rates': 0.8875268495586248, 'learning_rate': 0.0003680963507682687, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8421474927104786}. Best is trial 18 with value: 0.03955670599090426.[0m
[32m[I 2025-01-05 10:19:03,041][0m Trial 20 finished with value: 0.11748102076957166 and parameters: {'observation_period_num': 110, 'train_rates': 0.8011447297979061, 'learning_rate': 0.00010806047889098988, 'batch_size': 99, 'step_size': 3, 'gamma': 0.8311901509800902}. Best is trial 18 with value: 0.03955670599090426.[0m
[32m[I 2025-01-05 10:19:42,853][0m Trial 21 finished with value: 0.038236137797556274 and parameters: {'observation_period_num': 32, 'train_rates': 0.8319223630648608, 'learning_rate': 0.00014207606663358216, 'batch_size': 139, 'step_size': 9, 'gamma': 0.8560935331987892}. Best is trial 21 with value: 0.038236137797556274.[0m
[32m[I 2025-01-05 10:20:20,801][0m Trial 22 finished with value: 0.14409969967479508 and parameters: {'observation_period_num': 249, 'train_rates': 0.8225667742799897, 'learning_rate': 0.0001062729877699155, 'batch_size': 133, 'step_size': 9, 'gamma': 0.8760462453197873}. Best is trial 21 with value: 0.038236137797556274.[0m
[32m[I 2025-01-05 10:21:00,844][0m Trial 23 finished with value: 0.10233941627770064 and parameters: {'observation_period_num': 40, 'train_rates': 0.8461393958356338, 'learning_rate': 3.30327828734541e-05, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8284647583317116}. Best is trial 21 with value: 0.038236137797556274.[0m
[32m[I 2025-01-05 10:21:50,007][0m Trial 24 finished with value: 0.04815211191145489 and parameters: {'observation_period_num': 45, 'train_rates': 0.7880138264855837, 'learning_rate': 0.00043900014727341616, 'batch_size': 107, 'step_size': 12, 'gamma': 0.8990383835837041}. Best is trial 21 with value: 0.038236137797556274.[0m
[32m[I 2025-01-05 10:23:04,516][0m Trial 25 finished with value: 0.035420992054337824 and parameters: {'observation_period_num': 26, 'train_rates': 0.8870900274336732, 'learning_rate': 0.00021547623691193206, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8613568662757453}. Best is trial 25 with value: 0.035420992054337824.[0m
[32m[I 2025-01-05 10:24:18,666][0m Trial 26 finished with value: 0.04323777862497278 and parameters: {'observation_period_num': 27, 'train_rates': 0.9368503580266113, 'learning_rate': 0.00015650513403440657, 'batch_size': 79, 'step_size': 10, 'gamma': 0.8645526654096783}. Best is trial 25 with value: 0.035420992054337824.[0m
[32m[I 2025-01-05 10:25:44,350][0m Trial 27 finished with value: 0.05590552526215712 and parameters: {'observation_period_num': 58, 'train_rates': 0.869814021010114, 'learning_rate': 3.185221888773862e-05, 'batch_size': 63, 'step_size': 9, 'gamma': 0.8874398556045352}. Best is trial 25 with value: 0.035420992054337824.[0m
[32m[I 2025-01-05 10:26:35,858][0m Trial 28 finished with value: 0.03390502537141985 and parameters: {'observation_period_num': 5, 'train_rates': 0.8865765063229972, 'learning_rate': 0.00023141482397423364, 'batch_size': 116, 'step_size': 13, 'gamma': 0.933201522903996}. Best is trial 28 with value: 0.03390502537141985.[0m
[32m[I 2025-01-05 10:27:30,011][0m Trial 29 finished with value: 0.035714401768364774 and parameters: {'observation_period_num': 5, 'train_rates': 0.8907080584061896, 'learning_rate': 7.544748989852092e-05, 'batch_size': 106, 'step_size': 15, 'gamma': 0.944053229578905}. Best is trial 28 with value: 0.03390502537141985.[0m
[32m[I 2025-01-05 10:28:32,789][0m Trial 30 finished with value: 0.03614000024067031 and parameters: {'observation_period_num': 21, 'train_rates': 0.8897972262843604, 'learning_rate': 7.58810662318291e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9413602380065331}. Best is trial 28 with value: 0.03390502537141985.[0m
[32m[I 2025-01-05 10:29:35,241][0m Trial 31 finished with value: 0.03372766047493735 and parameters: {'observation_period_num': 14, 'train_rates': 0.886394265847912, 'learning_rate': 7.932940295433408e-05, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9401805483784882}. Best is trial 31 with value: 0.03372766047493735.[0m
[32m[I 2025-01-05 10:30:28,879][0m Trial 32 finished with value: 0.04581808364071415 and parameters: {'observation_period_num': 5, 'train_rates': 0.9402896303890546, 'learning_rate': 5.9343428942023106e-05, 'batch_size': 115, 'step_size': 14, 'gamma': 0.9536333974907403}. Best is trial 31 with value: 0.03372766047493735.[0m
[32m[I 2025-01-05 10:32:11,304][0m Trial 33 finished with value: 0.02815707226782955 and parameters: {'observation_period_num': 5, 'train_rates': 0.8773392884416733, 'learning_rate': 8.352850816800337e-05, 'batch_size': 55, 'step_size': 15, 'gamma': 0.9288429339945826}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:33:49,148][0m Trial 34 finished with value: 0.04617939283841936 and parameters: {'observation_period_num': 19, 'train_rates': 0.9311563705790241, 'learning_rate': 0.00022497724070155936, 'batch_size': 60, 'step_size': 13, 'gamma': 0.9210810252047532}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:34:58,288][0m Trial 35 finished with value: 0.07683190703392029 and parameters: {'observation_period_num': 56, 'train_rates': 0.9817983502876011, 'learning_rate': 3.6160298367297475e-05, 'batch_size': 88, 'step_size': 13, 'gamma': 0.9669775907292087}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:40:34,538][0m Trial 36 finished with value: 0.06649104143837009 and parameters: {'observation_period_num': 17, 'train_rates': 0.8683364800943376, 'learning_rate': 9.558413217307378e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9308579523608185}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:42:20,260][0m Trial 37 finished with value: 0.04282222886880239 and parameters: {'observation_period_num': 6, 'train_rates': 0.9596225927376589, 'learning_rate': 4.936637459263597e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9058987706740783}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:44:19,794][0m Trial 38 finished with value: 0.0745548691356044 and parameters: {'observation_period_num': 88, 'train_rates': 0.9099669827093975, 'learning_rate': 1.8482960116384966e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.9632764774729461}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:45:37,033][0m Trial 39 finished with value: 0.03327110059968105 and parameters: {'observation_period_num': 24, 'train_rates': 0.8632511764043056, 'learning_rate': 0.00019945085615359967, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8872328570476213}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:48:33,111][0m Trial 40 finished with value: 0.06421376182697713 and parameters: {'observation_period_num': 52, 'train_rates': 0.8566743581796449, 'learning_rate': 6.347489448151101e-06, 'batch_size': 30, 'step_size': 12, 'gamma': 0.9304005447314818}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:49:52,552][0m Trial 41 finished with value: 0.04438972130438752 and parameters: {'observation_period_num': 24, 'train_rates': 0.8785526758938408, 'learning_rate': 0.00020036897859899538, 'batch_size': 71, 'step_size': 11, 'gamma': 0.886523623526184}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:50:54,017][0m Trial 42 finished with value: 0.03331027475910054 and parameters: {'observation_period_num': 17, 'train_rates': 0.9082293664347196, 'learning_rate': 0.00027761604725959875, 'batch_size': 93, 'step_size': 14, 'gamma': 0.9204258347791506}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:51:57,927][0m Trial 43 finished with value: 0.13697528448261198 and parameters: {'observation_period_num': 156, 'train_rates': 0.913103062962337, 'learning_rate': 0.0005365000686050852, 'batch_size': 88, 'step_size': 15, 'gamma': 0.9121141421432334}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:52:59,776][0m Trial 44 finished with value: 0.27188099434842233 and parameters: {'observation_period_num': 12, 'train_rates': 0.9062702285137976, 'learning_rate': 1.2944684982440297e-06, 'batch_size': 94, 'step_size': 13, 'gamma': 0.929791645512817}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:54:48,066][0m Trial 45 finished with value: 0.052664907800184714 and parameters: {'observation_period_num': 19, 'train_rates': 0.9499143105441678, 'learning_rate': 4.812382734744719e-05, 'batch_size': 54, 'step_size': 14, 'gamma': 0.9192095954896841}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:55:36,282][0m Trial 46 finished with value: 0.04477739455805455 and parameters: {'observation_period_num': 53, 'train_rates': 0.8620331238445911, 'learning_rate': 8.198726208651333e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8963964236450978}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:58:22,408][0m Trial 47 finished with value: 0.05473785804756937 and parameters: {'observation_period_num': 30, 'train_rates': 0.9226299636431748, 'learning_rate': 0.00026332660141922045, 'batch_size': 34, 'step_size': 12, 'gamma': 0.9603414227406207}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 10:59:50,251][0m Trial 48 finished with value: 0.034898159967869824 and parameters: {'observation_period_num': 13, 'train_rates': 0.9670253946862085, 'learning_rate': 0.00033739738108847254, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9365403217838358}. Best is trial 33 with value: 0.02815707226782955.[0m
[32m[I 2025-01-05 11:00:37,975][0m Trial 49 finished with value: 0.08373630932551257 and parameters: {'observation_period_num': 62, 'train_rates': 0.8986774710099017, 'learning_rate': 2.2208921425825886e-05, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9033149499706}. Best is trial 33 with value: 0.02815707226782955.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 27, 'train_rates': 0.9893599021095496, 'learning_rate': 0.00011341307324445398, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9570948286432536}
Epoch 1/300, trend Loss: 0.2951 | 0.1967
Epoch 2/300, trend Loss: 0.1425 | 0.1383
Epoch 3/300, trend Loss: 0.1197 | 0.1060
Epoch 4/300, trend Loss: 0.1071 | 0.0870
Epoch 5/300, trend Loss: 0.0987 | 0.0757
Epoch 6/300, trend Loss: 0.0932 | 0.0707
Epoch 7/300, trend Loss: 0.0891 | 0.0724
Epoch 8/300, trend Loss: 0.0848 | 0.0737
Epoch 9/300, trend Loss: 0.0826 | 0.0723
Epoch 10/300, trend Loss: 0.0831 | 0.0715
Epoch 11/300, trend Loss: 0.0832 | 0.0632
Epoch 12/300, trend Loss: 0.0841 | 0.0497
Epoch 13/300, trend Loss: 0.0832 | 0.0570
Epoch 14/300, trend Loss: 0.0809 | 0.0619
Epoch 15/300, trend Loss: 0.0765 | 0.0581
Epoch 16/300, trend Loss: 0.0732 | 0.0539
Epoch 17/300, trend Loss: 0.0712 | 0.0500
Epoch 18/300, trend Loss: 0.0706 | 0.0493
Epoch 19/300, trend Loss: 0.0716 | 0.0551
Epoch 20/300, trend Loss: 0.0738 | 0.0627
Epoch 21/300, trend Loss: 0.0737 | 0.0552
Epoch 22/300, trend Loss: 0.0696 | 0.0533
Epoch 23/300, trend Loss: 0.0679 | 0.0510
Epoch 24/300, trend Loss: 0.0671 | 0.0466
Epoch 25/300, trend Loss: 0.0667 | 0.0428
Epoch 26/300, trend Loss: 0.0662 | 0.0406
Epoch 27/300, trend Loss: 0.0655 | 0.0390
Epoch 28/300, trend Loss: 0.0646 | 0.0375
Epoch 29/300, trend Loss: 0.0634 | 0.0363
Epoch 30/300, trend Loss: 0.0624 | 0.0356
Epoch 31/300, trend Loss: 0.0615 | 0.0351
Epoch 32/300, trend Loss: 0.0608 | 0.0348
Epoch 33/300, trend Loss: 0.0602 | 0.0344
Epoch 34/300, trend Loss: 0.0596 | 0.0341
Epoch 35/300, trend Loss: 0.0591 | 0.0339
Epoch 36/300, trend Loss: 0.0587 | 0.0337
Epoch 37/300, trend Loss: 0.0583 | 0.0337
Epoch 38/300, trend Loss: 0.0579 | 0.0337
Epoch 39/300, trend Loss: 0.0575 | 0.0337
Epoch 40/300, trend Loss: 0.0572 | 0.0338
Epoch 41/300, trend Loss: 0.0569 | 0.0339
Epoch 42/300, trend Loss: 0.0567 | 0.0341
Epoch 43/300, trend Loss: 0.0564 | 0.0344
Epoch 44/300, trend Loss: 0.0563 | 0.0348
Epoch 45/300, trend Loss: 0.0561 | 0.0347
Epoch 46/300, trend Loss: 0.0559 | 0.0344
Epoch 47/300, trend Loss: 0.0557 | 0.0339
Epoch 48/300, trend Loss: 0.0555 | 0.0334
Epoch 49/300, trend Loss: 0.0553 | 0.0330
Epoch 50/300, trend Loss: 0.0551 | 0.0324
Epoch 51/300, trend Loss: 0.0549 | 0.0321
Epoch 52/300, trend Loss: 0.0543 | 0.0315
Epoch 53/300, trend Loss: 0.0537 | 0.0308
Epoch 54/300, trend Loss: 0.0530 | 0.0302
Epoch 55/300, trend Loss: 0.0525 | 0.0296
Epoch 56/300, trend Loss: 0.0519 | 0.0289
Epoch 57/300, trend Loss: 0.0515 | 0.0283
Epoch 58/300, trend Loss: 0.0510 | 0.0276
Epoch 59/300, trend Loss: 0.0506 | 0.0269
Epoch 60/300, trend Loss: 0.0501 | 0.0261
Epoch 61/300, trend Loss: 0.0497 | 0.0251
Epoch 62/300, trend Loss: 0.0493 | 0.0240
Epoch 63/300, trend Loss: 0.0489 | 0.0227
Epoch 64/300, trend Loss: 0.0484 | 0.0213
Epoch 65/300, trend Loss: 0.0481 | 0.0199
Epoch 66/300, trend Loss: 0.0477 | 0.0185
Epoch 67/300, trend Loss: 0.0473 | 0.0171
Epoch 68/300, trend Loss: 0.0469 | 0.0157
Epoch 69/300, trend Loss: 0.0466 | 0.0146
Epoch 70/300, trend Loss: 0.0464 | 0.0141
Epoch 71/300, trend Loss: 0.0461 | 0.0147
Epoch 72/300, trend Loss: 0.0460 | 0.0154
Epoch 73/300, trend Loss: 0.0458 | 0.0166
Epoch 74/300, trend Loss: 0.0455 | 0.0156
Epoch 75/300, trend Loss: 0.0455 | 0.0210
Epoch 76/300, trend Loss: 0.0456 | 0.0167
Epoch 77/300, trend Loss: 0.0466 | 0.0195
Epoch 78/300, trend Loss: 0.0460 | 0.0158
Epoch 79/300, trend Loss: 0.0455 | 0.0145
Epoch 80/300, trend Loss: 0.0450 | 0.0148
Epoch 81/300, trend Loss: 0.0453 | 0.0168
Epoch 82/300, trend Loss: 0.0454 | 0.0173
Epoch 83/300, trend Loss: 0.0455 | 0.0172
Epoch 84/300, trend Loss: 0.0460 | 0.0175
Epoch 85/300, trend Loss: 0.0464 | 0.0175
Epoch 86/300, trend Loss: 0.0461 | 0.0175
Epoch 87/300, trend Loss: 0.0456 | 0.0173
Epoch 88/300, trend Loss: 0.0453 | 0.0172
Epoch 89/300, trend Loss: 0.0448 | 0.0172
Epoch 90/300, trend Loss: 0.0443 | 0.0172
Epoch 91/300, trend Loss: 0.0438 | 0.0171
Epoch 92/300, trend Loss: 0.0433 | 0.0167
Epoch 93/300, trend Loss: 0.0429 | 0.0166
Epoch 94/300, trend Loss: 0.0427 | 0.0183
Epoch 95/300, trend Loss: 0.0399 | 0.0172
Epoch 96/300, trend Loss: 0.0391 | 0.0179
Epoch 97/300, trend Loss: 0.0387 | 0.0179
Epoch 98/300, trend Loss: 0.0388 | 0.0174
Epoch 99/300, trend Loss: 0.0438 | 0.0195
Epoch 100/300, trend Loss: 0.0401 | 0.0208
Epoch 101/300, trend Loss: 0.0447 | 0.0227
Epoch 102/300, trend Loss: 0.0412 | 0.0328
Epoch 103/300, trend Loss: 0.0457 | 0.0447
Epoch 104/300, trend Loss: 0.0434 | 0.0265
Epoch 105/300, trend Loss: 0.0457 | 0.0243
Epoch 106/300, trend Loss: 0.0406 | 0.0201
Epoch 107/300, trend Loss: 0.0407 | 0.0182
Epoch 108/300, trend Loss: 0.0392 | 0.0175
Epoch 109/300, trend Loss: 0.0379 | 0.0176
Epoch 110/300, trend Loss: 0.0371 | 0.0176
Epoch 111/300, trend Loss: 0.0367 | 0.0184
Epoch 112/300, trend Loss: 0.0373 | 0.0190
Epoch 113/300, trend Loss: 0.0366 | 0.0198
Epoch 114/300, trend Loss: 0.0383 | 0.0216
Epoch 115/300, trend Loss: 0.0389 | 0.0201
Epoch 116/300, trend Loss: 0.0395 | 0.0213
Epoch 117/300, trend Loss: 0.0425 | 0.0200
Epoch 118/300, trend Loss: 0.0445 | 0.0209
Epoch 119/300, trend Loss: 0.0452 | 0.0441
Epoch 120/300, trend Loss: 0.0446 | 0.0539
Epoch 121/300, trend Loss: 0.0411 | 0.0229
Epoch 122/300, trend Loss: 0.0424 | 0.0172
Epoch 123/300, trend Loss: 0.0408 | 0.0175
Epoch 124/300, trend Loss: 0.0371 | 0.0168
Epoch 125/300, trend Loss: 0.0367 | 0.0176
Epoch 126/300, trend Loss: 0.0365 | 0.0171
Epoch 127/300, trend Loss: 0.0356 | 0.0173
Epoch 128/300, trend Loss: 0.0349 | 0.0175
Epoch 129/300, trend Loss: 0.0345 | 0.0177
Epoch 130/300, trend Loss: 0.0363 | 0.0179
Epoch 131/300, trend Loss: 0.0359 | 0.0184
Epoch 132/300, trend Loss: 0.0349 | 0.0185
Epoch 133/300, trend Loss: 0.0344 | 0.0183
Epoch 134/300, trend Loss: 0.0341 | 0.0181
Epoch 135/300, trend Loss: 0.0337 | 0.0180
Epoch 136/300, trend Loss: 0.0333 | 0.0179
Epoch 137/300, trend Loss: 0.0331 | 0.0177
Epoch 138/300, trend Loss: 0.0328 | 0.0174
Epoch 139/300, trend Loss: 0.0326 | 0.0173
Epoch 140/300, trend Loss: 0.0325 | 0.0168
Epoch 141/300, trend Loss: 0.0398 | 0.0172
Epoch 142/300, trend Loss: 0.0337 | 0.0175
Epoch 143/300, trend Loss: 0.0327 | 0.0171
Epoch 144/300, trend Loss: 0.0324 | 0.0170
Epoch 145/300, trend Loss: 0.0324 | 0.0172
Epoch 146/300, trend Loss: 0.0327 | 0.0171
Epoch 147/300, trend Loss: 0.0324 | 0.0172
Epoch 148/300, trend Loss: 0.0335 | 0.0204
Epoch 149/300, trend Loss: 0.0334 | 0.0213
Epoch 150/300, trend Loss: 0.0329 | 0.0216
Epoch 151/300, trend Loss: 0.0326 | 0.0208
Epoch 152/300, trend Loss: 0.0320 | 0.0200
Epoch 153/300, trend Loss: 0.0321 | 0.0199
Epoch 154/300, trend Loss: 0.0326 | 0.0192
Epoch 155/300, trend Loss: 0.0336 | 0.0168
Epoch 156/300, trend Loss: 0.0348 | 0.0180
Epoch 157/300, trend Loss: 0.0343 | 0.0190
Epoch 158/300, trend Loss: 0.0344 | 0.0197
Epoch 159/300, trend Loss: 0.0411 | 0.0168
Epoch 160/300, trend Loss: 0.0351 | 0.0156
Epoch 161/300, trend Loss: 0.0325 | 0.0160
Epoch 162/300, trend Loss: 0.0313 | 0.0180
Epoch 163/300, trend Loss: 0.0311 | 0.0160
Epoch 164/300, trend Loss: 0.0312 | 0.0167
Epoch 165/300, trend Loss: 0.0298 | 0.0171
Epoch 166/300, trend Loss: 0.0294 | 0.0169
Epoch 167/300, trend Loss: 0.0294 | 0.0172
Epoch 168/300, trend Loss: 0.0309 | 0.0180
Epoch 169/300, trend Loss: 0.0289 | 0.0173
Epoch 170/300, trend Loss: 0.0285 | 0.0177
Epoch 171/300, trend Loss: 0.0288 | 0.0175
Epoch 172/300, trend Loss: 0.0330 | 0.0176
Epoch 173/300, trend Loss: 0.0315 | 0.0178
Epoch 174/300, trend Loss: 0.0307 | 0.0173
Epoch 175/300, trend Loss: 0.0291 | 0.0187
Epoch 176/300, trend Loss: 0.0292 | 0.0171
Epoch 177/300, trend Loss: 0.0306 | 0.0192
Epoch 178/300, trend Loss: 0.0287 | 0.0169
Epoch 179/300, trend Loss: 0.0278 | 0.0170
Epoch 180/300, trend Loss: 0.0272 | 0.0171
Epoch 181/300, trend Loss: 0.0269 | 0.0173
Epoch 182/300, trend Loss: 0.0269 | 0.0176
Epoch 183/300, trend Loss: 0.0270 | 0.0173
Epoch 184/300, trend Loss: 0.0266 | 0.0187
Epoch 185/300, trend Loss: 0.0284 | 0.0175
Epoch 186/300, trend Loss: 0.0282 | 0.0183
Epoch 187/300, trend Loss: 0.0271 | 0.0168
Epoch 188/300, trend Loss: 0.0266 | 0.0173
Epoch 189/300, trend Loss: 0.0270 | 0.0184
Epoch 190/300, trend Loss: 0.0268 | 0.0191
Epoch 191/300, trend Loss: 0.0266 | 0.0184
Epoch 192/300, trend Loss: 0.0265 | 0.0177
Epoch 193/300, trend Loss: 0.0312 | 0.0204
Epoch 194/300, trend Loss: 0.0281 | 0.0171
Epoch 195/300, trend Loss: 0.0287 | 0.0199
Epoch 196/300, trend Loss: 0.0312 | 0.0202
Epoch 197/300, trend Loss: 0.0302 | 0.0173
Epoch 198/300, trend Loss: 0.0266 | 0.0170
Epoch 199/300, trend Loss: 0.0270 | 0.0184
Epoch 200/300, trend Loss: 0.0281 | 0.0180
Epoch 201/300, trend Loss: 0.0268 | 0.0176
Epoch 202/300, trend Loss: 0.0261 | 0.0176
Epoch 203/300, trend Loss: 0.0258 | 0.0178
Epoch 204/300, trend Loss: 0.0258 | 0.0182
Epoch 205/300, trend Loss: 0.0256 | 0.0183
Epoch 206/300, trend Loss: 0.0256 | 0.0185
Epoch 207/300, trend Loss: 0.0255 | 0.0187
Epoch 208/300, trend Loss: 0.0255 | 0.0190
Epoch 209/300, trend Loss: 0.0255 | 0.0193
Epoch 210/300, trend Loss: 0.0255 | 0.0195
Epoch 211/300, trend Loss: 0.0257 | 0.0197
Epoch 212/300, trend Loss: 0.0257 | 0.0197
Epoch 213/300, trend Loss: 0.0258 | 0.0198
Epoch 214/300, trend Loss: 0.0257 | 0.0197
Epoch 215/300, trend Loss: 0.0260 | 0.0199
Epoch 216/300, trend Loss: 0.0259 | 0.0197
Epoch 217/300, trend Loss: 0.0265 | 0.0212
Epoch 218/300, trend Loss: 0.0265 | 0.0184
Epoch 219/300, trend Loss: 0.0268 | 0.0224
Epoch 220/300, trend Loss: 0.0275 | 0.0195
Epoch 221/300, trend Loss: 0.0264 | 0.0193
Epoch 222/300, trend Loss: 0.0258 | 0.0193
Epoch 223/300, trend Loss: 0.0257 | 0.0197
Epoch 224/300, trend Loss: 0.0256 | 0.0192
Epoch 225/300, trend Loss: 0.0256 | 0.0199
Epoch 226/300, trend Loss: 0.0256 | 0.0195
Epoch 227/300, trend Loss: 0.0255 | 0.0206
Epoch 228/300, trend Loss: 0.0254 | 0.0205
Epoch 229/300, trend Loss: 0.0253 | 0.0215
Epoch 230/300, trend Loss: 0.0251 | 0.0223
Epoch 231/300, trend Loss: 0.0249 | 0.0231
Epoch 232/300, trend Loss: 0.0248 | 0.0250
Epoch 233/300, trend Loss: 0.0248 | 0.0226
Epoch 234/300, trend Loss: 0.0253 | 0.0178
Epoch 235/300, trend Loss: 0.0258 | 0.0174
Epoch 236/300, trend Loss: 0.0254 | 0.0170
Epoch 237/300, trend Loss: 0.0247 | 0.0184
Epoch 238/300, trend Loss: 0.0242 | 0.0180
Epoch 239/300, trend Loss: 0.0240 | 0.0184
Epoch 240/300, trend Loss: 0.0239 | 0.0177
Epoch 241/300, trend Loss: 0.0237 | 0.0179
Epoch 242/300, trend Loss: 0.0236 | 0.0176
Epoch 243/300, trend Loss: 0.0234 | 0.0179
Epoch 244/300, trend Loss: 0.0233 | 0.0179
Epoch 245/300, trend Loss: 0.0232 | 0.0181
Epoch 246/300, trend Loss: 0.0231 | 0.0182
Epoch 247/300, trend Loss: 0.0231 | 0.0184
Epoch 248/300, trend Loss: 0.0230 | 0.0183
Epoch 249/300, trend Loss: 0.0229 | 0.0185
Epoch 250/300, trend Loss: 0.0228 | 0.0183
Epoch 251/300, trend Loss: 0.0227 | 0.0185
Epoch 252/300, trend Loss: 0.0226 | 0.0183
Epoch 253/300, trend Loss: 0.0226 | 0.0186
Epoch 254/300, trend Loss: 0.0225 | 0.0183
Epoch 255/300, trend Loss: 0.0225 | 0.0185
Epoch 256/300, trend Loss: 0.0225 | 0.0186
Epoch 257/300, trend Loss: 0.0228 | 0.0193
Epoch 258/300, trend Loss: 0.0246 | 0.0186
Epoch 259/300, trend Loss: 0.0243 | 0.0182
Epoch 260/300, trend Loss: 0.0242 | 0.0216
Epoch 261/300, trend Loss: 0.0264 | 0.0182
Epoch 262/300, trend Loss: 0.0240 | 0.0200
Epoch 263/300, trend Loss: 0.0245 | 0.0180
Epoch 264/300, trend Loss: 0.0232 | 0.0211
Epoch 265/300, trend Loss: 0.0237 | 0.0183
Epoch 266/300, trend Loss: 0.0225 | 0.0192
Epoch 267/300, trend Loss: 0.0221 | 0.0186
Epoch 268/300, trend Loss: 0.0219 | 0.0187
Epoch 269/300, trend Loss: 0.0218 | 0.0184
Epoch 270/300, trend Loss: 0.0218 | 0.0183
Epoch 271/300, trend Loss: 0.0217 | 0.0182
Epoch 272/300, trend Loss: 0.0216 | 0.0183
Epoch 273/300, trend Loss: 0.0216 | 0.0182
Epoch 274/300, trend Loss: 0.0215 | 0.0183
Epoch 275/300, trend Loss: 0.0214 | 0.0183
Epoch 276/300, trend Loss: 0.0214 | 0.0183
Epoch 277/300, trend Loss: 0.0213 | 0.0183
Epoch 278/300, trend Loss: 0.0213 | 0.0183
Epoch 279/300, trend Loss: 0.0212 | 0.0182
Epoch 280/300, trend Loss: 0.0212 | 0.0183
Epoch 281/300, trend Loss: 0.0211 | 0.0182
Epoch 282/300, trend Loss: 0.0211 | 0.0183
Epoch 283/300, trend Loss: 0.0210 | 0.0183
Epoch 284/300, trend Loss: 0.0210 | 0.0184
Epoch 285/300, trend Loss: 0.0209 | 0.0183
Epoch 286/300, trend Loss: 0.0209 | 0.0185
Epoch 287/300, trend Loss: 0.0208 | 0.0183
Epoch 288/300, trend Loss: 0.0208 | 0.0188
Epoch 289/300, trend Loss: 0.0208 | 0.0184
Epoch 290/300, trend Loss: 0.0207 | 0.0189
Epoch 291/300, trend Loss: 0.0207 | 0.0183
Epoch 292/300, trend Loss: 0.0206 | 0.0193
Epoch 293/300, trend Loss: 0.0206 | 0.0181
Epoch 294/300, trend Loss: 0.0206 | 0.0200
Epoch 295/300, trend Loss: 0.0207 | 0.0178
Epoch 296/300, trend Loss: 0.0207 | 0.0209
Epoch 297/300, trend Loss: 0.0208 | 0.0175
Epoch 298/300, trend Loss: 0.0207 | 0.0208
Epoch 299/300, trend Loss: 0.0207 | 0.0176
Epoch 300/300, trend Loss: 0.0206 | 0.0204
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.9889566067833049, 'learning_rate': 0.00015500514411775907, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8029146535851374}
Epoch 1/300, seasonal_0 Loss: 0.1707 | 0.0895
Epoch 2/300, seasonal_0 Loss: 0.1045 | 0.0704
Epoch 3/300, seasonal_0 Loss: 0.0987 | 0.0658
Epoch 4/300, seasonal_0 Loss: 0.0928 | 0.0618
Epoch 5/300, seasonal_0 Loss: 0.0873 | 0.0578
Epoch 6/300, seasonal_0 Loss: 0.0829 | 0.0523
Epoch 7/300, seasonal_0 Loss: 0.0781 | 0.0487
Epoch 8/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 9/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 10/300, seasonal_0 Loss: 0.0750 | 0.0557
Epoch 11/300, seasonal_0 Loss: 0.0726 | 0.0542
Epoch 12/300, seasonal_0 Loss: 0.0695 | 0.0504
Epoch 13/300, seasonal_0 Loss: 0.0671 | 0.0438
Epoch 14/300, seasonal_0 Loss: 0.0653 | 0.0402
Epoch 15/300, seasonal_0 Loss: 0.0639 | 0.0384
Epoch 16/300, seasonal_0 Loss: 0.0631 | 0.0374
Epoch 17/300, seasonal_0 Loss: 0.0624 | 0.0366
Epoch 18/300, seasonal_0 Loss: 0.0618 | 0.0358
Epoch 19/300, seasonal_0 Loss: 0.0610 | 0.0365
Epoch 20/300, seasonal_0 Loss: 0.0605 | 0.0350
Epoch 21/300, seasonal_0 Loss: 0.0599 | 0.0338
Epoch 22/300, seasonal_0 Loss: 0.0594 | 0.0326
Epoch 23/300, seasonal_0 Loss: 0.0589 | 0.0315
Epoch 24/300, seasonal_0 Loss: 0.0584 | 0.0304
Epoch 25/300, seasonal_0 Loss: 0.0575 | 0.0302
Epoch 26/300, seasonal_0 Loss: 0.0571 | 0.0287
Epoch 27/300, seasonal_0 Loss: 0.0567 | 0.0276
Epoch 28/300, seasonal_0 Loss: 0.0564 | 0.0267
Epoch 29/300, seasonal_0 Loss: 0.0560 | 0.0259
Epoch 30/300, seasonal_0 Loss: 0.0557 | 0.0250
Epoch 31/300, seasonal_0 Loss: 0.0553 | 0.0257
Epoch 32/300, seasonal_0 Loss: 0.0549 | 0.0232
Epoch 33/300, seasonal_0 Loss: 0.0544 | 0.0218
Epoch 34/300, seasonal_0 Loss: 0.0540 | 0.0209
Epoch 35/300, seasonal_0 Loss: 0.0537 | 0.0203
Epoch 36/300, seasonal_0 Loss: 0.0534 | 0.0197
Epoch 37/300, seasonal_0 Loss: 0.0529 | 0.0193
Epoch 38/300, seasonal_0 Loss: 0.0527 | 0.0186
Epoch 39/300, seasonal_0 Loss: 0.0525 | 0.0181
Epoch 40/300, seasonal_0 Loss: 0.0523 | 0.0177
Epoch 41/300, seasonal_0 Loss: 0.0522 | 0.0174
Epoch 42/300, seasonal_0 Loss: 0.0521 | 0.0171
Epoch 43/300, seasonal_0 Loss: 0.0518 | 0.0175
Epoch 44/300, seasonal_0 Loss: 0.0516 | 0.0169
Epoch 45/300, seasonal_0 Loss: 0.0514 | 0.0165
Epoch 46/300, seasonal_0 Loss: 0.0513 | 0.0163
Epoch 47/300, seasonal_0 Loss: 0.0512 | 0.0162
Epoch 48/300, seasonal_0 Loss: 0.0511 | 0.0160
Epoch 49/300, seasonal_0 Loss: 0.0510 | 0.0157
Epoch 50/300, seasonal_0 Loss: 0.0509 | 0.0155
Epoch 51/300, seasonal_0 Loss: 0.0508 | 0.0154
Epoch 52/300, seasonal_0 Loss: 0.0507 | 0.0153
Epoch 53/300, seasonal_0 Loss: 0.0506 | 0.0153
Epoch 54/300, seasonal_0 Loss: 0.0505 | 0.0152
Epoch 55/300, seasonal_0 Loss: 0.0503 | 0.0149
Epoch 56/300, seasonal_0 Loss: 0.0502 | 0.0150
Epoch 57/300, seasonal_0 Loss: 0.0501 | 0.0150
Epoch 58/300, seasonal_0 Loss: 0.0500 | 0.0150
Epoch 59/300, seasonal_0 Loss: 0.0499 | 0.0150
Epoch 60/300, seasonal_0 Loss: 0.0499 | 0.0150
Epoch 61/300, seasonal_0 Loss: 0.0497 | 0.0150
Epoch 62/300, seasonal_0 Loss: 0.0496 | 0.0149
Epoch 63/300, seasonal_0 Loss: 0.0496 | 0.0149
Epoch 64/300, seasonal_0 Loss: 0.0495 | 0.0149
Epoch 65/300, seasonal_0 Loss: 0.0494 | 0.0149
Epoch 66/300, seasonal_0 Loss: 0.0494 | 0.0149
Epoch 67/300, seasonal_0 Loss: 0.0493 | 0.0147
Epoch 68/300, seasonal_0 Loss: 0.0492 | 0.0147
Epoch 69/300, seasonal_0 Loss: 0.0492 | 0.0147
Epoch 70/300, seasonal_0 Loss: 0.0491 | 0.0147
Epoch 71/300, seasonal_0 Loss: 0.0490 | 0.0147
Epoch 72/300, seasonal_0 Loss: 0.0490 | 0.0147
Epoch 73/300, seasonal_0 Loss: 0.0489 | 0.0147
Epoch 74/300, seasonal_0 Loss: 0.0488 | 0.0147
Epoch 75/300, seasonal_0 Loss: 0.0488 | 0.0147
Epoch 76/300, seasonal_0 Loss: 0.0487 | 0.0148
Epoch 77/300, seasonal_0 Loss: 0.0487 | 0.0148
Epoch 78/300, seasonal_0 Loss: 0.0486 | 0.0148
Epoch 79/300, seasonal_0 Loss: 0.0485 | 0.0148
Epoch 80/300, seasonal_0 Loss: 0.0485 | 0.0148
Epoch 81/300, seasonal_0 Loss: 0.0484 | 0.0148
Epoch 82/300, seasonal_0 Loss: 0.0484 | 0.0149
Epoch 83/300, seasonal_0 Loss: 0.0483 | 0.0149
Epoch 84/300, seasonal_0 Loss: 0.0483 | 0.0149
Epoch 85/300, seasonal_0 Loss: 0.0482 | 0.0149
Epoch 86/300, seasonal_0 Loss: 0.0482 | 0.0149
Epoch 87/300, seasonal_0 Loss: 0.0482 | 0.0149
Epoch 88/300, seasonal_0 Loss: 0.0481 | 0.0149
Epoch 89/300, seasonal_0 Loss: 0.0481 | 0.0149
Epoch 90/300, seasonal_0 Loss: 0.0481 | 0.0149
Epoch 91/300, seasonal_0 Loss: 0.0480 | 0.0149
Epoch 92/300, seasonal_0 Loss: 0.0480 | 0.0149
Epoch 93/300, seasonal_0 Loss: 0.0479 | 0.0149
Epoch 94/300, seasonal_0 Loss: 0.0479 | 0.0149
Epoch 95/300, seasonal_0 Loss: 0.0479 | 0.0149
Epoch 96/300, seasonal_0 Loss: 0.0479 | 0.0149
Epoch 97/300, seasonal_0 Loss: 0.0478 | 0.0150
Epoch 98/300, seasonal_0 Loss: 0.0478 | 0.0150
Epoch 99/300, seasonal_0 Loss: 0.0478 | 0.0150
Epoch 100/300, seasonal_0 Loss: 0.0477 | 0.0150
Epoch 101/300, seasonal_0 Loss: 0.0477 | 0.0150
Epoch 102/300, seasonal_0 Loss: 0.0477 | 0.0150
Epoch 103/300, seasonal_0 Loss: 0.0476 | 0.0151
Epoch 104/300, seasonal_0 Loss: 0.0476 | 0.0151
Epoch 105/300, seasonal_0 Loss: 0.0476 | 0.0151
Epoch 106/300, seasonal_0 Loss: 0.0476 | 0.0151
Epoch 107/300, seasonal_0 Loss: 0.0475 | 0.0151
Epoch 108/300, seasonal_0 Loss: 0.0475 | 0.0151
Epoch 109/300, seasonal_0 Loss: 0.0475 | 0.0152
Epoch 110/300, seasonal_0 Loss: 0.0475 | 0.0152
Epoch 111/300, seasonal_0 Loss: 0.0474 | 0.0152
Epoch 112/300, seasonal_0 Loss: 0.0474 | 0.0152
Epoch 113/300, seasonal_0 Loss: 0.0474 | 0.0152
Epoch 114/300, seasonal_0 Loss: 0.0474 | 0.0152
Epoch 115/300, seasonal_0 Loss: 0.0474 | 0.0153
Epoch 116/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 117/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 118/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 119/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 120/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 121/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 122/300, seasonal_0 Loss: 0.0473 | 0.0153
Epoch 123/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 124/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 125/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 126/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 127/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 128/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 129/300, seasonal_0 Loss: 0.0472 | 0.0153
Epoch 130/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 131/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 132/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 133/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 134/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 135/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 136/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 137/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 138/300, seasonal_0 Loss: 0.0471 | 0.0153
Epoch 139/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 140/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 141/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 142/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 143/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 144/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 145/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 146/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 147/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 148/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 149/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 150/300, seasonal_0 Loss: 0.0470 | 0.0152
Epoch 151/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 152/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 153/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 154/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 155/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 156/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 157/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 158/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 159/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 160/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 161/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 162/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 163/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 164/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 165/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 166/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 167/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 168/300, seasonal_0 Loss: 0.0469 | 0.0152
Epoch 169/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 170/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 171/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 172/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 173/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 174/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 175/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 176/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 177/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 178/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 179/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 180/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 181/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 182/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 183/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 184/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 185/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 186/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 187/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 188/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 189/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 190/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 191/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 192/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 193/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 194/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 195/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 196/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 197/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 198/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 199/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 200/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 201/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 202/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 203/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 204/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 205/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 206/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 207/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 208/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 209/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 210/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 211/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 212/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 213/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 214/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 215/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 216/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 217/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 218/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 219/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 220/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 221/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 222/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 223/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 224/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 225/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 226/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 227/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 228/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 229/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 230/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 231/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 232/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 233/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 234/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 235/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 236/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 237/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 238/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 239/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 240/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 241/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 242/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 243/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 244/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 245/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 246/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 247/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 248/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 249/300, seasonal_0 Loss: 0.0468 | 0.0152
Epoch 250/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 251/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 252/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 253/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 254/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 255/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 256/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 257/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 258/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 259/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 260/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 261/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 262/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 263/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 264/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 265/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 266/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 267/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 268/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 269/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 270/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 271/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 272/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 273/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 274/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 275/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 276/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 277/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 278/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 279/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 280/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 281/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 282/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 283/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 284/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 285/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 286/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 287/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 288/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 289/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 290/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 291/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 292/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 293/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 294/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 295/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 296/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 297/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 298/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 299/300, seasonal_0 Loss: 0.0467 | 0.0152
Epoch 300/300, seasonal_0 Loss: 0.0467 | 0.0152
Training seasonal_1 component with params: {'observation_period_num': 16, 'train_rates': 0.8700404739420563, 'learning_rate': 9.320639022296683e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7825574255273182}
Epoch 1/300, seasonal_1 Loss: 0.3247 | 0.2056
Epoch 2/300, seasonal_1 Loss: 0.1556 | 0.1652
Epoch 3/300, seasonal_1 Loss: 0.1322 | 0.1253
Epoch 4/300, seasonal_1 Loss: 0.1185 | 0.0967
Epoch 5/300, seasonal_1 Loss: 0.1108 | 0.0840
Epoch 6/300, seasonal_1 Loss: 0.1065 | 0.0762
Epoch 7/300, seasonal_1 Loss: 0.1032 | 0.0710
Epoch 8/300, seasonal_1 Loss: 0.1003 | 0.0673
Epoch 9/300, seasonal_1 Loss: 0.0977 | 0.0617
Epoch 10/300, seasonal_1 Loss: 0.0967 | 0.0597
Epoch 11/300, seasonal_1 Loss: 0.0960 | 0.0581
Epoch 12/300, seasonal_1 Loss: 0.0956 | 0.0566
Epoch 13/300, seasonal_1 Loss: 0.0961 | 0.0576
Epoch 14/300, seasonal_1 Loss: 0.0965 | 0.0560
Epoch 15/300, seasonal_1 Loss: 0.0949 | 0.0545
Epoch 16/300, seasonal_1 Loss: 0.0935 | 0.0530
Epoch 17/300, seasonal_1 Loss: 0.0926 | 0.0519
Epoch 18/300, seasonal_1 Loss: 0.0909 | 0.0508
Epoch 19/300, seasonal_1 Loss: 0.0894 | 0.0500
Epoch 20/300, seasonal_1 Loss: 0.0882 | 0.0496
Epoch 21/300, seasonal_1 Loss: 0.0875 | 0.0499
Epoch 22/300, seasonal_1 Loss: 0.0869 | 0.0502
Epoch 23/300, seasonal_1 Loss: 0.0865 | 0.0507
Epoch 24/300, seasonal_1 Loss: 0.0862 | 0.0509
Epoch 25/300, seasonal_1 Loss: 0.0860 | 0.0517
Epoch 26/300, seasonal_1 Loss: 0.0859 | 0.0510
Epoch 27/300, seasonal_1 Loss: 0.0852 | 0.0504
Epoch 28/300, seasonal_1 Loss: 0.0845 | 0.0496
Epoch 29/300, seasonal_1 Loss: 0.0840 | 0.0488
Epoch 30/300, seasonal_1 Loss: 0.0837 | 0.0484
Epoch 31/300, seasonal_1 Loss: 0.0830 | 0.0478
Epoch 32/300, seasonal_1 Loss: 0.0824 | 0.0472
Epoch 33/300, seasonal_1 Loss: 0.0818 | 0.0474
Epoch 34/300, seasonal_1 Loss: 0.0815 | 0.0472
Epoch 35/300, seasonal_1 Loss: 0.0810 | 0.0467
Epoch 36/300, seasonal_1 Loss: 0.0805 | 0.0461
Epoch 37/300, seasonal_1 Loss: 0.0800 | 0.0468
Epoch 38/300, seasonal_1 Loss: 0.0796 | 0.0462
Epoch 39/300, seasonal_1 Loss: 0.0792 | 0.0455
Epoch 40/300, seasonal_1 Loss: 0.0787 | 0.0448
Epoch 41/300, seasonal_1 Loss: 0.0783 | 0.0448
Epoch 42/300, seasonal_1 Loss: 0.0780 | 0.0442
Epoch 43/300, seasonal_1 Loss: 0.0776 | 0.0436
Epoch 44/300, seasonal_1 Loss: 0.0773 | 0.0431
Epoch 45/300, seasonal_1 Loss: 0.0770 | 0.0428
Epoch 46/300, seasonal_1 Loss: 0.0768 | 0.0424
Epoch 47/300, seasonal_1 Loss: 0.0766 | 0.0420
Epoch 48/300, seasonal_1 Loss: 0.0764 | 0.0417
Epoch 49/300, seasonal_1 Loss: 0.0762 | 0.0414
Epoch 50/300, seasonal_1 Loss: 0.0761 | 0.0412
Epoch 51/300, seasonal_1 Loss: 0.0759 | 0.0410
Epoch 52/300, seasonal_1 Loss: 0.0758 | 0.0408
Epoch 53/300, seasonal_1 Loss: 0.0756 | 0.0406
Epoch 54/300, seasonal_1 Loss: 0.0755 | 0.0405
Epoch 55/300, seasonal_1 Loss: 0.0754 | 0.0403
Epoch 56/300, seasonal_1 Loss: 0.0753 | 0.0402
Epoch 57/300, seasonal_1 Loss: 0.0752 | 0.0400
Epoch 58/300, seasonal_1 Loss: 0.0751 | 0.0399
Epoch 59/300, seasonal_1 Loss: 0.0751 | 0.0399
Epoch 60/300, seasonal_1 Loss: 0.0750 | 0.0398
Epoch 61/300, seasonal_1 Loss: 0.0749 | 0.0396
Epoch 62/300, seasonal_1 Loss: 0.0748 | 0.0396
Epoch 63/300, seasonal_1 Loss: 0.0748 | 0.0395
Epoch 64/300, seasonal_1 Loss: 0.0747 | 0.0394
Epoch 65/300, seasonal_1 Loss: 0.0746 | 0.0393
Epoch 66/300, seasonal_1 Loss: 0.0746 | 0.0393
Epoch 67/300, seasonal_1 Loss: 0.0746 | 0.0392
Epoch 68/300, seasonal_1 Loss: 0.0745 | 0.0392
Epoch 69/300, seasonal_1 Loss: 0.0745 | 0.0391
Epoch 70/300, seasonal_1 Loss: 0.0744 | 0.0390
Epoch 71/300, seasonal_1 Loss: 0.0744 | 0.0390
Epoch 72/300, seasonal_1 Loss: 0.0743 | 0.0389
Epoch 73/300, seasonal_1 Loss: 0.0743 | 0.0389
Epoch 74/300, seasonal_1 Loss: 0.0743 | 0.0388
Epoch 75/300, seasonal_1 Loss: 0.0743 | 0.0388
Epoch 76/300, seasonal_1 Loss: 0.0742 | 0.0388
Epoch 77/300, seasonal_1 Loss: 0.0742 | 0.0387
Epoch 78/300, seasonal_1 Loss: 0.0742 | 0.0387
Epoch 79/300, seasonal_1 Loss: 0.0741 | 0.0386
Epoch 80/300, seasonal_1 Loss: 0.0741 | 0.0386
Epoch 81/300, seasonal_1 Loss: 0.0741 | 0.0386
Epoch 82/300, seasonal_1 Loss: 0.0741 | 0.0386
Epoch 83/300, seasonal_1 Loss: 0.0740 | 0.0385
Epoch 84/300, seasonal_1 Loss: 0.0740 | 0.0385
Epoch 85/300, seasonal_1 Loss: 0.0740 | 0.0385
Epoch 86/300, seasonal_1 Loss: 0.0740 | 0.0385
Epoch 87/300, seasonal_1 Loss: 0.0739 | 0.0385
Epoch 88/300, seasonal_1 Loss: 0.0739 | 0.0384
Epoch 89/300, seasonal_1 Loss: 0.0739 | 0.0384
Epoch 90/300, seasonal_1 Loss: 0.0739 | 0.0384
Epoch 91/300, seasonal_1 Loss: 0.0739 | 0.0384
Epoch 92/300, seasonal_1 Loss: 0.0739 | 0.0384
Epoch 93/300, seasonal_1 Loss: 0.0738 | 0.0384
Epoch 94/300, seasonal_1 Loss: 0.0738 | 0.0384
Epoch 95/300, seasonal_1 Loss: 0.0738 | 0.0384
Epoch 96/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 97/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 98/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 99/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 100/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 101/300, seasonal_1 Loss: 0.0738 | 0.0383
Epoch 102/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 103/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 104/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 105/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 106/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 107/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 108/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 109/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 110/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 111/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 112/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 113/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 114/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 115/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 116/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 117/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 118/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 119/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 120/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 121/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 122/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 123/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 124/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 125/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 126/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 127/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 128/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 129/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 130/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 131/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 132/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 133/300, seasonal_1 Loss: 0.0737 | 0.0382
Epoch 134/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 135/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 136/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 137/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 138/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 139/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 140/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 141/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 142/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 143/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 144/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 145/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 146/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 147/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 148/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 149/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 150/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 151/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 152/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 153/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 154/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 155/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 156/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 157/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 158/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 159/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 160/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 161/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 162/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 163/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 164/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 165/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 166/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 167/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 168/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 169/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 170/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 171/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 172/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 173/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 174/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 175/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 176/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 177/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 178/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 179/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 180/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 181/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 182/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 183/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 184/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 185/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 186/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 187/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 188/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 189/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 190/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 191/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 192/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 193/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 194/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 195/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 196/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 197/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 198/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 199/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 200/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 201/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 202/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 203/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 204/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 205/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 206/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 207/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 208/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 209/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 210/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 211/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 212/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 213/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 214/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 215/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 216/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 217/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 218/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 219/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 220/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 221/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 222/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 223/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 224/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 225/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 226/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 227/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 228/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 229/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 230/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 231/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 232/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 233/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 234/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 235/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 236/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 237/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 238/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 239/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 240/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 241/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 242/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 243/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 244/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 245/300, seasonal_1 Loss: 0.0736 | 0.0382
Epoch 246/300, seasonal_1 Loss: 0.0736 | 0.0382
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.8586086135426391, 'learning_rate': 0.00071731686769784, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9487245629432022}
Epoch 1/300, seasonal_2 Loss: 0.3407 | 0.1799
Epoch 2/300, seasonal_2 Loss: 0.1585 | 0.1127
Epoch 3/300, seasonal_2 Loss: 0.1194 | 0.0794
Epoch 4/300, seasonal_2 Loss: 0.1173 | 0.0841
Epoch 5/300, seasonal_2 Loss: 0.1162 | 0.0777
Epoch 6/300, seasonal_2 Loss: 0.1232 | 0.0859
Epoch 7/300, seasonal_2 Loss: 0.1023 | 0.0628
Epoch 8/300, seasonal_2 Loss: 0.0995 | 0.0607
Epoch 9/300, seasonal_2 Loss: 0.0998 | 0.0611
Epoch 10/300, seasonal_2 Loss: 0.0991 | 0.0592
Epoch 11/300, seasonal_2 Loss: 0.0979 | 0.0545
Epoch 12/300, seasonal_2 Loss: 0.0959 | 0.0509
Epoch 13/300, seasonal_2 Loss: 0.0928 | 0.0491
Epoch 14/300, seasonal_2 Loss: 0.0887 | 0.0484
Epoch 15/300, seasonal_2 Loss: 0.0853 | 0.0478
Epoch 16/300, seasonal_2 Loss: 0.0831 | 0.0470
Epoch 17/300, seasonal_2 Loss: 0.0819 | 0.0456
Epoch 18/300, seasonal_2 Loss: 0.0812 | 0.0441
Epoch 19/300, seasonal_2 Loss: 0.0809 | 0.0435
Epoch 20/300, seasonal_2 Loss: 0.0809 | 0.0453
Epoch 21/300, seasonal_2 Loss: 0.0807 | 0.0493
Epoch 22/300, seasonal_2 Loss: 0.0800 | 0.0488
Epoch 23/300, seasonal_2 Loss: 0.0801 | 0.0453
Epoch 24/300, seasonal_2 Loss: 0.0813 | 0.0484
Epoch 25/300, seasonal_2 Loss: 0.0809 | 0.0493
Epoch 26/300, seasonal_2 Loss: 0.0799 | 0.0490
Epoch 27/300, seasonal_2 Loss: 0.0786 | 0.0456
Epoch 28/300, seasonal_2 Loss: 0.0774 | 0.0426
Epoch 29/300, seasonal_2 Loss: 0.0762 | 0.0408
Epoch 30/300, seasonal_2 Loss: 0.0754 | 0.0398
Epoch 31/300, seasonal_2 Loss: 0.0751 | 0.0394
Epoch 32/300, seasonal_2 Loss: 0.0748 | 0.0392
Epoch 33/300, seasonal_2 Loss: 0.0747 | 0.0390
Epoch 34/300, seasonal_2 Loss: 0.0745 | 0.0388
Epoch 35/300, seasonal_2 Loss: 0.0743 | 0.0387
Epoch 36/300, seasonal_2 Loss: 0.0741 | 0.0386
Epoch 37/300, seasonal_2 Loss: 0.0738 | 0.0385
Epoch 38/300, seasonal_2 Loss: 0.0736 | 0.0384
Epoch 39/300, seasonal_2 Loss: 0.0735 | 0.0382
Epoch 40/300, seasonal_2 Loss: 0.0733 | 0.0381
Epoch 41/300, seasonal_2 Loss: 0.0732 | 0.0380
Epoch 42/300, seasonal_2 Loss: 0.0731 | 0.0379
Epoch 43/300, seasonal_2 Loss: 0.0730 | 0.0379
Epoch 44/300, seasonal_2 Loss: 0.0730 | 0.0378
Epoch 45/300, seasonal_2 Loss: 0.0729 | 0.0377
Epoch 46/300, seasonal_2 Loss: 0.0728 | 0.0377
Epoch 47/300, seasonal_2 Loss: 0.0728 | 0.0376
Epoch 48/300, seasonal_2 Loss: 0.0727 | 0.0376
Epoch 49/300, seasonal_2 Loss: 0.0727 | 0.0375
Epoch 50/300, seasonal_2 Loss: 0.0727 | 0.0375
Epoch 51/300, seasonal_2 Loss: 0.0726 | 0.0375
Epoch 52/300, seasonal_2 Loss: 0.0726 | 0.0374
Epoch 53/300, seasonal_2 Loss: 0.0726 | 0.0374
Epoch 54/300, seasonal_2 Loss: 0.0725 | 0.0374
Epoch 55/300, seasonal_2 Loss: 0.0725 | 0.0373
Epoch 56/300, seasonal_2 Loss: 0.0725 | 0.0373
Epoch 57/300, seasonal_2 Loss: 0.0725 | 0.0373
Epoch 58/300, seasonal_2 Loss: 0.0725 | 0.0373
Epoch 59/300, seasonal_2 Loss: 0.0724 | 0.0373
Epoch 60/300, seasonal_2 Loss: 0.0724 | 0.0373
Epoch 61/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 62/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 63/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 64/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 65/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 66/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 67/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 68/300, seasonal_2 Loss: 0.0724 | 0.0372
Epoch 69/300, seasonal_2 Loss: 0.0723 | 0.0372
Epoch 70/300, seasonal_2 Loss: 0.0723 | 0.0372
Epoch 71/300, seasonal_2 Loss: 0.0723 | 0.0372
Epoch 72/300, seasonal_2 Loss: 0.0723 | 0.0372
Epoch 73/300, seasonal_2 Loss: 0.0723 | 0.0372
Epoch 74/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 75/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 76/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 77/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 78/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 79/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 80/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 81/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 82/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 83/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 84/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 85/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 86/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 87/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 88/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 89/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 90/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 91/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 92/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 93/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 94/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 95/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 96/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 97/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 98/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 99/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 100/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 101/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 102/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 103/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 104/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 105/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 106/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 107/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 108/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 109/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 110/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 111/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 112/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 113/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 114/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 115/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 116/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 117/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 118/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 119/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 120/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 121/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 122/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 123/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 124/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 125/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 126/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 127/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 128/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 129/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 130/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 131/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 132/300, seasonal_2 Loss: 0.0723 | 0.0371
Epoch 133/300, seasonal_2 Loss: 0.0723 | 0.0371
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 37, 'train_rates': 0.867330621222478, 'learning_rate': 0.00027141777063759126, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9444974498560493}
Epoch 1/300, seasonal_3 Loss: 0.3916 | 0.2085
Epoch 2/300, seasonal_3 Loss: 0.1788 | 0.1894
Epoch 3/300, seasonal_3 Loss: 0.1432 | 0.1133
Epoch 4/300, seasonal_3 Loss: 0.1314 | 0.0962
Epoch 5/300, seasonal_3 Loss: 0.1291 | 0.0839
Epoch 6/300, seasonal_3 Loss: 0.1212 | 0.1136
Epoch 7/300, seasonal_3 Loss: 0.1215 | 0.0795
Epoch 8/300, seasonal_3 Loss: 0.1032 | 0.0689
Epoch 9/300, seasonal_3 Loss: 0.1003 | 0.0749
Epoch 10/300, seasonal_3 Loss: 0.1032 | 0.0607
Epoch 11/300, seasonal_3 Loss: 0.0961 | 0.0605
Epoch 12/300, seasonal_3 Loss: 0.0949 | 0.0564
Epoch 13/300, seasonal_3 Loss: 0.0892 | 0.0530
Epoch 14/300, seasonal_3 Loss: 0.0931 | 0.0545
Epoch 15/300, seasonal_3 Loss: 0.0860 | 0.0589
Epoch 16/300, seasonal_3 Loss: 0.0858 | 0.0490
Epoch 17/300, seasonal_3 Loss: 0.0818 | 0.0496
Epoch 18/300, seasonal_3 Loss: 0.0776 | 0.0533
Epoch 19/300, seasonal_3 Loss: 0.0786 | 0.0539
Epoch 20/300, seasonal_3 Loss: 0.0891 | 0.0563
Epoch 21/300, seasonal_3 Loss: 0.0872 | 0.0486
Epoch 22/300, seasonal_3 Loss: 0.0796 | 0.0466
Epoch 23/300, seasonal_3 Loss: 0.0762 | 0.0434
Epoch 24/300, seasonal_3 Loss: 0.0833 | 0.0490
Epoch 25/300, seasonal_3 Loss: 0.0759 | 0.0429
Epoch 26/300, seasonal_3 Loss: 0.0788 | 0.0446
Epoch 27/300, seasonal_3 Loss: 0.0733 | 0.0432
Epoch 28/300, seasonal_3 Loss: 0.0772 | 0.0537
Epoch 29/300, seasonal_3 Loss: 0.0776 | 0.0638
Epoch 30/300, seasonal_3 Loss: 0.0767 | 0.0700
Epoch 31/300, seasonal_3 Loss: 0.0732 | 0.0612
Epoch 32/300, seasonal_3 Loss: 0.0721 | 0.0509
Epoch 33/300, seasonal_3 Loss: 0.0718 | 0.0437
Epoch 34/300, seasonal_3 Loss: 0.0717 | 0.0399
Epoch 35/300, seasonal_3 Loss: 0.0730 | 0.0423
Epoch 36/300, seasonal_3 Loss: 0.0722 | 0.0459
Epoch 37/300, seasonal_3 Loss: 0.0699 | 0.0445
Epoch 38/300, seasonal_3 Loss: 0.0687 | 0.0436
Epoch 39/300, seasonal_3 Loss: 0.0678 | 0.0430
Epoch 40/300, seasonal_3 Loss: 0.0671 | 0.0420
Epoch 41/300, seasonal_3 Loss: 0.0666 | 0.0410
Epoch 42/300, seasonal_3 Loss: 0.0661 | 0.0401
Epoch 43/300, seasonal_3 Loss: 0.0658 | 0.0393
Epoch 44/300, seasonal_3 Loss: 0.0657 | 0.0386
Epoch 45/300, seasonal_3 Loss: 0.0656 | 0.0384
Epoch 46/300, seasonal_3 Loss: 0.0654 | 0.0382
Epoch 47/300, seasonal_3 Loss: 0.0651 | 0.0380
Epoch 48/300, seasonal_3 Loss: 0.0648 | 0.0381
Epoch 49/300, seasonal_3 Loss: 0.0644 | 0.0381
Epoch 50/300, seasonal_3 Loss: 0.0641 | 0.0379
Epoch 51/300, seasonal_3 Loss: 0.0638 | 0.0379
Epoch 52/300, seasonal_3 Loss: 0.0636 | 0.0379
Epoch 53/300, seasonal_3 Loss: 0.0634 | 0.0376
Epoch 54/300, seasonal_3 Loss: 0.0633 | 0.0376
Epoch 55/300, seasonal_3 Loss: 0.0632 | 0.0376
Epoch 56/300, seasonal_3 Loss: 0.0631 | 0.0372
Epoch 57/300, seasonal_3 Loss: 0.0630 | 0.0372
Epoch 58/300, seasonal_3 Loss: 0.0629 | 0.0372
Epoch 59/300, seasonal_3 Loss: 0.0628 | 0.0368
Epoch 60/300, seasonal_3 Loss: 0.0627 | 0.0367
Epoch 61/300, seasonal_3 Loss: 0.0626 | 0.0365
Epoch 62/300, seasonal_3 Loss: 0.0626 | 0.0363
Epoch 63/300, seasonal_3 Loss: 0.0625 | 0.0362
Epoch 64/300, seasonal_3 Loss: 0.0624 | 0.0363
Epoch 65/300, seasonal_3 Loss: 0.0625 | 0.0365
Epoch 66/300, seasonal_3 Loss: 0.0625 | 0.0370
Epoch 67/300, seasonal_3 Loss: 0.0625 | 0.0381
Epoch 68/300, seasonal_3 Loss: 0.0625 | 0.0389
Epoch 69/300, seasonal_3 Loss: 0.0624 | 0.0401
Epoch 70/300, seasonal_3 Loss: 0.0623 | 0.0410
Epoch 71/300, seasonal_3 Loss: 0.0621 | 0.0406
Epoch 72/300, seasonal_3 Loss: 0.0618 | 0.0399
Epoch 73/300, seasonal_3 Loss: 0.0615 | 0.0386
Epoch 74/300, seasonal_3 Loss: 0.0612 | 0.0371
Epoch 75/300, seasonal_3 Loss: 0.0609 | 0.0360
Epoch 76/300, seasonal_3 Loss: 0.0607 | 0.0352
Epoch 77/300, seasonal_3 Loss: 0.0605 | 0.0347
Epoch 78/300, seasonal_3 Loss: 0.0603 | 0.0344
Epoch 79/300, seasonal_3 Loss: 0.0602 | 0.0342
Epoch 80/300, seasonal_3 Loss: 0.0600 | 0.0341
Epoch 81/300, seasonal_3 Loss: 0.0599 | 0.0340
Epoch 82/300, seasonal_3 Loss: 0.0598 | 0.0340
Epoch 83/300, seasonal_3 Loss: 0.0597 | 0.0340
Epoch 84/300, seasonal_3 Loss: 0.0597 | 0.0339
Epoch 85/300, seasonal_3 Loss: 0.0596 | 0.0339
Epoch 86/300, seasonal_3 Loss: 0.0595 | 0.0339
Epoch 87/300, seasonal_3 Loss: 0.0594 | 0.0339
Epoch 88/300, seasonal_3 Loss: 0.0594 | 0.0339
Epoch 89/300, seasonal_3 Loss: 0.0593 | 0.0339
Epoch 90/300, seasonal_3 Loss: 0.0593 | 0.0338
Epoch 91/300, seasonal_3 Loss: 0.0592 | 0.0338
Epoch 92/300, seasonal_3 Loss: 0.0592 | 0.0338
Epoch 93/300, seasonal_3 Loss: 0.0591 | 0.0338
Epoch 94/300, seasonal_3 Loss: 0.0591 | 0.0338
Epoch 95/300, seasonal_3 Loss: 0.0590 | 0.0337
Epoch 96/300, seasonal_3 Loss: 0.0590 | 0.0337
Epoch 97/300, seasonal_3 Loss: 0.0589 | 0.0337
Epoch 98/300, seasonal_3 Loss: 0.0589 | 0.0337
Epoch 99/300, seasonal_3 Loss: 0.0588 | 0.0337
Epoch 100/300, seasonal_3 Loss: 0.0588 | 0.0336
Epoch 101/300, seasonal_3 Loss: 0.0588 | 0.0336
Epoch 102/300, seasonal_3 Loss: 0.0587 | 0.0336
Epoch 103/300, seasonal_3 Loss: 0.0587 | 0.0336
Epoch 104/300, seasonal_3 Loss: 0.0587 | 0.0336
Epoch 105/300, seasonal_3 Loss: 0.0586 | 0.0336
Epoch 106/300, seasonal_3 Loss: 0.0586 | 0.0336
Epoch 107/300, seasonal_3 Loss: 0.0586 | 0.0335
Epoch 108/300, seasonal_3 Loss: 0.0585 | 0.0335
Epoch 109/300, seasonal_3 Loss: 0.0585 | 0.0335
Epoch 110/300, seasonal_3 Loss: 0.0585 | 0.0335
Epoch 111/300, seasonal_3 Loss: 0.0584 | 0.0335
Epoch 112/300, seasonal_3 Loss: 0.0584 | 0.0335
Epoch 113/300, seasonal_3 Loss: 0.0584 | 0.0335
Epoch 114/300, seasonal_3 Loss: 0.0584 | 0.0335
Epoch 115/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 116/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 117/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 118/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 119/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 120/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 121/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 122/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 123/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 124/300, seasonal_3 Loss: 0.0581 | 0.0334
Epoch 125/300, seasonal_3 Loss: 0.0581 | 0.0334
Epoch 126/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 127/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 128/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 129/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 130/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 131/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 132/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 133/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 134/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 135/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 136/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 137/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 138/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 139/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 140/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 141/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 142/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 143/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 144/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 145/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 146/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 147/300, seasonal_3 Loss: 0.0579 | 0.0332
Epoch 148/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 149/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 150/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 151/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 152/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 153/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 154/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 155/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 156/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 157/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 158/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 159/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 160/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 161/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 162/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 163/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 164/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 165/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 166/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 167/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 168/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 169/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 170/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 171/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 172/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 173/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 174/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 175/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 176/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 177/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 178/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 179/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 180/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 181/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 182/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 183/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 184/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 185/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 186/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 187/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 188/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 189/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 190/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 191/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 192/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 193/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 194/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 195/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 196/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 197/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 198/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 199/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 200/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 201/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 202/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 203/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 204/300, seasonal_3 Loss: 0.0577 | 0.0331
Epoch 205/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 206/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 207/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 208/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 209/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 210/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 211/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 212/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 213/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 214/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 215/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 216/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 217/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 218/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 219/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 220/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 221/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 222/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 223/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 224/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 225/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 226/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 227/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 228/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 229/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 230/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 231/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 232/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 233/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 234/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 235/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 236/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 237/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 238/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 239/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 240/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 241/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 242/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 243/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 244/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 245/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 246/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 247/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 248/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 249/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 250/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 251/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 252/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 253/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 254/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 255/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 256/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 257/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 258/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 259/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 260/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 261/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 262/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 263/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 264/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 265/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 266/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 267/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 268/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 269/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 270/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 271/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 272/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 273/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 274/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 275/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 276/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 277/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 278/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 279/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 280/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 281/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 282/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 283/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 284/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 285/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 286/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 287/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 288/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 289/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 290/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 291/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 292/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 293/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 294/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 295/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 296/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 297/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 298/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 299/300, seasonal_3 Loss: 0.0576 | 0.0331
Epoch 300/300, seasonal_3 Loss: 0.0576 | 0.0331
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8773392884416733, 'learning_rate': 8.352850816800337e-05, 'batch_size': 55, 'step_size': 15, 'gamma': 0.9288429339945826}
Epoch 1/300, resid Loss: 0.5281 | 0.2668
Epoch 2/300, resid Loss: 0.1668 | 0.1568
Epoch 3/300, resid Loss: 0.1377 | 0.1153
Epoch 4/300, resid Loss: 0.1279 | 0.0967
Epoch 5/300, resid Loss: 0.1211 | 0.0860
Epoch 6/300, resid Loss: 0.1159 | 0.0798
Epoch 7/300, resid Loss: 0.1123 | 0.0764
Epoch 8/300, resid Loss: 0.1101 | 0.0739
Epoch 9/300, resid Loss: 0.1089 | 0.0701
Epoch 10/300, resid Loss: 0.1085 | 0.0680
Epoch 11/300, resid Loss: 0.1083 | 0.0664
Epoch 12/300, resid Loss: 0.1081 | 0.0657
Epoch 13/300, resid Loss: 0.1080 | 0.0661
Epoch 14/300, resid Loss: 0.1075 | 0.0675
Epoch 15/300, resid Loss: 0.1066 | 0.0691
Epoch 16/300, resid Loss: 0.1052 | 0.0723
Epoch 17/300, resid Loss: 0.1028 | 0.0688
Epoch 18/300, resid Loss: 0.0993 | 0.0645
Epoch 19/300, resid Loss: 0.0974 | 0.0616
Epoch 20/300, resid Loss: 0.0984 | 0.0622
Epoch 21/300, resid Loss: 0.1021 | 0.0657
Epoch 22/300, resid Loss: 0.1035 | 0.0652
Epoch 23/300, resid Loss: 0.1016 | 0.0641
Epoch 24/300, resid Loss: 0.1004 | 0.0649
Epoch 25/300, resid Loss: 0.0993 | 0.0626
Epoch 26/300, resid Loss: 0.0974 | 0.0598
Epoch 27/300, resid Loss: 0.0954 | 0.0562
Epoch 28/300, resid Loss: 0.0931 | 0.0524
Epoch 29/300, resid Loss: 0.0907 | 0.0492
Epoch 30/300, resid Loss: 0.0883 | 0.0471
Epoch 31/300, resid Loss: 0.0866 | 0.0469
Epoch 32/300, resid Loss: 0.0850 | 0.0467
Epoch 33/300, resid Loss: 0.0837 | 0.0464
Epoch 34/300, resid Loss: 0.0827 | 0.0458
Epoch 35/300, resid Loss: 0.0818 | 0.0451
Epoch 36/300, resid Loss: 0.0811 | 0.0443
Epoch 37/300, resid Loss: 0.0803 | 0.0434
Epoch 38/300, resid Loss: 0.0796 | 0.0425
Epoch 39/300, resid Loss: 0.0790 | 0.0419
Epoch 40/300, resid Loss: 0.0786 | 0.0412
Epoch 41/300, resid Loss: 0.0781 | 0.0405
Epoch 42/300, resid Loss: 0.0777 | 0.0400
Epoch 43/300, resid Loss: 0.0773 | 0.0395
Epoch 44/300, resid Loss: 0.0769 | 0.0390
Epoch 45/300, resid Loss: 0.0765 | 0.0386
Epoch 46/300, resid Loss: 0.0761 | 0.0385
Epoch 47/300, resid Loss: 0.0758 | 0.0381
Epoch 48/300, resid Loss: 0.0753 | 0.0377
Epoch 49/300, resid Loss: 0.0749 | 0.0374
Epoch 50/300, resid Loss: 0.0745 | 0.0371
Epoch 51/300, resid Loss: 0.0741 | 0.0368
Epoch 52/300, resid Loss: 0.0737 | 0.0365
Epoch 53/300, resid Loss: 0.0733 | 0.0362
Epoch 54/300, resid Loss: 0.0729 | 0.0361
Epoch 55/300, resid Loss: 0.0726 | 0.0358
Epoch 56/300, resid Loss: 0.0722 | 0.0355
Epoch 57/300, resid Loss: 0.0719 | 0.0352
Epoch 58/300, resid Loss: 0.0716 | 0.0349
Epoch 59/300, resid Loss: 0.0713 | 0.0346
Epoch 60/300, resid Loss: 0.0710 | 0.0344
Epoch 61/300, resid Loss: 0.0708 | 0.0343
Epoch 62/300, resid Loss: 0.0706 | 0.0341
Epoch 63/300, resid Loss: 0.0704 | 0.0339
Epoch 64/300, resid Loss: 0.0703 | 0.0336
Epoch 65/300, resid Loss: 0.0701 | 0.0334
Epoch 66/300, resid Loss: 0.0700 | 0.0333
Epoch 67/300, resid Loss: 0.0699 | 0.0332
Epoch 68/300, resid Loss: 0.0698 | 0.0331
Epoch 69/300, resid Loss: 0.0698 | 0.0330
Epoch 70/300, resid Loss: 0.0699 | 0.0329
Epoch 71/300, resid Loss: 0.0699 | 0.0327
Epoch 72/300, resid Loss: 0.0699 | 0.0325
Epoch 73/300, resid Loss: 0.0699 | 0.0324
Epoch 74/300, resid Loss: 0.0699 | 0.0324
Epoch 75/300, resid Loss: 0.0698 | 0.0324
Epoch 76/300, resid Loss: 0.0697 | 0.0321
Epoch 77/300, resid Loss: 0.0695 | 0.0319
Epoch 78/300, resid Loss: 0.0691 | 0.0318
Epoch 79/300, resid Loss: 0.0687 | 0.0318
Epoch 80/300, resid Loss: 0.0683 | 0.0317
Epoch 81/300, resid Loss: 0.0679 | 0.0316
Epoch 82/300, resid Loss: 0.0677 | 0.0315
Epoch 83/300, resid Loss: 0.0679 | 0.0318
Epoch 84/300, resid Loss: 0.0682 | 0.0331
Epoch 85/300, resid Loss: 0.0683 | 0.0318
Epoch 86/300, resid Loss: 0.0673 | 0.0317
Epoch 87/300, resid Loss: 0.0671 | 0.0318
Epoch 88/300, resid Loss: 0.0671 | 0.0316
Epoch 89/300, resid Loss: 0.0671 | 0.0315
Epoch 90/300, resid Loss: 0.0671 | 0.0313
Epoch 91/300, resid Loss: 0.0671 | 0.0314
Epoch 92/300, resid Loss: 0.0671 | 0.0314
Epoch 93/300, resid Loss: 0.0670 | 0.0313
Epoch 94/300, resid Loss: 0.0669 | 0.0313
Epoch 95/300, resid Loss: 0.0667 | 0.0312
Epoch 96/300, resid Loss: 0.0665 | 0.0310
Epoch 97/300, resid Loss: 0.0662 | 0.0308
Epoch 98/300, resid Loss: 0.0659 | 0.0306
Epoch 99/300, resid Loss: 0.0656 | 0.0303
Epoch 100/300, resid Loss: 0.0652 | 0.0297
Epoch 101/300, resid Loss: 0.0648 | 0.0291
Epoch 102/300, resid Loss: 0.0645 | 0.0286
Epoch 103/300, resid Loss: 0.0643 | 0.0283
Epoch 104/300, resid Loss: 0.0644 | 0.0283
Epoch 105/300, resid Loss: 0.0645 | 0.0285
Epoch 106/300, resid Loss: 0.0646 | 0.0291
Epoch 107/300, resid Loss: 0.0644 | 0.0292
Epoch 108/300, resid Loss: 0.0640 | 0.0291
Epoch 109/300, resid Loss: 0.0637 | 0.0288
Epoch 110/300, resid Loss: 0.0634 | 0.0285
Epoch 111/300, resid Loss: 0.0631 | 0.0283
Epoch 112/300, resid Loss: 0.0629 | 0.0281
Epoch 113/300, resid Loss: 0.0626 | 0.0280
Epoch 114/300, resid Loss: 0.0624 | 0.0278
Epoch 115/300, resid Loss: 0.0622 | 0.0276
Epoch 116/300, resid Loss: 0.0620 | 0.0276
Epoch 117/300, resid Loss: 0.0618 | 0.0275
Epoch 118/300, resid Loss: 0.0616 | 0.0274
Epoch 119/300, resid Loss: 0.0615 | 0.0273
Epoch 120/300, resid Loss: 0.0614 | 0.0272
Epoch 121/300, resid Loss: 0.0614 | 0.0271
Epoch 122/300, resid Loss: 0.0614 | 0.0270
Epoch 123/300, resid Loss: 0.0614 | 0.0268
Epoch 124/300, resid Loss: 0.0612 | 0.0267
Epoch 125/300, resid Loss: 0.0611 | 0.0265
Epoch 126/300, resid Loss: 0.0610 | 0.0264
Epoch 127/300, resid Loss: 0.0609 | 0.0262
Epoch 128/300, resid Loss: 0.0607 | 0.0261
Epoch 129/300, resid Loss: 0.0606 | 0.0260
Epoch 130/300, resid Loss: 0.0606 | 0.0258
Epoch 131/300, resid Loss: 0.0604 | 0.0257
Epoch 132/300, resid Loss: 0.0603 | 0.0257
Epoch 133/300, resid Loss: 0.0602 | 0.0257
Epoch 134/300, resid Loss: 0.0602 | 0.0259
Epoch 135/300, resid Loss: 0.0603 | 0.0263
Epoch 136/300, resid Loss: 0.0605 | 0.0278
Epoch 137/300, resid Loss: 0.0609 | 0.0288
Epoch 138/300, resid Loss: 0.0614 | 0.0296
Epoch 139/300, resid Loss: 0.0615 | 0.0295
Epoch 140/300, resid Loss: 0.0610 | 0.0295
Epoch 141/300, resid Loss: 0.0608 | 0.0285
Epoch 142/300, resid Loss: 0.0615 | 0.0267
Epoch 143/300, resid Loss: 0.0680 | 0.0310
Epoch 144/300, resid Loss: 0.0643 | 0.0306
Epoch 145/300, resid Loss: 0.0640 | 0.0297
Epoch 146/300, resid Loss: 0.0632 | 0.0273
Epoch 147/300, resid Loss: 0.0620 | 0.0260
Epoch 148/300, resid Loss: 0.0623 | 0.0261
Epoch 149/300, resid Loss: 0.0620 | 0.0259
Epoch 150/300, resid Loss: 0.0612 | 0.0254
Epoch 151/300, resid Loss: 0.0605 | 0.0253
Epoch 152/300, resid Loss: 0.0600 | 0.0251
Epoch 153/300, resid Loss: 0.0595 | 0.0251
Epoch 154/300, resid Loss: 0.0591 | 0.0249
Epoch 155/300, resid Loss: 0.0589 | 0.0249
Epoch 156/300, resid Loss: 0.0588 | 0.0249
Epoch 157/300, resid Loss: 0.0587 | 0.0248
Epoch 158/300, resid Loss: 0.0586 | 0.0247
Epoch 159/300, resid Loss: 0.0586 | 0.0247
Epoch 160/300, resid Loss: 0.0586 | 0.0246
Epoch 161/300, resid Loss: 0.0585 | 0.0244
Epoch 162/300, resid Loss: 0.0583 | 0.0242
Epoch 163/300, resid Loss: 0.0582 | 0.0241
Epoch 164/300, resid Loss: 0.0581 | 0.0239
Epoch 165/300, resid Loss: 0.0580 | 0.0238
Epoch 166/300, resid Loss: 0.0580 | 0.0238
Epoch 167/300, resid Loss: 0.0580 | 0.0237
Epoch 168/300, resid Loss: 0.0579 | 0.0237
Epoch 169/300, resid Loss: 0.0579 | 0.0236
Epoch 170/300, resid Loss: 0.0578 | 0.0236
Epoch 171/300, resid Loss: 0.0577 | 0.0236
Epoch 172/300, resid Loss: 0.0576 | 0.0236
Epoch 173/300, resid Loss: 0.0575 | 0.0235
Epoch 174/300, resid Loss: 0.0574 | 0.0237
Epoch 175/300, resid Loss: 0.0572 | 0.0236
Epoch 176/300, resid Loss: 0.0571 | 0.0236
Epoch 177/300, resid Loss: 0.0569 | 0.0236
Epoch 178/300, resid Loss: 0.0568 | 0.0235
Epoch 179/300, resid Loss: 0.0567 | 0.0235
Epoch 180/300, resid Loss: 0.0567 | 0.0235
Epoch 181/300, resid Loss: 0.0566 | 0.0235
Epoch 182/300, resid Loss: 0.0566 | 0.0235
Epoch 183/300, resid Loss: 0.0565 | 0.0235
Epoch 184/300, resid Loss: 0.0565 | 0.0234
Epoch 185/300, resid Loss: 0.0564 | 0.0234
Epoch 186/300, resid Loss: 0.0564 | 0.0233
Epoch 187/300, resid Loss: 0.0564 | 0.0232
Epoch 188/300, resid Loss: 0.0563 | 0.0232
Epoch 189/300, resid Loss: 0.0563 | 0.0231
Epoch 190/300, resid Loss: 0.0563 | 0.0230
Epoch 191/300, resid Loss: 0.0563 | 0.0230
Epoch 192/300, resid Loss: 0.0562 | 0.0229
Epoch 193/300, resid Loss: 0.0561 | 0.0229
Epoch 194/300, resid Loss: 0.0561 | 0.0228
Epoch 195/300, resid Loss: 0.0560 | 0.0228
Epoch 196/300, resid Loss: 0.0560 | 0.0228
Epoch 197/300, resid Loss: 0.0559 | 0.0227
Epoch 198/300, resid Loss: 0.0558 | 0.0227
Epoch 199/300, resid Loss: 0.0557 | 0.0226
Epoch 200/300, resid Loss: 0.0557 | 0.0226
Epoch 201/300, resid Loss: 0.0556 | 0.0226
Epoch 202/300, resid Loss: 0.0555 | 0.0225
Epoch 203/300, resid Loss: 0.0555 | 0.0225
Epoch 204/300, resid Loss: 0.0554 | 0.0226
Epoch 205/300, resid Loss: 0.0554 | 0.0226
Epoch 206/300, resid Loss: 0.0554 | 0.0227
Epoch 207/300, resid Loss: 0.0554 | 0.0227
Epoch 208/300, resid Loss: 0.0553 | 0.0227
Epoch 209/300, resid Loss: 0.0553 | 0.0227
Epoch 210/300, resid Loss: 0.0553 | 0.0228
Epoch 211/300, resid Loss: 0.0553 | 0.0231
Epoch 212/300, resid Loss: 0.0553 | 0.0231
Epoch 213/300, resid Loss: 0.0552 | 0.0231
Epoch 214/300, resid Loss: 0.0552 | 0.0231
Epoch 215/300, resid Loss: 0.0551 | 0.0230
Epoch 216/300, resid Loss: 0.0551 | 0.0230
Epoch 217/300, resid Loss: 0.0550 | 0.0229
Epoch 218/300, resid Loss: 0.0549 | 0.0229
Epoch 219/300, resid Loss: 0.0549 | 0.0229
Epoch 220/300, resid Loss: 0.0549 | 0.0228
Epoch 221/300, resid Loss: 0.0548 | 0.0227
Epoch 222/300, resid Loss: 0.0548 | 0.0226
Epoch 223/300, resid Loss: 0.0548 | 0.0226
Epoch 224/300, resid Loss: 0.0548 | 0.0225
Epoch 225/300, resid Loss: 0.0548 | 0.0225
Epoch 226/300, resid Loss: 0.0548 | 0.0225
Epoch 227/300, resid Loss: 0.0548 | 0.0224
Epoch 228/300, resid Loss: 0.0548 | 0.0224
Epoch 229/300, resid Loss: 0.0548 | 0.0224
Epoch 230/300, resid Loss: 0.0548 | 0.0223
Epoch 231/300, resid Loss: 0.0548 | 0.0223
Epoch 232/300, resid Loss: 0.0548 | 0.0223
Epoch 233/300, resid Loss: 0.0548 | 0.0223
Epoch 234/300, resid Loss: 0.0548 | 0.0224
Epoch 235/300, resid Loss: 0.0547 | 0.0223
Epoch 236/300, resid Loss: 0.0547 | 0.0223
Epoch 237/300, resid Loss: 0.0547 | 0.0223
Epoch 238/300, resid Loss: 0.0547 | 0.0223
Epoch 239/300, resid Loss: 0.0547 | 0.0223
Epoch 240/300, resid Loss: 0.0547 | 0.0223
Epoch 241/300, resid Loss: 0.0546 | 0.0224
Epoch 242/300, resid Loss: 0.0546 | 0.0223
Epoch 243/300, resid Loss: 0.0546 | 0.0224
Epoch 244/300, resid Loss: 0.0546 | 0.0223
Epoch 245/300, resid Loss: 0.0546 | 0.0223
Epoch 246/300, resid Loss: 0.0546 | 0.0223
Epoch 247/300, resid Loss: 0.0546 | 0.0223
Epoch 248/300, resid Loss: 0.0546 | 0.0223
Epoch 249/300, resid Loss: 0.0546 | 0.0223
Epoch 250/300, resid Loss: 0.0546 | 0.0222
Epoch 251/300, resid Loss: 0.0546 | 0.0222
Epoch 252/300, resid Loss: 0.0546 | 0.0222
Epoch 253/300, resid Loss: 0.0546 | 0.0222
Epoch 254/300, resid Loss: 0.0546 | 0.0221
Epoch 255/300, resid Loss: 0.0546 | 0.0221
Epoch 256/300, resid Loss: 0.0547 | 0.0221
Epoch 257/300, resid Loss: 0.0547 | 0.0221
Epoch 258/300, resid Loss: 0.0547 | 0.0221
Epoch 259/300, resid Loss: 0.0547 | 0.0221
Epoch 260/300, resid Loss: 0.0547 | 0.0221
Epoch 261/300, resid Loss: 0.0547 | 0.0221
Epoch 262/300, resid Loss: 0.0547 | 0.0222
Epoch 263/300, resid Loss: 0.0548 | 0.0222
Epoch 264/300, resid Loss: 0.0549 | 0.0223
Epoch 265/300, resid Loss: 0.0551 | 0.0224
Epoch 266/300, resid Loss: 0.0555 | 0.0225
Epoch 267/300, resid Loss: 0.0558 | 0.0227
Epoch 268/300, resid Loss: 0.0559 | 0.0225
Epoch 269/300, resid Loss: 0.0556 | 0.0223
Epoch 270/300, resid Loss: 0.0552 | 0.0222
Epoch 271/300, resid Loss: 0.0549 | 0.0221
Epoch 272/300, resid Loss: 0.0547 | 0.0221
Epoch 273/300, resid Loss: 0.0545 | 0.0220
Epoch 274/300, resid Loss: 0.0544 | 0.0220
Epoch 275/300, resid Loss: 0.0542 | 0.0219
Epoch 276/300, resid Loss: 0.0541 | 0.0219
Epoch 277/300, resid Loss: 0.0540 | 0.0219
Epoch 278/300, resid Loss: 0.0540 | 0.0219
Epoch 279/300, resid Loss: 0.0539 | 0.0219
Epoch 280/300, resid Loss: 0.0539 | 0.0219
Epoch 281/300, resid Loss: 0.0539 | 0.0219
Epoch 282/300, resid Loss: 0.0538 | 0.0219
Epoch 283/300, resid Loss: 0.0538 | 0.0219
Epoch 284/300, resid Loss: 0.0538 | 0.0219
Epoch 285/300, resid Loss: 0.0538 | 0.0219
Epoch 286/300, resid Loss: 0.0538 | 0.0219
Epoch 287/300, resid Loss: 0.0538 | 0.0219
Epoch 288/300, resid Loss: 0.0538 | 0.0219
Epoch 289/300, resid Loss: 0.0538 | 0.0219
Epoch 290/300, resid Loss: 0.0537 | 0.0219
Epoch 291/300, resid Loss: 0.0537 | 0.0219
Epoch 292/300, resid Loss: 0.0537 | 0.0219
Epoch 293/300, resid Loss: 0.0537 | 0.0219
Epoch 294/300, resid Loss: 0.0537 | 0.0219
Epoch 295/300, resid Loss: 0.0537 | 0.0219
Epoch 296/300, resid Loss: 0.0537 | 0.0219
Epoch 297/300, resid Loss: 0.0537 | 0.0219
Epoch 298/300, resid Loss: 0.0537 | 0.0219
Epoch 299/300, resid Loss: 0.0537 | 0.0219
Epoch 300/300, resid Loss: 0.0537 | 0.0219
Runtime (seconds): 2485.847494363785
0.00011341307324445398
[211.75174]
[1.3446193]
[0.6100944]
[6.6878176]
[1.0044351]
[8.711449]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 52.125993948662654
RMSE: 7.2198333740234375
MAE: 7.2198333740234375
R-squared: nan
[230.11017]
