ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-07 07:48:54,832][0m A new study created in memory with name: no-name-b495dc20-3b94-4e08-b766-4e203695e672[0m
[32m[I 2025-02-07 07:49:21,535][0m Trial 0 finished with value: 0.7979702096314581 and parameters: {'observation_period_num': 140, 'train_rates': 0.6300009777102364, 'learning_rate': 1.6998582339728432e-06, 'batch_size': 194, 'step_size': 13, 'gamma': 0.9730721228373074}. Best is trial 0 with value: 0.7979702096314581.[0m
[32m[I 2025-02-07 07:50:00,704][0m Trial 1 finished with value: 0.7464251101963104 and parameters: {'observation_period_num': 247, 'train_rates': 0.9070648289662817, 'learning_rate': 2.7360222838406016e-06, 'batch_size': 148, 'step_size': 12, 'gamma': 0.7824699858198565}. Best is trial 1 with value: 0.7464251101963104.[0m
[32m[I 2025-02-07 07:50:21,981][0m Trial 2 finished with value: 0.4027325471425386 and parameters: {'observation_period_num': 219, 'train_rates': 0.6041296569097244, 'learning_rate': 0.000297026486714568, 'batch_size': 228, 'step_size': 1, 'gamma': 0.8863492471819265}. Best is trial 2 with value: 0.4027325471425386.[0m
[32m[I 2025-02-07 07:52:47,462][0m Trial 3 finished with value: 0.1410934625209768 and parameters: {'observation_period_num': 21, 'train_rates': 0.6497871089232612, 'learning_rate': 0.0009245146869418622, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9799210603999281}. Best is trial 3 with value: 0.1410934625209768.[0m
[32m[I 2025-02-07 07:53:16,078][0m Trial 4 finished with value: 0.17963986098766327 and parameters: {'observation_period_num': 132, 'train_rates': 0.9784151392381297, 'learning_rate': 0.00019289078571911066, 'batch_size': 217, 'step_size': 9, 'gamma': 0.9721679219029173}. Best is trial 3 with value: 0.1410934625209768.[0m
[32m[I 2025-02-07 07:54:05,719][0m Trial 5 finished with value: 0.04359883442521095 and parameters: {'observation_period_num': 6, 'train_rates': 0.9582493064755547, 'learning_rate': 0.0001285972629970269, 'batch_size': 124, 'step_size': 12, 'gamma': 0.8498016785346892}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:55:02,006][0m Trial 6 finished with value: 0.3056776757036984 and parameters: {'observation_period_num': 160, 'train_rates': 0.6165466217037736, 'learning_rate': 0.0003299438843755798, 'batch_size': 78, 'step_size': 3, 'gamma': 0.907399368678319}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:55:44,157][0m Trial 7 finished with value: 0.5256467165897161 and parameters: {'observation_period_num': 135, 'train_rates': 0.7289541491634314, 'learning_rate': 2.496324869975939e-05, 'batch_size': 116, 'step_size': 2, 'gamma': 0.8904327477964789}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:56:22,123][0m Trial 8 finished with value: 0.18448779284237668 and parameters: {'observation_period_num': 52, 'train_rates': 0.7497663626589239, 'learning_rate': 0.00027476999706338944, 'batch_size': 137, 'step_size': 11, 'gamma': 0.8684034776228797}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:56:45,572][0m Trial 9 finished with value: 0.18475312016148498 and parameters: {'observation_period_num': 43, 'train_rates': 0.6688995482805827, 'learning_rate': 0.0009267155579209745, 'batch_size': 232, 'step_size': 2, 'gamma': 0.8312294793964629}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:58:01,371][0m Trial 10 finished with value: 0.08510508265957903 and parameters: {'observation_period_num': 79, 'train_rates': 0.8604221839908796, 'learning_rate': 2.8713578124840475e-05, 'batch_size': 72, 'step_size': 15, 'gamma': 0.7605756397610552}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 07:59:22,901][0m Trial 11 finished with value: 0.08686123992360774 and parameters: {'observation_period_num': 80, 'train_rates': 0.8688343156285926, 'learning_rate': 2.8805176067678265e-05, 'batch_size': 69, 'step_size': 15, 'gamma': 0.7517889860217968}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 08:00:24,557][0m Trial 12 finished with value: 0.20597725800300007 and parameters: {'observation_period_num': 88, 'train_rates': 0.8629399393984091, 'learning_rate': 9.243782887199951e-06, 'batch_size': 90, 'step_size': 15, 'gamma': 0.82243927514042}. Best is trial 5 with value: 0.04359883442521095.[0m
[32m[I 2025-02-07 08:03:53,208][0m Trial 13 finished with value: 0.02911333109323795 and parameters: {'observation_period_num': 10, 'train_rates': 0.9869380172867523, 'learning_rate': 8.007600644772606e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.7978128754095672}. Best is trial 13 with value: 0.02911333109323795.[0m
[32m[I 2025-02-07 08:09:23,947][0m Trial 14 finished with value: 0.022085716897113757 and parameters: {'observation_period_num': 11, 'train_rates': 0.9813758966883848, 'learning_rate': 9.836743040381044e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8170992920585325}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:12:56,487][0m Trial 15 finished with value: 0.04398887744663577 and parameters: {'observation_period_num': 37, 'train_rates': 0.9352737502786445, 'learning_rate': 9.198950237742013e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.7998358286926236}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:17:57,545][0m Trial 16 finished with value: 0.03664952996922167 and parameters: {'observation_period_num': 5, 'train_rates': 0.9873073448842, 'learning_rate': 7.261309988564602e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.805634323226826}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:19:35,146][0m Trial 17 finished with value: 0.3515051264414745 and parameters: {'observation_period_num': 183, 'train_rates': 0.7965829906613207, 'learning_rate': 7.04387593445413e-06, 'batch_size': 51, 'step_size': 9, 'gamma': 0.7836644595570772}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:20:12,775][0m Trial 18 finished with value: 0.149377363873279 and parameters: {'observation_period_num': 92, 'train_rates': 0.9116011657237131, 'learning_rate': 5.35267786342812e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.9246030834032475}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:21:58,105][0m Trial 19 finished with value: 0.16654662305006274 and parameters: {'observation_period_num': 51, 'train_rates': 0.8142637833198394, 'learning_rate': 8.137736547977542e-06, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8488751410000382}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:22:58,794][0m Trial 20 finished with value: 0.4087806967171756 and parameters: {'observation_period_num': 108, 'train_rates': 0.9383121862740234, 'learning_rate': 1.359157431302925e-05, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8244486811580666}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:26:56,961][0m Trial 21 finished with value: 0.04979089790811905 and parameters: {'observation_period_num': 20, 'train_rates': 0.9867420625607571, 'learning_rate': 6.398427670662737e-05, 'batch_size': 25, 'step_size': 4, 'gamma': 0.8009000790770052}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:33:09,325][0m Trial 22 finished with value: 0.030883268088750217 and parameters: {'observation_period_num': 9, 'train_rates': 0.9844300112652286, 'learning_rate': 6.299782573371764e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8079561326135912}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:34:51,786][0m Trial 23 finished with value: 0.055576070438365675 and parameters: {'observation_period_num': 67, 'train_rates': 0.9104392766643639, 'learning_rate': 0.00013092228366779415, 'batch_size': 55, 'step_size': 8, 'gamma': 0.7813774766137764}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:37:07,619][0m Trial 24 finished with value: 0.05179798589673585 and parameters: {'observation_period_num': 30, 'train_rates': 0.9431172933568829, 'learning_rate': 4.024926483824485e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8407794381232923}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:42:27,062][0m Trial 25 finished with value: 0.0910550928559185 and parameters: {'observation_period_num': 62, 'train_rates': 0.9584677216897132, 'learning_rate': 1.8188497958684475e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8084296815023849}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:44:47,222][0m Trial 26 finished with value: 0.02445602585231104 and parameters: {'observation_period_num': 5, 'train_rates': 0.8846066330156814, 'learning_rate': 0.0004647947800336445, 'batch_size': 40, 'step_size': 6, 'gamma': 0.772284247853895}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:45:42,448][0m Trial 27 finished with value: 0.04939260143180226 and parameters: {'observation_period_num': 27, 'train_rates': 0.8388738939794023, 'learning_rate': 0.0005138977039260576, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7701250867643479}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:47:59,673][0m Trial 28 finished with value: 0.13930362030679797 and parameters: {'observation_period_num': 106, 'train_rates': 0.8864938189472883, 'learning_rate': 0.00015703669192621998, 'batch_size': 40, 'step_size': 11, 'gamma': 0.7706848114562803}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:49:26,477][0m Trial 29 finished with value: 0.18017347805615339 and parameters: {'observation_period_num': 40, 'train_rates': 0.7591146346982779, 'learning_rate': 0.00047734649900174614, 'batch_size': 59, 'step_size': 6, 'gamma': 0.8594988434746477}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:49:57,505][0m Trial 30 finished with value: 0.14419484152955916 and parameters: {'observation_period_num': 180, 'train_rates': 0.9262029893633035, 'learning_rate': 0.0005483378596594371, 'batch_size': 192, 'step_size': 10, 'gamma': 0.787917731100198}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:52:36,766][0m Trial 31 finished with value: 0.038893412897544624 and parameters: {'observation_period_num': 8, 'train_rates': 0.9619062685111215, 'learning_rate': 9.441544991292576e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8144587027193759}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 08:58:20,850][0m Trial 32 finished with value: 0.34291103140857565 and parameters: {'observation_period_num': 19, 'train_rates': 0.8902502404857994, 'learning_rate': 1.335332128969999e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7925395394013095}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:01:07,338][0m Trial 33 finished with value: 0.028580151424549595 and parameters: {'observation_period_num': 6, 'train_rates': 0.9659542943790814, 'learning_rate': 0.0002171410793503198, 'batch_size': 36, 'step_size': 6, 'gamma': 0.7691424381380958}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:02:41,621][0m Trial 34 finished with value: 0.04403527272136315 and parameters: {'observation_period_num': 29, 'train_rates': 0.9610341627240288, 'learning_rate': 0.00034103554589276795, 'batch_size': 63, 'step_size': 6, 'gamma': 0.76789096463996}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:04:57,711][0m Trial 35 finished with value: 0.17640172531955864 and parameters: {'observation_period_num': 236, 'train_rates': 0.8930909400926935, 'learning_rate': 0.0002087209944963578, 'batch_size': 38, 'step_size': 13, 'gamma': 0.7518413391882306}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:06:06,702][0m Trial 36 finished with value: 0.05913338322176163 and parameters: {'observation_period_num': 60, 'train_rates': 0.9212549514891377, 'learning_rate': 0.00023943736967868757, 'batch_size': 85, 'step_size': 7, 'gamma': 0.7743420209763995}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:06:59,266][0m Trial 37 finished with value: 0.036043763160705566 and parameters: {'observation_period_num': 24, 'train_rates': 0.8338285989440413, 'learning_rate': 0.000695628404247025, 'batch_size': 107, 'step_size': 8, 'gamma': 0.7939294714538473}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:10:03,590][0m Trial 38 finished with value: 0.0772863366447463 and parameters: {'observation_period_num': 43, 'train_rates': 0.965686427013724, 'learning_rate': 0.00010847157146957033, 'batch_size': 32, 'step_size': 10, 'gamma': 0.9560801712873902}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:10:39,604][0m Trial 39 finished with value: 0.9454364776611328 and parameters: {'observation_period_num': 151, 'train_rates': 0.9465279313058337, 'learning_rate': 3.7489778136915955e-06, 'batch_size': 175, 'step_size': 4, 'gamma': 0.8334542446921557}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:12:46,519][0m Trial 40 finished with value: 0.0533474258014134 and parameters: {'observation_period_num': 14, 'train_rates': 0.9715284637609444, 'learning_rate': 4.288215520081243e-05, 'batch_size': 48, 'step_size': 9, 'gamma': 0.7589105535633198}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:19:00,308][0m Trial 41 finished with value: 0.027939938891090844 and parameters: {'observation_period_num': 8, 'train_rates': 0.9808554563656269, 'learning_rate': 0.00017815693329537504, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8166537809207052}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:22:14,558][0m Trial 42 finished with value: 0.03197615221142769 and parameters: {'observation_period_num': 17, 'train_rates': 0.9897809905115454, 'learning_rate': 0.0003647029965720578, 'batch_size': 31, 'step_size': 6, 'gamma': 0.820315829718545}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:23:32,014][0m Trial 43 finished with value: 0.043430669167151686 and parameters: {'observation_period_num': 34, 'train_rates': 0.9520194441847932, 'learning_rate': 0.00020201068844920148, 'batch_size': 79, 'step_size': 8, 'gamma': 0.8807871869906981}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:25:05,936][0m Trial 44 finished with value: 0.05104590362996003 and parameters: {'observation_period_num': 6, 'train_rates': 0.9278690186632589, 'learning_rate': 0.00015479841579237898, 'batch_size': 65, 'step_size': 3, 'gamma': 0.7812700756359008}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:28:04,158][0m Trial 45 finished with value: 0.053942104262348856 and parameters: {'observation_period_num': 50, 'train_rates': 0.9741762365210533, 'learning_rate': 0.0002573269799766793, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8396675271362921}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:28:29,084][0m Trial 46 finished with value: 0.08054772967408444 and parameters: {'observation_period_num': 16, 'train_rates': 0.9017221431631921, 'learning_rate': 8.35065907644935e-05, 'batch_size': 249, 'step_size': 13, 'gamma': 0.761555182253857}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:29:08,319][0m Trial 47 finished with value: 0.1918182600916454 and parameters: {'observation_period_num': 72, 'train_rates': 0.6767373619313198, 'learning_rate': 0.00015643651789932722, 'batch_size': 127, 'step_size': 6, 'gamma': 0.7986937532750866}. Best is trial 14 with value: 0.022085716897113757.[0m
Early stopping at epoch 74
[32m[I 2025-02-07 09:31:40,543][0m Trial 48 finished with value: 0.04755319482764276 and parameters: {'observation_period_num': 38, 'train_rates': 0.8778624652698241, 'learning_rate': 0.00072671827986423, 'batch_size': 27, 'step_size': 1, 'gamma': 0.7865947684609943}. Best is trial 14 with value: 0.022085716897113757.[0m
[32m[I 2025-02-07 09:32:59,396][0m Trial 49 finished with value: 0.18781738805359807 and parameters: {'observation_period_num': 205, 'train_rates': 0.9474268519931737, 'learning_rate': 0.00037945541932345823, 'batch_size': 72, 'step_size': 11, 'gamma': 0.8162371684599391}. Best is trial 14 with value: 0.022085716897113757.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-07 09:32:59,407][0m A new study created in memory with name: no-name-3c1d4ebe-30e1-4b5a-b62b-02ec870fe31a[0m
[32m[I 2025-02-07 09:33:29,944][0m Trial 0 finished with value: 0.8700432229768201 and parameters: {'observation_period_num': 232, 'train_rates': 0.7833092002017249, 'learning_rate': 1.0188078477301713e-06, 'batch_size': 171, 'step_size': 10, 'gamma': 0.8747427858649032}. Best is trial 0 with value: 0.8700432229768201.[0m
[32m[I 2025-02-07 09:33:51,400][0m Trial 1 finished with value: 0.8407322454668604 and parameters: {'observation_period_num': 139, 'train_rates': 0.7651784550103556, 'learning_rate': 6.450211481491681e-06, 'batch_size': 249, 'step_size': 3, 'gamma': 0.8379006271036137}. Best is trial 1 with value: 0.8407322454668604.[0m
[32m[I 2025-02-07 09:34:16,909][0m Trial 2 finished with value: 0.3329493168535474 and parameters: {'observation_period_num': 229, 'train_rates': 0.6091018517386163, 'learning_rate': 0.0001210654362296146, 'batch_size': 173, 'step_size': 8, 'gamma': 0.8358854898116161}. Best is trial 2 with value: 0.3329493168535474.[0m
[32m[I 2025-02-07 09:34:42,521][0m Trial 3 finished with value: 0.14318729763502602 and parameters: {'observation_period_num': 71, 'train_rates': 0.8427199847661823, 'learning_rate': 0.0002474186251160448, 'batch_size': 228, 'step_size': 3, 'gamma': 0.9812506096814413}. Best is trial 3 with value: 0.14318729763502602.[0m
[32m[I 2025-02-07 09:35:08,110][0m Trial 4 finished with value: 0.1783178168631608 and parameters: {'observation_period_num': 209, 'train_rates': 0.8541122578534397, 'learning_rate': 0.000690873671555469, 'batch_size': 233, 'step_size': 6, 'gamma': 0.9352602744285353}. Best is trial 3 with value: 0.14318729763502602.[0m
[32m[I 2025-02-07 09:36:05,479][0m Trial 5 finished with value: 0.4600133116353995 and parameters: {'observation_period_num': 164, 'train_rates': 0.8901936831044304, 'learning_rate': 3.845476992187236e-06, 'batch_size': 97, 'step_size': 15, 'gamma': 0.8476080529280328}. Best is trial 3 with value: 0.14318729763502602.[0m
[32m[I 2025-02-07 09:36:36,706][0m Trial 6 finished with value: 0.03793383868980227 and parameters: {'observation_period_num': 10, 'train_rates': 0.8213134881550168, 'learning_rate': 0.0009960551334127897, 'batch_size': 188, 'step_size': 8, 'gamma': 0.891567743398706}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:37:16,488][0m Trial 7 finished with value: 0.39920630442803023 and parameters: {'observation_period_num': 27, 'train_rates': 0.7920246519226315, 'learning_rate': 8.569218452365779e-06, 'batch_size': 137, 'step_size': 2, 'gamma': 0.9640539460339843}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:37:53,052][0m Trial 8 finished with value: 0.11263056132818988 and parameters: {'observation_period_num': 112, 'train_rates': 0.8198655356328236, 'learning_rate': 0.00032008702571046125, 'batch_size': 151, 'step_size': 4, 'gamma': 0.9670356759907935}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:39:00,611][0m Trial 9 finished with value: 0.05159423474783767 and parameters: {'observation_period_num': 20, 'train_rates': 0.9503529011773864, 'learning_rate': 0.0001236429554794687, 'batch_size': 88, 'step_size': 14, 'gamma': 0.8667803346241726}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:43:11,066][0m Trial 10 finished with value: 0.18430296336450885 and parameters: {'observation_period_num': 71, 'train_rates': 0.6780361388500968, 'learning_rate': 3.227162021487861e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7663250681439555}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:44:21,209][0m Trial 11 finished with value: 0.0519086942869298 and parameters: {'observation_period_num': 5, 'train_rates': 0.9683012929962361, 'learning_rate': 6.916517073745852e-05, 'batch_size': 88, 'step_size': 15, 'gamma': 0.8983809655290308}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:45:36,798][0m Trial 12 finished with value: 0.06914244323141044 and parameters: {'observation_period_num': 48, 'train_rates': 0.9629769974509866, 'learning_rate': 0.0009808315712763784, 'batch_size': 78, 'step_size': 12, 'gamma': 0.7959478617841601}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:46:09,369][0m Trial 13 finished with value: 0.042291886726161465 and parameters: {'observation_period_num': 8, 'train_rates': 0.9133394232739724, 'learning_rate': 0.0002748670727016888, 'batch_size': 201, 'step_size': 8, 'gamma': 0.9097625080370167}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:46:39,146][0m Trial 14 finished with value: 0.10169671652137592 and parameters: {'observation_period_num': 85, 'train_rates': 0.9030462661503282, 'learning_rate': 0.0004006606563388231, 'batch_size': 213, 'step_size': 8, 'gamma': 0.91192184875174}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:47:06,338][0m Trial 15 finished with value: 0.17671214769545354 and parameters: {'observation_period_num': 50, 'train_rates': 0.7322825835511885, 'learning_rate': 0.00087532757022937, 'batch_size': 205, 'step_size': 6, 'gamma': 0.9288255989364145}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:47:37,587][0m Trial 16 finished with value: 0.2011850249255076 and parameters: {'observation_period_num': 111, 'train_rates': 0.9100995039206037, 'learning_rate': 3.888804910918633e-05, 'batch_size': 194, 'step_size': 6, 'gamma': 0.8891728416679452}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:48:07,811][0m Trial 17 finished with value: 0.16720809108319978 and parameters: {'observation_period_num': 36, 'train_rates': 0.7020958224375378, 'learning_rate': 0.0001609165361703891, 'batch_size': 171, 'step_size': 9, 'gamma': 0.9428625811675155}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:48:31,064][0m Trial 18 finished with value: 0.13208041331788015 and parameters: {'observation_period_num': 157, 'train_rates': 0.8724039372018739, 'learning_rate': 0.0004271771213480417, 'batch_size': 251, 'step_size': 12, 'gamma': 0.7976437069411355}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:49:20,116][0m Trial 19 finished with value: 0.09775294275221666 and parameters: {'observation_period_num': 9, 'train_rates': 0.9287273967865233, 'learning_rate': 1.786511772937743e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9102217637344944}. Best is trial 6 with value: 0.03793383868980227.[0m
Early stopping at epoch 97
[32m[I 2025-02-07 09:53:44,477][0m Trial 20 finished with value: 0.17792236412826337 and parameters: {'observation_period_num': 193, 'train_rates': 0.98659322874512, 'learning_rate': 7.126640428388792e-05, 'batch_size': 21, 'step_size': 1, 'gamma': 0.8689007039789451}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:55:37,725][0m Trial 21 finished with value: 0.044872732689747445 and parameters: {'observation_period_num': 22, 'train_rates': 0.9424883347353328, 'learning_rate': 0.00016374313038520878, 'batch_size': 52, 'step_size': 10, 'gamma': 0.8624692498394917}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:57:20,854][0m Trial 22 finished with value: 0.0864512175321579 and parameters: {'observation_period_num': 56, 'train_rates': 0.9340936741748078, 'learning_rate': 0.00023265494495078098, 'batch_size': 56, 'step_size': 10, 'gamma': 0.8516224295121443}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:57:51,741][0m Trial 23 finished with value: 0.04556613370776177 and parameters: {'observation_period_num': 27, 'train_rates': 0.8210913620634843, 'learning_rate': 0.0005817902782232399, 'batch_size': 189, 'step_size': 7, 'gamma': 0.8818550639133504}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 09:59:44,898][0m Trial 24 finished with value: 0.04260854121735867 and parameters: {'observation_period_num': 7, 'train_rates': 0.8850100976326775, 'learning_rate': 7.594845341174799e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8164865026743678}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:00:33,954][0m Trial 25 finished with value: 0.11246872588020304 and parameters: {'observation_period_num': 95, 'train_rates': 0.8716077474322709, 'learning_rate': 5.614595619001425e-05, 'batch_size': 117, 'step_size': 8, 'gamma': 0.8189135738699727}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:01:10,694][0m Trial 26 finished with value: 0.05567428690177956 and parameters: {'observation_period_num': 43, 'train_rates': 0.8209587173087869, 'learning_rate': 0.0004605854652535796, 'batch_size': 151, 'step_size': 9, 'gamma': 0.7544997168166281}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:01:35,826][0m Trial 27 finished with value: 0.06791591750723976 and parameters: {'observation_period_num': 6, 'train_rates': 0.845876976138187, 'learning_rate': 9.232401324435055e-05, 'batch_size': 229, 'step_size': 12, 'gamma': 0.8111392224681425}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:02:04,665][0m Trial 28 finished with value: 0.3172656763447912 and parameters: {'observation_period_num': 76, 'train_rates': 0.749107064049007, 'learning_rate': 2.0557825801341517e-05, 'batch_size': 185, 'step_size': 7, 'gamma': 0.9166829614782077}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:02:41,127][0m Trial 29 finished with value: 0.06572665278412892 and parameters: {'observation_period_num': 54, 'train_rates': 0.8800470101529171, 'learning_rate': 0.000216583054464857, 'batch_size': 163, 'step_size': 9, 'gamma': 0.7799738881293001}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:03:10,485][0m Trial 30 finished with value: 0.8255740975646816 and parameters: {'observation_period_num': 31, 'train_rates': 0.9170354541783917, 'learning_rate': 1.9524464887225583e-06, 'batch_size': 208, 'step_size': 11, 'gamma': 0.8959502423448292}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:05:17,981][0m Trial 31 finished with value: 0.043828611559976784 and parameters: {'observation_period_num': 20, 'train_rates': 0.9331873761884756, 'learning_rate': 0.00015065130000626995, 'batch_size': 46, 'step_size': 10, 'gamma': 0.8558394847740849}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:07:22,717][0m Trial 32 finished with value: 0.03967458611394433 and parameters: {'observation_period_num': 19, 'train_rates': 0.9055411945881836, 'learning_rate': 0.0002979685129307446, 'batch_size': 46, 'step_size': 7, 'gamma': 0.8211847234509992}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:08:41,414][0m Trial 33 finished with value: 0.1658582242324046 and parameters: {'observation_period_num': 7, 'train_rates': 0.7705881834434128, 'learning_rate': 0.0006245199423961411, 'batch_size': 66, 'step_size': 7, 'gamma': 0.8286786082049061}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:11:34,666][0m Trial 34 finished with value: 0.09955014791911779 and parameters: {'observation_period_num': 64, 'train_rates': 0.8990339776684739, 'learning_rate': 0.00028565129924757956, 'batch_size': 32, 'step_size': 8, 'gamma': 0.8380314190410936}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:14:03,533][0m Trial 35 finished with value: 0.06555329687720121 and parameters: {'observation_period_num': 18, 'train_rates': 0.8538115724625793, 'learning_rate': 4.510995167096871e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8037846046508577}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:14:55,660][0m Trial 36 finished with value: 0.1224517092783692 and parameters: {'observation_period_num': 39, 'train_rates': 0.8033966814091363, 'learning_rate': 1.934617265271558e-05, 'batch_size': 104, 'step_size': 7, 'gamma': 0.828040249833268}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:16:21,442][0m Trial 37 finished with value: 0.05588069662713168 and parameters: {'observation_period_num': 40, 'train_rates': 0.8634785540525227, 'learning_rate': 9.417813798758522e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.7783757525127591}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:16:42,440][0m Trial 38 finished with value: 0.2773407060046529 and parameters: {'observation_period_num': 233, 'train_rates': 0.621551232481236, 'learning_rate': 0.000358537526602212, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9473219091292164}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:17:26,131][0m Trial 39 finished with value: 0.11346918443524384 and parameters: {'observation_period_num': 130, 'train_rates': 0.8324986695121154, 'learning_rate': 0.0006803025744540133, 'batch_size': 129, 'step_size': 8, 'gamma': 0.8802435783585655}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:18:06,786][0m Trial 40 finished with value: 0.09146688586083528 and parameters: {'observation_period_num': 93, 'train_rates': 0.888270511293709, 'learning_rate': 0.00020948569389867756, 'batch_size': 142, 'step_size': 4, 'gamma': 0.842059288750088}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:20:23,602][0m Trial 41 finished with value: 0.04431348397679951 and parameters: {'observation_period_num': 19, 'train_rates': 0.9218182805296598, 'learning_rate': 0.0001489943839880816, 'batch_size': 42, 'step_size': 11, 'gamma': 0.8561599901587283}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:22:17,872][0m Trial 42 finished with value: 0.04311067214526168 and parameters: {'observation_period_num': 19, 'train_rates': 0.9629338030144375, 'learning_rate': 0.00010413614874780192, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8263386023040861}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:25:45,127][0m Trial 43 finished with value: 0.038367706181650814 and parameters: {'observation_period_num': 30, 'train_rates': 0.9850339411945779, 'learning_rate': 9.995380358270763e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8242310130227508}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:28:36,272][0m Trial 44 finished with value: 0.19503883759796523 and parameters: {'observation_period_num': 34, 'train_rates': 0.7904946113449773, 'learning_rate': 6.958560888298051e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.7855079375116264}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:29:05,365][0m Trial 45 finished with value: 0.0971376970410347 and parameters: {'observation_period_num': 62, 'train_rates': 0.9897853522178764, 'learning_rate': 0.0002867126823428065, 'batch_size': 221, 'step_size': 6, 'gamma': 0.813522744240352}. Best is trial 6 with value: 0.03793383868980227.[0m
[32m[I 2025-02-07 10:34:33,735][0m Trial 46 finished with value: 0.034676731772282546 and parameters: {'observation_period_num': 12, 'train_rates': 0.9714805056951582, 'learning_rate': 0.0004937659633207597, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8737322253347861}. Best is trial 46 with value: 0.034676731772282546.[0m
[32m[I 2025-02-07 10:38:36,681][0m Trial 47 finished with value: 0.053933531045913696 and parameters: {'observation_period_num': 47, 'train_rates': 0.9713769686010837, 'learning_rate': 0.0007992889420750394, 'batch_size': 24, 'step_size': 8, 'gamma': 0.9229076572312558}. Best is trial 46 with value: 0.034676731772282546.[0m
[32m[I 2025-02-07 10:44:39,970][0m Trial 48 finished with value: 0.05295637790376032 and parameters: {'observation_period_num': 30, 'train_rates': 0.9518261302305697, 'learning_rate': 0.0005287128744563994, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8995101364514739}. Best is trial 46 with value: 0.034676731772282546.[0m
[32m[I 2025-02-07 10:45:13,084][0m Trial 49 finished with value: 0.1488182246685028 and parameters: {'observation_period_num': 194, 'train_rates': 0.952799534457928, 'learning_rate': 0.00038106642666670067, 'batch_size': 179, 'step_size': 6, 'gamma': 0.8708571646249206}. Best is trial 46 with value: 0.034676731772282546.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-07 10:45:13,098][0m A new study created in memory with name: no-name-14227441-1bed-4c44-a4be-854ce6de3e87[0m
[32m[I 2025-02-07 10:45:48,382][0m Trial 0 finished with value: 0.06357162445783615 and parameters: {'observation_period_num': 74, 'train_rates': 0.9757981748723705, 'learning_rate': 0.0006600436638167973, 'batch_size': 178, 'step_size': 13, 'gamma': 0.8046427341991338}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:46:46,738][0m Trial 1 finished with value: 0.35346307888655654 and parameters: {'observation_period_num': 77, 'train_rates': 0.6383824615272037, 'learning_rate': 3.4436870974914416e-06, 'batch_size': 79, 'step_size': 10, 'gamma': 0.988008354551918}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:47:51,081][0m Trial 2 finished with value: 0.1642725169658661 and parameters: {'observation_period_num': 210, 'train_rates': 0.97106690472191, 'learning_rate': 6.56376471012827e-05, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9358936965897993}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:49:14,180][0m Trial 3 finished with value: 0.26642030285250756 and parameters: {'observation_period_num': 150, 'train_rates': 0.9451338003657054, 'learning_rate': 0.0006112006161889261, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9701002971971087}. Best is trial 0 with value: 0.06357162445783615.[0m
Early stopping at epoch 65
[32m[I 2025-02-07 10:49:50,180][0m Trial 4 finished with value: 0.4157785636255111 and parameters: {'observation_period_num': 137, 'train_rates': 0.7533874632598807, 'learning_rate': 0.0002514760705681016, 'batch_size': 97, 'step_size': 1, 'gamma': 0.7974683271639599}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:50:40,516][0m Trial 5 finished with value: 0.4186131346077692 and parameters: {'observation_period_num': 87, 'train_rates': 0.6777125268107752, 'learning_rate': 1.1467043788753025e-05, 'batch_size': 95, 'step_size': 9, 'gamma': 0.7652627787737976}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:51:14,342][0m Trial 6 finished with value: 0.07521668076515198 and parameters: {'observation_period_num': 78, 'train_rates': 0.9553295255025548, 'learning_rate': 0.00039401172276141294, 'batch_size': 196, 'step_size': 8, 'gamma': 0.8719786085563932}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:53:12,147][0m Trial 7 finished with value: 0.1773277910301379 and parameters: {'observation_period_num': 69, 'train_rates': 0.6334529742761077, 'learning_rate': 0.0005330287615745046, 'batch_size': 37, 'step_size': 7, 'gamma': 0.9832464699350483}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:53:45,508][0m Trial 8 finished with value: 0.06559247233553561 and parameters: {'observation_period_num': 16, 'train_rates': 0.8865375733914134, 'learning_rate': 3.877262993643339e-05, 'batch_size': 184, 'step_size': 14, 'gamma': 0.869906425199193}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:54:20,460][0m Trial 9 finished with value: 0.7323770785904727 and parameters: {'observation_period_num': 191, 'train_rates': 0.6541669026654309, 'learning_rate': 2.3943841035046338e-06, 'batch_size': 136, 'step_size': 6, 'gamma': 0.8216985643692103}. Best is trial 0 with value: 0.06357162445783615.[0m
[32m[I 2025-02-07 10:54:45,497][0m Trial 10 finished with value: 0.05458897122342946 and parameters: {'observation_period_num': 12, 'train_rates': 0.8451823855958329, 'learning_rate': 0.00011646780570458711, 'batch_size': 239, 'step_size': 12, 'gamma': 0.8535180963492531}. Best is trial 10 with value: 0.05458897122342946.[0m
[32m[I 2025-02-07 10:55:08,782][0m Trial 11 finished with value: 0.05912157480177488 and parameters: {'observation_period_num': 9, 'train_rates': 0.8472406768522343, 'learning_rate': 0.00012284231491165346, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8454041777958552}. Best is trial 10 with value: 0.05458897122342946.[0m
[32m[I 2025-02-07 10:55:31,747][0m Trial 12 finished with value: 0.05705058698314131 and parameters: {'observation_period_num': 10, 'train_rates': 0.8437158732307142, 'learning_rate': 0.0001229516813382769, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8736636466107653}. Best is trial 10 with value: 0.05458897122342946.[0m
[32m[I 2025-02-07 10:55:53,403][0m Trial 13 finished with value: 0.5365036763433142 and parameters: {'observation_period_num': 252, 'train_rates': 0.773589918195497, 'learning_rate': 1.118179016793382e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.9101389085785534}. Best is trial 10 with value: 0.05458897122342946.[0m
[32m[I 2025-02-07 10:56:21,691][0m Trial 14 finished with value: 0.05412637896496406 and parameters: {'observation_period_num': 30, 'train_rates': 0.8428088206108102, 'learning_rate': 0.00013036102614765718, 'batch_size': 220, 'step_size': 15, 'gamma': 0.8984021899833411}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:56:49,673][0m Trial 15 finished with value: 0.09730488910924556 and parameters: {'observation_period_num': 42, 'train_rates': 0.8230528372564904, 'learning_rate': 1.9418331156841616e-05, 'batch_size': 207, 'step_size': 15, 'gamma': 0.913046446100954}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:57:13,961][0m Trial 16 finished with value: 0.16459409146604412 and parameters: {'observation_period_num': 40, 'train_rates': 0.7152306669080085, 'learning_rate': 0.0001532666136175173, 'batch_size': 224, 'step_size': 15, 'gamma': 0.9244491048956678}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:57:46,166][0m Trial 17 finished with value: 0.2618805476498366 and parameters: {'observation_period_num': 105, 'train_rates': 0.7892417188933858, 'learning_rate': 5.1784227901416676e-05, 'batch_size': 164, 'step_size': 13, 'gamma': 0.8926346415954214}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:58:14,087][0m Trial 18 finished with value: 0.08946960260302333 and parameters: {'observation_period_num': 44, 'train_rates': 0.8988150732678403, 'learning_rate': 0.0002666413723635071, 'batch_size': 219, 'step_size': 1, 'gamma': 0.9482727953336061}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:58:54,201][0m Trial 19 finished with value: 0.16023893441472734 and parameters: {'observation_period_num': 109, 'train_rates': 0.9092903776296126, 'learning_rate': 8.377259590815234e-05, 'batch_size': 148, 'step_size': 4, 'gamma': 0.8431769764064848}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:59:19,353][0m Trial 20 finished with value: 0.3064687909876428 and parameters: {'observation_period_num': 166, 'train_rates': 0.8679545045929028, 'learning_rate': 2.1227160094186448e-05, 'batch_size': 231, 'step_size': 12, 'gamma': 0.7666503003135186}. Best is trial 14 with value: 0.05412637896496406.[0m
[32m[I 2025-02-07 10:59:45,541][0m Trial 21 finished with value: 0.04722751609764053 and parameters: {'observation_period_num': 9, 'train_rates': 0.8254038265142498, 'learning_rate': 0.00011819818916439431, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8749932591405928}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:00:11,453][0m Trial 22 finished with value: 0.05409801814356725 and parameters: {'observation_period_num': 32, 'train_rates': 0.8101047166158196, 'learning_rate': 0.00023081926364307412, 'batch_size': 230, 'step_size': 10, 'gamma': 0.8472890928144049}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:00:38,017][0m Trial 23 finished with value: 0.1637473531392595 and parameters: {'observation_period_num': 35, 'train_rates': 0.7414138549240005, 'learning_rate': 0.0009595656853864795, 'batch_size': 205, 'step_size': 8, 'gamma': 0.8937852809992736}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:01:12,487][0m Trial 24 finished with value: 0.05318858269637138 and parameters: {'observation_period_num': 51, 'train_rates': 0.8174814204299493, 'learning_rate': 0.0002311037444541089, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8256682465332625}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:01:58,372][0m Trial 25 finished with value: 0.053855853258182235 and parameters: {'observation_period_num': 53, 'train_rates': 0.8071055915444071, 'learning_rate': 0.00024259366770176602, 'batch_size': 118, 'step_size': 10, 'gamma': 0.8240216168531593}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:02:44,367][0m Trial 26 finished with value: 0.19877162741632848 and parameters: {'observation_period_num': 59, 'train_rates': 0.7919379022508142, 'learning_rate': 0.00034532434666064517, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8218560551531685}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:03:26,730][0m Trial 27 finished with value: 0.20056594801800592 and parameters: {'observation_period_num': 106, 'train_rates': 0.7105099281006642, 'learning_rate': 0.0001825339398591396, 'batch_size': 118, 'step_size': 10, 'gamma': 0.789556947418912}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:05:21,088][0m Trial 28 finished with value: 0.06168659261432111 and parameters: {'observation_period_num': 54, 'train_rates': 0.9253616961128187, 'learning_rate': 7.165818928755222e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8186889889505478}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:05:54,060][0m Trial 29 finished with value: 0.21590707296302322 and parameters: {'observation_period_num': 96, 'train_rates': 0.7664043497094976, 'learning_rate': 0.0004181772010375353, 'batch_size': 164, 'step_size': 9, 'gamma': 0.7842776934645642}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:06:40,570][0m Trial 30 finished with value: 0.08749971619721575 and parameters: {'observation_period_num': 125, 'train_rates': 0.8733860455181995, 'learning_rate': 0.0007164197758158824, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8072279294503295}. Best is trial 21 with value: 0.04722751609764053.[0m
[32m[I 2025-02-07 11:07:12,684][0m Trial 31 finished with value: 0.04643428347406596 and parameters: {'observation_period_num': 27, 'train_rates': 0.8131809919823979, 'learning_rate': 0.00022390364898234666, 'batch_size': 176, 'step_size': 11, 'gamma': 0.8356891294878946}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:07:45,525][0m Trial 32 finished with value: 0.05437829707057104 and parameters: {'observation_period_num': 63, 'train_rates': 0.8120554070641208, 'learning_rate': 0.0002997391778503143, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8275534028352051}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:08:22,103][0m Trial 33 finished with value: 0.16129588708281517 and parameters: {'observation_period_num': 28, 'train_rates': 0.7326762846623611, 'learning_rate': 0.00019687287001347433, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8613931485813785}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:08:59,034][0m Trial 34 finished with value: 0.06435918680708605 and parameters: {'observation_period_num': 53, 'train_rates': 0.820779998908666, 'learning_rate': 7.907520008556632e-05, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8325767809290464}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:09:30,056][0m Trial 35 finished with value: 0.17232284162725722 and parameters: {'observation_period_num': 23, 'train_rates': 0.780640225479056, 'learning_rate': 0.0004729271435400464, 'batch_size': 184, 'step_size': 11, 'gamma': 0.802838780874294}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:10:39,666][0m Trial 36 finished with value: 0.09971703389393431 and parameters: {'observation_period_num': 86, 'train_rates': 0.80502354560476, 'learning_rate': 4.781286813007142e-05, 'batch_size': 75, 'step_size': 8, 'gamma': 0.7511754066942171}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:11:14,939][0m Trial 37 finished with value: 0.11790622626142473 and parameters: {'observation_period_num': 5, 'train_rates': 0.6015775430541818, 'learning_rate': 0.0008142564305806986, 'batch_size': 128, 'step_size': 9, 'gamma': 0.8792149175171133}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:12:12,976][0m Trial 38 finished with value: 0.1368139088153839 and parameters: {'observation_period_num': 74, 'train_rates': 0.9867679302187476, 'learning_rate': 2.6490624879882392e-05, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8105443331226407}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:12:41,602][0m Trial 39 finished with value: 0.20524959852945365 and parameters: {'observation_period_num': 49, 'train_rates': 0.7574615936461572, 'learning_rate': 8.824578723145085e-05, 'batch_size': 192, 'step_size': 7, 'gamma': 0.8357170928986012}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:13:15,457][0m Trial 40 finished with value: 0.2473004879663937 and parameters: {'observation_period_num': 22, 'train_rates': 0.8644489316536046, 'learning_rate': 5.086249157365421e-06, 'batch_size': 178, 'step_size': 10, 'gamma': 0.8567156647897702}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:13:45,427][0m Trial 41 finished with value: 0.05397901042426376 and parameters: {'observation_period_num': 32, 'train_rates': 0.8302308496486431, 'learning_rate': 0.0002032009324572893, 'batch_size': 200, 'step_size': 10, 'gamma': 0.8435735043411031}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:14:13,659][0m Trial 42 finished with value: 0.07774124073618946 and parameters: {'observation_period_num': 67, 'train_rates': 0.8328518083884681, 'learning_rate': 0.0001901111295073628, 'batch_size': 201, 'step_size': 9, 'gamma': 0.862349443827294}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:14:49,999][0m Trial 43 finished with value: 0.17703250297879586 and parameters: {'observation_period_num': 22, 'train_rates': 0.7923825048113649, 'learning_rate': 0.0005250779047922758, 'batch_size': 155, 'step_size': 12, 'gamma': 0.8841108571794628}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:15:33,417][0m Trial 44 finished with value: 0.06734822031926353 and parameters: {'observation_period_num': 83, 'train_rates': 0.8682957416117755, 'learning_rate': 0.0002742186588849073, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8355125140856551}. Best is trial 31 with value: 0.04643428347406596.[0m
[32m[I 2025-02-07 11:16:08,146][0m Trial 45 finished with value: 0.036455749115402544 and parameters: {'observation_period_num': 17, 'train_rates': 0.8519977288393097, 'learning_rate': 0.00035277907521308755, 'batch_size': 173, 'step_size': 11, 'gamma': 0.7868738727430111}. Best is trial 45 with value: 0.036455749115402544.[0m
[32m[I 2025-02-07 11:17:10,267][0m Trial 46 finished with value: 0.027322748298203196 and parameters: {'observation_period_num': 13, 'train_rates': 0.8543673718464878, 'learning_rate': 0.00034793358913057557, 'batch_size': 90, 'step_size': 11, 'gamma': 0.7796929596758373}. Best is trial 46 with value: 0.027322748298203196.[0m
[32m[I 2025-02-07 11:20:30,357][0m Trial 47 finished with value: 0.02834871981655602 and parameters: {'observation_period_num': 6, 'train_rates': 0.885423213627326, 'learning_rate': 0.0005117453102235056, 'batch_size': 28, 'step_size': 14, 'gamma': 0.7764291093919617}. Best is trial 46 with value: 0.027322748298203196.[0m
[32m[I 2025-02-07 11:24:38,931][0m Trial 48 finished with value: 0.03242384979608038 and parameters: {'observation_period_num': 6, 'train_rates': 0.9233388719149489, 'learning_rate': 0.0006626176235969922, 'batch_size': 23, 'step_size': 14, 'gamma': 0.7749782913957982}. Best is trial 46 with value: 0.027322748298203196.[0m
[32m[I 2025-02-07 11:29:13,098][0m Trial 49 finished with value: 0.03720873856700916 and parameters: {'observation_period_num': 16, 'train_rates': 0.9387035404306528, 'learning_rate': 0.0006105588137348092, 'batch_size': 21, 'step_size': 14, 'gamma': 0.7756139526146791}. Best is trial 46 with value: 0.027322748298203196.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-07 11:29:13,109][0m A new study created in memory with name: no-name-420afac6-63dc-4eeb-b43d-db1a50c2ae99[0m
[32m[I 2025-02-07 11:29:59,999][0m Trial 0 finished with value: 0.12995294957715073 and parameters: {'observation_period_num': 132, 'train_rates': 0.8845278059971808, 'learning_rate': 0.0006944780220051774, 'batch_size': 130, 'step_size': 7, 'gamma': 0.8772082817719701}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:30:33,240][0m Trial 1 finished with value: 0.18441402682219632 and parameters: {'observation_period_num': 7, 'train_rates': 0.739811085236641, 'learning_rate': 6.776060314249728e-05, 'batch_size': 162, 'step_size': 2, 'gamma': 0.9059584326728426}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:30:57,198][0m Trial 2 finished with value: 0.8933634014765909 and parameters: {'observation_period_num': 26, 'train_rates': 0.7778711361925393, 'learning_rate': 2.800480714496139e-06, 'batch_size': 229, 'step_size': 7, 'gamma': 0.8066205122673086}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:31:22,740][0m Trial 3 finished with value: 0.3195568026764776 and parameters: {'observation_period_num': 7, 'train_rates': 0.6335747143514401, 'learning_rate': 1.008474109056824e-05, 'batch_size': 200, 'step_size': 7, 'gamma': 0.8038642775292306}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:32:36,642][0m Trial 4 finished with value: 0.1421335583665019 and parameters: {'observation_period_num': 188, 'train_rates': 0.8266262913565482, 'learning_rate': 0.0003028959872073381, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8136238588256874}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:33:34,838][0m Trial 5 finished with value: 0.3520701553734987 and parameters: {'observation_period_num': 245, 'train_rates': 0.6703939851077926, 'learning_rate': 0.0003733782004030881, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9581144236110659}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:34:02,045][0m Trial 6 finished with value: 0.14083876851594673 and parameters: {'observation_period_num': 146, 'train_rates': 0.9058417132594896, 'learning_rate': 0.0008011543779948279, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9133270998261984}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:34:26,378][0m Trial 7 finished with value: 0.37874889639673626 and parameters: {'observation_period_num': 199, 'train_rates': 0.8050256923118568, 'learning_rate': 6.032371208301737e-05, 'batch_size': 237, 'step_size': 2, 'gamma': 0.8241451219739321}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:34:48,546][0m Trial 8 finished with value: 1.0646264140627213 and parameters: {'observation_period_num': 239, 'train_rates': 0.6707693919180391, 'learning_rate': 1.4843145073336788e-06, 'batch_size': 226, 'step_size': 12, 'gamma': 0.9014927664248387}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:37:12,241][0m Trial 9 finished with value: 0.5520935812102373 and parameters: {'observation_period_num': 223, 'train_rates': 0.6131796963366684, 'learning_rate': 3.691675411393686e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9561892508980504}. Best is trial 0 with value: 0.12995294957715073.[0m
[32m[I 2025-02-07 11:38:01,458][0m Trial 10 finished with value: 0.0916261374950409 and parameters: {'observation_period_num': 80, 'train_rates': 0.9749855013376512, 'learning_rate': 0.00016485021995162668, 'batch_size': 123, 'step_size': 15, 'gamma': 0.8658895653780001}. Best is trial 10 with value: 0.0916261374950409.[0m
[32m[I 2025-02-07 11:38:48,848][0m Trial 11 finished with value: 0.10771775245666504 and parameters: {'observation_period_num': 78, 'train_rates': 0.9883454253237298, 'learning_rate': 0.00020302455048385094, 'batch_size': 130, 'step_size': 14, 'gamma': 0.8503877297578988}. Best is trial 10 with value: 0.0916261374950409.[0m
[32m[I 2025-02-07 11:39:37,544][0m Trial 12 finished with value: 0.08251523226499557 and parameters: {'observation_period_num': 69, 'train_rates': 0.9823643389120469, 'learning_rate': 0.00016424131355262815, 'batch_size': 124, 'step_size': 15, 'gamma': 0.7669007393578491}. Best is trial 12 with value: 0.08251523226499557.[0m
[32m[I 2025-02-07 11:40:38,704][0m Trial 13 finished with value: 0.10066169500350952 and parameters: {'observation_period_num': 72, 'train_rates': 0.9885175775598707, 'learning_rate': 0.00011456693171293016, 'batch_size': 99, 'step_size': 15, 'gamma': 0.7598178371775498}. Best is trial 12 with value: 0.08251523226499557.[0m
[32m[I 2025-02-07 11:41:14,152][0m Trial 14 finished with value: 0.26152606675791185 and parameters: {'observation_period_num': 83, 'train_rates': 0.9253532185411955, 'learning_rate': 1.649411023043041e-05, 'batch_size': 169, 'step_size': 13, 'gamma': 0.7612816854280803}. Best is trial 12 with value: 0.08251523226499557.[0m
[32m[I 2025-02-07 11:42:17,131][0m Trial 15 finished with value: 0.06264470361850479 and parameters: {'observation_period_num': 51, 'train_rates': 0.9435597204429282, 'learning_rate': 0.00013164538283529074, 'batch_size': 93, 'step_size': 12, 'gamma': 0.9868320327706526}. Best is trial 15 with value: 0.06264470361850479.[0m
[32m[I 2025-02-07 11:44:51,925][0m Trial 16 finished with value: 0.06119925435632467 and parameters: {'observation_period_num': 38, 'train_rates': 0.8562710031950822, 'learning_rate': 1.655247076255911e-05, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9899253440809792}. Best is trial 16 with value: 0.06119925435632467.[0m
[32m[I 2025-02-07 11:48:32,286][0m Trial 17 finished with value: 0.06696298619057506 and parameters: {'observation_period_num': 49, 'train_rates': 0.8415379330567271, 'learning_rate': 9.759879873809868e-06, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9889557142201}. Best is trial 16 with value: 0.06119925435632467.[0m
[32m[I 2025-02-07 11:49:58,751][0m Trial 18 finished with value: 0.23505441176891326 and parameters: {'observation_period_num': 109, 'train_rates': 0.8687015190726246, 'learning_rate': 5.273050418701942e-06, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9874669708294083}. Best is trial 16 with value: 0.06119925435632467.[0m
[32m[I 2025-02-07 11:52:10,427][0m Trial 19 finished with value: 0.06685365179807624 and parameters: {'observation_period_num': 35, 'train_rates': 0.9398218848753442, 'learning_rate': 2.599046435557688e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.9461615883459585}. Best is trial 16 with value: 0.06119925435632467.[0m
[32m[I 2025-02-07 11:53:03,699][0m Trial 20 finished with value: 0.21957769858486512 and parameters: {'observation_period_num': 105, 'train_rates': 0.7499895558981066, 'learning_rate': 6.165464230205875e-05, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9364323201981835}. Best is trial 16 with value: 0.06119925435632467.[0m
[32m[I 2025-02-07 11:55:18,503][0m Trial 21 finished with value: 0.055371349004550756 and parameters: {'observation_period_num': 43, 'train_rates': 0.9348045862890562, 'learning_rate': 2.3475140930093216e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.9628691331570131}. Best is trial 21 with value: 0.055371349004550756.[0m
[32m[I 2025-02-07 11:57:23,547][0m Trial 22 finished with value: 0.06772395041446354 and parameters: {'observation_period_num': 49, 'train_rates': 0.9411988078464967, 'learning_rate': 2.146580521768893e-05, 'batch_size': 47, 'step_size': 11, 'gamma': 0.9717710988883804}. Best is trial 21 with value: 0.055371349004550756.[0m
[32m[I 2025-02-07 12:02:44,091][0m Trial 23 finished with value: 0.046993912568778023 and parameters: {'observation_period_num': 31, 'train_rates': 0.8637271306122782, 'learning_rate': 9.61897608535065e-06, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9723184365000426}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:07:29,164][0m Trial 24 finished with value: 0.056211769576015556 and parameters: {'observation_period_num': 28, 'train_rates': 0.8684489881183662, 'learning_rate': 7.235867040131396e-06, 'batch_size': 19, 'step_size': 9, 'gamma': 0.9379180723038509}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:12:55,840][0m Trial 25 finished with value: 0.07987721985692431 and parameters: {'observation_period_num': 24, 'train_rates': 0.8931912130758755, 'learning_rate': 3.931632904411073e-06, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9259579941483145}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:14:35,004][0m Trial 26 finished with value: 0.0674951596842181 and parameters: {'observation_period_num': 10, 'train_rates': 0.8116806429657258, 'learning_rate': 7.680433266796573e-06, 'batch_size': 54, 'step_size': 9, 'gamma': 0.9674992693591334}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:17:02,847][0m Trial 27 finished with value: 0.5075484252855426 and parameters: {'observation_period_num': 156, 'train_rates': 0.9130191284162843, 'learning_rate': 1.6213047083389076e-06, 'batch_size': 37, 'step_size': 5, 'gamma': 0.9258178828946796}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:22:08,540][0m Trial 28 finished with value: 0.37182614976811296 and parameters: {'observation_period_num': 99, 'train_rates': 0.7767840583618679, 'learning_rate': 2.5315032309843843e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8886018078864109}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:23:22,614][0m Trial 29 finished with value: 0.241589605308524 and parameters: {'observation_period_num': 123, 'train_rates': 0.8846286973140374, 'learning_rate': 6.202172633870775e-06, 'batch_size': 75, 'step_size': 8, 'gamma': 0.9411784651552664}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:24:58,716][0m Trial 30 finished with value: 0.154405373315148 and parameters: {'observation_period_num': 62, 'train_rates': 0.8439165332946277, 'learning_rate': 1.2512480602153921e-05, 'batch_size': 56, 'step_size': 6, 'gamma': 0.8787336033232457}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:27:37,481][0m Trial 31 finished with value: 0.07997901616401451 and parameters: {'observation_period_num': 35, 'train_rates': 0.8823826966466551, 'learning_rate': 4.04231554124428e-05, 'batch_size': 35, 'step_size': 10, 'gamma': 0.970001578412687}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:30:16,833][0m Trial 32 finished with value: 0.05128243416547775 and parameters: {'observation_period_num': 22, 'train_rates': 0.8537360135075146, 'learning_rate': 1.5376409524591437e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9722124480274215}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:30:56,003][0m Trial 33 finished with value: 0.3054747087051547 and parameters: {'observation_period_num': 20, 'train_rates': 0.8663107483935863, 'learning_rate': 3.873197888289356e-06, 'batch_size': 151, 'step_size': 9, 'gamma': 0.9536432291342549}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:32:50,040][0m Trial 34 finished with value: 0.18764193820074884 and parameters: {'observation_period_num': 7, 'train_rates': 0.7862454066873238, 'learning_rate': 1.3454954612670538e-05, 'batch_size': 46, 'step_size': 7, 'gamma': 0.9705571111078166}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:33:42,504][0m Trial 35 finished with value: 0.07529442224125739 and parameters: {'observation_period_num': 57, 'train_rates': 0.8267339322326316, 'learning_rate': 2.5664216826985105e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.926552476425488}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:39:45,205][0m Trial 36 finished with value: 0.059435972107911676 and parameters: {'observation_period_num': 21, 'train_rates': 0.9568436263978713, 'learning_rate': 8.886829330809946e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.902647661030523}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:40:56,737][0m Trial 37 finished with value: 0.19345536303053548 and parameters: {'observation_period_num': 37, 'train_rates': 0.9038444263446015, 'learning_rate': 5.191097665056185e-06, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9592600327168318}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:42:11,660][0m Trial 38 finished with value: 0.27614213242971647 and parameters: {'observation_period_num': 6, 'train_rates': 0.7401312758647582, 'learning_rate': 2.8164837592237223e-06, 'batch_size': 67, 'step_size': 9, 'gamma': 0.9156858499587591}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:42:40,379][0m Trial 39 finished with value: 0.12172414195888183 and parameters: {'observation_period_num': 92, 'train_rates': 0.822289643828235, 'learning_rate': 4.614116352635703e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9762324165143667}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:45:39,345][0m Trial 40 finished with value: 0.10923360351681988 and parameters: {'observation_period_num': 160, 'train_rates': 0.8470995912232874, 'learning_rate': 1.7708728273268165e-05, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9450333563090604}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:51:45,436][0m Trial 41 finished with value: 0.06362894167933537 and parameters: {'observation_period_num': 20, 'train_rates': 0.9669835209330804, 'learning_rate': 8.74622619943808e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9003150220447328}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:54:13,295][0m Trial 42 finished with value: 0.07167644916396392 and parameters: {'observation_period_num': 25, 'train_rates': 0.9613746941661975, 'learning_rate': 1.2266677733012344e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9358639456521072}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:55:51,161][0m Trial 43 finished with value: 0.06281012574057558 and parameters: {'observation_period_num': 42, 'train_rates': 0.9210058474231625, 'learning_rate': 3.07499112188604e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.910726553942307}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 12:59:56,500][0m Trial 44 finished with value: 0.05420451611280441 and parameters: {'observation_period_num': 15, 'train_rates': 0.9512528330276228, 'learning_rate': 7.918810610761696e-06, 'batch_size': 24, 'step_size': 8, 'gamma': 0.9598411430003666}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 13:03:08,036][0m Trial 45 finished with value: 0.1021117351918512 and parameters: {'observation_period_num': 65, 'train_rates': 0.8742333234226489, 'learning_rate': 0.0005151131055400579, 'batch_size': 29, 'step_size': 1, 'gamma': 0.9589290399884873}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 13:04:39,377][0m Trial 46 finished with value: 0.2701123381682233 and parameters: {'observation_period_num': 13, 'train_rates': 0.6968714973481678, 'learning_rate': 4.009084493648501e-06, 'batch_size': 53, 'step_size': 6, 'gamma': 0.8443945908878991}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 13:08:04,770][0m Trial 47 finished with value: 0.07591039801257789 and parameters: {'observation_period_num': 45, 'train_rates': 0.8968386487659236, 'learning_rate': 6.825140230921916e-06, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9520576913047997}. Best is trial 23 with value: 0.046993912568778023.[0m
[32m[I 2025-02-07 13:09:19,670][0m Trial 48 finished with value: 0.04193293412818628 and parameters: {'observation_period_num': 30, 'train_rates': 0.924746879506371, 'learning_rate': 9.429915472998664e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9610731245810124}. Best is trial 48 with value: 0.04193293412818628.[0m
[32m[I 2025-02-07 13:10:13,876][0m Trial 49 finished with value: 0.061755883429438024 and parameters: {'observation_period_num': 31, 'train_rates': 0.9307714467807148, 'learning_rate': 9.060367693256642e-05, 'batch_size': 111, 'step_size': 7, 'gamma': 0.7899898476543881}. Best is trial 48 with value: 0.04193293412818628.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-07 13:10:13,888][0m A new study created in memory with name: no-name-d5181dfc-7e11-49e8-8f2d-0b4aada8bf39[0m
[32m[I 2025-02-07 13:12:04,282][0m Trial 0 finished with value: 0.1936449538520042 and parameters: {'observation_period_num': 203, 'train_rates': 0.7956068951629283, 'learning_rate': 0.00013565295097736407, 'batch_size': 44, 'step_size': 2, 'gamma': 0.7555911462306603}. Best is trial 0 with value: 0.1936449538520042.[0m
[32m[I 2025-02-07 13:13:15,804][0m Trial 1 finished with value: 0.6013734683334103 and parameters: {'observation_period_num': 193, 'train_rates': 0.6429428773751606, 'learning_rate': 2.816811595852746e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.8793702419105864}. Best is trial 0 with value: 0.1936449538520042.[0m
[32m[I 2025-02-07 13:14:39,477][0m Trial 2 finished with value: 0.4049094824010835 and parameters: {'observation_period_num': 62, 'train_rates': 0.7652885567662651, 'learning_rate': 5.749281441527567e-06, 'batch_size': 61, 'step_size': 6, 'gamma': 0.8349929970010224}. Best is trial 0 with value: 0.1936449538520042.[0m
[32m[I 2025-02-07 13:15:19,682][0m Trial 3 finished with value: 0.0675394231987785 and parameters: {'observation_period_num': 32, 'train_rates': 0.8237772834657964, 'learning_rate': 3.855394684390062e-05, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9292374333970289}. Best is trial 3 with value: 0.0675394231987785.[0m
[32m[I 2025-02-07 13:15:40,856][0m Trial 4 finished with value: 0.24961620139518073 and parameters: {'observation_period_num': 205, 'train_rates': 0.6002895414554487, 'learning_rate': 0.0004707802730674139, 'batch_size': 215, 'step_size': 7, 'gamma': 0.8531390431355047}. Best is trial 3 with value: 0.0675394231987785.[0m
Early stopping at epoch 96
[32m[I 2025-02-07 13:16:15,512][0m Trial 5 finished with value: 0.952103927231475 and parameters: {'observation_period_num': 72, 'train_rates': 0.690258659244792, 'learning_rate': 5.2158514451534394e-06, 'batch_size': 138, 'step_size': 1, 'gamma': 0.861913181942451}. Best is trial 3 with value: 0.0675394231987785.[0m
[32m[I 2025-02-07 13:16:36,561][0m Trial 6 finished with value: 0.34692204429357537 and parameters: {'observation_period_num': 170, 'train_rates': 0.6217066738775123, 'learning_rate': 1.5912353094123936e-05, 'batch_size': 229, 'step_size': 14, 'gamma': 0.9768472654600592}. Best is trial 3 with value: 0.0675394231987785.[0m
[32m[I 2025-02-07 13:21:07,661][0m Trial 7 finished with value: 0.061064397051234794 and parameters: {'observation_period_num': 27, 'train_rates': 0.8613817367573828, 'learning_rate': 4.5232529341332596e-06, 'batch_size': 20, 'step_size': 10, 'gamma': 0.9616720797003141}. Best is trial 7 with value: 0.061064397051234794.[0m
[32m[I 2025-02-07 13:21:34,477][0m Trial 8 finished with value: 0.4512025713920593 and parameters: {'observation_period_num': 53, 'train_rates': 0.9540861030657543, 'learning_rate': 4.749475755700079e-06, 'batch_size': 236, 'step_size': 9, 'gamma': 0.9778299135801605}. Best is trial 7 with value: 0.061064397051234794.[0m
[32m[I 2025-02-07 13:22:02,040][0m Trial 9 finished with value: 0.2426661566743595 and parameters: {'observation_period_num': 220, 'train_rates': 0.6388064525016792, 'learning_rate': 0.00041590201282743074, 'batch_size': 173, 'step_size': 7, 'gamma': 0.7683562638984561}. Best is trial 7 with value: 0.061064397051234794.[0m
[32m[I 2025-02-07 13:26:06,109][0m Trial 10 finished with value: 0.4240297219237766 and parameters: {'observation_period_num': 121, 'train_rates': 0.9350824511813235, 'learning_rate': 1.1834633377366187e-06, 'batch_size': 23, 'step_size': 12, 'gamma': 0.9201382243261664}. Best is trial 7 with value: 0.061064397051234794.[0m
[32m[I 2025-02-07 13:27:00,321][0m Trial 11 finished with value: 0.04666810162910601 and parameters: {'observation_period_num': 24, 'train_rates': 0.8605501955119979, 'learning_rate': 9.545035445811418e-05, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9317575211988114}. Best is trial 11 with value: 0.04666810162910601.[0m
[32m[I 2025-02-07 13:27:58,739][0m Trial 12 finished with value: 0.03916522452157024 and parameters: {'observation_period_num': 10, 'train_rates': 0.8837130090133382, 'learning_rate': 9.763801839506333e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.9331006364382138}. Best is trial 12 with value: 0.03916522452157024.[0m
[32m[I 2025-02-07 13:28:54,846][0m Trial 13 finished with value: 0.0380279580736677 and parameters: {'observation_period_num': 7, 'train_rates': 0.890872767936781, 'learning_rate': 0.00016409034646746288, 'batch_size': 109, 'step_size': 4, 'gamma': 0.9148980442903015}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:29:52,920][0m Trial 14 finished with value: 0.10425456103525664 and parameters: {'observation_period_num': 103, 'train_rates': 0.9068717181130646, 'learning_rate': 0.00019575832161633223, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8973256354559318}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:30:50,957][0m Trial 15 finished with value: 0.046353332698345184 and parameters: {'observation_period_num': 13, 'train_rates': 0.983243895311435, 'learning_rate': 6.566731450902304e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8034577133429454}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:31:24,730][0m Trial 16 finished with value: 0.1597060783896395 and parameters: {'observation_period_num': 249, 'train_rates': 0.8971992904509306, 'learning_rate': 0.0009179844375237349, 'batch_size': 170, 'step_size': 5, 'gamma': 0.8994237871655768}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:32:28,236][0m Trial 17 finished with value: 0.20313383474946023 and parameters: {'observation_period_num': 86, 'train_rates': 0.7652927349847292, 'learning_rate': 0.00026214838027502065, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9458953546884805}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:33:09,941][0m Trial 18 finished with value: 0.13465393056331232 and parameters: {'observation_period_num': 154, 'train_rates': 0.8493537842316901, 'learning_rate': 5.7591739072668264e-05, 'batch_size': 133, 'step_size': 12, 'gamma': 0.9092666487944894}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:33:45,208][0m Trial 19 finished with value: 0.13534630847641746 and parameters: {'observation_period_num': 10, 'train_rates': 0.8997733294686294, 'learning_rate': 1.6099145089513784e-05, 'batch_size': 173, 'step_size': 3, 'gamma': 0.9482667937705237}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:34:45,085][0m Trial 20 finished with value: 0.16837385192934506 and parameters: {'observation_period_num': 47, 'train_rates': 0.720690474560615, 'learning_rate': 0.0001278973551278541, 'batch_size': 81, 'step_size': 9, 'gamma': 0.830982351860637}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:35:41,052][0m Trial 21 finished with value: 0.03935888409614563 and parameters: {'observation_period_num': 7, 'train_rates': 0.9796658840107697, 'learning_rate': 6.721426278542201e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.7867703525939662}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:36:26,410][0m Trial 22 finished with value: 0.040941692888736725 and parameters: {'observation_period_num': 5, 'train_rates': 0.9831454116796214, 'learning_rate': 0.00033473624483877065, 'batch_size': 142, 'step_size': 15, 'gamma': 0.8799352613114592}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:37:18,958][0m Trial 23 finished with value: 0.09408719361174939 and parameters: {'observation_period_num': 40, 'train_rates': 0.9413871513935725, 'learning_rate': 2.7447180169045996e-05, 'batch_size': 117, 'step_size': 12, 'gamma': 0.8032364337015729}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:38:25,969][0m Trial 24 finished with value: 0.07976037489348337 and parameters: {'observation_period_num': 86, 'train_rates': 0.9207255465416913, 'learning_rate': 6.742893690301913e-05, 'batch_size': 86, 'step_size': 15, 'gamma': 0.7835759343022075}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:39:07,091][0m Trial 25 finished with value: 0.04551107808947563 and parameters: {'observation_period_num': 38, 'train_rates': 0.9599703354059949, 'learning_rate': 0.0006738377411792315, 'batch_size': 154, 'step_size': 13, 'gamma': 0.8885763797398868}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:39:56,170][0m Trial 26 finished with value: 0.06265026385991743 and parameters: {'observation_period_num': 70, 'train_rates': 0.8690082913628362, 'learning_rate': 0.0001904943858449114, 'batch_size': 118, 'step_size': 10, 'gamma': 0.8401448373716931}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:40:24,393][0m Trial 27 finished with value: 0.3026816905233901 and parameters: {'observation_period_num': 127, 'train_rates': 0.816773309207155, 'learning_rate': 1.6042871731449734e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9119814798555331}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:41:58,589][0m Trial 28 finished with value: 0.05680123243480921 and parameters: {'observation_period_num': 21, 'train_rates': 0.8300450297215924, 'learning_rate': 4.3970152872617554e-05, 'batch_size': 57, 'step_size': 8, 'gamma': 0.8105299948510685}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:43:53,474][0m Trial 29 finished with value: 0.18532611629073048 and parameters: {'observation_period_num': 5, 'train_rates': 0.783777957760534, 'learning_rate': 0.00012673094801927308, 'batch_size': 45, 'step_size': 3, 'gamma': 0.7547953078413343}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:44:19,175][0m Trial 30 finished with value: 0.11114047708451581 and parameters: {'observation_period_num': 55, 'train_rates': 0.8765835287727753, 'learning_rate': 9.156375282363395e-05, 'batch_size': 253, 'step_size': 11, 'gamma': 0.9502960123019155}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:45:02,670][0m Trial 31 finished with value: 0.045439526438713074 and parameters: {'observation_period_num': 5, 'train_rates': 0.9886333863494954, 'learning_rate': 0.00030508185647920235, 'batch_size': 148, 'step_size': 15, 'gamma': 0.8735266009743495}. Best is trial 13 with value: 0.0380279580736677.[0m
[32m[I 2025-02-07 13:45:51,429][0m Trial 32 finished with value: 0.03350295498967171 and parameters: {'observation_period_num': 22, 'train_rates': 0.9724893992187545, 'learning_rate': 0.00017806023682098804, 'batch_size': 126, 'step_size': 14, 'gamma': 0.8800762918222386}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:46:59,614][0m Trial 33 finished with value: 0.04665222817233631 and parameters: {'observation_period_num': 31, 'train_rates': 0.9668273701340186, 'learning_rate': 0.0001587670445052817, 'batch_size': 91, 'step_size': 13, 'gamma': 0.9298186340957431}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:47:49,304][0m Trial 34 finished with value: 0.0628830182853095 and parameters: {'observation_period_num': 23, 'train_rates': 0.9260224071530396, 'learning_rate': 2.851476448713825e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.8557102510359643}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:49:15,218][0m Trial 35 finished with value: 0.06272090127224524 and parameters: {'observation_period_num': 43, 'train_rates': 0.8895311070347118, 'learning_rate': 0.0001003620382269013, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8880313025578921}. Best is trial 32 with value: 0.03350295498967171.[0m
Early stopping at epoch 65
[32m[I 2025-02-07 13:49:47,004][0m Trial 36 finished with value: 0.24867338298911779 and parameters: {'observation_period_num': 68, 'train_rates': 0.9508665768908316, 'learning_rate': 0.00022862970285065425, 'batch_size': 125, 'step_size': 1, 'gamma': 0.8217660142917995}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:51:03,898][0m Trial 37 finished with value: 0.10694115832448006 and parameters: {'observation_period_num': 83, 'train_rates': 0.9237593078215549, 'learning_rate': 4.5861447815351695e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.86738265890404}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:51:57,711][0m Trial 38 finished with value: 0.05985010816853066 and parameters: {'observation_period_num': 22, 'train_rates': 0.8340168966171371, 'learning_rate': 0.0004844931186226165, 'batch_size': 106, 'step_size': 14, 'gamma': 0.9128817892800798}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:52:29,680][0m Trial 39 finished with value: 0.4593892934956128 and parameters: {'observation_period_num': 150, 'train_rates': 0.7188768999101527, 'learning_rate': 8.46957447516451e-05, 'batch_size': 164, 'step_size': 2, 'gamma': 0.8450383259217173}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:53:00,424][0m Trial 40 finished with value: 0.17387337274913756 and parameters: {'observation_period_num': 109, 'train_rates': 0.7971107336287272, 'learning_rate': 2.3948034153282045e-05, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9697073154307869}. Best is trial 32 with value: 0.03350295498967171.[0m
[32m[I 2025-02-07 13:53:45,686][0m Trial 41 finished with value: 0.028834691271185875 and parameters: {'observation_period_num': 15, 'train_rates': 0.9813458967482234, 'learning_rate': 0.00032350753795972554, 'batch_size': 142, 'step_size': 15, 'gamma': 0.8772190131814118}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:54:33,886][0m Trial 42 finished with value: 0.05609479174017906 and parameters: {'observation_period_num': 35, 'train_rates': 0.9711931891910147, 'learning_rate': 0.00013802436478039018, 'batch_size': 130, 'step_size': 15, 'gamma': 0.8990404128593563}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:55:16,031][0m Trial 43 finished with value: 0.04966479954049454 and parameters: {'observation_period_num': 19, 'train_rates': 0.9454862515523814, 'learning_rate': 0.0005920286356601933, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9392191165363712}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:56:20,302][0m Trial 44 finished with value: 0.077762295109661 and parameters: {'observation_period_num': 61, 'train_rates': 0.967314793736019, 'learning_rate': 0.00036859026536497616, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9220696463001525}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:57:15,215][0m Trial 45 finished with value: 0.06742443877362436 and parameters: {'observation_period_num': 51, 'train_rates': 0.9147657514291994, 'learning_rate': 0.00017747635385949618, 'batch_size': 109, 'step_size': 15, 'gamma': 0.9868011506012163}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:57:59,061][0m Trial 46 finished with value: 0.16170676562556607 and parameters: {'observation_period_num': 16, 'train_rates': 0.936460261416315, 'learning_rate': 8.687028319002413e-06, 'batch_size': 141, 'step_size': 10, 'gamma': 0.8628022915544725}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:58:55,553][0m Trial 47 finished with value: 0.03941589114921434 and parameters: {'observation_period_num': 30, 'train_rates': 0.8806218107795182, 'learning_rate': 0.00021951667636004903, 'batch_size': 100, 'step_size': 7, 'gamma': 0.883635993023038}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 13:59:47,495][0m Trial 48 finished with value: 0.23679469525814056 and parameters: {'observation_period_num': 198, 'train_rates': 0.9880685389964204, 'learning_rate': 6.945453697995582e-05, 'batch_size': 115, 'step_size': 5, 'gamma': 0.781966009485378}. Best is trial 41 with value: 0.028834691271185875.[0m
[32m[I 2025-02-07 14:00:27,215][0m Trial 49 finished with value: 0.7895296216011047 and parameters: {'observation_period_num': 13, 'train_rates': 0.9569336689261905, 'learning_rate': 1.5437223176311e-06, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9607418530077567}. Best is trial 41 with value: 0.028834691271185875.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-07 14:00:27,228][0m A new study created in memory with name: no-name-1b30f88b-70a8-4ae5-ae5f-5ac51e8cb73a[0m
[32m[I 2025-02-07 14:01:03,112][0m Trial 0 finished with value: 0.21420465685941023 and parameters: {'observation_period_num': 54, 'train_rates': 0.720420954770588, 'learning_rate': 0.00029906252583747204, 'batch_size': 149, 'step_size': 3, 'gamma': 0.7805636263163013}. Best is trial 0 with value: 0.21420465685941023.[0m
[32m[I 2025-02-07 14:01:30,882][0m Trial 1 finished with value: 0.833692412417714 and parameters: {'observation_period_num': 90, 'train_rates': 0.7185482582918471, 'learning_rate': 2.9587767363038217e-06, 'batch_size': 189, 'step_size': 7, 'gamma': 0.8416105143543154}. Best is trial 0 with value: 0.21420465685941023.[0m
[32m[I 2025-02-07 14:03:00,650][0m Trial 2 finished with value: 0.15436482968161236 and parameters: {'observation_period_num': 234, 'train_rates': 0.8913767099477214, 'learning_rate': 0.00045889611018552144, 'batch_size': 58, 'step_size': 4, 'gamma': 0.8377211327716724}. Best is trial 2 with value: 0.15436482968161236.[0m
[32m[I 2025-02-07 14:05:08,434][0m Trial 3 finished with value: 0.1566242163415277 and parameters: {'observation_period_num': 18, 'train_rates': 0.8088064113695874, 'learning_rate': 3.4940959581630823e-06, 'batch_size': 41, 'step_size': 8, 'gamma': 0.7803702603711846}. Best is trial 2 with value: 0.15436482968161236.[0m
[32m[I 2025-02-07 14:05:48,639][0m Trial 4 finished with value: 0.9077385852291325 and parameters: {'observation_period_num': 183, 'train_rates': 0.7171606427656726, 'learning_rate': 1.4076117676181544e-06, 'batch_size': 123, 'step_size': 11, 'gamma': 0.9358864513701879}. Best is trial 2 with value: 0.15436482968161236.[0m
[32m[I 2025-02-07 14:06:28,401][0m Trial 5 finished with value: 0.09973813513273834 and parameters: {'observation_period_num': 64, 'train_rates': 0.839509848379085, 'learning_rate': 3.9344945526041104e-05, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8594035686803203}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:06:53,972][0m Trial 6 finished with value: 0.14327921175187633 and parameters: {'observation_period_num': 169, 'train_rates': 0.8336145586400068, 'learning_rate': 0.0007283662073854271, 'batch_size': 216, 'step_size': 5, 'gamma': 0.9121787742852707}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:07:33,572][0m Trial 7 finished with value: 0.28606395057110423 and parameters: {'observation_period_num': 69, 'train_rates': 0.6480192902901838, 'learning_rate': 2.6722362650238265e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9284182938925785}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:08:14,885][0m Trial 8 finished with value: 0.434192677897014 and parameters: {'observation_period_num': 235, 'train_rates': 0.8041762928921444, 'learning_rate': 1.735240296369192e-05, 'batch_size': 127, 'step_size': 3, 'gamma': 0.838810343256911}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:08:59,617][0m Trial 9 finished with value: 0.4905937102225626 and parameters: {'observation_period_num': 151, 'train_rates': 0.7441211243136454, 'learning_rate': 1.1569855344056707e-05, 'batch_size': 115, 'step_size': 5, 'gamma': 0.8557014999434337}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:09:26,872][0m Trial 10 finished with value: 0.16365665197372437 and parameters: {'observation_period_num': 104, 'train_rates': 0.9442511472790385, 'learning_rate': 9.108932285452723e-05, 'batch_size': 240, 'step_size': 14, 'gamma': 0.9678305588227515}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:09:52,380][0m Trial 11 finished with value: 0.1269511619408325 and parameters: {'observation_period_num': 151, 'train_rates': 0.8823025930531806, 'learning_rate': 0.00012925352449766502, 'batch_size': 233, 'step_size': 12, 'gamma': 0.8980022126742759}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:10:25,718][0m Trial 12 finished with value: 0.1332480227459728 and parameters: {'observation_period_num': 126, 'train_rates': 0.8858385279712706, 'learning_rate': 8.989300872954911e-05, 'batch_size': 179, 'step_size': 13, 'gamma': 0.8877168631164757}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:10:53,490][0m Trial 13 finished with value: 0.10264133661985397 and parameters: {'observation_period_num': 19, 'train_rates': 0.9651364532425575, 'learning_rate': 9.028376123482293e-05, 'batch_size': 256, 'step_size': 11, 'gamma': 0.884949721973076}. Best is trial 5 with value: 0.09973813513273834.[0m
[32m[I 2025-02-07 14:12:08,611][0m Trial 14 finished with value: 0.06148731708526611 and parameters: {'observation_period_num': 6, 'train_rates': 0.9897639782591188, 'learning_rate': 6.269148305874307e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8061993072424676}. Best is trial 14 with value: 0.06148731708526611.[0m
[32m[I 2025-02-07 14:13:24,129][0m Trial 15 finished with value: 0.04713126433932263 and parameters: {'observation_period_num': 5, 'train_rates': 0.9222409174199184, 'learning_rate': 4.197733637058905e-05, 'batch_size': 80, 'step_size': 15, 'gamma': 0.8118633363256008}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:14:40,181][0m Trial 16 finished with value: 0.09367642551660538 and parameters: {'observation_period_num': 11, 'train_rates': 0.9770553937062247, 'learning_rate': 8.094831463947693e-06, 'batch_size': 80, 'step_size': 15, 'gamma': 0.7514911520029111}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:20:15,031][0m Trial 17 finished with value: 0.051000293110345686 and parameters: {'observation_period_num': 37, 'train_rates': 0.9266721479363048, 'learning_rate': 4.185464524965731e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8044494276651522}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:23:07,487][0m Trial 18 finished with value: 0.08124826381533919 and parameters: {'observation_period_num': 43, 'train_rates': 0.9270132951015484, 'learning_rate': 0.00020143941522923102, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8116892760420962}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:24:24,173][0m Trial 19 finished with value: 0.0613058346141245 and parameters: {'observation_period_num': 38, 'train_rates': 0.900432717075281, 'learning_rate': 3.8448475442265014e-05, 'batch_size': 74, 'step_size': 13, 'gamma': 0.8083892731976773}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:28:42,188][0m Trial 20 finished with value: 0.6575180380702852 and parameters: {'observation_period_num': 88, 'train_rates': 0.6014799579668175, 'learning_rate': 6.33866794462834e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7834831152093862}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:29:52,772][0m Trial 21 finished with value: 0.06840597684973869 and parameters: {'observation_period_num': 42, 'train_rates': 0.9266600123026383, 'learning_rate': 4.273154379822572e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.8149844221439317}. Best is trial 15 with value: 0.04713126433932263.[0m
Early stopping at epoch 45
[32m[I 2025-02-07 14:30:35,739][0m Trial 22 finished with value: 0.3873522398153357 and parameters: {'observation_period_num': 34, 'train_rates': 0.8605545887451246, 'learning_rate': 1.6849957754729763e-05, 'batch_size': 61, 'step_size': 1, 'gamma': 0.7548526531351732}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:31:31,855][0m Trial 23 finished with value: 0.11211475241900415 and parameters: {'observation_period_num': 74, 'train_rates': 0.9107731462057463, 'learning_rate': 2.735949060736323e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.7957649513469138}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:33:08,634][0m Trial 24 finished with value: 0.056937638378026435 and parameters: {'observation_period_num': 31, 'train_rates': 0.9479867110768829, 'learning_rate': 5.450236073585733e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8197072478459125}. Best is trial 15 with value: 0.04713126433932263.[0m
[32m[I 2025-02-07 14:38:50,484][0m Trial 25 finished with value: 0.04095493394364841 and parameters: {'observation_period_num': 23, 'train_rates': 0.9536341184224801, 'learning_rate': 0.00017857605391792701, 'batch_size': 17, 'step_size': 14, 'gamma': 0.824661530498101}. Best is trial 25 with value: 0.04095493394364841.[0m
[32m[I 2025-02-07 14:43:16,299][0m Trial 26 finished with value: 0.15916084183760196 and parameters: {'observation_period_num': 6, 'train_rates': 0.769467485908742, 'learning_rate': 0.00019505946144396857, 'batch_size': 19, 'step_size': 15, 'gamma': 0.76809764243347}. Best is trial 25 with value: 0.04095493394364841.[0m
[32m[I 2025-02-07 14:45:41,028][0m Trial 27 finished with value: 0.13635381576593292 and parameters: {'observation_period_num': 111, 'train_rates': 0.862193018883547, 'learning_rate': 0.00035083069102372625, 'batch_size': 37, 'step_size': 14, 'gamma': 0.831334744608262}. Best is trial 25 with value: 0.04095493394364841.[0m
[32m[I 2025-02-07 14:47:46,712][0m Trial 28 finished with value: 0.09093342810869216 and parameters: {'observation_period_num': 54, 'train_rates': 0.9569917692577469, 'learning_rate': 0.0007885695351713108, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8554453678662066}. Best is trial 25 with value: 0.04095493394364841.[0m
[32m[I 2025-02-07 14:48:45,261][0m Trial 29 finished with value: 0.060087730763714854 and parameters: {'observation_period_num': 54, 'train_rates': 0.9182579128510996, 'learning_rate': 0.00018859701197741218, 'batch_size': 101, 'step_size': 12, 'gamma': 0.7810355675028474}. Best is trial 25 with value: 0.04095493394364841.[0m
[32m[I 2025-02-07 14:53:04,073][0m Trial 30 finished with value: 0.03888488384998507 and parameters: {'observation_period_num': 25, 'train_rates': 0.9877422174981176, 'learning_rate': 0.00013013126665207682, 'batch_size': 23, 'step_size': 7, 'gamma': 0.7947236323821874}. Best is trial 30 with value: 0.03888488384998507.[0m
[32m[I 2025-02-07 14:57:02,821][0m Trial 31 finished with value: 0.029091901844367385 and parameters: {'observation_period_num': 21, 'train_rates': 0.9864236893422341, 'learning_rate': 0.0001305553500106674, 'batch_size': 25, 'step_size': 7, 'gamma': 0.7938259162142935}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:00:22,431][0m Trial 32 finished with value: 0.033610939513891935 and parameters: {'observation_period_num': 22, 'train_rates': 0.9866859134018918, 'learning_rate': 0.00013967332678842265, 'batch_size': 30, 'step_size': 7, 'gamma': 0.7897879851148961}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:03:31,955][0m Trial 33 finished with value: 0.057282157242298126 and parameters: {'observation_period_num': 25, 'train_rates': 0.9897404019464042, 'learning_rate': 0.0003993750339716736, 'batch_size': 32, 'step_size': 7, 'gamma': 0.7926459815504726}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:05:24,989][0m Trial 34 finished with value: 0.17072080346671017 and parameters: {'observation_period_num': 212, 'train_rates': 0.9681692188829789, 'learning_rate': 0.00024975516554096743, 'batch_size': 50, 'step_size': 7, 'gamma': 0.7700442645877494}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:08:32,944][0m Trial 35 finished with value: 0.08510918302212174 and parameters: {'observation_period_num': 83, 'train_rates': 0.9561541229203414, 'learning_rate': 0.00014825010242793297, 'batch_size': 31, 'step_size': 6, 'gamma': 0.8267276611822939}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:09:09,924][0m Trial 36 finished with value: 0.059254586696624756 and parameters: {'observation_period_num': 58, 'train_rates': 0.9860010273593489, 'learning_rate': 0.0005580561345388646, 'batch_size': 175, 'step_size': 8, 'gamma': 0.7716448915229834}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:10:59,293][0m Trial 37 finished with value: 0.04834851254798045 and parameters: {'observation_period_num': 23, 'train_rates': 0.9468426011280459, 'learning_rate': 0.00012048814436310386, 'batch_size': 54, 'step_size': 8, 'gamma': 0.796023832313524}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:11:31,735][0m Trial 38 finished with value: 0.16172414751455536 and parameters: {'observation_period_num': 50, 'train_rates': 0.6917556925416484, 'learning_rate': 0.0002645738759102624, 'batch_size': 155, 'step_size': 6, 'gamma': 0.8439589924763728}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:13:01,497][0m Trial 39 finished with value: 0.0918411234530007 and parameters: {'observation_period_num': 67, 'train_rates': 0.9719742339157126, 'learning_rate': 6.576496547688848e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.7872213263940968}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:15:15,443][0m Trial 40 finished with value: 0.10945721835725837 and parameters: {'observation_period_num': 101, 'train_rates': 0.9370458327981707, 'learning_rate': 0.00035108270527734354, 'batch_size': 43, 'step_size': 4, 'gamma': 0.7664868667875876}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:18:40,303][0m Trial 41 finished with value: 0.03379672456532717 and parameters: {'observation_period_num': 18, 'train_rates': 0.9663225800634112, 'learning_rate': 0.00011782547741198495, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8188789340459627}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:22:29,749][0m Trial 42 finished with value: 0.04357282444834709 and parameters: {'observation_period_num': 29, 'train_rates': 0.9736582506457451, 'learning_rate': 0.0005722424837737175, 'batch_size': 26, 'step_size': 6, 'gamma': 0.8279346712468372}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:24:44,549][0m Trial 43 finished with value: 0.03380641090715754 and parameters: {'observation_period_num': 17, 'train_rates': 0.9606024960743811, 'learning_rate': 0.00012943793508054746, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8673282604920683}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:26:45,149][0m Trial 44 finished with value: 0.4224660068142171 and parameters: {'observation_period_num': 16, 'train_rates': 0.9001458197852092, 'learning_rate': 1.248014749158419e-06, 'batch_size': 48, 'step_size': 5, 'gamma': 0.8687459959186558}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:28:54,820][0m Trial 45 finished with value: 0.058702451072214355 and parameters: {'observation_period_num': 50, 'train_rates': 0.8290486760172782, 'learning_rate': 0.00010370976983596015, 'batch_size': 41, 'step_size': 4, 'gamma': 0.965994611029713}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:32:13,197][0m Trial 46 finished with value: 0.16272916127653683 and parameters: {'observation_period_num': 206, 'train_rates': 0.9383156249482908, 'learning_rate': 7.320258289400686e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8442569108864976}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:33:38,314][0m Trial 47 finished with value: 0.15698784361867343 and parameters: {'observation_period_num': 248, 'train_rates': 0.9689786709958165, 'learning_rate': 0.00014826663914033384, 'batch_size': 66, 'step_size': 3, 'gamma': 0.9359112423949922}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:35:26,604][0m Trial 48 finished with value: 0.3187573254108429 and parameters: {'observation_period_num': 15, 'train_rates': 0.9897726500135073, 'learning_rate': 2.44336384168825e-06, 'batch_size': 57, 'step_size': 5, 'gamma': 0.7989414243876211}. Best is trial 31 with value: 0.029091901844367385.[0m
[32m[I 2025-02-07 15:38:39,326][0m Trial 49 finished with value: 0.20434559295152097 and parameters: {'observation_period_num': 77, 'train_rates': 0.7881713383052218, 'learning_rate': 7.996417764408001e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8756080213142987}. Best is trial 31 with value: 0.029091901844367385.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 11, 'train_rates': 0.9813758966883848, 'learning_rate': 9.836743040381044e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8170992920585325}
Epoch 1/300, trend Loss: 0.1786 | 0.1129
Epoch 2/300, trend Loss: 0.1145 | 0.0895
Epoch 3/300, trend Loss: 0.1052 | 0.0721
Epoch 4/300, trend Loss: 0.1004 | 0.0646
Epoch 5/300, trend Loss: 0.0955 | 0.0604
Epoch 6/300, trend Loss: 0.0928 | 0.0598
Epoch 7/300, trend Loss: 0.0897 | 0.0596
Epoch 8/300, trend Loss: 0.0866 | 0.0625
Epoch 9/300, trend Loss: 0.0832 | 0.0624
Epoch 10/300, trend Loss: 0.0820 | 0.0559
Epoch 11/300, trend Loss: 0.0818 | 0.0474
Epoch 12/300, trend Loss: 0.0809 | 0.0480
Epoch 13/300, trend Loss: 0.0795 | 0.0518
Epoch 14/300, trend Loss: 0.0762 | 0.0519
Epoch 15/300, trend Loss: 0.0738 | 0.0500
Epoch 16/300, trend Loss: 0.0722 | 0.0502
Epoch 17/300, trend Loss: 0.0707 | 0.0498
Epoch 18/300, trend Loss: 0.0694 | 0.0490
Epoch 19/300, trend Loss: 0.0681 | 0.0471
Epoch 20/300, trend Loss: 0.0672 | 0.0466
Epoch 21/300, trend Loss: 0.0664 | 0.0456
Epoch 22/300, trend Loss: 0.0656 | 0.0433
Epoch 23/300, trend Loss: 0.0651 | 0.0424
Epoch 24/300, trend Loss: 0.0647 | 0.0411
Epoch 25/300, trend Loss: 0.0643 | 0.0398
Epoch 26/300, trend Loss: 0.0638 | 0.0383
Epoch 27/300, trend Loss: 0.0636 | 0.0376
Epoch 28/300, trend Loss: 0.0633 | 0.0369
Epoch 29/300, trend Loss: 0.0629 | 0.0356
Epoch 30/300, trend Loss: 0.0627 | 0.0352
Epoch 31/300, trend Loss: 0.0625 | 0.0348
Epoch 32/300, trend Loss: 0.0622 | 0.0344
Epoch 33/300, trend Loss: 0.0619 | 0.0334
Epoch 34/300, trend Loss: 0.0617 | 0.0332
Epoch 35/300, trend Loss: 0.0615 | 0.0330
Epoch 36/300, trend Loss: 0.0613 | 0.0323
Epoch 37/300, trend Loss: 0.0611 | 0.0321
Epoch 38/300, trend Loss: 0.0609 | 0.0320
Epoch 39/300, trend Loss: 0.0608 | 0.0318
Epoch 40/300, trend Loss: 0.0606 | 0.0314
Epoch 41/300, trend Loss: 0.0605 | 0.0313
Epoch 42/300, trend Loss: 0.0603 | 0.0311
Epoch 43/300, trend Loss: 0.0602 | 0.0309
Epoch 44/300, trend Loss: 0.0601 | 0.0308
Epoch 45/300, trend Loss: 0.0600 | 0.0307
Epoch 46/300, trend Loss: 0.0599 | 0.0306
Epoch 47/300, trend Loss: 0.0598 | 0.0305
Epoch 48/300, trend Loss: 0.0597 | 0.0304
Epoch 49/300, trend Loss: 0.0596 | 0.0302
Epoch 50/300, trend Loss: 0.0595 | 0.0302
Epoch 51/300, trend Loss: 0.0594 | 0.0301
Epoch 52/300, trend Loss: 0.0593 | 0.0300
Epoch 53/300, trend Loss: 0.0592 | 0.0299
Epoch 54/300, trend Loss: 0.0592 | 0.0299
Epoch 55/300, trend Loss: 0.0591 | 0.0299
Epoch 56/300, trend Loss: 0.0591 | 0.0298
Epoch 57/300, trend Loss: 0.0590 | 0.0301
Epoch 58/300, trend Loss: 0.0589 | 0.0300
Epoch 59/300, trend Loss: 0.0589 | 0.0300
Epoch 60/300, trend Loss: 0.0588 | 0.0299
Epoch 61/300, trend Loss: 0.0588 | 0.0302
Epoch 62/300, trend Loss: 0.0587 | 0.0302
Epoch 63/300, trend Loss: 0.0587 | 0.0301
Epoch 64/300, trend Loss: 0.0586 | 0.0304
Epoch 65/300, trend Loss: 0.0586 | 0.0303
Epoch 66/300, trend Loss: 0.0585 | 0.0303
Epoch 67/300, trend Loss: 0.0585 | 0.0302
Epoch 68/300, trend Loss: 0.0585 | 0.0305
Epoch 69/300, trend Loss: 0.0584 | 0.0304
Epoch 70/300, trend Loss: 0.0584 | 0.0304
Epoch 71/300, trend Loss: 0.0584 | 0.0306
Epoch 72/300, trend Loss: 0.0583 | 0.0305
Epoch 73/300, trend Loss: 0.0583 | 0.0305
Epoch 74/300, trend Loss: 0.0583 | 0.0305
Epoch 75/300, trend Loss: 0.0583 | 0.0306
Epoch 76/300, trend Loss: 0.0583 | 0.0305
Epoch 77/300, trend Loss: 0.0582 | 0.0305
Epoch 78/300, trend Loss: 0.0582 | 0.0304
Epoch 79/300, trend Loss: 0.0582 | 0.0304
Epoch 80/300, trend Loss: 0.0582 | 0.0303
Epoch 81/300, trend Loss: 0.0582 | 0.0303
Epoch 82/300, trend Loss: 0.0582 | 0.0301
Epoch 83/300, trend Loss: 0.0581 | 0.0301
Epoch 84/300, trend Loss: 0.0581 | 0.0301
Epoch 85/300, trend Loss: 0.0581 | 0.0299
Epoch 86/300, trend Loss: 0.0581 | 0.0299
Epoch 87/300, trend Loss: 0.0581 | 0.0298
Epoch 88/300, trend Loss: 0.0580 | 0.0298
Epoch 89/300, trend Loss: 0.0580 | 0.0297
Epoch 90/300, trend Loss: 0.0580 | 0.0297
Epoch 91/300, trend Loss: 0.0580 | 0.0297
Epoch 92/300, trend Loss: 0.0580 | 0.0296
Epoch 93/300, trend Loss: 0.0580 | 0.0296
Epoch 94/300, trend Loss: 0.0580 | 0.0295
Epoch 95/300, trend Loss: 0.0579 | 0.0295
Epoch 96/300, trend Loss: 0.0579 | 0.0295
Epoch 97/300, trend Loss: 0.0579 | 0.0295
Epoch 98/300, trend Loss: 0.0579 | 0.0294
Epoch 99/300, trend Loss: 0.0579 | 0.0294
Epoch 100/300, trend Loss: 0.0579 | 0.0294
Epoch 101/300, trend Loss: 0.0579 | 0.0294
Epoch 102/300, trend Loss: 0.0579 | 0.0294
Epoch 103/300, trend Loss: 0.0579 | 0.0294
Epoch 104/300, trend Loss: 0.0578 | 0.0293
Epoch 105/300, trend Loss: 0.0578 | 0.0293
Epoch 106/300, trend Loss: 0.0578 | 0.0293
Epoch 107/300, trend Loss: 0.0578 | 0.0293
Epoch 108/300, trend Loss: 0.0578 | 0.0293
Epoch 109/300, trend Loss: 0.0578 | 0.0293
Epoch 110/300, trend Loss: 0.0578 | 0.0293
Epoch 111/300, trend Loss: 0.0578 | 0.0293
Epoch 112/300, trend Loss: 0.0578 | 0.0292
Epoch 113/300, trend Loss: 0.0578 | 0.0292
Epoch 114/300, trend Loss: 0.0578 | 0.0292
Epoch 115/300, trend Loss: 0.0578 | 0.0292
Epoch 116/300, trend Loss: 0.0578 | 0.0292
Epoch 117/300, trend Loss: 0.0578 | 0.0292
Epoch 118/300, trend Loss: 0.0578 | 0.0292
Epoch 119/300, trend Loss: 0.0578 | 0.0292
Epoch 120/300, trend Loss: 0.0577 | 0.0292
Epoch 121/300, trend Loss: 0.0577 | 0.0292
Epoch 122/300, trend Loss: 0.0577 | 0.0292
Epoch 123/300, trend Loss: 0.0577 | 0.0292
Epoch 124/300, trend Loss: 0.0577 | 0.0292
Epoch 125/300, trend Loss: 0.0577 | 0.0292
Epoch 126/300, trend Loss: 0.0577 | 0.0292
Epoch 127/300, trend Loss: 0.0577 | 0.0292
Epoch 128/300, trend Loss: 0.0577 | 0.0291
Epoch 129/300, trend Loss: 0.0577 | 0.0291
Epoch 130/300, trend Loss: 0.0577 | 0.0291
Epoch 131/300, trend Loss: 0.0577 | 0.0291
Epoch 132/300, trend Loss: 0.0577 | 0.0291
Epoch 133/300, trend Loss: 0.0577 | 0.0291
Epoch 134/300, trend Loss: 0.0577 | 0.0291
Epoch 135/300, trend Loss: 0.0577 | 0.0291
Epoch 136/300, trend Loss: 0.0577 | 0.0291
Epoch 137/300, trend Loss: 0.0577 | 0.0291
Epoch 138/300, trend Loss: 0.0577 | 0.0291
Epoch 139/300, trend Loss: 0.0577 | 0.0291
Epoch 140/300, trend Loss: 0.0577 | 0.0291
Epoch 141/300, trend Loss: 0.0577 | 0.0291
Epoch 142/300, trend Loss: 0.0577 | 0.0291
Epoch 143/300, trend Loss: 0.0577 | 0.0291
Epoch 144/300, trend Loss: 0.0577 | 0.0291
Epoch 145/300, trend Loss: 0.0577 | 0.0291
Epoch 146/300, trend Loss: 0.0577 | 0.0291
Epoch 147/300, trend Loss: 0.0577 | 0.0291
Epoch 148/300, trend Loss: 0.0577 | 0.0291
Epoch 149/300, trend Loss: 0.0577 | 0.0291
Epoch 150/300, trend Loss: 0.0577 | 0.0291
Epoch 151/300, trend Loss: 0.0577 | 0.0291
Epoch 152/300, trend Loss: 0.0577 | 0.0291
Epoch 153/300, trend Loss: 0.0577 | 0.0291
Epoch 154/300, trend Loss: 0.0577 | 0.0291
Epoch 155/300, trend Loss: 0.0577 | 0.0291
Epoch 156/300, trend Loss: 0.0577 | 0.0291
Epoch 157/300, trend Loss: 0.0577 | 0.0291
Epoch 158/300, trend Loss: 0.0577 | 0.0291
Epoch 159/300, trend Loss: 0.0577 | 0.0291
Epoch 160/300, trend Loss: 0.0577 | 0.0291
Epoch 161/300, trend Loss: 0.0577 | 0.0291
Epoch 162/300, trend Loss: 0.0577 | 0.0291
Epoch 163/300, trend Loss: 0.0577 | 0.0291
Epoch 164/300, trend Loss: 0.0577 | 0.0291
Epoch 165/300, trend Loss: 0.0577 | 0.0291
Epoch 166/300, trend Loss: 0.0577 | 0.0291
Epoch 167/300, trend Loss: 0.0577 | 0.0291
Epoch 168/300, trend Loss: 0.0577 | 0.0291
Epoch 169/300, trend Loss: 0.0577 | 0.0291
Epoch 170/300, trend Loss: 0.0577 | 0.0291
Epoch 171/300, trend Loss: 0.0577 | 0.0291
Epoch 172/300, trend Loss: 0.0577 | 0.0291
Epoch 173/300, trend Loss: 0.0577 | 0.0291
Epoch 174/300, trend Loss: 0.0577 | 0.0291
Epoch 175/300, trend Loss: 0.0577 | 0.0291
Epoch 176/300, trend Loss: 0.0577 | 0.0291
Epoch 177/300, trend Loss: 0.0577 | 0.0291
Epoch 178/300, trend Loss: 0.0577 | 0.0291
Epoch 179/300, trend Loss: 0.0577 | 0.0291
Epoch 180/300, trend Loss: 0.0577 | 0.0291
Epoch 181/300, trend Loss: 0.0577 | 0.0291
Epoch 182/300, trend Loss: 0.0577 | 0.0291
Epoch 183/300, trend Loss: 0.0577 | 0.0291
Epoch 184/300, trend Loss: 0.0577 | 0.0291
Epoch 185/300, trend Loss: 0.0577 | 0.0291
Epoch 186/300, trend Loss: 0.0577 | 0.0291
Epoch 187/300, trend Loss: 0.0577 | 0.0291
Epoch 188/300, trend Loss: 0.0577 | 0.0291
Epoch 189/300, trend Loss: 0.0577 | 0.0291
Epoch 190/300, trend Loss: 0.0577 | 0.0291
Epoch 191/300, trend Loss: 0.0577 | 0.0291
Epoch 192/300, trend Loss: 0.0577 | 0.0291
Epoch 193/300, trend Loss: 0.0577 | 0.0291
Epoch 194/300, trend Loss: 0.0577 | 0.0291
Epoch 195/300, trend Loss: 0.0577 | 0.0291
Epoch 196/300, trend Loss: 0.0577 | 0.0291
Epoch 197/300, trend Loss: 0.0577 | 0.0291
Epoch 198/300, trend Loss: 0.0577 | 0.0291
Epoch 199/300, trend Loss: 0.0577 | 0.0291
Epoch 200/300, trend Loss: 0.0577 | 0.0291
Epoch 201/300, trend Loss: 0.0577 | 0.0291
Epoch 202/300, trend Loss: 0.0577 | 0.0291
Epoch 203/300, trend Loss: 0.0577 | 0.0291
Epoch 204/300, trend Loss: 0.0577 | 0.0291
Epoch 205/300, trend Loss: 0.0577 | 0.0291
Epoch 206/300, trend Loss: 0.0577 | 0.0291
Epoch 207/300, trend Loss: 0.0577 | 0.0291
Epoch 208/300, trend Loss: 0.0577 | 0.0291
Epoch 209/300, trend Loss: 0.0577 | 0.0291
Epoch 210/300, trend Loss: 0.0577 | 0.0291
Epoch 211/300, trend Loss: 0.0577 | 0.0291
Epoch 212/300, trend Loss: 0.0577 | 0.0291
Epoch 213/300, trend Loss: 0.0577 | 0.0291
Epoch 214/300, trend Loss: 0.0577 | 0.0291
Epoch 215/300, trend Loss: 0.0577 | 0.0291
Epoch 216/300, trend Loss: 0.0577 | 0.0291
Epoch 217/300, trend Loss: 0.0577 | 0.0291
Epoch 218/300, trend Loss: 0.0577 | 0.0291
Epoch 219/300, trend Loss: 0.0577 | 0.0291
Epoch 220/300, trend Loss: 0.0577 | 0.0291
Epoch 221/300, trend Loss: 0.0577 | 0.0291
Epoch 222/300, trend Loss: 0.0577 | 0.0291
Epoch 223/300, trend Loss: 0.0577 | 0.0291
Epoch 224/300, trend Loss: 0.0577 | 0.0291
Epoch 225/300, trend Loss: 0.0577 | 0.0291
Epoch 226/300, trend Loss: 0.0577 | 0.0291
Epoch 227/300, trend Loss: 0.0577 | 0.0291
Epoch 228/300, trend Loss: 0.0577 | 0.0291
Epoch 229/300, trend Loss: 0.0577 | 0.0291
Epoch 230/300, trend Loss: 0.0577 | 0.0291
Epoch 231/300, trend Loss: 0.0577 | 0.0291
Epoch 232/300, trend Loss: 0.0577 | 0.0291
Epoch 233/300, trend Loss: 0.0577 | 0.0291
Epoch 234/300, trend Loss: 0.0577 | 0.0291
Epoch 235/300, trend Loss: 0.0577 | 0.0291
Epoch 236/300, trend Loss: 0.0577 | 0.0291
Epoch 237/300, trend Loss: 0.0577 | 0.0291
Epoch 238/300, trend Loss: 0.0577 | 0.0291
Epoch 239/300, trend Loss: 0.0577 | 0.0291
Epoch 240/300, trend Loss: 0.0577 | 0.0291
Epoch 241/300, trend Loss: 0.0577 | 0.0291
Epoch 242/300, trend Loss: 0.0577 | 0.0291
Epoch 243/300, trend Loss: 0.0577 | 0.0291
Epoch 244/300, trend Loss: 0.0577 | 0.0291
Epoch 245/300, trend Loss: 0.0577 | 0.0291
Epoch 246/300, trend Loss: 0.0577 | 0.0291
Epoch 247/300, trend Loss: 0.0577 | 0.0291
Epoch 248/300, trend Loss: 0.0577 | 0.0291
Epoch 249/300, trend Loss: 0.0577 | 0.0291
Epoch 250/300, trend Loss: 0.0577 | 0.0291
Epoch 251/300, trend Loss: 0.0577 | 0.0291
Epoch 252/300, trend Loss: 0.0577 | 0.0291
Epoch 253/300, trend Loss: 0.0577 | 0.0291
Epoch 254/300, trend Loss: 0.0577 | 0.0291
Epoch 255/300, trend Loss: 0.0577 | 0.0291
Epoch 256/300, trend Loss: 0.0577 | 0.0291
Epoch 257/300, trend Loss: 0.0577 | 0.0291
Epoch 258/300, trend Loss: 0.0577 | 0.0291
Epoch 259/300, trend Loss: 0.0577 | 0.0291
Epoch 260/300, trend Loss: 0.0577 | 0.0291
Epoch 261/300, trend Loss: 0.0577 | 0.0291
Epoch 262/300, trend Loss: 0.0577 | 0.0291
Epoch 263/300, trend Loss: 0.0577 | 0.0291
Epoch 264/300, trend Loss: 0.0577 | 0.0291
Epoch 265/300, trend Loss: 0.0577 | 0.0291
Epoch 266/300, trend Loss: 0.0577 | 0.0291
Epoch 267/300, trend Loss: 0.0577 | 0.0291
Epoch 268/300, trend Loss: 0.0577 | 0.0291
Epoch 269/300, trend Loss: 0.0577 | 0.0291
Epoch 270/300, trend Loss: 0.0577 | 0.0291
Epoch 271/300, trend Loss: 0.0577 | 0.0291
Epoch 272/300, trend Loss: 0.0577 | 0.0291
Epoch 273/300, trend Loss: 0.0577 | 0.0291
Epoch 274/300, trend Loss: 0.0577 | 0.0291
Epoch 275/300, trend Loss: 0.0577 | 0.0291
Epoch 276/300, trend Loss: 0.0577 | 0.0291
Epoch 277/300, trend Loss: 0.0577 | 0.0291
Epoch 278/300, trend Loss: 0.0577 | 0.0291
Epoch 279/300, trend Loss: 0.0577 | 0.0291
Epoch 280/300, trend Loss: 0.0577 | 0.0291
Epoch 281/300, trend Loss: 0.0577 | 0.0291
Epoch 282/300, trend Loss: 0.0577 | 0.0291
Epoch 283/300, trend Loss: 0.0577 | 0.0291
Epoch 284/300, trend Loss: 0.0577 | 0.0291
Epoch 285/300, trend Loss: 0.0577 | 0.0291
Epoch 286/300, trend Loss: 0.0577 | 0.0291
Epoch 287/300, trend Loss: 0.0577 | 0.0291
Epoch 288/300, trend Loss: 0.0577 | 0.0291
Epoch 289/300, trend Loss: 0.0577 | 0.0291
Epoch 290/300, trend Loss: 0.0577 | 0.0291
Epoch 291/300, trend Loss: 0.0577 | 0.0291
Epoch 292/300, trend Loss: 0.0577 | 0.0291
Epoch 293/300, trend Loss: 0.0577 | 0.0291
Epoch 294/300, trend Loss: 0.0577 | 0.0291
Epoch 295/300, trend Loss: 0.0577 | 0.0291
Epoch 296/300, trend Loss: 0.0577 | 0.0291
Epoch 297/300, trend Loss: 0.0577 | 0.0291
Epoch 298/300, trend Loss: 0.0577 | 0.0291
Epoch 299/300, trend Loss: 0.0577 | 0.0291
Epoch 300/300, trend Loss: 0.0577 | 0.0291
Training seasonal_0 component with params: {'observation_period_num': 12, 'train_rates': 0.9714805056951582, 'learning_rate': 0.0004937659633207597, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8737322253347861}
Epoch 1/300, seasonal_0 Loss: 0.1548 | 0.0914
Epoch 2/300, seasonal_0 Loss: 0.1118 | 0.0766
Epoch 3/300, seasonal_0 Loss: 0.1030 | 0.0736
Epoch 4/300, seasonal_0 Loss: 0.0997 | 0.0863
Epoch 5/300, seasonal_0 Loss: 0.0926 | 0.0693
Epoch 6/300, seasonal_0 Loss: 0.0867 | 0.0638
Epoch 7/300, seasonal_0 Loss: 0.0832 | 0.0628
Epoch 8/300, seasonal_0 Loss: 0.0798 | 0.0511
Epoch 9/300, seasonal_0 Loss: 0.0759 | 0.0483
Epoch 10/300, seasonal_0 Loss: 0.0725 | 0.0429
Epoch 11/300, seasonal_0 Loss: 0.0715 | 0.0428
Epoch 12/300, seasonal_0 Loss: 0.0689 | 0.0389
Epoch 13/300, seasonal_0 Loss: 0.0659 | 0.0393
Epoch 14/300, seasonal_0 Loss: 0.0653 | 0.0394
Epoch 15/300, seasonal_0 Loss: 0.0635 | 0.0387
Epoch 16/300, seasonal_0 Loss: 0.0623 | 0.0373
Epoch 17/300, seasonal_0 Loss: 0.0599 | 0.0373
Epoch 18/300, seasonal_0 Loss: 0.0592 | 0.0346
Epoch 19/300, seasonal_0 Loss: 0.0592 | 0.0338
Epoch 20/300, seasonal_0 Loss: 0.0586 | 0.0332
Epoch 21/300, seasonal_0 Loss: 0.0565 | 0.0341
Epoch 22/300, seasonal_0 Loss: 0.0556 | 0.0351
Epoch 23/300, seasonal_0 Loss: 0.0549 | 0.0350
Epoch 24/300, seasonal_0 Loss: 0.0561 | 0.0357
Epoch 25/300, seasonal_0 Loss: 0.0561 | 0.0343
Epoch 26/300, seasonal_0 Loss: 0.0534 | 0.0378
Epoch 27/300, seasonal_0 Loss: 0.0530 | 0.0367
Epoch 28/300, seasonal_0 Loss: 0.0528 | 0.0378
Epoch 29/300, seasonal_0 Loss: 0.0525 | 0.0348
Epoch 30/300, seasonal_0 Loss: 0.0512 | 0.0354
Epoch 31/300, seasonal_0 Loss: 0.0499 | 0.0354
Epoch 32/300, seasonal_0 Loss: 0.0501 | 0.0349
Epoch 33/300, seasonal_0 Loss: 0.0488 | 0.0337
Epoch 34/300, seasonal_0 Loss: 0.0476 | 0.0331
Epoch 35/300, seasonal_0 Loss: 0.0472 | 0.0332
Epoch 36/300, seasonal_0 Loss: 0.0468 | 0.0329
Epoch 37/300, seasonal_0 Loss: 0.0473 | 0.0393
Epoch 38/300, seasonal_0 Loss: 0.0473 | 0.0385
Epoch 39/300, seasonal_0 Loss: 0.0454 | 0.0350
Epoch 40/300, seasonal_0 Loss: 0.0468 | 0.0345
Epoch 41/300, seasonal_0 Loss: 0.0448 | 0.0345
Epoch 42/300, seasonal_0 Loss: 0.0441 | 0.0420
Epoch 43/300, seasonal_0 Loss: 0.0456 | 0.0442
Epoch 44/300, seasonal_0 Loss: 0.0450 | 0.0401
Epoch 45/300, seasonal_0 Loss: 0.0412 | 0.0415
Epoch 46/300, seasonal_0 Loss: 0.0436 | 0.0417
Epoch 47/300, seasonal_0 Loss: 0.0426 | 0.0491
Epoch 48/300, seasonal_0 Loss: 0.0386 | 0.0416
Epoch 49/300, seasonal_0 Loss: 0.0368 | 0.0384
Epoch 50/300, seasonal_0 Loss: 0.0404 | 0.0532
Epoch 51/300, seasonal_0 Loss: 0.0373 | 0.0431
Epoch 52/300, seasonal_0 Loss: 0.0355 | 0.0355
Epoch 53/300, seasonal_0 Loss: 0.0348 | 0.0372
Epoch 54/300, seasonal_0 Loss: 0.0356 | 0.0461
Epoch 55/300, seasonal_0 Loss: 0.0351 | 0.0336
Epoch 56/300, seasonal_0 Loss: 0.0339 | 0.0340
Epoch 57/300, seasonal_0 Loss: 0.0333 | 0.0330
Epoch 58/300, seasonal_0 Loss: 0.0327 | 0.0324
Epoch 59/300, seasonal_0 Loss: 0.0323 | 0.0334
Epoch 60/300, seasonal_0 Loss: 0.0335 | 0.0327
Epoch 61/300, seasonal_0 Loss: 0.0321 | 0.0331
Epoch 62/300, seasonal_0 Loss: 0.0314 | 0.0329
Epoch 63/300, seasonal_0 Loss: 0.0307 | 0.0322
Epoch 64/300, seasonal_0 Loss: 0.0303 | 0.0323
Epoch 65/300, seasonal_0 Loss: 0.0301 | 0.0323
Epoch 66/300, seasonal_0 Loss: 0.0298 | 0.0322
Epoch 67/300, seasonal_0 Loss: 0.0297 | 0.0329
Epoch 68/300, seasonal_0 Loss: 0.0295 | 0.0330
Epoch 69/300, seasonal_0 Loss: 0.0293 | 0.0330
Epoch 70/300, seasonal_0 Loss: 0.0293 | 0.0332
Epoch 71/300, seasonal_0 Loss: 0.0290 | 0.0338
Epoch 72/300, seasonal_0 Loss: 0.0287 | 0.0337
Epoch 73/300, seasonal_0 Loss: 0.0287 | 0.0358
Epoch 74/300, seasonal_0 Loss: 0.0284 | 0.0346
Epoch 75/300, seasonal_0 Loss: 0.0282 | 0.0350
Epoch 76/300, seasonal_0 Loss: 0.0280 | 0.0351
Epoch 77/300, seasonal_0 Loss: 0.0276 | 0.0336
Epoch 78/300, seasonal_0 Loss: 0.0275 | 0.0335
Epoch 79/300, seasonal_0 Loss: 0.0272 | 0.0333
Epoch 80/300, seasonal_0 Loss: 0.0269 | 0.0334
Epoch 81/300, seasonal_0 Loss: 0.0267 | 0.0334
Epoch 82/300, seasonal_0 Loss: 0.0265 | 0.0337
Epoch 83/300, seasonal_0 Loss: 0.0263 | 0.0340
Epoch 84/300, seasonal_0 Loss: 0.0262 | 0.0338
Epoch 85/300, seasonal_0 Loss: 0.0260 | 0.0341
Epoch 86/300, seasonal_0 Loss: 0.0259 | 0.0340
Epoch 87/300, seasonal_0 Loss: 0.0257 | 0.0340
Epoch 88/300, seasonal_0 Loss: 0.0256 | 0.0344
Epoch 89/300, seasonal_0 Loss: 0.0255 | 0.0341
Epoch 90/300, seasonal_0 Loss: 0.0254 | 0.0346
Epoch 91/300, seasonal_0 Loss: 0.0253 | 0.0347
Epoch 92/300, seasonal_0 Loss: 0.0252 | 0.0346
Epoch 93/300, seasonal_0 Loss: 0.0251 | 0.0354
Epoch 94/300, seasonal_0 Loss: 0.0250 | 0.0350
Epoch 95/300, seasonal_0 Loss: 0.0249 | 0.0353
Epoch 96/300, seasonal_0 Loss: 0.0248 | 0.0357
Epoch 97/300, seasonal_0 Loss: 0.0249 | 0.0357
Epoch 98/300, seasonal_0 Loss: 0.0247 | 0.0364
Epoch 99/300, seasonal_0 Loss: 0.0246 | 0.0363
Epoch 100/300, seasonal_0 Loss: 0.0245 | 0.0363
Epoch 101/300, seasonal_0 Loss: 0.0244 | 0.0371
Epoch 102/300, seasonal_0 Loss: 0.0243 | 0.0367
Epoch 103/300, seasonal_0 Loss: 0.0243 | 0.0370
Epoch 104/300, seasonal_0 Loss: 0.0243 | 0.0369
Epoch 105/300, seasonal_0 Loss: 0.0243 | 0.0367
Epoch 106/300, seasonal_0 Loss: 0.0242 | 0.0367
Epoch 107/300, seasonal_0 Loss: 0.0241 | 0.0365
Epoch 108/300, seasonal_0 Loss: 0.0240 | 0.0365
Epoch 109/300, seasonal_0 Loss: 0.0239 | 0.0364
Epoch 110/300, seasonal_0 Loss: 0.0238 | 0.0363
Epoch 111/300, seasonal_0 Loss: 0.0237 | 0.0364
Epoch 112/300, seasonal_0 Loss: 0.0237 | 0.0364
Epoch 113/300, seasonal_0 Loss: 0.0236 | 0.0363
Epoch 114/300, seasonal_0 Loss: 0.0236 | 0.0364
Epoch 115/300, seasonal_0 Loss: 0.0235 | 0.0364
Epoch 116/300, seasonal_0 Loss: 0.0234 | 0.0365
Epoch 117/300, seasonal_0 Loss: 0.0234 | 0.0364
Epoch 118/300, seasonal_0 Loss: 0.0234 | 0.0365
Epoch 119/300, seasonal_0 Loss: 0.0233 | 0.0365
Epoch 120/300, seasonal_0 Loss: 0.0233 | 0.0366
Epoch 121/300, seasonal_0 Loss: 0.0232 | 0.0366
Epoch 122/300, seasonal_0 Loss: 0.0232 | 0.0366
Epoch 123/300, seasonal_0 Loss: 0.0231 | 0.0367
Epoch 124/300, seasonal_0 Loss: 0.0231 | 0.0367
Epoch 125/300, seasonal_0 Loss: 0.0231 | 0.0367
Epoch 126/300, seasonal_0 Loss: 0.0230 | 0.0368
Epoch 127/300, seasonal_0 Loss: 0.0230 | 0.0368
Epoch 128/300, seasonal_0 Loss: 0.0230 | 0.0369
Epoch 129/300, seasonal_0 Loss: 0.0229 | 0.0368
Epoch 130/300, seasonal_0 Loss: 0.0229 | 0.0369
Epoch 131/300, seasonal_0 Loss: 0.0229 | 0.0369
Epoch 132/300, seasonal_0 Loss: 0.0228 | 0.0370
Epoch 133/300, seasonal_0 Loss: 0.0228 | 0.0370
Epoch 134/300, seasonal_0 Loss: 0.0228 | 0.0370
Epoch 135/300, seasonal_0 Loss: 0.0228 | 0.0370
Epoch 136/300, seasonal_0 Loss: 0.0227 | 0.0371
Epoch 137/300, seasonal_0 Loss: 0.0227 | 0.0371
Epoch 138/300, seasonal_0 Loss: 0.0227 | 0.0372
Epoch 139/300, seasonal_0 Loss: 0.0227 | 0.0372
Epoch 140/300, seasonal_0 Loss: 0.0227 | 0.0372
Epoch 141/300, seasonal_0 Loss: 0.0226 | 0.0373
Epoch 142/300, seasonal_0 Loss: 0.0226 | 0.0373
Epoch 143/300, seasonal_0 Loss: 0.0226 | 0.0373
Epoch 144/300, seasonal_0 Loss: 0.0226 | 0.0374
Epoch 145/300, seasonal_0 Loss: 0.0226 | 0.0374
Epoch 146/300, seasonal_0 Loss: 0.0225 | 0.0374
Epoch 147/300, seasonal_0 Loss: 0.0225 | 0.0374
Epoch 148/300, seasonal_0 Loss: 0.0225 | 0.0375
Epoch 149/300, seasonal_0 Loss: 0.0225 | 0.0375
Epoch 150/300, seasonal_0 Loss: 0.0225 | 0.0375
Epoch 151/300, seasonal_0 Loss: 0.0225 | 0.0375
Epoch 152/300, seasonal_0 Loss: 0.0225 | 0.0375
Epoch 153/300, seasonal_0 Loss: 0.0225 | 0.0376
Epoch 154/300, seasonal_0 Loss: 0.0224 | 0.0376
Epoch 155/300, seasonal_0 Loss: 0.0224 | 0.0376
Epoch 156/300, seasonal_0 Loss: 0.0224 | 0.0376
Epoch 157/300, seasonal_0 Loss: 0.0224 | 0.0376
Epoch 158/300, seasonal_0 Loss: 0.0224 | 0.0377
Epoch 159/300, seasonal_0 Loss: 0.0224 | 0.0377
Epoch 160/300, seasonal_0 Loss: 0.0224 | 0.0377
Epoch 161/300, seasonal_0 Loss: 0.0224 | 0.0377
Epoch 162/300, seasonal_0 Loss: 0.0224 | 0.0377
Epoch 163/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 164/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 165/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 166/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 167/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 168/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 169/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 170/300, seasonal_0 Loss: 0.0223 | 0.0377
Epoch 171/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 172/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 173/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 174/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 175/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 176/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 177/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 178/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 179/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 180/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 181/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 182/300, seasonal_0 Loss: 0.0222 | 0.0377
Epoch 183/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 184/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 185/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 186/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 187/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 188/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 189/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 190/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 191/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 192/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 193/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 194/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 195/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 196/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 197/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 198/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 199/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 200/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 201/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 202/300, seasonal_0 Loss: 0.0221 | 0.0377
Epoch 203/300, seasonal_0 Loss: 0.0221 | 0.0378
Epoch 204/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 205/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 206/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 207/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 208/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 209/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 210/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 211/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 212/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 213/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 214/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 215/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 216/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 217/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 218/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 219/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 220/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 221/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 222/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 223/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 224/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 225/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 226/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 227/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 228/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 229/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 230/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 231/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 232/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 233/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 234/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 235/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 236/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 237/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 238/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 239/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 240/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 241/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 242/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 243/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 244/300, seasonal_0 Loss: 0.0220 | 0.0378
Epoch 245/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 246/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 247/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 248/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 249/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 250/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 251/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 252/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 253/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 254/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 255/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 256/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 257/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 258/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 259/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 260/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 261/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 262/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 263/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 264/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 265/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 266/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 267/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 268/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 269/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 270/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 271/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 272/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 273/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 274/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 275/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 276/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 277/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 278/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 279/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 280/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 281/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 282/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 283/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 284/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 285/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 286/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 287/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 288/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 289/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 290/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 291/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 292/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 293/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 294/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 295/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 296/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 297/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 298/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 299/300, seasonal_0 Loss: 0.0220 | 0.0379
Epoch 300/300, seasonal_0 Loss: 0.0220 | 0.0379
Training seasonal_1 component with params: {'observation_period_num': 13, 'train_rates': 0.8543673718464878, 'learning_rate': 0.00034793358913057557, 'batch_size': 90, 'step_size': 11, 'gamma': 0.7796929596758373}
Epoch 1/300, seasonal_1 Loss: 0.3985 | 0.2868
Epoch 2/300, seasonal_1 Loss: 0.1609 | 0.1568
Epoch 3/300, seasonal_1 Loss: 0.1406 | 0.1056
Epoch 4/300, seasonal_1 Loss: 0.1287 | 0.1231
Epoch 5/300, seasonal_1 Loss: 0.1322 | 0.0980
Epoch 6/300, seasonal_1 Loss: 0.1419 | 0.1377
Epoch 7/300, seasonal_1 Loss: 0.1356 | 0.1790
Epoch 8/300, seasonal_1 Loss: 0.1199 | 0.1377
Epoch 9/300, seasonal_1 Loss: 0.1184 | 0.1148
Epoch 10/300, seasonal_1 Loss: 0.1227 | 0.1023
Epoch 11/300, seasonal_1 Loss: 0.1125 | 0.0806
Epoch 12/300, seasonal_1 Loss: 0.1246 | 0.0898
Epoch 13/300, seasonal_1 Loss: 0.1199 | 0.0708
Epoch 14/300, seasonal_1 Loss: 0.1074 | 0.0665
Epoch 15/300, seasonal_1 Loss: 0.1032 | 0.0623
Epoch 16/300, seasonal_1 Loss: 0.0987 | 0.0603
Epoch 17/300, seasonal_1 Loss: 0.0978 | 0.0654
Epoch 18/300, seasonal_1 Loss: 0.1010 | 0.0751
Epoch 19/300, seasonal_1 Loss: 0.0988 | 0.0635
Epoch 20/300, seasonal_1 Loss: 0.0924 | 0.0562
Epoch 21/300, seasonal_1 Loss: 0.0918 | 0.0539
Epoch 22/300, seasonal_1 Loss: 0.0928 | 0.0601
Epoch 23/300, seasonal_1 Loss: 0.0927 | 0.0746
Epoch 24/300, seasonal_1 Loss: 0.0902 | 0.0606
Epoch 25/300, seasonal_1 Loss: 0.0855 | 0.0502
Epoch 26/300, seasonal_1 Loss: 0.0830 | 0.0473
Epoch 27/300, seasonal_1 Loss: 0.0823 | 0.0464
Epoch 28/300, seasonal_1 Loss: 0.0821 | 0.0461
Epoch 29/300, seasonal_1 Loss: 0.0818 | 0.0454
Epoch 30/300, seasonal_1 Loss: 0.0816 | 0.0443
Epoch 31/300, seasonal_1 Loss: 0.0812 | 0.0443
Epoch 32/300, seasonal_1 Loss: 0.0807 | 0.0453
Epoch 33/300, seasonal_1 Loss: 0.0800 | 0.0456
Epoch 34/300, seasonal_1 Loss: 0.0791 | 0.0447
Epoch 35/300, seasonal_1 Loss: 0.0784 | 0.0431
Epoch 36/300, seasonal_1 Loss: 0.0783 | 0.0425
Epoch 37/300, seasonal_1 Loss: 0.0787 | 0.0423
Epoch 38/300, seasonal_1 Loss: 0.0792 | 0.0424
Epoch 39/300, seasonal_1 Loss: 0.0792 | 0.0424
Epoch 40/300, seasonal_1 Loss: 0.0784 | 0.0422
Epoch 41/300, seasonal_1 Loss: 0.0769 | 0.0416
Epoch 42/300, seasonal_1 Loss: 0.0759 | 0.0406
Epoch 43/300, seasonal_1 Loss: 0.0761 | 0.0409
Epoch 44/300, seasonal_1 Loss: 0.0763 | 0.0412
Epoch 45/300, seasonal_1 Loss: 0.0762 | 0.0405
Epoch 46/300, seasonal_1 Loss: 0.0756 | 0.0403
Epoch 47/300, seasonal_1 Loss: 0.0749 | 0.0406
Epoch 48/300, seasonal_1 Loss: 0.0748 | 0.0405
Epoch 49/300, seasonal_1 Loss: 0.0747 | 0.0401
Epoch 50/300, seasonal_1 Loss: 0.0744 | 0.0397
Epoch 51/300, seasonal_1 Loss: 0.0741 | 0.0393
Epoch 52/300, seasonal_1 Loss: 0.0738 | 0.0391
Epoch 53/300, seasonal_1 Loss: 0.0736 | 0.0390
Epoch 54/300, seasonal_1 Loss: 0.0735 | 0.0389
Epoch 55/300, seasonal_1 Loss: 0.0734 | 0.0388
Epoch 56/300, seasonal_1 Loss: 0.0733 | 0.0387
Epoch 57/300, seasonal_1 Loss: 0.0732 | 0.0386
Epoch 58/300, seasonal_1 Loss: 0.0732 | 0.0385
Epoch 59/300, seasonal_1 Loss: 0.0731 | 0.0384
Epoch 60/300, seasonal_1 Loss: 0.0730 | 0.0383
Epoch 61/300, seasonal_1 Loss: 0.0730 | 0.0382
Epoch 62/300, seasonal_1 Loss: 0.0729 | 0.0381
Epoch 63/300, seasonal_1 Loss: 0.0730 | 0.0381
Epoch 64/300, seasonal_1 Loss: 0.0730 | 0.0381
Epoch 65/300, seasonal_1 Loss: 0.0730 | 0.0380
Epoch 66/300, seasonal_1 Loss: 0.0729 | 0.0380
Epoch 67/300, seasonal_1 Loss: 0.0729 | 0.0380
Epoch 68/300, seasonal_1 Loss: 0.0728 | 0.0380
Epoch 69/300, seasonal_1 Loss: 0.0725 | 0.0380
Epoch 70/300, seasonal_1 Loss: 0.0723 | 0.0380
Epoch 71/300, seasonal_1 Loss: 0.0721 | 0.0380
Epoch 72/300, seasonal_1 Loss: 0.0719 | 0.0379
Epoch 73/300, seasonal_1 Loss: 0.0718 | 0.0381
Epoch 74/300, seasonal_1 Loss: 0.0717 | 0.0380
Epoch 75/300, seasonal_1 Loss: 0.0716 | 0.0379
Epoch 76/300, seasonal_1 Loss: 0.0715 | 0.0377
Epoch 77/300, seasonal_1 Loss: 0.0714 | 0.0376
Epoch 78/300, seasonal_1 Loss: 0.0713 | 0.0376
Epoch 79/300, seasonal_1 Loss: 0.0712 | 0.0375
Epoch 80/300, seasonal_1 Loss: 0.0712 | 0.0374
Epoch 81/300, seasonal_1 Loss: 0.0711 | 0.0373
Epoch 82/300, seasonal_1 Loss: 0.0711 | 0.0373
Epoch 83/300, seasonal_1 Loss: 0.0710 | 0.0373
Epoch 84/300, seasonal_1 Loss: 0.0710 | 0.0373
Epoch 85/300, seasonal_1 Loss: 0.0709 | 0.0372
Epoch 86/300, seasonal_1 Loss: 0.0709 | 0.0372
Epoch 87/300, seasonal_1 Loss: 0.0708 | 0.0372
Epoch 88/300, seasonal_1 Loss: 0.0708 | 0.0371
Epoch 89/300, seasonal_1 Loss: 0.0708 | 0.0371
Epoch 90/300, seasonal_1 Loss: 0.0707 | 0.0371
Epoch 91/300, seasonal_1 Loss: 0.0707 | 0.0371
Epoch 92/300, seasonal_1 Loss: 0.0707 | 0.0370
Epoch 93/300, seasonal_1 Loss: 0.0706 | 0.0370
Epoch 94/300, seasonal_1 Loss: 0.0706 | 0.0370
Epoch 95/300, seasonal_1 Loss: 0.0706 | 0.0370
Epoch 96/300, seasonal_1 Loss: 0.0706 | 0.0369
Epoch 97/300, seasonal_1 Loss: 0.0705 | 0.0369
Epoch 98/300, seasonal_1 Loss: 0.0705 | 0.0369
Epoch 99/300, seasonal_1 Loss: 0.0705 | 0.0369
Epoch 100/300, seasonal_1 Loss: 0.0705 | 0.0369
Epoch 101/300, seasonal_1 Loss: 0.0704 | 0.0369
Epoch 102/300, seasonal_1 Loss: 0.0704 | 0.0368
Epoch 103/300, seasonal_1 Loss: 0.0704 | 0.0368
Epoch 104/300, seasonal_1 Loss: 0.0704 | 0.0368
Epoch 105/300, seasonal_1 Loss: 0.0704 | 0.0368
Epoch 106/300, seasonal_1 Loss: 0.0703 | 0.0368
Epoch 107/300, seasonal_1 Loss: 0.0703 | 0.0368
Epoch 108/300, seasonal_1 Loss: 0.0703 | 0.0368
Epoch 109/300, seasonal_1 Loss: 0.0703 | 0.0367
Epoch 110/300, seasonal_1 Loss: 0.0703 | 0.0367
Epoch 111/300, seasonal_1 Loss: 0.0703 | 0.0367
Epoch 112/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 113/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 114/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 115/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 116/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 117/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 118/300, seasonal_1 Loss: 0.0702 | 0.0367
Epoch 119/300, seasonal_1 Loss: 0.0702 | 0.0366
Epoch 120/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 121/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 122/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 123/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 124/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 125/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 126/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 127/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 128/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 129/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 130/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 131/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 132/300, seasonal_1 Loss: 0.0701 | 0.0366
Epoch 133/300, seasonal_1 Loss: 0.0700 | 0.0366
Epoch 134/300, seasonal_1 Loss: 0.0700 | 0.0366
Epoch 135/300, seasonal_1 Loss: 0.0700 | 0.0366
Epoch 136/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 137/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 138/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 139/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 140/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 141/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 142/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 143/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 144/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 145/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 146/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 147/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 148/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 149/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 150/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 151/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 152/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 153/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 154/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 155/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 156/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 157/300, seasonal_1 Loss: 0.0700 | 0.0365
Epoch 158/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 159/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 160/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 161/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 162/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 163/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 164/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 165/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 166/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 167/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 168/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 169/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 170/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 171/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 172/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 173/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 174/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 175/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 176/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 177/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 178/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 179/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 180/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 181/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 182/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 183/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 184/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 185/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 186/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 187/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 188/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 189/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 190/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 191/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 192/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 193/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 194/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 195/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 196/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 197/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 198/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 199/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 200/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 201/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 202/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 203/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 204/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 205/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 206/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 207/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 208/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 209/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 210/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 211/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 212/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 213/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 214/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 215/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 216/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 217/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 218/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 219/300, seasonal_1 Loss: 0.0699 | 0.0365
Epoch 220/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 221/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 222/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 223/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 224/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 225/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 226/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 227/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 228/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 229/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 230/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 231/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 232/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 233/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 234/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 235/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 236/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 237/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 238/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 239/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 240/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 241/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 242/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 243/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 244/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 245/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 246/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 247/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 248/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 249/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 250/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 251/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 252/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 253/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 254/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 255/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 256/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 257/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 258/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 259/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 260/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 261/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 262/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 263/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 264/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 265/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 266/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 267/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 268/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 269/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 270/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 271/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 272/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 273/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 274/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 275/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 276/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 277/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 278/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 279/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 280/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 281/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 282/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 283/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 284/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 285/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 286/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 287/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 288/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 289/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 290/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 291/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 292/300, seasonal_1 Loss: 0.0699 | 0.0364
Epoch 293/300, seasonal_1 Loss: 0.0699 | 0.0364
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 30, 'train_rates': 0.924746879506371, 'learning_rate': 9.429915472998664e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9610731245810124}
Epoch 1/300, seasonal_2 Loss: 0.8520 | 0.5429
Epoch 2/300, seasonal_2 Loss: 0.3019 | 0.3721
Epoch 3/300, seasonal_2 Loss: 0.2176 | 0.3228
Epoch 4/300, seasonal_2 Loss: 0.1913 | 0.2583
Epoch 5/300, seasonal_2 Loss: 0.1821 | 0.2523
Epoch 6/300, seasonal_2 Loss: 0.1797 | 0.2980
Epoch 7/300, seasonal_2 Loss: 0.1714 | 0.3417
Epoch 8/300, seasonal_2 Loss: 0.1551 | 0.2987
Epoch 9/300, seasonal_2 Loss: 0.1409 | 0.2371
Epoch 10/300, seasonal_2 Loss: 0.1322 | 0.2014
Epoch 11/300, seasonal_2 Loss: 0.1275 | 0.1737
Epoch 12/300, seasonal_2 Loss: 0.1245 | 0.1497
Epoch 13/300, seasonal_2 Loss: 0.1222 | 0.1340
Epoch 14/300, seasonal_2 Loss: 0.1194 | 0.1233
Epoch 15/300, seasonal_2 Loss: 0.1163 | 0.1147
Epoch 16/300, seasonal_2 Loss: 0.1131 | 0.1091
Epoch 17/300, seasonal_2 Loss: 0.1099 | 0.1046
Epoch 18/300, seasonal_2 Loss: 0.1074 | 0.1006
Epoch 19/300, seasonal_2 Loss: 0.1055 | 0.0962
Epoch 20/300, seasonal_2 Loss: 0.1044 | 0.0932
Epoch 21/300, seasonal_2 Loss: 0.1041 | 0.0907
Epoch 22/300, seasonal_2 Loss: 0.1040 | 0.0880
Epoch 23/300, seasonal_2 Loss: 0.1038 | 0.0864
Epoch 24/300, seasonal_2 Loss: 0.1026 | 0.0851
Epoch 25/300, seasonal_2 Loss: 0.1001 | 0.0839
Epoch 26/300, seasonal_2 Loss: 0.0977 | 0.0822
Epoch 27/300, seasonal_2 Loss: 0.0963 | 0.0809
Epoch 28/300, seasonal_2 Loss: 0.0959 | 0.0796
Epoch 29/300, seasonal_2 Loss: 0.0963 | 0.0772
Epoch 30/300, seasonal_2 Loss: 0.0964 | 0.0760
Epoch 31/300, seasonal_2 Loss: 0.0950 | 0.0744
Epoch 32/300, seasonal_2 Loss: 0.0928 | 0.0723
Epoch 33/300, seasonal_2 Loss: 0.0908 | 0.0701
Epoch 34/300, seasonal_2 Loss: 0.0896 | 0.0680
Epoch 35/300, seasonal_2 Loss: 0.0889 | 0.0660
Epoch 36/300, seasonal_2 Loss: 0.0885 | 0.0645
Epoch 37/300, seasonal_2 Loss: 0.0890 | 0.0634
Epoch 38/300, seasonal_2 Loss: 0.0905 | 0.0634
Epoch 39/300, seasonal_2 Loss: 0.0934 | 0.0673
Epoch 40/300, seasonal_2 Loss: 0.0977 | 0.0912
Epoch 41/300, seasonal_2 Loss: 0.1010 | 0.1283
Epoch 42/300, seasonal_2 Loss: 0.0995 | 0.1456
Epoch 43/300, seasonal_2 Loss: 0.0936 | 0.1297
Epoch 44/300, seasonal_2 Loss: 0.0873 | 0.1017
Epoch 45/300, seasonal_2 Loss: 0.0842 | 0.0821
Epoch 46/300, seasonal_2 Loss: 0.0848 | 0.0688
Epoch 47/300, seasonal_2 Loss: 0.0865 | 0.0619
Epoch 48/300, seasonal_2 Loss: 0.0863 | 0.0601
Epoch 49/300, seasonal_2 Loss: 0.0839 | 0.0590
Epoch 50/300, seasonal_2 Loss: 0.0820 | 0.0579
Epoch 51/300, seasonal_2 Loss: 0.0820 | 0.0572
Epoch 52/300, seasonal_2 Loss: 0.0825 | 0.0571
Epoch 53/300, seasonal_2 Loss: 0.0824 | 0.0574
Epoch 54/300, seasonal_2 Loss: 0.0804 | 0.0587
Epoch 55/300, seasonal_2 Loss: 0.0790 | 0.0638
Epoch 56/300, seasonal_2 Loss: 0.0801 | 0.0744
Epoch 57/300, seasonal_2 Loss: 0.0816 | 0.0868
Epoch 58/300, seasonal_2 Loss: 0.0806 | 0.0852
Epoch 59/300, seasonal_2 Loss: 0.0781 | 0.0766
Epoch 60/300, seasonal_2 Loss: 0.0763 | 0.0688
Epoch 61/300, seasonal_2 Loss: 0.0755 | 0.0624
Epoch 62/300, seasonal_2 Loss: 0.0755 | 0.0570
Epoch 63/300, seasonal_2 Loss: 0.0764 | 0.0533
Epoch 64/300, seasonal_2 Loss: 0.0773 | 0.0519
Epoch 65/300, seasonal_2 Loss: 0.0768 | 0.0513
Epoch 66/300, seasonal_2 Loss: 0.0754 | 0.0508
Epoch 67/300, seasonal_2 Loss: 0.0745 | 0.0506
Epoch 68/300, seasonal_2 Loss: 0.0741 | 0.0505
Epoch 69/300, seasonal_2 Loss: 0.0737 | 0.0509
Epoch 70/300, seasonal_2 Loss: 0.0735 | 0.0521
Epoch 71/300, seasonal_2 Loss: 0.0738 | 0.0551
Epoch 72/300, seasonal_2 Loss: 0.0746 | 0.0599
Epoch 73/300, seasonal_2 Loss: 0.0754 | 0.0652
Epoch 74/300, seasonal_2 Loss: 0.0756 | 0.0686
Epoch 75/300, seasonal_2 Loss: 0.0748 | 0.0695
Epoch 76/300, seasonal_2 Loss: 0.0736 | 0.0653
Epoch 77/300, seasonal_2 Loss: 0.0724 | 0.0604
Epoch 78/300, seasonal_2 Loss: 0.0719 | 0.0560
Epoch 79/300, seasonal_2 Loss: 0.0721 | 0.0522
Epoch 80/300, seasonal_2 Loss: 0.0728 | 0.0497
Epoch 81/300, seasonal_2 Loss: 0.0736 | 0.0487
Epoch 82/300, seasonal_2 Loss: 0.0734 | 0.0483
Epoch 83/300, seasonal_2 Loss: 0.0723 | 0.0478
Epoch 84/300, seasonal_2 Loss: 0.0713 | 0.0477
Epoch 85/300, seasonal_2 Loss: 0.0710 | 0.0477
Epoch 86/300, seasonal_2 Loss: 0.0710 | 0.0485
Epoch 87/300, seasonal_2 Loss: 0.0711 | 0.0502
Epoch 88/300, seasonal_2 Loss: 0.0714 | 0.0532
Epoch 89/300, seasonal_2 Loss: 0.0718 | 0.0579
Epoch 90/300, seasonal_2 Loss: 0.0720 | 0.0612
Epoch 91/300, seasonal_2 Loss: 0.0718 | 0.0619
Epoch 92/300, seasonal_2 Loss: 0.0712 | 0.0608
Epoch 93/300, seasonal_2 Loss: 0.0706 | 0.0568
Epoch 94/300, seasonal_2 Loss: 0.0699 | 0.0528
Epoch 95/300, seasonal_2 Loss: 0.0696 | 0.0496
Epoch 96/300, seasonal_2 Loss: 0.0697 | 0.0473
Epoch 97/300, seasonal_2 Loss: 0.0700 | 0.0464
Epoch 98/300, seasonal_2 Loss: 0.0698 | 0.0461
Epoch 99/300, seasonal_2 Loss: 0.0692 | 0.0458
Epoch 100/300, seasonal_2 Loss: 0.0684 | 0.0456
Epoch 101/300, seasonal_2 Loss: 0.0680 | 0.0461
Epoch 102/300, seasonal_2 Loss: 0.0679 | 0.0471
Epoch 103/300, seasonal_2 Loss: 0.0679 | 0.0490
Epoch 104/300, seasonal_2 Loss: 0.0681 | 0.0510
Epoch 105/300, seasonal_2 Loss: 0.0681 | 0.0524
Epoch 106/300, seasonal_2 Loss: 0.0679 | 0.0532
Epoch 107/300, seasonal_2 Loss: 0.0676 | 0.0522
Epoch 108/300, seasonal_2 Loss: 0.0672 | 0.0506
Epoch 109/300, seasonal_2 Loss: 0.0668 | 0.0490
Epoch 110/300, seasonal_2 Loss: 0.0666 | 0.0473
Epoch 111/300, seasonal_2 Loss: 0.0666 | 0.0460
Epoch 112/300, seasonal_2 Loss: 0.0666 | 0.0451
Epoch 113/300, seasonal_2 Loss: 0.0667 | 0.0446
Epoch 114/300, seasonal_2 Loss: 0.0665 | 0.0444
Epoch 115/300, seasonal_2 Loss: 0.0662 | 0.0442
Epoch 116/300, seasonal_2 Loss: 0.0658 | 0.0443
Epoch 117/300, seasonal_2 Loss: 0.0656 | 0.0447
Epoch 118/300, seasonal_2 Loss: 0.0655 | 0.0456
Epoch 119/300, seasonal_2 Loss: 0.0656 | 0.0469
Epoch 120/300, seasonal_2 Loss: 0.0658 | 0.0484
Epoch 121/300, seasonal_2 Loss: 0.0658 | 0.0493
Epoch 122/300, seasonal_2 Loss: 0.0656 | 0.0495
Epoch 123/300, seasonal_2 Loss: 0.0654 | 0.0493
Epoch 124/300, seasonal_2 Loss: 0.0651 | 0.0488
Epoch 125/300, seasonal_2 Loss: 0.0648 | 0.0476
Epoch 126/300, seasonal_2 Loss: 0.0646 | 0.0464
Epoch 127/300, seasonal_2 Loss: 0.0645 | 0.0452
Epoch 128/300, seasonal_2 Loss: 0.0645 | 0.0443
Epoch 129/300, seasonal_2 Loss: 0.0646 | 0.0436
Epoch 130/300, seasonal_2 Loss: 0.0646 | 0.0433
Epoch 131/300, seasonal_2 Loss: 0.0645 | 0.0431
Epoch 132/300, seasonal_2 Loss: 0.0642 | 0.0430
Epoch 133/300, seasonal_2 Loss: 0.0639 | 0.0432
Epoch 134/300, seasonal_2 Loss: 0.0637 | 0.0437
Epoch 135/300, seasonal_2 Loss: 0.0637 | 0.0447
Epoch 136/300, seasonal_2 Loss: 0.0638 | 0.0460
Epoch 137/300, seasonal_2 Loss: 0.0639 | 0.0472
Epoch 138/300, seasonal_2 Loss: 0.0639 | 0.0483
Epoch 139/300, seasonal_2 Loss: 0.0637 | 0.0484
Epoch 140/300, seasonal_2 Loss: 0.0635 | 0.0479
Epoch 141/300, seasonal_2 Loss: 0.0632 | 0.0471
Epoch 142/300, seasonal_2 Loss: 0.0630 | 0.0459
Epoch 143/300, seasonal_2 Loss: 0.0628 | 0.0447
Epoch 144/300, seasonal_2 Loss: 0.0628 | 0.0437
Epoch 145/300, seasonal_2 Loss: 0.0628 | 0.0429
Epoch 146/300, seasonal_2 Loss: 0.0628 | 0.0426
Epoch 147/300, seasonal_2 Loss: 0.0628 | 0.0424
Epoch 148/300, seasonal_2 Loss: 0.0625 | 0.0423
Epoch 149/300, seasonal_2 Loss: 0.0623 | 0.0424
Epoch 150/300, seasonal_2 Loss: 0.0620 | 0.0428
Epoch 151/300, seasonal_2 Loss: 0.0620 | 0.0437
Epoch 152/300, seasonal_2 Loss: 0.0620 | 0.0447
Epoch 153/300, seasonal_2 Loss: 0.0620 | 0.0455
Epoch 154/300, seasonal_2 Loss: 0.0619 | 0.0459
Epoch 155/300, seasonal_2 Loss: 0.0618 | 0.0461
Epoch 156/300, seasonal_2 Loss: 0.0616 | 0.0455
Epoch 157/300, seasonal_2 Loss: 0.0614 | 0.0447
Epoch 158/300, seasonal_2 Loss: 0.0612 | 0.0440
Epoch 159/300, seasonal_2 Loss: 0.0612 | 0.0432
Epoch 160/300, seasonal_2 Loss: 0.0611 | 0.0427
Epoch 161/300, seasonal_2 Loss: 0.0611 | 0.0423
Epoch 162/300, seasonal_2 Loss: 0.0611 | 0.0421
Epoch 163/300, seasonal_2 Loss: 0.0610 | 0.0420
Epoch 164/300, seasonal_2 Loss: 0.0608 | 0.0420
Epoch 165/300, seasonal_2 Loss: 0.0606 | 0.0422
Epoch 166/300, seasonal_2 Loss: 0.0605 | 0.0426
Epoch 167/300, seasonal_2 Loss: 0.0605 | 0.0431
Epoch 168/300, seasonal_2 Loss: 0.0605 | 0.0436
Epoch 169/300, seasonal_2 Loss: 0.0604 | 0.0441
Epoch 170/300, seasonal_2 Loss: 0.0603 | 0.0443
Epoch 171/300, seasonal_2 Loss: 0.0602 | 0.0442
Epoch 172/300, seasonal_2 Loss: 0.0601 | 0.0439
Epoch 173/300, seasonal_2 Loss: 0.0600 | 0.0436
Epoch 174/300, seasonal_2 Loss: 0.0599 | 0.0431
Epoch 175/300, seasonal_2 Loss: 0.0598 | 0.0427
Epoch 176/300, seasonal_2 Loss: 0.0598 | 0.0422
Epoch 177/300, seasonal_2 Loss: 0.0598 | 0.0420
Epoch 178/300, seasonal_2 Loss: 0.0597 | 0.0418
Epoch 179/300, seasonal_2 Loss: 0.0597 | 0.0417
Epoch 180/300, seasonal_2 Loss: 0.0596 | 0.0417
Epoch 181/300, seasonal_2 Loss: 0.0595 | 0.0418
Epoch 182/300, seasonal_2 Loss: 0.0594 | 0.0421
Epoch 183/300, seasonal_2 Loss: 0.0593 | 0.0424
Epoch 184/300, seasonal_2 Loss: 0.0593 | 0.0427
Epoch 185/300, seasonal_2 Loss: 0.0592 | 0.0431
Epoch 186/300, seasonal_2 Loss: 0.0592 | 0.0433
Epoch 187/300, seasonal_2 Loss: 0.0591 | 0.0435
Epoch 188/300, seasonal_2 Loss: 0.0590 | 0.0434
Epoch 189/300, seasonal_2 Loss: 0.0589 | 0.0432
Epoch 190/300, seasonal_2 Loss: 0.0589 | 0.0429
Epoch 191/300, seasonal_2 Loss: 0.0588 | 0.0425
Epoch 192/300, seasonal_2 Loss: 0.0587 | 0.0422
Epoch 193/300, seasonal_2 Loss: 0.0587 | 0.0419
Epoch 194/300, seasonal_2 Loss: 0.0587 | 0.0416
Epoch 195/300, seasonal_2 Loss: 0.0586 | 0.0415
Epoch 196/300, seasonal_2 Loss: 0.0586 | 0.0414
Epoch 197/300, seasonal_2 Loss: 0.0585 | 0.0413
Epoch 198/300, seasonal_2 Loss: 0.0584 | 0.0414
Epoch 199/300, seasonal_2 Loss: 0.0583 | 0.0416
Epoch 200/300, seasonal_2 Loss: 0.0583 | 0.0418
Epoch 201/300, seasonal_2 Loss: 0.0582 | 0.0421
Epoch 202/300, seasonal_2 Loss: 0.0582 | 0.0423
Epoch 203/300, seasonal_2 Loss: 0.0582 | 0.0425
Epoch 204/300, seasonal_2 Loss: 0.0581 | 0.0427
Epoch 205/300, seasonal_2 Loss: 0.0581 | 0.0426
Epoch 206/300, seasonal_2 Loss: 0.0580 | 0.0425
Epoch 207/300, seasonal_2 Loss: 0.0579 | 0.0424
Epoch 208/300, seasonal_2 Loss: 0.0578 | 0.0421
Epoch 209/300, seasonal_2 Loss: 0.0578 | 0.0419
Epoch 210/300, seasonal_2 Loss: 0.0578 | 0.0417
Epoch 211/300, seasonal_2 Loss: 0.0577 | 0.0415
Epoch 212/300, seasonal_2 Loss: 0.0577 | 0.0413
Epoch 213/300, seasonal_2 Loss: 0.0577 | 0.0413
Epoch 214/300, seasonal_2 Loss: 0.0576 | 0.0412
Epoch 215/300, seasonal_2 Loss: 0.0575 | 0.0412
Epoch 216/300, seasonal_2 Loss: 0.0575 | 0.0413
Epoch 217/300, seasonal_2 Loss: 0.0574 | 0.0414
Epoch 218/300, seasonal_2 Loss: 0.0574 | 0.0415
Epoch 219/300, seasonal_2 Loss: 0.0573 | 0.0417
Epoch 220/300, seasonal_2 Loss: 0.0573 | 0.0419
Epoch 221/300, seasonal_2 Loss: 0.0573 | 0.0420
Epoch 222/300, seasonal_2 Loss: 0.0573 | 0.0421
Epoch 223/300, seasonal_2 Loss: 0.0573 | 0.0422
Epoch 224/300, seasonal_2 Loss: 0.0572 | 0.0421
Epoch 225/300, seasonal_2 Loss: 0.0572 | 0.0421
Epoch 226/300, seasonal_2 Loss: 0.0571 | 0.0419
Epoch 227/300, seasonal_2 Loss: 0.0570 | 0.0417
Epoch 228/300, seasonal_2 Loss: 0.0570 | 0.0416
Epoch 229/300, seasonal_2 Loss: 0.0570 | 0.0414
Epoch 230/300, seasonal_2 Loss: 0.0569 | 0.0412
Epoch 231/300, seasonal_2 Loss: 0.0569 | 0.0411
Epoch 232/300, seasonal_2 Loss: 0.0568 | 0.0410
Epoch 233/300, seasonal_2 Loss: 0.0568 | 0.0410
Epoch 234/300, seasonal_2 Loss: 0.0568 | 0.0411
Epoch 235/300, seasonal_2 Loss: 0.0567 | 0.0411
Epoch 236/300, seasonal_2 Loss: 0.0567 | 0.0413
Epoch 237/300, seasonal_2 Loss: 0.0567 | 0.0414
Epoch 238/300, seasonal_2 Loss: 0.0567 | 0.0416
Epoch 239/300, seasonal_2 Loss: 0.0567 | 0.0418
Epoch 240/300, seasonal_2 Loss: 0.0567 | 0.0418
Epoch 241/300, seasonal_2 Loss: 0.0566 | 0.0418
Epoch 242/300, seasonal_2 Loss: 0.0566 | 0.0417
Epoch 243/300, seasonal_2 Loss: 0.0565 | 0.0416
Epoch 244/300, seasonal_2 Loss: 0.0564 | 0.0414
Epoch 245/300, seasonal_2 Loss: 0.0564 | 0.0412
Epoch 246/300, seasonal_2 Loss: 0.0563 | 0.0410
Epoch 247/300, seasonal_2 Loss: 0.0563 | 0.0410
Epoch 248/300, seasonal_2 Loss: 0.0562 | 0.0409
Epoch 249/300, seasonal_2 Loss: 0.0562 | 0.0409
Epoch 250/300, seasonal_2 Loss: 0.0561 | 0.0410
Epoch 251/300, seasonal_2 Loss: 0.0561 | 0.0410
Epoch 252/300, seasonal_2 Loss: 0.0561 | 0.0411
Epoch 253/300, seasonal_2 Loss: 0.0560 | 0.0412
Epoch 254/300, seasonal_2 Loss: 0.0560 | 0.0413
Epoch 255/300, seasonal_2 Loss: 0.0560 | 0.0413
Epoch 256/300, seasonal_2 Loss: 0.0559 | 0.0414
Epoch 257/300, seasonal_2 Loss: 0.0559 | 0.0414
Epoch 258/300, seasonal_2 Loss: 0.0559 | 0.0413
Epoch 259/300, seasonal_2 Loss: 0.0558 | 0.0413
Epoch 260/300, seasonal_2 Loss: 0.0558 | 0.0412
Epoch 261/300, seasonal_2 Loss: 0.0558 | 0.0411
Epoch 262/300, seasonal_2 Loss: 0.0558 | 0.0411
Epoch 263/300, seasonal_2 Loss: 0.0557 | 0.0411
Epoch 264/300, seasonal_2 Loss: 0.0557 | 0.0411
Epoch 265/300, seasonal_2 Loss: 0.0557 | 0.0411
Epoch 266/300, seasonal_2 Loss: 0.0557 | 0.0411
Epoch 267/300, seasonal_2 Loss: 0.0556 | 0.0412
Epoch 268/300, seasonal_2 Loss: 0.0556 | 0.0412
Epoch 269/300, seasonal_2 Loss: 0.0556 | 0.0412
Epoch 270/300, seasonal_2 Loss: 0.0556 | 0.0412
Epoch 271/300, seasonal_2 Loss: 0.0555 | 0.0412
Epoch 272/300, seasonal_2 Loss: 0.0555 | 0.0412
Epoch 273/300, seasonal_2 Loss: 0.0555 | 0.0412
Epoch 274/300, seasonal_2 Loss: 0.0555 | 0.0412
Epoch 275/300, seasonal_2 Loss: 0.0554 | 0.0411
Epoch 276/300, seasonal_2 Loss: 0.0554 | 0.0411
Epoch 277/300, seasonal_2 Loss: 0.0554 | 0.0411
Epoch 278/300, seasonal_2 Loss: 0.0554 | 0.0411
Epoch 279/300, seasonal_2 Loss: 0.0553 | 0.0411
Epoch 280/300, seasonal_2 Loss: 0.0553 | 0.0411
Epoch 281/300, seasonal_2 Loss: 0.0553 | 0.0411
Epoch 282/300, seasonal_2 Loss: 0.0553 | 0.0411
Epoch 283/300, seasonal_2 Loss: 0.0552 | 0.0411
Epoch 284/300, seasonal_2 Loss: 0.0552 | 0.0411
Epoch 285/300, seasonal_2 Loss: 0.0552 | 0.0411
Epoch 286/300, seasonal_2 Loss: 0.0552 | 0.0410
Epoch 287/300, seasonal_2 Loss: 0.0552 | 0.0410
Epoch 288/300, seasonal_2 Loss: 0.0551 | 0.0410
Epoch 289/300, seasonal_2 Loss: 0.0551 | 0.0410
Epoch 290/300, seasonal_2 Loss: 0.0551 | 0.0410
Epoch 291/300, seasonal_2 Loss: 0.0551 | 0.0410
Epoch 292/300, seasonal_2 Loss: 0.0551 | 0.0410
Epoch 293/300, seasonal_2 Loss: 0.0550 | 0.0410
Epoch 294/300, seasonal_2 Loss: 0.0550 | 0.0410
Epoch 295/300, seasonal_2 Loss: 0.0550 | 0.0410
Epoch 296/300, seasonal_2 Loss: 0.0550 | 0.0410
Epoch 297/300, seasonal_2 Loss: 0.0550 | 0.0410
Epoch 298/300, seasonal_2 Loss: 0.0549 | 0.0410
Epoch 299/300, seasonal_2 Loss: 0.0549 | 0.0410
Epoch 300/300, seasonal_2 Loss: 0.0549 | 0.0410
Training seasonal_3 component with params: {'observation_period_num': 15, 'train_rates': 0.9813458967482234, 'learning_rate': 0.00032350753795972554, 'batch_size': 142, 'step_size': 15, 'gamma': 0.8772190131814118}
Epoch 1/300, seasonal_3 Loss: 0.4480 | 0.2745
Epoch 2/300, seasonal_3 Loss: 0.2033 | 0.1650
Epoch 3/300, seasonal_3 Loss: 0.1906 | 0.1641
Epoch 4/300, seasonal_3 Loss: 0.2511 | 0.3001
Epoch 5/300, seasonal_3 Loss: 0.3062 | 0.7201
Epoch 6/300, seasonal_3 Loss: 0.1977 | 0.1804
Epoch 7/300, seasonal_3 Loss: 0.1429 | 0.1787
Epoch 8/300, seasonal_3 Loss: 0.1271 | 0.1233
Epoch 9/300, seasonal_3 Loss: 0.1239 | 0.1066
Epoch 10/300, seasonal_3 Loss: 0.1210 | 0.0961
Epoch 11/300, seasonal_3 Loss: 0.1164 | 0.0902
Epoch 12/300, seasonal_3 Loss: 0.1147 | 0.0879
Epoch 13/300, seasonal_3 Loss: 0.1121 | 0.0851
Epoch 14/300, seasonal_3 Loss: 0.1101 | 0.0833
Epoch 15/300, seasonal_3 Loss: 0.1121 | 0.0765
Epoch 16/300, seasonal_3 Loss: 0.1184 | 0.0846
Epoch 17/300, seasonal_3 Loss: 0.1207 | 0.0790
Epoch 18/300, seasonal_3 Loss: 0.1145 | 0.0708
Epoch 19/300, seasonal_3 Loss: 0.1060 | 0.0670
Epoch 20/300, seasonal_3 Loss: 0.1011 | 0.0662
Epoch 21/300, seasonal_3 Loss: 0.1041 | 0.0673
Epoch 22/300, seasonal_3 Loss: 0.1318 | 0.1096
Epoch 23/300, seasonal_3 Loss: 0.1435 | 0.1994
Epoch 24/300, seasonal_3 Loss: 0.1257 | 0.1203
Epoch 25/300, seasonal_3 Loss: 0.1112 | 0.0831
Epoch 26/300, seasonal_3 Loss: 0.1044 | 0.0722
Epoch 27/300, seasonal_3 Loss: 0.1212 | 0.0767
Epoch 28/300, seasonal_3 Loss: 0.1110 | 0.0848
Epoch 29/300, seasonal_3 Loss: 0.1006 | 0.0801
Epoch 30/300, seasonal_3 Loss: 0.0913 | 0.0682
Epoch 31/300, seasonal_3 Loss: 0.0915 | 0.0605
Epoch 32/300, seasonal_3 Loss: 0.0971 | 0.0633
Epoch 33/300, seasonal_3 Loss: 0.0914 | 0.0592
Epoch 34/300, seasonal_3 Loss: 0.0854 | 0.0559
Epoch 35/300, seasonal_3 Loss: 0.0831 | 0.0538
Epoch 36/300, seasonal_3 Loss: 0.0821 | 0.0525
Epoch 37/300, seasonal_3 Loss: 0.0818 | 0.0518
Epoch 38/300, seasonal_3 Loss: 0.0813 | 0.0517
Epoch 39/300, seasonal_3 Loss: 0.0805 | 0.0531
Epoch 40/300, seasonal_3 Loss: 0.0797 | 0.0506
Epoch 41/300, seasonal_3 Loss: 0.0798 | 0.0475
Epoch 42/300, seasonal_3 Loss: 0.0823 | 0.0528
Epoch 43/300, seasonal_3 Loss: 0.0874 | 0.0856
Epoch 44/300, seasonal_3 Loss: 0.0865 | 0.0887
Epoch 45/300, seasonal_3 Loss: 0.0800 | 0.0550
Epoch 46/300, seasonal_3 Loss: 0.0765 | 0.0442
Epoch 47/300, seasonal_3 Loss: 0.0805 | 0.0478
Epoch 48/300, seasonal_3 Loss: 0.0797 | 0.0437
Epoch 49/300, seasonal_3 Loss: 0.0774 | 0.0448
Epoch 50/300, seasonal_3 Loss: 0.0773 | 0.0454
Epoch 51/300, seasonal_3 Loss: 0.0761 | 0.0430
Epoch 52/300, seasonal_3 Loss: 0.0744 | 0.0412
Epoch 53/300, seasonal_3 Loss: 0.0737 | 0.0411
Epoch 54/300, seasonal_3 Loss: 0.0738 | 0.0427
Epoch 55/300, seasonal_3 Loss: 0.0739 | 0.0428
Epoch 56/300, seasonal_3 Loss: 0.0730 | 0.0416
Epoch 57/300, seasonal_3 Loss: 0.0717 | 0.0402
Epoch 58/300, seasonal_3 Loss: 0.0713 | 0.0392
Epoch 59/300, seasonal_3 Loss: 0.0715 | 0.0387
Epoch 60/300, seasonal_3 Loss: 0.0711 | 0.0384
Epoch 61/300, seasonal_3 Loss: 0.0705 | 0.0380
Epoch 62/300, seasonal_3 Loss: 0.0705 | 0.0378
Epoch 63/300, seasonal_3 Loss: 0.0707 | 0.0381
Epoch 64/300, seasonal_3 Loss: 0.0712 | 0.0390
Epoch 65/300, seasonal_3 Loss: 0.0722 | 0.0399
Epoch 66/300, seasonal_3 Loss: 0.0734 | 0.0400
Epoch 67/300, seasonal_3 Loss: 0.0734 | 0.0388
Epoch 68/300, seasonal_3 Loss: 0.0712 | 0.0372
Epoch 69/300, seasonal_3 Loss: 0.0713 | 0.0374
Epoch 70/300, seasonal_3 Loss: 0.0761 | 0.0382
Epoch 71/300, seasonal_3 Loss: 0.0790 | 0.0397
Epoch 72/300, seasonal_3 Loss: 0.0755 | 0.0416
Epoch 73/300, seasonal_3 Loss: 0.0715 | 0.0425
Epoch 74/300, seasonal_3 Loss: 0.0720 | 0.0420
Epoch 75/300, seasonal_3 Loss: 0.0718 | 0.0394
Epoch 76/300, seasonal_3 Loss: 0.0695 | 0.0365
Epoch 77/300, seasonal_3 Loss: 0.0680 | 0.0360
Epoch 78/300, seasonal_3 Loss: 0.0675 | 0.0360
Epoch 79/300, seasonal_3 Loss: 0.0673 | 0.0359
Epoch 80/300, seasonal_3 Loss: 0.0668 | 0.0361
Epoch 81/300, seasonal_3 Loss: 0.0664 | 0.0363
Epoch 82/300, seasonal_3 Loss: 0.0660 | 0.0360
Epoch 83/300, seasonal_3 Loss: 0.0657 | 0.0352
Epoch 84/300, seasonal_3 Loss: 0.0654 | 0.0344
Epoch 85/300, seasonal_3 Loss: 0.0652 | 0.0338
Epoch 86/300, seasonal_3 Loss: 0.0650 | 0.0335
Epoch 87/300, seasonal_3 Loss: 0.0648 | 0.0334
Epoch 88/300, seasonal_3 Loss: 0.0647 | 0.0334
Epoch 89/300, seasonal_3 Loss: 0.0646 | 0.0333
Epoch 90/300, seasonal_3 Loss: 0.0645 | 0.0331
Epoch 91/300, seasonal_3 Loss: 0.0644 | 0.0327
Epoch 92/300, seasonal_3 Loss: 0.0643 | 0.0322
Epoch 93/300, seasonal_3 Loss: 0.0640 | 0.0319
Epoch 94/300, seasonal_3 Loss: 0.0638 | 0.0317
Epoch 95/300, seasonal_3 Loss: 0.0636 | 0.0316
Epoch 96/300, seasonal_3 Loss: 0.0635 | 0.0316
Epoch 97/300, seasonal_3 Loss: 0.0634 | 0.0315
Epoch 98/300, seasonal_3 Loss: 0.0634 | 0.0314
Epoch 99/300, seasonal_3 Loss: 0.0634 | 0.0317
Epoch 100/300, seasonal_3 Loss: 0.0634 | 0.0318
Epoch 101/300, seasonal_3 Loss: 0.0633 | 0.0318
Epoch 102/300, seasonal_3 Loss: 0.0632 | 0.0317
Epoch 103/300, seasonal_3 Loss: 0.0629 | 0.0314
Epoch 104/300, seasonal_3 Loss: 0.0627 | 0.0309
Epoch 105/300, seasonal_3 Loss: 0.0626 | 0.0305
Epoch 106/300, seasonal_3 Loss: 0.0627 | 0.0301
Epoch 107/300, seasonal_3 Loss: 0.0632 | 0.0303
Epoch 108/300, seasonal_3 Loss: 0.0637 | 0.0306
Epoch 109/300, seasonal_3 Loss: 0.0637 | 0.0306
Epoch 110/300, seasonal_3 Loss: 0.0632 | 0.0302
Epoch 111/300, seasonal_3 Loss: 0.0626 | 0.0297
Epoch 112/300, seasonal_3 Loss: 0.0623 | 0.0295
Epoch 113/300, seasonal_3 Loss: 0.0623 | 0.0296
Epoch 114/300, seasonal_3 Loss: 0.0625 | 0.0301
Epoch 115/300, seasonal_3 Loss: 0.0625 | 0.0300
Epoch 116/300, seasonal_3 Loss: 0.0619 | 0.0295
Epoch 117/300, seasonal_3 Loss: 0.0615 | 0.0290
Epoch 118/300, seasonal_3 Loss: 0.0614 | 0.0287
Epoch 119/300, seasonal_3 Loss: 0.0615 | 0.0285
Epoch 120/300, seasonal_3 Loss: 0.0614 | 0.0284
Epoch 121/300, seasonal_3 Loss: 0.0613 | 0.0284
Epoch 122/300, seasonal_3 Loss: 0.0611 | 0.0282
Epoch 123/300, seasonal_3 Loss: 0.0609 | 0.0281
Epoch 124/300, seasonal_3 Loss: 0.0609 | 0.0281
Epoch 125/300, seasonal_3 Loss: 0.0609 | 0.0282
Epoch 126/300, seasonal_3 Loss: 0.0608 | 0.0282
Epoch 127/300, seasonal_3 Loss: 0.0606 | 0.0281
Epoch 128/300, seasonal_3 Loss: 0.0605 | 0.0279
Epoch 129/300, seasonal_3 Loss: 0.0604 | 0.0277
Epoch 130/300, seasonal_3 Loss: 0.0604 | 0.0276
Epoch 131/300, seasonal_3 Loss: 0.0603 | 0.0275
Epoch 132/300, seasonal_3 Loss: 0.0603 | 0.0274
Epoch 133/300, seasonal_3 Loss: 0.0602 | 0.0273
Epoch 134/300, seasonal_3 Loss: 0.0601 | 0.0271
Epoch 135/300, seasonal_3 Loss: 0.0600 | 0.0270
Epoch 136/300, seasonal_3 Loss: 0.0599 | 0.0270
Epoch 137/300, seasonal_3 Loss: 0.0599 | 0.0270
Epoch 138/300, seasonal_3 Loss: 0.0598 | 0.0270
Epoch 139/300, seasonal_3 Loss: 0.0597 | 0.0270
Epoch 140/300, seasonal_3 Loss: 0.0597 | 0.0269
Epoch 141/300, seasonal_3 Loss: 0.0596 | 0.0269
Epoch 142/300, seasonal_3 Loss: 0.0595 | 0.0268
Epoch 143/300, seasonal_3 Loss: 0.0595 | 0.0267
Epoch 144/300, seasonal_3 Loss: 0.0594 | 0.0266
Epoch 145/300, seasonal_3 Loss: 0.0594 | 0.0265
Epoch 146/300, seasonal_3 Loss: 0.0593 | 0.0264
Epoch 147/300, seasonal_3 Loss: 0.0592 | 0.0263
Epoch 148/300, seasonal_3 Loss: 0.0592 | 0.0262
Epoch 149/300, seasonal_3 Loss: 0.0591 | 0.0262
Epoch 150/300, seasonal_3 Loss: 0.0591 | 0.0262
Epoch 151/300, seasonal_3 Loss: 0.0590 | 0.0262
Epoch 152/300, seasonal_3 Loss: 0.0590 | 0.0262
Epoch 153/300, seasonal_3 Loss: 0.0589 | 0.0262
Epoch 154/300, seasonal_3 Loss: 0.0589 | 0.0261
Epoch 155/300, seasonal_3 Loss: 0.0588 | 0.0261
Epoch 156/300, seasonal_3 Loss: 0.0588 | 0.0260
Epoch 157/300, seasonal_3 Loss: 0.0588 | 0.0259
Epoch 158/300, seasonal_3 Loss: 0.0587 | 0.0258
Epoch 159/300, seasonal_3 Loss: 0.0586 | 0.0257
Epoch 160/300, seasonal_3 Loss: 0.0586 | 0.0256
Epoch 161/300, seasonal_3 Loss: 0.0585 | 0.0256
Epoch 162/300, seasonal_3 Loss: 0.0585 | 0.0257
Epoch 163/300, seasonal_3 Loss: 0.0585 | 0.0257
Epoch 164/300, seasonal_3 Loss: 0.0584 | 0.0257
Epoch 165/300, seasonal_3 Loss: 0.0583 | 0.0257
Epoch 166/300, seasonal_3 Loss: 0.0583 | 0.0257
Epoch 167/300, seasonal_3 Loss: 0.0583 | 0.0256
Epoch 168/300, seasonal_3 Loss: 0.0582 | 0.0255
Epoch 169/300, seasonal_3 Loss: 0.0582 | 0.0255
Epoch 170/300, seasonal_3 Loss: 0.0581 | 0.0254
Epoch 171/300, seasonal_3 Loss: 0.0581 | 0.0253
Epoch 172/300, seasonal_3 Loss: 0.0580 | 0.0253
Epoch 173/300, seasonal_3 Loss: 0.0580 | 0.0253
Epoch 174/300, seasonal_3 Loss: 0.0580 | 0.0253
Epoch 175/300, seasonal_3 Loss: 0.0579 | 0.0253
Epoch 176/300, seasonal_3 Loss: 0.0579 | 0.0253
Epoch 177/300, seasonal_3 Loss: 0.0579 | 0.0253
Epoch 178/300, seasonal_3 Loss: 0.0578 | 0.0252
Epoch 179/300, seasonal_3 Loss: 0.0578 | 0.0252
Epoch 180/300, seasonal_3 Loss: 0.0578 | 0.0251
Epoch 181/300, seasonal_3 Loss: 0.0577 | 0.0251
Epoch 182/300, seasonal_3 Loss: 0.0577 | 0.0251
Epoch 183/300, seasonal_3 Loss: 0.0577 | 0.0251
Epoch 184/300, seasonal_3 Loss: 0.0576 | 0.0251
Epoch 185/300, seasonal_3 Loss: 0.0576 | 0.0251
Epoch 186/300, seasonal_3 Loss: 0.0576 | 0.0250
Epoch 187/300, seasonal_3 Loss: 0.0575 | 0.0250
Epoch 188/300, seasonal_3 Loss: 0.0575 | 0.0250
Epoch 189/300, seasonal_3 Loss: 0.0575 | 0.0250
Epoch 190/300, seasonal_3 Loss: 0.0574 | 0.0249
Epoch 191/300, seasonal_3 Loss: 0.0574 | 0.0249
Epoch 192/300, seasonal_3 Loss: 0.0574 | 0.0249
Epoch 193/300, seasonal_3 Loss: 0.0573 | 0.0249
Epoch 194/300, seasonal_3 Loss: 0.0573 | 0.0249
Epoch 195/300, seasonal_3 Loss: 0.0573 | 0.0249
Epoch 196/300, seasonal_3 Loss: 0.0573 | 0.0248
Epoch 197/300, seasonal_3 Loss: 0.0572 | 0.0248
Epoch 198/300, seasonal_3 Loss: 0.0572 | 0.0248
Epoch 199/300, seasonal_3 Loss: 0.0572 | 0.0248
Epoch 200/300, seasonal_3 Loss: 0.0572 | 0.0248
Epoch 201/300, seasonal_3 Loss: 0.0571 | 0.0248
Epoch 202/300, seasonal_3 Loss: 0.0571 | 0.0247
Epoch 203/300, seasonal_3 Loss: 0.0571 | 0.0247
Epoch 204/300, seasonal_3 Loss: 0.0571 | 0.0247
Epoch 205/300, seasonal_3 Loss: 0.0570 | 0.0247
Epoch 206/300, seasonal_3 Loss: 0.0570 | 0.0247
Epoch 207/300, seasonal_3 Loss: 0.0570 | 0.0247
Epoch 208/300, seasonal_3 Loss: 0.0570 | 0.0247
Epoch 209/300, seasonal_3 Loss: 0.0569 | 0.0246
Epoch 210/300, seasonal_3 Loss: 0.0569 | 0.0246
Epoch 211/300, seasonal_3 Loss: 0.0569 | 0.0246
Epoch 212/300, seasonal_3 Loss: 0.0569 | 0.0246
Epoch 213/300, seasonal_3 Loss: 0.0569 | 0.0246
Epoch 214/300, seasonal_3 Loss: 0.0568 | 0.0246
Epoch 215/300, seasonal_3 Loss: 0.0568 | 0.0246
Epoch 216/300, seasonal_3 Loss: 0.0568 | 0.0246
Epoch 217/300, seasonal_3 Loss: 0.0568 | 0.0246
Epoch 218/300, seasonal_3 Loss: 0.0568 | 0.0245
Epoch 219/300, seasonal_3 Loss: 0.0567 | 0.0245
Epoch 220/300, seasonal_3 Loss: 0.0567 | 0.0245
Epoch 221/300, seasonal_3 Loss: 0.0567 | 0.0245
Epoch 222/300, seasonal_3 Loss: 0.0567 | 0.0245
Epoch 223/300, seasonal_3 Loss: 0.0567 | 0.0245
Epoch 224/300, seasonal_3 Loss: 0.0566 | 0.0245
Epoch 225/300, seasonal_3 Loss: 0.0566 | 0.0245
Epoch 226/300, seasonal_3 Loss: 0.0566 | 0.0245
Epoch 227/300, seasonal_3 Loss: 0.0566 | 0.0245
Epoch 228/300, seasonal_3 Loss: 0.0566 | 0.0244
Epoch 229/300, seasonal_3 Loss: 0.0566 | 0.0244
Epoch 230/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 231/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 232/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 233/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 234/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 235/300, seasonal_3 Loss: 0.0565 | 0.0244
Epoch 236/300, seasonal_3 Loss: 0.0564 | 0.0244
Epoch 237/300, seasonal_3 Loss: 0.0564 | 0.0244
Epoch 238/300, seasonal_3 Loss: 0.0564 | 0.0244
Epoch 239/300, seasonal_3 Loss: 0.0564 | 0.0244
Epoch 240/300, seasonal_3 Loss: 0.0564 | 0.0243
Epoch 241/300, seasonal_3 Loss: 0.0564 | 0.0243
Epoch 242/300, seasonal_3 Loss: 0.0564 | 0.0243
Epoch 243/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 244/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 245/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 246/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 247/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 248/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 249/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 250/300, seasonal_3 Loss: 0.0563 | 0.0243
Epoch 251/300, seasonal_3 Loss: 0.0562 | 0.0243
Epoch 252/300, seasonal_3 Loss: 0.0562 | 0.0243
Epoch 253/300, seasonal_3 Loss: 0.0562 | 0.0243
Epoch 254/300, seasonal_3 Loss: 0.0562 | 0.0243
Epoch 255/300, seasonal_3 Loss: 0.0562 | 0.0243
Epoch 256/300, seasonal_3 Loss: 0.0562 | 0.0242
Epoch 257/300, seasonal_3 Loss: 0.0562 | 0.0242
Epoch 258/300, seasonal_3 Loss: 0.0562 | 0.0242
Epoch 259/300, seasonal_3 Loss: 0.0562 | 0.0242
Epoch 260/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 261/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 262/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 263/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 264/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 265/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 266/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 267/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 268/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 269/300, seasonal_3 Loss: 0.0561 | 0.0242
Epoch 270/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 271/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 272/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 273/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 274/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 275/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 276/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 277/300, seasonal_3 Loss: 0.0560 | 0.0242
Epoch 278/300, seasonal_3 Loss: 0.0560 | 0.0241
Epoch 279/300, seasonal_3 Loss: 0.0560 | 0.0241
Epoch 280/300, seasonal_3 Loss: 0.0560 | 0.0241
Epoch 281/300, seasonal_3 Loss: 0.0560 | 0.0241
Epoch 282/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 283/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 284/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 285/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 286/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 287/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 288/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 289/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 290/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 291/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 292/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 293/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 294/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 295/300, seasonal_3 Loss: 0.0559 | 0.0241
Epoch 296/300, seasonal_3 Loss: 0.0558 | 0.0241
Epoch 297/300, seasonal_3 Loss: 0.0558 | 0.0241
Epoch 298/300, seasonal_3 Loss: 0.0558 | 0.0241
Epoch 299/300, seasonal_3 Loss: 0.0558 | 0.0241
Epoch 300/300, seasonal_3 Loss: 0.0558 | 0.0241
Training resid component with params: {'observation_period_num': 21, 'train_rates': 0.9864236893422341, 'learning_rate': 0.0001305553500106674, 'batch_size': 25, 'step_size': 7, 'gamma': 0.7938259162142935}
Epoch 1/300, resid Loss: 0.1971 | 0.1252
Epoch 2/300, resid Loss: 0.1280 | 0.0936
Epoch 3/300, resid Loss: 0.1180 | 0.0946
Epoch 4/300, resid Loss: 0.1144 | 0.0843
Epoch 5/300, resid Loss: 0.1033 | 0.0781
Epoch 6/300, resid Loss: 0.0989 | 0.0755
Epoch 7/300, resid Loss: 0.0958 | 0.0736
Epoch 8/300, resid Loss: 0.0926 | 0.0773
Epoch 9/300, resid Loss: 0.0912 | 0.0787
Epoch 10/300, resid Loss: 0.0881 | 0.0773
Epoch 11/300, resid Loss: 0.0858 | 0.0744
Epoch 12/300, resid Loss: 0.0838 | 0.0727
Epoch 13/300, resid Loss: 0.0830 | 0.0737
Epoch 14/300, resid Loss: 0.0812 | 0.0741
Epoch 15/300, resid Loss: 0.0793 | 0.0683
Epoch 16/300, resid Loss: 0.0778 | 0.0696
Epoch 17/300, resid Loss: 0.0764 | 0.0710
Epoch 18/300, resid Loss: 0.0750 | 0.0724
Epoch 19/300, resid Loss: 0.0736 | 0.0674
Epoch 20/300, resid Loss: 0.0724 | 0.0683
Epoch 21/300, resid Loss: 0.0715 | 0.0689
Epoch 22/300, resid Loss: 0.0707 | 0.0640
Epoch 23/300, resid Loss: 0.0700 | 0.0637
Epoch 24/300, resid Loss: 0.0694 | 0.0630
Epoch 25/300, resid Loss: 0.0687 | 0.0622
Epoch 26/300, resid Loss: 0.0682 | 0.0600
Epoch 27/300, resid Loss: 0.0677 | 0.0591
Epoch 28/300, resid Loss: 0.0672 | 0.0580
Epoch 29/300, resid Loss: 0.0668 | 0.0578
Epoch 30/300, resid Loss: 0.0664 | 0.0569
Epoch 31/300, resid Loss: 0.0659 | 0.0560
Epoch 32/300, resid Loss: 0.0655 | 0.0551
Epoch 33/300, resid Loss: 0.0652 | 0.0557
Epoch 34/300, resid Loss: 0.0649 | 0.0549
Epoch 35/300, resid Loss: 0.0646 | 0.0541
Epoch 36/300, resid Loss: 0.0644 | 0.0553
Epoch 37/300, resid Loss: 0.0642 | 0.0544
Epoch 38/300, resid Loss: 0.0639 | 0.0536
Epoch 39/300, resid Loss: 0.0637 | 0.0527
Epoch 40/300, resid Loss: 0.0635 | 0.0527
Epoch 41/300, resid Loss: 0.0633 | 0.0519
Epoch 42/300, resid Loss: 0.0632 | 0.0511
Epoch 43/300, resid Loss: 0.0630 | 0.0494
Epoch 44/300, resid Loss: 0.0629 | 0.0487
Epoch 45/300, resid Loss: 0.0628 | 0.0481
Epoch 46/300, resid Loss: 0.0626 | 0.0475
Epoch 47/300, resid Loss: 0.0625 | 0.0466
Epoch 48/300, resid Loss: 0.0624 | 0.0462
Epoch 49/300, resid Loss: 0.0622 | 0.0460
Epoch 50/300, resid Loss: 0.0621 | 0.0457
Epoch 51/300, resid Loss: 0.0620 | 0.0455
Epoch 52/300, resid Loss: 0.0619 | 0.0452
Epoch 53/300, resid Loss: 0.0618 | 0.0450
Epoch 54/300, resid Loss: 0.0617 | 0.0451
Epoch 55/300, resid Loss: 0.0616 | 0.0448
Epoch 56/300, resid Loss: 0.0615 | 0.0446
Epoch 57/300, resid Loss: 0.0615 | 0.0449
Epoch 58/300, resid Loss: 0.0614 | 0.0447
Epoch 59/300, resid Loss: 0.0613 | 0.0446
Epoch 60/300, resid Loss: 0.0613 | 0.0444
Epoch 61/300, resid Loss: 0.0612 | 0.0446
Epoch 62/300, resid Loss: 0.0612 | 0.0445
Epoch 63/300, resid Loss: 0.0611 | 0.0443
Epoch 64/300, resid Loss: 0.0611 | 0.0445
Epoch 65/300, resid Loss: 0.0610 | 0.0444
Epoch 66/300, resid Loss: 0.0610 | 0.0444
Epoch 67/300, resid Loss: 0.0610 | 0.0443
Epoch 68/300, resid Loss: 0.0609 | 0.0444
Epoch 69/300, resid Loss: 0.0609 | 0.0443
Epoch 70/300, resid Loss: 0.0608 | 0.0442
Epoch 71/300, resid Loss: 0.0608 | 0.0441
Epoch 72/300, resid Loss: 0.0608 | 0.0440
Epoch 73/300, resid Loss: 0.0607 | 0.0439
Epoch 74/300, resid Loss: 0.0607 | 0.0438
Epoch 75/300, resid Loss: 0.0607 | 0.0437
Epoch 76/300, resid Loss: 0.0606 | 0.0436
Epoch 77/300, resid Loss: 0.0606 | 0.0436
Epoch 78/300, resid Loss: 0.0606 | 0.0435
Epoch 79/300, resid Loss: 0.0606 | 0.0435
Epoch 80/300, resid Loss: 0.0606 | 0.0434
Epoch 81/300, resid Loss: 0.0605 | 0.0434
Epoch 82/300, resid Loss: 0.0605 | 0.0433
Epoch 83/300, resid Loss: 0.0605 | 0.0433
Epoch 84/300, resid Loss: 0.0605 | 0.0433
Epoch 85/300, resid Loss: 0.0605 | 0.0433
Epoch 86/300, resid Loss: 0.0604 | 0.0433
Epoch 87/300, resid Loss: 0.0604 | 0.0433
Epoch 88/300, resid Loss: 0.0604 | 0.0432
Epoch 89/300, resid Loss: 0.0604 | 0.0432
Epoch 90/300, resid Loss: 0.0604 | 0.0432
Epoch 91/300, resid Loss: 0.0604 | 0.0432
Epoch 92/300, resid Loss: 0.0604 | 0.0432
Epoch 93/300, resid Loss: 0.0604 | 0.0432
Epoch 94/300, resid Loss: 0.0604 | 0.0432
Epoch 95/300, resid Loss: 0.0604 | 0.0432
Epoch 96/300, resid Loss: 0.0604 | 0.0432
Epoch 97/300, resid Loss: 0.0603 | 0.0432
Epoch 98/300, resid Loss: 0.0603 | 0.0431
Epoch 99/300, resid Loss: 0.0603 | 0.0431
Epoch 100/300, resid Loss: 0.0603 | 0.0431
Epoch 101/300, resid Loss: 0.0603 | 0.0431
Epoch 102/300, resid Loss: 0.0603 | 0.0431
Epoch 103/300, resid Loss: 0.0603 | 0.0431
Epoch 104/300, resid Loss: 0.0603 | 0.0431
Epoch 105/300, resid Loss: 0.0603 | 0.0431
Epoch 106/300, resid Loss: 0.0603 | 0.0431
Epoch 107/300, resid Loss: 0.0603 | 0.0431
Epoch 108/300, resid Loss: 0.0603 | 0.0431
Epoch 109/300, resid Loss: 0.0603 | 0.0431
Epoch 110/300, resid Loss: 0.0603 | 0.0431
Epoch 111/300, resid Loss: 0.0603 | 0.0431
Epoch 112/300, resid Loss: 0.0603 | 0.0431
Epoch 113/300, resid Loss: 0.0603 | 0.0431
Epoch 114/300, resid Loss: 0.0603 | 0.0431
Epoch 115/300, resid Loss: 0.0603 | 0.0431
Epoch 116/300, resid Loss: 0.0603 | 0.0431
Epoch 117/300, resid Loss: 0.0603 | 0.0431
Epoch 118/300, resid Loss: 0.0603 | 0.0431
Epoch 119/300, resid Loss: 0.0603 | 0.0431
Epoch 120/300, resid Loss: 0.0603 | 0.0431
Epoch 121/300, resid Loss: 0.0603 | 0.0431
Epoch 122/300, resid Loss: 0.0603 | 0.0431
Epoch 123/300, resid Loss: 0.0603 | 0.0431
Epoch 124/300, resid Loss: 0.0603 | 0.0431
Epoch 125/300, resid Loss: 0.0603 | 0.0431
Epoch 126/300, resid Loss: 0.0603 | 0.0431
Epoch 127/300, resid Loss: 0.0603 | 0.0431
Epoch 128/300, resid Loss: 0.0603 | 0.0431
Epoch 129/300, resid Loss: 0.0603 | 0.0431
Epoch 130/300, resid Loss: 0.0603 | 0.0431
Epoch 131/300, resid Loss: 0.0603 | 0.0431
Epoch 132/300, resid Loss: 0.0603 | 0.0431
Epoch 133/300, resid Loss: 0.0603 | 0.0431
Epoch 134/300, resid Loss: 0.0603 | 0.0431
Epoch 135/300, resid Loss: 0.0603 | 0.0431
Epoch 136/300, resid Loss: 0.0603 | 0.0431
Epoch 137/300, resid Loss: 0.0603 | 0.0431
Epoch 138/300, resid Loss: 0.0603 | 0.0431
Epoch 139/300, resid Loss: 0.0603 | 0.0431
Epoch 140/300, resid Loss: 0.0603 | 0.0431
Epoch 141/300, resid Loss: 0.0603 | 0.0431
Epoch 142/300, resid Loss: 0.0603 | 0.0431
Epoch 143/300, resid Loss: 0.0603 | 0.0431
Epoch 144/300, resid Loss: 0.0603 | 0.0431
Epoch 145/300, resid Loss: 0.0603 | 0.0431
Epoch 146/300, resid Loss: 0.0603 | 0.0431
Epoch 147/300, resid Loss: 0.0603 | 0.0431
Epoch 148/300, resid Loss: 0.0603 | 0.0431
Epoch 149/300, resid Loss: 0.0603 | 0.0431
Epoch 150/300, resid Loss: 0.0603 | 0.0431
Epoch 151/300, resid Loss: 0.0603 | 0.0431
Epoch 152/300, resid Loss: 0.0603 | 0.0431
Epoch 153/300, resid Loss: 0.0603 | 0.0431
Epoch 154/300, resid Loss: 0.0603 | 0.0431
Epoch 155/300, resid Loss: 0.0603 | 0.0431
Epoch 156/300, resid Loss: 0.0603 | 0.0431
Epoch 157/300, resid Loss: 0.0603 | 0.0431
Epoch 158/300, resid Loss: 0.0603 | 0.0431
Epoch 159/300, resid Loss: 0.0603 | 0.0431
Epoch 160/300, resid Loss: 0.0603 | 0.0431
Epoch 161/300, resid Loss: 0.0603 | 0.0431
Epoch 162/300, resid Loss: 0.0603 | 0.0431
Epoch 163/300, resid Loss: 0.0603 | 0.0431
Epoch 164/300, resid Loss: 0.0603 | 0.0431
Epoch 165/300, resid Loss: 0.0603 | 0.0431
Epoch 166/300, resid Loss: 0.0603 | 0.0431
Epoch 167/300, resid Loss: 0.0603 | 0.0431
Epoch 168/300, resid Loss: 0.0603 | 0.0431
Epoch 169/300, resid Loss: 0.0603 | 0.0431
Epoch 170/300, resid Loss: 0.0603 | 0.0431
Epoch 171/300, resid Loss: 0.0603 | 0.0431
Epoch 172/300, resid Loss: 0.0603 | 0.0431
Epoch 173/300, resid Loss: 0.0603 | 0.0431
Epoch 174/300, resid Loss: 0.0603 | 0.0431
Epoch 175/300, resid Loss: 0.0603 | 0.0431
Epoch 176/300, resid Loss: 0.0603 | 0.0431
Epoch 177/300, resid Loss: 0.0603 | 0.0431
Epoch 178/300, resid Loss: 0.0603 | 0.0431
Epoch 179/300, resid Loss: 0.0603 | 0.0431
Epoch 180/300, resid Loss: 0.0603 | 0.0431
Epoch 181/300, resid Loss: 0.0603 | 0.0431
Epoch 182/300, resid Loss: 0.0603 | 0.0431
Epoch 183/300, resid Loss: 0.0603 | 0.0431
Epoch 184/300, resid Loss: 0.0603 | 0.0431
Epoch 185/300, resid Loss: 0.0603 | 0.0431
Epoch 186/300, resid Loss: 0.0603 | 0.0431
Epoch 187/300, resid Loss: 0.0603 | 0.0431
Epoch 188/300, resid Loss: 0.0603 | 0.0431
Epoch 189/300, resid Loss: 0.0603 | 0.0431
Epoch 190/300, resid Loss: 0.0603 | 0.0431
Epoch 191/300, resid Loss: 0.0603 | 0.0431
Epoch 192/300, resid Loss: 0.0603 | 0.0431
Epoch 193/300, resid Loss: 0.0603 | 0.0431
Epoch 194/300, resid Loss: 0.0603 | 0.0431
Epoch 195/300, resid Loss: 0.0603 | 0.0431
Epoch 196/300, resid Loss: 0.0603 | 0.0431
Epoch 197/300, resid Loss: 0.0603 | 0.0431
Epoch 198/300, resid Loss: 0.0603 | 0.0431
Epoch 199/300, resid Loss: 0.0603 | 0.0431
Epoch 200/300, resid Loss: 0.0603 | 0.0431
Early stopping for resid
Runtime (seconds): 2991.077702522278
9.836743040381044e-05
[155.29828]
[-1.695295]
[-4.715299]
[10.542377]
[2.9232194]
[8.458318]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 10.895861994707957
RMSE: 3.3008880615234375
MAE: 3.3008880615234375
R-squared: nan
[170.81158]
