[32m[I 2025-02-06 05:47:23,520][0m A new study created in memory with name: no-name-bd954cca-c2c9-4114-b1f8-0031848ff46e[0m
[32m[I 2025-02-06 05:47:45,889][0m Trial 0 finished with value: 0.5370771118168589 and parameters: {'observation_period_num': 179, 'train_rates': 0.6618071722877412, 'learning_rate': 3.647964294082257e-05, 'batch_size': 223, 'step_size': 5, 'gamma': 0.754438317838197}. Best is trial 0 with value: 0.5370771118168589.[0m
[32m[I 2025-02-06 05:52:39,170][0m Trial 1 finished with value: 0.30741889980104237 and parameters: {'observation_period_num': 183, 'train_rates': 0.759121189964859, 'learning_rate': 0.0006038370428888706, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9722173806401836}. Best is trial 1 with value: 0.30741889980104237.[0m
[32m[I 2025-02-06 05:53:15,480][0m Trial 2 finished with value: 0.058114446573338266 and parameters: {'observation_period_num': 12, 'train_rates': 0.8412225041284636, 'learning_rate': 0.0005595155225110373, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9503665410331918}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:54:27,061][0m Trial 3 finished with value: 0.7866347479820252 and parameters: {'observation_period_num': 233, 'train_rates': 0.9547224496777552, 'learning_rate': 1.7316853094438315e-06, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9497812144581599}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:54:47,489][0m Trial 4 finished with value: 1.0032254977756325 and parameters: {'observation_period_num': 136, 'train_rates': 0.6255113218391553, 'learning_rate': 4.322984681284454e-06, 'batch_size': 223, 'step_size': 6, 'gamma': 0.8060768269884407}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:55:34,846][0m Trial 5 finished with value: 0.28366523253043774 and parameters: {'observation_period_num': 117, 'train_rates': 0.6026514294592247, 'learning_rate': 0.0004355292554893151, 'batch_size': 95, 'step_size': 5, 'gamma': 0.8235530262185655}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:57:43,812][0m Trial 6 finished with value: 0.8848283781069461 and parameters: {'observation_period_num': 123, 'train_rates': 0.9253481110646151, 'learning_rate': 2.803131576478935e-06, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8143655347136929}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:58:34,267][0m Trial 7 finished with value: 0.898843015485172 and parameters: {'observation_period_num': 85, 'train_rates': 0.8912434020745066, 'learning_rate': 4.07490486595426e-06, 'batch_size': 117, 'step_size': 2, 'gamma': 0.8317700339580182}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 05:59:20,728][0m Trial 8 finished with value: 0.4129692764399167 and parameters: {'observation_period_num': 131, 'train_rates': 0.7604443745844677, 'learning_rate': 9.71218040917894e-05, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8181667549915467}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 06:00:16,893][0m Trial 9 finished with value: 0.17522489259532395 and parameters: {'observation_period_num': 24, 'train_rates': 0.7669522029000235, 'learning_rate': 0.0005284848393838212, 'batch_size': 91, 'step_size': 9, 'gamma': 0.9463101586747158}. Best is trial 2 with value: 0.058114446573338266.[0m
[32m[I 2025-02-06 06:00:51,026][0m Trial 10 finished with value: 0.05393244099357854 and parameters: {'observation_period_num': 17, 'train_rates': 0.8528361600096356, 'learning_rate': 0.00011505411420235067, 'batch_size': 171, 'step_size': 15, 'gamma': 0.9006484151952159}. Best is trial 10 with value: 0.05393244099357854.[0m
[32m[I 2025-02-06 06:01:27,077][0m Trial 11 finished with value: 0.04832672240051912 and parameters: {'observation_period_num': 6, 'train_rates': 0.8560711991989637, 'learning_rate': 0.00013535945265044164, 'batch_size': 172, 'step_size': 15, 'gamma': 0.9015458526353897}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:02:01,192][0m Trial 12 finished with value: 0.13697667239290295 and parameters: {'observation_period_num': 57, 'train_rates': 0.8533683308772113, 'learning_rate': 0.00010627826123634393, 'batch_size': 169, 'step_size': 15, 'gamma': 0.8972983130646746}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:02:33,362][0m Trial 13 finished with value: 0.07173118878769692 and parameters: {'observation_period_num': 52, 'train_rates': 0.8219158614041353, 'learning_rate': 0.00011800327981162526, 'batch_size': 178, 'step_size': 12, 'gamma': 0.8939990268617083}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:02:56,498][0m Trial 14 finished with value: 0.25740053698125626 and parameters: {'observation_period_num': 10, 'train_rates': 0.7111043082077029, 'learning_rate': 1.6294455947932852e-05, 'batch_size': 255, 'step_size': 12, 'gamma': 0.8760186122026777}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:03:40,025][0m Trial 15 finished with value: 0.0822758355310985 and parameters: {'observation_period_num': 53, 'train_rates': 0.897303933744791, 'learning_rate': 3.2715126299321256e-05, 'batch_size': 142, 'step_size': 12, 'gamma': 0.9148000590034921}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:04:10,442][0m Trial 16 finished with value: 0.10834603024130109 and parameters: {'observation_period_num': 84, 'train_rates': 0.8771194679222598, 'learning_rate': 0.00023182862823981423, 'batch_size': 207, 'step_size': 14, 'gamma': 0.8565273396813898}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:04:53,707][0m Trial 17 finished with value: 0.1941959261894226 and parameters: {'observation_period_num': 36, 'train_rates': 0.9693181609589762, 'learning_rate': 1.3703833264071535e-05, 'batch_size': 143, 'step_size': 13, 'gamma': 0.8622122749425327}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:05:23,105][0m Trial 18 finished with value: 0.09694083474956204 and parameters: {'observation_period_num': 82, 'train_rates': 0.8039041280983134, 'learning_rate': 0.00018739531431824648, 'batch_size': 194, 'step_size': 11, 'gamma': 0.9242374183287136}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:05:48,383][0m Trial 19 finished with value: 0.3280014395713806 and parameters: {'observation_period_num': 247, 'train_rates': 0.9220102728736665, 'learning_rate': 5.49195474529726e-05, 'batch_size': 249, 'step_size': 8, 'gamma': 0.9789885452637794}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:06:24,517][0m Trial 20 finished with value: 0.22208716918861396 and parameters: {'observation_period_num': 5, 'train_rates': 0.6939317892249397, 'learning_rate': 1.2830586396703148e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.7809004001832468}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:06:58,854][0m Trial 21 finished with value: 0.08234853025325044 and parameters: {'observation_period_num': 30, 'train_rates': 0.8305851972137667, 'learning_rate': 0.000862779503209344, 'batch_size': 168, 'step_size': 15, 'gamma': 0.9320994462973675}. Best is trial 11 with value: 0.04832672240051912.[0m
[32m[I 2025-02-06 06:07:30,601][0m Trial 22 finished with value: 0.0403478326250429 and parameters: {'observation_period_num': 5, 'train_rates': 0.8530454608328291, 'learning_rate': 0.0002698649998099319, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8959340357633324}. Best is trial 22 with value: 0.0403478326250429.[0m
[32m[I 2025-02-06 06:07:59,259][0m Trial 23 finished with value: 0.09737105489169297 and parameters: {'observation_period_num': 37, 'train_rates': 0.8710828561159657, 'learning_rate': 0.00020248625829797756, 'batch_size': 203, 'step_size': 14, 'gamma': 0.8940176843536949}. Best is trial 22 with value: 0.0403478326250429.[0m
[32m[I 2025-02-06 06:08:29,907][0m Trial 24 finished with value: 0.2417381023460964 and parameters: {'observation_period_num': 69, 'train_rates': 0.7820036227972406, 'learning_rate': 5.778879347487107e-05, 'batch_size': 188, 'step_size': 13, 'gamma': 0.8448747470298286}. Best is trial 22 with value: 0.0403478326250429.[0m
[32m[I 2025-02-06 06:09:07,086][0m Trial 25 finished with value: 0.03688524563766466 and parameters: {'observation_period_num': 6, 'train_rates': 0.8041100318389616, 'learning_rate': 0.00028862448616711613, 'batch_size': 156, 'step_size': 14, 'gamma': 0.8777837417824021}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:09:41,200][0m Trial 26 finished with value: 0.25570105210427313 and parameters: {'observation_period_num': 100, 'train_rates': 0.7311596495975277, 'learning_rate': 0.0002948519767388799, 'batch_size': 152, 'step_size': 13, 'gamma': 0.8745540612127458}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:10:31,726][0m Trial 27 finished with value: 0.0784025407672926 and parameters: {'observation_period_num': 40, 'train_rates': 0.9188853182335981, 'learning_rate': 0.0003009618563017218, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8795657109729224}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:10:57,314][0m Trial 28 finished with value: 0.17728723577507163 and parameters: {'observation_period_num': 164, 'train_rates': 0.8044187483137728, 'learning_rate': 0.0009783382130455541, 'batch_size': 223, 'step_size': 11, 'gamma': 0.8483258110487943}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:11:19,038][0m Trial 29 finished with value: 0.33929023076150866 and parameters: {'observation_period_num': 204, 'train_rates': 0.7339422986184798, 'learning_rate': 6.982430040980062e-05, 'batch_size': 235, 'step_size': 7, 'gamma': 0.9109768437759159}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:11:50,126][0m Trial 30 finished with value: 0.07939716329927131 and parameters: {'observation_period_num': 66, 'train_rates': 0.8069418857184518, 'learning_rate': 0.00015150843093218025, 'batch_size': 185, 'step_size': 11, 'gamma': 0.7943164791981231}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:12:26,363][0m Trial 31 finished with value: 0.07439705988635188 and parameters: {'observation_period_num': 22, 'train_rates': 0.8526463252337986, 'learning_rate': 0.00031884528948543346, 'batch_size': 160, 'step_size': 15, 'gamma': 0.9044266329570552}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:13:11,017][0m Trial 32 finished with value: 0.07376268960754662 and parameters: {'observation_period_num': 18, 'train_rates': 0.8626595592202911, 'learning_rate': 2.187385678088517e-05, 'batch_size': 130, 'step_size': 14, 'gamma': 0.8891478695623353}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:13:38,812][0m Trial 33 finished with value: 0.059770855418231225 and parameters: {'observation_period_num': 10, 'train_rates': 0.8301593522709828, 'learning_rate': 7.667413655274499e-05, 'batch_size': 207, 'step_size': 15, 'gamma': 0.9280185026306151}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:14:12,227][0m Trial 34 finished with value: 0.08466847831874795 and parameters: {'observation_period_num': 43, 'train_rates': 0.8957985592579086, 'learning_rate': 0.00013760353750170488, 'batch_size': 179, 'step_size': 13, 'gamma': 0.9641838466811407}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:14:47,152][0m Trial 35 finished with value: 0.1843809349242077 and parameters: {'observation_period_num': 5, 'train_rates': 0.7815130644694651, 'learning_rate': 4.668655789948611e-05, 'batch_size': 158, 'step_size': 14, 'gamma': 0.9386011943025431}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:15:35,155][0m Trial 36 finished with value: 0.05068071610454855 and parameters: {'observation_period_num': 23, 'train_rates': 0.9512835102678923, 'learning_rate': 0.00039029999085308544, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9149988926867945}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:17:10,600][0m Trial 37 finished with value: 0.06673486466264592 and parameters: {'observation_period_num': 29, 'train_rates': 0.9396672476291605, 'learning_rate': 0.00043556447149473344, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9640634714038735}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:17:57,860][0m Trial 38 finished with value: 0.0742977038025856 and parameters: {'observation_period_num': 47, 'train_rates': 0.9566896556937459, 'learning_rate': 0.0006150279663287105, 'batch_size': 130, 'step_size': 10, 'gamma': 0.9174966746184651}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:18:53,202][0m Trial 39 finished with value: 0.11056240063247293 and parameters: {'observation_period_num': 105, 'train_rates': 0.9103795563495607, 'learning_rate': 0.0006557503693019838, 'batch_size': 105, 'step_size': 5, 'gamma': 0.8642034363892683}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:20:18,654][0m Trial 40 finished with value: 0.1743943938394872 and parameters: {'observation_period_num': 150, 'train_rates': 0.9712570338395107, 'learning_rate': 0.0003867933777760508, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8830712916168443}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:20:52,320][0m Trial 41 finished with value: 0.04288541557020452 and parameters: {'observation_period_num': 20, 'train_rates': 0.8319967056116515, 'learning_rate': 0.00023209261398575342, 'batch_size': 174, 'step_size': 15, 'gamma': 0.9058239432449008}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:21:28,683][0m Trial 42 finished with value: 0.0529729550654404 and parameters: {'observation_period_num': 29, 'train_rates': 0.8234193155273846, 'learning_rate': 0.0002467942816138823, 'batch_size': 152, 'step_size': 14, 'gamma': 0.9122463924052198}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:21:59,541][0m Trial 43 finished with value: 0.04996618566413721 and parameters: {'observation_period_num': 17, 'train_rates': 0.8807767680287913, 'learning_rate': 0.00018715053223332783, 'batch_size': 199, 'step_size': 15, 'gamma': 0.9471974386711676}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:22:30,794][0m Trial 44 finished with value: 0.08317977374153478 and parameters: {'observation_period_num': 18, 'train_rates': 0.8822169262109847, 'learning_rate': 0.0001786895630671184, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9899624347795442}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:22:59,117][0m Trial 45 finished with value: 0.057345521555322666 and parameters: {'observation_period_num': 6, 'train_rates': 0.8455028827935237, 'learning_rate': 7.956110634038774e-05, 'batch_size': 221, 'step_size': 14, 'gamma': 0.8848733975791926}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:23:31,466][0m Trial 46 finished with value: 0.11395845059063528 and parameters: {'observation_period_num': 65, 'train_rates': 0.8154231962557275, 'learning_rate': 0.0001290312413245294, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9444257956316389}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:23:57,817][0m Trial 47 finished with value: 0.36268750579313064 and parameters: {'observation_period_num': 16, 'train_rates': 0.781402270723121, 'learning_rate': 8.403527205591179e-06, 'batch_size': 217, 'step_size': 15, 'gamma': 0.8304565276968439}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:24:18,650][0m Trial 48 finished with value: 0.15624292891959102 and parameters: {'observation_period_num': 37, 'train_rates': 0.6439828641439728, 'learning_rate': 0.0005213490220233061, 'batch_size': 241, 'step_size': 1, 'gamma': 0.9585844523117806}. Best is trial 25 with value: 0.03688524563766466.[0m
[32m[I 2025-02-06 06:24:49,414][0m Trial 49 finished with value: 0.19439216590839584 and parameters: {'observation_period_num': 192, 'train_rates': 0.8386192019277932, 'learning_rate': 0.0002579616776151731, 'batch_size': 185, 'step_size': 13, 'gamma': 0.9040287701079404}. Best is trial 25 with value: 0.03688524563766466.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4109 | 0.2582
Epoch 2/300, Loss: 0.2160 | 0.1997
Epoch 3/300, Loss: 0.1793 | 0.2306
Epoch 4/300, Loss: 0.1581 | 0.1369
Epoch 5/300, Loss: 0.1462 | 0.1234
Epoch 6/300, Loss: 0.1348 | 0.1000
Epoch 7/300, Loss: 0.1265 | 0.0991
Epoch 8/300, Loss: 0.1233 | 0.0843
Epoch 9/300, Loss: 0.1188 | 0.0837
Epoch 10/300, Loss: 0.1174 | 0.0935
Epoch 11/300, Loss: 0.1143 | 0.0765
Epoch 12/300, Loss: 0.1114 | 0.0726
Epoch 13/300, Loss: 0.1118 | 0.0802
Epoch 14/300, Loss: 0.1110 | 0.0995
Epoch 15/300, Loss: 0.1088 | 0.0802
Epoch 16/300, Loss: 0.1104 | 0.0925
Epoch 17/300, Loss: 0.1443 | 0.1077
Epoch 18/300, Loss: 0.1629 | 0.0804
Epoch 19/300, Loss: 0.1485 | 0.0769
Epoch 20/300, Loss: 0.1269 | 0.0742
Epoch 21/300, Loss: 0.1304 | 0.1415
Epoch 22/300, Loss: 0.1312 | 0.0658
Epoch 23/300, Loss: 0.1256 | 0.1027
Epoch 24/300, Loss: 0.1492 | 0.0819
Epoch 25/300, Loss: 0.1250 | 0.0907
Epoch 26/300, Loss: 0.1182 | 0.0770
Epoch 27/300, Loss: 0.1261 | 0.0687
Epoch 28/300, Loss: 0.1116 | 0.0715
Epoch 29/300, Loss: 0.1159 | 0.0848
Epoch 30/300, Loss: 0.1187 | 0.0845
Epoch 31/300, Loss: 0.1081 | 0.0656
Epoch 32/300, Loss: 0.1042 | 0.0674
Epoch 33/300, Loss: 0.1025 | 0.0648
Epoch 34/300, Loss: 0.1030 | 0.0651
Epoch 35/300, Loss: 0.0985 | 0.0606
Epoch 36/300, Loss: 0.0977 | 0.0623
Epoch 37/300, Loss: 0.0962 | 0.0583
Epoch 38/300, Loss: 0.0962 | 0.0597
Epoch 39/300, Loss: 0.0943 | 0.0573
Epoch 40/300, Loss: 0.0941 | 0.0578
Epoch 41/300, Loss: 0.0928 | 0.0560
Epoch 42/300, Loss: 0.0921 | 0.0557
Epoch 43/300, Loss: 0.0907 | 0.0539
Epoch 44/300, Loss: 0.0901 | 0.0546
Epoch 45/300, Loss: 0.0893 | 0.0529
Epoch 46/300, Loss: 0.0888 | 0.0530
Epoch 47/300, Loss: 0.0881 | 0.0520
Epoch 48/300, Loss: 0.0877 | 0.0519
Epoch 49/300, Loss: 0.0872 | 0.0514
Epoch 50/300, Loss: 0.0867 | 0.0518
Epoch 51/300, Loss: 0.0864 | 0.0500
Epoch 52/300, Loss: 0.0861 | 0.0510
Epoch 53/300, Loss: 0.0860 | 0.0497
Epoch 54/300, Loss: 0.0856 | 0.0503
Epoch 55/300, Loss: 0.0855 | 0.0492
Epoch 56/300, Loss: 0.0851 | 0.0494
Epoch 57/300, Loss: 0.0848 | 0.0479
Epoch 58/300, Loss: 0.0845 | 0.0494
Epoch 59/300, Loss: 0.0845 | 0.0474
Epoch 60/300, Loss: 0.0842 | 0.0489
Epoch 61/300, Loss: 0.0844 | 0.0473
Epoch 62/300, Loss: 0.0841 | 0.0485
Epoch 63/300, Loss: 0.0849 | 0.0473
Epoch 64/300, Loss: 0.0842 | 0.0491
Epoch 65/300, Loss: 0.0852 | 0.0468
Epoch 66/300, Loss: 0.0849 | 0.0481
Epoch 67/300, Loss: 0.0850 | 0.0460
Epoch 68/300, Loss: 0.0829 | 0.0469
Epoch 69/300, Loss: 0.0834 | 0.0455
Epoch 70/300, Loss: 0.0829 | 0.0467
Epoch 71/300, Loss: 0.0829 | 0.0446
Epoch 72/300, Loss: 0.0824 | 0.0471
Epoch 73/300, Loss: 0.0810 | 0.0447
Epoch 74/300, Loss: 0.0809 | 0.0466
Epoch 75/300, Loss: 0.0810 | 0.0444
Epoch 76/300, Loss: 0.0803 | 0.0456
Epoch 77/300, Loss: 0.0804 | 0.0438
Epoch 78/300, Loss: 0.0796 | 0.0449
Epoch 79/300, Loss: 0.0796 | 0.0429
Epoch 80/300, Loss: 0.0790 | 0.0442
Epoch 81/300, Loss: 0.0790 | 0.0427
Epoch 82/300, Loss: 0.0788 | 0.0437
Epoch 83/300, Loss: 0.0786 | 0.0424
Epoch 84/300, Loss: 0.0784 | 0.0432
Epoch 85/300, Loss: 0.0781 | 0.0422
Epoch 86/300, Loss: 0.0778 | 0.0431
Epoch 87/300, Loss: 0.0776 | 0.0423
Epoch 88/300, Loss: 0.0775 | 0.0430
Epoch 89/300, Loss: 0.0776 | 0.0424
Epoch 90/300, Loss: 0.0776 | 0.0429
Epoch 91/300, Loss: 0.0779 | 0.0422
Epoch 92/300, Loss: 0.0778 | 0.0422
Epoch 93/300, Loss: 0.0779 | 0.0411
Epoch 94/300, Loss: 0.0772 | 0.0413
Epoch 95/300, Loss: 0.0771 | 0.0410
Epoch 96/300, Loss: 0.0772 | 0.0413
Epoch 97/300, Loss: 0.0773 | 0.0409
Epoch 98/300, Loss: 0.0769 | 0.0412
Epoch 99/300, Loss: 0.0762 | 0.0411
Epoch 100/300, Loss: 0.0759 | 0.0412
Epoch 101/300, Loss: 0.0759 | 0.0407
Epoch 102/300, Loss: 0.0757 | 0.0406
Epoch 103/300, Loss: 0.0756 | 0.0402
Epoch 104/300, Loss: 0.0754 | 0.0402
Epoch 105/300, Loss: 0.0753 | 0.0399
Epoch 106/300, Loss: 0.0752 | 0.0400
Epoch 107/300, Loss: 0.0751 | 0.0399
Epoch 108/300, Loss: 0.0749 | 0.0399
Epoch 109/300, Loss: 0.0748 | 0.0397
Epoch 110/300, Loss: 0.0747 | 0.0397
Epoch 111/300, Loss: 0.0746 | 0.0395
Epoch 112/300, Loss: 0.0745 | 0.0394
Epoch 113/300, Loss: 0.0744 | 0.0392
Epoch 114/300, Loss: 0.0743 | 0.0391
Epoch 115/300, Loss: 0.0742 | 0.0390
Epoch 116/300, Loss: 0.0742 | 0.0389
Epoch 117/300, Loss: 0.0741 | 0.0388
Epoch 118/300, Loss: 0.0740 | 0.0388
Epoch 119/300, Loss: 0.0739 | 0.0387
Epoch 120/300, Loss: 0.0738 | 0.0387
Epoch 121/300, Loss: 0.0737 | 0.0386
Epoch 122/300, Loss: 0.0737 | 0.0386
Epoch 123/300, Loss: 0.0736 | 0.0385
Epoch 124/300, Loss: 0.0735 | 0.0384
Epoch 125/300, Loss: 0.0734 | 0.0383
Epoch 126/300, Loss: 0.0733 | 0.0382
Epoch 127/300, Loss: 0.0733 | 0.0381
Epoch 128/300, Loss: 0.0732 | 0.0380
Epoch 129/300, Loss: 0.0731 | 0.0380
Epoch 130/300, Loss: 0.0731 | 0.0379
Epoch 131/300, Loss: 0.0730 | 0.0378
Epoch 132/300, Loss: 0.0730 | 0.0378
Epoch 133/300, Loss: 0.0729 | 0.0378
Epoch 134/300, Loss: 0.0729 | 0.0378
Epoch 135/300, Loss: 0.0728 | 0.0378
Epoch 136/300, Loss: 0.0727 | 0.0378
Epoch 137/300, Loss: 0.0726 | 0.0377
Epoch 138/300, Loss: 0.0726 | 0.0377
Epoch 139/300, Loss: 0.0725 | 0.0376
Epoch 140/300, Loss: 0.0724 | 0.0375
Epoch 141/300, Loss: 0.0724 | 0.0374
Epoch 142/300, Loss: 0.0723 | 0.0373
Epoch 143/300, Loss: 0.0722 | 0.0372
Epoch 144/300, Loss: 0.0722 | 0.0371
Epoch 145/300, Loss: 0.0722 | 0.0371
Epoch 146/300, Loss: 0.0722 | 0.0371
Epoch 147/300, Loss: 0.0721 | 0.0371
Epoch 148/300, Loss: 0.0721 | 0.0371
Epoch 149/300, Loss: 0.0720 | 0.0372
Epoch 150/300, Loss: 0.0719 | 0.0373
Epoch 151/300, Loss: 0.0718 | 0.0372
Epoch 152/300, Loss: 0.0718 | 0.0371
Epoch 153/300, Loss: 0.0717 | 0.0370
Epoch 154/300, Loss: 0.0717 | 0.0369
Epoch 155/300, Loss: 0.0716 | 0.0368
Epoch 156/300, Loss: 0.0715 | 0.0367
Epoch 157/300, Loss: 0.0715 | 0.0367
Epoch 158/300, Loss: 0.0715 | 0.0367
Epoch 159/300, Loss: 0.0715 | 0.0367
Epoch 160/300, Loss: 0.0714 | 0.0367
Epoch 161/300, Loss: 0.0713 | 0.0367
Epoch 162/300, Loss: 0.0713 | 0.0368
Epoch 163/300, Loss: 0.0712 | 0.0367
Epoch 164/300, Loss: 0.0712 | 0.0367
Epoch 165/300, Loss: 0.0711 | 0.0366
Epoch 166/300, Loss: 0.0711 | 0.0365
Epoch 167/300, Loss: 0.0710 | 0.0365
Epoch 168/300, Loss: 0.0710 | 0.0364
Epoch 169/300, Loss: 0.0710 | 0.0365
Epoch 170/300, Loss: 0.0709 | 0.0365
Epoch 171/300, Loss: 0.0709 | 0.0365
Epoch 172/300, Loss: 0.0708 | 0.0364
Epoch 173/300, Loss: 0.0708 | 0.0364
Epoch 174/300, Loss: 0.0708 | 0.0364
Epoch 175/300, Loss: 0.0707 | 0.0363
Epoch 176/300, Loss: 0.0707 | 0.0363
Epoch 177/300, Loss: 0.0706 | 0.0363
Epoch 178/300, Loss: 0.0706 | 0.0363
Epoch 179/300, Loss: 0.0706 | 0.0363
Epoch 180/300, Loss: 0.0706 | 0.0363
Epoch 181/300, Loss: 0.0705 | 0.0362
Epoch 182/300, Loss: 0.0705 | 0.0362
Epoch 183/300, Loss: 0.0705 | 0.0362
Epoch 184/300, Loss: 0.0704 | 0.0362
Epoch 185/300, Loss: 0.0704 | 0.0362
Epoch 186/300, Loss: 0.0704 | 0.0362
Epoch 187/300, Loss: 0.0703 | 0.0361
Epoch 188/300, Loss: 0.0703 | 0.0361
Epoch 189/300, Loss: 0.0703 | 0.0361
Epoch 190/300, Loss: 0.0703 | 0.0361
Epoch 191/300, Loss: 0.0702 | 0.0361
Epoch 192/300, Loss: 0.0702 | 0.0361
Epoch 193/300, Loss: 0.0702 | 0.0361
Epoch 194/300, Loss: 0.0702 | 0.0360
Epoch 195/300, Loss: 0.0701 | 0.0360
Epoch 196/300, Loss: 0.0701 | 0.0360
Epoch 197/300, Loss: 0.0701 | 0.0360
Epoch 198/300, Loss: 0.0701 | 0.0360
Epoch 199/300, Loss: 0.0700 | 0.0360
Epoch 200/300, Loss: 0.0700 | 0.0360
Epoch 201/300, Loss: 0.0700 | 0.0359
Epoch 202/300, Loss: 0.0700 | 0.0359
Epoch 203/300, Loss: 0.0700 | 0.0359
Epoch 204/300, Loss: 0.0699 | 0.0359
Epoch 205/300, Loss: 0.0699 | 0.0359
Epoch 206/300, Loss: 0.0699 | 0.0359
Epoch 207/300, Loss: 0.0699 | 0.0359
Epoch 208/300, Loss: 0.0699 | 0.0359
Epoch 209/300, Loss: 0.0699 | 0.0358
Epoch 210/300, Loss: 0.0698 | 0.0358
Epoch 211/300, Loss: 0.0698 | 0.0358
Epoch 212/300, Loss: 0.0698 | 0.0358
Epoch 213/300, Loss: 0.0698 | 0.0358
Epoch 214/300, Loss: 0.0698 | 0.0358
Epoch 215/300, Loss: 0.0697 | 0.0358
Epoch 216/300, Loss: 0.0697 | 0.0358
Epoch 217/300, Loss: 0.0697 | 0.0358
Epoch 218/300, Loss: 0.0697 | 0.0358
Epoch 219/300, Loss: 0.0697 | 0.0357
Epoch 220/300, Loss: 0.0697 | 0.0357
Epoch 221/300, Loss: 0.0697 | 0.0357
Epoch 222/300, Loss: 0.0696 | 0.0357
Epoch 223/300, Loss: 0.0696 | 0.0357
Epoch 224/300, Loss: 0.0696 | 0.0357
Epoch 225/300, Loss: 0.0696 | 0.0357
Epoch 226/300, Loss: 0.0696 | 0.0357
Epoch 227/300, Loss: 0.0696 | 0.0357
Epoch 228/300, Loss: 0.0696 | 0.0357
Epoch 229/300, Loss: 0.0695 | 0.0357
Epoch 230/300, Loss: 0.0695 | 0.0356
Epoch 231/300, Loss: 0.0695 | 0.0356
Epoch 232/300, Loss: 0.0695 | 0.0356
Epoch 233/300, Loss: 0.0695 | 0.0356
Epoch 234/300, Loss: 0.0695 | 0.0356
Epoch 235/300, Loss: 0.0695 | 0.0356
Epoch 236/300, Loss: 0.0695 | 0.0356
Epoch 237/300, Loss: 0.0695 | 0.0356
Epoch 238/300, Loss: 0.0694 | 0.0356
Epoch 239/300, Loss: 0.0694 | 0.0356
Epoch 240/300, Loss: 0.0694 | 0.0356
Epoch 241/300, Loss: 0.0694 | 0.0356
Epoch 242/300, Loss: 0.0694 | 0.0356
Epoch 243/300, Loss: 0.0694 | 0.0355
Epoch 244/300, Loss: 0.0694 | 0.0355
Epoch 245/300, Loss: 0.0694 | 0.0355
Epoch 246/300, Loss: 0.0694 | 0.0355
Epoch 247/300, Loss: 0.0694 | 0.0355
Epoch 248/300, Loss: 0.0693 | 0.0355
Epoch 249/300, Loss: 0.0693 | 0.0355
Epoch 250/300, Loss: 0.0693 | 0.0355
Epoch 251/300, Loss: 0.0693 | 0.0355
Epoch 252/300, Loss: 0.0693 | 0.0355
Epoch 253/300, Loss: 0.0693 | 0.0355
Epoch 254/300, Loss: 0.0693 | 0.0355
Epoch 255/300, Loss: 0.0693 | 0.0355
Epoch 256/300, Loss: 0.0693 | 0.0355
Epoch 257/300, Loss: 0.0693 | 0.0355
Epoch 258/300, Loss: 0.0693 | 0.0355
Epoch 259/300, Loss: 0.0693 | 0.0354
Epoch 260/300, Loss: 0.0692 | 0.0354
Epoch 261/300, Loss: 0.0692 | 0.0354
Epoch 262/300, Loss: 0.0692 | 0.0354
Epoch 263/300, Loss: 0.0692 | 0.0354
Epoch 264/300, Loss: 0.0692 | 0.0354
Epoch 265/300, Loss: 0.0692 | 0.0354
Epoch 266/300, Loss: 0.0692 | 0.0354
Epoch 267/300, Loss: 0.0692 | 0.0354
Epoch 268/300, Loss: 0.0692 | 0.0354
Epoch 269/300, Loss: 0.0692 | 0.0354
Epoch 270/300, Loss: 0.0692 | 0.0354
Epoch 271/300, Loss: 0.0692 | 0.0354
Epoch 272/300, Loss: 0.0692 | 0.0354
Epoch 273/300, Loss: 0.0692 | 0.0354
Epoch 274/300, Loss: 0.0691 | 0.0354
Epoch 275/300, Loss: 0.0691 | 0.0354
Epoch 276/300, Loss: 0.0691 | 0.0354
Epoch 277/300, Loss: 0.0691 | 0.0354
Epoch 278/300, Loss: 0.0691 | 0.0354
Epoch 279/300, Loss: 0.0691 | 0.0354
Epoch 280/300, Loss: 0.0691 | 0.0353
Epoch 281/300, Loss: 0.0691 | 0.0353
Epoch 282/300, Loss: 0.0691 | 0.0353
Epoch 283/300, Loss: 0.0691 | 0.0353
Epoch 284/300, Loss: 0.0691 | 0.0353
Epoch 285/300, Loss: 0.0691 | 0.0353
Epoch 286/300, Loss: 0.0691 | 0.0353
Epoch 287/300, Loss: 0.0691 | 0.0353
Epoch 288/300, Loss: 0.0691 | 0.0353
Epoch 289/300, Loss: 0.0691 | 0.0353
Epoch 290/300, Loss: 0.0691 | 0.0353
Epoch 291/300, Loss: 0.0691 | 0.0353
Epoch 292/300, Loss: 0.0691 | 0.0353
Epoch 293/300, Loss: 0.0691 | 0.0353
Epoch 294/300, Loss: 0.0691 | 0.0353
Epoch 295/300, Loss: 0.0690 | 0.0353
Epoch 296/300, Loss: 0.0690 | 0.0353
Epoch 297/300, Loss: 0.0690 | 0.0353
Epoch 298/300, Loss: 0.0690 | 0.0353
Epoch 299/300, Loss: 0.0690 | 0.0353
Epoch 300/300, Loss: 0.0690 | 0.0353
Runtime (seconds): 108.6747179031372
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 14.042022535810247
RMSE: 3.7472686767578125
MAE: 3.7472686767578125
R-squared: nan
[188.54272]
