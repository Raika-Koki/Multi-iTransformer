ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 23:26:15,390][0m A new study created in memory with name: no-name-f64cfc03-ad5d-40b4-96ab-ecfaa967f2fa[0m
[32m[I 2025-02-06 23:28:58,668][0m Trial 0 finished with value: 0.7271037984755822 and parameters: {'observation_period_num': 28, 'train_rates': 0.6937307737795703, 'learning_rate': 7.076593471279933e-05, 'batch_size': 23, 'step_size': 4, 'gamma': 0.7690751432328103}. Best is trial 0 with value: 0.7271037984755822.[0m
[32m[I 2025-02-06 23:32:01,474][0m Trial 1 finished with value: 0.30365129714495287 and parameters: {'observation_period_num': 156, 'train_rates': 0.9191689928665377, 'learning_rate': 0.00011968044661841008, 'batch_size': 67, 'step_size': 3, 'gamma': 0.8827422605957905}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:34:29,753][0m Trial 2 finished with value: 0.41381505131721497 and parameters: {'observation_period_num': 132, 'train_rates': 0.9547201017031617, 'learning_rate': 0.0005560479020636152, 'batch_size': 245, 'step_size': 3, 'gamma': 0.8501152882616114}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:39:18,679][0m Trial 3 finished with value: 0.5065804706063382 and parameters: {'observation_period_num': 228, 'train_rates': 0.9370900653832164, 'learning_rate': 0.0006179905073636175, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9061781818257082}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:43:14,575][0m Trial 4 finished with value: 1.9134032598563604 and parameters: {'observation_period_num': 240, 'train_rates': 0.664759458478432, 'learning_rate': 2.718329938282583e-06, 'batch_size': 65, 'step_size': 2, 'gamma': 0.896632541732963}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:46:02,601][0m Trial 5 finished with value: 1.497596246715883 and parameters: {'observation_period_num': 191, 'train_rates': 0.6030738696200615, 'learning_rate': 2.0616696746805695e-05, 'batch_size': 154, 'step_size': 4, 'gamma': 0.8357035958844221}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:46:56,152][0m Trial 6 finished with value: 1.5535022020339966 and parameters: {'observation_period_num': 54, 'train_rates': 0.9379379426585253, 'learning_rate': 2.9843299731233485e-06, 'batch_size': 233, 'step_size': 11, 'gamma': 0.8535310701148134}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:48:46,263][0m Trial 7 finished with value: 0.4722335803158143 and parameters: {'observation_period_num': 107, 'train_rates': 0.856978413967804, 'learning_rate': 5.580520132784969e-05, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8170607736098}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:52:16,663][0m Trial 8 finished with value: 1.8594800171509287 and parameters: {'observation_period_num': 201, 'train_rates': 0.7222708189094891, 'learning_rate': 1.0897624820144785e-06, 'batch_size': 82, 'step_size': 9, 'gamma': 0.881972427766839}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:54:02,719][0m Trial 9 finished with value: 0.4643873870372772 and parameters: {'observation_period_num': 99, 'train_rates': 0.9703687648500523, 'learning_rate': 4.5001455982652995e-05, 'batch_size': 179, 'step_size': 14, 'gamma': 0.9594199168669608}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-06 23:58:00,859][0m Trial 10 finished with value: 0.48516082719844933 and parameters: {'observation_period_num': 150, 'train_rates': 0.8423349918318774, 'learning_rate': 0.0001979677944807115, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9898547413730497}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-07 00:00:39,858][0m Trial 11 finished with value: 0.6155303990805538 and parameters: {'observation_period_num': 149, 'train_rates': 0.8800624686332235, 'learning_rate': 0.0008815870316824542, 'batch_size': 254, 'step_size': 2, 'gamma': 0.7961995225597642}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-07 00:03:32,862][0m Trial 12 finished with value: 0.581414335760577 and parameters: {'observation_period_num': 169, 'train_rates': 0.7919827525599274, 'learning_rate': 0.00022298196148808587, 'batch_size': 200, 'step_size': 5, 'gamma': 0.9295703910391705}. Best is trial 1 with value: 0.30365129714495287.[0m
Early stopping at epoch 82
[32m[I 2025-02-07 00:04:55,837][0m Trial 13 finished with value: 0.6340435157074545 and parameters: {'observation_period_num': 96, 'train_rates': 0.9041707847240515, 'learning_rate': 0.00022399425536676865, 'batch_size': 121, 'step_size': 1, 'gamma': 0.8505247958093901}. Best is trial 1 with value: 0.30365129714495287.[0m
[32m[I 2025-02-07 00:07:28,536][0m Trial 14 finished with value: 0.23459038138389587 and parameters: {'observation_period_num': 132, 'train_rates': 0.9856514758872683, 'learning_rate': 0.0004119794258520889, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9206155087322405}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:08:34,441][0m Trial 15 finished with value: 0.48170986771583557 and parameters: {'observation_period_num': 64, 'train_rates': 0.9839589818548381, 'learning_rate': 1.7644594018282685e-05, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9294284005863603}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:11:49,790][0m Trial 16 finished with value: 0.49258287397650297 and parameters: {'observation_period_num': 178, 'train_rates': 0.7949567503907264, 'learning_rate': 0.00011360870916796594, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9321076991496727}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:13:54,086][0m Trial 17 finished with value: 0.7509153059550694 and parameters: {'observation_period_num': 124, 'train_rates': 0.906311857440248, 'learning_rate': 9.733104622072776e-06, 'batch_size': 174, 'step_size': 6, 'gamma': 0.9697848038699978}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:18:02,958][0m Trial 18 finished with value: 0.4553622820433354 and parameters: {'observation_period_num': 217, 'train_rates': 0.8367547981619006, 'learning_rate': 0.00036565887144966237, 'batch_size': 109, 'step_size': 8, 'gamma': 0.9077951191281262}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:19:27,535][0m Trial 19 finished with value: 0.28340815741624403 and parameters: {'observation_period_num': 82, 'train_rates': 0.9069699760196497, 'learning_rate': 0.00012656688544981438, 'batch_size': 156, 'step_size': 15, 'gamma': 0.8785094286466238}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:20:37,603][0m Trial 20 finished with value: 0.784763791755344 and parameters: {'observation_period_num': 75, 'train_rates': 0.7526674241148693, 'learning_rate': 0.00041127047897616335, 'batch_size': 211, 'step_size': 15, 'gamma': 0.9583403840695468}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:21:11,021][0m Trial 21 finished with value: 0.27339357137680054 and parameters: {'observation_period_num': 27, 'train_rates': 0.9044514728910189, 'learning_rate': 0.00010833135173801487, 'batch_size': 160, 'step_size': 13, 'gamma': 0.8811028453770199}. Best is trial 14 with value: 0.23459038138389587.[0m
[32m[I 2025-02-07 00:21:44,400][0m Trial 22 finished with value: 0.20809853076934814 and parameters: {'observation_period_num': 14, 'train_rates': 0.9836287242360521, 'learning_rate': 8.185591068487184e-05, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8749215801918472}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:22:13,838][0m Trial 23 finished with value: 0.44438567757606506 and parameters: {'observation_period_num': 6, 'train_rates': 0.9899377163250801, 'learning_rate': 2.9017861717788214e-05, 'batch_size': 183, 'step_size': 13, 'gamma': 0.9170123916209616}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:23:00,194][0m Trial 24 finished with value: 0.2955026924610138 and parameters: {'observation_period_num': 39, 'train_rates': 0.9552811984019265, 'learning_rate': 7.967625798282147e-05, 'batch_size': 138, 'step_size': 12, 'gamma': 0.8629745235829586}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:23:24,662][0m Trial 25 finished with value: 0.6683845188536781 and parameters: {'observation_period_num': 6, 'train_rates': 0.870133518348254, 'learning_rate': 0.0009947891143180327, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8336572779849716}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:24:02,955][0m Trial 26 finished with value: 0.29169549725272437 and parameters: {'observation_period_num': 31, 'train_rates': 0.943593381922215, 'learning_rate': 0.00027987103596876226, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8963988297102735}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:24:56,275][0m Trial 27 finished with value: 0.6744264960289001 and parameters: {'observation_period_num': 47, 'train_rates': 0.9828574222499812, 'learning_rate': 1.0258826135077991e-05, 'batch_size': 130, 'step_size': 10, 'gamma': 0.944638761666993}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:25:23,161][0m Trial 28 finished with value: 0.5925506990785381 and parameters: {'observation_period_num': 22, 'train_rates': 0.8206850574661545, 'learning_rate': 3.8450852898194574e-05, 'batch_size': 193, 'step_size': 12, 'gamma': 0.8700555243250963}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:25:49,648][0m Trial 29 finished with value: 0.4633657733599345 and parameters: {'observation_period_num': 22, 'train_rates': 0.8854274555805505, 'learning_rate': 9.042686527938714e-05, 'batch_size': 224, 'step_size': 8, 'gamma': 0.7527174382604405}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:27:56,743][0m Trial 30 finished with value: 0.26303690671920776 and parameters: {'observation_period_num': 119, 'train_rates': 0.9268816234639564, 'learning_rate': 0.00014987324894952594, 'batch_size': 104, 'step_size': 14, 'gamma': 0.7901645777300814}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:30:04,432][0m Trial 31 finished with value: 0.2388433029929411 and parameters: {'observation_period_num': 119, 'train_rates': 0.9274493430414139, 'learning_rate': 0.00015393415255723848, 'batch_size': 100, 'step_size': 14, 'gamma': 0.7999237079989946}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:32:32,406][0m Trial 32 finished with value: 0.266718867760134 and parameters: {'observation_period_num': 129, 'train_rates': 0.9253109490150222, 'learning_rate': 0.00017319873933417893, 'batch_size': 97, 'step_size': 14, 'gamma': 0.7854596105009121}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:34:51,285][0m Trial 33 finished with value: 0.2323745100271134 and parameters: {'observation_period_num': 116, 'train_rates': 0.9631753573051153, 'learning_rate': 6.077295494159824e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8073364798497671}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:37:47,535][0m Trial 34 finished with value: 0.41183385338920814 and parameters: {'observation_period_num': 140, 'train_rates': 0.9632110015977402, 'learning_rate': 0.0004147715109497685, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8184928917090994}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:40:06,687][0m Trial 35 finished with value: 0.21689894903137022 and parameters: {'observation_period_num': 110, 'train_rates': 0.9709984235565561, 'learning_rate': 6.129422611172639e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.771047876773273}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:43:36,786][0m Trial 36 finished with value: 0.26120484362949026 and parameters: {'observation_period_num': 164, 'train_rates': 0.9607596493618941, 'learning_rate': 5.895366001996516e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.769483928945653}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:45:21,885][0m Trial 37 finished with value: 1.0515163631608644 and parameters: {'observation_period_num': 84, 'train_rates': 0.6569879333497943, 'learning_rate': 2.165591547112046e-05, 'batch_size': 36, 'step_size': 7, 'gamma': 0.7502652938728761}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:47:28,118][0m Trial 38 finished with value: 0.4011381153549467 and parameters: {'observation_period_num': 110, 'train_rates': 0.9509716694911671, 'learning_rate': 3.066490534368975e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7752784816969525}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:53:08,352][0m Trial 39 finished with value: 0.7129335999488831 and parameters: {'observation_period_num': 251, 'train_rates': 0.9894521071810992, 'learning_rate': 9.420507685725937e-06, 'batch_size': 83, 'step_size': 9, 'gamma': 0.8097245660467445}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:54:42,368][0m Trial 40 finished with value: 0.29702188954295883 and parameters: {'observation_period_num': 64, 'train_rates': 0.9713991421097985, 'learning_rate': 6.650412442990908e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8934903937304504}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:57:47,091][0m Trial 41 finished with value: 0.9552250544903642 and parameters: {'observation_period_num': 140, 'train_rates': 0.937416934659985, 'learning_rate': 0.000602781795176959, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8308263739275353}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 00:59:46,945][0m Trial 42 finished with value: 0.4986809194087982 and parameters: {'observation_period_num': 110, 'train_rates': 0.9657868197385933, 'learning_rate': 4.7934658620253434e-05, 'batch_size': 144, 'step_size': 13, 'gamma': 0.8016158337763728}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 01:01:29,292][0m Trial 43 finished with value: 0.3244786027836915 and parameters: {'observation_period_num': 97, 'train_rates': 0.9276340739174908, 'learning_rate': 8.129150491261029e-05, 'batch_size': 122, 'step_size': 15, 'gamma': 0.7771713185089474}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 01:04:49,064][0m Trial 44 finished with value: 0.2641609539459278 and parameters: {'observation_period_num': 137, 'train_rates': 0.9455773922735803, 'learning_rate': 0.00029114515622484527, 'batch_size': 24, 'step_size': 14, 'gamma': 0.757796113501948}. Best is trial 22 with value: 0.20809853076934814.[0m
[32m[I 2025-02-07 01:08:33,950][0m Trial 45 finished with value: 0.1647159308195114 and parameters: {'observation_period_num': 188, 'train_rates': 0.9769046115214166, 'learning_rate': 0.00014611903728488612, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8156896515554278}. Best is trial 45 with value: 0.1647159308195114.[0m
[32m[I 2025-02-07 01:12:51,067][0m Trial 46 finished with value: 0.4418431496620178 and parameters: {'observation_period_num': 198, 'train_rates': 0.9729833696422143, 'learning_rate': 1.4506366494273179e-05, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8253859677064571}. Best is trial 45 with value: 0.1647159308195114.[0m
[32m[I 2025-02-07 01:17:54,578][0m Trial 47 finished with value: 0.277119904756546 and parameters: {'observation_period_num': 229, 'train_rates': 0.9898470214225727, 'learning_rate': 3.82191775265358e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8391414402013355}. Best is trial 45 with value: 0.1647159308195114.[0m
[32m[I 2025-02-07 01:21:25,756][0m Trial 48 finished with value: 0.337181368744687 and parameters: {'observation_period_num': 186, 'train_rates': 0.8901823505705054, 'learning_rate': 5.981468707462095e-05, 'batch_size': 92, 'step_size': 9, 'gamma': 0.8610005588857887}. Best is trial 45 with value: 0.1647159308195114.[0m
[32m[I 2025-02-07 01:23:56,028][0m Trial 49 finished with value: 0.8372785186016115 and parameters: {'observation_period_num': 154, 'train_rates': 0.7506268321550648, 'learning_rate': 9.535529535397363e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.8463071089429015}. Best is trial 45 with value: 0.1647159308195114.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-07 01:23:56,035][0m A new study created in memory with name: no-name-6965a188-aed5-4d41-beb2-9cf5708ad1f5[0m
[32m[I 2025-02-07 01:25:49,318][0m Trial 0 finished with value: 1.498048654723092 and parameters: {'observation_period_num': 127, 'train_rates': 0.6644322096732218, 'learning_rate': 4.925067141259208e-06, 'batch_size': 77, 'step_size': 11, 'gamma': 0.7992549477079686}. Best is trial 0 with value: 1.498048654723092.[0m
[32m[I 2025-02-07 01:27:20,364][0m Trial 1 finished with value: 1.4125647430026202 and parameters: {'observation_period_num': 93, 'train_rates': 0.771643375829286, 'learning_rate': 1.1795909907313761e-06, 'batch_size': 95, 'step_size': 15, 'gamma': 0.9806344983136656}. Best is trial 1 with value: 1.4125647430026202.[0m
[32m[I 2025-02-07 01:28:17,060][0m Trial 2 finished with value: 1.5682823760798335 and parameters: {'observation_period_num': 64, 'train_rates': 0.8317428984242903, 'learning_rate': 1.4877480560888409e-05, 'batch_size': 237, 'step_size': 1, 'gamma': 0.9060706415856763}. Best is trial 1 with value: 1.4125647430026202.[0m
[32m[I 2025-02-07 01:33:03,058][0m Trial 3 finished with value: 0.7221188492366662 and parameters: {'observation_period_num': 197, 'train_rates': 0.881622757802067, 'learning_rate': 1.9682366980780604e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9297405143627135}. Best is trial 3 with value: 0.7221188492366662.[0m
[32m[I 2025-02-07 01:33:58,059][0m Trial 4 finished with value: 0.8010392980736187 and parameters: {'observation_period_num': 57, 'train_rates': 0.738701698039933, 'learning_rate': 0.00029572548705748635, 'batch_size': 93, 'step_size': 14, 'gamma': 0.9478293965377104}. Best is trial 3 with value: 0.7221188492366662.[0m
[32m[I 2025-02-07 01:36:47,516][0m Trial 5 finished with value: 1.1665449268964645 and parameters: {'observation_period_num': 33, 'train_rates': 0.7302664951248093, 'learning_rate': 0.0001870058798114037, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9007799468526497}. Best is trial 3 with value: 0.7221188492366662.[0m
[32m[I 2025-02-07 01:38:11,814][0m Trial 6 finished with value: 0.8957088780852983 and parameters: {'observation_period_num': 75, 'train_rates': 0.9264254719340963, 'learning_rate': 9.262418946436657e-06, 'batch_size': 97, 'step_size': 9, 'gamma': 0.8847753825642903}. Best is trial 3 with value: 0.7221188492366662.[0m
[32m[I 2025-02-07 01:40:33,674][0m Trial 7 finished with value: 0.2406355644491586 and parameters: {'observation_period_num': 42, 'train_rates': 0.9245210547709184, 'learning_rate': 8.294897819208294e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.844264954248167}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 01:44:40,420][0m Trial 8 finished with value: 0.9518450281388025 and parameters: {'observation_period_num': 251, 'train_rates': 0.6428452230685707, 'learning_rate': 0.000180189655646279, 'batch_size': 173, 'step_size': 6, 'gamma': 0.9679246428614222}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 01:49:47,375][0m Trial 9 finished with value: 0.26004125891774416 and parameters: {'observation_period_num': 244, 'train_rates': 0.8852630796103321, 'learning_rate': 0.00019803791207743967, 'batch_size': 48, 'step_size': 8, 'gamma': 0.8705696928961457}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 01:50:22,341][0m Trial 10 finished with value: 0.5492120981216431 and parameters: {'observation_period_num': 18, 'train_rates': 0.9897185999122841, 'learning_rate': 5.16790477130181e-05, 'batch_size': 158, 'step_size': 4, 'gamma': 0.8087145894538412}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 01:54:00,804][0m Trial 11 finished with value: 1.0335447783229732 and parameters: {'observation_period_num': 178, 'train_rates': 0.9144441434840211, 'learning_rate': 0.0008776514255342226, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8419684499729255}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 01:58:52,642][0m Trial 12 finished with value: 0.46161314837708334 and parameters: {'observation_period_num': 246, 'train_rates': 0.8510664643862057, 'learning_rate': 6.121181514257045e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.7523044627847028}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:01:53,132][0m Trial 13 finished with value: 0.2512127161026001 and parameters: {'observation_period_num': 160, 'train_rates': 0.9831719445406637, 'learning_rate': 0.0006565509482485996, 'batch_size': 128, 'step_size': 6, 'gamma': 0.846775727699176}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:04:38,790][0m Trial 14 finished with value: 0.3051590025424957 and parameters: {'observation_period_num': 142, 'train_rates': 0.9867217744620278, 'learning_rate': 0.0008511291394652239, 'batch_size': 209, 'step_size': 5, 'gamma': 0.8328357684178502}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:06:49,569][0m Trial 15 finished with value: 1.265543821717606 and parameters: {'observation_period_num': 121, 'train_rates': 0.9483995755377419, 'learning_rate': 2.5405416657209556e-05, 'batch_size': 136, 'step_size': 2, 'gamma': 0.8467757182201023}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:10:07,636][0m Trial 16 finished with value: 0.3338669240474701 and parameters: {'observation_period_num': 169, 'train_rates': 0.9602190307496495, 'learning_rate': 0.0004107782729676668, 'batch_size': 136, 'step_size': 7, 'gamma': 0.7789275237352857}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:13:58,927][0m Trial 17 finished with value: 0.6379007434039182 and parameters: {'observation_period_num': 209, 'train_rates': 0.8169838179544009, 'learning_rate': 3.6441586620479236e-05, 'batch_size': 190, 'step_size': 11, 'gamma': 0.815646292289015}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:15:38,489][0m Trial 18 finished with value: 0.522914739160598 and parameters: {'observation_period_num': 97, 'train_rates': 0.8895853172846729, 'learning_rate': 8.197605320814076e-05, 'batch_size': 123, 'step_size': 4, 'gamma': 0.8571211621824436}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:16:01,067][0m Trial 19 finished with value: 0.3766387403011322 and parameters: {'observation_period_num': 8, 'train_rates': 0.9353523108183927, 'learning_rate': 9.098187074349006e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.7882688750197389}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:18:45,859][0m Trial 20 finished with value: 0.6258314133506931 and parameters: {'observation_period_num': 166, 'train_rates': 0.7906778053283233, 'learning_rate': 0.0004096213405574193, 'batch_size': 116, 'step_size': 7, 'gamma': 0.8749883778724783}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:23:21,273][0m Trial 21 finished with value: 0.27397373767835753 and parameters: {'observation_period_num': 222, 'train_rates': 0.8718639791167034, 'learning_rate': 0.00015076478838617628, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8623239806118723}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:26:13,923][0m Trial 22 finished with value: 0.34891403352904055 and parameters: {'observation_period_num': 152, 'train_rates': 0.9043034308437535, 'learning_rate': 0.0005181251102073163, 'batch_size': 57, 'step_size': 8, 'gamma': 0.8269706670808399}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:29:00,531][0m Trial 23 finished with value: 0.24731582206811092 and parameters: {'observation_period_num': 103, 'train_rates': 0.9547816498210137, 'learning_rate': 0.00015160748635848945, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8944267905173784}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:31:53,400][0m Trial 24 finished with value: 0.25703622515384966 and parameters: {'observation_period_num': 106, 'train_rates': 0.9589478738788291, 'learning_rate': 0.0001109001174114535, 'batch_size': 28, 'step_size': 3, 'gamma': 0.9007377217521549}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:33:01,674][0m Trial 25 finished with value: 0.3605058789253235 and parameters: {'observation_period_num': 43, 'train_rates': 0.9795847808180056, 'learning_rate': 2.4342116200837064e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.9263562611281414}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:35:41,510][0m Trial 26 finished with value: 0.2927847918002836 and parameters: {'observation_period_num': 84, 'train_rates': 0.9355021917443695, 'learning_rate': 0.00029414078951553544, 'batch_size': 30, 'step_size': 5, 'gamma': 0.8842244338274758}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:37:36,349][0m Trial 27 finished with value: 0.3915992672086055 and parameters: {'observation_period_num': 118, 'train_rates': 0.856538530543842, 'learning_rate': 0.0006384426482392799, 'batch_size': 216, 'step_size': 10, 'gamma': 0.8255674499518489}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:39:30,525][0m Trial 28 finished with value: 1.0954811587449043 and parameters: {'observation_period_num': 138, 'train_rates': 0.6036760841926458, 'learning_rate': 4.895179232069123e-05, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8477478791286037}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:40:41,957][0m Trial 29 finished with value: 0.8078173123024128 and parameters: {'observation_period_num': 47, 'train_rates': 0.9630188599907068, 'learning_rate': 6.1064430850572256e-06, 'batch_size': 70, 'step_size': 12, 'gamma': 0.7949239856733321}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:44:16,362][0m Trial 30 finished with value: 0.25472892989424895 and parameters: {'observation_period_num': 190, 'train_rates': 0.9009587972401295, 'learning_rate': 0.00031618836817765, 'batch_size': 85, 'step_size': 5, 'gamma': 0.9186252793134523}. Best is trial 7 with value: 0.2406355644491586.[0m
[32m[I 2025-02-07 02:48:10,208][0m Trial 31 finished with value: 0.23926468061713072 and parameters: {'observation_period_num': 202, 'train_rates': 0.9056005249062064, 'learning_rate': 0.00033297268547632377, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9177999628659204}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 02:52:40,180][0m Trial 32 finished with value: 0.7134915090544255 and parameters: {'observation_period_num': 220, 'train_rates': 0.9163175698429299, 'learning_rate': 0.0009762515594772118, 'batch_size': 108, 'step_size': 6, 'gamma': 0.8867103349873382}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 02:55:36,426][0m Trial 33 finished with value: 0.3497215211391449 and parameters: {'observation_period_num': 158, 'train_rates': 0.9490425945665926, 'learning_rate': 0.00012656425341632944, 'batch_size': 153, 'step_size': 3, 'gamma': 0.9510842864202762}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 02:59:09,883][0m Trial 34 finished with value: 0.4845991002286182 and parameters: {'observation_period_num': 186, 'train_rates': 0.8159031552521283, 'learning_rate': 0.00024299467416551344, 'batch_size': 37, 'step_size': 4, 'gamma': 0.9086811411762401}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:03:29,442][0m Trial 35 finished with value: 0.34772980213165283 and parameters: {'observation_period_num': 208, 'train_rates': 0.9709325664173857, 'learning_rate': 0.0005598599157045286, 'batch_size': 103, 'step_size': 8, 'gamma': 0.9482591156170328}. Best is trial 31 with value: 0.23926468061713072.[0m
Early stopping at epoch 97
[32m[I 2025-02-07 03:05:51,180][0m Trial 36 finished with value: 1.8064516071299344 and parameters: {'observation_period_num': 136, 'train_rates': 0.86635628023594, 'learning_rate': 2.1426750025996964e-06, 'batch_size': 83, 'step_size': 1, 'gamma': 0.892661647768832}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:09:46,271][0m Trial 37 finished with value: 0.7555611880312503 and parameters: {'observation_period_num': 66, 'train_rates': 0.7541374505104526, 'learning_rate': 7.220512195510635e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.9346350356515541}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:11:37,402][0m Trial 38 finished with value: 0.5856941513593684 and parameters: {'observation_period_num': 105, 'train_rates': 0.9367251948326694, 'learning_rate': 1.6090909587944488e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9840674462235277}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:12:38,276][0m Trial 39 finished with value: 0.8788252652515688 and parameters: {'observation_period_num': 28, 'train_rates': 0.6850445614518317, 'learning_rate': 0.00012790641220857208, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8633073018835677}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:13:58,616][0m Trial 40 finished with value: 0.4625143879585052 and parameters: {'observation_period_num': 81, 'train_rates': 0.8452151770996452, 'learning_rate': 0.0002335194510505724, 'batch_size': 147, 'step_size': 3, 'gamma': 0.875694567998862}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:17:39,214][0m Trial 41 finished with value: 0.26136433761968886 and parameters: {'observation_period_num': 189, 'train_rates': 0.8997043297650261, 'learning_rate': 0.00039979334284145706, 'batch_size': 85, 'step_size': 5, 'gamma': 0.9253087537947505}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:22:31,354][0m Trial 42 finished with value: 0.25856824473301937 and parameters: {'observation_period_num': 233, 'train_rates': 0.9203584664529372, 'learning_rate': 0.00026340181292411265, 'batch_size': 114, 'step_size': 5, 'gamma': 0.9144661557098016}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:26:25,989][0m Trial 43 finished with value: 0.3021271203605222 and parameters: {'observation_period_num': 199, 'train_rates': 0.8970037212508368, 'learning_rate': 0.0003584662330642885, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9377486469018909}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:29:59,275][0m Trial 44 finished with value: 0.2937292209910062 and parameters: {'observation_period_num': 185, 'train_rates': 0.8795952793619493, 'learning_rate': 0.0001906674014161469, 'batch_size': 85, 'step_size': 4, 'gamma': 0.9153701706248852}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:33:07,322][0m Trial 45 finished with value: 0.3851543664932251 and parameters: {'observation_period_num': 153, 'train_rates': 0.9441027770364202, 'learning_rate': 0.0006491564876572969, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8513058353159765}. Best is trial 31 with value: 0.23926468061713072.[0m
[32m[I 2025-02-07 03:36:27,543][0m Trial 46 finished with value: 0.18845345079898834 and parameters: {'observation_period_num': 174, 'train_rates': 0.987787665675257, 'learning_rate': 0.0001648509238731016, 'batch_size': 169, 'step_size': 5, 'gamma': 0.957273278300249}. Best is trial 46 with value: 0.18845345079898834.[0m
[32m[I 2025-02-07 03:39:48,282][0m Trial 47 finished with value: 0.20205800235271454 and parameters: {'observation_period_num': 170, 'train_rates': 0.9896081790679921, 'learning_rate': 0.0001577243220023856, 'batch_size': 180, 'step_size': 10, 'gamma': 0.9662585599856395}. Best is trial 46 with value: 0.18845345079898834.[0m
[32m[I 2025-02-07 03:43:06,054][0m Trial 48 finished with value: 0.3280843198299408 and parameters: {'observation_period_num': 169, 'train_rates': 0.9706160825904099, 'learning_rate': 0.00010210135193036059, 'batch_size': 185, 'step_size': 10, 'gamma': 0.9689711635418857}. Best is trial 46 with value: 0.18845345079898834.[0m
[32m[I 2025-02-07 03:46:34,070][0m Trial 49 finished with value: 0.3196291923522949 and parameters: {'observation_period_num': 177, 'train_rates': 0.9563969659298623, 'learning_rate': 0.00015859189621667802, 'batch_size': 177, 'step_size': 9, 'gamma': 0.9691621360369981}. Best is trial 46 with value: 0.18845345079898834.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-07 03:46:34,077][0m A new study created in memory with name: no-name-8a2b6bd4-e0cb-41bf-82cf-593abfeb660f[0m
[32m[I 2025-02-07 03:47:56,433][0m Trial 0 finished with value: 0.8867753639555814 and parameters: {'observation_period_num': 97, 'train_rates': 0.6809092903528986, 'learning_rate': 0.0004685068439465878, 'batch_size': 227, 'step_size': 12, 'gamma': 0.7800394265131328}. Best is trial 0 with value: 0.8867753639555814.[0m
[32m[I 2025-02-07 03:48:52,425][0m Trial 1 finished with value: 0.42438309711568495 and parameters: {'observation_period_num': 60, 'train_rates': 0.8534274719510666, 'learning_rate': 0.00012560933801405595, 'batch_size': 234, 'step_size': 10, 'gamma': 0.9336706996843531}. Best is trial 1 with value: 0.42438309711568495.[0m
[32m[I 2025-02-07 03:52:56,208][0m Trial 2 finished with value: 0.744731943993955 and parameters: {'observation_period_num': 203, 'train_rates': 0.8655680430818242, 'learning_rate': 0.0005199551649390113, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8579680359271207}. Best is trial 1 with value: 0.42438309711568495.[0m
[32m[I 2025-02-07 03:56:34,392][0m Trial 3 finished with value: 1.226107858087374 and parameters: {'observation_period_num': 191, 'train_rates': 0.9210376526848301, 'learning_rate': 7.805656530149306e-06, 'batch_size': 203, 'step_size': 4, 'gamma': 0.8785532106584334}. Best is trial 1 with value: 0.42438309711568495.[0m
[32m[I 2025-02-07 03:58:06,272][0m Trial 4 finished with value: 0.2118496298789978 and parameters: {'observation_period_num': 87, 'train_rates': 0.9728016796460603, 'learning_rate': 0.00020593855073306567, 'batch_size': 230, 'step_size': 11, 'gamma': 0.8253208917885019}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:01:20,411][0m Trial 5 finished with value: 0.5105985188806379 and parameters: {'observation_period_num': 178, 'train_rates': 0.866872481266957, 'learning_rate': 3.667720450014513e-05, 'batch_size': 146, 'step_size': 5, 'gamma': 0.9632722087483483}. Best is trial 4 with value: 0.2118496298789978.[0m
Early stopping at epoch 64
[32m[I 2025-02-07 04:02:46,894][0m Trial 6 finished with value: 1.2099862098693848 and parameters: {'observation_period_num': 128, 'train_rates': 0.9485003930719835, 'learning_rate': 9.237800759541211e-05, 'batch_size': 252, 'step_size': 1, 'gamma': 0.8218670237078203}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:05:10,401][0m Trial 7 finished with value: 2.400999742101198 and parameters: {'observation_period_num': 116, 'train_rates': 0.7820989740340984, 'learning_rate': 0.0009704978973822443, 'batch_size': 29, 'step_size': 13, 'gamma': 0.823279417034736}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:07:46,418][0m Trial 8 finished with value: 1.5607369640692876 and parameters: {'observation_period_num': 164, 'train_rates': 0.7224933440193784, 'learning_rate': 3.98294273266137e-06, 'batch_size': 239, 'step_size': 1, 'gamma': 0.9830022149089277}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:08:13,036][0m Trial 9 finished with value: 1.3674318302859056 and parameters: {'observation_period_num': 21, 'train_rates': 0.8107147692339542, 'learning_rate': 1.1000972487926931e-05, 'batch_size': 197, 'step_size': 2, 'gamma': 0.846832534761807}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:08:45,210][0m Trial 10 finished with value: 0.8833960103377584 and parameters: {'observation_period_num': 12, 'train_rates': 0.6087161071294699, 'learning_rate': 0.0001391927730210956, 'batch_size': 117, 'step_size': 15, 'gamma': 0.776088006897302}. Best is trial 4 with value: 0.2118496298789978.[0m
[32m[I 2025-02-07 04:09:58,540][0m Trial 11 finished with value: 0.1995028257369995 and parameters: {'observation_period_num': 66, 'train_rates': 0.985409530244141, 'learning_rate': 0.00017919716862247446, 'batch_size': 163, 'step_size': 9, 'gamma': 0.9217279165972267}. Best is trial 11 with value: 0.1995028257369995.[0m
[32m[I 2025-02-07 04:11:14,020][0m Trial 12 finished with value: 0.34186840057373047 and parameters: {'observation_period_num': 67, 'train_rates': 0.9798822423559871, 'learning_rate': 4.349918074716116e-05, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9164715910197685}. Best is trial 11 with value: 0.1995028257369995.[0m
[32m[I 2025-02-07 04:12:34,041][0m Trial 13 finished with value: 0.1736164391040802 and parameters: {'observation_period_num': 67, 'train_rates': 0.9879816528844639, 'learning_rate': 0.0002807458967919485, 'batch_size': 98, 'step_size': 8, 'gamma': 0.901341376760143}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:17:36,768][0m Trial 14 finished with value: 0.2814127260293716 and parameters: {'observation_period_num': 248, 'train_rates': 0.9135603881193717, 'learning_rate': 0.00030572871020568247, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9029659359452139}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:18:35,388][0m Trial 15 finished with value: 1.1889695756965213 and parameters: {'observation_period_num': 42, 'train_rates': 0.9073752671618328, 'learning_rate': 1.5239702420453571e-06, 'batch_size': 85, 'step_size': 6, 'gamma': 0.9470135522601391}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:19:55,108][0m Trial 16 finished with value: 0.803454745373997 and parameters: {'observation_period_num': 84, 'train_rates': 0.7918310824648338, 'learning_rate': 5.687756729951269e-05, 'batch_size': 166, 'step_size': 9, 'gamma': 0.8941325876903834}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:20:49,797][0m Trial 17 finished with value: 0.5309824347496033 and parameters: {'observation_period_num': 46, 'train_rates': 0.9896194683080419, 'learning_rate': 1.5275858353060208e-05, 'batch_size': 111, 'step_size': 7, 'gamma': 0.924087106862746}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:23:34,736][0m Trial 18 finished with value: 0.39977553486824036 and parameters: {'observation_period_num': 152, 'train_rates': 0.944346569122526, 'learning_rate': 0.0009529927142363435, 'batch_size': 178, 'step_size': 4, 'gamma': 0.8887660055705597}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:25:13,628][0m Trial 19 finished with value: 1.001659265879927 and parameters: {'observation_period_num': 110, 'train_rates': 0.7247329802773859, 'learning_rate': 0.0003219970379399573, 'batch_size': 126, 'step_size': 9, 'gamma': 0.9891105422241688}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:26:23,069][0m Trial 20 finished with value: 0.2785889192110102 and parameters: {'observation_period_num': 32, 'train_rates': 0.8868224705550688, 'learning_rate': 7.668179663147333e-05, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9471494717862896}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:27:44,691][0m Trial 21 finished with value: 0.3159654140472412 and parameters: {'observation_period_num': 80, 'train_rates': 0.9647255277241342, 'learning_rate': 0.00018665198432777144, 'batch_size': 194, 'step_size': 11, 'gamma': 0.8103096946005467}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:29:24,245][0m Trial 22 finished with value: 0.2914943631584122 and parameters: {'observation_period_num': 93, 'train_rates': 0.941138658454987, 'learning_rate': 0.00025749698560233625, 'batch_size': 136, 'step_size': 10, 'gamma': 0.8440419284434131}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:30:26,753][0m Trial 23 finished with value: 0.6637464761734009 and parameters: {'observation_period_num': 62, 'train_rates': 0.9741727094955651, 'learning_rate': 2.2313152953767305e-05, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7987761191069046}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:32:39,087][0m Trial 24 finished with value: 0.5095231306293736 and parameters: {'observation_period_num': 129, 'train_rates': 0.8208221572427973, 'learning_rate': 0.00043616557840292627, 'batch_size': 173, 'step_size': 7, 'gamma': 0.7554759772429945}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:34:05,167][0m Trial 25 finished with value: 0.2052403688430786 and parameters: {'observation_period_num': 73, 'train_rates': 0.9882798002211269, 'learning_rate': 0.00023425388177955353, 'batch_size': 93, 'step_size': 10, 'gamma': 0.8682236906951925}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:34:58,983][0m Trial 26 finished with value: 0.3821940377566487 and parameters: {'observation_period_num': 48, 'train_rates': 0.9276202582217692, 'learning_rate': 0.0006292098888348041, 'batch_size': 97, 'step_size': 9, 'gamma': 0.8706055252812993}. Best is trial 13 with value: 0.1736164391040802.[0m
[32m[I 2025-02-07 04:36:48,012][0m Trial 27 finished with value: 0.1450873166322708 and parameters: {'observation_period_num': 30, 'train_rates': 0.9898982066904156, 'learning_rate': 0.00010750458884282798, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9083834793606768}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:39:02,943][0m Trial 28 finished with value: 0.2510702655099 and parameters: {'observation_period_num': 23, 'train_rates': 0.8929763762093186, 'learning_rate': 0.00010075303333041433, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9055744026546874}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:40:07,725][0m Trial 29 finished with value: 0.8135170840662579 and parameters: {'observation_period_num': 5, 'train_rates': 0.6353669536283029, 'learning_rate': 6.682183615738247e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.9406942767898088}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:44:06,450][0m Trial 30 finished with value: 0.6354804865666168 and parameters: {'observation_period_num': 107, 'train_rates': 0.7531078706861469, 'learning_rate': 2.5027185748941032e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9645698512280994}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:45:28,782][0m Trial 31 finished with value: 0.2840335795825178 and parameters: {'observation_period_num': 70, 'train_rates': 0.9543372709547672, 'learning_rate': 0.00016219617761330398, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9156315063743646}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:46:31,709][0m Trial 32 finished with value: 0.28229132294654846 and parameters: {'observation_period_num': 54, 'train_rates': 0.9854381827087455, 'learning_rate': 0.0003612137754272971, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8786480100941799}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:47:40,102][0m Trial 33 finished with value: 0.23315606463481398 and parameters: {'observation_period_num': 32, 'train_rates': 0.9537627174444617, 'learning_rate': 0.00011354056696968794, 'batch_size': 74, 'step_size': 11, 'gamma': 0.8626720054847573}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:49:11,056][0m Trial 34 finished with value: 1.0799809298649785 and parameters: {'observation_period_num': 77, 'train_rates': 0.8397788236200243, 'learning_rate': 0.0006099658210976487, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8940843077843458}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:51:00,888][0m Trial 35 finished with value: 0.1694345623254776 and parameters: {'observation_period_num': 101, 'train_rates': 0.9870309914500305, 'learning_rate': 0.00020607178076628314, 'batch_size': 131, 'step_size': 12, 'gamma': 0.9254078652804008}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:52:50,148][0m Trial 36 finished with value: 0.2821079320317015 and parameters: {'observation_period_num': 104, 'train_rates': 0.9238092680038965, 'learning_rate': 0.00016951095030888553, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9618775388332007}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:54:23,757][0m Trial 37 finished with value: 0.3611476288901435 and parameters: {'observation_period_num': 91, 'train_rates': 0.8900079264953927, 'learning_rate': 5.266369703666212e-05, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9301244490477412}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:56:40,123][0m Trial 38 finished with value: 0.39547690749168396 and parameters: {'observation_period_num': 125, 'train_rates': 0.9622161715254964, 'learning_rate': 0.0005162055526185264, 'batch_size': 108, 'step_size': 5, 'gamma': 0.9109400869542046}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 04:59:12,179][0m Trial 39 finished with value: 0.3791998124688785 and parameters: {'observation_period_num': 143, 'train_rates': 0.8652872081150703, 'learning_rate': 0.00011504815104178326, 'batch_size': 155, 'step_size': 14, 'gamma': 0.9257736066118918}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:00:53,464][0m Trial 40 finished with value: 0.27481078675815035 and parameters: {'observation_period_num': 57, 'train_rates': 0.9348087479248323, 'learning_rate': 0.00021549653091262777, 'batch_size': 48, 'step_size': 12, 'gamma': 0.9355627642307781}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:01:57,999][0m Trial 41 finished with value: 0.16327621042728424 and parameters: {'observation_period_num': 36, 'train_rates': 0.9884912817047894, 'learning_rate': 0.00023617135296268672, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8787510718229323}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:03:01,272][0m Trial 42 finished with value: 0.2583691437136043 and parameters: {'observation_period_num': 36, 'train_rates': 0.9663604174927828, 'learning_rate': 8.54083180975814e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8820168693500805}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:03:44,753][0m Trial 43 finished with value: 0.33388206362724304 and parameters: {'observation_period_num': 19, 'train_rates': 0.9681545253992375, 'learning_rate': 0.0003929315981846794, 'batch_size': 119, 'step_size': 9, 'gamma': 0.8497977642515642}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:05:08,494][0m Trial 44 finished with value: 0.15023596584796906 and parameters: {'observation_period_num': 50, 'train_rates': 0.98878765413577, 'learning_rate': 0.00013430704396199514, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9014079850415}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:07:14,559][0m Trial 45 finished with value: 0.24997681935754004 and parameters: {'observation_period_num': 27, 'train_rates': 0.9462571214487617, 'learning_rate': 0.0001284989352802797, 'batch_size': 39, 'step_size': 8, 'gamma': 0.9007276700924279}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:08:32,716][0m Trial 46 finished with value: 0.3244983750985438 and parameters: {'observation_period_num': 49, 'train_rates': 0.9137070207827593, 'learning_rate': 3.469112821739732e-05, 'batch_size': 62, 'step_size': 10, 'gamma': 0.8841089074563111}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:13:40,323][0m Trial 47 finished with value: 0.2680722773075104 and parameters: {'observation_period_num': 11, 'train_rates': 0.9716221109580332, 'learning_rate': 0.0002751700839048655, 'batch_size': 16, 'step_size': 7, 'gamma': 0.854312196569617}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:18:37,541][0m Trial 48 finished with value: 1.0031584886943592 and parameters: {'observation_period_num': 232, 'train_rates': 0.9377396846518119, 'learning_rate': 6.09298319404865e-06, 'batch_size': 43, 'step_size': 5, 'gamma': 0.8322749133098915}. Best is trial 27 with value: 0.1450873166322708.[0m
[32m[I 2025-02-07 05:19:23,518][0m Trial 49 finished with value: 1.2157481583667893 and parameters: {'observation_period_num': 40, 'train_rates': 0.6699028274925354, 'learning_rate': 0.0006756328013308521, 'batch_size': 85, 'step_size': 8, 'gamma': 0.898222715307257}. Best is trial 27 with value: 0.1450873166322708.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-07 05:19:23,525][0m A new study created in memory with name: no-name-eb9a15c4-a20c-4776-b40b-3b19bd3c5d83[0m
[32m[I 2025-02-07 05:22:45,614][0m Trial 0 finished with value: 0.8181328337231438 and parameters: {'observation_period_num': 205, 'train_rates': 0.6878119066869737, 'learning_rate': 9.356670843952122e-05, 'batch_size': 128, 'step_size': 13, 'gamma': 0.753182383774428}. Best is trial 0 with value: 0.8181328337231438.[0m
[32m[I 2025-02-07 05:24:26,713][0m Trial 1 finished with value: 0.6034097744337615 and parameters: {'observation_period_num': 110, 'train_rates': 0.7814175766369936, 'learning_rate': 0.00030844361946862314, 'batch_size': 176, 'step_size': 14, 'gamma': 0.829852763265023}. Best is trial 1 with value: 0.6034097744337615.[0m
[32m[I 2025-02-07 05:27:41,285][0m Trial 2 finished with value: 1.4061258164319126 and parameters: {'observation_period_num': 198, 'train_rates': 0.6927804317778634, 'learning_rate': 2.3516405867944403e-05, 'batch_size': 253, 'step_size': 4, 'gamma': 0.8375432876190814}. Best is trial 1 with value: 0.6034097744337615.[0m
[32m[I 2025-02-07 05:28:32,633][0m Trial 3 finished with value: 1.565216519056306 and parameters: {'observation_period_num': 54, 'train_rates': 0.672941466605532, 'learning_rate': 2.4281335779452047e-06, 'batch_size': 86, 'step_size': 9, 'gamma': 0.9857821932408196}. Best is trial 1 with value: 0.6034097744337615.[0m
[32m[I 2025-02-07 05:31:34,170][0m Trial 4 finished with value: 1.3532395822102907 and parameters: {'observation_period_num': 164, 'train_rates': 0.9125442849986305, 'learning_rate': 4.333549832775398e-06, 'batch_size': 229, 'step_size': 10, 'gamma': 0.8153986063696976}. Best is trial 1 with value: 0.6034097744337615.[0m
[32m[I 2025-02-07 05:35:27,623][0m Trial 5 finished with value: 0.30622862862504047 and parameters: {'observation_period_num': 178, 'train_rates': 0.9754599340856285, 'learning_rate': 2.8377386475566467e-05, 'batch_size': 36, 'step_size': 6, 'gamma': 0.840702785308513}. Best is trial 5 with value: 0.30622862862504047.[0m
[32m[I 2025-02-07 05:38:07,357][0m Trial 6 finished with value: 1.2886228627578729 and parameters: {'observation_period_num': 180, 'train_rates': 0.6420444044673062, 'learning_rate': 0.0009004476324797064, 'batch_size': 75, 'step_size': 7, 'gamma': 0.7644143199513559}. Best is trial 5 with value: 0.30622862862504047.[0m
[32m[I 2025-02-07 05:41:37,559][0m Trial 7 finished with value: 0.5632824219633864 and parameters: {'observation_period_num': 187, 'train_rates': 0.8544167252271095, 'learning_rate': 0.0004561203924923366, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9432794191404791}. Best is trial 5 with value: 0.30622862862504047.[0m
Early stopping at epoch 94
[32m[I 2025-02-07 05:44:53,841][0m Trial 8 finished with value: 2.248672266996258 and parameters: {'observation_period_num': 189, 'train_rates': 0.74239769367588, 'learning_rate': 3.140358530891045e-06, 'batch_size': 35, 'step_size': 2, 'gamma': 0.7640907252651404}. Best is trial 5 with value: 0.30622862862504047.[0m
[32m[I 2025-02-07 05:49:30,487][0m Trial 9 finished with value: 1.5647335052490234 and parameters: {'observation_period_num': 228, 'train_rates': 0.9130596930602988, 'learning_rate': 8.556156773961643e-06, 'batch_size': 246, 'step_size': 1, 'gamma': 0.9196668827637458}. Best is trial 5 with value: 0.30622862862504047.[0m
[32m[I 2025-02-07 05:54:34,109][0m Trial 10 finished with value: 0.15299142588829173 and parameters: {'observation_period_num': 111, 'train_rates': 0.9899352851084761, 'learning_rate': 4.729402729130221e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9079693481538222}. Best is trial 10 with value: 0.15299142588829173.[0m
[32m[I 2025-02-07 05:59:37,294][0m Trial 11 finished with value: 0.14616156643942782 and parameters: {'observation_period_num': 109, 'train_rates': 0.9802822462382372, 'learning_rate': 4.740828442554063e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8916450547674453}. Best is trial 11 with value: 0.14616156643942782.[0m
[32m[I 2025-02-07 06:04:43,057][0m Trial 12 finished with value: 0.1418846808373928 and parameters: {'observation_period_num': 89, 'train_rates': 0.9889936295572346, 'learning_rate': 9.848677724889745e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.895454797452437}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:05:41,493][0m Trial 13 finished with value: 0.2817829858798247 and parameters: {'observation_period_num': 55, 'train_rates': 0.9149511098538659, 'learning_rate': 9.429487150793272e-05, 'batch_size': 123, 'step_size': 12, 'gamma': 0.889616590608407}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:09:43,900][0m Trial 14 finished with value: 0.6293056366535333 and parameters: {'observation_period_num': 75, 'train_rates': 0.851204994503342, 'learning_rate': 0.0001380396507197477, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8686401366248867}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:10:18,611][0m Trial 15 finished with value: 0.6715403199195862 and parameters: {'observation_period_num': 27, 'train_rates': 0.9738404098307216, 'learning_rate': 1.4655580643892738e-05, 'batch_size': 174, 'step_size': 9, 'gamma': 0.9402299702923977}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:12:55,710][0m Trial 16 finished with value: 0.3973749924425183 and parameters: {'observation_period_num': 140, 'train_rates': 0.8470357430302069, 'learning_rate': 0.00022072476349953678, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8709034116335697}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:14:31,488][0m Trial 17 finished with value: 0.29049088809445006 and parameters: {'observation_period_num': 88, 'train_rates': 0.9264024064518432, 'learning_rate': 5.663276529156089e-05, 'batch_size': 95, 'step_size': 8, 'gamma': 0.9752807930107343}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:16:59,266][0m Trial 18 finished with value: 1.073529601097107 and parameters: {'observation_period_num': 132, 'train_rates': 0.9429977576349375, 'learning_rate': 7.928560735685716e-06, 'batch_size': 166, 'step_size': 11, 'gamma': 0.8054889288144388}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:17:45,035][0m Trial 19 finished with value: 1.2176820209342472 and parameters: {'observation_period_num': 11, 'train_rates': 0.8771553039331413, 'learning_rate': 1.29412745479543e-06, 'batch_size': 107, 'step_size': 15, 'gamma': 0.873777221527705}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:20:33,424][0m Trial 20 finished with value: 0.43532059018288244 and parameters: {'observation_period_num': 151, 'train_rates': 0.8134118998332435, 'learning_rate': 5.8096607359694e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.950455683018728}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:23:59,079][0m Trial 21 finished with value: 0.16268600179598883 and parameters: {'observation_period_num': 107, 'train_rates': 0.9864023811017353, 'learning_rate': 4.776033582864268e-05, 'batch_size': 24, 'step_size': 11, 'gamma': 0.9042129847269845}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:28:56,484][0m Trial 22 finished with value: 0.28130101309205496 and parameters: {'observation_period_num': 107, 'train_rates': 0.9520223057943825, 'learning_rate': 0.0001648786085124168, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9185035583512866}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:30:39,881][0m Trial 23 finished with value: 0.27885785698890686 and parameters: {'observation_period_num': 82, 'train_rates': 0.9841926509509464, 'learning_rate': 1.4790170711532872e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.8931199910233626}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:32:57,684][0m Trial 24 finished with value: 0.34110200591385365 and parameters: {'observation_period_num': 120, 'train_rates': 0.8814948883000688, 'learning_rate': 3.7589555260151554e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8570893098895349}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:34:13,528][0m Trial 25 finished with value: 0.2503506560274895 and parameters: {'observation_period_num': 58, 'train_rates': 0.9514819407543905, 'learning_rate': 8.803565743642962e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9219913479772224}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:35:48,794][0m Trial 26 finished with value: 0.757819414138794 and parameters: {'observation_period_num': 91, 'train_rates': 0.9586630932949289, 'learning_rate': 1.9292840205137836e-05, 'batch_size': 203, 'step_size': 12, 'gamma': 0.8929304769057784}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:38:15,637][0m Trial 27 finished with value: 0.2587302114724288 and parameters: {'observation_period_num': 36, 'train_rates': 0.9011634233179994, 'learning_rate': 6.974977723377331e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.9136561067957636}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:41:11,361][0m Trial 28 finished with value: 0.23798441324593886 and parameters: {'observation_period_num': 155, 'train_rates': 0.9432991474950537, 'learning_rate': 0.00040531715225364896, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9612097141565019}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:42:53,176][0m Trial 29 finished with value: 1.059109838881247 and parameters: {'observation_period_num': 125, 'train_rates': 0.6019970050824817, 'learning_rate': 0.00011861115688183868, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9362247185754626}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:44:05,241][0m Trial 30 finished with value: 0.553560935357295 and parameters: {'observation_period_num': 68, 'train_rates': 0.8111089554732962, 'learning_rate': 3.439657698796453e-05, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8550122181967708}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:47:32,083][0m Trial 31 finished with value: 0.1529229896409171 and parameters: {'observation_period_num': 96, 'train_rates': 0.9878122129510023, 'learning_rate': 4.471753890788308e-05, 'batch_size': 24, 'step_size': 11, 'gamma': 0.9004838667688063}. Best is trial 12 with value: 0.1418846808373928.[0m
[32m[I 2025-02-07 06:52:39,135][0m Trial 32 finished with value: 0.13823873935074643 and parameters: {'observation_period_num': 98, 'train_rates': 0.989981258418338, 'learning_rate': 0.0002545436498103557, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8824952673818741}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 06:55:04,023][0m Trial 33 finished with value: 0.27091801222917195 and parameters: {'observation_period_num': 96, 'train_rates': 0.9352759247571186, 'learning_rate': 0.00020814729817237036, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8819578006577785}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 06:56:44,499][0m Trial 34 finished with value: 1.56784084915458 and parameters: {'observation_period_num': 98, 'train_rates': 0.7460209871085327, 'learning_rate': 0.0007545282702020046, 'batch_size': 59, 'step_size': 14, 'gamma': 0.8530041968072777}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 06:58:30,537][0m Trial 35 finished with value: 0.303360679239597 and parameters: {'observation_period_num': 40, 'train_rates': 0.9638818251450874, 'learning_rate': 0.0003003262434744571, 'batch_size': 47, 'step_size': 13, 'gamma': 0.899174017824549}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 06:59:40,646][0m Trial 36 finished with value: 0.31712808091038713 and parameters: {'observation_period_num': 70, 'train_rates': 0.8917634451028587, 'learning_rate': 0.00010360044434273079, 'batch_size': 146, 'step_size': 15, 'gamma': 0.9274957519193394}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:02:31,305][0m Trial 37 finished with value: 0.2979756980086092 and parameters: {'observation_period_num': 118, 'train_rates': 0.9299322874610492, 'learning_rate': 2.1514372488613692e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8235292921006817}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:05:09,017][0m Trial 38 finished with value: 0.3354788730653484 and parameters: {'observation_period_num': 131, 'train_rates': 0.9685407957286621, 'learning_rate': 0.00048425826050765906, 'batch_size': 76, 'step_size': 6, 'gamma': 0.800146188473553}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:07:57,720][0m Trial 39 finished with value: 0.31130266189575195 and parameters: {'observation_period_num': 145, 'train_rates': 0.9871930390318666, 'learning_rate': 0.00021901709355313725, 'batch_size': 198, 'step_size': 4, 'gamma': 0.8427828038228784}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:11:03,713][0m Trial 40 finished with value: 0.7197899920879814 and parameters: {'observation_period_num': 169, 'train_rates': 0.7600474021813177, 'learning_rate': 7.479100420772553e-05, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8852669407174678}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:16:01,843][0m Trial 41 finished with value: 0.18806322157382965 and parameters: {'observation_period_num': 109, 'train_rates': 0.9650782279456034, 'learning_rate': 4.103937677059983e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9055974486796373}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:19:08,063][0m Trial 42 finished with value: 0.14543604177813377 and parameters: {'observation_period_num': 79, 'train_rates': 0.9892250563725793, 'learning_rate': 4.864837321155075e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.9082536906599123}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:22:04,553][0m Trial 43 finished with value: 0.24013566304655637 and parameters: {'observation_period_num': 79, 'train_rates': 0.9706261646021863, 'learning_rate': 2.5822070003923164e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8784479195455215}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:24:06,949][0m Trial 44 finished with value: 0.15062925219535828 and parameters: {'observation_period_num': 61, 'train_rates': 0.9899021229316777, 'learning_rate': 0.0001334484295176608, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8628803870202533}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:25:46,076][0m Trial 45 finished with value: 0.8098356004719707 and parameters: {'observation_period_num': 62, 'train_rates': 0.6958395348595219, 'learning_rate': 0.00032395496684615576, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8318908366254063}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:27:02,437][0m Trial 46 finished with value: 0.2255324266093262 and parameters: {'observation_period_num': 47, 'train_rates': 0.9190361709169973, 'learning_rate': 0.00015529104972913506, 'batch_size': 63, 'step_size': 9, 'gamma': 0.8649607040733477}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:31:31,602][0m Trial 47 finished with value: 0.26061122740308446 and parameters: {'observation_period_num': 211, 'train_rates': 0.9520053832215589, 'learning_rate': 0.00013661586560090262, 'batch_size': 77, 'step_size': 8, 'gamma': 0.843401336257126}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:33:22,503][0m Trial 48 finished with value: 1.0933110826893857 and parameters: {'observation_period_num': 23, 'train_rates': 0.9351757691219489, 'learning_rate': 0.0007134314328374896, 'batch_size': 44, 'step_size': 10, 'gamma': 0.9314214735615842}. Best is trial 32 with value: 0.13823873935074643.[0m
[32m[I 2025-02-07 07:36:20,872][0m Trial 49 finished with value: 0.24021767769601235 and parameters: {'observation_period_num': 79, 'train_rates': 0.9018035741377329, 'learning_rate': 6.855592283511056e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8640331119237953}. Best is trial 32 with value: 0.13823873935074643.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-07 07:36:20,879][0m A new study created in memory with name: no-name-c6a2fbd6-276c-46bd-985f-dd92407b6036[0m
[32m[I 2025-02-07 07:38:04,135][0m Trial 0 finished with value: 1.0077403506339229 and parameters: {'observation_period_num': 119, 'train_rates': 0.6676748739505476, 'learning_rate': 0.00010061898162224615, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9334687869623015}. Best is trial 0 with value: 1.0077403506339229.[0m
[32m[I 2025-02-07 07:42:21,885][0m Trial 1 finished with value: 0.292750437920158 and parameters: {'observation_period_num': 208, 'train_rates': 0.9597789362607306, 'learning_rate': 0.0004913919181551366, 'batch_size': 102, 'step_size': 6, 'gamma': 0.7998996676973111}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:43:49,088][0m Trial 2 finished with value: 1.3991462862953825 and parameters: {'observation_period_num': 105, 'train_rates': 0.6109289567046867, 'learning_rate': 0.0006518417019184753, 'batch_size': 110, 'step_size': 11, 'gamma': 0.9745477016438466}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:47:11,702][0m Trial 3 finished with value: 0.8582567825155744 and parameters: {'observation_period_num': 195, 'train_rates': 0.7433660185343975, 'learning_rate': 7.545761695204015e-05, 'batch_size': 225, 'step_size': 15, 'gamma': 0.8160641541517525}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:51:06,591][0m Trial 4 finished with value: 1.5251871803563513 and parameters: {'observation_period_num': 227, 'train_rates': 0.6938140063002576, 'learning_rate': 1.9745404809950153e-06, 'batch_size': 111, 'step_size': 14, 'gamma': 0.9339570680195183}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:52:13,622][0m Trial 5 finished with value: 1.0775074137584657 and parameters: {'observation_period_num': 41, 'train_rates': 0.6131236309788478, 'learning_rate': 1.3098312252849532e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.7511187092155968}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:53:47,574][0m Trial 6 finished with value: 0.8496695883108668 and parameters: {'observation_period_num': 105, 'train_rates': 0.7067767964475495, 'learning_rate': 0.00020307467002015164, 'batch_size': 138, 'step_size': 12, 'gamma': 0.7639404238437136}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 07:57:00,516][0m Trial 7 finished with value: 0.7560728230759709 and parameters: {'observation_period_num': 193, 'train_rates': 0.7135223547088105, 'learning_rate': 0.000346893499817758, 'batch_size': 227, 'step_size': 10, 'gamma': 0.9036857182067719}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 08:02:03,384][0m Trial 8 finished with value: 1.342687305505725 and parameters: {'observation_period_num': 250, 'train_rates': 0.872665075953015, 'learning_rate': 9.826667544566355e-06, 'batch_size': 172, 'step_size': 5, 'gamma': 0.754833572647331}. Best is trial 1 with value: 0.292750437920158.[0m
[32m[I 2025-02-07 08:02:28,130][0m Trial 9 finished with value: 0.24469837001787864 and parameters: {'observation_period_num': 10, 'train_rates': 0.8977350676052538, 'learning_rate': 0.0001599204357293261, 'batch_size': 218, 'step_size': 15, 'gamma': 0.8445923033951735}. Best is trial 9 with value: 0.24469837001787864.[0m
Early stopping at epoch 80
[32m[I 2025-02-07 08:02:50,329][0m Trial 10 finished with value: 1.3623430115073474 and parameters: {'observation_period_num': 19, 'train_rates': 0.86299474893793, 'learning_rate': 2.9163654730648565e-05, 'batch_size': 189, 'step_size': 1, 'gamma': 0.8597062930362942}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:06:44,165][0m Trial 11 finished with value: 2.2896147668361664 and parameters: {'observation_period_num': 60, 'train_rates': 0.9752027852217361, 'learning_rate': 0.0009547236896476058, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8245796802756167}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:09:40,031][0m Trial 12 finished with value: 0.3077581226825714 and parameters: {'observation_period_num': 160, 'train_rates': 0.9615320564205889, 'learning_rate': 0.00022004049155644538, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8081203189834977}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:12:45,841][0m Trial 13 finished with value: 0.5103706460153691 and parameters: {'observation_period_num': 162, 'train_rates': 0.8952022424355894, 'learning_rate': 6.344255641913107e-05, 'batch_size': 71, 'step_size': 3, 'gamma': 0.8419901697623008}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:14:06,650][0m Trial 14 finished with value: 0.44162327675182467 and parameters: {'observation_period_num': 87, 'train_rates': 0.8067717603110739, 'learning_rate': 0.00035290722231343726, 'batch_size': 251, 'step_size': 8, 'gamma': 0.789315495858463}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:16:58,276][0m Trial 15 finished with value: 0.35620033448817684 and parameters: {'observation_period_num': 153, 'train_rates': 0.927481055320323, 'learning_rate': 0.00013651782989724487, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8818074908293658}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:18:06,913][0m Trial 16 finished with value: 0.6335704904476195 and parameters: {'observation_period_num': 68, 'train_rates': 0.8184723957464013, 'learning_rate': 3.2321968866405186e-05, 'batch_size': 136, 'step_size': 9, 'gamma': 0.784845731364435}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:19:11,393][0m Trial 17 finished with value: 0.4714135809303963 and parameters: {'observation_period_num': 17, 'train_rates': 0.9254893775822685, 'learning_rate': 0.0005096636388221147, 'batch_size': 77, 'step_size': 2, 'gamma': 0.8519041542317813}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:23:23,816][0m Trial 18 finished with value: 1.9084649085998535 and parameters: {'observation_period_num': 201, 'train_rates': 0.9899742215318739, 'learning_rate': 1.0108810904531738e-06, 'batch_size': 141, 'step_size': 4, 'gamma': 0.8880144809374404}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:26:41,866][0m Trial 19 finished with value: 0.392969803288092 and parameters: {'observation_period_num': 137, 'train_rates': 0.8371175989715349, 'learning_rate': 3.015228518080309e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.8317929764995147}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:31:44,540][0m Trial 20 finished with value: 0.2583589255809784 and parameters: {'observation_period_num': 248, 'train_rates': 0.9299078298838078, 'learning_rate': 0.0002187432050585436, 'batch_size': 211, 'step_size': 15, 'gamma': 0.7916574021912851}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:36:57,454][0m Trial 21 finished with value: 0.2524893283843994 and parameters: {'observation_period_num': 249, 'train_rates': 0.9263415052603498, 'learning_rate': 0.00023884589540627466, 'batch_size': 215, 'step_size': 14, 'gamma': 0.7915162712973975}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:41:54,328][0m Trial 22 finished with value: 0.2519927252314787 and parameters: {'observation_period_num': 246, 'train_rates': 0.9168710506694203, 'learning_rate': 0.0002022629974203979, 'batch_size': 215, 'step_size': 15, 'gamma': 0.7816204670620448}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:45:51,979][0m Trial 23 finished with value: 0.8160718889005723 and parameters: {'observation_period_num': 226, 'train_rates': 0.7729722044185993, 'learning_rate': 4.79370284700209e-05, 'batch_size': 209, 'step_size': 13, 'gamma': 0.7721499642086433}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:50:28,603][0m Trial 24 finished with value: 0.3486141227569777 and parameters: {'observation_period_num': 229, 'train_rates': 0.8933840329493079, 'learning_rate': 0.00014809743858196914, 'batch_size': 253, 'step_size': 14, 'gamma': 0.7766631353985436}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:53:37,439][0m Trial 25 finished with value: 0.36557510466937204 and parameters: {'observation_period_num': 177, 'train_rates': 0.8481289643129591, 'learning_rate': 0.0003165645339860976, 'batch_size': 234, 'step_size': 15, 'gamma': 0.8122199631773531}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 08:58:04,542][0m Trial 26 finished with value: 0.3428334744016001 and parameters: {'observation_period_num': 220, 'train_rates': 0.9117662549724153, 'learning_rate': 0.00010081968841219609, 'batch_size': 196, 'step_size': 13, 'gamma': 0.8295122675671923}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:03:26,437][0m Trial 27 finished with value: 0.3704112470149994 and parameters: {'observation_period_num': 251, 'train_rates': 0.9457782461880612, 'learning_rate': 0.0008424485355815697, 'batch_size': 175, 'step_size': 14, 'gamma': 0.8622044052356775}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:06:37,935][0m Trial 28 finished with value: 0.9109669076071845 and parameters: {'observation_period_num': 175, 'train_rates': 0.8866866752640256, 'learning_rate': 1.4703735509511304e-05, 'batch_size': 212, 'step_size': 12, 'gamma': 0.8405427693404391}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:08:48,920][0m Trial 29 finished with value: 0.7697791018328823 and parameters: {'observation_period_num': 134, 'train_rates': 0.7740671318365094, 'learning_rate': 0.00014018624669726344, 'batch_size': 241, 'step_size': 11, 'gamma': 0.9397577671237459}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:10:34,332][0m Trial 30 finished with value: 1.137530381787461 and parameters: {'observation_period_num': 114, 'train_rates': 0.8289185904945118, 'learning_rate': 5.558318108904111e-06, 'batch_size': 200, 'step_size': 12, 'gamma': 0.9111322389477015}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:15:40,849][0m Trial 31 finished with value: 0.26067692041397095 and parameters: {'observation_period_num': 244, 'train_rates': 0.9338914509127172, 'learning_rate': 0.0002155458609501883, 'batch_size': 218, 'step_size': 15, 'gamma': 0.7988972386029517}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:19:54,756][0m Trial 32 finished with value: 0.3835605978965759 and parameters: {'observation_period_num': 213, 'train_rates': 0.9140987582900392, 'learning_rate': 8.754538677450826e-05, 'batch_size': 239, 'step_size': 15, 'gamma': 0.7938944835187464}. Best is trial 9 with value: 0.24469837001787864.[0m
[32m[I 2025-02-07 09:25:18,624][0m Trial 33 finished with value: 0.23718006908893585 and parameters: {'observation_period_num': 252, 'train_rates': 0.9521435462452029, 'learning_rate': 0.00046431233540970856, 'batch_size': 177, 'step_size': 14, 'gamma': 0.7731472678150284}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:30:19,208][0m Trial 34 finished with value: 0.2658139765262604 and parameters: {'observation_period_num': 235, 'train_rates': 0.9615378562875377, 'learning_rate': 0.0005543952103223677, 'batch_size': 165, 'step_size': 14, 'gamma': 0.7692877444400206}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:34:18,902][0m Trial 35 finished with value: 0.2912454004834799 and parameters: {'observation_period_num': 207, 'train_rates': 0.8715128370993853, 'learning_rate': 0.0003720937608521723, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8107218343046191}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:37:56,573][0m Trial 36 finished with value: 0.3165259063243866 and parameters: {'observation_period_num': 185, 'train_rates': 0.9549371036243023, 'learning_rate': 5.352208882799361e-05, 'batch_size': 183, 'step_size': 13, 'gamma': 0.9863562285963647}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:43:04,340][0m Trial 37 finished with value: 0.23876476287841797 and parameters: {'observation_period_num': 236, 'train_rates': 0.9899111449834594, 'learning_rate': 0.0007382435810692405, 'batch_size': 203, 'step_size': 14, 'gamma': 0.7603928628194808}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:43:31,879][0m Trial 38 finished with value: 0.26159319281578064 and parameters: {'observation_period_num': 5, 'train_rates': 0.98932130599469, 'learning_rate': 0.0007224267923429223, 'batch_size': 199, 'step_size': 12, 'gamma': 0.7574450133908821}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:48:40,321][0m Trial 39 finished with value: 0.2570441663265228 and parameters: {'observation_period_num': 239, 'train_rates': 0.964182711102317, 'learning_rate': 0.0005218803062398342, 'batch_size': 152, 'step_size': 10, 'gamma': 0.7774196394961707}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:50:12,040][0m Trial 40 finished with value: 0.45323227800821003 and parameters: {'observation_period_num': 91, 'train_rates': 0.900711809299096, 'learning_rate': 0.0009987941215710665, 'batch_size': 227, 'step_size': 13, 'gamma': 0.9526690206321361}. Best is trial 33 with value: 0.23718006908893585.[0m
[32m[I 2025-02-07 09:54:42,649][0m Trial 41 finished with value: 0.21215413361787797 and parameters: {'observation_period_num': 217, 'train_rates': 0.9454345142963176, 'learning_rate': 0.00028411700362331813, 'batch_size': 117, 'step_size': 14, 'gamma': 0.7639038462738393}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 09:59:13,046][0m Trial 42 finished with value: 0.2351374812424183 and parameters: {'observation_period_num': 217, 'train_rates': 0.948845804023138, 'learning_rate': 0.0004481098323196578, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7522407774082495}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:03:36,878][0m Trial 43 finished with value: 0.26368788792597536 and parameters: {'observation_period_num': 218, 'train_rates': 0.9449447485740479, 'learning_rate': 0.0004368647482848339, 'batch_size': 104, 'step_size': 11, 'gamma': 0.7501155322502138}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:07:34,005][0m Trial 44 finished with value: 0.24911269545555115 and parameters: {'observation_period_num': 196, 'train_rates': 0.9760710590697471, 'learning_rate': 0.0005004726111130486, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7597929044899385}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:08:34,197][0m Trial 45 finished with value: 0.42882972955703735 and parameters: {'observation_period_num': 46, 'train_rates': 0.9778765488826133, 'learning_rate': 0.0006859613298606681, 'batch_size': 87, 'step_size': 13, 'gamma': 0.7639256874649157}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:12:12,393][0m Trial 46 finished with value: 1.0038634304578105 and parameters: {'observation_period_num': 232, 'train_rates': 0.6474460382936063, 'learning_rate': 0.0003152777133960466, 'batch_size': 122, 'step_size': 12, 'gamma': 0.767169598761864}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:16:26,881][0m Trial 47 finished with value: 0.22985759377479553 and parameters: {'observation_period_num': 210, 'train_rates': 0.9570538281219482, 'learning_rate': 0.0002758344150694629, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7516930990143953}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:20:47,576][0m Trial 48 finished with value: 0.2214834262060639 and parameters: {'observation_period_num': 209, 'train_rates': 0.9458588650176286, 'learning_rate': 0.00029828728228907937, 'batch_size': 92, 'step_size': 14, 'gamma': 0.7530603375247509}. Best is trial 41 with value: 0.21215413361787797.[0m
[32m[I 2025-02-07 10:25:00,727][0m Trial 49 finished with value: 0.2752578543945098 and parameters: {'observation_period_num': 203, 'train_rates': 0.9486179035128729, 'learning_rate': 0.00010876620490216569, 'batch_size': 95, 'step_size': 11, 'gamma': 0.7517813643017417}. Best is trial 41 with value: 0.21215413361787797.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-07 10:25:00,735][0m A new study created in memory with name: no-name-f8846db9-6799-4896-bf22-270341b712df[0m
[32m[I 2025-02-07 10:25:49,181][0m Trial 0 finished with value: 1.975180613077604 and parameters: {'observation_period_num': 54, 'train_rates': 0.7086396691456617, 'learning_rate': 1.1746159766243174e-06, 'batch_size': 143, 'step_size': 3, 'gamma': 0.8467626117771208}. Best is trial 0 with value: 1.975180613077604.[0m
[32m[I 2025-02-07 10:27:32,411][0m Trial 1 finished with value: 2.124109983444214 and parameters: {'observation_period_num': 100, 'train_rates': 0.9634217037262804, 'learning_rate': 1.4264251130781171e-06, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8232225832247578}. Best is trial 0 with value: 1.975180613077604.[0m
[32m[I 2025-02-07 10:29:56,603][0m Trial 2 finished with value: 1.4815184883773327 and parameters: {'observation_period_num': 126, 'train_rates': 0.9436545156158422, 'learning_rate': 3.1165535319243696e-06, 'batch_size': 63, 'step_size': 6, 'gamma': 0.7725847426480601}. Best is trial 2 with value: 1.4815184883773327.[0m
[32m[I 2025-02-07 10:33:43,264][0m Trial 3 finished with value: 0.9371769821510854 and parameters: {'observation_period_num': 166, 'train_rates': 0.7267253921611453, 'learning_rate': 4.830964319475456e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8401990339837309}. Best is trial 3 with value: 0.9371769821510854.[0m
[32m[I 2025-02-07 10:35:07,647][0m Trial 4 finished with value: 0.23531943559646606 and parameters: {'observation_period_num': 77, 'train_rates': 0.9886409987796301, 'learning_rate': 0.00011986736655238938, 'batch_size': 159, 'step_size': 14, 'gamma': 0.9606002838423989}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:37:51,960][0m Trial 5 finished with value: 2.19921385451575 and parameters: {'observation_period_num': 160, 'train_rates': 0.8706953010722595, 'learning_rate': 1.6195637911763736e-06, 'batch_size': 218, 'step_size': 2, 'gamma': 0.8601796216567095}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:39:29,030][0m Trial 6 finished with value: 0.5706792681846978 and parameters: {'observation_period_num': 104, 'train_rates': 0.79568088737666, 'learning_rate': 0.00038263462751083195, 'batch_size': 199, 'step_size': 13, 'gamma': 0.8642542719629918}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:43:36,392][0m Trial 7 finished with value: 1.0345977638782091 and parameters: {'observation_period_num': 239, 'train_rates': 0.6986688039308714, 'learning_rate': 0.0006520703154322699, 'batch_size': 123, 'step_size': 1, 'gamma': 0.9716696057153844}. Best is trial 4 with value: 0.23531943559646606.[0m
Early stopping at epoch 48
[32m[I 2025-02-07 10:44:09,467][0m Trial 8 finished with value: 1.8275681195270261 and parameters: {'observation_period_num': 75, 'train_rates': 0.7032982006709942, 'learning_rate': 6.242581528008899e-05, 'batch_size': 176, 'step_size': 1, 'gamma': 0.7670015582223655}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:47:01,189][0m Trial 9 finished with value: 1.064029996172375 and parameters: {'observation_period_num': 194, 'train_rates': 0.6199121007539359, 'learning_rate': 7.227870662689483e-05, 'batch_size': 206, 'step_size': 1, 'gamma': 0.9841957360754181}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:47:33,634][0m Trial 10 finished with value: 0.769085179760818 and parameters: {'observation_period_num': 32, 'train_rates': 0.8695206580249599, 'learning_rate': 1.53874581155155e-05, 'batch_size': 254, 'step_size': 11, 'gamma': 0.9244627289388911}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:48:09,276][0m Trial 11 finished with value: 0.7141865500032085 and parameters: {'observation_period_num': 13, 'train_rates': 0.8262041662688543, 'learning_rate': 0.0005347318031230697, 'batch_size': 129, 'step_size': 15, 'gamma': 0.9084024560139701}. Best is trial 4 with value: 0.23531943559646606.[0m
[32m[I 2025-02-07 10:49:50,703][0m Trial 12 finished with value: 0.166324645280838 and parameters: {'observation_period_num': 93, 'train_rates': 0.981224654566133, 'learning_rate': 0.00021092434120410967, 'batch_size': 158, 'step_size': 12, 'gamma': 0.9045985755132188}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:51:11,489][0m Trial 13 finished with value: 0.18442608416080475 and parameters: {'observation_period_num': 68, 'train_rates': 0.9850242297800798, 'learning_rate': 0.00015332890899773854, 'batch_size': 101, 'step_size': 12, 'gamma': 0.9305649192906398}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:52:11,898][0m Trial 14 finished with value: 0.36650603111894403 and parameters: {'observation_period_num': 53, 'train_rates': 0.9161258308657185, 'learning_rate': 0.0001980423371656116, 'batch_size': 93, 'step_size': 11, 'gamma': 0.9109879889300516}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:54:35,687][0m Trial 15 finished with value: 0.5994302021249276 and parameters: {'observation_period_num': 130, 'train_rates': 0.9198109275788602, 'learning_rate': 1.3928775475724259e-05, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9352613022349008}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:56:23,608][0m Trial 16 finished with value: 0.32814703237781834 and parameters: {'observation_period_num': 90, 'train_rates': 0.8817136944634048, 'learning_rate': 0.0002463373348725029, 'batch_size': 47, 'step_size': 8, 'gamma': 0.8832729806536185}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:57:19,180][0m Trial 17 finished with value: 0.7927613258361816 and parameters: {'observation_period_num': 45, 'train_rates': 0.9885899331369423, 'learning_rate': 0.0009972384315604984, 'batch_size': 100, 'step_size': 8, 'gamma': 0.940520358782673}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 10:57:49,480][0m Trial 18 finished with value: 0.6160401368940343 and parameters: {'observation_period_num': 13, 'train_rates': 0.8174096852758772, 'learning_rate': 2.9492173417681458e-05, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8918530949849316}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:00:04,409][0m Trial 19 finished with value: 0.5329388249211195 and parameters: {'observation_period_num': 128, 'train_rates': 0.913096325582663, 'learning_rate': 6.254565934285196e-05, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8029023884609799}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:03:18,126][0m Trial 20 finished with value: 0.3052736688269316 and parameters: {'observation_period_num': 162, 'train_rates': 0.951224823719337, 'learning_rate': 0.00013954660300361538, 'batch_size': 65, 'step_size': 13, 'gamma': 0.9532974491701616}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:04:40,300][0m Trial 21 finished with value: 0.17885223031044006 and parameters: {'observation_period_num': 75, 'train_rates': 0.9820860656213128, 'learning_rate': 0.00011735181215544219, 'batch_size': 162, 'step_size': 14, 'gamma': 0.9655008245553858}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:05:51,857][0m Trial 22 finished with value: 0.49502718448638916 and parameters: {'observation_period_num': 66, 'train_rates': 0.9614682683240109, 'learning_rate': 3.298855715750314e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.9849714121220186}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:07:53,059][0m Trial 23 finished with value: 0.18752560019493103 and parameters: {'observation_period_num': 113, 'train_rates': 0.9857902495884739, 'learning_rate': 0.0002873212806520291, 'batch_size': 230, 'step_size': 11, 'gamma': 0.9052887952059969}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:08:30,119][0m Trial 24 finished with value: 0.33315531648818414 and parameters: {'observation_period_num': 32, 'train_rates': 0.8965756234683001, 'learning_rate': 0.00010843563999375974, 'batch_size': 146, 'step_size': 14, 'gamma': 0.952057933465378}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:09:55,813][0m Trial 25 finished with value: 0.4875126343094901 and parameters: {'observation_period_num': 84, 'train_rates': 0.8450243391677346, 'learning_rate': 3.4752715331233214e-05, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9228031026539647}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:10:58,321][0m Trial 26 finished with value: 0.38644105983521654 and parameters: {'observation_period_num': 62, 'train_rates': 0.9333002002346054, 'learning_rate': 0.00015865654135470148, 'batch_size': 180, 'step_size': 10, 'gamma': 0.9660031212265108}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:13:22,114][0m Trial 27 finished with value: 0.6982324799494957 and parameters: {'observation_period_num': 145, 'train_rates': 0.7614478937734664, 'learning_rate': 0.0004157483859997578, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8882772923344451}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:14:47,267][0m Trial 28 finished with value: 0.9432789260463323 and parameters: {'observation_period_num': 96, 'train_rates': 0.641831860769366, 'learning_rate': 1.7562801165418033e-05, 'batch_size': 79, 'step_size': 14, 'gamma': 0.9370999619982605}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:15:36,725][0m Trial 29 finished with value: 0.36223921179771423 and parameters: {'observation_period_num': 40, 'train_rates': 0.9632279064127743, 'learning_rate': 8.467836489606237e-05, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9216981164054507}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:17:31,693][0m Trial 30 finished with value: 0.4570869078588246 and parameters: {'observation_period_num': 114, 'train_rates': 0.930057724438501, 'learning_rate': 4.492552989382125e-05, 'batch_size': 152, 'step_size': 12, 'gamma': 0.87764092404307}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:19:32,643][0m Trial 31 finished with value: 0.19916141033172607 and parameters: {'observation_period_num': 113, 'train_rates': 0.9865755881872422, 'learning_rate': 0.0002808145742911637, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9053673754512043}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:20:45,896][0m Trial 32 finished with value: 0.2539478540420532 and parameters: {'observation_period_num': 69, 'train_rates': 0.9720381596799034, 'learning_rate': 0.00029181967449687667, 'batch_size': 256, 'step_size': 9, 'gamma': 0.896886313242257}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:22:17,398][0m Trial 33 finished with value: 0.35519182682037354 and parameters: {'observation_period_num': 92, 'train_rates': 0.9476838419273196, 'learning_rate': 0.0001896746702535119, 'batch_size': 233, 'step_size': 10, 'gamma': 0.9469320781469805}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:24:08,323][0m Trial 34 finished with value: 0.5076604519210519 and parameters: {'observation_period_num': 110, 'train_rates': 0.90045961146369, 'learning_rate': 0.0009168089147389258, 'batch_size': 186, 'step_size': 15, 'gamma': 0.8405519127539081}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:26:45,878][0m Trial 35 finished with value: 0.3034493327140808 and parameters: {'observation_period_num': 144, 'train_rates': 0.9697614421847391, 'learning_rate': 0.00010064760594628446, 'batch_size': 202, 'step_size': 13, 'gamma': 0.9244829239992579}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:30:44,971][0m Trial 36 finished with value: 0.9513001527614936 and parameters: {'observation_period_num': 185, 'train_rates': 0.9397052842988118, 'learning_rate': 0.000447710796691466, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9743610699792992}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:31:46,500][0m Trial 37 finished with value: 0.18285098671913147 and parameters: {'observation_period_num': 55, 'train_rates': 0.9832495168902219, 'learning_rate': 0.000291503339844574, 'batch_size': 135, 'step_size': 12, 'gamma': 0.8588246974117332}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:32:53,254][0m Trial 38 finished with value: 1.214778226493989 and parameters: {'observation_period_num': 62, 'train_rates': 0.948511571971246, 'learning_rate': 7.001637237015145e-06, 'batch_size': 137, 'step_size': 14, 'gamma': 0.834216900418849}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:34:02,790][0m Trial 39 finished with value: 1.0893903195109826 and parameters: {'observation_period_num': 81, 'train_rates': 0.6673071579342509, 'learning_rate': 0.0006653022643699581, 'batch_size': 164, 'step_size': 12, 'gamma': 0.857065894211297}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:35:00,691][0m Trial 40 finished with value: 0.6679550298305501 and parameters: {'observation_period_num': 25, 'train_rates': 0.7618880393319107, 'learning_rate': 0.00013747812045481733, 'batch_size': 74, 'step_size': 7, 'gamma': 0.7993278212926502}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:35:55,304][0m Trial 41 finished with value: 0.17704515159130096 and parameters: {'observation_period_num': 47, 'train_rates': 0.9831984155883814, 'learning_rate': 0.00031301548941976533, 'batch_size': 129, 'step_size': 11, 'gamma': 0.8692884559354898}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:36:46,598][0m Trial 42 finished with value: 0.26421162486076355 and parameters: {'observation_period_num': 43, 'train_rates': 0.9701005565485148, 'learning_rate': 0.00021220377149274222, 'batch_size': 127, 'step_size': 14, 'gamma': 0.8708218395867053}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:37:44,545][0m Trial 43 finished with value: 0.18585753440856934 and parameters: {'observation_period_num': 56, 'train_rates': 0.9771434846566682, 'learning_rate': 0.0003391428592868308, 'batch_size': 145, 'step_size': 12, 'gamma': 0.8190423734768387}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:39:06,907][0m Trial 44 finished with value: 0.6829470341474237 and parameters: {'observation_period_num': 74, 'train_rates': 0.9509435775933434, 'learning_rate': 0.0006403091372369731, 'batch_size': 115, 'step_size': 15, 'gamma': 0.8528996468003396}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:40:05,260][0m Trial 45 finished with value: 0.3988402756360861 and parameters: {'observation_period_num': 51, 'train_rates': 0.9330278320571009, 'learning_rate': 5.075471369906964e-05, 'batch_size': 103, 'step_size': 13, 'gamma': 0.869272130982686}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:40:35,988][0m Trial 46 finished with value: 1.9951475383313888 and parameters: {'observation_period_num': 23, 'train_rates': 0.8890157658449027, 'learning_rate': 1.064334519312602e-06, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8274729628601944}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:45:16,608][0m Trial 47 finished with value: 0.4151507321485253 and parameters: {'observation_period_num': 237, 'train_rates': 0.8576733688178045, 'learning_rate': 0.00017309819169904038, 'batch_size': 125, 'step_size': 10, 'gamma': 0.8770247893320039}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:46:48,810][0m Trial 48 finished with value: 0.2481558620929718 and parameters: {'observation_period_num': 85, 'train_rates': 0.989747203175983, 'learning_rate': 0.0005071175255051627, 'batch_size': 190, 'step_size': 13, 'gamma': 0.8468251866920729}. Best is trial 12 with value: 0.166324645280838.[0m
[32m[I 2025-02-07 11:48:37,469][0m Trial 49 finished with value: 0.3029039765689887 and parameters: {'observation_period_num': 101, 'train_rates': 0.9107651021545535, 'learning_rate': 8.351840626938696e-05, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8980438729379743}. Best is trial 12 with value: 0.166324645280838.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 188, 'train_rates': 0.9769046115214166, 'learning_rate': 0.00014611903728488612, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8156896515554278}
Epoch 1/300, trend Loss: 0.8595 | 1.3512
Epoch 2/300, trend Loss: 0.6880 | 0.9529
Epoch 3/300, trend Loss: 0.5553 | 0.8838
Epoch 4/300, trend Loss: 0.4637 | 0.8293
Epoch 5/300, trend Loss: 0.5172 | 0.7530
Epoch 6/300, trend Loss: 0.4208 | 0.7389
Epoch 7/300, trend Loss: 0.3983 | 0.6607
Epoch 8/300, trend Loss: 0.3969 | 0.6371
Epoch 9/300, trend Loss: 0.3621 | 0.6222
Epoch 10/300, trend Loss: 0.3125 | 0.5643
Epoch 11/300, trend Loss: 0.3330 | 0.5602
Epoch 12/300, trend Loss: 0.3228 | 0.5256
Epoch 13/300, trend Loss: 0.3466 | 0.5280
Epoch 14/300, trend Loss: 0.2985 | 0.4942
Epoch 15/300, trend Loss: 0.3266 | 0.4729
Epoch 16/300, trend Loss: 0.2801 | 0.4543
Epoch 17/300, trend Loss: 0.2890 | 0.4510
Epoch 18/300, trend Loss: 0.2839 | 0.4389
Epoch 19/300, trend Loss: 0.2621 | 0.4233
Epoch 20/300, trend Loss: 0.2642 | 0.4076
Epoch 21/300, trend Loss: 0.2416 | 0.3977
Epoch 22/300, trend Loss: 0.2358 | 0.3850
Epoch 23/300, trend Loss: 0.2334 | 0.3837
Epoch 24/300, trend Loss: 0.2249 | 0.3689
Epoch 25/300, trend Loss: 0.2245 | 0.3678
Epoch 26/300, trend Loss: 0.2142 | 0.3553
Epoch 27/300, trend Loss: 0.2093 | 0.3470
Epoch 28/300, trend Loss: 0.2086 | 0.3428
Epoch 29/300, trend Loss: 0.2015 | 0.3318
Epoch 30/300, trend Loss: 0.2008 | 0.3329
Epoch 31/300, trend Loss: 0.2012 | 0.3210
Epoch 32/300, trend Loss: 0.1978 | 0.3192
Epoch 33/300, trend Loss: 0.1959 | 0.3155
Epoch 34/300, trend Loss: 0.1930 | 0.3092
Epoch 35/300, trend Loss: 0.1920 | 0.3048
Epoch 36/300, trend Loss: 0.1942 | 0.3036
Epoch 37/300, trend Loss: 0.1892 | 0.2968
Epoch 38/300, trend Loss: 0.1866 | 0.2956
Epoch 39/300, trend Loss: 0.1845 | 0.2907
Epoch 40/300, trend Loss: 0.1829 | 0.2856
Epoch 41/300, trend Loss: 0.1813 | 0.2868
Epoch 42/300, trend Loss: 0.1796 | 0.2785
Epoch 43/300, trend Loss: 0.1797 | 0.2796
Epoch 44/300, trend Loss: 0.1783 | 0.2752
Epoch 45/300, trend Loss: 0.1765 | 0.2726
Epoch 46/300, trend Loss: 0.1755 | 0.2714
Epoch 47/300, trend Loss: 0.1754 | 0.2685
Epoch 48/300, trend Loss: 0.1733 | 0.2645
Epoch 49/300, trend Loss: 0.1728 | 0.2653
Epoch 50/300, trend Loss: 0.1708 | 0.2611
Epoch 51/300, trend Loss: 0.1700 | 0.2604
Epoch 52/300, trend Loss: 0.1697 | 0.2588
Epoch 53/300, trend Loss: 0.1693 | 0.2551
Epoch 54/300, trend Loss: 0.1677 | 0.2543
Epoch 55/300, trend Loss: 0.1672 | 0.2518
Epoch 56/300, trend Loss: 0.1665 | 0.2498
Epoch 57/300, trend Loss: 0.1664 | 0.2485
Epoch 58/300, trend Loss: 0.1658 | 0.2470
Epoch 59/300, trend Loss: 0.1649 | 0.2453
Epoch 60/300, trend Loss: 0.1648 | 0.2439
Epoch 61/300, trend Loss: 0.1637 | 0.2430
Epoch 62/300, trend Loss: 0.1636 | 0.2417
Epoch 63/300, trend Loss: 0.1629 | 0.2402
Epoch 64/300, trend Loss: 0.1631 | 0.2389
Epoch 65/300, trend Loss: 0.1617 | 0.2380
Epoch 66/300, trend Loss: 0.1625 | 0.2375
Epoch 67/300, trend Loss: 0.1615 | 0.2354
Epoch 68/300, trend Loss: 0.1615 | 0.2349
Epoch 69/300, trend Loss: 0.1599 | 0.2341
Epoch 70/300, trend Loss: 0.1601 | 0.2319
Epoch 71/300, trend Loss: 0.1594 | 0.2315
Epoch 72/300, trend Loss: 0.1593 | 0.2314
Epoch 73/300, trend Loss: 0.1585 | 0.2300
Epoch 74/300, trend Loss: 0.1585 | 0.2289
Epoch 75/300, trend Loss: 0.1585 | 0.2281
Epoch 76/300, trend Loss: 0.1578 | 0.2272
Epoch 77/300, trend Loss: 0.1575 | 0.2267
Epoch 78/300, trend Loss: 0.1579 | 0.2255
Epoch 79/300, trend Loss: 0.1571 | 0.2248
Epoch 80/300, trend Loss: 0.1564 | 0.2239
Epoch 81/300, trend Loss: 0.1564 | 0.2231
Epoch 82/300, trend Loss: 0.1565 | 0.2228
Epoch 83/300, trend Loss: 0.1560 | 0.2229
Epoch 84/300, trend Loss: 0.1549 | 0.2220
Epoch 85/300, trend Loss: 0.1564 | 0.2210
Epoch 86/300, trend Loss: 0.1560 | 0.2209
Epoch 87/300, trend Loss: 0.1555 | 0.2208
Epoch 88/300, trend Loss: 0.1550 | 0.2200
Epoch 89/300, trend Loss: 0.1544 | 0.2181
Epoch 90/300, trend Loss: 0.1542 | 0.2180
Epoch 91/300, trend Loss: 0.1536 | 0.2176
Epoch 92/300, trend Loss: 0.1534 | 0.2180
Epoch 93/300, trend Loss: 0.1536 | 0.2171
Epoch 94/300, trend Loss: 0.1534 | 0.2162
Epoch 95/300, trend Loss: 0.1536 | 0.2158
Epoch 96/300, trend Loss: 0.1537 | 0.2158
Epoch 97/300, trend Loss: 0.1527 | 0.2146
Epoch 98/300, trend Loss: 0.1532 | 0.2145
Epoch 99/300, trend Loss: 0.1523 | 0.2142
Epoch 100/300, trend Loss: 0.1525 | 0.2143
Epoch 101/300, trend Loss: 0.1524 | 0.2143
Epoch 102/300, trend Loss: 0.1528 | 0.2136
Epoch 103/300, trend Loss: 0.1521 | 0.2128
Epoch 104/300, trend Loss: 0.1523 | 0.2128
Epoch 105/300, trend Loss: 0.1517 | 0.2122
Epoch 106/300, trend Loss: 0.1515 | 0.2116
Epoch 107/300, trend Loss: 0.1512 | 0.2121
Epoch 108/300, trend Loss: 0.1505 | 0.2112
Epoch 109/300, trend Loss: 0.1514 | 0.2113
Epoch 110/300, trend Loss: 0.1516 | 0.2107
Epoch 111/300, trend Loss: 0.1509 | 0.2108
Epoch 112/300, trend Loss: 0.1510 | 0.2107
Epoch 113/300, trend Loss: 0.1505 | 0.2102
Epoch 114/300, trend Loss: 0.1507 | 0.2099
Epoch 115/300, trend Loss: 0.1505 | 0.2095
Epoch 116/300, trend Loss: 0.1508 | 0.2095
Epoch 117/300, trend Loss: 0.1505 | 0.2087
Epoch 118/300, trend Loss: 0.1502 | 0.2089
Epoch 119/300, trend Loss: 0.1497 | 0.2084
Epoch 120/300, trend Loss: 0.1497 | 0.2082
Epoch 121/300, trend Loss: 0.1503 | 0.2084
Epoch 122/300, trend Loss: 0.1502 | 0.2083
Epoch 123/300, trend Loss: 0.1495 | 0.2082
Epoch 124/300, trend Loss: 0.1497 | 0.2082
Epoch 125/300, trend Loss: 0.1501 | 0.2082
Epoch 126/300, trend Loss: 0.1496 | 0.2077
Epoch 127/300, trend Loss: 0.1498 | 0.2074
Epoch 128/300, trend Loss: 0.1495 | 0.2072
Epoch 129/300, trend Loss: 0.1488 | 0.2070
Epoch 130/300, trend Loss: 0.1498 | 0.2068
Epoch 131/300, trend Loss: 0.1492 | 0.2068
Epoch 132/300, trend Loss: 0.1493 | 0.2067
Epoch 133/300, trend Loss: 0.1491 | 0.2069
Epoch 134/300, trend Loss: 0.1493 | 0.2068
Epoch 135/300, trend Loss: 0.1502 | 0.2061
Epoch 136/300, trend Loss: 0.1494 | 0.2061
Epoch 137/300, trend Loss: 0.1493 | 0.2060
Epoch 138/300, trend Loss: 0.1494 | 0.2060
Epoch 139/300, trend Loss: 0.1489 | 0.2061
Epoch 140/300, trend Loss: 0.1490 | 0.2058
Epoch 141/300, trend Loss: 0.1491 | 0.2056
Epoch 142/300, trend Loss: 0.1486 | 0.2056
Epoch 143/300, trend Loss: 0.1489 | 0.2054
Epoch 144/300, trend Loss: 0.1489 | 0.2050
Epoch 145/300, trend Loss: 0.1489 | 0.2048
Epoch 146/300, trend Loss: 0.1491 | 0.2047
Epoch 147/300, trend Loss: 0.1478 | 0.2048
Epoch 148/300, trend Loss: 0.1488 | 0.2050
Epoch 149/300, trend Loss: 0.1484 | 0.2049
Epoch 150/300, trend Loss: 0.1486 | 0.2046
Epoch 151/300, trend Loss: 0.1486 | 0.2046
Epoch 152/300, trend Loss: 0.1486 | 0.2047
Epoch 153/300, trend Loss: 0.1490 | 0.2046
Epoch 154/300, trend Loss: 0.1483 | 0.2044
Epoch 155/300, trend Loss: 0.1488 | 0.2044
Epoch 156/300, trend Loss: 0.1490 | 0.2043
Epoch 157/300, trend Loss: 0.1487 | 0.2043
Epoch 158/300, trend Loss: 0.1484 | 0.2042
Epoch 159/300, trend Loss: 0.1493 | 0.2042
Epoch 160/300, trend Loss: 0.1486 | 0.2041
Epoch 161/300, trend Loss: 0.1481 | 0.2040
Epoch 162/300, trend Loss: 0.1481 | 0.2038
Epoch 163/300, trend Loss: 0.1482 | 0.2038
Epoch 164/300, trend Loss: 0.1481 | 0.2038
Epoch 165/300, trend Loss: 0.1490 | 0.2039
Epoch 166/300, trend Loss: 0.1480 | 0.2038
Epoch 167/300, trend Loss: 0.1480 | 0.2038
Epoch 168/300, trend Loss: 0.1482 | 0.2038
Epoch 169/300, trend Loss: 0.1483 | 0.2038
Epoch 170/300, trend Loss: 0.1480 | 0.2037
Epoch 171/300, trend Loss: 0.1486 | 0.2037
Epoch 172/300, trend Loss: 0.1479 | 0.2037
Epoch 173/300, trend Loss: 0.1478 | 0.2036
Epoch 174/300, trend Loss: 0.1474 | 0.2036
Epoch 175/300, trend Loss: 0.1485 | 0.2035
Epoch 176/300, trend Loss: 0.1476 | 0.2035
Epoch 177/300, trend Loss: 0.1480 | 0.2035
Epoch 178/300, trend Loss: 0.1477 | 0.2035
Epoch 179/300, trend Loss: 0.1477 | 0.2035
Epoch 180/300, trend Loss: 0.1483 | 0.2034
Epoch 181/300, trend Loss: 0.1480 | 0.2035
Epoch 182/300, trend Loss: 0.1477 | 0.2034
Epoch 183/300, trend Loss: 0.1482 | 0.2034
Epoch 184/300, trend Loss: 0.1485 | 0.2033
Epoch 185/300, trend Loss: 0.1477 | 0.2033
Epoch 186/300, trend Loss: 0.1480 | 0.2032
Epoch 187/300, trend Loss: 0.1483 | 0.2032
Epoch 188/300, trend Loss: 0.1479 | 0.2031
Epoch 189/300, trend Loss: 0.1474 | 0.2031
Epoch 190/300, trend Loss: 0.1471 | 0.2031
Epoch 191/300, trend Loss: 0.1478 | 0.2031
Epoch 192/300, trend Loss: 0.1478 | 0.2032
Epoch 193/300, trend Loss: 0.1481 | 0.2032
Epoch 194/300, trend Loss: 0.1479 | 0.2032
Epoch 195/300, trend Loss: 0.1477 | 0.2031
Epoch 196/300, trend Loss: 0.1481 | 0.2031
Epoch 197/300, trend Loss: 0.1478 | 0.2030
Epoch 198/300, trend Loss: 0.1474 | 0.2030
Epoch 199/300, trend Loss: 0.1482 | 0.2030
Epoch 200/300, trend Loss: 0.1480 | 0.2030
Epoch 201/300, trend Loss: 0.1481 | 0.2030
Epoch 202/300, trend Loss: 0.1477 | 0.2031
Epoch 203/300, trend Loss: 0.1476 | 0.2030
Epoch 204/300, trend Loss: 0.1472 | 0.2030
Epoch 205/300, trend Loss: 0.1476 | 0.2030
Epoch 206/300, trend Loss: 0.1475 | 0.2029
Epoch 207/300, trend Loss: 0.1474 | 0.2029
Epoch 208/300, trend Loss: 0.1481 | 0.2029
Epoch 209/300, trend Loss: 0.1474 | 0.2028
Epoch 210/300, trend Loss: 0.1479 | 0.2028
Epoch 211/300, trend Loss: 0.1478 | 0.2028
Epoch 212/300, trend Loss: 0.1473 | 0.2027
Epoch 213/300, trend Loss: 0.1476 | 0.2027
Epoch 214/300, trend Loss: 0.1478 | 0.2027
Epoch 215/300, trend Loss: 0.1482 | 0.2027
Epoch 216/300, trend Loss: 0.1475 | 0.2027
Epoch 217/300, trend Loss: 0.1477 | 0.2027
Epoch 218/300, trend Loss: 0.1470 | 0.2027
Epoch 219/300, trend Loss: 0.1483 | 0.2027
Epoch 220/300, trend Loss: 0.1477 | 0.2027
Epoch 221/300, trend Loss: 0.1480 | 0.2027
Epoch 222/300, trend Loss: 0.1481 | 0.2027
Epoch 223/300, trend Loss: 0.1476 | 0.2027
Epoch 224/300, trend Loss: 0.1471 | 0.2027
Epoch 225/300, trend Loss: 0.1483 | 0.2027
Epoch 226/300, trend Loss: 0.1476 | 0.2027
Epoch 227/300, trend Loss: 0.1479 | 0.2027
Epoch 228/300, trend Loss: 0.1479 | 0.2027
Epoch 229/300, trend Loss: 0.1478 | 0.2026
Epoch 230/300, trend Loss: 0.1469 | 0.2026
Epoch 231/300, trend Loss: 0.1478 | 0.2026
Epoch 232/300, trend Loss: 0.1480 | 0.2026
Epoch 233/300, trend Loss: 0.1476 | 0.2026
Epoch 234/300, trend Loss: 0.1475 | 0.2026
Epoch 235/300, trend Loss: 0.1480 | 0.2026
Epoch 236/300, trend Loss: 0.1476 | 0.2026
Epoch 237/300, trend Loss: 0.1472 | 0.2026
Epoch 238/300, trend Loss: 0.1469 | 0.2026
Epoch 239/300, trend Loss: 0.1477 | 0.2026
Epoch 240/300, trend Loss: 0.1475 | 0.2026
Epoch 241/300, trend Loss: 0.1480 | 0.2026
Epoch 242/300, trend Loss: 0.1477 | 0.2026
Epoch 243/300, trend Loss: 0.1481 | 0.2026
Epoch 244/300, trend Loss: 0.1481 | 0.2026
Epoch 245/300, trend Loss: 0.1473 | 0.2026
Epoch 246/300, trend Loss: 0.1480 | 0.2026
Epoch 247/300, trend Loss: 0.1479 | 0.2025
Epoch 248/300, trend Loss: 0.1473 | 0.2025
Epoch 249/300, trend Loss: 0.1476 | 0.2025
Epoch 250/300, trend Loss: 0.1472 | 0.2025
Epoch 251/300, trend Loss: 0.1480 | 0.2025
Epoch 252/300, trend Loss: 0.1471 | 0.2025
Epoch 253/300, trend Loss: 0.1479 | 0.2025
Epoch 254/300, trend Loss: 0.1478 | 0.2025
Epoch 255/300, trend Loss: 0.1480 | 0.2025
Epoch 256/300, trend Loss: 0.1474 | 0.2025
Epoch 257/300, trend Loss: 0.1477 | 0.2025
Epoch 258/300, trend Loss: 0.1469 | 0.2025
Epoch 259/300, trend Loss: 0.1478 | 0.2025
Epoch 260/300, trend Loss: 0.1476 | 0.2025
Epoch 261/300, trend Loss: 0.1481 | 0.2025
Epoch 262/300, trend Loss: 0.1472 | 0.2025
Epoch 263/300, trend Loss: 0.1480 | 0.2025
Epoch 264/300, trend Loss: 0.1478 | 0.2025
Epoch 265/300, trend Loss: 0.1478 | 0.2025
Epoch 266/300, trend Loss: 0.1479 | 0.2025
Epoch 267/300, trend Loss: 0.1473 | 0.2025
Epoch 268/300, trend Loss: 0.1474 | 0.2025
Epoch 269/300, trend Loss: 0.1478 | 0.2025
Epoch 270/300, trend Loss: 0.1482 | 0.2025
Epoch 271/300, trend Loss: 0.1472 | 0.2025
Epoch 272/300, trend Loss: 0.1475 | 0.2025
Epoch 273/300, trend Loss: 0.1473 | 0.2025
Epoch 274/300, trend Loss: 0.1480 | 0.2025
Epoch 275/300, trend Loss: 0.1478 | 0.2025
Epoch 276/300, trend Loss: 0.1472 | 0.2025
Epoch 277/300, trend Loss: 0.1475 | 0.2025
Epoch 278/300, trend Loss: 0.1477 | 0.2025
Epoch 279/300, trend Loss: 0.1475 | 0.2025
Epoch 280/300, trend Loss: 0.1479 | 0.2025
Epoch 281/300, trend Loss: 0.1477 | 0.2025
Epoch 282/300, trend Loss: 0.1476 | 0.2025
Epoch 283/300, trend Loss: 0.1475 | 0.2025
Epoch 284/300, trend Loss: 0.1469 | 0.2025
Epoch 285/300, trend Loss: 0.1475 | 0.2025
Epoch 286/300, trend Loss: 0.1478 | 0.2024
Epoch 287/300, trend Loss: 0.1479 | 0.2024
Epoch 288/300, trend Loss: 0.1473 | 0.2024
Epoch 289/300, trend Loss: 0.1469 | 0.2024
Epoch 290/300, trend Loss: 0.1482 | 0.2024
Epoch 291/300, trend Loss: 0.1473 | 0.2024
Epoch 292/300, trend Loss: 0.1475 | 0.2024
Epoch 293/300, trend Loss: 0.1475 | 0.2024
Epoch 294/300, trend Loss: 0.1470 | 0.2024
Epoch 295/300, trend Loss: 0.1477 | 0.2024
Epoch 296/300, trend Loss: 0.1472 | 0.2024
Epoch 297/300, trend Loss: 0.1475 | 0.2024
Epoch 298/300, trend Loss: 0.1478 | 0.2024
Epoch 299/300, trend Loss: 0.1476 | 0.2024
Epoch 300/300, trend Loss: 0.1477 | 0.2024
Training seasonal_0 component with params: {'observation_period_num': 174, 'train_rates': 0.987787665675257, 'learning_rate': 0.0001648509238731016, 'batch_size': 169, 'step_size': 5, 'gamma': 0.957273278300249}
Epoch 1/300, seasonal_0 Loss: 1.1876 | 2.2190
Epoch 2/300, seasonal_0 Loss: 0.8267 | 1.4807
Epoch 3/300, seasonal_0 Loss: 0.6483 | 1.1359
Epoch 4/300, seasonal_0 Loss: 0.5388 | 1.0069
Epoch 5/300, seasonal_0 Loss: 0.5091 | 0.9295
Epoch 6/300, seasonal_0 Loss: 0.4904 | 0.8846
Epoch 7/300, seasonal_0 Loss: 0.4391 | 0.8017
Epoch 8/300, seasonal_0 Loss: 0.4340 | 0.7592
Epoch 9/300, seasonal_0 Loss: 0.4024 | 0.7442
Epoch 10/300, seasonal_0 Loss: 0.4062 | 0.6993
Epoch 11/300, seasonal_0 Loss: 0.4304 | 0.7188
Epoch 12/300, seasonal_0 Loss: 0.3599 | 0.6413
Epoch 13/300, seasonal_0 Loss: 0.3912 | 0.6287
Epoch 14/300, seasonal_0 Loss: 0.3220 | 0.6112
Epoch 15/300, seasonal_0 Loss: 0.3036 | 0.5839
Epoch 16/300, seasonal_0 Loss: 0.3005 | 0.5589
Epoch 17/300, seasonal_0 Loss: 0.2868 | 0.5389
Epoch 18/300, seasonal_0 Loss: 0.2835 | 0.5146
Epoch 19/300, seasonal_0 Loss: 0.3023 | 0.5163
Epoch 20/300, seasonal_0 Loss: 0.2871 | 0.4839
Epoch 21/300, seasonal_0 Loss: 0.2908 | 0.4872
Epoch 22/300, seasonal_0 Loss: 0.2592 | 0.4640
Epoch 23/300, seasonal_0 Loss: 0.2492 | 0.4510
Epoch 24/300, seasonal_0 Loss: 0.2516 | 0.4402
Epoch 25/300, seasonal_0 Loss: 0.2470 | 0.4302
Epoch 26/300, seasonal_0 Loss: 0.2496 | 0.4185
Epoch 27/300, seasonal_0 Loss: 0.2338 | 0.4112
Epoch 28/300, seasonal_0 Loss: 0.2350 | 0.4047
Epoch 29/300, seasonal_0 Loss: 0.2579 | 0.3935
Epoch 30/300, seasonal_0 Loss: 0.2524 | 0.3882
Epoch 31/300, seasonal_0 Loss: 0.2641 | 0.3904
Epoch 32/300, seasonal_0 Loss: 0.2344 | 0.3779
Epoch 33/300, seasonal_0 Loss: 0.2318 | 0.3738
Epoch 34/300, seasonal_0 Loss: 0.2345 | 0.3698
Epoch 35/300, seasonal_0 Loss: 0.2371 | 0.3587
Epoch 36/300, seasonal_0 Loss: 0.2505 | 0.3613
Epoch 37/300, seasonal_0 Loss: 0.2384 | 0.3507
Epoch 38/300, seasonal_0 Loss: 0.2380 | 0.3462
Epoch 39/300, seasonal_0 Loss: 0.2250 | 0.3444
Epoch 40/300, seasonal_0 Loss: 0.2182 | 0.3287
Epoch 41/300, seasonal_0 Loss: 0.2105 | 0.3354
Epoch 42/300, seasonal_0 Loss: 0.2065 | 0.3168
Epoch 43/300, seasonal_0 Loss: 0.2001 | 0.3228
Epoch 44/300, seasonal_0 Loss: 0.1976 | 0.3088
Epoch 45/300, seasonal_0 Loss: 0.1953 | 0.3121
Epoch 46/300, seasonal_0 Loss: 0.1954 | 0.3014
Epoch 47/300, seasonal_0 Loss: 0.1919 | 0.3044
Epoch 48/300, seasonal_0 Loss: 0.1926 | 0.2948
Epoch 49/300, seasonal_0 Loss: 0.1905 | 0.2958
Epoch 50/300, seasonal_0 Loss: 0.1894 | 0.2892
Epoch 51/300, seasonal_0 Loss: 0.1873 | 0.2880
Epoch 52/300, seasonal_0 Loss: 0.1857 | 0.2860
Epoch 53/300, seasonal_0 Loss: 0.1845 | 0.2811
Epoch 54/300, seasonal_0 Loss: 0.1837 | 0.2809
Epoch 55/300, seasonal_0 Loss: 0.1842 | 0.2776
Epoch 56/300, seasonal_0 Loss: 0.1843 | 0.2749
Epoch 57/300, seasonal_0 Loss: 0.1839 | 0.2725
Epoch 58/300, seasonal_0 Loss: 0.1812 | 0.2689
Epoch 59/300, seasonal_0 Loss: 0.1805 | 0.2692
Epoch 60/300, seasonal_0 Loss: 0.1800 | 0.2654
Epoch 61/300, seasonal_0 Loss: 0.1807 | 0.2616
Epoch 62/300, seasonal_0 Loss: 0.1817 | 0.2612
Epoch 63/300, seasonal_0 Loss: 0.1821 | 0.2575
Epoch 64/300, seasonal_0 Loss: 0.1795 | 0.2585
Epoch 65/300, seasonal_0 Loss: 0.1769 | 0.2525
Epoch 66/300, seasonal_0 Loss: 0.1752 | 0.2565
Epoch 67/300, seasonal_0 Loss: 0.1741 | 0.2495
Epoch 68/300, seasonal_0 Loss: 0.1734 | 0.2519
Epoch 69/300, seasonal_0 Loss: 0.1745 | 0.2486
Epoch 70/300, seasonal_0 Loss: 0.1744 | 0.2477
Epoch 71/300, seasonal_0 Loss: 0.1727 | 0.2457
Epoch 72/300, seasonal_0 Loss: 0.1708 | 0.2424
Epoch 73/300, seasonal_0 Loss: 0.1687 | 0.2436
Epoch 74/300, seasonal_0 Loss: 0.1686 | 0.2404
Epoch 75/300, seasonal_0 Loss: 0.1689 | 0.2395
Epoch 76/300, seasonal_0 Loss: 0.1685 | 0.2384
Epoch 77/300, seasonal_0 Loss: 0.1673 | 0.2346
Epoch 78/300, seasonal_0 Loss: 0.1667 | 0.2352
Epoch 79/300, seasonal_0 Loss: 0.1658 | 0.2317
Epoch 80/300, seasonal_0 Loss: 0.1643 | 0.2331
Epoch 81/300, seasonal_0 Loss: 0.1639 | 0.2309
Epoch 82/300, seasonal_0 Loss: 0.1638 | 0.2304
Epoch 83/300, seasonal_0 Loss: 0.1637 | 0.2291
Epoch 84/300, seasonal_0 Loss: 0.1631 | 0.2282
Epoch 85/300, seasonal_0 Loss: 0.1620 | 0.2253
Epoch 86/300, seasonal_0 Loss: 0.1615 | 0.2260
Epoch 87/300, seasonal_0 Loss: 0.1616 | 0.2244
Epoch 88/300, seasonal_0 Loss: 0.1615 | 0.2240
Epoch 89/300, seasonal_0 Loss: 0.1621 | 0.2219
Epoch 90/300, seasonal_0 Loss: 0.1621 | 0.2223
Epoch 91/300, seasonal_0 Loss: 0.1617 | 0.2190
Epoch 92/300, seasonal_0 Loss: 0.1599 | 0.2199
Epoch 93/300, seasonal_0 Loss: 0.1596 | 0.2167
Epoch 94/300, seasonal_0 Loss: 0.1580 | 0.2184
Epoch 95/300, seasonal_0 Loss: 0.1578 | 0.2141
Epoch 96/300, seasonal_0 Loss: 0.1570 | 0.2140
Epoch 97/300, seasonal_0 Loss: 0.1571 | 0.2147
Epoch 98/300, seasonal_0 Loss: 0.1566 | 0.2125
Epoch 99/300, seasonal_0 Loss: 0.1561 | 0.2135
Epoch 100/300, seasonal_0 Loss: 0.1564 | 0.2098
Epoch 101/300, seasonal_0 Loss: 0.1557 | 0.2124
Epoch 102/300, seasonal_0 Loss: 0.1548 | 0.2087
Epoch 103/300, seasonal_0 Loss: 0.1548 | 0.2101
Epoch 104/300, seasonal_0 Loss: 0.1541 | 0.2072
Epoch 105/300, seasonal_0 Loss: 0.1543 | 0.2085
Epoch 106/300, seasonal_0 Loss: 0.1534 | 0.2058
Epoch 107/300, seasonal_0 Loss: 0.1538 | 0.2063
Epoch 108/300, seasonal_0 Loss: 0.1540 | 0.2068
Epoch 109/300, seasonal_0 Loss: 0.1529 | 0.2040
Epoch 110/300, seasonal_0 Loss: 0.1524 | 0.2050
Epoch 111/300, seasonal_0 Loss: 0.1522 | 0.2033
Epoch 112/300, seasonal_0 Loss: 0.1527 | 0.2043
Epoch 113/300, seasonal_0 Loss: 0.1513 | 0.2028
Epoch 114/300, seasonal_0 Loss: 0.1512 | 0.2024
Epoch 115/300, seasonal_0 Loss: 0.1510 | 0.2011
Epoch 116/300, seasonal_0 Loss: 0.1503 | 0.2022
Epoch 117/300, seasonal_0 Loss: 0.1501 | 0.2009
Epoch 118/300, seasonal_0 Loss: 0.1495 | 0.1993
Epoch 119/300, seasonal_0 Loss: 0.1489 | 0.1990
Epoch 120/300, seasonal_0 Loss: 0.1493 | 0.1980
Epoch 121/300, seasonal_0 Loss: 0.1489 | 0.1971
Epoch 122/300, seasonal_0 Loss: 0.1483 | 0.1969
Epoch 123/300, seasonal_0 Loss: 0.1488 | 0.1959
Epoch 124/300, seasonal_0 Loss: 0.1490 | 0.1950
Epoch 125/300, seasonal_0 Loss: 0.1476 | 0.1951
Epoch 126/300, seasonal_0 Loss: 0.1483 | 0.1945
Epoch 127/300, seasonal_0 Loss: 0.1468 | 0.1955
Epoch 128/300, seasonal_0 Loss: 0.1475 | 0.1937
Epoch 129/300, seasonal_0 Loss: 0.1469 | 0.1946
Epoch 130/300, seasonal_0 Loss: 0.1475 | 0.1927
Epoch 131/300, seasonal_0 Loss: 0.1461 | 0.1936
Epoch 132/300, seasonal_0 Loss: 0.1464 | 0.1919
Epoch 133/300, seasonal_0 Loss: 0.1459 | 0.1920
Epoch 134/300, seasonal_0 Loss: 0.1464 | 0.1921
Epoch 135/300, seasonal_0 Loss: 0.1459 | 0.1905
Epoch 136/300, seasonal_0 Loss: 0.1454 | 0.1902
Epoch 137/300, seasonal_0 Loss: 0.1457 | 0.1903
Epoch 138/300, seasonal_0 Loss: 0.1451 | 0.1904
Epoch 139/300, seasonal_0 Loss: 0.1454 | 0.1896
Epoch 140/300, seasonal_0 Loss: 0.1454 | 0.1900
Epoch 141/300, seasonal_0 Loss: 0.1444 | 0.1889
Epoch 142/300, seasonal_0 Loss: 0.1455 | 0.1901
Epoch 143/300, seasonal_0 Loss: 0.1445 | 0.1877
Epoch 144/300, seasonal_0 Loss: 0.1434 | 0.1886
Epoch 145/300, seasonal_0 Loss: 0.1441 | 0.1885
Epoch 146/300, seasonal_0 Loss: 0.1440 | 0.1881
Epoch 147/300, seasonal_0 Loss: 0.1433 | 0.1884
Epoch 148/300, seasonal_0 Loss: 0.1435 | 0.1879
Epoch 149/300, seasonal_0 Loss: 0.1439 | 0.1872
Epoch 150/300, seasonal_0 Loss: 0.1430 | 0.1868
Epoch 151/300, seasonal_0 Loss: 0.1433 | 0.1857
Epoch 152/300, seasonal_0 Loss: 0.1431 | 0.1863
Epoch 153/300, seasonal_0 Loss: 0.1431 | 0.1859
Epoch 154/300, seasonal_0 Loss: 0.1423 | 0.1850
Epoch 155/300, seasonal_0 Loss: 0.1422 | 0.1842
Epoch 156/300, seasonal_0 Loss: 0.1422 | 0.1851
Epoch 157/300, seasonal_0 Loss: 0.1428 | 0.1854
Epoch 158/300, seasonal_0 Loss: 0.1424 | 0.1849
Epoch 159/300, seasonal_0 Loss: 0.1418 | 0.1839
Epoch 160/300, seasonal_0 Loss: 0.1417 | 0.1842
Epoch 161/300, seasonal_0 Loss: 0.1418 | 0.1835
Epoch 162/300, seasonal_0 Loss: 0.1417 | 0.1832
Epoch 163/300, seasonal_0 Loss: 0.1418 | 0.1833
Epoch 164/300, seasonal_0 Loss: 0.1412 | 0.1837
Epoch 165/300, seasonal_0 Loss: 0.1415 | 0.1831
Epoch 166/300, seasonal_0 Loss: 0.1409 | 0.1823
Epoch 167/300, seasonal_0 Loss: 0.1406 | 0.1818
Epoch 168/300, seasonal_0 Loss: 0.1408 | 0.1817
Epoch 169/300, seasonal_0 Loss: 0.1404 | 0.1811
Epoch 170/300, seasonal_0 Loss: 0.1408 | 0.1812
Epoch 171/300, seasonal_0 Loss: 0.1403 | 0.1805
Epoch 172/300, seasonal_0 Loss: 0.1409 | 0.1809
Epoch 173/300, seasonal_0 Loss: 0.1401 | 0.1804
Epoch 174/300, seasonal_0 Loss: 0.1406 | 0.1802
Epoch 175/300, seasonal_0 Loss: 0.1402 | 0.1798
Epoch 176/300, seasonal_0 Loss: 0.1401 | 0.1802
Epoch 177/300, seasonal_0 Loss: 0.1403 | 0.1796
Epoch 178/300, seasonal_0 Loss: 0.1397 | 0.1789
Epoch 179/300, seasonal_0 Loss: 0.1394 | 0.1789
Epoch 180/300, seasonal_0 Loss: 0.1397 | 0.1791
Epoch 181/300, seasonal_0 Loss: 0.1399 | 0.1790
Epoch 182/300, seasonal_0 Loss: 0.1392 | 0.1783
Epoch 183/300, seasonal_0 Loss: 0.1401 | 0.1785
Epoch 184/300, seasonal_0 Loss: 0.1396 | 0.1785
Epoch 185/300, seasonal_0 Loss: 0.1395 | 0.1781
Epoch 186/300, seasonal_0 Loss: 0.1398 | 0.1779
Epoch 187/300, seasonal_0 Loss: 0.1393 | 0.1778
Epoch 188/300, seasonal_0 Loss: 0.1394 | 0.1774
Epoch 189/300, seasonal_0 Loss: 0.1385 | 0.1770
Epoch 190/300, seasonal_0 Loss: 0.1392 | 0.1765
Epoch 191/300, seasonal_0 Loss: 0.1389 | 0.1771
Epoch 192/300, seasonal_0 Loss: 0.1385 | 0.1770
Epoch 193/300, seasonal_0 Loss: 0.1380 | 0.1771
Epoch 194/300, seasonal_0 Loss: 0.1395 | 0.1771
Epoch 195/300, seasonal_0 Loss: 0.1393 | 0.1764
Epoch 196/300, seasonal_0 Loss: 0.1389 | 0.1757
Epoch 197/300, seasonal_0 Loss: 0.1387 | 0.1762
Epoch 198/300, seasonal_0 Loss: 0.1378 | 0.1762
Epoch 199/300, seasonal_0 Loss: 0.1381 | 0.1761
Epoch 200/300, seasonal_0 Loss: 0.1383 | 0.1763
Epoch 201/300, seasonal_0 Loss: 0.1377 | 0.1763
Epoch 202/300, seasonal_0 Loss: 0.1383 | 0.1756
Epoch 203/300, seasonal_0 Loss: 0.1383 | 0.1756
Epoch 204/300, seasonal_0 Loss: 0.1379 | 0.1756
Epoch 205/300, seasonal_0 Loss: 0.1382 | 0.1761
Epoch 206/300, seasonal_0 Loss: 0.1376 | 0.1758
Epoch 207/300, seasonal_0 Loss: 0.1384 | 0.1754
Epoch 208/300, seasonal_0 Loss: 0.1375 | 0.1750
Epoch 209/300, seasonal_0 Loss: 0.1373 | 0.1754
Epoch 210/300, seasonal_0 Loss: 0.1378 | 0.1754
Epoch 211/300, seasonal_0 Loss: 0.1376 | 0.1748
Epoch 212/300, seasonal_0 Loss: 0.1367 | 0.1741
Epoch 213/300, seasonal_0 Loss: 0.1370 | 0.1742
Epoch 214/300, seasonal_0 Loss: 0.1375 | 0.1743
Epoch 215/300, seasonal_0 Loss: 0.1371 | 0.1744
Epoch 216/300, seasonal_0 Loss: 0.1381 | 0.1743
Epoch 217/300, seasonal_0 Loss: 0.1376 | 0.1740
Epoch 218/300, seasonal_0 Loss: 0.1369 | 0.1738
Epoch 219/300, seasonal_0 Loss: 0.1366 | 0.1739
Epoch 220/300, seasonal_0 Loss: 0.1376 | 0.1736
Epoch 221/300, seasonal_0 Loss: 0.1370 | 0.1744
Epoch 222/300, seasonal_0 Loss: 0.1375 | 0.1739
Epoch 223/300, seasonal_0 Loss: 0.1366 | 0.1736
Epoch 224/300, seasonal_0 Loss: 0.1368 | 0.1739
Epoch 225/300, seasonal_0 Loss: 0.1361 | 0.1736
Epoch 226/300, seasonal_0 Loss: 0.1373 | 0.1733
Epoch 227/300, seasonal_0 Loss: 0.1368 | 0.1733
Epoch 228/300, seasonal_0 Loss: 0.1369 | 0.1731
Epoch 229/300, seasonal_0 Loss: 0.1366 | 0.1734
Epoch 230/300, seasonal_0 Loss: 0.1367 | 0.1733
Epoch 231/300, seasonal_0 Loss: 0.1362 | 0.1734
Epoch 232/300, seasonal_0 Loss: 0.1369 | 0.1737
Epoch 233/300, seasonal_0 Loss: 0.1364 | 0.1736
Epoch 234/300, seasonal_0 Loss: 0.1367 | 0.1725
Epoch 235/300, seasonal_0 Loss: 0.1358 | 0.1719
Epoch 236/300, seasonal_0 Loss: 0.1364 | 0.1717
Epoch 237/300, seasonal_0 Loss: 0.1366 | 0.1722
Epoch 238/300, seasonal_0 Loss: 0.1360 | 0.1716
Epoch 239/300, seasonal_0 Loss: 0.1361 | 0.1719
Epoch 240/300, seasonal_0 Loss: 0.1357 | 0.1722
Epoch 241/300, seasonal_0 Loss: 0.1362 | 0.1718
Epoch 242/300, seasonal_0 Loss: 0.1364 | 0.1721
Epoch 243/300, seasonal_0 Loss: 0.1365 | 0.1720
Epoch 244/300, seasonal_0 Loss: 0.1362 | 0.1717
Epoch 245/300, seasonal_0 Loss: 0.1357 | 0.1718
Epoch 246/300, seasonal_0 Loss: 0.1359 | 0.1718
Epoch 247/300, seasonal_0 Loss: 0.1357 | 0.1716
Epoch 248/300, seasonal_0 Loss: 0.1360 | 0.1715
Epoch 249/300, seasonal_0 Loss: 0.1359 | 0.1714
Epoch 250/300, seasonal_0 Loss: 0.1354 | 0.1713
Epoch 251/300, seasonal_0 Loss: 0.1364 | 0.1714
Epoch 252/300, seasonal_0 Loss: 0.1354 | 0.1714
Epoch 253/300, seasonal_0 Loss: 0.1363 | 0.1713
Epoch 254/300, seasonal_0 Loss: 0.1361 | 0.1713
Epoch 255/300, seasonal_0 Loss: 0.1354 | 0.1713
Epoch 256/300, seasonal_0 Loss: 0.1355 | 0.1715
Epoch 257/300, seasonal_0 Loss: 0.1358 | 0.1712
Epoch 258/300, seasonal_0 Loss: 0.1360 | 0.1711
Epoch 259/300, seasonal_0 Loss: 0.1358 | 0.1708
Epoch 260/300, seasonal_0 Loss: 0.1365 | 0.1709
Epoch 261/300, seasonal_0 Loss: 0.1349 | 0.1711
Epoch 262/300, seasonal_0 Loss: 0.1356 | 0.1709
Epoch 263/300, seasonal_0 Loss: 0.1356 | 0.1709
Epoch 264/300, seasonal_0 Loss: 0.1355 | 0.1704
Epoch 265/300, seasonal_0 Loss: 0.1355 | 0.1705
Epoch 266/300, seasonal_0 Loss: 0.1360 | 0.1708
Epoch 267/300, seasonal_0 Loss: 0.1357 | 0.1710
Epoch 268/300, seasonal_0 Loss: 0.1348 | 0.1706
Epoch 269/300, seasonal_0 Loss: 0.1356 | 0.1705
Epoch 270/300, seasonal_0 Loss: 0.1359 | 0.1703
Epoch 271/300, seasonal_0 Loss: 0.1354 | 0.1702
Epoch 272/300, seasonal_0 Loss: 0.1360 | 0.1700
Epoch 273/300, seasonal_0 Loss: 0.1356 | 0.1697
Epoch 274/300, seasonal_0 Loss: 0.1355 | 0.1697
Epoch 275/300, seasonal_0 Loss: 0.1352 | 0.1697
Epoch 276/300, seasonal_0 Loss: 0.1355 | 0.1702
Epoch 277/300, seasonal_0 Loss: 0.1356 | 0.1701
Epoch 278/300, seasonal_0 Loss: 0.1351 | 0.1698
Epoch 279/300, seasonal_0 Loss: 0.1350 | 0.1700
Epoch 280/300, seasonal_0 Loss: 0.1350 | 0.1701
Epoch 281/300, seasonal_0 Loss: 0.1347 | 0.1701
Epoch 282/300, seasonal_0 Loss: 0.1348 | 0.1706
Epoch 283/300, seasonal_0 Loss: 0.1349 | 0.1704
Epoch 284/300, seasonal_0 Loss: 0.1352 | 0.1702
Epoch 285/300, seasonal_0 Loss: 0.1350 | 0.1700
Epoch 286/300, seasonal_0 Loss: 0.1347 | 0.1699
Epoch 287/300, seasonal_0 Loss: 0.1355 | 0.1699
Epoch 288/300, seasonal_0 Loss: 0.1345 | 0.1698
Epoch 289/300, seasonal_0 Loss: 0.1343 | 0.1699
Epoch 290/300, seasonal_0 Loss: 0.1356 | 0.1700
Epoch 291/300, seasonal_0 Loss: 0.1349 | 0.1699
Epoch 292/300, seasonal_0 Loss: 0.1345 | 0.1698
Epoch 293/300, seasonal_0 Loss: 0.1341 | 0.1698
Epoch 294/300, seasonal_0 Loss: 0.1350 | 0.1698
Epoch 295/300, seasonal_0 Loss: 0.1346 | 0.1698
Epoch 296/300, seasonal_0 Loss: 0.1351 | 0.1697
Epoch 297/300, seasonal_0 Loss: 0.1349 | 0.1696
Epoch 298/300, seasonal_0 Loss: 0.1349 | 0.1698
Epoch 299/300, seasonal_0 Loss: 0.1348 | 0.1698
Epoch 300/300, seasonal_0 Loss: 0.1351 | 0.1696
Training seasonal_1 component with params: {'observation_period_num': 30, 'train_rates': 0.9898982066904156, 'learning_rate': 0.00010750458884282798, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9083834793606768}
Epoch 1/300, seasonal_1 Loss: 0.9261 | 1.1316
Epoch 2/300, seasonal_1 Loss: 0.6825 | 0.9510
Epoch 3/300, seasonal_1 Loss: 0.5240 | 0.8506
Epoch 4/300, seasonal_1 Loss: 0.4349 | 0.7236
Epoch 5/300, seasonal_1 Loss: 0.4056 | 0.6844
Epoch 6/300, seasonal_1 Loss: 0.4062 | 0.6186
Epoch 7/300, seasonal_1 Loss: 0.3761 | 0.5918
Epoch 8/300, seasonal_1 Loss: 0.3256 | 0.5814
Epoch 9/300, seasonal_1 Loss: 0.2993 | 0.5222
Epoch 10/300, seasonal_1 Loss: 0.2769 | 0.4907
Epoch 11/300, seasonal_1 Loss: 0.2648 | 0.4630
Epoch 12/300, seasonal_1 Loss: 0.2547 | 0.5054
Epoch 13/300, seasonal_1 Loss: 0.2553 | 0.4276
Epoch 14/300, seasonal_1 Loss: 0.2382 | 0.4033
Epoch 15/300, seasonal_1 Loss: 0.2397 | 0.3902
Epoch 16/300, seasonal_1 Loss: 0.2545 | 0.4161
Epoch 17/300, seasonal_1 Loss: 0.2434 | 0.3814
Epoch 18/300, seasonal_1 Loss: 0.2567 | 0.4039
Epoch 19/300, seasonal_1 Loss: 0.2695 | 0.3871
Epoch 20/300, seasonal_1 Loss: 0.2568 | 0.3837
Epoch 21/300, seasonal_1 Loss: 0.2283 | 0.3634
Epoch 22/300, seasonal_1 Loss: 0.2203 | 0.3479
Epoch 23/300, seasonal_1 Loss: 0.2132 | 0.3335
Epoch 24/300, seasonal_1 Loss: 0.1994 | 0.3246
Epoch 25/300, seasonal_1 Loss: 0.1998 | 0.3302
Epoch 26/300, seasonal_1 Loss: 0.1915 | 0.3109
Epoch 27/300, seasonal_1 Loss: 0.1894 | 0.3178
Epoch 28/300, seasonal_1 Loss: 0.1848 | 0.3014
Epoch 29/300, seasonal_1 Loss: 0.1786 | 0.2941
Epoch 30/300, seasonal_1 Loss: 0.1767 | 0.2916
Epoch 31/300, seasonal_1 Loss: 0.1738 | 0.2853
Epoch 32/300, seasonal_1 Loss: 0.1716 | 0.2767
Epoch 33/300, seasonal_1 Loss: 0.1708 | 0.2756
Epoch 34/300, seasonal_1 Loss: 0.1696 | 0.2738
Epoch 35/300, seasonal_1 Loss: 0.1663 | 0.2661
Epoch 36/300, seasonal_1 Loss: 0.1649 | 0.2555
Epoch 37/300, seasonal_1 Loss: 0.1653 | 0.2642
Epoch 38/300, seasonal_1 Loss: 0.1653 | 0.2542
Epoch 39/300, seasonal_1 Loss: 0.1634 | 0.2571
Epoch 40/300, seasonal_1 Loss: 0.1639 | 0.2449
Epoch 41/300, seasonal_1 Loss: 0.1602 | 0.2416
Epoch 42/300, seasonal_1 Loss: 0.1596 | 0.2391
Epoch 43/300, seasonal_1 Loss: 0.1575 | 0.2356
Epoch 44/300, seasonal_1 Loss: 0.1577 | 0.2298
Epoch 45/300, seasonal_1 Loss: 0.1537 | 0.2255
Epoch 46/300, seasonal_1 Loss: 0.1554 | 0.2277
Epoch 47/300, seasonal_1 Loss: 0.1530 | 0.2257
Epoch 48/300, seasonal_1 Loss: 0.1525 | 0.2202
Epoch 49/300, seasonal_1 Loss: 0.1521 | 0.2170
Epoch 50/300, seasonal_1 Loss: 0.1545 | 0.2327
Epoch 51/300, seasonal_1 Loss: 0.1526 | 0.2141
Epoch 52/300, seasonal_1 Loss: 0.1526 | 0.2115
Epoch 53/300, seasonal_1 Loss: 0.1566 | 0.2248
Epoch 54/300, seasonal_1 Loss: 0.1470 | 0.2064
Epoch 55/300, seasonal_1 Loss: 0.1499 | 0.2046
Epoch 56/300, seasonal_1 Loss: 0.1438 | 0.2056
Epoch 57/300, seasonal_1 Loss: 0.1414 | 0.2057
Epoch 58/300, seasonal_1 Loss: 0.1409 | 0.1997
Epoch 59/300, seasonal_1 Loss: 0.1395 | 0.1965
Epoch 60/300, seasonal_1 Loss: 0.1405 | 0.1996
Epoch 61/300, seasonal_1 Loss: 0.1373 | 0.1976
Epoch 62/300, seasonal_1 Loss: 0.1380 | 0.1923
Epoch 63/300, seasonal_1 Loss: 0.1359 | 0.1899
Epoch 64/300, seasonal_1 Loss: 0.1371 | 0.1909
Epoch 65/300, seasonal_1 Loss: 0.1348 | 0.1907
Epoch 66/300, seasonal_1 Loss: 0.1340 | 0.1859
Epoch 67/300, seasonal_1 Loss: 0.1338 | 0.1855
Epoch 68/300, seasonal_1 Loss: 0.1334 | 0.1852
Epoch 69/300, seasonal_1 Loss: 0.1324 | 0.1833
Epoch 70/300, seasonal_1 Loss: 0.1325 | 0.1833
Epoch 71/300, seasonal_1 Loss: 0.1326 | 0.1797
Epoch 72/300, seasonal_1 Loss: 0.1305 | 0.1791
Epoch 73/300, seasonal_1 Loss: 0.1304 | 0.1786
Epoch 74/300, seasonal_1 Loss: 0.1285 | 0.1771
Epoch 75/300, seasonal_1 Loss: 0.1297 | 0.1756
Epoch 76/300, seasonal_1 Loss: 0.1283 | 0.1752
Epoch 77/300, seasonal_1 Loss: 0.1292 | 0.1755
Epoch 78/300, seasonal_1 Loss: 0.1273 | 0.1719
Epoch 79/300, seasonal_1 Loss: 0.1274 | 0.1712
Epoch 80/300, seasonal_1 Loss: 0.1259 | 0.1712
Epoch 81/300, seasonal_1 Loss: 0.1252 | 0.1702
Epoch 82/300, seasonal_1 Loss: 0.1252 | 0.1692
Epoch 83/300, seasonal_1 Loss: 0.1242 | 0.1681
Epoch 84/300, seasonal_1 Loss: 0.1241 | 0.1683
Epoch 85/300, seasonal_1 Loss: 0.1244 | 0.1671
Epoch 86/300, seasonal_1 Loss: 0.1236 | 0.1660
Epoch 87/300, seasonal_1 Loss: 0.1234 | 0.1654
Epoch 88/300, seasonal_1 Loss: 0.1229 | 0.1652
Epoch 89/300, seasonal_1 Loss: 0.1222 | 0.1648
Epoch 90/300, seasonal_1 Loss: 0.1228 | 0.1641
Epoch 91/300, seasonal_1 Loss: 0.1223 | 0.1639
Epoch 92/300, seasonal_1 Loss: 0.1219 | 0.1630
Epoch 93/300, seasonal_1 Loss: 0.1212 | 0.1629
Epoch 94/300, seasonal_1 Loss: 0.1209 | 0.1612
Epoch 95/300, seasonal_1 Loss: 0.1211 | 0.1611
Epoch 96/300, seasonal_1 Loss: 0.1206 | 0.1608
Epoch 97/300, seasonal_1 Loss: 0.1202 | 0.1602
Epoch 98/300, seasonal_1 Loss: 0.1196 | 0.1600
Epoch 99/300, seasonal_1 Loss: 0.1199 | 0.1594
Epoch 100/300, seasonal_1 Loss: 0.1193 | 0.1595
Epoch 101/300, seasonal_1 Loss: 0.1192 | 0.1598
Epoch 102/300, seasonal_1 Loss: 0.1203 | 0.1590
Epoch 103/300, seasonal_1 Loss: 0.1186 | 0.1583
Epoch 104/300, seasonal_1 Loss: 0.1190 | 0.1580
Epoch 105/300, seasonal_1 Loss: 0.1181 | 0.1574
Epoch 106/300, seasonal_1 Loss: 0.1188 | 0.1569
Epoch 107/300, seasonal_1 Loss: 0.1189 | 0.1578
Epoch 108/300, seasonal_1 Loss: 0.1176 | 0.1571
Epoch 109/300, seasonal_1 Loss: 0.1178 | 0.1569
Epoch 110/300, seasonal_1 Loss: 0.1176 | 0.1561
Epoch 111/300, seasonal_1 Loss: 0.1177 | 0.1561
Epoch 112/300, seasonal_1 Loss: 0.1167 | 0.1558
Epoch 113/300, seasonal_1 Loss: 0.1178 | 0.1561
Epoch 114/300, seasonal_1 Loss: 0.1173 | 0.1558
Epoch 115/300, seasonal_1 Loss: 0.1167 | 0.1553
Epoch 116/300, seasonal_1 Loss: 0.1168 | 0.1555
Epoch 117/300, seasonal_1 Loss: 0.1170 | 0.1554
Epoch 118/300, seasonal_1 Loss: 0.1167 | 0.1547
Epoch 119/300, seasonal_1 Loss: 0.1160 | 0.1546
Epoch 120/300, seasonal_1 Loss: 0.1166 | 0.1544
Epoch 121/300, seasonal_1 Loss: 0.1156 | 0.1547
Epoch 122/300, seasonal_1 Loss: 0.1157 | 0.1542
Epoch 123/300, seasonal_1 Loss: 0.1153 | 0.1536
Epoch 124/300, seasonal_1 Loss: 0.1158 | 0.1535
Epoch 125/300, seasonal_1 Loss: 0.1158 | 0.1532
Epoch 126/300, seasonal_1 Loss: 0.1152 | 0.1534
Epoch 127/300, seasonal_1 Loss: 0.1154 | 0.1533
Epoch 128/300, seasonal_1 Loss: 0.1151 | 0.1532
Epoch 129/300, seasonal_1 Loss: 0.1151 | 0.1528
Epoch 130/300, seasonal_1 Loss: 0.1148 | 0.1528
Epoch 131/300, seasonal_1 Loss: 0.1148 | 0.1527
Epoch 132/300, seasonal_1 Loss: 0.1149 | 0.1524
Epoch 133/300, seasonal_1 Loss: 0.1136 | 0.1523
Epoch 134/300, seasonal_1 Loss: 0.1144 | 0.1517
Epoch 135/300, seasonal_1 Loss: 0.1149 | 0.1520
Epoch 136/300, seasonal_1 Loss: 0.1148 | 0.1517
Epoch 137/300, seasonal_1 Loss: 0.1145 | 0.1514
Epoch 138/300, seasonal_1 Loss: 0.1140 | 0.1516
Epoch 139/300, seasonal_1 Loss: 0.1142 | 0.1512
Epoch 140/300, seasonal_1 Loss: 0.1138 | 0.1512
Epoch 141/300, seasonal_1 Loss: 0.1138 | 0.1510
Epoch 142/300, seasonal_1 Loss: 0.1138 | 0.1511
Epoch 143/300, seasonal_1 Loss: 0.1139 | 0.1511
Epoch 144/300, seasonal_1 Loss: 0.1136 | 0.1511
Epoch 145/300, seasonal_1 Loss: 0.1136 | 0.1508
Epoch 146/300, seasonal_1 Loss: 0.1144 | 0.1509
Epoch 147/300, seasonal_1 Loss: 0.1139 | 0.1508
Epoch 148/300, seasonal_1 Loss: 0.1135 | 0.1505
Epoch 149/300, seasonal_1 Loss: 0.1136 | 0.1504
Epoch 150/300, seasonal_1 Loss: 0.1128 | 0.1505
Epoch 151/300, seasonal_1 Loss: 0.1135 | 0.1507
Epoch 152/300, seasonal_1 Loss: 0.1131 | 0.1507
Epoch 153/300, seasonal_1 Loss: 0.1132 | 0.1504
Epoch 154/300, seasonal_1 Loss: 0.1128 | 0.1502
Epoch 155/300, seasonal_1 Loss: 0.1135 | 0.1504
Epoch 156/300, seasonal_1 Loss: 0.1135 | 0.1503
Epoch 157/300, seasonal_1 Loss: 0.1134 | 0.1501
Epoch 158/300, seasonal_1 Loss: 0.1126 | 0.1503
Epoch 159/300, seasonal_1 Loss: 0.1132 | 0.1502
Epoch 160/300, seasonal_1 Loss: 0.1130 | 0.1503
Epoch 161/300, seasonal_1 Loss: 0.1124 | 0.1503
Epoch 162/300, seasonal_1 Loss: 0.1126 | 0.1499
Epoch 163/300, seasonal_1 Loss: 0.1128 | 0.1498
Epoch 164/300, seasonal_1 Loss: 0.1123 | 0.1498
Epoch 165/300, seasonal_1 Loss: 0.1125 | 0.1498
Epoch 166/300, seasonal_1 Loss: 0.1123 | 0.1498
Epoch 167/300, seasonal_1 Loss: 0.1128 | 0.1499
Epoch 168/300, seasonal_1 Loss: 0.1129 | 0.1497
Epoch 169/300, seasonal_1 Loss: 0.1126 | 0.1497
Epoch 170/300, seasonal_1 Loss: 0.1123 | 0.1498
Epoch 171/300, seasonal_1 Loss: 0.1125 | 0.1498
Epoch 172/300, seasonal_1 Loss: 0.1122 | 0.1498
Epoch 173/300, seasonal_1 Loss: 0.1130 | 0.1497
Epoch 174/300, seasonal_1 Loss: 0.1129 | 0.1496
Epoch 175/300, seasonal_1 Loss: 0.1121 | 0.1495
Epoch 176/300, seasonal_1 Loss: 0.1123 | 0.1495
Epoch 177/300, seasonal_1 Loss: 0.1114 | 0.1494
Epoch 178/300, seasonal_1 Loss: 0.1124 | 0.1494
Epoch 179/300, seasonal_1 Loss: 0.1126 | 0.1496
Epoch 180/300, seasonal_1 Loss: 0.1122 | 0.1496
Epoch 181/300, seasonal_1 Loss: 0.1121 | 0.1496
Epoch 182/300, seasonal_1 Loss: 0.1118 | 0.1495
Epoch 183/300, seasonal_1 Loss: 0.1123 | 0.1495
Epoch 184/300, seasonal_1 Loss: 0.1125 | 0.1494
Epoch 185/300, seasonal_1 Loss: 0.1120 | 0.1494
Epoch 186/300, seasonal_1 Loss: 0.1125 | 0.1493
Epoch 187/300, seasonal_1 Loss: 0.1120 | 0.1493
Epoch 188/300, seasonal_1 Loss: 0.1122 | 0.1492
Epoch 189/300, seasonal_1 Loss: 0.1116 | 0.1493
Epoch 190/300, seasonal_1 Loss: 0.1122 | 0.1493
Epoch 191/300, seasonal_1 Loss: 0.1123 | 0.1493
Epoch 192/300, seasonal_1 Loss: 0.1117 | 0.1492
Epoch 193/300, seasonal_1 Loss: 0.1119 | 0.1492
Epoch 194/300, seasonal_1 Loss: 0.1115 | 0.1492
Epoch 195/300, seasonal_1 Loss: 0.1125 | 0.1491
Epoch 196/300, seasonal_1 Loss: 0.1118 | 0.1490
Epoch 197/300, seasonal_1 Loss: 0.1118 | 0.1491
Epoch 198/300, seasonal_1 Loss: 0.1118 | 0.1490
Epoch 199/300, seasonal_1 Loss: 0.1122 | 0.1490
Epoch 200/300, seasonal_1 Loss: 0.1113 | 0.1490
Epoch 201/300, seasonal_1 Loss: 0.1113 | 0.1490
Epoch 202/300, seasonal_1 Loss: 0.1119 | 0.1489
Epoch 203/300, seasonal_1 Loss: 0.1120 | 0.1489
Epoch 204/300, seasonal_1 Loss: 0.1124 | 0.1489
Epoch 205/300, seasonal_1 Loss: 0.1119 | 0.1490
Epoch 206/300, seasonal_1 Loss: 0.1115 | 0.1490
Epoch 207/300, seasonal_1 Loss: 0.1123 | 0.1490
Epoch 208/300, seasonal_1 Loss: 0.1125 | 0.1490
Epoch 209/300, seasonal_1 Loss: 0.1121 | 0.1490
Epoch 210/300, seasonal_1 Loss: 0.1119 | 0.1490
Epoch 211/300, seasonal_1 Loss: 0.1118 | 0.1490
Epoch 212/300, seasonal_1 Loss: 0.1115 | 0.1489
Epoch 213/300, seasonal_1 Loss: 0.1116 | 0.1489
Epoch 214/300, seasonal_1 Loss: 0.1114 | 0.1488
Epoch 215/300, seasonal_1 Loss: 0.1118 | 0.1488
Epoch 216/300, seasonal_1 Loss: 0.1124 | 0.1488
Epoch 217/300, seasonal_1 Loss: 0.1111 | 0.1488
Epoch 218/300, seasonal_1 Loss: 0.1114 | 0.1487
Epoch 219/300, seasonal_1 Loss: 0.1116 | 0.1487
Epoch 220/300, seasonal_1 Loss: 0.1115 | 0.1487
Epoch 221/300, seasonal_1 Loss: 0.1118 | 0.1487
Epoch 222/300, seasonal_1 Loss: 0.1113 | 0.1486
Epoch 223/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 224/300, seasonal_1 Loss: 0.1115 | 0.1486
Epoch 225/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 226/300, seasonal_1 Loss: 0.1112 | 0.1487
Epoch 227/300, seasonal_1 Loss: 0.1120 | 0.1486
Epoch 228/300, seasonal_1 Loss: 0.1113 | 0.1487
Epoch 229/300, seasonal_1 Loss: 0.1116 | 0.1487
Epoch 230/300, seasonal_1 Loss: 0.1109 | 0.1487
Epoch 231/300, seasonal_1 Loss: 0.1116 | 0.1487
Epoch 232/300, seasonal_1 Loss: 0.1120 | 0.1487
Epoch 233/300, seasonal_1 Loss: 0.1114 | 0.1487
Epoch 234/300, seasonal_1 Loss: 0.1114 | 0.1487
Epoch 235/300, seasonal_1 Loss: 0.1112 | 0.1487
Epoch 236/300, seasonal_1 Loss: 0.1117 | 0.1487
Epoch 237/300, seasonal_1 Loss: 0.1113 | 0.1487
Epoch 238/300, seasonal_1 Loss: 0.1115 | 0.1487
Epoch 239/300, seasonal_1 Loss: 0.1112 | 0.1487
Epoch 240/300, seasonal_1 Loss: 0.1118 | 0.1487
Epoch 241/300, seasonal_1 Loss: 0.1114 | 0.1487
Epoch 242/300, seasonal_1 Loss: 0.1116 | 0.1487
Epoch 243/300, seasonal_1 Loss: 0.1116 | 0.1487
Epoch 244/300, seasonal_1 Loss: 0.1119 | 0.1487
Epoch 245/300, seasonal_1 Loss: 0.1113 | 0.1487
Epoch 246/300, seasonal_1 Loss: 0.1111 | 0.1486
Epoch 247/300, seasonal_1 Loss: 0.1113 | 0.1486
Epoch 248/300, seasonal_1 Loss: 0.1123 | 0.1486
Epoch 249/300, seasonal_1 Loss: 0.1108 | 0.1486
Epoch 250/300, seasonal_1 Loss: 0.1118 | 0.1486
Epoch 251/300, seasonal_1 Loss: 0.1120 | 0.1486
Epoch 252/300, seasonal_1 Loss: 0.1119 | 0.1486
Epoch 253/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 254/300, seasonal_1 Loss: 0.1110 | 0.1486
Epoch 255/300, seasonal_1 Loss: 0.1113 | 0.1486
Epoch 256/300, seasonal_1 Loss: 0.1116 | 0.1486
Epoch 257/300, seasonal_1 Loss: 0.1109 | 0.1486
Epoch 258/300, seasonal_1 Loss: 0.1113 | 0.1486
Epoch 259/300, seasonal_1 Loss: 0.1111 | 0.1486
Epoch 260/300, seasonal_1 Loss: 0.1112 | 0.1486
Epoch 261/300, seasonal_1 Loss: 0.1119 | 0.1486
Epoch 262/300, seasonal_1 Loss: 0.1112 | 0.1486
Epoch 263/300, seasonal_1 Loss: 0.1115 | 0.1486
Epoch 264/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 265/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 266/300, seasonal_1 Loss: 0.1117 | 0.1485
Epoch 267/300, seasonal_1 Loss: 0.1120 | 0.1485
Epoch 268/300, seasonal_1 Loss: 0.1118 | 0.1485
Epoch 269/300, seasonal_1 Loss: 0.1119 | 0.1485
Epoch 270/300, seasonal_1 Loss: 0.1117 | 0.1486
Epoch 271/300, seasonal_1 Loss: 0.1115 | 0.1486
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 98, 'train_rates': 0.989981258418338, 'learning_rate': 0.0002545436498103557, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8824952673818741}
Epoch 1/300, seasonal_2 Loss: 0.4942 | 0.7097
Epoch 2/300, seasonal_2 Loss: 0.4686 | 0.6151
Epoch 3/300, seasonal_2 Loss: 0.4853 | 0.4904
Epoch 4/300, seasonal_2 Loss: 0.4383 | 0.4487
Epoch 5/300, seasonal_2 Loss: 0.4235 | 0.4923
Epoch 6/300, seasonal_2 Loss: 0.4097 | 0.4440
Epoch 7/300, seasonal_2 Loss: 0.4201 | 0.4855
Epoch 8/300, seasonal_2 Loss: 0.4561 | 0.4438
Epoch 9/300, seasonal_2 Loss: 0.3957 | 0.5231
Epoch 10/300, seasonal_2 Loss: 0.4540 | 0.4366
Epoch 11/300, seasonal_2 Loss: 0.3999 | 0.4386
Epoch 12/300, seasonal_2 Loss: 0.3873 | 0.4371
Epoch 13/300, seasonal_2 Loss: 0.3147 | 0.3752
Epoch 14/300, seasonal_2 Loss: 0.3244 | 0.3719
Epoch 15/300, seasonal_2 Loss: 0.3028 | 0.4566
Epoch 16/300, seasonal_2 Loss: 0.2809 | 0.3984
Epoch 17/300, seasonal_2 Loss: 0.2739 | 0.4491
Epoch 18/300, seasonal_2 Loss: 0.2871 | 0.3648
Epoch 19/300, seasonal_2 Loss: 0.2701 | 0.3805
Epoch 20/300, seasonal_2 Loss: 0.2667 | 0.3251
Epoch 21/300, seasonal_2 Loss: 0.2789 | 0.3470
Epoch 22/300, seasonal_2 Loss: 0.2620 | 0.3684
Epoch 23/300, seasonal_2 Loss: 0.2459 | 0.3263
Epoch 24/300, seasonal_2 Loss: 0.2498 | 0.3346
Epoch 25/300, seasonal_2 Loss: 0.2487 | 0.3227
Epoch 26/300, seasonal_2 Loss: 0.2548 | 0.3931
Epoch 27/300, seasonal_2 Loss: 0.2650 | 0.3163
Epoch 28/300, seasonal_2 Loss: 0.2440 | 0.2970
Epoch 29/300, seasonal_2 Loss: 0.2362 | 0.2896
Epoch 30/300, seasonal_2 Loss: 0.2257 | 0.2889
Epoch 31/300, seasonal_2 Loss: 0.2090 | 0.3335
Epoch 32/300, seasonal_2 Loss: 0.2109 | 0.2899
Epoch 33/300, seasonal_2 Loss: 0.2079 | 0.2857
Epoch 34/300, seasonal_2 Loss: 0.2050 | 0.2754
Epoch 35/300, seasonal_2 Loss: 0.2021 | 0.3232
Epoch 36/300, seasonal_2 Loss: 0.2027 | 0.2546
Epoch 37/300, seasonal_2 Loss: 0.1975 | 0.2766
Epoch 38/300, seasonal_2 Loss: 0.2026 | 0.2545
Epoch 39/300, seasonal_2 Loss: 0.2068 | 0.2865
Epoch 40/300, seasonal_2 Loss: 0.1874 | 0.2522
Epoch 41/300, seasonal_2 Loss: 0.1937 | 0.2584
Epoch 42/300, seasonal_2 Loss: 0.1976 | 0.2577
Epoch 43/300, seasonal_2 Loss: 0.2253 | 0.2399
Epoch 44/300, seasonal_2 Loss: 0.2122 | 0.2287
Epoch 45/300, seasonal_2 Loss: 0.2057 | 0.2432
Epoch 46/300, seasonal_2 Loss: 0.1924 | 0.2247
Epoch 47/300, seasonal_2 Loss: 0.1821 | 0.2135
Epoch 48/300, seasonal_2 Loss: 0.1731 | 0.2187
Epoch 49/300, seasonal_2 Loss: 0.1645 | 0.2058
Epoch 50/300, seasonal_2 Loss: 0.1662 | 0.1986
Epoch 51/300, seasonal_2 Loss: 0.1625 | 0.2276
Epoch 52/300, seasonal_2 Loss: 0.1583 | 0.1935
Epoch 53/300, seasonal_2 Loss: 0.1550 | 0.1910
Epoch 54/300, seasonal_2 Loss: 0.1534 | 0.1869
Epoch 55/300, seasonal_2 Loss: 0.1494 | 0.1772
Epoch 56/300, seasonal_2 Loss: 0.1436 | 0.1674
Epoch 57/300, seasonal_2 Loss: 0.1507 | 0.1719
Epoch 58/300, seasonal_2 Loss: 0.1451 | 0.2261
Epoch 59/300, seasonal_2 Loss: 0.1464 | 0.1906
Epoch 60/300, seasonal_2 Loss: 0.1444 | 0.1803
Epoch 61/300, seasonal_2 Loss: 0.1428 | 0.1733
Epoch 62/300, seasonal_2 Loss: 0.1434 | 0.1779
Epoch 63/300, seasonal_2 Loss: 0.1383 | 0.1670
Epoch 64/300, seasonal_2 Loss: 0.1369 | 0.1587
Epoch 65/300, seasonal_2 Loss: 0.1322 | 0.1550
Epoch 66/300, seasonal_2 Loss: 0.1336 | 0.1780
Epoch 67/300, seasonal_2 Loss: 0.1299 | 0.1932
Epoch 68/300, seasonal_2 Loss: 0.1261 | 0.1538
Epoch 69/300, seasonal_2 Loss: 0.1247 | 0.1537
Epoch 70/300, seasonal_2 Loss: 0.1258 | 0.1475
Epoch 71/300, seasonal_2 Loss: 0.1262 | 0.1395
Epoch 72/300, seasonal_2 Loss: 0.1230 | 0.1505
Epoch 73/300, seasonal_2 Loss: 0.1265 | 0.1382
Epoch 74/300, seasonal_2 Loss: 0.1259 | 0.1710
Epoch 75/300, seasonal_2 Loss: 0.1292 | 0.1504
Epoch 76/300, seasonal_2 Loss: 0.1268 | 0.1379
Epoch 77/300, seasonal_2 Loss: 0.1279 | 0.1663
Epoch 78/300, seasonal_2 Loss: 0.1224 | 0.1457
Epoch 79/300, seasonal_2 Loss: 0.1164 | 0.1347
Epoch 80/300, seasonal_2 Loss: 0.1134 | 0.1291
Epoch 81/300, seasonal_2 Loss: 0.1114 | 0.1297
Epoch 82/300, seasonal_2 Loss: 0.1123 | 0.1659
Epoch 83/300, seasonal_2 Loss: 0.1113 | 0.1292
Epoch 84/300, seasonal_2 Loss: 0.1101 | 0.1257
Epoch 85/300, seasonal_2 Loss: 0.1105 | 0.1266
Epoch 86/300, seasonal_2 Loss: 0.1087 | 0.1250
Epoch 87/300, seasonal_2 Loss: 0.1082 | 0.1226
Epoch 88/300, seasonal_2 Loss: 0.1097 | 0.1299
Epoch 89/300, seasonal_2 Loss: 0.1118 | 0.1208
Epoch 90/300, seasonal_2 Loss: 0.1102 | 0.1516
Epoch 91/300, seasonal_2 Loss: 0.1125 | 0.1276
Epoch 92/300, seasonal_2 Loss: 0.1125 | 0.1415
Epoch 93/300, seasonal_2 Loss: 0.1101 | 0.1335
Epoch 94/300, seasonal_2 Loss: 0.1071 | 0.1160
Epoch 95/300, seasonal_2 Loss: 0.1041 | 0.1185
Epoch 96/300, seasonal_2 Loss: 0.1047 | 0.1262
Epoch 97/300, seasonal_2 Loss: 0.1037 | 0.1331
Epoch 98/300, seasonal_2 Loss: 0.1026 | 0.1161
Epoch 99/300, seasonal_2 Loss: 0.1036 | 0.1162
Epoch 100/300, seasonal_2 Loss: 0.1032 | 0.1156
Epoch 101/300, seasonal_2 Loss: 0.1007 | 0.1211
Epoch 102/300, seasonal_2 Loss: 0.0988 | 0.1175
Epoch 103/300, seasonal_2 Loss: 0.0984 | 0.1256
Epoch 104/300, seasonal_2 Loss: 0.0981 | 0.1170
Epoch 105/300, seasonal_2 Loss: 0.0981 | 0.1215
Epoch 106/300, seasonal_2 Loss: 0.0988 | 0.1190
Epoch 107/300, seasonal_2 Loss: 0.0987 | 0.1193
Epoch 108/300, seasonal_2 Loss: 0.0978 | 0.1160
Epoch 109/300, seasonal_2 Loss: 0.0972 | 0.1202
Epoch 110/300, seasonal_2 Loss: 0.0949 | 0.1238
Epoch 111/300, seasonal_2 Loss: 0.0949 | 0.1210
Epoch 112/300, seasonal_2 Loss: 0.0951 | 0.1151
Epoch 113/300, seasonal_2 Loss: 0.0951 | 0.1164
Epoch 114/300, seasonal_2 Loss: 0.0936 | 0.1157
Epoch 115/300, seasonal_2 Loss: 0.0938 | 0.1253
Epoch 116/300, seasonal_2 Loss: 0.0916 | 0.1170
Epoch 117/300, seasonal_2 Loss: 0.0921 | 0.1131
Epoch 118/300, seasonal_2 Loss: 0.0913 | 0.1121
Epoch 119/300, seasonal_2 Loss: 0.0908 | 0.1185
Epoch 120/300, seasonal_2 Loss: 0.0901 | 0.1163
Epoch 121/300, seasonal_2 Loss: 0.0896 | 0.1156
Epoch 122/300, seasonal_2 Loss: 0.0895 | 0.1141
Epoch 123/300, seasonal_2 Loss: 0.0899 | 0.1130
Epoch 124/300, seasonal_2 Loss: 0.0891 | 0.1155
Epoch 125/300, seasonal_2 Loss: 0.0893 | 0.1201
Epoch 126/300, seasonal_2 Loss: 0.0879 | 0.1125
Epoch 127/300, seasonal_2 Loss: 0.0874 | 0.1125
Epoch 128/300, seasonal_2 Loss: 0.0871 | 0.1120
Epoch 129/300, seasonal_2 Loss: 0.0866 | 0.1177
Epoch 130/300, seasonal_2 Loss: 0.0863 | 0.1160
Epoch 131/300, seasonal_2 Loss: 0.0872 | 0.1161
Epoch 132/300, seasonal_2 Loss: 0.0859 | 0.1103
Epoch 133/300, seasonal_2 Loss: 0.0855 | 0.1133
Epoch 134/300, seasonal_2 Loss: 0.0853 | 0.1111
Epoch 135/300, seasonal_2 Loss: 0.0850 | 0.1132
Epoch 136/300, seasonal_2 Loss: 0.0845 | 0.1182
Epoch 137/300, seasonal_2 Loss: 0.0846 | 0.1144
Epoch 138/300, seasonal_2 Loss: 0.0838 | 0.1145
Epoch 139/300, seasonal_2 Loss: 0.0851 | 0.1121
Epoch 140/300, seasonal_2 Loss: 0.0837 | 0.1102
Epoch 141/300, seasonal_2 Loss: 0.0828 | 0.1150
Epoch 142/300, seasonal_2 Loss: 0.0829 | 0.1130
Epoch 143/300, seasonal_2 Loss: 0.0832 | 0.1131
Epoch 144/300, seasonal_2 Loss: 0.0823 | 0.1150
Epoch 145/300, seasonal_2 Loss: 0.0826 | 0.1148
Epoch 146/300, seasonal_2 Loss: 0.0821 | 0.1131
Epoch 147/300, seasonal_2 Loss: 0.0824 | 0.1168
Epoch 148/300, seasonal_2 Loss: 0.0818 | 0.1134
Epoch 149/300, seasonal_2 Loss: 0.0816 | 0.1123
Epoch 150/300, seasonal_2 Loss: 0.0814 | 0.1159
Epoch 151/300, seasonal_2 Loss: 0.0818 | 0.1147
Epoch 152/300, seasonal_2 Loss: 0.0808 | 0.1143
Epoch 153/300, seasonal_2 Loss: 0.0817 | 0.1124
Epoch 154/300, seasonal_2 Loss: 0.0804 | 0.1141
Epoch 155/300, seasonal_2 Loss: 0.0802 | 0.1168
Epoch 156/300, seasonal_2 Loss: 0.0808 | 0.1161
Epoch 157/300, seasonal_2 Loss: 0.0805 | 0.1151
Epoch 158/300, seasonal_2 Loss: 0.0796 | 0.1138
Epoch 159/300, seasonal_2 Loss: 0.0797 | 0.1168
Epoch 160/300, seasonal_2 Loss: 0.0794 | 0.1167
Epoch 161/300, seasonal_2 Loss: 0.0795 | 0.1145
Epoch 162/300, seasonal_2 Loss: 0.0795 | 0.1146
Epoch 163/300, seasonal_2 Loss: 0.0789 | 0.1143
Epoch 164/300, seasonal_2 Loss: 0.0796 | 0.1132
Epoch 165/300, seasonal_2 Loss: 0.0791 | 0.1160
Epoch 166/300, seasonal_2 Loss: 0.0788 | 0.1147
Epoch 167/300, seasonal_2 Loss: 0.0785 | 0.1118
Epoch 168/300, seasonal_2 Loss: 0.0786 | 0.1106
Epoch 169/300, seasonal_2 Loss: 0.0788 | 0.1119
Epoch 170/300, seasonal_2 Loss: 0.0782 | 0.1126
Epoch 171/300, seasonal_2 Loss: 0.0780 | 0.1128
Epoch 172/300, seasonal_2 Loss: 0.0782 | 0.1128
Epoch 173/300, seasonal_2 Loss: 0.0776 | 0.1107
Epoch 174/300, seasonal_2 Loss: 0.0778 | 0.1121
Epoch 175/300, seasonal_2 Loss: 0.0781 | 0.1123
Epoch 176/300, seasonal_2 Loss: 0.0776 | 0.1116
Epoch 177/300, seasonal_2 Loss: 0.0775 | 0.1105
Epoch 178/300, seasonal_2 Loss: 0.0774 | 0.1115
Epoch 179/300, seasonal_2 Loss: 0.0774 | 0.1123
Epoch 180/300, seasonal_2 Loss: 0.0765 | 0.1127
Epoch 181/300, seasonal_2 Loss: 0.0762 | 0.1112
Epoch 182/300, seasonal_2 Loss: 0.0765 | 0.1127
Epoch 183/300, seasonal_2 Loss: 0.0762 | 0.1121
Epoch 184/300, seasonal_2 Loss: 0.0763 | 0.1120
Epoch 185/300, seasonal_2 Loss: 0.0764 | 0.1116
Epoch 186/300, seasonal_2 Loss: 0.0754 | 0.1119
Epoch 187/300, seasonal_2 Loss: 0.0759 | 0.1126
Epoch 188/300, seasonal_2 Loss: 0.0755 | 0.1113
Epoch 189/300, seasonal_2 Loss: 0.0764 | 0.1113
Epoch 190/300, seasonal_2 Loss: 0.0765 | 0.1133
Epoch 191/300, seasonal_2 Loss: 0.0760 | 0.1132
Epoch 192/300, seasonal_2 Loss: 0.0760 | 0.1124
Epoch 193/300, seasonal_2 Loss: 0.0755 | 0.1127
Epoch 194/300, seasonal_2 Loss: 0.0759 | 0.1126
Epoch 195/300, seasonal_2 Loss: 0.0751 | 0.1157
Epoch 196/300, seasonal_2 Loss: 0.0753 | 0.1110
Epoch 197/300, seasonal_2 Loss: 0.0746 | 0.1119
Epoch 198/300, seasonal_2 Loss: 0.0750 | 0.1125
Epoch 199/300, seasonal_2 Loss: 0.0751 | 0.1124
Epoch 200/300, seasonal_2 Loss: 0.0752 | 0.1118
Epoch 201/300, seasonal_2 Loss: 0.0744 | 0.1106
Epoch 202/300, seasonal_2 Loss: 0.0742 | 0.1121
Epoch 203/300, seasonal_2 Loss: 0.0739 | 0.1129
Epoch 204/300, seasonal_2 Loss: 0.0747 | 0.1122
Epoch 205/300, seasonal_2 Loss: 0.0738 | 0.1108
Epoch 206/300, seasonal_2 Loss: 0.0754 | 0.1102
Epoch 207/300, seasonal_2 Loss: 0.0746 | 0.1116
Epoch 208/300, seasonal_2 Loss: 0.0742 | 0.1119
Epoch 209/300, seasonal_2 Loss: 0.0744 | 0.1119
Epoch 210/300, seasonal_2 Loss: 0.0741 | 0.1113
Epoch 211/300, seasonal_2 Loss: 0.0741 | 0.1114
Epoch 212/300, seasonal_2 Loss: 0.0741 | 0.1117
Epoch 213/300, seasonal_2 Loss: 0.0741 | 0.1124
Epoch 214/300, seasonal_2 Loss: 0.0738 | 0.1130
Epoch 215/300, seasonal_2 Loss: 0.0741 | 0.1113
Epoch 216/300, seasonal_2 Loss: 0.0738 | 0.1120
Epoch 217/300, seasonal_2 Loss: 0.0738 | 0.1122
Epoch 218/300, seasonal_2 Loss: 0.0732 | 0.1122
Epoch 219/300, seasonal_2 Loss: 0.0728 | 0.1122
Epoch 220/300, seasonal_2 Loss: 0.0728 | 0.1132
Epoch 221/300, seasonal_2 Loss: 0.0737 | 0.1119
Epoch 222/300, seasonal_2 Loss: 0.0736 | 0.1140
Epoch 223/300, seasonal_2 Loss: 0.0733 | 0.1129
Epoch 224/300, seasonal_2 Loss: 0.0732 | 0.1123
Epoch 225/300, seasonal_2 Loss: 0.0727 | 0.1120
Epoch 226/300, seasonal_2 Loss: 0.0737 | 0.1125
Epoch 227/300, seasonal_2 Loss: 0.0722 | 0.1136
Epoch 228/300, seasonal_2 Loss: 0.0723 | 0.1133
Epoch 229/300, seasonal_2 Loss: 0.0732 | 0.1124
Epoch 230/300, seasonal_2 Loss: 0.0726 | 0.1120
Epoch 231/300, seasonal_2 Loss: 0.0729 | 0.1129
Epoch 232/300, seasonal_2 Loss: 0.0728 | 0.1126
Epoch 233/300, seasonal_2 Loss: 0.0729 | 0.1125
Epoch 234/300, seasonal_2 Loss: 0.0735 | 0.1117
Epoch 235/300, seasonal_2 Loss: 0.0722 | 0.1130
Epoch 236/300, seasonal_2 Loss: 0.0732 | 0.1122
Epoch 237/300, seasonal_2 Loss: 0.0728 | 0.1138
Epoch 238/300, seasonal_2 Loss: 0.0727 | 0.1129
Epoch 239/300, seasonal_2 Loss: 0.0724 | 0.1133
Epoch 240/300, seasonal_2 Loss: 0.0729 | 0.1131
Epoch 241/300, seasonal_2 Loss: 0.0723 | 0.1136
Epoch 242/300, seasonal_2 Loss: 0.0721 | 0.1130
Epoch 243/300, seasonal_2 Loss: 0.0725 | 0.1128
Epoch 244/300, seasonal_2 Loss: 0.0724 | 0.1128
Epoch 245/300, seasonal_2 Loss: 0.0730 | 0.1124
Epoch 246/300, seasonal_2 Loss: 0.0723 | 0.1130
Epoch 247/300, seasonal_2 Loss: 0.0715 | 0.1129
Epoch 248/300, seasonal_2 Loss: 0.0728 | 0.1120
Epoch 249/300, seasonal_2 Loss: 0.0720 | 0.1123
Epoch 250/300, seasonal_2 Loss: 0.0720 | 0.1122
Epoch 251/300, seasonal_2 Loss: 0.0722 | 0.1129
Epoch 252/300, seasonal_2 Loss: 0.0723 | 0.1120
Epoch 253/300, seasonal_2 Loss: 0.0719 | 0.1127
Epoch 254/300, seasonal_2 Loss: 0.0726 | 0.1126
Epoch 255/300, seasonal_2 Loss: 0.0723 | 0.1129
Epoch 256/300, seasonal_2 Loss: 0.0727 | 0.1124
Epoch 257/300, seasonal_2 Loss: 0.0719 | 0.1117
Epoch 258/300, seasonal_2 Loss: 0.0720 | 0.1124
Epoch 259/300, seasonal_2 Loss: 0.0715 | 0.1130
Epoch 260/300, seasonal_2 Loss: 0.0718 | 0.1122
Epoch 261/300, seasonal_2 Loss: 0.0723 | 0.1123
Epoch 262/300, seasonal_2 Loss: 0.0716 | 0.1118
Epoch 263/300, seasonal_2 Loss: 0.0722 | 0.1121
Epoch 264/300, seasonal_2 Loss: 0.0713 | 0.1116
Epoch 265/300, seasonal_2 Loss: 0.0718 | 0.1123
Epoch 266/300, seasonal_2 Loss: 0.0721 | 0.1127
Epoch 267/300, seasonal_2 Loss: 0.0712 | 0.1128
Epoch 268/300, seasonal_2 Loss: 0.0720 | 0.1128
Epoch 269/300, seasonal_2 Loss: 0.0721 | 0.1128
Epoch 270/300, seasonal_2 Loss: 0.0720 | 0.1119
Epoch 271/300, seasonal_2 Loss: 0.0717 | 0.1124
Epoch 272/300, seasonal_2 Loss: 0.0719 | 0.1134
Epoch 273/300, seasonal_2 Loss: 0.0716 | 0.1127
Epoch 274/300, seasonal_2 Loss: 0.0714 | 0.1131
Epoch 275/300, seasonal_2 Loss: 0.0723 | 0.1128
Epoch 276/300, seasonal_2 Loss: 0.0716 | 0.1126
Epoch 277/300, seasonal_2 Loss: 0.0712 | 0.1123
Epoch 278/300, seasonal_2 Loss: 0.0724 | 0.1115
Epoch 279/300, seasonal_2 Loss: 0.0714 | 0.1115
Epoch 280/300, seasonal_2 Loss: 0.0714 | 0.1121
Epoch 281/300, seasonal_2 Loss: 0.0714 | 0.1126
Epoch 282/300, seasonal_2 Loss: 0.0713 | 0.1126
Epoch 283/300, seasonal_2 Loss: 0.0715 | 0.1131
Epoch 284/300, seasonal_2 Loss: 0.0717 | 0.1127
Epoch 285/300, seasonal_2 Loss: 0.0717 | 0.1124
Epoch 286/300, seasonal_2 Loss: 0.0719 | 0.1124
Epoch 287/300, seasonal_2 Loss: 0.0710 | 0.1123
Epoch 288/300, seasonal_2 Loss: 0.0715 | 0.1118
Epoch 289/300, seasonal_2 Loss: 0.0718 | 0.1118
Epoch 290/300, seasonal_2 Loss: 0.0708 | 0.1118
Epoch 291/300, seasonal_2 Loss: 0.0713 | 0.1121
Epoch 292/300, seasonal_2 Loss: 0.0707 | 0.1126
Epoch 293/300, seasonal_2 Loss: 0.0713 | 0.1126
Epoch 294/300, seasonal_2 Loss: 0.0715 | 0.1124
Epoch 295/300, seasonal_2 Loss: 0.0710 | 0.1127
Epoch 296/300, seasonal_2 Loss: 0.0713 | 0.1124
Epoch 297/300, seasonal_2 Loss: 0.0720 | 0.1123
Epoch 298/300, seasonal_2 Loss: 0.0715 | 0.1125
Epoch 299/300, seasonal_2 Loss: 0.0713 | 0.1123
Epoch 300/300, seasonal_2 Loss: 0.0713 | 0.1120
Training seasonal_3 component with params: {'observation_period_num': 217, 'train_rates': 0.9454345142963176, 'learning_rate': 0.00028411700362331813, 'batch_size': 117, 'step_size': 14, 'gamma': 0.7639038462738393}
Epoch 1/300, seasonal_3 Loss: 0.9907 | 1.9095
Epoch 2/300, seasonal_3 Loss: 0.7499 | 1.3649
Epoch 3/300, seasonal_3 Loss: 0.6721 | 1.2292
Epoch 4/300, seasonal_3 Loss: 0.5957 | 1.0381
Epoch 5/300, seasonal_3 Loss: 0.4890 | 0.9575
Epoch 6/300, seasonal_3 Loss: 0.4667 | 0.8902
Epoch 7/300, seasonal_3 Loss: 0.6074 | 0.8545
Epoch 8/300, seasonal_3 Loss: 0.5006 | 0.7791
Epoch 9/300, seasonal_3 Loss: 0.5396 | 0.7877
Epoch 10/300, seasonal_3 Loss: 0.4621 | 0.7159
Epoch 11/300, seasonal_3 Loss: 0.4615 | 0.7537
Epoch 12/300, seasonal_3 Loss: 0.4530 | 0.6801
Epoch 13/300, seasonal_3 Loss: 0.4241 | 0.6246
Epoch 14/300, seasonal_3 Loss: 0.4041 | 0.6468
Epoch 15/300, seasonal_3 Loss: 0.3557 | 0.5804
Epoch 16/300, seasonal_3 Loss: 0.3185 | 0.5988
Epoch 17/300, seasonal_3 Loss: 0.3444 | 0.5452
Epoch 18/300, seasonal_3 Loss: 0.3094 | 0.5579
Epoch 19/300, seasonal_3 Loss: 0.2971 | 0.5081
Epoch 20/300, seasonal_3 Loss: 0.2906 | 0.5264
Epoch 21/300, seasonal_3 Loss: 0.2763 | 0.4764
Epoch 22/300, seasonal_3 Loss: 0.2797 | 0.4826
Epoch 23/300, seasonal_3 Loss: 0.2517 | 0.4623
Epoch 24/300, seasonal_3 Loss: 0.2437 | 0.4532
Epoch 25/300, seasonal_3 Loss: 0.2355 | 0.4367
Epoch 26/300, seasonal_3 Loss: 0.2290 | 0.4248
Epoch 27/300, seasonal_3 Loss: 0.2271 | 0.4130
Epoch 28/300, seasonal_3 Loss: 0.2242 | 0.4081
Epoch 29/300, seasonal_3 Loss: 0.2224 | 0.3976
Epoch 30/300, seasonal_3 Loss: 0.2159 | 0.3925
Epoch 31/300, seasonal_3 Loss: 0.2138 | 0.3828
Epoch 32/300, seasonal_3 Loss: 0.2123 | 0.3795
Epoch 33/300, seasonal_3 Loss: 0.2118 | 0.3723
Epoch 34/300, seasonal_3 Loss: 0.2136 | 0.3646
Epoch 35/300, seasonal_3 Loss: 0.2119 | 0.3609
Epoch 36/300, seasonal_3 Loss: 0.2099 | 0.3519
Epoch 37/300, seasonal_3 Loss: 0.2025 | 0.3517
Epoch 38/300, seasonal_3 Loss: 0.1990 | 0.3397
Epoch 39/300, seasonal_3 Loss: 0.1970 | 0.3434
Epoch 40/300, seasonal_3 Loss: 0.1939 | 0.3352
Epoch 41/300, seasonal_3 Loss: 0.1922 | 0.3305
Epoch 42/300, seasonal_3 Loss: 0.1916 | 0.3256
Epoch 43/300, seasonal_3 Loss: 0.1901 | 0.3238
Epoch 44/300, seasonal_3 Loss: 0.1874 | 0.3188
Epoch 45/300, seasonal_3 Loss: 0.1872 | 0.3184
Epoch 46/300, seasonal_3 Loss: 0.1864 | 0.3147
Epoch 47/300, seasonal_3 Loss: 0.1846 | 0.3110
Epoch 48/300, seasonal_3 Loss: 0.1835 | 0.3098
Epoch 49/300, seasonal_3 Loss: 0.1832 | 0.3061
Epoch 50/300, seasonal_3 Loss: 0.1814 | 0.3051
Epoch 51/300, seasonal_3 Loss: 0.1803 | 0.3041
Epoch 52/300, seasonal_3 Loss: 0.1798 | 0.3009
Epoch 53/300, seasonal_3 Loss: 0.1787 | 0.2972
Epoch 54/300, seasonal_3 Loss: 0.1782 | 0.2963
Epoch 55/300, seasonal_3 Loss: 0.1770 | 0.2938
Epoch 56/300, seasonal_3 Loss: 0.1769 | 0.2938
Epoch 57/300, seasonal_3 Loss: 0.1756 | 0.2900
Epoch 58/300, seasonal_3 Loss: 0.1743 | 0.2907
Epoch 59/300, seasonal_3 Loss: 0.1750 | 0.2883
Epoch 60/300, seasonal_3 Loss: 0.1739 | 0.2863
Epoch 61/300, seasonal_3 Loss: 0.1729 | 0.2872
Epoch 62/300, seasonal_3 Loss: 0.1735 | 0.2843
Epoch 63/300, seasonal_3 Loss: 0.1725 | 0.2838
Epoch 64/300, seasonal_3 Loss: 0.1713 | 0.2820
Epoch 65/300, seasonal_3 Loss: 0.1715 | 0.2810
Epoch 66/300, seasonal_3 Loss: 0.1715 | 0.2811
Epoch 67/300, seasonal_3 Loss: 0.1704 | 0.2781
Epoch 68/300, seasonal_3 Loss: 0.1704 | 0.2781
Epoch 69/300, seasonal_3 Loss: 0.1695 | 0.2775
Epoch 70/300, seasonal_3 Loss: 0.1693 | 0.2776
Epoch 71/300, seasonal_3 Loss: 0.1692 | 0.2754
Epoch 72/300, seasonal_3 Loss: 0.1685 | 0.2750
Epoch 73/300, seasonal_3 Loss: 0.1683 | 0.2731
Epoch 74/300, seasonal_3 Loss: 0.1678 | 0.2724
Epoch 75/300, seasonal_3 Loss: 0.1668 | 0.2711
Epoch 76/300, seasonal_3 Loss: 0.1674 | 0.2703
Epoch 77/300, seasonal_3 Loss: 0.1661 | 0.2704
Epoch 78/300, seasonal_3 Loss: 0.1662 | 0.2705
Epoch 79/300, seasonal_3 Loss: 0.1661 | 0.2700
Epoch 80/300, seasonal_3 Loss: 0.1668 | 0.2682
Epoch 81/300, seasonal_3 Loss: 0.1661 | 0.2675
Epoch 82/300, seasonal_3 Loss: 0.1654 | 0.2676
Epoch 83/300, seasonal_3 Loss: 0.1645 | 0.2669
Epoch 84/300, seasonal_3 Loss: 0.1647 | 0.2662
Epoch 85/300, seasonal_3 Loss: 0.1643 | 0.2656
Epoch 86/300, seasonal_3 Loss: 0.1648 | 0.2654
Epoch 87/300, seasonal_3 Loss: 0.1645 | 0.2654
Epoch 88/300, seasonal_3 Loss: 0.1645 | 0.2643
Epoch 89/300, seasonal_3 Loss: 0.1637 | 0.2649
Epoch 90/300, seasonal_3 Loss: 0.1632 | 0.2633
Epoch 91/300, seasonal_3 Loss: 0.1622 | 0.2623
Epoch 92/300, seasonal_3 Loss: 0.1633 | 0.2626
Epoch 93/300, seasonal_3 Loss: 0.1640 | 0.2621
Epoch 94/300, seasonal_3 Loss: 0.1637 | 0.2621
Epoch 95/300, seasonal_3 Loss: 0.1623 | 0.2612
Epoch 96/300, seasonal_3 Loss: 0.1618 | 0.2612
Epoch 97/300, seasonal_3 Loss: 0.1631 | 0.2615
Epoch 98/300, seasonal_3 Loss: 0.1626 | 0.2605
Epoch 99/300, seasonal_3 Loss: 0.1615 | 0.2598
Epoch 100/300, seasonal_3 Loss: 0.1622 | 0.2602
Epoch 101/300, seasonal_3 Loss: 0.1617 | 0.2609
Epoch 102/300, seasonal_3 Loss: 0.1614 | 0.2600
Epoch 103/300, seasonal_3 Loss: 0.1607 | 0.2601
Epoch 104/300, seasonal_3 Loss: 0.1618 | 0.2595
Epoch 105/300, seasonal_3 Loss: 0.1610 | 0.2588
Epoch 106/300, seasonal_3 Loss: 0.1607 | 0.2594
Epoch 107/300, seasonal_3 Loss: 0.1603 | 0.2589
Epoch 108/300, seasonal_3 Loss: 0.1603 | 0.2580
Epoch 109/300, seasonal_3 Loss: 0.1608 | 0.2580
Epoch 110/300, seasonal_3 Loss: 0.1606 | 0.2579
Epoch 111/300, seasonal_3 Loss: 0.1598 | 0.2581
Epoch 112/300, seasonal_3 Loss: 0.1601 | 0.2579
Epoch 113/300, seasonal_3 Loss: 0.1604 | 0.2576
Epoch 114/300, seasonal_3 Loss: 0.1604 | 0.2575
Epoch 115/300, seasonal_3 Loss: 0.1596 | 0.2573
Epoch 116/300, seasonal_3 Loss: 0.1604 | 0.2576
Epoch 117/300, seasonal_3 Loss: 0.1599 | 0.2572
Epoch 118/300, seasonal_3 Loss: 0.1592 | 0.2568
Epoch 119/300, seasonal_3 Loss: 0.1592 | 0.2562
Epoch 120/300, seasonal_3 Loss: 0.1599 | 0.2559
Epoch 121/300, seasonal_3 Loss: 0.1593 | 0.2561
Epoch 122/300, seasonal_3 Loss: 0.1588 | 0.2561
Epoch 123/300, seasonal_3 Loss: 0.1589 | 0.2564
Epoch 124/300, seasonal_3 Loss: 0.1596 | 0.2561
Epoch 125/300, seasonal_3 Loss: 0.1598 | 0.2555
Epoch 126/300, seasonal_3 Loss: 0.1590 | 0.2551
Epoch 127/300, seasonal_3 Loss: 0.1591 | 0.2550
Epoch 128/300, seasonal_3 Loss: 0.1590 | 0.2552
Epoch 129/300, seasonal_3 Loss: 0.1586 | 0.2554
Epoch 130/300, seasonal_3 Loss: 0.1591 | 0.2546
Epoch 131/300, seasonal_3 Loss: 0.1587 | 0.2543
Epoch 132/300, seasonal_3 Loss: 0.1590 | 0.2546
Epoch 133/300, seasonal_3 Loss: 0.1584 | 0.2542
Epoch 134/300, seasonal_3 Loss: 0.1587 | 0.2540
Epoch 135/300, seasonal_3 Loss: 0.1583 | 0.2539
Epoch 136/300, seasonal_3 Loss: 0.1582 | 0.2540
Epoch 137/300, seasonal_3 Loss: 0.1589 | 0.2538
Epoch 138/300, seasonal_3 Loss: 0.1587 | 0.2538
Epoch 139/300, seasonal_3 Loss: 0.1588 | 0.2537
Epoch 140/300, seasonal_3 Loss: 0.1576 | 0.2537
Epoch 141/300, seasonal_3 Loss: 0.1586 | 0.2536
Epoch 142/300, seasonal_3 Loss: 0.1582 | 0.2536
Epoch 143/300, seasonal_3 Loss: 0.1584 | 0.2535
Epoch 144/300, seasonal_3 Loss: 0.1589 | 0.2535
Epoch 145/300, seasonal_3 Loss: 0.1583 | 0.2536
Epoch 146/300, seasonal_3 Loss: 0.1585 | 0.2536
Epoch 147/300, seasonal_3 Loss: 0.1574 | 0.2534
Epoch 148/300, seasonal_3 Loss: 0.1586 | 0.2534
Epoch 149/300, seasonal_3 Loss: 0.1577 | 0.2533
Epoch 150/300, seasonal_3 Loss: 0.1590 | 0.2532
Epoch 151/300, seasonal_3 Loss: 0.1586 | 0.2530
Epoch 152/300, seasonal_3 Loss: 0.1578 | 0.2531
Epoch 153/300, seasonal_3 Loss: 0.1585 | 0.2532
Epoch 154/300, seasonal_3 Loss: 0.1587 | 0.2534
Epoch 155/300, seasonal_3 Loss: 0.1584 | 0.2532
Epoch 156/300, seasonal_3 Loss: 0.1581 | 0.2532
Epoch 157/300, seasonal_3 Loss: 0.1581 | 0.2530
Epoch 158/300, seasonal_3 Loss: 0.1580 | 0.2530
Epoch 159/300, seasonal_3 Loss: 0.1582 | 0.2530
Epoch 160/300, seasonal_3 Loss: 0.1571 | 0.2528
Epoch 161/300, seasonal_3 Loss: 0.1579 | 0.2527
Epoch 162/300, seasonal_3 Loss: 0.1583 | 0.2527
Epoch 163/300, seasonal_3 Loss: 0.1587 | 0.2527
Epoch 164/300, seasonal_3 Loss: 0.1580 | 0.2526
Epoch 165/300, seasonal_3 Loss: 0.1581 | 0.2525
Epoch 166/300, seasonal_3 Loss: 0.1575 | 0.2525
Epoch 167/300, seasonal_3 Loss: 0.1575 | 0.2524
Epoch 168/300, seasonal_3 Loss: 0.1579 | 0.2522
Epoch 169/300, seasonal_3 Loss: 0.1578 | 0.2522
Epoch 170/300, seasonal_3 Loss: 0.1574 | 0.2523
Epoch 171/300, seasonal_3 Loss: 0.1576 | 0.2522
Epoch 172/300, seasonal_3 Loss: 0.1579 | 0.2521
Epoch 173/300, seasonal_3 Loss: 0.1573 | 0.2521
Epoch 174/300, seasonal_3 Loss: 0.1573 | 0.2521
Epoch 175/300, seasonal_3 Loss: 0.1579 | 0.2521
Epoch 176/300, seasonal_3 Loss: 0.1582 | 0.2521
Epoch 177/300, seasonal_3 Loss: 0.1576 | 0.2521
Epoch 178/300, seasonal_3 Loss: 0.1582 | 0.2521
Epoch 179/300, seasonal_3 Loss: 0.1577 | 0.2520
Epoch 180/300, seasonal_3 Loss: 0.1582 | 0.2520
Epoch 181/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 182/300, seasonal_3 Loss: 0.1576 | 0.2519
Epoch 183/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 184/300, seasonal_3 Loss: 0.1573 | 0.2519
Epoch 185/300, seasonal_3 Loss: 0.1575 | 0.2519
Epoch 186/300, seasonal_3 Loss: 0.1575 | 0.2519
Epoch 187/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 188/300, seasonal_3 Loss: 0.1579 | 0.2519
Epoch 189/300, seasonal_3 Loss: 0.1577 | 0.2518
Epoch 190/300, seasonal_3 Loss: 0.1583 | 0.2519
Epoch 191/300, seasonal_3 Loss: 0.1576 | 0.2519
Epoch 192/300, seasonal_3 Loss: 0.1578 | 0.2518
Epoch 193/300, seasonal_3 Loss: 0.1573 | 0.2519
Epoch 194/300, seasonal_3 Loss: 0.1573 | 0.2519
Epoch 195/300, seasonal_3 Loss: 0.1565 | 0.2519
Epoch 196/300, seasonal_3 Loss: 0.1587 | 0.2519
Epoch 197/300, seasonal_3 Loss: 0.1574 | 0.2519
Epoch 198/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 199/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 200/300, seasonal_3 Loss: 0.1572 | 0.2519
Epoch 201/300, seasonal_3 Loss: 0.1578 | 0.2519
Epoch 202/300, seasonal_3 Loss: 0.1577 | 0.2519
Epoch 203/300, seasonal_3 Loss: 0.1574 | 0.2519
Epoch 204/300, seasonal_3 Loss: 0.1580 | 0.2519
Epoch 205/300, seasonal_3 Loss: 0.1574 | 0.2519
Epoch 206/300, seasonal_3 Loss: 0.1578 | 0.2519
Epoch 207/300, seasonal_3 Loss: 0.1566 | 0.2519
Epoch 208/300, seasonal_3 Loss: 0.1586 | 0.2519
Epoch 209/300, seasonal_3 Loss: 0.1574 | 0.2519
Epoch 210/300, seasonal_3 Loss: 0.1569 | 0.2519
Epoch 211/300, seasonal_3 Loss: 0.1572 | 0.2519
Epoch 212/300, seasonal_3 Loss: 0.1571 | 0.2519
Epoch 213/300, seasonal_3 Loss: 0.1582 | 0.2518
Epoch 214/300, seasonal_3 Loss: 0.1578 | 0.2518
Epoch 215/300, seasonal_3 Loss: 0.1571 | 0.2518
Epoch 216/300, seasonal_3 Loss: 0.1576 | 0.2518
Epoch 217/300, seasonal_3 Loss: 0.1576 | 0.2518
Epoch 218/300, seasonal_3 Loss: 0.1569 | 0.2518
Epoch 219/300, seasonal_3 Loss: 0.1572 | 0.2518
Epoch 220/300, seasonal_3 Loss: 0.1574 | 0.2518
Epoch 221/300, seasonal_3 Loss: 0.1572 | 0.2518
Epoch 222/300, seasonal_3 Loss: 0.1572 | 0.2518
Epoch 223/300, seasonal_3 Loss: 0.1573 | 0.2518
Epoch 224/300, seasonal_3 Loss: 0.1575 | 0.2518
Epoch 225/300, seasonal_3 Loss: 0.1569 | 0.2517
Epoch 226/300, seasonal_3 Loss: 0.1578 | 0.2517
Epoch 227/300, seasonal_3 Loss: 0.1571 | 0.2517
Epoch 228/300, seasonal_3 Loss: 0.1578 | 0.2517
Epoch 229/300, seasonal_3 Loss: 0.1579 | 0.2517
Epoch 230/300, seasonal_3 Loss: 0.1575 | 0.2517
Epoch 231/300, seasonal_3 Loss: 0.1578 | 0.2517
Epoch 232/300, seasonal_3 Loss: 0.1575 | 0.2517
Epoch 233/300, seasonal_3 Loss: 0.1581 | 0.2517
Epoch 234/300, seasonal_3 Loss: 0.1572 | 0.2517
Epoch 235/300, seasonal_3 Loss: 0.1574 | 0.2517
Epoch 236/300, seasonal_3 Loss: 0.1568 | 0.2517
Epoch 237/300, seasonal_3 Loss: 0.1570 | 0.2517
Epoch 238/300, seasonal_3 Loss: 0.1578 | 0.2517
Epoch 239/300, seasonal_3 Loss: 0.1569 | 0.2517
Epoch 240/300, seasonal_3 Loss: 0.1572 | 0.2517
Epoch 241/300, seasonal_3 Loss: 0.1575 | 0.2517
Epoch 242/300, seasonal_3 Loss: 0.1577 | 0.2517
Epoch 243/300, seasonal_3 Loss: 0.1576 | 0.2517
Epoch 244/300, seasonal_3 Loss: 0.1577 | 0.2517
Epoch 245/300, seasonal_3 Loss: 0.1575 | 0.2517
Epoch 246/300, seasonal_3 Loss: 0.1583 | 0.2517
Epoch 247/300, seasonal_3 Loss: 0.1580 | 0.2517
Epoch 248/300, seasonal_3 Loss: 0.1576 | 0.2517
Epoch 249/300, seasonal_3 Loss: 0.1577 | 0.2517
Epoch 250/300, seasonal_3 Loss: 0.1576 | 0.2517
Epoch 251/300, seasonal_3 Loss: 0.1574 | 0.2517
Epoch 252/300, seasonal_3 Loss: 0.1578 | 0.2517
Epoch 253/300, seasonal_3 Loss: 0.1581 | 0.2517
Epoch 254/300, seasonal_3 Loss: 0.1570 | 0.2517
Epoch 255/300, seasonal_3 Loss: 0.1569 | 0.2517
Epoch 256/300, seasonal_3 Loss: 0.1577 | 0.2517
Epoch 257/300, seasonal_3 Loss: 0.1573 | 0.2517
Epoch 258/300, seasonal_3 Loss: 0.1570 | 0.2517
Epoch 259/300, seasonal_3 Loss: 0.1577 | 0.2517
Epoch 260/300, seasonal_3 Loss: 0.1582 | 0.2517
Epoch 261/300, seasonal_3 Loss: 0.1580 | 0.2517
Epoch 262/300, seasonal_3 Loss: 0.1575 | 0.2516
Epoch 263/300, seasonal_3 Loss: 0.1576 | 0.2516
Epoch 264/300, seasonal_3 Loss: 0.1580 | 0.2516
Epoch 265/300, seasonal_3 Loss: 0.1585 | 0.2516
Epoch 266/300, seasonal_3 Loss: 0.1571 | 0.2516
Epoch 267/300, seasonal_3 Loss: 0.1570 | 0.2516
Epoch 268/300, seasonal_3 Loss: 0.1580 | 0.2516
Epoch 269/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 270/300, seasonal_3 Loss: 0.1576 | 0.2516
Epoch 271/300, seasonal_3 Loss: 0.1574 | 0.2516
Epoch 272/300, seasonal_3 Loss: 0.1572 | 0.2516
Epoch 273/300, seasonal_3 Loss: 0.1580 | 0.2516
Epoch 274/300, seasonal_3 Loss: 0.1576 | 0.2516
Epoch 275/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 276/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 277/300, seasonal_3 Loss: 0.1578 | 0.2516
Epoch 278/300, seasonal_3 Loss: 0.1572 | 0.2516
Epoch 279/300, seasonal_3 Loss: 0.1568 | 0.2516
Epoch 280/300, seasonal_3 Loss: 0.1572 | 0.2516
Epoch 281/300, seasonal_3 Loss: 0.1579 | 0.2516
Epoch 282/300, seasonal_3 Loss: 0.1580 | 0.2516
Epoch 283/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 284/300, seasonal_3 Loss: 0.1581 | 0.2516
Epoch 285/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 286/300, seasonal_3 Loss: 0.1581 | 0.2516
Epoch 287/300, seasonal_3 Loss: 0.1583 | 0.2516
Epoch 288/300, seasonal_3 Loss: 0.1575 | 0.2516
Epoch 289/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 290/300, seasonal_3 Loss: 0.1569 | 0.2516
Epoch 291/300, seasonal_3 Loss: 0.1569 | 0.2516
Epoch 292/300, seasonal_3 Loss: 0.1576 | 0.2516
Epoch 293/300, seasonal_3 Loss: 0.1566 | 0.2516
Epoch 294/300, seasonal_3 Loss: 0.1575 | 0.2516
Epoch 295/300, seasonal_3 Loss: 0.1578 | 0.2516
Epoch 296/300, seasonal_3 Loss: 0.1576 | 0.2516
Epoch 297/300, seasonal_3 Loss: 0.1577 | 0.2516
Epoch 298/300, seasonal_3 Loss: 0.1569 | 0.2516
Epoch 299/300, seasonal_3 Loss: 0.1570 | 0.2516
Epoch 300/300, seasonal_3 Loss: 0.1573 | 0.2516
Training resid component with params: {'observation_period_num': 93, 'train_rates': 0.981224654566133, 'learning_rate': 0.00021092434120410967, 'batch_size': 158, 'step_size': 12, 'gamma': 0.9045985755132188}
Epoch 1/300, resid Loss: 1.1849 | 2.2879
Epoch 2/300, resid Loss: 0.8008 | 1.4235
Epoch 3/300, resid Loss: 0.6521 | 1.0872
Epoch 4/300, resid Loss: 0.6428 | 1.0706
Epoch 5/300, resid Loss: 0.6011 | 0.9977
Epoch 6/300, resid Loss: 0.5168 | 0.9039
Epoch 7/300, resid Loss: 0.4911 | 0.8337
Epoch 8/300, resid Loss: 0.5348 | 0.8460
Epoch 9/300, resid Loss: 0.4856 | 0.7547
Epoch 10/300, resid Loss: 0.4370 | 0.7444
Epoch 11/300, resid Loss: 0.4057 | 0.6784
Epoch 12/300, resid Loss: 0.3916 | 0.6599
Epoch 13/300, resid Loss: 0.3738 | 0.6524
Epoch 14/300, resid Loss: 0.3521 | 0.5679
Epoch 15/300, resid Loss: 0.3634 | 0.6025
Epoch 16/300, resid Loss: 0.3128 | 0.5594
Epoch 17/300, resid Loss: 0.2919 | 0.5246
Epoch 18/300, resid Loss: 0.2889 | 0.4987
Epoch 19/300, resid Loss: 0.3114 | 0.4928
Epoch 20/300, resid Loss: 0.2849 | 0.4756
Epoch 21/300, resid Loss: 0.2812 | 0.4654
Epoch 22/300, resid Loss: 0.3385 | 0.4777
Epoch 23/300, resid Loss: 0.2882 | 0.4457
Epoch 24/300, resid Loss: 0.2856 | 0.4991
Epoch 25/300, resid Loss: 0.2502 | 0.4166
Epoch 26/300, resid Loss: 0.2415 | 0.4150
Epoch 27/300, resid Loss: 0.2371 | 0.4000
Epoch 28/300, resid Loss: 0.2277 | 0.3928
Epoch 29/300, resid Loss: 0.2241 | 0.3778
Epoch 30/300, resid Loss: 0.2203 | 0.3691
Epoch 31/300, resid Loss: 0.2195 | 0.3551
Epoch 32/300, resid Loss: 0.2171 | 0.3590
Epoch 33/300, resid Loss: 0.2147 | 0.3370
Epoch 34/300, resid Loss: 0.2147 | 0.3417
Epoch 35/300, resid Loss: 0.2254 | 0.3297
Epoch 36/300, resid Loss: 0.2178 | 0.3134
Epoch 37/300, resid Loss: 0.2192 | 0.3367
Epoch 38/300, resid Loss: 0.2324 | 0.3127
Epoch 39/300, resid Loss: 0.2426 | 0.3174
Epoch 40/300, resid Loss: 0.2684 | 0.3145
Epoch 41/300, resid Loss: 0.2295 | 0.3167
Epoch 42/300, resid Loss: 0.2275 | 0.3152
Epoch 43/300, resid Loss: 0.2184 | 0.2953
Epoch 44/300, resid Loss: 0.2075 | 0.3008
Epoch 45/300, resid Loss: 0.2030 | 0.2819
Epoch 46/300, resid Loss: 0.1987 | 0.2945
Epoch 47/300, resid Loss: 0.2009 | 0.2740
Epoch 48/300, resid Loss: 0.2011 | 0.2811
Epoch 49/300, resid Loss: 0.2073 | 0.2692
Epoch 50/300, resid Loss: 0.1936 | 0.2658
Epoch 51/300, resid Loss: 0.1917 | 0.2639
Epoch 52/300, resid Loss: 0.1961 | 0.2625
Epoch 53/300, resid Loss: 0.2054 | 0.2732
Epoch 54/300, resid Loss: 0.2172 | 0.2797
Epoch 55/300, resid Loss: 0.2014 | 0.2850
Epoch 56/300, resid Loss: 0.1965 | 0.2658
Epoch 57/300, resid Loss: 0.1849 | 0.2626
Epoch 58/300, resid Loss: 0.1824 | 0.2557
Epoch 59/300, resid Loss: 0.1793 | 0.2483
Epoch 60/300, resid Loss: 0.1791 | 0.2441
Epoch 61/300, resid Loss: 0.1798 | 0.2401
Epoch 62/300, resid Loss: 0.1754 | 0.2360
Epoch 63/300, resid Loss: 0.1713 | 0.2343
Epoch 64/300, resid Loss: 0.1707 | 0.2289
Epoch 65/300, resid Loss: 0.1720 | 0.2381
Epoch 66/300, resid Loss: 0.1757 | 0.2243
Epoch 67/300, resid Loss: 0.1746 | 0.2331
Epoch 68/300, resid Loss: 0.1727 | 0.2223
Epoch 69/300, resid Loss: 0.1676 | 0.2252
Epoch 70/300, resid Loss: 0.1650 | 0.2202
Epoch 71/300, resid Loss: 0.1647 | 0.2185
Epoch 72/300, resid Loss: 0.1646 | 0.2183
Epoch 73/300, resid Loss: 0.1650 | 0.2142
Epoch 74/300, resid Loss: 0.1644 | 0.2163
Epoch 75/300, resid Loss: 0.1630 | 0.2102
Epoch 76/300, resid Loss: 0.1606 | 0.2106
Epoch 77/300, resid Loss: 0.1604 | 0.2073
Epoch 78/300, resid Loss: 0.1591 | 0.2066
Epoch 79/300, resid Loss: 0.1591 | 0.2067
Epoch 80/300, resid Loss: 0.1585 | 0.2037
Epoch 81/300, resid Loss: 0.1590 | 0.2041
Epoch 82/300, resid Loss: 0.1588 | 0.1994
Epoch 83/300, resid Loss: 0.1572 | 0.2009
Epoch 84/300, resid Loss: 0.1561 | 0.1973
Epoch 85/300, resid Loss: 0.1543 | 0.1960
Epoch 86/300, resid Loss: 0.1546 | 0.1952
Epoch 87/300, resid Loss: 0.1538 | 0.1938
Epoch 88/300, resid Loss: 0.1547 | 0.1965
Epoch 89/300, resid Loss: 0.1555 | 0.1928
Epoch 90/300, resid Loss: 0.1547 | 0.1940
Epoch 91/300, resid Loss: 0.1541 | 0.1908
Epoch 92/300, resid Loss: 0.1525 | 0.1907
Epoch 93/300, resid Loss: 0.1509 | 0.1887
Epoch 94/300, resid Loss: 0.1492 | 0.1877
Epoch 95/300, resid Loss: 0.1491 | 0.1867
Epoch 96/300, resid Loss: 0.1500 | 0.1852
Epoch 97/300, resid Loss: 0.1491 | 0.1854
Epoch 98/300, resid Loss: 0.1490 | 0.1835
Epoch 99/300, resid Loss: 0.1482 | 0.1840
Epoch 100/300, resid Loss: 0.1474 | 0.1812
Epoch 101/300, resid Loss: 0.1459 | 0.1815
Epoch 102/300, resid Loss: 0.1463 | 0.1799
Epoch 103/300, resid Loss: 0.1456 | 0.1790
Epoch 104/300, resid Loss: 0.1457 | 0.1785
Epoch 105/300, resid Loss: 0.1458 | 0.1785
Epoch 106/300, resid Loss: 0.1448 | 0.1780
Epoch 107/300, resid Loss: 0.1447 | 0.1765
Epoch 108/300, resid Loss: 0.1439 | 0.1766
Epoch 109/300, resid Loss: 0.1435 | 0.1760
Epoch 110/300, resid Loss: 0.1423 | 0.1745
Epoch 111/300, resid Loss: 0.1432 | 0.1748
Epoch 112/300, resid Loss: 0.1435 | 0.1732
Epoch 113/300, resid Loss: 0.1429 | 0.1739
Epoch 114/300, resid Loss: 0.1432 | 0.1719
Epoch 115/300, resid Loss: 0.1430 | 0.1734
Epoch 116/300, resid Loss: 0.1428 | 0.1717
Epoch 117/300, resid Loss: 0.1423 | 0.1728
Epoch 118/300, resid Loss: 0.1416 | 0.1706
Epoch 119/300, resid Loss: 0.1413 | 0.1706
Epoch 120/300, resid Loss: 0.1406 | 0.1686
Epoch 121/300, resid Loss: 0.1405 | 0.1690
Epoch 122/300, resid Loss: 0.1394 | 0.1685
Epoch 123/300, resid Loss: 0.1398 | 0.1684
Epoch 124/300, resid Loss: 0.1386 | 0.1675
Epoch 125/300, resid Loss: 0.1385 | 0.1673
Epoch 126/300, resid Loss: 0.1382 | 0.1650
Epoch 127/300, resid Loss: 0.1388 | 0.1654
Epoch 128/300, resid Loss: 0.1382 | 0.1650
Epoch 129/300, resid Loss: 0.1375 | 0.1646
Epoch 130/300, resid Loss: 0.1376 | 0.1642
Epoch 131/300, resid Loss: 0.1380 | 0.1639
Epoch 132/300, resid Loss: 0.1375 | 0.1633
Epoch 133/300, resid Loss: 0.1371 | 0.1639
Epoch 134/300, resid Loss: 0.1374 | 0.1630
Epoch 135/300, resid Loss: 0.1365 | 0.1627
Epoch 136/300, resid Loss: 0.1368 | 0.1628
Epoch 137/300, resid Loss: 0.1373 | 0.1626
Epoch 138/300, resid Loss: 0.1379 | 0.1618
Epoch 139/300, resid Loss: 0.1365 | 0.1622
Epoch 140/300, resid Loss: 0.1361 | 0.1614
Epoch 141/300, resid Loss: 0.1354 | 0.1607
Epoch 142/300, resid Loss: 0.1354 | 0.1608
Epoch 143/300, resid Loss: 0.1342 | 0.1609
Epoch 144/300, resid Loss: 0.1349 | 0.1593
Epoch 145/300, resid Loss: 0.1337 | 0.1595
Epoch 146/300, resid Loss: 0.1343 | 0.1586
Epoch 147/300, resid Loss: 0.1335 | 0.1588
Epoch 148/300, resid Loss: 0.1335 | 0.1588
Epoch 149/300, resid Loss: 0.1332 | 0.1581
Epoch 150/300, resid Loss: 0.1336 | 0.1574
Epoch 151/300, resid Loss: 0.1331 | 0.1577
Epoch 152/300, resid Loss: 0.1336 | 0.1575
Epoch 153/300, resid Loss: 0.1322 | 0.1567
Epoch 154/300, resid Loss: 0.1318 | 0.1577
Epoch 155/300, resid Loss: 0.1329 | 0.1575
Epoch 156/300, resid Loss: 0.1323 | 0.1556
Epoch 157/300, resid Loss: 0.1327 | 0.1558
Epoch 158/300, resid Loss: 0.1322 | 0.1558
Epoch 159/300, resid Loss: 0.1315 | 0.1551
Epoch 160/300, resid Loss: 0.1319 | 0.1549
Epoch 161/300, resid Loss: 0.1318 | 0.1546
Epoch 162/300, resid Loss: 0.1312 | 0.1545
Epoch 163/300, resid Loss: 0.1305 | 0.1539
Epoch 164/300, resid Loss: 0.1301 | 0.1536
Epoch 165/300, resid Loss: 0.1301 | 0.1540
Epoch 166/300, resid Loss: 0.1312 | 0.1534
Epoch 167/300, resid Loss: 0.1306 | 0.1532
Epoch 168/300, resid Loss: 0.1304 | 0.1530
Epoch 169/300, resid Loss: 0.1299 | 0.1531
Epoch 170/300, resid Loss: 0.1298 | 0.1525
Epoch 171/300, resid Loss: 0.1296 | 0.1528
Epoch 172/300, resid Loss: 0.1293 | 0.1520
Epoch 173/300, resid Loss: 0.1292 | 0.1524
Epoch 174/300, resid Loss: 0.1298 | 0.1528
Epoch 175/300, resid Loss: 0.1295 | 0.1518
Epoch 176/300, resid Loss: 0.1285 | 0.1515
Epoch 177/300, resid Loss: 0.1292 | 0.1516
Epoch 178/300, resid Loss: 0.1291 | 0.1512
Epoch 179/300, resid Loss: 0.1287 | 0.1513
Epoch 180/300, resid Loss: 0.1283 | 0.1512
Epoch 181/300, resid Loss: 0.1280 | 0.1512
Epoch 182/300, resid Loss: 0.1283 | 0.1508
Epoch 183/300, resid Loss: 0.1288 | 0.1506
Epoch 184/300, resid Loss: 0.1278 | 0.1507
Epoch 185/300, resid Loss: 0.1276 | 0.1516
Epoch 186/300, resid Loss: 0.1277 | 0.1510
Epoch 187/300, resid Loss: 0.1271 | 0.1506
Epoch 188/300, resid Loss: 0.1281 | 0.1498
Epoch 189/300, resid Loss: 0.1272 | 0.1502
Epoch 190/300, resid Loss: 0.1271 | 0.1492
Epoch 191/300, resid Loss: 0.1274 | 0.1498
Epoch 192/300, resid Loss: 0.1267 | 0.1498
Epoch 193/300, resid Loss: 0.1279 | 0.1493
Epoch 194/300, resid Loss: 0.1266 | 0.1498
Epoch 195/300, resid Loss: 0.1265 | 0.1488
Epoch 196/300, resid Loss: 0.1263 | 0.1487
Epoch 197/300, resid Loss: 0.1264 | 0.1490
Epoch 198/300, resid Loss: 0.1270 | 0.1483
Epoch 199/300, resid Loss: 0.1264 | 0.1486
Epoch 200/300, resid Loss: 0.1256 | 0.1487
Epoch 201/300, resid Loss: 0.1265 | 0.1489
Epoch 202/300, resid Loss: 0.1260 | 0.1484
Epoch 203/300, resid Loss: 0.1260 | 0.1481
Epoch 204/300, resid Loss: 0.1260 | 0.1484
Epoch 205/300, resid Loss: 0.1260 | 0.1487
Epoch 206/300, resid Loss: 0.1258 | 0.1486
Epoch 207/300, resid Loss: 0.1255 | 0.1490
Epoch 208/300, resid Loss: 0.1257 | 0.1487
Epoch 209/300, resid Loss: 0.1253 | 0.1483
Epoch 210/300, resid Loss: 0.1253 | 0.1484
Epoch 211/300, resid Loss: 0.1253 | 0.1481
Epoch 212/300, resid Loss: 0.1259 | 0.1474
Epoch 213/300, resid Loss: 0.1250 | 0.1481
Epoch 214/300, resid Loss: 0.1252 | 0.1480
Epoch 215/300, resid Loss: 0.1250 | 0.1475
Epoch 216/300, resid Loss: 0.1248 | 0.1474
Epoch 217/300, resid Loss: 0.1249 | 0.1467
Epoch 218/300, resid Loss: 0.1243 | 0.1469
Epoch 219/300, resid Loss: 0.1243 | 0.1476
Epoch 220/300, resid Loss: 0.1245 | 0.1476
Epoch 221/300, resid Loss: 0.1242 | 0.1474
Epoch 222/300, resid Loss: 0.1242 | 0.1469
Epoch 223/300, resid Loss: 0.1242 | 0.1467
Epoch 224/300, resid Loss: 0.1245 | 0.1468
Epoch 225/300, resid Loss: 0.1242 | 0.1468
Epoch 226/300, resid Loss: 0.1242 | 0.1468
Epoch 227/300, resid Loss: 0.1242 | 0.1465
Epoch 228/300, resid Loss: 0.1235 | 0.1466
Epoch 229/300, resid Loss: 0.1229 | 0.1468
Epoch 230/300, resid Loss: 0.1241 | 0.1465
Epoch 231/300, resid Loss: 0.1231 | 0.1463
Epoch 232/300, resid Loss: 0.1236 | 0.1462
Epoch 233/300, resid Loss: 0.1237 | 0.1464
Epoch 234/300, resid Loss: 0.1244 | 0.1464
Epoch 235/300, resid Loss: 0.1236 | 0.1460
Epoch 236/300, resid Loss: 0.1230 | 0.1460
Epoch 237/300, resid Loss: 0.1231 | 0.1461
Epoch 238/300, resid Loss: 0.1233 | 0.1463
Epoch 239/300, resid Loss: 0.1227 | 0.1461
Epoch 240/300, resid Loss: 0.1231 | 0.1457
Epoch 241/300, resid Loss: 0.1234 | 0.1456
Epoch 242/300, resid Loss: 0.1222 | 0.1453
Epoch 243/300, resid Loss: 0.1231 | 0.1450
Epoch 244/300, resid Loss: 0.1231 | 0.1449
Epoch 245/300, resid Loss: 0.1238 | 0.1449
Epoch 246/300, resid Loss: 0.1237 | 0.1451
Epoch 247/300, resid Loss: 0.1225 | 0.1448
Epoch 248/300, resid Loss: 0.1222 | 0.1448
Epoch 249/300, resid Loss: 0.1228 | 0.1447
Epoch 250/300, resid Loss: 0.1223 | 0.1445
Epoch 251/300, resid Loss: 0.1230 | 0.1447
Epoch 252/300, resid Loss: 0.1229 | 0.1447
Epoch 253/300, resid Loss: 0.1227 | 0.1447
Epoch 254/300, resid Loss: 0.1223 | 0.1446
Epoch 255/300, resid Loss: 0.1222 | 0.1446
Epoch 256/300, resid Loss: 0.1227 | 0.1445
Epoch 257/300, resid Loss: 0.1230 | 0.1442
Epoch 258/300, resid Loss: 0.1221 | 0.1439
Epoch 259/300, resid Loss: 0.1221 | 0.1441
Epoch 260/300, resid Loss: 0.1226 | 0.1439
Epoch 261/300, resid Loss: 0.1226 | 0.1440
Epoch 262/300, resid Loss: 0.1218 | 0.1439
Epoch 263/300, resid Loss: 0.1230 | 0.1442
Epoch 264/300, resid Loss: 0.1221 | 0.1441
Epoch 265/300, resid Loss: 0.1221 | 0.1442
Epoch 266/300, resid Loss: 0.1221 | 0.1442
Epoch 267/300, resid Loss: 0.1210 | 0.1438
Epoch 268/300, resid Loss: 0.1220 | 0.1432
Epoch 269/300, resid Loss: 0.1218 | 0.1434
Epoch 270/300, resid Loss: 0.1213 | 0.1437
Epoch 271/300, resid Loss: 0.1221 | 0.1439
Epoch 272/300, resid Loss: 0.1223 | 0.1439
Epoch 273/300, resid Loss: 0.1217 | 0.1438
Epoch 274/300, resid Loss: 0.1217 | 0.1438
Epoch 275/300, resid Loss: 0.1209 | 0.1436
Epoch 276/300, resid Loss: 0.1218 | 0.1438
Epoch 277/300, resid Loss: 0.1220 | 0.1440
Epoch 278/300, resid Loss: 0.1215 | 0.1437
Epoch 279/300, resid Loss: 0.1218 | 0.1436
Epoch 280/300, resid Loss: 0.1214 | 0.1436
Epoch 281/300, resid Loss: 0.1213 | 0.1434
Epoch 282/300, resid Loss: 0.1214 | 0.1438
Epoch 283/300, resid Loss: 0.1214 | 0.1438
Epoch 284/300, resid Loss: 0.1213 | 0.1435
Epoch 285/300, resid Loss: 0.1215 | 0.1436
Epoch 286/300, resid Loss: 0.1202 | 0.1435
Epoch 287/300, resid Loss: 0.1212 | 0.1434
Epoch 288/300, resid Loss: 0.1209 | 0.1433
Epoch 289/300, resid Loss: 0.1209 | 0.1434
Epoch 290/300, resid Loss: 0.1212 | 0.1433
Epoch 291/300, resid Loss: 0.1209 | 0.1432
Epoch 292/300, resid Loss: 0.1214 | 0.1432
Epoch 293/300, resid Loss: 0.1210 | 0.1433
Epoch 294/300, resid Loss: 0.1209 | 0.1435
Epoch 295/300, resid Loss: 0.1208 | 0.1436
Epoch 296/300, resid Loss: 0.1215 | 0.1437
Epoch 297/300, resid Loss: 0.1212 | 0.1436
Epoch 298/300, resid Loss: 0.1216 | 0.1434
Epoch 299/300, resid Loss: 0.1211 | 0.1433
Epoch 300/300, resid Loss: 0.1211 | 0.1432
Runtime (seconds): 3578.700037240982
0.00014611903728488612
[144.59933]
[-0.8478474]
[-4.210306]
[8.610085]
[3.042644]
[5.420697]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 196.33168317377567
RMSE: 14.0118408203125
MAE: 14.0118408203125
R-squared: nan
[156.6146]
