[32m[I 2025-02-08 06:35:34,074][0m A new study created in memory with name: no-name-5c1a3bd1-a318-4e8c-9817-d87e34422e0e[0m
[32m[I 2025-02-08 06:36:20,242][0m Trial 0 finished with value: 0.7391693803571886 and parameters: {'observation_period_num': 42, 'train_rates': 0.7592501140924651, 'learning_rate': 3.1529259169139383e-06, 'batch_size': 120, 'step_size': 4, 'gamma': 0.9279223190395783}. Best is trial 0 with value: 0.7391693803571886.[0m
[32m[I 2025-02-08 06:37:23,648][0m Trial 1 finished with value: 0.1738345092999007 and parameters: {'observation_period_num': 127, 'train_rates': 0.941982006479325, 'learning_rate': 9.731136598152914e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.919016825725598}. Best is trial 1 with value: 0.1738345092999007.[0m
[32m[I 2025-02-08 06:38:09,438][0m Trial 2 finished with value: 0.1644988073544069 and parameters: {'observation_period_num': 172, 'train_rates': 0.9456209529834838, 'learning_rate': 0.0003733910019803329, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8187748674342157}. Best is trial 2 with value: 0.1644988073544069.[0m
[32m[I 2025-02-08 06:38:36,907][0m Trial 3 finished with value: 0.9982603166676774 and parameters: {'observation_period_num': 139, 'train_rates': 0.8621185732485452, 'learning_rate': 1.2541395905579415e-06, 'batch_size': 224, 'step_size': 6, 'gamma': 0.7616183741338461}. Best is trial 2 with value: 0.1644988073544069.[0m
[32m[I 2025-02-08 06:39:11,882][0m Trial 4 finished with value: 0.16158034359920495 and parameters: {'observation_period_num': 6, 'train_rates': 0.7636729516086418, 'learning_rate': 0.0002630722958560042, 'batch_size': 163, 'step_size': 14, 'gamma': 0.796974848127187}. Best is trial 4 with value: 0.16158034359920495.[0m
[32m[I 2025-02-08 06:40:02,130][0m Trial 5 finished with value: 0.9377242186923086 and parameters: {'observation_period_num': 222, 'train_rates': 0.912877861657563, 'learning_rate': 5.014662552078425e-06, 'batch_size': 114, 'step_size': 3, 'gamma': 0.7954498523438995}. Best is trial 4 with value: 0.16158034359920495.[0m
[32m[I 2025-02-08 06:41:01,041][0m Trial 6 finished with value: 0.082723668217659 and parameters: {'observation_period_num': 69, 'train_rates': 0.9302996292597905, 'learning_rate': 0.00040641334264381935, 'batch_size': 102, 'step_size': 2, 'gamma': 0.8861277256696174}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:41:40,021][0m Trial 7 finished with value: 0.2254057779627026 and parameters: {'observation_period_num': 166, 'train_rates': 0.8879157145370494, 'learning_rate': 4.4714939555172486e-05, 'batch_size': 157, 'step_size': 13, 'gamma': 0.7865098317776089}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:43:42,910][0m Trial 8 finished with value: 0.7595775978747249 and parameters: {'observation_period_num': 215, 'train_rates': 0.8074691804404832, 'learning_rate': 4.510291836759971e-06, 'batch_size': 41, 'step_size': 1, 'gamma': 0.9000864734770444}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:44:08,424][0m Trial 9 finished with value: 0.218677974991746 and parameters: {'observation_period_num': 232, 'train_rates': 0.8359808069565684, 'learning_rate': 5.2902146996234584e-05, 'batch_size': 230, 'step_size': 12, 'gamma': 0.8285950978707275}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:46:22,364][0m Trial 10 finished with value: 0.1986684590296085 and parameters: {'observation_period_num': 77, 'train_rates': 0.6566675410713023, 'learning_rate': 0.000943807943857004, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9836760999283216}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:46:53,280][0m Trial 11 finished with value: 0.15657558822358894 and parameters: {'observation_period_num': 17, 'train_rates': 0.7294425185189527, 'learning_rate': 0.0002075803554191612, 'batch_size': 181, 'step_size': 15, 'gamma': 0.8621700327299229}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:47:22,804][0m Trial 12 finished with value: 0.20439097426464778 and parameters: {'observation_period_num': 69, 'train_rates': 0.6815906906453684, 'learning_rate': 0.00019315133675547346, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8638446530098874}. Best is trial 6 with value: 0.082723668217659.[0m
[32m[I 2025-02-08 06:48:47,298][0m Trial 13 finished with value: 0.05050261691212654 and parameters: {'observation_period_num': 13, 'train_rates': 0.9860792053197565, 'learning_rate': 0.0008156197782599387, 'batch_size': 74, 'step_size': 15, 'gamma': 0.8642373722440054}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 06:50:18,867][0m Trial 14 finished with value: 0.09810511767864227 and parameters: {'observation_period_num': 86, 'train_rates': 0.9894378297359837, 'learning_rate': 0.0005549002199485325, 'batch_size': 70, 'step_size': 1, 'gamma': 0.9542857872329364}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 06:51:41,118][0m Trial 15 finished with value: 0.07406112551689148 and parameters: {'observation_period_num': 44, 'train_rates': 0.9898109661715759, 'learning_rate': 0.0009851478241942062, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8905080403385417}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 06:52:46,018][0m Trial 16 finished with value: 0.5363106522282571 and parameters: {'observation_period_num': 37, 'train_rates': 0.6022745610871276, 'learning_rate': 1.9760090733240058e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8465644351250537}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 06:57:57,105][0m Trial 17 finished with value: 0.10123562340935072 and parameters: {'observation_period_num': 111, 'train_rates': 0.9897395092837649, 'learning_rate': 0.0008654841169813358, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8987330144261522}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 06:59:31,000][0m Trial 18 finished with value: 0.09378783261836177 and parameters: {'observation_period_num': 40, 'train_rates': 0.96660460667104, 'learning_rate': 1.664440014584563e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9477646305720283}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 07:00:43,336][0m Trial 19 finished with value: 0.05058857514636546 and parameters: {'observation_period_num': 24, 'train_rates': 0.8899340021310405, 'learning_rate': 8.922938264791465e-05, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8356793429500992}. Best is trial 13 with value: 0.05050261691212654.[0m
[32m[I 2025-02-08 07:01:51,291][0m Trial 20 finished with value: 0.04123670413169298 and parameters: {'observation_period_num': 7, 'train_rates': 0.88114365898795, 'learning_rate': 9.324111163127383e-05, 'batch_size': 87, 'step_size': 8, 'gamma': 0.8350603356506898}. Best is trial 20 with value: 0.04123670413169298.[0m
[32m[I 2025-02-08 07:02:56,853][0m Trial 21 finished with value: 0.04007549029833908 and parameters: {'observation_period_num': 8, 'train_rates': 0.8945463073572109, 'learning_rate': 9.477157862529062e-05, 'batch_size': 90, 'step_size': 8, 'gamma': 0.8353780966726414}. Best is trial 21 with value: 0.04007549029833908.[0m
[32m[I 2025-02-08 07:04:58,465][0m Trial 22 finished with value: 0.035530759419108035 and parameters: {'observation_period_num': 7, 'train_rates': 0.8405068693911504, 'learning_rate': 9.221316341591961e-05, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8163021596510286}. Best is trial 22 with value: 0.035530759419108035.[0m
[32m[I 2025-02-08 07:07:27,322][0m Trial 23 finished with value: 0.03361513830152743 and parameters: {'observation_period_num': 6, 'train_rates': 0.8294354102135847, 'learning_rate': 0.0001061322535470865, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8113972968300284}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:09:19,460][0m Trial 24 finished with value: 0.13194265334876543 and parameters: {'observation_period_num': 55, 'train_rates': 0.8356451558199085, 'learning_rate': 1.8587884444683134e-05, 'batch_size': 48, 'step_size': 6, 'gamma': 0.768010402837705}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:12:45,534][0m Trial 25 finished with value: 0.14855889658279278 and parameters: {'observation_period_num': 98, 'train_rates': 0.8127346156948418, 'learning_rate': 0.0001372406979496951, 'batch_size': 25, 'step_size': 9, 'gamma': 0.8093881687612834}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:14:32,523][0m Trial 26 finished with value: 0.06736367735140573 and parameters: {'observation_period_num': 32, 'train_rates': 0.8488004253713001, 'learning_rate': 3.474398286609249e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.7808237232363703}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:16:05,197][0m Trial 27 finished with value: 0.1945108800063744 and parameters: {'observation_period_num': 55, 'train_rates': 0.7788035937596695, 'learning_rate': 6.661660037971564e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.8185949548269089}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:21:11,335][0m Trial 28 finished with value: 0.18524787354213743 and parameters: {'observation_period_num': 27, 'train_rates': 0.7335569189412126, 'learning_rate': 2.580952734043095e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7512846514419602}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:22:01,095][0m Trial 29 finished with value: 0.20449949068890216 and parameters: {'observation_period_num': 52, 'train_rates': 0.8054147928971415, 'learning_rate': 1.2416542189763724e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8088150953562594}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:22:45,264][0m Trial 30 finished with value: 0.057029543444514275 and parameters: {'observation_period_num': 23, 'train_rates': 0.9062521291466411, 'learning_rate': 0.00016446984111186832, 'batch_size': 140, 'step_size': 4, 'gamma': 0.8484442536097003}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:23:53,468][0m Trial 31 finished with value: 0.03757385900471269 and parameters: {'observation_period_num': 5, 'train_rates': 0.8768660496703763, 'learning_rate': 0.00010001136546955714, 'batch_size': 87, 'step_size': 8, 'gamma': 0.842581279379736}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:24:54,996][0m Trial 32 finished with value: 0.03969317143506343 and parameters: {'observation_period_num': 8, 'train_rates': 0.8651873649489036, 'learning_rate': 0.00012804394243772041, 'batch_size': 94, 'step_size': 6, 'gamma': 0.8460729634932428}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:25:44,773][0m Trial 33 finished with value: 0.04380864714155134 and parameters: {'observation_period_num': 5, 'train_rates': 0.8490664670624182, 'learning_rate': 0.0001332374595766822, 'batch_size': 123, 'step_size': 5, 'gamma': 0.8497810579946307}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:28:15,807][0m Trial 34 finished with value: 0.04375743902735896 and parameters: {'observation_period_num': 30, 'train_rates': 0.8704005685554043, 'learning_rate': 0.0002999929089595344, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8150705211860766}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:29:09,472][0m Trial 35 finished with value: 0.22214713051349302 and parameters: {'observation_period_num': 252, 'train_rates': 0.827518360066988, 'learning_rate': 6.463723452115639e-05, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8827824725350475}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:30:48,181][0m Trial 36 finished with value: 0.18968629025256456 and parameters: {'observation_period_num': 143, 'train_rates': 0.9296799643910142, 'learning_rate': 3.622848575960983e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.793346618300034}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:31:28,076][0m Trial 37 finished with value: 0.10850119100282185 and parameters: {'observation_period_num': 58, 'train_rates': 0.7834570326554293, 'learning_rate': 0.0004268108832687687, 'batch_size': 137, 'step_size': 10, 'gamma': 0.8767069507777202}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:31:53,769][0m Trial 38 finished with value: 0.886589313896609 and parameters: {'observation_period_num': 20, 'train_rates': 0.8570717857058113, 'learning_rate': 1.2940882385474992e-06, 'batch_size': 252, 'step_size': 4, 'gamma': 0.8027975890061461}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:32:52,345][0m Trial 39 finished with value: 0.0535714927926121 and parameters: {'observation_period_num': 44, 'train_rates': 0.9158375502824386, 'learning_rate': 0.0002789403252370315, 'batch_size': 104, 'step_size': 6, 'gamma': 0.8260252867132842}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:35:00,924][0m Trial 40 finished with value: 0.6000875556904278 and parameters: {'observation_period_num': 190, 'train_rates': 0.7467505830464347, 'learning_rate': 1.0133470346860253e-05, 'batch_size': 37, 'step_size': 3, 'gamma': 0.7715493677089765}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:36:05,812][0m Trial 41 finished with value: 0.03936455721886438 and parameters: {'observation_period_num': 6, 'train_rates': 0.8919587865227228, 'learning_rate': 0.00010946317069387802, 'batch_size': 89, 'step_size': 8, 'gamma': 0.8415538575979009}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:36:45,844][0m Trial 42 finished with value: 0.04427986997227693 and parameters: {'observation_period_num': 5, 'train_rates': 0.8706097508638917, 'learning_rate': 0.00012458436773417925, 'batch_size': 151, 'step_size': 8, 'gamma': 0.8548930075812151}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:37:35,669][0m Trial 43 finished with value: 0.07896809771938144 and parameters: {'observation_period_num': 19, 'train_rates': 0.9467156614218272, 'learning_rate': 6.380844125462207e-05, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8236645053720956}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:40:50,697][0m Trial 44 finished with value: 0.21037273654955943 and parameters: {'observation_period_num': 34, 'train_rates': 0.8235166528961497, 'learning_rate': 0.00020766930323164816, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8434158130522965}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:41:43,761][0m Trial 45 finished with value: 0.10918894842826134 and parameters: {'observation_period_num': 67, 'train_rates': 0.7912176650278743, 'learning_rate': 4.622092617055784e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.783463456437694}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:43:14,088][0m Trial 46 finished with value: 0.043458696467880635 and parameters: {'observation_period_num': 17, 'train_rates': 0.9048928138391928, 'learning_rate': 8.065236609107362e-05, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9136710985920742}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:45:13,295][0m Trial 47 finished with value: 0.06604899067885031 and parameters: {'observation_period_num': 44, 'train_rates': 0.8721531854525283, 'learning_rate': 0.00012286210303332181, 'batch_size': 46, 'step_size': 10, 'gamma': 0.7993881906634807}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:46:24,766][0m Trial 48 finished with value: 0.06148655851021272 and parameters: {'observation_period_num': 15, 'train_rates': 0.9271606399692732, 'learning_rate': 0.00019576392943435904, 'batch_size': 83, 'step_size': 11, 'gamma': 0.872818724877808}. Best is trial 23 with value: 0.03361513830152743.[0m
[32m[I 2025-02-08 07:47:13,240][0m Trial 49 finished with value: 0.14372038491423458 and parameters: {'observation_period_num': 153, 'train_rates': 0.8446073485296096, 'learning_rate': 0.000510672592149379, 'batch_size': 112, 'step_size': 7, 'gamma': 0.8547766388669066}. Best is trial 23 with value: 0.03361513830152743.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.2775 | 0.1567
Epoch 2/300, Loss: 0.1393 | 0.1213
Epoch 3/300, Loss: 0.1227 | 0.1000
Epoch 4/300, Loss: 0.1150 | 0.0933
Epoch 5/300, Loss: 0.1114 | 0.0808
Epoch 6/300, Loss: 0.1115 | 0.0763
Epoch 7/300, Loss: 0.1137 | 0.0737
Epoch 8/300, Loss: 0.1147 | 0.0690
Epoch 9/300, Loss: 0.1078 | 0.0653
Epoch 10/300, Loss: 0.1047 | 0.0636
Epoch 11/300, Loss: 0.1030 | 0.0626
Epoch 12/300, Loss: 0.1018 | 0.0622
Epoch 13/300, Loss: 0.1005 | 0.0616
Epoch 14/300, Loss: 0.0990 | 0.0619
Epoch 15/300, Loss: 0.0971 | 0.0625
Epoch 16/300, Loss: 0.0954 | 0.0632
Epoch 17/300, Loss: 0.0938 | 0.0612
Epoch 18/300, Loss: 0.0921 | 0.0620
Epoch 19/300, Loss: 0.0906 | 0.0624
Epoch 20/300, Loss: 0.0894 | 0.0613
Epoch 21/300, Loss: 0.0885 | 0.0586
Epoch 22/300, Loss: 0.0879 | 0.0565
Epoch 23/300, Loss: 0.0870 | 0.0551
Epoch 24/300, Loss: 0.0863 | 0.0541
Epoch 25/300, Loss: 0.0856 | 0.0520
Epoch 26/300, Loss: 0.0851 | 0.0513
Epoch 27/300, Loss: 0.0846 | 0.0510
Epoch 28/300, Loss: 0.0841 | 0.0507
Epoch 29/300, Loss: 0.0837 | 0.0492
Epoch 30/300, Loss: 0.0835 | 0.0487
Epoch 31/300, Loss: 0.0831 | 0.0484
Epoch 32/300, Loss: 0.0827 | 0.0479
Epoch 33/300, Loss: 0.0824 | 0.0477
Epoch 34/300, Loss: 0.0821 | 0.0473
Epoch 35/300, Loss: 0.0818 | 0.0468
Epoch 36/300, Loss: 0.0815 | 0.0463
Epoch 37/300, Loss: 0.0812 | 0.0460
Epoch 38/300, Loss: 0.0810 | 0.0456
Epoch 39/300, Loss: 0.0810 | 0.0453
Epoch 40/300, Loss: 0.0810 | 0.0449
Epoch 41/300, Loss: 0.0810 | 0.0442
Epoch 42/300, Loss: 0.0811 | 0.0440
Epoch 43/300, Loss: 0.0812 | 0.0436
Epoch 44/300, Loss: 0.0812 | 0.0433
Epoch 45/300, Loss: 0.0814 | 0.0431
Epoch 46/300, Loss: 0.0815 | 0.0430
Epoch 47/300, Loss: 0.0810 | 0.0428
Epoch 48/300, Loss: 0.0805 | 0.0427
Epoch 49/300, Loss: 0.0801 | 0.0429
Epoch 50/300, Loss: 0.0795 | 0.0426
Epoch 51/300, Loss: 0.0788 | 0.0424
Epoch 52/300, Loss: 0.0784 | 0.0423
Epoch 53/300, Loss: 0.0782 | 0.0422
Epoch 54/300, Loss: 0.0781 | 0.0421
Epoch 55/300, Loss: 0.0781 | 0.0420
Epoch 56/300, Loss: 0.0780 | 0.0419
Epoch 57/300, Loss: 0.0780 | 0.0419
Epoch 58/300, Loss: 0.0779 | 0.0418
Epoch 59/300, Loss: 0.0779 | 0.0418
Epoch 60/300, Loss: 0.0777 | 0.0417
Epoch 61/300, Loss: 0.0776 | 0.0417
Epoch 62/300, Loss: 0.0776 | 0.0416
Epoch 63/300, Loss: 0.0774 | 0.0416
Epoch 64/300, Loss: 0.0773 | 0.0415
Epoch 65/300, Loss: 0.0772 | 0.0415
Epoch 66/300, Loss: 0.0771 | 0.0414
Epoch 67/300, Loss: 0.0770 | 0.0414
Epoch 68/300, Loss: 0.0769 | 0.0413
Epoch 69/300, Loss: 0.0769 | 0.0413
Epoch 70/300, Loss: 0.0768 | 0.0412
Epoch 71/300, Loss: 0.0767 | 0.0412
Epoch 72/300, Loss: 0.0767 | 0.0411
Epoch 73/300, Loss: 0.0766 | 0.0411
Epoch 74/300, Loss: 0.0766 | 0.0411
Epoch 75/300, Loss: 0.0766 | 0.0410
Epoch 76/300, Loss: 0.0765 | 0.0410
Epoch 77/300, Loss: 0.0765 | 0.0410
Epoch 78/300, Loss: 0.0764 | 0.0409
Epoch 79/300, Loss: 0.0764 | 0.0409
Epoch 80/300, Loss: 0.0764 | 0.0409
Epoch 81/300, Loss: 0.0763 | 0.0409
Epoch 82/300, Loss: 0.0763 | 0.0408
Epoch 83/300, Loss: 0.0763 | 0.0408
Epoch 84/300, Loss: 0.0763 | 0.0408
Epoch 85/300, Loss: 0.0762 | 0.0408
Epoch 86/300, Loss: 0.0762 | 0.0408
Epoch 87/300, Loss: 0.0762 | 0.0407
Epoch 88/300, Loss: 0.0762 | 0.0407
Epoch 89/300, Loss: 0.0762 | 0.0407
Epoch 90/300, Loss: 0.0761 | 0.0407
Epoch 91/300, Loss: 0.0761 | 0.0407
Epoch 92/300, Loss: 0.0761 | 0.0407
Epoch 93/300, Loss: 0.0761 | 0.0407
Epoch 94/300, Loss: 0.0761 | 0.0406
Epoch 95/300, Loss: 0.0761 | 0.0406
Epoch 96/300, Loss: 0.0760 | 0.0406
Epoch 97/300, Loss: 0.0760 | 0.0406
Epoch 98/300, Loss: 0.0760 | 0.0406
Epoch 99/300, Loss: 0.0760 | 0.0406
Epoch 100/300, Loss: 0.0760 | 0.0406
Epoch 101/300, Loss: 0.0760 | 0.0406
Epoch 102/300, Loss: 0.0759 | 0.0406
Epoch 103/300, Loss: 0.0759 | 0.0406
Epoch 104/300, Loss: 0.0759 | 0.0406
Epoch 105/300, Loss: 0.0759 | 0.0406
Epoch 106/300, Loss: 0.0759 | 0.0406
Epoch 107/300, Loss: 0.0759 | 0.0406
Epoch 108/300, Loss: 0.0759 | 0.0406
Epoch 109/300, Loss: 0.0759 | 0.0406
Epoch 110/300, Loss: 0.0759 | 0.0406
Epoch 111/300, Loss: 0.0759 | 0.0406
Epoch 112/300, Loss: 0.0759 | 0.0405
Epoch 113/300, Loss: 0.0758 | 0.0405
Epoch 114/300, Loss: 0.0758 | 0.0405
Epoch 115/300, Loss: 0.0758 | 0.0405
Epoch 116/300, Loss: 0.0758 | 0.0405
Epoch 117/300, Loss: 0.0758 | 0.0405
Epoch 118/300, Loss: 0.0758 | 0.0405
Epoch 119/300, Loss: 0.0758 | 0.0405
Epoch 120/300, Loss: 0.0758 | 0.0405
Epoch 121/300, Loss: 0.0758 | 0.0405
Epoch 122/300, Loss: 0.0758 | 0.0405
Epoch 123/300, Loss: 0.0758 | 0.0405
Epoch 124/300, Loss: 0.0758 | 0.0405
Epoch 125/300, Loss: 0.0758 | 0.0405
Epoch 126/300, Loss: 0.0758 | 0.0405
Epoch 127/300, Loss: 0.0758 | 0.0405
Epoch 128/300, Loss: 0.0758 | 0.0405
Epoch 129/300, Loss: 0.0758 | 0.0405
Epoch 130/300, Loss: 0.0758 | 0.0405
Epoch 131/300, Loss: 0.0758 | 0.0405
Epoch 132/300, Loss: 0.0758 | 0.0405
Epoch 133/300, Loss: 0.0758 | 0.0405
Epoch 134/300, Loss: 0.0758 | 0.0405
Epoch 135/300, Loss: 0.0758 | 0.0405
Epoch 136/300, Loss: 0.0758 | 0.0405
Epoch 137/300, Loss: 0.0758 | 0.0405
Epoch 138/300, Loss: 0.0758 | 0.0405
Epoch 139/300, Loss: 0.0758 | 0.0405
Epoch 140/300, Loss: 0.0758 | 0.0405
Epoch 141/300, Loss: 0.0758 | 0.0405
Epoch 142/300, Loss: 0.0758 | 0.0405
Epoch 143/300, Loss: 0.0758 | 0.0405
Epoch 144/300, Loss: 0.0757 | 0.0405
Epoch 145/300, Loss: 0.0757 | 0.0405
Epoch 146/300, Loss: 0.0757 | 0.0405
Epoch 147/300, Loss: 0.0757 | 0.0405
Epoch 148/300, Loss: 0.0757 | 0.0405
Epoch 149/300, Loss: 0.0757 | 0.0405
Epoch 150/300, Loss: 0.0757 | 0.0405
Epoch 151/300, Loss: 0.0757 | 0.0405
Epoch 152/300, Loss: 0.0757 | 0.0405
Epoch 153/300, Loss: 0.0757 | 0.0405
Epoch 154/300, Loss: 0.0757 | 0.0405
Epoch 155/300, Loss: 0.0757 | 0.0405
Epoch 156/300, Loss: 0.0757 | 0.0405
Epoch 157/300, Loss: 0.0757 | 0.0405
Epoch 158/300, Loss: 0.0757 | 0.0405
Epoch 159/300, Loss: 0.0757 | 0.0405
Epoch 160/300, Loss: 0.0757 | 0.0405
Epoch 161/300, Loss: 0.0757 | 0.0405
Epoch 162/300, Loss: 0.0757 | 0.0405
Epoch 163/300, Loss: 0.0757 | 0.0405
Epoch 164/300, Loss: 0.0757 | 0.0405
Epoch 165/300, Loss: 0.0757 | 0.0405
Epoch 166/300, Loss: 0.0757 | 0.0405
Epoch 167/300, Loss: 0.0757 | 0.0405
Epoch 168/300, Loss: 0.0757 | 0.0405
Epoch 169/300, Loss: 0.0757 | 0.0405
Epoch 170/300, Loss: 0.0757 | 0.0405
Epoch 171/300, Loss: 0.0757 | 0.0405
Epoch 172/300, Loss: 0.0757 | 0.0405
Epoch 173/300, Loss: 0.0757 | 0.0405
Epoch 174/300, Loss: 0.0757 | 0.0405
Epoch 175/300, Loss: 0.0757 | 0.0405
Epoch 176/300, Loss: 0.0757 | 0.0405
Epoch 177/300, Loss: 0.0757 | 0.0405
Epoch 178/300, Loss: 0.0757 | 0.0405
Epoch 179/300, Loss: 0.0757 | 0.0405
Epoch 180/300, Loss: 0.0757 | 0.0405
Epoch 181/300, Loss: 0.0757 | 0.0405
Epoch 182/300, Loss: 0.0757 | 0.0405
Epoch 183/300, Loss: 0.0757 | 0.0405
Epoch 184/300, Loss: 0.0757 | 0.0405
Epoch 185/300, Loss: 0.0757 | 0.0405
Epoch 186/300, Loss: 0.0757 | 0.0405
Epoch 187/300, Loss: 0.0757 | 0.0405
Epoch 188/300, Loss: 0.0757 | 0.0405
Epoch 189/300, Loss: 0.0757 | 0.0405
Epoch 190/300, Loss: 0.0757 | 0.0405
Epoch 191/300, Loss: 0.0757 | 0.0405
Epoch 192/300, Loss: 0.0757 | 0.0405
Epoch 193/300, Loss: 0.0757 | 0.0405
Epoch 194/300, Loss: 0.0757 | 0.0405
Epoch 195/300, Loss: 0.0757 | 0.0405
Epoch 196/300, Loss: 0.0757 | 0.0405
Epoch 197/300, Loss: 0.0757 | 0.0405
Epoch 198/300, Loss: 0.0757 | 0.0405
Epoch 199/300, Loss: 0.0757 | 0.0405
Epoch 200/300, Loss: 0.0757 | 0.0405
Epoch 201/300, Loss: 0.0757 | 0.0405
Epoch 202/300, Loss: 0.0757 | 0.0405
Epoch 203/300, Loss: 0.0757 | 0.0405
Epoch 204/300, Loss: 0.0757 | 0.0405
Epoch 205/300, Loss: 0.0757 | 0.0405
Epoch 206/300, Loss: 0.0757 | 0.0405
Epoch 207/300, Loss: 0.0757 | 0.0405
Epoch 208/300, Loss: 0.0757 | 0.0405
Epoch 209/300, Loss: 0.0757 | 0.0405
Epoch 210/300, Loss: 0.0757 | 0.0405
Epoch 211/300, Loss: 0.0757 | 0.0405
Epoch 212/300, Loss: 0.0757 | 0.0405
Epoch 213/300, Loss: 0.0757 | 0.0405
Epoch 214/300, Loss: 0.0757 | 0.0405
Epoch 215/300, Loss: 0.0757 | 0.0405
Epoch 216/300, Loss: 0.0757 | 0.0405
Epoch 217/300, Loss: 0.0757 | 0.0405
Epoch 218/300, Loss: 0.0757 | 0.0405
Epoch 219/300, Loss: 0.0757 | 0.0405
Epoch 220/300, Loss: 0.0757 | 0.0405
Epoch 221/300, Loss: 0.0757 | 0.0405
Epoch 222/300, Loss: 0.0757 | 0.0405
Epoch 223/300, Loss: 0.0757 | 0.0405
Epoch 224/300, Loss: 0.0757 | 0.0405
Epoch 225/300, Loss: 0.0757 | 0.0405
Epoch 226/300, Loss: 0.0757 | 0.0405
Epoch 227/300, Loss: 0.0757 | 0.0405
Epoch 228/300, Loss: 0.0757 | 0.0405
Epoch 229/300, Loss: 0.0757 | 0.0405
Epoch 230/300, Loss: 0.0757 | 0.0405
Epoch 231/300, Loss: 0.0757 | 0.0405
Epoch 232/300, Loss: 0.0757 | 0.0405
Epoch 233/300, Loss: 0.0757 | 0.0405
Epoch 234/300, Loss: 0.0757 | 0.0405
Epoch 235/300, Loss: 0.0757 | 0.0405
Epoch 236/300, Loss: 0.0757 | 0.0405
Epoch 237/300, Loss: 0.0757 | 0.0405
Epoch 238/300, Loss: 0.0757 | 0.0405
Epoch 239/300, Loss: 0.0757 | 0.0405
Epoch 240/300, Loss: 0.0757 | 0.0405
Epoch 241/300, Loss: 0.0757 | 0.0405
Epoch 242/300, Loss: 0.0757 | 0.0405
Epoch 243/300, Loss: 0.0757 | 0.0405
Epoch 244/300, Loss: 0.0757 | 0.0405
Epoch 245/300, Loss: 0.0757 | 0.0405
Epoch 246/300, Loss: 0.0757 | 0.0405
Epoch 247/300, Loss: 0.0757 | 0.0405
Epoch 248/300, Loss: 0.0757 | 0.0405
Epoch 249/300, Loss: 0.0757 | 0.0405
Epoch 250/300, Loss: 0.0757 | 0.0405
Epoch 251/300, Loss: 0.0757 | 0.0405
Epoch 252/300, Loss: 0.0757 | 0.0405
Epoch 253/300, Loss: 0.0757 | 0.0405
Epoch 254/300, Loss: 0.0757 | 0.0405
Epoch 255/300, Loss: 0.0757 | 0.0405
Epoch 256/300, Loss: 0.0757 | 0.0405
Epoch 257/300, Loss: 0.0757 | 0.0405
Epoch 258/300, Loss: 0.0757 | 0.0405
Epoch 259/300, Loss: 0.0757 | 0.0405
Epoch 260/300, Loss: 0.0757 | 0.0405
Epoch 261/300, Loss: 0.0757 | 0.0405
Epoch 262/300, Loss: 0.0757 | 0.0405
Epoch 263/300, Loss: 0.0757 | 0.0405
Epoch 264/300, Loss: 0.0757 | 0.0405
Epoch 265/300, Loss: 0.0757 | 0.0405
Epoch 266/300, Loss: 0.0757 | 0.0405
Epoch 267/300, Loss: 0.0757 | 0.0405
Early stopping
Runtime (seconds): 391.2701873779297
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 181.32461820938624
RMSE: 13.465682983398438
MAE: 13.465682983398438
R-squared: nan
[200.60568]
