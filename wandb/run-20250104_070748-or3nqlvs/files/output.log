ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 07:07:50,970][0m A new study created in memory with name: no-name-421e6f3b-4973-42e7-9b38-a6d9e39c9de8[0m
[32m[I 2025-01-04 07:09:07,249][0m Trial 0 finished with value: 0.08746377994184909 and parameters: {'observation_period_num': 239, 'train_rates': 0.9154601506414125, 'learning_rate': 0.00028067194275016626, 'batch_size': 70, 'step_size': 2, 'gamma': 0.8450344396361733}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:09:57,679][0m Trial 1 finished with value: 0.18145778080657096 and parameters: {'observation_period_num': 54, 'train_rates': 0.6488673047076713, 'learning_rate': 0.000979539579415875, 'batch_size': 92, 'step_size': 10, 'gamma': 0.96637807286184}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:10:20,639][0m Trial 2 finished with value: 0.1998496129773976 and parameters: {'observation_period_num': 94, 'train_rates': 0.6887143262318087, 'learning_rate': 0.0006621361082667189, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9136962395717622}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:11:02,037][0m Trial 3 finished with value: 0.39900564041144493 and parameters: {'observation_period_num': 212, 'train_rates': 0.6162028363207475, 'learning_rate': 2.037262332959915e-05, 'batch_size': 105, 'step_size': 11, 'gamma': 0.8420971488591348}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:11:21,679][0m Trial 4 finished with value: 0.28109112811989934 and parameters: {'observation_period_num': 216, 'train_rates': 0.6568922943644503, 'learning_rate': 9.864804472575135e-05, 'batch_size': 239, 'step_size': 10, 'gamma': 0.8982986363933602}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:13:29,153][0m Trial 5 finished with value: 0.2646036328168105 and parameters: {'observation_period_num': 216, 'train_rates': 0.6691840357436343, 'learning_rate': 0.00020438429408370446, 'batch_size': 33, 'step_size': 13, 'gamma': 0.901840944920269}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:13:58,443][0m Trial 6 finished with value: 0.7995477690416224 and parameters: {'observation_period_num': 184, 'train_rates': 0.8467435601454518, 'learning_rate': 1.4489611488581343e-06, 'batch_size': 195, 'step_size': 4, 'gamma': 0.9642890337673515}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:14:22,083][0m Trial 7 finished with value: 0.5720694733663204 and parameters: {'observation_period_num': 225, 'train_rates': 0.8071734269056596, 'learning_rate': 2.1106988053623384e-06, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9894679581575023}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:14:48,553][0m Trial 8 finished with value: 0.2753353240505961 and parameters: {'observation_period_num': 230, 'train_rates': 0.7263054896911972, 'learning_rate': 0.0007188976316363902, 'batch_size': 184, 'step_size': 3, 'gamma': 0.9539333253612217}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:15:23,450][0m Trial 9 finished with value: 0.28956492315855387 and parameters: {'observation_period_num': 82, 'train_rates': 0.6291916073491901, 'learning_rate': 6.247880647556772e-05, 'batch_size': 133, 'step_size': 5, 'gamma': 0.9738547126282701}. Best is trial 0 with value: 0.08746377994184909.[0m
Early stopping at epoch 51
[32m[I 2025-01-04 07:18:30,813][0m Trial 10 finished with value: 0.10148682236242637 and parameters: {'observation_period_num': 5, 'train_rates': 0.953130923662636, 'learning_rate': 1.1323011371347086e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.7619635922449597}. Best is trial 0 with value: 0.08746377994184909.[0m
Early stopping at epoch 61
[32m[I 2025-01-04 07:22:18,140][0m Trial 11 finished with value: 0.11607127380904866 and parameters: {'observation_period_num': 11, 'train_rates': 0.9774508053408958, 'learning_rate': 1.110452005666472e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.7515488279115342}. Best is trial 0 with value: 0.08746377994184909.[0m
Early stopping at epoch 52
[32m[I 2025-01-04 07:23:06,710][0m Trial 12 finished with value: 0.5600281327962875 and parameters: {'observation_period_num': 146, 'train_rates': 0.9503261102809041, 'learning_rate': 6.63923116365074e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.7985394734027265}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:24:43,823][0m Trial 13 finished with value: 0.2754431768229722 and parameters: {'observation_period_num': 153, 'train_rates': 0.9012283977992678, 'learning_rate': 4.764583129013553e-06, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8141170489480806}. Best is trial 0 with value: 0.08746377994184909.[0m
[32m[I 2025-01-04 07:26:02,104][0m Trial 14 finished with value: 0.10984804130805079 and parameters: {'observation_period_num': 21, 'train_rates': 0.902278483971297, 'learning_rate': 3.107267705255718e-05, 'batch_size': 73, 'step_size': 3, 'gamma': 0.7510609630166141}. Best is trial 0 with value: 0.08746377994184909.[0m
Early stopping at epoch 74
[32m[I 2025-01-04 07:27:59,988][0m Trial 15 finished with value: 0.07923494546841352 and parameters: {'observation_period_num': 111, 'train_rates': 0.9176498512973009, 'learning_rate': 0.0001877547395295718, 'batch_size': 35, 'step_size': 1, 'gamma': 0.794117490754644}. Best is trial 15 with value: 0.07923494546841352.[0m
[32m[I 2025-01-04 07:28:44,048][0m Trial 16 finished with value: 0.0605903058592739 and parameters: {'observation_period_num': 102, 'train_rates': 0.871621574023101, 'learning_rate': 0.00017193609747135296, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8528757760367505}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:29:22,750][0m Trial 17 finished with value: 0.0705006177906647 and parameters: {'observation_period_num': 109, 'train_rates': 0.8484873437639575, 'learning_rate': 0.00021284594594301982, 'batch_size': 148, 'step_size': 5, 'gamma': 0.7969502027676699}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:29:58,543][0m Trial 18 finished with value: 0.06730787519989095 and parameters: {'observation_period_num': 65, 'train_rates': 0.8387179362785683, 'learning_rate': 7.600597056248036e-05, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8628569710339414}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:30:33,389][0m Trial 19 finished with value: 0.22235145037980072 and parameters: {'observation_period_num': 59, 'train_rates': 0.7654420541405399, 'learning_rate': 7.109239229405605e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8762399457782591}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:31:17,302][0m Trial 20 finished with value: 0.07360058606898484 and parameters: {'observation_period_num': 42, 'train_rates': 0.8500139786428262, 'learning_rate': 4.81749627318182e-05, 'batch_size': 124, 'step_size': 6, 'gamma': 0.8587593317231291}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:31:51,438][0m Trial 21 finished with value: 0.06664511846940753 and parameters: {'observation_period_num': 127, 'train_rates': 0.8501849046566758, 'learning_rate': 0.00031891874826038845, 'batch_size': 161, 'step_size': 5, 'gamma': 0.8234263331070095}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:32:23,362][0m Trial 22 finished with value: 0.08734855294395029 and parameters: {'observation_period_num': 127, 'train_rates': 0.8112493588103491, 'learning_rate': 0.00037362519463861505, 'batch_size': 174, 'step_size': 5, 'gamma': 0.8290542911359859}. Best is trial 16 with value: 0.0605903058592739.[0m
[32m[I 2025-01-04 07:32:59,696][0m Trial 23 finished with value: 0.06018685219458623 and parameters: {'observation_period_num': 80, 'train_rates': 0.8688774511073922, 'learning_rate': 0.0001185690317641057, 'batch_size': 163, 'step_size': 8, 'gamma': 0.8783105863857448}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:33:25,836][0m Trial 24 finished with value: 0.26922275667643025 and parameters: {'observation_period_num': 160, 'train_rates': 0.7547335458205794, 'learning_rate': 0.00011816812729310378, 'batch_size': 204, 'step_size': 8, 'gamma': 0.8867183129120773}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:33:59,061][0m Trial 25 finished with value: 0.07818547450006008 and parameters: {'observation_period_num': 85, 'train_rates': 0.8817452228739043, 'learning_rate': 0.0004446363345424637, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9371996173075116}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:34:49,192][0m Trial 26 finished with value: 0.08440836981745527 and parameters: {'observation_period_num': 120, 'train_rates': 0.8763968267164904, 'learning_rate': 0.00014770832137427932, 'batch_size': 109, 'step_size': 4, 'gamma': 0.822955560729928}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:35:14,737][0m Trial 27 finished with value: 0.14670101296500998 and parameters: {'observation_period_num': 139, 'train_rates': 0.7887761026431158, 'learning_rate': 3.969009133037913e-05, 'batch_size': 205, 'step_size': 7, 'gamma': 0.9289130577986948}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:35:59,817][0m Trial 28 finished with value: 0.11280909579231906 and parameters: {'observation_period_num': 171, 'train_rates': 0.8253789477759782, 'learning_rate': 0.0004131222926351716, 'batch_size': 118, 'step_size': 12, 'gamma': 0.8447317038528597}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:36:34,791][0m Trial 29 finished with value: 0.07263068642507317 and parameters: {'observation_period_num': 96, 'train_rates': 0.9351593533311912, 'learning_rate': 0.0002939976472283881, 'batch_size': 167, 'step_size': 6, 'gamma': 0.7810401513224062}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:37:31,030][0m Trial 30 finished with value: 0.2194485767529561 and parameters: {'observation_period_num': 250, 'train_rates': 0.8750176755093585, 'learning_rate': 2.397485125725825e-05, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8728780371391888}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:38:07,858][0m Trial 31 finished with value: 0.06995015520965908 and parameters: {'observation_period_num': 71, 'train_rates': 0.8294160890366127, 'learning_rate': 0.00010934857645327548, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8485568571939011}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:38:42,414][0m Trial 32 finished with value: 0.06682938448963414 and parameters: {'observation_period_num': 45, 'train_rates': 0.788935860089797, 'learning_rate': 8.750239070162175e-05, 'batch_size': 136, 'step_size': 9, 'gamma': 0.8606078651299824}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:39:20,187][0m Trial 33 finished with value: 0.18266572656929025 and parameters: {'observation_period_num': 38, 'train_rates': 0.7809992082728153, 'learning_rate': 0.0002785604173522007, 'batch_size': 133, 'step_size': 9, 'gamma': 0.8305208857618008}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:39:48,145][0m Trial 34 finished with value: 0.1684798592986276 and parameters: {'observation_period_num': 46, 'train_rates': 0.7054096638668881, 'learning_rate': 0.0006194312267897407, 'batch_size': 164, 'step_size': 10, 'gamma': 0.883718329113494}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:40:45,315][0m Trial 35 finished with value: 0.21229164029028857 and parameters: {'observation_period_num': 97, 'train_rates': 0.7370863348424419, 'learning_rate': 0.00014340974187776785, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8573653369265842}. Best is trial 23 with value: 0.06018685219458623.[0m
[32m[I 2025-01-04 07:41:12,420][0m Trial 36 finished with value: 0.053426395111901746 and parameters: {'observation_period_num': 26, 'train_rates': 0.8586956166498234, 'learning_rate': 9.020050046741894e-05, 'batch_size': 227, 'step_size': 9, 'gamma': 0.9047661548233661}. Best is trial 36 with value: 0.053426395111901746.[0m
[32m[I 2025-01-04 07:41:37,200][0m Trial 37 finished with value: 0.10781246863891115 and parameters: {'observation_period_num': 192, 'train_rates': 0.8664159461888027, 'learning_rate': 0.00022408413223197232, 'batch_size': 220, 'step_size': 11, 'gamma': 0.9108780658003343}. Best is trial 36 with value: 0.053426395111901746.[0m
[32m[I 2025-01-04 07:42:02,100][0m Trial 38 finished with value: 0.04002952410115136 and parameters: {'observation_period_num': 29, 'train_rates': 0.9015082190851376, 'learning_rate': 0.000861419282321279, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8979270182060769}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:42:28,714][0m Trial 39 finished with value: 0.046420227736234665 and parameters: {'observation_period_num': 29, 'train_rates': 0.9203744273349985, 'learning_rate': 0.000915674257564221, 'batch_size': 235, 'step_size': 14, 'gamma': 0.8964415419468496}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:42:53,594][0m Trial 40 finished with value: 0.048785317689180374 and parameters: {'observation_period_num': 30, 'train_rates': 0.9206400074761303, 'learning_rate': 0.0009835604308584126, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9252286502982006}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:43:18,278][0m Trial 41 finished with value: 0.04997067978040556 and parameters: {'observation_period_num': 27, 'train_rates': 0.9089361511482611, 'learning_rate': 0.0008795982905228395, 'batch_size': 255, 'step_size': 14, 'gamma': 0.9275265419138348}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:43:43,065][0m Trial 42 finished with value: 0.041351646184921265 and parameters: {'observation_period_num': 27, 'train_rates': 0.9247313428057565, 'learning_rate': 0.0009155086345161861, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9288755020384325}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:44:09,691][0m Trial 43 finished with value: 0.052531275898218155 and parameters: {'observation_period_num': 30, 'train_rates': 0.9663206856039023, 'learning_rate': 0.00097873875246283, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9251861549865524}. Best is trial 38 with value: 0.04002952410115136.[0m
[32m[I 2025-01-04 07:44:36,584][0m Trial 44 finished with value: 0.0361865758895874 and parameters: {'observation_period_num': 15, 'train_rates': 0.9880314122301481, 'learning_rate': 0.0009857567457238021, 'batch_size': 252, 'step_size': 14, 'gamma': 0.9453681098437497}. Best is trial 44 with value: 0.0361865758895874.[0m
[32m[I 2025-01-04 07:45:05,494][0m Trial 45 finished with value: 0.04850310459733009 and parameters: {'observation_period_num': 9, 'train_rates': 0.9898937036503893, 'learning_rate': 0.0005575886125731848, 'batch_size': 238, 'step_size': 14, 'gamma': 0.9485057261431581}. Best is trial 44 with value: 0.0361865758895874.[0m
[32m[I 2025-01-04 07:45:34,293][0m Trial 46 finished with value: 0.04473172873258591 and parameters: {'observation_period_num': 14, 'train_rates': 0.9893982835482844, 'learning_rate': 0.0005741010497496603, 'batch_size': 236, 'step_size': 13, 'gamma': 0.9517000684399477}. Best is trial 44 with value: 0.0361865758895874.[0m
[32m[I 2025-01-04 07:46:01,054][0m Trial 47 finished with value: 0.03480035439133644 and parameters: {'observation_period_num': 15, 'train_rates': 0.9453685994712511, 'learning_rate': 0.0005761106183365596, 'batch_size': 234, 'step_size': 13, 'gamma': 0.9773082397617302}. Best is trial 47 with value: 0.03480035439133644.[0m
[32m[I 2025-01-04 07:46:29,853][0m Trial 48 finished with value: 0.032516054809093475 and parameters: {'observation_period_num': 17, 'train_rates': 0.9459639057544847, 'learning_rate': 0.0005448001859417389, 'batch_size': 217, 'step_size': 13, 'gamma': 0.9801770269150581}. Best is trial 48 with value: 0.032516054809093475.[0m
[32m[I 2025-01-04 07:46:58,614][0m Trial 49 finished with value: 0.03411127254366875 and parameters: {'observation_period_num': 5, 'train_rates': 0.9408484654121819, 'learning_rate': 0.0007044274447052378, 'batch_size': 215, 'step_size': 13, 'gamma': 0.9882971376077985}. Best is trial 48 with value: 0.032516054809093475.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 07:46:58,624][0m A new study created in memory with name: no-name-c0517553-f74c-483c-81f9-3c7e934cb9ae[0m
[32m[I 2025-01-04 07:48:02,467][0m Trial 0 finished with value: 0.09116957483232999 and parameters: {'observation_period_num': 195, 'train_rates': 0.9561095962801003, 'learning_rate': 0.0008402489961586529, 'batch_size': 90, 'step_size': 3, 'gamma': 0.7710292380542603}. Best is trial 0 with value: 0.09116957483232999.[0m
[32m[I 2025-01-04 07:48:31,987][0m Trial 1 finished with value: 0.0799639502733839 and parameters: {'observation_period_num': 90, 'train_rates': 0.9025413417239072, 'learning_rate': 0.0003146348302081747, 'batch_size': 214, 'step_size': 13, 'gamma': 0.8574920496131418}. Best is trial 1 with value: 0.0799639502733839.[0m
[32m[I 2025-01-04 07:48:55,613][0m Trial 2 finished with value: 0.16843034699559212 and parameters: {'observation_period_num': 15, 'train_rates': 0.754031766665069, 'learning_rate': 0.0004716448918174998, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8330222144753532}. Best is trial 1 with value: 0.0799639502733839.[0m
[32m[I 2025-01-04 07:49:26,398][0m Trial 3 finished with value: 0.1201983605237568 and parameters: {'observation_period_num': 206, 'train_rates': 0.8763005803929148, 'learning_rate': 0.00042312605750398276, 'batch_size': 173, 'step_size': 5, 'gamma': 0.970990770812501}. Best is trial 1 with value: 0.0799639502733839.[0m
[32m[I 2025-01-04 07:50:07,042][0m Trial 4 finished with value: 0.4578336477279663 and parameters: {'observation_period_num': 46, 'train_rates': 0.9639169601958586, 'learning_rate': 4.633148127618534e-06, 'batch_size': 148, 'step_size': 1, 'gamma': 0.9747028081623413}. Best is trial 1 with value: 0.0799639502733839.[0m
[32m[I 2025-01-04 07:51:38,155][0m Trial 5 finished with value: 0.07714783501895991 and parameters: {'observation_period_num': 43, 'train_rates': 0.9775908271378395, 'learning_rate': 0.00037030352912196726, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9313888699604471}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:52:04,875][0m Trial 6 finished with value: 0.10623289415469536 and parameters: {'observation_period_num': 227, 'train_rates': 0.9143284545468211, 'learning_rate': 0.0004293815682445742, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8613686029214022}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:52:39,489][0m Trial 7 finished with value: 0.07860102741941204 and parameters: {'observation_period_num': 95, 'train_rates': 0.7848267644283609, 'learning_rate': 0.00028022649901164034, 'batch_size': 152, 'step_size': 7, 'gamma': 0.8492791174700106}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:53:15,609][0m Trial 8 finished with value: 0.18183328495306128 and parameters: {'observation_period_num': 40, 'train_rates': 0.8542671994556903, 'learning_rate': 3.6066777254592186e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9705545722223818}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:53:49,593][0m Trial 9 finished with value: 1.0298765439253588 and parameters: {'observation_period_num': 163, 'train_rates': 0.8137569663613738, 'learning_rate': 1.4632583291825425e-06, 'batch_size': 160, 'step_size': 4, 'gamma': 0.8126225413781913}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:57:54,295][0m Trial 10 finished with value: 0.2065906896670856 and parameters: {'observation_period_num': 128, 'train_rates': 0.642414882520931, 'learning_rate': 5.770680140641218e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.922042028073114}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:58:51,600][0m Trial 11 finished with value: 0.2166555508140074 and parameters: {'observation_period_num': 81, 'train_rates': 0.7379743446698006, 'learning_rate': 8.550066637936394e-05, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9096990437425334}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 07:59:41,544][0m Trial 12 finished with value: 0.1988493653026017 and parameters: {'observation_period_num': 98, 'train_rates': 0.6657450589449871, 'learning_rate': 0.00010241051489211327, 'batch_size': 94, 'step_size': 8, 'gamma': 0.9126707249898224}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 08:02:15,200][0m Trial 13 finished with value: 0.2847618818670124 and parameters: {'observation_period_num': 133, 'train_rates': 0.7269115307945991, 'learning_rate': 1.4208025828867311e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8870022049197068}. Best is trial 5 with value: 0.07714783501895991.[0m
[32m[I 2025-01-04 08:03:02,658][0m Trial 14 finished with value: 0.06075098980218172 and parameters: {'observation_period_num': 58, 'train_rates': 0.8068111169173656, 'learning_rate': 0.00017882007924546628, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9397376335611305}. Best is trial 14 with value: 0.06075098980218172.[0m
[32m[I 2025-01-04 08:04:28,641][0m Trial 15 finished with value: 0.043728687611142715 and parameters: {'observation_period_num': 7, 'train_rates': 0.8346586620493583, 'learning_rate': 2.326598493802214e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9403611617743082}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:05:15,659][0m Trial 16 finished with value: 0.06384238254123369 and parameters: {'observation_period_num': 14, 'train_rates': 0.8289164432949037, 'learning_rate': 1.82142651673018e-05, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9457000462663886}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:06:48,583][0m Trial 17 finished with value: 0.14691699905791325 and parameters: {'observation_period_num': 7, 'train_rates': 0.6948401545002316, 'learning_rate': 3.109102081552551e-05, 'batch_size': 51, 'step_size': 14, 'gamma': 0.8928873571108757}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:07:28,218][0m Trial 18 finished with value: 0.20535370640780615 and parameters: {'observation_period_num': 65, 'train_rates': 0.6017520284128104, 'learning_rate': 0.0001601872158448764, 'batch_size': 111, 'step_size': 9, 'gamma': 0.9511191404325415}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:08:57,970][0m Trial 19 finished with value: 0.25762657226076596 and parameters: {'observation_period_num': 66, 'train_rates': 0.7816869078330281, 'learning_rate': 8.394199249889526e-06, 'batch_size': 55, 'step_size': 6, 'gamma': 0.9850219276981094}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:09:39,812][0m Trial 20 finished with value: 0.1041616081373097 and parameters: {'observation_period_num': 125, 'train_rates': 0.8457562609669267, 'learning_rate': 3.236573952948587e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.8854676935148584}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:10:22,727][0m Trial 21 finished with value: 0.0688913818213004 and parameters: {'observation_period_num': 25, 'train_rates': 0.8112324385878058, 'learning_rate': 1.9964991463587432e-05, 'batch_size': 126, 'step_size': 10, 'gamma': 0.9530397120235448}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:11:13,767][0m Trial 22 finished with value: 0.049316677396234715 and parameters: {'observation_period_num': 27, 'train_rates': 0.8378804838186985, 'learning_rate': 4.586845983457557e-05, 'batch_size': 107, 'step_size': 13, 'gamma': 0.9374034924895018}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:12:31,204][0m Trial 23 finished with value: 0.049361842298289625 and parameters: {'observation_period_num': 33, 'train_rates': 0.8876996745309541, 'learning_rate': 5.627822338384094e-05, 'batch_size': 73, 'step_size': 15, 'gamma': 0.9368292230005371}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:13:47,300][0m Trial 24 finished with value: 0.052411269880696755 and parameters: {'observation_period_num': 33, 'train_rates': 0.9052525702009221, 'learning_rate': 3.945039276916487e-05, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9018350366169472}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:15:43,958][0m Trial 25 finished with value: 0.06151321384028472 and parameters: {'observation_period_num': 27, 'train_rates': 0.8775263982682584, 'learning_rate': 9.329487217749703e-06, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9261292002192982}. Best is trial 15 with value: 0.043728687611142715.[0m
[32m[I 2025-01-04 08:16:45,370][0m Trial 26 finished with value: 0.03906196320152029 and parameters: {'observation_period_num': 8, 'train_rates': 0.9363060945027469, 'learning_rate': 7.20361830641638e-05, 'batch_size': 97, 'step_size': 13, 'gamma': 0.9657375403309343}. Best is trial 26 with value: 0.03906196320152029.[0m
[32m[I 2025-01-04 08:17:44,410][0m Trial 27 finished with value: 0.036112108916947334 and parameters: {'observation_period_num': 6, 'train_rates': 0.9328595062848286, 'learning_rate': 9.181945166481782e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.9899590481737708}. Best is trial 27 with value: 0.036112108916947334.[0m
[32m[I 2025-01-04 08:20:28,137][0m Trial 28 finished with value: 0.029611416447621126 and parameters: {'observation_period_num': 6, 'train_rates': 0.9339350599113521, 'learning_rate': 0.0001011575233059157, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9664180353733899}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:23:30,260][0m Trial 29 finished with value: 0.08900146759473361 and parameters: {'observation_period_num': 155, 'train_rates': 0.9350450433802504, 'learning_rate': 0.00011142482405907301, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7501388565979599}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:24:29,480][0m Trial 30 finished with value: 0.07907147705554962 and parameters: {'observation_period_num': 53, 'train_rates': 0.9399842772856426, 'learning_rate': 0.0009880882046516795, 'batch_size': 99, 'step_size': 14, 'gamma': 0.9887535547843682}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:27:01,579][0m Trial 31 finished with value: 0.03229075948024072 and parameters: {'observation_period_num': 5, 'train_rates': 0.9414042652023062, 'learning_rate': 8.149669877760951e-05, 'batch_size': 38, 'step_size': 12, 'gamma': 0.9606906294276377}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:29:33,622][0m Trial 32 finished with value: 0.1026425648753236 and parameters: {'observation_period_num': 252, 'train_rates': 0.9848724718006521, 'learning_rate': 0.0001548925029626152, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9622845227171123}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:35:08,902][0m Trial 33 finished with value: 0.031564134698022495 and parameters: {'observation_period_num': 5, 'train_rates': 0.9442766993204862, 'learning_rate': 7.330775669944939e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9884434836752991}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:39:57,733][0m Trial 34 finished with value: 0.04432101987979629 and parameters: {'observation_period_num': 20, 'train_rates': 0.9552579311970484, 'learning_rate': 0.0002480489114893303, 'batch_size': 20, 'step_size': 14, 'gamma': 0.982690021080205}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:42:15,410][0m Trial 35 finished with value: 0.10775168220884299 and parameters: {'observation_period_num': 79, 'train_rates': 0.9190719816297198, 'learning_rate': 0.0006509344377371637, 'batch_size': 40, 'step_size': 12, 'gamma': 0.9886952537209819}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:42:50,426][0m Trial 36 finished with value: 0.06369658559560776 and parameters: {'observation_period_num': 44, 'train_rates': 0.9604102530596388, 'learning_rate': 0.0001236470551443487, 'batch_size': 179, 'step_size': 14, 'gamma': 0.9601680909899958}. Best is trial 28 with value: 0.029611416447621126.[0m
[32m[I 2025-01-04 08:46:47,633][0m Trial 37 finished with value: 0.027914311305705448 and parameters: {'observation_period_num': 5, 'train_rates': 0.8859484757634755, 'learning_rate': 0.00022311137896130736, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9740778183337702}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 08:52:16,107][0m Trial 38 finished with value: 0.03545098826289177 and parameters: {'observation_period_num': 21, 'train_rates': 0.9899054488980483, 'learning_rate': 0.00025324234313625084, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9744675035962052}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 08:54:21,997][0m Trial 39 finished with value: 0.11079474556487041 and parameters: {'observation_period_num': 107, 'train_rates': 0.897762201680665, 'learning_rate': 0.0006296796207775715, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9579483148370738}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 08:55:51,570][0m Trial 40 finished with value: 0.0850268762677591 and parameters: {'observation_period_num': 176, 'train_rates': 0.8647347101512166, 'learning_rate': 0.00020540248776875514, 'batch_size': 58, 'step_size': 2, 'gamma': 0.7993231282579901}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:00:07,785][0m Trial 41 finished with value: 0.039218379779615335 and parameters: {'observation_period_num': 18, 'train_rates': 0.975673518840433, 'learning_rate': 0.0003202372203326904, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9730374654373812}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:05:27,637][0m Trial 42 finished with value: 0.055306928874063896 and parameters: {'observation_period_num': 38, 'train_rates': 0.9593522627777745, 'learning_rate': 0.00023982536290648973, 'batch_size': 18, 'step_size': 2, 'gamma': 0.9750355743637197}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:05:55,069][0m Trial 43 finished with value: 0.058978430926799774 and parameters: {'observation_period_num': 16, 'train_rates': 0.9828846487830794, 'learning_rate': 6.837718852735127e-05, 'batch_size': 251, 'step_size': 4, 'gamma': 0.9738188018049526}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:08:50,522][0m Trial 44 finished with value: 0.07510306421787508 and parameters: {'observation_period_num': 48, 'train_rates': 0.9149037942603924, 'learning_rate': 0.0004959890006227712, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9498704189941863}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:14:51,051][0m Trial 45 finished with value: 0.03195106703788042 and parameters: {'observation_period_num': 5, 'train_rates': 0.9566569949902155, 'learning_rate': 0.00014094383305622734, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9772511714891126}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:17:22,770][0m Trial 46 finished with value: 0.043209898196587895 and parameters: {'observation_period_num': 34, 'train_rates': 0.9463794783110009, 'learning_rate': 0.00013409848578935715, 'batch_size': 38, 'step_size': 5, 'gamma': 0.9202245208509566}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:18:35,080][0m Trial 47 finished with value: 0.03740304362670076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9175299033833273, 'learning_rate': 8.124646333988153e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.8723239983437774}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:19:56,714][0m Trial 48 finished with value: 0.07826349574822587 and parameters: {'observation_period_num': 214, 'train_rates': 0.8856996427826473, 'learning_rate': 0.0003399781239601357, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8371754385505784}. Best is trial 37 with value: 0.027914311305705448.[0m
[32m[I 2025-01-04 09:20:29,628][0m Trial 49 finished with value: 0.11221308261156082 and parameters: {'observation_period_num': 56, 'train_rates': 0.9684104458842417, 'learning_rate': 4.9905795962021355e-05, 'batch_size': 190, 'step_size': 1, 'gamma': 0.966255620578892}. Best is trial 37 with value: 0.027914311305705448.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 09:20:29,638][0m A new study created in memory with name: no-name-7b0a6dbe-e88a-4f3d-999f-9880f6b8bd3a[0m
[32m[I 2025-01-04 09:22:10,313][0m Trial 0 finished with value: 0.5259258030576909 and parameters: {'observation_period_num': 95, 'train_rates': 0.9508126775277541, 'learning_rate': 1.0036583940751495e-06, 'batch_size': 57, 'step_size': 11, 'gamma': 0.9087739881980672}. Best is trial 0 with value: 0.5259258030576909.[0m
[32m[I 2025-01-04 09:23:30,566][0m Trial 1 finished with value: 0.29496250205327956 and parameters: {'observation_period_num': 66, 'train_rates': 0.7535197107339191, 'learning_rate': 7.845645614653537e-06, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8774525940380791}. Best is trial 1 with value: 0.29496250205327956.[0m
Early stopping at epoch 89
[32m[I 2025-01-04 09:26:14,463][0m Trial 2 finished with value: 0.3297765245957739 and parameters: {'observation_period_num': 146, 'train_rates': 0.7202984399624162, 'learning_rate': 6.221604012112969e-05, 'batch_size': 25, 'step_size': 1, 'gamma': 0.8209155846391639}. Best is trial 1 with value: 0.29496250205327956.[0m
[32m[I 2025-01-04 09:26:42,252][0m Trial 3 finished with value: 0.23909977935764887 and parameters: {'observation_period_num': 158, 'train_rates': 0.8433297370192709, 'learning_rate': 2.9715792566643628e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.7818790568819384}. Best is trial 3 with value: 0.23909977935764887.[0m
[32m[I 2025-01-04 09:27:05,003][0m Trial 4 finished with value: 0.39318249145699613 and parameters: {'observation_period_num': 121, 'train_rates': 0.6841325892821603, 'learning_rate': 2.4516082652502827e-05, 'batch_size': 220, 'step_size': 6, 'gamma': 0.8400040628552079}. Best is trial 3 with value: 0.23909977935764887.[0m
[32m[I 2025-01-04 09:27:38,707][0m Trial 5 finished with value: 0.10797756805780377 and parameters: {'observation_period_num': 59, 'train_rates': 0.82509542871441, 'learning_rate': 1.4292493079130977e-05, 'batch_size': 167, 'step_size': 9, 'gamma': 0.9340708788957044}. Best is trial 5 with value: 0.10797756805780377.[0m
[32m[I 2025-01-04 09:29:04,928][0m Trial 6 finished with value: 0.11895645461803259 and parameters: {'observation_period_num': 108, 'train_rates': 0.9245184503759847, 'learning_rate': 7.271956772563659e-06, 'batch_size': 65, 'step_size': 9, 'gamma': 0.9629563046468016}. Best is trial 5 with value: 0.10797756805780377.[0m
[32m[I 2025-01-04 09:29:29,667][0m Trial 7 finished with value: 0.11982393065935995 and parameters: {'observation_period_num': 229, 'train_rates': 0.8803501648776161, 'learning_rate': 0.00012236601360940738, 'batch_size': 235, 'step_size': 10, 'gamma': 0.7759530980220469}. Best is trial 5 with value: 0.10797756805780377.[0m
[32m[I 2025-01-04 09:30:39,699][0m Trial 8 finished with value: 0.521735410106943 and parameters: {'observation_period_num': 110, 'train_rates': 0.6528031602686097, 'learning_rate': 3.2413823076705366e-06, 'batch_size': 63, 'step_size': 5, 'gamma': 0.8765213669057104}. Best is trial 5 with value: 0.10797756805780377.[0m
[32m[I 2025-01-04 09:31:18,692][0m Trial 9 finished with value: 0.1748814301138548 and parameters: {'observation_period_num': 96, 'train_rates': 0.6439571918789211, 'learning_rate': 0.00014331504234633047, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8855781088416658}. Best is trial 5 with value: 0.10797756805780377.[0m
[32m[I 2025-01-04 09:31:52,696][0m Trial 10 finished with value: 0.05098876793836725 and parameters: {'observation_period_num': 29, 'train_rates': 0.8017746459370751, 'learning_rate': 0.0007400740332657095, 'batch_size': 165, 'step_size': 3, 'gamma': 0.980460646323528}. Best is trial 10 with value: 0.05098876793836725.[0m
[32m[I 2025-01-04 09:32:28,907][0m Trial 11 finished with value: 0.029458774314743693 and parameters: {'observation_period_num': 7, 'train_rates': 0.8033857963319848, 'learning_rate': 0.0008062079867485131, 'batch_size': 157, 'step_size': 1, 'gamma': 0.988781247647316}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:33:10,486][0m Trial 12 finished with value: 0.1775936936939839 and parameters: {'observation_period_num': 12, 'train_rates': 0.7760024817445037, 'learning_rate': 0.000879509970063693, 'batch_size': 133, 'step_size': 1, 'gamma': 0.9894810629734795}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:33:44,447][0m Trial 13 finished with value: 0.04834141569633747 and parameters: {'observation_period_num': 5, 'train_rates': 0.8097674643703784, 'learning_rate': 0.0009998720866203595, 'batch_size': 167, 'step_size': 3, 'gamma': 0.9848263134506581}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:34:20,135][0m Trial 14 finished with value: 0.043440275075663996 and parameters: {'observation_period_num': 48, 'train_rates': 0.8941008892945285, 'learning_rate': 0.0003963147626900482, 'batch_size': 171, 'step_size': 3, 'gamma': 0.9473180996142868}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:35:19,072][0m Trial 15 finished with value: 0.042417261749506 and parameters: {'observation_period_num': 46, 'train_rates': 0.9851936278813589, 'learning_rate': 0.00028702406582520166, 'batch_size': 104, 'step_size': 3, 'gamma': 0.9422860079787929}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:36:18,209][0m Trial 16 finished with value: 0.1366133838891983 and parameters: {'observation_period_num': 187, 'train_rates': 0.9876544640021495, 'learning_rate': 0.00026303178845604085, 'batch_size': 101, 'step_size': 1, 'gamma': 0.9222248053311786}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:36:58,359][0m Trial 17 finished with value: 0.1584326180141749 and parameters: {'observation_period_num': 38, 'train_rates': 0.6011933969403729, 'learning_rate': 0.0003307264397385474, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9615351891317333}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:38:07,589][0m Trial 18 finished with value: 0.08889058977365494 and parameters: {'observation_period_num': 74, 'train_rates': 0.9892730356826165, 'learning_rate': 9.697523131779264e-05, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9095122293000718}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:38:30,496][0m Trial 19 finished with value: 0.046378240379549206 and parameters: {'observation_period_num': 27, 'train_rates': 0.856425426501588, 'learning_rate': 0.00023011552495779121, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9503735590374803}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:39:05,895][0m Trial 20 finished with value: 0.32073446251104737 and parameters: {'observation_period_num': 78, 'train_rates': 0.7389225842302565, 'learning_rate': 6.532270315242236e-05, 'batch_size': 148, 'step_size': 2, 'gamma': 0.8283852311836662}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:39:36,813][0m Trial 21 finished with value: 0.04343956223532666 and parameters: {'observation_period_num': 48, 'train_rates': 0.9058193983580825, 'learning_rate': 0.000453960951694737, 'batch_size': 189, 'step_size': 4, 'gamma': 0.9416736871702005}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:40:07,558][0m Trial 22 finished with value: 0.03174176812171936 and parameters: {'observation_period_num': 6, 'train_rates': 0.9382832834680037, 'learning_rate': 0.0005194536884292207, 'batch_size': 204, 'step_size': 4, 'gamma': 0.9312494568890187}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:40:38,269][0m Trial 23 finished with value: 0.03410212695598602 and parameters: {'observation_period_num': 7, 'train_rates': 0.9368065821968155, 'learning_rate': 0.0006128259974748113, 'batch_size': 211, 'step_size': 5, 'gamma': 0.9014484729432914}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:41:08,881][0m Trial 24 finished with value: 0.033482056111097336 and parameters: {'observation_period_num': 8, 'train_rates': 0.9404000788723262, 'learning_rate': 0.0005806890136856983, 'batch_size': 204, 'step_size': 5, 'gamma': 0.8999454022080597}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:41:35,771][0m Trial 25 finished with value: 0.05491391569375992 and parameters: {'observation_period_num': 26, 'train_rates': 0.949310857732837, 'learning_rate': 0.0001790898844620085, 'batch_size': 237, 'step_size': 7, 'gamma': 0.8501497032503338}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:42:04,553][0m Trial 26 finished with value: 0.0851199472971384 and parameters: {'observation_period_num': 243, 'train_rates': 0.8666675810207785, 'learning_rate': 0.0005483610220679544, 'batch_size': 189, 'step_size': 5, 'gamma': 0.8611329321223984}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:42:43,703][0m Trial 27 finished with value: 0.08040779575764426 and parameters: {'observation_period_num': 20, 'train_rates': 0.9139633104332503, 'learning_rate': 6.819847302479331e-05, 'batch_size': 150, 'step_size': 2, 'gamma': 0.8899308703891075}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:43:09,619][0m Trial 28 finished with value: 0.14475786618876185 and parameters: {'observation_period_num': 202, 'train_rates': 0.7779523961037529, 'learning_rate': 0.0009975039414535924, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9249110376784407}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:43:36,268][0m Trial 29 finished with value: 1.088369607925415 and parameters: {'observation_period_num': 79, 'train_rates': 0.9587899164691231, 'learning_rate': 1.067635755978304e-06, 'batch_size': 234, 'step_size': 7, 'gamma': 0.909645726174544}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:44:11,168][0m Trial 30 finished with value: 0.04808446764945984 and parameters: {'observation_period_num': 35, 'train_rates': 0.9585544502357773, 'learning_rate': 0.0004884694085541033, 'batch_size': 184, 'step_size': 2, 'gamma': 0.9642088390217143}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:44:42,088][0m Trial 31 finished with value: 0.032995548099279404 and parameters: {'observation_period_num': 5, 'train_rates': 0.9370889283511663, 'learning_rate': 0.0006097375349590177, 'batch_size': 207, 'step_size': 5, 'gamma': 0.9103991631516285}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:45:06,843][0m Trial 32 finished with value: 0.034196287393569946 and parameters: {'observation_period_num': 5, 'train_rates': 0.8917261081640858, 'learning_rate': 0.0006096861994103958, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9023824211931049}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:45:35,723][0m Trial 33 finished with value: 0.05836217746600113 and parameters: {'observation_period_num': 62, 'train_rates': 0.9219886266466795, 'learning_rate': 0.00020284494960105383, 'batch_size': 215, 'step_size': 4, 'gamma': 0.9182052980258809}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:46:08,587][0m Trial 34 finished with value: 0.056434888392686844 and parameters: {'observation_period_num': 20, 'train_rates': 0.9603859268014656, 'learning_rate': 0.000360404821114371, 'batch_size': 199, 'step_size': 5, 'gamma': 0.7982463733307049}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:46:40,567][0m Trial 35 finished with value: 0.07010072187797443 and parameters: {'observation_period_num': 156, 'train_rates': 0.8412767111080726, 'learning_rate': 0.0005849014886397749, 'batch_size': 180, 'step_size': 8, 'gamma': 0.866921392453536}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:47:05,962][0m Trial 36 finished with value: 0.12599346934937 and parameters: {'observation_period_num': 137, 'train_rates': 0.8697410446988426, 'learning_rate': 3.777437596854234e-05, 'batch_size': 226, 'step_size': 6, 'gamma': 0.89510319195849}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:47:36,824][0m Trial 37 finished with value: 0.05563734844326973 and parameters: {'observation_period_num': 55, 'train_rates': 0.9373390429122233, 'learning_rate': 0.0003644777996247332, 'batch_size': 201, 'step_size': 1, 'gamma': 0.9722596232867411}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:48:10,311][0m Trial 38 finished with value: 0.18099137902385956 and parameters: {'observation_period_num': 18, 'train_rates': 0.7184694808057936, 'learning_rate': 0.00011970859289180367, 'batch_size': 153, 'step_size': 8, 'gamma': 0.7593269669085162}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:51:21,807][0m Trial 39 finished with value: 0.07614549624105525 and parameters: {'observation_period_num': 37, 'train_rates': 0.8232083268152532, 'learning_rate': 1.1247454208122554e-05, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9237104572359256}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:51:48,239][0m Trial 40 finished with value: 0.09305993765066033 and parameters: {'observation_period_num': 87, 'train_rates': 0.9012342725733569, 'learning_rate': 4.018115430777827e-05, 'batch_size': 241, 'step_size': 11, 'gamma': 0.952117267038776}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:52:19,221][0m Trial 41 finished with value: 0.034773778170347214 and parameters: {'observation_period_num': 6, 'train_rates': 0.9320368661519157, 'learning_rate': 0.0006922863394181956, 'batch_size': 211, 'step_size': 5, 'gamma': 0.8761346833352401}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:52:50,065][0m Trial 42 finished with value: 0.04076511785387993 and parameters: {'observation_period_num': 16, 'train_rates': 0.9716669307571418, 'learning_rate': 0.000730847627443138, 'batch_size': 219, 'step_size': 6, 'gamma': 0.9007606558605038}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:53:21,604][0m Trial 43 finished with value: 0.044646844267845154 and parameters: {'observation_period_num': 35, 'train_rates': 0.939075488076315, 'learning_rate': 0.0005456379481874644, 'batch_size': 202, 'step_size': 5, 'gamma': 0.8894318060694324}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:53:45,740][0m Trial 44 finished with value: 0.15000274858063029 and parameters: {'observation_period_num': 12, 'train_rates': 0.7032868529565734, 'learning_rate': 0.00015806379370346535, 'batch_size': 225, 'step_size': 8, 'gamma': 0.9116397517009468}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:54:19,179][0m Trial 45 finished with value: 0.03610632699205581 and parameters: {'observation_period_num': 27, 'train_rates': 0.9143723413100627, 'learning_rate': 0.0008120851194645717, 'batch_size': 181, 'step_size': 4, 'gamma': 0.9279213290381386}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:54:59,139][0m Trial 46 finished with value: 0.045209990920999504 and parameters: {'observation_period_num': 48, 'train_rates': 0.8437186982121255, 'learning_rate': 0.0002519429950382304, 'batch_size': 138, 'step_size': 3, 'gamma': 0.9343944457481083}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:55:31,941][0m Trial 47 finished with value: 0.03454201485466406 and parameters: {'observation_period_num': 6, 'train_rates': 0.8826338575496652, 'learning_rate': 0.00044387811226207326, 'batch_size': 174, 'step_size': 15, 'gamma': 0.8789115676352868}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:56:10,921][0m Trial 48 finished with value: 0.6125339865684509 and parameters: {'observation_period_num': 67, 'train_rates': 0.967865409871602, 'learning_rate': 3.3943181640921323e-06, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8554477138205054}. Best is trial 11 with value: 0.029458774314743693.[0m
[32m[I 2025-01-04 09:56:41,733][0m Trial 49 finished with value: 0.08618000894784927 and parameters: {'observation_period_num': 184, 'train_rates': 0.9414555543619567, 'learning_rate': 0.0003010813599260656, 'batch_size': 191, 'step_size': 9, 'gamma': 0.8406461733462527}. Best is trial 11 with value: 0.029458774314743693.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 09:56:41,743][0m A new study created in memory with name: no-name-0f5bc1e1-2eec-4af3-9f48-9940acc45809[0m
[32m[I 2025-01-04 09:57:34,107][0m Trial 0 finished with value: 0.44746743661954125 and parameters: {'observation_period_num': 238, 'train_rates': 0.7495440102853517, 'learning_rate': 7.09037743073263e-05, 'batch_size': 91, 'step_size': 1, 'gamma': 0.8810845944515364}. Best is trial 0 with value: 0.44746743661954125.[0m
[32m[I 2025-01-04 09:58:01,100][0m Trial 1 finished with value: 0.4310981040000916 and parameters: {'observation_period_num': 206, 'train_rates': 0.9093064876865846, 'learning_rate': 3.2502303403888933e-06, 'batch_size': 214, 'step_size': 13, 'gamma': 0.8709616318386031}. Best is trial 1 with value: 0.4310981040000916.[0m
[32m[I 2025-01-04 09:58:31,038][0m Trial 2 finished with value: 0.23811332495124252 and parameters: {'observation_period_num': 231, 'train_rates': 0.7819151725425454, 'learning_rate': 1.3043392419787737e-05, 'batch_size': 176, 'step_size': 11, 'gamma': 0.9403303109149235}. Best is trial 2 with value: 0.23811332495124252.[0m
[32m[I 2025-01-04 10:00:11,653][0m Trial 3 finished with value: 0.07859285369284394 and parameters: {'observation_period_num': 159, 'train_rates': 0.815277010997432, 'learning_rate': 0.00020497759410964824, 'batch_size': 50, 'step_size': 5, 'gamma': 0.863674047510675}. Best is trial 3 with value: 0.07859285369284394.[0m
[32m[I 2025-01-04 10:00:45,603][0m Trial 4 finished with value: 0.16899511845000667 and parameters: {'observation_period_num': 52, 'train_rates': 0.6675634722889221, 'learning_rate': 0.0001871223670151038, 'batch_size': 142, 'step_size': 6, 'gamma': 0.8327272160535301}. Best is trial 3 with value: 0.07859285369284394.[0m
[32m[I 2025-01-04 10:01:12,402][0m Trial 5 finished with value: 0.21633954346179962 and parameters: {'observation_period_num': 247, 'train_rates': 0.9732959646644836, 'learning_rate': 2.9373054509630298e-05, 'batch_size': 234, 'step_size': 7, 'gamma': 0.8465222729635775}. Best is trial 3 with value: 0.07859285369284394.[0m
[32m[I 2025-01-04 10:02:06,480][0m Trial 6 finished with value: 0.22457153879282044 and parameters: {'observation_period_num': 102, 'train_rates': 0.77726486738261, 'learning_rate': 4.461497227447234e-05, 'batch_size': 96, 'step_size': 15, 'gamma': 0.9788923703906124}. Best is trial 3 with value: 0.07859285369284394.[0m
[32m[I 2025-01-04 10:03:13,431][0m Trial 7 finished with value: 0.09207887947559357 and parameters: {'observation_period_num': 157, 'train_rates': 0.9818864552569189, 'learning_rate': 0.0003523311689499351, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8596141344239545}. Best is trial 3 with value: 0.07859285369284394.[0m
[32m[I 2025-01-04 10:03:49,155][0m Trial 8 finished with value: 0.043686759852183364 and parameters: {'observation_period_num': 40, 'train_rates': 0.8024650542961056, 'learning_rate': 0.0008201292019647084, 'batch_size': 150, 'step_size': 11, 'gamma': 0.7855564434289999}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:05:10,335][0m Trial 9 finished with value: 0.4215211808681488 and parameters: {'observation_period_num': 241, 'train_rates': 0.7861724339360865, 'learning_rate': 4.609281497511355e-06, 'batch_size': 58, 'step_size': 6, 'gamma': 0.7823357229295412}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:05:38,023][0m Trial 10 finished with value: 0.11866701632621394 and parameters: {'observation_period_num': 14, 'train_rates': 0.6090642805093716, 'learning_rate': 0.000877358080661204, 'batch_size': 166, 'step_size': 10, 'gamma': 0.7708812759838232}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:09:13,122][0m Trial 11 finished with value: 0.07481186373813732 and parameters: {'observation_period_num': 139, 'train_rates': 0.8686732978233092, 'learning_rate': 0.0008955553477273292, 'batch_size': 24, 'step_size': 2, 'gamma': 0.8007487416329628}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:12:19,290][0m Trial 12 finished with value: 0.06917386156244155 and parameters: {'observation_period_num': 98, 'train_rates': 0.8636810474918515, 'learning_rate': 0.0009952374254963102, 'batch_size': 28, 'step_size': 2, 'gamma': 0.7990102948211446}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:13:03,783][0m Trial 13 finished with value: 0.7233064131859022 and parameters: {'observation_period_num': 82, 'train_rates': 0.8642826512002955, 'learning_rate': 1.0697797873176091e-06, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8115860400195662}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:13:32,556][0m Trial 14 finished with value: 0.1762717127622593 and parameters: {'observation_period_num': 9, 'train_rates': 0.7146269299247728, 'learning_rate': 0.0003376209157277529, 'batch_size': 185, 'step_size': 3, 'gamma': 0.759984137714719}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:14:15,644][0m Trial 15 finished with value: 0.06312287317180051 and parameters: {'observation_period_num': 55, 'train_rates': 0.8445884703848265, 'learning_rate': 0.0009723116137786368, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9087749257042884}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:15:07,423][0m Trial 16 finished with value: 0.06881861388683319 and parameters: {'observation_period_num': 53, 'train_rates': 0.9305702475161487, 'learning_rate': 0.00012547766306312722, 'batch_size': 116, 'step_size': 9, 'gamma': 0.9117416132876258}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:15:45,813][0m Trial 17 finished with value: 0.05708623659282983 and parameters: {'observation_period_num': 52, 'train_rates': 0.8309204939955702, 'learning_rate': 0.0004691595095924604, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9070812968233861}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:16:11,794][0m Trial 18 finished with value: 0.17410703471993433 and parameters: {'observation_period_num': 27, 'train_rates': 0.738895320212004, 'learning_rate': 0.0003663158719025716, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9579746096414755}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:16:45,162][0m Trial 19 finished with value: 0.2016667376829914 and parameters: {'observation_period_num': 77, 'train_rates': 0.6915953238550252, 'learning_rate': 8.781532449767585e-05, 'batch_size': 152, 'step_size': 12, 'gamma': 0.9044027019093026}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:17:08,677][0m Trial 20 finished with value: 0.048203825716878854 and parameters: {'observation_period_num': 35, 'train_rates': 0.8255003241200055, 'learning_rate': 0.00045444077239096935, 'batch_size': 253, 'step_size': 15, 'gamma': 0.8217330203863094}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:17:32,868][0m Trial 21 finished with value: 0.04526715157898488 and parameters: {'observation_period_num': 31, 'train_rates': 0.8151815145755781, 'learning_rate': 0.00048341462397522944, 'batch_size': 251, 'step_size': 15, 'gamma': 0.8289936631747765}. Best is trial 8 with value: 0.043686759852183364.[0m
[32m[I 2025-01-04 10:17:56,588][0m Trial 22 finished with value: 0.043044180398987185 and parameters: {'observation_period_num': 31, 'train_rates': 0.8090495208528791, 'learning_rate': 0.0005127843788542908, 'batch_size': 246, 'step_size': 15, 'gamma': 0.8258266318322817}. Best is trial 22 with value: 0.043044180398987185.[0m
[32m[I 2025-01-04 10:18:21,574][0m Trial 23 finished with value: 0.046837152956530105 and parameters: {'observation_period_num': 33, 'train_rates': 0.9118317002936022, 'learning_rate': 0.00018031245494855633, 'batch_size': 252, 'step_size': 14, 'gamma': 0.7877780898856848}. Best is trial 22 with value: 0.043044180398987185.[0m
[32m[I 2025-01-04 10:18:45,993][0m Trial 24 finished with value: 0.22947969032154877 and parameters: {'observation_period_num': 115, 'train_rates': 0.7543326583895402, 'learning_rate': 0.0005390284283739225, 'batch_size': 220, 'step_size': 14, 'gamma': 0.8365504745962177}. Best is trial 22 with value: 0.043044180398987185.[0m
[32m[I 2025-01-04 10:19:11,766][0m Trial 25 finished with value: 0.1361217931191498 and parameters: {'observation_period_num': 83, 'train_rates': 0.8136096544816416, 'learning_rate': 2.5955144837231016e-05, 'batch_size': 225, 'step_size': 15, 'gamma': 0.7568048328353794}. Best is trial 22 with value: 0.043044180398987185.[0m
[32m[I 2025-01-04 10:19:39,818][0m Trial 26 finished with value: 0.04094029882916411 and parameters: {'observation_period_num': 6, 'train_rates': 0.8011708181139513, 'learning_rate': 0.0002573259742114079, 'batch_size': 200, 'step_size': 13, 'gamma': 0.8172350909534218}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:20:06,803][0m Trial 27 finished with value: 0.14984113968385623 and parameters: {'observation_period_num': 7, 'train_rates': 0.7198298394915451, 'learning_rate': 0.0002222061531690567, 'batch_size': 198, 'step_size': 12, 'gamma': 0.8116061205097677}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:20:36,383][0m Trial 28 finished with value: 0.06678520608693361 and parameters: {'observation_period_num': 66, 'train_rates': 0.8892392430419547, 'learning_rate': 5.646384253161703e-05, 'batch_size': 205, 'step_size': 13, 'gamma': 0.7814495056257106}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:21:00,478][0m Trial 29 finished with value: 0.18677394778320663 and parameters: {'observation_period_num': 21, 'train_rates': 0.7669115603280129, 'learning_rate': 0.00010041855905790545, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8853275215138671}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:21:30,187][0m Trial 30 finished with value: 0.14462649206281977 and parameters: {'observation_period_num': 41, 'train_rates': 0.6453868045845882, 'learning_rate': 0.0006145725292076702, 'batch_size': 168, 'step_size': 14, 'gamma': 0.8451798330339803}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:21:53,730][0m Trial 31 finished with value: 0.04678132180578825 and parameters: {'observation_period_num': 25, 'train_rates': 0.8039414551969671, 'learning_rate': 0.00023459343916766643, 'batch_size': 241, 'step_size': 14, 'gamma': 0.8217566981755658}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:22:21,614][0m Trial 32 finished with value: 0.046025521853672605 and parameters: {'observation_period_num': 40, 'train_rates': 0.8464923835439565, 'learning_rate': 0.0006223255158329428, 'batch_size': 216, 'step_size': 15, 'gamma': 0.8004214612202701}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:22:43,155][0m Trial 33 finished with value: 0.13998552127899552 and parameters: {'observation_period_num': 208, 'train_rates': 0.7964629984794138, 'learning_rate': 0.00028957163148879536, 'batch_size': 256, 'step_size': 13, 'gamma': 0.883269877536215}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:23:13,702][0m Trial 34 finished with value: 0.17168325919323088 and parameters: {'observation_period_num': 5, 'train_rates': 0.7584537501367763, 'learning_rate': 0.00014328683956707258, 'batch_size': 186, 'step_size': 11, 'gamma': 0.8249922779910752}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:23:39,690][0m Trial 35 finished with value: 0.2892809465312708 and parameters: {'observation_period_num': 71, 'train_rates': 0.735297326454017, 'learning_rate': 1.860657768431551e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8507311697117595}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:24:05,276][0m Trial 36 finished with value: 0.04247997493026841 and parameters: {'observation_period_num': 23, 'train_rates': 0.7968325233040511, 'learning_rate': 0.0006443722523698707, 'batch_size': 229, 'step_size': 14, 'gamma': 0.7683372758796084}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:24:28,906][0m Trial 37 finished with value: 0.0630894841446829 and parameters: {'observation_period_num': 63, 'train_rates': 0.7917300986180528, 'learning_rate': 0.0007201187824445607, 'batch_size': 230, 'step_size': 13, 'gamma': 0.770658425343514}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:24:56,669][0m Trial 38 finished with value: 0.24755210907031328 and parameters: {'observation_period_num': 178, 'train_rates': 0.7753797409492882, 'learning_rate': 0.00029544472972308427, 'batch_size': 184, 'step_size': 11, 'gamma': 0.7520574087555536}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:25:30,646][0m Trial 39 finished with value: 0.0690816105402346 and parameters: {'observation_period_num': 96, 'train_rates': 0.8341201929072835, 'learning_rate': 0.00014402860481450394, 'batch_size': 163, 'step_size': 8, 'gamma': 0.7709888542722935}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:26:25,100][0m Trial 40 finished with value: 0.12024589826207642 and parameters: {'observation_period_num': 18, 'train_rates': 0.8886362666857293, 'learning_rate': 7.64828473494968e-06, 'batch_size': 105, 'step_size': 14, 'gamma': 0.7877899932475871}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:26:48,936][0m Trial 41 finished with value: 0.05821731231426937 and parameters: {'observation_period_num': 49, 'train_rates': 0.8076352762298441, 'learning_rate': 0.0004827470837956644, 'batch_size': 241, 'step_size': 15, 'gamma': 0.8088706354884886}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:27:13,326][0m Trial 42 finished with value: 0.1784569924880921 and parameters: {'observation_period_num': 26, 'train_rates': 0.7816781317117494, 'learning_rate': 0.0006741189900178762, 'batch_size': 243, 'step_size': 14, 'gamma': 0.8681229549153348}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:27:38,925][0m Trial 43 finished with value: 0.048605655538617135 and parameters: {'observation_period_num': 42, 'train_rates': 0.8141718601648632, 'learning_rate': 0.00040375277193335806, 'batch_size': 226, 'step_size': 13, 'gamma': 0.8303340530923167}. Best is trial 26 with value: 0.04094029882916411.[0m
[32m[I 2025-01-04 10:28:06,953][0m Trial 44 finished with value: 0.03073573366251834 and parameters: {'observation_period_num': 16, 'train_rates': 0.8517527252431335, 'learning_rate': 0.0007274284965835926, 'batch_size': 210, 'step_size': 15, 'gamma': 0.8386421230237665}. Best is trial 44 with value: 0.03073573366251834.[0m
[32m[I 2025-01-04 10:28:34,585][0m Trial 45 finished with value: 0.029138841619715094 and parameters: {'observation_period_num': 18, 'train_rates': 0.8501033019424664, 'learning_rate': 0.0007503004757548931, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8386884221653349}. Best is trial 45 with value: 0.029138841619715094.[0m
[32m[I 2025-01-04 10:29:05,414][0m Trial 46 finished with value: 0.03128625825047493 and parameters: {'observation_period_num': 17, 'train_rates': 0.94867020174176, 'learning_rate': 0.0007261292008829266, 'batch_size': 212, 'step_size': 10, 'gamma': 0.8534002083626927}. Best is trial 45 with value: 0.029138841619715094.[0m
[32m[I 2025-01-04 10:29:36,124][0m Trial 47 finished with value: 0.03013896569609642 and parameters: {'observation_period_num': 15, 'train_rates': 0.9388904053942291, 'learning_rate': 0.0007321899551950423, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8582969800264182}. Best is trial 45 with value: 0.029138841619715094.[0m
[32m[I 2025-01-04 10:30:06,955][0m Trial 48 finished with value: 0.04158458486199379 and parameters: {'observation_period_num': 5, 'train_rates': 0.958269551218848, 'learning_rate': 0.00027602025191931454, 'batch_size': 207, 'step_size': 9, 'gamma': 0.8564492390395817}. Best is trial 45 with value: 0.029138841619715094.[0m
[32m[I 2025-01-04 10:30:39,695][0m Trial 49 finished with value: 0.027999334037303925 and parameters: {'observation_period_num': 15, 'train_rates': 0.942933330624439, 'learning_rate': 0.0008521571982962857, 'batch_size': 194, 'step_size': 10, 'gamma': 0.8723427194013}. Best is trial 49 with value: 0.027999334037303925.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 10:30:39,705][0m A new study created in memory with name: no-name-dd3a59ab-eff7-428d-aa3f-ce46a2cbac8c[0m
[32m[I 2025-01-04 10:31:34,190][0m Trial 0 finished with value: 0.061310994122034446 and parameters: {'observation_period_num': 94, 'train_rates': 0.8632859663585342, 'learning_rate': 6.0831295573693004e-05, 'batch_size': 99, 'step_size': 14, 'gamma': 0.7942529520789254}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:32:10,972][0m Trial 1 finished with value: 0.2527873836646492 and parameters: {'observation_period_num': 102, 'train_rates': 0.7647548723678379, 'learning_rate': 4.511880197226249e-05, 'batch_size': 145, 'step_size': 13, 'gamma': 0.8612164834184965}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:33:39,275][0m Trial 2 finished with value: 0.07874999895240321 and parameters: {'observation_period_num': 183, 'train_rates': 0.8454179113691638, 'learning_rate': 0.0005746478343178656, 'batch_size': 58, 'step_size': 6, 'gamma': 0.7691039064444933}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:34:04,023][0m Trial 3 finished with value: 0.24217020872245265 and parameters: {'observation_period_num': 210, 'train_rates': 0.6729662887509957, 'learning_rate': 0.0002840393513903597, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8585902870408736}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:34:34,336][0m Trial 4 finished with value: 0.07170894735854519 and parameters: {'observation_period_num': 86, 'train_rates': 0.7890138712886801, 'learning_rate': 0.00014280815452434568, 'batch_size': 180, 'step_size': 9, 'gamma': 0.9083462773846925}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:35:52,406][0m Trial 5 finished with value: 0.07291990164266407 and parameters: {'observation_period_num': 147, 'train_rates': 0.8178948792699868, 'learning_rate': 0.00012932419382113828, 'batch_size': 64, 'step_size': 13, 'gamma': 0.7522997842328095}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:38:29,366][0m Trial 6 finished with value: 0.4275466095317494 and parameters: {'observation_period_num': 210, 'train_rates': 0.9799567354658433, 'learning_rate': 1.2129693710866208e-06, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8804355400270785}. Best is trial 0 with value: 0.061310994122034446.[0m
[32m[I 2025-01-04 10:39:06,705][0m Trial 7 finished with value: 0.04710301292808823 and parameters: {'observation_period_num': 10, 'train_rates': 0.9018167408461718, 'learning_rate': 9.45849196356127e-05, 'batch_size': 158, 'step_size': 3, 'gamma': 0.9343122270165273}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:39:30,497][0m Trial 8 finished with value: 0.3012631914675466 and parameters: {'observation_period_num': 97, 'train_rates': 0.7279756526190493, 'learning_rate': 1.8106837769635798e-05, 'batch_size': 213, 'step_size': 8, 'gamma': 0.929895681311727}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:40:18,384][0m Trial 9 finished with value: 0.9405021256575579 and parameters: {'observation_period_num': 187, 'train_rates': 0.7242018368607726, 'learning_rate': 2.762876528656855e-06, 'batch_size': 99, 'step_size': 2, 'gamma': 0.8824571450318655}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:40:45,191][0m Trial 10 finished with value: 0.08019918203353882 and parameters: {'observation_period_num': 6, 'train_rates': 0.9494805872400972, 'learning_rate': 1.0095968136841493e-05, 'batch_size': 253, 'step_size': 2, 'gamma': 0.9860304347613467}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:41:31,331][0m Trial 11 finished with value: 0.056297813736575414 and parameters: {'observation_period_num': 7, 'train_rates': 0.8910339280140656, 'learning_rate': 4.61199553180482e-05, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8088631432112214}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:42:12,136][0m Trial 12 finished with value: 0.2001497611619424 and parameters: {'observation_period_num': 7, 'train_rates': 0.9065161284847348, 'learning_rate': 7.487420254086724e-06, 'batch_size': 142, 'step_size': 4, 'gamma': 0.8170084728700788}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:43:05,151][0m Trial 13 finished with value: 0.05695130825042725 and parameters: {'observation_period_num': 46, 'train_rates': 0.9141203015683257, 'learning_rate': 7.341229109609697e-05, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9639217135660204}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:43:33,036][0m Trial 14 finished with value: 0.3548772163974699 and parameters: {'observation_period_num': 40, 'train_rates': 0.6060470161936802, 'learning_rate': 2.4361614563205074e-05, 'batch_size': 163, 'step_size': 5, 'gamma': 0.8192990029879337}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:43:57,775][0m Trial 15 finished with value: 0.062191126804299814 and parameters: {'observation_period_num': 51, 'train_rates': 0.8893916545358297, 'learning_rate': 0.0006641999767190333, 'batch_size': 236, 'step_size': 1, 'gamma': 0.9391372490126331}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:44:44,835][0m Trial 16 finished with value: 0.05806218871944829 and parameters: {'observation_period_num': 29, 'train_rates': 0.9482678571050704, 'learning_rate': 0.00017836857045205255, 'batch_size': 128, 'step_size': 3, 'gamma': 0.8362192761650742}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:45:18,298][0m Trial 17 finished with value: 0.11646878046830873 and parameters: {'observation_period_num': 67, 'train_rates': 0.8541950889245224, 'learning_rate': 1.107237119381141e-05, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9147566504155938}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:46:31,359][0m Trial 18 finished with value: 0.10029938071966171 and parameters: {'observation_period_num': 147, 'train_rates': 0.9891940625753235, 'learning_rate': 0.0003581855434194243, 'batch_size': 80, 'step_size': 6, 'gamma': 0.7870238557941869}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:47:04,384][0m Trial 19 finished with value: 0.07637170612812043 and parameters: {'observation_period_num': 21, 'train_rates': 0.931899803248374, 'learning_rate': 3.28453903302085e-05, 'batch_size': 192, 'step_size': 1, 'gamma': 0.9607500600441319}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:47:46,181][0m Trial 20 finished with value: 0.37924801637916444 and parameters: {'observation_period_num': 252, 'train_rates': 0.88247536304342, 'learning_rate': 4.6399483477704065e-06, 'batch_size': 127, 'step_size': 4, 'gamma': 0.8960262497294198}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:48:37,928][0m Trial 21 finished with value: 0.05339289053871825 and parameters: {'observation_period_num': 56, 'train_rates': 0.9107473009299325, 'learning_rate': 8.294352048078995e-05, 'batch_size': 110, 'step_size': 4, 'gamma': 0.9886398828445907}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:49:13,913][0m Trial 22 finished with value: 0.057732331634833545 and parameters: {'observation_period_num': 61, 'train_rates': 0.8210135613494457, 'learning_rate': 9.634517600576183e-05, 'batch_size': 155, 'step_size': 5, 'gamma': 0.9858118189451711}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:50:07,134][0m Trial 23 finished with value: 0.058243966296963066 and parameters: {'observation_period_num': 23, 'train_rates': 0.9529529678865222, 'learning_rate': 4.303845301953524e-05, 'batch_size': 114, 'step_size': 3, 'gamma': 0.95575077348673}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:51:13,052][0m Trial 24 finished with value: 0.05237926448722487 and parameters: {'observation_period_num': 72, 'train_rates': 0.8928385961473575, 'learning_rate': 0.00023383653615849544, 'batch_size': 85, 'step_size': 6, 'gamma': 0.9381712504249582}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:52:19,138][0m Trial 25 finished with value: 0.06381233943941152 and parameters: {'observation_period_num': 72, 'train_rates': 0.8298010668407504, 'learning_rate': 0.00025720305188019026, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9433059581230173}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:53:29,407][0m Trial 26 finished with value: 0.1458932755965192 and parameters: {'observation_period_num': 122, 'train_rates': 0.9256705777645392, 'learning_rate': 0.0008340670312328588, 'batch_size': 80, 'step_size': 3, 'gamma': 0.97071087669145}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:55:44,508][0m Trial 27 finished with value: 0.13693643052186538 and parameters: {'observation_period_num': 122, 'train_rates': 0.8816929378978718, 'learning_rate': 0.000420948155398382, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9201865721297839}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 10:57:32,398][0m Trial 28 finished with value: 0.1169557748362422 and parameters: {'observation_period_num': 80, 'train_rates': 0.9668387788554946, 'learning_rate': 0.00019155099343907928, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9893186419441282}. Best is trial 7 with value: 0.04710301292808823.[0m
[32m[I 2025-01-04 11:01:46,714][0m Trial 29 finished with value: 0.04501473789291524 and parameters: {'observation_period_num': 36, 'train_rates': 0.8677390607877187, 'learning_rate': 7.50310055425977e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.9447580579997868}. Best is trial 29 with value: 0.04501473789291524.[0m
[32m[I 2025-01-04 11:03:38,205][0m Trial 30 finished with value: 0.041560233358228414 and parameters: {'observation_period_num': 35, 'train_rates': 0.8634398353313626, 'learning_rate': 9.238854246110395e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8971775584822537}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:08:16,531][0m Trial 31 finished with value: 0.14282163516396568 and parameters: {'observation_period_num': 35, 'train_rates': 0.8634972734970195, 'learning_rate': 9.483591905105825e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8994408762460129}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:13:28,361][0m Trial 32 finished with value: 0.1035614266540065 and parameters: {'observation_period_num': 26, 'train_rates': 0.7971644783135413, 'learning_rate': 6.0687640910459216e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9424522949561778}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:16:09,628][0m Trial 33 finished with value: 0.06928593184995024 and parameters: {'observation_period_num': 49, 'train_rates': 0.8562871852500353, 'learning_rate': 0.0001213317695330243, 'batch_size': 33, 'step_size': 13, 'gamma': 0.9301186971969784}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:17:47,587][0m Trial 34 finished with value: 0.07499594872582693 and parameters: {'observation_period_num': 108, 'train_rates': 0.8689012715931209, 'learning_rate': 0.00019123235459933663, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8592450719604902}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:19:03,113][0m Trial 35 finished with value: 0.17301689698340103 and parameters: {'observation_period_num': 18, 'train_rates': 0.7706568867867432, 'learning_rate': 5.296681883073412e-05, 'batch_size': 68, 'step_size': 10, 'gamma': 0.89479532139845}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:21:05,153][0m Trial 36 finished with value: 0.04816909864912043 and parameters: {'observation_period_num': 38, 'train_rates': 0.833088861846717, 'learning_rate': 2.5378991635143035e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9246909146895641}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:23:06,841][0m Trial 37 finished with value: 0.04723712787816399 and parameters: {'observation_period_num': 37, 'train_rates': 0.837105441437073, 'learning_rate': 2.550929693485962e-05, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8798492433453947}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:26:25,029][0m Trial 38 finished with value: 0.07252693774863186 and parameters: {'observation_period_num': 88, 'train_rates': 0.7958731068091139, 'learning_rate': 1.6450179464681844e-05, 'batch_size': 25, 'step_size': 15, 'gamma': 0.8714769833511018}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:28:05,254][0m Trial 39 finished with value: 0.2334497363331019 and parameters: {'observation_period_num': 139, 'train_rates': 0.7563379225898268, 'learning_rate': 3.9473620117935286e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.881178123707714}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:29:25,065][0m Trial 40 finished with value: 0.08492874858380707 and parameters: {'observation_period_num': 162, 'train_rates': 0.8408770558702983, 'learning_rate': 0.00012574074592957592, 'batch_size': 65, 'step_size': 14, 'gamma': 0.8509522211708755}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:31:30,624][0m Trial 41 finished with value: 0.045097358977875195 and parameters: {'observation_period_num': 32, 'train_rates': 0.8228061293333001, 'learning_rate': 2.4990203823861804e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9068213834054151}. Best is trial 30 with value: 0.041560233358228414.[0m
[32m[I 2025-01-04 11:34:29,090][0m Trial 42 finished with value: 0.03995953011890142 and parameters: {'observation_period_num': 17, 'train_rates': 0.8173325897575369, 'learning_rate': 1.9028051396510073e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9075947186261523}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:37:21,110][0m Trial 43 finished with value: 0.04141544536786511 and parameters: {'observation_period_num': 13, 'train_rates': 0.8084235481498807, 'learning_rate': 1.6895943678828368e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.9030144072131253}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:40:32,102][0m Trial 44 finished with value: 0.045970777059802126 and parameters: {'observation_period_num': 19, 'train_rates': 0.8122062394531845, 'learning_rate': 1.6203924128371243e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.9086169846967396}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:43:25,911][0m Trial 45 finished with value: 0.20198985507633693 and parameters: {'observation_period_num': 14, 'train_rates': 0.7726794734789728, 'learning_rate': 6.175635551816645e-06, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9023037320727575}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:44:53,236][0m Trial 46 finished with value: 0.22324952592017008 and parameters: {'observation_period_num': 56, 'train_rates': 0.740108011945093, 'learning_rate': 1.157985897136141e-05, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8896011344228351}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:49:35,665][0m Trial 47 finished with value: 0.20432247712255686 and parameters: {'observation_period_num': 31, 'train_rates': 0.6841348149128932, 'learning_rate': 2.2424828453169267e-06, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9171040710077197}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:51:58,662][0m Trial 48 finished with value: 0.054555744909191475 and parameters: {'observation_period_num': 45, 'train_rates': 0.8084314806076416, 'learning_rate': 1.8997449821315234e-05, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9508701121701514}. Best is trial 42 with value: 0.03995953011890142.[0m
[32m[I 2025-01-04 11:53:16,078][0m Trial 49 finished with value: 0.06746972214070059 and parameters: {'observation_period_num': 107, 'train_rates': 0.8694919382289614, 'learning_rate': 3.355432079122958e-05, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9088204434710617}. Best is trial 42 with value: 0.03995953011890142.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 11:53:16,089][0m A new study created in memory with name: no-name-065e3b00-a8d5-40e4-8365-5858389c45cf[0m
[32m[I 2025-01-04 11:53:47,182][0m Trial 0 finished with value: 0.08498736560344695 and parameters: {'observation_period_num': 196, 'train_rates': 0.9274896924076186, 'learning_rate': 0.000393286965932855, 'batch_size': 192, 'step_size': 4, 'gamma': 0.8322080577741731}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:55:04,037][0m Trial 1 finished with value: 0.10151736289793062 and parameters: {'observation_period_num': 96, 'train_rates': 0.803548716494477, 'learning_rate': 1.3163454086358568e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9798673926943933}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:55:48,232][0m Trial 2 finished with value: 0.15823686637376483 and parameters: {'observation_period_num': 47, 'train_rates': 0.8366440198354635, 'learning_rate': 4.681877751940638e-06, 'batch_size': 127, 'step_size': 15, 'gamma': 0.9500397509239922}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:56:54,966][0m Trial 3 finished with value: 0.14740208603402294 and parameters: {'observation_period_num': 123, 'train_rates': 0.9335193381333464, 'learning_rate': 1.0689437835104075e-05, 'batch_size': 86, 'step_size': 13, 'gamma': 0.821317760088819}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:57:15,957][0m Trial 4 finished with value: 0.5306470280908062 and parameters: {'observation_period_num': 165, 'train_rates': 0.6178189170224874, 'learning_rate': 1.1713724199931705e-05, 'batch_size': 237, 'step_size': 3, 'gamma': 0.9426404720346352}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:57:45,002][0m Trial 5 finished with value: 0.12697622168906117 and parameters: {'observation_period_num': 224, 'train_rates': 0.9015027429271474, 'learning_rate': 0.0001221171015308608, 'batch_size': 204, 'step_size': 9, 'gamma': 0.9323449172772347}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:59:00,696][0m Trial 6 finished with value: 0.22919483234195198 and parameters: {'observation_period_num': 132, 'train_rates': 0.6964124356545827, 'learning_rate': 0.00012525137050940248, 'batch_size': 60, 'step_size': 15, 'gamma': 0.9354694730831206}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:59:28,234][0m Trial 7 finished with value: 0.39388909935951233 and parameters: {'observation_period_num': 211, 'train_rates': 0.9523837101920323, 'learning_rate': 4.087744534077595e-06, 'batch_size': 218, 'step_size': 7, 'gamma': 0.9703863395192096}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 11:59:53,251][0m Trial 8 finished with value: 0.13846700880034216 and parameters: {'observation_period_num': 159, 'train_rates': 0.8444361446285529, 'learning_rate': 5.858322579405593e-05, 'batch_size': 226, 'step_size': 15, 'gamma': 0.9154431522721693}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 12:00:55,130][0m Trial 9 finished with value: 0.11256035080786501 and parameters: {'observation_period_num': 233, 'train_rates': 0.8363836196259372, 'learning_rate': 7.596978637469106e-05, 'batch_size': 84, 'step_size': 2, 'gamma': 0.9804874562017118}. Best is trial 0 with value: 0.08498736560344695.[0m
[32m[I 2025-01-04 12:01:34,224][0m Trial 10 finished with value: 0.03880533576011658 and parameters: {'observation_period_num': 10, 'train_rates': 0.9825649758818044, 'learning_rate': 0.0007995204241563428, 'batch_size': 168, 'step_size': 5, 'gamma': 0.7506551886329981}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:02:13,114][0m Trial 11 finished with value: 0.04470521956682205 and parameters: {'observation_period_num': 6, 'train_rates': 0.9843611900174916, 'learning_rate': 0.0009445020002604015, 'batch_size': 164, 'step_size': 5, 'gamma': 0.7555611287224053}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:02:55,125][0m Trial 12 finished with value: 0.046792808920145035 and parameters: {'observation_period_num': 10, 'train_rates': 0.9893051816454619, 'learning_rate': 0.000728715937307631, 'batch_size': 160, 'step_size': 6, 'gamma': 0.7521583245550464}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:03:30,421][0m Trial 13 finished with value: 0.1403659837366628 and parameters: {'observation_period_num': 6, 'train_rates': 0.7127764933849516, 'learning_rate': 0.0009220647343104764, 'batch_size': 146, 'step_size': 9, 'gamma': 0.7524132340269319}. Best is trial 10 with value: 0.03880533576011658.[0m
Early stopping at epoch 58
[32m[I 2025-01-04 12:03:51,077][0m Trial 14 finished with value: 0.14109963610101103 and parameters: {'observation_period_num': 55, 'train_rates': 0.8843936715749865, 'learning_rate': 0.00030172162146620296, 'batch_size': 176, 'step_size': 1, 'gamma': 0.7938659684512975}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:08:42,842][0m Trial 15 finished with value: 0.30702730019887287 and parameters: {'observation_period_num': 54, 'train_rates': 0.9857063802055158, 'learning_rate': 1.4920841543087083e-06, 'batch_size': 20, 'step_size': 5, 'gamma': 0.7853990042964093}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:09:04,614][0m Trial 16 finished with value: 0.1736065502300054 and parameters: {'observation_period_num': 34, 'train_rates': 0.7336628263619831, 'learning_rate': 0.00030670337678483176, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8769806638373808}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:09:50,393][0m Trial 17 finished with value: 0.05662068783156461 and parameters: {'observation_period_num': 83, 'train_rates': 0.8848639588817023, 'learning_rate': 0.0004925480717760473, 'batch_size': 121, 'step_size': 5, 'gamma': 0.7792705300633772}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:10:20,139][0m Trial 18 finished with value: 0.22196554226646256 and parameters: {'observation_period_num': 88, 'train_rates': 0.7604307287441155, 'learning_rate': 0.00019853817779797697, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8485865907742551}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:11:19,480][0m Trial 19 finished with value: 0.05815493508422975 and parameters: {'observation_period_num': 27, 'train_rates': 0.9527496586579245, 'learning_rate': 3.1565836671061074e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.883321967968343}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:11:49,653][0m Trial 20 finished with value: 0.27711788798770765 and parameters: {'observation_period_num': 252, 'train_rates': 0.6451659364035875, 'learning_rate': 0.0009001541456402213, 'batch_size': 147, 'step_size': 3, 'gamma': 0.8071412027046867}. Best is trial 10 with value: 0.03880533576011658.[0m
[32m[I 2025-01-04 12:12:28,997][0m Trial 21 finished with value: 0.03874798119068146 and parameters: {'observation_period_num': 11, 'train_rates': 0.9742950421899974, 'learning_rate': 0.0006927054035347757, 'batch_size': 164, 'step_size': 6, 'gamma': 0.7541427770910203}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:13:03,962][0m Trial 22 finished with value: 0.057316023856401443 and parameters: {'observation_period_num': 22, 'train_rates': 0.9885032791663884, 'learning_rate': 0.00047061781311567115, 'batch_size': 187, 'step_size': 5, 'gamma': 0.7620874116789303}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:13:41,077][0m Trial 23 finished with value: 0.0487575298867935 and parameters: {'observation_period_num': 65, 'train_rates': 0.9164103100424407, 'learning_rate': 0.0009925470234621076, 'batch_size': 160, 'step_size': 6, 'gamma': 0.773730324330071}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:14:31,862][0m Trial 24 finished with value: 0.062156252562999725 and parameters: {'observation_period_num': 6, 'train_rates': 0.9599147328380045, 'learning_rate': 0.00019869272079082236, 'batch_size': 120, 'step_size': 4, 'gamma': 0.7976988150513765}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:15:07,887][0m Trial 25 finished with value: 0.04178605561560773 and parameters: {'observation_period_num': 37, 'train_rates': 0.8711494626307991, 'learning_rate': 0.0004877039025428837, 'batch_size': 166, 'step_size': 7, 'gamma': 0.769186268634468}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:15:37,646][0m Trial 26 finished with value: 0.06018564262228205 and parameters: {'observation_period_num': 71, 'train_rates': 0.8799487358171788, 'learning_rate': 0.00020781788436226208, 'batch_size': 203, 'step_size': 10, 'gamma': 0.7730414611156635}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:16:17,621][0m Trial 27 finished with value: 0.0399225870360486 and parameters: {'observation_period_num': 39, 'train_rates': 0.8620159639442458, 'learning_rate': 0.0005136537037938759, 'batch_size': 143, 'step_size': 7, 'gamma': 0.8202286174005875}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:16:56,882][0m Trial 28 finished with value: 0.1313130141043466 and parameters: {'observation_period_num': 113, 'train_rates': 0.7870586218289088, 'learning_rate': 3.662637715369132e-05, 'batch_size': 139, 'step_size': 11, 'gamma': 0.8497306540364069}. Best is trial 21 with value: 0.03874798119068146.[0m
[32m[I 2025-01-04 12:17:29,877][0m Trial 29 finished with value: 0.03665935712927413 and parameters: {'observation_period_num': 24, 'train_rates': 0.927249435423794, 'learning_rate': 0.0005301922637017727, 'batch_size': 191, 'step_size': 8, 'gamma': 0.8200934533183782}. Best is trial 29 with value: 0.03665935712927413.[0m
[32m[I 2025-01-04 12:18:02,817][0m Trial 30 finished with value: 0.04203155577600559 and parameters: {'observation_period_num': 26, 'train_rates': 0.9271687575975798, 'learning_rate': 0.00030130828492869004, 'batch_size': 191, 'step_size': 8, 'gamma': 0.8424441577044189}. Best is trial 29 with value: 0.03665935712927413.[0m
[32m[I 2025-01-04 12:18:33,731][0m Trial 31 finished with value: 0.05991683527827263 and parameters: {'observation_period_num': 43, 'train_rates': 0.944174897913583, 'learning_rate': 0.0005290925017385216, 'batch_size': 200, 'step_size': 6, 'gamma': 0.8181128040151536}. Best is trial 29 with value: 0.03665935712927413.[0m
[32m[I 2025-01-04 12:19:06,646][0m Trial 32 finished with value: 0.03214202412026744 and parameters: {'observation_period_num': 21, 'train_rates': 0.9082756192020903, 'learning_rate': 0.0006038465728526143, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8928506173871197}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:19:39,085][0m Trial 33 finished with value: 0.03494019340382609 and parameters: {'observation_period_num': 21, 'train_rates': 0.906993176768368, 'learning_rate': 0.00032569583613315065, 'batch_size': 181, 'step_size': 8, 'gamma': 0.8928393900197329}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:20:08,129][0m Trial 34 finished with value: 0.06377735733985901 and parameters: {'observation_period_num': 72, 'train_rates': 0.9255707189090459, 'learning_rate': 0.0001269931582821234, 'batch_size': 217, 'step_size': 12, 'gamma': 0.9067934268958293}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:20:42,326][0m Trial 35 finished with value: 0.038411898284160785 and parameters: {'observation_period_num': 23, 'train_rates': 0.9076964513038249, 'learning_rate': 0.0003307208415349145, 'batch_size': 186, 'step_size': 9, 'gamma': 0.9076713157011498}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:21:05,862][0m Trial 36 finished with value: 0.06471809477585813 and parameters: {'observation_period_num': 56, 'train_rates': 0.8095579408699247, 'learning_rate': 0.0003322280689637128, 'batch_size': 246, 'step_size': 9, 'gamma': 0.9009884762525754}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:21:36,745][0m Trial 37 finished with value: 0.07989863975430435 and parameters: {'observation_period_num': 102, 'train_rates': 0.9070876368798027, 'learning_rate': 8.936871554387749e-05, 'batch_size': 188, 'step_size': 10, 'gamma': 0.8628827404470757}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:22:03,496][0m Trial 38 finished with value: 0.08672853741500018 and parameters: {'observation_period_num': 180, 'train_rates': 0.8997504397541367, 'learning_rate': 0.00020402592687850346, 'batch_size': 215, 'step_size': 8, 'gamma': 0.8914114793312748}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:22:29,079][0m Trial 39 finished with value: 0.07846953660661486 and parameters: {'observation_period_num': 21, 'train_rates': 0.8024955334754041, 'learning_rate': 2.2399206228923194e-05, 'batch_size': 232, 'step_size': 11, 'gamma': 0.9274405100069981}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:23:00,559][0m Trial 40 finished with value: 0.08382168489283529 and parameters: {'observation_period_num': 137, 'train_rates': 0.8455865265395666, 'learning_rate': 0.00015333961411579755, 'batch_size': 183, 'step_size': 9, 'gamma': 0.956261233457012}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:23:31,222][0m Trial 41 finished with value: 0.03825163468718529 and parameters: {'observation_period_num': 20, 'train_rates': 0.9638413111873763, 'learning_rate': 0.0006399642777000345, 'batch_size': 204, 'step_size': 8, 'gamma': 0.8639981526835944}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:24:02,239][0m Trial 42 finished with value: 0.050830259919166565 and parameters: {'observation_period_num': 47, 'train_rates': 0.9395466129512613, 'learning_rate': 0.0003852299509285436, 'batch_size': 200, 'step_size': 8, 'gamma': 0.869346591772232}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:24:31,164][0m Trial 43 finished with value: 0.16699551615495323 and parameters: {'observation_period_num': 22, 'train_rates': 0.9002656106815825, 'learning_rate': 7.0996100116829395e-06, 'batch_size': 213, 'step_size': 11, 'gamma': 0.9199662061073846}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:25:00,048][0m Trial 44 finished with value: 0.05081966891884804 and parameters: {'observation_period_num': 34, 'train_rates': 0.9683569093247437, 'learning_rate': 0.0006666888157487175, 'batch_size': 227, 'step_size': 8, 'gamma': 0.9033931309184203}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:25:35,815][0m Trial 45 finished with value: 0.07108500486737146 and parameters: {'observation_period_num': 62, 'train_rates': 0.9339651183437708, 'learning_rate': 8.172630978749818e-05, 'batch_size': 179, 'step_size': 13, 'gamma': 0.8610904152178355}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:26:03,488][0m Trial 46 finished with value: 0.038334025302901864 and parameters: {'observation_period_num': 19, 'train_rates': 0.8254603081406842, 'learning_rate': 0.00026314613148858946, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8907786798487805}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:26:28,323][0m Trial 47 finished with value: 0.059383296039773194 and parameters: {'observation_period_num': 47, 'train_rates': 0.8590953714834253, 'learning_rate': 0.00025130865608660215, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8915416675670171}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:26:55,950][0m Trial 48 finished with value: 0.03822258054819446 and parameters: {'observation_period_num': 15, 'train_rates': 0.8183005624620303, 'learning_rate': 0.0006154983850603769, 'batch_size': 205, 'step_size': 7, 'gamma': 0.831207554003264}. Best is trial 32 with value: 0.03214202412026744.[0m
[32m[I 2025-01-04 12:27:35,096][0m Trial 49 finished with value: 0.09023463726043701 and parameters: {'observation_period_num': 149, 'train_rates': 0.9601619272044606, 'learning_rate': 0.0006541053911134601, 'batch_size': 155, 'step_size': 7, 'gamma': 0.8299655537402302}. Best is trial 32 with value: 0.03214202412026744.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.9459639057544847, 'learning_rate': 0.0005448001859417389, 'batch_size': 217, 'step_size': 13, 'gamma': 0.9801770269150581}
Epoch 1/300, trend Loss: 0.5132 | 0.2519
Epoch 2/300, trend Loss: 0.2509 | 0.2037
Epoch 3/300, trend Loss: 0.2290 | 0.2074
Epoch 4/300, trend Loss: 0.2980 | 0.6150
Epoch 5/300, trend Loss: 0.2249 | 0.1776
Epoch 6/300, trend Loss: 0.1785 | 0.1740
Epoch 7/300, trend Loss: 0.1492 | 0.1491
Epoch 8/300, trend Loss: 0.1462 | 0.1574
Epoch 9/300, trend Loss: 0.1329 | 0.1298
Epoch 10/300, trend Loss: 0.1623 | 0.1020
Epoch 11/300, trend Loss: 0.1655 | 0.1126
Epoch 12/300, trend Loss: 0.2002 | 0.3261
Epoch 13/300, trend Loss: 0.1901 | 0.1185
Epoch 14/300, trend Loss: 0.1309 | 0.0931
Epoch 15/300, trend Loss: 0.1543 | 0.0827
Epoch 16/300, trend Loss: 0.1177 | 0.1060
Epoch 17/300, trend Loss: 0.1319 | 0.0740
Epoch 18/300, trend Loss: 0.1185 | 0.0742
Epoch 19/300, trend Loss: 0.1185 | 0.0666
Epoch 20/300, trend Loss: 0.1106 | 0.0741
Epoch 21/300, trend Loss: 0.1145 | 0.0660
Epoch 22/300, trend Loss: 0.1058 | 0.0614
Epoch 23/300, trend Loss: 0.0998 | 0.0605
Epoch 24/300, trend Loss: 0.1008 | 0.0626
Epoch 25/300, trend Loss: 0.1175 | 0.1627
Epoch 26/300, trend Loss: 0.1206 | 0.1533
Epoch 27/300, trend Loss: 0.1226 | 0.0716
Epoch 28/300, trend Loss: 0.1086 | 0.0649
Epoch 29/300, trend Loss: 0.1029 | 0.0656
Epoch 30/300, trend Loss: 0.0981 | 0.0828
Epoch 31/300, trend Loss: 0.0962 | 0.0586
Epoch 32/300, trend Loss: 0.0950 | 0.0572
Epoch 33/300, trend Loss: 0.0943 | 0.0630
Epoch 34/300, trend Loss: 0.0935 | 0.0569
Epoch 35/300, trend Loss: 0.0898 | 0.0758
Epoch 36/300, trend Loss: 0.0880 | 0.0680
Epoch 37/300, trend Loss: 0.0884 | 0.0552
Epoch 38/300, trend Loss: 0.0857 | 0.0508
Epoch 39/300, trend Loss: 0.0830 | 0.0472
Epoch 40/300, trend Loss: 0.0842 | 0.0484
Epoch 41/300, trend Loss: 0.0819 | 0.0460
Epoch 42/300, trend Loss: 0.0891 | 0.0817
Epoch 43/300, trend Loss: 0.0941 | 0.1005
Epoch 44/300, trend Loss: 0.0994 | 0.0704
Epoch 45/300, trend Loss: 0.0936 | 0.0512
Epoch 46/300, trend Loss: 0.0933 | 0.0672
Epoch 47/300, trend Loss: 0.0999 | 0.0618
Epoch 48/300, trend Loss: 0.0864 | 0.0624
Epoch 49/300, trend Loss: 0.0853 | 0.0516
Epoch 50/300, trend Loss: 0.0860 | 0.0521
Epoch 51/300, trend Loss: 0.0831 | 0.0561
Epoch 52/300, trend Loss: 0.0905 | 0.0905
Epoch 53/300, trend Loss: 0.0854 | 0.0469
Epoch 54/300, trend Loss: 0.0797 | 0.0474
Epoch 55/300, trend Loss: 0.0769 | 0.0427
Epoch 56/300, trend Loss: 0.0811 | 0.0792
Epoch 57/300, trend Loss: 0.0840 | 0.0542
Epoch 58/300, trend Loss: 0.0825 | 0.0423
Epoch 59/300, trend Loss: 0.0767 | 0.0436
Epoch 60/300, trend Loss: 0.0759 | 0.0405
Epoch 61/300, trend Loss: 0.0743 | 0.0385
Epoch 62/300, trend Loss: 0.0758 | 0.0568
Epoch 63/300, trend Loss: 0.0813 | 0.0562
Epoch 64/300, trend Loss: 0.0827 | 0.0450
Epoch 65/300, trend Loss: 0.0851 | 0.0435
Epoch 66/300, trend Loss: 0.0849 | 0.0643
Epoch 67/300, trend Loss: 0.0824 | 0.0599
Epoch 68/300, trend Loss: 0.0803 | 0.0431
Epoch 69/300, trend Loss: 0.0730 | 0.0399
Epoch 70/300, trend Loss: 0.0731 | 0.0375
Epoch 71/300, trend Loss: 0.0716 | 0.0456
Epoch 72/300, trend Loss: 0.0740 | 0.0693
Epoch 73/300, trend Loss: 0.0724 | 0.0370
Epoch 74/300, trend Loss: 0.0686 | 0.0388
Epoch 75/300, trend Loss: 0.0685 | 0.0400
Epoch 76/300, trend Loss: 0.0671 | 0.0333
Epoch 77/300, trend Loss: 0.0679 | 0.0354
Epoch 78/300, trend Loss: 0.0680 | 0.0498
Epoch 79/300, trend Loss: 0.0695 | 0.0371
Epoch 80/300, trend Loss: 0.0696 | 0.0335
Epoch 81/300, trend Loss: 0.0719 | 0.0378
Epoch 82/300, trend Loss: 0.0676 | 0.0411
Epoch 83/300, trend Loss: 0.0662 | 0.0350
Epoch 84/300, trend Loss: 0.0651 | 0.0339
Epoch 85/300, trend Loss: 0.0653 | 0.0348
Epoch 86/300, trend Loss: 0.0640 | 0.0344
Epoch 87/300, trend Loss: 0.0651 | 0.0345
Epoch 88/300, trend Loss: 0.0638 | 0.0380
Epoch 89/300, trend Loss: 0.0639 | 0.0352
Epoch 90/300, trend Loss: 0.0632 | 0.0314
Epoch 91/300, trend Loss: 0.0630 | 0.0341
Epoch 92/300, trend Loss: 0.0656 | 0.0382
Epoch 93/300, trend Loss: 0.0676 | 0.0383
Epoch 94/300, trend Loss: 0.0702 | 0.0345
Epoch 95/300, trend Loss: 0.0738 | 0.0572
Epoch 96/300, trend Loss: 0.0707 | 0.0742
Epoch 97/300, trend Loss: 0.0696 | 0.0331
Epoch 98/300, trend Loss: 0.0650 | 0.0376
Epoch 99/300, trend Loss: 0.0657 | 0.0364
Epoch 100/300, trend Loss: 0.0655 | 0.0409
Epoch 101/300, trend Loss: 0.0628 | 0.0353
Epoch 102/300, trend Loss: 0.0666 | 0.0352
Epoch 103/300, trend Loss: 0.0645 | 0.0341
Epoch 104/300, trend Loss: 0.0674 | 0.0369
Epoch 105/300, trend Loss: 0.0640 | 0.0397
Epoch 106/300, trend Loss: 0.0649 | 0.0351
Epoch 107/300, trend Loss: 0.0614 | 0.0323
Epoch 108/300, trend Loss: 0.0618 | 0.0391
Epoch 109/300, trend Loss: 0.0633 | 0.0513
Epoch 110/300, trend Loss: 0.0622 | 0.0306
Epoch 111/300, trend Loss: 0.0607 | 0.0340
Epoch 112/300, trend Loss: 0.0598 | 0.0361
Epoch 113/300, trend Loss: 0.0605 | 0.0325
Epoch 114/300, trend Loss: 0.0596 | 0.0331
Epoch 115/300, trend Loss: 0.0614 | 0.0330
Epoch 116/300, trend Loss: 0.0597 | 0.0322
Epoch 117/300, trend Loss: 0.0627 | 0.0332
Epoch 118/300, trend Loss: 0.0598 | 0.0448
Epoch 119/300, trend Loss: 0.0626 | 0.0329
Epoch 120/300, trend Loss: 0.0618 | 0.0369
Epoch 121/300, trend Loss: 0.0593 | 0.0319
Epoch 122/300, trend Loss: 0.0613 | 0.0469
Epoch 123/300, trend Loss: 0.0627 | 0.0461
Epoch 124/300, trend Loss: 0.0622 | 0.0343
Epoch 125/300, trend Loss: 0.0604 | 0.0358
Epoch 126/300, trend Loss: 0.0583 | 0.0294
Epoch 127/300, trend Loss: 0.0617 | 0.0480
Epoch 128/300, trend Loss: 0.0608 | 0.0489
Epoch 129/300, trend Loss: 0.0648 | 0.0369
Epoch 130/300, trend Loss: 0.0643 | 0.0392
Epoch 131/300, trend Loss: 0.0687 | 0.0486
Epoch 132/300, trend Loss: 0.0688 | 0.0596
Epoch 133/300, trend Loss: 0.0682 | 0.0371
Epoch 134/300, trend Loss: 0.0678 | 0.0502
Epoch 135/300, trend Loss: 0.0697 | 0.0578
Epoch 136/300, trend Loss: 0.0685 | 0.0592
Epoch 137/300, trend Loss: 0.0731 | 0.0394
Epoch 138/300, trend Loss: 0.0662 | 0.0392
Epoch 139/300, trend Loss: 0.0735 | 0.0874
Epoch 140/300, trend Loss: 0.0697 | 0.0370
Epoch 141/300, trend Loss: 0.0660 | 0.0320
Epoch 142/300, trend Loss: 0.0601 | 0.0364
Epoch 143/300, trend Loss: 0.0583 | 0.0492
Epoch 144/300, trend Loss: 0.0582 | 0.0313
Epoch 145/300, trend Loss: 0.0558 | 0.0304
Epoch 146/300, trend Loss: 0.0547 | 0.0303
Epoch 147/300, trend Loss: 0.0552 | 0.0349
Epoch 148/300, trend Loss: 0.0535 | 0.0324
Epoch 149/300, trend Loss: 0.0546 | 0.0295
Epoch 150/300, trend Loss: 0.0543 | 0.0317
Epoch 151/300, trend Loss: 0.0526 | 0.0306
Epoch 152/300, trend Loss: 0.0530 | 0.0299
Epoch 153/300, trend Loss: 0.0527 | 0.0299
Epoch 154/300, trend Loss: 0.0538 | 0.0335
Epoch 155/300, trend Loss: 0.0552 | 0.0343
Epoch 156/300, trend Loss: 0.0528 | 0.0293
Epoch 157/300, trend Loss: 0.0537 | 0.0294
Epoch 158/300, trend Loss: 0.0524 | 0.0336
Epoch 159/300, trend Loss: 0.0558 | 0.0297
Epoch 160/300, trend Loss: 0.0546 | 0.0301
Epoch 161/300, trend Loss: 0.0556 | 0.0297
Epoch 162/300, trend Loss: 0.0534 | 0.0291
Epoch 163/300, trend Loss: 0.0540 | 0.0368
Epoch 164/300, trend Loss: 0.0538 | 0.0368
Epoch 165/300, trend Loss: 0.0549 | 0.0322
Epoch 166/300, trend Loss: 0.0532 | 0.0297
Epoch 167/300, trend Loss: 0.0547 | 0.0404
Epoch 168/300, trend Loss: 0.0576 | 0.0335
Epoch 169/300, trend Loss: 0.0570 | 0.0351
Epoch 170/300, trend Loss: 0.0537 | 0.0289
Epoch 171/300, trend Loss: 0.0541 | 0.0461
Epoch 172/300, trend Loss: 0.0546 | 0.0380
Epoch 173/300, trend Loss: 0.0540 | 0.0311
Epoch 174/300, trend Loss: 0.0518 | 0.0339
Epoch 175/300, trend Loss: 0.0584 | 0.0358
Epoch 176/300, trend Loss: 0.0560 | 0.0346
Epoch 177/300, trend Loss: 0.0575 | 0.0341
Epoch 178/300, trend Loss: 0.0537 | 0.0321
Epoch 179/300, trend Loss: 0.0531 | 0.0513
Epoch 180/300, trend Loss: 0.0549 | 0.0312
Epoch 181/300, trend Loss: 0.0521 | 0.0304
Epoch 182/300, trend Loss: 0.0538 | 0.0421
Epoch 183/300, trend Loss: 0.0558 | 0.0338
Epoch 184/300, trend Loss: 0.0542 | 0.0324
Epoch 185/300, trend Loss: 0.0517 | 0.0316
Epoch 186/300, trend Loss: 0.0515 | 0.0437
Epoch 187/300, trend Loss: 0.0512 | 0.0317
Epoch 188/300, trend Loss: 0.0519 | 0.0305
Epoch 189/300, trend Loss: 0.0514 | 0.0341
Epoch 190/300, trend Loss: 0.0542 | 0.0364
Epoch 191/300, trend Loss: 0.0532 | 0.0318
Epoch 192/300, trend Loss: 0.0513 | 0.0308
Epoch 193/300, trend Loss: 0.0520 | 0.0472
Epoch 194/300, trend Loss: 0.0500 | 0.0335
Epoch 195/300, trend Loss: 0.0502 | 0.0322
Epoch 196/300, trend Loss: 0.0493 | 0.0325
Epoch 197/300, trend Loss: 0.0514 | 0.0396
Epoch 198/300, trend Loss: 0.0516 | 0.0314
Epoch 199/300, trend Loss: 0.0492 | 0.0314
Epoch 200/300, trend Loss: 0.0500 | 0.0425
Epoch 201/300, trend Loss: 0.0492 | 0.0334
Epoch 202/300, trend Loss: 0.0486 | 0.0310
Epoch 203/300, trend Loss: 0.0473 | 0.0315
Epoch 204/300, trend Loss: 0.0480 | 0.0364
Epoch 205/300, trend Loss: 0.0484 | 0.0312
Epoch 206/300, trend Loss: 0.0472 | 0.0298
Epoch 207/300, trend Loss: 0.0473 | 0.0338
Epoch 208/300, trend Loss: 0.0482 | 0.0348
Epoch 209/300, trend Loss: 0.0465 | 0.0320
Epoch 210/300, trend Loss: 0.0462 | 0.0335
Epoch 211/300, trend Loss: 0.0462 | 0.0314
Epoch 212/300, trend Loss: 0.0457 | 0.0303
Epoch 213/300, trend Loss: 0.0456 | 0.0309
Epoch 214/300, trend Loss: 0.0464 | 0.0315
Epoch 215/300, trend Loss: 0.0473 | 0.0403
Epoch 216/300, trend Loss: 0.0451 | 0.0329
Epoch 217/300, trend Loss: 0.0449 | 0.0320
Epoch 218/300, trend Loss: 0.0440 | 0.0311
Epoch 219/300, trend Loss: 0.0443 | 0.0315
Epoch 220/300, trend Loss: 0.0439 | 0.0331
Epoch 221/300, trend Loss: 0.0438 | 0.0302
Epoch 222/300, trend Loss: 0.0434 | 0.0301
Epoch 223/300, trend Loss: 0.0438 | 0.0323
Epoch 224/300, trend Loss: 0.0443 | 0.0341
Epoch 225/300, trend Loss: 0.0438 | 0.0333
Epoch 226/300, trend Loss: 0.0439 | 0.0343
Epoch 227/300, trend Loss: 0.0453 | 0.0397
Epoch 228/300, trend Loss: 0.0449 | 0.0343
Epoch 229/300, trend Loss: 0.0464 | 0.0339
Epoch 230/300, trend Loss: 0.0462 | 0.0350
Epoch 231/300, trend Loss: 0.0485 | 0.0432
Epoch 232/300, trend Loss: 0.0462 | 0.0450
Epoch 233/300, trend Loss: 0.0480 | 0.0370
Epoch 234/300, trend Loss: 0.0463 | 0.0344
Epoch 235/300, trend Loss: 0.0499 | 0.0457
Epoch 236/300, trend Loss: 0.0459 | 0.0411
Epoch 237/300, trend Loss: 0.0484 | 0.0354
Epoch 238/300, trend Loss: 0.0467 | 0.0333
Epoch 239/300, trend Loss: 0.0496 | 0.0506
Epoch 240/300, trend Loss: 0.0508 | 0.0382
Epoch 241/300, trend Loss: 0.0477 | 0.0393
Epoch 242/300, trend Loss: 0.0454 | 0.0400
Epoch 243/300, trend Loss: 0.0432 | 0.0373
Epoch 244/300, trend Loss: 0.0425 | 0.0326
Epoch 245/300, trend Loss: 0.0419 | 0.0320
Epoch 246/300, trend Loss: 0.0420 | 0.0355
Epoch 247/300, trend Loss: 0.0412 | 0.0376
Epoch 248/300, trend Loss: 0.0409 | 0.0327
Epoch 249/300, trend Loss: 0.0408 | 0.0330
Epoch 250/300, trend Loss: 0.0406 | 0.0332
Epoch 251/300, trend Loss: 0.0405 | 0.0345
Epoch 252/300, trend Loss: 0.0403 | 0.0339
Epoch 253/300, trend Loss: 0.0402 | 0.0343
Epoch 254/300, trend Loss: 0.0399 | 0.0334
Epoch 255/300, trend Loss: 0.0399 | 0.0335
Epoch 256/300, trend Loss: 0.0398 | 0.0335
Epoch 257/300, trend Loss: 0.0399 | 0.0370
Epoch 258/300, trend Loss: 0.0398 | 0.0351
Epoch 259/300, trend Loss: 0.0397 | 0.0344
Epoch 260/300, trend Loss: 0.0396 | 0.0343
Epoch 261/300, trend Loss: 0.0399 | 0.0345
Epoch 262/300, trend Loss: 0.0400 | 0.0370
Epoch 263/300, trend Loss: 0.0400 | 0.0372
Epoch 264/300, trend Loss: 0.0410 | 0.0378
Epoch 265/300, trend Loss: 0.0406 | 0.0355
Epoch 266/300, trend Loss: 0.0415 | 0.0358
Epoch 267/300, trend Loss: 0.0422 | 0.0399
Epoch 268/300, trend Loss: 0.0425 | 0.0386
Epoch 269/300, trend Loss: 0.0428 | 0.0489
Epoch 270/300, trend Loss: 0.0431 | 0.0362
Epoch 271/300, trend Loss: 0.0433 | 0.0379
Epoch 272/300, trend Loss: 0.0449 | 0.0386
Epoch 273/300, trend Loss: 0.0441 | 0.0473
Epoch 274/300, trend Loss: 0.0416 | 0.0394
Epoch 275/300, trend Loss: 0.0397 | 0.0356
Epoch 276/300, trend Loss: 0.0382 | 0.0391
Epoch 277/300, trend Loss: 0.0372 | 0.0373
Epoch 278/300, trend Loss: 0.0367 | 0.0442
Epoch 279/300, trend Loss: 0.0430 | 0.0375
Epoch 280/300, trend Loss: 0.0384 | 0.0369
Epoch 281/300, trend Loss: 0.0390 | 0.0343
Epoch 282/300, trend Loss: 0.0367 | 0.0388
Epoch 283/300, trend Loss: 0.0372 | 0.0362
Epoch 284/300, trend Loss: 0.0354 | 0.0351
Epoch 285/300, trend Loss: 0.0341 | 0.0365
Epoch 286/300, trend Loss: 0.0373 | 0.0390
Epoch 287/300, trend Loss: 0.0380 | 0.0404
Epoch 288/300, trend Loss: 0.0396 | 0.0472
Epoch 289/300, trend Loss: 0.0369 | 0.0461
Epoch 290/300, trend Loss: 0.0348 | 0.0367
Epoch 291/300, trend Loss: 0.0347 | 0.0354
Epoch 292/300, trend Loss: 0.0352 | 0.0415
Epoch 293/300, trend Loss: 0.0335 | 0.0378
Epoch 294/300, trend Loss: 0.0324 | 0.0344
Epoch 295/300, trend Loss: 0.0322 | 0.0356
Epoch 296/300, trend Loss: 0.0320 | 0.0393
Epoch 297/300, trend Loss: 0.0320 | 0.0400
Epoch 298/300, trend Loss: 0.0316 | 0.0347
Epoch 299/300, trend Loss: 0.0318 | 0.0359
Epoch 300/300, trend Loss: 0.0315 | 0.0364
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.8859484757634755, 'learning_rate': 0.00022311137896130736, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9740778183337702}
Epoch 1/300, seasonal_0 Loss: 0.2081 | 0.1130
Epoch 2/300, seasonal_0 Loss: 0.1300 | 0.0710
Epoch 3/300, seasonal_0 Loss: 0.1187 | 0.0604
Epoch 4/300, seasonal_0 Loss: 0.1130 | 0.0580
Epoch 5/300, seasonal_0 Loss: 0.1082 | 0.0549
Epoch 6/300, seasonal_0 Loss: 0.1031 | 0.0521
Epoch 7/300, seasonal_0 Loss: 0.0983 | 0.0494
Epoch 8/300, seasonal_0 Loss: 0.0949 | 0.0486
Epoch 9/300, seasonal_0 Loss: 0.0921 | 0.0483
Epoch 10/300, seasonal_0 Loss: 0.0896 | 0.0477
Epoch 11/300, seasonal_0 Loss: 0.0871 | 0.0469
Epoch 12/300, seasonal_0 Loss: 0.0849 | 0.0457
Epoch 13/300, seasonal_0 Loss: 0.0831 | 0.0443
Epoch 14/300, seasonal_0 Loss: 0.0815 | 0.0425
Epoch 15/300, seasonal_0 Loss: 0.0799 | 0.0404
Epoch 16/300, seasonal_0 Loss: 0.0784 | 0.0385
Epoch 17/300, seasonal_0 Loss: 0.0770 | 0.0372
Epoch 18/300, seasonal_0 Loss: 0.0760 | 0.0363
Epoch 19/300, seasonal_0 Loss: 0.0752 | 0.0358
Epoch 20/300, seasonal_0 Loss: 0.0745 | 0.0353
Epoch 21/300, seasonal_0 Loss: 0.0738 | 0.0348
Epoch 22/300, seasonal_0 Loss: 0.0732 | 0.0343
Epoch 23/300, seasonal_0 Loss: 0.0726 | 0.0338
Epoch 24/300, seasonal_0 Loss: 0.0721 | 0.0334
Epoch 25/300, seasonal_0 Loss: 0.0716 | 0.0331
Epoch 26/300, seasonal_0 Loss: 0.0712 | 0.0328
Epoch 27/300, seasonal_0 Loss: 0.0707 | 0.0326
Epoch 28/300, seasonal_0 Loss: 0.0703 | 0.0325
Epoch 29/300, seasonal_0 Loss: 0.0699 | 0.0325
Epoch 30/300, seasonal_0 Loss: 0.0696 | 0.0324
Epoch 31/300, seasonal_0 Loss: 0.0693 | 0.0323
Epoch 32/300, seasonal_0 Loss: 0.0690 | 0.0322
Epoch 33/300, seasonal_0 Loss: 0.0688 | 0.0320
Epoch 34/300, seasonal_0 Loss: 0.0685 | 0.0320
Epoch 35/300, seasonal_0 Loss: 0.0683 | 0.0319
Epoch 36/300, seasonal_0 Loss: 0.0680 | 0.0319
Epoch 37/300, seasonal_0 Loss: 0.0678 | 0.0319
Epoch 38/300, seasonal_0 Loss: 0.0676 | 0.0319
Epoch 39/300, seasonal_0 Loss: 0.0674 | 0.0321
Epoch 40/300, seasonal_0 Loss: 0.0672 | 0.0324
Epoch 41/300, seasonal_0 Loss: 0.0670 | 0.0326
Epoch 42/300, seasonal_0 Loss: 0.0668 | 0.0328
Epoch 43/300, seasonal_0 Loss: 0.0667 | 0.0329
Epoch 44/300, seasonal_0 Loss: 0.0665 | 0.0330
Epoch 45/300, seasonal_0 Loss: 0.0663 | 0.0331
Epoch 46/300, seasonal_0 Loss: 0.0662 | 0.0332
Epoch 47/300, seasonal_0 Loss: 0.0660 | 0.0332
Epoch 48/300, seasonal_0 Loss: 0.0658 | 0.0332
Epoch 49/300, seasonal_0 Loss: 0.0657 | 0.0330
Epoch 50/300, seasonal_0 Loss: 0.0655 | 0.0328
Epoch 51/300, seasonal_0 Loss: 0.0654 | 0.0325
Epoch 52/300, seasonal_0 Loss: 0.0653 | 0.0322
Epoch 53/300, seasonal_0 Loss: 0.0652 | 0.0318
Epoch 54/300, seasonal_0 Loss: 0.0651 | 0.0314
Epoch 55/300, seasonal_0 Loss: 0.0650 | 0.0311
Epoch 56/300, seasonal_0 Loss: 0.0649 | 0.0308
Epoch 57/300, seasonal_0 Loss: 0.0648 | 0.0305
Epoch 58/300, seasonal_0 Loss: 0.0647 | 0.0303
Epoch 59/300, seasonal_0 Loss: 0.0646 | 0.0301
Epoch 60/300, seasonal_0 Loss: 0.0645 | 0.0299
Epoch 61/300, seasonal_0 Loss: 0.0644 | 0.0298
Epoch 62/300, seasonal_0 Loss: 0.0643 | 0.0297
Epoch 63/300, seasonal_0 Loss: 0.0642 | 0.0296
Epoch 64/300, seasonal_0 Loss: 0.0641 | 0.0296
Epoch 65/300, seasonal_0 Loss: 0.0640 | 0.0295
Epoch 66/300, seasonal_0 Loss: 0.0639 | 0.0295
Epoch 67/300, seasonal_0 Loss: 0.0639 | 0.0295
Epoch 68/300, seasonal_0 Loss: 0.0638 | 0.0294
Epoch 69/300, seasonal_0 Loss: 0.0638 | 0.0294
Epoch 70/300, seasonal_0 Loss: 0.0637 | 0.0294
Epoch 71/300, seasonal_0 Loss: 0.0636 | 0.0293
Epoch 72/300, seasonal_0 Loss: 0.0636 | 0.0293
Epoch 73/300, seasonal_0 Loss: 0.0635 | 0.0293
Epoch 74/300, seasonal_0 Loss: 0.0635 | 0.0293
Epoch 75/300, seasonal_0 Loss: 0.0634 | 0.0293
Epoch 76/300, seasonal_0 Loss: 0.0634 | 0.0292
Epoch 77/300, seasonal_0 Loss: 0.0634 | 0.0292
Epoch 78/300, seasonal_0 Loss: 0.0633 | 0.0292
Epoch 79/300, seasonal_0 Loss: 0.0633 | 0.0292
Epoch 80/300, seasonal_0 Loss: 0.0632 | 0.0292
Epoch 81/300, seasonal_0 Loss: 0.0632 | 0.0292
Epoch 82/300, seasonal_0 Loss: 0.0632 | 0.0291
Epoch 83/300, seasonal_0 Loss: 0.0631 | 0.0291
Epoch 84/300, seasonal_0 Loss: 0.0631 | 0.0291
Epoch 85/300, seasonal_0 Loss: 0.0631 | 0.0291
Epoch 86/300, seasonal_0 Loss: 0.0631 | 0.0291
Epoch 87/300, seasonal_0 Loss: 0.0630 | 0.0291
Epoch 88/300, seasonal_0 Loss: 0.0630 | 0.0291
Epoch 89/300, seasonal_0 Loss: 0.0630 | 0.0290
Epoch 90/300, seasonal_0 Loss: 0.0629 | 0.0290
Epoch 91/300, seasonal_0 Loss: 0.0629 | 0.0290
Epoch 92/300, seasonal_0 Loss: 0.0629 | 0.0290
Epoch 93/300, seasonal_0 Loss: 0.0629 | 0.0290
Epoch 94/300, seasonal_0 Loss: 0.0629 | 0.0290
Epoch 95/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 96/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 97/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 98/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 99/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 100/300, seasonal_0 Loss: 0.0628 | 0.0290
Epoch 101/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 102/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 103/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 104/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 105/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 106/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 107/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 108/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 109/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 110/300, seasonal_0 Loss: 0.0627 | 0.0290
Epoch 111/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 112/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 113/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 114/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 115/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 116/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 117/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 118/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 119/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 120/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 121/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 122/300, seasonal_0 Loss: 0.0626 | 0.0290
Epoch 123/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 124/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 125/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 126/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 127/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 128/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 129/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 130/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 131/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 132/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 133/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 134/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 135/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 136/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 137/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 138/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 139/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 140/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 141/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 142/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 143/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 144/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 145/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 146/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 147/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 148/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 149/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 150/300, seasonal_0 Loss: 0.0625 | 0.0290
Epoch 151/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 152/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 153/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 154/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 155/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 156/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 157/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 158/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 159/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 160/300, seasonal_0 Loss: 0.0624 | 0.0290
Epoch 161/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 162/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 163/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 164/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 165/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 166/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 167/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 168/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 169/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 170/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 171/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 172/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 173/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 174/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 175/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 176/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 177/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 178/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 179/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 180/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 181/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 182/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 183/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 184/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 185/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 186/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 187/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 188/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 189/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 190/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 191/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 192/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 193/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 194/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 195/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 196/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 197/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 198/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 199/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 200/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 201/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 202/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 203/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 204/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 205/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 206/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 207/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 208/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 209/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 210/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 211/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 212/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 213/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 214/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 215/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 216/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 217/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 218/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 219/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 220/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 221/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 222/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 223/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 224/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 225/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 226/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 227/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 228/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 229/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 230/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 231/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 232/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 233/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 234/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 235/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 236/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 237/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 238/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 239/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 240/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 241/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 242/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 243/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 244/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 245/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 246/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 247/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 248/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 249/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 250/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 251/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 252/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 253/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 254/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 255/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 256/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 257/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 258/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 259/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 260/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 261/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 262/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 263/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 264/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 265/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 266/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 267/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 268/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 269/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 270/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 271/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 272/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 273/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 274/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 275/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 276/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 277/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 278/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 279/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 280/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 281/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 282/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 283/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 284/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 285/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 286/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 287/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 288/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 289/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 290/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 291/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 292/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 293/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 294/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 295/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 296/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 297/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 298/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 299/300, seasonal_0 Loss: 0.0624 | 0.0289
Epoch 300/300, seasonal_0 Loss: 0.0624 | 0.0289
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.8033857963319848, 'learning_rate': 0.0008062079867485131, 'batch_size': 157, 'step_size': 1, 'gamma': 0.988781247647316}
Epoch 1/300, seasonal_1 Loss: 0.5351 | 0.1860
Epoch 2/300, seasonal_1 Loss: 0.1915 | 0.1357
Epoch 3/300, seasonal_1 Loss: 0.1764 | 0.1345
Epoch 4/300, seasonal_1 Loss: 0.1530 | 0.0858
Epoch 5/300, seasonal_1 Loss: 0.1316 | 0.0708
Epoch 6/300, seasonal_1 Loss: 0.1216 | 0.0655
Epoch 7/300, seasonal_1 Loss: 0.1170 | 0.0650
Epoch 8/300, seasonal_1 Loss: 0.1209 | 0.0659
Epoch 9/300, seasonal_1 Loss: 0.1215 | 0.0645
Epoch 10/300, seasonal_1 Loss: 0.1175 | 0.0637
Epoch 11/300, seasonal_1 Loss: 0.1142 | 0.0623
Epoch 12/300, seasonal_1 Loss: 0.1094 | 0.0626
Epoch 13/300, seasonal_1 Loss: 0.1071 | 0.0796
Epoch 14/300, seasonal_1 Loss: 0.1093 | 0.0532
Epoch 15/300, seasonal_1 Loss: 0.1179 | 0.0870
Epoch 16/300, seasonal_1 Loss: 0.1174 | 0.0637
Epoch 17/300, seasonal_1 Loss: 0.1042 | 0.0559
Epoch 18/300, seasonal_1 Loss: 0.0985 | 0.0499
Epoch 19/300, seasonal_1 Loss: 0.0993 | 0.0492
Epoch 20/300, seasonal_1 Loss: 0.1081 | 0.0630
Epoch 21/300, seasonal_1 Loss: 0.1125 | 0.0830
Epoch 22/300, seasonal_1 Loss: 0.1146 | 0.0725
Epoch 23/300, seasonal_1 Loss: 0.1005 | 0.0498
Epoch 24/300, seasonal_1 Loss: 0.0979 | 0.0487
Epoch 25/300, seasonal_1 Loss: 0.0952 | 0.0500
Epoch 26/300, seasonal_1 Loss: 0.0976 | 0.0470
Epoch 27/300, seasonal_1 Loss: 0.0930 | 0.0550
Epoch 28/300, seasonal_1 Loss: 0.0907 | 0.0470
Epoch 29/300, seasonal_1 Loss: 0.0894 | 0.0435
Epoch 30/300, seasonal_1 Loss: 0.0889 | 0.0424
Epoch 31/300, seasonal_1 Loss: 0.0895 | 0.0440
Epoch 32/300, seasonal_1 Loss: 0.0898 | 0.0455
Epoch 33/300, seasonal_1 Loss: 0.0882 | 0.0433
Epoch 34/300, seasonal_1 Loss: 0.0871 | 0.0425
Epoch 35/300, seasonal_1 Loss: 0.0872 | 0.0418
Epoch 36/300, seasonal_1 Loss: 0.0885 | 0.0421
Epoch 37/300, seasonal_1 Loss: 0.0902 | 0.0442
Epoch 38/300, seasonal_1 Loss: 0.0921 | 0.0442
Epoch 39/300, seasonal_1 Loss: 0.0943 | 0.0449
Epoch 40/300, seasonal_1 Loss: 0.0961 | 0.0463
Epoch 41/300, seasonal_1 Loss: 0.0972 | 0.0458
Epoch 42/300, seasonal_1 Loss: 0.0982 | 0.0488
Epoch 43/300, seasonal_1 Loss: 0.0983 | 0.0497
Epoch 44/300, seasonal_1 Loss: 0.0963 | 0.0470
Epoch 45/300, seasonal_1 Loss: 0.0906 | 0.0418
Epoch 46/300, seasonal_1 Loss: 0.0910 | 0.0434
Epoch 47/300, seasonal_1 Loss: 0.0919 | 0.0488
Epoch 48/300, seasonal_1 Loss: 0.0898 | 0.0482
Epoch 49/300, seasonal_1 Loss: 0.0909 | 0.0457
Epoch 50/300, seasonal_1 Loss: 0.0839 | 0.0395
Epoch 51/300, seasonal_1 Loss: 0.0833 | 0.0404
Epoch 52/300, seasonal_1 Loss: 0.0817 | 0.0398
Epoch 53/300, seasonal_1 Loss: 0.0812 | 0.0400
Epoch 54/300, seasonal_1 Loss: 0.0792 | 0.0399
Epoch 55/300, seasonal_1 Loss: 0.0791 | 0.0385
Epoch 56/300, seasonal_1 Loss: 0.0787 | 0.0381
Epoch 57/300, seasonal_1 Loss: 0.0783 | 0.0383
Epoch 58/300, seasonal_1 Loss: 0.0785 | 0.0389
Epoch 59/300, seasonal_1 Loss: 0.0794 | 0.0397
Epoch 60/300, seasonal_1 Loss: 0.0801 | 0.0386
Epoch 61/300, seasonal_1 Loss: 0.0782 | 0.0382
Epoch 62/300, seasonal_1 Loss: 0.0770 | 0.0375
Epoch 63/300, seasonal_1 Loss: 0.0760 | 0.0370
Epoch 64/300, seasonal_1 Loss: 0.0756 | 0.0367
Epoch 65/300, seasonal_1 Loss: 0.0753 | 0.0366
Epoch 66/300, seasonal_1 Loss: 0.0750 | 0.0364
Epoch 67/300, seasonal_1 Loss: 0.0746 | 0.0363
Epoch 68/300, seasonal_1 Loss: 0.0743 | 0.0361
Epoch 69/300, seasonal_1 Loss: 0.0741 | 0.0361
Epoch 70/300, seasonal_1 Loss: 0.0738 | 0.0360
Epoch 71/300, seasonal_1 Loss: 0.0736 | 0.0359
Epoch 72/300, seasonal_1 Loss: 0.0733 | 0.0358
Epoch 73/300, seasonal_1 Loss: 0.0731 | 0.0358
Epoch 74/300, seasonal_1 Loss: 0.0729 | 0.0357
Epoch 75/300, seasonal_1 Loss: 0.0727 | 0.0356
Epoch 76/300, seasonal_1 Loss: 0.0724 | 0.0355
Epoch 77/300, seasonal_1 Loss: 0.0722 | 0.0355
Epoch 78/300, seasonal_1 Loss: 0.0720 | 0.0354
Epoch 79/300, seasonal_1 Loss: 0.0719 | 0.0353
Epoch 80/300, seasonal_1 Loss: 0.0717 | 0.0351
Epoch 81/300, seasonal_1 Loss: 0.0716 | 0.0350
Epoch 82/300, seasonal_1 Loss: 0.0716 | 0.0349
Epoch 83/300, seasonal_1 Loss: 0.0718 | 0.0349
Epoch 84/300, seasonal_1 Loss: 0.0722 | 0.0352
Epoch 85/300, seasonal_1 Loss: 0.0726 | 0.0346
Epoch 86/300, seasonal_1 Loss: 0.0721 | 0.0351
Epoch 87/300, seasonal_1 Loss: 0.0714 | 0.0350
Epoch 88/300, seasonal_1 Loss: 0.0716 | 0.0354
Epoch 89/300, seasonal_1 Loss: 0.0721 | 0.0367
Epoch 90/300, seasonal_1 Loss: 0.0724 | 0.0366
Epoch 91/300, seasonal_1 Loss: 0.0714 | 0.0354
Epoch 92/300, seasonal_1 Loss: 0.0703 | 0.0342
Epoch 93/300, seasonal_1 Loss: 0.0700 | 0.0337
Epoch 94/300, seasonal_1 Loss: 0.0697 | 0.0336
Epoch 95/300, seasonal_1 Loss: 0.0694 | 0.0333
Epoch 96/300, seasonal_1 Loss: 0.0692 | 0.0333
Epoch 97/300, seasonal_1 Loss: 0.0690 | 0.0332
Epoch 98/300, seasonal_1 Loss: 0.0689 | 0.0332
Epoch 99/300, seasonal_1 Loss: 0.0687 | 0.0332
Epoch 100/300, seasonal_1 Loss: 0.0687 | 0.0332
Epoch 101/300, seasonal_1 Loss: 0.0686 | 0.0334
Epoch 102/300, seasonal_1 Loss: 0.0687 | 0.0337
Epoch 103/300, seasonal_1 Loss: 0.0689 | 0.0335
Epoch 104/300, seasonal_1 Loss: 0.0689 | 0.0333
Epoch 105/300, seasonal_1 Loss: 0.0686 | 0.0331
Epoch 106/300, seasonal_1 Loss: 0.0683 | 0.0326
Epoch 107/300, seasonal_1 Loss: 0.0680 | 0.0323
Epoch 108/300, seasonal_1 Loss: 0.0678 | 0.0321
Epoch 109/300, seasonal_1 Loss: 0.0676 | 0.0320
Epoch 110/300, seasonal_1 Loss: 0.0674 | 0.0319
Epoch 111/300, seasonal_1 Loss: 0.0674 | 0.0318
Epoch 112/300, seasonal_1 Loss: 0.0673 | 0.0316
Epoch 113/300, seasonal_1 Loss: 0.0671 | 0.0315
Epoch 114/300, seasonal_1 Loss: 0.0670 | 0.0314
Epoch 115/300, seasonal_1 Loss: 0.0669 | 0.0313
Epoch 116/300, seasonal_1 Loss: 0.0668 | 0.0313
Epoch 117/300, seasonal_1 Loss: 0.0667 | 0.0312
Epoch 118/300, seasonal_1 Loss: 0.0666 | 0.0312
Epoch 119/300, seasonal_1 Loss: 0.0664 | 0.0312
Epoch 120/300, seasonal_1 Loss: 0.0664 | 0.0312
Epoch 121/300, seasonal_1 Loss: 0.0663 | 0.0312
Epoch 122/300, seasonal_1 Loss: 0.0664 | 0.0316
Epoch 123/300, seasonal_1 Loss: 0.0669 | 0.0328
Epoch 124/300, seasonal_1 Loss: 0.0683 | 0.0318
Epoch 125/300, seasonal_1 Loss: 0.0677 | 0.0320
Epoch 126/300, seasonal_1 Loss: 0.0663 | 0.0315
Epoch 127/300, seasonal_1 Loss: 0.0661 | 0.0313
Epoch 128/300, seasonal_1 Loss: 0.0659 | 0.0312
Epoch 129/300, seasonal_1 Loss: 0.0658 | 0.0311
Epoch 130/300, seasonal_1 Loss: 0.0657 | 0.0311
Epoch 131/300, seasonal_1 Loss: 0.0655 | 0.0311
Epoch 132/300, seasonal_1 Loss: 0.0655 | 0.0312
Epoch 133/300, seasonal_1 Loss: 0.0654 | 0.0313
Epoch 134/300, seasonal_1 Loss: 0.0654 | 0.0314
Epoch 135/300, seasonal_1 Loss: 0.0655 | 0.0316
Epoch 136/300, seasonal_1 Loss: 0.0655 | 0.0318
Epoch 137/300, seasonal_1 Loss: 0.0655 | 0.0319
Epoch 138/300, seasonal_1 Loss: 0.0655 | 0.0318
Epoch 139/300, seasonal_1 Loss: 0.0653 | 0.0316
Epoch 140/300, seasonal_1 Loss: 0.0651 | 0.0312
Epoch 141/300, seasonal_1 Loss: 0.0650 | 0.0311
Epoch 142/300, seasonal_1 Loss: 0.0650 | 0.0310
Epoch 143/300, seasonal_1 Loss: 0.0650 | 0.0311
Epoch 144/300, seasonal_1 Loss: 0.0651 | 0.0310
Epoch 145/300, seasonal_1 Loss: 0.0652 | 0.0311
Epoch 146/300, seasonal_1 Loss: 0.0652 | 0.0308
Epoch 147/300, seasonal_1 Loss: 0.0648 | 0.0310
Epoch 148/300, seasonal_1 Loss: 0.0649 | 0.0312
Epoch 149/300, seasonal_1 Loss: 0.0650 | 0.0316
Epoch 150/300, seasonal_1 Loss: 0.0651 | 0.0313
Epoch 151/300, seasonal_1 Loss: 0.0648 | 0.0314
Epoch 152/300, seasonal_1 Loss: 0.0647 | 0.0309
Epoch 153/300, seasonal_1 Loss: 0.0647 | 0.0312
Epoch 154/300, seasonal_1 Loss: 0.0648 | 0.0308
Epoch 155/300, seasonal_1 Loss: 0.0647 | 0.0311
Epoch 156/300, seasonal_1 Loss: 0.0647 | 0.0307
Epoch 157/300, seasonal_1 Loss: 0.0644 | 0.0312
Epoch 158/300, seasonal_1 Loss: 0.0645 | 0.0309
Epoch 159/300, seasonal_1 Loss: 0.0644 | 0.0313
Epoch 160/300, seasonal_1 Loss: 0.0644 | 0.0309
Epoch 161/300, seasonal_1 Loss: 0.0642 | 0.0311
Epoch 162/300, seasonal_1 Loss: 0.0642 | 0.0308
Epoch 163/300, seasonal_1 Loss: 0.0641 | 0.0310
Epoch 164/300, seasonal_1 Loss: 0.0641 | 0.0307
Epoch 165/300, seasonal_1 Loss: 0.0640 | 0.0309
Epoch 166/300, seasonal_1 Loss: 0.0639 | 0.0308
Epoch 167/300, seasonal_1 Loss: 0.0639 | 0.0310
Epoch 168/300, seasonal_1 Loss: 0.0639 | 0.0309
Epoch 169/300, seasonal_1 Loss: 0.0638 | 0.0309
Epoch 170/300, seasonal_1 Loss: 0.0638 | 0.0308
Epoch 171/300, seasonal_1 Loss: 0.0638 | 0.0308
Epoch 172/300, seasonal_1 Loss: 0.0637 | 0.0308
Epoch 173/300, seasonal_1 Loss: 0.0637 | 0.0308
Epoch 174/300, seasonal_1 Loss: 0.0637 | 0.0308
Epoch 175/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 176/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 177/300, seasonal_1 Loss: 0.0636 | 0.0308
Epoch 178/300, seasonal_1 Loss: 0.0636 | 0.0308
Epoch 179/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 180/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 181/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 182/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 183/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 184/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 185/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 186/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 187/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 188/300, seasonal_1 Loss: 0.0633 | 0.0308
Epoch 189/300, seasonal_1 Loss: 0.0633 | 0.0308
Epoch 190/300, seasonal_1 Loss: 0.0633 | 0.0309
Epoch 191/300, seasonal_1 Loss: 0.0633 | 0.0309
Epoch 192/300, seasonal_1 Loss: 0.0633 | 0.0309
Epoch 193/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 194/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 195/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 196/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 197/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 198/300, seasonal_1 Loss: 0.0632 | 0.0309
Epoch 199/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 200/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 201/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 202/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 203/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 204/300, seasonal_1 Loss: 0.0631 | 0.0309
Epoch 205/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 206/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 207/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 208/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 209/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 210/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 211/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 212/300, seasonal_1 Loss: 0.0630 | 0.0309
Epoch 213/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 214/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 215/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 216/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 217/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 218/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 219/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 220/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 221/300, seasonal_1 Loss: 0.0629 | 0.0309
Epoch 222/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 223/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 224/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 225/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 226/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 227/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 228/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 229/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 230/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 231/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 232/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 233/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 234/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 235/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 236/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 237/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 238/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 239/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 240/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 241/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 242/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 243/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 244/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 245/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 246/300, seasonal_1 Loss: 0.0627 | 0.0309
Epoch 247/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 248/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 249/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 250/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 251/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 252/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 253/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 254/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 255/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 256/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 257/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 258/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 259/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 260/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 261/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 262/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 263/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 264/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 265/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 266/300, seasonal_1 Loss: 0.0626 | 0.0309
Epoch 267/300, seasonal_1 Loss: 0.0625 | 0.0309
Epoch 268/300, seasonal_1 Loss: 0.0625 | 0.0309
Epoch 269/300, seasonal_1 Loss: 0.0625 | 0.0309
Epoch 270/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 271/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 272/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 273/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 274/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 275/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 276/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 277/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 278/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 279/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 280/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 281/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 282/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 283/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 284/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 285/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 286/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 287/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 288/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 289/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 290/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 291/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 292/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 293/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 294/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 295/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 296/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 297/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 298/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 299/300, seasonal_1 Loss: 0.0625 | 0.0310
Epoch 300/300, seasonal_1 Loss: 0.0624 | 0.0310
Training seasonal_2 component with params: {'observation_period_num': 15, 'train_rates': 0.942933330624439, 'learning_rate': 0.0008521571982962857, 'batch_size': 194, 'step_size': 10, 'gamma': 0.8723427194013}
Epoch 1/300, seasonal_2 Loss: 0.6432 | 0.2625
Epoch 2/300, seasonal_2 Loss: 0.1927 | 0.1432
Epoch 3/300, seasonal_2 Loss: 0.1636 | 0.0997
Epoch 4/300, seasonal_2 Loss: 0.1487 | 0.0928
Epoch 5/300, seasonal_2 Loss: 0.1483 | 0.1081
Epoch 6/300, seasonal_2 Loss: 0.1544 | 0.1059
Epoch 7/300, seasonal_2 Loss: 0.1942 | 0.3023
Epoch 8/300, seasonal_2 Loss: 0.1813 | 0.1924
Epoch 9/300, seasonal_2 Loss: 0.1731 | 0.0989
Epoch 10/300, seasonal_2 Loss: 0.1492 | 0.1073
Epoch 11/300, seasonal_2 Loss: 0.1690 | 0.1173
Epoch 12/300, seasonal_2 Loss: 0.1544 | 0.0872
Epoch 13/300, seasonal_2 Loss: 0.1270 | 0.1122
Epoch 14/300, seasonal_2 Loss: 0.1246 | 0.0661
Epoch 15/300, seasonal_2 Loss: 0.1238 | 0.0672
Epoch 16/300, seasonal_2 Loss: 0.1361 | 0.0772
Epoch 17/300, seasonal_2 Loss: 0.1358 | 0.0690
Epoch 18/300, seasonal_2 Loss: 0.1113 | 0.0659
Epoch 19/300, seasonal_2 Loss: 0.1218 | 0.0966
Epoch 20/300, seasonal_2 Loss: 0.1397 | 0.0983
Epoch 21/300, seasonal_2 Loss: 0.1194 | 0.0629
Epoch 22/300, seasonal_2 Loss: 0.1083 | 0.0603
Epoch 23/300, seasonal_2 Loss: 0.1234 | 0.0622
Epoch 24/300, seasonal_2 Loss: 0.1077 | 0.0855
Epoch 25/300, seasonal_2 Loss: 0.1085 | 0.0781
Epoch 26/300, seasonal_2 Loss: 0.0975 | 0.0549
Epoch 27/300, seasonal_2 Loss: 0.0973 | 0.0576
Epoch 28/300, seasonal_2 Loss: 0.0927 | 0.0531
Epoch 29/300, seasonal_2 Loss: 0.0911 | 0.0565
Epoch 30/300, seasonal_2 Loss: 0.0880 | 0.0565
Epoch 31/300, seasonal_2 Loss: 0.0843 | 0.0507
Epoch 32/300, seasonal_2 Loss: 0.0845 | 0.0463
Epoch 33/300, seasonal_2 Loss: 0.0823 | 0.0468
Epoch 34/300, seasonal_2 Loss: 0.0810 | 0.0461
Epoch 35/300, seasonal_2 Loss: 0.0803 | 0.0537
Epoch 36/300, seasonal_2 Loss: 0.0798 | 0.0520
Epoch 37/300, seasonal_2 Loss: 0.0798 | 0.0418
Epoch 38/300, seasonal_2 Loss: 0.0784 | 0.0442
Epoch 39/300, seasonal_2 Loss: 0.0782 | 0.0429
Epoch 40/300, seasonal_2 Loss: 0.0788 | 0.0554
Epoch 41/300, seasonal_2 Loss: 0.0786 | 0.0441
Epoch 42/300, seasonal_2 Loss: 0.0782 | 0.0409
Epoch 43/300, seasonal_2 Loss: 0.0765 | 0.0413
Epoch 44/300, seasonal_2 Loss: 0.0769 | 0.0516
Epoch 45/300, seasonal_2 Loss: 0.0771 | 0.0457
Epoch 46/300, seasonal_2 Loss: 0.0773 | 0.0433
Epoch 47/300, seasonal_2 Loss: 0.0761 | 0.0420
Epoch 48/300, seasonal_2 Loss: 0.0752 | 0.0467
Epoch 49/300, seasonal_2 Loss: 0.0741 | 0.0386
Epoch 50/300, seasonal_2 Loss: 0.0734 | 0.0386
Epoch 51/300, seasonal_2 Loss: 0.0733 | 0.0408
Epoch 52/300, seasonal_2 Loss: 0.0722 | 0.0410
Epoch 53/300, seasonal_2 Loss: 0.0721 | 0.0378
Epoch 54/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 55/300, seasonal_2 Loss: 0.0714 | 0.0404
Epoch 56/300, seasonal_2 Loss: 0.0712 | 0.0379
Epoch 57/300, seasonal_2 Loss: 0.0711 | 0.0372
Epoch 58/300, seasonal_2 Loss: 0.0711 | 0.0391
Epoch 59/300, seasonal_2 Loss: 0.0713 | 0.0393
Epoch 60/300, seasonal_2 Loss: 0.0721 | 0.0391
Epoch 61/300, seasonal_2 Loss: 0.0736 | 0.0413
Epoch 62/300, seasonal_2 Loss: 0.0748 | 0.0446
Epoch 63/300, seasonal_2 Loss: 0.0745 | 0.0416
Epoch 64/300, seasonal_2 Loss: 0.0723 | 0.0383
Epoch 65/300, seasonal_2 Loss: 0.0723 | 0.0384
Epoch 66/300, seasonal_2 Loss: 0.0723 | 0.0380
Epoch 67/300, seasonal_2 Loss: 0.0708 | 0.0388
Epoch 68/300, seasonal_2 Loss: 0.0694 | 0.0377
Epoch 69/300, seasonal_2 Loss: 0.0688 | 0.0371
Epoch 70/300, seasonal_2 Loss: 0.0684 | 0.0365
Epoch 71/300, seasonal_2 Loss: 0.0682 | 0.0361
Epoch 72/300, seasonal_2 Loss: 0.0679 | 0.0361
Epoch 73/300, seasonal_2 Loss: 0.0677 | 0.0360
Epoch 74/300, seasonal_2 Loss: 0.0674 | 0.0357
Epoch 75/300, seasonal_2 Loss: 0.0672 | 0.0354
Epoch 76/300, seasonal_2 Loss: 0.0669 | 0.0351
Epoch 77/300, seasonal_2 Loss: 0.0666 | 0.0349
Epoch 78/300, seasonal_2 Loss: 0.0664 | 0.0347
Epoch 79/300, seasonal_2 Loss: 0.0662 | 0.0344
Epoch 80/300, seasonal_2 Loss: 0.0660 | 0.0342
Epoch 81/300, seasonal_2 Loss: 0.0658 | 0.0339
Epoch 82/300, seasonal_2 Loss: 0.0655 | 0.0337
Epoch 83/300, seasonal_2 Loss: 0.0653 | 0.0335
Epoch 84/300, seasonal_2 Loss: 0.0651 | 0.0333
Epoch 85/300, seasonal_2 Loss: 0.0649 | 0.0330
Epoch 86/300, seasonal_2 Loss: 0.0647 | 0.0327
Epoch 87/300, seasonal_2 Loss: 0.0645 | 0.0326
Epoch 88/300, seasonal_2 Loss: 0.0644 | 0.0324
Epoch 89/300, seasonal_2 Loss: 0.0642 | 0.0322
Epoch 90/300, seasonal_2 Loss: 0.0641 | 0.0320
Epoch 91/300, seasonal_2 Loss: 0.0639 | 0.0318
Epoch 92/300, seasonal_2 Loss: 0.0637 | 0.0317
Epoch 93/300, seasonal_2 Loss: 0.0636 | 0.0315
Epoch 94/300, seasonal_2 Loss: 0.0634 | 0.0313
Epoch 95/300, seasonal_2 Loss: 0.0633 | 0.0312
Epoch 96/300, seasonal_2 Loss: 0.0631 | 0.0310
Epoch 97/300, seasonal_2 Loss: 0.0630 | 0.0309
Epoch 98/300, seasonal_2 Loss: 0.0629 | 0.0307
Epoch 99/300, seasonal_2 Loss: 0.0627 | 0.0306
Epoch 100/300, seasonal_2 Loss: 0.0626 | 0.0305
Epoch 101/300, seasonal_2 Loss: 0.0625 | 0.0303
Epoch 102/300, seasonal_2 Loss: 0.0623 | 0.0302
Epoch 103/300, seasonal_2 Loss: 0.0622 | 0.0301
Epoch 104/300, seasonal_2 Loss: 0.0621 | 0.0300
Epoch 105/300, seasonal_2 Loss: 0.0620 | 0.0299
Epoch 106/300, seasonal_2 Loss: 0.0618 | 0.0298
Epoch 107/300, seasonal_2 Loss: 0.0617 | 0.0297
Epoch 108/300, seasonal_2 Loss: 0.0616 | 0.0296
Epoch 109/300, seasonal_2 Loss: 0.0615 | 0.0295
Epoch 110/300, seasonal_2 Loss: 0.0614 | 0.0295
Epoch 111/300, seasonal_2 Loss: 0.0613 | 0.0294
Epoch 112/300, seasonal_2 Loss: 0.0612 | 0.0293
Epoch 113/300, seasonal_2 Loss: 0.0611 | 0.0293
Epoch 114/300, seasonal_2 Loss: 0.0610 | 0.0292
Epoch 115/300, seasonal_2 Loss: 0.0609 | 0.0292
Epoch 116/300, seasonal_2 Loss: 0.0608 | 0.0291
Epoch 117/300, seasonal_2 Loss: 0.0607 | 0.0291
Epoch 118/300, seasonal_2 Loss: 0.0606 | 0.0290
Epoch 119/300, seasonal_2 Loss: 0.0605 | 0.0290
Epoch 120/300, seasonal_2 Loss: 0.0604 | 0.0289
Epoch 121/300, seasonal_2 Loss: 0.0604 | 0.0289
Epoch 122/300, seasonal_2 Loss: 0.0603 | 0.0288
Epoch 123/300, seasonal_2 Loss: 0.0602 | 0.0288
Epoch 124/300, seasonal_2 Loss: 0.0601 | 0.0287
Epoch 125/300, seasonal_2 Loss: 0.0601 | 0.0287
Epoch 126/300, seasonal_2 Loss: 0.0600 | 0.0287
Epoch 127/300, seasonal_2 Loss: 0.0599 | 0.0286
Epoch 128/300, seasonal_2 Loss: 0.0599 | 0.0286
Epoch 129/300, seasonal_2 Loss: 0.0598 | 0.0285
Epoch 130/300, seasonal_2 Loss: 0.0598 | 0.0285
Epoch 131/300, seasonal_2 Loss: 0.0597 | 0.0285
Epoch 132/300, seasonal_2 Loss: 0.0597 | 0.0284
Epoch 133/300, seasonal_2 Loss: 0.0596 | 0.0284
Epoch 134/300, seasonal_2 Loss: 0.0596 | 0.0284
Epoch 135/300, seasonal_2 Loss: 0.0595 | 0.0283
Epoch 136/300, seasonal_2 Loss: 0.0595 | 0.0283
Epoch 137/300, seasonal_2 Loss: 0.0594 | 0.0282
Epoch 138/300, seasonal_2 Loss: 0.0594 | 0.0282
Epoch 139/300, seasonal_2 Loss: 0.0593 | 0.0282
Epoch 140/300, seasonal_2 Loss: 0.0593 | 0.0282
Epoch 141/300, seasonal_2 Loss: 0.0592 | 0.0281
Epoch 142/300, seasonal_2 Loss: 0.0592 | 0.0281
Epoch 143/300, seasonal_2 Loss: 0.0592 | 0.0281
Epoch 144/300, seasonal_2 Loss: 0.0591 | 0.0280
Epoch 145/300, seasonal_2 Loss: 0.0591 | 0.0280
Epoch 146/300, seasonal_2 Loss: 0.0591 | 0.0280
Epoch 147/300, seasonal_2 Loss: 0.0590 | 0.0279
Epoch 148/300, seasonal_2 Loss: 0.0590 | 0.0279
Epoch 149/300, seasonal_2 Loss: 0.0590 | 0.0279
Epoch 150/300, seasonal_2 Loss: 0.0589 | 0.0279
Epoch 151/300, seasonal_2 Loss: 0.0589 | 0.0278
Epoch 152/300, seasonal_2 Loss: 0.0589 | 0.0278
Epoch 153/300, seasonal_2 Loss: 0.0588 | 0.0278
Epoch 154/300, seasonal_2 Loss: 0.0588 | 0.0278
Epoch 155/300, seasonal_2 Loss: 0.0588 | 0.0278
Epoch 156/300, seasonal_2 Loss: 0.0588 | 0.0277
Epoch 157/300, seasonal_2 Loss: 0.0587 | 0.0277
Epoch 158/300, seasonal_2 Loss: 0.0587 | 0.0277
Epoch 159/300, seasonal_2 Loss: 0.0587 | 0.0277
Epoch 160/300, seasonal_2 Loss: 0.0587 | 0.0277
Epoch 161/300, seasonal_2 Loss: 0.0586 | 0.0276
Epoch 162/300, seasonal_2 Loss: 0.0586 | 0.0276
Epoch 163/300, seasonal_2 Loss: 0.0586 | 0.0276
Epoch 164/300, seasonal_2 Loss: 0.0586 | 0.0276
Epoch 165/300, seasonal_2 Loss: 0.0585 | 0.0276
Epoch 166/300, seasonal_2 Loss: 0.0585 | 0.0276
Epoch 167/300, seasonal_2 Loss: 0.0585 | 0.0276
Epoch 168/300, seasonal_2 Loss: 0.0585 | 0.0276
Epoch 169/300, seasonal_2 Loss: 0.0585 | 0.0275
Epoch 170/300, seasonal_2 Loss: 0.0585 | 0.0275
Epoch 171/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 172/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 173/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 174/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 175/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 176/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 177/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 178/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 179/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 180/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 181/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 182/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 183/300, seasonal_2 Loss: 0.0583 | 0.0274
Epoch 184/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 185/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 186/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 187/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 188/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 189/300, seasonal_2 Loss: 0.0582 | 0.0273
Epoch 190/300, seasonal_2 Loss: 0.0582 | 0.0273
Epoch 191/300, seasonal_2 Loss: 0.0582 | 0.0273
Epoch 192/300, seasonal_2 Loss: 0.0582 | 0.0273
Epoch 193/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 194/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 195/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 196/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 197/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 198/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 199/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 200/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 201/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 202/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 203/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 204/300, seasonal_2 Loss: 0.0580 | 0.0273
Epoch 205/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 206/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 207/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 208/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 209/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 210/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 211/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 212/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 213/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 214/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 215/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 216/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 217/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 218/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 219/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 220/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 221/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 222/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 223/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 224/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 225/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 226/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 227/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 228/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 229/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 230/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 231/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 232/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 233/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 234/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 235/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 236/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 237/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 238/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 239/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 240/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 241/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 242/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 243/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 244/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 245/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 246/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 247/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 248/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 249/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 250/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 251/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 252/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 253/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 254/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 255/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 256/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 257/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 258/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 259/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 260/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 261/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 262/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 263/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 264/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 265/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 266/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 267/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 268/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 269/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 270/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 271/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 272/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 273/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 274/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 275/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 276/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 277/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 278/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 279/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 280/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 281/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 282/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 283/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 284/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 285/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 286/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 287/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 288/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 289/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 290/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 291/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 292/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 293/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 294/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 295/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 296/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 297/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 298/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 299/300, seasonal_2 Loss: 0.0578 | 0.0271
Epoch 300/300, seasonal_2 Loss: 0.0578 | 0.0271
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.8173325897575369, 'learning_rate': 1.9028051396510073e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9075947186261523}
Epoch 1/300, seasonal_3 Loss: 1.0396 | 0.6657
Epoch 2/300, seasonal_3 Loss: 0.3959 | 0.3521
Epoch 3/300, seasonal_3 Loss: 0.2601 | 0.2377
Epoch 4/300, seasonal_3 Loss: 0.2145 | 0.1992
Epoch 5/300, seasonal_3 Loss: 0.1941 | 0.1746
Epoch 6/300, seasonal_3 Loss: 0.1798 | 0.1546
Epoch 7/300, seasonal_3 Loss: 0.1682 | 0.1380
Epoch 8/300, seasonal_3 Loss: 0.1586 | 0.1245
Epoch 9/300, seasonal_3 Loss: 0.1509 | 0.1138
Epoch 10/300, seasonal_3 Loss: 0.1447 | 0.1053
Epoch 11/300, seasonal_3 Loss: 0.1397 | 0.0982
Epoch 12/300, seasonal_3 Loss: 0.1356 | 0.0924
Epoch 13/300, seasonal_3 Loss: 0.1323 | 0.0876
Epoch 14/300, seasonal_3 Loss: 0.1298 | 0.0836
Epoch 15/300, seasonal_3 Loss: 0.1277 | 0.0803
Epoch 16/300, seasonal_3 Loss: 0.1260 | 0.0777
Epoch 17/300, seasonal_3 Loss: 0.1246 | 0.0756
Epoch 18/300, seasonal_3 Loss: 0.1232 | 0.0738
Epoch 19/300, seasonal_3 Loss: 0.1220 | 0.0723
Epoch 20/300, seasonal_3 Loss: 0.1208 | 0.0710
Epoch 21/300, seasonal_3 Loss: 0.1196 | 0.0699
Epoch 22/300, seasonal_3 Loss: 0.1185 | 0.0690
Epoch 23/300, seasonal_3 Loss: 0.1175 | 0.0682
Epoch 24/300, seasonal_3 Loss: 0.1163 | 0.0674
Epoch 25/300, seasonal_3 Loss: 0.1154 | 0.0667
Epoch 26/300, seasonal_3 Loss: 0.1145 | 0.0662
Epoch 27/300, seasonal_3 Loss: 0.1136 | 0.0658
Epoch 28/300, seasonal_3 Loss: 0.1127 | 0.0654
Epoch 29/300, seasonal_3 Loss: 0.1119 | 0.0651
Epoch 30/300, seasonal_3 Loss: 0.1111 | 0.0648
Epoch 31/300, seasonal_3 Loss: 0.1103 | 0.0639
Epoch 32/300, seasonal_3 Loss: 0.1096 | 0.0635
Epoch 33/300, seasonal_3 Loss: 0.1089 | 0.0632
Epoch 34/300, seasonal_3 Loss: 0.1083 | 0.0629
Epoch 35/300, seasonal_3 Loss: 0.1076 | 0.0626
Epoch 36/300, seasonal_3 Loss: 0.1070 | 0.0622
Epoch 37/300, seasonal_3 Loss: 0.1064 | 0.0619
Epoch 38/300, seasonal_3 Loss: 0.1058 | 0.0615
Epoch 39/300, seasonal_3 Loss: 0.1052 | 0.0605
Epoch 40/300, seasonal_3 Loss: 0.1047 | 0.0602
Epoch 41/300, seasonal_3 Loss: 0.1042 | 0.0598
Epoch 42/300, seasonal_3 Loss: 0.1038 | 0.0595
Epoch 43/300, seasonal_3 Loss: 0.1033 | 0.0591
Epoch 44/300, seasonal_3 Loss: 0.1028 | 0.0588
Epoch 45/300, seasonal_3 Loss: 0.1024 | 0.0584
Epoch 46/300, seasonal_3 Loss: 0.1019 | 0.0578
Epoch 47/300, seasonal_3 Loss: 0.1015 | 0.0575
Epoch 48/300, seasonal_3 Loss: 0.1011 | 0.0572
Epoch 49/300, seasonal_3 Loss: 0.1007 | 0.0569
Epoch 50/300, seasonal_3 Loss: 0.1004 | 0.0567
Epoch 51/300, seasonal_3 Loss: 0.1000 | 0.0564
Epoch 52/300, seasonal_3 Loss: 0.0996 | 0.0561
Epoch 53/300, seasonal_3 Loss: 0.0993 | 0.0559
Epoch 54/300, seasonal_3 Loss: 0.0989 | 0.0556
Epoch 55/300, seasonal_3 Loss: 0.0985 | 0.0554
Epoch 56/300, seasonal_3 Loss: 0.0982 | 0.0552
Epoch 57/300, seasonal_3 Loss: 0.0979 | 0.0550
Epoch 58/300, seasonal_3 Loss: 0.0976 | 0.0549
Epoch 59/300, seasonal_3 Loss: 0.0973 | 0.0547
Epoch 60/300, seasonal_3 Loss: 0.0970 | 0.0545
Epoch 61/300, seasonal_3 Loss: 0.0966 | 0.0544
Epoch 62/300, seasonal_3 Loss: 0.0963 | 0.0542
Epoch 63/300, seasonal_3 Loss: 0.0960 | 0.0541
Epoch 64/300, seasonal_3 Loss: 0.0958 | 0.0540
Epoch 65/300, seasonal_3 Loss: 0.0955 | 0.0538
Epoch 66/300, seasonal_3 Loss: 0.0952 | 0.0537
Epoch 67/300, seasonal_3 Loss: 0.0949 | 0.0535
Epoch 68/300, seasonal_3 Loss: 0.0946 | 0.0534
Epoch 69/300, seasonal_3 Loss: 0.0943 | 0.0532
Epoch 70/300, seasonal_3 Loss: 0.0941 | 0.0531
Epoch 71/300, seasonal_3 Loss: 0.0938 | 0.0530
Epoch 72/300, seasonal_3 Loss: 0.0936 | 0.0529
Epoch 73/300, seasonal_3 Loss: 0.0933 | 0.0528
Epoch 74/300, seasonal_3 Loss: 0.0931 | 0.0526
Epoch 75/300, seasonal_3 Loss: 0.0928 | 0.0525
Epoch 76/300, seasonal_3 Loss: 0.0925 | 0.0522
Epoch 77/300, seasonal_3 Loss: 0.0923 | 0.0521
Epoch 78/300, seasonal_3 Loss: 0.0921 | 0.0520
Epoch 79/300, seasonal_3 Loss: 0.0918 | 0.0519
Epoch 80/300, seasonal_3 Loss: 0.0916 | 0.0517
Epoch 81/300, seasonal_3 Loss: 0.0914 | 0.0516
Epoch 82/300, seasonal_3 Loss: 0.0912 | 0.0514
Epoch 83/300, seasonal_3 Loss: 0.0909 | 0.0513
Epoch 84/300, seasonal_3 Loss: 0.0907 | 0.0509
Epoch 85/300, seasonal_3 Loss: 0.0905 | 0.0508
Epoch 86/300, seasonal_3 Loss: 0.0903 | 0.0506
Epoch 87/300, seasonal_3 Loss: 0.0901 | 0.0505
Epoch 88/300, seasonal_3 Loss: 0.0899 | 0.0504
Epoch 89/300, seasonal_3 Loss: 0.0897 | 0.0502
Epoch 90/300, seasonal_3 Loss: 0.0895 | 0.0500
Epoch 91/300, seasonal_3 Loss: 0.0892 | 0.0497
Epoch 92/300, seasonal_3 Loss: 0.0891 | 0.0496
Epoch 93/300, seasonal_3 Loss: 0.0889 | 0.0494
Epoch 94/300, seasonal_3 Loss: 0.0887 | 0.0493
Epoch 95/300, seasonal_3 Loss: 0.0885 | 0.0491
Epoch 96/300, seasonal_3 Loss: 0.0883 | 0.0490
Epoch 97/300, seasonal_3 Loss: 0.0881 | 0.0488
Epoch 98/300, seasonal_3 Loss: 0.0879 | 0.0487
Epoch 99/300, seasonal_3 Loss: 0.0878 | 0.0484
Epoch 100/300, seasonal_3 Loss: 0.0876 | 0.0483
Epoch 101/300, seasonal_3 Loss: 0.0874 | 0.0481
Epoch 102/300, seasonal_3 Loss: 0.0873 | 0.0480
Epoch 103/300, seasonal_3 Loss: 0.0871 | 0.0478
Epoch 104/300, seasonal_3 Loss: 0.0869 | 0.0477
Epoch 105/300, seasonal_3 Loss: 0.0868 | 0.0475
Epoch 106/300, seasonal_3 Loss: 0.0866 | 0.0474
Epoch 107/300, seasonal_3 Loss: 0.0865 | 0.0472
Epoch 108/300, seasonal_3 Loss: 0.0863 | 0.0471
Epoch 109/300, seasonal_3 Loss: 0.0862 | 0.0469
Epoch 110/300, seasonal_3 Loss: 0.0860 | 0.0468
Epoch 111/300, seasonal_3 Loss: 0.0859 | 0.0466
Epoch 112/300, seasonal_3 Loss: 0.0857 | 0.0465
Epoch 113/300, seasonal_3 Loss: 0.0856 | 0.0464
Epoch 114/300, seasonal_3 Loss: 0.0854 | 0.0462
Epoch 115/300, seasonal_3 Loss: 0.0853 | 0.0461
Epoch 116/300, seasonal_3 Loss: 0.0852 | 0.0460
Epoch 117/300, seasonal_3 Loss: 0.0851 | 0.0458
Epoch 118/300, seasonal_3 Loss: 0.0849 | 0.0457
Epoch 119/300, seasonal_3 Loss: 0.0848 | 0.0456
Epoch 120/300, seasonal_3 Loss: 0.0847 | 0.0455
Epoch 121/300, seasonal_3 Loss: 0.0846 | 0.0454
Epoch 122/300, seasonal_3 Loss: 0.0845 | 0.0452
Epoch 123/300, seasonal_3 Loss: 0.0844 | 0.0451
Epoch 124/300, seasonal_3 Loss: 0.0843 | 0.0450
Epoch 125/300, seasonal_3 Loss: 0.0842 | 0.0449
Epoch 126/300, seasonal_3 Loss: 0.0841 | 0.0448
Epoch 127/300, seasonal_3 Loss: 0.0840 | 0.0446
Epoch 128/300, seasonal_3 Loss: 0.0839 | 0.0445
Epoch 129/300, seasonal_3 Loss: 0.0838 | 0.0444
Epoch 130/300, seasonal_3 Loss: 0.0837 | 0.0443
Epoch 131/300, seasonal_3 Loss: 0.0836 | 0.0442
Epoch 132/300, seasonal_3 Loss: 0.0835 | 0.0441
Epoch 133/300, seasonal_3 Loss: 0.0834 | 0.0440
Epoch 134/300, seasonal_3 Loss: 0.0834 | 0.0439
Epoch 135/300, seasonal_3 Loss: 0.0833 | 0.0438
Epoch 136/300, seasonal_3 Loss: 0.0832 | 0.0439
Epoch 137/300, seasonal_3 Loss: 0.0831 | 0.0438
Epoch 138/300, seasonal_3 Loss: 0.0830 | 0.0437
Epoch 139/300, seasonal_3 Loss: 0.0828 | 0.0436
Epoch 140/300, seasonal_3 Loss: 0.0827 | 0.0435
Epoch 141/300, seasonal_3 Loss: 0.0826 | 0.0434
Epoch 142/300, seasonal_3 Loss: 0.0825 | 0.0433
Epoch 143/300, seasonal_3 Loss: 0.0824 | 0.0432
Epoch 144/300, seasonal_3 Loss: 0.0823 | 0.0433
Epoch 145/300, seasonal_3 Loss: 0.0822 | 0.0432
Epoch 146/300, seasonal_3 Loss: 0.0821 | 0.0431
Epoch 147/300, seasonal_3 Loss: 0.0820 | 0.0430
Epoch 148/300, seasonal_3 Loss: 0.0819 | 0.0429
Epoch 149/300, seasonal_3 Loss: 0.0818 | 0.0428
Epoch 150/300, seasonal_3 Loss: 0.0817 | 0.0427
Epoch 151/300, seasonal_3 Loss: 0.0816 | 0.0427
Epoch 152/300, seasonal_3 Loss: 0.0815 | 0.0427
Epoch 153/300, seasonal_3 Loss: 0.0814 | 0.0426
Epoch 154/300, seasonal_3 Loss: 0.0813 | 0.0425
Epoch 155/300, seasonal_3 Loss: 0.0813 | 0.0424
Epoch 156/300, seasonal_3 Loss: 0.0812 | 0.0423
Epoch 157/300, seasonal_3 Loss: 0.0811 | 0.0423
Epoch 158/300, seasonal_3 Loss: 0.0811 | 0.0422
Epoch 159/300, seasonal_3 Loss: 0.0810 | 0.0422
Epoch 160/300, seasonal_3 Loss: 0.0809 | 0.0422
Epoch 161/300, seasonal_3 Loss: 0.0809 | 0.0421
Epoch 162/300, seasonal_3 Loss: 0.0808 | 0.0420
Epoch 163/300, seasonal_3 Loss: 0.0807 | 0.0420
Epoch 164/300, seasonal_3 Loss: 0.0807 | 0.0419
Epoch 165/300, seasonal_3 Loss: 0.0806 | 0.0419
Epoch 166/300, seasonal_3 Loss: 0.0805 | 0.0419
Epoch 167/300, seasonal_3 Loss: 0.0805 | 0.0418
Epoch 168/300, seasonal_3 Loss: 0.0804 | 0.0418
Epoch 169/300, seasonal_3 Loss: 0.0804 | 0.0417
Epoch 170/300, seasonal_3 Loss: 0.0803 | 0.0417
Epoch 171/300, seasonal_3 Loss: 0.0803 | 0.0416
Epoch 172/300, seasonal_3 Loss: 0.0802 | 0.0416
Epoch 173/300, seasonal_3 Loss: 0.0802 | 0.0415
Epoch 174/300, seasonal_3 Loss: 0.0801 | 0.0415
Epoch 175/300, seasonal_3 Loss: 0.0801 | 0.0415
Epoch 176/300, seasonal_3 Loss: 0.0800 | 0.0415
Epoch 177/300, seasonal_3 Loss: 0.0800 | 0.0414
Epoch 178/300, seasonal_3 Loss: 0.0800 | 0.0414
Epoch 179/300, seasonal_3 Loss: 0.0799 | 0.0413
Epoch 180/300, seasonal_3 Loss: 0.0799 | 0.0413
Epoch 181/300, seasonal_3 Loss: 0.0798 | 0.0413
Epoch 182/300, seasonal_3 Loss: 0.0798 | 0.0413
Epoch 183/300, seasonal_3 Loss: 0.0797 | 0.0413
Epoch 184/300, seasonal_3 Loss: 0.0797 | 0.0412
Epoch 185/300, seasonal_3 Loss: 0.0797 | 0.0412
Epoch 186/300, seasonal_3 Loss: 0.0796 | 0.0412
Epoch 187/300, seasonal_3 Loss: 0.0796 | 0.0411
Epoch 188/300, seasonal_3 Loss: 0.0796 | 0.0411
Epoch 189/300, seasonal_3 Loss: 0.0795 | 0.0411
Epoch 190/300, seasonal_3 Loss: 0.0795 | 0.0411
Epoch 191/300, seasonal_3 Loss: 0.0794 | 0.0411
Epoch 192/300, seasonal_3 Loss: 0.0794 | 0.0410
Epoch 193/300, seasonal_3 Loss: 0.0794 | 0.0410
Epoch 194/300, seasonal_3 Loss: 0.0793 | 0.0410
Epoch 195/300, seasonal_3 Loss: 0.0793 | 0.0409
Epoch 196/300, seasonal_3 Loss: 0.0793 | 0.0410
Epoch 197/300, seasonal_3 Loss: 0.0792 | 0.0409
Epoch 198/300, seasonal_3 Loss: 0.0792 | 0.0409
Epoch 199/300, seasonal_3 Loss: 0.0792 | 0.0409
Epoch 200/300, seasonal_3 Loss: 0.0791 | 0.0409
Epoch 201/300, seasonal_3 Loss: 0.0791 | 0.0408
Epoch 202/300, seasonal_3 Loss: 0.0791 | 0.0408
Epoch 203/300, seasonal_3 Loss: 0.0791 | 0.0408
Epoch 204/300, seasonal_3 Loss: 0.0790 | 0.0408
Epoch 205/300, seasonal_3 Loss: 0.0790 | 0.0408
Epoch 206/300, seasonal_3 Loss: 0.0790 | 0.0408
Epoch 207/300, seasonal_3 Loss: 0.0789 | 0.0407
Epoch 208/300, seasonal_3 Loss: 0.0789 | 0.0407
Epoch 209/300, seasonal_3 Loss: 0.0789 | 0.0407
Epoch 210/300, seasonal_3 Loss: 0.0789 | 0.0407
Epoch 211/300, seasonal_3 Loss: 0.0788 | 0.0407
Epoch 212/300, seasonal_3 Loss: 0.0788 | 0.0407
Epoch 213/300, seasonal_3 Loss: 0.0788 | 0.0406
Epoch 214/300, seasonal_3 Loss: 0.0788 | 0.0406
Epoch 215/300, seasonal_3 Loss: 0.0787 | 0.0406
Epoch 216/300, seasonal_3 Loss: 0.0787 | 0.0406
Epoch 217/300, seasonal_3 Loss: 0.0787 | 0.0406
Epoch 218/300, seasonal_3 Loss: 0.0787 | 0.0406
Epoch 219/300, seasonal_3 Loss: 0.0786 | 0.0406
Epoch 220/300, seasonal_3 Loss: 0.0786 | 0.0405
Epoch 221/300, seasonal_3 Loss: 0.0786 | 0.0405
Epoch 222/300, seasonal_3 Loss: 0.0786 | 0.0405
Epoch 223/300, seasonal_3 Loss: 0.0785 | 0.0405
Epoch 224/300, seasonal_3 Loss: 0.0785 | 0.0405
Epoch 225/300, seasonal_3 Loss: 0.0785 | 0.0405
Epoch 226/300, seasonal_3 Loss: 0.0785 | 0.0405
Epoch 227/300, seasonal_3 Loss: 0.0785 | 0.0405
Epoch 228/300, seasonal_3 Loss: 0.0784 | 0.0404
Epoch 229/300, seasonal_3 Loss: 0.0784 | 0.0404
Epoch 230/300, seasonal_3 Loss: 0.0784 | 0.0404
Epoch 231/300, seasonal_3 Loss: 0.0784 | 0.0404
Epoch 232/300, seasonal_3 Loss: 0.0784 | 0.0404
Epoch 233/300, seasonal_3 Loss: 0.0783 | 0.0404
Epoch 234/300, seasonal_3 Loss: 0.0783 | 0.0404
Epoch 235/300, seasonal_3 Loss: 0.0783 | 0.0404
Epoch 236/300, seasonal_3 Loss: 0.0783 | 0.0404
Epoch 237/300, seasonal_3 Loss: 0.0783 | 0.0403
Epoch 238/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 239/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 240/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 241/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 242/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 243/300, seasonal_3 Loss: 0.0782 | 0.0403
Epoch 244/300, seasonal_3 Loss: 0.0781 | 0.0403
Epoch 245/300, seasonal_3 Loss: 0.0781 | 0.0403
Epoch 246/300, seasonal_3 Loss: 0.0781 | 0.0403
Epoch 247/300, seasonal_3 Loss: 0.0781 | 0.0402
Epoch 248/300, seasonal_3 Loss: 0.0781 | 0.0402
Epoch 249/300, seasonal_3 Loss: 0.0781 | 0.0402
Epoch 250/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 251/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 252/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 253/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 254/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 255/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 256/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 257/300, seasonal_3 Loss: 0.0780 | 0.0402
Epoch 258/300, seasonal_3 Loss: 0.0779 | 0.0402
Epoch 259/300, seasonal_3 Loss: 0.0779 | 0.0402
Epoch 260/300, seasonal_3 Loss: 0.0779 | 0.0402
Epoch 261/300, seasonal_3 Loss: 0.0779 | 0.0401
Epoch 262/300, seasonal_3 Loss: 0.0779 | 0.0401
Epoch 263/300, seasonal_3 Loss: 0.0779 | 0.0401
Epoch 264/300, seasonal_3 Loss: 0.0779 | 0.0401
Epoch 265/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 266/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 267/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 268/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 269/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 270/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 271/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 272/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 273/300, seasonal_3 Loss: 0.0778 | 0.0401
Epoch 274/300, seasonal_3 Loss: 0.0777 | 0.0401
Epoch 275/300, seasonal_3 Loss: 0.0777 | 0.0401
Epoch 276/300, seasonal_3 Loss: 0.0777 | 0.0401
Epoch 277/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 278/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 279/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 280/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 281/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 282/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 283/300, seasonal_3 Loss: 0.0777 | 0.0400
Epoch 284/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 285/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 286/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 287/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 288/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 289/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 290/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 291/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 292/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 293/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 294/300, seasonal_3 Loss: 0.0776 | 0.0400
Epoch 295/300, seasonal_3 Loss: 0.0775 | 0.0400
Epoch 296/300, seasonal_3 Loss: 0.0775 | 0.0400
Epoch 297/300, seasonal_3 Loss: 0.0775 | 0.0399
Epoch 298/300, seasonal_3 Loss: 0.0775 | 0.0399
Epoch 299/300, seasonal_3 Loss: 0.0775 | 0.0399
Epoch 300/300, seasonal_3 Loss: 0.0775 | 0.0399
Training resid component with params: {'observation_period_num': 21, 'train_rates': 0.9082756192020903, 'learning_rate': 0.0006038465728526143, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8928506173871197}
Epoch 1/300, resid Loss: 0.6713 | 0.1943
Epoch 2/300, resid Loss: 0.2461 | 0.2001
Epoch 3/300, resid Loss: 0.1863 | 0.1294
Epoch 4/300, resid Loss: 0.1954 | 0.1169
Epoch 5/300, resid Loss: 0.2546 | 0.3163
Epoch 6/300, resid Loss: 0.2894 | 0.2106
Epoch 7/300, resid Loss: 0.2191 | 0.1640
Epoch 8/300, resid Loss: 0.1932 | 0.1373
Epoch 9/300, resid Loss: 0.1598 | 0.1452
Epoch 10/300, resid Loss: 0.1322 | 0.0796
Epoch 11/300, resid Loss: 0.1330 | 0.0846
Epoch 12/300, resid Loss: 0.1272 | 0.1313
Epoch 13/300, resid Loss: 0.1260 | 0.0765
Epoch 14/300, resid Loss: 0.1238 | 0.0727
Epoch 15/300, resid Loss: 0.1159 | 0.0643
Epoch 16/300, resid Loss: 0.1308 | 0.0861
Epoch 17/300, resid Loss: 0.1123 | 0.0641
Epoch 18/300, resid Loss: 0.1224 | 0.1145
Epoch 19/300, resid Loss: 0.1073 | 0.0639
Epoch 20/300, resid Loss: 0.1082 | 0.0619
Epoch 21/300, resid Loss: 0.1071 | 0.0651
Epoch 22/300, resid Loss: 0.1051 | 0.0855
Epoch 23/300, resid Loss: 0.0978 | 0.0607
Epoch 24/300, resid Loss: 0.0945 | 0.0539
Epoch 25/300, resid Loss: 0.0941 | 0.0560
Epoch 26/300, resid Loss: 0.0911 | 0.0519
Epoch 27/300, resid Loss: 0.0902 | 0.0569
Epoch 28/300, resid Loss: 0.0896 | 0.0616
Epoch 29/300, resid Loss: 0.0884 | 0.0556
Epoch 30/300, resid Loss: 0.0873 | 0.0499
Epoch 31/300, resid Loss: 0.0869 | 0.0492
Epoch 32/300, resid Loss: 0.0869 | 0.0499
Epoch 33/300, resid Loss: 0.0854 | 0.0482
Epoch 34/300, resid Loss: 0.0856 | 0.0552
Epoch 35/300, resid Loss: 0.0865 | 0.0706
Epoch 36/300, resid Loss: 0.0856 | 0.0525
Epoch 37/300, resid Loss: 0.0838 | 0.0487
Epoch 38/300, resid Loss: 0.0830 | 0.0468
Epoch 39/300, resid Loss: 0.0823 | 0.0459
Epoch 40/300, resid Loss: 0.0814 | 0.0478
Epoch 41/300, resid Loss: 0.0813 | 0.0524
Epoch 42/300, resid Loss: 0.0805 | 0.0473
Epoch 43/300, resid Loss: 0.0800 | 0.0444
Epoch 44/300, resid Loss: 0.0800 | 0.0442
Epoch 45/300, resid Loss: 0.0796 | 0.0457
Epoch 46/300, resid Loss: 0.0795 | 0.0497
Epoch 47/300, resid Loss: 0.0790 | 0.0476
Epoch 48/300, resid Loss: 0.0780 | 0.0450
Epoch 49/300, resid Loss: 0.0775 | 0.0445
Epoch 50/300, resid Loss: 0.0774 | 0.0442
Epoch 51/300, resid Loss: 0.0765 | 0.0438
Epoch 52/300, resid Loss: 0.0759 | 0.0424
Epoch 53/300, resid Loss: 0.0753 | 0.0414
Epoch 54/300, resid Loss: 0.0748 | 0.0413
Epoch 55/300, resid Loss: 0.0744 | 0.0416
Epoch 56/300, resid Loss: 0.0740 | 0.0419
Epoch 57/300, resid Loss: 0.0735 | 0.0408
Epoch 58/300, resid Loss: 0.0729 | 0.0400
Epoch 59/300, resid Loss: 0.0724 | 0.0397
Epoch 60/300, resid Loss: 0.0721 | 0.0398
Epoch 61/300, resid Loss: 0.0717 | 0.0393
Epoch 62/300, resid Loss: 0.0714 | 0.0387
Epoch 63/300, resid Loss: 0.0711 | 0.0383
Epoch 64/300, resid Loss: 0.0709 | 0.0381
Epoch 65/300, resid Loss: 0.0707 | 0.0378
Epoch 66/300, resid Loss: 0.0706 | 0.0378
Epoch 67/300, resid Loss: 0.0705 | 0.0378
Epoch 68/300, resid Loss: 0.0705 | 0.0376
Epoch 69/300, resid Loss: 0.0704 | 0.0371
Epoch 70/300, resid Loss: 0.0703 | 0.0366
Epoch 71/300, resid Loss: 0.0697 | 0.0364
Epoch 72/300, resid Loss: 0.0693 | 0.0364
Epoch 73/300, resid Loss: 0.0692 | 0.0365
Epoch 74/300, resid Loss: 0.0693 | 0.0363
Epoch 75/300, resid Loss: 0.0690 | 0.0363
Epoch 76/300, resid Loss: 0.0687 | 0.0361
Epoch 77/300, resid Loss: 0.0684 | 0.0354
Epoch 78/300, resid Loss: 0.0682 | 0.0349
Epoch 79/300, resid Loss: 0.0680 | 0.0347
Epoch 80/300, resid Loss: 0.0679 | 0.0346
Epoch 81/300, resid Loss: 0.0676 | 0.0346
Epoch 82/300, resid Loss: 0.0673 | 0.0345
Epoch 83/300, resid Loss: 0.0671 | 0.0345
Epoch 84/300, resid Loss: 0.0670 | 0.0343
Epoch 85/300, resid Loss: 0.0668 | 0.0340
Epoch 86/300, resid Loss: 0.0667 | 0.0339
Epoch 87/300, resid Loss: 0.0665 | 0.0338
Epoch 88/300, resid Loss: 0.0664 | 0.0336
Epoch 89/300, resid Loss: 0.0662 | 0.0336
Epoch 90/300, resid Loss: 0.0661 | 0.0335
Epoch 91/300, resid Loss: 0.0660 | 0.0334
Epoch 92/300, resid Loss: 0.0659 | 0.0333
Epoch 93/300, resid Loss: 0.0657 | 0.0332
Epoch 94/300, resid Loss: 0.0656 | 0.0331
Epoch 95/300, resid Loss: 0.0655 | 0.0330
Epoch 96/300, resid Loss: 0.0654 | 0.0329
Epoch 97/300, resid Loss: 0.0653 | 0.0329
Epoch 98/300, resid Loss: 0.0652 | 0.0328
Epoch 99/300, resid Loss: 0.0651 | 0.0327
Epoch 100/300, resid Loss: 0.0650 | 0.0326
Epoch 101/300, resid Loss: 0.0649 | 0.0326
Epoch 102/300, resid Loss: 0.0648 | 0.0325
Epoch 103/300, resid Loss: 0.0647 | 0.0325
Epoch 104/300, resid Loss: 0.0646 | 0.0324
Epoch 105/300, resid Loss: 0.0645 | 0.0323
Epoch 106/300, resid Loss: 0.0644 | 0.0323
Epoch 107/300, resid Loss: 0.0643 | 0.0322
Epoch 108/300, resid Loss: 0.0643 | 0.0322
Epoch 109/300, resid Loss: 0.0642 | 0.0321
Epoch 110/300, resid Loss: 0.0641 | 0.0321
Epoch 111/300, resid Loss: 0.0640 | 0.0320
Epoch 112/300, resid Loss: 0.0640 | 0.0320
Epoch 113/300, resid Loss: 0.0639 | 0.0320
Epoch 114/300, resid Loss: 0.0638 | 0.0319
Epoch 115/300, resid Loss: 0.0638 | 0.0319
Epoch 116/300, resid Loss: 0.0637 | 0.0318
Epoch 117/300, resid Loss: 0.0636 | 0.0318
Epoch 118/300, resid Loss: 0.0636 | 0.0317
Epoch 119/300, resid Loss: 0.0635 | 0.0317
Epoch 120/300, resid Loss: 0.0635 | 0.0317
Epoch 121/300, resid Loss: 0.0634 | 0.0317
Epoch 122/300, resid Loss: 0.0634 | 0.0316
Epoch 123/300, resid Loss: 0.0633 | 0.0316
Epoch 124/300, resid Loss: 0.0633 | 0.0316
Epoch 125/300, resid Loss: 0.0632 | 0.0315
Epoch 126/300, resid Loss: 0.0632 | 0.0315
Epoch 127/300, resid Loss: 0.0631 | 0.0315
Epoch 128/300, resid Loss: 0.0631 | 0.0314
Epoch 129/300, resid Loss: 0.0630 | 0.0314
Epoch 130/300, resid Loss: 0.0630 | 0.0314
Epoch 131/300, resid Loss: 0.0629 | 0.0314
Epoch 132/300, resid Loss: 0.0629 | 0.0314
Epoch 133/300, resid Loss: 0.0629 | 0.0313
Epoch 134/300, resid Loss: 0.0628 | 0.0313
Epoch 135/300, resid Loss: 0.0628 | 0.0313
Epoch 136/300, resid Loss: 0.0627 | 0.0313
Epoch 137/300, resid Loss: 0.0627 | 0.0312
Epoch 138/300, resid Loss: 0.0627 | 0.0312
Epoch 139/300, resid Loss: 0.0626 | 0.0312
Epoch 140/300, resid Loss: 0.0626 | 0.0312
Epoch 141/300, resid Loss: 0.0626 | 0.0312
Epoch 142/300, resid Loss: 0.0625 | 0.0312
Epoch 143/300, resid Loss: 0.0625 | 0.0311
Epoch 144/300, resid Loss: 0.0625 | 0.0311
Epoch 145/300, resid Loss: 0.0624 | 0.0311
Epoch 146/300, resid Loss: 0.0624 | 0.0311
Epoch 147/300, resid Loss: 0.0624 | 0.0311
Epoch 148/300, resid Loss: 0.0624 | 0.0311
Epoch 149/300, resid Loss: 0.0623 | 0.0311
Epoch 150/300, resid Loss: 0.0623 | 0.0310
Epoch 151/300, resid Loss: 0.0623 | 0.0310
Epoch 152/300, resid Loss: 0.0623 | 0.0310
Epoch 153/300, resid Loss: 0.0622 | 0.0310
Epoch 154/300, resid Loss: 0.0622 | 0.0310
Epoch 155/300, resid Loss: 0.0622 | 0.0310
Epoch 156/300, resid Loss: 0.0622 | 0.0310
Epoch 157/300, resid Loss: 0.0622 | 0.0310
Epoch 158/300, resid Loss: 0.0621 | 0.0309
Epoch 159/300, resid Loss: 0.0621 | 0.0309
Epoch 160/300, resid Loss: 0.0621 | 0.0309
Epoch 161/300, resid Loss: 0.0621 | 0.0309
Epoch 162/300, resid Loss: 0.0621 | 0.0309
Epoch 163/300, resid Loss: 0.0620 | 0.0309
Epoch 164/300, resid Loss: 0.0620 | 0.0309
Epoch 165/300, resid Loss: 0.0620 | 0.0309
Epoch 166/300, resid Loss: 0.0620 | 0.0309
Epoch 167/300, resid Loss: 0.0620 | 0.0309
Epoch 168/300, resid Loss: 0.0620 | 0.0309
Epoch 169/300, resid Loss: 0.0619 | 0.0308
Epoch 170/300, resid Loss: 0.0619 | 0.0308
Epoch 171/300, resid Loss: 0.0619 | 0.0308
Epoch 172/300, resid Loss: 0.0619 | 0.0308
Epoch 173/300, resid Loss: 0.0619 | 0.0308
Epoch 174/300, resid Loss: 0.0619 | 0.0308
Epoch 175/300, resid Loss: 0.0619 | 0.0308
Epoch 176/300, resid Loss: 0.0619 | 0.0308
Epoch 177/300, resid Loss: 0.0618 | 0.0308
Epoch 178/300, resid Loss: 0.0618 | 0.0308
Epoch 179/300, resid Loss: 0.0618 | 0.0308
Epoch 180/300, resid Loss: 0.0618 | 0.0308
Epoch 181/300, resid Loss: 0.0618 | 0.0308
Epoch 182/300, resid Loss: 0.0618 | 0.0308
Epoch 183/300, resid Loss: 0.0618 | 0.0308
Epoch 184/300, resid Loss: 0.0618 | 0.0308
Epoch 185/300, resid Loss: 0.0617 | 0.0307
Epoch 186/300, resid Loss: 0.0617 | 0.0307
Epoch 187/300, resid Loss: 0.0617 | 0.0307
Epoch 188/300, resid Loss: 0.0617 | 0.0307
Epoch 189/300, resid Loss: 0.0617 | 0.0307
Epoch 190/300, resid Loss: 0.0617 | 0.0307
Epoch 191/300, resid Loss: 0.0617 | 0.0307
Epoch 192/300, resid Loss: 0.0617 | 0.0307
Epoch 193/300, resid Loss: 0.0617 | 0.0307
Epoch 194/300, resid Loss: 0.0617 | 0.0307
Epoch 195/300, resid Loss: 0.0617 | 0.0307
Epoch 196/300, resid Loss: 0.0617 | 0.0307
Epoch 197/300, resid Loss: 0.0616 | 0.0307
Epoch 198/300, resid Loss: 0.0616 | 0.0307
Epoch 199/300, resid Loss: 0.0616 | 0.0307
Epoch 200/300, resid Loss: 0.0616 | 0.0307
Epoch 201/300, resid Loss: 0.0616 | 0.0307
Epoch 202/300, resid Loss: 0.0616 | 0.0307
Epoch 203/300, resid Loss: 0.0616 | 0.0307
Epoch 204/300, resid Loss: 0.0616 | 0.0307
Epoch 205/300, resid Loss: 0.0616 | 0.0307
Epoch 206/300, resid Loss: 0.0616 | 0.0307
Epoch 207/300, resid Loss: 0.0616 | 0.0307
Epoch 208/300, resid Loss: 0.0616 | 0.0307
Epoch 209/300, resid Loss: 0.0616 | 0.0307
Epoch 210/300, resid Loss: 0.0616 | 0.0307
Epoch 211/300, resid Loss: 0.0616 | 0.0307
Epoch 212/300, resid Loss: 0.0616 | 0.0307
Epoch 213/300, resid Loss: 0.0615 | 0.0306
Epoch 214/300, resid Loss: 0.0615 | 0.0306
Epoch 215/300, resid Loss: 0.0615 | 0.0306
Epoch 216/300, resid Loss: 0.0615 | 0.0306
Epoch 217/300, resid Loss: 0.0615 | 0.0306
Epoch 218/300, resid Loss: 0.0615 | 0.0306
Epoch 219/300, resid Loss: 0.0615 | 0.0306
Epoch 220/300, resid Loss: 0.0615 | 0.0306
Epoch 221/300, resid Loss: 0.0615 | 0.0306
Epoch 222/300, resid Loss: 0.0615 | 0.0306
Epoch 223/300, resid Loss: 0.0615 | 0.0306
Epoch 224/300, resid Loss: 0.0615 | 0.0306
Epoch 225/300, resid Loss: 0.0615 | 0.0306
Epoch 226/300, resid Loss: 0.0615 | 0.0306
Epoch 227/300, resid Loss: 0.0615 | 0.0306
Epoch 228/300, resid Loss: 0.0615 | 0.0306
Epoch 229/300, resid Loss: 0.0615 | 0.0306
Epoch 230/300, resid Loss: 0.0615 | 0.0306
Epoch 231/300, resid Loss: 0.0615 | 0.0306
Epoch 232/300, resid Loss: 0.0615 | 0.0306
Epoch 233/300, resid Loss: 0.0615 | 0.0306
Epoch 234/300, resid Loss: 0.0615 | 0.0306
Epoch 235/300, resid Loss: 0.0615 | 0.0306
Epoch 236/300, resid Loss: 0.0615 | 0.0306
Epoch 237/300, resid Loss: 0.0615 | 0.0306
Epoch 238/300, resid Loss: 0.0615 | 0.0306
Epoch 239/300, resid Loss: 0.0615 | 0.0306
Epoch 240/300, resid Loss: 0.0615 | 0.0306
Epoch 241/300, resid Loss: 0.0614 | 0.0306
Epoch 242/300, resid Loss: 0.0614 | 0.0306
Epoch 243/300, resid Loss: 0.0614 | 0.0306
Epoch 244/300, resid Loss: 0.0614 | 0.0306
Epoch 245/300, resid Loss: 0.0614 | 0.0306
Epoch 246/300, resid Loss: 0.0614 | 0.0306
Epoch 247/300, resid Loss: 0.0614 | 0.0306
Epoch 248/300, resid Loss: 0.0614 | 0.0306
Epoch 249/300, resid Loss: 0.0614 | 0.0306
Epoch 250/300, resid Loss: 0.0614 | 0.0306
Epoch 251/300, resid Loss: 0.0614 | 0.0306
Epoch 252/300, resid Loss: 0.0614 | 0.0306
Epoch 253/300, resid Loss: 0.0614 | 0.0306
Epoch 254/300, resid Loss: 0.0614 | 0.0306
Epoch 255/300, resid Loss: 0.0614 | 0.0306
Epoch 256/300, resid Loss: 0.0614 | 0.0306
Epoch 257/300, resid Loss: 0.0614 | 0.0306
Epoch 258/300, resid Loss: 0.0614 | 0.0306
Epoch 259/300, resid Loss: 0.0614 | 0.0306
Epoch 260/300, resid Loss: 0.0614 | 0.0306
Epoch 261/300, resid Loss: 0.0614 | 0.0306
Epoch 262/300, resid Loss: 0.0614 | 0.0306
Epoch 263/300, resid Loss: 0.0614 | 0.0306
Epoch 264/300, resid Loss: 0.0614 | 0.0306
Epoch 265/300, resid Loss: 0.0614 | 0.0306
Epoch 266/300, resid Loss: 0.0614 | 0.0306
Epoch 267/300, resid Loss: 0.0614 | 0.0306
Epoch 268/300, resid Loss: 0.0614 | 0.0306
Epoch 269/300, resid Loss: 0.0614 | 0.0306
Epoch 270/300, resid Loss: 0.0614 | 0.0306
Epoch 271/300, resid Loss: 0.0614 | 0.0306
Epoch 272/300, resid Loss: 0.0614 | 0.0306
Epoch 273/300, resid Loss: 0.0614 | 0.0306
Epoch 274/300, resid Loss: 0.0614 | 0.0306
Epoch 275/300, resid Loss: 0.0614 | 0.0306
Epoch 276/300, resid Loss: 0.0614 | 0.0306
Epoch 277/300, resid Loss: 0.0614 | 0.0306
Epoch 278/300, resid Loss: 0.0614 | 0.0306
Epoch 279/300, resid Loss: 0.0614 | 0.0306
Epoch 280/300, resid Loss: 0.0614 | 0.0306
Epoch 281/300, resid Loss: 0.0614 | 0.0306
Epoch 282/300, resid Loss: 0.0614 | 0.0306
Epoch 283/300, resid Loss: 0.0614 | 0.0306
Epoch 284/300, resid Loss: 0.0614 | 0.0306
Epoch 285/300, resid Loss: 0.0614 | 0.0306
Epoch 286/300, resid Loss: 0.0614 | 0.0306
Epoch 287/300, resid Loss: 0.0614 | 0.0306
Epoch 288/300, resid Loss: 0.0614 | 0.0306
Epoch 289/300, resid Loss: 0.0614 | 0.0306
Epoch 290/300, resid Loss: 0.0614 | 0.0306
Epoch 291/300, resid Loss: 0.0614 | 0.0306
Epoch 292/300, resid Loss: 0.0614 | 0.0306
Epoch 293/300, resid Loss: 0.0614 | 0.0306
Epoch 294/300, resid Loss: 0.0614 | 0.0306
Epoch 295/300, resid Loss: 0.0614 | 0.0306
Epoch 296/300, resid Loss: 0.0614 | 0.0306
Epoch 297/300, resid Loss: 0.0614 | 0.0306
Epoch 298/300, resid Loss: 0.0614 | 0.0306
Epoch 299/300, resid Loss: 0.0614 | 0.0306
Epoch 300/300, resid Loss: 0.0614 | 0.0306
Runtime (seconds): 1640.0716753005981
0.0005448001859417389
[160.392]
[-4.0522866]
[1.4757434]
[16.116926]
[1.5867848]
[19.91457]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 154.4121744632721
RMSE: 12.42626953125
MAE: 12.42626953125
R-squared: nan
[195.43373]
