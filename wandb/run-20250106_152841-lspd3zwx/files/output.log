[32m[I 2025-01-06 15:28:43,654][0m A new study created in memory with name: no-name-52a15cc8-9019-48a8-adee-eedcb8858016[0m
[32m[I 2025-01-06 15:29:52,830][0m Trial 0 finished with value: 1.4210758356796527 and parameters: {'observation_period_num': 42, 'train_rates': 0.6815075237061324, 'learning_rate': 1.0148158583110431e-06, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9412423747901951}. Best is trial 0 with value: 1.4210758356796527.[0m
[32m[I 2025-01-06 15:35:05,856][0m Trial 1 finished with value: 0.3738384427889338 and parameters: {'observation_period_num': 230, 'train_rates': 0.8057955504248061, 'learning_rate': 0.0007098186168937409, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8181964539573087}. Best is trial 1 with value: 0.3738384427889338.[0m
[32m[I 2025-01-06 15:36:47,926][0m Trial 2 finished with value: 1.0162978399625295 and parameters: {'observation_period_num': 90, 'train_rates': 0.6929542696713473, 'learning_rate': 2.3901699797933438e-05, 'batch_size': 93, 'step_size': 4, 'gamma': 0.7911790913287821}. Best is trial 1 with value: 0.3738384427889338.[0m
[32m[I 2025-01-06 15:38:59,664][0m Trial 3 finished with value: 0.3012929990482474 and parameters: {'observation_period_num': 78, 'train_rates': 0.8852423233148343, 'learning_rate': 6.633602927399587e-06, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8864871427321024}. Best is trial 3 with value: 0.3012929990482474.[0m
[32m[I 2025-01-06 15:43:30,237][0m Trial 4 finished with value: 0.6823517191536435 and parameters: {'observation_period_num': 202, 'train_rates': 0.7946813270610467, 'learning_rate': 0.00028979779243558216, 'batch_size': 159, 'step_size': 5, 'gamma': 0.9899542688822098}. Best is trial 3 with value: 0.3012929990482474.[0m
[32m[I 2025-01-06 15:45:44,036][0m Trial 5 finished with value: 1.3688098788261414 and parameters: {'observation_period_num': 114, 'train_rates': 0.7553116920315198, 'learning_rate': 1.961809187079444e-06, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9195252082047873}. Best is trial 3 with value: 0.3012929990482474.[0m
[32m[I 2025-01-06 15:47:44,419][0m Trial 6 finished with value: 0.12606445172658334 and parameters: {'observation_period_num': 22, 'train_rates': 0.9337998224595305, 'learning_rate': 0.00018290503256161632, 'batch_size': 33, 'step_size': 11, 'gamma': 0.7541170664344649}. Best is trial 6 with value: 0.12606445172658334.[0m
[32m[I 2025-01-06 15:52:04,228][0m Trial 7 finished with value: 0.40346939696205986 and parameters: {'observation_period_num': 192, 'train_rates': 0.8635204711567877, 'learning_rate': 1.3302872883513795e-05, 'batch_size': 112, 'step_size': 13, 'gamma': 0.8300798788636938}. Best is trial 6 with value: 0.12606445172658334.[0m
[32m[I 2025-01-06 15:57:42,873][0m Trial 8 finished with value: 0.21814287068732716 and parameters: {'observation_period_num': 226, 'train_rates': 0.9139490740602301, 'learning_rate': 4.79195412760491e-05, 'batch_size': 97, 'step_size': 8, 'gamma': 0.7574751496991151}. Best is trial 6 with value: 0.12606445172658334.[0m
[32m[I 2025-01-06 16:00:38,557][0m Trial 9 finished with value: 0.9634168352817465 and parameters: {'observation_period_num': 148, 'train_rates': 0.7100500992101076, 'learning_rate': 1.2321405071486446e-05, 'batch_size': 80, 'step_size': 11, 'gamma': 0.8577715552524654}. Best is trial 6 with value: 0.12606445172658334.[0m
[32m[I 2025-01-06 16:04:13,828][0m Trial 10 finished with value: 0.07461922251608934 and parameters: {'observation_period_num': 11, 'train_rates': 0.9395983545813996, 'learning_rate': 0.0001147586380100415, 'batch_size': 18, 'step_size': 15, 'gamma': 0.750245889877086}. Best is trial 10 with value: 0.07461922251608934.[0m
[32m[I 2025-01-06 16:08:00,937][0m Trial 11 finished with value: 0.05784338359770022 and parameters: {'observation_period_num': 9, 'train_rates': 0.9807295638781364, 'learning_rate': 0.00012465889849280575, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7601038854044136}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:08:28,466][0m Trial 12 finished with value: 0.11529805511236191 and parameters: {'observation_period_num': 17, 'train_rates': 0.9770950626633681, 'learning_rate': 8.832963993091715e-05, 'batch_size': 252, 'step_size': 15, 'gamma': 0.7911016848232916}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:12:39,632][0m Trial 13 finished with value: 0.10888233732792639 and parameters: {'observation_period_num': 59, 'train_rates': 0.9895691561582941, 'learning_rate': 0.00014197516819877928, 'batch_size': 17, 'step_size': 15, 'gamma': 0.784449863198685}. Best is trial 11 with value: 0.05784338359770022.[0m
Early stopping at epoch 60
[32m[I 2025-01-06 16:13:11,755][0m Trial 14 finished with value: 1.1101734240849812 and parameters: {'observation_period_num': 5, 'train_rates': 0.6009092356717254, 'learning_rate': 0.00047849441677360943, 'batch_size': 60, 'step_size': 1, 'gamma': 0.7550884952587952}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:16:34,514][0m Trial 15 finished with value: 0.13896825909614563 and parameters: {'observation_period_num': 143, 'train_rates': 0.9471737691998016, 'learning_rate': 8.429551545020096e-05, 'batch_size': 160, 'step_size': 14, 'gamma': 0.8256797511215174}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:17:47,757][0m Trial 16 finished with value: 0.9142918981899872 and parameters: {'observation_period_num': 58, 'train_rates': 0.8751553660216277, 'learning_rate': 0.0009282528397528925, 'batch_size': 130, 'step_size': 10, 'gamma': 0.865875472723984}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:20:10,002][0m Trial 17 finished with value: 0.3194795338697331 and parameters: {'observation_period_num': 109, 'train_rates': 0.8368626385487873, 'learning_rate': 3.52101793281818e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.7830884364846356}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:24:02,815][0m Trial 18 finished with value: 0.12114495507934514 and parameters: {'observation_period_num': 32, 'train_rates': 0.9418858471417592, 'learning_rate': 0.0002751690926768163, 'batch_size': 18, 'step_size': 9, 'gamma': 0.8074631650654118}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:25:26,222][0m Trial 19 finished with value: 0.20624308038007963 and parameters: {'observation_period_num': 65, 'train_rates': 0.9011143864702217, 'learning_rate': 7.379568172607467e-05, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8456756719874805}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:27:04,550][0m Trial 20 finished with value: 0.13015842990259105 and parameters: {'observation_period_num': 36, 'train_rates': 0.9698361457200338, 'learning_rate': 0.0001474452430352405, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9005136151459441}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:31:32,414][0m Trial 21 finished with value: 0.09860676013761097 and parameters: {'observation_period_num': 51, 'train_rates': 0.984664460786329, 'learning_rate': 0.00016249855000710262, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7746673492582938}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:32:42,113][0m Trial 22 finished with value: 0.13888803124427795 and parameters: {'observation_period_num': 6, 'train_rates': 0.985945295846803, 'learning_rate': 0.00030366283631129106, 'batch_size': 68, 'step_size': 14, 'gamma': 0.7703849662525477}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:37:36,268][0m Trial 23 finished with value: 0.10802148569088715 and parameters: {'observation_period_num': 43, 'train_rates': 0.9286895647702712, 'learning_rate': 5.7015080515299914e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8033955262107708}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:39:39,417][0m Trial 24 finished with value: 0.11609457389716685 and parameters: {'observation_period_num': 80, 'train_rates': 0.9525793796354016, 'learning_rate': 0.00015108339421433142, 'batch_size': 41, 'step_size': 15, 'gamma': 0.7720572632623527}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:40:40,888][0m Trial 25 finished with value: 0.2770437942445278 and parameters: {'observation_period_num': 19, 'train_rates': 0.8469434042156987, 'learning_rate': 2.8424286109349294e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.7509535906820025}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:42:40,638][0m Trial 26 finished with value: 0.3553140385843375 and parameters: {'observation_period_num': 45, 'train_rates': 0.9068180960846366, 'learning_rate': 0.0003832779385568708, 'batch_size': 35, 'step_size': 12, 'gamma': 0.7668656223878638}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:44:53,611][0m Trial 27 finished with value: 0.12798060476779938 and parameters: {'observation_period_num': 97, 'train_rates': 0.9595446665415039, 'learning_rate': 0.00012148270801542457, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8061001360479273}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:45:22,977][0m Trial 28 finished with value: 0.11367005558058434 and parameters: {'observation_period_num': 6, 'train_rates': 0.9184011789405944, 'learning_rate': 0.0002463792929569637, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8450101852955881}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:46:30,822][0m Trial 29 finished with value: 1.1956097542981565 and parameters: {'observation_period_num': 56, 'train_rates': 0.6019444955695021, 'learning_rate': 0.0005716860670865739, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9561718273590298}. Best is trial 11 with value: 0.05784338359770022.[0m
Early stopping at epoch 51
[32m[I 2025-01-06 16:46:59,961][0m Trial 30 finished with value: 1.0008466060688432 and parameters: {'observation_period_num': 30, 'train_rates': 0.8176638625637362, 'learning_rate': 1.655699625180184e-05, 'batch_size': 73, 'step_size': 1, 'gamma': 0.7773254007801365}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:50:49,547][0m Trial 31 finished with value: 0.10600350063580732 and parameters: {'observation_period_num': 46, 'train_rates': 0.9333356534306635, 'learning_rate': 5.5765794327538355e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8013924170868085}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:53:29,546][0m Trial 32 finished with value: 0.10509771096985787 and parameters: {'observation_period_num': 46, 'train_rates': 0.956312803411138, 'learning_rate': 4.416055695025582e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.8019464271471658}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:55:43,075][0m Trial 33 finished with value: 0.12588906901724198 and parameters: {'observation_period_num': 73, 'train_rates': 0.9649256627650427, 'learning_rate': 0.00010823373323182559, 'batch_size': 33, 'step_size': 15, 'gamma': 0.7655120516827199}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:57:08,865][0m Trial 34 finished with value: 0.16002391902350332 and parameters: {'observation_period_num': 31, 'train_rates': 0.8910622676532514, 'learning_rate': 3.864842080336535e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7901240602504068}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 16:59:48,871][0m Trial 35 finished with value: 0.22216785103082656 and parameters: {'observation_period_num': 91, 'train_rates': 0.989772745053869, 'learning_rate': 4.016663574330865e-06, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8257931794159427}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:00:17,486][0m Trial 36 finished with value: 0.5207591963770949 and parameters: {'observation_period_num': 21, 'train_rates': 0.7793040496228136, 'learning_rate': 0.00020462317066085206, 'batch_size': 191, 'step_size': 15, 'gamma': 0.8139816502278107}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:03:46,023][0m Trial 37 finished with value: 0.1144667143417784 and parameters: {'observation_period_num': 129, 'train_rates': 0.9575187199080784, 'learning_rate': 2.4480845582726383e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8857121512334979}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:09:16,055][0m Trial 38 finished with value: 0.9345144138001559 and parameters: {'observation_period_num': 252, 'train_rates': 0.7264061451432133, 'learning_rate': 7.566364304856188e-06, 'batch_size': 98, 'step_size': 13, 'gamma': 0.7782345440859921}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:12:35,858][0m Trial 39 finished with value: 1.7314757663436857 and parameters: {'observation_period_num': 173, 'train_rates': 0.6529364601898708, 'learning_rate': 1.0089437300792986e-06, 'batch_size': 53, 'step_size': 3, 'gamma': 0.764276268228008}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:14:12,663][0m Trial 40 finished with value: 0.31117503372035493 and parameters: {'observation_period_num': 73, 'train_rates': 0.8664670818094996, 'learning_rate': 6.848566667680131e-05, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7975589546454833}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:17:15,144][0m Trial 41 finished with value: 0.10733212279560773 and parameters: {'observation_period_num': 48, 'train_rates': 0.9370327749328143, 'learning_rate': 4.77115063893192e-05, 'batch_size': 22, 'step_size': 12, 'gamma': 0.7963521121952795}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:18:53,456][0m Trial 42 finished with value: 0.1548002357365655 and parameters: {'observation_period_num': 46, 'train_rates': 0.916316889070193, 'learning_rate': 1.8639059294557422e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.7500605739827051}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:21:28,797][0m Trial 43 finished with value: 0.07664576917886734 and parameters: {'observation_period_num': 16, 'train_rates': 0.9692343777243224, 'learning_rate': 9.411087560572407e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.7824634455315309}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:23:37,334][0m Trial 44 finished with value: 0.08161589006582896 and parameters: {'observation_period_num': 15, 'train_rates': 0.9716379830987604, 'learning_rate': 8.491866196179192e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.7619296865752224}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:25:14,713][0m Trial 45 finished with value: 0.06919210333869143 and parameters: {'observation_period_num': 15, 'train_rates': 0.9821057898078356, 'learning_rate': 0.00010320208365794296, 'batch_size': 42, 'step_size': 15, 'gamma': 0.7628142068146968}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:27:10,926][0m Trial 46 finished with value: 0.07930186312449605 and parameters: {'observation_period_num': 14, 'train_rates': 0.9680314316064109, 'learning_rate': 9.18713797764913e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.7553814048573934}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:28:28,646][0m Trial 47 finished with value: 0.10161915781417331 and parameters: {'observation_period_num': 13, 'train_rates': 0.9296881127381502, 'learning_rate': 0.00010069110563696154, 'batch_size': 58, 'step_size': 9, 'gamma': 0.7859516510886217}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:30:06,132][0m Trial 48 finished with value: 0.13367869634003865 and parameters: {'observation_period_num': 25, 'train_rates': 0.9713520741408794, 'learning_rate': 0.0002168413165296167, 'batch_size': 44, 'step_size': 10, 'gamma': 0.758525822110541}. Best is trial 11 with value: 0.05784338359770022.[0m
[32m[I 2025-01-06 17:30:50,675][0m Trial 49 finished with value: 0.169833289764983 and parameters: {'observation_period_num': 5, 'train_rates': 0.8907205904810962, 'learning_rate': 0.00038059511247773173, 'batch_size': 90, 'step_size': 11, 'gamma': 0.7513517752942863}. Best is trial 11 with value: 0.05784338359770022.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.4029 | 0.3384
Epoch 2/300, Loss: 0.3294 | 0.2927
Epoch 3/300, Loss: 0.2996 | 0.3410
Epoch 4/300, Loss: 0.2781 | 0.3421
Epoch 5/300, Loss: 0.2649 | 0.2619
Epoch 6/300, Loss: 0.2554 | 0.2029
Epoch 7/300, Loss: 0.2453 | 0.3232
Epoch 8/300, Loss: 0.2638 | 0.3428
Epoch 9/300, Loss: 0.2525 | 0.2429
Epoch 10/300, Loss: 0.2041 | 0.2029
Epoch 11/300, Loss: 0.1760 | 0.1785
Epoch 12/300, Loss: 0.1758 | 0.1734
Epoch 13/300, Loss: 0.1806 | 0.1736
Epoch 14/300, Loss: 0.1625 | 0.1693
Epoch 15/300, Loss: 0.1551 | 0.1779
Epoch 16/300, Loss: 0.1577 | 0.1671
Epoch 17/300, Loss: 0.1490 | 0.1636
Epoch 18/300, Loss: 0.1361 | 0.1824
Epoch 19/300, Loss: 0.1322 | 0.1670
Epoch 20/300, Loss: 0.1275 | 0.1305
Epoch 21/300, Loss: 0.1261 | 0.1339
Epoch 22/300, Loss: 0.1268 | 0.1474
Epoch 23/300, Loss: 0.1257 | 0.1503
Epoch 24/300, Loss: 0.1309 | 0.1389
Epoch 25/300, Loss: 0.1169 | 0.1307
Epoch 26/300, Loss: 0.1079 | 0.1912
Epoch 27/300, Loss: 0.1077 | 0.1107
Epoch 28/300, Loss: 0.1053 | 0.1092
Epoch 29/300, Loss: 0.1015 | 0.1230
Epoch 30/300, Loss: 0.0977 | 0.1642
Epoch 31/300, Loss: 0.1014 | 0.0981
Epoch 32/300, Loss: 0.0940 | 0.1099
Epoch 33/300, Loss: 0.0926 | 0.1282
Epoch 34/300, Loss: 0.0906 | 0.0975
Epoch 35/300, Loss: 0.0908 | 0.0920
Epoch 36/300, Loss: 0.0901 | 0.1007
Epoch 37/300, Loss: 0.0899 | 0.1476
Epoch 38/300, Loss: 0.0871 | 0.0870
Epoch 39/300, Loss: 0.0884 | 0.0837
Epoch 40/300, Loss: 0.0844 | 0.1221
Epoch 41/300, Loss: 0.0825 | 0.0902
Epoch 42/300, Loss: 0.0820 | 0.0733
Epoch 43/300, Loss: 0.0798 | 0.0926
Epoch 44/300, Loss: 0.0783 | 0.1207
Epoch 45/300, Loss: 0.0781 | 0.0815
Epoch 46/300, Loss: 0.0791 | 0.0806
Epoch 47/300, Loss: 0.0772 | 0.1027
Epoch 48/300, Loss: 0.0749 | 0.0953
Epoch 49/300, Loss: 0.0736 | 0.0720
Epoch 50/300, Loss: 0.0738 | 0.0734
Epoch 51/300, Loss: 0.0738 | 0.0952
Epoch 52/300, Loss: 0.0730 | 0.0877
Epoch 53/300, Loss: 0.0725 | 0.0754
Epoch 54/300, Loss: 0.0732 | 0.0765
Epoch 55/300, Loss: 0.0720 | 0.0886
Epoch 56/300, Loss: 0.0707 | 0.0893
Epoch 57/300, Loss: 0.0700 | 0.0718
Epoch 58/300, Loss: 0.0691 | 0.0724
Epoch 59/300, Loss: 0.0685 | 0.0771
Epoch 60/300, Loss: 0.0681 | 0.0948
Epoch 61/300, Loss: 0.0690 | 0.0752
Epoch 62/300, Loss: 0.0680 | 0.0707
Epoch 63/300, Loss: 0.0675 | 0.0712
Epoch 64/300, Loss: 0.0666 | 0.0944
Epoch 65/300, Loss: 0.0670 | 0.0773
Epoch 66/300, Loss: 0.0665 | 0.0713
Epoch 67/300, Loss: 0.0661 | 0.0721
Epoch 68/300, Loss: 0.0656 | 0.0877
Epoch 69/300, Loss: 0.0647 | 0.0763
Epoch 70/300, Loss: 0.0645 | 0.0733
Epoch 71/300, Loss: 0.0635 | 0.0732
Epoch 72/300, Loss: 0.0639 | 0.0816
Epoch 73/300, Loss: 0.0634 | 0.0761
Epoch 74/300, Loss: 0.0627 | 0.0724
Epoch 75/300, Loss: 0.0622 | 0.0742
Epoch 76/300, Loss: 0.0627 | 0.0771
Epoch 77/300, Loss: 0.0621 | 0.0733
Epoch 78/300, Loss: 0.0615 | 0.0750
Epoch 79/300, Loss: 0.0616 | 0.0757
Epoch 80/300, Loss: 0.0612 | 0.0730
Epoch 81/300, Loss: 0.0614 | 0.0737
Epoch 82/300, Loss: 0.0608 | 0.0740
Epoch 83/300, Loss: 0.0607 | 0.0753
Epoch 84/300, Loss: 0.0605 | 0.0729
Epoch 85/300, Loss: 0.0607 | 0.0718
Epoch 86/300, Loss: 0.0612 | 0.0750
Epoch 87/300, Loss: 0.0606 | 0.0733
Epoch 88/300, Loss: 0.0602 | 0.0734
Epoch 89/300, Loss: 0.0608 | 0.0730
Epoch 90/300, Loss: 0.0596 | 0.0731
Epoch 91/300, Loss: 0.0594 | 0.0725
Epoch 92/300, Loss: 0.0601 | 0.0723
Epoch 93/300, Loss: 0.0591 | 0.0731
Epoch 94/300, Loss: 0.0594 | 0.0730
Epoch 95/300, Loss: 0.0596 | 0.0717
Epoch 96/300, Loss: 0.0591 | 0.0727
Epoch 97/300, Loss: 0.0592 | 0.0741
Epoch 98/300, Loss: 0.0586 | 0.0726
Epoch 99/300, Loss: 0.0589 | 0.0717
Epoch 100/300, Loss: 0.0585 | 0.0724
Epoch 101/300, Loss: 0.0589 | 0.0718
Epoch 102/300, Loss: 0.0592 | 0.0726
Epoch 103/300, Loss: 0.0590 | 0.0718
Epoch 104/300, Loss: 0.0585 | 0.0716
Epoch 105/300, Loss: 0.0581 | 0.0736
Epoch 106/300, Loss: 0.0584 | 0.0726
Epoch 107/300, Loss: 0.0580 | 0.0726
Epoch 108/300, Loss: 0.0581 | 0.0710
Epoch 109/300, Loss: 0.0582 | 0.0717
Epoch 110/300, Loss: 0.0582 | 0.0712
Epoch 111/300, Loss: 0.0585 | 0.0706
Epoch 112/300, Loss: 0.0588 | 0.0708
Epoch 113/300, Loss: 0.0584 | 0.0720
Epoch 114/300, Loss: 0.0584 | 0.0718
Epoch 115/300, Loss: 0.0583 | 0.0712
Epoch 116/300, Loss: 0.0580 | 0.0707
Epoch 117/300, Loss: 0.0577 | 0.0713
Epoch 118/300, Loss: 0.0585 | 0.0713
Epoch 119/300, Loss: 0.0579 | 0.0714
Epoch 120/300, Loss: 0.0576 | 0.0716
Epoch 121/300, Loss: 0.0580 | 0.0719
Epoch 122/300, Loss: 0.0576 | 0.0720
Epoch 123/300, Loss: 0.0575 | 0.0713
Epoch 124/300, Loss: 0.0575 | 0.0711
Epoch 125/300, Loss: 0.0575 | 0.0714
Epoch 126/300, Loss: 0.0577 | 0.0716
Epoch 127/300, Loss: 0.0573 | 0.0713
Epoch 128/300, Loss: 0.0573 | 0.0706
Epoch 129/300, Loss: 0.0572 | 0.0707
Epoch 130/300, Loss: 0.0574 | 0.0712
Epoch 131/300, Loss: 0.0577 | 0.0715
Epoch 132/300, Loss: 0.0571 | 0.0709
Epoch 133/300, Loss: 0.0573 | 0.0710
Epoch 134/300, Loss: 0.0576 | 0.0707
Epoch 135/300, Loss: 0.0571 | 0.0707
Epoch 136/300, Loss: 0.0571 | 0.0706
Epoch 137/300, Loss: 0.0575 | 0.0712
Epoch 138/300, Loss: 0.0577 | 0.0715
Epoch 139/300, Loss: 0.0571 | 0.0714
Epoch 140/300, Loss: 0.0569 | 0.0710
Epoch 141/300, Loss: 0.0572 | 0.0711
Epoch 142/300, Loss: 0.0571 | 0.0711
Epoch 143/300, Loss: 0.0574 | 0.0712
Epoch 144/300, Loss: 0.0568 | 0.0715
Epoch 145/300, Loss: 0.0571 | 0.0716
Epoch 146/300, Loss: 0.0573 | 0.0717
Epoch 147/300, Loss: 0.0572 | 0.0714
Epoch 148/300, Loss: 0.0576 | 0.0717
Epoch 149/300, Loss: 0.0568 | 0.0721
Epoch 150/300, Loss: 0.0575 | 0.0717
Epoch 151/300, Loss: 0.0578 | 0.0716
Epoch 152/300, Loss: 0.0571 | 0.0712
Epoch 153/300, Loss: 0.0572 | 0.0711
Epoch 154/300, Loss: 0.0572 | 0.0711
Epoch 155/300, Loss: 0.0575 | 0.0713
Epoch 156/300, Loss: 0.0569 | 0.0713
Epoch 157/300, Loss: 0.0569 | 0.0712
Epoch 158/300, Loss: 0.0570 | 0.0712
Epoch 159/300, Loss: 0.0569 | 0.0711
Epoch 160/300, Loss: 0.0574 | 0.0710
Epoch 161/300, Loss: 0.0570 | 0.0709
Epoch 162/300, Loss: 0.0577 | 0.0709
Epoch 163/300, Loss: 0.0571 | 0.0710
Epoch 164/300, Loss: 0.0572 | 0.0709
Epoch 165/300, Loss: 0.0573 | 0.0710
Epoch 166/300, Loss: 0.0573 | 0.0709
Epoch 167/300, Loss: 0.0568 | 0.0709
Epoch 168/300, Loss: 0.0568 | 0.0710
Epoch 169/300, Loss: 0.0576 | 0.0710
Epoch 170/300, Loss: 0.0572 | 0.0710
Epoch 171/300, Loss: 0.0574 | 0.0710
Epoch 172/300, Loss: 0.0570 | 0.0710
Epoch 173/300, Loss: 0.0568 | 0.0710
Epoch 174/300, Loss: 0.0568 | 0.0710
Epoch 175/300, Loss: 0.0568 | 0.0709
Epoch 176/300, Loss: 0.0570 | 0.0708
Epoch 177/300, Loss: 0.0572 | 0.0709
Epoch 178/300, Loss: 0.0577 | 0.0709
Epoch 179/300, Loss: 0.0569 | 0.0709
Epoch 180/300, Loss: 0.0571 | 0.0710
Epoch 181/300, Loss: 0.0567 | 0.0709
Epoch 182/300, Loss: 0.0573 | 0.0709
Epoch 183/300, Loss: 0.0570 | 0.0709
Epoch 184/300, Loss: 0.0571 | 0.0708
Epoch 185/300, Loss: 0.0568 | 0.0710
Epoch 186/300, Loss: 0.0570 | 0.0710
Epoch 187/300, Loss: 0.0569 | 0.0711
Epoch 188/300, Loss: 0.0570 | 0.0710
Epoch 189/300, Loss: 0.0566 | 0.0710
Epoch 190/300, Loss: 0.0570 | 0.0709
Epoch 191/300, Loss: 0.0572 | 0.0709
Epoch 192/300, Loss: 0.0568 | 0.0708
Epoch 193/300, Loss: 0.0570 | 0.0708
Epoch 194/300, Loss: 0.0568 | 0.0709
Epoch 195/300, Loss: 0.0572 | 0.0708
Epoch 196/300, Loss: 0.0572 | 0.0709
Epoch 197/300, Loss: 0.0572 | 0.0709
Epoch 198/300, Loss: 0.0570 | 0.0709
Epoch 199/300, Loss: 0.0569 | 0.0709
Epoch 200/300, Loss: 0.0572 | 0.0709
Epoch 201/300, Loss: 0.0571 | 0.0708
Epoch 202/300, Loss: 0.0569 | 0.0708
Epoch 203/300, Loss: 0.0571 | 0.0708
Epoch 204/300, Loss: 0.0568 | 0.0708
Epoch 205/300, Loss: 0.0571 | 0.0708
Epoch 206/300, Loss: 0.0569 | 0.0708
Epoch 207/300, Loss: 0.0570 | 0.0708
Epoch 208/300, Loss: 0.0568 | 0.0708
Epoch 209/300, Loss: 0.0571 | 0.0708
Epoch 210/300, Loss: 0.0568 | 0.0708
Epoch 211/300, Loss: 0.0570 | 0.0709
Epoch 212/300, Loss: 0.0569 | 0.0709
Epoch 213/300, Loss: 0.0566 | 0.0709
Epoch 214/300, Loss: 0.0569 | 0.0709
Epoch 215/300, Loss: 0.0571 | 0.0709
Epoch 216/300, Loss: 0.0570 | 0.0709
Epoch 217/300, Loss: 0.0572 | 0.0709
Epoch 218/300, Loss: 0.0569 | 0.0709
Epoch 219/300, Loss: 0.0568 | 0.0709
Epoch 220/300, Loss: 0.0571 | 0.0709
Epoch 221/300, Loss: 0.0565 | 0.0709
Epoch 222/300, Loss: 0.0569 | 0.0709
Epoch 223/300, Loss: 0.0572 | 0.0709
Epoch 224/300, Loss: 0.0568 | 0.0709
Epoch 225/300, Loss: 0.0570 | 0.0709
Epoch 226/300, Loss: 0.0574 | 0.0709
Epoch 227/300, Loss: 0.0573 | 0.0709
Epoch 228/300, Loss: 0.0567 | 0.0709
Epoch 229/300, Loss: 0.0571 | 0.0709
Epoch 230/300, Loss: 0.0563 | 0.0708
Epoch 231/300, Loss: 0.0568 | 0.0708
Epoch 232/300, Loss: 0.0570 | 0.0708
Epoch 233/300, Loss: 0.0571 | 0.0708
Epoch 234/300, Loss: 0.0567 | 0.0708
Epoch 235/300, Loss: 0.0572 | 0.0708
Epoch 236/300, Loss: 0.0566 | 0.0708
Epoch 237/300, Loss: 0.0569 | 0.0708
Epoch 238/300, Loss: 0.0566 | 0.0708
Epoch 239/300, Loss: 0.0567 | 0.0708
Epoch 240/300, Loss: 0.0571 | 0.0708
Epoch 241/300, Loss: 0.0567 | 0.0708
Epoch 242/300, Loss: 0.0568 | 0.0708
Epoch 243/300, Loss: 0.0567 | 0.0708
Epoch 244/300, Loss: 0.0569 | 0.0708
Epoch 245/300, Loss: 0.0570 | 0.0708
Epoch 246/300, Loss: 0.0566 | 0.0708
Epoch 247/300, Loss: 0.0569 | 0.0708
Epoch 248/300, Loss: 0.0574 | 0.0708
Epoch 249/300, Loss: 0.0570 | 0.0708
Epoch 250/300, Loss: 0.0565 | 0.0708
Epoch 251/300, Loss: 0.0572 | 0.0708
Epoch 252/300, Loss: 0.0567 | 0.0708
Epoch 253/300, Loss: 0.0573 | 0.0708
Epoch 254/300, Loss: 0.0568 | 0.0708
Epoch 255/300, Loss: 0.0566 | 0.0708
Epoch 256/300, Loss: 0.0571 | 0.0708
Epoch 257/300, Loss: 0.0568 | 0.0708
Epoch 258/300, Loss: 0.0566 | 0.0708
Epoch 259/300, Loss: 0.0567 | 0.0708
Epoch 260/300, Loss: 0.0572 | 0.0708
Epoch 261/300, Loss: 0.0568 | 0.0708
Epoch 262/300, Loss: 0.0572 | 0.0709
Epoch 263/300, Loss: 0.0569 | 0.0708
Epoch 264/300, Loss: 0.0570 | 0.0709
Epoch 265/300, Loss: 0.0569 | 0.0708
Epoch 266/300, Loss: 0.0573 | 0.0709
Epoch 267/300, Loss: 0.0568 | 0.0709
Epoch 268/300, Loss: 0.0569 | 0.0709
Epoch 269/300, Loss: 0.0567 | 0.0709
Epoch 270/300, Loss: 0.0569 | 0.0708
Epoch 271/300, Loss: 0.0571 | 0.0708
Epoch 272/300, Loss: 0.0568 | 0.0709
Epoch 273/300, Loss: 0.0567 | 0.0708
Epoch 274/300, Loss: 0.0568 | 0.0709
Epoch 275/300, Loss: 0.0572 | 0.0709
Epoch 276/300, Loss: 0.0566 | 0.0709
Epoch 277/300, Loss: 0.0567 | 0.0709
Epoch 278/300, Loss: 0.0568 | 0.0708
Epoch 279/300, Loss: 0.0571 | 0.0708
Epoch 280/300, Loss: 0.0571 | 0.0708
Epoch 281/300, Loss: 0.0568 | 0.0708
Epoch 282/300, Loss: 0.0576 | 0.0708
Epoch 283/300, Loss: 0.0567 | 0.0708
Epoch 284/300, Loss: 0.0566 | 0.0708
Epoch 285/300, Loss: 0.0568 | 0.0708
Epoch 286/300, Loss: 0.0567 | 0.0708
Epoch 287/300, Loss: 0.0567 | 0.0708
Epoch 288/300, Loss: 0.0570 | 0.0708
Epoch 289/300, Loss: 0.0568 | 0.0708
Epoch 290/300, Loss: 0.0565 | 0.0708
Epoch 291/300, Loss: 0.0573 | 0.0708
Epoch 292/300, Loss: 0.0568 | 0.0708
Epoch 293/300, Loss: 0.0563 | 0.0708
Epoch 294/300, Loss: 0.0570 | 0.0708
Epoch 295/300, Loss: 0.0570 | 0.0708
Epoch 296/300, Loss: 0.0568 | 0.0708
Epoch 297/300, Loss: 0.0569 | 0.0708
Epoch 298/300, Loss: 0.0572 | 0.0708
Epoch 299/300, Loss: 0.0575 | 0.0708
Epoch 300/300, Loss: 0.0570 | 0.0708
Runtime (seconds): 798.5248558521271
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 275.8561317631975
RMSE: 16.608917236328125
MAE: 16.608917236328125
R-squared: nan
[201.55109]
