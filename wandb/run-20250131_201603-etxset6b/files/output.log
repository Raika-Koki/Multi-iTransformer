ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-31 20:16:09,852][0m A new study created in memory with name: no-name-79bbbe16-8af5-4692-81b6-a1ebf37bb415[0m
[32m[I 2025-01-31 20:16:34,142][0m Trial 0 finished with value: 0.17910052778352287 and parameters: {'observation_period_num': 142, 'train_rates': 0.7055800017728504, 'learning_rate': 0.0001396020680818682, 'batch_size': 206, 'step_size': 2, 'gamma': 0.9177107590633846}. Best is trial 0 with value: 0.17910052778352287.[0m
[32m[I 2025-01-31 20:17:49,939][0m Trial 1 finished with value: 0.19762587291741435 and parameters: {'observation_period_num': 204, 'train_rates': 0.7245430664880727, 'learning_rate': 1.5275621113157223e-05, 'batch_size': 59, 'step_size': 15, 'gamma': 0.986276315575345}. Best is trial 0 with value: 0.17910052778352287.[0m
[32m[I 2025-01-31 20:18:31,897][0m Trial 2 finished with value: 0.24919759700843944 and parameters: {'observation_period_num': 248, 'train_rates': 0.7028353432545598, 'learning_rate': 0.0001060510665598423, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8657158973636554}. Best is trial 0 with value: 0.17910052778352287.[0m
[32m[I 2025-01-31 20:18:54,940][0m Trial 3 finished with value: 0.18786489601722725 and parameters: {'observation_period_num': 190, 'train_rates': 0.698180059099155, 'learning_rate': 0.0003058386473416519, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9690506066731851}. Best is trial 0 with value: 0.17910052778352287.[0m
[32m[I 2025-01-31 20:21:05,721][0m Trial 4 finished with value: 0.309511403242747 and parameters: {'observation_period_num': 155, 'train_rates': 0.9551690879196393, 'learning_rate': 2.8150350862439505e-05, 'batch_size': 42, 'step_size': 8, 'gamma': 0.8635669026112316}. Best is trial 0 with value: 0.17910052778352287.[0m
[32m[I 2025-01-31 20:21:42,761][0m Trial 5 finished with value: 0.12065533917911093 and parameters: {'observation_period_num': 10, 'train_rates': 0.6453105473511134, 'learning_rate': 2.8372180297668e-05, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8916454441277155}. Best is trial 5 with value: 0.12065533917911093.[0m
[32m[I 2025-01-31 20:22:04,346][0m Trial 6 finished with value: 0.13946586850358814 and parameters: {'observation_period_num': 112, 'train_rates': 0.7423614048775118, 'learning_rate': 0.0006084390659285436, 'batch_size': 235, 'step_size': 4, 'gamma': 0.7681946863895658}. Best is trial 5 with value: 0.12065533917911093.[0m
[32m[I 2025-01-31 20:23:45,068][0m Trial 7 finished with value: 0.2615878964220727 and parameters: {'observation_period_num': 76, 'train_rates': 0.751791828264809, 'learning_rate': 6.7952986197514785e-06, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9789899708174297}. Best is trial 5 with value: 0.12065533917911093.[0m
[32m[I 2025-01-31 20:24:08,230][0m Trial 8 finished with value: 0.9564874353717961 and parameters: {'observation_period_num': 92, 'train_rates': 0.8222791861894465, 'learning_rate': 1.348277783455551e-06, 'batch_size': 236, 'step_size': 9, 'gamma': 0.8514369178464427}. Best is trial 5 with value: 0.12065533917911093.[0m
[32m[I 2025-01-31 20:27:09,319][0m Trial 9 finished with value: 0.08883650834435847 and parameters: {'observation_period_num': 244, 'train_rates': 0.9755955884926777, 'learning_rate': 0.0001006263086121154, 'batch_size': 30, 'step_size': 13, 'gamma': 0.794508190300265}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:27:43,787][0m Trial 10 finished with value: 0.12331222742795944 and parameters: {'observation_period_num': 251, 'train_rates': 0.9874064497186373, 'learning_rate': 0.0009359708953177389, 'batch_size': 167, 'step_size': 11, 'gamma': 0.7599651289974914}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:28:24,919][0m Trial 11 finished with value: 0.1121514659063452 and parameters: {'observation_period_num': 15, 'train_rates': 0.6013755057170185, 'learning_rate': 6.893882308830094e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8117634764346371}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:29:25,191][0m Trial 12 finished with value: 0.11097374478763437 and parameters: {'observation_period_num': 12, 'train_rates': 0.8729416187169682, 'learning_rate': 9.123202180076457e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.807417210855214}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:30:42,603][0m Trial 13 finished with value: 0.17706365476051966 and parameters: {'observation_period_num': 60, 'train_rates': 0.9223446240924715, 'learning_rate': 0.00020778320408329048, 'batch_size': 73, 'step_size': 8, 'gamma': 0.7929691343367913}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:34:33,409][0m Trial 14 finished with value: 0.12366426286458129 and parameters: {'observation_period_num': 47, 'train_rates': 0.8780356758400147, 'learning_rate': 5.70351500188446e-05, 'batch_size': 23, 'step_size': 10, 'gamma': 0.8213226860160888}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:35:35,496][0m Trial 15 finished with value: 0.265777338695645 and parameters: {'observation_period_num': 186, 'train_rates': 0.8552703882851967, 'learning_rate': 1.2961961172802992e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.828406278771333}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:36:13,851][0m Trial 16 finished with value: 0.1851548541950997 and parameters: {'observation_period_num': 112, 'train_rates': 0.9084317176835415, 'learning_rate': 0.00030436583987796125, 'batch_size': 151, 'step_size': 6, 'gamma': 0.7801485289197112}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:40:35,133][0m Trial 17 finished with value: 0.23352653498273854 and parameters: {'observation_period_num': 218, 'train_rates': 0.8054425165711165, 'learning_rate': 7.060087718298042e-06, 'batch_size': 18, 'step_size': 13, 'gamma': 0.7515876364398368}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:41:33,114][0m Trial 18 finished with value: 0.4543466567993164 and parameters: {'observation_period_num': 154, 'train_rates': 0.9712354697145689, 'learning_rate': 5.604451355817435e-05, 'batch_size': 98, 'step_size': 6, 'gamma': 0.7911983712479889}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:42:13,280][0m Trial 19 finished with value: 0.7695138637836163 and parameters: {'observation_period_num': 32, 'train_rates': 0.928982642404671, 'learning_rate': 2.202235470368421e-06, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8365852117769259}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:43:17,626][0m Trial 20 finished with value: 0.17668095859877594 and parameters: {'observation_period_num': 171, 'train_rates': 0.8597740876642991, 'learning_rate': 0.000380524569711355, 'batch_size': 80, 'step_size': 10, 'gamma': 0.8930587206810381}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:43:54,059][0m Trial 21 finished with value: 0.118109285419992 and parameters: {'observation_period_num': 10, 'train_rates': 0.6002358372600534, 'learning_rate': 6.73945076500068e-05, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8034180741044382}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:44:23,459][0m Trial 22 finished with value: 0.10563426383340942 and parameters: {'observation_period_num': 38, 'train_rates': 0.7758003661901766, 'learning_rate': 7.342372630317208e-05, 'batch_size': 181, 'step_size': 14, 'gamma': 0.8214510703231079}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:44:51,379][0m Trial 23 finished with value: 0.10668015734342758 and parameters: {'observation_period_num': 41, 'train_rates': 0.770803143797828, 'learning_rate': 0.00013657674326459098, 'batch_size': 188, 'step_size': 14, 'gamma': 0.8367157747166187}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:45:18,882][0m Trial 24 finished with value: 0.12770792785591967 and parameters: {'observation_period_num': 76, 'train_rates': 0.7794232865892831, 'learning_rate': 0.00013047236104026747, 'batch_size': 188, 'step_size': 14, 'gamma': 0.84067776892912}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:45:50,039][0m Trial 25 finished with value: 0.09721528932052644 and parameters: {'observation_period_num': 35, 'train_rates': 0.782963452348634, 'learning_rate': 0.0001687382953592537, 'batch_size': 173, 'step_size': 14, 'gamma': 0.8832100494783002}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:46:21,373][0m Trial 26 finished with value: 0.16226664571737756 and parameters: {'observation_period_num': 114, 'train_rates': 0.8275127316834026, 'learning_rate': 3.782002313690784e-05, 'batch_size': 178, 'step_size': 14, 'gamma': 0.9401362685199846}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:46:41,249][0m Trial 27 finished with value: 0.21995063117926641 and parameters: {'observation_period_num': 228, 'train_rates': 0.6652852597679166, 'learning_rate': 0.00021554167457865178, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8845270202590698}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:47:14,615][0m Trial 28 finished with value: 0.1432883056157176 and parameters: {'observation_period_num': 64, 'train_rates': 0.788030237109732, 'learning_rate': 0.0005298520775696392, 'batch_size': 154, 'step_size': 11, 'gamma': 0.9109277433689918}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:47:42,032][0m Trial 29 finished with value: 0.21554235669604518 and parameters: {'observation_period_num': 138, 'train_rates': 0.8178575461427062, 'learning_rate': 0.00019072489698222756, 'batch_size': 206, 'step_size': 14, 'gamma': 0.9216620985837269}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:48:09,443][0m Trial 30 finished with value: 0.17390421477642878 and parameters: {'observation_period_num': 87, 'train_rates': 0.8417817404383431, 'learning_rate': 3.5332998782580675e-05, 'batch_size': 207, 'step_size': 15, 'gamma': 0.8529777344752407}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:48:39,028][0m Trial 31 finished with value: 0.1192322278267119 and parameters: {'observation_period_num': 43, 'train_rates': 0.7653747483813479, 'learning_rate': 0.0001487317210010849, 'batch_size': 185, 'step_size': 14, 'gamma': 0.8777261851649454}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:49:09,183][0m Trial 32 finished with value: 0.0958813510766275 and parameters: {'observation_period_num': 35, 'train_rates': 0.7408507571542577, 'learning_rate': 0.00010439652497652797, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8223076337859201}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:49:38,983][0m Trial 33 finished with value: 0.16315813408098148 and parameters: {'observation_period_num': 30, 'train_rates': 0.7312651746623158, 'learning_rate': 1.8491227065619236e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.7781794875991638}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:50:16,069][0m Trial 34 finished with value: 0.12253099110546035 and parameters: {'observation_period_num': 30, 'train_rates': 0.6996333353282436, 'learning_rate': 9.419368243758337e-05, 'batch_size': 131, 'step_size': 15, 'gamma': 0.8238204371076535}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:50:46,542][0m Trial 35 finished with value: 0.12512934156086133 and parameters: {'observation_period_num': 57, 'train_rates': 0.7223150515697538, 'learning_rate': 4.119121773189971e-05, 'batch_size': 162, 'step_size': 11, 'gamma': 0.8598634115698709}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:51:19,782][0m Trial 36 finished with value: 0.1234561506654916 and parameters: {'observation_period_num': 99, 'train_rates': 0.6636063377948844, 'learning_rate': 9.688635233490098e-05, 'batch_size': 144, 'step_size': 15, 'gamma': 0.7971242469895363}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:51:44,952][0m Trial 37 finished with value: 0.08920546181336624 and parameters: {'observation_period_num': 23, 'train_rates': 0.797745946630248, 'learning_rate': 0.00030852112060699436, 'batch_size': 222, 'step_size': 13, 'gamma': 0.814811782077077}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:52:11,805][0m Trial 38 finished with value: 0.19800193954820502 and parameters: {'observation_period_num': 72, 'train_rates': 0.8986481130345624, 'learning_rate': 0.0003255545268237576, 'batch_size': 227, 'step_size': 13, 'gamma': 0.905575112374124}. Best is trial 9 with value: 0.08883650834435847.[0m
[32m[I 2025-01-31 20:52:37,941][0m Trial 39 finished with value: 0.07816489478777569 and parameters: {'observation_period_num': 22, 'train_rates': 0.6736725596761437, 'learning_rate': 0.0009961813741251734, 'batch_size': 197, 'step_size': 3, 'gamma': 0.9348495693843868}. Best is trial 39 with value: 0.07816489478777569.[0m
[32m[I 2025-01-31 20:53:02,471][0m Trial 40 finished with value: 0.09373236960249315 and parameters: {'observation_period_num': 20, 'train_rates': 0.6820450369229731, 'learning_rate': 0.0008655168249996316, 'batch_size': 216, 'step_size': 3, 'gamma': 0.7737640242307027}. Best is trial 39 with value: 0.07816489478777569.[0m
[32m[I 2025-01-31 20:53:24,882][0m Trial 41 finished with value: 0.09739985987622404 and parameters: {'observation_period_num': 19, 'train_rates': 0.675936510560381, 'learning_rate': 0.0008998942649867577, 'batch_size': 226, 'step_size': 3, 'gamma': 0.7731133384732496}. Best is trial 39 with value: 0.07816489478777569.[0m
[32m[I 2025-01-31 20:53:49,589][0m Trial 42 finished with value: 0.07403270176676817 and parameters: {'observation_period_num': 7, 'train_rates': 0.6353040606292266, 'learning_rate': 0.0006110099469373144, 'batch_size': 196, 'step_size': 1, 'gamma': 0.9603932894161283}. Best is trial 42 with value: 0.07403270176676817.[0m
[32m[I 2025-01-31 20:54:10,192][0m Trial 43 finished with value: 0.08916631097938388 and parameters: {'observation_period_num': 6, 'train_rates': 0.62727890998775, 'learning_rate': 0.0005292765188982288, 'batch_size': 247, 'step_size': 1, 'gamma': 0.9534964491885923}. Best is trial 42 with value: 0.07403270176676817.[0m
[32m[I 2025-01-31 20:54:30,933][0m Trial 44 finished with value: 0.07907516373707367 and parameters: {'observation_period_num': 6, 'train_rates': 0.6236704854597656, 'learning_rate': 0.0005160438009675114, 'batch_size': 239, 'step_size': 1, 'gamma': 0.96797179574501}. Best is trial 42 with value: 0.07403270176676817.[0m
[32m[I 2025-01-31 20:54:55,158][0m Trial 45 finished with value: 0.08504987031139516 and parameters: {'observation_period_num': 12, 'train_rates': 0.6288883097309036, 'learning_rate': 0.000585594583467962, 'batch_size': 198, 'step_size': 1, 'gamma': 0.960232758801743}. Best is trial 42 with value: 0.07403270176676817.[0m
[32m[I 2025-01-31 20:55:19,738][0m Trial 46 finished with value: 0.06302502923391082 and parameters: {'observation_period_num': 5, 'train_rates': 0.6270625729441969, 'learning_rate': 0.0006346333559577962, 'batch_size': 199, 'step_size': 2, 'gamma': 0.9834599174167772}. Best is trial 46 with value: 0.06302502923391082.[0m
[32m[I 2025-01-31 20:55:44,246][0m Trial 47 finished with value: 0.10830329323329156 and parameters: {'observation_period_num': 51, 'train_rates': 0.6287203766743885, 'learning_rate': 0.0006652115440046272, 'batch_size': 199, 'step_size': 2, 'gamma': 0.9890045725657023}. Best is trial 46 with value: 0.06302502923391082.[0m
[32m[I 2025-01-31 20:56:04,432][0m Trial 48 finished with value: 0.07170113233419564 and parameters: {'observation_period_num': 7, 'train_rates': 0.6250748347947029, 'learning_rate': 0.00045086498649385144, 'batch_size': 238, 'step_size': 2, 'gamma': 0.9661510876615103}. Best is trial 46 with value: 0.06302502923391082.[0m
[32m[I 2025-01-31 20:56:24,742][0m Trial 49 finished with value: 0.07183203137012717 and parameters: {'observation_period_num': 6, 'train_rates': 0.6474992325168797, 'learning_rate': 0.00042194181486016275, 'batch_size': 239, 'step_size': 2, 'gamma': 0.9749573112920139}. Best is trial 46 with value: 0.06302502923391082.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-31 20:56:24,753][0m A new study created in memory with name: no-name-25c5408c-6b6c-4f1a-9dc7-e803fcc4cc4c[0m
[32m[I 2025-01-31 20:57:20,026][0m Trial 0 finished with value: 1.3621870966692537 and parameters: {'observation_period_num': 60, 'train_rates': 0.6100847570657383, 'learning_rate': 1.2892861247048993e-06, 'batch_size': 79, 'step_size': 4, 'gamma': 0.8682562197029209}. Best is trial 0 with value: 1.3621870966692537.[0m
[32m[I 2025-01-31 20:57:45,615][0m Trial 1 finished with value: 0.13206291212592014 and parameters: {'observation_period_num': 129, 'train_rates': 0.7718712960245926, 'learning_rate': 0.0005495155986477028, 'batch_size': 201, 'step_size': 6, 'gamma': 0.8366559567966497}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 20:59:14,408][0m Trial 2 finished with value: 0.16752332465068714 and parameters: {'observation_period_num': 218, 'train_rates': 0.7971679040071272, 'learning_rate': 0.00015651059365515398, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9488185161524182}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:00:21,783][0m Trial 3 finished with value: 0.15180893707547696 and parameters: {'observation_period_num': 172, 'train_rates': 0.716650330331205, 'learning_rate': 0.00012995443613216033, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9014163376457157}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:01:07,619][0m Trial 4 finished with value: 0.1753566107110521 and parameters: {'observation_period_num': 196, 'train_rates': 0.8255589731024625, 'learning_rate': 0.0006840818351169018, 'batch_size': 112, 'step_size': 12, 'gamma': 0.9697997489140704}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:01:36,028][0m Trial 5 finished with value: 0.3600783348083496 and parameters: {'observation_period_num': 214, 'train_rates': 0.9577128730938885, 'learning_rate': 0.00020705028785748342, 'batch_size': 203, 'step_size': 8, 'gamma': 0.9423217450268846}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:01:57,569][0m Trial 6 finished with value: 0.30960709691047666 and parameters: {'observation_period_num': 203, 'train_rates': 0.7639217322690031, 'learning_rate': 2.388256619443774e-05, 'batch_size': 234, 'step_size': 10, 'gamma': 0.8511899260238088}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:02:23,872][0m Trial 7 finished with value: 0.32120199764476104 and parameters: {'observation_period_num': 216, 'train_rates': 0.9007759007426873, 'learning_rate': 4.419671254861857e-05, 'batch_size': 224, 'step_size': 3, 'gamma': 0.9370989326082841}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:03:58,977][0m Trial 8 finished with value: 0.4431183928104698 and parameters: {'observation_period_num': 47, 'train_rates': 0.7484422203812877, 'learning_rate': 4.065152012809758e-06, 'batch_size': 51, 'step_size': 3, 'gamma': 0.8353706902289291}. Best is trial 1 with value: 0.13206291212592014.[0m
[32m[I 2025-01-31 21:04:25,249][0m Trial 9 finished with value: 0.10089199502198469 and parameters: {'observation_period_num': 76, 'train_rates': 0.6790341519462115, 'learning_rate': 0.0004286996311700805, 'batch_size': 187, 'step_size': 10, 'gamma': 0.7846991176458714}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:04:56,543][0m Trial 10 finished with value: 0.13452166890857195 and parameters: {'observation_period_num': 6, 'train_rates': 0.6305063103814946, 'learning_rate': 2.7525087624308323e-05, 'batch_size': 152, 'step_size': 15, 'gamma': 0.754083075156871}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:05:25,304][0m Trial 11 finished with value: 0.12335837293679076 and parameters: {'observation_period_num': 117, 'train_rates': 0.6853481865239508, 'learning_rate': 0.0009240283096812695, 'batch_size': 176, 'step_size': 6, 'gamma': 0.7888617095772622}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:05:55,902][0m Trial 12 finished with value: 0.11960694426317514 and parameters: {'observation_period_num': 109, 'train_rates': 0.6755622873021978, 'learning_rate': 0.0008784305487755539, 'batch_size': 160, 'step_size': 7, 'gamma': 0.7763219843194894}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:06:31,443][0m Trial 13 finished with value: 0.22497987441527537 and parameters: {'observation_period_num': 100, 'train_rates': 0.6719360614071892, 'learning_rate': 0.0003085709498473188, 'batch_size': 128, 'step_size': 13, 'gamma': 0.797282444484476}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:06:50,830][0m Trial 14 finished with value: 0.18371315996055096 and parameters: {'observation_period_num': 78, 'train_rates': 0.6596978923188025, 'learning_rate': 8.010761735499401e-05, 'batch_size': 256, 'step_size': 8, 'gamma': 0.7577986625749799}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:11:26,290][0m Trial 15 finished with value: 0.20129036760356808 and parameters: {'observation_period_num': 145, 'train_rates': 0.8407471768263064, 'learning_rate': 0.00040751605786361176, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8035397502843107}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:11:56,873][0m Trial 16 finished with value: 0.24055347208586117 and parameters: {'observation_period_num': 37, 'train_rates': 0.6966390381834501, 'learning_rate': 1.5220585419634664e-05, 'batch_size': 161, 'step_size': 6, 'gamma': 0.7865252760881076}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:12:41,846][0m Trial 17 finished with value: 0.11928572539793945 and parameters: {'observation_period_num': 91, 'train_rates': 0.7251282148459941, 'learning_rate': 8.069014400304595e-05, 'batch_size': 108, 'step_size': 12, 'gamma': 0.899346674326482}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:13:29,360][0m Trial 18 finished with value: 0.1227144947961757 and parameters: {'observation_period_num': 82, 'train_rates': 0.7222669579187836, 'learning_rate': 6.873639350368149e-05, 'batch_size': 102, 'step_size': 12, 'gamma': 0.8959057685103403}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:14:08,715][0m Trial 19 finished with value: 0.4136425688451992 and parameters: {'observation_period_num': 251, 'train_rates': 0.8525615623249019, 'learning_rate': 4.2002536494450355e-06, 'batch_size': 134, 'step_size': 15, 'gamma': 0.9003672166218314}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:14:33,657][0m Trial 20 finished with value: 0.28035079781919614 and parameters: {'observation_period_num': 15, 'train_rates': 0.6014291461995659, 'learning_rate': 1.0362067372804205e-05, 'batch_size': 186, 'step_size': 13, 'gamma': 0.8133146288570471}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:15:02,701][0m Trial 21 finished with value: 0.12493959915580101 and parameters: {'observation_period_num': 98, 'train_rates': 0.6394129249750262, 'learning_rate': 0.00029185952829933597, 'batch_size': 155, 'step_size': 9, 'gamma': 0.7716713095898541}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:15:48,100][0m Trial 22 finished with value: 0.13673490796938206 and parameters: {'observation_period_num': 154, 'train_rates': 0.7284385471672429, 'learning_rate': 0.0009793835281470787, 'batch_size': 104, 'step_size': 8, 'gamma': 0.8172014212062173}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:16:14,538][0m Trial 23 finished with value: 0.1283933031923917 and parameters: {'observation_period_num': 72, 'train_rates': 0.6601040285406404, 'learning_rate': 0.00010484981704292272, 'batch_size': 176, 'step_size': 11, 'gamma': 0.8783634710578185}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:16:54,659][0m Trial 24 finished with value: 0.11303834064028492 and parameters: {'observation_period_num': 102, 'train_rates': 0.7012088977998228, 'learning_rate': 0.0004243456150068551, 'batch_size': 121, 'step_size': 7, 'gamma': 0.9147034903918637}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:17:52,084][0m Trial 25 finished with value: 0.14364671259067255 and parameters: {'observation_period_num': 132, 'train_rates': 0.7991117870129759, 'learning_rate': 0.0002430618489426247, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9244746772760283}. Best is trial 9 with value: 0.10089199502198469.[0m
[32m[I 2025-01-31 21:18:33,368][0m Trial 26 finished with value: 0.09509190634346679 and parameters: {'observation_period_num': 34, 'train_rates': 0.7075286490117045, 'learning_rate': 5.516027732098569e-05, 'batch_size': 120, 'step_size': 11, 'gamma': 0.917643573968212}. Best is trial 26 with value: 0.09509190634346679.[0m
[32m[I 2025-01-31 21:19:13,313][0m Trial 27 finished with value: 0.0899588397393624 and parameters: {'observation_period_num': 35, 'train_rates': 0.7018635989187049, 'learning_rate': 0.0004036517105996409, 'batch_size': 127, 'step_size': 9, 'gamma': 0.971721231511}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:19:44,959][0m Trial 28 finished with value: 0.1771755744873638 and parameters: {'observation_period_num': 29, 'train_rates': 0.635925590595891, 'learning_rate': 4.6294670646497166e-05, 'batch_size': 147, 'step_size': 9, 'gamma': 0.9762781688683273}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:20:49,977][0m Trial 29 finished with value: 0.19177007801945073 and parameters: {'observation_period_num': 59, 'train_rates': 0.752276694512206, 'learning_rate': 5.911305790275483e-06, 'batch_size': 76, 'step_size': 11, 'gamma': 0.9836856132172035}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:21:23,902][0m Trial 30 finished with value: 1.3525404013788331 and parameters: {'observation_period_num': 57, 'train_rates': 0.6211107568958185, 'learning_rate': 1.1289050940822257e-06, 'batch_size': 137, 'step_size': 5, 'gamma': 0.869415698566866}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:22:03,302][0m Trial 31 finished with value: 0.09202747360656136 and parameters: {'observation_period_num': 37, 'train_rates': 0.7069116370648698, 'learning_rate': 0.00042118498323646735, 'batch_size': 126, 'step_size': 9, 'gamma': 0.922295141089732}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:22:58,222][0m Trial 32 finished with value: 0.14277601832009013 and parameters: {'observation_period_num': 27, 'train_rates': 0.6996810972302812, 'learning_rate': 0.0005564947358053662, 'batch_size': 87, 'step_size': 9, 'gamma': 0.9606242162900587}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:23:25,735][0m Trial 33 finished with value: 0.10675748911664242 and parameters: {'observation_period_num': 42, 'train_rates': 0.7818805946285503, 'learning_rate': 0.00019046701189822648, 'batch_size': 202, 'step_size': 11, 'gamma': 0.9558847895853417}. Best is trial 27 with value: 0.0899588397393624.[0m
[32m[I 2025-01-31 21:24:07,154][0m Trial 34 finished with value: 0.08277780474267343 and parameters: {'observation_period_num': 18, 'train_rates': 0.7403285268507442, 'learning_rate': 0.0004439578480906209, 'batch_size': 121, 'step_size': 10, 'gamma': 0.9254058562280171}. Best is trial 34 with value: 0.08277780474267343.[0m
[32m[I 2025-01-31 21:24:50,732][0m Trial 35 finished with value: 0.08465526906274924 and parameters: {'observation_period_num': 17, 'train_rates': 0.7444135905023548, 'learning_rate': 0.0001434324552787856, 'batch_size': 120, 'step_size': 9, 'gamma': 0.925934618152519}. Best is trial 34 with value: 0.08277780474267343.[0m
[32m[I 2025-01-31 21:25:29,012][0m Trial 36 finished with value: 0.09463797178238736 and parameters: {'observation_period_num': 17, 'train_rates': 0.8086216654203278, 'learning_rate': 0.0001465239773760257, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9335556340216585}. Best is trial 34 with value: 0.08277780474267343.[0m
[32m[I 2025-01-31 21:26:20,296][0m Trial 37 finished with value: 0.08157009221463797 and parameters: {'observation_period_num': 8, 'train_rates': 0.7466452296578815, 'learning_rate': 0.0005592285731801036, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9633486064419011}. Best is trial 37 with value: 0.08157009221463797.[0m
[32m[I 2025-01-31 21:27:57,720][0m Trial 38 finished with value: 0.07764557850586766 and parameters: {'observation_period_num': 6, 'train_rates': 0.7449729933958217, 'learning_rate': 0.0005685748003530349, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9895828054511326}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:29:33,519][0m Trial 39 finished with value: 0.0828181818677637 and parameters: {'observation_period_num': 6, 'train_rates': 0.7772935663079914, 'learning_rate': 0.0006626706982853138, 'batch_size': 53, 'step_size': 5, 'gamma': 0.9889851833716626}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:31:05,020][0m Trial 40 finished with value: 0.08426977539399885 and parameters: {'observation_period_num': 9, 'train_rates': 0.7661751520925207, 'learning_rate': 0.0006834993435973806, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9481831744002911}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:32:38,898][0m Trial 41 finished with value: 0.07874930076246714 and parameters: {'observation_period_num': 5, 'train_rates': 0.7677622513839961, 'learning_rate': 0.0006308442006404902, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9879786333097319}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:35:09,923][0m Trial 42 finished with value: 0.08344434398152341 and parameters: {'observation_period_num': 7, 'train_rates': 0.7826671655712906, 'learning_rate': 0.000652083988027619, 'batch_size': 33, 'step_size': 1, 'gamma': 0.9864522112175139}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:37:15,580][0m Trial 43 finished with value: 0.0999643049760268 and parameters: {'observation_period_num': 22, 'train_rates': 0.8166739743613328, 'learning_rate': 0.00027355424915147806, 'batch_size': 41, 'step_size': 3, 'gamma': 0.96409582090217}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:38:40,103][0m Trial 44 finished with value: 0.1454927803257878 and parameters: {'observation_period_num': 55, 'train_rates': 0.8773449027635726, 'learning_rate': 0.0005795030077249438, 'batch_size': 64, 'step_size': 1, 'gamma': 0.9857901949805127}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:42:01,060][0m Trial 45 finished with value: 0.08770344762698463 and parameters: {'observation_period_num': 6, 'train_rates': 0.7425348326490058, 'learning_rate': 0.0007348197152236263, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9762374824306753}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:43:17,439][0m Trial 46 finished with value: 0.4472198013348278 and parameters: {'observation_period_num': 47, 'train_rates': 0.7766984430147144, 'learning_rate': 1.6799124489509313e-06, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9543635578529542}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:45:29,901][0m Trial 47 finished with value: 0.3331801767296651 and parameters: {'observation_period_num': 23, 'train_rates': 0.9711715558159705, 'learning_rate': 0.00019695044367264774, 'batch_size': 44, 'step_size': 2, 'gamma': 0.9898468516784519}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:46:28,701][0m Trial 48 finished with value: 0.09289125379159617 and parameters: {'observation_period_num': 5, 'train_rates': 0.8352235639729862, 'learning_rate': 0.0003326375890761543, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9412861300103921}. Best is trial 38 with value: 0.07764557850586766.[0m
[32m[I 2025-01-31 21:47:31,900][0m Trial 49 finished with value: 0.12335607388332014 and parameters: {'observation_period_num': 68, 'train_rates': 0.7925921653135538, 'learning_rate': 0.0009896612291405522, 'batch_size': 79, 'step_size': 5, 'gamma': 0.9721715764113148}. Best is trial 38 with value: 0.07764557850586766.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-31 21:47:31,910][0m A new study created in memory with name: no-name-f0012963-9cc1-461c-b737-8192eab6e3a7[0m
Early stopping at epoch 76
[32m[I 2025-01-31 21:50:01,452][0m Trial 0 finished with value: 0.12639879114742922 and parameters: {'observation_period_num': 43, 'train_rates': 0.8461961921485238, 'learning_rate': 0.0002593816813473081, 'batch_size': 27, 'step_size': 1, 'gamma': 0.7577159525370728}. Best is trial 0 with value: 0.12639879114742922.[0m
[32m[I 2025-01-31 21:50:41,636][0m Trial 1 finished with value: 0.29484299518550633 and parameters: {'observation_period_num': 64, 'train_rates': 0.6564679767870227, 'learning_rate': 9.482799319539274e-06, 'batch_size': 118, 'step_size': 8, 'gamma': 0.8089540453974251}. Best is trial 0 with value: 0.12639879114742922.[0m
Early stopping at epoch 98
[32m[I 2025-01-31 21:51:59,258][0m Trial 2 finished with value: 0.7344851107664512 and parameters: {'observation_period_num': 46, 'train_rates': 0.951466922992003, 'learning_rate': 3.75127901664886e-06, 'batch_size': 74, 'step_size': 1, 'gamma': 0.8979443982327389}. Best is trial 0 with value: 0.12639879114742922.[0m
[32m[I 2025-01-31 21:52:33,796][0m Trial 3 finished with value: 0.14290126518807197 and parameters: {'observation_period_num': 45, 'train_rates': 0.9008892317422057, 'learning_rate': 0.0002932246724313386, 'batch_size': 174, 'step_size': 4, 'gamma': 0.9302732428292402}. Best is trial 0 with value: 0.12639879114742922.[0m
[32m[I 2025-01-31 21:54:40,826][0m Trial 4 finished with value: 0.2019697622112606 and parameters: {'observation_period_num': 240, 'train_rates': 0.8390533543733643, 'learning_rate': 2.2530260508571847e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7818342171365291}. Best is trial 0 with value: 0.12639879114742922.[0m
[32m[I 2025-01-31 21:55:15,088][0m Trial 5 finished with value: 0.06238375127054097 and parameters: {'observation_period_num': 9, 'train_rates': 0.6129333912368159, 'learning_rate': 0.0006519925333204811, 'batch_size': 130, 'step_size': 15, 'gamma': 0.8865813595721506}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 21:55:53,723][0m Trial 6 finished with value: 0.2299070805311203 and parameters: {'observation_period_num': 81, 'train_rates': 0.988548228626712, 'learning_rate': 5.641372712567082e-06, 'batch_size': 164, 'step_size': 11, 'gamma': 0.934363280592384}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 21:57:42,348][0m Trial 7 finished with value: 0.1768193580736545 and parameters: {'observation_period_num': 75, 'train_rates': 0.7787897841131182, 'learning_rate': 2.676939926198483e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8242008210232555}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 21:58:09,250][0m Trial 8 finished with value: 0.4537317596678364 and parameters: {'observation_period_num': 195, 'train_rates': 0.8034826038465852, 'learning_rate': 8.86412327188362e-06, 'batch_size': 187, 'step_size': 14, 'gamma': 0.7536700117181637}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 21:59:00,666][0m Trial 9 finished with value: 0.13185455110832014 and parameters: {'observation_period_num': 174, 'train_rates': 0.7564353524687659, 'learning_rate': 0.0003232105321208422, 'batch_size': 94, 'step_size': 8, 'gamma': 0.8261713731281025}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 21:59:20,822][0m Trial 10 finished with value: 0.13435512175226608 and parameters: {'observation_period_num': 119, 'train_rates': 0.6166824697617157, 'learning_rate': 0.0009773340915658468, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9859210515124723}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:03:38,135][0m Trial 11 finished with value: 0.06379578854193405 and parameters: {'observation_period_num': 5, 'train_rates': 0.705875769565521, 'learning_rate': 0.00013366949546477343, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8705914232831609}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:04:02,278][0m Trial 12 finished with value: 0.09210669995191392 and parameters: {'observation_period_num': 5, 'train_rates': 0.6982038531632252, 'learning_rate': 0.0001280252806994408, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8698465248990586}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:04:40,280][0m Trial 13 finished with value: 0.09218671963070378 and parameters: {'observation_period_num': 13, 'train_rates': 0.6971400498588394, 'learning_rate': 9.549731689705894e-05, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8751031284142623}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:05:32,633][0m Trial 14 finished with value: 0.12383643626122462 and parameters: {'observation_period_num': 112, 'train_rates': 0.6084883425709915, 'learning_rate': 0.0006748657630797441, 'batch_size': 82, 'step_size': 12, 'gamma': 0.858967999990908}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:10:04,448][0m Trial 15 finished with value: 0.06497164598137838 and parameters: {'observation_period_num': 9, 'train_rates': 0.7097657663791342, 'learning_rate': 7.013144126176009e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9141978769609062}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:11:13,874][0m Trial 16 finished with value: 0.13340839585058453 and parameters: {'observation_period_num': 165, 'train_rates': 0.6528730338098715, 'learning_rate': 6.049919504338458e-05, 'batch_size': 62, 'step_size': 10, 'gamma': 0.9830074384933889}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:11:59,678][0m Trial 17 finished with value: 0.11519070141893396 and parameters: {'observation_period_num': 104, 'train_rates': 0.7345452946615861, 'learning_rate': 0.0005131141119036801, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8453024946532111}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:12:29,227][0m Trial 18 finished with value: 0.14185559454796126 and parameters: {'observation_period_num': 153, 'train_rates': 0.6527838953632821, 'learning_rate': 0.00023786496180907155, 'batch_size': 154, 'step_size': 3, 'gamma': 0.8988947383568433}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:12:53,588][0m Trial 19 finished with value: 0.09257162395082684 and parameters: {'observation_period_num': 32, 'train_rates': 0.669178101585049, 'learning_rate': 0.00014102182510745883, 'batch_size': 197, 'step_size': 9, 'gamma': 0.9574206537723943}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:13:13,715][0m Trial 20 finished with value: 1.9234287325015738 and parameters: {'observation_period_num': 95, 'train_rates': 0.6191636827141999, 'learning_rate': 1.1406582058312776e-06, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8853687407917787}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:18:03,823][0m Trial 21 finished with value: 0.07468029057249805 and parameters: {'observation_period_num': 19, 'train_rates': 0.7177795569578886, 'learning_rate': 5.181702856092951e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9183908197134292}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:19:34,435][0m Trial 22 finished with value: 0.0747521876567013 and parameters: {'observation_period_num': 6, 'train_rates': 0.7391763005939067, 'learning_rate': 6.442561339499325e-05, 'batch_size': 54, 'step_size': 7, 'gamma': 0.9081114769445487}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:23:32,728][0m Trial 23 finished with value: 0.08183560208107034 and parameters: {'observation_period_num': 30, 'train_rates': 0.6922696983186889, 'learning_rate': 1.7800832329385324e-05, 'batch_size': 19, 'step_size': 9, 'gamma': 0.9454409882065685}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:24:26,584][0m Trial 24 finished with value: 0.11403730793547592 and parameters: {'observation_period_num': 65, 'train_rates': 0.7797582088020854, 'learning_rate': 0.00016707227303638963, 'batch_size': 94, 'step_size': 4, 'gamma': 0.8401876880590329}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:26:31,880][0m Trial 25 finished with value: 0.18661641388466585 and parameters: {'observation_period_num': 139, 'train_rates': 0.8249522512368809, 'learning_rate': 0.0005042769351634818, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8858683220793782}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:27:35,335][0m Trial 26 finished with value: 0.12462085314381313 and parameters: {'observation_period_num': 55, 'train_rates': 0.6320537382581021, 'learning_rate': 3.995549281657882e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.8580245461991826}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:28:10,831][0m Trial 27 finished with value: 0.09340544691057945 and parameters: {'observation_period_num': 21, 'train_rates': 0.6791215649986574, 'learning_rate': 8.754052834938637e-05, 'batch_size': 140, 'step_size': 3, 'gamma': 0.9613808020619519}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:28:34,298][0m Trial 28 finished with value: 0.07998866565570147 and parameters: {'observation_period_num': 30, 'train_rates': 0.7146067179383637, 'learning_rate': 0.000460133654466866, 'batch_size': 220, 'step_size': 5, 'gamma': 0.9203412406137185}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:30:52,378][0m Trial 29 finished with value: 0.10830016622377425 and parameters: {'observation_period_num': 39, 'train_rates': 0.6007617174967819, 'learning_rate': 0.00019304412568988587, 'batch_size': 30, 'step_size': 2, 'gamma': 0.7939147891794941}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:32:32,285][0m Trial 30 finished with value: 0.17850984237156808 and parameters: {'observation_period_num': 84, 'train_rates': 0.8663129812907178, 'learning_rate': 0.0009375177038901922, 'batch_size': 54, 'step_size': 9, 'gamma': 0.8839777566651525}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:37:06,567][0m Trial 31 finished with value: 0.07373455894031138 and parameters: {'observation_period_num': 19, 'train_rates': 0.7245479644881061, 'learning_rate': 4.1746063235246456e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9145917786226747}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:40:37,998][0m Trial 32 finished with value: 0.07461381483669967 and parameters: {'observation_period_num': 7, 'train_rates': 0.7525441426827254, 'learning_rate': 3.5302993933571185e-05, 'batch_size': 23, 'step_size': 7, 'gamma': 0.9114437758216818}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:42:41,767][0m Trial 33 finished with value: 0.16088941964301176 and parameters: {'observation_period_num': 55, 'train_rates': 0.6407558630330947, 'learning_rate': 1.4902879188794593e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8991015827024658}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:44:13,462][0m Trial 34 finished with value: 0.08860419906812943 and parameters: {'observation_period_num': 23, 'train_rates': 0.7221023233811712, 'learning_rate': 9.581458334605069e-05, 'batch_size': 52, 'step_size': 8, 'gamma': 0.934131418331526}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:48:47,652][0m Trial 35 finished with value: 0.0984564123985668 and parameters: {'observation_period_num': 47, 'train_rates': 0.6698431006754513, 'learning_rate': 0.0003174006215742964, 'batch_size': 16, 'step_size': 1, 'gamma': 0.8666323477563318}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:49:32,451][0m Trial 36 finished with value: 0.24973817953696617 and parameters: {'observation_period_num': 40, 'train_rates': 0.7771756412523293, 'learning_rate': 1.111992718570625e-05, 'batch_size': 118, 'step_size': 4, 'gamma': 0.8966815350633499}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:50:39,326][0m Trial 37 finished with value: 0.20013054466880528 and parameters: {'observation_period_num': 224, 'train_rates': 0.876065774223392, 'learning_rate': 4.0732114016405925e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9527064937927404}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:52:39,479][0m Trial 38 finished with value: 0.11248619932466776 and parameters: {'observation_period_num': 61, 'train_rates': 0.8019424544082733, 'learning_rate': 2.2558548858700374e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.9278719698172252}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:55:13,311][0m Trial 39 finished with value: 0.20965567692094841 and parameters: {'observation_period_num': 71, 'train_rates': 0.7618661685998342, 'learning_rate': 5.8734386530394996e-06, 'batch_size': 31, 'step_size': 6, 'gamma': 0.847831954745292}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 22:56:40,504][0m Trial 40 finished with value: 0.21551260035836473 and parameters: {'observation_period_num': 23, 'train_rates': 0.945806444208043, 'learning_rate': 7.649962984480259e-05, 'batch_size': 66, 'step_size': 4, 'gamma': 0.9701039499629602}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:00:00,480][0m Trial 41 finished with value: 0.07353265698139484 and parameters: {'observation_period_num': 8, 'train_rates': 0.748688698294747, 'learning_rate': 3.701005594473882e-05, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9114439575588144}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:02:51,703][0m Trial 42 finished with value: 0.07889115660994624 and parameters: {'observation_period_num': 18, 'train_rates': 0.7383511331371415, 'learning_rate': 2.7462264035627586e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.9416829848613221}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:04:30,030][0m Trial 43 finished with value: 0.09822962684750419 and parameters: {'observation_period_num': 47, 'train_rates': 0.7022723399992379, 'learning_rate': 4.84569562641954e-05, 'batch_size': 47, 'step_size': 5, 'gamma': 0.9084169519911445}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:06:56,886][0m Trial 44 finished with value: 0.06501953488094801 and parameters: {'observation_period_num': 11, 'train_rates': 0.6809682269051309, 'learning_rate': 0.00012198555533164134, 'batch_size': 31, 'step_size': 7, 'gamma': 0.876001241929134}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:08:14,167][0m Trial 45 finished with value: 0.06677915909133114 and parameters: {'observation_period_num': 9, 'train_rates': 0.6910502318741938, 'learning_rate': 0.00011995409912114964, 'batch_size': 60, 'step_size': 10, 'gamma': 0.8314391768516444}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:09:02,782][0m Trial 46 finished with value: 0.08481016737194015 and parameters: {'observation_period_num': 31, 'train_rates': 0.6797804430763973, 'learning_rate': 0.0001159254208627927, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8188056911078789}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:10:15,366][0m Trial 47 finished with value: 0.07733353553168008 and parameters: {'observation_period_num': 37, 'train_rates': 0.6283260473666992, 'learning_rate': 0.00022071633612697386, 'batch_size': 60, 'step_size': 13, 'gamma': 0.836413610206108}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:11:06,784][0m Trial 48 finished with value: 0.06503740564772957 and parameters: {'observation_period_num': 13, 'train_rates': 0.6447649860236977, 'learning_rate': 0.0003371891825812567, 'batch_size': 88, 'step_size': 14, 'gamma': 0.7718012252247266}. Best is trial 5 with value: 0.06238375127054097.[0m
[32m[I 2025-01-31 23:11:42,900][0m Trial 49 finished with value: 0.08944724213393965 and parameters: {'observation_period_num': 50, 'train_rates': 0.6484954731658269, 'learning_rate': 0.0004200478388935941, 'batch_size': 134, 'step_size': 14, 'gamma': 0.7649632131440752}. Best is trial 5 with value: 0.06238375127054097.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-31 23:11:42,910][0m A new study created in memory with name: no-name-3399b11e-f90a-4810-9041-f4ac905f5b6b[0m
[32m[I 2025-01-31 23:12:57,878][0m Trial 0 finished with value: 0.13122284881077534 and parameters: {'observation_period_num': 91, 'train_rates': 0.8499308112682562, 'learning_rate': 5.83854356408374e-05, 'batch_size': 71, 'step_size': 13, 'gamma': 0.8101288334580139}. Best is trial 0 with value: 0.13122284881077534.[0m
[32m[I 2025-01-31 23:13:30,385][0m Trial 1 finished with value: 0.10944678044677202 and parameters: {'observation_period_num': 96, 'train_rates': 0.7031036356884, 'learning_rate': 0.00025734325459619617, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9208305264749215}. Best is trial 1 with value: 0.10944678044677202.[0m
Early stopping at epoch 45
[32m[I 2025-01-31 23:13:41,324][0m Trial 2 finished with value: 0.6246977936930773 and parameters: {'observation_period_num': 93, 'train_rates': 0.8569118680798062, 'learning_rate': 5.3325420595165685e-05, 'batch_size': 256, 'step_size': 1, 'gamma': 0.7668835410393819}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:14:05,657][0m Trial 3 finished with value: 0.6429545879364014 and parameters: {'observation_period_num': 198, 'train_rates': 0.9374594999017288, 'learning_rate': 9.10117801383829e-06, 'batch_size': 238, 'step_size': 4, 'gamma': 0.8261873403495745}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:19:28,096][0m Trial 4 finished with value: 0.20588269329253045 and parameters: {'observation_period_num': 186, 'train_rates': 0.9056478045127444, 'learning_rate': 3.68156812898727e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9216539061941911}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:20:06,595][0m Trial 5 finished with value: 0.25737893803413414 and parameters: {'observation_period_num': 7, 'train_rates': 0.7631608919529052, 'learning_rate': 3.822744633906616e-06, 'batch_size': 136, 'step_size': 7, 'gamma': 0.8676727579975081}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:20:30,943][0m Trial 6 finished with value: 0.19631534145148688 and parameters: {'observation_period_num': 203, 'train_rates': 0.6873494631800396, 'learning_rate': 0.0007067672818224808, 'batch_size': 191, 'step_size': 9, 'gamma': 0.9164204706669614}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:21:11,219][0m Trial 7 finished with value: 0.30110382527675267 and parameters: {'observation_period_num': 43, 'train_rates': 0.9455827514469874, 'learning_rate': 2.4797916570090337e-05, 'batch_size': 152, 'step_size': 10, 'gamma': 0.8269510549537237}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:21:36,222][0m Trial 8 finished with value: 0.26230066693627957 and parameters: {'observation_period_num': 157, 'train_rates': 0.6532197124190522, 'learning_rate': 3.9931294850030074e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.7675914147532252}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:22:01,509][0m Trial 9 finished with value: 0.5809219830011492 and parameters: {'observation_period_num': 213, 'train_rates': 0.6122352248634284, 'learning_rate': 6.382281011262028e-06, 'batch_size': 173, 'step_size': 13, 'gamma': 0.9384988168913013}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:22:49,910][0m Trial 10 finished with value: 0.1446797430968087 and parameters: {'observation_period_num': 119, 'train_rates': 0.7448365756721552, 'learning_rate': 0.0005913611910716833, 'batch_size': 101, 'step_size': 15, 'gamma': 0.9873568530921206}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:23:54,105][0m Trial 11 finished with value: 0.11454366857787514 and parameters: {'observation_period_num': 70, 'train_rates': 0.8217151229440058, 'learning_rate': 0.00014391786501702274, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8367203331244232}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:25:05,469][0m Trial 12 finished with value: 0.1250324347534695 and parameters: {'observation_period_num': 68, 'train_rates': 0.8078809933233306, 'learning_rate': 0.00020787110939205684, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8959176199478213}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:25:49,054][0m Trial 13 finished with value: 0.6130881253370896 and parameters: {'observation_period_num': 140, 'train_rates': 0.7190736922644186, 'learning_rate': 1.0865258871473438e-06, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8690270330538465}. Best is trial 1 with value: 0.10944678044677202.[0m
[32m[I 2025-01-31 23:27:52,933][0m Trial 14 finished with value: 0.10907268489404798 and parameters: {'observation_period_num': 44, 'train_rates': 0.7995193141568863, 'learning_rate': 0.00019065989025873796, 'batch_size': 41, 'step_size': 12, 'gamma': 0.9892233767134367}. Best is trial 14 with value: 0.10907268489404798.[0m
[32m[I 2025-01-31 23:31:33,674][0m Trial 15 finished with value: 0.2193826584314758 and parameters: {'observation_period_num': 246, 'train_rates': 0.6912299082190511, 'learning_rate': 0.00025300136256338636, 'batch_size': 19, 'step_size': 11, 'gamma': 0.9782810369147481}. Best is trial 14 with value: 0.10907268489404798.[0m
[32m[I 2025-01-31 23:33:12,427][0m Trial 16 finished with value: 0.07396681464802911 and parameters: {'observation_period_num': 9, 'train_rates': 0.7631594406147075, 'learning_rate': 0.00012030824842205252, 'batch_size': 50, 'step_size': 12, 'gamma': 0.9540238045873749}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:34:59,648][0m Trial 17 finished with value: 0.07457617004467568 and parameters: {'observation_period_num': 7, 'train_rates': 0.7592253098162457, 'learning_rate': 9.942505518174847e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.959256428406076}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:36:59,334][0m Trial 18 finished with value: 0.12601322545044458 and parameters: {'observation_period_num': 6, 'train_rates': 0.8864658820465343, 'learning_rate': 1.7106151610134537e-05, 'batch_size': 46, 'step_size': 5, 'gamma': 0.9583535588287861}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:37:44,422][0m Trial 19 finished with value: 0.09622365623733121 and parameters: {'observation_period_num': 31, 'train_rates': 0.7524934954843151, 'learning_rate': 0.00010556590220201314, 'batch_size': 113, 'step_size': 4, 'gamma': 0.9529179633165752}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:39:28,437][0m Trial 20 finished with value: 0.14686187611711185 and parameters: {'observation_period_num': 27, 'train_rates': 0.6426254732940546, 'learning_rate': 9.13113662567063e-05, 'batch_size': 42, 'step_size': 1, 'gamma': 0.8852993442563576}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:40:15,693][0m Trial 21 finished with value: 0.09588622848329308 and parameters: {'observation_period_num': 31, 'train_rates': 0.751028721355702, 'learning_rate': 9.644752288698808e-05, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9571215947619198}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:41:16,847][0m Trial 22 finished with value: 0.074702706806023 and parameters: {'observation_period_num': 5, 'train_rates': 0.7762968624029457, 'learning_rate': 0.00041249343650019054, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9584993717665696}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:42:35,940][0m Trial 23 finished with value: 0.1092416665309026 and parameters: {'observation_period_num': 56, 'train_rates': 0.7790042320762244, 'learning_rate': 0.0003386723207767606, 'batch_size': 63, 'step_size': 6, 'gamma': 0.941679699157638}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:43:34,263][0m Trial 24 finished with value: 0.08962979923454252 and parameters: {'observation_period_num': 12, 'train_rates': 0.8283421214391302, 'learning_rate': 0.0004722142569526098, 'batch_size': 91, 'step_size': 8, 'gamma': 0.9745901935790985}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:45:05,347][0m Trial 25 finished with value: 0.08736564832463804 and parameters: {'observation_period_num': 26, 'train_rates': 0.731298182738097, 'learning_rate': 0.000940058746288591, 'batch_size': 53, 'step_size': 3, 'gamma': 0.8989498341928229}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:47:44,524][0m Trial 26 finished with value: 0.11869792422404685 and parameters: {'observation_period_num': 63, 'train_rates': 0.7835291638037606, 'learning_rate': 0.0004800781504901582, 'batch_size': 31, 'step_size': 8, 'gamma': 0.9338922906829027}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:48:38,385][0m Trial 27 finished with value: 0.10131121416731427 and parameters: {'observation_period_num': 51, 'train_rates': 0.6694968892721047, 'learning_rate': 6.696045428087992e-05, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9664658851700132}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:49:58,613][0m Trial 28 finished with value: 0.08683306325503137 and parameters: {'observation_period_num': 19, 'train_rates': 0.7195063339257936, 'learning_rate': 0.00015803887699698149, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9080455834620947}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:50:39,932][0m Trial 29 finished with value: 0.14556570922627168 and parameters: {'observation_period_num': 110, 'train_rates': 0.8506181862148241, 'learning_rate': 0.0003403074343240002, 'batch_size': 129, 'step_size': 2, 'gamma': 0.8519472958297805}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:51:55,436][0m Trial 30 finished with value: 0.18103146029543946 and parameters: {'observation_period_num': 79, 'train_rates': 0.8809929161863943, 'learning_rate': 1.3994052451713649e-05, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9465525936930054}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:53:11,960][0m Trial 31 finished with value: 0.08460644382412416 and parameters: {'observation_period_num': 20, 'train_rates': 0.7199407642517268, 'learning_rate': 0.00014203082657528106, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9118745903232935}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:55:40,743][0m Trial 32 finished with value: 0.07555321750582242 and parameters: {'observation_period_num': 5, 'train_rates': 0.7725560529124281, 'learning_rate': 6.404468132191052e-05, 'batch_size': 33, 'step_size': 11, 'gamma': 0.927259637929576}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-01-31 23:58:14,245][0m Trial 33 finished with value: 0.09871965468180854 and parameters: {'observation_period_num': 38, 'train_rates': 0.7763096272424367, 'learning_rate': 8.274203189160394e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.9305215454907837}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:01:06,450][0m Trial 34 finished with value: 0.08028165791576974 and parameters: {'observation_period_num': 9, 'train_rates': 0.8231040170282059, 'learning_rate': 4.638854780152104e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.9681875949538661}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:02:45,686][0m Trial 35 finished with value: 0.12223448013171886 and parameters: {'observation_period_num': 78, 'train_rates': 0.7980030711379134, 'learning_rate': 2.644375943465175e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9276332822274762}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:07:34,366][0m Trial 36 finished with value: 0.09641064366109893 and parameters: {'observation_period_num': 6, 'train_rates': 0.8419539758043535, 'learning_rate': 5.213578081971501e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.7837835423731043}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:08:29,575][0m Trial 37 finished with value: 0.12250519084028391 and parameters: {'observation_period_num': 42, 'train_rates': 0.768597051076234, 'learning_rate': 0.0003230051243175153, 'batch_size': 93, 'step_size': 11, 'gamma': 0.9519958062025267}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:10:38,145][0m Trial 38 finished with value: 0.10815652598619178 and parameters: {'observation_period_num': 92, 'train_rates': 0.705675613989448, 'learning_rate': 6.259173724147601e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8838424250950085}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:11:54,025][0m Trial 39 finished with value: 0.29701651348156877 and parameters: {'observation_period_num': 17, 'train_rates': 0.9698105581148307, 'learning_rate': 0.00012707468252626215, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9652190586669763}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:12:17,367][0m Trial 40 finished with value: 0.21909874323450151 and parameters: {'observation_period_num': 154, 'train_rates': 0.7387419515521659, 'learning_rate': 3.707259959711687e-05, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9203061595068802}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:15:34,669][0m Trial 41 finished with value: 0.08374524273147638 and parameters: {'observation_period_num': 8, 'train_rates': 0.8208451569178243, 'learning_rate': 4.7967696470537706e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.9764913894694195}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:17:13,459][0m Trial 42 finished with value: 0.12556850617485388 and parameters: {'observation_period_num': 33, 'train_rates': 0.8682736627744244, 'learning_rate': 2.2583912903330005e-05, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9650996807018173}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:20:12,516][0m Trial 43 finished with value: 0.07819701365071305 and parameters: {'observation_period_num': 18, 'train_rates': 0.7621173970127137, 'learning_rate': 6.728974761989978e-05, 'batch_size': 27, 'step_size': 5, 'gamma': 0.9768733073712053}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:22:01,206][0m Trial 44 finished with value: 0.08197091768185298 and parameters: {'observation_period_num': 19, 'train_rates': 0.7610161257020428, 'learning_rate': 7.446509660506452e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.9420652775400901}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:27:06,009][0m Trial 45 finished with value: 0.10069827950971633 and parameters: {'observation_period_num': 49, 'train_rates': 0.7877759985638305, 'learning_rate': 3.162789705957481e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.98205073010731}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:28:17,440][0m Trial 46 finished with value: 0.17964690956927587 and parameters: {'observation_period_num': 178, 'train_rates': 0.8064833663530571, 'learning_rate': 0.00020879032667909102, 'batch_size': 69, 'step_size': 14, 'gamma': 0.9898542389373458}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:30:22,655][0m Trial 47 finished with value: 0.09109525499503432 and parameters: {'observation_period_num': 23, 'train_rates': 0.7348403506971696, 'learning_rate': 0.00012034268087780339, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9504166319195858}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:30:55,301][0m Trial 48 finished with value: 0.19901984698696223 and parameters: {'observation_period_num': 59, 'train_rates': 0.6975389415544409, 'learning_rate': 1.0865539753618345e-05, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9258894835452913}. Best is trial 16 with value: 0.07396681464802911.[0m
[32m[I 2025-02-01 00:31:35,662][0m Trial 49 finished with value: 0.1900234039677926 and parameters: {'observation_period_num': 5, 'train_rates': 0.7602892740394104, 'learning_rate': 3.4015967949458916e-06, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9725555394095364}. Best is trial 16 with value: 0.07396681464802911.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-01 00:31:35,673][0m A new study created in memory with name: no-name-8cbb68bb-774b-4fb7-881d-f53bd5f409e7[0m
[32m[I 2025-02-01 00:34:30,568][0m Trial 0 finished with value: 0.6519711742985923 and parameters: {'observation_period_num': 200, 'train_rates': 0.9616515777385972, 'learning_rate': 1.0755513735236974e-06, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8766565508587445}. Best is trial 0 with value: 0.6519711742985923.[0m
[32m[I 2025-02-01 00:35:51,518][0m Trial 1 finished with value: 0.1599583601523912 and parameters: {'observation_period_num': 231, 'train_rates': 0.8389996224145494, 'learning_rate': 0.00035297630546271487, 'batch_size': 61, 'step_size': 2, 'gamma': 0.9276440947768918}. Best is trial 1 with value: 0.1599583601523912.[0m
[32m[I 2025-02-01 00:36:20,113][0m Trial 2 finished with value: 0.3651864230632782 and parameters: {'observation_period_num': 236, 'train_rates': 0.9497735941871517, 'learning_rate': 3.344901548409223e-05, 'batch_size': 204, 'step_size': 13, 'gamma': 0.912567393545002}. Best is trial 1 with value: 0.1599583601523912.[0m
[32m[I 2025-02-01 00:37:18,034][0m Trial 3 finished with value: 0.4828349632806465 and parameters: {'observation_period_num': 16, 'train_rates': 0.7510640412140214, 'learning_rate': 1.631569025384355e-06, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8760862884038191}. Best is trial 1 with value: 0.1599583601523912.[0m
[32m[I 2025-02-01 00:38:04,947][0m Trial 4 finished with value: 0.18413646655090468 and parameters: {'observation_period_num': 72, 'train_rates': 0.6803852001555849, 'learning_rate': 1.0992224931550734e-05, 'batch_size': 101, 'step_size': 15, 'gamma': 0.8581847505018105}. Best is trial 1 with value: 0.1599583601523912.[0m
[32m[I 2025-02-01 00:38:29,202][0m Trial 5 finished with value: 0.5238971710205078 and parameters: {'observation_period_num': 148, 'train_rates': 0.9606671087747022, 'learning_rate': 1.6933680717324057e-05, 'batch_size': 253, 'step_size': 8, 'gamma': 0.881312477072689}. Best is trial 1 with value: 0.1599583601523912.[0m
[32m[I 2025-02-01 00:39:32,268][0m Trial 6 finished with value: 0.13086887969253871 and parameters: {'observation_period_num': 18, 'train_rates': 0.8455068816105781, 'learning_rate': 0.00010616565006754031, 'batch_size': 86, 'step_size': 2, 'gamma': 0.8417949077839956}. Best is trial 6 with value: 0.13086887969253871.[0m
[32m[I 2025-02-01 00:40:37,581][0m Trial 7 finished with value: 0.3376217108236719 and parameters: {'observation_period_num': 171, 'train_rates': 0.9207094575502499, 'learning_rate': 8.191707782842435e-06, 'batch_size': 83, 'step_size': 13, 'gamma': 0.9432882317916444}. Best is trial 6 with value: 0.13086887969253871.[0m
[32m[I 2025-02-01 00:41:16,563][0m Trial 8 finished with value: 0.09660837917498828 and parameters: {'observation_period_num': 53, 'train_rates': 0.7286168026495543, 'learning_rate': 0.00048448304294062183, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9872659041145789}. Best is trial 8 with value: 0.09660837917498828.[0m
[32m[I 2025-02-01 00:41:42,097][0m Trial 9 finished with value: 0.8821611223985739 and parameters: {'observation_period_num': 155, 'train_rates': 0.7237805365333122, 'learning_rate': 1.2299881324754438e-06, 'batch_size': 202, 'step_size': 4, 'gamma': 0.7977816571589023}. Best is trial 8 with value: 0.09660837917498828.[0m
[32m[I 2025-02-01 00:42:13,273][0m Trial 10 finished with value: 0.10097663704479157 and parameters: {'observation_period_num': 80, 'train_rates': 0.6411087295029276, 'learning_rate': 0.0009638936874375269, 'batch_size': 150, 'step_size': 9, 'gamma': 0.9867677942008667}. Best is trial 8 with value: 0.09660837917498828.[0m
[32m[I 2025-02-01 00:42:42,708][0m Trial 11 finished with value: 0.11081613811707978 and parameters: {'observation_period_num': 83, 'train_rates': 0.6214490092781757, 'learning_rate': 0.0009750537268187312, 'batch_size': 157, 'step_size': 9, 'gamma': 0.9894310878273479}. Best is trial 8 with value: 0.09660837917498828.[0m
[32m[I 2025-02-01 00:43:12,057][0m Trial 12 finished with value: 0.17541907199832615 and parameters: {'observation_period_num': 91, 'train_rates': 0.6021656075820953, 'learning_rate': 0.0009952612821670427, 'batch_size': 145, 'step_size': 8, 'gamma': 0.9812129670051033}. Best is trial 8 with value: 0.09660837917498828.[0m
[32m[I 2025-02-01 00:43:38,557][0m Trial 13 finished with value: 0.09415518897555784 and parameters: {'observation_period_num': 52, 'train_rates': 0.6767330967244943, 'learning_rate': 0.00019342963101435384, 'batch_size': 180, 'step_size': 10, 'gamma': 0.9605823682313415}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:44:06,167][0m Trial 14 finished with value: 0.10430493441827852 and parameters: {'observation_period_num': 45, 'train_rates': 0.7607106476074974, 'learning_rate': 0.00014880160458849, 'batch_size': 189, 'step_size': 11, 'gamma': 0.9469603296556303}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:44:45,555][0m Trial 15 finished with value: 0.127065153978765 and parameters: {'observation_period_num': 114, 'train_rates': 0.6902613105464683, 'learning_rate': 0.0001655175888667789, 'batch_size': 121, 'step_size': 6, 'gamma': 0.7754415992242976}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:45:11,310][0m Trial 16 finished with value: 0.11163147558730818 and parameters: {'observation_period_num': 46, 'train_rates': 0.8191392044418886, 'learning_rate': 0.0003588911934135092, 'batch_size': 236, 'step_size': 10, 'gamma': 0.9577904830402644}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:45:37,876][0m Trial 17 finished with value: 0.1429918959964075 and parameters: {'observation_period_num': 116, 'train_rates': 0.6765170151393742, 'learning_rate': 7.14126562967011e-05, 'batch_size': 179, 'step_size': 6, 'gamma': 0.9119488707633584}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:46:22,266][0m Trial 18 finished with value: 0.09555404569421495 and parameters: {'observation_period_num': 46, 'train_rates': 0.771281912476667, 'learning_rate': 0.0003506736922275031, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9625763623017428}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:50:39,465][0m Trial 19 finished with value: 0.12822764211934862 and parameters: {'observation_period_num': 42, 'train_rates': 0.8897577218030089, 'learning_rate': 3.918733317607668e-05, 'batch_size': 21, 'step_size': 15, 'gamma': 0.8215199364464087}. Best is trial 13 with value: 0.09415518897555784.[0m
[32m[I 2025-02-01 00:51:04,628][0m Trial 20 finished with value: 0.0858645808621536 and parameters: {'observation_period_num': 13, 'train_rates': 0.7846757190551622, 'learning_rate': 0.00021770355727857994, 'batch_size': 228, 'step_size': 14, 'gamma': 0.9093372977389846}. Best is trial 20 with value: 0.0858645808621536.[0m
[32m[I 2025-02-01 00:51:29,810][0m Trial 21 finished with value: 0.08476137676659752 and parameters: {'observation_period_num': 5, 'train_rates': 0.7925587354685818, 'learning_rate': 0.00023649890403614268, 'batch_size': 224, 'step_size': 14, 'gamma': 0.9005649409282086}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:51:56,893][0m Trial 22 finished with value: 0.1204096029309192 and parameters: {'observation_period_num': 10, 'train_rates': 0.8820585757075393, 'learning_rate': 0.0001895762951715513, 'batch_size': 227, 'step_size': 13, 'gamma': 0.9065950135489034}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:52:22,196][0m Trial 23 finished with value: 0.1052600972112013 and parameters: {'observation_period_num': 27, 'train_rates': 0.8007461898189682, 'learning_rate': 7.003449498457597e-05, 'batch_size': 221, 'step_size': 14, 'gamma': 0.8960241796430137}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:52:53,444][0m Trial 24 finished with value: 0.08872948959469795 and parameters: {'observation_period_num': 27, 'train_rates': 0.7780828746718494, 'learning_rate': 0.0002283541833421383, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9314816090664388}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:53:16,871][0m Trial 25 finished with value: 0.09910681690925206 and parameters: {'observation_period_num': 7, 'train_rates': 0.7882647118454978, 'learning_rate': 7.475458401863842e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9302573818818903}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:53:44,277][0m Trial 26 finished with value: 0.1082028259296675 and parameters: {'observation_period_num': 26, 'train_rates': 0.8486075782481172, 'learning_rate': 0.0004174106816721613, 'batch_size': 216, 'step_size': 14, 'gamma': 0.8490097081434297}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:54:05,733][0m Trial 27 finished with value: 0.41247610100592025 and parameters: {'observation_period_num': 63, 'train_rates': 0.7261276116478607, 'learning_rate': 4.451012798213161e-06, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8957165118215982}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:54:37,042][0m Trial 28 finished with value: 0.11933127118806754 and parameters: {'observation_period_num': 101, 'train_rates': 0.8038310728406691, 'learning_rate': 0.0005916109234125275, 'batch_size': 170, 'step_size': 14, 'gamma': 0.9314760575790805}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:55:04,424][0m Trial 29 finished with value: 0.10629788316035456 and parameters: {'observation_period_num': 5, 'train_rates': 0.7813377315746488, 'learning_rate': 4.383945435268438e-05, 'batch_size': 202, 'step_size': 11, 'gamma': 0.8882321176773779}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:55:33,828][0m Trial 30 finished with value: 0.11080872766692332 and parameters: {'observation_period_num': 27, 'train_rates': 0.8645282083245102, 'learning_rate': 0.00024963445685633774, 'batch_size': 195, 'step_size': 12, 'gamma': 0.8610814971611264}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:56:02,260][0m Trial 31 finished with value: 0.08999235671944916 and parameters: {'observation_period_num': 32, 'train_rates': 0.6988596506389534, 'learning_rate': 0.00011477301434465755, 'batch_size': 175, 'step_size': 11, 'gamma': 0.9623250305203948}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:56:34,289][0m Trial 32 finished with value: 0.09508642096930592 and parameters: {'observation_period_num': 31, 'train_rates': 0.7430757012513536, 'learning_rate': 0.00011095334586982142, 'batch_size': 159, 'step_size': 11, 'gamma': 0.9197678298206331}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:57:02,692][0m Trial 33 finished with value: 0.09605054211776669 and parameters: {'observation_period_num': 64, 'train_rates': 0.7038739967230494, 'learning_rate': 0.0002604677723580247, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9393831242430887}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:58:54,385][0m Trial 34 finished with value: 0.18645202290686794 and parameters: {'observation_period_num': 194, 'train_rates': 0.8041223578561613, 'learning_rate': 0.00012012766007839153, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9665048999227276}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:59:22,664][0m Trial 35 finished with value: 0.2097725123167038 and parameters: {'observation_period_num': 220, 'train_rates': 0.9896842906308049, 'learning_rate': 2.1479522486580333e-05, 'batch_size': 211, 'step_size': 13, 'gamma': 0.9082829192439431}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 00:59:48,036][0m Trial 36 finished with value: 0.10433593601334257 and parameters: {'observation_period_num': 34, 'train_rates': 0.8230513908378446, 'learning_rate': 0.00025915084728177066, 'batch_size': 240, 'step_size': 15, 'gamma': 0.9218574429627315}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:00:08,026][0m Trial 37 finished with value: 0.11404074439678388 and parameters: {'observation_period_num': 16, 'train_rates': 0.653074756302614, 'learning_rate': 5.410477233096644e-05, 'batch_size': 243, 'step_size': 11, 'gamma': 0.8775125342943861}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:00:34,308][0m Trial 38 finished with value: 0.09425115211464483 and parameters: {'observation_period_num': 69, 'train_rates': 0.7095147491618121, 'learning_rate': 0.0006083100364718745, 'batch_size': 187, 'step_size': 13, 'gamma': 0.9731031217529406}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:00:57,842][0m Trial 39 finished with value: 0.22373743292765946 and parameters: {'observation_period_num': 17, 'train_rates': 0.7482998675969323, 'learning_rate': 2.0975340000120806e-05, 'batch_size': 223, 'step_size': 1, 'gamma': 0.9455262636036631}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:01:36,098][0m Trial 40 finished with value: 0.08532272728376102 and parameters: {'observation_period_num': 5, 'train_rates': 0.7692331203665141, 'learning_rate': 0.00010910297754883795, 'batch_size': 139, 'step_size': 12, 'gamma': 0.864701208043635}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:02:17,511][0m Trial 41 finished with value: 0.10070379727547711 and parameters: {'observation_period_num': 35, 'train_rates': 0.8272192164043309, 'learning_rate': 0.00010704709046366722, 'batch_size': 132, 'step_size': 12, 'gamma': 0.8625972719208346}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:03:05,186][0m Trial 42 finished with value: 0.26413194053605493 and parameters: {'observation_period_num': 251, 'train_rates': 0.7735184844689913, 'learning_rate': 0.000250950821896187, 'batch_size': 101, 'step_size': 14, 'gamma': 0.8971169139115914}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:03:41,661][0m Trial 43 finished with value: 0.09509876673591548 and parameters: {'observation_period_num': 19, 'train_rates': 0.7530952610846776, 'learning_rate': 8.374106352617997e-05, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8408862870344849}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:04:12,026][0m Trial 44 finished with value: 0.09727675081367834 and parameters: {'observation_period_num': 56, 'train_rates': 0.7305633401460038, 'learning_rate': 0.0006908350228465069, 'batch_size': 168, 'step_size': 9, 'gamma': 0.873359839150744}. Best is trial 21 with value: 0.08476137676659752.[0m
[32m[I 2025-02-01 01:04:46,296][0m Trial 45 finished with value: 0.08447108352393434 and parameters: {'observation_period_num': 6, 'train_rates': 0.7679670402168549, 'learning_rate': 0.00015659613801278138, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8218690431463421}. Best is trial 45 with value: 0.08447108352393434.[0m
[32m[I 2025-02-01 01:05:20,335][0m Trial 46 finished with value: 0.08326613446221734 and parameters: {'observation_period_num': 5, 'train_rates': 0.7679108479047625, 'learning_rate': 0.0001549209965963491, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8092156546288423}. Best is trial 46 with value: 0.08326613446221734.[0m
[32m[I 2025-02-01 01:06:30,625][0m Trial 47 finished with value: 0.0757165850382992 and parameters: {'observation_period_num': 9, 'train_rates': 0.7629222385124196, 'learning_rate': 0.00016327307235987321, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8226944168681165}. Best is trial 47 with value: 0.0757165850382992.[0m
[32m[I 2025-02-01 01:07:56,013][0m Trial 48 finished with value: 0.1524677380427226 and parameters: {'observation_period_num': 144, 'train_rates': 0.763048609424107, 'learning_rate': 0.0001387977245546752, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8190309835462553}. Best is trial 47 with value: 0.0757165850382992.[0m
[32m[I 2025-02-01 01:09:03,302][0m Trial 49 finished with value: 0.07793003483003205 and parameters: {'observation_period_num': 19, 'train_rates': 0.7403327263496547, 'learning_rate': 0.00034609418457582915, 'batch_size': 74, 'step_size': 5, 'gamma': 0.7907153099665752}. Best is trial 47 with value: 0.0757165850382992.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-01 01:09:03,313][0m A new study created in memory with name: no-name-459984de-c9e4-4419-86c8-ea620594c7ed[0m
[32m[I 2025-02-01 01:10:08,507][0m Trial 0 finished with value: 0.10059244996443474 and parameters: {'observation_period_num': 38, 'train_rates': 0.634346447986364, 'learning_rate': 0.00016000751488908833, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9097432129301515}. Best is trial 0 with value: 0.10059244996443474.[0m
Early stopping at epoch 45
[32m[I 2025-02-01 01:11:11,396][0m Trial 1 finished with value: 0.9330583789262427 and parameters: {'observation_period_num': 197, 'train_rates': 0.9096789751001025, 'learning_rate': 2.4623149132939373e-06, 'batch_size': 39, 'step_size': 1, 'gamma': 0.7729050398566868}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:11:35,867][0m Trial 2 finished with value: 0.19122109165895018 and parameters: {'observation_period_num': 76, 'train_rates': 0.8747792103430545, 'learning_rate': 3.2091921491251314e-05, 'batch_size': 245, 'step_size': 6, 'gamma': 0.9225330934528433}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:12:31,001][0m Trial 3 finished with value: 0.1569070926075304 and parameters: {'observation_period_num': 187, 'train_rates': 0.7969232461142801, 'learning_rate': 0.0007772512532042039, 'batch_size': 89, 'step_size': 3, 'gamma': 0.9204859694664816}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:13:57,114][0m Trial 4 finished with value: 0.44843639509903416 and parameters: {'observation_period_num': 167, 'train_rates': 0.734589959467689, 'learning_rate': 1.7153289834666968e-06, 'batch_size': 53, 'step_size': 12, 'gamma': 0.9710609168086912}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:14:26,348][0m Trial 5 finished with value: 0.16534197838187548 and parameters: {'observation_period_num': 198, 'train_rates': 0.6069867548441896, 'learning_rate': 0.0007692498995488592, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9263768179156399}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:15:59,680][0m Trial 6 finished with value: 0.17900947323822788 and parameters: {'observation_period_num': 134, 'train_rates': 0.864286599407958, 'learning_rate': 0.00045935303076685403, 'batch_size': 56, 'step_size': 5, 'gamma': 0.900333446213802}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:16:46,991][0m Trial 7 finished with value: 0.23854617053856614 and parameters: {'observation_period_num': 42, 'train_rates': 0.748614898678167, 'learning_rate': 4.354530677567499e-06, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9731032790187034}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:19:30,931][0m Trial 8 finished with value: 0.21122154533382384 and parameters: {'observation_period_num': 7, 'train_rates': 0.9475264647888697, 'learning_rate': 0.00024263047080239555, 'batch_size': 35, 'step_size': 2, 'gamma': 0.848899075880827}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:20:22,725][0m Trial 9 finished with value: 0.1282304161268732 and parameters: {'observation_period_num': 14, 'train_rates': 0.8827533012368456, 'learning_rate': 4.304553840188503e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8905696138765771}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:20:49,860][0m Trial 10 finished with value: 0.14519392759891092 and parameters: {'observation_period_num': 93, 'train_rates': 0.60213620506405, 'learning_rate': 9.042988879443154e-05, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8254920814982996}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:21:32,370][0m Trial 11 finished with value: 0.11222897756558198 and parameters: {'observation_period_num': 7, 'train_rates': 0.6868907041296083, 'learning_rate': 1.510972651226495e-05, 'batch_size': 115, 'step_size': 15, 'gamma': 0.8833869413275046}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:22:00,684][0m Trial 12 finished with value: 0.46507208261714744 and parameters: {'observation_period_num': 57, 'train_rates': 0.6780230230925556, 'learning_rate': 5.364585623476866e-06, 'batch_size': 178, 'step_size': 15, 'gamma': 0.8475140789638543}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:22:55,625][0m Trial 13 finished with value: 0.34669039714016464 and parameters: {'observation_period_num': 251, 'train_rates': 0.6830241460036919, 'learning_rate': 1.0672637854734106e-05, 'batch_size': 80, 'step_size': 15, 'gamma': 0.7979695815713876}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:23:31,354][0m Trial 14 finished with value: 0.1111349602187029 and parameters: {'observation_period_num': 109, 'train_rates': 0.6639223339236672, 'learning_rate': 0.0001190346287749104, 'batch_size': 128, 'step_size': 8, 'gamma': 0.8733616431968583}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:23:55,924][0m Trial 15 finished with value: 0.12288968002920227 and parameters: {'observation_period_num': 116, 'train_rates': 0.6446282221483912, 'learning_rate': 9.510889766915183e-05, 'batch_size': 196, 'step_size': 8, 'gamma': 0.9415969675843744}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:24:35,998][0m Trial 16 finished with value: 0.14419827924751158 and parameters: {'observation_period_num': 126, 'train_rates': 0.7891313875000925, 'learning_rate': 0.00015095711767142497, 'batch_size': 131, 'step_size': 4, 'gamma': 0.8453573011871536}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:25:36,382][0m Trial 17 finished with value: 0.1290296213609882 and parameters: {'observation_period_num': 55, 'train_rates': 0.649064079173409, 'learning_rate': 0.00022987228760379654, 'batch_size': 75, 'step_size': 8, 'gamma': 0.7500176519432068}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:25:59,764][0m Trial 18 finished with value: 0.14816724660325636 and parameters: {'observation_period_num': 94, 'train_rates': 0.7299067200272795, 'learning_rate': 7.867047172228996e-05, 'batch_size': 222, 'step_size': 8, 'gamma': 0.8678159442640535}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:26:35,473][0m Trial 19 finished with value: 0.18247304454502497 and parameters: {'observation_period_num': 144, 'train_rates': 0.8129049946152871, 'learning_rate': 0.00031558707992835553, 'batch_size': 144, 'step_size': 10, 'gamma': 0.9476050641347562}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:27:40,634][0m Trial 20 finished with value: 0.15940860771492635 and parameters: {'observation_period_num': 36, 'train_rates': 0.6396761414092954, 'learning_rate': 1.9115304384688135e-05, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8076566253372488}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:28:22,084][0m Trial 21 finished with value: 0.12635687079886285 and parameters: {'observation_period_num': 24, 'train_rates': 0.693597494956291, 'learning_rate': 1.8322615150798025e-05, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8873058723128989}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:31:39,264][0m Trial 22 finished with value: 0.1265795516676804 and parameters: {'observation_period_num': 71, 'train_rates': 0.7069712287007296, 'learning_rate': 4.848988618958456e-05, 'batch_size': 23, 'step_size': 10, 'gamma': 0.8727864894930379}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:32:26,096][0m Trial 23 finished with value: 0.1417394516908604 and parameters: {'observation_period_num': 37, 'train_rates': 0.653049728727806, 'learning_rate': 1.3381808683798414e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9035874641820951}. Best is trial 0 with value: 0.10059244996443474.[0m
[32m[I 2025-02-01 01:33:06,764][0m Trial 24 finished with value: 0.09242249929509257 and parameters: {'observation_period_num': 6, 'train_rates': 0.7598283806242736, 'learning_rate': 0.0001316453540765533, 'batch_size': 127, 'step_size': 4, 'gamma': 0.8692890758850956}. Best is trial 24 with value: 0.09242249929509257.[0m
[32m[I 2025-02-01 01:33:37,920][0m Trial 25 finished with value: 0.14535925909876823 and parameters: {'observation_period_num': 98, 'train_rates': 0.7733127738081415, 'learning_rate': 0.00016818066044967963, 'batch_size': 162, 'step_size': 4, 'gamma': 0.8176249811862529}. Best is trial 24 with value: 0.09242249929509257.[0m
Early stopping at epoch 90
[32m[I 2025-02-01 01:34:15,782][0m Trial 26 finished with value: 0.15027943475241212 and parameters: {'observation_period_num': 69, 'train_rates': 0.8417331457690621, 'learning_rate': 0.0004353517609765519, 'batch_size': 131, 'step_size': 1, 'gamma': 0.8617946747026222}. Best is trial 24 with value: 0.09242249929509257.[0m
[32m[I 2025-02-01 01:34:43,081][0m Trial 27 finished with value: 0.09777728412008896 and parameters: {'observation_period_num': 24, 'train_rates': 0.760316688341446, 'learning_rate': 0.00013263971477928862, 'batch_size': 188, 'step_size': 4, 'gamma': 0.9895818455928123}. Best is trial 24 with value: 0.09242249929509257.[0m
[32m[I 2025-02-01 01:35:13,579][0m Trial 28 finished with value: 0.06455493718385696 and parameters: {'observation_period_num': 25, 'train_rates': 0.9863976782569264, 'learning_rate': 5.681852600878022e-05, 'batch_size': 207, 'step_size': 4, 'gamma': 0.9896973403798067}. Best is trial 28 with value: 0.06455493718385696.[0m
[32m[I 2025-02-01 01:35:45,805][0m Trial 29 finished with value: 0.057107847183942795 and parameters: {'observation_period_num': 30, 'train_rates': 0.9867764221645923, 'learning_rate': 5.696177205540662e-05, 'batch_size': 202, 'step_size': 4, 'gamma': 0.9495848880377725}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:36:16,154][0m Trial 30 finished with value: 0.08293697237968445 and parameters: {'observation_period_num': 49, 'train_rates': 0.9874006241676896, 'learning_rate': 4.527444783164371e-05, 'batch_size': 209, 'step_size': 3, 'gamma': 0.9552936491430314}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:36:46,605][0m Trial 31 finished with value: 0.3466125726699829 and parameters: {'observation_period_num': 51, 'train_rates': 0.9667888230582199, 'learning_rate': 6.063730054110226e-05, 'batch_size': 213, 'step_size': 3, 'gamma': 0.9571828431064261}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:37:13,055][0m Trial 32 finished with value: 0.06816855818033218 and parameters: {'observation_period_num': 23, 'train_rates': 0.9871790688801855, 'learning_rate': 2.85122577535063e-05, 'batch_size': 251, 'step_size': 5, 'gamma': 0.9881369408969143}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:37:41,204][0m Trial 33 finished with value: 0.06186280399560928 and parameters: {'observation_period_num': 24, 'train_rates': 0.9861711179382473, 'learning_rate': 3.098661881795789e-05, 'batch_size': 240, 'step_size': 5, 'gamma': 0.9825274545313678}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:38:05,775][0m Trial 34 finished with value: 0.22305795550346375 and parameters: {'observation_period_num': 26, 'train_rates': 0.92855247022205, 'learning_rate': 2.7088577750514e-05, 'batch_size': 248, 'step_size': 6, 'gamma': 0.9869036218182643}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:38:34,134][0m Trial 35 finished with value: 0.1342308521270752 and parameters: {'observation_period_num': 81, 'train_rates': 0.9833801737718014, 'learning_rate': 2.7314692131535786e-05, 'batch_size': 229, 'step_size': 5, 'gamma': 0.9711258634491834}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:39:00,514][0m Trial 36 finished with value: 0.39485326409339905 and parameters: {'observation_period_num': 27, 'train_rates': 0.9207812692037054, 'learning_rate': 7.106381830588353e-06, 'batch_size': 239, 'step_size': 7, 'gamma': 0.934962460172939}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:39:24,883][0m Trial 37 finished with value: 0.3156644403934479 and parameters: {'observation_period_num': 67, 'train_rates': 0.9493460498303923, 'learning_rate': 3.3861446980853826e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.979421605493663}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:39:51,358][0m Trial 38 finished with value: 0.18108313407346205 and parameters: {'observation_period_num': 36, 'train_rates': 0.8987722918593354, 'learning_rate': 6.240504397862461e-05, 'batch_size': 229, 'step_size': 2, 'gamma': 0.9604181292745455}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:40:21,895][0m Trial 39 finished with value: 1.0530974864959717 and parameters: {'observation_period_num': 20, 'train_rates': 0.9597107057951088, 'learning_rate': 1.2328847882083353e-06, 'batch_size': 205, 'step_size': 7, 'gamma': 0.9654661915546349}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:40:46,633][0m Trial 40 finished with value: 1.102985143661499 and parameters: {'observation_period_num': 233, 'train_rates': 0.9706098300036476, 'learning_rate': 7.774268050302753e-06, 'batch_size': 240, 'step_size': 1, 'gamma': 0.9295849379032068}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:41:19,073][0m Trial 41 finished with value: 0.1047588512301445 and parameters: {'observation_period_num': 48, 'train_rates': 0.979210925175021, 'learning_rate': 3.864670219562839e-05, 'batch_size': 201, 'step_size': 3, 'gamma': 0.9506291772410089}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:41:47,303][0m Trial 42 finished with value: 0.28622138500213623 and parameters: {'observation_period_num': 40, 'train_rates': 0.9342095566009455, 'learning_rate': 2.3399877002322446e-05, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9141672145885659}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:42:15,713][0m Trial 43 finished with value: 0.08045505732297897 and parameters: {'observation_period_num': 59, 'train_rates': 0.9894055855187178, 'learning_rate': 5.602878840378726e-05, 'batch_size': 231, 'step_size': 2, 'gamma': 0.979257207239115}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:42:40,338][0m Trial 44 finished with value: 0.16431840330050798 and parameters: {'observation_period_num': 15, 'train_rates': 0.9024634501193985, 'learning_rate': 6.425903487715491e-05, 'batch_size': 255, 'step_size': 2, 'gamma': 0.9783875708552903}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:43:07,083][0m Trial 45 finished with value: 0.8008745312690735 and parameters: {'observation_period_num': 62, 'train_rates': 0.9467051837731094, 'learning_rate': 3.134465073602493e-06, 'batch_size': 231, 'step_size': 2, 'gamma': 0.9813200721097836}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:43:41,304][0m Trial 46 finished with value: 0.07286103814840317 and parameters: {'observation_period_num': 5, 'train_rates': 0.9882978763145777, 'learning_rate': 3.258112692223382e-05, 'batch_size': 188, 'step_size': 3, 'gamma': 0.9678399255101879}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:44:15,694][0m Trial 47 finished with value: 0.30007699131965637 and parameters: {'observation_period_num': 5, 'train_rates': 0.9563971220391159, 'learning_rate': 1.1088338517669049e-05, 'batch_size': 187, 'step_size': 7, 'gamma': 0.9674229126076601}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:44:51,382][0m Trial 48 finished with value: 0.17010893011992834 and parameters: {'observation_period_num': 16, 'train_rates': 0.8738790554813969, 'learning_rate': 2.1349596815880768e-05, 'batch_size': 163, 'step_size': 4, 'gamma': 0.9409822603407244}. Best is trial 29 with value: 0.057107847183942795.[0m
[32m[I 2025-02-01 01:45:25,971][0m Trial 49 finished with value: 0.2704033721554077 and parameters: {'observation_period_num': 81, 'train_rates': 0.9384368437883589, 'learning_rate': 3.051459816480725e-05, 'batch_size': 176, 'step_size': 6, 'gamma': 0.9656867655685607}. Best is trial 29 with value: 0.057107847183942795.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.6270625729441969, 'learning_rate': 0.0006346333559577962, 'batch_size': 199, 'step_size': 2, 'gamma': 0.9834599174167772}
Epoch 1/300, trend Loss: 0.5599 | 0.2242
Epoch 2/300, trend Loss: 0.2682 | 0.2130
Epoch 3/300, trend Loss: 0.2038 | 0.2203
Epoch 4/300, trend Loss: 0.1641 | 0.1276
Epoch 5/300, trend Loss: 0.1538 | 0.1680
Epoch 6/300, trend Loss: 0.1383 | 0.1924
Epoch 7/300, trend Loss: 0.1302 | 0.1835
Epoch 8/300, trend Loss: 0.1282 | 0.1561
Epoch 9/300, trend Loss: 0.1272 | 0.1607
Epoch 10/300, trend Loss: 0.1283 | 0.1070
Epoch 11/300, trend Loss: 0.1514 | 0.1737
Epoch 12/300, trend Loss: 0.1496 | 0.1101
Epoch 13/300, trend Loss: 0.1340 | 0.1617
Epoch 14/300, trend Loss: 0.1438 | 0.1643
Epoch 15/300, trend Loss: 0.1561 | 0.1409
Epoch 16/300, trend Loss: 0.1539 | 0.1712
Epoch 17/300, trend Loss: 0.1505 | 0.1150
Epoch 18/300, trend Loss: 0.1439 | 0.1831
Epoch 19/300, trend Loss: 0.1259 | 0.1090
Epoch 20/300, trend Loss: 0.1216 | 0.1328
Epoch 21/300, trend Loss: 0.1244 | 0.1012
Epoch 22/300, trend Loss: 0.1157 | 0.1174
Epoch 23/300, trend Loss: 0.1217 | 0.0962
Epoch 24/300, trend Loss: 0.1171 | 0.1397
Epoch 25/300, trend Loss: 0.1163 | 0.0968
Epoch 26/300, trend Loss: 0.1178 | 0.1125
Epoch 27/300, trend Loss: 0.1156 | 0.0877
Epoch 28/300, trend Loss: 0.1104 | 0.1085
Epoch 29/300, trend Loss: 0.1145 | 0.0903
Epoch 30/300, trend Loss: 0.1131 | 0.1203
Epoch 31/300, trend Loss: 0.1110 | 0.0864
Epoch 32/300, trend Loss: 0.1080 | 0.0964
Epoch 33/300, trend Loss: 0.1064 | 0.0824
Epoch 34/300, trend Loss: 0.1043 | 0.0918
Epoch 35/300, trend Loss: 0.1045 | 0.0815
Epoch 36/300, trend Loss: 0.1042 | 0.0920
Epoch 37/300, trend Loss: 0.1046 | 0.0805
Epoch 38/300, trend Loss: 0.1031 | 0.0960
Epoch 39/300, trend Loss: 0.1042 | 0.0813
Epoch 40/300, trend Loss: 0.1019 | 0.0928
Epoch 41/300, trend Loss: 0.1015 | 0.0781
Epoch 42/300, trend Loss: 0.0999 | 0.0828
Epoch 43/300, trend Loss: 0.0993 | 0.0752
Epoch 44/300, trend Loss: 0.0972 | 0.0774
Epoch 45/300, trend Loss: 0.0969 | 0.0742
Epoch 46/300, trend Loss: 0.0959 | 0.0763
Epoch 47/300, trend Loss: 0.0964 | 0.0742
Epoch 48/300, trend Loss: 0.0956 | 0.0782
Epoch 49/300, trend Loss: 0.0974 | 0.0760
Epoch 50/300, trend Loss: 0.0957 | 0.0804
Epoch 51/300, trend Loss: 0.0974 | 0.0753
Epoch 52/300, trend Loss: 0.0950 | 0.0770
Epoch 53/300, trend Loss: 0.0961 | 0.0736
Epoch 54/300, trend Loss: 0.0933 | 0.0724
Epoch 55/300, trend Loss: 0.0936 | 0.0728
Epoch 56/300, trend Loss: 0.0922 | 0.0703
Epoch 57/300, trend Loss: 0.0919 | 0.0709
Epoch 58/300, trend Loss: 0.0911 | 0.0690
Epoch 59/300, trend Loss: 0.0908 | 0.0697
Epoch 60/300, trend Loss: 0.0901 | 0.0685
Epoch 61/300, trend Loss: 0.0902 | 0.0689
Epoch 62/300, trend Loss: 0.0895 | 0.0682
Epoch 63/300, trend Loss: 0.0901 | 0.0686
Epoch 64/300, trend Loss: 0.0893 | 0.0683
Epoch 65/300, trend Loss: 0.0903 | 0.0686
Epoch 66/300, trend Loss: 0.0893 | 0.0688
Epoch 67/300, trend Loss: 0.0907 | 0.0689
Epoch 68/300, trend Loss: 0.0896 | 0.0692
Epoch 69/300, trend Loss: 0.0915 | 0.0694
Epoch 70/300, trend Loss: 0.0891 | 0.0706
Epoch 71/300, trend Loss: 0.0910 | 0.0746
Epoch 72/300, trend Loss: 0.0895 | 0.0669
Epoch 73/300, trend Loss: 0.0884 | 0.0684
Epoch 74/300, trend Loss: 0.0883 | 0.0661
Epoch 75/300, trend Loss: 0.0872 | 0.0671
Epoch 76/300, trend Loss: 0.0869 | 0.0663
Epoch 77/300, trend Loss: 0.0866 | 0.0682
Epoch 78/300, trend Loss: 0.0868 | 0.0672
Epoch 79/300, trend Loss: 0.0867 | 0.0677
Epoch 80/300, trend Loss: 0.0874 | 0.0660
Epoch 81/300, trend Loss: 0.0874 | 0.0667
Epoch 82/300, trend Loss: 0.0878 | 0.0659
Epoch 83/300, trend Loss: 0.0873 | 0.0677
Epoch 84/300, trend Loss: 0.0874 | 0.0672
Epoch 85/300, trend Loss: 0.0883 | 0.0701
Epoch 86/300, trend Loss: 0.0887 | 0.0653
Epoch 87/300, trend Loss: 0.0871 | 0.0668
Epoch 88/300, trend Loss: 0.0860 | 0.0647
Epoch 89/300, trend Loss: 0.0850 | 0.0662
Epoch 90/300, trend Loss: 0.0850 | 0.0655
Epoch 91/300, trend Loss: 0.0847 | 0.0666
Epoch 92/300, trend Loss: 0.0844 | 0.0648
Epoch 93/300, trend Loss: 0.0842 | 0.0654
Epoch 94/300, trend Loss: 0.0840 | 0.0643
Epoch 95/300, trend Loss: 0.0837 | 0.0655
Epoch 96/300, trend Loss: 0.0836 | 0.0646
Epoch 97/300, trend Loss: 0.0834 | 0.0656
Epoch 98/300, trend Loss: 0.0833 | 0.0644
Epoch 99/300, trend Loss: 0.0832 | 0.0652
Epoch 100/300, trend Loss: 0.0831 | 0.0641
Epoch 101/300, trend Loss: 0.0829 | 0.0649
Epoch 102/300, trend Loss: 0.0829 | 0.0640
Epoch 103/300, trend Loss: 0.0827 | 0.0648
Epoch 104/300, trend Loss: 0.0826 | 0.0640
Epoch 105/300, trend Loss: 0.0825 | 0.0646
Epoch 106/300, trend Loss: 0.0824 | 0.0639
Epoch 107/300, trend Loss: 0.0823 | 0.0644
Epoch 108/300, trend Loss: 0.0822 | 0.0638
Epoch 109/300, trend Loss: 0.0821 | 0.0643
Epoch 110/300, trend Loss: 0.0820 | 0.0637
Epoch 111/300, trend Loss: 0.0818 | 0.0642
Epoch 112/300, trend Loss: 0.0817 | 0.0636
Epoch 113/300, trend Loss: 0.0816 | 0.0640
Epoch 114/300, trend Loss: 0.0815 | 0.0635
Epoch 115/300, trend Loss: 0.0814 | 0.0638
Epoch 116/300, trend Loss: 0.0813 | 0.0635
Epoch 117/300, trend Loss: 0.0812 | 0.0636
Epoch 118/300, trend Loss: 0.0811 | 0.0634
Epoch 119/300, trend Loss: 0.0810 | 0.0634
Epoch 120/300, trend Loss: 0.0808 | 0.0634
Epoch 121/300, trend Loss: 0.0808 | 0.0634
Epoch 122/300, trend Loss: 0.0806 | 0.0634
Epoch 123/300, trend Loss: 0.0805 | 0.0634
Epoch 124/300, trend Loss: 0.0804 | 0.0636
Epoch 125/300, trend Loss: 0.0803 | 0.0636
Epoch 126/300, trend Loss: 0.0802 | 0.0638
Epoch 127/300, trend Loss: 0.0802 | 0.0639
Epoch 128/300, trend Loss: 0.0801 | 0.0642
Epoch 129/300, trend Loss: 0.0801 | 0.0645
Epoch 130/300, trend Loss: 0.0802 | 0.0642
Epoch 131/300, trend Loss: 0.0803 | 0.0641
Epoch 132/300, trend Loss: 0.0804 | 0.0632
Epoch 133/300, trend Loss: 0.0803 | 0.0634
Epoch 134/300, trend Loss: 0.0801 | 0.0632
Epoch 135/300, trend Loss: 0.0800 | 0.0627
Epoch 136/300, trend Loss: 0.0804 | 0.0620
Epoch 137/300, trend Loss: 0.0810 | 0.0637
Epoch 138/300, trend Loss: 0.0805 | 0.0667
Epoch 139/300, trend Loss: 0.0797 | 0.0652
Epoch 140/300, trend Loss: 0.0795 | 0.0624
Epoch 141/300, trend Loss: 0.0792 | 0.0620
Epoch 142/300, trend Loss: 0.0790 | 0.0625
Epoch 143/300, trend Loss: 0.0790 | 0.0621
Epoch 144/300, trend Loss: 0.0791 | 0.0628
Epoch 145/300, trend Loss: 0.0789 | 0.0640
Epoch 146/300, trend Loss: 0.0787 | 0.0630
Epoch 147/300, trend Loss: 0.0786 | 0.0627
Epoch 148/300, trend Loss: 0.0785 | 0.0622
Epoch 149/300, trend Loss: 0.0784 | 0.0623
Epoch 150/300, trend Loss: 0.0783 | 0.0622
Epoch 151/300, trend Loss: 0.0783 | 0.0619
Epoch 152/300, trend Loss: 0.0783 | 0.0621
Epoch 153/300, trend Loss: 0.0782 | 0.0624
Epoch 154/300, trend Loss: 0.0780 | 0.0626
Epoch 155/300, trend Loss: 0.0780 | 0.0627
Epoch 156/300, trend Loss: 0.0780 | 0.0622
Epoch 157/300, trend Loss: 0.0779 | 0.0620
Epoch 158/300, trend Loss: 0.0777 | 0.0620
Epoch 159/300, trend Loss: 0.0776 | 0.0615
Epoch 160/300, trend Loss: 0.0777 | 0.0613
Epoch 161/300, trend Loss: 0.0778 | 0.0616
Epoch 162/300, trend Loss: 0.0777 | 0.0620
Epoch 163/300, trend Loss: 0.0774 | 0.0623
Epoch 164/300, trend Loss: 0.0774 | 0.0622
Epoch 165/300, trend Loss: 0.0775 | 0.0616
Epoch 166/300, trend Loss: 0.0773 | 0.0615
Epoch 167/300, trend Loss: 0.0771 | 0.0612
Epoch 168/300, trend Loss: 0.0771 | 0.0607
Epoch 169/300, trend Loss: 0.0772 | 0.0611
Epoch 170/300, trend Loss: 0.0771 | 0.0619
Epoch 171/300, trend Loss: 0.0769 | 0.0619
Epoch 172/300, trend Loss: 0.0769 | 0.0617
Epoch 173/300, trend Loss: 0.0769 | 0.0612
Epoch 174/300, trend Loss: 0.0767 | 0.0610
Epoch 175/300, trend Loss: 0.0766 | 0.0607
Epoch 176/300, trend Loss: 0.0767 | 0.0605
Epoch 177/300, trend Loss: 0.0767 | 0.0610
Epoch 178/300, trend Loss: 0.0765 | 0.0614
Epoch 179/300, trend Loss: 0.0765 | 0.0613
Epoch 180/300, trend Loss: 0.0764 | 0.0610
Epoch 181/300, trend Loss: 0.0763 | 0.0607
Epoch 182/300, trend Loss: 0.0762 | 0.0604
Epoch 183/300, trend Loss: 0.0763 | 0.0604
Epoch 184/300, trend Loss: 0.0763 | 0.0607
Epoch 185/300, trend Loss: 0.0762 | 0.0610
Epoch 186/300, trend Loss: 0.0761 | 0.0609
Epoch 187/300, trend Loss: 0.0761 | 0.0607
Epoch 188/300, trend Loss: 0.0759 | 0.0604
Epoch 189/300, trend Loss: 0.0759 | 0.0602
Epoch 190/300, trend Loss: 0.0759 | 0.0603
Epoch 191/300, trend Loss: 0.0759 | 0.0605
Epoch 192/300, trend Loss: 0.0758 | 0.0607
Epoch 193/300, trend Loss: 0.0758 | 0.0606
Epoch 194/300, trend Loss: 0.0757 | 0.0603
Epoch 195/300, trend Loss: 0.0756 | 0.0601
Epoch 196/300, trend Loss: 0.0756 | 0.0601
Epoch 197/300, trend Loss: 0.0756 | 0.0602
Epoch 198/300, trend Loss: 0.0756 | 0.0604
Epoch 199/300, trend Loss: 0.0755 | 0.0603
Epoch 200/300, trend Loss: 0.0755 | 0.0602
Epoch 201/300, trend Loss: 0.0754 | 0.0600
Epoch 202/300, trend Loss: 0.0754 | 0.0600
Epoch 203/300, trend Loss: 0.0754 | 0.0601
Epoch 204/300, trend Loss: 0.0754 | 0.0602
Epoch 205/300, trend Loss: 0.0753 | 0.0601
Epoch 206/300, trend Loss: 0.0753 | 0.0600
Epoch 207/300, trend Loss: 0.0752 | 0.0599
Epoch 208/300, trend Loss: 0.0752 | 0.0599
Epoch 209/300, trend Loss: 0.0752 | 0.0600
Epoch 210/300, trend Loss: 0.0752 | 0.0600
Epoch 211/300, trend Loss: 0.0751 | 0.0600
Epoch 212/300, trend Loss: 0.0751 | 0.0599
Epoch 213/300, trend Loss: 0.0751 | 0.0599
Epoch 214/300, trend Loss: 0.0751 | 0.0599
Epoch 215/300, trend Loss: 0.0750 | 0.0599
Epoch 216/300, trend Loss: 0.0750 | 0.0599
Epoch 217/300, trend Loss: 0.0750 | 0.0598
Epoch 218/300, trend Loss: 0.0749 | 0.0598
Epoch 219/300, trend Loss: 0.0749 | 0.0598
Epoch 220/300, trend Loss: 0.0749 | 0.0598
Epoch 221/300, trend Loss: 0.0749 | 0.0598
Epoch 222/300, trend Loss: 0.0749 | 0.0598
Epoch 223/300, trend Loss: 0.0748 | 0.0598
Epoch 224/300, trend Loss: 0.0748 | 0.0598
Epoch 225/300, trend Loss: 0.0748 | 0.0598
Epoch 226/300, trend Loss: 0.0748 | 0.0597
Epoch 227/300, trend Loss: 0.0747 | 0.0597
Epoch 228/300, trend Loss: 0.0747 | 0.0597
Epoch 229/300, trend Loss: 0.0747 | 0.0597
Epoch 230/300, trend Loss: 0.0747 | 0.0597
Epoch 231/300, trend Loss: 0.0747 | 0.0597
Epoch 232/300, trend Loss: 0.0746 | 0.0597
Epoch 233/300, trend Loss: 0.0746 | 0.0597
Epoch 234/300, trend Loss: 0.0746 | 0.0597
Epoch 235/300, trend Loss: 0.0746 | 0.0597
Epoch 236/300, trend Loss: 0.0746 | 0.0597
Epoch 237/300, trend Loss: 0.0746 | 0.0596
Epoch 238/300, trend Loss: 0.0745 | 0.0596
Epoch 239/300, trend Loss: 0.0745 | 0.0596
Epoch 240/300, trend Loss: 0.0745 | 0.0596
Epoch 241/300, trend Loss: 0.0745 | 0.0596
Epoch 242/300, trend Loss: 0.0745 | 0.0596
Epoch 243/300, trend Loss: 0.0745 | 0.0596
Epoch 244/300, trend Loss: 0.0744 | 0.0596
Epoch 245/300, trend Loss: 0.0744 | 0.0596
Epoch 246/300, trend Loss: 0.0744 | 0.0596
Epoch 247/300, trend Loss: 0.0744 | 0.0596
Epoch 248/300, trend Loss: 0.0744 | 0.0596
Epoch 249/300, trend Loss: 0.0744 | 0.0596
Epoch 250/300, trend Loss: 0.0744 | 0.0596
Epoch 251/300, trend Loss: 0.0743 | 0.0596
Epoch 252/300, trend Loss: 0.0743 | 0.0596
Epoch 253/300, trend Loss: 0.0743 | 0.0595
Epoch 254/300, trend Loss: 0.0743 | 0.0595
Epoch 255/300, trend Loss: 0.0743 | 0.0595
Epoch 256/300, trend Loss: 0.0743 | 0.0595
Epoch 257/300, trend Loss: 0.0743 | 0.0595
Epoch 258/300, trend Loss: 0.0743 | 0.0595
Epoch 259/300, trend Loss: 0.0742 | 0.0595
Epoch 260/300, trend Loss: 0.0742 | 0.0595
Epoch 261/300, trend Loss: 0.0742 | 0.0595
Epoch 262/300, trend Loss: 0.0742 | 0.0595
Epoch 263/300, trend Loss: 0.0742 | 0.0595
Epoch 264/300, trend Loss: 0.0742 | 0.0595
Epoch 265/300, trend Loss: 0.0742 | 0.0595
Epoch 266/300, trend Loss: 0.0742 | 0.0595
Epoch 267/300, trend Loss: 0.0742 | 0.0595
Epoch 268/300, trend Loss: 0.0741 | 0.0595
Epoch 269/300, trend Loss: 0.0741 | 0.0595
Epoch 270/300, trend Loss: 0.0741 | 0.0595
Epoch 271/300, trend Loss: 0.0741 | 0.0595
Epoch 272/300, trend Loss: 0.0741 | 0.0595
Epoch 273/300, trend Loss: 0.0741 | 0.0595
Epoch 274/300, trend Loss: 0.0741 | 0.0595
Epoch 275/300, trend Loss: 0.0741 | 0.0595
Epoch 276/300, trend Loss: 0.0741 | 0.0595
Epoch 277/300, trend Loss: 0.0741 | 0.0594
Epoch 278/300, trend Loss: 0.0740 | 0.0594
Epoch 279/300, trend Loss: 0.0740 | 0.0594
Epoch 280/300, trend Loss: 0.0740 | 0.0594
Epoch 281/300, trend Loss: 0.0740 | 0.0594
Epoch 282/300, trend Loss: 0.0740 | 0.0594
Epoch 283/300, trend Loss: 0.0740 | 0.0594
Epoch 284/300, trend Loss: 0.0740 | 0.0594
Epoch 285/300, trend Loss: 0.0740 | 0.0594
Epoch 286/300, trend Loss: 0.0740 | 0.0594
Epoch 287/300, trend Loss: 0.0740 | 0.0594
Epoch 288/300, trend Loss: 0.0740 | 0.0594
Epoch 289/300, trend Loss: 0.0740 | 0.0594
Epoch 290/300, trend Loss: 0.0740 | 0.0594
Epoch 291/300, trend Loss: 0.0739 | 0.0594
Epoch 292/300, trend Loss: 0.0739 | 0.0594
Epoch 293/300, trend Loss: 0.0739 | 0.0594
Epoch 294/300, trend Loss: 0.0739 | 0.0594
Epoch 295/300, trend Loss: 0.0739 | 0.0594
Epoch 296/300, trend Loss: 0.0739 | 0.0594
Epoch 297/300, trend Loss: 0.0739 | 0.0594
Epoch 298/300, trend Loss: 0.0739 | 0.0594
Epoch 299/300, trend Loss: 0.0739 | 0.0594
Epoch 300/300, trend Loss: 0.0739 | 0.0594
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.7449729933958217, 'learning_rate': 0.0005685748003530349, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9895828054511326}
Epoch 1/300, seasonal_0 Loss: 0.2398 | 0.1512
Epoch 2/300, seasonal_0 Loss: 0.1385 | 0.1336
Epoch 3/300, seasonal_0 Loss: 0.1300 | 0.1397
Epoch 4/300, seasonal_0 Loss: 0.1206 | 0.1394
Epoch 5/300, seasonal_0 Loss: 0.1130 | 0.1252
Epoch 6/300, seasonal_0 Loss: 0.1086 | 0.1225
Epoch 7/300, seasonal_0 Loss: 0.1060 | 0.1220
Epoch 8/300, seasonal_0 Loss: 0.1105 | 0.1358
Epoch 9/300, seasonal_0 Loss: 0.1033 | 0.1082
Epoch 10/300, seasonal_0 Loss: 0.1081 | 0.1273
Epoch 11/300, seasonal_0 Loss: 0.0975 | 0.0951
Epoch 12/300, seasonal_0 Loss: 0.0980 | 0.1195
Epoch 13/300, seasonal_0 Loss: 0.0924 | 0.0937
Epoch 14/300, seasonal_0 Loss: 0.0943 | 0.1131
Epoch 15/300, seasonal_0 Loss: 0.0907 | 0.0959
Epoch 16/300, seasonal_0 Loss: 0.0909 | 0.0947
Epoch 17/300, seasonal_0 Loss: 0.0874 | 0.0893
Epoch 18/300, seasonal_0 Loss: 0.0864 | 0.0910
Epoch 19/300, seasonal_0 Loss: 0.0838 | 0.0901
Epoch 20/300, seasonal_0 Loss: 0.0850 | 0.0908
Epoch 21/300, seasonal_0 Loss: 0.0819 | 0.0853
Epoch 22/300, seasonal_0 Loss: 0.0820 | 0.0901
Epoch 23/300, seasonal_0 Loss: 0.0801 | 0.0920
Epoch 24/300, seasonal_0 Loss: 0.0837 | 0.0925
Epoch 25/300, seasonal_0 Loss: 0.0807 | 0.0843
Epoch 26/300, seasonal_0 Loss: 0.0780 | 0.0857
Epoch 27/300, seasonal_0 Loss: 0.0766 | 0.0877
Epoch 28/300, seasonal_0 Loss: 0.0797 | 0.0918
Epoch 29/300, seasonal_0 Loss: 0.0780 | 0.0897
Epoch 30/300, seasonal_0 Loss: 0.0801 | 0.0876
Epoch 31/300, seasonal_0 Loss: 0.0769 | 0.0829
Epoch 32/300, seasonal_0 Loss: 0.0744 | 0.0835
Epoch 33/300, seasonal_0 Loss: 0.0752 | 0.0877
Epoch 34/300, seasonal_0 Loss: 0.0794 | 0.0911
Epoch 35/300, seasonal_0 Loss: 0.0783 | 0.0909
Epoch 36/300, seasonal_0 Loss: 0.0789 | 0.0903
Epoch 37/300, seasonal_0 Loss: 0.0775 | 0.0877
Epoch 38/300, seasonal_0 Loss: 0.0750 | 0.0891
Epoch 39/300, seasonal_0 Loss: 0.0738 | 0.0882
Epoch 40/300, seasonal_0 Loss: 0.0755 | 0.0902
Epoch 41/300, seasonal_0 Loss: 0.0741 | 0.0870
Epoch 42/300, seasonal_0 Loss: 0.0740 | 0.0895
Epoch 43/300, seasonal_0 Loss: 0.0725 | 0.0874
Epoch 44/300, seasonal_0 Loss: 0.0721 | 0.0842
Epoch 45/300, seasonal_0 Loss: 0.0712 | 0.0858
Epoch 46/300, seasonal_0 Loss: 0.0714 | 0.0836
Epoch 47/300, seasonal_0 Loss: 0.0700 | 0.0812
Epoch 48/300, seasonal_0 Loss: 0.0683 | 0.0816
Epoch 49/300, seasonal_0 Loss: 0.0674 | 0.0794
Epoch 50/300, seasonal_0 Loss: 0.0665 | 0.0804
Epoch 51/300, seasonal_0 Loss: 0.0666 | 0.0793
Epoch 52/300, seasonal_0 Loss: 0.0677 | 0.0815
Epoch 53/300, seasonal_0 Loss: 0.0671 | 0.0837
Epoch 54/300, seasonal_0 Loss: 0.0707 | 0.0883
Epoch 55/300, seasonal_0 Loss: 0.0694 | 0.0802
Epoch 56/300, seasonal_0 Loss: 0.0649 | 0.0786
Epoch 57/300, seasonal_0 Loss: 0.0642 | 0.0800
Epoch 58/300, seasonal_0 Loss: 0.0661 | 0.0826
Epoch 59/300, seasonal_0 Loss: 0.0697 | 0.0828
Epoch 60/300, seasonal_0 Loss: 0.0697 | 0.0862
Epoch 61/300, seasonal_0 Loss: 0.0712 | 0.0787
Epoch 62/300, seasonal_0 Loss: 0.0669 | 0.0793
Epoch 63/300, seasonal_0 Loss: 0.0654 | 0.0778
Epoch 64/300, seasonal_0 Loss: 0.0695 | 0.0881
Epoch 65/300, seasonal_0 Loss: 0.0662 | 0.0780
Epoch 66/300, seasonal_0 Loss: 0.0594 | 0.0791
Epoch 67/300, seasonal_0 Loss: 0.0603 | 0.0809
Epoch 68/300, seasonal_0 Loss: 0.0654 | 0.0848
Epoch 69/300, seasonal_0 Loss: 0.0645 | 0.0782
Epoch 70/300, seasonal_0 Loss: 0.0624 | 0.0779
Epoch 71/300, seasonal_0 Loss: 0.0607 | 0.0778
Epoch 72/300, seasonal_0 Loss: 0.0558 | 0.0770
Epoch 73/300, seasonal_0 Loss: 0.0607 | 0.0781
Epoch 74/300, seasonal_0 Loss: 0.0621 | 0.0797
Epoch 75/300, seasonal_0 Loss: 0.0725 | 0.0830
Epoch 76/300, seasonal_0 Loss: 0.0719 | 0.0860
Epoch 77/300, seasonal_0 Loss: 0.0669 | 0.0854
Epoch 78/300, seasonal_0 Loss: 0.0697 | 0.0924
Epoch 79/300, seasonal_0 Loss: 0.0660 | 0.0754
Epoch 80/300, seasonal_0 Loss: 0.0617 | 0.0758
Epoch 81/300, seasonal_0 Loss: 0.0592 | 0.0774
Epoch 82/300, seasonal_0 Loss: 0.0608 | 0.0843
Epoch 83/300, seasonal_0 Loss: 0.0607 | 0.0775
Epoch 84/300, seasonal_0 Loss: 0.0603 | 0.0796
Epoch 85/300, seasonal_0 Loss: 0.0593 | 0.0751
Epoch 86/300, seasonal_0 Loss: 0.0680 | 0.0862
Epoch 87/300, seasonal_0 Loss: 0.0658 | 0.0804
Epoch 88/300, seasonal_0 Loss: 0.0661 | 0.0839
Epoch 89/300, seasonal_0 Loss: 0.0601 | 0.0792
Epoch 90/300, seasonal_0 Loss: 0.0668 | 0.0801
Epoch 91/300, seasonal_0 Loss: 0.0656 | 0.0839
Epoch 92/300, seasonal_0 Loss: 0.0578 | 0.0778
Epoch 93/300, seasonal_0 Loss: 0.0527 | 0.0780
Epoch 94/300, seasonal_0 Loss: 0.0626 | 0.0871
Epoch 95/300, seasonal_0 Loss: 0.0764 | 0.0940
Epoch 96/300, seasonal_0 Loss: 0.0721 | 0.0813
Epoch 97/300, seasonal_0 Loss: 0.0652 | 0.0921
Epoch 98/300, seasonal_0 Loss: 0.0635 | 0.0811
Epoch 99/300, seasonal_0 Loss: 0.0639 | 0.0818
Epoch 100/300, seasonal_0 Loss: 0.0609 | 0.0775
Epoch 101/300, seasonal_0 Loss: 0.0597 | 0.0829
Epoch 102/300, seasonal_0 Loss: 0.0590 | 0.0775
Epoch 103/300, seasonal_0 Loss: 0.0596 | 0.0816
Epoch 104/300, seasonal_0 Loss: 0.0542 | 0.0762
Epoch 105/300, seasonal_0 Loss: 0.0594 | 0.0785
Epoch 106/300, seasonal_0 Loss: 0.0575 | 0.0754
Epoch 107/300, seasonal_0 Loss: 0.0566 | 0.0769
Epoch 108/300, seasonal_0 Loss: 0.0559 | 0.0757
Epoch 109/300, seasonal_0 Loss: 0.0557 | 0.0773
Epoch 110/300, seasonal_0 Loss: 0.0553 | 0.0754
Epoch 111/300, seasonal_0 Loss: 0.0548 | 0.0778
Epoch 112/300, seasonal_0 Loss: 0.0545 | 0.0773
Epoch 113/300, seasonal_0 Loss: 0.0588 | 0.0771
Epoch 114/300, seasonal_0 Loss: 0.0629 | 0.0788
Epoch 115/300, seasonal_0 Loss: 0.0592 | 0.0774
Epoch 116/300, seasonal_0 Loss: 0.0587 | 0.0799
Epoch 117/300, seasonal_0 Loss: 0.0581 | 0.0786
Epoch 118/300, seasonal_0 Loss: 0.0568 | 0.0802
Epoch 119/300, seasonal_0 Loss: 0.0610 | 0.0800
Epoch 120/300, seasonal_0 Loss: 0.0538 | 0.0820
Epoch 121/300, seasonal_0 Loss: 0.0676 | 0.0883
Epoch 122/300, seasonal_0 Loss: 0.0642 | 0.0842
Epoch 123/300, seasonal_0 Loss: 0.0820 | 0.0994
Epoch 124/300, seasonal_0 Loss: 0.0805 | 0.1089
Epoch 125/300, seasonal_0 Loss: 0.0782 | 0.0949
Epoch 126/300, seasonal_0 Loss: 0.0735 | 0.0867
Epoch 127/300, seasonal_0 Loss: 0.0627 | 0.0801
Epoch 128/300, seasonal_0 Loss: 0.0696 | 0.0858
Epoch 129/300, seasonal_0 Loss: 0.0666 | 0.0852
Epoch 130/300, seasonal_0 Loss: 0.0645 | 0.0870
Epoch 131/300, seasonal_0 Loss: 0.0610 | 0.0785
Epoch 132/300, seasonal_0 Loss: 0.0536 | 0.0812
Epoch 133/300, seasonal_0 Loss: 0.0588 | 0.0821
Epoch 134/300, seasonal_0 Loss: 0.0509 | 0.0793
Epoch 135/300, seasonal_0 Loss: 0.0511 | 0.0768
Epoch 136/300, seasonal_0 Loss: 0.0571 | 0.0779
Epoch 137/300, seasonal_0 Loss: 0.0491 | 0.0767
Epoch 138/300, seasonal_0 Loss: 0.0554 | 0.0776
Epoch 139/300, seasonal_0 Loss: 0.0544 | 0.0765
Epoch 140/300, seasonal_0 Loss: 0.0539 | 0.0775
Epoch 141/300, seasonal_0 Loss: 0.0523 | 0.0783
Epoch 142/300, seasonal_0 Loss: 0.0506 | 0.0772
Epoch 143/300, seasonal_0 Loss: 0.0565 | 0.0794
Epoch 144/300, seasonal_0 Loss: 0.0566 | 0.0839
Epoch 145/300, seasonal_0 Loss: 0.0577 | 0.0810
Epoch 146/300, seasonal_0 Loss: 0.0569 | 0.0822
Epoch 147/300, seasonal_0 Loss: 0.0542 | 0.0791
Epoch 148/300, seasonal_0 Loss: 0.0477 | 0.0807
Epoch 149/300, seasonal_0 Loss: 0.0557 | 0.0835
Epoch 150/300, seasonal_0 Loss: 0.0506 | 0.0824
Epoch 151/300, seasonal_0 Loss: 0.0471 | 0.0803
Epoch 152/300, seasonal_0 Loss: 0.0457 | 0.0807
Epoch 153/300, seasonal_0 Loss: 0.0448 | 0.0802
Epoch 154/300, seasonal_0 Loss: 0.0440 | 0.0796
Epoch 155/300, seasonal_0 Loss: 0.0435 | 0.0793
Epoch 156/300, seasonal_0 Loss: 0.0428 | 0.0783
Epoch 157/300, seasonal_0 Loss: 0.0525 | 0.0790
Epoch 158/300, seasonal_0 Loss: 0.0519 | 0.0799
Epoch 159/300, seasonal_0 Loss: 0.0516 | 0.0802
Epoch 160/300, seasonal_0 Loss: 0.0443 | 0.0832
Epoch 161/300, seasonal_0 Loss: 0.0478 | 0.0815
Epoch 162/300, seasonal_0 Loss: 0.0420 | 0.0808
Epoch 163/300, seasonal_0 Loss: 0.0410 | 0.0817
Epoch 164/300, seasonal_0 Loss: 0.0406 | 0.0825
Epoch 165/300, seasonal_0 Loss: 0.0407 | 0.0830
Epoch 166/300, seasonal_0 Loss: 0.0408 | 0.0833
Epoch 167/300, seasonal_0 Loss: 0.0402 | 0.0831
Epoch 168/300, seasonal_0 Loss: 0.0398 | 0.0831
Epoch 169/300, seasonal_0 Loss: 0.0393 | 0.0821
Epoch 170/300, seasonal_0 Loss: 0.0388 | 0.0841
Epoch 171/300, seasonal_0 Loss: 0.0386 | 0.0827
Epoch 172/300, seasonal_0 Loss: 0.0384 | 0.0848
Epoch 173/300, seasonal_0 Loss: 0.0386 | 0.0823
Epoch 174/300, seasonal_0 Loss: 0.0381 | 0.0852
Epoch 175/300, seasonal_0 Loss: 0.0385 | 0.0824
Epoch 176/300, seasonal_0 Loss: 0.0372 | 0.0846
Epoch 177/300, seasonal_0 Loss: 0.0374 | 0.0849
Epoch 178/300, seasonal_0 Loss: 0.0368 | 0.0843
Epoch 179/300, seasonal_0 Loss: 0.0376 | 0.0864
Epoch 180/300, seasonal_0 Loss: 0.0368 | 0.0833
Epoch 181/300, seasonal_0 Loss: 0.0361 | 0.0830
Epoch 182/300, seasonal_0 Loss: 0.0357 | 0.0853
Epoch 183/300, seasonal_0 Loss: 0.0361 | 0.0867
Epoch 184/300, seasonal_0 Loss: 0.0359 | 0.0864
Epoch 185/300, seasonal_0 Loss: 0.0358 | 0.0869
Epoch 186/300, seasonal_0 Loss: 0.0353 | 0.0865
Epoch 187/300, seasonal_0 Loss: 0.0350 | 0.0862
Epoch 188/300, seasonal_0 Loss: 0.0345 | 0.0881
Epoch 189/300, seasonal_0 Loss: 0.0353 | 0.0887
Epoch 190/300, seasonal_0 Loss: 0.0357 | 0.0925
Epoch 191/300, seasonal_0 Loss: 0.0370 | 0.0884
Epoch 192/300, seasonal_0 Loss: 0.0385 | 0.0891
Epoch 193/300, seasonal_0 Loss: 0.0382 | 0.0890
Epoch 194/300, seasonal_0 Loss: 0.0336 | 0.0888
Epoch 195/300, seasonal_0 Loss: 0.0339 | 0.0905
Epoch 196/300, seasonal_0 Loss: 0.0342 | 0.0868
Epoch 197/300, seasonal_0 Loss: 0.0375 | 0.0905
Epoch 198/300, seasonal_0 Loss: 0.0339 | 0.0874
Epoch 199/300, seasonal_0 Loss: 0.0325 | 0.0891
Epoch 200/300, seasonal_0 Loss: 0.0327 | 0.0875
Epoch 201/300, seasonal_0 Loss: 0.0322 | 0.0888
Epoch 202/300, seasonal_0 Loss: 0.0327 | 0.0853
Epoch 203/300, seasonal_0 Loss: 0.0325 | 0.0886
Epoch 204/300, seasonal_0 Loss: 0.0329 | 0.0853
Epoch 205/300, seasonal_0 Loss: 0.0324 | 0.0870
Epoch 206/300, seasonal_0 Loss: 0.0329 | 0.0850
Epoch 207/300, seasonal_0 Loss: 0.0326 | 0.0865
Epoch 208/300, seasonal_0 Loss: 0.0329 | 0.0871
Epoch 209/300, seasonal_0 Loss: 0.0330 | 0.0859
Epoch 210/300, seasonal_0 Loss: 0.0322 | 0.0870
Epoch 211/300, seasonal_0 Loss: 0.0318 | 0.0834
Epoch 212/300, seasonal_0 Loss: 0.0313 | 0.0878
Epoch 213/300, seasonal_0 Loss: 0.0320 | 0.0863
Epoch 214/300, seasonal_0 Loss: 0.0328 | 0.0886
Epoch 215/300, seasonal_0 Loss: 0.0320 | 0.0843
Epoch 216/300, seasonal_0 Loss: 0.0303 | 0.0855
Epoch 217/300, seasonal_0 Loss: 0.0300 | 0.0851
Epoch 218/300, seasonal_0 Loss: 0.0287 | 0.0833
Epoch 219/300, seasonal_0 Loss: 0.0287 | 0.0863
Epoch 220/300, seasonal_0 Loss: 0.0281 | 0.0847
Epoch 221/300, seasonal_0 Loss: 0.0277 | 0.0836
Epoch 222/300, seasonal_0 Loss: 0.0272 | 0.0853
Epoch 223/300, seasonal_0 Loss: 0.0272 | 0.0852
Epoch 224/300, seasonal_0 Loss: 0.0274 | 0.0860
Epoch 225/300, seasonal_0 Loss: 0.0280 | 0.0829
Epoch 226/300, seasonal_0 Loss: 0.0277 | 0.0848
Epoch 227/300, seasonal_0 Loss: 0.0274 | 0.0832
Epoch 228/300, seasonal_0 Loss: 0.0272 | 0.0859
Epoch 229/300, seasonal_0 Loss: 0.0267 | 0.0841
Epoch 230/300, seasonal_0 Loss: 0.0265 | 0.0843
Epoch 231/300, seasonal_0 Loss: 0.0261 | 0.0837
Epoch 232/300, seasonal_0 Loss: 0.0257 | 0.0831
Epoch 233/300, seasonal_0 Loss: 0.0255 | 0.0835
Epoch 234/300, seasonal_0 Loss: 0.0250 | 0.0827
Epoch 235/300, seasonal_0 Loss: 0.0249 | 0.0831
Epoch 236/300, seasonal_0 Loss: 0.0245 | 0.0829
Epoch 237/300, seasonal_0 Loss: 0.0245 | 0.0841
Epoch 238/300, seasonal_0 Loss: 0.0244 | 0.0839
Epoch 239/300, seasonal_0 Loss: 0.0246 | 0.0859
Epoch 240/300, seasonal_0 Loss: 0.0259 | 0.0871
Epoch 241/300, seasonal_0 Loss: 0.0249 | 0.0858
Epoch 242/300, seasonal_0 Loss: 0.0241 | 0.0841
Epoch 243/300, seasonal_0 Loss: 0.0239 | 0.0859
Epoch 244/300, seasonal_0 Loss: 0.0237 | 0.0863
Epoch 245/300, seasonal_0 Loss: 0.0238 | 0.0851
Epoch 246/300, seasonal_0 Loss: 0.0237 | 0.0853
Epoch 247/300, seasonal_0 Loss: 0.0241 | 0.0855
Epoch 248/300, seasonal_0 Loss: 0.0240 | 0.0903
Epoch 249/300, seasonal_0 Loss: 0.0245 | 0.0838
Epoch 250/300, seasonal_0 Loss: 0.0229 | 0.0868
Epoch 251/300, seasonal_0 Loss: 0.0236 | 0.0882
Epoch 252/300, seasonal_0 Loss: 0.0292 | 0.0898
Epoch 253/300, seasonal_0 Loss: 0.0260 | 0.0837
Epoch 254/300, seasonal_0 Loss: 0.0233 | 0.0856
Epoch 255/300, seasonal_0 Loss: 0.0233 | 0.0862
Epoch 256/300, seasonal_0 Loss: 0.0224 | 0.0858
Epoch 257/300, seasonal_0 Loss: 0.0215 | 0.0847
Epoch 258/300, seasonal_0 Loss: 0.0207 | 0.0866
Epoch 259/300, seasonal_0 Loss: 0.0212 | 0.0851
Epoch 260/300, seasonal_0 Loss: 0.0215 | 0.0891
Epoch 261/300, seasonal_0 Loss: 0.0215 | 0.0858
Epoch 262/300, seasonal_0 Loss: 0.0202 | 0.0843
Epoch 263/300, seasonal_0 Loss: 0.0195 | 0.0857
Epoch 264/300, seasonal_0 Loss: 0.0187 | 0.0848
Epoch 265/300, seasonal_0 Loss: 0.0186 | 0.0853
Epoch 266/300, seasonal_0 Loss: 0.0184 | 0.0852
Epoch 267/300, seasonal_0 Loss: 0.0185 | 0.0864
Epoch 268/300, seasonal_0 Loss: 0.0182 | 0.0855
Epoch 269/300, seasonal_0 Loss: 0.0181 | 0.0874
Epoch 270/300, seasonal_0 Loss: 0.0179 | 0.0854
Epoch 271/300, seasonal_0 Loss: 0.0176 | 0.0876
Epoch 272/300, seasonal_0 Loss: 0.0174 | 0.0878
Epoch 273/300, seasonal_0 Loss: 0.0177 | 0.0888
Epoch 274/300, seasonal_0 Loss: 0.0174 | 0.0906
Epoch 275/300, seasonal_0 Loss: 0.0174 | 0.0889
Epoch 276/300, seasonal_0 Loss: 0.0174 | 0.0930
Epoch 277/300, seasonal_0 Loss: 0.0184 | 0.0901
Epoch 278/300, seasonal_0 Loss: 0.0193 | 0.0908
Epoch 279/300, seasonal_0 Loss: 0.0205 | 0.0900
Epoch 280/300, seasonal_0 Loss: 0.0193 | 0.0910
Epoch 281/300, seasonal_0 Loss: 0.0201 | 0.0868
Epoch 282/300, seasonal_0 Loss: 0.0180 | 0.0882
Epoch 283/300, seasonal_0 Loss: 0.0190 | 0.0936
Epoch 284/300, seasonal_0 Loss: 0.0190 | 0.0869
Epoch 285/300, seasonal_0 Loss: 0.0177 | 0.0875
Epoch 286/300, seasonal_0 Loss: 0.0754 | 0.1024
Epoch 287/300, seasonal_0 Loss: 0.0767 | 0.0923
Epoch 288/300, seasonal_0 Loss: 0.0676 | 0.0903
Epoch 289/300, seasonal_0 Loss: 0.0633 | 0.0890
Epoch 290/300, seasonal_0 Loss: 0.0575 | 0.0868
Epoch 291/300, seasonal_0 Loss: 0.0600 | 0.0891
Epoch 292/300, seasonal_0 Loss: 0.0590 | 0.0854
Epoch 293/300, seasonal_0 Loss: 0.0563 | 0.0842
Epoch 294/300, seasonal_0 Loss: 0.0545 | 0.0832
Epoch 295/300, seasonal_0 Loss: 0.0663 | 0.0866
Epoch 296/300, seasonal_0 Loss: 0.0622 | 0.0853
Epoch 297/300, seasonal_0 Loss: 0.0574 | 0.0857
Epoch 298/300, seasonal_0 Loss: 0.0549 | 0.0842
Epoch 299/300, seasonal_0 Loss: 0.0535 | 0.0832
Epoch 300/300, seasonal_0 Loss: 0.0518 | 0.0814
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.6129333912368159, 'learning_rate': 0.0006519925333204811, 'batch_size': 130, 'step_size': 15, 'gamma': 0.8865813595721506}
Epoch 1/300, seasonal_1 Loss: 0.4316 | 0.2407
Epoch 2/300, seasonal_1 Loss: 0.1885 | 0.1816
Epoch 3/300, seasonal_1 Loss: 0.1792 | 0.1549
Epoch 4/300, seasonal_1 Loss: 0.1592 | 0.1251
Epoch 5/300, seasonal_1 Loss: 0.1547 | 0.1228
Epoch 6/300, seasonal_1 Loss: 0.1496 | 0.1338
Epoch 7/300, seasonal_1 Loss: 0.1333 | 0.1658
Epoch 8/300, seasonal_1 Loss: 0.1218 | 0.1682
Epoch 9/300, seasonal_1 Loss: 0.1189 | 0.1733
Epoch 10/300, seasonal_1 Loss: 0.1179 | 0.1771
Epoch 11/300, seasonal_1 Loss: 0.1169 | 0.1247
Epoch 12/300, seasonal_1 Loss: 0.1158 | 0.0983
Epoch 13/300, seasonal_1 Loss: 0.1131 | 0.1012
Epoch 14/300, seasonal_1 Loss: 0.1098 | 0.0955
Epoch 15/300, seasonal_1 Loss: 0.1138 | 0.0951
Epoch 16/300, seasonal_1 Loss: 0.1104 | 0.0925
Epoch 17/300, seasonal_1 Loss: 0.1149 | 0.0930
Epoch 18/300, seasonal_1 Loss: 0.1071 | 0.0908
Epoch 19/300, seasonal_1 Loss: 0.1025 | 0.0870
Epoch 20/300, seasonal_1 Loss: 0.1018 | 0.0786
Epoch 21/300, seasonal_1 Loss: 0.1029 | 0.0850
Epoch 22/300, seasonal_1 Loss: 0.1048 | 0.0846
Epoch 23/300, seasonal_1 Loss: 0.1151 | 0.0907
Epoch 24/300, seasonal_1 Loss: 0.1047 | 0.0893
Epoch 25/300, seasonal_1 Loss: 0.1002 | 0.0845
Epoch 26/300, seasonal_1 Loss: 0.1000 | 0.0778
Epoch 27/300, seasonal_1 Loss: 0.1023 | 0.0765
Epoch 28/300, seasonal_1 Loss: 0.1045 | 0.0825
Epoch 29/300, seasonal_1 Loss: 0.0983 | 0.0772
Epoch 30/300, seasonal_1 Loss: 0.1005 | 0.0738
Epoch 31/300, seasonal_1 Loss: 0.1006 | 0.0806
Epoch 32/300, seasonal_1 Loss: 0.1073 | 0.0852
Epoch 33/300, seasonal_1 Loss: 0.0970 | 0.0766
Epoch 34/300, seasonal_1 Loss: 0.0983 | 0.0748
Epoch 35/300, seasonal_1 Loss: 0.0957 | 0.0781
Epoch 36/300, seasonal_1 Loss: 0.1013 | 0.0865
Epoch 37/300, seasonal_1 Loss: 0.0934 | 0.0763
Epoch 38/300, seasonal_1 Loss: 0.0950 | 0.0737
Epoch 39/300, seasonal_1 Loss: 0.0934 | 0.0832
Epoch 40/300, seasonal_1 Loss: 0.0967 | 0.0831
Epoch 41/300, seasonal_1 Loss: 0.0921 | 0.0821
Epoch 42/300, seasonal_1 Loss: 0.0915 | 0.0740
Epoch 43/300, seasonal_1 Loss: 0.0904 | 0.0839
Epoch 44/300, seasonal_1 Loss: 0.0903 | 0.0747
Epoch 45/300, seasonal_1 Loss: 0.0892 | 0.0832
Epoch 46/300, seasonal_1 Loss: 0.0876 | 0.0728
Epoch 47/300, seasonal_1 Loss: 0.0867 | 0.0760
Epoch 48/300, seasonal_1 Loss: 0.0856 | 0.0695
Epoch 49/300, seasonal_1 Loss: 0.0851 | 0.0709
Epoch 50/300, seasonal_1 Loss: 0.0846 | 0.0678
Epoch 51/300, seasonal_1 Loss: 0.0843 | 0.0689
Epoch 52/300, seasonal_1 Loss: 0.0841 | 0.0670
Epoch 53/300, seasonal_1 Loss: 0.0838 | 0.0685
Epoch 54/300, seasonal_1 Loss: 0.0832 | 0.0678
Epoch 55/300, seasonal_1 Loss: 0.0825 | 0.0680
Epoch 56/300, seasonal_1 Loss: 0.0819 | 0.0673
Epoch 57/300, seasonal_1 Loss: 0.0816 | 0.0668
Epoch 58/300, seasonal_1 Loss: 0.0813 | 0.0664
Epoch 59/300, seasonal_1 Loss: 0.0815 | 0.0667
Epoch 60/300, seasonal_1 Loss: 0.0812 | 0.0659
Epoch 61/300, seasonal_1 Loss: 0.0810 | 0.0665
Epoch 62/300, seasonal_1 Loss: 0.0810 | 0.0650
Epoch 63/300, seasonal_1 Loss: 0.0806 | 0.0663
Epoch 64/300, seasonal_1 Loss: 0.0814 | 0.0677
Epoch 65/300, seasonal_1 Loss: 0.0803 | 0.0666
Epoch 66/300, seasonal_1 Loss: 0.0790 | 0.0633
Epoch 67/300, seasonal_1 Loss: 0.0790 | 0.0661
Epoch 68/300, seasonal_1 Loss: 0.0785 | 0.0625
Epoch 69/300, seasonal_1 Loss: 0.0782 | 0.0641
Epoch 70/300, seasonal_1 Loss: 0.0781 | 0.0630
Epoch 71/300, seasonal_1 Loss: 0.0785 | 0.0632
Epoch 72/300, seasonal_1 Loss: 0.0776 | 0.0630
Epoch 73/300, seasonal_1 Loss: 0.0772 | 0.0629
Epoch 74/300, seasonal_1 Loss: 0.0763 | 0.0625
Epoch 75/300, seasonal_1 Loss: 0.0760 | 0.0620
Epoch 76/300, seasonal_1 Loss: 0.0757 | 0.0624
Epoch 77/300, seasonal_1 Loss: 0.0776 | 0.0640
Epoch 78/300, seasonal_1 Loss: 0.0773 | 0.0643
Epoch 79/300, seasonal_1 Loss: 0.0755 | 0.0617
Epoch 80/300, seasonal_1 Loss: 0.0749 | 0.0623
Epoch 81/300, seasonal_1 Loss: 0.0747 | 0.0617
Epoch 82/300, seasonal_1 Loss: 0.0746 | 0.0621
Epoch 83/300, seasonal_1 Loss: 0.0747 | 0.0624
Epoch 84/300, seasonal_1 Loss: 0.0748 | 0.0631
Epoch 85/300, seasonal_1 Loss: 0.0744 | 0.0626
Epoch 86/300, seasonal_1 Loss: 0.0741 | 0.0630
Epoch 87/300, seasonal_1 Loss: 0.0744 | 0.0624
Epoch 88/300, seasonal_1 Loss: 0.0744 | 0.0623
Epoch 89/300, seasonal_1 Loss: 0.0744 | 0.0615
Epoch 90/300, seasonal_1 Loss: 0.0740 | 0.0616
Epoch 91/300, seasonal_1 Loss: 0.0738 | 0.0607
Epoch 92/300, seasonal_1 Loss: 0.0732 | 0.0620
Epoch 93/300, seasonal_1 Loss: 0.0726 | 0.0613
Epoch 94/300, seasonal_1 Loss: 0.0724 | 0.0624
Epoch 95/300, seasonal_1 Loss: 0.0723 | 0.0613
Epoch 96/300, seasonal_1 Loss: 0.0721 | 0.0622
Epoch 97/300, seasonal_1 Loss: 0.0723 | 0.0616
Epoch 98/300, seasonal_1 Loss: 0.0724 | 0.0620
Epoch 99/300, seasonal_1 Loss: 0.0721 | 0.0614
Epoch 100/300, seasonal_1 Loss: 0.0718 | 0.0626
Epoch 101/300, seasonal_1 Loss: 0.0719 | 0.0616
Epoch 102/300, seasonal_1 Loss: 0.0717 | 0.0621
Epoch 103/300, seasonal_1 Loss: 0.0717 | 0.0616
Epoch 104/300, seasonal_1 Loss: 0.0718 | 0.0623
Epoch 105/300, seasonal_1 Loss: 0.0719 | 0.0617
Epoch 106/300, seasonal_1 Loss: 0.0720 | 0.0628
Epoch 107/300, seasonal_1 Loss: 0.0727 | 0.0622
Epoch 108/300, seasonal_1 Loss: 0.0731 | 0.0637
Epoch 109/300, seasonal_1 Loss: 0.0743 | 0.0664
Epoch 110/300, seasonal_1 Loss: 0.0760 | 0.0656
Epoch 111/300, seasonal_1 Loss: 0.0739 | 0.0651
Epoch 112/300, seasonal_1 Loss: 0.0766 | 0.0654
Epoch 113/300, seasonal_1 Loss: 0.0746 | 0.0672
Epoch 114/300, seasonal_1 Loss: 0.0770 | 0.0698
Epoch 115/300, seasonal_1 Loss: 0.0757 | 0.0656
Epoch 116/300, seasonal_1 Loss: 0.0729 | 0.0666
Epoch 117/300, seasonal_1 Loss: 0.0736 | 0.0666
Epoch 118/300, seasonal_1 Loss: 0.0731 | 0.0648
Epoch 119/300, seasonal_1 Loss: 0.0727 | 0.0655
Epoch 120/300, seasonal_1 Loss: 0.0734 | 0.0648
Epoch 121/300, seasonal_1 Loss: 0.0739 | 0.0647
Epoch 122/300, seasonal_1 Loss: 0.0752 | 0.0636
Epoch 123/300, seasonal_1 Loss: 0.0755 | 0.0628
Epoch 124/300, seasonal_1 Loss: 0.0752 | 0.0652
Epoch 125/300, seasonal_1 Loss: 0.0745 | 0.0653
Epoch 126/300, seasonal_1 Loss: 0.0743 | 0.0655
Epoch 127/300, seasonal_1 Loss: 0.0756 | 0.0647
Epoch 128/300, seasonal_1 Loss: 0.0771 | 0.0639
Epoch 129/300, seasonal_1 Loss: 0.0752 | 0.0649
Epoch 130/300, seasonal_1 Loss: 0.0725 | 0.0654
Epoch 131/300, seasonal_1 Loss: 0.0716 | 0.0636
Epoch 132/300, seasonal_1 Loss: 0.0712 | 0.0657
Epoch 133/300, seasonal_1 Loss: 0.0706 | 0.0646
Epoch 134/300, seasonal_1 Loss: 0.0701 | 0.0644
Epoch 135/300, seasonal_1 Loss: 0.0701 | 0.0627
Epoch 136/300, seasonal_1 Loss: 0.0702 | 0.0637
Epoch 137/300, seasonal_1 Loss: 0.0701 | 0.0632
Epoch 138/300, seasonal_1 Loss: 0.0700 | 0.0654
Epoch 139/300, seasonal_1 Loss: 0.0696 | 0.0626
Epoch 140/300, seasonal_1 Loss: 0.0693 | 0.0649
Epoch 141/300, seasonal_1 Loss: 0.0694 | 0.0627
Epoch 142/300, seasonal_1 Loss: 0.0694 | 0.0651
Epoch 143/300, seasonal_1 Loss: 0.0694 | 0.0624
Epoch 144/300, seasonal_1 Loss: 0.0695 | 0.0650
Epoch 145/300, seasonal_1 Loss: 0.0693 | 0.0625
Epoch 146/300, seasonal_1 Loss: 0.0695 | 0.0657
Epoch 147/300, seasonal_1 Loss: 0.0693 | 0.0624
Epoch 148/300, seasonal_1 Loss: 0.0691 | 0.0657
Epoch 149/300, seasonal_1 Loss: 0.0691 | 0.0626
Epoch 150/300, seasonal_1 Loss: 0.0689 | 0.0654
Epoch 151/300, seasonal_1 Loss: 0.0689 | 0.0627
Epoch 152/300, seasonal_1 Loss: 0.0689 | 0.0646
Epoch 153/300, seasonal_1 Loss: 0.0687 | 0.0624
Epoch 154/300, seasonal_1 Loss: 0.0687 | 0.0644
Epoch 155/300, seasonal_1 Loss: 0.0685 | 0.0625
Epoch 156/300, seasonal_1 Loss: 0.0684 | 0.0644
Epoch 157/300, seasonal_1 Loss: 0.0683 | 0.0628
Epoch 158/300, seasonal_1 Loss: 0.0682 | 0.0642
Epoch 159/300, seasonal_1 Loss: 0.0682 | 0.0630
Epoch 160/300, seasonal_1 Loss: 0.0682 | 0.0639
Epoch 161/300, seasonal_1 Loss: 0.0682 | 0.0629
Epoch 162/300, seasonal_1 Loss: 0.0681 | 0.0635
Epoch 163/300, seasonal_1 Loss: 0.0680 | 0.0629
Epoch 164/300, seasonal_1 Loss: 0.0680 | 0.0637
Epoch 165/300, seasonal_1 Loss: 0.0680 | 0.0632
Epoch 166/300, seasonal_1 Loss: 0.0679 | 0.0636
Epoch 167/300, seasonal_1 Loss: 0.0680 | 0.0633
Epoch 168/300, seasonal_1 Loss: 0.0680 | 0.0635
Epoch 169/300, seasonal_1 Loss: 0.0680 | 0.0632
Epoch 170/300, seasonal_1 Loss: 0.0680 | 0.0632
Epoch 171/300, seasonal_1 Loss: 0.0680 | 0.0629
Epoch 172/300, seasonal_1 Loss: 0.0680 | 0.0631
Epoch 173/300, seasonal_1 Loss: 0.0680 | 0.0630
Epoch 174/300, seasonal_1 Loss: 0.0681 | 0.0632
Epoch 175/300, seasonal_1 Loss: 0.0681 | 0.0633
Epoch 176/300, seasonal_1 Loss: 0.0681 | 0.0637
Epoch 177/300, seasonal_1 Loss: 0.0679 | 0.0640
Epoch 178/300, seasonal_1 Loss: 0.0676 | 0.0638
Epoch 179/300, seasonal_1 Loss: 0.0675 | 0.0634
Epoch 180/300, seasonal_1 Loss: 0.0674 | 0.0631
Epoch 181/300, seasonal_1 Loss: 0.0673 | 0.0630
Epoch 182/300, seasonal_1 Loss: 0.0672 | 0.0630
Epoch 183/300, seasonal_1 Loss: 0.0672 | 0.0630
Epoch 184/300, seasonal_1 Loss: 0.0672 | 0.0631
Epoch 185/300, seasonal_1 Loss: 0.0671 | 0.0632
Epoch 186/300, seasonal_1 Loss: 0.0671 | 0.0631
Epoch 187/300, seasonal_1 Loss: 0.0671 | 0.0631
Epoch 188/300, seasonal_1 Loss: 0.0670 | 0.0631
Epoch 189/300, seasonal_1 Loss: 0.0670 | 0.0631
Epoch 190/300, seasonal_1 Loss: 0.0670 | 0.0632
Epoch 191/300, seasonal_1 Loss: 0.0669 | 0.0632
Epoch 192/300, seasonal_1 Loss: 0.0669 | 0.0633
Epoch 193/300, seasonal_1 Loss: 0.0669 | 0.0633
Epoch 194/300, seasonal_1 Loss: 0.0669 | 0.0633
Epoch 195/300, seasonal_1 Loss: 0.0668 | 0.0633
Epoch 196/300, seasonal_1 Loss: 0.0668 | 0.0633
Epoch 197/300, seasonal_1 Loss: 0.0668 | 0.0633
Epoch 198/300, seasonal_1 Loss: 0.0667 | 0.0633
Epoch 199/300, seasonal_1 Loss: 0.0667 | 0.0634
Epoch 200/300, seasonal_1 Loss: 0.0667 | 0.0634
Epoch 201/300, seasonal_1 Loss: 0.0667 | 0.0635
Epoch 202/300, seasonal_1 Loss: 0.0666 | 0.0635
Epoch 203/300, seasonal_1 Loss: 0.0666 | 0.0635
Epoch 204/300, seasonal_1 Loss: 0.0666 | 0.0635
Epoch 205/300, seasonal_1 Loss: 0.0666 | 0.0635
Epoch 206/300, seasonal_1 Loss: 0.0665 | 0.0635
Epoch 207/300, seasonal_1 Loss: 0.0665 | 0.0636
Epoch 208/300, seasonal_1 Loss: 0.0665 | 0.0636
Epoch 209/300, seasonal_1 Loss: 0.0665 | 0.0636
Epoch 210/300, seasonal_1 Loss: 0.0664 | 0.0636
Epoch 211/300, seasonal_1 Loss: 0.0664 | 0.0636
Epoch 212/300, seasonal_1 Loss: 0.0664 | 0.0636
Epoch 213/300, seasonal_1 Loss: 0.0664 | 0.0636
Epoch 214/300, seasonal_1 Loss: 0.0663 | 0.0637
Epoch 215/300, seasonal_1 Loss: 0.0663 | 0.0637
Epoch 216/300, seasonal_1 Loss: 0.0663 | 0.0637
Epoch 217/300, seasonal_1 Loss: 0.0663 | 0.0637
Epoch 218/300, seasonal_1 Loss: 0.0663 | 0.0637
Epoch 219/300, seasonal_1 Loss: 0.0662 | 0.0637
Epoch 220/300, seasonal_1 Loss: 0.0662 | 0.0638
Epoch 221/300, seasonal_1 Loss: 0.0662 | 0.0638
Epoch 222/300, seasonal_1 Loss: 0.0662 | 0.0638
Epoch 223/300, seasonal_1 Loss: 0.0661 | 0.0638
Epoch 224/300, seasonal_1 Loss: 0.0661 | 0.0638
Epoch 225/300, seasonal_1 Loss: 0.0661 | 0.0638
Epoch 226/300, seasonal_1 Loss: 0.0661 | 0.0638
Epoch 227/300, seasonal_1 Loss: 0.0661 | 0.0639
Epoch 228/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 229/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 230/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 231/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 232/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 233/300, seasonal_1 Loss: 0.0660 | 0.0639
Epoch 234/300, seasonal_1 Loss: 0.0659 | 0.0640
Epoch 235/300, seasonal_1 Loss: 0.0659 | 0.0640
Epoch 236/300, seasonal_1 Loss: 0.0659 | 0.0640
Epoch 237/300, seasonal_1 Loss: 0.0659 | 0.0640
Epoch 238/300, seasonal_1 Loss: 0.0659 | 0.0640
Epoch 239/300, seasonal_1 Loss: 0.0658 | 0.0640
Epoch 240/300, seasonal_1 Loss: 0.0658 | 0.0640
Epoch 241/300, seasonal_1 Loss: 0.0658 | 0.0640
Epoch 242/300, seasonal_1 Loss: 0.0658 | 0.0640
Epoch 243/300, seasonal_1 Loss: 0.0658 | 0.0641
Epoch 244/300, seasonal_1 Loss: 0.0658 | 0.0641
Epoch 245/300, seasonal_1 Loss: 0.0658 | 0.0641
Epoch 246/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 247/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 248/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 249/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 250/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 251/300, seasonal_1 Loss: 0.0657 | 0.0641
Epoch 252/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 253/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 254/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 255/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 256/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 257/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 258/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 259/300, seasonal_1 Loss: 0.0655 | 0.0642
Epoch 260/300, seasonal_1 Loss: 0.0655 | 0.0642
Epoch 261/300, seasonal_1 Loss: 0.0655 | 0.0642
Epoch 262/300, seasonal_1 Loss: 0.0655 | 0.0642
Epoch 263/300, seasonal_1 Loss: 0.0655 | 0.0642
Epoch 264/300, seasonal_1 Loss: 0.0655 | 0.0643
Epoch 265/300, seasonal_1 Loss: 0.0655 | 0.0643
Epoch 266/300, seasonal_1 Loss: 0.0655 | 0.0643
Epoch 267/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 268/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 269/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 270/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 271/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 272/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 273/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 274/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 275/300, seasonal_1 Loss: 0.0654 | 0.0643
Epoch 276/300, seasonal_1 Loss: 0.0653 | 0.0643
Epoch 277/300, seasonal_1 Loss: 0.0653 | 0.0643
Epoch 278/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 279/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 280/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 281/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 282/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 283/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 284/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 285/300, seasonal_1 Loss: 0.0653 | 0.0644
Epoch 286/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 287/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 288/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 289/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 290/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 291/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 292/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 293/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 294/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 295/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 296/300, seasonal_1 Loss: 0.0652 | 0.0645
Epoch 297/300, seasonal_1 Loss: 0.0652 | 0.0645
Epoch 298/300, seasonal_1 Loss: 0.0651 | 0.0645
Epoch 299/300, seasonal_1 Loss: 0.0651 | 0.0645
Epoch 300/300, seasonal_1 Loss: 0.0651 | 0.0645
Training seasonal_2 component with params: {'observation_period_num': 9, 'train_rates': 0.7631594406147075, 'learning_rate': 0.00012030824842205252, 'batch_size': 50, 'step_size': 12, 'gamma': 0.9540238045873749}
Epoch 1/300, seasonal_2 Loss: 0.5181 | 0.2615
Epoch 2/300, seasonal_2 Loss: 0.1769 | 0.1893
Epoch 3/300, seasonal_2 Loss: 0.1563 | 0.1614
Epoch 4/300, seasonal_2 Loss: 0.1444 | 0.1468
Epoch 5/300, seasonal_2 Loss: 0.1355 | 0.1423
Epoch 6/300, seasonal_2 Loss: 0.1286 | 0.1371
Epoch 7/300, seasonal_2 Loss: 0.1234 | 0.1319
Epoch 8/300, seasonal_2 Loss: 0.1204 | 0.1293
Epoch 9/300, seasonal_2 Loss: 0.1186 | 0.1297
Epoch 10/300, seasonal_2 Loss: 0.1173 | 0.1304
Epoch 11/300, seasonal_2 Loss: 0.1163 | 0.1302
Epoch 12/300, seasonal_2 Loss: 0.1155 | 0.1285
Epoch 13/300, seasonal_2 Loss: 0.1148 | 0.1226
Epoch 14/300, seasonal_2 Loss: 0.1144 | 0.1197
Epoch 15/300, seasonal_2 Loss: 0.1135 | 0.1172
Epoch 16/300, seasonal_2 Loss: 0.1120 | 0.1160
Epoch 17/300, seasonal_2 Loss: 0.1100 | 0.1158
Epoch 18/300, seasonal_2 Loss: 0.1080 | 0.1158
Epoch 19/300, seasonal_2 Loss: 0.1059 | 0.1128
Epoch 20/300, seasonal_2 Loss: 0.1039 | 0.1129
Epoch 21/300, seasonal_2 Loss: 0.1015 | 0.1129
Epoch 22/300, seasonal_2 Loss: 0.0993 | 0.1123
Epoch 23/300, seasonal_2 Loss: 0.0971 | 0.1111
Epoch 24/300, seasonal_2 Loss: 0.0949 | 0.1092
Epoch 25/300, seasonal_2 Loss: 0.0926 | 0.1057
Epoch 26/300, seasonal_2 Loss: 0.0907 | 0.1009
Epoch 27/300, seasonal_2 Loss: 0.0893 | 0.0950
Epoch 28/300, seasonal_2 Loss: 0.0887 | 0.0940
Epoch 29/300, seasonal_2 Loss: 0.0873 | 0.0928
Epoch 30/300, seasonal_2 Loss: 0.0862 | 0.0920
Epoch 31/300, seasonal_2 Loss: 0.0855 | 0.0910
Epoch 32/300, seasonal_2 Loss: 0.0850 | 0.0904
Epoch 33/300, seasonal_2 Loss: 0.0847 | 0.0898
Epoch 34/300, seasonal_2 Loss: 0.0844 | 0.0893
Epoch 35/300, seasonal_2 Loss: 0.0843 | 0.0884
Epoch 36/300, seasonal_2 Loss: 0.0842 | 0.0883
Epoch 37/300, seasonal_2 Loss: 0.0848 | 0.0869
Epoch 38/300, seasonal_2 Loss: 0.0856 | 0.0906
Epoch 39/300, seasonal_2 Loss: 0.0865 | 0.0913
Epoch 40/300, seasonal_2 Loss: 0.0854 | 0.0924
Epoch 41/300, seasonal_2 Loss: 0.0880 | 0.0935
Epoch 42/300, seasonal_2 Loss: 0.0849 | 0.0937
Epoch 43/300, seasonal_2 Loss: 0.0854 | 0.0895
Epoch 44/300, seasonal_2 Loss: 0.0814 | 0.0903
Epoch 45/300, seasonal_2 Loss: 0.0813 | 0.0873
Epoch 46/300, seasonal_2 Loss: 0.0796 | 0.0887
Epoch 47/300, seasonal_2 Loss: 0.0798 | 0.0862
Epoch 48/300, seasonal_2 Loss: 0.0789 | 0.0877
Epoch 49/300, seasonal_2 Loss: 0.0788 | 0.0848
Epoch 50/300, seasonal_2 Loss: 0.0780 | 0.0864
Epoch 51/300, seasonal_2 Loss: 0.0780 | 0.0835
Epoch 52/300, seasonal_2 Loss: 0.0774 | 0.0855
Epoch 53/300, seasonal_2 Loss: 0.0773 | 0.0826
Epoch 54/300, seasonal_2 Loss: 0.0768 | 0.0848
Epoch 55/300, seasonal_2 Loss: 0.0766 | 0.0817
Epoch 56/300, seasonal_2 Loss: 0.0761 | 0.0839
Epoch 57/300, seasonal_2 Loss: 0.0759 | 0.0810
Epoch 58/300, seasonal_2 Loss: 0.0755 | 0.0833
Epoch 59/300, seasonal_2 Loss: 0.0754 | 0.0805
Epoch 60/300, seasonal_2 Loss: 0.0750 | 0.0827
Epoch 61/300, seasonal_2 Loss: 0.0748 | 0.0799
Epoch 62/300, seasonal_2 Loss: 0.0743 | 0.0817
Epoch 63/300, seasonal_2 Loss: 0.0741 | 0.0794
Epoch 64/300, seasonal_2 Loss: 0.0738 | 0.0811
Epoch 65/300, seasonal_2 Loss: 0.0736 | 0.0789
Epoch 66/300, seasonal_2 Loss: 0.0733 | 0.0807
Epoch 67/300, seasonal_2 Loss: 0.0732 | 0.0785
Epoch 68/300, seasonal_2 Loss: 0.0728 | 0.0800
Epoch 69/300, seasonal_2 Loss: 0.0727 | 0.0779
Epoch 70/300, seasonal_2 Loss: 0.0724 | 0.0794
Epoch 71/300, seasonal_2 Loss: 0.0724 | 0.0776
Epoch 72/300, seasonal_2 Loss: 0.0721 | 0.0790
Epoch 73/300, seasonal_2 Loss: 0.0719 | 0.0774
Epoch 74/300, seasonal_2 Loss: 0.0716 | 0.0784
Epoch 75/300, seasonal_2 Loss: 0.0714 | 0.0770
Epoch 76/300, seasonal_2 Loss: 0.0711 | 0.0780
Epoch 77/300, seasonal_2 Loss: 0.0711 | 0.0768
Epoch 78/300, seasonal_2 Loss: 0.0708 | 0.0778
Epoch 79/300, seasonal_2 Loss: 0.0707 | 0.0767
Epoch 80/300, seasonal_2 Loss: 0.0704 | 0.0773
Epoch 81/300, seasonal_2 Loss: 0.0702 | 0.0763
Epoch 82/300, seasonal_2 Loss: 0.0700 | 0.0770
Epoch 83/300, seasonal_2 Loss: 0.0700 | 0.0762
Epoch 84/300, seasonal_2 Loss: 0.0697 | 0.0768
Epoch 85/300, seasonal_2 Loss: 0.0697 | 0.0761
Epoch 86/300, seasonal_2 Loss: 0.0694 | 0.0764
Epoch 87/300, seasonal_2 Loss: 0.0693 | 0.0758
Epoch 88/300, seasonal_2 Loss: 0.0691 | 0.0762
Epoch 89/300, seasonal_2 Loss: 0.0691 | 0.0755
Epoch 90/300, seasonal_2 Loss: 0.0688 | 0.0763
Epoch 91/300, seasonal_2 Loss: 0.0689 | 0.0754
Epoch 92/300, seasonal_2 Loss: 0.0686 | 0.0762
Epoch 93/300, seasonal_2 Loss: 0.0686 | 0.0752
Epoch 94/300, seasonal_2 Loss: 0.0683 | 0.0761
Epoch 95/300, seasonal_2 Loss: 0.0683 | 0.0751
Epoch 96/300, seasonal_2 Loss: 0.0680 | 0.0759
Epoch 97/300, seasonal_2 Loss: 0.0680 | 0.0749
Epoch 98/300, seasonal_2 Loss: 0.0677 | 0.0753
Epoch 99/300, seasonal_2 Loss: 0.0677 | 0.0746
Epoch 100/300, seasonal_2 Loss: 0.0674 | 0.0751
Epoch 101/300, seasonal_2 Loss: 0.0676 | 0.0745
Epoch 102/300, seasonal_2 Loss: 0.0671 | 0.0752
Epoch 103/300, seasonal_2 Loss: 0.0674 | 0.0746
Epoch 104/300, seasonal_2 Loss: 0.0669 | 0.0750
Epoch 105/300, seasonal_2 Loss: 0.0669 | 0.0747
Epoch 106/300, seasonal_2 Loss: 0.0666 | 0.0746
Epoch 107/300, seasonal_2 Loss: 0.0666 | 0.0746
Epoch 108/300, seasonal_2 Loss: 0.0664 | 0.0740
Epoch 109/300, seasonal_2 Loss: 0.0666 | 0.0736
Epoch 110/300, seasonal_2 Loss: 0.0667 | 0.0738
Epoch 111/300, seasonal_2 Loss: 0.0669 | 0.0737
Epoch 112/300, seasonal_2 Loss: 0.0666 | 0.0732
Epoch 113/300, seasonal_2 Loss: 0.0660 | 0.0729
Epoch 114/300, seasonal_2 Loss: 0.0656 | 0.0723
Epoch 115/300, seasonal_2 Loss: 0.0650 | 0.0717
Epoch 116/300, seasonal_2 Loss: 0.0648 | 0.0718
Epoch 117/300, seasonal_2 Loss: 0.0644 | 0.0715
Epoch 118/300, seasonal_2 Loss: 0.0642 | 0.0721
Epoch 119/300, seasonal_2 Loss: 0.0641 | 0.0717
Epoch 120/300, seasonal_2 Loss: 0.0639 | 0.0725
Epoch 121/300, seasonal_2 Loss: 0.0639 | 0.0718
Epoch 122/300, seasonal_2 Loss: 0.0636 | 0.0723
Epoch 123/300, seasonal_2 Loss: 0.0637 | 0.0717
Epoch 124/300, seasonal_2 Loss: 0.0633 | 0.0722
Epoch 125/300, seasonal_2 Loss: 0.0634 | 0.0717
Epoch 126/300, seasonal_2 Loss: 0.0631 | 0.0721
Epoch 127/300, seasonal_2 Loss: 0.0631 | 0.0714
Epoch 128/300, seasonal_2 Loss: 0.0628 | 0.0717
Epoch 129/300, seasonal_2 Loss: 0.0628 | 0.0710
Epoch 130/300, seasonal_2 Loss: 0.0624 | 0.0715
Epoch 131/300, seasonal_2 Loss: 0.0625 | 0.0709
Epoch 132/300, seasonal_2 Loss: 0.0622 | 0.0715
Epoch 133/300, seasonal_2 Loss: 0.0623 | 0.0708
Epoch 134/300, seasonal_2 Loss: 0.0620 | 0.0712
Epoch 135/300, seasonal_2 Loss: 0.0620 | 0.0706
Epoch 136/300, seasonal_2 Loss: 0.0617 | 0.0711
Epoch 137/300, seasonal_2 Loss: 0.0619 | 0.0705
Epoch 138/300, seasonal_2 Loss: 0.0616 | 0.0711
Epoch 139/300, seasonal_2 Loss: 0.0617 | 0.0703
Epoch 140/300, seasonal_2 Loss: 0.0614 | 0.0708
Epoch 141/300, seasonal_2 Loss: 0.0615 | 0.0702
Epoch 142/300, seasonal_2 Loss: 0.0612 | 0.0708
Epoch 143/300, seasonal_2 Loss: 0.0614 | 0.0702
Epoch 144/300, seasonal_2 Loss: 0.0610 | 0.0708
Epoch 145/300, seasonal_2 Loss: 0.0613 | 0.0702
Epoch 146/300, seasonal_2 Loss: 0.0609 | 0.0708
Epoch 147/300, seasonal_2 Loss: 0.0611 | 0.0702
Epoch 148/300, seasonal_2 Loss: 0.0608 | 0.0709
Epoch 149/300, seasonal_2 Loss: 0.0610 | 0.0703
Epoch 150/300, seasonal_2 Loss: 0.0606 | 0.0710
Epoch 151/300, seasonal_2 Loss: 0.0608 | 0.0705
Epoch 152/300, seasonal_2 Loss: 0.0605 | 0.0711
Epoch 153/300, seasonal_2 Loss: 0.0606 | 0.0703
Epoch 154/300, seasonal_2 Loss: 0.0603 | 0.0709
Epoch 155/300, seasonal_2 Loss: 0.0605 | 0.0701
Epoch 156/300, seasonal_2 Loss: 0.0602 | 0.0708
Epoch 157/300, seasonal_2 Loss: 0.0604 | 0.0699
Epoch 158/300, seasonal_2 Loss: 0.0602 | 0.0706
Epoch 159/300, seasonal_2 Loss: 0.0603 | 0.0697
Epoch 160/300, seasonal_2 Loss: 0.0603 | 0.0705
Epoch 161/300, seasonal_2 Loss: 0.0604 | 0.0696
Epoch 162/300, seasonal_2 Loss: 0.0603 | 0.0705
Epoch 163/300, seasonal_2 Loss: 0.0602 | 0.0696
Epoch 164/300, seasonal_2 Loss: 0.0600 | 0.0703
Epoch 165/300, seasonal_2 Loss: 0.0598 | 0.0697
Epoch 166/300, seasonal_2 Loss: 0.0597 | 0.0704
Epoch 167/300, seasonal_2 Loss: 0.0598 | 0.0699
Epoch 168/300, seasonal_2 Loss: 0.0597 | 0.0705
Epoch 169/300, seasonal_2 Loss: 0.0597 | 0.0699
Epoch 170/300, seasonal_2 Loss: 0.0596 | 0.0704
Epoch 171/300, seasonal_2 Loss: 0.0595 | 0.0700
Epoch 172/300, seasonal_2 Loss: 0.0595 | 0.0705
Epoch 173/300, seasonal_2 Loss: 0.0595 | 0.0701
Epoch 174/300, seasonal_2 Loss: 0.0594 | 0.0705
Epoch 175/300, seasonal_2 Loss: 0.0594 | 0.0702
Epoch 176/300, seasonal_2 Loss: 0.0593 | 0.0706
Epoch 177/300, seasonal_2 Loss: 0.0594 | 0.0702
Epoch 178/300, seasonal_2 Loss: 0.0594 | 0.0706
Epoch 179/300, seasonal_2 Loss: 0.0595 | 0.0704
Epoch 180/300, seasonal_2 Loss: 0.0595 | 0.0709
Epoch 181/300, seasonal_2 Loss: 0.0599 | 0.0715
Epoch 182/300, seasonal_2 Loss: 0.0602 | 0.0721
Epoch 183/300, seasonal_2 Loss: 0.0606 | 0.0715
Epoch 184/300, seasonal_2 Loss: 0.0605 | 0.0713
Epoch 185/300, seasonal_2 Loss: 0.0605 | 0.0705
Epoch 186/300, seasonal_2 Loss: 0.0603 | 0.0708
Epoch 187/300, seasonal_2 Loss: 0.0602 | 0.0704
Epoch 188/300, seasonal_2 Loss: 0.0602 | 0.0708
Epoch 189/300, seasonal_2 Loss: 0.0603 | 0.0710
Epoch 190/300, seasonal_2 Loss: 0.0603 | 0.0714
Epoch 191/300, seasonal_2 Loss: 0.0603 | 0.0714
Epoch 192/300, seasonal_2 Loss: 0.0600 | 0.0714
Epoch 193/300, seasonal_2 Loss: 0.0598 | 0.0709
Epoch 194/300, seasonal_2 Loss: 0.0595 | 0.0711
Epoch 195/300, seasonal_2 Loss: 0.0591 | 0.0706
Epoch 196/300, seasonal_2 Loss: 0.0587 | 0.0708
Epoch 197/300, seasonal_2 Loss: 0.0583 | 0.0705
Epoch 198/300, seasonal_2 Loss: 0.0581 | 0.0707
Epoch 199/300, seasonal_2 Loss: 0.0579 | 0.0705
Epoch 200/300, seasonal_2 Loss: 0.0577 | 0.0707
Epoch 201/300, seasonal_2 Loss: 0.0576 | 0.0705
Epoch 202/300, seasonal_2 Loss: 0.0575 | 0.0707
Epoch 203/300, seasonal_2 Loss: 0.0574 | 0.0706
Epoch 204/300, seasonal_2 Loss: 0.0573 | 0.0708
Epoch 205/300, seasonal_2 Loss: 0.0573 | 0.0707
Epoch 206/300, seasonal_2 Loss: 0.0572 | 0.0708
Epoch 207/300, seasonal_2 Loss: 0.0571 | 0.0707
Epoch 208/300, seasonal_2 Loss: 0.0570 | 0.0708
Epoch 209/300, seasonal_2 Loss: 0.0570 | 0.0708
Epoch 210/300, seasonal_2 Loss: 0.0568 | 0.0709
Epoch 211/300, seasonal_2 Loss: 0.0568 | 0.0708
Epoch 212/300, seasonal_2 Loss: 0.0566 | 0.0709
Epoch 213/300, seasonal_2 Loss: 0.0566 | 0.0709
Epoch 214/300, seasonal_2 Loss: 0.0564 | 0.0709
Epoch 215/300, seasonal_2 Loss: 0.0564 | 0.0709
Epoch 216/300, seasonal_2 Loss: 0.0562 | 0.0709
Epoch 217/300, seasonal_2 Loss: 0.0562 | 0.0709
Epoch 218/300, seasonal_2 Loss: 0.0560 | 0.0709
Epoch 219/300, seasonal_2 Loss: 0.0560 | 0.0710
Epoch 220/300, seasonal_2 Loss: 0.0558 | 0.0710
Epoch 221/300, seasonal_2 Loss: 0.0558 | 0.0710
Epoch 222/300, seasonal_2 Loss: 0.0556 | 0.0710
Epoch 223/300, seasonal_2 Loss: 0.0556 | 0.0711
Epoch 224/300, seasonal_2 Loss: 0.0554 | 0.0710
Epoch 225/300, seasonal_2 Loss: 0.0554 | 0.0711
Epoch 226/300, seasonal_2 Loss: 0.0551 | 0.0711
Epoch 227/300, seasonal_2 Loss: 0.0551 | 0.0712
Epoch 228/300, seasonal_2 Loss: 0.0549 | 0.0712
Epoch 229/300, seasonal_2 Loss: 0.0548 | 0.0712
Epoch 230/300, seasonal_2 Loss: 0.0546 | 0.0712
Epoch 231/300, seasonal_2 Loss: 0.0545 | 0.0713
Epoch 232/300, seasonal_2 Loss: 0.0543 | 0.0713
Epoch 233/300, seasonal_2 Loss: 0.0542 | 0.0714
Epoch 234/300, seasonal_2 Loss: 0.0539 | 0.0713
Epoch 235/300, seasonal_2 Loss: 0.0537 | 0.0714
Epoch 236/300, seasonal_2 Loss: 0.0534 | 0.0714
Epoch 237/300, seasonal_2 Loss: 0.0532 | 0.0714
Epoch 238/300, seasonal_2 Loss: 0.0529 | 0.0714
Epoch 239/300, seasonal_2 Loss: 0.0527 | 0.0715
Epoch 240/300, seasonal_2 Loss: 0.0522 | 0.0715
Epoch 241/300, seasonal_2 Loss: 0.0519 | 0.0715
Epoch 242/300, seasonal_2 Loss: 0.0515 | 0.0715
Epoch 243/300, seasonal_2 Loss: 0.0512 | 0.0716
Epoch 244/300, seasonal_2 Loss: 0.0507 | 0.0715
Epoch 245/300, seasonal_2 Loss: 0.0503 | 0.0716
Epoch 246/300, seasonal_2 Loss: 0.0499 | 0.0716
Epoch 247/300, seasonal_2 Loss: 0.0496 | 0.0717
Epoch 248/300, seasonal_2 Loss: 0.0492 | 0.0716
Epoch 249/300, seasonal_2 Loss: 0.0490 | 0.0716
Epoch 250/300, seasonal_2 Loss: 0.0486 | 0.0715
Epoch 251/300, seasonal_2 Loss: 0.0484 | 0.0716
Epoch 252/300, seasonal_2 Loss: 0.0481 | 0.0714
Epoch 253/300, seasonal_2 Loss: 0.0479 | 0.0714
Epoch 254/300, seasonal_2 Loss: 0.0477 | 0.0713
Epoch 255/300, seasonal_2 Loss: 0.0476 | 0.0712
Epoch 256/300, seasonal_2 Loss: 0.0475 | 0.0712
Epoch 257/300, seasonal_2 Loss: 0.0474 | 0.0710
Epoch 258/300, seasonal_2 Loss: 0.0471 | 0.0711
Epoch 259/300, seasonal_2 Loss: 0.0470 | 0.0707
Epoch 260/300, seasonal_2 Loss: 0.0468 | 0.0710
Epoch 261/300, seasonal_2 Loss: 0.0468 | 0.0706
Epoch 262/300, seasonal_2 Loss: 0.0466 | 0.0708
Epoch 263/300, seasonal_2 Loss: 0.0465 | 0.0705
Epoch 264/300, seasonal_2 Loss: 0.0464 | 0.0707
Epoch 265/300, seasonal_2 Loss: 0.0463 | 0.0704
Epoch 266/300, seasonal_2 Loss: 0.0462 | 0.0706
Epoch 267/300, seasonal_2 Loss: 0.0461 | 0.0703
Epoch 268/300, seasonal_2 Loss: 0.0460 | 0.0705
Epoch 269/300, seasonal_2 Loss: 0.0459 | 0.0703
Epoch 270/300, seasonal_2 Loss: 0.0459 | 0.0705
Epoch 271/300, seasonal_2 Loss: 0.0458 | 0.0702
Epoch 272/300, seasonal_2 Loss: 0.0458 | 0.0704
Epoch 273/300, seasonal_2 Loss: 0.0457 | 0.0702
Epoch 274/300, seasonal_2 Loss: 0.0457 | 0.0704
Epoch 275/300, seasonal_2 Loss: 0.0456 | 0.0702
Epoch 276/300, seasonal_2 Loss: 0.0456 | 0.0704
Epoch 277/300, seasonal_2 Loss: 0.0455 | 0.0702
Epoch 278/300, seasonal_2 Loss: 0.0455 | 0.0704
Epoch 279/300, seasonal_2 Loss: 0.0454 | 0.0702
Epoch 280/300, seasonal_2 Loss: 0.0454 | 0.0705
Epoch 281/300, seasonal_2 Loss: 0.0453 | 0.0702
Epoch 282/300, seasonal_2 Loss: 0.0453 | 0.0705
Epoch 283/300, seasonal_2 Loss: 0.0452 | 0.0702
Epoch 284/300, seasonal_2 Loss: 0.0452 | 0.0706
Epoch 285/300, seasonal_2 Loss: 0.0451 | 0.0702
Epoch 286/300, seasonal_2 Loss: 0.0452 | 0.0706
Epoch 287/300, seasonal_2 Loss: 0.0450 | 0.0702
Epoch 288/300, seasonal_2 Loss: 0.0451 | 0.0707
Epoch 289/300, seasonal_2 Loss: 0.0449 | 0.0703
Epoch 290/300, seasonal_2 Loss: 0.0450 | 0.0708
Epoch 291/300, seasonal_2 Loss: 0.0448 | 0.0703
Epoch 292/300, seasonal_2 Loss: 0.0449 | 0.0708
Epoch 293/300, seasonal_2 Loss: 0.0447 | 0.0703
Epoch 294/300, seasonal_2 Loss: 0.0448 | 0.0708
Epoch 295/300, seasonal_2 Loss: 0.0446 | 0.0703
Epoch 296/300, seasonal_2 Loss: 0.0446 | 0.0708
Epoch 297/300, seasonal_2 Loss: 0.0445 | 0.0703
Epoch 298/300, seasonal_2 Loss: 0.0445 | 0.0707
Epoch 299/300, seasonal_2 Loss: 0.0444 | 0.0703
Epoch 300/300, seasonal_2 Loss: 0.0444 | 0.0706
Training seasonal_3 component with params: {'observation_period_num': 9, 'train_rates': 0.7629222385124196, 'learning_rate': 0.00016327307235987321, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8226944168681165}
Epoch 1/300, seasonal_3 Loss: 0.2870 | 0.1926
Epoch 2/300, seasonal_3 Loss: 0.1675 | 0.1675
Epoch 3/300, seasonal_3 Loss: 0.1649 | 0.1645
Epoch 4/300, seasonal_3 Loss: 0.1855 | 0.1388
Epoch 5/300, seasonal_3 Loss: 0.1409 | 0.1322
Epoch 6/300, seasonal_3 Loss: 0.1242 | 0.1266
Epoch 7/300, seasonal_3 Loss: 0.1208 | 0.1236
Epoch 8/300, seasonal_3 Loss: 0.1192 | 0.1205
Epoch 9/300, seasonal_3 Loss: 0.1166 | 0.1163
Epoch 10/300, seasonal_3 Loss: 0.1135 | 0.1146
Epoch 11/300, seasonal_3 Loss: 0.1112 | 0.1132
Epoch 12/300, seasonal_3 Loss: 0.1096 | 0.1116
Epoch 13/300, seasonal_3 Loss: 0.1086 | 0.1105
Epoch 14/300, seasonal_3 Loss: 0.1066 | 0.1104
Epoch 15/300, seasonal_3 Loss: 0.1053 | 0.1093
Epoch 16/300, seasonal_3 Loss: 0.1045 | 0.1096
Epoch 17/300, seasonal_3 Loss: 0.1032 | 0.1101
Epoch 18/300, seasonal_3 Loss: 0.1020 | 0.1096
Epoch 19/300, seasonal_3 Loss: 0.1006 | 0.1085
Epoch 20/300, seasonal_3 Loss: 0.0998 | 0.1081
Epoch 21/300, seasonal_3 Loss: 0.0989 | 0.1078
Epoch 22/300, seasonal_3 Loss: 0.0978 | 0.1080
Epoch 23/300, seasonal_3 Loss: 0.0971 | 0.1073
Epoch 24/300, seasonal_3 Loss: 0.0965 | 0.1059
Epoch 25/300, seasonal_3 Loss: 0.0958 | 0.1042
Epoch 26/300, seasonal_3 Loss: 0.0951 | 0.1038
Epoch 27/300, seasonal_3 Loss: 0.0946 | 0.1030
Epoch 28/300, seasonal_3 Loss: 0.0941 | 0.1017
Epoch 29/300, seasonal_3 Loss: 0.0936 | 0.1018
Epoch 30/300, seasonal_3 Loss: 0.0933 | 0.1011
Epoch 31/300, seasonal_3 Loss: 0.0930 | 0.0999
Epoch 32/300, seasonal_3 Loss: 0.0926 | 0.0990
Epoch 33/300, seasonal_3 Loss: 0.0922 | 0.0993
Epoch 34/300, seasonal_3 Loss: 0.0920 | 0.0985
Epoch 35/300, seasonal_3 Loss: 0.0917 | 0.0976
Epoch 36/300, seasonal_3 Loss: 0.0914 | 0.0976
Epoch 37/300, seasonal_3 Loss: 0.0913 | 0.0966
Epoch 38/300, seasonal_3 Loss: 0.0910 | 0.0959
Epoch 39/300, seasonal_3 Loss: 0.0905 | 0.0952
Epoch 40/300, seasonal_3 Loss: 0.0902 | 0.0951
Epoch 41/300, seasonal_3 Loss: 0.0901 | 0.0944
Epoch 42/300, seasonal_3 Loss: 0.0897 | 0.0938
Epoch 43/300, seasonal_3 Loss: 0.0894 | 0.0938
Epoch 44/300, seasonal_3 Loss: 0.0892 | 0.0933
Epoch 45/300, seasonal_3 Loss: 0.0889 | 0.0930
Epoch 46/300, seasonal_3 Loss: 0.0885 | 0.0926
Epoch 47/300, seasonal_3 Loss: 0.0882 | 0.0924
Epoch 48/300, seasonal_3 Loss: 0.0880 | 0.0921
Epoch 49/300, seasonal_3 Loss: 0.0876 | 0.0917
Epoch 50/300, seasonal_3 Loss: 0.0874 | 0.0914
Epoch 51/300, seasonal_3 Loss: 0.0871 | 0.0912
Epoch 52/300, seasonal_3 Loss: 0.0869 | 0.0910
Epoch 53/300, seasonal_3 Loss: 0.0868 | 0.0907
Epoch 54/300, seasonal_3 Loss: 0.0866 | 0.0905
Epoch 55/300, seasonal_3 Loss: 0.0865 | 0.0903
Epoch 56/300, seasonal_3 Loss: 0.0863 | 0.0901
Epoch 57/300, seasonal_3 Loss: 0.0862 | 0.0900
Epoch 58/300, seasonal_3 Loss: 0.0861 | 0.0898
Epoch 59/300, seasonal_3 Loss: 0.0860 | 0.0897
Epoch 60/300, seasonal_3 Loss: 0.0859 | 0.0895
Epoch 61/300, seasonal_3 Loss: 0.0858 | 0.0894
Epoch 62/300, seasonal_3 Loss: 0.0858 | 0.0893
Epoch 63/300, seasonal_3 Loss: 0.0857 | 0.0892
Epoch 64/300, seasonal_3 Loss: 0.0856 | 0.0891
Epoch 65/300, seasonal_3 Loss: 0.0855 | 0.0890
Epoch 66/300, seasonal_3 Loss: 0.0855 | 0.0889
Epoch 67/300, seasonal_3 Loss: 0.0854 | 0.0889
Epoch 68/300, seasonal_3 Loss: 0.0853 | 0.0888
Epoch 69/300, seasonal_3 Loss: 0.0853 | 0.0887
Epoch 70/300, seasonal_3 Loss: 0.0852 | 0.0886
Epoch 71/300, seasonal_3 Loss: 0.0852 | 0.0886
Epoch 72/300, seasonal_3 Loss: 0.0851 | 0.0885
Epoch 73/300, seasonal_3 Loss: 0.0851 | 0.0885
Epoch 74/300, seasonal_3 Loss: 0.0851 | 0.0884
Epoch 75/300, seasonal_3 Loss: 0.0850 | 0.0884
Epoch 76/300, seasonal_3 Loss: 0.0850 | 0.0883
Epoch 77/300, seasonal_3 Loss: 0.0849 | 0.0883
Epoch 78/300, seasonal_3 Loss: 0.0849 | 0.0882
Epoch 79/300, seasonal_3 Loss: 0.0849 | 0.0882
Epoch 80/300, seasonal_3 Loss: 0.0848 | 0.0882
Epoch 81/300, seasonal_3 Loss: 0.0848 | 0.0881
Epoch 82/300, seasonal_3 Loss: 0.0848 | 0.0881
Epoch 83/300, seasonal_3 Loss: 0.0848 | 0.0880
Epoch 84/300, seasonal_3 Loss: 0.0847 | 0.0880
Epoch 85/300, seasonal_3 Loss: 0.0847 | 0.0880
Epoch 86/300, seasonal_3 Loss: 0.0847 | 0.0880
Epoch 87/300, seasonal_3 Loss: 0.0847 | 0.0879
Epoch 88/300, seasonal_3 Loss: 0.0846 | 0.0879
Epoch 89/300, seasonal_3 Loss: 0.0846 | 0.0879
Epoch 90/300, seasonal_3 Loss: 0.0846 | 0.0879
Epoch 91/300, seasonal_3 Loss: 0.0846 | 0.0878
Epoch 92/300, seasonal_3 Loss: 0.0846 | 0.0878
Epoch 93/300, seasonal_3 Loss: 0.0846 | 0.0878
Epoch 94/300, seasonal_3 Loss: 0.0845 | 0.0878
Epoch 95/300, seasonal_3 Loss: 0.0845 | 0.0878
Epoch 96/300, seasonal_3 Loss: 0.0845 | 0.0878
Epoch 97/300, seasonal_3 Loss: 0.0845 | 0.0877
Epoch 98/300, seasonal_3 Loss: 0.0845 | 0.0877
Epoch 99/300, seasonal_3 Loss: 0.0845 | 0.0877
Epoch 100/300, seasonal_3 Loss: 0.0845 | 0.0877
Epoch 101/300, seasonal_3 Loss: 0.0845 | 0.0877
Epoch 102/300, seasonal_3 Loss: 0.0844 | 0.0877
Epoch 103/300, seasonal_3 Loss: 0.0844 | 0.0877
Epoch 104/300, seasonal_3 Loss: 0.0844 | 0.0877
Epoch 105/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 106/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 107/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 108/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 109/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 110/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 111/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 112/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 113/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 114/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 115/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 116/300, seasonal_3 Loss: 0.0844 | 0.0876
Epoch 117/300, seasonal_3 Loss: 0.0843 | 0.0876
Epoch 118/300, seasonal_3 Loss: 0.0843 | 0.0876
Epoch 119/300, seasonal_3 Loss: 0.0843 | 0.0876
Epoch 120/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 121/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 122/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 123/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 124/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 125/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 126/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 127/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 128/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 129/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 130/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 131/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 132/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 133/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 134/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 135/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 136/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 137/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 138/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 139/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 140/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 141/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 142/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 143/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 144/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 145/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 146/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 147/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 148/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 149/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 150/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 151/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 152/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 153/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 154/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 155/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 156/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 157/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 158/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 159/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 160/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 161/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 162/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 163/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 164/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 165/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 166/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 167/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 168/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 169/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 170/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 171/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 172/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 173/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 174/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 175/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 176/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 177/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 178/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 179/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 180/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 181/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 182/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 183/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 184/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 185/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 186/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 187/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 188/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 189/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 190/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 191/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 192/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 193/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 194/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 195/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 196/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 197/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 198/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 199/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 200/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 201/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 202/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 203/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 204/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 205/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 206/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 207/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 208/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 209/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 210/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 211/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 212/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 213/300, seasonal_3 Loss: 0.0843 | 0.0875
Epoch 214/300, seasonal_3 Loss: 0.0843 | 0.0875
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 30, 'train_rates': 0.9867764221645923, 'learning_rate': 5.696177205540662e-05, 'batch_size': 202, 'step_size': 4, 'gamma': 0.9495848880377725}
Epoch 1/300, resid Loss: 0.5522 | 0.4556
Epoch 2/300, resid Loss: 0.4749 | 0.3686
Epoch 3/300, resid Loss: 0.3662 | 0.2896
Epoch 4/300, resid Loss: 0.3123 | 0.2617
Epoch 5/300, resid Loss: 0.2704 | 0.2062
Epoch 6/300, resid Loss: 0.2432 | 0.2005
Epoch 7/300, resid Loss: 0.2231 | 0.1714
Epoch 8/300, resid Loss: 0.2072 | 0.1626
Epoch 9/300, resid Loss: 0.1987 | 0.1581
Epoch 10/300, resid Loss: 0.1878 | 0.1408
Epoch 11/300, resid Loss: 0.1847 | 0.1514
Epoch 12/300, resid Loss: 0.1752 | 0.1281
Epoch 13/300, resid Loss: 0.1740 | 0.1444
Epoch 14/300, resid Loss: 0.1655 | 0.1209
Epoch 15/300, resid Loss: 0.1641 | 0.1352
Epoch 16/300, resid Loss: 0.1570 | 0.1162
Epoch 17/300, resid Loss: 0.1551 | 0.1254
Epoch 18/300, resid Loss: 0.1500 | 0.1125
Epoch 19/300, resid Loss: 0.1480 | 0.1174
Epoch 20/300, resid Loss: 0.1445 | 0.1087
Epoch 21/300, resid Loss: 0.1427 | 0.1115
Epoch 22/300, resid Loss: 0.1402 | 0.1047
Epoch 23/300, resid Loss: 0.1387 | 0.1069
Epoch 24/300, resid Loss: 0.1367 | 0.1009
Epoch 25/300, resid Loss: 0.1353 | 0.1027
Epoch 26/300, resid Loss: 0.1337 | 0.0978
Epoch 27/300, resid Loss: 0.1325 | 0.0987
Epoch 28/300, resid Loss: 0.1312 | 0.0951
Epoch 29/300, resid Loss: 0.1301 | 0.0954
Epoch 30/300, resid Loss: 0.1291 | 0.0927
Epoch 31/300, resid Loss: 0.1281 | 0.0926
Epoch 32/300, resid Loss: 0.1273 | 0.0906
Epoch 33/300, resid Loss: 0.1264 | 0.0903
Epoch 34/300, resid Loss: 0.1257 | 0.0888
Epoch 35/300, resid Loss: 0.1249 | 0.0884
Epoch 36/300, resid Loss: 0.1243 | 0.0872
Epoch 37/300, resid Loss: 0.1236 | 0.0867
Epoch 38/300, resid Loss: 0.1230 | 0.0857
Epoch 39/300, resid Loss: 0.1224 | 0.0853
Epoch 40/300, resid Loss: 0.1218 | 0.0844
Epoch 41/300, resid Loss: 0.1212 | 0.0840
Epoch 42/300, resid Loss: 0.1207 | 0.0832
Epoch 43/300, resid Loss: 0.1202 | 0.0828
Epoch 44/300, resid Loss: 0.1197 | 0.0821
Epoch 45/300, resid Loss: 0.1192 | 0.0818
Epoch 46/300, resid Loss: 0.1188 | 0.0811
Epoch 47/300, resid Loss: 0.1183 | 0.0808
Epoch 48/300, resid Loss: 0.1179 | 0.0802
Epoch 49/300, resid Loss: 0.1175 | 0.0800
Epoch 50/300, resid Loss: 0.1171 | 0.0793
Epoch 51/300, resid Loss: 0.1167 | 0.0792
Epoch 52/300, resid Loss: 0.1164 | 0.0784
Epoch 53/300, resid Loss: 0.1160 | 0.0785
Epoch 54/300, resid Loss: 0.1157 | 0.0777
Epoch 55/300, resid Loss: 0.1154 | 0.0779
Epoch 56/300, resid Loss: 0.1151 | 0.0769
Epoch 57/300, resid Loss: 0.1149 | 0.0775
Epoch 58/300, resid Loss: 0.1147 | 0.0762
Epoch 59/300, resid Loss: 0.1146 | 0.0773
Epoch 60/300, resid Loss: 0.1146 | 0.0756
Epoch 61/300, resid Loss: 0.1150 | 0.0774
Epoch 62/300, resid Loss: 0.1156 | 0.0751
Epoch 63/300, resid Loss: 0.1171 | 0.0777
Epoch 64/300, resid Loss: 0.1190 | 0.0751
Epoch 65/300, resid Loss: 0.1220 | 0.0777
Epoch 66/300, resid Loss: 0.1242 | 0.0782
Epoch 67/300, resid Loss: 0.1245 | 0.0788
Epoch 68/300, resid Loss: 0.1220 | 0.0811
Epoch 69/300, resid Loss: 0.1186 | 0.0777
Epoch 70/300, resid Loss: 0.1152 | 0.0765
Epoch 71/300, resid Loss: 0.1133 | 0.0751
Epoch 72/300, resid Loss: 0.1120 | 0.0740
Epoch 73/300, resid Loss: 0.1114 | 0.0740
Epoch 74/300, resid Loss: 0.1111 | 0.0734
Epoch 75/300, resid Loss: 0.1108 | 0.0735
Epoch 76/300, resid Loss: 0.1107 | 0.0731
Epoch 77/300, resid Loss: 0.1105 | 0.0731
Epoch 78/300, resid Loss: 0.1104 | 0.0728
Epoch 79/300, resid Loss: 0.1102 | 0.0727
Epoch 80/300, resid Loss: 0.1101 | 0.0726
Epoch 81/300, resid Loss: 0.1100 | 0.0725
Epoch 82/300, resid Loss: 0.1098 | 0.0723
Epoch 83/300, resid Loss: 0.1097 | 0.0722
Epoch 84/300, resid Loss: 0.1096 | 0.0721
Epoch 85/300, resid Loss: 0.1095 | 0.0720
Epoch 86/300, resid Loss: 0.1093 | 0.0719
Epoch 87/300, resid Loss: 0.1092 | 0.0718
Epoch 88/300, resid Loss: 0.1091 | 0.0717
Epoch 89/300, resid Loss: 0.1090 | 0.0715
Epoch 90/300, resid Loss: 0.1089 | 0.0714
Epoch 91/300, resid Loss: 0.1088 | 0.0714
Epoch 92/300, resid Loss: 0.1087 | 0.0713
Epoch 93/300, resid Loss: 0.1086 | 0.0712
Epoch 94/300, resid Loss: 0.1085 | 0.0711
Epoch 95/300, resid Loss: 0.1084 | 0.0710
Epoch 96/300, resid Loss: 0.1083 | 0.0709
Epoch 97/300, resid Loss: 0.1082 | 0.0708
Epoch 98/300, resid Loss: 0.1081 | 0.0707
Epoch 99/300, resid Loss: 0.1080 | 0.0707
Epoch 100/300, resid Loss: 0.1079 | 0.0706
Epoch 101/300, resid Loss: 0.1079 | 0.0705
Epoch 102/300, resid Loss: 0.1078 | 0.0704
Epoch 103/300, resid Loss: 0.1077 | 0.0704
Epoch 104/300, resid Loss: 0.1076 | 0.0703
Epoch 105/300, resid Loss: 0.1075 | 0.0702
Epoch 106/300, resid Loss: 0.1075 | 0.0702
Epoch 107/300, resid Loss: 0.1074 | 0.0701
Epoch 108/300, resid Loss: 0.1073 | 0.0700
Epoch 109/300, resid Loss: 0.1073 | 0.0700
Epoch 110/300, resid Loss: 0.1072 | 0.0699
Epoch 111/300, resid Loss: 0.1071 | 0.0699
Epoch 112/300, resid Loss: 0.1071 | 0.0698
Epoch 113/300, resid Loss: 0.1070 | 0.0698
Epoch 114/300, resid Loss: 0.1069 | 0.0697
Epoch 115/300, resid Loss: 0.1069 | 0.0696
Epoch 116/300, resid Loss: 0.1068 | 0.0696
Epoch 117/300, resid Loss: 0.1068 | 0.0695
Epoch 118/300, resid Loss: 0.1067 | 0.0695
Epoch 119/300, resid Loss: 0.1067 | 0.0694
Epoch 120/300, resid Loss: 0.1066 | 0.0694
Epoch 121/300, resid Loss: 0.1065 | 0.0694
Epoch 122/300, resid Loss: 0.1065 | 0.0693
Epoch 123/300, resid Loss: 0.1064 | 0.0693
Epoch 124/300, resid Loss: 0.1064 | 0.0692
Epoch 125/300, resid Loss: 0.1064 | 0.0692
Epoch 126/300, resid Loss: 0.1063 | 0.0691
Epoch 127/300, resid Loss: 0.1063 | 0.0691
Epoch 128/300, resid Loss: 0.1062 | 0.0691
Epoch 129/300, resid Loss: 0.1062 | 0.0690
Epoch 130/300, resid Loss: 0.1061 | 0.0690
Epoch 131/300, resid Loss: 0.1061 | 0.0690
Epoch 132/300, resid Loss: 0.1060 | 0.0689
Epoch 133/300, resid Loss: 0.1060 | 0.0689
Epoch 134/300, resid Loss: 0.1060 | 0.0689
Epoch 135/300, resid Loss: 0.1059 | 0.0688
Epoch 136/300, resid Loss: 0.1059 | 0.0688
Epoch 137/300, resid Loss: 0.1059 | 0.0688
Epoch 138/300, resid Loss: 0.1058 | 0.0687
Epoch 139/300, resid Loss: 0.1058 | 0.0687
Epoch 140/300, resid Loss: 0.1058 | 0.0687
Epoch 141/300, resid Loss: 0.1057 | 0.0687
Epoch 142/300, resid Loss: 0.1057 | 0.0686
Epoch 143/300, resid Loss: 0.1057 | 0.0686
Epoch 144/300, resid Loss: 0.1056 | 0.0686
Epoch 145/300, resid Loss: 0.1056 | 0.0686
Epoch 146/300, resid Loss: 0.1056 | 0.0685
Epoch 147/300, resid Loss: 0.1055 | 0.0685
Epoch 148/300, resid Loss: 0.1055 | 0.0685
Epoch 149/300, resid Loss: 0.1055 | 0.0685
Epoch 150/300, resid Loss: 0.1055 | 0.0684
Epoch 151/300, resid Loss: 0.1054 | 0.0684
Epoch 152/300, resid Loss: 0.1054 | 0.0684
Epoch 153/300, resid Loss: 0.1054 | 0.0684
Epoch 154/300, resid Loss: 0.1054 | 0.0684
Epoch 155/300, resid Loss: 0.1053 | 0.0683
Epoch 156/300, resid Loss: 0.1053 | 0.0683
Epoch 157/300, resid Loss: 0.1053 | 0.0683
Epoch 158/300, resid Loss: 0.1053 | 0.0683
Epoch 159/300, resid Loss: 0.1052 | 0.0683
Epoch 160/300, resid Loss: 0.1052 | 0.0682
Epoch 161/300, resid Loss: 0.1052 | 0.0682
Epoch 162/300, resid Loss: 0.1052 | 0.0682
Epoch 163/300, resid Loss: 0.1052 | 0.0682
Epoch 164/300, resid Loss: 0.1051 | 0.0682
Epoch 165/300, resid Loss: 0.1051 | 0.0682
Epoch 166/300, resid Loss: 0.1051 | 0.0681
Epoch 167/300, resid Loss: 0.1051 | 0.0681
Epoch 168/300, resid Loss: 0.1051 | 0.0681
Epoch 169/300, resid Loss: 0.1051 | 0.0681
Epoch 170/300, resid Loss: 0.1050 | 0.0681
Epoch 171/300, resid Loss: 0.1050 | 0.0681
Epoch 172/300, resid Loss: 0.1050 | 0.0681
Epoch 173/300, resid Loss: 0.1050 | 0.0681
Epoch 174/300, resid Loss: 0.1050 | 0.0680
Epoch 175/300, resid Loss: 0.1050 | 0.0680
Epoch 176/300, resid Loss: 0.1049 | 0.0680
Epoch 177/300, resid Loss: 0.1049 | 0.0680
Epoch 178/300, resid Loss: 0.1049 | 0.0680
Epoch 179/300, resid Loss: 0.1049 | 0.0680
Epoch 180/300, resid Loss: 0.1049 | 0.0680
Epoch 181/300, resid Loss: 0.1049 | 0.0680
Epoch 182/300, resid Loss: 0.1049 | 0.0680
Epoch 183/300, resid Loss: 0.1048 | 0.0679
Epoch 184/300, resid Loss: 0.1048 | 0.0679
Epoch 185/300, resid Loss: 0.1048 | 0.0679
Epoch 186/300, resid Loss: 0.1048 | 0.0679
Epoch 187/300, resid Loss: 0.1048 | 0.0679
Epoch 188/300, resid Loss: 0.1048 | 0.0679
Epoch 189/300, resid Loss: 0.1048 | 0.0679
Epoch 190/300, resid Loss: 0.1048 | 0.0679
Epoch 191/300, resid Loss: 0.1048 | 0.0679
Epoch 192/300, resid Loss: 0.1048 | 0.0679
Epoch 193/300, resid Loss: 0.1047 | 0.0679
Epoch 194/300, resid Loss: 0.1047 | 0.0678
Epoch 195/300, resid Loss: 0.1047 | 0.0678
Epoch 196/300, resid Loss: 0.1047 | 0.0678
Epoch 197/300, resid Loss: 0.1047 | 0.0678
Epoch 198/300, resid Loss: 0.1047 | 0.0678
Epoch 199/300, resid Loss: 0.1047 | 0.0678
Epoch 200/300, resid Loss: 0.1047 | 0.0678
Epoch 201/300, resid Loss: 0.1047 | 0.0678
Epoch 202/300, resid Loss: 0.1047 | 0.0678
Epoch 203/300, resid Loss: 0.1047 | 0.0678
Epoch 204/300, resid Loss: 0.1046 | 0.0678
Epoch 205/300, resid Loss: 0.1046 | 0.0678
Epoch 206/300, resid Loss: 0.1046 | 0.0678
Epoch 207/300, resid Loss: 0.1046 | 0.0678
Epoch 208/300, resid Loss: 0.1046 | 0.0678
Epoch 209/300, resid Loss: 0.1046 | 0.0678
Epoch 210/300, resid Loss: 0.1046 | 0.0677
Epoch 211/300, resid Loss: 0.1046 | 0.0677
Epoch 212/300, resid Loss: 0.1046 | 0.0677
Epoch 213/300, resid Loss: 0.1046 | 0.0677
Epoch 214/300, resid Loss: 0.1046 | 0.0677
Epoch 215/300, resid Loss: 0.1046 | 0.0677
Epoch 216/300, resid Loss: 0.1046 | 0.0677
Epoch 217/300, resid Loss: 0.1046 | 0.0677
Epoch 218/300, resid Loss: 0.1046 | 0.0677
Epoch 219/300, resid Loss: 0.1046 | 0.0677
Epoch 220/300, resid Loss: 0.1045 | 0.0677
Epoch 221/300, resid Loss: 0.1045 | 0.0677
Epoch 222/300, resid Loss: 0.1045 | 0.0677
Epoch 223/300, resid Loss: 0.1045 | 0.0677
Epoch 224/300, resid Loss: 0.1045 | 0.0677
Epoch 225/300, resid Loss: 0.1045 | 0.0677
Epoch 226/300, resid Loss: 0.1045 | 0.0677
Epoch 227/300, resid Loss: 0.1045 | 0.0677
Epoch 228/300, resid Loss: 0.1045 | 0.0677
Epoch 229/300, resid Loss: 0.1045 | 0.0677
Epoch 230/300, resid Loss: 0.1045 | 0.0677
Epoch 231/300, resid Loss: 0.1045 | 0.0677
Epoch 232/300, resid Loss: 0.1045 | 0.0677
Epoch 233/300, resid Loss: 0.1045 | 0.0677
Epoch 234/300, resid Loss: 0.1045 | 0.0677
Epoch 235/300, resid Loss: 0.1045 | 0.0677
Epoch 236/300, resid Loss: 0.1045 | 0.0676
Epoch 237/300, resid Loss: 0.1045 | 0.0676
Epoch 238/300, resid Loss: 0.1045 | 0.0676
Epoch 239/300, resid Loss: 0.1045 | 0.0676
Epoch 240/300, resid Loss: 0.1045 | 0.0676
Epoch 241/300, resid Loss: 0.1045 | 0.0676
Epoch 242/300, resid Loss: 0.1045 | 0.0676
Epoch 243/300, resid Loss: 0.1045 | 0.0676
Epoch 244/300, resid Loss: 0.1045 | 0.0676
Epoch 245/300, resid Loss: 0.1045 | 0.0676
Epoch 246/300, resid Loss: 0.1045 | 0.0676
Epoch 247/300, resid Loss: 0.1044 | 0.0676
Epoch 248/300, resid Loss: 0.1044 | 0.0676
Epoch 249/300, resid Loss: 0.1044 | 0.0676
Epoch 250/300, resid Loss: 0.1044 | 0.0676
Epoch 251/300, resid Loss: 0.1044 | 0.0676
Epoch 252/300, resid Loss: 0.1044 | 0.0676
Epoch 253/300, resid Loss: 0.1044 | 0.0676
Epoch 254/300, resid Loss: 0.1044 | 0.0676
Epoch 255/300, resid Loss: 0.1044 | 0.0676
Epoch 256/300, resid Loss: 0.1044 | 0.0676
Epoch 257/300, resid Loss: 0.1044 | 0.0676
Epoch 258/300, resid Loss: 0.1044 | 0.0676
Epoch 259/300, resid Loss: 0.1044 | 0.0676
Epoch 260/300, resid Loss: 0.1044 | 0.0676
Epoch 261/300, resid Loss: 0.1044 | 0.0676
Epoch 262/300, resid Loss: 0.1044 | 0.0676
Epoch 263/300, resid Loss: 0.1044 | 0.0676
Epoch 264/300, resid Loss: 0.1044 | 0.0676
Epoch 265/300, resid Loss: 0.1044 | 0.0676
Epoch 266/300, resid Loss: 0.1044 | 0.0676
Epoch 267/300, resid Loss: 0.1044 | 0.0676
Epoch 268/300, resid Loss: 0.1044 | 0.0676
Epoch 269/300, resid Loss: 0.1044 | 0.0676
Epoch 270/300, resid Loss: 0.1044 | 0.0676
Epoch 271/300, resid Loss: 0.1044 | 0.0676
Epoch 272/300, resid Loss: 0.1044 | 0.0676
Epoch 273/300, resid Loss: 0.1044 | 0.0676
Epoch 274/300, resid Loss: 0.1044 | 0.0676
Epoch 275/300, resid Loss: 0.1044 | 0.0676
Epoch 276/300, resid Loss: 0.1044 | 0.0676
Epoch 277/300, resid Loss: 0.1044 | 0.0676
Epoch 278/300, resid Loss: 0.1044 | 0.0676
Epoch 279/300, resid Loss: 0.1044 | 0.0676
Epoch 280/300, resid Loss: 0.1044 | 0.0676
Epoch 281/300, resid Loss: 0.1044 | 0.0676
Epoch 282/300, resid Loss: 0.1044 | 0.0676
Epoch 283/300, resid Loss: 0.1044 | 0.0676
Epoch 284/300, resid Loss: 0.1044 | 0.0676
Epoch 285/300, resid Loss: 0.1044 | 0.0676
Epoch 286/300, resid Loss: 0.1044 | 0.0676
Epoch 287/300, resid Loss: 0.1044 | 0.0676
Epoch 288/300, resid Loss: 0.1044 | 0.0676
Epoch 289/300, resid Loss: 0.1044 | 0.0676
Epoch 290/300, resid Loss: 0.1044 | 0.0676
Epoch 291/300, resid Loss: 0.1044 | 0.0676
Epoch 292/300, resid Loss: 0.1044 | 0.0676
Epoch 293/300, resid Loss: 0.1044 | 0.0676
Epoch 294/300, resid Loss: 0.1044 | 0.0676
Epoch 295/300, resid Loss: 0.1044 | 0.0676
Epoch 296/300, resid Loss: 0.1044 | 0.0676
Epoch 297/300, resid Loss: 0.1044 | 0.0676
Epoch 298/300, resid Loss: 0.1044 | 0.0676
Epoch 299/300, resid Loss: 0.1044 | 0.0676
Epoch 300/300, resid Loss: 0.1044 | 0.0676
Runtime (seconds): 995.0544798374176
0.0006346333559577962
[100.2921]
[5.2874236]
[10.367777]
[1.1101863]
[-2.4204051]
[-0.5908707]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 157.1563474732102
RMSE: 12.536201477050781
MAE: 12.536201477050781
R-squared: nan
[114.0462]
