[32m[I 2025-02-09 09:12:08,891][0m A new study created in memory with name: no-name-1b661551-0573-41bf-9700-6475721e8547[0m
[32m[I 2025-02-09 09:12:43,269][0m Trial 0 finished with value: 1.0053200119120234 and parameters: {'observation_period_num': 134, 'train_rates': 0.8103258124392135, 'learning_rate': 1.1453035844503284e-06, 'batch_size': 161, 'step_size': 8, 'gamma': 0.8061333858506541}. Best is trial 0 with value: 1.0053200119120234.[0m
[32m[I 2025-02-09 09:13:42,220][0m Trial 1 finished with value: 0.1585823140819283 and parameters: {'observation_period_num': 19, 'train_rates': 0.7265701830912646, 'learning_rate': 0.00012244535043497582, 'batch_size': 88, 'step_size': 8, 'gamma': 0.974001631036361}. Best is trial 1 with value: 0.1585823140819283.[0m
Early stopping at epoch 71
[32m[I 2025-02-09 09:14:50,602][0m Trial 2 finished with value: 0.7652462720870972 and parameters: {'observation_period_num': 189, 'train_rates': 0.9889709091176921, 'learning_rate': 2.7676392876214434e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.8743204958373031}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:15:47,562][0m Trial 3 finished with value: 0.23815907699453248 and parameters: {'observation_period_num': 203, 'train_rates': 0.795531722820732, 'learning_rate': 5.077862447486742e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8482981140721241}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:16:40,414][0m Trial 4 finished with value: 0.8038327970002827 and parameters: {'observation_period_num': 217, 'train_rates': 0.829511556893856, 'learning_rate': 1.6954050428632888e-06, 'batch_size': 98, 'step_size': 10, 'gamma': 0.8445686644330802}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:17:28,668][0m Trial 5 finished with value: 0.8518271806159458 and parameters: {'observation_period_num': 179, 'train_rates': 0.7798879536405686, 'learning_rate': 1.0289204455030634e-06, 'batch_size': 105, 'step_size': 15, 'gamma': 0.9012972657628513}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:17:54,111][0m Trial 6 finished with value: 0.2506126273518954 and parameters: {'observation_period_num': 82, 'train_rates': 0.8665725723481463, 'learning_rate': 3.397785659526429e-05, 'batch_size': 233, 'step_size': 6, 'gamma': 0.8112125626873843}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:18:22,937][0m Trial 7 finished with value: 0.3259390051888977 and parameters: {'observation_period_num': 207, 'train_rates': 0.8843597143596843, 'learning_rate': 1.6258614831212393e-05, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9777261694989727}. Best is trial 1 with value: 0.1585823140819283.[0m
[32m[I 2025-02-09 09:18:49,428][0m Trial 8 finished with value: 0.07123305449801849 and parameters: {'observation_period_num': 53, 'train_rates': 0.7929899911741624, 'learning_rate': 0.00015592374973954863, 'batch_size': 218, 'step_size': 14, 'gamma': 0.8283729965654022}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:19:23,268][0m Trial 9 finished with value: 0.19214727350173552 and parameters: {'observation_period_num': 72, 'train_rates': 0.721826701410089, 'learning_rate': 0.000374249541670302, 'batch_size': 152, 'step_size': 10, 'gamma': 0.884912810268711}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:19:49,392][0m Trial 10 finished with value: 0.15128160599441756 and parameters: {'observation_period_num': 7, 'train_rates': 0.6493956218001876, 'learning_rate': 0.0008682940433947134, 'batch_size': 202, 'step_size': 3, 'gamma': 0.7544592807140298}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:20:15,548][0m Trial 11 finished with value: 0.1496026920777708 and parameters: {'observation_period_num': 6, 'train_rates': 0.6116992530536629, 'learning_rate': 0.0009658770298883572, 'batch_size': 200, 'step_size': 2, 'gamma': 0.7621937105229295}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:20:41,176][0m Trial 12 finished with value: 0.2549987026425295 and parameters: {'observation_period_num': 53, 'train_rates': 0.6206927398080759, 'learning_rate': 0.0002507839535629924, 'batch_size': 187, 'step_size': 4, 'gamma': 0.7525200131236754}. Best is trial 8 with value: 0.07123305449801849.[0m
Early stopping at epoch 59
[32m[I 2025-02-09 09:20:53,654][0m Trial 13 finished with value: 0.3668462627475172 and parameters: {'observation_period_num': 112, 'train_rates': 0.6956958360818233, 'learning_rate': 0.0009008918661124264, 'batch_size': 255, 'step_size': 1, 'gamma': 0.7960730289705662}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:21:28,796][0m Trial 14 finished with value: 0.07483223080635071 and parameters: {'observation_period_num': 35, 'train_rates': 0.9677688689898692, 'learning_rate': 0.00012728274498780253, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9193736456989419}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:22:04,319][0m Trial 15 finished with value: 0.08850248903036118 and parameters: {'observation_period_num': 44, 'train_rates': 0.9435708336307305, 'learning_rate': 7.656386827465937e-05, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9324680173367247}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:25:44,723][0m Trial 16 finished with value: 0.1538394473993731 and parameters: {'observation_period_num': 127, 'train_rates': 0.9120760472978519, 'learning_rate': 1.1311737198596344e-05, 'batch_size': 25, 'step_size': 6, 'gamma': 0.9238568896619275}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:26:28,002][0m Trial 17 finished with value: 0.2307833846407265 and parameters: {'observation_period_num': 95, 'train_rates': 0.76031506581868, 'learning_rate': 0.00015403751936010813, 'batch_size': 129, 'step_size': 10, 'gamma': 0.8410550900366601}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:26:57,549][0m Trial 18 finished with value: 0.39696040749549866 and parameters: {'observation_period_num': 42, 'train_rates': 0.9841728162004244, 'learning_rate': 1.1079737113982208e-05, 'batch_size': 224, 'step_size': 15, 'gamma': 0.9431856218880216}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:27:21,695][0m Trial 19 finished with value: 0.21448715329170226 and parameters: {'observation_period_num': 156, 'train_rates': 0.8507765909686991, 'learning_rate': 0.00033960701985800073, 'batch_size': 255, 'step_size': 8, 'gamma': 0.9087729452159587}. Best is trial 8 with value: 0.07123305449801849.[0m
[32m[I 2025-02-09 09:28:07,925][0m Trial 20 finished with value: 0.06100720087898538 and parameters: {'observation_period_num': 31, 'train_rates': 0.9211460759854887, 'learning_rate': 8.945261957976984e-05, 'batch_size': 132, 'step_size': 12, 'gamma': 0.9523335734009373}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:28:50,259][0m Trial 21 finished with value: 0.3115788585153119 and parameters: {'observation_period_num': 247, 'train_rates': 0.9370740434288467, 'learning_rate': 9.233419754058292e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.9594987397038759}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:29:40,305][0m Trial 22 finished with value: 0.0966822716055919 and parameters: {'observation_period_num': 34, 'train_rates': 0.8984616151150161, 'learning_rate': 0.00020475565249698083, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9501118801934828}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:30:19,924][0m Trial 23 finished with value: 0.13384582102298737 and parameters: {'observation_period_num': 68, 'train_rates': 0.9503093085103359, 'learning_rate': 4.724784973930746e-05, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9101264038815041}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:30:51,931][0m Trial 24 finished with value: 0.12153527572136898 and parameters: {'observation_period_num': 28, 'train_rates': 0.8450075523042353, 'learning_rate': 1.8157752326912962e-05, 'batch_size': 181, 'step_size': 14, 'gamma': 0.8895114704100701}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:31:23,244][0m Trial 25 finished with value: 0.10801761011282603 and parameters: {'observation_period_num': 60, 'train_rates': 0.9234812650950264, 'learning_rate': 0.00046331072293115096, 'batch_size': 201, 'step_size': 11, 'gamma': 0.8664877941042656}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:31:52,827][0m Trial 26 finished with value: 0.21297426521778107 and parameters: {'observation_period_num': 88, 'train_rates': 0.9598178645686529, 'learning_rate': 7.036878609823861e-05, 'batch_size': 232, 'step_size': 6, 'gamma': 0.9288061928757138}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:32:39,756][0m Trial 27 finished with value: 0.2926762108697661 and parameters: {'observation_period_num': 104, 'train_rates': 0.7572829812622434, 'learning_rate': 0.00014302087496262663, 'batch_size': 118, 'step_size': 14, 'gamma': 0.9870292661963356}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:33:22,399][0m Trial 28 finished with value: 0.2199489519355784 and parameters: {'observation_period_num': 24, 'train_rates': 0.8734678232454938, 'learning_rate': 4.962558690806937e-06, 'batch_size': 144, 'step_size': 7, 'gamma': 0.960306481861811}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:33:55,043][0m Trial 29 finished with value: 0.30149769433028334 and parameters: {'observation_period_num': 149, 'train_rates': 0.8030741083314048, 'learning_rate': 2.891048002002621e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8194144120119237}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:35:49,094][0m Trial 30 finished with value: 0.09767952436512267 and parameters: {'observation_period_num': 54, 'train_rates': 0.8169257081469292, 'learning_rate': 0.0005046297373594331, 'batch_size': 47, 'step_size': 4, 'gamma': 0.7914481378334491}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:36:26,267][0m Trial 31 finished with value: 0.10447157919406891 and parameters: {'observation_period_num': 46, 'train_rates': 0.9616153681965862, 'learning_rate': 8.760692717103752e-05, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9363171191499076}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:36:57,470][0m Trial 32 finished with value: 0.10168127705994594 and parameters: {'observation_period_num': 19, 'train_rates': 0.922053388270605, 'learning_rate': 5.611035681638251e-05, 'batch_size': 210, 'step_size': 4, 'gamma': 0.9224676023496362}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:37:37,066][0m Trial 33 finished with value: 0.06540956348180771 and parameters: {'observation_period_num': 35, 'train_rates': 0.9845484892029326, 'learning_rate': 0.0001230994650840935, 'batch_size': 170, 'step_size': 7, 'gamma': 0.9633761643667478}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:38:18,204][0m Trial 34 finished with value: 0.10992903262376785 and parameters: {'observation_period_num': 74, 'train_rates': 0.9845050602061637, 'learning_rate': 0.00020755047385364974, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9704307926064173}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:38:53,645][0m Trial 35 finished with value: 0.07569567859172821 and parameters: {'observation_period_num': 33, 'train_rates': 0.9725326785960773, 'learning_rate': 0.00011544646850372179, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8621811543717097}. Best is trial 20 with value: 0.06100720087898538.[0m
[32m[I 2025-02-09 09:40:08,781][0m Trial 36 finished with value: 0.04562329896555765 and parameters: {'observation_period_num': 5, 'train_rates': 0.9085648017984886, 'learning_rate': 3.644057278362797e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9872259648221827}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:41:36,885][0m Trial 37 finished with value: 0.052208872279859965 and parameters: {'observation_period_num': 17, 'train_rates': 0.8898465410713184, 'learning_rate': 3.080344214180342e-05, 'batch_size': 69, 'step_size': 7, 'gamma': 0.980176839859582}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:42:57,205][0m Trial 38 finished with value: 0.04899732196141803 and parameters: {'observation_period_num': 11, 'train_rates': 0.8947164239923218, 'learning_rate': 2.957021163722635e-05, 'batch_size': 73, 'step_size': 8, 'gamma': 0.9873167096191874}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:44:23,096][0m Trial 39 finished with value: 0.055614192614608 and parameters: {'observation_period_num': 11, 'train_rates': 0.8942722228375615, 'learning_rate': 2.565544367612003e-05, 'batch_size': 67, 'step_size': 9, 'gamma': 0.9866720129746}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:45:40,545][0m Trial 40 finished with value: 0.047068419097594814 and parameters: {'observation_period_num': 5, 'train_rates': 0.881703078852438, 'learning_rate': 2.848563558400735e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9854619692903108}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:47:04,850][0m Trial 41 finished with value: 0.046202049660496416 and parameters: {'observation_period_num': 5, 'train_rates': 0.8933060209083639, 'learning_rate': 2.8032988914348734e-05, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9891959957152916}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:48:18,750][0m Trial 42 finished with value: 0.056324386274058856 and parameters: {'observation_period_num': 16, 'train_rates': 0.860241083973581, 'learning_rate': 3.6932124163976855e-05, 'batch_size': 79, 'step_size': 8, 'gamma': 0.9795070293514626}. Best is trial 36 with value: 0.04562329896555765.[0m
[32m[I 2025-02-09 09:50:23,477][0m Trial 43 finished with value: 0.04179523323156585 and parameters: {'observation_period_num': 7, 'train_rates': 0.8304054050593686, 'learning_rate': 2.030609823343322e-05, 'batch_size': 45, 'step_size': 9, 'gamma': 0.9899238549011592}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 09:52:25,053][0m Trial 44 finished with value: 0.06123563090785722 and parameters: {'observation_period_num': 5, 'train_rates': 0.8387193626279187, 'learning_rate': 6.405958015193218e-06, 'batch_size': 45, 'step_size': 9, 'gamma': 0.9709305849968121}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 09:53:26,988][0m Trial 45 finished with value: 0.056898392502303795 and parameters: {'observation_period_num': 5, 'train_rates': 0.8252263279891507, 'learning_rate': 1.874980435252781e-05, 'batch_size': 95, 'step_size': 8, 'gamma': 0.987465891493377}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 09:55:22,162][0m Trial 46 finished with value: 0.06848504402413355 and parameters: {'observation_period_num': 20, 'train_rates': 0.8770141318843254, 'learning_rate': 1.2203006180794988e-05, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9684411337479416}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 09:58:38,686][0m Trial 47 finished with value: 0.06590019012626527 and parameters: {'observation_period_num': 22, 'train_rates': 0.9018343491758686, 'learning_rate': 7.535496082863368e-06, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9495592916704253}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 09:59:49,019][0m Trial 48 finished with value: 0.04932045100423672 and parameters: {'observation_period_num': 5, 'train_rates': 0.8622145621178581, 'learning_rate': 2.173427846386873e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9887258951504587}. Best is trial 43 with value: 0.04179523323156585.[0m
[32m[I 2025-02-09 10:02:14,921][0m Trial 49 finished with value: 0.20912410918223065 and parameters: {'observation_period_num': 42, 'train_rates': 0.7767766937975427, 'learning_rate': 4.76166938146e-05, 'batch_size': 35, 'step_size': 8, 'gamma': 0.9797795195364944}. Best is trial 43 with value: 0.04179523323156585.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.4304 | 0.5273
Epoch 2/300, Loss: 0.2451 | 0.3037
Epoch 3/300, Loss: 0.1832 | 0.2316
Epoch 4/300, Loss: 0.1605 | 0.2052
Epoch 5/300, Loss: 0.1492 | 0.1897
Epoch 6/300, Loss: 0.1425 | 0.1755
Epoch 7/300, Loss: 0.1376 | 0.1623
Epoch 8/300, Loss: 0.1333 | 0.1496
Epoch 9/300, Loss: 0.1295 | 0.1375
Epoch 10/300, Loss: 0.1262 | 0.1262
Epoch 11/300, Loss: 0.1235 | 0.1162
Epoch 12/300, Loss: 0.1213 | 0.1078
Epoch 13/300, Loss: 0.1195 | 0.1010
Epoch 14/300, Loss: 0.1181 | 0.0955
Epoch 15/300, Loss: 0.1168 | 0.0912
Epoch 16/300, Loss: 0.1155 | 0.0876
Epoch 17/300, Loss: 0.1142 | 0.0847
Epoch 18/300, Loss: 0.1129 | 0.0821
Epoch 19/300, Loss: 0.1118 | 0.0800
Epoch 20/300, Loss: 0.1109 | 0.0781
Epoch 21/300, Loss: 0.1100 | 0.0764
Epoch 22/300, Loss: 0.1093 | 0.0749
Epoch 23/300, Loss: 0.1085 | 0.0737
Epoch 24/300, Loss: 0.1078 | 0.0726
Epoch 25/300, Loss: 0.1071 | 0.0717
Epoch 26/300, Loss: 0.1064 | 0.0709
Epoch 27/300, Loss: 0.1058 | 0.0703
Epoch 28/300, Loss: 0.1053 | 0.0697
Epoch 29/300, Loss: 0.1048 | 0.0693
Epoch 30/300, Loss: 0.1044 | 0.0688
Epoch 31/300, Loss: 0.1041 | 0.0684
Epoch 32/300, Loss: 0.1037 | 0.0679
Epoch 33/300, Loss: 0.1033 | 0.0674
Epoch 34/300, Loss: 0.1028 | 0.0671
Epoch 35/300, Loss: 0.1023 | 0.0669
Epoch 36/300, Loss: 0.1017 | 0.0667
Epoch 37/300, Loss: 0.1010 | 0.0664
Epoch 38/300, Loss: 0.1005 | 0.0660
Epoch 39/300, Loss: 0.1000 | 0.0654
Epoch 40/300, Loss: 0.0998 | 0.0647
Epoch 41/300, Loss: 0.0997 | 0.0641
Epoch 42/300, Loss: 0.0996 | 0.0636
Epoch 43/300, Loss: 0.0996 | 0.0634
Epoch 44/300, Loss: 0.0994 | 0.0635
Epoch 45/300, Loss: 0.0990 | 0.0638
Epoch 46/300, Loss: 0.0984 | 0.0643
Epoch 47/300, Loss: 0.0975 | 0.0645
Epoch 48/300, Loss: 0.0964 | 0.0642
Epoch 49/300, Loss: 0.0954 | 0.0636
Epoch 50/300, Loss: 0.0946 | 0.0627
Epoch 51/300, Loss: 0.0941 | 0.0618
Epoch 52/300, Loss: 0.0937 | 0.0607
Epoch 53/300, Loss: 0.0935 | 0.0596
Epoch 54/300, Loss: 0.0934 | 0.0585
Epoch 55/300, Loss: 0.0932 | 0.0575
Epoch 56/300, Loss: 0.0930 | 0.0566
Epoch 57/300, Loss: 0.0927 | 0.0558
Epoch 58/300, Loss: 0.0923 | 0.0552
Epoch 59/300, Loss: 0.0918 | 0.0546
Epoch 60/300, Loss: 0.0913 | 0.0542
Epoch 61/300, Loss: 0.0907 | 0.0539
Epoch 62/300, Loss: 0.0901 | 0.0536
Epoch 63/300, Loss: 0.0895 | 0.0534
Epoch 64/300, Loss: 0.0889 | 0.0532
Epoch 65/300, Loss: 0.0883 | 0.0530
Epoch 66/300, Loss: 0.0877 | 0.0527
Epoch 67/300, Loss: 0.0873 | 0.0525
Epoch 68/300, Loss: 0.0868 | 0.0521
Epoch 69/300, Loss: 0.0865 | 0.0517
Epoch 70/300, Loss: 0.0862 | 0.0513
Epoch 71/300, Loss: 0.0860 | 0.0508
Epoch 72/300, Loss: 0.0858 | 0.0503
Epoch 73/300, Loss: 0.0857 | 0.0499
Epoch 74/300, Loss: 0.0856 | 0.0495
Epoch 75/300, Loss: 0.0855 | 0.0493
Epoch 76/300, Loss: 0.0852 | 0.0493
Epoch 77/300, Loss: 0.0849 | 0.0493
Epoch 78/300, Loss: 0.0844 | 0.0494
Epoch 79/300, Loss: 0.0838 | 0.0490
Epoch 80/300, Loss: 0.0833 | 0.0483
Epoch 81/300, Loss: 0.0829 | 0.0476
Epoch 82/300, Loss: 0.0827 | 0.0468
Epoch 83/300, Loss: 0.0827 | 0.0460
Epoch 84/300, Loss: 0.0829 | 0.0454
Epoch 85/300, Loss: 0.0830 | 0.0448
Epoch 86/300, Loss: 0.0831 | 0.0444
Epoch 87/300, Loss: 0.0830 | 0.0440
Epoch 88/300, Loss: 0.0829 | 0.0438
Epoch 89/300, Loss: 0.0826 | 0.0436
Epoch 90/300, Loss: 0.0822 | 0.0435
Epoch 91/300, Loss: 0.0817 | 0.0434
Epoch 92/300, Loss: 0.0812 | 0.0434
Epoch 93/300, Loss: 0.0807 | 0.0434
Epoch 94/300, Loss: 0.0802 | 0.0434
Epoch 95/300, Loss: 0.0798 | 0.0434
Epoch 96/300, Loss: 0.0794 | 0.0434
Epoch 97/300, Loss: 0.0792 | 0.0435
Epoch 98/300, Loss: 0.0791 | 0.0436
Epoch 99/300, Loss: 0.0790 | 0.0437
Epoch 100/300, Loss: 0.0791 | 0.0438
Epoch 101/300, Loss: 0.0791 | 0.0439
Epoch 102/300, Loss: 0.0791 | 0.0440
Epoch 103/300, Loss: 0.0792 | 0.0441
Epoch 104/300, Loss: 0.0791 | 0.0442
Epoch 105/300, Loss: 0.0790 | 0.0442
Epoch 106/300, Loss: 0.0787 | 0.0440
Epoch 107/300, Loss: 0.0784 | 0.0435
Epoch 108/300, Loss: 0.0779 | 0.0429
Epoch 109/300, Loss: 0.0776 | 0.0423
Epoch 110/300, Loss: 0.0773 | 0.0418
Epoch 111/300, Loss: 0.0772 | 0.0414
Epoch 112/300, Loss: 0.0772 | 0.0410
Epoch 113/300, Loss: 0.0772 | 0.0408
Epoch 114/300, Loss: 0.0773 | 0.0405
Epoch 115/300, Loss: 0.0774 | 0.0404
Epoch 116/300, Loss: 0.0774 | 0.0402
Epoch 117/300, Loss: 0.0775 | 0.0402
Epoch 118/300, Loss: 0.0775 | 0.0401
Epoch 119/300, Loss: 0.0775 | 0.0400
Epoch 120/300, Loss: 0.0774 | 0.0400
Epoch 121/300, Loss: 0.0773 | 0.0401
Epoch 122/300, Loss: 0.0772 | 0.0401
Epoch 123/300, Loss: 0.0770 | 0.0403
Epoch 124/300, Loss: 0.0768 | 0.0405
Epoch 125/300, Loss: 0.0766 | 0.0408
Epoch 126/300, Loss: 0.0763 | 0.0412
Epoch 127/300, Loss: 0.0761 | 0.0416
Epoch 128/300, Loss: 0.0759 | 0.0421
Epoch 129/300, Loss: 0.0757 | 0.0425
Epoch 130/300, Loss: 0.0756 | 0.0428
Epoch 131/300, Loss: 0.0755 | 0.0430
Epoch 132/300, Loss: 0.0756 | 0.0431
Epoch 133/300, Loss: 0.0756 | 0.0429
Epoch 134/300, Loss: 0.0757 | 0.0427
Epoch 135/300, Loss: 0.0758 | 0.0424
Epoch 136/300, Loss: 0.0758 | 0.0421
Epoch 137/300, Loss: 0.0756 | 0.0417
Epoch 138/300, Loss: 0.0753 | 0.0411
Epoch 139/300, Loss: 0.0749 | 0.0404
Epoch 140/300, Loss: 0.0744 | 0.0397
Epoch 141/300, Loss: 0.0740 | 0.0392
Epoch 142/300, Loss: 0.0737 | 0.0387
Epoch 143/300, Loss: 0.0736 | 0.0383
Epoch 144/300, Loss: 0.0736 | 0.0380
Epoch 145/300, Loss: 0.0737 | 0.0378
Epoch 146/300, Loss: 0.0739 | 0.0376
Epoch 147/300, Loss: 0.0740 | 0.0374
Epoch 148/300, Loss: 0.0740 | 0.0372
Epoch 149/300, Loss: 0.0740 | 0.0371
Epoch 150/300, Loss: 0.0739 | 0.0370
Epoch 151/300, Loss: 0.0738 | 0.0369
Epoch 152/300, Loss: 0.0735 | 0.0368
Epoch 153/300, Loss: 0.0733 | 0.0368
Epoch 154/300, Loss: 0.0729 | 0.0368
Epoch 155/300, Loss: 0.0726 | 0.0368
Epoch 156/300, Loss: 0.0722 | 0.0367
Epoch 157/300, Loss: 0.0719 | 0.0368
Epoch 158/300, Loss: 0.0716 | 0.0368
Epoch 159/300, Loss: 0.0714 | 0.0368
Epoch 160/300, Loss: 0.0712 | 0.0369
Epoch 161/300, Loss: 0.0711 | 0.0370
Epoch 162/300, Loss: 0.0711 | 0.0371
Epoch 163/300, Loss: 0.0711 | 0.0372
Epoch 164/300, Loss: 0.0712 | 0.0373
Epoch 165/300, Loss: 0.0713 | 0.0374
Epoch 166/300, Loss: 0.0715 | 0.0375
Epoch 167/300, Loss: 0.0716 | 0.0376
Epoch 168/300, Loss: 0.0717 | 0.0376
Epoch 169/300, Loss: 0.0717 | 0.0375
Epoch 170/300, Loss: 0.0715 | 0.0373
Epoch 171/300, Loss: 0.0713 | 0.0370
Epoch 172/300, Loss: 0.0710 | 0.0366
Epoch 173/300, Loss: 0.0708 | 0.0362
Epoch 174/300, Loss: 0.0706 | 0.0358
Epoch 175/300, Loss: 0.0705 | 0.0356
Epoch 176/300, Loss: 0.0705 | 0.0354
Epoch 177/300, Loss: 0.0705 | 0.0353
Epoch 178/300, Loss: 0.0705 | 0.0352
Epoch 179/300, Loss: 0.0705 | 0.0351
Epoch 180/300, Loss: 0.0705 | 0.0351
Epoch 181/300, Loss: 0.0705 | 0.0350
Epoch 182/300, Loss: 0.0705 | 0.0351
Epoch 183/300, Loss: 0.0704 | 0.0351
Epoch 184/300, Loss: 0.0703 | 0.0351
Epoch 185/300, Loss: 0.0702 | 0.0352
Epoch 186/300, Loss: 0.0700 | 0.0353
Epoch 187/300, Loss: 0.0699 | 0.0354
Epoch 188/300, Loss: 0.0697 | 0.0355
Epoch 189/300, Loss: 0.0695 | 0.0355
Epoch 190/300, Loss: 0.0693 | 0.0356
Epoch 191/300, Loss: 0.0692 | 0.0356
Epoch 192/300, Loss: 0.0690 | 0.0357
Epoch 193/300, Loss: 0.0690 | 0.0357
Epoch 194/300, Loss: 0.0689 | 0.0357
Epoch 195/300, Loss: 0.0689 | 0.0356
Epoch 196/300, Loss: 0.0689 | 0.0356
Epoch 197/300, Loss: 0.0689 | 0.0356
Epoch 198/300, Loss: 0.0689 | 0.0355
Epoch 199/300, Loss: 0.0689 | 0.0354
Epoch 200/300, Loss: 0.0689 | 0.0353
Epoch 201/300, Loss: 0.0687 | 0.0352
Epoch 202/300, Loss: 0.0686 | 0.0350
Epoch 203/300, Loss: 0.0684 | 0.0349
Epoch 204/300, Loss: 0.0682 | 0.0347
Epoch 205/300, Loss: 0.0681 | 0.0345
Epoch 206/300, Loss: 0.0680 | 0.0344
Epoch 207/300, Loss: 0.0680 | 0.0342
Epoch 208/300, Loss: 0.0680 | 0.0341
Epoch 209/300, Loss: 0.0680 | 0.0340
Epoch 210/300, Loss: 0.0680 | 0.0340
Epoch 211/300, Loss: 0.0680 | 0.0339
Epoch 212/300, Loss: 0.0680 | 0.0339
Epoch 213/300, Loss: 0.0679 | 0.0339
Epoch 214/300, Loss: 0.0679 | 0.0339
Epoch 215/300, Loss: 0.0678 | 0.0339
Epoch 216/300, Loss: 0.0678 | 0.0340
Epoch 217/300, Loss: 0.0677 | 0.0341
Epoch 218/300, Loss: 0.0676 | 0.0342
Epoch 219/300, Loss: 0.0675 | 0.0343
Epoch 220/300, Loss: 0.0673 | 0.0344
Epoch 221/300, Loss: 0.0672 | 0.0346
Epoch 222/300, Loss: 0.0671 | 0.0347
Epoch 223/300, Loss: 0.0670 | 0.0348
Epoch 224/300, Loss: 0.0669 | 0.0349
Epoch 225/300, Loss: 0.0669 | 0.0350
Epoch 226/300, Loss: 0.0669 | 0.0351
Epoch 227/300, Loss: 0.0670 | 0.0352
Epoch 228/300, Loss: 0.0670 | 0.0353
Epoch 229/300, Loss: 0.0671 | 0.0353
Epoch 230/300, Loss: 0.0672 | 0.0353
Epoch 231/300, Loss: 0.0673 | 0.0353
Epoch 232/300, Loss: 0.0673 | 0.0353
Epoch 233/300, Loss: 0.0671 | 0.0351
Epoch 234/300, Loss: 0.0669 | 0.0350
Epoch 235/300, Loss: 0.0666 | 0.0347
Epoch 236/300, Loss: 0.0663 | 0.0344
Epoch 237/300, Loss: 0.0661 | 0.0341
Epoch 238/300, Loss: 0.0660 | 0.0339
Epoch 239/300, Loss: 0.0659 | 0.0338
Epoch 240/300, Loss: 0.0659 | 0.0337
Epoch 241/300, Loss: 0.0659 | 0.0336
Epoch 242/300, Loss: 0.0659 | 0.0335
Epoch 243/300, Loss: 0.0659 | 0.0335
Epoch 244/300, Loss: 0.0659 | 0.0334
Epoch 245/300, Loss: 0.0659 | 0.0334
Epoch 246/300, Loss: 0.0658 | 0.0334
Epoch 247/300, Loss: 0.0658 | 0.0334
Epoch 248/300, Loss: 0.0657 | 0.0334
Epoch 249/300, Loss: 0.0656 | 0.0334
Epoch 250/300, Loss: 0.0655 | 0.0333
Epoch 251/300, Loss: 0.0654 | 0.0333
Epoch 252/300, Loss: 0.0653 | 0.0333
Epoch 253/300, Loss: 0.0652 | 0.0332
Epoch 254/300, Loss: 0.0651 | 0.0331
Epoch 255/300, Loss: 0.0651 | 0.0330
Epoch 256/300, Loss: 0.0650 | 0.0330
Epoch 257/300, Loss: 0.0650 | 0.0330
Epoch 258/300, Loss: 0.0650 | 0.0330
Epoch 259/300, Loss: 0.0650 | 0.0330
Epoch 260/300, Loss: 0.0650 | 0.0330
Epoch 261/300, Loss: 0.0649 | 0.0331
Epoch 262/300, Loss: 0.0649 | 0.0331
Epoch 263/300, Loss: 0.0648 | 0.0331
Epoch 264/300, Loss: 0.0647 | 0.0331
Epoch 265/300, Loss: 0.0645 | 0.0331
Epoch 266/300, Loss: 0.0644 | 0.0330
Epoch 267/300, Loss: 0.0642 | 0.0329
Epoch 268/300, Loss: 0.0641 | 0.0328
Epoch 269/300, Loss: 0.0640 | 0.0326
Epoch 270/300, Loss: 0.0639 | 0.0325
Epoch 271/300, Loss: 0.0639 | 0.0324
Epoch 272/300, Loss: 0.0639 | 0.0323
Epoch 273/300, Loss: 0.0640 | 0.0321
Epoch 274/300, Loss: 0.0640 | 0.0320
Epoch 275/300, Loss: 0.0641 | 0.0319
Epoch 276/300, Loss: 0.0641 | 0.0318
Epoch 277/300, Loss: 0.0642 | 0.0317
Epoch 278/300, Loss: 0.0642 | 0.0317
Epoch 279/300, Loss: 0.0643 | 0.0316
Epoch 280/300, Loss: 0.0642 | 0.0317
Epoch 281/300, Loss: 0.0642 | 0.0317
Epoch 282/300, Loss: 0.0641 | 0.0317
Epoch 283/300, Loss: 0.0639 | 0.0317
Epoch 284/300, Loss: 0.0637 | 0.0317
Epoch 285/300, Loss: 0.0635 | 0.0317
Epoch 286/300, Loss: 0.0634 | 0.0317
Epoch 287/300, Loss: 0.0633 | 0.0317
Epoch 288/300, Loss: 0.0632 | 0.0317
Epoch 289/300, Loss: 0.0632 | 0.0317
Epoch 290/300, Loss: 0.0633 | 0.0318
Epoch 291/300, Loss: 0.0633 | 0.0318
Epoch 292/300, Loss: 0.0634 | 0.0318
Epoch 293/300, Loss: 0.0633 | 0.0318
Epoch 294/300, Loss: 0.0633 | 0.0318
Epoch 295/300, Loss: 0.0632 | 0.0317
Epoch 296/300, Loss: 0.0630 | 0.0316
Epoch 297/300, Loss: 0.0629 | 0.0315
Epoch 298/300, Loss: 0.0627 | 0.0314
Epoch 299/300, Loss: 0.0626 | 0.0312
Epoch 300/300, Loss: 0.0625 | 0.0311
Runtime (seconds): 365.7780394554138
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 306.07752368063666
RMSE: 17.495071411132812
MAE: 17.495071411132812
R-squared: nan
[204.63507]
