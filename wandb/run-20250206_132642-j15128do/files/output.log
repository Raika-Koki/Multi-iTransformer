[32m[I 2025-02-06 13:26:50,199][0m A new study created in memory with name: no-name-162ac623-b9d5-4205-8152-a5a1738f3312[0m
[32m[I 2025-02-06 13:27:26,386][0m Trial 0 finished with value: 0.20017003305484912 and parameters: {'observation_period_num': 92, 'train_rates': 0.7188474034122186, 'learning_rate': 0.0006792569235246432, 'batch_size': 144, 'step_size': 2, 'gamma': 0.9647493454037466}. Best is trial 0 with value: 0.20017003305484912.[0m
[32m[I 2025-02-06 13:29:45,190][0m Trial 1 finished with value: 0.3186170432631296 and parameters: {'observation_period_num': 173, 'train_rates': 0.8140380265502327, 'learning_rate': 4.980574709820188e-06, 'batch_size': 36, 'step_size': 11, 'gamma': 0.8374761414623951}. Best is trial 0 with value: 0.20017003305484912.[0m
[32m[I 2025-02-06 13:30:27,816][0m Trial 2 finished with value: 0.21486856614058453 and parameters: {'observation_period_num': 62, 'train_rates': 0.7065975196435612, 'learning_rate': 4.725742436772656e-05, 'batch_size': 120, 'step_size': 11, 'gamma': 0.7520904112776531}. Best is trial 0 with value: 0.20017003305484912.[0m
[32m[I 2025-02-06 13:31:00,509][0m Trial 3 finished with value: 0.19534928912805363 and parameters: {'observation_period_num': 60, 'train_rates': 0.7587353072150591, 'learning_rate': 0.0007450472388272356, 'batch_size': 159, 'step_size': 14, 'gamma': 0.7717071111422144}. Best is trial 3 with value: 0.19534928912805363.[0m
[32m[I 2025-02-06 13:31:49,000][0m Trial 4 finished with value: 0.3407437736303478 and parameters: {'observation_period_num': 111, 'train_rates': 0.616503373082354, 'learning_rate': 1.6815841348181144e-05, 'batch_size': 91, 'step_size': 14, 'gamma': 0.7768647111746863}. Best is trial 3 with value: 0.19534928912805363.[0m
[32m[I 2025-02-06 13:32:30,430][0m Trial 5 finished with value: 0.46936050925073747 and parameters: {'observation_period_num': 110, 'train_rates': 0.7807431674271111, 'learning_rate': 1.6363571208805674e-05, 'batch_size': 130, 'step_size': 4, 'gamma': 0.7922482578138946}. Best is trial 3 with value: 0.19534928912805363.[0m
Early stopping at epoch 93
[32m[I 2025-02-06 13:33:06,298][0m Trial 6 finished with value: 0.5224444557700241 and parameters: {'observation_period_num': 173, 'train_rates': 0.8786367590088312, 'learning_rate': 3.9370876733072654e-05, 'batch_size': 150, 'step_size': 1, 'gamma': 0.8804057053222958}. Best is trial 3 with value: 0.19534928912805363.[0m
[32m[I 2025-02-06 13:33:33,654][0m Trial 7 finished with value: 0.1968962699174881 and parameters: {'observation_period_num': 104, 'train_rates': 0.9367042928635017, 'learning_rate': 7.837419652357436e-05, 'batch_size': 239, 'step_size': 13, 'gamma': 0.8023206609021187}. Best is trial 3 with value: 0.19534928912805363.[0m
[32m[I 2025-02-06 13:34:56,937][0m Trial 8 finished with value: 0.18815243105219806 and parameters: {'observation_period_num': 32, 'train_rates': 0.7473321826426081, 'learning_rate': 5.031386628780675e-05, 'batch_size': 61, 'step_size': 5, 'gamma': 0.836561599160301}. Best is trial 8 with value: 0.18815243105219806.[0m
[32m[I 2025-02-06 13:35:33,781][0m Trial 9 finished with value: 0.14534206625286825 and parameters: {'observation_period_num': 19, 'train_rates': 0.6345890851164684, 'learning_rate': 0.00026826744331535294, 'batch_size': 130, 'step_size': 4, 'gamma': 0.9604820437333712}. Best is trial 9 with value: 0.14534206625286825.[0m
[32m[I 2025-02-06 13:35:57,353][0m Trial 10 finished with value: 0.38772283813184016 and parameters: {'observation_period_num': 236, 'train_rates': 0.6129820281325005, 'learning_rate': 0.00022051223745125106, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9889153153888137}. Best is trial 9 with value: 0.14534206625286825.[0m
[32m[I 2025-02-06 13:37:59,917][0m Trial 11 finished with value: 0.12642177270930202 and parameters: {'observation_period_num': 10, 'train_rates': 0.6734956543253694, 'learning_rate': 0.00015751541545211894, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9084827905578207}. Best is trial 11 with value: 0.12642177270930202.[0m
[32m[I 2025-02-06 13:41:28,820][0m Trial 12 finished with value: 0.13124049082398415 and parameters: {'observation_period_num': 8, 'train_rates': 0.666207416217861, 'learning_rate': 0.0001650948236507091, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9244070279706623}. Best is trial 11 with value: 0.12642177270930202.[0m
[32m[I 2025-02-06 13:45:32,560][0m Trial 13 finished with value: 0.12479962160601756 and parameters: {'observation_period_num': 6, 'train_rates': 0.6716799884936332, 'learning_rate': 0.00017140419561809353, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9081136064990887}. Best is trial 13 with value: 0.12479962160601756.[0m
[32m[I 2025-02-06 13:46:50,542][0m Trial 14 finished with value: 0.7614498390796337 and parameters: {'observation_period_num': 53, 'train_rates': 0.6801156743493866, 'learning_rate': 1.5252983955675403e-06, 'batch_size': 61, 'step_size': 9, 'gamma': 0.8979325596153288}. Best is trial 13 with value: 0.12479962160601756.[0m
[32m[I 2025-02-06 13:52:09,379][0m Trial 15 finished with value: 0.1679188354792689 and parameters: {'observation_period_num': 161, 'train_rates': 0.8386580907206626, 'learning_rate': 0.0001370614559898244, 'batch_size': 16, 'step_size': 9, 'gamma': 0.918749243345028}. Best is trial 13 with value: 0.12479962160601756.[0m
[32m[I 2025-02-06 13:53:51,957][0m Trial 16 finished with value: 0.08335395157337189 and parameters: {'observation_period_num': 41, 'train_rates': 0.989431092145122, 'learning_rate': 0.0004944678700067078, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8514477287955}. Best is trial 16 with value: 0.08335395157337189.[0m
[32m[I 2025-02-06 13:55:22,761][0m Trial 17 finished with value: 0.07874890344969013 and parameters: {'observation_period_num': 43, 'train_rates': 0.9422720335013953, 'learning_rate': 0.0004440584041996576, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8494860411223487}. Best is trial 17 with value: 0.07874890344969013.[0m
[32m[I 2025-02-06 13:56:30,043][0m Trial 18 finished with value: 0.1342964619398117 and parameters: {'observation_period_num': 77, 'train_rates': 0.9877088431314618, 'learning_rate': 0.0004516007280411326, 'batch_size': 92, 'step_size': 11, 'gamma': 0.8407889218905136}. Best is trial 17 with value: 0.07874890344969013.[0m
[32m[I 2025-02-06 13:57:35,257][0m Trial 19 finished with value: 0.3000127077102661 and parameters: {'observation_period_num': 141, 'train_rates': 0.9896232414061745, 'learning_rate': 0.0004350882758138597, 'batch_size': 91, 'step_size': 10, 'gamma': 0.8628059299973164}. Best is trial 17 with value: 0.07874890344969013.[0m
[32m[I 2025-02-06 13:58:10,464][0m Trial 20 finished with value: 0.05942120482312872 and parameters: {'observation_period_num': 38, 'train_rates': 0.9206352105954825, 'learning_rate': 0.0009069396564073145, 'batch_size': 180, 'step_size': 3, 'gamma': 0.8204926869397634}. Best is trial 20 with value: 0.05942120482312872.[0m
[32m[I 2025-02-06 13:58:40,213][0m Trial 21 finished with value: 0.05894931310562275 and parameters: {'observation_period_num': 41, 'train_rates': 0.9244133637458505, 'learning_rate': 0.0007964616772338106, 'batch_size': 219, 'step_size': 4, 'gamma': 0.8166491774527883}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 13:59:14,864][0m Trial 22 finished with value: 0.08976899481675016 and parameters: {'observation_period_num': 40, 'train_rates': 0.9245125654518318, 'learning_rate': 0.0008700855802439122, 'batch_size': 187, 'step_size': 2, 'gamma': 0.8141369183837124}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 13:59:39,874][0m Trial 23 finished with value: 0.10647199358101245 and parameters: {'observation_period_num': 66, 'train_rates': 0.9076816175362884, 'learning_rate': 0.0009187272528778161, 'batch_size': 250, 'step_size': 3, 'gamma': 0.8176307678530846}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:00:08,804][0m Trial 24 finished with value: 0.084680416015549 and parameters: {'observation_period_num': 86, 'train_rates': 0.8663026269538399, 'learning_rate': 0.0003251357063631889, 'batch_size': 201, 'step_size': 4, 'gamma': 0.8814014051169737}. Best is trial 21 with value: 0.05894931310562275.[0m
Early stopping at epoch 63
[32m[I 2025-02-06 14:00:27,989][0m Trial 25 finished with value: 0.1989525854587555 and parameters: {'observation_period_num': 29, 'train_rates': 0.9483977309992094, 'learning_rate': 0.000974157359782099, 'batch_size': 226, 'step_size': 1, 'gamma': 0.8101797309645432}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:01:00,794][0m Trial 26 finished with value: 0.251956182172875 and parameters: {'observation_period_num': 202, 'train_rates': 0.8904927074193785, 'learning_rate': 9.719691515367414e-05, 'batch_size': 176, 'step_size': 5, 'gamma': 0.8258296351305203}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:01:29,658][0m Trial 27 finished with value: 0.2385062426328659 and parameters: {'observation_period_num': 132, 'train_rates': 0.9576218169716555, 'learning_rate': 0.0003296430952321321, 'batch_size': 217, 'step_size': 3, 'gamma': 0.8624678904047672}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:02:03,899][0m Trial 28 finished with value: 0.0815108847383995 and parameters: {'observation_period_num': 50, 'train_rates': 0.8637130211312664, 'learning_rate': 0.0004212368043974204, 'batch_size': 177, 'step_size': 12, 'gamma': 0.7858334593753071}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:02:34,630][0m Trial 29 finished with value: 0.11693023663500081 and parameters: {'observation_period_num': 86, 'train_rates': 0.8259252607141534, 'learning_rate': 0.0005489649426352672, 'batch_size': 192, 'step_size': 8, 'gamma': 0.848475895402126}. Best is trial 21 with value: 0.05894931310562275.[0m
Early stopping at epoch 85
[32m[I 2025-02-06 14:03:06,353][0m Trial 30 finished with value: 0.7516166528358179 and parameters: {'observation_period_num': 74, 'train_rates': 0.9068578813339121, 'learning_rate': 2.135513379145787e-05, 'batch_size': 162, 'step_size': 2, 'gamma': 0.7606196913127821}. Best is trial 21 with value: 0.05894931310562275.[0m
[32m[I 2025-02-06 14:03:41,277][0m Trial 31 finished with value: 0.04804145001008639 and parameters: {'observation_period_num': 44, 'train_rates': 0.8487579211845263, 'learning_rate': 0.0005983598783747725, 'batch_size': 175, 'step_size': 12, 'gamma': 0.7870888613287556}. Best is trial 31 with value: 0.04804145001008639.[0m
[32m[I 2025-02-06 14:04:34,709][0m Trial 32 finished with value: 0.04561038596762551 and parameters: {'observation_period_num': 24, 'train_rates': 0.8482802161905754, 'learning_rate': 0.0006160686711315891, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8279996508470088}. Best is trial 32 with value: 0.04561038596762551.[0m
[32m[I 2025-02-06 14:05:26,425][0m Trial 33 finished with value: 0.21242816060333805 and parameters: {'observation_period_num': 24, 'train_rates': 0.842833485452583, 'learning_rate': 6.374781592609294e-06, 'batch_size': 113, 'step_size': 15, 'gamma': 0.7993816167058193}. Best is trial 32 with value: 0.04561038596762551.[0m
[32m[I 2025-02-06 14:06:01,807][0m Trial 34 finished with value: 0.10796965885451115 and parameters: {'observation_period_num': 57, 'train_rates': 0.8137453623346681, 'learning_rate': 0.0006787173735465893, 'batch_size': 163, 'step_size': 15, 'gamma': 0.8297797907933561}. Best is trial 32 with value: 0.04561038596762551.[0m
[32m[I 2025-02-06 14:06:42,415][0m Trial 35 finished with value: 0.04199157186083459 and parameters: {'observation_period_num': 23, 'train_rates': 0.7935222550965568, 'learning_rate': 0.0006283737370601818, 'batch_size': 145, 'step_size': 13, 'gamma': 0.7876328111174241}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:07:21,549][0m Trial 36 finished with value: 0.174157178313747 and parameters: {'observation_period_num': 21, 'train_rates': 0.7840023703568396, 'learning_rate': 0.0006067655525175987, 'batch_size': 148, 'step_size': 13, 'gamma': 0.7721862074062559}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:08:07,965][0m Trial 37 finished with value: 0.19058274088994318 and parameters: {'observation_period_num': 68, 'train_rates': 0.7401799252730255, 'learning_rate': 0.0002487390344822742, 'batch_size': 112, 'step_size': 14, 'gamma': 0.755157082348705}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:09:00,216][0m Trial 38 finished with value: 0.08223612518871531 and parameters: {'observation_period_num': 99, 'train_rates': 0.794373593425793, 'learning_rate': 0.00010160129945491768, 'batch_size': 100, 'step_size': 13, 'gamma': 0.786435776209367}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:09:38,005][0m Trial 39 finished with value: 0.6118187780650157 and parameters: {'observation_period_num': 19, 'train_rates': 0.7683424282716957, 'learning_rate': 5.6166820317501405e-06, 'batch_size': 144, 'step_size': 12, 'gamma': 0.7998486080175163}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:10:19,734][0m Trial 40 finished with value: 0.13695819684109845 and parameters: {'observation_period_num': 119, 'train_rates': 0.8099382375689457, 'learning_rate': 6.552395862225409e-05, 'batch_size': 134, 'step_size': 15, 'gamma': 0.7796499466561387}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:10:55,826][0m Trial 41 finished with value: 0.04385148814757063 and parameters: {'observation_period_num': 33, 'train_rates': 0.8661136034607783, 'learning_rate': 0.0007262106599297088, 'batch_size': 170, 'step_size': 12, 'gamma': 0.7652565071175724}. Best is trial 35 with value: 0.04199157186083459.[0m
[32m[I 2025-02-06 14:11:32,565][0m Trial 42 finished with value: 0.041558213481171564 and parameters: {'observation_period_num': 30, 'train_rates': 0.8514848479355617, 'learning_rate': 0.0006598406006499857, 'batch_size': 166, 'step_size': 12, 'gamma': 0.7686924686318742}. Best is trial 42 with value: 0.041558213481171564.[0m
[32m[I 2025-02-06 14:12:09,056][0m Trial 43 finished with value: 0.04311056766905111 and parameters: {'observation_period_num': 24, 'train_rates': 0.8521615159867534, 'learning_rate': 0.0003313123873008186, 'batch_size': 165, 'step_size': 12, 'gamma': 0.7632035851008923}. Best is trial 42 with value: 0.041558213481171564.[0m
[32m[I 2025-02-06 14:12:50,206][0m Trial 44 finished with value: 0.0416456099207464 and parameters: {'observation_period_num': 28, 'train_rates': 0.8845750636396568, 'learning_rate': 0.0003177681061902598, 'batch_size': 153, 'step_size': 14, 'gamma': 0.762984473839154}. Best is trial 42 with value: 0.041558213481171564.[0m
[32m[I 2025-02-06 14:13:28,733][0m Trial 45 finished with value: 0.043983056120215414 and parameters: {'observation_period_num': 13, 'train_rates': 0.8886523265302853, 'learning_rate': 0.00028954838602993435, 'batch_size': 157, 'step_size': 14, 'gamma': 0.765788136413682}. Best is trial 42 with value: 0.041558213481171564.[0m
[32m[I 2025-02-06 14:14:09,989][0m Trial 46 finished with value: 0.042594180412982637 and parameters: {'observation_period_num': 30, 'train_rates': 0.8715843336627888, 'learning_rate': 0.0003292273873162556, 'batch_size': 144, 'step_size': 11, 'gamma': 0.7514270808785792}. Best is trial 42 with value: 0.041558213481171564.[0m
[32m[I 2025-02-06 14:14:52,825][0m Trial 47 finished with value: 0.03977776229911044 and parameters: {'observation_period_num': 6, 'train_rates': 0.8021911218550802, 'learning_rate': 0.00023351607783887013, 'batch_size': 138, 'step_size': 11, 'gamma': 0.75172263979988}. Best is trial 47 with value: 0.03977776229911044.[0m
[32m[I 2025-02-06 14:15:38,607][0m Trial 48 finished with value: 0.04081339180469513 and parameters: {'observation_period_num': 6, 'train_rates': 0.824116379059233, 'learning_rate': 0.00018856356109243037, 'batch_size': 126, 'step_size': 11, 'gamma': 0.7533188845602546}. Best is trial 47 with value: 0.03977776229911044.[0m
[32m[I 2025-02-06 14:16:24,354][0m Trial 49 finished with value: 0.037858470532200095 and parameters: {'observation_period_num': 15, 'train_rates': 0.8261928373432448, 'learning_rate': 0.00021506543635735638, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7757979915487954}. Best is trial 49 with value: 0.037858470532200095.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3383 | 0.2952
Epoch 2/300, Loss: 0.1862 | 0.2165
Epoch 3/300, Loss: 0.1616 | 0.1919
Epoch 4/300, Loss: 0.1429 | 0.1597
Epoch 5/300, Loss: 0.1289 | 0.1330
Epoch 6/300, Loss: 0.1225 | 0.1153
Epoch 7/300, Loss: 0.1207 | 0.1003
Epoch 8/300, Loss: 0.1163 | 0.0968
Epoch 9/300, Loss: 0.1193 | 0.0915
Epoch 10/300, Loss: 0.1400 | 0.1478
Epoch 11/300, Loss: 0.1504 | 0.1097
Epoch 12/300, Loss: 0.1331 | 0.2151
Epoch 13/300, Loss: 0.1264 | 0.0953
Epoch 14/300, Loss: 0.1122 | 0.0746
Epoch 15/300, Loss: 0.1060 | 0.0717
Epoch 16/300, Loss: 0.1054 | 0.0713
Epoch 17/300, Loss: 0.1015 | 0.0708
Epoch 18/300, Loss: 0.1000 | 0.0685
Epoch 19/300, Loss: 0.0988 | 0.0676
Epoch 20/300, Loss: 0.0973 | 0.0655
Epoch 21/300, Loss: 0.0961 | 0.0671
Epoch 22/300, Loss: 0.0958 | 0.0685
Epoch 23/300, Loss: 0.0957 | 0.0651
Epoch 24/300, Loss: 0.0946 | 0.0637
Epoch 25/300, Loss: 0.0932 | 0.0623
Epoch 26/300, Loss: 0.0923 | 0.0612
Epoch 27/300, Loss: 0.0918 | 0.0616
Epoch 28/300, Loss: 0.0913 | 0.0647
Epoch 29/300, Loss: 0.0914 | 0.0643
Epoch 30/300, Loss: 0.0911 | 0.0600
Epoch 31/300, Loss: 0.0905 | 0.0621
Epoch 32/300, Loss: 0.0911 | 0.0586
Epoch 33/300, Loss: 0.0921 | 0.0565
Epoch 34/300, Loss: 0.0914 | 0.0630
Epoch 35/300, Loss: 0.0898 | 0.0638
Epoch 36/300, Loss: 0.0887 | 0.0565
Epoch 37/300, Loss: 0.0917 | 0.0622
Epoch 38/300, Loss: 0.0932 | 0.0553
Epoch 39/300, Loss: 0.0899 | 0.0552
Epoch 40/300, Loss: 0.0877 | 0.0562
Epoch 41/300, Loss: 0.0895 | 0.0566
Epoch 42/300, Loss: 0.0911 | 0.0571
Epoch 43/300, Loss: 0.0919 | 0.0548
Epoch 44/300, Loss: 0.0893 | 0.0523
Epoch 45/300, Loss: 0.0906 | 0.0536
Epoch 46/300, Loss: 0.0868 | 0.0522
Epoch 47/300, Loss: 0.0840 | 0.0542
Epoch 48/300, Loss: 0.0847 | 0.0522
Epoch 49/300, Loss: 0.0838 | 0.0503
Epoch 50/300, Loss: 0.0842 | 0.0503
Epoch 51/300, Loss: 0.0823 | 0.0511
Epoch 52/300, Loss: 0.0818 | 0.0508
Epoch 53/300, Loss: 0.0815 | 0.0493
Epoch 54/300, Loss: 0.0813 | 0.0491
Epoch 55/300, Loss: 0.0808 | 0.0493
Epoch 56/300, Loss: 0.0807 | 0.0494
Epoch 57/300, Loss: 0.0807 | 0.0488
Epoch 58/300, Loss: 0.0808 | 0.0486
Epoch 59/300, Loss: 0.0811 | 0.0488
Epoch 60/300, Loss: 0.0816 | 0.0487
Epoch 61/300, Loss: 0.0815 | 0.0480
Epoch 62/300, Loss: 0.0805 | 0.0478
Epoch 63/300, Loss: 0.0796 | 0.0478
Epoch 64/300, Loss: 0.0794 | 0.0477
Epoch 65/300, Loss: 0.0793 | 0.0476
Epoch 66/300, Loss: 0.0791 | 0.0474
Epoch 67/300, Loss: 0.0790 | 0.0473
Epoch 68/300, Loss: 0.0789 | 0.0472
Epoch 69/300, Loss: 0.0788 | 0.0471
Epoch 70/300, Loss: 0.0787 | 0.0470
Epoch 71/300, Loss: 0.0785 | 0.0469
Epoch 72/300, Loss: 0.0785 | 0.0468
Epoch 73/300, Loss: 0.0784 | 0.0467
Epoch 74/300, Loss: 0.0783 | 0.0466
Epoch 75/300, Loss: 0.0782 | 0.0465
Epoch 76/300, Loss: 0.0781 | 0.0464
Epoch 77/300, Loss: 0.0780 | 0.0463
Epoch 78/300, Loss: 0.0779 | 0.0462
Epoch 79/300, Loss: 0.0779 | 0.0461
Epoch 80/300, Loss: 0.0778 | 0.0460
Epoch 81/300, Loss: 0.0777 | 0.0460
Epoch 82/300, Loss: 0.0777 | 0.0459
Epoch 83/300, Loss: 0.0776 | 0.0458
Epoch 84/300, Loss: 0.0776 | 0.0457
Epoch 85/300, Loss: 0.0775 | 0.0457
Epoch 86/300, Loss: 0.0774 | 0.0456
Epoch 87/300, Loss: 0.0774 | 0.0455
Epoch 88/300, Loss: 0.0773 | 0.0455
Epoch 89/300, Loss: 0.0773 | 0.0454
Epoch 90/300, Loss: 0.0772 | 0.0454
Epoch 91/300, Loss: 0.0772 | 0.0453
Epoch 92/300, Loss: 0.0771 | 0.0453
Epoch 93/300, Loss: 0.0771 | 0.0452
Epoch 94/300, Loss: 0.0770 | 0.0452
Epoch 95/300, Loss: 0.0770 | 0.0451
Epoch 96/300, Loss: 0.0769 | 0.0451
Epoch 97/300, Loss: 0.0769 | 0.0450
Epoch 98/300, Loss: 0.0768 | 0.0450
Epoch 99/300, Loss: 0.0768 | 0.0449
Epoch 100/300, Loss: 0.0768 | 0.0449
Epoch 101/300, Loss: 0.0767 | 0.0449
Epoch 102/300, Loss: 0.0767 | 0.0448
Epoch 103/300, Loss: 0.0767 | 0.0448
Epoch 104/300, Loss: 0.0766 | 0.0448
Epoch 105/300, Loss: 0.0766 | 0.0447
Epoch 106/300, Loss: 0.0766 | 0.0447
Epoch 107/300, Loss: 0.0765 | 0.0447
Epoch 108/300, Loss: 0.0765 | 0.0446
Epoch 109/300, Loss: 0.0765 | 0.0446
Epoch 110/300, Loss: 0.0765 | 0.0446
Epoch 111/300, Loss: 0.0764 | 0.0446
Epoch 112/300, Loss: 0.0764 | 0.0445
Epoch 113/300, Loss: 0.0764 | 0.0445
Epoch 114/300, Loss: 0.0763 | 0.0445
Epoch 115/300, Loss: 0.0763 | 0.0445
Epoch 116/300, Loss: 0.0763 | 0.0444
Epoch 117/300, Loss: 0.0763 | 0.0444
Epoch 118/300, Loss: 0.0763 | 0.0444
Epoch 119/300, Loss: 0.0762 | 0.0444
Epoch 120/300, Loss: 0.0762 | 0.0444
Epoch 121/300, Loss: 0.0762 | 0.0443
Epoch 122/300, Loss: 0.0762 | 0.0443
Epoch 123/300, Loss: 0.0762 | 0.0443
Epoch 124/300, Loss: 0.0761 | 0.0443
Epoch 125/300, Loss: 0.0761 | 0.0443
Epoch 126/300, Loss: 0.0761 | 0.0442
Epoch 127/300, Loss: 0.0761 | 0.0442
Epoch 128/300, Loss: 0.0761 | 0.0442
Epoch 129/300, Loss: 0.0761 | 0.0442
Epoch 130/300, Loss: 0.0761 | 0.0442
Epoch 131/300, Loss: 0.0760 | 0.0442
Epoch 132/300, Loss: 0.0760 | 0.0442
Epoch 133/300, Loss: 0.0760 | 0.0442
Epoch 134/300, Loss: 0.0760 | 0.0441
Epoch 135/300, Loss: 0.0760 | 0.0441
Epoch 136/300, Loss: 0.0760 | 0.0441
Epoch 137/300, Loss: 0.0760 | 0.0441
Epoch 138/300, Loss: 0.0760 | 0.0441
Epoch 139/300, Loss: 0.0759 | 0.0441
Epoch 140/300, Loss: 0.0759 | 0.0441
Epoch 141/300, Loss: 0.0759 | 0.0441
Epoch 142/300, Loss: 0.0759 | 0.0441
Epoch 143/300, Loss: 0.0759 | 0.0441
Epoch 144/300, Loss: 0.0759 | 0.0440
Epoch 145/300, Loss: 0.0759 | 0.0440
Epoch 146/300, Loss: 0.0759 | 0.0440
Epoch 147/300, Loss: 0.0759 | 0.0440
Epoch 148/300, Loss: 0.0759 | 0.0440
Epoch 149/300, Loss: 0.0758 | 0.0440
Epoch 150/300, Loss: 0.0758 | 0.0440
Epoch 151/300, Loss: 0.0758 | 0.0440
Epoch 152/300, Loss: 0.0758 | 0.0440
Epoch 153/300, Loss: 0.0758 | 0.0440
Epoch 154/300, Loss: 0.0758 | 0.0440
Epoch 155/300, Loss: 0.0758 | 0.0440
Epoch 156/300, Loss: 0.0758 | 0.0440
Epoch 157/300, Loss: 0.0758 | 0.0439
Epoch 158/300, Loss: 0.0758 | 0.0439
Epoch 159/300, Loss: 0.0758 | 0.0439
Epoch 160/300, Loss: 0.0758 | 0.0439
Epoch 161/300, Loss: 0.0758 | 0.0439
Epoch 162/300, Loss: 0.0758 | 0.0439
Epoch 163/300, Loss: 0.0758 | 0.0439
Epoch 164/300, Loss: 0.0758 | 0.0439
Epoch 165/300, Loss: 0.0758 | 0.0439
Epoch 166/300, Loss: 0.0757 | 0.0439
Epoch 167/300, Loss: 0.0757 | 0.0439
Epoch 168/300, Loss: 0.0757 | 0.0439
Epoch 169/300, Loss: 0.0757 | 0.0439
Epoch 170/300, Loss: 0.0757 | 0.0439
Epoch 171/300, Loss: 0.0757 | 0.0439
Epoch 172/300, Loss: 0.0757 | 0.0439
Epoch 173/300, Loss: 0.0757 | 0.0439
Epoch 174/300, Loss: 0.0757 | 0.0439
Epoch 175/300, Loss: 0.0757 | 0.0439
Epoch 176/300, Loss: 0.0757 | 0.0439
Epoch 177/300, Loss: 0.0757 | 0.0439
Epoch 178/300, Loss: 0.0757 | 0.0439
Epoch 179/300, Loss: 0.0757 | 0.0439
Epoch 180/300, Loss: 0.0757 | 0.0439
Epoch 181/300, Loss: 0.0757 | 0.0439
Epoch 182/300, Loss: 0.0757 | 0.0439
Epoch 183/300, Loss: 0.0757 | 0.0439
Epoch 184/300, Loss: 0.0757 | 0.0438
Epoch 185/300, Loss: 0.0757 | 0.0438
Epoch 186/300, Loss: 0.0757 | 0.0438
Epoch 187/300, Loss: 0.0757 | 0.0438
Epoch 188/300, Loss: 0.0757 | 0.0438
Epoch 189/300, Loss: 0.0757 | 0.0438
Epoch 190/300, Loss: 0.0757 | 0.0438
Epoch 191/300, Loss: 0.0757 | 0.0438
Epoch 192/300, Loss: 0.0757 | 0.0438
Epoch 193/300, Loss: 0.0757 | 0.0438
Epoch 194/300, Loss: 0.0757 | 0.0438
Epoch 195/300, Loss: 0.0757 | 0.0438
Epoch 196/300, Loss: 0.0757 | 0.0438
Epoch 197/300, Loss: 0.0757 | 0.0438
Epoch 198/300, Loss: 0.0757 | 0.0438
Epoch 199/300, Loss: 0.0757 | 0.0438
Epoch 200/300, Loss: 0.0757 | 0.0438
Epoch 201/300, Loss: 0.0757 | 0.0438
Epoch 202/300, Loss: 0.0757 | 0.0438
Epoch 203/300, Loss: 0.0757 | 0.0438
Epoch 204/300, Loss: 0.0756 | 0.0438
Epoch 205/300, Loss: 0.0756 | 0.0438
Epoch 206/300, Loss: 0.0756 | 0.0438
Epoch 207/300, Loss: 0.0756 | 0.0438
Epoch 208/300, Loss: 0.0756 | 0.0438
Epoch 209/300, Loss: 0.0756 | 0.0438
Epoch 210/300, Loss: 0.0756 | 0.0438
Epoch 211/300, Loss: 0.0756 | 0.0438
Epoch 212/300, Loss: 0.0756 | 0.0438
Epoch 213/300, Loss: 0.0756 | 0.0438
Epoch 214/300, Loss: 0.0756 | 0.0438
Epoch 215/300, Loss: 0.0756 | 0.0438
Epoch 216/300, Loss: 0.0756 | 0.0438
Epoch 217/300, Loss: 0.0756 | 0.0438
Epoch 218/300, Loss: 0.0756 | 0.0438
Epoch 219/300, Loss: 0.0756 | 0.0438
Epoch 220/300, Loss: 0.0756 | 0.0438
Epoch 221/300, Loss: 0.0756 | 0.0438
Epoch 222/300, Loss: 0.0756 | 0.0438
Epoch 223/300, Loss: 0.0756 | 0.0438
Epoch 224/300, Loss: 0.0756 | 0.0438
Epoch 225/300, Loss: 0.0756 | 0.0438
Epoch 226/300, Loss: 0.0756 | 0.0438
Epoch 227/300, Loss: 0.0756 | 0.0438
Epoch 228/300, Loss: 0.0756 | 0.0438
Epoch 229/300, Loss: 0.0756 | 0.0438
Epoch 230/300, Loss: 0.0756 | 0.0438
Epoch 231/300, Loss: 0.0756 | 0.0438
Epoch 232/300, Loss: 0.0756 | 0.0438
Epoch 233/300, Loss: 0.0756 | 0.0438
Epoch 234/300, Loss: 0.0756 | 0.0438
Epoch 235/300, Loss: 0.0756 | 0.0438
Epoch 236/300, Loss: 0.0756 | 0.0438
Epoch 237/300, Loss: 0.0756 | 0.0438
Epoch 238/300, Loss: 0.0756 | 0.0438
Epoch 239/300, Loss: 0.0756 | 0.0438
Epoch 240/300, Loss: 0.0756 | 0.0438
Epoch 241/300, Loss: 0.0756 | 0.0438
Epoch 242/300, Loss: 0.0756 | 0.0438
Epoch 243/300, Loss: 0.0756 | 0.0438
Epoch 244/300, Loss: 0.0756 | 0.0438
Epoch 245/300, Loss: 0.0756 | 0.0438
Epoch 246/300, Loss: 0.0756 | 0.0438
Epoch 247/300, Loss: 0.0756 | 0.0438
Epoch 248/300, Loss: 0.0756 | 0.0438
Epoch 249/300, Loss: 0.0756 | 0.0438
Epoch 250/300, Loss: 0.0756 | 0.0438
Epoch 251/300, Loss: 0.0756 | 0.0438
Epoch 252/300, Loss: 0.0756 | 0.0438
Epoch 253/300, Loss: 0.0756 | 0.0438
Epoch 254/300, Loss: 0.0756 | 0.0438
Epoch 255/300, Loss: 0.0756 | 0.0438
Epoch 256/300, Loss: 0.0756 | 0.0438
Epoch 257/300, Loss: 0.0756 | 0.0438
Epoch 258/300, Loss: 0.0756 | 0.0438
Epoch 259/300, Loss: 0.0756 | 0.0438
Epoch 260/300, Loss: 0.0756 | 0.0438
Epoch 261/300, Loss: 0.0756 | 0.0438
Epoch 262/300, Loss: 0.0756 | 0.0438
Epoch 263/300, Loss: 0.0756 | 0.0438
Epoch 264/300, Loss: 0.0756 | 0.0438
Epoch 265/300, Loss: 0.0756 | 0.0438
Epoch 266/300, Loss: 0.0756 | 0.0438
Epoch 267/300, Loss: 0.0756 | 0.0438
Epoch 268/300, Loss: 0.0756 | 0.0438
Epoch 269/300, Loss: 0.0756 | 0.0438
Epoch 270/300, Loss: 0.0756 | 0.0438
Epoch 271/300, Loss: 0.0756 | 0.0438
Epoch 272/300, Loss: 0.0756 | 0.0438
Epoch 273/300, Loss: 0.0756 | 0.0438
Epoch 274/300, Loss: 0.0756 | 0.0438
Epoch 275/300, Loss: 0.0756 | 0.0438
Epoch 276/300, Loss: 0.0756 | 0.0438
Epoch 277/300, Loss: 0.0756 | 0.0438
Epoch 278/300, Loss: 0.0756 | 0.0438
Epoch 279/300, Loss: 0.0756 | 0.0438
Epoch 280/300, Loss: 0.0756 | 0.0438
Epoch 281/300, Loss: 0.0756 | 0.0438
Epoch 282/300, Loss: 0.0756 | 0.0438
Epoch 283/300, Loss: 0.0756 | 0.0438
Epoch 284/300, Loss: 0.0756 | 0.0438
Epoch 285/300, Loss: 0.0756 | 0.0438
Epoch 286/300, Loss: 0.0756 | 0.0438
Epoch 287/300, Loss: 0.0756 | 0.0438
Epoch 288/300, Loss: 0.0756 | 0.0438
Epoch 289/300, Loss: 0.0756 | 0.0438
Epoch 290/300, Loss: 0.0756 | 0.0438
Epoch 291/300, Loss: 0.0756 | 0.0438
Epoch 292/300, Loss: 0.0756 | 0.0438
Epoch 293/300, Loss: 0.0756 | 0.0438
Epoch 294/300, Loss: 0.0756 | 0.0438
Epoch 295/300, Loss: 0.0756 | 0.0438
Epoch 296/300, Loss: 0.0756 | 0.0438
Epoch 297/300, Loss: 0.0756 | 0.0438
Epoch 298/300, Loss: 0.0756 | 0.0438
Epoch 299/300, Loss: 0.0756 | 0.0438
Epoch 300/300, Loss: 0.0756 | 0.0438
Runtime (seconds): 136.2462043762207
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 7.831389084458351
RMSE: 2.7984619140625
MAE: 2.7984619140625
R-squared: nan
[196.78154]
