ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-05 20:15:32,955][0m A new study created in memory with name: no-name-c54d0f8d-62cb-487b-b201-70c5aaf0e79a[0m
[32m[I 2025-02-05 20:19:14,235][0m Trial 0 finished with value: 0.946730068212823 and parameters: {'observation_period_num': 176, 'train_rates': 0.9148676046232578, 'learning_rate': 0.0007846278396594626, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7754382715767426}. Best is trial 0 with value: 0.946730068212823.[0m
[32m[I 2025-02-05 20:22:14,346][0m Trial 1 finished with value: 1.1224744892721876 and parameters: {'observation_period_num': 196, 'train_rates': 0.604576077353511, 'learning_rate': 3.427505249123918e-05, 'batch_size': 110, 'step_size': 10, 'gamma': 0.900322284470296}. Best is trial 0 with value: 0.946730068212823.[0m
[32m[I 2025-02-05 20:25:25,473][0m Trial 2 finished with value: 1.1876551550055932 and parameters: {'observation_period_num': 178, 'train_rates': 0.838240536808265, 'learning_rate': 1.0026221229018791e-05, 'batch_size': 105, 'step_size': 4, 'gamma': 0.7995461921763548}. Best is trial 0 with value: 0.946730068212823.[0m
[32m[I 2025-02-05 20:26:56,930][0m Trial 3 finished with value: 1.5876729543585526 and parameters: {'observation_period_num': 104, 'train_rates': 0.6666388700908017, 'learning_rate': 5.162606550428498e-06, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8975788262595108}. Best is trial 0 with value: 0.946730068212823.[0m
[32m[I 2025-02-05 20:28:28,286][0m Trial 4 finished with value: 1.0466238351030783 and parameters: {'observation_period_num': 94, 'train_rates': 0.8460597907181167, 'learning_rate': 4.905295998382163e-05, 'batch_size': 239, 'step_size': 2, 'gamma': 0.7967302584538781}. Best is trial 0 with value: 0.946730068212823.[0m
[32m[I 2025-02-05 20:30:33,566][0m Trial 5 finished with value: 0.9352986156023466 and parameters: {'observation_period_num': 64, 'train_rates': 0.7752673774901703, 'learning_rate': 5.819271830037715e-06, 'batch_size': 33, 'step_size': 13, 'gamma': 0.8088937300737351}. Best is trial 5 with value: 0.9352986156023466.[0m
[32m[I 2025-02-05 20:32:11,745][0m Trial 6 finished with value: 1.3074614692764028 and parameters: {'observation_period_num': 95, 'train_rates': 0.8949227384196883, 'learning_rate': 7.523050430506876e-06, 'batch_size': 101, 'step_size': 2, 'gamma': 0.8792133855428893}. Best is trial 5 with value: 0.9352986156023466.[0m
[32m[I 2025-02-05 20:35:46,300][0m Trial 7 finished with value: 1.4155633404166807 and parameters: {'observation_period_num': 225, 'train_rates': 0.6727256810071687, 'learning_rate': 7.514110109964729e-06, 'batch_size': 176, 'step_size': 11, 'gamma': 0.8072675161126188}. Best is trial 5 with value: 0.9352986156023466.[0m
[32m[I 2025-02-05 20:37:22,716][0m Trial 8 finished with value: 1.0421134297274424 and parameters: {'observation_period_num': 116, 'train_rates': 0.6393339101349775, 'learning_rate': 3.0338176859240025e-05, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8648713071415679}. Best is trial 5 with value: 0.9352986156023466.[0m
[32m[I 2025-02-05 20:39:44,170][0m Trial 9 finished with value: 0.7635317489282408 and parameters: {'observation_period_num': 147, 'train_rates': 0.7375756891537876, 'learning_rate': 0.0006230064483728207, 'batch_size': 209, 'step_size': 7, 'gamma': 0.8493903922271031}. Best is trial 9 with value: 0.7635317489282408.[0m
[32m[I 2025-02-05 20:40:10,424][0m Trial 10 finished with value: 0.9347634514172872 and parameters: {'observation_period_num': 26, 'train_rates': 0.7386436067832295, 'learning_rate': 0.0009261128586938752, 'batch_size': 255, 'step_size': 7, 'gamma': 0.9863715754653437}. Best is trial 9 with value: 0.7635317489282408.[0m
[32m[I 2025-02-05 20:40:28,626][0m Trial 11 finished with value: 1.0712322480034098 and parameters: {'observation_period_num': 9, 'train_rates': 0.7450393075439209, 'learning_rate': 0.0008906746199281063, 'batch_size': 255, 'step_size': 7, 'gamma': 0.9858083037259501}. Best is trial 9 with value: 0.7635317489282408.[0m
[32m[I 2025-02-05 20:40:50,920][0m Trial 12 finished with value: 0.8430975682878435 and parameters: {'observation_period_num': 14, 'train_rates': 0.7221774736016663, 'learning_rate': 0.0002198362127865295, 'batch_size': 210, 'step_size': 8, 'gamma': 0.9819446139288587}. Best is trial 9 with value: 0.7635317489282408.[0m
[32m[I 2025-02-05 20:43:06,739][0m Trial 13 finished with value: 0.914540191050194 and parameters: {'observation_period_num': 151, 'train_rates': 0.7028068725348476, 'learning_rate': 0.00020485552669385364, 'batch_size': 207, 'step_size': 15, 'gamma': 0.9390712768808239}. Best is trial 9 with value: 0.7635317489282408.[0m
[32m[I 2025-02-05 20:44:02,065][0m Trial 14 finished with value: 0.1891755908727646 and parameters: {'observation_period_num': 55, 'train_rates': 0.9816849250900748, 'learning_rate': 0.00024289207162824682, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8450822781305919}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:45:03,865][0m Trial 15 finished with value: 2.5916504859924316 and parameters: {'observation_period_num': 63, 'train_rates': 0.970203564589434, 'learning_rate': 1.2248615341763414e-06, 'batch_size': 194, 'step_size': 4, 'gamma': 0.8457328581485772}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:47:54,293][0m Trial 16 finished with value: 0.19327384233474731 and parameters: {'observation_period_num': 146, 'train_rates': 0.9806740714750608, 'learning_rate': 0.00024357000768893586, 'batch_size': 144, 'step_size': 9, 'gamma': 0.8377735418977056}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:48:52,892][0m Trial 17 finished with value: 0.21134160459041595 and parameters: {'observation_period_num': 54, 'train_rates': 0.9802060002863422, 'learning_rate': 0.00014369013761736724, 'batch_size': 145, 'step_size': 9, 'gamma': 0.8413729339282116}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:54:21,061][0m Trial 18 finished with value: 0.2281427726253763 and parameters: {'observation_period_num': 249, 'train_rates': 0.9473102098110796, 'learning_rate': 0.00011899751078412581, 'batch_size': 68, 'step_size': 14, 'gamma': 0.93694207707553}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:56:46,680][0m Trial 19 finished with value: 0.5165713017986667 and parameters: {'observation_period_num': 137, 'train_rates': 0.8902124950913366, 'learning_rate': 7.611191673522345e-05, 'batch_size': 142, 'step_size': 5, 'gamma': 0.8252434675555427}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:57:34,393][0m Trial 20 finished with value: 0.4532199029592758 and parameters: {'observation_period_num': 48, 'train_rates': 0.8383705281611266, 'learning_rate': 0.0004121563697223014, 'batch_size': 173, 'step_size': 9, 'gamma': 0.7567380544364256}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 20:58:57,643][0m Trial 21 finished with value: 0.19749028980731964 and parameters: {'observation_period_num': 77, 'train_rates': 0.9867715162194781, 'learning_rate': 0.0002475645870455557, 'batch_size': 131, 'step_size': 9, 'gamma': 0.8296674606965642}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:00:27,567][0m Trial 22 finished with value: 0.2588553372770548 and parameters: {'observation_period_num': 84, 'train_rates': 0.9332526923716692, 'learning_rate': 0.00035592378829756667, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8283101736555479}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:01:11,767][0m Trial 23 finished with value: 0.18949878215789795 and parameters: {'observation_period_num': 37, 'train_rates': 0.9837445248792039, 'learning_rate': 0.0003103384974629194, 'batch_size': 167, 'step_size': 9, 'gamma': 0.8749572677067836}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:01:56,183][0m Trial 24 finished with value: 0.35609757900238037 and parameters: {'observation_period_num': 40, 'train_rates': 0.9473852952761976, 'learning_rate': 8.817845053290187e-05, 'batch_size': 181, 'step_size': 10, 'gamma': 0.8736210936047917}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:03:54,895][0m Trial 25 finished with value: 0.2960208040627812 and parameters: {'observation_period_num': 119, 'train_rates': 0.8754021702926408, 'learning_rate': 0.0004189906030052614, 'batch_size': 230, 'step_size': 8, 'gamma': 0.8995945535816209}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:04:33,114][0m Trial 26 finished with value: 0.688514769077301 and parameters: {'observation_period_num': 30, 'train_rates': 0.9564006690713336, 'learning_rate': 1.8740633789913516e-05, 'batch_size': 153, 'step_size': 13, 'gamma': 0.8604945283487719}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:07:45,557][0m Trial 27 finished with value: 0.2548847432230033 and parameters: {'observation_period_num': 170, 'train_rates': 0.9270192864838396, 'learning_rate': 0.00014801629930276037, 'batch_size': 166, 'step_size': 10, 'gamma': 0.9222808085872285}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:10:03,135][0m Trial 28 finished with value: 0.3661037087440491 and parameters: {'observation_period_num': 127, 'train_rates': 0.9818286740662782, 'learning_rate': 6.062816983527664e-05, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8822441728409764}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:13:42,114][0m Trial 29 finished with value: 0.263205885887146 and parameters: {'observation_period_num': 191, 'train_rates': 0.9217896696231819, 'learning_rate': 0.000594575208278124, 'batch_size': 227, 'step_size': 12, 'gamma': 0.7857286842881104}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:16:21,622][0m Trial 30 finished with value: 0.4926386851627504 and parameters: {'observation_period_num': 156, 'train_rates': 0.7967302151520647, 'learning_rate': 0.00028703976696239133, 'batch_size': 120, 'step_size': 8, 'gamma': 0.766363634306532}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:17:45,466][0m Trial 31 finished with value: 0.19199536740779877 and parameters: {'observation_period_num': 77, 'train_rates': 0.9810966406182456, 'learning_rate': 0.00021671354173662387, 'batch_size': 129, 'step_size': 9, 'gamma': 0.8273394271457772}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:19:10,067][0m Trial 32 finished with value: 0.3566786793095094 and parameters: {'observation_period_num': 75, 'train_rates': 0.9064215696523586, 'learning_rate': 0.0005235240597129848, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8209238075581762}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:19:54,254][0m Trial 33 finished with value: 0.28072652220726013 and parameters: {'observation_period_num': 37, 'train_rates': 0.9578151195116419, 'learning_rate': 0.00011362864040133857, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8565136694982577}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:21:45,120][0m Trial 34 finished with value: 0.24280203878879547 and parameters: {'observation_period_num': 107, 'train_rates': 0.989180972499665, 'learning_rate': 0.00016951438906123088, 'batch_size': 196, 'step_size': 9, 'gamma': 0.7821967391523628}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:22:44,532][0m Trial 35 finished with value: 0.4353267139893897 and parameters: {'observation_period_num': 56, 'train_rates': 0.8702631884563883, 'learning_rate': 4.0105200254399986e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8919795153575666}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:23:24,741][0m Trial 36 finished with value: 0.2599027454853058 and parameters: {'observation_period_num': 21, 'train_rates': 0.9635145556761391, 'learning_rate': 0.00031109584949032737, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8411652848262047}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:25:05,177][0m Trial 37 finished with value: 0.6572340848082203 and parameters: {'observation_period_num': 92, 'train_rates': 0.9384167443739675, 'learning_rate': 1.676148138895992e-05, 'batch_size': 96, 'step_size': 10, 'gamma': 0.9175519877019569}. Best is trial 14 with value: 0.1891755908727646.[0m
Early stopping at epoch 61
[32m[I 2025-02-05 21:25:59,511][0m Trial 38 finished with value: 0.8219777319629119 and parameters: {'observation_period_num': 70, 'train_rates': 0.9206285481995111, 'learning_rate': 8.809922415372444e-05, 'batch_size': 59, 'step_size': 1, 'gamma': 0.807223861586642}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:26:47,587][0m Trial 39 finished with value: 0.41948028405507404 and parameters: {'observation_period_num': 46, 'train_rates': 0.8639818510078657, 'learning_rate': 0.0006340993161634641, 'batch_size': 166, 'step_size': 12, 'gamma': 0.7943705067545112}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:30:42,035][0m Trial 40 finished with value: 0.531210405084322 and parameters: {'observation_period_num': 212, 'train_rates': 0.8068340772466756, 'learning_rate': 0.00021666058490854448, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8136065127154757}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:32:06,124][0m Trial 41 finished with value: 0.19640710949897766 and parameters: {'observation_period_num': 79, 'train_rates': 0.9837439898340294, 'learning_rate': 0.0002493478096893949, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8284812951977235}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:33:58,661][0m Trial 42 finished with value: 0.31976816058158875 and parameters: {'observation_period_num': 103, 'train_rates': 0.9685880704045705, 'learning_rate': 0.0004421862300673869, 'batch_size': 112, 'step_size': 9, 'gamma': 0.8344247369437986}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:35:01,972][0m Trial 43 finished with value: 1.1999923231847145 and parameters: {'observation_period_num': 61, 'train_rates': 0.9060367656798592, 'learning_rate': 2.571586873662729e-06, 'batch_size': 135, 'step_size': 7, 'gamma': 0.852957880809401}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:36:31,565][0m Trial 44 finished with value: 0.25924116373062134 and parameters: {'observation_period_num': 84, 'train_rates': 0.9456912899152329, 'learning_rate': 0.0003261448337448658, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8710425085247142}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:37:31,048][0m Trial 45 finished with value: 0.7565771341323853 and parameters: {'observation_period_num': 30, 'train_rates': 0.9734495626816911, 'learning_rate': 0.0007320776260419676, 'batch_size': 86, 'step_size': 10, 'gamma': 0.8872413417603373}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:40:52,977][0m Trial 46 finished with value: 0.2161814570426941 and parameters: {'observation_period_num': 169, 'train_rates': 0.9894034572208342, 'learning_rate': 0.00017359114636121456, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8157955600883356}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:43:35,233][0m Trial 47 finished with value: 0.28493341973849706 and parameters: {'observation_period_num': 138, 'train_rates': 0.9628595804900673, 'learning_rate': 0.00012135998768018376, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8652652771103211}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:45:26,807][0m Trial 48 finished with value: 0.28670541782980985 and parameters: {'observation_period_num': 111, 'train_rates': 0.8914289049945144, 'learning_rate': 0.0002730014491733364, 'batch_size': 172, 'step_size': 8, 'gamma': 0.8406064231069057}. Best is trial 14 with value: 0.1891755908727646.[0m
[32m[I 2025-02-05 21:45:51,393][0m Trial 49 finished with value: 0.8672139674894864 and parameters: {'observation_period_num': 6, 'train_rates': 0.6123807981836924, 'learning_rate': 5.5899717467718986e-05, 'batch_size': 151, 'step_size': 11, 'gamma': 0.7941468793451043}. Best is trial 14 with value: 0.1891755908727646.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-05 21:45:51,401][0m A new study created in memory with name: no-name-3b9ecd01-895e-485b-afda-fb5da81b7d00[0m
[32m[I 2025-02-05 21:48:35,915][0m Trial 0 finished with value: 1.0078193644154718 and parameters: {'observation_period_num': 158, 'train_rates': 0.6632402037986083, 'learning_rate': 0.00018625720481073876, 'batch_size': 33, 'step_size': 8, 'gamma': 0.7510523049659373}. Best is trial 0 with value: 1.0078193644154718.[0m
[32m[I 2025-02-05 21:51:03,216][0m Trial 1 finished with value: 0.5163389925868058 and parameters: {'observation_period_num': 141, 'train_rates': 0.8279894408946495, 'learning_rate': 0.0007127445692412072, 'batch_size': 147, 'step_size': 5, 'gamma': 0.903422550025066}. Best is trial 1 with value: 0.5163389925868058.[0m
[32m[I 2025-02-05 21:52:37,523][0m Trial 2 finished with value: 0.6310278177261353 and parameters: {'observation_period_num': 89, 'train_rates': 0.958210869223607, 'learning_rate': 4.619185803492899e-05, 'batch_size': 208, 'step_size': 8, 'gamma': 0.8091034998126334}. Best is trial 1 with value: 0.5163389925868058.[0m
[32m[I 2025-02-05 21:54:14,234][0m Trial 3 finished with value: 0.46258479795035196 and parameters: {'observation_period_num': 97, 'train_rates': 0.821682737857474, 'learning_rate': 0.00014865076133669227, 'batch_size': 130, 'step_size': 11, 'gamma': 0.8633939785615272}. Best is trial 3 with value: 0.46258479795035196.[0m
[32m[I 2025-02-05 21:58:34,440][0m Trial 4 finished with value: 1.0870666775633306 and parameters: {'observation_period_num': 245, 'train_rates': 0.623234577679138, 'learning_rate': 6.597245558110257e-06, 'batch_size': 25, 'step_size': 14, 'gamma': 0.9779134081755927}. Best is trial 3 with value: 0.46258479795035196.[0m
[32m[I 2025-02-05 22:02:56,674][0m Trial 5 finished with value: 0.3702832312579174 and parameters: {'observation_period_num': 205, 'train_rates': 0.9088306503714199, 'learning_rate': 0.0005105627541310047, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8683474529776904}. Best is trial 5 with value: 0.3702832312579174.[0m
[32m[I 2025-02-05 22:07:43,859][0m Trial 6 finished with value: 1.8129346432623925 and parameters: {'observation_period_num': 230, 'train_rates': 0.9435449561026426, 'learning_rate': 2.593719495586126e-06, 'batch_size': 152, 'step_size': 8, 'gamma': 0.7543578998786344}. Best is trial 5 with value: 0.3702832312579174.[0m
[32m[I 2025-02-05 22:10:04,638][0m Trial 7 finished with value: 0.28670719245868903 and parameters: {'observation_period_num': 125, 'train_rates': 0.8951702154534353, 'learning_rate': 5.419241962848042e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.8558095483179979}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:14:56,545][0m Trial 8 finished with value: 0.7274450389622468 and parameters: {'observation_period_num': 60, 'train_rates': 0.9300223420848974, 'learning_rate': 3.1026351540901675e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8393927877907148}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:18:45,736][0m Trial 9 finished with value: 0.4630178955232673 and parameters: {'observation_period_num': 207, 'train_rates': 0.8159253387398009, 'learning_rate': 0.00015086430263103797, 'batch_size': 120, 'step_size': 15, 'gamma': 0.9516429953939364}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:19:39,862][0m Trial 10 finished with value: 1.331851100245776 and parameters: {'observation_period_num': 30, 'train_rates': 0.7164517835402235, 'learning_rate': 1.652000561818822e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9237189260451325}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:22:57,657][0m Trial 11 finished with value: 0.9000490202278387 and parameters: {'observation_period_num': 174, 'train_rates': 0.8902980896215409, 'learning_rate': 0.0007560600370845024, 'batch_size': 79, 'step_size': 11, 'gamma': 0.8163216013911327}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:24:59,483][0m Trial 12 finished with value: 0.5065979186952467 and parameters: {'observation_period_num': 115, 'train_rates': 0.8810680327217671, 'learning_rate': 3.493071837330791e-05, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8885197565503591}. Best is trial 7 with value: 0.28670719245868903.[0m
[32m[I 2025-02-05 22:29:02,131][0m Trial 13 finished with value: 0.17932695150375366 and parameters: {'observation_period_num': 192, 'train_rates': 0.9857063006108758, 'learning_rate': 0.0002809112639014916, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8520785489203017}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:32:39,414][0m Trial 14 finished with value: 0.7160312533378601 and parameters: {'observation_period_num': 180, 'train_rates': 0.9835420395645936, 'learning_rate': 5.2861902982025145e-05, 'batch_size': 101, 'step_size': 2, 'gamma': 0.7948427202946917}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:34:48,062][0m Trial 15 finished with value: 0.7774242551541636 and parameters: {'observation_period_num': 132, 'train_rates': 0.7522114659677884, 'learning_rate': 0.0002678862359359585, 'batch_size': 251, 'step_size': 11, 'gamma': 0.8426984645791382}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:35:16,303][0m Trial 16 finished with value: 0.49637725798842997 and parameters: {'observation_period_num': 12, 'train_rates': 0.8587601897812391, 'learning_rate': 9.238200934989176e-05, 'batch_size': 175, 'step_size': 3, 'gamma': 0.9168314278672108}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:39:19,734][0m Trial 17 finished with value: 0.4863670777125531 and parameters: {'observation_period_num': 195, 'train_rates': 0.9702264778205497, 'learning_rate': 1.536612417635963e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8430328128424949}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:40:31,624][0m Trial 18 finished with value: 0.9198885542666254 and parameters: {'observation_period_num': 70, 'train_rates': 0.7739258632475825, 'learning_rate': 1.6388933562691626e-05, 'batch_size': 91, 'step_size': 10, 'gamma': 0.8855289805862375}. Best is trial 13 with value: 0.17932695150375366.[0m
[32m[I 2025-02-05 22:43:37,105][0m Trial 19 finished with value: 0.15594980120658875 and parameters: {'observation_period_num': 151, 'train_rates': 0.9882491281031751, 'learning_rate': 9.590390879565091e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.7841266359410203}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 22:46:42,450][0m Trial 20 finished with value: 0.2696150541305542 and parameters: {'observation_period_num': 156, 'train_rates': 0.9800038508600358, 'learning_rate': 0.0003245377640336575, 'batch_size': 106, 'step_size': 4, 'gamma': 0.7828707138041419}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 22:49:53,601][0m Trial 21 finished with value: 0.2713126540184021 and parameters: {'observation_period_num': 158, 'train_rates': 0.9878479738331923, 'learning_rate': 0.00038287618294939385, 'batch_size': 105, 'step_size': 4, 'gamma': 0.7791577664228183}. Best is trial 19 with value: 0.15594980120658875.[0m
Early stopping at epoch 59
[32m[I 2025-02-05 22:51:53,761][0m Trial 22 finished with value: 0.964008497263857 and parameters: {'observation_period_num': 164, 'train_rates': 0.9401595546513656, 'learning_rate': 8.754236184664166e-05, 'batch_size': 55, 'step_size': 1, 'gamma': 0.8137869878239126}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 22:56:41,639][0m Trial 23 finished with value: 0.2969812811328675 and parameters: {'observation_period_num': 229, 'train_rates': 0.9279455180177575, 'learning_rate': 0.0003187088290148286, 'batch_size': 114, 'step_size': 6, 'gamma': 0.7795987328290621}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:00:27,953][0m Trial 24 finished with value: 0.46766647696495056 and parameters: {'observation_period_num': 188, 'train_rates': 0.9871463588538817, 'learning_rate': 0.0009539536494454312, 'batch_size': 175, 'step_size': 3, 'gamma': 0.7797088423041875}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:03:10,800][0m Trial 25 finished with value: 0.407640362741383 and parameters: {'observation_period_num': 149, 'train_rates': 0.8598868565521495, 'learning_rate': 0.00010512553106426709, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8305185412380258}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:05:17,948][0m Trial 26 finished with value: 0.31686671657010546 and parameters: {'observation_period_num': 106, 'train_rates': 0.9577821820018713, 'learning_rate': 0.00019750817832756676, 'batch_size': 50, 'step_size': 5, 'gamma': 0.7664455218249229}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:09:55,348][0m Trial 27 finished with value: 1.8764923064263312 and parameters: {'observation_period_num': 220, 'train_rates': 0.9335361887819327, 'learning_rate': 1.1327974607114116e-06, 'batch_size': 66, 'step_size': 6, 'gamma': 0.7941774212485865}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:13:36,557][0m Trial 28 finished with value: 0.4462139058164704 and parameters: {'observation_period_num': 178, 'train_rates': 0.9167541194284333, 'learning_rate': 0.00054235539127916, 'batch_size': 38, 'step_size': 3, 'gamma': 0.7990120163757229}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:16:10,078][0m Trial 29 finished with value: 1.1244469237108874 and parameters: {'observation_period_num': 151, 'train_rates': 0.674084743349407, 'learning_rate': 0.00023808779107293874, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7518331620930803}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:18:19,763][0m Trial 30 finished with value: 0.3798908768321878 and parameters: {'observation_period_num': 128, 'train_rates': 0.868300266518089, 'learning_rate': 0.0004163910630274706, 'batch_size': 97, 'step_size': 9, 'gamma': 0.8307985036666214}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:21:34,462][0m Trial 31 finished with value: 0.27510687708854675 and parameters: {'observation_period_num': 162, 'train_rates': 0.9878753507366143, 'learning_rate': 0.00031975548481757684, 'batch_size': 109, 'step_size': 4, 'gamma': 0.7764107365886083}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:24:18,657][0m Trial 32 finished with value: 0.5821077227592468 and parameters: {'observation_period_num': 142, 'train_rates': 0.9606136303927031, 'learning_rate': 0.00013585710959516608, 'batch_size': 145, 'step_size': 5, 'gamma': 0.7653022532001849}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:27:28,648][0m Trial 33 finished with value: 0.41928520798683167 and parameters: {'observation_period_num': 164, 'train_rates': 0.9631643933368367, 'learning_rate': 0.00040109576859433745, 'batch_size': 128, 'step_size': 4, 'gamma': 0.790641730779841}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:31:35,891][0m Trial 34 finished with value: 0.21935796737670898 and parameters: {'observation_period_num': 198, 'train_rates': 0.9865545826423672, 'learning_rate': 0.000205831789754011, 'batch_size': 158, 'step_size': 7, 'gamma': 0.8156834696666168}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:35:43,743][0m Trial 35 finished with value: 0.40144723653793335 and parameters: {'observation_period_num': 202, 'train_rates': 0.9555176572881053, 'learning_rate': 0.00017938620262002237, 'batch_size': 163, 'step_size': 7, 'gamma': 0.8183441321631392}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:40:05,981][0m Trial 36 finished with value: 0.46102125523041704 and parameters: {'observation_period_num': 218, 'train_rates': 0.9106786997273101, 'learning_rate': 6.559320581294802e-05, 'batch_size': 214, 'step_size': 9, 'gamma': 0.8583748735837156}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:43:41,257][0m Trial 37 finished with value: 0.7528355121612549 and parameters: {'observation_period_num': 183, 'train_rates': 0.969675080098997, 'learning_rate': 3.135215843210055e-05, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8048014447536441}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:47:19,979][0m Trial 38 finished with value: 0.6852007920190999 and parameters: {'observation_period_num': 194, 'train_rates': 0.8418999586148713, 'learning_rate': 0.0006659215921380814, 'batch_size': 137, 'step_size': 2, 'gamma': 0.8255965570341515}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:51:13,626][0m Trial 39 finished with value: 1.215783629631189 and parameters: {'observation_period_num': 216, 'train_rates': 0.6007419971047301, 'learning_rate': 0.00010527813647009391, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8847646090744157}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:53:12,729][0m Trial 40 finished with value: 0.2500848690920238 and parameters: {'observation_period_num': 94, 'train_rates': 0.9447548424645882, 'learning_rate': 0.00021100192311520984, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8485877241503557}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:55:12,607][0m Trial 41 finished with value: 0.24429426379973376 and parameters: {'observation_period_num': 85, 'train_rates': 0.9450091341275593, 'learning_rate': 0.00023875810667661532, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8499289217324796}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:57:07,360][0m Trial 42 finished with value: 0.2614885002374649 and parameters: {'observation_period_num': 79, 'train_rates': 0.9461833264552904, 'learning_rate': 0.0002014131860225269, 'batch_size': 43, 'step_size': 8, 'gamma': 0.8523580410718321}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-05 23:59:43,192][0m Trial 43 finished with value: 0.22529746483011945 and parameters: {'observation_period_num': 50, 'train_rates': 0.9011088234850262, 'learning_rate': 0.00013649530406119879, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8701803300296912}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:02:19,367][0m Trial 44 finished with value: 0.22807955936487043 and parameters: {'observation_period_num': 49, 'train_rates': 0.9078385917611878, 'learning_rate': 0.00012938579926938764, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8654535250016651}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:05:09,902][0m Trial 45 finished with value: 0.26984234950989955 and parameters: {'observation_period_num': 44, 'train_rates': 0.8910659549722589, 'learning_rate': 7.287404334568938e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8669021802605914}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:10:23,178][0m Trial 46 finished with value: 0.5948874268527534 and parameters: {'observation_period_num': 251, 'train_rates': 0.7933389132624915, 'learning_rate': 0.00013338115085135502, 'batch_size': 27, 'step_size': 15, 'gamma': 0.9008386791191954}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:11:40,822][0m Trial 47 finished with value: 0.28594639861440085 and parameters: {'observation_period_num': 33, 'train_rates': 0.9149679864643692, 'learning_rate': 3.7713999751692466e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8689822452809393}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:16:10,759][0m Trial 48 finished with value: 0.41142998176550716 and parameters: {'observation_period_num': 53, 'train_rates': 0.8356518933392565, 'learning_rate': 2.5136465600373236e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8795791778636342}. Best is trial 19 with value: 0.15594980120658875.[0m
[32m[I 2025-02-06 00:17:10,573][0m Trial 49 finished with value: 0.22829893442755894 and parameters: {'observation_period_num': 13, 'train_rates': 0.9041402165491882, 'learning_rate': 0.0001417195478232304, 'batch_size': 82, 'step_size': 14, 'gamma': 0.945900154704228}. Best is trial 19 with value: 0.15594980120658875.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-06 00:17:10,580][0m A new study created in memory with name: no-name-099172c6-1534-4f03-b090-ce74e22b3f59[0m
[32m[I 2025-02-06 00:18:01,587][0m Trial 0 finished with value: 0.4599565267562866 and parameters: {'observation_period_num': 48, 'train_rates': 0.9646701013518397, 'learning_rate': 8.12421396342725e-05, 'batch_size': 185, 'step_size': 8, 'gamma': 0.8223129224509477}. Best is trial 0 with value: 0.4599565267562866.[0m
Early stopping at epoch 92
[32m[I 2025-02-06 00:20:25,327][0m Trial 1 finished with value: 0.7583459139568731 and parameters: {'observation_period_num': 150, 'train_rates': 0.8174117323257756, 'learning_rate': 0.00021368791224339424, 'batch_size': 133, 'step_size': 2, 'gamma': 0.7544067445576592}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:24:46,830][0m Trial 2 finished with value: 1.3418254544658046 and parameters: {'observation_period_num': 189, 'train_rates': 0.9663866575632377, 'learning_rate': 1.238159506928082e-06, 'batch_size': 21, 'step_size': 2, 'gamma': 0.9664461100628882}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:25:38,759][0m Trial 3 finished with value: 1.8392773393857278 and parameters: {'observation_period_num': 57, 'train_rates': 0.796322317327946, 'learning_rate': 1.1196380324067835e-06, 'batch_size': 164, 'step_size': 11, 'gamma': 0.9283261600092545}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:29:39,798][0m Trial 4 finished with value: 1.0117562034273249 and parameters: {'observation_period_num': 235, 'train_rates': 0.6539712777707578, 'learning_rate': 2.0561084581997614e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9448359535258292}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:33:44,710][0m Trial 5 finished with value: 0.7948644514811241 and parameters: {'observation_period_num': 227, 'train_rates': 0.7620685004200919, 'learning_rate': 0.00018399031318918512, 'batch_size': 143, 'step_size': 3, 'gamma': 0.8987966234996285}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:34:38,641][0m Trial 6 finished with value: 1.26610643092654 and parameters: {'observation_period_num': 16, 'train_rates': 0.7833775119985305, 'learning_rate': 1.740798803161778e-05, 'batch_size': 80, 'step_size': 3, 'gamma': 0.7938523207214161}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:37:56,848][0m Trial 7 finished with value: 0.5006316487605755 and parameters: {'observation_period_num': 190, 'train_rates': 0.7932210856345878, 'learning_rate': 0.0003251309040610699, 'batch_size': 128, 'step_size': 6, 'gamma': 0.8444441864901767}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:39:33,143][0m Trial 8 finished with value: 0.6522133350372314 and parameters: {'observation_period_num': 89, 'train_rates': 0.9739950035152999, 'learning_rate': 1.4178543292930177e-05, 'batch_size': 147, 'step_size': 15, 'gamma': 0.9132130794807678}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:41:49,828][0m Trial 9 finished with value: 0.5633387639631633 and parameters: {'observation_period_num': 122, 'train_rates': 0.838122059679625, 'learning_rate': 0.00038332303112508436, 'batch_size': 42, 'step_size': 14, 'gamma': 0.7664020139584116}. Best is trial 0 with value: 0.4599565267562866.[0m
[32m[I 2025-02-06 00:42:11,313][0m Trial 10 finished with value: 0.3732478604154679 and parameters: {'observation_period_num': 9, 'train_rates': 0.8952752576272998, 'learning_rate': 7.211483247313187e-05, 'batch_size': 249, 'step_size': 10, 'gamma': 0.8440401546902642}. Best is trial 10 with value: 0.3732478604154679.[0m
[32m[I 2025-02-06 00:42:34,360][0m Trial 11 finished with value: 0.375244258448135 and parameters: {'observation_period_num': 9, 'train_rates': 0.8977993689384403, 'learning_rate': 6.967903255458816e-05, 'batch_size': 238, 'step_size': 10, 'gamma': 0.8381057101274896}. Best is trial 10 with value: 0.3732478604154679.[0m
[32m[I 2025-02-06 00:42:55,867][0m Trial 12 finished with value: 0.36260117932982827 and parameters: {'observation_period_num': 11, 'train_rates': 0.8816897666979407, 'learning_rate': 7.101586533781815e-05, 'batch_size': 250, 'step_size': 11, 'gamma': 0.8665675588666113}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:43:46,001][0m Trial 13 finished with value: 0.4408575478312257 and parameters: {'observation_period_num': 52, 'train_rates': 0.8938425653352099, 'learning_rate': 5.5766765257086975e-05, 'batch_size': 254, 'step_size': 12, 'gamma': 0.8890908115045194}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:45:23,421][0m Trial 14 finished with value: 1.1690451659080459 and parameters: {'observation_period_num': 98, 'train_rates': 0.8852051891438827, 'learning_rate': 5.265649916344393e-06, 'batch_size': 217, 'step_size': 10, 'gamma': 0.8638596968853446}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:45:45,608][0m Trial 15 finished with value: 0.9835826950889706 and parameters: {'observation_period_num': 9, 'train_rates': 0.7142374122089029, 'learning_rate': 0.0009453027414563814, 'batch_size': 208, 'step_size': 13, 'gamma': 0.8068106403906732}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:46:25,516][0m Trial 16 finished with value: 1.1619430482387543 and parameters: {'observation_period_num': 41, 'train_rates': 0.8668375815531844, 'learning_rate': 6.59589944299374e-06, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8669049680292007}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:47:49,442][0m Trial 17 finished with value: 0.6814970374107361 and parameters: {'observation_period_num': 85, 'train_rates': 0.9413819725230541, 'learning_rate': 4.3340720379971714e-05, 'batch_size': 251, 'step_size': 10, 'gamma': 0.7876424211136275}. Best is trial 12 with value: 0.36260117932982827.[0m
[32m[I 2025-02-06 00:48:38,845][0m Trial 18 finished with value: 0.2626199119820677 and parameters: {'observation_period_num': 33, 'train_rates': 0.9205682071612873, 'learning_rate': 0.00012251516173579516, 'batch_size': 102, 'step_size': 8, 'gamma': 0.880915593393237}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:51:01,642][0m Trial 19 finished with value: 0.451857646512244 and parameters: {'observation_period_num': 129, 'train_rates': 0.9319464790584414, 'learning_rate': 0.0006038992216531177, 'batch_size': 100, 'step_size': 5, 'gamma': 0.9818017245573479}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:52:17,591][0m Trial 20 finished with value: 0.3778906682053128 and parameters: {'observation_period_num': 68, 'train_rates': 0.8461832577830077, 'learning_rate': 0.00013536083569533198, 'batch_size': 84, 'step_size': 8, 'gamma': 0.8935150848562824}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:52:53,869][0m Trial 21 finished with value: 0.31951590588218287 and parameters: {'observation_period_num': 31, 'train_rates': 0.9222181705964021, 'learning_rate': 0.00012173648288496966, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8501834442438202}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:53:32,207][0m Trial 22 finished with value: 0.28450463539329024 and parameters: {'observation_period_num': 33, 'train_rates': 0.9380274664928736, 'learning_rate': 0.0001452156975273703, 'batch_size': 169, 'step_size': 8, 'gamma': 0.8743318259925724}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:54:10,953][0m Trial 23 finished with value: 0.3198880382298213 and parameters: {'observation_period_num': 35, 'train_rates': 0.9326542012956222, 'learning_rate': 0.00013590181395159922, 'batch_size': 191, 'step_size': 7, 'gamma': 0.8829559727200355}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:54:57,040][0m Trial 24 finished with value: 0.270825367082249 and parameters: {'observation_period_num': 32, 'train_rates': 0.9247616186859082, 'learning_rate': 0.0003416643889889914, 'batch_size': 112, 'step_size': 9, 'gamma': 0.9180222352638482}. Best is trial 18 with value: 0.2626199119820677.[0m
[32m[I 2025-02-06 00:56:21,674][0m Trial 25 finished with value: 0.24899427592754364 and parameters: {'observation_period_num': 74, 'train_rates': 0.9807422946604004, 'learning_rate': 0.0003062034782574727, 'batch_size': 111, 'step_size': 6, 'gamma': 0.9244215923058205}. Best is trial 25 with value: 0.24899427592754364.[0m
[32m[I 2025-02-06 00:57:40,315][0m Trial 26 finished with value: 0.22463667392730713 and parameters: {'observation_period_num': 71, 'train_rates': 0.9895234396824725, 'learning_rate': 0.0003389297011722769, 'batch_size': 110, 'step_size': 5, 'gamma': 0.9435696578049092}. Best is trial 26 with value: 0.22463667392730713.[0m
[32m[I 2025-02-06 00:59:02,012][0m Trial 27 finished with value: 0.6033167243003845 and parameters: {'observation_period_num': 73, 'train_rates': 0.9892082223699284, 'learning_rate': 0.000945247231168841, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9449441399319238}. Best is trial 26 with value: 0.22463667392730713.[0m
[32m[I 2025-02-06 01:01:05,981][0m Trial 28 finished with value: 0.5123578035137029 and parameters: {'observation_period_num': 106, 'train_rates': 0.9640042977735306, 'learning_rate': 0.0005268834684857728, 'batch_size': 62, 'step_size': 6, 'gamma': 0.9449869108927283}. Best is trial 26 with value: 0.22463667392730713.[0m
[32m[I 2025-02-06 01:02:28,501][0m Trial 29 finished with value: 0.197734072804451 and parameters: {'observation_period_num': 70, 'train_rates': 0.9851837242349233, 'learning_rate': 0.0002771413247151722, 'batch_size': 94, 'step_size': 1, 'gamma': 0.9653340693851606}. Best is trial 29 with value: 0.197734072804451.[0m
[32m[I 2025-02-06 01:04:08,958][0m Trial 30 finished with value: 1.012272000525679 and parameters: {'observation_period_num': 116, 'train_rates': 0.6301105371535992, 'learning_rate': 0.000239169094857848, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9832381190669744}. Best is trial 29 with value: 0.197734072804451.[0m
[32m[I 2025-02-06 01:05:27,377][0m Trial 31 finished with value: 0.41628697514533997 and parameters: {'observation_period_num': 68, 'train_rates': 0.9799548947210474, 'learning_rate': 0.0005257259943175128, 'batch_size': 114, 'step_size': 5, 'gamma': 0.9603711326050769}. Best is trial 29 with value: 0.197734072804451.[0m
[32m[I 2025-02-06 01:08:18,781][0m Trial 32 finished with value: 0.44749848827205857 and parameters: {'observation_period_num': 147, 'train_rates': 0.9587503245756084, 'learning_rate': 0.00024837347050543494, 'batch_size': 97, 'step_size': 1, 'gamma': 0.9302200069763991}. Best is trial 29 with value: 0.197734072804451.[0m
[32m[I 2025-02-06 01:09:19,019][0m Trial 33 finished with value: 0.29921291241279013 and parameters: {'observation_period_num': 57, 'train_rates': 0.9554089164785547, 'learning_rate': 0.00010342228368071018, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9587554916776353}. Best is trial 29 with value: 0.197734072804451.[0m
[32m[I 2025-02-06 01:10:57,910][0m Trial 34 finished with value: 0.13968926668167114 and parameters: {'observation_period_num': 77, 'train_rates': 0.9899926089026055, 'learning_rate': 0.0002075764683800737, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9080082345919008}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:14:02,585][0m Trial 35 finished with value: 0.986516284254881 and parameters: {'observation_period_num': 86, 'train_rates': 0.9819195246311928, 'learning_rate': 0.0006546491905978486, 'batch_size': 27, 'step_size': 4, 'gamma': 0.9120811996506873}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:16:18,812][0m Trial 36 finished with value: 0.9300907135799231 and parameters: {'observation_period_num': 137, 'train_rates': 0.7320874127085186, 'learning_rate': 0.0002529827297894659, 'batch_size': 60, 'step_size': 6, 'gamma': 0.933391816305134}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:18:22,955][0m Trial 37 finished with value: 0.3276145014881103 and parameters: {'observation_period_num': 103, 'train_rates': 0.9578494467612687, 'learning_rate': 3.171530681345344e-05, 'batch_size': 49, 'step_size': 2, 'gamma': 0.9662677393465507}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:19:44,791][0m Trial 38 finished with value: 0.197431281208992 and parameters: {'observation_period_num': 75, 'train_rates': 0.9881342343755131, 'learning_rate': 0.00037656394529537784, 'batch_size': 153, 'step_size': 7, 'gamma': 0.9047613451567428}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:22:54,340][0m Trial 39 finished with value: 0.1903817355632782 and parameters: {'observation_period_num': 161, 'train_rates': 0.9883817392895705, 'learning_rate': 0.0004229906892382922, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9042177633014248}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:26:40,482][0m Trial 40 finished with value: 0.25753477215766907 and parameters: {'observation_period_num': 191, 'train_rates': 0.9533367261559715, 'learning_rate': 0.00045181554086104264, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9032600519070703}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:29:44,804][0m Trial 41 finished with value: 0.18819649517536163 and parameters: {'observation_period_num': 157, 'train_rates': 0.9849775155249545, 'learning_rate': 0.0001809119853147693, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9401084271043779}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:33:04,397][0m Trial 42 finished with value: 0.48091503977775574 and parameters: {'observation_period_num': 171, 'train_rates': 0.9675616315178538, 'learning_rate': 0.0007316052079169947, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9028438035234314}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:36:14,511][0m Trial 43 finished with value: 0.2847386125460673 and parameters: {'observation_period_num': 170, 'train_rates': 0.9078917941662059, 'learning_rate': 0.00018662542054327687, 'batch_size': 172, 'step_size': 9, 'gamma': 0.975261598024102}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:38:44,936][0m Trial 44 finished with value: 0.8017404130580352 and parameters: {'observation_period_num': 161, 'train_rates': 0.685951689968292, 'learning_rate': 0.00018370769318052757, 'batch_size': 135, 'step_size': 4, 'gamma': 0.9549456694294869}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:43:05,714][0m Trial 45 finished with value: 1.7470448017120361 and parameters: {'observation_period_num': 210, 'train_rates': 0.9494261005444038, 'learning_rate': 1.773237117893859e-06, 'batch_size': 154, 'step_size': 7, 'gamma': 0.9083864637265388}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:45:53,573][0m Trial 46 finished with value: 0.28209686279296875 and parameters: {'observation_period_num': 146, 'train_rates': 0.9707311100377122, 'learning_rate': 0.00040479652379008267, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9343497485391214}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:50:43,822][0m Trial 47 finished with value: 0.5009562410486554 and parameters: {'observation_period_num': 252, 'train_rates': 0.8096011873632345, 'learning_rate': 9.13224023612816e-05, 'batch_size': 139, 'step_size': 7, 'gamma': 0.9203964873014051}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:52:43,111][0m Trial 48 finished with value: 0.26445493985541724 and parameters: {'observation_period_num': 114, 'train_rates': 0.9109504433848087, 'learning_rate': 0.00018800533934959702, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8934447914688316}. Best is trial 34 with value: 0.13968926668167114.[0m
[32m[I 2025-02-06 01:56:04,726][0m Trial 49 finished with value: 0.2893812533219655 and parameters: {'observation_period_num': 158, 'train_rates': 0.946387050301209, 'learning_rate': 5.1503968042528805e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9722001082017684}. Best is trial 34 with value: 0.13968926668167114.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-06 01:56:04,734][0m A new study created in memory with name: no-name-8f72311f-c4d9-4b34-a26f-a3ebaf8fd48d[0m
[32m[I 2025-02-06 02:00:08,303][0m Trial 0 finished with value: 2.2230717650109515 and parameters: {'observation_period_num': 236, 'train_rates': 0.71390129282612, 'learning_rate': 1.2371836926056094e-06, 'batch_size': 231, 'step_size': 15, 'gamma': 0.8456232116819888}. Best is trial 0 with value: 2.2230717650109515.[0m
[32m[I 2025-02-06 02:01:21,551][0m Trial 1 finished with value: 0.9769235253334045 and parameters: {'observation_period_num': 68, 'train_rates': 0.9639855526094494, 'learning_rate': 8.54327085710373e-06, 'batch_size': 199, 'step_size': 8, 'gamma': 0.9596172285288069}. Best is trial 1 with value: 0.9769235253334045.[0m
[32m[I 2025-02-06 02:05:04,199][0m Trial 2 finished with value: 1.7761035362879436 and parameters: {'observation_period_num': 231, 'train_rates': 0.6760014118578865, 'learning_rate': 4.488323699260412e-06, 'batch_size': 182, 'step_size': 4, 'gamma': 0.8804121917234352}. Best is trial 1 with value: 0.9769235253334045.[0m
[32m[I 2025-02-06 02:10:29,147][0m Trial 3 finished with value: 1.0974403619766235 and parameters: {'observation_period_num': 249, 'train_rates': 0.9711253492557002, 'learning_rate': 3.905270813465047e-06, 'batch_size': 167, 'step_size': 14, 'gamma': 0.9030394781577415}. Best is trial 1 with value: 0.9769235253334045.[0m
[32m[I 2025-02-06 02:11:45,467][0m Trial 4 finished with value: 2.011711597442627 and parameters: {'observation_period_num': 70, 'train_rates': 0.9679014217247581, 'learning_rate': 1.3555927268012318e-06, 'batch_size': 213, 'step_size': 3, 'gamma': 0.9651195246440969}. Best is trial 1 with value: 0.9769235253334045.[0m
Early stopping at epoch 57
[32m[I 2025-02-06 02:13:26,454][0m Trial 5 finished with value: 1.6378704557713775 and parameters: {'observation_period_num': 166, 'train_rates': 0.8259887760967282, 'learning_rate': 4.4944284411444633e-05, 'batch_size': 176, 'step_size': 1, 'gamma': 0.8038042904892242}. Best is trial 1 with value: 0.9769235253334045.[0m
[32m[I 2025-02-06 02:16:01,395][0m Trial 6 finished with value: 0.5356607437133789 and parameters: {'observation_period_num': 143, 'train_rates': 0.9277161495081169, 'learning_rate': 0.0009089409621828858, 'batch_size': 217, 'step_size': 2, 'gamma': 0.8609213712662235}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:17:30,000][0m Trial 7 finished with value: 1.4881368768278254 and parameters: {'observation_period_num': 96, 'train_rates': 0.8016885285144559, 'learning_rate': 1.1622231404604608e-05, 'batch_size': 222, 'step_size': 3, 'gamma': 0.787966311523893}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:21:51,382][0m Trial 8 finished with value: 0.8399938163110765 and parameters: {'observation_period_num': 234, 'train_rates': 0.739836703636669, 'learning_rate': 0.00012221074327278562, 'batch_size': 48, 'step_size': 9, 'gamma': 0.8975136360328089}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:25:46,442][0m Trial 9 finished with value: 1.4298946942136588 and parameters: {'observation_period_num': 241, 'train_rates': 0.6626667628268874, 'learning_rate': 1.0129555052338158e-05, 'batch_size': 155, 'step_size': 13, 'gamma': 0.826118612418823}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:26:39,263][0m Trial 10 finished with value: 0.8013996379716056 and parameters: {'observation_period_num': 12, 'train_rates': 0.8812683722811331, 'learning_rate': 0.0009006969497733249, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9310759762498871}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:27:32,052][0m Trial 11 finished with value: 0.8616810768842698 and parameters: {'observation_period_num': 17, 'train_rates': 0.884376230690278, 'learning_rate': 0.000838951329177378, 'batch_size': 91, 'step_size': 7, 'gamma': 0.7502078596021685}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:30:20,207][0m Trial 12 finished with value: 1.0893018538834618 and parameters: {'observation_period_num': 154, 'train_rates': 0.8694524555629198, 'learning_rate': 0.0009090502595609154, 'batch_size': 116, 'step_size': 11, 'gamma': 0.9337131601769687}. Best is trial 6 with value: 0.5356607437133789.[0m
[32m[I 2025-02-06 02:34:12,104][0m Trial 13 finished with value: 0.29370226673905636 and parameters: {'observation_period_num': 8, 'train_rates': 0.898020224512687, 'learning_rate': 0.00021901545519686152, 'batch_size': 20, 'step_size': 6, 'gamma': 0.9173241499968371}. Best is trial 13 with value: 0.29370226673905636.[0m
[32m[I 2025-02-06 02:38:39,735][0m Trial 14 finished with value: 0.2719235413424347 and parameters: {'observation_period_num': 178, 'train_rates': 0.9174301987746548, 'learning_rate': 0.00022145963503955468, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8579751362349658}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:42:53,913][0m Trial 15 finished with value: 0.455872006714344 and parameters: {'observation_period_num': 190, 'train_rates': 0.915297079352587, 'learning_rate': 0.00020523155020812738, 'batch_size': 25, 'step_size': 5, 'gamma': 0.9881992359256518}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:44:54,225][0m Trial 16 finished with value: 0.4442828915833905 and parameters: {'observation_period_num': 110, 'train_rates': 0.8392536829398347, 'learning_rate': 0.0002233435656986312, 'batch_size': 52, 'step_size': 5, 'gamma': 0.9103066069165187}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:48:44,894][0m Trial 17 finished with value: 0.735906038681666 and parameters: {'observation_period_num': 190, 'train_rates': 0.7721326031579122, 'learning_rate': 4.4578019264143936e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.8642979164706011}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:50:03,993][0m Trial 18 finished with value: 0.2837970052483821 and parameters: {'observation_period_num': 43, 'train_rates': 0.9213864602953106, 'learning_rate': 0.00010890564747705047, 'batch_size': 62, 'step_size': 6, 'gamma': 0.8342389227317818}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:50:57,775][0m Trial 19 finished with value: 0.9924799694552165 and parameters: {'observation_period_num': 47, 'train_rates': 0.6028053924728434, 'learning_rate': 2.5938693243186605e-05, 'batch_size': 68, 'step_size': 12, 'gamma': 0.8279676009433367}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:52:56,017][0m Trial 20 finished with value: 0.385039654390588 and parameters: {'observation_period_num': 111, 'train_rates': 0.9362902522365574, 'learning_rate': 8.647846604459371e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.7761188264030089}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:57:26,493][0m Trial 21 finished with value: 0.3211647769560417 and parameters: {'observation_period_num': 38, 'train_rates': 0.8973074073233351, 'learning_rate': 0.0003627075223150002, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8367976072118315}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 02:59:09,145][0m Trial 22 finished with value: 0.5232802054313855 and parameters: {'observation_period_num': 36, 'train_rates': 0.8534713618327907, 'learning_rate': 0.0003979621203906966, 'batch_size': 44, 'step_size': 6, 'gamma': 0.8046598566937448}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 03:02:57,029][0m Trial 23 finished with value: 0.3647152992586295 and parameters: {'observation_period_num': 191, 'train_rates': 0.9305188884479565, 'learning_rate': 8.028238719628941e-05, 'batch_size': 79, 'step_size': 4, 'gamma': 0.8833559501875922}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 03:04:51,107][0m Trial 24 finished with value: 0.47521166355760247 and parameters: {'observation_period_num': 63, 'train_rates': 0.8079515239851702, 'learning_rate': 0.00015898976343060574, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9257016647940011}. Best is trial 14 with value: 0.2719235413424347.[0m
[32m[I 2025-02-06 03:06:11,681][0m Trial 25 finished with value: 0.21673224866390228 and parameters: {'observation_period_num': 9, 'train_rates': 0.9800509512358966, 'learning_rate': 0.00042102361610359845, 'batch_size': 64, 'step_size': 5, 'gamma': 0.8531655467543757}. Best is trial 25 with value: 0.21673224866390228.[0m
[32m[I 2025-02-06 03:07:58,142][0m Trial 26 finished with value: 0.3702811598777771 and parameters: {'observation_period_num': 89, 'train_rates': 0.9820674926534997, 'learning_rate': 0.000419836209583099, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8524869544904298}. Best is trial 25 with value: 0.21673224866390228.[0m
Early stopping at epoch 60
[32m[I 2025-02-06 03:10:39,585][0m Trial 27 finished with value: 0.9067711659840175 and parameters: {'observation_period_num': 213, 'train_rates': 0.9489758806396984, 'learning_rate': 9.513823931997441e-05, 'batch_size': 104, 'step_size': 1, 'gamma': 0.8131772268729219}. Best is trial 25 with value: 0.21673224866390228.[0m
[32m[I 2025-02-06 03:12:55,116][0m Trial 28 finished with value: 0.19247746467590332 and parameters: {'observation_period_num': 128, 'train_rates': 0.9893404874412334, 'learning_rate': 0.0004751800926419106, 'batch_size': 248, 'step_size': 5, 'gamma': 0.8749026207470476}. Best is trial 28 with value: 0.19247746467590332.[0m
[32m[I 2025-02-06 03:15:18,377][0m Trial 29 finished with value: 0.20518375933170319 and parameters: {'observation_period_num': 129, 'train_rates': 0.9886612995665063, 'learning_rate': 0.0005189540546223071, 'batch_size': 246, 'step_size': 3, 'gamma': 0.8783009103902514}. Best is trial 28 with value: 0.19247746467590332.[0m
[32m[I 2025-02-06 03:17:35,504][0m Trial 30 finished with value: 0.31679025292396545 and parameters: {'observation_period_num': 127, 'train_rates': 0.9874748370881881, 'learning_rate': 0.0005151048214789884, 'batch_size': 250, 'step_size': 2, 'gamma': 0.8849370058904787}. Best is trial 28 with value: 0.19247746467590332.[0m
[32m[I 2025-02-06 03:20:53,782][0m Trial 31 finished with value: 0.4391777813434601 and parameters: {'observation_period_num': 171, 'train_rates': 0.9559129464894366, 'learning_rate': 0.00028804319036964933, 'batch_size': 255, 'step_size': 3, 'gamma': 0.8519150908647706}. Best is trial 28 with value: 0.19247746467590332.[0m
[32m[I 2025-02-06 03:23:25,816][0m Trial 32 finished with value: 0.18678565323352814 and parameters: {'observation_period_num': 132, 'train_rates': 0.9824556206173333, 'learning_rate': 0.0005793586752285613, 'batch_size': 244, 'step_size': 5, 'gamma': 0.8744941284357831}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:25:35,811][0m Trial 33 finished with value: 0.21643908321857452 and parameters: {'observation_period_num': 123, 'train_rates': 0.9899041684971057, 'learning_rate': 0.0005406409296998063, 'batch_size': 238, 'step_size': 4, 'gamma': 0.8797005001479073}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:28:07,541][0m Trial 34 finished with value: 0.31358447670936584 and parameters: {'observation_period_num': 132, 'train_rates': 0.9890006712774715, 'learning_rate': 0.0005749597602449576, 'batch_size': 238, 'step_size': 2, 'gamma': 0.8774350848139911}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:30:10,934][0m Trial 35 finished with value: 0.282370924949646 and parameters: {'observation_period_num': 121, 'train_rates': 0.9534754941412314, 'learning_rate': 0.0006285638123483322, 'batch_size': 197, 'step_size': 4, 'gamma': 0.8963626471423697}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:32:48,035][0m Trial 36 finished with value: 0.23568537831306458 and parameters: {'observation_period_num': 143, 'train_rates': 0.958848244952217, 'learning_rate': 0.00028046737791285017, 'batch_size': 237, 'step_size': 15, 'gamma': 0.8739625097741617}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:34:23,075][0m Trial 37 finished with value: 2.388261318206787 and parameters: {'observation_period_num': 89, 'train_rates': 0.9657438715116408, 'learning_rate': 1.013859840687523e-06, 'batch_size': 198, 'step_size': 3, 'gamma': 0.8941023091294532}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:36:10,013][0m Trial 38 finished with value: 0.28341439366340637 and parameters: {'observation_period_num': 104, 'train_rates': 0.9439043581481629, 'learning_rate': 0.000651574964026212, 'batch_size': 227, 'step_size': 8, 'gamma': 0.907390193927613}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:38:51,489][0m Trial 39 finished with value: 2.4223480224609375 and parameters: {'observation_period_num': 140, 'train_rates': 0.9717968633921428, 'learning_rate': 2.380600554678394e-06, 'batch_size': 211, 'step_size': 1, 'gamma': 0.9429828083140385}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:41:15,231][0m Trial 40 finished with value: 1.1928920716009987 and parameters: {'observation_period_num': 152, 'train_rates': 0.7149257846479637, 'learning_rate': 5.1934801783639835e-05, 'batch_size': 245, 'step_size': 4, 'gamma': 0.8424231491862207}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:42:35,701][0m Trial 41 finished with value: 0.19451165199279785 and parameters: {'observation_period_num': 72, 'train_rates': 0.9870030252999067, 'learning_rate': 0.0003760253129547467, 'batch_size': 149, 'step_size': 5, 'gamma': 0.8687999952146467}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:44:07,216][0m Trial 42 finished with value: 0.3588637113571167 and parameters: {'observation_period_num': 84, 'train_rates': 0.9684706277834922, 'learning_rate': 0.00031756212196387505, 'batch_size': 154, 'step_size': 3, 'gamma': 0.8718676877276552}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:45:22,225][0m Trial 43 finished with value: 1.2047111988067627 and parameters: {'observation_period_num': 73, 'train_rates': 0.9411135709005475, 'learning_rate': 1.9801608685810833e-05, 'batch_size': 210, 'step_size': 2, 'gamma': 0.8915253554368903}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:48:31,943][0m Trial 44 finished with value: 0.2994159758090973 and parameters: {'observation_period_num': 161, 'train_rates': 0.988954599551923, 'learning_rate': 0.0001578391839809998, 'batch_size': 188, 'step_size': 4, 'gamma': 0.8666720840458149}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:50:50,691][0m Trial 45 finished with value: 0.2672482952770037 and parameters: {'observation_period_num': 129, 'train_rates': 0.9105483373633764, 'learning_rate': 0.0006962611836529595, 'batch_size': 233, 'step_size': 5, 'gamma': 0.8845054811009889}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:52:50,922][0m Trial 46 finished with value: 0.26455140113830566 and parameters: {'observation_period_num': 114, 'train_rates': 0.9520513356412217, 'learning_rate': 0.0005674688519529033, 'batch_size': 170, 'step_size': 7, 'gamma': 0.916559472060593}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:54:36,824][0m Trial 47 finished with value: 0.45880815386772156 and parameters: {'observation_period_num': 100, 'train_rates': 0.9645767322003798, 'learning_rate': 0.000988544917921883, 'batch_size': 221, 'step_size': 8, 'gamma': 0.8674483269588797}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:57:09,382][0m Trial 48 finished with value: 1.5523186863876703 and parameters: {'observation_period_num': 154, 'train_rates': 0.7732118393944005, 'learning_rate': 6.126796904485638e-06, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9006625824727056}. Best is trial 32 with value: 0.18678565323352814.[0m
[32m[I 2025-02-06 03:58:02,176][0m Trial 49 finished with value: 0.3956204171364124 and parameters: {'observation_period_num': 57, 'train_rates': 0.9013234786218799, 'learning_rate': 0.0001437228949234284, 'batch_size': 242, 'step_size': 3, 'gamma': 0.9473665845867624}. Best is trial 32 with value: 0.18678565323352814.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-06 03:58:02,183][0m A new study created in memory with name: no-name-5426b03a-48a7-408c-88ed-c1c96f7077b7[0m
[32m[I 2025-02-06 04:00:49,943][0m Trial 0 finished with value: 1.0722678521091176 and parameters: {'observation_period_num': 170, 'train_rates': 0.7213292110003582, 'learning_rate': 0.0003070516082837062, 'batch_size': 103, 'step_size': 6, 'gamma': 0.9793638127238266}. Best is trial 0 with value: 1.0722678521091176.[0m
[32m[I 2025-02-06 04:02:44,017][0m Trial 1 finished with value: 1.586334778297828 and parameters: {'observation_period_num': 131, 'train_rates': 0.6786091400367141, 'learning_rate': 1.2576748311321043e-05, 'batch_size': 241, 'step_size': 4, 'gamma': 0.7922981740570644}. Best is trial 0 with value: 1.0722678521091176.[0m
[32m[I 2025-02-06 04:06:26,656][0m Trial 2 finished with value: 0.25737671850170296 and parameters: {'observation_period_num': 197, 'train_rates': 0.888781624116154, 'learning_rate': 0.00034627366714029485, 'batch_size': 152, 'step_size': 4, 'gamma': 0.9320195671944969}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:10:04,954][0m Trial 3 finished with value: 0.956537133166867 and parameters: {'observation_period_num': 42, 'train_rates': 0.8937340468904527, 'learning_rate': 0.0005772318406522567, 'batch_size': 21, 'step_size': 2, 'gamma': 0.8231340116289472}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:12:11,827][0m Trial 4 finished with value: 0.9703445305695405 and parameters: {'observation_period_num': 132, 'train_rates': 0.750833557351575, 'learning_rate': 4.887976694618223e-05, 'batch_size': 228, 'step_size': 9, 'gamma': 0.7550500002317314}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:16:51,257][0m Trial 5 finished with value: 0.6541449204203129 and parameters: {'observation_period_num': 226, 'train_rates': 0.7644093658685882, 'learning_rate': 1.2066623951682521e-05, 'batch_size': 22, 'step_size': 14, 'gamma': 0.9368217646800392}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:21:34,887][0m Trial 6 finished with value: 0.6423361079189284 and parameters: {'observation_period_num': 237, 'train_rates': 0.8291183989480717, 'learning_rate': 9.744888708577649e-06, 'batch_size': 51, 'step_size': 4, 'gamma': 0.9484455690780951}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:23:30,646][0m Trial 7 finished with value: 1.3593268003242818 and parameters: {'observation_period_num': 117, 'train_rates': 0.9089460901323663, 'learning_rate': 1.7519747539247327e-06, 'batch_size': 200, 'step_size': 15, 'gamma': 0.887136887235737}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:25:37,719][0m Trial 8 finished with value: 2.034555211403963 and parameters: {'observation_period_num': 144, 'train_rates': 0.6521899012804395, 'learning_rate': 2.5493390320667615e-06, 'batch_size': 178, 'step_size': 9, 'gamma': 0.7937990665592127}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:29:43,588][0m Trial 9 finished with value: 1.5051435232162476 and parameters: {'observation_period_num': 203, 'train_rates': 0.9652600111769484, 'learning_rate': 2.967341880073396e-06, 'batch_size': 157, 'step_size': 12, 'gamma': 0.7619453000207114}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:30:35,637][0m Trial 10 finished with value: 0.498186893761158 and parameters: {'observation_period_num': 48, 'train_rates': 0.8526757318841194, 'learning_rate': 0.00012337722406206812, 'batch_size': 107, 'step_size': 2, 'gamma': 0.8873621999220834}. Best is trial 2 with value: 0.25737671850170296.[0m
Early stopping at epoch 98
[32m[I 2025-02-06 04:31:17,922][0m Trial 11 finished with value: 0.6267767824777742 and parameters: {'observation_period_num': 5, 'train_rates': 0.8472921602856545, 'learning_rate': 0.00013839165232014215, 'batch_size': 110, 'step_size': 1, 'gamma': 0.8833307306659378}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:32:45,846][0m Trial 12 finished with value: 0.28902190923690796 and parameters: {'observation_period_num': 80, 'train_rates': 0.9708836151736661, 'learning_rate': 0.0001126253995145925, 'batch_size': 117, 'step_size': 4, 'gamma': 0.9182609451173162}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:34:16,552][0m Trial 13 finished with value: 0.4121740162372589 and parameters: {'observation_period_num': 84, 'train_rates': 0.9740190425526223, 'learning_rate': 0.000747919477434494, 'batch_size': 136, 'step_size': 6, 'gamma': 0.9346269754972905}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:37:51,902][0m Trial 14 finished with value: 0.26246814239662863 and parameters: {'observation_period_num': 184, 'train_rates': 0.9124900447800505, 'learning_rate': 6.608785613960536e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.9162987728935152}. Best is trial 2 with value: 0.25737671850170296.[0m
[32m[I 2025-02-06 04:41:45,309][0m Trial 15 finished with value: 0.23020139928685118 and parameters: {'observation_period_num': 194, 'train_rates': 0.8905486181388541, 'learning_rate': 0.00025757616784945913, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8501055873544299}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 04:45:30,085][0m Trial 16 finished with value: 0.5184342748602152 and parameters: {'observation_period_num': 200, 'train_rates': 0.8172817762792345, 'learning_rate': 0.00034214980016124437, 'batch_size': 71, 'step_size': 11, 'gamma': 0.845764747657141}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 04:50:35,175][0m Trial 17 finished with value: 0.33924997349580127 and parameters: {'observation_period_num': 252, 'train_rates': 0.8801673057308266, 'learning_rate': 0.0009907005772764937, 'batch_size': 180, 'step_size': 8, 'gamma': 0.847079578780678}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 04:53:50,646][0m Trial 18 finished with value: 0.2451355904340744 and parameters: {'observation_period_num': 164, 'train_rates': 0.9409279369832008, 'learning_rate': 0.00026483801371075664, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9827947157083234}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 04:57:06,895][0m Trial 19 finished with value: 0.2782363583059872 and parameters: {'observation_period_num': 165, 'train_rates': 0.9392317596094645, 'learning_rate': 2.8935442389418267e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9831834748752224}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 04:59:23,119][0m Trial 20 finished with value: 1.0820357679733765 and parameters: {'observation_period_num': 159, 'train_rates': 0.6001973103711928, 'learning_rate': 0.00016477967576488162, 'batch_size': 82, 'step_size': 8, 'gamma': 0.8533826887890631}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:03:13,262][0m Trial 21 finished with value: 0.23529273143102383 and parameters: {'observation_period_num': 195, 'train_rates': 0.937150859468059, 'learning_rate': 0.000369851583561085, 'batch_size': 143, 'step_size': 7, 'gamma': 0.9591099335615727}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:07:52,587][0m Trial 22 finished with value: 0.29398119643595094 and parameters: {'observation_period_num': 215, 'train_rates': 0.9383802748476578, 'learning_rate': 0.00023167442843207437, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9633194916339973}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:11:40,662][0m Trial 23 finished with value: 0.26807701587677 and parameters: {'observation_period_num': 185, 'train_rates': 0.9879341247090709, 'learning_rate': 0.0004860093088387107, 'batch_size': 88, 'step_size': 10, 'gamma': 0.963538963291601}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:13:42,169][0m Trial 24 finished with value: 0.27420227444587064 and parameters: {'observation_period_num': 114, 'train_rates': 0.9292895470688527, 'learning_rate': 7.150703592723245e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9880589387660527}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:16:48,430][0m Trial 25 finished with value: 0.3903636100652971 and parameters: {'observation_period_num': 154, 'train_rates': 0.8644444888422109, 'learning_rate': 0.0002130329639030362, 'batch_size': 37, 'step_size': 5, 'gamma': 0.9102575722871333}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:19:58,817][0m Trial 26 finished with value: 0.5393743299745605 and parameters: {'observation_period_num': 179, 'train_rates': 0.8014486546518164, 'learning_rate': 2.6243150876984384e-05, 'batch_size': 92, 'step_size': 9, 'gamma': 0.9632690473823124}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:24:48,053][0m Trial 27 finished with value: 0.2918117344379425 and parameters: {'observation_period_num': 229, 'train_rates': 0.9486777314158745, 'learning_rate': 0.0005852704040687893, 'batch_size': 157, 'step_size': 5, 'gamma': 0.8229677283907482}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:29:10,972][0m Trial 28 finished with value: 0.2399903393135621 and parameters: {'observation_period_num': 213, 'train_rates': 0.9051609573311443, 'learning_rate': 8.782689101393778e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8686211103904653}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:33:45,831][0m Trial 29 finished with value: 0.6224043005155622 and parameters: {'observation_period_num': 243, 'train_rates': 0.7838311047626753, 'learning_rate': 8.284026082800386e-05, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8597988951301655}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:38:14,601][0m Trial 30 finished with value: 0.4077467072622578 and parameters: {'observation_period_num': 216, 'train_rates': 0.9078478458873188, 'learning_rate': 4.2198732472937384e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8309234302463533}. Best is trial 15 with value: 0.23020139928685118.[0m
[32m[I 2025-02-06 05:41:36,715][0m Trial 31 finished with value: 0.22011316293164304 and parameters: {'observation_period_num': 169, 'train_rates': 0.9523882556174285, 'learning_rate': 0.0002685545654932206, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9007398640351604}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 05:45:43,161][0m Trial 32 finished with value: 0.3201451665498841 and parameters: {'observation_period_num': 210, 'train_rates': 0.8705419826630312, 'learning_rate': 0.0003821985251606352, 'batch_size': 97, 'step_size': 8, 'gamma': 0.8994236480106921}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 05:49:22,613][0m Trial 33 finished with value: 0.24063593621498772 and parameters: {'observation_period_num': 189, 'train_rates': 0.9192375808332948, 'learning_rate': 0.0002116940640344492, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8748010600706496}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 05:52:53,736][0m Trial 34 finished with value: 0.2577988129616576 and parameters: {'observation_period_num': 173, 'train_rates': 0.8918181968116842, 'learning_rate': 9.427460104000166e-05, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8044378875067949}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 05:56:44,504][0m Trial 35 finished with value: 0.3343141973018646 and parameters: {'observation_period_num': 195, 'train_rates': 0.9584826539983134, 'learning_rate': 0.0003691884822410815, 'batch_size': 128, 'step_size': 3, 'gamma': 0.8645722153832774}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 05:59:07,098][0m Trial 36 finished with value: 0.7366409649769566 and parameters: {'observation_period_num': 150, 'train_rates': 0.7213623637279943, 'learning_rate': 0.0001672278595777968, 'batch_size': 146, 'step_size': 10, 'gamma': 0.8339888240192221}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:03:20,384][0m Trial 37 finished with value: 0.46172107907118776 and parameters: {'observation_period_num': 223, 'train_rates': 0.8337382128738862, 'learning_rate': 0.000542328957004156, 'batch_size': 174, 'step_size': 6, 'gamma': 0.9069433501932629}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:05:36,018][0m Trial 38 finished with value: 0.5640384680346439 and parameters: {'observation_period_num': 131, 'train_rates': 0.8924311635037753, 'learning_rate': 4.919090917372584e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.8130552362479714}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:09:22,018][0m Trial 39 finished with value: 0.6327005391535552 and parameters: {'observation_period_num': 174, 'train_rates': 0.983716976270589, 'learning_rate': 0.0009374746163624975, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8733495982223592}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:11:57,292][0m Trial 40 finished with value: 1.2025534109555212 and parameters: {'observation_period_num': 142, 'train_rates': 0.9043192380135074, 'learning_rate': 6.094478597201703e-06, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9482268444248054}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:15:42,504][0m Trial 41 finished with value: 0.2366791319508123 and parameters: {'observation_period_num': 192, 'train_rates': 0.9236352563197859, 'learning_rate': 0.00020223849990975087, 'batch_size': 69, 'step_size': 6, 'gamma': 0.8793547398373562}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:20:39,010][0m Trial 42 finished with value: 0.25107508663620265 and parameters: {'observation_period_num': 204, 'train_rates': 0.9238977371515265, 'learning_rate': 0.0002883800320283166, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8936140937199234}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:25:36,907][0m Trial 43 finished with value: 0.23011217845810783 and parameters: {'observation_period_num': 233, 'train_rates': 0.9571963873207028, 'learning_rate': 0.0001789698755771699, 'batch_size': 65, 'step_size': 4, 'gamma': 0.8731628623504762}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:30:41,868][0m Trial 44 finished with value: 0.3002530504827914 and parameters: {'observation_period_num': 235, 'train_rates': 0.9578502870457681, 'learning_rate': 0.0004143880839472327, 'batch_size': 98, 'step_size': 3, 'gamma': 0.8802366918890416}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:34:38,541][0m Trial 45 finished with value: 0.3834732860889075 and parameters: {'observation_period_num': 195, 'train_rates': 0.9617941948003623, 'learning_rate': 0.00017084771589470947, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8384260090031017}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:39:47,553][0m Trial 46 finished with value: 0.5264403183843188 and parameters: {'observation_period_num': 244, 'train_rates': 0.9288783172651239, 'learning_rate': 0.0006958549031110965, 'batch_size': 105, 'step_size': 4, 'gamma': 0.7721946577946894}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:42:01,545][0m Trial 47 finished with value: 1.1366699934005737 and parameters: {'observation_period_num': 108, 'train_rates': 0.9891367096010181, 'learning_rate': 1.0033295592025082e-06, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9266408598731032}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:46:44,359][0m Trial 48 finished with value: 0.5295288036035937 and parameters: {'observation_period_num': 225, 'train_rates': 0.9527670937599217, 'learning_rate': 1.9123029425648085e-05, 'batch_size': 69, 'step_size': 6, 'gamma': 0.9000372450117122}. Best is trial 31 with value: 0.22011316293164304.[0m
[32m[I 2025-02-06 06:51:03,079][0m Trial 49 finished with value: 0.2619073005640004 and parameters: {'observation_period_num': 203, 'train_rates': 0.8795906295011616, 'learning_rate': 0.00011806827407400943, 'batch_size': 27, 'step_size': 5, 'gamma': 0.9453155425319257}. Best is trial 31 with value: 0.22011316293164304.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 06:51:03,087][0m A new study created in memory with name: no-name-e3849903-3292-4e2f-a08f-0d2b7b1c9e66[0m
Early stopping at epoch 64
[32m[I 2025-02-06 06:52:13,912][0m Trial 0 finished with value: 2.1549655485439585 and parameters: {'observation_period_num': 118, 'train_rates': 0.7650265069436806, 'learning_rate': 2.1197913452177322e-06, 'batch_size': 179, 'step_size': 1, 'gamma': 0.8593121058119906}. Best is trial 0 with value: 2.1549655485439585.[0m
[32m[I 2025-02-06 06:56:11,396][0m Trial 1 finished with value: 1.483416666767814 and parameters: {'observation_period_num': 220, 'train_rates': 0.758533935698507, 'learning_rate': 2.289248269551788e-06, 'batch_size': 114, 'step_size': 5, 'gamma': 0.9746042858574636}. Best is trial 1 with value: 1.483416666767814.[0m
[32m[I 2025-02-06 06:56:53,549][0m Trial 2 finished with value: 1.192547470331192 and parameters: {'observation_period_num': 41, 'train_rates': 0.9233795866159155, 'learning_rate': 2.8285685027928538e-05, 'batch_size': 196, 'step_size': 1, 'gamma': 0.9047338852002872}. Best is trial 2 with value: 1.192547470331192.[0m
[32m[I 2025-02-06 06:58:56,450][0m Trial 3 finished with value: 0.6312037175031084 and parameters: {'observation_period_num': 126, 'train_rates': 0.8388966672862881, 'learning_rate': 0.0002159733572248731, 'batch_size': 177, 'step_size': 2, 'gamma': 0.8633961691182844}. Best is trial 3 with value: 0.6312037175031084.[0m
[32m[I 2025-02-06 07:04:03,938][0m Trial 4 finished with value: 0.5358374714851379 and parameters: {'observation_period_num': 248, 'train_rates': 0.9130871377466012, 'learning_rate': 0.00043239800635707607, 'batch_size': 250, 'step_size': 2, 'gamma': 0.8436689609902369}. Best is trial 4 with value: 0.5358374714851379.[0m
[32m[I 2025-02-06 07:05:30,749][0m Trial 5 finished with value: 1.4430434103714656 and parameters: {'observation_period_num': 89, 'train_rates': 0.7442921485136919, 'learning_rate': 6.8232383752817995e-06, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9095575666926883}. Best is trial 4 with value: 0.5358374714851379.[0m
[32m[I 2025-02-06 07:09:05,701][0m Trial 6 finished with value: 0.6666445192412951 and parameters: {'observation_period_num': 194, 'train_rates': 0.7817149111052999, 'learning_rate': 5.8800830774594605e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9347404519631011}. Best is trial 4 with value: 0.5358374714851379.[0m
[32m[I 2025-02-06 07:10:40,619][0m Trial 7 finished with value: 0.3622774020927708 and parameters: {'observation_period_num': 40, 'train_rates': 0.9297864698461127, 'learning_rate': 4.8065836407727225e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8727344437643226}. Best is trial 7 with value: 0.3622774020927708.[0m
[32m[I 2025-02-06 07:11:40,817][0m Trial 8 finished with value: 1.1918454951566197 and parameters: {'observation_period_num': 62, 'train_rates': 0.8409534061514391, 'learning_rate': 3.5964828883170643e-06, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8543879052425958}. Best is trial 7 with value: 0.3622774020927708.[0m
[32m[I 2025-02-06 07:15:12,004][0m Trial 9 finished with value: 1.092770322474588 and parameters: {'observation_period_num': 221, 'train_rates': 0.6325642855265043, 'learning_rate': 0.00030505053182748175, 'batch_size': 78, 'step_size': 15, 'gamma': 0.9339692356986928}. Best is trial 7 with value: 0.3622774020927708.[0m
[32m[I 2025-02-06 07:19:00,560][0m Trial 10 finished with value: 0.20170060098171233 and parameters: {'observation_period_num': 11, 'train_rates': 0.9898378591280224, 'learning_rate': 2.2549923854531203e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.7694943433893147}. Best is trial 10 with value: 0.20170060098171233.[0m
[32m[I 2025-02-06 07:23:33,567][0m Trial 11 finished with value: 0.1976630505868944 and parameters: {'observation_period_num': 20, 'train_rates': 0.9800275429868217, 'learning_rate': 2.2812407592792503e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7657186164323216}. Best is trial 11 with value: 0.1976630505868944.[0m
[32m[I 2025-02-06 07:27:08,904][0m Trial 12 finished with value: 0.3318633022052901 and parameters: {'observation_period_num': 7, 'train_rates': 0.9765506176397917, 'learning_rate': 1.371049901671569e-05, 'batch_size': 23, 'step_size': 10, 'gamma': 0.7540186832232814}. Best is trial 11 with value: 0.1976630505868944.[0m
[32m[I 2025-02-06 07:31:31,182][0m Trial 13 finished with value: 0.15984329829613367 and parameters: {'observation_period_num': 16, 'train_rates': 0.9857924747151573, 'learning_rate': 0.00011532866367755548, 'batch_size': 19, 'step_size': 10, 'gamma': 0.7556681230918687}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:36:01,867][0m Trial 14 finished with value: 0.34977088351323815 and parameters: {'observation_period_num': 80, 'train_rates': 0.865765771520251, 'learning_rate': 0.00011752612964436867, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7931832322853847}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:37:14,227][0m Trial 15 finished with value: 1.7229661667569083 and parameters: {'observation_period_num': 37, 'train_rates': 0.6688885091807093, 'learning_rate': 0.0009623003156191185, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8065579002397273}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:40:22,054][0m Trial 16 finished with value: 0.4064895076596219 and parameters: {'observation_period_num': 167, 'train_rates': 0.9507947260244507, 'learning_rate': 9.702621108155893e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8109372707975426}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:42:22,328][0m Trial 17 finished with value: 0.680388639102111 and parameters: {'observation_period_num': 97, 'train_rates': 0.8836444985201589, 'learning_rate': 8.427193105400232e-06, 'batch_size': 42, 'step_size': 8, 'gamma': 0.7793459879512828}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:45:32,868][0m Trial 18 finished with value: 0.31092401444911955 and parameters: {'observation_period_num': 161, 'train_rates': 0.9573354235487644, 'learning_rate': 0.0001094706804949919, 'batch_size': 87, 'step_size': 13, 'gamma': 0.750580771888163}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:45:51,018][0m Trial 19 finished with value: 1.9127285615046998 and parameters: {'observation_period_num': 5, 'train_rates': 0.6994142043856044, 'learning_rate': 1.0259649864149886e-06, 'batch_size': 238, 'step_size': 5, 'gamma': 0.8254182623097245}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:46:45,089][0m Trial 20 finished with value: 0.7898211771751935 and parameters: {'observation_period_num': 57, 'train_rates': 0.8117373832816807, 'learning_rate': 1.9064089667617684e-05, 'batch_size': 136, 'step_size': 10, 'gamma': 0.777605729310664}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:50:04,793][0m Trial 21 finished with value: 0.20049297102427077 and parameters: {'observation_period_num': 24, 'train_rates': 0.9800297866779736, 'learning_rate': 3.546414368380215e-05, 'batch_size': 25, 'step_size': 9, 'gamma': 0.7635675995872058}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:52:07,770][0m Trial 22 finished with value: 0.33618632349662664 and parameters: {'observation_period_num': 27, 'train_rates': 0.8881883971296368, 'learning_rate': 4.342828838976981e-05, 'batch_size': 38, 'step_size': 8, 'gamma': 0.7930369251316737}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:53:28,929][0m Trial 23 finished with value: 0.5909521579742432 and parameters: {'observation_period_num': 61, 'train_rates': 0.9867306443161938, 'learning_rate': 1.2663695994263598e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.7619755654094982}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:55:45,306][0m Trial 24 finished with value: 0.31315178686464334 and parameters: {'observation_period_num': 28, 'train_rates': 0.9514612131229653, 'learning_rate': 6.420707919762592e-05, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8212334962297293}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 07:57:08,447][0m Trial 25 finished with value: 0.2806176311016953 and parameters: {'observation_period_num': 72, 'train_rates': 0.9050018108940282, 'learning_rate': 0.00017789789288777166, 'batch_size': 69, 'step_size': 12, 'gamma': 0.7901086743798444}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:02:02,893][0m Trial 26 finished with value: 0.28662498106901674 and parameters: {'observation_period_num': 24, 'train_rates': 0.9406246248209255, 'learning_rate': 3.7008658302287514e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8338120000690942}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:04:06,771][0m Trial 27 finished with value: 0.8611743450164795 and parameters: {'observation_period_num': 57, 'train_rates': 0.9894832131679686, 'learning_rate': 5.7759364470972426e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.7505748022345927}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:05:32,893][0m Trial 28 finished with value: 1.2648724266655849 and parameters: {'observation_period_num': 109, 'train_rates': 0.6007367173403853, 'learning_rate': 0.0005168527116324416, 'batch_size': 103, 'step_size': 6, 'gamma': 0.7739370697274734}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:06:24,374][0m Trial 29 finished with value: 0.45654988288879395 and parameters: {'observation_period_num': 49, 'train_rates': 0.9630211554887732, 'learning_rate': 8.125548755478237e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8059500044839362}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:09:12,955][0m Trial 30 finished with value: 0.36340974858784486 and parameters: {'observation_period_num': 148, 'train_rates': 0.8634271352076992, 'learning_rate': 0.00015959330763162168, 'batch_size': 57, 'step_size': 11, 'gamma': 0.7832324191967575}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:11:58,516][0m Trial 31 finished with value: 0.2518195572652315 and parameters: {'observation_period_num': 15, 'train_rates': 0.9873519118990453, 'learning_rate': 2.5471641054212264e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.7680075339976685}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:17:04,561][0m Trial 32 finished with value: 0.27428394740389794 and parameters: {'observation_period_num': 20, 'train_rates': 0.9669707208203214, 'learning_rate': 1.834989119572025e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.76556585151793}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:19:44,705][0m Trial 33 finished with value: 0.3127696779039171 and parameters: {'observation_period_num': 5, 'train_rates': 0.935923766848385, 'learning_rate': 2.879126791317311e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8007532111512069}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:20:23,393][0m Trial 34 finished with value: 1.0735315915843218 and parameters: {'observation_period_num': 37, 'train_rates': 0.9070498014499386, 'learning_rate': 1.1828758873770384e-05, 'batch_size': 216, 'step_size': 8, 'gamma': 0.7705902911993799}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:22:03,183][0m Trial 35 finished with value: 0.35831128976760657 and parameters: {'observation_period_num': 21, 'train_rates': 0.9259467961228863, 'learning_rate': 1.8576982773935e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9719088700737863}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:22:58,320][0m Trial 36 finished with value: 0.8712209062960447 and parameters: {'observation_period_num': 48, 'train_rates': 0.7308648931157314, 'learning_rate': 2.5887209427693298e-05, 'batch_size': 78, 'step_size': 8, 'gamma': 0.874027581244186}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:25:28,252][0m Trial 37 finished with value: 0.33911933242640596 and parameters: {'observation_period_num': 76, 'train_rates': 0.9675284856476547, 'learning_rate': 5.8459586221823374e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.7575790099355492}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:28:16,821][0m Trial 38 finished with value: 0.5348618744876135 and parameters: {'observation_period_num': 34, 'train_rates': 0.9120160330008424, 'learning_rate': 8.446065288829949e-06, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8947423771915451}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:30:00,908][0m Trial 39 finished with value: 1.0070373097424785 and parameters: {'observation_period_num': 100, 'train_rates': 0.9337570789998206, 'learning_rate': 4.714057327790854e-06, 'batch_size': 188, 'step_size': 12, 'gamma': 0.9884506079638778}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:31:56,114][0m Trial 40 finished with value: 1.818093923539141 and parameters: {'observation_period_num': 119, 'train_rates': 0.8068705620422459, 'learning_rate': 2.46811555023643e-06, 'batch_size': 121, 'step_size': 3, 'gamma': 0.819529072321387}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:35:02,210][0m Trial 41 finished with value: 0.2461785738441077 and parameters: {'observation_period_num': 16, 'train_rates': 0.9852531741474494, 'learning_rate': 2.1653241850825663e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.766508113366385}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:36:47,249][0m Trial 42 finished with value: 0.3440305098711726 and parameters: {'observation_period_num': 18, 'train_rates': 0.9719483738412761, 'learning_rate': 3.6906454041849625e-05, 'batch_size': 48, 'step_size': 9, 'gamma': 0.7636651495385116}. Best is trial 13 with value: 0.15984329829613367.[0m
[32m[I 2025-02-06 08:40:15,942][0m Trial 43 finished with value: 0.13795020133256913 and parameters: {'observation_period_num': 48, 'train_rates': 0.9898529903835476, 'learning_rate': 7.103499801503369e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.7851961993436622}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:41:43,751][0m Trial 44 finished with value: 0.3281837736859041 and parameters: {'observation_period_num': 46, 'train_rates': 0.9475487791008796, 'learning_rate': 0.0002733046697091353, 'batch_size': 57, 'step_size': 11, 'gamma': 0.7855791323796243}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:46:50,302][0m Trial 45 finished with value: 0.17945808708667754 and parameters: {'observation_period_num': 33, 'train_rates': 0.9657801233811024, 'learning_rate': 7.549153658117405e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7994367118650882}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:48:03,596][0m Trial 46 finished with value: 0.31195863677283464 and parameters: {'observation_period_num': 33, 'train_rates': 0.9598164138212726, 'learning_rate': 8.114585597202427e-05, 'batch_size': 69, 'step_size': 10, 'gamma': 0.7981891486700402}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:53:08,974][0m Trial 47 finished with value: 0.1488575369119644 and parameters: {'observation_period_num': 69, 'train_rates': 0.9725733734912683, 'learning_rate': 5.524869540008096e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8502140832837859}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:57:53,405][0m Trial 48 finished with value: 0.3608032096723083 and parameters: {'observation_period_num': 70, 'train_rates': 0.8912242498997505, 'learning_rate': 0.0001362565940876559, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8486513062118876}. Best is trial 43 with value: 0.13795020133256913.[0m
[32m[I 2025-02-06 08:59:41,720][0m Trial 49 finished with value: 0.29036291874945164 and parameters: {'observation_period_num': 82, 'train_rates': 0.9249095657810181, 'learning_rate': 5.6587323742098774e-05, 'batch_size': 45, 'step_size': 14, 'gamma': 0.8668653663387165}. Best is trial 43 with value: 0.13795020133256913.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 55, 'train_rates': 0.9816849250900748, 'learning_rate': 0.00024289207162824682, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8450822781305919}
Epoch 1/300, trend Loss: 1.1933 | 1.9490
Epoch 2/300, trend Loss: 0.7438 | 1.0948
Epoch 3/300, trend Loss: 0.6258 | 0.9123
Epoch 4/300, trend Loss: 0.5411 | 0.8282
Epoch 5/300, trend Loss: 0.5930 | 0.8091
Epoch 6/300, trend Loss: 0.4965 | 0.7632
Epoch 7/300, trend Loss: 0.4983 | 0.7259
Epoch 8/300, trend Loss: 0.4422 | 0.7067
Epoch 9/300, trend Loss: 0.3854 | 0.6928
Epoch 10/300, trend Loss: 0.3754 | 0.6261
Epoch 11/300, trend Loss: 0.3793 | 0.6389
Epoch 12/300, trend Loss: 0.3576 | 0.5779
Epoch 13/300, trend Loss: 0.3404 | 0.5453
Epoch 14/300, trend Loss: 0.4124 | 0.5521
Epoch 15/300, trend Loss: 0.3585 | 0.5736
Epoch 16/300, trend Loss: 0.3085 | 0.5392
Epoch 17/300, trend Loss: 0.2753 | 0.4988
Epoch 18/300, trend Loss: 0.2639 | 0.4922
Epoch 19/300, trend Loss: 0.2569 | 0.4728
Epoch 20/300, trend Loss: 0.2499 | 0.4635
Epoch 21/300, trend Loss: 0.2456 | 0.4498
Epoch 22/300, trend Loss: 0.2401 | 0.4392
Epoch 23/300, trend Loss: 0.2361 | 0.4267
Epoch 24/300, trend Loss: 0.2337 | 0.4183
Epoch 25/300, trend Loss: 0.2310 | 0.4104
Epoch 26/300, trend Loss: 0.2318 | 0.3996
Epoch 27/300, trend Loss: 0.2310 | 0.3946
Epoch 28/300, trend Loss: 0.2323 | 0.3836
Epoch 29/300, trend Loss: 0.2237 | 0.3839
Epoch 30/300, trend Loss: 0.2196 | 0.3715
Epoch 31/300, trend Loss: 0.2151 | 0.3691
Epoch 32/300, trend Loss: 0.2119 | 0.3605
Epoch 33/300, trend Loss: 0.2095 | 0.3578
Epoch 34/300, trend Loss: 0.2095 | 0.3508
Epoch 35/300, trend Loss: 0.2064 | 0.3468
Epoch 36/300, trend Loss: 0.2051 | 0.3424
Epoch 37/300, trend Loss: 0.2041 | 0.3382
Epoch 38/300, trend Loss: 0.2031 | 0.3350
Epoch 39/300, trend Loss: 0.2006 | 0.3288
Epoch 40/300, trend Loss: 0.1993 | 0.3271
Epoch 41/300, trend Loss: 0.1980 | 0.3232
Epoch 42/300, trend Loss: 0.1970 | 0.3204
Epoch 43/300, trend Loss: 0.1952 | 0.3164
Epoch 44/300, trend Loss: 0.1944 | 0.3148
Epoch 45/300, trend Loss: 0.1925 | 0.3114
Epoch 46/300, trend Loss: 0.1925 | 0.3088
Epoch 47/300, trend Loss: 0.1913 | 0.3065
Epoch 48/300, trend Loss: 0.1911 | 0.3046
Epoch 49/300, trend Loss: 0.1902 | 0.3020
Epoch 50/300, trend Loss: 0.1887 | 0.2996
Epoch 51/300, trend Loss: 0.1879 | 0.2977
Epoch 52/300, trend Loss: 0.1878 | 0.2963
Epoch 53/300, trend Loss: 0.1865 | 0.2945
Epoch 54/300, trend Loss: 0.1855 | 0.2923
Epoch 55/300, trend Loss: 0.1858 | 0.2902
Epoch 56/300, trend Loss: 0.1845 | 0.2886
Epoch 57/300, trend Loss: 0.1836 | 0.2863
Epoch 58/300, trend Loss: 0.1831 | 0.2853
Epoch 59/300, trend Loss: 0.1832 | 0.2840
Epoch 60/300, trend Loss: 0.1823 | 0.2823
Epoch 61/300, trend Loss: 0.1821 | 0.2810
Epoch 62/300, trend Loss: 0.1820 | 0.2802
Epoch 63/300, trend Loss: 0.1811 | 0.2787
Epoch 64/300, trend Loss: 0.1802 | 0.2770
Epoch 65/300, trend Loss: 0.1798 | 0.2767
Epoch 66/300, trend Loss: 0.1795 | 0.2759
Epoch 67/300, trend Loss: 0.1794 | 0.2739
Epoch 68/300, trend Loss: 0.1780 | 0.2731
Epoch 69/300, trend Loss: 0.1789 | 0.2722
Epoch 70/300, trend Loss: 0.1776 | 0.2712
Epoch 71/300, trend Loss: 0.1769 | 0.2702
Epoch 72/300, trend Loss: 0.1770 | 0.2694
Epoch 73/300, trend Loss: 0.1769 | 0.2688
Epoch 74/300, trend Loss: 0.1770 | 0.2682
Epoch 75/300, trend Loss: 0.1757 | 0.2674
Epoch 76/300, trend Loss: 0.1771 | 0.2662
Epoch 77/300, trend Loss: 0.1760 | 0.2653
Epoch 78/300, trend Loss: 0.1752 | 0.2647
Epoch 79/300, trend Loss: 0.1752 | 0.2643
Epoch 80/300, trend Loss: 0.1754 | 0.2637
Epoch 81/300, trend Loss: 0.1744 | 0.2631
Epoch 82/300, trend Loss: 0.1746 | 0.2628
Epoch 83/300, trend Loss: 0.1737 | 0.2618
Epoch 84/300, trend Loss: 0.1736 | 0.2610
Epoch 85/300, trend Loss: 0.1739 | 0.2605
Epoch 86/300, trend Loss: 0.1737 | 0.2599
Epoch 87/300, trend Loss: 0.1730 | 0.2597
Epoch 88/300, trend Loss: 0.1728 | 0.2593
Epoch 89/300, trend Loss: 0.1723 | 0.2587
Epoch 90/300, trend Loss: 0.1728 | 0.2580
Epoch 91/300, trend Loss: 0.1728 | 0.2573
Epoch 92/300, trend Loss: 0.1726 | 0.2571
Epoch 93/300, trend Loss: 0.1720 | 0.2568
Epoch 94/300, trend Loss: 0.1713 | 0.2564
Epoch 95/300, trend Loss: 0.1729 | 0.2558
Epoch 96/300, trend Loss: 0.1721 | 0.2553
Epoch 97/300, trend Loss: 0.1713 | 0.2551
Epoch 98/300, trend Loss: 0.1717 | 0.2549
Epoch 99/300, trend Loss: 0.1715 | 0.2545
Epoch 100/300, trend Loss: 0.1711 | 0.2542
Epoch 101/300, trend Loss: 0.1705 | 0.2541
Epoch 102/300, trend Loss: 0.1714 | 0.2537
Epoch 103/300, trend Loss: 0.1701 | 0.2531
Epoch 104/300, trend Loss: 0.1709 | 0.2528
Epoch 105/300, trend Loss: 0.1704 | 0.2527
Epoch 106/300, trend Loss: 0.1702 | 0.2528
Epoch 107/300, trend Loss: 0.1706 | 0.2527
Epoch 108/300, trend Loss: 0.1703 | 0.2523
Epoch 109/300, trend Loss: 0.1708 | 0.2520
Epoch 110/300, trend Loss: 0.1701 | 0.2518
Epoch 111/300, trend Loss: 0.1706 | 0.2517
Epoch 112/300, trend Loss: 0.1710 | 0.2516
Epoch 113/300, trend Loss: 0.1698 | 0.2511
Epoch 114/300, trend Loss: 0.1700 | 0.2508
Epoch 115/300, trend Loss: 0.1702 | 0.2507
Epoch 116/300, trend Loss: 0.1702 | 0.2505
Epoch 117/300, trend Loss: 0.1702 | 0.2502
Epoch 118/300, trend Loss: 0.1697 | 0.2501
Epoch 119/300, trend Loss: 0.1700 | 0.2501
Epoch 120/300, trend Loss: 0.1690 | 0.2498
Epoch 121/300, trend Loss: 0.1693 | 0.2497
Epoch 122/300, trend Loss: 0.1687 | 0.2497
Epoch 123/300, trend Loss: 0.1695 | 0.2497
Epoch 124/300, trend Loss: 0.1694 | 0.2495
Epoch 125/300, trend Loss: 0.1685 | 0.2493
Epoch 126/300, trend Loss: 0.1696 | 0.2492
Epoch 127/300, trend Loss: 0.1689 | 0.2491
Epoch 128/300, trend Loss: 0.1688 | 0.2490
Epoch 129/300, trend Loss: 0.1678 | 0.2489
Epoch 130/300, trend Loss: 0.1688 | 0.2488
Epoch 131/300, trend Loss: 0.1681 | 0.2486
Epoch 132/300, trend Loss: 0.1693 | 0.2485
Epoch 133/300, trend Loss: 0.1690 | 0.2484
Epoch 134/300, trend Loss: 0.1689 | 0.2483
Epoch 135/300, trend Loss: 0.1691 | 0.2481
Epoch 136/300, trend Loss: 0.1682 | 0.2480
Epoch 137/300, trend Loss: 0.1685 | 0.2478
Epoch 138/300, trend Loss: 0.1687 | 0.2477
Epoch 139/300, trend Loss: 0.1689 | 0.2475
Epoch 140/300, trend Loss: 0.1690 | 0.2474
Epoch 141/300, trend Loss: 0.1684 | 0.2473
Epoch 142/300, trend Loss: 0.1689 | 0.2472
Epoch 143/300, trend Loss: 0.1685 | 0.2472
Epoch 144/300, trend Loss: 0.1682 | 0.2472
Epoch 145/300, trend Loss: 0.1686 | 0.2472
Epoch 146/300, trend Loss: 0.1681 | 0.2471
Epoch 147/300, trend Loss: 0.1679 | 0.2470
Epoch 148/300, trend Loss: 0.1679 | 0.2469
Epoch 149/300, trend Loss: 0.1684 | 0.2469
Epoch 150/300, trend Loss: 0.1685 | 0.2468
Epoch 151/300, trend Loss: 0.1676 | 0.2468
Epoch 152/300, trend Loss: 0.1679 | 0.2467
Epoch 153/300, trend Loss: 0.1680 | 0.2467
Epoch 154/300, trend Loss: 0.1679 | 0.2467
Epoch 155/300, trend Loss: 0.1682 | 0.2466
Epoch 156/300, trend Loss: 0.1683 | 0.2466
Epoch 157/300, trend Loss: 0.1681 | 0.2465
Epoch 158/300, trend Loss: 0.1686 | 0.2464
Epoch 159/300, trend Loss: 0.1683 | 0.2464
Epoch 160/300, trend Loss: 0.1686 | 0.2463
Epoch 161/300, trend Loss: 0.1687 | 0.2463
Epoch 162/300, trend Loss: 0.1686 | 0.2462
Epoch 163/300, trend Loss: 0.1684 | 0.2462
Epoch 164/300, trend Loss: 0.1683 | 0.2461
Epoch 165/300, trend Loss: 0.1677 | 0.2461
Epoch 166/300, trend Loss: 0.1681 | 0.2461
Epoch 167/300, trend Loss: 0.1682 | 0.2460
Epoch 168/300, trend Loss: 0.1682 | 0.2460
Epoch 169/300, trend Loss: 0.1673 | 0.2459
Epoch 170/300, trend Loss: 0.1684 | 0.2459
Epoch 171/300, trend Loss: 0.1680 | 0.2459
Epoch 172/300, trend Loss: 0.1678 | 0.2459
Epoch 173/300, trend Loss: 0.1673 | 0.2458
Epoch 174/300, trend Loss: 0.1674 | 0.2458
Epoch 175/300, trend Loss: 0.1677 | 0.2457
Epoch 176/300, trend Loss: 0.1680 | 0.2457
Epoch 177/300, trend Loss: 0.1681 | 0.2457
Epoch 178/300, trend Loss: 0.1678 | 0.2457
Epoch 179/300, trend Loss: 0.1673 | 0.2457
Epoch 180/300, trend Loss: 0.1679 | 0.2457
Epoch 181/300, trend Loss: 0.1682 | 0.2457
Epoch 182/300, trend Loss: 0.1683 | 0.2456
Epoch 183/300, trend Loss: 0.1678 | 0.2456
Epoch 184/300, trend Loss: 0.1669 | 0.2456
Epoch 185/300, trend Loss: 0.1672 | 0.2456
Epoch 186/300, trend Loss: 0.1676 | 0.2456
Epoch 187/300, trend Loss: 0.1686 | 0.2456
Epoch 188/300, trend Loss: 0.1673 | 0.2456
Epoch 189/300, trend Loss: 0.1682 | 0.2456
Epoch 190/300, trend Loss: 0.1673 | 0.2455
Epoch 191/300, trend Loss: 0.1677 | 0.2455
Epoch 192/300, trend Loss: 0.1677 | 0.2455
Epoch 193/300, trend Loss: 0.1681 | 0.2455
Epoch 194/300, trend Loss: 0.1674 | 0.2455
Epoch 195/300, trend Loss: 0.1686 | 0.2455
Epoch 196/300, trend Loss: 0.1686 | 0.2455
Epoch 197/300, trend Loss: 0.1677 | 0.2455
Epoch 198/300, trend Loss: 0.1683 | 0.2454
Epoch 199/300, trend Loss: 0.1673 | 0.2454
Epoch 200/300, trend Loss: 0.1684 | 0.2454
Epoch 201/300, trend Loss: 0.1680 | 0.2454
Epoch 202/300, trend Loss: 0.1674 | 0.2454
Epoch 203/300, trend Loss: 0.1674 | 0.2454
Epoch 204/300, trend Loss: 0.1675 | 0.2454
Epoch 205/300, trend Loss: 0.1677 | 0.2454
Epoch 206/300, trend Loss: 0.1676 | 0.2454
Epoch 207/300, trend Loss: 0.1676 | 0.2454
Epoch 208/300, trend Loss: 0.1677 | 0.2453
Epoch 209/300, trend Loss: 0.1679 | 0.2453
Epoch 210/300, trend Loss: 0.1679 | 0.2453
Epoch 211/300, trend Loss: 0.1678 | 0.2453
Epoch 212/300, trend Loss: 0.1674 | 0.2453
Epoch 213/300, trend Loss: 0.1677 | 0.2453
Epoch 214/300, trend Loss: 0.1676 | 0.2453
Epoch 215/300, trend Loss: 0.1681 | 0.2453
Epoch 216/300, trend Loss: 0.1671 | 0.2453
Epoch 217/300, trend Loss: 0.1674 | 0.2453
Epoch 218/300, trend Loss: 0.1683 | 0.2453
Epoch 219/300, trend Loss: 0.1674 | 0.2453
Epoch 220/300, trend Loss: 0.1676 | 0.2453
Epoch 221/300, trend Loss: 0.1675 | 0.2453
Epoch 222/300, trend Loss: 0.1673 | 0.2453
Epoch 223/300, trend Loss: 0.1679 | 0.2453
Epoch 224/300, trend Loss: 0.1673 | 0.2453
Epoch 225/300, trend Loss: 0.1680 | 0.2453
Epoch 226/300, trend Loss: 0.1677 | 0.2453
Epoch 227/300, trend Loss: 0.1671 | 0.2452
Epoch 228/300, trend Loss: 0.1679 | 0.2452
Epoch 229/300, trend Loss: 0.1670 | 0.2452
Epoch 230/300, trend Loss: 0.1679 | 0.2452
Epoch 231/300, trend Loss: 0.1674 | 0.2452
Epoch 232/300, trend Loss: 0.1678 | 0.2452
Epoch 233/300, trend Loss: 0.1672 | 0.2452
Epoch 234/300, trend Loss: 0.1681 | 0.2452
Epoch 235/300, trend Loss: 0.1673 | 0.2452
Epoch 236/300, trend Loss: 0.1667 | 0.2452
Epoch 237/300, trend Loss: 0.1684 | 0.2452
Epoch 238/300, trend Loss: 0.1679 | 0.2452
Epoch 239/300, trend Loss: 0.1676 | 0.2452
Epoch 240/300, trend Loss: 0.1679 | 0.2452
Epoch 241/300, trend Loss: 0.1671 | 0.2452
Epoch 242/300, trend Loss: 0.1676 | 0.2452
Epoch 243/300, trend Loss: 0.1683 | 0.2452
Epoch 244/300, trend Loss: 0.1673 | 0.2452
Epoch 245/300, trend Loss: 0.1674 | 0.2452
Epoch 246/300, trend Loss: 0.1676 | 0.2452
Epoch 247/300, trend Loss: 0.1684 | 0.2452
Epoch 248/300, trend Loss: 0.1670 | 0.2452
Epoch 249/300, trend Loss: 0.1673 | 0.2452
Epoch 250/300, trend Loss: 0.1686 | 0.2452
Epoch 251/300, trend Loss: 0.1674 | 0.2452
Epoch 252/300, trend Loss: 0.1670 | 0.2452
Epoch 253/300, trend Loss: 0.1670 | 0.2452
Epoch 254/300, trend Loss: 0.1683 | 0.2452
Epoch 255/300, trend Loss: 0.1671 | 0.2452
Epoch 256/300, trend Loss: 0.1676 | 0.2452
Epoch 257/300, trend Loss: 0.1680 | 0.2452
Epoch 258/300, trend Loss: 0.1676 | 0.2452
Epoch 259/300, trend Loss: 0.1673 | 0.2452
Epoch 260/300, trend Loss: 0.1670 | 0.2452
Epoch 261/300, trend Loss: 0.1678 | 0.2452
Epoch 262/300, trend Loss: 0.1679 | 0.2452
Epoch 263/300, trend Loss: 0.1675 | 0.2452
Epoch 264/300, trend Loss: 0.1675 | 0.2452
Epoch 265/300, trend Loss: 0.1675 | 0.2452
Epoch 266/300, trend Loss: 0.1675 | 0.2452
Epoch 267/300, trend Loss: 0.1669 | 0.2452
Epoch 268/300, trend Loss: 0.1673 | 0.2452
Epoch 269/300, trend Loss: 0.1683 | 0.2452
Epoch 270/300, trend Loss: 0.1678 | 0.2452
Epoch 271/300, trend Loss: 0.1674 | 0.2452
Epoch 272/300, trend Loss: 0.1681 | 0.2452
Epoch 273/300, trend Loss: 0.1674 | 0.2452
Epoch 274/300, trend Loss: 0.1678 | 0.2452
Epoch 275/300, trend Loss: 0.1676 | 0.2452
Epoch 276/300, trend Loss: 0.1674 | 0.2452
Epoch 277/300, trend Loss: 0.1678 | 0.2452
Epoch 278/300, trend Loss: 0.1678 | 0.2452
Epoch 279/300, trend Loss: 0.1677 | 0.2452
Epoch 280/300, trend Loss: 0.1675 | 0.2452
Epoch 281/300, trend Loss: 0.1680 | 0.2452
Epoch 282/300, trend Loss: 0.1677 | 0.2452
Epoch 283/300, trend Loss: 0.1677 | 0.2452
Epoch 284/300, trend Loss: 0.1678 | 0.2452
Epoch 285/300, trend Loss: 0.1674 | 0.2452
Epoch 286/300, trend Loss: 0.1678 | 0.2452
Epoch 287/300, trend Loss: 0.1672 | 0.2452
Epoch 288/300, trend Loss: 0.1681 | 0.2452
Epoch 289/300, trend Loss: 0.1670 | 0.2452
Epoch 290/300, trend Loss: 0.1674 | 0.2452
Epoch 291/300, trend Loss: 0.1677 | 0.2452
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 151, 'train_rates': 0.9882491281031751, 'learning_rate': 9.590390879565091e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.7841266359410203}
Epoch 1/300, seasonal_0 Loss: 0.8771 | 1.1034
Epoch 2/300, seasonal_0 Loss: 0.6679 | 0.8455
Epoch 3/300, seasonal_0 Loss: 0.4935 | 0.7262
Epoch 4/300, seasonal_0 Loss: 0.4320 | 0.6783
Epoch 5/300, seasonal_0 Loss: 0.4161 | 0.6374
Epoch 6/300, seasonal_0 Loss: 0.4032 | 0.5785
Epoch 7/300, seasonal_0 Loss: 0.3487 | 0.5241
Epoch 8/300, seasonal_0 Loss: 0.3208 | 0.5215
Epoch 9/300, seasonal_0 Loss: 0.3149 | 0.4672
Epoch 10/300, seasonal_0 Loss: 0.2839 | 0.4425
Epoch 11/300, seasonal_0 Loss: 0.2676 | 0.4280
Epoch 12/300, seasonal_0 Loss: 0.2733 | 0.4147
Epoch 13/300, seasonal_0 Loss: 0.2529 | 0.3921
Epoch 14/300, seasonal_0 Loss: 0.2560 | 0.3839
Epoch 15/300, seasonal_0 Loss: 0.2548 | 0.3735
Epoch 16/300, seasonal_0 Loss: 0.2371 | 0.3570
Epoch 17/300, seasonal_0 Loss: 0.2369 | 0.3509
Epoch 18/300, seasonal_0 Loss: 0.2310 | 0.3464
Epoch 19/300, seasonal_0 Loss: 0.2243 | 0.3253
Epoch 20/300, seasonal_0 Loss: 0.2192 | 0.3207
Epoch 21/300, seasonal_0 Loss: 0.2196 | 0.3201
Epoch 22/300, seasonal_0 Loss: 0.2084 | 0.2986
Epoch 23/300, seasonal_0 Loss: 0.2038 | 0.3049
Epoch 24/300, seasonal_0 Loss: 0.2013 | 0.2941
Epoch 25/300, seasonal_0 Loss: 0.1975 | 0.2856
Epoch 26/300, seasonal_0 Loss: 0.1962 | 0.2861
Epoch 27/300, seasonal_0 Loss: 0.1954 | 0.2766
Epoch 28/300, seasonal_0 Loss: 0.1929 | 0.2745
Epoch 29/300, seasonal_0 Loss: 0.1906 | 0.2706
Epoch 30/300, seasonal_0 Loss: 0.1908 | 0.2659
Epoch 31/300, seasonal_0 Loss: 0.1861 | 0.2617
Epoch 32/300, seasonal_0 Loss: 0.1859 | 0.2597
Epoch 33/300, seasonal_0 Loss: 0.1858 | 0.2608
Epoch 34/300, seasonal_0 Loss: 0.1823 | 0.2523
Epoch 35/300, seasonal_0 Loss: 0.1817 | 0.2465
Epoch 36/300, seasonal_0 Loss: 0.1818 | 0.2537
Epoch 37/300, seasonal_0 Loss: 0.1775 | 0.2437
Epoch 38/300, seasonal_0 Loss: 0.1775 | 0.2386
Epoch 39/300, seasonal_0 Loss: 0.1763 | 0.2475
Epoch 40/300, seasonal_0 Loss: 0.1739 | 0.2364
Epoch 41/300, seasonal_0 Loss: 0.1732 | 0.2331
Epoch 42/300, seasonal_0 Loss: 0.1715 | 0.2376
Epoch 43/300, seasonal_0 Loss: 0.1716 | 0.2315
Epoch 44/300, seasonal_0 Loss: 0.1700 | 0.2277
Epoch 45/300, seasonal_0 Loss: 0.1687 | 0.2277
Epoch 46/300, seasonal_0 Loss: 0.1687 | 0.2293
Epoch 47/300, seasonal_0 Loss: 0.1674 | 0.2218
Epoch 48/300, seasonal_0 Loss: 0.1675 | 0.2212
Epoch 49/300, seasonal_0 Loss: 0.1661 | 0.2225
Epoch 50/300, seasonal_0 Loss: 0.1648 | 0.2198
Epoch 51/300, seasonal_0 Loss: 0.1646 | 0.2178
Epoch 52/300, seasonal_0 Loss: 0.1644 | 0.2153
Epoch 53/300, seasonal_0 Loss: 0.1633 | 0.2152
Epoch 54/300, seasonal_0 Loss: 0.1632 | 0.2136
Epoch 55/300, seasonal_0 Loss: 0.1617 | 0.2134
Epoch 56/300, seasonal_0 Loss: 0.1618 | 0.2111
Epoch 57/300, seasonal_0 Loss: 0.1608 | 0.2108
Epoch 58/300, seasonal_0 Loss: 0.1605 | 0.2101
Epoch 59/300, seasonal_0 Loss: 0.1601 | 0.2070
Epoch 60/300, seasonal_0 Loss: 0.1590 | 0.2081
Epoch 61/300, seasonal_0 Loss: 0.1583 | 0.2052
Epoch 62/300, seasonal_0 Loss: 0.1590 | 0.2047
Epoch 63/300, seasonal_0 Loss: 0.1582 | 0.2046
Epoch 64/300, seasonal_0 Loss: 0.1583 | 0.2034
Epoch 65/300, seasonal_0 Loss: 0.1572 | 0.2037
Epoch 66/300, seasonal_0 Loss: 0.1565 | 0.2019
Epoch 67/300, seasonal_0 Loss: 0.1573 | 0.2010
Epoch 68/300, seasonal_0 Loss: 0.1561 | 0.2004
Epoch 69/300, seasonal_0 Loss: 0.1565 | 0.2003
Epoch 70/300, seasonal_0 Loss: 0.1558 | 0.1993
Epoch 71/300, seasonal_0 Loss: 0.1554 | 0.1977
Epoch 72/300, seasonal_0 Loss: 0.1546 | 0.1979
Epoch 73/300, seasonal_0 Loss: 0.1552 | 0.1963
Epoch 74/300, seasonal_0 Loss: 0.1548 | 0.1960
Epoch 75/300, seasonal_0 Loss: 0.1539 | 0.1962
Epoch 76/300, seasonal_0 Loss: 0.1534 | 0.1952
Epoch 77/300, seasonal_0 Loss: 0.1532 | 0.1955
Epoch 78/300, seasonal_0 Loss: 0.1535 | 0.1948
Epoch 79/300, seasonal_0 Loss: 0.1526 | 0.1937
Epoch 80/300, seasonal_0 Loss: 0.1530 | 0.1928
Epoch 81/300, seasonal_0 Loss: 0.1532 | 0.1922
Epoch 82/300, seasonal_0 Loss: 0.1522 | 0.1925
Epoch 83/300, seasonal_0 Loss: 0.1518 | 0.1917
Epoch 84/300, seasonal_0 Loss: 0.1516 | 0.1912
Epoch 85/300, seasonal_0 Loss: 0.1519 | 0.1903
Epoch 86/300, seasonal_0 Loss: 0.1517 | 0.1909
Epoch 87/300, seasonal_0 Loss: 0.1514 | 0.1903
Epoch 88/300, seasonal_0 Loss: 0.1512 | 0.1897
Epoch 89/300, seasonal_0 Loss: 0.1515 | 0.1900
Epoch 90/300, seasonal_0 Loss: 0.1511 | 0.1893
Epoch 91/300, seasonal_0 Loss: 0.1511 | 0.1897
Epoch 92/300, seasonal_0 Loss: 0.1512 | 0.1887
Epoch 93/300, seasonal_0 Loss: 0.1505 | 0.1883
Epoch 94/300, seasonal_0 Loss: 0.1507 | 0.1886
Epoch 95/300, seasonal_0 Loss: 0.1506 | 0.1887
Epoch 96/300, seasonal_0 Loss: 0.1502 | 0.1880
Epoch 97/300, seasonal_0 Loss: 0.1501 | 0.1876
Epoch 98/300, seasonal_0 Loss: 0.1500 | 0.1878
Epoch 99/300, seasonal_0 Loss: 0.1500 | 0.1871
Epoch 100/300, seasonal_0 Loss: 0.1496 | 0.1870
Epoch 101/300, seasonal_0 Loss: 0.1491 | 0.1870
Epoch 102/300, seasonal_0 Loss: 0.1492 | 0.1860
Epoch 103/300, seasonal_0 Loss: 0.1491 | 0.1862
Epoch 104/300, seasonal_0 Loss: 0.1491 | 0.1863
Epoch 105/300, seasonal_0 Loss: 0.1492 | 0.1862
Epoch 106/300, seasonal_0 Loss: 0.1484 | 0.1858
Epoch 107/300, seasonal_0 Loss: 0.1487 | 0.1855
Epoch 108/300, seasonal_0 Loss: 0.1486 | 0.1855
Epoch 109/300, seasonal_0 Loss: 0.1487 | 0.1853
Epoch 110/300, seasonal_0 Loss: 0.1488 | 0.1855
Epoch 111/300, seasonal_0 Loss: 0.1481 | 0.1852
Epoch 112/300, seasonal_0 Loss: 0.1483 | 0.1851
Epoch 113/300, seasonal_0 Loss: 0.1482 | 0.1849
Epoch 114/300, seasonal_0 Loss: 0.1489 | 0.1849
Epoch 115/300, seasonal_0 Loss: 0.1476 | 0.1851
Epoch 116/300, seasonal_0 Loss: 0.1481 | 0.1849
Epoch 117/300, seasonal_0 Loss: 0.1475 | 0.1848
Epoch 118/300, seasonal_0 Loss: 0.1485 | 0.1844
Epoch 119/300, seasonal_0 Loss: 0.1472 | 0.1844
Epoch 120/300, seasonal_0 Loss: 0.1475 | 0.1844
Epoch 121/300, seasonal_0 Loss: 0.1479 | 0.1841
Epoch 122/300, seasonal_0 Loss: 0.1475 | 0.1839
Epoch 123/300, seasonal_0 Loss: 0.1479 | 0.1839
Epoch 124/300, seasonal_0 Loss: 0.1481 | 0.1838
Epoch 125/300, seasonal_0 Loss: 0.1474 | 0.1836
Epoch 126/300, seasonal_0 Loss: 0.1472 | 0.1836
Epoch 127/300, seasonal_0 Loss: 0.1476 | 0.1836
Epoch 128/300, seasonal_0 Loss: 0.1474 | 0.1835
Epoch 129/300, seasonal_0 Loss: 0.1481 | 0.1833
Epoch 130/300, seasonal_0 Loss: 0.1468 | 0.1834
Epoch 131/300, seasonal_0 Loss: 0.1470 | 0.1834
Epoch 132/300, seasonal_0 Loss: 0.1474 | 0.1833
Epoch 133/300, seasonal_0 Loss: 0.1472 | 0.1832
Epoch 134/300, seasonal_0 Loss: 0.1473 | 0.1832
Epoch 135/300, seasonal_0 Loss: 0.1476 | 0.1831
Epoch 136/300, seasonal_0 Loss: 0.1471 | 0.1832
Epoch 137/300, seasonal_0 Loss: 0.1477 | 0.1830
Epoch 138/300, seasonal_0 Loss: 0.1473 | 0.1831
Epoch 139/300, seasonal_0 Loss: 0.1468 | 0.1832
Epoch 140/300, seasonal_0 Loss: 0.1467 | 0.1833
Epoch 141/300, seasonal_0 Loss: 0.1478 | 0.1830
Epoch 142/300, seasonal_0 Loss: 0.1472 | 0.1829
Epoch 143/300, seasonal_0 Loss: 0.1472 | 0.1828
Epoch 144/300, seasonal_0 Loss: 0.1469 | 0.1827
Epoch 145/300, seasonal_0 Loss: 0.1477 | 0.1826
Epoch 146/300, seasonal_0 Loss: 0.1469 | 0.1825
Epoch 147/300, seasonal_0 Loss: 0.1471 | 0.1826
Epoch 148/300, seasonal_0 Loss: 0.1468 | 0.1826
Epoch 149/300, seasonal_0 Loss: 0.1474 | 0.1825
Epoch 150/300, seasonal_0 Loss: 0.1468 | 0.1826
Epoch 151/300, seasonal_0 Loss: 0.1463 | 0.1825
Epoch 152/300, seasonal_0 Loss: 0.1470 | 0.1825
Epoch 153/300, seasonal_0 Loss: 0.1466 | 0.1824
Epoch 154/300, seasonal_0 Loss: 0.1464 | 0.1824
Epoch 155/300, seasonal_0 Loss: 0.1473 | 0.1824
Epoch 156/300, seasonal_0 Loss: 0.1467 | 0.1825
Epoch 157/300, seasonal_0 Loss: 0.1466 | 0.1824
Epoch 158/300, seasonal_0 Loss: 0.1471 | 0.1823
Epoch 159/300, seasonal_0 Loss: 0.1468 | 0.1823
Epoch 160/300, seasonal_0 Loss: 0.1459 | 0.1823
Epoch 161/300, seasonal_0 Loss: 0.1465 | 0.1823
Epoch 162/300, seasonal_0 Loss: 0.1469 | 0.1823
Epoch 163/300, seasonal_0 Loss: 0.1470 | 0.1822
Epoch 164/300, seasonal_0 Loss: 0.1465 | 0.1822
Epoch 165/300, seasonal_0 Loss: 0.1465 | 0.1822
Epoch 166/300, seasonal_0 Loss: 0.1470 | 0.1822
Epoch 167/300, seasonal_0 Loss: 0.1462 | 0.1822
Epoch 168/300, seasonal_0 Loss: 0.1464 | 0.1821
Epoch 169/300, seasonal_0 Loss: 0.1465 | 0.1821
Epoch 170/300, seasonal_0 Loss: 0.1475 | 0.1821
Epoch 171/300, seasonal_0 Loss: 0.1462 | 0.1821
Epoch 172/300, seasonal_0 Loss: 0.1465 | 0.1821
Epoch 173/300, seasonal_0 Loss: 0.1465 | 0.1820
Epoch 174/300, seasonal_0 Loss: 0.1471 | 0.1820
Epoch 175/300, seasonal_0 Loss: 0.1473 | 0.1820
Epoch 176/300, seasonal_0 Loss: 0.1470 | 0.1820
Epoch 177/300, seasonal_0 Loss: 0.1463 | 0.1820
Epoch 178/300, seasonal_0 Loss: 0.1468 | 0.1820
Epoch 179/300, seasonal_0 Loss: 0.1468 | 0.1819
Epoch 180/300, seasonal_0 Loss: 0.1464 | 0.1819
Epoch 181/300, seasonal_0 Loss: 0.1456 | 0.1819
Epoch 182/300, seasonal_0 Loss: 0.1461 | 0.1819
Epoch 183/300, seasonal_0 Loss: 0.1468 | 0.1819
Epoch 184/300, seasonal_0 Loss: 0.1474 | 0.1819
Epoch 185/300, seasonal_0 Loss: 0.1456 | 0.1818
Epoch 186/300, seasonal_0 Loss: 0.1463 | 0.1818
Epoch 187/300, seasonal_0 Loss: 0.1466 | 0.1818
Epoch 188/300, seasonal_0 Loss: 0.1475 | 0.1818
Epoch 189/300, seasonal_0 Loss: 0.1462 | 0.1818
Epoch 190/300, seasonal_0 Loss: 0.1466 | 0.1818
Epoch 191/300, seasonal_0 Loss: 0.1460 | 0.1818
Epoch 192/300, seasonal_0 Loss: 0.1467 | 0.1818
Epoch 193/300, seasonal_0 Loss: 0.1461 | 0.1818
Epoch 194/300, seasonal_0 Loss: 0.1470 | 0.1818
Epoch 195/300, seasonal_0 Loss: 0.1466 | 0.1818
Epoch 196/300, seasonal_0 Loss: 0.1463 | 0.1818
Epoch 197/300, seasonal_0 Loss: 0.1461 | 0.1818
Epoch 198/300, seasonal_0 Loss: 0.1463 | 0.1818
Epoch 199/300, seasonal_0 Loss: 0.1462 | 0.1818
Epoch 200/300, seasonal_0 Loss: 0.1466 | 0.1817
Epoch 201/300, seasonal_0 Loss: 0.1465 | 0.1817
Epoch 202/300, seasonal_0 Loss: 0.1463 | 0.1817
Epoch 203/300, seasonal_0 Loss: 0.1467 | 0.1817
Epoch 204/300, seasonal_0 Loss: 0.1465 | 0.1817
Epoch 205/300, seasonal_0 Loss: 0.1458 | 0.1817
Epoch 206/300, seasonal_0 Loss: 0.1472 | 0.1817
Epoch 207/300, seasonal_0 Loss: 0.1458 | 0.1817
Epoch 208/300, seasonal_0 Loss: 0.1467 | 0.1817
Epoch 209/300, seasonal_0 Loss: 0.1468 | 0.1817
Epoch 210/300, seasonal_0 Loss: 0.1465 | 0.1817
Epoch 211/300, seasonal_0 Loss: 0.1468 | 0.1817
Epoch 212/300, seasonal_0 Loss: 0.1463 | 0.1817
Epoch 213/300, seasonal_0 Loss: 0.1469 | 0.1817
Epoch 214/300, seasonal_0 Loss: 0.1467 | 0.1817
Epoch 215/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 216/300, seasonal_0 Loss: 0.1473 | 0.1817
Epoch 217/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 218/300, seasonal_0 Loss: 0.1457 | 0.1816
Epoch 219/300, seasonal_0 Loss: 0.1473 | 0.1817
Epoch 220/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 221/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 222/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 223/300, seasonal_0 Loss: 0.1467 | 0.1816
Epoch 224/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 225/300, seasonal_0 Loss: 0.1469 | 0.1816
Epoch 226/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 227/300, seasonal_0 Loss: 0.1457 | 0.1816
Epoch 228/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 229/300, seasonal_0 Loss: 0.1466 | 0.1816
Epoch 230/300, seasonal_0 Loss: 0.1465 | 0.1816
Epoch 231/300, seasonal_0 Loss: 0.1459 | 0.1816
Epoch 232/300, seasonal_0 Loss: 0.1465 | 0.1816
Epoch 233/300, seasonal_0 Loss: 0.1459 | 0.1816
Epoch 234/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 235/300, seasonal_0 Loss: 0.1458 | 0.1816
Epoch 236/300, seasonal_0 Loss: 0.1456 | 0.1816
Epoch 237/300, seasonal_0 Loss: 0.1470 | 0.1816
Epoch 238/300, seasonal_0 Loss: 0.1467 | 0.1816
Epoch 239/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 240/300, seasonal_0 Loss: 0.1464 | 0.1816
Epoch 241/300, seasonal_0 Loss: 0.1457 | 0.1816
Epoch 242/300, seasonal_0 Loss: 0.1465 | 0.1816
Epoch 243/300, seasonal_0 Loss: 0.1464 | 0.1816
Epoch 244/300, seasonal_0 Loss: 0.1460 | 0.1816
Epoch 245/300, seasonal_0 Loss: 0.1467 | 0.1816
Epoch 246/300, seasonal_0 Loss: 0.1469 | 0.1816
Epoch 247/300, seasonal_0 Loss: 0.1471 | 0.1816
Epoch 248/300, seasonal_0 Loss: 0.1471 | 0.1816
Epoch 249/300, seasonal_0 Loss: 0.1469 | 0.1816
Epoch 250/300, seasonal_0 Loss: 0.1465 | 0.1816
Epoch 251/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 252/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 253/300, seasonal_0 Loss: 0.1473 | 0.1816
Epoch 254/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 255/300, seasonal_0 Loss: 0.1466 | 0.1816
Epoch 256/300, seasonal_0 Loss: 0.1467 | 0.1816
Epoch 257/300, seasonal_0 Loss: 0.1459 | 0.1816
Epoch 258/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 259/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 260/300, seasonal_0 Loss: 0.1465 | 0.1816
Epoch 261/300, seasonal_0 Loss: 0.1466 | 0.1816
Epoch 262/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 263/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 264/300, seasonal_0 Loss: 0.1470 | 0.1816
Epoch 265/300, seasonal_0 Loss: 0.1466 | 0.1816
Epoch 266/300, seasonal_0 Loss: 0.1462 | 0.1816
Epoch 267/300, seasonal_0 Loss: 0.1469 | 0.1816
Epoch 268/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 269/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 270/300, seasonal_0 Loss: 0.1472 | 0.1816
Epoch 271/300, seasonal_0 Loss: 0.1463 | 0.1816
Epoch 272/300, seasonal_0 Loss: 0.1468 | 0.1816
Epoch 273/300, seasonal_0 Loss: 0.1466 | 0.1816
Epoch 274/300, seasonal_0 Loss: 0.1464 | 0.1816
Epoch 275/300, seasonal_0 Loss: 0.1468 | 0.1816
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 77, 'train_rates': 0.9899926089026055, 'learning_rate': 0.0002075764683800737, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9080082345919008}
Epoch 1/300, seasonal_1 Loss: 0.7862 | 1.0809
Epoch 2/300, seasonal_1 Loss: 0.6925 | 0.7820
Epoch 3/300, seasonal_1 Loss: 0.5789 | 0.7387
Epoch 4/300, seasonal_1 Loss: 0.5937 | 0.7824
Epoch 5/300, seasonal_1 Loss: 0.5775 | 0.6478
Epoch 6/300, seasonal_1 Loss: 0.5362 | 0.6561
Epoch 7/300, seasonal_1 Loss: 0.4823 | 0.6249
Epoch 8/300, seasonal_1 Loss: 0.3766 | 0.5618
Epoch 9/300, seasonal_1 Loss: 0.3452 | 0.5976
Epoch 10/300, seasonal_1 Loss: 0.3521 | 0.5476
Epoch 11/300, seasonal_1 Loss: 0.3643 | 0.4764
Epoch 12/300, seasonal_1 Loss: 0.3162 | 0.4531
Epoch 13/300, seasonal_1 Loss: 0.3064 | 0.4825
Epoch 14/300, seasonal_1 Loss: 0.2978 | 0.4253
Epoch 15/300, seasonal_1 Loss: 0.2768 | 0.3985
Epoch 16/300, seasonal_1 Loss: 0.2837 | 0.4195
Epoch 17/300, seasonal_1 Loss: 0.2763 | 0.4100
Epoch 18/300, seasonal_1 Loss: 0.2629 | 0.3650
Epoch 19/300, seasonal_1 Loss: 0.2511 | 0.3465
Epoch 20/300, seasonal_1 Loss: 0.2564 | 0.4045
Epoch 21/300, seasonal_1 Loss: 0.2437 | 0.3401
Epoch 22/300, seasonal_1 Loss: 0.2573 | 0.3449
Epoch 23/300, seasonal_1 Loss: 0.2355 | 0.3331
Epoch 24/300, seasonal_1 Loss: 0.2366 | 0.3545
Epoch 25/300, seasonal_1 Loss: 0.2233 | 0.3164
Epoch 26/300, seasonal_1 Loss: 0.2296 | 0.3120
Epoch 27/300, seasonal_1 Loss: 0.2394 | 0.3284
Epoch 28/300, seasonal_1 Loss: 0.2052 | 0.3061
Epoch 29/300, seasonal_1 Loss: 0.2010 | 0.3263
Epoch 30/300, seasonal_1 Loss: 0.2025 | 0.2943
Epoch 31/300, seasonal_1 Loss: 0.1967 | 0.3007
Epoch 32/300, seasonal_1 Loss: 0.1981 | 0.2986
Epoch 33/300, seasonal_1 Loss: 0.1955 | 0.2874
Epoch 34/300, seasonal_1 Loss: 0.1904 | 0.2795
Epoch 35/300, seasonal_1 Loss: 0.1844 | 0.2879
Epoch 36/300, seasonal_1 Loss: 0.1800 | 0.2753
Epoch 37/300, seasonal_1 Loss: 0.1756 | 0.2725
Epoch 38/300, seasonal_1 Loss: 0.1718 | 0.2677
Epoch 39/300, seasonal_1 Loss: 0.1705 | 0.2636
Epoch 40/300, seasonal_1 Loss: 0.1701 | 0.2679
Epoch 41/300, seasonal_1 Loss: 0.1664 | 0.2498
Epoch 42/300, seasonal_1 Loss: 0.1651 | 0.2489
Epoch 43/300, seasonal_1 Loss: 0.1720 | 0.2894
Epoch 44/300, seasonal_1 Loss: 0.1661 | 0.2441
Epoch 45/300, seasonal_1 Loss: 0.1687 | 0.2485
Epoch 46/300, seasonal_1 Loss: 0.1787 | 0.2873
Epoch 47/300, seasonal_1 Loss: 0.1641 | 0.2431
Epoch 48/300, seasonal_1 Loss: 0.1660 | 0.2365
Epoch 49/300, seasonal_1 Loss: 0.1674 | 0.2683
Epoch 50/300, seasonal_1 Loss: 0.1592 | 0.2501
Epoch 51/300, seasonal_1 Loss: 0.1605 | 0.2287
Epoch 52/300, seasonal_1 Loss: 0.1588 | 0.2357
Epoch 53/300, seasonal_1 Loss: 0.1534 | 0.2555
Epoch 54/300, seasonal_1 Loss: 0.1522 | 0.2271
Epoch 55/300, seasonal_1 Loss: 0.1512 | 0.2286
Epoch 56/300, seasonal_1 Loss: 0.1500 | 0.2271
Epoch 57/300, seasonal_1 Loss: 0.1498 | 0.2291
Epoch 58/300, seasonal_1 Loss: 0.1484 | 0.2213
Epoch 59/300, seasonal_1 Loss: 0.1454 | 0.2124
Epoch 60/300, seasonal_1 Loss: 0.1458 | 0.2273
Epoch 61/300, seasonal_1 Loss: 0.1437 | 0.2128
Epoch 62/300, seasonal_1 Loss: 0.1430 | 0.2082
Epoch 63/300, seasonal_1 Loss: 0.1428 | 0.2233
Epoch 64/300, seasonal_1 Loss: 0.1406 | 0.2077
Epoch 65/300, seasonal_1 Loss: 0.1413 | 0.2011
Epoch 66/300, seasonal_1 Loss: 0.1410 | 0.2173
Epoch 67/300, seasonal_1 Loss: 0.1389 | 0.2092
Epoch 68/300, seasonal_1 Loss: 0.1381 | 0.1992
Epoch 69/300, seasonal_1 Loss: 0.1374 | 0.2093
Epoch 70/300, seasonal_1 Loss: 0.1360 | 0.2029
Epoch 71/300, seasonal_1 Loss: 0.1361 | 0.1954
Epoch 72/300, seasonal_1 Loss: 0.1359 | 0.2027
Epoch 73/300, seasonal_1 Loss: 0.1343 | 0.1954
Epoch 74/300, seasonal_1 Loss: 0.1342 | 0.1955
Epoch 75/300, seasonal_1 Loss: 0.1333 | 0.1970
Epoch 76/300, seasonal_1 Loss: 0.1323 | 0.1935
Epoch 77/300, seasonal_1 Loss: 0.1323 | 0.1932
Epoch 78/300, seasonal_1 Loss: 0.1313 | 0.1920
Epoch 79/300, seasonal_1 Loss: 0.1305 | 0.1883
Epoch 80/300, seasonal_1 Loss: 0.1300 | 0.1890
Epoch 81/300, seasonal_1 Loss: 0.1291 | 0.1915
Epoch 82/300, seasonal_1 Loss: 0.1292 | 0.1839
Epoch 83/300, seasonal_1 Loss: 0.1291 | 0.1873
Epoch 84/300, seasonal_1 Loss: 0.1279 | 0.1817
Epoch 85/300, seasonal_1 Loss: 0.1275 | 0.1827
Epoch 86/300, seasonal_1 Loss: 0.1268 | 0.1840
Epoch 87/300, seasonal_1 Loss: 0.1276 | 0.1781
Epoch 88/300, seasonal_1 Loss: 0.1267 | 0.1807
Epoch 89/300, seasonal_1 Loss: 0.1266 | 0.1838
Epoch 90/300, seasonal_1 Loss: 0.1249 | 0.1794
Epoch 91/300, seasonal_1 Loss: 0.1249 | 0.1754
Epoch 92/300, seasonal_1 Loss: 0.1247 | 0.1817
Epoch 93/300, seasonal_1 Loss: 0.1233 | 0.1752
Epoch 94/300, seasonal_1 Loss: 0.1248 | 0.1773
Epoch 95/300, seasonal_1 Loss: 0.1235 | 0.1794
Epoch 96/300, seasonal_1 Loss: 0.1231 | 0.1738
Epoch 97/300, seasonal_1 Loss: 0.1223 | 0.1763
Epoch 98/300, seasonal_1 Loss: 0.1227 | 0.1727
Epoch 99/300, seasonal_1 Loss: 0.1217 | 0.1763
Epoch 100/300, seasonal_1 Loss: 0.1216 | 0.1743
Epoch 101/300, seasonal_1 Loss: 0.1211 | 0.1707
Epoch 102/300, seasonal_1 Loss: 0.1203 | 0.1720
Epoch 103/300, seasonal_1 Loss: 0.1211 | 0.1737
Epoch 104/300, seasonal_1 Loss: 0.1207 | 0.1692
Epoch 105/300, seasonal_1 Loss: 0.1200 | 0.1689
Epoch 106/300, seasonal_1 Loss: 0.1196 | 0.1734
Epoch 107/300, seasonal_1 Loss: 0.1199 | 0.1696
Epoch 108/300, seasonal_1 Loss: 0.1193 | 0.1718
Epoch 109/300, seasonal_1 Loss: 0.1181 | 0.1684
Epoch 110/300, seasonal_1 Loss: 0.1185 | 0.1695
Epoch 111/300, seasonal_1 Loss: 0.1185 | 0.1689
Epoch 112/300, seasonal_1 Loss: 0.1178 | 0.1668
Epoch 113/300, seasonal_1 Loss: 0.1173 | 0.1670
Epoch 114/300, seasonal_1 Loss: 0.1175 | 0.1667
Epoch 115/300, seasonal_1 Loss: 0.1180 | 0.1661
Epoch 116/300, seasonal_1 Loss: 0.1167 | 0.1651
Epoch 117/300, seasonal_1 Loss: 0.1165 | 0.1658
Epoch 118/300, seasonal_1 Loss: 0.1162 | 0.1670
Epoch 119/300, seasonal_1 Loss: 0.1167 | 0.1661
Epoch 120/300, seasonal_1 Loss: 0.1160 | 0.1647
Epoch 121/300, seasonal_1 Loss: 0.1160 | 0.1662
Epoch 122/300, seasonal_1 Loss: 0.1159 | 0.1663
Epoch 123/300, seasonal_1 Loss: 0.1156 | 0.1657
Epoch 124/300, seasonal_1 Loss: 0.1155 | 0.1648
Epoch 125/300, seasonal_1 Loss: 0.1149 | 0.1638
Epoch 126/300, seasonal_1 Loss: 0.1149 | 0.1632
Epoch 127/300, seasonal_1 Loss: 0.1146 | 0.1633
Epoch 128/300, seasonal_1 Loss: 0.1144 | 0.1640
Epoch 129/300, seasonal_1 Loss: 0.1139 | 0.1639
Epoch 130/300, seasonal_1 Loss: 0.1139 | 0.1620
Epoch 131/300, seasonal_1 Loss: 0.1138 | 0.1632
Epoch 132/300, seasonal_1 Loss: 0.1136 | 0.1631
Epoch 133/300, seasonal_1 Loss: 0.1135 | 0.1628
Epoch 134/300, seasonal_1 Loss: 0.1133 | 0.1630
Epoch 135/300, seasonal_1 Loss: 0.1136 | 0.1622
Epoch 136/300, seasonal_1 Loss: 0.1133 | 0.1603
Epoch 137/300, seasonal_1 Loss: 0.1136 | 0.1601
Epoch 138/300, seasonal_1 Loss: 0.1129 | 0.1607
Epoch 139/300, seasonal_1 Loss: 0.1130 | 0.1614
Epoch 140/300, seasonal_1 Loss: 0.1123 | 0.1607
Epoch 141/300, seasonal_1 Loss: 0.1124 | 0.1598
Epoch 142/300, seasonal_1 Loss: 0.1123 | 0.1597
Epoch 143/300, seasonal_1 Loss: 0.1128 | 0.1592
Epoch 144/300, seasonal_1 Loss: 0.1132 | 0.1589
Epoch 145/300, seasonal_1 Loss: 0.1120 | 0.1587
Epoch 146/300, seasonal_1 Loss: 0.1128 | 0.1599
Epoch 147/300, seasonal_1 Loss: 0.1117 | 0.1590
Epoch 148/300, seasonal_1 Loss: 0.1117 | 0.1587
Epoch 149/300, seasonal_1 Loss: 0.1118 | 0.1584
Epoch 150/300, seasonal_1 Loss: 0.1124 | 0.1584
Epoch 151/300, seasonal_1 Loss: 0.1118 | 0.1587
Epoch 152/300, seasonal_1 Loss: 0.1120 | 0.1579
Epoch 153/300, seasonal_1 Loss: 0.1125 | 0.1580
Epoch 154/300, seasonal_1 Loss: 0.1117 | 0.1577
Epoch 155/300, seasonal_1 Loss: 0.1113 | 0.1576
Epoch 156/300, seasonal_1 Loss: 0.1103 | 0.1566
Epoch 157/300, seasonal_1 Loss: 0.1115 | 0.1577
Epoch 158/300, seasonal_1 Loss: 0.1118 | 0.1576
Epoch 159/300, seasonal_1 Loss: 0.1106 | 0.1568
Epoch 160/300, seasonal_1 Loss: 0.1109 | 0.1575
Epoch 161/300, seasonal_1 Loss: 0.1112 | 0.1582
Epoch 162/300, seasonal_1 Loss: 0.1118 | 0.1580
Epoch 163/300, seasonal_1 Loss: 0.1107 | 0.1575
Epoch 164/300, seasonal_1 Loss: 0.1120 | 0.1574
Epoch 165/300, seasonal_1 Loss: 0.1105 | 0.1578
Epoch 166/300, seasonal_1 Loss: 0.1099 | 0.1570
Epoch 167/300, seasonal_1 Loss: 0.1100 | 0.1569
Epoch 168/300, seasonal_1 Loss: 0.1107 | 0.1566
Epoch 169/300, seasonal_1 Loss: 0.1115 | 0.1566
Epoch 170/300, seasonal_1 Loss: 0.1098 | 0.1565
Epoch 171/300, seasonal_1 Loss: 0.1105 | 0.1556
Epoch 172/300, seasonal_1 Loss: 0.1095 | 0.1560
Epoch 173/300, seasonal_1 Loss: 0.1108 | 0.1559
Epoch 174/300, seasonal_1 Loss: 0.1108 | 0.1564
Epoch 175/300, seasonal_1 Loss: 0.1102 | 0.1570
Epoch 176/300, seasonal_1 Loss: 0.1103 | 0.1571
Epoch 177/300, seasonal_1 Loss: 0.1101 | 0.1567
Epoch 178/300, seasonal_1 Loss: 0.1101 | 0.1561
Epoch 179/300, seasonal_1 Loss: 0.1105 | 0.1560
Epoch 180/300, seasonal_1 Loss: 0.1104 | 0.1559
Epoch 181/300, seasonal_1 Loss: 0.1106 | 0.1561
Epoch 182/300, seasonal_1 Loss: 0.1104 | 0.1563
Epoch 183/300, seasonal_1 Loss: 0.1091 | 0.1565
Epoch 184/300, seasonal_1 Loss: 0.1111 | 0.1566
Epoch 185/300, seasonal_1 Loss: 0.1099 | 0.1569
Epoch 186/300, seasonal_1 Loss: 0.1091 | 0.1569
Epoch 187/300, seasonal_1 Loss: 0.1106 | 0.1567
Epoch 188/300, seasonal_1 Loss: 0.1097 | 0.1563
Epoch 189/300, seasonal_1 Loss: 0.1093 | 0.1558
Epoch 190/300, seasonal_1 Loss: 0.1096 | 0.1562
Epoch 191/300, seasonal_1 Loss: 0.1093 | 0.1561
Epoch 192/300, seasonal_1 Loss: 0.1099 | 0.1559
Epoch 193/300, seasonal_1 Loss: 0.1097 | 0.1561
Epoch 194/300, seasonal_1 Loss: 0.1105 | 0.1560
Epoch 195/300, seasonal_1 Loss: 0.1093 | 0.1555
Epoch 196/300, seasonal_1 Loss: 0.1094 | 0.1555
Epoch 197/300, seasonal_1 Loss: 0.1088 | 0.1556
Epoch 198/300, seasonal_1 Loss: 0.1093 | 0.1558
Epoch 199/300, seasonal_1 Loss: 0.1092 | 0.1561
Epoch 200/300, seasonal_1 Loss: 0.1095 | 0.1562
Epoch 201/300, seasonal_1 Loss: 0.1099 | 0.1561
Epoch 202/300, seasonal_1 Loss: 0.1093 | 0.1561
Epoch 203/300, seasonal_1 Loss: 0.1087 | 0.1556
Epoch 204/300, seasonal_1 Loss: 0.1092 | 0.1554
Epoch 205/300, seasonal_1 Loss: 0.1095 | 0.1554
Epoch 206/300, seasonal_1 Loss: 0.1095 | 0.1555
Epoch 207/300, seasonal_1 Loss: 0.1098 | 0.1554
Epoch 208/300, seasonal_1 Loss: 0.1095 | 0.1552
Epoch 209/300, seasonal_1 Loss: 0.1091 | 0.1552
Epoch 210/300, seasonal_1 Loss: 0.1094 | 0.1551
Epoch 211/300, seasonal_1 Loss: 0.1101 | 0.1554
Epoch 212/300, seasonal_1 Loss: 0.1100 | 0.1553
Epoch 213/300, seasonal_1 Loss: 0.1091 | 0.1553
Epoch 214/300, seasonal_1 Loss: 0.1083 | 0.1555
Epoch 215/300, seasonal_1 Loss: 0.1082 | 0.1554
Epoch 216/300, seasonal_1 Loss: 0.1093 | 0.1552
Epoch 217/300, seasonal_1 Loss: 0.1086 | 0.1550
Epoch 218/300, seasonal_1 Loss: 0.1091 | 0.1551
Epoch 219/300, seasonal_1 Loss: 0.1094 | 0.1553
Epoch 220/300, seasonal_1 Loss: 0.1092 | 0.1554
Epoch 221/300, seasonal_1 Loss: 0.1091 | 0.1556
Epoch 222/300, seasonal_1 Loss: 0.1088 | 0.1555
Epoch 223/300, seasonal_1 Loss: 0.1087 | 0.1554
Epoch 224/300, seasonal_1 Loss: 0.1086 | 0.1551
Epoch 225/300, seasonal_1 Loss: 0.1097 | 0.1550
Epoch 226/300, seasonal_1 Loss: 0.1090 | 0.1549
Epoch 227/300, seasonal_1 Loss: 0.1085 | 0.1550
Epoch 228/300, seasonal_1 Loss: 0.1095 | 0.1551
Epoch 229/300, seasonal_1 Loss: 0.1093 | 0.1550
Epoch 230/300, seasonal_1 Loss: 0.1099 | 0.1551
Epoch 231/300, seasonal_1 Loss: 0.1084 | 0.1551
Epoch 232/300, seasonal_1 Loss: 0.1086 | 0.1550
Epoch 233/300, seasonal_1 Loss: 0.1084 | 0.1550
Epoch 234/300, seasonal_1 Loss: 0.1088 | 0.1551
Epoch 235/300, seasonal_1 Loss: 0.1082 | 0.1552
Epoch 236/300, seasonal_1 Loss: 0.1091 | 0.1553
Epoch 237/300, seasonal_1 Loss: 0.1085 | 0.1551
Epoch 238/300, seasonal_1 Loss: 0.1089 | 0.1552
Epoch 239/300, seasonal_1 Loss: 0.1088 | 0.1553
Epoch 240/300, seasonal_1 Loss: 0.1085 | 0.1552
Epoch 241/300, seasonal_1 Loss: 0.1093 | 0.1552
Epoch 242/300, seasonal_1 Loss: 0.1089 | 0.1551
Epoch 243/300, seasonal_1 Loss: 0.1091 | 0.1550
Epoch 244/300, seasonal_1 Loss: 0.1092 | 0.1550
Epoch 245/300, seasonal_1 Loss: 0.1091 | 0.1550
Epoch 246/300, seasonal_1 Loss: 0.1079 | 0.1549
Epoch 247/300, seasonal_1 Loss: 0.1087 | 0.1549
Epoch 248/300, seasonal_1 Loss: 0.1096 | 0.1549
Epoch 249/300, seasonal_1 Loss: 0.1083 | 0.1550
Epoch 250/300, seasonal_1 Loss: 0.1086 | 0.1549
Epoch 251/300, seasonal_1 Loss: 0.1085 | 0.1549
Epoch 252/300, seasonal_1 Loss: 0.1096 | 0.1548
Epoch 253/300, seasonal_1 Loss: 0.1094 | 0.1547
Epoch 254/300, seasonal_1 Loss: 0.1084 | 0.1547
Epoch 255/300, seasonal_1 Loss: 0.1081 | 0.1548
Epoch 256/300, seasonal_1 Loss: 0.1086 | 0.1547
Epoch 257/300, seasonal_1 Loss: 0.1089 | 0.1548
Epoch 258/300, seasonal_1 Loss: 0.1089 | 0.1548
Epoch 259/300, seasonal_1 Loss: 0.1087 | 0.1548
Epoch 260/300, seasonal_1 Loss: 0.1089 | 0.1548
Epoch 261/300, seasonal_1 Loss: 0.1091 | 0.1548
Epoch 262/300, seasonal_1 Loss: 0.1084 | 0.1547
Epoch 263/300, seasonal_1 Loss: 0.1091 | 0.1547
Epoch 264/300, seasonal_1 Loss: 0.1088 | 0.1547
Epoch 265/300, seasonal_1 Loss: 0.1085 | 0.1547
Epoch 266/300, seasonal_1 Loss: 0.1087 | 0.1547
Epoch 267/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 268/300, seasonal_1 Loss: 0.1088 | 0.1546
Epoch 269/300, seasonal_1 Loss: 0.1090 | 0.1546
Epoch 270/300, seasonal_1 Loss: 0.1084 | 0.1546
Epoch 271/300, seasonal_1 Loss: 0.1080 | 0.1546
Epoch 272/300, seasonal_1 Loss: 0.1089 | 0.1546
Epoch 273/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 274/300, seasonal_1 Loss: 0.1084 | 0.1546
Epoch 275/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 276/300, seasonal_1 Loss: 0.1089 | 0.1546
Epoch 277/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 278/300, seasonal_1 Loss: 0.1091 | 0.1546
Epoch 279/300, seasonal_1 Loss: 0.1088 | 0.1546
Epoch 280/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 281/300, seasonal_1 Loss: 0.1090 | 0.1546
Epoch 282/300, seasonal_1 Loss: 0.1094 | 0.1545
Epoch 283/300, seasonal_1 Loss: 0.1093 | 0.1546
Epoch 284/300, seasonal_1 Loss: 0.1084 | 0.1546
Epoch 285/300, seasonal_1 Loss: 0.1097 | 0.1546
Epoch 286/300, seasonal_1 Loss: 0.1092 | 0.1546
Epoch 287/300, seasonal_1 Loss: 0.1081 | 0.1546
Epoch 288/300, seasonal_1 Loss: 0.1090 | 0.1546
Epoch 289/300, seasonal_1 Loss: 0.1078 | 0.1545
Epoch 290/300, seasonal_1 Loss: 0.1087 | 0.1545
Epoch 291/300, seasonal_1 Loss: 0.1089 | 0.1545
Epoch 292/300, seasonal_1 Loss: 0.1091 | 0.1546
Epoch 293/300, seasonal_1 Loss: 0.1094 | 0.1546
Epoch 294/300, seasonal_1 Loss: 0.1089 | 0.1546
Epoch 295/300, seasonal_1 Loss: 0.1090 | 0.1546
Epoch 296/300, seasonal_1 Loss: 0.1085 | 0.1546
Epoch 297/300, seasonal_1 Loss: 0.1088 | 0.1546
Epoch 298/300, seasonal_1 Loss: 0.1088 | 0.1546
Epoch 299/300, seasonal_1 Loss: 0.1086 | 0.1546
Epoch 300/300, seasonal_1 Loss: 0.1083 | 0.1546
Training seasonal_2 component with params: {'observation_period_num': 132, 'train_rates': 0.9824556206173333, 'learning_rate': 0.0005793586752285613, 'batch_size': 244, 'step_size': 5, 'gamma': 0.8744941284357831}
Epoch 1/300, seasonal_2 Loss: 1.1584 | 1.7371
Epoch 2/300, seasonal_2 Loss: 0.9676 | 1.1341
Epoch 3/300, seasonal_2 Loss: 0.7839 | 1.0091
Epoch 4/300, seasonal_2 Loss: 0.6602 | 0.8438
Epoch 5/300, seasonal_2 Loss: 0.6857 | 0.8143
Epoch 6/300, seasonal_2 Loss: 0.7259 | 0.7382
Epoch 7/300, seasonal_2 Loss: 0.5880 | 0.7205
Epoch 8/300, seasonal_2 Loss: 0.5699 | 0.6951
Epoch 9/300, seasonal_2 Loss: 0.5478 | 0.6662
Epoch 10/300, seasonal_2 Loss: 0.6485 | 0.8809
Epoch 11/300, seasonal_2 Loss: 0.5587 | 0.7285
Epoch 12/300, seasonal_2 Loss: 0.5000 | 0.6168
Epoch 13/300, seasonal_2 Loss: 0.4572 | 0.5679
Epoch 14/300, seasonal_2 Loss: 0.4144 | 0.5495
Epoch 15/300, seasonal_2 Loss: 0.3954 | 0.5385
Epoch 16/300, seasonal_2 Loss: 0.3794 | 0.5252
Epoch 17/300, seasonal_2 Loss: 0.3728 | 0.5083
Epoch 18/300, seasonal_2 Loss: 0.3637 | 0.4920
Epoch 19/300, seasonal_2 Loss: 0.3415 | 0.4737
Epoch 20/300, seasonal_2 Loss: 0.3284 | 0.4683
Epoch 21/300, seasonal_2 Loss: 0.3150 | 0.4517
Epoch 22/300, seasonal_2 Loss: 0.3045 | 0.4435
Epoch 23/300, seasonal_2 Loss: 0.2976 | 0.4285
Epoch 24/300, seasonal_2 Loss: 0.2954 | 0.4179
Epoch 25/300, seasonal_2 Loss: 0.3062 | 0.4077
Epoch 26/300, seasonal_2 Loss: 0.2821 | 0.3999
Epoch 27/300, seasonal_2 Loss: 0.2709 | 0.3935
Epoch 28/300, seasonal_2 Loss: 0.2631 | 0.3802
Epoch 29/300, seasonal_2 Loss: 0.2588 | 0.3723
Epoch 30/300, seasonal_2 Loss: 0.2540 | 0.3671
Epoch 31/300, seasonal_2 Loss: 0.2516 | 0.3639
Epoch 32/300, seasonal_2 Loss: 0.2473 | 0.3597
Epoch 33/300, seasonal_2 Loss: 0.2444 | 0.3559
Epoch 34/300, seasonal_2 Loss: 0.2410 | 0.3528
Epoch 35/300, seasonal_2 Loss: 0.2396 | 0.3498
Epoch 36/300, seasonal_2 Loss: 0.2367 | 0.3457
Epoch 37/300, seasonal_2 Loss: 0.2345 | 0.3435
Epoch 38/300, seasonal_2 Loss: 0.2323 | 0.3407
Epoch 39/300, seasonal_2 Loss: 0.2305 | 0.3381
Epoch 40/300, seasonal_2 Loss: 0.2306 | 0.3356
Epoch 41/300, seasonal_2 Loss: 0.2276 | 0.3333
Epoch 42/300, seasonal_2 Loss: 0.2269 | 0.3310
Epoch 43/300, seasonal_2 Loss: 0.2254 | 0.3291
Epoch 44/300, seasonal_2 Loss: 0.2244 | 0.3262
Epoch 45/300, seasonal_2 Loss: 0.2236 | 0.3246
Epoch 46/300, seasonal_2 Loss: 0.2213 | 0.3225
Epoch 47/300, seasonal_2 Loss: 0.2208 | 0.3208
Epoch 48/300, seasonal_2 Loss: 0.2203 | 0.3186
Epoch 49/300, seasonal_2 Loss: 0.2184 | 0.3169
Epoch 50/300, seasonal_2 Loss: 0.2184 | 0.3150
Epoch 51/300, seasonal_2 Loss: 0.2169 | 0.3147
Epoch 52/300, seasonal_2 Loss: 0.2161 | 0.3136
Epoch 53/300, seasonal_2 Loss: 0.2161 | 0.3118
Epoch 54/300, seasonal_2 Loss: 0.2147 | 0.3095
Epoch 55/300, seasonal_2 Loss: 0.2140 | 0.3077
Epoch 56/300, seasonal_2 Loss: 0.2147 | 0.3075
Epoch 57/300, seasonal_2 Loss: 0.2144 | 0.3067
Epoch 58/300, seasonal_2 Loss: 0.2134 | 0.3061
Epoch 59/300, seasonal_2 Loss: 0.2130 | 0.3051
Epoch 60/300, seasonal_2 Loss: 0.2116 | 0.3040
Epoch 61/300, seasonal_2 Loss: 0.2112 | 0.3029
Epoch 62/300, seasonal_2 Loss: 0.2119 | 0.3019
Epoch 63/300, seasonal_2 Loss: 0.2106 | 0.3008
Epoch 64/300, seasonal_2 Loss: 0.2097 | 0.2997
Epoch 65/300, seasonal_2 Loss: 0.2097 | 0.2989
Epoch 66/300, seasonal_2 Loss: 0.2098 | 0.2981
Epoch 67/300, seasonal_2 Loss: 0.2095 | 0.2972
Epoch 68/300, seasonal_2 Loss: 0.2090 | 0.2965
Epoch 69/300, seasonal_2 Loss: 0.2084 | 0.2956
Epoch 70/300, seasonal_2 Loss: 0.2088 | 0.2952
Epoch 71/300, seasonal_2 Loss: 0.2086 | 0.2952
Epoch 72/300, seasonal_2 Loss: 0.2077 | 0.2946
Epoch 73/300, seasonal_2 Loss: 0.2086 | 0.2942
Epoch 74/300, seasonal_2 Loss: 0.2078 | 0.2938
Epoch 75/300, seasonal_2 Loss: 0.2077 | 0.2933
Epoch 76/300, seasonal_2 Loss: 0.2065 | 0.2928
Epoch 77/300, seasonal_2 Loss: 0.2069 | 0.2923
Epoch 78/300, seasonal_2 Loss: 0.2063 | 0.2921
Epoch 79/300, seasonal_2 Loss: 0.2069 | 0.2920
Epoch 80/300, seasonal_2 Loss: 0.2058 | 0.2917
Epoch 81/300, seasonal_2 Loss: 0.2066 | 0.2915
Epoch 82/300, seasonal_2 Loss: 0.2059 | 0.2911
Epoch 83/300, seasonal_2 Loss: 0.2059 | 0.2908
Epoch 84/300, seasonal_2 Loss: 0.2061 | 0.2904
Epoch 85/300, seasonal_2 Loss: 0.2056 | 0.2901
Epoch 86/300, seasonal_2 Loss: 0.2055 | 0.2900
Epoch 87/300, seasonal_2 Loss: 0.2056 | 0.2899
Epoch 88/300, seasonal_2 Loss: 0.2051 | 0.2897
Epoch 89/300, seasonal_2 Loss: 0.2046 | 0.2895
Epoch 90/300, seasonal_2 Loss: 0.2048 | 0.2891
Epoch 91/300, seasonal_2 Loss: 0.2049 | 0.2888
Epoch 92/300, seasonal_2 Loss: 0.2053 | 0.2887
Epoch 93/300, seasonal_2 Loss: 0.2039 | 0.2885
Epoch 94/300, seasonal_2 Loss: 0.2045 | 0.2884
Epoch 95/300, seasonal_2 Loss: 0.2049 | 0.2883
Epoch 96/300, seasonal_2 Loss: 0.2047 | 0.2881
Epoch 97/300, seasonal_2 Loss: 0.2046 | 0.2878
Epoch 98/300, seasonal_2 Loss: 0.2051 | 0.2877
Epoch 99/300, seasonal_2 Loss: 0.2046 | 0.2876
Epoch 100/300, seasonal_2 Loss: 0.2036 | 0.2876
Epoch 101/300, seasonal_2 Loss: 0.2045 | 0.2875
Epoch 102/300, seasonal_2 Loss: 0.2043 | 0.2874
Epoch 103/300, seasonal_2 Loss: 0.2038 | 0.2872
Epoch 104/300, seasonal_2 Loss: 0.2045 | 0.2872
Epoch 105/300, seasonal_2 Loss: 0.2036 | 0.2871
Epoch 106/300, seasonal_2 Loss: 0.2043 | 0.2870
Epoch 107/300, seasonal_2 Loss: 0.2040 | 0.2869
Epoch 108/300, seasonal_2 Loss: 0.2039 | 0.2868
Epoch 109/300, seasonal_2 Loss: 0.2038 | 0.2868
Epoch 110/300, seasonal_2 Loss: 0.2042 | 0.2868
Epoch 111/300, seasonal_2 Loss: 0.2036 | 0.2867
Epoch 112/300, seasonal_2 Loss: 0.2043 | 0.2866
Epoch 113/300, seasonal_2 Loss: 0.2041 | 0.2865
Epoch 114/300, seasonal_2 Loss: 0.2033 | 0.2864
Epoch 115/300, seasonal_2 Loss: 0.2035 | 0.2864
Epoch 116/300, seasonal_2 Loss: 0.2038 | 0.2863
Epoch 117/300, seasonal_2 Loss: 0.2040 | 0.2862
Epoch 118/300, seasonal_2 Loss: 0.2037 | 0.2861
Epoch 119/300, seasonal_2 Loss: 0.2034 | 0.2861
Epoch 120/300, seasonal_2 Loss: 0.2033 | 0.2860
Epoch 121/300, seasonal_2 Loss: 0.2039 | 0.2859
Epoch 122/300, seasonal_2 Loss: 0.2038 | 0.2859
Epoch 123/300, seasonal_2 Loss: 0.2029 | 0.2859
Epoch 124/300, seasonal_2 Loss: 0.2036 | 0.2858
Epoch 125/300, seasonal_2 Loss: 0.2040 | 0.2858
Epoch 126/300, seasonal_2 Loss: 0.2036 | 0.2857
Epoch 127/300, seasonal_2 Loss: 0.2040 | 0.2857
Epoch 128/300, seasonal_2 Loss: 0.2036 | 0.2857
Epoch 129/300, seasonal_2 Loss: 0.2031 | 0.2857
Epoch 130/300, seasonal_2 Loss: 0.2035 | 0.2857
Epoch 131/300, seasonal_2 Loss: 0.2041 | 0.2856
Epoch 132/300, seasonal_2 Loss: 0.2040 | 0.2856
Epoch 133/300, seasonal_2 Loss: 0.2038 | 0.2856
Epoch 134/300, seasonal_2 Loss: 0.2029 | 0.2856
Epoch 135/300, seasonal_2 Loss: 0.2027 | 0.2855
Epoch 136/300, seasonal_2 Loss: 0.2028 | 0.2855
Epoch 137/300, seasonal_2 Loss: 0.2035 | 0.2855
Epoch 138/300, seasonal_2 Loss: 0.2032 | 0.2855
Epoch 139/300, seasonal_2 Loss: 0.2032 | 0.2855
Epoch 140/300, seasonal_2 Loss: 0.2027 | 0.2854
Epoch 141/300, seasonal_2 Loss: 0.2027 | 0.2854
Epoch 142/300, seasonal_2 Loss: 0.2032 | 0.2854
Epoch 143/300, seasonal_2 Loss: 0.2032 | 0.2854
Epoch 144/300, seasonal_2 Loss: 0.2031 | 0.2854
Epoch 145/300, seasonal_2 Loss: 0.2022 | 0.2854
Epoch 146/300, seasonal_2 Loss: 0.2034 | 0.2854
Epoch 147/300, seasonal_2 Loss: 0.2031 | 0.2854
Epoch 148/300, seasonal_2 Loss: 0.2034 | 0.2853
Epoch 149/300, seasonal_2 Loss: 0.2030 | 0.2853
Epoch 150/300, seasonal_2 Loss: 0.2025 | 0.2853
Epoch 151/300, seasonal_2 Loss: 0.2027 | 0.2853
Epoch 152/300, seasonal_2 Loss: 0.2032 | 0.2853
Epoch 153/300, seasonal_2 Loss: 0.2028 | 0.2853
Epoch 154/300, seasonal_2 Loss: 0.2029 | 0.2853
Epoch 155/300, seasonal_2 Loss: 0.2030 | 0.2853
Epoch 156/300, seasonal_2 Loss: 0.2033 | 0.2853
Epoch 157/300, seasonal_2 Loss: 0.2024 | 0.2853
Epoch 158/300, seasonal_2 Loss: 0.2036 | 0.2853
Epoch 159/300, seasonal_2 Loss: 0.2034 | 0.2852
Epoch 160/300, seasonal_2 Loss: 0.2028 | 0.2852
Epoch 161/300, seasonal_2 Loss: 0.2029 | 0.2852
Epoch 162/300, seasonal_2 Loss: 0.2028 | 0.2852
Epoch 163/300, seasonal_2 Loss: 0.2024 | 0.2852
Epoch 164/300, seasonal_2 Loss: 0.2029 | 0.2852
Epoch 165/300, seasonal_2 Loss: 0.2030 | 0.2852
Epoch 166/300, seasonal_2 Loss: 0.2025 | 0.2852
Epoch 167/300, seasonal_2 Loss: 0.2031 | 0.2852
Epoch 168/300, seasonal_2 Loss: 0.2024 | 0.2852
Epoch 169/300, seasonal_2 Loss: 0.2033 | 0.2852
Epoch 170/300, seasonal_2 Loss: 0.2031 | 0.2852
Epoch 171/300, seasonal_2 Loss: 0.2038 | 0.2852
Epoch 172/300, seasonal_2 Loss: 0.2026 | 0.2852
Epoch 173/300, seasonal_2 Loss: 0.2033 | 0.2852
Epoch 174/300, seasonal_2 Loss: 0.2032 | 0.2852
Epoch 175/300, seasonal_2 Loss: 0.2025 | 0.2852
Epoch 176/300, seasonal_2 Loss: 0.2041 | 0.2852
Epoch 177/300, seasonal_2 Loss: 0.2030 | 0.2852
Epoch 178/300, seasonal_2 Loss: 0.2031 | 0.2852
Epoch 179/300, seasonal_2 Loss: 0.2026 | 0.2852
Epoch 180/300, seasonal_2 Loss: 0.2030 | 0.2852
Epoch 181/300, seasonal_2 Loss: 0.2030 | 0.2851
Epoch 182/300, seasonal_2 Loss: 0.2031 | 0.2851
Epoch 183/300, seasonal_2 Loss: 0.2032 | 0.2851
Epoch 184/300, seasonal_2 Loss: 0.2029 | 0.2851
Epoch 185/300, seasonal_2 Loss: 0.2027 | 0.2851
Epoch 186/300, seasonal_2 Loss: 0.2030 | 0.2851
Epoch 187/300, seasonal_2 Loss: 0.2031 | 0.2851
Epoch 188/300, seasonal_2 Loss: 0.2026 | 0.2851
Epoch 189/300, seasonal_2 Loss: 0.2032 | 0.2851
Epoch 190/300, seasonal_2 Loss: 0.2038 | 0.2851
Epoch 191/300, seasonal_2 Loss: 0.2031 | 0.2851
Epoch 192/300, seasonal_2 Loss: 0.2033 | 0.2851
Epoch 193/300, seasonal_2 Loss: 0.2031 | 0.2851
Epoch 194/300, seasonal_2 Loss: 0.2024 | 0.2851
Epoch 195/300, seasonal_2 Loss: 0.2028 | 0.2851
Epoch 196/300, seasonal_2 Loss: 0.2028 | 0.2851
Epoch 197/300, seasonal_2 Loss: 0.2030 | 0.2851
Epoch 198/300, seasonal_2 Loss: 0.2026 | 0.2851
Epoch 199/300, seasonal_2 Loss: 0.2034 | 0.2851
Epoch 200/300, seasonal_2 Loss: 0.2033 | 0.2851
Epoch 201/300, seasonal_2 Loss: 0.2034 | 0.2851
Epoch 202/300, seasonal_2 Loss: 0.2030 | 0.2851
Epoch 203/300, seasonal_2 Loss: 0.2035 | 0.2851
Epoch 204/300, seasonal_2 Loss: 0.2033 | 0.2851
Epoch 205/300, seasonal_2 Loss: 0.2025 | 0.2851
Epoch 206/300, seasonal_2 Loss: 0.2032 | 0.2851
Epoch 207/300, seasonal_2 Loss: 0.2036 | 0.2851
Epoch 208/300, seasonal_2 Loss: 0.2027 | 0.2851
Epoch 209/300, seasonal_2 Loss: 0.2028 | 0.2851
Epoch 210/300, seasonal_2 Loss: 0.2031 | 0.2851
Epoch 211/300, seasonal_2 Loss: 0.2028 | 0.2851
Epoch 212/300, seasonal_2 Loss: 0.2026 | 0.2851
Epoch 213/300, seasonal_2 Loss: 0.2035 | 0.2851
Epoch 214/300, seasonal_2 Loss: 0.2036 | 0.2851
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 169, 'train_rates': 0.9523882556174285, 'learning_rate': 0.0002685545654932206, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9007398640351604}
Epoch 1/300, seasonal_3 Loss: 0.8866 | 1.3682
Epoch 2/300, seasonal_3 Loss: 0.7550 | 1.1422
Epoch 3/300, seasonal_3 Loss: 0.5876 | 1.0205
Epoch 4/300, seasonal_3 Loss: 0.5711 | 0.9044
Epoch 5/300, seasonal_3 Loss: 0.5958 | 0.9018
Epoch 6/300, seasonal_3 Loss: 0.5764 | 0.8470
Epoch 7/300, seasonal_3 Loss: 0.5086 | 0.9424
Epoch 8/300, seasonal_3 Loss: 0.4848 | 0.7979
Epoch 9/300, seasonal_3 Loss: 0.4951 | 0.8066
Epoch 10/300, seasonal_3 Loss: 0.4570 | 0.7606
Epoch 11/300, seasonal_3 Loss: 0.4944 | 0.7729
Epoch 12/300, seasonal_3 Loss: 0.4047 | 0.7078
Epoch 13/300, seasonal_3 Loss: 0.3696 | 0.6854
Epoch 14/300, seasonal_3 Loss: 0.3532 | 0.6533
Epoch 15/300, seasonal_3 Loss: 0.3335 | 0.6966
Epoch 16/300, seasonal_3 Loss: 0.3190 | 0.5789
Epoch 17/300, seasonal_3 Loss: 0.3125 | 0.5907
Epoch 18/300, seasonal_3 Loss: 0.2918 | 0.5379
Epoch 19/300, seasonal_3 Loss: 0.2871 | 0.5421
Epoch 20/300, seasonal_3 Loss: 0.2829 | 0.5210
Epoch 21/300, seasonal_3 Loss: 0.2612 | 0.5293
Epoch 22/300, seasonal_3 Loss: 0.2514 | 0.4837
Epoch 23/300, seasonal_3 Loss: 0.2436 | 0.4807
Epoch 24/300, seasonal_3 Loss: 0.2420 | 0.4572
Epoch 25/300, seasonal_3 Loss: 0.2465 | 0.4576
Epoch 26/300, seasonal_3 Loss: 0.2399 | 0.4324
Epoch 27/300, seasonal_3 Loss: 0.2309 | 0.4323
Epoch 28/300, seasonal_3 Loss: 0.2345 | 0.4128
Epoch 29/300, seasonal_3 Loss: 0.2459 | 0.4143
Epoch 30/300, seasonal_3 Loss: 0.2324 | 0.3999
Epoch 31/300, seasonal_3 Loss: 0.2233 | 0.4089
Epoch 32/300, seasonal_3 Loss: 0.2254 | 0.3796
Epoch 33/300, seasonal_3 Loss: 0.2344 | 0.3820
Epoch 34/300, seasonal_3 Loss: 0.2436 | 0.3778
Epoch 35/300, seasonal_3 Loss: 0.2248 | 0.3755
Epoch 36/300, seasonal_3 Loss: 0.2255 | 0.3769
Epoch 37/300, seasonal_3 Loss: 0.2264 | 0.3803
Epoch 38/300, seasonal_3 Loss: 0.2452 | 0.3726
Epoch 39/300, seasonal_3 Loss: 0.2154 | 0.3626
Epoch 40/300, seasonal_3 Loss: 0.2131 | 0.3583
Epoch 41/300, seasonal_3 Loss: 0.1991 | 0.3496
Epoch 42/300, seasonal_3 Loss: 0.1941 | 0.3462
Epoch 43/300, seasonal_3 Loss: 0.1904 | 0.3374
Epoch 44/300, seasonal_3 Loss: 0.1862 | 0.3404
Epoch 45/300, seasonal_3 Loss: 0.1854 | 0.3307
Epoch 46/300, seasonal_3 Loss: 0.1835 | 0.3305
Epoch 47/300, seasonal_3 Loss: 0.1820 | 0.3228
Epoch 48/300, seasonal_3 Loss: 0.1796 | 0.3240
Epoch 49/300, seasonal_3 Loss: 0.1773 | 0.3155
Epoch 50/300, seasonal_3 Loss: 0.1758 | 0.3166
Epoch 51/300, seasonal_3 Loss: 0.1745 | 0.3132
Epoch 52/300, seasonal_3 Loss: 0.1723 | 0.3093
Epoch 53/300, seasonal_3 Loss: 0.1720 | 0.3053
Epoch 54/300, seasonal_3 Loss: 0.1704 | 0.3062
Epoch 55/300, seasonal_3 Loss: 0.1700 | 0.3008
Epoch 56/300, seasonal_3 Loss: 0.1687 | 0.2987
Epoch 57/300, seasonal_3 Loss: 0.1674 | 0.2988
Epoch 58/300, seasonal_3 Loss: 0.1664 | 0.2929
Epoch 59/300, seasonal_3 Loss: 0.1656 | 0.2908
Epoch 60/300, seasonal_3 Loss: 0.1652 | 0.2920
Epoch 61/300, seasonal_3 Loss: 0.1647 | 0.2861
Epoch 62/300, seasonal_3 Loss: 0.1630 | 0.2847
Epoch 63/300, seasonal_3 Loss: 0.1621 | 0.2876
Epoch 64/300, seasonal_3 Loss: 0.1620 | 0.2807
Epoch 65/300, seasonal_3 Loss: 0.1607 | 0.2797
Epoch 66/300, seasonal_3 Loss: 0.1602 | 0.2820
Epoch 67/300, seasonal_3 Loss: 0.1594 | 0.2763
Epoch 68/300, seasonal_3 Loss: 0.1590 | 0.2750
Epoch 69/300, seasonal_3 Loss: 0.1580 | 0.2770
Epoch 70/300, seasonal_3 Loss: 0.1572 | 0.2717
Epoch 71/300, seasonal_3 Loss: 0.1571 | 0.2708
Epoch 72/300, seasonal_3 Loss: 0.1563 | 0.2703
Epoch 73/300, seasonal_3 Loss: 0.1552 | 0.2666
Epoch 74/300, seasonal_3 Loss: 0.1538 | 0.2659
Epoch 75/300, seasonal_3 Loss: 0.1533 | 0.2660
Epoch 76/300, seasonal_3 Loss: 0.1533 | 0.2627
Epoch 77/300, seasonal_3 Loss: 0.1521 | 0.2635
Epoch 78/300, seasonal_3 Loss: 0.1520 | 0.2610
Epoch 79/300, seasonal_3 Loss: 0.1513 | 0.2605
Epoch 80/300, seasonal_3 Loss: 0.1506 | 0.2598
Epoch 81/300, seasonal_3 Loss: 0.1502 | 0.2574
Epoch 82/300, seasonal_3 Loss: 0.1492 | 0.2569
Epoch 83/300, seasonal_3 Loss: 0.1500 | 0.2562
Epoch 84/300, seasonal_3 Loss: 0.1489 | 0.2549
Epoch 85/300, seasonal_3 Loss: 0.1476 | 0.2529
Epoch 86/300, seasonal_3 Loss: 0.1488 | 0.2520
Epoch 87/300, seasonal_3 Loss: 0.1481 | 0.2507
Epoch 88/300, seasonal_3 Loss: 0.1466 | 0.2507
Epoch 89/300, seasonal_3 Loss: 0.1468 | 0.2496
Epoch 90/300, seasonal_3 Loss: 0.1464 | 0.2489
Epoch 91/300, seasonal_3 Loss: 0.1466 | 0.2477
Epoch 92/300, seasonal_3 Loss: 0.1460 | 0.2464
Epoch 93/300, seasonal_3 Loss: 0.1442 | 0.2462
Epoch 94/300, seasonal_3 Loss: 0.1446 | 0.2453
Epoch 95/300, seasonal_3 Loss: 0.1457 | 0.2444
Epoch 96/300, seasonal_3 Loss: 0.1437 | 0.2431
Epoch 97/300, seasonal_3 Loss: 0.1448 | 0.2433
Epoch 98/300, seasonal_3 Loss: 0.1438 | 0.2425
Epoch 99/300, seasonal_3 Loss: 0.1426 | 0.2415
Epoch 100/300, seasonal_3 Loss: 0.1431 | 0.2410
Epoch 101/300, seasonal_3 Loss: 0.1427 | 0.2400
Epoch 102/300, seasonal_3 Loss: 0.1422 | 0.2392
Epoch 103/300, seasonal_3 Loss: 0.1425 | 0.2384
Epoch 104/300, seasonal_3 Loss: 0.1425 | 0.2380
Epoch 105/300, seasonal_3 Loss: 0.1415 | 0.2375
Epoch 106/300, seasonal_3 Loss: 0.1416 | 0.2374
Epoch 107/300, seasonal_3 Loss: 0.1413 | 0.2363
Epoch 108/300, seasonal_3 Loss: 0.1409 | 0.2358
Epoch 109/300, seasonal_3 Loss: 0.1411 | 0.2352
Epoch 110/300, seasonal_3 Loss: 0.1402 | 0.2347
Epoch 111/300, seasonal_3 Loss: 0.1395 | 0.2335
Epoch 112/300, seasonal_3 Loss: 0.1399 | 0.2342
Epoch 113/300, seasonal_3 Loss: 0.1403 | 0.2330
Epoch 114/300, seasonal_3 Loss: 0.1398 | 0.2324
Epoch 115/300, seasonal_3 Loss: 0.1400 | 0.2318
Epoch 116/300, seasonal_3 Loss: 0.1395 | 0.2320
Epoch 117/300, seasonal_3 Loss: 0.1397 | 0.2309
Epoch 118/300, seasonal_3 Loss: 0.1393 | 0.2307
Epoch 119/300, seasonal_3 Loss: 0.1386 | 0.2302
Epoch 120/300, seasonal_3 Loss: 0.1389 | 0.2302
Epoch 121/300, seasonal_3 Loss: 0.1382 | 0.2296
Epoch 122/300, seasonal_3 Loss: 0.1385 | 0.2292
Epoch 123/300, seasonal_3 Loss: 0.1384 | 0.2290
Epoch 124/300, seasonal_3 Loss: 0.1382 | 0.2288
Epoch 125/300, seasonal_3 Loss: 0.1381 | 0.2284
Epoch 126/300, seasonal_3 Loss: 0.1378 | 0.2279
Epoch 127/300, seasonal_3 Loss: 0.1381 | 0.2278
Epoch 128/300, seasonal_3 Loss: 0.1371 | 0.2276
Epoch 129/300, seasonal_3 Loss: 0.1371 | 0.2267
Epoch 130/300, seasonal_3 Loss: 0.1376 | 0.2268
Epoch 131/300, seasonal_3 Loss: 0.1374 | 0.2265
Epoch 132/300, seasonal_3 Loss: 0.1379 | 0.2264
Epoch 133/300, seasonal_3 Loss: 0.1366 | 0.2257
Epoch 134/300, seasonal_3 Loss: 0.1373 | 0.2258
Epoch 135/300, seasonal_3 Loss: 0.1370 | 0.2251
Epoch 136/300, seasonal_3 Loss: 0.1367 | 0.2250
Epoch 137/300, seasonal_3 Loss: 0.1361 | 0.2246
Epoch 138/300, seasonal_3 Loss: 0.1368 | 0.2243
Epoch 139/300, seasonal_3 Loss: 0.1363 | 0.2238
Epoch 140/300, seasonal_3 Loss: 0.1367 | 0.2239
Epoch 141/300, seasonal_3 Loss: 0.1368 | 0.2239
Epoch 142/300, seasonal_3 Loss: 0.1360 | 0.2234
Epoch 143/300, seasonal_3 Loss: 0.1365 | 0.2234
Epoch 144/300, seasonal_3 Loss: 0.1361 | 0.2229
Epoch 145/300, seasonal_3 Loss: 0.1358 | 0.2228
Epoch 146/300, seasonal_3 Loss: 0.1358 | 0.2226
Epoch 147/300, seasonal_3 Loss: 0.1350 | 0.2223
Epoch 148/300, seasonal_3 Loss: 0.1353 | 0.2220
Epoch 149/300, seasonal_3 Loss: 0.1346 | 0.2222
Epoch 150/300, seasonal_3 Loss: 0.1358 | 0.2223
Epoch 151/300, seasonal_3 Loss: 0.1356 | 0.2217
Epoch 152/300, seasonal_3 Loss: 0.1353 | 0.2216
Epoch 153/300, seasonal_3 Loss: 0.1351 | 0.2217
Epoch 154/300, seasonal_3 Loss: 0.1349 | 0.2215
Epoch 155/300, seasonal_3 Loss: 0.1355 | 0.2212
Epoch 156/300, seasonal_3 Loss: 0.1352 | 0.2209
Epoch 157/300, seasonal_3 Loss: 0.1351 | 0.2207
Epoch 158/300, seasonal_3 Loss: 0.1354 | 0.2205
Epoch 159/300, seasonal_3 Loss: 0.1345 | 0.2205
Epoch 160/300, seasonal_3 Loss: 0.1351 | 0.2205
Epoch 161/300, seasonal_3 Loss: 0.1346 | 0.2204
Epoch 162/300, seasonal_3 Loss: 0.1348 | 0.2202
Epoch 163/300, seasonal_3 Loss: 0.1346 | 0.2201
Epoch 164/300, seasonal_3 Loss: 0.1347 | 0.2200
Epoch 165/300, seasonal_3 Loss: 0.1341 | 0.2198
Epoch 166/300, seasonal_3 Loss: 0.1344 | 0.2196
Epoch 167/300, seasonal_3 Loss: 0.1352 | 0.2197
Epoch 168/300, seasonal_3 Loss: 0.1351 | 0.2196
Epoch 169/300, seasonal_3 Loss: 0.1344 | 0.2194
Epoch 170/300, seasonal_3 Loss: 0.1348 | 0.2193
Epoch 171/300, seasonal_3 Loss: 0.1340 | 0.2191
Epoch 172/300, seasonal_3 Loss: 0.1336 | 0.2188
Epoch 173/300, seasonal_3 Loss: 0.1341 | 0.2186
Epoch 174/300, seasonal_3 Loss: 0.1344 | 0.2186
Epoch 175/300, seasonal_3 Loss: 0.1345 | 0.2184
Epoch 176/300, seasonal_3 Loss: 0.1338 | 0.2183
Epoch 177/300, seasonal_3 Loss: 0.1342 | 0.2182
Epoch 178/300, seasonal_3 Loss: 0.1340 | 0.2182
Epoch 179/300, seasonal_3 Loss: 0.1345 | 0.2181
Epoch 180/300, seasonal_3 Loss: 0.1337 | 0.2180
Epoch 181/300, seasonal_3 Loss: 0.1346 | 0.2179
Epoch 182/300, seasonal_3 Loss: 0.1335 | 0.2178
Epoch 183/300, seasonal_3 Loss: 0.1340 | 0.2178
Epoch 184/300, seasonal_3 Loss: 0.1338 | 0.2178
Epoch 185/300, seasonal_3 Loss: 0.1345 | 0.2178
Epoch 186/300, seasonal_3 Loss: 0.1333 | 0.2176
Epoch 187/300, seasonal_3 Loss: 0.1341 | 0.2176
Epoch 188/300, seasonal_3 Loss: 0.1344 | 0.2175
Epoch 189/300, seasonal_3 Loss: 0.1338 | 0.2174
Epoch 190/300, seasonal_3 Loss: 0.1344 | 0.2173
Epoch 191/300, seasonal_3 Loss: 0.1337 | 0.2173
Epoch 192/300, seasonal_3 Loss: 0.1341 | 0.2173
Epoch 193/300, seasonal_3 Loss: 0.1335 | 0.2174
Epoch 194/300, seasonal_3 Loss: 0.1339 | 0.2173
Epoch 195/300, seasonal_3 Loss: 0.1330 | 0.2173
Epoch 196/300, seasonal_3 Loss: 0.1341 | 0.2173
Epoch 197/300, seasonal_3 Loss: 0.1345 | 0.2171
Epoch 198/300, seasonal_3 Loss: 0.1338 | 0.2170
Epoch 199/300, seasonal_3 Loss: 0.1334 | 0.2170
Epoch 200/300, seasonal_3 Loss: 0.1332 | 0.2169
Epoch 201/300, seasonal_3 Loss: 0.1333 | 0.2169
Epoch 202/300, seasonal_3 Loss: 0.1339 | 0.2169
Epoch 203/300, seasonal_3 Loss: 0.1335 | 0.2167
Epoch 204/300, seasonal_3 Loss: 0.1337 | 0.2167
Epoch 205/300, seasonal_3 Loss: 0.1330 | 0.2168
Epoch 206/300, seasonal_3 Loss: 0.1336 | 0.2168
Epoch 207/300, seasonal_3 Loss: 0.1335 | 0.2167
Epoch 208/300, seasonal_3 Loss: 0.1338 | 0.2167
Epoch 209/300, seasonal_3 Loss: 0.1328 | 0.2167
Epoch 210/300, seasonal_3 Loss: 0.1330 | 0.2167
Epoch 211/300, seasonal_3 Loss: 0.1334 | 0.2167
Epoch 212/300, seasonal_3 Loss: 0.1334 | 0.2166
Epoch 213/300, seasonal_3 Loss: 0.1333 | 0.2166
Epoch 214/300, seasonal_3 Loss: 0.1329 | 0.2166
Epoch 215/300, seasonal_3 Loss: 0.1330 | 0.2165
Epoch 216/300, seasonal_3 Loss: 0.1334 | 0.2166
Epoch 217/300, seasonal_3 Loss: 0.1331 | 0.2166
Epoch 218/300, seasonal_3 Loss: 0.1335 | 0.2166
Epoch 219/300, seasonal_3 Loss: 0.1327 | 0.2165
Epoch 220/300, seasonal_3 Loss: 0.1332 | 0.2165
Epoch 221/300, seasonal_3 Loss: 0.1330 | 0.2164
Epoch 222/300, seasonal_3 Loss: 0.1331 | 0.2164
Epoch 223/300, seasonal_3 Loss: 0.1328 | 0.2164
Epoch 224/300, seasonal_3 Loss: 0.1333 | 0.2163
Epoch 225/300, seasonal_3 Loss: 0.1333 | 0.2162
Epoch 226/300, seasonal_3 Loss: 0.1336 | 0.2162
Epoch 227/300, seasonal_3 Loss: 0.1332 | 0.2162
Epoch 228/300, seasonal_3 Loss: 0.1333 | 0.2161
Epoch 229/300, seasonal_3 Loss: 0.1336 | 0.2162
Epoch 230/300, seasonal_3 Loss: 0.1333 | 0.2162
Epoch 231/300, seasonal_3 Loss: 0.1332 | 0.2161
Epoch 232/300, seasonal_3 Loss: 0.1328 | 0.2161
Epoch 233/300, seasonal_3 Loss: 0.1333 | 0.2161
Epoch 234/300, seasonal_3 Loss: 0.1332 | 0.2161
Epoch 235/300, seasonal_3 Loss: 0.1330 | 0.2161
Epoch 236/300, seasonal_3 Loss: 0.1336 | 0.2161
Epoch 237/300, seasonal_3 Loss: 0.1327 | 0.2161
Epoch 238/300, seasonal_3 Loss: 0.1330 | 0.2161
Epoch 239/300, seasonal_3 Loss: 0.1327 | 0.2161
Epoch 240/300, seasonal_3 Loss: 0.1337 | 0.2161
Epoch 241/300, seasonal_3 Loss: 0.1332 | 0.2160
Epoch 242/300, seasonal_3 Loss: 0.1326 | 0.2160
Epoch 243/300, seasonal_3 Loss: 0.1332 | 0.2160
Epoch 244/300, seasonal_3 Loss: 0.1333 | 0.2160
Epoch 245/300, seasonal_3 Loss: 0.1328 | 0.2159
Epoch 246/300, seasonal_3 Loss: 0.1336 | 0.2159
Epoch 247/300, seasonal_3 Loss: 0.1337 | 0.2159
Epoch 248/300, seasonal_3 Loss: 0.1333 | 0.2159
Epoch 249/300, seasonal_3 Loss: 0.1331 | 0.2159
Epoch 250/300, seasonal_3 Loss: 0.1329 | 0.2159
Epoch 251/300, seasonal_3 Loss: 0.1327 | 0.2159
Epoch 252/300, seasonal_3 Loss: 0.1328 | 0.2158
Epoch 253/300, seasonal_3 Loss: 0.1330 | 0.2158
Epoch 254/300, seasonal_3 Loss: 0.1334 | 0.2158
Epoch 255/300, seasonal_3 Loss: 0.1334 | 0.2158
Epoch 256/300, seasonal_3 Loss: 0.1338 | 0.2158
Epoch 257/300, seasonal_3 Loss: 0.1329 | 0.2158
Epoch 258/300, seasonal_3 Loss: 0.1321 | 0.2158
Epoch 259/300, seasonal_3 Loss: 0.1327 | 0.2158
Epoch 260/300, seasonal_3 Loss: 0.1336 | 0.2157
Epoch 261/300, seasonal_3 Loss: 0.1335 | 0.2157
Epoch 262/300, seasonal_3 Loss: 0.1333 | 0.2157
Epoch 263/300, seasonal_3 Loss: 0.1336 | 0.2157
Epoch 264/300, seasonal_3 Loss: 0.1332 | 0.2157
Epoch 265/300, seasonal_3 Loss: 0.1327 | 0.2157
Epoch 266/300, seasonal_3 Loss: 0.1326 | 0.2157
Epoch 267/300, seasonal_3 Loss: 0.1325 | 0.2157
Epoch 268/300, seasonal_3 Loss: 0.1330 | 0.2157
Epoch 269/300, seasonal_3 Loss: 0.1333 | 0.2157
Epoch 270/300, seasonal_3 Loss: 0.1332 | 0.2157
Epoch 271/300, seasonal_3 Loss: 0.1328 | 0.2157
Epoch 272/300, seasonal_3 Loss: 0.1333 | 0.2157
Epoch 273/300, seasonal_3 Loss: 0.1329 | 0.2157
Epoch 274/300, seasonal_3 Loss: 0.1329 | 0.2157
Epoch 275/300, seasonal_3 Loss: 0.1330 | 0.2157
Epoch 276/300, seasonal_3 Loss: 0.1334 | 0.2157
Epoch 277/300, seasonal_3 Loss: 0.1329 | 0.2157
Epoch 278/300, seasonal_3 Loss: 0.1344 | 0.2157
Epoch 279/300, seasonal_3 Loss: 0.1332 | 0.2157
Epoch 280/300, seasonal_3 Loss: 0.1323 | 0.2157
Epoch 281/300, seasonal_3 Loss: 0.1328 | 0.2157
Epoch 282/300, seasonal_3 Loss: 0.1333 | 0.2157
Epoch 283/300, seasonal_3 Loss: 0.1336 | 0.2156
Epoch 284/300, seasonal_3 Loss: 0.1327 | 0.2156
Epoch 285/300, seasonal_3 Loss: 0.1339 | 0.2156
Epoch 286/300, seasonal_3 Loss: 0.1322 | 0.2156
Epoch 287/300, seasonal_3 Loss: 0.1324 | 0.2156
Epoch 288/300, seasonal_3 Loss: 0.1329 | 0.2156
Epoch 289/300, seasonal_3 Loss: 0.1334 | 0.2156
Epoch 290/300, seasonal_3 Loss: 0.1330 | 0.2156
Epoch 291/300, seasonal_3 Loss: 0.1337 | 0.2156
Epoch 292/300, seasonal_3 Loss: 0.1336 | 0.2156
Epoch 293/300, seasonal_3 Loss: 0.1338 | 0.2156
Epoch 294/300, seasonal_3 Loss: 0.1323 | 0.2156
Epoch 295/300, seasonal_3 Loss: 0.1336 | 0.2156
Epoch 296/300, seasonal_3 Loss: 0.1326 | 0.2156
Epoch 297/300, seasonal_3 Loss: 0.1327 | 0.2156
Epoch 298/300, seasonal_3 Loss: 0.1322 | 0.2156
Epoch 299/300, seasonal_3 Loss: 0.1327 | 0.2156
Epoch 300/300, seasonal_3 Loss: 0.1328 | 0.2156
Training resid component with params: {'observation_period_num': 48, 'train_rates': 0.9898529903835476, 'learning_rate': 7.103499801503369e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.7851961993436622}
Epoch 1/300, resid Loss: 0.7009 | 0.9933
Epoch 2/300, resid Loss: 0.5226 | 0.8435
Epoch 3/300, resid Loss: 0.4244 | 0.7346
Epoch 4/300, resid Loss: 0.3903 | 0.6869
Epoch 5/300, resid Loss: 0.3730 | 0.7089
Epoch 6/300, resid Loss: 0.3320 | 0.5903
Epoch 7/300, resid Loss: 0.3064 | 0.5533
Epoch 8/300, resid Loss: 0.2918 | 0.5210
Epoch 9/300, resid Loss: 0.2734 | 0.5215
Epoch 10/300, resid Loss: 0.2598 | 0.4699
Epoch 11/300, resid Loss: 0.2455 | 0.4338
Epoch 12/300, resid Loss: 0.2453 | 0.4360
Epoch 13/300, resid Loss: 0.2297 | 0.4406
Epoch 14/300, resid Loss: 0.2240 | 0.3947
Epoch 15/300, resid Loss: 0.2170 | 0.3630
Epoch 16/300, resid Loss: 0.2204 | 0.3636
Epoch 17/300, resid Loss: 0.2119 | 0.3711
Epoch 18/300, resid Loss: 0.2049 | 0.3469
Epoch 19/300, resid Loss: 0.2021 | 0.3376
Epoch 20/300, resid Loss: 0.2009 | 0.3298
Epoch 21/300, resid Loss: 0.2014 | 0.3356
Epoch 22/300, resid Loss: 0.1909 | 0.3125
Epoch 23/300, resid Loss: 0.1889 | 0.3013
Epoch 24/300, resid Loss: 0.1879 | 0.2955
Epoch 25/300, resid Loss: 0.1863 | 0.2942
Epoch 26/300, resid Loss: 0.1823 | 0.3054
Epoch 27/300, resid Loss: 0.1787 | 0.2771
Epoch 28/300, resid Loss: 0.1759 | 0.2733
Epoch 29/300, resid Loss: 0.1749 | 0.2712
Epoch 30/300, resid Loss: 0.1714 | 0.2731
Epoch 31/300, resid Loss: 0.1700 | 0.2589
Epoch 32/300, resid Loss: 0.1683 | 0.2584
Epoch 33/300, resid Loss: 0.1671 | 0.2551
Epoch 34/300, resid Loss: 0.1655 | 0.2531
Epoch 35/300, resid Loss: 0.1633 | 0.2450
Epoch 36/300, resid Loss: 0.1619 | 0.2445
Epoch 37/300, resid Loss: 0.1609 | 0.2451
Epoch 38/300, resid Loss: 0.1591 | 0.2420
Epoch 39/300, resid Loss: 0.1581 | 0.2363
Epoch 40/300, resid Loss: 0.1573 | 0.2329
Epoch 41/300, resid Loss: 0.1567 | 0.2314
Epoch 42/300, resid Loss: 0.1554 | 0.2306
Epoch 43/300, resid Loss: 0.1548 | 0.2307
Epoch 44/300, resid Loss: 0.1539 | 0.2267
Epoch 45/300, resid Loss: 0.1532 | 0.2243
Epoch 46/300, resid Loss: 0.1518 | 0.2229
Epoch 47/300, resid Loss: 0.1508 | 0.2213
Epoch 48/300, resid Loss: 0.1506 | 0.2179
Epoch 49/300, resid Loss: 0.1506 | 0.2197
Epoch 50/300, resid Loss: 0.1502 | 0.2164
Epoch 51/300, resid Loss: 0.1484 | 0.2171
Epoch 52/300, resid Loss: 0.1481 | 0.2138
Epoch 53/300, resid Loss: 0.1476 | 0.2133
Epoch 54/300, resid Loss: 0.1474 | 0.2111
Epoch 55/300, resid Loss: 0.1461 | 0.2123
Epoch 56/300, resid Loss: 0.1457 | 0.2112
Epoch 57/300, resid Loss: 0.1458 | 0.2097
Epoch 58/300, resid Loss: 0.1452 | 0.2086
Epoch 59/300, resid Loss: 0.1451 | 0.2086
Epoch 60/300, resid Loss: 0.1442 | 0.2082
Epoch 61/300, resid Loss: 0.1440 | 0.2066
Epoch 62/300, resid Loss: 0.1439 | 0.2059
Epoch 63/300, resid Loss: 0.1433 | 0.2052
Epoch 64/300, resid Loss: 0.1434 | 0.2054
Epoch 65/300, resid Loss: 0.1432 | 0.2039
Epoch 66/300, resid Loss: 0.1421 | 0.2032
Epoch 67/300, resid Loss: 0.1421 | 0.2028
Epoch 68/300, resid Loss: 0.1421 | 0.2021
Epoch 69/300, resid Loss: 0.1419 | 0.2017
Epoch 70/300, resid Loss: 0.1414 | 0.2017
Epoch 71/300, resid Loss: 0.1417 | 0.2009
Epoch 72/300, resid Loss: 0.1405 | 0.2000
Epoch 73/300, resid Loss: 0.1406 | 0.2003
Epoch 74/300, resid Loss: 0.1406 | 0.2002
Epoch 75/300, resid Loss: 0.1399 | 0.1993
Epoch 76/300, resid Loss: 0.1404 | 0.1984
Epoch 77/300, resid Loss: 0.1401 | 0.1987
Epoch 78/300, resid Loss: 0.1394 | 0.1978
Epoch 79/300, resid Loss: 0.1400 | 0.1981
Epoch 80/300, resid Loss: 0.1398 | 0.1981
Epoch 81/300, resid Loss: 0.1399 | 0.1975
Epoch 82/300, resid Loss: 0.1398 | 0.1973
Epoch 83/300, resid Loss: 0.1384 | 0.1972
Epoch 84/300, resid Loss: 0.1393 | 0.1969
Epoch 85/300, resid Loss: 0.1391 | 0.1969
Epoch 86/300, resid Loss: 0.1398 | 0.1963
Epoch 87/300, resid Loss: 0.1390 | 0.1962
Epoch 88/300, resid Loss: 0.1383 | 0.1959
Epoch 89/300, resid Loss: 0.1384 | 0.1963
Epoch 90/300, resid Loss: 0.1390 | 0.1964
Epoch 91/300, resid Loss: 0.1379 | 0.1962
Epoch 92/300, resid Loss: 0.1380 | 0.1961
Epoch 93/300, resid Loss: 0.1381 | 0.1960
Epoch 94/300, resid Loss: 0.1380 | 0.1955
Epoch 95/300, resid Loss: 0.1379 | 0.1952
Epoch 96/300, resid Loss: 0.1380 | 0.1952
Epoch 97/300, resid Loss: 0.1384 | 0.1951
Epoch 98/300, resid Loss: 0.1383 | 0.1950
Epoch 99/300, resid Loss: 0.1375 | 0.1949
Epoch 100/300, resid Loss: 0.1377 | 0.1950
Epoch 101/300, resid Loss: 0.1379 | 0.1949
Epoch 102/300, resid Loss: 0.1376 | 0.1949
Epoch 103/300, resid Loss: 0.1372 | 0.1948
Epoch 104/300, resid Loss: 0.1369 | 0.1947
Epoch 105/300, resid Loss: 0.1377 | 0.1945
Epoch 106/300, resid Loss: 0.1377 | 0.1945
Epoch 107/300, resid Loss: 0.1376 | 0.1946
Epoch 108/300, resid Loss: 0.1374 | 0.1944
Epoch 109/300, resid Loss: 0.1379 | 0.1943
Epoch 110/300, resid Loss: 0.1371 | 0.1941
Epoch 111/300, resid Loss: 0.1380 | 0.1941
Epoch 112/300, resid Loss: 0.1370 | 0.1940
Epoch 113/300, resid Loss: 0.1375 | 0.1940
Epoch 114/300, resid Loss: 0.1370 | 0.1939
Epoch 115/300, resid Loss: 0.1369 | 0.1939
Epoch 116/300, resid Loss: 0.1382 | 0.1938
Epoch 117/300, resid Loss: 0.1371 | 0.1938
Epoch 118/300, resid Loss: 0.1371 | 0.1938
Epoch 119/300, resid Loss: 0.1367 | 0.1938
Epoch 120/300, resid Loss: 0.1375 | 0.1938
Epoch 121/300, resid Loss: 0.1374 | 0.1937
Epoch 122/300, resid Loss: 0.1372 | 0.1937
Epoch 123/300, resid Loss: 0.1371 | 0.1937
Epoch 124/300, resid Loss: 0.1377 | 0.1937
Epoch 125/300, resid Loss: 0.1371 | 0.1936
Epoch 126/300, resid Loss: 0.1364 | 0.1935
Epoch 127/300, resid Loss: 0.1378 | 0.1935
Epoch 128/300, resid Loss: 0.1373 | 0.1934
Epoch 129/300, resid Loss: 0.1368 | 0.1933
Epoch 130/300, resid Loss: 0.1368 | 0.1934
Epoch 131/300, resid Loss: 0.1381 | 0.1933
Epoch 132/300, resid Loss: 0.1365 | 0.1933
Epoch 133/300, resid Loss: 0.1368 | 0.1933
Epoch 134/300, resid Loss: 0.1367 | 0.1932
Epoch 135/300, resid Loss: 0.1366 | 0.1932
Epoch 136/300, resid Loss: 0.1371 | 0.1931
Epoch 137/300, resid Loss: 0.1370 | 0.1932
Epoch 138/300, resid Loss: 0.1367 | 0.1931
Epoch 139/300, resid Loss: 0.1367 | 0.1931
Epoch 140/300, resid Loss: 0.1366 | 0.1931
Epoch 141/300, resid Loss: 0.1373 | 0.1931
Epoch 142/300, resid Loss: 0.1373 | 0.1931
Epoch 143/300, resid Loss: 0.1374 | 0.1931
Epoch 144/300, resid Loss: 0.1374 | 0.1931
Epoch 145/300, resid Loss: 0.1368 | 0.1931
Epoch 146/300, resid Loss: 0.1366 | 0.1931
Epoch 147/300, resid Loss: 0.1367 | 0.1931
Epoch 148/300, resid Loss: 0.1371 | 0.1931
Epoch 149/300, resid Loss: 0.1372 | 0.1931
Epoch 150/300, resid Loss: 0.1365 | 0.1931
Epoch 151/300, resid Loss: 0.1367 | 0.1931
Epoch 152/300, resid Loss: 0.1369 | 0.1931
Epoch 153/300, resid Loss: 0.1371 | 0.1931
Epoch 154/300, resid Loss: 0.1366 | 0.1931
Epoch 155/300, resid Loss: 0.1361 | 0.1931
Epoch 156/300, resid Loss: 0.1365 | 0.1931
Epoch 157/300, resid Loss: 0.1365 | 0.1931
Epoch 158/300, resid Loss: 0.1366 | 0.1931
Epoch 159/300, resid Loss: 0.1373 | 0.1931
Epoch 160/300, resid Loss: 0.1369 | 0.1931
Epoch 161/300, resid Loss: 0.1358 | 0.1930
Epoch 162/300, resid Loss: 0.1364 | 0.1930
Epoch 163/300, resid Loss: 0.1365 | 0.1930
Epoch 164/300, resid Loss: 0.1371 | 0.1931
Epoch 165/300, resid Loss: 0.1365 | 0.1930
Epoch 166/300, resid Loss: 0.1365 | 0.1930
Epoch 167/300, resid Loss: 0.1364 | 0.1930
Epoch 168/300, resid Loss: 0.1366 | 0.1930
Epoch 169/300, resid Loss: 0.1364 | 0.1930
Epoch 170/300, resid Loss: 0.1363 | 0.1931
Epoch 171/300, resid Loss: 0.1369 | 0.1930
Epoch 172/300, resid Loss: 0.1369 | 0.1930
Epoch 173/300, resid Loss: 0.1365 | 0.1930
Epoch 174/300, resid Loss: 0.1373 | 0.1930
Epoch 175/300, resid Loss: 0.1363 | 0.1930
Epoch 176/300, resid Loss: 0.1376 | 0.1930
Epoch 177/300, resid Loss: 0.1368 | 0.1930
Epoch 178/300, resid Loss: 0.1364 | 0.1930
Epoch 179/300, resid Loss: 0.1367 | 0.1930
Epoch 180/300, resid Loss: 0.1371 | 0.1930
Epoch 181/300, resid Loss: 0.1369 | 0.1930
Epoch 182/300, resid Loss: 0.1368 | 0.1930
Epoch 183/300, resid Loss: 0.1368 | 0.1930
Epoch 184/300, resid Loss: 0.1360 | 0.1930
Epoch 185/300, resid Loss: 0.1364 | 0.1930
Epoch 186/300, resid Loss: 0.1363 | 0.1930
Epoch 187/300, resid Loss: 0.1375 | 0.1930
Epoch 188/300, resid Loss: 0.1373 | 0.1930
Epoch 189/300, resid Loss: 0.1371 | 0.1930
Epoch 190/300, resid Loss: 0.1369 | 0.1930
Epoch 191/300, resid Loss: 0.1367 | 0.1930
Epoch 192/300, resid Loss: 0.1361 | 0.1930
Epoch 193/300, resid Loss: 0.1365 | 0.1930
Epoch 194/300, resid Loss: 0.1368 | 0.1930
Epoch 195/300, resid Loss: 0.1364 | 0.1930
Epoch 196/300, resid Loss: 0.1367 | 0.1930
Epoch 197/300, resid Loss: 0.1365 | 0.1930
Epoch 198/300, resid Loss: 0.1373 | 0.1930
Epoch 199/300, resid Loss: 0.1370 | 0.1930
Epoch 200/300, resid Loss: 0.1365 | 0.1930
Epoch 201/300, resid Loss: 0.1372 | 0.1930
Epoch 202/300, resid Loss: 0.1369 | 0.1930
Epoch 203/300, resid Loss: 0.1370 | 0.1930
Epoch 204/300, resid Loss: 0.1367 | 0.1930
Epoch 205/300, resid Loss: 0.1378 | 0.1930
Epoch 206/300, resid Loss: 0.1363 | 0.1930
Epoch 207/300, resid Loss: 0.1372 | 0.1930
Epoch 208/300, resid Loss: 0.1375 | 0.1930
Epoch 209/300, resid Loss: 0.1365 | 0.1930
Epoch 210/300, resid Loss: 0.1365 | 0.1930
Epoch 211/300, resid Loss: 0.1365 | 0.1930
Epoch 212/300, resid Loss: 0.1363 | 0.1930
Epoch 213/300, resid Loss: 0.1368 | 0.1930
Epoch 214/300, resid Loss: 0.1369 | 0.1930
Epoch 215/300, resid Loss: 0.1370 | 0.1930
Epoch 216/300, resid Loss: 0.1369 | 0.1930
Epoch 217/300, resid Loss: 0.1368 | 0.1930
Epoch 218/300, resid Loss: 0.1371 | 0.1930
Epoch 219/300, resid Loss: 0.1374 | 0.1930
Epoch 220/300, resid Loss: 0.1370 | 0.1930
Epoch 221/300, resid Loss: 0.1369 | 0.1930
Epoch 222/300, resid Loss: 0.1360 | 0.1930
Epoch 223/300, resid Loss: 0.1369 | 0.1930
Epoch 224/300, resid Loss: 0.1368 | 0.1930
Epoch 225/300, resid Loss: 0.1364 | 0.1930
Epoch 226/300, resid Loss: 0.1368 | 0.1930
Epoch 227/300, resid Loss: 0.1366 | 0.1930
Epoch 228/300, resid Loss: 0.1368 | 0.1930
Epoch 229/300, resid Loss: 0.1368 | 0.1930
Epoch 230/300, resid Loss: 0.1375 | 0.1930
Epoch 231/300, resid Loss: 0.1367 | 0.1930
Epoch 232/300, resid Loss: 0.1370 | 0.1930
Epoch 233/300, resid Loss: 0.1365 | 0.1930
Epoch 234/300, resid Loss: 0.1373 | 0.1930
Epoch 235/300, resid Loss: 0.1358 | 0.1930
Epoch 236/300, resid Loss: 0.1362 | 0.1930
Epoch 237/300, resid Loss: 0.1364 | 0.1930
Epoch 238/300, resid Loss: 0.1365 | 0.1930
Epoch 239/300, resid Loss: 0.1368 | 0.1930
Epoch 240/300, resid Loss: 0.1377 | 0.1930
Epoch 241/300, resid Loss: 0.1361 | 0.1930
Epoch 242/300, resid Loss: 0.1374 | 0.1930
Epoch 243/300, resid Loss: 0.1359 | 0.1930
Epoch 244/300, resid Loss: 0.1364 | 0.1930
Epoch 245/300, resid Loss: 0.1369 | 0.1930
Epoch 246/300, resid Loss: 0.1370 | 0.1930
Epoch 247/300, resid Loss: 0.1364 | 0.1930
Epoch 248/300, resid Loss: 0.1376 | 0.1930
Epoch 249/300, resid Loss: 0.1370 | 0.1930
Epoch 250/300, resid Loss: 0.1370 | 0.1930
Epoch 251/300, resid Loss: 0.1367 | 0.1930
Epoch 252/300, resid Loss: 0.1367 | 0.1930
Epoch 253/300, resid Loss: 0.1371 | 0.1930
Early stopping for resid
Runtime (seconds): 2419.066559791565
0.00024289207162824682
[144.94464]
[-0.5701154]
[-4.8198357]
[6.36983]
[3.104296]
[6.514819]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 221.5044665792957
RMSE: 14.883026123046875
MAE: 14.883026123046875
R-squared: nan
[155.54362]
