[32m[I 2025-02-05 00:41:47,626][0m A new study created in memory with name: no-name-18754c71-a3ef-4b2f-ba11-b476147cbb0f[0m
[32m[I 2025-02-05 00:42:38,786][0m Trial 0 finished with value: 0.21914906306288293 and parameters: {'observation_period_num': 49, 'train_rates': 0.6666265425810661, 'learning_rate': 7.349282863823982e-05, 'batch_size': 95, 'step_size': 15, 'gamma': 0.9420529056852931}. Best is trial 0 with value: 0.21914906306288293.[0m
[32m[I 2025-02-05 00:43:33,551][0m Trial 1 finished with value: 0.26978582826261477 and parameters: {'observation_period_num': 49, 'train_rates': 0.7833472544946392, 'learning_rate': 1.8926795611135853e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.9253037931611179}. Best is trial 0 with value: 0.21914906306288293.[0m
[32m[I 2025-02-05 00:44:20,605][0m Trial 2 finished with value: 0.17565808607773348 and parameters: {'observation_period_num': 248, 'train_rates': 0.8581848192220416, 'learning_rate': 0.0005291781538989183, 'batch_size': 111, 'step_size': 3, 'gamma': 0.7927937539201391}. Best is trial 2 with value: 0.17565808607773348.[0m
[32m[I 2025-02-05 00:46:43,106][0m Trial 3 finished with value: 0.16053647156010306 and parameters: {'observation_period_num': 194, 'train_rates': 0.8473696239102824, 'learning_rate': 8.027071177246747e-06, 'batch_size': 36, 'step_size': 14, 'gamma': 0.9354559316880818}. Best is trial 3 with value: 0.16053647156010306.[0m
[32m[I 2025-02-05 00:48:27,819][0m Trial 4 finished with value: 0.47197383603239373 and parameters: {'observation_period_num': 86, 'train_rates': 0.9469530655382036, 'learning_rate': 7.749859474399843e-06, 'batch_size': 55, 'step_size': 4, 'gamma': 0.8650258835094052}. Best is trial 3 with value: 0.16053647156010306.[0m
[32m[I 2025-02-05 00:49:00,043][0m Trial 5 finished with value: 0.3679201956571004 and parameters: {'observation_period_num': 162, 'train_rates': 0.9253991634649921, 'learning_rate': 7.649860952459766e-05, 'batch_size': 191, 'step_size': 6, 'gamma': 0.9755569590135793}. Best is trial 3 with value: 0.16053647156010306.[0m
[32m[I 2025-02-05 00:49:57,693][0m Trial 6 finished with value: 0.46771883640465456 and parameters: {'observation_period_num': 103, 'train_rates': 0.6632819886413216, 'learning_rate': 6.544928971714746e-06, 'batch_size': 79, 'step_size': 12, 'gamma': 0.805563590069018}. Best is trial 3 with value: 0.16053647156010306.[0m
[32m[I 2025-02-05 00:50:23,955][0m Trial 7 finished with value: 0.7587898542851578 and parameters: {'observation_period_num': 223, 'train_rates': 0.7334669693619693, 'learning_rate': 9.68555780932737e-06, 'batch_size': 198, 'step_size': 3, 'gamma': 0.9087069984267828}. Best is trial 3 with value: 0.16053647156010306.[0m
[32m[I 2025-02-05 00:51:50,479][0m Trial 8 finished with value: 0.11494626249815966 and parameters: {'observation_period_num': 124, 'train_rates': 0.9349193867003094, 'learning_rate': 0.00010506627850995718, 'batch_size': 66, 'step_size': 6, 'gamma': 0.8313337132762356}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 00:52:28,829][0m Trial 9 finished with value: 2.8827355358271562 and parameters: {'observation_period_num': 244, 'train_rates': 0.8480308588748069, 'learning_rate': 1.0189281670924794e-06, 'batch_size': 138, 'step_size': 5, 'gamma': 0.8078398761019935}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 00:56:58,362][0m Trial 10 finished with value: 0.12287717868587864 and parameters: {'observation_period_num': 144, 'train_rates': 0.9765340723539853, 'learning_rate': 0.0005362293994749134, 'batch_size': 21, 'step_size': 9, 'gamma': 0.7502281506944574}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 01:00:46,041][0m Trial 11 finished with value: 0.12426362931728363 and parameters: {'observation_period_num': 146, 'train_rates': 0.9822257303319525, 'learning_rate': 0.0009580866500093812, 'batch_size': 25, 'step_size': 9, 'gamma': 0.7689900197999079}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 01:06:25,212][0m Trial 12 finished with value: 0.13746719402179383 and parameters: {'observation_period_num': 118, 'train_rates': 0.9197468336368536, 'learning_rate': 0.0002615097247447858, 'batch_size': 16, 'step_size': 9, 'gamma': 0.85298800589968}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 01:06:50,154][0m Trial 13 finished with value: 0.22219838201999664 and parameters: {'observation_period_num': 176, 'train_rates': 0.9856752137771165, 'learning_rate': 0.000188258927849562, 'batch_size': 253, 'step_size': 7, 'gamma': 0.753092371802027}. Best is trial 8 with value: 0.11494626249815966.[0m
[32m[I 2025-02-05 01:08:20,998][0m Trial 14 finished with value: 0.032848759782405534 and parameters: {'observation_period_num': 9, 'train_rates': 0.8673432948180133, 'learning_rate': 9.929634602358066e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8370222157957841}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:09:01,694][0m Trial 15 finished with value: 0.04058996475002429 and parameters: {'observation_period_num': 14, 'train_rates': 0.8870904759534112, 'learning_rate': 0.00011758569382886846, 'batch_size': 146, 'step_size': 12, 'gamma': 0.8401596455824294}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:09:38,676][0m Trial 16 finished with value: 0.20084596303701402 and parameters: {'observation_period_num': 13, 'train_rates': 0.7877683032268565, 'learning_rate': 3.215342287272538e-05, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8798090052601447}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:10:15,429][0m Trial 17 finished with value: 0.06442021951079369 and parameters: {'observation_period_num': 13, 'train_rates': 0.8871374582817718, 'learning_rate': 3.607638016385963e-05, 'batch_size': 171, 'step_size': 12, 'gamma': 0.8411070587607079}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:11:00,495][0m Trial 18 finished with value: 0.18556463448562946 and parameters: {'observation_period_num': 51, 'train_rates': 0.7517741110169892, 'learning_rate': 0.00018581291033742923, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8223828174426213}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:11:28,317][0m Trial 19 finished with value: 0.1128723234461065 and parameters: {'observation_period_num': 70, 'train_rates': 0.8224160443686126, 'learning_rate': 4.771076418431566e-05, 'batch_size': 213, 'step_size': 10, 'gamma': 0.8961212365321239}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:12:01,330][0m Trial 20 finished with value: 0.0774863423133383 and parameters: {'observation_period_num': 5, 'train_rates': 0.8840443185906848, 'learning_rate': 1.6533092320407513e-05, 'batch_size': 176, 'step_size': 14, 'gamma': 0.8775590673132232}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:12:39,093][0m Trial 21 finished with value: 0.07341869077685971 and parameters: {'observation_period_num': 25, 'train_rates': 0.8831268810991053, 'learning_rate': 3.90483364012539e-05, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8443489115543238}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:13:14,763][0m Trial 22 finished with value: 0.0585481126925775 and parameters: {'observation_period_num': 31, 'train_rates': 0.8807073269116114, 'learning_rate': 9.15668280047644e-05, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8355726054149142}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:13:59,885][0m Trial 23 finished with value: 0.05919607713314375 and parameters: {'observation_period_num': 36, 'train_rates': 0.817954031799318, 'learning_rate': 9.369834515138964e-05, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7836463665000009}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:14:27,955][0m Trial 24 finished with value: 0.08354956562581815 and parameters: {'observation_period_num': 69, 'train_rates': 0.901335149519234, 'learning_rate': 0.00029717677993566354, 'batch_size': 231, 'step_size': 13, 'gamma': 0.8182923942682905}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:15:21,426][0m Trial 25 finished with value: 0.04670547368179323 and parameters: {'observation_period_num': 28, 'train_rates': 0.8178937693079865, 'learning_rate': 0.00016524407659795605, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8618423652356084}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:16:54,713][0m Trial 26 finished with value: 0.25435991446407064 and parameters: {'observation_period_num': 82, 'train_rates': 0.7144922687206422, 'learning_rate': 0.00015150870928469132, 'batch_size': 51, 'step_size': 1, 'gamma': 0.8628855652833736}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:17:50,677][0m Trial 27 finished with value: 0.06156921492013181 and parameters: {'observation_period_num': 31, 'train_rates': 0.8090309295038408, 'learning_rate': 0.0003617125033427179, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8956495180310973}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:18:51,639][0m Trial 28 finished with value: 0.6836874838466593 and parameters: {'observation_period_num': 60, 'train_rates': 0.615889590055318, 'learning_rate': 5.4083253022780174e-05, 'batch_size': 74, 'step_size': 8, 'gamma': 0.8540455170393682}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:19:48,427][0m Trial 29 finished with value: 0.22792233922374175 and parameters: {'observation_period_num': 96, 'train_rates': 0.7751758230909414, 'learning_rate': 0.00015880544587332216, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9662550161129646}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:20:27,237][0m Trial 30 finished with value: 0.49269936828805294 and parameters: {'observation_period_num': 44, 'train_rates': 0.8467780565412201, 'learning_rate': 3.1382857720148296e-06, 'batch_size': 146, 'step_size': 11, 'gamma': 0.888222294787565}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:21:18,856][0m Trial 31 finished with value: 0.04607428966820994 and parameters: {'observation_period_num': 23, 'train_rates': 0.8655521380895138, 'learning_rate': 9.002672316275771e-05, 'batch_size': 113, 'step_size': 15, 'gamma': 0.8340623709939943}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:22:17,190][0m Trial 32 finished with value: 0.041108258807670134 and parameters: {'observation_period_num': 20, 'train_rates': 0.861590479090065, 'learning_rate': 0.0001228326705419927, 'batch_size': 101, 'step_size': 15, 'gamma': 0.8164573529949555}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:23:07,606][0m Trial 33 finished with value: 0.045556810773199514 and parameters: {'observation_period_num': 7, 'train_rates': 0.8579184792093021, 'learning_rate': 6.742555642340624e-05, 'batch_size': 115, 'step_size': 15, 'gamma': 0.7916774214698341}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:23:53,551][0m Trial 34 finished with value: 0.07695683284067012 and parameters: {'observation_period_num': 5, 'train_rates': 0.9044601851083071, 'learning_rate': 1.9591628539208015e-05, 'batch_size': 130, 'step_size': 15, 'gamma': 0.7841960752239117}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:25:54,358][0m Trial 35 finished with value: 0.05643639216820399 and parameters: {'observation_period_num': 48, 'train_rates': 0.9506323684136573, 'learning_rate': 5.6756238400550796e-05, 'batch_size': 48, 'step_size': 15, 'gamma': 0.7946639928416214}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:26:58,344][0m Trial 36 finished with value: 0.06714591405847493 and parameters: {'observation_period_num': 20, 'train_rates': 0.8380878907556085, 'learning_rate': 2.0680324340401374e-05, 'batch_size': 86, 'step_size': 13, 'gamma': 0.8143939521275808}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:27:52,711][0m Trial 37 finished with value: 0.061199538807832556 and parameters: {'observation_period_num': 40, 'train_rates': 0.9103693693824527, 'learning_rate': 6.318452968051234e-05, 'batch_size': 110, 'step_size': 14, 'gamma': 0.7705544818148873}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:29:21,166][0m Trial 38 finished with value: 0.061485516484498376 and parameters: {'observation_period_num': 64, 'train_rates': 0.8630962702802994, 'learning_rate': 0.00011086499796160691, 'batch_size': 62, 'step_size': 15, 'gamma': 0.7976486934114525}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:32:06,502][0m Trial 39 finished with value: 0.10024989600060508 and parameters: {'observation_period_num': 54, 'train_rates': 0.9559417796057817, 'learning_rate': 0.0004724078777292152, 'batch_size': 35, 'step_size': 13, 'gamma': 0.8218755017341971}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:33:11,447][0m Trial 40 finished with value: 0.19820305401772526 and parameters: {'observation_period_num': 81, 'train_rates': 0.7645138019482769, 'learning_rate': 0.0001304371478303045, 'batch_size': 77, 'step_size': 11, 'gamma': 0.8038472210352529}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:34:04,203][0m Trial 41 finished with value: 0.04402880922337122 and parameters: {'observation_period_num': 15, 'train_rates': 0.8660190768683563, 'learning_rate': 8.155378081444704e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8284680775087476}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:34:44,782][0m Trial 42 finished with value: 0.04760599505737864 and parameters: {'observation_period_num': 13, 'train_rates': 0.8390181684064324, 'learning_rate': 7.497005020051201e-05, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8277801755449677}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:35:35,674][0m Trial 43 finished with value: 0.03536045651523595 and parameters: {'observation_period_num': 5, 'train_rates': 0.8667796031387666, 'learning_rate': 0.00023298445167653465, 'batch_size': 114, 'step_size': 15, 'gamma': 0.8509186638407742}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:36:30,981][0m Trial 44 finished with value: 0.03580057069556995 and parameters: {'observation_period_num': 18, 'train_rates': 0.8019302311598699, 'learning_rate': 0.0002805817293511475, 'batch_size': 98, 'step_size': 13, 'gamma': 0.8488196734672099}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:38:00,357][0m Trial 45 finished with value: 0.06830180374284585 and parameters: {'observation_period_num': 36, 'train_rates': 0.9385013480847254, 'learning_rate': 0.0002519000470451849, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8568477095887281}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:38:50,391][0m Trial 46 finished with value: 0.22650392342833686 and parameters: {'observation_period_num': 201, 'train_rates': 0.7980831643901407, 'learning_rate': 0.0007512810685831143, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9202448268812448}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:39:23,882][0m Trial 47 finished with value: 0.14649751058670968 and parameters: {'observation_period_num': 23, 'train_rates': 0.7023921868890886, 'learning_rate': 0.00041671045269118834, 'batch_size': 153, 'step_size': 12, 'gamma': 0.8445020486620048}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:40:23,680][0m Trial 48 finished with value: 0.09617813064749672 and parameters: {'observation_period_num': 107, 'train_rates': 0.8312684597377384, 'learning_rate': 0.00022464312872253964, 'batch_size': 89, 'step_size': 12, 'gamma': 0.869969414849801}. Best is trial 14 with value: 0.032848759782405534.[0m
[32m[I 2025-02-05 01:41:07,015][0m Trial 49 finished with value: 0.18984702179458604 and parameters: {'observation_period_num': 40, 'train_rates': 0.7935389397543121, 'learning_rate': 0.0006544276811556456, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8751186122073289}. Best is trial 14 with value: 0.032848759782405534.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4360 | 0.2791
Epoch 2/300, Loss: 0.1804 | 0.2290
Epoch 3/300, Loss: 0.1590 | 0.1850
Epoch 4/300, Loss: 0.1406 | 0.1442
Epoch 5/300, Loss: 0.1274 | 0.1209
Epoch 6/300, Loss: 0.1215 | 0.1050
Epoch 7/300, Loss: 0.1197 | 0.1000
Epoch 8/300, Loss: 0.1181 | 0.0990
Epoch 9/300, Loss: 0.1162 | 0.1059
Epoch 10/300, Loss: 0.1157 | 0.1181
Epoch 11/300, Loss: 0.1191 | 0.1106
Epoch 12/300, Loss: 0.1314 | 0.1020
Epoch 13/300, Loss: 0.1322 | 0.0947
Epoch 14/300, Loss: 0.1190 | 0.0728
Epoch 15/300, Loss: 0.1052 | 0.0721
Epoch 16/300, Loss: 0.1085 | 0.0738
Epoch 17/300, Loss: 0.1123 | 0.0767
Epoch 18/300, Loss: 0.1135 | 0.0846
Epoch 19/300, Loss: 0.1115 | 0.0849
Epoch 20/300, Loss: 0.1062 | 0.0800
Epoch 21/300, Loss: 0.1015 | 0.0729
Epoch 22/300, Loss: 0.0982 | 0.0669
Epoch 23/300, Loss: 0.0963 | 0.0640
Epoch 24/300, Loss: 0.0952 | 0.0605
Epoch 25/300, Loss: 0.0948 | 0.0592
Epoch 26/300, Loss: 0.0945 | 0.0585
Epoch 27/300, Loss: 0.0939 | 0.0579
Epoch 28/300, Loss: 0.0931 | 0.0571
Epoch 29/300, Loss: 0.0921 | 0.0562
Epoch 30/300, Loss: 0.0912 | 0.0554
Epoch 31/300, Loss: 0.0900 | 0.0549
Epoch 32/300, Loss: 0.0890 | 0.0544
Epoch 33/300, Loss: 0.0881 | 0.0539
Epoch 34/300, Loss: 0.0873 | 0.0536
Epoch 35/300, Loss: 0.0866 | 0.0528
Epoch 36/300, Loss: 0.0859 | 0.0520
Epoch 37/300, Loss: 0.0852 | 0.0510
Epoch 38/300, Loss: 0.0845 | 0.0501
Epoch 39/300, Loss: 0.0839 | 0.0492
Epoch 40/300, Loss: 0.0833 | 0.0486
Epoch 41/300, Loss: 0.0828 | 0.0478
Epoch 42/300, Loss: 0.0823 | 0.0470
Epoch 43/300, Loss: 0.0818 | 0.0464
Epoch 44/300, Loss: 0.0813 | 0.0457
Epoch 45/300, Loss: 0.0809 | 0.0453
Epoch 46/300, Loss: 0.0805 | 0.0449
Epoch 47/300, Loss: 0.0802 | 0.0444
Epoch 48/300, Loss: 0.0798 | 0.0440
Epoch 49/300, Loss: 0.0795 | 0.0436
Epoch 50/300, Loss: 0.0792 | 0.0433
Epoch 51/300, Loss: 0.0790 | 0.0430
Epoch 52/300, Loss: 0.0787 | 0.0428
Epoch 53/300, Loss: 0.0785 | 0.0425
Epoch 54/300, Loss: 0.0783 | 0.0422
Epoch 55/300, Loss: 0.0781 | 0.0420
Epoch 56/300, Loss: 0.0779 | 0.0419
Epoch 57/300, Loss: 0.0777 | 0.0417
Epoch 58/300, Loss: 0.0776 | 0.0415
Epoch 59/300, Loss: 0.0774 | 0.0414
Epoch 60/300, Loss: 0.0773 | 0.0412
Epoch 61/300, Loss: 0.0771 | 0.0410
Epoch 62/300, Loss: 0.0769 | 0.0409
Epoch 63/300, Loss: 0.0768 | 0.0408
Epoch 64/300, Loss: 0.0767 | 0.0406
Epoch 65/300, Loss: 0.0766 | 0.0405
Epoch 66/300, Loss: 0.0764 | 0.0404
Epoch 67/300, Loss: 0.0763 | 0.0403
Epoch 68/300, Loss: 0.0762 | 0.0402
Epoch 69/300, Loss: 0.0761 | 0.0401
Epoch 70/300, Loss: 0.0760 | 0.0400
Epoch 71/300, Loss: 0.0759 | 0.0398
Epoch 72/300, Loss: 0.0758 | 0.0397
Epoch 73/300, Loss: 0.0757 | 0.0396
Epoch 74/300, Loss: 0.0756 | 0.0395
Epoch 75/300, Loss: 0.0755 | 0.0394
Epoch 76/300, Loss: 0.0754 | 0.0394
Epoch 77/300, Loss: 0.0753 | 0.0393
Epoch 78/300, Loss: 0.0753 | 0.0392
Epoch 79/300, Loss: 0.0752 | 0.0391
Epoch 80/300, Loss: 0.0751 | 0.0391
Epoch 81/300, Loss: 0.0750 | 0.0390
Epoch 82/300, Loss: 0.0749 | 0.0390
Epoch 83/300, Loss: 0.0748 | 0.0389
Epoch 84/300, Loss: 0.0747 | 0.0390
Epoch 85/300, Loss: 0.0746 | 0.0389
Epoch 86/300, Loss: 0.0746 | 0.0389
Epoch 87/300, Loss: 0.0745 | 0.0388
Epoch 88/300, Loss: 0.0745 | 0.0388
Epoch 89/300, Loss: 0.0744 | 0.0388
Epoch 90/300, Loss: 0.0743 | 0.0388
Epoch 91/300, Loss: 0.0743 | 0.0387
Epoch 92/300, Loss: 0.0742 | 0.0387
Epoch 93/300, Loss: 0.0742 | 0.0386
Epoch 94/300, Loss: 0.0741 | 0.0386
Epoch 95/300, Loss: 0.0741 | 0.0386
Epoch 96/300, Loss: 0.0740 | 0.0385
Epoch 97/300, Loss: 0.0740 | 0.0385
Epoch 98/300, Loss: 0.0739 | 0.0384
Epoch 99/300, Loss: 0.0739 | 0.0384
Epoch 100/300, Loss: 0.0739 | 0.0384
Epoch 101/300, Loss: 0.0738 | 0.0383
Epoch 102/300, Loss: 0.0738 | 0.0383
Epoch 103/300, Loss: 0.0738 | 0.0383
Epoch 104/300, Loss: 0.0737 | 0.0383
Epoch 105/300, Loss: 0.0737 | 0.0382
Epoch 106/300, Loss: 0.0737 | 0.0382
Epoch 107/300, Loss: 0.0737 | 0.0382
Epoch 108/300, Loss: 0.0736 | 0.0382
Epoch 109/300, Loss: 0.0736 | 0.0382
Epoch 110/300, Loss: 0.0736 | 0.0382
Epoch 111/300, Loss: 0.0736 | 0.0382
Epoch 112/300, Loss: 0.0735 | 0.0382
Epoch 113/300, Loss: 0.0735 | 0.0382
Epoch 114/300, Loss: 0.0735 | 0.0382
Epoch 115/300, Loss: 0.0735 | 0.0382
Epoch 116/300, Loss: 0.0734 | 0.0382
Epoch 117/300, Loss: 0.0734 | 0.0383
Epoch 118/300, Loss: 0.0734 | 0.0383
Epoch 119/300, Loss: 0.0734 | 0.0382
Epoch 120/300, Loss: 0.0733 | 0.0382
Epoch 121/300, Loss: 0.0733 | 0.0382
Epoch 122/300, Loss: 0.0732 | 0.0382
Epoch 123/300, Loss: 0.0732 | 0.0381
Epoch 124/300, Loss: 0.0731 | 0.0381
Epoch 125/300, Loss: 0.0731 | 0.0380
Epoch 126/300, Loss: 0.0731 | 0.0380
Epoch 127/300, Loss: 0.0731 | 0.0380
Epoch 128/300, Loss: 0.0730 | 0.0380
Epoch 129/300, Loss: 0.0730 | 0.0380
Epoch 130/300, Loss: 0.0730 | 0.0379
Epoch 131/300, Loss: 0.0730 | 0.0379
Epoch 132/300, Loss: 0.0730 | 0.0379
Epoch 133/300, Loss: 0.0730 | 0.0379
Epoch 134/300, Loss: 0.0729 | 0.0379
Epoch 135/300, Loss: 0.0729 | 0.0379
Epoch 136/300, Loss: 0.0729 | 0.0379
Epoch 137/300, Loss: 0.0729 | 0.0379
Epoch 138/300, Loss: 0.0729 | 0.0379
Epoch 139/300, Loss: 0.0729 | 0.0379
Epoch 140/300, Loss: 0.0729 | 0.0379
Epoch 141/300, Loss: 0.0728 | 0.0379
Epoch 142/300, Loss: 0.0728 | 0.0379
Epoch 143/300, Loss: 0.0728 | 0.0378
Epoch 144/300, Loss: 0.0728 | 0.0378
Epoch 145/300, Loss: 0.0728 | 0.0378
Epoch 146/300, Loss: 0.0728 | 0.0378
Epoch 147/300, Loss: 0.0728 | 0.0378
Epoch 148/300, Loss: 0.0728 | 0.0378
Epoch 149/300, Loss: 0.0728 | 0.0378
Epoch 150/300, Loss: 0.0727 | 0.0378
Epoch 151/300, Loss: 0.0727 | 0.0378
Epoch 152/300, Loss: 0.0727 | 0.0378
Epoch 153/300, Loss: 0.0727 | 0.0378
Epoch 154/300, Loss: 0.0727 | 0.0378
Epoch 155/300, Loss: 0.0727 | 0.0378
Epoch 156/300, Loss: 0.0727 | 0.0378
Epoch 157/300, Loss: 0.0727 | 0.0378
Epoch 158/300, Loss: 0.0727 | 0.0378
Epoch 159/300, Loss: 0.0727 | 0.0378
Epoch 160/300, Loss: 0.0727 | 0.0378
Epoch 161/300, Loss: 0.0727 | 0.0378
Epoch 162/300, Loss: 0.0726 | 0.0378
Epoch 163/300, Loss: 0.0726 | 0.0378
Epoch 164/300, Loss: 0.0726 | 0.0378
Epoch 165/300, Loss: 0.0726 | 0.0377
Epoch 166/300, Loss: 0.0726 | 0.0377
Epoch 167/300, Loss: 0.0726 | 0.0377
Epoch 168/300, Loss: 0.0726 | 0.0377
Epoch 169/300, Loss: 0.0726 | 0.0377
Epoch 170/300, Loss: 0.0726 | 0.0377
Epoch 171/300, Loss: 0.0726 | 0.0377
Epoch 172/300, Loss: 0.0726 | 0.0377
Epoch 173/300, Loss: 0.0726 | 0.0377
Epoch 174/300, Loss: 0.0726 | 0.0377
Epoch 175/300, Loss: 0.0726 | 0.0377
Epoch 176/300, Loss: 0.0726 | 0.0377
Epoch 177/300, Loss: 0.0726 | 0.0377
Epoch 178/300, Loss: 0.0726 | 0.0377
Epoch 179/300, Loss: 0.0726 | 0.0377
Epoch 180/300, Loss: 0.0726 | 0.0377
Epoch 181/300, Loss: 0.0726 | 0.0377
Epoch 182/300, Loss: 0.0725 | 0.0377
Epoch 183/300, Loss: 0.0725 | 0.0377
Epoch 184/300, Loss: 0.0725 | 0.0377
Epoch 185/300, Loss: 0.0725 | 0.0377
Epoch 186/300, Loss: 0.0725 | 0.0377
Epoch 187/300, Loss: 0.0725 | 0.0377
Epoch 188/300, Loss: 0.0725 | 0.0377
Epoch 189/300, Loss: 0.0725 | 0.0377
Epoch 190/300, Loss: 0.0725 | 0.0377
Epoch 191/300, Loss: 0.0725 | 0.0377
Epoch 192/300, Loss: 0.0725 | 0.0377
Epoch 193/300, Loss: 0.0725 | 0.0377
Epoch 194/300, Loss: 0.0725 | 0.0377
Epoch 195/300, Loss: 0.0725 | 0.0377
Epoch 196/300, Loss: 0.0725 | 0.0377
Epoch 197/300, Loss: 0.0725 | 0.0377
Epoch 198/300, Loss: 0.0725 | 0.0377
Epoch 199/300, Loss: 0.0725 | 0.0377
Epoch 200/300, Loss: 0.0725 | 0.0377
Epoch 201/300, Loss: 0.0725 | 0.0377
Epoch 202/300, Loss: 0.0725 | 0.0377
Epoch 203/300, Loss: 0.0725 | 0.0377
Epoch 204/300, Loss: 0.0725 | 0.0377
Epoch 205/300, Loss: 0.0725 | 0.0377
Epoch 206/300, Loss: 0.0725 | 0.0377
Epoch 207/300, Loss: 0.0725 | 0.0377
Epoch 208/300, Loss: 0.0725 | 0.0377
Epoch 209/300, Loss: 0.0725 | 0.0377
Epoch 210/300, Loss: 0.0725 | 0.0377
Epoch 211/300, Loss: 0.0725 | 0.0377
Epoch 212/300, Loss: 0.0725 | 0.0377
Epoch 213/300, Loss: 0.0725 | 0.0377
Epoch 214/300, Loss: 0.0725 | 0.0377
Epoch 215/300, Loss: 0.0725 | 0.0377
Epoch 216/300, Loss: 0.0725 | 0.0377
Epoch 217/300, Loss: 0.0725 | 0.0377
Epoch 218/300, Loss: 0.0725 | 0.0377
Epoch 219/300, Loss: 0.0725 | 0.0377
Epoch 220/300, Loss: 0.0725 | 0.0377
Epoch 221/300, Loss: 0.0725 | 0.0377
Epoch 222/300, Loss: 0.0725 | 0.0377
Epoch 223/300, Loss: 0.0725 | 0.0377
Epoch 224/300, Loss: 0.0725 | 0.0377
Epoch 225/300, Loss: 0.0725 | 0.0377
Epoch 226/300, Loss: 0.0725 | 0.0377
Epoch 227/300, Loss: 0.0725 | 0.0377
Epoch 228/300, Loss: 0.0725 | 0.0377
Epoch 229/300, Loss: 0.0725 | 0.0377
Epoch 230/300, Loss: 0.0725 | 0.0377
Epoch 231/300, Loss: 0.0725 | 0.0377
Epoch 232/300, Loss: 0.0725 | 0.0377
Epoch 233/300, Loss: 0.0725 | 0.0377
Epoch 234/300, Loss: 0.0725 | 0.0377
Epoch 235/300, Loss: 0.0725 | 0.0377
Epoch 236/300, Loss: 0.0725 | 0.0377
Epoch 237/300, Loss: 0.0725 | 0.0377
Epoch 238/300, Loss: 0.0725 | 0.0377
Epoch 239/300, Loss: 0.0724 | 0.0377
Epoch 240/300, Loss: 0.0724 | 0.0377
Epoch 241/300, Loss: 0.0724 | 0.0377
Epoch 242/300, Loss: 0.0724 | 0.0377
Epoch 243/300, Loss: 0.0724 | 0.0377
Epoch 244/300, Loss: 0.0724 | 0.0377
Epoch 245/300, Loss: 0.0724 | 0.0377
Epoch 246/300, Loss: 0.0724 | 0.0377
Epoch 247/300, Loss: 0.0724 | 0.0377
Epoch 248/300, Loss: 0.0724 | 0.0377
Epoch 249/300, Loss: 0.0724 | 0.0377
Epoch 250/300, Loss: 0.0724 | 0.0377
Epoch 251/300, Loss: 0.0724 | 0.0377
Epoch 252/300, Loss: 0.0724 | 0.0377
Epoch 253/300, Loss: 0.0724 | 0.0377
Epoch 254/300, Loss: 0.0724 | 0.0377
Epoch 255/300, Loss: 0.0724 | 0.0377
Epoch 256/300, Loss: 0.0724 | 0.0377
Epoch 257/300, Loss: 0.0724 | 0.0377
Epoch 258/300, Loss: 0.0724 | 0.0377
Epoch 259/300, Loss: 0.0724 | 0.0377
Epoch 260/300, Loss: 0.0724 | 0.0377
Epoch 261/300, Loss: 0.0724 | 0.0377
Epoch 262/300, Loss: 0.0724 | 0.0377
Epoch 263/300, Loss: 0.0724 | 0.0377
Epoch 264/300, Loss: 0.0724 | 0.0377
Epoch 265/300, Loss: 0.0724 | 0.0377
Epoch 266/300, Loss: 0.0724 | 0.0377
Epoch 267/300, Loss: 0.0724 | 0.0377
Epoch 268/300, Loss: 0.0724 | 0.0377
Epoch 269/300, Loss: 0.0724 | 0.0377
Epoch 270/300, Loss: 0.0724 | 0.0377
Epoch 271/300, Loss: 0.0724 | 0.0377
Epoch 272/300, Loss: 0.0724 | 0.0377
Epoch 273/300, Loss: 0.0724 | 0.0377
Epoch 274/300, Loss: 0.0724 | 0.0377
Epoch 275/300, Loss: 0.0724 | 0.0376
Epoch 276/300, Loss: 0.0724 | 0.0376
Epoch 277/300, Loss: 0.0724 | 0.0376
Epoch 278/300, Loss: 0.0724 | 0.0376
Epoch 279/300, Loss: 0.0724 | 0.0376
Epoch 280/300, Loss: 0.0724 | 0.0376
Epoch 281/300, Loss: 0.0724 | 0.0376
Epoch 282/300, Loss: 0.0724 | 0.0376
Epoch 283/300, Loss: 0.0724 | 0.0376
Epoch 284/300, Loss: 0.0724 | 0.0376
Epoch 285/300, Loss: 0.0724 | 0.0376
Epoch 286/300, Loss: 0.0724 | 0.0376
Epoch 287/300, Loss: 0.0724 | 0.0376
Epoch 288/300, Loss: 0.0724 | 0.0376
Epoch 289/300, Loss: 0.0724 | 0.0376
Epoch 290/300, Loss: 0.0724 | 0.0376
Epoch 291/300, Loss: 0.0724 | 0.0376
Epoch 292/300, Loss: 0.0724 | 0.0376
Epoch 293/300, Loss: 0.0724 | 0.0376
Epoch 294/300, Loss: 0.0724 | 0.0376
Epoch 295/300, Loss: 0.0724 | 0.0376
Epoch 296/300, Loss: 0.0724 | 0.0376
Epoch 297/300, Loss: 0.0724 | 0.0376
Epoch 298/300, Loss: 0.0724 | 0.0376
Epoch 299/300, Loss: 0.0724 | 0.0376
Epoch 300/300, Loss: 0.0724 | 0.0376
Runtime (seconds): 272.5511770248413
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 81.59131573722698
RMSE: 9.032791137695312
MAE: 9.032791137695312
R-squared: nan
[166.85767]
