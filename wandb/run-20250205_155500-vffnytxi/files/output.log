[32m[I 2025-02-05 15:55:05,488][0m A new study created in memory with name: no-name-19d283df-55b8-4fc2-9f36-dfdcd4672fd8[0m
Early stopping at epoch 48
[32m[I 2025-02-05 15:55:19,510][0m Trial 0 finished with value: 1.2050156814080697 and parameters: {'observation_period_num': 94, 'train_rates': 0.746872908886406, 'learning_rate': 3.928713553565701e-06, 'batch_size': 207, 'step_size': 1, 'gamma': 0.8020613934697406}. Best is trial 0 with value: 1.2050156814080697.[0m
[32m[I 2025-02-05 15:59:04,439][0m Trial 1 finished with value: 0.23478195069519353 and parameters: {'observation_period_num': 67, 'train_rates': 0.7740443003978138, 'learning_rate': 0.00031756076157927815, 'batch_size': 22, 'step_size': 14, 'gamma': 0.9383836126034402}. Best is trial 1 with value: 0.23478195069519353.[0m
[32m[I 2025-02-05 15:59:49,599][0m Trial 2 finished with value: 0.49339550733566284 and parameters: {'observation_period_num': 95, 'train_rates': 0.9708351409551345, 'learning_rate': 6.195161237636367e-06, 'batch_size': 137, 'step_size': 12, 'gamma': 0.9576101799480701}. Best is trial 1 with value: 0.23478195069519353.[0m
[32m[I 2025-02-05 16:00:29,596][0m Trial 3 finished with value: 0.27222872521871555 and parameters: {'observation_period_num': 231, 'train_rates': 0.848405142942056, 'learning_rate': 0.0004236638166354262, 'batch_size': 133, 'step_size': 11, 'gamma': 0.9841797302884043}. Best is trial 1 with value: 0.23478195069519353.[0m
[32m[I 2025-02-05 16:00:55,709][0m Trial 4 finished with value: 0.2688566359146586 and parameters: {'observation_period_num': 119, 'train_rates': 0.7215029012716105, 'learning_rate': 0.0005113833191306538, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8669870599610026}. Best is trial 1 with value: 0.23478195069519353.[0m
[32m[I 2025-02-05 16:01:35,512][0m Trial 5 finished with value: 0.18261904890948782 and parameters: {'observation_period_num': 53, 'train_rates': 0.721279008659637, 'learning_rate': 0.00025159047716529073, 'batch_size': 126, 'step_size': 5, 'gamma': 0.8462331351707274}. Best is trial 5 with value: 0.18261904890948782.[0m
[32m[I 2025-02-05 16:02:32,201][0m Trial 6 finished with value: 0.06519479703690324 and parameters: {'observation_period_num': 14, 'train_rates': 0.7964753976532916, 'learning_rate': 8.323304061217994e-05, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8568203331971245}. Best is trial 6 with value: 0.06519479703690324.[0m
[32m[I 2025-02-05 16:03:47,817][0m Trial 7 finished with value: 0.08737316157648473 and parameters: {'observation_period_num': 67, 'train_rates': 0.8741313523338613, 'learning_rate': 0.0003403917604709947, 'batch_size': 74, 'step_size': 1, 'gamma': 0.91655387457292}. Best is trial 6 with value: 0.06519479703690324.[0m
[32m[I 2025-02-05 16:04:35,682][0m Trial 8 finished with value: 0.2256031970005082 and parameters: {'observation_period_num': 92, 'train_rates': 0.6804069210748407, 'learning_rate': 0.00010464107861217144, 'batch_size': 99, 'step_size': 5, 'gamma': 0.8027403631845399}. Best is trial 6 with value: 0.06519479703690324.[0m
[32m[I 2025-02-05 16:05:57,905][0m Trial 9 finished with value: 0.06252930952874526 and parameters: {'observation_period_num': 42, 'train_rates': 0.8796820072909939, 'learning_rate': 4.271694938355024e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.9345293329728432}. Best is trial 9 with value: 0.06252930952874526.[0m
[32m[I 2025-02-05 16:09:14,524][0m Trial 10 finished with value: 0.35334013766712613 and parameters: {'observation_period_num': 187, 'train_rates': 0.9840695515918972, 'learning_rate': 1.622371903697251e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7525017276270893}. Best is trial 9 with value: 0.06252930952874526.[0m
[32m[I 2025-02-05 16:10:41,256][0m Trial 11 finished with value: 0.04094599912224232 and parameters: {'observation_period_num': 11, 'train_rates': 0.8568266426589235, 'learning_rate': 6.471614053013314e-05, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9016617281797066}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:12:25,477][0m Trial 12 finished with value: 0.04306921471578372 and parameters: {'observation_period_num': 8, 'train_rates': 0.8890805921196568, 'learning_rate': 2.9683009006484906e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9089638907992662}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:14:17,789][0m Trial 13 finished with value: 0.07455555006277327 and parameters: {'observation_period_num': 9, 'train_rates': 0.9314343552224141, 'learning_rate': 1.187741840914362e-05, 'batch_size': 52, 'step_size': 8, 'gamma': 0.8938659776858993}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:14:41,091][0m Trial 14 finished with value: 1.2594533640851258 and parameters: {'observation_period_num': 155, 'train_rates': 0.8349729010073809, 'learning_rate': 1.1028271312301853e-06, 'batch_size': 254, 'step_size': 3, 'gamma': 0.895901768729454}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:16:22,576][0m Trial 15 finished with value: 0.04324153237739357 and parameters: {'observation_period_num': 8, 'train_rates': 0.9206678203812494, 'learning_rate': 4.723051296790093e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.8970942867570466}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:17:08,417][0m Trial 16 finished with value: 0.3638850038405508 and parameters: {'observation_period_num': 36, 'train_rates': 0.6295078429625575, 'learning_rate': 2.1797555421966982e-05, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8299739626385605}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:17:43,798][0m Trial 17 finished with value: 0.2141108657799515 and parameters: {'observation_period_num': 164, 'train_rates': 0.9092029802071295, 'learning_rate': 0.00011676890564637754, 'batch_size': 161, 'step_size': 6, 'gamma': 0.9896170346940222}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:22:28,036][0m Trial 18 finished with value: 0.2052888405819734 and parameters: {'observation_period_num': 232, 'train_rates': 0.8138329268224846, 'learning_rate': 7.343243230320513e-06, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9536040119377133}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:24:48,889][0m Trial 19 finished with value: 0.14083528845000637 and parameters: {'observation_period_num': 26, 'train_rates': 0.8687283151248634, 'learning_rate': 2.8615079610540094e-06, 'batch_size': 39, 'step_size': 9, 'gamma': 0.9159942898080811}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:25:56,439][0m Trial 20 finished with value: 0.1717445639272531 and parameters: {'observation_period_num': 119, 'train_rates': 0.9497495786944872, 'learning_rate': 0.0009909552247017277, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8864098342698011}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:27:38,412][0m Trial 21 finished with value: 0.042894771594679755 and parameters: {'observation_period_num': 8, 'train_rates': 0.9099076606067809, 'learning_rate': 4.161231591309671e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9104650580210174}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:29:32,508][0m Trial 22 finished with value: 0.06830955740973302 and parameters: {'observation_period_num': 56, 'train_rates': 0.9004461920975999, 'learning_rate': 3.2439159642224684e-05, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9173059977154747}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:30:44,802][0m Trial 23 finished with value: 0.05071282467764357 and parameters: {'observation_period_num': 25, 'train_rates': 0.8360542367305734, 'learning_rate': 7.829418530278113e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.8746422375127247}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:31:38,677][0m Trial 24 finished with value: 0.07929912716150284 and parameters: {'observation_period_num': 5, 'train_rates': 0.9495818063379846, 'learning_rate': 2.4016540534951802e-05, 'batch_size': 116, 'step_size': 3, 'gamma': 0.9351554131087846}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:34:01,082][0m Trial 25 finished with value: 0.06806290073060914 and parameters: {'observation_period_num': 77, 'train_rates': 0.8908840559092428, 'learning_rate': 5.41984974991306e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9604993447094863}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:34:36,961][0m Trial 26 finished with value: 0.055025757296935665 and parameters: {'observation_period_num': 36, 'train_rates': 0.8540283536212496, 'learning_rate': 0.00017540409465397703, 'batch_size': 162, 'step_size': 8, 'gamma': 0.8325226155631053}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:35:59,771][0m Trial 27 finished with value: 0.08814926561413498 and parameters: {'observation_period_num': 49, 'train_rates': 0.8196948798615316, 'learning_rate': 1.3853732966789272e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.9144569095362212}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:37:02,046][0m Trial 28 finished with value: 0.17838661559208752 and parameters: {'observation_period_num': 28, 'train_rates': 0.776221039473956, 'learning_rate': 0.00018729363434127653, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8784719322858591}. Best is trial 11 with value: 0.04094599912224232.[0m
Early stopping at epoch 86
[32m[I 2025-02-05 16:39:13,935][0m Trial 29 finished with value: 0.31245771806005024 and parameters: {'observation_period_num': 87, 'train_rates': 0.9510707315930256, 'learning_rate': 6.345488431125861e-05, 'batch_size': 38, 'step_size': 1, 'gamma': 0.8533086632568239}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:39:54,946][0m Trial 30 finished with value: 0.17747757284052343 and parameters: {'observation_period_num': 23, 'train_rates': 0.9149962558475164, 'learning_rate': 2.7685925347821293e-05, 'batch_size': 148, 'step_size': 6, 'gamma': 0.7555090700844374}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:41:38,740][0m Trial 31 finished with value: 0.0445283750264809 and parameters: {'observation_period_num': 7, 'train_rates': 0.9220295779763114, 'learning_rate': 4.109892847422956e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.9013132546516075}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:43:16,637][0m Trial 32 finished with value: 0.04187993588714512 and parameters: {'observation_period_num': 5, 'train_rates': 0.8903421197609592, 'learning_rate': 5.1649348514574885e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9064198451873657}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:48:51,274][0m Trial 33 finished with value: 0.13973927450433019 and parameters: {'observation_period_num': 55, 'train_rates': 0.871016257922339, 'learning_rate': 0.00014143863323900398, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9291130764110248}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:50:01,822][0m Trial 34 finished with value: 0.10650409994617341 and parameters: {'observation_period_num': 20, 'train_rates': 0.8935519276386471, 'learning_rate': 9.815941731697232e-06, 'batch_size': 80, 'step_size': 4, 'gamma': 0.9695357660589331}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:50:50,372][0m Trial 35 finished with value: 0.23959692342018862 and parameters: {'observation_period_num': 38, 'train_rates': 0.8515281116910962, 'learning_rate': 4.578345889941951e-06, 'batch_size': 114, 'step_size': 12, 'gamma': 0.9464599188374083}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:53:19,112][0m Trial 36 finished with value: 0.20177528113126755 and parameters: {'observation_period_num': 73, 'train_rates': 0.7532953426457949, 'learning_rate': 1.8524520723328837e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.9086829378948005}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:55:23,669][0m Trial 37 finished with value: 0.19790751487016678 and parameters: {'observation_period_num': 212, 'train_rates': 0.9687474184306601, 'learning_rate': 3.4324589245474136e-05, 'batch_size': 45, 'step_size': 4, 'gamma': 0.9278358843374146}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:56:44,755][0m Trial 38 finished with value: 0.09888070017043883 and parameters: {'observation_period_num': 110, 'train_rates': 0.8124642564427323, 'learning_rate': 7.185586788035934e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8664521335291178}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:57:41,017][0m Trial 39 finished with value: 0.20099255547167794 and parameters: {'observation_period_num': 66, 'train_rates': 0.7866750311571854, 'learning_rate': 0.00010167695290935854, 'batch_size': 94, 'step_size': 8, 'gamma': 0.8841283872926637}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:58:09,773][0m Trial 40 finished with value: 0.2397717982530594 and parameters: {'observation_period_num': 148, 'train_rates': 0.9362085118576738, 'learning_rate': 5.936552749942829e-05, 'batch_size': 212, 'step_size': 5, 'gamma': 0.9456162110695836}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 16:59:53,318][0m Trial 41 finished with value: 0.04167934496925302 and parameters: {'observation_period_num': 15, 'train_rates': 0.901225165621039, 'learning_rate': 5.002731941085327e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9018174623234286}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:03:30,296][0m Trial 42 finished with value: 0.04661931051845545 and parameters: {'observation_period_num': 20, 'train_rates': 0.8606110243774017, 'learning_rate': 3.1244373613151634e-05, 'batch_size': 25, 'step_size': 7, 'gamma': 0.9052233500584894}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:04:53,968][0m Trial 43 finished with value: 0.06570399038587944 and parameters: {'observation_period_num': 43, 'train_rates': 0.8923090980339914, 'learning_rate': 4.721618728439539e-05, 'batch_size': 67, 'step_size': 9, 'gamma': 0.9257159683101632}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:06:45,599][0m Trial 44 finished with value: 0.05199046994931468 and parameters: {'observation_period_num': 16, 'train_rates': 0.8808635297835844, 'learning_rate': 0.00024275024598598865, 'batch_size': 51, 'step_size': 6, 'gamma': 0.8923047129598538}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:07:47,298][0m Trial 45 finished with value: 0.05902944059919906 and parameters: {'observation_period_num': 32, 'train_rates': 0.8400021669943388, 'learning_rate': 8.34885634920958e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8671593243974259}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:09:11,032][0m Trial 46 finished with value: 0.07599648088216782 and parameters: {'observation_period_num': 5, 'train_rates': 0.9830677887360613, 'learning_rate': 2.0151887457028845e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.9197321426825096}. Best is trial 11 with value: 0.04094599912224232.[0m
[32m[I 2025-02-05 17:12:38,785][0m Trial 47 finished with value: 0.03598472333508111 and parameters: {'observation_period_num': 17, 'train_rates': 0.9380636438982618, 'learning_rate': 3.611148797160303e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.973246447173335}. Best is trial 47 with value: 0.03598472333508111.[0m
[32m[I 2025-02-05 17:16:07,014][0m Trial 48 finished with value: 0.0921401852911169 and parameters: {'observation_period_num': 62, 'train_rates': 0.9622312717649104, 'learning_rate': 0.00012459247754078727, 'batch_size': 28, 'step_size': 2, 'gamma': 0.968074869307512}. Best is trial 47 with value: 0.03598472333508111.[0m
[32m[I 2025-02-05 17:17:02,646][0m Trial 49 finished with value: 0.09339253401909119 and parameters: {'observation_period_num': 47, 'train_rates': 0.9334626843815018, 'learning_rate': 4.142682691979481e-05, 'batch_size': 106, 'step_size': 11, 'gamma': 0.9398408776825989}. Best is trial 47 with value: 0.03598472333508111.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.5750 | 0.4677
Epoch 2/300, Loss: 0.2054 | 0.2981
Epoch 3/300, Loss: 0.1678 | 0.2600
Epoch 4/300, Loss: 0.1503 | 0.2357
Epoch 5/300, Loss: 0.1376 | 0.2187
Epoch 6/300, Loss: 0.1280 | 0.2032
Epoch 7/300, Loss: 0.1206 | 0.1857
Epoch 8/300, Loss: 0.1154 | 0.1671
Epoch 9/300, Loss: 0.1119 | 0.1529
Epoch 10/300, Loss: 0.1092 | 0.1384
Epoch 11/300, Loss: 0.1069 | 0.1260
Epoch 12/300, Loss: 0.1049 | 0.1156
Epoch 13/300, Loss: 0.1030 | 0.1088
Epoch 14/300, Loss: 0.1013 | 0.1015
Epoch 15/300, Loss: 0.0996 | 0.0953
Epoch 16/300, Loss: 0.0981 | 0.0901
Epoch 17/300, Loss: 0.0968 | 0.0866
Epoch 18/300, Loss: 0.0955 | 0.0827
Epoch 19/300, Loss: 0.0942 | 0.0796
Epoch 20/300, Loss: 0.0930 | 0.0770
Epoch 21/300, Loss: 0.0919 | 0.0752
Epoch 22/300, Loss: 0.0907 | 0.0734
Epoch 23/300, Loss: 0.0896 | 0.0720
Epoch 24/300, Loss: 0.0885 | 0.0709
Epoch 25/300, Loss: 0.0874 | 0.0696
Epoch 26/300, Loss: 0.0865 | 0.0687
Epoch 27/300, Loss: 0.0856 | 0.0678
Epoch 28/300, Loss: 0.0848 | 0.0669
Epoch 29/300, Loss: 0.0841 | 0.0653
Epoch 30/300, Loss: 0.0834 | 0.0643
Epoch 31/300, Loss: 0.0828 | 0.0633
Epoch 32/300, Loss: 0.0822 | 0.0622
Epoch 33/300, Loss: 0.0815 | 0.0605
Epoch 34/300, Loss: 0.0809 | 0.0595
Epoch 35/300, Loss: 0.0802 | 0.0585
Epoch 36/300, Loss: 0.0796 | 0.0575
Epoch 37/300, Loss: 0.0790 | 0.0560
Epoch 38/300, Loss: 0.0783 | 0.0551
Epoch 39/300, Loss: 0.0777 | 0.0543
Epoch 40/300, Loss: 0.0771 | 0.0535
Epoch 41/300, Loss: 0.0766 | 0.0521
Epoch 42/300, Loss: 0.0761 | 0.0514
Epoch 43/300, Loss: 0.0756 | 0.0507
Epoch 44/300, Loss: 0.0751 | 0.0501
Epoch 45/300, Loss: 0.0747 | 0.0490
Epoch 46/300, Loss: 0.0743 | 0.0485
Epoch 47/300, Loss: 0.0740 | 0.0480
Epoch 48/300, Loss: 0.0737 | 0.0476
Epoch 49/300, Loss: 0.0734 | 0.0468
Epoch 50/300, Loss: 0.0731 | 0.0464
Epoch 51/300, Loss: 0.0728 | 0.0460
Epoch 52/300, Loss: 0.0725 | 0.0457
Epoch 53/300, Loss: 0.0723 | 0.0456
Epoch 54/300, Loss: 0.0721 | 0.0456
Epoch 55/300, Loss: 0.0718 | 0.0457
Epoch 56/300, Loss: 0.0716 | 0.0460
Epoch 57/300, Loss: 0.0713 | 0.0474
Epoch 58/300, Loss: 0.0711 | 0.0483
Epoch 59/300, Loss: 0.0709 | 0.0491
Epoch 60/300, Loss: 0.0706 | 0.0498
Epoch 61/300, Loss: 0.0703 | 0.0514
Epoch 62/300, Loss: 0.0701 | 0.0515
Epoch 63/300, Loss: 0.0698 | 0.0509
Epoch 64/300, Loss: 0.0695 | 0.0497
Epoch 65/300, Loss: 0.0691 | 0.0483
Epoch 66/300, Loss: 0.0689 | 0.0465
Epoch 67/300, Loss: 0.0686 | 0.0447
Epoch 68/300, Loss: 0.0683 | 0.0432
Epoch 69/300, Loss: 0.0681 | 0.0419
Epoch 70/300, Loss: 0.0679 | 0.0412
Epoch 71/300, Loss: 0.0677 | 0.0408
Epoch 72/300, Loss: 0.0675 | 0.0405
Epoch 73/300, Loss: 0.0673 | 0.0403
Epoch 74/300, Loss: 0.0671 | 0.0402
Epoch 75/300, Loss: 0.0668 | 0.0401
Epoch 76/300, Loss: 0.0666 | 0.0399
Epoch 77/300, Loss: 0.0663 | 0.0397
Epoch 78/300, Loss: 0.0661 | 0.0396
Epoch 79/300, Loss: 0.0658 | 0.0395
Epoch 80/300, Loss: 0.0655 | 0.0394
Epoch 81/300, Loss: 0.0653 | 0.0392
Epoch 82/300, Loss: 0.0650 | 0.0391
Epoch 83/300, Loss: 0.0648 | 0.0390
Epoch 84/300, Loss: 0.0645 | 0.0389
Epoch 85/300, Loss: 0.0643 | 0.0387
Epoch 86/300, Loss: 0.0640 | 0.0386
Epoch 87/300, Loss: 0.0638 | 0.0385
Epoch 88/300, Loss: 0.0636 | 0.0384
Epoch 89/300, Loss: 0.0634 | 0.0382
Epoch 90/300, Loss: 0.0631 | 0.0381
Epoch 91/300, Loss: 0.0629 | 0.0380
Epoch 92/300, Loss: 0.0627 | 0.0378
Epoch 93/300, Loss: 0.0625 | 0.0376
Epoch 94/300, Loss: 0.0623 | 0.0375
Epoch 95/300, Loss: 0.0621 | 0.0374
Epoch 96/300, Loss: 0.0619 | 0.0373
Epoch 97/300, Loss: 0.0616 | 0.0371
Epoch 98/300, Loss: 0.0614 | 0.0371
Epoch 99/300, Loss: 0.0613 | 0.0370
Epoch 100/300, Loss: 0.0611 | 0.0369
Epoch 101/300, Loss: 0.0609 | 0.0367
Epoch 102/300, Loss: 0.0607 | 0.0366
Epoch 103/300, Loss: 0.0605 | 0.0366
Epoch 104/300, Loss: 0.0603 | 0.0365
Epoch 105/300, Loss: 0.0601 | 0.0364
Epoch 106/300, Loss: 0.0599 | 0.0363
Epoch 107/300, Loss: 0.0598 | 0.0363
Epoch 108/300, Loss: 0.0596 | 0.0362
Epoch 109/300, Loss: 0.0594 | 0.0361
Epoch 110/300, Loss: 0.0592 | 0.0361
Epoch 111/300, Loss: 0.0591 | 0.0361
Epoch 112/300, Loss: 0.0589 | 0.0361
Epoch 113/300, Loss: 0.0587 | 0.0360
Epoch 114/300, Loss: 0.0586 | 0.0360
Epoch 115/300, Loss: 0.0584 | 0.0360
Epoch 116/300, Loss: 0.0582 | 0.0360
Epoch 117/300, Loss: 0.0581 | 0.0360
Epoch 118/300, Loss: 0.0579 | 0.0360
Epoch 119/300, Loss: 0.0578 | 0.0360
Epoch 120/300, Loss: 0.0576 | 0.0360
Epoch 121/300, Loss: 0.0574 | 0.0359
Epoch 122/300, Loss: 0.0573 | 0.0359
Epoch 123/300, Loss: 0.0571 | 0.0360
Epoch 124/300, Loss: 0.0569 | 0.0360
Epoch 125/300, Loss: 0.0568 | 0.0359
Epoch 126/300, Loss: 0.0566 | 0.0359
Epoch 127/300, Loss: 0.0565 | 0.0359
Epoch 128/300, Loss: 0.0563 | 0.0359
Epoch 129/300, Loss: 0.0561 | 0.0359
Epoch 130/300, Loss: 0.0560 | 0.0359
Epoch 131/300, Loss: 0.0558 | 0.0359
Epoch 132/300, Loss: 0.0556 | 0.0359
Epoch 133/300, Loss: 0.0554 | 0.0358
Epoch 134/300, Loss: 0.0553 | 0.0358
Epoch 135/300, Loss: 0.0551 | 0.0357
Epoch 136/300, Loss: 0.0549 | 0.0357
Epoch 137/300, Loss: 0.0547 | 0.0357
Epoch 138/300, Loss: 0.0545 | 0.0357
Epoch 139/300, Loss: 0.0543 | 0.0356
Epoch 140/300, Loss: 0.0541 | 0.0356
Epoch 141/300, Loss: 0.0539 | 0.0356
Epoch 142/300, Loss: 0.0537 | 0.0356
Epoch 143/300, Loss: 0.0534 | 0.0355
Epoch 144/300, Loss: 0.0532 | 0.0355
Epoch 145/300, Loss: 0.0529 | 0.0355
Epoch 146/300, Loss: 0.0526 | 0.0354
Epoch 147/300, Loss: 0.0523 | 0.0354
Epoch 148/300, Loss: 0.0519 | 0.0354
Epoch 149/300, Loss: 0.0516 | 0.0353
Epoch 150/300, Loss: 0.0512 | 0.0353
Epoch 151/300, Loss: 0.0509 | 0.0353
Epoch 152/300, Loss: 0.0505 | 0.0352
Epoch 153/300, Loss: 0.0501 | 0.0352
Epoch 154/300, Loss: 0.0498 | 0.0352
Epoch 155/300, Loss: 0.0495 | 0.0352
Epoch 156/300, Loss: 0.0492 | 0.0352
Epoch 157/300, Loss: 0.0490 | 0.0352
Epoch 158/300, Loss: 0.0487 | 0.0352
Epoch 159/300, Loss: 0.0485 | 0.0351
Epoch 160/300, Loss: 0.0483 | 0.0350
Epoch 161/300, Loss: 0.0481 | 0.0350
Epoch 162/300, Loss: 0.0479 | 0.0351
Epoch 163/300, Loss: 0.0477 | 0.0354
Epoch 164/300, Loss: 0.0476 | 0.0358
Epoch 165/300, Loss: 0.0475 | 0.0365
Epoch 166/300, Loss: 0.0474 | 0.0373
Epoch 167/300, Loss: 0.0473 | 0.0381
Epoch 168/300, Loss: 0.0472 | 0.0389
Epoch 169/300, Loss: 0.0471 | 0.0400
Epoch 170/300, Loss: 0.0470 | 0.0409
Epoch 171/300, Loss: 0.0468 | 0.0415
Epoch 172/300, Loss: 0.0467 | 0.0418
Epoch 173/300, Loss: 0.0466 | 0.0418
Epoch 174/300, Loss: 0.0464 | 0.0412
Epoch 175/300, Loss: 0.0463 | 0.0402
Epoch 176/300, Loss: 0.0461 | 0.0389
Epoch 177/300, Loss: 0.0459 | 0.0375
Epoch 178/300, Loss: 0.0458 | 0.0363
Epoch 179/300, Loss: 0.0456 | 0.0354
Epoch 180/300, Loss: 0.0455 | 0.0347
Epoch 181/300, Loss: 0.0454 | 0.0343
Epoch 182/300, Loss: 0.0453 | 0.0339
Epoch 183/300, Loss: 0.0452 | 0.0338
Epoch 184/300, Loss: 0.0451 | 0.0336
Epoch 185/300, Loss: 0.0450 | 0.0337
Epoch 186/300, Loss: 0.0449 | 0.0335
Epoch 187/300, Loss: 0.0448 | 0.0339
Epoch 188/300, Loss: 0.0448 | 0.0334
Epoch 189/300, Loss: 0.0447 | 0.0344
Epoch 190/300, Loss: 0.0446 | 0.0337
Epoch 191/300, Loss: 0.0445 | 0.0344
Epoch 192/300, Loss: 0.0444 | 0.0338
Epoch 193/300, Loss: 0.0444 | 0.0346
Epoch 194/300, Loss: 0.0443 | 0.0340
Epoch 195/300, Loss: 0.0443 | 0.0347
Epoch 196/300, Loss: 0.0442 | 0.0341
Epoch 197/300, Loss: 0.0442 | 0.0350
Epoch 198/300, Loss: 0.0441 | 0.0345
Epoch 199/300, Loss: 0.0441 | 0.0353
Epoch 200/300, Loss: 0.0440 | 0.0347
Epoch 201/300, Loss: 0.0439 | 0.0358
Epoch 202/300, Loss: 0.0439 | 0.0353
Epoch 203/300, Loss: 0.0438 | 0.0362
Epoch 204/300, Loss: 0.0437 | 0.0355
Epoch 205/300, Loss: 0.0437 | 0.0366
Epoch 206/300, Loss: 0.0436 | 0.0358
Epoch 207/300, Loss: 0.0436 | 0.0364
Epoch 208/300, Loss: 0.0435 | 0.0354
Epoch 209/300, Loss: 0.0435 | 0.0358
Epoch 210/300, Loss: 0.0434 | 0.0348
Epoch 211/300, Loss: 0.0433 | 0.0351
Epoch 212/300, Loss: 0.0432 | 0.0341
Epoch 213/300, Loss: 0.0432 | 0.0343
Epoch 214/300, Loss: 0.0431 | 0.0335
Epoch 215/300, Loss: 0.0431 | 0.0338
Epoch 216/300, Loss: 0.0430 | 0.0331
Epoch 217/300, Loss: 0.0430 | 0.0335
Epoch 218/300, Loss: 0.0429 | 0.0330
Epoch 219/300, Loss: 0.0429 | 0.0334
Epoch 220/300, Loss: 0.0428 | 0.0329
Epoch 221/300, Loss: 0.0428 | 0.0334
Epoch 222/300, Loss: 0.0427 | 0.0330
Epoch 223/300, Loss: 0.0427 | 0.0335
Epoch 224/300, Loss: 0.0426 | 0.0330
Epoch 225/300, Loss: 0.0426 | 0.0335
Epoch 226/300, Loss: 0.0425 | 0.0331
Epoch 227/300, Loss: 0.0425 | 0.0336
Epoch 228/300, Loss: 0.0424 | 0.0331
Epoch 229/300, Loss: 0.0424 | 0.0337
Epoch 230/300, Loss: 0.0423 | 0.0332
Epoch 231/300, Loss: 0.0423 | 0.0337
Epoch 232/300, Loss: 0.0422 | 0.0332
Epoch 233/300, Loss: 0.0422 | 0.0337
Epoch 234/300, Loss: 0.0421 | 0.0332
Epoch 235/300, Loss: 0.0421 | 0.0338
Epoch 236/300, Loss: 0.0420 | 0.0333
Epoch 237/300, Loss: 0.0420 | 0.0338
Epoch 238/300, Loss: 0.0419 | 0.0333
Epoch 239/300, Loss: 0.0419 | 0.0339
Epoch 240/300, Loss: 0.0418 | 0.0334
Epoch 241/300, Loss: 0.0418 | 0.0340
Epoch 242/300, Loss: 0.0417 | 0.0335
Epoch 243/300, Loss: 0.0417 | 0.0341
Epoch 244/300, Loss: 0.0416 | 0.0336
Epoch 245/300, Loss: 0.0416 | 0.0342
Epoch 246/300, Loss: 0.0415 | 0.0338
Epoch 247/300, Loss: 0.0415 | 0.0344
Epoch 248/300, Loss: 0.0415 | 0.0339
Epoch 249/300, Loss: 0.0414 | 0.0345
Epoch 250/300, Loss: 0.0414 | 0.0340
Epoch 251/300, Loss: 0.0414 | 0.0345
Epoch 252/300, Loss: 0.0413 | 0.0340
Epoch 253/300, Loss: 0.0413 | 0.0345
Epoch 254/300, Loss: 0.0412 | 0.0340
Epoch 255/300, Loss: 0.0412 | 0.0344
Epoch 256/300, Loss: 0.0411 | 0.0338
Epoch 257/300, Loss: 0.0411 | 0.0342
Epoch 258/300, Loss: 0.0411 | 0.0336
Epoch 259/300, Loss: 0.0411 | 0.0340
Epoch 260/300, Loss: 0.0410 | 0.0334
Epoch 261/300, Loss: 0.0410 | 0.0337
Epoch 262/300, Loss: 0.0409 | 0.0332
Epoch 263/300, Loss: 0.0409 | 0.0335
Epoch 264/300, Loss: 0.0409 | 0.0331
Epoch 265/300, Loss: 0.0409 | 0.0334
Epoch 266/300, Loss: 0.0408 | 0.0330
Epoch 267/300, Loss: 0.0408 | 0.0333
Epoch 268/300, Loss: 0.0407 | 0.0329
Epoch 269/300, Loss: 0.0407 | 0.0332
Epoch 270/300, Loss: 0.0407 | 0.0329
Epoch 271/300, Loss: 0.0407 | 0.0332
Epoch 272/300, Loss: 0.0406 | 0.0329
Epoch 273/300, Loss: 0.0406 | 0.0332
Epoch 274/300, Loss: 0.0406 | 0.0329
Epoch 275/300, Loss: 0.0405 | 0.0332
Epoch 276/300, Loss: 0.0405 | 0.0329
Epoch 277/300, Loss: 0.0405 | 0.0332
Epoch 278/300, Loss: 0.0404 | 0.0329
Epoch 279/300, Loss: 0.0404 | 0.0332
Epoch 280/300, Loss: 0.0404 | 0.0329
Epoch 281/300, Loss: 0.0404 | 0.0331
Epoch 282/300, Loss: 0.0403 | 0.0329
Epoch 283/300, Loss: 0.0403 | 0.0331
Epoch 284/300, Loss: 0.0403 | 0.0329
Epoch 285/300, Loss: 0.0403 | 0.0331
Epoch 286/300, Loss: 0.0402 | 0.0328
Epoch 287/300, Loss: 0.0402 | 0.0331
Epoch 288/300, Loss: 0.0402 | 0.0329
Epoch 289/300, Loss: 0.0401 | 0.0331
Epoch 290/300, Loss: 0.0401 | 0.0328
Epoch 291/300, Loss: 0.0401 | 0.0331
Epoch 292/300, Loss: 0.0400 | 0.0328
Epoch 293/300, Loss: 0.0400 | 0.0330
Epoch 294/300, Loss: 0.0400 | 0.0328
Epoch 295/300, Loss: 0.0400 | 0.0330
Epoch 296/300, Loss: 0.0399 | 0.0328
Epoch 297/300, Loss: 0.0399 | 0.0330
Epoch 298/300, Loss: 0.0399 | 0.0327
Epoch 299/300, Loss: 0.0399 | 0.0329
Epoch 300/300, Loss: 0.0398 | 0.0327
Runtime (seconds): 620.4213044643402
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 205.58910965919495
RMSE: 14.33837890625
MAE: 14.33837890625
R-squared: nan
[183.23163]
