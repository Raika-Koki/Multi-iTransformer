ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-01 09:11:44,894][0m A new study created in memory with name: no-name-94ba1633-2970-4c75-bfd4-f3b10c88dbe9[0m
[32m[I 2025-02-01 09:14:31,375][0m Trial 0 finished with value: 0.20675250787347083 and parameters: {'observation_period_num': 102, 'train_rates': 0.9340681195988767, 'learning_rate': 0.0006004307779011519, 'batch_size': 33, 'step_size': 1, 'gamma': 0.8646586662645801}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:15:53,855][0m Trial 1 finished with value: 0.3348932280222001 and parameters: {'observation_period_num': 161, 'train_rates': 0.901124593100766, 'learning_rate': 6.5504638637208616e-06, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8636985964097419}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:17:14,679][0m Trial 2 finished with value: 0.30460267735752866 and parameters: {'observation_period_num': 211, 'train_rates': 0.6597958293453865, 'learning_rate': 1.2434727844788856e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8307636874592832}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:17:53,328][0m Trial 3 finished with value: 0.4053151731349562 and parameters: {'observation_period_num': 168, 'train_rates': 0.9215442595739656, 'learning_rate': 6.586954766374553e-06, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9618687663357005}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:18:40,589][0m Trial 4 finished with value: 0.21389692425727844 and parameters: {'observation_period_num': 38, 'train_rates': 0.7327723033123206, 'learning_rate': 5.196011859222709e-06, 'batch_size': 102, 'step_size': 13, 'gamma': 0.9648386941309177}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:19:06,918][0m Trial 5 finished with value: 0.6586091094768277 and parameters: {'observation_period_num': 225, 'train_rates': 0.6387534433745002, 'learning_rate': 2.745267259573619e-05, 'batch_size': 167, 'step_size': 1, 'gamma': 0.9179816543894398}. Best is trial 0 with value: 0.20675250787347083.[0m
[32m[I 2025-02-01 09:19:32,174][0m Trial 6 finished with value: 0.14456936767366196 and parameters: {'observation_period_num': 103, 'train_rates': 0.6056163600242936, 'learning_rate': 0.0003988666002220946, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8800656443767427}. Best is trial 6 with value: 0.14456936767366196.[0m
[32m[I 2025-02-01 09:20:20,404][0m Trial 7 finished with value: 0.3237462710210714 and parameters: {'observation_period_num': 66, 'train_rates': 0.7175062578008945, 'learning_rate': 2.5928134689557133e-06, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9532105606291938}. Best is trial 6 with value: 0.14456936767366196.[0m
[32m[I 2025-02-01 09:21:00,477][0m Trial 8 finished with value: 0.1740903976592291 and parameters: {'observation_period_num': 21, 'train_rates': 0.6579088701358116, 'learning_rate': 6.9425817265333405e-06, 'batch_size': 115, 'step_size': 7, 'gamma': 0.9331541998285307}. Best is trial 6 with value: 0.14456936767366196.[0m
[32m[I 2025-02-01 09:25:43,670][0m Trial 9 finished with value: 0.45880008048631926 and parameters: {'observation_period_num': 218, 'train_rates': 0.9719345544208801, 'learning_rate': 8.864892457060879e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.8155248037663456}. Best is trial 6 with value: 0.14456936767366196.[0m
[32m[I 2025-02-01 09:26:08,166][0m Trial 10 finished with value: 0.14359086751937866 and parameters: {'observation_period_num': 108, 'train_rates': 0.8384500815462304, 'learning_rate': 0.0006634004785815733, 'batch_size': 230, 'step_size': 4, 'gamma': 0.7601635436918259}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:26:31,452][0m Trial 11 finished with value: 0.14523108620004554 and parameters: {'observation_period_num': 105, 'train_rates': 0.8299957278510826, 'learning_rate': 0.0009020154111949468, 'batch_size': 238, 'step_size': 4, 'gamma': 0.7583171851065996}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:26:56,741][0m Trial 12 finished with value: 0.1439307033722902 and parameters: {'observation_period_num': 76, 'train_rates': 0.823484923513403, 'learning_rate': 0.00020379988828176252, 'batch_size': 219, 'step_size': 5, 'gamma': 0.7562445376340472}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:27:22,063][0m Trial 13 finished with value: 0.16815115048353416 and parameters: {'observation_period_num': 65, 'train_rates': 0.8265622153252451, 'learning_rate': 0.00015499093331788658, 'batch_size': 238, 'step_size': 4, 'gamma': 0.7536105349847704}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:27:50,677][0m Trial 14 finished with value: 0.20763417414129587 and parameters: {'observation_period_num': 142, 'train_rates': 0.8690424946383073, 'learning_rate': 0.00017308656120021667, 'batch_size': 203, 'step_size': 4, 'gamma': 0.7888738477566277}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:28:12,292][0m Trial 15 finished with value: 0.15888315230483982 and parameters: {'observation_period_num': 65, 'train_rates': 0.750804478318986, 'learning_rate': 5.434544552335402e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.7876499352870036}. Best is trial 10 with value: 0.14359086751937866.[0m
[32m[I 2025-02-01 09:28:39,756][0m Trial 16 finished with value: 0.09817053650658347 and parameters: {'observation_period_num': 7, 'train_rates': 0.7868604974809096, 'learning_rate': 0.00032748526148099427, 'batch_size': 205, 'step_size': 5, 'gamma': 0.7873178003512898}. Best is trial 16 with value: 0.09817053650658347.[0m
[32m[I 2025-02-01 09:29:07,253][0m Trial 17 finished with value: 0.09625358645436347 and parameters: {'observation_period_num': 5, 'train_rates': 0.769437882625767, 'learning_rate': 0.0003403579897011009, 'batch_size': 191, 'step_size': 5, 'gamma': 0.7942605433257061}. Best is trial 17 with value: 0.09625358645436347.[0m
[32m[I 2025-02-01 09:29:34,736][0m Trial 18 finished with value: 0.07866824533650067 and parameters: {'observation_period_num': 12, 'train_rates': 0.756474734318125, 'learning_rate': 0.00034902762167374066, 'batch_size': 196, 'step_size': 10, 'gamma': 0.8332540714797741}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:30:04,407][0m Trial 19 finished with value: 0.9585437289089722 and parameters: {'observation_period_num': 28, 'train_rates': 0.7714407769385391, 'learning_rate': 1.198627511565008e-06, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8352239639148873}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:30:41,627][0m Trial 20 finished with value: 0.09413300473351925 and parameters: {'observation_period_num': 34, 'train_rates': 0.7075961634204827, 'learning_rate': 7.378935903893223e-05, 'batch_size': 136, 'step_size': 12, 'gamma': 0.8881622909103624}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:31:19,430][0m Trial 21 finished with value: 0.1020228366936956 and parameters: {'observation_period_num': 43, 'train_rates': 0.6997063235013056, 'learning_rate': 6.554300676696277e-05, 'batch_size': 134, 'step_size': 11, 'gamma': 0.8929220079091948}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:31:54,461][0m Trial 22 finished with value: 0.08356001875230244 and parameters: {'observation_period_num': 20, 'train_rates': 0.7020166631908197, 'learning_rate': 0.000286292260289051, 'batch_size': 142, 'step_size': 15, 'gamma': 0.8439646581155966}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:32:29,710][0m Trial 23 finished with value: 0.12604784815341966 and parameters: {'observation_period_num': 42, 'train_rates': 0.6917315228431683, 'learning_rate': 2.7489134528479808e-05, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8399098907017041}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:33:00,315][0m Trial 24 finished with value: 0.10213867305544377 and parameters: {'observation_period_num': 21, 'train_rates': 0.6864342580184354, 'learning_rate': 0.0001225385705138305, 'batch_size': 160, 'step_size': 13, 'gamma': 0.9069299724073601}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:33:36,835][0m Trial 25 finished with value: 0.22471306395797835 and parameters: {'observation_period_num': 252, 'train_rates': 0.7361781287563529, 'learning_rate': 5.2052406284967104e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8518397096552798}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:34:27,029][0m Trial 26 finished with value: 0.10551513936353539 and parameters: {'observation_period_num': 54, 'train_rates': 0.6242476089900468, 'learning_rate': 0.0002477650766029844, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8115333296194578}. Best is trial 18 with value: 0.07866824533650067.[0m
[32m[I 2025-02-01 09:34:55,356][0m Trial 27 finished with value: 0.07710602946398502 and parameters: {'observation_period_num': 11, 'train_rates': 0.6784902648240697, 'learning_rate': 0.00010887188286742096, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9868453947136504}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:35:23,955][0m Trial 28 finished with value: 0.11178319301576384 and parameters: {'observation_period_num': 78, 'train_rates': 0.6768522957548956, 'learning_rate': 0.00012864500160799514, 'batch_size': 167, 'step_size': 9, 'gamma': 0.9806615901401597}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:35:51,255][0m Trial 29 finished with value: 0.1201754611754707 and parameters: {'observation_period_num': 86, 'train_rates': 0.7419725847603551, 'learning_rate': 0.0005310881472869518, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8615218651564712}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:36:18,630][0m Trial 30 finished with value: 0.08762079175981238 and parameters: {'observation_period_num': 6, 'train_rates': 0.8015827752493794, 'learning_rate': 0.0007674220714882769, 'batch_size': 213, 'step_size': 8, 'gamma': 0.8147202353142946}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:36:45,929][0m Trial 31 finished with value: 0.0875651236509005 and parameters: {'observation_period_num': 20, 'train_rates': 0.8116169662504205, 'learning_rate': 0.0007206367073296767, 'batch_size': 213, 'step_size': 8, 'gamma': 0.81624240432671}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:37:19,169][0m Trial 32 finished with value: 0.10449230724659103 and parameters: {'observation_period_num': 23, 'train_rates': 0.858123098969433, 'learning_rate': 0.0005097598660956032, 'batch_size': 177, 'step_size': 9, 'gamma': 0.853746887699075}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:37:54,662][0m Trial 33 finished with value: 0.10261962910061297 and parameters: {'observation_period_num': 50, 'train_rates': 0.802240942064814, 'learning_rate': 0.000256467670801132, 'batch_size': 154, 'step_size': 10, 'gamma': 0.870744195003448}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:38:22,123][0m Trial 34 finished with value: 0.08988994450123154 and parameters: {'observation_period_num': 19, 'train_rates': 0.7552399748656076, 'learning_rate': 0.0003650401269109807, 'batch_size': 197, 'step_size': 8, 'gamma': 0.8242653731227463}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:38:41,516][0m Trial 35 finished with value: 0.16292107367116537 and parameters: {'observation_period_num': 170, 'train_rates': 0.6570444318873808, 'learning_rate': 0.0009988108906933977, 'batch_size': 256, 'step_size': 6, 'gamma': 0.8004262659440128}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:39:08,083][0m Trial 36 finished with value: 0.2507847342097643 and parameters: {'observation_period_num': 132, 'train_rates': 0.8907904033720805, 'learning_rate': 3.6888540226171655e-05, 'batch_size': 222, 'step_size': 14, 'gamma': 0.8437651278731683}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:40:19,845][0m Trial 37 finished with value: 0.09331419461802261 and parameters: {'observation_period_num': 51, 'train_rates': 0.7235102456336275, 'learning_rate': 0.00010232457114146441, 'batch_size': 66, 'step_size': 10, 'gamma': 0.8251794997733588}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:40:51,042][0m Trial 38 finished with value: 0.16312897349034275 and parameters: {'observation_period_num': 35, 'train_rates': 0.7801344473759941, 'learning_rate': 1.5050801076396593e-05, 'batch_size': 172, 'step_size': 11, 'gamma': 0.8730743810297649}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:41:22,844][0m Trial 39 finished with value: 0.08192251555414663 and parameters: {'observation_period_num': 15, 'train_rates': 0.6329474515029181, 'learning_rate': 0.0004881983374238574, 'batch_size': 151, 'step_size': 13, 'gamma': 0.98955014863787}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:41:59,192][0m Trial 40 finished with value: 0.41776059134961485 and parameters: {'observation_period_num': 190, 'train_rates': 0.6056466623418141, 'learning_rate': 0.00025432942385014175, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9892414744350234}. Best is trial 27 with value: 0.07710602946398502.[0m
[32m[I 2025-02-01 09:42:30,593][0m Trial 41 finished with value: 0.0752622994660127 and parameters: {'observation_period_num': 15, 'train_rates': 0.6297898447441226, 'learning_rate': 0.00041669711109987496, 'batch_size': 154, 'step_size': 13, 'gamma': 0.9413629392862535}. Best is trial 41 with value: 0.0752622994660127.[0m
[32m[I 2025-02-01 09:43:03,872][0m Trial 42 finished with value: 0.06794213534852808 and parameters: {'observation_period_num': 5, 'train_rates': 0.6484904087455547, 'learning_rate': 0.0004899387299205314, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9435333891728429}. Best is trial 42 with value: 0.06794213534852808.[0m
[32m[I 2025-02-01 09:43:35,077][0m Trial 43 finished with value: 0.06689805871564201 and parameters: {'observation_period_num': 6, 'train_rates': 0.645038168787052, 'learning_rate': 0.00046105162863935317, 'batch_size': 151, 'step_size': 13, 'gamma': 0.9476918704328177}. Best is trial 43 with value: 0.06689805871564201.[0m
[32m[I 2025-02-01 09:44:15,338][0m Trial 44 finished with value: 0.06643738944253644 and parameters: {'observation_period_num': 9, 'train_rates': 0.6657248148427776, 'learning_rate': 0.0006401750543565856, 'batch_size': 122, 'step_size': 12, 'gamma': 0.9429481356791214}. Best is trial 44 with value: 0.06643738944253644.[0m
[32m[I 2025-02-01 09:44:52,771][0m Trial 45 finished with value: 0.09246933613585136 and parameters: {'observation_period_num': 34, 'train_rates': 0.6651506265668495, 'learning_rate': 0.0004875454961126555, 'batch_size': 125, 'step_size': 12, 'gamma': 0.9409496212889766}. Best is trial 44 with value: 0.06643738944253644.[0m
[32m[I 2025-02-01 09:45:44,796][0m Trial 46 finished with value: 0.08448451474995393 and parameters: {'observation_period_num': 31, 'train_rates': 0.6437697774227539, 'learning_rate': 0.00019725135181339587, 'batch_size': 86, 'step_size': 13, 'gamma': 0.9557767862684239}. Best is trial 44 with value: 0.06643738944253644.[0m
[32m[I 2025-02-01 09:46:26,036][0m Trial 47 finished with value: 0.11796726889553524 and parameters: {'observation_period_num': 54, 'train_rates': 0.6164156625795166, 'learning_rate': 0.0007186008268812797, 'batch_size': 106, 'step_size': 14, 'gamma': 0.926641109915612}. Best is trial 44 with value: 0.06643738944253644.[0m
[32m[I 2025-02-01 09:46:57,335][0m Trial 48 finished with value: 0.1820553029254151 and parameters: {'observation_period_num': 93, 'train_rates': 0.6399678344893464, 'learning_rate': 0.0005988170617714237, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9714527470018457}. Best is trial 44 with value: 0.06643738944253644.[0m
[32m[I 2025-02-01 09:47:24,813][0m Trial 49 finished with value: 0.07447782348273164 and parameters: {'observation_period_num': 5, 'train_rates': 0.6002809670486065, 'learning_rate': 0.0004213922342461783, 'batch_size': 161, 'step_size': 13, 'gamma': 0.9461997931257131}. Best is trial 44 with value: 0.06643738944253644.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-01 09:47:24,825][0m A new study created in memory with name: no-name-2455c495-8357-4032-ad69-198aff75f7e1[0m
[32m[I 2025-02-01 09:49:07,170][0m Trial 0 finished with value: 0.6210505503172777 and parameters: {'observation_period_num': 170, 'train_rates': 0.8947699099931816, 'learning_rate': 1.3474406471554434e-06, 'batch_size': 51, 'step_size': 8, 'gamma': 0.8988424773969611}. Best is trial 0 with value: 0.6210505503172777.[0m
[32m[I 2025-02-01 09:49:35,656][0m Trial 1 finished with value: 0.6755653454313156 and parameters: {'observation_period_num': 158, 'train_rates': 0.6923213739441925, 'learning_rate': 1.5899793466659498e-05, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8392595292800372}. Best is trial 0 with value: 0.6210505503172777.[0m
[32m[I 2025-02-01 09:50:04,129][0m Trial 2 finished with value: 0.3464057841407719 and parameters: {'observation_period_num': 110, 'train_rates': 0.9058083567642615, 'learning_rate': 1.931089189564993e-05, 'batch_size': 212, 'step_size': 8, 'gamma': 0.8467313548538647}. Best is trial 2 with value: 0.3464057841407719.[0m
[32m[I 2025-02-01 09:50:48,093][0m Trial 3 finished with value: 0.8637623399926079 and parameters: {'observation_period_num': 215, 'train_rates': 0.6712423857687086, 'learning_rate': 1.119675458338651e-06, 'batch_size': 101, 'step_size': 3, 'gamma': 0.946427872850608}. Best is trial 2 with value: 0.3464057841407719.[0m
[32m[I 2025-02-01 09:51:21,560][0m Trial 4 finished with value: 0.12683240624637943 and parameters: {'observation_period_num': 86, 'train_rates': 0.6474844204176954, 'learning_rate': 7.826741055475274e-05, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8423121623373429}. Best is trial 4 with value: 0.12683240624637943.[0m
Early stopping at epoch 89
[32m[I 2025-02-01 09:51:39,471][0m Trial 5 finished with value: 0.7051236068760908 and parameters: {'observation_period_num': 229, 'train_rates': 0.7030553364141279, 'learning_rate': 3.3496735304845446e-05, 'batch_size': 254, 'step_size': 2, 'gamma': 0.7613039826609592}. Best is trial 4 with value: 0.12683240624637943.[0m
[32m[I 2025-02-01 09:52:04,889][0m Trial 6 finished with value: 0.1182571325286482 and parameters: {'observation_period_num': 83, 'train_rates': 0.7482031719732405, 'learning_rate': 0.0003297794607416504, 'batch_size': 207, 'step_size': 7, 'gamma': 0.8008189945476402}. Best is trial 6 with value: 0.1182571325286482.[0m
[32m[I 2025-02-01 09:52:49,199][0m Trial 7 finished with value: 0.2986415574947993 and parameters: {'observation_period_num': 102, 'train_rates': 0.9453557031532602, 'learning_rate': 2.259702734573998e-05, 'batch_size': 134, 'step_size': 7, 'gamma': 0.9803866001844198}. Best is trial 6 with value: 0.1182571325286482.[0m
[32m[I 2025-02-01 09:55:24,474][0m Trial 8 finished with value: 0.21053498847570284 and parameters: {'observation_period_num': 51, 'train_rates': 0.7318313153192212, 'learning_rate': 1.4299716451473587e-05, 'batch_size': 30, 'step_size': 3, 'gamma': 0.7965804617060445}. Best is trial 6 with value: 0.1182571325286482.[0m
[32m[I 2025-02-01 09:55:50,012][0m Trial 9 finished with value: 1.4496688803900843 and parameters: {'observation_period_num': 149, 'train_rates': 0.8362514903241496, 'learning_rate': 1.0329166554295292e-06, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9185623144776722}. Best is trial 6 with value: 0.1182571325286482.[0m
[32m[I 2025-02-01 09:56:19,719][0m Trial 10 finished with value: 0.08005399553555448 and parameters: {'observation_period_num': 8, 'train_rates': 0.7854933642587503, 'learning_rate': 0.0009289772628600405, 'batch_size': 186, 'step_size': 12, 'gamma': 0.7927104755633362}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 09:56:49,608][0m Trial 11 finished with value: 0.08405206942168182 and parameters: {'observation_period_num': 7, 'train_rates': 0.7772828928733774, 'learning_rate': 0.0009136675399001155, 'batch_size': 185, 'step_size': 12, 'gamma': 0.7856500560360486}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 09:57:20,928][0m Trial 12 finished with value: 0.0826344212478275 and parameters: {'observation_period_num': 14, 'train_rates': 0.7882474233046874, 'learning_rate': 0.0007239335121048373, 'batch_size': 171, 'step_size': 13, 'gamma': 0.7552738375484087}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 09:57:56,510][0m Trial 13 finished with value: 0.08983809873461723 and parameters: {'observation_period_num': 9, 'train_rates': 0.8275904152213572, 'learning_rate': 0.000997904963581178, 'batch_size': 159, 'step_size': 12, 'gamma': 0.759751742014148}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 09:58:59,692][0m Trial 14 finished with value: 0.10241781888304981 and parameters: {'observation_period_num': 40, 'train_rates': 0.8237107435046657, 'learning_rate': 0.00023447043219731824, 'batch_size': 85, 'step_size': 12, 'gamma': 0.7513395089330176}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 09:59:18,203][0m Trial 15 finished with value: 0.12279991262642913 and parameters: {'observation_period_num': 47, 'train_rates': 0.60091167274743, 'learning_rate': 0.000299272966378303, 'batch_size': 255, 'step_size': 15, 'gamma': 0.814851295571699}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 10:00:09,272][0m Trial 16 finished with value: 0.09327702971685464 and parameters: {'observation_period_num': 30, 'train_rates': 0.7855675190609785, 'learning_rate': 0.00010788561356319889, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8872508897654798}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 10:00:46,351][0m Trial 17 finished with value: 0.5305698826735796 and parameters: {'observation_period_num': 71, 'train_rates': 0.8808802647115352, 'learning_rate': 4.593322800809014e-06, 'batch_size': 155, 'step_size': 10, 'gamma': 0.7798525981041479}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 10:01:12,087][0m Trial 18 finished with value: 0.17191395657016084 and parameters: {'observation_period_num': 195, 'train_rates': 0.7489992786902451, 'learning_rate': 0.0005333201353128087, 'batch_size': 193, 'step_size': 13, 'gamma': 0.8284917840886243}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 10:01:37,079][0m Trial 19 finished with value: 0.16615870307282646 and parameters: {'observation_period_num': 130, 'train_rates': 0.8571024968134029, 'learning_rate': 0.00012914092537579197, 'batch_size': 228, 'step_size': 10, 'gamma': 0.8661546686422246}. Best is trial 10 with value: 0.08005399553555448.[0m
[32m[I 2025-02-01 10:02:30,451][0m Trial 20 finished with value: 0.0599784255027771 and parameters: {'observation_period_num': 23, 'train_rates': 0.9821560972648063, 'learning_rate': 4.852481876004693e-05, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7720345609988948}. Best is trial 20 with value: 0.0599784255027771.[0m
[32m[I 2025-02-01 10:03:20,890][0m Trial 21 finished with value: 0.2954765856266022 and parameters: {'observation_period_num': 25, 'train_rates': 0.9655572887419396, 'learning_rate': 6.337719014048178e-05, 'batch_size': 122, 'step_size': 14, 'gamma': 0.7778824225455583}. Best is trial 20 with value: 0.0599784255027771.[0m
[32m[I 2025-02-01 10:03:54,406][0m Trial 22 finished with value: 0.3552850462707733 and parameters: {'observation_period_num': 60, 'train_rates': 0.7933476380314092, 'learning_rate': 6.858704685464826e-06, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8074664388193685}. Best is trial 20 with value: 0.0599784255027771.[0m
[32m[I 2025-02-01 10:05:05,402][0m Trial 23 finished with value: 0.18334572356025777 and parameters: {'observation_period_num': 6, 'train_rates': 0.9327598608025263, 'learning_rate': 0.000531719182876955, 'batch_size': 82, 'step_size': 11, 'gamma': 0.7724566186222503}. Best is trial 20 with value: 0.0599784255027771.[0m
[32m[I 2025-02-01 10:05:41,681][0m Trial 24 finished with value: 0.04628200829029083 and parameters: {'observation_period_num': 29, 'train_rates': 0.9831082480153257, 'learning_rate': 0.00019918870614241732, 'batch_size': 178, 'step_size': 14, 'gamma': 0.7536434606518703}. Best is trial 24 with value: 0.04628200829029083.[0m
[32m[I 2025-02-01 10:06:35,989][0m Trial 25 finished with value: 0.041092969477176666 and parameters: {'observation_period_num': 33, 'train_rates': 0.9822162804600922, 'learning_rate': 0.00016220051805170655, 'batch_size': 111, 'step_size': 15, 'gamma': 0.8236270320122207}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:08:10,209][0m Trial 26 finished with value: 0.10297110676765442 and parameters: {'observation_period_num': 76, 'train_rates': 0.989749581780564, 'learning_rate': 4.1438686961432345e-05, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8203402982188843}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:09:04,571][0m Trial 27 finished with value: 0.05017049238085747 and parameters: {'observation_period_num': 33, 'train_rates': 0.9748094271706989, 'learning_rate': 0.00017291273803922818, 'batch_size': 113, 'step_size': 14, 'gamma': 0.8554251655060437}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:10:03,878][0m Trial 28 finished with value: 0.21874129079544363 and parameters: {'observation_period_num': 101, 'train_rates': 0.9302866599349153, 'learning_rate': 0.0002240085937119172, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8684028471885703}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:11:25,223][0m Trial 29 finished with value: 0.15668153454229902 and parameters: {'observation_period_num': 63, 'train_rates': 0.9075153916143376, 'learning_rate': 0.00015163593873327607, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9064117089876527}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:14:04,612][0m Trial 30 finished with value: 0.23865693960996234 and parameters: {'observation_period_num': 37, 'train_rates': 0.9592815623286699, 'learning_rate': 0.00017152590086376784, 'batch_size': 36, 'step_size': 9, 'gamma': 0.8931268335963916}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:14:56,548][0m Trial 31 finished with value: 0.06469515711069107 and parameters: {'observation_period_num': 23, 'train_rates': 0.9858489593766006, 'learning_rate': 5.9089798061196815e-05, 'batch_size': 117, 'step_size': 14, 'gamma': 0.8567507102810696}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:15:46,947][0m Trial 32 finished with value: 0.31663385033607483 and parameters: {'observation_period_num': 52, 'train_rates': 0.9665973449927796, 'learning_rate': 8.95574987607267e-05, 'batch_size': 121, 'step_size': 14, 'gamma': 0.822639199269493}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:16:27,722][0m Trial 33 finished with value: 0.16819823794920025 and parameters: {'observation_period_num': 29, 'train_rates': 0.9106025966109249, 'learning_rate': 4.824413347435331e-05, 'batch_size': 145, 'step_size': 13, 'gamma': 0.8365996028968101}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:17:24,086][0m Trial 34 finished with value: 0.04753341153264046 and parameters: {'observation_period_num': 41, 'train_rates': 0.986157713898318, 'learning_rate': 0.00042019339400887855, 'batch_size': 109, 'step_size': 11, 'gamma': 0.7730218841258675}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:18:26,424][0m Trial 35 finished with value: 0.18907248990589312 and parameters: {'observation_period_num': 126, 'train_rates': 0.8744871967873193, 'learning_rate': 0.0004285315859142599, 'batch_size': 88, 'step_size': 11, 'gamma': 0.8778776439768738}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:19:19,090][0m Trial 36 finished with value: 0.2577650840418326 and parameters: {'observation_period_num': 184, 'train_rates': 0.930388451735831, 'learning_rate': 0.0002021276021449051, 'batch_size': 105, 'step_size': 11, 'gamma': 0.852744400147107}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:20:01,375][0m Trial 37 finished with value: 0.2671322524547577 and parameters: {'observation_period_num': 91, 'train_rates': 0.955818241595655, 'learning_rate': 0.0003157668475321352, 'batch_size': 139, 'step_size': 13, 'gamma': 0.8087665210108671}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:21:25,020][0m Trial 38 finished with value: 0.16942222726575296 and parameters: {'observation_period_num': 59, 'train_rates': 0.9198891552599391, 'learning_rate': 0.00010583737964082358, 'batch_size': 67, 'step_size': 15, 'gamma': 0.8332688977587754}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:23:27,860][0m Trial 39 finished with value: 0.1548613620721377 and parameters: {'observation_period_num': 42, 'train_rates': 0.8885851235641375, 'learning_rate': 0.0004323157277645192, 'batch_size': 44, 'step_size': 8, 'gamma': 0.9249507976672164}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:24:10,869][0m Trial 40 finished with value: 0.26498062357510604 and parameters: {'observation_period_num': 238, 'train_rates': 0.9463195574750848, 'learning_rate': 0.00026328364909187113, 'batch_size': 132, 'step_size': 9, 'gamma': 0.7674524899612183}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:25:05,894][0m Trial 41 finished with value: 0.08294007927179337 and parameters: {'observation_period_num': 19, 'train_rates': 0.9778957425399054, 'learning_rate': 2.378963378139394e-05, 'batch_size': 112, 'step_size': 14, 'gamma': 0.7889937441286378}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:25:54,643][0m Trial 42 finished with value: 0.17461983859539032 and parameters: {'observation_period_num': 31, 'train_rates': 0.9771788896961568, 'learning_rate': 1.1711797851532373e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.767606578112664}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:26:36,822][0m Trial 43 finished with value: 0.42064720392227173 and parameters: {'observation_period_num': 71, 'train_rates': 0.9899118189260302, 'learning_rate': 2.4177362339274013e-06, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8027372947572718}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:27:13,183][0m Trial 44 finished with value: 0.2759799063205719 and parameters: {'observation_period_num': 44, 'train_rates': 0.9466438094351441, 'learning_rate': 7.816979267343346e-05, 'batch_size': 170, 'step_size': 6, 'gamma': 0.7507825069191253}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:28:09,193][0m Trial 45 finished with value: 0.30082178115844727 and parameters: {'observation_period_num': 19, 'train_rates': 0.9693632722361543, 'learning_rate': 0.00014645329232366137, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8464107731391279}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:29:13,567][0m Trial 46 finished with value: 0.2521777721883366 and parameters: {'observation_period_num': 54, 'train_rates': 0.9453701925330801, 'learning_rate': 3.080479429465423e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.7931412351759997}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:30:26,907][0m Trial 47 finished with value: 0.16010085400836221 and parameters: {'observation_period_num': 36, 'train_rates': 0.9009658640552737, 'learning_rate': 0.0006363975820575842, 'batch_size': 76, 'step_size': 15, 'gamma': 0.9820143795798666}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:35:24,540][0m Trial 48 finished with value: 0.3481278268897787 and parameters: {'observation_period_num': 84, 'train_rates': 0.9675742188157839, 'learning_rate': 0.00043090880891393765, 'batch_size': 19, 'step_size': 4, 'gamma': 0.7625919159379017}. Best is trial 25 with value: 0.041092969477176666.[0m
[32m[I 2025-02-01 10:36:26,030][0m Trial 49 finished with value: 0.1904038896012788 and parameters: {'observation_period_num': 17, 'train_rates': 0.9328958942862758, 'learning_rate': 0.00019220226539577866, 'batch_size': 97, 'step_size': 12, 'gamma': 0.9605417677496039}. Best is trial 25 with value: 0.041092969477176666.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-01 10:36:26,040][0m A new study created in memory with name: no-name-0c02c59b-6cb9-4198-90c0-38b422d7b3a2[0m
[32m[I 2025-02-01 10:36:52,001][0m Trial 0 finished with value: 0.12816995067920298 and parameters: {'observation_period_num': 16, 'train_rates': 0.7058121386913814, 'learning_rate': 1.4067662152352638e-05, 'batch_size': 200, 'step_size': 14, 'gamma': 0.8574128596628344}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:37:28,354][0m Trial 1 finished with value: 0.16694582896988566 and parameters: {'observation_period_num': 31, 'train_rates': 0.9302263786896845, 'learning_rate': 0.0002722510570673029, 'batch_size': 163, 'step_size': 11, 'gamma': 0.8718962660515344}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:38:00,672][0m Trial 2 finished with value: 0.5055772581800029 and parameters: {'observation_period_num': 184, 'train_rates': 0.6054295540661685, 'learning_rate': 3.5916729417006553e-06, 'batch_size': 135, 'step_size': 9, 'gamma': 0.9464076117483232}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:41:29,263][0m Trial 3 finished with value: 0.15983053045997092 and parameters: {'observation_period_num': 128, 'train_rates': 0.6959067060646912, 'learning_rate': 6.7438135754228155e-06, 'batch_size': 21, 'step_size': 9, 'gamma': 0.9474065075951816}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:46:06,591][0m Trial 4 finished with value: 0.1487352898453965 and parameters: {'observation_period_num': 166, 'train_rates': 0.8540433423452652, 'learning_rate': 4.482334776599428e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.8853993616111441}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:47:57,136][0m Trial 5 finished with value: 0.3492945179248312 and parameters: {'observation_period_num': 157, 'train_rates': 0.7756792687972112, 'learning_rate': 1.7448360393643152e-06, 'batch_size': 43, 'step_size': 9, 'gamma': 0.96342148774809}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:49:20,605][0m Trial 6 finished with value: 0.3210116325206654 and parameters: {'observation_period_num': 185, 'train_rates': 0.9330779095441474, 'learning_rate': 8.452937790589226e-06, 'batch_size': 65, 'step_size': 12, 'gamma': 0.9654347230530107}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:49:57,920][0m Trial 7 finished with value: 0.5302057730742787 and parameters: {'observation_period_num': 196, 'train_rates': 0.672681886516884, 'learning_rate': 3.902322120220652e-06, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9748349400683761}. Best is trial 0 with value: 0.12816995067920298.[0m
[32m[I 2025-02-01 10:50:58,708][0m Trial 8 finished with value: 0.12721122179050032 and parameters: {'observation_period_num': 106, 'train_rates': 0.6190179429359091, 'learning_rate': 0.00031616492463948793, 'batch_size': 70, 'step_size': 13, 'gamma': 0.8968067839596847}. Best is trial 8 with value: 0.12721122179050032.[0m
[32m[I 2025-02-01 10:52:13,844][0m Trial 9 finished with value: 0.17781370379326103 and parameters: {'observation_period_num': 210, 'train_rates': 0.834916866243072, 'learning_rate': 0.00040285207283342065, 'batch_size': 67, 'step_size': 13, 'gamma': 0.7564963104767211}. Best is trial 8 with value: 0.12721122179050032.[0m
[32m[I 2025-02-01 10:52:57,237][0m Trial 10 finished with value: 0.1590885765435525 and parameters: {'observation_period_num': 90, 'train_rates': 0.6055413233311228, 'learning_rate': 0.00010257278473303464, 'batch_size': 99, 'step_size': 4, 'gamma': 0.8111395951882885}. Best is trial 8 with value: 0.12721122179050032.[0m
[32m[I 2025-02-01 10:53:18,541][0m Trial 11 finished with value: 0.12486705836895919 and parameters: {'observation_period_num': 13, 'train_rates': 0.7114487313716946, 'learning_rate': 2.1821451515986183e-05, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9017853342399369}. Best is trial 11 with value: 0.12486705836895919.[0m
[32m[I 2025-02-01 10:53:42,071][0m Trial 12 finished with value: 0.11679063370546917 and parameters: {'observation_period_num': 67, 'train_rates': 0.7506011701597938, 'learning_rate': 0.0009975800107523331, 'batch_size': 236, 'step_size': 15, 'gamma': 0.9083334556986083}. Best is trial 12 with value: 0.11679063370546917.[0m
[32m[I 2025-02-01 10:54:03,523][0m Trial 13 finished with value: 0.1339014131720131 and parameters: {'observation_period_num': 60, 'train_rates': 0.7593058177893475, 'learning_rate': 4.029375486100148e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.919824602429909}. Best is trial 12 with value: 0.11679063370546917.[0m
[32m[I 2025-02-01 10:54:24,795][0m Trial 14 finished with value: 0.11198333322801846 and parameters: {'observation_period_num': 59, 'train_rates': 0.7428608964356193, 'learning_rate': 9.02203567585119e-05, 'batch_size': 253, 'step_size': 15, 'gamma': 0.8338459547639088}. Best is trial 14 with value: 0.11198333322801846.[0m
Early stopping at epoch 77
[32m[I 2025-02-01 10:54:44,874][0m Trial 15 finished with value: 0.3042871881662878 and parameters: {'observation_period_num': 247, 'train_rates': 0.8328500836991161, 'learning_rate': 0.0006713594419479352, 'batch_size': 217, 'step_size': 1, 'gamma': 0.8383605335167125}. Best is trial 14 with value: 0.11198333322801846.[0m
[32m[I 2025-02-01 10:55:14,955][0m Trial 16 finished with value: 0.1295580314855053 and parameters: {'observation_period_num': 62, 'train_rates': 0.74770453614504, 'learning_rate': 0.00012890044747931447, 'batch_size': 180, 'step_size': 6, 'gamma': 0.8019280144468275}. Best is trial 14 with value: 0.11198333322801846.[0m
[32m[I 2025-02-01 10:55:41,339][0m Trial 17 finished with value: 0.1348128316501167 and parameters: {'observation_period_num': 58, 'train_rates': 0.8876726299133549, 'learning_rate': 0.0008740532926540753, 'batch_size': 228, 'step_size': 15, 'gamma': 0.8236647020648242}. Best is trial 14 with value: 0.11198333322801846.[0m
[32m[I 2025-02-01 10:56:10,704][0m Trial 18 finished with value: 0.13105154686946233 and parameters: {'observation_period_num': 87, 'train_rates': 0.8018886868390291, 'learning_rate': 8.463863035191381e-05, 'batch_size': 187, 'step_size': 11, 'gamma': 0.7584444669296143}. Best is trial 14 with value: 0.11198333322801846.[0m
[32m[I 2025-02-01 10:56:42,754][0m Trial 19 finished with value: 0.10489960660993974 and parameters: {'observation_period_num': 43, 'train_rates': 0.7389319934458617, 'learning_rate': 0.00016740788528833378, 'batch_size': 162, 'step_size': 7, 'gamma': 0.78647860357029}. Best is trial 19 with value: 0.10489960660993974.[0m
[32m[I 2025-02-01 10:57:13,814][0m Trial 20 finished with value: 0.09500118472943163 and parameters: {'observation_period_num': 40, 'train_rates': 0.6474560402812222, 'learning_rate': 0.0001940312270078571, 'batch_size': 147, 'step_size': 7, 'gamma': 0.7820545841608368}. Best is trial 20 with value: 0.09500118472943163.[0m
[32m[I 2025-02-01 10:57:43,023][0m Trial 21 finished with value: 0.10297158287618441 and parameters: {'observation_period_num': 40, 'train_rates': 0.6422830719776454, 'learning_rate': 0.00018722273818722694, 'batch_size': 156, 'step_size': 7, 'gamma': 0.7836416883194025}. Best is trial 20 with value: 0.09500118472943163.[0m
[32m[I 2025-02-01 10:58:13,984][0m Trial 22 finished with value: 0.09353964174465265 and parameters: {'observation_period_num': 35, 'train_rates': 0.6671269560937644, 'learning_rate': 0.00019053683178490272, 'batch_size': 157, 'step_size': 7, 'gamma': 0.7890612264558601}. Best is trial 22 with value: 0.09353964174465265.[0m
[32m[I 2025-02-01 10:58:45,218][0m Trial 23 finished with value: 0.08912233861767213 and parameters: {'observation_period_num': 6, 'train_rates': 0.651912682858413, 'learning_rate': 0.00021136461300468227, 'batch_size': 155, 'step_size': 5, 'gamma': 0.7796686496765313}. Best is trial 23 with value: 0.08912233861767213.[0m
[32m[I 2025-02-01 10:59:25,660][0m Trial 24 finished with value: 0.07102459669837924 and parameters: {'observation_period_num': 6, 'train_rates': 0.6516374244998486, 'learning_rate': 0.0004796668825073218, 'batch_size': 113, 'step_size': 5, 'gamma': 0.7809345723473412}. Best is trial 24 with value: 0.07102459669837924.[0m
[32m[I 2025-02-01 11:00:09,853][0m Trial 25 finished with value: 0.07903447598853686 and parameters: {'observation_period_num': 15, 'train_rates': 0.6806514273916143, 'learning_rate': 0.0004936036452144873, 'batch_size': 109, 'step_size': 4, 'gamma': 0.7699275094377348}. Best is trial 24 with value: 0.07102459669837924.[0m
[32m[I 2025-02-01 11:00:54,734][0m Trial 26 finished with value: 0.0737082474635258 and parameters: {'observation_period_num': 6, 'train_rates': 0.6399122399110887, 'learning_rate': 0.0005607429998709491, 'batch_size': 103, 'step_size': 4, 'gamma': 0.7518031766971978}. Best is trial 24 with value: 0.07102459669837924.[0m
Early stopping at epoch 83
[32m[I 2025-02-01 11:01:45,393][0m Trial 27 finished with value: 0.06162525713443756 and parameters: {'observation_period_num': 5, 'train_rates': 0.9847700050094805, 'learning_rate': 0.0004352743783481954, 'batch_size': 102, 'step_size': 2, 'gamma': 0.7616471009223855}. Best is trial 27 with value: 0.06162525713443756.[0m
Early stopping at epoch 47
[32m[I 2025-02-01 11:02:16,764][0m Trial 28 finished with value: 0.07444941252470016 and parameters: {'observation_period_num': 6, 'train_rates': 0.9891566625449723, 'learning_rate': 0.0005292182843479754, 'batch_size': 95, 'step_size': 1, 'gamma': 0.7565488408542201}. Best is trial 27 with value: 0.06162525713443756.[0m
[32m[I 2025-02-01 11:03:13,116][0m Trial 29 finished with value: 0.1359611523451565 and parameters: {'observation_period_num': 26, 'train_rates': 0.715568969162224, 'learning_rate': 5.790443163854421e-05, 'batch_size': 85, 'step_size': 2, 'gamma': 0.8569452460883089}. Best is trial 27 with value: 0.06162525713443756.[0m
[32m[I 2025-02-01 11:03:59,576][0m Trial 30 finished with value: 0.25525410240516067 and parameters: {'observation_period_num': 79, 'train_rates': 0.950031733235883, 'learning_rate': 0.00035905247067066206, 'batch_size': 129, 'step_size': 3, 'gamma': 0.8049820817691368}. Best is trial 27 with value: 0.06162525713443756.[0m
Early stopping at epoch 45
[32m[I 2025-02-01 11:04:31,753][0m Trial 31 finished with value: 0.06974199414253235 and parameters: {'observation_period_num': 10, 'train_rates': 0.9892124340390896, 'learning_rate': 0.0005771514397943658, 'batch_size': 90, 'step_size': 1, 'gamma': 0.751610780436464}. Best is trial 27 with value: 0.06162525713443756.[0m
Early stopping at epoch 91
[32m[I 2025-02-01 11:05:41,134][0m Trial 32 finished with value: 0.05036494508385658 and parameters: {'observation_period_num': 25, 'train_rates': 0.9787485485345683, 'learning_rate': 0.000560388626745131, 'batch_size': 81, 'step_size': 2, 'gamma': 0.7517895928958118}. Best is trial 32 with value: 0.05036494508385658.[0m
Early stopping at epoch 85
[32m[I 2025-02-01 11:07:27,841][0m Trial 33 finished with value: 0.05138711258769035 and parameters: {'observation_period_num': 27, 'train_rates': 0.9882058993947358, 'learning_rate': 0.0002988263248971846, 'batch_size': 48, 'step_size': 2, 'gamma': 0.7702832941513885}. Best is trial 32 with value: 0.05036494508385658.[0m
Early stopping at epoch 94
[32m[I 2025-02-01 11:09:16,201][0m Trial 34 finished with value: 0.05009311065077782 and parameters: {'observation_period_num': 26, 'train_rates': 0.9864913308507631, 'learning_rate': 0.00029617779690660934, 'batch_size': 52, 'step_size': 2, 'gamma': 0.7653403908163988}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:11:25,540][0m Trial 35 finished with value: 0.25192267992175543 and parameters: {'observation_period_num': 23, 'train_rates': 0.9601002051423102, 'learning_rate': 0.0003064561751327629, 'batch_size': 45, 'step_size': 2, 'gamma': 0.7991307675562203}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:13:38,142][0m Trial 36 finished with value: 0.34821472613961546 and parameters: {'observation_period_num': 129, 'train_rates': 0.898884998754493, 'learning_rate': 2.5121385728794482e-05, 'batch_size': 40, 'step_size': 2, 'gamma': 0.7680577544923195}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:15:31,803][0m Trial 37 finished with value: 0.30209976476564837 and parameters: {'observation_period_num': 46, 'train_rates': 0.9696115131788656, 'learning_rate': 0.0007864483598839902, 'batch_size': 51, 'step_size': 3, 'gamma': 0.7694496877683887}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:18:57,751][0m Trial 38 finished with value: 0.14613756626063296 and parameters: {'observation_period_num': 31, 'train_rates': 0.9152330810682876, 'learning_rate': 0.0002616637663911857, 'batch_size': 27, 'step_size': 3, 'gamma': 0.8175897082125623}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:20:07,756][0m Trial 39 finished with value: 0.2541403778289494 and parameters: {'observation_period_num': 126, 'train_rates': 0.9328462795512578, 'learning_rate': 0.00013572992718728015, 'batch_size': 80, 'step_size': 2, 'gamma': 0.858627980215441}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:21:44,191][0m Trial 40 finished with value: 1.0647427472439441 and parameters: {'observation_period_num': 24, 'train_rates': 0.9692656155180698, 'learning_rate': 1.0144073289392358e-06, 'batch_size': 61, 'step_size': 5, 'gamma': 0.7702708805150511}. Best is trial 34 with value: 0.05009311065077782.[0m
Early stopping at epoch 47
[32m[I 2025-02-01 11:22:19,607][0m Trial 41 finished with value: 0.07489907741546631 and parameters: {'observation_period_num': 49, 'train_rates': 0.9895430656235372, 'learning_rate': 0.0006872961341071884, 'batch_size': 83, 'step_size': 1, 'gamma': 0.7500390152698011}. Best is trial 34 with value: 0.05009311065077782.[0m
Early stopping at epoch 53
[32m[I 2025-02-01 11:23:01,735][0m Trial 42 finished with value: 0.2511997928280802 and parameters: {'observation_period_num': 20, 'train_rates': 0.9425773404206965, 'learning_rate': 0.0002822085333821534, 'batch_size': 75, 'step_size': 1, 'gamma': 0.7654354483093482}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:26:20,187][0m Trial 43 finished with value: 0.13854275157791562 and parameters: {'observation_period_num': 29, 'train_rates': 0.9125911667891694, 'learning_rate': 0.00039042379153074306, 'batch_size': 28, 'step_size': 3, 'gamma': 0.7505701200868592}. Best is trial 34 with value: 0.05009311065077782.[0m
Early stopping at epoch 94
[32m[I 2025-02-01 11:28:03,539][0m Trial 44 finished with value: 0.31721108767294115 and parameters: {'observation_period_num': 74, 'train_rates': 0.9785495207741858, 'learning_rate': 1.1369377287826463e-05, 'batch_size': 54, 'step_size': 2, 'gamma': 0.7915872302363717}. Best is trial 34 with value: 0.05009311065077782.[0m
Early stopping at epoch 49
[32m[I 2025-02-01 11:28:26,045][0m Trial 45 finished with value: 0.4074682092273628 and parameters: {'observation_period_num': 108, 'train_rates': 0.8722594041747767, 'learning_rate': 6.506736980784101e-05, 'batch_size': 125, 'step_size': 1, 'gamma': 0.7744717436224465}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:29:30,399][0m Trial 46 finished with value: 0.21427892859013664 and parameters: {'observation_period_num': 15, 'train_rates': 0.9487044779369003, 'learning_rate': 0.0006336390790467227, 'batch_size': 91, 'step_size': 3, 'gamma': 0.7641079783818332}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:31:01,829][0m Trial 47 finished with value: 0.16716423483322496 and parameters: {'observation_period_num': 47, 'train_rates': 0.9209403089214754, 'learning_rate': 0.0009363027343506808, 'batch_size': 61, 'step_size': 4, 'gamma': 0.7936729588196079}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:33:41,336][0m Trial 48 finished with value: 0.35514479007469407 and parameters: {'observation_period_num': 150, 'train_rates': 0.9614097574971795, 'learning_rate': 0.00041347655505133966, 'batch_size': 35, 'step_size': 2, 'gamma': 0.9357818171728708}. Best is trial 34 with value: 0.05009311065077782.[0m
[32m[I 2025-02-01 11:35:03,962][0m Trial 49 finished with value: 0.06096993759274483 and parameters: {'observation_period_num': 33, 'train_rates': 0.9777954391468064, 'learning_rate': 0.000259629532939768, 'batch_size': 72, 'step_size': 1, 'gamma': 0.8847160446805352}. Best is trial 34 with value: 0.05009311065077782.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-01 11:35:03,973][0m A new study created in memory with name: no-name-1e5dda9a-98e5-4e53-a163-2d3e3243d174[0m
[32m[I 2025-02-01 11:36:28,526][0m Trial 0 finished with value: 0.3419303814570109 and parameters: {'observation_period_num': 158, 'train_rates': 0.9733449305913573, 'learning_rate': 4.28667762889713e-06, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8779366158418587}. Best is trial 0 with value: 0.3419303814570109.[0m
[32m[I 2025-02-01 11:36:54,868][0m Trial 1 finished with value: 0.08317097118759359 and parameters: {'observation_period_num': 7, 'train_rates': 0.6420748100065807, 'learning_rate': 0.00025140890692451406, 'batch_size': 183, 'step_size': 8, 'gamma': 0.7679091413874373}. Best is trial 1 with value: 0.08317097118759359.[0m
Early stopping at epoch 72
[32m[I 2025-02-01 11:37:26,035][0m Trial 2 finished with value: 0.18588082018856691 and parameters: {'observation_period_num': 117, 'train_rates': 0.6608198396850408, 'learning_rate': 0.000439020028018373, 'batch_size': 105, 'step_size': 1, 'gamma': 0.8140410666098381}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:38:28,808][0m Trial 3 finished with value: 0.624877265058302 and parameters: {'observation_period_num': 199, 'train_rates': 0.6531681869422359, 'learning_rate': 1.5076345701224513e-06, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9499540380313801}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:39:59,334][0m Trial 4 finished with value: 0.4513541559378306 and parameters: {'observation_period_num': 236, 'train_rates': 0.9680524567212109, 'learning_rate': 0.00017447529302815033, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9198122506959239}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:41:19,450][0m Trial 5 finished with value: 0.1441692013759166 and parameters: {'observation_period_num': 188, 'train_rates': 0.7225407971685492, 'learning_rate': 7.161212604567445e-05, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9707939571741404}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:41:41,725][0m Trial 6 finished with value: 0.45118165382305886 and parameters: {'observation_period_num': 138, 'train_rates': 0.7315599941454421, 'learning_rate': 6.162185676176303e-06, 'batch_size': 246, 'step_size': 6, 'gamma': 0.8838417863629443}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:42:33,074][0m Trial 7 finished with value: 0.20041656053402054 and parameters: {'observation_period_num': 31, 'train_rates': 0.757403141023528, 'learning_rate': 9.927718707131132e-06, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9310613073592203}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:42:54,498][0m Trial 8 finished with value: 0.15949499607086182 and parameters: {'observation_period_num': 228, 'train_rates': 0.7580190421746649, 'learning_rate': 0.0008483389363262492, 'batch_size': 240, 'step_size': 1, 'gamma': 0.9560266732070015}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:44:07,012][0m Trial 9 finished with value: 0.234722820272205 and parameters: {'observation_period_num': 220, 'train_rates': 0.76836384589151, 'learning_rate': 0.0005809733294528564, 'batch_size': 64, 'step_size': 7, 'gamma': 0.9052632186474076}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:44:38,218][0m Trial 10 finished with value: 0.1349449166338201 and parameters: {'observation_period_num': 10, 'train_rates': 0.8536175387129262, 'learning_rate': 3.5467740810115825e-05, 'batch_size': 191, 'step_size': 13, 'gamma': 0.7576298518472532}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:45:08,917][0m Trial 11 finished with value: 0.1399252565511441 and parameters: {'observation_period_num': 9, 'train_rates': 0.8664752818266859, 'learning_rate': 3.444048350672058e-05, 'batch_size': 183, 'step_size': 15, 'gamma': 0.7576450462045806}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:45:40,588][0m Trial 12 finished with value: 0.13936493705685546 and parameters: {'observation_period_num': 78, 'train_rates': 0.8577537742301846, 'learning_rate': 0.0001263828814924111, 'batch_size': 177, 'step_size': 13, 'gamma': 0.7505681814715082}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:46:12,059][0m Trial 13 finished with value: 0.24964428526513716 and parameters: {'observation_period_num': 57, 'train_rates': 0.8536707562756302, 'learning_rate': 2.1715621220169694e-05, 'batch_size': 185, 'step_size': 5, 'gamma': 0.8125069750658548}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:46:48,893][0m Trial 14 finished with value: 0.1370596383840853 and parameters: {'observation_period_num': 83, 'train_rates': 0.8248836472699994, 'learning_rate': 5.4985348142240123e-05, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8034551969314798}. Best is trial 1 with value: 0.08317097118759359.[0m
[32m[I 2025-02-01 11:47:11,766][0m Trial 15 finished with value: 0.0722284387974512 and parameters: {'observation_period_num': 8, 'train_rates': 0.6153772978749987, 'learning_rate': 0.0002762195889775257, 'batch_size': 214, 'step_size': 15, 'gamma': 0.7819442096746705}. Best is trial 15 with value: 0.0722284387974512.[0m
[32m[I 2025-02-01 11:47:32,801][0m Trial 16 finished with value: 0.11326496839961585 and parameters: {'observation_period_num': 45, 'train_rates': 0.6029547680706036, 'learning_rate': 0.00025583560604310014, 'batch_size': 221, 'step_size': 15, 'gamma': 0.838583191789729}. Best is trial 15 with value: 0.0722284387974512.[0m
[32m[I 2025-02-01 11:47:55,774][0m Trial 17 finished with value: 0.17601028446794964 and parameters: {'observation_period_num': 105, 'train_rates': 0.6139458164520027, 'learning_rate': 0.00028455475701319135, 'batch_size': 215, 'step_size': 4, 'gamma': 0.7833271213996892}. Best is trial 15 with value: 0.0722284387974512.[0m
[32m[I 2025-02-01 11:48:33,113][0m Trial 18 finished with value: 0.11018689774358884 and parameters: {'observation_period_num': 34, 'train_rates': 0.679796808479969, 'learning_rate': 9.84220220200156e-05, 'batch_size': 132, 'step_size': 8, 'gamma': 0.8496927330173919}. Best is trial 15 with value: 0.0722284387974512.[0m
[32m[I 2025-02-01 11:49:03,874][0m Trial 19 finished with value: 0.0951142567945154 and parameters: {'observation_period_num': 70, 'train_rates': 0.6840326142779203, 'learning_rate': 0.0009681129599253285, 'batch_size': 156, 'step_size': 11, 'gamma': 0.7822095237896721}. Best is trial 15 with value: 0.0722284387974512.[0m
[32m[I 2025-02-01 11:52:12,116][0m Trial 20 finished with value: 0.06050522843478221 and parameters: {'observation_period_num': 5, 'train_rates': 0.6363464444394393, 'learning_rate': 0.0003159169793114995, 'batch_size': 23, 'step_size': 7, 'gamma': 0.8433780450808582}. Best is trial 20 with value: 0.06050522843478221.[0m
[32m[I 2025-02-01 11:54:12,680][0m Trial 21 finished with value: 0.05844426906794609 and parameters: {'observation_period_num': 5, 'train_rates': 0.629788923503527, 'learning_rate': 0.00025964304083904096, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8424356064345344}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 11:57:55,115][0m Trial 22 finished with value: 0.08825234480310178 and parameters: {'observation_period_num': 27, 'train_rates': 0.6229925595150904, 'learning_rate': 0.00036976825882351533, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8467015931594389}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:00:37,977][0m Trial 23 finished with value: 0.09400605909543495 and parameters: {'observation_period_num': 55, 'train_rates': 0.700563178686158, 'learning_rate': 0.00013762112585546983, 'batch_size': 28, 'step_size': 3, 'gamma': 0.8311640258026923}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:02:45,097][0m Trial 24 finished with value: 0.09290509016775503 and parameters: {'observation_period_num': 27, 'train_rates': 0.6360229840187748, 'learning_rate': 0.0005504660757029123, 'batch_size': 34, 'step_size': 7, 'gamma': 0.8585742322256502}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:03:26,503][0m Trial 25 finished with value: 0.24930209787931737 and parameters: {'observation_period_num': 92, 'train_rates': 0.6036859636688909, 'learning_rate': 1.7961686981203867e-05, 'batch_size': 106, 'step_size': 5, 'gamma': 0.8925347657958419}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:04:17,764][0m Trial 26 finished with value: 0.07265656026171856 and parameters: {'observation_period_num': 6, 'train_rates': 0.6690943915445408, 'learning_rate': 0.0001894920388667086, 'batch_size': 91, 'step_size': 9, 'gamma': 0.7900376902702115}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:06:04,924][0m Trial 27 finished with value: 0.09849970661838289 and parameters: {'observation_period_num': 48, 'train_rates': 0.7069904256079255, 'learning_rate': 6.978667129909446e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8203522094248114}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:06:54,007][0m Trial 28 finished with value: 0.16306604093599528 and parameters: {'observation_period_num': 24, 'train_rates': 0.9221286101555091, 'learning_rate': 0.0006505332976483653, 'batch_size': 121, 'step_size': 14, 'gamma': 0.8637788596096527}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:07:50,266][0m Trial 29 finished with value: 0.15858232262796296 and parameters: {'observation_period_num': 172, 'train_rates': 0.6417287097558811, 'learning_rate': 0.00038713445171633396, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8711922242661678}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:09:37,590][0m Trial 30 finished with value: 0.518877674759768 and parameters: {'observation_period_num': 148, 'train_rates': 0.7930651655221408, 'learning_rate': 1.801258954777167e-06, 'batch_size': 45, 'step_size': 11, 'gamma': 0.7997063522629595}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:10:31,186][0m Trial 31 finished with value: 0.07212312485823416 and parameters: {'observation_period_num': 9, 'train_rates': 0.6702051526533003, 'learning_rate': 0.00019914436273259667, 'batch_size': 86, 'step_size': 9, 'gamma': 0.7875894161198582}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:11:25,465][0m Trial 32 finished with value: 0.08104078396398753 and parameters: {'observation_period_num': 21, 'train_rates': 0.6307824260003774, 'learning_rate': 0.00020720133261510463, 'batch_size': 81, 'step_size': 8, 'gamma': 0.7731320143976487}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:15:13,859][0m Trial 33 finished with value: 0.10903193958736614 and parameters: {'observation_period_num': 65, 'train_rates': 0.6621907107683023, 'learning_rate': 0.0003104691190702016, 'batch_size': 19, 'step_size': 6, 'gamma': 0.834905635317342}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:16:47,955][0m Trial 34 finished with value: 0.10471676957421741 and parameters: {'observation_period_num': 41, 'train_rates': 0.6528764943578251, 'learning_rate': 9.932984245271078e-05, 'batch_size': 47, 'step_size': 4, 'gamma': 0.8233991570687568}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:17:45,544][0m Trial 35 finished with value: 0.0649783693120224 and parameters: {'observation_period_num': 6, 'train_rates': 0.6884325196361848, 'learning_rate': 0.0004432964669291629, 'batch_size': 82, 'step_size': 10, 'gamma': 0.7997808293488557}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:18:40,860][0m Trial 36 finished with value: 0.0711997071863944 and parameters: {'observation_period_num': 18, 'train_rates': 0.7000127538193203, 'learning_rate': 0.0006101150284828238, 'batch_size': 87, 'step_size': 10, 'gamma': 0.801151887312092}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:19:24,214][0m Trial 37 finished with value: 0.07810679278086485 and parameters: {'observation_period_num': 19, 'train_rates': 0.727385377331307, 'learning_rate': 0.0004666348157466837, 'batch_size': 113, 'step_size': 10, 'gamma': 0.8145016904048206}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:20:50,310][0m Trial 38 finished with value: 0.09359483593238127 and parameters: {'observation_period_num': 41, 'train_rates': 0.696104723218009, 'learning_rate': 0.0007558795208061815, 'batch_size': 54, 'step_size': 10, 'gamma': 0.8005073648259201}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:21:57,898][0m Trial 39 finished with value: 0.15676602708966764 and parameters: {'observation_period_num': 122, 'train_rates': 0.7424849586031166, 'learning_rate': 0.0009663075462134358, 'batch_size': 72, 'step_size': 8, 'gamma': 0.8519843984811764}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:24:09,145][0m Trial 40 finished with value: 0.2016131644303217 and parameters: {'observation_period_num': 249, 'train_rates': 0.708002591043927, 'learning_rate': 0.00046036145060673374, 'batch_size': 33, 'step_size': 11, 'gamma': 0.877053114331012}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:25:00,472][0m Trial 41 finished with value: 0.07484580011052244 and parameters: {'observation_period_num': 17, 'train_rates': 0.652856564826562, 'learning_rate': 0.0001991505182271304, 'batch_size': 88, 'step_size': 9, 'gamma': 0.7951486188839747}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:25:50,503][0m Trial 42 finished with value: 0.07494904000125627 and parameters: {'observation_period_num': 6, 'train_rates': 0.6816084325662413, 'learning_rate': 0.0001330885932057682, 'batch_size': 95, 'step_size': 9, 'gamma': 0.9881699662274229}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:26:59,214][0m Trial 43 finished with value: 0.08296780712711506 and parameters: {'observation_period_num': 29, 'train_rates': 0.6669978448814194, 'learning_rate': 0.0006390183294674403, 'batch_size': 67, 'step_size': 7, 'gamma': 0.7682602547496384}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:28:15,284][0m Trial 44 finished with value: 0.06708451064238316 and parameters: {'observation_period_num': 17, 'train_rates': 0.6402745383007439, 'learning_rate': 0.0003761663431621656, 'batch_size': 58, 'step_size': 8, 'gamma': 0.808305755924858}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:29:33,845][0m Trial 45 finished with value: 0.09926170680495222 and parameters: {'observation_period_num': 57, 'train_rates': 0.6438834759630914, 'learning_rate': 0.0004656246788239845, 'batch_size': 55, 'step_size': 12, 'gamma': 0.8100841609595932}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:31:30,079][0m Trial 46 finished with value: 0.08327824177257785 and parameters: {'observation_period_num': 36, 'train_rates': 0.6238494620572591, 'learning_rate': 0.0003137952573656491, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8340096874983259}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:36:38,011][0m Trial 47 finished with value: 0.08256902762290368 and parameters: {'observation_period_num': 17, 'train_rates': 0.7764728187059967, 'learning_rate': 0.0006366506805425118, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8225146046918272}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:37:59,812][0m Trial 48 finished with value: 0.0906670190609316 and parameters: {'observation_period_num': 17, 'train_rates': 0.741131300557865, 'learning_rate': 5.044838549833302e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8404219032198263}. Best is trial 21 with value: 0.05844426906794609.[0m
[32m[I 2025-02-01 12:41:01,221][0m Trial 49 finished with value: 0.0923706147205744 and parameters: {'observation_period_num': 35, 'train_rates': 0.6907062075210008, 'learning_rate': 0.000378686668153626, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8939211844578876}. Best is trial 21 with value: 0.05844426906794609.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-01 12:41:01,231][0m A new study created in memory with name: no-name-5912be28-b983-4def-a932-124f5f2af191[0m
[32m[I 2025-02-01 12:42:16,190][0m Trial 0 finished with value: 0.17777737924161385 and parameters: {'observation_period_num': 179, 'train_rates': 0.8474594747611357, 'learning_rate': 0.0003274739010934839, 'batch_size': 69, 'step_size': 9, 'gamma': 0.850607442091825}. Best is trial 0 with value: 0.17777737924161385.[0m
[32m[I 2025-02-01 12:42:39,807][0m Trial 1 finished with value: 0.7708672467101649 and parameters: {'observation_period_num': 76, 'train_rates': 0.7227100005804636, 'learning_rate': 1.361378619695319e-06, 'batch_size': 226, 'step_size': 6, 'gamma': 0.8536222842633978}. Best is trial 0 with value: 0.17777737924161385.[0m
[32m[I 2025-02-01 12:43:06,282][0m Trial 2 finished with value: 0.14507246305975122 and parameters: {'observation_period_num': 112, 'train_rates': 0.6913757686039136, 'learning_rate': 0.00011592295616888175, 'batch_size': 188, 'step_size': 6, 'gamma': 0.9747746677790416}. Best is trial 2 with value: 0.14507246305975122.[0m
[32m[I 2025-02-01 12:43:30,676][0m Trial 3 finished with value: 0.23847934059632478 and parameters: {'observation_period_num': 235, 'train_rates': 0.6378209402736457, 'learning_rate': 0.0005787232857726666, 'batch_size': 190, 'step_size': 3, 'gamma': 0.8860776025735086}. Best is trial 2 with value: 0.14507246305975122.[0m
Early stopping at epoch 82
[32m[I 2025-02-01 12:43:54,213][0m Trial 4 finished with value: 1.260130750603464 and parameters: {'observation_period_num': 63, 'train_rates': 0.7120971130233011, 'learning_rate': 2.2334485779794064e-06, 'batch_size': 177, 'step_size': 2, 'gamma': 0.7752370590336084}. Best is trial 2 with value: 0.14507246305975122.[0m
[32m[I 2025-02-01 12:44:35,752][0m Trial 5 finished with value: 0.13930606662764433 and parameters: {'observation_period_num': 164, 'train_rates': 0.7230813550592387, 'learning_rate': 0.00013430144189041998, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8615014865060071}. Best is trial 5 with value: 0.13930606662764433.[0m
[32m[I 2025-02-01 12:45:18,991][0m Trial 6 finished with value: 0.11035828331583425 and parameters: {'observation_period_num': 73, 'train_rates': 0.7494829731832751, 'learning_rate': 0.00011346066923619329, 'batch_size': 114, 'step_size': 6, 'gamma': 0.987061831034228}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:46:07,497][0m Trial 7 finished with value: 0.5364951592063154 and parameters: {'observation_period_num': 108, 'train_rates': 0.8214267848005504, 'learning_rate': 1.9679270390200664e-06, 'batch_size': 110, 'step_size': 14, 'gamma': 0.8553668641200026}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:47:04,760][0m Trial 8 finished with value: 0.15756095209789853 and parameters: {'observation_period_num': 20, 'train_rates': 0.9156171225808758, 'learning_rate': 7.051229206775424e-05, 'batch_size': 103, 'step_size': 9, 'gamma': 0.8641782401600642}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:47:43,989][0m Trial 9 finished with value: 0.12006431583344467 and parameters: {'observation_period_num': 108, 'train_rates': 0.73249447138694, 'learning_rate': 5.038534186186351e-05, 'batch_size': 126, 'step_size': 13, 'gamma': 0.9533360689862853}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:51:02,603][0m Trial 10 finished with value: 0.26489961207179086 and parameters: {'observation_period_num': 6, 'train_rates': 0.9620976446936627, 'learning_rate': 9.085387986073386e-06, 'batch_size': 29, 'step_size': 5, 'gamma': 0.9380919795665204}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:51:38,899][0m Trial 11 finished with value: 0.12621604210951112 and parameters: {'observation_period_num': 64, 'train_rates': 0.779425243551232, 'learning_rate': 2.18146512481898e-05, 'batch_size': 145, 'step_size': 11, 'gamma': 0.9899514580163955}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:52:38,512][0m Trial 12 finished with value: 0.20590590651429028 and parameters: {'observation_period_num': 161, 'train_rates': 0.6099740295535112, 'learning_rate': 2.7766606920284827e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9303653063765982}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:53:12,848][0m Trial 13 finished with value: 0.2336079851241715 and parameters: {'observation_period_num': 94, 'train_rates': 0.7680944549486143, 'learning_rate': 9.069549002725504e-06, 'batch_size': 147, 'step_size': 12, 'gamma': 0.9417028364490668}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:54:31,568][0m Trial 14 finished with value: 0.12941916676455753 and parameters: {'observation_period_num': 41, 'train_rates': 0.8796328570513474, 'learning_rate': 0.00026846868482986356, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9051161359140174}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:58:56,018][0m Trial 15 finished with value: 0.1976043362638135 and parameters: {'observation_period_num': 143, 'train_rates': 0.6716998720307205, 'learning_rate': 0.000986372976504464, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9643374903049353}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:59:17,337][0m Trial 16 finished with value: 0.36746784017003814 and parameters: {'observation_period_num': 214, 'train_rates': 0.7675744405734871, 'learning_rate': 4.6258020955929534e-05, 'batch_size': 253, 'step_size': 4, 'gamma': 0.7997952096543111}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 12:59:47,616][0m Trial 17 finished with value: 0.29022603915229944 and parameters: {'observation_period_num': 125, 'train_rates': 0.748642466420719, 'learning_rate': 7.805595457845877e-06, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9123749300857946}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:00:42,231][0m Trial 18 finished with value: 0.1280769407749176 and parameters: {'observation_period_num': 85, 'train_rates': 0.8244375609778695, 'learning_rate': 0.00014515380036192978, 'batch_size': 96, 'step_size': 1, 'gamma': 0.9586157231612276}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:01:18,495][0m Trial 19 finished with value: 0.1336054334953173 and parameters: {'observation_period_num': 40, 'train_rates': 0.658133949987559, 'learning_rate': 1.8783774031981772e-05, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9880353582584982}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:03:16,473][0m Trial 20 finished with value: 0.18761757373247506 and parameters: {'observation_period_num': 136, 'train_rates': 0.8121544555500813, 'learning_rate': 6.029005684917806e-05, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8157129209095633}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:03:50,894][0m Trial 21 finished with value: 0.13507514553706734 and parameters: {'observation_period_num': 47, 'train_rates': 0.7528695908999982, 'learning_rate': 1.81261733595863e-05, 'batch_size': 146, 'step_size': 12, 'gamma': 0.9879854010835829}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:04:29,065][0m Trial 22 finished with value: 0.12497233603168373 and parameters: {'observation_period_num': 64, 'train_rates': 0.7938229655303222, 'learning_rate': 2.7165296776441985e-05, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9605597679739589}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:05:27,636][0m Trial 23 finished with value: 0.1507168921646733 and parameters: {'observation_period_num': 98, 'train_rates': 0.8621650726996952, 'learning_rate': 8.24201721649974e-05, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9563213123646868}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:06:08,164][0m Trial 24 finished with value: 0.12044133312916488 and parameters: {'observation_period_num': 70, 'train_rates': 0.7843015505590634, 'learning_rate': 3.94016641131014e-05, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9166593651276616}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:06:49,484][0m Trial 25 finished with value: 0.14489858751354912 and parameters: {'observation_period_num': 119, 'train_rates': 0.738769959556102, 'learning_rate': 4.18011021798496e-05, 'batch_size': 122, 'step_size': 7, 'gamma': 0.9134991681188239}. Best is trial 6 with value: 0.11035828331583425.[0m
[32m[I 2025-02-01 13:07:46,423][0m Trial 26 finished with value: 0.0993796483022826 and parameters: {'observation_period_num': 81, 'train_rates': 0.6956894180423994, 'learning_rate': 0.0002527326771258533, 'batch_size': 82, 'step_size': 8, 'gamma': 0.8900047852334347}. Best is trial 26 with value: 0.0993796483022826.[0m
[32m[I 2025-02-01 13:09:10,565][0m Trial 27 finished with value: 0.1015401009034691 and parameters: {'observation_period_num': 87, 'train_rates': 0.6928890887112935, 'learning_rate': 0.0002639043753576883, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8895192407180595}. Best is trial 26 with value: 0.0993796483022826.[0m
[32m[I 2025-02-01 13:10:38,885][0m Trial 28 finished with value: 0.07516704196783563 and parameters: {'observation_period_num': 26, 'train_rates': 0.6923752209430871, 'learning_rate': 0.00029641061251933174, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8850037740385741}. Best is trial 28 with value: 0.07516704196783563.[0m
[32m[I 2025-02-01 13:12:13,103][0m Trial 29 finished with value: 0.09660069790459518 and parameters: {'observation_period_num': 26, 'train_rates': 0.6019890572569371, 'learning_rate': 0.0002837056813034562, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8832999278229465}. Best is trial 28 with value: 0.07516704196783563.[0m
Early stopping at epoch 88
[32m[I 2025-02-01 13:13:02,067][0m Trial 30 finished with value: 0.1265260151501269 and parameters: {'observation_period_num': 26, 'train_rates': 0.6126934021396766, 'learning_rate': 0.00043243749426954156, 'batch_size': 80, 'step_size': 1, 'gamma': 0.8338534036430698}. Best is trial 28 with value: 0.07516704196783563.[0m
[32m[I 2025-02-01 13:14:34,270][0m Trial 31 finished with value: 0.06589418628932217 and parameters: {'observation_period_num': 7, 'train_rates': 0.6896060340410133, 'learning_rate': 0.0002719918369689691, 'batch_size': 50, 'step_size': 4, 'gamma': 0.8856761786665857}. Best is trial 31 with value: 0.06589418628932217.[0m
[32m[I 2025-02-01 13:16:10,716][0m Trial 32 finished with value: 0.07057690798523891 and parameters: {'observation_period_num': 5, 'train_rates': 0.659672470058003, 'learning_rate': 0.00019213340796118432, 'batch_size': 47, 'step_size': 3, 'gamma': 0.8840626940195225}. Best is trial 31 with value: 0.06589418628932217.[0m
[32m[I 2025-02-01 13:17:38,237][0m Trial 33 finished with value: 0.06449537158524751 and parameters: {'observation_period_num': 5, 'train_rates': 0.6452860192009355, 'learning_rate': 0.0007724610406611285, 'batch_size': 51, 'step_size': 3, 'gamma': 0.8756513020897677}. Best is trial 33 with value: 0.06449537158524751.[0m
[32m[I 2025-02-01 13:18:59,636][0m Trial 34 finished with value: 0.06215068216005497 and parameters: {'observation_period_num': 6, 'train_rates': 0.6448401365118922, 'learning_rate': 0.0009356723658416945, 'batch_size': 55, 'step_size': 3, 'gamma': 0.8437044702741737}. Best is trial 34 with value: 0.06215068216005497.[0m
[32m[I 2025-02-01 13:21:40,215][0m Trial 35 finished with value: 0.059843652307147714 and parameters: {'observation_period_num': 9, 'train_rates': 0.6386910323643188, 'learning_rate': 0.0009937333419597683, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8357087015044975}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:25:39,115][0m Trial 36 finished with value: 0.06340626941330857 and parameters: {'observation_period_num': 14, 'train_rates': 0.6390060593992415, 'learning_rate': 0.0009645656082409054, 'batch_size': 18, 'step_size': 2, 'gamma': 0.8395251691089357}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:30:03,822][0m Trial 37 finished with value: 0.08560716245002514 and parameters: {'observation_period_num': 41, 'train_rates': 0.6338187833030897, 'learning_rate': 0.0009130425331324617, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8398898729614521}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:32:19,403][0m Trial 38 finished with value: 0.1020537529601461 and parameters: {'observation_period_num': 53, 'train_rates': 0.6409352115242236, 'learning_rate': 0.0005291721991046705, 'batch_size': 32, 'step_size': 2, 'gamma': 0.819495399280538}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:34:30,660][0m Trial 39 finished with value: 0.06757296501628814 and parameters: {'observation_period_num': 16, 'train_rates': 0.6310557080697466, 'learning_rate': 0.0007010769330728948, 'batch_size': 33, 'step_size': 3, 'gamma': 0.7808088916132487}. Best is trial 35 with value: 0.059843652307147714.[0m
Early stopping at epoch 57
[32m[I 2025-02-01 13:35:11,288][0m Trial 40 finished with value: 0.11165048581148897 and parameters: {'observation_period_num': 30, 'train_rates': 0.6650301041752482, 'learning_rate': 0.00040807937519637564, 'batch_size': 65, 'step_size': 1, 'gamma': 0.7511691443979391}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:36:29,475][0m Trial 41 finished with value: 0.06593997104563445 and parameters: {'observation_period_num': 11, 'train_rates': 0.7095308972970037, 'learning_rate': 0.0006881718588110131, 'batch_size': 60, 'step_size': 5, 'gamma': 0.8485154491615013}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:39:11,811][0m Trial 42 finished with value: 0.06317241023836276 and parameters: {'observation_period_num': 5, 'train_rates': 0.6761764094074812, 'learning_rate': 0.0006852930324359975, 'batch_size': 28, 'step_size': 2, 'gamma': 0.8710222370894114}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:41:59,037][0m Trial 43 finished with value: 0.06829553610653287 and parameters: {'observation_period_num': 17, 'train_rates': 0.6457375148026403, 'learning_rate': 0.0006740965984171964, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8706023576462607}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:42:22,051][0m Trial 44 finished with value: 0.1255273284932644 and parameters: {'observation_period_num': 35, 'train_rates': 0.6225116250794157, 'learning_rate': 0.00048662864284015217, 'batch_size': 206, 'step_size': 3, 'gamma': 0.826505672802746}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:44:19,823][0m Trial 45 finished with value: 0.09154077917096298 and parameters: {'observation_period_num': 54, 'train_rates': 0.6806174178342416, 'learning_rate': 0.00097919317085653, 'batch_size': 38, 'step_size': 2, 'gamma': 0.8698669916125698}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:47:15,371][0m Trial 46 finished with value: 0.07892718085422673 and parameters: {'observation_period_num': 18, 'train_rates': 0.6535792254298116, 'learning_rate': 0.0003895912276048589, 'batch_size': 25, 'step_size': 5, 'gamma': 0.849910065481506}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:51:49,164][0m Trial 47 finished with value: 0.0699572737821633 and parameters: {'observation_period_num': 7, 'train_rates': 0.7103749543047113, 'learning_rate': 0.0006840249388712214, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8060733109981882}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:52:43,432][0m Trial 48 finished with value: 0.20805515502766486 and parameters: {'observation_period_num': 194, 'train_rates': 0.6214711199022525, 'learning_rate': 0.0005573082066910143, 'batch_size': 76, 'step_size': 2, 'gamma': 0.8401073946620301}. Best is trial 35 with value: 0.059843652307147714.[0m
[32m[I 2025-02-01 13:54:35,708][0m Trial 49 finished with value: 1.1044504427290582 and parameters: {'observation_period_num': 55, 'train_rates': 0.6019031550744371, 'learning_rate': 1.1068249052715566e-06, 'batch_size': 37, 'step_size': 3, 'gamma': 0.8626372055225879}. Best is trial 35 with value: 0.059843652307147714.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-01 13:54:35,719][0m A new study created in memory with name: no-name-557c47d3-4e93-4e3d-aa49-ec8b39105529[0m
[32m[I 2025-02-01 13:55:10,214][0m Trial 0 finished with value: 0.6889171871306646 and parameters: {'observation_period_num': 244, 'train_rates': 0.7118441958803374, 'learning_rate': 1.5656457697099784e-06, 'batch_size': 135, 'step_size': 15, 'gamma': 0.7959529741182126}. Best is trial 0 with value: 0.6889171871306646.[0m
[32m[I 2025-02-01 13:55:39,494][0m Trial 1 finished with value: 0.41540650900217857 and parameters: {'observation_period_num': 96, 'train_rates': 0.7845198390645178, 'learning_rate': 3.416465545873198e-06, 'batch_size': 185, 'step_size': 12, 'gamma': 0.9377530779302456}. Best is trial 1 with value: 0.41540650900217857.[0m
[32m[I 2025-02-01 13:56:22,345][0m Trial 2 finished with value: 0.7786027105514612 and parameters: {'observation_period_num': 189, 'train_rates': 0.6163452071955723, 'learning_rate': 1.5246973656396124e-06, 'batch_size': 98, 'step_size': 8, 'gamma': 0.8618868945316962}. Best is trial 1 with value: 0.41540650900217857.[0m
[32m[I 2025-02-01 13:56:47,634][0m Trial 3 finished with value: 0.12046720591825169 and parameters: {'observation_period_num': 55, 'train_rates': 0.7633516702899825, 'learning_rate': 0.00011165489093315897, 'batch_size': 219, 'step_size': 7, 'gamma': 0.9263381503072001}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 13:57:14,153][0m Trial 4 finished with value: 0.40121151166429153 and parameters: {'observation_period_num': 40, 'train_rates': 0.8847220998595013, 'learning_rate': 5.584676538658981e-06, 'batch_size': 222, 'step_size': 15, 'gamma': 0.846535274766786}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 13:57:32,418][0m Trial 5 finished with value: 0.20718347045528765 and parameters: {'observation_period_num': 162, 'train_rates': 0.621037593456759, 'learning_rate': 0.00034091365555182147, 'batch_size': 250, 'step_size': 6, 'gamma': 0.7501778323771334}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 13:58:29,828][0m Trial 6 finished with value: 0.2635212188385462 and parameters: {'observation_period_num': 202, 'train_rates': 0.922932901745614, 'learning_rate': 2.4397125282738113e-05, 'batch_size': 95, 'step_size': 11, 'gamma': 0.891940101231085}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 13:59:29,453][0m Trial 7 finished with value: 0.16818270706894375 and parameters: {'observation_period_num': 42, 'train_rates': 0.8963054144292051, 'learning_rate': 0.00022686223407256015, 'batch_size': 95, 'step_size': 2, 'gamma': 0.8180146673273103}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 13:59:51,463][0m Trial 8 finished with value: 0.2020835403989001 and parameters: {'observation_period_num': 42, 'train_rates': 0.6625063513509233, 'learning_rate': 1.3589209848765209e-05, 'batch_size': 240, 'step_size': 9, 'gamma': 0.941376033870355}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 14:00:28,203][0m Trial 9 finished with value: 0.16239129401434807 and parameters: {'observation_period_num': 182, 'train_rates': 0.7564869342491228, 'learning_rate': 6.151831592805037e-05, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8503450317246646}. Best is trial 3 with value: 0.12046720591825169.[0m
[32m[I 2025-02-01 14:04:07,036][0m Trial 10 finished with value: 0.09352783900949191 and parameters: {'observation_period_num': 104, 'train_rates': 0.9816107531247044, 'learning_rate': 0.0009967081492505921, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9698770043596394}. Best is trial 10 with value: 0.09352783900949191.[0m
[32m[I 2025-02-01 14:07:05,050][0m Trial 11 finished with value: 0.07454623993900088 and parameters: {'observation_period_num': 96, 'train_rates': 0.9844602996957391, 'learning_rate': 0.0009742337832582254, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9684742434076136}. Best is trial 11 with value: 0.07454623993900088.[0m
[32m[I 2025-02-01 14:09:22,361][0m Trial 12 finished with value: 0.3773293329440818 and parameters: {'observation_period_num': 114, 'train_rates': 0.965595787269283, 'learning_rate': 0.0008136328899353148, 'batch_size': 41, 'step_size': 1, 'gamma': 0.9787104667794531}. Best is trial 11 with value: 0.07454623993900088.[0m
[32m[I 2025-02-01 14:14:39,285][0m Trial 13 finished with value: 0.057940245419740674 and parameters: {'observation_period_num': 88, 'train_rates': 0.9829063852233081, 'learning_rate': 0.0009459622748322215, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9873215427985044}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:16:11,213][0m Trial 14 finished with value: 0.09361586647957755 and parameters: {'observation_period_num': 7, 'train_rates': 0.8415931151682644, 'learning_rate': 0.0003578132818349095, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9876059428731313}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:17:44,272][0m Trial 15 finished with value: 0.2668458055357755 and parameters: {'observation_period_num': 142, 'train_rates': 0.9428218231984162, 'learning_rate': 0.00013033341655580274, 'batch_size': 60, 'step_size': 5, 'gamma': 0.9037999130148944}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:21:49,808][0m Trial 16 finished with value: 0.17824325734792754 and parameters: {'observation_period_num': 85, 'train_rates': 0.8504244126975765, 'learning_rate': 0.0005385384531233212, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9572585170838254}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:23:14,110][0m Trial 17 finished with value: 0.15234032273292542 and parameters: {'observation_period_num': 70, 'train_rates': 0.9899597382936323, 'learning_rate': 4.405403062750149e-05, 'batch_size': 70, 'step_size': 1, 'gamma': 0.906860570740411}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:23:45,650][0m Trial 18 finished with value: 0.15026056760141993 and parameters: {'observation_period_num': 132, 'train_rates': 0.8343150301246403, 'learning_rate': 0.00016421511764538174, 'batch_size': 178, 'step_size': 3, 'gamma': 0.9521743628224469}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:29:33,788][0m Trial 19 finished with value: 0.15083662908947235 and parameters: {'observation_period_num': 11, 'train_rates': 0.9223995770152585, 'learning_rate': 0.0004970459375029479, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9870577282108519}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:31:25,284][0m Trial 20 finished with value: 0.13170555597543718 and parameters: {'observation_period_num': 78, 'train_rates': 0.8699957809785149, 'learning_rate': 7.334093906557389e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.8836012013668735}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:34:29,122][0m Trial 21 finished with value: 0.08101750239729881 and parameters: {'observation_period_num': 103, 'train_rates': 0.9860615217785307, 'learning_rate': 0.0009542015865917496, 'batch_size': 31, 'step_size': 3, 'gamma': 0.9611142480850412}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:35:41,612][0m Trial 22 finished with value: 0.2791536735816741 and parameters: {'observation_period_num': 123, 'train_rates': 0.9500384469645852, 'learning_rate': 0.0007505658080512546, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9211916518280019}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:38:02,762][0m Trial 23 finished with value: 0.26024615066915247 and parameters: {'observation_period_num': 150, 'train_rates': 0.9158970614314325, 'learning_rate': 0.0002467101441092671, 'batch_size': 38, 'step_size': 5, 'gamma': 0.9679354642831727}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:38:57,325][0m Trial 24 finished with value: 0.08591033518314362 and parameters: {'observation_period_num': 108, 'train_rates': 0.9888781681477643, 'learning_rate': 0.000946471541275448, 'batch_size': 109, 'step_size': 2, 'gamma': 0.9532403052657223}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:41:26,681][0m Trial 25 finished with value: 0.3040007766103372 and parameters: {'observation_period_num': 62, 'train_rates': 0.9559916463921676, 'learning_rate': 0.0004523047308342059, 'batch_size': 38, 'step_size': 5, 'gamma': 0.9895362332125371}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:42:41,745][0m Trial 26 finished with value: 0.2612102564424276 and parameters: {'observation_period_num': 90, 'train_rates': 0.9442601033795996, 'learning_rate': 0.00021800004859634528, 'batch_size': 76, 'step_size': 2, 'gamma': 0.9260569211812569}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:43:18,629][0m Trial 27 finished with value: 0.20807171067384284 and parameters: {'observation_period_num': 154, 'train_rates': 0.909372243292157, 'learning_rate': 0.0006350053450737437, 'batch_size': 159, 'step_size': 7, 'gamma': 0.9643888507765928}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:44:09,014][0m Trial 28 finished with value: 0.3615378737449646 and parameters: {'observation_period_num': 127, 'train_rates': 0.9654236092020904, 'learning_rate': 0.00033366988004309263, 'batch_size': 118, 'step_size': 4, 'gamma': 0.9403640145289168}. Best is trial 13 with value: 0.057940245419740674.[0m
Early stopping at epoch 59
[32m[I 2025-02-01 14:44:57,319][0m Trial 29 finished with value: 0.3076634248423828 and parameters: {'observation_period_num': 169, 'train_rates': 0.6938662251893318, 'learning_rate': 8.902468734528927e-05, 'batch_size': 56, 'step_size': 1, 'gamma': 0.782951364956586}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:47:35,679][0m Trial 30 finished with value: 0.2049497898258915 and parameters: {'observation_period_num': 234, 'train_rates': 0.8094753089539551, 'learning_rate': 7.91386195464168e-06, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9738089544907614}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:48:27,093][0m Trial 31 finished with value: 0.06890719383955002 and parameters: {'observation_period_num': 104, 'train_rates': 0.9840348655690231, 'learning_rate': 0.0007563708939645838, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9503738438373418}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:49:07,585][0m Trial 32 finished with value: 0.2216771386992869 and parameters: {'observation_period_num': 99, 'train_rates': 0.9333299682136711, 'learning_rate': 0.0006417685175211386, 'batch_size': 147, 'step_size': 3, 'gamma': 0.9466965400500128}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:49:57,862][0m Trial 33 finished with value: 0.3377588987350464 and parameters: {'observation_period_num': 84, 'train_rates': 0.9652384848319989, 'learning_rate': 0.0009774868420994597, 'batch_size': 120, 'step_size': 2, 'gamma': 0.9622465076259514}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:55:10,352][0m Trial 34 finished with value: 0.4551527843653382 and parameters: {'observation_period_num': 112, 'train_rates': 0.9729401564920837, 'learning_rate': 0.00043658992520306247, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9178278633330853}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:55:40,786][0m Trial 35 finished with value: 0.1845180356502533 and parameters: {'observation_period_num': 58, 'train_rates': 0.8967505227081753, 'learning_rate': 0.0002834613603662316, 'batch_size': 192, 'step_size': 1, 'gamma': 0.934560365540518}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:56:51,293][0m Trial 36 finished with value: 0.5894413303445887 and parameters: {'observation_period_num': 138, 'train_rates': 0.9425355626138028, 'learning_rate': 2.4372898405424085e-06, 'batch_size': 79, 'step_size': 8, 'gamma': 0.976377444703752}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:58:46,384][0m Trial 37 finished with value: 0.12747932469245393 and parameters: {'observation_period_num': 26, 'train_rates': 0.8795237906362513, 'learning_rate': 0.00017972382211121037, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8740429113476489}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 14:59:43,715][0m Trial 38 finished with value: 0.23968314092529558 and parameters: {'observation_period_num': 69, 'train_rates': 0.7816397794460315, 'learning_rate': 2.3564142718045884e-05, 'batch_size': 90, 'step_size': 3, 'gamma': 0.8232455292231148}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:02:48,204][0m Trial 39 finished with value: 0.09389211609959602 and parameters: {'observation_period_num': 95, 'train_rates': 0.9854510667817433, 'learning_rate': 0.0005720019688089079, 'batch_size': 31, 'step_size': 5, 'gamma': 0.9063692131754625}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:03:56,928][0m Trial 40 finished with value: 0.1375892612869378 and parameters: {'observation_period_num': 117, 'train_rates': 0.7424213267361743, 'learning_rate': 0.00039363445039568474, 'batch_size': 69, 'step_size': 7, 'gamma': 0.9362255597315892}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:04:45,588][0m Trial 41 finished with value: 0.06729873269796371 and parameters: {'observation_period_num': 105, 'train_rates': 0.9805062445136214, 'learning_rate': 0.0008630814132933242, 'batch_size': 126, 'step_size': 2, 'gamma': 0.953377306763614}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:05:27,777][0m Trial 42 finished with value: 1.3995027542114258 and parameters: {'observation_period_num': 98, 'train_rates': 0.9650923175741045, 'learning_rate': 1.1742919509291748e-06, 'batch_size': 145, 'step_size': 2, 'gamma': 0.9551738935663586}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:06:20,223][0m Trial 43 finished with value: 0.20661372576483447 and parameters: {'observation_period_num': 77, 'train_rates': 0.9303829140593346, 'learning_rate': 0.0007030379243000841, 'batch_size': 109, 'step_size': 2, 'gamma': 0.9461017397465459}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:06:47,668][0m Trial 44 finished with value: 0.15584041253192596 and parameters: {'observation_period_num': 106, 'train_rates': 0.6023669728123346, 'learning_rate': 0.000993239941232778, 'batch_size': 169, 'step_size': 3, 'gamma': 0.9793443918807129}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:07:16,032][0m Trial 45 finished with value: 0.16873030084436827 and parameters: {'observation_period_num': 49, 'train_rates': 0.9060354164047557, 'learning_rate': 0.0006463782854049177, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9660601546814533}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:08:02,395][0m Trial 46 finished with value: 0.15415054559707642 and parameters: {'observation_period_num': 90, 'train_rates': 0.9736196767406866, 'learning_rate': 0.0003204203340913923, 'batch_size': 129, 'step_size': 1, 'gamma': 0.9337386431578822}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:08:58,826][0m Trial 47 finished with value: 0.12481584399938583 and parameters: {'observation_period_num': 119, 'train_rates': 0.9899858447017453, 'learning_rate': 0.0007483541044800311, 'batch_size': 105, 'step_size': 2, 'gamma': 0.980377658552194}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:10:48,116][0m Trial 48 finished with value: 0.27898384825043054 and parameters: {'observation_period_num': 133, 'train_rates': 0.9512201200334306, 'learning_rate': 0.0004771227735991949, 'batch_size': 51, 'step_size': 3, 'gamma': 0.8942031907131036}. Best is trial 13 with value: 0.057940245419740674.[0m
[32m[I 2025-02-01 15:11:51,336][0m Trial 49 finished with value: 0.2030504875104217 and parameters: {'observation_period_num': 66, 'train_rates': 0.9296294266187201, 'learning_rate': 0.00013963826103995084, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9592524314195868}. Best is trial 13 with value: 0.057940245419740674.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 9, 'train_rates': 0.6657248148427776, 'learning_rate': 0.0006401750543565856, 'batch_size': 122, 'step_size': 12, 'gamma': 0.9429481356791214}
Epoch 1/300, trend Loss: 0.4086 | 0.1884
Epoch 2/300, trend Loss: 0.2116 | 0.2018
Epoch 3/300, trend Loss: 0.2288 | 0.1659
Epoch 4/300, trend Loss: 0.1712 | 0.1313
Epoch 5/300, trend Loss: 0.1485 | 0.1145
Epoch 6/300, trend Loss: 0.1383 | 0.1109
Epoch 7/300, trend Loss: 0.1251 | 0.1161
Epoch 8/300, trend Loss: 0.1251 | 0.1102
Epoch 9/300, trend Loss: 0.1185 | 0.1003
Epoch 10/300, trend Loss: 0.1136 | 0.0955
Epoch 11/300, trend Loss: 0.1135 | 0.0999
Epoch 12/300, trend Loss: 0.1091 | 0.0938
Epoch 13/300, trend Loss: 0.1076 | 0.0920
Epoch 14/300, trend Loss: 0.1063 | 0.0906
Epoch 15/300, trend Loss: 0.1034 | 0.0896
Epoch 16/300, trend Loss: 0.1015 | 0.0890
Epoch 17/300, trend Loss: 0.0993 | 0.1025
Epoch 18/300, trend Loss: 0.1016 | 0.0886
Epoch 19/300, trend Loss: 0.0996 | 0.1106
Epoch 20/300, trend Loss: 0.1061 | 0.0851
Epoch 21/300, trend Loss: 0.1060 | 0.1140
Epoch 22/300, trend Loss: 0.1355 | 0.1061
Epoch 23/300, trend Loss: 0.1027 | 0.0804
Epoch 24/300, trend Loss: 0.1008 | 0.0825
Epoch 25/300, trend Loss: 0.1027 | 0.1182
Epoch 26/300, trend Loss: 0.1175 | 0.1042
Epoch 27/300, trend Loss: 0.1328 | 0.1127
Epoch 28/300, trend Loss: 0.1299 | 0.0950
Epoch 29/300, trend Loss: 0.1147 | 0.1313
Epoch 30/300, trend Loss: 0.1109 | 0.1025
Epoch 31/300, trend Loss: 0.1152 | 0.0991
Epoch 32/300, trend Loss: 0.1172 | 0.0914
Epoch 33/300, trend Loss: 0.1083 | 0.1242
Epoch 34/300, trend Loss: 0.1104 | 0.0861
Epoch 35/300, trend Loss: 0.1040 | 0.0795
Epoch 36/300, trend Loss: 0.0965 | 0.0931
Epoch 37/300, trend Loss: 0.0899 | 0.0757
Epoch 38/300, trend Loss: 0.0886 | 0.0751
Epoch 39/300, trend Loss: 0.0869 | 0.0907
Epoch 40/300, trend Loss: 0.0927 | 0.0755
Epoch 41/300, trend Loss: 0.0882 | 0.0820
Epoch 42/300, trend Loss: 0.0971 | 0.0769
Epoch 43/300, trend Loss: 0.0870 | 0.0733
Epoch 44/300, trend Loss: 0.0878 | 0.0727
Epoch 45/300, trend Loss: 0.0890 | 0.0877
Epoch 46/300, trend Loss: 0.0986 | 0.0819
Epoch 47/300, trend Loss: 0.0883 | 0.0765
Epoch 48/300, trend Loss: 0.0879 | 0.0725
Epoch 49/300, trend Loss: 0.0862 | 0.0751
Epoch 50/300, trend Loss: 0.0912 | 0.0720
Epoch 51/300, trend Loss: 0.0892 | 0.0748
Epoch 52/300, trend Loss: 0.0957 | 0.0806
Epoch 53/300, trend Loss: 0.0853 | 0.0719
Epoch 54/300, trend Loss: 0.0840 | 0.0686
Epoch 55/300, trend Loss: 0.0862 | 0.0743
Epoch 56/300, trend Loss: 0.0933 | 0.0792
Epoch 57/300, trend Loss: 0.0869 | 0.0752
Epoch 58/300, trend Loss: 0.0872 | 0.0704
Epoch 59/300, trend Loss: 0.0907 | 0.0791
Epoch 60/300, trend Loss: 0.0929 | 0.0817
Epoch 61/300, trend Loss: 0.0838 | 0.0712
Epoch 62/300, trend Loss: 0.0867 | 0.0710
Epoch 63/300, trend Loss: 0.0854 | 0.0715
Epoch 64/300, trend Loss: 0.0896 | 0.0740
Epoch 65/300, trend Loss: 0.0887 | 0.0699
Epoch 66/300, trend Loss: 0.0926 | 0.0742
Epoch 67/300, trend Loss: 0.0848 | 0.0678
Epoch 68/300, trend Loss: 0.0839 | 0.0726
Epoch 69/300, trend Loss: 0.0841 | 0.0740
Epoch 70/300, trend Loss: 0.0863 | 0.0784
Epoch 71/300, trend Loss: 0.0810 | 0.0695
Epoch 72/300, trend Loss: 0.0811 | 0.0725
Epoch 73/300, trend Loss: 0.0785 | 0.0670
Epoch 74/300, trend Loss: 0.0793 | 0.0712
Epoch 75/300, trend Loss: 0.0783 | 0.0667
Epoch 76/300, trend Loss: 0.0783 | 0.0708
Epoch 77/300, trend Loss: 0.0774 | 0.0667
Epoch 78/300, trend Loss: 0.0763 | 0.0707
Epoch 79/300, trend Loss: 0.0747 | 0.0667
Epoch 80/300, trend Loss: 0.0745 | 0.0692
Epoch 81/300, trend Loss: 0.0738 | 0.0675
Epoch 82/300, trend Loss: 0.0744 | 0.0684
Epoch 83/300, trend Loss: 0.0748 | 0.0677
Epoch 84/300, trend Loss: 0.0766 | 0.0714
Epoch 85/300, trend Loss: 0.0747 | 0.0681
Epoch 86/300, trend Loss: 0.0735 | 0.0670
Epoch 87/300, trend Loss: 0.0739 | 0.0690
Epoch 88/300, trend Loss: 0.0730 | 0.0667
Epoch 89/300, trend Loss: 0.0713 | 0.0656
Epoch 90/300, trend Loss: 0.0711 | 0.0660
Epoch 91/300, trend Loss: 0.0707 | 0.0645
Epoch 92/300, trend Loss: 0.0699 | 0.0651
Epoch 93/300, trend Loss: 0.0696 | 0.0646
Epoch 94/300, trend Loss: 0.0702 | 0.0659
Epoch 95/300, trend Loss: 0.0708 | 0.0636
Epoch 96/300, trend Loss: 0.0722 | 0.0656
Epoch 97/300, trend Loss: 0.0724 | 0.0631
Epoch 98/300, trend Loss: 0.0725 | 0.0666
Epoch 99/300, trend Loss: 0.0733 | 0.0637
Epoch 100/300, trend Loss: 0.0723 | 0.0667
Epoch 101/300, trend Loss: 0.0721 | 0.0667
Epoch 102/300, trend Loss: 0.0731 | 0.0679
Epoch 103/300, trend Loss: 0.0717 | 0.0679
Epoch 104/300, trend Loss: 0.0753 | 0.0712
Epoch 105/300, trend Loss: 0.0712 | 0.0649
Epoch 106/300, trend Loss: 0.0709 | 0.0718
Epoch 107/300, trend Loss: 0.0754 | 0.0707
Epoch 108/300, trend Loss: 0.0727 | 0.0714
Epoch 109/300, trend Loss: 0.0722 | 0.0679
Epoch 110/300, trend Loss: 0.0716 | 0.0690
Epoch 111/300, trend Loss: 0.0707 | 0.0659
Epoch 112/300, trend Loss: 0.0688 | 0.0661
Epoch 113/300, trend Loss: 0.0679 | 0.0639
Epoch 114/300, trend Loss: 0.0680 | 0.0650
Epoch 115/300, trend Loss: 0.0670 | 0.0631
Epoch 116/300, trend Loss: 0.0667 | 0.0632
Epoch 117/300, trend Loss: 0.0665 | 0.0627
Epoch 118/300, trend Loss: 0.0669 | 0.0660
Epoch 119/300, trend Loss: 0.0718 | 0.0691
Epoch 120/300, trend Loss: 0.0710 | 0.0690
Epoch 121/300, trend Loss: 0.0704 | 0.0671
Epoch 122/300, trend Loss: 0.0705 | 0.0677
Epoch 123/300, trend Loss: 0.0701 | 0.0670
Epoch 124/300, trend Loss: 0.0702 | 0.0679
Epoch 125/300, trend Loss: 0.0704 | 0.0649
Epoch 126/300, trend Loss: 0.0707 | 0.0673
Epoch 127/300, trend Loss: 0.0692 | 0.0653
Epoch 128/300, trend Loss: 0.0683 | 0.0652
Epoch 129/300, trend Loss: 0.0670 | 0.0638
Epoch 130/300, trend Loss: 0.0686 | 0.0665
Epoch 131/300, trend Loss: 0.0688 | 0.0638
Epoch 132/300, trend Loss: 0.0687 | 0.0644
Epoch 133/300, trend Loss: 0.0679 | 0.0625
Epoch 134/300, trend Loss: 0.0666 | 0.0643
Epoch 135/300, trend Loss: 0.0668 | 0.0640
Epoch 136/300, trend Loss: 0.0676 | 0.0658
Epoch 137/300, trend Loss: 0.0655 | 0.0631
Epoch 138/300, trend Loss: 0.0648 | 0.0648
Epoch 139/300, trend Loss: 0.0646 | 0.0634
Epoch 140/300, trend Loss: 0.0652 | 0.0645
Epoch 141/300, trend Loss: 0.0650 | 0.0633
Epoch 142/300, trend Loss: 0.0640 | 0.0647
Epoch 143/300, trend Loss: 0.0644 | 0.0635
Epoch 144/300, trend Loss: 0.0644 | 0.0638
Epoch 145/300, trend Loss: 0.0650 | 0.0642
Epoch 146/300, trend Loss: 0.0655 | 0.0652
Epoch 147/300, trend Loss: 0.0641 | 0.0626
Epoch 148/300, trend Loss: 0.0629 | 0.0633
Epoch 149/300, trend Loss: 0.0637 | 0.0635
Epoch 150/300, trend Loss: 0.0640 | 0.0641
Epoch 151/300, trend Loss: 0.0629 | 0.0638
Epoch 152/300, trend Loss: 0.0628 | 0.0637
Epoch 153/300, trend Loss: 0.0627 | 0.0630
Epoch 154/300, trend Loss: 0.0626 | 0.0635
Epoch 155/300, trend Loss: 0.0622 | 0.0627
Epoch 156/300, trend Loss: 0.0619 | 0.0635
Epoch 157/300, trend Loss: 0.0621 | 0.0635
Epoch 158/300, trend Loss: 0.0630 | 0.0640
Epoch 159/300, trend Loss: 0.0624 | 0.0644
Epoch 160/300, trend Loss: 0.0624 | 0.0640
Epoch 161/300, trend Loss: 0.0620 | 0.0633
Epoch 162/300, trend Loss: 0.0619 | 0.0643
Epoch 163/300, trend Loss: 0.0615 | 0.0633
Epoch 164/300, trend Loss: 0.0617 | 0.0647
Epoch 165/300, trend Loss: 0.0618 | 0.0640
Epoch 166/300, trend Loss: 0.0613 | 0.0653
Epoch 167/300, trend Loss: 0.0615 | 0.0635
Epoch 168/300, trend Loss: 0.0617 | 0.0644
Epoch 169/300, trend Loss: 0.0617 | 0.0638
Epoch 170/300, trend Loss: 0.0620 | 0.0652
Epoch 171/300, trend Loss: 0.0621 | 0.0638
Epoch 172/300, trend Loss: 0.0618 | 0.0641
Epoch 173/300, trend Loss: 0.0614 | 0.0644
Epoch 174/300, trend Loss: 0.0607 | 0.0637
Epoch 175/300, trend Loss: 0.0604 | 0.0631
Epoch 176/300, trend Loss: 0.0600 | 0.0638
Epoch 177/300, trend Loss: 0.0596 | 0.0633
Epoch 178/300, trend Loss: 0.0594 | 0.0639
Epoch 179/300, trend Loss: 0.0596 | 0.0633
Epoch 180/300, trend Loss: 0.0592 | 0.0656
Epoch 181/300, trend Loss: 0.0591 | 0.0632
Epoch 182/300, trend Loss: 0.0589 | 0.0637
Epoch 183/300, trend Loss: 0.0586 | 0.0632
Epoch 184/300, trend Loss: 0.0585 | 0.0637
Epoch 185/300, trend Loss: 0.0586 | 0.0639
Epoch 186/300, trend Loss: 0.0592 | 0.0641
Epoch 187/300, trend Loss: 0.0588 | 0.0654
Epoch 188/300, trend Loss: 0.0593 | 0.0645
Epoch 189/300, trend Loss: 0.0593 | 0.0648
Epoch 190/300, trend Loss: 0.0592 | 0.0651
Epoch 191/300, trend Loss: 0.0589 | 0.0649
Epoch 192/300, trend Loss: 0.0584 | 0.0650
Epoch 193/300, trend Loss: 0.0580 | 0.0654
Epoch 194/300, trend Loss: 0.0584 | 0.0653
Epoch 195/300, trend Loss: 0.0591 | 0.0652
Epoch 196/300, trend Loss: 0.0597 | 0.0656
Epoch 197/300, trend Loss: 0.0597 | 0.0646
Epoch 198/300, trend Loss: 0.0590 | 0.0651
Epoch 199/300, trend Loss: 0.0587 | 0.0656
Epoch 200/300, trend Loss: 0.0582 | 0.0663
Epoch 201/300, trend Loss: 0.0587 | 0.0654
Epoch 202/300, trend Loss: 0.0584 | 0.0658
Epoch 203/300, trend Loss: 0.0578 | 0.0653
Epoch 204/300, trend Loss: 0.0572 | 0.0655
Epoch 205/300, trend Loss: 0.0568 | 0.0645
Epoch 206/300, trend Loss: 0.0571 | 0.0653
Epoch 207/300, trend Loss: 0.0570 | 0.0643
Epoch 208/300, trend Loss: 0.0566 | 0.0651
Epoch 209/300, trend Loss: 0.0565 | 0.0647
Epoch 210/300, trend Loss: 0.0568 | 0.0657
Epoch 211/300, trend Loss: 0.0567 | 0.0668
Epoch 212/300, trend Loss: 0.0567 | 0.0659
Epoch 213/300, trend Loss: 0.0563 | 0.0656
Epoch 214/300, trend Loss: 0.0560 | 0.0654
Epoch 215/300, trend Loss: 0.0556 | 0.0654
Epoch 216/300, trend Loss: 0.0551 | 0.0662
Epoch 217/300, trend Loss: 0.0546 | 0.0664
Epoch 218/300, trend Loss: 0.0541 | 0.0673
Epoch 219/300, trend Loss: 0.0534 | 0.0666
Epoch 220/300, trend Loss: 0.0521 | 0.0659
Epoch 221/300, trend Loss: 0.0511 | 0.0662
Epoch 222/300, trend Loss: 0.0506 | 0.0659
Epoch 223/300, trend Loss: 0.0513 | 0.0670
Epoch 224/300, trend Loss: 0.0510 | 0.0673
Epoch 225/300, trend Loss: 0.0497 | 0.0672
Epoch 226/300, trend Loss: 0.0490 | 0.0675
Epoch 227/300, trend Loss: 0.0487 | 0.0668
Epoch 228/300, trend Loss: 0.0492 | 0.0666
Epoch 229/300, trend Loss: 0.0484 | 0.0670
Epoch 230/300, trend Loss: 0.0479 | 0.0671
Epoch 231/300, trend Loss: 0.0472 | 0.0674
Epoch 232/300, trend Loss: 0.0472 | 0.0674
Epoch 233/300, trend Loss: 0.0467 | 0.0678
Epoch 234/300, trend Loss: 0.0468 | 0.0666
Epoch 235/300, trend Loss: 0.0474 | 0.0682
Epoch 236/300, trend Loss: 0.0483 | 0.0667
Epoch 237/300, trend Loss: 0.0469 | 0.0669
Epoch 238/300, trend Loss: 0.0466 | 0.0677
Epoch 239/300, trend Loss: 0.0466 | 0.0676
Epoch 240/300, trend Loss: 0.0469 | 0.0685
Epoch 241/300, trend Loss: 0.0481 | 0.0677
Epoch 242/300, trend Loss: 0.0476 | 0.0672
Epoch 243/300, trend Loss: 0.0467 | 0.0670
Epoch 244/300, trend Loss: 0.0462 | 0.0675
Epoch 245/300, trend Loss: 0.0460 | 0.0675
Epoch 246/300, trend Loss: 0.0460 | 0.0678
Epoch 247/300, trend Loss: 0.0458 | 0.0685
Epoch 248/300, trend Loss: 0.0458 | 0.0678
Epoch 249/300, trend Loss: 0.0457 | 0.0675
Epoch 250/300, trend Loss: 0.0456 | 0.0674
Epoch 251/300, trend Loss: 0.0455 | 0.0676
Epoch 252/300, trend Loss: 0.0455 | 0.0680
Epoch 253/300, trend Loss: 0.0452 | 0.0682
Epoch 254/300, trend Loss: 0.0450 | 0.0685
Epoch 255/300, trend Loss: 0.0449 | 0.0674
Epoch 256/300, trend Loss: 0.0448 | 0.0676
Epoch 257/300, trend Loss: 0.0447 | 0.0675
Epoch 258/300, trend Loss: 0.0447 | 0.0681
Epoch 259/300, trend Loss: 0.0448 | 0.0679
Epoch 260/300, trend Loss: 0.0448 | 0.0690
Epoch 261/300, trend Loss: 0.0446 | 0.0683
Epoch 262/300, trend Loss: 0.0447 | 0.0679
Epoch 263/300, trend Loss: 0.0446 | 0.0678
Epoch 264/300, trend Loss: 0.0445 | 0.0680
Epoch 265/300, trend Loss: 0.0442 | 0.0681
Epoch 266/300, trend Loss: 0.0442 | 0.0687
Epoch 267/300, trend Loss: 0.0441 | 0.0686
Epoch 268/300, trend Loss: 0.0441 | 0.0682
Epoch 269/300, trend Loss: 0.0442 | 0.0678
Epoch 270/300, trend Loss: 0.0440 | 0.0681
Epoch 271/300, trend Loss: 0.0438 | 0.0683
Epoch 272/300, trend Loss: 0.0438 | 0.0686
Epoch 273/300, trend Loss: 0.0438 | 0.0690
Epoch 274/300, trend Loss: 0.0437 | 0.0690
Epoch 275/300, trend Loss: 0.0438 | 0.0680
Epoch 276/300, trend Loss: 0.0438 | 0.0684
Epoch 277/300, trend Loss: 0.0436 | 0.0683
Epoch 278/300, trend Loss: 0.0435 | 0.0688
Epoch 279/300, trend Loss: 0.0436 | 0.0690
Epoch 280/300, trend Loss: 0.0433 | 0.0693
Epoch 281/300, trend Loss: 0.0434 | 0.0684
Epoch 282/300, trend Loss: 0.0434 | 0.0685
Epoch 283/300, trend Loss: 0.0433 | 0.0683
Epoch 284/300, trend Loss: 0.0432 | 0.0690
Epoch 285/300, trend Loss: 0.0432 | 0.0689
Epoch 286/300, trend Loss: 0.0431 | 0.0695
Epoch 287/300, trend Loss: 0.0430 | 0.0692
Epoch 288/300, trend Loss: 0.0431 | 0.0688
Epoch 289/300, trend Loss: 0.0431 | 0.0685
Epoch 290/300, trend Loss: 0.0430 | 0.0689
Epoch 291/300, trend Loss: 0.0429 | 0.0690
Epoch 292/300, trend Loss: 0.0429 | 0.0694
Epoch 293/300, trend Loss: 0.0429 | 0.0696
Epoch 294/300, trend Loss: 0.0427 | 0.0694
Epoch 295/300, trend Loss: 0.0428 | 0.0688
Epoch 296/300, trend Loss: 0.0428 | 0.0689
Epoch 297/300, trend Loss: 0.0426 | 0.0691
Epoch 298/300, trend Loss: 0.0425 | 0.0694
Epoch 299/300, trend Loss: 0.0426 | 0.0697
Epoch 300/300, trend Loss: 0.0424 | 0.0698
Training seasonal_0 component with params: {'observation_period_num': 33, 'train_rates': 0.9822162804600922, 'learning_rate': 0.00016220051805170655, 'batch_size': 111, 'step_size': 15, 'gamma': 0.8236270320122207}
Epoch 1/300, seasonal_0 Loss: 0.4740 | 0.2895
Epoch 2/300, seasonal_0 Loss: 0.2654 | 0.2097
Epoch 3/300, seasonal_0 Loss: 0.2022 | 0.1569
Epoch 4/300, seasonal_0 Loss: 0.1770 | 0.1629
Epoch 5/300, seasonal_0 Loss: 0.1676 | 0.1532
Epoch 6/300, seasonal_0 Loss: 0.1636 | 0.1378
Epoch 7/300, seasonal_0 Loss: 0.1676 | 0.1537
Epoch 8/300, seasonal_0 Loss: 0.2126 | 0.1259
Epoch 9/300, seasonal_0 Loss: 0.2251 | 0.5499
Epoch 10/300, seasonal_0 Loss: 0.1934 | 0.1265
Epoch 11/300, seasonal_0 Loss: 0.1638 | 0.1183
Epoch 12/300, seasonal_0 Loss: 0.1506 | 0.1068
Epoch 13/300, seasonal_0 Loss: 0.1364 | 0.1014
Epoch 14/300, seasonal_0 Loss: 0.1315 | 0.0973
Epoch 15/300, seasonal_0 Loss: 0.1231 | 0.0905
Epoch 16/300, seasonal_0 Loss: 0.1160 | 0.0831
Epoch 17/300, seasonal_0 Loss: 0.1119 | 0.0808
Epoch 18/300, seasonal_0 Loss: 0.1099 | 0.0769
Epoch 19/300, seasonal_0 Loss: 0.1083 | 0.0733
Epoch 20/300, seasonal_0 Loss: 0.1066 | 0.0706
Epoch 21/300, seasonal_0 Loss: 0.1048 | 0.0684
Epoch 22/300, seasonal_0 Loss: 0.1031 | 0.0664
Epoch 23/300, seasonal_0 Loss: 0.1016 | 0.0645
Epoch 24/300, seasonal_0 Loss: 0.1000 | 0.0624
Epoch 25/300, seasonal_0 Loss: 0.0988 | 0.0616
Epoch 26/300, seasonal_0 Loss: 0.0977 | 0.0610
Epoch 27/300, seasonal_0 Loss: 0.0970 | 0.0607
Epoch 28/300, seasonal_0 Loss: 0.0970 | 0.0613
Epoch 29/300, seasonal_0 Loss: 0.0983 | 0.0665
Epoch 30/300, seasonal_0 Loss: 0.1011 | 0.0885
Epoch 31/300, seasonal_0 Loss: 0.1002 | 0.1178
Epoch 32/300, seasonal_0 Loss: 0.1007 | 0.0656
Epoch 33/300, seasonal_0 Loss: 0.1052 | 0.0608
Epoch 34/300, seasonal_0 Loss: 0.1094 | 0.0610
Epoch 35/300, seasonal_0 Loss: 0.1043 | 0.0589
Epoch 36/300, seasonal_0 Loss: 0.0980 | 0.0565
Epoch 37/300, seasonal_0 Loss: 0.0953 | 0.0556
Epoch 38/300, seasonal_0 Loss: 0.0948 | 0.0551
Epoch 39/300, seasonal_0 Loss: 0.0934 | 0.0538
Epoch 40/300, seasonal_0 Loss: 0.0915 | 0.0528
Epoch 41/300, seasonal_0 Loss: 0.0921 | 0.0548
Epoch 42/300, seasonal_0 Loss: 0.0916 | 0.0570
Epoch 43/300, seasonal_0 Loss: 0.0897 | 0.0565
Epoch 44/300, seasonal_0 Loss: 0.0881 | 0.0543
Epoch 45/300, seasonal_0 Loss: 0.0871 | 0.0521
Epoch 46/300, seasonal_0 Loss: 0.0869 | 0.0498
Epoch 47/300, seasonal_0 Loss: 0.0878 | 0.0505
Epoch 48/300, seasonal_0 Loss: 0.0903 | 0.0516
Epoch 49/300, seasonal_0 Loss: 0.0943 | 0.0529
Epoch 50/300, seasonal_0 Loss: 0.0982 | 0.0539
Epoch 51/300, seasonal_0 Loss: 0.0985 | 0.0538
Epoch 52/300, seasonal_0 Loss: 0.0932 | 0.0523
Epoch 53/300, seasonal_0 Loss: 0.0878 | 0.0502
Epoch 54/300, seasonal_0 Loss: 0.0968 | 0.0560
Epoch 55/300, seasonal_0 Loss: 0.0968 | 0.0584
Epoch 56/300, seasonal_0 Loss: 0.0888 | 0.0536
Epoch 57/300, seasonal_0 Loss: 0.0838 | 0.0498
Epoch 58/300, seasonal_0 Loss: 0.0836 | 0.0481
Epoch 59/300, seasonal_0 Loss: 0.0837 | 0.0482
Epoch 60/300, seasonal_0 Loss: 0.0829 | 0.0480
Epoch 61/300, seasonal_0 Loss: 0.0820 | 0.0478
Epoch 62/300, seasonal_0 Loss: 0.0817 | 0.0479
Epoch 63/300, seasonal_0 Loss: 0.0815 | 0.0479
Epoch 64/300, seasonal_0 Loss: 0.0813 | 0.0476
Epoch 65/300, seasonal_0 Loss: 0.0810 | 0.0473
Epoch 66/300, seasonal_0 Loss: 0.0808 | 0.0471
Epoch 67/300, seasonal_0 Loss: 0.0806 | 0.0470
Epoch 68/300, seasonal_0 Loss: 0.0804 | 0.0468
Epoch 69/300, seasonal_0 Loss: 0.0802 | 0.0467
Epoch 70/300, seasonal_0 Loss: 0.0800 | 0.0467
Epoch 71/300, seasonal_0 Loss: 0.0798 | 0.0466
Epoch 72/300, seasonal_0 Loss: 0.0796 | 0.0465
Epoch 73/300, seasonal_0 Loss: 0.0795 | 0.0464
Epoch 74/300, seasonal_0 Loss: 0.0793 | 0.0463
Epoch 75/300, seasonal_0 Loss: 0.0792 | 0.0462
Epoch 76/300, seasonal_0 Loss: 0.0790 | 0.0461
Epoch 77/300, seasonal_0 Loss: 0.0788 | 0.0460
Epoch 78/300, seasonal_0 Loss: 0.0787 | 0.0460
Epoch 79/300, seasonal_0 Loss: 0.0786 | 0.0459
Epoch 80/300, seasonal_0 Loss: 0.0784 | 0.0458
Epoch 81/300, seasonal_0 Loss: 0.0783 | 0.0457
Epoch 82/300, seasonal_0 Loss: 0.0782 | 0.0457
Epoch 83/300, seasonal_0 Loss: 0.0781 | 0.0456
Epoch 84/300, seasonal_0 Loss: 0.0779 | 0.0455
Epoch 85/300, seasonal_0 Loss: 0.0778 | 0.0454
Epoch 86/300, seasonal_0 Loss: 0.0777 | 0.0454
Epoch 87/300, seasonal_0 Loss: 0.0776 | 0.0453
Epoch 88/300, seasonal_0 Loss: 0.0775 | 0.0453
Epoch 89/300, seasonal_0 Loss: 0.0774 | 0.0452
Epoch 90/300, seasonal_0 Loss: 0.0772 | 0.0451
Epoch 91/300, seasonal_0 Loss: 0.0771 | 0.0451
Epoch 92/300, seasonal_0 Loss: 0.0770 | 0.0450
Epoch 93/300, seasonal_0 Loss: 0.0769 | 0.0449
Epoch 94/300, seasonal_0 Loss: 0.0768 | 0.0449
Epoch 95/300, seasonal_0 Loss: 0.0768 | 0.0448
Epoch 96/300, seasonal_0 Loss: 0.0767 | 0.0448
Epoch 97/300, seasonal_0 Loss: 0.0766 | 0.0447
Epoch 98/300, seasonal_0 Loss: 0.0765 | 0.0447
Epoch 99/300, seasonal_0 Loss: 0.0764 | 0.0446
Epoch 100/300, seasonal_0 Loss: 0.0763 | 0.0446
Epoch 101/300, seasonal_0 Loss: 0.0762 | 0.0445
Epoch 102/300, seasonal_0 Loss: 0.0761 | 0.0445
Epoch 103/300, seasonal_0 Loss: 0.0761 | 0.0444
Epoch 104/300, seasonal_0 Loss: 0.0760 | 0.0444
Epoch 105/300, seasonal_0 Loss: 0.0759 | 0.0444
Epoch 106/300, seasonal_0 Loss: 0.0758 | 0.0443
Epoch 107/300, seasonal_0 Loss: 0.0757 | 0.0443
Epoch 108/300, seasonal_0 Loss: 0.0757 | 0.0442
Epoch 109/300, seasonal_0 Loss: 0.0756 | 0.0442
Epoch 110/300, seasonal_0 Loss: 0.0756 | 0.0442
Epoch 111/300, seasonal_0 Loss: 0.0755 | 0.0441
Epoch 112/300, seasonal_0 Loss: 0.0754 | 0.0441
Epoch 113/300, seasonal_0 Loss: 0.0754 | 0.0440
Epoch 114/300, seasonal_0 Loss: 0.0753 | 0.0440
Epoch 115/300, seasonal_0 Loss: 0.0752 | 0.0440
Epoch 116/300, seasonal_0 Loss: 0.0752 | 0.0439
Epoch 117/300, seasonal_0 Loss: 0.0751 | 0.0439
Epoch 118/300, seasonal_0 Loss: 0.0751 | 0.0439
Epoch 119/300, seasonal_0 Loss: 0.0750 | 0.0438
Epoch 120/300, seasonal_0 Loss: 0.0749 | 0.0438
Epoch 121/300, seasonal_0 Loss: 0.0749 | 0.0438
Epoch 122/300, seasonal_0 Loss: 0.0748 | 0.0437
Epoch 123/300, seasonal_0 Loss: 0.0748 | 0.0437
Epoch 124/300, seasonal_0 Loss: 0.0747 | 0.0437
Epoch 125/300, seasonal_0 Loss: 0.0747 | 0.0436
Epoch 126/300, seasonal_0 Loss: 0.0746 | 0.0436
Epoch 127/300, seasonal_0 Loss: 0.0746 | 0.0436
Epoch 128/300, seasonal_0 Loss: 0.0745 | 0.0436
Epoch 129/300, seasonal_0 Loss: 0.0745 | 0.0435
Epoch 130/300, seasonal_0 Loss: 0.0744 | 0.0435
Epoch 131/300, seasonal_0 Loss: 0.0744 | 0.0435
Epoch 132/300, seasonal_0 Loss: 0.0744 | 0.0435
Epoch 133/300, seasonal_0 Loss: 0.0743 | 0.0434
Epoch 134/300, seasonal_0 Loss: 0.0743 | 0.0434
Epoch 135/300, seasonal_0 Loss: 0.0742 | 0.0434
Epoch 136/300, seasonal_0 Loss: 0.0742 | 0.0434
Epoch 137/300, seasonal_0 Loss: 0.0742 | 0.0433
Epoch 138/300, seasonal_0 Loss: 0.0741 | 0.0433
Epoch 139/300, seasonal_0 Loss: 0.0741 | 0.0433
Epoch 140/300, seasonal_0 Loss: 0.0741 | 0.0433
Epoch 141/300, seasonal_0 Loss: 0.0740 | 0.0433
Epoch 142/300, seasonal_0 Loss: 0.0740 | 0.0432
Epoch 143/300, seasonal_0 Loss: 0.0740 | 0.0432
Epoch 144/300, seasonal_0 Loss: 0.0739 | 0.0432
Epoch 145/300, seasonal_0 Loss: 0.0739 | 0.0432
Epoch 146/300, seasonal_0 Loss: 0.0739 | 0.0432
Epoch 147/300, seasonal_0 Loss: 0.0738 | 0.0431
Epoch 148/300, seasonal_0 Loss: 0.0738 | 0.0431
Epoch 149/300, seasonal_0 Loss: 0.0738 | 0.0431
Epoch 150/300, seasonal_0 Loss: 0.0737 | 0.0431
Epoch 151/300, seasonal_0 Loss: 0.0737 | 0.0431
Epoch 152/300, seasonal_0 Loss: 0.0737 | 0.0431
Epoch 153/300, seasonal_0 Loss: 0.0737 | 0.0430
Epoch 154/300, seasonal_0 Loss: 0.0736 | 0.0430
Epoch 155/300, seasonal_0 Loss: 0.0736 | 0.0430
Epoch 156/300, seasonal_0 Loss: 0.0736 | 0.0430
Epoch 157/300, seasonal_0 Loss: 0.0736 | 0.0430
Epoch 158/300, seasonal_0 Loss: 0.0735 | 0.0430
Epoch 159/300, seasonal_0 Loss: 0.0735 | 0.0430
Epoch 160/300, seasonal_0 Loss: 0.0735 | 0.0429
Epoch 161/300, seasonal_0 Loss: 0.0735 | 0.0429
Epoch 162/300, seasonal_0 Loss: 0.0734 | 0.0429
Epoch 163/300, seasonal_0 Loss: 0.0734 | 0.0429
Epoch 164/300, seasonal_0 Loss: 0.0734 | 0.0429
Epoch 165/300, seasonal_0 Loss: 0.0734 | 0.0429
Epoch 166/300, seasonal_0 Loss: 0.0734 | 0.0429
Epoch 167/300, seasonal_0 Loss: 0.0733 | 0.0429
Epoch 168/300, seasonal_0 Loss: 0.0733 | 0.0428
Epoch 169/300, seasonal_0 Loss: 0.0733 | 0.0428
Epoch 170/300, seasonal_0 Loss: 0.0733 | 0.0428
Epoch 171/300, seasonal_0 Loss: 0.0733 | 0.0428
Epoch 172/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 173/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 174/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 175/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 176/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 177/300, seasonal_0 Loss: 0.0732 | 0.0428
Epoch 178/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 179/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 180/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 181/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 182/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 183/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 184/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 185/300, seasonal_0 Loss: 0.0731 | 0.0427
Epoch 186/300, seasonal_0 Loss: 0.0730 | 0.0427
Epoch 187/300, seasonal_0 Loss: 0.0730 | 0.0427
Epoch 188/300, seasonal_0 Loss: 0.0730 | 0.0427
Epoch 189/300, seasonal_0 Loss: 0.0730 | 0.0427
Epoch 190/300, seasonal_0 Loss: 0.0730 | 0.0426
Epoch 191/300, seasonal_0 Loss: 0.0730 | 0.0426
Epoch 192/300, seasonal_0 Loss: 0.0730 | 0.0426
Epoch 193/300, seasonal_0 Loss: 0.0730 | 0.0426
Epoch 194/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 195/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 196/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 197/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 198/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 199/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 200/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 201/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 202/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 203/300, seasonal_0 Loss: 0.0729 | 0.0426
Epoch 204/300, seasonal_0 Loss: 0.0728 | 0.0426
Epoch 205/300, seasonal_0 Loss: 0.0728 | 0.0426
Epoch 206/300, seasonal_0 Loss: 0.0728 | 0.0426
Epoch 207/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 208/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 209/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 210/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 211/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 212/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 213/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 214/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 215/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 216/300, seasonal_0 Loss: 0.0728 | 0.0425
Epoch 217/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 218/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 219/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 220/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 221/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 222/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 223/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 224/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 225/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 226/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 227/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 228/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 229/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 230/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 231/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 232/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 233/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 234/300, seasonal_0 Loss: 0.0727 | 0.0425
Epoch 235/300, seasonal_0 Loss: 0.0727 | 0.0424
Epoch 236/300, seasonal_0 Loss: 0.0727 | 0.0424
Epoch 237/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 238/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 239/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 240/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 241/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 242/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 243/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 244/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 245/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 246/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 247/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 248/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 249/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 250/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 251/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 252/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 253/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 254/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 255/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 256/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 257/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 258/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 259/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 260/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 261/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 262/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 263/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 264/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 265/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 266/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 267/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 268/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 269/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 270/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 271/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 272/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 273/300, seasonal_0 Loss: 0.0726 | 0.0424
Epoch 274/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 275/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 276/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 277/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 278/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 279/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 280/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 281/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 282/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 283/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 284/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 285/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 286/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 287/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 288/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 289/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 290/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 291/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 292/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 293/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 294/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 295/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 296/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 297/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 298/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 299/300, seasonal_0 Loss: 0.0725 | 0.0424
Epoch 300/300, seasonal_0 Loss: 0.0725 | 0.0424
Training seasonal_1 component with params: {'observation_period_num': 26, 'train_rates': 0.9864913308507631, 'learning_rate': 0.00029617779690660934, 'batch_size': 52, 'step_size': 2, 'gamma': 0.7653403908163988}
Epoch 1/300, seasonal_1 Loss: 0.2658 | 0.1114
Epoch 2/300, seasonal_1 Loss: 0.1498 | 0.0873
Epoch 3/300, seasonal_1 Loss: 0.1302 | 0.0736
Epoch 4/300, seasonal_1 Loss: 0.1185 | 0.0714
Epoch 5/300, seasonal_1 Loss: 0.1148 | 0.0696
Epoch 6/300, seasonal_1 Loss: 0.1119 | 0.0643
Epoch 7/300, seasonal_1 Loss: 0.1092 | 0.0634
Epoch 8/300, seasonal_1 Loss: 0.1059 | 0.0627
Epoch 9/300, seasonal_1 Loss: 0.1034 | 0.0609
Epoch 10/300, seasonal_1 Loss: 0.1007 | 0.0600
Epoch 11/300, seasonal_1 Loss: 0.0992 | 0.0598
Epoch 12/300, seasonal_1 Loss: 0.0982 | 0.0603
Epoch 13/300, seasonal_1 Loss: 0.0975 | 0.0607
Epoch 14/300, seasonal_1 Loss: 0.0971 | 0.0601
Epoch 15/300, seasonal_1 Loss: 0.0969 | 0.0590
Epoch 16/300, seasonal_1 Loss: 0.0967 | 0.0588
Epoch 17/300, seasonal_1 Loss: 0.0965 | 0.0588
Epoch 18/300, seasonal_1 Loss: 0.0962 | 0.0588
Epoch 19/300, seasonal_1 Loss: 0.0959 | 0.0588
Epoch 20/300, seasonal_1 Loss: 0.0957 | 0.0587
Epoch 21/300, seasonal_1 Loss: 0.0956 | 0.0587
Epoch 22/300, seasonal_1 Loss: 0.0955 | 0.0587
Epoch 23/300, seasonal_1 Loss: 0.0955 | 0.0587
Epoch 24/300, seasonal_1 Loss: 0.0954 | 0.0587
Epoch 25/300, seasonal_1 Loss: 0.0954 | 0.0587
Epoch 26/300, seasonal_1 Loss: 0.0954 | 0.0587
Epoch 27/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 28/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 29/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 30/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 31/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 32/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 33/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 34/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 35/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 36/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 37/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 38/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 39/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 40/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 41/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 42/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 43/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 44/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 45/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 46/300, seasonal_1 Loss: 0.0953 | 0.0587
Epoch 47/300, seasonal_1 Loss: 0.0953 | 0.0587
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.629788923503527, 'learning_rate': 0.00025964304083904096, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8424356064345344}
Epoch 1/300, seasonal_2 Loss: 0.2498 | 0.1942
Epoch 2/300, seasonal_2 Loss: 0.1513 | 0.2377
Epoch 3/300, seasonal_2 Loss: 0.1354 | 0.2588
Epoch 4/300, seasonal_2 Loss: 0.1254 | 0.2328
Epoch 5/300, seasonal_2 Loss: 0.1193 | 0.1520
Epoch 6/300, seasonal_2 Loss: 0.1160 | 0.1145
Epoch 7/300, seasonal_2 Loss: 0.1132 | 0.1021
Epoch 8/300, seasonal_2 Loss: 0.1100 | 0.0966
Epoch 9/300, seasonal_2 Loss: 0.1073 | 0.0905
Epoch 10/300, seasonal_2 Loss: 0.1049 | 0.0883
Epoch 11/300, seasonal_2 Loss: 0.1030 | 0.0865
Epoch 12/300, seasonal_2 Loss: 0.1010 | 0.0839
Epoch 13/300, seasonal_2 Loss: 0.0999 | 0.0793
Epoch 14/300, seasonal_2 Loss: 0.0986 | 0.0769
Epoch 15/300, seasonal_2 Loss: 0.0975 | 0.0746
Epoch 16/300, seasonal_2 Loss: 0.0967 | 0.0732
Epoch 17/300, seasonal_2 Loss: 0.0956 | 0.0725
Epoch 18/300, seasonal_2 Loss: 0.0947 | 0.0718
Epoch 19/300, seasonal_2 Loss: 0.0939 | 0.0710
Epoch 20/300, seasonal_2 Loss: 0.0936 | 0.0706
Epoch 21/300, seasonal_2 Loss: 0.0923 | 0.0703
Epoch 22/300, seasonal_2 Loss: 0.0913 | 0.0694
Epoch 23/300, seasonal_2 Loss: 0.0906 | 0.0690
Epoch 24/300, seasonal_2 Loss: 0.0901 | 0.0688
Epoch 25/300, seasonal_2 Loss: 0.0894 | 0.0683
Epoch 26/300, seasonal_2 Loss: 0.0886 | 0.0678
Epoch 27/300, seasonal_2 Loss: 0.0878 | 0.0666
Epoch 28/300, seasonal_2 Loss: 0.0870 | 0.0660
Epoch 29/300, seasonal_2 Loss: 0.0865 | 0.0648
Epoch 30/300, seasonal_2 Loss: 0.0859 | 0.0649
Epoch 31/300, seasonal_2 Loss: 0.0856 | 0.0639
Epoch 32/300, seasonal_2 Loss: 0.0853 | 0.0649
Epoch 33/300, seasonal_2 Loss: 0.0850 | 0.0637
Epoch 34/300, seasonal_2 Loss: 0.0849 | 0.0648
Epoch 35/300, seasonal_2 Loss: 0.0849 | 0.0639
Epoch 36/300, seasonal_2 Loss: 0.0843 | 0.0675
Epoch 37/300, seasonal_2 Loss: 0.0845 | 0.0664
Epoch 38/300, seasonal_2 Loss: 0.0851 | 0.0837
Epoch 39/300, seasonal_2 Loss: 0.0869 | 0.0659
Epoch 40/300, seasonal_2 Loss: 0.0847 | 0.0712
Epoch 41/300, seasonal_2 Loss: 0.0851 | 0.0691
Epoch 42/300, seasonal_2 Loss: 0.0865 | 0.0753
Epoch 43/300, seasonal_2 Loss: 0.0850 | 0.0652
Epoch 44/300, seasonal_2 Loss: 0.0844 | 0.0664
Epoch 45/300, seasonal_2 Loss: 0.0836 | 0.0646
Epoch 46/300, seasonal_2 Loss: 0.0838 | 0.0653
Epoch 47/300, seasonal_2 Loss: 0.0828 | 0.0628
Epoch 48/300, seasonal_2 Loss: 0.0825 | 0.0631
Epoch 49/300, seasonal_2 Loss: 0.0820 | 0.0625
Epoch 50/300, seasonal_2 Loss: 0.0817 | 0.0621
Epoch 51/300, seasonal_2 Loss: 0.0812 | 0.0617
Epoch 52/300, seasonal_2 Loss: 0.0811 | 0.0618
Epoch 53/300, seasonal_2 Loss: 0.0808 | 0.0616
Epoch 54/300, seasonal_2 Loss: 0.0806 | 0.0614
Epoch 55/300, seasonal_2 Loss: 0.0804 | 0.0612
Epoch 56/300, seasonal_2 Loss: 0.0803 | 0.0612
Epoch 57/300, seasonal_2 Loss: 0.0801 | 0.0610
Epoch 58/300, seasonal_2 Loss: 0.0799 | 0.0610
Epoch 59/300, seasonal_2 Loss: 0.0798 | 0.0609
Epoch 60/300, seasonal_2 Loss: 0.0797 | 0.0609
Epoch 61/300, seasonal_2 Loss: 0.0796 | 0.0608
Epoch 62/300, seasonal_2 Loss: 0.0795 | 0.0608
Epoch 63/300, seasonal_2 Loss: 0.0794 | 0.0607
Epoch 64/300, seasonal_2 Loss: 0.0793 | 0.0607
Epoch 65/300, seasonal_2 Loss: 0.0792 | 0.0607
Epoch 66/300, seasonal_2 Loss: 0.0791 | 0.0607
Epoch 67/300, seasonal_2 Loss: 0.0790 | 0.0606
Epoch 68/300, seasonal_2 Loss: 0.0790 | 0.0606
Epoch 69/300, seasonal_2 Loss: 0.0789 | 0.0606
Epoch 70/300, seasonal_2 Loss: 0.0788 | 0.0606
Epoch 71/300, seasonal_2 Loss: 0.0788 | 0.0606
Epoch 72/300, seasonal_2 Loss: 0.0787 | 0.0605
Epoch 73/300, seasonal_2 Loss: 0.0787 | 0.0605
Epoch 74/300, seasonal_2 Loss: 0.0786 | 0.0605
Epoch 75/300, seasonal_2 Loss: 0.0786 | 0.0605
Epoch 76/300, seasonal_2 Loss: 0.0786 | 0.0605
Epoch 77/300, seasonal_2 Loss: 0.0785 | 0.0604
Epoch 78/300, seasonal_2 Loss: 0.0785 | 0.0605
Epoch 79/300, seasonal_2 Loss: 0.0785 | 0.0604
Epoch 80/300, seasonal_2 Loss: 0.0784 | 0.0604
Epoch 81/300, seasonal_2 Loss: 0.0784 | 0.0604
Epoch 82/300, seasonal_2 Loss: 0.0784 | 0.0604
Epoch 83/300, seasonal_2 Loss: 0.0783 | 0.0604
Epoch 84/300, seasonal_2 Loss: 0.0783 | 0.0604
Epoch 85/300, seasonal_2 Loss: 0.0783 | 0.0604
Epoch 86/300, seasonal_2 Loss: 0.0782 | 0.0604
Epoch 87/300, seasonal_2 Loss: 0.0782 | 0.0604
Epoch 88/300, seasonal_2 Loss: 0.0782 | 0.0604
Epoch 89/300, seasonal_2 Loss: 0.0781 | 0.0604
Epoch 90/300, seasonal_2 Loss: 0.0781 | 0.0604
Epoch 91/300, seasonal_2 Loss: 0.0781 | 0.0603
Epoch 92/300, seasonal_2 Loss: 0.0780 | 0.0603
Epoch 93/300, seasonal_2 Loss: 0.0780 | 0.0603
Epoch 94/300, seasonal_2 Loss: 0.0780 | 0.0603
Epoch 95/300, seasonal_2 Loss: 0.0780 | 0.0603
Epoch 96/300, seasonal_2 Loss: 0.0779 | 0.0603
Epoch 97/300, seasonal_2 Loss: 0.0779 | 0.0603
Epoch 98/300, seasonal_2 Loss: 0.0779 | 0.0603
Epoch 99/300, seasonal_2 Loss: 0.0779 | 0.0603
Epoch 100/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 101/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 102/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 103/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 104/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 105/300, seasonal_2 Loss: 0.0778 | 0.0602
Epoch 106/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 107/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 108/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 109/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 110/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 111/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 112/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 113/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 114/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 115/300, seasonal_2 Loss: 0.0777 | 0.0602
Epoch 116/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 117/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 118/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 119/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 120/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 121/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 122/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 123/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 124/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 125/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 126/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 127/300, seasonal_2 Loss: 0.0776 | 0.0602
Epoch 128/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 129/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 130/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 131/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 132/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 133/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 134/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 135/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 136/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 137/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 138/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 139/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 140/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 141/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 142/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 143/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 144/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 145/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 146/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 147/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 148/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 149/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 150/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 151/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 152/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 153/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 154/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 155/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 156/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 157/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 158/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 159/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 160/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 161/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 162/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 163/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 164/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 165/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 166/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 167/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 168/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 169/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 170/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 171/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 172/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 173/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 174/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 175/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 176/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 177/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 178/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 179/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 180/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 181/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 182/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 183/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 184/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 185/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 186/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 187/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 188/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 189/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 190/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 191/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 192/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 193/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 194/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 195/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 196/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 197/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 198/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 199/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 200/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 201/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 202/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 203/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 204/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 205/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 206/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 207/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 208/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 209/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 210/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 211/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 212/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 213/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 214/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 215/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 216/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 217/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 218/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 219/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 220/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 221/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 222/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 223/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 224/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 225/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 226/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 227/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 228/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 229/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 230/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 231/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 232/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 233/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 234/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 235/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 236/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 237/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 238/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 239/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 240/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 241/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 242/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 243/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 244/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 245/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 246/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 247/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 248/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 249/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 250/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 251/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 252/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 253/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 254/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 255/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 256/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 257/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 258/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 259/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 260/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 261/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 262/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 263/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 264/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 265/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 266/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 267/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 268/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 269/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 270/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 271/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 272/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 273/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 274/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 275/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 276/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 277/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 278/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 279/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 280/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 281/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 282/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 283/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 284/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 285/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 286/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 287/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 288/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 289/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 290/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 291/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 292/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 293/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 294/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 295/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 296/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 297/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 298/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 299/300, seasonal_2 Loss: 0.0775 | 0.0602
Epoch 300/300, seasonal_2 Loss: 0.0775 | 0.0602
Training seasonal_3 component with params: {'observation_period_num': 9, 'train_rates': 0.6386910323643188, 'learning_rate': 0.0009937333419597683, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8357087015044975}
Epoch 1/300, seasonal_3 Loss: 0.2649 | 0.3558
Epoch 2/300, seasonal_3 Loss: 0.1384 | 0.2577
Epoch 3/300, seasonal_3 Loss: 0.1254 | 0.2730
Epoch 4/300, seasonal_3 Loss: 0.1204 | 0.2518
Epoch 5/300, seasonal_3 Loss: 0.1140 | 0.1342
Epoch 6/300, seasonal_3 Loss: 0.1050 | 0.1079
Epoch 7/300, seasonal_3 Loss: 0.1000 | 0.0887
Epoch 8/300, seasonal_3 Loss: 0.0956 | 0.0830
Epoch 9/300, seasonal_3 Loss: 0.0922 | 0.0774
Epoch 10/300, seasonal_3 Loss: 0.0904 | 0.0726
Epoch 11/300, seasonal_3 Loss: 0.0895 | 0.0709
Epoch 12/300, seasonal_3 Loss: 0.0885 | 0.0703
Epoch 13/300, seasonal_3 Loss: 0.0877 | 0.0694
Epoch 14/300, seasonal_3 Loss: 0.0864 | 0.0679
Epoch 15/300, seasonal_3 Loss: 0.0855 | 0.0672
Epoch 16/300, seasonal_3 Loss: 0.0848 | 0.0666
Epoch 17/300, seasonal_3 Loss: 0.0840 | 0.0659
Epoch 18/300, seasonal_3 Loss: 0.0834 | 0.0654
Epoch 19/300, seasonal_3 Loss: 0.0829 | 0.0649
Epoch 20/300, seasonal_3 Loss: 0.0826 | 0.0646
Epoch 21/300, seasonal_3 Loss: 0.0824 | 0.0643
Epoch 22/300, seasonal_3 Loss: 0.0822 | 0.0642
Epoch 23/300, seasonal_3 Loss: 0.0820 | 0.0642
Epoch 24/300, seasonal_3 Loss: 0.0818 | 0.0642
Epoch 25/300, seasonal_3 Loss: 0.0817 | 0.0643
Epoch 26/300, seasonal_3 Loss: 0.0815 | 0.0645
Epoch 27/300, seasonal_3 Loss: 0.0814 | 0.0645
Epoch 28/300, seasonal_3 Loss: 0.0813 | 0.0646
Epoch 29/300, seasonal_3 Loss: 0.0812 | 0.0646
Epoch 30/300, seasonal_3 Loss: 0.0811 | 0.0646
Epoch 31/300, seasonal_3 Loss: 0.0810 | 0.0646
Epoch 32/300, seasonal_3 Loss: 0.0810 | 0.0646
Epoch 33/300, seasonal_3 Loss: 0.0809 | 0.0647
Epoch 34/300, seasonal_3 Loss: 0.0809 | 0.0647
Epoch 35/300, seasonal_3 Loss: 0.0808 | 0.0647
Epoch 36/300, seasonal_3 Loss: 0.0808 | 0.0647
Epoch 37/300, seasonal_3 Loss: 0.0807 | 0.0647
Epoch 38/300, seasonal_3 Loss: 0.0807 | 0.0647
Epoch 39/300, seasonal_3 Loss: 0.0807 | 0.0647
Epoch 40/300, seasonal_3 Loss: 0.0807 | 0.0647
Epoch 41/300, seasonal_3 Loss: 0.0806 | 0.0647
Epoch 42/300, seasonal_3 Loss: 0.0806 | 0.0647
Epoch 43/300, seasonal_3 Loss: 0.0806 | 0.0647
Epoch 44/300, seasonal_3 Loss: 0.0806 | 0.0647
Epoch 45/300, seasonal_3 Loss: 0.0806 | 0.0647
Epoch 46/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 47/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 48/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 49/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 50/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 51/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 52/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 53/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 54/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 55/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 56/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 57/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 58/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 59/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 60/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 61/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 62/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 63/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 64/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 65/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 66/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 67/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 68/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 69/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 70/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 71/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 72/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 73/300, seasonal_3 Loss: 0.0806 | 0.0646
Epoch 74/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 75/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 76/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 77/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 78/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 79/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 80/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 81/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 82/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 83/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 84/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 85/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 86/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 87/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 88/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 89/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 90/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 91/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 92/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 93/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 94/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 95/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 96/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 97/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 98/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 99/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 100/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 101/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 102/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 103/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 104/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 105/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 106/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 107/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 108/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 109/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 110/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 111/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 112/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 113/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 114/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 115/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 116/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 117/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 118/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 119/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 120/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 121/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 122/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 123/300, seasonal_3 Loss: 0.0805 | 0.0646
Epoch 124/300, seasonal_3 Loss: 0.0805 | 0.0646
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 88, 'train_rates': 0.9829063852233081, 'learning_rate': 0.0009459622748322215, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9873215427985044}
Epoch 1/300, resid Loss: 0.2910 | 0.1871
Epoch 2/300, resid Loss: 0.1436 | 0.1104
Epoch 3/300, resid Loss: 0.1247 | 0.1144
Epoch 4/300, resid Loss: 0.1191 | 0.1138
Epoch 5/300, resid Loss: 0.1126 | 0.1151
Epoch 6/300, resid Loss: 0.1063 | 0.0802
Epoch 7/300, resid Loss: 0.0949 | 0.0821
Epoch 8/300, resid Loss: 0.0904 | 0.0906
Epoch 9/300, resid Loss: 0.0859 | 0.0911
Epoch 10/300, resid Loss: 0.0781 | 0.1119
Epoch 11/300, resid Loss: 0.0896 | 0.1010
Epoch 12/300, resid Loss: 0.0834 | 0.1079
Epoch 13/300, resid Loss: 0.0799 | 0.1197
Epoch 14/300, resid Loss: 0.0829 | 0.0936
Epoch 15/300, resid Loss: 0.0813 | 0.0868
Epoch 16/300, resid Loss: 0.0835 | 0.0915
Epoch 17/300, resid Loss: 0.0724 | 0.0940
Epoch 18/300, resid Loss: 0.0717 | 0.1070
Epoch 19/300, resid Loss: 0.0698 | 0.1084
Epoch 20/300, resid Loss: 0.0715 | 0.0868
Epoch 21/300, resid Loss: 0.0699 | 0.1060
Epoch 22/300, resid Loss: 0.0723 | 0.0758
Epoch 23/300, resid Loss: 0.0722 | 0.1018
Epoch 24/300, resid Loss: 0.0760 | 0.0982
Epoch 25/300, resid Loss: 0.0674 | 0.0938
Epoch 26/300, resid Loss: 0.0693 | 0.1143
Epoch 27/300, resid Loss: 0.0715 | 0.1024
Epoch 28/300, resid Loss: 0.0641 | 0.0814
Epoch 29/300, resid Loss: 0.0640 | 0.0929
Epoch 30/300, resid Loss: 0.0622 | 0.0754
Epoch 31/300, resid Loss: 0.0638 | 0.0874
Epoch 32/300, resid Loss: 0.0616 | 0.0965
Epoch 33/300, resid Loss: 0.0602 | 0.0959
Epoch 34/300, resid Loss: 0.0621 | 0.0718
Epoch 35/300, resid Loss: 0.0601 | 0.1023
Epoch 36/300, resid Loss: 0.0593 | 0.1188
Epoch 37/300, resid Loss: 0.0607 | 0.1039
Epoch 38/300, resid Loss: 0.0582 | 0.0739
Epoch 39/300, resid Loss: 0.0584 | 0.0676
Epoch 40/300, resid Loss: 0.0638 | 0.0614
Epoch 41/300, resid Loss: 0.0603 | 0.0657
Epoch 42/300, resid Loss: 0.0561 | 0.0834
Epoch 43/300, resid Loss: 0.0555 | 0.0726
Epoch 44/300, resid Loss: 0.0556 | 0.0799
Epoch 45/300, resid Loss: 0.0544 | 0.1182
Epoch 46/300, resid Loss: 0.0577 | 0.0794
Epoch 47/300, resid Loss: 0.0544 | 0.0854
Epoch 48/300, resid Loss: 0.0518 | 0.0830
Epoch 49/300, resid Loss: 0.0537 | 0.1076
Epoch 50/300, resid Loss: 0.0566 | 0.0924
Epoch 51/300, resid Loss: 0.0535 | 5.2398
Epoch 52/300, resid Loss: 0.0712 | 0.0646
Epoch 53/300, resid Loss: 0.0594 | 0.0718
Epoch 54/300, resid Loss: 0.0582 | 0.0728
Epoch 55/300, resid Loss: 0.0541 | 0.0841
Epoch 56/300, resid Loss: 0.0546 | 0.1012
Epoch 57/300, resid Loss: 0.0523 | 0.1873
Epoch 58/300, resid Loss: 0.0494 | 0.0756
Epoch 59/300, resid Loss: 0.0471 | 0.0684
Epoch 60/300, resid Loss: 0.0528 | 0.0717
Epoch 61/300, resid Loss: 0.0509 | 0.0744
Epoch 62/300, resid Loss: 0.0494 | 0.0827
Epoch 63/300, resid Loss: 0.0520 | 0.0786
Epoch 64/300, resid Loss: 0.0480 | 0.0619
Epoch 65/300, resid Loss: 0.0487 | 0.0779
Epoch 66/300, resid Loss: 0.0484 | 0.0744
Epoch 67/300, resid Loss: 0.0501 | 0.0554
Epoch 68/300, resid Loss: 0.0479 | 0.0697
Epoch 69/300, resid Loss: 0.0478 | 0.0525
Epoch 70/300, resid Loss: 0.0597 | 0.0853
Epoch 71/300, resid Loss: 0.0521 | 0.0720
Epoch 72/300, resid Loss: 0.0485 | 0.0769
Epoch 73/300, resid Loss: 0.0489 | 0.0579
Epoch 74/300, resid Loss: 0.0725 | 0.0697
Epoch 75/300, resid Loss: 0.0796 | 0.0635
Epoch 76/300, resid Loss: 0.0597 | 0.0874
Epoch 77/300, resid Loss: 0.0530 | 0.0911
Epoch 78/300, resid Loss: 0.0520 | 0.0860
Epoch 79/300, resid Loss: 0.0487 | 0.0796
Epoch 80/300, resid Loss: 0.0482 | 0.0786
Epoch 81/300, resid Loss: 0.0486 | 0.0725
Epoch 82/300, resid Loss: 0.0483 | 0.0804
Epoch 83/300, resid Loss: 0.0474 | 0.0747
Epoch 84/300, resid Loss: 0.0477 | 0.0808
Epoch 85/300, resid Loss: 0.0454 | 0.1107
Epoch 86/300, resid Loss: 0.0450 | 0.0996
Epoch 87/300, resid Loss: 0.0448 | 0.1048
Epoch 88/300, resid Loss: 0.0452 | 0.0908
Epoch 89/300, resid Loss: 0.0449 | 0.0822
Epoch 90/300, resid Loss: 0.0422 | 0.0966
Epoch 91/300, resid Loss: 0.0471 | 0.0731
Epoch 92/300, resid Loss: 0.0452 | 0.0742
Epoch 93/300, resid Loss: 0.0468 | 0.0537
Epoch 94/300, resid Loss: 0.0421 | 0.0682
Epoch 95/300, resid Loss: 0.0397 | 0.0850
Epoch 96/300, resid Loss: 0.0429 | 0.0763
Epoch 97/300, resid Loss: 0.0487 | 0.0650
Epoch 98/300, resid Loss: 0.0470 | 0.0604
Epoch 99/300, resid Loss: 0.0480 | 0.0673
Epoch 100/300, resid Loss: 0.0450 | 0.0750
Epoch 101/300, resid Loss: 0.0444 | 0.0796
Epoch 102/300, resid Loss: 0.0596 | 0.0642
Epoch 103/300, resid Loss: 0.0478 | 0.0756
Epoch 104/300, resid Loss: 0.0467 | 0.0694
Epoch 105/300, resid Loss: 0.0434 | 0.0747
Epoch 106/300, resid Loss: 0.0457 | 0.0737
Epoch 107/300, resid Loss: 0.0438 | 0.0746
Epoch 108/300, resid Loss: 0.0454 | 0.0763
Epoch 109/300, resid Loss: 0.0439 | 0.0684
Epoch 110/300, resid Loss: 0.0464 | 0.0708
Epoch 111/300, resid Loss: 0.0412 | 0.0643
Epoch 112/300, resid Loss: 0.0427 | 0.0662
Epoch 113/300, resid Loss: 0.0423 | 0.0660
Epoch 114/300, resid Loss: 0.0410 | 0.0669
Epoch 115/300, resid Loss: 0.0416 | 0.0642
Epoch 116/300, resid Loss: 0.0414 | 0.0687
Epoch 117/300, resid Loss: 0.0484 | 0.0741
Epoch 118/300, resid Loss: 0.0447 | 0.0747
Epoch 119/300, resid Loss: 0.0437 | 0.0745
Epoch 120/300, resid Loss: 0.0432 | 0.0828
Epoch 121/300, resid Loss: 0.0403 | 0.0670
Epoch 122/300, resid Loss: 0.0415 | 0.0565
Epoch 123/300, resid Loss: 0.0398 | 0.0677
Epoch 124/300, resid Loss: 0.0402 | 0.0790
Epoch 125/300, resid Loss: 0.0367 | 0.0759
Epoch 126/300, resid Loss: 0.0415 | 0.0686
Epoch 127/300, resid Loss: 0.0532 | 0.0736
Epoch 128/300, resid Loss: 0.0464 | 0.0654
Epoch 129/300, resid Loss: 0.0435 | 0.0593
Epoch 130/300, resid Loss: 0.0403 | 0.0633
Epoch 131/300, resid Loss: 0.0441 | 0.0644
Epoch 132/300, resid Loss: 0.0424 | 0.0674
Epoch 133/300, resid Loss: 0.0402 | 0.0763
Epoch 134/300, resid Loss: 0.0377 | 0.0614
Epoch 135/300, resid Loss: 0.0364 | 0.0612
Epoch 136/300, resid Loss: 0.0358 | 0.0630
Epoch 137/300, resid Loss: 0.0409 | 0.0602
Epoch 138/300, resid Loss: 0.0423 | 0.0534
Epoch 139/300, resid Loss: 0.0402 | 0.0587
Epoch 140/300, resid Loss: 0.0354 | 0.0668
Epoch 141/300, resid Loss: 0.0417 | 0.0565
Epoch 142/300, resid Loss: 0.0334 | 0.0568
Epoch 143/300, resid Loss: 0.0281 | 0.0579
Epoch 144/300, resid Loss: 0.0380 | 0.0604
Epoch 145/300, resid Loss: 0.0284 | 0.0589
Epoch 146/300, resid Loss: 0.0322 | 0.0690
Epoch 147/300, resid Loss: 0.0399 | 0.0651
Epoch 148/300, resid Loss: 0.0385 | 0.0574
Epoch 149/300, resid Loss: 0.0376 | 0.0617
Epoch 150/300, resid Loss: 0.0373 | 0.0637
Epoch 151/300, resid Loss: 0.0370 | 0.0624
Epoch 152/300, resid Loss: 0.0370 | 0.0659
Epoch 153/300, resid Loss: 0.0367 | 0.0657
Epoch 154/300, resid Loss: 0.0307 | 0.0624
Epoch 155/300, resid Loss: 0.0496 | 0.0683
Epoch 156/300, resid Loss: 0.0429 | 0.0736
Epoch 157/300, resid Loss: 0.0399 | 0.0741
Epoch 158/300, resid Loss: 0.0386 | 0.0730
Epoch 159/300, resid Loss: 0.0379 | 0.0730
Epoch 160/300, resid Loss: 0.0373 | 0.0729
Epoch 161/300, resid Loss: 0.0369 | 0.0724
Epoch 162/300, resid Loss: 0.0367 | 0.0678
Epoch 163/300, resid Loss: 0.0381 | 0.0675
Epoch 164/300, resid Loss: 0.0326 | 0.0676
Epoch 165/300, resid Loss: 0.0376 | 0.0660
Epoch 166/300, resid Loss: 0.0370 | 0.0656
Epoch 167/300, resid Loss: 0.0361 | 0.0668
Epoch 168/300, resid Loss: 0.0323 | 0.0764
Epoch 169/300, resid Loss: 0.0492 | 0.0757
Epoch 170/300, resid Loss: 0.0296 | 0.0700
Epoch 171/300, resid Loss: 0.0227 | 0.0706
Epoch 172/300, resid Loss: 0.0209 | 0.0728
Epoch 173/300, resid Loss: 0.0209 | 0.0752
Epoch 174/300, resid Loss: 0.0194 | 0.0760
Epoch 175/300, resid Loss: 0.0185 | 0.0754
Epoch 176/300, resid Loss: 0.0183 | 0.0725
Epoch 177/300, resid Loss: 0.0180 | 0.0711
Epoch 178/300, resid Loss: 0.0175 | 0.0682
Epoch 179/300, resid Loss: 0.0165 | 0.0672
Epoch 180/300, resid Loss: 0.0158 | 0.0681
Epoch 181/300, resid Loss: 0.0153 | 0.0680
Epoch 182/300, resid Loss: 0.0150 | 0.0708
Epoch 183/300, resid Loss: 0.0148 | 0.0690
Epoch 184/300, resid Loss: 0.0147 | 0.0719
Epoch 185/300, resid Loss: 0.0148 | 0.0761
Epoch 186/300, resid Loss: 0.0154 | 0.0733
Epoch 187/300, resid Loss: 0.0162 | 0.0735
Epoch 188/300, resid Loss: 0.0165 | 0.0673
Epoch 189/300, resid Loss: 0.0173 | 0.0620
Epoch 190/300, resid Loss: 0.0161 | 0.0585
Epoch 191/300, resid Loss: 0.0146 | 0.0629
Epoch 192/300, resid Loss: 0.0142 | 0.0643
Epoch 193/300, resid Loss: 0.0141 | 0.0640
Epoch 194/300, resid Loss: 0.0133 | 0.0657
Epoch 195/300, resid Loss: 0.0136 | 0.0677
Epoch 196/300, resid Loss: 0.0136 | 0.0642
Epoch 197/300, resid Loss: 0.0133 | 0.0710
Epoch 198/300, resid Loss: 0.0137 | 0.0688
Epoch 199/300, resid Loss: 0.0145 | 0.0620
Epoch 200/300, resid Loss: 0.0145 | 0.0658
Epoch 201/300, resid Loss: 0.0138 | 0.0650
Epoch 202/300, resid Loss: 0.0129 | 0.0692
Epoch 203/300, resid Loss: 0.0127 | 0.0632
Epoch 204/300, resid Loss: 0.0126 | 0.0700
Epoch 205/300, resid Loss: 0.0136 | 0.0589
Epoch 206/300, resid Loss: 0.0128 | 0.0657
Epoch 207/300, resid Loss: 0.0132 | 0.0730
Epoch 208/300, resid Loss: 0.0136 | 0.0685
Epoch 209/300, resid Loss: 0.0125 | 0.0672
Epoch 210/300, resid Loss: 0.0125 | 0.0622
Epoch 211/300, resid Loss: 0.0134 | 0.0739
Epoch 212/300, resid Loss: 0.0137 | 0.0704
Epoch 213/300, resid Loss: 0.0140 | 0.0773
Epoch 214/300, resid Loss: 0.0134 | 0.0693
Epoch 215/300, resid Loss: 0.0146 | 0.0581
Epoch 216/300, resid Loss: 0.0116 | 0.0657
Epoch 217/300, resid Loss: 0.0162 | 0.0844
Epoch 218/300, resid Loss: 0.0130 | 0.0706
Epoch 219/300, resid Loss: 0.0224 | 0.0644
Epoch 220/300, resid Loss: 0.0144 | 0.0722
Epoch 221/300, resid Loss: 0.0348 | 0.0648
Epoch 222/300, resid Loss: 0.0376 | 0.0683
Epoch 223/300, resid Loss: 0.0332 | 0.0743
Epoch 224/300, resid Loss: 0.0142 | 0.0847
Epoch 225/300, resid Loss: 0.0152 | 0.0671
Epoch 226/300, resid Loss: 0.0172 | 0.0693
Epoch 227/300, resid Loss: 0.0120 | 0.0623
Epoch 228/300, resid Loss: 0.0117 | 0.0605
Epoch 229/300, resid Loss: 0.0114 | 0.0654
Epoch 230/300, resid Loss: 0.0120 | 0.0811
Epoch 231/300, resid Loss: 0.0112 | 0.0669
Epoch 232/300, resid Loss: 0.0130 | 0.0704
Epoch 233/300, resid Loss: 0.0139 | 0.0653
Epoch 234/300, resid Loss: 0.0114 | 0.0674
Epoch 235/300, resid Loss: 0.0123 | 0.0652
Epoch 236/300, resid Loss: 0.0112 | 0.0668
Epoch 237/300, resid Loss: 0.0110 | 0.0719
Epoch 238/300, resid Loss: 0.0130 | 0.0700
Epoch 239/300, resid Loss: 0.0110 | 0.0697
Epoch 240/300, resid Loss: 0.0123 | 0.0666
Epoch 241/300, resid Loss: 0.0113 | 0.0741
Epoch 242/300, resid Loss: 0.0114 | 0.0869
Epoch 243/300, resid Loss: 0.0113 | 0.0908
Epoch 244/300, resid Loss: 0.0121 | 0.0737
Epoch 245/300, resid Loss: 0.0097 | 0.0772
Epoch 246/300, resid Loss: 0.0093 | 0.0731
Epoch 247/300, resid Loss: 0.0088 | 0.0666
Epoch 248/300, resid Loss: 0.0081 | 0.0661
Epoch 249/300, resid Loss: 0.0078 | 0.0677
Epoch 250/300, resid Loss: 0.0076 | 0.0706
Epoch 251/300, resid Loss: 0.0078 | 0.0698
Epoch 252/300, resid Loss: 0.0081 | 0.0648
Epoch 253/300, resid Loss: 0.0085 | 0.0666
Epoch 254/300, resid Loss: 0.0088 | 0.0680
Epoch 255/300, resid Loss: 0.0093 | 0.0687
Epoch 256/300, resid Loss: 0.0091 | 0.0674
Epoch 257/300, resid Loss: 0.0091 | 0.0643
Epoch 258/300, resid Loss: 0.0092 | 0.0672
Epoch 259/300, resid Loss: 0.0096 | 0.0842
Epoch 260/300, resid Loss: 0.0095 | 0.0770
Epoch 261/300, resid Loss: 0.0092 | 0.0664
Epoch 262/300, resid Loss: 0.0077 | 0.0663
Epoch 263/300, resid Loss: 0.0081 | 0.0653
Epoch 264/300, resid Loss: 0.0078 | 0.0676
Epoch 265/300, resid Loss: 0.0069 | 0.0654
Epoch 266/300, resid Loss: 0.0067 | 0.0646
Epoch 267/300, resid Loss: 0.0066 | 0.0714
Epoch 268/300, resid Loss: 0.0071 | 0.0766
Epoch 269/300, resid Loss: 0.0084 | 0.0697
Epoch 270/300, resid Loss: 0.0082 | 0.0686
Epoch 271/300, resid Loss: 0.0079 | 0.0655
Epoch 272/300, resid Loss: 0.0068 | 0.0947
Epoch 273/300, resid Loss: 0.0065 | 0.1031
Epoch 274/300, resid Loss: 0.0064 | 0.1223
Epoch 275/300, resid Loss: 0.0070 | 0.1104
Epoch 276/300, resid Loss: 0.0070 | 0.0735
Epoch 277/300, resid Loss: 0.0062 | 0.0753
Epoch 278/300, resid Loss: 0.0050 | 0.0755
Epoch 279/300, resid Loss: 0.0050 | 0.0723
Epoch 280/300, resid Loss: 0.0050 | 0.0668
Epoch 281/300, resid Loss: 0.0048 | 0.0670
Epoch 282/300, resid Loss: 0.0048 | 0.0694
Epoch 283/300, resid Loss: 0.0049 | 0.0653
Epoch 284/300, resid Loss: 0.0048 | 0.0684
Epoch 285/300, resid Loss: 0.0048 | 0.0684
Epoch 286/300, resid Loss: 0.0048 | 0.0677
Epoch 287/300, resid Loss: 0.0051 | 0.0824
Epoch 288/300, resid Loss: 0.0057 | 0.0748
Epoch 289/300, resid Loss: 0.0064 | 0.0711
Epoch 290/300, resid Loss: 0.0069 | 0.0880
Epoch 291/300, resid Loss: 0.0062 | 0.1007
Epoch 292/300, resid Loss: 0.0059 | 0.0876
Epoch 293/300, resid Loss: 0.0048 | 0.0787
Epoch 294/300, resid Loss: 0.0044 | 0.0760
Epoch 295/300, resid Loss: 0.0043 | 0.0724
Epoch 296/300, resid Loss: 0.0042 | 0.0725
Epoch 297/300, resid Loss: 0.0044 | 0.0672
Epoch 298/300, resid Loss: 0.0041 | 0.0705
Epoch 299/300, resid Loss: 0.0039 | 0.0697
Epoch 300/300, resid Loss: 0.0039 | 0.0685
Runtime (seconds): 1840.7892796993256
0.0006401750543565856
[100.38455]
[5.2924542]
[-1.0872244]
[0.9881651]
[-4.032689]
[-0.9530008]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.842260469275061
RMSE: 0.9177474975585938
MAE: 0.9177474975585938
R-squared: nan
[100.592255]
