[32m[I 2025-02-03 20:02:11,526][0m A new study created in memory with name: no-name-423a259c-7f2a-40b7-a5e2-dc19d5185937[0m
[32m[I 2025-02-03 20:04:54,201][0m Trial 0 finished with value: 0.11647687980126008 and parameters: {'observation_period_num': 127, 'train_rates': 0.8030747372437153, 'learning_rate': 1.9288818826537315e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8236938465341058}. Best is trial 0 with value: 0.11647687980126008.[0m
[32m[I 2025-02-03 20:05:34,133][0m Trial 1 finished with value: 0.9239060577756154 and parameters: {'observation_period_num': 98, 'train_rates': 0.6491474698349997, 'learning_rate': 1.5051714506078757e-06, 'batch_size': 122, 'step_size': 1, 'gamma': 0.9707633892792401}. Best is trial 0 with value: 0.11647687980126008.[0m
[32m[I 2025-02-03 20:06:01,401][0m Trial 2 finished with value: 0.3506648540496826 and parameters: {'observation_period_num': 65, 'train_rates': 0.9892107381078558, 'learning_rate': 1.0835738208396295e-05, 'batch_size': 253, 'step_size': 2, 'gamma': 0.9735612095067584}. Best is trial 0 with value: 0.11647687980126008.[0m
[32m[I 2025-02-03 20:07:06,520][0m Trial 3 finished with value: 0.21724466985623206 and parameters: {'observation_period_num': 230, 'train_rates': 0.6422555945454212, 'learning_rate': 0.0006273926037162649, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9214297905068574}. Best is trial 0 with value: 0.11647687980126008.[0m
[32m[I 2025-02-03 20:08:17,638][0m Trial 4 finished with value: 0.07694171463901346 and parameters: {'observation_period_num': 26, 'train_rates': 0.962484152940767, 'learning_rate': 1.8827046017473243e-05, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8193705770430704}. Best is trial 4 with value: 0.07694171463901346.[0m
[32m[I 2025-02-03 20:12:43,707][0m Trial 5 finished with value: 0.08537622598270254 and parameters: {'observation_period_num': 58, 'train_rates': 0.9145508175585366, 'learning_rate': 2.2430751863454817e-05, 'batch_size': 21, 'step_size': 12, 'gamma': 0.9849875727096169}. Best is trial 4 with value: 0.07694171463901346.[0m
[32m[I 2025-02-03 20:13:14,745][0m Trial 6 finished with value: 0.2173088788986206 and parameters: {'observation_period_num': 181, 'train_rates': 0.974973302438986, 'learning_rate': 4.3571259672051616e-05, 'batch_size': 206, 'step_size': 11, 'gamma': 0.9230246009887318}. Best is trial 4 with value: 0.07694171463901346.[0m
[32m[I 2025-02-03 20:13:34,481][0m Trial 7 finished with value: 0.7641065954674817 and parameters: {'observation_period_num': 221, 'train_rates': 0.7312971176065876, 'learning_rate': 6.69947872514205e-06, 'batch_size': 253, 'step_size': 7, 'gamma': 0.9432791863190986}. Best is trial 4 with value: 0.07694171463901346.[0m
[32m[I 2025-02-03 20:14:22,973][0m Trial 8 finished with value: 0.23826221844384388 and parameters: {'observation_period_num': 86, 'train_rates': 0.6382986719552269, 'learning_rate': 2.8699980588267775e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.7702274802723843}. Best is trial 4 with value: 0.07694171463901346.[0m
[32m[I 2025-02-03 20:15:03,225][0m Trial 9 finished with value: 0.05350432522505163 and parameters: {'observation_period_num': 53, 'train_rates': 0.9430660136738941, 'learning_rate': 0.0002169897892150269, 'batch_size': 158, 'step_size': 5, 'gamma': 0.9418860438256159}. Best is trial 9 with value: 0.05350432522505163.[0m
[32m[I 2025-02-03 20:15:37,794][0m Trial 10 finished with value: 0.1181813611559672 and parameters: {'observation_period_num': 153, 'train_rates': 0.8698289192532442, 'learning_rate': 0.00029981287711029173, 'batch_size': 169, 'step_size': 4, 'gamma': 0.8818630634947597}. Best is trial 9 with value: 0.05350432522505163.[0m
[32m[I 2025-02-03 20:16:16,378][0m Trial 11 finished with value: 0.04855907802172259 and parameters: {'observation_period_num': 19, 'train_rates': 0.8998687063730512, 'learning_rate': 0.00011851261119446364, 'batch_size': 160, 'step_size': 15, 'gamma': 0.8266915609835436}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:16:54,040][0m Trial 12 finished with value: 0.06390763134523123 and parameters: {'observation_period_num': 11, 'train_rates': 0.8531853885663594, 'learning_rate': 0.00012631051053236863, 'batch_size': 166, 'step_size': 5, 'gamma': 0.8677783914496802}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:17:32,528][0m Trial 13 finished with value: 0.050726876399963566 and parameters: {'observation_period_num': 38, 'train_rates': 0.9030711082597819, 'learning_rate': 0.00011828088568075017, 'batch_size': 158, 'step_size': 9, 'gamma': 0.8581975590713102}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:18:03,217][0m Trial 14 finished with value: 0.06112947278498542 and parameters: {'observation_period_num': 12, 'train_rates': 0.885428369901965, 'learning_rate': 7.714826263044743e-05, 'batch_size': 201, 'step_size': 9, 'gamma': 0.8296411343885831}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:18:42,683][0m Trial 15 finished with value: 0.10391179099678993 and parameters: {'observation_period_num': 109, 'train_rates': 0.8072655782567957, 'learning_rate': 0.0008113674962029055, 'batch_size': 137, 'step_size': 13, 'gamma': 0.76878979304646}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:19:10,064][0m Trial 16 finished with value: 0.11877050722131924 and parameters: {'observation_period_num': 43, 'train_rates': 0.7473443770377084, 'learning_rate': 8.549878351019967e-05, 'batch_size': 200, 'step_size': 8, 'gamma': 0.864079387631761}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:19:53,597][0m Trial 17 finished with value: 0.05849194959286721 and parameters: {'observation_period_num': 75, 'train_rates': 0.8384587926354512, 'learning_rate': 0.0003244880571803219, 'batch_size': 128, 'step_size': 15, 'gamma': 0.7888877072818252}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:20:30,579][0m Trial 18 finished with value: 0.3650000304915011 and parameters: {'observation_period_num': 28, 'train_rates': 0.9124862832895057, 'learning_rate': 3.769205740103784e-06, 'batch_size': 175, 'step_size': 12, 'gamma': 0.8464915514624952}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:20:55,161][0m Trial 19 finished with value: 0.12374402777388932 and parameters: {'observation_period_num': 161, 'train_rates': 0.7562737464612627, 'learning_rate': 0.00015489155023963496, 'batch_size': 217, 'step_size': 10, 'gamma': 0.8925512840688248}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:21:51,898][0m Trial 20 finished with value: 0.0994058833880858 and parameters: {'observation_period_num': 37, 'train_rates': 0.920868075443472, 'learning_rate': 6.290116233561869e-05, 'batch_size': 104, 'step_size': 3, 'gamma': 0.7938852084631612}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:22:33,563][0m Trial 21 finished with value: 0.050257496535778046 and parameters: {'observation_period_num': 49, 'train_rates': 0.9498888627449571, 'learning_rate': 0.00024634241542395384, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8945665183358734}. Best is trial 11 with value: 0.04855907802172259.[0m
[32m[I 2025-02-03 20:23:13,925][0m Trial 22 finished with value: 0.03495299065513397 and parameters: {'observation_period_num': 5, 'train_rates': 0.8866607391777377, 'learning_rate': 0.0004552227646548977, 'batch_size': 150, 'step_size': 7, 'gamma': 0.9020768219051417}. Best is trial 22 with value: 0.03495299065513397.[0m
[32m[I 2025-02-03 20:23:57,852][0m Trial 23 finished with value: 0.031910482122272744 and parameters: {'observation_period_num': 11, 'train_rates': 0.9381104267253123, 'learning_rate': 0.0005377133119142419, 'batch_size': 141, 'step_size': 7, 'gamma': 0.8968621897631455}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:24:48,420][0m Trial 24 finished with value: 0.033442783980223975 and parameters: {'observation_period_num': 9, 'train_rates': 0.8375571488209486, 'learning_rate': 0.00047573040302648014, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9054707576718205}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:26:12,159][0m Trial 25 finished with value: 0.03743439575657249 and parameters: {'observation_period_num': 11, 'train_rates': 0.8301135798031046, 'learning_rate': 0.0005037852081780386, 'batch_size': 65, 'step_size': 7, 'gamma': 0.9174963700927894}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:26:59,358][0m Trial 26 finished with value: 0.04927947994188538 and parameters: {'observation_period_num': 7, 'train_rates': 0.7721101194350125, 'learning_rate': 0.0009752668643059184, 'batch_size': 114, 'step_size': 8, 'gamma': 0.907828741413865}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:28:00,764][0m Trial 27 finished with value: 0.16761226853514466 and parameters: {'observation_period_num': 75, 'train_rates': 0.6901715269644519, 'learning_rate': 0.00034198003595185746, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9412288868823825}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:28:41,548][0m Trial 28 finished with value: 0.12220578527628245 and parameters: {'observation_period_num': 197, 'train_rates': 0.8661541464499682, 'learning_rate': 0.000495344979522562, 'batch_size': 139, 'step_size': 4, 'gamma': 0.8847273121992775}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:30:15,183][0m Trial 29 finished with value: 0.12850285796727057 and parameters: {'observation_period_num': 126, 'train_rates': 0.7872325061757492, 'learning_rate': 0.0004880102818032545, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9012931372267012}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:30:45,544][0m Trial 30 finished with value: 0.1735561458976722 and parameters: {'observation_period_num': 247, 'train_rates': 0.8194955420212762, 'learning_rate': 0.00018956345020772826, 'batch_size': 179, 'step_size': 9, 'gamma': 0.957493312425631}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:32:52,256][0m Trial 31 finished with value: 0.042432555590163575 and parameters: {'observation_period_num': 8, 'train_rates': 0.8393930299233466, 'learning_rate': 0.0004938455564303343, 'batch_size': 43, 'step_size': 8, 'gamma': 0.9117575577674362}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:33:34,160][0m Trial 32 finished with value: 0.1557894573130789 and parameters: {'observation_period_num': 31, 'train_rates': 0.604099329794689, 'learning_rate': 0.000728125754818892, 'batch_size': 114, 'step_size': 7, 'gamma': 0.9232395354270406}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:34:43,812][0m Trial 33 finished with value: 0.9161946890671129 and parameters: {'observation_period_num': 23, 'train_rates': 0.822929257243431, 'learning_rate': 1.182379591975845e-06, 'batch_size': 78, 'step_size': 5, 'gamma': 0.8768836094158788}. Best is trial 23 with value: 0.031910482122272744.[0m
[32m[I 2025-02-03 20:35:30,867][0m Trial 34 finished with value: 0.030820159422499792 and parameters: {'observation_period_num': 6, 'train_rates': 0.8814635348936289, 'learning_rate': 0.0004361027607373599, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9341206624544935}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:36:15,029][0m Trial 35 finished with value: 0.07975176138722379 and parameters: {'observation_period_num': 71, 'train_rates': 0.9363527213297407, 'learning_rate': 0.00040822836305914206, 'batch_size': 140, 'step_size': 10, 'gamma': 0.9588441610920111}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:37:02,658][0m Trial 36 finished with value: 0.07789637497345726 and parameters: {'observation_period_num': 97, 'train_rates': 0.8838959655535661, 'learning_rate': 0.0009849555133195674, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9342834770022891}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:38:04,519][0m Trial 37 finished with value: 0.0758703202009201 and parameters: {'observation_period_num': 59, 'train_rates': 0.975989515936895, 'learning_rate': 0.0006190054110450406, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9632228815669509}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:38:54,312][0m Trial 38 finished with value: 0.04627175145031173 and parameters: {'observation_period_num': 23, 'train_rates': 0.8588113960578521, 'learning_rate': 0.00024284811103351055, 'batch_size': 116, 'step_size': 8, 'gamma': 0.8452547450250342}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:39:27,792][0m Trial 39 finished with value: 0.13523413052021618 and parameters: {'observation_period_num': 42, 'train_rates': 0.9271150564668924, 'learning_rate': 1.3161247074998812e-05, 'batch_size': 186, 'step_size': 4, 'gamma': 0.9777555402293654}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:40:09,023][0m Trial 40 finished with value: 0.05742381707481716 and parameters: {'observation_period_num': 26, 'train_rates': 0.8823855963558562, 'learning_rate': 4.5763476641240164e-05, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9307377843292387}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:41:33,255][0m Trial 41 finished with value: 0.03269063140885226 and parameters: {'observation_period_num': 10, 'train_rates': 0.8376063698210254, 'learning_rate': 0.0005957868879923521, 'batch_size': 65, 'step_size': 7, 'gamma': 0.9168750954869176}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:45:07,064][0m Trial 42 finished with value: 0.04926595286062659 and parameters: {'observation_period_num': 18, 'train_rates': 0.7933802076946852, 'learning_rate': 0.0006984530937849125, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9026194178231096}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:46:16,463][0m Trial 43 finished with value: 0.03654196162231907 and parameters: {'observation_period_num': 6, 'train_rates': 0.850097735381146, 'learning_rate': 0.0003541792015931452, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9491630894327099}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:47:14,485][0m Trial 44 finished with value: 0.052980112593348436 and parameters: {'observation_period_num': 53, 'train_rates': 0.957621657038197, 'learning_rate': 0.00021201371406222953, 'batch_size': 105, 'step_size': 8, 'gamma': 0.9303510583357238}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:48:18,687][0m Trial 45 finished with value: 0.05722242768349821 and parameters: {'observation_period_num': 35, 'train_rates': 0.8963135834657916, 'learning_rate': 0.0005816393344312176, 'batch_size': 91, 'step_size': 5, 'gamma': 0.9164381748735487}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:50:39,089][0m Trial 46 finished with value: 0.0911436626097051 and parameters: {'observation_period_num': 5, 'train_rates': 0.8750511941498292, 'learning_rate': 2.307258403384366e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.8879257673369729}. Best is trial 34 with value: 0.030820159422499792.[0m
[32m[I 2025-02-03 20:52:17,118][0m Trial 47 finished with value: 0.029031777754426003 and parameters: {'observation_period_num': 17, 'train_rates': 0.9893981145214713, 'learning_rate': 0.0001662391874420177, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8734808858160072}. Best is trial 47 with value: 0.029031777754426003.[0m
[32m[I 2025-02-03 20:53:51,908][0m Trial 48 finished with value: 0.03228132054209709 and parameters: {'observation_period_num': 21, 'train_rates': 0.9887540123755272, 'learning_rate': 0.00015670320845685067, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8746152178435133}. Best is trial 47 with value: 0.029031777754426003.[0m
[32m[I 2025-02-03 20:55:29,168][0m Trial 49 finished with value: 0.07197624444961548 and parameters: {'observation_period_num': 62, 'train_rates': 0.9799658355628037, 'learning_rate': 0.00014131587895646925, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8498532246909651}. Best is trial 47 with value: 0.029031777754426003.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2629 | 0.1728
Epoch 2/300, Loss: 0.1341 | 0.1468
Epoch 3/300, Loss: 0.1123 | 0.1129
Epoch 4/300, Loss: 0.0985 | 0.0964
Epoch 5/300, Loss: 0.0935 | 0.0909
Epoch 6/300, Loss: 0.0886 | 0.0862
Epoch 7/300, Loss: 0.0814 | 0.0820
Epoch 8/300, Loss: 0.0768 | 0.0835
Epoch 9/300, Loss: 0.0736 | 0.0790
Epoch 10/300, Loss: 0.0710 | 0.0762
Epoch 11/300, Loss: 0.0702 | 0.0740
Epoch 12/300, Loss: 0.0690 | 0.0730
Epoch 13/300, Loss: 0.0679 | 0.0718
Epoch 14/300, Loss: 0.0673 | 0.0704
Epoch 15/300, Loss: 0.0652 | 0.0685
Epoch 16/300, Loss: 0.0634 | 0.0669
Epoch 17/300, Loss: 0.0625 | 0.0654
Epoch 18/300, Loss: 0.0616 | 0.0644
Epoch 19/300, Loss: 0.0605 | 0.0629
Epoch 20/300, Loss: 0.0597 | 0.0622
Epoch 21/300, Loss: 0.0584 | 0.0616
Epoch 22/300, Loss: 0.0569 | 0.0626
Epoch 23/300, Loss: 0.0559 | 0.0614
Epoch 24/300, Loss: 0.0535 | 0.0597
Epoch 25/300, Loss: 0.0520 | 0.0580
Epoch 26/300, Loss: 0.0510 | 0.0580
Epoch 27/300, Loss: 0.0505 | 0.0567
Epoch 28/300, Loss: 0.0497 | 0.0559
Epoch 29/300, Loss: 0.0492 | 0.0582
Epoch 30/300, Loss: 0.0484 | 0.0588
Epoch 31/300, Loss: 0.0482 | 0.0609
Epoch 32/300, Loss: 0.0481 | 0.0648
Epoch 33/300, Loss: 0.0485 | 0.0691
Epoch 34/300, Loss: 0.0510 | 0.0646
Epoch 35/300, Loss: 0.0575 | 0.0653
Epoch 36/300, Loss: 0.0613 | 0.0730
Epoch 37/300, Loss: 0.0586 | 0.0613
Epoch 38/300, Loss: 0.0520 | 0.0580
Epoch 39/300, Loss: 0.0478 | 0.0544
Epoch 40/300, Loss: 0.0445 | 0.0507
Epoch 41/300, Loss: 0.0431 | 0.0496
Epoch 42/300, Loss: 0.0438 | 0.0485
Epoch 43/300, Loss: 0.0457 | 0.0509
Epoch 44/300, Loss: 0.0481 | 0.0530
Epoch 45/300, Loss: 0.0502 | 0.0575
Epoch 46/300, Loss: 0.0515 | 0.0633
Epoch 47/300, Loss: 0.0512 | 0.0743
Epoch 48/300, Loss: 0.0477 | 0.0623
Epoch 49/300, Loss: 0.0427 | 0.0522
Epoch 50/300, Loss: 0.0399 | 0.0467
Epoch 51/300, Loss: 0.0388 | 0.0450
Epoch 52/300, Loss: 0.0384 | 0.0445
Epoch 53/300, Loss: 0.0382 | 0.0442
Epoch 54/300, Loss: 0.0380 | 0.0441
Epoch 55/300, Loss: 0.0378 | 0.0440
Epoch 56/300, Loss: 0.0376 | 0.0439
Epoch 57/300, Loss: 0.0374 | 0.0438
Epoch 58/300, Loss: 0.0372 | 0.0436
Epoch 59/300, Loss: 0.0370 | 0.0435
Epoch 60/300, Loss: 0.0369 | 0.0434
Epoch 61/300, Loss: 0.0368 | 0.0433
Epoch 62/300, Loss: 0.0366 | 0.0432
Epoch 63/300, Loss: 0.0365 | 0.0431
Epoch 64/300, Loss: 0.0364 | 0.0430
Epoch 65/300, Loss: 0.0363 | 0.0430
Epoch 66/300, Loss: 0.0361 | 0.0429
Epoch 67/300, Loss: 0.0360 | 0.0428
Epoch 68/300, Loss: 0.0359 | 0.0428
Epoch 69/300, Loss: 0.0358 | 0.0427
Epoch 70/300, Loss: 0.0357 | 0.0427
Epoch 71/300, Loss: 0.0357 | 0.0426
Epoch 72/300, Loss: 0.0356 | 0.0426
Epoch 73/300, Loss: 0.0355 | 0.0426
Epoch 74/300, Loss: 0.0354 | 0.0425
Epoch 75/300, Loss: 0.0353 | 0.0425
Epoch 76/300, Loss: 0.0353 | 0.0424
Epoch 77/300, Loss: 0.0352 | 0.0424
Epoch 78/300, Loss: 0.0351 | 0.0424
Epoch 79/300, Loss: 0.0351 | 0.0423
Epoch 80/300, Loss: 0.0350 | 0.0423
Epoch 81/300, Loss: 0.0349 | 0.0423
Epoch 82/300, Loss: 0.0349 | 0.0422
Epoch 83/300, Loss: 0.0348 | 0.0422
Epoch 84/300, Loss: 0.0347 | 0.0421
Epoch 85/300, Loss: 0.0347 | 0.0421
Epoch 86/300, Loss: 0.0346 | 0.0420
Epoch 87/300, Loss: 0.0346 | 0.0420
Epoch 88/300, Loss: 0.0345 | 0.0420
Epoch 89/300, Loss: 0.0345 | 0.0419
Epoch 90/300, Loss: 0.0344 | 0.0419
Epoch 91/300, Loss: 0.0344 | 0.0419
Epoch 92/300, Loss: 0.0343 | 0.0418
Epoch 93/300, Loss: 0.0343 | 0.0418
Epoch 94/300, Loss: 0.0343 | 0.0417
Epoch 95/300, Loss: 0.0342 | 0.0417
Epoch 96/300, Loss: 0.0342 | 0.0416
Epoch 97/300, Loss: 0.0341 | 0.0416
Epoch 98/300, Loss: 0.0341 | 0.0416
Epoch 99/300, Loss: 0.0341 | 0.0415
Epoch 100/300, Loss: 0.0340 | 0.0415
Epoch 101/300, Loss: 0.0340 | 0.0415
Epoch 102/300, Loss: 0.0340 | 0.0415
Epoch 103/300, Loss: 0.0339 | 0.0414
Epoch 104/300, Loss: 0.0339 | 0.0414
Epoch 105/300, Loss: 0.0339 | 0.0414
Epoch 106/300, Loss: 0.0338 | 0.0414
Epoch 107/300, Loss: 0.0338 | 0.0413
Epoch 108/300, Loss: 0.0338 | 0.0413
Epoch 109/300, Loss: 0.0337 | 0.0413
Epoch 110/300, Loss: 0.0337 | 0.0413
Epoch 111/300, Loss: 0.0337 | 0.0413
Epoch 112/300, Loss: 0.0336 | 0.0413
Epoch 113/300, Loss: 0.0336 | 0.0413
Epoch 114/300, Loss: 0.0336 | 0.0412
Epoch 115/300, Loss: 0.0336 | 0.0412
Epoch 116/300, Loss: 0.0335 | 0.0412
Epoch 117/300, Loss: 0.0335 | 0.0412
Epoch 118/300, Loss: 0.0335 | 0.0412
Epoch 119/300, Loss: 0.0335 | 0.0412
Epoch 120/300, Loss: 0.0334 | 0.0412
Epoch 121/300, Loss: 0.0334 | 0.0411
Epoch 122/300, Loss: 0.0334 | 0.0411
Epoch 123/300, Loss: 0.0334 | 0.0411
Epoch 124/300, Loss: 0.0334 | 0.0411
Epoch 125/300, Loss: 0.0334 | 0.0411
Epoch 126/300, Loss: 0.0333 | 0.0411
Epoch 127/300, Loss: 0.0333 | 0.0411
Epoch 128/300, Loss: 0.0333 | 0.0410
Epoch 129/300, Loss: 0.0333 | 0.0410
Epoch 130/300, Loss: 0.0333 | 0.0410
Epoch 131/300, Loss: 0.0333 | 0.0410
Epoch 132/300, Loss: 0.0332 | 0.0410
Epoch 133/300, Loss: 0.0332 | 0.0410
Epoch 134/300, Loss: 0.0332 | 0.0410
Epoch 135/300, Loss: 0.0332 | 0.0410
Epoch 136/300, Loss: 0.0332 | 0.0410
Epoch 137/300, Loss: 0.0332 | 0.0410
Epoch 138/300, Loss: 0.0332 | 0.0410
Epoch 139/300, Loss: 0.0332 | 0.0409
Epoch 140/300, Loss: 0.0332 | 0.0409
Epoch 141/300, Loss: 0.0331 | 0.0409
Epoch 142/300, Loss: 0.0331 | 0.0409
Epoch 143/300, Loss: 0.0331 | 0.0409
Epoch 144/300, Loss: 0.0331 | 0.0409
Epoch 145/300, Loss: 0.0331 | 0.0409
Epoch 146/300, Loss: 0.0331 | 0.0409
Epoch 147/300, Loss: 0.0331 | 0.0409
Epoch 148/300, Loss: 0.0331 | 0.0409
Epoch 149/300, Loss: 0.0331 | 0.0409
Epoch 150/300, Loss: 0.0331 | 0.0409
Epoch 151/300, Loss: 0.0331 | 0.0409
Epoch 152/300, Loss: 0.0331 | 0.0409
Epoch 153/300, Loss: 0.0331 | 0.0409
Epoch 154/300, Loss: 0.0331 | 0.0409
Epoch 155/300, Loss: 0.0330 | 0.0409
Epoch 156/300, Loss: 0.0330 | 0.0408
Epoch 157/300, Loss: 0.0330 | 0.0408
Epoch 158/300, Loss: 0.0330 | 0.0408
Epoch 159/300, Loss: 0.0330 | 0.0408
Epoch 160/300, Loss: 0.0330 | 0.0408
Epoch 161/300, Loss: 0.0330 | 0.0408
Epoch 162/300, Loss: 0.0330 | 0.0408
Epoch 163/300, Loss: 0.0330 | 0.0408
Epoch 164/300, Loss: 0.0330 | 0.0408
Epoch 165/300, Loss: 0.0330 | 0.0408
Epoch 166/300, Loss: 0.0330 | 0.0408
Epoch 167/300, Loss: 0.0330 | 0.0408
Epoch 168/300, Loss: 0.0330 | 0.0408
Epoch 169/300, Loss: 0.0330 | 0.0408
Epoch 170/300, Loss: 0.0330 | 0.0408
Epoch 171/300, Loss: 0.0330 | 0.0408
Epoch 172/300, Loss: 0.0330 | 0.0408
Epoch 173/300, Loss: 0.0330 | 0.0408
Epoch 174/300, Loss: 0.0330 | 0.0408
Epoch 175/300, Loss: 0.0330 | 0.0408
Epoch 176/300, Loss: 0.0330 | 0.0408
Epoch 177/300, Loss: 0.0330 | 0.0408
Epoch 178/300, Loss: 0.0330 | 0.0408
Epoch 179/300, Loss: 0.0330 | 0.0408
Epoch 180/300, Loss: 0.0330 | 0.0408
Epoch 181/300, Loss: 0.0330 | 0.0408
Epoch 182/300, Loss: 0.0329 | 0.0408
Epoch 183/300, Loss: 0.0329 | 0.0408
Epoch 184/300, Loss: 0.0329 | 0.0408
Epoch 185/300, Loss: 0.0329 | 0.0408
Epoch 186/300, Loss: 0.0329 | 0.0408
Epoch 187/300, Loss: 0.0329 | 0.0408
Epoch 188/300, Loss: 0.0329 | 0.0408
Epoch 189/300, Loss: 0.0329 | 0.0408
Epoch 190/300, Loss: 0.0329 | 0.0408
Epoch 191/300, Loss: 0.0329 | 0.0408
Epoch 192/300, Loss: 0.0329 | 0.0408
Epoch 193/300, Loss: 0.0329 | 0.0408
Epoch 194/300, Loss: 0.0329 | 0.0408
Epoch 195/300, Loss: 0.0329 | 0.0408
Epoch 196/300, Loss: 0.0329 | 0.0408
Epoch 197/300, Loss: 0.0329 | 0.0408
Epoch 198/300, Loss: 0.0329 | 0.0408
Epoch 199/300, Loss: 0.0329 | 0.0408
Epoch 200/300, Loss: 0.0329 | 0.0408
Epoch 201/300, Loss: 0.0329 | 0.0408
Epoch 202/300, Loss: 0.0329 | 0.0407
Epoch 203/300, Loss: 0.0329 | 0.0407
Epoch 204/300, Loss: 0.0329 | 0.0407
Epoch 205/300, Loss: 0.0329 | 0.0407
Epoch 206/300, Loss: 0.0329 | 0.0407
Epoch 207/300, Loss: 0.0329 | 0.0407
Epoch 208/300, Loss: 0.0329 | 0.0407
Epoch 209/300, Loss: 0.0329 | 0.0407
Epoch 210/300, Loss: 0.0329 | 0.0407
Epoch 211/300, Loss: 0.0329 | 0.0407
Epoch 212/300, Loss: 0.0329 | 0.0407
Epoch 213/300, Loss: 0.0329 | 0.0407
Epoch 214/300, Loss: 0.0329 | 0.0407
Epoch 215/300, Loss: 0.0329 | 0.0407
Epoch 216/300, Loss: 0.0329 | 0.0407
Epoch 217/300, Loss: 0.0329 | 0.0407
Epoch 218/300, Loss: 0.0329 | 0.0407
Epoch 219/300, Loss: 0.0329 | 0.0407
Epoch 220/300, Loss: 0.0329 | 0.0407
Epoch 221/300, Loss: 0.0329 | 0.0407
Epoch 222/300, Loss: 0.0329 | 0.0407
Epoch 223/300, Loss: 0.0329 | 0.0407
Epoch 224/300, Loss: 0.0329 | 0.0407
Epoch 225/300, Loss: 0.0329 | 0.0407
Epoch 226/300, Loss: 0.0329 | 0.0407
Epoch 227/300, Loss: 0.0329 | 0.0407
Epoch 228/300, Loss: 0.0329 | 0.0407
Epoch 229/300, Loss: 0.0329 | 0.0407
Epoch 230/300, Loss: 0.0329 | 0.0407
Epoch 231/300, Loss: 0.0329 | 0.0407
Epoch 232/300, Loss: 0.0329 | 0.0407
Epoch 233/300, Loss: 0.0329 | 0.0407
Epoch 234/300, Loss: 0.0329 | 0.0407
Epoch 235/300, Loss: 0.0329 | 0.0407
Epoch 236/300, Loss: 0.0329 | 0.0407
Epoch 237/300, Loss: 0.0329 | 0.0407
Epoch 238/300, Loss: 0.0329 | 0.0407
Epoch 239/300, Loss: 0.0329 | 0.0407
Epoch 240/300, Loss: 0.0329 | 0.0407
Epoch 241/300, Loss: 0.0329 | 0.0407
Epoch 242/300, Loss: 0.0329 | 0.0407
Epoch 243/300, Loss: 0.0329 | 0.0407
Epoch 244/300, Loss: 0.0329 | 0.0407
Epoch 245/300, Loss: 0.0329 | 0.0407
Epoch 246/300, Loss: 0.0329 | 0.0407
Epoch 247/300, Loss: 0.0329 | 0.0407
Epoch 248/300, Loss: 0.0329 | 0.0407
Epoch 249/300, Loss: 0.0329 | 0.0407
Epoch 250/300, Loss: 0.0329 | 0.0407
Epoch 251/300, Loss: 0.0329 | 0.0407
Epoch 252/300, Loss: 0.0329 | 0.0407
Epoch 253/300, Loss: 0.0329 | 0.0407
Epoch 254/300, Loss: 0.0329 | 0.0407
Epoch 255/300, Loss: 0.0329 | 0.0407
Epoch 256/300, Loss: 0.0329 | 0.0407
Epoch 257/300, Loss: 0.0329 | 0.0407
Epoch 258/300, Loss: 0.0329 | 0.0407
Epoch 259/300, Loss: 0.0329 | 0.0407
Epoch 260/300, Loss: 0.0329 | 0.0407
Epoch 261/300, Loss: 0.0329 | 0.0407
Epoch 262/300, Loss: 0.0329 | 0.0407
Epoch 263/300, Loss: 0.0329 | 0.0407
Epoch 264/300, Loss: 0.0329 | 0.0407
Epoch 265/300, Loss: 0.0329 | 0.0407
Epoch 266/300, Loss: 0.0329 | 0.0407
Epoch 267/300, Loss: 0.0329 | 0.0407
Epoch 268/300, Loss: 0.0329 | 0.0407
Epoch 269/300, Loss: 0.0329 | 0.0407
Early stopping
Runtime (seconds): 263.3040645122528
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 2.8216320346691646
RMSE: 1.6797714233398438
MAE: 1.6797714233398438
R-squared: nan
[119.64977]
