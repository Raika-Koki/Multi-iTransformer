ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-05 04:47:54,103][0m A new study created in memory with name: no-name-74eb87b8-44c5-4176-bd05-04ef12a30ca7[0m
[32m[I 2025-02-05 04:48:22,182][0m Trial 0 finished with value: 0.4201387335822851 and parameters: {'observation_period_num': 45, 'train_rates': 0.7985585128160465, 'learning_rate': 5.3337220681122305e-06, 'batch_size': 198, 'step_size': 7, 'gamma': 0.9388657929889166}. Best is trial 0 with value: 0.4201387335822851.[0m
[32m[I 2025-02-05 04:48:56,287][0m Trial 1 finished with value: 0.6057896150720826 and parameters: {'observation_period_num': 50, 'train_rates': 0.6162507600158774, 'learning_rate': 1.917686003209422e-06, 'batch_size': 139, 'step_size': 5, 'gamma': 0.9211359890557422}. Best is trial 0 with value: 0.4201387335822851.[0m
[32m[I 2025-02-05 04:51:29,839][0m Trial 2 finished with value: 0.606725309576307 and parameters: {'observation_period_num': 231, 'train_rates': 0.9589508505799158, 'learning_rate': 2.1378379149001616e-06, 'batch_size': 36, 'step_size': 11, 'gamma': 0.9046921811414084}. Best is trial 0 with value: 0.4201387335822851.[0m
[32m[I 2025-02-05 04:52:07,396][0m Trial 3 finished with value: 1.230951168907248 and parameters: {'observation_period_num': 11, 'train_rates': 0.9059942745453775, 'learning_rate': 2.2399717001116643e-06, 'batch_size': 160, 'step_size': 7, 'gamma': 0.813958343629926}. Best is trial 0 with value: 0.4201387335822851.[0m
[32m[I 2025-02-05 04:54:11,046][0m Trial 4 finished with value: 0.05850524410475005 and parameters: {'observation_period_num': 59, 'train_rates': 0.8422285161365508, 'learning_rate': 4.289004451656101e-05, 'batch_size': 43, 'step_size': 12, 'gamma': 0.7737739790520503}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 04:54:49,841][0m Trial 5 finished with value: 0.2052996818224589 and parameters: {'observation_period_num': 124, 'train_rates': 0.6022978046464437, 'learning_rate': 0.0002029805950914991, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8534156729047734}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 04:55:34,213][0m Trial 6 finished with value: 0.5652301413031806 and parameters: {'observation_period_num': 78, 'train_rates': 0.8427545576724986, 'learning_rate': 3.280421145574053e-06, 'batch_size': 129, 'step_size': 7, 'gamma': 0.901290466847785}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 04:57:51,173][0m Trial 7 finished with value: 0.14430693164072345 and parameters: {'observation_period_num': 12, 'train_rates': 0.6798135944672885, 'learning_rate': 0.00024322102245655873, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8909357547761905}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 04:58:26,906][0m Trial 8 finished with value: 0.79696462672739 and parameters: {'observation_period_num': 163, 'train_rates': 0.6233071785194929, 'learning_rate': 0.00019415052878494636, 'batch_size': 127, 'step_size': 5, 'gamma': 0.793848444263468}. Best is trial 4 with value: 0.05850524410475005.[0m
Early stopping at epoch 54
[32m[I 2025-02-05 04:58:41,802][0m Trial 9 finished with value: 0.9735949549181708 and parameters: {'observation_period_num': 60, 'train_rates': 0.8498660324950957, 'learning_rate': 1.5197002107169582e-05, 'batch_size': 226, 'step_size': 1, 'gamma': 0.8066319908640299}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 04:59:44,374][0m Trial 10 finished with value: 0.25708182551869563 and parameters: {'observation_period_num': 122, 'train_rates': 0.7113533480917263, 'learning_rate': 5.108926032774219e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.7567345183308474}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:02:15,764][0m Trial 11 finished with value: 0.1373775315902582 and parameters: {'observation_period_num': 7, 'train_rates': 0.72176182781619, 'learning_rate': 0.0009886263964441059, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9883712772030471}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:03:32,999][0m Trial 12 finished with value: 0.24662116090030517 and parameters: {'observation_period_num': 96, 'train_rates': 0.7391101379646956, 'learning_rate': 0.0007159195334062938, 'batch_size': 63, 'step_size': 15, 'gamma': 0.9859988277402795}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:04:34,715][0m Trial 13 finished with value: 0.1714357737360931 and parameters: {'observation_period_num': 11, 'train_rates': 0.7632723963270446, 'learning_rate': 0.0009682360143573593, 'batch_size': 85, 'step_size': 12, 'gamma': 0.9855428940958788}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:09:09,182][0m Trial 14 finished with value: 0.13133916392319464 and parameters: {'observation_period_num': 177, 'train_rates': 0.8133202878747198, 'learning_rate': 4.397697230004514e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8556654651587844}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:14:27,904][0m Trial 15 finished with value: 0.11294886058248828 and parameters: {'observation_period_num': 191, 'train_rates': 0.8585981586403916, 'learning_rate': 3.221795559787565e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8522143634500305}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:15:27,870][0m Trial 16 finished with value: 0.3579346266748074 and parameters: {'observation_period_num': 252, 'train_rates': 0.8863383122400804, 'learning_rate': 1.4653945111841221e-05, 'batch_size': 90, 'step_size': 9, 'gamma': 0.7549636794191337}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:17:12,232][0m Trial 17 finished with value: 0.19960704445838928 and parameters: {'observation_period_num': 192, 'train_rates': 0.9892468585993494, 'learning_rate': 9.085385508723982e-05, 'batch_size': 55, 'step_size': 9, 'gamma': 0.82821341087716}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:18:11,333][0m Trial 18 finished with value: 0.33609748231040104 and parameters: {'observation_period_num': 202, 'train_rates': 0.9182847698553142, 'learning_rate': 1.55390597850521e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.7842247041422079}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:23:14,429][0m Trial 19 finished with value: 0.1304578704875894 and parameters: {'observation_period_num': 154, 'train_rates': 0.8629721663988552, 'learning_rate': 7.642039838926495e-06, 'batch_size': 17, 'step_size': 13, 'gamma': 0.83742636628122}. Best is trial 4 with value: 0.05850524410475005.[0m
Early stopping at epoch 51
[32m[I 2025-02-05 05:23:28,465][0m Trial 20 finished with value: 0.6750927567481995 and parameters: {'observation_period_num': 101, 'train_rates': 0.9415213884839554, 'learning_rate': 7.488633959114626e-05, 'batch_size': 251, 'step_size': 1, 'gamma': 0.7754792857362676}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:28:16,605][0m Trial 21 finished with value: 0.1329348406234345 and parameters: {'observation_period_num': 145, 'train_rates': 0.8630460363541649, 'learning_rate': 7.538192466254644e-06, 'batch_size': 18, 'step_size': 13, 'gamma': 0.839870826271402}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:29:49,293][0m Trial 22 finished with value: 0.13550661537249084 and parameters: {'observation_period_num': 150, 'train_rates': 0.8229539873516712, 'learning_rate': 2.2969037141016777e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8721694491495936}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:31:46,559][0m Trial 23 finished with value: 0.23359950014271919 and parameters: {'observation_period_num': 208, 'train_rates': 0.8765613163734058, 'learning_rate': 7.868940059366508e-06, 'batch_size': 44, 'step_size': 14, 'gamma': 0.8745659899498641}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:36:49,155][0m Trial 24 finished with value: 0.11211682112311658 and parameters: {'observation_period_num': 174, 'train_rates': 0.7919319606528958, 'learning_rate': 2.970589448014947e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.824976659238408}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:38:01,711][0m Trial 25 finished with value: 0.3296961108189716 and parameters: {'observation_period_num': 187, 'train_rates': 0.774857271310844, 'learning_rate': 2.7886088464454333e-05, 'batch_size': 69, 'step_size': 9, 'gamma': 0.8181791629158554}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:38:48,053][0m Trial 26 finished with value: 0.30562444552078516 and parameters: {'observation_period_num': 219, 'train_rates': 0.7666591812904175, 'learning_rate': 9.778470214617555e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.7985854080770883}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:39:20,414][0m Trial 27 finished with value: 0.20994015512141315 and parameters: {'observation_period_num': 166, 'train_rates': 0.8263721301598604, 'learning_rate': 5.410078335187988e-05, 'batch_size': 175, 'step_size': 11, 'gamma': 0.7723183312557422}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:41:11,925][0m Trial 28 finished with value: 0.10327635086694006 and parameters: {'observation_period_num': 135, 'train_rates': 0.7906364961583426, 'learning_rate': 0.00012347569603247412, 'batch_size': 45, 'step_size': 12, 'gamma': 0.8507031654438759}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:43:00,160][0m Trial 29 finished with value: 0.12808559118845575 and parameters: {'observation_period_num': 102, 'train_rates': 0.7924702161582977, 'learning_rate': 0.00028204776062261904, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9506590977500914}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:43:59,019][0m Trial 30 finished with value: 0.22326644670442153 and parameters: {'observation_period_num': 138, 'train_rates': 0.674951687878022, 'learning_rate': 0.0001429377975334617, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8293001949527319}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:46:39,824][0m Trial 31 finished with value: 0.13622529290918742 and parameters: {'observation_period_num': 177, 'train_rates': 0.7959956564047123, 'learning_rate': 3.234387335363411e-05, 'batch_size': 31, 'step_size': 12, 'gamma': 0.856595511132193}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:51:49,435][0m Trial 32 finished with value: 0.2568275130220822 and parameters: {'observation_period_num': 31, 'train_rates': 0.8373147139694612, 'learning_rate': 1.00988200096588e-06, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8808905467091903}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:53:44,740][0m Trial 33 finished with value: 0.11229481630855137 and parameters: {'observation_period_num': 114, 'train_rates': 0.8034986175450849, 'learning_rate': 0.0003788629089836211, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8498323035892364}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:54:20,419][0m Trial 34 finished with value: 0.19007616266608238 and parameters: {'observation_period_num': 68, 'train_rates': 0.7435509699449133, 'learning_rate': 0.0004200928426412513, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8181405331720715}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:56:06,202][0m Trial 35 finished with value: 0.17511087648392537 and parameters: {'observation_period_num': 118, 'train_rates': 0.7934427691717683, 'learning_rate': 0.00034906919068134016, 'batch_size': 47, 'step_size': 12, 'gamma': 0.925138797464327}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:57:36,351][0m Trial 36 finished with value: 0.08213257094726016 and parameters: {'observation_period_num': 82, 'train_rates': 0.8966069733012905, 'learning_rate': 0.00013265263764216615, 'batch_size': 62, 'step_size': 8, 'gamma': 0.8409996124003719}. Best is trial 4 with value: 0.05850524410475005.[0m
[32m[I 2025-02-05 05:58:32,446][0m Trial 37 finished with value: 0.0551289509197599 and parameters: {'observation_period_num': 40, 'train_rates': 0.9088285497616954, 'learning_rate': 0.00012423930865437677, 'batch_size': 103, 'step_size': 6, 'gamma': 0.8000930050470623}. Best is trial 37 with value: 0.0551289509197599.[0m
[32m[I 2025-02-05 05:59:26,886][0m Trial 38 finished with value: 0.058630545106199056 and parameters: {'observation_period_num': 41, 'train_rates': 0.9104061365892104, 'learning_rate': 0.0001381103805754259, 'batch_size': 107, 'step_size': 6, 'gamma': 0.801033103744864}. Best is trial 37 with value: 0.0551289509197599.[0m
[32m[I 2025-02-05 06:00:19,095][0m Trial 39 finished with value: 0.054648118051276964 and parameters: {'observation_period_num': 38, 'train_rates': 0.9094817928614541, 'learning_rate': 0.00015543227525878113, 'batch_size': 111, 'step_size': 6, 'gamma': 0.7891316700892769}. Best is trial 39 with value: 0.054648118051276964.[0m
[32m[I 2025-02-05 06:01:12,719][0m Trial 40 finished with value: 0.09498903191146943 and parameters: {'observation_period_num': 37, 'train_rates': 0.9289809887920161, 'learning_rate': 0.00018734466614889862, 'batch_size': 111, 'step_size': 3, 'gamma': 0.7701995689922719}. Best is trial 39 with value: 0.054648118051276964.[0m
[32m[I 2025-02-05 06:01:54,642][0m Trial 41 finished with value: 0.07456548111192111 and parameters: {'observation_period_num': 47, 'train_rates': 0.9004367567483467, 'learning_rate': 0.00014316834405692148, 'batch_size': 144, 'step_size': 6, 'gamma': 0.7897017773267608}. Best is trial 39 with value: 0.054648118051276964.[0m
[32m[I 2025-02-05 06:02:36,547][0m Trial 42 finished with value: 0.1115182536067786 and parameters: {'observation_period_num': 49, 'train_rates': 0.9071101950181417, 'learning_rate': 6.85772974357466e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.790981567737833}. Best is trial 39 with value: 0.054648118051276964.[0m
[32m[I 2025-02-05 06:03:15,753][0m Trial 43 finished with value: 0.051450639963150024 and parameters: {'observation_period_num': 25, 'train_rates': 0.9654968537380212, 'learning_rate': 0.0005101690266922343, 'batch_size': 162, 'step_size': 6, 'gamma': 0.8035448671002676}. Best is trial 43 with value: 0.051450639963150024.[0m
[32m[I 2025-02-05 06:03:53,244][0m Trial 44 finished with value: 0.07171234488487244 and parameters: {'observation_period_num': 27, 'train_rates': 0.9666813219386435, 'learning_rate': 0.0005242408525951548, 'batch_size': 175, 'step_size': 4, 'gamma': 0.7991605042993304}. Best is trial 43 with value: 0.051450639963150024.[0m
[32m[I 2025-02-05 06:04:41,262][0m Trial 45 finished with value: 0.0495237686591489 and parameters: {'observation_period_num': 21, 'train_rates': 0.9403350108396735, 'learning_rate': 0.00022810216429418082, 'batch_size': 129, 'step_size': 6, 'gamma': 0.8069805684635248}. Best is trial 45 with value: 0.0495237686591489.[0m
[32m[I 2025-02-05 06:05:20,633][0m Trial 46 finished with value: 0.0414227657020092 and parameters: {'observation_period_num': 22, 'train_rates': 0.9650821404153942, 'learning_rate': 0.0005949696896725158, 'batch_size': 164, 'step_size': 7, 'gamma': 0.7627307357528028}. Best is trial 46 with value: 0.0414227657020092.[0m
[32m[I 2025-02-05 06:05:59,984][0m Trial 47 finished with value: 0.04107614606618881 and parameters: {'observation_period_num': 20, 'train_rates': 0.9561786315286788, 'learning_rate': 0.0005946047055748292, 'batch_size': 160, 'step_size': 7, 'gamma': 0.7640200006019118}. Best is trial 47 with value: 0.04107614606618881.[0m
[32m[I 2025-02-05 06:06:34,332][0m Trial 48 finished with value: 0.04427757486701012 and parameters: {'observation_period_num': 15, 'train_rates': 0.9563760966949473, 'learning_rate': 0.0006113883610874538, 'batch_size': 196, 'step_size': 7, 'gamma': 0.7607486915588199}. Best is trial 47 with value: 0.04107614606618881.[0m
[32m[I 2025-02-05 06:07:06,172][0m Trial 49 finished with value: 0.05457179248332977 and parameters: {'observation_period_num': 24, 'train_rates': 0.9593232914538782, 'learning_rate': 0.0006641009843170421, 'batch_size': 201, 'step_size': 7, 'gamma': 0.7514971875222345}. Best is trial 47 with value: 0.04107614606618881.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-05 06:07:06,183][0m A new study created in memory with name: no-name-93d87cfe-b101-416a-9a81-738657b831b7[0m
[32m[I 2025-02-05 06:07:24,914][0m Trial 0 finished with value: 0.6266268158522178 and parameters: {'observation_period_num': 236, 'train_rates': 0.6259207929440983, 'learning_rate': 1.080907388989577e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.8934700065648025}. Best is trial 0 with value: 0.6266268158522178.[0m
[32m[I 2025-02-05 06:08:01,576][0m Trial 1 finished with value: 0.545901407996615 and parameters: {'observation_period_num': 170, 'train_rates': 0.6404151008224952, 'learning_rate': 6.213099831009631e-06, 'batch_size': 127, 'step_size': 11, 'gamma': 0.903397468027732}. Best is trial 1 with value: 0.545901407996615.[0m
[32m[I 2025-02-05 06:08:38,736][0m Trial 2 finished with value: 1.014381313646162 and parameters: {'observation_period_num': 235, 'train_rates': 0.9320836944678732, 'learning_rate': 1.641829642577288e-06, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8539930382422143}. Best is trial 1 with value: 0.545901407996615.[0m
[32m[I 2025-02-05 06:09:21,961][0m Trial 3 finished with value: 0.28233546018600464 and parameters: {'observation_period_num': 226, 'train_rates': 0.9683502524556304, 'learning_rate': 2.410673913953641e-05, 'batch_size': 132, 'step_size': 13, 'gamma': 0.8309394977599764}. Best is trial 3 with value: 0.28233546018600464.[0m
[32m[I 2025-02-05 06:10:01,034][0m Trial 4 finished with value: 0.12187323524608268 and parameters: {'observation_period_num': 125, 'train_rates': 0.8039469865583211, 'learning_rate': 0.00014885240483065076, 'batch_size': 140, 'step_size': 7, 'gamma': 0.8217592050904348}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:10:36,097][0m Trial 5 finished with value: 0.43540808111429213 and parameters: {'observation_period_num': 84, 'train_rates': 0.9093712912893985, 'learning_rate': 4.870108281494744e-06, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8689089740075551}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:11:05,116][0m Trial 6 finished with value: 0.22555974021292569 and parameters: {'observation_period_num': 217, 'train_rates': 0.875261287182853, 'learning_rate': 3.511919280432224e-05, 'batch_size': 192, 'step_size': 13, 'gamma': 0.830700936016776}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:13:18,197][0m Trial 7 finished with value: 0.18672263802125535 and parameters: {'observation_period_num': 73, 'train_rates': 0.6979684118536786, 'learning_rate': 0.0009684172890329254, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9609534762993741}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:14:16,441][0m Trial 8 finished with value: 0.14551468482192273 and parameters: {'observation_period_num': 196, 'train_rates': 0.9310385227125715, 'learning_rate': 0.0005583824610517512, 'batch_size': 95, 'step_size': 11, 'gamma': 0.7733701231225814}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:15:37,375][0m Trial 9 finished with value: 1.2214237058349928 and parameters: {'observation_period_num': 141, 'train_rates': 0.8643772825001048, 'learning_rate': 1.922285459879079e-06, 'batch_size': 69, 'step_size': 3, 'gamma': 0.7525932379346881}. Best is trial 4 with value: 0.12187323524608268.[0m
Early stopping at epoch 57
[32m[I 2025-02-05 06:15:53,254][0m Trial 10 finished with value: 0.2924019572218651 and parameters: {'observation_period_num': 13, 'train_rates': 0.7590858001811212, 'learning_rate': 0.00015989528587950823, 'batch_size': 222, 'step_size': 1, 'gamma': 0.7928080585704822}. Best is trial 4 with value: 0.12187323524608268.[0m
[32m[I 2025-02-05 06:17:10,350][0m Trial 11 finished with value: 0.11583077338382214 and parameters: {'observation_period_num': 175, 'train_rates': 0.8064833195356389, 'learning_rate': 0.00040628167702493653, 'batch_size': 65, 'step_size': 8, 'gamma': 0.7530104615013035}. Best is trial 11 with value: 0.11583077338382214.[0m
[32m[I 2025-02-05 06:21:31,223][0m Trial 12 finished with value: 0.12827606971796918 and parameters: {'observation_period_num': 114, 'train_rates': 0.7967913456489384, 'learning_rate': 0.00015049903997402036, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7879885167660006}. Best is trial 11 with value: 0.11583077338382214.[0m
[32m[I 2025-02-05 06:22:37,470][0m Trial 13 finished with value: 0.1046051207633145 and parameters: {'observation_period_num': 154, 'train_rates': 0.7982465429987206, 'learning_rate': 0.00016799798954102824, 'batch_size': 75, 'step_size': 9, 'gamma': 0.8098163228645191}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:23:43,659][0m Trial 14 finished with value: 0.25763854076725673 and parameters: {'observation_period_num': 183, 'train_rates': 0.7386095438777901, 'learning_rate': 0.0003405618076528658, 'batch_size': 71, 'step_size': 9, 'gamma': 0.7588019224558588}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:24:40,834][0m Trial 15 finished with value: 0.19169484538122497 and parameters: {'observation_period_num': 157, 'train_rates': 0.8341633269402926, 'learning_rate': 6.427304984340505e-05, 'batch_size': 91, 'step_size': 4, 'gamma': 0.8067112256885336}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:26:16,650][0m Trial 16 finished with value: 0.2391951283487871 and parameters: {'observation_period_num': 97, 'train_rates': 0.713938472736648, 'learning_rate': 0.00036903383015025876, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9602207559694056}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:27:07,038][0m Trial 17 finished with value: 0.2015204601078253 and parameters: {'observation_period_num': 45, 'train_rates': 0.7682007188986304, 'learning_rate': 7.048871499488003e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.783744679052938}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:28:16,751][0m Trial 18 finished with value: 0.2673643377362465 and parameters: {'observation_period_num': 204, 'train_rates': 0.6791005497816119, 'learning_rate': 0.0009634312902465389, 'batch_size': 64, 'step_size': 5, 'gamma': 0.7513606975230065}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:31:24,968][0m Trial 19 finished with value: 0.1405290844301209 and parameters: {'observation_period_num': 147, 'train_rates': 0.837011903235943, 'learning_rate': 0.00023797026756966236, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9156716557657063}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:32:13,296][0m Trial 20 finished with value: 0.22298121240348498 and parameters: {'observation_period_num': 251, 'train_rates': 0.8231910334986899, 'learning_rate': 7.345131664561805e-05, 'batch_size': 109, 'step_size': 11, 'gamma': 0.8095874351542189}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:32:47,741][0m Trial 21 finished with value: 0.1304249002311055 and parameters: {'observation_period_num': 123, 'train_rates': 0.792652185044783, 'learning_rate': 0.00010257704508218374, 'batch_size': 155, 'step_size': 7, 'gamma': 0.8349039259265485}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:33:50,172][0m Trial 22 finished with value: 0.26962348226427046 and parameters: {'observation_period_num': 181, 'train_rates': 0.7817024424040301, 'learning_rate': 0.0005113890577321532, 'batch_size': 81, 'step_size': 8, 'gamma': 0.805724340431437}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:34:22,128][0m Trial 23 finished with value: 0.12108548882709452 and parameters: {'observation_period_num': 129, 'train_rates': 0.8805121935661845, 'learning_rate': 0.00019848543360118814, 'batch_size': 187, 'step_size': 6, 'gamma': 0.8515277046424943}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:34:53,589][0m Trial 24 finished with value: 0.19552653778198711 and parameters: {'observation_period_num': 104, 'train_rates': 0.8744290032675086, 'learning_rate': 3.672979502096541e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8579419852545366}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:35:27,274][0m Trial 25 finished with value: 0.1264879248267391 and parameters: {'observation_period_num': 162, 'train_rates': 0.8993284020999426, 'learning_rate': 0.00024926864930706024, 'batch_size': 187, 'step_size': 3, 'gamma': 0.9321273247649776}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:37:33,255][0m Trial 26 finished with value: 0.16941849458840938 and parameters: {'observation_period_num': 138, 'train_rates': 0.8371890206221633, 'learning_rate': 0.0005665025503679279, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8778930680065761}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:37:58,030][0m Trial 27 finished with value: 0.20776196788686427 and parameters: {'observation_period_num': 61, 'train_rates': 0.7384171121466537, 'learning_rate': 0.00023509907450736236, 'batch_size': 221, 'step_size': 6, 'gamma': 0.7729192066659135}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:39:36,425][0m Trial 28 finished with value: 0.19678908128004807 and parameters: {'observation_period_num': 196, 'train_rates': 0.9765596832166176, 'learning_rate': 2.134940506170678e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.7709808959642099}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:39:59,357][0m Trial 29 finished with value: 0.26754103467250484 and parameters: {'observation_period_num': 174, 'train_rates': 0.8596370054072855, 'learning_rate': 5.293757808143209e-05, 'batch_size': 245, 'step_size': 6, 'gamma': 0.8439212129630684}. Best is trial 13 with value: 0.1046051207633145.[0m
Early stopping at epoch 89
[32m[I 2025-02-05 06:40:20,251][0m Trial 30 finished with value: 0.8579010398072355 and parameters: {'observation_period_num': 158, 'train_rates': 0.6107295835545585, 'learning_rate': 1.2807696851696286e-05, 'batch_size': 210, 'step_size': 1, 'gamma': 0.8834085707231838}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:41:06,178][0m Trial 31 finished with value: 0.11846373717212144 and parameters: {'observation_period_num': 121, 'train_rates': 0.8105383332933963, 'learning_rate': 0.00013300428292858623, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8198619753050631}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:41:51,933][0m Trial 32 finished with value: 0.1330044303428043 and parameters: {'observation_period_num': 134, 'train_rates': 0.8127366098304484, 'learning_rate': 0.00010868811896433856, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8174361269199472}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:42:53,534][0m Trial 33 finished with value: 0.2173420094711104 and parameters: {'observation_period_num': 111, 'train_rates': 0.7681561329042838, 'learning_rate': 0.00035402298654370237, 'batch_size': 82, 'step_size': 5, 'gamma': 0.8469404359541085}. Best is trial 13 with value: 0.1046051207633145.[0m
[32m[I 2025-02-05 06:43:35,213][0m Trial 34 finished with value: 0.10367987518175752 and parameters: {'observation_period_num': 90, 'train_rates': 0.9013832772724372, 'learning_rate': 0.00011137876561343893, 'batch_size': 143, 'step_size': 10, 'gamma': 0.860228627765127}. Best is trial 34 with value: 0.10367987518175752.[0m
[32m[I 2025-02-05 06:44:16,370][0m Trial 35 finished with value: 0.10553640872240067 and parameters: {'observation_period_num': 93, 'train_rates': 0.954705901843564, 'learning_rate': 0.00010311504216291576, 'batch_size': 148, 'step_size': 12, 'gamma': 0.897393459427974}. Best is trial 34 with value: 0.10367987518175752.[0m
[32m[I 2025-02-05 06:44:57,794][0m Trial 36 finished with value: 0.09028040595790919 and parameters: {'observation_period_num': 88, 'train_rates': 0.9409002072632054, 'learning_rate': 9.774607005706168e-05, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8993063005561616}. Best is trial 36 with value: 0.09028040595790919.[0m
[32m[I 2025-02-05 06:45:35,263][0m Trial 37 finished with value: 0.12804508209228516 and parameters: {'observation_period_num': 87, 'train_rates': 0.9522802365329206, 'learning_rate': 4.827979573653582e-05, 'batch_size': 170, 'step_size': 15, 'gamma': 0.9035515478828442}. Best is trial 36 with value: 0.09028040595790919.[0m
[32m[I 2025-02-05 06:46:17,704][0m Trial 38 finished with value: 0.14322943854256995 and parameters: {'observation_period_num': 47, 'train_rates': 0.9454000353811491, 'learning_rate': 1.8533010464080977e-05, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9372372249577804}. Best is trial 36 with value: 0.09028040595790919.[0m
[32m[I 2025-02-05 06:46:59,365][0m Trial 39 finished with value: 0.08637882774883462 and parameters: {'observation_period_num': 71, 'train_rates': 0.9105786524110903, 'learning_rate': 0.0001017837166070808, 'batch_size': 142, 'step_size': 15, 'gamma': 0.8955753434221845}. Best is trial 39 with value: 0.08637882774883462.[0m
[32m[I 2025-02-05 06:47:43,035][0m Trial 40 finished with value: 0.28495784877221797 and parameters: {'observation_period_num': 65, 'train_rates': 0.9179735584286929, 'learning_rate': 5.332671572891822e-06, 'batch_size': 136, 'step_size': 15, 'gamma': 0.9874810823679464}. Best is trial 39 with value: 0.08637882774883462.[0m
[32m[I 2025-02-05 06:48:20,778][0m Trial 41 finished with value: 0.13749736547470093 and parameters: {'observation_period_num': 90, 'train_rates': 0.9518518359606544, 'learning_rate': 0.00010167256829313344, 'batch_size': 169, 'step_size': 14, 'gamma': 0.8922817925555537}. Best is trial 39 with value: 0.08637882774883462.[0m
[32m[I 2025-02-05 06:49:06,251][0m Trial 42 finished with value: 0.12525290250778198 and parameters: {'observation_period_num': 76, 'train_rates': 0.9864282831703153, 'learning_rate': 3.126941912652179e-05, 'batch_size': 140, 'step_size': 12, 'gamma': 0.8678681374439006}. Best is trial 39 with value: 0.08637882774883462.[0m
[32m[I 2025-02-05 06:49:48,823][0m Trial 43 finished with value: 0.06562228697707982 and parameters: {'observation_period_num': 47, 'train_rates': 0.9155395432098302, 'learning_rate': 8.554592292286218e-05, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9167643979155453}. Best is trial 43 with value: 0.06562228697707982.[0m
[32m[I 2025-02-05 06:50:35,466][0m Trial 44 finished with value: 0.04998286195983321 and parameters: {'observation_period_num': 14, 'train_rates': 0.899864258804407, 'learning_rate': 8.09229056485971e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.9183072606594335}. Best is trial 44 with value: 0.04998286195983321.[0m
[32m[I 2025-02-05 06:51:23,709][0m Trial 45 finished with value: 0.05929474138823532 and parameters: {'observation_period_num': 16, 'train_rates': 0.8986574637384608, 'learning_rate': 4.692572790965492e-05, 'batch_size': 125, 'step_size': 14, 'gamma': 0.9156446142339978}. Best is trial 44 with value: 0.04998286195983321.[0m
[32m[I 2025-02-05 06:52:11,805][0m Trial 46 finished with value: 0.061404698066883256 and parameters: {'observation_period_num': 5, 'train_rates': 0.9249493510319123, 'learning_rate': 4.3806722424060034e-05, 'batch_size': 128, 'step_size': 14, 'gamma': 0.9194304227040274}. Best is trial 44 with value: 0.04998286195983321.[0m
[32m[I 2025-02-05 06:52:59,283][0m Trial 47 finished with value: 0.0867430600165217 and parameters: {'observation_period_num': 5, 'train_rates': 0.9267746060873698, 'learning_rate': 9.820333365906248e-06, 'batch_size': 127, 'step_size': 14, 'gamma': 0.9224672298000476}. Best is trial 44 with value: 0.04998286195983321.[0m
[32m[I 2025-02-05 06:53:52,129][0m Trial 48 finished with value: 0.059448925830493465 and parameters: {'observation_period_num': 23, 'train_rates': 0.8905244027235051, 'learning_rate': 4.418338638982233e-05, 'batch_size': 109, 'step_size': 13, 'gamma': 0.9483835015630339}. Best is trial 44 with value: 0.04998286195983321.[0m
[32m[I 2025-02-05 06:54:49,308][0m Trial 49 finished with value: 0.06905351082483928 and parameters: {'observation_period_num': 24, 'train_rates': 0.8936380341556616, 'learning_rate': 4.0927118901944595e-05, 'batch_size': 104, 'step_size': 13, 'gamma': 0.9532812926241161}. Best is trial 44 with value: 0.04998286195983321.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-05 06:54:49,319][0m A new study created in memory with name: no-name-162ec62a-a820-4f8f-8dd7-a075411ba098[0m
Early stopping at epoch 86
[32m[I 2025-02-05 06:55:11,847][0m Trial 0 finished with value: 1.8768701011076905 and parameters: {'observation_period_num': 70, 'train_rates': 0.8155128584702296, 'learning_rate': 1.702249208237467e-06, 'batch_size': 218, 'step_size': 1, 'gamma': 0.8951108973447699}. Best is trial 0 with value: 1.8768701011076905.[0m
[32m[I 2025-02-05 06:55:59,176][0m Trial 1 finished with value: 0.2291295484890445 and parameters: {'observation_period_num': 32, 'train_rates': 0.7152596994374992, 'learning_rate': 0.00010645969783311126, 'batch_size': 109, 'step_size': 2, 'gamma': 0.7831259780265861}. Best is trial 1 with value: 0.2291295484890445.[0m
[32m[I 2025-02-05 06:56:35,241][0m Trial 2 finished with value: 0.3248161169989356 and parameters: {'observation_period_num': 141, 'train_rates': 0.9384392896038968, 'learning_rate': 1.6376045486125416e-05, 'batch_size': 168, 'step_size': 14, 'gamma': 0.8228457881126233}. Best is trial 1 with value: 0.2291295484890445.[0m
[32m[I 2025-02-05 06:57:22,273][0m Trial 3 finished with value: 0.1723973328052532 and parameters: {'observation_period_num': 104, 'train_rates': 0.6315113474613784, 'learning_rate': 0.0005219717793369789, 'batch_size': 97, 'step_size': 7, 'gamma': 0.7714850209746157}. Best is trial 3 with value: 0.1723973328052532.[0m
[32m[I 2025-02-05 06:57:48,672][0m Trial 4 finished with value: 0.19559199787146142 and parameters: {'observation_period_num': 81, 'train_rates': 0.7210740622550396, 'learning_rate': 0.0009593283420143161, 'batch_size': 201, 'step_size': 2, 'gamma': 0.8508470625215057}. Best is trial 3 with value: 0.1723973328052532.[0m
[32m[I 2025-02-05 06:58:37,372][0m Trial 5 finished with value: 0.43739818359980853 and parameters: {'observation_period_num': 231, 'train_rates': 0.6295415358549923, 'learning_rate': 8.784363109013583e-05, 'batch_size': 90, 'step_size': 9, 'gamma': 0.7576132292122163}. Best is trial 3 with value: 0.1723973328052532.[0m
[32m[I 2025-02-05 06:59:07,902][0m Trial 6 finished with value: 0.33364513516426086 and parameters: {'observation_period_num': 44, 'train_rates': 0.967220133512077, 'learning_rate': 1.4953609972906824e-05, 'batch_size': 219, 'step_size': 4, 'gamma': 0.8933754517670383}. Best is trial 3 with value: 0.1723973328052532.[0m
[32m[I 2025-02-05 07:00:00,345][0m Trial 7 finished with value: 0.45184259353012873 and parameters: {'observation_period_num': 244, 'train_rates': 0.8932661551983156, 'learning_rate': 3.762958175489829e-06, 'batch_size': 101, 'step_size': 8, 'gamma': 0.961626068407133}. Best is trial 3 with value: 0.1723973328052532.[0m
[32m[I 2025-02-05 07:01:12,614][0m Trial 8 finished with value: 0.17185692489147186 and parameters: {'observation_period_num': 160, 'train_rates': 0.9896399906156592, 'learning_rate': 0.0001167383344758292, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8654444295280758}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:01:36,213][0m Trial 9 finished with value: 0.21810171809928222 and parameters: {'observation_period_num': 41, 'train_rates': 0.6078157029745508, 'learning_rate': 0.00029565636447031834, 'batch_size': 205, 'step_size': 3, 'gamma': 0.794603731697346}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:06:35,610][0m Trial 10 finished with value: 0.26961903083067257 and parameters: {'observation_period_num': 169, 'train_rates': 0.8586761518589134, 'learning_rate': 5.6088540977499826e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.952494740008866}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:08:10,722][0m Trial 11 finished with value: 0.27489941461881 and parameters: {'observation_period_num': 178, 'train_rates': 0.729652708830561, 'learning_rate': 0.0005080227341653708, 'batch_size': 49, 'step_size': 10, 'gamma': 0.8517548631581515}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:08:47,362][0m Trial 12 finished with value: 0.22849720379998606 and parameters: {'observation_period_num': 108, 'train_rates': 0.7820444339922483, 'learning_rate': 0.00018989300057420115, 'batch_size': 146, 'step_size': 6, 'gamma': 0.9196775515750015}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:09:46,299][0m Trial 13 finished with value: 0.2575144418485605 and parameters: {'observation_period_num': 137, 'train_rates': 0.665624915939736, 'learning_rate': 0.0008914035898017023, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8169258455507954}. Best is trial 8 with value: 0.17185692489147186.[0m
[32m[I 2025-02-05 07:10:26,093][0m Trial 14 finished with value: 0.11932362163419365 and parameters: {'observation_period_num': 194, 'train_rates': 0.7977641177426087, 'learning_rate': 0.00023779514306206603, 'batch_size': 133, 'step_size': 12, 'gamma': 0.7504125830101744}. Best is trial 14 with value: 0.11932362163419365.[0m
[32m[I 2025-02-05 07:11:07,880][0m Trial 15 finished with value: 0.19585263454322274 and parameters: {'observation_period_num': 203, 'train_rates': 0.8975146790102341, 'learning_rate': 2.486029639073347e-05, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9897533792980244}. Best is trial 14 with value: 0.11932362163419365.[0m
[32m[I 2025-02-05 07:11:33,617][0m Trial 16 finished with value: 0.22552244365215302 and parameters: {'observation_period_num': 202, 'train_rates': 0.9868410307020912, 'learning_rate': 0.00018316449306611516, 'batch_size': 252, 'step_size': 12, 'gamma': 0.810682276250422}. Best is trial 14 with value: 0.11932362163419365.[0m
[32m[I 2025-02-05 07:12:59,069][0m Trial 17 finished with value: 0.09766527305224112 and parameters: {'observation_period_num': 165, 'train_rates': 0.8395168231281567, 'learning_rate': 4.58166531815954e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.7506933827266216}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:14:45,027][0m Trial 18 finished with value: 0.24539122719616785 and parameters: {'observation_period_num': 208, 'train_rates': 0.8004048649039499, 'learning_rate': 8.922050184267197e-06, 'batch_size': 46, 'step_size': 15, 'gamma': 0.7660786833852234}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:15:17,062][0m Trial 19 finished with value: 0.19019868266716433 and parameters: {'observation_period_num': 191, 'train_rates': 0.8389762092527528, 'learning_rate': 3.908866454171036e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.751186081703692}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:15:58,578][0m Trial 20 finished with value: 0.25963075305476335 and parameters: {'observation_period_num': 153, 'train_rates': 0.7643731503836488, 'learning_rate': 5.199994265533433e-05, 'batch_size': 124, 'step_size': 15, 'gamma': 0.7954975694515074}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:17:35,867][0m Trial 21 finished with value: 0.12993022872180474 and parameters: {'observation_period_num': 161, 'train_rates': 0.9119695476550587, 'learning_rate': 0.00011468538311478616, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8424334503313186}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:19:04,070][0m Trial 22 finished with value: 0.1687776067733588 and parameters: {'observation_period_num': 117, 'train_rates': 0.8812196783942299, 'learning_rate': 0.00027746794842464757, 'batch_size': 61, 'step_size': 13, 'gamma': 0.8326547344359977}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:21:30,689][0m Trial 23 finished with value: 0.16741520233218446 and parameters: {'observation_period_num': 224, 'train_rates': 0.9183290929693387, 'learning_rate': 6.917771273916205e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.7844509163442857}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:24:37,051][0m Trial 24 finished with value: 0.10577687241377369 and parameters: {'observation_period_num': 182, 'train_rates': 0.8323305105908408, 'learning_rate': 2.9488260703581864e-05, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8876752218229477}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:29:45,355][0m Trial 25 finished with value: 0.142740260250067 and parameters: {'observation_period_num': 180, 'train_rates': 0.8319980197407304, 'learning_rate': 5.701825877456725e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8868944924348822}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:32:11,868][0m Trial 26 finished with value: 0.2753279653395185 and parameters: {'observation_period_num': 213, 'train_rates': 0.7652312110911269, 'learning_rate': 2.592339975240681e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.9212721354388268}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:33:20,882][0m Trial 27 finished with value: 0.19925922575358776 and parameters: {'observation_period_num': 249, 'train_rates': 0.8635543041962722, 'learning_rate': 1.2952395493180019e-05, 'batch_size': 74, 'step_size': 15, 'gamma': 0.9180043065847899}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:33:53,698][0m Trial 28 finished with value: 0.19120712617117322 and parameters: {'observation_period_num': 187, 'train_rates': 0.8197786547767826, 'learning_rate': 3.4761339582615704e-05, 'batch_size': 163, 'step_size': 12, 'gamma': 0.8713168215361717}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:35:13,225][0m Trial 29 finished with value: 0.7661640987091336 and parameters: {'observation_period_num': 127, 'train_rates': 0.8009510448104312, 'learning_rate': 1.002762604266544e-06, 'batch_size': 64, 'step_size': 9, 'gamma': 0.7502472636166612}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:37:47,122][0m Trial 30 finished with value: 0.3517830930650234 and parameters: {'observation_period_num': 87, 'train_rates': 0.740570540576387, 'learning_rate': 3.7842262820396818e-06, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8053363698806401}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:38:34,487][0m Trial 31 finished with value: 0.12390718901656685 and parameters: {'observation_period_num': 153, 'train_rates': 0.9320338332656543, 'learning_rate': 0.00014950231742743322, 'batch_size': 118, 'step_size': 12, 'gamma': 0.8774056224755175}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:39:19,358][0m Trial 32 finished with value: 0.1001580113859188 and parameters: {'observation_period_num': 150, 'train_rates': 0.8499774767054284, 'learning_rate': 0.00017069782536086826, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8713809139066082}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:39:56,838][0m Trial 33 finished with value: 0.10710819278952614 and parameters: {'observation_period_num': 142, 'train_rates': 0.8512414002723843, 'learning_rate': 0.00031298717292716703, 'batch_size': 147, 'step_size': 13, 'gamma': 0.9114452147857431}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:40:34,880][0m Trial 34 finished with value: 0.14866166200323644 and parameters: {'observation_period_num': 138, 'train_rates': 0.8560203482079335, 'learning_rate': 0.0005623738755196799, 'batch_size': 149, 'step_size': 14, 'gamma': 0.908107401786717}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:41:04,479][0m Trial 35 finished with value: 0.16562299666527128 and parameters: {'observation_period_num': 124, 'train_rates': 0.8418413307871743, 'learning_rate': 0.0003913690081385191, 'batch_size': 185, 'step_size': 13, 'gamma': 0.9427824086823883}. Best is trial 17 with value: 0.09766527305224112.[0m
[32m[I 2025-02-05 07:41:54,505][0m Trial 36 finished with value: 0.07179885652320962 and parameters: {'observation_period_num': 98, 'train_rates': 0.8743656324245632, 'learning_rate': 9.100773719622059e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.906729642164511}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:42:50,953][0m Trial 37 finished with value: 0.1350346192203719 and parameters: {'observation_period_num': 69, 'train_rates': 0.9497925497617503, 'learning_rate': 8.396119509193515e-05, 'batch_size': 105, 'step_size': 15, 'gamma': 0.9004818351957927}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:43:38,757][0m Trial 38 finished with value: 0.22038454961640103 and parameters: {'observation_period_num': 96, 'train_rates': 0.8778850453707715, 'learning_rate': 1.942068377040385e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.8581999708108958}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:44:34,996][0m Trial 39 finished with value: 0.14690512527298455 and parameters: {'observation_period_num': 5, 'train_rates': 0.6927434656342024, 'learning_rate': 4.9030956259806504e-05, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8808564368981922}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:45:30,644][0m Trial 40 finished with value: 0.5457360239746178 and parameters: {'observation_period_num': 64, 'train_rates': 0.8757917950567867, 'learning_rate': 9.787587895852445e-06, 'batch_size': 98, 'step_size': 1, 'gamma': 0.9357804278425912}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:46:03,970][0m Trial 41 finished with value: 0.11953976388998375 and parameters: {'observation_period_num': 144, 'train_rates': 0.8217488659247282, 'learning_rate': 0.00012654165719065707, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9006494304682688}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:46:43,125][0m Trial 42 finished with value: 0.12745581704836625 and parameters: {'observation_period_num': 112, 'train_rates': 0.8400493334855809, 'learning_rate': 7.989900258778435e-05, 'batch_size': 136, 'step_size': 13, 'gamma': 0.9293307202351936}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:47:32,710][0m Trial 43 finished with value: 0.15020992950137171 and parameters: {'observation_period_num': 171, 'train_rates': 0.8993724636340312, 'learning_rate': 0.0003356248308997894, 'batch_size': 109, 'step_size': 14, 'gamma': 0.8900450489265503}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:48:02,378][0m Trial 44 finished with value: 0.11661115163061993 and parameters: {'observation_period_num': 148, 'train_rates': 0.8521685460262088, 'learning_rate': 0.00018393195611969028, 'batch_size': 188, 'step_size': 10, 'gamma': 0.9084702609473253}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:49:04,053][0m Trial 45 finished with value: 0.24712452875162527 and parameters: {'observation_period_num': 162, 'train_rates': 0.7798932505088928, 'learning_rate': 4.2558008037311344e-05, 'batch_size': 77, 'step_size': 15, 'gamma': 0.8655514501261939}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:49:59,806][0m Trial 46 finished with value: 0.09900425723620823 and parameters: {'observation_period_num': 135, 'train_rates': 0.8137600670550443, 'learning_rate': 6.536325466368796e-05, 'batch_size': 92, 'step_size': 7, 'gamma': 0.9629189976728463}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:51:00,873][0m Trial 47 finished with value: 0.11879883085687955 and parameters: {'observation_period_num': 128, 'train_rates': 0.8154516581332809, 'learning_rate': 6.30657592564246e-05, 'batch_size': 87, 'step_size': 7, 'gamma': 0.9685915734854356}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:51:59,468][0m Trial 48 finished with value: 0.08236989788426698 and parameters: {'observation_period_num': 92, 'train_rates': 0.8168626369310116, 'learning_rate': 9.661576762485094e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9775436977243355}. Best is trial 36 with value: 0.07179885652320962.[0m
[32m[I 2025-02-05 07:52:54,021][0m Trial 49 finished with value: 0.20604440050196893 and parameters: {'observation_period_num': 99, 'train_rates': 0.7616297558151168, 'learning_rate': 9.4806113446473e-05, 'batch_size': 92, 'step_size': 7, 'gamma': 0.9818853231645603}. Best is trial 36 with value: 0.07179885652320962.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-05 07:52:54,031][0m A new study created in memory with name: no-name-3dc89a35-9cb5-47fe-a322-a2500d9be6b1[0m
[32m[I 2025-02-05 07:53:28,685][0m Trial 0 finished with value: 0.9542251779774952 and parameters: {'observation_period_num': 144, 'train_rates': 0.7204832030059898, 'learning_rate': 1.5975755039043073e-06, 'batch_size': 143, 'step_size': 4, 'gamma': 0.8965405964900711}. Best is trial 0 with value: 0.9542251779774952.[0m
[32m[I 2025-02-05 07:54:18,164][0m Trial 1 finished with value: 0.059541887651055544 and parameters: {'observation_period_num': 29, 'train_rates': 0.9195088570070827, 'learning_rate': 5.664017056185634e-05, 'batch_size': 127, 'step_size': 12, 'gamma': 0.7918061134307074}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:54:45,923][0m Trial 2 finished with value: 0.13201422976654137 and parameters: {'observation_period_num': 5, 'train_rates': 0.6023719512500421, 'learning_rate': 0.0003350151170752745, 'batch_size': 171, 'step_size': 2, 'gamma': 0.9892171867427881}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:55:19,041][0m Trial 3 finished with value: 0.31690580084023295 and parameters: {'observation_period_num': 11, 'train_rates': 0.7628172418748299, 'learning_rate': 4.535059541598991e-06, 'batch_size': 170, 'step_size': 11, 'gamma': 0.9449054945826312}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:56:18,302][0m Trial 4 finished with value: 0.10126991898525418 and parameters: {'observation_period_num': 93, 'train_rates': 0.9556239967812746, 'learning_rate': 7.083596502586439e-05, 'batch_size': 101, 'step_size': 14, 'gamma': 0.8488206230492717}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:56:56,734][0m Trial 5 finished with value: 0.310653777049922 and parameters: {'observation_period_num': 69, 'train_rates': 0.6655042063483625, 'learning_rate': 4.256446027838492e-05, 'batch_size': 127, 'step_size': 12, 'gamma': 0.75996686241995}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:59:16,467][0m Trial 6 finished with value: 0.14206692210414953 and parameters: {'observation_period_num': 128, 'train_rates': 0.9598658249032079, 'learning_rate': 0.0003131333061048082, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8391215350407919}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 07:59:50,300][0m Trial 7 finished with value: 0.26137528818482963 and parameters: {'observation_period_num': 69, 'train_rates': 0.6553041074132862, 'learning_rate': 0.0008169613435411816, 'batch_size': 145, 'step_size': 2, 'gamma': 0.8396383542017041}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:00:23,631][0m Trial 8 finished with value: 0.11600209554140481 and parameters: {'observation_period_num': 85, 'train_rates': 0.9337084057783194, 'learning_rate': 6.645335881224096e-05, 'batch_size': 181, 'step_size': 12, 'gamma': 0.9886025401974023}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:00:49,002][0m Trial 9 finished with value: 0.20582747276699837 and parameters: {'observation_period_num': 12, 'train_rates': 0.8754551428012696, 'learning_rate': 4.1779627600072685e-06, 'batch_size': 247, 'step_size': 9, 'gamma': 0.96872576505907}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:02:23,573][0m Trial 10 finished with value: 0.3508114832076842 and parameters: {'observation_period_num': 246, 'train_rates': 0.8473959093542585, 'learning_rate': 1.2804100864606203e-05, 'batch_size': 53, 'step_size': 6, 'gamma': 0.7538888645793179}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:03:29,214][0m Trial 11 finished with value: 0.12415449321269989 and parameters: {'observation_period_num': 115, 'train_rates': 0.9825425005099844, 'learning_rate': 0.0001044902051758542, 'batch_size': 91, 'step_size': 15, 'gamma': 0.8007734318743116}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:04:34,351][0m Trial 12 finished with value: 0.19798861124685832 and parameters: {'observation_period_num': 200, 'train_rates': 0.8882126616141203, 'learning_rate': 1.5085633828606603e-05, 'batch_size': 83, 'step_size': 15, 'gamma': 0.8777666159126558}. Best is trial 1 with value: 0.059541887651055544.[0m
[32m[I 2025-02-05 08:05:30,961][0m Trial 13 finished with value: 0.0529693514396325 and parameters: {'observation_period_num': 48, 'train_rates': 0.8219135033742733, 'learning_rate': 0.00014327727163802526, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8058735682708033}. Best is trial 13 with value: 0.0529693514396325.[0m
[32m[I 2025-02-05 08:09:35,824][0m Trial 14 finished with value: 0.05503195005779465 and parameters: {'observation_period_num': 44, 'train_rates': 0.8143848555353334, 'learning_rate': 0.0002205068652712144, 'batch_size': 21, 'step_size': 7, 'gamma': 0.7958606470434281}. Best is trial 13 with value: 0.0529693514396325.[0m
[32m[I 2025-02-05 08:13:40,897][0m Trial 15 finished with value: 0.05198593875514222 and parameters: {'observation_period_num': 41, 'train_rates': 0.8148628845674084, 'learning_rate': 0.00019632914960639097, 'batch_size': 21, 'step_size': 7, 'gamma': 0.8003559644383961}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:14:57,780][0m Trial 16 finished with value: 0.19406381023485633 and parameters: {'observation_period_num': 56, 'train_rates': 0.7668275792088171, 'learning_rate': 0.00016344835421863783, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8157519292157864}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:19:17,686][0m Trial 17 finished with value: 0.16923831567284878 and parameters: {'observation_period_num': 163, 'train_rates': 0.8169490831121791, 'learning_rate': 0.0008783833745678948, 'batch_size': 19, 'step_size': 8, 'gamma': 0.8932243477961276}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:19:42,378][0m Trial 18 finished with value: 0.6623752847958361 and parameters: {'observation_period_num': 107, 'train_rates': 0.7321079557234477, 'learning_rate': 2.2562497607149186e-05, 'batch_size': 225, 'step_size': 4, 'gamma': 0.7743568020338109}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:20:32,457][0m Trial 19 finished with value: 0.13435302799853094 and parameters: {'observation_period_num': 173, 'train_rates': 0.8522751796005966, 'learning_rate': 0.0004159848213169417, 'batch_size': 107, 'step_size': 8, 'gamma': 0.8207999682576099}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:21:59,055][0m Trial 20 finished with value: 0.1873114691669918 and parameters: {'observation_period_num': 40, 'train_rates': 0.7867364241404473, 'learning_rate': 0.00015115591513358072, 'batch_size': 60, 'step_size': 10, 'gamma': 0.921268266096832}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:26:08,796][0m Trial 21 finished with value: 0.05965548920936472 and parameters: {'observation_period_num': 43, 'train_rates': 0.8255560157213061, 'learning_rate': 0.00022457162434771468, 'batch_size': 21, 'step_size': 7, 'gamma': 0.7880099681789192}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:28:20,181][0m Trial 22 finished with value: 0.05684013464078441 and parameters: {'observation_period_num': 63, 'train_rates': 0.8075402687395099, 'learning_rate': 0.00012394739033735793, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8161068112454342}. Best is trial 15 with value: 0.05198593875514222.[0m
[32m[I 2025-02-05 08:34:01,916][0m Trial 23 finished with value: 0.0428722236766916 and parameters: {'observation_period_num': 32, 'train_rates': 0.88668431540847, 'learning_rate': 0.0003845710574239123, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7768549957738947}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:35:15,017][0m Trial 24 finished with value: 0.06398961259174847 and parameters: {'observation_period_num': 30, 'train_rates': 0.8859290947143442, 'learning_rate': 0.000610465064371425, 'batch_size': 78, 'step_size': 9, 'gamma': 0.7747330175647039}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:37:25,114][0m Trial 25 finished with value: 0.13690785586906465 and parameters: {'observation_period_num': 85, 'train_rates': 0.8566678210718321, 'learning_rate': 0.0005322230657950743, 'batch_size': 41, 'step_size': 5, 'gamma': 0.8564658550840749}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:38:36,242][0m Trial 26 finished with value: 0.17460281635897282 and parameters: {'observation_period_num': 26, 'train_rates': 0.7337561314070398, 'learning_rate': 2.889726088415211e-05, 'batch_size': 70, 'step_size': 13, 'gamma': 0.825739772505221}. Best is trial 23 with value: 0.0428722236766916.[0m
Early stopping at epoch 73
[32m[I 2025-02-05 08:40:32,272][0m Trial 27 finished with value: 0.09212180451118857 and parameters: {'observation_period_num': 50, 'train_rates': 0.9085714682649417, 'learning_rate': 0.0002385161738287876, 'batch_size': 36, 'step_size': 1, 'gamma': 0.7723302017761609}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:41:20,401][0m Trial 28 finished with value: 0.2136005616456366 and parameters: {'observation_period_num': 73, 'train_rates': 0.7841803010948529, 'learning_rate': 9.752928950884473e-05, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8034030771326649}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:41:48,334][0m Trial 29 finished with value: 0.7879228192815635 and parameters: {'observation_period_num': 105, 'train_rates': 0.8395385484521023, 'learning_rate': 1.5208096138400184e-06, 'batch_size': 206, 'step_size': 7, 'gamma': 0.8698559619235218}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:42:22,289][0m Trial 30 finished with value: 0.257289852601636 and parameters: {'observation_period_num': 130, 'train_rates': 0.7136282091162018, 'learning_rate': 0.0004734298085265022, 'batch_size': 145, 'step_size': 4, 'gamma': 0.7528850091632724}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:45:53,904][0m Trial 31 finished with value: 0.044710474912718026 and parameters: {'observation_period_num': 26, 'train_rates': 0.819442472241855, 'learning_rate': 0.00021253165409753892, 'batch_size': 25, 'step_size': 7, 'gamma': 0.7872333343123719}. Best is trial 23 with value: 0.0428722236766916.[0m
[32m[I 2025-02-05 08:51:33,756][0m Trial 32 finished with value: 0.04028962428693648 and parameters: {'observation_period_num': 24, 'train_rates': 0.8688354656185127, 'learning_rate': 0.00021691271582725304, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7872420368179396}. Best is trial 32 with value: 0.04028962428693648.[0m
[32m[I 2025-02-05 08:57:03,322][0m Trial 33 finished with value: 0.038382087086188045 and parameters: {'observation_period_num': 22, 'train_rates': 0.9133697064077807, 'learning_rate': 0.00028336055774678964, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7833772961903817}. Best is trial 33 with value: 0.038382087086188045.[0m
[32m[I 2025-02-05 08:58:53,771][0m Trial 34 finished with value: 0.034961034480790444 and parameters: {'observation_period_num': 18, 'train_rates': 0.9093399412762281, 'learning_rate': 0.00032726380136585676, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7795405408848887}. Best is trial 34 with value: 0.034961034480790444.[0m
[32m[I 2025-02-05 09:01:04,645][0m Trial 35 finished with value: 0.03342509332532976 and parameters: {'observation_period_num': 6, 'train_rates': 0.9136334970731501, 'learning_rate': 0.0003662611103123754, 'batch_size': 44, 'step_size': 3, 'gamma': 0.7729562282665482}. Best is trial 35 with value: 0.03342509332532976.[0m
[32m[I 2025-02-05 09:02:54,669][0m Trial 36 finished with value: 0.031909739337355844 and parameters: {'observation_period_num': 14, 'train_rates': 0.9249848181832773, 'learning_rate': 0.0006894424315495376, 'batch_size': 53, 'step_size': 3, 'gamma': 0.7639131537660816}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:04:36,712][0m Trial 37 finished with value: 0.03419598434930262 and parameters: {'observation_period_num': 8, 'train_rates': 0.9376909275053259, 'learning_rate': 0.0006886607301438631, 'batch_size': 57, 'step_size': 3, 'gamma': 0.761360888019643}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:06:34,231][0m Trial 38 finished with value: 0.03444423910918509 and parameters: {'observation_period_num': 14, 'train_rates': 0.9466478374182662, 'learning_rate': 0.0007191536888360551, 'batch_size': 50, 'step_size': 3, 'gamma': 0.7635169032359718}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:08:28,124][0m Trial 39 finished with value: 0.04061644047237279 and parameters: {'observation_period_num': 5, 'train_rates': 0.9421305773160908, 'learning_rate': 0.000736373713161979, 'batch_size': 52, 'step_size': 2, 'gamma': 0.7612354537841985}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:09:21,177][0m Trial 40 finished with value: 0.04146430641412735 and parameters: {'observation_period_num': 5, 'train_rates': 0.9816851922672277, 'learning_rate': 0.0009329313449585527, 'batch_size': 119, 'step_size': 3, 'gamma': 0.7632601398657353}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:11:20,526][0m Trial 41 finished with value: 0.03352618005091899 and parameters: {'observation_period_num': 17, 'train_rates': 0.9638618004386791, 'learning_rate': 0.0005335223334599604, 'batch_size': 50, 'step_size': 3, 'gamma': 0.7654640667988456}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:12:42,501][0m Trial 42 finished with value: 0.03833131957799196 and parameters: {'observation_period_num': 13, 'train_rates': 0.9592759838869335, 'learning_rate': 0.0006040150749161336, 'batch_size': 74, 'step_size': 3, 'gamma': 0.7623725112595783}. Best is trial 36 with value: 0.031909739337355844.[0m
Early stopping at epoch 55
[32m[I 2025-02-05 09:13:40,667][0m Trial 43 finished with value: 0.05241911391268915 and parameters: {'observation_period_num': 5, 'train_rates': 0.9318813643717416, 'learning_rate': 0.0009817594870552893, 'batch_size': 57, 'step_size': 1, 'gamma': 0.7521267244946263}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:16:32,559][0m Trial 44 finished with value: 0.044667754714426244 and parameters: {'observation_period_num': 35, 'train_rates': 0.9677222418633982, 'learning_rate': 0.0005674171623343834, 'batch_size': 34, 'step_size': 3, 'gamma': 0.7670726699147639}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:17:43,141][0m Trial 45 finished with value: 0.051807140254161575 and parameters: {'observation_period_num': 20, 'train_rates': 0.9401246290599139, 'learning_rate': 0.00038309035076508586, 'batch_size': 86, 'step_size': 2, 'gamma': 0.8338490382774156}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:19:44,847][0m Trial 46 finished with value: 0.2888874844008801 and parameters: {'observation_period_num': 55, 'train_rates': 0.9826401133999288, 'learning_rate': 4.034183885884846e-06, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9329825915318837}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:21:18,251][0m Trial 47 finished with value: 0.05083883191104484 and parameters: {'observation_period_num': 15, 'train_rates': 0.9232866768953485, 'learning_rate': 0.0006370000826104462, 'batch_size': 64, 'step_size': 2, 'gamma': 0.7525475536457894}. Best is trial 36 with value: 0.031909739337355844.[0m
[32m[I 2025-02-05 09:24:19,752][0m Trial 48 finished with value: 0.07312790830652197 and parameters: {'observation_period_num': 68, 'train_rates': 0.9504460281807503, 'learning_rate': 0.00043688960815096035, 'batch_size': 32, 'step_size': 4, 'gamma': 0.7669360872600757}. Best is trial 36 with value: 0.031909739337355844.[0m
Early stopping at epoch 67
[32m[I 2025-02-05 09:25:48,611][0m Trial 49 finished with value: 0.08821459557160292 and parameters: {'observation_period_num': 34, 'train_rates': 0.9655129550275834, 'learning_rate': 0.00030797728837509195, 'batch_size': 46, 'step_size': 1, 'gamma': 0.8082034313387111}. Best is trial 36 with value: 0.031909739337355844.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-05 09:25:48,620][0m A new study created in memory with name: no-name-bcc61e51-fc41-4d6a-b6fa-88fe6f6a197d[0m
[32m[I 2025-02-05 09:26:14,151][0m Trial 0 finished with value: 0.7973365498477204 and parameters: {'observation_period_num': 41, 'train_rates': 0.640425307381296, 'learning_rate': 3.3213297566445644e-06, 'batch_size': 198, 'step_size': 6, 'gamma': 0.8458664857850093}. Best is trial 0 with value: 0.7973365498477204.[0m
[32m[I 2025-02-05 09:31:04,736][0m Trial 1 finished with value: 0.17141289057501827 and parameters: {'observation_period_num': 65, 'train_rates': 0.7128366805150107, 'learning_rate': 8.224902059966213e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8091758332364163}. Best is trial 1 with value: 0.17141289057501827.[0m
[32m[I 2025-02-05 09:31:32,047][0m Trial 2 finished with value: 0.9635410308837891 and parameters: {'observation_period_num': 226, 'train_rates': 0.9309110483523882, 'learning_rate': 2.074012163151109e-06, 'batch_size': 215, 'step_size': 11, 'gamma': 0.8144207288706593}. Best is trial 1 with value: 0.17141289057501827.[0m
[32m[I 2025-02-05 09:32:44,906][0m Trial 3 finished with value: 0.145905003359873 and parameters: {'observation_period_num': 177, 'train_rates': 0.8161720535330899, 'learning_rate': 3.30222382546071e-05, 'batch_size': 70, 'step_size': 13, 'gamma': 0.7711698448552257}. Best is trial 3 with value: 0.145905003359873.[0m
[32m[I 2025-02-05 09:33:11,506][0m Trial 4 finished with value: 0.20428104893519328 and parameters: {'observation_period_num': 29, 'train_rates': 0.7554525733404043, 'learning_rate': 3.1325773457324036e-05, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8949170091294749}. Best is trial 3 with value: 0.145905003359873.[0m
[32m[I 2025-02-05 09:36:00,378][0m Trial 5 finished with value: 0.03904128077830005 and parameters: {'observation_period_num': 28, 'train_rates': 0.9199957386096558, 'learning_rate': 0.0003040755995916566, 'batch_size': 34, 'step_size': 6, 'gamma': 0.764661031696124}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:38:37,782][0m Trial 6 finished with value: 0.1649936705827713 and parameters: {'observation_period_num': 140, 'train_rates': 0.95042153796614, 'learning_rate': 1.078655204483943e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9470430735637262}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:39:16,456][0m Trial 7 finished with value: 0.209271047599253 and parameters: {'observation_period_num': 42, 'train_rates': 0.7517169874448437, 'learning_rate': 9.670928760645995e-05, 'batch_size': 140, 'step_size': 6, 'gamma': 0.8017768985235006}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:39:41,974][0m Trial 8 finished with value: 0.3341299436324647 and parameters: {'observation_period_num': 234, 'train_rates': 0.7432074315176258, 'learning_rate': 0.0002685843273101916, 'batch_size': 208, 'step_size': 6, 'gamma': 0.981538328099381}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:40:03,434][0m Trial 9 finished with value: 0.1531336932166028 and parameters: {'observation_period_num': 20, 'train_rates': 0.6069084016320728, 'learning_rate': 0.00017495124842725616, 'batch_size': 227, 'step_size': 12, 'gamma': 0.9350097685222474}. Best is trial 5 with value: 0.03904128077830005.[0m
Early stopping at epoch 64
[32m[I 2025-02-05 09:40:39,511][0m Trial 10 finished with value: 0.1581175329826646 and parameters: {'observation_period_num': 98, 'train_rates': 0.876022362692668, 'learning_rate': 0.0007279814793029783, 'batch_size': 102, 'step_size': 1, 'gamma': 0.7671321768265758}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:41:53,491][0m Trial 11 finished with value: 0.14191008018595833 and parameters: {'observation_period_num': 148, 'train_rates': 0.8504374844723422, 'learning_rate': 0.0009314602296036153, 'batch_size': 71, 'step_size': 15, 'gamma': 0.750313051872978}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:43:17,531][0m Trial 12 finished with value: 0.1108592905966211 and parameters: {'observation_period_num': 157, 'train_rates': 0.8703560247615998, 'learning_rate': 0.0009214205351874345, 'batch_size': 63, 'step_size': 15, 'gamma': 0.7631338519678592}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:43:58,373][0m Trial 13 finished with value: 0.14608411490917206 and parameters: {'observation_period_num': 183, 'train_rates': 0.9830434946573479, 'learning_rate': 0.0003752382815746643, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8681877314405684}. Best is trial 5 with value: 0.03904128077830005.[0m
Early stopping at epoch 61
[32m[I 2025-02-05 09:44:59,546][0m Trial 14 finished with value: 0.10935478582978249 and parameters: {'observation_period_num': 91, 'train_rates': 0.9004985650872556, 'learning_rate': 0.0009299924225662483, 'batch_size': 57, 'step_size': 1, 'gamma': 0.7891430530128207}. Best is trial 5 with value: 0.03904128077830005.[0m
Early stopping at epoch 74
[32m[I 2025-02-05 09:45:40,825][0m Trial 15 finished with value: 0.1773731550380188 and parameters: {'observation_period_num': 96, 'train_rates': 0.9089409217966353, 'learning_rate': 0.0003459850585146622, 'batch_size': 106, 'step_size': 1, 'gamma': 0.8393239078745771}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:47:53,308][0m Trial 16 finished with value: 0.07902551487007071 and parameters: {'observation_period_num': 82, 'train_rates': 0.8081651239980805, 'learning_rate': 0.00010506944742633083, 'batch_size': 39, 'step_size': 3, 'gamma': 0.7921567999344242}. Best is trial 5 with value: 0.03904128077830005.[0m
[32m[I 2025-02-05 09:53:17,755][0m Trial 17 finished with value: 0.03858417589344135 and parameters: {'observation_period_num': 6, 'train_rates': 0.8005452706366487, 'learning_rate': 3.9521706941618875e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8255256428427511}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 09:54:07,421][0m Trial 18 finished with value: 0.1897343974818924 and parameters: {'observation_period_num': 8, 'train_rates': 0.6819575878423526, 'learning_rate': 2.358448952978921e-05, 'batch_size': 100, 'step_size': 4, 'gamma': 0.8290701257111366}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 09:59:34,364][0m Trial 19 finished with value: 0.07932276146139129 and parameters: {'observation_period_num': 61, 'train_rates': 0.8370742887821904, 'learning_rate': 8.529185535022862e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8707555303960224}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:00:12,961][0m Trial 20 finished with value: 0.061576828360557556 and parameters: {'observation_period_num': 6, 'train_rates': 0.9802647565683914, 'learning_rate': 5.569322164010084e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.899758549728134}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:00:51,960][0m Trial 21 finished with value: 0.06316331773996353 and parameters: {'observation_period_num': 11, 'train_rates': 0.9885795426568507, 'learning_rate': 5.6068998594471496e-05, 'batch_size': 166, 'step_size': 7, 'gamma': 0.9060510982040968}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:01:43,267][0m Trial 22 finished with value: 0.13565701246261597 and parameters: {'observation_period_num': 5, 'train_rates': 0.9426385532804388, 'learning_rate': 1.8178445222908526e-05, 'batch_size': 120, 'step_size': 4, 'gamma': 0.860961448783078}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:02:10,818][0m Trial 23 finished with value: 0.15356166660785675 and parameters: {'observation_period_num': 53, 'train_rates': 0.9680503861660873, 'learning_rate': 4.8221676800966587e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9206080526642777}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:02:42,037][0m Trial 24 finished with value: 0.1989045983159935 and parameters: {'observation_period_num': 31, 'train_rates': 0.7821805447953006, 'learning_rate': 0.0001661950523915911, 'batch_size': 180, 'step_size': 3, 'gamma': 0.8878963233003684}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:04:51,194][0m Trial 25 finished with value: 0.16573756332764647 and parameters: {'observation_period_num': 119, 'train_rates': 0.9166271340514158, 'learning_rate': 8.174304364553983e-06, 'batch_size': 43, 'step_size': 5, 'gamma': 0.9616492301257219}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:05:30,736][0m Trial 26 finished with value: 1.0452243089675903 and parameters: {'observation_period_num': 73, 'train_rates': 0.9584475501449115, 'learning_rate': 1.0157399772160355e-06, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8294067131455981}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:06:37,642][0m Trial 27 finished with value: 0.050217644291051126 and parameters: {'observation_period_num': 42, 'train_rates': 0.8905299339313999, 'learning_rate': 0.00015717728565473204, 'batch_size': 86, 'step_size': 9, 'gamma': 0.7851207323012208}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:07:40,383][0m Trial 28 finished with value: 0.10389140798898736 and parameters: {'observation_period_num': 110, 'train_rates': 0.8802650735697127, 'learning_rate': 0.000494431711988538, 'batch_size': 89, 'step_size': 10, 'gamma': 0.7832137065802763}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:08:49,924][0m Trial 29 finished with value: 0.0458730598995546 and parameters: {'observation_period_num': 37, 'train_rates': 0.8478969511765082, 'learning_rate': 0.00019341455654231767, 'batch_size': 82, 'step_size': 9, 'gamma': 0.8220466916129441}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:12:02,701][0m Trial 30 finished with value: 0.04264685444867433 and parameters: {'observation_period_num': 32, 'train_rates': 0.8396685864210964, 'learning_rate': 0.0002490783995930442, 'batch_size': 28, 'step_size': 5, 'gamma': 0.8469160673525493}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:14:29,761][0m Trial 31 finished with value: 0.046351860868542094 and parameters: {'observation_period_num': 44, 'train_rates': 0.8419030709911405, 'learning_rate': 0.00025214079040432373, 'batch_size': 36, 'step_size': 3, 'gamma': 0.8533756799243063}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:19:43,582][0m Trial 32 finished with value: 0.18711797131192473 and parameters: {'observation_period_num': 29, 'train_rates': 0.7876332619777533, 'learning_rate': 0.00042372478621928813, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8241288365270367}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:21:26,246][0m Trial 33 finished with value: 0.0564929969652101 and parameters: {'observation_period_num': 67, 'train_rates': 0.8211774033149788, 'learning_rate': 0.00011135847532116528, 'batch_size': 51, 'step_size': 5, 'gamma': 0.8436715045930183}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:25:19,840][0m Trial 34 finished with value: 0.04052151878090466 and parameters: {'observation_period_num': 23, 'train_rates': 0.8551104245655025, 'learning_rate': 0.0002427883555921229, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8087379583869366}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:28:03,810][0m Trial 35 finished with value: 0.5321002387062684 and parameters: {'observation_period_num': 22, 'train_rates': 0.774487798250749, 'learning_rate': 4.8056366574214096e-06, 'batch_size': 31, 'step_size': 2, 'gamma': 0.8083555832675446}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:31:30,579][0m Trial 36 finished with value: 0.07084343506942821 and parameters: {'observation_period_num': 53, 'train_rates': 0.8639985763417524, 'learning_rate': 7.156044888510019e-05, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8050100870046318}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:32:55,663][0m Trial 37 finished with value: 0.27378248321131254 and parameters: {'observation_period_num': 211, 'train_rates': 0.7156355255065738, 'learning_rate': 0.0006132592955247919, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8810204304134298}. Best is trial 17 with value: 0.03858417589344135.[0m
[32m[I 2025-02-05 10:37:04,510][0m Trial 38 finished with value: 0.038459170039172645 and parameters: {'observation_period_num': 20, 'train_rates': 0.9244549557898518, 'learning_rate': 0.0002756471250624022, 'batch_size': 23, 'step_size': 2, 'gamma': 0.7561063237465689}. Best is trial 38 with value: 0.038459170039172645.[0m
Early stopping at epoch 82
[32m[I 2025-02-05 10:38:13,325][0m Trial 39 finished with value: 0.45857130531716134 and parameters: {'observation_period_num': 15, 'train_rates': 0.9233685791292594, 'learning_rate': 1.576844948223485e-05, 'batch_size': 71, 'step_size': 2, 'gamma': 0.7501944880827575}. Best is trial 38 with value: 0.038459170039172645.[0m
[32m[I 2025-02-05 10:40:07,071][0m Trial 40 finished with value: 0.14296844512869736 and parameters: {'observation_period_num': 50, 'train_rates': 0.8092565750462689, 'learning_rate': 3.65077296178754e-05, 'batch_size': 46, 'step_size': 3, 'gamma': 0.7748685568826851}. Best is trial 38 with value: 0.038459170039172645.[0m
[32m[I 2025-02-05 10:43:38,356][0m Trial 41 finished with value: 0.040337783790799595 and parameters: {'observation_period_num': 27, 'train_rates': 0.8278149033906177, 'learning_rate': 0.00025899883282530143, 'batch_size': 25, 'step_size': 6, 'gamma': 0.8006752083569504}. Best is trial 38 with value: 0.038459170039172645.[0m
[32m[I 2025-02-05 10:48:39,413][0m Trial 42 finished with value: 0.03592815404885976 and parameters: {'observation_period_num': 22, 'train_rates': 0.9357077771252033, 'learning_rate': 0.00012767965981185997, 'batch_size': 19, 'step_size': 6, 'gamma': 0.7960460402784234}. Best is trial 42 with value: 0.03592815404885976.[0m
[32m[I 2025-02-05 10:54:39,150][0m Trial 43 finished with value: 0.03846067832837328 and parameters: {'observation_period_num': 19, 'train_rates': 0.9419315286417487, 'learning_rate': 0.00012001521521353893, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7622916581366594}. Best is trial 42 with value: 0.03592815404885976.[0m
[32m[I 2025-02-05 11:00:15,666][0m Trial 44 finished with value: 0.027934818620754715 and parameters: {'observation_period_num': 16, 'train_rates': 0.9347066193836381, 'learning_rate': 0.00012878469187759236, 'batch_size': 17, 'step_size': 6, 'gamma': 0.760605987871093}. Best is trial 44 with value: 0.027934818620754715.[0m
[32m[I 2025-02-05 11:02:34,101][0m Trial 45 finished with value: 0.03574599479527577 and parameters: {'observation_period_num': 19, 'train_rates': 0.9374867156567674, 'learning_rate': 0.00012602431802488922, 'batch_size': 42, 'step_size': 7, 'gamma': 0.7609342641841048}. Best is trial 44 with value: 0.027934818620754715.[0m
[32m[I 2025-02-05 11:04:11,041][0m Trial 46 finished with value: 0.07331507662139787 and parameters: {'observation_period_num': 59, 'train_rates': 0.9437233645438453, 'learning_rate': 0.00012767860337180387, 'batch_size': 62, 'step_size': 7, 'gamma': 0.758745708348468}. Best is trial 44 with value: 0.027934818620754715.[0m
[32m[I 2025-02-05 11:06:25,151][0m Trial 47 finished with value: 0.07318164417908217 and parameters: {'observation_period_num': 76, 'train_rates': 0.9334956005707621, 'learning_rate': 0.000129546740178624, 'batch_size': 43, 'step_size': 10, 'gamma': 0.7727191980178019}. Best is trial 44 with value: 0.027934818620754715.[0m
[32m[I 2025-02-05 11:09:20,469][0m Trial 48 finished with value: 0.047967351220472895 and parameters: {'observation_period_num': 19, 'train_rates': 0.9663910423821804, 'learning_rate': 8.597346832399132e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.7571856551838068}. Best is trial 44 with value: 0.027934818620754715.[0m
[32m[I 2025-02-05 11:10:46,318][0m Trial 49 finished with value: 0.052733057769980746 and parameters: {'observation_period_num': 41, 'train_rates': 0.9062144652948941, 'learning_rate': 0.000530302244653741, 'batch_size': 67, 'step_size': 8, 'gamma': 0.777899737508881}. Best is trial 44 with value: 0.027934818620754715.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-05 11:10:46,329][0m A new study created in memory with name: no-name-f183bc2a-8664-4f89-bf1c-5cd61e3bafd3[0m
[32m[I 2025-02-05 11:12:21,495][0m Trial 0 finished with value: 0.5582396068694134 and parameters: {'observation_period_num': 135, 'train_rates': 0.6929007523131824, 'learning_rate': 1.8949260790758957e-06, 'batch_size': 48, 'step_size': 15, 'gamma': 0.9398380784645524}. Best is trial 0 with value: 0.5582396068694134.[0m
[32m[I 2025-02-05 11:15:53,423][0m Trial 1 finished with value: 0.23756122961640358 and parameters: {'observation_period_num': 164, 'train_rates': 0.9871471954652172, 'learning_rate': 7.013674570887591e-06, 'batch_size': 27, 'step_size': 10, 'gamma': 0.864867013034009}. Best is trial 1 with value: 0.23756122961640358.[0m
[32m[I 2025-02-05 11:16:15,791][0m Trial 2 finished with value: 0.3277609112709942 and parameters: {'observation_period_num': 33, 'train_rates': 0.7188962623124832, 'learning_rate': 1.9168454080778163e-05, 'batch_size': 251, 'step_size': 2, 'gamma': 0.9672824090474338}. Best is trial 1 with value: 0.23756122961640358.[0m
[32m[I 2025-02-05 11:16:39,973][0m Trial 3 finished with value: 0.310749143541818 and parameters: {'observation_period_num': 133, 'train_rates': 0.7354079227003685, 'learning_rate': 0.00022611579186353098, 'batch_size': 209, 'step_size': 1, 'gamma': 0.9355947145165747}. Best is trial 1 with value: 0.23756122961640358.[0m
[32m[I 2025-02-05 11:17:26,308][0m Trial 4 finished with value: 1.4583134661875499 and parameters: {'observation_period_num': 127, 'train_rates': 0.7632852408816584, 'learning_rate': 1.1839248794561776e-06, 'batch_size': 110, 'step_size': 10, 'gamma': 0.7614131696657478}. Best is trial 1 with value: 0.23756122961640358.[0m
[32m[I 2025-02-05 11:17:48,532][0m Trial 5 finished with value: 0.29206967891431324 and parameters: {'observation_period_num': 128, 'train_rates': 0.729242317109474, 'learning_rate': 4.092618440272327e-05, 'batch_size': 246, 'step_size': 15, 'gamma': 0.8341221969211768}. Best is trial 1 with value: 0.23756122961640358.[0m
[32m[I 2025-02-05 11:18:59,067][0m Trial 6 finished with value: 0.1163745753467083 and parameters: {'observation_period_num': 136, 'train_rates': 0.8297184818906458, 'learning_rate': 3.480033729716667e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9098318602500207}. Best is trial 6 with value: 0.1163745753467083.[0m
[32m[I 2025-02-05 11:19:47,546][0m Trial 7 finished with value: 0.3751781271851581 and parameters: {'observation_period_num': 189, 'train_rates': 0.7505691080558631, 'learning_rate': 2.8688151124988253e-05, 'batch_size': 100, 'step_size': 8, 'gamma': 0.7980285973171815}. Best is trial 6 with value: 0.1163745753467083.[0m
[32m[I 2025-02-05 11:20:07,889][0m Trial 8 finished with value: 1.791312990225118 and parameters: {'observation_period_num': 199, 'train_rates': 0.6680895720951746, 'learning_rate': 1.0830061599904655e-06, 'batch_size': 239, 'step_size': 4, 'gamma': 0.8106019106993366}. Best is trial 6 with value: 0.1163745753467083.[0m
[32m[I 2025-02-05 11:20:34,122][0m Trial 9 finished with value: 0.8581511647969227 and parameters: {'observation_period_num': 123, 'train_rates': 0.7547525191495441, 'learning_rate': 1.8152197281588805e-06, 'batch_size': 208, 'step_size': 9, 'gamma': 0.7841273958373993}. Best is trial 6 with value: 0.1163745753467083.[0m
[32m[I 2025-02-05 11:21:10,106][0m Trial 10 finished with value: 0.2386886495632478 and parameters: {'observation_period_num': 252, 'train_rates': 0.8663238277189367, 'learning_rate': 0.0006225516063616836, 'batch_size': 151, 'step_size': 7, 'gamma': 0.8899893540912088}. Best is trial 6 with value: 0.1163745753467083.[0m
[32m[I 2025-02-05 11:26:50,420][0m Trial 11 finished with value: 0.10731118934592981 and parameters: {'observation_period_num': 62, 'train_rates': 0.9700569525234156, 'learning_rate': 8.028023573569146e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8773112918841037}. Best is trial 11 with value: 0.10731118934592981.[0m
[32m[I 2025-02-05 11:28:17,219][0m Trial 12 finished with value: 0.06294090361372243 and parameters: {'observation_period_num': 52, 'train_rates': 0.872996617102772, 'learning_rate': 0.00010273607586871978, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8950176422476868}. Best is trial 12 with value: 0.06294090361372243.[0m
[32m[I 2025-02-05 11:32:47,394][0m Trial 13 finished with value: 0.06943347186252878 and parameters: {'observation_period_num': 47, 'train_rates': 0.936325747171161, 'learning_rate': 0.00014234389583349047, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8500505138992901}. Best is trial 12 with value: 0.06294090361372243.[0m
[32m[I 2025-02-05 11:34:12,773][0m Trial 14 finished with value: 0.03577377824178824 and parameters: {'observation_period_num': 7, 'train_rates': 0.904437881583961, 'learning_rate': 0.0001263851126566087, 'batch_size': 68, 'step_size': 13, 'gamma': 0.8393546439482878}. Best is trial 14 with value: 0.03577377824178824.[0m
[32m[I 2025-02-05 11:34:43,150][0m Trial 15 finished with value: 0.1286036676453973 and parameters: {'observation_period_num': 5, 'train_rates': 0.6034027345347031, 'learning_rate': 0.00011408848133623638, 'batch_size': 151, 'step_size': 13, 'gamma': 0.9080113167731777}. Best is trial 14 with value: 0.03577377824178824.[0m
[32m[I 2025-02-05 11:36:03,066][0m Trial 16 finished with value: 0.14611676298081874 and parameters: {'observation_period_num': 79, 'train_rates': 0.8956912860995748, 'learning_rate': 0.0009172501656525898, 'batch_size': 70, 'step_size': 13, 'gamma': 0.8230038174767825}. Best is trial 14 with value: 0.03577377824178824.[0m
[32m[I 2025-02-05 11:36:53,553][0m Trial 17 finished with value: 0.03339420156403978 and parameters: {'observation_period_num': 8, 'train_rates': 0.8096988905402002, 'learning_rate': 0.0003277494629021198, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9863436049050169}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:37:39,653][0m Trial 18 finished with value: 0.03381972894683935 and parameters: {'observation_period_num': 9, 'train_rates': 0.8221347717629188, 'learning_rate': 0.00036035672922771667, 'batch_size': 121, 'step_size': 5, 'gamma': 0.9841710827518484}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:38:22,996][0m Trial 19 finished with value: 0.0993745140144161 and parameters: {'observation_period_num': 85, 'train_rates': 0.8011046114902647, 'learning_rate': 0.00038968154362776505, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9767354221184145}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:38:55,305][0m Trial 20 finished with value: 0.0638320611849908 and parameters: {'observation_period_num': 29, 'train_rates': 0.8199387082832734, 'learning_rate': 0.00035612850047245595, 'batch_size': 175, 'step_size': 5, 'gamma': 0.9894856350061559}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:39:55,393][0m Trial 21 finished with value: 0.03846802128371128 and parameters: {'observation_period_num': 5, 'train_rates': 0.9180701955770283, 'learning_rate': 0.00023824415128544707, 'batch_size': 97, 'step_size': 6, 'gamma': 0.943927055592078}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:40:39,823][0m Trial 22 finished with value: 0.05404040960756611 and parameters: {'observation_period_num': 22, 'train_rates': 0.850812047136764, 'learning_rate': 6.942015525142212e-05, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9690315060851376}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:41:38,110][0m Trial 23 finished with value: 0.22919331926336273 and parameters: {'observation_period_num': 78, 'train_rates': 0.7915811942054367, 'learning_rate': 0.0006137263668751231, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9510824655298278}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:43:35,853][0m Trial 24 finished with value: 0.039110244472616726 and parameters: {'observation_period_num': 21, 'train_rates': 0.9008257860440355, 'learning_rate': 0.0002454646425806991, 'batch_size': 48, 'step_size': 4, 'gamma': 0.924562285770316}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:44:14,967][0m Trial 25 finished with value: 0.1855863332748413 and parameters: {'observation_period_num': 93, 'train_rates': 0.9505871459786972, 'learning_rate': 5.8836134313111025e-05, 'batch_size': 154, 'step_size': 6, 'gamma': 0.849489919791978}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:45:00,539][0m Trial 26 finished with value: 0.1929303531191851 and parameters: {'observation_period_num': 43, 'train_rates': 0.7912195996545142, 'learning_rate': 0.0001507374847049968, 'batch_size': 118, 'step_size': 3, 'gamma': 0.9629318664617567}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:45:34,909][0m Trial 27 finished with value: 0.036483154699918884 and parameters: {'observation_period_num': 5, 'train_rates': 0.836847245909065, 'learning_rate': 0.00037504186274932, 'batch_size': 173, 'step_size': 7, 'gamma': 0.9889499212921967}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:47:39,949][0m Trial 28 finished with value: 0.10681204881570111 and parameters: {'observation_period_num': 64, 'train_rates': 0.8738725812258481, 'learning_rate': 0.0009683430531529969, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9252929161201903}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:48:38,439][0m Trial 29 finished with value: 0.16462512360352946 and parameters: {'observation_period_num': 22, 'train_rates': 0.7017067795764779, 'learning_rate': 0.0005491092498220231, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9519688548536724}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:49:12,730][0m Trial 30 finished with value: 0.21065701398680958 and parameters: {'observation_period_num': 96, 'train_rates': 0.6634838977170423, 'learning_rate': 7.705051338088696e-05, 'batch_size': 140, 'step_size': 10, 'gamma': 0.8644143638915093}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:49:45,872][0m Trial 31 finished with value: 0.03747535436004705 and parameters: {'observation_period_num': 9, 'train_rates': 0.8341239328080267, 'learning_rate': 0.0003614521323629479, 'batch_size': 178, 'step_size': 7, 'gamma': 0.9847632113485384}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:50:18,113][0m Trial 32 finished with value: 0.08960131169517765 and parameters: {'observation_period_num': 36, 'train_rates': 0.8049331973124093, 'learning_rate': 0.00018735636447445958, 'batch_size': 180, 'step_size': 8, 'gamma': 0.986350484816754}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:50:54,336][0m Trial 33 finished with value: 0.0388015241726585 and parameters: {'observation_period_num': 17, 'train_rates': 0.8435871587150843, 'learning_rate': 0.0003659239989591481, 'batch_size': 164, 'step_size': 6, 'gamma': 0.9779361984753406}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:51:25,792][0m Trial 34 finished with value: 0.1265766976439223 and parameters: {'observation_period_num': 37, 'train_rates': 0.8992614457361203, 'learning_rate': 1.7334061855851174e-05, 'batch_size': 198, 'step_size': 9, 'gamma': 0.9590476172193434}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:52:05,576][0m Trial 35 finished with value: 0.2197460798229815 and parameters: {'observation_period_num': 56, 'train_rates': 0.7828696237521252, 'learning_rate': 0.00025799984100002344, 'batch_size': 138, 'step_size': 1, 'gamma': 0.93457593339347}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:53:02,761][0m Trial 36 finished with value: 0.09737459947208515 and parameters: {'observation_period_num': 30, 'train_rates': 0.8589334896237606, 'learning_rate': 0.0005755613292192077, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9709369295616054}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:54:54,107][0m Trial 37 finished with value: 0.04552389793529474 and parameters: {'observation_period_num': 14, 'train_rates': 0.8239116037769809, 'learning_rate': 0.00017135409531453884, 'batch_size': 48, 'step_size': 4, 'gamma': 0.7561255772031706}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:55:40,553][0m Trial 38 finished with value: 0.22375404571785645 and parameters: {'observation_period_num': 70, 'train_rates': 0.7761942409427932, 'learning_rate': 4.9910765658042605e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8363337752315331}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:56:10,110][0m Trial 39 finished with value: 0.585355240557848 and parameters: {'observation_period_num': 162, 'train_rates': 0.8865849329341285, 'learning_rate': 2.108043620648354e-05, 'batch_size': 195, 'step_size': 3, 'gamma': 0.7775029445551204}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:57:48,096][0m Trial 40 finished with value: 0.48179835194478865 and parameters: {'observation_period_num': 103, 'train_rates': 0.9354451835620553, 'learning_rate': 6.6622901941128285e-06, 'batch_size': 59, 'step_size': 6, 'gamma': 0.8129152985630657}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:58:22,029][0m Trial 41 finished with value: 0.03851544698935096 and parameters: {'observation_period_num': 5, 'train_rates': 0.838790949301647, 'learning_rate': 0.00037393761099787135, 'batch_size': 167, 'step_size': 7, 'gamma': 0.9832357393781216}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:58:47,937][0m Trial 42 finished with value: 0.04542368459118198 and parameters: {'observation_period_num': 14, 'train_rates': 0.824403102518415, 'learning_rate': 0.000298989372257215, 'batch_size': 221, 'step_size': 7, 'gamma': 0.9896028606585845}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:59:17,997][0m Trial 43 finished with value: 0.20700427793582743 and parameters: {'observation_period_num': 41, 'train_rates': 0.7635591541306229, 'learning_rate': 0.0004753591980240526, 'batch_size': 185, 'step_size': 8, 'gamma': 0.9661964175939264}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 11:59:43,885][0m Trial 44 finished with value: 0.05927937953005524 and parameters: {'observation_period_num': 25, 'train_rates': 0.813017116421463, 'learning_rate': 0.00010074293694966934, 'batch_size': 225, 'step_size': 5, 'gamma': 0.9579195237077995}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 12:00:24,864][0m Trial 45 finished with value: 0.14793744407875908 and parameters: {'observation_period_num': 11, 'train_rates': 0.7394830818200034, 'learning_rate': 0.0008031108626592878, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9426856691781839}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 12:01:28,956][0m Trial 46 finished with value: 0.07863141254832347 and parameters: {'observation_period_num': 46, 'train_rates': 0.843404963058425, 'learning_rate': 0.00020580262031728725, 'batch_size': 85, 'step_size': 9, 'gamma': 0.9698383602608566}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 12:02:11,380][0m Trial 47 finished with value: 0.19816619157791138 and parameters: {'observation_period_num': 232, 'train_rates': 0.9865149208412881, 'learning_rate': 0.00011791630360829757, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9784509116077087}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 12:02:49,413][0m Trial 48 finished with value: 0.05511034325022122 and parameters: {'observation_period_num': 35, 'train_rates': 0.8809616481534202, 'learning_rate': 0.0007120538393704157, 'batch_size': 158, 'step_size': 4, 'gamma': 0.8818693526207728}. Best is trial 17 with value: 0.03339420156403978.[0m
[32m[I 2025-02-05 12:03:22,529][0m Trial 49 finished with value: 0.0980901478205697 and parameters: {'observation_period_num': 53, 'train_rates': 0.9187652128822582, 'learning_rate': 0.00030067673248451536, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9307109700493342}. Best is trial 17 with value: 0.03339420156403978.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 20, 'train_rates': 0.9561786315286788, 'learning_rate': 0.0005946047055748292, 'batch_size': 160, 'step_size': 7, 'gamma': 0.7640200006019118}
Epoch 1/300, trend Loss: 0.6036 | 0.3797
Epoch 2/300, trend Loss: 0.2171 | 0.2242
Epoch 3/300, trend Loss: 0.2994 | 0.1748
Epoch 4/300, trend Loss: 0.4571 | 1.3960
Epoch 5/300, trend Loss: 0.2024 | 0.2616
Epoch 6/300, trend Loss: 0.1876 | 0.3300
Epoch 7/300, trend Loss: 0.1457 | 0.2149
Epoch 8/300, trend Loss: 0.1545 | 0.2065
Epoch 9/300, trend Loss: 0.1438 | 0.1527
Epoch 10/300, trend Loss: 0.1527 | 0.1686
Epoch 11/300, trend Loss: 0.1268 | 0.1303
Epoch 12/300, trend Loss: 0.1201 | 0.1442
Epoch 13/300, trend Loss: 0.1476 | 0.1467
Epoch 14/300, trend Loss: 0.1450 | 0.1865
Epoch 15/300, trend Loss: 0.1188 | 0.1083
Epoch 16/300, trend Loss: 0.1267 | 0.1084
Epoch 17/300, trend Loss: 0.1179 | 0.1696
Epoch 18/300, trend Loss: 0.1237 | 0.1054
Epoch 19/300, trend Loss: 0.1186 | 0.1226
Epoch 20/300, trend Loss: 0.1203 | 0.1158
Epoch 21/300, trend Loss: 0.1289 | 0.1420
Epoch 22/300, trend Loss: 0.1407 | 0.2093
Epoch 23/300, trend Loss: 0.1312 | 0.1093
Epoch 24/300, trend Loss: 0.1208 | 0.1592
Epoch 25/300, trend Loss: 0.1139 | 0.1035
Epoch 26/300, trend Loss: 0.1056 | 0.1286
Epoch 27/300, trend Loss: 0.1009 | 0.0934
Epoch 28/300, trend Loss: 0.0987 | 0.1047
Epoch 29/300, trend Loss: 0.0962 | 0.0889
Epoch 30/300, trend Loss: 0.0953 | 0.0932
Epoch 31/300, trend Loss: 0.0942 | 0.0862
Epoch 32/300, trend Loss: 0.0937 | 0.0869
Epoch 33/300, trend Loss: 0.0930 | 0.0838
Epoch 34/300, trend Loss: 0.0926 | 0.0837
Epoch 35/300, trend Loss: 0.0922 | 0.0819
Epoch 36/300, trend Loss: 0.0918 | 0.0815
Epoch 37/300, trend Loss: 0.0916 | 0.0806
Epoch 38/300, trend Loss: 0.0913 | 0.0800
Epoch 39/300, trend Loss: 0.0910 | 0.0793
Epoch 40/300, trend Loss: 0.0908 | 0.0789
Epoch 41/300, trend Loss: 0.0906 | 0.0784
Epoch 42/300, trend Loss: 0.0904 | 0.0779
Epoch 43/300, trend Loss: 0.0902 | 0.0776
Epoch 44/300, trend Loss: 0.0900 | 0.0773
Epoch 45/300, trend Loss: 0.0899 | 0.0770
Epoch 46/300, trend Loss: 0.0897 | 0.0767
Epoch 47/300, trend Loss: 0.0896 | 0.0765
Epoch 48/300, trend Loss: 0.0895 | 0.0763
Epoch 49/300, trend Loss: 0.0894 | 0.0760
Epoch 50/300, trend Loss: 0.0892 | 0.0759
Epoch 51/300, trend Loss: 0.0892 | 0.0757
Epoch 52/300, trend Loss: 0.0891 | 0.0755
Epoch 53/300, trend Loss: 0.0890 | 0.0754
Epoch 54/300, trend Loss: 0.0889 | 0.0753
Epoch 55/300, trend Loss: 0.0888 | 0.0751
Epoch 56/300, trend Loss: 0.0888 | 0.0750
Epoch 57/300, trend Loss: 0.0887 | 0.0749
Epoch 58/300, trend Loss: 0.0887 | 0.0748
Epoch 59/300, trend Loss: 0.0886 | 0.0747
Epoch 60/300, trend Loss: 0.0886 | 0.0747
Epoch 61/300, trend Loss: 0.0885 | 0.0746
Epoch 62/300, trend Loss: 0.0885 | 0.0745
Epoch 63/300, trend Loss: 0.0884 | 0.0744
Epoch 64/300, trend Loss: 0.0884 | 0.0744
Epoch 65/300, trend Loss: 0.0884 | 0.0743
Epoch 66/300, trend Loss: 0.0883 | 0.0743
Epoch 67/300, trend Loss: 0.0883 | 0.0742
Epoch 68/300, trend Loss: 0.0883 | 0.0742
Epoch 69/300, trend Loss: 0.0882 | 0.0741
Epoch 70/300, trend Loss: 0.0882 | 0.0741
Epoch 71/300, trend Loss: 0.0882 | 0.0741
Epoch 72/300, trend Loss: 0.0882 | 0.0740
Epoch 73/300, trend Loss: 0.0882 | 0.0740
Epoch 74/300, trend Loss: 0.0881 | 0.0740
Epoch 75/300, trend Loss: 0.0881 | 0.0739
Epoch 76/300, trend Loss: 0.0881 | 0.0739
Epoch 77/300, trend Loss: 0.0881 | 0.0739
Epoch 78/300, trend Loss: 0.0881 | 0.0739
Epoch 79/300, trend Loss: 0.0881 | 0.0739
Epoch 80/300, trend Loss: 0.0881 | 0.0738
Epoch 81/300, trend Loss: 0.0880 | 0.0738
Epoch 82/300, trend Loss: 0.0880 | 0.0738
Epoch 83/300, trend Loss: 0.0880 | 0.0738
Epoch 84/300, trend Loss: 0.0880 | 0.0738
Epoch 85/300, trend Loss: 0.0880 | 0.0738
Epoch 86/300, trend Loss: 0.0880 | 0.0737
Epoch 87/300, trend Loss: 0.0880 | 0.0737
Epoch 88/300, trend Loss: 0.0880 | 0.0737
Epoch 89/300, trend Loss: 0.0880 | 0.0737
Epoch 90/300, trend Loss: 0.0880 | 0.0737
Epoch 91/300, trend Loss: 0.0880 | 0.0737
Epoch 92/300, trend Loss: 0.0880 | 0.0737
Epoch 93/300, trend Loss: 0.0880 | 0.0737
Epoch 94/300, trend Loss: 0.0879 | 0.0737
Epoch 95/300, trend Loss: 0.0879 | 0.0737
Epoch 96/300, trend Loss: 0.0879 | 0.0737
Epoch 97/300, trend Loss: 0.0879 | 0.0737
Epoch 98/300, trend Loss: 0.0879 | 0.0737
Epoch 99/300, trend Loss: 0.0879 | 0.0736
Epoch 100/300, trend Loss: 0.0879 | 0.0736
Epoch 101/300, trend Loss: 0.0879 | 0.0736
Epoch 102/300, trend Loss: 0.0879 | 0.0736
Epoch 103/300, trend Loss: 0.0879 | 0.0736
Epoch 104/300, trend Loss: 0.0879 | 0.0736
Epoch 105/300, trend Loss: 0.0879 | 0.0736
Epoch 106/300, trend Loss: 0.0879 | 0.0736
Epoch 107/300, trend Loss: 0.0879 | 0.0736
Epoch 108/300, trend Loss: 0.0879 | 0.0736
Epoch 109/300, trend Loss: 0.0879 | 0.0736
Epoch 110/300, trend Loss: 0.0879 | 0.0736
Epoch 111/300, trend Loss: 0.0879 | 0.0736
Epoch 112/300, trend Loss: 0.0879 | 0.0736
Epoch 113/300, trend Loss: 0.0879 | 0.0736
Epoch 114/300, trend Loss: 0.0879 | 0.0736
Epoch 115/300, trend Loss: 0.0879 | 0.0736
Epoch 116/300, trend Loss: 0.0879 | 0.0736
Epoch 117/300, trend Loss: 0.0879 | 0.0736
Epoch 118/300, trend Loss: 0.0879 | 0.0736
Epoch 119/300, trend Loss: 0.0879 | 0.0736
Epoch 120/300, trend Loss: 0.0879 | 0.0736
Epoch 121/300, trend Loss: 0.0879 | 0.0736
Epoch 122/300, trend Loss: 0.0879 | 0.0736
Epoch 123/300, trend Loss: 0.0879 | 0.0736
Epoch 124/300, trend Loss: 0.0879 | 0.0736
Epoch 125/300, trend Loss: 0.0879 | 0.0736
Epoch 126/300, trend Loss: 0.0879 | 0.0736
Epoch 127/300, trend Loss: 0.0879 | 0.0736
Epoch 128/300, trend Loss: 0.0879 | 0.0736
Epoch 129/300, trend Loss: 0.0879 | 0.0736
Epoch 130/300, trend Loss: 0.0879 | 0.0736
Epoch 131/300, trend Loss: 0.0879 | 0.0736
Epoch 132/300, trend Loss: 0.0879 | 0.0736
Epoch 133/300, trend Loss: 0.0879 | 0.0736
Epoch 134/300, trend Loss: 0.0879 | 0.0736
Epoch 135/300, trend Loss: 0.0879 | 0.0736
Epoch 136/300, trend Loss: 0.0879 | 0.0736
Epoch 137/300, trend Loss: 0.0879 | 0.0736
Epoch 138/300, trend Loss: 0.0879 | 0.0736
Epoch 139/300, trend Loss: 0.0879 | 0.0736
Epoch 140/300, trend Loss: 0.0879 | 0.0736
Epoch 141/300, trend Loss: 0.0879 | 0.0736
Epoch 142/300, trend Loss: 0.0879 | 0.0736
Epoch 143/300, trend Loss: 0.0879 | 0.0736
Epoch 144/300, trend Loss: 0.0879 | 0.0736
Epoch 145/300, trend Loss: 0.0879 | 0.0736
Epoch 146/300, trend Loss: 0.0879 | 0.0736
Epoch 147/300, trend Loss: 0.0879 | 0.0736
Epoch 148/300, trend Loss: 0.0879 | 0.0736
Epoch 149/300, trend Loss: 0.0879 | 0.0736
Epoch 150/300, trend Loss: 0.0879 | 0.0736
Epoch 151/300, trend Loss: 0.0879 | 0.0736
Epoch 152/300, trend Loss: 0.0879 | 0.0736
Epoch 153/300, trend Loss: 0.0879 | 0.0736
Epoch 154/300, trend Loss: 0.0879 | 0.0736
Epoch 155/300, trend Loss: 0.0879 | 0.0736
Epoch 156/300, trend Loss: 0.0879 | 0.0736
Epoch 157/300, trend Loss: 0.0879 | 0.0736
Epoch 158/300, trend Loss: 0.0879 | 0.0736
Epoch 159/300, trend Loss: 0.0879 | 0.0736
Epoch 160/300, trend Loss: 0.0879 | 0.0736
Epoch 161/300, trend Loss: 0.0879 | 0.0736
Epoch 162/300, trend Loss: 0.0879 | 0.0736
Epoch 163/300, trend Loss: 0.0879 | 0.0736
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 14, 'train_rates': 0.899864258804407, 'learning_rate': 8.09229056485971e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.9183072606594335}
Epoch 1/300, seasonal_0 Loss: 0.7118 | 0.6021
Epoch 2/300, seasonal_0 Loss: 0.3254 | 0.4359
Epoch 3/300, seasonal_0 Loss: 0.2103 | 0.2768
Epoch 4/300, seasonal_0 Loss: 0.1826 | 0.2523
Epoch 5/300, seasonal_0 Loss: 0.1683 | 0.2453
Epoch 6/300, seasonal_0 Loss: 0.1612 | 0.2145
Epoch 7/300, seasonal_0 Loss: 0.1523 | 0.1869
Epoch 8/300, seasonal_0 Loss: 0.1456 | 0.1693
Epoch 9/300, seasonal_0 Loss: 0.1394 | 0.1547
Epoch 10/300, seasonal_0 Loss: 0.1340 | 0.1407
Epoch 11/300, seasonal_0 Loss: 0.1300 | 0.1277
Epoch 12/300, seasonal_0 Loss: 0.1273 | 0.1175
Epoch 13/300, seasonal_0 Loss: 0.1256 | 0.1110
Epoch 14/300, seasonal_0 Loss: 0.1244 | 0.1087
Epoch 15/300, seasonal_0 Loss: 0.1222 | 0.1062
Epoch 16/300, seasonal_0 Loss: 0.1184 | 0.0984
Epoch 17/300, seasonal_0 Loss: 0.1175 | 0.1025
Epoch 18/300, seasonal_0 Loss: 0.1253 | 0.1381
Epoch 19/300, seasonal_0 Loss: 0.1337 | 0.1869
Epoch 20/300, seasonal_0 Loss: 0.1281 | 0.1077
Epoch 21/300, seasonal_0 Loss: 0.1206 | 0.1049
Epoch 22/300, seasonal_0 Loss: 0.1227 | 0.1039
Epoch 23/300, seasonal_0 Loss: 0.1132 | 0.0891
Epoch 24/300, seasonal_0 Loss: 0.1086 | 0.0903
Epoch 25/300, seasonal_0 Loss: 0.1071 | 0.0861
Epoch 26/300, seasonal_0 Loss: 0.1061 | 0.0853
Epoch 27/300, seasonal_0 Loss: 0.1057 | 0.0847
Epoch 28/300, seasonal_0 Loss: 0.1047 | 0.0829
Epoch 29/300, seasonal_0 Loss: 0.1036 | 0.0812
Epoch 30/300, seasonal_0 Loss: 0.1029 | 0.0802
Epoch 31/300, seasonal_0 Loss: 0.1023 | 0.0795
Epoch 32/300, seasonal_0 Loss: 0.1018 | 0.0787
Epoch 33/300, seasonal_0 Loss: 0.1012 | 0.0781
Epoch 34/300, seasonal_0 Loss: 0.1006 | 0.0778
Epoch 35/300, seasonal_0 Loss: 0.1000 | 0.0779
Epoch 36/300, seasonal_0 Loss: 0.0994 | 0.0780
Epoch 37/300, seasonal_0 Loss: 0.0989 | 0.0769
Epoch 38/300, seasonal_0 Loss: 0.0983 | 0.0753
Epoch 39/300, seasonal_0 Loss: 0.0980 | 0.0742
Epoch 40/300, seasonal_0 Loss: 0.0980 | 0.0739
Epoch 41/300, seasonal_0 Loss: 0.0982 | 0.0745
Epoch 42/300, seasonal_0 Loss: 0.0981 | 0.0751
Epoch 43/300, seasonal_0 Loss: 0.0976 | 0.0746
Epoch 44/300, seasonal_0 Loss: 0.0966 | 0.0718
Epoch 45/300, seasonal_0 Loss: 0.0961 | 0.0752
Epoch 46/300, seasonal_0 Loss: 0.0973 | 0.0859
Epoch 47/300, seasonal_0 Loss: 0.0992 | 0.0922
Epoch 48/300, seasonal_0 Loss: 0.0985 | 0.0815
Epoch 49/300, seasonal_0 Loss: 0.0960 | 0.0707
Epoch 50/300, seasonal_0 Loss: 0.0960 | 0.0742
Epoch 51/300, seasonal_0 Loss: 0.0977 | 0.0878
Epoch 52/300, seasonal_0 Loss: 0.0978 | 0.0816
Epoch 53/300, seasonal_0 Loss: 0.0953 | 0.0690
Epoch 54/300, seasonal_0 Loss: 0.0934 | 0.0713
Epoch 55/300, seasonal_0 Loss: 0.0937 | 0.0756
Epoch 56/300, seasonal_0 Loss: 0.0930 | 0.0718
Epoch 57/300, seasonal_0 Loss: 0.0921 | 0.0667
Epoch 58/300, seasonal_0 Loss: 0.0917 | 0.0670
Epoch 59/300, seasonal_0 Loss: 0.0912 | 0.0675
Epoch 60/300, seasonal_0 Loss: 0.0904 | 0.0654
Epoch 61/300, seasonal_0 Loss: 0.0898 | 0.0644
Epoch 62/300, seasonal_0 Loss: 0.0895 | 0.0652
Epoch 63/300, seasonal_0 Loss: 0.0892 | 0.0652
Epoch 64/300, seasonal_0 Loss: 0.0888 | 0.0638
Epoch 65/300, seasonal_0 Loss: 0.0884 | 0.0626
Epoch 66/300, seasonal_0 Loss: 0.0885 | 0.0628
Epoch 67/300, seasonal_0 Loss: 0.0888 | 0.0626
Epoch 68/300, seasonal_0 Loss: 0.0889 | 0.0617
Epoch 69/300, seasonal_0 Loss: 0.0885 | 0.0614
Epoch 70/300, seasonal_0 Loss: 0.0880 | 0.0615
Epoch 71/300, seasonal_0 Loss: 0.0875 | 0.0613
Epoch 72/300, seasonal_0 Loss: 0.0872 | 0.0600
Epoch 73/300, seasonal_0 Loss: 0.0871 | 0.0594
Epoch 74/300, seasonal_0 Loss: 0.0870 | 0.0591
Epoch 75/300, seasonal_0 Loss: 0.0866 | 0.0585
Epoch 76/300, seasonal_0 Loss: 0.0858 | 0.0581
Epoch 77/300, seasonal_0 Loss: 0.0852 | 0.0584
Epoch 78/300, seasonal_0 Loss: 0.0852 | 0.0592
Epoch 79/300, seasonal_0 Loss: 0.0859 | 0.0592
Epoch 80/300, seasonal_0 Loss: 0.0872 | 0.0593
Epoch 81/300, seasonal_0 Loss: 0.0887 | 0.0597
Epoch 82/300, seasonal_0 Loss: 0.0899 | 0.0599
Epoch 83/300, seasonal_0 Loss: 0.0903 | 0.0594
Epoch 84/300, seasonal_0 Loss: 0.0894 | 0.0580
Epoch 85/300, seasonal_0 Loss: 0.0893 | 0.0579
Epoch 86/300, seasonal_0 Loss: 0.0936 | 0.0633
Epoch 87/300, seasonal_0 Loss: 0.1018 | 0.0677
Epoch 88/300, seasonal_0 Loss: 0.1033 | 0.0614
Epoch 89/300, seasonal_0 Loss: 0.0910 | 0.0572
Epoch 90/300, seasonal_0 Loss: 0.0867 | 0.0579
Epoch 91/300, seasonal_0 Loss: 0.0914 | 0.0717
Epoch 92/300, seasonal_0 Loss: 0.0923 | 0.0680
Epoch 93/300, seasonal_0 Loss: 0.0873 | 0.0554
Epoch 94/300, seasonal_0 Loss: 0.0845 | 0.0578
Epoch 95/300, seasonal_0 Loss: 0.0842 | 0.0554
Epoch 96/300, seasonal_0 Loss: 0.0824 | 0.0541
Epoch 97/300, seasonal_0 Loss: 0.0820 | 0.0545
Epoch 98/300, seasonal_0 Loss: 0.0820 | 0.0542
Epoch 99/300, seasonal_0 Loss: 0.0817 | 0.0536
Epoch 100/300, seasonal_0 Loss: 0.0815 | 0.0533
Epoch 101/300, seasonal_0 Loss: 0.0813 | 0.0531
Epoch 102/300, seasonal_0 Loss: 0.0810 | 0.0529
Epoch 103/300, seasonal_0 Loss: 0.0809 | 0.0527
Epoch 104/300, seasonal_0 Loss: 0.0808 | 0.0525
Epoch 105/300, seasonal_0 Loss: 0.0806 | 0.0523
Epoch 106/300, seasonal_0 Loss: 0.0805 | 0.0521
Epoch 107/300, seasonal_0 Loss: 0.0804 | 0.0519
Epoch 108/300, seasonal_0 Loss: 0.0802 | 0.0517
Epoch 109/300, seasonal_0 Loss: 0.0801 | 0.0516
Epoch 110/300, seasonal_0 Loss: 0.0800 | 0.0514
Epoch 111/300, seasonal_0 Loss: 0.0799 | 0.0512
Epoch 112/300, seasonal_0 Loss: 0.0798 | 0.0511
Epoch 113/300, seasonal_0 Loss: 0.0796 | 0.0509
Epoch 114/300, seasonal_0 Loss: 0.0795 | 0.0508
Epoch 115/300, seasonal_0 Loss: 0.0794 | 0.0506
Epoch 116/300, seasonal_0 Loss: 0.0793 | 0.0505
Epoch 117/300, seasonal_0 Loss: 0.0792 | 0.0503
Epoch 118/300, seasonal_0 Loss: 0.0791 | 0.0502
Epoch 119/300, seasonal_0 Loss: 0.0790 | 0.0500
Epoch 120/300, seasonal_0 Loss: 0.0789 | 0.0499
Epoch 121/300, seasonal_0 Loss: 0.0788 | 0.0498
Epoch 122/300, seasonal_0 Loss: 0.0787 | 0.0497
Epoch 123/300, seasonal_0 Loss: 0.0786 | 0.0496
Epoch 124/300, seasonal_0 Loss: 0.0786 | 0.0494
Epoch 125/300, seasonal_0 Loss: 0.0785 | 0.0493
Epoch 126/300, seasonal_0 Loss: 0.0784 | 0.0492
Epoch 127/300, seasonal_0 Loss: 0.0783 | 0.0491
Epoch 128/300, seasonal_0 Loss: 0.0782 | 0.0490
Epoch 129/300, seasonal_0 Loss: 0.0781 | 0.0489
Epoch 130/300, seasonal_0 Loss: 0.0780 | 0.0488
Epoch 131/300, seasonal_0 Loss: 0.0780 | 0.0487
Epoch 132/300, seasonal_0 Loss: 0.0779 | 0.0486
Epoch 133/300, seasonal_0 Loss: 0.0778 | 0.0485
Epoch 134/300, seasonal_0 Loss: 0.0777 | 0.0484
Epoch 135/300, seasonal_0 Loss: 0.0777 | 0.0483
Epoch 136/300, seasonal_0 Loss: 0.0776 | 0.0483
Epoch 137/300, seasonal_0 Loss: 0.0775 | 0.0482
Epoch 138/300, seasonal_0 Loss: 0.0774 | 0.0481
Epoch 139/300, seasonal_0 Loss: 0.0774 | 0.0480
Epoch 140/300, seasonal_0 Loss: 0.0773 | 0.0479
Epoch 141/300, seasonal_0 Loss: 0.0772 | 0.0478
Epoch 142/300, seasonal_0 Loss: 0.0772 | 0.0478
Epoch 143/300, seasonal_0 Loss: 0.0771 | 0.0477
Epoch 144/300, seasonal_0 Loss: 0.0770 | 0.0476
Epoch 145/300, seasonal_0 Loss: 0.0770 | 0.0476
Epoch 146/300, seasonal_0 Loss: 0.0769 | 0.0475
Epoch 147/300, seasonal_0 Loss: 0.0768 | 0.0474
Epoch 148/300, seasonal_0 Loss: 0.0767 | 0.0473
Epoch 149/300, seasonal_0 Loss: 0.0767 | 0.0473
Epoch 150/300, seasonal_0 Loss: 0.0766 | 0.0472
Epoch 151/300, seasonal_0 Loss: 0.0766 | 0.0472
Epoch 152/300, seasonal_0 Loss: 0.0765 | 0.0471
Epoch 153/300, seasonal_0 Loss: 0.0764 | 0.0470
Epoch 154/300, seasonal_0 Loss: 0.0764 | 0.0470
Epoch 155/300, seasonal_0 Loss: 0.0763 | 0.0469
Epoch 156/300, seasonal_0 Loss: 0.0763 | 0.0468
Epoch 157/300, seasonal_0 Loss: 0.0762 | 0.0468
Epoch 158/300, seasonal_0 Loss: 0.0761 | 0.0467
Epoch 159/300, seasonal_0 Loss: 0.0761 | 0.0467
Epoch 160/300, seasonal_0 Loss: 0.0760 | 0.0466
Epoch 161/300, seasonal_0 Loss: 0.0760 | 0.0466
Epoch 162/300, seasonal_0 Loss: 0.0759 | 0.0465
Epoch 163/300, seasonal_0 Loss: 0.0759 | 0.0465
Epoch 164/300, seasonal_0 Loss: 0.0758 | 0.0464
Epoch 165/300, seasonal_0 Loss: 0.0758 | 0.0464
Epoch 166/300, seasonal_0 Loss: 0.0757 | 0.0463
Epoch 167/300, seasonal_0 Loss: 0.0757 | 0.0463
Epoch 168/300, seasonal_0 Loss: 0.0756 | 0.0462
Epoch 169/300, seasonal_0 Loss: 0.0755 | 0.0462
Epoch 170/300, seasonal_0 Loss: 0.0755 | 0.0461
Epoch 171/300, seasonal_0 Loss: 0.0754 | 0.0461
Epoch 172/300, seasonal_0 Loss: 0.0754 | 0.0460
Epoch 173/300, seasonal_0 Loss: 0.0753 | 0.0460
Epoch 174/300, seasonal_0 Loss: 0.0753 | 0.0459
Epoch 175/300, seasonal_0 Loss: 0.0753 | 0.0459
Epoch 176/300, seasonal_0 Loss: 0.0752 | 0.0458
Epoch 177/300, seasonal_0 Loss: 0.0752 | 0.0458
Epoch 178/300, seasonal_0 Loss: 0.0751 | 0.0458
Epoch 179/300, seasonal_0 Loss: 0.0751 | 0.0457
Epoch 180/300, seasonal_0 Loss: 0.0750 | 0.0457
Epoch 181/300, seasonal_0 Loss: 0.0750 | 0.0456
Epoch 182/300, seasonal_0 Loss: 0.0749 | 0.0456
Epoch 183/300, seasonal_0 Loss: 0.0749 | 0.0455
Epoch 184/300, seasonal_0 Loss: 0.0748 | 0.0455
Epoch 185/300, seasonal_0 Loss: 0.0748 | 0.0455
Epoch 186/300, seasonal_0 Loss: 0.0747 | 0.0454
Epoch 187/300, seasonal_0 Loss: 0.0747 | 0.0454
Epoch 188/300, seasonal_0 Loss: 0.0747 | 0.0453
Epoch 189/300, seasonal_0 Loss: 0.0746 | 0.0453
Epoch 190/300, seasonal_0 Loss: 0.0746 | 0.0453
Epoch 191/300, seasonal_0 Loss: 0.0745 | 0.0452
Epoch 192/300, seasonal_0 Loss: 0.0745 | 0.0452
Epoch 193/300, seasonal_0 Loss: 0.0744 | 0.0452
Epoch 194/300, seasonal_0 Loss: 0.0744 | 0.0451
Epoch 195/300, seasonal_0 Loss: 0.0744 | 0.0451
Epoch 196/300, seasonal_0 Loss: 0.0743 | 0.0451
Epoch 197/300, seasonal_0 Loss: 0.0743 | 0.0451
Epoch 198/300, seasonal_0 Loss: 0.0742 | 0.0450
Epoch 199/300, seasonal_0 Loss: 0.0742 | 0.0450
Epoch 200/300, seasonal_0 Loss: 0.0742 | 0.0449
Epoch 201/300, seasonal_0 Loss: 0.0741 | 0.0449
Epoch 202/300, seasonal_0 Loss: 0.0741 | 0.0448
Epoch 203/300, seasonal_0 Loss: 0.0740 | 0.0448
Epoch 204/300, seasonal_0 Loss: 0.0740 | 0.0447
Epoch 205/300, seasonal_0 Loss: 0.0740 | 0.0447
Epoch 206/300, seasonal_0 Loss: 0.0740 | 0.0447
Epoch 207/300, seasonal_0 Loss: 0.0739 | 0.0446
Epoch 208/300, seasonal_0 Loss: 0.0739 | 0.0446
Epoch 209/300, seasonal_0 Loss: 0.0739 | 0.0446
Epoch 210/300, seasonal_0 Loss: 0.0738 | 0.0446
Epoch 211/300, seasonal_0 Loss: 0.0738 | 0.0447
Epoch 212/300, seasonal_0 Loss: 0.0738 | 0.0447
Epoch 213/300, seasonal_0 Loss: 0.0738 | 0.0448
Epoch 214/300, seasonal_0 Loss: 0.0738 | 0.0448
Epoch 215/300, seasonal_0 Loss: 0.0738 | 0.0448
Epoch 216/300, seasonal_0 Loss: 0.0738 | 0.0446
Epoch 217/300, seasonal_0 Loss: 0.0737 | 0.0445
Epoch 218/300, seasonal_0 Loss: 0.0736 | 0.0443
Epoch 219/300, seasonal_0 Loss: 0.0736 | 0.0443
Epoch 220/300, seasonal_0 Loss: 0.0738 | 0.0443
Epoch 221/300, seasonal_0 Loss: 0.0739 | 0.0443
Epoch 222/300, seasonal_0 Loss: 0.0738 | 0.0442
Epoch 223/300, seasonal_0 Loss: 0.0736 | 0.0443
Epoch 224/300, seasonal_0 Loss: 0.0734 | 0.0445
Epoch 225/300, seasonal_0 Loss: 0.0734 | 0.0446
Epoch 226/300, seasonal_0 Loss: 0.0735 | 0.0445
Epoch 227/300, seasonal_0 Loss: 0.0734 | 0.0443
Epoch 228/300, seasonal_0 Loss: 0.0733 | 0.0442
Epoch 229/300, seasonal_0 Loss: 0.0732 | 0.0441
Epoch 230/300, seasonal_0 Loss: 0.0732 | 0.0440
Epoch 231/300, seasonal_0 Loss: 0.0732 | 0.0440
Epoch 232/300, seasonal_0 Loss: 0.0731 | 0.0440
Epoch 233/300, seasonal_0 Loss: 0.0731 | 0.0440
Epoch 234/300, seasonal_0 Loss: 0.0730 | 0.0441
Epoch 235/300, seasonal_0 Loss: 0.0730 | 0.0440
Epoch 236/300, seasonal_0 Loss: 0.0730 | 0.0440
Epoch 237/300, seasonal_0 Loss: 0.0729 | 0.0439
Epoch 238/300, seasonal_0 Loss: 0.0729 | 0.0439
Epoch 239/300, seasonal_0 Loss: 0.0729 | 0.0438
Epoch 240/300, seasonal_0 Loss: 0.0729 | 0.0438
Epoch 241/300, seasonal_0 Loss: 0.0728 | 0.0438
Epoch 242/300, seasonal_0 Loss: 0.0728 | 0.0438
Epoch 243/300, seasonal_0 Loss: 0.0728 | 0.0438
Epoch 244/300, seasonal_0 Loss: 0.0727 | 0.0438
Epoch 245/300, seasonal_0 Loss: 0.0727 | 0.0437
Epoch 246/300, seasonal_0 Loss: 0.0727 | 0.0437
Epoch 247/300, seasonal_0 Loss: 0.0727 | 0.0437
Epoch 248/300, seasonal_0 Loss: 0.0726 | 0.0437
Epoch 249/300, seasonal_0 Loss: 0.0726 | 0.0437
Epoch 250/300, seasonal_0 Loss: 0.0726 | 0.0436
Epoch 251/300, seasonal_0 Loss: 0.0726 | 0.0436
Epoch 252/300, seasonal_0 Loss: 0.0725 | 0.0436
Epoch 253/300, seasonal_0 Loss: 0.0725 | 0.0436
Epoch 254/300, seasonal_0 Loss: 0.0725 | 0.0436
Epoch 255/300, seasonal_0 Loss: 0.0725 | 0.0435
Epoch 256/300, seasonal_0 Loss: 0.0724 | 0.0435
Epoch 257/300, seasonal_0 Loss: 0.0724 | 0.0435
Epoch 258/300, seasonal_0 Loss: 0.0724 | 0.0435
Epoch 259/300, seasonal_0 Loss: 0.0724 | 0.0435
Epoch 260/300, seasonal_0 Loss: 0.0723 | 0.0435
Epoch 261/300, seasonal_0 Loss: 0.0723 | 0.0434
Epoch 262/300, seasonal_0 Loss: 0.0723 | 0.0434
Epoch 263/300, seasonal_0 Loss: 0.0723 | 0.0434
Epoch 264/300, seasonal_0 Loss: 0.0723 | 0.0434
Epoch 265/300, seasonal_0 Loss: 0.0722 | 0.0434
Epoch 266/300, seasonal_0 Loss: 0.0722 | 0.0433
Epoch 267/300, seasonal_0 Loss: 0.0722 | 0.0433
Epoch 268/300, seasonal_0 Loss: 0.0722 | 0.0433
Epoch 269/300, seasonal_0 Loss: 0.0721 | 0.0433
Epoch 270/300, seasonal_0 Loss: 0.0721 | 0.0433
Epoch 271/300, seasonal_0 Loss: 0.0721 | 0.0433
Epoch 272/300, seasonal_0 Loss: 0.0721 | 0.0432
Epoch 273/300, seasonal_0 Loss: 0.0721 | 0.0432
Epoch 274/300, seasonal_0 Loss: 0.0720 | 0.0432
Epoch 275/300, seasonal_0 Loss: 0.0720 | 0.0432
Epoch 276/300, seasonal_0 Loss: 0.0720 | 0.0432
Epoch 277/300, seasonal_0 Loss: 0.0720 | 0.0432
Epoch 278/300, seasonal_0 Loss: 0.0720 | 0.0431
Epoch 279/300, seasonal_0 Loss: 0.0719 | 0.0431
Epoch 280/300, seasonal_0 Loss: 0.0719 | 0.0431
Epoch 281/300, seasonal_0 Loss: 0.0719 | 0.0431
Epoch 282/300, seasonal_0 Loss: 0.0719 | 0.0431
Epoch 283/300, seasonal_0 Loss: 0.0718 | 0.0431
Epoch 284/300, seasonal_0 Loss: 0.0718 | 0.0430
Epoch 285/300, seasonal_0 Loss: 0.0718 | 0.0430
Epoch 286/300, seasonal_0 Loss: 0.0718 | 0.0430
Epoch 287/300, seasonal_0 Loss: 0.0718 | 0.0430
Epoch 288/300, seasonal_0 Loss: 0.0717 | 0.0430
Epoch 289/300, seasonal_0 Loss: 0.0717 | 0.0430
Epoch 290/300, seasonal_0 Loss: 0.0717 | 0.0430
Epoch 291/300, seasonal_0 Loss: 0.0717 | 0.0429
Epoch 292/300, seasonal_0 Loss: 0.0717 | 0.0429
Epoch 293/300, seasonal_0 Loss: 0.0717 | 0.0429
Epoch 294/300, seasonal_0 Loss: 0.0716 | 0.0429
Epoch 295/300, seasonal_0 Loss: 0.0716 | 0.0429
Epoch 296/300, seasonal_0 Loss: 0.0716 | 0.0429
Epoch 297/300, seasonal_0 Loss: 0.0716 | 0.0429
Epoch 298/300, seasonal_0 Loss: 0.0716 | 0.0428
Epoch 299/300, seasonal_0 Loss: 0.0715 | 0.0428
Epoch 300/300, seasonal_0 Loss: 0.0715 | 0.0428
Training seasonal_1 component with params: {'observation_period_num': 98, 'train_rates': 0.8743656324245632, 'learning_rate': 9.100773719622059e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.906729642164511}
Epoch 1/300, seasonal_1 Loss: 0.6736 | 0.7249
Epoch 2/300, seasonal_1 Loss: 0.4146 | 0.5193
Epoch 3/300, seasonal_1 Loss: 0.3076 | 0.4774
Epoch 4/300, seasonal_1 Loss: 0.2689 | 0.4089
Epoch 5/300, seasonal_1 Loss: 0.2426 | 0.3432
Epoch 6/300, seasonal_1 Loss: 0.2257 | 0.3220
Epoch 7/300, seasonal_1 Loss: 0.2121 | 0.3154
Epoch 8/300, seasonal_1 Loss: 0.2005 | 0.3109
Epoch 9/300, seasonal_1 Loss: 0.1902 | 0.3287
Epoch 10/300, seasonal_1 Loss: 0.1804 | 0.3423
Epoch 11/300, seasonal_1 Loss: 0.1672 | 0.3090
Epoch 12/300, seasonal_1 Loss: 0.1571 | 0.2330
Epoch 13/300, seasonal_1 Loss: 0.1617 | 0.2022
Epoch 14/300, seasonal_1 Loss: 0.1705 | 0.2052
Epoch 15/300, seasonal_1 Loss: 0.1610 | 0.2534
Epoch 16/300, seasonal_1 Loss: 0.1696 | 0.3734
Epoch 17/300, seasonal_1 Loss: 0.1494 | 0.1914
Epoch 18/300, seasonal_1 Loss: 0.1375 | 0.1707
Epoch 19/300, seasonal_1 Loss: 0.1309 | 0.1896
Epoch 20/300, seasonal_1 Loss: 0.1275 | 0.1885
Epoch 21/300, seasonal_1 Loss: 0.1236 | 0.1638
Epoch 22/300, seasonal_1 Loss: 0.1208 | 0.1490
Epoch 23/300, seasonal_1 Loss: 0.1189 | 0.1415
Epoch 24/300, seasonal_1 Loss: 0.1164 | 0.1376
Epoch 25/300, seasonal_1 Loss: 0.1150 | 0.1391
Epoch 26/300, seasonal_1 Loss: 0.1129 | 0.1397
Epoch 27/300, seasonal_1 Loss: 0.1102 | 0.1370
Epoch 28/300, seasonal_1 Loss: 0.1078 | 0.1307
Epoch 29/300, seasonal_1 Loss: 0.1064 | 0.1229
Epoch 30/300, seasonal_1 Loss: 0.1062 | 0.1186
Epoch 31/300, seasonal_1 Loss: 0.1068 | 0.1286
Epoch 32/300, seasonal_1 Loss: 0.1054 | 0.1241
Epoch 33/300, seasonal_1 Loss: 0.1033 | 0.1107
Epoch 34/300, seasonal_1 Loss: 0.1072 | 0.1276
Epoch 35/300, seasonal_1 Loss: 0.1140 | 0.2322
Epoch 36/300, seasonal_1 Loss: 0.1159 | 0.2781
Epoch 37/300, seasonal_1 Loss: 0.1121 | 0.1214
Epoch 38/300, seasonal_1 Loss: 0.1158 | 0.1235
Epoch 39/300, seasonal_1 Loss: 0.1044 | 0.1149
Epoch 40/300, seasonal_1 Loss: 0.0990 | 0.1465
Epoch 41/300, seasonal_1 Loss: 0.0982 | 0.1220
Epoch 42/300, seasonal_1 Loss: 0.0961 | 0.1035
Epoch 43/300, seasonal_1 Loss: 0.0951 | 0.0997
Epoch 44/300, seasonal_1 Loss: 0.0921 | 0.1003
Epoch 45/300, seasonal_1 Loss: 0.0916 | 0.1087
Epoch 46/300, seasonal_1 Loss: 0.0911 | 0.1145
Epoch 47/300, seasonal_1 Loss: 0.0893 | 0.1028
Epoch 48/300, seasonal_1 Loss: 0.0892 | 0.0942
Epoch 49/300, seasonal_1 Loss: 0.0895 | 0.0935
Epoch 50/300, seasonal_1 Loss: 0.0891 | 0.0915
Epoch 51/300, seasonal_1 Loss: 0.0890 | 0.0935
Epoch 52/300, seasonal_1 Loss: 0.0881 | 0.0984
Epoch 53/300, seasonal_1 Loss: 0.0864 | 0.0993
Epoch 54/300, seasonal_1 Loss: 0.0854 | 0.0936
Epoch 55/300, seasonal_1 Loss: 0.0857 | 0.0892
Epoch 56/300, seasonal_1 Loss: 0.0859 | 0.0928
Epoch 57/300, seasonal_1 Loss: 0.0842 | 0.0883
Epoch 58/300, seasonal_1 Loss: 0.0836 | 0.0878
Epoch 59/300, seasonal_1 Loss: 0.0858 | 0.1050
Epoch 60/300, seasonal_1 Loss: 0.0892 | 0.1411
Epoch 61/300, seasonal_1 Loss: 0.0908 | 0.1603
Epoch 62/300, seasonal_1 Loss: 0.0890 | 0.0914
Epoch 63/300, seasonal_1 Loss: 0.0875 | 0.0891
Epoch 64/300, seasonal_1 Loss: 0.0837 | 0.0835
Epoch 65/300, seasonal_1 Loss: 0.0844 | 0.1029
Epoch 66/300, seasonal_1 Loss: 0.0839 | 0.1023
Epoch 67/300, seasonal_1 Loss: 0.0809 | 0.0842
Epoch 68/300, seasonal_1 Loss: 0.0826 | 0.0899
Epoch 69/300, seasonal_1 Loss: 0.0810 | 0.0821
Epoch 70/300, seasonal_1 Loss: 0.0802 | 0.0964
Epoch 71/300, seasonal_1 Loss: 0.0805 | 0.1009
Epoch 72/300, seasonal_1 Loss: 0.0802 | 0.0863
Epoch 73/300, seasonal_1 Loss: 0.0807 | 0.0801
Epoch 74/300, seasonal_1 Loss: 0.0788 | 0.0788
Epoch 75/300, seasonal_1 Loss: 0.0774 | 0.0823
Epoch 76/300, seasonal_1 Loss: 0.0777 | 0.0906
Epoch 77/300, seasonal_1 Loss: 0.0769 | 0.0840
Epoch 78/300, seasonal_1 Loss: 0.0767 | 0.0793
Epoch 79/300, seasonal_1 Loss: 0.0770 | 0.0799
Epoch 80/300, seasonal_1 Loss: 0.0759 | 0.0782
Epoch 81/300, seasonal_1 Loss: 0.0760 | 0.0864
Epoch 82/300, seasonal_1 Loss: 0.0760 | 0.0893
Epoch 83/300, seasonal_1 Loss: 0.0761 | 0.0840
Epoch 84/300, seasonal_1 Loss: 0.0761 | 0.0787
Epoch 85/300, seasonal_1 Loss: 0.0750 | 0.0761
Epoch 86/300, seasonal_1 Loss: 0.0742 | 0.0770
Epoch 87/300, seasonal_1 Loss: 0.0742 | 0.0796
Epoch 88/300, seasonal_1 Loss: 0.0741 | 0.0781
Epoch 89/300, seasonal_1 Loss: 0.0739 | 0.0761
Epoch 90/300, seasonal_1 Loss: 0.0740 | 0.0766
Epoch 91/300, seasonal_1 Loss: 0.0733 | 0.0750
Epoch 92/300, seasonal_1 Loss: 0.0731 | 0.0797
Epoch 93/300, seasonal_1 Loss: 0.0737 | 0.0866
Epoch 94/300, seasonal_1 Loss: 0.0742 | 0.0861
Epoch 95/300, seasonal_1 Loss: 0.0739 | 0.0802
Epoch 96/300, seasonal_1 Loss: 0.0727 | 0.0748
Epoch 97/300, seasonal_1 Loss: 0.0720 | 0.0739
Epoch 98/300, seasonal_1 Loss: 0.0723 | 0.0744
Epoch 99/300, seasonal_1 Loss: 0.0728 | 0.0746
Epoch 100/300, seasonal_1 Loss: 0.0726 | 0.0746
Epoch 101/300, seasonal_1 Loss: 0.0718 | 0.0740
Epoch 102/300, seasonal_1 Loss: 0.0713 | 0.0743
Epoch 103/300, seasonal_1 Loss: 0.0717 | 0.0795
Epoch 104/300, seasonal_1 Loss: 0.0726 | 0.0874
Epoch 105/300, seasonal_1 Loss: 0.0724 | 0.0874
Epoch 106/300, seasonal_1 Loss: 0.0710 | 0.0767
Epoch 107/300, seasonal_1 Loss: 0.0705 | 0.0725
Epoch 108/300, seasonal_1 Loss: 0.0713 | 0.0736
Epoch 109/300, seasonal_1 Loss: 0.0715 | 0.0732
Epoch 110/300, seasonal_1 Loss: 0.0707 | 0.0738
Epoch 111/300, seasonal_1 Loss: 0.0699 | 0.0740
Epoch 112/300, seasonal_1 Loss: 0.0702 | 0.0743
Epoch 113/300, seasonal_1 Loss: 0.0707 | 0.0767
Epoch 114/300, seasonal_1 Loss: 0.0702 | 0.0790
Epoch 115/300, seasonal_1 Loss: 0.0693 | 0.0751
Epoch 116/300, seasonal_1 Loss: 0.0691 | 0.0720
Epoch 117/300, seasonal_1 Loss: 0.0694 | 0.0724
Epoch 118/300, seasonal_1 Loss: 0.0692 | 0.0717
Epoch 119/300, seasonal_1 Loss: 0.0686 | 0.0731
Epoch 120/300, seasonal_1 Loss: 0.0686 | 0.0767
Epoch 121/300, seasonal_1 Loss: 0.0689 | 0.0772
Epoch 122/300, seasonal_1 Loss: 0.0686 | 0.0728
Epoch 123/300, seasonal_1 Loss: 0.0680 | 0.0709
Epoch 124/300, seasonal_1 Loss: 0.0679 | 0.0712
Epoch 125/300, seasonal_1 Loss: 0.0681 | 0.0715
Epoch 126/300, seasonal_1 Loss: 0.0678 | 0.0713
Epoch 127/300, seasonal_1 Loss: 0.0675 | 0.0710
Epoch 128/300, seasonal_1 Loss: 0.0674 | 0.0723
Epoch 129/300, seasonal_1 Loss: 0.0675 | 0.0753
Epoch 130/300, seasonal_1 Loss: 0.0672 | 0.0737
Epoch 131/300, seasonal_1 Loss: 0.0669 | 0.0708
Epoch 132/300, seasonal_1 Loss: 0.0669 | 0.0702
Epoch 133/300, seasonal_1 Loss: 0.0668 | 0.0704
Epoch 134/300, seasonal_1 Loss: 0.0667 | 0.0710
Epoch 135/300, seasonal_1 Loss: 0.0665 | 0.0717
Epoch 136/300, seasonal_1 Loss: 0.0664 | 0.0718
Epoch 137/300, seasonal_1 Loss: 0.0664 | 0.0713
Epoch 138/300, seasonal_1 Loss: 0.0662 | 0.0709
Epoch 139/300, seasonal_1 Loss: 0.0660 | 0.0707
Epoch 140/300, seasonal_1 Loss: 0.0660 | 0.0703
Epoch 141/300, seasonal_1 Loss: 0.0659 | 0.0700
Epoch 142/300, seasonal_1 Loss: 0.0658 | 0.0700
Epoch 143/300, seasonal_1 Loss: 0.0656 | 0.0706
Epoch 144/300, seasonal_1 Loss: 0.0656 | 0.0716
Epoch 145/300, seasonal_1 Loss: 0.0655 | 0.0709
Epoch 146/300, seasonal_1 Loss: 0.0653 | 0.0700
Epoch 147/300, seasonal_1 Loss: 0.0652 | 0.0697
Epoch 148/300, seasonal_1 Loss: 0.0652 | 0.0698
Epoch 149/300, seasonal_1 Loss: 0.0651 | 0.0700
Epoch 150/300, seasonal_1 Loss: 0.0650 | 0.0699
Epoch 151/300, seasonal_1 Loss: 0.0649 | 0.0700
Epoch 152/300, seasonal_1 Loss: 0.0648 | 0.0701
Epoch 153/300, seasonal_1 Loss: 0.0647 | 0.0701
Epoch 154/300, seasonal_1 Loss: 0.0646 | 0.0698
Epoch 155/300, seasonal_1 Loss: 0.0645 | 0.0694
Epoch 156/300, seasonal_1 Loss: 0.0644 | 0.0693
Epoch 157/300, seasonal_1 Loss: 0.0644 | 0.0694
Epoch 158/300, seasonal_1 Loss: 0.0643 | 0.0697
Epoch 159/300, seasonal_1 Loss: 0.0642 | 0.0698
Epoch 160/300, seasonal_1 Loss: 0.0641 | 0.0696
Epoch 161/300, seasonal_1 Loss: 0.0640 | 0.0693
Epoch 162/300, seasonal_1 Loss: 0.0639 | 0.0693
Epoch 163/300, seasonal_1 Loss: 0.0639 | 0.0693
Epoch 164/300, seasonal_1 Loss: 0.0638 | 0.0692
Epoch 165/300, seasonal_1 Loss: 0.0637 | 0.0691
Epoch 166/300, seasonal_1 Loss: 0.0636 | 0.0692
Epoch 167/300, seasonal_1 Loss: 0.0635 | 0.0693
Epoch 168/300, seasonal_1 Loss: 0.0635 | 0.0693
Epoch 169/300, seasonal_1 Loss: 0.0634 | 0.0691
Epoch 170/300, seasonal_1 Loss: 0.0633 | 0.0689
Epoch 171/300, seasonal_1 Loss: 0.0633 | 0.0689
Epoch 172/300, seasonal_1 Loss: 0.0632 | 0.0690
Epoch 173/300, seasonal_1 Loss: 0.0631 | 0.0690
Epoch 174/300, seasonal_1 Loss: 0.0630 | 0.0690
Epoch 175/300, seasonal_1 Loss: 0.0630 | 0.0689
Epoch 176/300, seasonal_1 Loss: 0.0629 | 0.0689
Epoch 177/300, seasonal_1 Loss: 0.0628 | 0.0688
Epoch 178/300, seasonal_1 Loss: 0.0628 | 0.0688
Epoch 179/300, seasonal_1 Loss: 0.0627 | 0.0687
Epoch 180/300, seasonal_1 Loss: 0.0626 | 0.0687
Epoch 181/300, seasonal_1 Loss: 0.0626 | 0.0688
Epoch 182/300, seasonal_1 Loss: 0.0625 | 0.0687
Epoch 183/300, seasonal_1 Loss: 0.0624 | 0.0687
Epoch 184/300, seasonal_1 Loss: 0.0624 | 0.0686
Epoch 185/300, seasonal_1 Loss: 0.0623 | 0.0686
Epoch 186/300, seasonal_1 Loss: 0.0623 | 0.0685
Epoch 187/300, seasonal_1 Loss: 0.0622 | 0.0685
Epoch 188/300, seasonal_1 Loss: 0.0621 | 0.0685
Epoch 189/300, seasonal_1 Loss: 0.0621 | 0.0685
Epoch 190/300, seasonal_1 Loss: 0.0620 | 0.0685
Epoch 191/300, seasonal_1 Loss: 0.0620 | 0.0684
Epoch 192/300, seasonal_1 Loss: 0.0619 | 0.0684
Epoch 193/300, seasonal_1 Loss: 0.0618 | 0.0684
Epoch 194/300, seasonal_1 Loss: 0.0618 | 0.0684
Epoch 195/300, seasonal_1 Loss: 0.0617 | 0.0684
Epoch 196/300, seasonal_1 Loss: 0.0617 | 0.0684
Epoch 197/300, seasonal_1 Loss: 0.0616 | 0.0683
Epoch 198/300, seasonal_1 Loss: 0.0616 | 0.0683
Epoch 199/300, seasonal_1 Loss: 0.0615 | 0.0683
Epoch 200/300, seasonal_1 Loss: 0.0615 | 0.0683
Epoch 201/300, seasonal_1 Loss: 0.0614 | 0.0682
Epoch 202/300, seasonal_1 Loss: 0.0614 | 0.0682
Epoch 203/300, seasonal_1 Loss: 0.0613 | 0.0682
Epoch 204/300, seasonal_1 Loss: 0.0612 | 0.0682
Epoch 205/300, seasonal_1 Loss: 0.0612 | 0.0682
Epoch 206/300, seasonal_1 Loss: 0.0612 | 0.0682
Epoch 207/300, seasonal_1 Loss: 0.0611 | 0.0682
Epoch 208/300, seasonal_1 Loss: 0.0611 | 0.0681
Epoch 209/300, seasonal_1 Loss: 0.0610 | 0.0681
Epoch 210/300, seasonal_1 Loss: 0.0610 | 0.0681
Epoch 211/300, seasonal_1 Loss: 0.0609 | 0.0681
Epoch 212/300, seasonal_1 Loss: 0.0609 | 0.0681
Epoch 213/300, seasonal_1 Loss: 0.0608 | 0.0681
Epoch 214/300, seasonal_1 Loss: 0.0608 | 0.0681
Epoch 215/300, seasonal_1 Loss: 0.0607 | 0.0680
Epoch 216/300, seasonal_1 Loss: 0.0607 | 0.0680
Epoch 217/300, seasonal_1 Loss: 0.0606 | 0.0680
Epoch 218/300, seasonal_1 Loss: 0.0606 | 0.0680
Epoch 219/300, seasonal_1 Loss: 0.0606 | 0.0680
Epoch 220/300, seasonal_1 Loss: 0.0605 | 0.0680
Epoch 221/300, seasonal_1 Loss: 0.0605 | 0.0680
Epoch 222/300, seasonal_1 Loss: 0.0604 | 0.0680
Epoch 223/300, seasonal_1 Loss: 0.0604 | 0.0679
Epoch 224/300, seasonal_1 Loss: 0.0604 | 0.0679
Epoch 225/300, seasonal_1 Loss: 0.0603 | 0.0679
Epoch 226/300, seasonal_1 Loss: 0.0603 | 0.0679
Epoch 227/300, seasonal_1 Loss: 0.0602 | 0.0679
Epoch 228/300, seasonal_1 Loss: 0.0602 | 0.0679
Epoch 229/300, seasonal_1 Loss: 0.0602 | 0.0679
Epoch 230/300, seasonal_1 Loss: 0.0601 | 0.0679
Epoch 231/300, seasonal_1 Loss: 0.0601 | 0.0679
Epoch 232/300, seasonal_1 Loss: 0.0600 | 0.0679
Epoch 233/300, seasonal_1 Loss: 0.0600 | 0.0679
Epoch 234/300, seasonal_1 Loss: 0.0600 | 0.0679
Epoch 235/300, seasonal_1 Loss: 0.0599 | 0.0678
Epoch 236/300, seasonal_1 Loss: 0.0599 | 0.0679
Epoch 237/300, seasonal_1 Loss: 0.0599 | 0.0678
Epoch 238/300, seasonal_1 Loss: 0.0598 | 0.0678
Epoch 239/300, seasonal_1 Loss: 0.0598 | 0.0678
Epoch 240/300, seasonal_1 Loss: 0.0598 | 0.0678
Epoch 241/300, seasonal_1 Loss: 0.0597 | 0.0678
Epoch 242/300, seasonal_1 Loss: 0.0597 | 0.0678
Epoch 243/300, seasonal_1 Loss: 0.0597 | 0.0678
Epoch 244/300, seasonal_1 Loss: 0.0596 | 0.0678
Epoch 245/300, seasonal_1 Loss: 0.0596 | 0.0678
Epoch 246/300, seasonal_1 Loss: 0.0596 | 0.0678
Epoch 247/300, seasonal_1 Loss: 0.0595 | 0.0678
Epoch 248/300, seasonal_1 Loss: 0.0595 | 0.0678
Epoch 249/300, seasonal_1 Loss: 0.0595 | 0.0678
Epoch 250/300, seasonal_1 Loss: 0.0594 | 0.0677
Epoch 251/300, seasonal_1 Loss: 0.0594 | 0.0678
Epoch 252/300, seasonal_1 Loss: 0.0594 | 0.0677
Epoch 253/300, seasonal_1 Loss: 0.0593 | 0.0677
Epoch 254/300, seasonal_1 Loss: 0.0593 | 0.0677
Epoch 255/300, seasonal_1 Loss: 0.0593 | 0.0677
Epoch 256/300, seasonal_1 Loss: 0.0593 | 0.0677
Epoch 257/300, seasonal_1 Loss: 0.0592 | 0.0677
Epoch 258/300, seasonal_1 Loss: 0.0592 | 0.0677
Epoch 259/300, seasonal_1 Loss: 0.0592 | 0.0677
Epoch 260/300, seasonal_1 Loss: 0.0592 | 0.0677
Epoch 261/300, seasonal_1 Loss: 0.0591 | 0.0677
Epoch 262/300, seasonal_1 Loss: 0.0591 | 0.0677
Epoch 263/300, seasonal_1 Loss: 0.0591 | 0.0677
Epoch 264/300, seasonal_1 Loss: 0.0590 | 0.0677
Epoch 265/300, seasonal_1 Loss: 0.0590 | 0.0677
Epoch 266/300, seasonal_1 Loss: 0.0590 | 0.0677
Epoch 267/300, seasonal_1 Loss: 0.0590 | 0.0677
Epoch 268/300, seasonal_1 Loss: 0.0589 | 0.0677
Epoch 269/300, seasonal_1 Loss: 0.0589 | 0.0677
Epoch 270/300, seasonal_1 Loss: 0.0589 | 0.0677
Epoch 271/300, seasonal_1 Loss: 0.0589 | 0.0677
Epoch 272/300, seasonal_1 Loss: 0.0588 | 0.0677
Epoch 273/300, seasonal_1 Loss: 0.0588 | 0.0677
Epoch 274/300, seasonal_1 Loss: 0.0588 | 0.0676
Epoch 275/300, seasonal_1 Loss: 0.0588 | 0.0676
Epoch 276/300, seasonal_1 Loss: 0.0588 | 0.0676
Epoch 277/300, seasonal_1 Loss: 0.0587 | 0.0676
Epoch 278/300, seasonal_1 Loss: 0.0587 | 0.0676
Epoch 279/300, seasonal_1 Loss: 0.0587 | 0.0676
Epoch 280/300, seasonal_1 Loss: 0.0587 | 0.0676
Epoch 281/300, seasonal_1 Loss: 0.0586 | 0.0676
Epoch 282/300, seasonal_1 Loss: 0.0586 | 0.0676
Epoch 283/300, seasonal_1 Loss: 0.0586 | 0.0676
Epoch 284/300, seasonal_1 Loss: 0.0586 | 0.0676
Epoch 285/300, seasonal_1 Loss: 0.0586 | 0.0676
Epoch 286/300, seasonal_1 Loss: 0.0585 | 0.0676
Epoch 287/300, seasonal_1 Loss: 0.0585 | 0.0676
Epoch 288/300, seasonal_1 Loss: 0.0585 | 0.0676
Epoch 289/300, seasonal_1 Loss: 0.0585 | 0.0676
Epoch 290/300, seasonal_1 Loss: 0.0585 | 0.0676
Epoch 291/300, seasonal_1 Loss: 0.0584 | 0.0676
Epoch 292/300, seasonal_1 Loss: 0.0584 | 0.0676
Epoch 293/300, seasonal_1 Loss: 0.0584 | 0.0676
Epoch 294/300, seasonal_1 Loss: 0.0584 | 0.0676
Epoch 295/300, seasonal_1 Loss: 0.0584 | 0.0676
Epoch 296/300, seasonal_1 Loss: 0.0583 | 0.0676
Epoch 297/300, seasonal_1 Loss: 0.0583 | 0.0676
Epoch 298/300, seasonal_1 Loss: 0.0583 | 0.0676
Epoch 299/300, seasonal_1 Loss: 0.0583 | 0.0676
Epoch 300/300, seasonal_1 Loss: 0.0583 | 0.0676
Training seasonal_2 component with params: {'observation_period_num': 14, 'train_rates': 0.9249848181832773, 'learning_rate': 0.0006894424315495376, 'batch_size': 53, 'step_size': 3, 'gamma': 0.7639131537660816}
Epoch 1/300, seasonal_2 Loss: 0.2790 | 0.1451
Epoch 2/300, seasonal_2 Loss: 0.1250 | 0.1343
Epoch 3/300, seasonal_2 Loss: 0.1107 | 0.0871
Epoch 4/300, seasonal_2 Loss: 0.1070 | 0.0843
Epoch 5/300, seasonal_2 Loss: 0.1110 | 0.0825
Epoch 6/300, seasonal_2 Loss: 0.1010 | 0.0818
Epoch 7/300, seasonal_2 Loss: 0.0964 | 0.0858
Epoch 8/300, seasonal_2 Loss: 0.0930 | 0.0801
Epoch 9/300, seasonal_2 Loss: 0.0933 | 0.0663
Epoch 10/300, seasonal_2 Loss: 0.0916 | 0.0653
Epoch 11/300, seasonal_2 Loss: 0.0882 | 0.0662
Epoch 12/300, seasonal_2 Loss: 0.0852 | 0.0670
Epoch 13/300, seasonal_2 Loss: 0.0837 | 0.0661
Epoch 14/300, seasonal_2 Loss: 0.0822 | 0.0667
Epoch 15/300, seasonal_2 Loss: 0.0823 | 0.0563
Epoch 16/300, seasonal_2 Loss: 0.0821 | 0.0514
Epoch 17/300, seasonal_2 Loss: 0.0820 | 0.0511
Epoch 18/300, seasonal_2 Loss: 0.0819 | 0.0519
Epoch 19/300, seasonal_2 Loss: 0.0820 | 0.0577
Epoch 20/300, seasonal_2 Loss: 0.0814 | 0.0579
Epoch 21/300, seasonal_2 Loss: 0.0802 | 0.0616
Epoch 22/300, seasonal_2 Loss: 0.0799 | 0.0645
Epoch 23/300, seasonal_2 Loss: 0.0798 | 0.0626
Epoch 24/300, seasonal_2 Loss: 0.0793 | 0.0622
Epoch 25/300, seasonal_2 Loss: 0.0789 | 0.0602
Epoch 26/300, seasonal_2 Loss: 0.0782 | 0.0592
Epoch 27/300, seasonal_2 Loss: 0.0778 | 0.0582
Epoch 28/300, seasonal_2 Loss: 0.0773 | 0.0575
Epoch 29/300, seasonal_2 Loss: 0.0770 | 0.0572
Epoch 30/300, seasonal_2 Loss: 0.0768 | 0.0568
Epoch 31/300, seasonal_2 Loss: 0.0767 | 0.0565
Epoch 32/300, seasonal_2 Loss: 0.0766 | 0.0564
Epoch 33/300, seasonal_2 Loss: 0.0765 | 0.0562
Epoch 34/300, seasonal_2 Loss: 0.0764 | 0.0560
Epoch 35/300, seasonal_2 Loss: 0.0764 | 0.0560
Epoch 36/300, seasonal_2 Loss: 0.0763 | 0.0559
Epoch 37/300, seasonal_2 Loss: 0.0763 | 0.0558
Epoch 38/300, seasonal_2 Loss: 0.0763 | 0.0557
Epoch 39/300, seasonal_2 Loss: 0.0762 | 0.0557
Epoch 40/300, seasonal_2 Loss: 0.0762 | 0.0556
Epoch 41/300, seasonal_2 Loss: 0.0762 | 0.0556
Epoch 42/300, seasonal_2 Loss: 0.0762 | 0.0556
Epoch 43/300, seasonal_2 Loss: 0.0762 | 0.0556
Epoch 44/300, seasonal_2 Loss: 0.0762 | 0.0555
Epoch 45/300, seasonal_2 Loss: 0.0762 | 0.0555
Epoch 46/300, seasonal_2 Loss: 0.0762 | 0.0555
Epoch 47/300, seasonal_2 Loss: 0.0762 | 0.0555
Epoch 48/300, seasonal_2 Loss: 0.0761 | 0.0555
Epoch 49/300, seasonal_2 Loss: 0.0761 | 0.0555
Epoch 50/300, seasonal_2 Loss: 0.0761 | 0.0555
Epoch 51/300, seasonal_2 Loss: 0.0761 | 0.0555
Epoch 52/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 53/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 54/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 55/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 56/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 57/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 58/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 59/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 60/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 61/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 62/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 63/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 64/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 65/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 66/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 67/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 68/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 69/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 70/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 71/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 72/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 73/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 74/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 75/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 76/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 77/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 78/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 79/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 80/300, seasonal_2 Loss: 0.0761 | 0.0554
Epoch 81/300, seasonal_2 Loss: 0.0761 | 0.0554
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.9347066193836381, 'learning_rate': 0.00012878469187759236, 'batch_size': 17, 'step_size': 6, 'gamma': 0.760605987871093}
Epoch 1/300, seasonal_3 Loss: 0.1791 | 0.1454
Epoch 2/300, seasonal_3 Loss: 0.1201 | 0.1007
Epoch 3/300, seasonal_3 Loss: 0.1085 | 0.0854
Epoch 4/300, seasonal_3 Loss: 0.1004 | 0.0726
Epoch 5/300, seasonal_3 Loss: 0.0963 | 0.0670
Epoch 6/300, seasonal_3 Loss: 0.0926 | 0.0617
Epoch 7/300, seasonal_3 Loss: 0.0868 | 0.0568
Epoch 8/300, seasonal_3 Loss: 0.0836 | 0.0525
Epoch 9/300, seasonal_3 Loss: 0.0814 | 0.0494
Epoch 10/300, seasonal_3 Loss: 0.0784 | 0.0485
Epoch 11/300, seasonal_3 Loss: 0.0767 | 0.0468
Epoch 12/300, seasonal_3 Loss: 0.0754 | 0.0453
Epoch 13/300, seasonal_3 Loss: 0.0738 | 0.0445
Epoch 14/300, seasonal_3 Loss: 0.0728 | 0.0438
Epoch 15/300, seasonal_3 Loss: 0.0719 | 0.0429
Epoch 16/300, seasonal_3 Loss: 0.0711 | 0.0420
Epoch 17/300, seasonal_3 Loss: 0.0703 | 0.0415
Epoch 18/300, seasonal_3 Loss: 0.0696 | 0.0409
Epoch 19/300, seasonal_3 Loss: 0.0689 | 0.0398
Epoch 20/300, seasonal_3 Loss: 0.0685 | 0.0394
Epoch 21/300, seasonal_3 Loss: 0.0681 | 0.0391
Epoch 22/300, seasonal_3 Loss: 0.0677 | 0.0381
Epoch 23/300, seasonal_3 Loss: 0.0675 | 0.0375
Epoch 24/300, seasonal_3 Loss: 0.0670 | 0.0371
Epoch 25/300, seasonal_3 Loss: 0.0666 | 0.0372
Epoch 26/300, seasonal_3 Loss: 0.0662 | 0.0368
Epoch 27/300, seasonal_3 Loss: 0.0658 | 0.0365
Epoch 28/300, seasonal_3 Loss: 0.0655 | 0.0366
Epoch 29/300, seasonal_3 Loss: 0.0652 | 0.0365
Epoch 30/300, seasonal_3 Loss: 0.0650 | 0.0364
Epoch 31/300, seasonal_3 Loss: 0.0647 | 0.0359
Epoch 32/300, seasonal_3 Loss: 0.0646 | 0.0358
Epoch 33/300, seasonal_3 Loss: 0.0644 | 0.0357
Epoch 34/300, seasonal_3 Loss: 0.0643 | 0.0352
Epoch 35/300, seasonal_3 Loss: 0.0642 | 0.0351
Epoch 36/300, seasonal_3 Loss: 0.0640 | 0.0351
Epoch 37/300, seasonal_3 Loss: 0.0640 | 0.0350
Epoch 38/300, seasonal_3 Loss: 0.0639 | 0.0350
Epoch 39/300, seasonal_3 Loss: 0.0638 | 0.0349
Epoch 40/300, seasonal_3 Loss: 0.0637 | 0.0351
Epoch 41/300, seasonal_3 Loss: 0.0637 | 0.0350
Epoch 42/300, seasonal_3 Loss: 0.0636 | 0.0349
Epoch 43/300, seasonal_3 Loss: 0.0636 | 0.0350
Epoch 44/300, seasonal_3 Loss: 0.0635 | 0.0350
Epoch 45/300, seasonal_3 Loss: 0.0634 | 0.0349
Epoch 46/300, seasonal_3 Loss: 0.0634 | 0.0349
Epoch 47/300, seasonal_3 Loss: 0.0634 | 0.0349
Epoch 48/300, seasonal_3 Loss: 0.0633 | 0.0349
Epoch 49/300, seasonal_3 Loss: 0.0633 | 0.0348
Epoch 50/300, seasonal_3 Loss: 0.0632 | 0.0348
Epoch 51/300, seasonal_3 Loss: 0.0632 | 0.0348
Epoch 52/300, seasonal_3 Loss: 0.0631 | 0.0346
Epoch 53/300, seasonal_3 Loss: 0.0631 | 0.0346
Epoch 54/300, seasonal_3 Loss: 0.0630 | 0.0346
Epoch 55/300, seasonal_3 Loss: 0.0630 | 0.0345
Epoch 56/300, seasonal_3 Loss: 0.0630 | 0.0345
Epoch 57/300, seasonal_3 Loss: 0.0630 | 0.0345
Epoch 58/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 59/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 60/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 61/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 62/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 63/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 64/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 65/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 66/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 67/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 68/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 69/300, seasonal_3 Loss: 0.0628 | 0.0344
Epoch 70/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 71/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 72/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 73/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 74/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 75/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 76/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 77/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 78/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 79/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 80/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 81/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 82/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 83/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 84/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 85/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 86/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 87/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 88/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 89/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 90/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 91/300, seasonal_3 Loss: 0.0627 | 0.0344
Epoch 92/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 93/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 94/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 95/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 96/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 97/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 98/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 99/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 100/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 101/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 102/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 103/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 104/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 105/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 106/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 107/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 108/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 109/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 110/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 111/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 112/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 113/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 114/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 115/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 116/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 117/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 118/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 119/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 120/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 121/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 122/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 123/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 124/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 125/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 126/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 127/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 128/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 129/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 130/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 131/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 132/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 133/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 134/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 135/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 136/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 137/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 138/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 139/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 140/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 141/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 142/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 143/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 144/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 145/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 146/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 147/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 148/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 149/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 150/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 151/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 152/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 153/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 154/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 155/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 156/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 157/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 158/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 159/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 160/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 161/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 162/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 163/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 164/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 165/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 166/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 167/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 168/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 169/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 170/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 171/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 172/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 173/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 174/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 175/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 176/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 177/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 178/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 179/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 180/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 181/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 182/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 183/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 184/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 185/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 186/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 187/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 188/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 189/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 190/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 191/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 192/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 193/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 194/300, seasonal_3 Loss: 0.0626 | 0.0344
Epoch 195/300, seasonal_3 Loss: 0.0626 | 0.0344
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 8, 'train_rates': 0.8096988905402002, 'learning_rate': 0.0003277494629021198, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9863436049050169}
Epoch 1/300, resid Loss: 0.2643 | 0.1881
Epoch 2/300, resid Loss: 0.1505 | 0.1842
Epoch 3/300, resid Loss: 0.1477 | 0.1701
Epoch 4/300, resid Loss: 0.1418 | 0.1177
Epoch 5/300, resid Loss: 0.1248 | 0.0934
Epoch 6/300, resid Loss: 0.1428 | 0.1373
Epoch 7/300, resid Loss: 0.1466 | 0.0880
Epoch 8/300, resid Loss: 0.1304 | 0.0795
Epoch 9/300, resid Loss: 0.1214 | 0.0897
Epoch 10/300, resid Loss: 0.1242 | 0.0937
Epoch 11/300, resid Loss: 0.1183 | 0.0703
Epoch 12/300, resid Loss: 0.1086 | 0.0707
Epoch 13/300, resid Loss: 0.1086 | 0.0736
Epoch 14/300, resid Loss: 0.1116 | 0.0913
Epoch 15/300, resid Loss: 0.1222 | 0.1587
Epoch 16/300, resid Loss: 0.1413 | 0.0911
Epoch 17/300, resid Loss: 0.1254 | 0.0911
Epoch 18/300, resid Loss: 0.1254 | 0.1270
Epoch 19/300, resid Loss: 0.1165 | 0.1000
Epoch 20/300, resid Loss: 0.1048 | 0.0830
Epoch 21/300, resid Loss: 0.0999 | 0.0620
Epoch 22/300, resid Loss: 0.0969 | 0.0532
Epoch 23/300, resid Loss: 0.0970 | 0.0616
Epoch 24/300, resid Loss: 0.1018 | 0.0776
Epoch 25/300, resid Loss: 0.1032 | 0.0612
Epoch 26/300, resid Loss: 0.0956 | 0.0506
Epoch 27/300, resid Loss: 0.0888 | 0.0485
Epoch 28/300, resid Loss: 0.0894 | 0.0502
Epoch 29/300, resid Loss: 0.0928 | 0.0518
Epoch 30/300, resid Loss: 0.0947 | 0.0513
Epoch 31/300, resid Loss: 0.0917 | 0.0499
Epoch 32/300, resid Loss: 0.0868 | 0.0462
Epoch 33/300, resid Loss: 0.0848 | 0.0523
Epoch 34/300, resid Loss: 0.0932 | 0.0690
Epoch 35/300, resid Loss: 0.1066 | 0.1017
Epoch 36/300, resid Loss: 0.1038 | 0.0743
Epoch 37/300, resid Loss: 0.0885 | 0.0445
Epoch 38/300, resid Loss: 0.0845 | 0.0546
Epoch 39/300, resid Loss: 0.0886 | 0.0451
Epoch 40/300, resid Loss: 0.0818 | 0.0522
Epoch 41/300, resid Loss: 0.0806 | 0.0436
Epoch 42/300, resid Loss: 0.0786 | 0.0403
Epoch 43/300, resid Loss: 0.0766 | 0.0440
Epoch 44/300, resid Loss: 0.0761 | 0.0418
Epoch 45/300, resid Loss: 0.0761 | 0.0424
Epoch 46/300, resid Loss: 0.0776 | 0.0423
Epoch 47/300, resid Loss: 0.0776 | 0.0418
Epoch 48/300, resid Loss: 0.0756 | 0.0392
Epoch 49/300, resid Loss: 0.0748 | 0.0388
Epoch 50/300, resid Loss: 0.0777 | 0.0412
Epoch 51/300, resid Loss: 0.0813 | 0.0431
Epoch 52/300, resid Loss: 0.0819 | 0.0414
Epoch 53/300, resid Loss: 0.0766 | 0.0435
Epoch 54/300, resid Loss: 0.0764 | 0.0468
Epoch 55/300, resid Loss: 0.0805 | 0.0482
Epoch 56/300, resid Loss: 0.0815 | 0.0480
Epoch 57/300, resid Loss: 0.0767 | 0.0387
Epoch 58/300, resid Loss: 0.0762 | 0.0437
Epoch 59/300, resid Loss: 0.0797 | 0.0442
Epoch 60/300, resid Loss: 0.0794 | 0.0458
Epoch 61/300, resid Loss: 0.0786 | 0.0473
Epoch 62/300, resid Loss: 0.0772 | 0.0417
Epoch 63/300, resid Loss: 0.0759 | 0.0397
Epoch 64/300, resid Loss: 0.0758 | 0.0407
Epoch 65/300, resid Loss: 0.0734 | 0.0381
Epoch 66/300, resid Loss: 0.0702 | 0.0354
Epoch 67/300, resid Loss: 0.0714 | 0.0385
Epoch 68/300, resid Loss: 0.0729 | 0.0371
Epoch 69/300, resid Loss: 0.0728 | 0.0384
Epoch 70/300, resid Loss: 0.0709 | 0.0390
Epoch 71/300, resid Loss: 0.0705 | 0.0399
Epoch 72/300, resid Loss: 0.0703 | 0.0423
Epoch 73/300, resid Loss: 0.0711 | 0.0399
Epoch 74/300, resid Loss: 0.0700 | 0.0372
Epoch 75/300, resid Loss: 0.0692 | 0.0367
Epoch 76/300, resid Loss: 0.0703 | 0.0374
Epoch 77/300, resid Loss: 0.0697 | 0.0367
Epoch 78/300, resid Loss: 0.0680 | 0.0345
Epoch 79/300, resid Loss: 0.0694 | 0.0386
Epoch 80/300, resid Loss: 0.0765 | 0.0473
Epoch 81/300, resid Loss: 0.0798 | 0.0437
Epoch 82/300, resid Loss: 0.0777 | 0.0515
Epoch 83/300, resid Loss: 0.0754 | 0.0452
Epoch 84/300, resid Loss: 0.0700 | 0.0400
Epoch 85/300, resid Loss: 0.0693 | 0.0410
Epoch 86/300, resid Loss: 0.0698 | 0.0378
Epoch 87/300, resid Loss: 0.0688 | 0.0359
Epoch 88/300, resid Loss: 0.0670 | 0.0348
Epoch 89/300, resid Loss: 0.0664 | 0.0329
Epoch 90/300, resid Loss: 0.0670 | 0.0322
Epoch 91/300, resid Loss: 0.0676 | 0.0336
Epoch 92/300, resid Loss: 0.0673 | 0.0333
Epoch 93/300, resid Loss: 0.0661 | 0.0325
Epoch 94/300, resid Loss: 0.0652 | 0.0327
Epoch 95/300, resid Loss: 0.0648 | 0.0320
Epoch 96/300, resid Loss: 0.0652 | 0.0315
Epoch 97/300, resid Loss: 0.0666 | 0.0355
Epoch 98/300, resid Loss: 0.0702 | 0.0394
Epoch 99/300, resid Loss: 0.0696 | 0.0350
Epoch 100/300, resid Loss: 0.0704 | 0.0390
Epoch 101/300, resid Loss: 0.0840 | 0.0502
Epoch 102/300, resid Loss: 0.0813 | 0.0419
Epoch 103/300, resid Loss: 0.0782 | 0.0436
Epoch 104/300, resid Loss: 0.0796 | 0.0405
Epoch 105/300, resid Loss: 0.0740 | 0.0389
Epoch 106/300, resid Loss: 0.0714 | 0.0400
Epoch 107/300, resid Loss: 0.0741 | 0.0370
Epoch 108/300, resid Loss: 0.0746 | 0.0397
Epoch 109/300, resid Loss: 0.0766 | 0.0397
Epoch 110/300, resid Loss: 0.0756 | 0.0363
Epoch 111/300, resid Loss: 0.0739 | 0.0351
Epoch 112/300, resid Loss: 0.0713 | 0.0336
Epoch 113/300, resid Loss: 0.0696 | 0.0401
Epoch 114/300, resid Loss: 0.0714 | 0.0366
Epoch 115/300, resid Loss: 0.0743 | 0.0356
Epoch 116/300, resid Loss: 0.0733 | 0.0414
Epoch 117/300, resid Loss: 0.0715 | 0.0358
Epoch 118/300, resid Loss: 0.0699 | 0.0427
Epoch 119/300, resid Loss: 0.0727 | 0.0425
Epoch 120/300, resid Loss: 0.0747 | 0.0471
Epoch 121/300, resid Loss: 0.0765 | 0.0416
Epoch 122/300, resid Loss: 0.0719 | 0.0419
Epoch 123/300, resid Loss: 0.0672 | 0.0316
Epoch 124/300, resid Loss: 0.0650 | 0.0336
Epoch 125/300, resid Loss: 0.0716 | 0.0372
Epoch 126/300, resid Loss: 0.0720 | 0.0360
Epoch 127/300, resid Loss: 0.0715 | 0.0364
Epoch 128/300, resid Loss: 0.0679 | 0.0348
Epoch 129/300, resid Loss: 0.0642 | 0.0339
Epoch 130/300, resid Loss: 0.0629 | 0.0360
Epoch 131/300, resid Loss: 0.0656 | 0.0349
Epoch 132/300, resid Loss: 0.0657 | 0.0336
Epoch 133/300, resid Loss: 0.0642 | 0.0368
Epoch 134/300, resid Loss: 0.0644 | 0.0345
Epoch 135/300, resid Loss: 0.0653 | 0.0339
Epoch 136/300, resid Loss: 0.0652 | 0.0330
Epoch 137/300, resid Loss: 0.0641 | 0.0335
Epoch 138/300, resid Loss: 0.0625 | 0.0298
Epoch 139/300, resid Loss: 0.0609 | 0.0305
Epoch 140/300, resid Loss: 0.0624 | 0.0360
Epoch 141/300, resid Loss: 0.0646 | 0.0366
Epoch 142/300, resid Loss: 0.0655 | 0.0366
Epoch 143/300, resid Loss: 0.0636 | 0.0359
Epoch 144/300, resid Loss: 0.0617 | 0.0310
Epoch 145/300, resid Loss: 0.0613 | 0.0304
Epoch 146/300, resid Loss: 0.0610 | 0.0314
Epoch 147/300, resid Loss: 0.0605 | 0.0319
Epoch 148/300, resid Loss: 0.0597 | 0.0304
Epoch 149/300, resid Loss: 0.0596 | 0.0313
Epoch 150/300, resid Loss: 0.0601 | 0.0320
Epoch 151/300, resid Loss: 0.0616 | 0.0328
Epoch 152/300, resid Loss: 0.0625 | 0.0306
Epoch 153/300, resid Loss: 0.0599 | 0.0323
Epoch 154/300, resid Loss: 0.0597 | 0.0324
Epoch 155/300, resid Loss: 0.0598 | 0.0313
Epoch 156/300, resid Loss: 0.0594 | 0.0312
Epoch 157/300, resid Loss: 0.0597 | 0.0315
Epoch 158/300, resid Loss: 0.0604 | 0.0311
Epoch 159/300, resid Loss: 0.0617 | 0.0386
Epoch 160/300, resid Loss: 0.0634 | 0.0338
Epoch 161/300, resid Loss: 0.0659 | 0.0422
Epoch 162/300, resid Loss: 0.0678 | 0.0390
Epoch 163/300, resid Loss: 0.0624 | 0.0329
Epoch 164/300, resid Loss: 0.0616 | 0.0344
Epoch 165/300, resid Loss: 0.0633 | 0.0366
Epoch 166/300, resid Loss: 0.0658 | 0.0380
Epoch 167/300, resid Loss: 0.0659 | 0.0387
Epoch 168/300, resid Loss: 0.0627 | 0.0313
Epoch 169/300, resid Loss: 0.0609 | 0.0333
Epoch 170/300, resid Loss: 0.0613 | 0.0322
Epoch 171/300, resid Loss: 0.0644 | 0.0350
Epoch 172/300, resid Loss: 0.0628 | 0.0323
Epoch 173/300, resid Loss: 0.0600 | 0.0330
Epoch 174/300, resid Loss: 0.0586 | 0.0331
Epoch 175/300, resid Loss: 0.0590 | 0.0347
Epoch 176/300, resid Loss: 0.0597 | 0.0312
Epoch 177/300, resid Loss: 0.0570 | 0.0316
Epoch 178/300, resid Loss: 0.0563 | 0.0337
Epoch 179/300, resid Loss: 0.0565 | 0.0316
Epoch 180/300, resid Loss: 0.0559 | 0.0304
Epoch 181/300, resid Loss: 0.0558 | 0.0321
Epoch 182/300, resid Loss: 0.0557 | 0.0301
Epoch 183/300, resid Loss: 0.0553 | 0.0308
Epoch 184/300, resid Loss: 0.0550 | 0.0319
Epoch 185/300, resid Loss: 0.0550 | 0.0314
Epoch 186/300, resid Loss: 0.0551 | 0.0314
Epoch 187/300, resid Loss: 0.0559 | 0.0335
Epoch 188/300, resid Loss: 0.0569 | 0.0319
Epoch 189/300, resid Loss: 0.0553 | 0.0308
Epoch 190/300, resid Loss: 0.0552 | 0.0317
Epoch 191/300, resid Loss: 0.0556 | 0.0330
Epoch 192/300, resid Loss: 0.0560 | 0.0350
Epoch 193/300, resid Loss: 0.0550 | 0.0329
Epoch 194/300, resid Loss: 0.0546 | 0.0313
Epoch 195/300, resid Loss: 0.0546 | 0.0316
Epoch 196/300, resid Loss: 0.0543 | 0.0309
Epoch 197/300, resid Loss: 0.0542 | 0.0324
Epoch 198/300, resid Loss: 0.0544 | 0.0334
Epoch 199/300, resid Loss: 0.0550 | 0.0331
Epoch 200/300, resid Loss: 0.0554 | 0.0322
Epoch 201/300, resid Loss: 0.0566 | 0.0334
Epoch 202/300, resid Loss: 0.0568 | 0.0331
Epoch 203/300, resid Loss: 0.0555 | 0.0349
Epoch 204/300, resid Loss: 0.0547 | 0.0347
Epoch 205/300, resid Loss: 0.0553 | 0.0338
Epoch 206/300, resid Loss: 0.0564 | 0.0325
Epoch 207/300, resid Loss: 0.0578 | 0.0355
Epoch 208/300, resid Loss: 0.0604 | 0.0403
Epoch 209/300, resid Loss: 0.0597 | 0.0351
Epoch 210/300, resid Loss: 0.0601 | 0.0512
Epoch 211/300, resid Loss: 0.0590 | 0.0351
Epoch 212/300, resid Loss: 0.0601 | 0.0374
Epoch 213/300, resid Loss: 0.0594 | 0.0364
Epoch 214/300, resid Loss: 0.0595 | 0.0335
Epoch 215/300, resid Loss: 0.0568 | 0.0315
Epoch 216/300, resid Loss: 0.0546 | 0.0340
Epoch 217/300, resid Loss: 0.0549 | 0.0334
Epoch 218/300, resid Loss: 0.0552 | 0.0317
Epoch 219/300, resid Loss: 0.0548 | 0.0344
Epoch 220/300, resid Loss: 0.0542 | 0.0350
Epoch 221/300, resid Loss: 0.0534 | 0.0327
Epoch 222/300, resid Loss: 0.0536 | 0.0340
Epoch 223/300, resid Loss: 0.0546 | 0.0342
Epoch 224/300, resid Loss: 0.0529 | 0.0325
Epoch 225/300, resid Loss: 0.0518 | 0.0329
Epoch 226/300, resid Loss: 0.0521 | 0.0369
Epoch 227/300, resid Loss: 0.0531 | 0.0358
Epoch 228/300, resid Loss: 0.0527 | 0.0333
Epoch 229/300, resid Loss: 0.0522 | 0.0360
Epoch 230/300, resid Loss: 0.0522 | 0.0360
Epoch 231/300, resid Loss: 0.0529 | 0.0359
Epoch 232/300, resid Loss: 0.0516 | 0.0438
Epoch 233/300, resid Loss: 0.0503 | 0.0406
Epoch 234/300, resid Loss: 0.0488 | 0.0328
Epoch 235/300, resid Loss: 0.0529 | 0.0310
Epoch 236/300, resid Loss: 0.0524 | 0.0315
Epoch 237/300, resid Loss: 0.0521 | 0.0326
Epoch 238/300, resid Loss: 0.0514 | 0.0335
Epoch 239/300, resid Loss: 0.0513 | 0.0322
Epoch 240/300, resid Loss: 0.0511 | 0.0325
Epoch 241/300, resid Loss: 0.0510 | 0.0334
Epoch 242/300, resid Loss: 0.0512 | 0.0346
Epoch 243/300, resid Loss: 0.0513 | 0.0330
Epoch 244/300, resid Loss: 0.0507 | 0.0341
Epoch 245/300, resid Loss: 0.0505 | 0.0339
Epoch 246/300, resid Loss: 0.0502 | 0.0333
Epoch 247/300, resid Loss: 0.0502 | 0.0344
Epoch 248/300, resid Loss: 0.0504 | 0.0347
Epoch 249/300, resid Loss: 0.0504 | 0.0343
Epoch 250/300, resid Loss: 0.0501 | 0.0351
Epoch 251/300, resid Loss: 0.0498 | 0.0352
Epoch 252/300, resid Loss: 0.0500 | 0.0352
Epoch 253/300, resid Loss: 0.0499 | 0.0359
Epoch 254/300, resid Loss: 0.0498 | 0.0373
Epoch 255/300, resid Loss: 0.0496 | 0.0370
Epoch 256/300, resid Loss: 0.0489 | 0.0414
Epoch 257/300, resid Loss: 0.0478 | 0.0536
Epoch 258/300, resid Loss: 0.0472 | 0.0378
Epoch 259/300, resid Loss: 0.0524 | 0.0361
Epoch 260/300, resid Loss: 0.0517 | 0.0363
Epoch 261/300, resid Loss: 0.0490 | 0.0462
Epoch 262/300, resid Loss: 0.0504 | 0.0840
Epoch 263/300, resid Loss: 0.0529 | 0.0431
Epoch 264/300, resid Loss: 0.0552 | 0.0357
Epoch 265/300, resid Loss: 0.0533 | 0.0372
Epoch 266/300, resid Loss: 0.0533 | 0.0389
Epoch 267/300, resid Loss: 0.0554 | 0.0380
Epoch 268/300, resid Loss: 0.0515 | 0.0368
Epoch 269/300, resid Loss: 0.0462 | 0.0542
Epoch 270/300, resid Loss: 0.0495 | 0.0463
Epoch 271/300, resid Loss: 0.0553 | 0.0388
Epoch 272/300, resid Loss: 0.0529 | 0.0348
Epoch 273/300, resid Loss: 0.0522 | 0.0356
Epoch 274/300, resid Loss: 0.0513 | 0.0371
Epoch 275/300, resid Loss: 0.0449 | 0.0390
Epoch 276/300, resid Loss: 0.0452 | 0.0383
Epoch 277/300, resid Loss: 0.0448 | 0.0384
Epoch 278/300, resid Loss: 0.0509 | 0.0402
Epoch 279/300, resid Loss: 0.0540 | 0.0357
Epoch 280/300, resid Loss: 0.0520 | 0.0350
Epoch 281/300, resid Loss: 0.0511 | 0.0373
Epoch 282/300, resid Loss: 0.0485 | 0.0410
Epoch 283/300, resid Loss: 0.0484 | 0.0483
Epoch 284/300, resid Loss: 0.0512 | 0.0346
Epoch 285/300, resid Loss: 0.0444 | 0.0377
Epoch 286/300, resid Loss: 0.0437 | 0.0363
Epoch 287/300, resid Loss: 0.0437 | 0.0399
Epoch 288/300, resid Loss: 0.0421 | 0.0370
Epoch 289/300, resid Loss: 0.0463 | 0.0374
Epoch 290/300, resid Loss: 0.0437 | 0.0570
Epoch 291/300, resid Loss: 0.0416 | 0.0386
Epoch 292/300, resid Loss: 0.0409 | 0.0364
Epoch 293/300, resid Loss: 0.0405 | 0.0383
Epoch 294/300, resid Loss: 0.0405 | 0.0378
Epoch 295/300, resid Loss: 0.0403 | 0.0381
Epoch 296/300, resid Loss: 0.0400 | 0.0381
Epoch 297/300, resid Loss: 0.0400 | 0.0387
Epoch 298/300, resid Loss: 0.0399 | 0.0384
Epoch 299/300, resid Loss: 0.0400 | 0.0392
Epoch 300/300, resid Loss: 0.0399 | 0.0380
Runtime (seconds): 1249.334543466568
0.0005946047055748292
[153.25412]
[-0.49761155]
[-5.0613065]
[10.167953]
[0.7305356]
[3.9327438]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 62.41351994709112
RMSE: 7.9002227783203125
MAE: 7.9002227783203125
R-squared: nan
[162.52643]
