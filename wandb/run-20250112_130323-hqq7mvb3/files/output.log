ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-12 13:03:28,490][0m A new study created in memory with name: no-name-7590fec9-dcbd-4a99-98d0-b2f794929c41[0m
[32m[I 2025-01-12 13:03:58,959][0m Trial 0 finished with value: 0.11473296164338145 and parameters: {'observation_period_num': 240, 'train_rates': 0.6496651207237796, 'learning_rate': 0.0003642356659508269, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8061929907588089}. Best is trial 0 with value: 0.11473296164338145.[0m
[32m[I 2025-01-12 13:04:42,338][0m Trial 1 finished with value: 0.10261316635186159 and parameters: {'observation_period_num': 200, 'train_rates': 0.8281213066131423, 'learning_rate': 0.0002570866316407707, 'batch_size': 115, 'step_size': 15, 'gamma': 0.7886420004432817}. Best is trial 1 with value: 0.10261316635186159.[0m
[32m[I 2025-01-12 13:05:06,949][0m Trial 2 finished with value: 0.16320355179218146 and parameters: {'observation_period_num': 55, 'train_rates': 0.8205927659106407, 'learning_rate': 8.676232477117498e-05, 'batch_size': 229, 'step_size': 13, 'gamma': 0.7710000310279015}. Best is trial 1 with value: 0.10261316635186159.[0m
[32m[I 2025-01-12 13:05:34,234][0m Trial 3 finished with value: 0.08275224295132323 and parameters: {'observation_period_num': 94, 'train_rates': 0.7852185057191323, 'learning_rate': 0.0003494326377509551, 'batch_size': 194, 'step_size': 14, 'gamma': 0.7502915240944583}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:10:17,613][0m Trial 4 finished with value: 0.16765410734074457 and parameters: {'observation_period_num': 126, 'train_rates': 0.8762398349775569, 'learning_rate': 1.7238574385471545e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7967966498016219}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:10:38,648][0m Trial 5 finished with value: 0.18516121391488724 and parameters: {'observation_period_num': 239, 'train_rates': 0.7807277740879496, 'learning_rate': 0.0004899878815024525, 'batch_size': 254, 'step_size': 4, 'gamma': 0.8854682708690751}. Best is trial 3 with value: 0.08275224295132323.[0m
Early stopping at epoch 91
[32m[I 2025-01-12 13:11:04,998][0m Trial 6 finished with value: 1.2192485341606358 and parameters: {'observation_period_num': 69, 'train_rates': 0.8188795152971146, 'learning_rate': 3.985186982672083e-06, 'batch_size': 186, 'step_size': 2, 'gamma': 0.80061823895718}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:12:10,661][0m Trial 7 finished with value: 0.14550782825290193 and parameters: {'observation_period_num': 210, 'train_rates': 0.8422247298736942, 'learning_rate': 0.0009126228945944046, 'batch_size': 75, 'step_size': 13, 'gamma': 0.8671480338176756}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:13:12,696][0m Trial 8 finished with value: 1.132426659266154 and parameters: {'observation_period_num': 101, 'train_rates': 0.9085267412258431, 'learning_rate': 1.6608580425619465e-06, 'batch_size': 87, 'step_size': 10, 'gamma': 0.8048545686666371}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:14:00,077][0m Trial 9 finished with value: 0.7563549876213074 and parameters: {'observation_period_num': 192, 'train_rates': 0.9793153841149198, 'learning_rate': 6.419956036081094e-06, 'batch_size': 121, 'step_size': 6, 'gamma': 0.8640008648786738}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:14:26,017][0m Trial 10 finished with value: 0.11312390654375006 and parameters: {'observation_period_num': 22, 'train_rates': 0.7076631923227936, 'learning_rate': 6.820334584532972e-05, 'batch_size': 195, 'step_size': 10, 'gamma': 0.9672417692650056}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:14:55,317][0m Trial 11 finished with value: 0.18337910593780754 and parameters: {'observation_period_num': 159, 'train_rates': 0.7389317178452841, 'learning_rate': 0.00016019050850587697, 'batch_size': 161, 'step_size': 15, 'gamma': 0.7567586110316958}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:15:41,615][0m Trial 12 finished with value: 0.1025702680729916 and parameters: {'observation_period_num': 162, 'train_rates': 0.7542950313544745, 'learning_rate': 0.00020667118962658848, 'batch_size': 101, 'step_size': 14, 'gamma': 0.7509627247087297}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:17:01,619][0m Trial 13 finished with value: 0.31326431603969646 and parameters: {'observation_period_num': 147, 'train_rates': 0.674772906404833, 'learning_rate': 3.201973004300166e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.7502795945415432}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:17:24,191][0m Trial 14 finished with value: 0.1104899230379638 and parameters: {'observation_period_num': 101, 'train_rates': 0.6101282808306924, 'learning_rate': 0.0009629843248053091, 'batch_size': 194, 'step_size': 6, 'gamma': 0.9172416707515146}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:17:55,884][0m Trial 15 finished with value: 0.18728822058865002 and parameters: {'observation_period_num': 164, 'train_rates': 0.7489592359834812, 'learning_rate': 0.00012425011434017756, 'batch_size': 157, 'step_size': 11, 'gamma': 0.8345752426890104}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:18:44,584][0m Trial 16 finished with value: 0.2180307959927652 and parameters: {'observation_period_num': 111, 'train_rates': 0.7660177721498735, 'learning_rate': 4.364067015740524e-05, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8340566857649472}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:19:07,541][0m Trial 17 finished with value: 0.7628252447778733 and parameters: {'observation_period_num': 44, 'train_rates': 0.709222649688884, 'learning_rate': 1.3122079690363562e-05, 'batch_size': 221, 'step_size': 12, 'gamma': 0.833587163333991}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:19:43,590][0m Trial 18 finished with value: 0.17885537349019456 and parameters: {'observation_period_num': 78, 'train_rates': 0.9142048212482823, 'learning_rate': 0.00023856380109721305, 'batch_size': 162, 'step_size': 7, 'gamma': 0.9860393481314369}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:21:23,872][0m Trial 19 finished with value: 0.3277670592069626 and parameters: {'observation_period_num': 177, 'train_rates': 0.7186478538349116, 'learning_rate': 0.0004935369219891942, 'batch_size': 44, 'step_size': 8, 'gamma': 0.9285072998989601}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:22:01,447][0m Trial 20 finished with value: 0.30383291152807385 and parameters: {'observation_period_num': 136, 'train_rates': 0.7878577969483426, 'learning_rate': 5.6614204980876745e-05, 'batch_size': 138, 'step_size': 14, 'gamma': 0.7708917609689089}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:22:48,158][0m Trial 21 finished with value: 0.1297146582580877 and parameters: {'observation_period_num': 211, 'train_rates': 0.8558468026607381, 'learning_rate': 0.00017933243246634557, 'batch_size': 111, 'step_size': 15, 'gamma': 0.7757013066204054}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:23:46,652][0m Trial 22 finished with value: 0.09288654532084685 and parameters: {'observation_period_num': 197, 'train_rates': 0.7963616700243493, 'learning_rate': 0.00027745515295207453, 'batch_size': 84, 'step_size': 14, 'gamma': 0.7848075087055147}. Best is trial 3 with value: 0.08275224295132323.[0m
[32m[I 2025-01-12 13:24:48,958][0m Trial 23 finished with value: 0.07084127058153566 and parameters: {'observation_period_num': 180, 'train_rates': 0.7926298786850695, 'learning_rate': 0.0005067858062038617, 'batch_size': 76, 'step_size': 11, 'gamma': 0.7560040388980285}. Best is trial 23 with value: 0.07084127058153566.[0m
[32m[I 2025-01-12 13:26:01,921][0m Trial 24 finished with value: 0.13000722883279406 and parameters: {'observation_period_num': 226, 'train_rates': 0.7993510273664244, 'learning_rate': 0.0005831821845878181, 'batch_size': 66, 'step_size': 11, 'gamma': 0.8169455241714257}. Best is trial 23 with value: 0.07084127058153566.[0m
[32m[I 2025-01-12 13:28:53,022][0m Trial 25 finished with value: 0.06670752388378742 and parameters: {'observation_period_num': 127, 'train_rates': 0.8794754075645956, 'learning_rate': 0.00010859704568477469, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7773701429712634}. Best is trial 25 with value: 0.06670752388378742.[0m
[32m[I 2025-01-12 13:34:13,353][0m Trial 26 finished with value: 0.05271576217823875 and parameters: {'observation_period_num': 122, 'train_rates': 0.8967007258760927, 'learning_rate': 0.00010685566781066802, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7647103928311025}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:38:36,021][0m Trial 27 finished with value: 0.06886548911662478 and parameters: {'observation_period_num': 115, 'train_rates': 0.9600531758099528, 'learning_rate': 0.00010935231837427062, 'batch_size': 21, 'step_size': 9, 'gamma': 0.7694906190724576}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:42:38,773][0m Trial 28 finished with value: 0.08268391358297925 and parameters: {'observation_period_num': 128, 'train_rates': 0.9848513277437456, 'learning_rate': 9.922235594822275e-05, 'batch_size': 23, 'step_size': 9, 'gamma': 0.8510784612664645}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:45:04,625][0m Trial 29 finished with value: 0.25751239502871476 and parameters: {'observation_period_num': 85, 'train_rates': 0.9435518076580041, 'learning_rate': 1.7935632293942014e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8185917871005404}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:47:47,153][0m Trial 30 finished with value: 0.13131132751703262 and parameters: {'observation_period_num': 113, 'train_rates': 0.9472343167642091, 'learning_rate': 3.91616251374839e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.8895540780258677}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:53:01,891][0m Trial 31 finished with value: 0.05900287694191631 and parameters: {'observation_period_num': 148, 'train_rates': 0.8874392215902341, 'learning_rate': 0.0001369126877490804, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7650824557632274}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:58:18,404][0m Trial 32 finished with value: 0.06739240427418511 and parameters: {'observation_period_num': 142, 'train_rates': 0.8921770045435189, 'learning_rate': 0.000124358579735754, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7850520044483531}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 13:59:54,922][0m Trial 33 finished with value: 0.11187956319998492 and parameters: {'observation_period_num': 145, 'train_rates': 0.8873026327339159, 'learning_rate': 6.850554314650311e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.7929408378255718}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:02:20,393][0m Trial 34 finished with value: 0.08296120174083055 and parameters: {'observation_period_num': 144, 'train_rates': 0.8731857291150089, 'learning_rate': 0.0001349523041075984, 'batch_size': 35, 'step_size': 10, 'gamma': 0.7828664319255008}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:07:10,522][0m Trial 35 finished with value: 0.055203280764153276 and parameters: {'observation_period_num': 124, 'train_rates': 0.9113365251005738, 'learning_rate': 7.764768177823109e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8175116023416709}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:08:48,515][0m Trial 36 finished with value: 0.13988857635972546 and parameters: {'observation_period_num': 119, 'train_rates': 0.9280649280389555, 'learning_rate': 4.950773269817653e-05, 'batch_size': 55, 'step_size': 12, 'gamma': 0.8161592735014658}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:11:47,720][0m Trial 37 finished with value: 0.09953061126503247 and parameters: {'observation_period_num': 53, 'train_rates': 0.8679076605081998, 'learning_rate': 2.2923746137920885e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.7672835734748662}. Best is trial 26 with value: 0.05271576217823875.[0m
Early stopping at epoch 55
[32m[I 2025-01-12 14:12:49,130][0m Trial 38 finished with value: 0.5051187662662807 and parameters: {'observation_period_num': 94, 'train_rates': 0.8313898875407614, 'learning_rate': 7.84802321430922e-05, 'batch_size': 46, 'step_size': 1, 'gamma': 0.80212428502001}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:18:03,079][0m Trial 39 finished with value: 0.08229185724741385 and parameters: {'observation_period_num': 69, 'train_rates': 0.9123629002619426, 'learning_rate': 3.0873886353432965e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.762960652847616}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:19:17,345][0m Trial 40 finished with value: 0.05974878465181672 and parameters: {'observation_period_num': 127, 'train_rates': 0.8464059442041919, 'learning_rate': 0.0003763382776460245, 'batch_size': 69, 'step_size': 13, 'gamma': 0.7925826650997221}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:20:34,065][0m Trial 41 finished with value: 0.10001558035821001 and parameters: {'observation_period_num': 129, 'train_rates': 0.8487170154072952, 'learning_rate': 0.000378205682515165, 'batch_size': 66, 'step_size': 13, 'gamma': 0.7947020828367001}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:23:32,788][0m Trial 42 finished with value: 0.13933376175626902 and parameters: {'observation_period_num': 157, 'train_rates': 0.8984134169710186, 'learning_rate': 0.00032140941301951755, 'batch_size': 29, 'step_size': 12, 'gamma': 0.7769938599257992}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:25:18,135][0m Trial 43 finished with value: 0.07478635575637281 and parameters: {'observation_period_num': 178, 'train_rates': 0.8207373592751864, 'learning_rate': 0.00020253699920849466, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8082988026037002}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:26:46,280][0m Trial 44 finished with value: 0.12560638677901947 and parameters: {'observation_period_num': 128, 'train_rates': 0.9264570589434786, 'learning_rate': 7.763630084997145e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.7623658276635287}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:29:21,500][0m Trial 45 finished with value: 0.05363882056223956 and parameters: {'observation_period_num': 104, 'train_rates': 0.86086451097175, 'learning_rate': 0.00016604206367249594, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7918595596961264}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:31:12,050][0m Trial 46 finished with value: 0.15446919144748092 and parameters: {'observation_period_num': 106, 'train_rates': 0.8649045057928991, 'learning_rate': 0.0007129492454440413, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8457098043462009}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:33:26,627][0m Trial 47 finished with value: 0.06429199563009212 and parameters: {'observation_period_num': 97, 'train_rates': 0.8400132364637469, 'learning_rate': 0.0003637727042027537, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8076313144987037}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:38:30,497][0m Trial 48 finished with value: 0.8325585208155892 and parameters: {'observation_period_num': 80, 'train_rates': 0.8164342862866916, 'learning_rate': 1.3404498016950761e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7936403618456143}. Best is trial 26 with value: 0.05271576217823875.[0m
[32m[I 2025-01-12 14:39:16,492][0m Trial 49 finished with value: 0.1374528706073761 and parameters: {'observation_period_num': 154, 'train_rates': 0.9619687897488206, 'learning_rate': 0.0001568089954669868, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8257198445801056}. Best is trial 26 with value: 0.05271576217823875.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-12 14:39:16,502][0m A new study created in memory with name: no-name-6eb51190-1ab7-4e98-b7c1-eb821c973c1c[0m
[32m[I 2025-01-12 14:39:40,712][0m Trial 0 finished with value: 0.10862486064434052 and parameters: {'observation_period_num': 224, 'train_rates': 0.9406267545864253, 'learning_rate': 0.0005793951960041539, 'batch_size': 242, 'step_size': 2, 'gamma': 0.9679358846079238}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:40:04,791][0m Trial 1 finished with value: 1.4331696612312195 and parameters: {'observation_period_num': 151, 'train_rates': 0.6663225384595051, 'learning_rate': 1.3320222129685557e-06, 'batch_size': 196, 'step_size': 9, 'gamma': 0.9244010460365462}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:40:33,082][0m Trial 2 finished with value: 0.1431066256390442 and parameters: {'observation_period_num': 188, 'train_rates': 0.7153475964872211, 'learning_rate': 0.0001001315373579088, 'batch_size': 166, 'step_size': 13, 'gamma': 0.7920944820218646}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:40:55,064][0m Trial 3 finished with value: 0.27500902114522613 and parameters: {'observation_period_num': 77, 'train_rates': 0.7085104792374304, 'learning_rate': 0.00026789766762453824, 'batch_size': 246, 'step_size': 1, 'gamma': 0.8842715208049462}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:41:31,381][0m Trial 4 finished with value: 0.23840980231761932 and parameters: {'observation_period_num': 114, 'train_rates': 0.9722524029564406, 'learning_rate': 1.4973252532597795e-05, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8804025519172277}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:42:13,309][0m Trial 5 finished with value: 0.30360307373814077 and parameters: {'observation_period_num': 192, 'train_rates': 0.8055426605866842, 'learning_rate': 2.333544337112497e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.7930313184448621}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:42:48,200][0m Trial 6 finished with value: 0.37325809685622946 and parameters: {'observation_period_num': 230, 'train_rates': 0.8752375479614296, 'learning_rate': 7.79461180300628e-06, 'batch_size': 158, 'step_size': 12, 'gamma': 0.7965826562953642}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:43:10,725][0m Trial 7 finished with value: 0.13434152669071603 and parameters: {'observation_period_num': 202, 'train_rates': 0.8422429434300962, 'learning_rate': 0.0004746026648898203, 'batch_size': 232, 'step_size': 8, 'gamma': 0.9465577768624159}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:43:46,380][0m Trial 8 finished with value: 0.4240221019039787 and parameters: {'observation_period_num': 155, 'train_rates': 0.6228752941359923, 'learning_rate': 3.89171373487258e-06, 'batch_size': 124, 'step_size': 15, 'gamma': 0.9472043323769506}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:44:17,545][0m Trial 9 finished with value: 0.6607021776217858 and parameters: {'observation_period_num': 125, 'train_rates': 0.7814547233214437, 'learning_rate': 3.724269764088641e-06, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8322128008675156}. Best is trial 0 with value: 0.10862486064434052.[0m
[32m[I 2025-01-12 14:48:50,534][0m Trial 10 finished with value: 0.032553801157822214 and parameters: {'observation_period_num': 24, 'train_rates': 0.975599102481409, 'learning_rate': 0.0009398360041545116, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9828125439276457}. Best is trial 10 with value: 0.032553801157822214.[0m
[32m[I 2025-01-12 14:53:25,640][0m Trial 11 finished with value: 0.03192827636515027 and parameters: {'observation_period_num': 16, 'train_rates': 0.9773734777199828, 'learning_rate': 0.0009547536754352316, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9750272474197847}. Best is trial 11 with value: 0.03192827636515027.[0m
[32m[I 2025-01-12 14:57:48,968][0m Trial 12 finished with value: 0.031726778874270105 and parameters: {'observation_period_num': 7, 'train_rates': 0.9215344692941814, 'learning_rate': 0.00010117847481860274, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9867450554301873}. Best is trial 12 with value: 0.031726778874270105.[0m
[32m[I 2025-01-12 15:03:11,965][0m Trial 13 finished with value: 0.027538805827498437 and parameters: {'observation_period_num': 5, 'train_rates': 0.9085962360670672, 'learning_rate': 0.00010122252567723479, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9211695486793317}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:04:35,243][0m Trial 14 finished with value: 0.05270712893260153 and parameters: {'observation_period_num': 57, 'train_rates': 0.9016751157777982, 'learning_rate': 7.382967118915153e-05, 'batch_size': 66, 'step_size': 4, 'gamma': 0.9120911290008743}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:05:52,627][0m Trial 15 finished with value: 0.0527949650459184 and parameters: {'observation_period_num': 57, 'train_rates': 0.9065596135815625, 'learning_rate': 8.346745087090518e-05, 'batch_size': 71, 'step_size': 4, 'gamma': 0.9104518974968252}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:07:14,943][0m Trial 16 finished with value: 0.03701296890940587 and parameters: {'observation_period_num': 17, 'train_rates': 0.8361882958978539, 'learning_rate': 0.00017005359752383313, 'batch_size': 64, 'step_size': 4, 'gamma': 0.8371511533715515}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:08:15,674][0m Trial 17 finished with value: 0.086427675918975 and parameters: {'observation_period_num': 82, 'train_rates': 0.9240811945804378, 'learning_rate': 2.9642632246688094e-05, 'batch_size': 93, 'step_size': 6, 'gamma': 0.942393007294355}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:10:25,044][0m Trial 18 finished with value: 0.04148201413244614 and parameters: {'observation_period_num': 45, 'train_rates': 0.8646040468611454, 'learning_rate': 4.921887073020666e-05, 'batch_size': 41, 'step_size': 6, 'gamma': 0.9887956234880464}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:11:16,081][0m Trial 19 finished with value: 0.07900135690930665 and parameters: {'observation_period_num': 100, 'train_rates': 0.762101048583267, 'learning_rate': 0.0002064569077695874, 'batch_size': 96, 'step_size': 3, 'gamma': 0.85940474503305}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:13:30,232][0m Trial 20 finished with value: 0.051582047572502725 and parameters: {'observation_period_num': 38, 'train_rates': 0.9376647713173393, 'learning_rate': 0.00014146889881837946, 'batch_size': 42, 'step_size': 5, 'gamma': 0.754553254314519}. Best is trial 13 with value: 0.027538805827498437.[0m
[32m[I 2025-01-12 15:18:35,230][0m Trial 21 finished with value: 0.02667983944781802 and parameters: {'observation_period_num': 7, 'train_rates': 0.9814452072933391, 'learning_rate': 0.0003130590506239591, 'batch_size': 19, 'step_size': 2, 'gamma': 0.961882828301654}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:21:01,268][0m Trial 22 finished with value: 0.03218224282230599 and parameters: {'observation_period_num': 6, 'train_rates': 0.9438200989509525, 'learning_rate': 0.00028788760024427997, 'batch_size': 39, 'step_size': 3, 'gamma': 0.9580660608424362}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:26:05,243][0m Trial 23 finished with value: 0.03893580243867986 and parameters: {'observation_period_num': 37, 'train_rates': 0.9886139441816697, 'learning_rate': 4.7359131851204046e-05, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9259349505810123}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:27:42,731][0m Trial 24 finished with value: 0.14758085186747005 and parameters: {'observation_period_num': 70, 'train_rates': 0.8887712985723413, 'learning_rate': 0.00035819541192931743, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9634658538277785}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:28:43,114][0m Trial 25 finished with value: 0.048207496079502574 and parameters: {'observation_period_num': 6, 'train_rates': 0.8416529965507973, 'learning_rate': 0.00013731437551522632, 'batch_size': 89, 'step_size': 2, 'gamma': 0.9013360847143473}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:34:23,240][0m Trial 26 finished with value: 0.03923487049921038 and parameters: {'observation_period_num': 28, 'train_rates': 0.9183917963273683, 'learning_rate': 4.7508872911679395e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9874507716679123}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:36:45,568][0m Trial 27 finished with value: 0.127589913625871 and parameters: {'observation_period_num': 56, 'train_rates': 0.9571915768227236, 'learning_rate': 1.7381653360866916e-05, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9361031181737584}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:37:49,121][0m Trial 28 finished with value: 0.07222351664802777 and parameters: {'observation_period_num': 97, 'train_rates': 0.8701012953649873, 'learning_rate': 0.0005289811077747327, 'batch_size': 85, 'step_size': 5, 'gamma': 0.9568878917516926}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:39:39,655][0m Trial 29 finished with value: 0.03817149313787619 and parameters: {'observation_period_num': 5, 'train_rates': 0.9512789116189109, 'learning_rate': 7.06079830227553e-05, 'batch_size': 52, 'step_size': 8, 'gamma': 0.9681340957633681}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:40:34,132][0m Trial 30 finished with value: 0.0775310168432635 and parameters: {'observation_period_num': 41, 'train_rates': 0.9263053678573275, 'learning_rate': 0.00010782915238904741, 'batch_size': 107, 'step_size': 2, 'gamma': 0.8889767778130296}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:44:05,464][0m Trial 31 finished with value: 0.032962137282046736 and parameters: {'observation_period_num': 21, 'train_rates': 0.9666165797091346, 'learning_rate': 0.0009049386126574258, 'batch_size': 27, 'step_size': 1, 'gamma': 0.9731096848880827}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:47:02,546][0m Trial 32 finished with value: 0.02994251750409603 and parameters: {'observation_period_num': 19, 'train_rates': 0.9832130170747164, 'learning_rate': 0.0006981155806552446, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9753683668395677}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:49:53,453][0m Trial 33 finished with value: 0.04229953409670744 and parameters: {'observation_period_num': 29, 'train_rates': 0.941362730834282, 'learning_rate': 0.0004385243906385272, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9261857887230215}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:51:40,672][0m Trial 34 finished with value: 0.1272499352829023 and parameters: {'observation_period_num': 146, 'train_rates': 0.9021679185500455, 'learning_rate': 0.00023933953897718272, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9564273909679173}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:53:00,369][0m Trial 35 finished with value: 0.042819321155548096 and parameters: {'observation_period_num': 51, 'train_rates': 0.984845705295899, 'learning_rate': 0.00032970270693568775, 'batch_size': 74, 'step_size': 7, 'gamma': 0.9310771569168421}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:54:27,558][0m Trial 36 finished with value: 0.08260266970868078 and parameters: {'observation_period_num': 69, 'train_rates': 0.7418697269811558, 'learning_rate': 0.0006770446395301178, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9752848595167004}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:54:54,870][0m Trial 37 finished with value: 0.04805920985729798 and parameters: {'observation_period_num': 15, 'train_rates': 0.8122779629864291, 'learning_rate': 0.00020768865523893804, 'batch_size': 208, 'step_size': 10, 'gamma': 0.946954618643407}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:57:17,725][0m Trial 38 finished with value: 0.05852440904798448 and parameters: {'observation_period_num': 33, 'train_rates': 0.6751536864751737, 'learning_rate': 0.0006669520731169013, 'batch_size': 31, 'step_size': 5, 'gamma': 0.9150968172299194}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:58:04,203][0m Trial 39 finished with value: 0.05206920641163985 and parameters: {'observation_period_num': 5, 'train_rates': 0.9513440905773451, 'learning_rate': 0.00011577670440801207, 'batch_size': 132, 'step_size': 3, 'gamma': 0.9643134224721307}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 15:59:09,859][0m Trial 40 finished with value: 0.6960813602432608 and parameters: {'observation_period_num': 246, 'train_rates': 0.8818678104239914, 'learning_rate': 1.2108643722915539e-06, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8693121165678925}. Best is trial 21 with value: 0.02667983944781802.[0m
[32m[I 2025-01-12 16:04:51,230][0m Trial 41 finished with value: 0.025896226055920124 and parameters: {'observation_period_num': 19, 'train_rates': 0.9887191730783742, 'learning_rate': 0.0007674456913542746, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9772739861210255}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:08:07,878][0m Trial 42 finished with value: 0.04101924290790141 and parameters: {'observation_period_num': 21, 'train_rates': 0.96513076128195, 'learning_rate': 0.0004411647057066705, 'batch_size': 29, 'step_size': 1, 'gamma': 0.9783081255317153}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:13:25,195][0m Trial 43 finished with value: 0.04567811191900746 and parameters: {'observation_period_num': 32, 'train_rates': 0.9688904381362499, 'learning_rate': 0.0006440673152866351, 'batch_size': 18, 'step_size': 2, 'gamma': 0.989789056094515}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:14:05,092][0m Trial 44 finished with value: 0.11435359716415405 and parameters: {'observation_period_num': 170, 'train_rates': 0.9875188826174787, 'learning_rate': 0.00035488996034861833, 'batch_size': 149, 'step_size': 1, 'gamma': 0.9528079031616215}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:15:41,831][0m Trial 45 finished with value: 0.08306216217216945 and parameters: {'observation_period_num': 15, 'train_rates': 0.9155824223915554, 'learning_rate': 8.664608150928534e-06, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9368806270391254}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:16:13,546][0m Trial 46 finished with value: 0.05269340662460022 and parameters: {'observation_period_num': 48, 'train_rates': 0.9302698031868017, 'learning_rate': 0.00017399208554369966, 'batch_size': 184, 'step_size': 3, 'gamma': 0.9695000183416209}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:19:21,357][0m Trial 47 finished with value: 0.13042809026983548 and parameters: {'observation_period_num': 81, 'train_rates': 0.9630958836451299, 'learning_rate': 0.0007711695213328117, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8949511680703814}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:24:42,790][0m Trial 48 finished with value: 0.1377298137981543 and parameters: {'observation_period_num': 65, 'train_rates': 0.8556000137484138, 'learning_rate': 0.0004980812812805929, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9804910003285882}. Best is trial 41 with value: 0.025896226055920124.[0m
[32m[I 2025-01-12 16:26:34,932][0m Trial 49 finished with value: 0.13682684476995643 and parameters: {'observation_period_num': 209, 'train_rates': 0.900602275728582, 'learning_rate': 0.00025987283891578696, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9476221063747999}. Best is trial 41 with value: 0.025896226055920124.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-12 16:26:34,942][0m A new study created in memory with name: no-name-be6f4695-733d-4606-a745-79646e0594e7[0m
[32m[I 2025-01-12 16:29:08,345][0m Trial 0 finished with value: 0.1366101461866675 and parameters: {'observation_period_num': 190, 'train_rates': 0.9210007774437765, 'learning_rate': 5.2465188817027816e-05, 'batch_size': 34, 'step_size': 3, 'gamma': 0.8317313321894623}. Best is trial 0 with value: 0.1366101461866675.[0m
[32m[I 2025-01-12 16:29:36,002][0m Trial 1 finished with value: 0.4536208685971459 and parameters: {'observation_period_num': 225, 'train_rates': 0.7575962554014967, 'learning_rate': 1.8445308969582094e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8238684068507912}. Best is trial 0 with value: 0.1366101461866675.[0m
[32m[I 2025-01-12 16:30:11,369][0m Trial 2 finished with value: 0.14801293080873626 and parameters: {'observation_period_num': 9, 'train_rates': 0.6222916605965942, 'learning_rate': 5.4779660708662245e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9637421394101316}. Best is trial 0 with value: 0.1366101461866675.[0m
Early stopping at epoch 97
[32m[I 2025-01-12 16:30:43,192][0m Trial 3 finished with value: 0.9756678615668813 and parameters: {'observation_period_num': 237, 'train_rates': 0.7298140460929101, 'learning_rate': 1.8131041071363988e-06, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9127653023682917}. Best is trial 0 with value: 0.1366101461866675.[0m
[32m[I 2025-01-12 16:31:03,732][0m Trial 4 finished with value: 0.33628540076651015 and parameters: {'observation_period_num': 168, 'train_rates': 0.6015324835639699, 'learning_rate': 3.263872184603168e-05, 'batch_size': 220, 'step_size': 5, 'gamma': 0.854555611997854}. Best is trial 0 with value: 0.1366101461866675.[0m
[32m[I 2025-01-12 16:31:46,082][0m Trial 5 finished with value: 0.044269655928921306 and parameters: {'observation_period_num': 31, 'train_rates': 0.9382544938309763, 'learning_rate': 0.00017176880662207413, 'batch_size': 140, 'step_size': 9, 'gamma': 0.934413740745436}. Best is trial 5 with value: 0.044269655928921306.[0m
[32m[I 2025-01-12 16:33:39,929][0m Trial 6 finished with value: 0.21239534733342189 and parameters: {'observation_period_num': 161, 'train_rates': 0.7261180728278378, 'learning_rate': 1.8239822887343484e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8821557796955406}. Best is trial 5 with value: 0.044269655928921306.[0m
[32m[I 2025-01-12 16:34:11,987][0m Trial 7 finished with value: 1.2532988786697388 and parameters: {'observation_period_num': 101, 'train_rates': 0.9664993418967545, 'learning_rate': 3.353182509917417e-06, 'batch_size': 185, 'step_size': 2, 'gamma': 0.8828411826164652}. Best is trial 5 with value: 0.044269655928921306.[0m
[32m[I 2025-01-12 16:37:35,264][0m Trial 8 finished with value: 0.2874774255035048 and parameters: {'observation_period_num': 217, 'train_rates': 0.6143221857074963, 'learning_rate': 1.2326714493814387e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.7595641476728724}. Best is trial 5 with value: 0.044269655928921306.[0m
[32m[I 2025-01-12 16:40:03,405][0m Trial 9 finished with value: 0.25232092973761533 and parameters: {'observation_period_num': 170, 'train_rates': 0.7254619671466644, 'learning_rate': 0.00036000632594394307, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8377213597635285}. Best is trial 5 with value: 0.044269655928921306.[0m
[32m[I 2025-01-12 16:40:58,384][0m Trial 10 finished with value: 0.03595102532766759 and parameters: {'observation_period_num': 16, 'train_rates': 0.8639480865994077, 'learning_rate': 0.0007847586990390004, 'batch_size': 98, 'step_size': 15, 'gamma': 0.9767817243868199}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:42:01,493][0m Trial 11 finished with value: 0.05073712323496057 and parameters: {'observation_period_num': 11, 'train_rates': 0.8667007397025799, 'learning_rate': 0.0009604537871348381, 'batch_size': 85, 'step_size': 15, 'gamma': 0.9798542756065902}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:42:58,872][0m Trial 12 finished with value: 0.06587878528765394 and parameters: {'observation_period_num': 63, 'train_rates': 0.8576315172412271, 'learning_rate': 0.0001758874887267987, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9328669830883177}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:44:02,967][0m Trial 13 finished with value: 0.07885856222873333 and parameters: {'observation_period_num': 54, 'train_rates': 0.8545107048016622, 'learning_rate': 0.0009818964935573644, 'batch_size': 83, 'step_size': 15, 'gamma': 0.9379063704797019}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:44:29,044][0m Trial 14 finished with value: 0.07871303707361221 and parameters: {'observation_period_num': 50, 'train_rates': 0.9820853650220487, 'learning_rate': 0.00019135464735405233, 'batch_size': 255, 'step_size': 10, 'gamma': 0.9874917353223707}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:45:11,353][0m Trial 15 finished with value: 0.11509588809514587 and parameters: {'observation_period_num': 109, 'train_rates': 0.9072615798189891, 'learning_rate': 0.0003113019663409986, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9493229105140014}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:45:44,507][0m Trial 16 finished with value: 0.0445950607871254 and parameters: {'observation_period_num': 35, 'train_rates': 0.7999185503214787, 'learning_rate': 0.00010006511498967995, 'batch_size': 163, 'step_size': 9, 'gamma': 0.9111944149479206}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:47:10,496][0m Trial 17 finished with value: 0.10529202447000859 and parameters: {'observation_period_num': 96, 'train_rates': 0.9211163999513347, 'learning_rate': 0.0004742635910985686, 'batch_size': 64, 'step_size': 7, 'gamma': 0.9144931150023774}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:47:37,743][0m Trial 18 finished with value: 0.07539923726589519 and parameters: {'observation_period_num': 77, 'train_rates': 0.809234199951805, 'learning_rate': 0.00010467679011139531, 'batch_size': 208, 'step_size': 13, 'gamma': 0.9619077738002141}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:48:25,807][0m Trial 19 finished with value: 0.10574569715964084 and parameters: {'observation_period_num': 133, 'train_rates': 0.946635233026568, 'learning_rate': 0.0005537936706744046, 'batch_size': 116, 'step_size': 7, 'gamma': 0.8949690236984615}. Best is trial 10 with value: 0.03595102532766759.[0m
[32m[I 2025-01-12 16:49:53,315][0m Trial 20 finished with value: 0.028008469996058334 and parameters: {'observation_period_num': 7, 'train_rates': 0.879906221637099, 'learning_rate': 0.00014285612227425505, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7979980568363136}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:51:27,575][0m Trial 21 finished with value: 0.03765157996633343 and parameters: {'observation_period_num': 29, 'train_rates': 0.8902183460171038, 'learning_rate': 0.00017635170943795903, 'batch_size': 57, 'step_size': 13, 'gamma': 0.787335644244763}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:53:04,824][0m Trial 22 finished with value: 0.03717630170285702 and parameters: {'observation_period_num': 28, 'train_rates': 0.8852629831119915, 'learning_rate': 8.98093188218943e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.7776337462098206}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:54:25,543][0m Trial 23 finished with value: 0.03439244607172559 and parameters: {'observation_period_num': 7, 'train_rates': 0.832622889834244, 'learning_rate': 7.00302660908574e-05, 'batch_size': 65, 'step_size': 14, 'gamma': 0.786667189532412}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:55:15,070][0m Trial 24 finished with value: 0.1385776504931652 and parameters: {'observation_period_num': 9, 'train_rates': 0.8238831499605699, 'learning_rate': 5.207308431341243e-06, 'batch_size': 109, 'step_size': 14, 'gamma': 0.8065851206234681}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:56:22,322][0m Trial 25 finished with value: 0.08251613781258867 and parameters: {'observation_period_num': 69, 'train_rates': 0.7692442874878627, 'learning_rate': 3.527395955099521e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.7517905094852667}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:57:16,305][0m Trial 26 finished with value: 0.037708660934028054 and parameters: {'observation_period_num': 46, 'train_rates': 0.8335332200810751, 'learning_rate': 0.00028819358409641816, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8058369439540873}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:58:41,441][0m Trial 27 finished with value: 0.07441215096564775 and parameters: {'observation_period_num': 82, 'train_rates': 0.689978684955439, 'learning_rate': 5.961045211055374e-05, 'batch_size': 48, 'step_size': 14, 'gamma': 0.8571792181343056}. Best is trial 20 with value: 0.028008469996058334.[0m
[32m[I 2025-01-12 16:59:46,245][0m Trial 28 finished with value: 0.02463365374308712 and parameters: {'observation_period_num': 5, 'train_rates': 0.7843751869457062, 'learning_rate': 0.0006091292706173852, 'batch_size': 75, 'step_size': 11, 'gamma': 0.7752946997587957}. Best is trial 28 with value: 0.02463365374308712.[0m
[32m[I 2025-01-12 17:03:38,286][0m Trial 29 finished with value: 0.024418423867928488 and parameters: {'observation_period_num': 6, 'train_rates': 0.7785433258334205, 'learning_rate': 8.252252537573258e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.776859528181741}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:07:21,010][0m Trial 30 finished with value: 0.1268290900710993 and parameters: {'observation_period_num': 129, 'train_rates': 0.7693906663288878, 'learning_rate': 0.0005598836426006882, 'batch_size': 21, 'step_size': 11, 'gamma': 0.7688659261308053}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:09:31,351][0m Trial 31 finished with value: 0.0373882693853022 and parameters: {'observation_period_num': 23, 'train_rates': 0.7854961290511678, 'learning_rate': 7.344315473694776e-05, 'batch_size': 38, 'step_size': 12, 'gamma': 0.7938915471673144}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:10:30,285][0m Trial 32 finished with value: 0.05934675927149421 and parameters: {'observation_period_num': 45, 'train_rates': 0.6785500527988786, 'learning_rate': 0.0001252881132314884, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8155946533027252}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:12:24,741][0m Trial 33 finished with value: 0.04548175954203374 and parameters: {'observation_period_num': 7, 'train_rates': 0.8323941479823165, 'learning_rate': 2.191754429711006e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.7870759973118326}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:13:36,894][0m Trial 34 finished with value: 0.0570483219648921 and parameters: {'observation_period_num': 38, 'train_rates': 0.7381304738844268, 'learning_rate': 3.866105844248659e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.8353367586868828}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:18:22,160][0m Trial 35 finished with value: 0.02666842038533999 and parameters: {'observation_period_num': 6, 'train_rates': 0.7961398747134582, 'learning_rate': 5.396913261211416e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7760500597974823}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:21:08,107][0m Trial 36 finished with value: 0.08769712110420703 and parameters: {'observation_period_num': 26, 'train_rates': 0.6902397501103793, 'learning_rate': 1.1380976544088298e-05, 'batch_size': 27, 'step_size': 8, 'gamma': 0.7682781597202851}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:24:49,085][0m Trial 37 finished with value: 0.05762924585927438 and parameters: {'observation_period_num': 62, 'train_rates': 0.6630378993933942, 'learning_rate': 4.9559880344540495e-05, 'batch_size': 19, 'step_size': 10, 'gamma': 0.8017625689983496}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:26:30,570][0m Trial 38 finished with value: 0.046408166237287274 and parameters: {'observation_period_num': 25, 'train_rates': 0.7495783613927424, 'learning_rate': 0.00024536045680975736, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8202478479463531}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:28:42,812][0m Trial 39 finished with value: 0.11143990970104448 and parameters: {'observation_period_num': 199, 'train_rates': 0.7771600893747613, 'learning_rate': 0.00014463623656190604, 'batch_size': 34, 'step_size': 8, 'gamma': 0.7723780318473773}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:30:54,815][0m Trial 40 finished with value: 0.03865905848534211 and parameters: {'observation_period_num': 20, 'train_rates': 0.8042163295488235, 'learning_rate': 4.6131797736655375e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7593350278534681}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:31:29,717][0m Trial 41 finished with value: 0.0820090094316138 and parameters: {'observation_period_num': 5, 'train_rates': 0.8165123509461903, 'learning_rate': 2.389929943602663e-05, 'batch_size': 153, 'step_size': 12, 'gamma': 0.7833492576030785}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:36:18,324][0m Trial 42 finished with value: 0.044261442406448526 and parameters: {'observation_period_num': 37, 'train_rates': 0.7542562599611683, 'learning_rate': 7.211488471743975e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7550107254484485}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:37:48,268][0m Trial 43 finished with value: 0.0574023275392818 and parameters: {'observation_period_num': 16, 'train_rates': 0.849070374043896, 'learning_rate': 1.4467074088331484e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.7954941039247951}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:38:46,930][0m Trial 44 finished with value: 0.053751393968162066 and parameters: {'observation_period_num': 5, 'train_rates': 0.7877126633955133, 'learning_rate': 2.5225534668075662e-05, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8249778214586047}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:39:55,671][0m Trial 45 finished with value: 0.10868326540916197 and parameters: {'observation_period_num': 246, 'train_rates': 0.8855756888883309, 'learning_rate': 0.00022991268951590052, 'batch_size': 74, 'step_size': 9, 'gamma': 0.8424037966136363}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:42:35,136][0m Trial 46 finished with value: 0.26858368658460674 and parameters: {'observation_period_num': 150, 'train_rates': 0.7148322158463041, 'learning_rate': 7.915680157935512e-06, 'batch_size': 27, 'step_size': 12, 'gamma': 0.7769282778022982}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:43:25,275][0m Trial 47 finished with value: 0.7423680062662035 and parameters: {'observation_period_num': 18, 'train_rates': 0.7970478726064518, 'learning_rate': 1.178575354796659e-06, 'batch_size': 106, 'step_size': 4, 'gamma': 0.767356299881677}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:45:12,954][0m Trial 48 finished with value: 0.04536480526939482 and parameters: {'observation_period_num': 54, 'train_rates': 0.839213836218654, 'learning_rate': 6.489793508966709e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8103787689078976}. Best is trial 29 with value: 0.024418423867928488.[0m
[32m[I 2025-01-12 17:45:56,851][0m Trial 49 finished with value: 0.0481411161468947 and parameters: {'observation_period_num': 38, 'train_rates': 0.871952038811359, 'learning_rate': 0.00013597068953251636, 'batch_size': 130, 'step_size': 7, 'gamma': 0.7967322022812398}. Best is trial 29 with value: 0.024418423867928488.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-12 17:45:56,860][0m A new study created in memory with name: no-name-d01e170f-2f81-484b-9911-fb85496a0cb5[0m
[32m[I 2025-01-12 17:47:34,315][0m Trial 0 finished with value: 0.04742633727203299 and parameters: {'observation_period_num': 40, 'train_rates': 0.9511891238849179, 'learning_rate': 0.0005378600064695174, 'batch_size': 61, 'step_size': 6, 'gamma': 0.7747192420126893}. Best is trial 0 with value: 0.04742633727203299.[0m
[32m[I 2025-01-12 17:48:41,128][0m Trial 1 finished with value: 0.11198921455769567 and parameters: {'observation_period_num': 218, 'train_rates': 0.756521519967895, 'learning_rate': 0.0002294854633755391, 'batch_size': 67, 'step_size': 4, 'gamma': 0.82985143807703}. Best is trial 0 with value: 0.04742633727203299.[0m
[32m[I 2025-01-12 17:49:13,290][0m Trial 2 finished with value: 0.28797808357260446 and parameters: {'observation_period_num': 92, 'train_rates': 0.6656019706365431, 'learning_rate': 4.233852741012362e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.7646756791728427}. Best is trial 0 with value: 0.04742633727203299.[0m
[32m[I 2025-01-12 17:49:38,949][0m Trial 3 finished with value: 0.04045889560471882 and parameters: {'observation_period_num': 32, 'train_rates': 0.7478870319443376, 'learning_rate': 0.0003129216257841588, 'batch_size': 201, 'step_size': 15, 'gamma': 0.9553684587074472}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:51:46,808][0m Trial 4 finished with value: 0.2529584610131925 and parameters: {'observation_period_num': 162, 'train_rates': 0.7283824347168315, 'learning_rate': 0.00023949353836611272, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9471638347931167}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:52:30,942][0m Trial 5 finished with value: 0.17432297683893452 and parameters: {'observation_period_num': 178, 'train_rates': 0.7456077276140629, 'learning_rate': 0.0005358179567342097, 'batch_size': 106, 'step_size': 5, 'gamma': 0.9297948170714608}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:52:57,098][0m Trial 6 finished with value: 0.19767331059288284 and parameters: {'observation_period_num': 40, 'train_rates': 0.6813024704468074, 'learning_rate': 3.528672069783754e-05, 'batch_size': 193, 'step_size': 6, 'gamma': 0.7656800332188389}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:53:21,600][0m Trial 7 finished with value: 0.47839223734000397 and parameters: {'observation_period_num': 34, 'train_rates': 0.6148394780295036, 'learning_rate': 4.996569410215887e-06, 'batch_size': 183, 'step_size': 6, 'gamma': 0.9188710190479457}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:54:01,353][0m Trial 8 finished with value: 0.07806078830806466 and parameters: {'observation_period_num': 132, 'train_rates': 0.7817196836714851, 'learning_rate': 0.00010722460796831381, 'batch_size': 124, 'step_size': 14, 'gamma': 0.8520428229544018}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:55:04,050][0m Trial 9 finished with value: 0.07955651669456146 and parameters: {'observation_period_num': 134, 'train_rates': 0.8763564921904126, 'learning_rate': 3.8514372672502305e-05, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9250114401097972}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:55:26,938][0m Trial 10 finished with value: 0.5883166278109832 and parameters: {'observation_period_num': 5, 'train_rates': 0.884868203590164, 'learning_rate': 2.1717937567908466e-06, 'batch_size': 248, 'step_size': 1, 'gamma': 0.9893480167783291}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:55:54,631][0m Trial 11 finished with value: 0.09125705808401108 and parameters: {'observation_period_num': 79, 'train_rates': 0.9690852439781052, 'learning_rate': 0.0007767005811511665, 'batch_size': 231, 'step_size': 10, 'gamma': 0.8087234357886025}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 17:59:24,264][0m Trial 12 finished with value: 0.11794180066382151 and parameters: {'observation_period_num': 74, 'train_rates': 0.8534096155358424, 'learning_rate': 0.0009935044200218809, 'batch_size': 24, 'step_size': 9, 'gamma': 0.870160664811182}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:00:00,977][0m Trial 13 finished with value: 0.15166670083999634 and parameters: {'observation_period_num': 8, 'train_rates': 0.9729832284385815, 'learning_rate': 8.823280822630242e-06, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8799919881830522}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:00:27,600][0m Trial 14 finished with value: 0.17754157455971992 and parameters: {'observation_period_num': 50, 'train_rates': 0.8270978847997297, 'learning_rate': 0.0001744787315914008, 'batch_size': 203, 'step_size': 2, 'gamma': 0.8015380098957396}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:01:05,471][0m Trial 15 finished with value: 0.1355982229315857 and parameters: {'observation_period_num': 102, 'train_rates': 0.9225272345665164, 'learning_rate': 0.00041238413025098476, 'batch_size': 147, 'step_size': 8, 'gamma': 0.9879393802691656}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:02:24,879][0m Trial 16 finished with value: 0.04496643293116774 and parameters: {'observation_period_num': 54, 'train_rates': 0.8142407490133817, 'learning_rate': 8.234425531806836e-05, 'batch_size': 63, 'step_size': 15, 'gamma': 0.895518089648203}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:02:49,099][0m Trial 17 finished with value: 0.07707829444167186 and parameters: {'observation_period_num': 59, 'train_rates': 0.8096271387675209, 'learning_rate': 0.00011100411407812367, 'batch_size': 213, 'step_size': 15, 'gamma': 0.8970684535504385}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:03:31,988][0m Trial 18 finished with value: 0.21172735417938662 and parameters: {'observation_period_num': 107, 'train_rates': 0.6888096523327836, 'learning_rate': 1.4286621432654254e-05, 'batch_size': 108, 'step_size': 13, 'gamma': 0.9632046778710748}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:04:03,142][0m Trial 19 finished with value: 0.2020528540684034 and parameters: {'observation_period_num': 229, 'train_rates': 0.7956240316630037, 'learning_rate': 7.3797538350347e-05, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8996263961647467}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:05:22,149][0m Trial 20 finished with value: 0.06916173755981846 and parameters: {'observation_period_num': 23, 'train_rates': 0.7110457370441332, 'learning_rate': 1.7226248609374404e-05, 'batch_size': 57, 'step_size': 12, 'gamma': 0.9592616531692365}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:07:06,904][0m Trial 21 finished with value: 0.0666342526179763 and parameters: {'observation_period_num': 64, 'train_rates': 0.9217918772630317, 'learning_rate': 0.0003907131188731192, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8282198219212232}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:07:57,449][0m Trial 22 finished with value: 0.10857837247325762 and parameters: {'observation_period_num': 25, 'train_rates': 0.6418848802304359, 'learning_rate': 8.659580778140502e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.849853480677057}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:10:16,719][0m Trial 23 finished with value: 0.06475438087275534 and parameters: {'observation_period_num': 53, 'train_rates': 0.9317635557234811, 'learning_rate': 0.00029708752361121344, 'batch_size': 39, 'step_size': 13, 'gamma': 0.7898382875123409}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:15:01,735][0m Trial 24 finished with value: 0.05501749417764912 and parameters: {'observation_period_num': 26, 'train_rates': 0.8437241310757359, 'learning_rate': 0.00016344534779437437, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9023395203010562}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:16:05,321][0m Trial 25 finished with value: 0.07533163537505928 and parameters: {'observation_period_num': 86, 'train_rates': 0.7716085447840199, 'learning_rate': 0.000649277549779104, 'batch_size': 78, 'step_size': 7, 'gamma': 0.752127546111413}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:16:53,568][0m Trial 26 finished with value: 0.6617985415595309 and parameters: {'observation_period_num': 116, 'train_rates': 0.8771075281949773, 'learning_rate': 1.001057121090909e-06, 'batch_size': 109, 'step_size': 15, 'gamma': 0.9452898644469274}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:18:55,045][0m Trial 27 finished with value: 0.05617988482117653 and parameters: {'observation_period_num': 44, 'train_rates': 0.9869507402575711, 'learning_rate': 6.55389670501175e-05, 'batch_size': 46, 'step_size': 3, 'gamma': 0.8858118974464521}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:19:32,195][0m Trial 28 finished with value: 0.11620206782696453 and parameters: {'observation_period_num': 158, 'train_rates': 0.7260272761951437, 'learning_rate': 0.00044276026394419283, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8518686491074593}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:20:41,752][0m Trial 29 finished with value: 0.05569025076322817 and parameters: {'observation_period_num': 70, 'train_rates': 0.7599849489110561, 'learning_rate': 0.00016148217158174782, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8289812817788111}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:21:04,501][0m Trial 30 finished with value: 0.0426056832075119 and parameters: {'observation_period_num': 15, 'train_rates': 0.9457496072593888, 'learning_rate': 0.0002261505811179325, 'batch_size': 254, 'step_size': 4, 'gamma': 0.9664846352499229}. Best is trial 3 with value: 0.04045889560471882.[0m
[32m[I 2025-01-12 18:21:32,005][0m Trial 31 finished with value: 0.03967702388763428 and parameters: {'observation_period_num': 15, 'train_rates': 0.932309931058126, 'learning_rate': 0.000244617738629381, 'batch_size': 227, 'step_size': 4, 'gamma': 0.9723028314109218}. Best is trial 31 with value: 0.03967702388763428.[0m
[32m[I 2025-01-12 18:21:57,058][0m Trial 32 finished with value: 0.0394959177116421 and parameters: {'observation_period_num': 16, 'train_rates': 0.8977119492191576, 'learning_rate': 0.0002530250229958232, 'batch_size': 253, 'step_size': 4, 'gamma': 0.9691542380969185}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:22:20,854][0m Trial 33 finished with value: 0.04234671965241432 and parameters: {'observation_period_num': 14, 'train_rates': 0.9451204726145672, 'learning_rate': 0.0002833490327745188, 'batch_size': 254, 'step_size': 4, 'gamma': 0.972146936237658}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:22:46,998][0m Trial 34 finished with value: 0.05345827051334911 and parameters: {'observation_period_num': 33, 'train_rates': 0.898364234520716, 'learning_rate': 0.000331128186153014, 'batch_size': 231, 'step_size': 3, 'gamma': 0.9758675372035057}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:23:13,357][0m Trial 35 finished with value: 0.048578181479881695 and parameters: {'observation_period_num': 6, 'train_rates': 0.9032259624169604, 'learning_rate': 5.4102122354768005e-05, 'batch_size': 233, 'step_size': 5, 'gamma': 0.9452590302606947}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:23:41,144][0m Trial 36 finished with value: 0.09177548438310623 and parameters: {'observation_period_num': 19, 'train_rates': 0.9498299495901822, 'learning_rate': 0.00023915919290679402, 'batch_size': 216, 'step_size': 1, 'gamma': 0.9337468704769692}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:24:03,738][0m Trial 37 finished with value: 0.1403784255683422 and parameters: {'observation_period_num': 201, 'train_rates': 0.8545988163097009, 'learning_rate': 0.0001490496611998272, 'batch_size': 242, 'step_size': 5, 'gamma': 0.9737142652837637}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:24:31,772][0m Trial 38 finished with value: 0.12100046128034592 and parameters: {'observation_period_num': 252, 'train_rates': 0.9655836534241444, 'learning_rate': 0.0006564620786656589, 'batch_size': 215, 'step_size': 3, 'gamma': 0.9146546133985588}. Best is trial 32 with value: 0.0394959177116421.[0m
[32m[I 2025-01-12 18:25:02,118][0m Trial 39 finished with value: 0.039086858665709816 and parameters: {'observation_period_num': 40, 'train_rates': 0.9035165116758931, 'learning_rate': 0.0005016112485969052, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9552819052557818}. Best is trial 39 with value: 0.039086858665709816.[0m
[32m[I 2025-01-12 18:25:29,883][0m Trial 40 finished with value: 0.044432316741099945 and parameters: {'observation_period_num': 36, 'train_rates': 0.7398965213874502, 'learning_rate': 0.00097714458249608, 'batch_size': 186, 'step_size': 7, 'gamma': 0.9534807893381609}. Best is trial 39 with value: 0.039086858665709816.[0m
[32m[I 2025-01-12 18:25:56,387][0m Trial 41 finished with value: 0.07202664619772196 and parameters: {'observation_period_num': 38, 'train_rates': 0.9072357466876405, 'learning_rate': 0.0002855086312792407, 'batch_size': 238, 'step_size': 2, 'gamma': 0.9781447501655519}. Best is trial 39 with value: 0.039086858665709816.[0m
[32m[I 2025-01-12 18:26:26,767][0m Trial 42 finished with value: 0.02970966988952335 and parameters: {'observation_period_num': 16, 'train_rates': 0.925821857624237, 'learning_rate': 0.0005757522063584108, 'batch_size': 200, 'step_size': 6, 'gamma': 0.9318663327202252}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:27:00,917][0m Trial 43 finished with value: 0.05411679787063821 and parameters: {'observation_period_num': 29, 'train_rates': 0.8899158110288495, 'learning_rate': 0.0005492862833039818, 'batch_size': 173, 'step_size': 7, 'gamma': 0.9365238237945185}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:27:29,561][0m Trial 44 finished with value: 0.0424308262071021 and parameters: {'observation_period_num': 45, 'train_rates': 0.8635575924043507, 'learning_rate': 0.00047402559400249643, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9513931234021324}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:27:57,968][0m Trial 45 finished with value: 0.04345487809814183 and parameters: {'observation_period_num': 5, 'train_rates': 0.9235684634431028, 'learning_rate': 0.00012226125372449457, 'batch_size': 224, 'step_size': 5, 'gamma': 0.9176557067715173}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:28:26,941][0m Trial 46 finished with value: 0.05411976549129949 and parameters: {'observation_period_num': 17, 'train_rates': 0.8386083216909015, 'learning_rate': 0.0007601202191629165, 'batch_size': 205, 'step_size': 5, 'gamma': 0.9857758336944641}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:29:00,534][0m Trial 47 finished with value: 0.06480708718299866 and parameters: {'observation_period_num': 63, 'train_rates': 0.9869236742023284, 'learning_rate': 0.0005556092432895931, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9387041765785821}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:29:36,547][0m Trial 48 finished with value: 0.05687888383642951 and parameters: {'observation_period_num': 85, 'train_rates': 0.9067195379176594, 'learning_rate': 0.00021387382618911333, 'batch_size': 154, 'step_size': 9, 'gamma': 0.9254661929466611}. Best is trial 42 with value: 0.02970966988952335.[0m
[32m[I 2025-01-12 18:30:06,716][0m Trial 49 finished with value: 0.1096423476609612 and parameters: {'observation_period_num': 44, 'train_rates': 0.8738500465856855, 'learning_rate': 0.00039082771516251325, 'batch_size': 195, 'step_size': 9, 'gamma': 0.9584837343399715}. Best is trial 42 with value: 0.02970966988952335.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-12 18:30:06,726][0m A new study created in memory with name: no-name-da8f2e95-03f9-4f32-ad0b-4cbf48ff22d7[0m
Early stopping at epoch 47
[32m[I 2025-01-12 18:30:36,619][0m Trial 0 finished with value: 0.6581238967897389 and parameters: {'observation_period_num': 234, 'train_rates': 0.8137358694778425, 'learning_rate': 1.1438704301476694e-05, 'batch_size': 79, 'step_size': 1, 'gamma': 0.7831641960252907}. Best is trial 0 with value: 0.6581238967897389.[0m
[32m[I 2025-01-12 18:31:17,510][0m Trial 1 finished with value: 0.3297813190745817 and parameters: {'observation_period_num': 106, 'train_rates': 0.8633321530127747, 'learning_rate': 2.0132217341483992e-05, 'batch_size': 130, 'step_size': 3, 'gamma': 0.8233069620265666}. Best is trial 1 with value: 0.3297813190745817.[0m
[32m[I 2025-01-12 18:32:41,988][0m Trial 2 finished with value: 0.11496706554163276 and parameters: {'observation_period_num': 110, 'train_rates': 0.7669132692720461, 'learning_rate': 0.0007326483117707607, 'batch_size': 56, 'step_size': 15, 'gamma': 0.7952736656370384}. Best is trial 2 with value: 0.11496706554163276.[0m
[32m[I 2025-01-12 18:33:54,419][0m Trial 3 finished with value: 0.3856760958830516 and parameters: {'observation_period_num': 220, 'train_rates': 0.9650865531184789, 'learning_rate': 1.0114570960154302e-05, 'batch_size': 76, 'step_size': 7, 'gamma': 0.7698167074812036}. Best is trial 2 with value: 0.11496706554163276.[0m
[32m[I 2025-01-12 18:34:33,429][0m Trial 4 finished with value: 0.20227947560223666 and parameters: {'observation_period_num': 98, 'train_rates': 0.8768574966318661, 'learning_rate': 2.686726167060545e-05, 'batch_size': 144, 'step_size': 5, 'gamma': 0.8356546016252359}. Best is trial 2 with value: 0.11496706554163276.[0m
[32m[I 2025-01-12 18:36:41,106][0m Trial 5 finished with value: 0.13891924121800592 and parameters: {'observation_period_num': 54, 'train_rates': 0.8534743169889053, 'learning_rate': 0.0002759937865586525, 'batch_size': 41, 'step_size': 12, 'gamma': 0.9306399031638702}. Best is trial 2 with value: 0.11496706554163276.[0m
[32m[I 2025-01-12 18:37:44,085][0m Trial 6 finished with value: 0.05620518922805786 and parameters: {'observation_period_num': 95, 'train_rates': 0.7673478527856337, 'learning_rate': 0.00017705303743493944, 'batch_size': 76, 'step_size': 7, 'gamma': 0.8311398655023157}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:38:24,536][0m Trial 7 finished with value: 0.31578010525673056 and parameters: {'observation_period_num': 48, 'train_rates': 0.7300102454730423, 'learning_rate': 5.105939636895685e-06, 'batch_size': 124, 'step_size': 8, 'gamma': 0.9109148055831395}. Best is trial 6 with value: 0.05620518922805786.[0m
Early stopping at epoch 55
[32m[I 2025-01-12 18:39:06,263][0m Trial 8 finished with value: 0.4320768759374725 and parameters: {'observation_period_num': 41, 'train_rates': 0.9079135965883498, 'learning_rate': 1.3546189031238283e-05, 'batch_size': 77, 'step_size': 1, 'gamma': 0.8066876753933142}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:39:29,763][0m Trial 9 finished with value: 0.317812887272184 and parameters: {'observation_period_num': 27, 'train_rates': 0.7395181448536601, 'learning_rate': 4.489634949218761e-06, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9737399214916778}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:39:47,847][0m Trial 10 finished with value: 0.2748253834518519 and parameters: {'observation_period_num': 161, 'train_rates': 0.606122720351092, 'learning_rate': 0.0001586379101362667, 'batch_size': 245, 'step_size': 11, 'gamma': 0.8719369701634119}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:42:58,913][0m Trial 11 finished with value: 0.11863702522459055 and parameters: {'observation_period_num': 155, 'train_rates': 0.7170037605656053, 'learning_rate': 0.0009042306016076542, 'batch_size': 23, 'step_size': 15, 'gamma': 0.8648122155782778}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:44:15,254][0m Trial 12 finished with value: 0.08751827313672358 and parameters: {'observation_period_num': 97, 'train_rates': 0.6517147748771954, 'learning_rate': 0.00010135739364913882, 'batch_size': 56, 'step_size': 11, 'gamma': 0.768129275628687}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:44:55,409][0m Trial 13 finished with value: 0.09686068154530493 and parameters: {'observation_period_num': 79, 'train_rates': 0.6426598877961849, 'learning_rate': 8.336754373180684e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.7562136529695144}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:45:23,571][0m Trial 14 finished with value: 1.0398443750757624 and parameters: {'observation_period_num': 145, 'train_rates': 0.6678115109732355, 'learning_rate': 1.2049929105691797e-06, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8532705245725646}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:49:16,648][0m Trial 15 finished with value: 0.13386345403535024 and parameters: {'observation_period_num': 179, 'train_rates': 0.6847759858769693, 'learning_rate': 9.238739540282988e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.7522583018702974}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:50:06,558][0m Trial 16 finished with value: 0.06875711005992022 and parameters: {'observation_period_num': 72, 'train_rates': 0.7960976095493736, 'learning_rate': 5.570363872123985e-05, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8136122298027686}. Best is trial 6 with value: 0.05620518922805786.[0m
[32m[I 2025-01-12 18:50:37,900][0m Trial 17 finished with value: 0.029049222184654555 and parameters: {'observation_period_num': 16, 'train_rates': 0.7911801261520416, 'learning_rate': 0.00037909250762655666, 'batch_size': 174, 'step_size': 7, 'gamma': 0.8854178230298061}. Best is trial 17 with value: 0.029049222184654555.[0m
[32m[I 2025-01-12 18:51:08,935][0m Trial 18 finished with value: 0.030325498083992947 and parameters: {'observation_period_num': 8, 'train_rates': 0.8199991835324331, 'learning_rate': 0.0003715389645779819, 'batch_size': 181, 'step_size': 6, 'gamma': 0.9001086202999485}. Best is trial 17 with value: 0.029049222184654555.[0m
[32m[I 2025-01-12 18:51:41,176][0m Trial 19 finished with value: 0.03775855898857117 and parameters: {'observation_period_num': 5, 'train_rates': 0.9362176306670574, 'learning_rate': 0.00045048638316580767, 'batch_size': 189, 'step_size': 5, 'gamma': 0.8991211406597127}. Best is trial 17 with value: 0.029049222184654555.[0m
[32m[I 2025-01-12 18:52:10,205][0m Trial 20 finished with value: 0.030804898955726197 and parameters: {'observation_period_num': 18, 'train_rates': 0.8285117423979387, 'learning_rate': 0.0003629963504518538, 'batch_size': 195, 'step_size': 3, 'gamma': 0.9525300088910024}. Best is trial 17 with value: 0.029049222184654555.[0m
[32m[I 2025-01-12 18:52:39,065][0m Trial 21 finished with value: 0.030960903151004804 and parameters: {'observation_period_num': 10, 'train_rates': 0.827585436728475, 'learning_rate': 0.0003783873978019511, 'batch_size': 189, 'step_size': 3, 'gamma': 0.9575144504149327}. Best is trial 17 with value: 0.029049222184654555.[0m
[32m[I 2025-01-12 18:53:11,985][0m Trial 22 finished with value: 0.02830846823721391 and parameters: {'observation_period_num': 21, 'train_rates': 0.833618909743636, 'learning_rate': 0.00050282984988223, 'batch_size': 165, 'step_size': 3, 'gamma': 0.9406681386752137}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:53:48,309][0m Trial 23 finished with value: 0.031844190976305586 and parameters: {'observation_period_num': 35, 'train_rates': 0.8987064504446239, 'learning_rate': 0.000619249083151253, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8948235530972377}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:54:13,494][0m Trial 24 finished with value: 0.0646636990744334 and parameters: {'observation_period_num': 67, 'train_rates': 0.7750322717057426, 'learning_rate': 0.00021266933504228866, 'batch_size': 222, 'step_size': 4, 'gamma': 0.9221630488362167}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:54:46,808][0m Trial 25 finished with value: 0.03558135533989486 and parameters: {'observation_period_num': 5, 'train_rates': 0.8000984998583652, 'learning_rate': 0.000975865130087777, 'batch_size': 168, 'step_size': 8, 'gamma': 0.9373075113417478}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:55:13,714][0m Trial 26 finished with value: 0.06247696977522638 and parameters: {'observation_period_num': 30, 'train_rates': 0.846215117563186, 'learning_rate': 4.6864552269522754e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.8850326927736397}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:55:38,138][0m Trial 27 finished with value: 0.10465113897072642 and parameters: {'observation_period_num': 126, 'train_rates': 0.8993014690402298, 'learning_rate': 0.000541682679709947, 'batch_size': 250, 'step_size': 2, 'gamma': 0.9748100612589705}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:56:07,763][0m Trial 28 finished with value: 0.06746328019283035 and parameters: {'observation_period_num': 60, 'train_rates': 0.7529547092476285, 'learning_rate': 0.00013272994556257612, 'batch_size': 169, 'step_size': 4, 'gamma': 0.9138566100773208}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:56:43,307][0m Trial 29 finished with value: 0.14490604800422016 and parameters: {'observation_period_num': 235, 'train_rates': 0.8191853347147471, 'learning_rate': 0.00027358394728843957, 'batch_size': 144, 'step_size': 1, 'gamma': 0.9481679139695396}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:57:08,932][0m Trial 30 finished with value: 0.09138270537974423 and parameters: {'observation_period_num': 22, 'train_rates': 0.7115234102389174, 'learning_rate': 5.371371954321211e-05, 'batch_size': 202, 'step_size': 7, 'gamma': 0.8515735938362468}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:57:37,714][0m Trial 31 finished with value: 0.03140698051423702 and parameters: {'observation_period_num': 16, 'train_rates': 0.8246909384725297, 'learning_rate': 0.0003628069280570087, 'batch_size': 192, 'step_size': 2, 'gamma': 0.9526639295562759}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:58:00,863][0m Trial 32 finished with value: 0.04785894770776072 and parameters: {'observation_period_num': 42, 'train_rates': 0.7871880057249055, 'learning_rate': 0.00027725388672223707, 'batch_size': 230, 'step_size': 3, 'gamma': 0.987652869230669}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:58:34,010][0m Trial 33 finished with value: 0.028348699515202826 and parameters: {'observation_period_num': 21, 'train_rates': 0.8407150604862067, 'learning_rate': 0.0005516324787417077, 'batch_size': 169, 'step_size': 4, 'gamma': 0.8859209346505276}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:59:04,327][0m Trial 34 finished with value: 0.09499222741284397 and parameters: {'observation_period_num': 209, 'train_rates': 0.8702220988800715, 'learning_rate': 0.0006271169640753431, 'batch_size': 179, 'step_size': 4, 'gamma': 0.8815869155524771}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 18:59:41,567][0m Trial 35 finished with value: 0.04667388892813817 and parameters: {'observation_period_num': 78, 'train_rates': 0.8444768459835257, 'learning_rate': 0.000628053554800687, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9005395048079915}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:00:23,971][0m Trial 36 finished with value: 0.03904787372265543 and parameters: {'observation_period_num': 55, 'train_rates': 0.8793973074640247, 'learning_rate': 0.00022241361925713846, 'batch_size': 134, 'step_size': 5, 'gamma': 0.9291810144173407}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:00:55,347][0m Trial 37 finished with value: 0.14710876244756194 and parameters: {'observation_period_num': 251, 'train_rates': 0.8086912999343283, 'learning_rate': 0.000996918513849081, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8528377237131144}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:01:29,667][0m Trial 38 finished with value: 0.034065855110620526 and parameters: {'observation_period_num': 28, 'train_rates': 0.9213144483222317, 'learning_rate': 0.0004357207721880715, 'batch_size': 177, 'step_size': 8, 'gamma': 0.9107382911052744}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:02:17,746][0m Trial 39 finished with value: 0.08103757047653198 and parameters: {'observation_period_num': 41, 'train_rates': 0.9572731403934609, 'learning_rate': 0.00013338870315220008, 'batch_size': 122, 'step_size': 2, 'gamma': 0.8842800788615474}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:03:14,851][0m Trial 40 finished with value: 0.19562508578312368 and parameters: {'observation_period_num': 87, 'train_rates': 0.8603721082261677, 'learning_rate': 2.525253303015555e-05, 'batch_size': 93, 'step_size': 4, 'gamma': 0.8381247774338418}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:03:43,541][0m Trial 41 finished with value: 0.031228665658272804 and parameters: {'observation_period_num': 18, 'train_rates': 0.8366255682170962, 'learning_rate': 0.00034390026991812513, 'batch_size': 194, 'step_size': 3, 'gamma': 0.9409274148894909}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:04:10,615][0m Trial 42 finished with value: 0.0441682830278296 and parameters: {'observation_period_num': 20, 'train_rates': 0.7817828355127149, 'learning_rate': 0.00020730820716799585, 'batch_size': 205, 'step_size': 6, 'gamma': 0.9212224158624956}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:04:42,227][0m Trial 43 finished with value: 0.05041971097771938 and parameters: {'observation_period_num': 53, 'train_rates': 0.7536133889202254, 'learning_rate': 0.00046987288538065484, 'batch_size': 157, 'step_size': 5, 'gamma': 0.9626193236975771}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:05:13,666][0m Trial 44 finished with value: 0.05224215775946159 and parameters: {'observation_period_num': 35, 'train_rates': 0.8127834991261662, 'learning_rate': 0.00029331727860626184, 'batch_size': 178, 'step_size': 2, 'gamma': 0.8756936497383824}. Best is trial 22 with value: 0.02830846823721391.[0m
Early stopping at epoch 92
[32m[I 2025-01-12 19:05:49,200][0m Trial 45 finished with value: 0.11132260624851499 and parameters: {'observation_period_num': 110, 'train_rates': 0.88184836259267, 'learning_rate': 0.0008226040225560832, 'batch_size': 144, 'step_size': 1, 'gamma': 0.8623270323223936}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:06:13,606][0m Trial 46 finished with value: 0.037057452828892126 and parameters: {'observation_period_num': 14, 'train_rates': 0.848076180142616, 'learning_rate': 0.0001604836954543532, 'batch_size': 234, 'step_size': 9, 'gamma': 0.902971389501939}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:06:42,994][0m Trial 47 finished with value: 0.042647603610254645 and parameters: {'observation_period_num': 46, 'train_rates': 0.7563018540891913, 'learning_rate': 0.0007096406266813561, 'batch_size': 183, 'step_size': 4, 'gamma': 0.9393995233943673}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:07:10,172][0m Trial 48 finished with value: 0.19165825402858305 and parameters: {'observation_period_num': 24, 'train_rates': 0.7994488266349898, 'learning_rate': 1.4636221603968247e-05, 'batch_size': 201, 'step_size': 3, 'gamma': 0.9274402672273897}. Best is trial 22 with value: 0.02830846823721391.[0m
[32m[I 2025-01-12 19:07:53,294][0m Trial 49 finished with value: 0.36622069412084113 and parameters: {'observation_period_num': 62, 'train_rates': 0.8342806689033884, 'learning_rate': 3.908851702880481e-06, 'batch_size': 124, 'step_size': 7, 'gamma': 0.892870323042774}. Best is trial 22 with value: 0.02830846823721391.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 19:07:53,304][0m A new study created in memory with name: no-name-41e8b3b6-c8da-450a-b18b-b4a72e885f00[0m
[32m[I 2025-01-12 19:08:51,124][0m Trial 0 finished with value: 0.1475228037727013 and parameters: {'observation_period_num': 223, 'train_rates': 0.9348329215843977, 'learning_rate': 9.450144950175057e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.9314133986457483}. Best is trial 0 with value: 0.1475228037727013.[0m
[32m[I 2025-01-12 19:09:22,878][0m Trial 1 finished with value: 0.2507212822478512 and parameters: {'observation_period_num': 223, 'train_rates': 0.7459711418797804, 'learning_rate': 4.4568421872993195e-05, 'batch_size': 152, 'step_size': 4, 'gamma': 0.8846848153141642}. Best is trial 0 with value: 0.1475228037727013.[0m
[32m[I 2025-01-12 19:09:40,769][0m Trial 2 finished with value: 0.4197003751993179 and parameters: {'observation_period_num': 239, 'train_rates': 0.6149713739967223, 'learning_rate': 9.569672525437672e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.9318521036443347}. Best is trial 0 with value: 0.1475228037727013.[0m
[32m[I 2025-01-12 19:10:03,799][0m Trial 3 finished with value: 0.054050024124709044 and parameters: {'observation_period_num': 46, 'train_rates': 0.8299030404513699, 'learning_rate': 0.0004729271860792435, 'batch_size': 244, 'step_size': 10, 'gamma': 0.8574968959760376}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:10:42,482][0m Trial 4 finished with value: 0.10574063793695015 and parameters: {'observation_period_num': 47, 'train_rates': 0.7112369449896961, 'learning_rate': 4.418831311834113e-05, 'batch_size': 128, 'step_size': 8, 'gamma': 0.8251264518567804}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:11:04,143][0m Trial 5 finished with value: 0.8639214515686036 and parameters: {'observation_period_num': 220, 'train_rates': 0.6982404772067266, 'learning_rate': 2.198436171575546e-06, 'batch_size': 220, 'step_size': 14, 'gamma': 0.9883679030342957}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:11:52,401][0m Trial 6 finished with value: 0.36949775031968657 and parameters: {'observation_period_num': 79, 'train_rates': 0.7873050941455506, 'learning_rate': 3.4847938610826058e-06, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8974095560909578}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:13:14,450][0m Trial 7 finished with value: 0.16588746856998754 and parameters: {'observation_period_num': 232, 'train_rates': 0.6602121789357276, 'learning_rate': 0.0001845958203386198, 'batch_size': 50, 'step_size': 8, 'gamma': 0.85466390539292}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:13:47,812][0m Trial 8 finished with value: 0.5061432806582287 and parameters: {'observation_period_num': 120, 'train_rates': 0.7954728120909493, 'learning_rate': 5.957203939420009e-06, 'batch_size': 153, 'step_size': 4, 'gamma': 0.9117100373884803}. Best is trial 3 with value: 0.054050024124709044.[0m
Early stopping at epoch 81
[32m[I 2025-01-12 19:14:09,428][0m Trial 9 finished with value: 0.704965353012085 and parameters: {'observation_period_num': 210, 'train_rates': 0.9380689958386121, 'learning_rate': 1.4131719622281194e-05, 'batch_size': 220, 'step_size': 2, 'gamma': 0.7752202308526717}. Best is trial 3 with value: 0.054050024124709044.[0m
[32m[I 2025-01-12 19:14:40,407][0m Trial 10 finished with value: 0.029539618755177577 and parameters: {'observation_period_num': 10, 'train_rates': 0.8669304781845765, 'learning_rate': 0.0007552251947529725, 'batch_size': 195, 'step_size': 11, 'gamma': 0.7636327215629216}. Best is trial 10 with value: 0.029539618755177577.[0m
[32m[I 2025-01-12 19:15:09,347][0m Trial 11 finished with value: 0.030794306541507017 and parameters: {'observation_period_num': 7, 'train_rates': 0.8620180366055111, 'learning_rate': 0.000977892600874967, 'batch_size': 196, 'step_size': 11, 'gamma': 0.7528877459030977}. Best is trial 10 with value: 0.029539618755177577.[0m
[32m[I 2025-01-12 19:15:40,041][0m Trial 12 finished with value: 0.02699876918947113 and parameters: {'observation_period_num': 5, 'train_rates': 0.868677037207574, 'learning_rate': 0.0009772123325891267, 'batch_size': 189, 'step_size': 12, 'gamma': 0.750377265977899}. Best is trial 12 with value: 0.02699876918947113.[0m
[32m[I 2025-01-12 19:16:12,073][0m Trial 13 finished with value: 0.025429720884527673 and parameters: {'observation_period_num': 9, 'train_rates': 0.8805480719989272, 'learning_rate': 0.0009820552546436058, 'batch_size': 182, 'step_size': 15, 'gamma': 0.7974631091789224}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:16:45,497][0m Trial 14 finished with value: 0.09343437850475311 and parameters: {'observation_period_num': 161, 'train_rates': 0.9811508132823527, 'learning_rate': 0.0002800892065914751, 'batch_size': 175, 'step_size': 15, 'gamma': 0.8045949093577067}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:21:10,097][0m Trial 15 finished with value: 0.12132350552532681 and parameters: {'observation_period_num': 89, 'train_rates': 0.8985424461905359, 'learning_rate': 0.0003233270369111788, 'batch_size': 20, 'step_size': 13, 'gamma': 0.7997042553226021}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:21:42,511][0m Trial 16 finished with value: 0.07042788306925458 and parameters: {'observation_period_num': 44, 'train_rates': 0.9043986864981076, 'learning_rate': 0.00015126500906829392, 'batch_size': 187, 'step_size': 15, 'gamma': 0.8189453699791619}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:22:30,205][0m Trial 17 finished with value: 0.33812254667282104 and parameters: {'observation_period_num': 159, 'train_rates': 0.986923654149273, 'learning_rate': 1.3758114838740515e-05, 'batch_size': 124, 'step_size': 12, 'gamma': 0.7810718069917882}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:23:03,330][0m Trial 18 finished with value: 0.033731931728571383 and parameters: {'observation_period_num': 31, 'train_rates': 0.812805949993378, 'learning_rate': 0.000537793445270384, 'batch_size': 162, 'step_size': 15, 'gamma': 0.7507717323067954}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:23:29,680][0m Trial 19 finished with value: 0.7281982907166717 and parameters: {'observation_period_num': 75, 'train_rates': 0.8530085517384437, 'learning_rate': 1.2740150892050398e-06, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8396163300827155}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:24:30,801][0m Trial 20 finished with value: 0.24362233769049688 and parameters: {'observation_period_num': 125, 'train_rates': 0.7603111802420497, 'learning_rate': 1.923443196689113e-05, 'batch_size': 78, 'step_size': 6, 'gamma': 0.7930683581718851}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:24:59,260][0m Trial 21 finished with value: 0.02897313228476117 and parameters: {'observation_period_num': 6, 'train_rates': 0.8840441223792284, 'learning_rate': 0.000990448729258098, 'batch_size': 202, 'step_size': 10, 'gamma': 0.7626090545697644}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:25:27,597][0m Trial 22 finished with value: 0.029625360161945637 and parameters: {'observation_period_num': 18, 'train_rates': 0.9018685474976498, 'learning_rate': 0.0009454428626671457, 'batch_size': 209, 'step_size': 10, 'gamma': 0.7821079019352694}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:26:03,938][0m Trial 23 finished with value: 0.04645607306569966 and parameters: {'observation_period_num': 61, 'train_rates': 0.931913917901091, 'learning_rate': 0.00037949553452819534, 'batch_size': 168, 'step_size': 12, 'gamma': 0.7711429435648594}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:26:28,954][0m Trial 24 finished with value: 0.035395649862129334 and parameters: {'observation_period_num': 27, 'train_rates': 0.8345816264751452, 'learning_rate': 0.0005864162056767535, 'batch_size': 233, 'step_size': 9, 'gamma': 0.8186594858649922}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:26:59,329][0m Trial 25 finished with value: 0.07497304035084588 and parameters: {'observation_period_num': 106, 'train_rates': 0.8773228289490347, 'learning_rate': 0.00019700144899515305, 'batch_size': 183, 'step_size': 14, 'gamma': 0.7507323602549831}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:27:41,435][0m Trial 26 finished with value: 0.039707280695438385 and parameters: {'observation_period_num': 6, 'train_rates': 0.9633909958987064, 'learning_rate': 0.0002958747926684094, 'batch_size': 146, 'step_size': 12, 'gamma': 0.7956815843225814}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:28:11,458][0m Trial 27 finished with value: 0.056604102895061764 and parameters: {'observation_period_num': 30, 'train_rates': 0.8849727925475731, 'learning_rate': 9.1500895522714e-05, 'batch_size': 194, 'step_size': 14, 'gamma': 0.7645609948748477}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:28:35,839][0m Trial 28 finished with value: 0.04723225038358219 and parameters: {'observation_period_num': 63, 'train_rates': 0.8440343689856195, 'learning_rate': 0.0009891783514725334, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8091140963038856}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:29:25,537][0m Trial 29 finished with value: 0.117692575018893 and parameters: {'observation_period_num': 191, 'train_rates': 0.932787719862862, 'learning_rate': 9.783723733149366e-05, 'batch_size': 110, 'step_size': 6, 'gamma': 0.786990984683994}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:29:59,558][0m Trial 30 finished with value: 0.06902680207382549 and parameters: {'observation_period_num': 97, 'train_rates': 0.9192891460868606, 'learning_rate': 0.0005500737387024258, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8388387058638788}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:30:28,083][0m Trial 31 finished with value: 0.03239437609199603 and parameters: {'observation_period_num': 17, 'train_rates': 0.8647220908193993, 'learning_rate': 0.0006069393900435776, 'batch_size': 197, 'step_size': 11, 'gamma': 0.7647926526732391}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:30:54,682][0m Trial 32 finished with value: 0.03212409102661931 and parameters: {'observation_period_num': 6, 'train_rates': 0.8034707764230435, 'learning_rate': 0.0007343267804340749, 'batch_size': 209, 'step_size': 9, 'gamma': 0.7654841909830243}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:31:36,583][0m Trial 33 finished with value: 0.045344963669776917 and parameters: {'observation_period_num': 38, 'train_rates': 0.9531181611320705, 'learning_rate': 0.00024750884529669995, 'batch_size': 141, 'step_size': 12, 'gamma': 0.7641299788153161}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:32:05,057][0m Trial 34 finished with value: 0.052558687269345095 and parameters: {'observation_period_num': 57, 'train_rates': 0.8800753424126924, 'learning_rate': 0.0007315581430662322, 'batch_size': 205, 'step_size': 13, 'gamma': 0.9530593594235967}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:32:26,332][0m Trial 35 finished with value: 0.03690429679261693 and parameters: {'observation_period_num': 18, 'train_rates': 0.7679203543769564, 'learning_rate': 0.0004346385352812633, 'batch_size': 251, 'step_size': 11, 'gamma': 0.7824669964231783}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:32:59,136][0m Trial 36 finished with value: 0.06528811082529382 and parameters: {'observation_period_num': 50, 'train_rates': 0.8202632177900435, 'learning_rate': 6.722329737941397e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8694960358282054}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:33:24,194][0m Trial 37 finished with value: 0.038682421621705675 and parameters: {'observation_period_num': 21, 'train_rates': 0.8400432926131168, 'learning_rate': 0.00043591998808121147, 'batch_size': 230, 'step_size': 10, 'gamma': 0.7570386022112351}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:33:56,901][0m Trial 38 finished with value: 0.042766057655176354 and parameters: {'observation_period_num': 36, 'train_rates': 0.8686712061119793, 'learning_rate': 0.00012851643199033726, 'batch_size': 178, 'step_size': 14, 'gamma': 0.8367577693596754}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:34:22,056][0m Trial 39 finished with value: 0.053074996631879076 and parameters: {'observation_period_num': 74, 'train_rates': 0.9006625709266989, 'learning_rate': 0.0007857933395058654, 'batch_size': 220, 'step_size': 8, 'gamma': 0.7740074077872066}. Best is trial 13 with value: 0.025429720884527673.[0m
Early stopping at epoch 70
[32m[I 2025-01-12 19:34:46,159][0m Trial 40 finished with value: 0.2088848065998819 and parameters: {'observation_period_num': 6, 'train_rates': 0.6031826606584026, 'learning_rate': 3.8763589109463083e-05, 'batch_size': 131, 'step_size': 1, 'gamma': 0.8106001384406959}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:35:14,197][0m Trial 41 finished with value: 0.03252668136444229 and parameters: {'observation_period_num': 20, 'train_rates': 0.9115707316959231, 'learning_rate': 0.0009980162303625767, 'batch_size': 209, 'step_size': 10, 'gamma': 0.7865219987026905}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:35:44,549][0m Trial 42 finished with value: 0.029259861307957817 and parameters: {'observation_period_num': 17, 'train_rates': 0.8927626187299886, 'learning_rate': 0.0007232330465889395, 'batch_size': 191, 'step_size': 10, 'gamma': 0.7805049187554488}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:36:12,340][0m Trial 43 finished with value: 0.05670945874143176 and parameters: {'observation_period_num': 43, 'train_rates': 0.7212121181703337, 'learning_rate': 0.00039378079263749946, 'batch_size': 189, 'step_size': 9, 'gamma': 0.7740076102959264}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:36:45,545][0m Trial 44 finished with value: 0.03407598557528138 and parameters: {'observation_period_num': 31, 'train_rates': 0.7848101296567657, 'learning_rate': 0.0006672562383424462, 'batch_size': 159, 'step_size': 11, 'gamma': 0.7574004401923748}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:37:11,875][0m Trial 45 finished with value: 0.1524710190267517 and parameters: {'observation_period_num': 250, 'train_rates': 0.8847429529341878, 'learning_rate': 0.0002434597270829059, 'batch_size': 201, 'step_size': 8, 'gamma': 0.7962915612705205}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:37:45,792][0m Trial 46 finished with value: 0.078263059258461 and parameters: {'observation_period_num': 15, 'train_rates': 0.9548747943684417, 'learning_rate': 0.0004697801232252951, 'batch_size': 179, 'step_size': 13, 'gamma': 0.9815251546948762}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:38:18,651][0m Trial 47 finished with value: 0.0714716019063461 and parameters: {'observation_period_num': 145, 'train_rates': 0.8540042628779264, 'learning_rate': 0.0007348973969080657, 'batch_size': 169, 'step_size': 10, 'gamma': 0.825632265658997}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:38:43,850][0m Trial 48 finished with value: 0.05893599063993884 and parameters: {'observation_period_num': 49, 'train_rates': 0.822130367523895, 'learning_rate': 0.0002055072425349121, 'batch_size': 221, 'step_size': 7, 'gamma': 0.7756763304625302}. Best is trial 13 with value: 0.025429720884527673.[0m
[32m[I 2025-01-12 19:39:09,831][0m Trial 49 finished with value: 0.18532568216323853 and parameters: {'observation_period_num': 5, 'train_rates': 0.9217070383370615, 'learning_rate': 4.6571384873217914e-06, 'batch_size': 244, 'step_size': 12, 'gamma': 0.8869048650998703}. Best is trial 13 with value: 0.025429720884527673.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 122, 'train_rates': 0.8967007258760927, 'learning_rate': 0.00010685566781066802, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7647103928311025}
Epoch 1/300, trend Loss: 0.3558 | 0.3685
Epoch 2/300, trend Loss: 0.2643 | 0.2998
Epoch 3/300, trend Loss: 0.2268 | 0.2547
Epoch 4/300, trend Loss: 0.2000 | 0.2140
Epoch 5/300, trend Loss: 0.1788 | 0.1828
Epoch 6/300, trend Loss: 0.1622 | 0.1624
Epoch 7/300, trend Loss: 0.1498 | 0.1610
Epoch 8/300, trend Loss: 0.1417 | 0.1508
Epoch 9/300, trend Loss: 0.1342 | 0.1420
Epoch 10/300, trend Loss: 0.1278 | 0.1341
Epoch 11/300, trend Loss: 0.1224 | 0.1273
Epoch 12/300, trend Loss: 0.1181 | 0.1407
Epoch 13/300, trend Loss: 0.1150 | 0.1376
Epoch 14/300, trend Loss: 0.1118 | 0.1339
Epoch 15/300, trend Loss: 0.1089 | 0.1302
Epoch 16/300, trend Loss: 0.1063 | 0.1266
Epoch 17/300, trend Loss: 0.1040 | 0.1231
Epoch 18/300, trend Loss: 0.1023 | 0.1232
Epoch 19/300, trend Loss: 0.1008 | 0.1151
Epoch 20/300, trend Loss: 0.0989 | 0.1105
Epoch 21/300, trend Loss: 0.0973 | 0.1070
Epoch 22/300, trend Loss: 0.0959 | 0.1039
Epoch 23/300, trend Loss: 0.0948 | 0.0877
Epoch 24/300, trend Loss: 0.0940 | 0.0853
Epoch 25/300, trend Loss: 0.0930 | 0.0838
Epoch 26/300, trend Loss: 0.0921 | 0.0826
Epoch 27/300, trend Loss: 0.0913 | 0.0815
Epoch 28/300, trend Loss: 0.0906 | 0.0806
Epoch 29/300, trend Loss: 0.0899 | 0.0733
Epoch 30/300, trend Loss: 0.0895 | 0.0727
Epoch 31/300, trend Loss: 0.0889 | 0.0722
Epoch 32/300, trend Loss: 0.0883 | 0.0718
Epoch 33/300, trend Loss: 0.0877 | 0.0715
Epoch 34/300, trend Loss: 0.0873 | 0.0690
Epoch 35/300, trend Loss: 0.0869 | 0.0685
Epoch 36/300, trend Loss: 0.0864 | 0.0681
Epoch 37/300, trend Loss: 0.0860 | 0.0678
Epoch 38/300, trend Loss: 0.0855 | 0.0674
Epoch 39/300, trend Loss: 0.0851 | 0.0671
Epoch 40/300, trend Loss: 0.0847 | 0.0670
Epoch 41/300, trend Loss: 0.0844 | 0.0666
Epoch 42/300, trend Loss: 0.0841 | 0.0662
Epoch 43/300, trend Loss: 0.0838 | 0.0658
Epoch 44/300, trend Loss: 0.0835 | 0.0655
Epoch 45/300, trend Loss: 0.0832 | 0.0672
Epoch 46/300, trend Loss: 0.0829 | 0.0668
Epoch 47/300, trend Loss: 0.0827 | 0.0663
Epoch 48/300, trend Loss: 0.0825 | 0.0659
Epoch 49/300, trend Loss: 0.0823 | 0.0655
Epoch 50/300, trend Loss: 0.0821 | 0.0651
Epoch 51/300, trend Loss: 0.0819 | 0.0673
Epoch 52/300, trend Loss: 0.0817 | 0.0668
Epoch 53/300, trend Loss: 0.0815 | 0.0664
Epoch 54/300, trend Loss: 0.0814 | 0.0660
Epoch 55/300, trend Loss: 0.0812 | 0.0656
Epoch 56/300, trend Loss: 0.0810 | 0.0665
Epoch 57/300, trend Loss: 0.0809 | 0.0661
Epoch 58/300, trend Loss: 0.0808 | 0.0658
Epoch 59/300, trend Loss: 0.0807 | 0.0655
Epoch 60/300, trend Loss: 0.0805 | 0.0652
Epoch 61/300, trend Loss: 0.0804 | 0.0649
Epoch 62/300, trend Loss: 0.0803 | 0.0648
Epoch 63/300, trend Loss: 0.0802 | 0.0646
Epoch 64/300, trend Loss: 0.0801 | 0.0644
Epoch 65/300, trend Loss: 0.0800 | 0.0642
Epoch 66/300, trend Loss: 0.0799 | 0.0640
Epoch 67/300, trend Loss: 0.0798 | 0.0639
Epoch 68/300, trend Loss: 0.0798 | 0.0638
Epoch 69/300, trend Loss: 0.0797 | 0.0636
Epoch 70/300, trend Loss: 0.0796 | 0.0635
Epoch 71/300, trend Loss: 0.0796 | 0.0633
Epoch 72/300, trend Loss: 0.0795 | 0.0632
Epoch 73/300, trend Loss: 0.0794 | 0.0641
Epoch 74/300, trend Loss: 0.0794 | 0.0640
Epoch 75/300, trend Loss: 0.0794 | 0.0640
Epoch 76/300, trend Loss: 0.0793 | 0.0640
Epoch 77/300, trend Loss: 0.0793 | 0.0640
Epoch 78/300, trend Loss: 0.0793 | 0.0660
Epoch 79/300, trend Loss: 0.0792 | 0.0660
Epoch 80/300, trend Loss: 0.0792 | 0.0660
Epoch 81/300, trend Loss: 0.0792 | 0.0660
Epoch 82/300, trend Loss: 0.0791 | 0.0660
Epoch 83/300, trend Loss: 0.0791 | 0.0659
Epoch 84/300, trend Loss: 0.0790 | 0.0669
Epoch 85/300, trend Loss: 0.0790 | 0.0666
Epoch 86/300, trend Loss: 0.0790 | 0.0664
Epoch 87/300, trend Loss: 0.0789 | 0.0663
Epoch 88/300, trend Loss: 0.0789 | 0.0662
Epoch 89/300, trend Loss: 0.0788 | 0.0660
Epoch 90/300, trend Loss: 0.0788 | 0.0659
Epoch 91/300, trend Loss: 0.0788 | 0.0658
Epoch 92/300, trend Loss: 0.0788 | 0.0657
Epoch 93/300, trend Loss: 0.0787 | 0.0656
Epoch 94/300, trend Loss: 0.0787 | 0.0656
Epoch 95/300, trend Loss: 0.0787 | 0.0653
Epoch 96/300, trend Loss: 0.0786 | 0.0652
Epoch 97/300, trend Loss: 0.0786 | 0.0652
Epoch 98/300, trend Loss: 0.0786 | 0.0652
Epoch 99/300, trend Loss: 0.0786 | 0.0651
Epoch 100/300, trend Loss: 0.0785 | 0.0648
Epoch 101/300, trend Loss: 0.0785 | 0.0648
Epoch 102/300, trend Loss: 0.0785 | 0.0648
Epoch 103/300, trend Loss: 0.0785 | 0.0648
Epoch 104/300, trend Loss: 0.0785 | 0.0648
Epoch 105/300, trend Loss: 0.0785 | 0.0648
Epoch 106/300, trend Loss: 0.0784 | 0.0645
Epoch 107/300, trend Loss: 0.0784 | 0.0645
Epoch 108/300, trend Loss: 0.0784 | 0.0645
Epoch 109/300, trend Loss: 0.0784 | 0.0645
Epoch 110/300, trend Loss: 0.0784 | 0.0645
Epoch 111/300, trend Loss: 0.0784 | 0.0643
Epoch 112/300, trend Loss: 0.0784 | 0.0643
Epoch 113/300, trend Loss: 0.0783 | 0.0643
Epoch 114/300, trend Loss: 0.0783 | 0.0643
Epoch 115/300, trend Loss: 0.0783 | 0.0643
Epoch 116/300, trend Loss: 0.0783 | 0.0643
Epoch 117/300, trend Loss: 0.0783 | 0.0642
Epoch 118/300, trend Loss: 0.0783 | 0.0642
Epoch 119/300, trend Loss: 0.0783 | 0.0642
Epoch 120/300, trend Loss: 0.0783 | 0.0642
Epoch 121/300, trend Loss: 0.0783 | 0.0642
Epoch 122/300, trend Loss: 0.0783 | 0.0641
Epoch 123/300, trend Loss: 0.0783 | 0.0641
Epoch 124/300, trend Loss: 0.0783 | 0.0641
Epoch 125/300, trend Loss: 0.0783 | 0.0641
Epoch 126/300, trend Loss: 0.0782 | 0.0641
Epoch 127/300, trend Loss: 0.0782 | 0.0641
Epoch 128/300, trend Loss: 0.0782 | 0.0640
Epoch 129/300, trend Loss: 0.0782 | 0.0640
Epoch 130/300, trend Loss: 0.0782 | 0.0640
Epoch 131/300, trend Loss: 0.0782 | 0.0640
Epoch 132/300, trend Loss: 0.0782 | 0.0640
Epoch 133/300, trend Loss: 0.0782 | 0.0640
Epoch 134/300, trend Loss: 0.0782 | 0.0639
Epoch 135/300, trend Loss: 0.0782 | 0.0639
Epoch 136/300, trend Loss: 0.0782 | 0.0639
Epoch 137/300, trend Loss: 0.0782 | 0.0639
Epoch 138/300, trend Loss: 0.0782 | 0.0639
Epoch 139/300, trend Loss: 0.0782 | 0.0639
Epoch 140/300, trend Loss: 0.0782 | 0.0639
Epoch 141/300, trend Loss: 0.0782 | 0.0639
Epoch 142/300, trend Loss: 0.0782 | 0.0639
Epoch 143/300, trend Loss: 0.0782 | 0.0639
Epoch 144/300, trend Loss: 0.0782 | 0.0639
Epoch 145/300, trend Loss: 0.0782 | 0.0639
Epoch 146/300, trend Loss: 0.0782 | 0.0639
Epoch 147/300, trend Loss: 0.0782 | 0.0639
Epoch 148/300, trend Loss: 0.0782 | 0.0639
Epoch 149/300, trend Loss: 0.0782 | 0.0639
Epoch 150/300, trend Loss: 0.0782 | 0.0638
Epoch 151/300, trend Loss: 0.0782 | 0.0638
Epoch 152/300, trend Loss: 0.0782 | 0.0638
Epoch 153/300, trend Loss: 0.0782 | 0.0638
Epoch 154/300, trend Loss: 0.0782 | 0.0638
Epoch 155/300, trend Loss: 0.0782 | 0.0638
Epoch 156/300, trend Loss: 0.0782 | 0.0638
Epoch 157/300, trend Loss: 0.0782 | 0.0638
Epoch 158/300, trend Loss: 0.0782 | 0.0638
Epoch 159/300, trend Loss: 0.0782 | 0.0638
Epoch 160/300, trend Loss: 0.0782 | 0.0638
Epoch 161/300, trend Loss: 0.0781 | 0.0638
Epoch 162/300, trend Loss: 0.0781 | 0.0638
Epoch 163/300, trend Loss: 0.0781 | 0.0638
Epoch 164/300, trend Loss: 0.0781 | 0.0638
Epoch 165/300, trend Loss: 0.0781 | 0.0638
Epoch 166/300, trend Loss: 0.0781 | 0.0638
Epoch 167/300, trend Loss: 0.0781 | 0.0638
Epoch 168/300, trend Loss: 0.0781 | 0.0638
Epoch 169/300, trend Loss: 0.0781 | 0.0638
Epoch 170/300, trend Loss: 0.0781 | 0.0638
Epoch 171/300, trend Loss: 0.0781 | 0.0638
Epoch 172/300, trend Loss: 0.0781 | 0.0638
Epoch 173/300, trend Loss: 0.0781 | 0.0638
Epoch 174/300, trend Loss: 0.0781 | 0.0638
Epoch 175/300, trend Loss: 0.0781 | 0.0638
Epoch 176/300, trend Loss: 0.0781 | 0.0638
Epoch 177/300, trend Loss: 0.0781 | 0.0638
Epoch 178/300, trend Loss: 0.0781 | 0.0638
Epoch 179/300, trend Loss: 0.0781 | 0.0638
Epoch 180/300, trend Loss: 0.0781 | 0.0638
Epoch 181/300, trend Loss: 0.0781 | 0.0638
Epoch 182/300, trend Loss: 0.0781 | 0.0638
Epoch 183/300, trend Loss: 0.0781 | 0.0638
Epoch 184/300, trend Loss: 0.0781 | 0.0638
Epoch 185/300, trend Loss: 0.0781 | 0.0638
Epoch 186/300, trend Loss: 0.0781 | 0.0638
Epoch 187/300, trend Loss: 0.0781 | 0.0638
Epoch 188/300, trend Loss: 0.0781 | 0.0638
Epoch 189/300, trend Loss: 0.0781 | 0.0638
Epoch 190/300, trend Loss: 0.0781 | 0.0638
Epoch 191/300, trend Loss: 0.0781 | 0.0638
Epoch 192/300, trend Loss: 0.0781 | 0.0638
Epoch 193/300, trend Loss: 0.0781 | 0.0638
Epoch 194/300, trend Loss: 0.0781 | 0.0638
Epoch 195/300, trend Loss: 0.0781 | 0.0638
Epoch 196/300, trend Loss: 0.0781 | 0.0638
Epoch 197/300, trend Loss: 0.0781 | 0.0638
Epoch 198/300, trend Loss: 0.0781 | 0.0638
Epoch 199/300, trend Loss: 0.0781 | 0.0638
Epoch 200/300, trend Loss: 0.0781 | 0.0638
Epoch 201/300, trend Loss: 0.0781 | 0.0638
Epoch 202/300, trend Loss: 0.0781 | 0.0638
Epoch 203/300, trend Loss: 0.0781 | 0.0638
Epoch 204/300, trend Loss: 0.0781 | 0.0638
Epoch 205/300, trend Loss: 0.0781 | 0.0638
Epoch 206/300, trend Loss: 0.0781 | 0.0638
Epoch 207/300, trend Loss: 0.0781 | 0.0638
Epoch 208/300, trend Loss: 0.0781 | 0.0638
Epoch 209/300, trend Loss: 0.0781 | 0.0637
Epoch 210/300, trend Loss: 0.0781 | 0.0637
Epoch 211/300, trend Loss: 0.0781 | 0.0637
Epoch 212/300, trend Loss: 0.0781 | 0.0637
Epoch 213/300, trend Loss: 0.0781 | 0.0637
Epoch 214/300, trend Loss: 0.0781 | 0.0637
Epoch 215/300, trend Loss: 0.0781 | 0.0637
Epoch 216/300, trend Loss: 0.0781 | 0.0637
Epoch 217/300, trend Loss: 0.0781 | 0.0637
Epoch 218/300, trend Loss: 0.0781 | 0.0637
Epoch 219/300, trend Loss: 0.0781 | 0.0637
Epoch 220/300, trend Loss: 0.0781 | 0.0637
Epoch 221/300, trend Loss: 0.0781 | 0.0637
Epoch 222/300, trend Loss: 0.0781 | 0.0637
Epoch 223/300, trend Loss: 0.0781 | 0.0637
Epoch 224/300, trend Loss: 0.0781 | 0.0637
Epoch 225/300, trend Loss: 0.0781 | 0.0637
Epoch 226/300, trend Loss: 0.0781 | 0.0637
Epoch 227/300, trend Loss: 0.0781 | 0.0637
Epoch 228/300, trend Loss: 0.0781 | 0.0637
Epoch 229/300, trend Loss: 0.0781 | 0.0637
Epoch 230/300, trend Loss: 0.0781 | 0.0637
Epoch 231/300, trend Loss: 0.0781 | 0.0637
Epoch 232/300, trend Loss: 0.0781 | 0.0637
Epoch 233/300, trend Loss: 0.0781 | 0.0637
Epoch 234/300, trend Loss: 0.0781 | 0.0637
Epoch 235/300, trend Loss: 0.0781 | 0.0637
Epoch 236/300, trend Loss: 0.0781 | 0.0637
Epoch 237/300, trend Loss: 0.0781 | 0.0637
Epoch 238/300, trend Loss: 0.0781 | 0.0637
Epoch 239/300, trend Loss: 0.0781 | 0.0637
Epoch 240/300, trend Loss: 0.0781 | 0.0637
Epoch 241/300, trend Loss: 0.0781 | 0.0637
Epoch 242/300, trend Loss: 0.0781 | 0.0637
Epoch 243/300, trend Loss: 0.0781 | 0.0637
Epoch 244/300, trend Loss: 0.0781 | 0.0637
Epoch 245/300, trend Loss: 0.0781 | 0.0637
Epoch 246/300, trend Loss: 0.0781 | 0.0637
Epoch 247/300, trend Loss: 0.0781 | 0.0637
Epoch 248/300, trend Loss: 0.0781 | 0.0637
Epoch 249/300, trend Loss: 0.0781 | 0.0637
Epoch 250/300, trend Loss: 0.0781 | 0.0637
Epoch 251/300, trend Loss: 0.0781 | 0.0637
Epoch 252/300, trend Loss: 0.0781 | 0.0637
Epoch 253/300, trend Loss: 0.0781 | 0.0637
Epoch 254/300, trend Loss: 0.0781 | 0.0637
Epoch 255/300, trend Loss: 0.0781 | 0.0637
Epoch 256/300, trend Loss: 0.0781 | 0.0637
Epoch 257/300, trend Loss: 0.0781 | 0.0637
Epoch 258/300, trend Loss: 0.0781 | 0.0637
Epoch 259/300, trend Loss: 0.0781 | 0.0637
Epoch 260/300, trend Loss: 0.0781 | 0.0637
Epoch 261/300, trend Loss: 0.0781 | 0.0637
Epoch 262/300, trend Loss: 0.0781 | 0.0637
Epoch 263/300, trend Loss: 0.0781 | 0.0637
Epoch 264/300, trend Loss: 0.0781 | 0.0637
Epoch 265/300, trend Loss: 0.0781 | 0.0637
Epoch 266/300, trend Loss: 0.0781 | 0.0637
Epoch 267/300, trend Loss: 0.0781 | 0.0637
Epoch 268/300, trend Loss: 0.0781 | 0.0637
Epoch 269/300, trend Loss: 0.0781 | 0.0637
Epoch 270/300, trend Loss: 0.0781 | 0.0637
Epoch 271/300, trend Loss: 0.0781 | 0.0637
Epoch 272/300, trend Loss: 0.0781 | 0.0637
Epoch 273/300, trend Loss: 0.0781 | 0.0637
Epoch 274/300, trend Loss: 0.0781 | 0.0637
Epoch 275/300, trend Loss: 0.0781 | 0.0637
Epoch 276/300, trend Loss: 0.0781 | 0.0637
Epoch 277/300, trend Loss: 0.0781 | 0.0637
Epoch 278/300, trend Loss: 0.0781 | 0.0637
Epoch 279/300, trend Loss: 0.0781 | 0.0637
Epoch 280/300, trend Loss: 0.0781 | 0.0637
Epoch 281/300, trend Loss: 0.0781 | 0.0637
Epoch 282/300, trend Loss: 0.0781 | 0.0637
Epoch 283/300, trend Loss: 0.0781 | 0.0637
Epoch 284/300, trend Loss: 0.0781 | 0.0637
Epoch 285/300, trend Loss: 0.0781 | 0.0637
Epoch 286/300, trend Loss: 0.0781 | 0.0637
Epoch 287/300, trend Loss: 0.0781 | 0.0637
Epoch 288/300, trend Loss: 0.0781 | 0.0637
Epoch 289/300, trend Loss: 0.0781 | 0.0637
Epoch 290/300, trend Loss: 0.0781 | 0.0637
Epoch 291/300, trend Loss: 0.0781 | 0.0637
Epoch 292/300, trend Loss: 0.0781 | 0.0637
Epoch 293/300, trend Loss: 0.0781 | 0.0637
Epoch 294/300, trend Loss: 0.0781 | 0.0637
Epoch 295/300, trend Loss: 0.0781 | 0.0637
Epoch 296/300, trend Loss: 0.0781 | 0.0637
Epoch 297/300, trend Loss: 0.0781 | 0.0637
Epoch 298/300, trend Loss: 0.0781 | 0.0637
Epoch 299/300, trend Loss: 0.0781 | 0.0637
Epoch 300/300, trend Loss: 0.0781 | 0.0637
Training seasonal_0 component with params: {'observation_period_num': 19, 'train_rates': 0.9887191730783742, 'learning_rate': 0.0007674456913542746, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9772739861210255}
Epoch 1/300, seasonal_0 Loss: 0.5112 | 0.1218
Epoch 2/300, seasonal_0 Loss: 0.2258 | 0.1498
Epoch 3/300, seasonal_0 Loss: 0.1808 | 0.1251
Epoch 4/300, seasonal_0 Loss: 0.1541 | 0.0880
Epoch 5/300, seasonal_0 Loss: 0.1346 | 0.0900
Epoch 6/300, seasonal_0 Loss: 0.1386 | 0.0892
Epoch 7/300, seasonal_0 Loss: 0.1295 | 0.0790
Epoch 8/300, seasonal_0 Loss: 0.1188 | 0.0975
Epoch 9/300, seasonal_0 Loss: 0.1161 | 0.0776
Epoch 10/300, seasonal_0 Loss: 0.1079 | 0.0678
Epoch 11/300, seasonal_0 Loss: 0.1052 | 0.0671
Epoch 12/300, seasonal_0 Loss: 0.1088 | 0.0821
Epoch 13/300, seasonal_0 Loss: 0.1075 | 0.0597
Epoch 14/300, seasonal_0 Loss: 0.1090 | 0.1001
Epoch 15/300, seasonal_0 Loss: 0.0951 | 0.0599
Epoch 16/300, seasonal_0 Loss: 0.0963 | 0.0739
Epoch 17/300, seasonal_0 Loss: 0.0883 | 0.0579
Epoch 18/300, seasonal_0 Loss: 0.0887 | 0.0443
Epoch 19/300, seasonal_0 Loss: 0.0835 | 0.0365
Epoch 20/300, seasonal_0 Loss: 0.0858 | 0.0645
Epoch 21/300, seasonal_0 Loss: 0.0805 | 0.0504
Epoch 22/300, seasonal_0 Loss: 0.0772 | 0.0405
Epoch 23/300, seasonal_0 Loss: 0.0716 | 0.0425
Epoch 24/300, seasonal_0 Loss: 0.0713 | 0.0354
Epoch 25/300, seasonal_0 Loss: 0.0699 | 0.0380
Epoch 26/300, seasonal_0 Loss: 0.0686 | 0.0482
Epoch 27/300, seasonal_0 Loss: 0.0650 | 0.0334
Epoch 28/300, seasonal_0 Loss: 0.0625 | 0.0303
Epoch 29/300, seasonal_0 Loss: 0.0611 | 0.0295
Epoch 30/300, seasonal_0 Loss: 0.0631 | 0.0249
Epoch 31/300, seasonal_0 Loss: 0.0588 | 0.0239
Epoch 32/300, seasonal_0 Loss: 0.0573 | 0.0240
Epoch 33/300, seasonal_0 Loss: 0.0558 | 0.0223
Epoch 34/300, seasonal_0 Loss: 0.0544 | 0.0233
Epoch 35/300, seasonal_0 Loss: 0.0520 | 0.0336
Epoch 36/300, seasonal_0 Loss: 0.0499 | 0.0214
Epoch 37/300, seasonal_0 Loss: 0.0495 | 0.0249
Epoch 38/300, seasonal_0 Loss: 0.0491 | 0.0193
Epoch 39/300, seasonal_0 Loss: 0.0478 | 0.0226
Epoch 40/300, seasonal_0 Loss: 0.0482 | 0.0227
Epoch 41/300, seasonal_0 Loss: 0.0472 | 0.0251
Epoch 42/300, seasonal_0 Loss: 0.0459 | 0.0227
Epoch 43/300, seasonal_0 Loss: 0.0443 | 0.0223
Epoch 44/300, seasonal_0 Loss: 0.0425 | 0.0196
Epoch 45/300, seasonal_0 Loss: 0.0424 | 0.0205
Epoch 46/300, seasonal_0 Loss: 0.0438 | 0.0183
Epoch 47/300, seasonal_0 Loss: 0.0424 | 0.0191
Epoch 48/300, seasonal_0 Loss: 0.0417 | 0.0207
Epoch 49/300, seasonal_0 Loss: 0.0401 | 0.0197
Epoch 50/300, seasonal_0 Loss: 0.0413 | 0.0187
Epoch 51/300, seasonal_0 Loss: 0.0427 | 0.0204
Epoch 52/300, seasonal_0 Loss: 0.0414 | 0.0198
Epoch 53/300, seasonal_0 Loss: 0.0395 | 0.0200
Epoch 54/300, seasonal_0 Loss: 0.0427 | 0.0176
Epoch 55/300, seasonal_0 Loss: 0.0403 | 0.0198
Epoch 56/300, seasonal_0 Loss: 0.0373 | 0.0223
Epoch 57/300, seasonal_0 Loss: 0.0383 | 0.0193
Epoch 58/300, seasonal_0 Loss: 0.0361 | 0.0207
Epoch 59/300, seasonal_0 Loss: 0.0386 | 0.0230
Epoch 60/300, seasonal_0 Loss: 0.0379 | 0.0305
Epoch 61/300, seasonal_0 Loss: 0.0368 | 0.0337
Epoch 62/300, seasonal_0 Loss: 0.0357 | 0.0236
Epoch 63/300, seasonal_0 Loss: 0.0349 | 0.0195
Epoch 64/300, seasonal_0 Loss: 0.0333 | 0.0196
Epoch 65/300, seasonal_0 Loss: 0.0362 | 0.0205
Epoch 66/300, seasonal_0 Loss: 0.0334 | 0.0215
Epoch 67/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 68/300, seasonal_0 Loss: 0.0317 | 0.0213
Epoch 69/300, seasonal_0 Loss: 0.0310 | 0.0203
Epoch 70/300, seasonal_0 Loss: 0.0316 | 0.0214
Epoch 71/300, seasonal_0 Loss: 0.0310 | 0.0219
Epoch 72/300, seasonal_0 Loss: 0.0315 | 0.0218
Epoch 73/300, seasonal_0 Loss: 0.0299 | 0.0218
Epoch 74/300, seasonal_0 Loss: 0.0313 | 0.0217
Epoch 75/300, seasonal_0 Loss: 0.0311 | 0.0211
Epoch 76/300, seasonal_0 Loss: 0.0310 | 0.0201
Epoch 77/300, seasonal_0 Loss: 0.0307 | 0.0198
Epoch 78/300, seasonal_0 Loss: 0.0301 | 0.0198
Epoch 79/300, seasonal_0 Loss: 0.0293 | 0.0207
Epoch 80/300, seasonal_0 Loss: 0.0296 | 0.0201
Epoch 81/300, seasonal_0 Loss: 0.0288 | 0.0209
Epoch 82/300, seasonal_0 Loss: 0.0295 | 0.0211
Epoch 83/300, seasonal_0 Loss: 0.0289 | 0.0219
Epoch 84/300, seasonal_0 Loss: 0.0284 | 0.0223
Epoch 85/300, seasonal_0 Loss: 0.0285 | 0.0212
Epoch 86/300, seasonal_0 Loss: 0.0275 | 0.0219
Epoch 87/300, seasonal_0 Loss: 0.0265 | 0.0221
Epoch 88/300, seasonal_0 Loss: 0.0253 | 0.0223
Epoch 89/300, seasonal_0 Loss: 0.0268 | 0.0220
Epoch 90/300, seasonal_0 Loss: 0.0273 | 0.0231
Epoch 91/300, seasonal_0 Loss: 0.0266 | 0.0228
Epoch 92/300, seasonal_0 Loss: 0.0262 | 0.0237
Epoch 93/300, seasonal_0 Loss: 0.0263 | 0.0239
Epoch 94/300, seasonal_0 Loss: 0.0267 | 0.0270
Epoch 95/300, seasonal_0 Loss: 0.0301 | 0.0290
Epoch 96/300, seasonal_0 Loss: 0.0255 | 0.0255
Epoch 97/300, seasonal_0 Loss: 0.0243 | 0.0236
Epoch 98/300, seasonal_0 Loss: 0.0248 | 0.0230
Epoch 99/300, seasonal_0 Loss: 0.0249 | 0.0245
Epoch 100/300, seasonal_0 Loss: 0.0236 | 0.0238
Epoch 101/300, seasonal_0 Loss: 0.0239 | 0.0236
Epoch 102/300, seasonal_0 Loss: 0.0234 | 0.0230
Epoch 103/300, seasonal_0 Loss: 0.0229 | 0.0235
Epoch 104/300, seasonal_0 Loss: 0.0228 | 0.0235
Epoch 105/300, seasonal_0 Loss: 0.0227 | 0.0236
Epoch 106/300, seasonal_0 Loss: 0.0226 | 0.0236
Epoch 107/300, seasonal_0 Loss: 0.0226 | 0.0237
Epoch 108/300, seasonal_0 Loss: 0.0225 | 0.0237
Epoch 109/300, seasonal_0 Loss: 0.0224 | 0.0237
Epoch 110/300, seasonal_0 Loss: 0.0223 | 0.0237
Epoch 111/300, seasonal_0 Loss: 0.0221 | 0.0241
Epoch 112/300, seasonal_0 Loss: 0.0222 | 0.0246
Epoch 113/300, seasonal_0 Loss: 0.0236 | 0.0242
Epoch 114/300, seasonal_0 Loss: 0.0223 | 0.0250
Epoch 115/300, seasonal_0 Loss: 0.0221 | 0.0251
Epoch 116/300, seasonal_0 Loss: 0.0220 | 0.0252
Epoch 117/300, seasonal_0 Loss: 0.0219 | 0.0254
Epoch 118/300, seasonal_0 Loss: 0.0217 | 0.0254
Epoch 119/300, seasonal_0 Loss: 0.0217 | 0.0253
Epoch 120/300, seasonal_0 Loss: 0.0228 | 0.0250
Epoch 121/300, seasonal_0 Loss: 0.0219 | 0.0253
Epoch 122/300, seasonal_0 Loss: 0.0218 | 0.0256
Epoch 123/300, seasonal_0 Loss: 0.0216 | 0.0256
Epoch 124/300, seasonal_0 Loss: 0.0215 | 0.0260
Epoch 125/300, seasonal_0 Loss: 0.0218 | 0.0259
Epoch 126/300, seasonal_0 Loss: 0.0214 | 0.0260
Epoch 127/300, seasonal_0 Loss: 0.0213 | 0.0260
Epoch 128/300, seasonal_0 Loss: 0.0212 | 0.0260
Epoch 129/300, seasonal_0 Loss: 0.0213 | 0.0261
Epoch 130/300, seasonal_0 Loss: 0.0213 | 0.0256
Epoch 131/300, seasonal_0 Loss: 0.0211 | 0.0256
Epoch 132/300, seasonal_0 Loss: 0.0211 | 0.0258
Epoch 133/300, seasonal_0 Loss: 0.0211 | 0.0257
Epoch 134/300, seasonal_0 Loss: 0.0210 | 0.0259
Epoch 135/300, seasonal_0 Loss: 0.0211 | 0.0258
Epoch 136/300, seasonal_0 Loss: 0.0209 | 0.0259
Epoch 137/300, seasonal_0 Loss: 0.0209 | 0.0258
Epoch 138/300, seasonal_0 Loss: 0.0209 | 0.0258
Epoch 139/300, seasonal_0 Loss: 0.0209 | 0.0259
Epoch 140/300, seasonal_0 Loss: 0.0209 | 0.0259
Epoch 141/300, seasonal_0 Loss: 0.0208 | 0.0259
Epoch 142/300, seasonal_0 Loss: 0.0208 | 0.0259
Epoch 143/300, seasonal_0 Loss: 0.0208 | 0.0259
Epoch 144/300, seasonal_0 Loss: 0.0208 | 0.0259
Epoch 145/300, seasonal_0 Loss: 0.0208 | 0.0260
Epoch 146/300, seasonal_0 Loss: 0.0208 | 0.0260
Epoch 147/300, seasonal_0 Loss: 0.0207 | 0.0260
Epoch 148/300, seasonal_0 Loss: 0.0207 | 0.0260
Epoch 149/300, seasonal_0 Loss: 0.0207 | 0.0261
Epoch 150/300, seasonal_0 Loss: 0.0207 | 0.0260
Epoch 151/300, seasonal_0 Loss: 0.0207 | 0.0261
Epoch 152/300, seasonal_0 Loss: 0.0207 | 0.0261
Epoch 153/300, seasonal_0 Loss: 0.0207 | 0.0261
Epoch 154/300, seasonal_0 Loss: 0.0207 | 0.0261
Epoch 155/300, seasonal_0 Loss: 0.0206 | 0.0261
Epoch 156/300, seasonal_0 Loss: 0.0206 | 0.0261
Epoch 157/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 158/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 159/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 160/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 161/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 162/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 163/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 164/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 165/300, seasonal_0 Loss: 0.0206 | 0.0262
Epoch 166/300, seasonal_0 Loss: 0.0205 | 0.0262
Epoch 167/300, seasonal_0 Loss: 0.0205 | 0.0262
Epoch 168/300, seasonal_0 Loss: 0.0205 | 0.0262
Epoch 169/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 170/300, seasonal_0 Loss: 0.0205 | 0.0262
Epoch 171/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 172/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 173/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 174/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 175/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 176/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 177/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 178/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 179/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 180/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 181/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 182/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 183/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 184/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 185/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 186/300, seasonal_0 Loss: 0.0205 | 0.0263
Epoch 187/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 188/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 189/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 190/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 191/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 192/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 193/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 194/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 195/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 196/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 197/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 198/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 199/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 200/300, seasonal_0 Loss: 0.0204 | 0.0263
Epoch 201/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 202/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 203/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 204/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 205/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 206/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 207/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 208/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 209/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 210/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 211/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 212/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 213/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 214/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 215/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 216/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 217/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 218/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 219/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 220/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 221/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 222/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 223/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 224/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 225/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 226/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 227/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 228/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 229/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 230/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 231/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 232/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 233/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 234/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 235/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 236/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 237/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 238/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 239/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 240/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 241/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 242/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 243/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 244/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 245/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 246/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 247/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 248/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 249/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 250/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 251/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 252/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 253/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 254/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 255/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 256/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 257/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 258/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 259/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 260/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 261/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 262/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 263/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 264/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 265/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 266/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 267/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 268/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 269/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 270/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 271/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 272/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 273/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 274/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 275/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 276/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 277/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 278/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 279/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 280/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 281/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 282/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 283/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 284/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 285/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 286/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 287/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 288/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 289/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 290/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 291/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 292/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 293/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 294/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 295/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 296/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 297/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 298/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 299/300, seasonal_0 Loss: 0.0204 | 0.0264
Epoch 300/300, seasonal_0 Loss: 0.0204 | 0.0264
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.7785433258334205, 'learning_rate': 8.252252537573258e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.776859528181741}
Epoch 1/300, seasonal_1 Loss: 0.2634 | 0.1030
Epoch 2/300, seasonal_1 Loss: 0.1141 | 0.0934
Epoch 3/300, seasonal_1 Loss: 0.1081 | 0.1558
Epoch 4/300, seasonal_1 Loss: 0.1061 | 0.1301
Epoch 5/300, seasonal_1 Loss: 0.1023 | 0.0904
Epoch 6/300, seasonal_1 Loss: 0.0988 | 0.0887
Epoch 7/300, seasonal_1 Loss: 0.0917 | 0.0660
Epoch 8/300, seasonal_1 Loss: 0.0955 | 0.0692
Epoch 9/300, seasonal_1 Loss: 0.0923 | 0.0587
Epoch 10/300, seasonal_1 Loss: 0.0903 | 0.0613
Epoch 11/300, seasonal_1 Loss: 0.0843 | 0.0536
Epoch 12/300, seasonal_1 Loss: 0.0810 | 0.0461
Epoch 13/300, seasonal_1 Loss: 0.0779 | 0.0443
Epoch 14/300, seasonal_1 Loss: 0.0766 | 0.0416
Epoch 15/300, seasonal_1 Loss: 0.0734 | 0.0430
Epoch 16/300, seasonal_1 Loss: 0.0724 | 0.0451
Epoch 17/300, seasonal_1 Loss: 0.0717 | 0.0458
Epoch 18/300, seasonal_1 Loss: 0.0698 | 0.0401
Epoch 19/300, seasonal_1 Loss: 0.0682 | 0.0387
Epoch 20/300, seasonal_1 Loss: 0.0673 | 0.0394
Epoch 21/300, seasonal_1 Loss: 0.0667 | 0.0628
Epoch 22/300, seasonal_1 Loss: 0.0660 | 0.0703
Epoch 23/300, seasonal_1 Loss: 0.0651 | 0.0508
Epoch 24/300, seasonal_1 Loss: 0.0646 | 0.0599
Epoch 25/300, seasonal_1 Loss: 0.0642 | 0.0477
Epoch 26/300, seasonal_1 Loss: 0.0636 | 0.0396
Epoch 27/300, seasonal_1 Loss: 0.0627 | 0.0388
Epoch 28/300, seasonal_1 Loss: 0.0623 | 0.0380
Epoch 29/300, seasonal_1 Loss: 0.0622 | 0.0398
Epoch 30/300, seasonal_1 Loss: 0.0618 | 0.0389
Epoch 31/300, seasonal_1 Loss: 0.0607 | 0.0368
Epoch 32/300, seasonal_1 Loss: 0.0599 | 0.0385
Epoch 33/300, seasonal_1 Loss: 0.0594 | 0.0378
Epoch 34/300, seasonal_1 Loss: 0.0587 | 0.0319
Epoch 35/300, seasonal_1 Loss: 0.0582 | 0.0332
Epoch 36/300, seasonal_1 Loss: 0.0577 | 0.0331
Epoch 37/300, seasonal_1 Loss: 0.0572 | 0.0334
Epoch 38/300, seasonal_1 Loss: 0.0567 | 0.0341
Epoch 39/300, seasonal_1 Loss: 0.0563 | 0.0340
Epoch 40/300, seasonal_1 Loss: 0.0558 | 0.0320
Epoch 41/300, seasonal_1 Loss: 0.0554 | 0.0322
Epoch 42/300, seasonal_1 Loss: 0.0551 | 0.0333
Epoch 43/300, seasonal_1 Loss: 0.0552 | 0.0364
Epoch 44/300, seasonal_1 Loss: 0.0552 | 0.0352
Epoch 45/300, seasonal_1 Loss: 0.0543 | 0.0361
Epoch 46/300, seasonal_1 Loss: 0.0537 | 0.0375
Epoch 47/300, seasonal_1 Loss: 0.0535 | 0.0395
Epoch 48/300, seasonal_1 Loss: 0.0533 | 0.0404
Epoch 49/300, seasonal_1 Loss: 0.0530 | 0.0423
Epoch 50/300, seasonal_1 Loss: 0.0525 | 0.0444
Epoch 51/300, seasonal_1 Loss: 0.0522 | 0.0470
Epoch 52/300, seasonal_1 Loss: 0.0520 | 0.0456
Epoch 53/300, seasonal_1 Loss: 0.0516 | 0.0444
Epoch 54/300, seasonal_1 Loss: 0.0514 | 0.0420
Epoch 55/300, seasonal_1 Loss: 0.0485 | 0.0434
Epoch 56/300, seasonal_1 Loss: 0.0519 | 0.0368
Epoch 57/300, seasonal_1 Loss: 0.0478 | 0.0354
Epoch 58/300, seasonal_1 Loss: 0.0455 | 0.0379
Epoch 59/300, seasonal_1 Loss: 0.0447 | 0.0392
Epoch 60/300, seasonal_1 Loss: 0.0444 | 0.0361
Epoch 61/300, seasonal_1 Loss: 0.0471 | 0.0399
Epoch 62/300, seasonal_1 Loss: 0.0465 | 0.0410
Epoch 63/300, seasonal_1 Loss: 0.0456 | 0.0396
Epoch 64/300, seasonal_1 Loss: 0.0447 | 0.0421
Epoch 65/300, seasonal_1 Loss: 0.0435 | 0.0379
Epoch 66/300, seasonal_1 Loss: 0.0430 | 0.0394
Epoch 67/300, seasonal_1 Loss: 0.0427 | 0.0386
Epoch 68/300, seasonal_1 Loss: 0.0425 | 0.0400
Epoch 69/300, seasonal_1 Loss: 0.0423 | 0.0383
Epoch 70/300, seasonal_1 Loss: 0.0422 | 0.0438
Epoch 71/300, seasonal_1 Loss: 0.0422 | 0.0379
Epoch 72/300, seasonal_1 Loss: 0.0423 | 0.0462
Epoch 73/300, seasonal_1 Loss: 0.0417 | 0.0389
Epoch 74/300, seasonal_1 Loss: 0.0411 | 0.0423
Epoch 75/300, seasonal_1 Loss: 0.0400 | 0.0405
Epoch 76/300, seasonal_1 Loss: 0.0397 | 0.0417
Epoch 77/300, seasonal_1 Loss: 0.0405 | 0.0424
Epoch 78/300, seasonal_1 Loss: 0.0385 | 0.0417
Epoch 79/300, seasonal_1 Loss: 0.0390 | 0.0431
Epoch 80/300, seasonal_1 Loss: 0.0394 | 0.0416
Epoch 81/300, seasonal_1 Loss: 0.0379 | 0.0435
Epoch 82/300, seasonal_1 Loss: 0.0378 | 0.0409
Epoch 83/300, seasonal_1 Loss: 0.0373 | 0.0459
Epoch 84/300, seasonal_1 Loss: 0.0374 | 0.0419
Epoch 85/300, seasonal_1 Loss: 0.0369 | 0.0450
Epoch 86/300, seasonal_1 Loss: 0.0369 | 0.0421
Epoch 87/300, seasonal_1 Loss: 0.0367 | 0.0469
Epoch 88/300, seasonal_1 Loss: 0.0369 | 0.0431
Epoch 89/300, seasonal_1 Loss: 0.0364 | 0.0468
Epoch 90/300, seasonal_1 Loss: 0.0364 | 0.0460
Epoch 91/300, seasonal_1 Loss: 0.0361 | 0.0477
Epoch 92/300, seasonal_1 Loss: 0.0360 | 0.0470
Epoch 93/300, seasonal_1 Loss: 0.0358 | 0.0479
Epoch 94/300, seasonal_1 Loss: 0.0357 | 0.0466
Epoch 95/300, seasonal_1 Loss: 0.0355 | 0.0473
Epoch 96/300, seasonal_1 Loss: 0.0355 | 0.0468
Epoch 97/300, seasonal_1 Loss: 0.0353 | 0.0472
Epoch 98/300, seasonal_1 Loss: 0.0353 | 0.0467
Epoch 99/300, seasonal_1 Loss: 0.0352 | 0.0470
Epoch 100/300, seasonal_1 Loss: 0.0351 | 0.0466
Epoch 101/300, seasonal_1 Loss: 0.0350 | 0.0469
Epoch 102/300, seasonal_1 Loss: 0.0350 | 0.0468
Epoch 103/300, seasonal_1 Loss: 0.0349 | 0.0473
Epoch 104/300, seasonal_1 Loss: 0.0348 | 0.0469
Epoch 105/300, seasonal_1 Loss: 0.0348 | 0.0481
Epoch 106/300, seasonal_1 Loss: 0.0348 | 0.0475
Epoch 107/300, seasonal_1 Loss: 0.0346 | 0.0473
Epoch 108/300, seasonal_1 Loss: 0.0346 | 0.0474
Epoch 109/300, seasonal_1 Loss: 0.0345 | 0.0481
Epoch 110/300, seasonal_1 Loss: 0.0346 | 0.0476
Epoch 111/300, seasonal_1 Loss: 0.0345 | 0.0481
Epoch 112/300, seasonal_1 Loss: 0.0345 | 0.0483
Epoch 113/300, seasonal_1 Loss: 0.0343 | 0.0479
Epoch 114/300, seasonal_1 Loss: 0.0343 | 0.0483
Epoch 115/300, seasonal_1 Loss: 0.0344 | 0.0482
Epoch 116/300, seasonal_1 Loss: 0.0342 | 0.0486
Epoch 117/300, seasonal_1 Loss: 0.0344 | 0.0489
Epoch 118/300, seasonal_1 Loss: 0.0342 | 0.0483
Epoch 119/300, seasonal_1 Loss: 0.0341 | 0.0485
Epoch 120/300, seasonal_1 Loss: 0.0342 | 0.0489
Epoch 121/300, seasonal_1 Loss: 0.0340 | 0.0484
Epoch 122/300, seasonal_1 Loss: 0.0340 | 0.0486
Epoch 123/300, seasonal_1 Loss: 0.0339 | 0.0487
Epoch 124/300, seasonal_1 Loss: 0.0339 | 0.0487
Epoch 125/300, seasonal_1 Loss: 0.0339 | 0.0488
Epoch 126/300, seasonal_1 Loss: 0.0339 | 0.0489
Epoch 127/300, seasonal_1 Loss: 0.0340 | 0.0494
Epoch 128/300, seasonal_1 Loss: 0.0338 | 0.0489
Epoch 129/300, seasonal_1 Loss: 0.0337 | 0.0489
Epoch 130/300, seasonal_1 Loss: 0.0337 | 0.0490
Epoch 131/300, seasonal_1 Loss: 0.0337 | 0.0490
Epoch 132/300, seasonal_1 Loss: 0.0337 | 0.0491
Epoch 133/300, seasonal_1 Loss: 0.0337 | 0.0490
Epoch 134/300, seasonal_1 Loss: 0.0337 | 0.0493
Epoch 135/300, seasonal_1 Loss: 0.0336 | 0.0491
Epoch 136/300, seasonal_1 Loss: 0.0336 | 0.0493
Epoch 137/300, seasonal_1 Loss: 0.0336 | 0.0491
Epoch 138/300, seasonal_1 Loss: 0.0336 | 0.0494
Epoch 139/300, seasonal_1 Loss: 0.0335 | 0.0492
Epoch 140/300, seasonal_1 Loss: 0.0335 | 0.0493
Epoch 141/300, seasonal_1 Loss: 0.0335 | 0.0493
Epoch 142/300, seasonal_1 Loss: 0.0335 | 0.0494
Epoch 143/300, seasonal_1 Loss: 0.0334 | 0.0493
Epoch 144/300, seasonal_1 Loss: 0.0334 | 0.0494
Epoch 145/300, seasonal_1 Loss: 0.0334 | 0.0494
Epoch 146/300, seasonal_1 Loss: 0.0334 | 0.0494
Epoch 147/300, seasonal_1 Loss: 0.0334 | 0.0495
Epoch 148/300, seasonal_1 Loss: 0.0334 | 0.0495
Epoch 149/300, seasonal_1 Loss: 0.0333 | 0.0495
Epoch 150/300, seasonal_1 Loss: 0.0333 | 0.0495
Epoch 151/300, seasonal_1 Loss: 0.0333 | 0.0495
Epoch 152/300, seasonal_1 Loss: 0.0333 | 0.0496
Epoch 153/300, seasonal_1 Loss: 0.0333 | 0.0496
Epoch 154/300, seasonal_1 Loss: 0.0333 | 0.0496
Epoch 155/300, seasonal_1 Loss: 0.0333 | 0.0496
Epoch 156/300, seasonal_1 Loss: 0.0333 | 0.0496
Epoch 157/300, seasonal_1 Loss: 0.0332 | 0.0496
Epoch 158/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 159/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 160/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 161/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 162/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 163/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 164/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 165/300, seasonal_1 Loss: 0.0332 | 0.0497
Epoch 166/300, seasonal_1 Loss: 0.0332 | 0.0498
Epoch 167/300, seasonal_1 Loss: 0.0332 | 0.0498
Epoch 168/300, seasonal_1 Loss: 0.0332 | 0.0498
Epoch 169/300, seasonal_1 Loss: 0.0332 | 0.0498
Epoch 170/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 171/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 172/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 173/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 174/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 175/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 176/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 177/300, seasonal_1 Loss: 0.0331 | 0.0498
Epoch 178/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 179/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 180/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 181/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 182/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 183/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 184/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 185/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 186/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 187/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 188/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 189/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 190/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 191/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 192/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 193/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 194/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 195/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 196/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 197/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 198/300, seasonal_1 Loss: 0.0331 | 0.0499
Epoch 199/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 200/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 201/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 202/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 203/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 204/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 205/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 206/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 207/300, seasonal_1 Loss: 0.0330 | 0.0499
Epoch 208/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 209/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 210/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 211/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 212/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 213/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 214/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 215/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 216/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 217/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 218/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 219/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 220/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 221/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 222/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 223/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 224/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 225/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 226/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 227/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 228/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 229/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 230/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 231/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 232/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 233/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 234/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 235/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 236/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 237/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 238/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 239/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 240/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 241/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 242/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 243/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 244/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 245/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 246/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 247/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 248/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 249/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 250/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 251/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 252/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 253/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 254/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 255/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 256/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 257/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 258/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 259/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 260/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 261/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 262/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 263/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 264/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 265/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 266/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 267/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 268/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 269/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 270/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 271/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 272/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 273/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 274/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 275/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 276/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 277/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 278/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 279/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 280/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 281/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 282/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 283/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 284/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 285/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 286/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 287/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 288/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 289/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 290/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 291/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 292/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 293/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 294/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 295/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 296/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 297/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 298/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 299/300, seasonal_1 Loss: 0.0330 | 0.0500
Epoch 300/300, seasonal_1 Loss: 0.0330 | 0.0500
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.925821857624237, 'learning_rate': 0.0005757522063584108, 'batch_size': 200, 'step_size': 6, 'gamma': 0.9318663327202252}
Epoch 1/300, seasonal_2 Loss: 2.3767 | 1.6250
Epoch 2/300, seasonal_2 Loss: 1.2319 | 1.5974
Epoch 3/300, seasonal_2 Loss: 1.3404 | 1.5760
Epoch 4/300, seasonal_2 Loss: 1.5591 | 1.9822
Epoch 5/300, seasonal_2 Loss: 1.1967 | 2.3494
Epoch 6/300, seasonal_2 Loss: 0.9675 | 2.1150
Epoch 7/300, seasonal_2 Loss: 1.0046 | 2.1900
Epoch 8/300, seasonal_2 Loss: 0.9885 | 2.1746
Epoch 9/300, seasonal_2 Loss: 0.9878 | 2.1699
Epoch 10/300, seasonal_2 Loss: 0.9849 | 2.1922
Epoch 11/300, seasonal_2 Loss: 0.9766 | 2.1762
Epoch 12/300, seasonal_2 Loss: 0.9820 | 2.1854
Epoch 13/300, seasonal_2 Loss: 0.9736 | 2.1934
Epoch 14/300, seasonal_2 Loss: 0.9703 | 2.1872
Epoch 15/300, seasonal_2 Loss: 0.9725 | 2.1911
Epoch 16/300, seasonal_2 Loss: 0.9658 | 2.1990
Epoch 17/300, seasonal_2 Loss: 0.9628 | 2.1935
Epoch 18/300, seasonal_2 Loss: 0.9632 | 2.1598
Epoch 19/300, seasonal_2 Loss: 1.2007 | 2.1578
Epoch 20/300, seasonal_2 Loss: 0.9772 | 2.1988
Epoch 21/300, seasonal_2 Loss: 0.9702 | 2.1966
Epoch 22/300, seasonal_2 Loss: 0.9600 | 2.2104
Epoch 23/300, seasonal_2 Loss: 0.9545 | 2.2042
Epoch 24/300, seasonal_2 Loss: 0.9548 | 2.2086
Epoch 25/300, seasonal_2 Loss: 0.9488 | 2.2126
Epoch 26/300, seasonal_2 Loss: 0.9471 | 2.2127
Epoch 27/300, seasonal_2 Loss: 0.9468 | 2.2123
Epoch 28/300, seasonal_2 Loss: 0.9430 | 2.2171
Epoch 29/300, seasonal_2 Loss: 0.9417 | 2.2168
Epoch 30/300, seasonal_2 Loss: 0.9417 | 2.2162
Epoch 31/300, seasonal_2 Loss: 0.9384 | 2.2201
Epoch 32/300, seasonal_2 Loss: 0.9376 | 2.2205
Epoch 33/300, seasonal_2 Loss: 0.9374 | 2.2194
Epoch 34/300, seasonal_2 Loss: 0.9347 | 2.2227
Epoch 35/300, seasonal_2 Loss: 0.9341 | 2.2236
Epoch 36/300, seasonal_2 Loss: 0.9338 | 2.2222
Epoch 37/300, seasonal_2 Loss: 0.9315 | 2.2250
Epoch 38/300, seasonal_2 Loss: 0.9310 | 2.2262
Epoch 39/300, seasonal_2 Loss: 0.9307 | 2.2246
Epoch 40/300, seasonal_2 Loss: 0.9286 | 2.2269
Epoch 41/300, seasonal_2 Loss: 0.9284 | 2.2284
Epoch 42/300, seasonal_2 Loss: 0.9280 | 2.2268
Epoch 43/300, seasonal_2 Loss: 0.9261 | 2.2286
Epoch 44/300, seasonal_2 Loss: 0.9260 | 2.2303
Epoch 45/300, seasonal_2 Loss: 0.9256 | 2.2288
Epoch 46/300, seasonal_2 Loss: 0.9239 | 2.2302
Epoch 47/300, seasonal_2 Loss: 0.9239 | 2.2320
Epoch 48/300, seasonal_2 Loss: 0.9235 | 2.2306
Epoch 49/300, seasonal_2 Loss: 0.9219 | 2.2316
Epoch 50/300, seasonal_2 Loss: 0.9219 | 2.2334
Epoch 51/300, seasonal_2 Loss: 0.9216 | 2.2322
Epoch 52/300, seasonal_2 Loss: 0.9201 | 2.2329
Epoch 53/300, seasonal_2 Loss: 0.9202 | 2.2347
Epoch 54/300, seasonal_2 Loss: 0.9199 | 2.2337
Epoch 55/300, seasonal_2 Loss: 0.9184 | 2.2340
Epoch 56/300, seasonal_2 Loss: 0.9186 | 2.2358
Epoch 57/300, seasonal_2 Loss: 0.9183 | 2.2350
Epoch 58/300, seasonal_2 Loss: 0.9169 | 2.2351
Epoch 59/300, seasonal_2 Loss: 0.9171 | 2.2367
Epoch 60/300, seasonal_2 Loss: 0.9168 | 2.2361
Epoch 61/300, seasonal_2 Loss: 0.9155 | 2.2360
Epoch 62/300, seasonal_2 Loss: 0.9157 | 2.2375
Epoch 63/300, seasonal_2 Loss: 0.9155 | 2.2370
Epoch 64/300, seasonal_2 Loss: 0.9143 | 2.2368
Epoch 65/300, seasonal_2 Loss: 0.9144 | 2.2380
Epoch 66/300, seasonal_2 Loss: 0.9142 | 2.2376
Epoch 67/300, seasonal_2 Loss: 0.9130 | 2.2373
Epoch 68/300, seasonal_2 Loss: 0.9131 | 2.2382
Epoch 69/300, seasonal_2 Loss: 0.9128 | 2.2376
Epoch 70/300, seasonal_2 Loss: 0.9116 | 2.2368
Epoch 71/300, seasonal_2 Loss: 0.9114 | 2.2367
Epoch 72/300, seasonal_2 Loss: 0.9106 | 2.2340
Epoch 73/300, seasonal_2 Loss: 0.9078 | 2.2261
Epoch 74/300, seasonal_2 Loss: 0.8988 | 2.1794
Epoch 75/300, seasonal_2 Loss: 0.7806 | 1.0655
Epoch 76/300, seasonal_2 Loss: 0.5351 | 0.4694
Epoch 77/300, seasonal_2 Loss: 0.5254 | 0.6268
Epoch 78/300, seasonal_2 Loss: 0.4561 | 0.4126
Epoch 79/300, seasonal_2 Loss: 0.4159 | 0.6194
Epoch 80/300, seasonal_2 Loss: 0.3983 | 0.4967
Epoch 81/300, seasonal_2 Loss: 0.3868 | 0.5557
Epoch 82/300, seasonal_2 Loss: 0.3753 | 0.5429
Epoch 83/300, seasonal_2 Loss: 0.3573 | 0.4801
Epoch 84/300, seasonal_2 Loss: 0.3737 | 0.3641
Epoch 85/300, seasonal_2 Loss: 0.3921 | 0.5362
Epoch 86/300, seasonal_2 Loss: 0.3738 | 0.5634
Epoch 87/300, seasonal_2 Loss: 0.3014 | 0.4070
Epoch 88/300, seasonal_2 Loss: 0.2152 | 0.1956
Epoch 89/300, seasonal_2 Loss: 0.1890 | 0.1656
Epoch 90/300, seasonal_2 Loss: 0.2084 | 0.1987
Epoch 91/300, seasonal_2 Loss: 0.2394 | 0.2763
Epoch 92/300, seasonal_2 Loss: 0.2255 | 0.1965
Epoch 93/300, seasonal_2 Loss: 0.1790 | 0.1700
Epoch 94/300, seasonal_2 Loss: 0.1612 | 0.2593
Epoch 95/300, seasonal_2 Loss: 0.1602 | 0.1698
Epoch 96/300, seasonal_2 Loss: 0.1419 | 0.1563
Epoch 97/300, seasonal_2 Loss: 0.1449 | 0.2226
Epoch 98/300, seasonal_2 Loss: 0.1450 | 0.1476
Epoch 99/300, seasonal_2 Loss: 0.1374 | 0.1122
Epoch 100/300, seasonal_2 Loss: 0.1259 | 0.1312
Epoch 101/300, seasonal_2 Loss: 0.1288 | 0.1819
Epoch 102/300, seasonal_2 Loss: 0.1210 | 0.1129
Epoch 103/300, seasonal_2 Loss: 0.1183 | 0.0866
Epoch 104/300, seasonal_2 Loss: 0.1294 | 0.1190
Epoch 105/300, seasonal_2 Loss: 0.1259 | 0.0967
Epoch 106/300, seasonal_2 Loss: 0.1146 | 0.0888
Epoch 107/300, seasonal_2 Loss: 0.1060 | 0.0811
Epoch 108/300, seasonal_2 Loss: 0.1145 | 0.0741
Epoch 109/300, seasonal_2 Loss: 0.1019 | 0.0784
Epoch 110/300, seasonal_2 Loss: 0.0927 | 0.0681
Epoch 111/300, seasonal_2 Loss: 0.1008 | 0.0729
Epoch 112/300, seasonal_2 Loss: 0.0938 | 0.0672
Epoch 113/300, seasonal_2 Loss: 0.0976 | 0.0771
Epoch 114/300, seasonal_2 Loss: 0.0971 | 0.0692
Epoch 115/300, seasonal_2 Loss: 0.0998 | 0.0725
Epoch 116/300, seasonal_2 Loss: 0.0913 | 0.0673
Epoch 117/300, seasonal_2 Loss: 0.0905 | 0.0627
Epoch 118/300, seasonal_2 Loss: 0.0908 | 0.0635
Epoch 119/300, seasonal_2 Loss: 0.0859 | 0.0634
Epoch 120/300, seasonal_2 Loss: 0.0865 | 0.0575
Epoch 121/300, seasonal_2 Loss: 0.0849 | 0.0583
Epoch 122/300, seasonal_2 Loss: 0.0841 | 0.0590
Epoch 123/300, seasonal_2 Loss: 0.0824 | 0.0558
Epoch 124/300, seasonal_2 Loss: 0.0815 | 0.0582
Epoch 125/300, seasonal_2 Loss: 0.0810 | 0.0559
Epoch 126/300, seasonal_2 Loss: 0.0803 | 0.0549
Epoch 127/300, seasonal_2 Loss: 0.0800 | 0.0566
Epoch 128/300, seasonal_2 Loss: 0.0794 | 0.0540
Epoch 129/300, seasonal_2 Loss: 0.0791 | 0.0555
Epoch 130/300, seasonal_2 Loss: 0.0787 | 0.0543
Epoch 131/300, seasonal_2 Loss: 0.0783 | 0.0523
Epoch 132/300, seasonal_2 Loss: 0.0781 | 0.0529
Epoch 133/300, seasonal_2 Loss: 0.0780 | 0.0509
Epoch 134/300, seasonal_2 Loss: 0.0776 | 0.0516
Epoch 135/300, seasonal_2 Loss: 0.0772 | 0.0520
Epoch 136/300, seasonal_2 Loss: 0.0772 | 0.0514
Epoch 137/300, seasonal_2 Loss: 0.0769 | 0.0505
Epoch 138/300, seasonal_2 Loss: 0.0765 | 0.0486
Epoch 139/300, seasonal_2 Loss: 0.0774 | 0.0497
Epoch 140/300, seasonal_2 Loss: 0.0763 | 0.0495
Epoch 141/300, seasonal_2 Loss: 0.0768 | 0.0487
Epoch 142/300, seasonal_2 Loss: 0.0762 | 0.0485
Epoch 143/300, seasonal_2 Loss: 0.0755 | 0.0473
Epoch 144/300, seasonal_2 Loss: 0.0753 | 0.0492
Epoch 145/300, seasonal_2 Loss: 0.0748 | 0.0467
Epoch 146/300, seasonal_2 Loss: 0.0746 | 0.0470
Epoch 147/300, seasonal_2 Loss: 0.0741 | 0.0463
Epoch 148/300, seasonal_2 Loss: 0.0739 | 0.0462
Epoch 149/300, seasonal_2 Loss: 0.0736 | 0.0458
Epoch 150/300, seasonal_2 Loss: 0.0734 | 0.0447
Epoch 151/300, seasonal_2 Loss: 0.0732 | 0.0451
Epoch 152/300, seasonal_2 Loss: 0.0728 | 0.0444
Epoch 153/300, seasonal_2 Loss: 0.0728 | 0.0452
Epoch 154/300, seasonal_2 Loss: 0.0729 | 0.0442
Epoch 155/300, seasonal_2 Loss: 0.0725 | 0.0437
Epoch 156/300, seasonal_2 Loss: 0.0725 | 0.0433
Epoch 157/300, seasonal_2 Loss: 0.0738 | 0.0431
Epoch 158/300, seasonal_2 Loss: 0.0736 | 0.0433
Epoch 159/300, seasonal_2 Loss: 0.0724 | 0.0438
Epoch 160/300, seasonal_2 Loss: 0.0740 | 0.0439
Epoch 161/300, seasonal_2 Loss: 0.0720 | 0.0428
Epoch 162/300, seasonal_2 Loss: 0.0719 | 0.0434
Epoch 163/300, seasonal_2 Loss: 0.0712 | 0.0422
Epoch 164/300, seasonal_2 Loss: 0.0709 | 0.0421
Epoch 165/300, seasonal_2 Loss: 0.0707 | 0.0415
Epoch 166/300, seasonal_2 Loss: 0.0705 | 0.0415
Epoch 167/300, seasonal_2 Loss: 0.0703 | 0.0410
Epoch 168/300, seasonal_2 Loss: 0.0701 | 0.0409
Epoch 169/300, seasonal_2 Loss: 0.0700 | 0.0405
Epoch 170/300, seasonal_2 Loss: 0.0698 | 0.0405
Epoch 171/300, seasonal_2 Loss: 0.0697 | 0.0401
Epoch 172/300, seasonal_2 Loss: 0.0695 | 0.0401
Epoch 173/300, seasonal_2 Loss: 0.0694 | 0.0398
Epoch 174/300, seasonal_2 Loss: 0.0693 | 0.0398
Epoch 175/300, seasonal_2 Loss: 0.0692 | 0.0396
Epoch 176/300, seasonal_2 Loss: 0.0691 | 0.0394
Epoch 177/300, seasonal_2 Loss: 0.0690 | 0.0391
Epoch 178/300, seasonal_2 Loss: 0.0690 | 0.0391
Epoch 179/300, seasonal_2 Loss: 0.0689 | 0.0390
Epoch 180/300, seasonal_2 Loss: 0.0688 | 0.0392
Epoch 181/300, seasonal_2 Loss: 0.0688 | 0.0392
Epoch 182/300, seasonal_2 Loss: 0.0687 | 0.0389
Epoch 183/300, seasonal_2 Loss: 0.0686 | 0.0385
Epoch 184/300, seasonal_2 Loss: 0.0689 | 0.0384
Epoch 185/300, seasonal_2 Loss: 0.0693 | 0.0386
Epoch 186/300, seasonal_2 Loss: 0.0686 | 0.0390
Epoch 187/300, seasonal_2 Loss: 0.0693 | 0.0385
Epoch 188/300, seasonal_2 Loss: 0.0690 | 0.0384
Epoch 189/300, seasonal_2 Loss: 0.0687 | 0.0384
Epoch 190/300, seasonal_2 Loss: 0.0688 | 0.0391
Epoch 191/300, seasonal_2 Loss: 0.0681 | 0.0382
Epoch 192/300, seasonal_2 Loss: 0.0681 | 0.0380
Epoch 193/300, seasonal_2 Loss: 0.0679 | 0.0381
Epoch 194/300, seasonal_2 Loss: 0.0678 | 0.0381
Epoch 195/300, seasonal_2 Loss: 0.0677 | 0.0379
Epoch 196/300, seasonal_2 Loss: 0.0676 | 0.0378
Epoch 197/300, seasonal_2 Loss: 0.0675 | 0.0378
Epoch 198/300, seasonal_2 Loss: 0.0675 | 0.0377
Epoch 199/300, seasonal_2 Loss: 0.0674 | 0.0377
Epoch 200/300, seasonal_2 Loss: 0.0674 | 0.0376
Epoch 201/300, seasonal_2 Loss: 0.0673 | 0.0375
Epoch 202/300, seasonal_2 Loss: 0.0672 | 0.0375
Epoch 203/300, seasonal_2 Loss: 0.0672 | 0.0374
Epoch 204/300, seasonal_2 Loss: 0.0671 | 0.0374
Epoch 205/300, seasonal_2 Loss: 0.0671 | 0.0373
Epoch 206/300, seasonal_2 Loss: 0.0670 | 0.0373
Epoch 207/300, seasonal_2 Loss: 0.0670 | 0.0372
Epoch 208/300, seasonal_2 Loss: 0.0669 | 0.0372
Epoch 209/300, seasonal_2 Loss: 0.0669 | 0.0372
Epoch 210/300, seasonal_2 Loss: 0.0668 | 0.0371
Epoch 211/300, seasonal_2 Loss: 0.0668 | 0.0371
Epoch 212/300, seasonal_2 Loss: 0.0667 | 0.0370
Epoch 213/300, seasonal_2 Loss: 0.0667 | 0.0370
Epoch 214/300, seasonal_2 Loss: 0.0666 | 0.0370
Epoch 215/300, seasonal_2 Loss: 0.0666 | 0.0369
Epoch 216/300, seasonal_2 Loss: 0.0666 | 0.0369
Epoch 217/300, seasonal_2 Loss: 0.0665 | 0.0369
Epoch 218/300, seasonal_2 Loss: 0.0665 | 0.0368
Epoch 219/300, seasonal_2 Loss: 0.0664 | 0.0368
Epoch 220/300, seasonal_2 Loss: 0.0664 | 0.0368
Epoch 221/300, seasonal_2 Loss: 0.0663 | 0.0368
Epoch 222/300, seasonal_2 Loss: 0.0663 | 0.0367
Epoch 223/300, seasonal_2 Loss: 0.0663 | 0.0367
Epoch 224/300, seasonal_2 Loss: 0.0662 | 0.0367
Epoch 225/300, seasonal_2 Loss: 0.0662 | 0.0367
Epoch 226/300, seasonal_2 Loss: 0.0662 | 0.0366
Epoch 227/300, seasonal_2 Loss: 0.0661 | 0.0366
Epoch 228/300, seasonal_2 Loss: 0.0661 | 0.0366
Epoch 229/300, seasonal_2 Loss: 0.0660 | 0.0366
Epoch 230/300, seasonal_2 Loss: 0.0660 | 0.0365
Epoch 231/300, seasonal_2 Loss: 0.0660 | 0.0365
Epoch 232/300, seasonal_2 Loss: 0.0659 | 0.0365
Epoch 233/300, seasonal_2 Loss: 0.0659 | 0.0365
Epoch 234/300, seasonal_2 Loss: 0.0659 | 0.0365
Epoch 235/300, seasonal_2 Loss: 0.0658 | 0.0364
Epoch 236/300, seasonal_2 Loss: 0.0658 | 0.0364
Epoch 237/300, seasonal_2 Loss: 0.0658 | 0.0364
Epoch 238/300, seasonal_2 Loss: 0.0657 | 0.0364
Epoch 239/300, seasonal_2 Loss: 0.0657 | 0.0364
Epoch 240/300, seasonal_2 Loss: 0.0657 | 0.0363
Epoch 241/300, seasonal_2 Loss: 0.0657 | 0.0363
Epoch 242/300, seasonal_2 Loss: 0.0656 | 0.0363
Epoch 243/300, seasonal_2 Loss: 0.0656 | 0.0363
Epoch 244/300, seasonal_2 Loss: 0.0656 | 0.0363
Epoch 245/300, seasonal_2 Loss: 0.0655 | 0.0363
Epoch 246/300, seasonal_2 Loss: 0.0655 | 0.0362
Epoch 247/300, seasonal_2 Loss: 0.0655 | 0.0362
Epoch 248/300, seasonal_2 Loss: 0.0655 | 0.0362
Epoch 249/300, seasonal_2 Loss: 0.0654 | 0.0362
Epoch 250/300, seasonal_2 Loss: 0.0654 | 0.0362
Epoch 251/300, seasonal_2 Loss: 0.0654 | 0.0362
Epoch 252/300, seasonal_2 Loss: 0.0654 | 0.0362
Epoch 253/300, seasonal_2 Loss: 0.0653 | 0.0361
Epoch 254/300, seasonal_2 Loss: 0.0653 | 0.0361
Epoch 255/300, seasonal_2 Loss: 0.0653 | 0.0361
Epoch 256/300, seasonal_2 Loss: 0.0653 | 0.0361
Epoch 257/300, seasonal_2 Loss: 0.0652 | 0.0361
Epoch 258/300, seasonal_2 Loss: 0.0652 | 0.0361
Epoch 259/300, seasonal_2 Loss: 0.0652 | 0.0361
Epoch 260/300, seasonal_2 Loss: 0.0652 | 0.0361
Epoch 261/300, seasonal_2 Loss: 0.0651 | 0.0360
Epoch 262/300, seasonal_2 Loss: 0.0651 | 0.0360
Epoch 263/300, seasonal_2 Loss: 0.0651 | 0.0360
Epoch 264/300, seasonal_2 Loss: 0.0651 | 0.0360
Epoch 265/300, seasonal_2 Loss: 0.0651 | 0.0360
Epoch 266/300, seasonal_2 Loss: 0.0650 | 0.0360
Epoch 267/300, seasonal_2 Loss: 0.0650 | 0.0360
Epoch 268/300, seasonal_2 Loss: 0.0650 | 0.0360
Epoch 269/300, seasonal_2 Loss: 0.0650 | 0.0360
Epoch 270/300, seasonal_2 Loss: 0.0650 | 0.0359
Epoch 271/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 272/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 273/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 274/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 275/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 276/300, seasonal_2 Loss: 0.0649 | 0.0359
Epoch 277/300, seasonal_2 Loss: 0.0648 | 0.0359
Epoch 278/300, seasonal_2 Loss: 0.0648 | 0.0359
Epoch 279/300, seasonal_2 Loss: 0.0648 | 0.0359
Epoch 280/300, seasonal_2 Loss: 0.0648 | 0.0359
Epoch 281/300, seasonal_2 Loss: 0.0648 | 0.0359
Epoch 282/300, seasonal_2 Loss: 0.0648 | 0.0358
Epoch 283/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 284/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 285/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 286/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 287/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 288/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 289/300, seasonal_2 Loss: 0.0647 | 0.0358
Epoch 290/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 291/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 292/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 293/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 294/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 295/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 296/300, seasonal_2 Loss: 0.0646 | 0.0358
Epoch 297/300, seasonal_2 Loss: 0.0646 | 0.0357
Epoch 298/300, seasonal_2 Loss: 0.0645 | 0.0357
Epoch 299/300, seasonal_2 Loss: 0.0645 | 0.0357
Epoch 300/300, seasonal_2 Loss: 0.0645 | 0.0357
Training seasonal_3 component with params: {'observation_period_num': 21, 'train_rates': 0.833618909743636, 'learning_rate': 0.00050282984988223, 'batch_size': 165, 'step_size': 3, 'gamma': 0.9406681386752137}
Epoch 1/300, seasonal_3 Loss: 1.9832 | 1.4453
Epoch 2/300, seasonal_3 Loss: 1.0481 | 1.4326
Epoch 3/300, seasonal_3 Loss: 0.8289 | 0.8849
Epoch 4/300, seasonal_3 Loss: 0.7351 | 0.8664
Epoch 5/300, seasonal_3 Loss: 0.5949 | 0.5419
Epoch 6/300, seasonal_3 Loss: 0.4147 | 0.4297
Epoch 7/300, seasonal_3 Loss: 0.3534 | 1.7208
Epoch 8/300, seasonal_3 Loss: 0.4317 | 0.4634
Epoch 9/300, seasonal_3 Loss: 0.3849 | 0.2899
Epoch 10/300, seasonal_3 Loss: 0.2844 | 0.2196
Epoch 11/300, seasonal_3 Loss: 0.2427 | 0.2208
Epoch 12/300, seasonal_3 Loss: 0.2146 | 0.1476
Epoch 13/300, seasonal_3 Loss: 0.1430 | 0.1244
Epoch 14/300, seasonal_3 Loss: 0.1432 | 0.1346
Epoch 15/300, seasonal_3 Loss: 0.1380 | 0.1649
Epoch 16/300, seasonal_3 Loss: 0.1523 | 0.1240
Epoch 17/300, seasonal_3 Loss: 0.1507 | 0.1106
Epoch 18/300, seasonal_3 Loss: 0.1385 | 0.0980
Epoch 19/300, seasonal_3 Loss: 0.1241 | 0.0789
Epoch 20/300, seasonal_3 Loss: 0.1149 | 0.0844
Epoch 21/300, seasonal_3 Loss: 0.1138 | 0.0719
Epoch 22/300, seasonal_3 Loss: 0.1124 | 0.0850
Epoch 23/300, seasonal_3 Loss: 0.1081 | 0.0673
Epoch 24/300, seasonal_3 Loss: 0.1198 | 0.0828
Epoch 25/300, seasonal_3 Loss: 0.1175 | 0.0649
Epoch 26/300, seasonal_3 Loss: 0.1016 | 0.0575
Epoch 27/300, seasonal_3 Loss: 0.1094 | 0.0970
Epoch 28/300, seasonal_3 Loss: 0.1188 | 0.0740
Epoch 29/300, seasonal_3 Loss: 0.1273 | 0.0644
Epoch 30/300, seasonal_3 Loss: 0.1084 | 0.0722
Epoch 31/300, seasonal_3 Loss: 0.1593 | 0.1483
Epoch 32/300, seasonal_3 Loss: 0.2086 | 0.1098
Epoch 33/300, seasonal_3 Loss: 0.2487 | 0.1593
Epoch 34/300, seasonal_3 Loss: 0.1813 | 0.1799
Epoch 35/300, seasonal_3 Loss: 0.1735 | 0.3109
Epoch 36/300, seasonal_3 Loss: 0.1712 | 0.1475
Epoch 37/300, seasonal_3 Loss: 0.1420 | 0.0868
Epoch 38/300, seasonal_3 Loss: 0.1203 | 0.0691
Epoch 39/300, seasonal_3 Loss: 0.1027 | 0.0644
Epoch 40/300, seasonal_3 Loss: 0.0950 | 0.0637
Epoch 41/300, seasonal_3 Loss: 0.0880 | 0.0523
Epoch 42/300, seasonal_3 Loss: 0.0861 | 0.0510
Epoch 43/300, seasonal_3 Loss: 0.0837 | 0.0495
Epoch 44/300, seasonal_3 Loss: 0.0846 | 0.0513
Epoch 45/300, seasonal_3 Loss: 0.0844 | 0.0506
Epoch 46/300, seasonal_3 Loss: 0.0826 | 0.0503
Epoch 47/300, seasonal_3 Loss: 0.0864 | 0.0482
Epoch 48/300, seasonal_3 Loss: 0.0904 | 0.0472
Epoch 49/300, seasonal_3 Loss: 0.0840 | 0.0509
Epoch 50/300, seasonal_3 Loss: 0.0795 | 0.0462
Epoch 51/300, seasonal_3 Loss: 0.0822 | 0.0456
Epoch 52/300, seasonal_3 Loss: 0.0869 | 0.0487
Epoch 53/300, seasonal_3 Loss: 0.0800 | 0.0435
Epoch 54/300, seasonal_3 Loss: 0.0791 | 0.0453
Epoch 55/300, seasonal_3 Loss: 0.0751 | 0.0436
Epoch 56/300, seasonal_3 Loss: 0.0745 | 0.0419
Epoch 57/300, seasonal_3 Loss: 0.0735 | 0.0385
Epoch 58/300, seasonal_3 Loss: 0.0728 | 0.0395
Epoch 59/300, seasonal_3 Loss: 0.0719 | 0.0395
Epoch 60/300, seasonal_3 Loss: 0.0715 | 0.0393
Epoch 61/300, seasonal_3 Loss: 0.0712 | 0.0371
Epoch 62/300, seasonal_3 Loss: 0.0708 | 0.0375
Epoch 63/300, seasonal_3 Loss: 0.0703 | 0.0374
Epoch 64/300, seasonal_3 Loss: 0.0697 | 0.0373
Epoch 65/300, seasonal_3 Loss: 0.0694 | 0.0368
Epoch 66/300, seasonal_3 Loss: 0.0692 | 0.0370
Epoch 67/300, seasonal_3 Loss: 0.0691 | 0.0365
Epoch 68/300, seasonal_3 Loss: 0.0688 | 0.0358
Epoch 69/300, seasonal_3 Loss: 0.0686 | 0.0358
Epoch 70/300, seasonal_3 Loss: 0.0694 | 0.0358
Epoch 71/300, seasonal_3 Loss: 0.0703 | 0.0356
Epoch 72/300, seasonal_3 Loss: 0.0687 | 0.0370
Epoch 73/300, seasonal_3 Loss: 0.0692 | 0.0356
Epoch 74/300, seasonal_3 Loss: 0.0684 | 0.0347
Epoch 75/300, seasonal_3 Loss: 0.0677 | 0.0344
Epoch 76/300, seasonal_3 Loss: 0.0676 | 0.0348
Epoch 77/300, seasonal_3 Loss: 0.0671 | 0.0346
Epoch 78/300, seasonal_3 Loss: 0.0671 | 0.0343
Epoch 79/300, seasonal_3 Loss: 0.0668 | 0.0340
Epoch 80/300, seasonal_3 Loss: 0.0667 | 0.0340
Epoch 81/300, seasonal_3 Loss: 0.0665 | 0.0340
Epoch 82/300, seasonal_3 Loss: 0.0664 | 0.0339
Epoch 83/300, seasonal_3 Loss: 0.0663 | 0.0337
Epoch 84/300, seasonal_3 Loss: 0.0662 | 0.0336
Epoch 85/300, seasonal_3 Loss: 0.0661 | 0.0336
Epoch 86/300, seasonal_3 Loss: 0.0660 | 0.0335
Epoch 87/300, seasonal_3 Loss: 0.0659 | 0.0335
Epoch 88/300, seasonal_3 Loss: 0.0658 | 0.0334
Epoch 89/300, seasonal_3 Loss: 0.0657 | 0.0333
Epoch 90/300, seasonal_3 Loss: 0.0657 | 0.0333
Epoch 91/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 92/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 93/300, seasonal_3 Loss: 0.0654 | 0.0331
Epoch 94/300, seasonal_3 Loss: 0.0654 | 0.0331
Epoch 95/300, seasonal_3 Loss: 0.0653 | 0.0330
Epoch 96/300, seasonal_3 Loss: 0.0653 | 0.0330
Epoch 97/300, seasonal_3 Loss: 0.0652 | 0.0330
Epoch 98/300, seasonal_3 Loss: 0.0651 | 0.0329
Epoch 99/300, seasonal_3 Loss: 0.0651 | 0.0329
Epoch 100/300, seasonal_3 Loss: 0.0650 | 0.0329
Epoch 101/300, seasonal_3 Loss: 0.0650 | 0.0328
Epoch 102/300, seasonal_3 Loss: 0.0649 | 0.0328
Epoch 103/300, seasonal_3 Loss: 0.0649 | 0.0328
Epoch 104/300, seasonal_3 Loss: 0.0649 | 0.0327
Epoch 105/300, seasonal_3 Loss: 0.0648 | 0.0327
Epoch 106/300, seasonal_3 Loss: 0.0648 | 0.0327
Epoch 107/300, seasonal_3 Loss: 0.0647 | 0.0326
Epoch 108/300, seasonal_3 Loss: 0.0647 | 0.0326
Epoch 109/300, seasonal_3 Loss: 0.0646 | 0.0326
Epoch 110/300, seasonal_3 Loss: 0.0646 | 0.0326
Epoch 111/300, seasonal_3 Loss: 0.0646 | 0.0325
Epoch 112/300, seasonal_3 Loss: 0.0645 | 0.0325
Epoch 113/300, seasonal_3 Loss: 0.0645 | 0.0325
Epoch 114/300, seasonal_3 Loss: 0.0645 | 0.0325
Epoch 115/300, seasonal_3 Loss: 0.0645 | 0.0325
Epoch 116/300, seasonal_3 Loss: 0.0644 | 0.0324
Epoch 117/300, seasonal_3 Loss: 0.0644 | 0.0324
Epoch 118/300, seasonal_3 Loss: 0.0644 | 0.0324
Epoch 119/300, seasonal_3 Loss: 0.0643 | 0.0324
Epoch 120/300, seasonal_3 Loss: 0.0643 | 0.0324
Epoch 121/300, seasonal_3 Loss: 0.0643 | 0.0323
Epoch 122/300, seasonal_3 Loss: 0.0643 | 0.0323
Epoch 123/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 124/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 125/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 126/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 127/300, seasonal_3 Loss: 0.0642 | 0.0323
Epoch 128/300, seasonal_3 Loss: 0.0641 | 0.0323
Epoch 129/300, seasonal_3 Loss: 0.0641 | 0.0322
Epoch 130/300, seasonal_3 Loss: 0.0641 | 0.0322
Epoch 131/300, seasonal_3 Loss: 0.0641 | 0.0322
Epoch 132/300, seasonal_3 Loss: 0.0641 | 0.0322
Epoch 133/300, seasonal_3 Loss: 0.0641 | 0.0322
Epoch 134/300, seasonal_3 Loss: 0.0640 | 0.0322
Epoch 135/300, seasonal_3 Loss: 0.0640 | 0.0322
Epoch 136/300, seasonal_3 Loss: 0.0640 | 0.0322
Epoch 137/300, seasonal_3 Loss: 0.0640 | 0.0322
Epoch 138/300, seasonal_3 Loss: 0.0640 | 0.0322
Epoch 139/300, seasonal_3 Loss: 0.0640 | 0.0321
Epoch 140/300, seasonal_3 Loss: 0.0640 | 0.0321
Epoch 141/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 142/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 143/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 144/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 145/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 146/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 147/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 148/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 149/300, seasonal_3 Loss: 0.0639 | 0.0321
Epoch 150/300, seasonal_3 Loss: 0.0638 | 0.0321
Epoch 151/300, seasonal_3 Loss: 0.0638 | 0.0321
Epoch 152/300, seasonal_3 Loss: 0.0638 | 0.0321
Epoch 153/300, seasonal_3 Loss: 0.0638 | 0.0321
Epoch 154/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 155/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 156/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 157/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 158/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 159/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 160/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 161/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 162/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 163/300, seasonal_3 Loss: 0.0638 | 0.0320
Epoch 164/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 165/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 166/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 167/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 168/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 169/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 170/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 171/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 172/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 173/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 174/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 175/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 176/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 177/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 178/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 179/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 180/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 181/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 182/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 183/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 184/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 185/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 186/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 187/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 188/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 189/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 190/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 191/300, seasonal_3 Loss: 0.0637 | 0.0320
Epoch 192/300, seasonal_3 Loss: 0.0637 | 0.0319
Epoch 193/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 194/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 195/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 196/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 197/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 198/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 199/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 200/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 201/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 202/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 203/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 204/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 205/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 206/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 207/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 208/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 209/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 210/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 211/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 212/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 213/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 214/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 215/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 216/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 217/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 218/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 219/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 220/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 221/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 222/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 223/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 224/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 225/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 226/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 227/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 228/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 229/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 230/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 231/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 232/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 233/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 234/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 235/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 236/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 237/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 238/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 239/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 240/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 241/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 242/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 243/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 244/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 245/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 246/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 247/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 248/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 249/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 250/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 251/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 252/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 253/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 254/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 255/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 256/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 257/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 258/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 259/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 260/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 261/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 262/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 263/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 264/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 265/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 266/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 267/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 268/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 269/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 270/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 271/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 272/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 273/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 274/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 275/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 276/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 277/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 278/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 279/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 280/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 281/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 282/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 283/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 284/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 285/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 286/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 287/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 288/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 289/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 290/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 291/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 292/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 293/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 294/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 295/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 296/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 297/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 298/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 299/300, seasonal_3 Loss: 0.0636 | 0.0319
Epoch 300/300, seasonal_3 Loss: 0.0636 | 0.0319
Training resid component with params: {'observation_period_num': 9, 'train_rates': 0.8805480719989272, 'learning_rate': 0.0009820552546436058, 'batch_size': 182, 'step_size': 15, 'gamma': 0.7974631091789224}
Epoch 1/300, resid Loss: 1.7174 | 1.6870
Epoch 2/300, resid Loss: 1.1280 | 1.6277
Epoch 3/300, resid Loss: 1.1202 | 1.6451
Epoch 4/300, resid Loss: 1.4759 | 1.8270
Epoch 5/300, resid Loss: 1.1955 | 2.2738
Epoch 6/300, resid Loss: 0.9152 | 2.0493
Epoch 7/300, resid Loss: 0.9500 | 2.1156
Epoch 8/300, resid Loss: 0.9376 | 2.1005
Epoch 9/300, resid Loss: 0.9143 | 2.1144
Epoch 10/300, resid Loss: 0.9093 | 2.1090
Epoch 11/300, resid Loss: 0.9120 | 2.1110
Epoch 12/300, resid Loss: 0.9112 | 2.1105
Epoch 13/300, resid Loss: 0.9113 | 2.1109
Epoch 14/300, resid Loss: 0.9108 | 2.1112
Epoch 15/300, resid Loss: 0.9102 | 2.1115
Epoch 16/300, resid Loss: 0.8946 | 2.1137
Epoch 17/300, resid Loss: 0.8933 | 2.1193
Epoch 18/300, resid Loss: 0.8925 | 2.1143
Epoch 19/300, resid Loss: 0.8936 | 2.1173
Epoch 20/300, resid Loss: 0.8929 | 2.1160
Epoch 21/300, resid Loss: 0.8931 | 2.1166
Epoch 22/300, resid Loss: 0.8927 | 2.1165
Epoch 23/300, resid Loss: 0.8925 | 2.1167
Epoch 24/300, resid Loss: 0.8816 | 2.1155
Epoch 25/300, resid Loss: 0.8818 | 2.1224
Epoch 26/300, resid Loss: 0.8805 | 2.1181
Epoch 27/300, resid Loss: 0.8814 | 2.1195
Epoch 28/300, resid Loss: 0.8811 | 2.1195
Epoch 29/300, resid Loss: 0.8811 | 2.1192
Epoch 30/300, resid Loss: 0.8811 | 2.1194
Epoch 31/300, resid Loss: 0.8732 | 2.1174
Epoch 32/300, resid Loss: 0.8738 | 2.1229
Epoch 33/300, resid Loss: 0.8729 | 2.1208
Epoch 34/300, resid Loss: 0.8733 | 2.1206
Epoch 35/300, resid Loss: 0.8734 | 2.1211
Epoch 36/300, resid Loss: 0.8733 | 2.1209
Epoch 37/300, resid Loss: 0.8734 | 2.1209
Epoch 38/300, resid Loss: 0.8734 | 2.1210
Epoch 39/300, resid Loss: 0.8675 | 2.1190
Epoch 40/300, resid Loss: 0.8680 | 2.1228
Epoch 41/300, resid Loss: 0.8675 | 2.1223
Epoch 42/300, resid Loss: 0.8676 | 2.1216
Epoch 43/300, resid Loss: 0.8678 | 2.1219
Epoch 44/300, resid Loss: 0.8678 | 2.1220
Epoch 45/300, resid Loss: 0.8678 | 2.1219
Epoch 46/300, resid Loss: 0.8633 | 2.1203
Epoch 47/300, resid Loss: 0.8637 | 2.1228
Epoch 48/300, resid Loss: 0.8635 | 2.1230
Epoch 49/300, resid Loss: 0.8634 | 2.1225
Epoch 50/300, resid Loss: 0.8636 | 2.1224
Epoch 51/300, resid Loss: 0.8636 | 2.1225
Epoch 52/300, resid Loss: 0.8637 | 2.1225
Epoch 53/300, resid Loss: 0.8637 | 2.1225
Epoch 54/300, resid Loss: 0.8602 | 2.1213
Epoch 55/300, resid Loss: 0.8605 | 2.1228
Epoch 56/300, resid Loss: 0.8604 | 2.1233
Epoch 57/300, resid Loss: 0.8604 | 2.1230
Epoch 58/300, resid Loss: 0.8604 | 2.1229
Epoch 59/300, resid Loss: 0.8605 | 2.1229
Epoch 60/300, resid Loss: 0.8605 | 2.1230
Epoch 61/300, resid Loss: 0.8578 | 2.1220
Epoch 62/300, resid Loss: 0.8580 | 2.1230
Epoch 63/300, resid Loss: 0.8580 | 2.1234
Epoch 64/300, resid Loss: 0.8579 | 2.1234
Epoch 65/300, resid Loss: 0.8580 | 2.1232
Epoch 66/300, resid Loss: 0.8580 | 2.1232
Epoch 67/300, resid Loss: 0.8581 | 2.1232
Epoch 68/300, resid Loss: 0.8581 | 2.1232
Epoch 69/300, resid Loss: 0.8560 | 2.1226
Epoch 70/300, resid Loss: 0.8561 | 2.1231
Epoch 71/300, resid Loss: 0.8561 | 2.1235
Epoch 72/300, resid Loss: 0.8561 | 2.1235
Epoch 73/300, resid Loss: 0.8561 | 2.1235
Epoch 74/300, resid Loss: 0.8561 | 2.1234
Epoch 75/300, resid Loss: 0.8562 | 2.1234
Epoch 76/300, resid Loss: 0.8544 | 2.1230
Epoch 77/300, resid Loss: 0.8545 | 2.1233
Epoch 78/300, resid Loss: 0.8545 | 2.1236
Epoch 79/300, resid Loss: 0.8545 | 2.1236
Epoch 80/300, resid Loss: 0.8546 | 2.1236
Epoch 81/300, resid Loss: 0.8546 | 2.1236
Epoch 82/300, resid Loss: 0.8546 | 2.1236
Epoch 83/300, resid Loss: 0.8546 | 2.1236
Epoch 84/300, resid Loss: 0.8533 | 2.1233
Epoch 85/300, resid Loss: 0.8533 | 2.1234
Epoch 86/300, resid Loss: 0.8533 | 2.1236
Epoch 87/300, resid Loss: 0.8533 | 2.1237
Epoch 88/300, resid Loss: 0.8533 | 2.1237
Epoch 89/300, resid Loss: 0.8534 | 2.1237
Epoch 90/300, resid Loss: 0.8534 | 2.1237
Epoch 91/300, resid Loss: 0.8523 | 2.1235
Epoch 92/300, resid Loss: 0.8523 | 2.1236
Epoch 93/300, resid Loss: 0.8523 | 2.1237
Epoch 94/300, resid Loss: 0.8523 | 2.1238
Epoch 95/300, resid Loss: 0.8524 | 2.1238
Epoch 96/300, resid Loss: 0.8524 | 2.1238
Epoch 97/300, resid Loss: 0.8524 | 2.1238
Epoch 98/300, resid Loss: 0.8524 | 2.1238
Epoch 99/300, resid Loss: 0.8515 | 2.1236
Epoch 100/300, resid Loss: 0.8515 | 2.1237
Epoch 101/300, resid Loss: 0.8515 | 2.1238
Epoch 102/300, resid Loss: 0.8516 | 2.1238
Epoch 103/300, resid Loss: 0.8516 | 2.1238
Epoch 104/300, resid Loss: 0.8516 | 2.1238
Epoch 105/300, resid Loss: 0.8516 | 2.1239
Epoch 106/300, resid Loss: 0.8509 | 2.1237
Epoch 107/300, resid Loss: 0.8509 | 2.1238
Epoch 108/300, resid Loss: 0.8509 | 2.1238
Epoch 109/300, resid Loss: 0.8509 | 2.1239
Epoch 110/300, resid Loss: 0.8509 | 2.1239
Epoch 111/300, resid Loss: 0.8509 | 2.1239
Epoch 112/300, resid Loss: 0.8509 | 2.1239
Epoch 113/300, resid Loss: 0.8509 | 2.1239
Epoch 114/300, resid Loss: 0.8504 | 2.1238
Epoch 115/300, resid Loss: 0.8504 | 2.1238
Epoch 116/300, resid Loss: 0.8504 | 2.1239
Epoch 117/300, resid Loss: 0.8504 | 2.1239
Epoch 118/300, resid Loss: 0.8504 | 2.1239
Epoch 119/300, resid Loss: 0.8504 | 2.1239
Epoch 120/300, resid Loss: 0.8504 | 2.1239
Epoch 121/300, resid Loss: 0.8499 | 2.1239
Epoch 122/300, resid Loss: 0.8499 | 2.1239
Epoch 123/300, resid Loss: 0.8499 | 2.1239
Epoch 124/300, resid Loss: 0.8499 | 2.1239
Epoch 125/300, resid Loss: 0.8500 | 2.1239
Epoch 126/300, resid Loss: 0.8500 | 2.1239
Epoch 127/300, resid Loss: 0.8500 | 2.1240
Epoch 128/300, resid Loss: 0.8500 | 2.1240
Epoch 129/300, resid Loss: 0.8496 | 2.1239
Epoch 130/300, resid Loss: 0.8496 | 2.1239
Epoch 131/300, resid Loss: 0.8496 | 2.1239
Epoch 132/300, resid Loss: 0.8496 | 2.1240
Epoch 133/300, resid Loss: 0.8496 | 2.1240
Epoch 134/300, resid Loss: 0.8496 | 2.1240
Epoch 135/300, resid Loss: 0.8496 | 2.1240
Epoch 136/300, resid Loss: 0.8493 | 2.1240
Epoch 137/300, resid Loss: 0.8493 | 2.1240
Epoch 138/300, resid Loss: 0.8493 | 2.1240
Epoch 139/300, resid Loss: 0.8493 | 2.1240
Epoch 140/300, resid Loss: 0.8493 | 2.1240
Epoch 141/300, resid Loss: 0.8493 | 2.1240
Epoch 142/300, resid Loss: 0.8493 | 2.1240
Epoch 143/300, resid Loss: 0.8493 | 2.1240
Epoch 144/300, resid Loss: 0.8491 | 2.1240
Epoch 145/300, resid Loss: 0.8491 | 2.1240
Epoch 146/300, resid Loss: 0.8491 | 2.1240
Epoch 147/300, resid Loss: 0.8491 | 2.1240
Epoch 148/300, resid Loss: 0.8491 | 2.1240
Epoch 149/300, resid Loss: 0.8491 | 2.1240
Epoch 150/300, resid Loss: 0.8491 | 2.1240
Epoch 151/300, resid Loss: 0.8489 | 2.1240
Epoch 152/300, resid Loss: 0.8489 | 2.1240
Epoch 153/300, resid Loss: 0.8489 | 2.1240
Epoch 154/300, resid Loss: 0.8489 | 2.1240
Epoch 155/300, resid Loss: 0.8489 | 2.1240
Epoch 156/300, resid Loss: 0.8489 | 2.1240
Epoch 157/300, resid Loss: 0.8489 | 2.1240
Epoch 158/300, resid Loss: 0.8489 | 2.1240
Epoch 159/300, resid Loss: 0.8488 | 2.1240
Epoch 160/300, resid Loss: 0.8488 | 2.1240
Epoch 161/300, resid Loss: 0.8488 | 2.1240
Epoch 162/300, resid Loss: 0.8488 | 2.1240
Epoch 163/300, resid Loss: 0.8488 | 2.1240
Epoch 164/300, resid Loss: 0.8488 | 2.1240
Epoch 165/300, resid Loss: 0.8488 | 2.1240
Epoch 166/300, resid Loss: 0.8486 | 2.1240
Epoch 167/300, resid Loss: 0.8486 | 2.1240
Epoch 168/300, resid Loss: 0.8486 | 2.1240
Epoch 169/300, resid Loss: 0.8486 | 2.1240
Epoch 170/300, resid Loss: 0.8486 | 2.1240
Epoch 171/300, resid Loss: 0.8486 | 2.1240
Epoch 172/300, resid Loss: 0.8486 | 2.1240
Epoch 173/300, resid Loss: 0.8486 | 2.1240
Epoch 174/300, resid Loss: 0.8485 | 2.1240
Epoch 175/300, resid Loss: 0.8485 | 2.1240
Epoch 176/300, resid Loss: 0.8485 | 2.1240
Epoch 177/300, resid Loss: 0.8485 | 2.1240
Epoch 178/300, resid Loss: 0.8485 | 2.1240
Epoch 179/300, resid Loss: 0.8485 | 2.1240
Epoch 180/300, resid Loss: 0.8485 | 2.1240
Epoch 181/300, resid Loss: 0.8485 | 2.1240
Epoch 182/300, resid Loss: 0.8485 | 2.1240
Epoch 183/300, resid Loss: 0.8485 | 2.1240
Epoch 184/300, resid Loss: 0.8485 | 2.1240
Epoch 185/300, resid Loss: 0.8485 | 2.1240
Epoch 186/300, resid Loss: 0.8485 | 2.1240
Epoch 187/300, resid Loss: 0.8485 | 2.1240
Epoch 188/300, resid Loss: 0.8485 | 2.1240
Epoch 189/300, resid Loss: 0.8484 | 2.1240
Epoch 190/300, resid Loss: 0.8484 | 2.1240
Epoch 191/300, resid Loss: 0.8484 | 2.1240
Epoch 192/300, resid Loss: 0.8484 | 2.1240
Epoch 193/300, resid Loss: 0.8484 | 2.1240
Epoch 194/300, resid Loss: 0.8484 | 2.1241
Epoch 195/300, resid Loss: 0.8484 | 2.1241
Epoch 196/300, resid Loss: 0.8483 | 2.1241
Epoch 197/300, resid Loss: 0.8483 | 2.1241
Epoch 198/300, resid Loss: 0.8483 | 2.1241
Epoch 199/300, resid Loss: 0.8483 | 2.1241
Epoch 200/300, resid Loss: 0.8483 | 2.1241
Epoch 201/300, resid Loss: 0.8483 | 2.1241
Epoch 202/300, resid Loss: 0.8483 | 2.1241
Epoch 203/300, resid Loss: 0.8483 | 2.1241
Epoch 204/300, resid Loss: 0.8483 | 2.1241
Epoch 205/300, resid Loss: 0.8483 | 2.1241
Epoch 206/300, resid Loss: 0.8483 | 2.1241
Epoch 207/300, resid Loss: 0.8483 | 2.1241
Epoch 208/300, resid Loss: 0.8483 | 2.1241
Epoch 209/300, resid Loss: 0.8483 | 2.1241
Epoch 210/300, resid Loss: 0.8483 | 2.1241
Epoch 211/300, resid Loss: 0.8483 | 2.1241
Early stopping for resid
Runtime (seconds): 11953.574466228485
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[204.60037]
[1.3661708]
[0.30723876]
[5.05452]
[-0.54849076]
[-2.7670174]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 731.5516062909737
RMSE: 27.047210693359375
MAE: 27.047210693359375
R-squared: nan
[208.01279]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
