ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-01 02:03:03,697][0m A new study created in memory with name: no-name-e43bf218-a7c2-465f-9115-81407e0ddca2[0m
[32m[I 2025-02-01 02:03:34,035][0m Trial 0 finished with value: 0.27433425188064575 and parameters: {'observation_period_num': 45, 'train_rates': 0.9368187161465671, 'learning_rate': 1.8364727718580514e-05, 'batch_size': 208, 'step_size': 4, 'gamma': 0.9621579117063707}. Best is trial 0 with value: 0.27433425188064575.[0m
[32m[I 2025-02-01 02:04:11,202][0m Trial 1 finished with value: 0.1610188960867959 and parameters: {'observation_period_num': 24, 'train_rates': 0.6844354192613026, 'learning_rate': 1.2353129808486529e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.8844945887944713}. Best is trial 1 with value: 0.1610188960867959.[0m
[32m[I 2025-02-01 02:04:47,300][0m Trial 2 finished with value: 0.6704135245084762 and parameters: {'observation_period_num': 145, 'train_rates': 0.9290950320348349, 'learning_rate': 2.116015470461021e-06, 'batch_size': 163, 'step_size': 9, 'gamma': 0.9502805456519448}. Best is trial 1 with value: 0.1610188960867959.[0m
[32m[I 2025-02-01 02:05:43,525][0m Trial 3 finished with value: 0.21532635390758514 and parameters: {'observation_period_num': 176, 'train_rates': 0.9842274086715355, 'learning_rate': 8.545447939638314e-06, 'batch_size': 103, 'step_size': 15, 'gamma': 0.914577913933238}. Best is trial 1 with value: 0.1610188960867959.[0m
[32m[I 2025-02-01 02:06:29,792][0m Trial 4 finished with value: 0.2145672646335891 and parameters: {'observation_period_num': 222, 'train_rates': 0.8214374791520287, 'learning_rate': 2.600239388185522e-05, 'batch_size': 108, 'step_size': 13, 'gamma': 0.8948836746022245}. Best is trial 1 with value: 0.1610188960867959.[0m
[32m[I 2025-02-01 02:07:09,520][0m Trial 5 finished with value: 0.09820084588287713 and parameters: {'observation_period_num': 16, 'train_rates': 0.8430597168817493, 'learning_rate': 0.00015218359433094502, 'batch_size': 138, 'step_size': 13, 'gamma': 0.8706655942777017}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:09:01,470][0m Trial 6 finished with value: 0.21113947074071995 and parameters: {'observation_period_num': 156, 'train_rates': 0.9141487363416897, 'learning_rate': 0.00028844351068015695, 'batch_size': 48, 'step_size': 5, 'gamma': 0.8549514987385037}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:09:28,744][0m Trial 7 finished with value: 0.284984612289597 and parameters: {'observation_period_num': 192, 'train_rates': 0.8217861014948886, 'learning_rate': 0.00039433782724201114, 'batch_size': 203, 'step_size': 1, 'gamma': 0.8909408618135735}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:09:55,009][0m Trial 8 finished with value: 0.4638505937222161 and parameters: {'observation_period_num': 97, 'train_rates': 0.6777843558864556, 'learning_rate': 9.237418972357369e-06, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9016465032355345}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:10:31,240][0m Trial 9 finished with value: 0.11244863334901725 and parameters: {'observation_period_num': 31, 'train_rates': 0.7298694750016246, 'learning_rate': 8.098082778629387e-05, 'batch_size': 139, 'step_size': 3, 'gamma': 0.8866192689702115}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:10:52,754][0m Trial 10 finished with value: 0.14002259122608043 and parameters: {'observation_period_num': 93, 'train_rates': 0.7561878023871303, 'learning_rate': 0.00010544743812263256, 'batch_size': 256, 'step_size': 10, 'gamma': 0.7840438060996098}. Best is trial 5 with value: 0.09820084588287713.[0m
Early stopping at epoch 93
[32m[I 2025-02-01 02:12:04,853][0m Trial 11 finished with value: 0.12558713724667375 and parameters: {'observation_period_num': 12, 'train_rates': 0.7571369632392076, 'learning_rate': 0.00010182066419856323, 'batch_size': 65, 'step_size': 1, 'gamma': 0.8235198932905355}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:12:55,365][0m Trial 12 finished with value: 0.12123938196547195 and parameters: {'observation_period_num': 63, 'train_rates': 0.6001072741621405, 'learning_rate': 0.0008596918249506579, 'batch_size': 83, 'step_size': 11, 'gamma': 0.8385700052197034}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:13:34,583][0m Trial 13 finished with value: 0.16124741251652058 and parameters: {'observation_period_num': 70, 'train_rates': 0.8650220723168255, 'learning_rate': 8.652356341958816e-05, 'batch_size': 144, 'step_size': 7, 'gamma': 0.7543119703308376}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:14:11,545][0m Trial 14 finished with value: 0.13834290302625285 and parameters: {'observation_period_num': 112, 'train_rates': 0.7005545891850856, 'learning_rate': 5.308944919251291e-05, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9397826794114479}. Best is trial 5 with value: 0.09820084588287713.[0m
[32m[I 2025-02-01 02:17:45,133][0m Trial 15 finished with value: 0.08073368038361271 and parameters: {'observation_period_num': 8, 'train_rates': 0.7658813168773879, 'learning_rate': 0.0002402169779171259, 'batch_size': 23, 'step_size': 3, 'gamma': 0.9856503291909308}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:22:26,266][0m Trial 16 finished with value: 0.10438789288993135 and parameters: {'observation_period_num': 11, 'train_rates': 0.8657066228135337, 'learning_rate': 0.0002831011522729012, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9786923531062083}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:27:35,679][0m Trial 17 finished with value: 0.10427193133644824 and parameters: {'observation_period_num': 60, 'train_rates': 0.8021946812169896, 'learning_rate': 0.0009232997254047859, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8029394440030942}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:29:18,852][0m Trial 18 finished with value: 0.19752442910045873 and parameters: {'observation_period_num': 243, 'train_rates': 0.8650007679915114, 'learning_rate': 0.000177910501589453, 'batch_size': 48, 'step_size': 8, 'gamma': 0.98457970741054}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:30:12,465][0m Trial 19 finished with value: 0.09105118456552094 and parameters: {'observation_period_num': 6, 'train_rates': 0.7664271067819776, 'learning_rate': 4.241995202025575e-05, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9336441689149134}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:31:04,457][0m Trial 20 finished with value: 0.5565247398243058 and parameters: {'observation_period_num': 82, 'train_rates': 0.6319584587541112, 'learning_rate': 1.326000462439519e-06, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9270775478389311}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:31:51,083][0m Trial 21 finished with value: 0.10144502111629808 and parameters: {'observation_period_num': 38, 'train_rates': 0.7748351925271878, 'learning_rate': 3.203438945866789e-05, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9678791628590623}. Best is trial 15 with value: 0.08073368038361271.[0m
[32m[I 2025-02-01 02:33:22,203][0m Trial 22 finished with value: 0.07597047843406966 and parameters: {'observation_period_num': 15, 'train_rates': 0.7246649626499949, 'learning_rate': 0.00017873453404323186, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9317715815359745}. Best is trial 22 with value: 0.07597047843406966.[0m
[32m[I 2025-02-01 02:35:28,303][0m Trial 23 finished with value: 0.09767711359028616 and parameters: {'observation_period_num': 46, 'train_rates': 0.7173527395825974, 'learning_rate': 0.00045511600627483285, 'batch_size': 37, 'step_size': 15, 'gamma': 0.9327025141548638}. Best is trial 22 with value: 0.07597047843406966.[0m
[32m[I 2025-02-01 02:36:31,063][0m Trial 24 finished with value: 0.09778330987095259 and parameters: {'observation_period_num': 11, 'train_rates': 0.6470369531941529, 'learning_rate': 4.489323290053078e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.9477976514920188}. Best is trial 22 with value: 0.07597047843406966.[0m
[32m[I 2025-02-01 02:38:47,121][0m Trial 25 finished with value: 0.06683555719846825 and parameters: {'observation_period_num': 5, 'train_rates': 0.730754482777284, 'learning_rate': 0.00018107474368800645, 'batch_size': 35, 'step_size': 11, 'gamma': 0.9142211247507744}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:40:59,260][0m Trial 26 finished with value: 0.10560172398916891 and parameters: {'observation_period_num': 48, 'train_rates': 0.73143283291549, 'learning_rate': 0.0005545122494812089, 'batch_size': 36, 'step_size': 14, 'gamma': 0.9133592690828789}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:42:11,164][0m Trial 27 finished with value: 0.09005075363600218 and parameters: {'observation_period_num': 30, 'train_rates': 0.665458455596242, 'learning_rate': 0.0001741148394698815, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9876807715304172}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:44:32,661][0m Trial 28 finished with value: 0.155829019844532 and parameters: {'observation_period_num': 134, 'train_rates': 0.7892717688158909, 'learning_rate': 0.00026138923581409823, 'batch_size': 34, 'step_size': 3, 'gamma': 0.955545522336815}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:46:00,136][0m Trial 29 finished with value: 0.12184897577169382 and parameters: {'observation_period_num': 51, 'train_rates': 0.7093545660795303, 'learning_rate': 1.9101243327772657e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9686194595762211}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:48:58,456][0m Trial 30 finished with value: 0.12719863489633654 and parameters: {'observation_period_num': 118, 'train_rates': 0.7398252785809362, 'learning_rate': 0.0006526136430588253, 'batch_size': 26, 'step_size': 12, 'gamma': 0.9129927089043413}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:50:07,564][0m Trial 31 finished with value: 0.08835613351406121 and parameters: {'observation_period_num': 29, 'train_rates': 0.6496213625572456, 'learning_rate': 0.0001830623697637581, 'batch_size': 65, 'step_size': 9, 'gamma': 0.9673762726549724}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:51:04,672][0m Trial 32 finished with value: 0.07840661756895684 and parameters: {'observation_period_num': 29, 'train_rates': 0.6205976289923, 'learning_rate': 0.00019021847260537492, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9662904994521078}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:52:00,059][0m Trial 33 finished with value: 0.08636672864306158 and parameters: {'observation_period_num': 24, 'train_rates': 0.6050131359120898, 'learning_rate': 0.0003461681243297723, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9488500192674413}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:53:54,345][0m Trial 34 finished with value: 0.06776436354094892 and parameters: {'observation_period_num': 5, 'train_rates': 0.7011585247096614, 'learning_rate': 6.620200980034996e-05, 'batch_size': 41, 'step_size': 14, 'gamma': 0.9239417004247918}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:55:25,493][0m Trial 35 finished with value: 0.10044111411305184 and parameters: {'observation_period_num': 74, 'train_rates': 0.6736954547629369, 'learning_rate': 6.873874043375372e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.8688699132799671}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:56:04,966][0m Trial 36 finished with value: 0.08692500500355736 and parameters: {'observation_period_num': 38, 'train_rates': 0.6967686088116973, 'learning_rate': 0.00012584057518862514, 'batch_size': 120, 'step_size': 15, 'gamma': 0.9168917820706055}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:56:57,138][0m Trial 37 finished with value: 0.14379946477505004 and parameters: {'observation_period_num': 23, 'train_rates': 0.6304142322723126, 'learning_rate': 6.186006493659943e-05, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9057646302110093}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:58:42,478][0m Trial 38 finished with value: 0.147573606204035 and parameters: {'observation_period_num': 54, 'train_rates': 0.6889965476659469, 'learning_rate': 5.947799196415142e-06, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9205154978763199}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 02:59:52,739][0m Trial 39 finished with value: 0.1654414105678222 and parameters: {'observation_period_num': 173, 'train_rates': 0.6250617216655192, 'learning_rate': 3.0315439313400955e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.87646737057425}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 03:00:41,729][0m Trial 40 finished with value: 0.17690992215048335 and parameters: {'observation_period_num': 22, 'train_rates': 0.6560506586164436, 'learning_rate': 4.700435050748406e-06, 'batch_size': 94, 'step_size': 14, 'gamma': 0.9435862865593391}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 03:03:33,708][0m Trial 41 finished with value: 0.07902711408524045 and parameters: {'observation_period_num': 10, 'train_rates': 0.7439296816127821, 'learning_rate': 0.00023588595890944135, 'batch_size': 28, 'step_size': 15, 'gamma': 0.9604945138383825}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 03:06:18,205][0m Trial 42 finished with value: 0.09783057369019438 and parameters: {'observation_period_num': 38, 'train_rates': 0.737421142710538, 'learning_rate': 0.00013350743161885393, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9576318861115899}. Best is trial 25 with value: 0.06683555719846825.[0m
[32m[I 2025-02-01 03:08:44,651][0m Trial 43 finished with value: 0.050932540877224645 and parameters: {'observation_period_num': 19, 'train_rates': 0.9752609385794304, 'learning_rate': 0.00021714884423369222, 'batch_size': 40, 'step_size': 14, 'gamma': 0.9309374131420991}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:10:07,286][0m Trial 44 finished with value: 0.2206746765831173 and parameters: {'observation_period_num': 21, 'train_rates': 0.9492172792945321, 'learning_rate': 0.00011610517047867137, 'batch_size': 70, 'step_size': 13, 'gamma': 0.8989241032906318}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:12:20,486][0m Trial 45 finished with value: 0.12418012638041313 and parameters: {'observation_period_num': 5, 'train_rates': 0.904653505647742, 'learning_rate': 0.00041263333729252657, 'batch_size': 42, 'step_size': 11, 'gamma': 0.926406103821203}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:12:49,145][0m Trial 46 finished with value: 0.31862789392471313 and parameters: {'observation_period_num': 35, 'train_rates': 0.9637607770138003, 'learning_rate': 8.197846914424878e-05, 'batch_size': 234, 'step_size': 7, 'gamma': 0.8820771458778469}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:14:10,213][0m Trial 47 finished with value: 0.17344011885504568 and parameters: {'observation_period_num': 209, 'train_rates': 0.7178021650887387, 'learning_rate': 0.00019241253834319076, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9408265046914807}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:14:42,560][0m Trial 48 finished with value: 0.13782154154480056 and parameters: {'observation_period_num': 22, 'train_rates': 0.8157570274218684, 'learning_rate': 1.9288716535648627e-05, 'batch_size': 172, 'step_size': 10, 'gamma': 0.9055044483242309}. Best is trial 43 with value: 0.050932540877224645.[0m
[32m[I 2025-02-01 03:15:52,434][0m Trial 49 finished with value: 0.10038580134705463 and parameters: {'observation_period_num': 63, 'train_rates': 0.7880316351847229, 'learning_rate': 0.0003310803244120524, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8609228523719908}. Best is trial 43 with value: 0.050932540877224645.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-01 03:15:52,445][0m A new study created in memory with name: no-name-14160301-4d4d-4611-8936-02fec57ac6c8[0m
[32m[I 2025-02-01 03:17:42,674][0m Trial 0 finished with value: 0.21282512701778528 and parameters: {'observation_period_num': 188, 'train_rates': 0.850187679812993, 'learning_rate': 1.1013739067993728e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8287841332147985}. Best is trial 0 with value: 0.21282512701778528.[0m
[32m[I 2025-02-01 03:18:11,140][0m Trial 1 finished with value: 0.1153736504852709 and parameters: {'observation_period_num': 53, 'train_rates': 0.673055825601294, 'learning_rate': 0.0006615778720390183, 'batch_size': 172, 'step_size': 12, 'gamma': 0.809134009846404}. Best is trial 1 with value: 0.1153736504852709.[0m
Early stopping at epoch 73
[32m[I 2025-02-01 03:18:41,339][0m Trial 2 finished with value: 0.745223479906565 and parameters: {'observation_period_num': 107, 'train_rates': 0.6351451878953909, 'learning_rate': 2.319869710937225e-06, 'batch_size': 111, 'step_size': 2, 'gamma': 0.762212191324657}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:20:20,923][0m Trial 3 finished with value: 0.23379325506671164 and parameters: {'observation_period_num': 249, 'train_rates': 0.7798337538399297, 'learning_rate': 1.440285788233079e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.8260273799272343}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:20:48,238][0m Trial 4 finished with value: 0.16790936131712417 and parameters: {'observation_period_num': 105, 'train_rates': 0.7759683598339434, 'learning_rate': 4.122366900250867e-05, 'batch_size': 185, 'step_size': 10, 'gamma': 0.8865031622370478}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:21:28,924][0m Trial 5 finished with value: 0.13456291205396778 and parameters: {'observation_period_num': 132, 'train_rates': 0.7578106909646994, 'learning_rate': 0.0004939155640442278, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8978787699872921}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:22:11,003][0m Trial 6 finished with value: 0.4824221406442424 and parameters: {'observation_period_num': 237, 'train_rates': 0.647017605216244, 'learning_rate': 1.148652661723897e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.7576062339656056}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:22:33,647][0m Trial 7 finished with value: 0.11542496016167787 and parameters: {'observation_period_num': 81, 'train_rates': 0.6091923501406438, 'learning_rate': 0.0009612161996642332, 'batch_size': 212, 'step_size': 4, 'gamma': 0.8075863640594853}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:23:02,832][0m Trial 8 finished with value: 0.20628238557414574 and parameters: {'observation_period_num': 164, 'train_rates': 0.8424673196501057, 'learning_rate': 2.2735388508381897e-05, 'batch_size': 189, 'step_size': 14, 'gamma': 0.989455680902128}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:23:28,443][0m Trial 9 finished with value: 0.7665085306592808 and parameters: {'observation_period_num': 89, 'train_rates': 0.6712881547163894, 'learning_rate': 1.3610646988148555e-06, 'batch_size': 192, 'step_size': 6, 'gamma': 0.7623927176013882}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:23:52,342][0m Trial 10 finished with value: 0.21147893369197845 and parameters: {'observation_period_num': 7, 'train_rates': 0.9435682176200269, 'learning_rate': 0.00015670453850671038, 'batch_size': 256, 'step_size': 9, 'gamma': 0.9401035075255569}. Best is trial 1 with value: 0.1153736504852709.[0m
Early stopping at epoch 77
[32m[I 2025-02-01 03:24:08,323][0m Trial 11 finished with value: 0.19390567889457935 and parameters: {'observation_period_num': 39, 'train_rates': 0.6019908525067553, 'learning_rate': 0.0009613606261166353, 'batch_size': 239, 'step_size': 1, 'gamma': 0.8167833182416652}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:24:38,948][0m Trial 12 finished with value: 0.12002994946031337 and parameters: {'observation_period_num': 57, 'train_rates': 0.7044070547089931, 'learning_rate': 0.00020042105521776135, 'batch_size': 166, 'step_size': 4, 'gamma': 0.8011124106748138}. Best is trial 1 with value: 0.1153736504852709.[0m
[32m[I 2025-02-01 03:25:02,585][0m Trial 13 finished with value: 0.10736447953219924 and parameters: {'observation_period_num': 58, 'train_rates': 0.7101494317294205, 'learning_rate': 0.00022360584344752177, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8484322865480044}. Best is trial 13 with value: 0.10736447953219924.[0m
[32m[I 2025-02-01 03:25:36,646][0m Trial 14 finished with value: 0.08405698504909484 and parameters: {'observation_period_num': 8, 'train_rates': 0.7142096672928562, 'learning_rate': 0.00012394529492444552, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8542033043067276}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:26:11,618][0m Trial 15 finished with value: 0.09856824959896825 and parameters: {'observation_period_num': 27, 'train_rates': 0.7216505684913421, 'learning_rate': 7.951559950682648e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.8624859763139074}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:27:22,451][0m Trial 16 finished with value: 0.10334283141361204 and parameters: {'observation_period_num': 6, 'train_rates': 0.8369548228402908, 'learning_rate': 6.932634212553417e-05, 'batch_size': 76, 'step_size': 12, 'gamma': 0.9170159644488914}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:27:56,589][0m Trial 17 finished with value: 0.10809510493651033 and parameters: {'observation_period_num': 27, 'train_rates': 0.7268161332591062, 'learning_rate': 7.657074809373413e-05, 'batch_size': 147, 'step_size': 8, 'gamma': 0.857702944378544}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:28:36,031][0m Trial 18 finished with value: 0.24691231152223003 and parameters: {'observation_period_num': 27, 'train_rates': 0.8944274220534523, 'learning_rate': 5.8884094140242614e-06, 'batch_size': 149, 'step_size': 13, 'gamma': 0.946897733287662}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:29:27,556][0m Trial 19 finished with value: 0.1376894111811773 and parameters: {'observation_period_num': 167, 'train_rates': 0.7498705403053724, 'learning_rate': 9.16773682566211e-05, 'batch_size': 91, 'step_size': 10, 'gamma': 0.873033916774011}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:30:07,165][0m Trial 20 finished with value: 0.11182263043573064 and parameters: {'observation_period_num': 71, 'train_rates': 0.8148623206321107, 'learning_rate': 0.00031452764537401355, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8472606815870515}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:31:23,451][0m Trial 21 finished with value: 0.12234237392361348 and parameters: {'observation_period_num': 8, 'train_rates': 0.8899078598823261, 'learning_rate': 5.804596419699438e-05, 'batch_size': 73, 'step_size': 12, 'gamma': 0.9040928893313036}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:36:43,347][0m Trial 22 finished with value: 0.08535089850149773 and parameters: {'observation_period_num': 5, 'train_rates': 0.8169203682934484, 'learning_rate': 0.0001234336862321609, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9220457097637006}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:39:15,298][0m Trial 23 finished with value: 0.09214334405874412 and parameters: {'observation_period_num': 33, 'train_rates': 0.7276680162590233, 'learning_rate': 0.0001204356528189272, 'batch_size': 31, 'step_size': 11, 'gamma': 0.9360044692005656}. Best is trial 14 with value: 0.08405698504909484.[0m
[32m[I 2025-02-01 03:43:52,426][0m Trial 24 finished with value: 0.03584699705243111 and parameters: {'observation_period_num': 37, 'train_rates': 0.9843570676206204, 'learning_rate': 0.00012641439405510662, 'batch_size': 21, 'step_size': 11, 'gamma': 0.9622630081690576}. Best is trial 24 with value: 0.03584699705243111.[0m
[32m[I 2025-02-01 03:47:29,010][0m Trial 25 finished with value: 0.034770054885974296 and parameters: {'observation_period_num': 42, 'train_rates': 0.9869342496226307, 'learning_rate': 0.0003550424028657619, 'batch_size': 27, 'step_size': 14, 'gamma': 0.9772806955025298}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 03:49:09,299][0m Trial 26 finished with value: 0.3261492537955443 and parameters: {'observation_period_num': 43, 'train_rates': 0.9704241390607986, 'learning_rate': 0.0004358167314224708, 'batch_size': 58, 'step_size': 15, 'gamma': 0.9869673533864459}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 03:54:48,451][0m Trial 27 finished with value: 0.06541428215256552 and parameters: {'observation_period_num': 72, 'train_rates': 0.9861174482013193, 'learning_rate': 3.418416614874991e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.959330205333907}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:00:42,435][0m Trial 28 finished with value: 0.08378430569962579 and parameters: {'observation_period_num': 102, 'train_rates': 0.9829498875200459, 'learning_rate': 2.9465426780463834e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9633298817732066}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:03:01,957][0m Trial 29 finished with value: 0.23984168502299683 and parameters: {'observation_period_num': 80, 'train_rates': 0.9361651248815627, 'learning_rate': 0.0002888794496417579, 'batch_size': 40, 'step_size': 15, 'gamma': 0.9654050612189611}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:04:30,687][0m Trial 30 finished with value: 0.20538337676017457 and parameters: {'observation_period_num': 134, 'train_rates': 0.90695160141563, 'learning_rate': 4.149700396238817e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9676303482833375}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:08:02,862][0m Trial 31 finished with value: 0.08957956663586876 and parameters: {'observation_period_num': 111, 'train_rates': 0.9886784806376252, 'learning_rate': 2.17968803984992e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.9670384092736533}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:11:49,026][0m Trial 32 finished with value: 0.28486192420246154 and parameters: {'observation_period_num': 66, 'train_rates': 0.9577430937821402, 'learning_rate': 6.317446590691579e-06, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9507513834014558}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:17:23,711][0m Trial 33 finished with value: 0.09449642434202392 and parameters: {'observation_period_num': 92, 'train_rates': 0.9899307038621704, 'learning_rate': 3.067101587624386e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9742931664628656}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:19:21,505][0m Trial 34 finished with value: 0.2328133473521498 and parameters: {'observation_period_num': 118, 'train_rates': 0.9229265427141489, 'learning_rate': 1.901483293489694e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.959072259370596}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:20:54,034][0m Trial 35 finished with value: 0.18550417803065933 and parameters: {'observation_period_num': 49, 'train_rates': 0.8733382984869639, 'learning_rate': 6.059024467901708e-06, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9300478831906599}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:23:31,263][0m Trial 36 finished with value: 0.3400914444751346 and parameters: {'observation_period_num': 97, 'train_rates': 0.966112183223415, 'learning_rate': 3.296450474289046e-05, 'batch_size': 36, 'step_size': 11, 'gamma': 0.9787029172030426}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:24:40,904][0m Trial 37 finished with value: 0.18939050981034972 and parameters: {'observation_period_num': 69, 'train_rates': 0.9242400516727108, 'learning_rate': 0.0005452918041319992, 'batch_size': 83, 'step_size': 13, 'gamma': 0.9484627552033842}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:26:35,332][0m Trial 38 finished with value: 0.30469617177182295 and parameters: {'observation_period_num': 214, 'train_rates': 0.9536886980763237, 'learning_rate': 5.005121160138068e-05, 'batch_size': 47, 'step_size': 15, 'gamma': 0.9110335001254282}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:27:29,912][0m Trial 39 finished with value: 0.24932235479354858 and parameters: {'observation_period_num': 146, 'train_rates': 0.9763212059171772, 'learning_rate': 9.051568961445683e-06, 'batch_size': 107, 'step_size': 14, 'gamma': 0.8884167350877938}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:32:53,788][0m Trial 40 finished with value: 0.16151434418615918 and parameters: {'observation_period_num': 81, 'train_rates': 0.8726002679546782, 'learning_rate': 3.581779944548172e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9802291966646617}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:35:37,175][0m Trial 41 finished with value: 0.20407498713869315 and parameters: {'observation_period_num': 49, 'train_rates': 0.9331063808036205, 'learning_rate': 0.00034667765743784693, 'batch_size': 34, 'step_size': 12, 'gamma': 0.9566266417806129}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:36:15,840][0m Trial 42 finished with value: 0.062246039509773254 and parameters: {'observation_period_num': 22, 'train_rates': 0.9764989948414398, 'learning_rate': 0.00011006191364892094, 'batch_size': 161, 'step_size': 13, 'gamma': 0.832916768826642}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:37:04,105][0m Trial 43 finished with value: 0.20630926504097086 and parameters: {'observation_period_num': 17, 'train_rates': 0.952236802086141, 'learning_rate': 0.00021384486332353048, 'batch_size': 122, 'step_size': 13, 'gamma': 0.7913155914876802}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:37:42,469][0m Trial 44 finished with value: 0.16935914754867554 and parameters: {'observation_period_num': 61, 'train_rates': 0.9747561336183282, 'learning_rate': 1.454140670364448e-05, 'batch_size': 161, 'step_size': 14, 'gamma': 0.9294250801156748}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:38:14,945][0m Trial 45 finished with value: 0.20893825120650805 and parameters: {'observation_period_num': 39, 'train_rates': 0.9110432802200602, 'learning_rate': 0.000691437376391582, 'batch_size': 178, 'step_size': 15, 'gamma': 0.9740839392197215}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:40:01,320][0m Trial 46 finished with value: 0.06259914487600327 and parameters: {'observation_period_num': 20, 'train_rates': 0.9864622597086887, 'learning_rate': 3.17354267236985e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8336738927563545}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:40:33,391][0m Trial 47 finished with value: 0.21307829022407532 and parameters: {'observation_period_num': 17, 'train_rates': 0.9450533636118638, 'learning_rate': 0.0001654043905946258, 'batch_size': 198, 'step_size': 11, 'gamma': 0.8338611915911305}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:42:37,787][0m Trial 48 finished with value: 0.24420648385166077 and parameters: {'observation_period_num': 19, 'train_rates': 0.9613212223620264, 'learning_rate': 9.653521319024242e-05, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8371834222930982}. Best is trial 25 with value: 0.034770054885974296.[0m
[32m[I 2025-02-01 04:44:08,021][0m Trial 49 finished with value: 0.3158973954109983 and parameters: {'observation_period_num': 47, 'train_rates': 0.9698246450304463, 'learning_rate': 5.603059527330919e-05, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8215119583642261}. Best is trial 25 with value: 0.034770054885974296.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-01 04:44:08,031][0m A new study created in memory with name: no-name-28b5f5e3-905f-4a5f-bdc2-8c4c2f5fa051[0m
[32m[I 2025-02-01 04:44:29,616][0m Trial 0 finished with value: 0.41716674947545596 and parameters: {'observation_period_num': 19, 'train_rates': 0.7476913584101335, 'learning_rate': 3.150133691020356e-06, 'batch_size': 256, 'step_size': 8, 'gamma': 0.7849207218576147}. Best is trial 0 with value: 0.41716674947545596.[0m
[32m[I 2025-02-01 04:45:00,012][0m Trial 1 finished with value: 0.5613841255096448 and parameters: {'observation_period_num': 37, 'train_rates': 0.6842304023654693, 'learning_rate': 1.868166169070949e-06, 'batch_size': 161, 'step_size': 15, 'gamma': 0.8487809575007357}. Best is trial 0 with value: 0.41716674947545596.[0m
[32m[I 2025-02-01 04:45:28,951][0m Trial 2 finished with value: 0.45465679122851443 and parameters: {'observation_period_num': 235, 'train_rates': 0.923745516531861, 'learning_rate': 1.650947667459947e-05, 'batch_size': 200, 'step_size': 7, 'gamma': 0.7678374641978388}. Best is trial 0 with value: 0.41716674947545596.[0m
[32m[I 2025-02-01 04:45:51,099][0m Trial 3 finished with value: 0.10355301886796951 and parameters: {'observation_period_num': 48, 'train_rates': 0.6818561028907064, 'learning_rate': 6.0263206466856235e-05, 'batch_size': 222, 'step_size': 10, 'gamma': 0.9665837237961488}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:47:08,907][0m Trial 4 finished with value: 0.17615576163499352 and parameters: {'observation_period_num': 244, 'train_rates': 0.6594440972416349, 'learning_rate': 0.00033610917778508306, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8602483926243336}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:47:31,288][0m Trial 5 finished with value: 0.18259241178954447 and parameters: {'observation_period_num': 103, 'train_rates': 0.6436629401033876, 'learning_rate': 0.00012959283877349311, 'batch_size': 215, 'step_size': 4, 'gamma': 0.7527747758167062}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:47:57,506][0m Trial 6 finished with value: 0.1720277705802362 and parameters: {'observation_period_num': 231, 'train_rates': 0.6817441667747538, 'learning_rate': 0.00016306981686070793, 'batch_size': 181, 'step_size': 12, 'gamma': 0.870029787568759}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:51:38,995][0m Trial 7 finished with value: 0.29410845369471117 and parameters: {'observation_period_num': 124, 'train_rates': 0.8010939297958459, 'learning_rate': 1.3474383140405151e-06, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8881314762916022}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:54:52,058][0m Trial 8 finished with value: 0.19459517317062075 and parameters: {'observation_period_num': 144, 'train_rates': 0.8866260130104027, 'learning_rate': 0.00042943441944726335, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9600659067670522}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:56:03,226][0m Trial 9 finished with value: 0.4408676808445968 and parameters: {'observation_period_num': 83, 'train_rates': 0.7547957388973953, 'learning_rate': 1.2393582002503869e-06, 'batch_size': 69, 'step_size': 14, 'gamma': 0.9131458482330792}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:56:39,589][0m Trial 10 finished with value: 0.2041574628786607 and parameters: {'observation_period_num': 172, 'train_rates': 0.6047651562529828, 'learning_rate': 2.349127209229044e-05, 'batch_size': 116, 'step_size': 11, 'gamma': 0.9776394693398412}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:57:07,823][0m Trial 11 finished with value: 0.20034749078608693 and parameters: {'observation_period_num': 205, 'train_rates': 0.72499559986771, 'learning_rate': 8.540424144186331e-05, 'batch_size': 171, 'step_size': 11, 'gamma': 0.932118993297016}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:57:31,154][0m Trial 12 finished with value: 0.12427094725289327 and parameters: {'observation_period_num': 50, 'train_rates': 0.8129006955171729, 'learning_rate': 8.192788721671687e-05, 'batch_size': 237, 'step_size': 11, 'gamma': 0.816562731506839}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:57:53,568][0m Trial 13 finished with value: 0.15008822562550203 and parameters: {'observation_period_num': 70, 'train_rates': 0.8340151405657888, 'learning_rate': 4.9390306253692395e-05, 'batch_size': 246, 'step_size': 10, 'gamma': 0.8211022220513059}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:58:37,718][0m Trial 14 finished with value: 0.4956975258835899 and parameters: {'observation_period_num': 52, 'train_rates': 0.8490162801443962, 'learning_rate': 1.094897344633348e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8122045010722458}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:59:05,912][0m Trial 15 finished with value: 0.491141676902771 and parameters: {'observation_period_num': 9, 'train_rates': 0.9723509535299489, 'learning_rate': 6.962334169334538e-06, 'batch_size': 224, 'step_size': 9, 'gamma': 0.8179508655808397}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:59:29,326][0m Trial 16 finished with value: 0.12334634574494414 and parameters: {'observation_period_num': 54, 'train_rates': 0.7833183211011882, 'learning_rate': 4.884238519405044e-05, 'batch_size': 231, 'step_size': 12, 'gamma': 0.9361919657778954}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 04:59:55,613][0m Trial 17 finished with value: 0.12558728290125004 and parameters: {'observation_period_num': 93, 'train_rates': 0.7121077339503507, 'learning_rate': 0.0009835464248700832, 'batch_size': 200, 'step_size': 13, 'gamma': 0.9438687105484942}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 05:00:27,892][0m Trial 18 finished with value: 0.1853390649253688 and parameters: {'observation_period_num': 122, 'train_rates': 0.7662810159255778, 'learning_rate': 4.284921628176657e-05, 'batch_size': 159, 'step_size': 5, 'gamma': 0.9852619716336233}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 05:01:09,233][0m Trial 19 finished with value: 0.263539276285605 and parameters: {'observation_period_num': 66, 'train_rates': 0.6193074105359964, 'learning_rate': 6.330164543946599e-06, 'batch_size': 108, 'step_size': 8, 'gamma': 0.9033305051209609}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 05:01:37,586][0m Trial 20 finished with value: 0.1469869370327184 and parameters: {'observation_period_num': 33, 'train_rates': 0.8699927163235391, 'learning_rate': 3.098971175858696e-05, 'batch_size': 197, 'step_size': 9, 'gamma': 0.9332056357956053}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 05:02:00,782][0m Trial 21 finished with value: 0.11262494483369558 and parameters: {'observation_period_num': 50, 'train_rates': 0.8000898727131152, 'learning_rate': 7.625775774239128e-05, 'batch_size': 233, 'step_size': 12, 'gamma': 0.958264399697326}. Best is trial 3 with value: 0.10355301886796951.[0m
[32m[I 2025-02-01 05:02:26,113][0m Trial 22 finished with value: 0.08339499772288078 and parameters: {'observation_period_num': 5, 'train_rates': 0.7814238694750743, 'learning_rate': 0.0002080502883131267, 'batch_size': 226, 'step_size': 12, 'gamma': 0.9545598436790846}. Best is trial 22 with value: 0.08339499772288078.[0m
[32m[I 2025-02-01 05:03:01,161][0m Trial 23 finished with value: 0.07466501174979127 and parameters: {'observation_period_num': 8, 'train_rates': 0.7064255653966977, 'learning_rate': 0.00023275756609382662, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9622713260095364}. Best is trial 23 with value: 0.07466501174979127.[0m
[32m[I 2025-02-01 05:03:51,061][0m Trial 24 finished with value: 0.06917466920503466 and parameters: {'observation_period_num': 6, 'train_rates': 0.7170999839631446, 'learning_rate': 0.0002859583237110737, 'batch_size': 100, 'step_size': 13, 'gamma': 0.9685512088252466}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:04:45,194][0m Trial 25 finished with value: 0.07396036702060083 and parameters: {'observation_period_num': 5, 'train_rates': 0.7251997106075952, 'learning_rate': 0.000299515949269793, 'batch_size': 92, 'step_size': 13, 'gamma': 0.987910058909193}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:05:37,442][0m Trial 26 finished with value: 0.08660901702658452 and parameters: {'observation_period_num': 24, 'train_rates': 0.720764741583977, 'learning_rate': 0.0006553590800411376, 'batch_size': 92, 'step_size': 14, 'gamma': 0.986072433163201}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:06:13,997][0m Trial 27 finished with value: 0.08924264548009891 and parameters: {'observation_period_num': 21, 'train_rates': 0.7322555387232212, 'learning_rate': 0.0003052737113513389, 'batch_size': 142, 'step_size': 13, 'gamma': 0.9130672556444176}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:07:08,581][0m Trial 28 finished with value: 0.07232780683568507 and parameters: {'observation_period_num': 7, 'train_rates': 0.6976208302385369, 'learning_rate': 0.0005686322299486727, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9701545520601706}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:08:02,576][0m Trial 29 finished with value: 0.08533496614436657 and parameters: {'observation_period_num': 35, 'train_rates': 0.6486123847943713, 'learning_rate': 0.0006210783405551694, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9736737096954851}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:09:36,642][0m Trial 30 finished with value: 0.09519659873226594 and parameters: {'observation_period_num': 23, 'train_rates': 0.7528358172464841, 'learning_rate': 0.0005569578700139735, 'batch_size': 52, 'step_size': 9, 'gamma': 0.9437221630345232}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:10:26,359][0m Trial 31 finished with value: 0.06970078528705817 and parameters: {'observation_period_num': 6, 'train_rates': 0.7048676936222962, 'learning_rate': 0.00027219569354767234, 'batch_size': 97, 'step_size': 13, 'gamma': 0.9883528218019078}. Best is trial 24 with value: 0.06917466920503466.[0m
[32m[I 2025-02-01 05:11:16,374][0m Trial 32 finished with value: 0.06525366316032079 and parameters: {'observation_period_num': 5, 'train_rates': 0.694960824811071, 'learning_rate': 0.00012363174461713704, 'batch_size': 97, 'step_size': 14, 'gamma': 0.9854710800963754}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:12:02,650][0m Trial 33 finished with value: 0.09372766836417973 and parameters: {'observation_period_num': 33, 'train_rates': 0.6873433814305715, 'learning_rate': 0.0001249384508363693, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9896686276071646}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:13:04,966][0m Trial 34 finished with value: 0.10668197398515539 and parameters: {'observation_period_num': 71, 'train_rates': 0.671628060145393, 'learning_rate': 0.00018130596948092673, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9703457428366158}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:14:35,342][0m Trial 35 finished with value: 0.07988025161326212 and parameters: {'observation_period_num': 20, 'train_rates': 0.6384179445105461, 'learning_rate': 0.0009203371614358085, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9214229448623306}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:15:12,664][0m Trial 36 finished with value: 0.08436117267334148 and parameters: {'observation_period_num': 39, 'train_rates': 0.6874445871016907, 'learning_rate': 0.00011412321397814483, 'batch_size': 126, 'step_size': 14, 'gamma': 0.9498211107835076}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:16:15,582][0m Trial 37 finished with value: 0.144828873177488 and parameters: {'observation_period_num': 161, 'train_rates': 0.7003106769853185, 'learning_rate': 0.00043031595000943415, 'batch_size': 71, 'step_size': 15, 'gamma': 0.9719184258076787}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:17:01,851][0m Trial 38 finished with value: 0.09870563073814097 and parameters: {'observation_period_num': 16, 'train_rates': 0.6640604847293649, 'learning_rate': 0.0002501072672106954, 'batch_size': 102, 'step_size': 1, 'gamma': 0.8896714085828432}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:18:57,774][0m Trial 39 finished with value: 0.1014368271338908 and parameters: {'observation_period_num': 39, 'train_rates': 0.7416848878651316, 'learning_rate': 0.00036606793179397046, 'batch_size': 41, 'step_size': 7, 'gamma': 0.9746352967429047}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:19:53,959][0m Trial 40 finished with value: 0.12962828779730096 and parameters: {'observation_period_num': 101, 'train_rates': 0.633162025237553, 'learning_rate': 0.00012255321747088856, 'batch_size': 77, 'step_size': 13, 'gamma': 0.8461189015310098}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:20:46,182][0m Trial 41 finished with value: 0.06741550037773644 and parameters: {'observation_period_num': 5, 'train_rates': 0.6944494462201644, 'learning_rate': 0.000285912521771803, 'batch_size': 92, 'step_size': 13, 'gamma': 0.9873006627089981}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:22:00,794][0m Trial 42 finished with value: 0.08825809011856715 and parameters: {'observation_period_num': 18, 'train_rates': 0.6927217961672356, 'learning_rate': 0.00046466631939533687, 'batch_size': 62, 'step_size': 14, 'gamma': 0.9636707789552893}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:22:36,342][0m Trial 43 finished with value: 0.07870521308225993 and parameters: {'observation_period_num': 26, 'train_rates': 0.6539715177834702, 'learning_rate': 0.00016505530966179948, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9775146062383356}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:23:27,486][0m Trial 44 finished with value: 0.07672240772698809 and parameters: {'observation_period_num': 6, 'train_rates': 0.671614202318544, 'learning_rate': 0.0007430191541886111, 'batch_size': 91, 'step_size': 12, 'gamma': 0.948888171356755}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:24:09,687][0m Trial 45 finished with value: 0.14837529849129832 and parameters: {'observation_period_num': 204, 'train_rates': 0.7627078593506463, 'learning_rate': 0.00027319571391924745, 'batch_size': 112, 'step_size': 13, 'gamma': 0.7802587901157585}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:25:07,334][0m Trial 46 finished with value: 0.11093394840210281 and parameters: {'observation_period_num': 63, 'train_rates': 0.7394573792910496, 'learning_rate': 0.0004686216249666017, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9790239147207684}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:27:27,732][0m Trial 47 finished with value: 0.09046112954789816 and parameters: {'observation_period_num': 43, 'train_rates': 0.7046478009582178, 'learning_rate': 0.00015481964811106118, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9591061756556273}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:28:06,220][0m Trial 48 finished with value: 0.07856733131140109 and parameters: {'observation_period_num': 15, 'train_rates': 0.6075603927182082, 'learning_rate': 8.809481706478863e-05, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9898563117630307}. Best is trial 32 with value: 0.06525366316032079.[0m
[32m[I 2025-02-01 05:29:19,140][0m Trial 49 finished with value: 0.09701740737048743 and parameters: {'observation_period_num': 28, 'train_rates': 0.6738955629427041, 'learning_rate': 0.0003882668705987749, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9659759672743478}. Best is trial 32 with value: 0.06525366316032079.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-01 05:29:19,150][0m A new study created in memory with name: no-name-14431c07-b2b6-487a-a78a-d9bad7b17d1f[0m
[32m[I 2025-02-01 05:29:47,369][0m Trial 0 finished with value: 0.09040138418207297 and parameters: {'observation_period_num': 50, 'train_rates': 0.6816650879708739, 'learning_rate': 0.0005232291881657118, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8866425393893321}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:31:51,178][0m Trial 1 finished with value: 0.31855831879216273 and parameters: {'observation_period_num': 243, 'train_rates': 0.6042398107167761, 'learning_rate': 5.186265264709822e-06, 'batch_size': 31, 'step_size': 10, 'gamma': 0.8812443738853564}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:32:36,528][0m Trial 2 finished with value: 0.21003021573806557 and parameters: {'observation_period_num': 104, 'train_rates': 0.9219076990299664, 'learning_rate': 0.0007562647707340498, 'batch_size': 129, 'step_size': 14, 'gamma': 0.8970801141861217}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:33:22,825][0m Trial 3 finished with value: 0.23080228927302665 and parameters: {'observation_period_num': 232, 'train_rates': 0.7984349645050012, 'learning_rate': 0.0006203470524425191, 'batch_size': 107, 'step_size': 14, 'gamma': 0.9191153037639929}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:33:44,283][0m Trial 4 finished with value: 0.15324965931017306 and parameters: {'observation_period_num': 98, 'train_rates': 0.7496494861023179, 'learning_rate': 9.270609234685028e-05, 'batch_size': 254, 'step_size': 11, 'gamma': 0.768309306183095}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:34:07,668][0m Trial 5 finished with value: 0.13715529545248495 and parameters: {'observation_period_num': 42, 'train_rates': 0.7823560563017222, 'learning_rate': 4.07050263180625e-05, 'batch_size': 241, 'step_size': 14, 'gamma': 0.8254257548972961}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:35:29,501][0m Trial 6 finished with value: 0.09119010086547821 and parameters: {'observation_period_num': 48, 'train_rates': 0.7089112042250998, 'learning_rate': 0.000512696641731665, 'batch_size': 57, 'step_size': 6, 'gamma': 0.79204942436095}. Best is trial 0 with value: 0.09040138418207297.[0m
Early stopping at epoch 88
[32m[I 2025-02-01 05:36:06,888][0m Trial 7 finished with value: 0.12623287429754002 and parameters: {'observation_period_num': 52, 'train_rates': 0.6896967758721682, 'learning_rate': 0.0005222660426116362, 'batch_size': 114, 'step_size': 1, 'gamma': 0.8406163228350166}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:36:35,361][0m Trial 8 finished with value: 0.8509752154350281 and parameters: {'observation_period_num': 141, 'train_rates': 0.9595331767028497, 'learning_rate': 2.871469192459105e-06, 'batch_size': 219, 'step_size': 5, 'gamma': 0.8982434132561586}. Best is trial 0 with value: 0.09040138418207297.[0m
Early stopping at epoch 64
[32m[I 2025-02-01 05:36:54,844][0m Trial 9 finished with value: 1.1949888187025504 and parameters: {'observation_period_num': 117, 'train_rates': 0.6350300981292211, 'learning_rate': 6.952524832710087e-06, 'batch_size': 160, 'step_size': 1, 'gamma': 0.8318521514364727}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:37:26,082][0m Trial 10 finished with value: 0.09941796603656951 and parameters: {'observation_period_num': 10, 'train_rates': 0.8362295249791992, 'learning_rate': 0.0001471088585792072, 'batch_size': 184, 'step_size': 7, 'gamma': 0.9735230644656749}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:38:58,423][0m Trial 11 finished with value: 0.09469268841390636 and parameters: {'observation_period_num': 62, 'train_rates': 0.6992093124424091, 'learning_rate': 0.00018874065430764958, 'batch_size': 50, 'step_size': 5, 'gamma': 0.7892848747518231}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:39:57,744][0m Trial 12 finished with value: 0.20325505310712857 and parameters: {'observation_period_num': 158, 'train_rates': 0.6924625616281468, 'learning_rate': 1.9090582086519656e-05, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9436268229921523}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:40:24,037][0m Trial 13 finished with value: 0.10671060162788641 and parameters: {'observation_period_num': 9, 'train_rates': 0.6611596752073092, 'learning_rate': 0.00020372823041940185, 'batch_size': 187, 'step_size': 4, 'gamma': 0.7563045356022783}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:41:20,684][0m Trial 14 finished with value: 0.125262065248175 and parameters: {'observation_period_num': 79, 'train_rates': 0.7468218274021797, 'learning_rate': 5.029884017728247e-05, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8545764390277408}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:41:55,879][0m Trial 15 finished with value: 0.15457791059559634 and parameters: {'observation_period_num': 177, 'train_rates': 0.86815669383893, 'learning_rate': 0.00034702241480230177, 'batch_size': 156, 'step_size': 11, 'gamma': 0.7993529452822584}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:45:41,199][0m Trial 16 finished with value: 0.09431182501415156 and parameters: {'observation_period_num': 35, 'train_rates': 0.7343216773317737, 'learning_rate': 0.0008663975103072866, 'batch_size': 21, 'step_size': 4, 'gamma': 0.8676906984439857}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:46:03,895][0m Trial 17 finished with value: 0.854548044003436 and parameters: {'observation_period_num': 74, 'train_rates': 0.6012770491491043, 'learning_rate': 1.1810267450840891e-06, 'batch_size': 197, 'step_size': 8, 'gamma': 0.7975860467950235}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:47:11,210][0m Trial 18 finished with value: 0.20776273208375304 and parameters: {'observation_period_num': 185, 'train_rates': 0.6514884274353226, 'learning_rate': 1.9558379152411874e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.9391339729561371}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:47:44,041][0m Trial 19 finished with value: 0.14313639855974322 and parameters: {'observation_period_num': 32, 'train_rates': 0.7198161689106638, 'learning_rate': 8.038269710374043e-05, 'batch_size': 159, 'step_size': 3, 'gamma': 0.8215457150067434}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:48:22,337][0m Trial 20 finished with value: 0.11713490348216916 and parameters: {'observation_period_num': 86, 'train_rates': 0.7892241940377362, 'learning_rate': 0.0003146473705304685, 'batch_size': 138, 'step_size': 12, 'gamma': 0.9747112514951162}. Best is trial 0 with value: 0.09040138418207297.[0m
[32m[I 2025-02-01 05:53:02,046][0m Trial 21 finished with value: 0.0885275241265434 and parameters: {'observation_period_num': 28, 'train_rates': 0.7459091202029356, 'learning_rate': 0.0009536477266830747, 'batch_size': 17, 'step_size': 3, 'gamma': 0.8645806730419676}. Best is trial 21 with value: 0.0885275241265434.[0m
[32m[I 2025-02-01 05:54:53,794][0m Trial 22 finished with value: 0.08427678855124657 and parameters: {'observation_period_num': 29, 'train_rates': 0.6644626684817964, 'learning_rate': 0.0003742614391459365, 'batch_size': 40, 'step_size': 3, 'gamma': 0.8835072836775205}. Best is trial 22 with value: 0.08427678855124657.[0m
[32m[I 2025-02-01 05:56:46,899][0m Trial 23 finished with value: 0.08777350218672501 and parameters: {'observation_period_num': 22, 'train_rates': 0.676282469013651, 'learning_rate': 0.000989003539659539, 'batch_size': 40, 'step_size': 3, 'gamma': 0.8937863695028206}. Best is trial 22 with value: 0.08427678855124657.[0m
[32m[I 2025-02-01 06:01:00,691][0m Trial 24 finished with value: 0.0792518670930237 and parameters: {'observation_period_num': 22, 'train_rates': 0.6466419450906739, 'learning_rate': 0.0009449875970196547, 'batch_size': 17, 'step_size': 2, 'gamma': 0.91399178108622}. Best is trial 24 with value: 0.0792518670930237.[0m
[32m[I 2025-02-01 06:02:42,665][0m Trial 25 finished with value: 0.06028276238126964 and parameters: {'observation_period_num': 9, 'train_rates': 0.6318059419719969, 'learning_rate': 0.00030156500087220883, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9211940973894313}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:03:35,066][0m Trial 26 finished with value: 0.07015563836824883 and parameters: {'observation_period_num': 5, 'train_rates': 0.630148923712527, 'learning_rate': 0.0002964863252553971, 'batch_size': 88, 'step_size': 2, 'gamma': 0.9202462126794042}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:04:25,315][0m Trial 27 finished with value: 0.09551771151589918 and parameters: {'observation_period_num': 7, 'train_rates': 0.6233727707672753, 'learning_rate': 0.00023935142855223182, 'batch_size': 88, 'step_size': 1, 'gamma': 0.926129470069886}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:05:28,259][0m Trial 28 finished with value: 0.07254406360984948 and parameters: {'observation_period_num': 5, 'train_rates': 0.6327220962373892, 'learning_rate': 0.00010112986806341412, 'batch_size': 71, 'step_size': 2, 'gamma': 0.9599272161107469}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:06:11,620][0m Trial 29 finished with value: 0.1476175585111273 and parameters: {'observation_period_num': 60, 'train_rates': 0.6221574595223011, 'learning_rate': 0.00011110332970828839, 'batch_size': 104, 'step_size': 2, 'gamma': 0.9510829936650573}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:07:14,807][0m Trial 30 finished with value: 0.0775659553097053 and parameters: {'observation_period_num': 6, 'train_rates': 0.6268697655057452, 'learning_rate': 6.346195999794082e-05, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9606876190208112}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:08:20,601][0m Trial 31 finished with value: 0.07923176335085423 and parameters: {'observation_period_num': 7, 'train_rates': 0.6303529042840837, 'learning_rate': 5.9936385659465324e-05, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9575471653443735}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:09:11,037][0m Trial 32 finished with value: 0.12724274590343312 and parameters: {'observation_period_num': 6, 'train_rates': 0.6043828451826418, 'learning_rate': 2.3824420174755785e-05, 'batch_size': 87, 'step_size': 4, 'gamma': 0.9832005644442295}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:10:53,154][0m Trial 33 finished with value: 0.08567227479051163 and parameters: {'observation_period_num': 42, 'train_rates': 0.6680442725655252, 'learning_rate': 0.00012290552135524479, 'batch_size': 44, 'step_size': 2, 'gamma': 0.9599168034559091}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:11:50,307][0m Trial 34 finished with value: 0.12010052807839794 and parameters: {'observation_period_num': 19, 'train_rates': 0.6027005960383579, 'learning_rate': 6.875386645540841e-05, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9264280384234564}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:12:35,258][0m Trial 35 finished with value: 0.1864335958550616 and parameters: {'observation_period_num': 63, 'train_rates': 0.6456175480994847, 'learning_rate': 3.605887741981178e-05, 'batch_size': 100, 'step_size': 5, 'gamma': 0.9104819290231417}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:13:22,836][0m Trial 36 finished with value: 0.22999089698683708 and parameters: {'observation_period_num': 40, 'train_rates': 0.8713071679042153, 'learning_rate': 1.1985452597385224e-05, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9320831874133486}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:14:47,638][0m Trial 37 finished with value: 0.12045347748930196 and parameters: {'observation_period_num': 106, 'train_rates': 0.7670544309565264, 'learning_rate': 0.0001613560252557332, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9687659695744001}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:17:14,055][0m Trial 38 finished with value: 0.24222504824669827 and parameters: {'observation_period_num': 224, 'train_rates': 0.7171330359165825, 'learning_rate': 0.0002533029440545066, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9874098065840355}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:17:49,450][0m Trial 39 finished with value: 0.08698463503757943 and parameters: {'observation_period_num': 54, 'train_rates': 0.6781261567865557, 'learning_rate': 0.00047101077120614794, 'batch_size': 137, 'step_size': 15, 'gamma': 0.9447207812207379}. Best is trial 25 with value: 0.06028276238126964.[0m
[32m[I 2025-02-01 06:19:05,708][0m Trial 40 finished with value: 0.05637454241514206 and parameters: {'observation_period_num': 18, 'train_rates': 0.9893996669445926, 'learning_rate': 9.513649328880267e-05, 'batch_size': 79, 'step_size': 3, 'gamma': 0.906406551804169}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:20:20,254][0m Trial 41 finished with value: 0.24090247842669488 and parameters: {'observation_period_num': 21, 'train_rates': 0.9574647649125894, 'learning_rate': 9.58517112213782e-05, 'batch_size': 78, 'step_size': 3, 'gamma': 0.9076338819362333}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:21:50,277][0m Trial 42 finished with value: 0.17462583946493956 and parameters: {'observation_period_num': 17, 'train_rates': 0.9116679857738259, 'learning_rate': 4.766384268796316e-05, 'batch_size': 63, 'step_size': 2, 'gamma': 0.9018418463192444}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:22:34,839][0m Trial 43 finished with value: 0.12781462431984378 and parameters: {'observation_period_num': 45, 'train_rates': 0.8062576147000782, 'learning_rate': 0.0001298048055453492, 'batch_size': 123, 'step_size': 1, 'gamma': 0.933897097785545}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:23:23,138][0m Trial 44 finished with value: 0.06047584990316883 and parameters: {'observation_period_num': 5, 'train_rates': 0.6176879934996786, 'learning_rate': 0.0006245153180707074, 'batch_size': 92, 'step_size': 3, 'gamma': 0.9192577907120243}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:24:23,832][0m Trial 45 finished with value: 0.20993334483243747 and parameters: {'observation_period_num': 33, 'train_rates': 0.9428552222945613, 'learning_rate': 0.0006217903166412097, 'batch_size': 95, 'step_size': 5, 'gamma': 0.8862901830185034}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:25:02,552][0m Trial 46 finished with value: 0.1872855864118698 and parameters: {'observation_period_num': 251, 'train_rates': 0.618540578268874, 'learning_rate': 0.0004162568045507854, 'batch_size': 110, 'step_size': 6, 'gamma': 0.9201801442093362}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:26:21,307][0m Trial 47 finished with value: 0.14270554782230493 and parameters: {'observation_period_num': 129, 'train_rates': 0.6470903760229229, 'learning_rate': 0.0002744972618138749, 'batch_size': 55, 'step_size': 4, 'gamma': 0.8764498358105157}. Best is trial 40 with value: 0.05637454241514206.[0m
[32m[I 2025-02-01 06:29:25,840][0m Trial 48 finished with value: 0.03855673136080012 and parameters: {'observation_period_num': 16, 'train_rates': 0.9887139569176365, 'learning_rate': 0.0006761670769280813, 'batch_size': 32, 'step_size': 3, 'gamma': 0.8981349836288143}. Best is trial 48 with value: 0.03855673136080012.[0m
[32m[I 2025-02-01 06:32:05,995][0m Trial 49 finished with value: 0.03483516350388527 and parameters: {'observation_period_num': 18, 'train_rates': 0.9875570188288523, 'learning_rate': 0.0006171957612463179, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8926433335235231}. Best is trial 49 with value: 0.03483516350388527.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-01 06:32:06,005][0m A new study created in memory with name: no-name-fc22623b-6064-45f1-8f90-cefe545c0376[0m
Early stopping at epoch 45
[32m[I 2025-02-01 06:32:17,369][0m Trial 0 finished with value: 2.545069688661093 and parameters: {'observation_period_num': 62, 'train_rates': 0.7187809280372746, 'learning_rate': 1.37789876750063e-06, 'batch_size': 228, 'step_size': 1, 'gamma': 0.8106428263237411}. Best is trial 0 with value: 2.545069688661093.[0m
[32m[I 2025-02-01 06:33:17,912][0m Trial 1 finished with value: 0.162089836882784 and parameters: {'observation_period_num': 212, 'train_rates': 0.7087434975667406, 'learning_rate': 0.00011025628292125218, 'batch_size': 73, 'step_size': 15, 'gamma': 0.9493343441304096}. Best is trial 1 with value: 0.162089836882784.[0m
[32m[I 2025-02-01 06:33:45,955][0m Trial 2 finished with value: 0.10527067878191045 and parameters: {'observation_period_num': 15, 'train_rates': 0.6919481412547203, 'learning_rate': 5.034679658840859e-05, 'batch_size': 183, 'step_size': 14, 'gamma': 0.7652936581952884}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:34:06,214][0m Trial 3 finished with value: 0.26466664787410765 and parameters: {'observation_period_num': 38, 'train_rates': 0.6462605112376153, 'learning_rate': 2.512572413078243e-05, 'batch_size': 256, 'step_size': 3, 'gamma': 0.8990768537016071}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:36:51,751][0m Trial 4 finished with value: 0.17431799363856915 and parameters: {'observation_period_num': 199, 'train_rates': 0.6937827193467413, 'learning_rate': 0.0006887251815729112, 'batch_size': 26, 'step_size': 13, 'gamma': 0.7987608484155503}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:39:23,278][0m Trial 5 finished with value: 0.167757176098425 and parameters: {'observation_period_num': 198, 'train_rates': 0.6387440431382572, 'learning_rate': 0.00026380490124780335, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8425725344331529}. Best is trial 2 with value: 0.10527067878191045.[0m
Early stopping at epoch 48
[32m[I 2025-02-01 06:39:51,538][0m Trial 6 finished with value: 0.49678193382175784 and parameters: {'observation_period_num': 242, 'train_rates': 0.7267301960950633, 'learning_rate': 3.9241708982087075e-05, 'batch_size': 79, 'step_size': 1, 'gamma': 0.769488995953723}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:40:28,107][0m Trial 7 finished with value: 0.21521770392919515 and parameters: {'observation_period_num': 74, 'train_rates': 0.9201031426990036, 'learning_rate': 2.414543948499774e-05, 'batch_size': 162, 'step_size': 4, 'gamma': 0.9827153047052474}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:40:50,291][0m Trial 8 finished with value: 1.3104551977971022 and parameters: {'observation_period_num': 58, 'train_rates': 0.671524675906171, 'learning_rate': 1.116515949250284e-06, 'batch_size': 231, 'step_size': 2, 'gamma': 0.8277673571198373}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:41:44,696][0m Trial 9 finished with value: 0.2568894922733307 and parameters: {'observation_period_num': 234, 'train_rates': 0.9273848692804167, 'learning_rate': 0.00010424817213290334, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9590587170367781}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:42:16,717][0m Trial 10 finished with value: 0.5222975747258055 and parameters: {'observation_period_num': 132, 'train_rates': 0.8194894813951614, 'learning_rate': 5.631647564346348e-06, 'batch_size': 167, 'step_size': 10, 'gamma': 0.7555463752464525}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:43:03,227][0m Trial 11 finished with value: 0.13757298450350375 and parameters: {'observation_period_num': 127, 'train_rates': 0.7814549414608815, 'learning_rate': 8.558189153004181e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8921172692163521}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:43:43,588][0m Trial 12 finished with value: 0.3090701334262162 and parameters: {'observation_period_num': 142, 'train_rates': 0.8101756103823188, 'learning_rate': 7.0736220335554665e-06, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8867953062340391}. Best is trial 2 with value: 0.10527067878191045.[0m
[32m[I 2025-02-01 06:44:13,146][0m Trial 13 finished with value: 0.08851494437804187 and parameters: {'observation_period_num': 12, 'train_rates': 0.7672983672324226, 'learning_rate': 8.46122711827137e-05, 'batch_size': 186, 'step_size': 15, 'gamma': 0.9141616730683847}. Best is trial 13 with value: 0.08851494437804187.[0m
[32m[I 2025-02-01 06:44:38,411][0m Trial 14 finished with value: 0.07315906493836997 and parameters: {'observation_period_num': 9, 'train_rates': 0.6028188409225465, 'learning_rate': 0.0003643424609034026, 'batch_size': 185, 'step_size': 6, 'gamma': 0.924036717974032}. Best is trial 14 with value: 0.07315906493836997.[0m
[32m[I 2025-02-01 06:45:00,759][0m Trial 15 finished with value: 0.06151338362376032 and parameters: {'observation_period_num': 8, 'train_rates': 0.6055117103397516, 'learning_rate': 0.00083312569430461, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9244474333705247}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:45:23,185][0m Trial 16 finished with value: 0.15116455282101846 and parameters: {'observation_period_num': 95, 'train_rates': 0.6026447754093325, 'learning_rate': 0.0008151961485863954, 'batch_size': 206, 'step_size': 6, 'gamma': 0.9304365375456144}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:46:02,569][0m Trial 17 finished with value: 0.10113930878670592 and parameters: {'observation_period_num': 5, 'train_rates': 0.8649745312472348, 'learning_rate': 0.0003323638747452154, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8596763325435749}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:46:24,976][0m Trial 18 finished with value: 0.14142250212339255 and parameters: {'observation_period_num': 97, 'train_rates': 0.6041391437556183, 'learning_rate': 0.000292911206024686, 'batch_size': 213, 'step_size': 7, 'gamma': 0.9836803257338959}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:46:51,409][0m Trial 19 finished with value: 0.29771050810813904 and parameters: {'observation_period_num': 35, 'train_rates': 0.9699040562813424, 'learning_rate': 0.0008884954854007069, 'batch_size': 246, 'step_size': 5, 'gamma': 0.9311444914602809}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:47:16,465][0m Trial 20 finished with value: 0.16471772521188519 and parameters: {'observation_period_num': 167, 'train_rates': 0.6472959368031003, 'learning_rate': 0.0003992693745987735, 'batch_size': 193, 'step_size': 8, 'gamma': 0.8774563664370318}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:47:45,876][0m Trial 21 finished with value: 0.09078411235876963 and parameters: {'observation_period_num': 29, 'train_rates': 0.7590118593019549, 'learning_rate': 0.00020097148922183005, 'batch_size': 182, 'step_size': 11, 'gamma': 0.9195495089471459}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:48:15,032][0m Trial 22 finished with value: 0.09934036660746043 and parameters: {'observation_period_num': 5, 'train_rates': 0.8498968510567693, 'learning_rate': 0.0001553375257922997, 'batch_size': 206, 'step_size': 8, 'gamma': 0.91310507254259}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:48:44,360][0m Trial 23 finished with value: 0.08510297277136233 and parameters: {'observation_period_num': 45, 'train_rates': 0.6245737181036211, 'learning_rate': 0.000515812001786431, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9527864596258777}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:49:15,686][0m Trial 24 finished with value: 0.0816546840564195 and parameters: {'observation_period_num': 44, 'train_rates': 0.6252553876720423, 'learning_rate': 0.0005305668310286043, 'batch_size': 143, 'step_size': 4, 'gamma': 0.9651743399297489}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:49:51,315][0m Trial 25 finished with value: 0.10640798652341135 and parameters: {'observation_period_num': 83, 'train_rates': 0.6686263186054426, 'learning_rate': 0.0009862855155760451, 'batch_size': 131, 'step_size': 4, 'gamma': 0.9634460019436807}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:50:23,663][0m Trial 26 finished with value: 0.09169849935967012 and parameters: {'observation_period_num': 28, 'train_rates': 0.6019767380806963, 'learning_rate': 0.0004731808495534981, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9365510834237866}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:51:03,770][0m Trial 27 finished with value: 0.09503861252169911 and parameters: {'observation_period_num': 49, 'train_rates': 0.6568877721635754, 'learning_rate': 0.0005213953786268056, 'batch_size': 114, 'step_size': 4, 'gamma': 0.9697217103253516}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:51:27,593][0m Trial 28 finished with value: 0.12644796596651667 and parameters: {'observation_period_num': 111, 'train_rates': 0.74967247864481, 'learning_rate': 0.00020202940398919358, 'batch_size': 220, 'step_size': 7, 'gamma': 0.9432094067540691}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:51:48,007][0m Trial 29 finished with value: 0.662931759306725 and parameters: {'observation_period_num': 72, 'train_rates': 0.625101266673779, 'learning_rate': 2.287415115983921e-06, 'batch_size': 238, 'step_size': 3, 'gamma': 0.9891707506372102}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:52:20,243][0m Trial 30 finished with value: 0.10317208222513996 and parameters: {'observation_period_num': 56, 'train_rates': 0.7194397438086475, 'learning_rate': 0.00016320304385461523, 'batch_size': 153, 'step_size': 5, 'gamma': 0.861313899650583}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:52:47,279][0m Trial 31 finished with value: 0.07960747212117467 and parameters: {'observation_period_num': 26, 'train_rates': 0.6237638449129926, 'learning_rate': 0.0005635120974781203, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9629043397367604}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:53:14,650][0m Trial 32 finished with value: 0.0766827714844392 and parameters: {'observation_period_num': 22, 'train_rates': 0.6275854404589143, 'learning_rate': 0.0005835070187528688, 'batch_size': 175, 'step_size': 7, 'gamma': 0.9742955059043}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:53:41,569][0m Trial 33 finished with value: 0.08075300292535262 and parameters: {'observation_period_num': 27, 'train_rates': 0.6245060745665392, 'learning_rate': 0.0003217084237946871, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9461320323611383}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:54:07,683][0m Trial 34 finished with value: 0.0742838777213415 and parameters: {'observation_period_num': 20, 'train_rates': 0.6861423899885362, 'learning_rate': 0.0006625591666957511, 'batch_size': 191, 'step_size': 9, 'gamma': 0.9048758376565077}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:54:34,237][0m Trial 35 finished with value: 0.13650693777733305 and parameters: {'observation_period_num': 16, 'train_rates': 0.6880164897302605, 'learning_rate': 1.6383761567220947e-05, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9056672803396223}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:54:58,808][0m Trial 36 finished with value: 0.07086930893838468 and parameters: {'observation_period_num': 15, 'train_rates': 0.675753841058302, 'learning_rate': 0.000714287231359337, 'batch_size': 201, 'step_size': 9, 'gamma': 0.9245871691556651}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:55:22,148][0m Trial 37 finished with value: 0.11546738296415333 and parameters: {'observation_period_num': 63, 'train_rates': 0.6896667335861587, 'learning_rate': 5.8194795788320886e-05, 'batch_size': 200, 'step_size': 10, 'gamma': 0.9261070769084269}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:55:46,062][0m Trial 38 finished with value: 0.07497779977914687 and parameters: {'observation_period_num': 6, 'train_rates': 0.7292773424030904, 'learning_rate': 0.0007766875080014615, 'batch_size': 223, 'step_size': 10, 'gamma': 0.8996950559763839}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:56:05,624][0m Trial 39 finished with value: 0.08761002076647333 and parameters: {'observation_period_num': 40, 'train_rates': 0.6634630514059078, 'learning_rate': 0.00023398089564338954, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8767372401343763}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:56:27,138][0m Trial 40 finished with value: 0.14086283883079886 and parameters: {'observation_period_num': 163, 'train_rates': 0.7022423702272955, 'learning_rate': 0.00033040528219942987, 'batch_size': 234, 'step_size': 9, 'gamma': 0.9060063493490568}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:56:52,805][0m Trial 41 finished with value: 0.07354196272601331 and parameters: {'observation_period_num': 5, 'train_rates': 0.7331146043995012, 'learning_rate': 0.0007198652053652744, 'batch_size': 214, 'step_size': 11, 'gamma': 0.8943489381739105}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:57:17,169][0m Trial 42 finished with value: 0.07430589100572346 and parameters: {'observation_period_num': 17, 'train_rates': 0.6808144245484261, 'learning_rate': 0.0009849153637789992, 'batch_size': 216, 'step_size': 12, 'gamma': 0.891440722529706}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:58:45,830][0m Trial 43 finished with value: 0.08457971751670708 and parameters: {'observation_period_num': 20, 'train_rates': 0.7358267342826472, 'learning_rate': 0.0007131895406103285, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8457530602818827}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:59:11,995][0m Trial 44 finished with value: 0.08726048951411054 and parameters: {'observation_period_num': 38, 'train_rates': 0.7045162519640711, 'learning_rate': 0.00040542331279076494, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9223527509500015}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 06:59:34,117][0m Trial 45 finished with value: 0.10829421808381402 and parameters: {'observation_period_num': 55, 'train_rates': 0.6525955504436584, 'learning_rate': 0.0006747783876350288, 'batch_size': 211, 'step_size': 9, 'gamma': 0.8039847261388682}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 07:00:02,438][0m Trial 46 finished with value: 0.0876270311557694 and parameters: {'observation_period_num': 17, 'train_rates': 0.7429974241032067, 'learning_rate': 0.0001346051310444065, 'batch_size': 182, 'step_size': 11, 'gamma': 0.8824256172057836}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 07:00:27,877][0m Trial 47 finished with value: 0.08331443271165492 and parameters: {'observation_period_num': 7, 'train_rates': 0.7890598552466678, 'learning_rate': 0.00041381541505764927, 'batch_size': 230, 'step_size': 14, 'gamma': 0.9012411796058168}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 07:00:49,441][0m Trial 48 finished with value: 0.1056732686017366 and parameters: {'observation_period_num': 69, 'train_rates': 0.7174257616606614, 'learning_rate': 0.0002501165859386607, 'batch_size': 242, 'step_size': 12, 'gamma': 0.9360231298974627}. Best is trial 15 with value: 0.06151338362376032.[0m
[32m[I 2025-02-01 07:01:13,907][0m Trial 49 finished with value: 0.21002559905357196 and parameters: {'observation_period_num': 222, 'train_rates': 0.641146884670317, 'learning_rate': 0.0007145647614459393, 'batch_size': 185, 'step_size': 10, 'gamma': 0.9131657038092162}. Best is trial 15 with value: 0.06151338362376032.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-01 07:01:13,917][0m A new study created in memory with name: no-name-80f08aae-1596-4903-a888-a6593e8875c7[0m
[32m[I 2025-02-01 07:01:42,431][0m Trial 0 finished with value: 0.15552434087647293 and parameters: {'observation_period_num': 73, 'train_rates': 0.8843210773793397, 'learning_rate': 0.0008357512682658588, 'batch_size': 203, 'step_size': 14, 'gamma': 0.9520667011470005}. Best is trial 0 with value: 0.15552434087647293.[0m
[32m[I 2025-02-01 07:03:15,774][0m Trial 1 finished with value: 0.13705815659581563 and parameters: {'observation_period_num': 84, 'train_rates': 0.8420790395106067, 'learning_rate': 0.00021123214398326487, 'batch_size': 55, 'step_size': 13, 'gamma': 0.8636022467267441}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:04:22,118][0m Trial 2 finished with value: 0.21103307204220884 and parameters: {'observation_period_num': 108, 'train_rates': 0.869434855998412, 'learning_rate': 0.0007838628466448947, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9694053426406625}. Best is trial 1 with value: 0.13705815659581563.[0m
Early stopping at epoch 75
[32m[I 2025-02-01 07:04:42,517][0m Trial 3 finished with value: 0.2677462089753685 and parameters: {'observation_period_num': 161, 'train_rates': 0.6484994867635978, 'learning_rate': 0.0005917392292926154, 'batch_size': 177, 'step_size': 1, 'gamma': 0.8320770726783867}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:05:17,684][0m Trial 4 finished with value: 0.1938378588504383 and parameters: {'observation_period_num': 54, 'train_rates': 0.8873352204594458, 'learning_rate': 2.6802284453117475e-05, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9114093739481668}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:06:07,144][0m Trial 5 finished with value: 0.2068944223856522 and parameters: {'observation_period_num': 180, 'train_rates': 0.8939137012850197, 'learning_rate': 0.0002406798083158495, 'batch_size': 108, 'step_size': 4, 'gamma': 0.7923510151971637}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:06:41,589][0m Trial 6 finished with value: 0.450145845413208 and parameters: {'observation_period_num': 74, 'train_rates': 0.9393582832152135, 'learning_rate': 1.537251953257179e-05, 'batch_size': 172, 'step_size': 5, 'gamma': 0.892447253859376}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:07:04,916][0m Trial 7 finished with value: 0.2131558528777917 and parameters: {'observation_period_num': 178, 'train_rates': 0.7583231467887099, 'learning_rate': 2.5705293541478192e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.966936403822486}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:07:27,734][0m Trial 8 finished with value: 0.1954849345464233 and parameters: {'observation_period_num': 234, 'train_rates': 0.6890205444559694, 'learning_rate': 8.226009711009963e-05, 'batch_size': 210, 'step_size': 7, 'gamma': 0.9644251792532145}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:08:06,210][0m Trial 9 finished with value: 0.16400195324663505 and parameters: {'observation_period_num': 163, 'train_rates': 0.630563097980653, 'learning_rate': 5.1755191483136234e-05, 'batch_size': 114, 'step_size': 15, 'gamma': 0.8148249569406787}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:10:46,690][0m Trial 10 finished with value: 0.21459990044901256 and parameters: {'observation_period_num': 8, 'train_rates': 0.7831844567169881, 'learning_rate': 1.0322709009166747e-06, 'batch_size': 31, 'step_size': 10, 'gamma': 0.7525610456527811}. Best is trial 1 with value: 0.13705815659581563.[0m
[32m[I 2025-02-01 07:15:49,201][0m Trial 11 finished with value: 0.04756772335280072 and parameters: {'observation_period_num': 60, 'train_rates': 0.9810154734030505, 'learning_rate': 0.00025087005947634436, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8604249058688656}. Best is trial 11 with value: 0.04756772335280072.[0m
[32m[I 2025-02-01 07:20:29,395][0m Trial 12 finished with value: 0.031074515776708722 and parameters: {'observation_period_num': 15, 'train_rates': 0.9894345013358946, 'learning_rate': 0.00015780686567911655, 'batch_size': 21, 'step_size': 13, 'gamma': 0.8567373686323754}. Best is trial 12 with value: 0.031074515776708722.[0m
[32m[I 2025-02-01 07:24:56,564][0m Trial 13 finished with value: 0.017107598120119513 and parameters: {'observation_period_num': 7, 'train_rates': 0.9863055721223846, 'learning_rate': 0.00015343286780489403, 'batch_size': 22, 'step_size': 11, 'gamma': 0.8620149056296053}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:26:26,819][0m Trial 14 finished with value: 0.03171456232666969 and parameters: {'observation_period_num': 7, 'train_rates': 0.9844262984265967, 'learning_rate': 9.284461248663943e-05, 'batch_size': 66, 'step_size': 11, 'gamma': 0.9117503441229169}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:28:39,623][0m Trial 15 finished with value: 0.3461469729506486 and parameters: {'observation_period_num': 31, 'train_rates': 0.949385729844609, 'learning_rate': 4.557894894233183e-06, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8365142091049521}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:29:43,066][0m Trial 16 finished with value: 0.18263860814729516 and parameters: {'observation_period_num': 35, 'train_rates': 0.9330465305239483, 'learning_rate': 0.0001325352679237833, 'batch_size': 92, 'step_size': 7, 'gamma': 0.8884197646197646}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:30:04,375][0m Trial 17 finished with value: 0.4093887351041463 and parameters: {'observation_period_num': 124, 'train_rates': 0.7189661007712433, 'learning_rate': 9.859020542175042e-06, 'batch_size': 245, 'step_size': 12, 'gamma': 0.7824177506348873}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:30:44,089][0m Trial 18 finished with value: 0.10087686235187796 and parameters: {'observation_period_num': 30, 'train_rates': 0.8265668697391105, 'learning_rate': 0.00038023720995596295, 'batch_size': 137, 'step_size': 13, 'gamma': 0.9290844468445172}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:35:16,503][0m Trial 19 finished with value: 0.2723845842069593 and parameters: {'observation_period_num': 242, 'train_rates': 0.9254105242989881, 'learning_rate': 6.247930478262897e-05, 'batch_size': 19, 'step_size': 11, 'gamma': 0.8422642917592877}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:36:46,945][0m Trial 20 finished with value: 0.355326497598606 and parameters: {'observation_period_num': 114, 'train_rates': 0.9643027945309398, 'learning_rate': 0.0001384165754291458, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8713804901271143}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:38:12,676][0m Trial 21 finished with value: 0.0345458909869194 and parameters: {'observation_period_num': 18, 'train_rates': 0.9885811270919046, 'learning_rate': 0.0001091551200301103, 'batch_size': 70, 'step_size': 11, 'gamma': 0.9180203079476709}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:40:23,498][0m Trial 22 finished with value: 0.03604423627257347 and parameters: {'observation_period_num': 5, 'train_rates': 0.9883765471099571, 'learning_rate': 4.473132702747652e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8940681233127981}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:46:05,077][0m Trial 23 finished with value: 0.17865719242239153 and parameters: {'observation_period_num': 48, 'train_rates': 0.9206410839689931, 'learning_rate': 0.0003869333920783587, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9282599092160079}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:48:15,346][0m Trial 24 finished with value: 0.14834662752400618 and parameters: {'observation_period_num': 24, 'train_rates': 0.912637295776258, 'learning_rate': 0.00016237618389856176, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8779497468040676}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:49:19,765][0m Trial 25 finished with value: 0.2853127221266429 and parameters: {'observation_period_num': 94, 'train_rates': 0.957092766464486, 'learning_rate': 8.295037240236484e-05, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8552329406584823}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:51:48,948][0m Trial 26 finished with value: 0.13838817946263826 and parameters: {'observation_period_num': 46, 'train_rates': 0.8314563121071443, 'learning_rate': 0.000398358720975447, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9052513632657342}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:53:21,346][0m Trial 27 finished with value: 0.2945305079094365 and parameters: {'observation_period_num': 5, 'train_rates': 0.9604622228324984, 'learning_rate': 9.765495154562215e-06, 'batch_size': 64, 'step_size': 10, 'gamma': 0.816697689714099}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:54:13,640][0m Trial 28 finished with value: 0.34080520272254944 and parameters: {'observation_period_num': 62, 'train_rates': 0.9661521396152494, 'learning_rate': 4.218631964128774e-05, 'batch_size': 113, 'step_size': 14, 'gamma': 0.9396649052311348}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:54:54,905][0m Trial 29 finished with value: 0.11392754342564394 and parameters: {'observation_period_num': 40, 'train_rates': 0.8539725288524714, 'learning_rate': 0.0005852012085422492, 'batch_size': 137, 'step_size': 8, 'gamma': 0.9472596277340901}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:58:07,607][0m Trial 30 finished with value: 0.16308207494200122 and parameters: {'observation_period_num': 77, 'train_rates': 0.9022711528227336, 'learning_rate': 0.0009997502143021869, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8489917927003123}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 07:59:29,659][0m Trial 31 finished with value: 0.03483802080154419 and parameters: {'observation_period_num': 21, 'train_rates': 0.9890493389497219, 'learning_rate': 0.00010239951223719866, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9897441531783112}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:01:18,028][0m Trial 32 finished with value: 0.03311635181307793 and parameters: {'observation_period_num': 21, 'train_rates': 0.987182832278969, 'learning_rate': 0.00018766033721857644, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9131813683355681}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:03:14,832][0m Trial 33 finished with value: 0.18916911259293556 and parameters: {'observation_period_num': 18, 'train_rates': 0.9358933704242406, 'learning_rate': 0.00019403300734441759, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8854640857212335}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:04:53,891][0m Trial 34 finished with value: 0.3932291839433753 and parameters: {'observation_period_num': 87, 'train_rates': 0.9680366039299254, 'learning_rate': 0.00031185541891400845, 'batch_size': 58, 'step_size': 12, 'gamma': 0.8690074677456252}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:05:58,256][0m Trial 35 finished with value: 0.13110893829302353 and parameters: {'observation_period_num': 42, 'train_rates': 0.8717510439578857, 'learning_rate': 7.018323777044259e-05, 'batch_size': 85, 'step_size': 9, 'gamma': 0.9086141152313073}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:09:00,878][0m Trial 36 finished with value: 0.28815051884787857 and parameters: {'observation_period_num': 61, 'train_rates': 0.9458253687088305, 'learning_rate': 0.00020036738588073195, 'batch_size': 31, 'step_size': 13, 'gamma': 0.9254690939338724}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:09:54,118][0m Trial 37 finished with value: 0.21644399678095794 and parameters: {'observation_period_num': 146, 'train_rates': 0.9028502820942657, 'learning_rate': 0.000565572440199151, 'batch_size': 103, 'step_size': 12, 'gamma': 0.8171060564068411}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:11:32,673][0m Trial 38 finished with value: 0.1213067376613617 and parameters: {'observation_period_num': 16, 'train_rates': 0.8811371223247944, 'learning_rate': 2.7133036720400218e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9009843348244947}. Best is trial 13 with value: 0.017107598120119513.[0m
Early stopping at epoch 93
[32m[I 2025-02-01 08:12:44,520][0m Trial 39 finished with value: 0.4378164239132252 and parameters: {'observation_period_num': 31, 'train_rates': 0.9735038908922111, 'learning_rate': 0.00012699827731549277, 'batch_size': 77, 'step_size': 1, 'gamma': 0.8660338469212424}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:13:17,809][0m Trial 40 finished with value: 0.12331051295527826 and parameters: {'observation_period_num': 104, 'train_rates': 0.7945358577490256, 'learning_rate': 0.0002418767420509672, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9571835856423089}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:14:48,204][0m Trial 41 finished with value: 0.029902944341301918 and parameters: {'observation_period_num': 17, 'train_rates': 0.9883811403331887, 'learning_rate': 9.453680689270375e-05, 'batch_size': 66, 'step_size': 11, 'gamma': 0.9191145744879413}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:17:13,430][0m Trial 42 finished with value: 0.20799299457487258 and parameters: {'observation_period_num': 7, 'train_rates': 0.9556353657911564, 'learning_rate': 8.77998803219368e-05, 'batch_size': 40, 'step_size': 11, 'gamma': 0.939982158638325}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:18:58,737][0m Trial 43 finished with value: 0.2337601009161162 and parameters: {'observation_period_num': 50, 'train_rates': 0.9418246461245511, 'learning_rate': 0.00016578334416094475, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8775849221297572}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:19:31,503][0m Trial 44 finished with value: 0.0861089900135994 and parameters: {'observation_period_num': 20, 'train_rates': 0.9768172318563129, 'learning_rate': 5.3062018056258706e-05, 'batch_size': 194, 'step_size': 3, 'gamma': 0.915068621136156}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:21:48,057][0m Trial 45 finished with value: 0.09732100266610577 and parameters: {'observation_period_num': 36, 'train_rates': 0.6853370235734122, 'learning_rate': 3.349521341738311e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.8271165711318421}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:22:45,471][0m Trial 46 finished with value: 0.31154654269593623 and parameters: {'observation_period_num': 222, 'train_rates': 0.928212626718062, 'learning_rate': 1.951710258924624e-05, 'batch_size': 94, 'step_size': 6, 'gamma': 0.8999298416585987}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:27:18,895][0m Trial 47 finished with value: 0.0711560836609672 and parameters: {'observation_period_num': 66, 'train_rates': 0.9885187272859302, 'learning_rate': 0.0003309815752923011, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9793746916656129}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:28:09,405][0m Trial 48 finished with value: 0.5059023978826883 and parameters: {'observation_period_num': 16, 'train_rates': 0.9515172450473992, 'learning_rate': 1.5827522847391375e-06, 'batch_size': 120, 'step_size': 9, 'gamma': 0.8832701288286473}. Best is trial 13 with value: 0.017107598120119513.[0m
[32m[I 2025-02-01 08:29:20,098][0m Trial 49 finished with value: 0.09344241954386234 and parameters: {'observation_period_num': 29, 'train_rates': 0.7553991241180198, 'learning_rate': 7.580247272051462e-05, 'batch_size': 70, 'step_size': 12, 'gamma': 0.851482639852879}. Best is trial 13 with value: 0.017107598120119513.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 19, 'train_rates': 0.9752609385794304, 'learning_rate': 0.00021714884423369222, 'batch_size': 40, 'step_size': 14, 'gamma': 0.9309374131420991}
Epoch 1/300, trend Loss: 0.3663 | 0.2165
Epoch 2/300, trend Loss: 0.1533 | 0.1558
Epoch 3/300, trend Loss: 0.1330 | 0.1321
Epoch 4/300, trend Loss: 0.1229 | 0.1068
Epoch 5/300, trend Loss: 0.1169 | 0.1024
Epoch 6/300, trend Loss: 0.1111 | 0.0966
Epoch 7/300, trend Loss: 0.1067 | 0.0892
Epoch 8/300, trend Loss: 0.1027 | 0.0851
Epoch 9/300, trend Loss: 0.0995 | 0.0795
Epoch 10/300, trend Loss: 0.0959 | 0.0753
Epoch 11/300, trend Loss: 0.0926 | 0.0728
Epoch 12/300, trend Loss: 0.0894 | 0.0681
Epoch 13/300, trend Loss: 0.0861 | 0.0624
Epoch 14/300, trend Loss: 0.0829 | 0.0585
Epoch 15/300, trend Loss: 0.0803 | 0.0562
Epoch 16/300, trend Loss: 0.0783 | 0.0552
Epoch 17/300, trend Loss: 0.0766 | 0.0546
Epoch 18/300, trend Loss: 0.0752 | 0.0544
Epoch 19/300, trend Loss: 0.0740 | 0.0548
Epoch 20/300, trend Loss: 0.0729 | 0.0546
Epoch 21/300, trend Loss: 0.0719 | 0.0533
Epoch 22/300, trend Loss: 0.0706 | 0.0514
Epoch 23/300, trend Loss: 0.0697 | 0.0511
Epoch 24/300, trend Loss: 0.0688 | 0.0490
Epoch 25/300, trend Loss: 0.0679 | 0.0488
Epoch 26/300, trend Loss: 0.0669 | 0.0463
Epoch 27/300, trend Loss: 0.0660 | 0.0475
Epoch 28/300, trend Loss: 0.0652 | 0.0434
Epoch 29/300, trend Loss: 0.0643 | 0.0478
Epoch 30/300, trend Loss: 0.0640 | 0.0418
Epoch 31/300, trend Loss: 0.0634 | 0.0474
Epoch 32/300, trend Loss: 0.0631 | 0.0421
Epoch 33/300, trend Loss: 0.0627 | 0.0459
Epoch 34/300, trend Loss: 0.0622 | 0.0439
Epoch 35/300, trend Loss: 0.0624 | 0.0434
Epoch 36/300, trend Loss: 0.0613 | 0.0424
Epoch 37/300, trend Loss: 0.0614 | 0.0420
Epoch 38/300, trend Loss: 0.0606 | 0.0411
Epoch 39/300, trend Loss: 0.0602 | 0.0407
Epoch 40/300, trend Loss: 0.0601 | 0.0385
Epoch 41/300, trend Loss: 0.0590 | 0.0397
Epoch 42/300, trend Loss: 0.0583 | 0.0405
Epoch 43/300, trend Loss: 0.0576 | 0.0414
Epoch 44/300, trend Loss: 0.0573 | 0.0395
Epoch 45/300, trend Loss: 0.0569 | 0.0407
Epoch 46/300, trend Loss: 0.0570 | 0.0389
Epoch 47/300, trend Loss: 0.0561 | 0.0407
Epoch 48/300, trend Loss: 0.0554 | 0.0396
Epoch 49/300, trend Loss: 0.0550 | 0.0404
Epoch 50/300, trend Loss: 0.0548 | 0.0396
Epoch 51/300, trend Loss: 0.0543 | 0.0401
Epoch 52/300, trend Loss: 0.0538 | 0.0395
Epoch 53/300, trend Loss: 0.0535 | 0.0399
Epoch 54/300, trend Loss: 0.0534 | 0.0404
Epoch 55/300, trend Loss: 0.0534 | 0.0407
Epoch 56/300, trend Loss: 0.0533 | 0.0414
Epoch 57/300, trend Loss: 0.0533 | 0.0415
Epoch 58/300, trend Loss: 0.0534 | 0.0410
Epoch 59/300, trend Loss: 0.0529 | 0.0407
Epoch 60/300, trend Loss: 0.0522 | 0.0402
Epoch 61/300, trend Loss: 0.0516 | 0.0408
Epoch 62/300, trend Loss: 0.0516 | 0.0408
Epoch 63/300, trend Loss: 0.0512 | 0.0442
Epoch 64/300, trend Loss: 0.0516 | 0.0435
Epoch 65/300, trend Loss: 0.0514 | 0.0482
Epoch 66/300, trend Loss: 0.0516 | 0.0456
Epoch 67/300, trend Loss: 0.0521 | 0.0481
Epoch 68/300, trend Loss: 0.0502 | 0.0452
Epoch 69/300, trend Loss: 0.0499 | 0.0493
Epoch 70/300, trend Loss: 0.0507 | 0.0504
Epoch 71/300, trend Loss: 0.0536 | 0.0478
Epoch 72/300, trend Loss: 0.0497 | 0.0493
Epoch 73/300, trend Loss: 0.0494 | 0.0500
Epoch 74/300, trend Loss: 0.0510 | 0.0527
Epoch 75/300, trend Loss: 0.0496 | 0.0521
Epoch 76/300, trend Loss: 0.0489 | 0.0531
Epoch 77/300, trend Loss: 0.0489 | 0.0529
Epoch 78/300, trend Loss: 0.0469 | 0.0530
Epoch 79/300, trend Loss: 0.0438 | 0.0579
Epoch 80/300, trend Loss: 0.0423 | 0.0609
Epoch 81/300, trend Loss: 0.0461 | 0.0576
Epoch 82/300, trend Loss: 0.0430 | 0.0572
Epoch 83/300, trend Loss: 0.0406 | 0.0555
Epoch 84/300, trend Loss: 0.0388 | 0.0533
Epoch 85/300, trend Loss: 0.0387 | 0.0562
Epoch 86/300, trend Loss: 0.0385 | 0.0519
Epoch 87/300, trend Loss: 0.0379 | 0.0578
Epoch 88/300, trend Loss: 0.0379 | 0.0509
Epoch 89/300, trend Loss: 0.0375 | 0.0535
Epoch 90/300, trend Loss: 0.0375 | 0.0534
Epoch 91/300, trend Loss: 0.0384 | 0.0486
Epoch 92/300, trend Loss: 0.0374 | 0.0559
Epoch 93/300, trend Loss: 0.0377 | 0.0484
Epoch 94/300, trend Loss: 0.0372 | 0.0598
Epoch 95/300, trend Loss: 0.0376 | 0.0512
Epoch 96/300, trend Loss: 0.0381 | 0.0553
Epoch 97/300, trend Loss: 0.0371 | 0.0510
Epoch 98/300, trend Loss: 0.0377 | 0.0527
Epoch 99/300, trend Loss: 0.0379 | 0.0555
Epoch 100/300, trend Loss: 0.0374 | 0.0565
Epoch 101/300, trend Loss: 0.0367 | 0.0616
Epoch 102/300, trend Loss: 0.0364 | 0.0659
Epoch 103/300, trend Loss: 0.0369 | 0.0596
Epoch 104/300, trend Loss: 0.0364 | 0.0648
Epoch 105/300, trend Loss: 0.0383 | 0.0537
Epoch 106/300, trend Loss: 0.0380 | 0.0536
Epoch 107/300, trend Loss: 0.0380 | 0.0561
Epoch 108/300, trend Loss: 0.0400 | 0.0538
Epoch 109/300, trend Loss: 0.0370 | 0.0494
Epoch 110/300, trend Loss: 0.0361 | 0.0501
Epoch 111/300, trend Loss: 0.0347 | 0.0512
Epoch 112/300, trend Loss: 0.0336 | 0.0508
Epoch 113/300, trend Loss: 0.0342 | 0.0540
Epoch 114/300, trend Loss: 0.0343 | 0.0528
Epoch 115/300, trend Loss: 0.0341 | 0.0550
Epoch 116/300, trend Loss: 0.0344 | 0.0522
Epoch 117/300, trend Loss: 0.0338 | 0.0541
Epoch 118/300, trend Loss: 0.0339 | 0.0518
Epoch 119/300, trend Loss: 0.0332 | 0.0528
Epoch 120/300, trend Loss: 0.0331 | 0.0516
Epoch 121/300, trend Loss: 0.0325 | 0.0514
Epoch 122/300, trend Loss: 0.0321 | 0.0516
Epoch 123/300, trend Loss: 0.0321 | 0.0519
Epoch 124/300, trend Loss: 0.0319 | 0.0527
Epoch 125/300, trend Loss: 0.0319 | 0.0529
Epoch 126/300, trend Loss: 0.0318 | 0.0530
Epoch 127/300, trend Loss: 0.0315 | 0.0522
Epoch 128/300, trend Loss: 0.0313 | 0.0526
Epoch 129/300, trend Loss: 0.0312 | 0.0516
Epoch 130/300, trend Loss: 0.0310 | 0.0528
Epoch 131/300, trend Loss: 0.0308 | 0.0519
Epoch 132/300, trend Loss: 0.0305 | 0.0529
Epoch 133/300, trend Loss: 0.0304 | 0.0521
Epoch 134/300, trend Loss: 0.0301 | 0.0526
Epoch 135/300, trend Loss: 0.0300 | 0.0518
Epoch 136/300, trend Loss: 0.0297 | 0.0526
Epoch 137/300, trend Loss: 0.0298 | 0.0524
Epoch 138/300, trend Loss: 0.0296 | 0.0527
Epoch 139/300, trend Loss: 0.0296 | 0.0526
Epoch 140/300, trend Loss: 0.0296 | 0.0531
Epoch 141/300, trend Loss: 0.0294 | 0.0529
Epoch 142/300, trend Loss: 0.0292 | 0.0530
Epoch 143/300, trend Loss: 0.0292 | 0.0528
Epoch 144/300, trend Loss: 0.0290 | 0.0532
Epoch 145/300, trend Loss: 0.0289 | 0.0538
Epoch 146/300, trend Loss: 0.0286 | 0.0536
Epoch 147/300, trend Loss: 0.0286 | 0.0538
Epoch 148/300, trend Loss: 0.0285 | 0.0534
Epoch 149/300, trend Loss: 0.0284 | 0.0540
Epoch 150/300, trend Loss: 0.0291 | 0.0534
Epoch 151/300, trend Loss: 0.0392 | 0.0541
Epoch 152/300, trend Loss: 0.0280 | 0.0540
Epoch 153/300, trend Loss: 0.0281 | 0.0553
Epoch 154/300, trend Loss: 0.0276 | 0.0553
Epoch 155/300, trend Loss: 0.0280 | 0.0568
Epoch 156/300, trend Loss: 0.0277 | 0.0561
Epoch 157/300, trend Loss: 0.0280 | 0.0569
Epoch 158/300, trend Loss: 0.0279 | 0.0573
Epoch 159/300, trend Loss: 0.0278 | 0.0568
Epoch 160/300, trend Loss: 0.0274 | 0.0567
Epoch 161/300, trend Loss: 0.0275 | 0.0565
Epoch 162/300, trend Loss: 0.0272 | 0.0554
Epoch 163/300, trend Loss: 0.0273 | 0.0558
Epoch 164/300, trend Loss: 0.0270 | 0.0553
Epoch 165/300, trend Loss: 0.0270 | 0.0552
Epoch 166/300, trend Loss: 0.0268 | 0.0546
Epoch 167/300, trend Loss: 0.0268 | 0.0547
Epoch 168/300, trend Loss: 0.0267 | 0.0544
Epoch 169/300, trend Loss: 0.0268 | 0.0546
Epoch 170/300, trend Loss: 0.0267 | 0.0542
Epoch 171/300, trend Loss: 0.0267 | 0.0546
Epoch 172/300, trend Loss: 0.0266 | 0.0542
Epoch 173/300, trend Loss: 0.0265 | 0.0544
Epoch 174/300, trend Loss: 0.0264 | 0.0543
Epoch 175/300, trend Loss: 0.0263 | 0.0543
Epoch 176/300, trend Loss: 0.0264 | 0.0541
Epoch 177/300, trend Loss: 0.0263 | 0.0539
Epoch 178/300, trend Loss: 0.0261 | 0.0541
Epoch 179/300, trend Loss: 0.0260 | 0.0541
Epoch 180/300, trend Loss: 0.0259 | 0.0543
Epoch 181/300, trend Loss: 0.0257 | 0.0545
Epoch 182/300, trend Loss: 0.0256 | 0.0547
Epoch 183/300, trend Loss: 0.0256 | 0.0551
Epoch 184/300, trend Loss: 0.0256 | 0.0550
Epoch 185/300, trend Loss: 0.0254 | 0.0554
Epoch 186/300, trend Loss: 0.0254 | 0.0552
Epoch 187/300, trend Loss: 0.0253 | 0.0554
Epoch 188/300, trend Loss: 0.0252 | 0.0553
Epoch 189/300, trend Loss: 0.0251 | 0.0552
Epoch 190/300, trend Loss: 0.0251 | 0.0550
Epoch 191/300, trend Loss: 0.0250 | 0.0548
Epoch 192/300, trend Loss: 0.0249 | 0.0546
Epoch 193/300, trend Loss: 0.0248 | 0.0546
Epoch 194/300, trend Loss: 0.0247 | 0.0544
Epoch 195/300, trend Loss: 0.0246 | 0.0544
Epoch 196/300, trend Loss: 0.0245 | 0.0542
Epoch 197/300, trend Loss: 0.0245 | 0.0543
Epoch 198/300, trend Loss: 0.0244 | 0.0542
Epoch 199/300, trend Loss: 0.0244 | 0.0542
Epoch 200/300, trend Loss: 0.0243 | 0.0542
Epoch 201/300, trend Loss: 0.0243 | 0.0540
Epoch 202/300, trend Loss: 0.0242 | 0.0542
Epoch 203/300, trend Loss: 0.0243 | 0.0538
Epoch 204/300, trend Loss: 0.0242 | 0.0540
Epoch 205/300, trend Loss: 0.0242 | 0.0540
Epoch 206/300, trend Loss: 0.0243 | 0.0539
Epoch 207/300, trend Loss: 0.0244 | 0.0539
Epoch 208/300, trend Loss: 0.0244 | 0.0542
Epoch 209/300, trend Loss: 0.0245 | 0.0544
Epoch 210/300, trend Loss: 0.0244 | 0.0549
Epoch 211/300, trend Loss: 0.0244 | 0.0551
Epoch 212/300, trend Loss: 0.0244 | 0.0553
Epoch 213/300, trend Loss: 0.0243 | 0.0550
Epoch 214/300, trend Loss: 0.0243 | 0.0549
Epoch 215/300, trend Loss: 0.0243 | 0.0551
Epoch 216/300, trend Loss: 0.0243 | 0.0551
Epoch 217/300, trend Loss: 0.0246 | 0.0561
Epoch 218/300, trend Loss: 0.0250 | 0.0555
Epoch 219/300, trend Loss: 0.0253 | 0.0564
Epoch 220/300, trend Loss: 0.0254 | 0.0560
Epoch 221/300, trend Loss: 0.0257 | 0.0569
Epoch 222/300, trend Loss: 0.0254 | 0.0561
Epoch 223/300, trend Loss: 0.0254 | 0.0573
Epoch 224/300, trend Loss: 0.0249 | 0.0560
Epoch 225/300, trend Loss: 0.0246 | 0.0565
Epoch 226/300, trend Loss: 0.0241 | 0.0554
Epoch 227/300, trend Loss: 0.0239 | 0.0553
Epoch 228/300, trend Loss: 0.0237 | 0.0548
Epoch 229/300, trend Loss: 0.0235 | 0.0546
Epoch 230/300, trend Loss: 0.0234 | 0.0543
Epoch 231/300, trend Loss: 0.0232 | 0.0542
Epoch 232/300, trend Loss: 0.0231 | 0.0539
Epoch 233/300, trend Loss: 0.0230 | 0.0539
Epoch 234/300, trend Loss: 0.0229 | 0.0540
Epoch 235/300, trend Loss: 0.0228 | 0.0542
Epoch 236/300, trend Loss: 0.0227 | 0.0544
Epoch 237/300, trend Loss: 0.0226 | 0.0546
Epoch 238/300, trend Loss: 0.0225 | 0.0548
Epoch 239/300, trend Loss: 0.0224 | 0.0548
Epoch 240/300, trend Loss: 0.0224 | 0.0549
Epoch 241/300, trend Loss: 0.0223 | 0.0551
Epoch 242/300, trend Loss: 0.0223 | 0.0552
Epoch 243/300, trend Loss: 0.0222 | 0.0552
Epoch 244/300, trend Loss: 0.0222 | 0.0553
Epoch 245/300, trend Loss: 0.0221 | 0.0553
Epoch 246/300, trend Loss: 0.0221 | 0.0551
Epoch 247/300, trend Loss: 0.0221 | 0.0551
Epoch 248/300, trend Loss: 0.0221 | 0.0551
Epoch 249/300, trend Loss: 0.0220 | 0.0551
Epoch 250/300, trend Loss: 0.0220 | 0.0551
Epoch 251/300, trend Loss: 0.0220 | 0.0551
Epoch 252/300, trend Loss: 0.0219 | 0.0551
Epoch 253/300, trend Loss: 0.0219 | 0.0550
Epoch 254/300, trend Loss: 0.0219 | 0.0550
Epoch 255/300, trend Loss: 0.0219 | 0.0551
Epoch 256/300, trend Loss: 0.0219 | 0.0551
Epoch 257/300, trend Loss: 0.0218 | 0.0552
Epoch 258/300, trend Loss: 0.0218 | 0.0552
Epoch 259/300, trend Loss: 0.0217 | 0.0552
Epoch 260/300, trend Loss: 0.0217 | 0.0552
Epoch 261/300, trend Loss: 0.0217 | 0.0553
Epoch 262/300, trend Loss: 0.0216 | 0.0554
Epoch 263/300, trend Loss: 0.0216 | 0.0555
Epoch 264/300, trend Loss: 0.0215 | 0.0556
Epoch 265/300, trend Loss: 0.0215 | 0.0558
Epoch 266/300, trend Loss: 0.0214 | 0.0558
Epoch 267/300, trend Loss: 0.0214 | 0.0559
Epoch 268/300, trend Loss: 0.0214 | 0.0560
Epoch 269/300, trend Loss: 0.0213 | 0.0563
Epoch 270/300, trend Loss: 0.0213 | 0.0563
Epoch 271/300, trend Loss: 0.0213 | 0.0565
Epoch 272/300, trend Loss: 0.0213 | 0.0565
Epoch 273/300, trend Loss: 0.0212 | 0.0567
Epoch 274/300, trend Loss: 0.0213 | 0.0564
Epoch 275/300, trend Loss: 0.0212 | 0.0568
Epoch 276/300, trend Loss: 0.0213 | 0.0567
Epoch 277/300, trend Loss: 0.0212 | 0.0570
Epoch 278/300, trend Loss: 0.0213 | 0.0569
Epoch 279/300, trend Loss: 0.0212 | 0.0572
Epoch 280/300, trend Loss: 0.0213 | 0.0571
Epoch 281/300, trend Loss: 0.0212 | 0.0573
Epoch 282/300, trend Loss: 0.0213 | 0.0573
Epoch 283/300, trend Loss: 0.0212 | 0.0575
Epoch 284/300, trend Loss: 0.0212 | 0.0576
Epoch 285/300, trend Loss: 0.0212 | 0.0577
Epoch 286/300, trend Loss: 0.0212 | 0.0577
Epoch 287/300, trend Loss: 0.0211 | 0.0577
Epoch 288/300, trend Loss: 0.0211 | 0.0578
Epoch 289/300, trend Loss: 0.0211 | 0.0578
Epoch 290/300, trend Loss: 0.0211 | 0.0578
Epoch 291/300, trend Loss: 0.0211 | 0.0578
Epoch 292/300, trend Loss: 0.0210 | 0.0579
Epoch 293/300, trend Loss: 0.0209 | 0.0580
Epoch 294/300, trend Loss: 0.0209 | 0.0581
Epoch 295/300, trend Loss: 0.0208 | 0.0581
Epoch 296/300, trend Loss: 0.0207 | 0.0582
Epoch 297/300, trend Loss: 0.0207 | 0.0582
Epoch 298/300, trend Loss: 0.0206 | 0.0582
Epoch 299/300, trend Loss: 0.0206 | 0.0582
Epoch 300/300, trend Loss: 0.0205 | 0.0582
Training seasonal_0 component with params: {'observation_period_num': 42, 'train_rates': 0.9869342496226307, 'learning_rate': 0.0003550424028657619, 'batch_size': 27, 'step_size': 14, 'gamma': 0.9772806955025298}
Epoch 1/300, seasonal_0 Loss: 0.2456 | 0.1309
Epoch 2/300, seasonal_0 Loss: 0.1609 | 0.1396
Epoch 3/300, seasonal_0 Loss: 0.1350 | 0.0918
Epoch 4/300, seasonal_0 Loss: 0.1207 | 0.0827
Epoch 5/300, seasonal_0 Loss: 0.1109 | 0.0741
Epoch 6/300, seasonal_0 Loss: 0.1028 | 0.0656
Epoch 7/300, seasonal_0 Loss: 0.0952 | 0.0602
Epoch 8/300, seasonal_0 Loss: 0.0882 | 0.0567
Epoch 9/300, seasonal_0 Loss: 0.0833 | 0.0564
Epoch 10/300, seasonal_0 Loss: 0.0797 | 0.0566
Epoch 11/300, seasonal_0 Loss: 0.0775 | 0.0575
Epoch 12/300, seasonal_0 Loss: 0.0767 | 0.0666
Epoch 13/300, seasonal_0 Loss: 0.0782 | 0.0593
Epoch 14/300, seasonal_0 Loss: 0.0734 | 0.0569
Epoch 15/300, seasonal_0 Loss: 0.0708 | 0.0516
Epoch 16/300, seasonal_0 Loss: 0.0707 | 0.0592
Epoch 17/300, seasonal_0 Loss: 0.0696 | 0.0625
Epoch 18/300, seasonal_0 Loss: 0.0743 | 0.0631
Epoch 19/300, seasonal_0 Loss: 0.0732 | 0.0519
Epoch 20/300, seasonal_0 Loss: 0.0690 | 0.0585
Epoch 21/300, seasonal_0 Loss: 0.0693 | 0.0538
Epoch 22/300, seasonal_0 Loss: 0.0645 | 0.0496
Epoch 23/300, seasonal_0 Loss: 0.0626 | 0.0522
Epoch 24/300, seasonal_0 Loss: 0.0589 | 0.0592
Epoch 25/300, seasonal_0 Loss: 0.0594 | 0.0796
Epoch 26/300, seasonal_0 Loss: 0.0574 | 0.0479
Epoch 27/300, seasonal_0 Loss: 0.0564 | 0.0498
Epoch 28/300, seasonal_0 Loss: 0.0529 | 0.0405
Epoch 29/300, seasonal_0 Loss: 0.0523 | 0.0426
Epoch 30/300, seasonal_0 Loss: 0.0513 | 0.0473
Epoch 31/300, seasonal_0 Loss: 0.0510 | 0.0455
Epoch 32/300, seasonal_0 Loss: 0.0499 | 0.0384
Epoch 33/300, seasonal_0 Loss: 0.0524 | 0.0442
Epoch 34/300, seasonal_0 Loss: 0.0513 | 0.0406
Epoch 35/300, seasonal_0 Loss: 0.0572 | 0.0627
Epoch 36/300, seasonal_0 Loss: 0.0603 | 0.0659
Epoch 37/300, seasonal_0 Loss: 0.0562 | 0.0553
Epoch 38/300, seasonal_0 Loss: 0.0492 | 0.0447
Epoch 39/300, seasonal_0 Loss: 0.0500 | 0.0444
Epoch 40/300, seasonal_0 Loss: 0.0486 | 0.0402
Epoch 41/300, seasonal_0 Loss: 0.0463 | 0.0374
Epoch 42/300, seasonal_0 Loss: 0.0475 | 0.0420
Epoch 43/300, seasonal_0 Loss: 0.0478 | 0.0375
Epoch 44/300, seasonal_0 Loss: 0.0482 | 0.0398
Epoch 45/300, seasonal_0 Loss: 0.0487 | 0.0415
Epoch 46/300, seasonal_0 Loss: 0.0429 | 0.0462
Epoch 47/300, seasonal_0 Loss: 0.0390 | 0.0478
Epoch 48/300, seasonal_0 Loss: 0.0401 | 0.0459
Epoch 49/300, seasonal_0 Loss: 0.0453 | 0.0438
Epoch 50/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 51/300, seasonal_0 Loss: 0.0461 | 0.0399
Epoch 52/300, seasonal_0 Loss: 0.0456 | 0.0416
Epoch 53/300, seasonal_0 Loss: 0.0403 | 0.0437
Epoch 54/300, seasonal_0 Loss: 0.0386 | 0.0427
Epoch 55/300, seasonal_0 Loss: 0.0384 | 0.0443
Epoch 56/300, seasonal_0 Loss: 0.0390 | 0.0454
Epoch 57/300, seasonal_0 Loss: 0.0382 | 0.0458
Epoch 58/300, seasonal_0 Loss: 0.0404 | 0.0497
Epoch 59/300, seasonal_0 Loss: 0.0387 | 0.0510
Epoch 60/300, seasonal_0 Loss: 0.0361 | 0.0514
Epoch 61/300, seasonal_0 Loss: 0.0366 | 0.0513
Epoch 62/300, seasonal_0 Loss: 0.0348 | 0.0512
Epoch 63/300, seasonal_0 Loss: 0.0348 | 0.0528
Epoch 64/300, seasonal_0 Loss: 0.0338 | 0.0478
Epoch 65/300, seasonal_0 Loss: 0.0331 | 0.0498
Epoch 66/300, seasonal_0 Loss: 0.0306 | 0.0483
Epoch 67/300, seasonal_0 Loss: 0.0315 | 0.0509
Epoch 68/300, seasonal_0 Loss: 0.0319 | 0.0465
Epoch 69/300, seasonal_0 Loss: 0.0313 | 0.0522
Epoch 70/300, seasonal_0 Loss: 0.0314 | 0.0482
Epoch 71/300, seasonal_0 Loss: 0.0301 | 0.0532
Epoch 72/300, seasonal_0 Loss: 0.0308 | 0.0427
Epoch 73/300, seasonal_0 Loss: 0.0292 | 0.0420
Epoch 74/300, seasonal_0 Loss: 0.0338 | 0.0392
Epoch 75/300, seasonal_0 Loss: 0.0323 | 0.0433
Epoch 76/300, seasonal_0 Loss: 0.0325 | 0.0468
Epoch 77/300, seasonal_0 Loss: 0.0309 | 0.0412
Epoch 78/300, seasonal_0 Loss: 0.0340 | 0.0429
Epoch 79/300, seasonal_0 Loss: 0.0320 | 0.0481
Epoch 80/300, seasonal_0 Loss: 0.0333 | 0.0388
Epoch 81/300, seasonal_0 Loss: 0.0307 | 0.0481
Epoch 82/300, seasonal_0 Loss: 0.0363 | 0.0507
Epoch 83/300, seasonal_0 Loss: 0.0347 | 0.0426
Epoch 84/300, seasonal_0 Loss: 0.0400 | 0.0444
Epoch 85/300, seasonal_0 Loss: 0.0337 | 0.0357
Epoch 86/300, seasonal_0 Loss: 0.0330 | 0.0353
Epoch 87/300, seasonal_0 Loss: 0.0427 | 0.0407
Epoch 88/300, seasonal_0 Loss: 0.0400 | 0.0450
Epoch 89/300, seasonal_0 Loss: 0.0373 | 0.0443
Epoch 90/300, seasonal_0 Loss: 0.0300 | 0.0425
Epoch 91/300, seasonal_0 Loss: 0.0292 | 0.0454
Epoch 92/300, seasonal_0 Loss: 0.0281 | 0.0386
Epoch 93/300, seasonal_0 Loss: 0.0276 | 0.0424
Epoch 94/300, seasonal_0 Loss: 0.0311 | 0.0410
Epoch 95/300, seasonal_0 Loss: 0.0273 | 0.0382
Epoch 96/300, seasonal_0 Loss: 0.0372 | 0.0411
Epoch 97/300, seasonal_0 Loss: 0.0507 | 0.0637
Epoch 98/300, seasonal_0 Loss: 0.0466 | 0.0450
Epoch 99/300, seasonal_0 Loss: 0.0442 | 0.0420
Epoch 100/300, seasonal_0 Loss: 0.0386 | 0.0476
Epoch 101/300, seasonal_0 Loss: 0.0378 | 0.0465
Epoch 102/300, seasonal_0 Loss: 0.0404 | 0.0432
Epoch 103/300, seasonal_0 Loss: 0.0391 | 0.0444
Epoch 104/300, seasonal_0 Loss: 0.0387 | 0.0434
Epoch 105/300, seasonal_0 Loss: 0.0373 | 0.0436
Epoch 106/300, seasonal_0 Loss: 0.0372 | 0.0416
Epoch 107/300, seasonal_0 Loss: 0.0312 | 0.0431
Epoch 108/300, seasonal_0 Loss: 0.0280 | 0.0430
Epoch 109/300, seasonal_0 Loss: 0.0378 | 0.0448
Epoch 110/300, seasonal_0 Loss: 0.0273 | 0.0396
Epoch 111/300, seasonal_0 Loss: 0.0257 | 0.0431
Epoch 112/300, seasonal_0 Loss: 0.0236 | 0.0413
Epoch 113/300, seasonal_0 Loss: 0.0229 | 0.0434
Epoch 114/300, seasonal_0 Loss: 0.0230 | 0.0433
Epoch 115/300, seasonal_0 Loss: 0.0225 | 0.0440
Epoch 116/300, seasonal_0 Loss: 0.0239 | 0.0466
Epoch 117/300, seasonal_0 Loss: 0.0231 | 0.0450
Epoch 118/300, seasonal_0 Loss: 0.0214 | 0.0414
Epoch 119/300, seasonal_0 Loss: 0.0205 | 0.0457
Epoch 120/300, seasonal_0 Loss: 0.0222 | 0.0483
Epoch 121/300, seasonal_0 Loss: 0.0217 | 0.0453
Epoch 122/300, seasonal_0 Loss: 0.0210 | 0.0476
Epoch 123/300, seasonal_0 Loss: 0.0220 | 0.0465
Epoch 124/300, seasonal_0 Loss: 0.0215 | 0.0507
Epoch 125/300, seasonal_0 Loss: 0.0224 | 0.0488
Epoch 126/300, seasonal_0 Loss: 0.0211 | 0.0445
Epoch 127/300, seasonal_0 Loss: 0.0212 | 0.0452
Epoch 128/300, seasonal_0 Loss: 0.0214 | 0.0426
Epoch 129/300, seasonal_0 Loss: 0.0226 | 0.0443
Epoch 130/300, seasonal_0 Loss: 0.0268 | 0.0466
Epoch 131/300, seasonal_0 Loss: 0.0282 | 0.0549
Epoch 132/300, seasonal_0 Loss: 0.0257 | 0.0410
Epoch 133/300, seasonal_0 Loss: 0.0229 | 0.0454
Epoch 134/300, seasonal_0 Loss: 0.0216 | 0.0391
Epoch 135/300, seasonal_0 Loss: 0.0208 | 0.0397
Epoch 136/300, seasonal_0 Loss: 0.0207 | 0.0387
Epoch 137/300, seasonal_0 Loss: 0.0213 | 0.0408
Epoch 138/300, seasonal_0 Loss: 0.0208 | 0.0409
Epoch 139/300, seasonal_0 Loss: 0.0194 | 0.0424
Epoch 140/300, seasonal_0 Loss: 0.0189 | 0.0409
Epoch 141/300, seasonal_0 Loss: 0.0189 | 0.0402
Epoch 142/300, seasonal_0 Loss: 0.0180 | 0.0427
Epoch 143/300, seasonal_0 Loss: 0.0176 | 0.0468
Epoch 144/300, seasonal_0 Loss: 0.0167 | 0.0404
Epoch 145/300, seasonal_0 Loss: 0.0166 | 0.0441
Epoch 146/300, seasonal_0 Loss: 0.0173 | 0.0483
Epoch 147/300, seasonal_0 Loss: 0.0186 | 0.0399
Epoch 148/300, seasonal_0 Loss: 0.0188 | 0.0402
Epoch 149/300, seasonal_0 Loss: 0.0186 | 0.0411
Epoch 150/300, seasonal_0 Loss: 0.0187 | 0.0390
Epoch 151/300, seasonal_0 Loss: 0.0172 | 0.0397
Epoch 152/300, seasonal_0 Loss: 0.0162 | 0.0416
Epoch 153/300, seasonal_0 Loss: 0.0172 | 0.0405
Epoch 154/300, seasonal_0 Loss: 0.0184 | 0.0458
Epoch 155/300, seasonal_0 Loss: 0.0173 | 0.0398
Epoch 156/300, seasonal_0 Loss: 0.0173 | 0.0475
Epoch 157/300, seasonal_0 Loss: 0.0165 | 0.0423
Epoch 158/300, seasonal_0 Loss: 0.0153 | 0.0378
Epoch 159/300, seasonal_0 Loss: 0.0153 | 0.0410
Epoch 160/300, seasonal_0 Loss: 0.0160 | 0.0441
Epoch 161/300, seasonal_0 Loss: 0.0160 | 0.0336
Epoch 162/300, seasonal_0 Loss: 0.0154 | 0.0369
Epoch 163/300, seasonal_0 Loss: 0.0161 | 0.0390
Epoch 164/300, seasonal_0 Loss: 0.0147 | 0.0434
Epoch 165/300, seasonal_0 Loss: 0.0152 | 0.0402
Epoch 166/300, seasonal_0 Loss: 0.0154 | 0.0425
Epoch 167/300, seasonal_0 Loss: 0.0153 | 0.0400
Epoch 168/300, seasonal_0 Loss: 0.0147 | 0.0355
Epoch 169/300, seasonal_0 Loss: 0.0140 | 0.0432
Epoch 170/300, seasonal_0 Loss: 0.0138 | 0.0398
Epoch 171/300, seasonal_0 Loss: 0.0141 | 0.0441
Epoch 172/300, seasonal_0 Loss: 0.0143 | 0.0398
Epoch 173/300, seasonal_0 Loss: 0.0154 | 0.0407
Epoch 174/300, seasonal_0 Loss: 0.0149 | 0.0356
Epoch 175/300, seasonal_0 Loss: 0.0196 | 0.0404
Epoch 176/300, seasonal_0 Loss: 0.0169 | 0.0395
Epoch 177/300, seasonal_0 Loss: 0.0160 | 0.0353
Epoch 178/300, seasonal_0 Loss: 0.0173 | 0.0402
Epoch 179/300, seasonal_0 Loss: 0.0158 | 0.0324
Epoch 180/300, seasonal_0 Loss: 0.0205 | 0.0355
Epoch 181/300, seasonal_0 Loss: 0.0165 | 0.0407
Epoch 182/300, seasonal_0 Loss: 0.0159 | 0.0409
Epoch 183/300, seasonal_0 Loss: 0.0154 | 0.0395
Epoch 184/300, seasonal_0 Loss: 0.0144 | 0.0354
Epoch 185/300, seasonal_0 Loss: 0.0131 | 0.0324
Epoch 186/300, seasonal_0 Loss: 0.0123 | 0.0323
Epoch 187/300, seasonal_0 Loss: 0.0122 | 0.0331
Epoch 188/300, seasonal_0 Loss: 0.0119 | 0.0328
Epoch 189/300, seasonal_0 Loss: 0.0119 | 0.0332
Epoch 190/300, seasonal_0 Loss: 0.0123 | 0.0335
Epoch 191/300, seasonal_0 Loss: 0.0127 | 0.0345
Epoch 192/300, seasonal_0 Loss: 0.0132 | 0.0396
Epoch 193/300, seasonal_0 Loss: 0.0136 | 0.0491
Epoch 194/300, seasonal_0 Loss: 0.0140 | 0.0358
Epoch 195/300, seasonal_0 Loss: 0.0130 | 0.0372
Epoch 196/300, seasonal_0 Loss: 0.0126 | 0.0310
Epoch 197/300, seasonal_0 Loss: 0.0121 | 0.0352
Epoch 198/300, seasonal_0 Loss: 0.0115 | 0.0341
Epoch 199/300, seasonal_0 Loss: 0.0113 | 0.0338
Epoch 200/300, seasonal_0 Loss: 0.0115 | 0.0352
Epoch 201/300, seasonal_0 Loss: 0.0115 | 0.0376
Epoch 202/300, seasonal_0 Loss: 0.0124 | 0.0385
Epoch 203/300, seasonal_0 Loss: 0.0121 | 0.0374
Epoch 204/300, seasonal_0 Loss: 0.0125 | 0.0429
Epoch 205/300, seasonal_0 Loss: 0.0121 | 0.0357
Epoch 206/300, seasonal_0 Loss: 0.0121 | 0.0348
Epoch 207/300, seasonal_0 Loss: 0.0112 | 0.0370
Epoch 208/300, seasonal_0 Loss: 0.0110 | 0.0345
Epoch 209/300, seasonal_0 Loss: 0.0112 | 0.0358
Epoch 210/300, seasonal_0 Loss: 0.0107 | 0.0397
Epoch 211/300, seasonal_0 Loss: 0.0103 | 0.0387
Epoch 212/300, seasonal_0 Loss: 0.0101 | 0.0514
Epoch 213/300, seasonal_0 Loss: 0.0110 | 0.0490
Epoch 214/300, seasonal_0 Loss: 0.0115 | 0.0498
Epoch 215/300, seasonal_0 Loss: 0.0114 | 0.0543
Epoch 216/300, seasonal_0 Loss: 0.0119 | 0.0418
Epoch 217/300, seasonal_0 Loss: 0.0140 | 0.0416
Epoch 218/300, seasonal_0 Loss: 0.0219 | 0.0425
Epoch 219/300, seasonal_0 Loss: 0.0365 | 0.0378
Epoch 220/300, seasonal_0 Loss: 0.0343 | 0.0451
Epoch 221/300, seasonal_0 Loss: 0.0287 | 0.0580
Epoch 222/300, seasonal_0 Loss: 0.0384 | 0.0641
Epoch 223/300, seasonal_0 Loss: 0.0356 | 0.0469
Epoch 224/300, seasonal_0 Loss: 0.0335 | 0.0433
Epoch 225/300, seasonal_0 Loss: 0.0297 | 0.0418
Epoch 226/300, seasonal_0 Loss: 0.0353 | 0.0375
Epoch 227/300, seasonal_0 Loss: 0.0353 | 0.0363
Epoch 228/300, seasonal_0 Loss: 0.0331 | 0.0355
Epoch 229/300, seasonal_0 Loss: 0.0310 | 0.0360
Epoch 230/300, seasonal_0 Loss: 0.0258 | 0.0359
Epoch 231/300, seasonal_0 Loss: 0.0297 | 0.0371
Epoch 232/300, seasonal_0 Loss: 0.0329 | 0.0467
Epoch 233/300, seasonal_0 Loss: 0.0316 | 0.0448
Epoch 234/300, seasonal_0 Loss: 0.0415 | 0.0432
Epoch 235/300, seasonal_0 Loss: 0.0283 | 0.0412
Epoch 236/300, seasonal_0 Loss: 0.0238 | 0.0376
Epoch 237/300, seasonal_0 Loss: 0.0353 | 0.0360
Epoch 238/300, seasonal_0 Loss: 0.0317 | 0.0356
Epoch 239/300, seasonal_0 Loss: 0.0249 | 0.0357
Epoch 240/300, seasonal_0 Loss: 0.0134 | 0.0372
Epoch 241/300, seasonal_0 Loss: 0.0128 | 0.0383
Epoch 242/300, seasonal_0 Loss: 0.0112 | 0.0363
Epoch 243/300, seasonal_0 Loss: 0.0105 | 0.0403
Epoch 244/300, seasonal_0 Loss: 0.0099 | 0.0450
Epoch 245/300, seasonal_0 Loss: 0.0093 | 0.0376
Epoch 246/300, seasonal_0 Loss: 0.0095 | 0.0354
Epoch 247/300, seasonal_0 Loss: 0.0095 | 0.0350
Epoch 248/300, seasonal_0 Loss: 0.0099 | 0.0480
Epoch 249/300, seasonal_0 Loss: 0.0098 | 0.0561
Epoch 250/300, seasonal_0 Loss: 0.0097 | 0.0364
Epoch 251/300, seasonal_0 Loss: 0.0096 | 0.0359
Epoch 252/300, seasonal_0 Loss: 0.0095 | 0.0399
Epoch 253/300, seasonal_0 Loss: 0.0091 | 0.0351
Epoch 254/300, seasonal_0 Loss: 0.0092 | 0.0361
Epoch 255/300, seasonal_0 Loss: 0.0088 | 0.0371
Epoch 256/300, seasonal_0 Loss: 0.0094 | 0.0342
Epoch 257/300, seasonal_0 Loss: 0.0102 | 0.0428
Epoch 258/300, seasonal_0 Loss: 0.0106 | 0.0445
Epoch 259/300, seasonal_0 Loss: 0.0120 | 0.0384
Epoch 260/300, seasonal_0 Loss: 0.0112 | 0.0340
Epoch 261/300, seasonal_0 Loss: 0.0107 | 0.0362
Epoch 262/300, seasonal_0 Loss: 0.0096 | 0.0628
Epoch 263/300, seasonal_0 Loss: 0.0091 | 0.0448
Epoch 264/300, seasonal_0 Loss: 0.0087 | 0.0324
Epoch 265/300, seasonal_0 Loss: 0.0086 | 0.0436
Epoch 266/300, seasonal_0 Loss: 0.0083 | 0.0596
Epoch 267/300, seasonal_0 Loss: 0.0081 | 0.0694
Epoch 268/300, seasonal_0 Loss: 0.0088 | 0.0509
Epoch 269/300, seasonal_0 Loss: 0.0090 | 0.0513
Epoch 270/300, seasonal_0 Loss: 0.0092 | 0.0522
Epoch 271/300, seasonal_0 Loss: 0.0122 | 0.0498
Epoch 272/300, seasonal_0 Loss: 0.0101 | 0.0511
Epoch 273/300, seasonal_0 Loss: 0.0099 | 0.0551
Epoch 274/300, seasonal_0 Loss: 0.0108 | 0.0388
Epoch 275/300, seasonal_0 Loss: 0.0118 | 0.0842
Epoch 276/300, seasonal_0 Loss: 0.0118 | 0.0556
Epoch 277/300, seasonal_0 Loss: 0.0130 | 0.0715
Epoch 278/300, seasonal_0 Loss: 0.0114 | 0.0507
Epoch 279/300, seasonal_0 Loss: 0.0105 | 0.0537
Epoch 280/300, seasonal_0 Loss: 0.0112 | 0.0569
Epoch 281/300, seasonal_0 Loss: 0.0118 | 0.0668
Epoch 282/300, seasonal_0 Loss: 0.0113 | 0.0554
Epoch 283/300, seasonal_0 Loss: 0.0115 | 0.0546
Epoch 284/300, seasonal_0 Loss: 0.0097 | 0.0466
Epoch 285/300, seasonal_0 Loss: 0.0089 | 0.0663
Epoch 286/300, seasonal_0 Loss: 0.0089 | 0.0537
Epoch 287/300, seasonal_0 Loss: 0.0102 | 0.0561
Epoch 288/300, seasonal_0 Loss: 0.0095 | 0.0617
Epoch 289/300, seasonal_0 Loss: 0.0088 | 0.0430
Epoch 290/300, seasonal_0 Loss: 0.0083 | 0.0394
Epoch 291/300, seasonal_0 Loss: 0.0077 | 0.0411
Epoch 292/300, seasonal_0 Loss: 0.0075 | 0.0453
Epoch 293/300, seasonal_0 Loss: 0.0082 | 0.0441
Epoch 294/300, seasonal_0 Loss: 0.0084 | 0.0439
Epoch 295/300, seasonal_0 Loss: 0.0089 | 0.0410
Epoch 296/300, seasonal_0 Loss: 0.0093 | 0.0470
Epoch 297/300, seasonal_0 Loss: 0.0104 | 0.0493
Epoch 298/300, seasonal_0 Loss: 0.0135 | 0.0559
Epoch 299/300, seasonal_0 Loss: 0.0100 | 0.0676
Epoch 300/300, seasonal_0 Loss: 0.0102 | 0.0538
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.694960824811071, 'learning_rate': 0.00012363174461713704, 'batch_size': 97, 'step_size': 14, 'gamma': 0.9854710800963754}
Epoch 1/300, seasonal_1 Loss: 0.3012 | 0.2625
Epoch 2/300, seasonal_1 Loss: 0.1968 | 0.1870
Epoch 3/300, seasonal_1 Loss: 0.1705 | 0.1714
Epoch 4/300, seasonal_1 Loss: 0.1570 | 0.1504
Epoch 5/300, seasonal_1 Loss: 0.1461 | 0.1407
Epoch 6/300, seasonal_1 Loss: 0.1401 | 0.1335
Epoch 7/300, seasonal_1 Loss: 0.1369 | 0.1252
Epoch 8/300, seasonal_1 Loss: 0.1357 | 0.1193
Epoch 9/300, seasonal_1 Loss: 0.1354 | 0.1150
Epoch 10/300, seasonal_1 Loss: 0.1346 | 0.1123
Epoch 11/300, seasonal_1 Loss: 0.1341 | 0.1109
Epoch 12/300, seasonal_1 Loss: 0.1338 | 0.1101
Epoch 13/300, seasonal_1 Loss: 0.1331 | 0.1095
Epoch 14/300, seasonal_1 Loss: 0.1319 | 0.1091
Epoch 15/300, seasonal_1 Loss: 0.1301 | 0.1093
Epoch 16/300, seasonal_1 Loss: 0.1272 | 0.1080
Epoch 17/300, seasonal_1 Loss: 0.1209 | 0.1042
Epoch 18/300, seasonal_1 Loss: 0.1142 | 0.1001
Epoch 19/300, seasonal_1 Loss: 0.1106 | 0.0978
Epoch 20/300, seasonal_1 Loss: 0.1087 | 0.0965
Epoch 21/300, seasonal_1 Loss: 0.1072 | 0.0954
Epoch 22/300, seasonal_1 Loss: 0.1057 | 0.0943
Epoch 23/300, seasonal_1 Loss: 0.1044 | 0.0931
Epoch 24/300, seasonal_1 Loss: 0.1033 | 0.0917
Epoch 25/300, seasonal_1 Loss: 0.1026 | 0.0902
Epoch 26/300, seasonal_1 Loss: 0.1021 | 0.0887
Epoch 27/300, seasonal_1 Loss: 0.1017 | 0.0872
Epoch 28/300, seasonal_1 Loss: 0.1013 | 0.0860
Epoch 29/300, seasonal_1 Loss: 0.1009 | 0.0850
Epoch 30/300, seasonal_1 Loss: 0.1006 | 0.0840
Epoch 31/300, seasonal_1 Loss: 0.1000 | 0.0831
Epoch 32/300, seasonal_1 Loss: 0.0994 | 0.0822
Epoch 33/300, seasonal_1 Loss: 0.0988 | 0.0813
Epoch 34/300, seasonal_1 Loss: 0.0980 | 0.0804
Epoch 35/300, seasonal_1 Loss: 0.0972 | 0.0797
Epoch 36/300, seasonal_1 Loss: 0.0965 | 0.0791
Epoch 37/300, seasonal_1 Loss: 0.0960 | 0.0786
Epoch 38/300, seasonal_1 Loss: 0.0953 | 0.0781
Epoch 39/300, seasonal_1 Loss: 0.0945 | 0.0777
Epoch 40/300, seasonal_1 Loss: 0.0937 | 0.0773
Epoch 41/300, seasonal_1 Loss: 0.0930 | 0.0769
Epoch 42/300, seasonal_1 Loss: 0.0923 | 0.0765
Epoch 43/300, seasonal_1 Loss: 0.0916 | 0.0762
Epoch 44/300, seasonal_1 Loss: 0.0909 | 0.0759
Epoch 45/300, seasonal_1 Loss: 0.0901 | 0.0754
Epoch 46/300, seasonal_1 Loss: 0.0895 | 0.0757
Epoch 47/300, seasonal_1 Loss: 0.0889 | 0.0754
Epoch 48/300, seasonal_1 Loss: 0.0892 | 0.0833
Epoch 49/300, seasonal_1 Loss: 0.0922 | 0.0874
Epoch 50/300, seasonal_1 Loss: 0.0936 | 0.0934
Epoch 51/300, seasonal_1 Loss: 0.0962 | 0.0817
Epoch 52/300, seasonal_1 Loss: 0.0906 | 0.0768
Epoch 53/300, seasonal_1 Loss: 0.0907 | 0.0790
Epoch 54/300, seasonal_1 Loss: 0.0899 | 0.0797
Epoch 55/300, seasonal_1 Loss: 0.0908 | 0.0784
Epoch 56/300, seasonal_1 Loss: 0.0899 | 0.0820
Epoch 57/300, seasonal_1 Loss: 0.0916 | 0.0789
Epoch 58/300, seasonal_1 Loss: 0.0904 | 0.0822
Epoch 59/300, seasonal_1 Loss: 0.0924 | 0.0785
Epoch 60/300, seasonal_1 Loss: 0.0911 | 0.0810
Epoch 61/300, seasonal_1 Loss: 0.0928 | 0.0781
Epoch 62/300, seasonal_1 Loss: 0.0907 | 0.0790
Epoch 63/300, seasonal_1 Loss: 0.0922 | 0.0774
Epoch 64/300, seasonal_1 Loss: 0.0905 | 0.0791
Epoch 65/300, seasonal_1 Loss: 0.0900 | 0.0762
Epoch 66/300, seasonal_1 Loss: 0.0866 | 0.0791
Epoch 67/300, seasonal_1 Loss: 0.0860 | 0.0735
Epoch 68/300, seasonal_1 Loss: 0.0851 | 0.0800
Epoch 69/300, seasonal_1 Loss: 0.0855 | 0.0733
Epoch 70/300, seasonal_1 Loss: 0.0851 | 0.0807
Epoch 71/300, seasonal_1 Loss: 0.0854 | 0.0732
Epoch 72/300, seasonal_1 Loss: 0.0848 | 0.0811
Epoch 73/300, seasonal_1 Loss: 0.0848 | 0.0725
Epoch 74/300, seasonal_1 Loss: 0.0842 | 0.0803
Epoch 75/300, seasonal_1 Loss: 0.0838 | 0.0716
Epoch 76/300, seasonal_1 Loss: 0.0833 | 0.0791
Epoch 77/300, seasonal_1 Loss: 0.0829 | 0.0709
Epoch 78/300, seasonal_1 Loss: 0.0823 | 0.0781
Epoch 79/300, seasonal_1 Loss: 0.0821 | 0.0703
Epoch 80/300, seasonal_1 Loss: 0.0813 | 0.0775
Epoch 81/300, seasonal_1 Loss: 0.0815 | 0.0700
Epoch 82/300, seasonal_1 Loss: 0.0807 | 0.0770
Epoch 83/300, seasonal_1 Loss: 0.0811 | 0.0696
Epoch 84/300, seasonal_1 Loss: 0.0803 | 0.0760
Epoch 85/300, seasonal_1 Loss: 0.0807 | 0.0692
Epoch 86/300, seasonal_1 Loss: 0.0801 | 0.0748
Epoch 87/300, seasonal_1 Loss: 0.0804 | 0.0686
Epoch 88/300, seasonal_1 Loss: 0.0797 | 0.0739
Epoch 89/300, seasonal_1 Loss: 0.0801 | 0.0683
Epoch 90/300, seasonal_1 Loss: 0.0793 | 0.0737
Epoch 91/300, seasonal_1 Loss: 0.0799 | 0.0682
Epoch 92/300, seasonal_1 Loss: 0.0790 | 0.0736
Epoch 93/300, seasonal_1 Loss: 0.0798 | 0.0679
Epoch 94/300, seasonal_1 Loss: 0.0788 | 0.0733
Epoch 95/300, seasonal_1 Loss: 0.0796 | 0.0675
Epoch 96/300, seasonal_1 Loss: 0.0785 | 0.0728
Epoch 97/300, seasonal_1 Loss: 0.0792 | 0.0672
Epoch 98/300, seasonal_1 Loss: 0.0782 | 0.0722
Epoch 99/300, seasonal_1 Loss: 0.0788 | 0.0667
Epoch 100/300, seasonal_1 Loss: 0.0779 | 0.0716
Epoch 101/300, seasonal_1 Loss: 0.0782 | 0.0662
Epoch 102/300, seasonal_1 Loss: 0.0776 | 0.0712
Epoch 103/300, seasonal_1 Loss: 0.0778 | 0.0659
Epoch 104/300, seasonal_1 Loss: 0.0772 | 0.0706
Epoch 105/300, seasonal_1 Loss: 0.0776 | 0.0658
Epoch 106/300, seasonal_1 Loss: 0.0773 | 0.0702
Epoch 107/300, seasonal_1 Loss: 0.0770 | 0.0650
Epoch 108/300, seasonal_1 Loss: 0.0764 | 0.0700
Epoch 109/300, seasonal_1 Loss: 0.0775 | 0.0655
Epoch 110/300, seasonal_1 Loss: 0.0770 | 0.0689
Epoch 111/300, seasonal_1 Loss: 0.0794 | 0.0668
Epoch 112/300, seasonal_1 Loss: 0.0774 | 0.0698
Epoch 113/300, seasonal_1 Loss: 0.0769 | 0.0638
Epoch 114/300, seasonal_1 Loss: 0.0762 | 0.0679
Epoch 115/300, seasonal_1 Loss: 0.0758 | 0.0640
Epoch 116/300, seasonal_1 Loss: 0.0756 | 0.0702
Epoch 117/300, seasonal_1 Loss: 0.0768 | 0.0653
Epoch 118/300, seasonal_1 Loss: 0.0761 | 0.0703
Epoch 119/300, seasonal_1 Loss: 0.0762 | 0.0647
Epoch 120/300, seasonal_1 Loss: 0.0763 | 0.0710
Epoch 121/300, seasonal_1 Loss: 0.0814 | 0.0710
Epoch 122/300, seasonal_1 Loss: 0.0792 | 0.0750
Epoch 123/300, seasonal_1 Loss: 0.0787 | 0.0647
Epoch 124/300, seasonal_1 Loss: 0.0780 | 0.0723
Epoch 125/300, seasonal_1 Loss: 0.0786 | 0.0646
Epoch 126/300, seasonal_1 Loss: 0.0776 | 0.0717
Epoch 127/300, seasonal_1 Loss: 0.0775 | 0.0648
Epoch 128/300, seasonal_1 Loss: 0.0778 | 0.0718
Epoch 129/300, seasonal_1 Loss: 0.0779 | 0.0641
Epoch 130/300, seasonal_1 Loss: 0.0777 | 0.0706
Epoch 131/300, seasonal_1 Loss: 0.0769 | 0.0639
Epoch 132/300, seasonal_1 Loss: 0.0767 | 0.0710
Epoch 133/300, seasonal_1 Loss: 0.0751 | 0.0639
Epoch 134/300, seasonal_1 Loss: 0.0756 | 0.0694
Epoch 135/300, seasonal_1 Loss: 0.0748 | 0.0637
Epoch 136/300, seasonal_1 Loss: 0.0752 | 0.0689
Epoch 137/300, seasonal_1 Loss: 0.0743 | 0.0641
Epoch 138/300, seasonal_1 Loss: 0.0753 | 0.0685
Epoch 139/300, seasonal_1 Loss: 0.0747 | 0.0658
Epoch 140/300, seasonal_1 Loss: 0.0761 | 0.0693
Epoch 141/300, seasonal_1 Loss: 0.0743 | 0.0639
Epoch 142/300, seasonal_1 Loss: 0.0742 | 0.0683
Epoch 143/300, seasonal_1 Loss: 0.0742 | 0.0636
Epoch 144/300, seasonal_1 Loss: 0.0741 | 0.0670
Epoch 145/300, seasonal_1 Loss: 0.0744 | 0.0634
Epoch 146/300, seasonal_1 Loss: 0.0743 | 0.0674
Epoch 147/300, seasonal_1 Loss: 0.0757 | 0.0642
Epoch 148/300, seasonal_1 Loss: 0.0752 | 0.0670
Epoch 149/300, seasonal_1 Loss: 0.0761 | 0.0644
Epoch 150/300, seasonal_1 Loss: 0.0751 | 0.0662
Epoch 151/300, seasonal_1 Loss: 0.0758 | 0.0644
Epoch 152/300, seasonal_1 Loss: 0.0756 | 0.0669
Epoch 153/300, seasonal_1 Loss: 0.0763 | 0.0647
Epoch 154/300, seasonal_1 Loss: 0.0755 | 0.0675
Epoch 155/300, seasonal_1 Loss: 0.0759 | 0.0638
Epoch 156/300, seasonal_1 Loss: 0.0748 | 0.0671
Epoch 157/300, seasonal_1 Loss: 0.0747 | 0.0630
Epoch 158/300, seasonal_1 Loss: 0.0745 | 0.0666
Epoch 159/300, seasonal_1 Loss: 0.0753 | 0.0629
Epoch 160/300, seasonal_1 Loss: 0.0743 | 0.0679
Epoch 161/300, seasonal_1 Loss: 0.0765 | 0.0631
Epoch 162/300, seasonal_1 Loss: 0.0738 | 0.0670
Epoch 163/300, seasonal_1 Loss: 0.0740 | 0.0619
Epoch 164/300, seasonal_1 Loss: 0.0730 | 0.0639
Epoch 165/300, seasonal_1 Loss: 0.0725 | 0.0618
Epoch 166/300, seasonal_1 Loss: 0.0728 | 0.0647
Epoch 167/300, seasonal_1 Loss: 0.0732 | 0.0629
Epoch 168/300, seasonal_1 Loss: 0.0731 | 0.0665
Epoch 169/300, seasonal_1 Loss: 0.0720 | 0.0628
Epoch 170/300, seasonal_1 Loss: 0.0715 | 0.0659
Epoch 171/300, seasonal_1 Loss: 0.0717 | 0.0621
Epoch 172/300, seasonal_1 Loss: 0.0719 | 0.0653
Epoch 173/300, seasonal_1 Loss: 0.0721 | 0.0630
Epoch 174/300, seasonal_1 Loss: 0.0720 | 0.0663
Epoch 175/300, seasonal_1 Loss: 0.0719 | 0.0626
Epoch 176/300, seasonal_1 Loss: 0.0721 | 0.0689
Epoch 177/300, seasonal_1 Loss: 0.0733 | 0.0628
Epoch 178/300, seasonal_1 Loss: 0.0718 | 0.0685
Epoch 179/300, seasonal_1 Loss: 0.0707 | 0.0624
Epoch 180/300, seasonal_1 Loss: 0.0715 | 0.0688
Epoch 181/300, seasonal_1 Loss: 0.0713 | 0.0625
Epoch 182/300, seasonal_1 Loss: 0.0706 | 0.0682
Epoch 183/300, seasonal_1 Loss: 0.0707 | 0.0630
Epoch 184/300, seasonal_1 Loss: 0.0704 | 0.0708
Epoch 185/300, seasonal_1 Loss: 0.0702 | 0.0635
Epoch 186/300, seasonal_1 Loss: 0.0699 | 0.0698
Epoch 187/300, seasonal_1 Loss: 0.0693 | 0.0634
Epoch 188/300, seasonal_1 Loss: 0.0696 | 0.0694
Epoch 189/300, seasonal_1 Loss: 0.0691 | 0.0626
Epoch 190/300, seasonal_1 Loss: 0.0694 | 0.0681
Epoch 191/300, seasonal_1 Loss: 0.0690 | 0.0620
Epoch 192/300, seasonal_1 Loss: 0.0695 | 0.0668
Epoch 193/300, seasonal_1 Loss: 0.0690 | 0.0616
Epoch 194/300, seasonal_1 Loss: 0.0696 | 0.0667
Epoch 195/300, seasonal_1 Loss: 0.0697 | 0.0618
Epoch 196/300, seasonal_1 Loss: 0.0706 | 0.0667
Epoch 197/300, seasonal_1 Loss: 0.0708 | 0.0624
Epoch 198/300, seasonal_1 Loss: 0.0710 | 0.0652
Epoch 199/300, seasonal_1 Loss: 0.0716 | 0.0633
Epoch 200/300, seasonal_1 Loss: 0.0711 | 0.0661
Epoch 201/300, seasonal_1 Loss: 0.0711 | 0.0632
Epoch 202/300, seasonal_1 Loss: 0.0701 | 0.0658
Epoch 203/300, seasonal_1 Loss: 0.0703 | 0.0628
Epoch 204/300, seasonal_1 Loss: 0.0693 | 0.0638
Epoch 205/300, seasonal_1 Loss: 0.0696 | 0.0626
Epoch 206/300, seasonal_1 Loss: 0.0688 | 0.0641
Epoch 207/300, seasonal_1 Loss: 0.0691 | 0.0621
Epoch 208/300, seasonal_1 Loss: 0.0682 | 0.0648
Epoch 209/300, seasonal_1 Loss: 0.0683 | 0.0621
Epoch 210/300, seasonal_1 Loss: 0.0678 | 0.0658
Epoch 211/300, seasonal_1 Loss: 0.0683 | 0.0624
Epoch 212/300, seasonal_1 Loss: 0.0685 | 0.0682
Epoch 213/300, seasonal_1 Loss: 0.0693 | 0.0635
Epoch 214/300, seasonal_1 Loss: 0.0700 | 0.0681
Epoch 215/300, seasonal_1 Loss: 0.0712 | 0.0641
Epoch 216/300, seasonal_1 Loss: 0.0758 | 0.0715
Epoch 217/300, seasonal_1 Loss: 0.0776 | 0.0726
Epoch 218/300, seasonal_1 Loss: 0.0763 | 0.0701
Epoch 219/300, seasonal_1 Loss: 0.0732 | 0.0704
Epoch 220/300, seasonal_1 Loss: 0.0743 | 0.0701
Epoch 221/300, seasonal_1 Loss: 0.0697 | 0.0652
Epoch 222/300, seasonal_1 Loss: 0.0700 | 0.0702
Epoch 223/300, seasonal_1 Loss: 0.0683 | 0.0627
Epoch 224/300, seasonal_1 Loss: 0.0677 | 0.0662
Epoch 225/300, seasonal_1 Loss: 0.0672 | 0.0624
Epoch 226/300, seasonal_1 Loss: 0.0675 | 0.0631
Epoch 227/300, seasonal_1 Loss: 0.0670 | 0.0618
Epoch 228/300, seasonal_1 Loss: 0.0668 | 0.0631
Epoch 229/300, seasonal_1 Loss: 0.0666 | 0.0616
Epoch 230/300, seasonal_1 Loss: 0.0664 | 0.0637
Epoch 231/300, seasonal_1 Loss: 0.0664 | 0.0615
Epoch 232/300, seasonal_1 Loss: 0.0660 | 0.0641
Epoch 233/300, seasonal_1 Loss: 0.0658 | 0.0615
Epoch 234/300, seasonal_1 Loss: 0.0658 | 0.0643
Epoch 235/300, seasonal_1 Loss: 0.0658 | 0.0617
Epoch 236/300, seasonal_1 Loss: 0.0661 | 0.0649
Epoch 237/300, seasonal_1 Loss: 0.0661 | 0.0618
Epoch 238/300, seasonal_1 Loss: 0.0663 | 0.0657
Epoch 239/300, seasonal_1 Loss: 0.0661 | 0.0620
Epoch 240/300, seasonal_1 Loss: 0.0661 | 0.0661
Epoch 241/300, seasonal_1 Loss: 0.0664 | 0.0637
Epoch 242/300, seasonal_1 Loss: 0.0670 | 0.0669
Epoch 243/300, seasonal_1 Loss: 0.0674 | 0.0645
Epoch 244/300, seasonal_1 Loss: 0.0673 | 0.0676
Epoch 245/300, seasonal_1 Loss: 0.0668 | 0.0627
Epoch 246/300, seasonal_1 Loss: 0.0680 | 0.0665
Epoch 247/300, seasonal_1 Loss: 0.0741 | 0.0678
Epoch 248/300, seasonal_1 Loss: 0.0711 | 0.0694
Epoch 249/300, seasonal_1 Loss: 0.0688 | 0.0638
Epoch 250/300, seasonal_1 Loss: 0.0665 | 0.0669
Epoch 251/300, seasonal_1 Loss: 0.0657 | 0.0619
Epoch 252/300, seasonal_1 Loss: 0.0658 | 0.0641
Epoch 253/300, seasonal_1 Loss: 0.0657 | 0.0623
Epoch 254/300, seasonal_1 Loss: 0.0658 | 0.0641
Epoch 255/300, seasonal_1 Loss: 0.0658 | 0.0623
Epoch 256/300, seasonal_1 Loss: 0.0660 | 0.0648
Epoch 257/300, seasonal_1 Loss: 0.0663 | 0.0628
Epoch 258/300, seasonal_1 Loss: 0.0666 | 0.0661
Epoch 259/300, seasonal_1 Loss: 0.0675 | 0.0641
Epoch 260/300, seasonal_1 Loss: 0.0675 | 0.0677
Epoch 261/300, seasonal_1 Loss: 0.0684 | 0.0639
Epoch 262/300, seasonal_1 Loss: 0.0680 | 0.0676
Epoch 263/300, seasonal_1 Loss: 0.0722 | 0.0673
Epoch 264/300, seasonal_1 Loss: 0.0690 | 0.0659
Epoch 265/300, seasonal_1 Loss: 0.0684 | 0.0633
Epoch 266/300, seasonal_1 Loss: 0.0703 | 0.0651
Epoch 267/300, seasonal_1 Loss: 0.0717 | 0.0644
Epoch 268/300, seasonal_1 Loss: 0.0715 | 0.0667
Epoch 269/300, seasonal_1 Loss: 0.0746 | 0.0668
Epoch 270/300, seasonal_1 Loss: 0.0753 | 0.0668
Epoch 271/300, seasonal_1 Loss: 0.0754 | 0.0664
Epoch 272/300, seasonal_1 Loss: 0.0717 | 0.0659
Epoch 273/300, seasonal_1 Loss: 0.0735 | 0.0661
Epoch 274/300, seasonal_1 Loss: 0.0744 | 0.0671
Epoch 275/300, seasonal_1 Loss: 0.0773 | 0.0652
Epoch 276/300, seasonal_1 Loss: 0.0702 | 0.0665
Epoch 277/300, seasonal_1 Loss: 0.0677 | 0.0643
Epoch 278/300, seasonal_1 Loss: 0.0671 | 0.0681
Epoch 279/300, seasonal_1 Loss: 0.0669 | 0.0639
Epoch 280/300, seasonal_1 Loss: 0.0666 | 0.0697
Epoch 281/300, seasonal_1 Loss: 0.0671 | 0.0657
Epoch 282/300, seasonal_1 Loss: 0.0668 | 0.0707
Epoch 283/300, seasonal_1 Loss: 0.0656 | 0.0642
Epoch 284/300, seasonal_1 Loss: 0.0648 | 0.0670
Epoch 285/300, seasonal_1 Loss: 0.0638 | 0.0629
Epoch 286/300, seasonal_1 Loss: 0.0634 | 0.0649
Epoch 287/300, seasonal_1 Loss: 0.0633 | 0.0624
Epoch 288/300, seasonal_1 Loss: 0.0636 | 0.0635
Epoch 289/300, seasonal_1 Loss: 0.0636 | 0.0624
Epoch 290/300, seasonal_1 Loss: 0.0633 | 0.0633
Epoch 291/300, seasonal_1 Loss: 0.0629 | 0.0622
Epoch 292/300, seasonal_1 Loss: 0.0622 | 0.0642
Epoch 293/300, seasonal_1 Loss: 0.0618 | 0.0624
Epoch 294/300, seasonal_1 Loss: 0.0613 | 0.0663
Epoch 295/300, seasonal_1 Loss: 0.0605 | 0.0630
Epoch 296/300, seasonal_1 Loss: 0.0596 | 0.0681
Epoch 297/300, seasonal_1 Loss: 0.0590 | 0.0632
Epoch 298/300, seasonal_1 Loss: 0.0628 | 0.0695
Epoch 299/300, seasonal_1 Loss: 0.0595 | 0.0636
Epoch 300/300, seasonal_1 Loss: 0.0582 | 0.0703
Training seasonal_2 component with params: {'observation_period_num': 18, 'train_rates': 0.9875570188288523, 'learning_rate': 0.0006171957612463179, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8926433335235231}
Epoch 1/300, seasonal_2 Loss: 0.2346 | 0.0890
Epoch 2/300, seasonal_2 Loss: 0.1355 | 0.0720
Epoch 3/300, seasonal_2 Loss: 0.1179 | 0.0665
Epoch 4/300, seasonal_2 Loss: 0.1078 | 0.0613
Epoch 5/300, seasonal_2 Loss: 0.0996 | 0.0555
Epoch 6/300, seasonal_2 Loss: 0.0923 | 0.0498
Epoch 7/300, seasonal_2 Loss: 0.0869 | 0.0482
Epoch 8/300, seasonal_2 Loss: 0.0825 | 0.0443
Epoch 9/300, seasonal_2 Loss: 0.0790 | 0.0388
Epoch 10/300, seasonal_2 Loss: 0.0799 | 0.0389
Epoch 11/300, seasonal_2 Loss: 0.0760 | 0.0349
Epoch 12/300, seasonal_2 Loss: 0.0758 | 0.0334
Epoch 13/300, seasonal_2 Loss: 0.0720 | 0.0310
Epoch 14/300, seasonal_2 Loss: 0.0697 | 0.0308
Epoch 15/300, seasonal_2 Loss: 0.0736 | 0.0296
Epoch 16/300, seasonal_2 Loss: 0.0699 | 0.0277
Epoch 17/300, seasonal_2 Loss: 0.0663 | 0.0270
Epoch 18/300, seasonal_2 Loss: 0.0660 | 0.0265
Epoch 19/300, seasonal_2 Loss: 0.0659 | 0.0277
Epoch 20/300, seasonal_2 Loss: 0.0646 | 0.0256
Epoch 21/300, seasonal_2 Loss: 0.0630 | 0.0256
Epoch 22/300, seasonal_2 Loss: 0.0627 | 0.0250
Epoch 23/300, seasonal_2 Loss: 0.0608 | 0.0245
Epoch 24/300, seasonal_2 Loss: 0.0599 | 0.0239
Epoch 25/300, seasonal_2 Loss: 0.0590 | 0.0235
Epoch 26/300, seasonal_2 Loss: 0.0582 | 0.0240
Epoch 27/300, seasonal_2 Loss: 0.0577 | 0.0237
Epoch 28/300, seasonal_2 Loss: 0.0571 | 0.0239
Epoch 29/300, seasonal_2 Loss: 0.0569 | 0.0279
Epoch 30/300, seasonal_2 Loss: 0.0567 | 0.0286
Epoch 31/300, seasonal_2 Loss: 0.0561 | 0.0273
Epoch 32/300, seasonal_2 Loss: 0.0553 | 0.0264
Epoch 33/300, seasonal_2 Loss: 0.0548 | 0.0274
Epoch 34/300, seasonal_2 Loss: 0.0538 | 0.0279
Epoch 35/300, seasonal_2 Loss: 0.0533 | 0.0262
Epoch 36/300, seasonal_2 Loss: 0.0527 | 0.0271
Epoch 37/300, seasonal_2 Loss: 0.0525 | 0.0269
Epoch 38/300, seasonal_2 Loss: 0.0518 | 0.0267
Epoch 39/300, seasonal_2 Loss: 0.0514 | 0.0262
Epoch 40/300, seasonal_2 Loss: 0.0511 | 0.0274
Epoch 41/300, seasonal_2 Loss: 0.0504 | 0.0254
Epoch 42/300, seasonal_2 Loss: 0.0489 | 0.0260
Epoch 43/300, seasonal_2 Loss: 0.0474 | 0.0295
Epoch 44/300, seasonal_2 Loss: 0.0468 | 0.0252
Epoch 45/300, seasonal_2 Loss: 0.0510 | 0.0242
Epoch 46/300, seasonal_2 Loss: 0.0467 | 0.0272
Epoch 47/300, seasonal_2 Loss: 0.0458 | 0.0273
Epoch 48/300, seasonal_2 Loss: 0.0489 | 0.0262
Epoch 49/300, seasonal_2 Loss: 0.0449 | 0.0247
Epoch 50/300, seasonal_2 Loss: 0.0480 | 0.0235
Epoch 51/300, seasonal_2 Loss: 0.0452 | 0.0246
Epoch 52/300, seasonal_2 Loss: 0.0440 | 0.0234
Epoch 53/300, seasonal_2 Loss: 0.0421 | 0.0236
Epoch 54/300, seasonal_2 Loss: 0.0415 | 0.0231
Epoch 55/300, seasonal_2 Loss: 0.0410 | 0.0240
Epoch 56/300, seasonal_2 Loss: 0.0406 | 0.0231
Epoch 57/300, seasonal_2 Loss: 0.0403 | 0.0240
Epoch 58/300, seasonal_2 Loss: 0.0400 | 0.0231
Epoch 59/300, seasonal_2 Loss: 0.0398 | 0.0240
Epoch 60/300, seasonal_2 Loss: 0.0396 | 0.0231
Epoch 61/300, seasonal_2 Loss: 0.0394 | 0.0239
Epoch 62/300, seasonal_2 Loss: 0.0392 | 0.0233
Epoch 63/300, seasonal_2 Loss: 0.0390 | 0.0240
Epoch 64/300, seasonal_2 Loss: 0.0388 | 0.0235
Epoch 65/300, seasonal_2 Loss: 0.0387 | 0.0240
Epoch 66/300, seasonal_2 Loss: 0.0385 | 0.0237
Epoch 67/300, seasonal_2 Loss: 0.0384 | 0.0240
Epoch 68/300, seasonal_2 Loss: 0.0382 | 0.0238
Epoch 69/300, seasonal_2 Loss: 0.0381 | 0.0241
Epoch 70/300, seasonal_2 Loss: 0.0380 | 0.0239
Epoch 71/300, seasonal_2 Loss: 0.0379 | 0.0242
Epoch 72/300, seasonal_2 Loss: 0.0378 | 0.0241
Epoch 73/300, seasonal_2 Loss: 0.0376 | 0.0243
Epoch 74/300, seasonal_2 Loss: 0.0375 | 0.0243
Epoch 75/300, seasonal_2 Loss: 0.0374 | 0.0244
Epoch 76/300, seasonal_2 Loss: 0.0373 | 0.0245
Epoch 77/300, seasonal_2 Loss: 0.0373 | 0.0245
Epoch 78/300, seasonal_2 Loss: 0.0371 | 0.0246
Epoch 79/300, seasonal_2 Loss: 0.0371 | 0.0247
Epoch 80/300, seasonal_2 Loss: 0.0370 | 0.0247
Epoch 81/300, seasonal_2 Loss: 0.0369 | 0.0247
Epoch 82/300, seasonal_2 Loss: 0.0368 | 0.0247
Epoch 83/300, seasonal_2 Loss: 0.0367 | 0.0248
Epoch 84/300, seasonal_2 Loss: 0.0366 | 0.0247
Epoch 85/300, seasonal_2 Loss: 0.0365 | 0.0247
Epoch 86/300, seasonal_2 Loss: 0.0365 | 0.0246
Epoch 87/300, seasonal_2 Loss: 0.0364 | 0.0247
Epoch 88/300, seasonal_2 Loss: 0.0363 | 0.0247
Epoch 89/300, seasonal_2 Loss: 0.0362 | 0.0246
Epoch 90/300, seasonal_2 Loss: 0.0362 | 0.0246
Epoch 91/300, seasonal_2 Loss: 0.0361 | 0.0245
Epoch 92/300, seasonal_2 Loss: 0.0360 | 0.0246
Epoch 93/300, seasonal_2 Loss: 0.0360 | 0.0246
Epoch 94/300, seasonal_2 Loss: 0.0359 | 0.0245
Epoch 95/300, seasonal_2 Loss: 0.0359 | 0.0246
Epoch 96/300, seasonal_2 Loss: 0.0358 | 0.0246
Epoch 97/300, seasonal_2 Loss: 0.0358 | 0.0246
Epoch 98/300, seasonal_2 Loss: 0.0358 | 0.0246
Epoch 99/300, seasonal_2 Loss: 0.0357 | 0.0247
Epoch 100/300, seasonal_2 Loss: 0.0357 | 0.0247
Epoch 101/300, seasonal_2 Loss: 0.0357 | 0.0247
Epoch 102/300, seasonal_2 Loss: 0.0356 | 0.0248
Epoch 103/300, seasonal_2 Loss: 0.0356 | 0.0248
Epoch 104/300, seasonal_2 Loss: 0.0356 | 0.0248
Epoch 105/300, seasonal_2 Loss: 0.0355 | 0.0249
Epoch 106/300, seasonal_2 Loss: 0.0355 | 0.0249
Epoch 107/300, seasonal_2 Loss: 0.0355 | 0.0249
Epoch 108/300, seasonal_2 Loss: 0.0355 | 0.0250
Epoch 109/300, seasonal_2 Loss: 0.0355 | 0.0250
Epoch 110/300, seasonal_2 Loss: 0.0354 | 0.0250
Epoch 111/300, seasonal_2 Loss: 0.0354 | 0.0251
Epoch 112/300, seasonal_2 Loss: 0.0354 | 0.0252
Epoch 113/300, seasonal_2 Loss: 0.0354 | 0.0253
Epoch 114/300, seasonal_2 Loss: 0.0354 | 0.0257
Epoch 115/300, seasonal_2 Loss: 0.0354 | 0.0258
Epoch 116/300, seasonal_2 Loss: 0.0354 | 0.0263
Epoch 117/300, seasonal_2 Loss: 0.0353 | 0.0264
Epoch 118/300, seasonal_2 Loss: 0.0353 | 0.0264
Epoch 119/300, seasonal_2 Loss: 0.0353 | 0.0264
Epoch 120/300, seasonal_2 Loss: 0.0352 | 0.0263
Epoch 121/300, seasonal_2 Loss: 0.0352 | 0.0260
Epoch 122/300, seasonal_2 Loss: 0.0352 | 0.0259
Epoch 123/300, seasonal_2 Loss: 0.0352 | 0.0258
Epoch 124/300, seasonal_2 Loss: 0.0352 | 0.0255
Epoch 125/300, seasonal_2 Loss: 0.0352 | 0.0255
Epoch 126/300, seasonal_2 Loss: 0.0351 | 0.0253
Epoch 127/300, seasonal_2 Loss: 0.0351 | 0.0253
Epoch 128/300, seasonal_2 Loss: 0.0351 | 0.0254
Epoch 129/300, seasonal_2 Loss: 0.0350 | 0.0253
Epoch 130/300, seasonal_2 Loss: 0.0350 | 0.0253
Epoch 131/300, seasonal_2 Loss: 0.0350 | 0.0253
Epoch 132/300, seasonal_2 Loss: 0.0350 | 0.0253
Epoch 133/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 134/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 135/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 136/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 137/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 138/300, seasonal_2 Loss: 0.0349 | 0.0253
Epoch 139/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 140/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 141/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 142/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 143/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 144/300, seasonal_2 Loss: 0.0348 | 0.0253
Epoch 145/300, seasonal_2 Loss: 0.0348 | 0.0254
Epoch 146/300, seasonal_2 Loss: 0.0348 | 0.0254
Epoch 147/300, seasonal_2 Loss: 0.0348 | 0.0254
Epoch 148/300, seasonal_2 Loss: 0.0348 | 0.0254
Epoch 149/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 150/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 151/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 152/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 153/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 154/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 155/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 156/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 157/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 158/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 159/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 160/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 161/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 162/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 163/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 164/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 165/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 166/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 167/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 168/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 169/300, seasonal_2 Loss: 0.0347 | 0.0254
Epoch 170/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 171/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 172/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 173/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 174/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 175/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 176/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 177/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 178/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 179/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 180/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 181/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 182/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 183/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 184/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 185/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 186/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 187/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 188/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 189/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 190/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 191/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 192/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 193/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 194/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 195/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 196/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 197/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 198/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 199/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 200/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 201/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 202/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 203/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 204/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 205/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 206/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 207/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 208/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 209/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 210/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 211/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 212/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 213/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 214/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 215/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 216/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 217/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 218/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 219/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 220/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 221/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 222/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 223/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 224/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 225/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 226/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 227/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 228/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 229/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 230/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 231/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 232/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 233/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 234/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 235/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 236/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 237/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 238/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 239/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 240/300, seasonal_2 Loss: 0.0346 | 0.0254
Epoch 241/300, seasonal_2 Loss: 0.0346 | 0.0254
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 8, 'train_rates': 0.6055117103397516, 'learning_rate': 0.00083312569430461, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9244474333705247}
Epoch 1/300, seasonal_3 Loss: 0.5226 | 0.2854
Epoch 2/300, seasonal_3 Loss: 0.2994 | 0.2268
Epoch 3/300, seasonal_3 Loss: 0.2026 | 0.1673
Epoch 4/300, seasonal_3 Loss: 0.1631 | 0.1599
Epoch 5/300, seasonal_3 Loss: 0.1461 | 0.1498
Epoch 6/300, seasonal_3 Loss: 0.1406 | 0.1305
Epoch 7/300, seasonal_3 Loss: 0.1356 | 0.1341
Epoch 8/300, seasonal_3 Loss: 0.1288 | 0.1115
Epoch 9/300, seasonal_3 Loss: 0.1240 | 0.1148
Epoch 10/300, seasonal_3 Loss: 0.1228 | 0.1302
Epoch 11/300, seasonal_3 Loss: 0.1209 | 0.1222
Epoch 12/300, seasonal_3 Loss: 0.1195 | 0.1117
Epoch 13/300, seasonal_3 Loss: 0.1167 | 0.1359
Epoch 14/300, seasonal_3 Loss: 0.1140 | 0.1648
Epoch 15/300, seasonal_3 Loss: 0.1124 | 0.1629
Epoch 16/300, seasonal_3 Loss: 0.1122 | 0.1266
Epoch 17/300, seasonal_3 Loss: 0.1117 | 0.1258
Epoch 18/300, seasonal_3 Loss: 0.1100 | 0.1545
Epoch 19/300, seasonal_3 Loss: 0.1082 | 0.1700
Epoch 20/300, seasonal_3 Loss: 0.1066 | 0.1561
Epoch 21/300, seasonal_3 Loss: 0.1054 | 0.1311
Epoch 22/300, seasonal_3 Loss: 0.1045 | 0.1190
Epoch 23/300, seasonal_3 Loss: 0.1034 | 0.1148
Epoch 24/300, seasonal_3 Loss: 0.1021 | 0.1181
Epoch 25/300, seasonal_3 Loss: 0.1011 | 0.1214
Epoch 26/300, seasonal_3 Loss: 0.0999 | 0.1101
Epoch 27/300, seasonal_3 Loss: 0.0986 | 0.0953
Epoch 28/300, seasonal_3 Loss: 0.0976 | 0.0885
Epoch 29/300, seasonal_3 Loss: 0.0967 | 0.0874
Epoch 30/300, seasonal_3 Loss: 0.0960 | 0.0905
Epoch 31/300, seasonal_3 Loss: 0.0955 | 0.0900
Epoch 32/300, seasonal_3 Loss: 0.0949 | 0.0851
Epoch 33/300, seasonal_3 Loss: 0.0944 | 0.0822
Epoch 34/300, seasonal_3 Loss: 0.0939 | 0.0811
Epoch 35/300, seasonal_3 Loss: 0.0935 | 0.0809
Epoch 36/300, seasonal_3 Loss: 0.0932 | 0.0801
Epoch 37/300, seasonal_3 Loss: 0.0929 | 0.0789
Epoch 38/300, seasonal_3 Loss: 0.0926 | 0.0778
Epoch 39/300, seasonal_3 Loss: 0.0921 | 0.0769
Epoch 40/300, seasonal_3 Loss: 0.0918 | 0.0763
Epoch 41/300, seasonal_3 Loss: 0.0916 | 0.0763
Epoch 42/300, seasonal_3 Loss: 0.0913 | 0.0759
Epoch 43/300, seasonal_3 Loss: 0.0911 | 0.0755
Epoch 44/300, seasonal_3 Loss: 0.0908 | 0.0754
Epoch 45/300, seasonal_3 Loss: 0.0905 | 0.0749
Epoch 46/300, seasonal_3 Loss: 0.0903 | 0.0742
Epoch 47/300, seasonal_3 Loss: 0.0897 | 0.0738
Epoch 48/300, seasonal_3 Loss: 0.0898 | 0.0736
Epoch 49/300, seasonal_3 Loss: 0.0895 | 0.0737
Epoch 50/300, seasonal_3 Loss: 0.0891 | 0.0729
Epoch 51/300, seasonal_3 Loss: 0.0891 | 0.0723
Epoch 52/300, seasonal_3 Loss: 0.0887 | 0.0733
Epoch 53/300, seasonal_3 Loss: 0.0883 | 0.0727
Epoch 54/300, seasonal_3 Loss: 0.0879 | 0.0723
Epoch 55/300, seasonal_3 Loss: 0.0878 | 0.0727
Epoch 56/300, seasonal_3 Loss: 0.0876 | 0.0718
Epoch 57/300, seasonal_3 Loss: 0.0874 | 0.0719
Epoch 58/300, seasonal_3 Loss: 0.0872 | 0.0719
Epoch 59/300, seasonal_3 Loss: 0.0870 | 0.0714
Epoch 60/300, seasonal_3 Loss: 0.0868 | 0.0715
Epoch 61/300, seasonal_3 Loss: 0.0866 | 0.0714
Epoch 62/300, seasonal_3 Loss: 0.0864 | 0.0710
Epoch 63/300, seasonal_3 Loss: 0.0863 | 0.0711
Epoch 64/300, seasonal_3 Loss: 0.0861 | 0.0708
Epoch 65/300, seasonal_3 Loss: 0.0859 | 0.0707
Epoch 66/300, seasonal_3 Loss: 0.0858 | 0.0706
Epoch 67/300, seasonal_3 Loss: 0.0856 | 0.0702
Epoch 68/300, seasonal_3 Loss: 0.0855 | 0.0704
Epoch 69/300, seasonal_3 Loss: 0.0854 | 0.0700
Epoch 70/300, seasonal_3 Loss: 0.0852 | 0.0697
Epoch 71/300, seasonal_3 Loss: 0.0851 | 0.0700
Epoch 72/300, seasonal_3 Loss: 0.0853 | 0.0693
Epoch 73/300, seasonal_3 Loss: 0.0852 | 0.0698
Epoch 74/300, seasonal_3 Loss: 0.0852 | 0.0702
Epoch 75/300, seasonal_3 Loss: 0.0852 | 0.0700
Epoch 76/300, seasonal_3 Loss: 0.0851 | 0.0693
Epoch 77/300, seasonal_3 Loss: 0.0845 | 0.0685
Epoch 78/300, seasonal_3 Loss: 0.0840 | 0.0692
Epoch 79/300, seasonal_3 Loss: 0.0840 | 0.0683
Epoch 80/300, seasonal_3 Loss: 0.0838 | 0.0687
Epoch 81/300, seasonal_3 Loss: 0.0837 | 0.0685
Epoch 82/300, seasonal_3 Loss: 0.0836 | 0.0681
Epoch 83/300, seasonal_3 Loss: 0.0834 | 0.0686
Epoch 84/300, seasonal_3 Loss: 0.0835 | 0.0679
Epoch 85/300, seasonal_3 Loss: 0.0834 | 0.0683
Epoch 86/300, seasonal_3 Loss: 0.0834 | 0.0681
Epoch 87/300, seasonal_3 Loss: 0.0835 | 0.0679
Epoch 88/300, seasonal_3 Loss: 0.0835 | 0.0686
Epoch 89/300, seasonal_3 Loss: 0.0838 | 0.0691
Epoch 90/300, seasonal_3 Loss: 0.0843 | 0.0709
Epoch 91/300, seasonal_3 Loss: 0.0855 | 0.0695
Epoch 92/300, seasonal_3 Loss: 0.0862 | 0.0680
Epoch 93/300, seasonal_3 Loss: 0.0841 | 0.0692
Epoch 94/300, seasonal_3 Loss: 0.0830 | 0.0676
Epoch 95/300, seasonal_3 Loss: 0.0826 | 0.0680
Epoch 96/300, seasonal_3 Loss: 0.0827 | 0.0674
Epoch 97/300, seasonal_3 Loss: 0.0826 | 0.0679
Epoch 98/300, seasonal_3 Loss: 0.0823 | 0.0673
Epoch 99/300, seasonal_3 Loss: 0.0823 | 0.0678
Epoch 100/300, seasonal_3 Loss: 0.0822 | 0.0670
Epoch 101/300, seasonal_3 Loss: 0.0823 | 0.0677
Epoch 102/300, seasonal_3 Loss: 0.0822 | 0.0668
Epoch 103/300, seasonal_3 Loss: 0.0823 | 0.0678
Epoch 104/300, seasonal_3 Loss: 0.0821 | 0.0666
Epoch 105/300, seasonal_3 Loss: 0.0823 | 0.0678
Epoch 106/300, seasonal_3 Loss: 0.0821 | 0.0665
Epoch 107/300, seasonal_3 Loss: 0.0822 | 0.0676
Epoch 108/300, seasonal_3 Loss: 0.0820 | 0.0664
Epoch 109/300, seasonal_3 Loss: 0.0820 | 0.0673
Epoch 110/300, seasonal_3 Loss: 0.0818 | 0.0664
Epoch 111/300, seasonal_3 Loss: 0.0818 | 0.0670
Epoch 112/300, seasonal_3 Loss: 0.0817 | 0.0664
Epoch 113/300, seasonal_3 Loss: 0.0817 | 0.0667
Epoch 114/300, seasonal_3 Loss: 0.0816 | 0.0664
Epoch 115/300, seasonal_3 Loss: 0.0816 | 0.0665
Epoch 116/300, seasonal_3 Loss: 0.0815 | 0.0664
Epoch 117/300, seasonal_3 Loss: 0.0815 | 0.0664
Epoch 118/300, seasonal_3 Loss: 0.0815 | 0.0664
Epoch 119/300, seasonal_3 Loss: 0.0814 | 0.0663
Epoch 120/300, seasonal_3 Loss: 0.0814 | 0.0663
Epoch 121/300, seasonal_3 Loss: 0.0814 | 0.0663
Epoch 122/300, seasonal_3 Loss: 0.0814 | 0.0663
Epoch 123/300, seasonal_3 Loss: 0.0813 | 0.0662
Epoch 124/300, seasonal_3 Loss: 0.0813 | 0.0662
Epoch 125/300, seasonal_3 Loss: 0.0813 | 0.0662
Epoch 126/300, seasonal_3 Loss: 0.0813 | 0.0662
Epoch 127/300, seasonal_3 Loss: 0.0812 | 0.0661
Epoch 128/300, seasonal_3 Loss: 0.0812 | 0.0661
Epoch 129/300, seasonal_3 Loss: 0.0812 | 0.0661
Epoch 130/300, seasonal_3 Loss: 0.0812 | 0.0661
Epoch 131/300, seasonal_3 Loss: 0.0811 | 0.0661
Epoch 132/300, seasonal_3 Loss: 0.0811 | 0.0660
Epoch 133/300, seasonal_3 Loss: 0.0811 | 0.0660
Epoch 134/300, seasonal_3 Loss: 0.0811 | 0.0660
Epoch 135/300, seasonal_3 Loss: 0.0811 | 0.0660
Epoch 136/300, seasonal_3 Loss: 0.0811 | 0.0660
Epoch 137/300, seasonal_3 Loss: 0.0810 | 0.0660
Epoch 138/300, seasonal_3 Loss: 0.0810 | 0.0659
Epoch 139/300, seasonal_3 Loss: 0.0810 | 0.0659
Epoch 140/300, seasonal_3 Loss: 0.0810 | 0.0659
Epoch 141/300, seasonal_3 Loss: 0.0810 | 0.0659
Epoch 142/300, seasonal_3 Loss: 0.0810 | 0.0659
Epoch 143/300, seasonal_3 Loss: 0.0809 | 0.0659
Epoch 144/300, seasonal_3 Loss: 0.0809 | 0.0659
Epoch 145/300, seasonal_3 Loss: 0.0809 | 0.0659
Epoch 146/300, seasonal_3 Loss: 0.0809 | 0.0659
Epoch 147/300, seasonal_3 Loss: 0.0809 | 0.0658
Epoch 148/300, seasonal_3 Loss: 0.0809 | 0.0658
Epoch 149/300, seasonal_3 Loss: 0.0809 | 0.0658
Epoch 150/300, seasonal_3 Loss: 0.0809 | 0.0658
Epoch 151/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 152/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 153/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 154/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 155/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 156/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 157/300, seasonal_3 Loss: 0.0808 | 0.0658
Epoch 158/300, seasonal_3 Loss: 0.0808 | 0.0657
Epoch 159/300, seasonal_3 Loss: 0.0808 | 0.0657
Epoch 160/300, seasonal_3 Loss: 0.0808 | 0.0657
Epoch 161/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 162/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 163/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 164/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 165/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 166/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 167/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 168/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 169/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 170/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 171/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 172/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 173/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 174/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 175/300, seasonal_3 Loss: 0.0807 | 0.0657
Epoch 176/300, seasonal_3 Loss: 0.0806 | 0.0657
Epoch 177/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 178/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 179/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 180/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 181/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 182/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 183/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 184/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 185/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 186/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 187/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 188/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 189/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 190/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 191/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 192/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 193/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 194/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 195/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 196/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 197/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 198/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 199/300, seasonal_3 Loss: 0.0806 | 0.0656
Epoch 200/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 201/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 202/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 203/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 204/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 205/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 206/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 207/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 208/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 209/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 210/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 211/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 212/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 213/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 214/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 215/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 216/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 217/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 218/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 219/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 220/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 221/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 222/300, seasonal_3 Loss: 0.0805 | 0.0656
Epoch 223/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 224/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 225/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 226/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 227/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 228/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 229/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 230/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 231/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 232/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 233/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 234/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 235/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 236/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 237/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 238/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 239/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 240/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 241/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 242/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 243/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 244/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 245/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 246/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 247/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 248/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 249/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 250/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 251/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 252/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 253/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 254/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 255/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 256/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 257/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 258/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 259/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 260/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 261/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 262/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 263/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 264/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 265/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 266/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 267/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 268/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 269/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 270/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 271/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 272/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 273/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 274/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 275/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 276/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 277/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 278/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 279/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 280/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 281/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 282/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 283/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 284/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 285/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 286/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 287/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 288/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 289/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 290/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 291/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 292/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 293/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 294/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 295/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 296/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 297/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 298/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 299/300, seasonal_3 Loss: 0.0805 | 0.0655
Epoch 300/300, seasonal_3 Loss: 0.0805 | 0.0655
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.9863055721223846, 'learning_rate': 0.00015343286780489403, 'batch_size': 22, 'step_size': 11, 'gamma': 0.8620149056296053}
Epoch 1/300, resid Loss: 0.3931 | 0.0983
Epoch 2/300, resid Loss: 0.1373 | 0.0882
Epoch 3/300, resid Loss: 0.1206 | 0.0717
Epoch 4/300, resid Loss: 0.1126 | 0.0687
Epoch 5/300, resid Loss: 0.1080 | 0.0661
Epoch 6/300, resid Loss: 0.1039 | 0.0628
Epoch 7/300, resid Loss: 0.0989 | 0.0583
Epoch 8/300, resid Loss: 0.0940 | 0.0554
Epoch 9/300, resid Loss: 0.0905 | 0.0525
Epoch 10/300, resid Loss: 0.0882 | 0.0503
Epoch 11/300, resid Loss: 0.0864 | 0.0486
Epoch 12/300, resid Loss: 0.0848 | 0.0474
Epoch 13/300, resid Loss: 0.0835 | 0.0470
Epoch 14/300, resid Loss: 0.0824 | 0.0466
Epoch 15/300, resid Loss: 0.0813 | 0.0460
Epoch 16/300, resid Loss: 0.0800 | 0.0450
Epoch 17/300, resid Loss: 0.0787 | 0.0438
Epoch 18/300, resid Loss: 0.0771 | 0.0425
Epoch 19/300, resid Loss: 0.0762 | 0.0413
Epoch 20/300, resid Loss: 0.0751 | 0.0403
Epoch 21/300, resid Loss: 0.0740 | 0.0394
Epoch 22/300, resid Loss: 0.0732 | 0.0387
Epoch 23/300, resid Loss: 0.0722 | 0.0376
Epoch 24/300, resid Loss: 0.0716 | 0.0372
Epoch 25/300, resid Loss: 0.0710 | 0.0365
Epoch 26/300, resid Loss: 0.0704 | 0.0354
Epoch 27/300, resid Loss: 0.0698 | 0.0342
Epoch 28/300, resid Loss: 0.0694 | 0.0333
Epoch 29/300, resid Loss: 0.0687 | 0.0330
Epoch 30/300, resid Loss: 0.0683 | 0.0323
Epoch 31/300, resid Loss: 0.0679 | 0.0318
Epoch 32/300, resid Loss: 0.0675 | 0.0314
Epoch 33/300, resid Loss: 0.0672 | 0.0310
Epoch 34/300, resid Loss: 0.0667 | 0.0308
Epoch 35/300, resid Loss: 0.0664 | 0.0303
Epoch 36/300, resid Loss: 0.0660 | 0.0299
Epoch 37/300, resid Loss: 0.0656 | 0.0294
Epoch 38/300, resid Loss: 0.0652 | 0.0289
Epoch 39/300, resid Loss: 0.0648 | 0.0283
Epoch 40/300, resid Loss: 0.0644 | 0.0267
Epoch 41/300, resid Loss: 0.0642 | 0.0261
Epoch 42/300, resid Loss: 0.0639 | 0.0255
Epoch 43/300, resid Loss: 0.0636 | 0.0248
Epoch 44/300, resid Loss: 0.0634 | 0.0242
Epoch 45/300, resid Loss: 0.0629 | 0.0242
Epoch 46/300, resid Loss: 0.0627 | 0.0233
Epoch 47/300, resid Loss: 0.0620 | 0.0225
Epoch 48/300, resid Loss: 0.0616 | 0.0221
Epoch 49/300, resid Loss: 0.0613 | 0.0217
Epoch 50/300, resid Loss: 0.0610 | 0.0215
Epoch 51/300, resid Loss: 0.0606 | 0.0209
Epoch 52/300, resid Loss: 0.0604 | 0.0208
Epoch 53/300, resid Loss: 0.0601 | 0.0204
Epoch 54/300, resid Loss: 0.0599 | 0.0203
Epoch 55/300, resid Loss: 0.0598 | 0.0200
Epoch 56/300, resid Loss: 0.0595 | 0.0197
Epoch 57/300, resid Loss: 0.0594 | 0.0195
Epoch 58/300, resid Loss: 0.0592 | 0.0195
Epoch 59/300, resid Loss: 0.0591 | 0.0193
Epoch 60/300, resid Loss: 0.0589 | 0.0194
Epoch 61/300, resid Loss: 0.0588 | 0.0192
Epoch 62/300, resid Loss: 0.0585 | 0.0188
Epoch 63/300, resid Loss: 0.0584 | 0.0186
Epoch 64/300, resid Loss: 0.0583 | 0.0186
Epoch 65/300, resid Loss: 0.0582 | 0.0185
Epoch 66/300, resid Loss: 0.0580 | 0.0185
Epoch 67/300, resid Loss: 0.0578 | 0.0181
Epoch 68/300, resid Loss: 0.0578 | 0.0182
Epoch 69/300, resid Loss: 0.0577 | 0.0181
Epoch 70/300, resid Loss: 0.0576 | 0.0181
Epoch 71/300, resid Loss: 0.0575 | 0.0180
Epoch 72/300, resid Loss: 0.0574 | 0.0181
Epoch 73/300, resid Loss: 0.0572 | 0.0179
Epoch 74/300, resid Loss: 0.0572 | 0.0180
Epoch 75/300, resid Loss: 0.0571 | 0.0179
Epoch 76/300, resid Loss: 0.0570 | 0.0180
Epoch 77/300, resid Loss: 0.0569 | 0.0179
Epoch 78/300, resid Loss: 0.0568 | 0.0180
Epoch 79/300, resid Loss: 0.0567 | 0.0179
Epoch 80/300, resid Loss: 0.0566 | 0.0180
Epoch 81/300, resid Loss: 0.0565 | 0.0179
Epoch 82/300, resid Loss: 0.0564 | 0.0179
Epoch 83/300, resid Loss: 0.0563 | 0.0178
Epoch 84/300, resid Loss: 0.0562 | 0.0179
Epoch 85/300, resid Loss: 0.0562 | 0.0178
Epoch 86/300, resid Loss: 0.0561 | 0.0178
Epoch 87/300, resid Loss: 0.0560 | 0.0177
Epoch 88/300, resid Loss: 0.0559 | 0.0177
Epoch 89/300, resid Loss: 0.0559 | 0.0178
Epoch 90/300, resid Loss: 0.0558 | 0.0178
Epoch 91/300, resid Loss: 0.0557 | 0.0178
Epoch 92/300, resid Loss: 0.0557 | 0.0177
Epoch 93/300, resid Loss: 0.0556 | 0.0177
Epoch 94/300, resid Loss: 0.0556 | 0.0177
Epoch 95/300, resid Loss: 0.0555 | 0.0179
Epoch 96/300, resid Loss: 0.0554 | 0.0179
Epoch 97/300, resid Loss: 0.0554 | 0.0179
Epoch 98/300, resid Loss: 0.0553 | 0.0178
Epoch 99/300, resid Loss: 0.0552 | 0.0178
Epoch 100/300, resid Loss: 0.0552 | 0.0179
Epoch 101/300, resid Loss: 0.0551 | 0.0179
Epoch 102/300, resid Loss: 0.0551 | 0.0179
Epoch 103/300, resid Loss: 0.0550 | 0.0178
Epoch 104/300, resid Loss: 0.0549 | 0.0178
Epoch 105/300, resid Loss: 0.0549 | 0.0178
Epoch 106/300, resid Loss: 0.0548 | 0.0178
Epoch 107/300, resid Loss: 0.0548 | 0.0178
Epoch 108/300, resid Loss: 0.0548 | 0.0178
Epoch 109/300, resid Loss: 0.0547 | 0.0178
Epoch 110/300, resid Loss: 0.0547 | 0.0178
Epoch 111/300, resid Loss: 0.0546 | 0.0178
Epoch 112/300, resid Loss: 0.0546 | 0.0178
Epoch 113/300, resid Loss: 0.0546 | 0.0178
Epoch 114/300, resid Loss: 0.0545 | 0.0178
Epoch 115/300, resid Loss: 0.0545 | 0.0178
Epoch 116/300, resid Loss: 0.0544 | 0.0178
Epoch 117/300, resid Loss: 0.0544 | 0.0177
Epoch 118/300, resid Loss: 0.0544 | 0.0177
Epoch 119/300, resid Loss: 0.0543 | 0.0177
Epoch 120/300, resid Loss: 0.0543 | 0.0177
Epoch 121/300, resid Loss: 0.0542 | 0.0177
Epoch 122/300, resid Loss: 0.0542 | 0.0175
Epoch 123/300, resid Loss: 0.0541 | 0.0176
Epoch 124/300, resid Loss: 0.0541 | 0.0176
Epoch 125/300, resid Loss: 0.0541 | 0.0176
Epoch 126/300, resid Loss: 0.0540 | 0.0176
Epoch 127/300, resid Loss: 0.0540 | 0.0175
Epoch 128/300, resid Loss: 0.0540 | 0.0175
Epoch 129/300, resid Loss: 0.0539 | 0.0175
Epoch 130/300, resid Loss: 0.0539 | 0.0175
Epoch 131/300, resid Loss: 0.0539 | 0.0175
Epoch 132/300, resid Loss: 0.0538 | 0.0175
Epoch 133/300, resid Loss: 0.0538 | 0.0175
Epoch 134/300, resid Loss: 0.0538 | 0.0174
Epoch 135/300, resid Loss: 0.0538 | 0.0174
Epoch 136/300, resid Loss: 0.0537 | 0.0174
Epoch 137/300, resid Loss: 0.0537 | 0.0174
Epoch 138/300, resid Loss: 0.0537 | 0.0174
Epoch 139/300, resid Loss: 0.0536 | 0.0174
Epoch 140/300, resid Loss: 0.0536 | 0.0174
Epoch 141/300, resid Loss: 0.0536 | 0.0174
Epoch 142/300, resid Loss: 0.0536 | 0.0174
Epoch 143/300, resid Loss: 0.0536 | 0.0174
Epoch 144/300, resid Loss: 0.0535 | 0.0174
Epoch 145/300, resid Loss: 0.0535 | 0.0174
Epoch 146/300, resid Loss: 0.0535 | 0.0174
Epoch 147/300, resid Loss: 0.0535 | 0.0174
Epoch 148/300, resid Loss: 0.0535 | 0.0174
Epoch 149/300, resid Loss: 0.0534 | 0.0174
Epoch 150/300, resid Loss: 0.0534 | 0.0174
Epoch 151/300, resid Loss: 0.0534 | 0.0174
Epoch 152/300, resid Loss: 0.0534 | 0.0174
Epoch 153/300, resid Loss: 0.0534 | 0.0174
Epoch 154/300, resid Loss: 0.0534 | 0.0174
Epoch 155/300, resid Loss: 0.0533 | 0.0174
Epoch 156/300, resid Loss: 0.0533 | 0.0174
Epoch 157/300, resid Loss: 0.0533 | 0.0174
Epoch 158/300, resid Loss: 0.0533 | 0.0174
Epoch 159/300, resid Loss: 0.0533 | 0.0174
Epoch 160/300, resid Loss: 0.0533 | 0.0174
Epoch 161/300, resid Loss: 0.0533 | 0.0175
Epoch 162/300, resid Loss: 0.0532 | 0.0175
Epoch 163/300, resid Loss: 0.0532 | 0.0175
Epoch 164/300, resid Loss: 0.0532 | 0.0175
Epoch 165/300, resid Loss: 0.0532 | 0.0175
Epoch 166/300, resid Loss: 0.0532 | 0.0176
Epoch 167/300, resid Loss: 0.0532 | 0.0176
Epoch 168/300, resid Loss: 0.0532 | 0.0176
Epoch 169/300, resid Loss: 0.0532 | 0.0176
Epoch 170/300, resid Loss: 0.0532 | 0.0176
Epoch 171/300, resid Loss: 0.0531 | 0.0176
Epoch 172/300, resid Loss: 0.0531 | 0.0176
Epoch 173/300, resid Loss: 0.0531 | 0.0176
Epoch 174/300, resid Loss: 0.0531 | 0.0176
Epoch 175/300, resid Loss: 0.0531 | 0.0176
Epoch 176/300, resid Loss: 0.0531 | 0.0176
Epoch 177/300, resid Loss: 0.0531 | 0.0177
Epoch 178/300, resid Loss: 0.0531 | 0.0177
Epoch 179/300, resid Loss: 0.0531 | 0.0177
Epoch 180/300, resid Loss: 0.0530 | 0.0177
Epoch 181/300, resid Loss: 0.0530 | 0.0177
Epoch 182/300, resid Loss: 0.0530 | 0.0177
Epoch 183/300, resid Loss: 0.0530 | 0.0177
Epoch 184/300, resid Loss: 0.0530 | 0.0177
Epoch 185/300, resid Loss: 0.0530 | 0.0177
Epoch 186/300, resid Loss: 0.0530 | 0.0177
Epoch 187/300, resid Loss: 0.0530 | 0.0177
Epoch 188/300, resid Loss: 0.0530 | 0.0176
Epoch 189/300, resid Loss: 0.0530 | 0.0176
Epoch 190/300, resid Loss: 0.0530 | 0.0176
Epoch 191/300, resid Loss: 0.0529 | 0.0176
Epoch 192/300, resid Loss: 0.0529 | 0.0176
Epoch 193/300, resid Loss: 0.0529 | 0.0176
Epoch 194/300, resid Loss: 0.0529 | 0.0176
Epoch 195/300, resid Loss: 0.0529 | 0.0176
Epoch 196/300, resid Loss: 0.0529 | 0.0176
Epoch 197/300, resid Loss: 0.0529 | 0.0176
Epoch 198/300, resid Loss: 0.0529 | 0.0176
Epoch 199/300, resid Loss: 0.0529 | 0.0176
Epoch 200/300, resid Loss: 0.0529 | 0.0176
Epoch 201/300, resid Loss: 0.0529 | 0.0176
Epoch 202/300, resid Loss: 0.0529 | 0.0176
Epoch 203/300, resid Loss: 0.0529 | 0.0176
Epoch 204/300, resid Loss: 0.0528 | 0.0176
Epoch 205/300, resid Loss: 0.0528 | 0.0176
Epoch 206/300, resid Loss: 0.0528 | 0.0176
Epoch 207/300, resid Loss: 0.0528 | 0.0176
Epoch 208/300, resid Loss: 0.0528 | 0.0176
Epoch 209/300, resid Loss: 0.0528 | 0.0176
Epoch 210/300, resid Loss: 0.0528 | 0.0176
Epoch 211/300, resid Loss: 0.0528 | 0.0176
Epoch 212/300, resid Loss: 0.0528 | 0.0176
Epoch 213/300, resid Loss: 0.0528 | 0.0176
Epoch 214/300, resid Loss: 0.0528 | 0.0176
Epoch 215/300, resid Loss: 0.0528 | 0.0176
Epoch 216/300, resid Loss: 0.0528 | 0.0176
Epoch 217/300, resid Loss: 0.0528 | 0.0176
Epoch 218/300, resid Loss: 0.0528 | 0.0176
Epoch 219/300, resid Loss: 0.0528 | 0.0176
Epoch 220/300, resid Loss: 0.0528 | 0.0176
Epoch 221/300, resid Loss: 0.0528 | 0.0176
Epoch 222/300, resid Loss: 0.0528 | 0.0176
Epoch 223/300, resid Loss: 0.0528 | 0.0176
Epoch 224/300, resid Loss: 0.0527 | 0.0176
Epoch 225/300, resid Loss: 0.0527 | 0.0176
Epoch 226/300, resid Loss: 0.0527 | 0.0176
Epoch 227/300, resid Loss: 0.0527 | 0.0176
Epoch 228/300, resid Loss: 0.0527 | 0.0176
Epoch 229/300, resid Loss: 0.0527 | 0.0176
Epoch 230/300, resid Loss: 0.0527 | 0.0176
Epoch 231/300, resid Loss: 0.0527 | 0.0176
Epoch 232/300, resid Loss: 0.0527 | 0.0176
Epoch 233/300, resid Loss: 0.0527 | 0.0176
Epoch 234/300, resid Loss: 0.0527 | 0.0176
Epoch 235/300, resid Loss: 0.0527 | 0.0176
Epoch 236/300, resid Loss: 0.0527 | 0.0176
Epoch 237/300, resid Loss: 0.0527 | 0.0176
Epoch 238/300, resid Loss: 0.0527 | 0.0176
Epoch 239/300, resid Loss: 0.0527 | 0.0176
Epoch 240/300, resid Loss: 0.0527 | 0.0176
Epoch 241/300, resid Loss: 0.0527 | 0.0176
Epoch 242/300, resid Loss: 0.0527 | 0.0176
Epoch 243/300, resid Loss: 0.0527 | 0.0176
Epoch 244/300, resid Loss: 0.0527 | 0.0176
Epoch 245/300, resid Loss: 0.0527 | 0.0176
Epoch 246/300, resid Loss: 0.0527 | 0.0176
Epoch 247/300, resid Loss: 0.0527 | 0.0176
Epoch 248/300, resid Loss: 0.0527 | 0.0176
Epoch 249/300, resid Loss: 0.0527 | 0.0176
Epoch 250/300, resid Loss: 0.0527 | 0.0176
Epoch 251/300, resid Loss: 0.0527 | 0.0176
Epoch 252/300, resid Loss: 0.0527 | 0.0176
Epoch 253/300, resid Loss: 0.0527 | 0.0176
Epoch 254/300, resid Loss: 0.0527 | 0.0176
Epoch 255/300, resid Loss: 0.0527 | 0.0176
Epoch 256/300, resid Loss: 0.0527 | 0.0176
Epoch 257/300, resid Loss: 0.0527 | 0.0176
Epoch 258/300, resid Loss: 0.0527 | 0.0176
Epoch 259/300, resid Loss: 0.0527 | 0.0176
Epoch 260/300, resid Loss: 0.0527 | 0.0176
Epoch 261/300, resid Loss: 0.0527 | 0.0176
Epoch 262/300, resid Loss: 0.0527 | 0.0176
Epoch 263/300, resid Loss: 0.0527 | 0.0176
Epoch 264/300, resid Loss: 0.0527 | 0.0176
Epoch 265/300, resid Loss: 0.0527 | 0.0176
Epoch 266/300, resid Loss: 0.0527 | 0.0176
Epoch 267/300, resid Loss: 0.0527 | 0.0176
Epoch 268/300, resid Loss: 0.0527 | 0.0176
Epoch 269/300, resid Loss: 0.0527 | 0.0176
Epoch 270/300, resid Loss: 0.0527 | 0.0176
Epoch 271/300, resid Loss: 0.0527 | 0.0176
Epoch 272/300, resid Loss: 0.0527 | 0.0176
Epoch 273/300, resid Loss: 0.0527 | 0.0176
Epoch 274/300, resid Loss: 0.0527 | 0.0176
Epoch 275/300, resid Loss: 0.0526 | 0.0176
Epoch 276/300, resid Loss: 0.0526 | 0.0176
Epoch 277/300, resid Loss: 0.0526 | 0.0176
Epoch 278/300, resid Loss: 0.0526 | 0.0176
Epoch 279/300, resid Loss: 0.0526 | 0.0176
Epoch 280/300, resid Loss: 0.0526 | 0.0176
Epoch 281/300, resid Loss: 0.0526 | 0.0176
Epoch 282/300, resid Loss: 0.0526 | 0.0176
Epoch 283/300, resid Loss: 0.0526 | 0.0176
Epoch 284/300, resid Loss: 0.0526 | 0.0176
Epoch 285/300, resid Loss: 0.0526 | 0.0176
Epoch 286/300, resid Loss: 0.0526 | 0.0176
Epoch 287/300, resid Loss: 0.0526 | 0.0176
Epoch 288/300, resid Loss: 0.0526 | 0.0176
Epoch 289/300, resid Loss: 0.0526 | 0.0176
Epoch 290/300, resid Loss: 0.0526 | 0.0176
Epoch 291/300, resid Loss: 0.0526 | 0.0176
Epoch 292/300, resid Loss: 0.0526 | 0.0176
Epoch 293/300, resid Loss: 0.0526 | 0.0176
Epoch 294/300, resid Loss: 0.0526 | 0.0176
Epoch 295/300, resid Loss: 0.0526 | 0.0176
Epoch 296/300, resid Loss: 0.0526 | 0.0176
Epoch 297/300, resid Loss: 0.0526 | 0.0176
Epoch 298/300, resid Loss: 0.0526 | 0.0176
Epoch 299/300, resid Loss: 0.0526 | 0.0176
Epoch 300/300, resid Loss: 0.0526 | 0.0176
Runtime (seconds): 2483.077346801758
0.00021714884423369222
[101.86684]
[5.541331]
[-1.252101]
[0.9841614]
[-4.013758]
[-1.5594227]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.7973632123903371
RMSE: 0.8929519653320312
MAE: 0.8929519653320312
R-squared: nan
[101.56705]
