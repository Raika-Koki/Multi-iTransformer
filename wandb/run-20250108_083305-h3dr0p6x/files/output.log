ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-08 08:33:08,730][0m A new study created in memory with name: no-name-04686932-ea3a-4ece-b2ec-73b69669efca[0m
[32m[I 2025-01-08 08:33:46,627][0m Trial 0 finished with value: 1.0985963761389672 and parameters: {'observation_period_num': 219, 'train_rates': 0.8432687836667457, 'learning_rate': 1.3141330562090603e-06, 'batch_size': 138, 'step_size': 1, 'gamma': 0.9613539263063281}. Best is trial 0 with value: 1.0985963761389672.[0m
[32m[I 2025-01-08 08:34:13,141][0m Trial 1 finished with value: 0.19644374334750397 and parameters: {'observation_period_num': 158, 'train_rates': 0.8623092727095474, 'learning_rate': 0.0006189578282704562, 'batch_size': 220, 'step_size': 2, 'gamma': 0.8319351990504855}. Best is trial 1 with value: 0.19644374334750397.[0m
[32m[I 2025-01-08 08:35:07,621][0m Trial 2 finished with value: 0.02547876081038457 and parameters: {'observation_period_num': 6, 'train_rates': 0.8016173885880493, 'learning_rate': 0.00028668094896057135, 'batch_size': 96, 'step_size': 14, 'gamma': 0.9708721067482967}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:35:55,334][0m Trial 3 finished with value: 0.38098335659578375 and parameters: {'observation_period_num': 41, 'train_rates': 0.6153780593048332, 'learning_rate': 8.011295698750125e-06, 'batch_size': 94, 'step_size': 8, 'gamma': 0.7554416026252666}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:37:49,038][0m Trial 4 finished with value: 0.6149679586291313 and parameters: {'observation_period_num': 241, 'train_rates': 0.7201987095508167, 'learning_rate': 2.702536215809749e-06, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9143827569122523}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:38:30,161][0m Trial 5 finished with value: 0.22472473644401011 and parameters: {'observation_period_num': 91, 'train_rates': 0.8665019805524248, 'learning_rate': 2.5467623666931377e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8063870499208932}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:40:02,310][0m Trial 6 finished with value: 0.10525277152254775 and parameters: {'observation_period_num': 166, 'train_rates': 0.7348131597295675, 'learning_rate': 0.000106623237253051, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9838372936887435}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:40:21,513][0m Trial 7 finished with value: 0.2953671437289034 and parameters: {'observation_period_num': 137, 'train_rates': 0.6026168161261688, 'learning_rate': 6.119233100274489e-05, 'batch_size': 248, 'step_size': 6, 'gamma': 0.868214905902747}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:40:51,881][0m Trial 8 finished with value: 0.13903146982192993 and parameters: {'observation_period_num': 130, 'train_rates': 0.9550992034565267, 'learning_rate': 0.0002116257426806845, 'batch_size': 203, 'step_size': 1, 'gamma': 0.9881460666307628}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:41:15,566][0m Trial 9 finished with value: 0.6770278613958786 and parameters: {'observation_period_num': 188, 'train_rates': 0.7581409772216291, 'learning_rate': 3.328272609125741e-06, 'batch_size': 222, 'step_size': 15, 'gamma': 0.868869698343161}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:42:18,355][0m Trial 10 finished with value: 0.028382479389808302 and parameters: {'observation_period_num': 9, 'train_rates': 0.9214923151986725, 'learning_rate': 0.0008927469985668682, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9318215654110917}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:43:18,295][0m Trial 11 finished with value: 0.05971064992657263 and parameters: {'observation_period_num': 17, 'train_rates': 0.9505459920795987, 'learning_rate': 0.0008877193217580458, 'batch_size': 97, 'step_size': 14, 'gamma': 0.9247682607703596}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:44:23,973][0m Trial 12 finished with value: 0.06663578030936447 and parameters: {'observation_period_num': 74, 'train_rates': 0.9053948799264729, 'learning_rate': 0.00032829624482391326, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9330790465628624}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:45:02,121][0m Trial 13 finished with value: 0.04334982889864979 and parameters: {'observation_period_num': 24, 'train_rates': 0.8021264478471363, 'learning_rate': 0.0002613112427595399, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9432511878890292}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:50:01,323][0m Trial 14 finished with value: 0.058864802998654986 and parameters: {'observation_period_num': 76, 'train_rates': 0.9882005443862476, 'learning_rate': 0.0009594629725844128, 'batch_size': 19, 'step_size': 11, 'gamma': 0.8946425185162781}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:50:31,319][0m Trial 15 finished with value: 0.11592002962759992 and parameters: {'observation_period_num': 49, 'train_rates': 0.6727632622196142, 'learning_rate': 2.2984407933498733e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.9587104126655406}. Best is trial 2 with value: 0.02547876081038457.[0m
[32m[I 2025-01-08 08:51:44,427][0m Trial 16 finished with value: 0.025237511908118095 and parameters: {'observation_period_num': 7, 'train_rates': 0.8047731955483413, 'learning_rate': 9.849567013580725e-05, 'batch_size': 71, 'step_size': 14, 'gamma': 0.8961330261809679}. Best is trial 16 with value: 0.025237511908118095.[0m
[32m[I 2025-01-08 08:53:16,116][0m Trial 17 finished with value: 0.05623807630699533 and parameters: {'observation_period_num': 105, 'train_rates': 0.7970841253054319, 'learning_rate': 9.190206917354131e-05, 'batch_size': 54, 'step_size': 10, 'gamma': 0.8390918450105405}. Best is trial 16 with value: 0.025237511908118095.[0m
[32m[I 2025-01-08 08:54:34,979][0m Trial 18 finished with value: 0.04937048353578733 and parameters: {'observation_period_num': 52, 'train_rates': 0.8258431753282494, 'learning_rate': 4.9016637096701024e-05, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8934815179975544}. Best is trial 16 with value: 0.025237511908118095.[0m
[32m[I 2025-01-08 08:55:16,071][0m Trial 19 finished with value: 0.08654097440126149 and parameters: {'observation_period_num': 5, 'train_rates': 0.688207717844231, 'learning_rate': 1.2805764992150193e-05, 'batch_size': 119, 'step_size': 8, 'gamma': 0.7952733097951726}. Best is trial 16 with value: 0.025237511908118095.[0m
[32m[I 2025-01-08 08:55:47,765][0m Trial 20 finished with value: 0.06900401667364255 and parameters: {'observation_period_num': 38, 'train_rates': 0.7632024623864692, 'learning_rate': 0.00015076837273190197, 'batch_size': 165, 'step_size': 9, 'gamma': 0.9038488611641367}. Best is trial 16 with value: 0.025237511908118095.[0m
[32m[I 2025-01-08 08:57:02,921][0m Trial 21 finished with value: 0.02450229652301484 and parameters: {'observation_period_num': 7, 'train_rates': 0.8999158970501184, 'learning_rate': 0.00040474923647906407, 'batch_size': 75, 'step_size': 13, 'gamma': 0.9555505346640719}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 08:58:15,157][0m Trial 22 finished with value: 0.10115794549208312 and parameters: {'observation_period_num': 65, 'train_rates': 0.883933775032635, 'learning_rate': 0.00040696294784196824, 'batch_size': 76, 'step_size': 13, 'gamma': 0.9672860559777335}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 08:59:04,207][0m Trial 23 finished with value: 0.033231076452703706 and parameters: {'observation_period_num': 26, 'train_rates': 0.8208698911526525, 'learning_rate': 0.0001613123438079499, 'batch_size': 113, 'step_size': 14, 'gamma': 0.955299765657666}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:02:58,573][0m Trial 24 finished with value: 0.02771119244317425 and parameters: {'observation_period_num': 6, 'train_rates': 0.7771510680441155, 'learning_rate': 0.00040607944957498114, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9744332070787732}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:04:19,884][0m Trial 25 finished with value: 0.058680483214170465 and parameters: {'observation_period_num': 105, 'train_rates': 0.8988365454146623, 'learning_rate': 8.645091804064364e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8859553329510937}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:06:23,951][0m Trial 26 finished with value: 0.03487571069727773 and parameters: {'observation_period_num': 32, 'train_rates': 0.8426598076752042, 'learning_rate': 3.942931427386286e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9398858419740312}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:07:07,397][0m Trial 27 finished with value: 0.06395230359851665 and parameters: {'observation_period_num': 44, 'train_rates': 0.715965940102512, 'learning_rate': 0.0004640303041912567, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8496657625144738}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:08:24,111][0m Trial 28 finished with value: 0.05287795277312398 and parameters: {'observation_period_num': 60, 'train_rates': 0.9311576451152029, 'learning_rate': 0.000195710550017286, 'batch_size': 73, 'step_size': 10, 'gamma': 0.9171618740701527}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:09:01,596][0m Trial 29 finished with value: 0.05931220639811018 and parameters: {'observation_period_num': 87, 'train_rates': 0.8472333096606648, 'learning_rate': 0.00010436896429563306, 'batch_size': 145, 'step_size': 12, 'gamma': 0.9512794580771409}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:09:42,113][0m Trial 30 finished with value: 0.16698141271107522 and parameters: {'observation_period_num': 196, 'train_rates': 0.8229259192214606, 'learning_rate': 0.0002820594824395799, 'batch_size': 124, 'step_size': 14, 'gamma': 0.9731528338714556}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:14:43,148][0m Trial 31 finished with value: 0.027213044990575298 and parameters: {'observation_period_num': 6, 'train_rates': 0.7671430658459645, 'learning_rate': 0.0005675631896745693, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9728320962866791}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:16:02,344][0m Trial 32 finished with value: 0.052094654133855896 and parameters: {'observation_period_num': 25, 'train_rates': 0.7457374527307247, 'learning_rate': 0.0005682257886462655, 'batch_size': 61, 'step_size': 15, 'gamma': 0.9656045710412651}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:18:27,811][0m Trial 33 finished with value: 0.039148123543651124 and parameters: {'observation_period_num': 21, 'train_rates': 0.7826643939626281, 'learning_rate': 0.0005645826418931498, 'batch_size': 34, 'step_size': 13, 'gamma': 0.9840905670357453}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:19:13,188][0m Trial 34 finished with value: 0.04530594525316937 and parameters: {'observation_period_num': 40, 'train_rates': 0.6875076955615043, 'learning_rate': 0.00013338646768535853, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9468722803316582}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:22:06,559][0m Trial 35 finished with value: 0.131562551862355 and parameters: {'observation_period_num': 5, 'train_rates': 0.880270274239987, 'learning_rate': 1.173253677220924e-06, 'batch_size': 31, 'step_size': 15, 'gamma': 0.9100664212163593}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:23:04,619][0m Trial 36 finished with value: 0.04363689861363835 and parameters: {'observation_period_num': 18, 'train_rates': 0.7212126412554971, 'learning_rate': 0.00029392461492513364, 'batch_size': 82, 'step_size': 5, 'gamma': 0.8177400751656896}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:24:19,864][0m Trial 37 finished with value: 0.20971840832914626 and parameters: {'observation_period_num': 245, 'train_rates': 0.6514095914073463, 'learning_rate': 6.292964614041936e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9745250013826356}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:25:21,326][0m Trial 38 finished with value: 0.06928134616464376 and parameters: {'observation_period_num': 34, 'train_rates': 0.8610113830551673, 'learning_rate': 0.0005935577113015228, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9224032442401536}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:27:08,881][0m Trial 39 finished with value: 0.040505619862157366 and parameters: {'observation_period_num': 49, 'train_rates': 0.802203033644295, 'learning_rate': 0.00016394871038259403, 'batch_size': 47, 'step_size': 14, 'gamma': 0.7585026474849366}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:27:47,480][0m Trial 40 finished with value: 0.05749757914367984 and parameters: {'observation_period_num': 58, 'train_rates': 0.7407442677851654, 'learning_rate': 0.00021337196802636058, 'batch_size': 129, 'step_size': 10, 'gamma': 0.9842764284775181}. Best is trial 21 with value: 0.02450229652301484.[0m
[32m[I 2025-01-08 09:31:37,927][0m Trial 41 finished with value: 0.023384853884385062 and parameters: {'observation_period_num': 6, 'train_rates': 0.7640856420447427, 'learning_rate': 0.0003624766556945417, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9707899851485432}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:36:21,175][0m Trial 42 finished with value: 0.03480798991084316 and parameters: {'observation_period_num': 16, 'train_rates': 0.767076588463037, 'learning_rate': 0.0007291452129551909, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9609309833896508}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:38:58,404][0m Trial 43 finished with value: 0.14301585616463441 and parameters: {'observation_period_num': 148, 'train_rates': 0.785589493748968, 'learning_rate': 0.00039852641050120997, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9350565307073181}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:41:03,650][0m Trial 44 finished with value: 0.13537453268301616 and parameters: {'observation_period_num': 16, 'train_rates': 0.8115665221813783, 'learning_rate': 2.1804195979129217e-06, 'batch_size': 41, 'step_size': 13, 'gamma': 0.9878499435246375}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:41:55,150][0m Trial 45 finished with value: 0.15929821636570402 and parameters: {'observation_period_num': 230, 'train_rates': 0.8399789250618838, 'learning_rate': 0.0002377576544430066, 'batch_size': 99, 'step_size': 15, 'gamma': 0.9480738145924744}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:44:48,994][0m Trial 46 finished with value: 0.0971912821661555 and parameters: {'observation_period_num': 32, 'train_rates': 0.7472962649205396, 'learning_rate': 0.00031847929066253146, 'batch_size': 27, 'step_size': 12, 'gamma': 0.9744751354711308}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:46:08,913][0m Trial 47 finished with value: 0.05750561633585697 and parameters: {'observation_period_num': 14, 'train_rates': 0.7097149923810838, 'learning_rate': 0.0009949346640649261, 'batch_size': 59, 'step_size': 2, 'gamma': 0.8790529725480435}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:47:07,344][0m Trial 48 finished with value: 0.17587395134597578 and parameters: {'observation_period_num': 174, 'train_rates': 0.635358759897521, 'learning_rate': 0.0005015643494710807, 'batch_size': 71, 'step_size': 14, 'gamma': 0.9638770849929985}. Best is trial 41 with value: 0.023384853884385062.[0m
[32m[I 2025-01-08 09:47:47,441][0m Trial 49 finished with value: 0.06065817549824715 and parameters: {'observation_period_num': 5, 'train_rates': 0.9711483229420005, 'learning_rate': 6.953836291893285e-05, 'batch_size': 154, 'step_size': 6, 'gamma': 0.8629870060772779}. Best is trial 41 with value: 0.023384853884385062.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-08 09:47:47,452][0m A new study created in memory with name: no-name-1e3e79e1-bfa0-4704-b76d-39da7d1126e4[0m
[32m[I 2025-01-08 09:48:26,095][0m Trial 0 finished with value: 0.15186211595481092 and parameters: {'observation_period_num': 145, 'train_rates': 0.9217146582600035, 'learning_rate': 0.00014514091224111737, 'batch_size': 151, 'step_size': 1, 'gamma': 0.9625325226939799}. Best is trial 0 with value: 0.15186211595481092.[0m
[32m[I 2025-01-08 09:48:48,671][0m Trial 1 finished with value: 0.9786952094333928 and parameters: {'observation_period_num': 22, 'train_rates': 0.6228712589995241, 'learning_rate': 1.9344234239960083e-06, 'batch_size': 225, 'step_size': 14, 'gamma': 0.7791546204114728}. Best is trial 0 with value: 0.15186211595481092.[0m
[32m[I 2025-01-08 09:49:58,735][0m Trial 2 finished with value: 0.964118915575522 and parameters: {'observation_period_num': 9, 'train_rates': 0.9543931158248289, 'learning_rate': 2.2608282184086744e-06, 'batch_size': 85, 'step_size': 1, 'gamma': 0.9339147284362472}. Best is trial 0 with value: 0.15186211595481092.[0m
[32m[I 2025-01-08 09:50:30,929][0m Trial 3 finished with value: 0.09136685085699679 and parameters: {'observation_period_num': 50, 'train_rates': 0.7353384167359726, 'learning_rate': 0.00024967186890482345, 'batch_size': 161, 'step_size': 8, 'gamma': 0.8111623166057164}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:51:58,517][0m Trial 4 finished with value: 0.13570464718711855 and parameters: {'observation_period_num': 113, 'train_rates': 0.685525540210661, 'learning_rate': 9.144983506813795e-05, 'batch_size': 51, 'step_size': 12, 'gamma': 0.9772147004321938}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:53:14,600][0m Trial 5 finished with value: 0.10979716025166593 and parameters: {'observation_period_num': 15, 'train_rates': 0.7792836089465529, 'learning_rate': 1.0314382141531824e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.854543791561408}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:53:39,082][0m Trial 6 finished with value: 0.6087243616514066 and parameters: {'observation_period_num': 102, 'train_rates': 0.6879780332651981, 'learning_rate': 6.446447665208889e-06, 'batch_size': 217, 'step_size': 4, 'gamma': 0.9631983628264671}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:54:09,553][0m Trial 7 finished with value: 0.14474958913811184 and parameters: {'observation_period_num': 80, 'train_rates': 0.8975448732081884, 'learning_rate': 3.998783380141365e-05, 'batch_size': 185, 'step_size': 12, 'gamma': 0.9807167689090833}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:54:41,953][0m Trial 8 finished with value: 0.21411608824706138 and parameters: {'observation_period_num': 216, 'train_rates': 0.707500287539392, 'learning_rate': 0.0006561858741506472, 'batch_size': 144, 'step_size': 3, 'gamma': 0.9894829363744797}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:55:12,798][0m Trial 9 finished with value: 0.25834357716364437 and parameters: {'observation_period_num': 90, 'train_rates': 0.8590705240531029, 'learning_rate': 9.250590541118506e-06, 'batch_size': 185, 'step_size': 15, 'gamma': 0.9617036212730139}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:56:03,311][0m Trial 10 finished with value: 0.09881988260895014 and parameters: {'observation_period_num': 172, 'train_rates': 0.7933745039651459, 'learning_rate': 0.0009670562148981545, 'batch_size': 99, 'step_size': 7, 'gamma': 0.756159533443521}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:56:51,942][0m Trial 11 finished with value: 0.10818885178384134 and parameters: {'observation_period_num': 169, 'train_rates': 0.8097093737971589, 'learning_rate': 0.0009633158897055889, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7520487594773028}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 09:59:30,823][0m Trial 12 finished with value: 0.15239470397146085 and parameters: {'observation_period_num': 206, 'train_rates': 0.7733183880890921, 'learning_rate': 0.00028560217736649165, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8111134678899221}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 10:00:13,665][0m Trial 13 finished with value: 0.21180591251648648 and parameters: {'observation_period_num': 249, 'train_rates': 0.737657081379071, 'learning_rate': 0.0003323121966679401, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8241324087122874}. Best is trial 3 with value: 0.09136685085699679.[0m
[32m[I 2025-01-08 10:00:57,511][0m Trial 14 finished with value: 0.06006503695910446 and parameters: {'observation_period_num': 67, 'train_rates': 0.8347238657614776, 'learning_rate': 0.0003072114317180521, 'batch_size': 126, 'step_size': 10, 'gamma': 0.7535949627695087}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:01:20,171][0m Trial 15 finished with value: 0.12576352830590873 and parameters: {'observation_period_num': 60, 'train_rates': 0.8400669346372683, 'learning_rate': 5.654350106943159e-05, 'batch_size': 255, 'step_size': 10, 'gamma': 0.794628854484013}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:01:55,015][0m Trial 16 finished with value: 0.06648621379791461 and parameters: {'observation_period_num': 48, 'train_rates': 0.8668660330008013, 'learning_rate': 0.00018830789663492874, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8922549366666734}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:02:39,236][0m Trial 17 finished with value: 0.11167079210281372 and parameters: {'observation_period_num': 48, 'train_rates': 0.9720479564379728, 'learning_rate': 0.00011947090155718822, 'batch_size': 135, 'step_size': 10, 'gamma': 0.9026027425385494}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:03:24,880][0m Trial 18 finished with value: 0.1298441589652718 and parameters: {'observation_period_num': 68, 'train_rates': 0.8667281387356417, 'learning_rate': 2.0076584720852926e-05, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8745586476168885}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:03:57,920][0m Trial 19 finished with value: 0.12380049436192238 and parameters: {'observation_period_num': 136, 'train_rates': 0.9325162658274029, 'learning_rate': 0.00040249621205900807, 'batch_size': 177, 'step_size': 11, 'gamma': 0.9008655762664158}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:04:26,560][0m Trial 20 finished with value: 0.07777004216502352 and parameters: {'observation_period_num': 39, 'train_rates': 0.901808324181227, 'learning_rate': 0.000156760971959981, 'batch_size': 212, 'step_size': 9, 'gamma': 0.850556865777588}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:04:55,463][0m Trial 21 finished with value: 0.07551205700494834 and parameters: {'observation_period_num': 42, 'train_rates': 0.8842073714284315, 'learning_rate': 0.00016394919983305626, 'batch_size': 214, 'step_size': 9, 'gamma': 0.8522782154647869}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:05:21,172][0m Trial 22 finished with value: 0.09922477130190914 and parameters: {'observation_period_num': 33, 'train_rates': 0.8296563614203628, 'learning_rate': 6.840576504690226e-05, 'batch_size': 242, 'step_size': 5, 'gamma': 0.8942581546121154}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:05:49,810][0m Trial 23 finished with value: 0.16645201072096824 and parameters: {'observation_period_num': 75, 'train_rates': 0.8787805176717679, 'learning_rate': 2.39349985252295e-05, 'batch_size': 203, 'step_size': 9, 'gamma': 0.9228327910150363}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:06:21,449][0m Trial 24 finished with value: 0.08750170720095682 and parameters: {'observation_period_num': 112, 'train_rates': 0.8225236292230829, 'learning_rate': 0.0005804440764548216, 'batch_size': 167, 'step_size': 12, 'gamma': 0.8397807797277012}. Best is trial 14 with value: 0.06006503695910446.[0m
[32m[I 2025-01-08 10:07:05,018][0m Trial 25 finished with value: 0.05965394105220065 and parameters: {'observation_period_num': 34, 'train_rates': 0.8541714929580221, 'learning_rate': 0.00018363403104670645, 'batch_size': 127, 'step_size': 11, 'gamma': 0.8726714684705681}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:07:50,669][0m Trial 26 finished with value: 0.07713145867594672 and parameters: {'observation_period_num': 92, 'train_rates': 0.853861148172961, 'learning_rate': 0.00019262759362074995, 'batch_size': 118, 'step_size': 11, 'gamma': 0.8806394384949229}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:09:05,888][0m Trial 27 finished with value: 0.07216998290414463 and parameters: {'observation_period_num': 5, 'train_rates': 0.9210695711034431, 'learning_rate': 0.0005432547688949033, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9317601325940229}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:09:44,378][0m Trial 28 finished with value: 0.09806768639999278 and parameters: {'observation_period_num': 61, 'train_rates': 0.7652656630002069, 'learning_rate': 8.754673617457157e-05, 'batch_size': 132, 'step_size': 11, 'gamma': 0.9147170432636132}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:10:23,280][0m Trial 29 finished with value: 0.07254196024525945 and parameters: {'observation_period_num': 29, 'train_rates': 0.9269153351600751, 'learning_rate': 5.031283275665494e-05, 'batch_size': 153, 'step_size': 13, 'gamma': 0.8815130258104862}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:10:59,018][0m Trial 30 finished with value: 0.12082899634365563 and parameters: {'observation_period_num': 148, 'train_rates': 0.802359206792992, 'learning_rate': 0.00011411840132011612, 'batch_size': 142, 'step_size': 7, 'gamma': 0.8310535387066971}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:12:14,237][0m Trial 31 finished with value: 0.07662787020206452 and parameters: {'observation_period_num': 7, 'train_rates': 0.9068480848940326, 'learning_rate': 0.0004887097748348416, 'batch_size': 76, 'step_size': 14, 'gamma': 0.9384875801872506}. Best is trial 25 with value: 0.05965394105220065.[0m
[32m[I 2025-01-08 10:16:08,470][0m Trial 32 finished with value: 0.04351364899002298 and parameters: {'observation_period_num': 24, 'train_rates': 0.9464267196987084, 'learning_rate': 0.00023685435398743753, 'batch_size': 24, 'step_size': 15, 'gamma': 0.8647661728579773}. Best is trial 32 with value: 0.04351364899002298.[0m
[32m[I 2025-01-08 10:16:41,002][0m Trial 33 finished with value: 0.05245661735534668 and parameters: {'observation_period_num': 29, 'train_rates': 0.9828501808951171, 'learning_rate': 0.00022895053547615358, 'batch_size': 196, 'step_size': 15, 'gamma': 0.7812724747739441}. Best is trial 32 with value: 0.04351364899002298.[0m
[32m[I 2025-01-08 10:19:32,726][0m Trial 34 finished with value: 0.027383675312866337 and parameters: {'observation_period_num': 24, 'train_rates': 0.9843492330298622, 'learning_rate': 0.00025942950672053607, 'batch_size': 34, 'step_size': 15, 'gamma': 0.7666238632989854}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:22:51,499][0m Trial 35 finished with value: 0.04359191133663422 and parameters: {'observation_period_num': 26, 'train_rates': 0.9722706834942166, 'learning_rate': 0.00023426946415708956, 'batch_size': 29, 'step_size': 15, 'gamma': 0.7790721305832099}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:28:30,510][0m Trial 36 finished with value: 0.032001858483999966 and parameters: {'observation_period_num': 21, 'train_rates': 0.9864078859680573, 'learning_rate': 0.0002816131136319432, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7783532760648461}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:34:23,072][0m Trial 37 finished with value: 0.16535419537577517 and parameters: {'observation_period_num': 27, 'train_rates': 0.9562006011881048, 'learning_rate': 2.5624153870102413e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7721469904233368}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:36:33,976][0m Trial 38 finished with value: 0.047708084185918175 and parameters: {'observation_period_num': 18, 'train_rates': 0.9553399187414711, 'learning_rate': 8.91319818098981e-05, 'batch_size': 44, 'step_size': 14, 'gamma': 0.8035452872958267}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:38:17,375][0m Trial 39 finished with value: 0.03634807839989662 and parameters: {'observation_period_num': 18, 'train_rates': 0.9886889913974756, 'learning_rate': 0.0004145114866259595, 'batch_size': 57, 'step_size': 13, 'gamma': 0.784163605589065}. Best is trial 34 with value: 0.027383675312866337.[0m
[32m[I 2025-01-08 10:40:09,515][0m Trial 40 finished with value: 0.022290969267487526 and parameters: {'observation_period_num': 16, 'train_rates': 0.985828775969476, 'learning_rate': 0.0007188140150581616, 'batch_size': 53, 'step_size': 13, 'gamma': 0.766264211428258}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:41:55,041][0m Trial 41 finished with value: 0.06523066985210932 and parameters: {'observation_period_num': 15, 'train_rates': 0.941333786105471, 'learning_rate': 0.0007785725150961716, 'batch_size': 55, 'step_size': 13, 'gamma': 0.790536097407697}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:44:48,774][0m Trial 42 finished with value: 0.03347904235124588 and parameters: {'observation_period_num': 5, 'train_rates': 0.9895192581767488, 'learning_rate': 0.0003800919242749136, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7670319222424247}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:46:32,894][0m Trial 43 finished with value: 0.09418401364421158 and parameters: {'observation_period_num': 5, 'train_rates': 0.6229733525825604, 'learning_rate': 0.00041302511800268946, 'batch_size': 41, 'step_size': 14, 'gamma': 0.7675185019298256}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:48:16,492][0m Trial 44 finished with value: 0.04454132169485092 and parameters: {'observation_period_num': 52, 'train_rates': 0.9894833130714702, 'learning_rate': 0.0003690916143055154, 'batch_size': 57, 'step_size': 13, 'gamma': 0.7670996433170003}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:50:44,886][0m Trial 45 finished with value: 0.04150585504738908 and parameters: {'observation_period_num': 19, 'train_rates': 0.967931475081434, 'learning_rate': 0.0006719205916368985, 'batch_size': 39, 'step_size': 12, 'gamma': 0.7967567660038831}. Best is trial 40 with value: 0.022290969267487526.[0m
[32m[I 2025-01-08 10:51:51,671][0m Trial 46 finished with value: 0.02204042859375477 and parameters: {'observation_period_num': 12, 'train_rates': 0.9885682316212955, 'learning_rate': 0.0007812705820300369, 'batch_size': 91, 'step_size': 14, 'gamma': 0.7853421739183611}. Best is trial 46 with value: 0.02204042859375477.[0m
[32m[I 2025-01-08 10:52:56,186][0m Trial 47 finished with value: 0.18132473634822027 and parameters: {'observation_period_num': 41, 'train_rates': 0.9617187316215358, 'learning_rate': 0.0009780623782490604, 'batch_size': 92, 'step_size': 14, 'gamma': 0.814329810700198}. Best is trial 46 with value: 0.02204042859375477.[0m
[32m[I 2025-01-08 10:54:19,314][0m Trial 48 finished with value: 0.06094981545209885 and parameters: {'observation_period_num': 13, 'train_rates': 0.9152845810099646, 'learning_rate': 0.0007488684653888392, 'batch_size': 68, 'step_size': 15, 'gamma': 0.7660299676033041}. Best is trial 46 with value: 0.02204042859375477.[0m
[32m[I 2025-01-08 10:59:25,329][0m Trial 49 finished with value: 0.08715111212577761 and parameters: {'observation_period_num': 55, 'train_rates': 0.9436163227803513, 'learning_rate': 0.0005068500188068187, 'batch_size': 18, 'step_size': 14, 'gamma': 0.7502145103060158}. Best is trial 46 with value: 0.02204042859375477.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-08 10:59:25,339][0m A new study created in memory with name: no-name-e35477a4-e752-4ce2-b145-7a31043d4257[0m
[32m[I 2025-01-08 11:03:23,995][0m Trial 0 finished with value: 0.18538285980019906 and parameters: {'observation_period_num': 242, 'train_rates': 0.6718567174055846, 'learning_rate': 4.0944032604657756e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8980434068316978}. Best is trial 0 with value: 0.18538285980019906.[0m
[32m[I 2025-01-08 11:04:02,424][0m Trial 1 finished with value: 0.6049748179770623 and parameters: {'observation_period_num': 88, 'train_rates': 0.6544107490313941, 'learning_rate': 1.6593193800769726e-06, 'batch_size': 118, 'step_size': 13, 'gamma': 0.7770908583400425}. Best is trial 0 with value: 0.18538285980019906.[0m
[32m[I 2025-01-08 11:04:26,453][0m Trial 2 finished with value: 0.9793462958098588 and parameters: {'observation_period_num': 45, 'train_rates': 0.7100023048843264, 'learning_rate': 1.6106536884592126e-06, 'batch_size': 223, 'step_size': 8, 'gamma': 0.8069396288475487}. Best is trial 0 with value: 0.18538285980019906.[0m
[32m[I 2025-01-08 11:05:16,235][0m Trial 3 finished with value: 0.3751649046239774 and parameters: {'observation_period_num': 210, 'train_rates': 0.8239956905291735, 'learning_rate': 6.6082833680564375e-06, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8199209998978912}. Best is trial 0 with value: 0.18538285980019906.[0m
[32m[I 2025-01-08 11:09:38,731][0m Trial 4 finished with value: 0.11114064537963424 and parameters: {'observation_period_num': 127, 'train_rates': 0.8406757327866767, 'learning_rate': 0.00014623129409689682, 'batch_size': 19, 'step_size': 10, 'gamma': 0.8682252294664553}. Best is trial 4 with value: 0.11114064537963424.[0m
[32m[I 2025-01-08 11:10:05,500][0m Trial 5 finished with value: 1.0040963783496764 and parameters: {'observation_period_num': 61, 'train_rates': 0.6459447621061808, 'learning_rate': 1.2698470165938586e-06, 'batch_size': 180, 'step_size': 7, 'gamma': 0.9188571362511099}. Best is trial 4 with value: 0.11114064537963424.[0m
[32m[I 2025-01-08 11:10:45,553][0m Trial 6 finished with value: 0.22730677666926477 and parameters: {'observation_period_num': 213, 'train_rates': 0.811103090964182, 'learning_rate': 1.7267906337714155e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.8202717582382766}. Best is trial 4 with value: 0.11114064537963424.[0m
[32m[I 2025-01-08 11:12:42,614][0m Trial 7 finished with value: 0.06072609794671547 and parameters: {'observation_period_num': 77, 'train_rates': 0.9262549059582983, 'learning_rate': 7.949857053746671e-05, 'batch_size': 47, 'step_size': 5, 'gamma': 0.7877179315372237}. Best is trial 7 with value: 0.06072609794671547.[0m
[32m[I 2025-01-08 11:13:07,279][0m Trial 8 finished with value: 0.14465079287977983 and parameters: {'observation_period_num': 212, 'train_rates': 0.8503833406965076, 'learning_rate': 0.00011680542710304349, 'batch_size': 218, 'step_size': 9, 'gamma': 0.8266073842492722}. Best is trial 7 with value: 0.06072609794671547.[0m
[32m[I 2025-01-08 11:13:36,174][0m Trial 9 finished with value: 0.7057869556622628 and parameters: {'observation_period_num': 150, 'train_rates': 0.8194901027910432, 'learning_rate': 2.0541665244488862e-06, 'batch_size': 190, 'step_size': 8, 'gamma': 0.9147446714111354}. Best is trial 7 with value: 0.06072609794671547.[0m
[32m[I 2025-01-08 11:14:57,794][0m Trial 10 finished with value: 0.0670180544257164 and parameters: {'observation_period_num': 18, 'train_rates': 0.9821504506668619, 'learning_rate': 0.0009546376941048445, 'batch_size': 73, 'step_size': 2, 'gamma': 0.9848978448429324}. Best is trial 7 with value: 0.06072609794671547.[0m
[32m[I 2025-01-08 11:16:21,469][0m Trial 11 finished with value: 0.0270387585695884 and parameters: {'observation_period_num': 13, 'train_rates': 0.9752823542410277, 'learning_rate': 0.00047247463363518487, 'batch_size': 70, 'step_size': 2, 'gamma': 0.9836246008787323}. Best is trial 11 with value: 0.0270387585695884.[0m
[32m[I 2025-01-08 11:17:57,338][0m Trial 12 finished with value: 0.025309756640600017 and parameters: {'observation_period_num': 7, 'train_rates': 0.9754113694840079, 'learning_rate': 0.0007150427449408761, 'batch_size': 62, 'step_size': 2, 'gamma': 0.9805599681635818}. Best is trial 12 with value: 0.025309756640600017.[0m
[32m[I 2025-01-08 11:19:15,999][0m Trial 13 finished with value: 0.022437936744263656 and parameters: {'observation_period_num': 7, 'train_rates': 0.9734097799354189, 'learning_rate': 0.0009114576043213219, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9854728274828737}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:20:19,335][0m Trial 14 finished with value: 0.0650263674757033 and parameters: {'observation_period_num': 118, 'train_rates': 0.9080148292068715, 'learning_rate': 0.00034818717540534744, 'batch_size': 86, 'step_size': 1, 'gamma': 0.9523883612950309}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:20:51,492][0m Trial 15 finished with value: 0.061611768746185555 and parameters: {'observation_period_num': 33, 'train_rates': 0.7432102231800018, 'learning_rate': 0.0008435464233870542, 'batch_size': 156, 'step_size': 4, 'gamma': 0.9489475282003046}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:22:45,127][0m Trial 16 finished with value: 0.02787309961065594 and parameters: {'observation_period_num': 7, 'train_rates': 0.909634396400441, 'learning_rate': 0.00024071581079535397, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9512938743396904}. Best is trial 13 with value: 0.022437936744263656.[0m
Early stopping at epoch 89
[32m[I 2025-01-08 11:23:35,958][0m Trial 17 finished with value: 0.12375538137651258 and parameters: {'observation_period_num': 102, 'train_rates': 0.9459546570542857, 'learning_rate': 0.0004677565309461792, 'batch_size': 102, 'step_size': 1, 'gamma': 0.8700592706552137}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:25:14,874][0m Trial 18 finished with value: 0.0806941266717582 and parameters: {'observation_period_num': 171, 'train_rates': 0.8646265580949202, 'learning_rate': 4.689923078845812e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.964280784374023}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:25:51,829][0m Trial 19 finished with value: 0.23445850485060588 and parameters: {'observation_period_num': 55, 'train_rates': 0.8855372236483355, 'learning_rate': 1.6222388975303998e-05, 'batch_size': 153, 'step_size': 3, 'gamma': 0.9361140345682797}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:26:13,453][0m Trial 20 finished with value: 0.057803505998858605 and parameters: {'observation_period_num': 35, 'train_rates': 0.7666808622832938, 'learning_rate': 0.00020444347906395843, 'batch_size': 251, 'step_size': 3, 'gamma': 0.8785209705451492}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:27:30,840][0m Trial 21 finished with value: 0.06214885041117668 and parameters: {'observation_period_num': 6, 'train_rates': 0.9791166084635319, 'learning_rate': 0.0004918784020889095, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9835075111529015}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:28:59,059][0m Trial 22 finished with value: 0.0462712532078678 and parameters: {'observation_period_num': 24, 'train_rates': 0.9514259149517305, 'learning_rate': 0.0009334734415707134, 'batch_size': 65, 'step_size': 3, 'gamma': 0.9846506603672357}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:31:51,546][0m Trial 23 finished with value: 0.055067630800998434 and parameters: {'observation_period_num': 69, 'train_rates': 0.9816721028397531, 'learning_rate': 0.0004808430289371386, 'batch_size': 33, 'step_size': 5, 'gamma': 0.7514335690690268}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:32:49,934][0m Trial 24 finished with value: 0.04550285181551246 and parameters: {'observation_period_num': 47, 'train_rates': 0.9447724467795868, 'learning_rate': 0.0003007227544979397, 'batch_size': 100, 'step_size': 2, 'gamma': 0.9639633804041589}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:34:24,038][0m Trial 25 finished with value: 0.045048917306290154 and parameters: {'observation_period_num': 24, 'train_rates': 0.9530471384613153, 'learning_rate': 0.0006151238510431639, 'batch_size': 61, 'step_size': 2, 'gamma': 0.9708656217344999}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:35:29,444][0m Trial 26 finished with value: 0.03374721553463202 and parameters: {'observation_period_num': 5, 'train_rates': 0.8900269011435925, 'learning_rate': 9.939424209702367e-05, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9285947149226623}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:38:08,431][0m Trial 27 finished with value: 0.11815341856590537 and parameters: {'observation_period_num': 81, 'train_rates': 0.9850517190835716, 'learning_rate': 0.0001735427225081259, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9895809384486778}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:38:52,081][0m Trial 28 finished with value: 0.04558658090616585 and parameters: {'observation_period_num': 42, 'train_rates': 0.9254193637149545, 'learning_rate': 0.00029979787880399575, 'batch_size': 130, 'step_size': 4, 'gamma': 0.9008127662877949}. Best is trial 13 with value: 0.022437936744263656.[0m
Early stopping at epoch 99
[32m[I 2025-01-08 11:43:22,909][0m Trial 29 finished with value: 0.22622830269083513 and parameters: {'observation_period_num': 251, 'train_rates': 0.8748546251972569, 'learning_rate': 4.43194795349866e-05, 'batch_size': 18, 'step_size': 1, 'gamma': 0.8524414166538592}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:44:07,609][0m Trial 30 finished with value: 0.08628434211404638 and parameters: {'observation_period_num': 151, 'train_rates': 0.7815861213727984, 'learning_rate': 6.930420186105048e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.9433339140632253}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:45:47,972][0m Trial 31 finished with value: 0.076091055937581 and parameters: {'observation_period_num': 13, 'train_rates': 0.6010946235169367, 'learning_rate': 0.00023851988538854782, 'batch_size': 42, 'step_size': 3, 'gamma': 0.9685752249183995}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:47:23,502][0m Trial 32 finished with value: 0.03539960465957003 and parameters: {'observation_period_num': 26, 'train_rates': 0.9103348273857697, 'learning_rate': 0.0007336838304693987, 'batch_size': 57, 'step_size': 4, 'gamma': 0.9553120630441387}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:48:39,209][0m Trial 33 finished with value: 0.034553192110489246 and parameters: {'observation_period_num': 5, 'train_rates': 0.9533873807685067, 'learning_rate': 0.00042289582388213776, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9718904781545515}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:51:44,126][0m Trial 34 finished with value: 0.08375728831997195 and parameters: {'observation_period_num': 97, 'train_rates': 0.964111113371953, 'learning_rate': 0.0006490486669902429, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9289455894837361}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:52:46,151][0m Trial 35 finished with value: 0.047976088192727834 and parameters: {'observation_period_num': 50, 'train_rates': 0.9197661349978798, 'learning_rate': 0.00025726216086772107, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9743059125721106}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:54:07,250][0m Trial 36 finished with value: 0.07483745669644024 and parameters: {'observation_period_num': 35, 'train_rates': 0.8989674774111179, 'learning_rate': 0.00017764976255061002, 'batch_size': 69, 'step_size': 1, 'gamma': 0.9005329183140138}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:55:00,047][0m Trial 37 finished with value: 0.26954137631084607 and parameters: {'observation_period_num': 62, 'train_rates': 0.9366126252816335, 'learning_rate': 5.0673242672429636e-06, 'batch_size': 112, 'step_size': 6, 'gamma': 0.9531979411030239}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:56:46,286][0m Trial 38 finished with value: 0.052913062274456024 and parameters: {'observation_period_num': 17, 'train_rates': 0.9628714034270648, 'learning_rate': 2.247041920020597e-05, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9177957596921069}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 11:59:38,552][0m Trial 39 finished with value: 0.053203334611133336 and parameters: {'observation_period_num': 39, 'train_rates': 0.6981155701205376, 'learning_rate': 0.0006163144637014302, 'batch_size': 26, 'step_size': 7, 'gamma': 0.9890509346618279}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:01:34,531][0m Trial 40 finished with value: 0.04364365713119927 and parameters: {'observation_period_num': 71, 'train_rates': 0.8523290648371138, 'learning_rate': 0.00012733715359123445, 'batch_size': 45, 'step_size': 3, 'gamma': 0.939994833159475}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:02:48,580][0m Trial 41 finished with value: 0.04069323092699051 and parameters: {'observation_period_num': 5, 'train_rates': 0.9888573064613098, 'learning_rate': 9.349206199706805e-05, 'batch_size': 82, 'step_size': 5, 'gamma': 0.9290676809856213}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:03:51,963][0m Trial 42 finished with value: 0.027620991044564218 and parameters: {'observation_period_num': 14, 'train_rates': 0.8887360001988623, 'learning_rate': 0.0003573135140796713, 'batch_size': 90, 'step_size': 2, 'gamma': 0.9610112451710942}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:04:52,808][0m Trial 43 finished with value: 0.03271925006220691 and parameters: {'observation_period_num': 21, 'train_rates': 0.92832859931073, 'learning_rate': 0.0004256034618285338, 'batch_size': 94, 'step_size': 2, 'gamma': 0.9591144837522406}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:06:20,802][0m Trial 44 finished with value: 0.04409014154225588 and parameters: {'observation_period_num': 27, 'train_rates': 0.9648146129357239, 'learning_rate': 0.0009822363026471255, 'batch_size': 66, 'step_size': 1, 'gamma': 0.9767432097335939}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:07:06,483][0m Trial 45 finished with value: 0.031204884347368966 and parameters: {'observation_period_num': 16, 'train_rates': 0.8359705702137302, 'learning_rate': 0.0003553370170025151, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9769963124858632}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:07:43,627][0m Trial 46 finished with value: 0.13711555699507397 and parameters: {'observation_period_num': 200, 'train_rates': 0.8693858354331467, 'learning_rate': 0.0005995183515158399, 'batch_size': 146, 'step_size': 3, 'gamma': 0.9625375889729011}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:08:38,037][0m Trial 47 finished with value: 0.05454352450497607 and parameters: {'observation_period_num': 56, 'train_rates': 0.9028244030116609, 'learning_rate': 0.00035464712998646667, 'batch_size': 102, 'step_size': 15, 'gamma': 0.8447739956512289}. Best is trial 13 with value: 0.022437936744263656.[0m
[32m[I 2025-01-08 12:10:21,113][0m Trial 48 finished with value: 0.04505551891764931 and parameters: {'observation_period_num': 33, 'train_rates': 0.80194555828668, 'learning_rate': 6.378053051224694e-05, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9472609088064566}. Best is trial 13 with value: 0.022437936744263656.[0m
Early stopping at epoch 68
[32m[I 2025-01-08 12:11:19,172][0m Trial 49 finished with value: 0.08607820893933134 and parameters: {'observation_period_num': 15, 'train_rates': 0.9721387012800917, 'learning_rate': 0.0002275816984816224, 'batch_size': 72, 'step_size': 1, 'gamma': 0.7940737929202779}. Best is trial 13 with value: 0.022437936744263656.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-08 12:11:19,183][0m A new study created in memory with name: no-name-5377f9ec-1375-423d-8ad5-c1b114dd64ed[0m
[32m[I 2025-01-08 12:13:28,317][0m Trial 0 finished with value: 0.1666674294857962 and parameters: {'observation_period_num': 156, 'train_rates': 0.7008885257824649, 'learning_rate': 8.282383449610449e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.9195544567238476}. Best is trial 0 with value: 0.1666674294857962.[0m
[32m[I 2025-01-08 12:15:14,944][0m Trial 1 finished with value: 0.8765549628358138 and parameters: {'observation_period_num': 243, 'train_rates': 0.9160408728770193, 'learning_rate': 2.694203747260068e-06, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9183278238912654}. Best is trial 0 with value: 0.1666674294857962.[0m
[32m[I 2025-01-08 12:15:39,630][0m Trial 2 finished with value: 0.1070045221382551 and parameters: {'observation_period_num': 30, 'train_rates': 0.6198566355263212, 'learning_rate': 0.00013405560671306025, 'batch_size': 198, 'step_size': 4, 'gamma': 0.8944780685810161}. Best is trial 2 with value: 0.1070045221382551.[0m
[32m[I 2025-01-08 12:16:13,219][0m Trial 3 finished with value: 0.05953197357302997 and parameters: {'observation_period_num': 97, 'train_rates': 0.8210665687878314, 'learning_rate': 0.0008705211883682656, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8527633358939902}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:16:39,774][0m Trial 4 finished with value: 0.4771742522716522 and parameters: {'observation_period_num': 229, 'train_rates': 0.9479689703675482, 'learning_rate': 7.066046670140042e-06, 'batch_size': 231, 'step_size': 13, 'gamma': 0.8235540069382112}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:17:03,600][0m Trial 5 finished with value: 0.7591517596244812 and parameters: {'observation_period_num': 124, 'train_rates': 0.7352591353393438, 'learning_rate': 1.8605550936004838e-06, 'batch_size': 231, 'step_size': 7, 'gamma': 0.9878212432574196}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:18:35,464][0m Trial 6 finished with value: 0.32364911206592856 and parameters: {'observation_period_num': 32, 'train_rates': 0.7240398913580253, 'learning_rate': 4.29669870547208e-06, 'batch_size': 51, 'step_size': 6, 'gamma': 0.788706255782992}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:23:09,947][0m Trial 7 finished with value: 0.059568439126014706 and parameters: {'observation_period_num': 6, 'train_rates': 0.9748502156113263, 'learning_rate': 1.2092467314986055e-05, 'batch_size': 21, 'step_size': 3, 'gamma': 0.8318108006060181}. Best is trial 3 with value: 0.05953197357302997.[0m
Early stopping at epoch 97
[32m[I 2025-01-08 12:24:50,508][0m Trial 8 finished with value: 0.13937302651839478 and parameters: {'observation_period_num': 47, 'train_rates': 0.6582626277968278, 'learning_rate': 6.932519183180129e-05, 'batch_size': 43, 'step_size': 1, 'gamma': 0.8518609731105076}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:27:02,256][0m Trial 9 finished with value: 0.3216906681503217 and parameters: {'observation_period_num': 54, 'train_rates': 0.8664167109213343, 'learning_rate': 1.4610503588139483e-06, 'batch_size': 40, 'step_size': 12, 'gamma': 0.8320940968968698}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:27:42,406][0m Trial 10 finished with value: 0.0606773789244819 and parameters: {'observation_period_num': 108, 'train_rates': 0.8238491099109172, 'learning_rate': 0.0009487255793788026, 'batch_size': 132, 'step_size': 10, 'gamma': 0.7541728909320464}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:28:26,681][0m Trial 11 finished with value: 0.2447071224451065 and parameters: {'observation_period_num': 86, 'train_rates': 0.9803487857317953, 'learning_rate': 1.2666729510521077e-05, 'batch_size': 134, 'step_size': 10, 'gamma': 0.872759258913197}. Best is trial 3 with value: 0.05953197357302997.[0m
[32m[I 2025-01-08 12:29:24,046][0m Trial 12 finished with value: 0.02792192138939017 and parameters: {'observation_period_num': 9, 'train_rates': 0.878292000188504, 'learning_rate': 0.0008274297366611683, 'batch_size': 99, 'step_size': 15, 'gamma': 0.8052827960132116}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:30:18,605][0m Trial 13 finished with value: 0.13869904952820475 and parameters: {'observation_period_num': 171, 'train_rates': 0.8119419880975465, 'learning_rate': 0.0009930676258939491, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7932300327619406}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:30:51,647][0m Trial 14 finished with value: 0.04597456366354351 and parameters: {'observation_period_num': 81, 'train_rates': 0.8830814643994763, 'learning_rate': 0.00032953016820016866, 'batch_size': 172, 'step_size': 15, 'gamma': 0.7882247911829964}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:31:53,474][0m Trial 15 finished with value: 0.048669016195668116 and parameters: {'observation_period_num': 67, 'train_rates': 0.8849271980960685, 'learning_rate': 0.0002815137088318462, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7507201256504662}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:32:28,740][0m Trial 16 finished with value: 0.030331401025446562 and parameters: {'observation_period_num': 12, 'train_rates': 0.8677981092899698, 'learning_rate': 0.0003383647055863679, 'batch_size': 167, 'step_size': 10, 'gamma': 0.7912404498773941}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:33:19,189][0m Trial 17 finished with value: 0.03192121166571536 and parameters: {'observation_period_num': 16, 'train_rates': 0.7687854324124197, 'learning_rate': 0.00030447338024198914, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8095092240743138}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:33:46,095][0m Trial 18 finished with value: 0.205889095400655 and parameters: {'observation_period_num': 197, 'train_rates': 0.8597384240422292, 'learning_rate': 3.757465113068714e-05, 'batch_size': 198, 'step_size': 11, 'gamma': 0.7700062233279455}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:34:38,475][0m Trial 19 finished with value: 0.03437330318031026 and parameters: {'observation_period_num': 10, 'train_rates': 0.9319040140841087, 'learning_rate': 0.00046925442813194505, 'batch_size': 114, 'step_size': 8, 'gamma': 0.9816053191510152}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:35:44,366][0m Trial 20 finished with value: 0.08731072559589292 and parameters: {'observation_period_num': 146, 'train_rates': 0.7810975513322435, 'learning_rate': 0.00013113193981030118, 'batch_size': 75, 'step_size': 5, 'gamma': 0.8026865719624607}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:36:31,126][0m Trial 21 finished with value: 0.03673300656235023 and parameters: {'observation_period_num': 28, 'train_rates': 0.7704715623486472, 'learning_rate': 0.00041256069388032057, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8105603547514966}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:37:05,958][0m Trial 22 finished with value: 0.04393618435038431 and parameters: {'observation_period_num': 18, 'train_rates': 0.849789511560241, 'learning_rate': 0.00016852469564638742, 'batch_size': 161, 'step_size': 8, 'gamma': 0.7753928701582182}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:38:09,245][0m Trial 23 finished with value: 0.060404680863389404 and parameters: {'observation_period_num': 52, 'train_rates': 0.7619828988446865, 'learning_rate': 0.0005618233134620607, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8151556398996733}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:39:00,379][0m Trial 24 finished with value: 0.03223531885858724 and parameters: {'observation_period_num': 5, 'train_rates': 0.9111051658653143, 'learning_rate': 0.0002383156829011052, 'batch_size': 114, 'step_size': 9, 'gamma': 0.8462896527370204}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:39:29,718][0m Trial 25 finished with value: 0.05846556553901252 and parameters: {'observation_period_num': 40, 'train_rates': 0.8381806549379969, 'learning_rate': 6.743266647386563e-05, 'batch_size': 190, 'step_size': 7, 'gamma': 0.8755217089963516}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:40:08,323][0m Trial 26 finished with value: 0.13039282476194472 and parameters: {'observation_period_num': 65, 'train_rates': 0.9038469089462073, 'learning_rate': 3.410245000381472e-05, 'batch_size': 147, 'step_size': 14, 'gamma': 0.7688857698499104}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:41:28,748][0m Trial 27 finished with value: 0.07482996583601695 and parameters: {'observation_period_num': 72, 'train_rates': 0.7906035456087807, 'learning_rate': 0.0005619493032870566, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8073431133851506}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:42:17,149][0m Trial 28 finished with value: 0.05294821446896918 and parameters: {'observation_period_num': 22, 'train_rates': 0.6765785962859767, 'learning_rate': 0.00017140341897173148, 'batch_size': 99, 'step_size': 9, 'gamma': 0.7806940128911807}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:42:57,661][0m Trial 29 finished with value: 0.05931237668850609 and parameters: {'observation_period_num': 42, 'train_rates': 0.7422589824948768, 'learning_rate': 8.526438646218432e-05, 'batch_size': 127, 'step_size': 12, 'gamma': 0.9496989508072872}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:43:25,897][0m Trial 30 finished with value: 0.09742507097028455 and parameters: {'observation_period_num': 103, 'train_rates': 0.707995623405846, 'learning_rate': 0.0002674112230061789, 'batch_size': 178, 'step_size': 6, 'gamma': 0.837154540560091}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:44:22,541][0m Trial 31 finished with value: 0.038638079315423964 and parameters: {'observation_period_num': 6, 'train_rates': 0.9491821692866758, 'learning_rate': 0.0002337558477768297, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8390407293860018}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:45:10,846][0m Trial 32 finished with value: 0.0336415388172479 and parameters: {'observation_period_num': 21, 'train_rates': 0.9054311085450509, 'learning_rate': 0.0006514197335851985, 'batch_size': 120, 'step_size': 10, 'gamma': 0.8537080647197236}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:45:49,868][0m Trial 33 finished with value: 0.038572503558996704 and parameters: {'observation_period_num': 5, 'train_rates': 0.8801836067087023, 'learning_rate': 0.00011518828055714563, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9002617281392024}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:46:14,575][0m Trial 34 finished with value: 0.05744072049856186 and parameters: {'observation_period_num': 33, 'train_rates': 0.9194998138797742, 'learning_rate': 0.00020942099407211993, 'batch_size': 256, 'step_size': 8, 'gamma': 0.7999603362400517}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:47:23,910][0m Trial 35 finished with value: 0.05167427770000823 and parameters: {'observation_period_num': 58, 'train_rates': 0.8055413880604998, 'learning_rate': 0.00034731062578308446, 'batch_size': 75, 'step_size': 12, 'gamma': 0.820143589532987}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:48:00,766][0m Trial 36 finished with value: 0.036200593521154165 and parameters: {'observation_period_num': 34, 'train_rates': 0.8435595517411051, 'learning_rate': 0.0007044907513273759, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8594950251386858}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:48:59,209][0m Trial 37 finished with value: 0.10067494087908642 and parameters: {'observation_period_num': 244, 'train_rates': 0.939081636320633, 'learning_rate': 0.0004303104823401634, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8832151838351379}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:50:11,538][0m Trial 38 finished with value: 0.09502097060986325 and parameters: {'observation_period_num': 19, 'train_rates': 0.6152799740843512, 'learning_rate': 4.909203906386996e-05, 'batch_size': 60, 'step_size': 13, 'gamma': 0.824483455017919}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:50:39,774][0m Trial 39 finished with value: 0.4683755338191986 and parameters: {'observation_period_num': 222, 'train_rates': 0.9603477225118157, 'learning_rate': 1.3539145338285503e-05, 'batch_size': 217, 'step_size': 6, 'gamma': 0.847659952659851}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:51:17,379][0m Trial 40 finished with value: 0.22018154678403473 and parameters: {'observation_period_num': 132, 'train_rates': 0.8275743072341842, 'learning_rate': 1.9326726009362072e-05, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9158748184476501}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:52:04,874][0m Trial 41 finished with value: 0.03218193783123099 and parameters: {'observation_period_num': 18, 'train_rates': 0.9034281365682815, 'learning_rate': 0.0006806433714663534, 'batch_size': 121, 'step_size': 10, 'gamma': 0.8613327998596854}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:52:51,655][0m Trial 42 finished with value: 0.03275705866440378 and parameters: {'observation_period_num': 19, 'train_rates': 0.8990543247060869, 'learning_rate': 0.0007083417222424208, 'batch_size': 120, 'step_size': 10, 'gamma': 0.8651882594716719}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:53:48,702][0m Trial 43 finished with value: 0.044720693146465 and parameters: {'observation_period_num': 44, 'train_rates': 0.9260622862091008, 'learning_rate': 0.0003867080900912733, 'batch_size': 103, 'step_size': 9, 'gamma': 0.8441099690187969}. Best is trial 12 with value: 0.02792192138939017.[0m
[32m[I 2025-01-08 12:54:54,434][0m Trial 44 finished with value: 0.02513658060215816 and parameters: {'observation_period_num': 5, 'train_rates': 0.8681814748711696, 'learning_rate': 0.0008579355185219953, 'batch_size': 83, 'step_size': 11, 'gamma': 0.8890929133962644}. Best is trial 44 with value: 0.02513658060215816.[0m
[32m[I 2025-01-08 12:56:00,291][0m Trial 45 finished with value: 0.052948713912205264 and parameters: {'observation_period_num': 32, 'train_rates': 0.8686105110899054, 'learning_rate': 0.0009145185448075019, 'batch_size': 83, 'step_size': 12, 'gamma': 0.8986198766870797}. Best is trial 44 with value: 0.02513658060215816.[0m
[32m[I 2025-01-08 12:59:37,419][0m Trial 46 finished with value: 0.034215863611173 and parameters: {'observation_period_num': 16, 'train_rates': 0.7540443042099649, 'learning_rate': 0.000703917449023298, 'batch_size': 22, 'step_size': 11, 'gamma': 0.933715328652749}. Best is trial 44 with value: 0.02513658060215816.[0m
[32m[I 2025-01-08 13:01:09,377][0m Trial 47 finished with value: 0.2276268920881285 and parameters: {'observation_period_num': 54, 'train_rates': 0.8086539355830799, 'learning_rate': 4.459785442251561e-06, 'batch_size': 55, 'step_size': 14, 'gamma': 0.7626333052376818}. Best is trial 44 with value: 0.02513658060215816.[0m
[32m[I 2025-01-08 13:01:43,680][0m Trial 48 finished with value: 0.04027647909513393 and parameters: {'observation_period_num': 27, 'train_rates': 0.8828409818349289, 'learning_rate': 0.0005021611745207855, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8886346416267151}. Best is trial 44 with value: 0.02513658060215816.[0m
[32m[I 2025-01-08 13:02:26,778][0m Trial 49 finished with value: 0.08093343803019666 and parameters: {'observation_period_num': 87, 'train_rates': 0.860004261537365, 'learning_rate': 0.00090628406549146, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9130282108767501}. Best is trial 44 with value: 0.02513658060215816.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-08 13:02:26,788][0m A new study created in memory with name: no-name-4e544328-c219-4b4b-928b-eeae4c0c7eb1[0m
[32m[I 2025-01-08 13:03:39,105][0m Trial 0 finished with value: 0.09868815913796425 and parameters: {'observation_period_num': 107, 'train_rates': 0.9452518033957937, 'learning_rate': 8.457801038433391e-05, 'batch_size': 78, 'step_size': 7, 'gamma': 0.7518513996588523}. Best is trial 0 with value: 0.09868815913796425.[0m
[32m[I 2025-01-08 13:05:09,681][0m Trial 1 finished with value: 0.2162154114672116 and parameters: {'observation_period_num': 150, 'train_rates': 0.7754945505586602, 'learning_rate': 1.2369275675206358e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9258670163055814}. Best is trial 0 with value: 0.09868815913796425.[0m
[32m[I 2025-01-08 13:05:49,729][0m Trial 2 finished with value: 0.1520169329994734 and parameters: {'observation_period_num': 7, 'train_rates': 0.6578849441641466, 'learning_rate': 3.8191255266393464e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.808833749405296}. Best is trial 0 with value: 0.09868815913796425.[0m
[32m[I 2025-01-08 13:07:00,092][0m Trial 3 finished with value: 0.34069306471131067 and parameters: {'observation_period_num': 163, 'train_rates': 0.8545567040997191, 'learning_rate': 4.220064504643451e-06, 'batch_size': 74, 'step_size': 11, 'gamma': 0.8629754634673021}. Best is trial 0 with value: 0.09868815913796425.[0m
[32m[I 2025-01-08 13:07:26,973][0m Trial 4 finished with value: 0.23839237880037756 and parameters: {'observation_period_num': 241, 'train_rates': 0.6391625255003197, 'learning_rate': 0.00028010848420113784, 'batch_size': 159, 'step_size': 4, 'gamma': 0.7510338716767573}. Best is trial 0 with value: 0.09868815913796425.[0m
[32m[I 2025-01-08 13:08:01,639][0m Trial 5 finished with value: 0.08644846081733704 and parameters: {'observation_period_num': 68, 'train_rates': 0.9474943579811945, 'learning_rate': 0.00019428127622652996, 'batch_size': 178, 'step_size': 2, 'gamma': 0.9529215209304521}. Best is trial 5 with value: 0.08644846081733704.[0m
[32m[I 2025-01-08 13:09:47,976][0m Trial 6 finished with value: 0.10001512416979162 and parameters: {'observation_period_num': 195, 'train_rates': 0.8811866700087827, 'learning_rate': 0.0002119621521798745, 'batch_size': 48, 'step_size': 6, 'gamma': 0.8070546131260293}. Best is trial 5 with value: 0.08644846081733704.[0m
[32m[I 2025-01-08 13:10:12,612][0m Trial 7 finished with value: 0.8272533678647244 and parameters: {'observation_period_num': 201, 'train_rates': 0.6407537097096458, 'learning_rate': 1.138543043630829e-06, 'batch_size': 185, 'step_size': 9, 'gamma': 0.785870877962538}. Best is trial 5 with value: 0.08644846081733704.[0m
[32m[I 2025-01-08 13:10:49,667][0m Trial 8 finished with value: 0.23561099697561824 and parameters: {'observation_period_num': 184, 'train_rates': 0.8528332653256752, 'learning_rate': 0.00010944003543114123, 'batch_size': 144, 'step_size': 1, 'gamma': 0.9169152494499059}. Best is trial 5 with value: 0.08644846081733704.[0m
[32m[I 2025-01-08 13:11:11,379][0m Trial 9 finished with value: 0.07434901428499302 and parameters: {'observation_period_num': 103, 'train_rates': 0.7509101876307339, 'learning_rate': 0.0003978526002939598, 'batch_size': 242, 'step_size': 12, 'gamma': 0.8037942920400745}. Best is trial 9 with value: 0.07434901428499302.[0m
[32m[I 2025-01-08 13:11:33,059][0m Trial 10 finished with value: 0.07956380805066232 and parameters: {'observation_period_num': 89, 'train_rates': 0.7326281692265265, 'learning_rate': 0.000973378725089514, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8646363508793579}. Best is trial 9 with value: 0.07434901428499302.[0m
[32m[I 2025-01-08 13:11:55,173][0m Trial 11 finished with value: 0.07804598838844884 and parameters: {'observation_period_num': 95, 'train_rates': 0.7374673490873599, 'learning_rate': 0.0009665713850790327, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8745020748405241}. Best is trial 9 with value: 0.07434901428499302.[0m
[32m[I 2025-01-08 13:12:17,063][0m Trial 12 finished with value: 0.11966867548373579 and parameters: {'observation_period_num': 64, 'train_rates': 0.7183184485467283, 'learning_rate': 0.000646550249291422, 'batch_size': 245, 'step_size': 15, 'gamma': 0.8372311187458517}. Best is trial 9 with value: 0.07434901428499302.[0m
[32m[I 2025-01-08 13:12:42,588][0m Trial 13 finished with value: 0.06472517050301227 and parameters: {'observation_period_num': 35, 'train_rates': 0.786381295958, 'learning_rate': 4.295961038441284e-05, 'batch_size': 211, 'step_size': 13, 'gamma': 0.8913650665785526}. Best is trial 13 with value: 0.06472517050301227.[0m
[32m[I 2025-01-08 13:13:10,068][0m Trial 14 finished with value: 0.06343081958387813 and parameters: {'observation_period_num': 20, 'train_rates': 0.80129727805478, 'learning_rate': 3.006906969300005e-05, 'batch_size': 210, 'step_size': 11, 'gamma': 0.9823260900445858}. Best is trial 14 with value: 0.06343081958387813.[0m
[32m[I 2025-01-08 13:13:37,568][0m Trial 15 finished with value: 0.05498081072530308 and parameters: {'observation_period_num': 10, 'train_rates': 0.8008717273797665, 'learning_rate': 2.221623174489365e-05, 'batch_size': 203, 'step_size': 10, 'gamma': 0.981463116193295}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:14:05,011][0m Trial 16 finished with value: 0.06326480972695636 and parameters: {'observation_period_num': 7, 'train_rates': 0.8297390636521591, 'learning_rate': 1.658469234093577e-05, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9898225505290001}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:14:56,177][0m Trial 17 finished with value: 0.08168016325027959 and parameters: {'observation_period_num': 44, 'train_rates': 0.8983548157402177, 'learning_rate': 1.4420217962310695e-05, 'batch_size': 113, 'step_size': 9, 'gamma': 0.9695993626360128}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:15:21,547][0m Trial 18 finished with value: 0.1887890809246535 and parameters: {'observation_period_num': 51, 'train_rates': 0.8136348395253908, 'learning_rate': 1.1023876632522027e-05, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9458339650626764}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:15:47,900][0m Trial 19 finished with value: 0.1315940797328949 and parameters: {'observation_period_num': 8, 'train_rates': 0.6845153117797793, 'learning_rate': 4.73214829331804e-06, 'batch_size': 186, 'step_size': 6, 'gamma': 0.9888418741201253}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:16:20,708][0m Trial 20 finished with value: 0.12387388986404811 and parameters: {'observation_period_num': 136, 'train_rates': 0.8483141515790549, 'learning_rate': 3.986518261149083e-05, 'batch_size': 162, 'step_size': 10, 'gamma': 0.9146628312494531}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:16:48,086][0m Trial 21 finished with value: 0.06753768134192065 and parameters: {'observation_period_num': 24, 'train_rates': 0.8239070863263723, 'learning_rate': 2.467574602396059e-05, 'batch_size': 211, 'step_size': 11, 'gamma': 0.9858103903854867}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:17:13,477][0m Trial 22 finished with value: 0.07967666686541025 and parameters: {'observation_period_num': 26, 'train_rates': 0.7880036688899053, 'learning_rate': 2.477816177640274e-05, 'batch_size': 220, 'step_size': 8, 'gamma': 0.9552536587560441}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:17:44,241][0m Trial 23 finished with value: 0.08102964675085311 and parameters: {'observation_period_num': 67, 'train_rates': 0.8974089824276967, 'learning_rate': 6.351477909613825e-05, 'batch_size': 194, 'step_size': 11, 'gamma': 0.9703766720090419}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:18:12,808][0m Trial 24 finished with value: 0.10247678309679031 and parameters: {'observation_period_num': 5, 'train_rates': 0.9819921680063971, 'learning_rate': 8.411118186546287e-06, 'batch_size': 228, 'step_size': 13, 'gamma': 0.936917677925573}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:18:40,206][0m Trial 25 finished with value: 0.05661554722224965 and parameters: {'observation_period_num': 26, 'train_rates': 0.8085307374491887, 'learning_rate': 2.2806381769947827e-05, 'batch_size': 198, 'step_size': 8, 'gamma': 0.9687383028321437}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:19:10,890][0m Trial 26 finished with value: 0.6472170003899583 and parameters: {'observation_period_num': 47, 'train_rates': 0.6946749740047655, 'learning_rate': 1.655106643131154e-06, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9664402315831455}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:23:47,647][0m Trial 27 finished with value: 0.07031424111754597 and parameters: {'observation_period_num': 77, 'train_rates': 0.7573514535348614, 'learning_rate': 1.8702915850836418e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8929180079550293}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:24:30,998][0m Trial 28 finished with value: 0.2225554525351324 and parameters: {'observation_period_num': 35, 'train_rates': 0.8371140857482418, 'learning_rate': 7.429010981271501e-06, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9385856478474965}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:24:59,799][0m Trial 29 finished with value: 0.10005669542721339 and parameters: {'observation_period_num': 113, 'train_rates': 0.8768617468457203, 'learning_rate': 8.736384156040036e-05, 'batch_size': 192, 'step_size': 7, 'gamma': 0.962047822372937}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:25:34,418][0m Trial 30 finished with value: 0.1252174692362854 and parameters: {'observation_period_num': 120, 'train_rates': 0.9316869048707359, 'learning_rate': 5.4334498125336245e-05, 'batch_size': 171, 'step_size': 10, 'gamma': 0.9041785772897781}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:26:01,935][0m Trial 31 finished with value: 0.06037108460613062 and parameters: {'observation_period_num': 20, 'train_rates': 0.8154329115543241, 'learning_rate': 3.2370868361062286e-05, 'batch_size': 205, 'step_size': 10, 'gamma': 0.9829563528827879}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:26:30,158][0m Trial 32 finished with value: 0.08042125675103647 and parameters: {'observation_period_num': 24, 'train_rates': 0.7631405427363787, 'learning_rate': 1.6960048284679864e-05, 'batch_size': 201, 'step_size': 10, 'gamma': 0.9747418160807241}. Best is trial 15 with value: 0.05498081072530308.[0m
[32m[I 2025-01-08 13:26:56,031][0m Trial 33 finished with value: 0.041226681833834156 and parameters: {'observation_period_num': 5, 'train_rates': 0.8128032392658141, 'learning_rate': 0.00012364868619792172, 'batch_size': 230, 'step_size': 8, 'gamma': 0.9338543375014172}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:27:21,453][0m Trial 34 finished with value: 0.05247417927076402 and parameters: {'observation_period_num': 48, 'train_rates': 0.8042837433054505, 'learning_rate': 0.00013082097800239432, 'batch_size': 233, 'step_size': 8, 'gamma': 0.9296430232586481}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:27:45,263][0m Trial 35 finished with value: 0.05839203113824083 and parameters: {'observation_period_num': 55, 'train_rates': 0.7751486968771198, 'learning_rate': 0.00014281183729961713, 'batch_size': 235, 'step_size': 6, 'gamma': 0.9260294240944609}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:28:10,693][0m Trial 36 finished with value: 0.049775351800889656 and parameters: {'observation_period_num': 32, 'train_rates': 0.8027715917303293, 'learning_rate': 7.153969179926122e-05, 'batch_size': 229, 'step_size': 8, 'gamma': 0.9282510293101159}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:28:35,587][0m Trial 37 finished with value: 0.07177903102734909 and parameters: {'observation_period_num': 81, 'train_rates': 0.8654446611552573, 'learning_rate': 0.000123048247759804, 'batch_size': 230, 'step_size': 5, 'gamma': 0.9274773740406266}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:28:54,426][0m Trial 38 finished with value: 0.10097162705884936 and parameters: {'observation_period_num': 38, 'train_rates': 0.6053153297069347, 'learning_rate': 0.0003911080792673075, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8901993659483934}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:29:45,382][0m Trial 39 finished with value: 0.11457594998180866 and parameters: {'observation_period_num': 230, 'train_rates': 0.779963407551777, 'learning_rate': 0.00019851719905743218, 'batch_size': 93, 'step_size': 5, 'gamma': 0.908154536259183}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:30:07,597][0m Trial 40 finished with value: 0.12192263318005428 and parameters: {'observation_period_num': 155, 'train_rates': 0.7013914726729256, 'learning_rate': 7.201524915724351e-05, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9483337345566445}. Best is trial 33 with value: 0.041226681833834156.[0m
[32m[I 2025-01-08 13:30:33,333][0m Trial 41 finished with value: 0.040226668623896934 and parameters: {'observation_period_num': 12, 'train_rates': 0.8045511348696094, 'learning_rate': 8.67954265151491e-05, 'batch_size': 219, 'step_size': 8, 'gamma': 0.9351142782785312}. Best is trial 41 with value: 0.040226668623896934.[0m
[32m[I 2025-01-08 13:30:58,941][0m Trial 42 finished with value: 0.03715341836790141 and parameters: {'observation_period_num': 14, 'train_rates': 0.8023048580831199, 'learning_rate': 0.0002970149179972511, 'batch_size': 223, 'step_size': 8, 'gamma': 0.9292061411262936}. Best is trial 42 with value: 0.03715341836790141.[0m
[32m[I 2025-01-08 13:31:26,928][0m Trial 43 finished with value: 0.09601639315359674 and parameters: {'observation_period_num': 58, 'train_rates': 0.8371400289442559, 'learning_rate': 0.0003182700779630546, 'batch_size': 220, 'step_size': 8, 'gamma': 0.9339789043376813}. Best is trial 42 with value: 0.03715341836790141.[0m
[32m[I 2025-01-08 13:31:51,226][0m Trial 44 finished with value: 0.10259528183676861 and parameters: {'observation_period_num': 34, 'train_rates': 0.7493145438819038, 'learning_rate': 0.00015511316517667867, 'batch_size': 242, 'step_size': 3, 'gamma': 0.8425745508000688}. Best is trial 42 with value: 0.03715341836790141.[0m
[32m[I 2025-01-08 13:32:21,016][0m Trial 45 finished with value: 0.044057382211695965 and parameters: {'observation_period_num': 43, 'train_rates': 0.7725788881679131, 'learning_rate': 0.0002581285980631503, 'batch_size': 175, 'step_size': 7, 'gamma': 0.9137907093111899}. Best is trial 42 with value: 0.03715341836790141.[0m
[32m[I 2025-01-08 13:32:51,001][0m Trial 46 finished with value: 0.028935510561436038 and parameters: {'observation_period_num': 14, 'train_rates': 0.7316087954444779, 'learning_rate': 0.0005699121417227269, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9180099776477296}. Best is trial 46 with value: 0.028935510561436038.[0m
[32m[I 2025-01-08 13:33:27,355][0m Trial 47 finished with value: 0.032530059260929944 and parameters: {'observation_period_num': 14, 'train_rates': 0.7189759226668689, 'learning_rate': 0.0005241587524919314, 'batch_size': 140, 'step_size': 4, 'gamma': 0.8997822197575531}. Best is trial 46 with value: 0.028935510561436038.[0m
[32m[I 2025-01-08 13:33:59,981][0m Trial 48 finished with value: 0.03925835967913664 and parameters: {'observation_period_num': 17, 'train_rates': 0.6658794824834477, 'learning_rate': 0.0006192217198180137, 'batch_size': 148, 'step_size': 4, 'gamma': 0.9011447502279848}. Best is trial 46 with value: 0.028935510561436038.[0m
[32m[I 2025-01-08 13:34:32,850][0m Trial 49 finished with value: 0.04377244431238908 and parameters: {'observation_period_num': 16, 'train_rates': 0.6682967696362785, 'learning_rate': 0.0005860712890104795, 'batch_size': 144, 'step_size': 3, 'gamma': 0.8752924244555123}. Best is trial 46 with value: 0.028935510561436038.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-08 13:34:32,860][0m A new study created in memory with name: no-name-8a564bfd-bec1-47f7-a0f0-8a22b442d2fe[0m
[32m[I 2025-01-08 13:35:05,141][0m Trial 0 finished with value: 0.4268629252910614 and parameters: {'observation_period_num': 89, 'train_rates': 0.9800419467625525, 'learning_rate': 3.6551655989694518e-06, 'batch_size': 190, 'step_size': 15, 'gamma': 0.9578197068754071}. Best is trial 0 with value: 0.4268629252910614.[0m
[32m[I 2025-01-08 13:35:45,610][0m Trial 1 finished with value: 0.10974956303834915 and parameters: {'observation_period_num': 119, 'train_rates': 0.9492192449732617, 'learning_rate': 0.0008105797760787784, 'batch_size': 149, 'step_size': 2, 'gamma': 0.8158750168320283}. Best is trial 1 with value: 0.10974956303834915.[0m
[32m[I 2025-01-08 13:36:30,509][0m Trial 2 finished with value: 0.6598593175411225 and parameters: {'observation_period_num': 190, 'train_rates': 0.7505521265518441, 'learning_rate': 3.952535493520717e-06, 'batch_size': 107, 'step_size': 7, 'gamma': 0.839350730811721}. Best is trial 1 with value: 0.10974956303834915.[0m
[32m[I 2025-01-08 13:37:44,638][0m Trial 3 finished with value: 0.32615951208624466 and parameters: {'observation_period_num': 230, 'train_rates': 0.8464832363869187, 'learning_rate': 1.3799639140496566e-05, 'batch_size': 67, 'step_size': 3, 'gamma': 0.8850601662789949}. Best is trial 1 with value: 0.10974956303834915.[0m
[32m[I 2025-01-08 13:38:26,939][0m Trial 4 finished with value: 0.05445977654613433 and parameters: {'observation_period_num': 6, 'train_rates': 0.9172855862478375, 'learning_rate': 4.833071619337321e-05, 'batch_size': 140, 'step_size': 9, 'gamma': 0.9803165298692939}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:38:47,380][0m Trial 5 finished with value: 0.3773739242820236 and parameters: {'observation_period_num': 142, 'train_rates': 0.6502376023468945, 'learning_rate': 1.7073373276819906e-05, 'batch_size': 230, 'step_size': 3, 'gamma': 0.9548093765082384}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:40:06,837][0m Trial 6 finished with value: 0.05453040464242224 and parameters: {'observation_period_num': 71, 'train_rates': 0.8175212896945083, 'learning_rate': 7.03992862744571e-05, 'batch_size': 64, 'step_size': 8, 'gamma': 0.9312482598023815}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:40:59,351][0m Trial 7 finished with value: 0.907790225188611 and parameters: {'observation_period_num': 136, 'train_rates': 0.7070970992502137, 'learning_rate': 2.3236831768530903e-06, 'batch_size': 87, 'step_size': 2, 'gamma': 0.9257540537940157}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:41:58,290][0m Trial 8 finished with value: 0.17847809124501254 and parameters: {'observation_period_num': 129, 'train_rates': 0.6828655675312456, 'learning_rate': 0.0006076461707654803, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9434032922871597}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:42:48,558][0m Trial 9 finished with value: 0.08678462356328964 and parameters: {'observation_period_num': 179, 'train_rates': 0.9730244069512777, 'learning_rate': 0.00026813633881745335, 'batch_size': 116, 'step_size': 6, 'gamma': 0.8320693178581191}. Best is trial 4 with value: 0.05445977654613433.[0m
[32m[I 2025-01-08 13:46:40,725][0m Trial 10 finished with value: 0.028250253609111233 and parameters: {'observation_period_num': 5, 'train_rates': 0.8845619274424112, 'learning_rate': 6.989150330015841e-05, 'batch_size': 23, 'step_size': 11, 'gamma': 0.763118806549035}. Best is trial 10 with value: 0.028250253609111233.[0m
[32m[I 2025-01-08 13:51:23,439][0m Trial 11 finished with value: 0.023965259135436658 and parameters: {'observation_period_num': 7, 'train_rates': 0.886396979725396, 'learning_rate': 8.012917276576874e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7539683187512453}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 13:56:00,747][0m Trial 12 finished with value: 0.02955162129251332 and parameters: {'observation_period_num': 16, 'train_rates': 0.8607259047505098, 'learning_rate': 0.00014583020107102648, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7567362857836705}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 13:59:41,663][0m Trial 13 finished with value: 0.04356111969133156 and parameters: {'observation_period_num': 47, 'train_rates': 0.8905041539288995, 'learning_rate': 0.0001345890816134034, 'batch_size': 24, 'step_size': 11, 'gamma': 0.7533959513433003}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:01:37,992][0m Trial 14 finished with value: 0.06431199666226935 and parameters: {'observation_period_num': 39, 'train_rates': 0.776589250252461, 'learning_rate': 2.199249731844662e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.79277174219494}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:03:28,716][0m Trial 15 finished with value: 0.1309694651823612 and parameters: {'observation_period_num': 53, 'train_rates': 0.904691329917501, 'learning_rate': 8.296544533957445e-06, 'batch_size': 49, 'step_size': 11, 'gamma': 0.7853096006488304}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:06:33,017][0m Trial 16 finished with value: 0.14741982327794845 and parameters: {'observation_period_num': 24, 'train_rates': 0.6005822796123699, 'learning_rate': 7.193070407427418e-05, 'batch_size': 22, 'step_size': 13, 'gamma': 0.8741741654798137}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:07:01,852][0m Trial 17 finished with value: 0.08008969453812549 and parameters: {'observation_period_num': 99, 'train_rates': 0.815387283996107, 'learning_rate': 0.00022370142890069502, 'batch_size': 206, 'step_size': 10, 'gamma': 0.783393239708999}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:07:37,402][0m Trial 18 finished with value: 0.08937035784449267 and parameters: {'observation_period_num': 69, 'train_rates': 0.8726824305818033, 'learning_rate': 3.78399064003352e-05, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8033958481323868}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:08:38,210][0m Trial 19 finished with value: 0.04745044765564112 and parameters: {'observation_period_num': 30, 'train_rates': 0.933449822268967, 'learning_rate': 0.00040991126187227874, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8504056336783135}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:10:28,339][0m Trial 20 finished with value: 0.15213986340221192 and parameters: {'observation_period_num': 242, 'train_rates': 0.765041341026763, 'learning_rate': 0.00010903596800085564, 'batch_size': 41, 'step_size': 15, 'gamma': 0.7685538047685218}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:15:16,571][0m Trial 21 finished with value: 0.027730832843693543 and parameters: {'observation_period_num': 5, 'train_rates': 0.8422897614516777, 'learning_rate': 0.00016837341740763827, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7538501240721558}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:17:04,410][0m Trial 22 finished with value: 0.026049490647057732 and parameters: {'observation_period_num': 8, 'train_rates': 0.8348448076070987, 'learning_rate': 0.00021571181721550278, 'batch_size': 49, 'step_size': 12, 'gamma': 0.7697871083152517}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:18:38,631][0m Trial 23 finished with value: 0.0568539166341504 and parameters: {'observation_period_num': 55, 'train_rates': 0.8282474426869186, 'learning_rate': 0.0003559026258140927, 'batch_size': 55, 'step_size': 13, 'gamma': 0.816430715277706}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:20:39,528][0m Trial 24 finished with value: 0.05253859352986169 and parameters: {'observation_period_num': 31, 'train_rates': 0.7304689658662683, 'learning_rate': 0.00019381104963223382, 'batch_size': 39, 'step_size': 9, 'gamma': 0.7722320451878434}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:25:40,313][0m Trial 25 finished with value: 0.10806170394834207 and parameters: {'observation_period_num': 81, 'train_rates': 0.7954694063848398, 'learning_rate': 0.0005043214313214917, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7514104818498634}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:26:26,171][0m Trial 26 finished with value: 0.03475835734786207 and parameters: {'observation_period_num': 7, 'train_rates': 0.8489936501084195, 'learning_rate': 0.00010664263102384455, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8951488595942942}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:27:31,417][0m Trial 27 finished with value: 0.05865173491830635 and parameters: {'observation_period_num': 27, 'train_rates': 0.7949659785588801, 'learning_rate': 2.9379622845254296e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8042188973627424}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:27:54,150][0m Trial 28 finished with value: 0.04933169842460903 and parameters: {'observation_period_num': 57, 'train_rates': 0.8371415955967081, 'learning_rate': 0.0009645661322837956, 'batch_size': 253, 'step_size': 9, 'gamma': 0.7763909535247725}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:28:28,414][0m Trial 29 finished with value: 0.08786096423864365 and parameters: {'observation_period_num': 110, 'train_rates': 0.9737950445942092, 'learning_rate': 0.00029455529761116255, 'batch_size': 178, 'step_size': 15, 'gamma': 0.7973339921729863}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:30:52,416][0m Trial 30 finished with value: 0.13389390406194426 and parameters: {'observation_period_num': 160, 'train_rates': 0.9239298285014165, 'learning_rate': 4.9839235701008734e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8180862705113455}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:33:42,039][0m Trial 31 finished with value: 0.028954006163430704 and parameters: {'observation_period_num': 10, 'train_rates': 0.8844146702414496, 'learning_rate': 6.309246028741559e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7687320594718695}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:35:14,810][0m Trial 32 finished with value: 0.043193124375253356 and parameters: {'observation_period_num': 39, 'train_rates': 0.8910657043382705, 'learning_rate': 8.975385207857272e-05, 'batch_size': 60, 'step_size': 10, 'gamma': 0.7613567870271779}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:37:05,752][0m Trial 33 finished with value: 0.038715205465753876 and parameters: {'observation_period_num': 19, 'train_rates': 0.9469711877543322, 'learning_rate': 0.0001578129622421958, 'batch_size': 52, 'step_size': 12, 'gamma': 0.7822736492156163}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:42:32,657][0m Trial 34 finished with value: 0.04722269777865971 and parameters: {'observation_period_num': 6, 'train_rates': 0.8618463862906844, 'learning_rate': 9.86976064360733e-06, 'batch_size': 16, 'step_size': 11, 'gamma': 0.751249738478437}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:45:30,307][0m Trial 35 finished with value: 0.0456031162898536 and parameters: {'observation_period_num': 41, 'train_rates': 0.9094108005601542, 'learning_rate': 3.198515356139465e-05, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8422464225662984}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:46:43,793][0m Trial 36 finished with value: 0.05833154424945016 and parameters: {'observation_period_num': 67, 'train_rates': 0.8065393644447847, 'learning_rate': 0.00063366783409658, 'batch_size': 70, 'step_size': 8, 'gamma': 0.767374384296101}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:47:37,680][0m Trial 37 finished with value: 1.0942579985180974 and parameters: {'observation_period_num': 90, 'train_rates': 0.8390921429605382, 'learning_rate': 1.0984200831068674e-06, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8269327525618774}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:49:18,247][0m Trial 38 finished with value: 0.10832429653354012 and parameters: {'observation_period_num': 222, 'train_rates': 0.8746898100624719, 'learning_rate': 5.160770230708885e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9047279556717249}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:52:41,823][0m Trial 39 finished with value: 0.033449091920346924 and parameters: {'observation_period_num': 20, 'train_rates': 0.9890680267857365, 'learning_rate': 0.00021087615275811375, 'batch_size': 29, 'step_size': 14, 'gamma': 0.7962961473886381}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:54:02,586][0m Trial 40 finished with value: 0.10581890866160393 and parameters: {'observation_period_num': 212, 'train_rates': 0.9505362372511912, 'learning_rate': 7.834958603246089e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9743867202873359}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:57:03,556][0m Trial 41 finished with value: 0.03214500002430741 and parameters: {'observation_period_num': 6, 'train_rates': 0.8856816259046989, 'learning_rate': 4.714503452752787e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7660559290001488}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 14:59:39,110][0m Trial 42 finished with value: 0.0280744731426239 and parameters: {'observation_period_num': 12, 'train_rates': 0.8556410710348211, 'learning_rate': 6.543385155902078e-05, 'batch_size': 34, 'step_size': 11, 'gamma': 0.7765215812689098}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:01:53,991][0m Trial 43 finished with value: 0.032204031378907316 and parameters: {'observation_period_num': 19, 'train_rates': 0.8554837813332371, 'learning_rate': 0.00011945135012927698, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7801235393661705}. Best is trial 11 with value: 0.023965259135436658.[0m
Early stopping at epoch 51
[32m[I 2025-01-08 15:02:27,688][0m Trial 44 finished with value: 0.12429563670365279 and parameters: {'observation_period_num': 34, 'train_rates': 0.8346004774591157, 'learning_rate': 0.00018218334300024655, 'batch_size': 82, 'step_size': 1, 'gamma': 0.7595104448990804}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:04:21,428][0m Trial 45 finished with value: 0.055940674610749974 and parameters: {'observation_period_num': 16, 'train_rates': 0.911324947824986, 'learning_rate': 2.2867377684634767e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7895126091141442}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:05:43,999][0m Trial 46 finished with value: 0.04635615807241657 and parameters: {'observation_period_num': 43, 'train_rates': 0.7798839693680703, 'learning_rate': 9.201242360555356e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.8601537315398134}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:09:23,125][0m Trial 47 finished with value: 0.03620618235033292 and parameters: {'observation_period_num': 25, 'train_rates': 0.8671509845116994, 'learning_rate': 0.00025153088476660327, 'batch_size': 24, 'step_size': 11, 'gamma': 0.7510254121629123}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:10:00,518][0m Trial 48 finished with value: 0.05232051302960446 and parameters: {'observation_period_num': 5, 'train_rates': 0.8251398257275113, 'learning_rate': 6.426547713802574e-05, 'batch_size': 149, 'step_size': 10, 'gamma': 0.8081938062045818}. Best is trial 11 with value: 0.023965259135436658.[0m
[32m[I 2025-01-08 15:15:32,812][0m Trial 49 finished with value: 0.095674922085526 and parameters: {'observation_period_num': 61, 'train_rates': 0.8968998516232364, 'learning_rate': 1.3798665959804039e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7621097508582922}. Best is trial 11 with value: 0.023965259135436658.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.7640856420447427, 'learning_rate': 0.0003624766556945417, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9707899851485432}
Epoch 1/300, trend Loss: 0.1791 | 0.0956
Epoch 2/300, trend Loss: 0.1139 | 0.0821
Epoch 3/300, trend Loss: 0.1061 | 0.0718
Epoch 4/300, trend Loss: 0.1005 | 0.0733
Epoch 5/300, trend Loss: 0.0940 | 0.0548
Epoch 6/300, trend Loss: 0.0889 | 0.0547
Epoch 7/300, trend Loss: 0.0858 | 0.0492
Epoch 8/300, trend Loss: 0.0833 | 0.0494
Epoch 9/300, trend Loss: 0.0817 | 0.0487
Epoch 10/300, trend Loss: 0.0796 | 0.0505
Epoch 11/300, trend Loss: 0.0788 | 0.0531
Epoch 12/300, trend Loss: 0.0779 | 0.0506
Epoch 13/300, trend Loss: 0.0764 | 0.0493
Epoch 14/300, trend Loss: 0.0751 | 0.0468
Epoch 15/300, trend Loss: 0.0742 | 0.0524
Epoch 16/300, trend Loss: 0.0743 | 0.0400
Epoch 17/300, trend Loss: 0.0715 | 0.0357
Epoch 18/300, trend Loss: 0.0696 | 0.0318
Epoch 19/300, trend Loss: 0.0699 | 0.0324
Epoch 20/300, trend Loss: 0.0679 | 0.0362
Epoch 21/300, trend Loss: 0.0669 | 0.0360
Epoch 22/300, trend Loss: 0.0666 | 0.0354
Epoch 23/300, trend Loss: 0.0661 | 0.0321
Epoch 24/300, trend Loss: 0.0673 | 0.0378
Epoch 25/300, trend Loss: 0.0646 | 0.0286
Epoch 26/300, trend Loss: 0.0627 | 0.0293
Epoch 27/300, trend Loss: 0.0624 | 0.0287
Epoch 28/300, trend Loss: 0.0622 | 0.0319
Epoch 29/300, trend Loss: 0.0630 | 0.0303
Epoch 30/300, trend Loss: 0.0619 | 0.0356
Epoch 31/300, trend Loss: 0.0608 | 0.0343
Epoch 32/300, trend Loss: 0.0603 | 0.0320
Epoch 33/300, trend Loss: 0.0620 | 0.0344
Epoch 34/300, trend Loss: 0.0608 | 0.0310
Epoch 35/300, trend Loss: 0.0615 | 0.0291
Epoch 36/300, trend Loss: 0.0590 | 0.0314
Epoch 37/300, trend Loss: 0.0579 | 0.0349
Epoch 38/300, trend Loss: 0.0582 | 0.0448
Epoch 39/300, trend Loss: 0.0578 | 0.0568
Epoch 40/300, trend Loss: 0.0573 | 0.0440
Epoch 41/300, trend Loss: 0.0569 | 0.0434
Epoch 42/300, trend Loss: 0.0583 | 0.0388
Epoch 43/300, trend Loss: 0.0572 | 0.0379
Epoch 44/300, trend Loss: 0.0566 | 0.0280
Epoch 45/300, trend Loss: 0.0571 | 0.0336
Epoch 46/300, trend Loss: 0.0553 | 0.0528
Epoch 47/300, trend Loss: 0.0556 | 0.0373
Epoch 48/300, trend Loss: 0.0547 | 0.0303
Epoch 49/300, trend Loss: 0.0557 | 0.0301
Epoch 50/300, trend Loss: 0.0558 | 0.0389
Epoch 51/300, trend Loss: 0.0541 | 0.0328
Epoch 52/300, trend Loss: 0.0550 | 0.0311
Epoch 53/300, trend Loss: 0.0560 | 0.0363
Epoch 54/300, trend Loss: 0.0562 | 0.0384
Epoch 55/300, trend Loss: 0.0560 | 0.0344
Epoch 56/300, trend Loss: 0.0549 | 0.0458
Epoch 57/300, trend Loss: 0.0536 | 0.0474
Epoch 58/300, trend Loss: 0.0531 | 0.0314
Epoch 59/300, trend Loss: 0.0537 | 0.0280
Epoch 60/300, trend Loss: 0.0529 | 0.0312
Epoch 61/300, trend Loss: 0.0517 | 0.0309
Epoch 62/300, trend Loss: 0.0522 | 0.0315
Epoch 63/300, trend Loss: 0.0509 | 0.0375
Epoch 64/300, trend Loss: 0.0512 | 0.0401
Epoch 65/300, trend Loss: 0.0512 | 0.0306
Epoch 66/300, trend Loss: 0.0540 | 0.0324
Epoch 67/300, trend Loss: 0.0531 | 0.0331
Epoch 68/300, trend Loss: 0.0540 | 0.0310
Epoch 69/300, trend Loss: 0.0547 | 0.0353
Epoch 70/300, trend Loss: 0.0607 | 0.0261
Epoch 71/300, trend Loss: 0.0575 | 0.0249
Epoch 72/300, trend Loss: 0.0531 | 0.0263
Epoch 73/300, trend Loss: 0.0513 | 0.0235
Epoch 74/300, trend Loss: 0.0504 | 0.0252
Epoch 75/300, trend Loss: 0.0502 | 0.0271
Epoch 76/300, trend Loss: 0.0491 | 0.0251
Epoch 77/300, trend Loss: 0.0485 | 0.0286
Epoch 78/300, trend Loss: 0.0491 | 0.0269
Epoch 79/300, trend Loss: 0.0502 | 0.0265
Epoch 80/300, trend Loss: 0.0467 | 0.0270
Epoch 81/300, trend Loss: 0.0517 | 0.0277
Epoch 82/300, trend Loss: 0.0564 | 0.0271
Epoch 83/300, trend Loss: 0.0506 | 0.0284
Epoch 84/300, trend Loss: 0.0475 | 0.0278
Epoch 85/300, trend Loss: 0.0475 | 0.0250
Epoch 86/300, trend Loss: 0.0504 | 0.0267
Epoch 87/300, trend Loss: 0.0492 | 0.0247
Epoch 88/300, trend Loss: 0.0487 | 0.0268
Epoch 89/300, trend Loss: 0.0475 | 0.0269
Epoch 90/300, trend Loss: 0.0470 | 0.0277
Epoch 91/300, trend Loss: 0.0466 | 0.0279
Epoch 92/300, trend Loss: 0.0460 | 0.0275
Epoch 93/300, trend Loss: 0.0433 | 0.0277
Epoch 94/300, trend Loss: 0.0470 | 0.0293
Epoch 95/300, trend Loss: 0.0503 | 0.0350
Epoch 96/300, trend Loss: 0.0456 | 0.0418
Epoch 97/300, trend Loss: 0.0457 | 0.0333
Epoch 98/300, trend Loss: 0.0491 | 0.0273
Epoch 99/300, trend Loss: 0.0481 | 0.0312
Epoch 100/300, trend Loss: 0.0474 | 0.0333
Epoch 101/300, trend Loss: 0.0466 | 0.0308
Epoch 102/300, trend Loss: 0.0465 | 0.0351
Epoch 103/300, trend Loss: 0.0453 | 0.0368
Epoch 104/300, trend Loss: 0.0442 | 0.0364
Epoch 105/300, trend Loss: 0.0563 | 0.0300
Epoch 106/300, trend Loss: 0.0645 | 0.0315
Epoch 107/300, trend Loss: 0.0591 | 0.0410
Epoch 108/300, trend Loss: 0.0604 | 0.0316
Epoch 109/300, trend Loss: 0.0555 | 0.0264
Epoch 110/300, trend Loss: 0.0505 | 0.0265
Epoch 111/300, trend Loss: 0.0467 | 0.0248
Epoch 112/300, trend Loss: 0.0499 | 0.0307
Epoch 113/300, trend Loss: 0.0459 | 0.0236
Epoch 114/300, trend Loss: 0.0479 | 0.0276
Epoch 115/300, trend Loss: 0.0452 | 0.0270
Epoch 116/300, trend Loss: 0.0475 | 0.0239
Epoch 117/300, trend Loss: 0.0485 | 0.0237
Epoch 118/300, trend Loss: 0.0467 | 0.0239
Epoch 119/300, trend Loss: 0.0461 | 0.0253
Epoch 120/300, trend Loss: 0.0416 | 0.0244
Epoch 121/300, trend Loss: 0.0601 | 0.0351
Epoch 122/300, trend Loss: 0.0556 | 0.0358
Epoch 123/300, trend Loss: 0.0468 | 0.0302
Epoch 124/300, trend Loss: 0.0437 | 0.0329
Epoch 125/300, trend Loss: 0.0418 | 0.0254
Epoch 126/300, trend Loss: 0.0480 | 0.0254
Epoch 127/300, trend Loss: 0.0466 | 0.0254
Epoch 128/300, trend Loss: 0.0458 | 0.0262
Epoch 129/300, trend Loss: 0.0423 | 0.0251
Epoch 130/300, trend Loss: 0.0418 | 0.0258
Epoch 131/300, trend Loss: 0.0392 | 0.0270
Epoch 132/300, trend Loss: 0.0385 | 0.0359
Epoch 133/300, trend Loss: 0.0391 | 0.0261
Epoch 134/300, trend Loss: 0.0404 | 0.0257
Epoch 135/300, trend Loss: 0.0392 | 0.0259
Epoch 136/300, trend Loss: 0.0385 | 0.0260
Epoch 137/300, trend Loss: 0.0380 | 0.0272
Epoch 138/300, trend Loss: 0.0377 | 0.0280
Epoch 139/300, trend Loss: 0.0370 | 0.0268
Epoch 140/300, trend Loss: 0.0381 | 0.0318
Epoch 141/300, trend Loss: 0.0381 | 0.0268
Epoch 142/300, trend Loss: 0.0389 | 0.0267
Epoch 143/300, trend Loss: 0.0382 | 0.0261
Epoch 144/300, trend Loss: 0.0398 | 0.0307
Epoch 145/300, trend Loss: 0.0500 | 0.0369
Epoch 146/300, trend Loss: 0.0601 | 0.0273
Epoch 147/300, trend Loss: 0.0507 | 0.0273
Epoch 148/300, trend Loss: 0.0463 | 0.0288
Epoch 149/300, trend Loss: 0.0524 | 0.0265
Epoch 150/300, trend Loss: 0.0499 | 0.0263
Epoch 151/300, trend Loss: 0.0424 | 0.0249
Epoch 152/300, trend Loss: 0.0406 | 0.0268
Epoch 153/300, trend Loss: 0.0471 | 0.0264
Epoch 154/300, trend Loss: 0.0398 | 0.0239
Epoch 155/300, trend Loss: 0.0437 | 0.0251
Epoch 156/300, trend Loss: 0.0382 | 0.0244
Epoch 157/300, trend Loss: 0.0371 | 0.0253
Epoch 158/300, trend Loss: 0.0370 | 0.0255
Epoch 159/300, trend Loss: 0.0365 | 0.0252
Epoch 160/300, trend Loss: 0.0359 | 0.0263
Epoch 161/300, trend Loss: 0.0370 | 0.0265
Epoch 162/300, trend Loss: 0.0377 | 0.0262
Epoch 163/300, trend Loss: 0.0404 | 0.0282
Epoch 164/300, trend Loss: 0.0405 | 0.0271
Epoch 165/300, trend Loss: 0.0397 | 0.0258
Epoch 166/300, trend Loss: 0.0446 | 0.0272
Epoch 167/300, trend Loss: 0.0431 | 0.0264
Epoch 168/300, trend Loss: 0.0421 | 0.0275
Epoch 169/300, trend Loss: 0.0395 | 0.0276
Epoch 170/300, trend Loss: 0.0353 | 0.0271
Epoch 171/300, trend Loss: 0.0346 | 0.0272
Epoch 172/300, trend Loss: 0.0361 | 0.0277
Epoch 173/300, trend Loss: 0.0353 | 0.0277
Epoch 174/300, trend Loss: 0.0354 | 0.0274
Epoch 175/300, trend Loss: 0.0355 | 0.0276
Epoch 176/300, trend Loss: 0.0350 | 0.0286
Epoch 177/300, trend Loss: 0.0337 | 0.0315
Epoch 178/300, trend Loss: 0.0345 | 0.0291
Epoch 179/300, trend Loss: 0.0337 | 0.0325
Epoch 180/300, trend Loss: 0.0374 | 0.0310
Epoch 181/300, trend Loss: 0.0379 | 0.0271
Epoch 182/300, trend Loss: 0.0436 | 0.0285
Epoch 183/300, trend Loss: 0.0423 | 0.0329
Epoch 184/300, trend Loss: 0.0400 | 0.0277
Epoch 185/300, trend Loss: 0.0373 | 0.0295
Epoch 186/300, trend Loss: 0.0330 | 0.0287
Epoch 187/300, trend Loss: 0.0501 | 0.0295
Epoch 188/300, trend Loss: 0.0462 | 0.0253
Epoch 189/300, trend Loss: 0.0443 | 0.0261
Epoch 190/300, trend Loss: 0.0429 | 0.0275
Epoch 191/300, trend Loss: 0.0367 | 0.0269
Epoch 192/300, trend Loss: 0.0336 | 0.0269
Epoch 193/300, trend Loss: 0.0347 | 0.0273
Epoch 194/300, trend Loss: 0.0330 | 0.0273
Epoch 195/300, trend Loss: 0.0338 | 0.0274
Epoch 196/300, trend Loss: 0.0405 | 0.0279
Epoch 197/300, trend Loss: 0.0429 | 0.0323
Epoch 198/300, trend Loss: 0.0393 | 0.0298
Epoch 199/300, trend Loss: 0.0371 | 0.0270
Epoch 200/300, trend Loss: 0.0429 | 0.0277
Epoch 201/300, trend Loss: 0.0415 | 0.0281
Epoch 202/300, trend Loss: 0.0322 | 0.0283
Epoch 203/300, trend Loss: 0.0327 | 0.0283
Epoch 204/300, trend Loss: 0.0309 | 0.0288
Epoch 205/300, trend Loss: 0.0357 | 0.0273
Epoch 206/300, trend Loss: 0.0298 | 0.0270
Epoch 207/300, trend Loss: 0.0302 | 0.0277
Epoch 208/300, trend Loss: 0.0298 | 0.0274
Epoch 209/300, trend Loss: 0.0276 | 0.0283
Epoch 210/300, trend Loss: 0.0278 | 0.0277
Epoch 211/300, trend Loss: 0.0296 | 0.0282
Epoch 212/300, trend Loss: 0.0313 | 0.0300
Epoch 213/300, trend Loss: 0.0320 | 0.0283
Epoch 214/300, trend Loss: 0.0311 | 0.0286
Epoch 215/300, trend Loss: 0.0292 | 0.0286
Epoch 216/300, trend Loss: 0.0271 | 0.0289
Epoch 217/300, trend Loss: 0.0291 | 0.0289
Epoch 218/300, trend Loss: 0.0271 | 0.0281
Epoch 219/300, trend Loss: 0.0264 | 0.0306
Epoch 220/300, trend Loss: 0.0266 | 0.0280
Epoch 221/300, trend Loss: 0.0261 | 0.0295
Epoch 222/300, trend Loss: 0.0279 | 0.0299
Epoch 223/300, trend Loss: 0.0380 | 0.0311
Epoch 224/300, trend Loss: 0.0473 | 0.0273
Epoch 225/300, trend Loss: 0.0433 | 0.0273
Epoch 226/300, trend Loss: 0.0391 | 0.0259
Epoch 227/300, trend Loss: 0.0421 | 0.0274
Epoch 228/300, trend Loss: 0.0402 | 0.0284
Epoch 229/300, trend Loss: 0.0347 | 0.0266
Epoch 230/300, trend Loss: 0.0395 | 0.0282
Epoch 231/300, trend Loss: 0.0374 | 0.0278
Epoch 232/300, trend Loss: 0.0439 | 0.0274
Epoch 233/300, trend Loss: 0.0366 | 0.0296
Epoch 234/300, trend Loss: 0.0343 | 0.0278
Epoch 235/300, trend Loss: 0.0331 | 0.0273
Epoch 236/300, trend Loss: 0.0418 | 0.0285
Epoch 237/300, trend Loss: 0.0409 | 0.0292
Epoch 238/300, trend Loss: 0.0403 | 0.0301
Epoch 239/300, trend Loss: 0.0390 | 0.0311
Epoch 240/300, trend Loss: 0.0350 | 0.0300
Epoch 241/300, trend Loss: 0.0400 | 0.0306
Epoch 242/300, trend Loss: 0.0392 | 0.0304
Epoch 243/300, trend Loss: 0.0404 | 0.0318
Epoch 244/300, trend Loss: 0.0336 | 0.0298
Epoch 245/300, trend Loss: 0.0310 | 0.0305
Epoch 246/300, trend Loss: 0.0335 | 0.0296
Epoch 247/300, trend Loss: 0.0417 | 0.0315
Epoch 248/300, trend Loss: 0.0353 | 0.0437
Epoch 249/300, trend Loss: 0.0365 | 0.0481
Epoch 250/300, trend Loss: 0.0401 | 0.0291
Epoch 251/300, trend Loss: 0.0452 | 0.0318
Epoch 252/300, trend Loss: 0.0409 | 0.0311
Epoch 253/300, trend Loss: 0.0363 | 0.0305
Epoch 254/300, trend Loss: 0.0422 | 0.0293
Epoch 255/300, trend Loss: 0.0414 | 0.0294
Epoch 256/300, trend Loss: 0.0418 | 0.0298
Epoch 257/300, trend Loss: 0.0403 | 0.0310
Epoch 258/300, trend Loss: 0.0363 | 0.0303
Epoch 259/300, trend Loss: 0.0322 | 0.0295
Epoch 260/300, trend Loss: 0.0323 | 0.0339
Epoch 261/300, trend Loss: 0.0317 | 0.0301
Epoch 262/300, trend Loss: 0.0395 | 0.0299
Epoch 263/300, trend Loss: 0.0387 | 0.0317
Epoch 264/300, trend Loss: 0.0316 | 0.0303
Epoch 265/300, trend Loss: 0.0333 | 0.0291
Epoch 266/300, trend Loss: 0.0351 | 0.0320
Epoch 267/300, trend Loss: 0.0435 | 0.0317
Epoch 268/300, trend Loss: 0.0415 | 0.0318
Epoch 269/300, trend Loss: 0.0338 | 0.0307
Epoch 270/300, trend Loss: 0.0328 | 0.0311
Epoch 271/300, trend Loss: 0.0372 | 0.0288
Epoch 272/300, trend Loss: 0.0298 | 0.0315
Epoch 273/300, trend Loss: 0.0262 | 0.0310
Epoch 274/300, trend Loss: 0.0256 | 0.0319
Epoch 275/300, trend Loss: 0.0253 | 0.0310
Epoch 276/300, trend Loss: 0.0252 | 0.0313
Epoch 277/300, trend Loss: 0.0241 | 0.0306
Epoch 278/300, trend Loss: 0.0236 | 0.0306
Epoch 279/300, trend Loss: 0.0235 | 0.0309
Epoch 280/300, trend Loss: 0.0246 | 0.0303
Epoch 281/300, trend Loss: 0.0231 | 0.0308
Epoch 282/300, trend Loss: 0.0227 | 0.0306
Epoch 283/300, trend Loss: 0.0222 | 0.0310
Epoch 284/300, trend Loss: 0.0223 | 0.0305
Epoch 285/300, trend Loss: 0.0219 | 0.0311
Epoch 286/300, trend Loss: 0.0219 | 0.0306
Epoch 287/300, trend Loss: 0.0249 | 0.0304
Epoch 288/300, trend Loss: 0.0249 | 0.0326
Epoch 289/300, trend Loss: 0.0273 | 0.0312
Epoch 290/300, trend Loss: 0.0250 | 0.0299
Epoch 291/300, trend Loss: 0.0242 | 0.0285
Epoch 292/300, trend Loss: 0.0235 | 0.0295
Epoch 293/300, trend Loss: 0.0216 | 0.0295
Epoch 294/300, trend Loss: 0.0211 | 0.0300
Epoch 295/300, trend Loss: 0.0209 | 0.0297
Epoch 296/300, trend Loss: 0.0210 | 0.0310
Epoch 297/300, trend Loss: 0.0211 | 0.0306
Epoch 298/300, trend Loss: 0.0205 | 0.0307
Epoch 299/300, trend Loss: 0.0207 | 0.0297
Epoch 300/300, trend Loss: 0.0216 | 0.0305
Training seasonal_0 component with params: {'observation_period_num': 12, 'train_rates': 0.9885682316212955, 'learning_rate': 0.0007812705820300369, 'batch_size': 91, 'step_size': 14, 'gamma': 0.7853421739183611}
Epoch 1/300, seasonal_0 Loss: 0.3263 | 0.1617
Epoch 2/300, seasonal_0 Loss: 0.1484 | 0.1187
Epoch 3/300, seasonal_0 Loss: 0.1302 | 0.1158
Epoch 4/300, seasonal_0 Loss: 0.1228 | 0.1051
Epoch 5/300, seasonal_0 Loss: 0.1244 | 0.0910
Epoch 6/300, seasonal_0 Loss: 0.1115 | 0.0789
Epoch 7/300, seasonal_0 Loss: 0.1013 | 0.0725
Epoch 8/300, seasonal_0 Loss: 0.0955 | 0.0687
Epoch 9/300, seasonal_0 Loss: 0.0926 | 0.0676
Epoch 10/300, seasonal_0 Loss: 0.0898 | 0.0644
Epoch 11/300, seasonal_0 Loss: 0.0875 | 0.0619
Epoch 12/300, seasonal_0 Loss: 0.0854 | 0.0613
Epoch 13/300, seasonal_0 Loss: 0.0833 | 0.0613
Epoch 14/300, seasonal_0 Loss: 0.0810 | 0.0611
Epoch 15/300, seasonal_0 Loss: 0.0790 | 0.0595
Epoch 16/300, seasonal_0 Loss: 0.0784 | 0.0606
Epoch 17/300, seasonal_0 Loss: 0.0770 | 0.0588
Epoch 18/300, seasonal_0 Loss: 0.0827 | 0.0691
Epoch 19/300, seasonal_0 Loss: 0.0809 | 0.0645
Epoch 20/300, seasonal_0 Loss: 0.0757 | 0.0508
Epoch 21/300, seasonal_0 Loss: 0.0786 | 0.0562
Epoch 22/300, seasonal_0 Loss: 0.0734 | 0.0502
Epoch 23/300, seasonal_0 Loss: 0.0732 | 0.0488
Epoch 24/300, seasonal_0 Loss: 0.0719 | 0.0489
Epoch 25/300, seasonal_0 Loss: 0.0715 | 0.0489
Epoch 26/300, seasonal_0 Loss: 0.0707 | 0.0487
Epoch 27/300, seasonal_0 Loss: 0.0705 | 0.0519
Epoch 28/300, seasonal_0 Loss: 0.0694 | 0.0509
Epoch 29/300, seasonal_0 Loss: 0.0689 | 0.0477
Epoch 30/300, seasonal_0 Loss: 0.0681 | 0.0476
Epoch 31/300, seasonal_0 Loss: 0.0676 | 0.0515
Epoch 32/300, seasonal_0 Loss: 0.0668 | 0.0492
Epoch 33/300, seasonal_0 Loss: 0.0670 | 0.0491
Epoch 34/300, seasonal_0 Loss: 0.0677 | 0.0497
Epoch 35/300, seasonal_0 Loss: 0.0700 | 0.0535
Epoch 36/300, seasonal_0 Loss: 0.0766 | 0.0541
Epoch 37/300, seasonal_0 Loss: 0.0792 | 0.0523
Epoch 38/300, seasonal_0 Loss: 0.0820 | 0.0526
Epoch 39/300, seasonal_0 Loss: 0.0720 | 0.0426
Epoch 40/300, seasonal_0 Loss: 0.0720 | 0.0468
Epoch 41/300, seasonal_0 Loss: 0.0708 | 0.0418
Epoch 42/300, seasonal_0 Loss: 0.0685 | 0.0391
Epoch 43/300, seasonal_0 Loss: 0.0690 | 0.0406
Epoch 44/300, seasonal_0 Loss: 0.0685 | 0.0407
Epoch 45/300, seasonal_0 Loss: 0.0694 | 0.0418
Epoch 46/300, seasonal_0 Loss: 0.0713 | 0.0463
Epoch 47/300, seasonal_0 Loss: 0.0710 | 0.0476
Epoch 48/300, seasonal_0 Loss: 0.0741 | 0.0521
Epoch 49/300, seasonal_0 Loss: 0.0746 | 0.0469
Epoch 50/300, seasonal_0 Loss: 0.0842 | 0.0413
Epoch 51/300, seasonal_0 Loss: 0.0797 | 0.0456
Epoch 52/300, seasonal_0 Loss: 0.0943 | 0.0667
Epoch 53/300, seasonal_0 Loss: 0.0987 | 0.1008
Epoch 54/300, seasonal_0 Loss: 0.1066 | 0.2162
Epoch 55/300, seasonal_0 Loss: 0.0941 | 0.0948
Epoch 56/300, seasonal_0 Loss: 0.0762 | 0.0394
Epoch 57/300, seasonal_0 Loss: 0.0759 | 0.0498
Epoch 58/300, seasonal_0 Loss: 0.0744 | 0.0353
Epoch 59/300, seasonal_0 Loss: 0.0636 | 0.0370
Epoch 60/300, seasonal_0 Loss: 0.0613 | 0.0374
Epoch 61/300, seasonal_0 Loss: 0.0599 | 0.0341
Epoch 62/300, seasonal_0 Loss: 0.0595 | 0.0324
Epoch 63/300, seasonal_0 Loss: 0.0590 | 0.0311
Epoch 64/300, seasonal_0 Loss: 0.0586 | 0.0301
Epoch 65/300, seasonal_0 Loss: 0.0580 | 0.0301
Epoch 66/300, seasonal_0 Loss: 0.0572 | 0.0298
Epoch 67/300, seasonal_0 Loss: 0.0567 | 0.0298
Epoch 68/300, seasonal_0 Loss: 0.0564 | 0.0295
Epoch 69/300, seasonal_0 Loss: 0.0562 | 0.0292
Epoch 70/300, seasonal_0 Loss: 0.0560 | 0.0288
Epoch 71/300, seasonal_0 Loss: 0.0559 | 0.0281
Epoch 72/300, seasonal_0 Loss: 0.0558 | 0.0278
Epoch 73/300, seasonal_0 Loss: 0.0555 | 0.0276
Epoch 74/300, seasonal_0 Loss: 0.0552 | 0.0273
Epoch 75/300, seasonal_0 Loss: 0.0549 | 0.0271
Epoch 76/300, seasonal_0 Loss: 0.0547 | 0.0269
Epoch 77/300, seasonal_0 Loss: 0.0545 | 0.0266
Epoch 78/300, seasonal_0 Loss: 0.0543 | 0.0257
Epoch 79/300, seasonal_0 Loss: 0.0541 | 0.0255
Epoch 80/300, seasonal_0 Loss: 0.0539 | 0.0255
Epoch 81/300, seasonal_0 Loss: 0.0537 | 0.0253
Epoch 82/300, seasonal_0 Loss: 0.0536 | 0.0252
Epoch 83/300, seasonal_0 Loss: 0.0535 | 0.0251
Epoch 84/300, seasonal_0 Loss: 0.0533 | 0.0251
Epoch 85/300, seasonal_0 Loss: 0.0532 | 0.0246
Epoch 86/300, seasonal_0 Loss: 0.0530 | 0.0246
Epoch 87/300, seasonal_0 Loss: 0.0529 | 0.0246
Epoch 88/300, seasonal_0 Loss: 0.0528 | 0.0246
Epoch 89/300, seasonal_0 Loss: 0.0527 | 0.0246
Epoch 90/300, seasonal_0 Loss: 0.0526 | 0.0246
Epoch 91/300, seasonal_0 Loss: 0.0525 | 0.0246
Epoch 92/300, seasonal_0 Loss: 0.0524 | 0.0243
Epoch 93/300, seasonal_0 Loss: 0.0523 | 0.0244
Epoch 94/300, seasonal_0 Loss: 0.0522 | 0.0244
Epoch 95/300, seasonal_0 Loss: 0.0521 | 0.0243
Epoch 96/300, seasonal_0 Loss: 0.0520 | 0.0243
Epoch 97/300, seasonal_0 Loss: 0.0519 | 0.0243
Epoch 98/300, seasonal_0 Loss: 0.0519 | 0.0242
Epoch 99/300, seasonal_0 Loss: 0.0518 | 0.0240
Epoch 100/300, seasonal_0 Loss: 0.0517 | 0.0240
Epoch 101/300, seasonal_0 Loss: 0.0517 | 0.0239
Epoch 102/300, seasonal_0 Loss: 0.0516 | 0.0238
Epoch 103/300, seasonal_0 Loss: 0.0516 | 0.0237
Epoch 104/300, seasonal_0 Loss: 0.0515 | 0.0236
Epoch 105/300, seasonal_0 Loss: 0.0515 | 0.0235
Epoch 106/300, seasonal_0 Loss: 0.0515 | 0.0231
Epoch 107/300, seasonal_0 Loss: 0.0514 | 0.0230
Epoch 108/300, seasonal_0 Loss: 0.0514 | 0.0229
Epoch 109/300, seasonal_0 Loss: 0.0514 | 0.0228
Epoch 110/300, seasonal_0 Loss: 0.0515 | 0.0226
Epoch 111/300, seasonal_0 Loss: 0.0515 | 0.0225
Epoch 112/300, seasonal_0 Loss: 0.0515 | 0.0223
Epoch 113/300, seasonal_0 Loss: 0.0516 | 0.0221
Epoch 114/300, seasonal_0 Loss: 0.0518 | 0.0221
Epoch 115/300, seasonal_0 Loss: 0.0516 | 0.0221
Epoch 116/300, seasonal_0 Loss: 0.0514 | 0.0222
Epoch 117/300, seasonal_0 Loss: 0.0512 | 0.0223
Epoch 118/300, seasonal_0 Loss: 0.0511 | 0.0225
Epoch 119/300, seasonal_0 Loss: 0.0510 | 0.0226
Epoch 120/300, seasonal_0 Loss: 0.0509 | 0.0230
Epoch 121/300, seasonal_0 Loss: 0.0509 | 0.0230
Epoch 122/300, seasonal_0 Loss: 0.0508 | 0.0230
Epoch 123/300, seasonal_0 Loss: 0.0508 | 0.0229
Epoch 124/300, seasonal_0 Loss: 0.0507 | 0.0229
Epoch 125/300, seasonal_0 Loss: 0.0507 | 0.0229
Epoch 126/300, seasonal_0 Loss: 0.0506 | 0.0228
Epoch 127/300, seasonal_0 Loss: 0.0506 | 0.0228
Epoch 128/300, seasonal_0 Loss: 0.0506 | 0.0227
Epoch 129/300, seasonal_0 Loss: 0.0505 | 0.0227
Epoch 130/300, seasonal_0 Loss: 0.0505 | 0.0227
Epoch 131/300, seasonal_0 Loss: 0.0505 | 0.0227
Epoch 132/300, seasonal_0 Loss: 0.0504 | 0.0227
Epoch 133/300, seasonal_0 Loss: 0.0504 | 0.0226
Epoch 134/300, seasonal_0 Loss: 0.0504 | 0.0227
Epoch 135/300, seasonal_0 Loss: 0.0503 | 0.0226
Epoch 136/300, seasonal_0 Loss: 0.0503 | 0.0226
Epoch 137/300, seasonal_0 Loss: 0.0503 | 0.0226
Epoch 138/300, seasonal_0 Loss: 0.0503 | 0.0226
Epoch 139/300, seasonal_0 Loss: 0.0503 | 0.0226
Epoch 140/300, seasonal_0 Loss: 0.0502 | 0.0226
Epoch 141/300, seasonal_0 Loss: 0.0502 | 0.0226
Epoch 142/300, seasonal_0 Loss: 0.0502 | 0.0226
Epoch 143/300, seasonal_0 Loss: 0.0502 | 0.0226
Epoch 144/300, seasonal_0 Loss: 0.0502 | 0.0226
Epoch 145/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 146/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 147/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 148/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 149/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 150/300, seasonal_0 Loss: 0.0501 | 0.0225
Epoch 151/300, seasonal_0 Loss: 0.0500 | 0.0225
Epoch 152/300, seasonal_0 Loss: 0.0500 | 0.0225
Epoch 153/300, seasonal_0 Loss: 0.0500 | 0.0225
Epoch 154/300, seasonal_0 Loss: 0.0500 | 0.0225
Epoch 155/300, seasonal_0 Loss: 0.0500 | 0.0225
Epoch 156/300, seasonal_0 Loss: 0.0500 | 0.0224
Epoch 157/300, seasonal_0 Loss: 0.0500 | 0.0224
Epoch 158/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 159/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 160/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 161/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 162/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 163/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 164/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 165/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 166/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 167/300, seasonal_0 Loss: 0.0499 | 0.0224
Epoch 168/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 169/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 170/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 171/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 172/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 173/300, seasonal_0 Loss: 0.0498 | 0.0224
Epoch 174/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 175/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 176/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 177/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 178/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 179/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 180/300, seasonal_0 Loss: 0.0498 | 0.0223
Epoch 181/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 182/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 183/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 184/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 185/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 186/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 187/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 188/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 189/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 190/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 191/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 192/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 193/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 194/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 195/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 196/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 197/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 198/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 199/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 200/300, seasonal_0 Loss: 0.0497 | 0.0223
Epoch 201/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 202/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 203/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 204/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 205/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 206/300, seasonal_0 Loss: 0.0496 | 0.0223
Epoch 207/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 208/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 209/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 210/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 211/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 212/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 213/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 214/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 215/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 216/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 217/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 218/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 219/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 220/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 221/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 222/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 223/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 224/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 225/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 226/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 227/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 228/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 229/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 230/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 231/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 232/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 233/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 234/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 235/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 236/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 237/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 238/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 239/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 240/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 241/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 242/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 243/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 244/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 245/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 246/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 247/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 248/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 249/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 250/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 251/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 252/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 253/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 254/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 255/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 256/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 257/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 258/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 259/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 260/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 261/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 262/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 263/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 264/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 265/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 266/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 267/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 268/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 269/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 270/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 271/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 272/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 273/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 274/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 275/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 276/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 277/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 278/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 279/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 280/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 281/300, seasonal_0 Loss: 0.0496 | 0.0222
Epoch 282/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 283/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 284/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 285/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 286/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 287/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 288/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 289/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 290/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 291/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 292/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 293/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 294/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 295/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 296/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 297/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 298/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 299/300, seasonal_0 Loss: 0.0495 | 0.0222
Epoch 300/300, seasonal_0 Loss: 0.0495 | 0.0222
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.9734097799354189, 'learning_rate': 0.0009114576043213219, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9854728274828737}
Epoch 1/300, seasonal_1 Loss: 0.2993 | 0.1176
Epoch 2/300, seasonal_1 Loss: 0.1257 | 0.0852
Epoch 3/300, seasonal_1 Loss: 0.1046 | 0.0726
Epoch 4/300, seasonal_1 Loss: 0.1007 | 0.0660
Epoch 5/300, seasonal_1 Loss: 0.0906 | 0.0602
Epoch 6/300, seasonal_1 Loss: 0.0842 | 0.0519
Epoch 7/300, seasonal_1 Loss: 0.0796 | 0.0476
Epoch 8/300, seasonal_1 Loss: 0.0756 | 0.0455
Epoch 9/300, seasonal_1 Loss: 0.0734 | 0.0444
Epoch 10/300, seasonal_1 Loss: 0.0756 | 0.0462
Epoch 11/300, seasonal_1 Loss: 0.0833 | 0.1492
Epoch 12/300, seasonal_1 Loss: 0.0938 | 0.0535
Epoch 13/300, seasonal_1 Loss: 0.0749 | 0.0408
Epoch 14/300, seasonal_1 Loss: 0.0736 | 0.0440
Epoch 15/300, seasonal_1 Loss: 0.0725 | 0.0431
Epoch 16/300, seasonal_1 Loss: 0.0711 | 0.0416
Epoch 17/300, seasonal_1 Loss: 0.0703 | 0.0408
Epoch 18/300, seasonal_1 Loss: 0.0688 | 0.0394
Epoch 19/300, seasonal_1 Loss: 0.0673 | 0.0386
Epoch 20/300, seasonal_1 Loss: 0.0655 | 0.0377
Epoch 21/300, seasonal_1 Loss: 0.0648 | 0.0373
Epoch 22/300, seasonal_1 Loss: 0.0647 | 0.0374
Epoch 23/300, seasonal_1 Loss: 0.0638 | 0.0345
Epoch 24/300, seasonal_1 Loss: 0.0643 | 0.0348
Epoch 25/300, seasonal_1 Loss: 0.0652 | 0.0348
Epoch 26/300, seasonal_1 Loss: 0.0643 | 0.0339
Epoch 27/300, seasonal_1 Loss: 0.0627 | 0.0326
Epoch 28/300, seasonal_1 Loss: 0.0620 | 0.0322
Epoch 29/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 30/300, seasonal_1 Loss: 0.0612 | 0.0314
Epoch 31/300, seasonal_1 Loss: 0.0630 | 0.0356
Epoch 32/300, seasonal_1 Loss: 0.0649 | 0.0365
Epoch 33/300, seasonal_1 Loss: 0.0622 | 0.0299
Epoch 34/300, seasonal_1 Loss: 0.0632 | 0.0313
Epoch 35/300, seasonal_1 Loss: 0.0621 | 0.0330
Epoch 36/300, seasonal_1 Loss: 0.0609 | 0.0388
Epoch 37/300, seasonal_1 Loss: 0.0651 | 0.0442
Epoch 38/300, seasonal_1 Loss: 0.0651 | 0.0602
Epoch 39/300, seasonal_1 Loss: 0.0680 | 0.0337
Epoch 40/300, seasonal_1 Loss: 0.0625 | 0.0321
Epoch 41/300, seasonal_1 Loss: 0.0618 | 0.0330
Epoch 42/300, seasonal_1 Loss: 0.0621 | 0.0306
Epoch 43/300, seasonal_1 Loss: 0.0601 | 0.0288
Epoch 44/300, seasonal_1 Loss: 0.0581 | 0.0285
Epoch 45/300, seasonal_1 Loss: 0.0585 | 0.0282
Epoch 46/300, seasonal_1 Loss: 0.0588 | 0.0305
Epoch 47/300, seasonal_1 Loss: 0.0627 | 0.0413
Epoch 48/300, seasonal_1 Loss: 0.0671 | 0.0311
Epoch 49/300, seasonal_1 Loss: 0.0597 | 0.0267
Epoch 50/300, seasonal_1 Loss: 0.0575 | 0.0261
Epoch 51/300, seasonal_1 Loss: 0.0585 | 0.0288
Epoch 52/300, seasonal_1 Loss: 0.0617 | 0.0301
Epoch 53/300, seasonal_1 Loss: 0.0600 | 0.0296
Epoch 54/300, seasonal_1 Loss: 0.0605 | 0.0314
Epoch 55/300, seasonal_1 Loss: 0.0600 | 0.0322
Epoch 56/300, seasonal_1 Loss: 0.0595 | 0.0329
Epoch 57/300, seasonal_1 Loss: 0.0618 | 0.0312
Epoch 58/300, seasonal_1 Loss: 0.0659 | 0.0355
Epoch 59/300, seasonal_1 Loss: 0.0653 | 0.0310
Epoch 60/300, seasonal_1 Loss: 0.0617 | 0.0300
Epoch 61/300, seasonal_1 Loss: 0.0608 | 0.0315
Epoch 62/300, seasonal_1 Loss: 0.0598 | 0.0315
Epoch 63/300, seasonal_1 Loss: 0.0575 | 0.0301
Epoch 64/300, seasonal_1 Loss: 0.0556 | 0.0289
Epoch 65/300, seasonal_1 Loss: 0.0547 | 0.0281
Epoch 66/300, seasonal_1 Loss: 0.0549 | 0.0274
Epoch 67/300, seasonal_1 Loss: 0.0554 | 0.0272
Epoch 68/300, seasonal_1 Loss: 0.0561 | 0.0271
Epoch 69/300, seasonal_1 Loss: 0.0570 | 0.0270
Epoch 70/300, seasonal_1 Loss: 0.0582 | 0.0268
Epoch 71/300, seasonal_1 Loss: 0.0602 | 0.0268
Epoch 72/300, seasonal_1 Loss: 0.0639 | 0.0290
Epoch 73/300, seasonal_1 Loss: 0.0702 | 0.0422
Epoch 74/300, seasonal_1 Loss: 0.0778 | 0.0851
Epoch 75/300, seasonal_1 Loss: 0.0788 | 0.1021
Epoch 76/300, seasonal_1 Loss: 0.0699 | 0.0481
Epoch 77/300, seasonal_1 Loss: 0.0634 | 0.0283
Epoch 78/300, seasonal_1 Loss: 0.0587 | 0.0302
Epoch 79/300, seasonal_1 Loss: 0.0557 | 0.0281
Epoch 80/300, seasonal_1 Loss: 0.0540 | 0.0271
Epoch 81/300, seasonal_1 Loss: 0.0529 | 0.0267
Epoch 82/300, seasonal_1 Loss: 0.0528 | 0.0266
Epoch 83/300, seasonal_1 Loss: 0.0533 | 0.0270
Epoch 84/300, seasonal_1 Loss: 0.0564 | 0.0307
Epoch 85/300, seasonal_1 Loss: 0.0536 | 0.0261
Epoch 86/300, seasonal_1 Loss: 0.0561 | 0.0304
Epoch 87/300, seasonal_1 Loss: 0.0526 | 0.0259
Epoch 88/300, seasonal_1 Loss: 0.0523 | 0.0265
Epoch 89/300, seasonal_1 Loss: 0.0520 | 0.0262
Epoch 90/300, seasonal_1 Loss: 0.0517 | 0.0258
Epoch 91/300, seasonal_1 Loss: 0.0515 | 0.0255
Epoch 92/300, seasonal_1 Loss: 0.0513 | 0.0254
Epoch 93/300, seasonal_1 Loss: 0.0511 | 0.0253
Epoch 94/300, seasonal_1 Loss: 0.0510 | 0.0252
Epoch 95/300, seasonal_1 Loss: 0.0509 | 0.0253
Epoch 96/300, seasonal_1 Loss: 0.0509 | 0.0254
Epoch 97/300, seasonal_1 Loss: 0.0509 | 0.0255
Epoch 98/300, seasonal_1 Loss: 0.0509 | 0.0257
Epoch 99/300, seasonal_1 Loss: 0.0511 | 0.0260
Epoch 100/300, seasonal_1 Loss: 0.0515 | 0.0265
Epoch 101/300, seasonal_1 Loss: 0.0525 | 0.0275
Epoch 102/300, seasonal_1 Loss: 0.0543 | 0.0288
Epoch 103/300, seasonal_1 Loss: 0.0524 | 0.0260
Epoch 104/300, seasonal_1 Loss: 0.0512 | 0.0264
Epoch 105/300, seasonal_1 Loss: 0.0506 | 0.0259
Epoch 106/300, seasonal_1 Loss: 0.0503 | 0.0257
Epoch 107/300, seasonal_1 Loss: 0.0501 | 0.0255
Epoch 108/300, seasonal_1 Loss: 0.0500 | 0.0252
Epoch 109/300, seasonal_1 Loss: 0.0500 | 0.0252
Epoch 110/300, seasonal_1 Loss: 0.0501 | 0.0253
Epoch 111/300, seasonal_1 Loss: 0.0502 | 0.0252
Epoch 112/300, seasonal_1 Loss: 0.0504 | 0.0251
Epoch 113/300, seasonal_1 Loss: 0.0504 | 0.0250
Epoch 114/300, seasonal_1 Loss: 0.0504 | 0.0249
Epoch 115/300, seasonal_1 Loss: 0.0502 | 0.0248
Epoch 116/300, seasonal_1 Loss: 0.0500 | 0.0248
Epoch 117/300, seasonal_1 Loss: 0.0497 | 0.0246
Epoch 118/300, seasonal_1 Loss: 0.0495 | 0.0245
Epoch 119/300, seasonal_1 Loss: 0.0493 | 0.0243
Epoch 120/300, seasonal_1 Loss: 0.0491 | 0.0242
Epoch 121/300, seasonal_1 Loss: 0.0490 | 0.0242
Epoch 122/300, seasonal_1 Loss: 0.0489 | 0.0241
Epoch 123/300, seasonal_1 Loss: 0.0488 | 0.0241
Epoch 124/300, seasonal_1 Loss: 0.0488 | 0.0240
Epoch 125/300, seasonal_1 Loss: 0.0487 | 0.0240
Epoch 126/300, seasonal_1 Loss: 0.0487 | 0.0240
Epoch 127/300, seasonal_1 Loss: 0.0486 | 0.0239
Epoch 128/300, seasonal_1 Loss: 0.0485 | 0.0239
Epoch 129/300, seasonal_1 Loss: 0.0485 | 0.0238
Epoch 130/300, seasonal_1 Loss: 0.0484 | 0.0238
Epoch 131/300, seasonal_1 Loss: 0.0484 | 0.0238
Epoch 132/300, seasonal_1 Loss: 0.0484 | 0.0237
Epoch 133/300, seasonal_1 Loss: 0.0483 | 0.0237
Epoch 134/300, seasonal_1 Loss: 0.0483 | 0.0237
Epoch 135/300, seasonal_1 Loss: 0.0482 | 0.0237
Epoch 136/300, seasonal_1 Loss: 0.0482 | 0.0236
Epoch 137/300, seasonal_1 Loss: 0.0482 | 0.0236
Epoch 138/300, seasonal_1 Loss: 0.0482 | 0.0236
Epoch 139/300, seasonal_1 Loss: 0.0481 | 0.0236
Epoch 140/300, seasonal_1 Loss: 0.0481 | 0.0236
Epoch 141/300, seasonal_1 Loss: 0.0481 | 0.0236
Epoch 142/300, seasonal_1 Loss: 0.0481 | 0.0236
Epoch 143/300, seasonal_1 Loss: 0.0480 | 0.0236
Epoch 144/300, seasonal_1 Loss: 0.0480 | 0.0235
Epoch 145/300, seasonal_1 Loss: 0.0480 | 0.0235
Epoch 146/300, seasonal_1 Loss: 0.0480 | 0.0235
Epoch 147/300, seasonal_1 Loss: 0.0479 | 0.0235
Epoch 148/300, seasonal_1 Loss: 0.0479 | 0.0235
Epoch 149/300, seasonal_1 Loss: 0.0479 | 0.0235
Epoch 150/300, seasonal_1 Loss: 0.0479 | 0.0235
Epoch 151/300, seasonal_1 Loss: 0.0478 | 0.0235
Epoch 152/300, seasonal_1 Loss: 0.0478 | 0.0235
Epoch 153/300, seasonal_1 Loss: 0.0478 | 0.0235
Epoch 154/300, seasonal_1 Loss: 0.0478 | 0.0235
Epoch 155/300, seasonal_1 Loss: 0.0478 | 0.0235
Epoch 156/300, seasonal_1 Loss: 0.0477 | 0.0235
Epoch 157/300, seasonal_1 Loss: 0.0477 | 0.0235
Epoch 158/300, seasonal_1 Loss: 0.0477 | 0.0235
Epoch 159/300, seasonal_1 Loss: 0.0477 | 0.0235
Epoch 160/300, seasonal_1 Loss: 0.0477 | 0.0235
Epoch 161/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 162/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 163/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 164/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 165/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 166/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 167/300, seasonal_1 Loss: 0.0476 | 0.0235
Epoch 168/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 169/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 170/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 171/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 172/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 173/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 174/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 175/300, seasonal_1 Loss: 0.0475 | 0.0235
Epoch 176/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 177/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 178/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 179/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 180/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 181/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 182/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 183/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 184/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 185/300, seasonal_1 Loss: 0.0474 | 0.0235
Epoch 186/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 187/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 188/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 189/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 190/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 191/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 192/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 193/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 194/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 195/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 196/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 197/300, seasonal_1 Loss: 0.0473 | 0.0235
Epoch 198/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 199/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 200/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 201/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 202/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 203/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 204/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 205/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 206/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 207/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 208/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 209/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 210/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 211/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 212/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 213/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 214/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 215/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 216/300, seasonal_1 Loss: 0.0472 | 0.0235
Epoch 217/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 218/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 219/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 220/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 221/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 222/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 223/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 224/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 225/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 226/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 227/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 228/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 229/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 230/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 231/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 232/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 233/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 234/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 235/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 236/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 237/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 238/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 239/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 240/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 241/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 242/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 243/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 244/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 245/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 246/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 247/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 248/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 249/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 250/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 251/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 252/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 253/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 254/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 255/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 256/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 257/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 258/300, seasonal_1 Loss: 0.0471 | 0.0235
Epoch 259/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 260/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 261/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 262/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 263/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 264/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 265/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 266/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 267/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 268/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 269/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 270/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 271/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 272/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 273/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 274/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 275/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 276/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 277/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 278/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 279/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 280/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 281/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 282/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 283/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 284/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 285/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 286/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 287/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 288/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 289/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 290/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 291/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 292/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 293/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 294/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 295/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 296/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 297/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 298/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 299/300, seasonal_1 Loss: 0.0470 | 0.0235
Epoch 300/300, seasonal_1 Loss: 0.0470 | 0.0235
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8681814748711696, 'learning_rate': 0.0008579355185219953, 'batch_size': 83, 'step_size': 11, 'gamma': 0.8890929133962644}
Epoch 1/300, seasonal_2 Loss: 0.3341 | 0.1192
Epoch 2/300, seasonal_2 Loss: 0.1324 | 0.0821
Epoch 3/300, seasonal_2 Loss: 0.1133 | 0.0670
Epoch 4/300, seasonal_2 Loss: 0.1078 | 0.0944
Epoch 5/300, seasonal_2 Loss: 0.1140 | 0.0708
Epoch 6/300, seasonal_2 Loss: 0.1037 | 0.0736
Epoch 7/300, seasonal_2 Loss: 0.1009 | 0.0824
Epoch 8/300, seasonal_2 Loss: 0.1005 | 0.0675
Epoch 9/300, seasonal_2 Loss: 0.0954 | 0.0593
Epoch 10/300, seasonal_2 Loss: 0.0910 | 0.0567
Epoch 11/300, seasonal_2 Loss: 0.0915 | 0.0617
Epoch 12/300, seasonal_2 Loss: 0.0886 | 0.0490
Epoch 13/300, seasonal_2 Loss: 0.0883 | 0.0448
Epoch 14/300, seasonal_2 Loss: 0.0847 | 0.0423
Epoch 15/300, seasonal_2 Loss: 0.0848 | 0.0426
Epoch 16/300, seasonal_2 Loss: 0.0809 | 0.0392
Epoch 17/300, seasonal_2 Loss: 0.0817 | 0.0398
Epoch 18/300, seasonal_2 Loss: 0.0821 | 0.0432
Epoch 19/300, seasonal_2 Loss: 0.0801 | 0.0431
Epoch 20/300, seasonal_2 Loss: 0.0783 | 0.0408
Epoch 21/300, seasonal_2 Loss: 0.0759 | 0.0395
Epoch 22/300, seasonal_2 Loss: 0.0741 | 0.0383
Epoch 23/300, seasonal_2 Loss: 0.0727 | 0.0391
Epoch 24/300, seasonal_2 Loss: 0.0721 | 0.0342
Epoch 25/300, seasonal_2 Loss: 0.0709 | 0.0392
Epoch 26/300, seasonal_2 Loss: 0.0720 | 0.0335
Epoch 27/300, seasonal_2 Loss: 0.0703 | 0.0373
Epoch 28/300, seasonal_2 Loss: 0.0713 | 0.0355
Epoch 29/300, seasonal_2 Loss: 0.0700 | 0.0368
Epoch 30/300, seasonal_2 Loss: 0.0710 | 0.0324
Epoch 31/300, seasonal_2 Loss: 0.0689 | 0.0383
Epoch 32/300, seasonal_2 Loss: 0.0701 | 0.0400
Epoch 33/300, seasonal_2 Loss: 0.0699 | 0.0384
Epoch 34/300, seasonal_2 Loss: 0.0686 | 0.0411
Epoch 35/300, seasonal_2 Loss: 0.0685 | 0.0370
Epoch 36/300, seasonal_2 Loss: 0.0679 | 0.0340
Epoch 37/300, seasonal_2 Loss: 0.0663 | 0.0357
Epoch 38/300, seasonal_2 Loss: 0.0661 | 0.0368
Epoch 39/300, seasonal_2 Loss: 0.0662 | 0.0388
Epoch 40/300, seasonal_2 Loss: 0.0685 | 0.0401
Epoch 41/300, seasonal_2 Loss: 0.0670 | 0.0281
Epoch 42/300, seasonal_2 Loss: 0.0665 | 0.0306
Epoch 43/300, seasonal_2 Loss: 0.0658 | 0.0293
Epoch 44/300, seasonal_2 Loss: 0.0639 | 0.0279
Epoch 45/300, seasonal_2 Loss: 0.0628 | 0.0254
Epoch 46/300, seasonal_2 Loss: 0.0631 | 0.0254
Epoch 47/300, seasonal_2 Loss: 0.0671 | 0.0289
Epoch 48/300, seasonal_2 Loss: 0.0660 | 0.0300
Epoch 49/300, seasonal_2 Loss: 0.0638 | 0.0270
Epoch 50/300, seasonal_2 Loss: 0.0629 | 0.0268
Epoch 51/300, seasonal_2 Loss: 0.0624 | 0.0275
Epoch 52/300, seasonal_2 Loss: 0.0630 | 0.0290
Epoch 53/300, seasonal_2 Loss: 0.0621 | 0.0315
Epoch 54/300, seasonal_2 Loss: 0.0622 | 0.0286
Epoch 55/300, seasonal_2 Loss: 0.0622 | 0.0287
Epoch 56/300, seasonal_2 Loss: 0.0627 | 0.0279
Epoch 57/300, seasonal_2 Loss: 0.0636 | 0.0287
Epoch 58/300, seasonal_2 Loss: 0.0657 | 0.0289
Epoch 59/300, seasonal_2 Loss: 0.0646 | 0.0280
Epoch 60/300, seasonal_2 Loss: 0.0663 | 0.0262
Epoch 61/300, seasonal_2 Loss: 0.0624 | 0.0246
Epoch 62/300, seasonal_2 Loss: 0.0625 | 0.0263
Epoch 63/300, seasonal_2 Loss: 0.0598 | 0.0240
Epoch 64/300, seasonal_2 Loss: 0.0583 | 0.0239
Epoch 65/300, seasonal_2 Loss: 0.0585 | 0.0274
Epoch 66/300, seasonal_2 Loss: 0.0587 | 0.0225
Epoch 67/300, seasonal_2 Loss: 0.0574 | 0.0251
Epoch 68/300, seasonal_2 Loss: 0.0573 | 0.0243
Epoch 69/300, seasonal_2 Loss: 0.0572 | 0.0238
Epoch 70/300, seasonal_2 Loss: 0.0567 | 0.0237
Epoch 71/300, seasonal_2 Loss: 0.0563 | 0.0229
Epoch 72/300, seasonal_2 Loss: 0.0559 | 0.0229
Epoch 73/300, seasonal_2 Loss: 0.0559 | 0.0236
Epoch 74/300, seasonal_2 Loss: 0.0558 | 0.0236
Epoch 75/300, seasonal_2 Loss: 0.0558 | 0.0234
Epoch 76/300, seasonal_2 Loss: 0.0563 | 0.0261
Epoch 77/300, seasonal_2 Loss: 0.0575 | 0.0242
Epoch 78/300, seasonal_2 Loss: 0.0586 | 0.0275
Epoch 79/300, seasonal_2 Loss: 0.0590 | 0.0233
Epoch 80/300, seasonal_2 Loss: 0.0561 | 0.0230
Epoch 81/300, seasonal_2 Loss: 0.0555 | 0.0227
Epoch 82/300, seasonal_2 Loss: 0.0552 | 0.0228
Epoch 83/300, seasonal_2 Loss: 0.0548 | 0.0237
Epoch 84/300, seasonal_2 Loss: 0.0546 | 0.0228
Epoch 85/300, seasonal_2 Loss: 0.0546 | 0.0249
Epoch 86/300, seasonal_2 Loss: 0.0548 | 0.0236
Epoch 87/300, seasonal_2 Loss: 0.0545 | 0.0234
Epoch 88/300, seasonal_2 Loss: 0.0543 | 0.0234
Epoch 89/300, seasonal_2 Loss: 0.0547 | 0.0232
Epoch 90/300, seasonal_2 Loss: 0.0586 | 0.0271
Epoch 91/300, seasonal_2 Loss: 0.0556 | 0.0237
Epoch 92/300, seasonal_2 Loss: 0.0557 | 0.0233
Epoch 93/300, seasonal_2 Loss: 0.0554 | 0.0230
Epoch 94/300, seasonal_2 Loss: 0.0545 | 0.0229
Epoch 95/300, seasonal_2 Loss: 0.0541 | 0.0228
Epoch 96/300, seasonal_2 Loss: 0.0539 | 0.0229
Epoch 97/300, seasonal_2 Loss: 0.0536 | 0.0224
Epoch 98/300, seasonal_2 Loss: 0.0534 | 0.0224
Epoch 99/300, seasonal_2 Loss: 0.0533 | 0.0229
Epoch 100/300, seasonal_2 Loss: 0.0532 | 0.0225
Epoch 101/300, seasonal_2 Loss: 0.0532 | 0.0224
Epoch 102/300, seasonal_2 Loss: 0.0535 | 0.0231
Epoch 103/300, seasonal_2 Loss: 0.0536 | 0.0226
Epoch 104/300, seasonal_2 Loss: 0.0534 | 0.0226
Epoch 105/300, seasonal_2 Loss: 0.0532 | 0.0229
Epoch 106/300, seasonal_2 Loss: 0.0530 | 0.0228
Epoch 107/300, seasonal_2 Loss: 0.0529 | 0.0226
Epoch 108/300, seasonal_2 Loss: 0.0530 | 0.0227
Epoch 109/300, seasonal_2 Loss: 0.0533 | 0.0229
Epoch 110/300, seasonal_2 Loss: 0.0535 | 0.0228
Epoch 111/300, seasonal_2 Loss: 0.0538 | 0.0237
Epoch 112/300, seasonal_2 Loss: 0.0538 | 0.0233
Epoch 113/300, seasonal_2 Loss: 0.0535 | 0.0228
Epoch 114/300, seasonal_2 Loss: 0.0533 | 0.0224
Epoch 115/300, seasonal_2 Loss: 0.0531 | 0.0225
Epoch 116/300, seasonal_2 Loss: 0.0531 | 0.0226
Epoch 117/300, seasonal_2 Loss: 0.0532 | 0.0229
Epoch 118/300, seasonal_2 Loss: 0.0534 | 0.0225
Epoch 119/300, seasonal_2 Loss: 0.0544 | 0.0228
Epoch 120/300, seasonal_2 Loss: 0.0579 | 0.0272
Epoch 121/300, seasonal_2 Loss: 0.0557 | 0.0235
Epoch 122/300, seasonal_2 Loss: 0.0572 | 0.0267
Epoch 123/300, seasonal_2 Loss: 0.0537 | 0.0231
Epoch 124/300, seasonal_2 Loss: 0.0530 | 0.0232
Epoch 125/300, seasonal_2 Loss: 0.0530 | 0.0232
Epoch 126/300, seasonal_2 Loss: 0.0525 | 0.0228
Epoch 127/300, seasonal_2 Loss: 0.0520 | 0.0224
Epoch 128/300, seasonal_2 Loss: 0.0517 | 0.0221
Epoch 129/300, seasonal_2 Loss: 0.0516 | 0.0221
Epoch 130/300, seasonal_2 Loss: 0.0515 | 0.0220
Epoch 131/300, seasonal_2 Loss: 0.0514 | 0.0221
Epoch 132/300, seasonal_2 Loss: 0.0513 | 0.0221
Epoch 133/300, seasonal_2 Loss: 0.0512 | 0.0221
Epoch 134/300, seasonal_2 Loss: 0.0510 | 0.0220
Epoch 135/300, seasonal_2 Loss: 0.0510 | 0.0220
Epoch 136/300, seasonal_2 Loss: 0.0509 | 0.0220
Epoch 137/300, seasonal_2 Loss: 0.0508 | 0.0219
Epoch 138/300, seasonal_2 Loss: 0.0508 | 0.0219
Epoch 139/300, seasonal_2 Loss: 0.0507 | 0.0219
Epoch 140/300, seasonal_2 Loss: 0.0507 | 0.0218
Epoch 141/300, seasonal_2 Loss: 0.0507 | 0.0218
Epoch 142/300, seasonal_2 Loss: 0.0507 | 0.0219
Epoch 143/300, seasonal_2 Loss: 0.0507 | 0.0219
Epoch 144/300, seasonal_2 Loss: 0.0507 | 0.0219
Epoch 145/300, seasonal_2 Loss: 0.0507 | 0.0219
Epoch 146/300, seasonal_2 Loss: 0.0506 | 0.0219
Epoch 147/300, seasonal_2 Loss: 0.0505 | 0.0219
Epoch 148/300, seasonal_2 Loss: 0.0505 | 0.0218
Epoch 149/300, seasonal_2 Loss: 0.0504 | 0.0218
Epoch 150/300, seasonal_2 Loss: 0.0504 | 0.0218
Epoch 151/300, seasonal_2 Loss: 0.0504 | 0.0218
Epoch 152/300, seasonal_2 Loss: 0.0504 | 0.0219
Epoch 153/300, seasonal_2 Loss: 0.0503 | 0.0220
Epoch 154/300, seasonal_2 Loss: 0.0503 | 0.0221
Epoch 155/300, seasonal_2 Loss: 0.0502 | 0.0222
Epoch 156/300, seasonal_2 Loss: 0.0501 | 0.0223
Epoch 157/300, seasonal_2 Loss: 0.0499 | 0.0223
Epoch 158/300, seasonal_2 Loss: 0.0498 | 0.0223
Epoch 159/300, seasonal_2 Loss: 0.0497 | 0.0222
Epoch 160/300, seasonal_2 Loss: 0.0496 | 0.0222
Epoch 161/300, seasonal_2 Loss: 0.0495 | 0.0221
Epoch 162/300, seasonal_2 Loss: 0.0494 | 0.0221
Epoch 163/300, seasonal_2 Loss: 0.0493 | 0.0221
Epoch 164/300, seasonal_2 Loss: 0.0491 | 0.0220
Epoch 165/300, seasonal_2 Loss: 0.0490 | 0.0220
Epoch 166/300, seasonal_2 Loss: 0.0488 | 0.0220
Epoch 167/300, seasonal_2 Loss: 0.0486 | 0.0220
Epoch 168/300, seasonal_2 Loss: 0.0484 | 0.0220
Epoch 169/300, seasonal_2 Loss: 0.0481 | 0.0220
Epoch 170/300, seasonal_2 Loss: 0.0479 | 0.0220
Epoch 171/300, seasonal_2 Loss: 0.0477 | 0.0221
Epoch 172/300, seasonal_2 Loss: 0.0475 | 0.0220
Epoch 173/300, seasonal_2 Loss: 0.0473 | 0.0221
Epoch 174/300, seasonal_2 Loss: 0.0471 | 0.0221
Epoch 175/300, seasonal_2 Loss: 0.0470 | 0.0221
Epoch 176/300, seasonal_2 Loss: 0.0468 | 0.0221
Epoch 177/300, seasonal_2 Loss: 0.0466 | 0.0222
Epoch 178/300, seasonal_2 Loss: 0.0464 | 0.0223
Epoch 179/300, seasonal_2 Loss: 0.0463 | 0.0223
Epoch 180/300, seasonal_2 Loss: 0.0461 | 0.0224
Epoch 181/300, seasonal_2 Loss: 0.0460 | 0.0220
Epoch 182/300, seasonal_2 Loss: 0.0506 | 0.0225
Epoch 183/300, seasonal_2 Loss: 0.0491 | 0.0219
Epoch 184/300, seasonal_2 Loss: 0.0482 | 0.0228
Epoch 185/300, seasonal_2 Loss: 0.0470 | 0.0227
Epoch 186/300, seasonal_2 Loss: 0.0463 | 0.0224
Epoch 187/300, seasonal_2 Loss: 0.0457 | 0.0222
Epoch 188/300, seasonal_2 Loss: 0.0456 | 0.0222
Epoch 189/300, seasonal_2 Loss: 0.0455 | 0.0219
Epoch 190/300, seasonal_2 Loss: 0.0453 | 0.0223
Epoch 191/300, seasonal_2 Loss: 0.0454 | 0.0219
Epoch 192/300, seasonal_2 Loss: 0.0453 | 0.0230
Epoch 193/300, seasonal_2 Loss: 0.0457 | 0.0222
Epoch 194/300, seasonal_2 Loss: 0.0450 | 0.0220
Epoch 195/300, seasonal_2 Loss: 0.0450 | 0.0229
Epoch 196/300, seasonal_2 Loss: 0.0453 | 0.0222
Epoch 197/300, seasonal_2 Loss: 0.0447 | 0.0220
Epoch 198/300, seasonal_2 Loss: 0.0446 | 0.0225
Epoch 199/300, seasonal_2 Loss: 0.0449 | 0.0220
Epoch 200/300, seasonal_2 Loss: 0.0447 | 0.0235
Epoch 201/300, seasonal_2 Loss: 0.0452 | 0.0223
Epoch 202/300, seasonal_2 Loss: 0.0445 | 0.0219
Epoch 203/300, seasonal_2 Loss: 0.0445 | 0.0233
Epoch 204/300, seasonal_2 Loss: 0.0450 | 0.0222
Epoch 205/300, seasonal_2 Loss: 0.0443 | 0.0219
Epoch 206/300, seasonal_2 Loss: 0.0442 | 0.0225
Epoch 207/300, seasonal_2 Loss: 0.0445 | 0.0220
Epoch 208/300, seasonal_2 Loss: 0.0440 | 0.0222
Epoch 209/300, seasonal_2 Loss: 0.0440 | 0.0219
Epoch 210/300, seasonal_2 Loss: 0.0441 | 0.0231
Epoch 211/300, seasonal_2 Loss: 0.0445 | 0.0222
Epoch 212/300, seasonal_2 Loss: 0.0438 | 0.0219
Epoch 213/300, seasonal_2 Loss: 0.0438 | 0.0225
Epoch 214/300, seasonal_2 Loss: 0.0440 | 0.0220
Epoch 215/300, seasonal_2 Loss: 0.0437 | 0.0224
Epoch 216/300, seasonal_2 Loss: 0.0439 | 0.0220
Epoch 217/300, seasonal_2 Loss: 0.0436 | 0.0223
Epoch 218/300, seasonal_2 Loss: 0.0437 | 0.0220
Epoch 219/300, seasonal_2 Loss: 0.0436 | 0.0225
Epoch 220/300, seasonal_2 Loss: 0.0438 | 0.0220
Epoch 221/300, seasonal_2 Loss: 0.0435 | 0.0223
Epoch 222/300, seasonal_2 Loss: 0.0435 | 0.0220
Epoch 223/300, seasonal_2 Loss: 0.0434 | 0.0223
Epoch 224/300, seasonal_2 Loss: 0.0434 | 0.0220
Epoch 225/300, seasonal_2 Loss: 0.0433 | 0.0224
Epoch 226/300, seasonal_2 Loss: 0.0434 | 0.0220
Epoch 227/300, seasonal_2 Loss: 0.0433 | 0.0223
Epoch 228/300, seasonal_2 Loss: 0.0433 | 0.0220
Epoch 229/300, seasonal_2 Loss: 0.0432 | 0.0222
Epoch 230/300, seasonal_2 Loss: 0.0432 | 0.0220
Epoch 231/300, seasonal_2 Loss: 0.0431 | 0.0222
Epoch 232/300, seasonal_2 Loss: 0.0431 | 0.0220
Epoch 233/300, seasonal_2 Loss: 0.0431 | 0.0222
Epoch 234/300, seasonal_2 Loss: 0.0431 | 0.0220
Epoch 235/300, seasonal_2 Loss: 0.0430 | 0.0221
Epoch 236/300, seasonal_2 Loss: 0.0430 | 0.0220
Epoch 237/300, seasonal_2 Loss: 0.0430 | 0.0221
Epoch 238/300, seasonal_2 Loss: 0.0430 | 0.0220
Epoch 239/300, seasonal_2 Loss: 0.0430 | 0.0221
Epoch 240/300, seasonal_2 Loss: 0.0429 | 0.0220
Epoch 241/300, seasonal_2 Loss: 0.0429 | 0.0221
Epoch 242/300, seasonal_2 Loss: 0.0429 | 0.0220
Epoch 243/300, seasonal_2 Loss: 0.0429 | 0.0221
Epoch 244/300, seasonal_2 Loss: 0.0429 | 0.0220
Epoch 245/300, seasonal_2 Loss: 0.0429 | 0.0221
Epoch 246/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 247/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 248/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 249/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 250/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 251/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 252/300, seasonal_2 Loss: 0.0428 | 0.0221
Epoch 253/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 254/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 255/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 256/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 257/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 258/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 259/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 260/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 261/300, seasonal_2 Loss: 0.0427 | 0.0221
Epoch 262/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 263/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 264/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 265/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 266/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 267/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 268/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 269/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 270/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 271/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 272/300, seasonal_2 Loss: 0.0426 | 0.0221
Epoch 273/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 274/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 275/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 276/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 277/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 278/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 279/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 280/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 281/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 282/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 283/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 284/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 285/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 286/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 287/300, seasonal_2 Loss: 0.0425 | 0.0221
Epoch 288/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 289/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 290/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 291/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 292/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 293/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 294/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 295/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 296/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 297/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 298/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 299/300, seasonal_2 Loss: 0.0424 | 0.0221
Epoch 300/300, seasonal_2 Loss: 0.0424 | 0.0221
Training seasonal_3 component with params: {'observation_period_num': 14, 'train_rates': 0.7316087954444779, 'learning_rate': 0.0005699121417227269, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9180099776477296}
Epoch 1/300, seasonal_3 Loss: 0.4031 | 0.2346
Epoch 2/300, seasonal_3 Loss: 0.2077 | 0.1832
Epoch 3/300, seasonal_3 Loss: 0.2081 | 0.2078
Epoch 4/300, seasonal_3 Loss: 0.1718 | 0.1822
Epoch 5/300, seasonal_3 Loss: 0.1547 | 0.1463
Epoch 6/300, seasonal_3 Loss: 0.1315 | 0.0962
Epoch 7/300, seasonal_3 Loss: 0.1241 | 0.0828
Epoch 8/300, seasonal_3 Loss: 0.1123 | 0.0727
Epoch 9/300, seasonal_3 Loss: 0.1077 | 0.0676
Epoch 10/300, seasonal_3 Loss: 0.1047 | 0.0667
Epoch 11/300, seasonal_3 Loss: 0.1042 | 0.0728
Epoch 12/300, seasonal_3 Loss: 0.1100 | 0.0786
Epoch 13/300, seasonal_3 Loss: 0.1079 | 0.0681
Epoch 14/300, seasonal_3 Loss: 0.1030 | 0.0745
Epoch 15/300, seasonal_3 Loss: 0.1029 | 0.1256
Epoch 16/300, seasonal_3 Loss: 0.1053 | 0.0614
Epoch 17/300, seasonal_3 Loss: 0.1015 | 0.1076
Epoch 18/300, seasonal_3 Loss: 0.1028 | 0.0634
Epoch 19/300, seasonal_3 Loss: 0.0999 | 0.0597
Epoch 20/300, seasonal_3 Loss: 0.0994 | 0.0553
Epoch 21/300, seasonal_3 Loss: 0.0963 | 0.1157
Epoch 22/300, seasonal_3 Loss: 0.0993 | 0.0568
Epoch 23/300, seasonal_3 Loss: 0.0985 | 0.1079
Epoch 24/300, seasonal_3 Loss: 0.0986 | 0.0577
Epoch 25/300, seasonal_3 Loss: 0.0924 | 0.0598
Epoch 26/300, seasonal_3 Loss: 0.0898 | 0.0520
Epoch 27/300, seasonal_3 Loss: 0.0894 | 0.0702
Epoch 28/300, seasonal_3 Loss: 0.0889 | 0.0531
Epoch 29/300, seasonal_3 Loss: 0.0886 | 0.0681
Epoch 30/300, seasonal_3 Loss: 0.0870 | 0.0497
Epoch 31/300, seasonal_3 Loss: 0.0865 | 0.0588
Epoch 32/300, seasonal_3 Loss: 0.0864 | 0.0511
Epoch 33/300, seasonal_3 Loss: 0.0858 | 0.0605
Epoch 34/300, seasonal_3 Loss: 0.0851 | 0.0503
Epoch 35/300, seasonal_3 Loss: 0.0847 | 0.0609
Epoch 36/300, seasonal_3 Loss: 0.0860 | 0.0487
Epoch 37/300, seasonal_3 Loss: 0.0856 | 0.0607
Epoch 38/300, seasonal_3 Loss: 0.0880 | 0.0501
Epoch 39/300, seasonal_3 Loss: 0.0880 | 0.0685
Epoch 40/300, seasonal_3 Loss: 0.0901 | 0.0497
Epoch 41/300, seasonal_3 Loss: 0.0894 | 0.0646
Epoch 42/300, seasonal_3 Loss: 0.0852 | 0.0448
Epoch 43/300, seasonal_3 Loss: 0.0837 | 0.0487
Epoch 44/300, seasonal_3 Loss: 0.0860 | 0.0448
Epoch 45/300, seasonal_3 Loss: 0.0837 | 0.0525
Epoch 46/300, seasonal_3 Loss: 0.0815 | 0.0445
Epoch 47/300, seasonal_3 Loss: 0.0816 | 0.0554
Epoch 48/300, seasonal_3 Loss: 0.0828 | 0.0442
Epoch 49/300, seasonal_3 Loss: 0.0813 | 0.0505
Epoch 50/300, seasonal_3 Loss: 0.0803 | 0.0434
Epoch 51/300, seasonal_3 Loss: 0.0797 | 0.0470
Epoch 52/300, seasonal_3 Loss: 0.0797 | 0.0440
Epoch 53/300, seasonal_3 Loss: 0.0790 | 0.0459
Epoch 54/300, seasonal_3 Loss: 0.0789 | 0.0434
Epoch 55/300, seasonal_3 Loss: 0.0785 | 0.0444
Epoch 56/300, seasonal_3 Loss: 0.0784 | 0.0436
Epoch 57/300, seasonal_3 Loss: 0.0780 | 0.0439
Epoch 58/300, seasonal_3 Loss: 0.0778 | 0.0438
Epoch 59/300, seasonal_3 Loss: 0.0775 | 0.0425
Epoch 60/300, seasonal_3 Loss: 0.0774 | 0.0433
Epoch 61/300, seasonal_3 Loss: 0.0770 | 0.0417
Epoch 62/300, seasonal_3 Loss: 0.0769 | 0.0432
Epoch 63/300, seasonal_3 Loss: 0.0768 | 0.0415
Epoch 64/300, seasonal_3 Loss: 0.0766 | 0.0435
Epoch 65/300, seasonal_3 Loss: 0.0767 | 0.0412
Epoch 66/300, seasonal_3 Loss: 0.0765 | 0.0424
Epoch 67/300, seasonal_3 Loss: 0.0760 | 0.0405
Epoch 68/300, seasonal_3 Loss: 0.0756 | 0.0418
Epoch 69/300, seasonal_3 Loss: 0.0754 | 0.0402
Epoch 70/300, seasonal_3 Loss: 0.0752 | 0.0413
Epoch 71/300, seasonal_3 Loss: 0.0750 | 0.0398
Epoch 72/300, seasonal_3 Loss: 0.0747 | 0.0406
Epoch 73/300, seasonal_3 Loss: 0.0745 | 0.0394
Epoch 74/300, seasonal_3 Loss: 0.0743 | 0.0402
Epoch 75/300, seasonal_3 Loss: 0.0742 | 0.0392
Epoch 76/300, seasonal_3 Loss: 0.0741 | 0.0398
Epoch 77/300, seasonal_3 Loss: 0.0740 | 0.0390
Epoch 78/300, seasonal_3 Loss: 0.0738 | 0.0394
Epoch 79/300, seasonal_3 Loss: 0.0737 | 0.0388
Epoch 80/300, seasonal_3 Loss: 0.0735 | 0.0390
Epoch 81/300, seasonal_3 Loss: 0.0734 | 0.0386
Epoch 82/300, seasonal_3 Loss: 0.0733 | 0.0386
Epoch 83/300, seasonal_3 Loss: 0.0732 | 0.0383
Epoch 84/300, seasonal_3 Loss: 0.0731 | 0.0383
Epoch 85/300, seasonal_3 Loss: 0.0730 | 0.0381
Epoch 86/300, seasonal_3 Loss: 0.0728 | 0.0380
Epoch 87/300, seasonal_3 Loss: 0.0727 | 0.0379
Epoch 88/300, seasonal_3 Loss: 0.0726 | 0.0377
Epoch 89/300, seasonal_3 Loss: 0.0725 | 0.0376
Epoch 90/300, seasonal_3 Loss: 0.0724 | 0.0375
Epoch 91/300, seasonal_3 Loss: 0.0723 | 0.0374
Epoch 92/300, seasonal_3 Loss: 0.0723 | 0.0373
Epoch 93/300, seasonal_3 Loss: 0.0722 | 0.0371
Epoch 94/300, seasonal_3 Loss: 0.0721 | 0.0370
Epoch 95/300, seasonal_3 Loss: 0.0720 | 0.0369
Epoch 96/300, seasonal_3 Loss: 0.0719 | 0.0368
Epoch 97/300, seasonal_3 Loss: 0.0718 | 0.0367
Epoch 98/300, seasonal_3 Loss: 0.0718 | 0.0366
Epoch 99/300, seasonal_3 Loss: 0.0717 | 0.0365
Epoch 100/300, seasonal_3 Loss: 0.0716 | 0.0364
Epoch 101/300, seasonal_3 Loss: 0.0715 | 0.0363
Epoch 102/300, seasonal_3 Loss: 0.0715 | 0.0362
Epoch 103/300, seasonal_3 Loss: 0.0714 | 0.0361
Epoch 104/300, seasonal_3 Loss: 0.0713 | 0.0360
Epoch 105/300, seasonal_3 Loss: 0.0712 | 0.0359
Epoch 106/300, seasonal_3 Loss: 0.0712 | 0.0359
Epoch 107/300, seasonal_3 Loss: 0.0711 | 0.0358
Epoch 108/300, seasonal_3 Loss: 0.0711 | 0.0357
Epoch 109/300, seasonal_3 Loss: 0.0710 | 0.0356
Epoch 110/300, seasonal_3 Loss: 0.0709 | 0.0355
Epoch 111/300, seasonal_3 Loss: 0.0709 | 0.0355
Epoch 112/300, seasonal_3 Loss: 0.0708 | 0.0354
Epoch 113/300, seasonal_3 Loss: 0.0708 | 0.0353
Epoch 114/300, seasonal_3 Loss: 0.0707 | 0.0353
Epoch 115/300, seasonal_3 Loss: 0.0707 | 0.0352
Epoch 116/300, seasonal_3 Loss: 0.0706 | 0.0351
Epoch 117/300, seasonal_3 Loss: 0.0706 | 0.0351
Epoch 118/300, seasonal_3 Loss: 0.0705 | 0.0350
Epoch 119/300, seasonal_3 Loss: 0.0705 | 0.0350
Epoch 120/300, seasonal_3 Loss: 0.0704 | 0.0349
Epoch 121/300, seasonal_3 Loss: 0.0704 | 0.0349
Epoch 122/300, seasonal_3 Loss: 0.0703 | 0.0348
Epoch 123/300, seasonal_3 Loss: 0.0703 | 0.0348
Epoch 124/300, seasonal_3 Loss: 0.0702 | 0.0347
Epoch 125/300, seasonal_3 Loss: 0.0702 | 0.0347
Epoch 126/300, seasonal_3 Loss: 0.0702 | 0.0346
Epoch 127/300, seasonal_3 Loss: 0.0701 | 0.0346
Epoch 128/300, seasonal_3 Loss: 0.0701 | 0.0346
Epoch 129/300, seasonal_3 Loss: 0.0701 | 0.0345
Epoch 130/300, seasonal_3 Loss: 0.0700 | 0.0345
Epoch 131/300, seasonal_3 Loss: 0.0700 | 0.0344
Epoch 132/300, seasonal_3 Loss: 0.0699 | 0.0344
Epoch 133/300, seasonal_3 Loss: 0.0699 | 0.0344
Epoch 134/300, seasonal_3 Loss: 0.0699 | 0.0343
Epoch 135/300, seasonal_3 Loss: 0.0698 | 0.0343
Epoch 136/300, seasonal_3 Loss: 0.0698 | 0.0343
Epoch 137/300, seasonal_3 Loss: 0.0698 | 0.0342
Epoch 138/300, seasonal_3 Loss: 0.0698 | 0.0342
Epoch 139/300, seasonal_3 Loss: 0.0697 | 0.0342
Epoch 140/300, seasonal_3 Loss: 0.0697 | 0.0341
Epoch 141/300, seasonal_3 Loss: 0.0697 | 0.0341
Epoch 142/300, seasonal_3 Loss: 0.0696 | 0.0341
Epoch 143/300, seasonal_3 Loss: 0.0696 | 0.0341
Epoch 144/300, seasonal_3 Loss: 0.0696 | 0.0340
Epoch 145/300, seasonal_3 Loss: 0.0696 | 0.0340
Epoch 146/300, seasonal_3 Loss: 0.0695 | 0.0340
Epoch 147/300, seasonal_3 Loss: 0.0695 | 0.0340
Epoch 148/300, seasonal_3 Loss: 0.0695 | 0.0339
Epoch 149/300, seasonal_3 Loss: 0.0695 | 0.0339
Epoch 150/300, seasonal_3 Loss: 0.0694 | 0.0339
Epoch 151/300, seasonal_3 Loss: 0.0694 | 0.0339
Epoch 152/300, seasonal_3 Loss: 0.0694 | 0.0339
Epoch 153/300, seasonal_3 Loss: 0.0694 | 0.0338
Epoch 154/300, seasonal_3 Loss: 0.0694 | 0.0338
Epoch 155/300, seasonal_3 Loss: 0.0693 | 0.0338
Epoch 156/300, seasonal_3 Loss: 0.0693 | 0.0338
Epoch 157/300, seasonal_3 Loss: 0.0693 | 0.0338
Epoch 158/300, seasonal_3 Loss: 0.0693 | 0.0338
Epoch 159/300, seasonal_3 Loss: 0.0693 | 0.0337
Epoch 160/300, seasonal_3 Loss: 0.0693 | 0.0337
Epoch 161/300, seasonal_3 Loss: 0.0692 | 0.0337
Epoch 162/300, seasonal_3 Loss: 0.0692 | 0.0337
Epoch 163/300, seasonal_3 Loss: 0.0692 | 0.0337
Epoch 164/300, seasonal_3 Loss: 0.0692 | 0.0337
Epoch 165/300, seasonal_3 Loss: 0.0692 | 0.0336
Epoch 166/300, seasonal_3 Loss: 0.0692 | 0.0336
Epoch 167/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 168/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 169/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 170/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 171/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 172/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 173/300, seasonal_3 Loss: 0.0691 | 0.0336
Epoch 174/300, seasonal_3 Loss: 0.0691 | 0.0335
Epoch 175/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 176/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 177/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 178/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 179/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 180/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 181/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 182/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 183/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 184/300, seasonal_3 Loss: 0.0690 | 0.0335
Epoch 185/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 186/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 187/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 188/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 189/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 190/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 191/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 192/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 193/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 194/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 195/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 196/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 197/300, seasonal_3 Loss: 0.0688 | 0.0334
Epoch 198/300, seasonal_3 Loss: 0.0688 | 0.0334
Epoch 199/300, seasonal_3 Loss: 0.0688 | 0.0334
Epoch 200/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 201/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 202/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 203/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 204/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 205/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 206/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 207/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 208/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 209/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 210/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 211/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 212/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 213/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 214/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 215/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 216/300, seasonal_3 Loss: 0.0688 | 0.0333
Epoch 217/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 218/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 219/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 220/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 221/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 222/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 223/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 224/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 225/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 226/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 227/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 228/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 229/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 230/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 231/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 232/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 233/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 234/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 235/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 236/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 237/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 238/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 239/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 240/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 241/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 242/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 243/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 244/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 245/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 246/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 247/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 248/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 249/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 250/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 251/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 252/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 253/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 254/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 255/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 256/300, seasonal_3 Loss: 0.0687 | 0.0332
Epoch 257/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 258/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 259/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 260/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 261/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 262/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 263/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 264/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 265/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 266/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 267/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 268/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 269/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 270/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 271/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 272/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 273/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 274/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 275/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 276/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 277/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 278/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 279/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 280/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 281/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 282/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 283/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 284/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 285/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 286/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 287/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 288/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 289/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 290/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 291/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 292/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 293/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 294/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 295/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 296/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 297/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 298/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 299/300, seasonal_3 Loss: 0.0686 | 0.0332
Epoch 300/300, seasonal_3 Loss: 0.0686 | 0.0332
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.886396979725396, 'learning_rate': 8.012917276576874e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7539683187512453}
Epoch 1/300, resid Loss: 0.2258 | 0.1118
Epoch 2/300, resid Loss: 0.1203 | 0.0833
Epoch 3/300, resid Loss: 0.1070 | 0.0711
Epoch 4/300, resid Loss: 0.1010 | 0.0651
Epoch 5/300, resid Loss: 0.0974 | 0.0613
Epoch 6/300, resid Loss: 0.0947 | 0.0581
Epoch 7/300, resid Loss: 0.0916 | 0.0557
Epoch 8/300, resid Loss: 0.0893 | 0.0535
Epoch 9/300, resid Loss: 0.0871 | 0.0508
Epoch 10/300, resid Loss: 0.0848 | 0.0481
Epoch 11/300, resid Loss: 0.0827 | 0.0459
Epoch 12/300, resid Loss: 0.0809 | 0.0441
Epoch 13/300, resid Loss: 0.0787 | 0.0435
Epoch 14/300, resid Loss: 0.0771 | 0.0419
Epoch 15/300, resid Loss: 0.0757 | 0.0406
Epoch 16/300, resid Loss: 0.0746 | 0.0396
Epoch 17/300, resid Loss: 0.0736 | 0.0390
Epoch 18/300, resid Loss: 0.0727 | 0.0384
Epoch 19/300, resid Loss: 0.0716 | 0.0377
Epoch 20/300, resid Loss: 0.0710 | 0.0372
Epoch 21/300, resid Loss: 0.0705 | 0.0367
Epoch 22/300, resid Loss: 0.0700 | 0.0362
Epoch 23/300, resid Loss: 0.0695 | 0.0357
Epoch 24/300, resid Loss: 0.0690 | 0.0352
Epoch 25/300, resid Loss: 0.0682 | 0.0343
Epoch 26/300, resid Loss: 0.0676 | 0.0338
Epoch 27/300, resid Loss: 0.0671 | 0.0333
Epoch 28/300, resid Loss: 0.0667 | 0.0329
Epoch 29/300, resid Loss: 0.0662 | 0.0325
Epoch 30/300, resid Loss: 0.0658 | 0.0322
Epoch 31/300, resid Loss: 0.0654 | 0.0315
Epoch 32/300, resid Loss: 0.0650 | 0.0313
Epoch 33/300, resid Loss: 0.0648 | 0.0312
Epoch 34/300, resid Loss: 0.0646 | 0.0310
Epoch 35/300, resid Loss: 0.0644 | 0.0309
Epoch 36/300, resid Loss: 0.0642 | 0.0308
Epoch 37/300, resid Loss: 0.0639 | 0.0307
Epoch 38/300, resid Loss: 0.0637 | 0.0306
Epoch 39/300, resid Loss: 0.0636 | 0.0305
Epoch 40/300, resid Loss: 0.0635 | 0.0304
Epoch 41/300, resid Loss: 0.0633 | 0.0303
Epoch 42/300, resid Loss: 0.0632 | 0.0302
Epoch 43/300, resid Loss: 0.0630 | 0.0302
Epoch 44/300, resid Loss: 0.0628 | 0.0301
Epoch 45/300, resid Loss: 0.0627 | 0.0301
Epoch 46/300, resid Loss: 0.0626 | 0.0300
Epoch 47/300, resid Loss: 0.0625 | 0.0299
Epoch 48/300, resid Loss: 0.0624 | 0.0298
Epoch 49/300, resid Loss: 0.0622 | 0.0300
Epoch 50/300, resid Loss: 0.0621 | 0.0299
Epoch 51/300, resid Loss: 0.0620 | 0.0298
Epoch 52/300, resid Loss: 0.0619 | 0.0297
Epoch 53/300, resid Loss: 0.0618 | 0.0297
Epoch 54/300, resid Loss: 0.0617 | 0.0296
Epoch 55/300, resid Loss: 0.0616 | 0.0297
Epoch 56/300, resid Loss: 0.0615 | 0.0296
Epoch 57/300, resid Loss: 0.0614 | 0.0295
Epoch 58/300, resid Loss: 0.0613 | 0.0295
Epoch 59/300, resid Loss: 0.0613 | 0.0294
Epoch 60/300, resid Loss: 0.0612 | 0.0293
Epoch 61/300, resid Loss: 0.0611 | 0.0295
Epoch 62/300, resid Loss: 0.0611 | 0.0295
Epoch 63/300, resid Loss: 0.0610 | 0.0294
Epoch 64/300, resid Loss: 0.0609 | 0.0294
Epoch 65/300, resid Loss: 0.0609 | 0.0293
Epoch 66/300, resid Loss: 0.0608 | 0.0293
Epoch 67/300, resid Loss: 0.0607 | 0.0295
Epoch 68/300, resid Loss: 0.0607 | 0.0294
Epoch 69/300, resid Loss: 0.0607 | 0.0294
Epoch 70/300, resid Loss: 0.0606 | 0.0293
Epoch 71/300, resid Loss: 0.0606 | 0.0293
Epoch 72/300, resid Loss: 0.0605 | 0.0292
Epoch 73/300, resid Loss: 0.0605 | 0.0291
Epoch 74/300, resid Loss: 0.0605 | 0.0291
Epoch 75/300, resid Loss: 0.0604 | 0.0291
Epoch 76/300, resid Loss: 0.0604 | 0.0291
Epoch 77/300, resid Loss: 0.0604 | 0.0291
Epoch 78/300, resid Loss: 0.0603 | 0.0290
Epoch 79/300, resid Loss: 0.0603 | 0.0290
Epoch 80/300, resid Loss: 0.0603 | 0.0290
Epoch 81/300, resid Loss: 0.0602 | 0.0290
Epoch 82/300, resid Loss: 0.0602 | 0.0290
Epoch 83/300, resid Loss: 0.0602 | 0.0290
Epoch 84/300, resid Loss: 0.0602 | 0.0290
Epoch 85/300, resid Loss: 0.0601 | 0.0291
Epoch 86/300, resid Loss: 0.0601 | 0.0291
Epoch 87/300, resid Loss: 0.0601 | 0.0291
Epoch 88/300, resid Loss: 0.0601 | 0.0291
Epoch 89/300, resid Loss: 0.0601 | 0.0291
Epoch 90/300, resid Loss: 0.0601 | 0.0291
Epoch 91/300, resid Loss: 0.0601 | 0.0292
Epoch 92/300, resid Loss: 0.0600 | 0.0292
Epoch 93/300, resid Loss: 0.0600 | 0.0292
Epoch 94/300, resid Loss: 0.0600 | 0.0292
Epoch 95/300, resid Loss: 0.0600 | 0.0292
Epoch 96/300, resid Loss: 0.0600 | 0.0292
Epoch 97/300, resid Loss: 0.0600 | 0.0291
Epoch 98/300, resid Loss: 0.0600 | 0.0291
Epoch 99/300, resid Loss: 0.0599 | 0.0291
Epoch 100/300, resid Loss: 0.0599 | 0.0290
Epoch 101/300, resid Loss: 0.0599 | 0.0290
Epoch 102/300, resid Loss: 0.0599 | 0.0290
Epoch 103/300, resid Loss: 0.0599 | 0.0289
Epoch 104/300, resid Loss: 0.0599 | 0.0289
Epoch 105/300, resid Loss: 0.0599 | 0.0289
Epoch 106/300, resid Loss: 0.0599 | 0.0289
Epoch 107/300, resid Loss: 0.0598 | 0.0289
Epoch 108/300, resid Loss: 0.0598 | 0.0289
Epoch 109/300, resid Loss: 0.0598 | 0.0289
Epoch 110/300, resid Loss: 0.0598 | 0.0289
Epoch 111/300, resid Loss: 0.0598 | 0.0289
Epoch 112/300, resid Loss: 0.0598 | 0.0289
Epoch 113/300, resid Loss: 0.0598 | 0.0289
Epoch 114/300, resid Loss: 0.0598 | 0.0289
Epoch 115/300, resid Loss: 0.0598 | 0.0289
Epoch 116/300, resid Loss: 0.0598 | 0.0289
Epoch 117/300, resid Loss: 0.0597 | 0.0289
Epoch 118/300, resid Loss: 0.0597 | 0.0288
Epoch 119/300, resid Loss: 0.0597 | 0.0288
Epoch 120/300, resid Loss: 0.0597 | 0.0288
Epoch 121/300, resid Loss: 0.0597 | 0.0288
Epoch 122/300, resid Loss: 0.0597 | 0.0288
Epoch 123/300, resid Loss: 0.0597 | 0.0288
Epoch 124/300, resid Loss: 0.0597 | 0.0288
Epoch 125/300, resid Loss: 0.0597 | 0.0288
Epoch 126/300, resid Loss: 0.0597 | 0.0288
Epoch 127/300, resid Loss: 0.0597 | 0.0288
Epoch 128/300, resid Loss: 0.0597 | 0.0288
Epoch 129/300, resid Loss: 0.0596 | 0.0288
Epoch 130/300, resid Loss: 0.0596 | 0.0288
Epoch 131/300, resid Loss: 0.0596 | 0.0288
Epoch 132/300, resid Loss: 0.0596 | 0.0288
Epoch 133/300, resid Loss: 0.0596 | 0.0288
Epoch 134/300, resid Loss: 0.0596 | 0.0288
Epoch 135/300, resid Loss: 0.0596 | 0.0288
Epoch 136/300, resid Loss: 0.0596 | 0.0288
Epoch 137/300, resid Loss: 0.0596 | 0.0288
Epoch 138/300, resid Loss: 0.0596 | 0.0288
Epoch 139/300, resid Loss: 0.0596 | 0.0288
Epoch 140/300, resid Loss: 0.0596 | 0.0288
Epoch 141/300, resid Loss: 0.0596 | 0.0288
Epoch 142/300, resid Loss: 0.0596 | 0.0288
Epoch 143/300, resid Loss: 0.0596 | 0.0288
Epoch 144/300, resid Loss: 0.0596 | 0.0288
Epoch 145/300, resid Loss: 0.0596 | 0.0288
Epoch 146/300, resid Loss: 0.0596 | 0.0288
Epoch 147/300, resid Loss: 0.0596 | 0.0288
Epoch 148/300, resid Loss: 0.0596 | 0.0288
Epoch 149/300, resid Loss: 0.0596 | 0.0288
Epoch 150/300, resid Loss: 0.0596 | 0.0288
Epoch 151/300, resid Loss: 0.0596 | 0.0288
Epoch 152/300, resid Loss: 0.0596 | 0.0288
Epoch 153/300, resid Loss: 0.0596 | 0.0288
Epoch 154/300, resid Loss: 0.0596 | 0.0288
Epoch 155/300, resid Loss: 0.0596 | 0.0288
Epoch 156/300, resid Loss: 0.0596 | 0.0288
Epoch 157/300, resid Loss: 0.0596 | 0.0288
Epoch 158/300, resid Loss: 0.0596 | 0.0288
Epoch 159/300, resid Loss: 0.0596 | 0.0288
Epoch 160/300, resid Loss: 0.0596 | 0.0288
Epoch 161/300, resid Loss: 0.0596 | 0.0288
Epoch 162/300, resid Loss: 0.0596 | 0.0288
Epoch 163/300, resid Loss: 0.0596 | 0.0288
Epoch 164/300, resid Loss: 0.0596 | 0.0288
Epoch 165/300, resid Loss: 0.0596 | 0.0288
Epoch 166/300, resid Loss: 0.0596 | 0.0288
Epoch 167/300, resid Loss: 0.0596 | 0.0288
Epoch 168/300, resid Loss: 0.0596 | 0.0288
Epoch 169/300, resid Loss: 0.0595 | 0.0288
Epoch 170/300, resid Loss: 0.0595 | 0.0288
Epoch 171/300, resid Loss: 0.0595 | 0.0288
Epoch 172/300, resid Loss: 0.0595 | 0.0288
Epoch 173/300, resid Loss: 0.0595 | 0.0288
Epoch 174/300, resid Loss: 0.0595 | 0.0288
Epoch 175/300, resid Loss: 0.0595 | 0.0288
Epoch 176/300, resid Loss: 0.0595 | 0.0288
Epoch 177/300, resid Loss: 0.0595 | 0.0288
Epoch 178/300, resid Loss: 0.0595 | 0.0288
Epoch 179/300, resid Loss: 0.0595 | 0.0288
Epoch 180/300, resid Loss: 0.0595 | 0.0288
Epoch 181/300, resid Loss: 0.0595 | 0.0288
Epoch 182/300, resid Loss: 0.0595 | 0.0288
Epoch 183/300, resid Loss: 0.0595 | 0.0288
Epoch 184/300, resid Loss: 0.0595 | 0.0288
Epoch 185/300, resid Loss: 0.0595 | 0.0288
Epoch 186/300, resid Loss: 0.0595 | 0.0288
Epoch 187/300, resid Loss: 0.0595 | 0.0288
Epoch 188/300, resid Loss: 0.0595 | 0.0288
Epoch 189/300, resid Loss: 0.0595 | 0.0288
Epoch 190/300, resid Loss: 0.0595 | 0.0288
Epoch 191/300, resid Loss: 0.0595 | 0.0288
Epoch 192/300, resid Loss: 0.0595 | 0.0288
Epoch 193/300, resid Loss: 0.0595 | 0.0288
Epoch 194/300, resid Loss: 0.0595 | 0.0288
Epoch 195/300, resid Loss: 0.0595 | 0.0288
Epoch 196/300, resid Loss: 0.0595 | 0.0288
Epoch 197/300, resid Loss: 0.0595 | 0.0288
Epoch 198/300, resid Loss: 0.0595 | 0.0288
Epoch 199/300, resid Loss: 0.0595 | 0.0288
Epoch 200/300, resid Loss: 0.0595 | 0.0288
Epoch 201/300, resid Loss: 0.0595 | 0.0288
Epoch 202/300, resid Loss: 0.0595 | 0.0288
Epoch 203/300, resid Loss: 0.0595 | 0.0288
Epoch 204/300, resid Loss: 0.0595 | 0.0288
Epoch 205/300, resid Loss: 0.0595 | 0.0288
Epoch 206/300, resid Loss: 0.0595 | 0.0288
Epoch 207/300, resid Loss: 0.0595 | 0.0288
Epoch 208/300, resid Loss: 0.0595 | 0.0288
Epoch 209/300, resid Loss: 0.0595 | 0.0288
Epoch 210/300, resid Loss: 0.0595 | 0.0288
Epoch 211/300, resid Loss: 0.0595 | 0.0288
Epoch 212/300, resid Loss: 0.0595 | 0.0288
Epoch 213/300, resid Loss: 0.0595 | 0.0288
Epoch 214/300, resid Loss: 0.0595 | 0.0288
Epoch 215/300, resid Loss: 0.0595 | 0.0288
Epoch 216/300, resid Loss: 0.0595 | 0.0288
Epoch 217/300, resid Loss: 0.0595 | 0.0288
Epoch 218/300, resid Loss: 0.0595 | 0.0288
Epoch 219/300, resid Loss: 0.0595 | 0.0288
Epoch 220/300, resid Loss: 0.0595 | 0.0288
Epoch 221/300, resid Loss: 0.0595 | 0.0288
Epoch 222/300, resid Loss: 0.0595 | 0.0288
Epoch 223/300, resid Loss: 0.0595 | 0.0288
Epoch 224/300, resid Loss: 0.0595 | 0.0288
Epoch 225/300, resid Loss: 0.0595 | 0.0288
Epoch 226/300, resid Loss: 0.0595 | 0.0288
Epoch 227/300, resid Loss: 0.0595 | 0.0288
Epoch 228/300, resid Loss: 0.0595 | 0.0288
Epoch 229/300, resid Loss: 0.0595 | 0.0288
Epoch 230/300, resid Loss: 0.0595 | 0.0288
Epoch 231/300, resid Loss: 0.0595 | 0.0288
Epoch 232/300, resid Loss: 0.0595 | 0.0288
Epoch 233/300, resid Loss: 0.0595 | 0.0288
Epoch 234/300, resid Loss: 0.0595 | 0.0288
Epoch 235/300, resid Loss: 0.0595 | 0.0288
Epoch 236/300, resid Loss: 0.0595 | 0.0288
Epoch 237/300, resid Loss: 0.0595 | 0.0288
Epoch 238/300, resid Loss: 0.0595 | 0.0288
Epoch 239/300, resid Loss: 0.0595 | 0.0288
Epoch 240/300, resid Loss: 0.0595 | 0.0288
Epoch 241/300, resid Loss: 0.0595 | 0.0288
Epoch 242/300, resid Loss: 0.0595 | 0.0288
Epoch 243/300, resid Loss: 0.0595 | 0.0288
Epoch 244/300, resid Loss: 0.0595 | 0.0288
Epoch 245/300, resid Loss: 0.0595 | 0.0288
Epoch 246/300, resid Loss: 0.0595 | 0.0288
Epoch 247/300, resid Loss: 0.0595 | 0.0288
Epoch 248/300, resid Loss: 0.0595 | 0.0288
Epoch 249/300, resid Loss: 0.0595 | 0.0288
Epoch 250/300, resid Loss: 0.0595 | 0.0288
Epoch 251/300, resid Loss: 0.0595 | 0.0288
Epoch 252/300, resid Loss: 0.0595 | 0.0288
Epoch 253/300, resid Loss: 0.0595 | 0.0288
Epoch 254/300, resid Loss: 0.0595 | 0.0288
Epoch 255/300, resid Loss: 0.0595 | 0.0288
Epoch 256/300, resid Loss: 0.0595 | 0.0288
Epoch 257/300, resid Loss: 0.0595 | 0.0288
Epoch 258/300, resid Loss: 0.0595 | 0.0288
Epoch 259/300, resid Loss: 0.0595 | 0.0288
Epoch 260/300, resid Loss: 0.0595 | 0.0288
Epoch 261/300, resid Loss: 0.0595 | 0.0288
Epoch 262/300, resid Loss: 0.0595 | 0.0288
Epoch 263/300, resid Loss: 0.0595 | 0.0288
Epoch 264/300, resid Loss: 0.0595 | 0.0288
Epoch 265/300, resid Loss: 0.0595 | 0.0288
Epoch 266/300, resid Loss: 0.0595 | 0.0288
Epoch 267/300, resid Loss: 0.0595 | 0.0288
Epoch 268/300, resid Loss: 0.0595 | 0.0288
Epoch 269/300, resid Loss: 0.0595 | 0.0288
Epoch 270/300, resid Loss: 0.0595 | 0.0288
Epoch 271/300, resid Loss: 0.0595 | 0.0288
Epoch 272/300, resid Loss: 0.0595 | 0.0288
Epoch 273/300, resid Loss: 0.0595 | 0.0288
Epoch 274/300, resid Loss: 0.0595 | 0.0288
Epoch 275/300, resid Loss: 0.0595 | 0.0288
Epoch 276/300, resid Loss: 0.0595 | 0.0288
Epoch 277/300, resid Loss: 0.0595 | 0.0288
Epoch 278/300, resid Loss: 0.0595 | 0.0288
Epoch 279/300, resid Loss: 0.0595 | 0.0288
Epoch 280/300, resid Loss: 0.0595 | 0.0288
Epoch 281/300, resid Loss: 0.0595 | 0.0288
Epoch 282/300, resid Loss: 0.0595 | 0.0288
Epoch 283/300, resid Loss: 0.0595 | 0.0288
Epoch 284/300, resid Loss: 0.0595 | 0.0288
Epoch 285/300, resid Loss: 0.0595 | 0.0288
Epoch 286/300, resid Loss: 0.0595 | 0.0288
Epoch 287/300, resid Loss: 0.0595 | 0.0288
Epoch 288/300, resid Loss: 0.0595 | 0.0288
Epoch 289/300, resid Loss: 0.0595 | 0.0288
Epoch 290/300, resid Loss: 0.0595 | 0.0288
Epoch 291/300, resid Loss: 0.0595 | 0.0288
Epoch 292/300, resid Loss: 0.0595 | 0.0288
Epoch 293/300, resid Loss: 0.0595 | 0.0288
Epoch 294/300, resid Loss: 0.0595 | 0.0288
Epoch 295/300, resid Loss: 0.0595 | 0.0288
Epoch 296/300, resid Loss: 0.0595 | 0.0288
Epoch 297/300, resid Loss: 0.0595 | 0.0288
Epoch 298/300, resid Loss: 0.0595 | 0.0288
Epoch 299/300, resid Loss: 0.0595 | 0.0288
Epoch 300/300, resid Loss: 0.0595 | 0.0288
Runtime (seconds): 2248.9843034744263
0.0003624766556945417
[221.14856]
[0.8102105]
[-3.1810467]
[3.6722538]
[0.766646]
[14.406941]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.8668495565652847
RMSE: 1.9664306640625
MAE: 1.9664306640625
R-squared: nan
[237.62357]
