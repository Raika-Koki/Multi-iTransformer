[32m[I 2025-02-06 10:42:09,403][0m A new study created in memory with name: no-name-0e9f061e-6c36-419d-b558-18d5cb4036ac[0m
[32m[I 2025-02-06 10:42:44,237][0m Trial 0 finished with value: 0.34106681490933133 and parameters: {'observation_period_num': 97, 'train_rates': 0.6373491474004775, 'learning_rate': 2.228709865012108e-05, 'batch_size': 141, 'step_size': 5, 'gamma': 0.879956452431039}. Best is trial 0 with value: 0.34106681490933133.[0m
[32m[I 2025-02-06 10:43:15,184][0m Trial 1 finished with value: 0.1754811275246981 and parameters: {'observation_period_num': 111, 'train_rates': 0.7943795928477599, 'learning_rate': 4.88584914669027e-05, 'batch_size': 180, 'step_size': 5, 'gamma': 0.9256450703869777}. Best is trial 1 with value: 0.1754811275246981.[0m
[32m[I 2025-02-06 10:45:40,497][0m Trial 2 finished with value: 0.29943323680269185 and parameters: {'observation_period_num': 204, 'train_rates': 0.8075443234174207, 'learning_rate': 9.662246491109264e-06, 'batch_size': 34, 'step_size': 11, 'gamma': 0.7718549224829904}. Best is trial 1 with value: 0.1754811275246981.[0m
[32m[I 2025-02-06 10:46:55,238][0m Trial 3 finished with value: 0.25900344044299417 and parameters: {'observation_period_num': 34, 'train_rates': 0.7548575176658809, 'learning_rate': 8.515483633904042e-06, 'batch_size': 68, 'step_size': 10, 'gamma': 0.8724964792132415}. Best is trial 1 with value: 0.1754811275246981.[0m
[32m[I 2025-02-06 10:47:26,563][0m Trial 4 finished with value: 0.08392346901653787 and parameters: {'observation_period_num': 64, 'train_rates': 0.8846207868610128, 'learning_rate': 0.0009480791801008228, 'batch_size': 196, 'step_size': 8, 'gamma': 0.8643101351845299}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:48:01,940][0m Trial 5 finished with value: 0.26848986981336226 and parameters: {'observation_period_num': 67, 'train_rates': 0.7143611175688208, 'learning_rate': 3.0354840290294433e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8804487691242517}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:48:59,466][0m Trial 6 finished with value: 0.3436140101327587 and parameters: {'observation_period_num': 230, 'train_rates': 0.6417567331405281, 'learning_rate': 0.0002329268630420985, 'batch_size': 74, 'step_size': 4, 'gamma': 0.7731400671448564}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:49:24,270][0m Trial 7 finished with value: 0.3239600019896682 and parameters: {'observation_period_num': 125, 'train_rates': 0.8530020162514917, 'learning_rate': 1.1331163502059904e-05, 'batch_size': 236, 'step_size': 15, 'gamma': 0.9297533421335689}. Best is trial 4 with value: 0.08392346901653787.[0m
Early stopping at epoch 41
[32m[I 2025-02-06 10:49:36,704][0m Trial 8 finished with value: 0.8453222068996937 and parameters: {'observation_period_num': 84, 'train_rates': 0.9094974277289287, 'learning_rate': 2.3897722199770217e-05, 'batch_size': 224, 'step_size': 1, 'gamma': 0.7624002379458068}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:50:03,777][0m Trial 9 finished with value: 0.22604916989803314 and parameters: {'observation_period_num': 80, 'train_rates': 0.9766057885472053, 'learning_rate': 5.973385352515073e-05, 'batch_size': 242, 'step_size': 9, 'gamma': 0.8403575976718339}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:50:36,655][0m Trial 10 finished with value: 0.3778463304042816 and parameters: {'observation_period_num': 170, 'train_rates': 0.9851209769847603, 'learning_rate': 0.0009075667151122194, 'batch_size': 188, 'step_size': 13, 'gamma': 0.9711240245112261}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:51:10,757][0m Trial 11 finished with value: 0.8627091358943157 and parameters: {'observation_period_num': 8, 'train_rates': 0.8609676791257413, 'learning_rate': 1.2279255017161475e-06, 'batch_size': 182, 'step_size': 7, 'gamma': 0.9295648691114866}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:51:42,401][0m Trial 12 finished with value: 0.13768846393103087 and parameters: {'observation_period_num': 157, 'train_rates': 0.8885311811088222, 'learning_rate': 0.0006674919096311562, 'batch_size': 181, 'step_size': 3, 'gamma': 0.8258531901353409}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:52:35,126][0m Trial 13 finished with value: 0.1394259510413995 and parameters: {'observation_period_num': 162, 'train_rates': 0.9108752178179165, 'learning_rate': 0.0009641110589346402, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8221069243264942}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:53:06,600][0m Trial 14 finished with value: 0.24850768584292382 and parameters: {'observation_period_num': 151, 'train_rates': 0.9099065603886779, 'learning_rate': 0.00022295019924537543, 'batch_size': 198, 'step_size': 3, 'gamma': 0.8193827105422812}. Best is trial 4 with value: 0.08392346901653787.[0m
[32m[I 2025-02-06 10:53:46,926][0m Trial 15 finished with value: 0.06444999683176128 and parameters: {'observation_period_num': 41, 'train_rates': 0.8556744646226869, 'learning_rate': 0.00030968565310566165, 'batch_size': 147, 'step_size': 8, 'gamma': 0.8440318638242641}. Best is trial 15 with value: 0.06444999683176128.[0m
[32m[I 2025-02-06 10:54:35,088][0m Trial 16 finished with value: 0.05199660265773146 and parameters: {'observation_period_num': 49, 'train_rates': 0.817487570656672, 'learning_rate': 0.0002732906582579192, 'batch_size': 115, 'step_size': 12, 'gamma': 0.8498737548084037}. Best is trial 16 with value: 0.05199660265773146.[0m
[32m[I 2025-02-06 10:55:21,541][0m Trial 17 finished with value: 0.16157518490744394 and parameters: {'observation_period_num': 40, 'train_rates': 0.7001010412575246, 'learning_rate': 0.000178616327279804, 'batch_size': 109, 'step_size': 12, 'gamma': 0.7972498630121608}. Best is trial 16 with value: 0.05199660265773146.[0m
[32m[I 2025-02-06 10:56:09,767][0m Trial 18 finished with value: 0.04094884577991715 and parameters: {'observation_period_num': 5, 'train_rates': 0.8131014733620466, 'learning_rate': 0.00010811651378846416, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8507923264902665}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 10:57:02,547][0m Trial 19 finished with value: 0.1740679008729547 and parameters: {'observation_period_num': 5, 'train_rates': 0.7847096382918698, 'learning_rate': 0.00010810745351242855, 'batch_size': 102, 'step_size': 15, 'gamma': 0.907689835255749}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 10:58:15,742][0m Trial 20 finished with value: 0.16516044403452323 and parameters: {'observation_period_num': 26, 'train_rates': 0.7457445416538065, 'learning_rate': 9.569113882108221e-05, 'batch_size': 70, 'step_size': 13, 'gamma': 0.794989063779608}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 10:58:52,504][0m Trial 21 finished with value: 0.09047182218758332 and parameters: {'observation_period_num': 55, 'train_rates': 0.8361864976646997, 'learning_rate': 0.0003182943769460373, 'batch_size': 154, 'step_size': 13, 'gamma': 0.851655660206216}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 10:59:37,805][0m Trial 22 finished with value: 0.04480819198379823 and parameters: {'observation_period_num': 26, 'train_rates': 0.8162419462352887, 'learning_rate': 0.0004324103799996593, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8458585502374526}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:00:35,598][0m Trial 23 finished with value: 0.04239070036653745 and parameters: {'observation_period_num': 19, 'train_rates': 0.8212273893513735, 'learning_rate': 0.0004555658634533396, 'batch_size': 94, 'step_size': 11, 'gamma': 0.900156365347437}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:01:37,727][0m Trial 24 finished with value: 0.1688476261496544 and parameters: {'observation_period_num': 20, 'train_rates': 0.7642121572671373, 'learning_rate': 0.0004903237689985076, 'batch_size': 84, 'step_size': 10, 'gamma': 0.9064854257093851}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:06:05,307][0m Trial 25 finished with value: 0.1348287450422725 and parameters: {'observation_period_num': 7, 'train_rates': 0.7186645698232439, 'learning_rate': 0.00010409992176579456, 'batch_size': 18, 'step_size': 14, 'gamma': 0.9014775907492133}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:07:41,081][0m Trial 26 finished with value: 0.14334982537059454 and parameters: {'observation_period_num': 24, 'train_rates': 0.6764185945929391, 'learning_rate': 0.0004680734241413574, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8913900863535703}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:08:31,535][0m Trial 27 finished with value: 0.09588419796922538 and parameters: {'observation_period_num': 75, 'train_rates': 0.945996439592458, 'learning_rate': 0.00016424099122229181, 'batch_size': 122, 'step_size': 10, 'gamma': 0.9783610289921499}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:09:27,702][0m Trial 28 finished with value: 0.12956101898576172 and parameters: {'observation_period_num': 101, 'train_rates': 0.827991866054423, 'learning_rate': 0.0004951864671675577, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9544557307601891}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:10:26,637][0m Trial 29 finished with value: 0.19854977815362068 and parameters: {'observation_period_num': 53, 'train_rates': 0.7777110841289687, 'learning_rate': 5.957674265563877e-05, 'batch_size': 89, 'step_size': 9, 'gamma': 0.861564050992549}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:11:09,571][0m Trial 30 finished with value: 0.6030110828578472 and parameters: {'observation_period_num': 22, 'train_rates': 0.7384329236517679, 'learning_rate': 1.5372641092797803e-06, 'batch_size': 127, 'step_size': 11, 'gamma': 0.88960593716978}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:11:54,530][0m Trial 31 finished with value: 0.062239363715187825 and parameters: {'observation_period_num': 47, 'train_rates': 0.8128544256741321, 'learning_rate': 0.0003303999562343223, 'batch_size': 125, 'step_size': 12, 'gamma': 0.8345933432184041}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:12:31,317][0m Trial 32 finished with value: 0.05851603516582715 and parameters: {'observation_period_num': 19, 'train_rates': 0.8013119063192322, 'learning_rate': 0.0001649088867476393, 'batch_size': 156, 'step_size': 12, 'gamma': 0.8108294466187371}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:13:05,468][0m Trial 33 finished with value: 0.0971677837272485 and parameters: {'observation_period_num': 96, 'train_rates': 0.8311405126310957, 'learning_rate': 0.00011793710055039667, 'batch_size': 163, 'step_size': 14, 'gamma': 0.858955806312788}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:14:48,945][0m Trial 34 finished with value: 0.06420925598433043 and parameters: {'observation_period_num': 36, 'train_rates': 0.8090575868238011, 'learning_rate': 0.0003559256472532275, 'batch_size': 51, 'step_size': 12, 'gamma': 0.8729951530813668}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:15:28,865][0m Trial 35 finished with value: 0.19091295165031455 and parameters: {'observation_period_num': 58, 'train_rates': 0.7798327740509458, 'learning_rate': 0.000620937820990458, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8470041889015054}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:16:17,766][0m Trial 36 finished with value: 0.0588745757780577 and parameters: {'observation_period_num': 33, 'train_rates': 0.8394637512808313, 'learning_rate': 4.216141421477549e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9187117671469106}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:17:19,514][0m Trial 37 finished with value: 0.23639018069952727 and parameters: {'observation_period_num': 251, 'train_rates': 0.8687224789108167, 'learning_rate': 8.25411082251454e-05, 'batch_size': 87, 'step_size': 10, 'gamma': 0.8826022008701943}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:18:37,374][0m Trial 38 finished with value: 0.6489710293543178 and parameters: {'observation_period_num': 123, 'train_rates': 0.6107635667459563, 'learning_rate': 3.466121237328674e-06, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9450477021679857}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:19:09,532][0m Trial 39 finished with value: 0.3246714268645195 and parameters: {'observation_period_num': 190, 'train_rates': 0.8176016525776415, 'learning_rate': 1.4823318314574162e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.8034225692661598}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:19:46,509][0m Trial 40 finished with value: 0.22151985075513464 and parameters: {'observation_period_num': 92, 'train_rates': 0.761410735898209, 'learning_rate': 0.0002346470255246812, 'batch_size': 141, 'step_size': 6, 'gamma': 0.7820855447251027}. Best is trial 18 with value: 0.04094884577991715.[0m
[32m[I 2025-02-06 11:20:37,014][0m Trial 41 finished with value: 0.04074576395479115 and parameters: {'observation_period_num': 16, 'train_rates': 0.8038682361610734, 'learning_rate': 0.0001754341788875228, 'batch_size': 113, 'step_size': 12, 'gamma': 0.8106513093947604}. Best is trial 41 with value: 0.04074576395479115.[0m
[32m[I 2025-02-06 11:21:25,244][0m Trial 42 finished with value: 0.0404741421882664 and parameters: {'observation_period_num': 14, 'train_rates': 0.7915531336358202, 'learning_rate': 0.00015453622019072187, 'batch_size': 115, 'step_size': 11, 'gamma': 0.8358524017493056}. Best is trial 42 with value: 0.0404741421882664.[0m
[32m[I 2025-02-06 11:22:21,539][0m Trial 43 finished with value: 0.03558615671674346 and parameters: {'observation_period_num': 13, 'train_rates': 0.7941255356960583, 'learning_rate': 0.0001463697757948502, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8350670556811687}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:23:19,926][0m Trial 44 finished with value: 0.04703633294661264 and parameters: {'observation_period_num': 12, 'train_rates': 0.7914487800999814, 'learning_rate': 7.15510055296155e-05, 'batch_size': 93, 'step_size': 15, 'gamma': 0.832213735483921}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:24:25,970][0m Trial 45 finished with value: 0.172044599437859 and parameters: {'observation_period_num': 14, 'train_rates': 0.7242270535338038, 'learning_rate': 3.611455987679889e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8690644167372852}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:25:50,064][0m Trial 46 finished with value: 0.18748039476849415 and parameters: {'observation_period_num': 64, 'train_rates': 0.7692061935278263, 'learning_rate': 0.0001328508346938828, 'batch_size': 61, 'step_size': 14, 'gamma': 0.7503926179024536}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:26:42,214][0m Trial 47 finished with value: 0.06015009288706922 and parameters: {'observation_period_num': 33, 'train_rates': 0.7958011031422476, 'learning_rate': 5.529828498731475e-05, 'batch_size': 105, 'step_size': 13, 'gamma': 0.8136474561266595}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:27:26,455][0m Trial 48 finished with value: 0.182300517029146 and parameters: {'observation_period_num': 71, 'train_rates': 0.8688150580820069, 'learning_rate': 2.416531533631201e-05, 'batch_size': 131, 'step_size': 11, 'gamma': 0.7844227297847097}. Best is trial 43 with value: 0.03558615671674346.[0m
[32m[I 2025-02-06 11:28:14,502][0m Trial 49 finished with value: 0.15468726941778255 and parameters: {'observation_period_num': 6, 'train_rates': 0.7455964688255434, 'learning_rate': 0.00021638059126300949, 'batch_size': 111, 'step_size': 9, 'gamma': 0.8313273984458676}. Best is trial 43 with value: 0.03558615671674346.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6775 | 0.4268
Epoch 2/300, Loss: 0.2335 | 0.2675
Epoch 3/300, Loss: 0.1807 | 0.2063
Epoch 4/300, Loss: 0.1574 | 0.1700
Epoch 5/300, Loss: 0.1458 | 0.1461
Epoch 6/300, Loss: 0.1384 | 0.1282
Epoch 7/300, Loss: 0.1335 | 0.1133
Epoch 8/300, Loss: 0.1292 | 0.1021
Epoch 9/300, Loss: 0.1210 | 0.0922
Epoch 10/300, Loss: 0.1174 | 0.0859
Epoch 11/300, Loss: 0.1158 | 0.0827
Epoch 12/300, Loss: 0.1140 | 0.0889
Epoch 13/300, Loss: 0.1120 | 0.0911
Epoch 14/300, Loss: 0.1100 | 0.0839
Epoch 15/300, Loss: 0.1076 | 0.0740
Epoch 16/300, Loss: 0.1068 | 0.0723
Epoch 17/300, Loss: 0.1094 | 0.0884
Epoch 18/300, Loss: 0.1120 | 0.0927
Epoch 19/300, Loss: 0.1092 | 0.0739
Epoch 20/300, Loss: 0.1033 | 0.0678
Epoch 21/300, Loss: 0.1032 | 0.0687
Epoch 22/300, Loss: 0.1038 | 0.0699
Epoch 23/300, Loss: 0.1027 | 0.0668
Epoch 24/300, Loss: 0.0996 | 0.0645
Epoch 25/300, Loss: 0.1001 | 0.0687
Epoch 26/300, Loss: 0.1013 | 0.0696
Epoch 27/300, Loss: 0.0987 | 0.0664
Epoch 28/300, Loss: 0.0963 | 0.0654
Epoch 29/300, Loss: 0.0956 | 0.0657
Epoch 30/300, Loss: 0.0950 | 0.0640
Epoch 31/300, Loss: 0.0945 | 0.0616
Epoch 32/300, Loss: 0.0944 | 0.0601
Epoch 33/300, Loss: 0.0946 | 0.0598
Epoch 34/300, Loss: 0.0947 | 0.0599
Epoch 35/300, Loss: 0.0941 | 0.0585
Epoch 36/300, Loss: 0.0930 | 0.0599
Epoch 37/300, Loss: 0.0925 | 0.0626
Epoch 38/300, Loss: 0.0921 | 0.0635
Epoch 39/300, Loss: 0.0914 | 0.0613
Epoch 40/300, Loss: 0.0911 | 0.0569
Epoch 41/300, Loss: 0.0915 | 0.0564
Epoch 42/300, Loss: 0.0925 | 0.0565
Epoch 43/300, Loss: 0.0922 | 0.0552
Epoch 44/300, Loss: 0.0911 | 0.0550
Epoch 45/300, Loss: 0.0905 | 0.0558
Epoch 46/300, Loss: 0.0905 | 0.0555
Epoch 47/300, Loss: 0.0905 | 0.0540
Epoch 48/300, Loss: 0.0902 | 0.0533
Epoch 49/300, Loss: 0.0897 | 0.0530
Epoch 50/300, Loss: 0.0891 | 0.0528
Epoch 51/300, Loss: 0.0884 | 0.0526
Epoch 52/300, Loss: 0.0881 | 0.0526
Epoch 53/300, Loss: 0.0877 | 0.0528
Epoch 54/300, Loss: 0.0876 | 0.0528
Epoch 55/300, Loss: 0.0875 | 0.0528
Epoch 56/300, Loss: 0.0873 | 0.0530
Epoch 57/300, Loss: 0.0869 | 0.0526
Epoch 58/300, Loss: 0.0861 | 0.0521
Epoch 59/300, Loss: 0.0853 | 0.0514
Epoch 60/300, Loss: 0.0849 | 0.0506
Epoch 61/300, Loss: 0.0848 | 0.0501
Epoch 62/300, Loss: 0.0848 | 0.0497
Epoch 63/300, Loss: 0.0847 | 0.0494
Epoch 64/300, Loss: 0.0843 | 0.0492
Epoch 65/300, Loss: 0.0839 | 0.0491
Epoch 66/300, Loss: 0.0835 | 0.0490
Epoch 67/300, Loss: 0.0833 | 0.0490
Epoch 68/300, Loss: 0.0831 | 0.0489
Epoch 69/300, Loss: 0.0829 | 0.0486
Epoch 70/300, Loss: 0.0828 | 0.0484
Epoch 71/300, Loss: 0.0826 | 0.0481
Epoch 72/300, Loss: 0.0825 | 0.0479
Epoch 73/300, Loss: 0.0823 | 0.0477
Epoch 74/300, Loss: 0.0822 | 0.0475
Epoch 75/300, Loss: 0.0821 | 0.0474
Epoch 76/300, Loss: 0.0820 | 0.0473
Epoch 77/300, Loss: 0.0818 | 0.0472
Epoch 78/300, Loss: 0.0817 | 0.0471
Epoch 79/300, Loss: 0.0816 | 0.0470
Epoch 80/300, Loss: 0.0815 | 0.0468
Epoch 81/300, Loss: 0.0814 | 0.0467
Epoch 82/300, Loss: 0.0813 | 0.0466
Epoch 83/300, Loss: 0.0812 | 0.0465
Epoch 84/300, Loss: 0.0811 | 0.0464
Epoch 85/300, Loss: 0.0810 | 0.0463
Epoch 86/300, Loss: 0.0809 | 0.0462
Epoch 87/300, Loss: 0.0809 | 0.0461
Epoch 88/300, Loss: 0.0808 | 0.0460
Epoch 89/300, Loss: 0.0807 | 0.0459
Epoch 90/300, Loss: 0.0806 | 0.0458
Epoch 91/300, Loss: 0.0806 | 0.0457
Epoch 92/300, Loss: 0.0805 | 0.0457
Epoch 93/300, Loss: 0.0804 | 0.0456
Epoch 94/300, Loss: 0.0804 | 0.0455
Epoch 95/300, Loss: 0.0803 | 0.0454
Epoch 96/300, Loss: 0.0802 | 0.0454
Epoch 97/300, Loss: 0.0802 | 0.0453
Epoch 98/300, Loss: 0.0801 | 0.0452
Epoch 99/300, Loss: 0.0801 | 0.0452
Epoch 100/300, Loss: 0.0800 | 0.0451
Epoch 101/300, Loss: 0.0800 | 0.0451
Epoch 102/300, Loss: 0.0799 | 0.0450
Epoch 103/300, Loss: 0.0799 | 0.0450
Epoch 104/300, Loss: 0.0798 | 0.0449
Epoch 105/300, Loss: 0.0798 | 0.0448
Epoch 106/300, Loss: 0.0797 | 0.0448
Epoch 107/300, Loss: 0.0797 | 0.0448
Epoch 108/300, Loss: 0.0797 | 0.0447
Epoch 109/300, Loss: 0.0796 | 0.0447
Epoch 110/300, Loss: 0.0796 | 0.0446
Epoch 111/300, Loss: 0.0795 | 0.0446
Epoch 112/300, Loss: 0.0795 | 0.0446
Epoch 113/300, Loss: 0.0795 | 0.0445
Epoch 114/300, Loss: 0.0794 | 0.0445
Epoch 115/300, Loss: 0.0794 | 0.0444
Epoch 116/300, Loss: 0.0794 | 0.0444
Epoch 117/300, Loss: 0.0793 | 0.0444
Epoch 118/300, Loss: 0.0793 | 0.0443
Epoch 119/300, Loss: 0.0793 | 0.0443
Epoch 120/300, Loss: 0.0792 | 0.0443
Epoch 121/300, Loss: 0.0792 | 0.0443
Epoch 122/300, Loss: 0.0792 | 0.0442
Epoch 123/300, Loss: 0.0792 | 0.0442
Epoch 124/300, Loss: 0.0791 | 0.0442
Epoch 125/300, Loss: 0.0791 | 0.0442
Epoch 126/300, Loss: 0.0791 | 0.0441
Epoch 127/300, Loss: 0.0791 | 0.0441
Epoch 128/300, Loss: 0.0790 | 0.0441
Epoch 129/300, Loss: 0.0790 | 0.0441
Epoch 130/300, Loss: 0.0790 | 0.0440
Epoch 131/300, Loss: 0.0790 | 0.0440
Epoch 132/300, Loss: 0.0789 | 0.0440
Epoch 133/300, Loss: 0.0789 | 0.0440
Epoch 134/300, Loss: 0.0789 | 0.0440
Epoch 135/300, Loss: 0.0789 | 0.0439
Epoch 136/300, Loss: 0.0789 | 0.0439
Epoch 137/300, Loss: 0.0788 | 0.0439
Epoch 138/300, Loss: 0.0788 | 0.0439
Epoch 139/300, Loss: 0.0788 | 0.0439
Epoch 140/300, Loss: 0.0788 | 0.0439
Epoch 141/300, Loss: 0.0788 | 0.0438
Epoch 142/300, Loss: 0.0788 | 0.0438
Epoch 143/300, Loss: 0.0787 | 0.0438
Epoch 144/300, Loss: 0.0787 | 0.0438
Epoch 145/300, Loss: 0.0787 | 0.0438
Epoch 146/300, Loss: 0.0787 | 0.0438
Epoch 147/300, Loss: 0.0787 | 0.0438
Epoch 148/300, Loss: 0.0787 | 0.0437
Epoch 149/300, Loss: 0.0787 | 0.0437
Epoch 150/300, Loss: 0.0786 | 0.0437
Epoch 151/300, Loss: 0.0786 | 0.0437
Epoch 152/300, Loss: 0.0786 | 0.0437
Epoch 153/300, Loss: 0.0786 | 0.0437
Epoch 154/300, Loss: 0.0786 | 0.0437
Epoch 155/300, Loss: 0.0786 | 0.0437
Epoch 156/300, Loss: 0.0786 | 0.0437
Epoch 157/300, Loss: 0.0786 | 0.0437
Epoch 158/300, Loss: 0.0786 | 0.0436
Epoch 159/300, Loss: 0.0786 | 0.0436
Epoch 160/300, Loss: 0.0785 | 0.0436
Epoch 161/300, Loss: 0.0785 | 0.0436
Epoch 162/300, Loss: 0.0785 | 0.0436
Epoch 163/300, Loss: 0.0785 | 0.0436
Epoch 164/300, Loss: 0.0785 | 0.0436
Epoch 165/300, Loss: 0.0785 | 0.0436
Epoch 166/300, Loss: 0.0785 | 0.0436
Epoch 167/300, Loss: 0.0785 | 0.0436
Epoch 168/300, Loss: 0.0785 | 0.0436
Epoch 169/300, Loss: 0.0785 | 0.0436
Epoch 170/300, Loss: 0.0785 | 0.0436
Epoch 171/300, Loss: 0.0785 | 0.0435
Epoch 172/300, Loss: 0.0784 | 0.0435
Epoch 173/300, Loss: 0.0784 | 0.0435
Epoch 174/300, Loss: 0.0784 | 0.0435
Epoch 175/300, Loss: 0.0784 | 0.0435
Epoch 176/300, Loss: 0.0784 | 0.0435
Epoch 177/300, Loss: 0.0784 | 0.0435
Epoch 178/300, Loss: 0.0784 | 0.0435
Epoch 179/300, Loss: 0.0784 | 0.0435
Epoch 180/300, Loss: 0.0784 | 0.0435
Epoch 181/300, Loss: 0.0784 | 0.0435
Epoch 182/300, Loss: 0.0784 | 0.0435
Epoch 183/300, Loss: 0.0784 | 0.0435
Epoch 184/300, Loss: 0.0784 | 0.0435
Epoch 185/300, Loss: 0.0784 | 0.0435
Epoch 186/300, Loss: 0.0784 | 0.0435
Epoch 187/300, Loss: 0.0784 | 0.0435
Epoch 188/300, Loss: 0.0784 | 0.0435
Epoch 189/300, Loss: 0.0784 | 0.0435
Epoch 190/300, Loss: 0.0784 | 0.0435
Epoch 191/300, Loss: 0.0784 | 0.0435
Epoch 192/300, Loss: 0.0784 | 0.0435
Epoch 193/300, Loss: 0.0783 | 0.0435
Epoch 194/300, Loss: 0.0783 | 0.0435
Epoch 195/300, Loss: 0.0783 | 0.0434
Epoch 196/300, Loss: 0.0783 | 0.0434
Epoch 197/300, Loss: 0.0783 | 0.0434
Epoch 198/300, Loss: 0.0783 | 0.0434
Epoch 199/300, Loss: 0.0783 | 0.0434
Epoch 200/300, Loss: 0.0783 | 0.0434
Epoch 201/300, Loss: 0.0783 | 0.0434
Epoch 202/300, Loss: 0.0783 | 0.0434
Epoch 203/300, Loss: 0.0783 | 0.0434
Epoch 204/300, Loss: 0.0783 | 0.0434
Epoch 205/300, Loss: 0.0783 | 0.0434
Epoch 206/300, Loss: 0.0783 | 0.0434
Epoch 207/300, Loss: 0.0783 | 0.0434
Epoch 208/300, Loss: 0.0783 | 0.0434
Epoch 209/300, Loss: 0.0783 | 0.0434
Epoch 210/300, Loss: 0.0783 | 0.0434
Epoch 211/300, Loss: 0.0783 | 0.0434
Epoch 212/300, Loss: 0.0783 | 0.0434
Epoch 213/300, Loss: 0.0783 | 0.0434
Epoch 214/300, Loss: 0.0783 | 0.0434
Epoch 215/300, Loss: 0.0783 | 0.0434
Epoch 216/300, Loss: 0.0783 | 0.0434
Epoch 217/300, Loss: 0.0783 | 0.0434
Epoch 218/300, Loss: 0.0783 | 0.0434
Epoch 219/300, Loss: 0.0783 | 0.0434
Epoch 220/300, Loss: 0.0783 | 0.0434
Epoch 221/300, Loss: 0.0783 | 0.0434
Epoch 222/300, Loss: 0.0783 | 0.0434
Epoch 223/300, Loss: 0.0783 | 0.0434
Epoch 224/300, Loss: 0.0783 | 0.0434
Epoch 225/300, Loss: 0.0783 | 0.0434
Epoch 226/300, Loss: 0.0783 | 0.0434
Epoch 227/300, Loss: 0.0783 | 0.0434
Epoch 228/300, Loss: 0.0783 | 0.0434
Epoch 229/300, Loss: 0.0783 | 0.0434
Epoch 230/300, Loss: 0.0783 | 0.0434
Epoch 231/300, Loss: 0.0783 | 0.0434
Epoch 232/300, Loss: 0.0783 | 0.0434
Epoch 233/300, Loss: 0.0783 | 0.0434
Epoch 234/300, Loss: 0.0783 | 0.0434
Epoch 235/300, Loss: 0.0783 | 0.0434
Epoch 236/300, Loss: 0.0783 | 0.0434
Epoch 237/300, Loss: 0.0783 | 0.0434
Epoch 238/300, Loss: 0.0783 | 0.0434
Epoch 239/300, Loss: 0.0783 | 0.0434
Epoch 240/300, Loss: 0.0783 | 0.0434
Epoch 241/300, Loss: 0.0783 | 0.0434
Epoch 242/300, Loss: 0.0783 | 0.0434
Epoch 243/300, Loss: 0.0783 | 0.0434
Epoch 244/300, Loss: 0.0783 | 0.0434
Epoch 245/300, Loss: 0.0783 | 0.0434
Epoch 246/300, Loss: 0.0783 | 0.0434
Epoch 247/300, Loss: 0.0783 | 0.0434
Epoch 248/300, Loss: 0.0783 | 0.0434
Epoch 249/300, Loss: 0.0783 | 0.0434
Epoch 250/300, Loss: 0.0783 | 0.0434
Epoch 251/300, Loss: 0.0783 | 0.0434
Epoch 252/300, Loss: 0.0783 | 0.0434
Epoch 253/300, Loss: 0.0783 | 0.0434
Epoch 254/300, Loss: 0.0783 | 0.0434
Epoch 255/300, Loss: 0.0783 | 0.0434
Epoch 256/300, Loss: 0.0783 | 0.0434
Epoch 257/300, Loss: 0.0783 | 0.0434
Epoch 258/300, Loss: 0.0783 | 0.0434
Epoch 259/300, Loss: 0.0783 | 0.0434
Epoch 260/300, Loss: 0.0783 | 0.0434
Epoch 261/300, Loss: 0.0783 | 0.0434
Epoch 262/300, Loss: 0.0782 | 0.0434
Epoch 263/300, Loss: 0.0782 | 0.0434
Epoch 264/300, Loss: 0.0782 | 0.0434
Epoch 265/300, Loss: 0.0782 | 0.0434
Epoch 266/300, Loss: 0.0782 | 0.0434
Epoch 267/300, Loss: 0.0782 | 0.0434
Epoch 268/300, Loss: 0.0782 | 0.0434
Epoch 269/300, Loss: 0.0782 | 0.0434
Epoch 270/300, Loss: 0.0782 | 0.0434
Epoch 271/300, Loss: 0.0782 | 0.0434
Epoch 272/300, Loss: 0.0782 | 0.0434
Epoch 273/300, Loss: 0.0782 | 0.0434
Epoch 274/300, Loss: 0.0782 | 0.0434
Epoch 275/300, Loss: 0.0782 | 0.0434
Epoch 276/300, Loss: 0.0782 | 0.0434
Epoch 277/300, Loss: 0.0782 | 0.0434
Epoch 278/300, Loss: 0.0782 | 0.0434
Epoch 279/300, Loss: 0.0782 | 0.0434
Epoch 280/300, Loss: 0.0782 | 0.0434
Epoch 281/300, Loss: 0.0782 | 0.0434
Epoch 282/300, Loss: 0.0782 | 0.0434
Epoch 283/300, Loss: 0.0782 | 0.0434
Epoch 284/300, Loss: 0.0782 | 0.0434
Epoch 285/300, Loss: 0.0782 | 0.0434
Epoch 286/300, Loss: 0.0782 | 0.0434
Epoch 287/300, Loss: 0.0782 | 0.0434
Epoch 288/300, Loss: 0.0782 | 0.0434
Epoch 289/300, Loss: 0.0782 | 0.0434
Epoch 290/300, Loss: 0.0782 | 0.0434
Epoch 291/300, Loss: 0.0782 | 0.0434
Epoch 292/300, Loss: 0.0782 | 0.0434
Epoch 293/300, Loss: 0.0782 | 0.0434
Epoch 294/300, Loss: 0.0782 | 0.0434
Epoch 295/300, Loss: 0.0782 | 0.0434
Epoch 296/300, Loss: 0.0782 | 0.0434
Epoch 297/300, Loss: 0.0782 | 0.0434
Epoch 298/300, Loss: 0.0782 | 0.0434
Epoch 299/300, Loss: 0.0782 | 0.0434
Epoch 300/300, Loss: 0.0782 | 0.0434
Runtime (seconds): 168.64321851730347
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.2534911073744297
RMSE: 0.50347900390625
MAE: 0.50347900390625
R-squared: nan
[200.13348]
