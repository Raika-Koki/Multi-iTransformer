ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-11 08:56:45,025][0m A new study created in memory with name: no-name-a1b81e6b-7dbc-41b0-ba4b-5f3903559bc4[0m
[32m[I 2025-01-11 08:57:58,974][0m Trial 0 finished with value: 0.24194367011852036 and parameters: {'observation_period_num': 153, 'train_rates': 0.6862401391102172, 'learning_rate': 8.164312877640719e-05, 'batch_size': 59, 'step_size': 15, 'gamma': 0.7782975565993498}. Best is trial 0 with value: 0.24194367011852036.[0m
[32m[I 2025-01-11 08:58:23,539][0m Trial 1 finished with value: 0.5552901596077218 and parameters: {'observation_period_num': 225, 'train_rates': 0.8225271189281812, 'learning_rate': 1.643789147978876e-05, 'batch_size': 206, 'step_size': 8, 'gamma': 0.7966917739850603}. Best is trial 0 with value: 0.24194367011852036.[0m
[32m[I 2025-01-11 08:58:54,948][0m Trial 2 finished with value: 0.9576237358013293 and parameters: {'observation_period_num': 114, 'train_rates': 0.6051447399382178, 'learning_rate': 8.64113036564767e-06, 'batch_size': 136, 'step_size': 4, 'gamma': 0.862046306846661}. Best is trial 0 with value: 0.24194367011852036.[0m
[32m[I 2025-01-11 08:59:19,363][0m Trial 3 finished with value: 0.8412332092751648 and parameters: {'observation_period_num': 84, 'train_rates': 0.6466474253916884, 'learning_rate': 1.1664149914058295e-05, 'batch_size': 193, 'step_size': 1, 'gamma': 0.9084101096286397}. Best is trial 0 with value: 0.24194367011852036.[0m
[32m[I 2025-01-11 09:00:12,472][0m Trial 4 finished with value: 0.42059475164259635 and parameters: {'observation_period_num': 206, 'train_rates': 0.8872772045153691, 'learning_rate': 4.143505696942117e-06, 'batch_size': 101, 'step_size': 11, 'gamma': 0.971425336884373}. Best is trial 0 with value: 0.24194367011852036.[0m
[32m[I 2025-01-11 09:01:03,914][0m Trial 5 finished with value: 0.06763370363539412 and parameters: {'observation_period_num': 96, 'train_rates': 0.9541758202749933, 'learning_rate': 0.0005595960801799701, 'batch_size': 113, 'step_size': 10, 'gamma': 0.7734209370868995}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:02:46,590][0m Trial 6 finished with value: 0.5424211635074763 and parameters: {'observation_period_num': 29, 'train_rates': 0.867020334891258, 'learning_rate': 2.8692243446747763e-06, 'batch_size': 52, 'step_size': 15, 'gamma': 0.7519500927705033}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:03:14,248][0m Trial 7 finished with value: 0.5512190461158752 and parameters: {'observation_period_num': 82, 'train_rates': 0.979383440160521, 'learning_rate': 1.1031151041268866e-05, 'batch_size': 228, 'step_size': 12, 'gamma': 0.9296403740199188}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:03:35,493][0m Trial 8 finished with value: 0.3458605002583438 and parameters: {'observation_period_num': 107, 'train_rates': 0.723793416687693, 'learning_rate': 0.0001440581190740142, 'batch_size': 240, 'step_size': 1, 'gamma': 0.9778285264044144}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:04:01,227][0m Trial 9 finished with value: 0.29605209827423096 and parameters: {'observation_period_num': 187, 'train_rates': 0.9426576835040252, 'learning_rate': 7.483149678758668e-05, 'batch_size': 234, 'step_size': 8, 'gamma': 0.7736477393194892}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:04:34,508][0m Trial 10 finished with value: 0.18473813519028298 and parameters: {'observation_period_num': 17, 'train_rates': 0.7574985216751371, 'learning_rate': 0.0009446551893030832, 'batch_size': 155, 'step_size': 5, 'gamma': 0.8371628293163301}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:05:08,368][0m Trial 11 finished with value: 0.191583571429512 and parameters: {'observation_period_num': 22, 'train_rates': 0.7631055335311873, 'learning_rate': 0.0008278932082459669, 'batch_size': 153, 'step_size': 5, 'gamma': 0.8301261963753006}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:05:52,364][0m Trial 12 finished with value: 0.06922507432185271 and parameters: {'observation_period_num': 53, 'train_rates': 0.7874005048176517, 'learning_rate': 0.0007449188741570965, 'batch_size': 118, 'step_size': 5, 'gamma': 0.8252131030261801}. Best is trial 5 with value: 0.06763370363539412.[0m
[32m[I 2025-01-11 09:06:44,780][0m Trial 13 finished with value: 0.06187585820324997 and parameters: {'observation_period_num': 61, 'train_rates': 0.8239127858746946, 'learning_rate': 0.00030693123942017724, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8131088789989267}. Best is trial 13 with value: 0.06187585820324997.[0m
[32m[I 2025-01-11 09:11:18,185][0m Trial 14 finished with value: 0.07428646707601512 and parameters: {'observation_period_num': 142, 'train_rates': 0.9046601119566684, 'learning_rate': 0.00019316241037183962, 'batch_size': 19, 'step_size': 11, 'gamma': 0.7997335518673757}. Best is trial 13 with value: 0.06187585820324997.[0m
[32m[I 2025-01-11 09:12:18,240][0m Trial 15 finished with value: 0.057310065193498724 and parameters: {'observation_period_num': 66, 'train_rates': 0.8311096054594221, 'learning_rate': 0.00025999708073517537, 'batch_size': 86, 'step_size': 10, 'gamma': 0.8768186078539931}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:13:26,383][0m Trial 16 finished with value: 0.0671751602043095 and parameters: {'observation_period_num': 58, 'train_rates': 0.8322794354442007, 'learning_rate': 0.000271290179194339, 'batch_size': 77, 'step_size': 13, 'gamma': 0.8831280588086954}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:14:28,630][0m Trial 17 finished with value: 0.08764704075342013 and parameters: {'observation_period_num': 53, 'train_rates': 0.8372782274378041, 'learning_rate': 4.236683028974828e-05, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8716267121062793}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:18:25,008][0m Trial 18 finished with value: 0.20875056147938822 and parameters: {'observation_period_num': 63, 'train_rates': 0.7162621363712472, 'learning_rate': 0.0002898782015877011, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9315061636929883}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:19:59,294][0m Trial 19 finished with value: 0.15161093969627754 and parameters: {'observation_period_num': 150, 'train_rates': 0.792322039488135, 'learning_rate': 3.5034883285440914e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8530510578862247}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:20:31,938][0m Trial 20 finished with value: 0.1361258144650008 and parameters: {'observation_period_num': 126, 'train_rates': 0.861745518278951, 'learning_rate': 8.339795786658176e-05, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9104487151620202}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:21:34,219][0m Trial 21 finished with value: 0.05739297012001441 and parameters: {'observation_period_num': 64, 'train_rates': 0.8128529149590047, 'learning_rate': 0.0003235495320292599, 'batch_size': 81, 'step_size': 13, 'gamma': 0.8806189352785619}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:22:36,489][0m Trial 22 finished with value: 0.5674871369163589 and parameters: {'observation_period_num': 39, 'train_rates': 0.8268506965437324, 'learning_rate': 1.0530375578126734e-06, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8817499750788417}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:23:20,637][0m Trial 23 finished with value: 0.08124111455361374 and parameters: {'observation_period_num': 79, 'train_rates': 0.9152897592356118, 'learning_rate': 0.0003805241861061238, 'batch_size': 127, 'step_size': 10, 'gamma': 0.9025402073341398}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:24:26,922][0m Trial 24 finished with value: 0.1834927165187528 and parameters: {'observation_period_num': 8, 'train_rates': 0.7728946558706713, 'learning_rate': 0.00013561961328606531, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8113065924583824}. Best is trial 15 with value: 0.057310065193498724.[0m
[32m[I 2025-01-11 09:25:17,308][0m Trial 25 finished with value: 0.056137369673871486 and parameters: {'observation_period_num': 40, 'train_rates': 0.8072137617531044, 'learning_rate': 0.00041781647366621, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8457525946658616}. Best is trial 25 with value: 0.056137369673871486.[0m
[32m[I 2025-01-11 09:27:10,523][0m Trial 26 finished with value: 0.17536438143319766 and parameters: {'observation_period_num': 38, 'train_rates': 0.7308519699432959, 'learning_rate': 0.00047730532169526984, 'batch_size': 41, 'step_size': 12, 'gamma': 0.850464143539892}. Best is trial 25 with value: 0.056137369673871486.[0m
[32m[I 2025-01-11 09:28:03,729][0m Trial 27 finished with value: 0.0653593889909026 and parameters: {'observation_period_num': 72, 'train_rates': 0.8594975556683184, 'learning_rate': 0.00018466354327922448, 'batch_size': 101, 'step_size': 12, 'gamma': 0.8862720828713212}. Best is trial 25 with value: 0.056137369673871486.[0m
[32m[I 2025-01-11 09:29:18,128][0m Trial 28 finished with value: 0.05232897553167717 and parameters: {'observation_period_num': 5, 'train_rates': 0.8009455515978081, 'learning_rate': 5.612431646803901e-05, 'batch_size': 68, 'step_size': 14, 'gamma': 0.9351692574645588}. Best is trial 28 with value: 0.05232897553167717.[0m
[32m[I 2025-01-11 09:30:34,409][0m Trial 29 finished with value: 0.1514120353679908 and parameters: {'observation_period_num': 6, 'train_rates': 0.6775184243002783, 'learning_rate': 6.622225635737184e-05, 'batch_size': 60, 'step_size': 15, 'gamma': 0.963627806396661}. Best is trial 28 with value: 0.05232897553167717.[0m
[32m[I 2025-01-11 09:31:54,838][0m Trial 30 finished with value: 0.2729542017718358 and parameters: {'observation_period_num': 172, 'train_rates': 0.7523971674000549, 'learning_rate': 0.00011266097591949993, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9492513444862096}. Best is trial 28 with value: 0.05232897553167717.[0m
[32m[I 2025-01-11 09:34:04,967][0m Trial 31 finished with value: 0.05975908742079304 and parameters: {'observation_period_num': 40, 'train_rates': 0.8023448292180914, 'learning_rate': 4.986274441025678e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8979669536722327}. Best is trial 28 with value: 0.05232897553167717.[0m
[32m[I 2025-01-11 09:35:17,865][0m Trial 32 finished with value: 0.05090211479571359 and parameters: {'observation_period_num': 28, 'train_rates': 0.8018270355543619, 'learning_rate': 0.0005095410787255088, 'batch_size': 70, 'step_size': 14, 'gamma': 0.9336389755859127}. Best is trial 32 with value: 0.05090211479571359.[0m
[32m[I 2025-01-11 09:36:37,707][0m Trial 33 finished with value: 0.046866259451770614 and parameters: {'observation_period_num': 26, 'train_rates': 0.8555064509053344, 'learning_rate': 0.0005099840156021122, 'batch_size': 67, 'step_size': 14, 'gamma': 0.9279353190864974}. Best is trial 33 with value: 0.046866259451770614.[0m
[32m[I 2025-01-11 09:39:06,430][0m Trial 34 finished with value: 0.04651506298969936 and parameters: {'observation_period_num': 28, 'train_rates': 0.8818409245546304, 'learning_rate': 0.000459320600131331, 'batch_size': 36, 'step_size': 14, 'gamma': 0.936812749993677}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:41:34,027][0m Trial 35 finished with value: 0.1254822273013646 and parameters: {'observation_period_num': 233, 'train_rates': 0.8872987307520548, 'learning_rate': 2.1892633970133182e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9311934238646843}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:44:19,959][0m Trial 36 finished with value: 0.11102020543930197 and parameters: {'observation_period_num': 249, 'train_rates': 0.8864606818202643, 'learning_rate': 0.0006621752106235912, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9896063738698533}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:45:37,423][0m Trial 37 finished with value: 0.09157958551901116 and parameters: {'observation_period_num': 24, 'train_rates': 0.8504390971428347, 'learning_rate': 2.1845807808542384e-05, 'batch_size': 68, 'step_size': 14, 'gamma': 0.9512553917006985}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:47:39,933][0m Trial 38 finished with value: 0.07792467637020245 and parameters: {'observation_period_num': 6, 'train_rates': 0.9225855247484296, 'learning_rate': 7.148194610398657e-06, 'batch_size': 46, 'step_size': 15, 'gamma': 0.9182322425030823}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:48:01,834][0m Trial 39 finished with value: 0.11297615404639925 and parameters: {'observation_period_num': 93, 'train_rates': 0.8753826244463574, 'learning_rate': 0.0005482124109628644, 'batch_size': 255, 'step_size': 14, 'gamma': 0.9492035308047911}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:49:10,171][0m Trial 40 finished with value: 0.1693060461620729 and parameters: {'observation_period_num': 28, 'train_rates': 0.6021642580100823, 'learning_rate': 0.00021194192467608112, 'batch_size': 61, 'step_size': 12, 'gamma': 0.9408625033483868}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:50:00,313][0m Trial 41 finished with value: 0.19812935237574766 and parameters: {'observation_period_num': 43, 'train_rates': 0.7818388394980818, 'learning_rate': 0.00040385974369238443, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9596745381403295}. Best is trial 34 with value: 0.04651506298969936.[0m
[32m[I 2025-01-11 09:50:59,383][0m Trial 42 finished with value: 0.04169627683527313 and parameters: {'observation_period_num': 17, 'train_rates': 0.849310135213044, 'learning_rate': 0.0005479970978316979, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9190387837960687}. Best is trial 42 with value: 0.04169627683527313.[0m
[32m[I 2025-01-11 09:52:15,581][0m Trial 43 finished with value: 0.04390397439955881 and parameters: {'observation_period_num': 14, 'train_rates': 0.8448037735238935, 'learning_rate': 0.0006111872174414408, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9190245530587783}. Best is trial 42 with value: 0.04169627683527313.[0m
[32m[I 2025-01-11 09:55:19,812][0m Trial 44 finished with value: 0.04471210806636747 and parameters: {'observation_period_num': 22, 'train_rates': 0.846165901906314, 'learning_rate': 0.0009602879872586975, 'batch_size': 28, 'step_size': 15, 'gamma': 0.923552503115952}. Best is trial 42 with value: 0.04169627683527313.[0m
[32m[I 2025-01-11 09:58:56,598][0m Trial 45 finished with value: 0.04363532355226622 and parameters: {'observation_period_num': 17, 'train_rates': 0.9447212510638238, 'learning_rate': 0.0009899485512162107, 'batch_size': 26, 'step_size': 15, 'gamma': 0.9185718783488559}. Best is trial 42 with value: 0.04169627683527313.[0m
[32m[I 2025-01-11 10:03:14,084][0m Trial 46 finished with value: 0.03424568738759929 and parameters: {'observation_period_num': 16, 'train_rates': 0.9556319240430787, 'learning_rate': 0.000983583342322892, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9172512567577513}. Best is trial 46 with value: 0.03424568738759929.[0m
[32m[I 2025-01-11 10:09:08,698][0m Trial 47 finished with value: 0.034346861989943536 and parameters: {'observation_period_num': 19, 'train_rates': 0.9691457564558719, 'learning_rate': 0.0009884237532026881, 'batch_size': 16, 'step_size': 15, 'gamma': 0.919911812955388}. Best is trial 46 with value: 0.03424568738759929.[0m
[32m[I 2025-01-11 10:15:04,676][0m Trial 48 finished with value: 0.046492930832836364 and parameters: {'observation_period_num': 16, 'train_rates': 0.9696617176364257, 'learning_rate': 0.0007335144254262335, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8966104949521403}. Best is trial 46 with value: 0.03424568738759929.[0m
[32m[I 2025-01-11 10:18:56,953][0m Trial 49 finished with value: 0.05270516973089527 and parameters: {'observation_period_num': 47, 'train_rates': 0.9492556345942225, 'learning_rate': 0.0009707850438668814, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9140776166173769}. Best is trial 46 with value: 0.03424568738759929.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-11 10:18:56,963][0m A new study created in memory with name: no-name-c28a765f-dc08-4c5c-9271-a9a628f91f1d[0m
[32m[I 2025-01-11 10:19:29,096][0m Trial 0 finished with value: 0.08369573950767517 and parameters: {'observation_period_num': 47, 'train_rates': 0.9583046456337112, 'learning_rate': 0.0001826700513475186, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8902356577192297}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:20:33,005][0m Trial 1 finished with value: 0.23875265935613654 and parameters: {'observation_period_num': 67, 'train_rates': 0.6657769173012555, 'learning_rate': 0.00010334059111587578, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9372359653177283}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:25:39,642][0m Trial 2 finished with value: 0.09252718364870226 and parameters: {'observation_period_num': 181, 'train_rates': 0.8665085763596452, 'learning_rate': 1.1684846684837532e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9737538104332356}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:26:07,774][0m Trial 3 finished with value: 1.0847440958023071 and parameters: {'observation_period_num': 64, 'train_rates': 0.9570906583679499, 'learning_rate': 5.432820232701697e-06, 'batch_size': 219, 'step_size': 13, 'gamma': 0.7863272847284467}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:26:56,305][0m Trial 4 finished with value: 0.4266785642411824 and parameters: {'observation_period_num': 131, 'train_rates': 0.7497924948932428, 'learning_rate': 3.242096825109342e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.7643258868401489}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:28:07,861][0m Trial 5 finished with value: 0.28853951313480614 and parameters: {'observation_period_num': 18, 'train_rates': 0.7024771317040714, 'learning_rate': 1.066499666764692e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9629142981076722}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:29:39,984][0m Trial 6 finished with value: 0.26281581625238165 and parameters: {'observation_period_num': 245, 'train_rates': 0.735791038043243, 'learning_rate': 0.0006568452590317108, 'batch_size': 48, 'step_size': 12, 'gamma': 0.9621594438958537}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:30:06,167][0m Trial 7 finished with value: 2.078816835849863 and parameters: {'observation_period_num': 113, 'train_rates': 0.6410961565003146, 'learning_rate': 2.3724513360461555e-06, 'batch_size': 179, 'step_size': 15, 'gamma': 0.9473782983270796}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:31:01,681][0m Trial 8 finished with value: 0.7413347705816611 and parameters: {'observation_period_num': 188, 'train_rates': 0.8591504319256646, 'learning_rate': 5.1359978521970595e-06, 'batch_size': 92, 'step_size': 5, 'gamma': 0.8058376534127447}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:31:38,999][0m Trial 9 finished with value: 0.2943620394383158 and parameters: {'observation_period_num': 172, 'train_rates': 0.7785308028896102, 'learning_rate': 2.0401692899917212e-05, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9824144603285517}. Best is trial 0 with value: 0.08369573950767517.[0m
[32m[I 2025-01-11 10:32:04,803][0m Trial 10 finished with value: 0.058729588985443115 and parameters: {'observation_period_num': 26, 'train_rates': 0.9889000714286666, 'learning_rate': 0.0007352130690398308, 'batch_size': 242, 'step_size': 9, 'gamma': 0.8776693224614347}. Best is trial 10 with value: 0.058729588985443115.[0m
[32m[I 2025-01-11 10:32:30,922][0m Trial 11 finished with value: 0.04928094893693924 and parameters: {'observation_period_num': 7, 'train_rates': 0.9815102018036622, 'learning_rate': 0.0008821159435935896, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8797989747536238}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:32:56,792][0m Trial 12 finished with value: 0.05894557386636734 and parameters: {'observation_period_num': 12, 'train_rates': 0.9881407377002707, 'learning_rate': 0.0007776749152413779, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8552519539910411}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:33:21,127][0m Trial 13 finished with value: 0.05465206748266129 and parameters: {'observation_period_num': 6, 'train_rates': 0.893447715278597, 'learning_rate': 0.00029078041995749324, 'batch_size': 247, 'step_size': 9, 'gamma': 0.8716856641800398}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:33:55,376][0m Trial 14 finished with value: 0.07669013337090008 and parameters: {'observation_period_num': 80, 'train_rates': 0.8884490769159951, 'learning_rate': 0.000192861933027313, 'batch_size': 161, 'step_size': 11, 'gamma': 0.8399785356595204}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:34:21,866][0m Trial 15 finished with value: 0.09888396459439444 and parameters: {'observation_period_num': 98, 'train_rates': 0.8897263101671393, 'learning_rate': 0.00028162268712473215, 'batch_size': 215, 'step_size': 8, 'gamma': 0.9070069212118158}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:34:44,207][0m Trial 16 finished with value: 0.09310845541859804 and parameters: {'observation_period_num': 5, 'train_rates': 0.8282487999703544, 'learning_rate': 6.203237313266271e-05, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8242998944355389}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:35:24,576][0m Trial 17 finished with value: 0.056136846790711084 and parameters: {'observation_period_num': 43, 'train_rates': 0.9280557109501282, 'learning_rate': 0.00033516816594927114, 'batch_size': 148, 'step_size': 10, 'gamma': 0.9142901217774067}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:35:50,750][0m Trial 18 finished with value: 0.15449431538581848 and parameters: {'observation_period_num': 143, 'train_rates': 0.9277697074207916, 'learning_rate': 7.885716049952154e-05, 'batch_size': 223, 'step_size': 15, 'gamma': 0.8613344034507997}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:36:19,700][0m Trial 19 finished with value: 0.06836593629856073 and parameters: {'observation_period_num': 41, 'train_rates': 0.8193642255067112, 'learning_rate': 0.00046441239664416937, 'batch_size': 192, 'step_size': 3, 'gamma': 0.9195157685476365}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:36:46,044][0m Trial 20 finished with value: 0.1047892878517444 and parameters: {'observation_period_num': 87, 'train_rates': 0.9196321794278937, 'learning_rate': 0.00016683739391873447, 'batch_size': 229, 'step_size': 7, 'gamma': 0.8934123317943065}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:37:30,486][0m Trial 21 finished with value: 0.05185437124205507 and parameters: {'observation_period_num': 41, 'train_rates': 0.9249054532113385, 'learning_rate': 0.00031731589985726117, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9220732237154801}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:38:19,144][0m Trial 22 finished with value: 0.05522105153253738 and parameters: {'observation_period_num': 33, 'train_rates': 0.9519682032131237, 'learning_rate': 0.0008940866840223829, 'batch_size': 121, 'step_size': 10, 'gamma': 0.8739400302717368}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:38:53,526][0m Trial 23 finished with value: 0.059914413878383734 and parameters: {'observation_period_num': 6, 'train_rates': 0.9025452779232299, 'learning_rate': 0.0005131234657721816, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9306434315784717}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:39:21,975][0m Trial 24 finished with value: 0.07797897709986133 and parameters: {'observation_period_num': 58, 'train_rates': 0.8650549225735515, 'learning_rate': 0.0003340204417477128, 'batch_size': 201, 'step_size': 13, 'gamma': 0.843390407992864}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:40:03,400][0m Trial 25 finished with value: 0.24747356456028272 and parameters: {'observation_period_num': 30, 'train_rates': 0.6002623520210443, 'learning_rate': 4.2545937373381213e-05, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8995644699193731}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:40:40,723][0m Trial 26 finished with value: 0.07503214799093477 and parameters: {'observation_period_num': 54, 'train_rates': 0.8291741090709278, 'learning_rate': 0.00011039010671878004, 'batch_size': 148, 'step_size': 11, 'gamma': 0.8786705903824961}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:41:06,830][0m Trial 27 finished with value: 0.0620240680873394 and parameters: {'observation_period_num': 26, 'train_rates': 0.9564103156608772, 'learning_rate': 0.00026934894034743245, 'batch_size': 237, 'step_size': 12, 'gamma': 0.8301442067301256}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:41:37,283][0m Trial 28 finished with value: 0.09551582676477921 and parameters: {'observation_period_num': 239, 'train_rates': 0.9140406360052172, 'learning_rate': 0.0009980908891601967, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8084784185005474}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:42:07,499][0m Trial 29 finished with value: 0.1185762956738472 and parameters: {'observation_period_num': 73, 'train_rates': 0.9726029901208386, 'learning_rate': 0.00014993448193143872, 'batch_size': 202, 'step_size': 6, 'gamma': 0.8905282247574493}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:42:53,843][0m Trial 30 finished with value: 0.06279189333319664 and parameters: {'observation_period_num': 47, 'train_rates': 0.9397903697448684, 'learning_rate': 0.0004521622397797123, 'batch_size': 129, 'step_size': 7, 'gamma': 0.8579939665469657}. Best is trial 11 with value: 0.04928094893693924.[0m
[32m[I 2025-01-11 10:43:44,103][0m Trial 31 finished with value: 0.04859839040477564 and parameters: {'observation_period_num': 33, 'train_rates': 0.9462476077072094, 'learning_rate': 0.000832486044019568, 'batch_size': 118, 'step_size': 10, 'gamma': 0.8836399626444033}. Best is trial 31 with value: 0.04859839040477564.[0m
[32m[I 2025-01-11 10:44:37,978][0m Trial 32 finished with value: 0.058652620762586594 and parameters: {'observation_period_num': 5, 'train_rates': 0.9679512742091819, 'learning_rate': 0.00023393169541534545, 'batch_size': 114, 'step_size': 10, 'gamma': 0.9329477810043092}. Best is trial 31 with value: 0.04859839040477564.[0m
[32m[I 2025-01-11 10:45:47,322][0m Trial 33 finished with value: 0.04464865091500009 and parameters: {'observation_period_num': 22, 'train_rates': 0.8930692762910136, 'learning_rate': 0.00046857070991352295, 'batch_size': 81, 'step_size': 12, 'gamma': 0.88279774462812}. Best is trial 33 with value: 0.04464865091500009.[0m
[32m[I 2025-01-11 10:46:57,641][0m Trial 34 finished with value: 0.07727853096754853 and parameters: {'observation_period_num': 64, 'train_rates': 0.8551143594394522, 'learning_rate': 0.00047405800851795356, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9206823607847895}. Best is trial 33 with value: 0.04464865091500009.[0m
[32m[I 2025-01-11 10:48:34,750][0m Trial 35 finished with value: 0.08678773636447973 and parameters: {'observation_period_num': 100, 'train_rates': 0.9392121434460704, 'learning_rate': 0.0006077893667980167, 'batch_size': 57, 'step_size': 1, 'gamma': 0.9476841645162123}. Best is trial 33 with value: 0.04464865091500009.[0m
[32m[I 2025-01-11 10:49:42,568][0m Trial 36 finished with value: 0.9597482681274414 and parameters: {'observation_period_num': 34, 'train_rates': 0.9763489570085899, 'learning_rate': 1.0207467637904632e-06, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9065513509984586}. Best is trial 33 with value: 0.04464865091500009.[0m
[32m[I 2025-01-11 10:53:01,479][0m Trial 37 finished with value: 0.04579763788654756 and parameters: {'observation_period_num': 21, 'train_rates': 0.8817152198504281, 'learning_rate': 0.00011557873583161966, 'batch_size': 27, 'step_size': 13, 'gamma': 0.8896611350608655}. Best is trial 33 with value: 0.04464865091500009.[0m
[32m[I 2025-01-11 10:57:08,910][0m Trial 38 finished with value: 0.0440133987557168 and parameters: {'observation_period_num': 23, 'train_rates': 0.7965433176018238, 'learning_rate': 0.00011616369376544115, 'batch_size': 20, 'step_size': 13, 'gamma': 0.8872492603253566}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:01:55,300][0m Trial 39 finished with value: 0.18046487406220124 and parameters: {'observation_period_num': 18, 'train_rates': 0.7806451339737286, 'learning_rate': 0.00010825267447400136, 'batch_size': 17, 'step_size': 14, 'gamma': 0.760177129885548}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:04:31,707][0m Trial 40 finished with value: 0.23104198904931939 and parameters: {'observation_period_num': 54, 'train_rates': 0.7388438316518982, 'learning_rate': 2.3297730673830872e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8892003883126582}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:06:36,968][0m Trial 41 finished with value: 0.05291040950048138 and parameters: {'observation_period_num': 25, 'train_rates': 0.8788221708159487, 'learning_rate': 6.0353032953186705e-05, 'batch_size': 43, 'step_size': 13, 'gamma': 0.8848996704183345}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:08:57,693][0m Trial 42 finished with value: 0.04530180699235417 and parameters: {'observation_period_num': 20, 'train_rates': 0.8027244970676147, 'learning_rate': 0.00013880832387992947, 'batch_size': 36, 'step_size': 12, 'gamma': 0.8586964045200224}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:10:56,969][0m Trial 43 finished with value: 0.16585182281736713 and parameters: {'observation_period_num': 20, 'train_rates': 0.706414784953989, 'learning_rate': 0.00012401973355097165, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8650252764734173}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:14:16,268][0m Trial 44 finished with value: 0.06460177650729167 and parameters: {'observation_period_num': 68, 'train_rates': 0.8087996004007172, 'learning_rate': 8.209629891181463e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.84745189064887}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:15:34,895][0m Trial 45 finished with value: 0.1389221051486872 and parameters: {'observation_period_num': 216, 'train_rates': 0.8492550017360709, 'learning_rate': 3.734382701863566e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9011086406510599}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:16:35,479][0m Trial 46 finished with value: 0.3650628285748618 and parameters: {'observation_period_num': 149, 'train_rates': 0.7928533727748046, 'learning_rate': 1.0322064299957254e-05, 'batch_size': 81, 'step_size': 14, 'gamma': 0.8510774949581543}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:18:06,673][0m Trial 47 finished with value: 0.1854389601035489 and parameters: {'observation_period_num': 18, 'train_rates': 0.7635241116287377, 'learning_rate': 0.00019193860960576474, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8304668289312811}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:20:39,744][0m Trial 48 finished with value: 0.07618412889974534 and parameters: {'observation_period_num': 49, 'train_rates': 0.8466146756346197, 'learning_rate': 2.3348613256564e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8691640151870964}. Best is trial 38 with value: 0.0440133987557168.[0m
[32m[I 2025-01-11 11:25:47,372][0m Trial 49 finished with value: 0.06320912582718807 and parameters: {'observation_period_num': 38, 'train_rates': 0.8029443377485146, 'learning_rate': 4.747850358126263e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7825612097026768}. Best is trial 38 with value: 0.0440133987557168.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-11 11:25:47,383][0m A new study created in memory with name: no-name-89ea77b1-37d6-4266-b48b-383e9a391a1e[0m
[32m[I 2025-01-11 11:26:14,474][0m Trial 0 finished with value: 1.3264838167275232 and parameters: {'observation_period_num': 88, 'train_rates': 0.8347996447577168, 'learning_rate': 2.3798832590158783e-06, 'batch_size': 211, 'step_size': 6, 'gamma': 0.9155115201624547}. Best is trial 0 with value: 1.3264838167275232.[0m
[32m[I 2025-01-11 11:26:39,435][0m Trial 1 finished with value: 1.14494632259881 and parameters: {'observation_period_num': 87, 'train_rates': 0.8207243368406922, 'learning_rate': 1.9533589844112265e-06, 'batch_size': 218, 'step_size': 8, 'gamma': 0.9241442298262795}. Best is trial 1 with value: 1.14494632259881.[0m
[32m[I 2025-01-11 11:27:57,639][0m Trial 2 finished with value: 0.096696514474309 and parameters: {'observation_period_num': 156, 'train_rates': 0.9506699936521656, 'learning_rate': 8.238135348054708e-05, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8862839154512627}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:28:45,815][0m Trial 3 finished with value: 0.7006676261645018 and parameters: {'observation_period_num': 74, 'train_rates': 0.6761227978480627, 'learning_rate': 2.4625643523000998e-06, 'batch_size': 97, 'step_size': 5, 'gamma': 0.9442328113132283}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:29:22,339][0m Trial 4 finished with value: 0.12148087900696379 and parameters: {'observation_period_num': 65, 'train_rates': 0.9088170467970159, 'learning_rate': 4.032818798525174e-05, 'batch_size': 158, 'step_size': 10, 'gamma': 0.9765512438293501}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:32:16,293][0m Trial 5 finished with value: 0.09788889955666105 and parameters: {'observation_period_num': 235, 'train_rates': 0.8482802571705835, 'learning_rate': 0.00039553751606213444, 'batch_size': 28, 'step_size': 7, 'gamma': 0.7818707461245464}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:33:00,894][0m Trial 6 finished with value: 0.593646622383335 and parameters: {'observation_period_num': 31, 'train_rates': 0.7328358525173234, 'learning_rate': 1.7521280054267385e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.7836848614369355}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:33:37,198][0m Trial 7 finished with value: 0.265910133511479 and parameters: {'observation_period_num': 91, 'train_rates': 0.7120470563541812, 'learning_rate': 0.0006720678119597458, 'batch_size': 135, 'step_size': 1, 'gamma': 0.9121427792845025}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:34:56,515][0m Trial 8 finished with value: 0.45867833378522294 and parameters: {'observation_period_num': 70, 'train_rates': 0.9203766840890535, 'learning_rate': 2.3069334035726697e-06, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9365800180287174}. Best is trial 2 with value: 0.096696514474309.[0m
[32m[I 2025-01-11 11:35:52,160][0m Trial 9 finished with value: 0.08761423836354496 and parameters: {'observation_period_num': 130, 'train_rates': 0.8439371506409732, 'learning_rate': 0.0005090036722755788, 'batch_size': 92, 'step_size': 15, 'gamma': 0.8830067329426419}. Best is trial 9 with value: 0.08761423836354496.[0m
[32m[I 2025-01-11 11:39:29,562][0m Trial 10 finished with value: 0.23228277039298434 and parameters: {'observation_period_num': 164, 'train_rates': 0.6086052630263454, 'learning_rate': 0.00013653949790807504, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8373671362569952}. Best is trial 9 with value: 0.08761423836354496.[0m
[32m[I 2025-01-11 11:40:57,460][0m Trial 11 finished with value: 0.09098313003778458 and parameters: {'observation_period_num': 153, 'train_rates': 0.9880455779971598, 'learning_rate': 0.0001365625277796281, 'batch_size': 65, 'step_size': 15, 'gamma': 0.8669360597397892}. Best is trial 9 with value: 0.08761423836354496.[0m
[32m[I 2025-01-11 11:42:29,834][0m Trial 12 finished with value: 0.08698900597225322 and parameters: {'observation_period_num': 197, 'train_rates': 0.9635086721983792, 'learning_rate': 0.0002875643704548606, 'batch_size': 60, 'step_size': 15, 'gamma': 0.8457738103514211}. Best is trial 12 with value: 0.08698900597225322.[0m
[32m[I 2025-01-11 11:43:08,559][0m Trial 13 finished with value: 0.08568188739297665 and parameters: {'observation_period_num': 212, 'train_rates': 0.8762615126616145, 'learning_rate': 0.0008567744532412578, 'batch_size': 138, 'step_size': 13, 'gamma': 0.8223015066410634}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:43:38,883][0m Trial 14 finished with value: 0.11180426421752229 and parameters: {'observation_period_num': 223, 'train_rates': 0.8823673169846162, 'learning_rate': 0.0008396691228264087, 'batch_size': 175, 'step_size': 12, 'gamma': 0.8194769388064572}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:44:00,180][0m Trial 15 finished with value: 0.30920857656153905 and parameters: {'observation_period_num': 197, 'train_rates': 0.772071714267145, 'learning_rate': 0.0002716101214212531, 'batch_size': 253, 'step_size': 13, 'gamma': 0.8221500647568982}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:44:42,239][0m Trial 16 finished with value: 0.48215121030807495 and parameters: {'observation_period_num': 193, 'train_rates': 0.9673685837203205, 'learning_rate': 1.1795716845592785e-05, 'batch_size': 134, 'step_size': 13, 'gamma': 0.7570561224399348}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:45:14,684][0m Trial 17 finished with value: 0.10214684908760005 and parameters: {'observation_period_num': 249, 'train_rates': 0.8928252375234367, 'learning_rate': 0.00020138228178844753, 'batch_size': 166, 'step_size': 13, 'gamma': 0.8535891269233745}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:47:35,596][0m Trial 18 finished with value: 0.18082327872329618 and parameters: {'observation_period_num': 199, 'train_rates': 0.941735561400789, 'learning_rate': 7.220345972896928e-05, 'batch_size': 38, 'step_size': 3, 'gamma': 0.8035278701590263}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:48:17,106][0m Trial 19 finished with value: 0.13241582522451983 and parameters: {'observation_period_num': 212, 'train_rates': 0.7962309313118003, 'learning_rate': 0.00033445773811789485, 'batch_size': 118, 'step_size': 14, 'gamma': 0.8448601309553354}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:48:45,401][0m Trial 20 finished with value: 0.5915875986653234 and parameters: {'observation_period_num': 176, 'train_rates': 0.8644001992109777, 'learning_rate': 6.050958878224123e-06, 'batch_size': 191, 'step_size': 10, 'gamma': 0.793162545213516}. Best is trial 13 with value: 0.08568188739297665.[0m
[32m[I 2025-01-11 11:49:46,597][0m Trial 21 finished with value: 0.0819779762532562 and parameters: {'observation_period_num': 114, 'train_rates': 0.9212644009415475, 'learning_rate': 0.0009817970964336748, 'batch_size': 91, 'step_size': 15, 'gamma': 0.8654576602181493}. Best is trial 21 with value: 0.0819779762532562.[0m
[32m[I 2025-01-11 11:51:18,327][0m Trial 22 finished with value: 0.06952859461307526 and parameters: {'observation_period_num': 128, 'train_rates': 0.9878516288205113, 'learning_rate': 0.0009495280701395545, 'batch_size': 63, 'step_size': 14, 'gamma': 0.8779527236108575}. Best is trial 22 with value: 0.06952859461307526.[0m
[32m[I 2025-01-11 11:52:18,743][0m Trial 23 finished with value: 0.0747826027170736 and parameters: {'observation_period_num': 119, 'train_rates': 0.9308698630779824, 'learning_rate': 0.0008987774932910075, 'batch_size': 92, 'step_size': 12, 'gamma': 0.891822798589191}. Best is trial 22 with value: 0.06952859461307526.[0m
[32m[I 2025-01-11 11:54:15,403][0m Trial 24 finished with value: 0.07033410654315408 and parameters: {'observation_period_num': 131, 'train_rates': 0.9250864261918231, 'learning_rate': 0.000881737282633126, 'batch_size': 46, 'step_size': 11, 'gamma': 0.886238571973226}. Best is trial 22 with value: 0.06952859461307526.[0m
[32m[I 2025-01-11 11:56:20,855][0m Trial 25 finished with value: 0.08942493796348572 and parameters: {'observation_period_num': 127, 'train_rates': 0.9848434567179759, 'learning_rate': 0.00047937237515085823, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8925531808463094}. Best is trial 22 with value: 0.06952859461307526.[0m
[32m[I 2025-01-11 11:58:12,119][0m Trial 26 finished with value: 0.6248894592365587 and parameters: {'observation_period_num': 113, 'train_rates': 0.9416558371679905, 'learning_rate': 1.0202326290256967e-06, 'batch_size': 50, 'step_size': 9, 'gamma': 0.9040094178539203}. Best is trial 22 with value: 0.06952859461307526.[0m
[32m[I 2025-01-11 11:59:25,765][0m Trial 27 finished with value: 0.05610135942697525 and parameters: {'observation_period_num': 37, 'train_rates': 0.9898481826922333, 'learning_rate': 0.00019087385366214213, 'batch_size': 81, 'step_size': 12, 'gamma': 0.9659832868102002}. Best is trial 27 with value: 0.05610135942697525.[0m
[32m[I 2025-01-11 12:04:48,052][0m Trial 28 finished with value: 0.03410117993397372 and parameters: {'observation_period_num': 8, 'train_rates': 0.9857823473501858, 'learning_rate': 0.00015337174501261206, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9851337089951198}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:09:38,701][0m Trial 29 finished with value: 0.03892554046416825 and parameters: {'observation_period_num': 8, 'train_rates': 0.9851314897156439, 'learning_rate': 5.034809692201317e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9849955992007466}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:14:23,693][0m Trial 30 finished with value: 0.038142110886318345 and parameters: {'observation_period_num': 7, 'train_rates': 0.9644382372064061, 'learning_rate': 4.45857591398779e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9859428914488971}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:19:41,543][0m Trial 31 finished with value: 0.04111238158863 and parameters: {'observation_period_num': 5, 'train_rates': 0.966448843740329, 'learning_rate': 3.632851743649523e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9845041085408182}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:25:12,719][0m Trial 32 finished with value: 0.03988396855337279 and parameters: {'observation_period_num': 5, 'train_rates': 0.9596642386136721, 'learning_rate': 3.330070740023575e-05, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9857148586692595}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:28:36,895][0m Trial 33 finished with value: 0.03986743914860266 and parameters: {'observation_period_num': 6, 'train_rates': 0.9544332623416504, 'learning_rate': 6.37834544709388e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9578948000686517}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:31:25,947][0m Trial 34 finished with value: 0.04563525065779686 and parameters: {'observation_period_num': 30, 'train_rates': 0.8975568066091256, 'learning_rate': 6.968600823341395e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9564718848074009}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:34:22,551][0m Trial 35 finished with value: 0.05739483400698631 and parameters: {'observation_period_num': 20, 'train_rates': 0.9472060143438833, 'learning_rate': 5.4810091462542515e-05, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9633784492288355}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:36:01,081][0m Trial 36 finished with value: 0.11438753564586843 and parameters: {'observation_period_num': 47, 'train_rates': 0.8067614408708348, 'learning_rate': 1.973535978309943e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.9408293843218664}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:38:56,713][0m Trial 37 finished with value: 0.05364017700776458 and parameters: {'observation_period_num': 48, 'train_rates': 0.9066013383090474, 'learning_rate': 0.0001038015279532729, 'batch_size': 31, 'step_size': 7, 'gamma': 0.97289641124026}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:42:37,491][0m Trial 38 finished with value: 0.19271465720273942 and parameters: {'observation_period_num': 17, 'train_rates': 0.7709343442091136, 'learning_rate': 2.4561562724580394e-05, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9526143187301557}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:44:58,209][0m Trial 39 finished with value: 0.1830445280734529 and parameters: {'observation_period_num': 55, 'train_rates': 0.9677634887307298, 'learning_rate': 1.0064922688451347e-05, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9271295089020218}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:46:18,451][0m Trial 40 finished with value: 0.1701670717121852 and parameters: {'observation_period_num': 17, 'train_rates': 0.6497964118815809, 'learning_rate': 4.500045164661714e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9897531261810781}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:50:19,301][0m Trial 41 finished with value: 0.04655693063423747 and parameters: {'observation_period_num': 5, 'train_rates': 0.9574348631899807, 'learning_rate': 3.0049035418120774e-05, 'batch_size': 24, 'step_size': 9, 'gamma': 0.979612228526488}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:55:02,402][0m Trial 42 finished with value: 0.04327415471727198 and parameters: {'observation_period_num': 25, 'train_rates': 0.9436103326395718, 'learning_rate': 9.715261009530652e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.971183175036809}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:57:38,885][0m Trial 43 finished with value: 0.06367771493944716 and parameters: {'observation_period_num': 38, 'train_rates': 0.96807149031164, 'learning_rate': 5.9160316422679985e-05, 'batch_size': 37, 'step_size': 11, 'gamma': 0.9511047228750503}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 12:58:58,230][0m Trial 44 finished with value: 0.1583604616924417 and parameters: {'observation_period_num': 86, 'train_rates': 0.9187644717737465, 'learning_rate': 1.3294705039461785e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.986175563814401}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 13:03:43,128][0m Trial 45 finished with value: 0.04094907647997785 and parameters: {'observation_period_num': 5, 'train_rates': 0.8274083275839844, 'learning_rate': 4.455776823268952e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9265978906745969}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 13:04:56,965][0m Trial 46 finished with value: 0.06838738173246384 and parameters: {'observation_period_num': 58, 'train_rates': 0.9763246253003129, 'learning_rate': 0.00012077234897844913, 'batch_size': 79, 'step_size': 10, 'gamma': 0.9620259572719975}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 13:08:08,950][0m Trial 47 finished with value: 0.05151050630956888 and parameters: {'observation_period_num': 14, 'train_rates': 0.9513065378474906, 'learning_rate': 2.7520763249320666e-05, 'batch_size': 30, 'step_size': 5, 'gamma': 0.9756667807980933}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 13:08:35,299][0m Trial 48 finished with value: 0.578646509420304 and parameters: {'observation_period_num': 39, 'train_rates': 0.8558135006284966, 'learning_rate': 7.244627686252917e-06, 'batch_size': 220, 'step_size': 14, 'gamma': 0.9434556408684766}. Best is trial 28 with value: 0.03410117993397372.[0m
[32m[I 2025-01-11 13:10:12,606][0m Trial 49 finished with value: 0.0839563063273467 and parameters: {'observation_period_num': 79, 'train_rates': 0.9330971447013988, 'learning_rate': 0.0001677056939943232, 'batch_size': 57, 'step_size': 12, 'gamma': 0.9896497400228098}. Best is trial 28 with value: 0.03410117993397372.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-11 13:10:12,616][0m A new study created in memory with name: no-name-4f279bc2-3e7f-4c41-855e-71bf0f410b44[0m
[32m[I 2025-01-11 13:10:39,332][0m Trial 0 finished with value: 1.3177213395966423 and parameters: {'observation_period_num': 191, 'train_rates': 0.8371848632925891, 'learning_rate': 1.0544764696024138e-06, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9137655400596187}. Best is trial 0 with value: 1.3177213395966423.[0m
[32m[I 2025-01-11 13:11:09,951][0m Trial 1 finished with value: 0.1344701851953517 and parameters: {'observation_period_num': 73, 'train_rates': 0.8419414601082192, 'learning_rate': 8.365208325233723e-05, 'batch_size': 183, 'step_size': 7, 'gamma': 0.8454794589084358}. Best is trial 1 with value: 0.1344701851953517.[0m
[32m[I 2025-01-11 13:13:03,829][0m Trial 2 finished with value: 0.2955542167609042 and parameters: {'observation_period_num': 242, 'train_rates': 0.6530705094418919, 'learning_rate': 0.00024346377182686977, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8200580192864428}. Best is trial 1 with value: 0.1344701851953517.[0m
[32m[I 2025-01-11 13:13:29,972][0m Trial 3 finished with value: 1.4312604665756226 and parameters: {'observation_period_num': 99, 'train_rates': 0.9436472102458091, 'learning_rate': 1.6508519948914434e-06, 'batch_size': 238, 'step_size': 4, 'gamma': 0.9198915039505896}. Best is trial 1 with value: 0.1344701851953517.[0m
[32m[I 2025-01-11 13:13:52,316][0m Trial 4 finished with value: 0.33234124925542385 and parameters: {'observation_period_num': 160, 'train_rates': 0.8316407753740865, 'learning_rate': 0.0001838092360846226, 'batch_size': 249, 'step_size': 2, 'gamma': 0.7941499194469566}. Best is trial 1 with value: 0.1344701851953517.[0m
[32m[I 2025-01-11 13:14:28,456][0m Trial 5 finished with value: 0.08590847478792218 and parameters: {'observation_period_num': 155, 'train_rates': 0.9277654317894395, 'learning_rate': 0.00021002047100658345, 'batch_size': 157, 'step_size': 9, 'gamma': 0.835276102395279}. Best is trial 5 with value: 0.08590847478792218.[0m
[32m[I 2025-01-11 13:15:08,849][0m Trial 6 finished with value: 0.44498511426937504 and parameters: {'observation_period_num': 66, 'train_rates': 0.7212945351249417, 'learning_rate': 1.3801170189342227e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9682343329313745}. Best is trial 5 with value: 0.08590847478792218.[0m
[32m[I 2025-01-11 13:15:31,175][0m Trial 7 finished with value: 1.0116313898851022 and parameters: {'observation_period_num': 93, 'train_rates': 0.6086193694243253, 'learning_rate': 2.8677954155132294e-06, 'batch_size': 207, 'step_size': 11, 'gamma': 0.7729989799770908}. Best is trial 5 with value: 0.08590847478792218.[0m
[32m[I 2025-01-11 13:19:50,806][0m Trial 8 finished with value: 0.3325794284204835 and parameters: {'observation_period_num': 149, 'train_rates': 0.7797834711435716, 'learning_rate': 4.2059482271295575e-06, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8674312804579223}. Best is trial 5 with value: 0.08590847478792218.[0m
[32m[I 2025-01-11 13:21:50,352][0m Trial 9 finished with value: 0.14751005719919674 and parameters: {'observation_period_num': 72, 'train_rates': 0.7883629860464979, 'learning_rate': 1.645158683525799e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.8825979308255572}. Best is trial 5 with value: 0.08590847478792218.[0m
[32m[I 2025-01-11 13:22:40,088][0m Trial 10 finished with value: 0.044033024460077286 and parameters: {'observation_period_num': 29, 'train_rates': 0.9872365274360003, 'learning_rate': 0.0008256828216641147, 'batch_size': 123, 'step_size': 13, 'gamma': 0.7592496232586541}. Best is trial 10 with value: 0.044033024460077286.[0m
[32m[I 2025-01-11 13:23:29,759][0m Trial 11 finished with value: 0.033611495047807693 and parameters: {'observation_period_num': 12, 'train_rates': 0.9788174254718891, 'learning_rate': 0.000958763199771286, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7506729872334668}. Best is trial 11 with value: 0.033611495047807693.[0m
[32m[I 2025-01-11 13:24:31,429][0m Trial 12 finished with value: 0.03712201863527298 and parameters: {'observation_period_num': 12, 'train_rates': 0.9886572111190468, 'learning_rate': 0.0009311163497201493, 'batch_size': 100, 'step_size': 15, 'gamma': 0.7558663829058465}. Best is trial 11 with value: 0.033611495047807693.[0m
[32m[I 2025-01-11 13:25:41,648][0m Trial 13 finished with value: 0.03063520623018613 and parameters: {'observation_period_num': 11, 'train_rates': 0.924623602059246, 'learning_rate': 0.0007972264601018071, 'batch_size': 82, 'step_size': 15, 'gamma': 0.7508138514663495}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:26:56,912][0m Trial 14 finished with value: 0.04923655199356206 and parameters: {'observation_period_num': 37, 'train_rates': 0.9103747026232522, 'learning_rate': 0.0005732787113843045, 'batch_size': 75, 'step_size': 12, 'gamma': 0.7975799625452776}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:28:12,592][0m Trial 15 finished with value: 0.0646997506127638 and parameters: {'observation_period_num': 39, 'train_rates': 0.8893031955235917, 'learning_rate': 6.497422602205893e-05, 'batch_size': 73, 'step_size': 13, 'gamma': 0.7864814233009852}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:28:48,908][0m Trial 16 finished with value: 0.03785420020812013 and parameters: {'observation_period_num': 5, 'train_rates': 0.8794105026915557, 'learning_rate': 0.00045226764476393973, 'batch_size': 156, 'step_size': 14, 'gamma': 0.7536897953930782}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:29:55,502][0m Trial 17 finished with value: 0.08481237212665209 and parameters: {'observation_period_num': 55, 'train_rates': 0.9548306950446332, 'learning_rate': 6.158846449398434e-05, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8087042051281398}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:30:39,585][0m Trial 18 finished with value: 0.2543927773550597 and parameters: {'observation_period_num': 111, 'train_rates': 0.7404697417332934, 'learning_rate': 0.00033194063013486765, 'batch_size': 107, 'step_size': 5, 'gamma': 0.9579684197161308}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:31:15,787][0m Trial 19 finished with value: 0.10727935982021418 and parameters: {'observation_period_num': 249, 'train_rates': 0.8779721804913353, 'learning_rate': 0.0001450063871233517, 'batch_size': 147, 'step_size': 11, 'gamma': 0.8587214576573985}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:32:53,872][0m Trial 20 finished with value: 0.19805049111968592 and parameters: {'observation_period_num': 204, 'train_rates': 0.9588325476799522, 'learning_rate': 2.1688218365440337e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.7806492371970509}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:33:55,100][0m Trial 21 finished with value: 0.037042804062366486 and parameters: {'observation_period_num': 9, 'train_rates': 0.9785363014346699, 'learning_rate': 0.000880400155497319, 'batch_size': 99, 'step_size': 15, 'gamma': 0.7518952702131941}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:34:45,417][0m Trial 22 finished with value: 0.04113800451159477 and parameters: {'observation_period_num': 5, 'train_rates': 0.9747267195431853, 'learning_rate': 0.0005103547912269444, 'batch_size': 120, 'step_size': 13, 'gamma': 0.7502419278279622}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:35:46,346][0m Trial 23 finished with value: 0.04387674352273028 and parameters: {'observation_period_num': 27, 'train_rates': 0.919746028673037, 'learning_rate': 0.0009404202499472333, 'batch_size': 93, 'step_size': 14, 'gamma': 0.7703356901952486}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:37:14,974][0m Trial 24 finished with value: 0.053056190588644574 and parameters: {'observation_period_num': 50, 'train_rates': 0.9423121730814353, 'learning_rate': 0.00037904459810155015, 'batch_size': 64, 'step_size': 14, 'gamma': 0.806708341232008}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:37:57,403][0m Trial 25 finished with value: 0.05934670261182021 and parameters: {'observation_period_num': 23, 'train_rates': 0.9022261497013175, 'learning_rate': 9.36027023237172e-05, 'batch_size': 137, 'step_size': 15, 'gamma': 0.8260183375266622}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:38:33,611][0m Trial 26 finished with value: 0.6361645460128784 and parameters: {'observation_period_num': 46, 'train_rates': 0.9612360785769678, 'learning_rate': 7.981416358529389e-06, 'batch_size': 172, 'step_size': 12, 'gamma': 0.7778152014937298}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:39:22,748][0m Trial 27 finished with value: 0.07345864634721651 and parameters: {'observation_period_num': 21, 'train_rates': 0.851354704512325, 'learning_rate': 4.210648764891183e-05, 'batch_size': 110, 'step_size': 12, 'gamma': 0.8873860091690965}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:40:27,042][0m Trial 28 finished with value: 0.09104409706934788 and parameters: {'observation_period_num': 129, 'train_rates': 0.9261745208443535, 'learning_rate': 0.00063166569250979, 'batch_size': 86, 'step_size': 14, 'gamma': 0.7736736563076442}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:41:06,361][0m Trial 29 finished with value: 0.08795288407593249 and parameters: {'observation_period_num': 87, 'train_rates': 0.8111835170966771, 'learning_rate': 0.000129157420119227, 'batch_size': 133, 'step_size': 10, 'gamma': 0.9059151757703849}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:41:32,947][0m Trial 30 finished with value: 0.12067987708771816 and parameters: {'observation_period_num': 188, 'train_rates': 0.8677471048735915, 'learning_rate': 0.00029419646457995517, 'batch_size': 208, 'step_size': 6, 'gamma': 0.8096727740488079}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:42:32,594][0m Trial 31 finished with value: 0.0405639223754406 and parameters: {'observation_period_num': 7, 'train_rates': 0.98667309647204, 'learning_rate': 0.0009202683195281301, 'batch_size': 103, 'step_size': 15, 'gamma': 0.7583181962143767}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:43:34,033][0m Trial 32 finished with value: 0.03877260163426399 and parameters: {'observation_period_num': 14, 'train_rates': 0.9866335880506294, 'learning_rate': 0.0006379969254498161, 'batch_size': 98, 'step_size': 15, 'gamma': 0.764579518851971}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:45:13,837][0m Trial 33 finished with value: 0.06207681154814836 and parameters: {'observation_period_num': 59, 'train_rates': 0.9630603678815957, 'learning_rate': 0.000954671133431917, 'batch_size': 57, 'step_size': 13, 'gamma': 0.751660055390246}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:46:24,552][0m Trial 34 finished with value: 0.051754328876552055 and parameters: {'observation_period_num': 35, 'train_rates': 0.9438547939044203, 'learning_rate': 0.0003202986793283725, 'batch_size': 81, 'step_size': 14, 'gamma': 0.7836260480376153}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:48:23,554][0m Trial 35 finished with value: 0.03241679631173611 and parameters: {'observation_period_num': 20, 'train_rates': 0.9347750691220725, 'learning_rate': 0.0004843972410378243, 'batch_size': 48, 'step_size': 2, 'gamma': 0.9407398164923755}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:51:21,425][0m Trial 36 finished with value: 0.0667420065576079 and parameters: {'observation_period_num': 85, 'train_rates': 0.9311314861605126, 'learning_rate': 0.00020859007261316335, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9405071538958648}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:53:30,867][0m Trial 37 finished with value: 0.0352237999620961 and parameters: {'observation_period_num': 21, 'train_rates': 0.9020526349427593, 'learning_rate': 0.0004698697341411388, 'batch_size': 42, 'step_size': 2, 'gamma': 0.9210379543322355}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 13:55:32,299][0m Trial 38 finished with value: 0.049374313295801296 and parameters: {'observation_period_num': 45, 'train_rates': 0.8541283423633234, 'learning_rate': 0.0004103911640453395, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9366416354953354}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:00:34,625][0m Trial 39 finished with value: 0.13431911673245392 and parameters: {'observation_period_num': 110, 'train_rates': 0.8270067866144031, 'learning_rate': 0.00022184087092880968, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9860900101091644}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:03:29,493][0m Trial 40 finished with value: 0.06816280977497893 and parameters: {'observation_period_num': 68, 'train_rates': 0.9040772584879528, 'learning_rate': 0.0005982607576390106, 'batch_size': 31, 'step_size': 2, 'gamma': 0.9134595514306726}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:05:16,278][0m Trial 41 finished with value: 0.04674405129305247 and parameters: {'observation_period_num': 21, 'train_rates': 0.9407194817203244, 'learning_rate': 0.0006600991206545065, 'batch_size': 54, 'step_size': 1, 'gamma': 0.9304715873191295}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:06:38,095][0m Trial 42 finished with value: 0.03737962590530515 and parameters: {'observation_period_num': 20, 'train_rates': 0.8911273185941658, 'learning_rate': 0.00047219508896388873, 'batch_size': 68, 'step_size': 4, 'gamma': 0.9581996535709737}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:08:15,075][0m Trial 43 finished with value: 0.15524243864984738 and parameters: {'observation_period_num': 29, 'train_rates': 0.6694786176987491, 'learning_rate': 0.0007291405327152668, 'batch_size': 46, 'step_size': 7, 'gamma': 0.8987027730032771}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:09:06,536][0m Trial 44 finished with value: 0.10346865653991699 and parameters: {'observation_period_num': 37, 'train_rates': 0.9699804978069584, 'learning_rate': 0.00024080011863816824, 'batch_size': 117, 'step_size': 2, 'gamma': 0.8512631179333736}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:12:22,283][0m Trial 45 finished with value: 0.9454476550395386 and parameters: {'observation_period_num': 15, 'train_rates': 0.9127677267688253, 'learning_rate': 1.296904338385343e-06, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9221564626053064}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:13:50,782][0m Trial 46 finished with value: 0.0977145479807001 and parameters: {'observation_period_num': 61, 'train_rates': 0.94787252798295, 'learning_rate': 0.0001516196919241327, 'batch_size': 65, 'step_size': 6, 'gamma': 0.9469137199734687}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:15:02,514][0m Trial 47 finished with value: 0.044803051031985376 and parameters: {'observation_period_num': 5, 'train_rates': 0.9281967295749948, 'learning_rate': 0.00042847515148126833, 'batch_size': 79, 'step_size': 3, 'gamma': 0.8835441494678586}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:17:04,336][0m Trial 48 finished with value: 0.0485079862177372 and parameters: {'observation_period_num': 33, 'train_rates': 0.972709164827019, 'learning_rate': 0.0003036509074934191, 'batch_size': 48, 'step_size': 9, 'gamma': 0.9743801539107588}. Best is trial 13 with value: 0.03063520623018613.[0m
[32m[I 2025-01-11 14:17:42,705][0m Trial 49 finished with value: 0.21211142719842263 and parameters: {'observation_period_num': 47, 'train_rates': 0.7655517820794904, 'learning_rate': 0.0007935763782948885, 'batch_size': 132, 'step_size': 5, 'gamma': 0.7646336901265707}. Best is trial 13 with value: 0.03063520623018613.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-11 14:17:42,715][0m A new study created in memory with name: no-name-f0076831-e6fd-41bd-99ef-8f1947951448[0m
[32m[I 2025-01-11 14:18:02,391][0m Trial 0 finished with value: 0.7322088522884679 and parameters: {'observation_period_num': 75, 'train_rates': 0.6232897441880808, 'learning_rate': 1.0890931766063773e-05, 'batch_size': 226, 'step_size': 8, 'gamma': 0.8183932965540317}. Best is trial 0 with value: 0.7322088522884679.[0m
[32m[I 2025-01-11 14:18:26,580][0m Trial 1 finished with value: 0.27935256867940667 and parameters: {'observation_period_num': 141, 'train_rates': 0.662952226351368, 'learning_rate': 0.00014774939982049125, 'batch_size': 189, 'step_size': 10, 'gamma': 0.7732009930826039}. Best is trial 1 with value: 0.27935256867940667.[0m
[32m[I 2025-01-11 14:19:01,827][0m Trial 2 finished with value: 0.7493763478010292 and parameters: {'observation_period_num': 13, 'train_rates': 0.7972966031525858, 'learning_rate': 4.724787809765939e-06, 'batch_size': 152, 'step_size': 9, 'gamma': 0.7945688576868245}. Best is trial 1 with value: 0.27935256867940667.[0m
[32m[I 2025-01-11 14:19:50,178][0m Trial 3 finished with value: 0.20278462552750448 and parameters: {'observation_period_num': 18, 'train_rates': 0.6514852838315709, 'learning_rate': 2.440608051487295e-05, 'batch_size': 95, 'step_size': 14, 'gamma': 0.9356562158773245}. Best is trial 3 with value: 0.20278462552750448.[0m
[32m[I 2025-01-11 14:20:20,161][0m Trial 4 finished with value: 0.05990329384803772 and parameters: {'observation_period_num': 28, 'train_rates': 0.9875675972773355, 'learning_rate': 0.0008905405079343694, 'batch_size': 211, 'step_size': 7, 'gamma': 0.9724827267201086}. Best is trial 4 with value: 0.05990329384803772.[0m
[32m[I 2025-01-11 14:20:43,170][0m Trial 5 finished with value: 0.10097893637915452 and parameters: {'observation_period_num': 53, 'train_rates': 0.7973217631729732, 'learning_rate': 0.0003064072252015804, 'batch_size': 245, 'step_size': 2, 'gamma': 0.9482275084105691}. Best is trial 4 with value: 0.05990329384803772.[0m
[32m[I 2025-01-11 14:21:10,463][0m Trial 6 finished with value: 0.15232260715344856 and parameters: {'observation_period_num': 27, 'train_rates': 0.6035947926622705, 'learning_rate': 0.0007987542688834086, 'batch_size': 164, 'step_size': 13, 'gamma': 0.9857163671207582}. Best is trial 4 with value: 0.05990329384803772.[0m
[32m[I 2025-01-11 14:21:48,573][0m Trial 7 finished with value: 0.2052759702321137 and parameters: {'observation_period_num': 86, 'train_rates': 0.7380541666917729, 'learning_rate': 0.0008058740792999151, 'batch_size': 131, 'step_size': 10, 'gamma': 0.810886565127799}. Best is trial 4 with value: 0.05990329384803772.[0m
[32m[I 2025-01-11 14:25:16,380][0m Trial 8 finished with value: 0.07043839372166498 and parameters: {'observation_period_num': 50, 'train_rates': 0.9125361533575338, 'learning_rate': 1.7169233318184042e-05, 'batch_size': 26, 'step_size': 5, 'gamma': 0.9524156579672072}. Best is trial 4 with value: 0.05990329384803772.[0m
[32m[I 2025-01-11 14:25:44,834][0m Trial 9 finished with value: 0.039087015514572464 and parameters: {'observation_period_num': 10, 'train_rates': 0.9226494621618742, 'learning_rate': 0.0005409811229430167, 'batch_size': 217, 'step_size': 7, 'gamma': 0.949060125001567}. Best is trial 9 with value: 0.039087015514572464.[0m
Early stopping at epoch 91
[32m[I 2025-01-11 14:26:39,313][0m Trial 10 finished with value: 1.033035400095365 and parameters: {'observation_period_num': 218, 'train_rates': 0.8897592311062488, 'learning_rate': 1.5879803766994839e-06, 'batch_size': 89, 'step_size': 1, 'gamma': 0.8909560407632322}. Best is trial 9 with value: 0.039087015514572464.[0m
[32m[I 2025-01-11 14:27:09,579][0m Trial 11 finished with value: 0.18754850327968597 and parameters: {'observation_period_num': 130, 'train_rates': 0.9855416735333752, 'learning_rate': 0.00011636576238127566, 'batch_size': 210, 'step_size': 5, 'gamma': 0.8855581325838288}. Best is trial 9 with value: 0.039087015514572464.[0m
[32m[I 2025-01-11 14:27:34,848][0m Trial 12 finished with value: 0.1628447026014328 and parameters: {'observation_period_num': 175, 'train_rates': 0.9887071026866072, 'learning_rate': 0.00010235017152271126, 'batch_size': 247, 'step_size': 6, 'gamma': 0.9841726803901562}. Best is trial 9 with value: 0.039087015514572464.[0m
[32m[I 2025-01-11 14:28:04,709][0m Trial 13 finished with value: 0.08383743999932046 and parameters: {'observation_period_num': 98, 'train_rates': 0.9070976283198036, 'learning_rate': 0.0003376565149598333, 'batch_size': 193, 'step_size': 7, 'gamma': 0.9173948008757246}. Best is trial 9 with value: 0.039087015514572464.[0m
[32m[I 2025-01-11 14:28:35,999][0m Trial 14 finished with value: 0.3594046589206247 and parameters: {'observation_period_num': 44, 'train_rates': 0.8541420448639213, 'learning_rate': 5.12119280031837e-05, 'batch_size': 186, 'step_size': 3, 'gamma': 0.8531690274387417}. Best is trial 9 with value: 0.039087015514572464.[0m
[32m[I 2025-01-11 14:29:23,834][0m Trial 15 finished with value: 0.03561515996561331 and parameters: {'observation_period_num': 6, 'train_rates': 0.9367616675123246, 'learning_rate': 0.0009329230335260097, 'batch_size': 121, 'step_size': 12, 'gamma': 0.919139629548181}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:30:09,119][0m Trial 16 finished with value: 0.07457770377789673 and parameters: {'observation_period_num': 172, 'train_rates': 0.8496890332670046, 'learning_rate': 0.00034984053561582537, 'batch_size': 113, 'step_size': 12, 'gamma': 0.9095261204046748}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:31:43,013][0m Trial 17 finished with value: 0.054277359655028896 and parameters: {'observation_period_num': 5, 'train_rates': 0.9355114209133719, 'learning_rate': 5.0249475687431935e-05, 'batch_size': 60, 'step_size': 15, 'gamma': 0.8748152568532013}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:32:23,232][0m Trial 18 finished with value: 0.06478234612477754 and parameters: {'observation_period_num': 69, 'train_rates': 0.9421722964567427, 'learning_rate': 0.00033636497922403565, 'batch_size': 146, 'step_size': 11, 'gamma': 0.8453970287867942}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:33:32,005][0m Trial 19 finished with value: 0.21631204471188156 and parameters: {'observation_period_num': 106, 'train_rates': 0.7332070841279827, 'learning_rate': 0.00020152559265113347, 'batch_size': 69, 'step_size': 12, 'gamma': 0.9175883282101426}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:34:18,381][0m Trial 20 finished with value: 0.08588044100428281 and parameters: {'observation_period_num': 223, 'train_rates': 0.9467959926443144, 'learning_rate': 0.0005514689045141792, 'batch_size': 118, 'step_size': 4, 'gamma': 0.9630947094871395}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:36:08,493][0m Trial 21 finished with value: 0.040769497861704196 and parameters: {'observation_period_num': 9, 'train_rates': 0.8589607569334254, 'learning_rate': 5.589969255786186e-05, 'batch_size': 48, 'step_size': 15, 'gamma': 0.8732787510542018}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:39:59,822][0m Trial 22 finished with value: 0.10287019587225384 and parameters: {'observation_period_num': 39, 'train_rates': 0.8456012302026368, 'learning_rate': 6.432384948846191e-06, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9339732488042252}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:41:46,126][0m Trial 23 finished with value: 0.04687487800452537 and parameters: {'observation_period_num': 6, 'train_rates': 0.8781187790830601, 'learning_rate': 5.7146771411364235e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9046031986923744}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:42:18,764][0m Trial 24 finished with value: 0.8075470004904542 and parameters: {'observation_period_num': 62, 'train_rates': 0.8336473622200353, 'learning_rate': 1.0423781354603156e-06, 'batch_size': 168, 'step_size': 14, 'gamma': 0.8588104660737668}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:43:19,168][0m Trial 25 finished with value: 0.0848094387849172 and parameters: {'observation_period_num': 245, 'train_rates': 0.9447773762838549, 'learning_rate': 0.000463893245381916, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8422309544937672}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:45:36,804][0m Trial 26 finished with value: 0.040896732598543165 and parameters: {'observation_period_num': 34, 'train_rates': 0.8715195222488008, 'learning_rate': 0.0002052329069166934, 'batch_size': 38, 'step_size': 9, 'gamma': 0.9302570346805226}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:46:35,714][0m Trial 27 finished with value: 0.2560755369067192 and parameters: {'observation_period_num': 107, 'train_rates': 0.7628219412658755, 'learning_rate': 6.594486381781058e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8905517018624166}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:47:23,192][0m Trial 28 finished with value: 0.03871353258230597 and parameters: {'observation_period_num': 23, 'train_rates': 0.8264217909498042, 'learning_rate': 0.0005815780656388649, 'batch_size': 113, 'step_size': 13, 'gamma': 0.8636660345420762}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:48:09,051][0m Trial 29 finished with value: 0.06577263548547695 and parameters: {'observation_period_num': 87, 'train_rates': 0.8155011606769992, 'learning_rate': 0.000595574007986403, 'batch_size': 114, 'step_size': 9, 'gamma': 0.8325919702763281}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:48:35,437][0m Trial 30 finished with value: 0.11646362923850578 and parameters: {'observation_period_num': 73, 'train_rates': 0.9087215766372998, 'learning_rate': 0.00016967355351428311, 'batch_size': 224, 'step_size': 8, 'gamma': 0.8224440692250462}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:49:15,275][0m Trial 31 finished with value: 0.17118581725950266 and parameters: {'observation_period_num': 23, 'train_rates': 0.7642278002174328, 'learning_rate': 0.0009301697350796166, 'batch_size': 131, 'step_size': 13, 'gamma': 0.8648799215422893}. Best is trial 15 with value: 0.03561515996561331.[0m
[32m[I 2025-01-11 14:50:07,205][0m Trial 32 finished with value: 0.03478415490297431 and parameters: {'observation_period_num': 6, 'train_rates': 0.8208434604536282, 'learning_rate': 0.0005375314758159338, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8729664752799243}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:50:51,980][0m Trial 33 finished with value: 0.16793090718116346 and parameters: {'observation_period_num': 33, 'train_rates': 0.695604811814655, 'learning_rate': 0.0005053607643105121, 'batch_size': 105, 'step_size': 11, 'gamma': 0.7994626742824876}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:51:27,895][0m Trial 34 finished with value: 0.051458981462887354 and parameters: {'observation_period_num': 18, 'train_rates': 0.8213078847592186, 'learning_rate': 0.00023037740214294956, 'batch_size': 143, 'step_size': 12, 'gamma': 0.8997894210059435}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:52:06,213][0m Trial 35 finished with value: 0.1116059422492981 and parameters: {'observation_period_num': 56, 'train_rates': 0.9615223458817144, 'learning_rate': 0.000593395873133428, 'batch_size': 161, 'step_size': 14, 'gamma': 0.9561904159913001}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:52:48,505][0m Trial 36 finished with value: 0.24732285408118299 and parameters: {'observation_period_num': 25, 'train_rates': 0.7762170510792299, 'learning_rate': 0.00010672744753924107, 'batch_size': 124, 'step_size': 10, 'gamma': 0.7578377560073927}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:53:59,785][0m Trial 37 finished with value: 0.11428715732423651 and parameters: {'observation_period_num': 155, 'train_rates': 0.893483335102439, 'learning_rate': 0.0009846386964019857, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9402571716033499}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:54:49,954][0m Trial 38 finished with value: 0.05241735841248937 and parameters: {'observation_period_num': 43, 'train_rates': 0.8011098868780886, 'learning_rate': 0.00026301451857463527, 'batch_size': 105, 'step_size': 13, 'gamma': 0.9242255603448778}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:55:18,211][0m Trial 39 finished with value: 0.20471668844543628 and parameters: {'observation_period_num': 15, 'train_rates': 0.6811964748536522, 'learning_rate': 3.0104321051313974e-05, 'batch_size': 180, 'step_size': 9, 'gamma': 0.9669352282801261}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:56:15,140][0m Trial 40 finished with value: 0.04596551679043327 and parameters: {'observation_period_num': 20, 'train_rates': 0.9194034109604493, 'learning_rate': 0.00043627480351824446, 'batch_size': 100, 'step_size': 14, 'gamma': 0.8798368635103226}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:58:05,722][0m Trial 41 finished with value: 0.07070000501942701 and parameters: {'observation_period_num': 7, 'train_rates': 0.8785391454829704, 'learning_rate': 1.1498679915612509e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8680650422446925}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:58:46,330][0m Trial 42 finished with value: 0.03962764345884028 and parameters: {'observation_period_num': 6, 'train_rates': 0.8630845338224463, 'learning_rate': 0.0006628073692127387, 'batch_size': 137, 'step_size': 14, 'gamma': 0.893888368295191}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:59:10,118][0m Trial 43 finished with value: 0.05452273041009903 and parameters: {'observation_period_num': 31, 'train_rates': 0.9628674427943368, 'learning_rate': 0.0007528167144006582, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8967713694603364}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 14:59:49,313][0m Trial 44 finished with value: 0.06133187562227249 and parameters: {'observation_period_num': 50, 'train_rates': 0.8213804937906808, 'learning_rate': 0.000703351094459404, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9471163147518648}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 15:00:25,707][0m Trial 45 finished with value: 0.03638129261807321 and parameters: {'observation_period_num': 18, 'train_rates': 0.8934421082532255, 'learning_rate': 0.0009982768336968104, 'batch_size': 157, 'step_size': 12, 'gamma': 0.9124413087898688}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 15:01:03,853][0m Trial 46 finished with value: 0.04514886786090842 and parameters: {'observation_period_num': 24, 'train_rates': 0.9247948249307388, 'learning_rate': 0.00040232672782274456, 'batch_size': 157, 'step_size': 12, 'gamma': 0.9158557240032027}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 15:01:34,223][0m Trial 47 finished with value: 0.06594850270512005 and parameters: {'observation_period_num': 36, 'train_rates': 0.8928167562554001, 'learning_rate': 0.00026355775056552844, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9786934391867318}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 15:02:04,328][0m Trial 48 finished with value: 0.0709199458360672 and parameters: {'observation_period_num': 62, 'train_rates': 0.9684344316932428, 'learning_rate': 0.0009800609602051116, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8506927914979737}. Best is trial 32 with value: 0.03478415490297431.[0m
[32m[I 2025-01-11 15:02:35,652][0m Trial 49 finished with value: 0.23735697724134008 and parameters: {'observation_period_num': 46, 'train_rates': 0.7791777216855871, 'learning_rate': 0.0001445417188007285, 'batch_size': 173, 'step_size': 11, 'gamma': 0.9415194849544487}. Best is trial 32 with value: 0.03478415490297431.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-11 15:02:35,662][0m A new study created in memory with name: no-name-f5266031-6c55-4b32-9f30-7e2708d3e1f5[0m
[32m[I 2025-01-11 15:04:05,249][0m Trial 0 finished with value: 0.10392056148967996 and parameters: {'observation_period_num': 146, 'train_rates': 0.8925471926247095, 'learning_rate': 0.00025008592031852396, 'batch_size': 59, 'step_size': 14, 'gamma': 0.878133754280528}. Best is trial 0 with value: 0.10392056148967996.[0m
[32m[I 2025-01-11 15:04:24,936][0m Trial 1 finished with value: 0.699625079796232 and parameters: {'observation_period_num': 88, 'train_rates': 0.6457797301365625, 'learning_rate': 4.9888614769441524e-05, 'batch_size': 238, 'step_size': 2, 'gamma': 0.8693707538228292}. Best is trial 0 with value: 0.10392056148967996.[0m
[32m[I 2025-01-11 15:04:54,321][0m Trial 2 finished with value: 0.28855969286397143 and parameters: {'observation_period_num': 113, 'train_rates': 0.7669608381064071, 'learning_rate': 0.00019559967538111802, 'batch_size': 179, 'step_size': 14, 'gamma': 0.7873226508517833}. Best is trial 0 with value: 0.10392056148967996.[0m
[32m[I 2025-01-11 15:05:27,381][0m Trial 3 finished with value: 0.6516855877879971 and parameters: {'observation_period_num': 164, 'train_rates': 0.8147351516387406, 'learning_rate': 7.639256355592764e-06, 'batch_size': 159, 'step_size': 8, 'gamma': 0.7539743427906762}. Best is trial 0 with value: 0.10392056148967996.[0m
[32m[I 2025-01-11 15:06:59,634][0m Trial 4 finished with value: 0.09003892675676245 and parameters: {'observation_period_num': 162, 'train_rates': 0.8654426346520575, 'learning_rate': 0.0005344520324895005, 'batch_size': 56, 'step_size': 7, 'gamma': 0.8689341487393368}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:07:36,753][0m Trial 5 finished with value: 0.32415472188042205 and parameters: {'observation_period_num': 243, 'train_rates': 0.7718617677501318, 'learning_rate': 3.535205715354664e-05, 'batch_size': 129, 'step_size': 8, 'gamma': 0.9242311201772955}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:07:58,667][0m Trial 6 finished with value: 0.6852979833753176 and parameters: {'observation_period_num': 130, 'train_rates': 0.6560926791013537, 'learning_rate': 1.6777576657134636e-05, 'batch_size': 206, 'step_size': 13, 'gamma': 0.9486044341182235}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:09:31,228][0m Trial 7 finished with value: 0.14087747206627313 and parameters: {'observation_period_num': 167, 'train_rates': 0.8943444775300926, 'learning_rate': 2.6762216924050797e-05, 'batch_size': 57, 'step_size': 9, 'gamma': 0.9174554163341107}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:10:15,180][0m Trial 8 finished with value: 0.371725527092445 and parameters: {'observation_period_num': 100, 'train_rates': 0.6606061490024075, 'learning_rate': 2.7363171805736035e-05, 'batch_size': 100, 'step_size': 6, 'gamma': 0.986070555536309}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:10:41,529][0m Trial 9 finished with value: 1.0191537099182901 and parameters: {'observation_period_num': 84, 'train_rates': 0.6489288501913439, 'learning_rate': 1.907260328257812e-06, 'batch_size': 172, 'step_size': 1, 'gamma': 0.9748581148943488}. Best is trial 4 with value: 0.09003892675676245.[0m
[32m[I 2025-01-11 15:15:26,285][0m Trial 10 finished with value: 0.02822481076184072 and parameters: {'observation_period_num': 10, 'train_rates': 0.9614250769842756, 'learning_rate': 0.0007497827900101974, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8289382207436825}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:21:23,227][0m Trial 11 finished with value: 0.031473426638465175 and parameters: {'observation_period_num': 24, 'train_rates': 0.9743754654262586, 'learning_rate': 0.0008451489323482731, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8307712140346432}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:25:05,540][0m Trial 12 finished with value: 0.03245364135456464 and parameters: {'observation_period_num': 19, 'train_rates': 0.9785737584321411, 'learning_rate': 0.0009768360723641238, 'batch_size': 26, 'step_size': 4, 'gamma': 0.8156095102071863}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:31:03,106][0m Trial 13 finished with value: 0.046206266778271374 and parameters: {'observation_period_num': 5, 'train_rates': 0.9862518688384218, 'learning_rate': 0.00017095784281745361, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8256629789580642}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:31:57,225][0m Trial 14 finished with value: 0.054644434416995326 and parameters: {'observation_period_num': 50, 'train_rates': 0.9414252067249913, 'learning_rate': 0.0009439710381121832, 'batch_size': 108, 'step_size': 4, 'gamma': 0.8278569067189373}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:33:11,147][0m Trial 15 finished with value: 0.15722797645462883 and parameters: {'observation_period_num': 55, 'train_rates': 0.9317472738004154, 'learning_rate': 7.957856558425993e-05, 'batch_size': 77, 'step_size': 3, 'gamma': 0.7904460138185672}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:36:19,237][0m Trial 16 finished with value: 0.06475408875591439 and parameters: {'observation_period_num': 40, 'train_rates': 0.8294256308877906, 'learning_rate': 0.0004276992198207443, 'batch_size': 27, 'step_size': 10, 'gamma': 0.852970682084605}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:37:08,673][0m Trial 17 finished with value: 0.3930732230496497 and parameters: {'observation_period_num': 227, 'train_rates': 0.7106376087268025, 'learning_rate': 0.00011709220011599498, 'batch_size': 89, 'step_size': 6, 'gamma': 0.7537945649362292}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:39:30,879][0m Trial 18 finished with value: 0.04073352981727607 and parameters: {'observation_period_num': 29, 'train_rates': 0.9309250296162953, 'learning_rate': 0.00039403596863182903, 'batch_size': 39, 'step_size': 11, 'gamma': 0.7917918441311056}. Best is trial 10 with value: 0.02822481076184072.[0m
Early stopping at epoch 88
[32m[I 2025-01-11 15:40:08,517][0m Trial 19 finished with value: 3.202918538860246 and parameters: {'observation_period_num': 65, 'train_rates': 0.87637909684185, 'learning_rate': 1.2350578891014217e-06, 'batch_size': 133, 'step_size': 1, 'gamma': 0.8978586992586682}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:41:30,572][0m Trial 20 finished with value: 0.18240762147747103 and parameters: {'observation_period_num': 5, 'train_rates': 0.9589208913637679, 'learning_rate': 7.923130683603079e-06, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8415312587575744}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:44:09,323][0m Trial 21 finished with value: 0.03376922011375427 and parameters: {'observation_period_num': 21, 'train_rates': 0.9877680354176145, 'learning_rate': 0.000970855796386456, 'batch_size': 37, 'step_size': 3, 'gamma': 0.8117786619159499}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:49:11,537][0m Trial 22 finished with value: 0.03223507342414117 and parameters: {'observation_period_num': 25, 'train_rates': 0.9759942452637449, 'learning_rate': 0.000623984792377927, 'batch_size': 19, 'step_size': 5, 'gamma': 0.811509475946475}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:51:19,284][0m Trial 23 finished with value: 0.06305841051440671 and parameters: {'observation_period_num': 70, 'train_rates': 0.9159683331884804, 'learning_rate': 0.00047926579954431377, 'batch_size': 43, 'step_size': 6, 'gamma': 0.8485734832255046}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:57:05,428][0m Trial 24 finished with value: 0.047281025499105456 and parameters: {'observation_period_num': 34, 'train_rates': 0.9572597431295478, 'learning_rate': 0.00029990249062641726, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7718774300064154}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 15:58:59,592][0m Trial 25 finished with value: 0.0944343966952825 and parameters: {'observation_period_num': 197, 'train_rates': 0.9146589896664488, 'learning_rate': 0.0006375727201288159, 'batch_size': 46, 'step_size': 3, 'gamma': 0.8046089370183523}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:00:11,902][0m Trial 26 finished with value: 0.12758616286341615 and parameters: {'observation_period_num': 50, 'train_rates': 0.8354913215782845, 'learning_rate': 0.000135701768755774, 'batch_size': 72, 'step_size': 2, 'gamma': 0.8315873400297391}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:01:04,155][0m Trial 27 finished with value: 0.07134294722761426 and parameters: {'observation_period_num': 5, 'train_rates': 0.9572584834259883, 'learning_rate': 8.117386948452124e-05, 'batch_size': 114, 'step_size': 5, 'gamma': 0.8870716239171608}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:02:05,192][0m Trial 28 finished with value: 0.09355361916802146 and parameters: {'observation_period_num': 80, 'train_rates': 0.9045398930538793, 'learning_rate': 0.0002744527083814158, 'batch_size': 91, 'step_size': 2, 'gamma': 0.8616322596648363}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:03:33,623][0m Trial 29 finished with value: 0.06190186086387768 and parameters: {'observation_period_num': 37, 'train_rates': 0.8537823261352286, 'learning_rate': 0.000613491437401496, 'batch_size': 60, 'step_size': 7, 'gamma': 0.8399247668186072}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:05:38,492][0m Trial 30 finished with value: 0.19446459548775427 and parameters: {'observation_period_num': 126, 'train_rates': 0.6090084065349686, 'learning_rate': 0.00026782879147146054, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8044721171236081}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:09:41,612][0m Trial 31 finished with value: 0.03588045388460159 and parameters: {'observation_period_num': 19, 'train_rates': 0.9878911460482535, 'learning_rate': 0.0007993853430011941, 'batch_size': 24, 'step_size': 4, 'gamma': 0.816413809704155}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:15:31,248][0m Trial 32 finished with value: 0.03996786127023608 and parameters: {'observation_period_num': 21, 'train_rates': 0.9636415306248985, 'learning_rate': 0.0009268839679479242, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7723378480816614}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:17:30,179][0m Trial 33 finished with value: 0.07549273152100412 and parameters: {'observation_period_num': 63, 'train_rates': 0.9673264626512802, 'learning_rate': 0.000412796234182688, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8198748220641825}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:20:35,810][0m Trial 34 finished with value: 0.04475809628697666 and parameters: {'observation_period_num': 19, 'train_rates': 0.9370993529905164, 'learning_rate': 0.00021631491683608804, 'batch_size': 30, 'step_size': 5, 'gamma': 0.7812117418087206}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:20:59,579][0m Trial 35 finished with value: 0.06481789258666008 and parameters: {'observation_period_num': 44, 'train_rates': 0.8909397583805235, 'learning_rate': 0.0005731423465075652, 'batch_size': 256, 'step_size': 6, 'gamma': 0.8003350568240732}. Best is trial 10 with value: 0.02822481076184072.[0m
Early stopping at epoch 95
[32m[I 2025-01-11 16:21:28,394][0m Trial 36 finished with value: 0.2269197255373001 and parameters: {'observation_period_num': 92, 'train_rates': 0.9713351408645703, 'learning_rate': 0.0003353602674994929, 'batch_size': 212, 'step_size': 1, 'gamma': 0.8785207309001195}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:23:06,103][0m Trial 37 finished with value: 0.05447021123082912 and parameters: {'observation_period_num': 27, 'train_rates': 0.7916830322346836, 'learning_rate': 0.0007116523335454377, 'batch_size': 51, 'step_size': 2, 'gamma': 0.8593135673629498}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:23:43,988][0m Trial 38 finished with value: 0.07044065203023168 and parameters: {'observation_period_num': 15, 'train_rates': 0.9230543848209235, 'learning_rate': 0.00017480412944823862, 'batch_size': 156, 'step_size': 4, 'gamma': 0.8371866332702126}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:25:15,921][0m Trial 39 finished with value: 0.07901346238402578 and parameters: {'observation_period_num': 108, 'train_rates': 0.9459696127542989, 'learning_rate': 0.0005386354885382272, 'batch_size': 60, 'step_size': 15, 'gamma': 0.7744935836620364}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:28:17,279][0m Trial 40 finished with value: 0.06704554058500191 and parameters: {'observation_period_num': 73, 'train_rates': 0.8838791568627331, 'learning_rate': 6.407074828730004e-05, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8121567421568793}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:30:37,765][0m Trial 41 finished with value: 0.043735411168346484 and parameters: {'observation_period_num': 32, 'train_rates': 0.9788777024363469, 'learning_rate': 0.0009549938168511183, 'batch_size': 41, 'step_size': 3, 'gamma': 0.8045067301373892}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:33:16,497][0m Trial 42 finished with value: 0.030402973294258118 and parameters: {'observation_period_num': 16, 'train_rates': 0.9878544539791891, 'learning_rate': 0.000977301085626773, 'batch_size': 37, 'step_size': 3, 'gamma': 0.813315732032475}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:36:37,644][0m Trial 43 finished with value: 0.14715942748729025 and parameters: {'observation_period_num': 15, 'train_rates': 0.7246934132097851, 'learning_rate': 0.0006575781880306575, 'batch_size': 23, 'step_size': 5, 'gamma': 0.7936122312224859}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:38:03,946][0m Trial 44 finished with value: 0.545277445865192 and parameters: {'observation_period_num': 58, 'train_rates': 0.9522136853622493, 'learning_rate': 1.4710326883554428e-05, 'batch_size': 67, 'step_size': 2, 'gamma': 0.8219557558720413}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:40:44,604][0m Trial 45 finished with value: 0.3875424861907959 and parameters: {'observation_period_num': 41, 'train_rates': 0.9899370112207622, 'learning_rate': 3.0995808335666004e-06, 'batch_size': 36, 'step_size': 9, 'gamma': 0.8465262807936491}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:42:34,133][0m Trial 46 finished with value: 0.04147096758379656 and parameters: {'observation_period_num': 6, 'train_rates': 0.9712659078178579, 'learning_rate': 0.000375554425781624, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8313325030355808}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:43:40,642][0m Trial 47 finished with value: 0.05826157941059633 and parameters: {'observation_period_num': 48, 'train_rates': 0.943299096887637, 'learning_rate': 0.0007357429768383389, 'batch_size': 87, 'step_size': 3, 'gamma': 0.8746974084774034}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:48:57,461][0m Trial 48 finished with value: 0.0966087230583177 and parameters: {'observation_period_num': 150, 'train_rates': 0.9025004806107406, 'learning_rate': 0.00046796592585853983, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7828309290942256}. Best is trial 10 with value: 0.02822481076184072.[0m
[32m[I 2025-01-11 16:51:43,129][0m Trial 49 finished with value: 0.057280640289621446 and parameters: {'observation_period_num': 182, 'train_rates': 0.9257359767117382, 'learning_rate': 0.0002254708864648788, 'batch_size': 32, 'step_size': 6, 'gamma': 0.8249856009760322}. Best is trial 10 with value: 0.02822481076184072.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 16, 'train_rates': 0.9556319240430787, 'learning_rate': 0.000983583342322892, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9172512567577513}
Epoch 1/300, trend Loss: 0.2155 | 0.0783
Epoch 2/300, trend Loss: 0.1363 | 0.0711
Epoch 3/300, trend Loss: 0.1200 | 0.0702
Epoch 4/300, trend Loss: 0.1132 | 0.0615
Epoch 5/300, trend Loss: 0.1039 | 0.0612
Epoch 6/300, trend Loss: 0.0951 | 0.0548
Epoch 7/300, trend Loss: 0.0905 | 0.0497
Epoch 8/300, trend Loss: 0.0869 | 0.0480
Epoch 9/300, trend Loss: 0.0830 | 0.0467
Epoch 10/300, trend Loss: 0.0805 | 0.0461
Epoch 11/300, trend Loss: 0.0791 | 0.0483
Epoch 12/300, trend Loss: 0.0773 | 0.0501
Epoch 13/300, trend Loss: 0.0756 | 0.0476
Epoch 14/300, trend Loss: 0.0739 | 0.0467
Epoch 15/300, trend Loss: 0.0718 | 0.0445
Epoch 16/300, trend Loss: 0.0698 | 0.0407
Epoch 17/300, trend Loss: 0.0690 | 0.0404
Epoch 18/300, trend Loss: 0.0678 | 0.0391
Epoch 19/300, trend Loss: 0.0666 | 0.0390
Epoch 20/300, trend Loss: 0.0660 | 0.0394
Epoch 21/300, trend Loss: 0.0646 | 0.0385
Epoch 22/300, trend Loss: 0.0637 | 0.0392
Epoch 23/300, trend Loss: 0.0637 | 0.0389
Epoch 24/300, trend Loss: 0.0633 | 0.0404
Epoch 25/300, trend Loss: 0.0625 | 0.0371
Epoch 26/300, trend Loss: 0.0623 | 0.0352
Epoch 27/300, trend Loss: 0.0623 | 0.0328
Epoch 28/300, trend Loss: 0.0618 | 0.0350
Epoch 29/300, trend Loss: 0.0620 | 0.0372
Epoch 30/300, trend Loss: 0.0615 | 0.0338
Epoch 31/300, trend Loss: 0.0600 | 0.0317
Epoch 32/300, trend Loss: 0.0590 | 0.0306
Epoch 33/300, trend Loss: 0.0596 | 0.0377
Epoch 34/300, trend Loss: 0.0598 | 0.0329
Epoch 35/300, trend Loss: 0.0585 | 0.0360
Epoch 36/300, trend Loss: 0.0596 | 0.0345
Epoch 37/300, trend Loss: 0.0589 | 0.0365
Epoch 38/300, trend Loss: 0.0579 | 0.0409
Epoch 39/300, trend Loss: 0.0593 | 0.0346
Epoch 40/300, trend Loss: 0.0569 | 0.0348
Epoch 41/300, trend Loss: 0.0576 | 0.0335
Epoch 42/300, trend Loss: 0.0578 | 0.0334
Epoch 43/300, trend Loss: 0.0563 | 0.0370
Epoch 44/300, trend Loss: 0.0551 | 0.0409
Epoch 45/300, trend Loss: 0.0604 | 0.0447
Epoch 46/300, trend Loss: 0.0709 | 0.0389
Epoch 47/300, trend Loss: 0.0601 | 0.0332
Epoch 48/300, trend Loss: 0.0549 | 0.0328
Epoch 49/300, trend Loss: 0.0533 | 0.0309
Epoch 50/300, trend Loss: 0.0512 | 0.0299
Epoch 51/300, trend Loss: 0.0498 | 0.0297
Epoch 52/300, trend Loss: 0.0490 | 0.0296
Epoch 53/300, trend Loss: 0.0485 | 0.0295
Epoch 54/300, trend Loss: 0.0481 | 0.0295
Epoch 55/300, trend Loss: 0.0477 | 0.0303
Epoch 56/300, trend Loss: 0.0472 | 0.0319
Epoch 57/300, trend Loss: 0.0468 | 0.0305
Epoch 58/300, trend Loss: 0.0467 | 0.0320
Epoch 59/300, trend Loss: 0.0462 | 0.0332
Epoch 60/300, trend Loss: 0.0459 | 0.0327
Epoch 61/300, trend Loss: 0.0432 | 0.0322
Epoch 62/300, trend Loss: 0.0424 | 0.0298
Epoch 63/300, trend Loss: 0.0480 | 0.0326
Epoch 64/300, trend Loss: 0.0471 | 0.0343
Epoch 65/300, trend Loss: 0.0459 | 0.0358
Epoch 66/300, trend Loss: 0.0421 | 0.0383
Epoch 67/300, trend Loss: 0.0448 | 0.0340
Epoch 68/300, trend Loss: 0.0470 | 0.0333
Epoch 69/300, trend Loss: 0.0466 | 0.0410
Epoch 70/300, trend Loss: 0.0510 | 0.0444
Epoch 71/300, trend Loss: 0.0451 | 0.0334
Epoch 72/300, trend Loss: 0.0401 | 0.0303
Epoch 73/300, trend Loss: 0.0387 | 0.0303
Epoch 74/300, trend Loss: 0.0379 | 0.0308
Epoch 75/300, trend Loss: 0.0378 | 0.0299
Epoch 76/300, trend Loss: 0.0379 | 0.0307
Epoch 77/300, trend Loss: 0.0418 | 0.0466
Epoch 78/300, trend Loss: 0.0564 | 0.0410
Epoch 79/300, trend Loss: 0.0506 | 0.0332
Epoch 80/300, trend Loss: 0.0446 | 0.0340
Epoch 81/300, trend Loss: 0.0412 | 0.0324
Epoch 82/300, trend Loss: 0.0411 | 0.0400
Epoch 83/300, trend Loss: 0.0470 | 0.0390
Epoch 84/300, trend Loss: 0.0459 | 0.0353
Epoch 85/300, trend Loss: 0.0425 | 0.0345
Epoch 86/300, trend Loss: 0.0419 | 0.0332
Epoch 87/300, trend Loss: 0.0387 | 0.0319
Epoch 88/300, trend Loss: 0.0368 | 0.0315
Epoch 89/300, trend Loss: 0.0369 | 0.0314
Epoch 90/300, trend Loss: 0.0355 | 0.0311
Epoch 91/300, trend Loss: 0.0348 | 0.0318
Epoch 92/300, trend Loss: 0.0342 | 0.0320
Epoch 93/300, trend Loss: 0.0338 | 0.0322
Epoch 94/300, trend Loss: 0.0335 | 0.0326
Epoch 95/300, trend Loss: 0.0332 | 0.0327
Epoch 96/300, trend Loss: 0.0330 | 0.0333
Epoch 97/300, trend Loss: 0.0328 | 0.0332
Epoch 98/300, trend Loss: 0.0327 | 0.0340
Epoch 99/300, trend Loss: 0.0328 | 0.0331
Epoch 100/300, trend Loss: 0.0332 | 0.0343
Epoch 101/300, trend Loss: 0.0332 | 0.0324
Epoch 102/300, trend Loss: 0.0327 | 0.0342
Epoch 103/300, trend Loss: 0.0331 | 0.0329
Epoch 104/300, trend Loss: 0.0327 | 0.0345
Epoch 105/300, trend Loss: 0.0320 | 0.0346
Epoch 106/300, trend Loss: 0.0318 | 0.0344
Epoch 107/300, trend Loss: 0.0312 | 0.0359
Epoch 108/300, trend Loss: 0.0311 | 0.0343
Epoch 109/300, trend Loss: 0.0304 | 0.0367
Epoch 110/300, trend Loss: 0.0306 | 0.0350
Epoch 111/300, trend Loss: 0.0303 | 0.0363
Epoch 112/300, trend Loss: 0.0310 | 0.0370
Epoch 113/300, trend Loss: 0.0313 | 0.0356
Epoch 114/300, trend Loss: 0.0310 | 0.0353
Epoch 115/300, trend Loss: 0.0301 | 0.0374
Epoch 116/300, trend Loss: 0.0302 | 0.0357
Epoch 117/300, trend Loss: 0.0298 | 0.0383
Epoch 118/300, trend Loss: 0.0294 | 0.0345
Epoch 119/300, trend Loss: 0.0288 | 0.0375
Epoch 120/300, trend Loss: 0.0289 | 0.0358
Epoch 121/300, trend Loss: 0.0287 | 0.0380
Epoch 122/300, trend Loss: 0.0284 | 0.0371
Epoch 123/300, trend Loss: 0.0283 | 0.0375
Epoch 124/300, trend Loss: 0.0281 | 0.0367
Epoch 125/300, trend Loss: 0.0282 | 0.0393
Epoch 126/300, trend Loss: 0.0280 | 0.0382
Epoch 127/300, trend Loss: 0.0278 | 0.0368
Epoch 128/300, trend Loss: 0.0273 | 0.0390
Epoch 129/300, trend Loss: 0.0277 | 0.0393
Epoch 130/300, trend Loss: 0.0278 | 0.0386
Epoch 131/300, trend Loss: 0.0274 | 0.0373
Epoch 132/300, trend Loss: 0.0269 | 0.0391
Epoch 133/300, trend Loss: 0.0269 | 0.0368
Epoch 134/300, trend Loss: 0.0272 | 0.0403
Epoch 135/300, trend Loss: 0.0271 | 0.0390
Epoch 136/300, trend Loss: 0.0266 | 0.0382
Epoch 137/300, trend Loss: 0.0264 | 0.0379
Epoch 138/300, trend Loss: 0.0266 | 0.0375
Epoch 139/300, trend Loss: 0.0264 | 0.0396
Epoch 140/300, trend Loss: 0.0262 | 0.0396
Epoch 141/300, trend Loss: 0.0257 | 0.0388
Epoch 142/300, trend Loss: 0.0254 | 0.0378
Epoch 143/300, trend Loss: 0.0253 | 0.0381
Epoch 144/300, trend Loss: 0.0257 | 0.0392
Epoch 145/300, trend Loss: 0.0251 | 0.0389
Epoch 146/300, trend Loss: 0.0246 | 0.0397
Epoch 147/300, trend Loss: 0.0245 | 0.0385
Epoch 148/300, trend Loss: 0.0244 | 0.0388
Epoch 149/300, trend Loss: 0.0243 | 0.0388
Epoch 150/300, trend Loss: 0.0242 | 0.0384
Epoch 151/300, trend Loss: 0.0242 | 0.0392
Epoch 152/300, trend Loss: 0.0242 | 0.0378
Epoch 153/300, trend Loss: 0.0251 | 0.0398
Epoch 154/300, trend Loss: 0.0371 | 0.0393
Epoch 155/300, trend Loss: 0.0330 | 0.0455
Epoch 156/300, trend Loss: 0.0308 | 0.0394
Epoch 157/300, trend Loss: 0.0273 | 0.0397
Epoch 158/300, trend Loss: 0.0250 | 0.0397
Epoch 159/300, trend Loss: 0.0242 | 0.0398
Epoch 160/300, trend Loss: 0.0238 | 0.0400
Epoch 161/300, trend Loss: 0.0236 | 0.0399
Epoch 162/300, trend Loss: 0.0234 | 0.0401
Epoch 163/300, trend Loss: 0.0232 | 0.0401
Epoch 164/300, trend Loss: 0.0231 | 0.0402
Epoch 165/300, trend Loss: 0.0229 | 0.0401
Epoch 166/300, trend Loss: 0.0228 | 0.0402
Epoch 167/300, trend Loss: 0.0227 | 0.0401
Epoch 168/300, trend Loss: 0.0226 | 0.0402
Epoch 169/300, trend Loss: 0.0225 | 0.0401
Epoch 170/300, trend Loss: 0.0224 | 0.0402
Epoch 171/300, trend Loss: 0.0223 | 0.0402
Epoch 172/300, trend Loss: 0.0222 | 0.0402
Epoch 173/300, trend Loss: 0.0221 | 0.0403
Epoch 174/300, trend Loss: 0.0220 | 0.0403
Epoch 175/300, trend Loss: 0.0219 | 0.0403
Epoch 176/300, trend Loss: 0.0218 | 0.0404
Epoch 177/300, trend Loss: 0.0218 | 0.0403
Epoch 178/300, trend Loss: 0.0217 | 0.0403
Epoch 179/300, trend Loss: 0.0216 | 0.0404
Epoch 180/300, trend Loss: 0.0217 | 0.0402
Epoch 181/300, trend Loss: 0.0218 | 0.0404
Epoch 182/300, trend Loss: 0.0219 | 0.0401
Epoch 183/300, trend Loss: 0.0216 | 0.0407
Epoch 184/300, trend Loss: 0.0215 | 0.0398
Epoch 185/300, trend Loss: 0.0214 | 0.0407
Epoch 186/300, trend Loss: 0.0213 | 0.0397
Epoch 187/300, trend Loss: 0.0212 | 0.0407
Epoch 188/300, trend Loss: 0.0211 | 0.0397
Epoch 189/300, trend Loss: 0.0210 | 0.0401
Epoch 190/300, trend Loss: 0.0209 | 0.0402
Epoch 191/300, trend Loss: 0.0208 | 0.0402
Epoch 192/300, trend Loss: 0.0207 | 0.0400
Epoch 193/300, trend Loss: 0.0206 | 0.0402
Epoch 194/300, trend Loss: 0.0206 | 0.0399
Epoch 195/300, trend Loss: 0.0205 | 0.0406
Epoch 196/300, trend Loss: 0.0205 | 0.0403
Epoch 197/300, trend Loss: 0.0204 | 0.0402
Epoch 198/300, trend Loss: 0.0204 | 0.0404
Epoch 199/300, trend Loss: 0.0203 | 0.0406
Epoch 200/300, trend Loss: 0.0204 | 0.0410
Epoch 201/300, trend Loss: 0.0203 | 0.0404
Epoch 202/300, trend Loss: 0.0202 | 0.0412
Epoch 203/300, trend Loss: 0.0202 | 0.0408
Epoch 204/300, trend Loss: 0.0201 | 0.0411
Epoch 205/300, trend Loss: 0.0199 | 0.0413
Epoch 206/300, trend Loss: 0.0199 | 0.0411
Epoch 207/300, trend Loss: 0.0199 | 0.0407
Epoch 208/300, trend Loss: 0.0198 | 0.0411
Epoch 209/300, trend Loss: 0.0198 | 0.0410
Epoch 210/300, trend Loss: 0.0198 | 0.0411
Epoch 211/300, trend Loss: 0.0198 | 0.0409
Epoch 212/300, trend Loss: 0.0197 | 0.0406
Epoch 213/300, trend Loss: 0.0196 | 0.0406
Epoch 214/300, trend Loss: 0.0195 | 0.0405
Epoch 215/300, trend Loss: 0.0195 | 0.0408
Epoch 216/300, trend Loss: 0.0194 | 0.0409
Epoch 217/300, trend Loss: 0.0193 | 0.0411
Epoch 218/300, trend Loss: 0.0192 | 0.0414
Epoch 219/300, trend Loss: 0.0191 | 0.0417
Epoch 220/300, trend Loss: 0.0191 | 0.0418
Epoch 221/300, trend Loss: 0.0190 | 0.0422
Epoch 222/300, trend Loss: 0.0189 | 0.0420
Epoch 223/300, trend Loss: 0.0189 | 0.0420
Epoch 224/300, trend Loss: 0.0189 | 0.0420
Epoch 225/300, trend Loss: 0.0189 | 0.0416
Epoch 226/300, trend Loss: 0.0189 | 0.0417
Epoch 227/300, trend Loss: 0.0188 | 0.0416
Epoch 228/300, trend Loss: 0.0188 | 0.0417
Epoch 229/300, trend Loss: 0.0187 | 0.0419
Epoch 230/300, trend Loss: 0.0187 | 0.0420
Epoch 231/300, trend Loss: 0.0186 | 0.0423
Epoch 232/300, trend Loss: 0.0186 | 0.0422
Epoch 233/300, trend Loss: 0.0185 | 0.0427
Epoch 234/300, trend Loss: 0.0184 | 0.0428
Epoch 235/300, trend Loss: 0.0184 | 0.0431
Epoch 236/300, trend Loss: 0.0183 | 0.0431
Epoch 237/300, trend Loss: 0.0182 | 0.0432
Epoch 238/300, trend Loss: 0.0182 | 0.0429
Epoch 239/300, trend Loss: 0.0182 | 0.0431
Epoch 240/300, trend Loss: 0.0182 | 0.0429
Epoch 241/300, trend Loss: 0.0182 | 0.0431
Epoch 242/300, trend Loss: 0.0181 | 0.0430
Epoch 243/300, trend Loss: 0.0181 | 0.0432
Epoch 244/300, trend Loss: 0.0181 | 0.0433
Epoch 245/300, trend Loss: 0.0180 | 0.0434
Epoch 246/300, trend Loss: 0.0180 | 0.0437
Epoch 247/300, trend Loss: 0.0179 | 0.0437
Epoch 248/300, trend Loss: 0.0179 | 0.0440
Epoch 249/300, trend Loss: 0.0179 | 0.0441
Epoch 250/300, trend Loss: 0.0178 | 0.0443
Epoch 251/300, trend Loss: 0.0178 | 0.0441
Epoch 252/300, trend Loss: 0.0177 | 0.0442
Epoch 253/300, trend Loss: 0.0177 | 0.0440
Epoch 254/300, trend Loss: 0.0177 | 0.0442
Epoch 255/300, trend Loss: 0.0176 | 0.0440
Epoch 256/300, trend Loss: 0.0176 | 0.0445
Epoch 257/300, trend Loss: 0.0176 | 0.0443
Epoch 258/300, trend Loss: 0.0175 | 0.0445
Epoch 259/300, trend Loss: 0.0175 | 0.0445
Epoch 260/300, trend Loss: 0.0175 | 0.0447
Epoch 261/300, trend Loss: 0.0174 | 0.0448
Epoch 262/300, trend Loss: 0.0174 | 0.0450
Epoch 263/300, trend Loss: 0.0173 | 0.0449
Epoch 264/300, trend Loss: 0.0173 | 0.0451
Epoch 265/300, trend Loss: 0.0173 | 0.0449
Epoch 266/300, trend Loss: 0.0173 | 0.0449
Epoch 267/300, trend Loss: 0.0173 | 0.0447
Epoch 268/300, trend Loss: 0.0173 | 0.0448
Epoch 269/300, trend Loss: 0.0173 | 0.0446
Epoch 270/300, trend Loss: 0.0173 | 0.0447
Epoch 271/300, trend Loss: 0.0173 | 0.0446
Epoch 272/300, trend Loss: 0.0173 | 0.0450
Epoch 273/300, trend Loss: 0.0173 | 0.0452
Epoch 274/300, trend Loss: 0.0173 | 0.0458
Epoch 275/300, trend Loss: 0.0172 | 0.0460
Epoch 276/300, trend Loss: 0.0172 | 0.0466
Epoch 277/300, trend Loss: 0.0171 | 0.0466
Epoch 278/300, trend Loss: 0.0171 | 0.0470
Epoch 279/300, trend Loss: 0.0171 | 0.0472
Epoch 280/300, trend Loss: 0.0171 | 0.0473
Epoch 281/300, trend Loss: 0.0171 | 0.0469
Epoch 282/300, trend Loss: 0.0170 | 0.0470
Epoch 283/300, trend Loss: 0.0170 | 0.0469
Epoch 284/300, trend Loss: 0.0169 | 0.0470
Epoch 285/300, trend Loss: 0.0169 | 0.0469
Epoch 286/300, trend Loss: 0.0169 | 0.0470
Epoch 287/300, trend Loss: 0.0169 | 0.0470
Epoch 288/300, trend Loss: 0.0168 | 0.0469
Epoch 289/300, trend Loss: 0.0168 | 0.0467
Epoch 290/300, trend Loss: 0.0168 | 0.0466
Epoch 291/300, trend Loss: 0.0167 | 0.0464
Epoch 292/300, trend Loss: 0.0167 | 0.0463
Epoch 293/300, trend Loss: 0.0166 | 0.0462
Epoch 294/300, trend Loss: 0.0166 | 0.0463
Epoch 295/300, trend Loss: 0.0166 | 0.0463
Epoch 296/300, trend Loss: 0.0166 | 0.0464
Epoch 297/300, trend Loss: 0.0165 | 0.0464
Epoch 298/300, trend Loss: 0.0165 | 0.0465
Epoch 299/300, trend Loss: 0.0165 | 0.0465
Epoch 300/300, trend Loss: 0.0165 | 0.0465
Training seasonal_0 component with params: {'observation_period_num': 23, 'train_rates': 0.7965433176018238, 'learning_rate': 0.00011616369376544115, 'batch_size': 20, 'step_size': 13, 'gamma': 0.8872492603253566}
Epoch 1/300, seasonal_0 Loss: 0.3253 | 0.1186
Epoch 2/300, seasonal_0 Loss: 0.1469 | 0.1309
Epoch 3/300, seasonal_0 Loss: 0.1157 | 0.1967
Epoch 4/300, seasonal_0 Loss: 0.1126 | 0.1462
Epoch 5/300, seasonal_0 Loss: 0.1141 | 0.2202
Epoch 6/300, seasonal_0 Loss: 0.1067 | 0.1049
Epoch 7/300, seasonal_0 Loss: 0.1117 | 0.0908
Epoch 8/300, seasonal_0 Loss: 0.1058 | 0.0840
Epoch 9/300, seasonal_0 Loss: 0.1063 | 0.1232
Epoch 10/300, seasonal_0 Loss: 0.0906 | 0.1161
Epoch 11/300, seasonal_0 Loss: 0.0858 | 0.0891
Epoch 12/300, seasonal_0 Loss: 0.0906 | 0.1217
Epoch 13/300, seasonal_0 Loss: 0.0933 | 0.2388
Epoch 14/300, seasonal_0 Loss: 0.0911 | 0.1534
Epoch 15/300, seasonal_0 Loss: 0.0958 | 0.1857
Epoch 16/300, seasonal_0 Loss: 0.0910 | 0.1574
Epoch 17/300, seasonal_0 Loss: 0.0821 | 0.2042
Epoch 18/300, seasonal_0 Loss: 0.0881 | 0.0850
Epoch 19/300, seasonal_0 Loss: 0.0881 | 0.1086
Epoch 20/300, seasonal_0 Loss: 0.0773 | 0.0844
Epoch 21/300, seasonal_0 Loss: 0.0727 | 0.0961
Epoch 22/300, seasonal_0 Loss: 0.0784 | 0.1082
Epoch 23/300, seasonal_0 Loss: 0.0717 | 0.1028
Epoch 24/300, seasonal_0 Loss: 0.0702 | 0.1157
Epoch 25/300, seasonal_0 Loss: 0.0649 | 0.1046
Epoch 26/300, seasonal_0 Loss: 0.0672 | 0.1111
Epoch 27/300, seasonal_0 Loss: 0.0620 | 0.1018
Epoch 28/300, seasonal_0 Loss: 0.0632 | 0.1062
Epoch 29/300, seasonal_0 Loss: 0.0597 | 0.0791
Epoch 30/300, seasonal_0 Loss: 0.0599 | 0.0877
Epoch 31/300, seasonal_0 Loss: 0.0568 | 0.1223
Epoch 32/300, seasonal_0 Loss: 0.0583 | 0.1043
Epoch 33/300, seasonal_0 Loss: 0.0592 | 0.1015
Epoch 34/300, seasonal_0 Loss: 0.0529 | 0.1020
Epoch 35/300, seasonal_0 Loss: 0.0537 | 0.1945
Epoch 36/300, seasonal_0 Loss: 0.0576 | 0.6451
Epoch 37/300, seasonal_0 Loss: 0.0650 | 0.1177
Epoch 38/300, seasonal_0 Loss: 0.0583 | 0.0955
Epoch 39/300, seasonal_0 Loss: 0.0535 | 0.1287
Epoch 40/300, seasonal_0 Loss: 0.0540 | 0.1146
Epoch 41/300, seasonal_0 Loss: 0.0530 | 0.1271
Epoch 42/300, seasonal_0 Loss: 0.0536 | 0.0845
Epoch 43/300, seasonal_0 Loss: 0.0523 | 0.0956
Epoch 44/300, seasonal_0 Loss: 0.0496 | 0.1081
Epoch 45/300, seasonal_0 Loss: 0.0465 | 0.0945
Epoch 46/300, seasonal_0 Loss: 0.0447 | 0.0849
Epoch 47/300, seasonal_0 Loss: 0.0414 | 0.0830
Epoch 48/300, seasonal_0 Loss: 0.0419 | 0.0803
Epoch 49/300, seasonal_0 Loss: 0.0374 | 0.0821
Epoch 50/300, seasonal_0 Loss: 0.0353 | 0.0818
Epoch 51/300, seasonal_0 Loss: 0.0356 | 0.0852
Epoch 52/300, seasonal_0 Loss: 0.0351 | 0.0817
Epoch 53/300, seasonal_0 Loss: 0.0362 | 0.0878
Epoch 54/300, seasonal_0 Loss: 0.0351 | 0.0810
Epoch 55/300, seasonal_0 Loss: 0.0356 | 0.0842
Epoch 56/300, seasonal_0 Loss: 0.0320 | 0.0898
Epoch 57/300, seasonal_0 Loss: 0.0324 | 0.0962
Epoch 58/300, seasonal_0 Loss: 0.0320 | 0.0897
Epoch 59/300, seasonal_0 Loss: 0.0339 | 0.0950
Epoch 60/300, seasonal_0 Loss: 0.0333 | 0.0791
Epoch 61/300, seasonal_0 Loss: 0.0344 | 0.0793
Epoch 62/300, seasonal_0 Loss: 0.0326 | 0.0936
Epoch 63/300, seasonal_0 Loss: 0.0330 | 0.1044
Epoch 64/300, seasonal_0 Loss: 0.0318 | 0.0949
Epoch 65/300, seasonal_0 Loss: 0.0339 | 0.0900
Epoch 66/300, seasonal_0 Loss: 0.0324 | 0.0849
Epoch 67/300, seasonal_0 Loss: 0.0323 | 0.0824
Epoch 68/300, seasonal_0 Loss: 0.0325 | 0.0833
Epoch 69/300, seasonal_0 Loss: 0.0314 | 0.0963
Epoch 70/300, seasonal_0 Loss: 0.0330 | 0.0893
Epoch 71/300, seasonal_0 Loss: 0.0318 | 0.0789
Epoch 72/300, seasonal_0 Loss: 0.0319 | 0.0850
Epoch 73/300, seasonal_0 Loss: 0.0313 | 0.0850
Epoch 74/300, seasonal_0 Loss: 0.0314 | 0.0684
Epoch 75/300, seasonal_0 Loss: 0.0318 | 0.0908
Epoch 76/300, seasonal_0 Loss: 0.0314 | 0.0725
Epoch 77/300, seasonal_0 Loss: 0.0311 | 0.0686
Epoch 78/300, seasonal_0 Loss: 0.0312 | 0.0754
Epoch 79/300, seasonal_0 Loss: 0.0418 | 0.0684
Epoch 80/300, seasonal_0 Loss: 0.0407 | 0.0669
Epoch 81/300, seasonal_0 Loss: 0.0356 | 0.0605
Epoch 82/300, seasonal_0 Loss: 0.0453 | 0.0964
Epoch 83/300, seasonal_0 Loss: 0.0420 | 0.0790
Epoch 84/300, seasonal_0 Loss: 0.0391 | 0.1136
Epoch 85/300, seasonal_0 Loss: 0.0383 | 0.1083
Epoch 86/300, seasonal_0 Loss: 0.0371 | 0.0891
Epoch 87/300, seasonal_0 Loss: 0.0355 | 0.0793
Epoch 88/300, seasonal_0 Loss: 0.0405 | 0.0748
Epoch 89/300, seasonal_0 Loss: 0.0349 | 0.0829
Epoch 90/300, seasonal_0 Loss: 0.0324 | 0.0801
Epoch 91/300, seasonal_0 Loss: 0.0300 | 0.0749
Epoch 92/300, seasonal_0 Loss: 0.0300 | 0.0708
Epoch 93/300, seasonal_0 Loss: 0.0316 | 0.0752
Epoch 94/300, seasonal_0 Loss: 0.0297 | 0.0755
Epoch 95/300, seasonal_0 Loss: 0.0318 | 0.0747
Epoch 96/300, seasonal_0 Loss: 0.0338 | 0.0809
Epoch 97/300, seasonal_0 Loss: 0.0411 | 0.0591
Epoch 98/300, seasonal_0 Loss: 0.0301 | 0.0623
Epoch 99/300, seasonal_0 Loss: 0.0291 | 0.0664
Epoch 100/300, seasonal_0 Loss: 0.0285 | 0.0659
Epoch 101/300, seasonal_0 Loss: 0.0286 | 0.0648
Epoch 102/300, seasonal_0 Loss: 0.0280 | 0.0647
Epoch 103/300, seasonal_0 Loss: 0.0284 | 0.0650
Epoch 104/300, seasonal_0 Loss: 0.0276 | 0.0662
Epoch 105/300, seasonal_0 Loss: 0.0282 | 0.0675
Epoch 106/300, seasonal_0 Loss: 0.0275 | 0.0664
Epoch 107/300, seasonal_0 Loss: 0.0274 | 0.0656
Epoch 108/300, seasonal_0 Loss: 0.0271 | 0.0650
Epoch 109/300, seasonal_0 Loss: 0.0269 | 0.0644
Epoch 110/300, seasonal_0 Loss: 0.0268 | 0.0642
Epoch 111/300, seasonal_0 Loss: 0.0267 | 0.0643
Epoch 112/300, seasonal_0 Loss: 0.0268 | 0.0659
Epoch 113/300, seasonal_0 Loss: 0.0267 | 0.0653
Epoch 114/300, seasonal_0 Loss: 0.0267 | 0.0640
Epoch 115/300, seasonal_0 Loss: 0.0267 | 0.0614
Epoch 116/300, seasonal_0 Loss: 0.0266 | 0.0602
Epoch 117/300, seasonal_0 Loss: 0.0264 | 0.0595
Epoch 118/300, seasonal_0 Loss: 0.0264 | 0.0622
Epoch 119/300, seasonal_0 Loss: 0.0263 | 0.0679
Epoch 120/300, seasonal_0 Loss: 0.0263 | 0.0749
Epoch 121/300, seasonal_0 Loss: 0.0264 | 0.0788
Epoch 122/300, seasonal_0 Loss: 0.0262 | 0.0767
Epoch 123/300, seasonal_0 Loss: 0.0264 | 0.0700
Epoch 124/300, seasonal_0 Loss: 0.0265 | 0.0720
Epoch 125/300, seasonal_0 Loss: 0.0263 | 0.0756
Epoch 126/300, seasonal_0 Loss: 0.0263 | 0.0795
Epoch 127/300, seasonal_0 Loss: 0.0259 | 0.0804
Epoch 128/300, seasonal_0 Loss: 0.0258 | 0.0803
Epoch 129/300, seasonal_0 Loss: 0.0257 | 0.0811
Epoch 130/300, seasonal_0 Loss: 0.0255 | 0.0808
Epoch 131/300, seasonal_0 Loss: 0.0254 | 0.0782
Epoch 132/300, seasonal_0 Loss: 0.0254 | 0.0739
Epoch 133/300, seasonal_0 Loss: 0.0254 | 0.0673
Epoch 134/300, seasonal_0 Loss: 0.0253 | 0.0646
Epoch 135/300, seasonal_0 Loss: 0.0255 | 0.0667
Epoch 136/300, seasonal_0 Loss: 0.0256 | 0.0709
Epoch 137/300, seasonal_0 Loss: 0.0257 | 0.0742
Epoch 138/300, seasonal_0 Loss: 0.0259 | 0.0684
Epoch 139/300, seasonal_0 Loss: 0.0259 | 0.0623
Epoch 140/300, seasonal_0 Loss: 0.0259 | 0.0612
Epoch 141/300, seasonal_0 Loss: 0.0257 | 0.0647
Epoch 142/300, seasonal_0 Loss: 0.0253 | 0.0657
Epoch 143/300, seasonal_0 Loss: 0.0251 | 0.0642
Epoch 144/300, seasonal_0 Loss: 0.0250 | 0.0623
Epoch 145/300, seasonal_0 Loss: 0.0249 | 0.0620
Epoch 146/300, seasonal_0 Loss: 0.0248 | 0.0641
Epoch 147/300, seasonal_0 Loss: 0.0248 | 0.0648
Epoch 148/300, seasonal_0 Loss: 0.0248 | 0.0647
Epoch 149/300, seasonal_0 Loss: 0.0247 | 0.0633
Epoch 150/300, seasonal_0 Loss: 0.0248 | 0.0617
Epoch 151/300, seasonal_0 Loss: 0.0249 | 0.0590
Epoch 152/300, seasonal_0 Loss: 0.0249 | 0.0592
Epoch 153/300, seasonal_0 Loss: 0.0249 | 0.0604
Epoch 154/300, seasonal_0 Loss: 0.0249 | 0.0630
Epoch 155/300, seasonal_0 Loss: 0.0248 | 0.0674
Epoch 156/300, seasonal_0 Loss: 0.0248 | 0.0707
Epoch 157/300, seasonal_0 Loss: 0.0249 | 0.0720
Epoch 158/300, seasonal_0 Loss: 0.0250 | 0.0683
Epoch 159/300, seasonal_0 Loss: 0.0251 | 0.0697
Epoch 160/300, seasonal_0 Loss: 0.0249 | 0.0693
Epoch 161/300, seasonal_0 Loss: 0.0248 | 0.0703
Epoch 162/300, seasonal_0 Loss: 0.0247 | 0.0712
Epoch 163/300, seasonal_0 Loss: 0.0246 | 0.0714
Epoch 164/300, seasonal_0 Loss: 0.0247 | 0.0713
Epoch 165/300, seasonal_0 Loss: 0.0247 | 0.0688
Epoch 166/300, seasonal_0 Loss: 0.0247 | 0.0665
Epoch 167/300, seasonal_0 Loss: 0.0246 | 0.0645
Epoch 168/300, seasonal_0 Loss: 0.0246 | 0.0656
Epoch 169/300, seasonal_0 Loss: 0.0246 | 0.0650
Epoch 170/300, seasonal_0 Loss: 0.0246 | 0.0645
Epoch 171/300, seasonal_0 Loss: 0.0245 | 0.0630
Epoch 172/300, seasonal_0 Loss: 0.0244 | 0.0626
Epoch 173/300, seasonal_0 Loss: 0.0244 | 0.0623
Epoch 174/300, seasonal_0 Loss: 0.0243 | 0.0627
Epoch 175/300, seasonal_0 Loss: 0.0243 | 0.0631
Epoch 176/300, seasonal_0 Loss: 0.0243 | 0.0636
Epoch 177/300, seasonal_0 Loss: 0.0242 | 0.0642
Epoch 178/300, seasonal_0 Loss: 0.0242 | 0.0648
Epoch 179/300, seasonal_0 Loss: 0.0241 | 0.0651
Epoch 180/300, seasonal_0 Loss: 0.0241 | 0.0652
Epoch 181/300, seasonal_0 Loss: 0.0241 | 0.0655
Epoch 182/300, seasonal_0 Loss: 0.0240 | 0.0655
Epoch 183/300, seasonal_0 Loss: 0.0240 | 0.0657
Epoch 184/300, seasonal_0 Loss: 0.0240 | 0.0651
Epoch 185/300, seasonal_0 Loss: 0.0240 | 0.0650
Epoch 186/300, seasonal_0 Loss: 0.0240 | 0.0644
Epoch 187/300, seasonal_0 Loss: 0.0239 | 0.0642
Epoch 188/300, seasonal_0 Loss: 0.0239 | 0.0636
Epoch 189/300, seasonal_0 Loss: 0.0239 | 0.0634
Epoch 190/300, seasonal_0 Loss: 0.0240 | 0.0633
Epoch 191/300, seasonal_0 Loss: 0.0239 | 0.0638
Epoch 192/300, seasonal_0 Loss: 0.0239 | 0.0640
Epoch 193/300, seasonal_0 Loss: 0.0239 | 0.0643
Epoch 194/300, seasonal_0 Loss: 0.0239 | 0.0639
Epoch 195/300, seasonal_0 Loss: 0.0239 | 0.0637
Epoch 196/300, seasonal_0 Loss: 0.0239 | 0.0635
Epoch 197/300, seasonal_0 Loss: 0.0238 | 0.0637
Epoch 198/300, seasonal_0 Loss: 0.0238 | 0.0637
Epoch 199/300, seasonal_0 Loss: 0.0238 | 0.0636
Epoch 200/300, seasonal_0 Loss: 0.0238 | 0.0636
Epoch 201/300, seasonal_0 Loss: 0.0238 | 0.0636
Epoch 202/300, seasonal_0 Loss: 0.0238 | 0.0637
Epoch 203/300, seasonal_0 Loss: 0.0238 | 0.0638
Epoch 204/300, seasonal_0 Loss: 0.0237 | 0.0640
Epoch 205/300, seasonal_0 Loss: 0.0237 | 0.0643
Epoch 206/300, seasonal_0 Loss: 0.0237 | 0.0644
Epoch 207/300, seasonal_0 Loss: 0.0237 | 0.0642
Epoch 208/300, seasonal_0 Loss: 0.0237 | 0.0641
Epoch 209/300, seasonal_0 Loss: 0.0238 | 0.0642
Epoch 210/300, seasonal_0 Loss: 0.0237 | 0.0646
Epoch 211/300, seasonal_0 Loss: 0.0237 | 0.0641
Epoch 212/300, seasonal_0 Loss: 0.0237 | 0.0640
Epoch 213/300, seasonal_0 Loss: 0.0238 | 0.0641
Epoch 214/300, seasonal_0 Loss: 0.0238 | 0.0643
Epoch 215/300, seasonal_0 Loss: 0.0238 | 0.0646
Epoch 216/300, seasonal_0 Loss: 0.0237 | 0.0644
Epoch 217/300, seasonal_0 Loss: 0.0237 | 0.0644
Epoch 218/300, seasonal_0 Loss: 0.0237 | 0.0645
Epoch 219/300, seasonal_0 Loss: 0.0237 | 0.0644
Epoch 220/300, seasonal_0 Loss: 0.0237 | 0.0642
Epoch 221/300, seasonal_0 Loss: 0.0237 | 0.0644
Epoch 222/300, seasonal_0 Loss: 0.0237 | 0.0648
Epoch 223/300, seasonal_0 Loss: 0.0237 | 0.0650
Epoch 224/300, seasonal_0 Loss: 0.0237 | 0.0651
Epoch 225/300, seasonal_0 Loss: 0.0237 | 0.0650
Epoch 226/300, seasonal_0 Loss: 0.0236 | 0.0647
Epoch 227/300, seasonal_0 Loss: 0.0236 | 0.0644
Epoch 228/300, seasonal_0 Loss: 0.0236 | 0.0642
Epoch 229/300, seasonal_0 Loss: 0.0236 | 0.0638
Epoch 230/300, seasonal_0 Loss: 0.0236 | 0.0636
Epoch 231/300, seasonal_0 Loss: 0.0236 | 0.0635
Epoch 232/300, seasonal_0 Loss: 0.0236 | 0.0634
Epoch 233/300, seasonal_0 Loss: 0.0236 | 0.0633
Epoch 234/300, seasonal_0 Loss: 0.0235 | 0.0634
Epoch 235/300, seasonal_0 Loss: 0.0235 | 0.0634
Epoch 236/300, seasonal_0 Loss: 0.0235 | 0.0634
Epoch 237/300, seasonal_0 Loss: 0.0235 | 0.0634
Epoch 238/300, seasonal_0 Loss: 0.0235 | 0.0634
Epoch 239/300, seasonal_0 Loss: 0.0235 | 0.0633
Epoch 240/300, seasonal_0 Loss: 0.0235 | 0.0632
Epoch 241/300, seasonal_0 Loss: 0.0235 | 0.0630
Epoch 242/300, seasonal_0 Loss: 0.0235 | 0.0627
Epoch 243/300, seasonal_0 Loss: 0.0235 | 0.0624
Epoch 244/300, seasonal_0 Loss: 0.0235 | 0.0622
Epoch 245/300, seasonal_0 Loss: 0.0234 | 0.0620
Epoch 246/300, seasonal_0 Loss: 0.0234 | 0.0619
Epoch 247/300, seasonal_0 Loss: 0.0234 | 0.0618
Epoch 248/300, seasonal_0 Loss: 0.0234 | 0.0619
Epoch 249/300, seasonal_0 Loss: 0.0234 | 0.0619
Epoch 250/300, seasonal_0 Loss: 0.0234 | 0.0619
Epoch 251/300, seasonal_0 Loss: 0.0234 | 0.0620
Epoch 252/300, seasonal_0 Loss: 0.0234 | 0.0621
Epoch 253/300, seasonal_0 Loss: 0.0234 | 0.0621
Epoch 254/300, seasonal_0 Loss: 0.0234 | 0.0621
Epoch 255/300, seasonal_0 Loss: 0.0234 | 0.0621
Epoch 256/300, seasonal_0 Loss: 0.0234 | 0.0620
Epoch 257/300, seasonal_0 Loss: 0.0234 | 0.0619
Epoch 258/300, seasonal_0 Loss: 0.0234 | 0.0618
Epoch 259/300, seasonal_0 Loss: 0.0233 | 0.0617
Epoch 260/300, seasonal_0 Loss: 0.0233 | 0.0616
Epoch 261/300, seasonal_0 Loss: 0.0233 | 0.0615
Epoch 262/300, seasonal_0 Loss: 0.0233 | 0.0616
Epoch 263/300, seasonal_0 Loss: 0.0233 | 0.0617
Epoch 264/300, seasonal_0 Loss: 0.0233 | 0.0618
Epoch 265/300, seasonal_0 Loss: 0.0233 | 0.0619
Epoch 266/300, seasonal_0 Loss: 0.0233 | 0.0620
Epoch 267/300, seasonal_0 Loss: 0.0233 | 0.0621
Epoch 268/300, seasonal_0 Loss: 0.0233 | 0.0622
Epoch 269/300, seasonal_0 Loss: 0.0233 | 0.0622
Epoch 270/300, seasonal_0 Loss: 0.0233 | 0.0621
Epoch 271/300, seasonal_0 Loss: 0.0233 | 0.0621
Epoch 272/300, seasonal_0 Loss: 0.0233 | 0.0620
Epoch 273/300, seasonal_0 Loss: 0.0233 | 0.0619
Epoch 274/300, seasonal_0 Loss: 0.0233 | 0.0617
Epoch 275/300, seasonal_0 Loss: 0.0233 | 0.0617
Epoch 276/300, seasonal_0 Loss: 0.0233 | 0.0616
Epoch 277/300, seasonal_0 Loss: 0.0233 | 0.0616
Epoch 278/300, seasonal_0 Loss: 0.0233 | 0.0616
Epoch 279/300, seasonal_0 Loss: 0.0233 | 0.0617
Epoch 280/300, seasonal_0 Loss: 0.0233 | 0.0618
Epoch 281/300, seasonal_0 Loss: 0.0233 | 0.0619
Epoch 282/300, seasonal_0 Loss: 0.0233 | 0.0620
Epoch 283/300, seasonal_0 Loss: 0.0232 | 0.0620
Epoch 284/300, seasonal_0 Loss: 0.0232 | 0.0620
Epoch 285/300, seasonal_0 Loss: 0.0232 | 0.0620
Epoch 286/300, seasonal_0 Loss: 0.0232 | 0.0620
Epoch 287/300, seasonal_0 Loss: 0.0232 | 0.0619
Epoch 288/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 289/300, seasonal_0 Loss: 0.0232 | 0.0617
Epoch 290/300, seasonal_0 Loss: 0.0232 | 0.0617
Epoch 291/300, seasonal_0 Loss: 0.0232 | 0.0616
Epoch 292/300, seasonal_0 Loss: 0.0232 | 0.0616
Epoch 293/300, seasonal_0 Loss: 0.0232 | 0.0616
Epoch 294/300, seasonal_0 Loss: 0.0232 | 0.0617
Epoch 295/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 296/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 297/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 298/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 299/300, seasonal_0 Loss: 0.0232 | 0.0618
Epoch 300/300, seasonal_0 Loss: 0.0232 | 0.0618
Training seasonal_1 component with params: {'observation_period_num': 8, 'train_rates': 0.9857823473501858, 'learning_rate': 0.00015337174501261206, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9851337089951198}
Epoch 1/300, seasonal_1 Loss: 0.2832 | 0.0607
Epoch 2/300, seasonal_1 Loss: 0.1168 | 0.0646
Epoch 3/300, seasonal_1 Loss: 0.1104 | 0.0545
Epoch 4/300, seasonal_1 Loss: 0.1094 | 0.0562
Epoch 5/300, seasonal_1 Loss: 0.0963 | 0.0474
Epoch 6/300, seasonal_1 Loss: 0.0933 | 0.0482
Epoch 7/300, seasonal_1 Loss: 0.0955 | 0.0482
Epoch 8/300, seasonal_1 Loss: 0.0920 | 0.0492
Epoch 9/300, seasonal_1 Loss: 0.1009 | 0.0494
Epoch 10/300, seasonal_1 Loss: 0.0975 | 0.0512
Epoch 11/300, seasonal_1 Loss: 0.1026 | 0.0509
Epoch 12/300, seasonal_1 Loss: 0.0852 | 0.0511
Epoch 13/300, seasonal_1 Loss: 0.0840 | 0.0469
Epoch 14/300, seasonal_1 Loss: 0.0824 | 0.0510
Epoch 15/300, seasonal_1 Loss: 0.0799 | 0.0533
Epoch 16/300, seasonal_1 Loss: 0.0745 | 0.0486
Epoch 17/300, seasonal_1 Loss: 0.0746 | 0.0490
Epoch 18/300, seasonal_1 Loss: 0.0752 | 0.0488
Epoch 19/300, seasonal_1 Loss: 0.0735 | 0.0422
Epoch 20/300, seasonal_1 Loss: 0.0680 | 0.0447
Epoch 21/300, seasonal_1 Loss: 0.0724 | 0.0468
Epoch 22/300, seasonal_1 Loss: 0.0728 | 0.0587
Epoch 23/300, seasonal_1 Loss: 0.0745 | 0.0485
Epoch 24/300, seasonal_1 Loss: 0.0756 | 0.0458
Epoch 25/300, seasonal_1 Loss: 0.0809 | 0.0436
Epoch 26/300, seasonal_1 Loss: 0.0772 | 0.0558
Epoch 27/300, seasonal_1 Loss: 0.0699 | 0.0427
Epoch 28/300, seasonal_1 Loss: 0.0679 | 0.0517
Epoch 29/300, seasonal_1 Loss: 0.0714 | 0.0474
Epoch 30/300, seasonal_1 Loss: 0.0692 | 0.0405
Epoch 31/300, seasonal_1 Loss: 0.0635 | 0.0446
Epoch 32/300, seasonal_1 Loss: 0.0581 | 0.0417
Epoch 33/300, seasonal_1 Loss: 0.0633 | 0.0559
Epoch 34/300, seasonal_1 Loss: 0.0727 | 0.0454
Epoch 35/300, seasonal_1 Loss: 0.0681 | 0.0475
Epoch 36/300, seasonal_1 Loss: 0.0613 | 0.0450
Epoch 37/300, seasonal_1 Loss: 0.0600 | 0.0478
Epoch 38/300, seasonal_1 Loss: 0.0587 | 0.0373
Epoch 39/300, seasonal_1 Loss: 0.0557 | 0.0450
Epoch 40/300, seasonal_1 Loss: 0.0545 | 0.0394
Epoch 41/300, seasonal_1 Loss: 0.0562 | 0.0454
Epoch 42/300, seasonal_1 Loss: 0.0564 | 0.0413
Epoch 43/300, seasonal_1 Loss: 0.0574 | 0.0429
Epoch 44/300, seasonal_1 Loss: 0.0529 | 0.0513
Epoch 45/300, seasonal_1 Loss: 0.0571 | 0.0610
Epoch 46/300, seasonal_1 Loss: 0.0591 | 0.0510
Epoch 47/300, seasonal_1 Loss: 0.0548 | 0.0466
Epoch 48/300, seasonal_1 Loss: 0.0548 | 0.0449
Epoch 49/300, seasonal_1 Loss: 0.0510 | 0.0460
Epoch 50/300, seasonal_1 Loss: 0.0472 | 0.0488
Epoch 51/300, seasonal_1 Loss: 0.0537 | 0.0529
Epoch 52/300, seasonal_1 Loss: 0.0516 | 0.0398
Epoch 53/300, seasonal_1 Loss: 0.0511 | 0.0368
Epoch 54/300, seasonal_1 Loss: 0.0517 | 0.0382
Epoch 55/300, seasonal_1 Loss: 0.0544 | 0.0451
Epoch 56/300, seasonal_1 Loss: 0.0511 | 0.0413
Epoch 57/300, seasonal_1 Loss: 0.0474 | 0.0389
Epoch 58/300, seasonal_1 Loss: 0.0474 | 0.0386
Epoch 59/300, seasonal_1 Loss: 0.0460 | 0.0427
Epoch 60/300, seasonal_1 Loss: 0.0426 | 0.0445
Epoch 61/300, seasonal_1 Loss: 0.0473 | 0.0556
Epoch 62/300, seasonal_1 Loss: 0.0503 | 0.0470
Epoch 63/300, seasonal_1 Loss: 0.0539 | 0.0460
Epoch 64/300, seasonal_1 Loss: 0.0572 | 0.0490
Epoch 65/300, seasonal_1 Loss: 0.0498 | 0.0555
Epoch 66/300, seasonal_1 Loss: 0.0468 | 0.0401
Epoch 67/300, seasonal_1 Loss: 0.0439 | 0.0472
Epoch 68/300, seasonal_1 Loss: 0.0426 | 0.0530
Epoch 69/300, seasonal_1 Loss: 0.0438 | 0.0422
Epoch 70/300, seasonal_1 Loss: 0.0391 | 0.0352
Epoch 71/300, seasonal_1 Loss: 0.0431 | 0.0363
Epoch 72/300, seasonal_1 Loss: 0.0422 | 0.0399
Epoch 73/300, seasonal_1 Loss: 0.0388 | 0.0522
Epoch 74/300, seasonal_1 Loss: 0.0438 | 0.0417
Epoch 75/300, seasonal_1 Loss: 0.0402 | 0.0414
Epoch 76/300, seasonal_1 Loss: 0.0482 | 0.0353
Epoch 77/300, seasonal_1 Loss: 0.0525 | 0.0442
Epoch 78/300, seasonal_1 Loss: 0.0433 | 0.0475
Epoch 79/300, seasonal_1 Loss: 0.0414 | 0.0362
Epoch 80/300, seasonal_1 Loss: 0.0377 | 0.0494
Epoch 81/300, seasonal_1 Loss: 0.0347 | 0.0496
Epoch 82/300, seasonal_1 Loss: 0.0342 | 0.0530
Epoch 83/300, seasonal_1 Loss: 0.0384 | 0.0525
Epoch 84/300, seasonal_1 Loss: 0.0440 | 0.0472
Epoch 85/300, seasonal_1 Loss: 0.0412 | 0.0513
Epoch 86/300, seasonal_1 Loss: 0.0444 | 0.0448
Epoch 87/300, seasonal_1 Loss: 0.0391 | 0.0559
Epoch 88/300, seasonal_1 Loss: 0.0430 | 0.0482
Epoch 89/300, seasonal_1 Loss: 0.0360 | 0.0373
Epoch 90/300, seasonal_1 Loss: 0.0386 | 0.0482
Epoch 91/300, seasonal_1 Loss: 0.0326 | 0.0416
Epoch 92/300, seasonal_1 Loss: 0.0396 | 0.0534
Epoch 93/300, seasonal_1 Loss: 0.0381 | 0.0557
Epoch 94/300, seasonal_1 Loss: 0.0372 | 0.0437
Epoch 95/300, seasonal_1 Loss: 0.0384 | 0.0473
Epoch 96/300, seasonal_1 Loss: 0.0394 | 0.0524
Epoch 97/300, seasonal_1 Loss: 0.0360 | 0.0571
Epoch 98/300, seasonal_1 Loss: 0.0411 | 0.0410
Epoch 99/300, seasonal_1 Loss: 0.0391 | 0.0475
Epoch 100/300, seasonal_1 Loss: 0.0411 | 0.0382
Epoch 101/300, seasonal_1 Loss: 0.0437 | 0.0383
Epoch 102/300, seasonal_1 Loss: 0.0344 | 0.0429
Epoch 103/300, seasonal_1 Loss: 0.0373 | 0.0422
Epoch 104/300, seasonal_1 Loss: 0.0404 | 0.0521
Epoch 105/300, seasonal_1 Loss: 0.0436 | 0.0539
Epoch 106/300, seasonal_1 Loss: 0.0422 | 0.0415
Epoch 107/300, seasonal_1 Loss: 0.0435 | 0.0478
Epoch 108/300, seasonal_1 Loss: 0.0503 | 0.0556
Epoch 109/300, seasonal_1 Loss: 0.0424 | 0.0477
Epoch 110/300, seasonal_1 Loss: 0.0458 | 0.0547
Epoch 111/300, seasonal_1 Loss: 0.0391 | 0.0541
Epoch 112/300, seasonal_1 Loss: 0.0336 | 0.0523
Epoch 113/300, seasonal_1 Loss: 0.0323 | 0.0536
Epoch 114/300, seasonal_1 Loss: 0.0310 | 0.0545
Epoch 115/300, seasonal_1 Loss: 0.0370 | 0.0550
Epoch 116/300, seasonal_1 Loss: 0.0325 | 0.0536
Epoch 117/300, seasonal_1 Loss: 0.0347 | 0.0548
Epoch 118/300, seasonal_1 Loss: 0.0333 | 0.0454
Epoch 119/300, seasonal_1 Loss: 0.0333 | 0.0436
Epoch 120/300, seasonal_1 Loss: 0.0389 | 0.0496
Epoch 121/300, seasonal_1 Loss: 0.0377 | 0.0510
Epoch 122/300, seasonal_1 Loss: 0.0347 | 0.0430
Epoch 123/300, seasonal_1 Loss: 0.0377 | 0.0449
Epoch 124/300, seasonal_1 Loss: 0.0316 | 0.0468
Epoch 125/300, seasonal_1 Loss: 0.0382 | 0.0450
Epoch 126/300, seasonal_1 Loss: 0.0370 | 0.0445
Epoch 127/300, seasonal_1 Loss: 0.0368 | 0.0428
Epoch 128/300, seasonal_1 Loss: 0.0364 | 0.0415
Epoch 129/300, seasonal_1 Loss: 0.0367 | 0.0490
Epoch 130/300, seasonal_1 Loss: 0.0442 | 0.0351
Epoch 131/300, seasonal_1 Loss: 0.0335 | 0.0372
Epoch 132/300, seasonal_1 Loss: 0.0288 | 0.0485
Epoch 133/300, seasonal_1 Loss: 0.0285 | 0.0511
Epoch 134/300, seasonal_1 Loss: 0.0294 | 0.0396
Epoch 135/300, seasonal_1 Loss: 0.0273 | 0.0415
Epoch 136/300, seasonal_1 Loss: 0.0278 | 0.0430
Epoch 137/300, seasonal_1 Loss: 0.0313 | 0.0343
Epoch 138/300, seasonal_1 Loss: 0.0280 | 0.0332
Epoch 139/300, seasonal_1 Loss: 0.0314 | 0.0432
Epoch 140/300, seasonal_1 Loss: 0.0281 | 0.0367
Epoch 141/300, seasonal_1 Loss: 0.0281 | 0.0407
Epoch 142/300, seasonal_1 Loss: 0.0281 | 0.0409
Epoch 143/300, seasonal_1 Loss: 0.0260 | 0.0397
Epoch 144/300, seasonal_1 Loss: 0.0266 | 0.0462
Epoch 145/300, seasonal_1 Loss: 0.0265 | 0.0512
Epoch 146/300, seasonal_1 Loss: 0.0259 | 0.0503
Epoch 147/300, seasonal_1 Loss: 0.0260 | 0.0490
Epoch 148/300, seasonal_1 Loss: 0.0269 | 0.0492
Epoch 149/300, seasonal_1 Loss: 0.0405 | 0.0483
Epoch 150/300, seasonal_1 Loss: 0.0346 | 0.0408
Epoch 151/300, seasonal_1 Loss: 0.0406 | 0.0508
Epoch 152/300, seasonal_1 Loss: 0.0349 | 0.0405
Epoch 153/300, seasonal_1 Loss: 0.0299 | 0.0483
Epoch 154/300, seasonal_1 Loss: 0.0278 | 0.0538
Epoch 155/300, seasonal_1 Loss: 0.0365 | 0.0421
Epoch 156/300, seasonal_1 Loss: 0.0426 | 0.0317
Epoch 157/300, seasonal_1 Loss: 0.0316 | 0.0463
Epoch 158/300, seasonal_1 Loss: 0.0392 | 0.0447
Epoch 159/300, seasonal_1 Loss: 0.0336 | 0.0443
Epoch 160/300, seasonal_1 Loss: 0.0317 | 0.0501
Epoch 161/300, seasonal_1 Loss: 0.0464 | 0.0500
Epoch 162/300, seasonal_1 Loss: 0.0362 | 0.0467
Epoch 163/300, seasonal_1 Loss: 0.0390 | 0.0574
Epoch 164/300, seasonal_1 Loss: 0.0358 | 0.0629
Epoch 165/300, seasonal_1 Loss: 0.0356 | 0.0641
Epoch 166/300, seasonal_1 Loss: 0.0317 | 0.0549
Epoch 167/300, seasonal_1 Loss: 0.0352 | 0.0621
Epoch 168/300, seasonal_1 Loss: 0.0344 | 0.0664
Epoch 169/300, seasonal_1 Loss: 0.0311 | 0.0631
Epoch 170/300, seasonal_1 Loss: 0.0340 | 0.0572
Epoch 171/300, seasonal_1 Loss: 0.0335 | 0.0541
Epoch 172/300, seasonal_1 Loss: 0.0302 | 0.0519
Epoch 173/300, seasonal_1 Loss: 0.0273 | 0.0601
Epoch 174/300, seasonal_1 Loss: 0.0268 | 0.0571
Epoch 175/300, seasonal_1 Loss: 0.0233 | 0.0487
Epoch 176/300, seasonal_1 Loss: 0.0237 | 0.0530
Epoch 177/300, seasonal_1 Loss: 0.0261 | 0.0539
Epoch 178/300, seasonal_1 Loss: 0.0227 | 0.0507
Epoch 179/300, seasonal_1 Loss: 0.0232 | 0.0405
Epoch 180/300, seasonal_1 Loss: 0.0227 | 0.0355
Epoch 181/300, seasonal_1 Loss: 0.0241 | 0.0388
Epoch 182/300, seasonal_1 Loss: 0.0234 | 0.0485
Epoch 183/300, seasonal_1 Loss: 0.0242 | 0.0508
Epoch 184/300, seasonal_1 Loss: 0.0230 | 0.0492
Epoch 185/300, seasonal_1 Loss: 0.0242 | 0.0403
Epoch 186/300, seasonal_1 Loss: 0.0253 | 0.0385
Epoch 187/300, seasonal_1 Loss: 0.0258 | 0.0528
Epoch 188/300, seasonal_1 Loss: 0.0271 | 0.0473
Epoch 189/300, seasonal_1 Loss: 0.0261 | 0.0353
Epoch 190/300, seasonal_1 Loss: 0.0262 | 0.0346
Epoch 191/300, seasonal_1 Loss: 0.0254 | 0.0362
Epoch 192/300, seasonal_1 Loss: 0.0248 | 0.0439
Epoch 193/300, seasonal_1 Loss: 0.0231 | 0.0536
Epoch 194/300, seasonal_1 Loss: 0.0279 | 0.0363
Epoch 195/300, seasonal_1 Loss: 0.0252 | 0.0409
Epoch 196/300, seasonal_1 Loss: 0.0297 | 0.0406
Epoch 197/300, seasonal_1 Loss: 0.0435 | 0.0450
Epoch 198/300, seasonal_1 Loss: 0.0340 | 0.0444
Epoch 199/300, seasonal_1 Loss: 0.0421 | 0.0618
Epoch 200/300, seasonal_1 Loss: 0.0436 | 0.0657
Epoch 201/300, seasonal_1 Loss: 0.0370 | 0.0569
Epoch 202/300, seasonal_1 Loss: 0.0272 | 0.0518
Epoch 203/300, seasonal_1 Loss: 0.0360 | 0.0514
Epoch 204/300, seasonal_1 Loss: 0.0400 | 0.0507
Epoch 205/300, seasonal_1 Loss: 0.0293 | 0.0353
Epoch 206/300, seasonal_1 Loss: 0.0346 | 0.0381
Epoch 207/300, seasonal_1 Loss: 0.0319 | 0.0390
Epoch 208/300, seasonal_1 Loss: 0.0248 | 0.0351
Epoch 209/300, seasonal_1 Loss: 0.0270 | 0.0345
Epoch 210/300, seasonal_1 Loss: 0.0245 | 0.0339
Epoch 211/300, seasonal_1 Loss: 0.0211 | 0.0340
Epoch 212/300, seasonal_1 Loss: 0.0200 | 0.0312
Epoch 213/300, seasonal_1 Loss: 0.0198 | 0.0317
Epoch 214/300, seasonal_1 Loss: 0.0186 | 0.0327
Epoch 215/300, seasonal_1 Loss: 0.0184 | 0.0339
Epoch 216/300, seasonal_1 Loss: 0.0184 | 0.0298
Epoch 217/300, seasonal_1 Loss: 0.0181 | 0.0355
Epoch 218/300, seasonal_1 Loss: 0.0180 | 0.0357
Epoch 219/300, seasonal_1 Loss: 0.0176 | 0.0381
Epoch 220/300, seasonal_1 Loss: 0.0178 | 0.0325
Epoch 221/300, seasonal_1 Loss: 0.0182 | 0.0357
Epoch 222/300, seasonal_1 Loss: 0.0185 | 0.0380
Epoch 223/300, seasonal_1 Loss: 0.0186 | 0.0473
Epoch 224/300, seasonal_1 Loss: 0.0195 | 0.0500
Epoch 225/300, seasonal_1 Loss: 0.0206 | 0.0637
Epoch 226/300, seasonal_1 Loss: 0.0210 | 0.0655
Epoch 227/300, seasonal_1 Loss: 0.0213 | 0.0497
Epoch 228/300, seasonal_1 Loss: 0.0195 | 0.0307
Epoch 229/300, seasonal_1 Loss: 0.0193 | 0.0395
Epoch 230/300, seasonal_1 Loss: 0.0193 | 0.0322
Epoch 231/300, seasonal_1 Loss: 0.0185 | 0.0363
Epoch 232/300, seasonal_1 Loss: 0.0187 | 0.0426
Epoch 233/300, seasonal_1 Loss: 0.0184 | 0.0433
Epoch 234/300, seasonal_1 Loss: 0.0272 | 0.0376
Epoch 235/300, seasonal_1 Loss: 0.0372 | 0.0417
Epoch 236/300, seasonal_1 Loss: 0.0258 | 0.0372
Epoch 237/300, seasonal_1 Loss: 0.0210 | 0.0318
Epoch 238/300, seasonal_1 Loss: 0.0208 | 0.0322
Epoch 239/300, seasonal_1 Loss: 0.0203 | 0.0319
Epoch 240/300, seasonal_1 Loss: 0.0201 | 0.0315
Epoch 241/300, seasonal_1 Loss: 0.0199 | 0.0307
Epoch 242/300, seasonal_1 Loss: 0.0193 | 0.0311
Epoch 243/300, seasonal_1 Loss: 0.0199 | 0.0324
Epoch 244/300, seasonal_1 Loss: 0.0202 | 0.0340
Epoch 245/300, seasonal_1 Loss: 0.0235 | 0.0351
Epoch 246/300, seasonal_1 Loss: 0.0337 | 0.0371
Epoch 247/300, seasonal_1 Loss: 0.0325 | 0.0346
Epoch 248/300, seasonal_1 Loss: 0.0311 | 0.0340
Epoch 249/300, seasonal_1 Loss: 0.0306 | 0.0504
Epoch 250/300, seasonal_1 Loss: 0.0391 | 0.0276
Epoch 251/300, seasonal_1 Loss: 0.0289 | 0.0391
Epoch 252/300, seasonal_1 Loss: 0.0449 | 0.0311
Epoch 253/300, seasonal_1 Loss: 0.0342 | 0.0327
Epoch 254/300, seasonal_1 Loss: 0.0304 | 0.0304
Epoch 255/300, seasonal_1 Loss: 0.0279 | 0.0310
Epoch 256/300, seasonal_1 Loss: 0.0277 | 0.0337
Epoch 257/300, seasonal_1 Loss: 0.0358 | 0.0451
Epoch 258/300, seasonal_1 Loss: 0.0427 | 0.0338
Epoch 259/300, seasonal_1 Loss: 0.0343 | 0.0328
Epoch 260/300, seasonal_1 Loss: 0.0305 | 0.0323
Epoch 261/300, seasonal_1 Loss: 0.0216 | 0.0315
Epoch 262/300, seasonal_1 Loss: 0.0160 | 0.0316
Epoch 263/300, seasonal_1 Loss: 0.0152 | 0.0313
Epoch 264/300, seasonal_1 Loss: 0.0148 | 0.0320
Epoch 265/300, seasonal_1 Loss: 0.0216 | 0.0352
Epoch 266/300, seasonal_1 Loss: 0.0466 | 0.0293
Epoch 267/300, seasonal_1 Loss: 0.0363 | 0.0322
Epoch 268/300, seasonal_1 Loss: 0.0338 | 0.0325
Epoch 269/300, seasonal_1 Loss: 0.0326 | 0.0308
Epoch 270/300, seasonal_1 Loss: 0.0367 | 0.0297
Epoch 271/300, seasonal_1 Loss: 0.0264 | 0.0427
Epoch 272/300, seasonal_1 Loss: 0.0410 | 0.0305
Epoch 273/300, seasonal_1 Loss: 0.0351 | 0.0292
Epoch 274/300, seasonal_1 Loss: 0.0298 | 0.0285
Epoch 275/300, seasonal_1 Loss: 0.0307 | 0.0375
Epoch 276/300, seasonal_1 Loss: 0.0476 | 0.0280
Epoch 277/300, seasonal_1 Loss: 0.0384 | 0.0283
Epoch 278/300, seasonal_1 Loss: 0.0343 | 0.0269
Epoch 279/300, seasonal_1 Loss: 0.0329 | 0.0269
Epoch 280/300, seasonal_1 Loss: 0.0324 | 0.0268
Epoch 281/300, seasonal_1 Loss: 0.0316 | 0.0267
Epoch 282/300, seasonal_1 Loss: 0.0290 | 0.0267
Epoch 283/300, seasonal_1 Loss: 0.0232 | 0.0293
Epoch 284/300, seasonal_1 Loss: 0.0399 | 0.0399
Epoch 285/300, seasonal_1 Loss: 0.0397 | 0.0342
Epoch 286/300, seasonal_1 Loss: 0.0341 | 0.0330
Epoch 287/300, seasonal_1 Loss: 0.0324 | 0.0327
Epoch 288/300, seasonal_1 Loss: 0.0310 | 0.0325
Epoch 289/300, seasonal_1 Loss: 0.0302 | 0.0300
Epoch 290/300, seasonal_1 Loss: 0.0338 | 0.0282
Epoch 291/300, seasonal_1 Loss: 0.0294 | 0.0294
Epoch 292/300, seasonal_1 Loss: 0.0341 | 0.0462
Epoch 293/300, seasonal_1 Loss: 0.0425 | 0.0281
Epoch 294/300, seasonal_1 Loss: 0.0350 | 0.0291
Epoch 295/300, seasonal_1 Loss: 0.0329 | 0.0289
Epoch 296/300, seasonal_1 Loss: 0.0319 | 0.0294
Epoch 297/300, seasonal_1 Loss: 0.0289 | 0.0304
Epoch 298/300, seasonal_1 Loss: 0.0206 | 0.0285
Epoch 299/300, seasonal_1 Loss: 0.0365 | 0.0309
Epoch 300/300, seasonal_1 Loss: 0.0204 | 0.0299
Training seasonal_2 component with params: {'observation_period_num': 11, 'train_rates': 0.924623602059246, 'learning_rate': 0.0007972264601018071, 'batch_size': 82, 'step_size': 15, 'gamma': 0.7508138514663495}
Epoch 1/300, seasonal_2 Loss: 2.0224 | 1.3479
Epoch 2/300, seasonal_2 Loss: 1.0274 | 1.4746
Epoch 3/300, seasonal_2 Loss: 1.0318 | 1.3582
Epoch 4/300, seasonal_2 Loss: 0.9644 | 1.4661
Epoch 5/300, seasonal_2 Loss: 0.9344 | 1.3772
Epoch 6/300, seasonal_2 Loss: 0.9824 | 1.3557
Epoch 7/300, seasonal_2 Loss: 1.0101 | 1.3756
Epoch 8/300, seasonal_2 Loss: 0.9868 | 1.4293
Epoch 9/300, seasonal_2 Loss: 0.9627 | 1.4529
Epoch 10/300, seasonal_2 Loss: 0.9588 | 1.4816
Epoch 11/300, seasonal_2 Loss: 0.9365 | 1.4298
Epoch 12/300, seasonal_2 Loss: 0.9563 | 1.3809
Epoch 13/300, seasonal_2 Loss: 1.0261 | 1.3503
Epoch 14/300, seasonal_2 Loss: 1.1051 | 1.3549
Epoch 15/300, seasonal_2 Loss: 1.1586 | 1.3905
Epoch 16/300, seasonal_2 Loss: 1.1848 | 1.6828
Epoch 17/300, seasonal_2 Loss: 1.0433 | 1.7421
Epoch 18/300, seasonal_2 Loss: 1.0198 | 1.7480
Epoch 19/300, seasonal_2 Loss: 1.0167 | 1.7579
Epoch 20/300, seasonal_2 Loss: 1.0127 | 1.7526
Epoch 21/300, seasonal_2 Loss: 1.0126 | 1.7733
Epoch 22/300, seasonal_2 Loss: 1.0076 | 1.7837
Epoch 23/300, seasonal_2 Loss: 1.0035 | 1.7911
Epoch 24/300, seasonal_2 Loss: 0.9916 | 1.8744
Epoch 25/300, seasonal_2 Loss: 0.9703 | 1.8623
Epoch 26/300, seasonal_2 Loss: 0.9731 | 1.8648
Epoch 27/300, seasonal_2 Loss: 0.9724 | 1.8655
Epoch 28/300, seasonal_2 Loss: 0.9721 | 1.8665
Epoch 29/300, seasonal_2 Loss: 0.9717 | 1.8676
Epoch 30/300, seasonal_2 Loss: 0.9713 | 1.8688
Epoch 31/300, seasonal_2 Loss: 0.9628 | 1.9091
Epoch 32/300, seasonal_2 Loss: 0.9549 | 1.9099
Epoch 33/300, seasonal_2 Loss: 0.9549 | 1.9083
Epoch 34/300, seasonal_2 Loss: 0.9552 | 1.9085
Epoch 35/300, seasonal_2 Loss: 0.9552 | 1.9084
Epoch 36/300, seasonal_2 Loss: 0.9552 | 1.9084
Epoch 37/300, seasonal_2 Loss: 0.9552 | 1.9084
Epoch 38/300, seasonal_2 Loss: 0.9552 | 1.9086
Epoch 39/300, seasonal_2 Loss: 0.9487 | 1.9302
Epoch 40/300, seasonal_2 Loss: 0.9454 | 1.9352
Epoch 41/300, seasonal_2 Loss: 0.9447 | 1.9337
Epoch 42/300, seasonal_2 Loss: 0.9450 | 1.9334
Epoch 43/300, seasonal_2 Loss: 0.9451 | 1.9332
Epoch 44/300, seasonal_2 Loss: 0.9452 | 1.9330
Epoch 45/300, seasonal_2 Loss: 0.9453 | 1.9329
Epoch 46/300, seasonal_2 Loss: 0.9402 | 1.9449
Epoch 47/300, seasonal_2 Loss: 0.9389 | 1.9504
Epoch 48/300, seasonal_2 Loss: 0.9382 | 1.9501
Epoch 49/300, seasonal_2 Loss: 0.9383 | 1.9497
Epoch 50/300, seasonal_2 Loss: 0.9385 | 1.9494
Epoch 51/300, seasonal_2 Loss: 0.9386 | 1.9492
Epoch 52/300, seasonal_2 Loss: 0.9387 | 1.9490
Epoch 53/300, seasonal_2 Loss: 0.9388 | 1.9488
Epoch 54/300, seasonal_2 Loss: 0.9348 | 1.9558
Epoch 55/300, seasonal_2 Loss: 0.9342 | 1.9604
Epoch 56/300, seasonal_2 Loss: 0.9337 | 1.9610
Epoch 57/300, seasonal_2 Loss: 0.9337 | 1.9608
Epoch 58/300, seasonal_2 Loss: 0.9338 | 1.9606
Epoch 59/300, seasonal_2 Loss: 0.9339 | 1.9604
Epoch 60/300, seasonal_2 Loss: 0.9340 | 1.9602
Epoch 61/300, seasonal_2 Loss: 0.9309 | 1.9643
Epoch 62/300, seasonal_2 Loss: 0.9307 | 1.9677
Epoch 63/300, seasonal_2 Loss: 0.9304 | 1.9687
Epoch 64/300, seasonal_2 Loss: 0.9303 | 1.9689
Epoch 65/300, seasonal_2 Loss: 0.9304 | 1.9688
Epoch 66/300, seasonal_2 Loss: 0.9304 | 1.9686
Epoch 67/300, seasonal_2 Loss: 0.9305 | 1.9685
Epoch 68/300, seasonal_2 Loss: 0.9305 | 1.9683
Epoch 69/300, seasonal_2 Loss: 0.9281 | 1.9708
Epoch 70/300, seasonal_2 Loss: 0.9280 | 1.9732
Epoch 71/300, seasonal_2 Loss: 0.9279 | 1.9742
Epoch 72/300, seasonal_2 Loss: 0.9278 | 1.9746
Epoch 73/300, seasonal_2 Loss: 0.9278 | 1.9746
Epoch 74/300, seasonal_2 Loss: 0.9279 | 1.9746
Epoch 75/300, seasonal_2 Loss: 0.9279 | 1.9745
Epoch 76/300, seasonal_2 Loss: 0.9260 | 1.9760
Epoch 77/300, seasonal_2 Loss: 0.9260 | 1.9776
Epoch 78/300, seasonal_2 Loss: 0.9259 | 1.9785
Epoch 79/300, seasonal_2 Loss: 0.9259 | 1.9789
Epoch 80/300, seasonal_2 Loss: 0.9259 | 1.9791
Epoch 81/300, seasonal_2 Loss: 0.9259 | 1.9791
Epoch 82/300, seasonal_2 Loss: 0.9259 | 1.9791
Epoch 83/300, seasonal_2 Loss: 0.9259 | 1.9791
Epoch 84/300, seasonal_2 Loss: 0.9244 | 1.9799
Epoch 85/300, seasonal_2 Loss: 0.9245 | 1.9810
Epoch 86/300, seasonal_2 Loss: 0.9244 | 1.9817
Epoch 87/300, seasonal_2 Loss: 0.9244 | 1.9821
Epoch 88/300, seasonal_2 Loss: 0.9244 | 1.9823
Epoch 89/300, seasonal_2 Loss: 0.9244 | 1.9825
Epoch 90/300, seasonal_2 Loss: 0.9244 | 1.9825
Epoch 91/300, seasonal_2 Loss: 0.9232 | 1.9831
Epoch 92/300, seasonal_2 Loss: 0.9233 | 1.9838
Epoch 93/300, seasonal_2 Loss: 0.9232 | 1.9843
Epoch 94/300, seasonal_2 Loss: 0.9232 | 1.9846
Epoch 95/300, seasonal_2 Loss: 0.9232 | 1.9849
Epoch 96/300, seasonal_2 Loss: 0.9232 | 1.9850
Epoch 97/300, seasonal_2 Loss: 0.9232 | 1.9851
Epoch 98/300, seasonal_2 Loss: 0.9232 | 1.9851
Epoch 99/300, seasonal_2 Loss: 0.9223 | 1.9855
Epoch 100/300, seasonal_2 Loss: 0.9223 | 1.9860
Epoch 101/300, seasonal_2 Loss: 0.9223 | 1.9863
Epoch 102/300, seasonal_2 Loss: 0.9223 | 1.9866
Epoch 103/300, seasonal_2 Loss: 0.9223 | 1.9868
Epoch 104/300, seasonal_2 Loss: 0.9223 | 1.9869
Epoch 105/300, seasonal_2 Loss: 0.9223 | 1.9870
Epoch 106/300, seasonal_2 Loss: 0.9216 | 1.9873
Epoch 107/300, seasonal_2 Loss: 0.9216 | 1.9876
Epoch 108/300, seasonal_2 Loss: 0.9216 | 1.9879
Epoch 109/300, seasonal_2 Loss: 0.9216 | 1.9881
Epoch 110/300, seasonal_2 Loss: 0.9216 | 1.9883
Epoch 111/300, seasonal_2 Loss: 0.9216 | 1.9884
Epoch 112/300, seasonal_2 Loss: 0.9216 | 1.9885
Epoch 113/300, seasonal_2 Loss: 0.9216 | 1.9886
Epoch 114/300, seasonal_2 Loss: 0.9211 | 1.9888
Epoch 115/300, seasonal_2 Loss: 0.9211 | 1.9890
Epoch 116/300, seasonal_2 Loss: 0.9211 | 1.9892
Epoch 117/300, seasonal_2 Loss: 0.9211 | 1.9893
Epoch 118/300, seasonal_2 Loss: 0.9211 | 1.9895
Epoch 119/300, seasonal_2 Loss: 0.9211 | 1.9896
Epoch 120/300, seasonal_2 Loss: 0.9211 | 1.9897
Epoch 121/300, seasonal_2 Loss: 0.9207 | 1.9898
Epoch 122/300, seasonal_2 Loss: 0.9207 | 1.9900
Epoch 123/300, seasonal_2 Loss: 0.9207 | 1.9901
Epoch 124/300, seasonal_2 Loss: 0.9207 | 1.9902
Epoch 125/300, seasonal_2 Loss: 0.9207 | 1.9903
Epoch 126/300, seasonal_2 Loss: 0.9207 | 1.9904
Epoch 127/300, seasonal_2 Loss: 0.9207 | 1.9905
Epoch 128/300, seasonal_2 Loss: 0.9207 | 1.9906
Epoch 129/300, seasonal_2 Loss: 0.9203 | 1.9907
Epoch 130/300, seasonal_2 Loss: 0.9203 | 1.9908
Epoch 131/300, seasonal_2 Loss: 0.9203 | 1.9909
Epoch 132/300, seasonal_2 Loss: 0.9203 | 1.9910
Epoch 133/300, seasonal_2 Loss: 0.9203 | 1.9911
Epoch 134/300, seasonal_2 Loss: 0.9203 | 1.9911
Epoch 135/300, seasonal_2 Loss: 0.9203 | 1.9912
Epoch 136/300, seasonal_2 Loss: 0.9201 | 1.9913
Epoch 137/300, seasonal_2 Loss: 0.9201 | 1.9914
Epoch 138/300, seasonal_2 Loss: 0.9201 | 1.9914
Epoch 139/300, seasonal_2 Loss: 0.9201 | 1.9915
Epoch 140/300, seasonal_2 Loss: 0.9201 | 1.9916
Epoch 141/300, seasonal_2 Loss: 0.9201 | 1.9916
Epoch 142/300, seasonal_2 Loss: 0.9201 | 1.9917
Epoch 143/300, seasonal_2 Loss: 0.9201 | 1.9918
Epoch 144/300, seasonal_2 Loss: 0.9199 | 1.9918
Epoch 145/300, seasonal_2 Loss: 0.9199 | 1.9919
Epoch 146/300, seasonal_2 Loss: 0.9199 | 1.9919
Epoch 147/300, seasonal_2 Loss: 0.9199 | 1.9920
Epoch 148/300, seasonal_2 Loss: 0.9199 | 1.9920
Epoch 149/300, seasonal_2 Loss: 0.9199 | 1.9921
Epoch 150/300, seasonal_2 Loss: 0.9199 | 1.9921
Epoch 151/300, seasonal_2 Loss: 0.9198 | 1.9922
Epoch 152/300, seasonal_2 Loss: 0.9198 | 1.9922
Epoch 153/300, seasonal_2 Loss: 0.9198 | 1.9922
Epoch 154/300, seasonal_2 Loss: 0.9198 | 1.9923
Epoch 155/300, seasonal_2 Loss: 0.9198 | 1.9923
Epoch 156/300, seasonal_2 Loss: 0.9198 | 1.9924
Epoch 157/300, seasonal_2 Loss: 0.9198 | 1.9924
Epoch 158/300, seasonal_2 Loss: 0.9198 | 1.9924
Epoch 159/300, seasonal_2 Loss: 0.9196 | 1.9925
Epoch 160/300, seasonal_2 Loss: 0.9196 | 1.9925
Epoch 161/300, seasonal_2 Loss: 0.9196 | 1.9925
Epoch 162/300, seasonal_2 Loss: 0.9196 | 1.9926
Epoch 163/300, seasonal_2 Loss: 0.9196 | 1.9926
Epoch 164/300, seasonal_2 Loss: 0.9196 | 1.9926
Epoch 165/300, seasonal_2 Loss: 0.9196 | 1.9926
Epoch 166/300, seasonal_2 Loss: 0.9195 | 1.9927
Epoch 167/300, seasonal_2 Loss: 0.9195 | 1.9927
Epoch 168/300, seasonal_2 Loss: 0.9195 | 1.9927
Epoch 169/300, seasonal_2 Loss: 0.9195 | 1.9927
Epoch 170/300, seasonal_2 Loss: 0.9195 | 1.9928
Epoch 171/300, seasonal_2 Loss: 0.9195 | 1.9928
Epoch 172/300, seasonal_2 Loss: 0.9195 | 1.9928
Epoch 173/300, seasonal_2 Loss: 0.9195 | 1.9928
Epoch 174/300, seasonal_2 Loss: 0.9195 | 1.9928
Epoch 175/300, seasonal_2 Loss: 0.9195 | 1.9929
Epoch 176/300, seasonal_2 Loss: 0.9195 | 1.9929
Epoch 177/300, seasonal_2 Loss: 0.9195 | 1.9929
Epoch 178/300, seasonal_2 Loss: 0.9195 | 1.9929
Epoch 179/300, seasonal_2 Loss: 0.9195 | 1.9929
Epoch 180/300, seasonal_2 Loss: 0.9195 | 1.9930
Epoch 181/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 182/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 183/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 184/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 185/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 186/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 187/300, seasonal_2 Loss: 0.9194 | 1.9930
Epoch 188/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 189/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 190/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 191/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 192/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 193/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 194/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 195/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 196/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 197/300, seasonal_2 Loss: 0.9194 | 1.9931
Epoch 198/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 199/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 200/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 201/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 202/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 203/300, seasonal_2 Loss: 0.9194 | 1.9932
Epoch 204/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 205/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 206/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 207/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 208/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 209/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 210/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 211/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 212/300, seasonal_2 Loss: 0.9193 | 1.9932
Epoch 213/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 214/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 215/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 216/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 217/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 218/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 219/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 220/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 221/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 222/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 223/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 224/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 225/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 226/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 227/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 228/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 229/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 230/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 231/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 232/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 233/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 234/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 235/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 236/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 237/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 238/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 239/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 240/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 241/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 242/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 243/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 244/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 245/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 246/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 247/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 248/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 249/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 250/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 251/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 252/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 253/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 254/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 255/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 256/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 257/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 258/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 259/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 260/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 261/300, seasonal_2 Loss: 0.9193 | 1.9933
Epoch 262/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 263/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 264/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 265/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 266/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 267/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 268/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 269/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 270/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 271/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 272/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 273/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 274/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 275/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 276/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 277/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 278/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 279/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 280/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 281/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 282/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 283/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 284/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 285/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 286/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 287/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 288/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 289/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 290/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 291/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 292/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 293/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 294/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 295/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 296/300, seasonal_2 Loss: 0.9193 | 1.9934
Epoch 297/300, seasonal_2 Loss: 0.9193 | 1.9934
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.8208434604536282, 'learning_rate': 0.0005375314758159338, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8729664752799243}
Epoch 1/300, seasonal_3 Loss: 1.5699 | 0.3187
Epoch 2/300, seasonal_3 Loss: 0.2389 | 0.2693
Epoch 3/300, seasonal_3 Loss: 0.1705 | 0.1973
Epoch 4/300, seasonal_3 Loss: 0.1715 | 0.2692
Epoch 5/300, seasonal_3 Loss: 0.2799 | 0.1754
Epoch 6/300, seasonal_3 Loss: 0.3103 | 0.2767
Epoch 7/300, seasonal_3 Loss: 0.4204 | 0.2818
Epoch 8/300, seasonal_3 Loss: 0.2699 | 0.1928
Epoch 9/300, seasonal_3 Loss: 0.2071 | 0.1484
Epoch 10/300, seasonal_3 Loss: 0.2476 | 0.1363
Epoch 11/300, seasonal_3 Loss: 0.2435 | 0.1654
Epoch 12/300, seasonal_3 Loss: 0.2195 | 0.1676
Epoch 13/300, seasonal_3 Loss: 0.2042 | 0.1063
Epoch 14/300, seasonal_3 Loss: 0.2186 | 0.1011
Epoch 15/300, seasonal_3 Loss: 0.1750 | 0.0749
Epoch 16/300, seasonal_3 Loss: 0.1647 | 0.0732
Epoch 17/300, seasonal_3 Loss: 0.1579 | 0.0821
Epoch 18/300, seasonal_3 Loss: 0.1600 | 0.0955
Epoch 19/300, seasonal_3 Loss: 0.1829 | 0.1678
Epoch 20/300, seasonal_3 Loss: 0.1958 | 0.1264
Epoch 21/300, seasonal_3 Loss: 0.1901 | 0.0796
Epoch 22/300, seasonal_3 Loss: 0.2166 | 0.0783
Epoch 23/300, seasonal_3 Loss: 0.2033 | 0.0820
Epoch 24/300, seasonal_3 Loss: 0.2265 | 0.0758
Epoch 25/300, seasonal_3 Loss: 0.1725 | 0.0704
Epoch 26/300, seasonal_3 Loss: 0.1812 | 0.0917
Epoch 27/300, seasonal_3 Loss: 0.1933 | 0.1035
Epoch 28/300, seasonal_3 Loss: 0.2390 | 0.1078
Epoch 29/300, seasonal_3 Loss: 0.1764 | 0.1022
Epoch 30/300, seasonal_3 Loss: 0.1601 | 0.0717
Epoch 31/300, seasonal_3 Loss: 0.1465 | 0.0609
Epoch 32/300, seasonal_3 Loss: 0.1365 | 0.0776
Epoch 33/300, seasonal_3 Loss: 0.1606 | 0.0849
Epoch 34/300, seasonal_3 Loss: 0.1618 | 0.0821
Epoch 35/300, seasonal_3 Loss: 0.1530 | 0.0822
Epoch 36/300, seasonal_3 Loss: 0.1339 | 0.0940
Epoch 37/300, seasonal_3 Loss: 0.1184 | 0.0879
Epoch 38/300, seasonal_3 Loss: 0.1051 | 0.0610
Epoch 39/300, seasonal_3 Loss: 0.1001 | 0.0501
Epoch 40/300, seasonal_3 Loss: 0.1180 | 0.0580
Epoch 41/300, seasonal_3 Loss: 0.1068 | 0.0510
Epoch 42/300, seasonal_3 Loss: 0.0943 | 0.0448
Epoch 43/300, seasonal_3 Loss: 0.0959 | 0.0488
Epoch 44/300, seasonal_3 Loss: 0.0992 | 0.0505
Epoch 45/300, seasonal_3 Loss: 0.0997 | 0.0604
Epoch 46/300, seasonal_3 Loss: 0.1022 | 0.0796
Epoch 47/300, seasonal_3 Loss: 0.1004 | 0.1080
Epoch 48/300, seasonal_3 Loss: 0.0956 | 0.0680
Epoch 49/300, seasonal_3 Loss: 0.0990 | 0.0617
Epoch 50/300, seasonal_3 Loss: 0.1058 | 0.0569
Epoch 51/300, seasonal_3 Loss: 0.1107 | 0.0582
Epoch 52/300, seasonal_3 Loss: 0.0959 | 0.0452
Epoch 53/300, seasonal_3 Loss: 0.0886 | 0.0531
Epoch 54/300, seasonal_3 Loss: 0.0828 | 0.0546
Epoch 55/300, seasonal_3 Loss: 0.0837 | 0.0425
Epoch 56/300, seasonal_3 Loss: 0.0987 | 0.0495
Epoch 57/300, seasonal_3 Loss: 0.1008 | 0.0491
Epoch 58/300, seasonal_3 Loss: 0.0839 | 0.0505
Epoch 59/300, seasonal_3 Loss: 0.0789 | 0.0499
Epoch 60/300, seasonal_3 Loss: 0.0783 | 0.0577
Epoch 61/300, seasonal_3 Loss: 0.0789 | 0.0617
Epoch 62/300, seasonal_3 Loss: 0.0893 | 0.0489
Epoch 63/300, seasonal_3 Loss: 0.0860 | 0.0436
Epoch 64/300, seasonal_3 Loss: 0.0925 | 0.0440
Epoch 65/300, seasonal_3 Loss: 0.0866 | 0.0402
Epoch 66/300, seasonal_3 Loss: 0.0762 | 0.0452
Epoch 67/300, seasonal_3 Loss: 0.0813 | 0.0504
Epoch 68/300, seasonal_3 Loss: 0.0846 | 0.0477
Epoch 69/300, seasonal_3 Loss: 0.0946 | 0.0463
Epoch 70/300, seasonal_3 Loss: 0.0937 | 0.0597
Epoch 71/300, seasonal_3 Loss: 0.0840 | 0.0690
Epoch 72/300, seasonal_3 Loss: 0.0808 | 0.0426
Epoch 73/300, seasonal_3 Loss: 0.0873 | 0.0535
Epoch 74/300, seasonal_3 Loss: 0.0910 | 0.0432
Epoch 75/300, seasonal_3 Loss: 0.0889 | 0.0414
Epoch 76/300, seasonal_3 Loss: 0.0788 | 0.0443
Epoch 77/300, seasonal_3 Loss: 0.0763 | 0.0464
Epoch 78/300, seasonal_3 Loss: 0.0932 | 0.0498
Epoch 79/300, seasonal_3 Loss: 0.0854 | 0.0498
Epoch 80/300, seasonal_3 Loss: 0.0836 | 0.0472
Epoch 81/300, seasonal_3 Loss: 0.0765 | 0.0472
Epoch 82/300, seasonal_3 Loss: 0.0730 | 0.0395
Epoch 83/300, seasonal_3 Loss: 0.0703 | 0.0358
Epoch 84/300, seasonal_3 Loss: 0.0692 | 0.0364
Epoch 85/300, seasonal_3 Loss: 0.0671 | 0.0367
Epoch 86/300, seasonal_3 Loss: 0.0668 | 0.0402
Epoch 87/300, seasonal_3 Loss: 0.0699 | 0.0486
Epoch 88/300, seasonal_3 Loss: 0.0682 | 0.0403
Epoch 89/300, seasonal_3 Loss: 0.0651 | 0.0361
Epoch 90/300, seasonal_3 Loss: 0.0643 | 0.0342
Epoch 91/300, seasonal_3 Loss: 0.0642 | 0.0340
Epoch 92/300, seasonal_3 Loss: 0.0629 | 0.0350
Epoch 93/300, seasonal_3 Loss: 0.0619 | 0.0368
Epoch 94/300, seasonal_3 Loss: 0.0619 | 0.0345
Epoch 95/300, seasonal_3 Loss: 0.0625 | 0.0344
Epoch 96/300, seasonal_3 Loss: 0.0642 | 0.0384
Epoch 97/300, seasonal_3 Loss: 0.0642 | 0.0343
Epoch 98/300, seasonal_3 Loss: 0.0631 | 0.0354
Epoch 99/300, seasonal_3 Loss: 0.0644 | 0.0363
Epoch 100/300, seasonal_3 Loss: 0.0615 | 0.0358
Epoch 101/300, seasonal_3 Loss: 0.0612 | 0.0339
Epoch 102/300, seasonal_3 Loss: 0.0591 | 0.0336
Epoch 103/300, seasonal_3 Loss: 0.0566 | 0.0328
Epoch 104/300, seasonal_3 Loss: 0.0546 | 0.0335
Epoch 105/300, seasonal_3 Loss: 0.0611 | 0.0331
Epoch 106/300, seasonal_3 Loss: 0.0615 | 0.0329
Epoch 107/300, seasonal_3 Loss: 0.0601 | 0.0314
Epoch 108/300, seasonal_3 Loss: 0.0596 | 0.0333
Epoch 109/300, seasonal_3 Loss: 0.0595 | 0.0325
Epoch 110/300, seasonal_3 Loss: 0.0590 | 0.0319
Epoch 111/300, seasonal_3 Loss: 0.0588 | 0.0304
Epoch 112/300, seasonal_3 Loss: 0.0591 | 0.0310
Epoch 113/300, seasonal_3 Loss: 0.0597 | 0.0330
Epoch 114/300, seasonal_3 Loss: 0.0603 | 0.0335
Epoch 115/300, seasonal_3 Loss: 0.0586 | 0.0332
Epoch 116/300, seasonal_3 Loss: 0.0575 | 0.0311
Epoch 117/300, seasonal_3 Loss: 0.0571 | 0.0302
Epoch 118/300, seasonal_3 Loss: 0.0573 | 0.0298
Epoch 119/300, seasonal_3 Loss: 0.0568 | 0.0310
Epoch 120/300, seasonal_3 Loss: 0.0563 | 0.0329
Epoch 121/300, seasonal_3 Loss: 0.0554 | 0.0332
Epoch 122/300, seasonal_3 Loss: 0.0516 | 0.0323
Epoch 123/300, seasonal_3 Loss: 0.0590 | 0.0308
Epoch 124/300, seasonal_3 Loss: 0.0572 | 0.0313
Epoch 125/300, seasonal_3 Loss: 0.0509 | 0.0318
Epoch 126/300, seasonal_3 Loss: 0.0566 | 0.0300
Epoch 127/300, seasonal_3 Loss: 0.0502 | 0.0316
Epoch 128/300, seasonal_3 Loss: 0.0503 | 0.0306
Epoch 129/300, seasonal_3 Loss: 0.0522 | 0.0316
Epoch 130/300, seasonal_3 Loss: 0.0645 | 0.0358
Epoch 131/300, seasonal_3 Loss: 0.0644 | 0.0309
Epoch 132/300, seasonal_3 Loss: 0.0608 | 0.0323
Epoch 133/300, seasonal_3 Loss: 0.0579 | 0.0317
Epoch 134/300, seasonal_3 Loss: 0.0576 | 0.0328
Epoch 135/300, seasonal_3 Loss: 0.0579 | 0.0331
Epoch 136/300, seasonal_3 Loss: 0.0569 | 0.0303
Epoch 137/300, seasonal_3 Loss: 0.0562 | 0.0308
Epoch 138/300, seasonal_3 Loss: 0.0560 | 0.0308
Epoch 139/300, seasonal_3 Loss: 0.0550 | 0.0312
Epoch 140/300, seasonal_3 Loss: 0.0548 | 0.0316
Epoch 141/300, seasonal_3 Loss: 0.0556 | 0.0324
Epoch 142/300, seasonal_3 Loss: 0.0548 | 0.0316
Epoch 143/300, seasonal_3 Loss: 0.0541 | 0.0312
Epoch 144/300, seasonal_3 Loss: 0.0537 | 0.0307
Epoch 145/300, seasonal_3 Loss: 0.0535 | 0.0310
Epoch 146/300, seasonal_3 Loss: 0.0535 | 0.0325
Epoch 147/300, seasonal_3 Loss: 0.0543 | 0.0343
Epoch 148/300, seasonal_3 Loss: 0.0526 | 0.0330
Epoch 149/300, seasonal_3 Loss: 0.0508 | 0.0309
Epoch 150/300, seasonal_3 Loss: 0.0533 | 0.0306
Epoch 151/300, seasonal_3 Loss: 0.0487 | 0.0318
Epoch 152/300, seasonal_3 Loss: 0.0494 | 0.0318
Epoch 153/300, seasonal_3 Loss: 0.0499 | 0.0349
Epoch 154/300, seasonal_3 Loss: 0.0591 | 0.0306
Epoch 155/300, seasonal_3 Loss: 0.0555 | 0.0301
Epoch 156/300, seasonal_3 Loss: 0.0530 | 0.0306
Epoch 157/300, seasonal_3 Loss: 0.0478 | 0.0301
Epoch 158/300, seasonal_3 Loss: 0.0452 | 0.0316
Epoch 159/300, seasonal_3 Loss: 0.0448 | 0.0307
Epoch 160/300, seasonal_3 Loss: 0.0445 | 0.0307
Epoch 161/300, seasonal_3 Loss: 0.0442 | 0.0308
Epoch 162/300, seasonal_3 Loss: 0.0442 | 0.0315
Epoch 163/300, seasonal_3 Loss: 0.0441 | 0.0311
Epoch 164/300, seasonal_3 Loss: 0.0438 | 0.0308
Epoch 165/300, seasonal_3 Loss: 0.0441 | 0.0313
Epoch 166/300, seasonal_3 Loss: 0.0444 | 0.0319
Epoch 167/300, seasonal_3 Loss: 0.0437 | 0.0319
Epoch 168/300, seasonal_3 Loss: 0.0436 | 0.0308
Epoch 169/300, seasonal_3 Loss: 0.0436 | 0.0309
Epoch 170/300, seasonal_3 Loss: 0.0434 | 0.0323
Epoch 171/300, seasonal_3 Loss: 0.0436 | 0.0318
Epoch 172/300, seasonal_3 Loss: 0.0431 | 0.0311
Epoch 173/300, seasonal_3 Loss: 0.0436 | 0.0310
Epoch 174/300, seasonal_3 Loss: 0.0431 | 0.0319
Epoch 175/300, seasonal_3 Loss: 0.0436 | 0.0327
Epoch 176/300, seasonal_3 Loss: 0.0434 | 0.0314
Epoch 177/300, seasonal_3 Loss: 0.0436 | 0.0310
Epoch 178/300, seasonal_3 Loss: 0.0430 | 0.0320
Epoch 179/300, seasonal_3 Loss: 0.0429 | 0.0323
Epoch 180/300, seasonal_3 Loss: 0.0425 | 0.0315
Epoch 181/300, seasonal_3 Loss: 0.0423 | 0.0315
Epoch 182/300, seasonal_3 Loss: 0.0422 | 0.0325
Epoch 183/300, seasonal_3 Loss: 0.0421 | 0.0322
Epoch 184/300, seasonal_3 Loss: 0.0419 | 0.0319
Epoch 185/300, seasonal_3 Loss: 0.0418 | 0.0320
Epoch 186/300, seasonal_3 Loss: 0.0417 | 0.0323
Epoch 187/300, seasonal_3 Loss: 0.0419 | 0.0325
Epoch 188/300, seasonal_3 Loss: 0.0416 | 0.0325
Epoch 189/300, seasonal_3 Loss: 0.0417 | 0.0321
Epoch 190/300, seasonal_3 Loss: 0.0416 | 0.0329
Epoch 191/300, seasonal_3 Loss: 0.0417 | 0.0330
Epoch 192/300, seasonal_3 Loss: 0.0415 | 0.0323
Epoch 193/300, seasonal_3 Loss: 0.0416 | 0.0321
Epoch 194/300, seasonal_3 Loss: 0.0415 | 0.0333
Epoch 195/300, seasonal_3 Loss: 0.0418 | 0.0337
Epoch 196/300, seasonal_3 Loss: 0.0413 | 0.0330
Epoch 197/300, seasonal_3 Loss: 0.0417 | 0.0327
Epoch 198/300, seasonal_3 Loss: 0.0413 | 0.0332
Epoch 199/300, seasonal_3 Loss: 0.0415 | 0.0330
Epoch 200/300, seasonal_3 Loss: 0.0413 | 0.0330
Epoch 201/300, seasonal_3 Loss: 0.0414 | 0.0325
Epoch 202/300, seasonal_3 Loss: 0.0410 | 0.0334
Epoch 203/300, seasonal_3 Loss: 0.0412 | 0.0330
Epoch 204/300, seasonal_3 Loss: 0.0411 | 0.0326
Epoch 205/300, seasonal_3 Loss: 0.0408 | 0.0328
Epoch 206/300, seasonal_3 Loss: 0.0407 | 0.0337
Epoch 207/300, seasonal_3 Loss: 0.0404 | 0.0328
Epoch 208/300, seasonal_3 Loss: 0.0404 | 0.0330
Epoch 209/300, seasonal_3 Loss: 0.0402 | 0.0339
Epoch 210/300, seasonal_3 Loss: 0.0402 | 0.0336
Epoch 211/300, seasonal_3 Loss: 0.0401 | 0.0332
Epoch 212/300, seasonal_3 Loss: 0.0400 | 0.0339
Epoch 213/300, seasonal_3 Loss: 0.0399 | 0.0340
Epoch 214/300, seasonal_3 Loss: 0.0398 | 0.0338
Epoch 215/300, seasonal_3 Loss: 0.0399 | 0.0336
Epoch 216/300, seasonal_3 Loss: 0.0397 | 0.0340
Epoch 217/300, seasonal_3 Loss: 0.0400 | 0.0341
Epoch 218/300, seasonal_3 Loss: 0.0399 | 0.0340
Epoch 219/300, seasonal_3 Loss: 0.0400 | 0.0337
Epoch 220/300, seasonal_3 Loss: 0.0398 | 0.0345
Epoch 221/300, seasonal_3 Loss: 0.0399 | 0.0340
Epoch 222/300, seasonal_3 Loss: 0.0398 | 0.0334
Epoch 223/300, seasonal_3 Loss: 0.0396 | 0.0340
Epoch 224/300, seasonal_3 Loss: 0.0399 | 0.0348
Epoch 225/300, seasonal_3 Loss: 0.0395 | 0.0337
Epoch 226/300, seasonal_3 Loss: 0.0396 | 0.0340
Epoch 227/300, seasonal_3 Loss: 0.0394 | 0.0345
Epoch 228/300, seasonal_3 Loss: 0.0392 | 0.0339
Epoch 229/300, seasonal_3 Loss: 0.0392 | 0.0342
Epoch 230/300, seasonal_3 Loss: 0.0390 | 0.0348
Epoch 231/300, seasonal_3 Loss: 0.0390 | 0.0345
Epoch 232/300, seasonal_3 Loss: 0.0389 | 0.0343
Epoch 233/300, seasonal_3 Loss: 0.0388 | 0.0348
Epoch 234/300, seasonal_3 Loss: 0.0388 | 0.0348
Epoch 235/300, seasonal_3 Loss: 0.0387 | 0.0347
Epoch 236/300, seasonal_3 Loss: 0.0387 | 0.0348
Epoch 237/300, seasonal_3 Loss: 0.0386 | 0.0351
Epoch 238/300, seasonal_3 Loss: 0.0385 | 0.0349
Epoch 239/300, seasonal_3 Loss: 0.0386 | 0.0349
Epoch 240/300, seasonal_3 Loss: 0.0384 | 0.0353
Epoch 241/300, seasonal_3 Loss: 0.0385 | 0.0351
Epoch 242/300, seasonal_3 Loss: 0.0384 | 0.0349
Epoch 243/300, seasonal_3 Loss: 0.0383 | 0.0352
Epoch 244/300, seasonal_3 Loss: 0.0384 | 0.0354
Epoch 245/300, seasonal_3 Loss: 0.0383 | 0.0351
Epoch 246/300, seasonal_3 Loss: 0.0383 | 0.0353
Epoch 247/300, seasonal_3 Loss: 0.0383 | 0.0354
Epoch 248/300, seasonal_3 Loss: 0.0381 | 0.0351
Epoch 249/300, seasonal_3 Loss: 0.0382 | 0.0353
Epoch 250/300, seasonal_3 Loss: 0.0380 | 0.0357
Epoch 251/300, seasonal_3 Loss: 0.0380 | 0.0355
Epoch 252/300, seasonal_3 Loss: 0.0380 | 0.0353
Epoch 253/300, seasonal_3 Loss: 0.0379 | 0.0357
Epoch 254/300, seasonal_3 Loss: 0.0380 | 0.0356
Epoch 255/300, seasonal_3 Loss: 0.0378 | 0.0356
Epoch 256/300, seasonal_3 Loss: 0.0378 | 0.0357
Epoch 257/300, seasonal_3 Loss: 0.0378 | 0.0357
Epoch 258/300, seasonal_3 Loss: 0.0377 | 0.0356
Epoch 259/300, seasonal_3 Loss: 0.0376 | 0.0358
Epoch 260/300, seasonal_3 Loss: 0.0377 | 0.0359
Epoch 261/300, seasonal_3 Loss: 0.0375 | 0.0358
Epoch 262/300, seasonal_3 Loss: 0.0375 | 0.0359
Epoch 263/300, seasonal_3 Loss: 0.0375 | 0.0360
Epoch 264/300, seasonal_3 Loss: 0.0374 | 0.0359
Epoch 265/300, seasonal_3 Loss: 0.0374 | 0.0360
Epoch 266/300, seasonal_3 Loss: 0.0373 | 0.0362
Epoch 267/300, seasonal_3 Loss: 0.0373 | 0.0360
Epoch 268/300, seasonal_3 Loss: 0.0373 | 0.0361
Epoch 269/300, seasonal_3 Loss: 0.0372 | 0.0362
Epoch 270/300, seasonal_3 Loss: 0.0372 | 0.0363
Epoch 271/300, seasonal_3 Loss: 0.0372 | 0.0363
Epoch 272/300, seasonal_3 Loss: 0.0371 | 0.0363
Epoch 273/300, seasonal_3 Loss: 0.0371 | 0.0364
Epoch 274/300, seasonal_3 Loss: 0.0371 | 0.0364
Epoch 275/300, seasonal_3 Loss: 0.0370 | 0.0365
Epoch 276/300, seasonal_3 Loss: 0.0370 | 0.0365
Epoch 277/300, seasonal_3 Loss: 0.0369 | 0.0365
Epoch 278/300, seasonal_3 Loss: 0.0369 | 0.0366
Epoch 279/300, seasonal_3 Loss: 0.0369 | 0.0366
Epoch 280/300, seasonal_3 Loss: 0.0368 | 0.0366
Epoch 281/300, seasonal_3 Loss: 0.0368 | 0.0367
Epoch 282/300, seasonal_3 Loss: 0.0368 | 0.0367
Epoch 283/300, seasonal_3 Loss: 0.0367 | 0.0367
Epoch 284/300, seasonal_3 Loss: 0.0367 | 0.0368
Epoch 285/300, seasonal_3 Loss: 0.0367 | 0.0368
Epoch 286/300, seasonal_3 Loss: 0.0366 | 0.0368
Epoch 287/300, seasonal_3 Loss: 0.0366 | 0.0369
Epoch 288/300, seasonal_3 Loss: 0.0366 | 0.0369
Epoch 289/300, seasonal_3 Loss: 0.0366 | 0.0370
Epoch 290/300, seasonal_3 Loss: 0.0365 | 0.0370
Epoch 291/300, seasonal_3 Loss: 0.0365 | 0.0370
Epoch 292/300, seasonal_3 Loss: 0.0365 | 0.0370
Epoch 293/300, seasonal_3 Loss: 0.0365 | 0.0371
Epoch 294/300, seasonal_3 Loss: 0.0364 | 0.0371
Epoch 295/300, seasonal_3 Loss: 0.0364 | 0.0372
Epoch 296/300, seasonal_3 Loss: 0.0364 | 0.0371
Epoch 297/300, seasonal_3 Loss: 0.0364 | 0.0372
Epoch 298/300, seasonal_3 Loss: 0.0363 | 0.0372
Epoch 299/300, seasonal_3 Loss: 0.0363 | 0.0373
Epoch 300/300, seasonal_3 Loss: 0.0363 | 0.0373
Training resid component with params: {'observation_period_num': 10, 'train_rates': 0.9614250769842756, 'learning_rate': 0.0007497827900101974, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8289382207436825}
Epoch 1/300, resid Loss: 1.0852 | 1.4085
Epoch 2/300, resid Loss: 0.8953 | 1.4082
Epoch 3/300, resid Loss: 0.8915 | 1.4080
Epoch 4/300, resid Loss: 0.8848 | 1.4088
Epoch 5/300, resid Loss: 0.8797 | 1.4075
Epoch 6/300, resid Loss: 0.8807 | 1.4076
Epoch 7/300, resid Loss: 0.8831 | 1.4114
Epoch 8/300, resid Loss: 0.8855 | 1.4120
Epoch 9/300, resid Loss: 0.8845 | 1.4152
Epoch 10/300, resid Loss: 0.8804 | 1.4219
Epoch 11/300, resid Loss: 0.8824 | 1.4251
Epoch 12/300, resid Loss: 0.8847 | 1.4959
Epoch 13/300, resid Loss: 0.8953 | 1.4977
Epoch 14/300, resid Loss: 0.9004 | 1.4977
Epoch 15/300, resid Loss: 0.9161 | 1.5024
Epoch 16/300, resid Loss: 0.9220 | 1.5055
Epoch 17/300, resid Loss: 0.9437 | 1.5247
Epoch 18/300, resid Loss: 0.9485 | 1.5343
Epoch 19/300, resid Loss: 0.9729 | 1.5816
Epoch 20/300, resid Loss: 0.9704 | 1.5987
Epoch 21/300, resid Loss: 0.9892 | 1.6666
Epoch 22/300, resid Loss: 0.9790 | 1.6817
Epoch 23/300, resid Loss: 0.9905 | 1.7464
Epoch 24/300, resid Loss: 0.9795 | 1.7568
Epoch 25/300, resid Loss: 0.9861 | 1.8091
Epoch 26/300, resid Loss: 0.9776 | 1.8173
Epoch 27/300, resid Loss: 0.9810 | 1.8568
Epoch 28/300, resid Loss: 0.9751 | 1.8643
Epoch 29/300, resid Loss: 0.9765 | 1.8938
Epoch 30/300, resid Loss: 0.9725 | 1.9008
Epoch 31/300, resid Loss: 0.9729 | 1.9228
Epoch 32/300, resid Loss: 0.9702 | 1.9294
Epoch 33/300, resid Loss: 0.9699 | 1.9460
Epoch 34/300, resid Loss: 0.9681 | 1.9522
Epoch 35/300, resid Loss: 0.9675 | 1.9649
Epoch 36/300, resid Loss: 0.9663 | 1.9704
Epoch 37/300, resid Loss: 0.9656 | 1.9802
Epoch 38/300, resid Loss: 0.9648 | 1.9852
Epoch 39/300, resid Loss: 0.9640 | 1.9929
Epoch 40/300, resid Loss: 0.9634 | 1.9973
Epoch 41/300, resid Loss: 0.9626 | 2.0034
Epoch 42/300, resid Loss: 0.9622 | 2.0072
Epoch 43/300, resid Loss: 0.9615 | 2.0121
Epoch 44/300, resid Loss: 0.9613 | 2.0154
Epoch 45/300, resid Loss: 0.9606 | 2.0193
Epoch 46/300, resid Loss: 0.9604 | 2.0222
Epoch 47/300, resid Loss: 0.9598 | 2.0254
Epoch 48/300, resid Loss: 0.9597 | 2.0278
Epoch 49/300, resid Loss: 0.9592 | 2.0304
Epoch 50/300, resid Loss: 0.9591 | 2.0325
Epoch 51/300, resid Loss: 0.9586 | 2.0346
Epoch 52/300, resid Loss: 0.9586 | 2.0364
Epoch 53/300, resid Loss: 0.9582 | 2.0382
Epoch 54/300, resid Loss: 0.9581 | 2.0397
Epoch 55/300, resid Loss: 0.9578 | 2.0411
Epoch 56/300, resid Loss: 0.9578 | 2.0424
Epoch 57/300, resid Loss: 0.9575 | 2.0436
Epoch 58/300, resid Loss: 0.9575 | 2.0447
Epoch 59/300, resid Loss: 0.9572 | 2.0457
Epoch 60/300, resid Loss: 0.9572 | 2.0466
Epoch 61/300, resid Loss: 0.9570 | 2.0474
Epoch 62/300, resid Loss: 0.9570 | 2.0482
Epoch 63/300, resid Loss: 0.9568 | 2.0488
Epoch 64/300, resid Loss: 0.9568 | 2.0495
Epoch 65/300, resid Loss: 0.9567 | 2.0500
Epoch 66/300, resid Loss: 0.9567 | 2.0506
Epoch 67/300, resid Loss: 0.9565 | 2.0510
Epoch 68/300, resid Loss: 0.9565 | 2.0515
Epoch 69/300, resid Loss: 0.9564 | 2.0519
Epoch 70/300, resid Loss: 0.9564 | 2.0523
Epoch 71/300, resid Loss: 0.9563 | 2.0526
Epoch 72/300, resid Loss: 0.9563 | 2.0529
Epoch 73/300, resid Loss: 0.9563 | 2.0532
Epoch 74/300, resid Loss: 0.9563 | 2.0534
Epoch 75/300, resid Loss: 0.9562 | 2.0537
Epoch 76/300, resid Loss: 0.9562 | 2.0539
Epoch 77/300, resid Loss: 0.9562 | 2.0541
Epoch 78/300, resid Loss: 0.9562 | 2.0543
Epoch 79/300, resid Loss: 0.9561 | 2.0544
Epoch 80/300, resid Loss: 0.9561 | 2.0546
Epoch 81/300, resid Loss: 0.9561 | 2.0547
Epoch 82/300, resid Loss: 0.9561 | 2.0548
Epoch 83/300, resid Loss: 0.9560 | 2.0549
Epoch 84/300, resid Loss: 0.9560 | 2.0550
Epoch 85/300, resid Loss: 0.9560 | 2.0551
Epoch 86/300, resid Loss: 0.9560 | 2.0552
Epoch 87/300, resid Loss: 0.9560 | 2.0553
Epoch 88/300, resid Loss: 0.9560 | 2.0554
Epoch 89/300, resid Loss: 0.9560 | 2.0554
Epoch 90/300, resid Loss: 0.9560 | 2.0555
Epoch 91/300, resid Loss: 0.9560 | 2.0555
Epoch 92/300, resid Loss: 0.9560 | 2.0556
Epoch 93/300, resid Loss: 0.9560 | 2.0556
Epoch 94/300, resid Loss: 0.9560 | 2.0557
Epoch 95/300, resid Loss: 0.9559 | 2.0557
Epoch 96/300, resid Loss: 0.9559 | 2.0558
Epoch 97/300, resid Loss: 0.9559 | 2.0558
Epoch 98/300, resid Loss: 0.9559 | 2.0558
Epoch 99/300, resid Loss: 0.9559 | 2.0558
Epoch 100/300, resid Loss: 0.9559 | 2.0559
Epoch 101/300, resid Loss: 0.9559 | 2.0559
Epoch 102/300, resid Loss: 0.9559 | 2.0559
Epoch 103/300, resid Loss: 0.9559 | 2.0559
Epoch 104/300, resid Loss: 0.9559 | 2.0559
Epoch 105/300, resid Loss: 0.9559 | 2.0560
Epoch 106/300, resid Loss: 0.9559 | 2.0560
Epoch 107/300, resid Loss: 0.9559 | 2.0560
Epoch 108/300, resid Loss: 0.9559 | 2.0560
Epoch 109/300, resid Loss: 0.9559 | 2.0560
Epoch 110/300, resid Loss: 0.9559 | 2.0560
Epoch 111/300, resid Loss: 0.9559 | 2.0560
Epoch 112/300, resid Loss: 0.9559 | 2.0560
Epoch 113/300, resid Loss: 0.9559 | 2.0560
Epoch 114/300, resid Loss: 0.9559 | 2.0560
Epoch 115/300, resid Loss: 0.9559 | 2.0560
Epoch 116/300, resid Loss: 0.9559 | 2.0560
Epoch 117/300, resid Loss: 0.9559 | 2.0561
Epoch 118/300, resid Loss: 0.9559 | 2.0561
Epoch 119/300, resid Loss: 0.9559 | 2.0561
Epoch 120/300, resid Loss: 0.9559 | 2.0561
Epoch 121/300, resid Loss: 0.9559 | 2.0561
Epoch 122/300, resid Loss: 0.9559 | 2.0561
Epoch 123/300, resid Loss: 0.9559 | 2.0561
Epoch 124/300, resid Loss: 0.9559 | 2.0561
Epoch 125/300, resid Loss: 0.9559 | 2.0561
Epoch 126/300, resid Loss: 0.9559 | 2.0561
Epoch 127/300, resid Loss: 0.9559 | 2.0561
Epoch 128/300, resid Loss: 0.9559 | 2.0561
Epoch 129/300, resid Loss: 0.9559 | 2.0561
Epoch 130/300, resid Loss: 0.9559 | 2.0561
Epoch 131/300, resid Loss: 0.9559 | 2.0561
Epoch 132/300, resid Loss: 0.9559 | 2.0561
Epoch 133/300, resid Loss: 0.9559 | 2.0561
Epoch 134/300, resid Loss: 0.9559 | 2.0561
Epoch 135/300, resid Loss: 0.9559 | 2.0561
Epoch 136/300, resid Loss: 0.9559 | 2.0561
Epoch 137/300, resid Loss: 0.9559 | 2.0561
Epoch 138/300, resid Loss: 0.9559 | 2.0561
Epoch 139/300, resid Loss: 0.9559 | 2.0561
Epoch 140/300, resid Loss: 0.9559 | 2.0561
Epoch 141/300, resid Loss: 0.9559 | 2.0561
Epoch 142/300, resid Loss: 0.9559 | 2.0561
Epoch 143/300, resid Loss: 0.9559 | 2.0561
Epoch 144/300, resid Loss: 0.9559 | 2.0561
Epoch 145/300, resid Loss: 0.9559 | 2.0561
Epoch 146/300, resid Loss: 0.9559 | 2.0561
Epoch 147/300, resid Loss: 0.9559 | 2.0561
Epoch 148/300, resid Loss: 0.9559 | 2.0561
Epoch 149/300, resid Loss: 0.9559 | 2.0561
Epoch 150/300, resid Loss: 0.9559 | 2.0561
Epoch 151/300, resid Loss: 0.9559 | 2.0561
Early stopping for resid
Runtime (seconds): 13970.840845108032
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[160.18105]
[-2.5934799]
[2.8556063]
[-0.4248956]
[2.6138856]
[-0.97453654]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1583.4317869096994
RMSE: 39.7923583984375
MAE: 39.7923583984375
R-squared: nan
[161.65764]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
