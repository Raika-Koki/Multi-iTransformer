[32m[I 2025-01-05 20:51:45,450][0m A new study created in memory with name: no-name-415d71fd-eb8d-4fd2-88ee-725c01762434[0m
[32m[I 2025-01-05 20:53:57,809][0m Trial 0 finished with value: 0.7694025039672852 and parameters: {'observation_period_num': 97, 'train_rates': 0.9611147858260956, 'learning_rate': 0.0006977406113192963, 'batch_size': 123, 'step_size': 12, 'gamma': 0.9039201930850466}. Best is trial 0 with value: 0.7694025039672852.[0m
[32m[I 2025-01-05 20:59:15,719][0m Trial 1 finished with value: 0.4876778820968851 and parameters: {'observation_period_num': 210, 'train_rates': 0.8006594881587528, 'learning_rate': 2.1483965027989154e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.7897993674518228}. Best is trial 1 with value: 0.4876778820968851.[0m
[32m[I 2025-01-05 21:01:51,262][0m Trial 2 finished with value: 0.2647155225276947 and parameters: {'observation_period_num': 115, 'train_rates': 0.9482159191784889, 'learning_rate': 3.114954132614549e-05, 'batch_size': 192, 'step_size': 10, 'gamma': 0.8679377869403546}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:02:19,897][0m Trial 3 finished with value: 0.5532422709180631 and parameters: {'observation_period_num': 19, 'train_rates': 0.828687106000422, 'learning_rate': 7.707571348240316e-06, 'batch_size': 184, 'step_size': 5, 'gamma': 0.9771984496557138}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:04:11,753][0m Trial 4 finished with value: 0.8740065509782118 and parameters: {'observation_period_num': 90, 'train_rates': 0.833659551716533, 'learning_rate': 9.825870185105406e-06, 'batch_size': 219, 'step_size': 7, 'gamma': 0.7752180268513104}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:04:57,468][0m Trial 5 finished with value: 1.8358835100071507 and parameters: {'observation_period_num': 41, 'train_rates': 0.6899991354573822, 'learning_rate': 1.2180133518080913e-06, 'batch_size': 157, 'step_size': 5, 'gamma': 0.8546157112990256}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:08:15,522][0m Trial 6 finished with value: 0.7355478664416643 and parameters: {'observation_period_num': 167, 'train_rates': 0.6983060813510316, 'learning_rate': 2.0447308568045843e-05, 'batch_size': 209, 'step_size': 15, 'gamma': 0.9817049553633935}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:10:56,666][0m Trial 7 finished with value: 1.1215122360171694 and parameters: {'observation_period_num': 141, 'train_rates': 0.6126066372225315, 'learning_rate': 3.8027198489659266e-05, 'batch_size': 44, 'step_size': 3, 'gamma': 0.8552863165146445}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:12:51,814][0m Trial 8 finished with value: 0.7209635140427259 and parameters: {'observation_period_num': 98, 'train_rates': 0.7164986185177873, 'learning_rate': 0.00048236294289833587, 'batch_size': 68, 'step_size': 14, 'gamma': 0.828408521034915}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:16:20,301][0m Trial 9 finished with value: 0.36999067218504217 and parameters: {'observation_period_num': 159, 'train_rates': 0.808508530362583, 'learning_rate': 3.093213046020881e-05, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8247601498834007}. Best is trial 2 with value: 0.2647155225276947.[0m
[32m[I 2025-01-05 21:23:08,144][0m Trial 10 finished with value: 0.1098354235291481 and parameters: {'observation_period_num': 252, 'train_rates': 0.9864602967305677, 'learning_rate': 0.0001460058550873957, 'batch_size': 244, 'step_size': 9, 'gamma': 0.9129507198170159}. Best is trial 10 with value: 0.1098354235291481.[0m
[32m[I 2025-01-05 21:29:44,253][0m Trial 11 finished with value: 0.1227620393037796 and parameters: {'observation_period_num': 243, 'train_rates': 0.988924660014803, 'learning_rate': 0.00011401624527690297, 'batch_size': 247, 'step_size': 9, 'gamma': 0.9128491246219276}. Best is trial 10 with value: 0.1098354235291481.[0m
[32m[I 2025-01-05 21:35:51,680][0m Trial 12 finished with value: 0.16953327865304804 and parameters: {'observation_period_num': 244, 'train_rates': 0.9018786637794207, 'learning_rate': 0.00015808270127178994, 'batch_size': 253, 'step_size': 8, 'gamma': 0.9119179452863779}. Best is trial 10 with value: 0.1098354235291481.[0m
[32m[I 2025-01-05 21:42:12,260][0m Trial 13 finished with value: 0.18570954871041692 and parameters: {'observation_period_num': 251, 'train_rates': 0.9027784329841674, 'learning_rate': 0.0001364139850245831, 'batch_size': 245, 'step_size': 10, 'gamma': 0.9321472307905203}. Best is trial 10 with value: 0.1098354235291481.[0m
[32m[I 2025-01-05 21:47:32,296][0m Trial 14 finished with value: 0.09975570440292358 and parameters: {'observation_period_num': 204, 'train_rates': 0.9858137220533397, 'learning_rate': 0.00014265776280790546, 'batch_size': 126, 'step_size': 6, 'gamma': 0.942041867155865}. Best is trial 14 with value: 0.09975570440292358.[0m
[32m[I 2025-01-05 21:52:18,624][0m Trial 15 finished with value: 0.2043660157230271 and parameters: {'observation_period_num': 195, 'train_rates': 0.9089943944769748, 'learning_rate': 0.00034537567875624867, 'batch_size': 111, 'step_size': 1, 'gamma': 0.94451473929961}. Best is trial 14 with value: 0.09975570440292358.[0m
[32m[I 2025-01-05 21:57:18,325][0m Trial 16 finished with value: 0.2267176515271521 and parameters: {'observation_period_num': 209, 'train_rates': 0.8723020630420838, 'learning_rate': 8.211256467662226e-05, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9492328482329764}. Best is trial 14 with value: 0.09975570440292358.[0m
[32m[I 2025-01-05 22:01:57,113][0m Trial 17 finished with value: 0.09728581458330154 and parameters: {'observation_period_num': 185, 'train_rates': 0.986249071114432, 'learning_rate': 0.0002782536070996631, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8905671010938594}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:05:46,939][0m Trial 18 finished with value: 1.2768126238324788 and parameters: {'observation_period_num': 184, 'train_rates': 0.7415635826309624, 'learning_rate': 0.0009324319624632499, 'batch_size': 95, 'step_size': 12, 'gamma': 0.8836570091764714}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:09:09,985][0m Trial 19 finished with value: 0.16608866819125764 and parameters: {'observation_period_num': 141, 'train_rates': 0.9276024275387734, 'learning_rate': 0.0002443303805237225, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9620104524047035}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:14:23,594][0m Trial 20 finished with value: 0.2716499250284664 and parameters: {'observation_period_num': 219, 'train_rates': 0.8603905282675789, 'learning_rate': 5.998814266313213e-05, 'batch_size': 136, 'step_size': 6, 'gamma': 0.8823714323564131}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:20:11,279][0m Trial 21 finished with value: 0.10843021422624588 and parameters: {'observation_period_num': 222, 'train_rates': 0.9785926529403175, 'learning_rate': 0.00022542495793126903, 'batch_size': 169, 'step_size': 13, 'gamma': 0.9150932152242155}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:24:37,986][0m Trial 22 finished with value: 0.17973138391971588 and parameters: {'observation_period_num': 181, 'train_rates': 0.9517285583771149, 'learning_rate': 0.00030340472564646816, 'batch_size': 166, 'step_size': 13, 'gamma': 0.9306066582854577}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:30:19,717][0m Trial 23 finished with value: 0.09998498111963272 and parameters: {'observation_period_num': 217, 'train_rates': 0.9869581987387844, 'learning_rate': 0.00023413032857381086, 'batch_size': 117, 'step_size': 15, 'gamma': 0.8955698282789138}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:34:14,448][0m Trial 24 finished with value: 0.595287851748928 and parameters: {'observation_period_num': 162, 'train_rates': 0.9336099850511871, 'learning_rate': 0.000566480683022927, 'batch_size': 109, 'step_size': 15, 'gamma': 0.8873278286417202}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:38:24,925][0m Trial 25 finished with value: 0.5103742505564834 and parameters: {'observation_period_num': 194, 'train_rates': 0.7607023424580845, 'learning_rate': 7.419703655466733e-05, 'batch_size': 134, 'step_size': 14, 'gamma': 0.8458293094541917}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:43:56,530][0m Trial 26 finished with value: 0.2791232936554127 and parameters: {'observation_period_num': 227, 'train_rates': 0.8779384283047872, 'learning_rate': 0.00036182836803127805, 'batch_size': 87, 'step_size': 11, 'gamma': 0.896452336761959}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:49:05,085][0m Trial 27 finished with value: 0.22414363796512285 and parameters: {'observation_period_num': 200, 'train_rates': 0.9523641935971531, 'learning_rate': 0.00020745702236366707, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9572350808547072}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:53:07,843][0m Trial 28 finished with value: 1.0100913964579459 and parameters: {'observation_period_num': 171, 'train_rates': 0.9200371516573025, 'learning_rate': 0.0009379524650931488, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9343049409515394}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 22:59:04,540][0m Trial 29 finished with value: 0.37644508481025696 and parameters: {'observation_period_num': 228, 'train_rates': 0.9642911124966934, 'learning_rate': 0.0005181225290028414, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8705890393597078}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 23:02:31,755][0m Trial 30 finished with value: 0.217813640832901 and parameters: {'observation_period_num': 143, 'train_rates': 0.9655467443424501, 'learning_rate': 5.432805232507075e-05, 'batch_size': 127, 'step_size': 11, 'gamma': 0.8180801207405527}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 23:08:28,085][0m Trial 31 finished with value: 0.10779954493045807 and parameters: {'observation_period_num': 225, 'train_rates': 0.9852556591089806, 'learning_rate': 0.0002497503449204098, 'batch_size': 149, 'step_size': 13, 'gamma': 0.9208848350290919}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 23:14:21,348][0m Trial 32 finished with value: 0.20629546322204448 and parameters: {'observation_period_num': 232, 'train_rates': 0.9405263972584474, 'learning_rate': 0.00010339530463499535, 'batch_size': 147, 'step_size': 14, 'gamma': 0.9023858828940364}. Best is trial 17 with value: 0.09728581458330154.[0m
[32m[I 2025-01-05 23:19:53,293][0m Trial 33 finished with value: 0.09212413430213928 and parameters: {'observation_period_num': 209, 'train_rates': 0.9890884185596168, 'learning_rate': 0.00018988743650738988, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9229440280372778}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:25:18,028][0m Trial 34 finished with value: 0.2758143027039135 and parameters: {'observation_period_num': 210, 'train_rates': 0.963147220290694, 'learning_rate': 0.00041545365361313164, 'batch_size': 93, 'step_size': 11, 'gamma': 0.9663032458555805}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:29:59,925][0m Trial 35 finished with value: 0.38457003618839763 and parameters: {'observation_period_num': 180, 'train_rates': 0.8976737972378677, 'learning_rate': 2.15281581709131e-06, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8953817306581294}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:35:17,232][0m Trial 36 finished with value: 0.16956443000923505 and parameters: {'observation_period_num': 206, 'train_rates': 0.9400082904896797, 'learning_rate': 0.00018652491755805659, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8738209437565083}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:37:22,268][0m Trial 37 finished with value: 0.9296542810790733 and parameters: {'observation_period_num': 119, 'train_rates': 0.6371916170074534, 'learning_rate': 1.1551165913026573e-05, 'batch_size': 101, 'step_size': 9, 'gamma': 0.9415738297624313}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:38:35,419][0m Trial 38 finished with value: 0.6581825458755096 and parameters: {'observation_period_num': 59, 'train_rates': 0.8508567508928009, 'learning_rate': 0.0006262901352918301, 'batch_size': 119, 'step_size': 7, 'gamma': 0.9738843264653655}. Best is trial 33 with value: 0.09212413430213928.[0m
Early stopping at epoch 55
[32m[I 2025-01-05 23:41:17,148][0m Trial 39 finished with value: 0.7616982330446658 and parameters: {'observation_period_num': 191, 'train_rates': 0.9666851989789684, 'learning_rate': 4.465093574216044e-05, 'batch_size': 80, 'step_size': 1, 'gamma': 0.8004460944273738}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:45:20,283][0m Trial 40 finished with value: 0.21986761989985934 and parameters: {'observation_period_num': 174, 'train_rates': 0.88415983927831, 'learning_rate': 9.278674348125214e-05, 'batch_size': 183, 'step_size': 12, 'gamma': 0.7606162025620631}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:50:57,793][0m Trial 41 finished with value: 0.10644634813070297 and parameters: {'observation_period_num': 215, 'train_rates': 0.9872147180203922, 'learning_rate': 0.00027326453785033464, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9217429602934102}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-05 23:56:30,834][0m Trial 42 finished with value: 0.16482405364513397 and parameters: {'observation_period_num': 214, 'train_rates': 0.972805423626607, 'learning_rate': 0.0002843505751634066, 'batch_size': 128, 'step_size': 14, 'gamma': 0.854547492395416}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:00:12,982][0m Trial 43 finished with value: 0.21893870934739812 and parameters: {'observation_period_num': 153, 'train_rates': 0.9488921688034587, 'learning_rate': 0.0001575963793805373, 'batch_size': 138, 'step_size': 15, 'gamma': 0.9266622837384496}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:06:39,132][0m Trial 44 finished with value: 0.1736096888780594 and parameters: {'observation_period_num': 238, 'train_rates': 0.987681769837495, 'learning_rate': 2.1907572626165562e-05, 'batch_size': 164, 'step_size': 14, 'gamma': 0.9034244911714219}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:11:38,909][0m Trial 45 finished with value: 0.28794857101006943 and parameters: {'observation_period_num': 200, 'train_rates': 0.920295924597465, 'learning_rate': 0.0003854202696796518, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9204041353258993}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:16:23,303][0m Trial 46 finished with value: 0.09867282956838608 and parameters: {'observation_period_num': 188, 'train_rates': 0.9889970326340758, 'learning_rate': 0.0001275948898171125, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9493348004300689}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:20:30,123][0m Trial 47 finished with value: 0.3831710532101889 and parameters: {'observation_period_num': 187, 'train_rates': 0.8167829895090274, 'learning_rate': 0.0001378097798722728, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9859980501257681}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:22:18,317][0m Trial 48 finished with value: 0.3661242346066275 and parameters: {'observation_period_num': 89, 'train_rates': 0.7862547211525951, 'learning_rate': 6.877320591109903e-05, 'batch_size': 103, 'step_size': 9, 'gamma': 0.9504982857316058}. Best is trial 33 with value: 0.09212413430213928.[0m
[32m[I 2025-01-06 00:28:40,130][0m Trial 49 finished with value: 0.12364260852336884 and parameters: {'observation_period_num': 239, 'train_rates': 0.9721016677333644, 'learning_rate': 0.00011446708970107254, 'batch_size': 77, 'step_size': 11, 'gamma': 0.9391858132332181}. Best is trial 33 with value: 0.09212413430213928.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.8314 | 1.0273
Epoch 2/300, Loss: 0.7173 | 0.7435
Epoch 3/300, Loss: 0.5854 | 0.7612
Epoch 4/300, Loss: 0.4890 | 0.5802
Epoch 5/300, Loss: 0.5353 | 0.5905
Epoch 6/300, Loss: 0.5067 | 0.5382
Epoch 7/300, Loss: 0.3933 | 0.4537
Epoch 8/300, Loss: 0.3670 | 0.4835
Epoch 9/300, Loss: 0.4318 | 0.3757
Epoch 10/300, Loss: 0.4493 | 0.4117
Epoch 11/300, Loss: 0.3794 | 0.5016
Epoch 12/300, Loss: 0.3401 | 0.3618
Epoch 13/300, Loss: 0.3515 | 0.4730
Epoch 14/300, Loss: 0.3251 | 0.3449
Epoch 15/300, Loss: 0.2711 | 0.2907
Epoch 16/300, Loss: 0.2453 | 0.2715
Epoch 17/300, Loss: 0.2481 | 0.2542
Epoch 18/300, Loss: 0.2610 | 0.2371
Epoch 19/300, Loss: 0.3460 | 0.2821
Epoch 20/300, Loss: 0.3317 | 0.2303
Epoch 21/300, Loss: 0.2897 | 0.2349
Epoch 22/300, Loss: 0.3099 | 0.2381
Epoch 23/300, Loss: 0.2483 | 0.1944
Epoch 24/300, Loss: 0.2633 | 0.2346
Epoch 25/300, Loss: 0.2558 | 0.2331
Epoch 26/300, Loss: 0.2444 | 0.2023
Epoch 27/300, Loss: 0.2576 | 0.2132
Epoch 28/300, Loss: 0.2281 | 0.1924
Epoch 29/300, Loss: 0.2318 | 0.1877
Epoch 30/300, Loss: 0.2070 | 0.2290
Epoch 31/300, Loss: 0.2068 | 0.1939
Epoch 32/300, Loss: 0.1995 | 0.2062
Epoch 33/300, Loss: 0.1831 | 0.1687
Epoch 34/300, Loss: 0.1857 | 0.1566
Epoch 35/300, Loss: 0.1757 | 0.1599
Epoch 36/300, Loss: 0.1728 | 0.1549
Epoch 37/300, Loss: 0.1738 | 0.1641
Epoch 38/300, Loss: 0.1705 | 0.1444
Epoch 39/300, Loss: 0.1813 | 0.1483
Epoch 40/300, Loss: 0.1754 | 0.1382
Epoch 41/300, Loss: 0.1894 | 0.1590
Epoch 42/300, Loss: 0.2199 | 0.1723
Epoch 43/300, Loss: 0.2178 | 0.1554
Epoch 44/300, Loss: 0.2882 | 0.1782
Epoch 45/300, Loss: 0.2543 | 0.1686
Epoch 46/300, Loss: 0.2176 | 0.1735
Epoch 47/300, Loss: 0.1914 | 0.1706
Epoch 48/300, Loss: 0.1721 | 0.1509
Epoch 49/300, Loss: 0.1625 | 0.1522
Epoch 50/300, Loss: 0.1618 | 0.1405
Epoch 51/300, Loss: 0.1583 | 0.1447
Epoch 52/300, Loss: 0.1539 | 0.1355
Epoch 53/300, Loss: 0.1495 | 0.1369
Epoch 54/300, Loss: 0.1446 | 0.1279
Epoch 55/300, Loss: 0.1417 | 0.1284
Epoch 56/300, Loss: 0.1394 | 0.1206
Epoch 57/300, Loss: 0.1377 | 0.1218
Epoch 58/300, Loss: 0.1373 | 0.1183
Epoch 59/300, Loss: 0.1364 | 0.1184
Epoch 60/300, Loss: 0.1361 | 0.1172
Epoch 61/300, Loss: 0.1384 | 0.1181
Epoch 62/300, Loss: 0.1414 | 0.1215
Epoch 63/300, Loss: 0.1462 | 0.1161
Epoch 64/300, Loss: 0.1499 | 0.1254
Epoch 65/300, Loss: 0.1499 | 0.1134
Epoch 66/300, Loss: 0.1605 | 0.1209
Epoch 67/300, Loss: 0.1568 | 0.1161
Epoch 68/300, Loss: 0.1546 | 0.1208
Epoch 69/300, Loss: 0.1504 | 0.1201
Epoch 70/300, Loss: 0.1446 | 0.1269
Epoch 71/300, Loss: 0.1393 | 0.1144
Epoch 72/300, Loss: 0.1329 | 0.1178
Epoch 73/300, Loss: 0.1289 | 0.1074
Epoch 74/300, Loss: 0.1259 | 0.1091
Epoch 75/300, Loss: 0.1246 | 0.1057
Epoch 76/300, Loss: 0.1248 | 0.1062
Epoch 77/300, Loss: 0.1257 | 0.1058
Epoch 78/300, Loss: 0.1291 | 0.1042
Epoch 79/300, Loss: 0.1310 | 0.1093
Epoch 80/300, Loss: 0.1311 | 0.1025
Epoch 81/300, Loss: 0.1313 | 0.1065
Epoch 82/300, Loss: 0.1309 | 0.1015
Epoch 83/300, Loss: 0.1310 | 0.1060
Epoch 84/300, Loss: 0.1282 | 0.1009
Epoch 85/300, Loss: 0.1261 | 0.1060
Epoch 86/300, Loss: 0.1243 | 0.1005
Epoch 87/300, Loss: 0.1227 | 0.1054
Epoch 88/300, Loss: 0.1223 | 0.0999
Epoch 89/300, Loss: 0.1210 | 0.1042
Epoch 90/300, Loss: 0.1199 | 0.0986
Epoch 91/300, Loss: 0.1181 | 0.1023
Epoch 92/300, Loss: 0.1176 | 0.0982
Epoch 93/300, Loss: 0.1164 | 0.1001
Epoch 94/300, Loss: 0.1167 | 0.0968
Epoch 95/300, Loss: 0.1156 | 0.0992
Epoch 96/300, Loss: 0.1160 | 0.0975
Epoch 97/300, Loss: 0.1163 | 0.0998
Epoch 98/300, Loss: 0.1172 | 0.0993
Epoch 99/300, Loss: 0.1183 | 0.1038
Epoch 100/300, Loss: 0.1180 | 0.0989
Epoch 101/300, Loss: 0.1148 | 0.0996
Epoch 102/300, Loss: 0.1126 | 0.0953
Epoch 103/300, Loss: 0.1111 | 0.0959
Epoch 104/300, Loss: 0.1099 | 0.0940
Epoch 105/300, Loss: 0.1094 | 0.0930
Epoch 106/300, Loss: 0.1088 | 0.0919
Epoch 107/300, Loss: 0.1083 | 0.0923
Epoch 108/300, Loss: 0.1081 | 0.0920
Epoch 109/300, Loss: 0.1075 | 0.0918
Epoch 110/300, Loss: 0.1072 | 0.0913
Epoch 111/300, Loss: 0.1071 | 0.0906
Epoch 112/300, Loss: 0.1062 | 0.0896
Epoch 113/300, Loss: 0.1065 | 0.0903
Epoch 114/300, Loss: 0.1057 | 0.0899
Epoch 115/300, Loss: 0.1058 | 0.0901
Epoch 116/300, Loss: 0.1062 | 0.0891
Epoch 117/300, Loss: 0.1066 | 0.0897
Epoch 118/300, Loss: 0.1070 | 0.0891
Epoch 119/300, Loss: 0.1084 | 0.0905
Epoch 120/300, Loss: 0.1078 | 0.0878
Epoch 121/300, Loss: 0.1089 | 0.0915
Epoch 122/300, Loss: 0.1084 | 0.0887
Epoch 123/300, Loss: 0.1059 | 0.0917
Epoch 124/300, Loss: 0.1052 | 0.0877
Epoch 125/300, Loss: 0.1043 | 0.0899
Epoch 126/300, Loss: 0.1036 | 0.0871
Epoch 127/300, Loss: 0.1028 | 0.0888
Epoch 128/300, Loss: 0.1031 | 0.0873
Epoch 129/300, Loss: 0.1032 | 0.0885
Epoch 130/300, Loss: 0.1036 | 0.0869
Epoch 131/300, Loss: 0.1028 | 0.0879
Epoch 132/300, Loss: 0.1025 | 0.0862
Epoch 133/300, Loss: 0.1018 | 0.0862
Epoch 134/300, Loss: 0.1004 | 0.0852
Epoch 135/300, Loss: 0.0998 | 0.0853
Epoch 136/300, Loss: 0.0990 | 0.0845
Epoch 137/300, Loss: 0.0988 | 0.0848
Epoch 138/300, Loss: 0.0983 | 0.0840
Epoch 139/300, Loss: 0.0974 | 0.0840
Epoch 140/300, Loss: 0.0962 | 0.0839
Epoch 141/300, Loss: 0.0965 | 0.0833
Epoch 142/300, Loss: 0.0964 | 0.0827
Epoch 143/300, Loss: 0.0964 | 0.0832
Epoch 144/300, Loss: 0.0968 | 0.0828
Epoch 145/300, Loss: 0.0957 | 0.0824
Epoch 146/300, Loss: 0.0957 | 0.0821
Epoch 147/300, Loss: 0.0957 | 0.0826
Epoch 148/300, Loss: 0.0949 | 0.0824
Epoch 149/300, Loss: 0.0956 | 0.0823
Epoch 150/300, Loss: 0.0961 | 0.0817
Epoch 151/300, Loss: 0.0944 | 0.0828
Epoch 152/300, Loss: 0.0947 | 0.0820
Epoch 153/300, Loss: 0.0949 | 0.0823
Epoch 154/300, Loss: 0.0945 | 0.0813
Epoch 155/300, Loss: 0.0948 | 0.0812
Epoch 156/300, Loss: 0.0950 | 0.0810
Epoch 157/300, Loss: 0.0947 | 0.0809
Epoch 158/300, Loss: 0.0938 | 0.0803
Epoch 159/300, Loss: 0.0937 | 0.0807
Epoch 160/300, Loss: 0.0935 | 0.0800
Epoch 161/300, Loss: 0.0937 | 0.0793
Epoch 162/300, Loss: 0.0932 | 0.0790
Epoch 163/300, Loss: 0.0924 | 0.0788
Epoch 164/300, Loss: 0.0925 | 0.0789
Epoch 165/300, Loss: 0.0929 | 0.0800
Epoch 166/300, Loss: 0.0933 | 0.0794
Epoch 167/300, Loss: 0.0918 | 0.0792
Epoch 168/300, Loss: 0.0913 | 0.0785
Epoch 169/300, Loss: 0.0912 | 0.0787
Epoch 170/300, Loss: 0.0904 | 0.0781
Epoch 171/300, Loss: 0.0899 | 0.0782
Epoch 172/300, Loss: 0.0901 | 0.0781
Epoch 173/300, Loss: 0.0897 | 0.0788
Epoch 174/300, Loss: 0.0899 | 0.0781
Epoch 175/300, Loss: 0.0897 | 0.0782
Epoch 176/300, Loss: 0.0886 | 0.0778
Epoch 177/300, Loss: 0.0889 | 0.0785
Epoch 178/300, Loss: 0.0886 | 0.0777
Epoch 179/300, Loss: 0.0879 | 0.0774
Epoch 180/300, Loss: 0.0876 | 0.0778
Epoch 181/300, Loss: 0.0884 | 0.0771
Epoch 182/300, Loss: 0.0877 | 0.0773
Epoch 183/300, Loss: 0.0875 | 0.0775
Epoch 184/300, Loss: 0.0878 | 0.0773
Epoch 185/300, Loss: 0.0875 | 0.0766
Epoch 186/300, Loss: 0.0881 | 0.0766
Epoch 187/300, Loss: 0.0870 | 0.0768
Epoch 188/300, Loss: 0.0874 | 0.0764
Epoch 189/300, Loss: 0.0862 | 0.0765
Epoch 190/300, Loss: 0.0865 | 0.0763
Epoch 191/300, Loss: 0.0866 | 0.0761
Epoch 192/300, Loss: 0.0874 | 0.0764
Epoch 193/300, Loss: 0.0857 | 0.0756
Epoch 194/300, Loss: 0.0855 | 0.0753
Epoch 195/300, Loss: 0.0859 | 0.0761
Epoch 196/300, Loss: 0.0856 | 0.0751
Epoch 197/300, Loss: 0.0859 | 0.0751
Epoch 198/300, Loss: 0.0856 | 0.0752
Epoch 199/300, Loss: 0.0855 | 0.0756
Epoch 200/300, Loss: 0.0852 | 0.0747
Epoch 201/300, Loss: 0.0853 | 0.0756
Epoch 202/300, Loss: 0.0848 | 0.0748
Epoch 203/300, Loss: 0.0845 | 0.0755
Epoch 204/300, Loss: 0.0847 | 0.0743
Epoch 205/300, Loss: 0.0851 | 0.0743
Epoch 206/300, Loss: 0.0844 | 0.0742
Epoch 207/300, Loss: 0.0849 | 0.0748
Epoch 208/300, Loss: 0.0840 | 0.0739
Epoch 209/300, Loss: 0.0843 | 0.0743
Epoch 210/300, Loss: 0.0847 | 0.0748
Epoch 211/300, Loss: 0.0840 | 0.0742
Epoch 212/300, Loss: 0.0847 | 0.0743
Epoch 213/300, Loss: 0.0854 | 0.0732
Epoch 214/300, Loss: 0.0844 | 0.0749
Epoch 215/300, Loss: 0.0852 | 0.0729
Epoch 216/300, Loss: 0.0843 | 0.0739
Epoch 217/300, Loss: 0.0836 | 0.0738
Epoch 218/300, Loss: 0.0834 | 0.0738
Epoch 219/300, Loss: 0.0835 | 0.0730
Epoch 220/300, Loss: 0.0828 | 0.0738
Epoch 221/300, Loss: 0.0826 | 0.0736
Epoch 222/300, Loss: 0.0822 | 0.0731
Epoch 223/300, Loss: 0.0825 | 0.0735
Epoch 224/300, Loss: 0.0826 | 0.0731
Epoch 225/300, Loss: 0.0823 | 0.0731
Epoch 226/300, Loss: 0.0828 | 0.0725
Epoch 227/300, Loss: 0.0821 | 0.0730
Epoch 228/300, Loss: 0.0817 | 0.0728
Epoch 229/300, Loss: 0.0828 | 0.0730
Epoch 230/300, Loss: 0.0823 | 0.0730
Epoch 231/300, Loss: 0.0815 | 0.0724
Epoch 232/300, Loss: 0.0814 | 0.0729
Epoch 233/300, Loss: 0.0819 | 0.0725
Epoch 234/300, Loss: 0.0817 | 0.0727
Epoch 235/300, Loss: 0.0808 | 0.0721
Epoch 236/300, Loss: 0.0818 | 0.0727
Epoch 237/300, Loss: 0.0813 | 0.0722
Epoch 238/300, Loss: 0.0819 | 0.0721
Epoch 239/300, Loss: 0.0809 | 0.0721
Epoch 240/300, Loss: 0.0818 | 0.0721
Epoch 241/300, Loss: 0.0810 | 0.0717
Epoch 242/300, Loss: 0.0816 | 0.0716
Epoch 243/300, Loss: 0.0809 | 0.0718
Epoch 244/300, Loss: 0.0813 | 0.0715
Epoch 245/300, Loss: 0.0806 | 0.0717
Epoch 246/300, Loss: 0.0807 | 0.0717
Epoch 247/300, Loss: 0.0803 | 0.0719
Epoch 248/300, Loss: 0.0805 | 0.0720
Epoch 249/300, Loss: 0.0810 | 0.0723
Epoch 250/300, Loss: 0.0801 | 0.0719
Epoch 251/300, Loss: 0.0807 | 0.0712
Epoch 252/300, Loss: 0.0805 | 0.0710
Epoch 253/300, Loss: 0.0802 | 0.0714
Epoch 254/300, Loss: 0.0801 | 0.0711
Epoch 255/300, Loss: 0.0801 | 0.0713
Epoch 256/300, Loss: 0.0798 | 0.0716
Epoch 257/300, Loss: 0.0805 | 0.0719
Epoch 258/300, Loss: 0.0805 | 0.0716
Epoch 259/300, Loss: 0.0794 | 0.0718
Epoch 260/300, Loss: 0.0804 | 0.0713
Epoch 261/300, Loss: 0.0799 | 0.0715
Epoch 262/300, Loss: 0.0806 | 0.0717
Epoch 263/300, Loss: 0.0800 | 0.0716
Epoch 264/300, Loss: 0.0795 | 0.0716
Epoch 265/300, Loss: 0.0798 | 0.0714
Epoch 266/300, Loss: 0.0799 | 0.0713
Epoch 267/300, Loss: 0.0796 | 0.0712
Epoch 268/300, Loss: 0.0793 | 0.0711
Epoch 269/300, Loss: 0.0795 | 0.0711
Epoch 270/300, Loss: 0.0803 | 0.0715
Epoch 271/300, Loss: 0.0787 | 0.0709
Epoch 272/300, Loss: 0.0797 | 0.0709
Epoch 273/300, Loss: 0.0786 | 0.0708
Epoch 274/300, Loss: 0.0795 | 0.0709
Epoch 275/300, Loss: 0.0788 | 0.0710
Epoch 276/300, Loss: 0.0789 | 0.0708
Epoch 277/300, Loss: 0.0789 | 0.0705
Epoch 278/300, Loss: 0.0791 | 0.0706
Epoch 279/300, Loss: 0.0797 | 0.0706
Epoch 280/300, Loss: 0.0781 | 0.0709
Epoch 281/300, Loss: 0.0787 | 0.0711
Epoch 282/300, Loss: 0.0784 | 0.0713
Epoch 283/300, Loss: 0.0787 | 0.0711
Epoch 284/300, Loss: 0.0791 | 0.0707
Epoch 285/300, Loss: 0.0785 | 0.0708
Epoch 286/300, Loss: 0.0782 | 0.0708
Epoch 287/300, Loss: 0.0786 | 0.0708
Epoch 288/300, Loss: 0.0787 | 0.0706
Epoch 289/300, Loss: 0.0791 | 0.0706
Epoch 290/300, Loss: 0.0782 | 0.0709
Epoch 291/300, Loss: 0.0785 | 0.0712
Epoch 292/300, Loss: 0.0791 | 0.0707
Epoch 293/300, Loss: 0.0779 | 0.0703
Epoch 294/300, Loss: 0.0781 | 0.0699
Epoch 295/300, Loss: 0.0784 | 0.0702
Epoch 296/300, Loss: 0.0783 | 0.0701
Epoch 297/300, Loss: 0.0794 | 0.0705
Epoch 298/300, Loss: 0.0788 | 0.0705
Epoch 299/300, Loss: 0.0787 | 0.0702
Epoch 300/300, Loss: 0.0785 | 0.0705
Runtime (seconds): 993.7081065177917
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 120.24787652865052
RMSE: 10.96575927734375
MAE: 10.96575927734375
R-squared: nan
[221.90424]
