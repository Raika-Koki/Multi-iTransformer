[32m[I 2025-01-06 05:44:00,415][0m A new study created in memory with name: no-name-1b52b34a-a3a0-4d08-ab61-a01d60004e54[0m
Early stopping at epoch 96
[32m[I 2025-01-06 05:46:50,425][0m Trial 0 finished with value: 0.5133956687742489 and parameters: {'observation_period_num': 126, 'train_rates': 0.8761231621592693, 'learning_rate': 0.0004222705327770039, 'batch_size': 37, 'step_size': 1, 'gamma': 0.8599761687495977}. Best is trial 0 with value: 0.5133956687742489.[0m
[32m[I 2025-01-06 05:53:22,595][0m Trial 1 finished with value: 2.0515609553100864 and parameters: {'observation_period_num': 246, 'train_rates': 0.9198012449023589, 'learning_rate': 0.0006764244930091448, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8657302815847727}. Best is trial 0 with value: 0.5133956687742489.[0m
[32m[I 2025-01-06 05:57:02,196][0m Trial 2 finished with value: 0.18119588705498402 and parameters: {'observation_period_num': 158, 'train_rates': 0.9092918379965331, 'learning_rate': 0.00020802453199120037, 'batch_size': 108, 'step_size': 15, 'gamma': 0.7909884403380049}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 05:58:25,035][0m Trial 3 finished with value: 0.8789812363400423 and parameters: {'observation_period_num': 77, 'train_rates': 0.6807331781891074, 'learning_rate': 0.00020876066145909895, 'batch_size': 182, 'step_size': 6, 'gamma': 0.8607978911046331}. Best is trial 2 with value: 0.18119588705498402.[0m
Early stopping at epoch 52
[32m[I 2025-01-06 06:00:01,421][0m Trial 4 finished with value: 1.9949634026836705 and parameters: {'observation_period_num': 156, 'train_rates': 0.6828326037852023, 'learning_rate': 1.1349763408877889e-06, 'batch_size': 204, 'step_size': 1, 'gamma': 0.8634752434227511}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:00:41,101][0m Trial 5 finished with value: 1.3203401073311152 and parameters: {'observation_period_num': 31, 'train_rates': 0.7566012013679094, 'learning_rate': 4.47129659371529e-06, 'batch_size': 145, 'step_size': 6, 'gamma': 0.7661525904477369}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:05:06,455][0m Trial 6 finished with value: 1.020473575324155 and parameters: {'observation_period_num': 232, 'train_rates': 0.6078131429277949, 'learning_rate': 0.0004444397216311506, 'batch_size': 224, 'step_size': 7, 'gamma': 0.8250478483336355}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:08:13,673][0m Trial 7 finished with value: 1.4532274480523735 and parameters: {'observation_period_num': 152, 'train_rates': 0.7725005356796039, 'learning_rate': 4.659763321073609e-06, 'batch_size': 119, 'step_size': 3, 'gamma': 0.8330730909067495}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:11:51,063][0m Trial 8 finished with value: 1.5279332961676255 and parameters: {'observation_period_num': 172, 'train_rates': 0.771521231221481, 'learning_rate': 1.3994725224364256e-06, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9384417174882855}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:17:00,363][0m Trial 9 finished with value: 0.6716192435277136 and parameters: {'observation_period_num': 221, 'train_rates': 0.770773849894103, 'learning_rate': 0.00015423770245172902, 'batch_size': 33, 'step_size': 2, 'gamma': 0.7877278466518364}. Best is trial 2 with value: 0.18119588705498402.[0m
[32m[I 2025-01-06 06:19:10,229][0m Trial 10 finished with value: 0.13858389854431152 and parameters: {'observation_period_num': 93, 'train_rates': 0.9711656716272482, 'learning_rate': 5.5429729307168484e-05, 'batch_size': 95, 'step_size': 15, 'gamma': 0.7533868424630479}. Best is trial 10 with value: 0.13858389854431152.[0m
[32m[I 2025-01-06 06:21:23,930][0m Trial 11 finished with value: 0.15174201130867004 and parameters: {'observation_period_num': 94, 'train_rates': 0.9830287415405811, 'learning_rate': 4.849142693246435e-05, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7570925648007715}. Best is trial 10 with value: 0.13858389854431152.[0m
[32m[I 2025-01-06 06:23:22,409][0m Trial 12 finished with value: 0.15736784040927887 and parameters: {'observation_period_num': 84, 'train_rates': 0.9889898760870253, 'learning_rate': 4.0573625796740335e-05, 'batch_size': 80, 'step_size': 12, 'gamma': 0.7570425085216078}. Best is trial 10 with value: 0.13858389854431152.[0m
[32m[I 2025-01-06 06:25:22,940][0m Trial 13 finished with value: 0.13543619215488434 and parameters: {'observation_period_num': 83, 'train_rates': 0.9864908526214599, 'learning_rate': 4.016033577975337e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9485936846349585}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:26:24,304][0m Trial 14 finished with value: 0.2465854792360695 and parameters: {'observation_period_num': 5, 'train_rates': 0.8654276233069246, 'learning_rate': 1.4011738600552833e-05, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9833888588894053}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:27:26,630][0m Trial 15 finished with value: 0.2224401590436004 and parameters: {'observation_period_num': 48, 'train_rates': 0.926076452517258, 'learning_rate': 1.88539465586366e-05, 'batch_size': 152, 'step_size': 12, 'gamma': 0.9238926428163179}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:29:58,128][0m Trial 16 finished with value: 0.40865197849216506 and parameters: {'observation_period_num': 115, 'train_rates': 0.8532868730550559, 'learning_rate': 7.296950954593161e-05, 'batch_size': 70, 'step_size': 10, 'gamma': 0.905141774812853}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:31:15,919][0m Trial 17 finished with value: 0.19499472265404866 and parameters: {'observation_period_num': 58, 'train_rates': 0.9543149061888738, 'learning_rate': 1.1882274614084259e-05, 'batch_size': 123, 'step_size': 13, 'gamma': 0.986212119281227}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:33:24,515][0m Trial 18 finished with value: 0.33578614332523954 and parameters: {'observation_period_num': 106, 'train_rates': 0.8237376933670928, 'learning_rate': 9.354842952105936e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.9494267222549481}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:34:47,786][0m Trial 19 finished with value: 0.17199269804491926 and parameters: {'observation_period_num': 63, 'train_rates': 0.9536558230546623, 'learning_rate': 1.925807952882548e-05, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8991923356446299}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:39:23,804][0m Trial 20 finished with value: 0.24283458789189658 and parameters: {'observation_period_num': 191, 'train_rates': 0.8959682374022321, 'learning_rate': 6.0237067093118616e-06, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9606198130904466}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:41:35,336][0m Trial 21 finished with value: 0.14555484056472778 and parameters: {'observation_period_num': 92, 'train_rates': 0.9895945660306158, 'learning_rate': 6.037385557345365e-05, 'batch_size': 94, 'step_size': 15, 'gamma': 0.7928054250507479}. Best is trial 13 with value: 0.13543619215488434.[0m
[32m[I 2025-01-06 06:42:27,672][0m Trial 22 finished with value: 0.111648212952219 and parameters: {'observation_period_num': 29, 'train_rates': 0.9464849690169475, 'learning_rate': 9.076426475709987e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.7907390078436485}. Best is trial 22 with value: 0.111648212952219.[0m
[32m[I 2025-01-06 06:43:46,480][0m Trial 23 finished with value: 0.13208253495395184 and parameters: {'observation_period_num': 23, 'train_rates': 0.9605944960993398, 'learning_rate': 2.7687766941705216e-05, 'batch_size': 57, 'step_size': 11, 'gamma': 0.8259266085751258}. Best is trial 22 with value: 0.111648212952219.[0m
[32m[I 2025-01-06 06:45:26,624][0m Trial 24 finished with value: 0.10870848269901466 and parameters: {'observation_period_num': 7, 'train_rates': 0.9318102032240988, 'learning_rate': 3.234158253433312e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.8314024783641558}. Best is trial 24 with value: 0.10870848269901466.[0m
[32m[I 2025-01-06 06:49:17,497][0m Trial 25 finished with value: 0.08769417701467956 and parameters: {'observation_period_num': 6, 'train_rates': 0.9401715331896612, 'learning_rate': 0.000126110372061075, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8293976324408011}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 06:52:59,292][0m Trial 26 finished with value: 0.21252139224836478 and parameters: {'observation_period_num': 5, 'train_rates': 0.8284692957578235, 'learning_rate': 0.00011600588704452042, 'batch_size': 16, 'step_size': 8, 'gamma': 0.811822302102592}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 06:56:12,796][0m Trial 27 finished with value: 0.18610207877348905 and parameters: {'observation_period_num': 33, 'train_rates': 0.931387758968794, 'learning_rate': 0.00029303948029067413, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8345581599724491}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 06:57:37,856][0m Trial 28 finished with value: 0.14391019678599126 and parameters: {'observation_period_num': 20, 'train_rates': 0.8866695356221429, 'learning_rate': 0.00010089565120658405, 'batch_size': 45, 'step_size': 8, 'gamma': 0.809796728063129}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 06:58:56,843][0m Trial 29 finished with value: 2.0356647862663753 and parameters: {'observation_period_num': 45, 'train_rates': 0.8371730357544689, 'learning_rate': 0.0009621158545572476, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8451265614071015}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:00:49,054][0m Trial 30 finished with value: 0.10466952885900226 and parameters: {'observation_period_num': 16, 'train_rates': 0.9404273704640275, 'learning_rate': 2.7098647555355414e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8896902624479702}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:02:43,326][0m Trial 31 finished with value: 0.1573905372345934 and parameters: {'observation_period_num': 5, 'train_rates': 0.9336746065096988, 'learning_rate': 8.512917144300939e-06, 'batch_size': 37, 'step_size': 11, 'gamma': 0.888962056478723}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:04:49,860][0m Trial 32 finished with value: 0.1873918600248162 and parameters: {'observation_period_num': 41, 'train_rates': 0.8841004185291463, 'learning_rate': 2.9301330173648224e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.8774398910830166}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:08:20,730][0m Trial 33 finished with value: 0.16521437301793518 and parameters: {'observation_period_num': 19, 'train_rates': 0.9074194363698542, 'learning_rate': 0.00017096319541215523, 'batch_size': 18, 'step_size': 11, 'gamma': 0.8444778018079289}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:09:57,062][0m Trial 34 finished with value: 0.4488836008738848 and parameters: {'observation_period_num': 60, 'train_rates': 0.9474566483180517, 'learning_rate': 0.00039949077019610753, 'batch_size': 44, 'step_size': 14, 'gamma': 0.7821237586150527}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:10:56,565][0m Trial 35 finished with value: 0.18311826441739057 and parameters: {'observation_period_num': 18, 'train_rates': 0.9118498670296759, 'learning_rate': 2.290588559477426e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8032140735467086}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:11:42,296][0m Trial 36 finished with value: 0.5964496378686018 and parameters: {'observation_period_num': 34, 'train_rates': 0.7312597956402095, 'learning_rate': 7.966253104193274e-05, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8490298152129292}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:13:17,073][0m Trial 37 finished with value: 0.16480895648888552 and parameters: {'observation_period_num': 53, 'train_rates': 0.9369456183581618, 'learning_rate': 0.00023815967533720355, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8699379432783332}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:15:28,064][0m Trial 38 finished with value: 0.352502254809407 and parameters: {'observation_period_num': 70, 'train_rates': 0.8074113557585186, 'learning_rate': 0.00013839600806424686, 'batch_size': 28, 'step_size': 4, 'gamma': 0.7720562957679419}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:18:22,125][0m Trial 39 finished with value: 0.5904401748421344 and parameters: {'observation_period_num': 131, 'train_rates': 0.8668048670928811, 'learning_rate': 1.0844772293573974e-05, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8202204329394833}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:18:51,236][0m Trial 40 finished with value: 1.7742091425678186 and parameters: {'observation_period_num': 25, 'train_rates': 0.6107446320290111, 'learning_rate': 2.515790734534637e-06, 'batch_size': 196, 'step_size': 5, 'gamma': 0.8534775232595881}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:20:04,300][0m Trial 41 finished with value: 0.12970639416767704 and parameters: {'observation_period_num': 15, 'train_rates': 0.9580913369204116, 'learning_rate': 2.8532705404805593e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8254155543517402}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:21:19,825][0m Trial 42 finished with value: 0.1381636752553706 and parameters: {'observation_period_num': 14, 'train_rates': 0.9100250812997832, 'learning_rate': 3.309626590102096e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7996383634834447}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:22:12,762][0m Trial 43 finished with value: 0.1319819147357289 and parameters: {'observation_period_num': 36, 'train_rates': 0.9449491081464444, 'learning_rate': 5.820846618792777e-05, 'batch_size': 108, 'step_size': 9, 'gamma': 0.8760614287683499}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:24:21,980][0m Trial 44 finished with value: 0.08840016529117663 and parameters: {'observation_period_num': 14, 'train_rates': 0.9671068835152556, 'learning_rate': 4.1158247003705786e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8328603217347863}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:26:39,985][0m Trial 45 finished with value: 0.11698694047498089 and parameters: {'observation_period_num': 29, 'train_rates': 0.9204853761058162, 'learning_rate': 4.2053124571964735e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8590400578027061}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:28:33,544][0m Trial 46 finished with value: 0.11043250978550065 and parameters: {'observation_period_num': 44, 'train_rates': 0.9634932085479317, 'learning_rate': 7.129972014115105e-05, 'batch_size': 38, 'step_size': 7, 'gamma': 0.8376727290462257}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:30:28,470][0m Trial 47 finished with value: 0.15618727557799394 and parameters: {'observation_period_num': 43, 'train_rates': 0.971128485284174, 'learning_rate': 1.6219923152940723e-05, 'batch_size': 38, 'step_size': 7, 'gamma': 0.8346432431566209}. Best is trial 25 with value: 0.08769417701467956.[0m
[32m[I 2025-01-06 07:33:09,070][0m Trial 48 finished with value: 0.08152102396405976 and parameters: {'observation_period_num': 12, 'train_rates': 0.9706370506297205, 'learning_rate': 6.813069823710514e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.889541132111941}. Best is trial 48 with value: 0.08152102396405976.[0m
[32m[I 2025-01-06 07:36:08,541][0m Trial 49 finished with value: 0.060445611675580345 and parameters: {'observation_period_num': 12, 'train_rates': 0.9756183614208547, 'learning_rate': 4.743947090330123e-05, 'batch_size': 24, 'step_size': 5, 'gamma': 0.9192374316669044}. Best is trial 49 with value: 0.060445611675580345.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.6246 | 0.6017
Epoch 2/300, Loss: 0.4788 | 0.5632
Epoch 3/300, Loss: 0.3619 | 0.4949
Epoch 4/300, Loss: 0.2864 | 0.3741
Epoch 5/300, Loss: 0.2516 | 0.3154
Epoch 6/300, Loss: 0.2507 | 0.2955
Epoch 7/300, Loss: 0.2179 | 0.2948
Epoch 8/300, Loss: 0.2017 | 0.2512
Epoch 9/300, Loss: 0.1981 | 0.2245
Epoch 10/300, Loss: 0.1923 | 0.2715
Epoch 11/300, Loss: 0.1807 | 0.2210
Epoch 12/300, Loss: 0.1736 | 0.2091
Epoch 13/300, Loss: 0.1704 | 0.2198
Epoch 14/300, Loss: 0.1670 | 0.2249
Epoch 15/300, Loss: 0.1614 | 0.1936
Epoch 16/300, Loss: 0.1606 | 0.2103
Epoch 17/300, Loss: 0.1576 | 0.1918
Epoch 18/300, Loss: 0.1527 | 0.1851
Epoch 19/300, Loss: 0.1521 | 0.1578
Epoch 20/300, Loss: 0.1501 | 0.2029
Epoch 21/300, Loss: 0.1453 | 0.1596
Epoch 22/300, Loss: 0.1434 | 0.1448
Epoch 23/300, Loss: 0.1394 | 0.1486
Epoch 24/300, Loss: 0.1383 | 0.1608
Epoch 25/300, Loss: 0.1354 | 0.1639
Epoch 26/300, Loss: 0.1334 | 0.1293
Epoch 27/300, Loss: 0.1316 | 0.1443
Epoch 28/300, Loss: 0.1284 | 0.1390
Epoch 29/300, Loss: 0.1274 | 0.1541
Epoch 30/300, Loss: 0.1254 | 0.1205
Epoch 31/300, Loss: 0.1251 | 0.1263
Epoch 32/300, Loss: 0.1210 | 0.1259
Epoch 33/300, Loss: 0.1185 | 0.1255
Epoch 34/300, Loss: 0.1167 | 0.1096
Epoch 35/300, Loss: 0.1162 | 0.1098
Epoch 36/300, Loss: 0.1129 | 0.1145
Epoch 37/300, Loss: 0.1120 | 0.1094
Epoch 38/300, Loss: 0.1094 | 0.1003
Epoch 39/300, Loss: 0.1092 | 0.1016
Epoch 40/300, Loss: 0.1069 | 0.1069
Epoch 41/300, Loss: 0.1051 | 0.1028
Epoch 42/300, Loss: 0.1057 | 0.0961
Epoch 43/300, Loss: 0.1044 | 0.0946
Epoch 44/300, Loss: 0.1032 | 0.1008
Epoch 45/300, Loss: 0.1022 | 0.0995
Epoch 46/300, Loss: 0.1018 | 0.0932
Epoch 47/300, Loss: 0.1005 | 0.0913
Epoch 48/300, Loss: 0.1002 | 0.0930
Epoch 49/300, Loss: 0.0990 | 0.0955
Epoch 50/300, Loss: 0.0987 | 0.0910
Epoch 51/300, Loss: 0.0978 | 0.0865
Epoch 52/300, Loss: 0.0962 | 0.0885
Epoch 53/300, Loss: 0.0964 | 0.0893
Epoch 54/300, Loss: 0.0960 | 0.0868
Epoch 55/300, Loss: 0.0957 | 0.0849
Epoch 56/300, Loss: 0.0945 | 0.0853
Epoch 57/300, Loss: 0.0941 | 0.0841
Epoch 58/300, Loss: 0.0936 | 0.0833
Epoch 59/300, Loss: 0.0932 | 0.0832
Epoch 60/300, Loss: 0.0929 | 0.0839
Epoch 61/300, Loss: 0.0922 | 0.0808
Epoch 62/300, Loss: 0.0918 | 0.0825
Epoch 63/300, Loss: 0.0916 | 0.0819
Epoch 64/300, Loss: 0.0914 | 0.0828
Epoch 65/300, Loss: 0.0912 | 0.0807
Epoch 66/300, Loss: 0.0909 | 0.0803
Epoch 67/300, Loss: 0.0903 | 0.0805
Epoch 68/300, Loss: 0.0892 | 0.0812
Epoch 69/300, Loss: 0.0897 | 0.0789
Epoch 70/300, Loss: 0.0898 | 0.0797
Epoch 71/300, Loss: 0.0892 | 0.0796
Epoch 72/300, Loss: 0.0881 | 0.0782
Epoch 73/300, Loss: 0.0884 | 0.0788
Epoch 74/300, Loss: 0.0878 | 0.0785
Epoch 75/300, Loss: 0.0882 | 0.0778
Epoch 76/300, Loss: 0.0880 | 0.0784
Epoch 77/300, Loss: 0.0873 | 0.0781
Epoch 78/300, Loss: 0.0872 | 0.0769
Epoch 79/300, Loss: 0.0879 | 0.0785
Epoch 80/300, Loss: 0.0874 | 0.0774
Epoch 81/300, Loss: 0.0873 | 0.0768
Epoch 82/300, Loss: 0.0865 | 0.0768
Epoch 83/300, Loss: 0.0862 | 0.0761
Epoch 84/300, Loss: 0.0865 | 0.0770
Epoch 85/300, Loss: 0.0862 | 0.0762
Epoch 86/300, Loss: 0.0855 | 0.0763
Epoch 87/300, Loss: 0.0857 | 0.0761
Epoch 88/300, Loss: 0.0854 | 0.0759
Epoch 89/300, Loss: 0.0855 | 0.0760
Epoch 90/300, Loss: 0.0851 | 0.0755
Epoch 91/300, Loss: 0.0853 | 0.0749
Epoch 92/300, Loss: 0.0855 | 0.0758
Epoch 93/300, Loss: 0.0859 | 0.0750
Epoch 94/300, Loss: 0.0851 | 0.0749
Epoch 95/300, Loss: 0.0848 | 0.0745
Epoch 96/300, Loss: 0.0847 | 0.0750
Epoch 97/300, Loss: 0.0847 | 0.0751
Epoch 98/300, Loss: 0.0848 | 0.0750
Epoch 99/300, Loss: 0.0848 | 0.0744
Epoch 100/300, Loss: 0.0844 | 0.0746
Epoch 101/300, Loss: 0.0844 | 0.0745
Epoch 102/300, Loss: 0.0837 | 0.0747
Epoch 103/300, Loss: 0.0843 | 0.0740
Epoch 104/300, Loss: 0.0841 | 0.0736
Epoch 105/300, Loss: 0.0836 | 0.0738
Epoch 106/300, Loss: 0.0835 | 0.0738
Epoch 107/300, Loss: 0.0837 | 0.0734
Epoch 108/300, Loss: 0.0837 | 0.0737
Epoch 109/300, Loss: 0.0840 | 0.0736
Epoch 110/300, Loss: 0.0834 | 0.0738
Epoch 111/300, Loss: 0.0832 | 0.0735
Epoch 112/300, Loss: 0.0837 | 0.0732
Epoch 113/300, Loss: 0.0834 | 0.0728
Epoch 114/300, Loss: 0.0835 | 0.0734
Epoch 115/300, Loss: 0.0831 | 0.0738
Epoch 116/300, Loss: 0.0837 | 0.0727
Epoch 117/300, Loss: 0.0831 | 0.0728
Epoch 118/300, Loss: 0.0829 | 0.0731
Epoch 119/300, Loss: 0.0828 | 0.0731
Epoch 120/300, Loss: 0.0827 | 0.0730
Epoch 121/300, Loss: 0.0827 | 0.0733
Epoch 122/300, Loss: 0.0827 | 0.0728
Epoch 123/300, Loss: 0.0826 | 0.0731
Epoch 124/300, Loss: 0.0827 | 0.0730
Epoch 125/300, Loss: 0.0830 | 0.0730
Epoch 126/300, Loss: 0.0829 | 0.0728
Epoch 127/300, Loss: 0.0824 | 0.0731
Epoch 128/300, Loss: 0.0826 | 0.0734
Epoch 129/300, Loss: 0.0826 | 0.0732
Epoch 130/300, Loss: 0.0819 | 0.0728
Epoch 131/300, Loss: 0.0828 | 0.0727
Epoch 132/300, Loss: 0.0824 | 0.0726
Epoch 133/300, Loss: 0.0823 | 0.0726
Epoch 134/300, Loss: 0.0827 | 0.0725
Epoch 135/300, Loss: 0.0826 | 0.0724
Epoch 136/300, Loss: 0.0830 | 0.0723
Epoch 137/300, Loss: 0.0827 | 0.0724
Epoch 138/300, Loss: 0.0823 | 0.0725
Epoch 139/300, Loss: 0.0820 | 0.0727
Epoch 140/300, Loss: 0.0823 | 0.0725
Epoch 141/300, Loss: 0.0821 | 0.0725
Epoch 142/300, Loss: 0.0827 | 0.0723
Epoch 143/300, Loss: 0.0822 | 0.0722
Epoch 144/300, Loss: 0.0817 | 0.0725
Epoch 145/300, Loss: 0.0824 | 0.0725
Epoch 146/300, Loss: 0.0823 | 0.0724
Epoch 147/300, Loss: 0.0824 | 0.0723
Epoch 148/300, Loss: 0.0827 | 0.0723
Epoch 149/300, Loss: 0.0823 | 0.0723
Epoch 150/300, Loss: 0.0824 | 0.0723
Epoch 151/300, Loss: 0.0826 | 0.0723
Epoch 152/300, Loss: 0.0823 | 0.0724
Epoch 153/300, Loss: 0.0824 | 0.0723
Epoch 154/300, Loss: 0.0825 | 0.0723
Epoch 155/300, Loss: 0.0816 | 0.0721
Epoch 156/300, Loss: 0.0819 | 0.0721
Epoch 157/300, Loss: 0.0829 | 0.0722
Epoch 158/300, Loss: 0.0821 | 0.0722
Epoch 159/300, Loss: 0.0818 | 0.0723
Epoch 160/300, Loss: 0.0825 | 0.0723
Epoch 161/300, Loss: 0.0818 | 0.0722
Epoch 162/300, Loss: 0.0821 | 0.0721
Epoch 163/300, Loss: 0.0821 | 0.0720
Epoch 164/300, Loss: 0.0834 | 0.0720
Epoch 165/300, Loss: 0.0824 | 0.0720
Epoch 166/300, Loss: 0.0823 | 0.0722
Epoch 167/300, Loss: 0.0816 | 0.0722
Epoch 168/300, Loss: 0.0821 | 0.0720
Epoch 169/300, Loss: 0.0820 | 0.0721
Epoch 170/300, Loss: 0.0821 | 0.0721
Epoch 171/300, Loss: 0.0823 | 0.0722
Epoch 172/300, Loss: 0.0819 | 0.0721
Epoch 173/300, Loss: 0.0820 | 0.0721
Epoch 174/300, Loss: 0.0817 | 0.0721
Epoch 175/300, Loss: 0.0820 | 0.0721
Epoch 176/300, Loss: 0.0817 | 0.0721
Epoch 177/300, Loss: 0.0818 | 0.0721
Epoch 178/300, Loss: 0.0826 | 0.0720
Epoch 179/300, Loss: 0.0822 | 0.0721
Epoch 180/300, Loss: 0.0814 | 0.0720
Epoch 181/300, Loss: 0.0818 | 0.0720
Epoch 182/300, Loss: 0.0822 | 0.0719
Epoch 183/300, Loss: 0.0815 | 0.0719
Epoch 184/300, Loss: 0.0818 | 0.0719
Epoch 185/300, Loss: 0.0816 | 0.0719
Epoch 186/300, Loss: 0.0821 | 0.0719
Epoch 187/300, Loss: 0.0820 | 0.0719
Epoch 188/300, Loss: 0.0816 | 0.0719
Epoch 189/300, Loss: 0.0817 | 0.0718
Epoch 190/300, Loss: 0.0815 | 0.0719
Epoch 191/300, Loss: 0.0819 | 0.0719
Epoch 192/300, Loss: 0.0822 | 0.0718
Epoch 193/300, Loss: 0.0819 | 0.0718
Epoch 194/300, Loss: 0.0820 | 0.0718
Epoch 195/300, Loss: 0.0820 | 0.0718
Epoch 196/300, Loss: 0.0821 | 0.0718
Epoch 197/300, Loss: 0.0822 | 0.0718
Epoch 198/300, Loss: 0.0821 | 0.0719
Epoch 199/300, Loss: 0.0821 | 0.0719
Epoch 200/300, Loss: 0.0814 | 0.0719
Epoch 201/300, Loss: 0.0821 | 0.0719
Epoch 202/300, Loss: 0.0819 | 0.0719
Epoch 203/300, Loss: 0.0820 | 0.0719
Epoch 204/300, Loss: 0.0824 | 0.0719
Epoch 205/300, Loss: 0.0813 | 0.0719
Epoch 206/300, Loss: 0.0822 | 0.0719
Epoch 207/300, Loss: 0.0822 | 0.0719
Epoch 208/300, Loss: 0.0817 | 0.0719
Epoch 209/300, Loss: 0.0815 | 0.0719
Epoch 210/300, Loss: 0.0819 | 0.0719
Epoch 211/300, Loss: 0.0817 | 0.0719
Epoch 212/300, Loss: 0.0815 | 0.0719
Epoch 213/300, Loss: 0.0820 | 0.0719
Epoch 214/300, Loss: 0.0823 | 0.0719
Epoch 215/300, Loss: 0.0820 | 0.0719
Epoch 216/300, Loss: 0.0820 | 0.0719
Epoch 217/300, Loss: 0.0823 | 0.0719
Epoch 218/300, Loss: 0.0820 | 0.0719
Epoch 219/300, Loss: 0.0815 | 0.0719
Epoch 220/300, Loss: 0.0823 | 0.0719
Epoch 221/300, Loss: 0.0820 | 0.0719
Epoch 222/300, Loss: 0.0819 | 0.0718
Epoch 223/300, Loss: 0.0816 | 0.0718
Epoch 224/300, Loss: 0.0821 | 0.0718
Epoch 225/300, Loss: 0.0817 | 0.0718
Epoch 226/300, Loss: 0.0811 | 0.0718
Epoch 227/300, Loss: 0.0823 | 0.0718
Epoch 228/300, Loss: 0.0821 | 0.0718
Epoch 229/300, Loss: 0.0814 | 0.0718
Epoch 230/300, Loss: 0.0823 | 0.0718
Epoch 231/300, Loss: 0.0820 | 0.0718
Epoch 232/300, Loss: 0.0816 | 0.0718
Epoch 233/300, Loss: 0.0817 | 0.0718
Epoch 234/300, Loss: 0.0820 | 0.0718
Epoch 235/300, Loss: 0.0822 | 0.0718
Epoch 236/300, Loss: 0.0822 | 0.0718
Epoch 237/300, Loss: 0.0819 | 0.0718
Epoch 238/300, Loss: 0.0820 | 0.0718
Epoch 239/300, Loss: 0.0824 | 0.0718
Epoch 240/300, Loss: 0.0816 | 0.0718
Epoch 241/300, Loss: 0.0810 | 0.0718
Epoch 242/300, Loss: 0.0821 | 0.0718
Epoch 243/300, Loss: 0.0814 | 0.0718
Epoch 244/300, Loss: 0.0817 | 0.0718
Epoch 245/300, Loss: 0.0821 | 0.0718
Epoch 246/300, Loss: 0.0816 | 0.0718
Epoch 247/300, Loss: 0.0817 | 0.0718
Epoch 248/300, Loss: 0.0819 | 0.0718
Epoch 249/300, Loss: 0.0818 | 0.0718
Epoch 250/300, Loss: 0.0822 | 0.0718
Epoch 251/300, Loss: 0.0819 | 0.0718
Epoch 252/300, Loss: 0.0819 | 0.0718
Epoch 253/300, Loss: 0.0823 | 0.0718
Epoch 254/300, Loss: 0.0817 | 0.0718
Epoch 255/300, Loss: 0.0811 | 0.0718
Epoch 256/300, Loss: 0.0817 | 0.0718
Epoch 257/300, Loss: 0.0823 | 0.0718
Epoch 258/300, Loss: 0.0817 | 0.0718
Epoch 259/300, Loss: 0.0819 | 0.0718
Epoch 260/300, Loss: 0.0814 | 0.0718
Epoch 261/300, Loss: 0.0814 | 0.0718
Epoch 262/300, Loss: 0.0820 | 0.0718
Epoch 263/300, Loss: 0.0814 | 0.0718
Epoch 264/300, Loss: 0.0816 | 0.0718
Epoch 265/300, Loss: 0.0822 | 0.0718
Epoch 266/300, Loss: 0.0816 | 0.0718
Epoch 267/300, Loss: 0.0825 | 0.0718
Early stopping
Runtime (seconds): 517.6362860202789
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 285.8971369510982
RMSE: 16.908493041992188
MAE: 16.908493041992188
R-squared: nan
[188.83151]
