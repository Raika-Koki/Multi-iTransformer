ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 16:51:13,841][0m A new study created in memory with name: no-name-90e56b9d-ae29-4c31-ba91-aa4ad935dd2f[0m
[32m[I 2025-01-03 16:55:33,075][0m Trial 0 finished with value: 0.1953449358674775 and parameters: {'observation_period_num': 187, 'train_rates': 0.8913220626311986, 'learning_rate': 8.253063402993487e-05, 'batch_size': 141, 'step_size': 7, 'gamma': 0.8550705570349041}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 16:58:53,439][0m Trial 1 finished with value: 0.6733758431560588 and parameters: {'observation_period_num': 150, 'train_rates': 0.7163093186946404, 'learning_rate': 7.643839054771645e-06, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8831795364127453}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 16:59:58,669][0m Trial 2 finished with value: 0.7719302662583285 and parameters: {'observation_period_num': 52, 'train_rates': 0.8813966349235012, 'learning_rate': 4.986406012702393e-06, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8824693275138074}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 17:02:44,230][0m Trial 3 finished with value: 0.5569427486996092 and parameters: {'observation_period_num': 131, 'train_rates': 0.7913291606514682, 'learning_rate': 0.0008176301290344812, 'batch_size': 172, 'step_size': 14, 'gamma': 0.7871740740872616}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 17:05:34,443][0m Trial 4 finished with value: 0.5124154564525399 and parameters: {'observation_period_num': 126, 'train_rates': 0.881353715348584, 'learning_rate': 4.699554610499595e-06, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8631770607918522}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 17:06:06,034][0m Trial 5 finished with value: 0.29705732880216656 and parameters: {'observation_period_num': 22, 'train_rates': 0.8422706133927003, 'learning_rate': 7.360061016708088e-05, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8670742072980774}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 17:11:20,214][0m Trial 6 finished with value: 0.31241506881533415 and parameters: {'observation_period_num': 220, 'train_rates': 0.8406757252957215, 'learning_rate': 0.0001754255311315175, 'batch_size': 58, 'step_size': 14, 'gamma': 0.8176732422690841}. Best is trial 0 with value: 0.1953449358674775.[0m
[32m[I 2025-01-03 17:15:50,618][0m Trial 7 finished with value: 0.1622232098632784 and parameters: {'observation_period_num': 188, 'train_rates': 0.9273241042610338, 'learning_rate': 0.00016735374876285694, 'batch_size': 167, 'step_size': 12, 'gamma': 0.8863027656309376}. Best is trial 7 with value: 0.1622232098632784.[0m
[32m[I 2025-01-03 17:16:34,468][0m Trial 8 finished with value: 0.8724396387736003 and parameters: {'observation_period_num': 39, 'train_rates': 0.6548778631483247, 'learning_rate': 0.000479320425415985, 'batch_size': 173, 'step_size': 13, 'gamma': 0.7751819019790239}. Best is trial 7 with value: 0.1622232098632784.[0m
[32m[I 2025-01-03 17:17:24,602][0m Trial 9 finished with value: 1.2631983510860998 and parameters: {'observation_period_num': 49, 'train_rates': 0.6180575515081959, 'learning_rate': 0.0005041343667391086, 'batch_size': 201, 'step_size': 8, 'gamma': 0.9844374004372292}. Best is trial 7 with value: 0.1622232098632784.[0m
[32m[I 2025-01-03 17:23:55,769][0m Trial 10 finished with value: 0.5856320858001709 and parameters: {'observation_period_num': 248, 'train_rates': 0.9603715407371398, 'learning_rate': 1.7056789014697325e-05, 'batch_size': 253, 'step_size': 3, 'gamma': 0.9528860755594772}. Best is trial 7 with value: 0.1622232098632784.[0m
[32m[I 2025-01-03 17:28:37,972][0m Trial 11 finished with value: 0.1041894257068634 and parameters: {'observation_period_num': 188, 'train_rates': 0.9854605729180616, 'learning_rate': 8.083161566480782e-05, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9115135882393838}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:33:27,548][0m Trial 12 finished with value: 0.8990923762321472 and parameters: {'observation_period_num': 191, 'train_rates': 0.989838068500364, 'learning_rate': 1.4410366818342659e-06, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9308096886354239}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:37:23,356][0m Trial 13 finished with value: 0.18929105228299548 and parameters: {'observation_period_num': 163, 'train_rates': 0.9438851177329143, 'learning_rate': 0.0001599519562020165, 'batch_size': 109, 'step_size': 11, 'gamma': 0.9243674477343984}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:39:51,078][0m Trial 14 finished with value: 0.19116626059645928 and parameters: {'observation_period_num': 111, 'train_rates': 0.9309748634608017, 'learning_rate': 4.15467097381103e-05, 'batch_size': 144, 'step_size': 11, 'gamma': 0.905553474787042}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:41:49,539][0m Trial 15 finished with value: 0.39066579542160035 and parameters: {'observation_period_num': 95, 'train_rates': 0.7814522996498685, 'learning_rate': 0.00018182861730444447, 'batch_size': 86, 'step_size': 10, 'gamma': 0.8278589583048411}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:47:16,019][0m Trial 16 finished with value: 0.3241146206855774 and parameters: {'observation_period_num': 209, 'train_rates': 0.9782695967129625, 'learning_rate': 1.928312158207234e-05, 'batch_size': 145, 'step_size': 1, 'gamma': 0.96805107862612}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:53:40,887][0m Trial 17 finished with value: 0.16793141180747434 and parameters: {'observation_period_num': 251, 'train_rates': 0.9156861074393173, 'learning_rate': 5.227823915833597e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.9105297171398752}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 17:57:06,548][0m Trial 18 finished with value: 0.46246843069831584 and parameters: {'observation_period_num': 168, 'train_rates': 0.731061524119496, 'learning_rate': 0.0002512018376489302, 'batch_size': 213, 'step_size': 5, 'gamma': 0.83112898064004}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 18:02:13,371][0m Trial 19 finished with value: 0.28976511103766306 and parameters: {'observation_period_num': 218, 'train_rates': 0.8466078512143772, 'learning_rate': 0.00011047647956905288, 'batch_size': 120, 'step_size': 9, 'gamma': 0.9388979533564357}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 18:03:59,442][0m Trial 20 finished with value: 0.23783008940517902 and parameters: {'observation_period_num': 80, 'train_rates': 0.9165902717231085, 'learning_rate': 1.9256490170800877e-05, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8988659516692927}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 18:10:11,085][0m Trial 21 finished with value: 0.20985661198695502 and parameters: {'observation_period_num': 240, 'train_rates': 0.9382936811965806, 'learning_rate': 4.515389897640412e-05, 'batch_size': 91, 'step_size': 12, 'gamma': 0.9122713277178391}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 18:14:31,639][0m Trial 22 finished with value: 0.17759409295708944 and parameters: {'observation_period_num': 183, 'train_rates': 0.9037087330866074, 'learning_rate': 5.929800212894305e-05, 'batch_size': 90, 'step_size': 10, 'gamma': 0.8915006491569283}. Best is trial 11 with value: 0.1041894257068634.[0m
[32m[I 2025-01-03 18:20:59,599][0m Trial 23 finished with value: 0.08861668407917023 and parameters: {'observation_period_num': 231, 'train_rates': 0.9892328385395216, 'learning_rate': 2.9934062097314505e-05, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9519192831312473}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:26:51,562][0m Trial 24 finished with value: 0.2359962113593754 and parameters: {'observation_period_num': 203, 'train_rates': 0.9655112235747328, 'learning_rate': 0.0002971129356155486, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9616535190162028}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:33:19,109][0m Trial 25 finished with value: 0.09164371341466904 and parameters: {'observation_period_num': 233, 'train_rates': 0.9882041128713724, 'learning_rate': 2.773108507096636e-05, 'batch_size': 41, 'step_size': 13, 'gamma': 0.947764612262473}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:39:32,529][0m Trial 26 finished with value: 0.12875940368093294 and parameters: {'observation_period_num': 229, 'train_rates': 0.9789129640800651, 'learning_rate': 9.416883780980291e-06, 'batch_size': 42, 'step_size': 13, 'gamma': 0.9860136987297036}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:45:40,924][0m Trial 27 finished with value: 0.19074267894029617 and parameters: {'observation_period_num': 231, 'train_rates': 0.956092606489839, 'learning_rate': 2.717420206713199e-05, 'batch_size': 57, 'step_size': 10, 'gamma': 0.9432767769270485}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:50:54,147][0m Trial 28 finished with value: 0.2660940146510492 and parameters: {'observation_period_num': 206, 'train_rates': 0.8650554920626534, 'learning_rate': 1.1765503562826801e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9684497914205109}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 18:54:44,785][0m Trial 29 finished with value: 0.31421924073947594 and parameters: {'observation_period_num': 171, 'train_rates': 0.81617861058364, 'learning_rate': 3.105913464884612e-05, 'batch_size': 71, 'step_size': 9, 'gamma': 0.9502824049136237}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 19:00:28,579][0m Trial 30 finished with value: 0.7289015023604684 and parameters: {'observation_period_num': 234, 'train_rates': 0.7462707837593864, 'learning_rate': 2.2642379190158813e-06, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9264632986004895}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 19:06:39,386][0m Trial 31 finished with value: 0.13807513987024625 and parameters: {'observation_period_num': 229, 'train_rates': 0.9782249485215612, 'learning_rate': 9.424327665008148e-06, 'batch_size': 46, 'step_size': 13, 'gamma': 0.9884278984908671}. Best is trial 23 with value: 0.08861668407917023.[0m
[32m[I 2025-01-03 19:12:05,834][0m Trial 32 finished with value: 0.08031769841909409 and parameters: {'observation_period_num': 200, 'train_rates': 0.9880778068132221, 'learning_rate': 0.00010165307808629653, 'batch_size': 41, 'step_size': 13, 'gamma': 0.9777358735225424}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:17:26,225][0m Trial 33 finished with value: 0.18887723534178913 and parameters: {'observation_period_num': 202, 'train_rates': 0.9519244521048134, 'learning_rate': 0.00010700094884899118, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9731736299651791}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:21:46,017][0m Trial 34 finished with value: 0.19589332820939237 and parameters: {'observation_period_num': 181, 'train_rates': 0.8900401762820216, 'learning_rate': 9.370765350238277e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9548463178057954}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:25:28,951][0m Trial 35 finished with value: 0.2150460141665727 and parameters: {'observation_period_num': 149, 'train_rates': 0.9635206064648119, 'learning_rate': 2.9185866202654393e-05, 'batch_size': 73, 'step_size': 6, 'gamma': 0.9217338508839916}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:29:25,180][0m Trial 36 finished with value: 0.09724430171772838 and parameters: {'observation_period_num': 146, 'train_rates': 0.98593066025706, 'learning_rate': 6.379282562245438e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.9419749321295416}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:32:31,522][0m Trial 37 finished with value: 0.7644889047641432 and parameters: {'observation_period_num': 147, 'train_rates': 0.6829115098747603, 'learning_rate': 5.5548843893564086e-06, 'batch_size': 31, 'step_size': 9, 'gamma': 0.9398002971170498}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:35:54,353][0m Trial 38 finished with value: 0.16053270728614694 and parameters: {'observation_period_num': 138, 'train_rates': 0.9046181927033745, 'learning_rate': 3.585981753636048e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9755427906586789}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:41:50,870][0m Trial 39 finished with value: 0.15792928247170135 and parameters: {'observation_period_num': 218, 'train_rates': 0.933733113789231, 'learning_rate': 6.869990969449204e-05, 'batch_size': 23, 'step_size': 14, 'gamma': 0.9617776635670063}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:44:24,494][0m Trial 40 finished with value: 0.2954254096466533 and parameters: {'observation_period_num': 117, 'train_rates': 0.8722713279259324, 'learning_rate': 1.4853770563464673e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8790079888677246}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:49:39,226][0m Trial 41 finished with value: 0.08351121842861176 and parameters: {'observation_period_num': 199, 'train_rates': 0.9882997952551592, 'learning_rate': 7.249020284218171e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.9342444317830975}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 19:55:30,782][0m Trial 42 finished with value: 0.11252553015947342 and parameters: {'observation_period_num': 220, 'train_rates': 0.988781638458854, 'learning_rate': 2.3368212240172977e-05, 'batch_size': 62, 'step_size': 12, 'gamma': 0.851809189395104}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:02:00,789][0m Trial 43 finished with value: 0.15252375029600584 and parameters: {'observation_period_num': 243, 'train_rates': 0.9667403573352339, 'learning_rate': 5.061659465619565e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.9483970581431556}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:07:11,095][0m Trial 44 finished with value: 0.19074022557054246 and parameters: {'observation_period_num': 194, 'train_rates': 0.9544242429037946, 'learning_rate': 0.00013839949746317145, 'batch_size': 30, 'step_size': 11, 'gamma': 0.9779109349624046}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:12:41,219][0m Trial 45 finished with value: 0.16793998810526442 and parameters: {'observation_period_num': 212, 'train_rates': 0.9467785883373694, 'learning_rate': 6.759109070523145e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9323103187172792}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:16:47,285][0m Trial 46 finished with value: 0.2251655631831714 and parameters: {'observation_period_num': 164, 'train_rates': 0.9248600614924883, 'learning_rate': 0.0003471099796146414, 'batch_size': 37, 'step_size': 13, 'gamma': 0.7555765427828955}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:18:33,638][0m Trial 47 finished with value: 0.14916655566634202 and parameters: {'observation_period_num': 72, 'train_rates': 0.97156335169919, 'learning_rate': 3.885956878796453e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.9577863012908128}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:23:46,490][0m Trial 48 finished with value: 0.09955685585737228 and parameters: {'observation_period_num': 200, 'train_rates': 0.9893717229870509, 'learning_rate': 0.0001284333488385303, 'batch_size': 106, 'step_size': 11, 'gamma': 0.9374778149821066}. Best is trial 32 with value: 0.08031769841909409.[0m
[32m[I 2025-01-03 20:30:41,172][0m Trial 49 finished with value: 0.16768531890866023 and parameters: {'observation_period_num': 252, 'train_rates': 0.943365399227822, 'learning_rate': 8.198975299891426e-05, 'batch_size': 26, 'step_size': 8, 'gamma': 0.9216292908962075}. Best is trial 32 with value: 0.08031769841909409.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 20:30:41,181][0m A new study created in memory with name: no-name-e62b90aa-721c-45fd-947a-373554f74679[0m
[32m[I 2025-01-03 20:31:50,168][0m Trial 0 finished with value: 0.16810876964512517 and parameters: {'observation_period_num': 47, 'train_rates': 0.9596245371427728, 'learning_rate': 0.0002283350752232031, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9525491492347351}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:35:01,237][0m Trial 1 finished with value: 0.6663155823887449 and parameters: {'observation_period_num': 138, 'train_rates': 0.8191272851957574, 'learning_rate': 2.513018619167925e-06, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8625189000588394}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:40:03,162][0m Trial 2 finished with value: 0.5871578360476145 and parameters: {'observation_period_num': 217, 'train_rates': 0.715072149665489, 'learning_rate': 5.6958272374863426e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.8097274077185856}. Best is trial 0 with value: 0.16810876964512517.[0m
Early stopping at epoch 65
[32m[I 2025-01-03 20:40:32,880][0m Trial 3 finished with value: 1.455618051589208 and parameters: {'observation_period_num': 40, 'train_rates': 0.6957617115343406, 'learning_rate': 2.9667663315284016e-05, 'batch_size': 218, 'step_size': 1, 'gamma': 0.8171322131192812}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:44:06,988][0m Trial 4 finished with value: 1.335261344909668 and parameters: {'observation_period_num': 147, 'train_rates': 0.9697183194949032, 'learning_rate': 2.579703257953866e-06, 'batch_size': 242, 'step_size': 13, 'gamma': 0.7666080882519553}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:47:27,013][0m Trial 5 finished with value: 1.2083344092428456 and parameters: {'observation_period_num': 184, 'train_rates': 0.6227414611231818, 'learning_rate': 9.391495661293238e-06, 'batch_size': 154, 'step_size': 13, 'gamma': 0.8807737896912251}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:53:26,206][0m Trial 6 finished with value: 0.27355960303280435 and parameters: {'observation_period_num': 237, 'train_rates': 0.8590788548538311, 'learning_rate': 0.0002523224763628321, 'batch_size': 31, 'step_size': 2, 'gamma': 0.8541983116276683}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 20:58:11,127][0m Trial 7 finished with value: 0.6312292418539277 and parameters: {'observation_period_num': 194, 'train_rates': 0.9127670354674805, 'learning_rate': 0.0007666855108452402, 'batch_size': 90, 'step_size': 4, 'gamma': 0.8478470094680957}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 21:00:44,104][0m Trial 8 finished with value: 1.302069973878646 and parameters: {'observation_period_num': 129, 'train_rates': 0.6848995465903686, 'learning_rate': 0.0008881615184961993, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9481199634383352}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 21:04:31,295][0m Trial 9 finished with value: 1.2799769070348128 and parameters: {'observation_period_num': 198, 'train_rates': 0.6314244090412684, 'learning_rate': 1.0767357950908401e-05, 'batch_size': 240, 'step_size': 11, 'gamma': 0.8277507567084543}. Best is trial 0 with value: 0.16810876964512517.[0m
[32m[I 2025-01-03 21:05:15,183][0m Trial 10 finished with value: 0.15465447306632996 and parameters: {'observation_period_num': 30, 'train_rates': 0.9738313235488691, 'learning_rate': 0.00018729715050117425, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9897181695724314}. Best is trial 10 with value: 0.15465447306632996.[0m
[32m[I 2025-01-03 21:05:49,275][0m Trial 11 finished with value: 0.14382000267505646 and parameters: {'observation_period_num': 21, 'train_rates': 0.9779508418595892, 'learning_rate': 0.00011305350419805764, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9883465259754196}. Best is trial 11 with value: 0.14382000267505646.[0m
[32m[I 2025-01-03 21:06:13,908][0m Trial 12 finished with value: 0.1619097339836034 and parameters: {'observation_period_num': 9, 'train_rates': 0.8953964458358709, 'learning_rate': 0.00013635882849135074, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9888140978721255}. Best is trial 11 with value: 0.14382000267505646.[0m
[32m[I 2025-01-03 21:08:03,359][0m Trial 13 finished with value: 0.13380558788776398 and parameters: {'observation_period_num': 79, 'train_rates': 0.9848704536722028, 'learning_rate': 7.33136138999124e-05, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9124578123459325}. Best is trial 13 with value: 0.13380558788776398.[0m
[32m[I 2025-01-03 21:09:41,992][0m Trial 14 finished with value: 0.40716326852992585 and parameters: {'observation_period_num': 83, 'train_rates': 0.7843608107663242, 'learning_rate': 6.520038427270785e-05, 'batch_size': 188, 'step_size': 9, 'gamma': 0.905038321006439}. Best is trial 13 with value: 0.13380558788776398.[0m
[32m[I 2025-01-03 21:11:38,076][0m Trial 15 finished with value: 0.29368820786476135 and parameters: {'observation_period_num': 86, 'train_rates': 0.931764414566126, 'learning_rate': 1.860229790373288e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.9283032299231131}. Best is trial 13 with value: 0.13380558788776398.[0m
[32m[I 2025-01-03 21:13:23,207][0m Trial 16 finished with value: 0.27625590314467746 and parameters: {'observation_period_num': 82, 'train_rates': 0.8746122549600266, 'learning_rate': 8.425816910003778e-05, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9047903661875933}. Best is trial 13 with value: 0.13380558788776398.[0m
[32m[I 2025-01-03 21:14:51,076][0m Trial 17 finished with value: 0.1260441392660141 and parameters: {'observation_period_num': 63, 'train_rates': 0.9899467545845931, 'learning_rate': 0.0003561724091747794, 'batch_size': 124, 'step_size': 8, 'gamma': 0.9610338687254635}. Best is trial 17 with value: 0.1260441392660141.[0m
[32m[I 2025-01-03 21:16:13,788][0m Trial 18 finished with value: 0.41551128244288615 and parameters: {'observation_period_num': 65, 'train_rates': 0.8147668164473392, 'learning_rate': 0.0003447113693354851, 'batch_size': 113, 'step_size': 9, 'gamma': 0.9509768393722632}. Best is trial 17 with value: 0.1260441392660141.[0m
[32m[I 2025-01-03 21:18:18,328][0m Trial 19 finished with value: 0.623849908788721 and parameters: {'observation_period_num': 107, 'train_rates': 0.7486440018048705, 'learning_rate': 0.0004295798272886857, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9083905861791726}. Best is trial 17 with value: 0.1260441392660141.[0m
[32m[I 2025-01-03 21:22:18,002][0m Trial 20 finished with value: 0.16931415131179298 and parameters: {'observation_period_num': 161, 'train_rates': 0.941513965037268, 'learning_rate': 4.2735426735640974e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9298293274848816}. Best is trial 17 with value: 0.1260441392660141.[0m
[32m[I 2025-01-03 21:22:42,474][0m Trial 21 finished with value: 0.09575357288122177 and parameters: {'observation_period_num': 10, 'train_rates': 0.9895935675771657, 'learning_rate': 0.00010963039897106989, 'batch_size': 195, 'step_size': 7, 'gamma': 0.9715215568850393}. Best is trial 21 with value: 0.09575357288122177.[0m
[32m[I 2025-01-03 21:23:56,547][0m Trial 22 finished with value: 0.09238211810588837 and parameters: {'observation_period_num': 56, 'train_rates': 0.9886672729115791, 'learning_rate': 0.00048737209514102485, 'batch_size': 206, 'step_size': 9, 'gamma': 0.9682369561819397}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:25:04,776][0m Trial 23 finished with value: 0.18421991169452667 and parameters: {'observation_period_num': 52, 'train_rates': 0.9298549802869787, 'learning_rate': 0.0005218630042468953, 'batch_size': 223, 'step_size': 10, 'gamma': 0.9686238939471008}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:26:03,573][0m Trial 24 finished with value: 0.9492454073594123 and parameters: {'observation_period_num': 7, 'train_rates': 0.8577170012540437, 'learning_rate': 0.0005633037841136331, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9689583825480804}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:28:24,194][0m Trial 25 finished with value: 0.21346528455615044 and parameters: {'observation_period_num': 109, 'train_rates': 0.898743409169112, 'learning_rate': 0.00015853016112616052, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9665351799391283}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:29:06,548][0m Trial 26 finished with value: 0.17351875296578959 and parameters: {'observation_period_num': 28, 'train_rates': 0.946952001966967, 'learning_rate': 0.00029185014085211824, 'batch_size': 141, 'step_size': 12, 'gamma': 0.9316970351601312}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:30:29,082][0m Trial 27 finished with value: 0.15538448095321655 and parameters: {'observation_period_num': 63, 'train_rates': 0.9818470931782473, 'learning_rate': 0.0008247292590650128, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8808106681789656}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:33:01,909][0m Trial 28 finished with value: 0.10419800132513046 and parameters: {'observation_period_num': 112, 'train_rates': 0.9893552058022055, 'learning_rate': 0.00039296714627476246, 'batch_size': 214, 'step_size': 8, 'gamma': 0.9685399090293227}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:35:30,447][0m Trial 29 finished with value: 0.201636403799057 and parameters: {'observation_period_num': 109, 'train_rates': 0.9494453180283097, 'learning_rate': 0.00020601473227549894, 'batch_size': 223, 'step_size': 4, 'gamma': 0.9444973657163485}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:36:34,031][0m Trial 30 finished with value: 0.1706675380923366 and parameters: {'observation_period_num': 49, 'train_rates': 0.9099327044770678, 'learning_rate': 0.00012372556812564425, 'batch_size': 200, 'step_size': 6, 'gamma': 0.9748152902333084}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:38:50,933][0m Trial 31 finished with value: 0.10757963359355927 and parameters: {'observation_period_num': 97, 'train_rates': 0.9885339491072602, 'learning_rate': 0.0003743430742064601, 'batch_size': 121, 'step_size': 8, 'gamma': 0.9627580799677549}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:41:08,434][0m Trial 32 finished with value: 1.3095943927764893 and parameters: {'observation_period_num': 101, 'train_rates': 0.9565912302911658, 'learning_rate': 1.1056847730306331e-06, 'batch_size': 214, 'step_size': 10, 'gamma': 0.9405789332419079}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:44:55,850][0m Trial 33 finished with value: 0.22252915799617767 and parameters: {'observation_period_num': 155, 'train_rates': 0.9607282389264737, 'learning_rate': 0.0005281595340817489, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9762325279318679}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:47:42,564][0m Trial 34 finished with value: 0.1799914389848709 and parameters: {'observation_period_num': 125, 'train_rates': 0.9249637346403683, 'learning_rate': 0.00024221788724644846, 'batch_size': 256, 'step_size': 10, 'gamma': 0.9563845258386637}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:50:21,695][0m Trial 35 finished with value: 0.3871339746268399 and parameters: {'observation_period_num': 124, 'train_rates': 0.8826513590828072, 'learning_rate': 3.46201273704263e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.78453960741034}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:51:15,380][0m Trial 36 finished with value: 0.3061763346195221 and parameters: {'observation_period_num': 40, 'train_rates': 0.9579065290809583, 'learning_rate': 0.0006398102741121308, 'batch_size': 164, 'step_size': 7, 'gamma': 0.9242009337775372}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:53:10,679][0m Trial 37 finished with value: 0.3160123022048028 and parameters: {'observation_period_num': 93, 'train_rates': 0.8410540236029206, 'learning_rate': 0.00010217897260043054, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9780257616742175}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:54:44,458][0m Trial 38 finished with value: 0.1268538534641266 and parameters: {'observation_period_num': 68, 'train_rates': 0.9880405901436001, 'learning_rate': 0.000372217954214516, 'batch_size': 209, 'step_size': 11, 'gamma': 0.8917863944359279}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 21:58:05,814][0m Trial 39 finished with value: 0.1919552607346425 and parameters: {'observation_period_num': 142, 'train_rates': 0.9195730368256055, 'learning_rate': 0.00022248393135843104, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9572060164294404}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:02:15,088][0m Trial 40 finished with value: 1.0082807540893555 and parameters: {'observation_period_num': 170, 'train_rates': 0.9633144194271308, 'learning_rate': 0.000991408447791785, 'batch_size': 149, 'step_size': 3, 'gamma': 0.9404827313600519}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:03:34,236][0m Trial 41 finished with value: 0.11024533957242966 and parameters: {'observation_period_num': 57, 'train_rates': 0.9896294783637679, 'learning_rate': 0.0004359700254313087, 'batch_size': 121, 'step_size': 8, 'gamma': 0.9597934415105005}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:04:42,095][0m Trial 42 finished with value: 0.31498682270852885 and parameters: {'observation_period_num': 45, 'train_rates': 0.9664258580117233, 'learning_rate': 0.00043766761319353266, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9798666985592394}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:05:20,831][0m Trial 43 finished with value: 0.38343554735183716 and parameters: {'observation_period_num': 22, 'train_rates': 0.9897429432194585, 'learning_rate': 0.0006985689270958931, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9573370169949125}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:07:03,128][0m Trial 44 finished with value: 0.170075868361095 and parameters: {'observation_period_num': 73, 'train_rates': 0.9415284523878044, 'learning_rate': 0.0002671247599122308, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9419654222249738}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:08:01,000][0m Trial 45 finished with value: 0.7193769484758377 and parameters: {'observation_period_num': 54, 'train_rates': 0.6687515674502056, 'learning_rate': 0.00017793841633083015, 'batch_size': 156, 'step_size': 9, 'gamma': 0.750763045558296}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:10:08,812][0m Trial 46 finished with value: 0.6826880574226379 and parameters: {'observation_period_num': 94, 'train_rates': 0.9697159835047421, 'learning_rate': 5.201161782063364e-06, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9158469080734826}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:11:00,483][0m Trial 47 finished with value: 0.13476154754853106 and parameters: {'observation_period_num': 17, 'train_rates': 0.942422127534503, 'learning_rate': 5.647528488111612e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.9829560853817358}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:11:36,420][0m Trial 48 finished with value: 0.46666650657563474 and parameters: {'observation_period_num': 30, 'train_rates': 0.7471112790285747, 'learning_rate': 0.0002928989409221218, 'batch_size': 245, 'step_size': 12, 'gamma': 0.8305000128615742}. Best is trial 22 with value: 0.09238211810588837.[0m
[32m[I 2025-01-03 22:14:12,607][0m Trial 49 finished with value: 0.30043136989735214 and parameters: {'observation_period_num': 120, 'train_rates': 0.9123480782045374, 'learning_rate': 2.4030780851183316e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8643698053685583}. Best is trial 22 with value: 0.09238211810588837.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 22:14:12,614][0m A new study created in memory with name: no-name-6c4ff9e8-8bdb-4495-bc8c-4eb02208984f[0m
[32m[I 2025-01-03 22:15:07,034][0m Trial 0 finished with value: 0.9155033242170978 and parameters: {'observation_period_num': 16, 'train_rates': 0.7327567403107317, 'learning_rate': 1.8381683333403256e-06, 'batch_size': 60, 'step_size': 14, 'gamma': 0.9602703335009171}. Best is trial 0 with value: 0.9155033242170978.[0m
[32m[I 2025-01-03 22:18:35,596][0m Trial 1 finished with value: 0.6936476230621338 and parameters: {'observation_period_num': 143, 'train_rates': 0.9669994232655748, 'learning_rate': 3.5058237830682454e-06, 'batch_size': 179, 'step_size': 10, 'gamma': 0.9653190745313218}. Best is trial 1 with value: 0.6936476230621338.[0m
[32m[I 2025-01-03 22:20:13,994][0m Trial 2 finished with value: 0.5143566499096016 and parameters: {'observation_period_num': 25, 'train_rates': 0.7061826915209672, 'learning_rate': 9.293017467775196e-06, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9837829462048936}. Best is trial 2 with value: 0.5143566499096016.[0m
[32m[I 2025-01-03 22:20:48,935][0m Trial 3 finished with value: 1.4812314994395919 and parameters: {'observation_period_num': 26, 'train_rates': 0.7854890665721622, 'learning_rate': 1.4091975043970893e-06, 'batch_size': 171, 'step_size': 4, 'gamma': 0.8906330681121432}. Best is trial 2 with value: 0.5143566499096016.[0m
[32m[I 2025-01-03 22:21:24,103][0m Trial 4 finished with value: 0.5145650128964508 and parameters: {'observation_period_num': 22, 'train_rates': 0.7079517714792839, 'learning_rate': 4.1096893933758005e-05, 'batch_size': 106, 'step_size': 14, 'gamma': 0.7753351350743972}. Best is trial 2 with value: 0.5143566499096016.[0m
[32m[I 2025-01-03 22:24:23,292][0m Trial 5 finished with value: 0.4015339059394293 and parameters: {'observation_period_num': 137, 'train_rates': 0.8404646734161645, 'learning_rate': 1.836176441698075e-05, 'batch_size': 214, 'step_size': 15, 'gamma': 0.8542154442906139}. Best is trial 5 with value: 0.4015339059394293.[0m
[32m[I 2025-01-03 22:26:50,822][0m Trial 6 finished with value: 0.80442435942687 and parameters: {'observation_period_num': 113, 'train_rates': 0.7832162735317847, 'learning_rate': 3.0324035927175033e-06, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8636351345599432}. Best is trial 5 with value: 0.4015339059394293.[0m
Early stopping at epoch 78
[32m[I 2025-01-03 22:27:39,100][0m Trial 7 finished with value: 0.5399982183871854 and parameters: {'observation_period_num': 48, 'train_rates': 0.8288244104508473, 'learning_rate': 9.016994221790214e-05, 'batch_size': 159, 'step_size': 1, 'gamma': 0.8501225058841027}. Best is trial 5 with value: 0.4015339059394293.[0m
[32m[I 2025-01-03 22:31:56,018][0m Trial 8 finished with value: 1.3681177826436242 and parameters: {'observation_period_num': 213, 'train_rates': 0.6096976222135432, 'learning_rate': 4.1593832645617215e-06, 'batch_size': 34, 'step_size': 3, 'gamma': 0.7842530355263875}. Best is trial 5 with value: 0.4015339059394293.[0m
[32m[I 2025-01-03 22:35:22,140][0m Trial 9 finished with value: 0.9640273723868128 and parameters: {'observation_period_num': 148, 'train_rates': 0.897828918354042, 'learning_rate': 1.5493401893070325e-05, 'batch_size': 254, 'step_size': 2, 'gamma': 0.8673755533978115}. Best is trial 5 with value: 0.4015339059394293.[0m
[32m[I 2025-01-03 22:41:13,678][0m Trial 10 finished with value: 0.29119367702253934 and parameters: {'observation_period_num': 237, 'train_rates': 0.8933084278492388, 'learning_rate': 0.0009231466755970458, 'batch_size': 239, 'step_size': 15, 'gamma': 0.8158144277168302}. Best is trial 10 with value: 0.29119367702253934.[0m
[32m[I 2025-01-03 22:47:28,616][0m Trial 11 finished with value: 0.3084012243732716 and parameters: {'observation_period_num': 252, 'train_rates': 0.8823619560271079, 'learning_rate': 0.000988059027153097, 'batch_size': 247, 'step_size': 15, 'gamma': 0.8171582503511948}. Best is trial 10 with value: 0.29119367702253934.[0m
[32m[I 2025-01-03 22:53:55,201][0m Trial 12 finished with value: 0.32400667667388916 and parameters: {'observation_period_num': 250, 'train_rates': 0.9304464926196929, 'learning_rate': 0.0008976382783498077, 'batch_size': 245, 'step_size': 12, 'gamma': 0.8169207264051771}. Best is trial 10 with value: 0.29119367702253934.[0m
[32m[I 2025-01-03 22:59:02,098][0m Trial 13 finished with value: 0.41456540199361236 and parameters: {'observation_period_num': 209, 'train_rates': 0.8978735320084135, 'learning_rate': 0.0009195736372823904, 'batch_size': 212, 'step_size': 12, 'gamma': 0.802117375784266}. Best is trial 10 with value: 0.29119367702253934.[0m
[32m[I 2025-01-03 23:05:44,033][0m Trial 14 finished with value: 0.1308739334344864 and parameters: {'observation_period_num': 252, 'train_rates': 0.9766165806953286, 'learning_rate': 0.00023095169277605808, 'batch_size': 216, 'step_size': 6, 'gamma': 0.7521865339662537}. Best is trial 14 with value: 0.1308739334344864.[0m
[32m[I 2025-01-03 23:10:54,174][0m Trial 15 finished with value: 0.1294008493423462 and parameters: {'observation_period_num': 199, 'train_rates': 0.9871142445055634, 'learning_rate': 0.00024646129678558164, 'batch_size': 118, 'step_size': 6, 'gamma': 0.7565417875819191}. Best is trial 15 with value: 0.1294008493423462.[0m
[32m[I 2025-01-03 23:15:27,773][0m Trial 16 finished with value: 0.11998002976179123 and parameters: {'observation_period_num': 182, 'train_rates': 0.9894902483495127, 'learning_rate': 0.00019410400214123986, 'batch_size': 110, 'step_size': 6, 'gamma': 0.7533059759420568}. Best is trial 16 with value: 0.11998002976179123.[0m
[32m[I 2025-01-03 23:19:54,607][0m Trial 17 finished with value: 0.09987547248601913 and parameters: {'observation_period_num': 177, 'train_rates': 0.9791353367032793, 'learning_rate': 0.0002711482880540439, 'batch_size': 104, 'step_size': 5, 'gamma': 0.9132415869593953}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:24:08,992][0m Trial 18 finished with value: 0.17323177651717112 and parameters: {'observation_period_num': 176, 'train_rates': 0.9437957772250407, 'learning_rate': 0.0002635838040930937, 'batch_size': 81, 'step_size': 5, 'gamma': 0.91570185043892}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:25:49,346][0m Trial 19 finished with value: 0.7941147013562372 and parameters: {'observation_period_num': 99, 'train_rates': 0.6096267581473558, 'learning_rate': 9.617292646786648e-05, 'batch_size': 141, 'step_size': 8, 'gamma': 0.9015335856215808}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:30:08,021][0m Trial 20 finished with value: 0.16770648449659348 and parameters: {'observation_period_num': 179, 'train_rates': 0.9281086455476171, 'learning_rate': 8.867415709886606e-05, 'batch_size': 94, 'step_size': 9, 'gamma': 0.9385839044595964}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:34:28,681][0m Trial 21 finished with value: 0.13780997693538666 and parameters: {'observation_period_num': 176, 'train_rates': 0.9856765613713693, 'learning_rate': 0.0003134542114793392, 'batch_size': 115, 'step_size': 5, 'gamma': 0.7525621150702961}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:39:40,942][0m Trial 22 finished with value: 0.20528677298176673 and parameters: {'observation_period_num': 208, 'train_rates': 0.9549716495936639, 'learning_rate': 0.0004201881806888351, 'batch_size': 122, 'step_size': 7, 'gamma': 0.784897518574974}. Best is trial 17 with value: 0.09987547248601913.[0m
[32m[I 2025-01-03 23:44:30,368][0m Trial 23 finished with value: 0.09088844060897827 and parameters: {'observation_period_num': 189, 'train_rates': 0.9896510651521607, 'learning_rate': 0.00016475626674858812, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9283936002496453}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-03 23:48:15,119][0m Trial 24 finished with value: 0.25032247268744695 and parameters: {'observation_period_num': 164, 'train_rates': 0.8642224783766345, 'learning_rate': 0.00015302378095275438, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9204000078725262}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-03 23:50:15,562][0m Trial 25 finished with value: 0.20420005708788905 and parameters: {'observation_period_num': 86, 'train_rates': 0.9279640828087121, 'learning_rate': 3.735047533393486e-05, 'batch_size': 67, 'step_size': 4, 'gamma': 0.8879908020885053}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-03 23:54:54,702][0m Trial 26 finished with value: 0.2485891431570053 and parameters: {'observation_period_num': 192, 'train_rates': 0.9530944695952275, 'learning_rate': 0.000510463534391995, 'batch_size': 141, 'step_size': 1, 'gamma': 0.9371704256815686}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:00:54,205][0m Trial 27 finished with value: 0.0946788489818573 and parameters: {'observation_period_num': 226, 'train_rates': 0.9898634299865697, 'learning_rate': 0.00015071461998397687, 'batch_size': 92, 'step_size': 5, 'gamma': 0.9369799310687686}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:07:21,390][0m Trial 28 finished with value: 0.14151974163086117 and parameters: {'observation_period_num': 230, 'train_rates': 0.9145570471852797, 'learning_rate': 0.00013446199057230666, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9436752662618217}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:12:48,702][0m Trial 29 finished with value: 0.25867500517347136 and parameters: {'observation_period_num': 225, 'train_rates': 0.8557885940842643, 'learning_rate': 5.441976186777563e-05, 'batch_size': 57, 'step_size': 4, 'gamma': 0.9615312528744083}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:16:10,374][0m Trial 30 finished with value: 0.5470008173348595 and parameters: {'observation_period_num': 155, 'train_rates': 0.8151764003097511, 'learning_rate': 0.00050938625638479, 'batch_size': 92, 'step_size': 5, 'gamma': 0.9883638949955409}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:21:01,730][0m Trial 31 finished with value: 0.09251926839351654 and parameters: {'observation_period_num': 188, 'train_rates': 0.9887908448009393, 'learning_rate': 0.00015271088623746524, 'batch_size': 58, 'step_size': 6, 'gamma': 0.9210770635470391}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:23:59,619][0m Trial 32 finished with value: 0.16844875802960965 and parameters: {'observation_period_num': 124, 'train_rates': 0.961601728677, 'learning_rate': 6.486889759475478e-05, 'batch_size': 54, 'step_size': 7, 'gamma': 0.9143029119047102}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:28:59,132][0m Trial 33 finished with value: 0.1672966226492778 and parameters: {'observation_period_num': 194, 'train_rates': 0.9634021793681207, 'learning_rate': 0.00011269495339462935, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9520229505514664}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:33:59,187][0m Trial 34 finished with value: 0.6331015511345096 and parameters: {'observation_period_num': 222, 'train_rates': 0.7496636591046587, 'learning_rate': 2.2585773642891057e-05, 'batch_size': 47, 'step_size': 5, 'gamma': 0.9300253904013407}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:38:00,086][0m Trial 35 finished with value: 0.18410507851923016 and parameters: {'observation_period_num': 162, 'train_rates': 0.9491722814482607, 'learning_rate': 0.0003825442746507316, 'batch_size': 75, 'step_size': 2, 'gamma': 0.9707330695827627}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:42:25,540][0m Trial 36 finished with value: 0.7568469453799961 and parameters: {'observation_period_num': 194, 'train_rates': 0.6706912769521644, 'learning_rate': 0.00017156205240906352, 'batch_size': 19, 'step_size': 4, 'gamma': 0.8963257295572236}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:45:34,711][0m Trial 37 finished with value: 0.19545958936214447 and parameters: {'observation_period_num': 130, 'train_rates': 0.9697383646924694, 'learning_rate': 4.631740939450079e-05, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8817447949208146}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:49:34,370][0m Trial 38 finished with value: 0.26041622171478884 and parameters: {'observation_period_num': 169, 'train_rates': 0.9108934781393179, 'learning_rate': 1.0608719923080452e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9729910889473576}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 00:55:07,696][0m Trial 39 finished with value: 0.17012996477789658 and parameters: {'observation_period_num': 216, 'train_rates': 0.9373381000574289, 'learning_rate': 6.851155938561428e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9026345065069247}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:01:31,792][0m Trial 40 finished with value: 0.14415833353996277 and parameters: {'observation_period_num': 238, 'train_rates': 0.9892019458913178, 'learning_rate': 0.0005964277233198988, 'batch_size': 86, 'step_size': 2, 'gamma': 0.927520987194371}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:06:02,500][0m Trial 41 finished with value: 0.1604154109954834 and parameters: {'observation_period_num': 184, 'train_rates': 0.971660120090662, 'learning_rate': 0.00018967305608864326, 'batch_size': 108, 'step_size': 6, 'gamma': 0.9509024635939389}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:09:44,309][0m Trial 42 finished with value: 0.10893365740776062 and parameters: {'observation_period_num': 150, 'train_rates': 0.9889279383789531, 'learning_rate': 0.00017678723020998484, 'batch_size': 157, 'step_size': 7, 'gamma': 0.843135476503089}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:13:14,444][0m Trial 43 finished with value: 0.17999394237995148 and parameters: {'observation_period_num': 145, 'train_rates': 0.9663513891211181, 'learning_rate': 0.0001329687427704853, 'batch_size': 187, 'step_size': 7, 'gamma': 0.84308164539408}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:18:15,890][0m Trial 44 finished with value: 0.2776606954255347 and parameters: {'observation_period_num': 205, 'train_rates': 0.9142025955811875, 'learning_rate': 2.633319103537158e-05, 'batch_size': 173, 'step_size': 9, 'gamma': 0.8325007077201187}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:21:55,594][0m Trial 45 finished with value: 0.21600485059522814 and parameters: {'observation_period_num': 153, 'train_rates': 0.9447811737316388, 'learning_rate': 0.00032257256579852205, 'batch_size': 131, 'step_size': 3, 'gamma': 0.8762511557548383}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:23:32,329][0m Trial 46 finished with value: 0.16643203794956207 and parameters: {'observation_period_num': 69, 'train_rates': 0.9723063003068362, 'learning_rate': 8.323602936271678e-05, 'batch_size': 157, 'step_size': 5, 'gamma': 0.9085403288862682}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:26:32,930][0m Trial 47 finished with value: 0.2564045934982243 and parameters: {'observation_period_num': 131, 'train_rates': 0.8808700494722903, 'learning_rate': 0.0005926396139640587, 'batch_size': 155, 'step_size': 4, 'gamma': 0.8557241512058386}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:28:33,058][0m Trial 48 finished with value: 1.4107974648475647 and parameters: {'observation_period_num': 113, 'train_rates': 0.6599936260335166, 'learning_rate': 2.1372031535073134e-06, 'batch_size': 193, 'step_size': 7, 'gamma': 0.9526701396203104}. Best is trial 23 with value: 0.09088844060897827.[0m
[32m[I 2025-01-04 01:32:44,554][0m Trial 49 finished with value: 0.3510258155948356 and parameters: {'observation_period_num': 188, 'train_rates': 0.7706031629746813, 'learning_rate': 0.00012592516839077638, 'batch_size': 43, 'step_size': 4, 'gamma': 0.9313156707415162}. Best is trial 23 with value: 0.09088844060897827.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 01:32:44,562][0m A new study created in memory with name: no-name-4ae6716b-eef7-4767-b7ca-cb8e966d17bb[0m
[32m[I 2025-01-04 01:33:16,352][0m Trial 0 finished with value: 1.3443878608131181 and parameters: {'observation_period_num': 17, 'train_rates': 0.7145366874629953, 'learning_rate': 2.522854911297645e-06, 'batch_size': 125, 'step_size': 6, 'gamma': 0.828666502141382}. Best is trial 0 with value: 1.3443878608131181.[0m
[32m[I 2025-01-04 01:34:45,204][0m Trial 1 finished with value: 0.6179178490043522 and parameters: {'observation_period_num': 73, 'train_rates': 0.7525629015130462, 'learning_rate': 1.947144658452532e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.7679502404164424}. Best is trial 1 with value: 0.6179178490043522.[0m
[32m[I 2025-01-04 01:37:31,802][0m Trial 2 finished with value: 0.37007541882414974 and parameters: {'observation_period_num': 129, 'train_rates': 0.8243700646183734, 'learning_rate': 3.645841563184851e-05, 'batch_size': 156, 'step_size': 14, 'gamma': 0.8453708831679769}. Best is trial 2 with value: 0.37007541882414974.[0m
[32m[I 2025-01-04 01:38:06,896][0m Trial 3 finished with value: 0.1624944806098938 and parameters: {'observation_period_num': 17, 'train_rates': 0.9574424342681007, 'learning_rate': 0.00025947040285086766, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9440658912288692}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:41:59,459][0m Trial 4 finished with value: 1.1810480614578769 and parameters: {'observation_period_num': 191, 'train_rates': 0.7144910249160505, 'learning_rate': 3.1897929899770663e-06, 'batch_size': 84, 'step_size': 6, 'gamma': 0.9360999877611038}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:47:11,918][0m Trial 5 finished with value: 0.3561462401393875 and parameters: {'observation_period_num': 226, 'train_rates': 0.815991019134957, 'learning_rate': 0.00020269020770544486, 'batch_size': 172, 'step_size': 8, 'gamma': 0.9564649783199362}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:49:08,954][0m Trial 6 finished with value: 0.7768461316627028 and parameters: {'observation_period_num': 106, 'train_rates': 0.6810134092581444, 'learning_rate': 0.00014112467537540958, 'batch_size': 138, 'step_size': 2, 'gamma': 0.765506100956899}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:52:35,521][0m Trial 7 finished with value: 0.38171997178129774 and parameters: {'observation_period_num': 163, 'train_rates': 0.7899884722046662, 'learning_rate': 0.00019729753608238782, 'batch_size': 156, 'step_size': 3, 'gamma': 0.941658405223128}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:53:03,475][0m Trial 8 finished with value: 0.6874240370860613 and parameters: {'observation_period_num': 22, 'train_rates': 0.7431822770115519, 'learning_rate': 3.31917038522346e-05, 'batch_size': 246, 'step_size': 10, 'gamma': 0.7930939012185504}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:53:33,032][0m Trial 9 finished with value: 0.7051866004517028 and parameters: {'observation_period_num': 23, 'train_rates': 0.658166372811801, 'learning_rate': 9.895906476180516e-05, 'batch_size': 175, 'step_size': 5, 'gamma': 0.9175143344078173}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 01:56:08,099][0m Trial 10 finished with value: 2.2310486833254495 and parameters: {'observation_period_num': 73, 'train_rates': 0.9834638395169985, 'learning_rate': 0.000979056582090731, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8893714034725254}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:01:58,047][0m Trial 11 finished with value: 0.2153752194524306 and parameters: {'observation_period_num': 233, 'train_rates': 0.9115526407467585, 'learning_rate': 0.0006995955415519406, 'batch_size': 213, 'step_size': 9, 'gamma': 0.9884708640470704}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:08:20,489][0m Trial 12 finished with value: 0.9478660821914673 and parameters: {'observation_period_num': 244, 'train_rates': 0.9523596040487405, 'learning_rate': 0.0008698986465257037, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9878912743300224}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:13:12,001][0m Trial 13 finished with value: 0.20853017206243474 and parameters: {'observation_period_num': 199, 'train_rates': 0.8993165111003665, 'learning_rate': 0.0005599294842877807, 'batch_size': 207, 'step_size': 9, 'gamma': 0.9664216093413096}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:17:24,908][0m Trial 14 finished with value: 0.2266661087864811 and parameters: {'observation_period_num': 182, 'train_rates': 0.8884562082863063, 'learning_rate': 0.0004801927462421619, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8974668876293513}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:20:38,429][0m Trial 15 finished with value: 0.4879193365573883 and parameters: {'observation_period_num': 139, 'train_rates': 0.87588730002169, 'learning_rate': 0.00034958298331369796, 'batch_size': 70, 'step_size': 8, 'gamma': 0.969010912464197}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:21:57,013][0m Trial 16 finished with value: 0.17653056979179382 and parameters: {'observation_period_num': 61, 'train_rates': 0.9396517883687022, 'learning_rate': 6.877437434049064e-05, 'batch_size': 191, 'step_size': 15, 'gamma': 0.9185981769997729}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:22:53,416][0m Trial 17 finished with value: 1.1377951913738844 and parameters: {'observation_period_num': 57, 'train_rates': 0.6108069370841915, 'learning_rate': 9.55782579234494e-06, 'batch_size': 182, 'step_size': 15, 'gamma': 0.8771287693690469}. Best is trial 3 with value: 0.1624944806098938.[0m
[32m[I 2025-01-04 02:24:08,816][0m Trial 18 finished with value: 0.09885379672050476 and parameters: {'observation_period_num': 54, 'train_rates': 0.9892786617270337, 'learning_rate': 7.694130268258843e-05, 'batch_size': 113, 'step_size': 13, 'gamma': 0.9153238991569628}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:25:22,700][0m Trial 19 finished with value: 0.20025061070919037 and parameters: {'observation_period_num': 50, 'train_rates': 0.9884757347263682, 'learning_rate': 9.29036108702791e-06, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8596691103672909}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:27:44,379][0m Trial 20 finished with value: 0.3154735108946031 and parameters: {'observation_period_num': 101, 'train_rates': 0.8495938140902193, 'learning_rate': 5.42864079764129e-05, 'batch_size': 41, 'step_size': 13, 'gamma': 0.913238457977267}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:28:21,253][0m Trial 21 finished with value: 0.1358840570319444 and parameters: {'observation_period_num': 7, 'train_rates': 0.9349127478278999, 'learning_rate': 7.200241758310216e-05, 'batch_size': 111, 'step_size': 15, 'gamma': 0.9237605202409773}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:29:11,286][0m Trial 22 finished with value: 0.18282656647906398 and parameters: {'observation_period_num': 33, 'train_rates': 0.9483294009348913, 'learning_rate': 0.00026646506309575563, 'batch_size': 109, 'step_size': 13, 'gamma': 0.9360435497575437}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:29:43,023][0m Trial 23 finished with value: 0.13876840050774392 and parameters: {'observation_period_num': 10, 'train_rates': 0.9329134581526448, 'learning_rate': 9.978923954833592e-05, 'batch_size': 131, 'step_size': 14, 'gamma': 0.8978657845542376}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:30:43,232][0m Trial 24 finished with value: 0.21286272233532322 and parameters: {'observation_period_num': 43, 'train_rates': 0.9148330434645833, 'learning_rate': 1.550553503955698e-05, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8976449677689349}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:32:51,041][0m Trial 25 finished with value: 0.2840822325308721 and parameters: {'observation_period_num': 92, 'train_rates': 0.8599217742351495, 'learning_rate': 9.878179063450711e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.8682956749604634}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:33:28,087][0m Trial 26 finished with value: 0.15163427025079726 and parameters: {'observation_period_num': 5, 'train_rates': 0.9321129814767344, 'learning_rate': 5.94478932114542e-05, 'batch_size': 124, 'step_size': 12, 'gamma': 0.8327694444519992}. Best is trial 18 with value: 0.09885379672050476.[0m
[32m[I 2025-01-04 02:34:38,205][0m Trial 27 finished with value: 0.06448657810688019 and parameters: {'observation_period_num': 7, 'train_rates': 0.9797159905331455, 'learning_rate': 0.00012131007164058054, 'batch_size': 65, 'step_size': 15, 'gamma': 0.9087806753692035}. Best is trial 27 with value: 0.06448657810688019.[0m
[32m[I 2025-01-04 02:35:50,407][0m Trial 28 finished with value: 0.10487128049135208 and parameters: {'observation_period_num': 38, 'train_rates': 0.9896486638938214, 'learning_rate': 2.036172823768797e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.9185062359722918}. Best is trial 27 with value: 0.06448657810688019.[0m
[32m[I 2025-01-04 02:36:58,613][0m Trial 29 finished with value: 0.402123941921852 and parameters: {'observation_period_num': 35, 'train_rates': 0.975781462877992, 'learning_rate': 3.7123548915845796e-06, 'batch_size': 65, 'step_size': 13, 'gamma': 0.8097813788051145}. Best is trial 27 with value: 0.06448657810688019.[0m
[32m[I 2025-01-04 02:39:03,423][0m Trial 30 finished with value: 0.5752257140907081 and parameters: {'observation_period_num': 86, 'train_rates': 0.9744325977167871, 'learning_rate': 1.5218015440512301e-06, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8790757117117868}. Best is trial 27 with value: 0.06448657810688019.[0m
[32m[I 2025-01-04 02:39:54,516][0m Trial 31 finished with value: 0.24474320216339177 and parameters: {'observation_period_num': 32, 'train_rates': 0.9592532860314749, 'learning_rate': 1.5471427101373885e-05, 'batch_size': 89, 'step_size': 15, 'gamma': 0.914071009176726}. Best is trial 27 with value: 0.06448657810688019.[0m
[32m[I 2025-01-04 02:43:14,376][0m Trial 32 finished with value: 0.05879228007124395 and parameters: {'observation_period_num': 6, 'train_rates': 0.9835394424623525, 'learning_rate': 2.0536466288316713e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9264800637713873}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:46:33,231][0m Trial 33 finished with value: 0.11218235705947054 and parameters: {'observation_period_num': 67, 'train_rates': 0.9899491256763462, 'learning_rate': 2.4241417720993226e-05, 'batch_size': 22, 'step_size': 14, 'gamma': 0.9036764163538127}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:48:40,061][0m Trial 34 finished with value: 0.15915946709482293 and parameters: {'observation_period_num': 46, 'train_rates': 0.9674239225672844, 'learning_rate': 3.995221826238888e-05, 'batch_size': 36, 'step_size': 15, 'gamma': 0.9518147019931312}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:49:57,652][0m Trial 35 finished with value: 0.28384388667695665 and parameters: {'observation_period_num': 22, 'train_rates': 0.9130859963413492, 'learning_rate': 6.110179731492291e-06, 'batch_size': 57, 'step_size': 14, 'gamma': 0.9306294755694262}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:51:49,546][0m Trial 36 finished with value: 0.22050334763999033 and parameters: {'observation_period_num': 79, 'train_rates': 0.9648706568431643, 'learning_rate': 2.5313242005640072e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.8528132501464871}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:54:14,414][0m Trial 37 finished with value: 0.4268296410297525 and parameters: {'observation_period_num': 111, 'train_rates': 0.7959490760418113, 'learning_rate': 1.2317495809184513e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8823215319080102}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 02:58:14,688][0m Trial 38 finished with value: 0.07015173733234406 and parameters: {'observation_period_num': 34, 'train_rates': 0.9899518796857367, 'learning_rate': 0.00015423883685636378, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9593371922688998}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:02:12,083][0m Trial 39 finished with value: 0.11836600931067216 and parameters: {'observation_period_num': 23, 'train_rates': 0.9224416477475512, 'learning_rate': 0.00016049553746012992, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9694135307075336}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:04:59,096][0m Trial 40 finished with value: 0.3170732640502995 and parameters: {'observation_period_num': 120, 'train_rates': 0.8400006648991963, 'learning_rate': 4.2776705170003545e-05, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9508997166668592}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:06:26,679][0m Trial 41 finished with value: 0.1710038974658767 and parameters: {'observation_period_num': 46, 'train_rates': 0.9539435240405787, 'learning_rate': 0.00012738597723714417, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9301447417850992}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:08:41,430][0m Trial 42 finished with value: 0.06662555194149415 and parameters: {'observation_period_num': 17, 'train_rates': 0.9877403918472998, 'learning_rate': 2.3258424029799944e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9423262231365988}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:11:01,475][0m Trial 43 finished with value: 0.17516495388443187 and parameters: {'observation_period_num': 19, 'train_rates': 0.95983434686576, 'learning_rate': 2.9214816150208842e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.7515082260571755}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:13:48,351][0m Trial 44 finished with value: 0.1633168081394755 and parameters: {'observation_period_num': 6, 'train_rates': 0.8943279926670566, 'learning_rate': 4.724216623139245e-05, 'batch_size': 23, 'step_size': 14, 'gamma': 0.9588959226520372}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:15:18,124][0m Trial 45 finished with value: 0.15203591883182527 and parameters: {'observation_period_num': 55, 'train_rates': 0.9724324651570735, 'learning_rate': 0.00021386305965768367, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9444821814890609}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:19:33,183][0m Trial 46 finished with value: 0.11458521813154221 and parameters: {'observation_period_num': 28, 'train_rates': 0.9489112408187813, 'learning_rate': 9.388390267633918e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9771657449932153}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:21:11,392][0m Trial 47 finished with value: 1.0148581592752361 and parameters: {'observation_period_num': 14, 'train_rates': 0.7047023197928494, 'learning_rate': 0.0003457551170176983, 'batch_size': 37, 'step_size': 15, 'gamma': 0.9407314618097739}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:24:07,775][0m Trial 48 finished with value: 0.5285247516339342 and parameters: {'observation_period_num': 146, 'train_rates': 0.7389283464408365, 'learning_rate': 0.0001488975623014031, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9061425173985591}. Best is trial 32 with value: 0.05879228007124395.[0m
[32m[I 2025-01-04 03:25:35,944][0m Trial 49 finished with value: 0.42885766037598183 and parameters: {'observation_period_num': 68, 'train_rates': 0.7744362777924809, 'learning_rate': 3.315511396931363e-05, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9594466756208605}. Best is trial 32 with value: 0.05879228007124395.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 03:25:35,952][0m A new study created in memory with name: no-name-f981a8d9-18cf-4e7b-8bc0-6923eb921700[0m
[32m[I 2025-01-04 03:31:10,900][0m Trial 0 finished with value: 2.3233324394506565 and parameters: {'observation_period_num': 213, 'train_rates': 0.950432583392921, 'learning_rate': 0.0009325609472582728, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9146871147657332}. Best is trial 0 with value: 2.3233324394506565.[0m
[32m[I 2025-01-04 03:32:24,922][0m Trial 1 finished with value: 0.7168523087793467 and parameters: {'observation_period_num': 63, 'train_rates': 0.8303893605729491, 'learning_rate': 1.0005099621446785e-05, 'batch_size': 232, 'step_size': 9, 'gamma': 0.8810156843447008}. Best is trial 1 with value: 0.7168523087793467.[0m
[32m[I 2025-01-04 03:35:14,198][0m Trial 2 finished with value: 0.3874362887255477 and parameters: {'observation_period_num': 129, 'train_rates': 0.8220207313456948, 'learning_rate': 0.0001554260786634126, 'batch_size': 234, 'step_size': 4, 'gamma': 0.7670010359725863}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:38:28,439][0m Trial 3 finished with value: 0.486487130964956 and parameters: {'observation_period_num': 143, 'train_rates': 0.8677399085763581, 'learning_rate': 1.2505809481256895e-05, 'batch_size': 150, 'step_size': 2, 'gamma': 0.9614666318019685}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:40:30,990][0m Trial 4 finished with value: 0.4191894658974239 and parameters: {'observation_period_num': 91, 'train_rates': 0.8900923839472032, 'learning_rate': 3.923721758875386e-06, 'batch_size': 79, 'step_size': 5, 'gamma': 0.985085082781449}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:44:17,318][0m Trial 5 finished with value: 0.6894603045123398 and parameters: {'observation_period_num': 169, 'train_rates': 0.8308734024537976, 'learning_rate': 7.76082743073248e-06, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9472875002372909}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:48:22,290][0m Trial 6 finished with value: 0.9405310623556654 and parameters: {'observation_period_num': 197, 'train_rates': 0.6117555711430352, 'learning_rate': 6.06665294312158e-05, 'batch_size': 22, 'step_size': 7, 'gamma': 0.765435026105365}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:49:50,283][0m Trial 7 finished with value: 0.7732460421670747 and parameters: {'observation_period_num': 73, 'train_rates': 0.6660061688881012, 'learning_rate': 4.2244612833102675e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9321237305722692}. Best is trial 2 with value: 0.3874362887255477.[0m
[32m[I 2025-01-04 03:54:53,765][0m Trial 8 finished with value: 0.3497903388811321 and parameters: {'observation_period_num': 212, 'train_rates': 0.7907269627369954, 'learning_rate': 7.195804046601809e-05, 'batch_size': 27, 'step_size': 7, 'gamma': 0.8850598024050487}. Best is trial 8 with value: 0.3497903388811321.[0m
[32m[I 2025-01-04 03:58:42,516][0m Trial 9 finished with value: 0.415917096728528 and parameters: {'observation_period_num': 151, 'train_rates': 0.884341030514241, 'learning_rate': 2.12131763100872e-06, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9799367718968721}. Best is trial 8 with value: 0.3497903388811321.[0m
[32m[I 2025-01-04 04:03:49,373][0m Trial 10 finished with value: 1.2147971413135528 and parameters: {'observation_period_num': 240, 'train_rates': 0.7233696667046982, 'learning_rate': 0.000666396633594041, 'batch_size': 106, 'step_size': 15, 'gamma': 0.8221146674596806}. Best is trial 8 with value: 0.3497903388811321.[0m
[32m[I 2025-01-04 04:06:00,716][0m Trial 11 finished with value: 0.6189565310451041 and parameters: {'observation_period_num': 115, 'train_rates': 0.752499558069785, 'learning_rate': 0.00013595200913139245, 'batch_size': 248, 'step_size': 3, 'gamma': 0.8253033388475154}. Best is trial 8 with value: 0.3497903388811321.[0m
[32m[I 2025-01-04 04:06:41,002][0m Trial 12 finished with value: 0.47464546424950044 and parameters: {'observation_period_num': 33, 'train_rates': 0.7752951246913368, 'learning_rate': 0.0002105565875669023, 'batch_size': 191, 'step_size': 5, 'gamma': 0.7552478775076625}. Best is trial 8 with value: 0.3497903388811321.[0m
[32m[I 2025-01-04 04:13:09,473][0m Trial 13 finished with value: 0.11981520801782608 and parameters: {'observation_period_num': 244, 'train_rates': 0.9760521701647832, 'learning_rate': 0.00021436012292981558, 'batch_size': 198, 'step_size': 5, 'gamma': 0.8435317603754994}. Best is trial 13 with value: 0.11981520801782608.[0m
Early stopping at epoch 84
[32m[I 2025-01-04 04:18:41,130][0m Trial 14 finished with value: 0.3211199939250946 and parameters: {'observation_period_num': 247, 'train_rates': 0.96463284764044, 'learning_rate': 0.0002965609379439856, 'batch_size': 199, 'step_size': 1, 'gamma': 0.8612806227570717}. Best is trial 13 with value: 0.11981520801782608.[0m
Early stopping at epoch 75
[32m[I 2025-01-04 04:23:32,700][0m Trial 15 finished with value: 0.3130980432033539 and parameters: {'observation_period_num': 241, 'train_rates': 0.9722633512728535, 'learning_rate': 0.0003645789178200323, 'batch_size': 197, 'step_size': 1, 'gamma': 0.8356346081431351}. Best is trial 13 with value: 0.11981520801782608.[0m
Early stopping at epoch 67
[32m[I 2025-01-04 04:26:22,926][0m Trial 16 finished with value: 0.37432029843330383 and parameters: {'observation_period_num': 175, 'train_rates': 0.9322939679335643, 'learning_rate': 0.0003571023995942894, 'batch_size': 191, 'step_size': 1, 'gamma': 0.8168303813221977}. Best is trial 13 with value: 0.11981520801782608.[0m
[32m[I 2025-01-04 04:33:03,477][0m Trial 17 finished with value: 0.27324944734573364 and parameters: {'observation_period_num': 249, 'train_rates': 0.9846430436345752, 'learning_rate': 2.427164548338369e-05, 'batch_size': 120, 'step_size': 3, 'gamma': 0.8455175043573858}. Best is trial 13 with value: 0.11981520801782608.[0m
[32m[I 2025-01-04 04:33:37,515][0m Trial 18 finished with value: 0.3487234636198116 and parameters: {'observation_period_num': 5, 'train_rates': 0.9196772479456399, 'learning_rate': 1.8273918453970023e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.7930781268890748}. Best is trial 13 with value: 0.11981520801782608.[0m
[32m[I 2025-01-04 04:38:45,264][0m Trial 19 finished with value: 0.2933235168457031 and parameters: {'observation_period_num': 196, 'train_rates': 0.9847726878931103, 'learning_rate': 2.5856069184059988e-05, 'batch_size': 101, 'step_size': 3, 'gamma': 0.8595417708128863}. Best is trial 13 with value: 0.11981520801782608.[0m
[32m[I 2025-01-04 04:44:11,114][0m Trial 20 finished with value: 0.16980648118516672 and parameters: {'observation_period_num': 219, 'train_rates': 0.9159103324123076, 'learning_rate': 8.59021861991612e-05, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9064170475965122}. Best is trial 13 with value: 0.11981520801782608.[0m
[32m[I 2025-01-04 04:50:05,118][0m Trial 21 finished with value: 0.09655880182981491 and parameters: {'observation_period_num': 222, 'train_rates': 0.9879092975483533, 'learning_rate': 0.00011228306264914298, 'batch_size': 140, 'step_size': 11, 'gamma': 0.902367328146199}. Best is trial 21 with value: 0.09655880182981491.[0m
[32m[I 2025-01-04 04:55:28,324][0m Trial 22 finished with value: 0.15691846116967156 and parameters: {'observation_period_num': 216, 'train_rates': 0.9204218215390825, 'learning_rate': 0.00010353451315878517, 'batch_size': 158, 'step_size': 12, 'gamma': 0.9014770110816948}. Best is trial 21 with value: 0.09655880182981491.[0m
[32m[I 2025-01-04 04:59:57,089][0m Trial 23 finished with value: 0.18588881194591522 and parameters: {'observation_period_num': 184, 'train_rates': 0.9462990942699001, 'learning_rate': 9.669252536160413e-05, 'batch_size': 166, 'step_size': 12, 'gamma': 0.8939571498032299}. Best is trial 21 with value: 0.09655880182981491.[0m
[32m[I 2025-01-04 05:05:19,862][0m Trial 24 finished with value: 0.2122610984710011 and parameters: {'observation_period_num': 220, 'train_rates': 0.8998001644042172, 'learning_rate': 4.301592898886411e-05, 'batch_size': 162, 'step_size': 14, 'gamma': 0.9235468998847396}. Best is trial 21 with value: 0.09655880182981491.[0m
[32m[I 2025-01-04 05:10:51,407][0m Trial 25 finished with value: 0.23668823012000764 and parameters: {'observation_period_num': 232, 'train_rates': 0.8563584896805331, 'learning_rate': 0.0005968144198454106, 'batch_size': 217, 'step_size': 12, 'gamma': 0.8706022811416914}. Best is trial 21 with value: 0.09655880182981491.[0m
[32m[I 2025-01-04 05:15:53,808][0m Trial 26 finished with value: 0.0956888496875763 and parameters: {'observation_period_num': 194, 'train_rates': 0.9887844577819261, 'learning_rate': 0.00017196722218563954, 'batch_size': 178, 'step_size': 10, 'gamma': 0.9020895781537156}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:20:49,165][0m Trial 27 finished with value: 0.18835552036762238 and parameters: {'observation_period_num': 197, 'train_rates': 0.9493584744158043, 'learning_rate': 0.00020691210170743738, 'batch_size': 182, 'step_size': 10, 'gamma': 0.7938550388850157}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:24:48,872][0m Trial 28 finished with value: 0.10753077268600464 and parameters: {'observation_period_num': 162, 'train_rates': 0.987209575704864, 'learning_rate': 0.0005141485575743569, 'batch_size': 217, 'step_size': 6, 'gamma': 0.9402570170011247}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:28:20,798][0m Trial 29 finished with value: 0.6859883666038513 and parameters: {'observation_period_num': 147, 'train_rates': 0.9506652806998013, 'learning_rate': 0.0007332295552543349, 'batch_size': 220, 'step_size': 7, 'gamma': 0.9424907260207251}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:32:29,170][0m Trial 30 finished with value: 0.11302035301923752 and parameters: {'observation_period_num': 165, 'train_rates': 0.9879190610812973, 'learning_rate': 0.0004206707242556648, 'batch_size': 131, 'step_size': 8, 'gamma': 0.9163832728627229}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:36:25,955][0m Trial 31 finished with value: 0.1197630986571312 and parameters: {'observation_period_num': 160, 'train_rates': 0.9892911997650875, 'learning_rate': 0.0004610523294077537, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9155419670906522}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:39:04,724][0m Trial 32 finished with value: 0.8205732703208923 and parameters: {'observation_period_num': 120, 'train_rates': 0.9427722452424032, 'learning_rate': 0.0008177707401127677, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9564748135392898}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:43:30,165][0m Trial 33 finished with value: 0.1696750819683075 and parameters: {'observation_period_num': 179, 'train_rates': 0.9610075848172869, 'learning_rate': 0.0002554345479573743, 'batch_size': 213, 'step_size': 8, 'gamma': 0.93118505216844}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:47:59,656][0m Trial 34 finished with value: 0.18753060165082433 and parameters: {'observation_period_num': 190, 'train_rates': 0.9070758046369441, 'learning_rate': 0.0004826680718399141, 'batch_size': 249, 'step_size': 10, 'gamma': 0.9143909825802162}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:50:40,740][0m Trial 35 finished with value: 0.2649620929185082 and parameters: {'observation_period_num': 128, 'train_rates': 0.8677309443173233, 'learning_rate': 0.00014298707795598015, 'batch_size': 145, 'step_size': 6, 'gamma': 0.968805113142894}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:54:39,907][0m Trial 36 finished with value: 2.316915731172304 and parameters: {'observation_period_num': 164, 'train_rates': 0.9339316028229289, 'learning_rate': 0.000989944691158423, 'batch_size': 85, 'step_size': 11, 'gamma': 0.889979340605586}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:57:07,744][0m Trial 37 finished with value: 0.11009035259485245 and parameters: {'observation_period_num': 107, 'train_rates': 0.988950059750832, 'learning_rate': 0.00013620319397625807, 'batch_size': 231, 'step_size': 6, 'gamma': 0.876387565246115}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 05:59:07,812][0m Trial 38 finished with value: 0.35683060342392314 and parameters: {'observation_period_num': 98, 'train_rates': 0.829673801761664, 'learning_rate': 5.7715937583715126e-05, 'batch_size': 230, 'step_size': 6, 'gamma': 0.8737582253051756}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:00:48,813][0m Trial 39 finished with value: 0.5726334171819576 and parameters: {'observation_period_num': 91, 'train_rates': 0.7005626368217298, 'learning_rate': 0.00016081063884937108, 'batch_size': 233, 'step_size': 6, 'gamma': 0.9416768014334685}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:02:03,546][0m Trial 40 finished with value: 0.22340895235538483 and parameters: {'observation_period_num': 57, 'train_rates': 0.9606509929209127, 'learning_rate': 5.1576388840675853e-05, 'batch_size': 207, 'step_size': 9, 'gamma': 0.894726885609935}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:05:20,341][0m Trial 41 finished with value: 0.11048838496208191 and parameters: {'observation_period_num': 134, 'train_rates': 0.9885152234389969, 'learning_rate': 0.0004360171992786338, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9142483373704622}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:08:38,577][0m Trial 42 finished with value: 0.16523289680480957 and parameters: {'observation_period_num': 137, 'train_rates': 0.9682818003542043, 'learning_rate': 0.00011926457544915068, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9291678738358902}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:11:11,963][0m Trial 43 finished with value: 0.09732753038406372 and parameters: {'observation_period_num': 112, 'train_rates': 0.9894423060994757, 'learning_rate': 0.00030438382294035863, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8773430284908661}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:13:00,734][0m Trial 44 finished with value: 0.9420018309481366 and parameters: {'observation_period_num': 107, 'train_rates': 0.619244585334872, 'learning_rate': 0.00029882970040347303, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8798969531338894}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:14:30,885][0m Trial 45 finished with value: 0.2246070146560669 and parameters: {'observation_period_num': 71, 'train_rates': 0.8892339290245217, 'learning_rate': 0.0001783164444728833, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8612511952075144}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:18:07,540][0m Trial 46 finished with value: 0.18543606996536255 and parameters: {'observation_period_num': 153, 'train_rates': 0.9310933565312975, 'learning_rate': 8.115708335994679e-05, 'batch_size': 226, 'step_size': 10, 'gamma': 0.8800280590813042}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:20:01,259][0m Trial 47 finished with value: 1.381193995475769 and parameters: {'observation_period_num': 83, 'train_rates': 0.9685397887231458, 'learning_rate': 1.1420503757454598e-06, 'batch_size': 243, 'step_size': 13, 'gamma': 0.9023630633825886}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:25:21,054][0m Trial 48 finished with value: 0.15340229868888855 and parameters: {'observation_period_num': 207, 'train_rates': 0.9588609695433484, 'learning_rate': 0.00026859045597469846, 'batch_size': 256, 'step_size': 7, 'gamma': 0.9536616746740394}. Best is trial 26 with value: 0.0956888496875763.[0m
[32m[I 2025-01-04 06:27:38,380][0m Trial 49 finished with value: 0.6470773816108704 and parameters: {'observation_period_num': 104, 'train_rates': 0.9362844188467163, 'learning_rate': 7.2892890967559185e-06, 'batch_size': 211, 'step_size': 9, 'gamma': 0.8558104485594811}. Best is trial 26 with value: 0.0956888496875763.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 06:27:38,387][0m A new study created in memory with name: no-name-e64e5ad4-9a5e-4ed3-8407-3c796a115b9e[0m
[32m[I 2025-01-04 06:29:04,345][0m Trial 0 finished with value: 0.6845637732024851 and parameters: {'observation_period_num': 7, 'train_rates': 0.7244036096131429, 'learning_rate': 6.972037598388328e-06, 'batch_size': 42, 'step_size': 3, 'gamma': 0.9338807766884851}. Best is trial 0 with value: 0.6845637732024851.[0m
[32m[I 2025-01-04 06:34:19,863][0m Trial 1 finished with value: 1.520153923437629 and parameters: {'observation_period_num': 246, 'train_rates': 0.6064804671215753, 'learning_rate': 0.00029179139714713447, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9627032804720326}. Best is trial 0 with value: 0.6845637732024851.[0m
[32m[I 2025-01-04 06:37:18,027][0m Trial 2 finished with value: 0.5759851751940934 and parameters: {'observation_period_num': 142, 'train_rates': 0.6824710144672358, 'learning_rate': 1.4334310019135848e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9527606036340026}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:37:40,849][0m Trial 3 finished with value: 0.841420251589555 and parameters: {'observation_period_num': 9, 'train_rates': 0.6334101224442767, 'learning_rate': 1.611222430947206e-05, 'batch_size': 166, 'step_size': 13, 'gamma': 0.9464276357440089}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:38:56,193][0m Trial 4 finished with value: 0.6627258548736572 and parameters: {'observation_period_num': 59, 'train_rates': 0.9136287408333599, 'learning_rate': 3.4625165111023395e-06, 'batch_size': 201, 'step_size': 6, 'gamma': 0.9728887854909285}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:43:58,121][0m Trial 5 finished with value: 1.5223398401875545 and parameters: {'observation_period_num': 223, 'train_rates': 0.7853349665065448, 'learning_rate': 2.0843686586804795e-06, 'batch_size': 99, 'step_size': 3, 'gamma': 0.8462357655195584}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:46:49,204][0m Trial 6 finished with value: 0.8449328660262702 and parameters: {'observation_period_num': 139, 'train_rates': 0.7588506292704024, 'learning_rate': 7.113708948353257e-06, 'batch_size': 152, 'step_size': 14, 'gamma': 0.9492251583800884}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:47:18,997][0m Trial 7 finished with value: 0.7103457066589866 and parameters: {'observation_period_num': 25, 'train_rates': 0.6707390622210523, 'learning_rate': 0.00030544246098195644, 'batch_size': 201, 'step_size': 5, 'gamma': 0.8979383805232242}. Best is trial 2 with value: 0.5759851751940934.[0m
[32m[I 2025-01-04 06:47:58,360][0m Trial 8 finished with value: 0.462527420801612 and parameters: {'observation_period_num': 8, 'train_rates': 0.9296312173350586, 'learning_rate': 6.917442723313006e-06, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8286469780850211}. Best is trial 8 with value: 0.462527420801612.[0m
[32m[I 2025-01-04 06:50:41,780][0m Trial 9 finished with value: 0.24473407864570618 and parameters: {'observation_period_num': 119, 'train_rates': 0.967769575341931, 'learning_rate': 0.0003356713444942572, 'batch_size': 122, 'step_size': 11, 'gamma': 0.8921221072210354}. Best is trial 9 with value: 0.24473407864570618.[0m
[32m[I 2025-01-04 06:52:34,379][0m Trial 10 finished with value: 0.43664705844325874 and parameters: {'observation_period_num': 90, 'train_rates': 0.8627593384173867, 'learning_rate': 0.000919932842011897, 'batch_size': 249, 'step_size': 11, 'gamma': 0.777443069369633}. Best is trial 9 with value: 0.24473407864570618.[0m
[32m[I 2025-01-04 06:54:40,358][0m Trial 11 finished with value: 0.1504669189453125 and parameters: {'observation_period_num': 92, 'train_rates': 0.9847682016408396, 'learning_rate': 0.0005997110300103027, 'batch_size': 242, 'step_size': 11, 'gamma': 0.7756369342149325}. Best is trial 11 with value: 0.1504669189453125.[0m
[32m[I 2025-01-04 06:57:06,293][0m Trial 12 finished with value: 0.11287839710712433 and parameters: {'observation_period_num': 102, 'train_rates': 0.9863493916571662, 'learning_rate': 7.696816840214404e-05, 'batch_size': 89, 'step_size': 11, 'gamma': 0.7569106807153934}. Best is trial 12 with value: 0.11287839710712433.[0m
[32m[I 2025-01-04 07:02:11,272][0m Trial 13 finished with value: 0.09585876762866974 and parameters: {'observation_period_num': 194, 'train_rates': 0.9872053450553365, 'learning_rate': 6.113974617948114e-05, 'batch_size': 75, 'step_size': 15, 'gamma': 0.7520308118739937}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:06:30,137][0m Trial 14 finished with value: 0.29474072781937544 and parameters: {'observation_period_num': 186, 'train_rates': 0.8514306989010826, 'learning_rate': 7.18725335209072e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.7550340735006001}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:10:41,765][0m Trial 15 finished with value: 0.25901273224088883 and parameters: {'observation_period_num': 184, 'train_rates': 0.8572343066944381, 'learning_rate': 8.631717782913338e-05, 'batch_size': 71, 'step_size': 13, 'gamma': 0.8098610817653219}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:15:00,310][0m Trial 16 finished with value: 0.17904840247786563 and parameters: {'observation_period_num': 180, 'train_rates': 0.9171795080137866, 'learning_rate': 6.652637777973312e-05, 'batch_size': 77, 'step_size': 15, 'gamma': 0.7962822171088699}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:19:06,106][0m Trial 17 finished with value: 0.2716242844058621 and parameters: {'observation_period_num': 166, 'train_rates': 0.9555079468863275, 'learning_rate': 4.006760378263473e-05, 'batch_size': 92, 'step_size': 9, 'gamma': 0.7542881922752552}. Best is trial 13 with value: 0.09585876762866974.[0m
Early stopping at epoch 81
[32m[I 2025-01-04 07:23:42,610][0m Trial 18 finished with value: 0.4047422972417647 and parameters: {'observation_period_num': 227, 'train_rates': 0.8865042198235319, 'learning_rate': 0.00014862771685047741, 'batch_size': 138, 'step_size': 1, 'gamma': 0.8556397741626383}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:25:49,860][0m Trial 19 finished with value: 0.3457330425058476 and parameters: {'observation_period_num': 94, 'train_rates': 0.8135720335429355, 'learning_rate': 2.7903071037201732e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.7980431439134774}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:27:14,923][0m Trial 20 finished with value: 0.11448650062084198 and parameters: {'observation_period_num': 64, 'train_rates': 0.9877822101400143, 'learning_rate': 0.00015881900266394042, 'batch_size': 176, 'step_size': 10, 'gamma': 0.8220623204175884}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:28:58,933][0m Trial 21 finished with value: 0.14704392850399017 and parameters: {'observation_period_num': 74, 'train_rates': 0.980916594125096, 'learning_rate': 0.00014330917718604775, 'batch_size': 171, 'step_size': 7, 'gamma': 0.776676953742742}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:30:20,293][0m Trial 22 finished with value: 0.2658799380626319 and parameters: {'observation_period_num': 60, 'train_rates': 0.9451022915560379, 'learning_rate': 3.814125326711267e-05, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8161355629246214}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:33:03,295][0m Trial 23 finished with value: 0.13068890571594238 and parameters: {'observation_period_num': 117, 'train_rates': 0.9856728852011821, 'learning_rate': 0.00013565083307738354, 'batch_size': 223, 'step_size': 12, 'gamma': 0.7557613159645797}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:33:56,420][0m Trial 24 finished with value: 0.27250075399021095 and parameters: {'observation_period_num': 40, 'train_rates': 0.9043012726364551, 'learning_rate': 2.1957439889146945e-05, 'batch_size': 191, 'step_size': 9, 'gamma': 0.8817353522919621}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:37:42,838][0m Trial 25 finished with value: 0.20940675692898886 and parameters: {'observation_period_num': 157, 'train_rates': 0.9451753859700656, 'learning_rate': 5.576585891387692e-05, 'batch_size': 88, 'step_size': 13, 'gamma': 0.8348591564760246}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:40:10,174][0m Trial 26 finished with value: 0.21238555073373544 and parameters: {'observation_period_num': 109, 'train_rates': 0.884987179383357, 'learning_rate': 0.00021186837951087697, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7936392169711248}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:45:22,201][0m Trial 27 finished with value: 0.2144564986228943 and parameters: {'observation_period_num': 205, 'train_rates': 0.952354888371658, 'learning_rate': 9.499725463733636e-05, 'batch_size': 150, 'step_size': 14, 'gamma': 0.7708936363923821}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:46:35,975][0m Trial 28 finished with value: 0.22016136348247528 and parameters: {'observation_period_num': 52, 'train_rates': 0.9873739472061737, 'learning_rate': 0.000558522245645056, 'batch_size': 113, 'step_size': 10, 'gamma': 0.921306933470315}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:48:20,632][0m Trial 29 finished with value: 0.32503952965612076 and parameters: {'observation_period_num': 77, 'train_rates': 0.8133278143969025, 'learning_rate': 4.151298884823589e-05, 'batch_size': 58, 'step_size': 12, 'gamma': 0.8653089818427632}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:51:05,584][0m Trial 30 finished with value: 0.9553440615328083 and parameters: {'observation_period_num': 138, 'train_rates': 0.7181896747707862, 'learning_rate': 1.060032377882281e-05, 'batch_size': 180, 'step_size': 8, 'gamma': 0.8112933742919238}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:53:37,395][0m Trial 31 finished with value: 0.124565489590168 and parameters: {'observation_period_num': 110, 'train_rates': 0.989518710453701, 'learning_rate': 0.0001373121449881997, 'batch_size': 225, 'step_size': 12, 'gamma': 0.7583710273209138}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:55:54,838][0m Trial 32 finished with value: 0.20855088531970978 and parameters: {'observation_period_num': 104, 'train_rates': 0.9343081821423986, 'learning_rate': 0.0001921153155137981, 'batch_size': 214, 'step_size': 10, 'gamma': 0.7662066845029373}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 07:59:26,267][0m Trial 33 finished with value: 0.18444623564945833 and parameters: {'observation_period_num': 129, 'train_rates': 0.9537122699828517, 'learning_rate': 9.921421020329503e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.7890691260695503}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:06:12,488][0m Trial 34 finished with value: 0.13832074403762817 and parameters: {'observation_period_num': 252, 'train_rates': 0.9692331901661253, 'learning_rate': 0.00036916781943661047, 'batch_size': 230, 'step_size': 12, 'gamma': 0.7645084390103628}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:10:41,068][0m Trial 35 finished with value: 0.21026533298248792 and parameters: {'observation_period_num': 154, 'train_rates': 0.9022446515961557, 'learning_rate': 0.00019960479522431092, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7845411026117741}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:12:26,139][0m Trial 36 finished with value: 0.17427438497543335 and parameters: {'observation_period_num': 75, 'train_rates': 0.9899614206324194, 'learning_rate': 4.9314890009698985e-05, 'batch_size': 146, 'step_size': 11, 'gamma': 0.7529459416413912}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:15:09,074][0m Trial 37 finished with value: 0.1805355465918217 and parameters: {'observation_period_num': 123, 'train_rates': 0.9253702893010026, 'learning_rate': 0.00012176286392424241, 'batch_size': 170, 'step_size': 15, 'gamma': 0.826343595132709}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:15:57,127][0m Trial 38 finished with value: 0.29387256503105164 and parameters: {'observation_period_num': 35, 'train_rates': 0.9625575279867998, 'learning_rate': 2.1159399348945563e-05, 'batch_size': 201, 'step_size': 14, 'gamma': 0.9820769901612145}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:17:54,321][0m Trial 39 finished with value: 1.0445865254423934 and parameters: {'observation_period_num': 99, 'train_rates': 0.6161391871952739, 'learning_rate': 0.0002393080727279465, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8039943627577975}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:23:01,122][0m Trial 40 finished with value: 0.5840229294401534 and parameters: {'observation_period_num': 235, 'train_rates': 0.7406921808586249, 'learning_rate': 0.00046297981752978825, 'batch_size': 159, 'step_size': 10, 'gamma': 0.8426993476397827}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:25:34,974][0m Trial 41 finished with value: 0.2041141241788864 and parameters: {'observation_period_num': 113, 'train_rates': 0.9692929769977118, 'learning_rate': 0.00013318988068403265, 'batch_size': 219, 'step_size': 12, 'gamma': 0.7609017600338228}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:28:54,945][0m Trial 42 finished with value: 0.19283567368984222 and parameters: {'observation_period_num': 135, 'train_rates': 0.9887860546812866, 'learning_rate': 7.023768742234002e-05, 'batch_size': 233, 'step_size': 12, 'gamma': 0.7523316747483632}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:32:22,705][0m Trial 43 finished with value: 0.22562982141971588 and parameters: {'observation_period_num': 147, 'train_rates': 0.9367026649315938, 'learning_rate': 0.00010615415500765789, 'batch_size': 217, 'step_size': 11, 'gamma': 0.7865126309525268}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:33:44,174][0m Trial 44 finished with value: 1.2805134057998657 and parameters: {'observation_period_num': 62, 'train_rates': 0.9683470394026372, 'learning_rate': 1.1353217614136926e-06, 'batch_size': 191, 'step_size': 13, 'gamma': 0.7683208110054945}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:35:36,564][0m Trial 45 finished with value: 0.3459358215332031 and parameters: {'observation_period_num': 82, 'train_rates': 0.9706489376569083, 'learning_rate': 2.888044319478117e-05, 'batch_size': 248, 'step_size': 9, 'gamma': 0.7781320061696012}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:39:39,066][0m Trial 46 finished with value: 0.8153335865843233 and parameters: {'observation_period_num': 204, 'train_rates': 0.6560469793723811, 'learning_rate': 0.00016677778327790156, 'batch_size': 107, 'step_size': 11, 'gamma': 0.9294824866082266}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:42:18,445][0m Trial 47 finished with value: 0.23326431214809418 and parameters: {'observation_period_num': 121, 'train_rates': 0.9200163891747464, 'learning_rate': 5.667684504125196e-05, 'batch_size': 233, 'step_size': 13, 'gamma': 0.7625122767706939}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:42:47,010][0m Trial 48 finished with value: 0.16003335131441845 and parameters: {'observation_period_num': 19, 'train_rates': 0.8841725141019693, 'learning_rate': 0.00026361637360212796, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9107053953053005}. Best is trial 13 with value: 0.09585876762866974.[0m
[32m[I 2025-01-04 08:44:47,450][0m Trial 49 finished with value: 0.2459256887435913 and parameters: {'observation_period_num': 87, 'train_rates': 0.9408649909166135, 'learning_rate': 8.754159392565103e-05, 'batch_size': 83, 'step_size': 5, 'gamma': 0.7813180029595705}. Best is trial 13 with value: 0.09585876762866974.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 200, 'train_rates': 0.9880778068132221, 'learning_rate': 0.00010165307808629653, 'batch_size': 41, 'step_size': 13, 'gamma': 0.9777358735225424}
Epoch 1/300, trend Loss: 0.6433 | 0.6888
Epoch 2/300, trend Loss: 0.5282 | 0.5000
Epoch 3/300, trend Loss: 0.4019 | 0.4556
Epoch 4/300, trend Loss: 0.3484 | 0.3636
Epoch 5/300, trend Loss: 0.3859 | 0.4300
Epoch 6/300, trend Loss: 0.3181 | 0.3800
Epoch 7/300, trend Loss: 0.3904 | 0.3636
Epoch 8/300, trend Loss: 0.3363 | 0.2826
Epoch 9/300, trend Loss: 0.2893 | 0.3171
Epoch 10/300, trend Loss: 0.2798 | 0.2446
Epoch 11/300, trend Loss: 0.2391 | 0.2236
Epoch 12/300, trend Loss: 0.2290 | 0.1933
Epoch 13/300, trend Loss: 0.2119 | 0.1873
Epoch 14/300, trend Loss: 0.2035 | 0.1678
Epoch 15/300, trend Loss: 0.2060 | 0.1813
Epoch 16/300, trend Loss: 0.1934 | 0.1536
Epoch 17/300, trend Loss: 0.2123 | 0.1796
Epoch 18/300, trend Loss: 0.2062 | 0.1678
Epoch 19/300, trend Loss: 0.1977 | 0.1479
Epoch 20/300, trend Loss: 0.2030 | 0.1569
Epoch 21/300, trend Loss: 0.1933 | 0.1642
Epoch 22/300, trend Loss: 0.1904 | 0.1480
Epoch 23/300, trend Loss: 0.1892 | 0.1455
Epoch 24/300, trend Loss: 0.1970 | 0.1386
Epoch 25/300, trend Loss: 0.1980 | 0.1720
Epoch 26/300, trend Loss: 0.2033 | 0.1496
Epoch 27/300, trend Loss: 0.2232 | 0.1572
Epoch 28/300, trend Loss: 0.2069 | 0.1882
Epoch 29/300, trend Loss: 0.2029 | 0.1739
Epoch 30/300, trend Loss: 0.2021 | 0.1664
Epoch 31/300, trend Loss: 0.2079 | 0.1609
Epoch 32/300, trend Loss: 0.1869 | 0.1631
Epoch 33/300, trend Loss: 0.1779 | 0.1247
Epoch 34/300, trend Loss: 0.1725 | 0.1229
Epoch 35/300, trend Loss: 0.1678 | 0.1339
Epoch 36/300, trend Loss: 0.1593 | 0.1186
Epoch 37/300, trend Loss: 0.1617 | 0.1248
Epoch 38/300, trend Loss: 0.1693 | 0.1270
Epoch 39/300, trend Loss: 0.1665 | 0.1192
Epoch 40/300, trend Loss: 0.1585 | 0.1130
Epoch 41/300, trend Loss: 0.1575 | 0.1096
Epoch 42/300, trend Loss: 0.1658 | 0.1155
Epoch 43/300, trend Loss: 0.1599 | 0.1120
Epoch 44/300, trend Loss: 0.1615 | 0.1063
Epoch 45/300, trend Loss: 0.1622 | 0.1357
Epoch 46/300, trend Loss: 0.1565 | 0.1059
Epoch 47/300, trend Loss: 0.1462 | 0.1017
Epoch 48/300, trend Loss: 0.1487 | 0.1149
Epoch 49/300, trend Loss: 0.1396 | 0.1097
Epoch 50/300, trend Loss: 0.1367 | 0.1206
Epoch 51/300, trend Loss: 0.1414 | 0.1103
Epoch 52/300, trend Loss: 0.1501 | 0.1244
Epoch 53/300, trend Loss: 0.1485 | 0.1153
Epoch 54/300, trend Loss: 0.1473 | 0.0941
Epoch 55/300, trend Loss: 0.1434 | 0.1001
Epoch 56/300, trend Loss: 0.1490 | 0.1192
Epoch 57/300, trend Loss: 0.1461 | 0.1040
Epoch 58/300, trend Loss: 0.1489 | 0.0949
Epoch 59/300, trend Loss: 0.1463 | 0.1109
Epoch 60/300, trend Loss: 0.1432 | 0.1075
Epoch 61/300, trend Loss: 0.1433 | 0.0919
Epoch 62/300, trend Loss: 0.1526 | 0.1076
Epoch 63/300, trend Loss: 0.1493 | 0.1160
Epoch 64/300, trend Loss: 0.1362 | 0.0924
Epoch 65/300, trend Loss: 0.1378 | 0.0932
Epoch 66/300, trend Loss: 0.1291 | 0.0998
Epoch 67/300, trend Loss: 0.1241 | 0.0877
Epoch 68/300, trend Loss: 0.1226 | 0.0847
Epoch 69/300, trend Loss: 0.1233 | 0.1119
Epoch 70/300, trend Loss: 0.1174 | 0.0796
Epoch 71/300, trend Loss: 0.1195 | 0.0890
Epoch 72/300, trend Loss: 0.1164 | 0.0961
Epoch 73/300, trend Loss: 0.1169 | 0.0874
Epoch 74/300, trend Loss: 0.1196 | 0.0888
Epoch 75/300, trend Loss: 0.1209 | 0.0801
Epoch 76/300, trend Loss: 0.1246 | 0.1143
Epoch 77/300, trend Loss: 0.1177 | 0.0951
Epoch 78/300, trend Loss: 0.1198 | 0.0833
Epoch 79/300, trend Loss: 0.1233 | 0.0878
Epoch 80/300, trend Loss: 0.1243 | 0.0987
Epoch 81/300, trend Loss: 0.1272 | 0.0823
Epoch 82/300, trend Loss: 0.1279 | 0.0919
Epoch 83/300, trend Loss: 0.1231 | 0.0964
Epoch 84/300, trend Loss: 0.1187 | 0.0807
Epoch 85/300, trend Loss: 0.1249 | 0.0864
Epoch 86/300, trend Loss: 0.1090 | 0.0820
Epoch 87/300, trend Loss: 0.1053 | 0.0828
Epoch 88/300, trend Loss: 0.1067 | 0.0744
Epoch 89/300, trend Loss: 0.1020 | 0.0752
Epoch 90/300, trend Loss: 0.1016 | 0.0815
Epoch 91/300, trend Loss: 0.0969 | 0.0750
Epoch 92/300, trend Loss: 0.0964 | 0.0721
Epoch 93/300, trend Loss: 0.0932 | 0.0770
Epoch 94/300, trend Loss: 0.0977 | 0.0735
Epoch 95/300, trend Loss: 0.0982 | 0.0705
Epoch 96/300, trend Loss: 0.0939 | 0.0712
Epoch 97/300, trend Loss: 0.0934 | 0.0736
Epoch 98/300, trend Loss: 0.0941 | 0.0766
Epoch 99/300, trend Loss: 0.0962 | 0.0732
Epoch 100/300, trend Loss: 0.0994 | 0.0793
Epoch 101/300, trend Loss: 0.1043 | 0.0899
Epoch 102/300, trend Loss: 0.1035 | 0.0740
Epoch 103/300, trend Loss: 0.1054 | 0.0823
Epoch 104/300, trend Loss: 0.1092 | 0.0860
Epoch 105/300, trend Loss: 0.1103 | 0.0868
Epoch 106/300, trend Loss: 0.1088 | 0.0810
Epoch 107/300, trend Loss: 0.1057 | 0.0779
Epoch 108/300, trend Loss: 0.1005 | 0.0794
Epoch 109/300, trend Loss: 0.0998 | 0.0716
Epoch 110/300, trend Loss: 0.1012 | 0.0755
Epoch 111/300, trend Loss: 0.0984 | 0.0788
Epoch 112/300, trend Loss: 0.0978 | 0.0682
Epoch 113/300, trend Loss: 0.0956 | 0.0726
Epoch 114/300, trend Loss: 0.0868 | 0.0729
Epoch 115/300, trend Loss: 0.0849 | 0.0659
Epoch 116/300, trend Loss: 0.0894 | 0.0643
Epoch 117/300, trend Loss: 0.0852 | 0.0742
Epoch 118/300, trend Loss: 0.0795 | 0.0662
Epoch 119/300, trend Loss: 0.0769 | 0.0651
Epoch 120/300, trend Loss: 0.0788 | 0.0673
Epoch 121/300, trend Loss: 0.0778 | 0.0659
Epoch 122/300, trend Loss: 0.0772 | 0.0630
Epoch 123/300, trend Loss: 0.0770 | 0.0684
Epoch 124/300, trend Loss: 0.0765 | 0.0691
Epoch 125/300, trend Loss: 0.0783 | 0.0665
Epoch 126/300, trend Loss: 0.0809 | 0.0719
Epoch 127/300, trend Loss: 0.0796 | 0.0715
Epoch 128/300, trend Loss: 0.0803 | 0.0710
Epoch 129/300, trend Loss: 0.0827 | 0.0782
Epoch 130/300, trend Loss: 0.0818 | 0.0681
Epoch 131/300, trend Loss: 0.0890 | 0.0682
Epoch 132/300, trend Loss: 0.0908 | 0.0681
Epoch 133/300, trend Loss: 0.0909 | 0.0746
Epoch 134/300, trend Loss: 0.0906 | 0.0671
Epoch 135/300, trend Loss: 0.0877 | 0.0712
Epoch 136/300, trend Loss: 0.0905 | 0.0739
Epoch 137/300, trend Loss: 0.0838 | 0.0638
Epoch 138/300, trend Loss: 0.0798 | 0.0663
Epoch 139/300, trend Loss: 0.0773 | 0.0645
Epoch 140/300, trend Loss: 0.0742 | 0.0668
Epoch 141/300, trend Loss: 0.0724 | 0.0636
Epoch 142/300, trend Loss: 0.0717 | 0.0617
Epoch 143/300, trend Loss: 0.0743 | 0.0655
Epoch 144/300, trend Loss: 0.0711 | 0.0654
Epoch 145/300, trend Loss: 0.0668 | 0.0634
Epoch 146/300, trend Loss: 0.0669 | 0.0665
Epoch 147/300, trend Loss: 0.0698 | 0.0639
Epoch 148/300, trend Loss: 0.0673 | 0.0620
Epoch 149/300, trend Loss: 0.0689 | 0.0656
Epoch 150/300, trend Loss: 0.0656 | 0.0622
Epoch 151/300, trend Loss: 0.0669 | 0.0655
Epoch 152/300, trend Loss: 0.0668 | 0.0641
Epoch 153/300, trend Loss: 0.0674 | 0.0627
Epoch 154/300, trend Loss: 0.0697 | 0.0624
Epoch 155/300, trend Loss: 0.0682 | 0.0612
Epoch 156/300, trend Loss: 0.0677 | 0.0702
Epoch 157/300, trend Loss: 0.0676 | 0.0642
Epoch 158/300, trend Loss: 0.0693 | 0.0654
Epoch 159/300, trend Loss: 0.0727 | 0.0766
Epoch 160/300, trend Loss: 0.0722 | 0.0699
Epoch 161/300, trend Loss: 0.0752 | 0.0661
Epoch 162/300, trend Loss: 0.0740 | 0.0676
Epoch 163/300, trend Loss: 0.0735 | 0.0685
Epoch 164/300, trend Loss: 0.0691 | 0.0631
Epoch 165/300, trend Loss: 0.0742 | 0.0627
Epoch 166/300, trend Loss: 0.0722 | 0.0701
Epoch 167/300, trend Loss: 0.0743 | 0.0630
Epoch 168/300, trend Loss: 0.0669 | 0.0681
Epoch 169/300, trend Loss: 0.0660 | 0.0664
Epoch 170/300, trend Loss: 0.0657 | 0.0601
Epoch 171/300, trend Loss: 0.0670 | 0.0661
Epoch 172/300, trend Loss: 0.0618 | 0.0624
Epoch 173/300, trend Loss: 0.0606 | 0.0598
Epoch 174/300, trend Loss: 0.0577 | 0.0640
Epoch 175/300, trend Loss: 0.0571 | 0.0624
Epoch 176/300, trend Loss: 0.0573 | 0.0581
Epoch 177/300, trend Loss: 0.0576 | 0.0642
Epoch 178/300, trend Loss: 0.0557 | 0.0643
Epoch 179/300, trend Loss: 0.0552 | 0.0595
Epoch 180/300, trend Loss: 0.0556 | 0.0608
Epoch 181/300, trend Loss: 0.0571 | 0.0637
Epoch 182/300, trend Loss: 0.0581 | 0.0579
Epoch 183/300, trend Loss: 0.0582 | 0.0576
Epoch 184/300, trend Loss: 0.0601 | 0.0649
Epoch 185/300, trend Loss: 0.0602 | 0.0616
Epoch 186/300, trend Loss: 0.0641 | 0.0689
Epoch 187/300, trend Loss: 0.0608 | 0.0645
Epoch 188/300, trend Loss: 0.0619 | 0.0619
Epoch 189/300, trend Loss: 0.0594 | 0.0615
Epoch 190/300, trend Loss: 0.0613 | 0.0564
Epoch 191/300, trend Loss: 0.0606 | 0.0654
Epoch 192/300, trend Loss: 0.0617 | 0.0608
Epoch 193/300, trend Loss: 0.0600 | 0.0601
Epoch 194/300, trend Loss: 0.0570 | 0.0657
Epoch 195/300, trend Loss: 0.0556 | 0.0590
Epoch 196/300, trend Loss: 0.0548 | 0.0610
Epoch 197/300, trend Loss: 0.0562 | 0.0617
Epoch 198/300, trend Loss: 0.0525 | 0.0588
Epoch 199/300, trend Loss: 0.0518 | 0.0594
Epoch 200/300, trend Loss: 0.0522 | 0.0603
Epoch 201/300, trend Loss: 0.0523 | 0.0615
Epoch 202/300, trend Loss: 0.0510 | 0.0597
Epoch 203/300, trend Loss: 0.0528 | 0.0598
Epoch 204/300, trend Loss: 0.0518 | 0.0604
Epoch 205/300, trend Loss: 0.0532 | 0.0621
Epoch 206/300, trend Loss: 0.0521 | 0.0641
Epoch 207/300, trend Loss: 0.0591 | 0.0583
Epoch 208/300, trend Loss: 0.0538 | 0.0601
Epoch 209/300, trend Loss: 0.0516 | 0.0614
Epoch 210/300, trend Loss: 0.0523 | 0.0590
Epoch 211/300, trend Loss: 0.0509 | 0.0559
Epoch 212/300, trend Loss: 0.0533 | 0.0632
Epoch 213/300, trend Loss: 0.0520 | 0.0655
Epoch 214/300, trend Loss: 0.0510 | 0.0583
Epoch 215/300, trend Loss: 0.0495 | 0.0611
Epoch 216/300, trend Loss: 0.0511 | 0.0604
Epoch 217/300, trend Loss: 0.0474 | 0.0573
Epoch 218/300, trend Loss: 0.0474 | 0.0590
Epoch 219/300, trend Loss: 0.0487 | 0.0661
Epoch 220/300, trend Loss: 0.0470 | 0.0594
Epoch 221/300, trend Loss: 0.0457 | 0.0606
Epoch 222/300, trend Loss: 0.0476 | 0.0600
Epoch 223/300, trend Loss: 0.0466 | 0.0547
Epoch 224/300, trend Loss: 0.0537 | 0.0601
Epoch 225/300, trend Loss: 0.0536 | 0.0621
Epoch 226/300, trend Loss: 0.0504 | 0.0607
Epoch 227/300, trend Loss: 0.0476 | 0.0629
Epoch 228/300, trend Loss: 0.0459 | 0.0609
Epoch 229/300, trend Loss: 0.0464 | 0.0596
Epoch 230/300, trend Loss: 0.0441 | 0.0585
Epoch 231/300, trend Loss: 0.0431 | 0.0612
Epoch 232/300, trend Loss: 0.0439 | 0.0592
Epoch 233/300, trend Loss: 0.0442 | 0.0571
Epoch 234/300, trend Loss: 0.0441 | 0.0598
Epoch 235/300, trend Loss: 0.0454 | 0.0599
Epoch 236/300, trend Loss: 0.0463 | 0.0576
Epoch 237/300, trend Loss: 0.0475 | 0.0606
Epoch 238/300, trend Loss: 0.0460 | 0.0592
Epoch 239/300, trend Loss: 0.0440 | 0.0576
Epoch 240/300, trend Loss: 0.0462 | 0.0574
Epoch 241/300, trend Loss: 0.0469 | 0.0587
Epoch 242/300, trend Loss: 0.0458 | 0.0606
Epoch 243/300, trend Loss: 0.0426 | 0.0583
Epoch 244/300, trend Loss: 0.0427 | 0.0611
Epoch 245/300, trend Loss: 0.0424 | 0.0641
Epoch 246/300, trend Loss: 0.0451 | 0.0565
Epoch 247/300, trend Loss: 0.0442 | 0.0572
Epoch 248/300, trend Loss: 0.0445 | 0.0604
Epoch 249/300, trend Loss: 0.0419 | 0.0573
Epoch 250/300, trend Loss: 0.0416 | 0.0576
Epoch 251/300, trend Loss: 0.0459 | 0.0628
Epoch 252/300, trend Loss: 0.0456 | 0.0576
Epoch 253/300, trend Loss: 0.0413 | 0.0599
Epoch 254/300, trend Loss: 0.0423 | 0.0623
Epoch 255/300, trend Loss: 0.0403 | 0.0586
Epoch 256/300, trend Loss: 0.0400 | 0.0601
Epoch 257/300, trend Loss: 0.0399 | 0.0576
Epoch 258/300, trend Loss: 0.0395 | 0.0559
Epoch 259/300, trend Loss: 0.0394 | 0.0545
Epoch 260/300, trend Loss: 0.0388 | 0.0589
Epoch 261/300, trend Loss: 0.0392 | 0.0589
Epoch 262/300, trend Loss: 0.0383 | 0.0573
Epoch 263/300, trend Loss: 0.0385 | 0.0616
Epoch 264/300, trend Loss: 0.0402 | 0.0567
Epoch 265/300, trend Loss: 0.0399 | 0.0574
Epoch 266/300, trend Loss: 0.0498 | 0.0584
Epoch 267/300, trend Loss: 0.0409 | 0.0585
Epoch 268/300, trend Loss: 0.0371 | 0.0551
Epoch 269/300, trend Loss: 0.0362 | 0.0567
Epoch 270/300, trend Loss: 0.0370 | 0.0602
Epoch 271/300, trend Loss: 0.0370 | 0.0578
Epoch 272/300, trend Loss: 0.0366 | 0.0595
Epoch 273/300, trend Loss: 0.0372 | 0.0578
Epoch 274/300, trend Loss: 0.0382 | 0.0581
Epoch 275/300, trend Loss: 0.0376 | 0.0575
Epoch 276/300, trend Loss: 0.0422 | 0.0589
Epoch 277/300, trend Loss: 0.0407 | 0.0571
Epoch 278/300, trend Loss: 0.0391 | 0.0573
Epoch 279/300, trend Loss: 0.0380 | 0.0573
Epoch 280/300, trend Loss: 0.0370 | 0.0588
Epoch 281/300, trend Loss: 0.0367 | 0.0590
Epoch 282/300, trend Loss: 0.0462 | 0.0578
Epoch 283/300, trend Loss: 0.0378 | 0.0595
Epoch 284/300, trend Loss: 0.0368 | 0.0574
Epoch 285/300, trend Loss: 0.0357 | 0.0556
Epoch 286/300, trend Loss: 0.0353 | 0.0579
Epoch 287/300, trend Loss: 0.0352 | 0.0542
Epoch 288/300, trend Loss: 0.0346 | 0.0556
Epoch 289/300, trend Loss: 0.0336 | 0.0584
Epoch 290/300, trend Loss: 0.0336 | 0.0564
Epoch 291/300, trend Loss: 0.0335 | 0.0554
Epoch 292/300, trend Loss: 0.0340 | 0.0586
Epoch 293/300, trend Loss: 0.0337 | 0.0565
Epoch 294/300, trend Loss: 0.0337 | 0.0547
Epoch 295/300, trend Loss: 0.0341 | 0.0576
Epoch 296/300, trend Loss: 0.0339 | 0.0590
Epoch 297/300, trend Loss: 0.0349 | 0.0601
Epoch 298/300, trend Loss: 0.0335 | 0.0603
Epoch 299/300, trend Loss: 0.0339 | 0.0584
Epoch 300/300, trend Loss: 0.0347 | 0.0570
Training seasonal_0 component with params: {'observation_period_num': 56, 'train_rates': 0.9886672729115791, 'learning_rate': 0.00048737209514102485, 'batch_size': 206, 'step_size': 9, 'gamma': 0.9682369561819397}
Epoch 1/300, seasonal_0 Loss: 0.9510 | 0.8486
Epoch 2/300, seasonal_0 Loss: 0.9319 | 0.7079
Epoch 3/300, seasonal_0 Loss: 0.8026 | 0.7221
Epoch 4/300, seasonal_0 Loss: 0.6788 | 0.6656
Epoch 5/300, seasonal_0 Loss: 0.6450 | 0.6071
Epoch 6/300, seasonal_0 Loss: 0.6034 | 0.5621
Epoch 7/300, seasonal_0 Loss: 0.5926 | 0.5183
Epoch 8/300, seasonal_0 Loss: 0.7693 | 0.8550
Epoch 9/300, seasonal_0 Loss: 0.7186 | 0.6289
Epoch 10/300, seasonal_0 Loss: 0.6821 | 0.5982
Epoch 11/300, seasonal_0 Loss: 0.6401 | 0.5723
Epoch 12/300, seasonal_0 Loss: 0.6007 | 0.5551
Epoch 13/300, seasonal_0 Loss: 0.6397 | 0.5323
Epoch 14/300, seasonal_0 Loss: 0.6940 | 0.5688
Epoch 15/300, seasonal_0 Loss: 0.6329 | 0.5613
Epoch 16/300, seasonal_0 Loss: 0.6788 | 0.7627
Epoch 17/300, seasonal_0 Loss: 0.6070 | 0.4905
Epoch 18/300, seasonal_0 Loss: 0.5476 | 0.5321
Epoch 19/300, seasonal_0 Loss: 0.4948 | 0.5059
Epoch 20/300, seasonal_0 Loss: 0.4371 | 0.4587
Epoch 21/300, seasonal_0 Loss: 0.4185 | 0.4013
Epoch 22/300, seasonal_0 Loss: 0.4127 | 0.3646
Epoch 23/300, seasonal_0 Loss: 0.5086 | 0.4173
Epoch 24/300, seasonal_0 Loss: 0.4337 | 0.3764
Epoch 25/300, seasonal_0 Loss: 0.4029 | 0.5407
Epoch 26/300, seasonal_0 Loss: 0.3873 | 0.3284
Epoch 27/300, seasonal_0 Loss: 0.4235 | 0.4076
Epoch 28/300, seasonal_0 Loss: 0.3766 | 0.4380
Epoch 29/300, seasonal_0 Loss: 0.4159 | 0.2852
Epoch 30/300, seasonal_0 Loss: 0.4681 | 0.3176
Epoch 31/300, seasonal_0 Loss: 0.4119 | 0.3335
Epoch 32/300, seasonal_0 Loss: 0.6243 | 0.4334
Epoch 33/300, seasonal_0 Loss: 0.7383 | 0.5954
Epoch 34/300, seasonal_0 Loss: 0.4771 | 0.3453
Epoch 35/300, seasonal_0 Loss: 0.4036 | 0.4376
Epoch 36/300, seasonal_0 Loss: 0.3528 | 0.3056
Epoch 37/300, seasonal_0 Loss: 0.2963 | 0.3532
Epoch 38/300, seasonal_0 Loss: 0.2688 | 0.2715
Epoch 39/300, seasonal_0 Loss: 0.2493 | 0.2221
Epoch 40/300, seasonal_0 Loss: 0.2432 | 0.2291
Epoch 41/300, seasonal_0 Loss: 0.2488 | 0.2011
Epoch 42/300, seasonal_0 Loss: 0.2753 | 0.2021
Epoch 43/300, seasonal_0 Loss: 0.2618 | 0.2130
Epoch 44/300, seasonal_0 Loss: 0.2947 | 0.1763
Epoch 45/300, seasonal_0 Loss: 0.2548 | 0.2325
Epoch 46/300, seasonal_0 Loss: 0.3139 | 0.2426
Epoch 47/300, seasonal_0 Loss: 0.2727 | 0.1972
Epoch 48/300, seasonal_0 Loss: 0.2859 | 0.2760
Epoch 49/300, seasonal_0 Loss: 0.2650 | 0.2286
Epoch 50/300, seasonal_0 Loss: 0.2617 | 0.2202
Epoch 51/300, seasonal_0 Loss: 0.2501 | 0.3225
Epoch 52/300, seasonal_0 Loss: 0.2483 | 0.1901
Epoch 53/300, seasonal_0 Loss: 0.2030 | 0.2040
Epoch 54/300, seasonal_0 Loss: 0.1860 | 0.1800
Epoch 55/300, seasonal_0 Loss: 0.1771 | 0.1747
Epoch 56/300, seasonal_0 Loss: 0.1747 | 0.1692
Epoch 57/300, seasonal_0 Loss: 0.1708 | 0.1633
Epoch 58/300, seasonal_0 Loss: 0.1682 | 0.1698
Epoch 59/300, seasonal_0 Loss: 0.1660 | 0.1545
Epoch 60/300, seasonal_0 Loss: 0.1667 | 0.1690
Epoch 61/300, seasonal_0 Loss: 0.1659 | 0.1515
Epoch 62/300, seasonal_0 Loss: 0.1671 | 0.1665
Epoch 63/300, seasonal_0 Loss: 0.1649 | 0.1492
Epoch 64/300, seasonal_0 Loss: 0.1654 | 0.1587
Epoch 65/300, seasonal_0 Loss: 0.1593 | 0.1474
Epoch 66/300, seasonal_0 Loss: 0.1564 | 0.1476
Epoch 67/300, seasonal_0 Loss: 0.1531 | 0.1411
Epoch 68/300, seasonal_0 Loss: 0.1559 | 0.1454
Epoch 69/300, seasonal_0 Loss: 0.1553 | 0.1341
Epoch 70/300, seasonal_0 Loss: 0.1732 | 0.1473
Epoch 71/300, seasonal_0 Loss: 0.1807 | 0.1334
Epoch 72/300, seasonal_0 Loss: 0.2229 | 0.1640
Epoch 73/300, seasonal_0 Loss: 0.2064 | 0.1353
Epoch 74/300, seasonal_0 Loss: 0.1837 | 0.1840
Epoch 75/300, seasonal_0 Loss: 0.1626 | 0.1633
Epoch 76/300, seasonal_0 Loss: 0.1535 | 0.1511
Epoch 77/300, seasonal_0 Loss: 0.1547 | 0.1562
Epoch 78/300, seasonal_0 Loss: 0.1468 | 0.1448
Epoch 79/300, seasonal_0 Loss: 0.1395 | 0.1351
Epoch 80/300, seasonal_0 Loss: 0.1319 | 0.1299
Epoch 81/300, seasonal_0 Loss: 0.1313 | 0.1329
Epoch 82/300, seasonal_0 Loss: 0.1330 | 0.1302
Epoch 83/300, seasonal_0 Loss: 0.1355 | 0.1305
Epoch 84/300, seasonal_0 Loss: 0.1338 | 0.1166
Epoch 85/300, seasonal_0 Loss: 0.1283 | 0.1233
Epoch 86/300, seasonal_0 Loss: 0.1248 | 0.1096
Epoch 87/300, seasonal_0 Loss: 0.1245 | 0.1137
Epoch 88/300, seasonal_0 Loss: 0.1253 | 0.1122
Epoch 89/300, seasonal_0 Loss: 0.1339 | 0.1096
Epoch 90/300, seasonal_0 Loss: 0.1389 | 0.1134
Epoch 91/300, seasonal_0 Loss: 0.1391 | 0.1233
Epoch 92/300, seasonal_0 Loss: 0.1391 | 0.1061
Epoch 93/300, seasonal_0 Loss: 0.1469 | 0.1513
Epoch 94/300, seasonal_0 Loss: 0.1500 | 0.1156
Epoch 95/300, seasonal_0 Loss: 0.1561 | 0.1790
Epoch 96/300, seasonal_0 Loss: 0.1370 | 0.1035
Epoch 97/300, seasonal_0 Loss: 0.1369 | 0.1451
Epoch 98/300, seasonal_0 Loss: 0.1240 | 0.1039
Epoch 99/300, seasonal_0 Loss: 0.1165 | 0.1182
Epoch 100/300, seasonal_0 Loss: 0.1133 | 0.1025
Epoch 101/300, seasonal_0 Loss: 0.1115 | 0.1146
Epoch 102/300, seasonal_0 Loss: 0.1112 | 0.1013
Epoch 103/300, seasonal_0 Loss: 0.1076 | 0.1082
Epoch 104/300, seasonal_0 Loss: 0.1063 | 0.0955
Epoch 105/300, seasonal_0 Loss: 0.1045 | 0.1037
Epoch 106/300, seasonal_0 Loss: 0.1030 | 0.0932
Epoch 107/300, seasonal_0 Loss: 0.1014 | 0.1020
Epoch 108/300, seasonal_0 Loss: 0.1012 | 0.0907
Epoch 109/300, seasonal_0 Loss: 0.1032 | 0.1053
Epoch 110/300, seasonal_0 Loss: 0.1075 | 0.0883
Epoch 111/300, seasonal_0 Loss: 0.1127 | 0.1156
Epoch 112/300, seasonal_0 Loss: 0.1127 | 0.0962
Epoch 113/300, seasonal_0 Loss: 0.1190 | 0.1086
Epoch 114/300, seasonal_0 Loss: 0.1144 | 0.0982
Epoch 115/300, seasonal_0 Loss: 0.1087 | 0.1050
Epoch 116/300, seasonal_0 Loss: 0.1061 | 0.1006
Epoch 117/300, seasonal_0 Loss: 0.1074 | 0.1012
Epoch 118/300, seasonal_0 Loss: 0.1049 | 0.0965
Epoch 119/300, seasonal_0 Loss: 0.1043 | 0.0938
Epoch 120/300, seasonal_0 Loss: 0.1002 | 0.0898
Epoch 121/300, seasonal_0 Loss: 0.1000 | 0.0954
Epoch 122/300, seasonal_0 Loss: 0.1002 | 0.0843
Epoch 123/300, seasonal_0 Loss: 0.1000 | 0.1011
Epoch 124/300, seasonal_0 Loss: 0.1045 | 0.0819
Epoch 125/300, seasonal_0 Loss: 0.1059 | 0.1022
Epoch 126/300, seasonal_0 Loss: 0.1010 | 0.0800
Epoch 127/300, seasonal_0 Loss: 0.0987 | 0.1030
Epoch 128/300, seasonal_0 Loss: 0.0974 | 0.0827
Epoch 129/300, seasonal_0 Loss: 0.0988 | 0.0994
Epoch 130/300, seasonal_0 Loss: 0.0950 | 0.0856
Epoch 131/300, seasonal_0 Loss: 0.0937 | 0.0926
Epoch 132/300, seasonal_0 Loss: 0.0904 | 0.0848
Epoch 133/300, seasonal_0 Loss: 0.0903 | 0.0877
Epoch 134/300, seasonal_0 Loss: 0.0892 | 0.0826
Epoch 135/300, seasonal_0 Loss: 0.0891 | 0.0906
Epoch 136/300, seasonal_0 Loss: 0.0925 | 0.0834
Epoch 137/300, seasonal_0 Loss: 0.0915 | 0.0926
Epoch 138/300, seasonal_0 Loss: 0.0912 | 0.0790
Epoch 139/300, seasonal_0 Loss: 0.0898 | 0.0955
Epoch 140/300, seasonal_0 Loss: 0.0891 | 0.0806
Epoch 141/300, seasonal_0 Loss: 0.0876 | 0.0956
Epoch 142/300, seasonal_0 Loss: 0.0861 | 0.0814
Epoch 143/300, seasonal_0 Loss: 0.0862 | 0.0933
Epoch 144/300, seasonal_0 Loss: 0.0849 | 0.0814
Epoch 145/300, seasonal_0 Loss: 0.0864 | 0.0951
Epoch 146/300, seasonal_0 Loss: 0.0851 | 0.0783
Epoch 147/300, seasonal_0 Loss: 0.0850 | 0.0916
Epoch 148/300, seasonal_0 Loss: 0.0818 | 0.0789
Epoch 149/300, seasonal_0 Loss: 0.0812 | 0.0894
Epoch 150/300, seasonal_0 Loss: 0.0811 | 0.0753
Epoch 151/300, seasonal_0 Loss: 0.0812 | 0.0907
Epoch 152/300, seasonal_0 Loss: 0.0854 | 0.0783
Epoch 153/300, seasonal_0 Loss: 0.0834 | 0.0885
Epoch 154/300, seasonal_0 Loss: 0.0834 | 0.0809
Epoch 155/300, seasonal_0 Loss: 0.0815 | 0.0862
Epoch 156/300, seasonal_0 Loss: 0.0811 | 0.0796
Epoch 157/300, seasonal_0 Loss: 0.0798 | 0.0866
Epoch 158/300, seasonal_0 Loss: 0.0792 | 0.0810
Epoch 159/300, seasonal_0 Loss: 0.0805 | 0.0856
Epoch 160/300, seasonal_0 Loss: 0.0793 | 0.0818
Epoch 161/300, seasonal_0 Loss: 0.0799 | 0.0824
Epoch 162/300, seasonal_0 Loss: 0.0776 | 0.0797
Epoch 163/300, seasonal_0 Loss: 0.0749 | 0.0833
Epoch 164/300, seasonal_0 Loss: 0.0745 | 0.0794
Epoch 165/300, seasonal_0 Loss: 0.0744 | 0.0828
Epoch 166/300, seasonal_0 Loss: 0.0736 | 0.0782
Epoch 167/300, seasonal_0 Loss: 0.0741 | 0.0805
Epoch 168/300, seasonal_0 Loss: 0.0732 | 0.0807
Epoch 169/300, seasonal_0 Loss: 0.0730 | 0.0790
Epoch 170/300, seasonal_0 Loss: 0.0725 | 0.0804
Epoch 171/300, seasonal_0 Loss: 0.0713 | 0.0750
Epoch 172/300, seasonal_0 Loss: 0.0704 | 0.0787
Epoch 173/300, seasonal_0 Loss: 0.0716 | 0.0784
Epoch 174/300, seasonal_0 Loss: 0.0752 | 0.0793
Epoch 175/300, seasonal_0 Loss: 0.0845 | 0.0766
Epoch 176/300, seasonal_0 Loss: 0.0852 | 0.0809
Epoch 177/300, seasonal_0 Loss: 0.0813 | 0.0808
Epoch 178/300, seasonal_0 Loss: 0.0786 | 0.0783
Epoch 179/300, seasonal_0 Loss: 0.0763 | 0.0778
Epoch 180/300, seasonal_0 Loss: 0.0757 | 0.0804
Epoch 181/300, seasonal_0 Loss: 0.0759 | 0.0752
Epoch 182/300, seasonal_0 Loss: 0.0717 | 0.0838
Epoch 183/300, seasonal_0 Loss: 0.0715 | 0.0763
Epoch 184/300, seasonal_0 Loss: 0.0710 | 0.0775
Epoch 185/300, seasonal_0 Loss: 0.0719 | 0.0776
Epoch 186/300, seasonal_0 Loss: 0.0700 | 0.0730
Epoch 187/300, seasonal_0 Loss: 0.0688 | 0.0786
Epoch 188/300, seasonal_0 Loss: 0.0678 | 0.0722
Epoch 189/300, seasonal_0 Loss: 0.0686 | 0.0796
Epoch 190/300, seasonal_0 Loss: 0.0678 | 0.0715
Epoch 191/300, seasonal_0 Loss: 0.0680 | 0.0784
Epoch 192/300, seasonal_0 Loss: 0.0676 | 0.0730
Epoch 193/300, seasonal_0 Loss: 0.0682 | 0.0798
Epoch 194/300, seasonal_0 Loss: 0.0682 | 0.0759
Epoch 195/300, seasonal_0 Loss: 0.0693 | 0.0814
Epoch 196/300, seasonal_0 Loss: 0.0699 | 0.0755
Epoch 197/300, seasonal_0 Loss: 0.0707 | 0.0823
Epoch 198/300, seasonal_0 Loss: 0.0698 | 0.0753
Epoch 199/300, seasonal_0 Loss: 0.0702 | 0.0840
Epoch 200/300, seasonal_0 Loss: 0.0694 | 0.0746
Epoch 201/300, seasonal_0 Loss: 0.0703 | 0.0818
Epoch 202/300, seasonal_0 Loss: 0.0680 | 0.0770
Epoch 203/300, seasonal_0 Loss: 0.0678 | 0.0790
Epoch 204/300, seasonal_0 Loss: 0.0667 | 0.0764
Epoch 205/300, seasonal_0 Loss: 0.0662 | 0.0776
Epoch 206/300, seasonal_0 Loss: 0.0643 | 0.0756
Epoch 207/300, seasonal_0 Loss: 0.0637 | 0.0768
Epoch 208/300, seasonal_0 Loss: 0.0636 | 0.0770
Epoch 209/300, seasonal_0 Loss: 0.0648 | 0.0782
Epoch 210/300, seasonal_0 Loss: 0.0656 | 0.0758
Epoch 211/300, seasonal_0 Loss: 0.0625 | 0.0779
Epoch 212/300, seasonal_0 Loss: 0.0623 | 0.0758
Epoch 213/300, seasonal_0 Loss: 0.0624 | 0.0797
Epoch 214/300, seasonal_0 Loss: 0.0609 | 0.0756
Epoch 215/300, seasonal_0 Loss: 0.0608 | 0.0810
Epoch 216/300, seasonal_0 Loss: 0.0607 | 0.0733
Epoch 217/300, seasonal_0 Loss: 0.0609 | 0.0787
Epoch 218/300, seasonal_0 Loss: 0.0604 | 0.0744
Epoch 219/300, seasonal_0 Loss: 0.0612 | 0.0784
Epoch 220/300, seasonal_0 Loss: 0.0611 | 0.0740
Epoch 221/300, seasonal_0 Loss: 0.0587 | 0.0771
Epoch 222/300, seasonal_0 Loss: 0.0584 | 0.0747
Epoch 223/300, seasonal_0 Loss: 0.0583 | 0.0787
Epoch 224/300, seasonal_0 Loss: 0.0575 | 0.0742
Epoch 225/300, seasonal_0 Loss: 0.0572 | 0.0793
Epoch 226/300, seasonal_0 Loss: 0.0578 | 0.0735
Epoch 227/300, seasonal_0 Loss: 0.0570 | 0.0752
Epoch 228/300, seasonal_0 Loss: 0.0568 | 0.0748
Epoch 229/300, seasonal_0 Loss: 0.0566 | 0.0747
Epoch 230/300, seasonal_0 Loss: 0.0572 | 0.0800
Epoch 231/300, seasonal_0 Loss: 0.0574 | 0.0722
Epoch 232/300, seasonal_0 Loss: 0.0564 | 0.0791
Epoch 233/300, seasonal_0 Loss: 0.0566 | 0.0732
Epoch 234/300, seasonal_0 Loss: 0.0590 | 0.0771
Epoch 235/300, seasonal_0 Loss: 0.0559 | 0.0728
Epoch 236/300, seasonal_0 Loss: 0.0547 | 0.0755
Epoch 237/300, seasonal_0 Loss: 0.0539 | 0.0762
Epoch 238/300, seasonal_0 Loss: 0.0544 | 0.0743
Epoch 239/300, seasonal_0 Loss: 0.0543 | 0.0777
Epoch 240/300, seasonal_0 Loss: 0.0558 | 0.0764
Epoch 241/300, seasonal_0 Loss: 0.0543 | 0.0745
Epoch 242/300, seasonal_0 Loss: 0.0539 | 0.0777
Epoch 243/300, seasonal_0 Loss: 0.0541 | 0.0754
Epoch 244/300, seasonal_0 Loss: 0.0545 | 0.0763
Epoch 245/300, seasonal_0 Loss: 0.0563 | 0.0779
Epoch 246/300, seasonal_0 Loss: 0.0555 | 0.0757
Epoch 247/300, seasonal_0 Loss: 0.0547 | 0.0771
Epoch 248/300, seasonal_0 Loss: 0.0539 | 0.0771
Epoch 249/300, seasonal_0 Loss: 0.0545 | 0.0769
Epoch 250/300, seasonal_0 Loss: 0.0543 | 0.0780
Epoch 251/300, seasonal_0 Loss: 0.0531 | 0.0745
Epoch 252/300, seasonal_0 Loss: 0.0537 | 0.0743
Epoch 253/300, seasonal_0 Loss: 0.0585 | 0.0795
Epoch 254/300, seasonal_0 Loss: 0.0683 | 0.0753
Epoch 255/300, seasonal_0 Loss: 0.0586 | 0.0775
Epoch 256/300, seasonal_0 Loss: 0.0546 | 0.0760
Epoch 257/300, seasonal_0 Loss: 0.0535 | 0.0751
Epoch 258/300, seasonal_0 Loss: 0.0516 | 0.0762
Epoch 259/300, seasonal_0 Loss: 0.0516 | 0.0762
Epoch 260/300, seasonal_0 Loss: 0.0511 | 0.0743
Epoch 261/300, seasonal_0 Loss: 0.0505 | 0.0757
Epoch 262/300, seasonal_0 Loss: 0.0503 | 0.0752
Epoch 263/300, seasonal_0 Loss: 0.0506 | 0.0777
Epoch 264/300, seasonal_0 Loss: 0.0511 | 0.0742
Epoch 265/300, seasonal_0 Loss: 0.0515 | 0.0772
Epoch 266/300, seasonal_0 Loss: 0.0504 | 0.0718
Epoch 267/300, seasonal_0 Loss: 0.0507 | 0.0779
Epoch 268/300, seasonal_0 Loss: 0.0503 | 0.0714
Epoch 269/300, seasonal_0 Loss: 0.0505 | 0.0776
Epoch 270/300, seasonal_0 Loss: 0.0518 | 0.0739
Epoch 271/300, seasonal_0 Loss: 0.0503 | 0.0752
Epoch 272/300, seasonal_0 Loss: 0.0505 | 0.0748
Epoch 273/300, seasonal_0 Loss: 0.0509 | 0.0748
Epoch 274/300, seasonal_0 Loss: 0.0525 | 0.0764
Epoch 275/300, seasonal_0 Loss: 0.0503 | 0.0723
Epoch 276/300, seasonal_0 Loss: 0.0494 | 0.0766
Epoch 277/300, seasonal_0 Loss: 0.0493 | 0.0715
Epoch 278/300, seasonal_0 Loss: 0.0483 | 0.0762
Epoch 279/300, seasonal_0 Loss: 0.0494 | 0.0736
Epoch 280/300, seasonal_0 Loss: 0.0483 | 0.0757
Epoch 281/300, seasonal_0 Loss: 0.0471 | 0.0741
Epoch 282/300, seasonal_0 Loss: 0.0476 | 0.0734
Epoch 283/300, seasonal_0 Loss: 0.0471 | 0.0736
Epoch 284/300, seasonal_0 Loss: 0.0468 | 0.0758
Epoch 285/300, seasonal_0 Loss: 0.0479 | 0.0755
Epoch 286/300, seasonal_0 Loss: 0.0466 | 0.0760
Epoch 287/300, seasonal_0 Loss: 0.0471 | 0.0729
Epoch 288/300, seasonal_0 Loss: 0.0465 | 0.0728
Epoch 289/300, seasonal_0 Loss: 0.0461 | 0.0737
Epoch 290/300, seasonal_0 Loss: 0.0462 | 0.0719
Epoch 291/300, seasonal_0 Loss: 0.0462 | 0.0727
Epoch 292/300, seasonal_0 Loss: 0.0458 | 0.0748
Epoch 293/300, seasonal_0 Loss: 0.0473 | 0.0722
Epoch 294/300, seasonal_0 Loss: 0.0491 | 0.0764
Epoch 295/300, seasonal_0 Loss: 0.0472 | 0.0739
Epoch 296/300, seasonal_0 Loss: 0.0460 | 0.0719
Epoch 297/300, seasonal_0 Loss: 0.0449 | 0.0749
Epoch 298/300, seasonal_0 Loss: 0.0457 | 0.0748
Epoch 299/300, seasonal_0 Loss: 0.0475 | 0.0748
Epoch 300/300, seasonal_0 Loss: 0.0464 | 0.0725
Training seasonal_1 component with params: {'observation_period_num': 189, 'train_rates': 0.9896510651521607, 'learning_rate': 0.00016475626674858812, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9283936002496453}
Epoch 1/300, seasonal_1 Loss: 0.7650 | 0.7305
Epoch 2/300, seasonal_1 Loss: 0.6621 | 0.4638
Epoch 3/300, seasonal_1 Loss: 0.4940 | 0.4441
Epoch 4/300, seasonal_1 Loss: 0.4181 | 0.3775
Epoch 5/300, seasonal_1 Loss: 0.3845 | 0.3079
Epoch 6/300, seasonal_1 Loss: 0.4318 | 0.3682
Epoch 7/300, seasonal_1 Loss: 0.5095 | 0.3977
Epoch 8/300, seasonal_1 Loss: 0.3400 | 0.3103
Epoch 9/300, seasonal_1 Loss: 0.2961 | 0.3002
Epoch 10/300, seasonal_1 Loss: 0.2756 | 0.2325
Epoch 11/300, seasonal_1 Loss: 0.2520 | 0.2199
Epoch 12/300, seasonal_1 Loss: 0.2381 | 0.2057
Epoch 13/300, seasonal_1 Loss: 0.2204 | 0.1929
Epoch 14/300, seasonal_1 Loss: 0.2173 | 0.1800
Epoch 15/300, seasonal_1 Loss: 0.2152 | 0.1785
Epoch 16/300, seasonal_1 Loss: 0.2005 | 0.1674
Epoch 17/300, seasonal_1 Loss: 0.1992 | 0.1682
Epoch 18/300, seasonal_1 Loss: 0.1982 | 0.1635
Epoch 19/300, seasonal_1 Loss: 0.2162 | 0.1654
Epoch 20/300, seasonal_1 Loss: 0.2000 | 0.1515
Epoch 21/300, seasonal_1 Loss: 0.1850 | 0.1508
Epoch 22/300, seasonal_1 Loss: 0.1962 | 0.1517
Epoch 23/300, seasonal_1 Loss: 0.1740 | 0.1408
Epoch 24/300, seasonal_1 Loss: 0.1715 | 0.1433
Epoch 25/300, seasonal_1 Loss: 0.1638 | 0.1402
Epoch 26/300, seasonal_1 Loss: 0.1613 | 0.1365
Epoch 27/300, seasonal_1 Loss: 0.1603 | 0.1366
Epoch 28/300, seasonal_1 Loss: 0.1580 | 0.1339
Epoch 29/300, seasonal_1 Loss: 0.1567 | 0.1323
Epoch 30/300, seasonal_1 Loss: 0.1555 | 0.1313
Epoch 31/300, seasonal_1 Loss: 0.1542 | 0.1284
Epoch 32/300, seasonal_1 Loss: 0.1527 | 0.1277
Epoch 33/300, seasonal_1 Loss: 0.1515 | 0.1270
Epoch 34/300, seasonal_1 Loss: 0.1495 | 0.1249
Epoch 35/300, seasonal_1 Loss: 0.1480 | 0.1246
Epoch 36/300, seasonal_1 Loss: 0.1471 | 0.1231
Epoch 37/300, seasonal_1 Loss: 0.1470 | 0.1223
Epoch 38/300, seasonal_1 Loss: 0.1455 | 0.1214
Epoch 39/300, seasonal_1 Loss: 0.1439 | 0.1203
Epoch 40/300, seasonal_1 Loss: 0.1432 | 0.1199
Epoch 41/300, seasonal_1 Loss: 0.1427 | 0.1183
Epoch 42/300, seasonal_1 Loss: 0.1415 | 0.1173
Epoch 43/300, seasonal_1 Loss: 0.1413 | 0.1169
Epoch 44/300, seasonal_1 Loss: 0.1406 | 0.1160
Epoch 45/300, seasonal_1 Loss: 0.1397 | 0.1147
Epoch 46/300, seasonal_1 Loss: 0.1382 | 0.1147
Epoch 47/300, seasonal_1 Loss: 0.1380 | 0.1140
Epoch 48/300, seasonal_1 Loss: 0.1359 | 0.1129
Epoch 49/300, seasonal_1 Loss: 0.1356 | 0.1122
Epoch 50/300, seasonal_1 Loss: 0.1353 | 0.1122
Epoch 51/300, seasonal_1 Loss: 0.1347 | 0.1116
Epoch 52/300, seasonal_1 Loss: 0.1341 | 0.1115
Epoch 53/300, seasonal_1 Loss: 0.1337 | 0.1108
Epoch 54/300, seasonal_1 Loss: 0.1336 | 0.1104
Epoch 55/300, seasonal_1 Loss: 0.1324 | 0.1096
Epoch 56/300, seasonal_1 Loss: 0.1326 | 0.1095
Epoch 57/300, seasonal_1 Loss: 0.1313 | 0.1089
Epoch 58/300, seasonal_1 Loss: 0.1311 | 0.1088
Epoch 59/300, seasonal_1 Loss: 0.1318 | 0.1078
Epoch 60/300, seasonal_1 Loss: 0.1313 | 0.1079
Epoch 61/300, seasonal_1 Loss: 0.1312 | 0.1069
Epoch 62/300, seasonal_1 Loss: 0.1311 | 0.1070
Epoch 63/300, seasonal_1 Loss: 0.1316 | 0.1067
Epoch 64/300, seasonal_1 Loss: 0.1318 | 0.1065
Epoch 65/300, seasonal_1 Loss: 0.1314 | 0.1055
Epoch 66/300, seasonal_1 Loss: 0.1303 | 0.1058
Epoch 67/300, seasonal_1 Loss: 0.1297 | 0.1051
Epoch 68/300, seasonal_1 Loss: 0.1288 | 0.1050
Epoch 69/300, seasonal_1 Loss: 0.1274 | 0.1046
Epoch 70/300, seasonal_1 Loss: 0.1271 | 0.1048
Epoch 71/300, seasonal_1 Loss: 0.1267 | 0.1049
Epoch 72/300, seasonal_1 Loss: 0.1260 | 0.1041
Epoch 73/300, seasonal_1 Loss: 0.1264 | 0.1044
Epoch 74/300, seasonal_1 Loss: 0.1260 | 0.1038
Epoch 75/300, seasonal_1 Loss: 0.1253 | 0.1036
Epoch 76/300, seasonal_1 Loss: 0.1255 | 0.1037
Epoch 77/300, seasonal_1 Loss: 0.1249 | 0.1034
Epoch 78/300, seasonal_1 Loss: 0.1246 | 0.1031
Epoch 79/300, seasonal_1 Loss: 0.1243 | 0.1027
Epoch 80/300, seasonal_1 Loss: 0.1245 | 0.1020
Epoch 81/300, seasonal_1 Loss: 0.1241 | 0.1019
Epoch 82/300, seasonal_1 Loss: 0.1239 | 0.1019
Epoch 83/300, seasonal_1 Loss: 0.1238 | 0.1019
Epoch 84/300, seasonal_1 Loss: 0.1238 | 0.1016
Epoch 85/300, seasonal_1 Loss: 0.1234 | 0.1015
Epoch 86/300, seasonal_1 Loss: 0.1236 | 0.1012
Epoch 87/300, seasonal_1 Loss: 0.1231 | 0.1011
Epoch 88/300, seasonal_1 Loss: 0.1223 | 0.1010
Epoch 89/300, seasonal_1 Loss: 0.1228 | 0.1010
Epoch 90/300, seasonal_1 Loss: 0.1230 | 0.1009
Epoch 91/300, seasonal_1 Loss: 0.1223 | 0.1007
Epoch 92/300, seasonal_1 Loss: 0.1220 | 0.1003
Epoch 93/300, seasonal_1 Loss: 0.1223 | 0.1001
Epoch 94/300, seasonal_1 Loss: 0.1218 | 0.1004
Epoch 95/300, seasonal_1 Loss: 0.1222 | 0.1002
Epoch 96/300, seasonal_1 Loss: 0.1217 | 0.1002
Epoch 97/300, seasonal_1 Loss: 0.1216 | 0.1002
Epoch 98/300, seasonal_1 Loss: 0.1215 | 0.1000
Epoch 99/300, seasonal_1 Loss: 0.1211 | 0.0997
Epoch 100/300, seasonal_1 Loss: 0.1216 | 0.0993
Epoch 101/300, seasonal_1 Loss: 0.1211 | 0.0993
Epoch 102/300, seasonal_1 Loss: 0.1214 | 0.0993
Epoch 103/300, seasonal_1 Loss: 0.1209 | 0.0993
Epoch 104/300, seasonal_1 Loss: 0.1211 | 0.0993
Epoch 105/300, seasonal_1 Loss: 0.1213 | 0.0992
Epoch 106/300, seasonal_1 Loss: 0.1201 | 0.0992
Epoch 107/300, seasonal_1 Loss: 0.1206 | 0.0992
Epoch 108/300, seasonal_1 Loss: 0.1201 | 0.0989
Epoch 109/300, seasonal_1 Loss: 0.1203 | 0.0988
Epoch 110/300, seasonal_1 Loss: 0.1203 | 0.0987
Epoch 111/300, seasonal_1 Loss: 0.1200 | 0.0990
Epoch 112/300, seasonal_1 Loss: 0.1204 | 0.0989
Epoch 113/300, seasonal_1 Loss: 0.1202 | 0.0987
Epoch 114/300, seasonal_1 Loss: 0.1200 | 0.0988
Epoch 115/300, seasonal_1 Loss: 0.1199 | 0.0988
Epoch 116/300, seasonal_1 Loss: 0.1203 | 0.0986
Epoch 117/300, seasonal_1 Loss: 0.1200 | 0.0985
Epoch 118/300, seasonal_1 Loss: 0.1203 | 0.0985
Epoch 119/300, seasonal_1 Loss: 0.1200 | 0.0984
Epoch 120/300, seasonal_1 Loss: 0.1197 | 0.0985
Epoch 121/300, seasonal_1 Loss: 0.1198 | 0.0984
Epoch 122/300, seasonal_1 Loss: 0.1194 | 0.0984
Epoch 123/300, seasonal_1 Loss: 0.1200 | 0.0983
Epoch 124/300, seasonal_1 Loss: 0.1202 | 0.0983
Epoch 125/300, seasonal_1 Loss: 0.1196 | 0.0984
Epoch 126/300, seasonal_1 Loss: 0.1202 | 0.0983
Epoch 127/300, seasonal_1 Loss: 0.1197 | 0.0983
Epoch 128/300, seasonal_1 Loss: 0.1196 | 0.0982
Epoch 129/300, seasonal_1 Loss: 0.1189 | 0.0981
Epoch 130/300, seasonal_1 Loss: 0.1201 | 0.0981
Epoch 131/300, seasonal_1 Loss: 0.1195 | 0.0981
Epoch 132/300, seasonal_1 Loss: 0.1198 | 0.0981
Epoch 133/300, seasonal_1 Loss: 0.1193 | 0.0981
Epoch 134/300, seasonal_1 Loss: 0.1194 | 0.0981
Epoch 135/300, seasonal_1 Loss: 0.1191 | 0.0980
Epoch 136/300, seasonal_1 Loss: 0.1191 | 0.0979
Epoch 137/300, seasonal_1 Loss: 0.1187 | 0.0979
Epoch 138/300, seasonal_1 Loss: 0.1189 | 0.0978
Epoch 139/300, seasonal_1 Loss: 0.1193 | 0.0977
Epoch 140/300, seasonal_1 Loss: 0.1191 | 0.0978
Epoch 141/300, seasonal_1 Loss: 0.1185 | 0.0977
Epoch 142/300, seasonal_1 Loss: 0.1187 | 0.0976
Epoch 143/300, seasonal_1 Loss: 0.1195 | 0.0976
Epoch 144/300, seasonal_1 Loss: 0.1192 | 0.0977
Epoch 145/300, seasonal_1 Loss: 0.1187 | 0.0976
Epoch 146/300, seasonal_1 Loss: 0.1188 | 0.0976
Epoch 147/300, seasonal_1 Loss: 0.1189 | 0.0977
Epoch 148/300, seasonal_1 Loss: 0.1187 | 0.0977
Epoch 149/300, seasonal_1 Loss: 0.1188 | 0.0976
Epoch 150/300, seasonal_1 Loss: 0.1185 | 0.0976
Epoch 151/300, seasonal_1 Loss: 0.1186 | 0.0976
Epoch 152/300, seasonal_1 Loss: 0.1190 | 0.0976
Epoch 153/300, seasonal_1 Loss: 0.1183 | 0.0977
Epoch 154/300, seasonal_1 Loss: 0.1189 | 0.0977
Epoch 155/300, seasonal_1 Loss: 0.1187 | 0.0977
Epoch 156/300, seasonal_1 Loss: 0.1190 | 0.0976
Epoch 157/300, seasonal_1 Loss: 0.1186 | 0.0976
Epoch 158/300, seasonal_1 Loss: 0.1186 | 0.0976
Epoch 159/300, seasonal_1 Loss: 0.1187 | 0.0975
Epoch 160/300, seasonal_1 Loss: 0.1188 | 0.0975
Epoch 161/300, seasonal_1 Loss: 0.1183 | 0.0975
Epoch 162/300, seasonal_1 Loss: 0.1190 | 0.0975
Epoch 163/300, seasonal_1 Loss: 0.1182 | 0.0975
Epoch 164/300, seasonal_1 Loss: 0.1185 | 0.0975
Epoch 165/300, seasonal_1 Loss: 0.1187 | 0.0975
Epoch 166/300, seasonal_1 Loss: 0.1192 | 0.0974
Epoch 167/300, seasonal_1 Loss: 0.1187 | 0.0974
Epoch 168/300, seasonal_1 Loss: 0.1181 | 0.0974
Epoch 169/300, seasonal_1 Loss: 0.1183 | 0.0975
Epoch 170/300, seasonal_1 Loss: 0.1182 | 0.0974
Epoch 171/300, seasonal_1 Loss: 0.1183 | 0.0974
Epoch 172/300, seasonal_1 Loss: 0.1185 | 0.0974
Epoch 173/300, seasonal_1 Loss: 0.1190 | 0.0975
Epoch 174/300, seasonal_1 Loss: 0.1189 | 0.0974
Epoch 175/300, seasonal_1 Loss: 0.1176 | 0.0974
Epoch 176/300, seasonal_1 Loss: 0.1186 | 0.0974
Epoch 177/300, seasonal_1 Loss: 0.1185 | 0.0974
Epoch 178/300, seasonal_1 Loss: 0.1189 | 0.0974
Epoch 179/300, seasonal_1 Loss: 0.1186 | 0.0974
Epoch 180/300, seasonal_1 Loss: 0.1186 | 0.0974
Epoch 181/300, seasonal_1 Loss: 0.1183 | 0.0973
Epoch 182/300, seasonal_1 Loss: 0.1185 | 0.0973
Epoch 183/300, seasonal_1 Loss: 0.1176 | 0.0973
Epoch 184/300, seasonal_1 Loss: 0.1184 | 0.0973
Epoch 185/300, seasonal_1 Loss: 0.1182 | 0.0973
Epoch 186/300, seasonal_1 Loss: 0.1184 | 0.0973
Epoch 187/300, seasonal_1 Loss: 0.1188 | 0.0973
Epoch 188/300, seasonal_1 Loss: 0.1181 | 0.0973
Epoch 189/300, seasonal_1 Loss: 0.1186 | 0.0973
Epoch 190/300, seasonal_1 Loss: 0.1179 | 0.0973
Epoch 191/300, seasonal_1 Loss: 0.1186 | 0.0973
Epoch 192/300, seasonal_1 Loss: 0.1184 | 0.0973
Epoch 193/300, seasonal_1 Loss: 0.1184 | 0.0973
Epoch 194/300, seasonal_1 Loss: 0.1181 | 0.0973
Epoch 195/300, seasonal_1 Loss: 0.1183 | 0.0973
Epoch 196/300, seasonal_1 Loss: 0.1177 | 0.0973
Epoch 197/300, seasonal_1 Loss: 0.1185 | 0.0973
Epoch 198/300, seasonal_1 Loss: 0.1179 | 0.0973
Epoch 199/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 200/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 201/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 202/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 203/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 204/300, seasonal_1 Loss: 0.1188 | 0.0972
Epoch 205/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 206/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 207/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 208/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 209/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 210/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 211/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 212/300, seasonal_1 Loss: 0.1178 | 0.0972
Epoch 213/300, seasonal_1 Loss: 0.1189 | 0.0972
Epoch 214/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 215/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 216/300, seasonal_1 Loss: 0.1177 | 0.0972
Epoch 217/300, seasonal_1 Loss: 0.1188 | 0.0972
Epoch 218/300, seasonal_1 Loss: 0.1186 | 0.0972
Epoch 219/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 220/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 221/300, seasonal_1 Loss: 0.1177 | 0.0972
Epoch 222/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 223/300, seasonal_1 Loss: 0.1179 | 0.0972
Epoch 224/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 225/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 226/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 227/300, seasonal_1 Loss: 0.1179 | 0.0972
Epoch 228/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 229/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 230/300, seasonal_1 Loss: 0.1189 | 0.0972
Epoch 231/300, seasonal_1 Loss: 0.1188 | 0.0972
Epoch 232/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 233/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 234/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 235/300, seasonal_1 Loss: 0.1186 | 0.0972
Epoch 236/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 237/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 238/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 239/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 240/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 241/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 242/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 243/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 244/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 245/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 246/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 247/300, seasonal_1 Loss: 0.1186 | 0.0972
Epoch 248/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 249/300, seasonal_1 Loss: 0.1189 | 0.0972
Epoch 250/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 251/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 252/300, seasonal_1 Loss: 0.1186 | 0.0972
Epoch 253/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 254/300, seasonal_1 Loss: 0.1178 | 0.0972
Epoch 255/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 256/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 257/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 258/300, seasonal_1 Loss: 0.1188 | 0.0972
Epoch 259/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 260/300, seasonal_1 Loss: 0.1179 | 0.0972
Epoch 261/300, seasonal_1 Loss: 0.1178 | 0.0972
Epoch 262/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 263/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 264/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 265/300, seasonal_1 Loss: 0.1178 | 0.0972
Epoch 266/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 267/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 268/300, seasonal_1 Loss: 0.1179 | 0.0972
Epoch 269/300, seasonal_1 Loss: 0.1188 | 0.0972
Epoch 270/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 271/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 272/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 273/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 274/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 275/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 276/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 277/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 278/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 279/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 280/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 281/300, seasonal_1 Loss: 0.1184 | 0.0972
Epoch 282/300, seasonal_1 Loss: 0.1190 | 0.0972
Epoch 283/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 284/300, seasonal_1 Loss: 0.1185 | 0.0972
Epoch 285/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 286/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 287/300, seasonal_1 Loss: 0.1186 | 0.0972
Epoch 288/300, seasonal_1 Loss: 0.1182 | 0.0972
Epoch 289/300, seasonal_1 Loss: 0.1183 | 0.0972
Epoch 290/300, seasonal_1 Loss: 0.1180 | 0.0972
Epoch 291/300, seasonal_1 Loss: 0.1187 | 0.0972
Epoch 292/300, seasonal_1 Loss: 0.1181 | 0.0972
Epoch 293/300, seasonal_1 Loss: 0.1182 | 0.0972
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9835394424623525, 'learning_rate': 2.0536466288316713e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9264800637713873}
Epoch 1/300, seasonal_2 Loss: 0.7714 | 0.7782
Epoch 2/300, seasonal_2 Loss: 0.4976 | 0.5864
Epoch 3/300, seasonal_2 Loss: 0.3739 | 0.4858
Epoch 4/300, seasonal_2 Loss: 0.3053 | 0.4166
Epoch 5/300, seasonal_2 Loss: 0.2630 | 0.3654
Epoch 6/300, seasonal_2 Loss: 0.2368 | 0.3290
Epoch 7/300, seasonal_2 Loss: 0.2175 | 0.3019
Epoch 8/300, seasonal_2 Loss: 0.2026 | 0.2844
Epoch 9/300, seasonal_2 Loss: 0.1928 | 0.2684
Epoch 10/300, seasonal_2 Loss: 0.1845 | 0.2585
Epoch 11/300, seasonal_2 Loss: 0.1768 | 0.2384
Epoch 12/300, seasonal_2 Loss: 0.1706 | 0.2190
Epoch 13/300, seasonal_2 Loss: 0.1644 | 0.2054
Epoch 14/300, seasonal_2 Loss: 0.1604 | 0.1915
Epoch 15/300, seasonal_2 Loss: 0.1566 | 0.1821
Epoch 16/300, seasonal_2 Loss: 0.1541 | 0.1746
Epoch 17/300, seasonal_2 Loss: 0.1507 | 0.1709
Epoch 18/300, seasonal_2 Loss: 0.1475 | 0.1621
Epoch 19/300, seasonal_2 Loss: 0.1443 | 0.1592
Epoch 20/300, seasonal_2 Loss: 0.1432 | 0.1532
Epoch 21/300, seasonal_2 Loss: 0.1415 | 0.1501
Epoch 22/300, seasonal_2 Loss: 0.1371 | 0.1439
Epoch 23/300, seasonal_2 Loss: 0.1365 | 0.1399
Epoch 24/300, seasonal_2 Loss: 0.1359 | 0.1359
Epoch 25/300, seasonal_2 Loss: 0.1337 | 0.1384
Epoch 26/300, seasonal_2 Loss: 0.1301 | 0.1335
Epoch 27/300, seasonal_2 Loss: 0.1280 | 0.1318
Epoch 28/300, seasonal_2 Loss: 0.1272 | 0.1312
Epoch 29/300, seasonal_2 Loss: 0.1251 | 0.1263
Epoch 30/300, seasonal_2 Loss: 0.1237 | 0.1191
Epoch 31/300, seasonal_2 Loss: 0.1247 | 0.1183
Epoch 32/300, seasonal_2 Loss: 0.1216 | 0.1139
Epoch 33/300, seasonal_2 Loss: 0.1208 | 0.1115
Epoch 34/300, seasonal_2 Loss: 0.1185 | 0.1156
Epoch 35/300, seasonal_2 Loss: 0.1196 | 0.1239
Epoch 36/300, seasonal_2 Loss: 0.1194 | 0.1261
Epoch 37/300, seasonal_2 Loss: 0.1163 | 0.1147
Epoch 38/300, seasonal_2 Loss: 0.1157 | 0.1030
Epoch 39/300, seasonal_2 Loss: 0.1150 | 0.1020
Epoch 40/300, seasonal_2 Loss: 0.1100 | 0.1060
Epoch 41/300, seasonal_2 Loss: 0.1108 | 0.1056
Epoch 42/300, seasonal_2 Loss: 0.1094 | 0.0997
Epoch 43/300, seasonal_2 Loss: 0.1069 | 0.0952
Epoch 44/300, seasonal_2 Loss: 0.1083 | 0.0999
Epoch 45/300, seasonal_2 Loss: 0.1070 | 0.1025
Epoch 46/300, seasonal_2 Loss: 0.1045 | 0.0949
Epoch 47/300, seasonal_2 Loss: 0.1044 | 0.0962
Epoch 48/300, seasonal_2 Loss: 0.1039 | 0.0994
Epoch 49/300, seasonal_2 Loss: 0.1018 | 0.0863
Epoch 50/300, seasonal_2 Loss: 0.1008 | 0.0822
Epoch 51/300, seasonal_2 Loss: 0.1005 | 0.0836
Epoch 52/300, seasonal_2 Loss: 0.0988 | 0.0874
Epoch 53/300, seasonal_2 Loss: 0.0987 | 0.0882
Epoch 54/300, seasonal_2 Loss: 0.0980 | 0.0878
Epoch 55/300, seasonal_2 Loss: 0.0968 | 0.0846
Epoch 56/300, seasonal_2 Loss: 0.0957 | 0.0790
Epoch 57/300, seasonal_2 Loss: 0.0953 | 0.0798
Epoch 58/300, seasonal_2 Loss: 0.0937 | 0.0776
Epoch 59/300, seasonal_2 Loss: 0.0928 | 0.0770
Epoch 60/300, seasonal_2 Loss: 0.0931 | 0.0808
Epoch 61/300, seasonal_2 Loss: 0.0931 | 0.0799
Epoch 62/300, seasonal_2 Loss: 0.0907 | 0.0743
Epoch 63/300, seasonal_2 Loss: 0.0907 | 0.0711
Epoch 64/300, seasonal_2 Loss: 0.0902 | 0.0727
Epoch 65/300, seasonal_2 Loss: 0.0888 | 0.0732
Epoch 66/300, seasonal_2 Loss: 0.0892 | 0.0763
Epoch 67/300, seasonal_2 Loss: 0.0881 | 0.0744
Epoch 68/300, seasonal_2 Loss: 0.0864 | 0.0703
Epoch 69/300, seasonal_2 Loss: 0.0867 | 0.0677
Epoch 70/300, seasonal_2 Loss: 0.0861 | 0.0680
Epoch 71/300, seasonal_2 Loss: 0.0855 | 0.0684
Epoch 72/300, seasonal_2 Loss: 0.0848 | 0.0681
Epoch 73/300, seasonal_2 Loss: 0.0849 | 0.0696
Epoch 74/300, seasonal_2 Loss: 0.0837 | 0.0667
Epoch 75/300, seasonal_2 Loss: 0.0827 | 0.0646
Epoch 76/300, seasonal_2 Loss: 0.0825 | 0.0631
Epoch 77/300, seasonal_2 Loss: 0.0820 | 0.0627
Epoch 78/300, seasonal_2 Loss: 0.0818 | 0.0649
Epoch 79/300, seasonal_2 Loss: 0.0814 | 0.0674
Epoch 80/300, seasonal_2 Loss: 0.0814 | 0.0655
Epoch 81/300, seasonal_2 Loss: 0.0803 | 0.0619
Epoch 82/300, seasonal_2 Loss: 0.0804 | 0.0605
Epoch 83/300, seasonal_2 Loss: 0.0792 | 0.0614
Epoch 84/300, seasonal_2 Loss: 0.0794 | 0.0621
Epoch 85/300, seasonal_2 Loss: 0.0786 | 0.0630
Epoch 86/300, seasonal_2 Loss: 0.0789 | 0.0633
Epoch 87/300, seasonal_2 Loss: 0.0780 | 0.0611
Epoch 88/300, seasonal_2 Loss: 0.0780 | 0.0588
Epoch 89/300, seasonal_2 Loss: 0.0772 | 0.0581
Epoch 90/300, seasonal_2 Loss: 0.0774 | 0.0587
Epoch 91/300, seasonal_2 Loss: 0.0764 | 0.0603
Epoch 92/300, seasonal_2 Loss: 0.0766 | 0.0627
Epoch 93/300, seasonal_2 Loss: 0.0761 | 0.0601
Epoch 94/300, seasonal_2 Loss: 0.0759 | 0.0581
Epoch 95/300, seasonal_2 Loss: 0.0756 | 0.0579
Epoch 96/300, seasonal_2 Loss: 0.0746 | 0.0571
Epoch 97/300, seasonal_2 Loss: 0.0744 | 0.0566
Epoch 98/300, seasonal_2 Loss: 0.0742 | 0.0578
Epoch 99/300, seasonal_2 Loss: 0.0748 | 0.0591
Epoch 100/300, seasonal_2 Loss: 0.0737 | 0.0576
Epoch 101/300, seasonal_2 Loss: 0.0738 | 0.0560
Epoch 102/300, seasonal_2 Loss: 0.0735 | 0.0563
Epoch 103/300, seasonal_2 Loss: 0.0734 | 0.0565
Epoch 104/300, seasonal_2 Loss: 0.0732 | 0.0559
Epoch 105/300, seasonal_2 Loss: 0.0737 | 0.0568
Epoch 106/300, seasonal_2 Loss: 0.0724 | 0.0572
Epoch 107/300, seasonal_2 Loss: 0.0721 | 0.0548
Epoch 108/300, seasonal_2 Loss: 0.0723 | 0.0550
Epoch 109/300, seasonal_2 Loss: 0.0715 | 0.0546
Epoch 110/300, seasonal_2 Loss: 0.0714 | 0.0553
Epoch 111/300, seasonal_2 Loss: 0.0715 | 0.0557
Epoch 112/300, seasonal_2 Loss: 0.0712 | 0.0547
Epoch 113/300, seasonal_2 Loss: 0.0709 | 0.0541
Epoch 114/300, seasonal_2 Loss: 0.0708 | 0.0532
Epoch 115/300, seasonal_2 Loss: 0.0704 | 0.0531
Epoch 116/300, seasonal_2 Loss: 0.0701 | 0.0535
Epoch 117/300, seasonal_2 Loss: 0.0699 | 0.0545
Epoch 118/300, seasonal_2 Loss: 0.0698 | 0.0540
Epoch 119/300, seasonal_2 Loss: 0.0695 | 0.0537
Epoch 120/300, seasonal_2 Loss: 0.0685 | 0.0531
Epoch 121/300, seasonal_2 Loss: 0.0691 | 0.0531
Epoch 122/300, seasonal_2 Loss: 0.0690 | 0.0523
Epoch 123/300, seasonal_2 Loss: 0.0680 | 0.0535
Epoch 124/300, seasonal_2 Loss: 0.0680 | 0.0525
Epoch 125/300, seasonal_2 Loss: 0.0683 | 0.0522
Epoch 126/300, seasonal_2 Loss: 0.0677 | 0.0515
Epoch 127/300, seasonal_2 Loss: 0.0677 | 0.0514
Epoch 128/300, seasonal_2 Loss: 0.0668 | 0.0513
Epoch 129/300, seasonal_2 Loss: 0.0672 | 0.0521
Epoch 130/300, seasonal_2 Loss: 0.0676 | 0.0515
Epoch 131/300, seasonal_2 Loss: 0.0671 | 0.0513
Epoch 132/300, seasonal_2 Loss: 0.0666 | 0.0508
Epoch 133/300, seasonal_2 Loss: 0.0669 | 0.0504
Epoch 134/300, seasonal_2 Loss: 0.0667 | 0.0504
Epoch 135/300, seasonal_2 Loss: 0.0664 | 0.0507
Epoch 136/300, seasonal_2 Loss: 0.0663 | 0.0505
Epoch 137/300, seasonal_2 Loss: 0.0659 | 0.0505
Epoch 138/300, seasonal_2 Loss: 0.0660 | 0.0506
Epoch 139/300, seasonal_2 Loss: 0.0657 | 0.0508
Epoch 140/300, seasonal_2 Loss: 0.0653 | 0.0503
Epoch 141/300, seasonal_2 Loss: 0.0654 | 0.0500
Epoch 142/300, seasonal_2 Loss: 0.0648 | 0.0507
Epoch 143/300, seasonal_2 Loss: 0.0653 | 0.0503
Epoch 144/300, seasonal_2 Loss: 0.0647 | 0.0500
Epoch 145/300, seasonal_2 Loss: 0.0646 | 0.0496
Epoch 146/300, seasonal_2 Loss: 0.0646 | 0.0497
Epoch 147/300, seasonal_2 Loss: 0.0650 | 0.0497
Epoch 148/300, seasonal_2 Loss: 0.0643 | 0.0495
Epoch 149/300, seasonal_2 Loss: 0.0640 | 0.0495
Epoch 150/300, seasonal_2 Loss: 0.0648 | 0.0501
Epoch 151/300, seasonal_2 Loss: 0.0638 | 0.0490
Epoch 152/300, seasonal_2 Loss: 0.0641 | 0.0497
Epoch 153/300, seasonal_2 Loss: 0.0636 | 0.0498
Epoch 154/300, seasonal_2 Loss: 0.0629 | 0.0492
Epoch 155/300, seasonal_2 Loss: 0.0632 | 0.0495
Epoch 156/300, seasonal_2 Loss: 0.0632 | 0.0492
Epoch 157/300, seasonal_2 Loss: 0.0631 | 0.0495
Epoch 158/300, seasonal_2 Loss: 0.0629 | 0.0491
Epoch 159/300, seasonal_2 Loss: 0.0630 | 0.0485
Epoch 160/300, seasonal_2 Loss: 0.0632 | 0.0483
Epoch 161/300, seasonal_2 Loss: 0.0629 | 0.0488
Epoch 162/300, seasonal_2 Loss: 0.0627 | 0.0487
Epoch 163/300, seasonal_2 Loss: 0.0625 | 0.0483
Epoch 164/300, seasonal_2 Loss: 0.0623 | 0.0487
Epoch 165/300, seasonal_2 Loss: 0.0623 | 0.0484
Epoch 166/300, seasonal_2 Loss: 0.0620 | 0.0483
Epoch 167/300, seasonal_2 Loss: 0.0622 | 0.0482
Epoch 168/300, seasonal_2 Loss: 0.0622 | 0.0486
Epoch 169/300, seasonal_2 Loss: 0.0618 | 0.0485
Epoch 170/300, seasonal_2 Loss: 0.0619 | 0.0482
Epoch 171/300, seasonal_2 Loss: 0.0620 | 0.0484
Epoch 172/300, seasonal_2 Loss: 0.0621 | 0.0483
Epoch 173/300, seasonal_2 Loss: 0.0619 | 0.0484
Epoch 174/300, seasonal_2 Loss: 0.0613 | 0.0480
Epoch 175/300, seasonal_2 Loss: 0.0618 | 0.0485
Epoch 176/300, seasonal_2 Loss: 0.0614 | 0.0480
Epoch 177/300, seasonal_2 Loss: 0.0610 | 0.0481
Epoch 178/300, seasonal_2 Loss: 0.0612 | 0.0480
Epoch 179/300, seasonal_2 Loss: 0.0612 | 0.0481
Epoch 180/300, seasonal_2 Loss: 0.0613 | 0.0477
Epoch 181/300, seasonal_2 Loss: 0.0608 | 0.0478
Epoch 182/300, seasonal_2 Loss: 0.0606 | 0.0479
Epoch 183/300, seasonal_2 Loss: 0.0611 | 0.0476
Epoch 184/300, seasonal_2 Loss: 0.0612 | 0.0478
Epoch 185/300, seasonal_2 Loss: 0.0607 | 0.0479
Epoch 186/300, seasonal_2 Loss: 0.0605 | 0.0477
Epoch 187/300, seasonal_2 Loss: 0.0607 | 0.0476
Epoch 188/300, seasonal_2 Loss: 0.0605 | 0.0478
Epoch 189/300, seasonal_2 Loss: 0.0606 | 0.0477
Epoch 190/300, seasonal_2 Loss: 0.0601 | 0.0476
Epoch 191/300, seasonal_2 Loss: 0.0601 | 0.0475
Epoch 192/300, seasonal_2 Loss: 0.0605 | 0.0474
Epoch 193/300, seasonal_2 Loss: 0.0604 | 0.0473
Epoch 194/300, seasonal_2 Loss: 0.0604 | 0.0478
Epoch 195/300, seasonal_2 Loss: 0.0602 | 0.0474
Epoch 196/300, seasonal_2 Loss: 0.0599 | 0.0472
Epoch 197/300, seasonal_2 Loss: 0.0601 | 0.0468
Epoch 198/300, seasonal_2 Loss: 0.0597 | 0.0470
Epoch 199/300, seasonal_2 Loss: 0.0603 | 0.0471
Epoch 200/300, seasonal_2 Loss: 0.0600 | 0.0473
Epoch 201/300, seasonal_2 Loss: 0.0603 | 0.0474
Epoch 202/300, seasonal_2 Loss: 0.0597 | 0.0472
Epoch 203/300, seasonal_2 Loss: 0.0594 | 0.0472
Epoch 204/300, seasonal_2 Loss: 0.0600 | 0.0468
Epoch 205/300, seasonal_2 Loss: 0.0598 | 0.0464
Epoch 206/300, seasonal_2 Loss: 0.0594 | 0.0466
Epoch 207/300, seasonal_2 Loss: 0.0591 | 0.0469
Epoch 208/300, seasonal_2 Loss: 0.0594 | 0.0473
Epoch 209/300, seasonal_2 Loss: 0.0593 | 0.0470
Epoch 210/300, seasonal_2 Loss: 0.0593 | 0.0473
Epoch 211/300, seasonal_2 Loss: 0.0589 | 0.0471
Epoch 212/300, seasonal_2 Loss: 0.0588 | 0.0471
Epoch 213/300, seasonal_2 Loss: 0.0590 | 0.0470
Epoch 214/300, seasonal_2 Loss: 0.0591 | 0.0469
Epoch 215/300, seasonal_2 Loss: 0.0587 | 0.0467
Epoch 216/300, seasonal_2 Loss: 0.0590 | 0.0467
Epoch 217/300, seasonal_2 Loss: 0.0586 | 0.0464
Epoch 218/300, seasonal_2 Loss: 0.0590 | 0.0466
Epoch 219/300, seasonal_2 Loss: 0.0590 | 0.0468
Epoch 220/300, seasonal_2 Loss: 0.0586 | 0.0467
Epoch 221/300, seasonal_2 Loss: 0.0584 | 0.0464
Epoch 222/300, seasonal_2 Loss: 0.0587 | 0.0465
Epoch 223/300, seasonal_2 Loss: 0.0587 | 0.0467
Epoch 224/300, seasonal_2 Loss: 0.0590 | 0.0465
Epoch 225/300, seasonal_2 Loss: 0.0582 | 0.0465
Epoch 226/300, seasonal_2 Loss: 0.0583 | 0.0468
Epoch 227/300, seasonal_2 Loss: 0.0577 | 0.0463
Epoch 228/300, seasonal_2 Loss: 0.0581 | 0.0467
Epoch 229/300, seasonal_2 Loss: 0.0582 | 0.0468
Epoch 230/300, seasonal_2 Loss: 0.0584 | 0.0467
Epoch 231/300, seasonal_2 Loss: 0.0578 | 0.0464
Epoch 232/300, seasonal_2 Loss: 0.0582 | 0.0464
Epoch 233/300, seasonal_2 Loss: 0.0585 | 0.0467
Epoch 234/300, seasonal_2 Loss: 0.0583 | 0.0466
Epoch 235/300, seasonal_2 Loss: 0.0582 | 0.0463
Epoch 236/300, seasonal_2 Loss: 0.0582 | 0.0464
Epoch 237/300, seasonal_2 Loss: 0.0581 | 0.0463
Epoch 238/300, seasonal_2 Loss: 0.0579 | 0.0464
Epoch 239/300, seasonal_2 Loss: 0.0572 | 0.0464
Epoch 240/300, seasonal_2 Loss: 0.0578 | 0.0460
Epoch 241/300, seasonal_2 Loss: 0.0577 | 0.0459
Epoch 242/300, seasonal_2 Loss: 0.0575 | 0.0457
Epoch 243/300, seasonal_2 Loss: 0.0577 | 0.0459
Epoch 244/300, seasonal_2 Loss: 0.0576 | 0.0458
Epoch 245/300, seasonal_2 Loss: 0.0579 | 0.0459
Epoch 246/300, seasonal_2 Loss: 0.0579 | 0.0460
Epoch 247/300, seasonal_2 Loss: 0.0577 | 0.0459
Epoch 248/300, seasonal_2 Loss: 0.0575 | 0.0461
Epoch 249/300, seasonal_2 Loss: 0.0578 | 0.0455
Epoch 250/300, seasonal_2 Loss: 0.0573 | 0.0454
Epoch 251/300, seasonal_2 Loss: 0.0573 | 0.0458
Epoch 252/300, seasonal_2 Loss: 0.0573 | 0.0457
Epoch 253/300, seasonal_2 Loss: 0.0575 | 0.0460
Epoch 254/300, seasonal_2 Loss: 0.0576 | 0.0462
Epoch 255/300, seasonal_2 Loss: 0.0573 | 0.0460
Epoch 256/300, seasonal_2 Loss: 0.0568 | 0.0457
Epoch 257/300, seasonal_2 Loss: 0.0570 | 0.0458
Epoch 258/300, seasonal_2 Loss: 0.0570 | 0.0455
Epoch 259/300, seasonal_2 Loss: 0.0573 | 0.0453
Epoch 260/300, seasonal_2 Loss: 0.0575 | 0.0454
Epoch 261/300, seasonal_2 Loss: 0.0570 | 0.0453
Epoch 262/300, seasonal_2 Loss: 0.0575 | 0.0455
Epoch 263/300, seasonal_2 Loss: 0.0573 | 0.0455
Epoch 264/300, seasonal_2 Loss: 0.0571 | 0.0456
Epoch 265/300, seasonal_2 Loss: 0.0572 | 0.0456
Epoch 266/300, seasonal_2 Loss: 0.0570 | 0.0459
Epoch 267/300, seasonal_2 Loss: 0.0570 | 0.0458
Epoch 268/300, seasonal_2 Loss: 0.0565 | 0.0457
Epoch 269/300, seasonal_2 Loss: 0.0567 | 0.0457
Epoch 270/300, seasonal_2 Loss: 0.0566 | 0.0456
Epoch 271/300, seasonal_2 Loss: 0.0566 | 0.0455
Epoch 272/300, seasonal_2 Loss: 0.0571 | 0.0454
Epoch 273/300, seasonal_2 Loss: 0.0567 | 0.0455
Epoch 274/300, seasonal_2 Loss: 0.0570 | 0.0455
Epoch 275/300, seasonal_2 Loss: 0.0569 | 0.0457
Epoch 276/300, seasonal_2 Loss: 0.0568 | 0.0456
Epoch 277/300, seasonal_2 Loss: 0.0561 | 0.0453
Epoch 278/300, seasonal_2 Loss: 0.0565 | 0.0453
Epoch 279/300, seasonal_2 Loss: 0.0568 | 0.0455
Epoch 280/300, seasonal_2 Loss: 0.0569 | 0.0457
Epoch 281/300, seasonal_2 Loss: 0.0563 | 0.0453
Epoch 282/300, seasonal_2 Loss: 0.0567 | 0.0452
Epoch 283/300, seasonal_2 Loss: 0.0563 | 0.0453
Epoch 284/300, seasonal_2 Loss: 0.0563 | 0.0455
Epoch 285/300, seasonal_2 Loss: 0.0564 | 0.0456
Epoch 286/300, seasonal_2 Loss: 0.0563 | 0.0456
Epoch 287/300, seasonal_2 Loss: 0.0563 | 0.0455
Epoch 288/300, seasonal_2 Loss: 0.0560 | 0.0453
Epoch 289/300, seasonal_2 Loss: 0.0564 | 0.0457
Epoch 290/300, seasonal_2 Loss: 0.0566 | 0.0455
Epoch 291/300, seasonal_2 Loss: 0.0565 | 0.0457
Epoch 292/300, seasonal_2 Loss: 0.0565 | 0.0456
Epoch 293/300, seasonal_2 Loss: 0.0563 | 0.0456
Epoch 294/300, seasonal_2 Loss: 0.0563 | 0.0458
Epoch 295/300, seasonal_2 Loss: 0.0561 | 0.0455
Epoch 296/300, seasonal_2 Loss: 0.0560 | 0.0453
Epoch 297/300, seasonal_2 Loss: 0.0567 | 0.0454
Epoch 298/300, seasonal_2 Loss: 0.0564 | 0.0453
Epoch 299/300, seasonal_2 Loss: 0.0559 | 0.0453
Epoch 300/300, seasonal_2 Loss: 0.0560 | 0.0455
Training seasonal_3 component with params: {'observation_period_num': 194, 'train_rates': 0.9887844577819261, 'learning_rate': 0.00017196722218563954, 'batch_size': 178, 'step_size': 10, 'gamma': 0.9020895781537156}
Epoch 1/300, seasonal_3 Loss: 1.1030 | 1.4216
Epoch 2/300, seasonal_3 Loss: 0.7013 | 0.7987
Epoch 3/300, seasonal_3 Loss: 0.5724 | 0.6734
Epoch 4/300, seasonal_3 Loss: 0.4868 | 0.5997
Epoch 5/300, seasonal_3 Loss: 0.5031 | 0.5689
Epoch 6/300, seasonal_3 Loss: 0.4035 | 0.4666
Epoch 7/300, seasonal_3 Loss: 0.3644 | 0.4282
Epoch 8/300, seasonal_3 Loss: 0.3533 | 0.3932
Epoch 9/300, seasonal_3 Loss: 0.3958 | 0.3840
Epoch 10/300, seasonal_3 Loss: 0.3275 | 0.3831
Epoch 11/300, seasonal_3 Loss: 0.3513 | 0.3626
Epoch 12/300, seasonal_3 Loss: 0.2973 | 0.3307
Epoch 13/300, seasonal_3 Loss: 0.2795 | 0.2909
Epoch 14/300, seasonal_3 Loss: 0.2488 | 0.2633
Epoch 15/300, seasonal_3 Loss: 0.2584 | 0.2498
Epoch 16/300, seasonal_3 Loss: 0.2421 | 0.2480
Epoch 17/300, seasonal_3 Loss: 0.2458 | 0.2232
Epoch 18/300, seasonal_3 Loss: 0.2501 | 0.2185
Epoch 19/300, seasonal_3 Loss: 0.2472 | 0.2100
Epoch 20/300, seasonal_3 Loss: 0.2977 | 0.2394
Epoch 21/300, seasonal_3 Loss: 0.2493 | 0.2133
Epoch 22/300, seasonal_3 Loss: 0.2191 | 0.1986
Epoch 23/300, seasonal_3 Loss: 0.1958 | 0.1863
Epoch 24/300, seasonal_3 Loss: 0.1891 | 0.1776
Epoch 25/300, seasonal_3 Loss: 0.1899 | 0.1695
Epoch 26/300, seasonal_3 Loss: 0.1861 | 0.1690
Epoch 27/300, seasonal_3 Loss: 0.1851 | 0.1635
Epoch 28/300, seasonal_3 Loss: 0.1777 | 0.1616
Epoch 29/300, seasonal_3 Loss: 0.1747 | 0.1556
Epoch 30/300, seasonal_3 Loss: 0.1724 | 0.1564
Epoch 31/300, seasonal_3 Loss: 0.1720 | 0.1503
Epoch 32/300, seasonal_3 Loss: 0.1724 | 0.1513
Epoch 33/300, seasonal_3 Loss: 0.1700 | 0.1428
Epoch 34/300, seasonal_3 Loss: 0.1721 | 0.1437
Epoch 35/300, seasonal_3 Loss: 0.1693 | 0.1403
Epoch 36/300, seasonal_3 Loss: 0.1694 | 0.1398
Epoch 37/300, seasonal_3 Loss: 0.1635 | 0.1381
Epoch 38/300, seasonal_3 Loss: 0.1613 | 0.1368
Epoch 39/300, seasonal_3 Loss: 0.1589 | 0.1343
Epoch 40/300, seasonal_3 Loss: 0.1583 | 0.1334
Epoch 41/300, seasonal_3 Loss: 0.1583 | 0.1303
Epoch 42/300, seasonal_3 Loss: 0.1567 | 0.1305
Epoch 43/300, seasonal_3 Loss: 0.1575 | 0.1276
Epoch 44/300, seasonal_3 Loss: 0.1554 | 0.1286
Epoch 45/300, seasonal_3 Loss: 0.1550 | 0.1254
Epoch 46/300, seasonal_3 Loss: 0.1529 | 0.1274
Epoch 47/300, seasonal_3 Loss: 0.1513 | 0.1243
Epoch 48/300, seasonal_3 Loss: 0.1498 | 0.1239
Epoch 49/300, seasonal_3 Loss: 0.1500 | 0.1220
Epoch 50/300, seasonal_3 Loss: 0.1499 | 0.1209
Epoch 51/300, seasonal_3 Loss: 0.1499 | 0.1210
Epoch 52/300, seasonal_3 Loss: 0.1508 | 0.1197
Epoch 53/300, seasonal_3 Loss: 0.1502 | 0.1196
Epoch 54/300, seasonal_3 Loss: 0.1521 | 0.1182
Epoch 55/300, seasonal_3 Loss: 0.1520 | 0.1179
Epoch 56/300, seasonal_3 Loss: 0.1542 | 0.1177
Epoch 57/300, seasonal_3 Loss: 0.1526 | 0.1174
Epoch 58/300, seasonal_3 Loss: 0.1532 | 0.1169
Epoch 59/300, seasonal_3 Loss: 0.1539 | 0.1159
Epoch 60/300, seasonal_3 Loss: 0.1537 | 0.1172
Epoch 61/300, seasonal_3 Loss: 0.1512 | 0.1161
Epoch 62/300, seasonal_3 Loss: 0.1480 | 0.1165
Epoch 63/300, seasonal_3 Loss: 0.1456 | 0.1148
Epoch 64/300, seasonal_3 Loss: 0.1440 | 0.1153
Epoch 65/300, seasonal_3 Loss: 0.1433 | 0.1130
Epoch 66/300, seasonal_3 Loss: 0.1405 | 0.1130
Epoch 67/300, seasonal_3 Loss: 0.1396 | 0.1118
Epoch 68/300, seasonal_3 Loss: 0.1385 | 0.1122
Epoch 69/300, seasonal_3 Loss: 0.1379 | 0.1107
Epoch 70/300, seasonal_3 Loss: 0.1369 | 0.1110
Epoch 71/300, seasonal_3 Loss: 0.1364 | 0.1100
Epoch 72/300, seasonal_3 Loss: 0.1365 | 0.1095
Epoch 73/300, seasonal_3 Loss: 0.1361 | 0.1095
Epoch 74/300, seasonal_3 Loss: 0.1357 | 0.1089
Epoch 75/300, seasonal_3 Loss: 0.1348 | 0.1084
Epoch 76/300, seasonal_3 Loss: 0.1348 | 0.1075
Epoch 77/300, seasonal_3 Loss: 0.1343 | 0.1072
Epoch 78/300, seasonal_3 Loss: 0.1335 | 0.1068
Epoch 79/300, seasonal_3 Loss: 0.1337 | 0.1067
Epoch 80/300, seasonal_3 Loss: 0.1329 | 0.1065
Epoch 81/300, seasonal_3 Loss: 0.1328 | 0.1056
Epoch 82/300, seasonal_3 Loss: 0.1326 | 0.1061
Epoch 83/300, seasonal_3 Loss: 0.1321 | 0.1056
Epoch 84/300, seasonal_3 Loss: 0.1318 | 0.1057
Epoch 85/300, seasonal_3 Loss: 0.1310 | 0.1049
Epoch 86/300, seasonal_3 Loss: 0.1311 | 0.1049
Epoch 87/300, seasonal_3 Loss: 0.1302 | 0.1043
Epoch 88/300, seasonal_3 Loss: 0.1303 | 0.1044
Epoch 89/300, seasonal_3 Loss: 0.1304 | 0.1039
Epoch 90/300, seasonal_3 Loss: 0.1298 | 0.1033
Epoch 91/300, seasonal_3 Loss: 0.1297 | 0.1034
Epoch 92/300, seasonal_3 Loss: 0.1293 | 0.1031
Epoch 93/300, seasonal_3 Loss: 0.1288 | 0.1035
Epoch 94/300, seasonal_3 Loss: 0.1285 | 0.1031
Epoch 95/300, seasonal_3 Loss: 0.1282 | 0.1027
Epoch 96/300, seasonal_3 Loss: 0.1282 | 0.1023
Epoch 97/300, seasonal_3 Loss: 0.1280 | 0.1023
Epoch 98/300, seasonal_3 Loss: 0.1274 | 0.1026
Epoch 99/300, seasonal_3 Loss: 0.1275 | 0.1022
Epoch 100/300, seasonal_3 Loss: 0.1277 | 0.1018
Epoch 101/300, seasonal_3 Loss: 0.1269 | 0.1012
Epoch 102/300, seasonal_3 Loss: 0.1260 | 0.1013
Epoch 103/300, seasonal_3 Loss: 0.1266 | 0.1013
Epoch 104/300, seasonal_3 Loss: 0.1262 | 0.1011
Epoch 105/300, seasonal_3 Loss: 0.1256 | 0.1009
Epoch 106/300, seasonal_3 Loss: 0.1256 | 0.1007
Epoch 107/300, seasonal_3 Loss: 0.1256 | 0.1006
Epoch 108/300, seasonal_3 Loss: 0.1258 | 0.1004
Epoch 109/300, seasonal_3 Loss: 0.1252 | 0.1000
Epoch 110/300, seasonal_3 Loss: 0.1247 | 0.0998
Epoch 111/300, seasonal_3 Loss: 0.1242 | 0.1003
Epoch 112/300, seasonal_3 Loss: 0.1245 | 0.0997
Epoch 113/300, seasonal_3 Loss: 0.1242 | 0.0996
Epoch 114/300, seasonal_3 Loss: 0.1243 | 0.0989
Epoch 115/300, seasonal_3 Loss: 0.1242 | 0.0988
Epoch 116/300, seasonal_3 Loss: 0.1242 | 0.0992
Epoch 117/300, seasonal_3 Loss: 0.1235 | 0.0989
Epoch 118/300, seasonal_3 Loss: 0.1230 | 0.0991
Epoch 119/300, seasonal_3 Loss: 0.1230 | 0.0985
Epoch 120/300, seasonal_3 Loss: 0.1229 | 0.0990
Epoch 121/300, seasonal_3 Loss: 0.1228 | 0.0984
Epoch 122/300, seasonal_3 Loss: 0.1224 | 0.0985
Epoch 123/300, seasonal_3 Loss: 0.1230 | 0.0978
Epoch 124/300, seasonal_3 Loss: 0.1216 | 0.0978
Epoch 125/300, seasonal_3 Loss: 0.1221 | 0.0981
Epoch 126/300, seasonal_3 Loss: 0.1218 | 0.0981
Epoch 127/300, seasonal_3 Loss: 0.1219 | 0.0978
Epoch 128/300, seasonal_3 Loss: 0.1218 | 0.0974
Epoch 129/300, seasonal_3 Loss: 0.1221 | 0.0972
Epoch 130/300, seasonal_3 Loss: 0.1215 | 0.0974
Epoch 131/300, seasonal_3 Loss: 0.1213 | 0.0972
Epoch 132/300, seasonal_3 Loss: 0.1216 | 0.0968
Epoch 133/300, seasonal_3 Loss: 0.1216 | 0.0969
Epoch 134/300, seasonal_3 Loss: 0.1217 | 0.0968
Epoch 135/300, seasonal_3 Loss: 0.1215 | 0.0969
Epoch 136/300, seasonal_3 Loss: 0.1209 | 0.0969
Epoch 137/300, seasonal_3 Loss: 0.1203 | 0.0970
Epoch 138/300, seasonal_3 Loss: 0.1202 | 0.0965
Epoch 139/300, seasonal_3 Loss: 0.1203 | 0.0965
Epoch 140/300, seasonal_3 Loss: 0.1200 | 0.0962
Epoch 141/300, seasonal_3 Loss: 0.1202 | 0.0962
Epoch 142/300, seasonal_3 Loss: 0.1194 | 0.0967
Epoch 143/300, seasonal_3 Loss: 0.1196 | 0.0963
Epoch 144/300, seasonal_3 Loss: 0.1202 | 0.0963
Epoch 145/300, seasonal_3 Loss: 0.1195 | 0.0962
Epoch 146/300, seasonal_3 Loss: 0.1199 | 0.0964
Epoch 147/300, seasonal_3 Loss: 0.1202 | 0.0957
Epoch 148/300, seasonal_3 Loss: 0.1194 | 0.0956
Epoch 149/300, seasonal_3 Loss: 0.1194 | 0.0958
Epoch 150/300, seasonal_3 Loss: 0.1189 | 0.0958
Epoch 151/300, seasonal_3 Loss: 0.1192 | 0.0956
Epoch 152/300, seasonal_3 Loss: 0.1190 | 0.0956
Epoch 153/300, seasonal_3 Loss: 0.1185 | 0.0954
Epoch 154/300, seasonal_3 Loss: 0.1188 | 0.0951
Epoch 155/300, seasonal_3 Loss: 0.1191 | 0.0952
Epoch 156/300, seasonal_3 Loss: 0.1187 | 0.0952
Epoch 157/300, seasonal_3 Loss: 0.1185 | 0.0952
Epoch 158/300, seasonal_3 Loss: 0.1186 | 0.0950
Epoch 159/300, seasonal_3 Loss: 0.1183 | 0.0949
Epoch 160/300, seasonal_3 Loss: 0.1184 | 0.0949
Epoch 161/300, seasonal_3 Loss: 0.1182 | 0.0951
Epoch 162/300, seasonal_3 Loss: 0.1184 | 0.0948
Epoch 163/300, seasonal_3 Loss: 0.1179 | 0.0948
Epoch 164/300, seasonal_3 Loss: 0.1185 | 0.0950
Epoch 165/300, seasonal_3 Loss: 0.1183 | 0.0950
Epoch 166/300, seasonal_3 Loss: 0.1179 | 0.0948
Epoch 167/300, seasonal_3 Loss: 0.1178 | 0.0947
Epoch 168/300, seasonal_3 Loss: 0.1180 | 0.0951
Epoch 169/300, seasonal_3 Loss: 0.1175 | 0.0951
Epoch 170/300, seasonal_3 Loss: 0.1177 | 0.0948
Epoch 171/300, seasonal_3 Loss: 0.1181 | 0.0945
Epoch 172/300, seasonal_3 Loss: 0.1168 | 0.0943
Epoch 173/300, seasonal_3 Loss: 0.1168 | 0.0944
Epoch 174/300, seasonal_3 Loss: 0.1177 | 0.0945
Epoch 175/300, seasonal_3 Loss: 0.1176 | 0.0947
Epoch 176/300, seasonal_3 Loss: 0.1178 | 0.0944
Epoch 177/300, seasonal_3 Loss: 0.1173 | 0.0943
Epoch 178/300, seasonal_3 Loss: 0.1174 | 0.0941
Epoch 179/300, seasonal_3 Loss: 0.1175 | 0.0939
Epoch 180/300, seasonal_3 Loss: 0.1168 | 0.0939
Epoch 181/300, seasonal_3 Loss: 0.1172 | 0.0943
Epoch 182/300, seasonal_3 Loss: 0.1167 | 0.0943
Epoch 183/300, seasonal_3 Loss: 0.1162 | 0.0942
Epoch 184/300, seasonal_3 Loss: 0.1164 | 0.0943
Epoch 185/300, seasonal_3 Loss: 0.1163 | 0.0944
Epoch 186/300, seasonal_3 Loss: 0.1164 | 0.0941
Epoch 187/300, seasonal_3 Loss: 0.1171 | 0.0938
Epoch 188/300, seasonal_3 Loss: 0.1167 | 0.0938
Epoch 189/300, seasonal_3 Loss: 0.1163 | 0.0939
Epoch 190/300, seasonal_3 Loss: 0.1165 | 0.0941
Epoch 191/300, seasonal_3 Loss: 0.1166 | 0.0942
Epoch 192/300, seasonal_3 Loss: 0.1161 | 0.0939
Epoch 193/300, seasonal_3 Loss: 0.1158 | 0.0936
Epoch 194/300, seasonal_3 Loss: 0.1167 | 0.0936
Epoch 195/300, seasonal_3 Loss: 0.1162 | 0.0935
Epoch 196/300, seasonal_3 Loss: 0.1162 | 0.0934
Epoch 197/300, seasonal_3 Loss: 0.1164 | 0.0935
Epoch 198/300, seasonal_3 Loss: 0.1159 | 0.0935
Epoch 199/300, seasonal_3 Loss: 0.1159 | 0.0935
Epoch 200/300, seasonal_3 Loss: 0.1159 | 0.0933
Epoch 201/300, seasonal_3 Loss: 0.1162 | 0.0933
Epoch 202/300, seasonal_3 Loss: 0.1155 | 0.0935
Epoch 203/300, seasonal_3 Loss: 0.1165 | 0.0936
Epoch 204/300, seasonal_3 Loss: 0.1164 | 0.0932
Epoch 205/300, seasonal_3 Loss: 0.1163 | 0.0932
Epoch 206/300, seasonal_3 Loss: 0.1156 | 0.0933
Epoch 207/300, seasonal_3 Loss: 0.1160 | 0.0934
Epoch 208/300, seasonal_3 Loss: 0.1159 | 0.0932
Epoch 209/300, seasonal_3 Loss: 0.1156 | 0.0931
Epoch 210/300, seasonal_3 Loss: 0.1156 | 0.0932
Epoch 211/300, seasonal_3 Loss: 0.1158 | 0.0932
Epoch 212/300, seasonal_3 Loss: 0.1150 | 0.0932
Epoch 213/300, seasonal_3 Loss: 0.1156 | 0.0931
Epoch 214/300, seasonal_3 Loss: 0.1157 | 0.0931
Epoch 215/300, seasonal_3 Loss: 0.1156 | 0.0931
Epoch 216/300, seasonal_3 Loss: 0.1154 | 0.0930
Epoch 217/300, seasonal_3 Loss: 0.1163 | 0.0928
Epoch 218/300, seasonal_3 Loss: 0.1151 | 0.0926
Epoch 219/300, seasonal_3 Loss: 0.1152 | 0.0926
Epoch 220/300, seasonal_3 Loss: 0.1149 | 0.0927
Epoch 221/300, seasonal_3 Loss: 0.1155 | 0.0926
Epoch 222/300, seasonal_3 Loss: 0.1156 | 0.0927
Epoch 223/300, seasonal_3 Loss: 0.1161 | 0.0928
Epoch 224/300, seasonal_3 Loss: 0.1152 | 0.0928
Epoch 225/300, seasonal_3 Loss: 0.1155 | 0.0927
Epoch 226/300, seasonal_3 Loss: 0.1151 | 0.0927
Epoch 227/300, seasonal_3 Loss: 0.1148 | 0.0927
Epoch 228/300, seasonal_3 Loss: 0.1154 | 0.0926
Epoch 229/300, seasonal_3 Loss: 0.1155 | 0.0925
Epoch 230/300, seasonal_3 Loss: 0.1155 | 0.0926
Epoch 231/300, seasonal_3 Loss: 0.1148 | 0.0926
Epoch 232/300, seasonal_3 Loss: 0.1151 | 0.0926
Epoch 233/300, seasonal_3 Loss: 0.1153 | 0.0926
Epoch 234/300, seasonal_3 Loss: 0.1147 | 0.0926
Epoch 235/300, seasonal_3 Loss: 0.1151 | 0.0924
Epoch 236/300, seasonal_3 Loss: 0.1154 | 0.0924
Epoch 237/300, seasonal_3 Loss: 0.1151 | 0.0924
Epoch 238/300, seasonal_3 Loss: 0.1149 | 0.0924
Epoch 239/300, seasonal_3 Loss: 0.1149 | 0.0924
Epoch 240/300, seasonal_3 Loss: 0.1147 | 0.0924
Epoch 241/300, seasonal_3 Loss: 0.1150 | 0.0924
Epoch 242/300, seasonal_3 Loss: 0.1148 | 0.0925
Epoch 243/300, seasonal_3 Loss: 0.1149 | 0.0924
Epoch 244/300, seasonal_3 Loss: 0.1154 | 0.0924
Epoch 245/300, seasonal_3 Loss: 0.1152 | 0.0924
Epoch 246/300, seasonal_3 Loss: 0.1148 | 0.0926
Epoch 247/300, seasonal_3 Loss: 0.1150 | 0.0926
Epoch 248/300, seasonal_3 Loss: 0.1147 | 0.0926
Epoch 249/300, seasonal_3 Loss: 0.1146 | 0.0925
Epoch 250/300, seasonal_3 Loss: 0.1154 | 0.0925
Epoch 251/300, seasonal_3 Loss: 0.1149 | 0.0925
Epoch 252/300, seasonal_3 Loss: 0.1146 | 0.0925
Epoch 253/300, seasonal_3 Loss: 0.1147 | 0.0924
Epoch 254/300, seasonal_3 Loss: 0.1151 | 0.0923
Epoch 255/300, seasonal_3 Loss: 0.1143 | 0.0922
Epoch 256/300, seasonal_3 Loss: 0.1148 | 0.0922
Epoch 257/300, seasonal_3 Loss: 0.1149 | 0.0922
Epoch 258/300, seasonal_3 Loss: 0.1149 | 0.0922
Epoch 259/300, seasonal_3 Loss: 0.1149 | 0.0921
Epoch 260/300, seasonal_3 Loss: 0.1144 | 0.0922
Epoch 261/300, seasonal_3 Loss: 0.1148 | 0.0922
Epoch 262/300, seasonal_3 Loss: 0.1149 | 0.0922
Epoch 263/300, seasonal_3 Loss: 0.1145 | 0.0922
Epoch 264/300, seasonal_3 Loss: 0.1145 | 0.0922
Epoch 265/300, seasonal_3 Loss: 0.1152 | 0.0922
Epoch 266/300, seasonal_3 Loss: 0.1139 | 0.0921
Epoch 267/300, seasonal_3 Loss: 0.1147 | 0.0921
Epoch 268/300, seasonal_3 Loss: 0.1142 | 0.0921
Epoch 269/300, seasonal_3 Loss: 0.1144 | 0.0921
Epoch 270/300, seasonal_3 Loss: 0.1145 | 0.0922
Epoch 271/300, seasonal_3 Loss: 0.1146 | 0.0921
Epoch 272/300, seasonal_3 Loss: 0.1147 | 0.0922
Epoch 273/300, seasonal_3 Loss: 0.1139 | 0.0921
Epoch 274/300, seasonal_3 Loss: 0.1147 | 0.0921
Epoch 275/300, seasonal_3 Loss: 0.1143 | 0.0922
Epoch 276/300, seasonal_3 Loss: 0.1138 | 0.0922
Epoch 277/300, seasonal_3 Loss: 0.1141 | 0.0921
Epoch 278/300, seasonal_3 Loss: 0.1142 | 0.0922
Epoch 279/300, seasonal_3 Loss: 0.1147 | 0.0921
Epoch 280/300, seasonal_3 Loss: 0.1144 | 0.0921
Epoch 281/300, seasonal_3 Loss: 0.1144 | 0.0921
Epoch 282/300, seasonal_3 Loss: 0.1140 | 0.0921
Epoch 283/300, seasonal_3 Loss: 0.1146 | 0.0921
Epoch 284/300, seasonal_3 Loss: 0.1145 | 0.0921
Epoch 285/300, seasonal_3 Loss: 0.1145 | 0.0922
Epoch 286/300, seasonal_3 Loss: 0.1145 | 0.0922
Epoch 287/300, seasonal_3 Loss: 0.1147 | 0.0922
Epoch 288/300, seasonal_3 Loss: 0.1143 | 0.0922
Epoch 289/300, seasonal_3 Loss: 0.1147 | 0.0921
Epoch 290/300, seasonal_3 Loss: 0.1144 | 0.0921
Epoch 291/300, seasonal_3 Loss: 0.1143 | 0.0921
Epoch 292/300, seasonal_3 Loss: 0.1144 | 0.0920
Epoch 293/300, seasonal_3 Loss: 0.1149 | 0.0920
Epoch 294/300, seasonal_3 Loss: 0.1142 | 0.0920
Epoch 295/300, seasonal_3 Loss: 0.1143 | 0.0920
Epoch 296/300, seasonal_3 Loss: 0.1146 | 0.0921
Epoch 297/300, seasonal_3 Loss: 0.1142 | 0.0922
Epoch 298/300, seasonal_3 Loss: 0.1142 | 0.0922
Epoch 299/300, seasonal_3 Loss: 0.1137 | 0.0922
Epoch 300/300, seasonal_3 Loss: 0.1143 | 0.0922
Training resid component with params: {'observation_period_num': 194, 'train_rates': 0.9872053450553365, 'learning_rate': 6.113974617948114e-05, 'batch_size': 75, 'step_size': 15, 'gamma': 0.7520308118739937}
Epoch 1/300, resid Loss: 0.8996 | 1.0945
Epoch 2/300, resid Loss: 0.6021 | 0.7440
Epoch 3/300, resid Loss: 0.4368 | 0.6196
Epoch 4/300, resid Loss: 0.3774 | 0.4873
Epoch 5/300, resid Loss: 0.3391 | 0.4516
Epoch 6/300, resid Loss: 0.3277 | 0.3711
Epoch 7/300, resid Loss: 0.2975 | 0.3516
Epoch 8/300, resid Loss: 0.2988 | 0.3240
Epoch 9/300, resid Loss: 0.2830 | 0.3075
Epoch 10/300, resid Loss: 0.2493 | 0.2843
Epoch 11/300, resid Loss: 0.2468 | 0.2685
Epoch 12/300, resid Loss: 0.2429 | 0.2599
Epoch 13/300, resid Loss: 0.2293 | 0.2378
Epoch 14/300, resid Loss: 0.2244 | 0.2379
Epoch 15/300, resid Loss: 0.2175 | 0.2213
Epoch 16/300, resid Loss: 0.2022 | 0.2137
Epoch 17/300, resid Loss: 0.1951 | 0.2082
Epoch 18/300, resid Loss: 0.1969 | 0.2022
Epoch 19/300, resid Loss: 0.1916 | 0.1945
Epoch 20/300, resid Loss: 0.1871 | 0.1931
Epoch 21/300, resid Loss: 0.1814 | 0.1858
Epoch 22/300, resid Loss: 0.1785 | 0.1816
Epoch 23/300, resid Loss: 0.1763 | 0.1759
Epoch 24/300, resid Loss: 0.1739 | 0.1749
Epoch 25/300, resid Loss: 0.1701 | 0.1717
Epoch 26/300, resid Loss: 0.1678 | 0.1676
Epoch 27/300, resid Loss: 0.1671 | 0.1650
Epoch 28/300, resid Loss: 0.1670 | 0.1614
Epoch 29/300, resid Loss: 0.1643 | 0.1611
Epoch 30/300, resid Loss: 0.1644 | 0.1583
Epoch 31/300, resid Loss: 0.1652 | 0.1548
Epoch 32/300, resid Loss: 0.1634 | 0.1534
Epoch 33/300, resid Loss: 0.1614 | 0.1508
Epoch 34/300, resid Loss: 0.1606 | 0.1499
Epoch 35/300, resid Loss: 0.1594 | 0.1481
Epoch 36/300, resid Loss: 0.1583 | 0.1466
Epoch 37/300, resid Loss: 0.1576 | 0.1453
Epoch 38/300, resid Loss: 0.1578 | 0.1434
Epoch 39/300, resid Loss: 0.1555 | 0.1418
Epoch 40/300, resid Loss: 0.1550 | 0.1410
Epoch 41/300, resid Loss: 0.1534 | 0.1405
Epoch 42/300, resid Loss: 0.1526 | 0.1391
Epoch 43/300, resid Loss: 0.1512 | 0.1377
Epoch 44/300, resid Loss: 0.1516 | 0.1370
Epoch 45/300, resid Loss: 0.1508 | 0.1357
Epoch 46/300, resid Loss: 0.1508 | 0.1355
Epoch 47/300, resid Loss: 0.1496 | 0.1345
Epoch 48/300, resid Loss: 0.1497 | 0.1337
Epoch 49/300, resid Loss: 0.1497 | 0.1326
Epoch 50/300, resid Loss: 0.1486 | 0.1319
Epoch 51/300, resid Loss: 0.1488 | 0.1310
Epoch 52/300, resid Loss: 0.1482 | 0.1309
Epoch 53/300, resid Loss: 0.1479 | 0.1298
Epoch 54/300, resid Loss: 0.1471 | 0.1293
Epoch 55/300, resid Loss: 0.1465 | 0.1289
Epoch 56/300, resid Loss: 0.1464 | 0.1283
Epoch 57/300, resid Loss: 0.1464 | 0.1274
Epoch 58/300, resid Loss: 0.1458 | 0.1271
Epoch 59/300, resid Loss: 0.1455 | 0.1267
Epoch 60/300, resid Loss: 0.1460 | 0.1262
Epoch 61/300, resid Loss: 0.1449 | 0.1256
Epoch 62/300, resid Loss: 0.1447 | 0.1253
Epoch 63/300, resid Loss: 0.1450 | 0.1249
Epoch 64/300, resid Loss: 0.1450 | 0.1244
Epoch 65/300, resid Loss: 0.1442 | 0.1242
Epoch 66/300, resid Loss: 0.1447 | 0.1239
Epoch 67/300, resid Loss: 0.1441 | 0.1234
Epoch 68/300, resid Loss: 0.1439 | 0.1230
Epoch 69/300, resid Loss: 0.1436 | 0.1227
Epoch 70/300, resid Loss: 0.1431 | 0.1222
Epoch 71/300, resid Loss: 0.1432 | 0.1218
Epoch 72/300, resid Loss: 0.1430 | 0.1217
Epoch 73/300, resid Loss: 0.1422 | 0.1211
Epoch 74/300, resid Loss: 0.1427 | 0.1211
Epoch 75/300, resid Loss: 0.1427 | 0.1207
Epoch 76/300, resid Loss: 0.1420 | 0.1204
Epoch 77/300, resid Loss: 0.1420 | 0.1203
Epoch 78/300, resid Loss: 0.1421 | 0.1200
Epoch 79/300, resid Loss: 0.1417 | 0.1197
Epoch 80/300, resid Loss: 0.1421 | 0.1198
Epoch 81/300, resid Loss: 0.1414 | 0.1199
Epoch 82/300, resid Loss: 0.1415 | 0.1192
Epoch 83/300, resid Loss: 0.1415 | 0.1192
Epoch 84/300, resid Loss: 0.1408 | 0.1191
Epoch 85/300, resid Loss: 0.1411 | 0.1189
Epoch 86/300, resid Loss: 0.1412 | 0.1187
Epoch 87/300, resid Loss: 0.1410 | 0.1186
Epoch 88/300, resid Loss: 0.1407 | 0.1181
Epoch 89/300, resid Loss: 0.1407 | 0.1180
Epoch 90/300, resid Loss: 0.1412 | 0.1180
Epoch 91/300, resid Loss: 0.1404 | 0.1179
Epoch 92/300, resid Loss: 0.1400 | 0.1176
Epoch 93/300, resid Loss: 0.1408 | 0.1173
Epoch 94/300, resid Loss: 0.1404 | 0.1173
Epoch 95/300, resid Loss: 0.1400 | 0.1171
Epoch 96/300, resid Loss: 0.1396 | 0.1173
Epoch 97/300, resid Loss: 0.1403 | 0.1171
Epoch 98/300, resid Loss: 0.1396 | 0.1169
Epoch 99/300, resid Loss: 0.1391 | 0.1167
Epoch 100/300, resid Loss: 0.1391 | 0.1166
Epoch 101/300, resid Loss: 0.1396 | 0.1164
Epoch 102/300, resid Loss: 0.1400 | 0.1164
Epoch 103/300, resid Loss: 0.1403 | 0.1163
Epoch 104/300, resid Loss: 0.1394 | 0.1163
Epoch 105/300, resid Loss: 0.1393 | 0.1162
Epoch 106/300, resid Loss: 0.1390 | 0.1160
Epoch 107/300, resid Loss: 0.1391 | 0.1160
Epoch 108/300, resid Loss: 0.1392 | 0.1160
Epoch 109/300, resid Loss: 0.1396 | 0.1158
Epoch 110/300, resid Loss: 0.1394 | 0.1157
Epoch 111/300, resid Loss: 0.1388 | 0.1156
Epoch 112/300, resid Loss: 0.1388 | 0.1154
Epoch 113/300, resid Loss: 0.1387 | 0.1153
Epoch 114/300, resid Loss: 0.1396 | 0.1152
Epoch 115/300, resid Loss: 0.1394 | 0.1152
Epoch 116/300, resid Loss: 0.1387 | 0.1152
Epoch 117/300, resid Loss: 0.1391 | 0.1150
Epoch 118/300, resid Loss: 0.1389 | 0.1151
Epoch 119/300, resid Loss: 0.1388 | 0.1150
Epoch 120/300, resid Loss: 0.1390 | 0.1149
Epoch 121/300, resid Loss: 0.1384 | 0.1149
Epoch 122/300, resid Loss: 0.1384 | 0.1148
Epoch 123/300, resid Loss: 0.1389 | 0.1149
Epoch 124/300, resid Loss: 0.1381 | 0.1148
Epoch 125/300, resid Loss: 0.1386 | 0.1148
Epoch 126/300, resid Loss: 0.1387 | 0.1146
Epoch 127/300, resid Loss: 0.1384 | 0.1146
Epoch 128/300, resid Loss: 0.1386 | 0.1147
Epoch 129/300, resid Loss: 0.1382 | 0.1146
Epoch 130/300, resid Loss: 0.1386 | 0.1145
Epoch 131/300, resid Loss: 0.1390 | 0.1145
Epoch 132/300, resid Loss: 0.1383 | 0.1145
Epoch 133/300, resid Loss: 0.1387 | 0.1146
Epoch 134/300, resid Loss: 0.1386 | 0.1146
Epoch 135/300, resid Loss: 0.1386 | 0.1145
Epoch 136/300, resid Loss: 0.1382 | 0.1144
Epoch 137/300, resid Loss: 0.1386 | 0.1144
Epoch 138/300, resid Loss: 0.1376 | 0.1143
Epoch 139/300, resid Loss: 0.1382 | 0.1143
Epoch 140/300, resid Loss: 0.1384 | 0.1143
Epoch 141/300, resid Loss: 0.1383 | 0.1143
Epoch 142/300, resid Loss: 0.1382 | 0.1142
Epoch 143/300, resid Loss: 0.1379 | 0.1141
Epoch 144/300, resid Loss: 0.1381 | 0.1141
Epoch 145/300, resid Loss: 0.1384 | 0.1141
Epoch 146/300, resid Loss: 0.1383 | 0.1140
Epoch 147/300, resid Loss: 0.1382 | 0.1140
Epoch 148/300, resid Loss: 0.1380 | 0.1140
Epoch 149/300, resid Loss: 0.1378 | 0.1140
Epoch 150/300, resid Loss: 0.1376 | 0.1140
Epoch 151/300, resid Loss: 0.1381 | 0.1140
Epoch 152/300, resid Loss: 0.1380 | 0.1140
Epoch 153/300, resid Loss: 0.1376 | 0.1140
Epoch 154/300, resid Loss: 0.1379 | 0.1139
Epoch 155/300, resid Loss: 0.1384 | 0.1139
Epoch 156/300, resid Loss: 0.1377 | 0.1139
Epoch 157/300, resid Loss: 0.1385 | 0.1139
Epoch 158/300, resid Loss: 0.1381 | 0.1139
Epoch 159/300, resid Loss: 0.1377 | 0.1139
Epoch 160/300, resid Loss: 0.1382 | 0.1139
Epoch 161/300, resid Loss: 0.1377 | 0.1138
Epoch 162/300, resid Loss: 0.1385 | 0.1138
Epoch 163/300, resid Loss: 0.1379 | 0.1138
Epoch 164/300, resid Loss: 0.1380 | 0.1138
Epoch 165/300, resid Loss: 0.1372 | 0.1138
Epoch 166/300, resid Loss: 0.1374 | 0.1138
Epoch 167/300, resid Loss: 0.1381 | 0.1137
Epoch 168/300, resid Loss: 0.1379 | 0.1137
Epoch 169/300, resid Loss: 0.1378 | 0.1137
Epoch 170/300, resid Loss: 0.1378 | 0.1137
Epoch 171/300, resid Loss: 0.1380 | 0.1137
Epoch 172/300, resid Loss: 0.1380 | 0.1137
Epoch 173/300, resid Loss: 0.1375 | 0.1137
Epoch 174/300, resid Loss: 0.1377 | 0.1137
Epoch 175/300, resid Loss: 0.1369 | 0.1137
Epoch 176/300, resid Loss: 0.1380 | 0.1137
Epoch 177/300, resid Loss: 0.1378 | 0.1137
Epoch 178/300, resid Loss: 0.1375 | 0.1137
Epoch 179/300, resid Loss: 0.1372 | 0.1137
Epoch 180/300, resid Loss: 0.1381 | 0.1137
Epoch 181/300, resid Loss: 0.1376 | 0.1137
Epoch 182/300, resid Loss: 0.1381 | 0.1136
Epoch 183/300, resid Loss: 0.1376 | 0.1136
Epoch 184/300, resid Loss: 0.1376 | 0.1136
Epoch 185/300, resid Loss: 0.1374 | 0.1136
Epoch 186/300, resid Loss: 0.1378 | 0.1136
Epoch 187/300, resid Loss: 0.1375 | 0.1136
Epoch 188/300, resid Loss: 0.1377 | 0.1136
Epoch 189/300, resid Loss: 0.1376 | 0.1136
Epoch 190/300, resid Loss: 0.1373 | 0.1136
Epoch 191/300, resid Loss: 0.1379 | 0.1136
Epoch 192/300, resid Loss: 0.1383 | 0.1136
Epoch 193/300, resid Loss: 0.1379 | 0.1136
Epoch 194/300, resid Loss: 0.1373 | 0.1136
Epoch 195/300, resid Loss: 0.1381 | 0.1136
Epoch 196/300, resid Loss: 0.1377 | 0.1136
Epoch 197/300, resid Loss: 0.1374 | 0.1136
Epoch 198/300, resid Loss: 0.1376 | 0.1136
Epoch 199/300, resid Loss: 0.1382 | 0.1136
Epoch 200/300, resid Loss: 0.1378 | 0.1136
Epoch 201/300, resid Loss: 0.1384 | 0.1136
Epoch 202/300, resid Loss: 0.1371 | 0.1136
Epoch 203/300, resid Loss: 0.1374 | 0.1136
Epoch 204/300, resid Loss: 0.1369 | 0.1136
Epoch 205/300, resid Loss: 0.1381 | 0.1136
Epoch 206/300, resid Loss: 0.1376 | 0.1136
Epoch 207/300, resid Loss: 0.1379 | 0.1136
Epoch 208/300, resid Loss: 0.1376 | 0.1136
Epoch 209/300, resid Loss: 0.1374 | 0.1136
Epoch 210/300, resid Loss: 0.1378 | 0.1136
Epoch 211/300, resid Loss: 0.1378 | 0.1136
Epoch 212/300, resid Loss: 0.1381 | 0.1136
Epoch 213/300, resid Loss: 0.1368 | 0.1135
Epoch 214/300, resid Loss: 0.1374 | 0.1135
Epoch 215/300, resid Loss: 0.1372 | 0.1135
Epoch 216/300, resid Loss: 0.1370 | 0.1135
Epoch 217/300, resid Loss: 0.1380 | 0.1135
Epoch 218/300, resid Loss: 0.1373 | 0.1135
Epoch 219/300, resid Loss: 0.1377 | 0.1135
Epoch 220/300, resid Loss: 0.1371 | 0.1135
Epoch 221/300, resid Loss: 0.1376 | 0.1135
Epoch 222/300, resid Loss: 0.1379 | 0.1135
Epoch 223/300, resid Loss: 0.1377 | 0.1135
Epoch 224/300, resid Loss: 0.1377 | 0.1135
Epoch 225/300, resid Loss: 0.1376 | 0.1135
Epoch 226/300, resid Loss: 0.1374 | 0.1135
Epoch 227/300, resid Loss: 0.1377 | 0.1135
Epoch 228/300, resid Loss: 0.1378 | 0.1135
Epoch 229/300, resid Loss: 0.1373 | 0.1135
Epoch 230/300, resid Loss: 0.1373 | 0.1135
Epoch 231/300, resid Loss: 0.1383 | 0.1135
Epoch 232/300, resid Loss: 0.1379 | 0.1135
Epoch 233/300, resid Loss: 0.1380 | 0.1135
Epoch 234/300, resid Loss: 0.1370 | 0.1135
Epoch 235/300, resid Loss: 0.1381 | 0.1135
Epoch 236/300, resid Loss: 0.1382 | 0.1135
Epoch 237/300, resid Loss: 0.1369 | 0.1135
Epoch 238/300, resid Loss: 0.1378 | 0.1135
Epoch 239/300, resid Loss: 0.1378 | 0.1135
Epoch 240/300, resid Loss: 0.1381 | 0.1135
Early stopping for resid
Runtime (seconds): 4326.564239025116
0.00010165307808629653
[216.44194]
[1.7781004]
[1.0997725]
[5.1665215]
[-1.4994448]
[5.58085]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 18.509321451187134
RMSE: 4.30224609375
MAE: 4.30224609375
R-squared: nan
[228.56775]
File aapl_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
