ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 05:21:23,546][0m A new study created in memory with name: no-name-edc4c019-8299-49bc-9ed6-946eb06c74e3[0m
[32m[I 2025-01-07 05:21:57,369][0m Trial 0 finished with value: 0.0956356307802101 and parameters: {'observation_period_num': 58, 'train_rates': 0.834323058629991, 'learning_rate': 0.00019860896840278086, 'batch_size': 182, 'step_size': 5, 'gamma': 0.8723397719526462}. Best is trial 0 with value: 0.0956356307802101.[0m
[32m[I 2025-01-07 05:23:06,644][0m Trial 1 finished with value: 0.1000079509572989 and parameters: {'observation_period_num': 134, 'train_rates': 0.8646656736689198, 'learning_rate': 0.00037042819923876206, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9191530738986895}. Best is trial 0 with value: 0.0956356307802101.[0m
[32m[I 2025-01-07 05:24:54,925][0m Trial 2 finished with value: 0.7391584383845329 and parameters: {'observation_period_num': 115, 'train_rates': 0.6477319871093605, 'learning_rate': 3.6078711737355445e-06, 'batch_size': 41, 'step_size': 2, 'gamma': 0.8096634747121636}. Best is trial 0 with value: 0.0956356307802101.[0m
[32m[I 2025-01-07 05:26:42,927][0m Trial 3 finished with value: 0.05096358491801748 and parameters: {'observation_period_num': 43, 'train_rates': 0.9474925298215884, 'learning_rate': 0.0009901312953223486, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9272724731802165}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:27:36,856][0m Trial 4 finished with value: 0.6188086271286011 and parameters: {'observation_period_num': 159, 'train_rates': 0.9769346621944776, 'learning_rate': 1.0803568589876066e-05, 'batch_size': 110, 'step_size': 7, 'gamma': 0.7667858359426736}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:29:33,541][0m Trial 5 finished with value: 0.18473745457580326 and parameters: {'observation_period_num': 67, 'train_rates': 0.7292215035241365, 'learning_rate': 0.000435701288736917, 'batch_size': 41, 'step_size': 8, 'gamma': 0.7771962941864814}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:30:05,448][0m Trial 6 finished with value: 0.5013966854680237 and parameters: {'observation_period_num': 96, 'train_rates': 0.7185575429861439, 'learning_rate': 1.8901999866339905e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.8258944778129198}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:30:43,131][0m Trial 7 finished with value: 1.8404725945514182 and parameters: {'observation_period_num': 145, 'train_rates': 0.9264845003292482, 'learning_rate': 1.2280397448276165e-06, 'batch_size': 159, 'step_size': 9, 'gamma': 0.789847271531118}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:33:46,941][0m Trial 8 finished with value: 0.1568331304293987 and parameters: {'observation_period_num': 31, 'train_rates': 0.7008503679700436, 'learning_rate': 0.00021864362563295313, 'batch_size': 25, 'step_size': 9, 'gamma': 0.8774964609576239}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:34:13,914][0m Trial 9 finished with value: 0.9018619431363706 and parameters: {'observation_period_num': 117, 'train_rates': 0.6406294678100153, 'learning_rate': 1.9916702743104997e-06, 'batch_size': 190, 'step_size': 14, 'gamma': 0.9809551117870148}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:34:41,104][0m Trial 10 finished with value: 0.22713156044483185 and parameters: {'observation_period_num': 233, 'train_rates': 0.9141922807437554, 'learning_rate': 5.084668386567493e-05, 'batch_size': 248, 'step_size': 12, 'gamma': 0.9598735967640729}. Best is trial 3 with value: 0.05096358491801748.[0m
[32m[I 2025-01-07 05:35:09,806][0m Trial 11 finished with value: 0.04792610917618309 and parameters: {'observation_period_num': 10, 'train_rates': 0.8124368030550269, 'learning_rate': 0.0007766950153161616, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8784437804684554}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:35:34,407][0m Trial 12 finished with value: 0.22757398687129796 and parameters: {'observation_period_num': 6, 'train_rates': 0.7812421446770279, 'learning_rate': 0.0007317280991646761, 'batch_size': 242, 'step_size': 1, 'gamma': 0.9233566223841454}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:36:31,806][0m Trial 13 finished with value: 0.06975759565830231 and parameters: {'observation_period_num': 7, 'train_rates': 0.9887111420055098, 'learning_rate': 5.85897784112959e-05, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9227881329483985}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:36:58,575][0m Trial 14 finished with value: 0.08553804122932153 and parameters: {'observation_period_num': 57, 'train_rates': 0.7984254390535567, 'learning_rate': 0.0008912760235102727, 'batch_size': 213, 'step_size': 4, 'gamma': 0.8377510473924583}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:38:01,498][0m Trial 15 finished with value: 0.10548804071495088 and parameters: {'observation_period_num': 216, 'train_rates': 0.8925043218768931, 'learning_rate': 0.00011508728179213469, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8991389169875342}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:38:41,152][0m Trial 16 finished with value: 0.12390790761736788 and parameters: {'observation_period_num': 181, 'train_rates': 0.7974658736848239, 'learning_rate': 0.0009671968947804807, 'batch_size': 135, 'step_size': 3, 'gamma': 0.9597064827275192}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:39:10,974][0m Trial 17 finished with value: 0.11670468002557755 and parameters: {'observation_period_num': 86, 'train_rates': 0.9534425838285491, 'learning_rate': 0.00012084321578688548, 'batch_size': 216, 'step_size': 6, 'gamma': 0.8497402149039834}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:40:37,248][0m Trial 18 finished with value: 0.07148286991261549 and parameters: {'observation_period_num': 33, 'train_rates': 0.843664927673045, 'learning_rate': 0.0003745076495678656, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8994490331059364}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:41:10,332][0m Trial 19 finished with value: 0.6528840623376689 and parameters: {'observation_period_num': 35, 'train_rates': 0.604318567036034, 'learning_rate': 9.454378302491681e-06, 'batch_size': 142, 'step_size': 3, 'gamma': 0.9459329362070406}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:41:37,728][0m Trial 20 finished with value: 0.1511747152732976 and parameters: {'observation_period_num': 82, 'train_rates': 0.8796140208167528, 'learning_rate': 7.679936873926028e-05, 'batch_size': 219, 'step_size': 6, 'gamma': 0.8918255303377429}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:42:32,473][0m Trial 21 finished with value: 0.08454837650060654 and parameters: {'observation_period_num': 6, 'train_rates': 0.9852065525205194, 'learning_rate': 3.45720107645848e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9341464477029355}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:43:32,277][0m Trial 22 finished with value: 0.04829884021980873 and parameters: {'observation_period_num': 25, 'train_rates': 0.9486878423185953, 'learning_rate': 0.0005069995169643131, 'batch_size': 100, 'step_size': 4, 'gamma': 0.9159904201854585}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:44:53,877][0m Trial 23 finished with value: 0.07600527169166976 and parameters: {'observation_period_num': 38, 'train_rates': 0.9196990370431071, 'learning_rate': 0.0005217455568547254, 'batch_size': 71, 'step_size': 1, 'gamma': 0.8623557621056344}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:46:52,456][0m Trial 24 finished with value: 0.04943393984515416 and parameters: {'observation_period_num': 26, 'train_rates': 0.9481576369758433, 'learning_rate': 0.0002193694235118592, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9044717545943856}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:47:50,466][0m Trial 25 finished with value: 0.06226037085895807 and parameters: {'observation_period_num': 19, 'train_rates': 0.8307880711840658, 'learning_rate': 0.0002352666810225963, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8895964508056756}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:53:02,121][0m Trial 26 finished with value: 0.07756939925142188 and parameters: {'observation_period_num': 68, 'train_rates': 0.9537051967360786, 'learning_rate': 0.00018238727673151788, 'batch_size': 18, 'step_size': 2, 'gamma': 0.9025694988583373}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:53:45,873][0m Trial 27 finished with value: 0.20185784255233358 and parameters: {'observation_period_num': 27, 'train_rates': 0.7582435719321945, 'learning_rate': 0.0005108769677104514, 'batch_size': 123, 'step_size': 4, 'gamma': 0.8517220296629168}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:54:44,868][0m Trial 28 finished with value: 0.09185810047801178 and parameters: {'observation_period_num': 51, 'train_rates': 0.8610745597743145, 'learning_rate': 0.00010151420247686754, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8778203796745601}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:55:15,419][0m Trial 29 finished with value: 0.156302425329718 and parameters: {'observation_period_num': 71, 'train_rates': 0.8234947767823535, 'learning_rate': 0.0002633192537153216, 'batch_size': 184, 'step_size': 2, 'gamma': 0.8652889419715846}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:55:53,927][0m Trial 30 finished with value: 0.07354274911707953 and parameters: {'observation_period_num': 102, 'train_rates': 0.8934643736843306, 'learning_rate': 0.0005330328961802074, 'batch_size': 150, 'step_size': 6, 'gamma': 0.989769651675673}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:57:48,709][0m Trial 31 finished with value: 0.05844185977827671 and parameters: {'observation_period_num': 50, 'train_rates': 0.9557690782974736, 'learning_rate': 0.0006649669254296595, 'batch_size': 51, 'step_size': 7, 'gamma': 0.9112841250077527}. Best is trial 11 with value: 0.04792610917618309.[0m
[32m[I 2025-01-07 05:59:24,681][0m Trial 32 finished with value: 0.03365275936985984 and parameters: {'observation_period_num': 19, 'train_rates': 0.9329529269366065, 'learning_rate': 0.0003198612734853119, 'batch_size': 62, 'step_size': 5, 'gamma': 0.936379582708067}. Best is trial 32 with value: 0.03365275936985984.[0m
[32m[I 2025-01-07 06:00:43,667][0m Trial 33 finished with value: 0.040751704635719456 and parameters: {'observation_period_num': 19, 'train_rates': 0.9263649779028958, 'learning_rate': 0.000322571790184257, 'batch_size': 75, 'step_size': 4, 'gamma': 0.9482256684512541}. Best is trial 32 with value: 0.03365275936985984.[0m
[32m[I 2025-01-07 06:02:04,631][0m Trial 34 finished with value: 0.03499341868924174 and parameters: {'observation_period_num': 16, 'train_rates': 0.9164373709685392, 'learning_rate': 0.000317595438290583, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9442473722772186}. Best is trial 32 with value: 0.03365275936985984.[0m
[32m[I 2025-01-07 06:03:28,141][0m Trial 35 finished with value: 0.05030509516034486 and parameters: {'observation_period_num': 19, 'train_rates': 0.9098856197047912, 'learning_rate': 0.00015292811861071487, 'batch_size': 69, 'step_size': 5, 'gamma': 0.965707519161734}. Best is trial 32 with value: 0.03365275936985984.[0m
[32m[I 2025-01-07 06:04:35,840][0m Trial 36 finished with value: 0.05805855229772266 and parameters: {'observation_period_num': 49, 'train_rates': 0.8509343445132742, 'learning_rate': 0.0002846351122281755, 'batch_size': 81, 'step_size': 7, 'gamma': 0.9446933677429126}. Best is trial 32 with value: 0.03365275936985984.[0m
[32m[I 2025-01-07 06:07:05,876][0m Trial 37 finished with value: 0.0284716590588528 and parameters: {'observation_period_num': 17, 'train_rates': 0.8928646906733584, 'learning_rate': 0.00031820913256764356, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9416656566011461}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:09:44,420][0m Trial 38 finished with value: 0.13991894465577495 and parameters: {'observation_period_num': 167, 'train_rates': 0.8807721205966871, 'learning_rate': 0.0003527739879012025, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9753738433986997}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:11:17,278][0m Trial 39 finished with value: 0.1079710065862354 and parameters: {'observation_period_num': 65, 'train_rates': 0.9323690406588541, 'learning_rate': 2.0605559876211436e-05, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9414611052620435}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:13:46,710][0m Trial 40 finished with value: 0.04803209380569683 and parameters: {'observation_period_num': 41, 'train_rates': 0.8980843093800055, 'learning_rate': 0.00015560336960365203, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9537243235781755}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:15:00,803][0m Trial 41 finished with value: 0.04308891553169131 and parameters: {'observation_period_num': 15, 'train_rates': 0.869802504604521, 'learning_rate': 0.0003681206004571593, 'batch_size': 76, 'step_size': 5, 'gamma': 0.9333499218538477}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:16:49,835][0m Trial 42 finished with value: 0.042247686185230865 and parameters: {'observation_period_num': 18, 'train_rates': 0.8767775732512897, 'learning_rate': 0.0003449730515265074, 'batch_size': 53, 'step_size': 5, 'gamma': 0.936706271520606}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:18:37,556][0m Trial 43 finished with value: 0.04198325793821733 and parameters: {'observation_period_num': 19, 'train_rates': 0.9692939083227193, 'learning_rate': 0.0003087990859914498, 'batch_size': 55, 'step_size': 5, 'gamma': 0.9740682817914967}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:22:30,133][0m Trial 44 finished with value: 0.054909493637884534 and parameters: {'observation_period_num': 40, 'train_rates': 0.971937311529822, 'learning_rate': 0.00015352768606235247, 'batch_size': 25, 'step_size': 8, 'gamma': 0.9704037938535812}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:24:43,569][0m Trial 45 finished with value: 0.06947516893396283 and parameters: {'observation_period_num': 61, 'train_rates': 0.9654049546302828, 'learning_rate': 8.177348110713554e-05, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9854599911842149}. Best is trial 37 with value: 0.0284716590588528.[0m
Early stopping at epoch 85
[32m[I 2025-01-07 06:26:06,842][0m Trial 46 finished with value: 0.16141447603702544 and parameters: {'observation_period_num': 136, 'train_rates': 0.929294039258288, 'learning_rate': 0.00029185961573665833, 'batch_size': 58, 'step_size': 2, 'gamma': 0.7527188675486979}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:29:00,222][0m Trial 47 finished with value: 0.0590891232616023 and parameters: {'observation_period_num': 78, 'train_rates': 0.9341427488364289, 'learning_rate': 0.0006984730249368551, 'batch_size': 32, 'step_size': 7, 'gamma': 0.9546142795044402}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:30:06,823][0m Trial 48 finished with value: 0.11922199270603331 and parameters: {'observation_period_num': 5, 'train_rates': 0.910575654987916, 'learning_rate': 7.031123427180841e-06, 'batch_size': 87, 'step_size': 4, 'gamma': 0.977396394553072}. Best is trial 37 with value: 0.0284716590588528.[0m
[32m[I 2025-01-07 06:32:12,071][0m Trial 49 finished with value: 0.07879905042688498 and parameters: {'observation_period_num': 95, 'train_rates': 0.9688971558062184, 'learning_rate': 5.3383970981841216e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9270994468350374}. Best is trial 37 with value: 0.0284716590588528.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 06:32:12,082][0m A new study created in memory with name: no-name-412cd23e-4883-4031-a00f-82b274815ae9[0m
[32m[I 2025-01-07 06:33:44,891][0m Trial 0 finished with value: 0.4428829017136402 and parameters: {'observation_period_num': 141, 'train_rates': 0.6858980589739002, 'learning_rate': 1.784256226560213e-06, 'batch_size': 116, 'step_size': 3, 'gamma': 0.9690540801444814}. Best is trial 0 with value: 0.4428829017136402.[0m
[32m[I 2025-01-07 06:35:06,490][0m Trial 1 finished with value: 0.291721647668683 and parameters: {'observation_period_num': 177, 'train_rates': 0.6719835992973014, 'learning_rate': 0.0008043872047995447, 'batch_size': 255, 'step_size': 14, 'gamma': 0.9053608296196178}. Best is trial 1 with value: 0.291721647668683.[0m
[32m[I 2025-01-07 06:36:50,047][0m Trial 2 finished with value: 0.05980733597100201 and parameters: {'observation_period_num': 53, 'train_rates': 0.804182354000906, 'learning_rate': 0.0003104920271820312, 'batch_size': 125, 'step_size': 12, 'gamma': 0.7675209877066403}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:38:32,380][0m Trial 3 finished with value: 0.16525486982326476 and parameters: {'observation_period_num': 114, 'train_rates': 0.789021485379392, 'learning_rate': 1.1214151218600204e-05, 'batch_size': 201, 'step_size': 11, 'gamma': 0.9824623678008013}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:40:31,523][0m Trial 4 finished with value: 0.16029980739133548 and parameters: {'observation_period_num': 144, 'train_rates': 0.8792878505027132, 'learning_rate': 1.8272520208594948e-05, 'batch_size': 164, 'step_size': 1, 'gamma': 0.9490759243731702}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:42:16,901][0m Trial 5 finished with value: 0.26240169312343126 and parameters: {'observation_period_num': 181, 'train_rates': 0.6509373475884981, 'learning_rate': 0.0008073662473371851, 'batch_size': 80, 'step_size': 14, 'gamma': 0.7894158057139926}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:44:02,699][0m Trial 6 finished with value: 0.07521233508702029 and parameters: {'observation_period_num': 24, 'train_rates': 0.8799097184168914, 'learning_rate': 7.682007933115353e-06, 'batch_size': 230, 'step_size': 14, 'gamma': 0.8465946483650323}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:46:06,045][0m Trial 7 finished with value: 0.08284953981637955 and parameters: {'observation_period_num': 91, 'train_rates': 0.9786268672278017, 'learning_rate': 4.547217254714974e-05, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8218360893448399}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:47:46,867][0m Trial 8 finished with value: 0.20193868282508884 and parameters: {'observation_period_num': 10, 'train_rates': 0.7595051550137819, 'learning_rate': 4.326613723051373e-05, 'batch_size': 209, 'step_size': 3, 'gamma': 0.8866071574138167}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:49:52,598][0m Trial 9 finished with value: 0.1008795965462923 and parameters: {'observation_period_num': 81, 'train_rates': 0.858164851832743, 'learning_rate': 7.71839317737094e-06, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7886143928822184}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:52:30,736][0m Trial 10 finished with value: 0.30052545136875575 and parameters: {'observation_period_num': 238, 'train_rates': 0.602238029566904, 'learning_rate': 0.00020864760564870803, 'batch_size': 44, 'step_size': 9, 'gamma': 0.7542861030786726}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:54:28,781][0m Trial 11 finished with value: 0.12437267147976419 and parameters: {'observation_period_num': 15, 'train_rates': 0.8982229539486788, 'learning_rate': 3.2521514965061054e-06, 'batch_size': 247, 'step_size': 11, 'gamma': 0.8432724964239222}. Best is trial 2 with value: 0.05980733597100201.[0m
[32m[I 2025-01-07 06:56:52,973][0m Trial 12 finished with value: 0.053024250934732724 and parameters: {'observation_period_num': 37, 'train_rates': 0.9517297435440323, 'learning_rate': 0.0001072282245605888, 'batch_size': 87, 'step_size': 12, 'gamma': 0.8531651342863493}. Best is trial 12 with value: 0.053024250934732724.[0m
[32m[I 2025-01-07 06:59:39,332][0m Trial 13 finished with value: 0.09419230732225603 and parameters: {'observation_period_num': 54, 'train_rates': 0.9681989143814166, 'learning_rate': 0.00015454189652306044, 'batch_size': 66, 'step_size': 11, 'gamma': 0.9151936308929333}. Best is trial 12 with value: 0.053024250934732724.[0m
[32m[I 2025-01-07 07:01:34,070][0m Trial 14 finished with value: 0.18897763555906863 and parameters: {'observation_period_num': 56, 'train_rates': 0.7420967155339282, 'learning_rate': 0.00015891039274512972, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7595254661395332}. Best is trial 12 with value: 0.053024250934732724.[0m
[32m[I 2025-01-07 07:06:15,970][0m Trial 15 finished with value: 0.06477967591490597 and parameters: {'observation_period_num': 55, 'train_rates': 0.8288178131162212, 'learning_rate': 0.0003445022886976966, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8021024406441969}. Best is trial 12 with value: 0.053024250934732724.[0m
[32m[I 2025-01-07 07:08:04,498][0m Trial 16 finished with value: 0.06499832029339303 and parameters: {'observation_period_num': 87, 'train_rates': 0.9381111615005504, 'learning_rate': 8.90648989881e-05, 'batch_size': 153, 'step_size': 9, 'gamma': 0.8460134056710905}. Best is trial 12 with value: 0.053024250934732724.[0m
[32m[I 2025-01-07 07:10:14,737][0m Trial 17 finished with value: 0.049442938996844314 and parameters: {'observation_period_num': 37, 'train_rates': 0.9283318665124403, 'learning_rate': 0.0003540428773856663, 'batch_size': 90, 'step_size': 5, 'gamma': 0.8706172080480437}. Best is trial 17 with value: 0.049442938996844314.[0m
[32m[I 2025-01-07 07:12:24,839][0m Trial 18 finished with value: 0.0448338022528618 and parameters: {'observation_period_num': 33, 'train_rates': 0.9303113169062609, 'learning_rate': 6.846161724870353e-05, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8737769413363236}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:15:14,082][0m Trial 19 finished with value: 0.09578545174815438 and parameters: {'observation_period_num': 109, 'train_rates': 0.9228852380278443, 'learning_rate': 0.00045080845004357947, 'batch_size': 56, 'step_size': 6, 'gamma': 0.8763353041388902}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:21:35,671][0m Trial 20 finished with value: 0.21825503334403037 and parameters: {'observation_period_num': 252, 'train_rates': 0.8447533860067563, 'learning_rate': 7.045373701977405e-05, 'batch_size': 21, 'step_size': 5, 'gamma': 0.9282359896530118}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:23:47,028][0m Trial 21 finished with value: 0.04868315695784986 and parameters: {'observation_period_num': 32, 'train_rates': 0.9452340134454051, 'learning_rate': 9.140374735028301e-05, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8610778271248778}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:25:54,764][0m Trial 22 finished with value: 0.05444268120759462 and parameters: {'observation_period_num': 36, 'train_rates': 0.9146358435400709, 'learning_rate': 2.271156368439827e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.8879977123574768}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:28:48,889][0m Trial 23 finished with value: 0.0451870821416378 and parameters: {'observation_period_num': 5, 'train_rates': 0.9870925239910131, 'learning_rate': 5.982239297019271e-05, 'batch_size': 74, 'step_size': 5, 'gamma': 0.8636348541690246}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:32:00,800][0m Trial 24 finished with value: 0.0490243174135685 and parameters: {'observation_period_num': 11, 'train_rates': 0.9887536491787544, 'learning_rate': 6.163190337074063e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8227260889903221}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:34:55,292][0m Trial 25 finished with value: 0.08861246298090751 and parameters: {'observation_period_num': 71, 'train_rates': 0.9590049802619685, 'learning_rate': 3.08077156830008e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.9062430586618727}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:39:08,470][0m Trial 26 finished with value: 0.05583749711513519 and parameters: {'observation_period_num': 7, 'train_rates': 0.9889392843057991, 'learning_rate': 1.768278087589628e-05, 'batch_size': 40, 'step_size': 4, 'gamma': 0.8239712830186887}. Best is trial 18 with value: 0.0448338022528618.[0m
Early stopping at epoch 89
[32m[I 2025-01-07 07:41:06,765][0m Trial 27 finished with value: 0.06120648723224114 and parameters: {'observation_period_num': 30, 'train_rates': 0.9110087755635581, 'learning_rate': 0.00013464463243185592, 'batch_size': 108, 'step_size': 1, 'gamma': 0.8651627426835614}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:43:01,739][0m Trial 28 finished with value: 0.07493352144956589 and parameters: {'observation_period_num': 74, 'train_rates': 0.9504156412312467, 'learning_rate': 4.2136268202574696e-05, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9291725618304285}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:44:56,713][0m Trial 29 finished with value: 0.2494477938080943 and parameters: {'observation_period_num': 139, 'train_rates': 0.8779121722659537, 'learning_rate': 2.766955756806169e-06, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8920891086347045}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:46:41,008][0m Trial 30 finished with value: 0.3995261060078735 and parameters: {'observation_period_num': 99, 'train_rates': 0.7260340362608856, 'learning_rate': 1.2701500662448214e-06, 'batch_size': 90, 'step_size': 8, 'gamma': 0.9560699571969378}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:49:45,072][0m Trial 31 finished with value: 0.05182517319917679 and parameters: {'observation_period_num': 6, 'train_rates': 0.9834654519604724, 'learning_rate': 6.105449095609742e-05, 'batch_size': 55, 'step_size': 2, 'gamma': 0.8265409133592997}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:52:26,072][0m Trial 32 finished with value: 0.056981299072504044 and parameters: {'observation_period_num': 25, 'train_rates': 0.9889435824660819, 'learning_rate': 7.904795626936037e-05, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8596220318298239}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 07:55:35,626][0m Trial 33 finished with value: 0.07284140755091945 and parameters: {'observation_period_num': 43, 'train_rates': 0.9575244389379004, 'learning_rate': 2.813012477465688e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.822457051069093}. Best is trial 18 with value: 0.0448338022528618.[0m
[32m[I 2025-01-07 08:03:01,530][0m Trial 34 finished with value: 0.0341548930623175 and parameters: {'observation_period_num': 22, 'train_rates': 0.933820601560436, 'learning_rate': 0.00021149643429965986, 'batch_size': 21, 'step_size': 5, 'gamma': 0.8373334555028187}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:11:46,232][0m Trial 35 finished with value: 0.06377101495935775 and parameters: {'observation_period_num': 63, 'train_rates': 0.8952105293121417, 'learning_rate': 0.00022473031546394373, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8364039719685968}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:16:46,789][0m Trial 36 finished with value: 0.11397175314395051 and parameters: {'observation_period_num': 180, 'train_rates': 0.9315559824835912, 'learning_rate': 0.00021127551303752417, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8028732431713649}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:18:53,802][0m Trial 37 finished with value: 0.04684901009075833 and parameters: {'observation_period_num': 24, 'train_rates': 0.8128855584585019, 'learning_rate': 0.0005588369961071877, 'batch_size': 77, 'step_size': 7, 'gamma': 0.8757995716937059}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:20:57,366][0m Trial 38 finished with value: 0.10003992521537924 and parameters: {'observation_period_num': 46, 'train_rates': 0.8177340162566487, 'learning_rate': 0.0006649926113238036, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8789135037770758}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:24:38,321][0m Trial 39 finished with value: 0.04952430535463224 and parameters: {'observation_period_num': 21, 'train_rates': 0.7928075781216252, 'learning_rate': 0.00048394639430801285, 'batch_size': 39, 'step_size': 9, 'gamma': 0.9016730818486619}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:26:04,776][0m Trial 40 finished with value: 0.2795734634589867 and parameters: {'observation_period_num': 203, 'train_rates': 0.6927509827373396, 'learning_rate': 0.000953690658235359, 'batch_size': 183, 'step_size': 5, 'gamma': 0.8368225938846079}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:28:02,488][0m Trial 41 finished with value: 0.04052083240133788 and parameters: {'observation_period_num': 22, 'train_rates': 0.8615760310101293, 'learning_rate': 0.00011631078073311702, 'batch_size': 125, 'step_size': 6, 'gamma': 0.8605627066183456}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:29:49,232][0m Trial 42 finished with value: 0.17350893264259298 and parameters: {'observation_period_num': 17, 'train_rates': 0.7780959613558859, 'learning_rate': 0.0005590059322694355, 'batch_size': 118, 'step_size': 7, 'gamma': 0.8763529511763658}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:31:30,364][0m Trial 43 finished with value: 0.039309641004400556 and parameters: {'observation_period_num': 23, 'train_rates': 0.8589522412275045, 'learning_rate': 0.00028913869764139955, 'batch_size': 133, 'step_size': 4, 'gamma': 0.8556084584376764}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:33:43,963][0m Trial 44 finished with value: 0.05548624219395529 and parameters: {'observation_period_num': 5, 'train_rates': 0.860756642276429, 'learning_rate': 0.0002720119257235562, 'batch_size': 130, 'step_size': 2, 'gamma': 0.8097944174496446}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:35:31,471][0m Trial 45 finished with value: 0.05459873613558317 and parameters: {'observation_period_num': 43, 'train_rates': 0.8824568078728411, 'learning_rate': 0.00011750085775868081, 'batch_size': 144, 'step_size': 4, 'gamma': 0.8511110864105909}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:37:11,878][0m Trial 46 finished with value: 0.0923029664724935 and parameters: {'observation_period_num': 67, 'train_rates': 0.8415332487310487, 'learning_rate': 5.222054011845316e-05, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8593090894119582}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:38:38,678][0m Trial 47 finished with value: 0.09315617453040821 and parameters: {'observation_period_num': 158, 'train_rates': 0.8998708677585303, 'learning_rate': 0.00016159384246092881, 'batch_size': 141, 'step_size': 5, 'gamma': 0.9886023725982361}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:40:30,227][0m Trial 48 finished with value: 0.06348222278826814 and parameters: {'observation_period_num': 20, 'train_rates': 0.8579162100237351, 'learning_rate': 1.3699418245049292e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.8426467752495642}. Best is trial 34 with value: 0.0341548930623175.[0m
[32m[I 2025-01-07 08:42:36,107][0m Trial 49 finished with value: 0.07493096590042114 and parameters: {'observation_period_num': 48, 'train_rates': 0.9688359242010532, 'learning_rate': 0.00030687082225535207, 'batch_size': 152, 'step_size': 4, 'gamma': 0.7854086982850662}. Best is trial 34 with value: 0.0341548930623175.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 08:42:36,117][0m A new study created in memory with name: no-name-19e05739-24ae-4af2-8b0d-dbef474b1774[0m
[32m[I 2025-01-07 08:44:06,530][0m Trial 0 finished with value: 0.327682840118158 and parameters: {'observation_period_num': 252, 'train_rates': 0.7672324545747903, 'learning_rate': 3.196663530943814e-05, 'batch_size': 243, 'step_size': 15, 'gamma': 0.9898739353051939}. Best is trial 0 with value: 0.327682840118158.[0m
[32m[I 2025-01-07 08:45:39,767][0m Trial 1 finished with value: 0.37684397116709556 and parameters: {'observation_period_num': 230, 'train_rates': 0.6495835358912271, 'learning_rate': 1.3102684097761394e-05, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8350934943999067}. Best is trial 0 with value: 0.327682840118158.[0m
[32m[I 2025-01-07 08:47:22,869][0m Trial 2 finished with value: 0.11428987968062597 and parameters: {'observation_period_num': 169, 'train_rates': 0.8644335214706402, 'learning_rate': 0.0003846078006388964, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8469197785670389}. Best is trial 2 with value: 0.11428987968062597.[0m
[32m[I 2025-01-07 08:49:22,683][0m Trial 3 finished with value: 0.028231589509361596 and parameters: {'observation_period_num': 12, 'train_rates': 0.8851406389471477, 'learning_rate': 0.0009646210844056339, 'batch_size': 151, 'step_size': 6, 'gamma': 0.8308179145198721}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 08:51:07,358][0m Trial 4 finished with value: 0.25592202627963395 and parameters: {'observation_period_num': 156, 'train_rates': 0.7061715013711396, 'learning_rate': 0.000530239058080976, 'batch_size': 158, 'step_size': 3, 'gamma': 0.8012829178917629}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 08:52:51,933][0m Trial 5 finished with value: 0.25071260202016715 and parameters: {'observation_period_num': 140, 'train_rates': 0.6786254928865953, 'learning_rate': 2.3985269104798122e-05, 'batch_size': 115, 'step_size': 3, 'gamma': 0.9486446330742422}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 08:55:09,142][0m Trial 6 finished with value: 0.253329807502834 and parameters: {'observation_period_num': 139, 'train_rates': 0.7521708263026714, 'learning_rate': 1.7723340217446252e-05, 'batch_size': 71, 'step_size': 14, 'gamma': 0.8799487130144712}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 08:56:54,215][0m Trial 7 finished with value: 0.07206618971409012 and parameters: {'observation_period_num': 149, 'train_rates': 0.9069846508729302, 'learning_rate': 4.9635998777628624e-05, 'batch_size': 173, 'step_size': 7, 'gamma': 0.9249913185849356}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 08:58:38,118][0m Trial 8 finished with value: 0.09106019139289856 and parameters: {'observation_period_num': 166, 'train_rates': 0.9306832825721192, 'learning_rate': 0.00014497503514562086, 'batch_size': 251, 'step_size': 6, 'gamma': 0.8139395697978744}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:00:16,907][0m Trial 9 finished with value: 0.20379132654507337 and parameters: {'observation_period_num': 72, 'train_rates': 0.7551532028790934, 'learning_rate': 6.0943024381380706e-05, 'batch_size': 141, 'step_size': 13, 'gamma': 0.9824310364713242}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:05:33,794][0m Trial 10 finished with value: 0.06406591680421624 and parameters: {'observation_period_num': 6, 'train_rates': 0.842440055408446, 'learning_rate': 1.4806710608918028e-06, 'batch_size': 28, 'step_size': 10, 'gamma': 0.7501021048691419}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:08:29,348][0m Trial 11 finished with value: 0.11100728471651145 and parameters: {'observation_period_num': 8, 'train_rates': 0.8564638202751066, 'learning_rate': 1.02843394784949e-06, 'batch_size': 53, 'step_size': 10, 'gamma': 0.7569599826991261}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:10:33,734][0m Trial 12 finished with value: 0.14007799879268365 and parameters: {'observation_period_num': 7, 'train_rates': 0.8351546184329655, 'learning_rate': 2.394225408670861e-06, 'batch_size': 91, 'step_size': 6, 'gamma': 0.756846050888093}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:15:28,119][0m Trial 13 finished with value: 0.10607441640402897 and parameters: {'observation_period_num': 58, 'train_rates': 0.9714853860699679, 'learning_rate': 4.422119312117454e-06, 'batch_size': 33, 'step_size': 10, 'gamma': 0.883402362956718}. Best is trial 3 with value: 0.028231589509361596.[0m
Early stopping at epoch 55
[32m[I 2025-01-07 09:16:36,242][0m Trial 14 finished with value: 0.11362442286252096 and parameters: {'observation_period_num': 61, 'train_rates': 0.9064677746980403, 'learning_rate': 0.0008396627959642124, 'batch_size': 196, 'step_size': 1, 'gamma': 0.7791407725889087}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:24:15,738][0m Trial 15 finished with value: 0.10817605667234806 and parameters: {'observation_period_num': 93, 'train_rates': 0.8135127544286785, 'learning_rate': 0.0001340084260341385, 'batch_size': 18, 'step_size': 4, 'gamma': 0.7962279586040706}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:26:28,286][0m Trial 16 finished with value: 0.11216912418603897 and parameters: {'observation_period_num': 28, 'train_rates': 0.9841825056536978, 'learning_rate': 7.208351551162571e-06, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8485233380420092}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:28:17,925][0m Trial 17 finished with value: 0.7901162391268924 and parameters: {'observation_period_num': 99, 'train_rates': 0.6002362369921428, 'learning_rate': 1.252409799399106e-06, 'batch_size': 103, 'step_size': 9, 'gamma': 0.9031512076688935}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:30:12,073][0m Trial 18 finished with value: 0.049842651826815576 and parameters: {'observation_period_num': 37, 'train_rates': 0.7963421840408407, 'learning_rate': 0.0001915898135551017, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8239850576837849}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:32:11,036][0m Trial 19 finished with value: 0.04554184066022143 and parameters: {'observation_period_num': 39, 'train_rates': 0.8074868574087222, 'learning_rate': 0.00025205005768277953, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8220138813693204}. Best is trial 3 with value: 0.028231589509361596.[0m
Early stopping at epoch 96
[32m[I 2025-01-07 09:33:57,667][0m Trial 20 finished with value: 0.09002474563219297 and parameters: {'observation_period_num': 114, 'train_rates': 0.8860204052144408, 'learning_rate': 0.0009817751221516495, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8653352513986536}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:36:15,017][0m Trial 21 finished with value: 0.18640037220768846 and parameters: {'observation_period_num': 38, 'train_rates': 0.7852844946556571, 'learning_rate': 0.0002326655926012495, 'batch_size': 75, 'step_size': 5, 'gamma': 0.8228716858290257}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:38:54,139][0m Trial 22 finished with value: 0.0456028738241086 and parameters: {'observation_period_num': 29, 'train_rates': 0.8216561121345127, 'learning_rate': 0.0002881106665363724, 'batch_size': 60, 'step_size': 4, 'gamma': 0.783999040889002}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:41:39,805][0m Trial 23 finished with value: 0.1763774555429301 and parameters: {'observation_period_num': 34, 'train_rates': 0.7325981153392441, 'learning_rate': 0.0004038235314299179, 'batch_size': 50, 'step_size': 3, 'gamma': 0.782754305867532}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:44:23,695][0m Trial 24 finished with value: 0.06635038037266996 and parameters: {'observation_period_num': 77, 'train_rates': 0.9374718791208083, 'learning_rate': 7.756231977297907e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.7803985626040079}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:45:53,980][0m Trial 25 finished with value: 0.04902059052671705 and parameters: {'observation_period_num': 51, 'train_rates': 0.8167117470258166, 'learning_rate': 0.00028391938900909283, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8554732899254869}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:47:51,840][0m Trial 26 finished with value: 0.04242250694732162 and parameters: {'observation_period_num': 22, 'train_rates': 0.8837470523285095, 'learning_rate': 0.0005165027156325809, 'batch_size': 94, 'step_size': 4, 'gamma': 0.8030906095683836}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:49:38,470][0m Trial 27 finished with value: 0.047821456012153755 and parameters: {'observation_period_num': 20, 'train_rates': 0.872828748079643, 'learning_rate': 0.0005846428543156569, 'batch_size': 97, 'step_size': 2, 'gamma': 0.8074766305767238}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:51:12,970][0m Trial 28 finished with value: 0.10325630754232407 and parameters: {'observation_period_num': 201, 'train_rates': 0.9516799086027332, 'learning_rate': 0.0006824185294205418, 'batch_size': 179, 'step_size': 7, 'gamma': 0.8329573406029143}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:53:01,516][0m Trial 29 finished with value: 0.05918556951381118 and parameters: {'observation_period_num': 88, 'train_rates': 0.89464177823465, 'learning_rate': 0.00010396774071686429, 'batch_size': 123, 'step_size': 5, 'gamma': 0.9020971077090345}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:54:45,525][0m Trial 30 finished with value: 0.067041169017306 and parameters: {'observation_period_num': 51, 'train_rates': 0.916957110239068, 'learning_rate': 3.493707442533156e-05, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8411229626438828}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:56:29,927][0m Trial 31 finished with value: 0.036484715125457506 and parameters: {'observation_period_num': 23, 'train_rates': 0.8266708936831406, 'learning_rate': 0.00035839382196290863, 'batch_size': 88, 'step_size': 4, 'gamma': 0.7912478338051095}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 09:58:16,187][0m Trial 32 finished with value: 0.04212086422365875 and parameters: {'observation_period_num': 19, 'train_rates': 0.7892873188535254, 'learning_rate': 0.0004341521499422551, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8156759531709775}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:00:02,894][0m Trial 33 finished with value: 0.1721543706782379 and parameters: {'observation_period_num': 18, 'train_rates': 0.7777630614961951, 'learning_rate': 0.00037458142707037675, 'batch_size': 136, 'step_size': 8, 'gamma': 0.7958367770266853}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:02:03,289][0m Trial 34 finished with value: 0.11551947945644006 and parameters: {'observation_period_num': 246, 'train_rates': 0.8622174303286178, 'learning_rate': 0.0005007531478577508, 'batch_size': 91, 'step_size': 7, 'gamma': 0.7684376072324619}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:04:03,025][0m Trial 35 finished with value: 0.12051922026797231 and parameters: {'observation_period_num': 200, 'train_rates': 0.8794795802578077, 'learning_rate': 0.0007323478096518912, 'batch_size': 215, 'step_size': 2, 'gamma': 0.8004824559557849}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:05:29,757][0m Trial 36 finished with value: 0.16401378359424812 and parameters: {'observation_period_num': 17, 'train_rates': 0.7299323825691544, 'learning_rate': 0.00040747270552003815, 'batch_size': 146, 'step_size': 3, 'gamma': 0.8317303776474744}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:07:05,006][0m Trial 37 finished with value: 0.07135389126215043 and parameters: {'observation_period_num': 69, 'train_rates': 0.8382083550325798, 'learning_rate': 0.00018810714149225302, 'batch_size': 168, 'step_size': 7, 'gamma': 0.861648605421121}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:08:31,344][0m Trial 38 finished with value: 0.19916016428041564 and parameters: {'observation_period_num': 48, 'train_rates': 0.7689785420496332, 'learning_rate': 0.0009531543627487964, 'batch_size': 113, 'step_size': 6, 'gamma': 0.8129581585171274}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:10:03,704][0m Trial 39 finished with value: 0.14302342950895025 and parameters: {'observation_period_num': 19, 'train_rates': 0.6846913922338014, 'learning_rate': 0.00048306673624540355, 'batch_size': 128, 'step_size': 15, 'gamma': 0.767217831585469}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:11:28,009][0m Trial 40 finished with value: 0.07500564572668432 and parameters: {'observation_period_num': 118, 'train_rates': 0.8585382681812239, 'learning_rate': 0.00010583553479214716, 'batch_size': 233, 'step_size': 8, 'gamma': 0.8782344130930233}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:13:22,645][0m Trial 41 finished with value: 0.058249979258430576 and parameters: {'observation_period_num': 44, 'train_rates': 0.8065374153801533, 'learning_rate': 0.00031794257112645986, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8193744652504317}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:15:12,819][0m Trial 42 finished with value: 0.042095710976670184 and parameters: {'observation_period_num': 29, 'train_rates': 0.7949432506563727, 'learning_rate': 0.000622151744117801, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8050133787078354}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:16:56,756][0m Trial 43 finished with value: 0.17474600725940295 and parameters: {'observation_period_num': 28, 'train_rates': 0.7463080862788218, 'learning_rate': 0.0006727635495836766, 'batch_size': 86, 'step_size': 4, 'gamma': 0.7910157592424742}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:20:14,859][0m Trial 44 finished with value: 0.031199986655378438 and parameters: {'observation_period_num': 6, 'train_rates': 0.8326946763603577, 'learning_rate': 0.0005412106456583527, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8066266810609789}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:23:46,714][0m Trial 45 finished with value: 0.030607733694992402 and parameters: {'observation_period_num': 5, 'train_rates': 0.8460775114550344, 'learning_rate': 0.0001884824467236863, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8466135665371552}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:27:45,844][0m Trial 46 finished with value: 0.031375374182274464 and parameters: {'observation_period_num': 6, 'train_rates': 0.838932693891594, 'learning_rate': 0.00019002531858621275, 'batch_size': 37, 'step_size': 2, 'gamma': 0.8398412994671174}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:31:15,092][0m Trial 47 finished with value: 0.03355842275717824 and parameters: {'observation_period_num': 7, 'train_rates': 0.8355859976757524, 'learning_rate': 0.0001457583518094499, 'batch_size': 42, 'step_size': 2, 'gamma': 0.8415889157976488}. Best is trial 3 with value: 0.028231589509361596.[0m
[32m[I 2025-01-07 10:34:45,768][0m Trial 48 finished with value: 0.04450312701364358 and parameters: {'observation_period_num': 6, 'train_rates': 0.8475649590052994, 'learning_rate': 4.1796615902362854e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.8494634979649943}. Best is trial 3 with value: 0.028231589509361596.[0m
Early stopping at epoch 78
[32m[I 2025-01-07 10:38:21,797][0m Trial 49 finished with value: 0.04585823355438678 and parameters: {'observation_period_num': 8, 'train_rates': 0.8451218991831857, 'learning_rate': 6.49586117267351e-05, 'batch_size': 33, 'step_size': 1, 'gamma': 0.8412788920927914}. Best is trial 3 with value: 0.028231589509361596.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 10:38:21,804][0m A new study created in memory with name: no-name-6b461b77-4545-4691-8327-22b33a5392cd[0m
[32m[I 2025-01-07 10:39:50,537][0m Trial 0 finished with value: 0.3002873359899211 and parameters: {'observation_period_num': 247, 'train_rates': 0.7273147075307558, 'learning_rate': 2.4739115576405774e-05, 'batch_size': 87, 'step_size': 12, 'gamma': 0.8286742224609922}. Best is trial 0 with value: 0.3002873359899211.[0m
[32m[I 2025-01-07 10:41:12,970][0m Trial 1 finished with value: 0.5215377013079264 and parameters: {'observation_period_num': 248, 'train_rates': 0.6147756237290414, 'learning_rate': 2.906588996294047e-05, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9279169521842883}. Best is trial 0 with value: 0.3002873359899211.[0m
[32m[I 2025-01-07 10:42:53,205][0m Trial 2 finished with value: 0.1593115668530385 and parameters: {'observation_period_num': 249, 'train_rates': 0.8001204295074396, 'learning_rate': 2.445110891171174e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.8100628198051172}. Best is trial 2 with value: 0.1593115668530385.[0m
[32m[I 2025-01-07 10:44:16,271][0m Trial 3 finished with value: 0.29344255919497897 and parameters: {'observation_period_num': 7, 'train_rates': 0.7753218041356901, 'learning_rate': 2.4708454166011093e-06, 'batch_size': 185, 'step_size': 8, 'gamma': 0.7850797541210293}. Best is trial 2 with value: 0.1593115668530385.[0m
[32m[I 2025-01-07 10:46:38,103][0m Trial 4 finished with value: 0.12451256284095945 and parameters: {'observation_period_num': 203, 'train_rates': 0.9003282978544054, 'learning_rate': 4.525478901100628e-06, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8622096790702263}. Best is trial 4 with value: 0.12451256284095945.[0m
[32m[I 2025-01-07 10:48:28,786][0m Trial 5 finished with value: 0.11003210811799848 and parameters: {'observation_period_num': 227, 'train_rates': 0.8155209764995047, 'learning_rate': 0.00011921590126893932, 'batch_size': 94, 'step_size': 10, 'gamma': 0.9717510652907931}. Best is trial 5 with value: 0.11003210811799848.[0m
[32m[I 2025-01-07 10:49:56,932][0m Trial 6 finished with value: 0.3086747575994473 and parameters: {'observation_period_num': 194, 'train_rates': 0.6620228999756391, 'learning_rate': 2.850350504354417e-05, 'batch_size': 152, 'step_size': 12, 'gamma': 0.8519783780029387}. Best is trial 5 with value: 0.11003210811799848.[0m
[32m[I 2025-01-07 10:51:38,710][0m Trial 7 finished with value: 0.2917096484671621 and parameters: {'observation_period_num': 202, 'train_rates': 0.7528099717669421, 'learning_rate': 0.00018408554895614038, 'batch_size': 124, 'step_size': 14, 'gamma': 0.8662092848058439}. Best is trial 5 with value: 0.11003210811799848.[0m
[32m[I 2025-01-07 10:53:19,513][0m Trial 8 finished with value: 0.29640337389249066 and parameters: {'observation_period_num': 221, 'train_rates': 0.7621336742618305, 'learning_rate': 2.541689807615141e-05, 'batch_size': 84, 'step_size': 12, 'gamma': 0.7977286530986765}. Best is trial 5 with value: 0.11003210811799848.[0m
[32m[I 2025-01-07 10:56:03,109][0m Trial 9 finished with value: 0.10355767412111164 and parameters: {'observation_period_num': 176, 'train_rates': 0.9281064080963433, 'learning_rate': 0.00011904432869429827, 'batch_size': 55, 'step_size': 14, 'gamma': 0.767065905688508}. Best is trial 9 with value: 0.10355767412111164.[0m
Early stopping at epoch 62
[32m[I 2025-01-07 11:00:44,508][0m Trial 10 finished with value: 0.0856629379093647 and parameters: {'observation_period_num': 133, 'train_rates': 0.9859090949745021, 'learning_rate': 0.0005559742980221936, 'batch_size': 21, 'step_size': 1, 'gamma': 0.7586409178716028}. Best is trial 10 with value: 0.0856629379093647.[0m
[32m[I 2025-01-07 11:10:20,337][0m Trial 11 finished with value: 0.06937629422720741 and parameters: {'observation_period_num': 128, 'train_rates': 0.9819442916587661, 'learning_rate': 0.0009723295884397189, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7514758542567916}. Best is trial 11 with value: 0.06937629422720741.[0m
Early stopping at epoch 62
[32m[I 2025-01-07 11:15:51,084][0m Trial 12 finished with value: 0.06186167183129684 and parameters: {'observation_period_num': 102, 'train_rates': 0.9759322609782476, 'learning_rate': 0.0008190979807545752, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7550779943536206}. Best is trial 12 with value: 0.06186167183129684.[0m
[32m[I 2025-01-07 11:22:32,622][0m Trial 13 finished with value: 0.061666291807928396 and parameters: {'observation_period_num': 71, 'train_rates': 0.9785850621909434, 'learning_rate': 0.000948746135126812, 'batch_size': 24, 'step_size': 1, 'gamma': 0.9140440802946591}. Best is trial 13 with value: 0.061666291807928396.[0m
[32m[I 2025-01-07 11:23:56,023][0m Trial 14 finished with value: 0.06596498759019942 and parameters: {'observation_period_num': 63, 'train_rates': 0.8767401448662071, 'learning_rate': 0.0003417491103704789, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9110261143225338}. Best is trial 13 with value: 0.061666291807928396.[0m
[32m[I 2025-01-07 11:27:00,528][0m Trial 15 finished with value: 0.08241477113207872 and parameters: {'observation_period_num': 82, 'train_rates': 0.8596403263367611, 'learning_rate': 0.000906499248762232, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9111280833836867}. Best is trial 13 with value: 0.061666291807928396.[0m
[32m[I 2025-01-07 11:28:53,439][0m Trial 16 finished with value: 0.07636687735264952 and parameters: {'observation_period_num': 63, 'train_rates': 0.9393871531555105, 'learning_rate': 0.0002885065485829349, 'batch_size': 172, 'step_size': 6, 'gamma': 0.9642511061306459}. Best is trial 13 with value: 0.061666291807928396.[0m
[32m[I 2025-01-07 11:33:36,760][0m Trial 17 finished with value: 0.07142026373936285 and parameters: {'observation_period_num': 111, 'train_rates': 0.9456740247331797, 'learning_rate': 7.39730611684805e-05, 'batch_size': 33, 'step_size': 2, 'gamma': 0.8928395588410809}. Best is trial 13 with value: 0.061666291807928396.[0m
[32m[I 2025-01-07 11:35:24,832][0m Trial 18 finished with value: 0.07270073038713815 and parameters: {'observation_period_num': 14, 'train_rates': 0.8481980090088942, 'learning_rate': 8.513583116441734e-06, 'batch_size': 221, 'step_size': 3, 'gamma': 0.9499764478826384}. Best is trial 13 with value: 0.061666291807928396.[0m
Early stopping at epoch 94
[32m[I 2025-01-07 11:37:47,089][0m Trial 19 finished with value: 0.05142042572023692 and parameters: {'observation_period_num': 41, 'train_rates': 0.9073332451320023, 'learning_rate': 0.0004265572069871881, 'batch_size': 65, 'step_size': 1, 'gamma': 0.8359981716423932}. Best is trial 19 with value: 0.05142042572023692.[0m
[32m[I 2025-01-07 11:40:18,053][0m Trial 20 finished with value: 0.17349726133166798 and parameters: {'observation_period_num': 33, 'train_rates': 0.9138517052840484, 'learning_rate': 1.0179897572902075e-06, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8405733713379974}. Best is trial 19 with value: 0.05142042572023692.[0m
[32m[I 2025-01-07 11:44:08,319][0m Trial 21 finished with value: 0.08178614476259718 and parameters: {'observation_period_num': 92, 'train_rates': 0.967421626941934, 'learning_rate': 0.0004861648906863484, 'batch_size': 41, 'step_size': 1, 'gamma': 0.8804605370075979}. Best is trial 19 with value: 0.05142042572023692.[0m
Early stopping at epoch 85
[32m[I 2025-01-07 11:48:13,002][0m Trial 22 finished with value: 0.05461157892796859 and parameters: {'observation_period_num': 42, 'train_rates': 0.8961316003898399, 'learning_rate': 0.0006148522630858264, 'batch_size': 32, 'step_size': 1, 'gamma': 0.818165059165889}. Best is trial 19 with value: 0.05142042572023692.[0m
[32m[I 2025-01-07 11:50:33,727][0m Trial 23 finished with value: 0.05792898747576288 and parameters: {'observation_period_num': 41, 'train_rates': 0.8860295273895693, 'learning_rate': 0.0002686144541353378, 'batch_size': 66, 'step_size': 3, 'gamma': 0.8256337431793239}. Best is trial 19 with value: 0.05142042572023692.[0m
[32m[I 2025-01-07 11:52:38,988][0m Trial 24 finished with value: 0.058757626267552016 and parameters: {'observation_period_num': 46, 'train_rates': 0.8870376376600637, 'learning_rate': 6.0662607374084414e-05, 'batch_size': 75, 'step_size': 3, 'gamma': 0.8156081768339366}. Best is trial 19 with value: 0.05142042572023692.[0m
[32m[I 2025-01-07 11:54:15,839][0m Trial 25 finished with value: 0.045910711139440535 and parameters: {'observation_period_num': 32, 'train_rates': 0.8374832414106979, 'learning_rate': 0.0002196934899592041, 'batch_size': 133, 'step_size': 3, 'gamma': 0.8311326480706019}. Best is trial 25 with value: 0.045910711139440535.[0m
[32m[I 2025-01-07 11:56:05,498][0m Trial 26 finished with value: 0.04492435778082595 and parameters: {'observation_period_num': 23, 'train_rates': 0.8275288736928998, 'learning_rate': 0.00017032814576884247, 'batch_size': 156, 'step_size': 5, 'gamma': 0.7913944085925007}. Best is trial 26 with value: 0.04492435778082595.[0m
[32m[I 2025-01-07 11:57:55,608][0m Trial 27 finished with value: 0.051722862399541415 and parameters: {'observation_period_num': 16, 'train_rates': 0.8364724187358126, 'learning_rate': 7.58868188894973e-05, 'batch_size': 148, 'step_size': 5, 'gamma': 0.7846016179927657}. Best is trial 26 with value: 0.04492435778082595.[0m
[32m[I 2025-01-07 11:59:34,650][0m Trial 28 finished with value: 0.040534247404444 and parameters: {'observation_period_num': 25, 'train_rates': 0.8291504845825987, 'learning_rate': 0.00017704824158251155, 'batch_size': 192, 'step_size': 6, 'gamma': 0.8460722852091568}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:00:57,887][0m Trial 29 finished with value: 0.19947357215290148 and parameters: {'observation_period_num': 22, 'train_rates': 0.7064108625493499, 'learning_rate': 1.4166705966115415e-05, 'batch_size': 196, 'step_size': 9, 'gamma': 0.7945697182364229}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:02:29,578][0m Trial 30 finished with value: 0.08514574352237914 and parameters: {'observation_period_num': 164, 'train_rates': 0.822559190656767, 'learning_rate': 0.00017118218364690755, 'batch_size': 209, 'step_size': 6, 'gamma': 0.8472769302130756}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:04:04,952][0m Trial 31 finished with value: 0.05866946074051586 and parameters: {'observation_period_num': 52, 'train_rates': 0.788005871474514, 'learning_rate': 0.00018303742945791575, 'batch_size': 163, 'step_size': 5, 'gamma': 0.8365915165868293}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:05:40,149][0m Trial 32 finished with value: 0.04295094615772635 and parameters: {'observation_period_num': 32, 'train_rates': 0.8668148922230741, 'learning_rate': 4.6095392074472645e-05, 'batch_size': 122, 'step_size': 6, 'gamma': 0.8835384308252258}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:07:13,049][0m Trial 33 finished with value: 0.04228446279603644 and parameters: {'observation_period_num': 27, 'train_rates': 0.8630432232670984, 'learning_rate': 4.5589805361107515e-05, 'batch_size': 128, 'step_size': 7, 'gamma': 0.8872440309860694}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:08:51,826][0m Trial 34 finished with value: 0.0456685913567567 and parameters: {'observation_period_num': 5, 'train_rates': 0.8650335601372089, 'learning_rate': 3.992433561939361e-05, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8899024684368737}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:10:59,589][0m Trial 35 finished with value: 0.04385302761750939 and parameters: {'observation_period_num': 26, 'train_rates': 0.8072143791992781, 'learning_rate': 4.585534128137823e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8786065727748931}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:12:36,689][0m Trial 36 finished with value: 0.176613813111897 and parameters: {'observation_period_num': 57, 'train_rates': 0.7206786233332513, 'learning_rate': 4.8914017306485e-05, 'batch_size': 108, 'step_size': 10, 'gamma': 0.8782547259674993}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:14:25,992][0m Trial 37 finished with value: 0.05427713902944306 and parameters: {'observation_period_num': 29, 'train_rates': 0.8023603631090737, 'learning_rate': 1.5278768802717592e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.9268732521423914}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:16:29,147][0m Trial 38 finished with value: 0.08364843200105904 and parameters: {'observation_period_num': 79, 'train_rates': 0.7989834917223646, 'learning_rate': 1.5976127640052754e-05, 'batch_size': 97, 'step_size': 7, 'gamma': 0.8556556154673327}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:18:05,453][0m Trial 39 finished with value: 0.1542807479123093 and parameters: {'observation_period_num': 9, 'train_rates': 0.7395746217683363, 'learning_rate': 0.00011982836481073169, 'batch_size': 138, 'step_size': 9, 'gamma': 0.89949320314406}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:19:43,967][0m Trial 40 finished with value: 0.20077187793147294 and parameters: {'observation_period_num': 5, 'train_rates': 0.7743621584520844, 'learning_rate': 7.107978329321749e-06, 'batch_size': 118, 'step_size': 10, 'gamma': 0.8716876876741483}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:21:14,437][0m Trial 41 finished with value: 0.04111253456017817 and parameters: {'observation_period_num': 23, 'train_rates': 0.8205438852984257, 'learning_rate': 9.760263758165303e-05, 'batch_size': 166, 'step_size': 6, 'gamma': 0.8666494738494487}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:22:42,729][0m Trial 42 finished with value: 0.042551406910386655 and parameters: {'observation_period_num': 27, 'train_rates': 0.813831015014323, 'learning_rate': 0.00010074903141099908, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8673601209123283}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:24:11,366][0m Trial 43 finished with value: 0.05998623059897483 and parameters: {'observation_period_num': 53, 'train_rates': 0.8639320984378934, 'learning_rate': 9.797491289935899e-05, 'batch_size': 180, 'step_size': 6, 'gamma': 0.8634611173894232}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:25:38,252][0m Trial 44 finished with value: 0.048308611299500905 and parameters: {'observation_period_num': 34, 'train_rates': 0.847005651856091, 'learning_rate': 9.173197903327889e-05, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8562152362361775}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:26:57,894][0m Trial 45 finished with value: 0.26347415218464076 and parameters: {'observation_period_num': 71, 'train_rates': 0.7836646565438838, 'learning_rate': 3.3988069112583604e-05, 'batch_size': 226, 'step_size': 8, 'gamma': 0.8985032376932612}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:28:16,744][0m Trial 46 finished with value: 0.05712282513352958 and parameters: {'observation_period_num': 15, 'train_rates': 0.8128207526143916, 'learning_rate': 2.122812490957462e-05, 'batch_size': 169, 'step_size': 7, 'gamma': 0.8678284646457655}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:29:31,716][0m Trial 47 finished with value: 0.16121143147947992 and parameters: {'observation_period_num': 47, 'train_rates': 0.6832412491093927, 'learning_rate': 0.00012343720207992165, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9261090985004453}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:30:47,213][0m Trial 48 finished with value: 0.19454231748499884 and parameters: {'observation_period_num': 20, 'train_rates': 0.7595206813969744, 'learning_rate': 5.934023162256838e-05, 'batch_size': 194, 'step_size': 4, 'gamma': 0.8465424965349361}. Best is trial 28 with value: 0.040534247404444.[0m
[32m[I 2025-01-07 12:31:53,479][0m Trial 49 finished with value: 0.3089691427749866 and parameters: {'observation_period_num': 153, 'train_rates': 0.6246756499819214, 'learning_rate': 3.075864187253838e-05, 'batch_size': 180, 'step_size': 11, 'gamma': 0.8898228535895242}. Best is trial 28 with value: 0.040534247404444.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 12:31:53,487][0m A new study created in memory with name: no-name-3752b20d-2a8f-4803-8e73-12004fd7143d[0m
[32m[I 2025-01-07 12:35:00,872][0m Trial 0 finished with value: 0.1481550128991939 and parameters: {'observation_period_num': 232, 'train_rates': 0.9555391011697154, 'learning_rate': 5.285374760524152e-06, 'batch_size': 49, 'step_size': 11, 'gamma': 0.9146688567724961}. Best is trial 0 with value: 0.1481550128991939.[0m
[32m[I 2025-01-07 12:36:12,702][0m Trial 1 finished with value: 0.21565496264135137 and parameters: {'observation_period_num': 127, 'train_rates': 0.6571596042452019, 'learning_rate': 9.800367713126468e-05, 'batch_size': 219, 'step_size': 14, 'gamma': 0.8154098054224768}. Best is trial 0 with value: 0.1481550128991939.[0m
[32m[I 2025-01-07 12:38:33,843][0m Trial 2 finished with value: 0.07837232393877847 and parameters: {'observation_period_num': 126, 'train_rates': 0.938173713923723, 'learning_rate': 5.519773873277158e-05, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9184603461412969}. Best is trial 2 with value: 0.07837232393877847.[0m
[32m[I 2025-01-07 12:40:53,586][0m Trial 3 finished with value: 0.2860497878100302 and parameters: {'observation_period_num': 229, 'train_rates': 0.6912311951223576, 'learning_rate': 9.349736341505966e-06, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9896804576554671}. Best is trial 2 with value: 0.07837232393877847.[0m
[32m[I 2025-01-07 12:42:25,805][0m Trial 4 finished with value: 0.2730461932718754 and parameters: {'observation_period_num': 130, 'train_rates': 0.7450578474866834, 'learning_rate': 3.4003307746067026e-05, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8301526091620456}. Best is trial 2 with value: 0.07837232393877847.[0m
[32m[I 2025-01-07 12:45:21,792][0m Trial 5 finished with value: 0.07347263160008419 and parameters: {'observation_period_num': 137, 'train_rates': 0.9160111295008904, 'learning_rate': 1.656722181958919e-05, 'batch_size': 57, 'step_size': 11, 'gamma': 0.8457954533368482}. Best is trial 5 with value: 0.07347263160008419.[0m
[32m[I 2025-01-07 12:46:58,152][0m Trial 6 finished with value: 0.37071035133027375 and parameters: {'observation_period_num': 175, 'train_rates': 0.713027775655886, 'learning_rate': 8.700452720578786e-06, 'batch_size': 187, 'step_size': 7, 'gamma': 0.8414942255986014}. Best is trial 5 with value: 0.07347263160008419.[0m
[32m[I 2025-01-07 12:49:01,838][0m Trial 7 finished with value: 0.38306716343630914 and parameters: {'observation_period_num': 222, 'train_rates': 0.7895175786421844, 'learning_rate': 5.275627532320502e-06, 'batch_size': 178, 'step_size': 4, 'gamma': 0.7712777954899803}. Best is trial 5 with value: 0.07347263160008419.[0m
[32m[I 2025-01-07 12:51:28,465][0m Trial 8 finished with value: 0.12399906136335866 and parameters: {'observation_period_num': 141, 'train_rates': 0.7796430006463705, 'learning_rate': 0.0007042792452143006, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8799326985146697}. Best is trial 5 with value: 0.07347263160008419.[0m
[32m[I 2025-01-07 12:53:05,177][0m Trial 9 finished with value: 0.4164042748030155 and parameters: {'observation_period_num': 96, 'train_rates': 0.6094895723168453, 'learning_rate': 3.056998165434771e-05, 'batch_size': 125, 'step_size': 1, 'gamma': 0.9478417593805109}. Best is trial 5 with value: 0.07347263160008419.[0m
[32m[I 2025-01-07 13:01:50,327][0m Trial 10 finished with value: 0.05694427699382816 and parameters: {'observation_period_num': 14, 'train_rates': 0.8809385855191033, 'learning_rate': 1.1502414422425614e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7545801628695771}. Best is trial 10 with value: 0.05694427699382816.[0m
[32m[I 2025-01-07 13:09:17,864][0m Trial 11 finished with value: 0.055263971790988395 and parameters: {'observation_period_num': 11, 'train_rates': 0.8722015132264559, 'learning_rate': 1.5113157080852224e-06, 'batch_size': 20, 'step_size': 15, 'gamma': 0.7578541288667393}. Best is trial 11 with value: 0.055263971790988395.[0m
[32m[I 2025-01-07 13:16:08,706][0m Trial 12 finished with value: 0.0636622901359687 and parameters: {'observation_period_num': 15, 'train_rates': 0.8667463301270242, 'learning_rate': 1.0097718606250752e-06, 'batch_size': 22, 'step_size': 15, 'gamma': 0.7508798728256512}. Best is trial 11 with value: 0.055263971790988395.[0m
[32m[I 2025-01-07 13:24:24,703][0m Trial 13 finished with value: 0.053563018424564755 and parameters: {'observation_period_num': 11, 'train_rates': 0.8569794182444044, 'learning_rate': 1.6423167098686802e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7895351728549972}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:26:37,541][0m Trial 14 finished with value: 0.15513234025479963 and parameters: {'observation_period_num': 61, 'train_rates': 0.8454956368438536, 'learning_rate': 2.400444296862501e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.7923884959331852}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:29:16,451][0m Trial 15 finished with value: 0.2125532031059265 and parameters: {'observation_period_num': 48, 'train_rates': 0.9883534101873229, 'learning_rate': 2.8178517187747308e-06, 'batch_size': 93, 'step_size': 8, 'gamma': 0.7966792442052404}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:30:53,364][0m Trial 16 finished with value: 0.05701865738582447 and parameters: {'observation_period_num': 56, 'train_rates': 0.8495146788212636, 'learning_rate': 0.00023765441086679416, 'batch_size': 253, 'step_size': 15, 'gamma': 0.7904646648983209}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:32:23,797][0m Trial 17 finished with value: 0.41495283101368874 and parameters: {'observation_period_num': 84, 'train_rates': 0.8206786372889361, 'learning_rate': 1.72291155144549e-06, 'batch_size': 157, 'step_size': 5, 'gamma': 0.8675454196293015}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:37:18,151][0m Trial 18 finished with value: 0.06647290894602753 and parameters: {'observation_period_num': 33, 'train_rates': 0.8997242212067049, 'learning_rate': 3.603857138286493e-06, 'batch_size': 32, 'step_size': 9, 'gamma': 0.7750326735221619}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:39:21,351][0m Trial 19 finished with value: 0.07859178487625387 and parameters: {'observation_period_num': 81, 'train_rates': 0.8121556143965576, 'learning_rate': 1.142361827655834e-05, 'batch_size': 93, 'step_size': 13, 'gamma': 0.8142366604291702}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:42:39,256][0m Trial 20 finished with value: 0.20223311597108842 and parameters: {'observation_period_num': 7, 'train_rates': 0.7456703481211461, 'learning_rate': 1.8202122867387312e-06, 'batch_size': 41, 'step_size': 13, 'gamma': 0.8677356077988782}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:51:18,088][0m Trial 21 finished with value: 0.06956533645145122 and parameters: {'observation_period_num': 27, 'train_rates': 0.8820226309621548, 'learning_rate': 1.001912663958753e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7506054689641557}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 13:58:11,625][0m Trial 22 finished with value: 0.07378061431356603 and parameters: {'observation_period_num': 32, 'train_rates': 0.9015374793162434, 'learning_rate': 1.3496679757065048e-06, 'batch_size': 22, 'step_size': 14, 'gamma': 0.767325115255299}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 14:00:35,997][0m Trial 23 finished with value: 0.056249865545676306 and parameters: {'observation_period_num': 5, 'train_rates': 0.8546054014362797, 'learning_rate': 4.191019351367152e-06, 'batch_size': 66, 'step_size': 15, 'gamma': 0.7744409647763533}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 14:03:01,119][0m Trial 24 finished with value: 0.07277785179911146 and parameters: {'observation_period_num': 47, 'train_rates': 0.8382046846711907, 'learning_rate': 4.505627310588653e-06, 'batch_size': 60, 'step_size': 14, 'gamma': 0.8087277781670883}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 14:05:12,240][0m Trial 25 finished with value: 0.0682197562483854 and parameters: {'observation_period_num': 5, 'train_rates': 0.9270869598728197, 'learning_rate': 2.6993214341062063e-06, 'batch_size': 72, 'step_size': 12, 'gamma': 0.7797824610318947}. Best is trial 13 with value: 0.053563018424564755.[0m
Early stopping at epoch 53
[32m[I 2025-01-07 14:06:03,603][0m Trial 26 finished with value: 0.5130567224945609 and parameters: {'observation_period_num': 66, 'train_rates': 0.7802176310797957, 'learning_rate': 6.6733793159894225e-06, 'batch_size': 106, 'step_size': 1, 'gamma': 0.798376493925916}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 14:09:56,363][0m Trial 27 finished with value: 0.16504323646262453 and parameters: {'observation_period_num': 103, 'train_rates': 0.9682696066261844, 'learning_rate': 2.058884331263563e-06, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8315877979076259}. Best is trial 13 with value: 0.053563018424564755.[0m
[32m[I 2025-01-07 14:14:31,140][0m Trial 28 finished with value: 0.04225369651196347 and parameters: {'observation_period_num': 30, 'train_rates': 0.8201543475096928, 'learning_rate': 1.9692835074467553e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.7667742400029454}. Best is trial 28 with value: 0.04225369651196347.[0m
[32m[I 2025-01-07 14:17:58,003][0m Trial 29 finished with value: 0.08809385665721123 and parameters: {'observation_period_num': 40, 'train_rates': 0.8109532804783387, 'learning_rate': 0.0001476119946351994, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9018437919488645}. Best is trial 28 with value: 0.04225369651196347.[0m
[32m[I 2025-01-07 14:19:16,979][0m Trial 30 finished with value: 0.3265708653439938 and parameters: {'observation_period_num': 185, 'train_rates': 0.7436165312211364, 'learning_rate': 2.3257595963104893e-05, 'batch_size': 140, 'step_size': 13, 'gamma': 0.7659084862185265}. Best is trial 28 with value: 0.04225369651196347.[0m
[32m[I 2025-01-07 14:23:28,837][0m Trial 31 finished with value: 0.052979507822420155 and parameters: {'observation_period_num': 25, 'train_rates': 0.8658051568941733, 'learning_rate': 3.3457887003344276e-06, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7817543146976297}. Best is trial 28 with value: 0.04225369651196347.[0m
[32m[I 2025-01-07 14:28:04,148][0m Trial 32 finished with value: 0.03715270296063112 and parameters: {'observation_period_num': 24, 'train_rates': 0.8822884708058532, 'learning_rate': 1.4304106375958219e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7837502172422082}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:31:23,857][0m Trial 33 finished with value: 0.04069439610787507 and parameters: {'observation_period_num': 26, 'train_rates': 0.9506973437240428, 'learning_rate': 6.823798693283742e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.7846329077407792}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:34:44,917][0m Trial 34 finished with value: 0.07876442959074115 and parameters: {'observation_period_num': 70, 'train_rates': 0.936583959265828, 'learning_rate': 6.558028261558314e-05, 'batch_size': 48, 'step_size': 12, 'gamma': 0.8115757516295085}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:37:22,038][0m Trial 35 finished with value: 0.04672137878157876 and parameters: {'observation_period_num': 26, 'train_rates': 0.9625939023960167, 'learning_rate': 5.3513700242276416e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8230087403533656}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:39:43,434][0m Trial 36 finished with value: 0.12403606725366492 and parameters: {'observation_period_num': 250, 'train_rates': 0.9650774792347128, 'learning_rate': 4.7687169972928066e-05, 'batch_size': 88, 'step_size': 11, 'gamma': 0.8246072871930322}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:42:47,024][0m Trial 37 finished with value: 0.0859602394585426 and parameters: {'observation_period_num': 114, 'train_rates': 0.9498352715088252, 'learning_rate': 8.936298255257115e-05, 'batch_size': 55, 'step_size': 14, 'gamma': 0.8055558047432424}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:45:11,716][0m Trial 38 finished with value: 0.05802743136882782 and parameters: {'observation_period_num': 49, 'train_rates': 0.9849241091568841, 'learning_rate': 1.9335497791358197e-05, 'batch_size': 81, 'step_size': 12, 'gamma': 0.846539082578734}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:47:29,261][0m Trial 39 finished with value: 0.062409350355922946 and parameters: {'observation_period_num': 75, 'train_rates': 0.9095546626886454, 'learning_rate': 0.00014729286918704006, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8326809320680074}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:50:13,208][0m Trial 40 finished with value: 0.0470485240607064 and parameters: {'observation_period_num': 25, 'train_rates': 0.9343535391688349, 'learning_rate': 1.3764053526058974e-05, 'batch_size': 65, 'step_size': 13, 'gamma': 0.8204492213210005}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:53:00,971][0m Trial 41 finished with value: 0.05364321314280839 and parameters: {'observation_period_num': 24, 'train_rates': 0.9528150973282736, 'learning_rate': 1.3572876432099183e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8545370276184181}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:56:04,290][0m Trial 42 finished with value: 0.03896602885558819 and parameters: {'observation_period_num': 36, 'train_rates': 0.9199275400608273, 'learning_rate': 3.5747117409899106e-05, 'batch_size': 51, 'step_size': 14, 'gamma': 0.8206926603534161}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 14:59:14,155][0m Trial 43 finished with value: 0.051070647699294716 and parameters: {'observation_period_num': 39, 'train_rates': 0.9188076930123472, 'learning_rate': 2.9408462045302813e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.800222043277436}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 15:03:51,945][0m Trial 44 finished with value: 0.05982066326491211 and parameters: {'observation_period_num': 41, 'train_rates': 0.8973488559775579, 'learning_rate': 4.7384870604347246e-05, 'batch_size': 33, 'step_size': 12, 'gamma': 0.7820415409900378}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 15:07:02,197][0m Trial 45 finished with value: 0.12189208394424482 and parameters: {'observation_period_num': 159, 'train_rates': 0.968773806911138, 'learning_rate': 3.959273294112489e-05, 'batch_size': 50, 'step_size': 14, 'gamma': 0.7632617180704653}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 15:09:20,822][0m Trial 46 finished with value: 0.0726924758495354 and parameters: {'observation_period_num': 58, 'train_rates': 0.8867160851323425, 'learning_rate': 7.08749553972089e-05, 'batch_size': 80, 'step_size': 6, 'gamma': 0.9709683854777535}. Best is trial 32 with value: 0.03715270296063112.[0m
[32m[I 2025-01-07 15:14:50,547][0m Trial 47 finished with value: 0.03450246174223052 and parameters: {'observation_period_num': 20, 'train_rates': 0.9430792814665933, 'learning_rate': 2.1129031057999455e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8547360256999255}. Best is trial 47 with value: 0.03450246174223052.[0m
[32m[I 2025-01-07 15:19:44,210][0m Trial 48 finished with value: 0.06788189029457176 and parameters: {'observation_period_num': 93, 'train_rates': 0.8326382526785376, 'learning_rate': 7.494095635141934e-06, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8843439081274713}. Best is trial 47 with value: 0.03450246174223052.[0m
[32m[I 2025-01-07 15:24:08,249][0m Trial 49 finished with value: 0.14189819090636988 and parameters: {'observation_period_num': 19, 'train_rates': 0.6069399425569595, 'learning_rate': 2.4023521170361106e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8460016090142853}. Best is trial 47 with value: 0.03450246174223052.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 15:24:08,257][0m A new study created in memory with name: no-name-2b9d06d7-f192-403e-87e0-3a725e30a178[0m
[32m[I 2025-01-07 15:26:27,617][0m Trial 0 finished with value: 0.12035027074629097 and parameters: {'observation_period_num': 59, 'train_rates': 0.7883977383095405, 'learning_rate': 2.9904634227992168e-06, 'batch_size': 63, 'step_size': 2, 'gamma': 0.9792690701341371}. Best is trial 0 with value: 0.12035027074629097.[0m
[32m[I 2025-01-07 15:28:20,594][0m Trial 1 finished with value: 0.24725559572963154 and parameters: {'observation_period_num': 51, 'train_rates': 0.7893013025908557, 'learning_rate': 1.1311482267709015e-06, 'batch_size': 82, 'step_size': 8, 'gamma': 0.7876871068218351}. Best is trial 0 with value: 0.12035027074629097.[0m
[32m[I 2025-01-07 15:33:58,896][0m Trial 2 finished with value: 0.16560270529878032 and parameters: {'observation_period_num': 174, 'train_rates': 0.8801801608742188, 'learning_rate': 0.0002469337111997769, 'batch_size': 25, 'step_size': 8, 'gamma': 0.8562758642387742}. Best is trial 0 with value: 0.12035027074629097.[0m
[32m[I 2025-01-07 15:35:44,850][0m Trial 3 finished with value: 0.09927359968423843 and parameters: {'observation_period_num': 57, 'train_rates': 0.9736612503474965, 'learning_rate': 1.5498884291752907e-05, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8764594445160325}. Best is trial 3 with value: 0.09927359968423843.[0m
[32m[I 2025-01-07 15:36:58,115][0m Trial 4 finished with value: 0.5487019255269384 and parameters: {'observation_period_num': 234, 'train_rates': 0.7082356516932432, 'learning_rate': 1.3299057561513026e-06, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9146871094659934}. Best is trial 3 with value: 0.09927359968423843.[0m
[32m[I 2025-01-07 15:38:24,247][0m Trial 5 finished with value: 0.295411348095962 and parameters: {'observation_period_num': 183, 'train_rates': 0.6843145153809288, 'learning_rate': 0.0008048315517544737, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8256875025123233}. Best is trial 3 with value: 0.09927359968423843.[0m
[32m[I 2025-01-07 15:40:43,102][0m Trial 6 finished with value: 0.08780719829301764 and parameters: {'observation_period_num': 8, 'train_rates': 0.8140458361812772, 'learning_rate': 1.681669461811799e-06, 'batch_size': 64, 'step_size': 13, 'gamma': 0.7560117617166539}. Best is trial 6 with value: 0.08780719829301764.[0m
[32m[I 2025-01-07 15:42:06,835][0m Trial 7 finished with value: 0.10835819869066897 and parameters: {'observation_period_num': 67, 'train_rates': 0.9026840762382449, 'learning_rate': 0.0005660867422121308, 'batch_size': 229, 'step_size': 13, 'gamma': 0.9436789078981269}. Best is trial 6 with value: 0.08780719829301764.[0m
[32m[I 2025-01-07 15:43:37,916][0m Trial 8 finished with value: 0.06257849456509045 and parameters: {'observation_period_num': 82, 'train_rates': 0.8471245535809908, 'learning_rate': 0.0007900519810913161, 'batch_size': 146, 'step_size': 4, 'gamma': 0.8479969794126694}. Best is trial 8 with value: 0.06257849456509045.[0m
[32m[I 2025-01-07 15:45:18,018][0m Trial 9 finished with value: 0.1362050289372534 and parameters: {'observation_period_num': 100, 'train_rates': 0.7988516847202288, 'learning_rate': 1.0467015272969274e-05, 'batch_size': 132, 'step_size': 4, 'gamma': 0.8985301527613184}. Best is trial 8 with value: 0.06257849456509045.[0m
[32m[I 2025-01-07 15:46:36,815][0m Trial 10 finished with value: 0.20167758925132048 and parameters: {'observation_period_num': 128, 'train_rates': 0.6348927464021353, 'learning_rate': 0.00014790926661635686, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8263876486671901}. Best is trial 8 with value: 0.06257849456509045.[0m
[32m[I 2025-01-07 15:52:04,414][0m Trial 11 finished with value: 0.029507846356584475 and parameters: {'observation_period_num': 9, 'train_rates': 0.8412087684153158, 'learning_rate': 4.366899460652889e-05, 'batch_size': 27, 'step_size': 11, 'gamma': 0.7600497476029823}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 15:53:46,970][0m Trial 12 finished with value: 0.045402102229834214 and parameters: {'observation_period_num': 17, 'train_rates': 0.867667767261797, 'learning_rate': 6.600444591693058e-05, 'batch_size': 171, 'step_size': 10, 'gamma': 0.7614095919301748}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 15:55:44,676][0m Trial 13 finished with value: 0.0581432580947876 and parameters: {'observation_period_num': 12, 'train_rates': 0.951481050553493, 'learning_rate': 6.600629326499041e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.7696199658056789}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 15:57:24,564][0m Trial 14 finished with value: 0.052331225263129034 and parameters: {'observation_period_num': 22, 'train_rates': 0.9053684936605355, 'learning_rate': 3.928262795821616e-05, 'batch_size': 196, 'step_size': 10, 'gamma': 0.7974968007582439}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:03:39,061][0m Trial 15 finished with value: 0.17207533357521662 and parameters: {'observation_period_num': 34, 'train_rates': 0.7417704384480487, 'learning_rate': 8.198689333493169e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.7518831925514409}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:05:09,180][0m Trial 16 finished with value: 0.11882375446084427 and parameters: {'observation_period_num': 113, 'train_rates': 0.8458413501555914, 'learning_rate': 1.0512668670198415e-05, 'batch_size': 182, 'step_size': 11, 'gamma': 0.7973574523100997}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:06:33,174][0m Trial 17 finished with value: 0.13562576472759247 and parameters: {'observation_period_num': 161, 'train_rates': 0.938651130698648, 'learning_rate': 2.561719718387029e-05, 'batch_size': 218, 'step_size': 6, 'gamma': 0.8317938571026051}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:07:58,369][0m Trial 18 finished with value: 0.3196144305908124 and parameters: {'observation_period_num': 249, 'train_rates': 0.7463600360674268, 'learning_rate': 0.00024414877403256956, 'batch_size': 131, 'step_size': 15, 'gamma': 0.7849588850672622}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:10:58,752][0m Trial 19 finished with value: 0.052628382476284985 and parameters: {'observation_period_num': 39, 'train_rates': 0.8688214361443326, 'learning_rate': 4.573387348175233e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.8061249082684142}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:12:34,125][0m Trial 20 finished with value: 0.20942394565249875 and parameters: {'observation_period_num': 94, 'train_rates': 0.8314538388427476, 'learning_rate': 4.431330241549862e-06, 'batch_size': 152, 'step_size': 7, 'gamma': 0.7726410900786849}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:14:15,054][0m Trial 21 finished with value: 0.054866900620982054 and parameters: {'observation_period_num': 22, 'train_rates': 0.9183029469411931, 'learning_rate': 2.6432498446952513e-05, 'batch_size': 197, 'step_size': 10, 'gamma': 0.7500017213884584}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:15:54,435][0m Trial 22 finished with value: 0.03724622116163836 and parameters: {'observation_period_num': 10, 'train_rates': 0.8906693688631429, 'learning_rate': 0.000111610584171853, 'batch_size': 169, 'step_size': 9, 'gamma': 0.796720040712569}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:17:45,852][0m Trial 23 finished with value: 0.04518526498072162 and parameters: {'observation_period_num': 37, 'train_rates': 0.8854287910306663, 'learning_rate': 0.00011504880892696555, 'batch_size': 115, 'step_size': 9, 'gamma': 0.7739076961849216}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:19:40,964][0m Trial 24 finished with value: 0.07830566167831421 and parameters: {'observation_period_num': 76, 'train_rates': 0.9882312460337954, 'learning_rate': 0.00013703897766840017, 'batch_size': 108, 'step_size': 9, 'gamma': 0.80660711929415}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:23:05,635][0m Trial 25 finished with value: 0.05907367551584303 and parameters: {'observation_period_num': 38, 'train_rates': 0.8889600504219459, 'learning_rate': 0.0003553234340181816, 'batch_size': 46, 'step_size': 7, 'gamma': 0.7813821835134642}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:25:23,221][0m Trial 26 finished with value: 0.029980739043589603 and parameters: {'observation_period_num': 7, 'train_rates': 0.9363646009914096, 'learning_rate': 0.0001175745403537863, 'batch_size': 90, 'step_size': 11, 'gamma': 0.812501235719366}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:27:34,841][0m Trial 27 finished with value: 0.036284979862662464 and parameters: {'observation_period_num': 8, 'train_rates': 0.9517334175543776, 'learning_rate': 0.00019117263610765096, 'batch_size': 85, 'step_size': 12, 'gamma': 0.8154607031566868}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:30:13,169][0m Trial 28 finished with value: 0.033317779935896394 and parameters: {'observation_period_num': 5, 'train_rates': 0.9458551410876088, 'learning_rate': 0.00022677424055344048, 'batch_size': 88, 'step_size': 12, 'gamma': 0.8185190848278753}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:32:34,876][0m Trial 29 finished with value: 0.07315623674541712 and parameters: {'observation_period_num': 51, 'train_rates': 0.9312284081737697, 'learning_rate': 1.7496651334190707e-05, 'batch_size': 73, 'step_size': 14, 'gamma': 0.9782473411844647}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:36:04,372][0m Trial 30 finished with value: 0.0393349493543307 and parameters: {'observation_period_num': 31, 'train_rates': 0.9795199161269815, 'learning_rate': 0.0003251886688170392, 'batch_size': 49, 'step_size': 12, 'gamma': 0.8450580473674182}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:38:27,211][0m Trial 31 finished with value: 0.031231941522232125 and parameters: {'observation_period_num': 9, 'train_rates': 0.9526298166645899, 'learning_rate': 0.00019849456465688292, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8180560778370157}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:40:40,033][0m Trial 32 finished with value: 0.032658337224875725 and parameters: {'observation_period_num': 5, 'train_rates': 0.9581743352680466, 'learning_rate': 0.00047187917756985446, 'batch_size': 88, 'step_size': 14, 'gamma': 0.8704960908841025}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:45:05,238][0m Trial 33 finished with value: 0.062235522939234365 and parameters: {'observation_period_num': 48, 'train_rates': 0.9663981815639774, 'learning_rate': 0.0004933195435949058, 'batch_size': 36, 'step_size': 14, 'gamma': 0.8693158800060774}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:47:35,467][0m Trial 34 finished with value: 0.07967107916871706 and parameters: {'observation_period_num': 64, 'train_rates': 0.9224186781552722, 'learning_rate': 0.0004652691483626818, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8734840543202285}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:51:40,736][0m Trial 35 finished with value: 0.17636114197650127 and parameters: {'observation_period_num': 27, 'train_rates': 0.7606949114311046, 'learning_rate': 7.828424303971772e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9018363558662558}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:53:39,390][0m Trial 36 finished with value: 0.1310772052874752 and parameters: {'observation_period_num': 214, 'train_rates': 0.9630821898208691, 'learning_rate': 0.00017437798012134048, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8898105105127262}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:55:31,016][0m Trial 37 finished with value: 0.06943430006504059 and parameters: {'observation_period_num': 50, 'train_rates': 0.9881648446748235, 'learning_rate': 5.0225781640879125e-05, 'batch_size': 117, 'step_size': 15, 'gamma': 0.8555958421572294}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 16:58:17,050][0m Trial 38 finished with value: 0.1284779082622206 and parameters: {'observation_period_num': 156, 'train_rates': 0.9153689674658316, 'learning_rate': 0.0003024436368231027, 'batch_size': 56, 'step_size': 11, 'gamma': 0.926385323095985}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 17:00:15,324][0m Trial 39 finished with value: 0.19698280785493494 and parameters: {'observation_period_num': 24, 'train_rates': 0.7806628259996795, 'learning_rate': 0.0007476376808731461, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8418494782593083}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 17:01:58,334][0m Trial 40 finished with value: 0.054027607788157184 and parameters: {'observation_period_num': 73, 'train_rates': 0.8224121671614041, 'learning_rate': 0.00010800547871419446, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8618976243957448}. Best is trial 11 with value: 0.029507846356584475.[0m
[32m[I 2025-01-07 17:04:20,144][0m Trial 41 finished with value: 0.026556115794707746 and parameters: {'observation_period_num': 6, 'train_rates': 0.942632524997232, 'learning_rate': 0.00023909995625234355, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8157585247260108}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:13:39,007][0m Trial 42 finished with value: 0.04580681668206429 and parameters: {'observation_period_num': 21, 'train_rates': 0.9674518070762895, 'learning_rate': 0.00039127189326619913, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8854872629536157}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:16:32,981][0m Trial 43 finished with value: 0.08477456562898376 and parameters: {'observation_period_num': 45, 'train_rates': 0.9319795172304395, 'learning_rate': 0.0006458523029922771, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8366701231720775}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:18:36,165][0m Trial 44 finished with value: 0.030653379892437462 and parameters: {'observation_period_num': 5, 'train_rates': 0.8553402639066704, 'learning_rate': 0.000190841067167255, 'batch_size': 102, 'step_size': 12, 'gamma': 0.8155422760492348}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:20:29,187][0m Trial 45 finished with value: 0.0563720744493958 and parameters: {'observation_period_num': 59, 'train_rates': 0.8519008876879122, 'learning_rate': 0.0001780683967588592, 'batch_size': 102, 'step_size': 12, 'gamma': 0.813378663512556}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:22:21,416][0m Trial 46 finished with value: 0.035831627994775773 and parameters: {'observation_period_num': 18, 'train_rates': 0.8672533589060044, 'learning_rate': 9.681616250186897e-05, 'batch_size': 127, 'step_size': 11, 'gamma': 0.8247954170911406}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:24:25,525][0m Trial 47 finished with value: 0.03183537956532322 and parameters: {'observation_period_num': 15, 'train_rates': 0.8085606005286128, 'learning_rate': 0.0009475030956403595, 'batch_size': 146, 'step_size': 8, 'gamma': 0.764749441766101}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:26:30,872][0m Trial 48 finished with value: 0.15213777236680381 and parameters: {'observation_period_num': 29, 'train_rates': 0.6808729180949058, 'learning_rate': 3.6189154502811656e-05, 'batch_size': 79, 'step_size': 13, 'gamma': 0.7917482155340533}. Best is trial 41 with value: 0.026556115794707746.[0m
[32m[I 2025-01-07 17:28:23,629][0m Trial 49 finished with value: 0.08525963752544012 and parameters: {'observation_period_num': 190, 'train_rates': 0.9002442788839317, 'learning_rate': 5.895426263002645e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.8054885414535512}. Best is trial 41 with value: 0.026556115794707746.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.8928646906733584, 'learning_rate': 0.00031820913256764356, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9416656566011461}
Epoch 1/300, trend Loss: 0.3679 | 0.1677
Epoch 2/300, trend Loss: 0.1684 | 0.1162
Epoch 3/300, trend Loss: 0.1413 | 0.0900
Epoch 4/300, trend Loss: 0.1288 | 0.0753
Epoch 5/300, trend Loss: 0.1215 | 0.0680
Epoch 6/300, trend Loss: 0.1159 | 0.0657
Epoch 7/300, trend Loss: 0.1122 | 0.0654
Epoch 8/300, trend Loss: 0.1099 | 0.0692
Epoch 9/300, trend Loss: 0.1086 | 0.0766
Epoch 10/300, trend Loss: 0.1079 | 0.1006
Epoch 11/300, trend Loss: 0.1077 | 0.1155
Epoch 12/300, trend Loss: 0.1066 | 0.1183
Epoch 13/300, trend Loss: 0.1050 | 0.1159
Epoch 14/300, trend Loss: 0.1033 | 0.1074
Epoch 15/300, trend Loss: 0.1020 | 0.0982
Epoch 16/300, trend Loss: 0.1015 | 0.0814
Epoch 17/300, trend Loss: 0.1019 | 0.0674
Epoch 18/300, trend Loss: 0.1041 | 0.0537
Epoch 19/300, trend Loss: 0.1065 | 0.0549
Epoch 20/300, trend Loss: 0.1006 | 0.0536
Epoch 21/300, trend Loss: 0.0925 | 0.0526
Epoch 22/300, trend Loss: 0.0900 | 0.0511
Epoch 23/300, trend Loss: 0.0886 | 0.0498
Epoch 24/300, trend Loss: 0.0872 | 0.0487
Epoch 25/300, trend Loss: 0.0859 | 0.0472
Epoch 26/300, trend Loss: 0.0848 | 0.0465
Epoch 27/300, trend Loss: 0.0839 | 0.0460
Epoch 28/300, trend Loss: 0.0830 | 0.0449
Epoch 29/300, trend Loss: 0.0823 | 0.0447
Epoch 30/300, trend Loss: 0.0816 | 0.0448
Epoch 31/300, trend Loss: 0.0808 | 0.0438
Epoch 32/300, trend Loss: 0.0802 | 0.0439
Epoch 33/300, trend Loss: 0.0796 | 0.0440
Epoch 34/300, trend Loss: 0.0790 | 0.0430
Epoch 35/300, trend Loss: 0.0785 | 0.0430
Epoch 36/300, trend Loss: 0.0780 | 0.0431
Epoch 37/300, trend Loss: 0.0774 | 0.0420
Epoch 38/300, trend Loss: 0.0769 | 0.0419
Epoch 39/300, trend Loss: 0.0764 | 0.0419
Epoch 40/300, trend Loss: 0.0759 | 0.0408
Epoch 41/300, trend Loss: 0.0754 | 0.0406
Epoch 42/300, trend Loss: 0.0750 | 0.0405
Epoch 43/300, trend Loss: 0.0745 | 0.0395
Epoch 44/300, trend Loss: 0.0741 | 0.0393
Epoch 45/300, trend Loss: 0.0737 | 0.0391
Epoch 46/300, trend Loss: 0.0733 | 0.0383
Epoch 47/300, trend Loss: 0.0729 | 0.0381
Epoch 48/300, trend Loss: 0.0726 | 0.0379
Epoch 49/300, trend Loss: 0.0722 | 0.0372
Epoch 50/300, trend Loss: 0.0719 | 0.0370
Epoch 51/300, trend Loss: 0.0716 | 0.0369
Epoch 52/300, trend Loss: 0.0713 | 0.0362
Epoch 53/300, trend Loss: 0.0710 | 0.0361
Epoch 54/300, trend Loss: 0.0708 | 0.0359
Epoch 55/300, trend Loss: 0.0705 | 0.0354
Epoch 56/300, trend Loss: 0.0702 | 0.0352
Epoch 57/300, trend Loss: 0.0700 | 0.0350
Epoch 58/300, trend Loss: 0.0697 | 0.0345
Epoch 59/300, trend Loss: 0.0694 | 0.0344
Epoch 60/300, trend Loss: 0.0692 | 0.0342
Epoch 61/300, trend Loss: 0.0689 | 0.0339
Epoch 62/300, trend Loss: 0.0687 | 0.0338
Epoch 63/300, trend Loss: 0.0684 | 0.0337
Epoch 64/300, trend Loss: 0.0682 | 0.0335
Epoch 65/300, trend Loss: 0.0680 | 0.0334
Epoch 66/300, trend Loss: 0.0678 | 0.0333
Epoch 67/300, trend Loss: 0.0676 | 0.0333
Epoch 68/300, trend Loss: 0.0674 | 0.0332
Epoch 69/300, trend Loss: 0.0672 | 0.0332
Epoch 70/300, trend Loss: 0.0670 | 0.0332
Epoch 71/300, trend Loss: 0.0669 | 0.0331
Epoch 72/300, trend Loss: 0.0667 | 0.0331
Epoch 73/300, trend Loss: 0.0665 | 0.0330
Epoch 74/300, trend Loss: 0.0664 | 0.0329
Epoch 75/300, trend Loss: 0.0662 | 0.0327
Epoch 76/300, trend Loss: 0.0660 | 0.0325
Epoch 77/300, trend Loss: 0.0659 | 0.0323
Epoch 78/300, trend Loss: 0.0657 | 0.0321
Epoch 79/300, trend Loss: 0.0656 | 0.0318
Epoch 80/300, trend Loss: 0.0654 | 0.0316
Epoch 81/300, trend Loss: 0.0653 | 0.0315
Epoch 82/300, trend Loss: 0.0651 | 0.0311
Epoch 83/300, trend Loss: 0.0650 | 0.0310
Epoch 84/300, trend Loss: 0.0649 | 0.0308
Epoch 85/300, trend Loss: 0.0648 | 0.0305
Epoch 86/300, trend Loss: 0.0647 | 0.0304
Epoch 87/300, trend Loss: 0.0645 | 0.0303
Epoch 88/300, trend Loss: 0.0644 | 0.0300
Epoch 89/300, trend Loss: 0.0643 | 0.0300
Epoch 90/300, trend Loss: 0.0642 | 0.0299
Epoch 91/300, trend Loss: 0.0641 | 0.0297
Epoch 92/300, trend Loss: 0.0640 | 0.0297
Epoch 93/300, trend Loss: 0.0639 | 0.0296
Epoch 94/300, trend Loss: 0.0638 | 0.0295
Epoch 95/300, trend Loss: 0.0637 | 0.0295
Epoch 96/300, trend Loss: 0.0636 | 0.0294
Epoch 97/300, trend Loss: 0.0635 | 0.0294
Epoch 98/300, trend Loss: 0.0634 | 0.0295
Epoch 99/300, trend Loss: 0.0634 | 0.0295
Epoch 100/300, trend Loss: 0.0633 | 0.0296
Epoch 101/300, trend Loss: 0.0632 | 0.0297
Epoch 102/300, trend Loss: 0.0632 | 0.0297
Epoch 103/300, trend Loss: 0.0631 | 0.0300
Epoch 104/300, trend Loss: 0.0631 | 0.0300
Epoch 105/300, trend Loss: 0.0630 | 0.0300
Epoch 106/300, trend Loss: 0.0629 | 0.0301
Epoch 107/300, trend Loss: 0.0629 | 0.0301
Epoch 108/300, trend Loss: 0.0628 | 0.0300
Epoch 109/300, trend Loss: 0.0628 | 0.0301
Epoch 110/300, trend Loss: 0.0627 | 0.0300
Epoch 111/300, trend Loss: 0.0626 | 0.0300
Epoch 112/300, trend Loss: 0.0626 | 0.0300
Epoch 113/300, trend Loss: 0.0625 | 0.0300
Epoch 114/300, trend Loss: 0.0625 | 0.0299
Epoch 115/300, trend Loss: 0.0624 | 0.0300
Epoch 116/300, trend Loss: 0.0623 | 0.0299
Epoch 117/300, trend Loss: 0.0623 | 0.0299
Epoch 118/300, trend Loss: 0.0622 | 0.0299
Epoch 119/300, trend Loss: 0.0622 | 0.0298
Epoch 120/300, trend Loss: 0.0621 | 0.0298
Epoch 121/300, trend Loss: 0.0620 | 0.0297
Epoch 122/300, trend Loss: 0.0619 | 0.0297
Epoch 123/300, trend Loss: 0.0619 | 0.0297
Epoch 124/300, trend Loss: 0.0618 | 0.0296
Epoch 125/300, trend Loss: 0.0617 | 0.0296
Epoch 126/300, trend Loss: 0.0617 | 0.0296
Epoch 127/300, trend Loss: 0.0616 | 0.0296
Epoch 128/300, trend Loss: 0.0615 | 0.0296
Epoch 129/300, trend Loss: 0.0615 | 0.0296
Epoch 130/300, trend Loss: 0.0614 | 0.0295
Epoch 131/300, trend Loss: 0.0614 | 0.0296
Epoch 132/300, trend Loss: 0.0613 | 0.0296
Epoch 133/300, trend Loss: 0.0613 | 0.0295
Epoch 134/300, trend Loss: 0.0612 | 0.0295
Epoch 135/300, trend Loss: 0.0612 | 0.0295
Epoch 136/300, trend Loss: 0.0611 | 0.0295
Epoch 137/300, trend Loss: 0.0611 | 0.0295
Epoch 138/300, trend Loss: 0.0610 | 0.0295
Epoch 139/300, trend Loss: 0.0610 | 0.0295
Epoch 140/300, trend Loss: 0.0609 | 0.0295
Epoch 141/300, trend Loss: 0.0609 | 0.0295
Epoch 142/300, trend Loss: 0.0609 | 0.0294
Epoch 143/300, trend Loss: 0.0608 | 0.0294
Epoch 144/300, trend Loss: 0.0608 | 0.0294
Epoch 145/300, trend Loss: 0.0607 | 0.0293
Epoch 146/300, trend Loss: 0.0607 | 0.0293
Epoch 147/300, trend Loss: 0.0607 | 0.0293
Epoch 148/300, trend Loss: 0.0606 | 0.0292
Epoch 149/300, trend Loss: 0.0606 | 0.0292
Epoch 150/300, trend Loss: 0.0606 | 0.0292
Epoch 151/300, trend Loss: 0.0605 | 0.0291
Epoch 152/300, trend Loss: 0.0605 | 0.0291
Epoch 153/300, trend Loss: 0.0605 | 0.0291
Epoch 154/300, trend Loss: 0.0604 | 0.0290
Epoch 155/300, trend Loss: 0.0604 | 0.0290
Epoch 156/300, trend Loss: 0.0604 | 0.0290
Epoch 157/300, trend Loss: 0.0603 | 0.0289
Epoch 158/300, trend Loss: 0.0603 | 0.0289
Epoch 159/300, trend Loss: 0.0603 | 0.0289
Epoch 160/300, trend Loss: 0.0602 | 0.0288
Epoch 161/300, trend Loss: 0.0602 | 0.0288
Epoch 162/300, trend Loss: 0.0602 | 0.0288
Epoch 163/300, trend Loss: 0.0602 | 0.0287
Epoch 164/300, trend Loss: 0.0601 | 0.0287
Epoch 165/300, trend Loss: 0.0601 | 0.0287
Epoch 166/300, trend Loss: 0.0601 | 0.0287
Epoch 167/300, trend Loss: 0.0601 | 0.0286
Epoch 168/300, trend Loss: 0.0600 | 0.0286
Epoch 169/300, trend Loss: 0.0600 | 0.0286
Epoch 170/300, trend Loss: 0.0600 | 0.0286
Epoch 171/300, trend Loss: 0.0599 | 0.0286
Epoch 172/300, trend Loss: 0.0599 | 0.0286
Epoch 173/300, trend Loss: 0.0599 | 0.0285
Epoch 174/300, trend Loss: 0.0599 | 0.0285
Epoch 175/300, trend Loss: 0.0598 | 0.0286
Epoch 176/300, trend Loss: 0.0598 | 0.0285
Epoch 177/300, trend Loss: 0.0598 | 0.0285
Epoch 178/300, trend Loss: 0.0598 | 0.0286
Epoch 179/300, trend Loss: 0.0597 | 0.0286
Epoch 180/300, trend Loss: 0.0597 | 0.0286
Epoch 181/300, trend Loss: 0.0597 | 0.0286
Epoch 182/300, trend Loss: 0.0596 | 0.0286
Epoch 183/300, trend Loss: 0.0596 | 0.0286
Epoch 184/300, trend Loss: 0.0596 | 0.0287
Epoch 185/300, trend Loss: 0.0596 | 0.0287
Epoch 186/300, trend Loss: 0.0596 | 0.0287
Epoch 187/300, trend Loss: 0.0595 | 0.0287
Epoch 188/300, trend Loss: 0.0595 | 0.0287
Epoch 189/300, trend Loss: 0.0595 | 0.0287
Epoch 190/300, trend Loss: 0.0595 | 0.0287
Epoch 191/300, trend Loss: 0.0594 | 0.0287
Epoch 192/300, trend Loss: 0.0594 | 0.0287
Epoch 193/300, trend Loss: 0.0594 | 0.0287
Epoch 194/300, trend Loss: 0.0594 | 0.0287
Epoch 195/300, trend Loss: 0.0594 | 0.0287
Epoch 196/300, trend Loss: 0.0593 | 0.0287
Epoch 197/300, trend Loss: 0.0593 | 0.0287
Epoch 198/300, trend Loss: 0.0593 | 0.0287
Epoch 199/300, trend Loss: 0.0593 | 0.0287
Epoch 200/300, trend Loss: 0.0593 | 0.0287
Epoch 201/300, trend Loss: 0.0593 | 0.0287
Epoch 202/300, trend Loss: 0.0592 | 0.0287
Epoch 203/300, trend Loss: 0.0592 | 0.0287
Epoch 204/300, trend Loss: 0.0592 | 0.0287
Epoch 205/300, trend Loss: 0.0592 | 0.0287
Epoch 206/300, trend Loss: 0.0592 | 0.0287
Epoch 207/300, trend Loss: 0.0592 | 0.0287
Epoch 208/300, trend Loss: 0.0592 | 0.0287
Epoch 209/300, trend Loss: 0.0591 | 0.0287
Epoch 210/300, trend Loss: 0.0591 | 0.0287
Epoch 211/300, trend Loss: 0.0591 | 0.0287
Epoch 212/300, trend Loss: 0.0591 | 0.0287
Epoch 213/300, trend Loss: 0.0591 | 0.0287
Epoch 214/300, trend Loss: 0.0591 | 0.0287
Epoch 215/300, trend Loss: 0.0591 | 0.0287
Epoch 216/300, trend Loss: 0.0591 | 0.0287
Epoch 217/300, trend Loss: 0.0591 | 0.0287
Epoch 218/300, trend Loss: 0.0590 | 0.0287
Epoch 219/300, trend Loss: 0.0590 | 0.0287
Epoch 220/300, trend Loss: 0.0590 | 0.0287
Epoch 221/300, trend Loss: 0.0590 | 0.0287
Epoch 222/300, trend Loss: 0.0590 | 0.0287
Epoch 223/300, trend Loss: 0.0590 | 0.0287
Epoch 224/300, trend Loss: 0.0590 | 0.0287
Epoch 225/300, trend Loss: 0.0590 | 0.0287
Epoch 226/300, trend Loss: 0.0590 | 0.0287
Epoch 227/300, trend Loss: 0.0590 | 0.0287
Epoch 228/300, trend Loss: 0.0590 | 0.0287
Epoch 229/300, trend Loss: 0.0590 | 0.0287
Epoch 230/300, trend Loss: 0.0589 | 0.0287
Epoch 231/300, trend Loss: 0.0589 | 0.0287
Epoch 232/300, trend Loss: 0.0589 | 0.0287
Epoch 233/300, trend Loss: 0.0589 | 0.0287
Epoch 234/300, trend Loss: 0.0589 | 0.0287
Epoch 235/300, trend Loss: 0.0589 | 0.0287
Epoch 236/300, trend Loss: 0.0589 | 0.0287
Epoch 237/300, trend Loss: 0.0589 | 0.0287
Epoch 238/300, trend Loss: 0.0589 | 0.0287
Epoch 239/300, trend Loss: 0.0589 | 0.0287
Epoch 240/300, trend Loss: 0.0589 | 0.0287
Epoch 241/300, trend Loss: 0.0589 | 0.0287
Epoch 242/300, trend Loss: 0.0589 | 0.0287
Epoch 243/300, trend Loss: 0.0589 | 0.0287
Epoch 244/300, trend Loss: 0.0589 | 0.0287
Epoch 245/300, trend Loss: 0.0588 | 0.0287
Epoch 246/300, trend Loss: 0.0588 | 0.0287
Epoch 247/300, trend Loss: 0.0588 | 0.0287
Epoch 248/300, trend Loss: 0.0588 | 0.0287
Epoch 249/300, trend Loss: 0.0588 | 0.0287
Epoch 250/300, trend Loss: 0.0588 | 0.0287
Epoch 251/300, trend Loss: 0.0588 | 0.0287
Epoch 252/300, trend Loss: 0.0588 | 0.0287
Epoch 253/300, trend Loss: 0.0588 | 0.0287
Epoch 254/300, trend Loss: 0.0588 | 0.0287
Epoch 255/300, trend Loss: 0.0588 | 0.0287
Epoch 256/300, trend Loss: 0.0588 | 0.0287
Epoch 257/300, trend Loss: 0.0588 | 0.0287
Epoch 258/300, trend Loss: 0.0588 | 0.0287
Epoch 259/300, trend Loss: 0.0588 | 0.0287
Epoch 260/300, trend Loss: 0.0588 | 0.0287
Epoch 261/300, trend Loss: 0.0588 | 0.0287
Epoch 262/300, trend Loss: 0.0588 | 0.0287
Epoch 263/300, trend Loss: 0.0588 | 0.0287
Epoch 264/300, trend Loss: 0.0588 | 0.0287
Epoch 265/300, trend Loss: 0.0588 | 0.0287
Epoch 266/300, trend Loss: 0.0588 | 0.0287
Epoch 267/300, trend Loss: 0.0587 | 0.0287
Epoch 268/300, trend Loss: 0.0587 | 0.0287
Epoch 269/300, trend Loss: 0.0587 | 0.0287
Epoch 270/300, trend Loss: 0.0587 | 0.0287
Epoch 271/300, trend Loss: 0.0587 | 0.0287
Epoch 272/300, trend Loss: 0.0587 | 0.0287
Epoch 273/300, trend Loss: 0.0587 | 0.0287
Epoch 274/300, trend Loss: 0.0587 | 0.0287
Epoch 275/300, trend Loss: 0.0587 | 0.0287
Epoch 276/300, trend Loss: 0.0587 | 0.0287
Epoch 277/300, trend Loss: 0.0587 | 0.0287
Epoch 278/300, trend Loss: 0.0587 | 0.0287
Epoch 279/300, trend Loss: 0.0587 | 0.0287
Epoch 280/300, trend Loss: 0.0587 | 0.0287
Epoch 281/300, trend Loss: 0.0587 | 0.0287
Epoch 282/300, trend Loss: 0.0587 | 0.0287
Epoch 283/300, trend Loss: 0.0587 | 0.0287
Epoch 284/300, trend Loss: 0.0587 | 0.0287
Epoch 285/300, trend Loss: 0.0587 | 0.0287
Epoch 286/300, trend Loss: 0.0587 | 0.0287
Epoch 287/300, trend Loss: 0.0587 | 0.0287
Epoch 288/300, trend Loss: 0.0587 | 0.0287
Epoch 289/300, trend Loss: 0.0587 | 0.0287
Epoch 290/300, trend Loss: 0.0587 | 0.0287
Epoch 291/300, trend Loss: 0.0587 | 0.0287
Epoch 292/300, trend Loss: 0.0587 | 0.0287
Epoch 293/300, trend Loss: 0.0587 | 0.0287
Epoch 294/300, trend Loss: 0.0587 | 0.0287
Epoch 295/300, trend Loss: 0.0587 | 0.0287
Epoch 296/300, trend Loss: 0.0587 | 0.0287
Epoch 297/300, trend Loss: 0.0587 | 0.0287
Epoch 298/300, trend Loss: 0.0587 | 0.0287
Epoch 299/300, trend Loss: 0.0587 | 0.0287
Epoch 300/300, trend Loss: 0.0587 | 0.0287
Training seasonal_0 component with params: {'observation_period_num': 22, 'train_rates': 0.933820601560436, 'learning_rate': 0.00021149643429965986, 'batch_size': 21, 'step_size': 5, 'gamma': 0.8373334555028187}
Epoch 1/300, seasonal_0 Loss: 0.2750 | 0.1396
Epoch 2/300, seasonal_0 Loss: 0.1527 | 0.1266
Epoch 3/300, seasonal_0 Loss: 0.1227 | 0.1121
Epoch 4/300, seasonal_0 Loss: 0.1172 | 0.0759
Epoch 5/300, seasonal_0 Loss: 0.1247 | 0.0918
Epoch 6/300, seasonal_0 Loss: 0.1193 | 0.0929
Epoch 7/300, seasonal_0 Loss: 0.1162 | 0.1384
Epoch 8/300, seasonal_0 Loss: 0.1192 | 0.0945
Epoch 9/300, seasonal_0 Loss: 0.1084 | 0.0616
Epoch 10/300, seasonal_0 Loss: 0.1046 | 0.0811
Epoch 11/300, seasonal_0 Loss: 0.0970 | 0.0591
Epoch 12/300, seasonal_0 Loss: 0.0903 | 0.0682
Epoch 13/300, seasonal_0 Loss: 0.0836 | 0.0540
Epoch 14/300, seasonal_0 Loss: 0.0783 | 0.0579
Epoch 15/300, seasonal_0 Loss: 0.0743 | 0.0490
Epoch 16/300, seasonal_0 Loss: 0.0682 | 0.0491
Epoch 17/300, seasonal_0 Loss: 0.0648 | 0.0472
Epoch 18/300, seasonal_0 Loss: 0.0601 | 0.0463
Epoch 19/300, seasonal_0 Loss: 0.0601 | 0.0454
Epoch 20/300, seasonal_0 Loss: 0.0590 | 0.0394
Epoch 21/300, seasonal_0 Loss: 0.0567 | 0.0407
Epoch 22/300, seasonal_0 Loss: 0.0552 | 0.0375
Epoch 23/300, seasonal_0 Loss: 0.0525 | 0.0374
Epoch 24/300, seasonal_0 Loss: 0.0501 | 0.0393
Epoch 25/300, seasonal_0 Loss: 0.0479 | 0.0352
Epoch 26/300, seasonal_0 Loss: 0.0442 | 0.0375
Epoch 27/300, seasonal_0 Loss: 0.0410 | 0.0356
Epoch 28/300, seasonal_0 Loss: 0.0465 | 0.0424
Epoch 29/300, seasonal_0 Loss: 0.0468 | 0.0485
Epoch 30/300, seasonal_0 Loss: 0.0465 | 0.0368
Epoch 31/300, seasonal_0 Loss: 0.0432 | 0.0363
Epoch 32/300, seasonal_0 Loss: 0.0380 | 0.0359
Epoch 33/300, seasonal_0 Loss: 0.0348 | 0.0357
Epoch 34/300, seasonal_0 Loss: 0.0337 | 0.0356
Epoch 35/300, seasonal_0 Loss: 0.0327 | 0.0352
Epoch 36/300, seasonal_0 Loss: 0.0322 | 0.0354
Epoch 37/300, seasonal_0 Loss: 0.0317 | 0.0349
Epoch 38/300, seasonal_0 Loss: 0.0312 | 0.0347
Epoch 39/300, seasonal_0 Loss: 0.0309 | 0.0353
Epoch 40/300, seasonal_0 Loss: 0.0305 | 0.0353
Epoch 41/300, seasonal_0 Loss: 0.0303 | 0.0355
Epoch 42/300, seasonal_0 Loss: 0.0300 | 0.0357
Epoch 43/300, seasonal_0 Loss: 0.0298 | 0.0356
Epoch 44/300, seasonal_0 Loss: 0.0296 | 0.0360
Epoch 45/300, seasonal_0 Loss: 0.0292 | 0.0357
Epoch 46/300, seasonal_0 Loss: 0.0291 | 0.0354
Epoch 47/300, seasonal_0 Loss: 0.0288 | 0.0352
Epoch 48/300, seasonal_0 Loss: 0.0285 | 0.0351
Epoch 49/300, seasonal_0 Loss: 0.0284 | 0.0346
Epoch 50/300, seasonal_0 Loss: 0.0283 | 0.0347
Epoch 51/300, seasonal_0 Loss: 0.0282 | 0.0342
Epoch 52/300, seasonal_0 Loss: 0.0279 | 0.0343
Epoch 53/300, seasonal_0 Loss: 0.0276 | 0.0344
Epoch 54/300, seasonal_0 Loss: 0.0274 | 0.0347
Epoch 55/300, seasonal_0 Loss: 0.0272 | 0.0347
Epoch 56/300, seasonal_0 Loss: 0.0271 | 0.0353
Epoch 57/300, seasonal_0 Loss: 0.0269 | 0.0353
Epoch 58/300, seasonal_0 Loss: 0.0268 | 0.0353
Epoch 59/300, seasonal_0 Loss: 0.0267 | 0.0357
Epoch 60/300, seasonal_0 Loss: 0.0266 | 0.0356
Epoch 61/300, seasonal_0 Loss: 0.0265 | 0.0352
Epoch 62/300, seasonal_0 Loss: 0.0264 | 0.0351
Epoch 63/300, seasonal_0 Loss: 0.0262 | 0.0350
Epoch 64/300, seasonal_0 Loss: 0.0262 | 0.0342
Epoch 65/300, seasonal_0 Loss: 0.0260 | 0.0342
Epoch 66/300, seasonal_0 Loss: 0.0259 | 0.0338
Epoch 67/300, seasonal_0 Loss: 0.0258 | 0.0337
Epoch 68/300, seasonal_0 Loss: 0.0256 | 0.0336
Epoch 69/300, seasonal_0 Loss: 0.0256 | 0.0335
Epoch 70/300, seasonal_0 Loss: 0.0255 | 0.0334
Epoch 71/300, seasonal_0 Loss: 0.0254 | 0.0334
Epoch 72/300, seasonal_0 Loss: 0.0253 | 0.0332
Epoch 73/300, seasonal_0 Loss: 0.0252 | 0.0331
Epoch 74/300, seasonal_0 Loss: 0.0252 | 0.0332
Epoch 75/300, seasonal_0 Loss: 0.0252 | 0.0331
Epoch 76/300, seasonal_0 Loss: 0.0251 | 0.0333
Epoch 77/300, seasonal_0 Loss: 0.0251 | 0.0331
Epoch 78/300, seasonal_0 Loss: 0.0251 | 0.0331
Epoch 79/300, seasonal_0 Loss: 0.0250 | 0.0333
Epoch 80/300, seasonal_0 Loss: 0.0250 | 0.0332
Epoch 81/300, seasonal_0 Loss: 0.0250 | 0.0334
Epoch 82/300, seasonal_0 Loss: 0.0250 | 0.0333
Epoch 83/300, seasonal_0 Loss: 0.0249 | 0.0333
Epoch 84/300, seasonal_0 Loss: 0.0249 | 0.0334
Epoch 85/300, seasonal_0 Loss: 0.0249 | 0.0334
Epoch 86/300, seasonal_0 Loss: 0.0249 | 0.0335
Epoch 87/300, seasonal_0 Loss: 0.0249 | 0.0335
Epoch 88/300, seasonal_0 Loss: 0.0248 | 0.0335
Epoch 89/300, seasonal_0 Loss: 0.0248 | 0.0335
Epoch 90/300, seasonal_0 Loss: 0.0248 | 0.0335
Epoch 91/300, seasonal_0 Loss: 0.0248 | 0.0334
Epoch 92/300, seasonal_0 Loss: 0.0248 | 0.0334
Epoch 93/300, seasonal_0 Loss: 0.0248 | 0.0334
Epoch 94/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 95/300, seasonal_0 Loss: 0.0247 | 0.0334
Epoch 96/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 97/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 98/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 99/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 100/300, seasonal_0 Loss: 0.0247 | 0.0333
Epoch 101/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 102/300, seasonal_0 Loss: 0.0246 | 0.0333
Epoch 103/300, seasonal_0 Loss: 0.0246 | 0.0333
Epoch 104/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 105/300, seasonal_0 Loss: 0.0246 | 0.0333
Epoch 106/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 107/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 108/300, seasonal_0 Loss: 0.0246 | 0.0333
Epoch 109/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 110/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 111/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 112/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 113/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 114/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 115/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 116/300, seasonal_0 Loss: 0.0246 | 0.0332
Epoch 117/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 118/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 119/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 120/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 121/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 122/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 123/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 124/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 125/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 126/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 127/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 128/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 129/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 130/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 131/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 132/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 133/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 134/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 135/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 136/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 137/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 138/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 139/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 140/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 141/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 142/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 143/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 144/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 145/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 146/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 147/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 148/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 149/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 150/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 151/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 152/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 153/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 154/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 155/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 156/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 157/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 158/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 159/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 160/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 161/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 162/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 163/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 164/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 165/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 166/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 167/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 168/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 169/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 170/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 171/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 172/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 173/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 174/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 175/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 176/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 177/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 178/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 179/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 180/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 181/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 182/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 183/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 184/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 185/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 186/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 187/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 188/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 189/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 190/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 191/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 192/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 193/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 194/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 195/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 196/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 197/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 198/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 199/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 200/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 201/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 202/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 203/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 204/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 205/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 206/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 207/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 208/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 209/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 210/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 211/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 212/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 213/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 214/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 215/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 216/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 217/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 218/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 219/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 220/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 221/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 222/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 223/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 224/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 225/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 226/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 227/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 228/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 229/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 230/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 231/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 232/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 233/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 234/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 235/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 236/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 237/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 238/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 239/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 240/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 241/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 242/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 243/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 244/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 245/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 246/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 247/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 248/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 249/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 250/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 251/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 252/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 253/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 254/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 255/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 256/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 257/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 258/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 259/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 260/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 261/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 262/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 263/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 264/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 265/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 266/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 267/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 268/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 269/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 270/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 271/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 272/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 273/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 274/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 275/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 276/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 277/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 278/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 279/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 280/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 281/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 282/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 283/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 284/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 285/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 286/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 287/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 288/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 289/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 290/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 291/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 292/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 293/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 294/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 295/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 296/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 297/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 298/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 299/300, seasonal_0 Loss: 0.0245 | 0.0332
Epoch 300/300, seasonal_0 Loss: 0.0245 | 0.0332
Training seasonal_1 component with params: {'observation_period_num': 12, 'train_rates': 0.8851406389471477, 'learning_rate': 0.0009646210844056339, 'batch_size': 151, 'step_size': 6, 'gamma': 0.8308179145198721}
Epoch 1/300, seasonal_1 Loss: 3.3618 | 1.4018
Epoch 2/300, seasonal_1 Loss: 1.0313 | 1.3952
Epoch 3/300, seasonal_1 Loss: 1.2076 | 1.3940
Epoch 4/300, seasonal_1 Loss: 0.9804 | 1.3542
Epoch 5/300, seasonal_1 Loss: 1.2643 | 1.3555
Epoch 6/300, seasonal_1 Loss: 1.4545 | 1.5399
Epoch 7/300, seasonal_1 Loss: 1.1321 | 1.8330
Epoch 8/300, seasonal_1 Loss: 0.9506 | 1.6539
Epoch 9/300, seasonal_1 Loss: 1.0104 | 1.7085
Epoch 10/300, seasonal_1 Loss: 0.9801 | 1.7499
Epoch 11/300, seasonal_1 Loss: 0.9626 | 1.7250
Epoch 12/300, seasonal_1 Loss: 0.9723 | 1.7355
Epoch 13/300, seasonal_1 Loss: 0.9586 | 1.7657
Epoch 14/300, seasonal_1 Loss: 0.9492 | 1.7526
Epoch 15/300, seasonal_1 Loss: 0.9537 | 1.7566
Epoch 16/300, seasonal_1 Loss: 0.9443 | 1.7784
Epoch 17/300, seasonal_1 Loss: 0.9387 | 1.7721
Epoch 18/300, seasonal_1 Loss: 0.9407 | 1.7722
Epoch 19/300, seasonal_1 Loss: 0.9339 | 1.7884
Epoch 20/300, seasonal_1 Loss: 0.9305 | 1.7865
Epoch 21/300, seasonal_1 Loss: 0.9311 | 1.7843
Epoch 22/300, seasonal_1 Loss: 0.9261 | 1.7964
Epoch 23/300, seasonal_1 Loss: 0.9241 | 1.7973
Epoch 24/300, seasonal_1 Loss: 0.9239 | 1.7942
Epoch 25/300, seasonal_1 Loss: 0.9200 | 1.8027
Epoch 26/300, seasonal_1 Loss: 0.9189 | 1.8054
Epoch 27/300, seasonal_1 Loss: 0.9184 | 1.8024
Epoch 28/300, seasonal_1 Loss: 0.9151 | 1.8080
Epoch 29/300, seasonal_1 Loss: 0.9147 | 1.8115
Epoch 30/300, seasonal_1 Loss: 0.9141 | 1.8092
Epoch 31/300, seasonal_1 Loss: 0.9112 | 1.8126
Epoch 32/300, seasonal_1 Loss: 0.9112 | 1.8161
Epoch 33/300, seasonal_1 Loss: 0.9106 | 1.8147
Epoch 34/300, seasonal_1 Loss: 0.9081 | 1.8167
Epoch 35/300, seasonal_1 Loss: 0.9082 | 1.8197
Epoch 36/300, seasonal_1 Loss: 0.9078 | 1.8191
Epoch 37/300, seasonal_1 Loss: 0.9056 | 1.8203
Epoch 38/300, seasonal_1 Loss: 0.9058 | 1.8226
Epoch 39/300, seasonal_1 Loss: 0.9055 | 1.8225
Epoch 40/300, seasonal_1 Loss: 0.9036 | 1.8233
Epoch 41/300, seasonal_1 Loss: 0.9038 | 1.8251
Epoch 42/300, seasonal_1 Loss: 0.9036 | 1.8253
Epoch 43/300, seasonal_1 Loss: 0.9020 | 1.8259
Epoch 44/300, seasonal_1 Loss: 0.9021 | 1.8273
Epoch 45/300, seasonal_1 Loss: 0.9020 | 1.8276
Epoch 46/300, seasonal_1 Loss: 0.9006 | 1.8280
Epoch 47/300, seasonal_1 Loss: 0.9007 | 1.8291
Epoch 48/300, seasonal_1 Loss: 0.9006 | 1.8294
Epoch 49/300, seasonal_1 Loss: 0.8995 | 1.8298
Epoch 50/300, seasonal_1 Loss: 0.8995 | 1.8306
Epoch 51/300, seasonal_1 Loss: 0.8995 | 1.8310
Epoch 52/300, seasonal_1 Loss: 0.8985 | 1.8313
Epoch 53/300, seasonal_1 Loss: 0.8986 | 1.8319
Epoch 54/300, seasonal_1 Loss: 0.8985 | 1.8322
Epoch 55/300, seasonal_1 Loss: 0.8977 | 1.8325
Epoch 56/300, seasonal_1 Loss: 0.8977 | 1.8330
Epoch 57/300, seasonal_1 Loss: 0.8977 | 1.8333
Epoch 58/300, seasonal_1 Loss: 0.8970 | 1.8336
Epoch 59/300, seasonal_1 Loss: 0.8971 | 1.8340
Epoch 60/300, seasonal_1 Loss: 0.8971 | 1.8342
Epoch 61/300, seasonal_1 Loss: 0.8965 | 1.8344
Epoch 62/300, seasonal_1 Loss: 0.8965 | 1.8347
Epoch 63/300, seasonal_1 Loss: 0.8965 | 1.8350
Epoch 64/300, seasonal_1 Loss: 0.8960 | 1.8352
Epoch 65/300, seasonal_1 Loss: 0.8960 | 1.8354
Epoch 66/300, seasonal_1 Loss: 0.8960 | 1.8356
Epoch 67/300, seasonal_1 Loss: 0.8956 | 1.8358
Epoch 68/300, seasonal_1 Loss: 0.8956 | 1.8360
Epoch 69/300, seasonal_1 Loss: 0.8956 | 1.8361
Epoch 70/300, seasonal_1 Loss: 0.8953 | 1.8363
Epoch 71/300, seasonal_1 Loss: 0.8953 | 1.8364
Epoch 72/300, seasonal_1 Loss: 0.8953 | 1.8366
Epoch 73/300, seasonal_1 Loss: 0.8950 | 1.8367
Epoch 74/300, seasonal_1 Loss: 0.8950 | 1.8368
Epoch 75/300, seasonal_1 Loss: 0.8950 | 1.8370
Epoch 76/300, seasonal_1 Loss: 0.8948 | 1.8371
Epoch 77/300, seasonal_1 Loss: 0.8948 | 1.8372
Epoch 78/300, seasonal_1 Loss: 0.8948 | 1.8373
Epoch 79/300, seasonal_1 Loss: 0.8946 | 1.8374
Epoch 80/300, seasonal_1 Loss: 0.8946 | 1.8375
Epoch 81/300, seasonal_1 Loss: 0.8946 | 1.8376
Epoch 82/300, seasonal_1 Loss: 0.8944 | 1.8376
Epoch 83/300, seasonal_1 Loss: 0.8944 | 1.8377
Epoch 84/300, seasonal_1 Loss: 0.8944 | 1.8378
Epoch 85/300, seasonal_1 Loss: 0.8943 | 1.8378
Epoch 86/300, seasonal_1 Loss: 0.8943 | 1.8379
Epoch 87/300, seasonal_1 Loss: 0.8943 | 1.8380
Epoch 88/300, seasonal_1 Loss: 0.8941 | 1.8380
Epoch 89/300, seasonal_1 Loss: 0.8941 | 1.8381
Epoch 90/300, seasonal_1 Loss: 0.8941 | 1.8381
Epoch 91/300, seasonal_1 Loss: 0.8940 | 1.8382
Epoch 92/300, seasonal_1 Loss: 0.8940 | 1.8382
Epoch 93/300, seasonal_1 Loss: 0.8940 | 1.8383
Epoch 94/300, seasonal_1 Loss: 0.8940 | 1.8383
Epoch 95/300, seasonal_1 Loss: 0.8940 | 1.8383
Epoch 96/300, seasonal_1 Loss: 0.8940 | 1.8384
Epoch 97/300, seasonal_1 Loss: 0.8939 | 1.8384
Epoch 98/300, seasonal_1 Loss: 0.8939 | 1.8384
Epoch 99/300, seasonal_1 Loss: 0.8939 | 1.8385
Epoch 100/300, seasonal_1 Loss: 0.8938 | 1.8385
Epoch 101/300, seasonal_1 Loss: 0.8938 | 1.8385
Epoch 102/300, seasonal_1 Loss: 0.8938 | 1.8386
Epoch 103/300, seasonal_1 Loss: 0.8938 | 1.8386
Epoch 104/300, seasonal_1 Loss: 0.8938 | 1.8386
Epoch 105/300, seasonal_1 Loss: 0.8938 | 1.8386
Epoch 106/300, seasonal_1 Loss: 0.8937 | 1.8386
Epoch 107/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 108/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 109/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 110/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 111/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 112/300, seasonal_1 Loss: 0.8937 | 1.8387
Epoch 113/300, seasonal_1 Loss: 0.8937 | 1.8388
Epoch 114/300, seasonal_1 Loss: 0.8937 | 1.8388
Epoch 115/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 116/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 117/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 118/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 119/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 120/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 121/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 122/300, seasonal_1 Loss: 0.8936 | 1.8388
Epoch 123/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 124/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 125/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 126/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 127/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 128/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 129/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 130/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 131/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 132/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 133/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 134/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 135/300, seasonal_1 Loss: 0.8936 | 1.8389
Epoch 136/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 137/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 138/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 139/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 140/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 141/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 142/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 143/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 144/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 145/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 146/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 147/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 148/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 149/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 150/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 151/300, seasonal_1 Loss: 0.8935 | 1.8389
Epoch 152/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 153/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 154/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 155/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 156/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 157/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 158/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 159/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 160/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 161/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 162/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 163/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 164/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 165/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 166/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 167/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 168/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 169/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 170/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 171/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 172/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 173/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 174/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 175/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 176/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 177/300, seasonal_1 Loss: 0.8935 | 1.8390
Epoch 178/300, seasonal_1 Loss: 0.8935 | 1.8390
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 25, 'train_rates': 0.8291504845825987, 'learning_rate': 0.00017704824158251155, 'batch_size': 192, 'step_size': 6, 'gamma': 0.8460722852091568}
Epoch 1/300, seasonal_2 Loss: 1.8059 | 0.3410
Epoch 2/300, seasonal_2 Loss: 0.3785 | 0.3303
Epoch 3/300, seasonal_2 Loss: 0.5672 | 0.2295
Epoch 4/300, seasonal_2 Loss: 0.3760 | 0.2272
Epoch 5/300, seasonal_2 Loss: 0.2316 | 0.3844
Epoch 6/300, seasonal_2 Loss: 0.2688 | 0.1294
Epoch 7/300, seasonal_2 Loss: 0.2366 | 0.1519
Epoch 8/300, seasonal_2 Loss: 0.1753 | 0.1168
Epoch 9/300, seasonal_2 Loss: 0.1399 | 0.0935
Epoch 10/300, seasonal_2 Loss: 0.1498 | 0.0982
Epoch 11/300, seasonal_2 Loss: 0.1765 | 0.1366
Epoch 12/300, seasonal_2 Loss: 0.1639 | 0.0853
Epoch 13/300, seasonal_2 Loss: 0.1475 | 0.0853
Epoch 14/300, seasonal_2 Loss: 0.1149 | 0.0725
Epoch 15/300, seasonal_2 Loss: 0.1113 | 0.0627
Epoch 16/300, seasonal_2 Loss: 0.1057 | 0.0649
Epoch 17/300, seasonal_2 Loss: 0.1059 | 0.0761
Epoch 18/300, seasonal_2 Loss: 0.1025 | 0.0690
Epoch 19/300, seasonal_2 Loss: 0.1007 | 0.0630
Epoch 20/300, seasonal_2 Loss: 0.1099 | 0.0637
Epoch 21/300, seasonal_2 Loss: 0.1061 | 0.0889
Epoch 22/300, seasonal_2 Loss: 0.1108 | 0.0659
Epoch 23/300, seasonal_2 Loss: 0.1020 | 0.0596
Epoch 24/300, seasonal_2 Loss: 0.1040 | 0.0693
Epoch 25/300, seasonal_2 Loss: 0.0982 | 0.0752
Epoch 26/300, seasonal_2 Loss: 0.0953 | 0.0609
Epoch 27/300, seasonal_2 Loss: 0.0927 | 0.0594
Epoch 28/300, seasonal_2 Loss: 0.0899 | 0.0655
Epoch 29/300, seasonal_2 Loss: 0.0882 | 0.0562
Epoch 30/300, seasonal_2 Loss: 0.0876 | 0.0577
Epoch 31/300, seasonal_2 Loss: 0.0862 | 0.0602
Epoch 32/300, seasonal_2 Loss: 0.0855 | 0.0553
Epoch 33/300, seasonal_2 Loss: 0.0851 | 0.0565
Epoch 34/300, seasonal_2 Loss: 0.0843 | 0.0566
Epoch 35/300, seasonal_2 Loss: 0.0838 | 0.0547
Epoch 36/300, seasonal_2 Loss: 0.0834 | 0.0554
Epoch 37/300, seasonal_2 Loss: 0.0829 | 0.0548
Epoch 38/300, seasonal_2 Loss: 0.0826 | 0.0543
Epoch 39/300, seasonal_2 Loss: 0.0822 | 0.0540
Epoch 40/300, seasonal_2 Loss: 0.0819 | 0.0541
Epoch 41/300, seasonal_2 Loss: 0.0816 | 0.0534
Epoch 42/300, seasonal_2 Loss: 0.0814 | 0.0537
Epoch 43/300, seasonal_2 Loss: 0.0811 | 0.0530
Epoch 44/300, seasonal_2 Loss: 0.0809 | 0.0532
Epoch 45/300, seasonal_2 Loss: 0.0807 | 0.0525
Epoch 46/300, seasonal_2 Loss: 0.0805 | 0.0529
Epoch 47/300, seasonal_2 Loss: 0.0803 | 0.0523
Epoch 48/300, seasonal_2 Loss: 0.0802 | 0.0529
Epoch 49/300, seasonal_2 Loss: 0.0801 | 0.0523
Epoch 50/300, seasonal_2 Loss: 0.0801 | 0.0522
Epoch 51/300, seasonal_2 Loss: 0.0803 | 0.0526
Epoch 52/300, seasonal_2 Loss: 0.0805 | 0.0512
Epoch 53/300, seasonal_2 Loss: 0.0810 | 0.0559
Epoch 54/300, seasonal_2 Loss: 0.0813 | 0.0505
Epoch 55/300, seasonal_2 Loss: 0.0818 | 0.0605
Epoch 56/300, seasonal_2 Loss: 0.0808 | 0.0489
Epoch 57/300, seasonal_2 Loss: 0.0807 | 0.0588
Epoch 58/300, seasonal_2 Loss: 0.0796 | 0.0491
Epoch 59/300, seasonal_2 Loss: 0.0791 | 0.0554
Epoch 60/300, seasonal_2 Loss: 0.0787 | 0.0500
Epoch 61/300, seasonal_2 Loss: 0.0785 | 0.0530
Epoch 62/300, seasonal_2 Loss: 0.0783 | 0.0507
Epoch 63/300, seasonal_2 Loss: 0.0782 | 0.0516
Epoch 64/300, seasonal_2 Loss: 0.0781 | 0.0511
Epoch 65/300, seasonal_2 Loss: 0.0780 | 0.0512
Epoch 66/300, seasonal_2 Loss: 0.0779 | 0.0511
Epoch 67/300, seasonal_2 Loss: 0.0779 | 0.0510
Epoch 68/300, seasonal_2 Loss: 0.0778 | 0.0510
Epoch 69/300, seasonal_2 Loss: 0.0777 | 0.0509
Epoch 70/300, seasonal_2 Loss: 0.0777 | 0.0509
Epoch 71/300, seasonal_2 Loss: 0.0776 | 0.0508
Epoch 72/300, seasonal_2 Loss: 0.0775 | 0.0508
Epoch 73/300, seasonal_2 Loss: 0.0775 | 0.0507
Epoch 74/300, seasonal_2 Loss: 0.0774 | 0.0507
Epoch 75/300, seasonal_2 Loss: 0.0774 | 0.0507
Epoch 76/300, seasonal_2 Loss: 0.0773 | 0.0506
Epoch 77/300, seasonal_2 Loss: 0.0773 | 0.0506
Epoch 78/300, seasonal_2 Loss: 0.0773 | 0.0506
Epoch 79/300, seasonal_2 Loss: 0.0772 | 0.0505
Epoch 80/300, seasonal_2 Loss: 0.0772 | 0.0505
Epoch 81/300, seasonal_2 Loss: 0.0771 | 0.0505
Epoch 82/300, seasonal_2 Loss: 0.0771 | 0.0505
Epoch 83/300, seasonal_2 Loss: 0.0771 | 0.0504
Epoch 84/300, seasonal_2 Loss: 0.0770 | 0.0504
Epoch 85/300, seasonal_2 Loss: 0.0770 | 0.0504
Epoch 86/300, seasonal_2 Loss: 0.0770 | 0.0504
Epoch 87/300, seasonal_2 Loss: 0.0770 | 0.0503
Epoch 88/300, seasonal_2 Loss: 0.0769 | 0.0503
Epoch 89/300, seasonal_2 Loss: 0.0769 | 0.0503
Epoch 90/300, seasonal_2 Loss: 0.0769 | 0.0503
Epoch 91/300, seasonal_2 Loss: 0.0769 | 0.0503
Epoch 92/300, seasonal_2 Loss: 0.0768 | 0.0503
Epoch 93/300, seasonal_2 Loss: 0.0768 | 0.0502
Epoch 94/300, seasonal_2 Loss: 0.0768 | 0.0502
Epoch 95/300, seasonal_2 Loss: 0.0768 | 0.0502
Epoch 96/300, seasonal_2 Loss: 0.0768 | 0.0502
Epoch 97/300, seasonal_2 Loss: 0.0767 | 0.0502
Epoch 98/300, seasonal_2 Loss: 0.0767 | 0.0502
Epoch 99/300, seasonal_2 Loss: 0.0767 | 0.0502
Epoch 100/300, seasonal_2 Loss: 0.0767 | 0.0502
Epoch 101/300, seasonal_2 Loss: 0.0767 | 0.0502
Epoch 102/300, seasonal_2 Loss: 0.0767 | 0.0501
Epoch 103/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 104/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 105/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 106/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 107/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 108/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 109/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 110/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 111/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 112/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 113/300, seasonal_2 Loss: 0.0766 | 0.0501
Epoch 114/300, seasonal_2 Loss: 0.0765 | 0.0501
Epoch 115/300, seasonal_2 Loss: 0.0765 | 0.0501
Epoch 116/300, seasonal_2 Loss: 0.0765 | 0.0501
Epoch 117/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 118/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 119/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 120/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 121/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 122/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 123/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 124/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 125/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 126/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 127/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 128/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 129/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 130/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 131/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 132/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 133/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 134/300, seasonal_2 Loss: 0.0765 | 0.0500
Epoch 135/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 136/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 137/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 138/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 139/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 140/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 141/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 142/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 143/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 144/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 145/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 146/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 147/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 148/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 149/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 150/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 151/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 152/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 153/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 154/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 155/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 156/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 157/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 158/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 159/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 160/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 161/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 162/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 163/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 164/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 165/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 166/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 167/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 168/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 169/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 170/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 171/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 172/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 173/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 174/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 175/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 176/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 177/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 178/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 179/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 180/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 181/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 182/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 183/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 184/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 185/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 186/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 187/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 188/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 189/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 190/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 191/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 192/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 193/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 194/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 195/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 196/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 197/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 198/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 199/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 200/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 201/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 202/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 203/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 204/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 205/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 206/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 207/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 208/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 209/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 210/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 211/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 212/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 213/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 214/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 215/300, seasonal_2 Loss: 0.0764 | 0.0500
Epoch 216/300, seasonal_2 Loss: 0.0764 | 0.0500
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 20, 'train_rates': 0.9430792814665933, 'learning_rate': 2.1129031057999455e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8547360256999255}
Epoch 1/300, seasonal_3 Loss: 0.1873 | 0.0983
Epoch 2/300, seasonal_3 Loss: 0.1379 | 0.0959
Epoch 3/300, seasonal_3 Loss: 0.1332 | 0.0709
Epoch 4/300, seasonal_3 Loss: 0.1234 | 0.0743
Epoch 5/300, seasonal_3 Loss: 0.1128 | 0.0929
Epoch 6/300, seasonal_3 Loss: 0.1071 | 0.0860
Epoch 7/300, seasonal_3 Loss: 0.0990 | 0.0705
Epoch 8/300, seasonal_3 Loss: 0.0943 | 0.0649
Epoch 9/300, seasonal_3 Loss: 0.0902 | 0.0599
Epoch 10/300, seasonal_3 Loss: 0.0860 | 0.0577
Epoch 11/300, seasonal_3 Loss: 0.0824 | 0.0557
Epoch 12/300, seasonal_3 Loss: 0.0789 | 0.0535
Epoch 13/300, seasonal_3 Loss: 0.0766 | 0.0531
Epoch 14/300, seasonal_3 Loss: 0.0747 | 0.0525
Epoch 15/300, seasonal_3 Loss: 0.0733 | 0.0518
Epoch 16/300, seasonal_3 Loss: 0.0720 | 0.0511
Epoch 17/300, seasonal_3 Loss: 0.0709 | 0.0505
Epoch 18/300, seasonal_3 Loss: 0.0696 | 0.0511
Epoch 19/300, seasonal_3 Loss: 0.0689 | 0.0517
Epoch 20/300, seasonal_3 Loss: 0.0683 | 0.0521
Epoch 21/300, seasonal_3 Loss: 0.0671 | 0.0524
Epoch 22/300, seasonal_3 Loss: 0.0660 | 0.0527
Epoch 23/300, seasonal_3 Loss: 0.0652 | 0.0567
Epoch 24/300, seasonal_3 Loss: 0.0645 | 0.0563
Epoch 25/300, seasonal_3 Loss: 0.0641 | 0.0566
Epoch 26/300, seasonal_3 Loss: 0.0636 | 0.0565
Epoch 27/300, seasonal_3 Loss: 0.0629 | 0.0554
Epoch 28/300, seasonal_3 Loss: 0.0621 | 0.0539
Epoch 29/300, seasonal_3 Loss: 0.0617 | 0.0495
Epoch 30/300, seasonal_3 Loss: 0.0609 | 0.0470
Epoch 31/300, seasonal_3 Loss: 0.0597 | 0.0451
Epoch 32/300, seasonal_3 Loss: 0.0585 | 0.0440
Epoch 33/300, seasonal_3 Loss: 0.0574 | 0.0431
Epoch 34/300, seasonal_3 Loss: 0.0567 | 0.0424
Epoch 35/300, seasonal_3 Loss: 0.0562 | 0.0432
Epoch 36/300, seasonal_3 Loss: 0.0561 | 0.0434
Epoch 37/300, seasonal_3 Loss: 0.0558 | 0.0431
Epoch 38/300, seasonal_3 Loss: 0.0550 | 0.0426
Epoch 39/300, seasonal_3 Loss: 0.0539 | 0.0423
Epoch 40/300, seasonal_3 Loss: 0.0530 | 0.0416
Epoch 41/300, seasonal_3 Loss: 0.0523 | 0.0418
Epoch 42/300, seasonal_3 Loss: 0.0517 | 0.0420
Epoch 43/300, seasonal_3 Loss: 0.0511 | 0.0422
Epoch 44/300, seasonal_3 Loss: 0.0505 | 0.0423
Epoch 45/300, seasonal_3 Loss: 0.0500 | 0.0429
Epoch 46/300, seasonal_3 Loss: 0.0497 | 0.0443
Epoch 47/300, seasonal_3 Loss: 0.0494 | 0.0457
Epoch 48/300, seasonal_3 Loss: 0.0492 | 0.0474
Epoch 49/300, seasonal_3 Loss: 0.0489 | 0.0493
Epoch 50/300, seasonal_3 Loss: 0.0486 | 0.0507
Epoch 51/300, seasonal_3 Loss: 0.0485 | 0.0474
Epoch 52/300, seasonal_3 Loss: 0.0488 | 0.0441
Epoch 53/300, seasonal_3 Loss: 0.0497 | 0.0437
Epoch 54/300, seasonal_3 Loss: 0.0496 | 0.0431
Epoch 55/300, seasonal_3 Loss: 0.0482 | 0.0417
Epoch 56/300, seasonal_3 Loss: 0.0473 | 0.0406
Epoch 57/300, seasonal_3 Loss: 0.0471 | 0.0410
Epoch 58/300, seasonal_3 Loss: 0.0464 | 0.0410
Epoch 59/300, seasonal_3 Loss: 0.0458 | 0.0411
Epoch 60/300, seasonal_3 Loss: 0.0454 | 0.0415
Epoch 61/300, seasonal_3 Loss: 0.0449 | 0.0418
Epoch 62/300, seasonal_3 Loss: 0.0446 | 0.0422
Epoch 63/300, seasonal_3 Loss: 0.0443 | 0.0426
Epoch 64/300, seasonal_3 Loss: 0.0440 | 0.0430
Epoch 65/300, seasonal_3 Loss: 0.0437 | 0.0433
Epoch 66/300, seasonal_3 Loss: 0.0435 | 0.0434
Epoch 67/300, seasonal_3 Loss: 0.0433 | 0.0432
Epoch 68/300, seasonal_3 Loss: 0.0431 | 0.0432
Epoch 69/300, seasonal_3 Loss: 0.0429 | 0.0432
Epoch 70/300, seasonal_3 Loss: 0.0427 | 0.0431
Epoch 71/300, seasonal_3 Loss: 0.0425 | 0.0431
Epoch 72/300, seasonal_3 Loss: 0.0422 | 0.0430
Epoch 73/300, seasonal_3 Loss: 0.0421 | 0.0427
Epoch 74/300, seasonal_3 Loss: 0.0419 | 0.0428
Epoch 75/300, seasonal_3 Loss: 0.0416 | 0.0429
Epoch 76/300, seasonal_3 Loss: 0.0413 | 0.0429
Epoch 77/300, seasonal_3 Loss: 0.0411 | 0.0430
Epoch 78/300, seasonal_3 Loss: 0.0409 | 0.0429
Epoch 79/300, seasonal_3 Loss: 0.0407 | 0.0431
Epoch 80/300, seasonal_3 Loss: 0.0405 | 0.0433
Epoch 81/300, seasonal_3 Loss: 0.0402 | 0.0434
Epoch 82/300, seasonal_3 Loss: 0.0400 | 0.0435
Epoch 83/300, seasonal_3 Loss: 0.0398 | 0.0436
Epoch 84/300, seasonal_3 Loss: 0.0397 | 0.0431
Epoch 85/300, seasonal_3 Loss: 0.0396 | 0.0431
Epoch 86/300, seasonal_3 Loss: 0.0394 | 0.0431
Epoch 87/300, seasonal_3 Loss: 0.0393 | 0.0431
Epoch 88/300, seasonal_3 Loss: 0.0391 | 0.0431
Epoch 89/300, seasonal_3 Loss: 0.0390 | 0.0431
Epoch 90/300, seasonal_3 Loss: 0.0388 | 0.0432
Epoch 91/300, seasonal_3 Loss: 0.0387 | 0.0434
Epoch 92/300, seasonal_3 Loss: 0.0385 | 0.0437
Epoch 93/300, seasonal_3 Loss: 0.0382 | 0.0439
Epoch 94/300, seasonal_3 Loss: 0.0379 | 0.0442
Epoch 95/300, seasonal_3 Loss: 0.0378 | 0.0455
Epoch 96/300, seasonal_3 Loss: 0.0376 | 0.0455
Epoch 97/300, seasonal_3 Loss: 0.0375 | 0.0458
Epoch 98/300, seasonal_3 Loss: 0.0373 | 0.0459
Epoch 99/300, seasonal_3 Loss: 0.0371 | 0.0460
Epoch 100/300, seasonal_3 Loss: 0.0370 | 0.0475
Epoch 101/300, seasonal_3 Loss: 0.0369 | 0.0473
Epoch 102/300, seasonal_3 Loss: 0.0367 | 0.0473
Epoch 103/300, seasonal_3 Loss: 0.0365 | 0.0473
Epoch 104/300, seasonal_3 Loss: 0.0362 | 0.0473
Epoch 105/300, seasonal_3 Loss: 0.0360 | 0.0472
Epoch 106/300, seasonal_3 Loss: 0.0358 | 0.0486
Epoch 107/300, seasonal_3 Loss: 0.0356 | 0.0487
Epoch 108/300, seasonal_3 Loss: 0.0353 | 0.0486
Epoch 109/300, seasonal_3 Loss: 0.0351 | 0.0484
Epoch 110/300, seasonal_3 Loss: 0.0348 | 0.0482
Epoch 111/300, seasonal_3 Loss: 0.0346 | 0.0486
Epoch 112/300, seasonal_3 Loss: 0.0344 | 0.0487
Epoch 113/300, seasonal_3 Loss: 0.0342 | 0.0485
Epoch 114/300, seasonal_3 Loss: 0.0340 | 0.0482
Epoch 115/300, seasonal_3 Loss: 0.0338 | 0.0478
Epoch 116/300, seasonal_3 Loss: 0.0337 | 0.0475
Epoch 117/300, seasonal_3 Loss: 0.0335 | 0.0471
Epoch 118/300, seasonal_3 Loss: 0.0334 | 0.0471
Epoch 119/300, seasonal_3 Loss: 0.0332 | 0.0470
Epoch 120/300, seasonal_3 Loss: 0.0331 | 0.0468
Epoch 121/300, seasonal_3 Loss: 0.0330 | 0.0466
Epoch 122/300, seasonal_3 Loss: 0.0329 | 0.0460
Epoch 123/300, seasonal_3 Loss: 0.0328 | 0.0460
Epoch 124/300, seasonal_3 Loss: 0.0327 | 0.0460
Epoch 125/300, seasonal_3 Loss: 0.0326 | 0.0459
Epoch 126/300, seasonal_3 Loss: 0.0325 | 0.0458
Epoch 127/300, seasonal_3 Loss: 0.0324 | 0.0457
Epoch 128/300, seasonal_3 Loss: 0.0323 | 0.0452
Epoch 129/300, seasonal_3 Loss: 0.0322 | 0.0452
Epoch 130/300, seasonal_3 Loss: 0.0322 | 0.0452
Epoch 131/300, seasonal_3 Loss: 0.0321 | 0.0452
Epoch 132/300, seasonal_3 Loss: 0.0320 | 0.0451
Epoch 133/300, seasonal_3 Loss: 0.0319 | 0.0448
Epoch 134/300, seasonal_3 Loss: 0.0319 | 0.0448
Epoch 135/300, seasonal_3 Loss: 0.0318 | 0.0448
Epoch 136/300, seasonal_3 Loss: 0.0317 | 0.0448
Epoch 137/300, seasonal_3 Loss: 0.0317 | 0.0448
Epoch 138/300, seasonal_3 Loss: 0.0316 | 0.0448
Epoch 139/300, seasonal_3 Loss: 0.0316 | 0.0445
Epoch 140/300, seasonal_3 Loss: 0.0315 | 0.0445
Epoch 141/300, seasonal_3 Loss: 0.0315 | 0.0445
Epoch 142/300, seasonal_3 Loss: 0.0314 | 0.0445
Epoch 143/300, seasonal_3 Loss: 0.0314 | 0.0445
Epoch 144/300, seasonal_3 Loss: 0.0313 | 0.0443
Epoch 145/300, seasonal_3 Loss: 0.0313 | 0.0443
Epoch 146/300, seasonal_3 Loss: 0.0313 | 0.0443
Epoch 147/300, seasonal_3 Loss: 0.0312 | 0.0443
Epoch 148/300, seasonal_3 Loss: 0.0312 | 0.0443
Epoch 149/300, seasonal_3 Loss: 0.0311 | 0.0443
Epoch 150/300, seasonal_3 Loss: 0.0311 | 0.0441
Epoch 151/300, seasonal_3 Loss: 0.0311 | 0.0441
Epoch 152/300, seasonal_3 Loss: 0.0310 | 0.0441
Epoch 153/300, seasonal_3 Loss: 0.0310 | 0.0441
Epoch 154/300, seasonal_3 Loss: 0.0310 | 0.0441
Epoch 155/300, seasonal_3 Loss: 0.0309 | 0.0440
Epoch 156/300, seasonal_3 Loss: 0.0309 | 0.0440
Epoch 157/300, seasonal_3 Loss: 0.0309 | 0.0440
Epoch 158/300, seasonal_3 Loss: 0.0308 | 0.0440
Epoch 159/300, seasonal_3 Loss: 0.0308 | 0.0440
Epoch 160/300, seasonal_3 Loss: 0.0308 | 0.0440
Epoch 161/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 162/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 163/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 164/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 165/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 166/300, seasonal_3 Loss: 0.0306 | 0.0437
Epoch 167/300, seasonal_3 Loss: 0.0306 | 0.0437
Epoch 168/300, seasonal_3 Loss: 0.0306 | 0.0437
Epoch 169/300, seasonal_3 Loss: 0.0306 | 0.0437
Epoch 170/300, seasonal_3 Loss: 0.0305 | 0.0437
Epoch 171/300, seasonal_3 Loss: 0.0305 | 0.0437
Epoch 172/300, seasonal_3 Loss: 0.0305 | 0.0436
Epoch 173/300, seasonal_3 Loss: 0.0305 | 0.0436
Epoch 174/300, seasonal_3 Loss: 0.0305 | 0.0436
Epoch 175/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 176/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 177/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 178/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 179/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 180/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 181/300, seasonal_3 Loss: 0.0304 | 0.0436
Epoch 182/300, seasonal_3 Loss: 0.0303 | 0.0436
Epoch 183/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 184/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 185/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 186/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 187/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 188/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 189/300, seasonal_3 Loss: 0.0303 | 0.0435
Epoch 190/300, seasonal_3 Loss: 0.0302 | 0.0435
Epoch 191/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 192/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 193/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 194/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 195/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 196/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 197/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 198/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 199/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 200/300, seasonal_3 Loss: 0.0302 | 0.0434
Epoch 201/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 202/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 203/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 204/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 205/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 206/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 207/300, seasonal_3 Loss: 0.0301 | 0.0434
Epoch 208/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 209/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 210/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 211/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 212/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 213/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 214/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 215/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 216/300, seasonal_3 Loss: 0.0301 | 0.0433
Epoch 217/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 218/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 219/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 220/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 221/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 222/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 223/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 224/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 225/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 226/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 227/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 228/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 229/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 230/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 231/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 232/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 233/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 234/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 235/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 236/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 237/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 238/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 239/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 240/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 241/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 242/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 243/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 244/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 245/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 246/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 247/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 248/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 249/300, seasonal_3 Loss: 0.0300 | 0.0433
Epoch 250/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 251/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 252/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 253/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 254/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 255/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 256/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 257/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 258/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 259/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 260/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 261/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 262/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 263/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 264/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 265/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 266/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 267/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 268/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 269/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 270/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 271/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 272/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 273/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 274/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 275/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 276/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 277/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 278/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 279/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 280/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 281/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 282/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 283/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 284/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 285/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 286/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 287/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 288/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 289/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 290/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 291/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 292/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 293/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 294/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 295/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 296/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 297/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 298/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 299/300, seasonal_3 Loss: 0.0299 | 0.0432
Epoch 300/300, seasonal_3 Loss: 0.0299 | 0.0432
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.942632524997232, 'learning_rate': 0.00023909995625234355, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8157585247260108}
Epoch 1/300, resid Loss: 0.8209 | 0.1532
Epoch 2/300, resid Loss: 0.1926 | 0.1144
Epoch 3/300, resid Loss: 0.1447 | 0.0816
Epoch 4/300, resid Loss: 0.1496 | 0.0709
Epoch 5/300, resid Loss: 0.1540 | 0.0970
Epoch 6/300, resid Loss: 0.1477 | 0.0709
Epoch 7/300, resid Loss: 0.1514 | 0.0788
Epoch 8/300, resid Loss: 0.1577 | 0.0711
Epoch 9/300, resid Loss: 0.1523 | 0.0679
Epoch 10/300, resid Loss: 0.1324 | 0.0665
Epoch 11/300, resid Loss: 0.1168 | 0.0597
Epoch 12/300, resid Loss: 0.1127 | 0.0592
Epoch 13/300, resid Loss: 0.1146 | 0.0487
Epoch 14/300, resid Loss: 0.1168 | 0.0535
Epoch 15/300, resid Loss: 0.1247 | 0.1068
Epoch 16/300, resid Loss: 0.1143 | 0.0614
Epoch 17/300, resid Loss: 0.1237 | 0.0644
Epoch 18/300, resid Loss: 0.1010 | 0.0609
Epoch 19/300, resid Loss: 0.1050 | 0.0851
Epoch 20/300, resid Loss: 0.1250 | 0.0668
Epoch 21/300, resid Loss: 0.1329 | 0.0804
Epoch 22/300, resid Loss: 0.1086 | 0.0722
Epoch 23/300, resid Loss: 0.1239 | 0.0624
Epoch 24/300, resid Loss: 0.1538 | 0.1033
Epoch 25/300, resid Loss: 0.1377 | 0.0755
Epoch 26/300, resid Loss: 0.1361 | 0.0520
Epoch 27/300, resid Loss: 0.1400 | 0.0564
Epoch 28/300, resid Loss: 0.1328 | 0.0508
Epoch 29/300, resid Loss: 0.1249 | 0.0534
Epoch 30/300, resid Loss: 0.1458 | 0.0512
Epoch 31/300, resid Loss: 0.1312 | 0.0418
Epoch 32/300, resid Loss: 0.1402 | 0.0405
Epoch 33/300, resid Loss: 0.1216 | 0.0450
Epoch 34/300, resid Loss: 0.1170 | 0.0460
Epoch 35/300, resid Loss: 0.1114 | 0.0511
Epoch 36/300, resid Loss: 0.1059 | 0.0554
Epoch 37/300, resid Loss: 0.1180 | 0.0612
Epoch 38/300, resid Loss: 0.1178 | 0.1030
Epoch 39/300, resid Loss: 0.1002 | 0.0427
Epoch 40/300, resid Loss: 0.0961 | 0.0564
Epoch 41/300, resid Loss: 0.0964 | 0.0458
Epoch 42/300, resid Loss: 0.0901 | 0.0431
Epoch 43/300, resid Loss: 0.0976 | 0.0570
Epoch 44/300, resid Loss: 0.0902 | 0.0505
Epoch 45/300, resid Loss: 0.0858 | 0.0489
Epoch 46/300, resid Loss: 0.0965 | 0.0705
Epoch 47/300, resid Loss: 0.0887 | 0.0598
Epoch 48/300, resid Loss: 0.0773 | 0.0697
Epoch 49/300, resid Loss: 0.0867 | 0.0572
Epoch 50/300, resid Loss: 0.0882 | 0.0370
Epoch 51/300, resid Loss: 0.0786 | 0.0412
Epoch 52/300, resid Loss: 0.0822 | 0.0499
Epoch 53/300, resid Loss: 0.0858 | 0.0415
Epoch 54/300, resid Loss: 0.0751 | 0.0440
Epoch 55/300, resid Loss: 0.0713 | 0.0384
Epoch 56/300, resid Loss: 0.0803 | 0.0425
Epoch 57/300, resid Loss: 0.0744 | 0.0441
Epoch 58/300, resid Loss: 0.0745 | 0.0428
Epoch 59/300, resid Loss: 0.0765 | 0.0418
Epoch 60/300, resid Loss: 0.0759 | 0.0388
Epoch 61/300, resid Loss: 0.0689 | 0.0486
Epoch 62/300, resid Loss: 0.0720 | 0.0513
Epoch 63/300, resid Loss: 0.0663 | 0.0465
Epoch 64/300, resid Loss: 0.0729 | 0.0458
Epoch 65/300, resid Loss: 0.0825 | 0.0445
Epoch 66/300, resid Loss: 0.0780 | 0.0420
Epoch 67/300, resid Loss: 0.0731 | 0.0367
Epoch 68/300, resid Loss: 0.0728 | 0.0429
Epoch 69/300, resid Loss: 0.0800 | 0.0504
Epoch 70/300, resid Loss: 0.0751 | 0.0604
Epoch 71/300, resid Loss: 0.0659 | 0.0418
Epoch 72/300, resid Loss: 0.0738 | 0.0382
Epoch 73/300, resid Loss: 0.0722 | 0.0349
Epoch 74/300, resid Loss: 0.0650 | 0.0394
Epoch 75/300, resid Loss: 0.0635 | 0.0391
Epoch 76/300, resid Loss: 0.0633 | 0.0377
Epoch 77/300, resid Loss: 0.0626 | 0.0376
Epoch 78/300, resid Loss: 0.0633 | 0.0362
Epoch 79/300, resid Loss: 0.0625 | 0.0357
Epoch 80/300, resid Loss: 0.0610 | 0.0353
Epoch 81/300, resid Loss: 0.0601 | 0.0360
Epoch 82/300, resid Loss: 0.0587 | 0.0330
Epoch 83/300, resid Loss: 0.0577 | 0.0324
Epoch 84/300, resid Loss: 0.0573 | 0.0318
Epoch 85/300, resid Loss: 0.0570 | 0.0316
Epoch 86/300, resid Loss: 0.0568 | 0.0318
Epoch 87/300, resid Loss: 0.0566 | 0.0319
Epoch 88/300, resid Loss: 0.0565 | 0.0316
Epoch 89/300, resid Loss: 0.0563 | 0.0315
Epoch 90/300, resid Loss: 0.0562 | 0.0313
Epoch 91/300, resid Loss: 0.0561 | 0.0310
Epoch 92/300, resid Loss: 0.0560 | 0.0307
Epoch 93/300, resid Loss: 0.0560 | 0.0306
Epoch 94/300, resid Loss: 0.0559 | 0.0303
Epoch 95/300, resid Loss: 0.0557 | 0.0301
Epoch 96/300, resid Loss: 0.0557 | 0.0304
Epoch 97/300, resid Loss: 0.0557 | 0.0299
Epoch 98/300, resid Loss: 0.0561 | 0.0313
Epoch 99/300, resid Loss: 0.0562 | 0.0308
Epoch 100/300, resid Loss: 0.0555 | 0.0305
Epoch 101/300, resid Loss: 0.0555 | 0.0298
Epoch 102/300, resid Loss: 0.0558 | 0.0296
Epoch 103/300, resid Loss: 0.0555 | 0.0292
Epoch 104/300, resid Loss: 0.0548 | 0.0288
Epoch 105/300, resid Loss: 0.0548 | 0.0292
Epoch 106/300, resid Loss: 0.0547 | 0.0300
Epoch 107/300, resid Loss: 0.0546 | 0.0298
Epoch 108/300, resid Loss: 0.0543 | 0.0294
Epoch 109/300, resid Loss: 0.0543 | 0.0288
Epoch 110/300, resid Loss: 0.0542 | 0.0287
Epoch 111/300, resid Loss: 0.0540 | 0.0285
Epoch 112/300, resid Loss: 0.0539 | 0.0285
Epoch 113/300, resid Loss: 0.0538 | 0.0289
Epoch 114/300, resid Loss: 0.0538 | 0.0293
Epoch 115/300, resid Loss: 0.0536 | 0.0291
Epoch 116/300, resid Loss: 0.0535 | 0.0286
Epoch 117/300, resid Loss: 0.0536 | 0.0284
Epoch 118/300, resid Loss: 0.0535 | 0.0283
Epoch 119/300, resid Loss: 0.0533 | 0.0282
Epoch 120/300, resid Loss: 0.0533 | 0.0281
Epoch 121/300, resid Loss: 0.0533 | 0.0292
Epoch 122/300, resid Loss: 0.0532 | 0.0290
Epoch 123/300, resid Loss: 0.0530 | 0.0287
Epoch 124/300, resid Loss: 0.0530 | 0.0283
Epoch 125/300, resid Loss: 0.0529 | 0.0283
Epoch 126/300, resid Loss: 0.0528 | 0.0282
Epoch 127/300, resid Loss: 0.0527 | 0.0282
Epoch 128/300, resid Loss: 0.0528 | 0.0288
Epoch 129/300, resid Loss: 0.0528 | 0.0289
Epoch 130/300, resid Loss: 0.0526 | 0.0286
Epoch 131/300, resid Loss: 0.0526 | 0.0282
Epoch 132/300, resid Loss: 0.0526 | 0.0282
Epoch 133/300, resid Loss: 0.0525 | 0.0282
Epoch 134/300, resid Loss: 0.0524 | 0.0282
Epoch 135/300, resid Loss: 0.0525 | 0.0287
Epoch 136/300, resid Loss: 0.0524 | 0.0286
Epoch 137/300, resid Loss: 0.0523 | 0.0283
Epoch 138/300, resid Loss: 0.0523 | 0.0281
Epoch 139/300, resid Loss: 0.0522 | 0.0282
Epoch 140/300, resid Loss: 0.0521 | 0.0281
Epoch 141/300, resid Loss: 0.0521 | 0.0285
Epoch 142/300, resid Loss: 0.0521 | 0.0284
Epoch 143/300, resid Loss: 0.0520 | 0.0282
Epoch 144/300, resid Loss: 0.0520 | 0.0281
Epoch 145/300, resid Loss: 0.0519 | 0.0281
Epoch 146/300, resid Loss: 0.0519 | 0.0282
Epoch 147/300, resid Loss: 0.0519 | 0.0284
Epoch 148/300, resid Loss: 0.0518 | 0.0283
Epoch 149/300, resid Loss: 0.0518 | 0.0281
Epoch 150/300, resid Loss: 0.0518 | 0.0281
Epoch 151/300, resid Loss: 0.0517 | 0.0281
Epoch 152/300, resid Loss: 0.0517 | 0.0282
Epoch 153/300, resid Loss: 0.0517 | 0.0283
Epoch 154/300, resid Loss: 0.0516 | 0.0282
Epoch 155/300, resid Loss: 0.0516 | 0.0281
Epoch 156/300, resid Loss: 0.0516 | 0.0281
Epoch 157/300, resid Loss: 0.0515 | 0.0281
Epoch 158/300, resid Loss: 0.0515 | 0.0282
Epoch 159/300, resid Loss: 0.0515 | 0.0282
Epoch 160/300, resid Loss: 0.0515 | 0.0281
Epoch 161/300, resid Loss: 0.0515 | 0.0281
Epoch 162/300, resid Loss: 0.0514 | 0.0281
Epoch 163/300, resid Loss: 0.0514 | 0.0281
Epoch 164/300, resid Loss: 0.0514 | 0.0281
Epoch 165/300, resid Loss: 0.0514 | 0.0281
Epoch 166/300, resid Loss: 0.0514 | 0.0281
Epoch 167/300, resid Loss: 0.0513 | 0.0281
Epoch 168/300, resid Loss: 0.0513 | 0.0281
Epoch 169/300, resid Loss: 0.0513 | 0.0281
Epoch 170/300, resid Loss: 0.0513 | 0.0281
Epoch 171/300, resid Loss: 0.0513 | 0.0281
Epoch 172/300, resid Loss: 0.0512 | 0.0281
Epoch 173/300, resid Loss: 0.0512 | 0.0281
Epoch 174/300, resid Loss: 0.0512 | 0.0281
Epoch 175/300, resid Loss: 0.0512 | 0.0281
Epoch 176/300, resid Loss: 0.0512 | 0.0281
Epoch 177/300, resid Loss: 0.0512 | 0.0281
Epoch 178/300, resid Loss: 0.0512 | 0.0281
Epoch 179/300, resid Loss: 0.0511 | 0.0281
Epoch 180/300, resid Loss: 0.0511 | 0.0281
Epoch 181/300, resid Loss: 0.0511 | 0.0281
Epoch 182/300, resid Loss: 0.0511 | 0.0281
Epoch 183/300, resid Loss: 0.0511 | 0.0281
Epoch 184/300, resid Loss: 0.0511 | 0.0281
Epoch 185/300, resid Loss: 0.0511 | 0.0281
Epoch 186/300, resid Loss: 0.0511 | 0.0281
Epoch 187/300, resid Loss: 0.0510 | 0.0281
Epoch 188/300, resid Loss: 0.0510 | 0.0281
Epoch 189/300, resid Loss: 0.0510 | 0.0281
Epoch 190/300, resid Loss: 0.0510 | 0.0281
Epoch 191/300, resid Loss: 0.0510 | 0.0281
Epoch 192/300, resid Loss: 0.0510 | 0.0281
Epoch 193/300, resid Loss: 0.0510 | 0.0280
Epoch 194/300, resid Loss: 0.0510 | 0.0280
Epoch 195/300, resid Loss: 0.0510 | 0.0280
Epoch 196/300, resid Loss: 0.0510 | 0.0280
Epoch 197/300, resid Loss: 0.0509 | 0.0280
Epoch 198/300, resid Loss: 0.0509 | 0.0280
Epoch 199/300, resid Loss: 0.0509 | 0.0280
Epoch 200/300, resid Loss: 0.0509 | 0.0280
Epoch 201/300, resid Loss: 0.0509 | 0.0280
Epoch 202/300, resid Loss: 0.0509 | 0.0280
Epoch 203/300, resid Loss: 0.0509 | 0.0280
Epoch 204/300, resid Loss: 0.0509 | 0.0280
Epoch 205/300, resid Loss: 0.0509 | 0.0280
Epoch 206/300, resid Loss: 0.0509 | 0.0280
Epoch 207/300, resid Loss: 0.0509 | 0.0280
Epoch 208/300, resid Loss: 0.0509 | 0.0280
Epoch 209/300, resid Loss: 0.0509 | 0.0280
Epoch 210/300, resid Loss: 0.0508 | 0.0280
Epoch 211/300, resid Loss: 0.0508 | 0.0280
Epoch 212/300, resid Loss: 0.0508 | 0.0280
Epoch 213/300, resid Loss: 0.0508 | 0.0280
Epoch 214/300, resid Loss: 0.0508 | 0.0280
Epoch 215/300, resid Loss: 0.0508 | 0.0280
Epoch 216/300, resid Loss: 0.0508 | 0.0280
Epoch 217/300, resid Loss: 0.0508 | 0.0280
Epoch 218/300, resid Loss: 0.0508 | 0.0280
Epoch 219/300, resid Loss: 0.0508 | 0.0280
Epoch 220/300, resid Loss: 0.0508 | 0.0280
Epoch 221/300, resid Loss: 0.0508 | 0.0280
Epoch 222/300, resid Loss: 0.0508 | 0.0280
Epoch 223/300, resid Loss: 0.0508 | 0.0280
Epoch 224/300, resid Loss: 0.0508 | 0.0280
Epoch 225/300, resid Loss: 0.0508 | 0.0280
Epoch 226/300, resid Loss: 0.0508 | 0.0280
Epoch 227/300, resid Loss: 0.0508 | 0.0280
Epoch 228/300, resid Loss: 0.0508 | 0.0280
Epoch 229/300, resid Loss: 0.0508 | 0.0280
Epoch 230/300, resid Loss: 0.0508 | 0.0280
Epoch 231/300, resid Loss: 0.0508 | 0.0280
Epoch 232/300, resid Loss: 0.0508 | 0.0280
Epoch 233/300, resid Loss: 0.0507 | 0.0280
Epoch 234/300, resid Loss: 0.0507 | 0.0280
Epoch 235/300, resid Loss: 0.0507 | 0.0280
Epoch 236/300, resid Loss: 0.0507 | 0.0280
Epoch 237/300, resid Loss: 0.0507 | 0.0280
Epoch 238/300, resid Loss: 0.0507 | 0.0280
Epoch 239/300, resid Loss: 0.0507 | 0.0280
Epoch 240/300, resid Loss: 0.0507 | 0.0280
Epoch 241/300, resid Loss: 0.0507 | 0.0280
Epoch 242/300, resid Loss: 0.0507 | 0.0280
Epoch 243/300, resid Loss: 0.0507 | 0.0280
Epoch 244/300, resid Loss: 0.0507 | 0.0280
Epoch 245/300, resid Loss: 0.0507 | 0.0280
Epoch 246/300, resid Loss: 0.0507 | 0.0280
Epoch 247/300, resid Loss: 0.0507 | 0.0280
Epoch 248/300, resid Loss: 0.0507 | 0.0280
Epoch 249/300, resid Loss: 0.0507 | 0.0280
Epoch 250/300, resid Loss: 0.0507 | 0.0280
Epoch 251/300, resid Loss: 0.0507 | 0.0280
Epoch 252/300, resid Loss: 0.0507 | 0.0280
Epoch 253/300, resid Loss: 0.0507 | 0.0280
Epoch 254/300, resid Loss: 0.0507 | 0.0280
Epoch 255/300, resid Loss: 0.0507 | 0.0280
Epoch 256/300, resid Loss: 0.0507 | 0.0280
Epoch 257/300, resid Loss: 0.0507 | 0.0280
Epoch 258/300, resid Loss: 0.0507 | 0.0280
Epoch 259/300, resid Loss: 0.0507 | 0.0280
Epoch 260/300, resid Loss: 0.0507 | 0.0280
Epoch 261/300, resid Loss: 0.0507 | 0.0280
Epoch 262/300, resid Loss: 0.0507 | 0.0280
Epoch 263/300, resid Loss: 0.0507 | 0.0280
Epoch 264/300, resid Loss: 0.0507 | 0.0280
Epoch 265/300, resid Loss: 0.0507 | 0.0280
Epoch 266/300, resid Loss: 0.0507 | 0.0280
Epoch 267/300, resid Loss: 0.0507 | 0.0280
Epoch 268/300, resid Loss: 0.0507 | 0.0280
Epoch 269/300, resid Loss: 0.0507 | 0.0280
Epoch 270/300, resid Loss: 0.0507 | 0.0280
Epoch 271/300, resid Loss: 0.0507 | 0.0280
Epoch 272/300, resid Loss: 0.0507 | 0.0280
Epoch 273/300, resid Loss: 0.0507 | 0.0280
Epoch 274/300, resid Loss: 0.0507 | 0.0280
Epoch 275/300, resid Loss: 0.0507 | 0.0280
Epoch 276/300, resid Loss: 0.0507 | 0.0280
Epoch 277/300, resid Loss: 0.0507 | 0.0280
Epoch 278/300, resid Loss: 0.0507 | 0.0280
Epoch 279/300, resid Loss: 0.0507 | 0.0280
Epoch 280/300, resid Loss: 0.0507 | 0.0280
Epoch 281/300, resid Loss: 0.0507 | 0.0280
Epoch 282/300, resid Loss: 0.0507 | 0.0280
Epoch 283/300, resid Loss: 0.0507 | 0.0280
Epoch 284/300, resid Loss: 0.0507 | 0.0280
Epoch 285/300, resid Loss: 0.0507 | 0.0280
Epoch 286/300, resid Loss: 0.0507 | 0.0280
Epoch 287/300, resid Loss: 0.0507 | 0.0280
Epoch 288/300, resid Loss: 0.0507 | 0.0280
Epoch 289/300, resid Loss: 0.0507 | 0.0280
Epoch 290/300, resid Loss: 0.0507 | 0.0280
Epoch 291/300, resid Loss: 0.0507 | 0.0280
Epoch 292/300, resid Loss: 0.0507 | 0.0280
Epoch 293/300, resid Loss: 0.0507 | 0.0280
Epoch 294/300, resid Loss: 0.0507 | 0.0280
Epoch 295/300, resid Loss: 0.0507 | 0.0280
Epoch 296/300, resid Loss: 0.0507 | 0.0280
Epoch 297/300, resid Loss: 0.0507 | 0.0280
Epoch 298/300, resid Loss: 0.0507 | 0.0280
Epoch 299/300, resid Loss: 0.0507 | 0.0280
Epoch 300/300, resid Loss: 0.0507 | 0.0280
Runtime (seconds): 11169.68670964241
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[161.03757]
[-4.884067]
[-0.106794]
[13.707309]
[0.6811399]
[20.176504]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 297.50564846768975
RMSE: 17.24835205078125
MAE: 17.24835205078125
R-squared: nan
[190.61165]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
