[32m[I 2025-02-05 22:59:31,605][0m A new study created in memory with name: no-name-a6fbc35a-bbbb-4c66-a901-46800ea3fc8f[0m
[32m[I 2025-02-05 23:00:00,783][0m Trial 0 finished with value: 0.579648494720459 and parameters: {'observation_period_num': 202, 'train_rates': 0.9808162590590399, 'learning_rate': 7.687751002155975e-06, 'batch_size': 217, 'step_size': 12, 'gamma': 0.9630508786281498}. Best is trial 0 with value: 0.579648494720459.[0m
[32m[I 2025-02-05 23:00:40,274][0m Trial 1 finished with value: 0.40464831466343315 and parameters: {'observation_period_num': 178, 'train_rates': 0.7995593923231701, 'learning_rate': 2.5171063518431556e-05, 'batch_size': 139, 'step_size': 2, 'gamma': 0.9018853940599934}. Best is trial 1 with value: 0.40464831466343315.[0m
Early stopping at epoch 66
[32m[I 2025-02-05 23:01:06,062][0m Trial 2 finished with value: 0.796906479868093 and parameters: {'observation_period_num': 118, 'train_rates': 0.7337837614407376, 'learning_rate': 2.63074814817216e-05, 'batch_size': 134, 'step_size': 1, 'gamma': 0.8369826270939865}. Best is trial 1 with value: 0.40464831466343315.[0m
[32m[I 2025-02-05 23:01:28,052][0m Trial 3 finished with value: 0.683498956851269 and parameters: {'observation_period_num': 154, 'train_rates': 0.7844772166406792, 'learning_rate': 1.165327940144139e-06, 'batch_size': 253, 'step_size': 9, 'gamma': 0.9232357811100672}. Best is trial 1 with value: 0.40464831466343315.[0m
[32m[I 2025-02-05 23:03:49,037][0m Trial 4 finished with value: 0.20016212441377193 and parameters: {'observation_period_num': 247, 'train_rates': 0.9456728676510078, 'learning_rate': 0.00023281941409636649, 'batch_size': 38, 'step_size': 11, 'gamma': 0.9118262030473716}. Best is trial 4 with value: 0.20016212441377193.[0m
[32m[I 2025-02-05 23:04:46,323][0m Trial 5 finished with value: 0.9425418916530477 and parameters: {'observation_period_num': 153, 'train_rates': 0.8468325528045701, 'learning_rate': 1.9024363588097203e-06, 'batch_size': 94, 'step_size': 6, 'gamma': 0.7680249860621619}. Best is trial 4 with value: 0.20016212441377193.[0m
[32m[I 2025-02-05 23:06:00,548][0m Trial 6 finished with value: 0.335534037399292 and parameters: {'observation_period_num': 173, 'train_rates': 0.7770831713187653, 'learning_rate': 1.3918199321312808e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8853035538756091}. Best is trial 4 with value: 0.20016212441377193.[0m
[32m[I 2025-02-05 23:06:58,969][0m Trial 7 finished with value: 0.40012409015504086 and parameters: {'observation_period_num': 53, 'train_rates': 0.6379260895010971, 'learning_rate': 1.9302374047323433e-06, 'batch_size': 80, 'step_size': 9, 'gamma': 0.9713745587483853}. Best is trial 4 with value: 0.20016212441377193.[0m
[32m[I 2025-02-05 23:07:32,661][0m Trial 8 finished with value: 0.11827900867705674 and parameters: {'observation_period_num': 96, 'train_rates': 0.8540686841909191, 'learning_rate': 0.000393402434952261, 'batch_size': 164, 'step_size': 4, 'gamma': 0.7777795097531358}. Best is trial 8 with value: 0.11827900867705674.[0m
[32m[I 2025-02-05 23:08:03,051][0m Trial 9 finished with value: 0.7506303235029368 and parameters: {'observation_period_num': 137, 'train_rates': 0.8231126555265736, 'learning_rate': 1.1146869833016437e-06, 'batch_size': 183, 'step_size': 15, 'gamma': 0.960612565959763}. Best is trial 8 with value: 0.11827900867705674.[0m
[32m[I 2025-02-05 23:08:38,275][0m Trial 10 finished with value: 0.05359353345165897 and parameters: {'observation_period_num': 24, 'train_rates': 0.9049365589137541, 'learning_rate': 0.0009741868088392046, 'batch_size': 168, 'step_size': 5, 'gamma': 0.7739014454316042}. Best is trial 10 with value: 0.05359353345165897.[0m
[32m[I 2025-02-05 23:09:13,357][0m Trial 11 finished with value: 0.04325260678367899 and parameters: {'observation_period_num': 5, 'train_rates': 0.8986062582508381, 'learning_rate': 0.0009286398972471974, 'batch_size': 167, 'step_size': 5, 'gamma': 0.7681773987331723}. Best is trial 11 with value: 0.04325260678367899.[0m
[32m[I 2025-02-05 23:09:44,226][0m Trial 12 finished with value: 0.042640632261400635 and parameters: {'observation_period_num': 7, 'train_rates': 0.9071453877922334, 'learning_rate': 0.0007126965127908826, 'batch_size': 198, 'step_size': 6, 'gamma': 0.8232783072333371}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:10:14,206][0m Trial 13 finished with value: 0.06766668346884487 and parameters: {'observation_period_num': 7, 'train_rates': 0.9082683093879372, 'learning_rate': 0.00012073763967232145, 'batch_size': 213, 'step_size': 6, 'gamma': 0.8273671938271739}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:10:43,505][0m Trial 14 finished with value: 0.08796489416591582 and parameters: {'observation_period_num': 59, 'train_rates': 0.8938146380145391, 'learning_rate': 0.0009285994312367932, 'batch_size': 202, 'step_size': 3, 'gamma': 0.8165864282995006}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:11:10,479][0m Trial 15 finished with value: 0.1751258224248886 and parameters: {'observation_period_num': 55, 'train_rates': 0.9819628863821208, 'learning_rate': 9.21330736161468e-05, 'batch_size': 244, 'step_size': 7, 'gamma': 0.7983112177388882}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:11:50,771][0m Trial 16 finished with value: 0.14810813940157092 and parameters: {'observation_period_num': 28, 'train_rates': 0.7066361656073001, 'learning_rate': 0.0003824275754971099, 'batch_size': 129, 'step_size': 8, 'gamma': 0.8527227589553943}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:12:42,834][0m Trial 17 finished with value: 0.14671190226885358 and parameters: {'observation_period_num': 88, 'train_rates': 0.8801434399854845, 'learning_rate': 7.125826791269323e-05, 'batch_size': 109, 'step_size': 4, 'gamma': 0.8037845002696153}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:13:17,800][0m Trial 18 finished with value: 0.04590500883527638 and parameters: {'observation_period_num': 9, 'train_rates': 0.9347800704184694, 'learning_rate': 0.0003928431830662102, 'batch_size': 182, 'step_size': 10, 'gamma': 0.7513337449015906}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:13:57,132][0m Trial 19 finished with value: 0.11488244154993792 and parameters: {'observation_period_num': 76, 'train_rates': 0.9447957266095541, 'learning_rate': 0.00016735222931217443, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8587373436441955}. Best is trial 12 with value: 0.042640632261400635.[0m
Early stopping at epoch 69
[32m[I 2025-02-05 23:14:12,602][0m Trial 20 finished with value: 0.23790426635223885 and parameters: {'observation_period_num': 37, 'train_rates': 0.6087519678100958, 'learning_rate': 0.0006597860744603364, 'batch_size': 229, 'step_size': 1, 'gamma': 0.7958014233171031}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:14:45,318][0m Trial 21 finished with value: 0.042802851647138596 and parameters: {'observation_period_num': 12, 'train_rates': 0.9417416482734883, 'learning_rate': 0.0004098302839549526, 'batch_size': 190, 'step_size': 10, 'gamma': 0.7514666412723975}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:15:17,405][0m Trial 22 finished with value: 0.05978393242182329 and parameters: {'observation_period_num': 6, 'train_rates': 0.8683971635874588, 'learning_rate': 0.0004761537382085509, 'batch_size': 196, 'step_size': 5, 'gamma': 0.7548295802394123}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:15:50,813][0m Trial 23 finished with value: 0.06700536357769694 and parameters: {'observation_period_num': 35, 'train_rates': 0.9344837930294212, 'learning_rate': 0.00022550828573008396, 'batch_size': 184, 'step_size': 8, 'gamma': 0.7856888081086457}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:16:33,807][0m Trial 24 finished with value: 0.130389004945755 and parameters: {'observation_period_num': 69, 'train_rates': 0.9888315493301184, 'learning_rate': 5.865563423724217e-05, 'batch_size': 150, 'step_size': 10, 'gamma': 0.8142019318607817}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:16:59,709][0m Trial 25 finished with value: 0.051889330280695374 and parameters: {'observation_period_num': 41, 'train_rates': 0.8269000435968938, 'learning_rate': 0.0005940999546681397, 'batch_size': 230, 'step_size': 14, 'gamma': 0.7509770767309993}. Best is trial 12 with value: 0.042640632261400635.[0m
[32m[I 2025-02-05 23:17:51,716][0m Trial 26 finished with value: 0.04211641587619859 and parameters: {'observation_period_num': 21, 'train_rates': 0.9167943874759414, 'learning_rate': 0.00025249084062720474, 'batch_size': 116, 'step_size': 6, 'gamma': 0.8427529921631925}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:21:30,666][0m Trial 27 finished with value: 0.15893746451252982 and parameters: {'observation_period_num': 104, 'train_rates': 0.9563045135154682, 'learning_rate': 0.00025796343179326466, 'batch_size': 26, 'step_size': 11, 'gamma': 0.8734925781829019}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:22:23,567][0m Trial 28 finished with value: 0.04612018784255157 and parameters: {'observation_period_num': 21, 'train_rates': 0.9179241442298076, 'learning_rate': 0.00017455652842602684, 'batch_size': 115, 'step_size': 7, 'gamma': 0.837010009293974}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:24:11,026][0m Trial 29 finished with value: 0.1849303010564584 and parameters: {'observation_period_num': 50, 'train_rates': 0.9556411707954003, 'learning_rate': 7.125527101473199e-06, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9358239687915363}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:24:40,768][0m Trial 30 finished with value: 0.3108072280883789 and parameters: {'observation_period_num': 210, 'train_rates': 0.9671252401787758, 'learning_rate': 4.9467029405486475e-05, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8504272066626796}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:25:14,616][0m Trial 31 finished with value: 0.04986163601279259 and parameters: {'observation_period_num': 19, 'train_rates': 0.8817077316025068, 'learning_rate': 0.0009658517020951202, 'batch_size': 175, 'step_size': 5, 'gamma': 0.7871936112625412}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:25:55,960][0m Trial 32 finished with value: 0.04590534772105322 and parameters: {'observation_period_num': 23, 'train_rates': 0.9083507111158886, 'learning_rate': 0.0005569120352803851, 'batch_size': 146, 'step_size': 6, 'gamma': 0.7655257603789435}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:26:26,833][0m Trial 33 finished with value: 0.06494166875421177 and parameters: {'observation_period_num': 6, 'train_rates': 0.9272319386919372, 'learning_rate': 0.00028218343101035385, 'batch_size': 197, 'step_size': 4, 'gamma': 0.8191790569064574}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:27:08,212][0m Trial 34 finished with value: 0.17275434249102026 and parameters: {'observation_period_num': 41, 'train_rates': 0.7469832050936653, 'learning_rate': 0.0006990624422807097, 'batch_size': 126, 'step_size': 3, 'gamma': 0.88722863294902}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:27:35,710][0m Trial 35 finished with value: 0.27739150850957145 and parameters: {'observation_period_num': 67, 'train_rates': 0.8578408860491736, 'learning_rate': 0.0001340941543360606, 'batch_size': 226, 'step_size': 2, 'gamma': 0.840050621194338}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:28:32,764][0m Trial 36 finished with value: 0.07939456010783766 and parameters: {'observation_period_num': 82, 'train_rates': 0.8387360019720274, 'learning_rate': 0.0003247169206842615, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8693390782171542}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:29:14,006][0m Trial 37 finished with value: 0.22237465460784733 and parameters: {'observation_period_num': 18, 'train_rates': 0.805360228445312, 'learning_rate': 1.3061967900399419e-05, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8063513888293415}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:29:50,909][0m Trial 38 finished with value: 0.11298343681153797 and parameters: {'observation_period_num': 123, 'train_rates': 0.8897035509749301, 'learning_rate': 0.0006884495708121942, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7626636794707521}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:30:22,583][0m Trial 39 finished with value: 0.48445674777030945 and parameters: {'observation_period_num': 201, 'train_rates': 0.9638218630065863, 'learning_rate': 3.365388234737671e-05, 'batch_size': 193, 'step_size': 7, 'gamma': 0.7787406248932013}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:30:48,077][0m Trial 40 finished with value: 0.08938353371463324 and parameters: {'observation_period_num': 44, 'train_rates': 0.8705852165766902, 'learning_rate': 0.00020019256514502734, 'batch_size': 255, 'step_size': 3, 'gamma': 0.8986371430707729}. Best is trial 26 with value: 0.04211641587619859.[0m
[32m[I 2025-02-05 23:31:23,446][0m Trial 41 finished with value: 0.04144103270024061 and parameters: {'observation_period_num': 6, 'train_rates': 0.932659199431963, 'learning_rate': 0.0003814300184586202, 'batch_size': 182, 'step_size': 10, 'gamma': 0.7501337361129041}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:31:58,857][0m Trial 42 finished with value: 0.04200633106825178 and parameters: {'observation_period_num': 31, 'train_rates': 0.9211257731348836, 'learning_rate': 0.0004679364928275137, 'batch_size': 174, 'step_size': 12, 'gamma': 0.7647287028063352}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:32:33,727][0m Trial 43 finished with value: 0.05082180720191818 and parameters: {'observation_period_num': 36, 'train_rates': 0.924639632043352, 'learning_rate': 0.0004466021779637681, 'batch_size': 176, 'step_size': 13, 'gamma': 0.7827443665994493}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:33:04,702][0m Trial 44 finished with value: 0.05105166882276535 and parameters: {'observation_period_num': 16, 'train_rates': 0.9724825254348131, 'learning_rate': 0.00030764052852291974, 'batch_size': 206, 'step_size': 11, 'gamma': 0.7613433685928517}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:33:47,777][0m Trial 45 finished with value: 0.058891255148621494 and parameters: {'observation_period_num': 28, 'train_rates': 0.941744838709361, 'learning_rate': 0.0001032243327662686, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8282204425491841}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:34:16,567][0m Trial 46 finished with value: 0.07644649734099707 and parameters: {'observation_period_num': 63, 'train_rates': 0.9227611403347497, 'learning_rate': 0.00047438006784632626, 'batch_size': 217, 'step_size': 9, 'gamma': 0.7910153404929098}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:34:47,860][0m Trial 47 finished with value: 0.6809079264417107 and parameters: {'observation_period_num': 243, 'train_rates': 0.911258203574808, 'learning_rate': 4.780594105673466e-06, 'batch_size': 186, 'step_size': 12, 'gamma': 0.7771742545960431}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:35:28,629][0m Trial 48 finished with value: 0.25780170658415225 and parameters: {'observation_period_num': 136, 'train_rates': 0.675622474276334, 'learning_rate': 0.00013793073899643195, 'batch_size': 117, 'step_size': 11, 'gamma': 0.7666668638667526}. Best is trial 41 with value: 0.04144103270024061.[0m
[32m[I 2025-02-05 23:36:35,292][0m Trial 49 finished with value: 0.117518954763287 and parameters: {'observation_period_num': 51, 'train_rates': 0.8961375012026156, 'learning_rate': 0.0007267488686441147, 'batch_size': 87, 'step_size': 13, 'gamma': 0.8091960711581228}. Best is trial 41 with value: 0.04144103270024061.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.3723 | 0.2933
Epoch 2/300, Loss: 0.1702 | 0.1874
Epoch 3/300, Loss: 0.1468 | 0.1603
Epoch 4/300, Loss: 0.1847 | 0.3274
Epoch 5/300, Loss: 0.2271 | 0.5883
Epoch 6/300, Loss: 0.2524 | 0.2077
Epoch 7/300, Loss: 0.2048 | 0.2524
Epoch 8/300, Loss: 0.2171 | 0.2113
Epoch 9/300, Loss: 0.2031 | 0.1602
Epoch 10/300, Loss: 0.1465 | 0.1394
Epoch 11/300, Loss: 0.1450 | 0.1475
Epoch 12/300, Loss: 0.1283 | 0.1240
Epoch 13/300, Loss: 0.1197 | 0.1224
Epoch 14/300, Loss: 0.1173 | 0.1093
Epoch 15/300, Loss: 0.1132 | 0.1136
Epoch 16/300, Loss: 0.1165 | 0.1138
Epoch 17/300, Loss: 0.1167 | 0.1010
Epoch 18/300, Loss: 0.1284 | 0.1629
Epoch 19/300, Loss: 0.1266 | 0.0996
Epoch 20/300, Loss: 0.1281 | 0.1603
Epoch 21/300, Loss: 0.1157 | 0.0928
Epoch 22/300, Loss: 0.1104 | 0.1227
Epoch 23/300, Loss: 0.1051 | 0.0856
Epoch 24/300, Loss: 0.1046 | 0.1010
Epoch 25/300, Loss: 0.1008 | 0.0804
Epoch 26/300, Loss: 0.1006 | 0.0939
Epoch 27/300, Loss: 0.0980 | 0.0771
Epoch 28/300, Loss: 0.0982 | 0.0837
Epoch 29/300, Loss: 0.0965 | 0.0745
Epoch 30/300, Loss: 0.0968 | 0.0782
Epoch 31/300, Loss: 0.0953 | 0.0725
Epoch 32/300, Loss: 0.0953 | 0.0750
Epoch 33/300, Loss: 0.0943 | 0.0711
Epoch 34/300, Loss: 0.0942 | 0.0719
Epoch 35/300, Loss: 0.0935 | 0.0698
Epoch 36/300, Loss: 0.0932 | 0.0703
Epoch 37/300, Loss: 0.0927 | 0.0691
Epoch 38/300, Loss: 0.0924 | 0.0689
Epoch 39/300, Loss: 0.0921 | 0.0683
Epoch 40/300, Loss: 0.0917 | 0.0679
Epoch 41/300, Loss: 0.0914 | 0.0676
Epoch 42/300, Loss: 0.0911 | 0.0673
Epoch 43/300, Loss: 0.0908 | 0.0669
Epoch 44/300, Loss: 0.0905 | 0.0666
Epoch 45/300, Loss: 0.0902 | 0.0663
Epoch 46/300, Loss: 0.0900 | 0.0661
Epoch 47/300, Loss: 0.0897 | 0.0659
Epoch 48/300, Loss: 0.0895 | 0.0656
Epoch 49/300, Loss: 0.0893 | 0.0654
Epoch 50/300, Loss: 0.0891 | 0.0652
Epoch 51/300, Loss: 0.0889 | 0.0650
Epoch 52/300, Loss: 0.0887 | 0.0649
Epoch 53/300, Loss: 0.0885 | 0.0647
Epoch 54/300, Loss: 0.0884 | 0.0645
Epoch 55/300, Loss: 0.0882 | 0.0643
Epoch 56/300, Loss: 0.0880 | 0.0642
Epoch 57/300, Loss: 0.0879 | 0.0641
Epoch 58/300, Loss: 0.0878 | 0.0640
Epoch 59/300, Loss: 0.0877 | 0.0638
Epoch 60/300, Loss: 0.0875 | 0.0637
Epoch 61/300, Loss: 0.0874 | 0.0636
Epoch 62/300, Loss: 0.0873 | 0.0635
Epoch 63/300, Loss: 0.0872 | 0.0634
Epoch 64/300, Loss: 0.0871 | 0.0633
Epoch 65/300, Loss: 0.0870 | 0.0632
Epoch 66/300, Loss: 0.0869 | 0.0631
Epoch 67/300, Loss: 0.0869 | 0.0631
Epoch 68/300, Loss: 0.0868 | 0.0630
Epoch 69/300, Loss: 0.0867 | 0.0629
Epoch 70/300, Loss: 0.0866 | 0.0628
Epoch 71/300, Loss: 0.0866 | 0.0628
Epoch 72/300, Loss: 0.0865 | 0.0627
Epoch 73/300, Loss: 0.0865 | 0.0627
Epoch 74/300, Loss: 0.0864 | 0.0626
Epoch 75/300, Loss: 0.0863 | 0.0626
Epoch 76/300, Loss: 0.0863 | 0.0625
Epoch 77/300, Loss: 0.0863 | 0.0625
Epoch 78/300, Loss: 0.0862 | 0.0624
Epoch 79/300, Loss: 0.0862 | 0.0624
Epoch 80/300, Loss: 0.0861 | 0.0623
Epoch 81/300, Loss: 0.0861 | 0.0623
Epoch 82/300, Loss: 0.0860 | 0.0623
Epoch 83/300, Loss: 0.0860 | 0.0622
Epoch 84/300, Loss: 0.0860 | 0.0622
Epoch 85/300, Loss: 0.0859 | 0.0622
Epoch 86/300, Loss: 0.0859 | 0.0621
Epoch 87/300, Loss: 0.0859 | 0.0621
Epoch 88/300, Loss: 0.0859 | 0.0621
Epoch 89/300, Loss: 0.0858 | 0.0621
Epoch 90/300, Loss: 0.0858 | 0.0620
Epoch 91/300, Loss: 0.0858 | 0.0620
Epoch 92/300, Loss: 0.0858 | 0.0620
Epoch 93/300, Loss: 0.0857 | 0.0620
Epoch 94/300, Loss: 0.0857 | 0.0620
Epoch 95/300, Loss: 0.0857 | 0.0619
Epoch 96/300, Loss: 0.0857 | 0.0619
Epoch 97/300, Loss: 0.0857 | 0.0619
Epoch 98/300, Loss: 0.0857 | 0.0619
Epoch 99/300, Loss: 0.0856 | 0.0619
Epoch 100/300, Loss: 0.0856 | 0.0619
Epoch 101/300, Loss: 0.0856 | 0.0618
Epoch 102/300, Loss: 0.0856 | 0.0618
Epoch 103/300, Loss: 0.0856 | 0.0618
Epoch 104/300, Loss: 0.0856 | 0.0618
Epoch 105/300, Loss: 0.0856 | 0.0618
Epoch 106/300, Loss: 0.0856 | 0.0618
Epoch 107/300, Loss: 0.0856 | 0.0618
Epoch 108/300, Loss: 0.0855 | 0.0618
Epoch 109/300, Loss: 0.0855 | 0.0618
Epoch 110/300, Loss: 0.0855 | 0.0617
Epoch 111/300, Loss: 0.0855 | 0.0617
Epoch 112/300, Loss: 0.0855 | 0.0617
Epoch 113/300, Loss: 0.0855 | 0.0617
Epoch 114/300, Loss: 0.0855 | 0.0617
Epoch 115/300, Loss: 0.0855 | 0.0617
Epoch 116/300, Loss: 0.0855 | 0.0617
Epoch 117/300, Loss: 0.0855 | 0.0617
Epoch 118/300, Loss: 0.0855 | 0.0617
Epoch 119/300, Loss: 0.0855 | 0.0617
Epoch 120/300, Loss: 0.0855 | 0.0617
Epoch 121/300, Loss: 0.0855 | 0.0617
Epoch 122/300, Loss: 0.0855 | 0.0617
Epoch 123/300, Loss: 0.0854 | 0.0617
Epoch 124/300, Loss: 0.0854 | 0.0617
Epoch 125/300, Loss: 0.0854 | 0.0617
Epoch 126/300, Loss: 0.0854 | 0.0617
Epoch 127/300, Loss: 0.0854 | 0.0617
Epoch 128/300, Loss: 0.0854 | 0.0617
Epoch 129/300, Loss: 0.0854 | 0.0616
Epoch 130/300, Loss: 0.0854 | 0.0616
Epoch 131/300, Loss: 0.0854 | 0.0616
Epoch 132/300, Loss: 0.0854 | 0.0616
Epoch 133/300, Loss: 0.0854 | 0.0616
Epoch 134/300, Loss: 0.0854 | 0.0616
Epoch 135/300, Loss: 0.0854 | 0.0616
Epoch 136/300, Loss: 0.0854 | 0.0616
Epoch 137/300, Loss: 0.0854 | 0.0616
Epoch 138/300, Loss: 0.0854 | 0.0616
Epoch 139/300, Loss: 0.0854 | 0.0616
Epoch 140/300, Loss: 0.0854 | 0.0616
Epoch 141/300, Loss: 0.0854 | 0.0616
Epoch 142/300, Loss: 0.0854 | 0.0616
Epoch 143/300, Loss: 0.0854 | 0.0616
Epoch 144/300, Loss: 0.0854 | 0.0616
Epoch 145/300, Loss: 0.0854 | 0.0616
Epoch 146/300, Loss: 0.0854 | 0.0616
Epoch 147/300, Loss: 0.0854 | 0.0616
Epoch 148/300, Loss: 0.0854 | 0.0616
Epoch 149/300, Loss: 0.0854 | 0.0616
Epoch 150/300, Loss: 0.0854 | 0.0616
Epoch 151/300, Loss: 0.0854 | 0.0616
Epoch 152/300, Loss: 0.0854 | 0.0616
Epoch 153/300, Loss: 0.0854 | 0.0616
Epoch 154/300, Loss: 0.0854 | 0.0616
Epoch 155/300, Loss: 0.0854 | 0.0616
Epoch 156/300, Loss: 0.0854 | 0.0616
Epoch 157/300, Loss: 0.0854 | 0.0616
Epoch 158/300, Loss: 0.0854 | 0.0616
Epoch 159/300, Loss: 0.0854 | 0.0616
Epoch 160/300, Loss: 0.0854 | 0.0616
Epoch 161/300, Loss: 0.0854 | 0.0616
Epoch 162/300, Loss: 0.0854 | 0.0616
Epoch 163/300, Loss: 0.0854 | 0.0616
Epoch 164/300, Loss: 0.0854 | 0.0616
Epoch 165/300, Loss: 0.0854 | 0.0616
Epoch 166/300, Loss: 0.0854 | 0.0616
Epoch 167/300, Loss: 0.0854 | 0.0616
Epoch 168/300, Loss: 0.0854 | 0.0616
Epoch 169/300, Loss: 0.0854 | 0.0616
Epoch 170/300, Loss: 0.0854 | 0.0616
Epoch 171/300, Loss: 0.0854 | 0.0616
Epoch 172/300, Loss: 0.0854 | 0.0616
Epoch 173/300, Loss: 0.0854 | 0.0616
Epoch 174/300, Loss: 0.0854 | 0.0616
Epoch 175/300, Loss: 0.0854 | 0.0616
Epoch 176/300, Loss: 0.0854 | 0.0616
Epoch 177/300, Loss: 0.0854 | 0.0616
Epoch 178/300, Loss: 0.0854 | 0.0616
Epoch 179/300, Loss: 0.0854 | 0.0616
Epoch 180/300, Loss: 0.0854 | 0.0616
Epoch 181/300, Loss: 0.0854 | 0.0616
Epoch 182/300, Loss: 0.0854 | 0.0616
Epoch 183/300, Loss: 0.0854 | 0.0616
Epoch 184/300, Loss: 0.0854 | 0.0616
Epoch 185/300, Loss: 0.0854 | 0.0616
Epoch 186/300, Loss: 0.0854 | 0.0616
Epoch 187/300, Loss: 0.0854 | 0.0616
Epoch 188/300, Loss: 0.0854 | 0.0616
Epoch 189/300, Loss: 0.0854 | 0.0616
Epoch 190/300, Loss: 0.0854 | 0.0616
Epoch 191/300, Loss: 0.0854 | 0.0616
Epoch 192/300, Loss: 0.0854 | 0.0616
Epoch 193/300, Loss: 0.0854 | 0.0616
Epoch 194/300, Loss: 0.0854 | 0.0616
Epoch 195/300, Loss: 0.0854 | 0.0616
Epoch 196/300, Loss: 0.0854 | 0.0616
Epoch 197/300, Loss: 0.0854 | 0.0616
Epoch 198/300, Loss: 0.0854 | 0.0616
Epoch 199/300, Loss: 0.0854 | 0.0616
Epoch 200/300, Loss: 0.0854 | 0.0616
Epoch 201/300, Loss: 0.0854 | 0.0616
Epoch 202/300, Loss: 0.0854 | 0.0616
Epoch 203/300, Loss: 0.0854 | 0.0616
Epoch 204/300, Loss: 0.0854 | 0.0616
Epoch 205/300, Loss: 0.0854 | 0.0616
Epoch 206/300, Loss: 0.0854 | 0.0616
Epoch 207/300, Loss: 0.0854 | 0.0616
Epoch 208/300, Loss: 0.0854 | 0.0616
Epoch 209/300, Loss: 0.0854 | 0.0616
Epoch 210/300, Loss: 0.0854 | 0.0616
Epoch 211/300, Loss: 0.0854 | 0.0616
Epoch 212/300, Loss: 0.0854 | 0.0616
Epoch 213/300, Loss: 0.0854 | 0.0616
Epoch 214/300, Loss: 0.0854 | 0.0616
Epoch 215/300, Loss: 0.0854 | 0.0616
Epoch 216/300, Loss: 0.0854 | 0.0616
Epoch 217/300, Loss: 0.0854 | 0.0616
Epoch 218/300, Loss: 0.0854 | 0.0616
Epoch 219/300, Loss: 0.0854 | 0.0616
Epoch 220/300, Loss: 0.0854 | 0.0616
Epoch 221/300, Loss: 0.0854 | 0.0616
Epoch 222/300, Loss: 0.0854 | 0.0616
Epoch 223/300, Loss: 0.0854 | 0.0616
Epoch 224/300, Loss: 0.0854 | 0.0616
Early stopping
Runtime (seconds): 78.5882887840271
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 17.837656317511573
RMSE: 4.2234649658203125
MAE: 4.2234649658203125
R-squared: nan
[194.66347]
