[32m[I 2025-01-07 16:35:31,029][0m A new study created in memory with name: no-name-46bb56eb-0b10-4a80-9c53-c432008cae91[0m
[32m[I 2025-01-07 16:41:22,960][0m Trial 0 finished with value: 0.49846479328729776 and parameters: {'observation_period_num': 240, 'train_rates': 0.8979482037110486, 'learning_rate': 3.552116167549751e-05, 'batch_size': 156, 'step_size': 2, 'gamma': 0.8722458133054962}. Best is trial 0 with value: 0.49846479328729776.[0m
Early stopping at epoch 84
[32m[I 2025-01-07 16:44:51,513][0m Trial 1 finished with value: 1.6797711254747223 and parameters: {'observation_period_num': 195, 'train_rates': 0.7282552006368183, 'learning_rate': 1.3451263827170094e-06, 'batch_size': 160, 'step_size': 2, 'gamma': 0.7997551870799476}. Best is trial 0 with value: 0.49846479328729776.[0m
[32m[I 2025-01-07 16:47:45,763][0m Trial 2 finished with value: 0.9363133286583353 and parameters: {'observation_period_num': 142, 'train_rates': 0.7606541959225586, 'learning_rate': 0.0009029352333843435, 'batch_size': 147, 'step_size': 5, 'gamma': 0.7896561168984073}. Best is trial 0 with value: 0.49846479328729776.[0m
[32m[I 2025-01-07 16:51:05,345][0m Trial 3 finished with value: 0.8672372757998096 and parameters: {'observation_period_num': 174, 'train_rates': 0.6721260997854251, 'learning_rate': 8.942704135048976e-05, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8870141923532079}. Best is trial 0 with value: 0.49846479328729776.[0m
[32m[I 2025-01-07 16:51:41,192][0m Trial 4 finished with value: 1.294620664583312 and parameters: {'observation_period_num': 18, 'train_rates': 0.7314790194516181, 'learning_rate': 7.1968335713842025e-06, 'batch_size': 118, 'step_size': 5, 'gamma': 0.7920028015995192}. Best is trial 0 with value: 0.49846479328729776.[0m
[32m[I 2025-01-07 16:54:54,160][0m Trial 5 finished with value: 1.2886314459776473 and parameters: {'observation_period_num': 167, 'train_rates': 0.6839589601550102, 'learning_rate': 4.108779061495894e-06, 'batch_size': 113, 'step_size': 4, 'gamma': 0.9751326375273813}. Best is trial 0 with value: 0.49846479328729776.[0m
[32m[I 2025-01-07 16:59:43,181][0m Trial 6 finished with value: 0.7289008917534245 and parameters: {'observation_period_num': 218, 'train_rates': 0.753551833932788, 'learning_rate': 0.00010928689601767291, 'batch_size': 62, 'step_size': 2, 'gamma': 0.8246935760252165}. Best is trial 0 with value: 0.49846479328729776.[0m
Early stopping at epoch 88
[32m[I 2025-01-07 17:02:37,489][0m Trial 7 finished with value: 0.3628255524635315 and parameters: {'observation_period_num': 142, 'train_rates': 0.9114788425756144, 'learning_rate': 0.0007234568905933653, 'batch_size': 194, 'step_size': 1, 'gamma': 0.8570864550592787}. Best is trial 7 with value: 0.3628255524635315.[0m
[32m[I 2025-01-07 17:07:03,225][0m Trial 8 finished with value: 1.68922045655915 and parameters: {'observation_period_num': 223, 'train_rates': 0.6442634552805264, 'learning_rate': 3.0398536811766266e-06, 'batch_size': 237, 'step_size': 8, 'gamma': 0.7677799967782437}. Best is trial 7 with value: 0.3628255524635315.[0m
[32m[I 2025-01-07 17:08:36,430][0m Trial 9 finished with value: 1.321594313186704 and parameters: {'observation_period_num': 82, 'train_rates': 0.728545156333623, 'learning_rate': 1.5203058413189975e-06, 'batch_size': 157, 'step_size': 12, 'gamma': 0.9227667591414678}. Best is trial 7 with value: 0.3628255524635315.[0m
[32m[I 2025-01-07 17:10:36,351][0m Trial 10 finished with value: 0.2795909345149994 and parameters: {'observation_period_num': 89, 'train_rates': 0.981532503027227, 'learning_rate': 0.0006781768529854213, 'batch_size': 230, 'step_size': 15, 'gamma': 0.841909460708823}. Best is trial 10 with value: 0.2795909345149994.[0m
[32m[I 2025-01-07 17:12:41,653][0m Trial 11 finished with value: 2.16884446144104 and parameters: {'observation_period_num': 92, 'train_rates': 0.9854829732870527, 'learning_rate': 0.000985190594550252, 'batch_size': 231, 'step_size': 15, 'gamma': 0.8415619826367547}. Best is trial 10 with value: 0.2795909345149994.[0m
[32m[I 2025-01-07 17:14:40,917][0m Trial 12 finished with value: 0.22375567450339473 and parameters: {'observation_period_num': 95, 'train_rates': 0.8736591657037595, 'learning_rate': 0.0002241632829227896, 'batch_size': 202, 'step_size': 10, 'gamma': 0.9107259517344666}. Best is trial 12 with value: 0.22375567450339473.[0m
[32m[I 2025-01-07 17:16:20,156][0m Trial 13 finished with value: 0.28399190706450766 and parameters: {'observation_period_num': 79, 'train_rates': 0.8580877586459059, 'learning_rate': 0.000255611003460912, 'batch_size': 202, 'step_size': 12, 'gamma': 0.9139693125216642}. Best is trial 12 with value: 0.22375567450339473.[0m
[32m[I 2025-01-07 17:17:13,399][0m Trial 14 finished with value: 0.13707546889781952 and parameters: {'observation_period_num': 40, 'train_rates': 0.9797526733166813, 'learning_rate': 0.00027106912807197374, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9490296260766661}. Best is trial 14 with value: 0.13707546889781952.[0m
[32m[I 2025-01-07 17:17:51,747][0m Trial 15 finished with value: 0.25537064023145284 and parameters: {'observation_period_num': 30, 'train_rates': 0.8215838957095764, 'learning_rate': 0.00020594347536663325, 'batch_size': 252, 'step_size': 11, 'gamma': 0.9785311221035895}. Best is trial 14 with value: 0.13707546889781952.[0m
[32m[I 2025-01-07 17:19:01,617][0m Trial 16 finished with value: 0.20546108325129575 and parameters: {'observation_period_num': 54, 'train_rates': 0.9293483832879239, 'learning_rate': 2.2494696657163762e-05, 'batch_size': 194, 'step_size': 9, 'gamma': 0.9439435771674359}. Best is trial 14 with value: 0.13707546889781952.[0m
[32m[I 2025-01-07 17:20:03,978][0m Trial 17 finished with value: 0.2808123528957367 and parameters: {'observation_period_num': 47, 'train_rates': 0.9384059993599848, 'learning_rate': 1.655707683874904e-05, 'batch_size': 184, 'step_size': 8, 'gamma': 0.9506287625111658}. Best is trial 14 with value: 0.13707546889781952.[0m
[32m[I 2025-01-07 17:23:08,627][0m Trial 18 finished with value: 0.11138737802543948 and parameters: {'observation_period_num': 48, 'train_rates': 0.9471365259139121, 'learning_rate': 2.813846453005801e-05, 'batch_size': 23, 'step_size': 13, 'gamma': 0.9445362599629609}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:26:09,939][0m Trial 19 finished with value: 0.20638066068291663 and parameters: {'observation_period_num': 13, 'train_rates': 0.8309999369765891, 'learning_rate': 4.6973204086182306e-05, 'batch_size': 21, 'step_size': 14, 'gamma': 0.9443251506724324}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:29:15,715][0m Trial 20 finished with value: 0.13668323167431073 and parameters: {'observation_period_num': 114, 'train_rates': 0.9557091215856903, 'learning_rate': 1.1524624154088927e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.8893345352501344}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:32:58,003][0m Trial 21 finished with value: 0.11824195512703486 and parameters: {'observation_period_num': 112, 'train_rates': 0.9510649269496866, 'learning_rate': 1.3910781004617904e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.895466868221148}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:37:18,101][0m Trial 22 finished with value: 0.11499130315828643 and parameters: {'observation_period_num': 115, 'train_rates': 0.9479302157164066, 'learning_rate': 1.1148909988134356e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8901165000303816}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:39:57,271][0m Trial 23 finished with value: 0.2732937470078468 and parameters: {'observation_period_num': 117, 'train_rates': 0.8773798413509992, 'learning_rate': 9.479774237529625e-06, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8934506781858055}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:41:32,903][0m Trial 24 finished with value: 0.2401562864853911 and parameters: {'observation_period_num': 63, 'train_rates': 0.9497236056215671, 'learning_rate': 5.15568145149024e-06, 'batch_size': 57, 'step_size': 11, 'gamma': 0.9259549824169062}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:44:46,616][0m Trial 25 finished with value: 0.20042472084363303 and parameters: {'observation_period_num': 138, 'train_rates': 0.9047730602448119, 'learning_rate': 5.6696687849983475e-05, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8732654666935763}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:47:17,549][0m Trial 26 finished with value: 0.376565055269748 and parameters: {'observation_period_num': 111, 'train_rates': 0.832073310691049, 'learning_rate': 2.080594591138269e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9008629528019264}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:51:01,092][0m Trial 27 finished with value: 0.6224524562879634 and parameters: {'observation_period_num': 161, 'train_rates': 0.791441823599438, 'learning_rate': 2.632093615711464e-06, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9660004339152387}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:52:28,682][0m Trial 28 finished with value: 0.17805928164634152 and parameters: {'observation_period_num': 64, 'train_rates': 0.9526588178233031, 'learning_rate': 1.4668178237998175e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9873987758242708}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:56:35,606][0m Trial 29 finished with value: 0.16477711351201085 and parameters: {'observation_period_num': 125, 'train_rates': 0.8934500227261821, 'learning_rate': 2.6784473965647298e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.869870301931004}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 17:59:11,724][0m Trial 30 finished with value: 0.17222056648245565 and parameters: {'observation_period_num': 103, 'train_rates': 0.9247542625121977, 'learning_rate': 3.402823714462232e-05, 'batch_size': 38, 'step_size': 14, 'gamma': 0.9316904681983703}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:03:35,604][0m Trial 31 finished with value: 0.12945138466245723 and parameters: {'observation_period_num': 132, 'train_rates': 0.9628937414474622, 'learning_rate': 1.0817695210164202e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8781983881806334}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:07:25,160][0m Trial 32 finished with value: 0.2540535874226514 and parameters: {'observation_period_num': 154, 'train_rates': 0.9637528278098992, 'learning_rate': 6.6641534972254554e-06, 'batch_size': 64, 'step_size': 11, 'gamma': 0.8736143235627133}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:12:35,423][0m Trial 33 finished with value: 0.16881354620833416 and parameters: {'observation_period_num': 190, 'train_rates': 0.9170535101143586, 'learning_rate': 1.342837042179912e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8191687389143388}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:15:05,565][0m Trial 34 finished with value: 0.9711677458937668 and parameters: {'observation_period_num': 129, 'train_rates': 0.6013602461666188, 'learning_rate': 8.548018533205354e-06, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8604050198760518}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:19:22,334][0m Trial 35 finished with value: 0.2109673673953485 and parameters: {'observation_period_num': 180, 'train_rates': 0.882750176536823, 'learning_rate': 5.1476638170699045e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.9096467995626901}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:25:25,817][0m Trial 36 finished with value: 0.34118512019515035 and parameters: {'observation_period_num': 250, 'train_rates': 0.8529130975721756, 'learning_rate': 3.4096460743901656e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8823635593099037}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:26:01,505][0m Trial 37 finished with value: 0.42202311754226685 and parameters: {'observation_period_num': 5, 'train_rates': 0.966588186927529, 'learning_rate': 5.788833461097457e-06, 'batch_size': 120, 'step_size': 11, 'gamma': 0.8515648343580728}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:29:59,212][0m Trial 38 finished with value: 0.13469336293637751 and parameters: {'observation_period_num': 154, 'train_rates': 0.9362759387575385, 'learning_rate': 8.510351533393996e-05, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8992335774874731}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:31:46,555][0m Trial 39 finished with value: 0.44317396744242255 and parameters: {'observation_period_num': 78, 'train_rates': 0.9084839815716694, 'learning_rate': 3.7570675681824353e-06, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9626282120429516}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:35:07,050][0m Trial 40 finished with value: 1.4256846904754639 and parameters: {'observation_period_num': 136, 'train_rates': 0.9888845765441537, 'learning_rate': 2.2664556536916636e-06, 'batch_size': 138, 'step_size': 3, 'gamma': 0.8183277420815441}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:38:57,360][0m Trial 41 finished with value: 0.13512645154025968 and parameters: {'observation_period_num': 152, 'train_rates': 0.9366303219565064, 'learning_rate': 7.188946945026261e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.899434441649878}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:44:33,836][0m Trial 42 finished with value: 0.14543458306557172 and parameters: {'observation_period_num': 199, 'train_rates': 0.9443967486981933, 'learning_rate': 0.00012443495887406683, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8807339898661859}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:49:07,522][0m Trial 43 finished with value: 0.1313255581666123 and parameters: {'observation_period_num': 175, 'train_rates': 0.968659780756613, 'learning_rate': 1.8362842047376733e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9307145530665699}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 18:54:31,626][0m Trial 44 finished with value: 0.1375781772441642 and parameters: {'observation_period_num': 204, 'train_rates': 0.9689718219718451, 'learning_rate': 1.7406089291606582e-05, 'batch_size': 45, 'step_size': 12, 'gamma': 0.9354306559407606}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 19:00:14,295][0m Trial 45 finished with value: 0.18371040027501973 and parameters: {'observation_period_num': 215, 'train_rates': 0.9688606675828195, 'learning_rate': 9.840251825096279e-06, 'batch_size': 57, 'step_size': 15, 'gamma': 0.9174831233529034}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 19:04:17,992][0m Trial 46 finished with value: 0.2068716469962718 and parameters: {'observation_period_num': 171, 'train_rates': 0.8947647317393836, 'learning_rate': 2.6743611232541047e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9601256817539703}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 19:09:11,395][0m Trial 47 finished with value: 0.2307892582326565 and parameters: {'observation_period_num': 188, 'train_rates': 0.9240145969011024, 'learning_rate': 4.40136184014281e-06, 'batch_size': 26, 'step_size': 12, 'gamma': 0.9077481341191107}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 19:11:00,218][0m Trial 48 finished with value: 0.9514369932256004 and parameters: {'observation_period_num': 97, 'train_rates': 0.6855699228157223, 'learning_rate': 7.491301853881796e-06, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7558650330662419}. Best is trial 18 with value: 0.11138737802543948.[0m
[32m[I 2025-01-07 19:17:19,329][0m Trial 49 finished with value: 0.15231850743293762 and parameters: {'observation_period_num': 231, 'train_rates': 0.9883276605863828, 'learning_rate': 1.9313557855540217e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9261241990525192}. Best is trial 18 with value: 0.11138737802543948.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7144 | 0.7206
Epoch 2/300, Loss: 0.4929 | 0.5851
Epoch 3/300, Loss: 0.3462 | 0.5059
Epoch 4/300, Loss: 0.2898 | 0.3765
Epoch 5/300, Loss: 0.2526 | 0.3460
Epoch 6/300, Loss: 0.2386 | 0.3476
Epoch 7/300, Loss: 0.2241 | 0.3193
Epoch 8/300, Loss: 0.2118 | 0.2874
Epoch 9/300, Loss: 0.2081 | 0.2785
Epoch 10/300, Loss: 0.1952 | 0.2605
Epoch 11/300, Loss: 0.1924 | 0.2634
Epoch 12/300, Loss: 0.1908 | 0.2605
Epoch 13/300, Loss: 0.1861 | 0.2368
Epoch 14/300, Loss: 0.1843 | 0.2409
Epoch 15/300, Loss: 0.1816 | 0.2225
Epoch 16/300, Loss: 0.1803 | 0.2429
Epoch 17/300, Loss: 0.1762 | 0.2171
Epoch 18/300, Loss: 0.1748 | 0.2212
Epoch 19/300, Loss: 0.1701 | 0.1986
Epoch 20/300, Loss: 0.1688 | 0.2383
Epoch 21/300, Loss: 0.1642 | 0.2100
Epoch 22/300, Loss: 0.1650 | 0.1845
Epoch 23/300, Loss: 0.1647 | 0.2212
Epoch 24/300, Loss: 0.1634 | 0.2311
Epoch 25/300, Loss: 0.1624 | 0.2179
Epoch 26/300, Loss: 0.1610 | 0.1760
Epoch 27/300, Loss: 0.1691 | 0.2514
Epoch 28/300, Loss: 0.1657 | 0.2126
Epoch 29/300, Loss: 0.1585 | 0.2007
Epoch 30/300, Loss: 0.1614 | 0.1938
Epoch 31/300, Loss: 0.1548 | 0.1805
Epoch 32/300, Loss: 0.1507 | 0.1798
Epoch 33/300, Loss: 0.1567 | 0.2128
Epoch 34/300, Loss: 0.1495 | 0.1801
Epoch 35/300, Loss: 0.1486 | 0.1635
Epoch 36/300, Loss: 0.1465 | 0.1760
Epoch 37/300, Loss: 0.1426 | 0.1947
Epoch 38/300, Loss: 0.1392 | 0.1729
Epoch 39/300, Loss: 0.1381 | 0.1504
Epoch 40/300, Loss: 0.1412 | 0.1675
Epoch 41/300, Loss: 0.1360 | 0.1782
Epoch 42/300, Loss: 0.1327 | 0.1726
Epoch 43/300, Loss: 0.1317 | 0.1526
Epoch 44/300, Loss: 0.1334 | 0.1615
Epoch 45/300, Loss: 0.1328 | 0.1958
Epoch 46/300, Loss: 0.1325 | 0.1655
Epoch 47/300, Loss: 0.1360 | 0.1646
Epoch 48/300, Loss: 0.1333 | 0.1530
Epoch 49/300, Loss: 0.1296 | 0.1593
Epoch 50/300, Loss: 0.1246 | 0.1471
Epoch 51/300, Loss: 0.1256 | 0.1456
Epoch 52/300, Loss: 0.1232 | 0.1548
Epoch 53/300, Loss: 0.1225 | 0.1348
Epoch 54/300, Loss: 0.1197 | 0.1487
Epoch 55/300, Loss: 0.1192 | 0.1460
Epoch 56/300, Loss: 0.1183 | 0.1494
Epoch 57/300, Loss: 0.1167 | 0.1264
Epoch 58/300, Loss: 0.1162 | 0.1457
Epoch 59/300, Loss: 0.1156 | 0.1627
Epoch 60/300, Loss: 0.1158 | 0.1467
Epoch 61/300, Loss: 0.1183 | 0.1329
Epoch 62/300, Loss: 0.1155 | 0.1456
Epoch 63/300, Loss: 0.1153 | 0.1546
Epoch 64/300, Loss: 0.1151 | 0.1413
Epoch 65/300, Loss: 0.1174 | 0.1392
Epoch 66/300, Loss: 0.1186 | 0.2096
Epoch 67/300, Loss: 0.1208 | 0.1344
Epoch 68/300, Loss: 0.1170 | 0.1385
Epoch 69/300, Loss: 0.1164 | 0.1263
Epoch 70/300, Loss: 0.1180 | 0.1544
Epoch 71/300, Loss: 0.1122 | 0.1429
Epoch 72/300, Loss: 0.1112 | 0.1208
Epoch 73/300, Loss: 0.1100 | 0.1305
Epoch 74/300, Loss: 0.1079 | 0.1346
Epoch 75/300, Loss: 0.1064 | 0.1328
Epoch 76/300, Loss: 0.1036 | 0.1205
Epoch 77/300, Loss: 0.1033 | 0.1176
Epoch 78/300, Loss: 0.1013 | 0.1225
Epoch 79/300, Loss: 0.1011 | 0.1283
Epoch 80/300, Loss: 0.0998 | 0.1401
Epoch 81/300, Loss: 0.1005 | 0.1179
Epoch 82/300, Loss: 0.1014 | 0.1270
Epoch 83/300, Loss: 0.1030 | 0.1173
Epoch 84/300, Loss: 0.1045 | 0.1640
Epoch 85/300, Loss: 0.1048 | 0.1295
Epoch 86/300, Loss: 0.1053 | 0.1154
Epoch 87/300, Loss: 0.1011 | 0.1125
Epoch 88/300, Loss: 0.1000 | 0.1281
Epoch 89/300, Loss: 0.0958 | 0.1254
Epoch 90/300, Loss: 0.0953 | 0.1080
Epoch 91/300, Loss: 0.0961 | 0.1064
Epoch 92/300, Loss: 0.0959 | 0.1233
Epoch 93/300, Loss: 0.0952 | 0.1418
Epoch 94/300, Loss: 0.0957 | 0.1114
Epoch 95/300, Loss: 0.0960 | 0.1061
Epoch 96/300, Loss: 0.0933 | 0.1119
Epoch 97/300, Loss: 0.0930 | 0.1274
Epoch 98/300, Loss: 0.0924 | 0.1153
Epoch 99/300, Loss: 0.0934 | 0.1087
Epoch 100/300, Loss: 0.0926 | 0.1095
Epoch 101/300, Loss: 0.0931 | 0.1351
Epoch 102/300, Loss: 0.0928 | 0.1162
Epoch 103/300, Loss: 0.0944 | 0.1100
Epoch 104/300, Loss: 0.0915 | 0.1045
Epoch 105/300, Loss: 0.0904 | 0.1269
Epoch 106/300, Loss: 0.0897 | 0.1207
Epoch 107/300, Loss: 0.0910 | 0.1064
Epoch 108/300, Loss: 0.0891 | 0.1013
Epoch 109/300, Loss: 0.0889 | 0.1124
Epoch 110/300, Loss: 0.0876 | 0.1184
Epoch 111/300, Loss: 0.0869 | 0.1084
Epoch 112/300, Loss: 0.0869 | 0.0997
Epoch 113/300, Loss: 0.0877 | 0.1064
Epoch 114/300, Loss: 0.0866 | 0.1177
Epoch 115/300, Loss: 0.0866 | 0.1149
Epoch 116/300, Loss: 0.0869 | 0.1003
Epoch 117/300, Loss: 0.0879 | 0.0971
Epoch 118/300, Loss: 0.0874 | 0.1270
Epoch 119/300, Loss: 0.0860 | 0.1226
Epoch 120/300, Loss: 0.0866 | 0.1021
Epoch 121/300, Loss: 0.0852 | 0.0960
Epoch 122/300, Loss: 0.0849 | 0.1050
Epoch 123/300, Loss: 0.0835 | 0.1186
Epoch 124/300, Loss: 0.0849 | 0.1050
Epoch 125/300, Loss: 0.0852 | 0.0971
Epoch 126/300, Loss: 0.0839 | 0.1098
Epoch 127/300, Loss: 0.0834 | 0.1149
Epoch 128/300, Loss: 0.0829 | 0.1030
Epoch 129/300, Loss: 0.0823 | 0.0983
Epoch 130/300, Loss: 0.0829 | 0.1026
Epoch 131/300, Loss: 0.0830 | 0.1260
Epoch 132/300, Loss: 0.0818 | 0.1066
Epoch 133/300, Loss: 0.0827 | 0.0986
Epoch 134/300, Loss: 0.0813 | 0.1021
Epoch 135/300, Loss: 0.0811 | 0.1144
Epoch 136/300, Loss: 0.0805 | 0.1023
Epoch 137/300, Loss: 0.0806 | 0.0993
Epoch 138/300, Loss: 0.0812 | 0.1038
Epoch 139/300, Loss: 0.0799 | 0.1119
Epoch 140/300, Loss: 0.0790 | 0.1043
Epoch 141/300, Loss: 0.0785 | 0.0961
Epoch 142/300, Loss: 0.0786 | 0.1007
Epoch 143/300, Loss: 0.0786 | 0.1084
Epoch 144/300, Loss: 0.0779 | 0.1023
Epoch 145/300, Loss: 0.0778 | 0.0986
Epoch 146/300, Loss: 0.0776 | 0.0981
Epoch 147/300, Loss: 0.0773 | 0.1040
Epoch 148/300, Loss: 0.0769 | 0.1032
Epoch 149/300, Loss: 0.0771 | 0.1003
Epoch 150/300, Loss: 0.0760 | 0.1022
Epoch 151/300, Loss: 0.0769 | 0.1044
Epoch 152/300, Loss: 0.0764 | 0.1049
Epoch 153/300, Loss: 0.0758 | 0.1037
Epoch 154/300, Loss: 0.0758 | 0.0976
Epoch 155/300, Loss: 0.0756 | 0.0997
Epoch 156/300, Loss: 0.0758 | 0.1044
Epoch 157/300, Loss: 0.0756 | 0.1039
Epoch 158/300, Loss: 0.0748 | 0.1020
Epoch 159/300, Loss: 0.0756 | 0.1018
Epoch 160/300, Loss: 0.0752 | 0.1039
Epoch 161/300, Loss: 0.0753 | 0.0978
Epoch 162/300, Loss: 0.0756 | 0.0976
Epoch 163/300, Loss: 0.0746 | 0.1071
Epoch 164/300, Loss: 0.0750 | 0.1052
Epoch 165/300, Loss: 0.0741 | 0.0983
Epoch 166/300, Loss: 0.0731 | 0.0999
Epoch 167/300, Loss: 0.0738 | 0.1053
Epoch 168/300, Loss: 0.0731 | 0.1038
Epoch 169/300, Loss: 0.0733 | 0.0946
Epoch 170/300, Loss: 0.0737 | 0.0947
Epoch 171/300, Loss: 0.0727 | 0.1091
Epoch 172/300, Loss: 0.0737 | 0.1105
Epoch 173/300, Loss: 0.0733 | 0.0954
Epoch 174/300, Loss: 0.0729 | 0.0926
Epoch 175/300, Loss: 0.0730 | 0.1055
Epoch 176/300, Loss: 0.0735 | 0.1098
Epoch 177/300, Loss: 0.0725 | 0.0986
Epoch 178/300, Loss: 0.0718 | 0.0954
Epoch 179/300, Loss: 0.0723 | 0.1026
Epoch 180/300, Loss: 0.0722 | 0.1020
Epoch 181/300, Loss: 0.0717 | 0.0947
Epoch 182/300, Loss: 0.0708 | 0.0974
Epoch 183/300, Loss: 0.0707 | 0.1035
Epoch 184/300, Loss: 0.0705 | 0.1021
Epoch 185/300, Loss: 0.0702 | 0.0953
Epoch 186/300, Loss: 0.0713 | 0.0988
Epoch 187/300, Loss: 0.0706 | 0.1008
Epoch 188/300, Loss: 0.0700 | 0.1031
Epoch 189/300, Loss: 0.0706 | 0.0978
Epoch 190/300, Loss: 0.0700 | 0.0962
Epoch 191/300, Loss: 0.0696 | 0.1055
Epoch 192/300, Loss: 0.0699 | 0.1009
Epoch 193/300, Loss: 0.0699 | 0.0951
Epoch 194/300, Loss: 0.0692 | 0.0965
Epoch 195/300, Loss: 0.0694 | 0.1024
Epoch 196/300, Loss: 0.0694 | 0.1010
Epoch 197/300, Loss: 0.0691 | 0.0958
Epoch 198/300, Loss: 0.0690 | 0.0993
Epoch 199/300, Loss: 0.0690 | 0.1033
Epoch 200/300, Loss: 0.0698 | 0.0963
Epoch 201/300, Loss: 0.0699 | 0.0932
Epoch 202/300, Loss: 0.0689 | 0.1043
Epoch 203/300, Loss: 0.0688 | 0.1042
Epoch 204/300, Loss: 0.0686 | 0.0943
Epoch 205/300, Loss: 0.0686 | 0.0954
Epoch 206/300, Loss: 0.0688 | 0.1069
Epoch 207/300, Loss: 0.0686 | 0.1042
Epoch 208/300, Loss: 0.0681 | 0.0935
Epoch 209/300, Loss: 0.0690 | 0.0953
Epoch 210/300, Loss: 0.0682 | 0.1027
Epoch 211/300, Loss: 0.0676 | 0.0991
Epoch 212/300, Loss: 0.0682 | 0.0939
Epoch 213/300, Loss: 0.0672 | 0.0964
Epoch 214/300, Loss: 0.0676 | 0.1028
Epoch 215/300, Loss: 0.0677 | 0.0965
Epoch 216/300, Loss: 0.0676 | 0.0956
Epoch 217/300, Loss: 0.0681 | 0.1010
Epoch 218/300, Loss: 0.0670 | 0.0997
Epoch 219/300, Loss: 0.0671 | 0.0962
Epoch 220/300, Loss: 0.0670 | 0.0971
Epoch 221/300, Loss: 0.0668 | 0.1009
Epoch 222/300, Loss: 0.0668 | 0.1014
Epoch 223/300, Loss: 0.0665 | 0.0961
Epoch 224/300, Loss: 0.0663 | 0.0957
Epoch 225/300, Loss: 0.0664 | 0.0999
Epoch 226/300, Loss: 0.0666 | 0.1025
Epoch 227/300, Loss: 0.0666 | 0.0963
Epoch 228/300, Loss: 0.0663 | 0.0955
Epoch 229/300, Loss: 0.0663 | 0.1010
Epoch 230/300, Loss: 0.0665 | 0.1017
Epoch 231/300, Loss: 0.0663 | 0.0977
Epoch 232/300, Loss: 0.0664 | 0.0945
Epoch 233/300, Loss: 0.0662 | 0.1005
Epoch 234/300, Loss: 0.0656 | 0.1049
Epoch 235/300, Loss: 0.0660 | 0.0960
Epoch 236/300, Loss: 0.0666 | 0.0970
Epoch 237/300, Loss: 0.0658 | 0.1010
Epoch 238/300, Loss: 0.0660 | 0.1014
Epoch 239/300, Loss: 0.0657 | 0.0963
Epoch 240/300, Loss: 0.0650 | 0.0974
Epoch 241/300, Loss: 0.0657 | 0.0998
Epoch 242/300, Loss: 0.0646 | 0.0997
Epoch 243/300, Loss: 0.0652 | 0.1019
Epoch 244/300, Loss: 0.0650 | 0.0976
Epoch 245/300, Loss: 0.0649 | 0.0979
Epoch 246/300, Loss: 0.0651 | 0.1023
Epoch 247/300, Loss: 0.0647 | 0.0988
Epoch 248/300, Loss: 0.0650 | 0.0986
Epoch 249/300, Loss: 0.0647 | 0.0982
Epoch 250/300, Loss: 0.0647 | 0.0991
Epoch 251/300, Loss: 0.0644 | 0.0997
Epoch 252/300, Loss: 0.0643 | 0.0981
Epoch 253/300, Loss: 0.0644 | 0.0980
Epoch 254/300, Loss: 0.0645 | 0.1003
Epoch 255/300, Loss: 0.0643 | 0.0990
Epoch 256/300, Loss: 0.0643 | 0.0979
Epoch 257/300, Loss: 0.0648 | 0.0970
Epoch 258/300, Loss: 0.0643 | 0.1029
Epoch 259/300, Loss: 0.0645 | 0.1011
Epoch 260/300, Loss: 0.0641 | 0.0986
Epoch 261/300, Loss: 0.0640 | 0.0981
Epoch 262/300, Loss: 0.0636 | 0.1006
Epoch 263/300, Loss: 0.0641 | 0.0980
Epoch 264/300, Loss: 0.0634 | 0.0989
Epoch 265/300, Loss: 0.0641 | 0.0985
Epoch 266/300, Loss: 0.0637 | 0.0996
Epoch 267/300, Loss: 0.0633 | 0.0995
Epoch 268/300, Loss: 0.0636 | 0.0989
Epoch 269/300, Loss: 0.0639 | 0.1002
Epoch 270/300, Loss: 0.0638 | 0.1007
Epoch 271/300, Loss: 0.0637 | 0.0986
Epoch 272/300, Loss: 0.0637 | 0.0991
Epoch 273/300, Loss: 0.0633 | 0.0987
Epoch 274/300, Loss: 0.0636 | 0.1008
Epoch 275/300, Loss: 0.0631 | 0.0997
Epoch 276/300, Loss: 0.0633 | 0.0990
Epoch 277/300, Loss: 0.0636 | 0.0987
Epoch 278/300, Loss: 0.0631 | 0.0989
Epoch 279/300, Loss: 0.0631 | 0.0998
Epoch 280/300, Loss: 0.0629 | 0.1019
Epoch 281/300, Loss: 0.0627 | 0.0987
Epoch 282/300, Loss: 0.0625 | 0.0998
Epoch 283/300, Loss: 0.0629 | 0.1006
Epoch 284/300, Loss: 0.0627 | 0.0977
Epoch 285/300, Loss: 0.0627 | 0.0982
Epoch 286/300, Loss: 0.0626 | 0.1002
Epoch 287/300, Loss: 0.0630 | 0.0996
Epoch 288/300, Loss: 0.0625 | 0.0977
Epoch 289/300, Loss: 0.0623 | 0.0995
Epoch 290/300, Loss: 0.0627 | 0.0988
Epoch 291/300, Loss: 0.0624 | 0.0981
Epoch 292/300, Loss: 0.0629 | 0.0989
Epoch 293/300, Loss: 0.0622 | 0.0989
Epoch 294/300, Loss: 0.0619 | 0.0989
Epoch 295/300, Loss: 0.0622 | 0.0990
Epoch 296/300, Loss: 0.0626 | 0.1001
Epoch 297/300, Loss: 0.0622 | 0.0986
Epoch 298/300, Loss: 0.0620 | 0.0989
Epoch 299/300, Loss: 0.0625 | 0.0999
Epoch 300/300, Loss: 0.0624 | 0.1014
Runtime (seconds): 524.6955502033234
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 706.4648625589907
RMSE: 26.57940673828125
MAE: 26.57940673828125
R-squared: nan
[204.57059]
