[32m[I 2025-02-02 05:49:59,866][0m A new study created in memory with name: no-name-47dc016e-0da9-4615-964c-d7013db51541[0m
[32m[I 2025-02-02 05:50:46,173][0m Trial 0 finished with value: 0.28387412142662016 and parameters: {'observation_period_num': 150, 'train_rates': 0.7214092035478123, 'learning_rate': 3.218993316754487e-05, 'batch_size': 106, 'step_size': 6, 'gamma': 0.7769772323370105}. Best is trial 0 with value: 0.28387412142662016.[0m
[32m[I 2025-02-02 05:52:27,394][0m Trial 1 finished with value: 0.16566133482506495 and parameters: {'observation_period_num': 190, 'train_rates': 0.8542248567106523, 'learning_rate': 6.850148220865561e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.7619574448785618}. Best is trial 1 with value: 0.16566133482506495.[0m
[32m[I 2025-02-02 05:56:27,569][0m Trial 2 finished with value: 0.2064775401847466 and parameters: {'observation_period_num': 35, 'train_rates': 0.8896019072477377, 'learning_rate': 0.0004837596196441467, 'batch_size': 23, 'step_size': 9, 'gamma': 0.9385915471113404}. Best is trial 1 with value: 0.16566133482506495.[0m
[32m[I 2025-02-02 05:57:01,136][0m Trial 3 finished with value: 0.42819178104400635 and parameters: {'observation_period_num': 13, 'train_rates': 0.9705298161104563, 'learning_rate': 1.3160830976887347e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.9534871234591764}. Best is trial 1 with value: 0.16566133482506495.[0m
[32m[I 2025-02-02 05:58:20,359][0m Trial 4 finished with value: 0.3490877723453021 and parameters: {'observation_period_num': 8, 'train_rates': 0.9329177396027695, 'learning_rate': 1.3245667062406358e-06, 'batch_size': 76, 'step_size': 12, 'gamma': 0.9647795283996576}. Best is trial 1 with value: 0.16566133482506495.[0m
[32m[I 2025-02-02 05:58:44,957][0m Trial 5 finished with value: 0.11299119803767937 and parameters: {'observation_period_num': 57, 'train_rates': 0.7846918703704344, 'learning_rate': 0.00018926480444095522, 'batch_size': 248, 'step_size': 12, 'gamma': 0.9172805058454805}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 05:59:33,808][0m Trial 6 finished with value: 0.44870440918513294 and parameters: {'observation_period_num': 186, 'train_rates': 0.641292117150964, 'learning_rate': 3.5220414963111775e-06, 'batch_size': 93, 'step_size': 12, 'gamma': 0.8065997713431321}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:00:07,650][0m Trial 7 finished with value: 0.2667918368553122 and parameters: {'observation_period_num': 175, 'train_rates': 0.9138647611170981, 'learning_rate': 2.6346289824476208e-05, 'batch_size': 173, 'step_size': 7, 'gamma': 0.9711256240620032}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:00:43,941][0m Trial 8 finished with value: 0.23434562053043623 and parameters: {'observation_period_num': 245, 'train_rates': 0.6814584232675515, 'learning_rate': 0.0006031788612819471, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9225729836017267}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:01:27,329][0m Trial 9 finished with value: 0.1397872936199693 and parameters: {'observation_period_num': 117, 'train_rates': 0.6706509460788825, 'learning_rate': 0.0006455924706478172, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8879691766993314}. Best is trial 5 with value: 0.11299119803767937.[0m
Early stopping at epoch 68
[32m[I 2025-02-02 06:01:42,975][0m Trial 10 finished with value: 0.2968290730062964 and parameters: {'observation_period_num': 74, 'train_rates': 0.7885176853591037, 'learning_rate': 0.0001424460077482854, 'batch_size': 256, 'step_size': 1, 'gamma': 0.8326977244424462}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:02:05,224][0m Trial 11 finished with value: 0.13886787398833705 and parameters: {'observation_period_num': 90, 'train_rates': 0.7578193703743656, 'learning_rate': 0.00022749590693736337, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8978096004524869}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:02:28,518][0m Trial 12 finished with value: 0.13387031127123317 and parameters: {'observation_period_num': 85, 'train_rates': 0.7811395180363535, 'learning_rate': 0.0001755475773423101, 'batch_size': 252, 'step_size': 15, 'gamma': 0.8828161194932619}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:02:56,884][0m Trial 13 finished with value: 0.11884072930525558 and parameters: {'observation_period_num': 63, 'train_rates': 0.8177862498092789, 'learning_rate': 0.000138931776894865, 'batch_size': 207, 'step_size': 15, 'gamma': 0.8529908376731916}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:03:26,983][0m Trial 14 finished with value: 0.14909046212747945 and parameters: {'observation_period_num': 47, 'train_rates': 0.8376275189489586, 'learning_rate': 7.01006353586827e-05, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8422123850460733}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:03:53,238][0m Trial 15 finished with value: 0.1163130074441817 and parameters: {'observation_period_num': 57, 'train_rates': 0.7321209973156351, 'learning_rate': 8.078323149008105e-05, 'batch_size': 209, 'step_size': 15, 'gamma': 0.8499244496488275}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:04:24,253][0m Trial 16 finished with value: 0.27991748242681286 and parameters: {'observation_period_num': 117, 'train_rates': 0.7282350733921428, 'learning_rate': 9.486850911257114e-06, 'batch_size': 169, 'step_size': 13, 'gamma': 0.9050928670861128}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:04:47,048][0m Trial 17 finished with value: 0.15262778856392417 and parameters: {'observation_period_num': 36, 'train_rates': 0.6162344532364477, 'learning_rate': 5.7185721656233885e-05, 'batch_size': 213, 'step_size': 9, 'gamma': 0.815730841795971}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:05:08,698][0m Trial 18 finished with value: 0.12768326773608898 and parameters: {'observation_period_num': 96, 'train_rates': 0.7108738634381584, 'learning_rate': 0.00028161222655846967, 'batch_size': 226, 'step_size': 14, 'gamma': 0.8742410924984737}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:05:40,094][0m Trial 19 finished with value: 0.235650505913898 and parameters: {'observation_period_num': 139, 'train_rates': 0.7557587514380928, 'learning_rate': 1.4985919840027074e-05, 'batch_size': 166, 'step_size': 11, 'gamma': 0.9873503623385574}. Best is trial 5 with value: 0.11299119803767937.[0m
[32m[I 2025-02-02 06:06:05,981][0m Trial 20 finished with value: 0.10260162502527237 and parameters: {'observation_period_num': 51, 'train_rates': 0.8077650059829253, 'learning_rate': 0.0009743188645203561, 'batch_size': 229, 'step_size': 6, 'gamma': 0.92843869870405}. Best is trial 20 with value: 0.10260162502527237.[0m
[32m[I 2025-02-02 06:06:31,347][0m Trial 21 finished with value: 0.12639994754130582 and parameters: {'observation_period_num': 63, 'train_rates': 0.8565987193629796, 'learning_rate': 0.0008474840332403149, 'batch_size': 228, 'step_size': 5, 'gamma': 0.9222193285680961}. Best is trial 20 with value: 0.10260162502527237.[0m
[32m[I 2025-02-02 06:07:01,746][0m Trial 22 finished with value: 0.09588534592482068 and parameters: {'observation_period_num': 33, 'train_rates': 0.7961548824273567, 'learning_rate': 0.00034551809875146074, 'batch_size': 186, 'step_size': 3, 'gamma': 0.9197132903218844}. Best is trial 22 with value: 0.09588534592482068.[0m
[32m[I 2025-02-02 06:07:28,305][0m Trial 23 finished with value: 0.10394046027213336 and parameters: {'observation_period_num': 23, 'train_rates': 0.8157524846542801, 'learning_rate': 0.0003494019079211488, 'batch_size': 231, 'step_size': 3, 'gamma': 0.9223251748841569}. Best is trial 22 with value: 0.09588534592482068.[0m
[32m[I 2025-02-02 06:07:59,203][0m Trial 24 finished with value: 0.09884079357338998 and parameters: {'observation_period_num': 26, 'train_rates': 0.8210732173407677, 'learning_rate': 0.0004035877018425563, 'batch_size': 185, 'step_size': 2, 'gamma': 0.9446417334795816}. Best is trial 22 with value: 0.09588534592482068.[0m
[32m[I 2025-02-02 06:08:40,836][0m Trial 25 finished with value: 0.1093101572300413 and parameters: {'observation_period_num': 28, 'train_rates': 0.8613196476294442, 'learning_rate': 0.0009930780714135085, 'batch_size': 148, 'step_size': 1, 'gamma': 0.9448288627087286}. Best is trial 22 with value: 0.09588534592482068.[0m
[32m[I 2025-02-02 06:09:13,561][0m Trial 26 finished with value: 0.08732568221045779 and parameters: {'observation_period_num': 6, 'train_rates': 0.8171973971353074, 'learning_rate': 0.0005589006962380714, 'batch_size': 182, 'step_size': 2, 'gamma': 0.9840724410408623}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:09:45,130][0m Trial 27 finished with value: 0.11119936535813216 and parameters: {'observation_period_num': 5, 'train_rates': 0.8826941371988074, 'learning_rate': 0.0003734265518351923, 'batch_size': 186, 'step_size': 3, 'gamma': 0.989443702692642}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:10:23,233][0m Trial 28 finished with value: 0.08759016401960816 and parameters: {'observation_period_num': 29, 'train_rates': 0.7572593440776711, 'learning_rate': 0.00045521542110832776, 'batch_size': 146, 'step_size': 2, 'gamma': 0.9672576232323837}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:11:01,407][0m Trial 29 finished with value: 0.21424131256933432 and parameters: {'observation_period_num': 250, 'train_rates': 0.7580333184578109, 'learning_rate': 3.448135830266103e-05, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9698454852382319}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:11:32,975][0m Trial 30 finished with value: 0.17575074064562907 and parameters: {'observation_period_num': 157, 'train_rates': 0.6967223635036501, 'learning_rate': 0.00012407595736681703, 'batch_size': 152, 'step_size': 2, 'gamma': 0.9603242234738717}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:12:05,522][0m Trial 31 finished with value: 0.10632772520814587 and parameters: {'observation_period_num': 22, 'train_rates': 0.8388915325487282, 'learning_rate': 0.0004062698446345054, 'batch_size': 185, 'step_size': 2, 'gamma': 0.979752277295369}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:12:54,783][0m Trial 32 finished with value: 0.1052713678559152 and parameters: {'observation_period_num': 40, 'train_rates': 0.8312112816501707, 'learning_rate': 0.0002679594030123694, 'batch_size': 120, 'step_size': 2, 'gamma': 0.9456232192522165}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:13:30,804][0m Trial 33 finished with value: 0.08933576019319324 and parameters: {'observation_period_num': 25, 'train_rates': 0.7636640135801716, 'learning_rate': 0.0005995341936027212, 'batch_size': 155, 'step_size': 4, 'gamma': 0.9409952063932577}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:14:04,865][0m Trial 34 finished with value: 0.09072818149897185 and parameters: {'observation_period_num': 38, 'train_rates': 0.7663105111483878, 'learning_rate': 0.0004891327313077404, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9073846654655703}. Best is trial 26 with value: 0.08732568221045779.[0m
[32m[I 2025-02-02 06:14:40,753][0m Trial 35 finished with value: 0.07208113033102442 and parameters: {'observation_period_num': 13, 'train_rates': 0.7432943826042208, 'learning_rate': 0.0005863940430755616, 'batch_size': 153, 'step_size': 5, 'gamma': 0.9578845552326676}. Best is trial 35 with value: 0.07208113033102442.[0m
[32m[I 2025-02-02 06:15:16,280][0m Trial 36 finished with value: 0.07352114465448165 and parameters: {'observation_period_num': 5, 'train_rates': 0.7372142486017468, 'learning_rate': 0.0006645008038647731, 'batch_size': 148, 'step_size': 7, 'gamma': 0.9591780613781863}. Best is trial 35 with value: 0.07208113033102442.[0m
[32m[I 2025-02-02 06:16:15,970][0m Trial 37 finished with value: 0.07082496460420189 and parameters: {'observation_period_num': 5, 'train_rates': 0.740404248493238, 'learning_rate': 0.0006949350966842061, 'batch_size': 87, 'step_size': 7, 'gamma': 0.9568976996254108}. Best is trial 37 with value: 0.07082496460420189.[0m
[32m[I 2025-02-02 06:17:51,480][0m Trial 38 finished with value: 0.07069462333925446 and parameters: {'observation_period_num': 6, 'train_rates': 0.6514424022792819, 'learning_rate': 0.00010360663082830921, 'batch_size': 47, 'step_size': 8, 'gamma': 0.7743430868951369}. Best is trial 38 with value: 0.07069462333925446.[0m
[32m[I 2025-02-02 06:21:15,781][0m Trial 39 finished with value: 0.07106602975879522 and parameters: {'observation_period_num': 16, 'train_rates': 0.6527522187302293, 'learning_rate': 8.895652372524455e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.7629900831251081}. Best is trial 38 with value: 0.07069462333925446.[0m
[32m[I 2025-02-02 06:23:46,929][0m Trial 40 finished with value: 0.2739224329061946 and parameters: {'observation_period_num': 222, 'train_rates': 0.6492189913978152, 'learning_rate': 3.58047062248355e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.7565640825410352}. Best is trial 38 with value: 0.07069462333925446.[0m
[32m[I 2025-02-02 06:24:53,538][0m Trial 41 finished with value: 0.12506524495908927 and parameters: {'observation_period_num': 13, 'train_rates': 0.6056133098241866, 'learning_rate': 9.914016572468303e-05, 'batch_size': 66, 'step_size': 7, 'gamma': 0.7774893842639579}. Best is trial 38 with value: 0.07069462333925446.[0m
[32m[I 2025-02-02 06:27:01,266][0m Trial 42 finished with value: 0.0649015632176208 and parameters: {'observation_period_num': 13, 'train_rates': 0.6606709587606237, 'learning_rate': 0.00021247734893196108, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7754368986647188}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:29:03,936][0m Trial 43 finished with value: 0.09479894820084371 and parameters: {'observation_period_num': 17, 'train_rates': 0.6561387984468693, 'learning_rate': 4.8317927754774526e-05, 'batch_size': 37, 'step_size': 9, 'gamma': 0.7740819993668934}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:30:26,872][0m Trial 44 finished with value: 0.0705808040611174 and parameters: {'observation_period_num': 17, 'train_rates': 0.6268355517291011, 'learning_rate': 0.00022288722811358315, 'batch_size': 54, 'step_size': 8, 'gamma': 0.7921614731016056}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:31:54,015][0m Trial 45 finished with value: 0.15325957026449677 and parameters: {'observation_period_num': 76, 'train_rates': 0.6241067388202685, 'learning_rate': 0.00019530381795359678, 'batch_size': 50, 'step_size': 8, 'gamma': 0.7944623853714522}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:32:51,374][0m Trial 46 finished with value: 0.0997385124648862 and parameters: {'observation_period_num': 46, 'train_rates': 0.6806568226534466, 'learning_rate': 0.00010196998835133879, 'batch_size': 84, 'step_size': 8, 'gamma': 0.7692837311334519}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:37:10,885][0m Trial 47 finished with value: 0.06844288200838708 and parameters: {'observation_period_num': 17, 'train_rates': 0.6333296167451655, 'learning_rate': 0.00016258827104516393, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7502259183145247}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:38:23,864][0m Trial 48 finished with value: 0.11727002195208078 and parameters: {'observation_period_num': 104, 'train_rates': 0.6302391433249309, 'learning_rate': 0.00018034570028062084, 'batch_size': 60, 'step_size': 10, 'gamma': 0.7501862175036291}. Best is trial 42 with value: 0.0649015632176208.[0m
[32m[I 2025-02-02 06:40:18,144][0m Trial 49 finished with value: 0.12589386834280333 and parameters: {'observation_period_num': 67, 'train_rates': 0.6679348436765631, 'learning_rate': 0.00014156705917942053, 'batch_size': 40, 'step_size': 11, 'gamma': 0.786201519800866}. Best is trial 42 with value: 0.0649015632176208.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_SBUX_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4311 | 0.2135
Epoch 2/300, Loss: 0.1656 | 0.1598
Epoch 3/300, Loss: 0.1438 | 0.1514
Epoch 4/300, Loss: 0.1343 | 0.1567
Epoch 5/300, Loss: 0.1277 | 0.1574
Epoch 6/300, Loss: 0.1252 | 0.1561
Epoch 7/300, Loss: 0.1213 | 0.1544
Epoch 8/300, Loss: 0.1182 | 0.1514
Epoch 9/300, Loss: 0.1156 | 0.1424
Epoch 10/300, Loss: 0.1125 | 0.1408
Epoch 11/300, Loss: 0.1075 | 0.1167
Epoch 12/300, Loss: 0.1046 | 0.1073
Epoch 13/300, Loss: 0.1022 | 0.1031
Epoch 14/300, Loss: 0.1004 | 0.1037
Epoch 15/300, Loss: 0.0987 | 0.1023
Epoch 16/300, Loss: 0.0969 | 0.1007
Epoch 17/300, Loss: 0.0952 | 0.0965
Epoch 18/300, Loss: 0.0945 | 0.0955
Epoch 19/300, Loss: 0.0935 | 0.0938
Epoch 20/300, Loss: 0.0926 | 0.0942
Epoch 21/300, Loss: 0.0915 | 0.0895
Epoch 22/300, Loss: 0.0908 | 0.0897
Epoch 23/300, Loss: 0.0897 | 0.0885
Epoch 24/300, Loss: 0.0889 | 0.0891
Epoch 25/300, Loss: 0.0883 | 0.0873
Epoch 26/300, Loss: 0.0877 | 0.0870
Epoch 27/300, Loss: 0.0872 | 0.0864
Epoch 28/300, Loss: 0.0866 | 0.0861
Epoch 29/300, Loss: 0.0862 | 0.0851
Epoch 30/300, Loss: 0.0859 | 0.0828
Epoch 31/300, Loss: 0.0855 | 0.0865
Epoch 32/300, Loss: 0.0853 | 0.0795
Epoch 33/300, Loss: 0.0850 | 0.0894
Epoch 34/300, Loss: 0.0854 | 0.0754
Epoch 35/300, Loss: 0.0853 | 0.0989
Epoch 36/300, Loss: 0.0875 | 0.0772
Epoch 37/300, Loss: 0.0858 | 0.0901
Epoch 38/300, Loss: 0.0870 | 0.0732
Epoch 39/300, Loss: 0.0852 | 0.0858
Epoch 40/300, Loss: 0.0862 | 0.0727
Epoch 41/300, Loss: 0.0843 | 0.0789
Epoch 42/300, Loss: 0.0842 | 0.0731
Epoch 43/300, Loss: 0.0835 | 0.0777
Epoch 44/300, Loss: 0.0834 | 0.0732
Epoch 45/300, Loss: 0.0829 | 0.0757
Epoch 46/300, Loss: 0.0827 | 0.0738
Epoch 47/300, Loss: 0.0825 | 0.0752
Epoch 48/300, Loss: 0.0824 | 0.0739
Epoch 49/300, Loss: 0.0822 | 0.0746
Epoch 50/300, Loss: 0.0821 | 0.0742
Epoch 51/300, Loss: 0.0820 | 0.0743
Epoch 52/300, Loss: 0.0819 | 0.0740
Epoch 53/300, Loss: 0.0818 | 0.0743
Epoch 54/300, Loss: 0.0817 | 0.0742
Epoch 55/300, Loss: 0.0816 | 0.0741
Epoch 56/300, Loss: 0.0815 | 0.0740
Epoch 57/300, Loss: 0.0815 | 0.0742
Epoch 58/300, Loss: 0.0814 | 0.0742
Epoch 59/300, Loss: 0.0813 | 0.0741
Epoch 60/300, Loss: 0.0812 | 0.0741
Epoch 61/300, Loss: 0.0812 | 0.0741
Epoch 62/300, Loss: 0.0811 | 0.0742
Epoch 63/300, Loss: 0.0810 | 0.0742
Epoch 64/300, Loss: 0.0809 | 0.0742
Epoch 65/300, Loss: 0.0809 | 0.0741
Epoch 66/300, Loss: 0.0808 | 0.0742
Epoch 67/300, Loss: 0.0808 | 0.0743
Epoch 68/300, Loss: 0.0807 | 0.0743
Epoch 69/300, Loss: 0.0807 | 0.0741
Epoch 70/300, Loss: 0.0806 | 0.0742
Epoch 71/300, Loss: 0.0806 | 0.0742
Epoch 72/300, Loss: 0.0806 | 0.0743
Epoch 73/300, Loss: 0.0805 | 0.0742
Epoch 74/300, Loss: 0.0805 | 0.0742
Epoch 75/300, Loss: 0.0805 | 0.0743
Epoch 76/300, Loss: 0.0804 | 0.0743
Epoch 77/300, Loss: 0.0804 | 0.0742
Epoch 78/300, Loss: 0.0804 | 0.0743
Epoch 79/300, Loss: 0.0804 | 0.0743
Epoch 80/300, Loss: 0.0804 | 0.0743
Epoch 81/300, Loss: 0.0803 | 0.0743
Epoch 82/300, Loss: 0.0803 | 0.0743
Epoch 83/300, Loss: 0.0803 | 0.0743
Epoch 84/300, Loss: 0.0803 | 0.0743
Epoch 85/300, Loss: 0.0803 | 0.0742
Epoch 86/300, Loss: 0.0802 | 0.0742
Epoch 87/300, Loss: 0.0802 | 0.0742
Epoch 88/300, Loss: 0.0802 | 0.0742
Epoch 89/300, Loss: 0.0802 | 0.0741
Epoch 90/300, Loss: 0.0802 | 0.0741
Epoch 91/300, Loss: 0.0802 | 0.0741
Epoch 92/300, Loss: 0.0801 | 0.0741
Epoch 93/300, Loss: 0.0801 | 0.0741
Epoch 94/300, Loss: 0.0801 | 0.0741
Epoch 95/300, Loss: 0.0801 | 0.0740
Epoch 96/300, Loss: 0.0801 | 0.0740
Epoch 97/300, Loss: 0.0801 | 0.0740
Epoch 98/300, Loss: 0.0801 | 0.0740
Epoch 99/300, Loss: 0.0801 | 0.0740
Epoch 100/300, Loss: 0.0801 | 0.0740
Epoch 101/300, Loss: 0.0801 | 0.0740
Epoch 102/300, Loss: 0.0800 | 0.0740
Epoch 103/300, Loss: 0.0800 | 0.0739
Epoch 104/300, Loss: 0.0800 | 0.0739
Epoch 105/300, Loss: 0.0800 | 0.0739
Epoch 106/300, Loss: 0.0800 | 0.0739
Epoch 107/300, Loss: 0.0800 | 0.0739
Epoch 108/300, Loss: 0.0800 | 0.0739
Epoch 109/300, Loss: 0.0800 | 0.0739
Epoch 110/300, Loss: 0.0800 | 0.0739
Epoch 111/300, Loss: 0.0800 | 0.0739
Epoch 112/300, Loss: 0.0800 | 0.0739
Epoch 113/300, Loss: 0.0800 | 0.0739
Epoch 114/300, Loss: 0.0800 | 0.0739
Epoch 115/300, Loss: 0.0800 | 0.0739
Epoch 116/300, Loss: 0.0800 | 0.0739
Epoch 117/300, Loss: 0.0800 | 0.0739
Epoch 118/300, Loss: 0.0800 | 0.0739
Epoch 119/300, Loss: 0.0800 | 0.0739
Epoch 120/300, Loss: 0.0800 | 0.0739
Epoch 121/300, Loss: 0.0800 | 0.0739
Epoch 122/300, Loss: 0.0800 | 0.0739
Epoch 123/300, Loss: 0.0800 | 0.0739
Epoch 124/300, Loss: 0.0800 | 0.0738
Epoch 125/300, Loss: 0.0800 | 0.0738
Epoch 126/300, Loss: 0.0800 | 0.0738
Epoch 127/300, Loss: 0.0800 | 0.0738
Epoch 128/300, Loss: 0.0800 | 0.0738
Epoch 129/300, Loss: 0.0800 | 0.0738
Epoch 130/300, Loss: 0.0800 | 0.0738
Epoch 131/300, Loss: 0.0800 | 0.0738
Epoch 132/300, Loss: 0.0800 | 0.0738
Epoch 133/300, Loss: 0.0800 | 0.0738
Epoch 134/300, Loss: 0.0800 | 0.0738
Epoch 135/300, Loss: 0.0800 | 0.0738
Epoch 136/300, Loss: 0.0800 | 0.0738
Epoch 137/300, Loss: 0.0800 | 0.0738
Epoch 138/300, Loss: 0.0800 | 0.0738
Epoch 139/300, Loss: 0.0800 | 0.0738
Epoch 140/300, Loss: 0.0800 | 0.0738
Epoch 141/300, Loss: 0.0800 | 0.0738
Epoch 142/300, Loss: 0.0800 | 0.0738
Epoch 143/300, Loss: 0.0800 | 0.0738
Epoch 144/300, Loss: 0.0800 | 0.0738
Epoch 145/300, Loss: 0.0800 | 0.0738
Epoch 146/300, Loss: 0.0800 | 0.0738
Epoch 147/300, Loss: 0.0800 | 0.0738
Epoch 148/300, Loss: 0.0800 | 0.0738
Epoch 149/300, Loss: 0.0799 | 0.0738
Epoch 150/300, Loss: 0.0799 | 0.0738
Epoch 151/300, Loss: 0.0799 | 0.0738
Epoch 152/300, Loss: 0.0799 | 0.0738
Epoch 153/300, Loss: 0.0799 | 0.0738
Epoch 154/300, Loss: 0.0799 | 0.0738
Epoch 155/300, Loss: 0.0799 | 0.0738
Epoch 156/300, Loss: 0.0799 | 0.0738
Epoch 157/300, Loss: 0.0799 | 0.0738
Epoch 158/300, Loss: 0.0799 | 0.0738
Epoch 159/300, Loss: 0.0799 | 0.0738
Epoch 160/300, Loss: 0.0799 | 0.0738
Epoch 161/300, Loss: 0.0799 | 0.0738
Epoch 162/300, Loss: 0.0799 | 0.0738
Epoch 163/300, Loss: 0.0799 | 0.0738
Epoch 164/300, Loss: 0.0799 | 0.0738
Epoch 165/300, Loss: 0.0799 | 0.0738
Epoch 166/300, Loss: 0.0799 | 0.0738
Epoch 167/300, Loss: 0.0799 | 0.0738
Epoch 168/300, Loss: 0.0799 | 0.0738
Epoch 169/300, Loss: 0.0799 | 0.0738
Epoch 170/300, Loss: 0.0799 | 0.0738
Epoch 171/300, Loss: 0.0799 | 0.0738
Epoch 172/300, Loss: 0.0799 | 0.0738
Epoch 173/300, Loss: 0.0799 | 0.0738
Epoch 174/300, Loss: 0.0799 | 0.0738
Epoch 175/300, Loss: 0.0799 | 0.0738
Epoch 176/300, Loss: 0.0799 | 0.0738
Epoch 177/300, Loss: 0.0799 | 0.0738
Epoch 178/300, Loss: 0.0799 | 0.0738
Epoch 179/300, Loss: 0.0799 | 0.0738
Epoch 180/300, Loss: 0.0799 | 0.0738
Epoch 181/300, Loss: 0.0799 | 0.0738
Epoch 182/300, Loss: 0.0799 | 0.0738
Epoch 183/300, Loss: 0.0799 | 0.0738
Epoch 184/300, Loss: 0.0799 | 0.0738
Epoch 185/300, Loss: 0.0799 | 0.0738
Epoch 186/300, Loss: 0.0799 | 0.0738
Epoch 187/300, Loss: 0.0799 | 0.0738
Epoch 188/300, Loss: 0.0799 | 0.0738
Epoch 189/300, Loss: 0.0799 | 0.0738
Epoch 190/300, Loss: 0.0799 | 0.0738
Epoch 191/300, Loss: 0.0799 | 0.0738
Epoch 192/300, Loss: 0.0799 | 0.0738
Epoch 193/300, Loss: 0.0799 | 0.0738
Epoch 194/300, Loss: 0.0799 | 0.0738
Epoch 195/300, Loss: 0.0799 | 0.0738
Epoch 196/300, Loss: 0.0799 | 0.0738
Epoch 197/300, Loss: 0.0799 | 0.0738
Epoch 198/300, Loss: 0.0799 | 0.0738
Epoch 199/300, Loss: 0.0799 | 0.0738
Epoch 200/300, Loss: 0.0799 | 0.0738
Epoch 201/300, Loss: 0.0799 | 0.0738
Epoch 202/300, Loss: 0.0799 | 0.0738
Epoch 203/300, Loss: 0.0799 | 0.0738
Epoch 204/300, Loss: 0.0799 | 0.0738
Epoch 205/300, Loss: 0.0799 | 0.0738
Epoch 206/300, Loss: 0.0799 | 0.0738
Epoch 207/300, Loss: 0.0799 | 0.0738
Epoch 208/300, Loss: 0.0799 | 0.0738
Epoch 209/300, Loss: 0.0799 | 0.0738
Epoch 210/300, Loss: 0.0799 | 0.0738
Epoch 211/300, Loss: 0.0799 | 0.0738
Epoch 212/300, Loss: 0.0799 | 0.0738
Epoch 213/300, Loss: 0.0799 | 0.0738
Epoch 214/300, Loss: 0.0799 | 0.0738
Epoch 215/300, Loss: 0.0799 | 0.0738
Epoch 216/300, Loss: 0.0799 | 0.0738
Epoch 217/300, Loss: 0.0799 | 0.0738
Epoch 218/300, Loss: 0.0799 | 0.0738
Epoch 219/300, Loss: 0.0799 | 0.0738
Epoch 220/300, Loss: 0.0799 | 0.0738
Epoch 221/300, Loss: 0.0799 | 0.0738
Epoch 222/300, Loss: 0.0799 | 0.0738
Epoch 223/300, Loss: 0.0799 | 0.0738
Epoch 224/300, Loss: 0.0799 | 0.0738
Epoch 225/300, Loss: 0.0799 | 0.0738
Epoch 226/300, Loss: 0.0799 | 0.0738
Epoch 227/300, Loss: 0.0799 | 0.0738
Epoch 228/300, Loss: 0.0799 | 0.0738
Epoch 229/300, Loss: 0.0799 | 0.0738
Epoch 230/300, Loss: 0.0799 | 0.0738
Epoch 231/300, Loss: 0.0799 | 0.0738
Epoch 232/300, Loss: 0.0799 | 0.0738
Epoch 233/300, Loss: 0.0799 | 0.0738
Epoch 234/300, Loss: 0.0799 | 0.0738
Epoch 235/300, Loss: 0.0799 | 0.0738
Epoch 236/300, Loss: 0.0799 | 0.0738
Epoch 237/300, Loss: 0.0799 | 0.0738
Epoch 238/300, Loss: 0.0799 | 0.0738
Epoch 239/300, Loss: 0.0799 | 0.0738
Epoch 240/300, Loss: 0.0799 | 0.0738
Epoch 241/300, Loss: 0.0799 | 0.0738
Epoch 242/300, Loss: 0.0799 | 0.0738
Epoch 243/300, Loss: 0.0799 | 0.0738
Epoch 244/300, Loss: 0.0799 | 0.0738
Epoch 245/300, Loss: 0.0799 | 0.0738
Epoch 246/300, Loss: 0.0799 | 0.0738
Epoch 247/300, Loss: 0.0799 | 0.0738
Epoch 248/300, Loss: 0.0799 | 0.0738
Epoch 249/300, Loss: 0.0799 | 0.0738
Epoch 250/300, Loss: 0.0799 | 0.0738
Epoch 251/300, Loss: 0.0799 | 0.0738
Epoch 252/300, Loss: 0.0799 | 0.0738
Epoch 253/300, Loss: 0.0799 | 0.0738
Epoch 254/300, Loss: 0.0799 | 0.0738
Epoch 255/300, Loss: 0.0799 | 0.0738
Epoch 256/300, Loss: 0.0799 | 0.0738
Epoch 257/300, Loss: 0.0799 | 0.0738
Epoch 258/300, Loss: 0.0799 | 0.0738
Epoch 259/300, Loss: 0.0799 | 0.0738
Epoch 260/300, Loss: 0.0799 | 0.0738
Epoch 261/300, Loss: 0.0799 | 0.0738
Epoch 262/300, Loss: 0.0799 | 0.0738
Epoch 263/300, Loss: 0.0799 | 0.0738
Epoch 264/300, Loss: 0.0799 | 0.0738
Epoch 265/300, Loss: 0.0799 | 0.0738
Epoch 266/300, Loss: 0.0799 | 0.0738
Epoch 267/300, Loss: 0.0799 | 0.0738
Epoch 268/300, Loss: 0.0799 | 0.0738
Epoch 269/300, Loss: 0.0799 | 0.0738
Epoch 270/300, Loss: 0.0799 | 0.0738
Epoch 271/300, Loss: 0.0799 | 0.0738
Epoch 272/300, Loss: 0.0799 | 0.0738
Early stopping
Runtime (seconds): 344.36209535598755
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 4.391811700130347
RMSE: 2.0956649780273438
MAE: 2.0956649780273438
R-squared: nan
[99.41434]
