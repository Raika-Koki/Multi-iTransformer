[32m[I 2025-01-07 20:50:22,574][0m A new study created in memory with name: no-name-514e67a8-6c8a-4abf-b969-2c28a0d8ef52[0m
[32m[I 2025-01-07 20:54:47,530][0m Trial 0 finished with value: 1.1324572592565458 and parameters: {'observation_period_num': 203, 'train_rates': 0.736327434769066, 'learning_rate': 3.090774875291559e-06, 'batch_size': 53, 'step_size': 3, 'gamma': 0.9234307165925815}. Best is trial 0 with value: 1.1324572592565458.[0m
[32m[I 2025-01-07 20:56:15,517][0m Trial 1 finished with value: 0.5527142882347107 and parameters: {'observation_period_num': 73, 'train_rates': 0.782423068611663, 'learning_rate': 0.00011290297450791235, 'batch_size': 180, 'step_size': 11, 'gamma': 0.8686099327162534}. Best is trial 1 with value: 0.5527142882347107.[0m
[32m[I 2025-01-07 21:01:47,573][0m Trial 2 finished with value: 0.3515366373020549 and parameters: {'observation_period_num': 236, 'train_rates': 0.8112082832609567, 'learning_rate': 4.705452797638392e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7838511340859065}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:02:48,859][0m Trial 3 finished with value: 1.0320390490657192 and parameters: {'observation_period_num': 54, 'train_rates': 0.6558091588167732, 'learning_rate': 0.0004273852291272162, 'batch_size': 85, 'step_size': 5, 'gamma': 0.9831945559822111}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:07:36,559][0m Trial 4 finished with value: 0.5421713468036041 and parameters: {'observation_period_num': 200, 'train_rates': 0.853530701896615, 'learning_rate': 3.863164330354096e-06, 'batch_size': 45, 'step_size': 9, 'gamma': 0.7958549261936706}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:08:16,723][0m Trial 5 finished with value: 0.7362240248648547 and parameters: {'observation_period_num': 34, 'train_rates': 0.7066453567363264, 'learning_rate': 2.5944811084232164e-05, 'batch_size': 183, 'step_size': 3, 'gamma': 0.9658245260726109}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:13:48,467][0m Trial 6 finished with value: 0.5494710206985474 and parameters: {'observation_period_num': 211, 'train_rates': 0.9810138168830548, 'learning_rate': 1.503672271551914e-05, 'batch_size': 236, 'step_size': 3, 'gamma': 0.8571633130501934}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:17:00,876][0m Trial 7 finished with value: 0.4418427897419067 and parameters: {'observation_period_num': 132, 'train_rates': 0.8012843406777221, 'learning_rate': 1.1611307465395535e-05, 'batch_size': 27, 'step_size': 11, 'gamma': 0.7674527356889769}. Best is trial 2 with value: 0.3515366373020549.[0m
[32m[I 2025-01-07 21:19:00,651][0m Trial 8 finished with value: 0.3384879231452942 and parameters: {'observation_period_num': 89, 'train_rates': 0.9659043344738457, 'learning_rate': 1.8734964872398888e-05, 'batch_size': 238, 'step_size': 8, 'gamma': 0.8618886230109355}. Best is trial 8 with value: 0.3384879231452942.[0m
[32m[I 2025-01-07 21:23:23,374][0m Trial 9 finished with value: 0.24902581473508964 and parameters: {'observation_period_num': 188, 'train_rates': 0.8874523950867201, 'learning_rate': 6.226949622425199e-05, 'batch_size': 235, 'step_size': 10, 'gamma': 0.904091849091071}. Best is trial 9 with value: 0.24902581473508964.[0m
[32m[I 2025-01-07 21:27:03,724][0m Trial 10 finished with value: 2.192199814858748 and parameters: {'observation_period_num': 154, 'train_rates': 0.9293669203188064, 'learning_rate': 0.0009919988973533455, 'batch_size': 131, 'step_size': 14, 'gamma': 0.9214450094935418}. Best is trial 9 with value: 0.24902581473508964.[0m
[32m[I 2025-01-07 21:29:25,857][0m Trial 11 finished with value: 0.19129740019137248 and parameters: {'observation_period_num': 109, 'train_rates': 0.9031353659348287, 'learning_rate': 0.00012596960587434868, 'batch_size': 256, 'step_size': 7, 'gamma': 0.833022084288184}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:33:10,825][0m Trial 12 finished with value: 0.23224799068558294 and parameters: {'observation_period_num': 164, 'train_rates': 0.8925725919922344, 'learning_rate': 0.00016388908261220138, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8227356844903841}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:35:37,605][0m Trial 13 finished with value: 0.20175621637867558 and parameters: {'observation_period_num': 113, 'train_rates': 0.89939368596269, 'learning_rate': 0.00020815593596035259, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8214247071873186}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:37:52,199][0m Trial 14 finished with value: 0.3235499597432321 and parameters: {'observation_period_num': 108, 'train_rates': 0.8608237695162794, 'learning_rate': 0.0002761998464196668, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8095997239451014}. Best is trial 11 with value: 0.19129740019137248.[0m
Early stopping at epoch 78
[32m[I 2025-01-07 21:38:17,638][0m Trial 15 finished with value: 0.4386238040427816 and parameters: {'observation_period_num': 9, 'train_rates': 0.9256475708909063, 'learning_rate': 0.0006168042258912103, 'batch_size': 145, 'step_size': 1, 'gamma': 0.8325506646825547}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:40:54,343][0m Trial 16 finished with value: 0.22013947367668152 and parameters: {'observation_period_num': 117, 'train_rates': 0.9307376511219841, 'learning_rate': 0.00012799636110025602, 'batch_size': 211, 'step_size': 5, 'gamma': 0.7551821850304579}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:43:27,735][0m Trial 17 finished with value: 0.9170738267695605 and parameters: {'observation_period_num': 143, 'train_rates': 0.6052857775203282, 'learning_rate': 0.0002757038754219302, 'batch_size': 158, 'step_size': 8, 'gamma': 0.8402875480757956}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:45:22,187][0m Trial 18 finished with value: 0.3157292681419297 and parameters: {'observation_period_num': 92, 'train_rates': 0.8425850707470416, 'learning_rate': 7.499611359887095e-05, 'batch_size': 210, 'step_size': 6, 'gamma': 0.8947196453175691}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:46:29,340][0m Trial 19 finished with value: 1.3746009327627875 and parameters: {'observation_period_num': 56, 'train_rates': 0.7714766273170272, 'learning_rate': 1.392648795876059e-06, 'batch_size': 112, 'step_size': 15, 'gamma': 0.7867079966174256}. Best is trial 11 with value: 0.19129740019137248.[0m
Early stopping at epoch 61
[32m[I 2025-01-07 21:49:00,179][0m Trial 20 finished with value: 0.30871649223861963 and parameters: {'observation_period_num': 174, 'train_rates': 0.899338079383645, 'learning_rate': 0.00023747325193752383, 'batch_size': 166, 'step_size': 1, 'gamma': 0.816040219859967}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:51:35,229][0m Trial 21 finished with value: 0.206948384642601 and parameters: {'observation_period_num': 116, 'train_rates': 0.9419560538053187, 'learning_rate': 0.00010002171208670465, 'batch_size': 204, 'step_size': 5, 'gamma': 0.7558402499815109}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:54:01,518][0m Trial 22 finished with value: 0.338890016078949 and parameters: {'observation_period_num': 108, 'train_rates': 0.9541063775240523, 'learning_rate': 4.798237650813529e-05, 'batch_size': 212, 'step_size': 4, 'gamma': 0.7697914371711416}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:56:59,231][0m Trial 23 finished with value: 0.27416669570396995 and parameters: {'observation_period_num': 129, 'train_rates': 0.8870283578929579, 'learning_rate': 0.00010742505126707921, 'batch_size': 250, 'step_size': 7, 'gamma': 0.8425953694735477}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 21:58:55,883][0m Trial 24 finished with value: 0.19783441722393036 and parameters: {'observation_period_num': 86, 'train_rates': 0.981904195586799, 'learning_rate': 0.0005375509880128396, 'batch_size': 219, 'step_size': 5, 'gamma': 0.803113984265819}. Best is trial 11 with value: 0.19129740019137248.[0m
[32m[I 2025-01-07 22:00:54,612][0m Trial 25 finished with value: 0.18492014706134796 and parameters: {'observation_period_num': 88, 'train_rates': 0.9851447092021277, 'learning_rate': 0.0005228263591392262, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8113583666357445}. Best is trial 25 with value: 0.18492014706134796.[0m
[32m[I 2025-01-07 22:02:44,300][0m Trial 26 finished with value: 0.18109570443630219 and parameters: {'observation_period_num': 80, 'train_rates': 0.9835640014689764, 'learning_rate': 0.0005186575875094078, 'batch_size': 225, 'step_size': 9, 'gamma': 0.8001248628563744}. Best is trial 26 with value: 0.18109570443630219.[0m
[32m[I 2025-01-07 22:04:05,101][0m Trial 27 finished with value: 0.9135405421257019 and parameters: {'observation_period_num': 62, 'train_rates': 0.989229907717548, 'learning_rate': 0.0009738893417072036, 'batch_size': 224, 'step_size': 9, 'gamma': 0.847440361105974}. Best is trial 26 with value: 0.18109570443630219.[0m
[32m[I 2025-01-07 22:04:37,560][0m Trial 28 finished with value: 0.11882322281599045 and parameters: {'observation_period_num': 21, 'train_rates': 0.9586315313834431, 'learning_rate': 0.00037533863967913384, 'batch_size': 254, 'step_size': 13, 'gamma': 0.8820932765531233}. Best is trial 28 with value: 0.11882322281599045.[0m
[32m[I 2025-01-07 22:05:02,242][0m Trial 29 finished with value: 0.1126302033662796 and parameters: {'observation_period_num': 9, 'train_rates': 0.9605336407417484, 'learning_rate': 0.0003756024406490661, 'batch_size': 182, 'step_size': 13, 'gamma': 0.8894923758872988}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:05:23,910][0m Trial 30 finished with value: 0.7548286025887293 and parameters: {'observation_period_num': 5, 'train_rates': 0.7452993854092406, 'learning_rate': 0.0003687365175964524, 'batch_size': 173, 'step_size': 13, 'gamma': 0.8857803966597408}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:06:00,927][0m Trial 31 finished with value: 0.21128082275390625 and parameters: {'observation_period_num': 26, 'train_rates': 0.9620648464955402, 'learning_rate': 0.0006812752071945326, 'batch_size': 236, 'step_size': 13, 'gamma': 0.8804250814925254}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:06:50,400][0m Trial 32 finished with value: 0.1316143423318863 and parameters: {'observation_period_num': 36, 'train_rates': 0.9631289962474013, 'learning_rate': 0.00038201049382193913, 'batch_size': 232, 'step_size': 15, 'gamma': 0.9159695548276354}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:07:34,212][0m Trial 33 finished with value: 0.14018386602401733 and parameters: {'observation_period_num': 32, 'train_rates': 0.9498111409259112, 'learning_rate': 0.0003482241770940416, 'batch_size': 190, 'step_size': 15, 'gamma': 0.9434996560962978}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:08:19,193][0m Trial 34 finished with value: 0.14784051477909088 and parameters: {'observation_period_num': 32, 'train_rates': 0.9494747976977738, 'learning_rate': 0.0002836949264431096, 'batch_size': 183, 'step_size': 15, 'gamma': 0.9373913727580254}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:08:57,703][0m Trial 35 finished with value: 0.1719289907158223 and parameters: {'observation_period_num': 24, 'train_rates': 0.9243269109840468, 'learning_rate': 0.000360729981317994, 'batch_size': 122, 'step_size': 14, 'gamma': 0.9494373990221731}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:09:55,684][0m Trial 36 finished with value: 0.46759270404942616 and parameters: {'observation_period_num': 42, 'train_rates': 0.8472582267051102, 'learning_rate': 6.494873286359367e-06, 'batch_size': 95, 'step_size': 12, 'gamma': 0.9130247364711506}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:10:26,550][0m Trial 37 finished with value: 0.2740779373173912 and parameters: {'observation_period_num': 16, 'train_rates': 0.8699794058390714, 'learning_rate': 0.00017553751515900792, 'batch_size': 152, 'step_size': 14, 'gamma': 0.9450161755688589}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:11:10,790][0m Trial 38 finished with value: 0.972418018271605 and parameters: {'observation_period_num': 39, 'train_rates': 0.7007560521967149, 'learning_rate': 0.00038216483155588757, 'batch_size': 195, 'step_size': 12, 'gamma': 0.970096790940786}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:12:40,673][0m Trial 39 finished with value: 0.1984754779333931 and parameters: {'observation_period_num': 68, 'train_rates': 0.9162376107053548, 'learning_rate': 3.401071278470508e-05, 'batch_size': 181, 'step_size': 13, 'gamma': 0.8766977307976725}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:13:38,523][0m Trial 40 finished with value: 0.5918788213766258 and parameters: {'observation_period_num': 49, 'train_rates': 0.8207976915545889, 'learning_rate': 0.0007734358263836267, 'batch_size': 242, 'step_size': 11, 'gamma': 0.9313874178007867}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:14:21,063][0m Trial 41 finished with value: 0.12326887995004654 and parameters: {'observation_period_num': 30, 'train_rates': 0.9512552529602031, 'learning_rate': 0.00029569556514337183, 'batch_size': 189, 'step_size': 15, 'gamma': 0.9350090854692944}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:14:53,061][0m Trial 42 finished with value: 0.119340680539608 and parameters: {'observation_period_num': 19, 'train_rates': 0.9662831269707959, 'learning_rate': 0.00018518758533981212, 'batch_size': 165, 'step_size': 15, 'gamma': 0.9065334583000326}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:15:22,849][0m Trial 43 finished with value: 0.12379101663827896 and parameters: {'observation_period_num': 18, 'train_rates': 0.9657616170292408, 'learning_rate': 0.00018143330746275938, 'batch_size': 168, 'step_size': 14, 'gamma': 0.8999900938199656}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:15:55,606][0m Trial 44 finished with value: 0.11983425170183182 and parameters: {'observation_period_num': 19, 'train_rates': 0.968303039583177, 'learning_rate': 0.00016782000320634025, 'batch_size': 165, 'step_size': 14, 'gamma': 0.901589397189224}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:22:00,896][0m Trial 45 finished with value: 0.140487811330593 and parameters: {'observation_period_num': 240, 'train_rates': 0.9396078726165477, 'learning_rate': 7.080008632128187e-05, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8679193878325034}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:22:31,200][0m Trial 46 finished with value: 0.13787718106280356 and parameters: {'observation_period_num': 5, 'train_rates': 0.9082117476747902, 'learning_rate': 0.00018719813123132457, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8882248954245028}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:23:40,503][0m Trial 47 finished with value: 0.19611531496047974 and parameters: {'observation_period_num': 51, 'train_rates': 0.9690005957505321, 'learning_rate': 0.00015951965119558728, 'batch_size': 129, 'step_size': 14, 'gamma': 0.9078702981141369}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:24:21,606][0m Trial 48 finished with value: 0.19563670391621796 and parameters: {'observation_period_num': 16, 'train_rates': 0.8753702140374632, 'learning_rate': 9.159478162435101e-05, 'batch_size': 104, 'step_size': 12, 'gamma': 0.9272332624574023}. Best is trial 29 with value: 0.1126302033662796.[0m
[32m[I 2025-01-07 22:25:21,836][0m Trial 49 finished with value: 0.1713773357707101 and parameters: {'observation_period_num': 45, 'train_rates': 0.9370187167188027, 'learning_rate': 0.00028844871375715723, 'batch_size': 149, 'step_size': 10, 'gamma': 0.9640902208219099}. Best is trial 29 with value: 0.1126302033662796.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.8849 | 0.8685
Epoch 2/300, Loss: 0.9391 | 0.8447
Epoch 3/300, Loss: 0.7062 | 0.7749
Epoch 4/300, Loss: 0.6454 | 0.7089
Epoch 5/300, Loss: 0.5239 | 0.7478
Epoch 6/300, Loss: 0.4955 | 0.6330
Epoch 7/300, Loss: 0.5985 | 0.6310
Epoch 8/300, Loss: 0.5532 | 0.5929
Epoch 9/300, Loss: 0.6183 | 0.6372
Epoch 10/300, Loss: 0.5667 | 0.7012
Epoch 11/300, Loss: 0.5234 | 0.5934
Epoch 12/300, Loss: 0.4220 | 0.5062
Epoch 13/300, Loss: 0.3604 | 0.4541
Epoch 14/300, Loss: 0.3347 | 0.4240
Epoch 15/300, Loss: 0.3969 | 0.5128
Epoch 16/300, Loss: 0.4181 | 0.5335
Epoch 17/300, Loss: 0.3978 | 0.4847
Epoch 18/300, Loss: 0.3779 | 0.4706
Epoch 19/300, Loss: 0.3514 | 0.4532
Epoch 20/300, Loss: 0.3219 | 0.3835
Epoch 21/300, Loss: 0.3279 | 0.3655
Epoch 22/300, Loss: 0.4833 | 0.3749
Epoch 23/300, Loss: 0.3231 | 0.3678
Epoch 24/300, Loss: 0.2995 | 0.3506
Epoch 25/300, Loss: 0.3137 | 0.3215
Epoch 26/300, Loss: 0.2676 | 0.3495
Epoch 27/300, Loss: 0.2419 | 0.3198
Epoch 28/300, Loss: 0.2304 | 0.3033
Epoch 29/300, Loss: 0.2112 | 0.2725
Epoch 30/300, Loss: 0.2113 | 0.2621
Epoch 31/300, Loss: 0.2248 | 0.2608
Epoch 32/300, Loss: 0.2238 | 0.2588
Epoch 33/300, Loss: 0.1955 | 0.2341
Epoch 34/300, Loss: 0.1909 | 0.2482
Epoch 35/300, Loss: 0.1968 | 0.2338
Epoch 36/300, Loss: 0.1859 | 0.2470
Epoch 37/300, Loss: 0.1818 | 0.2207
Epoch 38/300, Loss: 0.1682 | 0.2341
Epoch 39/300, Loss: 0.1712 | 0.2196
Epoch 40/300, Loss: 0.1687 | 0.2307
Epoch 41/300, Loss: 0.1741 | 0.2176
Epoch 42/300, Loss: 0.1656 | 0.2160
Epoch 43/300, Loss: 0.1591 | 0.2033
Epoch 44/300, Loss: 0.1518 | 0.2085
Epoch 45/300, Loss: 0.1509 | 0.1953
Epoch 46/300, Loss: 0.1531 | 0.2149
Epoch 47/300, Loss: 0.1599 | 0.1899
Epoch 48/300, Loss: 0.1630 | 0.2219
Epoch 49/300, Loss: 0.1548 | 0.1880
Epoch 50/300, Loss: 0.1480 | 0.2080
Epoch 51/300, Loss: 0.1422 | 0.1871
Epoch 52/300, Loss: 0.1385 | 0.1987
Epoch 53/300, Loss: 0.1348 | 0.1844
Epoch 54/300, Loss: 0.1297 | 0.1906
Epoch 55/300, Loss: 0.1277 | 0.1782
Epoch 56/300, Loss: 0.1254 | 0.1828
Epoch 57/300, Loss: 0.1261 | 0.1743
Epoch 58/300, Loss: 0.1251 | 0.1784
Epoch 59/300, Loss: 0.1256 | 0.1688
Epoch 60/300, Loss: 0.1254 | 0.1742
Epoch 61/300, Loss: 0.1214 | 0.1630
Epoch 62/300, Loss: 0.1195 | 0.1688
Epoch 63/300, Loss: 0.1174 | 0.1561
Epoch 64/300, Loss: 0.1164 | 0.1644
Epoch 65/300, Loss: 0.1161 | 0.1521
Epoch 66/300, Loss: 0.1158 | 0.1625
Epoch 67/300, Loss: 0.1154 | 0.1494
Epoch 68/300, Loss: 0.1139 | 0.1579
Epoch 69/300, Loss: 0.1147 | 0.1457
Epoch 70/300, Loss: 0.1152 | 0.1573
Epoch 71/300, Loss: 0.1153 | 0.1408
Epoch 72/300, Loss: 0.1167 | 0.1579
Epoch 73/300, Loss: 0.1163 | 0.1378
Epoch 74/300, Loss: 0.1126 | 0.1575
Epoch 75/300, Loss: 0.1091 | 0.1383
Epoch 76/300, Loss: 0.1074 | 0.1520
Epoch 77/300, Loss: 0.1073 | 0.1388
Epoch 78/300, Loss: 0.1058 | 0.1456
Epoch 79/300, Loss: 0.1060 | 0.1396
Epoch 80/300, Loss: 0.1032 | 0.1370
Epoch 81/300, Loss: 0.1022 | 0.1355
Epoch 82/300, Loss: 0.1000 | 0.1331
Epoch 83/300, Loss: 0.0990 | 0.1322
Epoch 84/300, Loss: 0.0982 | 0.1297
Epoch 85/300, Loss: 0.0979 | 0.1291
Epoch 86/300, Loss: 0.0969 | 0.1268
Epoch 87/300, Loss: 0.0967 | 0.1283
Epoch 88/300, Loss: 0.0966 | 0.1234
Epoch 89/300, Loss: 0.0957 | 0.1268
Epoch 90/300, Loss: 0.0957 | 0.1192
Epoch 91/300, Loss: 0.0959 | 0.1281
Epoch 92/300, Loss: 0.0968 | 0.1157
Epoch 93/300, Loss: 0.0960 | 0.1279
Epoch 94/300, Loss: 0.0955 | 0.1146
Epoch 95/300, Loss: 0.0938 | 0.1282
Epoch 96/300, Loss: 0.0937 | 0.1141
Epoch 97/300, Loss: 0.0925 | 0.1258
Epoch 98/300, Loss: 0.0924 | 0.1132
Epoch 99/300, Loss: 0.0914 | 0.1244
Epoch 100/300, Loss: 0.0910 | 0.1134
Epoch 101/300, Loss: 0.0897 | 0.1215
Epoch 102/300, Loss: 0.0899 | 0.1129
Epoch 103/300, Loss: 0.0890 | 0.1185
Epoch 104/300, Loss: 0.0889 | 0.1114
Epoch 105/300, Loss: 0.0883 | 0.1157
Epoch 106/300, Loss: 0.0877 | 0.1101
Epoch 107/300, Loss: 0.0868 | 0.1145
Epoch 108/300, Loss: 0.0869 | 0.1106
Epoch 109/300, Loss: 0.0872 | 0.1117
Epoch 110/300, Loss: 0.0860 | 0.1097
Epoch 111/300, Loss: 0.0865 | 0.1099
Epoch 112/300, Loss: 0.0856 | 0.1084
Epoch 113/300, Loss: 0.0850 | 0.1086
Epoch 114/300, Loss: 0.0843 | 0.1065
Epoch 115/300, Loss: 0.0845 | 0.1078
Epoch 116/300, Loss: 0.0844 | 0.1052
Epoch 117/300, Loss: 0.0847 | 0.1068
Epoch 118/300, Loss: 0.0838 | 0.1049
Epoch 119/300, Loss: 0.0836 | 0.1056
Epoch 120/300, Loss: 0.0831 | 0.1044
Epoch 121/300, Loss: 0.0828 | 0.1045
Epoch 122/300, Loss: 0.0827 | 0.1042
Epoch 123/300, Loss: 0.0823 | 0.1028
Epoch 124/300, Loss: 0.0821 | 0.1032
Epoch 125/300, Loss: 0.0820 | 0.1025
Epoch 126/300, Loss: 0.0823 | 0.1021
Epoch 127/300, Loss: 0.0814 | 0.1018
Epoch 128/300, Loss: 0.0817 | 0.1022
Epoch 129/300, Loss: 0.0821 | 0.1013
Epoch 130/300, Loss: 0.0811 | 0.1009
Epoch 131/300, Loss: 0.0806 | 0.0998
Epoch 132/300, Loss: 0.0807 | 0.1002
Epoch 133/300, Loss: 0.0802 | 0.0995
Epoch 134/300, Loss: 0.0806 | 0.0993
Epoch 135/300, Loss: 0.0800 | 0.0998
Epoch 136/300, Loss: 0.0798 | 0.0986
Epoch 137/300, Loss: 0.0794 | 0.0980
Epoch 138/300, Loss: 0.0790 | 0.0987
Epoch 139/300, Loss: 0.0791 | 0.0983
Epoch 140/300, Loss: 0.0793 | 0.0973
Epoch 141/300, Loss: 0.0789 | 0.0972
Epoch 142/300, Loss: 0.0786 | 0.0979
Epoch 143/300, Loss: 0.0787 | 0.0972
Epoch 144/300, Loss: 0.0784 | 0.0969
Epoch 145/300, Loss: 0.0785 | 0.0967
Epoch 146/300, Loss: 0.0780 | 0.0959
Epoch 147/300, Loss: 0.0787 | 0.0949
Epoch 148/300, Loss: 0.0777 | 0.0953
Epoch 149/300, Loss: 0.0783 | 0.0954
Epoch 150/300, Loss: 0.0777 | 0.0957
Epoch 151/300, Loss: 0.0775 | 0.0949
Epoch 152/300, Loss: 0.0772 | 0.0955
Epoch 153/300, Loss: 0.0767 | 0.0952
Epoch 154/300, Loss: 0.0778 | 0.0951
Epoch 155/300, Loss: 0.0772 | 0.0952
Epoch 156/300, Loss: 0.0759 | 0.0947
Epoch 157/300, Loss: 0.0753 | 0.0948
Epoch 158/300, Loss: 0.0768 | 0.0941
Epoch 159/300, Loss: 0.0760 | 0.0939
Epoch 160/300, Loss: 0.0766 | 0.0938
Epoch 161/300, Loss: 0.0758 | 0.0935
Epoch 162/300, Loss: 0.0760 | 0.0944
Epoch 163/300, Loss: 0.0758 | 0.0935
Epoch 164/300, Loss: 0.0764 | 0.0941
Epoch 165/300, Loss: 0.0749 | 0.0925
Epoch 166/300, Loss: 0.0760 | 0.0929
Epoch 167/300, Loss: 0.0756 | 0.0927
Epoch 168/300, Loss: 0.0754 | 0.0929
Epoch 169/300, Loss: 0.0750 | 0.0928
Epoch 170/300, Loss: 0.0750 | 0.0926
Epoch 171/300, Loss: 0.0748 | 0.0922
Epoch 172/300, Loss: 0.0747 | 0.0916
Epoch 173/300, Loss: 0.0746 | 0.0918
Epoch 174/300, Loss: 0.0748 | 0.0918
Epoch 175/300, Loss: 0.0744 | 0.0914
Epoch 176/300, Loss: 0.0749 | 0.0913
Epoch 177/300, Loss: 0.0745 | 0.0914
Epoch 178/300, Loss: 0.0745 | 0.0914
Epoch 179/300, Loss: 0.0741 | 0.0917
Epoch 180/300, Loss: 0.0745 | 0.0916
Epoch 181/300, Loss: 0.0741 | 0.0911
Epoch 182/300, Loss: 0.0743 | 0.0915
Epoch 183/300, Loss: 0.0733 | 0.0912
Epoch 184/300, Loss: 0.0740 | 0.0900
Epoch 185/300, Loss: 0.0749 | 0.0903
Epoch 186/300, Loss: 0.0739 | 0.0904
Epoch 187/300, Loss: 0.0732 | 0.0905
Epoch 188/300, Loss: 0.0743 | 0.0904
Epoch 189/300, Loss: 0.0732 | 0.0903
Epoch 190/300, Loss: 0.0736 | 0.0904
Epoch 191/300, Loss: 0.0732 | 0.0899
Epoch 192/300, Loss: 0.0739 | 0.0898
Epoch 193/300, Loss: 0.0735 | 0.0893
Epoch 194/300, Loss: 0.0732 | 0.0896
Epoch 195/300, Loss: 0.0727 | 0.0896
Epoch 196/300, Loss: 0.0733 | 0.0894
Epoch 197/300, Loss: 0.0727 | 0.0896
Epoch 198/300, Loss: 0.0730 | 0.0896
Epoch 199/300, Loss: 0.0733 | 0.0892
Epoch 200/300, Loss: 0.0734 | 0.0892
Epoch 201/300, Loss: 0.0734 | 0.0896
Epoch 202/300, Loss: 0.0725 | 0.0895
Epoch 203/300, Loss: 0.0725 | 0.0891
Epoch 204/300, Loss: 0.0728 | 0.0891
Epoch 205/300, Loss: 0.0721 | 0.0890
Epoch 206/300, Loss: 0.0725 | 0.0888
Epoch 207/300, Loss: 0.0723 | 0.0891
Epoch 208/300, Loss: 0.0723 | 0.0891
Epoch 209/300, Loss: 0.0728 | 0.0882
Epoch 210/300, Loss: 0.0722 | 0.0885
Epoch 211/300, Loss: 0.0717 | 0.0889
Epoch 212/300, Loss: 0.0723 | 0.0887
Epoch 213/300, Loss: 0.0722 | 0.0882
Epoch 214/300, Loss: 0.0720 | 0.0883
Epoch 215/300, Loss: 0.0724 | 0.0884
Epoch 216/300, Loss: 0.0721 | 0.0881
Epoch 217/300, Loss: 0.0720 | 0.0879
Epoch 218/300, Loss: 0.0716 | 0.0879
Epoch 219/300, Loss: 0.0718 | 0.0883
Epoch 220/300, Loss: 0.0719 | 0.0885
Epoch 221/300, Loss: 0.0712 | 0.0886
Epoch 222/300, Loss: 0.0713 | 0.0885
Epoch 223/300, Loss: 0.0714 | 0.0882
Epoch 224/300, Loss: 0.0721 | 0.0883
Epoch 225/300, Loss: 0.0713 | 0.0880
Epoch 226/300, Loss: 0.0716 | 0.0878
Epoch 227/300, Loss: 0.0717 | 0.0879
Epoch 228/300, Loss: 0.0711 | 0.0879
Epoch 229/300, Loss: 0.0717 | 0.0879
Epoch 230/300, Loss: 0.0706 | 0.0879
Epoch 231/300, Loss: 0.0711 | 0.0878
Epoch 232/300, Loss: 0.0719 | 0.0877
Epoch 233/300, Loss: 0.0716 | 0.0877
Epoch 234/300, Loss: 0.0718 | 0.0877
Epoch 235/300, Loss: 0.0705 | 0.0878
Epoch 236/300, Loss: 0.0712 | 0.0878
Epoch 237/300, Loss: 0.0706 | 0.0878
Epoch 238/300, Loss: 0.0715 | 0.0876
Epoch 239/300, Loss: 0.0715 | 0.0876
Epoch 240/300, Loss: 0.0711 | 0.0877
Epoch 241/300, Loss: 0.0706 | 0.0876
Epoch 242/300, Loss: 0.0710 | 0.0876
Epoch 243/300, Loss: 0.0711 | 0.0877
Epoch 244/300, Loss: 0.0708 | 0.0877
Epoch 245/300, Loss: 0.0705 | 0.0875
Epoch 246/300, Loss: 0.0706 | 0.0872
Epoch 247/300, Loss: 0.0701 | 0.0873
Epoch 248/300, Loss: 0.0710 | 0.0870
Epoch 249/300, Loss: 0.0707 | 0.0871
Epoch 250/300, Loss: 0.0709 | 0.0872
Epoch 251/300, Loss: 0.0707 | 0.0874
Epoch 252/300, Loss: 0.0704 | 0.0874
Epoch 253/300, Loss: 0.0711 | 0.0873
Epoch 254/300, Loss: 0.0708 | 0.0871
Epoch 255/300, Loss: 0.0706 | 0.0871
Epoch 256/300, Loss: 0.0706 | 0.0873
Epoch 257/300, Loss: 0.0700 | 0.0874
Epoch 258/300, Loss: 0.0705 | 0.0873
Epoch 259/300, Loss: 0.0709 | 0.0870
Epoch 260/300, Loss: 0.0702 | 0.0868
Epoch 261/300, Loss: 0.0706 | 0.0867
Epoch 262/300, Loss: 0.0705 | 0.0866
Epoch 263/300, Loss: 0.0703 | 0.0866
Epoch 264/300, Loss: 0.0705 | 0.0869
Epoch 265/300, Loss: 0.0701 | 0.0870
Epoch 266/300, Loss: 0.0704 | 0.0869
Epoch 267/300, Loss: 0.0703 | 0.0868
Epoch 268/300, Loss: 0.0703 | 0.0866
Epoch 269/300, Loss: 0.0704 | 0.0866
Epoch 270/300, Loss: 0.0701 | 0.0866
Epoch 271/300, Loss: 0.0701 | 0.0867
Epoch 272/300, Loss: 0.0696 | 0.0868
Epoch 273/300, Loss: 0.0705 | 0.0867
Epoch 274/300, Loss: 0.0701 | 0.0865
Epoch 275/300, Loss: 0.0699 | 0.0863
Epoch 276/300, Loss: 0.0701 | 0.0863
Epoch 277/300, Loss: 0.0700 | 0.0865
Epoch 278/300, Loss: 0.0700 | 0.0864
Epoch 279/300, Loss: 0.0697 | 0.0863
Epoch 280/300, Loss: 0.0702 | 0.0863
Epoch 281/300, Loss: 0.0700 | 0.0864
Epoch 282/300, Loss: 0.0695 | 0.0864
Epoch 283/300, Loss: 0.0704 | 0.0864
Epoch 284/300, Loss: 0.0697 | 0.0864
Epoch 285/300, Loss: 0.0698 | 0.0864
Epoch 286/300, Loss: 0.0701 | 0.0864
Epoch 287/300, Loss: 0.0693 | 0.0863
Epoch 288/300, Loss: 0.0699 | 0.0863
Epoch 289/300, Loss: 0.0704 | 0.0864
Epoch 290/300, Loss: 0.0697 | 0.0863
Epoch 291/300, Loss: 0.0699 | 0.0863
Epoch 292/300, Loss: 0.0699 | 0.0863
Epoch 293/300, Loss: 0.0699 | 0.0862
Epoch 294/300, Loss: 0.0702 | 0.0863
Epoch 295/300, Loss: 0.0700 | 0.0862
Epoch 296/300, Loss: 0.0699 | 0.0863
Epoch 297/300, Loss: 0.0695 | 0.0862
Epoch 298/300, Loss: 0.0698 | 0.0862
Epoch 299/300, Loss: 0.0692 | 0.0861
Epoch 300/300, Loss: 0.0694 | 0.0860
Runtime (seconds): 81.30850577354431
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 588.2393855529372
RMSE: 24.253646850585938
MAE: 24.253646850585938
R-squared: nan
[199.03635]
