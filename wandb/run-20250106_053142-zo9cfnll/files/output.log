[32m[I 2025-01-06 05:31:44,190][0m A new study created in memory with name: no-name-0ed2ce47-4f62-4fa2-b2b4-dfb647228e1a[0m
[32m[I 2025-01-06 05:36:49,160][0m Trial 0 finished with value: 0.7236548494112048 and parameters: {'observation_period_num': 235, 'train_rates': 0.732716623691714, 'learning_rate': 5.567673815072241e-05, 'batch_size': 193, 'step_size': 4, 'gamma': 0.8292040457269503}. Best is trial 0 with value: 0.7236548494112048.[0m
[32m[I 2025-01-06 05:38:29,174][0m Trial 1 finished with value: 1.7926444508308588 and parameters: {'observation_period_num': 89, 'train_rates': 0.7298526860922832, 'learning_rate': 3.6363363299862074e-06, 'batch_size': 234, 'step_size': 1, 'gamma': 0.9182073328137358}. Best is trial 0 with value: 0.7236548494112048.[0m
[32m[I 2025-01-06 05:42:03,492][0m Trial 2 finished with value: 0.6871704630149428 and parameters: {'observation_period_num': 163, 'train_rates': 0.7690390653273412, 'learning_rate': 0.0006821606287674172, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8923347548926189}. Best is trial 2 with value: 0.6871704630149428.[0m
[32m[I 2025-01-06 05:47:23,980][0m Trial 3 finished with value: 0.5394124166194876 and parameters: {'observation_period_num': 242, 'train_rates': 0.7572308003670855, 'learning_rate': 0.00025448678788825274, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9785606779942907}. Best is trial 3 with value: 0.5394124166194876.[0m
[32m[I 2025-01-06 05:49:52,611][0m Trial 4 finished with value: 0.683317418166455 and parameters: {'observation_period_num': 54, 'train_rates': 0.7947498203237122, 'learning_rate': 1.2336673034951793e-06, 'batch_size': 25, 'step_size': 12, 'gamma': 0.9109267792239664}. Best is trial 3 with value: 0.5394124166194876.[0m
[32m[I 2025-01-06 05:51:46,548][0m Trial 5 finished with value: 0.33704125242573874 and parameters: {'observation_period_num': 70, 'train_rates': 0.8300825039245943, 'learning_rate': 0.00011835667829932029, 'batch_size': 35, 'step_size': 5, 'gamma': 0.953680333407944}. Best is trial 5 with value: 0.33704125242573874.[0m
[32m[I 2025-01-06 05:54:31,461][0m Trial 6 finished with value: 1.1888024792826848 and parameters: {'observation_period_num': 147, 'train_rates': 0.6399461352453631, 'learning_rate': 1.2792699626670903e-05, 'batch_size': 100, 'step_size': 11, 'gamma': 0.9351067442455088}. Best is trial 5 with value: 0.33704125242573874.[0m
[32m[I 2025-01-06 05:56:57,410][0m Trial 7 finished with value: 0.2353386700153351 and parameters: {'observation_period_num': 109, 'train_rates': 0.8770552606503434, 'learning_rate': 2.9131574067266345e-05, 'batch_size': 72, 'step_size': 14, 'gamma': 0.7803485771684777}. Best is trial 7 with value: 0.2353386700153351.[0m
[32m[I 2025-01-06 05:58:43,542][0m Trial 8 finished with value: 0.7561497107688627 and parameters: {'observation_period_num': 89, 'train_rates': 0.7980334801462574, 'learning_rate': 1.4920081547359691e-05, 'batch_size': 219, 'step_size': 2, 'gamma': 0.9313975270716317}. Best is trial 7 with value: 0.2353386700153351.[0m
[32m[I 2025-01-06 06:02:31,450][0m Trial 9 finished with value: 0.5143721475022895 and parameters: {'observation_period_num': 163, 'train_rates': 0.7954027827844441, 'learning_rate': 5.699391614414971e-06, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9354140517428566}. Best is trial 7 with value: 0.2353386700153351.[0m
[32m[I 2025-01-06 06:03:10,804][0m Trial 10 finished with value: 0.21194184414900033 and parameters: {'observation_period_num': 12, 'train_rates': 0.9467476849278222, 'learning_rate': 3.5170053322513184e-05, 'batch_size': 104, 'step_size': 15, 'gamma': 0.7635920958156474}. Best is trial 10 with value: 0.21194184414900033.[0m
[32m[I 2025-01-06 06:03:53,274][0m Trial 11 finished with value: 0.2018996880256704 and parameters: {'observation_period_num': 10, 'train_rates': 0.9619898835415407, 'learning_rate': 4.008638848765695e-05, 'batch_size': 99, 'step_size': 15, 'gamma': 0.7518694306801915}. Best is trial 11 with value: 0.2018996880256704.[0m
[32m[I 2025-01-06 06:04:28,975][0m Trial 12 finished with value: 0.10130122303962708 and parameters: {'observation_period_num': 12, 'train_rates': 0.9801423748128826, 'learning_rate': 5.833347357027014e-05, 'batch_size': 124, 'step_size': 15, 'gamma': 0.7544422631828961}. Best is trial 12 with value: 0.10130122303962708.[0m
[32m[I 2025-01-06 06:05:00,231][0m Trial 13 finished with value: 0.08009106665849686 and parameters: {'observation_period_num': 7, 'train_rates': 0.981828787621757, 'learning_rate': 0.00012749640930153906, 'batch_size': 139, 'step_size': 11, 'gamma': 0.8100301230377495}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:06:01,395][0m Trial 14 finished with value: 0.16557861150296266 and parameters: {'observation_period_num': 46, 'train_rates': 0.9169984334136494, 'learning_rate': 0.00018135195116724355, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8164451972355816}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:06:41,179][0m Trial 15 finished with value: 0.24646586179733276 and parameters: {'observation_period_num': 25, 'train_rates': 0.9838502221454664, 'learning_rate': 0.0007841398373567765, 'batch_size': 178, 'step_size': 13, 'gamma': 0.8084067698813304}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:07:32,848][0m Trial 16 finished with value: 0.17813280820846558 and parameters: {'observation_period_num': 38, 'train_rates': 0.8870336745031939, 'learning_rate': 0.0003419553974992166, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8500089317329763}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:12:58,366][0m Trial 17 finished with value: 0.12494642287492752 and parameters: {'observation_period_num': 209, 'train_rates': 0.9868746379369705, 'learning_rate': 9.07556949609711e-05, 'batch_size': 133, 'step_size': 9, 'gamma': 0.7902310932799532}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:14:46,030][0m Trial 18 finished with value: 0.15382102504372597 and parameters: {'observation_period_num': 75, 'train_rates': 0.9196376488229295, 'learning_rate': 9.011864466113306e-05, 'batch_size': 71, 'step_size': 13, 'gamma': 0.8377370543766532}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:17:38,723][0m Trial 19 finished with value: 0.4208027454688377 and parameters: {'observation_period_num': 129, 'train_rates': 0.8627176875899049, 'learning_rate': 1.6578435851248755e-05, 'batch_size': 190, 'step_size': 13, 'gamma': 0.8695669041500214}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:17:59,395][0m Trial 20 finished with value: 0.9976877120824961 and parameters: {'observation_period_num': 10, 'train_rates': 0.6027283251413473, 'learning_rate': 0.0004528697922837923, 'batch_size': 163, 'step_size': 6, 'gamma': 0.7909396971981191}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:23:24,294][0m Trial 21 finished with value: 0.1272609829902649 and parameters: {'observation_period_num': 209, 'train_rates': 0.9888398552535236, 'learning_rate': 8.75041166775673e-05, 'batch_size': 128, 'step_size': 9, 'gamma': 0.7852465849763836}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:28:14,434][0m Trial 22 finished with value: 0.18195131232201214 and parameters: {'observation_period_num': 194, 'train_rates': 0.923114007232762, 'learning_rate': 0.00014802769577727822, 'batch_size': 137, 'step_size': 9, 'gamma': 0.7687418163049832}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:33:15,875][0m Trial 23 finished with value: 0.2013115052665983 and parameters: {'observation_period_num': 198, 'train_rates': 0.9542739942008156, 'learning_rate': 6.310970183238525e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8019243500472587}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:36:08,138][0m Trial 24 finished with value: 0.10150864720344543 and parameters: {'observation_period_num': 120, 'train_rates': 0.9899518737794049, 'learning_rate': 0.00021728313944797704, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8601989072700706}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:38:48,822][0m Trial 25 finished with value: 0.17354369863688227 and parameters: {'observation_period_num': 115, 'train_rates': 0.94168711241798, 'learning_rate': 0.0002550177717745209, 'batch_size': 76, 'step_size': 7, 'gamma': 0.8636752324922419}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:40:02,079][0m Trial 26 finished with value: 0.4310953300446272 and parameters: {'observation_period_num': 30, 'train_rates': 0.901622267990413, 'learning_rate': 0.0005051532877419038, 'batch_size': 54, 'step_size': 10, 'gamma': 0.8911557050769034}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:41:19,213][0m Trial 27 finished with value: 0.2693665667196338 and parameters: {'observation_period_num': 62, 'train_rates': 0.8462383575889967, 'learning_rate': 0.0002228127974206614, 'batch_size': 100, 'step_size': 14, 'gamma': 0.8252970302913559}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:42:04,590][0m Trial 28 finished with value: 0.6376580766951426 and parameters: {'observation_period_num': 28, 'train_rates': 0.6801120230028214, 'learning_rate': 0.0001450052353944259, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8559492427865352}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:44:13,958][0m Trial 29 finished with value: 0.2584076225757599 and parameters: {'observation_period_num': 95, 'train_rates': 0.9676217285693938, 'learning_rate': 5.4108888763090105e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8857873811804193}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:45:21,460][0m Trial 30 finished with value: 0.2384992769578608 and parameters: {'observation_period_num': 51, 'train_rates': 0.9295417930627689, 'learning_rate': 2.5228552926993416e-05, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8390789459648356}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:51:11,997][0m Trial 31 finished with value: 0.15540170669555664 and parameters: {'observation_period_num': 224, 'train_rates': 0.9876619086351364, 'learning_rate': 8.047641020435409e-05, 'batch_size': 137, 'step_size': 9, 'gamma': 0.751520698075236}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 06:57:50,470][0m Trial 32 finished with value: 0.22062334418296814 and parameters: {'observation_period_num': 251, 'train_rates': 0.9663349286214749, 'learning_rate': 5.176567231395042e-05, 'batch_size': 119, 'step_size': 8, 'gamma': 0.797730562582909}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:02:14,336][0m Trial 33 finished with value: 0.17306371864633283 and parameters: {'observation_period_num': 180, 'train_rates': 0.9008205662833054, 'learning_rate': 0.00011054792894485708, 'batch_size': 53, 'step_size': 8, 'gamma': 0.7760046466489823}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:05:50,166][0m Trial 34 finished with value: 0.1185092031955719 and parameters: {'observation_period_num': 146, 'train_rates': 0.9896109467336827, 'learning_rate': 0.0003182673648313992, 'batch_size': 253, 'step_size': 10, 'gamma': 0.8293115454274257}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:09:08,259][0m Trial 35 finished with value: 0.3606831729412079 and parameters: {'observation_period_num': 138, 'train_rates': 0.9468868826286422, 'learning_rate': 0.0009489952297880431, 'batch_size': 251, 'step_size': 10, 'gamma': 0.8210631505216011}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:12:10,666][0m Trial 36 finished with value: 0.6648019190132618 and parameters: {'observation_period_num': 154, 'train_rates': 0.714185406887508, 'learning_rate': 0.00039005199643762364, 'batch_size': 219, 'step_size': 12, 'gamma': 0.8802278249045536}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:14:46,059][0m Trial 37 finished with value: 0.19619666039943695 and parameters: {'observation_period_num': 115, 'train_rates': 0.959729315012033, 'learning_rate': 0.00029497148946351707, 'batch_size': 222, 'step_size': 6, 'gamma': 0.9071995184828312}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:19:00,782][0m Trial 38 finished with value: 1.2312427759170532 and parameters: {'observation_period_num': 177, 'train_rates': 0.9333604578870345, 'learning_rate': 1.0203108021423558e-06, 'batch_size': 199, 'step_size': 14, 'gamma': 0.8389821098089253}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:20:42,820][0m Trial 39 finished with value: 0.1893327534198761 and parameters: {'observation_period_num': 75, 'train_rates': 0.970878262633275, 'learning_rate': 0.0006658694079753441, 'batch_size': 241, 'step_size': 12, 'gamma': 0.8106422092128013}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:22:51,993][0m Trial 40 finished with value: 0.26793695491540503 and parameters: {'observation_period_num': 105, 'train_rates': 0.82369208100928, 'learning_rate': 0.00018026491720172795, 'batch_size': 197, 'step_size': 11, 'gamma': 0.8502186192153094}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:28:22,937][0m Trial 41 finished with value: 0.12062592059373856 and parameters: {'observation_period_num': 210, 'train_rates': 0.9898128377975296, 'learning_rate': 0.00013302315060838513, 'batch_size': 89, 'step_size': 9, 'gamma': 0.7951398181048023}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:34:28,109][0m Trial 42 finished with value: 0.12863880395889282 and parameters: {'observation_period_num': 232, 'train_rates': 0.9719688467001031, 'learning_rate': 0.0001286724202495726, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7627773426372393}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:38:49,733][0m Trial 43 finished with value: 0.09668146073818207 and parameters: {'observation_period_num': 169, 'train_rates': 0.9894123084014187, 'learning_rate': 0.00020902456999546411, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8275125413704723}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:43:21,903][0m Trial 44 finished with value: 0.18191083531771132 and parameters: {'observation_period_num': 159, 'train_rates': 0.9041378492431635, 'learning_rate': 0.00023293593141405583, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8297045436800002}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:47:46,266][0m Trial 45 finished with value: 0.9910124325923783 and parameters: {'observation_period_num': 174, 'train_rates': 0.9502397425962952, 'learning_rate': 0.0005467617818628233, 'batch_size': 50, 'step_size': 6, 'gamma': 0.9863860642263589}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:50:37,456][0m Trial 46 finished with value: 0.6101151494228322 and parameters: {'observation_period_num': 134, 'train_rates': 0.7553570355108248, 'learning_rate': 0.00033405083911969724, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8570717856361754}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:54:16,316][0m Trial 47 finished with value: 0.1450259868013725 and parameters: {'observation_period_num': 142, 'train_rates': 0.9368487654589689, 'learning_rate': 0.00017078344678144987, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8778703113775657}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:55:21,746][0m Trial 48 finished with value: 0.5510710014237298 and parameters: {'observation_period_num': 19, 'train_rates': 0.9726419973928788, 'learning_rate': 8.156403086005064e-06, 'batch_size': 63, 'step_size': 3, 'gamma': 0.833195487821357}. Best is trial 13 with value: 0.08009106665849686.[0m
[32m[I 2025-01-06 07:56:57,872][0m Trial 49 finished with value: 0.19965179306758363 and parameters: {'observation_period_num': 42, 'train_rates': 0.8787478041823064, 'learning_rate': 2.325558965573814e-05, 'batch_size': 39, 'step_size': 15, 'gamma': 0.8129401830905895}. Best is trial 13 with value: 0.08009106665849686.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 1.0603 | 1.5720
Epoch 2/300, Loss: 0.5950 | 0.8596
Epoch 3/300, Loss: 0.4653 | 0.7197
Epoch 4/300, Loss: 0.4053 | 0.5182
Epoch 5/300, Loss: 0.4192 | 0.5832
Epoch 6/300, Loss: 0.3688 | 0.5139
Epoch 7/300, Loss: 0.3869 | 0.4248
Epoch 8/300, Loss: 0.3276 | 0.4602
Epoch 9/300, Loss: 0.3271 | 0.4317
Epoch 10/300, Loss: 0.2954 | 0.3984
Epoch 11/300, Loss: 0.3241 | 0.3726
Epoch 12/300, Loss: 0.2880 | 0.4199
Epoch 13/300, Loss: 0.2521 | 0.3315
Epoch 14/300, Loss: 0.2261 | 0.3525
Epoch 15/300, Loss: 0.2133 | 0.2825
Epoch 16/300, Loss: 0.2109 | 0.2932
Epoch 17/300, Loss: 0.2033 | 0.2613
Epoch 18/300, Loss: 0.2027 | 0.2566
Epoch 19/300, Loss: 0.1985 | 0.2519
Epoch 20/300, Loss: 0.2074 | 0.2522
Epoch 21/300, Loss: 0.2205 | 0.2679
Epoch 22/300, Loss: 0.1983 | 0.2322
Epoch 23/300, Loss: 0.1909 | 0.2317
Epoch 24/300, Loss: 0.1700 | 0.2216
Epoch 25/300, Loss: 0.1641 | 0.2169
Epoch 26/300, Loss: 0.1588 | 0.2106
Epoch 27/300, Loss: 0.1560 | 0.2046
Epoch 28/300, Loss: 0.1546 | 0.2009
Epoch 29/300, Loss: 0.1536 | 0.1955
Epoch 30/300, Loss: 0.1525 | 0.1948
Epoch 31/300, Loss: 0.1498 | 0.1888
Epoch 32/300, Loss: 0.1493 | 0.1890
Epoch 33/300, Loss: 0.1463 | 0.1829
Epoch 34/300, Loss: 0.1450 | 0.1824
Epoch 35/300, Loss: 0.1424 | 0.1779
Epoch 36/300, Loss: 0.1410 | 0.1768
Epoch 37/300, Loss: 0.1407 | 0.1731
Epoch 38/300, Loss: 0.1391 | 0.1718
Epoch 39/300, Loss: 0.1376 | 0.1689
Epoch 40/300, Loss: 0.1371 | 0.1678
Epoch 41/300, Loss: 0.1362 | 0.1648
Epoch 42/300, Loss: 0.1355 | 0.1632
Epoch 43/300, Loss: 0.1338 | 0.1613
Epoch 44/300, Loss: 0.1332 | 0.1600
Epoch 45/300, Loss: 0.1328 | 0.1581
Epoch 46/300, Loss: 0.1316 | 0.1572
Epoch 47/300, Loss: 0.1313 | 0.1559
Epoch 48/300, Loss: 0.1307 | 0.1548
Epoch 49/300, Loss: 0.1303 | 0.1532
Epoch 50/300, Loss: 0.1299 | 0.1521
Epoch 51/300, Loss: 0.1287 | 0.1507
Epoch 52/300, Loss: 0.1286 | 0.1495
Epoch 53/300, Loss: 0.1279 | 0.1481
Epoch 54/300, Loss: 0.1278 | 0.1475
Epoch 55/300, Loss: 0.1268 | 0.1465
Epoch 56/300, Loss: 0.1265 | 0.1458
Epoch 57/300, Loss: 0.1270 | 0.1446
Epoch 58/300, Loss: 0.1261 | 0.1440
Epoch 59/300, Loss: 0.1253 | 0.1430
Epoch 60/300, Loss: 0.1251 | 0.1421
Epoch 61/300, Loss: 0.1253 | 0.1411
Epoch 62/300, Loss: 0.1245 | 0.1402
Epoch 63/300, Loss: 0.1237 | 0.1394
Epoch 64/300, Loss: 0.1235 | 0.1390
Epoch 65/300, Loss: 0.1229 | 0.1384
Epoch 66/300, Loss: 0.1228 | 0.1375
Epoch 67/300, Loss: 0.1228 | 0.1367
Epoch 68/300, Loss: 0.1225 | 0.1366
Epoch 69/300, Loss: 0.1221 | 0.1362
Epoch 70/300, Loss: 0.1214 | 0.1354
Epoch 71/300, Loss: 0.1216 | 0.1348
Epoch 72/300, Loss: 0.1221 | 0.1345
Epoch 73/300, Loss: 0.1212 | 0.1341
Epoch 74/300, Loss: 0.1206 | 0.1334
Epoch 75/300, Loss: 0.1208 | 0.1329
Epoch 76/300, Loss: 0.1204 | 0.1326
Epoch 77/300, Loss: 0.1204 | 0.1320
Epoch 78/300, Loss: 0.1202 | 0.1318
Epoch 79/300, Loss: 0.1199 | 0.1315
Epoch 80/300, Loss: 0.1194 | 0.1309
Epoch 81/300, Loss: 0.1191 | 0.1303
Epoch 82/300, Loss: 0.1190 | 0.1300
Epoch 83/300, Loss: 0.1185 | 0.1295
Epoch 84/300, Loss: 0.1189 | 0.1292
Epoch 85/300, Loss: 0.1184 | 0.1291
Epoch 86/300, Loss: 0.1187 | 0.1289
Epoch 87/300, Loss: 0.1182 | 0.1288
Epoch 88/300, Loss: 0.1180 | 0.1285
Epoch 89/300, Loss: 0.1186 | 0.1282
Epoch 90/300, Loss: 0.1182 | 0.1279
Epoch 91/300, Loss: 0.1167 | 0.1276
Epoch 92/300, Loss: 0.1179 | 0.1274
Epoch 93/300, Loss: 0.1174 | 0.1272
Epoch 94/300, Loss: 0.1174 | 0.1270
Epoch 95/300, Loss: 0.1176 | 0.1267
Epoch 96/300, Loss: 0.1167 | 0.1265
Epoch 97/300, Loss: 0.1170 | 0.1263
Epoch 98/300, Loss: 0.1167 | 0.1262
Epoch 99/300, Loss: 0.1168 | 0.1262
Epoch 100/300, Loss: 0.1163 | 0.1260
Epoch 101/300, Loss: 0.1171 | 0.1257
Epoch 102/300, Loss: 0.1164 | 0.1254
Epoch 103/300, Loss: 0.1171 | 0.1251
Epoch 104/300, Loss: 0.1162 | 0.1250
Epoch 105/300, Loss: 0.1167 | 0.1248
Epoch 106/300, Loss: 0.1166 | 0.1246
Epoch 107/300, Loss: 0.1161 | 0.1245
Epoch 108/300, Loss: 0.1156 | 0.1243
Epoch 109/300, Loss: 0.1156 | 0.1243
Epoch 110/300, Loss: 0.1162 | 0.1241
Epoch 111/300, Loss: 0.1162 | 0.1240
Epoch 112/300, Loss: 0.1158 | 0.1239
Epoch 113/300, Loss: 0.1158 | 0.1237
Epoch 114/300, Loss: 0.1156 | 0.1236
Epoch 115/300, Loss: 0.1149 | 0.1235
Epoch 116/300, Loss: 0.1158 | 0.1234
Epoch 117/300, Loss: 0.1154 | 0.1233
Epoch 118/300, Loss: 0.1154 | 0.1233
Epoch 119/300, Loss: 0.1156 | 0.1232
Epoch 120/300, Loss: 0.1153 | 0.1231
Epoch 121/300, Loss: 0.1152 | 0.1230
Epoch 122/300, Loss: 0.1153 | 0.1229
Epoch 123/300, Loss: 0.1149 | 0.1228
Epoch 124/300, Loss: 0.1152 | 0.1227
Epoch 125/300, Loss: 0.1153 | 0.1225
Epoch 126/300, Loss: 0.1146 | 0.1225
Epoch 127/300, Loss: 0.1155 | 0.1224
Epoch 128/300, Loss: 0.1155 | 0.1224
Epoch 129/300, Loss: 0.1147 | 0.1224
Epoch 130/300, Loss: 0.1147 | 0.1223
Epoch 131/300, Loss: 0.1145 | 0.1223
Epoch 132/300, Loss: 0.1155 | 0.1222
Epoch 133/300, Loss: 0.1146 | 0.1222
Epoch 134/300, Loss: 0.1148 | 0.1221
Epoch 135/300, Loss: 0.1146 | 0.1221
Epoch 136/300, Loss: 0.1146 | 0.1221
Epoch 137/300, Loss: 0.1147 | 0.1220
Epoch 138/300, Loss: 0.1144 | 0.1219
Epoch 139/300, Loss: 0.1147 | 0.1218
Epoch 140/300, Loss: 0.1146 | 0.1218
Epoch 141/300, Loss: 0.1149 | 0.1217
Epoch 142/300, Loss: 0.1151 | 0.1217
Epoch 143/300, Loss: 0.1145 | 0.1216
Epoch 144/300, Loss: 0.1149 | 0.1215
Epoch 145/300, Loss: 0.1142 | 0.1215
Epoch 146/300, Loss: 0.1141 | 0.1215
Epoch 147/300, Loss: 0.1146 | 0.1214
Epoch 148/300, Loss: 0.1138 | 0.1214
Epoch 149/300, Loss: 0.1145 | 0.1213
Epoch 150/300, Loss: 0.1138 | 0.1212
Epoch 151/300, Loss: 0.1140 | 0.1212
Epoch 152/300, Loss: 0.1140 | 0.1212
Epoch 153/300, Loss: 0.1143 | 0.1211
Epoch 154/300, Loss: 0.1137 | 0.1211
Epoch 155/300, Loss: 0.1143 | 0.1210
Epoch 156/300, Loss: 0.1143 | 0.1210
Epoch 157/300, Loss: 0.1140 | 0.1210
Epoch 158/300, Loss: 0.1149 | 0.1209
Epoch 159/300, Loss: 0.1145 | 0.1209
Epoch 160/300, Loss: 0.1142 | 0.1209
Epoch 161/300, Loss: 0.1144 | 0.1209
Epoch 162/300, Loss: 0.1149 | 0.1209
Epoch 163/300, Loss: 0.1140 | 0.1208
Epoch 164/300, Loss: 0.1141 | 0.1208
Epoch 165/300, Loss: 0.1138 | 0.1208
Epoch 166/300, Loss: 0.1143 | 0.1207
Epoch 167/300, Loss: 0.1136 | 0.1207
Epoch 168/300, Loss: 0.1142 | 0.1207
Epoch 169/300, Loss: 0.1140 | 0.1207
Epoch 170/300, Loss: 0.1138 | 0.1207
Epoch 171/300, Loss: 0.1134 | 0.1207
Epoch 172/300, Loss: 0.1137 | 0.1207
Epoch 173/300, Loss: 0.1141 | 0.1207
Epoch 174/300, Loss: 0.1135 | 0.1207
Epoch 175/300, Loss: 0.1140 | 0.1206
Epoch 176/300, Loss: 0.1139 | 0.1206
Epoch 177/300, Loss: 0.1136 | 0.1206
Epoch 178/300, Loss: 0.1139 | 0.1206
Epoch 179/300, Loss: 0.1144 | 0.1206
Epoch 180/300, Loss: 0.1136 | 0.1206
Epoch 181/300, Loss: 0.1134 | 0.1205
Epoch 182/300, Loss: 0.1142 | 0.1205
Epoch 183/300, Loss: 0.1138 | 0.1205
Epoch 184/300, Loss: 0.1141 | 0.1205
Epoch 185/300, Loss: 0.1138 | 0.1205
Epoch 186/300, Loss: 0.1140 | 0.1205
Epoch 187/300, Loss: 0.1141 | 0.1205
Epoch 188/300, Loss: 0.1138 | 0.1204
Epoch 189/300, Loss: 0.1142 | 0.1204
Epoch 190/300, Loss: 0.1143 | 0.1204
Epoch 191/300, Loss: 0.1143 | 0.1204
Epoch 192/300, Loss: 0.1139 | 0.1204
Epoch 193/300, Loss: 0.1136 | 0.1204
Epoch 194/300, Loss: 0.1133 | 0.1204
Epoch 195/300, Loss: 0.1138 | 0.1204
Epoch 196/300, Loss: 0.1142 | 0.1204
Epoch 197/300, Loss: 0.1139 | 0.1204
Epoch 198/300, Loss: 0.1144 | 0.1204
Epoch 199/300, Loss: 0.1141 | 0.1204
Epoch 200/300, Loss: 0.1136 | 0.1204
Epoch 201/300, Loss: 0.1137 | 0.1204
Epoch 202/300, Loss: 0.1140 | 0.1204
Epoch 203/300, Loss: 0.1140 | 0.1204
Epoch 204/300, Loss: 0.1140 | 0.1203
Epoch 205/300, Loss: 0.1135 | 0.1203
Epoch 206/300, Loss: 0.1142 | 0.1203
Epoch 207/300, Loss: 0.1138 | 0.1203
Epoch 208/300, Loss: 0.1142 | 0.1203
Epoch 209/300, Loss: 0.1133 | 0.1203
Epoch 210/300, Loss: 0.1135 | 0.1203
Epoch 211/300, Loss: 0.1137 | 0.1203
Epoch 212/300, Loss: 0.1139 | 0.1203
Epoch 213/300, Loss: 0.1136 | 0.1203
Epoch 214/300, Loss: 0.1139 | 0.1203
Epoch 215/300, Loss: 0.1135 | 0.1203
Epoch 216/300, Loss: 0.1132 | 0.1203
Epoch 217/300, Loss: 0.1138 | 0.1203
Epoch 218/300, Loss: 0.1135 | 0.1203
Epoch 219/300, Loss: 0.1143 | 0.1203
Epoch 220/300, Loss: 0.1139 | 0.1203
Epoch 221/300, Loss: 0.1137 | 0.1203
Epoch 222/300, Loss: 0.1136 | 0.1203
Epoch 223/300, Loss: 0.1142 | 0.1203
Epoch 224/300, Loss: 0.1137 | 0.1203
Epoch 225/300, Loss: 0.1140 | 0.1203
Epoch 226/300, Loss: 0.1139 | 0.1203
Epoch 227/300, Loss: 0.1142 | 0.1203
Epoch 228/300, Loss: 0.1142 | 0.1203
Epoch 229/300, Loss: 0.1142 | 0.1203
Epoch 230/300, Loss: 0.1136 | 0.1203
Epoch 231/300, Loss: 0.1138 | 0.1203
Epoch 232/300, Loss: 0.1141 | 0.1203
Epoch 233/300, Loss: 0.1137 | 0.1203
Epoch 234/300, Loss: 0.1140 | 0.1203
Epoch 235/300, Loss: 0.1137 | 0.1203
Epoch 236/300, Loss: 0.1136 | 0.1203
Epoch 237/300, Loss: 0.1135 | 0.1203
Epoch 238/300, Loss: 0.1139 | 0.1203
Epoch 239/300, Loss: 0.1135 | 0.1203
Epoch 240/300, Loss: 0.1138 | 0.1203
Epoch 241/300, Loss: 0.1139 | 0.1203
Epoch 242/300, Loss: 0.1133 | 0.1203
Epoch 243/300, Loss: 0.1135 | 0.1203
Epoch 244/300, Loss: 0.1137 | 0.1202
Epoch 245/300, Loss: 0.1135 | 0.1202
Epoch 246/300, Loss: 0.1138 | 0.1202
Epoch 247/300, Loss: 0.1142 | 0.1202
Epoch 248/300, Loss: 0.1142 | 0.1202
Epoch 249/300, Loss: 0.1146 | 0.1202
Epoch 250/300, Loss: 0.1135 | 0.1202
Epoch 251/300, Loss: 0.1135 | 0.1202
Epoch 252/300, Loss: 0.1136 | 0.1202
Epoch 253/300, Loss: 0.1137 | 0.1202
Epoch 254/300, Loss: 0.1136 | 0.1202
Epoch 255/300, Loss: 0.1135 | 0.1202
Epoch 256/300, Loss: 0.1143 | 0.1202
Epoch 257/300, Loss: 0.1137 | 0.1202
Epoch 258/300, Loss: 0.1139 | 0.1202
Epoch 259/300, Loss: 0.1143 | 0.1202
Epoch 260/300, Loss: 0.1139 | 0.1202
Epoch 261/300, Loss: 0.1136 | 0.1202
Epoch 262/300, Loss: 0.1139 | 0.1202
Epoch 263/300, Loss: 0.1138 | 0.1202
Epoch 264/300, Loss: 0.1136 | 0.1202
Epoch 265/300, Loss: 0.1142 | 0.1202
Epoch 266/300, Loss: 0.1134 | 0.1202
Epoch 267/300, Loss: 0.1137 | 0.1202
Epoch 268/300, Loss: 0.1138 | 0.1202
Epoch 269/300, Loss: 0.1134 | 0.1202
Epoch 270/300, Loss: 0.1136 | 0.1202
Epoch 271/300, Loss: 0.1134 | 0.1202
Epoch 272/300, Loss: 0.1132 | 0.1202
Epoch 273/300, Loss: 0.1141 | 0.1202
Epoch 274/300, Loss: 0.1137 | 0.1202
Epoch 275/300, Loss: 0.1135 | 0.1202
Epoch 276/300, Loss: 0.1133 | 0.1202
Epoch 277/300, Loss: 0.1134 | 0.1202
Epoch 278/300, Loss: 0.1139 | 0.1202
Epoch 279/300, Loss: 0.1134 | 0.1202
Epoch 280/300, Loss: 0.1139 | 0.1202
Epoch 281/300, Loss: 0.1143 | 0.1202
Epoch 282/300, Loss: 0.1138 | 0.1202
Epoch 283/300, Loss: 0.1142 | 0.1202
Early stopping
Runtime (seconds): 85.88078212738037
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 538.991316691041
RMSE: 23.2161865234375
MAE: 23.2161865234375
R-squared: nan
[211.7138]
