ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 01:19:03,364][0m A new study created in memory with name: no-name-ec37c4e7-bc30-4f31-a366-e7273938b705[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-04 01:20:03,064][0m Trial 0 finished with value: 0.09327546241199089 and parameters: {'observation_period_num': 84, 'train_rates': 0.891698876466372, 'learning_rate': 0.0005579538228985599, 'batch_size': 95, 'step_size': 5, 'gamma': 0.918362621817816}. Best is trial 0 with value: 0.09327546241199089.[0m
[32m[I 2025-01-04 01:21:22,814][0m Trial 1 finished with value: 0.33452453943357313 and parameters: {'observation_period_num': 57, 'train_rates': 0.6983293139661144, 'learning_rate': 4.0734440528503745e-06, 'batch_size': 58, 'step_size': 10, 'gamma': 0.7743400553276774}. Best is trial 0 with value: 0.09327546241199089.[0m
[32m[I 2025-01-04 01:24:21,998][0m Trial 2 finished with value: 0.12753472460882506 and parameters: {'observation_period_num': 42, 'train_rates': 0.7176272857338228, 'learning_rate': 0.00010233140789534826, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8829800097132873}. Best is trial 0 with value: 0.09327546241199089.[0m
[32m[I 2025-01-04 01:25:48,365][0m Trial 3 finished with value: 0.03466440762867493 and parameters: {'observation_period_num': 25, 'train_rates': 0.9082335204669713, 'learning_rate': 0.0001999213805090657, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8097975753154978}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:26:16,253][0m Trial 4 finished with value: 0.2977877925398082 and parameters: {'observation_period_num': 25, 'train_rates': 0.7690003128836835, 'learning_rate': 1.1146540757255363e-05, 'batch_size': 190, 'step_size': 7, 'gamma': 0.7831322149830953}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:26:56,612][0m Trial 5 finished with value: 0.22367170367103356 and parameters: {'observation_period_num': 244, 'train_rates': 0.8465431291323502, 'learning_rate': 2.551134740948063e-05, 'batch_size': 129, 'step_size': 15, 'gamma': 0.9689084504333954}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:27:48,083][0m Trial 6 finished with value: 0.08234412171305182 and parameters: {'observation_period_num': 133, 'train_rates': 0.9338180566181544, 'learning_rate': 0.0001636540252349573, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9858648808903958}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:28:12,042][0m Trial 7 finished with value: 0.35775157023853343 and parameters: {'observation_period_num': 137, 'train_rates': 0.7047742725680076, 'learning_rate': 1.8155546209688337e-05, 'batch_size': 216, 'step_size': 14, 'gamma': 0.8772546742077751}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:28:54,454][0m Trial 8 finished with value: 0.08060732618363622 and parameters: {'observation_period_num': 43, 'train_rates': 0.7105703089868126, 'learning_rate': 5.341189834453041e-05, 'batch_size': 118, 'step_size': 8, 'gamma': 0.8693905129768346}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:30:04,361][0m Trial 9 finished with value: 0.11125557801940224 and parameters: {'observation_period_num': 95, 'train_rates': 0.946217038151224, 'learning_rate': 0.0007212456359005903, 'batch_size': 84, 'step_size': 11, 'gamma': 0.9392348773138052}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:30:32,107][0m Trial 10 finished with value: 2.4416022794973933 and parameters: {'observation_period_num': 206, 'train_rates': 0.6311018999332157, 'learning_rate': 1.1355126608563003e-06, 'batch_size': 162, 'step_size': 2, 'gamma': 0.8307117287946223}. Best is trial 3 with value: 0.03466440762867493.[0m
[32m[I 2025-01-04 01:34:27,859][0m Trial 11 finished with value: 0.015044975244710523 and parameters: {'observation_period_num': 21, 'train_rates': 0.9897206092042286, 'learning_rate': 9.483941512009565e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.8181973876236445}. Best is trial 11 with value: 0.015044975244710523.[0m
[32m[I 2025-01-04 01:37:23,483][0m Trial 12 finished with value: 0.014956396073102951 and parameters: {'observation_period_num': 10, 'train_rates': 0.9893632335324659, 'learning_rate': 0.00017030802740725277, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8167557500582184}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:42:36,188][0m Trial 13 finished with value: 0.015565812813513207 and parameters: {'observation_period_num': 6, 'train_rates': 0.9889869348785609, 'learning_rate': 0.000380163744691029, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8321340074853412}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:43:02,992][0m Trial 14 finished with value: 0.13006441295146942 and parameters: {'observation_period_num': 86, 'train_rates': 0.9842031425109872, 'learning_rate': 6.0010632811561144e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.8077060160310188}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:45:04,233][0m Trial 15 finished with value: 0.14817910037677864 and parameters: {'observation_period_num': 167, 'train_rates': 0.8542671383902821, 'learning_rate': 0.0002616981794668872, 'batch_size': 42, 'step_size': 12, 'gamma': 0.8460485615597131}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:46:38,926][0m Trial 16 finished with value: 0.042555385628311904 and parameters: {'observation_period_num': 5, 'train_rates': 0.8355972730878076, 'learning_rate': 0.00010374414446136346, 'batch_size': 56, 'step_size': 4, 'gamma': 0.7536501235103338}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:52:09,863][0m Trial 17 finished with value: 0.07208568426643232 and parameters: {'observation_period_num': 70, 'train_rates': 0.9554547885492871, 'learning_rate': 1.1110173997304727e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.7991118777007143}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:52:44,891][0m Trial 18 finished with value: 0.12618497258013361 and parameters: {'observation_period_num': 110, 'train_rates': 0.7810196331163466, 'learning_rate': 4.961215646452463e-05, 'batch_size': 150, 'step_size': 10, 'gamma': 0.855624322759906}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:53:53,969][0m Trial 19 finished with value: 0.20748201785962792 and parameters: {'observation_period_num': 181, 'train_rates': 0.8862328544294809, 'learning_rate': 0.00011177444705888022, 'batch_size': 78, 'step_size': 1, 'gamma': 0.899630921945782}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 01:55:41,278][0m Trial 20 finished with value: 0.17158351403424962 and parameters: {'observation_period_num': 60, 'train_rates': 0.6071051290007906, 'learning_rate': 0.0009786283704912256, 'batch_size': 39, 'step_size': 14, 'gamma': 0.754347275896248}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:00:52,277][0m Trial 21 finished with value: 0.027066886611282825 and parameters: {'observation_period_num': 11, 'train_rates': 0.9865082958873241, 'learning_rate': 0.00032448700662558765, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8308276846455656}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:03:11,856][0m Trial 22 finished with value: 0.03367968648672104 and parameters: {'observation_period_num': 30, 'train_rates': 0.9889595926600812, 'learning_rate': 0.0004540851110141424, 'batch_size': 43, 'step_size': 12, 'gamma': 0.83338997916672}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:08:43,312][0m Trial 23 finished with value: 0.0254913975413029 and parameters: {'observation_period_num': 8, 'train_rates': 0.9252031851171226, 'learning_rate': 0.00026170804410227216, 'batch_size': 17, 'step_size': 9, 'gamma': 0.816517395482853}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:10:56,123][0m Trial 24 finished with value: 0.07029883082835905 and parameters: {'observation_period_num': 42, 'train_rates': 0.9575966569359714, 'learning_rate': 0.0004562963321008686, 'batch_size': 44, 'step_size': 13, 'gamma': 0.7788083192177021}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:11:54,079][0m Trial 25 finished with value: 0.0510449000119164 and parameters: {'observation_period_num': 64, 'train_rates': 0.8760796814605479, 'learning_rate': 7.559939333511659e-05, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8526347526321643}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:13:21,436][0m Trial 26 finished with value: 0.04348665923481026 and parameters: {'observation_period_num': 26, 'train_rates': 0.9579058013147072, 'learning_rate': 0.00015207829154138176, 'batch_size': 67, 'step_size': 14, 'gamma': 0.7968455073038374}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:15:58,396][0m Trial 27 finished with value: 0.03090609639071305 and parameters: {'observation_period_num': 7, 'train_rates': 0.9128452731383073, 'learning_rate': 3.193788788925935e-05, 'batch_size': 36, 'step_size': 15, 'gamma': 0.8234965879560323}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:17:29,605][0m Trial 28 finished with value: 0.06726573678144418 and parameters: {'observation_period_num': 109, 'train_rates': 0.8087152390862017, 'learning_rate': 0.0003614732631943994, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8493304121836416}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:18:27,665][0m Trial 29 finished with value: 0.08006249820819894 and parameters: {'observation_period_num': 73, 'train_rates': 0.898526761180485, 'learning_rate': 0.0006762742580622994, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9085974500873877}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:19:43,345][0m Trial 30 finished with value: 0.04937984122289825 and parameters: {'observation_period_num': 50, 'train_rates': 0.966702429988677, 'learning_rate': 0.00017477971615476902, 'batch_size': 78, 'step_size': 9, 'gamma': 0.7903656977508366}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:23:14,423][0m Trial 31 finished with value: 0.03079645913670358 and parameters: {'observation_period_num': 16, 'train_rates': 0.9341948085602051, 'learning_rate': 0.000258333946196332, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8177585007723571}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:29:03,586][0m Trial 32 finished with value: 0.041244372019815026 and parameters: {'observation_period_num': 34, 'train_rates': 0.9222821820588497, 'learning_rate': 0.000546720062688719, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7652084307192082}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:32:26,087][0m Trial 33 finished with value: 0.017388623133301737 and parameters: {'observation_period_num': 6, 'train_rates': 0.9746423553743385, 'learning_rate': 0.00010944565036509824, 'batch_size': 29, 'step_size': 12, 'gamma': 0.8123917742906794}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:35:28,796][0m Trial 34 finished with value: 0.033397731360519066 and parameters: {'observation_period_num': 22, 'train_rates': 0.970350968809144, 'learning_rate': 9.720418528299838e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8653228985835916}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:36:57,913][0m Trial 35 finished with value: 0.06234999533508207 and parameters: {'observation_period_num': 48, 'train_rates': 0.7540362548118011, 'learning_rate': 3.5112702044006694e-05, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8034223247980474}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:38:27,186][0m Trial 36 finished with value: 0.035200878977775574 and parameters: {'observation_period_num': 35, 'train_rates': 0.9882402167788589, 'learning_rate': 0.00012738040438804068, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8374518445113346}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:40:34,627][0m Trial 37 finished with value: 0.03689646323007592 and parameters: {'observation_period_num': 21, 'train_rates': 0.9425356674218011, 'learning_rate': 8.127138318653403e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8881320772459509}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:41:03,590][0m Trial 38 finished with value: 0.04842935185732804 and parameters: {'observation_period_num': 54, 'train_rates': 0.8684860120790827, 'learning_rate': 0.0002471140153777696, 'batch_size': 195, 'step_size': 14, 'gamma': 0.7694573226041223}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:44:06,563][0m Trial 39 finished with value: 0.029259171076985294 and parameters: {'observation_period_num': 17, 'train_rates': 0.9706984547097056, 'learning_rate': 0.00017719563088298406, 'batch_size': 32, 'step_size': 11, 'gamma': 0.8110373829625771}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:45:02,653][0m Trial 40 finished with value: 0.06918654502513483 and parameters: {'observation_period_num': 30, 'train_rates': 0.9006119266168636, 'learning_rate': 2.2591255834343976e-05, 'batch_size': 105, 'step_size': 13, 'gamma': 0.7898300686418971}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:48:32,570][0m Trial 41 finished with value: 0.022038926382083445 and parameters: {'observation_period_num': 6, 'train_rates': 0.9185198374886434, 'learning_rate': 0.0002233603194894188, 'batch_size': 27, 'step_size': 8, 'gamma': 0.8187904927710311}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:51:51,407][0m Trial 42 finished with value: 0.029399876913044885 and parameters: {'observation_period_num': 5, 'train_rates': 0.9422205908825392, 'learning_rate': 0.0003863468318615618, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8385900555441141}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:53:11,789][0m Trial 43 finished with value: 0.27556300696682434 and parameters: {'observation_period_num': 249, 'train_rates': 0.6797218247408188, 'learning_rate': 4.382266073335186e-05, 'batch_size': 53, 'step_size': 6, 'gamma': 0.8241295577378396}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:56:46,994][0m Trial 44 finished with value: 0.040766248116042555 and parameters: {'observation_period_num': 38, 'train_rates': 0.9719182356081417, 'learning_rate': 7.275269303201253e-05, 'batch_size': 27, 'step_size': 8, 'gamma': 0.7871588305804188}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:58:09,551][0m Trial 45 finished with value: 0.03855892104697434 and parameters: {'observation_period_num': 18, 'train_rates': 0.9213068836305578, 'learning_rate': 0.00020254922955933479, 'batch_size': 71, 'step_size': 14, 'gamma': 0.9360006897106806}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 02:59:59,298][0m Trial 46 finished with value: 0.20843711942434312 and parameters: {'observation_period_num': 224, 'train_rates': 0.9487048073501607, 'learning_rate': 1.3299620629491693e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8642592873580213}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 03:00:26,450][0m Trial 47 finished with value: 1.853512167930603 and parameters: {'observation_period_num': 44, 'train_rates': 0.9724637858361085, 'learning_rate': 1.310271197111536e-06, 'batch_size': 248, 'step_size': 4, 'gamma': 0.8059378523098644}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 03:01:16,179][0m Trial 48 finished with value: 0.08128564804792404 and parameters: {'observation_period_num': 74, 'train_rates': 0.9891643548696891, 'learning_rate': 0.00012569384726959874, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8168990422851737}. Best is trial 12 with value: 0.014956396073102951.[0m
[32m[I 2025-01-04 03:04:52,755][0m Trial 49 finished with value: 0.17089487829239935 and parameters: {'observation_period_num': 154, 'train_rates': 0.8246403234764487, 'learning_rate': 6.97064551856356e-06, 'batch_size': 23, 'step_size': 10, 'gamma': 0.8463442915978046}. Best is trial 12 with value: 0.014956396073102951.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 03:04:52,765][0m A new study created in memory with name: no-name-6f15767a-1b47-4103-a03b-de4f66e47653[0m
[32m[I 2025-01-04 03:06:04,026][0m Trial 0 finished with value: 0.5249881582415622 and parameters: {'observation_period_num': 241, 'train_rates': 0.7455941659482235, 'learning_rate': 3.5145626017346066e-06, 'batch_size': 65, 'step_size': 14, 'gamma': 0.7602411395677361}. Best is trial 0 with value: 0.5249881582415622.[0m
[32m[I 2025-01-04 03:09:04,952][0m Trial 1 finished with value: 0.025984720100422163 and parameters: {'observation_period_num': 7, 'train_rates': 0.8744780047068916, 'learning_rate': 5.5858395933364815e-05, 'batch_size': 30, 'step_size': 13, 'gamma': 0.910871246241528}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:09:36,577][0m Trial 2 finished with value: 0.2278362438082695 and parameters: {'observation_period_num': 40, 'train_rates': 0.845659812478159, 'learning_rate': 6.300948109544789e-06, 'batch_size': 177, 'step_size': 3, 'gamma': 0.9809499515142525}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:10:02,097][0m Trial 3 finished with value: 0.39793013007544803 and parameters: {'observation_period_num': 53, 'train_rates': 0.8029707548199807, 'learning_rate': 6.872235104326933e-06, 'batch_size': 227, 'step_size': 13, 'gamma': 0.786471841521957}. Best is trial 1 with value: 0.025984720100422163.[0m
Early stopping at epoch 86
[32m[I 2025-01-04 03:11:01,456][0m Trial 4 finished with value: 0.17184208204903578 and parameters: {'observation_period_num': 108, 'train_rates': 0.9310326999430716, 'learning_rate': 0.0002199372839919741, 'batch_size': 83, 'step_size': 1, 'gamma': 0.8690668718307618}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:11:53,940][0m Trial 5 finished with value: 0.4611596311292341 and parameters: {'observation_period_num': 155, 'train_rates': 0.8560309751844383, 'learning_rate': 5.355187265453093e-06, 'batch_size': 102, 'step_size': 6, 'gamma': 0.8417725252514716}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:12:13,820][0m Trial 6 finished with value: 0.9449536908378999 and parameters: {'observation_period_num': 179, 'train_rates': 0.6896016808530835, 'learning_rate': 3.5640226810033035e-06, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8832935178264987}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:12:50,780][0m Trial 7 finished with value: 0.09725563714999964 and parameters: {'observation_period_num': 113, 'train_rates': 0.7511393836325031, 'learning_rate': 0.00013099635317386367, 'batch_size': 140, 'step_size': 12, 'gamma': 0.9783640185547318}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:13:22,547][0m Trial 8 finished with value: 1.1586014431502138 and parameters: {'observation_period_num': 249, 'train_rates': 0.6686533151267006, 'learning_rate': 1.8281655428292845e-06, 'batch_size': 141, 'step_size': 1, 'gamma': 0.9139540731934803}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:14:05,764][0m Trial 9 finished with value: 0.18771791196608972 and parameters: {'observation_period_num': 175, 'train_rates': 0.6605613994005824, 'learning_rate': 4.606986556026476e-05, 'batch_size': 105, 'step_size': 3, 'gamma': 0.946050481250034}. Best is trial 1 with value: 0.025984720100422163.[0m
[32m[I 2025-01-04 03:20:15,227][0m Trial 10 finished with value: 0.020162517670542 and parameters: {'observation_period_num': 5, 'train_rates': 0.9892755801306656, 'learning_rate': 0.0009977694301320937, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8163844923584722}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:26:22,240][0m Trial 11 finished with value: 0.022427862510085106 and parameters: {'observation_period_num': 22, 'train_rates': 0.9863973626761281, 'learning_rate': 0.0009323759803304101, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8179469357207011}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:31:49,049][0m Trial 12 finished with value: 0.024928698886413962 and parameters: {'observation_period_num': 10, 'train_rates': 0.9836532368116329, 'learning_rate': 0.0009316421308704553, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8179808883064502}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:33:42,898][0m Trial 13 finished with value: 0.08337529003620148 and parameters: {'observation_period_num': 68, 'train_rates': 0.9894446629323724, 'learning_rate': 0.0009776344610856404, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8121685437764131}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:35:46,150][0m Trial 14 finished with value: 0.06922179537813693 and parameters: {'observation_period_num': 67, 'train_rates': 0.9167613183340728, 'learning_rate': 0.0003267211916886704, 'batch_size': 45, 'step_size': 10, 'gamma': 0.7557682683092961}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:36:19,244][0m Trial 15 finished with value: 0.042304145523171494 and parameters: {'observation_period_num': 31, 'train_rates': 0.9292596921482291, 'learning_rate': 0.0005081895504145743, 'batch_size': 186, 'step_size': 5, 'gamma': 0.8404714780976676}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:40:35,540][0m Trial 16 finished with value: 0.12651858395257873 and parameters: {'observation_period_num': 88, 'train_rates': 0.6107296775134202, 'learning_rate': 2.105008615576944e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7941974139392706}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:41:58,977][0m Trial 17 finished with value: 0.04843445701731576 and parameters: {'observation_period_num': 27, 'train_rates': 0.953851130180886, 'learning_rate': 9.662563562561871e-05, 'batch_size': 71, 'step_size': 9, 'gamma': 0.8486815525627497}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:42:54,997][0m Trial 18 finished with value: 0.05570990238011929 and parameters: {'observation_period_num': 86, 'train_rates': 0.90061950150868, 'learning_rate': 0.0004602521029264779, 'batch_size': 103, 'step_size': 5, 'gamma': 0.7837827492860452}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:45:00,660][0m Trial 19 finished with value: 0.1601326195698864 and parameters: {'observation_period_num': 135, 'train_rates': 0.8119771786574388, 'learning_rate': 1.3715533874488781e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.816693161751193}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:45:35,495][0m Trial 20 finished with value: 0.12750598788261414 and parameters: {'observation_period_num': 212, 'train_rates': 0.9544305661604642, 'learning_rate': 0.0001709834519608126, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8699465865799529}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:51:02,609][0m Trial 21 finished with value: 0.036181724677651614 and parameters: {'observation_period_num': 5, 'train_rates': 0.9802202077212957, 'learning_rate': 0.0007465917038481557, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8102797337455716}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:56:48,800][0m Trial 22 finished with value: 0.027565880528380795 and parameters: {'observation_period_num': 20, 'train_rates': 0.9896025036152599, 'learning_rate': 0.0009378400327723631, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8250039623324608}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 03:58:27,026][0m Trial 23 finished with value: 0.049532334390166285 and parameters: {'observation_period_num': 50, 'train_rates': 0.8854966967466728, 'learning_rate': 0.00031924877776437526, 'batch_size': 56, 'step_size': 6, 'gamma': 0.7795464938874916}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:01:12,370][0m Trial 24 finished with value: 0.035241530680625174 and parameters: {'observation_period_num': 8, 'train_rates': 0.9515467248164784, 'learning_rate': 0.0005403019305071648, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8551963985594967}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:02:23,003][0m Trial 25 finished with value: 0.04613571783358401 and parameters: {'observation_period_num': 35, 'train_rates': 0.96243828875524, 'learning_rate': 0.00030245617829128434, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8298050129381737}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:05:22,855][0m Trial 26 finished with value: 0.06029782493519409 and parameters: {'observation_period_num': 71, 'train_rates': 0.9226346828085678, 'learning_rate': 9.990856316283118e-05, 'batch_size': 31, 'step_size': 4, 'gamma': 0.8012617114568986}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:06:34,407][0m Trial 27 finished with value: 0.07208480533472328 and parameters: {'observation_period_num': 51, 'train_rates': 0.8518103178996316, 'learning_rate': 0.0006156684949881532, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8916192220642071}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:08:12,169][0m Trial 28 finished with value: 0.03250304190441966 and parameters: {'observation_period_num': 18, 'train_rates': 0.8992765904115231, 'learning_rate': 0.0009870098286193637, 'batch_size': 58, 'step_size': 6, 'gamma': 0.773053918618669}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:08:55,242][0m Trial 29 finished with value: 0.9326598156504706 and parameters: {'observation_period_num': 87, 'train_rates': 0.756009324963332, 'learning_rate': 1.1300559735609633e-06, 'batch_size': 119, 'step_size': 11, 'gamma': 0.7686946462858703}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:10:28,782][0m Trial 30 finished with value: 0.04315838945466419 and parameters: {'observation_period_num': 39, 'train_rates': 0.9704992172927982, 'learning_rate': 0.0002423478894082271, 'batch_size': 64, 'step_size': 8, 'gamma': 0.7528844724165948}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:13:15,591][0m Trial 31 finished with value: 0.03281224057515407 and parameters: {'observation_period_num': 11, 'train_rates': 0.8783007422254195, 'learning_rate': 2.901154609810778e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9153472262586368}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:16:28,411][0m Trial 32 finished with value: 0.03367523016430527 and parameters: {'observation_period_num': 6, 'train_rates': 0.9438347437394446, 'learning_rate': 6.994009700774625e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.9193765048661359}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:22:03,716][0m Trial 33 finished with value: 0.04142924950520198 and parameters: {'observation_period_num': 23, 'train_rates': 0.8719756064811717, 'learning_rate': 0.0004101807859460105, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9546206397685493}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:23:53,106][0m Trial 34 finished with value: 0.05390051348804492 and parameters: {'observation_period_num': 46, 'train_rates': 0.7807730144187431, 'learning_rate': 5.202803292306763e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8975694527052849}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:27:04,165][0m Trial 35 finished with value: 0.060496299748303575 and parameters: {'observation_period_num': 29, 'train_rates': 0.8224022997666148, 'learning_rate': 8.431989924134904e-06, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8579151266172659}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:28:07,917][0m Trial 36 finished with value: 0.04516266488159696 and parameters: {'observation_period_num': 61, 'train_rates': 0.8341769906030623, 'learning_rate': 0.00017646738104144528, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8345789240754194}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:30:11,348][0m Trial 37 finished with value: 0.05173438120307578 and parameters: {'observation_period_num': 41, 'train_rates': 0.9098991797454973, 'learning_rate': 0.0006487258818388533, 'batch_size': 45, 'step_size': 11, 'gamma': 0.797781981443897}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:30:50,071][0m Trial 38 finished with value: 0.17619070410728455 and parameters: {'observation_period_num': 19, 'train_rates': 0.9696275266484506, 'learning_rate': 1.3870054832241655e-05, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9437513592180907}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:31:18,761][0m Trial 39 finished with value: 0.13868941366672516 and parameters: {'observation_period_num': 228, 'train_rates': 0.9363451501381539, 'learning_rate': 0.00021474470014332, 'batch_size': 207, 'step_size': 7, 'gamma': 0.8851379276300648}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:31:45,761][0m Trial 40 finished with value: 0.09507054090499878 and parameters: {'observation_period_num': 113, 'train_rates': 0.9748169773988904, 'learning_rate': 0.000707756432515803, 'batch_size': 247, 'step_size': 3, 'gamma': 0.9024702978525653}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:37:49,258][0m Trial 41 finished with value: 0.028096050230993166 and parameters: {'observation_period_num': 18, 'train_rates': 0.9757015648570011, 'learning_rate': 0.0009741452361105665, 'batch_size': 16, 'step_size': 7, 'gamma': 0.820327621610324}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:41:27,893][0m Trial 42 finished with value: 0.021201293519698083 and parameters: {'observation_period_num': 25, 'train_rates': 0.9866343796460983, 'learning_rate': 0.000756205973438964, 'batch_size': 27, 'step_size': 10, 'gamma': 0.8232487185630242}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:44:16,054][0m Trial 43 finished with value: 0.056812131550930275 and parameters: {'observation_period_num': 36, 'train_rates': 0.723553057376888, 'learning_rate': 0.00038805643864295194, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8078707374819065}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:46:07,902][0m Trial 44 finished with value: 0.24725370619507783 and parameters: {'observation_period_num': 55, 'train_rates': 0.9377727756292848, 'learning_rate': 3.495132800147335e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8472706754306614}. Best is trial 10 with value: 0.020162517670542.[0m
[32m[I 2025-01-04 04:47:40,767][0m Trial 45 finished with value: 0.015052521601319313 and parameters: {'observation_period_num': 7, 'train_rates': 0.9888345190812218, 'learning_rate': 0.0005065208855896543, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8671270107279989}. Best is trial 45 with value: 0.015052521601319313.[0m
[32m[I 2025-01-04 04:49:13,927][0m Trial 46 finished with value: 0.08652327209711075 and parameters: {'observation_period_num': 78, 'train_rates': 0.9861673800268289, 'learning_rate': 0.0007242090866689255, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8604979974552132}. Best is trial 45 with value: 0.015052521601319313.[0m
[32m[I 2025-01-04 04:51:40,485][0m Trial 47 finished with value: 0.034591358787949805 and parameters: {'observation_period_num': 15, 'train_rates': 0.9650825677946151, 'learning_rate': 0.0004874159152766933, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8347741489848507}. Best is trial 45 with value: 0.015052521601319313.[0m
[32m[I 2025-01-04 04:52:27,609][0m Trial 48 finished with value: 0.06412842619803644 and parameters: {'observation_period_num': 128, 'train_rates': 0.9453570876753764, 'learning_rate': 0.0003480539867687551, 'batch_size': 126, 'step_size': 12, 'gamma': 0.7922473276704314}. Best is trial 45 with value: 0.015052521601319313.[0m
[32m[I 2025-01-04 04:53:46,281][0m Trial 49 finished with value: 0.028020044788718224 and parameters: {'observation_period_num': 29, 'train_rates': 0.9887005467424852, 'learning_rate': 0.0005607650532033728, 'batch_size': 77, 'step_size': 11, 'gamma': 0.8755542602748194}. Best is trial 45 with value: 0.015052521601319313.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 04:53:46,291][0m A new study created in memory with name: no-name-41d10880-760f-41ea-a1a7-1c37f8f15834[0m
[32m[I 2025-01-04 04:54:04,647][0m Trial 0 finished with value: 0.2621492976716513 and parameters: {'observation_period_num': 51, 'train_rates': 0.6006796130805676, 'learning_rate': 1.7178798694942225e-05, 'batch_size': 254, 'step_size': 12, 'gamma': 0.9526821474652689}. Best is trial 0 with value: 0.2621492976716513.[0m
[32m[I 2025-01-04 04:54:55,634][0m Trial 1 finished with value: 0.5739573521608391 and parameters: {'observation_period_num': 61, 'train_rates': 0.7099522986850291, 'learning_rate': 1.558680686186452e-06, 'batch_size': 96, 'step_size': 10, 'gamma': 0.8891995198627713}. Best is trial 0 with value: 0.2621492976716513.[0m
[32m[I 2025-01-04 04:55:29,054][0m Trial 2 finished with value: 0.24503222107887268 and parameters: {'observation_period_num': 79, 'train_rates': 0.9681995592695019, 'learning_rate': 3.6774366897213603e-05, 'batch_size': 197, 'step_size': 6, 'gamma': 0.77910748548336}. Best is trial 2 with value: 0.24503222107887268.[0m
[32m[I 2025-01-04 04:55:57,505][0m Trial 3 finished with value: 0.7637265044940661 and parameters: {'observation_period_num': 18, 'train_rates': 0.8010693215762088, 'learning_rate': 1.4663263427294225e-06, 'batch_size': 205, 'step_size': 11, 'gamma': 0.7612233494863585}. Best is trial 2 with value: 0.24503222107887268.[0m
[32m[I 2025-01-04 04:56:47,039][0m Trial 4 finished with value: 0.08443621169872485 and parameters: {'observation_period_num': 19, 'train_rates': 0.7899382405988994, 'learning_rate': 5.8629201755840534e-05, 'batch_size': 109, 'step_size': 4, 'gamma': 0.7533160037721721}. Best is trial 4 with value: 0.08443621169872485.[0m
[32m[I 2025-01-04 04:57:11,543][0m Trial 5 finished with value: 0.4332402936748931 and parameters: {'observation_period_num': 84, 'train_rates': 0.6943952927135288, 'learning_rate': 1.2863669795714887e-05, 'batch_size': 219, 'step_size': 11, 'gamma': 0.7830009915716059}. Best is trial 4 with value: 0.08443621169872485.[0m
[32m[I 2025-01-04 04:58:55,093][0m Trial 6 finished with value: 0.11437070369720459 and parameters: {'observation_period_num': 99, 'train_rates': 0.9636529652661525, 'learning_rate': 0.00043524118126452347, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9098433751144113}. Best is trial 4 with value: 0.08443621169872485.[0m
[32m[I 2025-01-04 04:59:34,248][0m Trial 7 finished with value: 0.37216805274359843 and parameters: {'observation_period_num': 136, 'train_rates': 0.9265999376223043, 'learning_rate': 3.4602030495501736e-06, 'batch_size': 152, 'step_size': 8, 'gamma': 0.9804877727163946}. Best is trial 4 with value: 0.08443621169872485.[0m
[32m[I 2025-01-04 05:00:09,048][0m Trial 8 finished with value: 0.21772753018630694 and parameters: {'observation_period_num': 148, 'train_rates': 0.7917312994611103, 'learning_rate': 0.00019410747363341014, 'batch_size': 156, 'step_size': 15, 'gamma': 0.9552876526133071}. Best is trial 4 with value: 0.08443621169872485.[0m
[32m[I 2025-01-04 05:00:32,684][0m Trial 9 finished with value: 0.061067583063283026 and parameters: {'observation_period_num': 84, 'train_rates': 0.7826994777507468, 'learning_rate': 0.0004471915919876399, 'batch_size': 235, 'step_size': 9, 'gamma': 0.8533721133866896}. Best is trial 9 with value: 0.061067583063283026.[0m
[32m[I 2025-01-04 05:04:11,043][0m Trial 10 finished with value: 0.15377387282054775 and parameters: {'observation_period_num': 230, 'train_rates': 0.8727042014817922, 'learning_rate': 0.0007936077171244682, 'batch_size': 23, 'step_size': 1, 'gamma': 0.839055190776264}. Best is trial 9 with value: 0.061067583063283026.[0m
[32m[I 2025-01-04 05:05:02,671][0m Trial 11 finished with value: 0.05252573824516213 and parameters: {'observation_period_num': 8, 'train_rates': 0.7897418765156985, 'learning_rate': 9.051983660706038e-05, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8296090617985774}. Best is trial 11 with value: 0.05252573824516213.[0m
[32m[I 2025-01-04 05:05:57,934][0m Trial 12 finished with value: 0.0973248390894281 and parameters: {'observation_period_num': 184, 'train_rates': 0.8550961021974578, 'learning_rate': 0.00013456241579288308, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8310165923324352}. Best is trial 11 with value: 0.05252573824516213.[0m
[32m[I 2025-01-04 05:06:36,110][0m Trial 13 finished with value: 0.038398536593075044 and parameters: {'observation_period_num': 5, 'train_rates': 0.713809730975046, 'learning_rate': 0.00018266540198115877, 'batch_size': 134, 'step_size': 7, 'gamma': 0.8340956707700711}. Best is trial 13 with value: 0.038398536593075044.[0m
Early stopping at epoch 60
[32m[I 2025-01-04 05:06:59,691][0m Trial 14 finished with value: 0.13217511320480307 and parameters: {'observation_period_num': 20, 'train_rates': 0.7002541019281892, 'learning_rate': 0.00010081829912388186, 'batch_size': 129, 'step_size': 1, 'gamma': 0.8083084498162113}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:08:09,914][0m Trial 15 finished with value: 0.05498213727629905 and parameters: {'observation_period_num': 40, 'train_rates': 0.6476184106759731, 'learning_rate': 0.0002282099767423818, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8712640782565129}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:08:42,753][0m Trial 16 finished with value: 0.22986060314752854 and parameters: {'observation_period_num': 7, 'train_rates': 0.7477159975806835, 'learning_rate': 1.2246176363859307e-05, 'batch_size': 169, 'step_size': 4, 'gamma': 0.8095715229338514}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:09:24,795][0m Trial 17 finished with value: 0.1329431997526011 and parameters: {'observation_period_num': 188, 'train_rates': 0.8514682115474966, 'learning_rate': 5.208409917138501e-05, 'batch_size': 124, 'step_size': 6, 'gamma': 0.9115055889602296}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:10:32,567][0m Trial 18 finished with value: 0.1301320094401922 and parameters: {'observation_period_num': 105, 'train_rates': 0.7362779364496683, 'learning_rate': 8.353242998599833e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8202751623873555}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:10:59,844][0m Trial 19 finished with value: 0.15844282371094054 and parameters: {'observation_period_num': 42, 'train_rates': 0.6518453291329135, 'learning_rate': 2.4506991976433364e-05, 'batch_size': 186, 'step_size': 7, 'gamma': 0.8611623796191122}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:13:44,416][0m Trial 20 finished with value: 0.2695655380878332 and parameters: {'observation_period_num': 155, 'train_rates': 0.8245396931184567, 'learning_rate': 6.35952193871671e-06, 'batch_size': 30, 'step_size': 3, 'gamma': 0.8900613632228433}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:15:01,692][0m Trial 21 finished with value: 0.0499012027651512 and parameters: {'observation_period_num': 37, 'train_rates': 0.6517073673415023, 'learning_rate': 0.0002452354471675457, 'batch_size': 59, 'step_size': 6, 'gamma': 0.8798554624572954}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:15:53,861][0m Trial 22 finished with value: 0.054225366609668826 and parameters: {'observation_period_num': 34, 'train_rates': 0.6474230047305364, 'learning_rate': 0.00032882739743238986, 'batch_size': 88, 'step_size': 6, 'gamma': 0.7984554182734331}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:17:27,478][0m Trial 23 finished with value: 0.1016746639215853 and parameters: {'observation_period_num': 67, 'train_rates': 0.6005961551902901, 'learning_rate': 0.0007882677035004856, 'batch_size': 45, 'step_size': 5, 'gamma': 0.8414718257833809}. Best is trial 13 with value: 0.038398536593075044.[0m
[32m[I 2025-01-04 05:18:11,659][0m Trial 24 finished with value: 0.034881032458165796 and parameters: {'observation_period_num': 10, 'train_rates': 0.7492411505873846, 'learning_rate': 0.00015707617280463985, 'batch_size': 116, 'step_size': 8, 'gamma': 0.8715130330357549}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:18:48,851][0m Trial 25 finished with value: 0.04528345754621802 and parameters: {'observation_period_num': 32, 'train_rates': 0.7467483676028869, 'learning_rate': 0.00017658153078255737, 'batch_size': 138, 'step_size': 8, 'gamma': 0.8808528996844158}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:19:22,417][0m Trial 26 finished with value: 0.08131931996127158 and parameters: {'observation_period_num': 116, 'train_rates': 0.740052436075161, 'learning_rate': 0.0001547786889500721, 'batch_size': 156, 'step_size': 8, 'gamma': 0.9126187753751998}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:20:01,492][0m Trial 27 finished with value: 0.05559489151999333 and parameters: {'observation_period_num': 58, 'train_rates': 0.7613230745625387, 'learning_rate': 0.0004893627515137766, 'batch_size': 135, 'step_size': 9, 'gamma': 0.92813949940313}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:20:39,408][0m Trial 28 finished with value: 0.06539254042912614 and parameters: {'observation_period_num': 25, 'train_rates': 0.6844699779819119, 'learning_rate': 5.127616601738627e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.855005933746265}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:21:10,036][0m Trial 29 finished with value: 0.1253045282816303 and parameters: {'observation_period_num': 49, 'train_rates': 0.7326758610647975, 'learning_rate': 2.5125063749689698e-05, 'batch_size': 177, 'step_size': 13, 'gamma': 0.8879054027733566}. Best is trial 24 with value: 0.034881032458165796.[0m
[32m[I 2025-01-04 05:21:59,018][0m Trial 30 finished with value: 0.03069466230160785 and parameters: {'observation_period_num': 7, 'train_rates': 0.8201394274001084, 'learning_rate': 0.00014291514994357985, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9328565521500002}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:22:48,127][0m Trial 31 finished with value: 0.033602213671871724 and parameters: {'observation_period_num': 5, 'train_rates': 0.8223375236595419, 'learning_rate': 0.00013001302925472048, 'batch_size': 112, 'step_size': 12, 'gamma': 0.9423291900522046}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:23:40,275][0m Trial 32 finished with value: 0.03427093077001188 and parameters: {'observation_period_num': 5, 'train_rates': 0.8904940214066782, 'learning_rate': 0.00010718831616075798, 'batch_size': 113, 'step_size': 12, 'gamma': 0.9483982865957427}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:24:50,000][0m Trial 33 finished with value: 0.07879861007476675 and parameters: {'observation_period_num': 67, 'train_rates': 0.8997475874779267, 'learning_rate': 9.252063632785121e-05, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9518636641604022}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:25:41,355][0m Trial 34 finished with value: 0.05217506027748997 and parameters: {'observation_period_num': 19, 'train_rates': 0.82873016513304, 'learning_rate': 3.567584891780207e-05, 'batch_size': 110, 'step_size': 12, 'gamma': 0.944944458538026}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:26:33,071][0m Trial 35 finished with value: 0.10986836029262077 and parameters: {'observation_period_num': 48, 'train_rates': 0.9294608151246974, 'learning_rate': 0.0002913661478327032, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9776700544330936}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:27:40,324][0m Trial 36 finished with value: 0.032176685192736615 and parameters: {'observation_period_num': 8, 'train_rates': 0.8838135278800936, 'learning_rate': 6.657096315244428e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.966846446756438}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:28:44,199][0m Trial 37 finished with value: 0.04905095491969623 and parameters: {'observation_period_num': 28, 'train_rates': 0.8918464331242634, 'learning_rate': 5.751625435858821e-05, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9638194162961644}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:30:40,582][0m Trial 38 finished with value: 0.07390026199587944 and parameters: {'observation_period_num': 71, 'train_rates': 0.829066343646372, 'learning_rate': 7.566497557436471e-05, 'batch_size': 45, 'step_size': 15, 'gamma': 0.9339858724534658}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:31:22,378][0m Trial 39 finished with value: 0.0697206137607796 and parameters: {'observation_period_num': 55, 'train_rates': 0.9316663250364189, 'learning_rate': 4.392100681321977e-05, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9877385416037198}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:32:23,359][0m Trial 40 finished with value: 0.11222846806049347 and parameters: {'observation_period_num': 19, 'train_rates': 0.9880117637220805, 'learning_rate': 0.0001280022534813038, 'batch_size': 100, 'step_size': 11, 'gamma': 0.9636811874181576}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:33:13,410][0m Trial 41 finished with value: 0.0383860926586585 and parameters: {'observation_period_num': 5, 'train_rates': 0.8757718699203022, 'learning_rate': 6.684247079466823e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.9338614879069391}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:34:21,366][0m Trial 42 finished with value: 0.030900658595652778 and parameters: {'observation_period_num': 17, 'train_rates': 0.8100400991254993, 'learning_rate': 0.0001289101837037223, 'batch_size': 77, 'step_size': 12, 'gamma': 0.9736583744711628}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:35:35,450][0m Trial 43 finished with value: 0.032737972851047355 and parameters: {'observation_period_num': 26, 'train_rates': 0.8478713138006346, 'learning_rate': 0.0001121158429790615, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9708456065889178}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:36:41,411][0m Trial 44 finished with value: 0.03933616860067462 and parameters: {'observation_period_num': 27, 'train_rates': 0.8083875764163501, 'learning_rate': 0.0003215383306233266, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9710240153295091}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:38:31,665][0m Trial 45 finished with value: 0.03391971771412411 and parameters: {'observation_period_num': 17, 'train_rates': 0.8470249756170918, 'learning_rate': 2.735131956650471e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.987181294539041}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:39:47,406][0m Trial 46 finished with value: 0.1013169750823813 and parameters: {'observation_period_num': 94, 'train_rates': 0.7730784330380975, 'learning_rate': 1.806065683306525e-05, 'batch_size': 66, 'step_size': 10, 'gamma': 0.9259492789250532}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:40:40,994][0m Trial 47 finished with value: 0.5694273780340446 and parameters: {'observation_period_num': 233, 'train_rates': 0.8078103103414929, 'learning_rate': 1.2084466089346035e-06, 'batch_size': 95, 'step_size': 12, 'gamma': 0.9622625257835054}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:41:56,224][0m Trial 48 finished with value: 0.03905908511830615 and parameters: {'observation_period_num': 41, 'train_rates': 0.8677077966546347, 'learning_rate': 0.000124552080080166, 'batch_size': 73, 'step_size': 14, 'gamma': 0.9411937398371517}. Best is trial 30 with value: 0.03069466230160785.[0m
[32m[I 2025-01-04 05:43:00,269][0m Trial 49 finished with value: 0.02938742033667921 and parameters: {'observation_period_num': 15, 'train_rates': 0.9136040382929589, 'learning_rate': 0.0005554523333265239, 'batch_size': 91, 'step_size': 10, 'gamma': 0.9777060915216246}. Best is trial 49 with value: 0.02938742033667921.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 05:43:00,279][0m A new study created in memory with name: no-name-222ea0f7-f176-4bde-9620-a094a12e2773[0m
[32m[I 2025-01-04 05:43:38,326][0m Trial 0 finished with value: 0.05845372918829845 and parameters: {'observation_period_num': 128, 'train_rates': 0.8386252297605699, 'learning_rate': 0.0004182733912456942, 'batch_size': 143, 'step_size': 6, 'gamma': 0.8478464991516083}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:44:40,874][0m Trial 1 finished with value: 0.29668880314753826 and parameters: {'observation_period_num': 161, 'train_rates': 0.608656893757217, 'learning_rate': 0.000504696143734128, 'batch_size': 67, 'step_size': 6, 'gamma': 0.7779587847133277}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:45:21,611][0m Trial 2 finished with value: 0.20389477463347397 and parameters: {'observation_period_num': 119, 'train_rates': 0.7922902059870944, 'learning_rate': 2.7276789194736377e-05, 'batch_size': 131, 'step_size': 7, 'gamma': 0.809757577986789}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:45:54,620][0m Trial 3 finished with value: 0.20635913892539307 and parameters: {'observation_period_num': 63, 'train_rates': 0.6035790203393003, 'learning_rate': 0.0007231131325688798, 'batch_size': 139, 'step_size': 3, 'gamma': 0.8413612412205566}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:46:27,288][0m Trial 4 finished with value: 0.3324871392496701 and parameters: {'observation_period_num': 121, 'train_rates': 0.7440745613631485, 'learning_rate': 2.966986014096214e-05, 'batch_size': 161, 'step_size': 6, 'gamma': 0.763682557042258}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:46:53,339][0m Trial 5 finished with value: 0.6999740816142461 and parameters: {'observation_period_num': 212, 'train_rates': 0.6005102838182352, 'learning_rate': 2.1252758478523214e-06, 'batch_size': 168, 'step_size': 2, 'gamma': 0.966760060943994}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:47:47,157][0m Trial 6 finished with value: 0.49043476581573486 and parameters: {'observation_period_num': 152, 'train_rates': 0.9760662113979734, 'learning_rate': 2.378577251200887e-06, 'batch_size': 112, 'step_size': 10, 'gamma': 0.8947871798791743}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:48:42,611][0m Trial 7 finished with value: 0.12208439823126002 and parameters: {'observation_period_num': 101, 'train_rates': 0.7251286267490145, 'learning_rate': 4.271137604196214e-05, 'batch_size': 87, 'step_size': 11, 'gamma': 0.7819370577366291}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:49:51,695][0m Trial 8 finished with value: 0.08401555567979813 and parameters: {'observation_period_num': 230, 'train_rates': 0.9726516397297612, 'learning_rate': 0.0003234863308729067, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8227788398616103}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:50:31,112][0m Trial 9 finished with value: 0.6425827085730843 and parameters: {'observation_period_num': 51, 'train_rates': 0.744141702483641, 'learning_rate': 1.1694232178993106e-06, 'batch_size': 129, 'step_size': 14, 'gamma': 0.8051115515781917}. Best is trial 0 with value: 0.05845372918829845.[0m
[32m[I 2025-01-04 05:50:56,243][0m Trial 10 finished with value: 0.044869732156146966 and parameters: {'observation_period_num': 15, 'train_rates': 0.8573800992689369, 'learning_rate': 0.00013121720495378868, 'batch_size': 232, 'step_size': 15, 'gamma': 0.9124551307662759}. Best is trial 10 with value: 0.044869732156146966.[0m
[32m[I 2025-01-04 05:51:21,432][0m Trial 11 finished with value: 0.04099253475152213 and parameters: {'observation_period_num': 9, 'train_rates': 0.89182082317947, 'learning_rate': 0.00015108411736817286, 'batch_size': 243, 'step_size': 14, 'gamma': 0.8972650155050867}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 05:51:46,951][0m Trial 12 finished with value: 0.048293728879610455 and parameters: {'observation_period_num': 16, 'train_rates': 0.8882027387994668, 'learning_rate': 0.00011168224217568056, 'batch_size': 246, 'step_size': 15, 'gamma': 0.9065411544475137}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 05:52:12,232][0m Trial 13 finished with value: 0.04563212614779767 and parameters: {'observation_period_num': 7, 'train_rates': 0.912195657439532, 'learning_rate': 0.0001338304891310957, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9410383990058787}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 05:52:39,312][0m Trial 14 finished with value: 0.29559522637620916 and parameters: {'observation_period_num': 52, 'train_rates': 0.8814776390218506, 'learning_rate': 8.547204669481232e-06, 'batch_size': 225, 'step_size': 12, 'gamma': 0.8911314289646106}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 05:53:07,327][0m Trial 15 finished with value: 0.0672467435215367 and parameters: {'observation_period_num': 79, 'train_rates': 0.821934149916504, 'learning_rate': 0.00012917813012959905, 'batch_size': 210, 'step_size': 9, 'gamma': 0.9329832168355682}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 05:56:50,634][0m Trial 16 finished with value: 0.0421531869954354 and parameters: {'observation_period_num': 29, 'train_rates': 0.92434493981443, 'learning_rate': 6.874589274720562e-05, 'batch_size': 25, 'step_size': 15, 'gamma': 0.8756971251091391}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:02:37,137][0m Trial 17 finished with value: 0.04371939125386151 and parameters: {'observation_period_num': 31, 'train_rates': 0.924792597899159, 'learning_rate': 1.0837609208964248e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8657589031097929}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:06:07,919][0m Trial 18 finished with value: 0.05314067006111145 and parameters: {'observation_period_num': 85, 'train_rates': 0.9229474983058267, 'learning_rate': 5.277382507202234e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.8675519277654681}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:06:40,721][0m Trial 19 finished with value: 0.06543397903442383 and parameters: {'observation_period_num': 38, 'train_rates': 0.9506494151817333, 'learning_rate': 0.0002483967293660605, 'batch_size': 191, 'step_size': 15, 'gamma': 0.9637061012876842}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:08:27,734][0m Trial 20 finished with value: 0.13506941425467506 and parameters: {'observation_period_num': 193, 'train_rates': 0.7857908233734168, 'learning_rate': 1.3795735165771756e-05, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8875349484253328}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:12:11,554][0m Trial 21 finished with value: 0.05515293732447469 and parameters: {'observation_period_num': 27, 'train_rates': 0.9216259421106959, 'learning_rate': 7.819053139146814e-06, 'batch_size': 25, 'step_size': 12, 'gamma': 0.8644445000358539}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:14:09,685][0m Trial 22 finished with value: 0.06531626762765827 and parameters: {'observation_period_num': 37, 'train_rates': 0.9434191998313403, 'learning_rate': 1.4608314785461345e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.9270603307444008}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:19:03,741][0m Trial 23 finished with value: 0.08451846165534778 and parameters: {'observation_period_num': 72, 'train_rates': 0.8783224544130681, 'learning_rate': 4.8858631137249226e-06, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8442445458268485}. Best is trial 11 with value: 0.04099253475152213.[0m
[32m[I 2025-01-04 06:20:04,504][0m Trial 24 finished with value: 0.038582943379879 and parameters: {'observation_period_num': 6, 'train_rates': 0.9893165791849272, 'learning_rate': 7.186084478540135e-05, 'batch_size': 103, 'step_size': 9, 'gamma': 0.8771215965175543}. Best is trial 24 with value: 0.038582943379879.[0m
[32m[I 2025-01-04 06:21:07,107][0m Trial 25 finished with value: 0.03190927952528 and parameters: {'observation_period_num': 9, 'train_rates': 0.9888096276261658, 'learning_rate': 6.901769289731051e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.9878119498647087}. Best is trial 25 with value: 0.03190927952528.[0m
[32m[I 2025-01-04 06:22:17,357][0m Trial 26 finished with value: 0.024748308584094048 and parameters: {'observation_period_num': 8, 'train_rates': 0.9741517312878949, 'learning_rate': 0.0002149535490773464, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9818500054033034}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:23:15,986][0m Trial 27 finished with value: 0.050434909760951996 and parameters: {'observation_period_num': 55, 'train_rates': 0.9874121974781225, 'learning_rate': 6.978064157027485e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.989256642321687}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:24:13,172][0m Trial 28 finished with value: 0.1050058244981549 and parameters: {'observation_period_num': 96, 'train_rates': 0.9616694581292635, 'learning_rate': 0.00021536515503868512, 'batch_size': 103, 'step_size': 4, 'gamma': 0.9823957335438406}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:25:16,191][0m Trial 29 finished with value: 0.03181570277347113 and parameters: {'observation_period_num': 5, 'train_rates': 0.6672915719656768, 'learning_rate': 0.0009259188749871583, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9564100597161904}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:26:23,523][0m Trial 30 finished with value: 0.2743968882032572 and parameters: {'observation_period_num': 152, 'train_rates': 0.6583160603742659, 'learning_rate': 0.0008479595287310033, 'batch_size': 64, 'step_size': 8, 'gamma': 0.9598463425471184}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:27:17,661][0m Trial 31 finished with value: 0.04769123698233455 and parameters: {'observation_period_num': 5, 'train_rates': 0.669968209898352, 'learning_rate': 0.0005262535526189112, 'batch_size': 87, 'step_size': 9, 'gamma': 0.9516283550346266}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:27:58,486][0m Trial 32 finished with value: 0.037984970344018334 and parameters: {'observation_period_num': 23, 'train_rates': 0.6493328983805962, 'learning_rate': 0.0004206569792267661, 'batch_size': 117, 'step_size': 9, 'gamma': 0.9757258090733898}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:28:37,346][0m Trial 33 finished with value: 0.06305231724693937 and parameters: {'observation_period_num': 44, 'train_rates': 0.6456867100519937, 'learning_rate': 0.0004358470679262624, 'batch_size': 118, 'step_size': 7, 'gamma': 0.9772251517304956}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:29:50,389][0m Trial 34 finished with value: 0.0693921833314571 and parameters: {'observation_period_num': 24, 'train_rates': 0.6915875530017368, 'learning_rate': 0.0006230851702660812, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9736708824723498}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:30:20,146][0m Trial 35 finished with value: 0.08910386081175622 and parameters: {'observation_period_num': 69, 'train_rates': 0.62693596792254, 'learning_rate': 0.00032042278377892826, 'batch_size': 159, 'step_size': 7, 'gamma': 0.9458785067949738}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:31:24,101][0m Trial 36 finished with value: 0.05681973267507789 and parameters: {'observation_period_num': 22, 'train_rates': 0.7238476735620856, 'learning_rate': 0.0002100125717668751, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9890815744389367}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:31:56,759][0m Trial 37 finished with value: 0.18197681259577886 and parameters: {'observation_period_num': 178, 'train_rates': 0.6925279413611636, 'learning_rate': 0.000898863290166922, 'batch_size': 144, 'step_size': 10, 'gamma': 0.9268020634857257}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:33:46,807][0m Trial 38 finished with value: 0.0484771055246269 and parameters: {'observation_period_num': 60, 'train_rates': 0.8198604065087025, 'learning_rate': 2.3873147630571728e-05, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9596549427965334}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:34:34,600][0m Trial 39 finished with value: 0.087830843007261 and parameters: {'observation_period_num': 44, 'train_rates': 0.6288086423233534, 'learning_rate': 0.0003324568314090964, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9724504585789688}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:35:15,339][0m Trial 40 finished with value: 0.10684063804980654 and parameters: {'observation_period_num': 135, 'train_rates': 0.7784111506609692, 'learning_rate': 0.0006417757402705744, 'batch_size': 123, 'step_size': 9, 'gamma': 0.9517309472030876}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:36:00,034][0m Trial 41 finished with value: 0.04541494697332382 and parameters: {'observation_period_num': 7, 'train_rates': 0.9872929176314854, 'learning_rate': 7.290896037159348e-05, 'batch_size': 145, 'step_size': 9, 'gamma': 0.9771579144361541}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:37:00,908][0m Trial 42 finished with value: 0.03569470392865735 and parameters: {'observation_period_num': 18, 'train_rates': 0.9514272681031121, 'learning_rate': 0.0009928820815291674, 'batch_size': 98, 'step_size': 7, 'gamma': 0.9186267876434046}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:38:22,319][0m Trial 43 finished with value: 0.0439567376570021 and parameters: {'observation_period_num': 21, 'train_rates': 0.9498030061844019, 'learning_rate': 0.0008866965431630824, 'batch_size': 73, 'step_size': 6, 'gamma': 0.9416029705780832}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:39:12,846][0m Trial 44 finished with value: 0.2289354813463977 and parameters: {'observation_period_num': 248, 'train_rates': 0.6860101242223825, 'learning_rate': 0.00041655909689635844, 'batch_size': 88, 'step_size': 5, 'gamma': 0.9127359634208969}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:39:58,072][0m Trial 45 finished with value: 0.07925534248352051 and parameters: {'observation_period_num': 36, 'train_rates': 0.9646738852944952, 'learning_rate': 0.000536482719209581, 'batch_size': 136, 'step_size': 7, 'gamma': 0.9652524434302651}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:41:22,756][0m Trial 46 finished with value: 0.0394273683486972 and parameters: {'observation_period_num': 17, 'train_rates': 0.7382615109981572, 'learning_rate': 0.00018545221131548135, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9899186467373284}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:42:02,088][0m Trial 47 finished with value: 0.21707724944426313 and parameters: {'observation_period_num': 109, 'train_rates': 0.6161752978860524, 'learning_rate': 0.00029325193096802697, 'batch_size': 115, 'step_size': 1, 'gamma': 0.931523046924095}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:42:58,155][0m Trial 48 finished with value: 0.0497137351326414 and parameters: {'observation_period_num': 47, 'train_rates': 0.7664868439399233, 'learning_rate': 0.00040582120636407806, 'batch_size': 91, 'step_size': 6, 'gamma': 0.9545219355409823}. Best is trial 26 with value: 0.024748308584094048.[0m
[32m[I 2025-01-04 06:43:42,974][0m Trial 49 finished with value: 0.02660507194164482 and parameters: {'observation_period_num': 16, 'train_rates': 0.8478486870380791, 'learning_rate': 0.0009998281343481584, 'batch_size': 130, 'step_size': 8, 'gamma': 0.9197281362157782}. Best is trial 26 with value: 0.024748308584094048.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 06:43:42,984][0m A new study created in memory with name: no-name-eb0f2aa6-b06c-4c9e-aabe-298d58d3fb82[0m
[32m[I 2025-01-04 06:44:12,526][0m Trial 0 finished with value: 0.2707884503110872 and parameters: {'observation_period_num': 200, 'train_rates': 0.7957185858563545, 'learning_rate': 1.1163081721852097e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.9746350777545199}. Best is trial 0 with value: 0.2707884503110872.[0m
[32m[I 2025-01-04 06:44:45,131][0m Trial 1 finished with value: 0.0442014169785079 and parameters: {'observation_period_num': 48, 'train_rates': 0.7547566318371379, 'learning_rate': 0.0007049402937870359, 'batch_size': 166, 'step_size': 13, 'gamma': 0.7838850487632112}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:45:39,989][0m Trial 2 finished with value: 0.9735016226768494 and parameters: {'observation_period_num': 148, 'train_rates': 0.9628136674461292, 'learning_rate': 1.3447416608413878e-06, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8408930385650938}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:48:31,579][0m Trial 3 finished with value: 0.11543145969990208 and parameters: {'observation_period_num': 120, 'train_rates': 0.6931211374005749, 'learning_rate': 0.00023834196525331368, 'batch_size': 26, 'step_size': 5, 'gamma': 0.7500596682764922}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:49:42,698][0m Trial 4 finished with value: 0.07586440396821731 and parameters: {'observation_period_num': 28, 'train_rates': 0.6839297986419495, 'learning_rate': 3.3955339248220833e-05, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8103971025514602}. Best is trial 1 with value: 0.0442014169785079.[0m
Early stopping at epoch 48
[32m[I 2025-01-04 06:50:01,403][0m Trial 5 finished with value: 5.712349465974062 and parameters: {'observation_period_num': 88, 'train_rates': 0.8993705468657905, 'learning_rate': 2.2286965842651415e-06, 'batch_size': 156, 'step_size': 1, 'gamma': 0.8073499248892718}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:51:09,476][0m Trial 6 finished with value: 0.104077259971958 and parameters: {'observation_period_num': 136, 'train_rates': 0.7474552135792374, 'learning_rate': 4.3917368767387415e-05, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9636057081527056}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:52:13,022][0m Trial 7 finished with value: 0.1427227398045629 and parameters: {'observation_period_num': 143, 'train_rates': 0.7031275094197778, 'learning_rate': 0.00010572970197925447, 'batch_size': 72, 'step_size': 8, 'gamma': 0.9057707006159947}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:55:55,397][0m Trial 8 finished with value: 0.07928591600522913 and parameters: {'observation_period_num': 110, 'train_rates': 0.9593470427275395, 'learning_rate': 2.285215429890726e-05, 'batch_size': 25, 'step_size': 9, 'gamma': 0.800710191775544}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:56:23,148][0m Trial 9 finished with value: 2.1535334773514987 and parameters: {'observation_period_num': 60, 'train_rates': 0.8319019897194049, 'learning_rate': 1.5913502855891774e-06, 'batch_size': 209, 'step_size': 6, 'gamma': 0.9132947307226003}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:56:44,039][0m Trial 10 finished with value: 0.06064905754544518 and parameters: {'observation_period_num': 8, 'train_rates': 0.6263445831736401, 'learning_rate': 0.0006308490425707114, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7526755971825446}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:57:04,729][0m Trial 11 finished with value: 0.05836137811696068 and parameters: {'observation_period_num': 6, 'train_rates': 0.6228278013405377, 'learning_rate': 0.0009115092704354872, 'batch_size': 254, 'step_size': 15, 'gamma': 0.7588190319179137}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:57:23,276][0m Trial 12 finished with value: 0.08331902815857231 and parameters: {'observation_period_num': 56, 'train_rates': 0.6070108789064534, 'learning_rate': 0.0007676796880105122, 'batch_size': 255, 'step_size': 12, 'gamma': 0.7847074388220219}. Best is trial 1 with value: 0.0442014169785079.[0m
[32m[I 2025-01-04 06:57:53,008][0m Trial 13 finished with value: 0.031682451910062415 and parameters: {'observation_period_num': 6, 'train_rates': 0.8494045901019301, 'learning_rate': 0.0002758514854438045, 'batch_size': 205, 'step_size': 15, 'gamma': 0.8537496444091829}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 06:58:22,927][0m Trial 14 finished with value: 0.04048080561128822 and parameters: {'observation_period_num': 50, 'train_rates': 0.860003842885495, 'learning_rate': 0.00023069556860156473, 'batch_size': 198, 'step_size': 12, 'gamma': 0.8633907558038995}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 06:58:49,700][0m Trial 15 finished with value: 0.12317300098395284 and parameters: {'observation_period_num': 191, 'train_rates': 0.8629297371011709, 'learning_rate': 0.0002263373521406895, 'batch_size': 205, 'step_size': 11, 'gamma': 0.8669745398915597}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 06:59:19,032][0m Trial 16 finished with value: 0.0644595179208151 and parameters: {'observation_period_num': 81, 'train_rates': 0.9117304269672026, 'learning_rate': 0.000159541210295823, 'batch_size': 210, 'step_size': 11, 'gamma': 0.8553283267228242}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 06:59:46,835][0m Trial 17 finished with value: 0.14298385917648393 and parameters: {'observation_period_num': 250, 'train_rates': 0.8432465098485964, 'learning_rate': 7.399256231400653e-05, 'batch_size': 191, 'step_size': 15, 'gamma': 0.8998541770545989}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:00:31,327][0m Trial 18 finished with value: 0.16468160073846988 and parameters: {'observation_period_num': 29, 'train_rates': 0.9001907855976035, 'learning_rate': 6.320063017997366e-06, 'batch_size': 138, 'step_size': 13, 'gamma': 0.8377017004325987}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:00:57,158][0m Trial 19 finished with value: 0.053177166996745885 and parameters: {'observation_period_num': 88, 'train_rates': 0.8097313370043351, 'learning_rate': 0.0003282865175919469, 'batch_size': 226, 'step_size': 10, 'gamma': 0.9417421047345612}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:01:48,400][0m Trial 20 finished with value: 0.07641177398586704 and parameters: {'observation_period_num': 35, 'train_rates': 0.9244593149269674, 'learning_rate': 7.623344982881832e-05, 'batch_size': 114, 'step_size': 2, 'gamma': 0.8854625443653642}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:02:19,166][0m Trial 21 finished with value: 0.05678523783043695 and parameters: {'observation_period_num': 55, 'train_rates': 0.7384979261327255, 'learning_rate': 0.00044309071083484133, 'batch_size': 170, 'step_size': 13, 'gamma': 0.8316239241893157}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:02:48,651][0m Trial 22 finished with value: 0.03669602085390816 and parameters: {'observation_period_num': 35, 'train_rates': 0.7635276305088611, 'learning_rate': 0.000410618723182665, 'batch_size': 182, 'step_size': 14, 'gamma': 0.8841347075013242}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:03:20,336][0m Trial 23 finished with value: 0.033734727694880305 and parameters: {'observation_period_num': 6, 'train_rates': 0.8709621370987166, 'learning_rate': 0.0001589525774639686, 'batch_size': 184, 'step_size': 14, 'gamma': 0.8737482842350706}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:03:45,926][0m Trial 24 finished with value: 0.04069333882899555 and parameters: {'observation_period_num': 11, 'train_rates': 0.7785182551343497, 'learning_rate': 0.00012144119450920768, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8842750607749759}. Best is trial 13 with value: 0.031682451910062415.[0m
[32m[I 2025-01-04 07:04:17,325][0m Trial 25 finished with value: 0.031653526529060345 and parameters: {'observation_period_num': 24, 'train_rates': 0.8708530512687733, 'learning_rate': 0.0003881430351565454, 'batch_size': 188, 'step_size': 14, 'gamma': 0.9230931725167244}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:04:56,884][0m Trial 26 finished with value: 0.056660755519391426 and parameters: {'observation_period_num': 73, 'train_rates': 0.8745861904919997, 'learning_rate': 5.8058006979465785e-05, 'batch_size': 140, 'step_size': 14, 'gamma': 0.9297205924073818}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:05:23,615][0m Trial 27 finished with value: 0.10490384697914124 and parameters: {'observation_period_num': 18, 'train_rates': 0.9383693882692142, 'learning_rate': 1.7683484935968e-05, 'batch_size': 232, 'step_size': 11, 'gamma': 0.9318402853793841}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:05:59,339][0m Trial 28 finished with value: 0.05688323453068733 and parameters: {'observation_period_num': 104, 'train_rates': 0.8211836653289594, 'learning_rate': 0.00013473747560153286, 'batch_size': 150, 'step_size': 15, 'gamma': 0.9605082743492587}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:06:43,842][0m Trial 29 finished with value: 0.24110893552579205 and parameters: {'observation_period_num': 167, 'train_rates': 0.7923009655806086, 'learning_rate': 6.470665904878888e-06, 'batch_size': 116, 'step_size': 12, 'gamma': 0.9841608077892642}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:07:14,648][0m Trial 30 finished with value: 0.17049186917521605 and parameters: {'observation_period_num': 246, 'train_rates': 0.8789566864551255, 'learning_rate': 0.00040010763547195953, 'batch_size': 181, 'step_size': 14, 'gamma': 0.9479000921925335}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:07:46,095][0m Trial 31 finished with value: 0.04194726971154277 and parameters: {'observation_period_num': 37, 'train_rates': 0.8477575581649219, 'learning_rate': 0.00036506817716684876, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8835530487219402}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:08:17,889][0m Trial 32 finished with value: 0.0362183842220423 and parameters: {'observation_period_num': 29, 'train_rates': 0.7831747904539041, 'learning_rate': 0.0005183031487576478, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9214386981810645}. Best is trial 25 with value: 0.031653526529060345.[0m
[32m[I 2025-01-04 07:08:51,761][0m Trial 33 finished with value: 0.02862495508981333 and parameters: {'observation_period_num': 21, 'train_rates': 0.8112789097108054, 'learning_rate': 0.0006494518584417543, 'batch_size': 163, 'step_size': 13, 'gamma': 0.9138447720260146}. Best is trial 33 with value: 0.02862495508981333.[0m
[32m[I 2025-01-04 07:09:25,509][0m Trial 34 finished with value: 0.025711881745858547 and parameters: {'observation_period_num': 17, 'train_rates': 0.8087379279657719, 'learning_rate': 0.0009490396188823025, 'batch_size': 160, 'step_size': 12, 'gamma': 0.8521578491278692}. Best is trial 34 with value: 0.025711881745858547.[0m
[32m[I 2025-01-04 07:10:01,273][0m Trial 35 finished with value: 0.0461404383489079 and parameters: {'observation_period_num': 66, 'train_rates': 0.8064684059847015, 'learning_rate': 0.0009574484739236948, 'batch_size': 154, 'step_size': 10, 'gamma': 0.8543922414273488}. Best is trial 34 with value: 0.025711881745858547.[0m
[32m[I 2025-01-04 07:10:40,963][0m Trial 36 finished with value: 0.04770897106841357 and parameters: {'observation_period_num': 42, 'train_rates': 0.7279903261511345, 'learning_rate': 0.0005848402825669409, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8298449269765117}. Best is trial 34 with value: 0.025711881745858547.[0m
[32m[I 2025-01-04 07:11:41,688][0m Trial 37 finished with value: 0.03172873724289313 and parameters: {'observation_period_num': 23, 'train_rates': 0.8296923282692568, 'learning_rate': 0.0002681364331064222, 'batch_size': 87, 'step_size': 12, 'gamma': 0.897953195828041}. Best is trial 34 with value: 0.025711881745858547.[0m
[32m[I 2025-01-04 07:12:16,787][0m Trial 38 finished with value: 0.040880549420541694 and parameters: {'observation_period_num': 45, 'train_rates': 0.8896207148104393, 'learning_rate': 0.0009933352207907606, 'batch_size': 169, 'step_size': 10, 'gamma': 0.8466440063689179}. Best is trial 34 with value: 0.025711881745858547.[0m
[32m[I 2025-01-04 07:12:57,590][0m Trial 39 finished with value: 0.017437776550650597 and parameters: {'observation_period_num': 20, 'train_rates': 0.9853519325567738, 'learning_rate': 0.0005858258774353322, 'batch_size': 159, 'step_size': 8, 'gamma': 0.8178265604661301}. Best is trial 39 with value: 0.017437776550650597.[0m
[32m[I 2025-01-04 07:13:46,432][0m Trial 40 finished with value: 0.02683108113706112 and parameters: {'observation_period_num': 19, 'train_rates': 0.9714248906505849, 'learning_rate': 0.0005704885880481655, 'batch_size': 129, 'step_size': 8, 'gamma': 0.7937127881007621}. Best is trial 39 with value: 0.017437776550650597.[0m
[32m[I 2025-01-04 07:14:35,421][0m Trial 41 finished with value: 0.016577795147895813 and parameters: {'observation_period_num': 21, 'train_rates': 0.9860033460337184, 'learning_rate': 0.0006475033857525854, 'batch_size': 130, 'step_size': 8, 'gamma': 0.821796322650269}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:15:26,272][0m Trial 42 finished with value: 0.019536470994353294 and parameters: {'observation_period_num': 21, 'train_rates': 0.9864083944437428, 'learning_rate': 0.0006621141550550512, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8204950501492325}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:16:34,025][0m Trial 43 finished with value: 0.025632761418819427 and parameters: {'observation_period_num': 44, 'train_rates': 0.9841047333462406, 'learning_rate': 0.0006720231607879424, 'batch_size': 91, 'step_size': 7, 'gamma': 0.8153705535824974}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:17:34,721][0m Trial 44 finished with value: 0.058885641396045685 and parameters: {'observation_period_num': 69, 'train_rates': 0.9883887120640651, 'learning_rate': 0.0007766121119380313, 'batch_size': 100, 'step_size': 7, 'gamma': 0.8176102293339222}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:20:01,414][0m Trial 45 finished with value: 0.044848301441677466 and parameters: {'observation_period_num': 46, 'train_rates': 0.9489028502145271, 'learning_rate': 0.00020104182465136762, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8179340873626294}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:21:06,768][0m Trial 46 finished with value: 0.07540661841630936 and parameters: {'observation_period_num': 101, 'train_rates': 0.9881196823092236, 'learning_rate': 0.0007268389684661569, 'batch_size': 92, 'step_size': 7, 'gamma': 0.7726321657703624}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:23:12,454][0m Trial 47 finished with value: 0.026291744571856478 and parameters: {'observation_period_num': 15, 'train_rates': 0.9703618050396212, 'learning_rate': 0.00047792968449920746, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8072116887171988}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:24:23,455][0m Trial 48 finished with value: 0.04385052541397629 and parameters: {'observation_period_num': 54, 'train_rates': 0.9343869923347825, 'learning_rate': 0.00029894145765931723, 'batch_size': 84, 'step_size': 9, 'gamma': 0.8224120688105497}. Best is trial 41 with value: 0.016577795147895813.[0m
[32m[I 2025-01-04 07:25:13,093][0m Trial 49 finished with value: 0.08580580085515976 and parameters: {'observation_period_num': 158, 'train_rates': 0.9555452470112101, 'learning_rate': 0.0007447385103756759, 'batch_size': 120, 'step_size': 8, 'gamma': 0.7931028060325386}. Best is trial 41 with value: 0.016577795147895813.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 07:25:13,104][0m A new study created in memory with name: no-name-50287938-f431-43bd-bd96-09d39874e99a[0m
[32m[I 2025-01-04 07:25:38,796][0m Trial 0 finished with value: 0.13307678763052863 and parameters: {'observation_period_num': 145, 'train_rates': 0.7745254508013802, 'learning_rate': 4.571693873466573e-05, 'batch_size': 207, 'step_size': 13, 'gamma': 0.9849331306609213}. Best is trial 0 with value: 0.13307678763052863.[0m
[32m[I 2025-01-04 07:26:05,660][0m Trial 1 finished with value: 0.3322155197297246 and parameters: {'observation_period_num': 172, 'train_rates': 0.6518852434607181, 'learning_rate': 2.6335373994454764e-05, 'batch_size': 167, 'step_size': 8, 'gamma': 0.81630618052811}. Best is trial 0 with value: 0.13307678763052863.[0m
[32m[I 2025-01-04 07:28:11,933][0m Trial 2 finished with value: 0.22515239504943216 and parameters: {'observation_period_num': 112, 'train_rates': 0.6723501776956369, 'learning_rate': 2.291677314636615e-05, 'batch_size': 35, 'step_size': 3, 'gamma': 0.8229111673700431}. Best is trial 0 with value: 0.13307678763052863.[0m
[32m[I 2025-01-04 07:28:46,903][0m Trial 3 finished with value: 0.4210148231530275 and parameters: {'observation_period_num': 124, 'train_rates': 0.9013295239564848, 'learning_rate': 4.316715078634586e-06, 'batch_size': 161, 'step_size': 11, 'gamma': 0.9365989296006587}. Best is trial 0 with value: 0.13307678763052863.[0m
[32m[I 2025-01-04 07:29:14,658][0m Trial 4 finished with value: 0.11057502263917318 and parameters: {'observation_period_num': 135, 'train_rates': 0.7929994400925111, 'learning_rate': 0.0008311182832037531, 'batch_size': 202, 'step_size': 4, 'gamma': 0.9823077444037012}. Best is trial 4 with value: 0.11057502263917318.[0m
[32m[I 2025-01-04 07:29:43,282][0m Trial 5 finished with value: 0.28181045705621893 and parameters: {'observation_period_num': 113, 'train_rates': 0.6706885145179767, 'learning_rate': 2.1121643235016693e-05, 'batch_size': 170, 'step_size': 8, 'gamma': 0.825206474035195}. Best is trial 4 with value: 0.11057502263917318.[0m
[32m[I 2025-01-04 07:30:06,767][0m Trial 6 finished with value: 0.09137374524365773 and parameters: {'observation_period_num': 87, 'train_rates': 0.8081623895154642, 'learning_rate': 0.0005072157484941098, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8059359227781959}. Best is trial 6 with value: 0.09137374524365773.[0m
[32m[I 2025-01-04 07:30:41,349][0m Trial 7 finished with value: 0.46642632246737753 and parameters: {'observation_period_num': 159, 'train_rates': 0.7628996585127039, 'learning_rate': 3.995956291877167e-06, 'batch_size': 148, 'step_size': 8, 'gamma': 0.9357275828154664}. Best is trial 6 with value: 0.09137374524365773.[0m
[32m[I 2025-01-04 07:31:16,997][0m Trial 8 finished with value: 0.16679892338794483 and parameters: {'observation_period_num': 69, 'train_rates': 0.8553412973251362, 'learning_rate': 1.1112880786491674e-05, 'batch_size': 161, 'step_size': 9, 'gamma': 0.9541134736725514}. Best is trial 6 with value: 0.09137374524365773.[0m
[32m[I 2025-01-04 07:31:47,867][0m Trial 9 finished with value: 0.13549764630904637 and parameters: {'observation_period_num': 91, 'train_rates': 0.7390788912828721, 'learning_rate': 3.6056049436552306e-05, 'batch_size': 165, 'step_size': 3, 'gamma': 0.9658274821915191}. Best is trial 6 with value: 0.09137374524365773.[0m
[32m[I 2025-01-04 07:32:14,686][0m Trial 10 finished with value: 0.040891796350479126 and parameters: {'observation_period_num': 7, 'train_rates': 0.9873641512469292, 'learning_rate': 0.0006577471759464785, 'batch_size': 254, 'step_size': 5, 'gamma': 0.7636782302002623}. Best is trial 10 with value: 0.040891796350479126.[0m
Early stopping at epoch 49
[32m[I 2025-01-04 07:32:28,556][0m Trial 11 finished with value: 0.17641401290893555 and parameters: {'observation_period_num': 23, 'train_rates': 0.9377049901132917, 'learning_rate': 0.0008092809741243669, 'batch_size': 238, 'step_size': 1, 'gamma': 0.7513038168565801}. Best is trial 10 with value: 0.040891796350479126.[0m
[32m[I 2025-01-04 07:32:55,596][0m Trial 12 finished with value: 0.1986463963985443 and parameters: {'observation_period_num': 226, 'train_rates': 0.9795620244059724, 'learning_rate': 0.0001821607147015822, 'batch_size': 241, 'step_size': 5, 'gamma': 0.7604317553209066}. Best is trial 10 with value: 0.040891796350479126.[0m
[32m[I 2025-01-04 07:33:50,455][0m Trial 13 finished with value: 0.03630840374433545 and parameters: {'observation_period_num': 10, 'train_rates': 0.8468682299532856, 'learning_rate': 0.00017559043026646416, 'batch_size': 101, 'step_size': 6, 'gamma': 0.7902939760209559}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:34:50,381][0m Trial 14 finished with value: 0.036310914101192304 and parameters: {'observation_period_num': 6, 'train_rates': 0.877091026130849, 'learning_rate': 0.00015013882000317007, 'batch_size': 93, 'step_size': 6, 'gamma': 0.8666430946346819}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:35:52,975][0m Trial 15 finished with value: 0.039105682390577656 and parameters: {'observation_period_num': 40, 'train_rates': 0.8542414738389559, 'learning_rate': 0.00013409243031679212, 'batch_size': 86, 'step_size': 6, 'gamma': 0.8882237034050379}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:36:46,790][0m Trial 16 finished with value: 0.03891533270628009 and parameters: {'observation_period_num': 55, 'train_rates': 0.8626398409226187, 'learning_rate': 0.00015627438421927792, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8796846579097992}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:37:42,554][0m Trial 17 finished with value: 0.39716199906463295 and parameters: {'observation_period_num': 9, 'train_rates': 0.9207085014674627, 'learning_rate': 1.2183508802904567e-06, 'batch_size': 106, 'step_size': 15, 'gamma': 0.8537249291306837}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:39:18,585][0m Trial 18 finished with value: 0.17417901476561012 and parameters: {'observation_period_num': 203, 'train_rates': 0.7210006251338696, 'learning_rate': 0.0002900516728760538, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9105787801586914}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:40:18,068][0m Trial 19 finished with value: 0.10297372817285458 and parameters: {'observation_period_num': 40, 'train_rates': 0.609503058681059, 'learning_rate': 7.084230342767434e-05, 'batch_size': 66, 'step_size': 10, 'gamma': 0.7828842699960284}. Best is trial 13 with value: 0.03630840374433545.[0m
Early stopping at epoch 77
[32m[I 2025-01-04 07:40:52,634][0m Trial 20 finished with value: 0.16808269730593897 and parameters: {'observation_period_num': 34, 'train_rates': 0.8270587588319155, 'learning_rate': 8.445311832700267e-05, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8515274669815627}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:42:00,973][0m Trial 21 finished with value: 0.08094546142695606 and parameters: {'observation_period_num': 61, 'train_rates': 0.8807776575578147, 'learning_rate': 0.0002313059860562055, 'batch_size': 82, 'step_size': 12, 'gamma': 0.8847926549264841}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:42:46,888][0m Trial 22 finished with value: 0.038650330081416134 and parameters: {'observation_period_num': 57, 'train_rates': 0.8450879483883127, 'learning_rate': 0.00037887099849793056, 'batch_size': 118, 'step_size': 7, 'gamma': 0.8446125402689834}. Best is trial 13 with value: 0.03630840374433545.[0m
[32m[I 2025-01-04 07:43:31,643][0m Trial 23 finished with value: 0.03032842266111402 and parameters: {'observation_period_num': 10, 'train_rates': 0.8259124470763493, 'learning_rate': 0.00037778678374666855, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8497957718532873}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:44:16,469][0m Trial 24 finished with value: 0.062018481393655144 and parameters: {'observation_period_num': 5, 'train_rates': 0.9420786817363483, 'learning_rate': 8.522404092414365e-05, 'batch_size': 133, 'step_size': 6, 'gamma': 0.7926507264005563}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:45:53,517][0m Trial 25 finished with value: 0.034194459135715775 and parameters: {'observation_period_num': 23, 'train_rates': 0.8936524057712718, 'learning_rate': 0.0003925764798099639, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9054495055413696}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:47:17,611][0m Trial 26 finished with value: 0.04109883738467698 and parameters: {'observation_period_num': 28, 'train_rates': 0.8184092081502826, 'learning_rate': 0.00040567638909073234, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9130429638057628}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:48:52,330][0m Trial 27 finished with value: 0.08548794038317821 and parameters: {'observation_period_num': 80, 'train_rates': 0.9060737896642019, 'learning_rate': 0.000299748287917722, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9028711103921266}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:50:10,556][0m Trial 28 finished with value: 0.04562413588596078 and parameters: {'observation_period_num': 26, 'train_rates': 0.9466865317601431, 'learning_rate': 0.00047070188254513123, 'batch_size': 75, 'step_size': 4, 'gamma': 0.7828615246877083}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:51:57,820][0m Trial 29 finished with value: 0.06091636453989808 and parameters: {'observation_period_num': 45, 'train_rates': 0.7748361652723842, 'learning_rate': 0.0009775703170686353, 'batch_size': 46, 'step_size': 7, 'gamma': 0.843653053961078}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:52:27,386][0m Trial 30 finished with value: 0.08065408097474643 and parameters: {'observation_period_num': 21, 'train_rates': 0.7832688662840098, 'learning_rate': 4.783514936966653e-05, 'batch_size': 189, 'step_size': 5, 'gamma': 0.8690455401266051}. Best is trial 23 with value: 0.03032842266111402.[0m
[32m[I 2025-01-04 07:56:35,552][0m Trial 31 finished with value: 0.02193335571291624 and parameters: {'observation_period_num': 5, 'train_rates': 0.88160448450886, 'learning_rate': 0.00011461454332786872, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8652707023637851}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:01:22,659][0m Trial 32 finished with value: 0.029294169876450227 and parameters: {'observation_period_num': 20, 'train_rates': 0.8890151962085507, 'learning_rate': 0.00010948788225659993, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8334468962461419}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:06:35,488][0m Trial 33 finished with value: 0.03896733373403549 and parameters: {'observation_period_num': 46, 'train_rates': 0.8832337235635238, 'learning_rate': 5.511938747774121e-05, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8327436444810784}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:10:11,884][0m Trial 34 finished with value: 0.07079726705948512 and parameters: {'observation_period_num': 75, 'train_rates': 0.9032251839858115, 'learning_rate': 0.00011798734487250312, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8600806542477679}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:13:02,016][0m Trial 35 finished with value: 0.18517744359762772 and parameters: {'observation_period_num': 191, 'train_rates': 0.9584720827054675, 'learning_rate': 0.00025588350954946885, 'batch_size': 32, 'step_size': 14, 'gamma': 0.900211253420788}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:18:51,210][0m Trial 36 finished with value: 0.03206954277417761 and parameters: {'observation_period_num': 21, 'train_rates': 0.9201185686019583, 'learning_rate': 0.00010067853879375576, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8091726331801863}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:22:45,974][0m Trial 37 finished with value: 0.08808276579751596 and parameters: {'observation_period_num': 102, 'train_rates': 0.9189213509929604, 'learning_rate': 1.7631654835184847e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8187383323314453}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:25:05,860][0m Trial 38 finished with value: 0.07189393867183169 and parameters: {'observation_period_num': 137, 'train_rates': 0.8063753642521109, 'learning_rate': 9.607373172488942e-05, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8079901928872487}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:26:50,422][0m Trial 39 finished with value: 0.1304144321677414 and parameters: {'observation_period_num': 252, 'train_rates': 0.833986809693285, 'learning_rate': 3.269234510583324e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.8301272584427671}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:30:52,976][0m Trial 40 finished with value: 0.07772600751470875 and parameters: {'observation_period_num': 152, 'train_rates': 0.9209606169189253, 'learning_rate': 6.012834882714802e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.8087397667471361}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:32:33,473][0m Trial 41 finished with value: 0.05126309891541799 and parameters: {'observation_period_num': 23, 'train_rates': 0.8873103900761525, 'learning_rate': 0.0005212183648967025, 'batch_size': 55, 'step_size': 7, 'gamma': 0.837363512844773}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:35:07,377][0m Trial 42 finished with value: 0.03531056348807537 and parameters: {'observation_period_num': 19, 'train_rates': 0.9648595818264323, 'learning_rate': 0.00021743895936914936, 'batch_size': 38, 'step_size': 4, 'gamma': 0.8804385553338566}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:38:00,813][0m Trial 43 finished with value: 0.0552478015967511 and parameters: {'observation_period_num': 54, 'train_rates': 0.8680570015997029, 'learning_rate': 1.3776712388600025e-05, 'batch_size': 31, 'step_size': 9, 'gamma': 0.9301235463013049}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:43:03,771][0m Trial 44 finished with value: 0.038616497839083434 and parameters: {'observation_period_num': 35, 'train_rates': 0.8957292589536316, 'learning_rate': 0.000646133514524399, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9260745083786858}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:44:55,579][0m Trial 45 finished with value: 0.03872503357764223 and parameters: {'observation_period_num': 15, 'train_rates': 0.9314237618394273, 'learning_rate': 0.00010813956214502853, 'batch_size': 51, 'step_size': 5, 'gamma': 0.8228122690969358}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:47:22,071][0m Trial 46 finished with value: 0.10513758481866793 and parameters: {'observation_period_num': 64, 'train_rates': 0.757350571520165, 'learning_rate': 7.556308682703519e-06, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9499089242843026}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:48:41,882][0m Trial 47 finished with value: 0.21402411296209936 and parameters: {'observation_period_num': 119, 'train_rates': 0.9095787354574197, 'learning_rate': 3.9517208920093215e-05, 'batch_size': 68, 'step_size': 2, 'gamma': 0.8593745595674843}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:49:14,045][0m Trial 48 finished with value: 0.2219048662079593 and parameters: {'observation_period_num': 34, 'train_rates': 0.8384708322921296, 'learning_rate': 2.4105834839354165e-05, 'batch_size': 179, 'step_size': 4, 'gamma': 0.843642461753795}. Best is trial 31 with value: 0.02193335571291624.[0m
[32m[I 2025-01-04 08:54:28,943][0m Trial 49 finished with value: 0.028022975067064266 and parameters: {'observation_period_num': 16, 'train_rates': 0.7972529231773271, 'learning_rate': 0.00018712098061919404, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7992673398149451}. Best is trial 31 with value: 0.02193335571291624.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 10, 'train_rates': 0.9893632335324659, 'learning_rate': 0.00017030802740725277, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8167557500582184}
Epoch 1/300, trend Loss: 0.1690 | 0.0821
Epoch 2/300, trend Loss: 0.1052 | 0.0567
Epoch 3/300, trend Loss: 0.0949 | 0.0600
Epoch 4/300, trend Loss: 0.0916 | 0.0600
Epoch 5/300, trend Loss: 0.0919 | 0.0517
Epoch 6/300, trend Loss: 0.0961 | 0.0471
Epoch 7/300, trend Loss: 0.0855 | 0.0440
Epoch 8/300, trend Loss: 0.0803 | 0.0399
Epoch 9/300, trend Loss: 0.0773 | 0.0371
Epoch 10/300, trend Loss: 0.0742 | 0.0349
Epoch 11/300, trend Loss: 0.0713 | 0.0337
Epoch 12/300, trend Loss: 0.0689 | 0.0335
Epoch 13/300, trend Loss: 0.0669 | 0.0334
Epoch 14/300, trend Loss: 0.0650 | 0.0311
Epoch 15/300, trend Loss: 0.0635 | 0.0311
Epoch 16/300, trend Loss: 0.0623 | 0.0319
Epoch 17/300, trend Loss: 0.0616 | 0.0310
Epoch 18/300, trend Loss: 0.0610 | 0.0299
Epoch 19/300, trend Loss: 0.0604 | 0.0289
Epoch 20/300, trend Loss: 0.0598 | 0.0281
Epoch 21/300, trend Loss: 0.0591 | 0.0276
Epoch 22/300, trend Loss: 0.0586 | 0.0270
Epoch 23/300, trend Loss: 0.0582 | 0.0264
Epoch 24/300, trend Loss: 0.0578 | 0.0257
Epoch 25/300, trend Loss: 0.0574 | 0.0251
Epoch 26/300, trend Loss: 0.0571 | 0.0244
Epoch 27/300, trend Loss: 0.0565 | 0.0240
Epoch 28/300, trend Loss: 0.0561 | 0.0232
Epoch 29/300, trend Loss: 0.0558 | 0.0225
Epoch 30/300, trend Loss: 0.0555 | 0.0217
Epoch 31/300, trend Loss: 0.0552 | 0.0209
Epoch 32/300, trend Loss: 0.0550 | 0.0201
Epoch 33/300, trend Loss: 0.0547 | 0.0193
Epoch 34/300, trend Loss: 0.0541 | 0.0191
Epoch 35/300, trend Loss: 0.0538 | 0.0184
Epoch 36/300, trend Loss: 0.0535 | 0.0180
Epoch 37/300, trend Loss: 0.0534 | 0.0177
Epoch 38/300, trend Loss: 0.0532 | 0.0177
Epoch 39/300, trend Loss: 0.0530 | 0.0178
Epoch 40/300, trend Loss: 0.0531 | 0.0160
Epoch 41/300, trend Loss: 0.0525 | 0.0153
Epoch 42/300, trend Loss: 0.0521 | 0.0152
Epoch 43/300, trend Loss: 0.0517 | 0.0150
Epoch 44/300, trend Loss: 0.0514 | 0.0147
Epoch 45/300, trend Loss: 0.0511 | 0.0145
Epoch 46/300, trend Loss: 0.0509 | 0.0143
Epoch 47/300, trend Loss: 0.0506 | 0.0145
Epoch 48/300, trend Loss: 0.0505 | 0.0142
Epoch 49/300, trend Loss: 0.0503 | 0.0141
Epoch 50/300, trend Loss: 0.0501 | 0.0140
Epoch 51/300, trend Loss: 0.0499 | 0.0139
Epoch 52/300, trend Loss: 0.0497 | 0.0139
Epoch 53/300, trend Loss: 0.0494 | 0.0143
Epoch 54/300, trend Loss: 0.0493 | 0.0144
Epoch 55/300, trend Loss: 0.0493 | 0.0146
Epoch 56/300, trend Loss: 0.0494 | 0.0150
Epoch 57/300, trend Loss: 0.0494 | 0.0152
Epoch 58/300, trend Loss: 0.0495 | 0.0151
Epoch 59/300, trend Loss: 0.0494 | 0.0147
Epoch 60/300, trend Loss: 0.0492 | 0.0150
Epoch 61/300, trend Loss: 0.0490 | 0.0147
Epoch 62/300, trend Loss: 0.0489 | 0.0142
Epoch 63/300, trend Loss: 0.0487 | 0.0136
Epoch 64/300, trend Loss: 0.0484 | 0.0132
Epoch 65/300, trend Loss: 0.0483 | 0.0130
Epoch 66/300, trend Loss: 0.0481 | 0.0129
Epoch 67/300, trend Loss: 0.0480 | 0.0129
Epoch 68/300, trend Loss: 0.0479 | 0.0128
Epoch 69/300, trend Loss: 0.0478 | 0.0128
Epoch 70/300, trend Loss: 0.0477 | 0.0128
Epoch 71/300, trend Loss: 0.0476 | 0.0128
Epoch 72/300, trend Loss: 0.0475 | 0.0127
Epoch 73/300, trend Loss: 0.0475 | 0.0128
Epoch 74/300, trend Loss: 0.0474 | 0.0128
Epoch 75/300, trend Loss: 0.0473 | 0.0128
Epoch 76/300, trend Loss: 0.0473 | 0.0128
Epoch 77/300, trend Loss: 0.0472 | 0.0129
Epoch 78/300, trend Loss: 0.0472 | 0.0129
Epoch 79/300, trend Loss: 0.0472 | 0.0127
Epoch 80/300, trend Loss: 0.0472 | 0.0127
Epoch 81/300, trend Loss: 0.0471 | 0.0127
Epoch 82/300, trend Loss: 0.0471 | 0.0126
Epoch 83/300, trend Loss: 0.0471 | 0.0126
Epoch 84/300, trend Loss: 0.0471 | 0.0126
Epoch 85/300, trend Loss: 0.0471 | 0.0126
Epoch 86/300, trend Loss: 0.0471 | 0.0124
Epoch 87/300, trend Loss: 0.0472 | 0.0124
Epoch 88/300, trend Loss: 0.0471 | 0.0125
Epoch 89/300, trend Loss: 0.0471 | 0.0125
Epoch 90/300, trend Loss: 0.0470 | 0.0125
Epoch 91/300, trend Loss: 0.0470 | 0.0125
Epoch 92/300, trend Loss: 0.0470 | 0.0125
Epoch 93/300, trend Loss: 0.0470 | 0.0125
Epoch 94/300, trend Loss: 0.0469 | 0.0124
Epoch 95/300, trend Loss: 0.0468 | 0.0123
Epoch 96/300, trend Loss: 0.0467 | 0.0122
Epoch 97/300, trend Loss: 0.0466 | 0.0122
Epoch 98/300, trend Loss: 0.0465 | 0.0121
Epoch 99/300, trend Loss: 0.0465 | 0.0121
Epoch 100/300, trend Loss: 0.0464 | 0.0121
Epoch 101/300, trend Loss: 0.0464 | 0.0120
Epoch 102/300, trend Loss: 0.0463 | 0.0120
Epoch 103/300, trend Loss: 0.0462 | 0.0119
Epoch 104/300, trend Loss: 0.0462 | 0.0119
Epoch 105/300, trend Loss: 0.0462 | 0.0119
Epoch 106/300, trend Loss: 0.0462 | 0.0119
Epoch 107/300, trend Loss: 0.0461 | 0.0119
Epoch 108/300, trend Loss: 0.0461 | 0.0119
Epoch 109/300, trend Loss: 0.0461 | 0.0118
Epoch 110/300, trend Loss: 0.0461 | 0.0118
Epoch 111/300, trend Loss: 0.0460 | 0.0118
Epoch 112/300, trend Loss: 0.0460 | 0.0119
Epoch 113/300, trend Loss: 0.0460 | 0.0119
Epoch 114/300, trend Loss: 0.0460 | 0.0118
Epoch 115/300, trend Loss: 0.0460 | 0.0118
Epoch 116/300, trend Loss: 0.0459 | 0.0118
Epoch 117/300, trend Loss: 0.0459 | 0.0118
Epoch 118/300, trend Loss: 0.0459 | 0.0119
Epoch 119/300, trend Loss: 0.0459 | 0.0119
Epoch 120/300, trend Loss: 0.0459 | 0.0119
Epoch 121/300, trend Loss: 0.0458 | 0.0119
Epoch 122/300, trend Loss: 0.0458 | 0.0119
Epoch 123/300, trend Loss: 0.0458 | 0.0119
Epoch 124/300, trend Loss: 0.0457 | 0.0119
Epoch 125/300, trend Loss: 0.0457 | 0.0119
Epoch 126/300, trend Loss: 0.0457 | 0.0120
Epoch 127/300, trend Loss: 0.0457 | 0.0120
Epoch 128/300, trend Loss: 0.0457 | 0.0120
Epoch 129/300, trend Loss: 0.0456 | 0.0120
Epoch 130/300, trend Loss: 0.0456 | 0.0120
Epoch 131/300, trend Loss: 0.0456 | 0.0122
Epoch 132/300, trend Loss: 0.0456 | 0.0122
Epoch 133/300, trend Loss: 0.0456 | 0.0122
Epoch 134/300, trend Loss: 0.0456 | 0.0122
Epoch 135/300, trend Loss: 0.0455 | 0.0122
Epoch 136/300, trend Loss: 0.0455 | 0.0122
Epoch 137/300, trend Loss: 0.0455 | 0.0122
Epoch 138/300, trend Loss: 0.0455 | 0.0123
Epoch 139/300, trend Loss: 0.0455 | 0.0122
Epoch 140/300, trend Loss: 0.0454 | 0.0122
Epoch 141/300, trend Loss: 0.0454 | 0.0121
Epoch 142/300, trend Loss: 0.0454 | 0.0121
Epoch 143/300, trend Loss: 0.0454 | 0.0120
Epoch 144/300, trend Loss: 0.0454 | 0.0120
Epoch 145/300, trend Loss: 0.0453 | 0.0120
Epoch 146/300, trend Loss: 0.0453 | 0.0119
Epoch 147/300, trend Loss: 0.0453 | 0.0119
Epoch 148/300, trend Loss: 0.0453 | 0.0118
Epoch 149/300, trend Loss: 0.0453 | 0.0118
Epoch 150/300, trend Loss: 0.0453 | 0.0118
Epoch 151/300, trend Loss: 0.0453 | 0.0117
Epoch 152/300, trend Loss: 0.0452 | 0.0117
Epoch 153/300, trend Loss: 0.0452 | 0.0117
Epoch 154/300, trend Loss: 0.0452 | 0.0117
Epoch 155/300, trend Loss: 0.0452 | 0.0116
Epoch 156/300, trend Loss: 0.0452 | 0.0116
Epoch 157/300, trend Loss: 0.0452 | 0.0116
Epoch 158/300, trend Loss: 0.0452 | 0.0116
Epoch 159/300, trend Loss: 0.0452 | 0.0116
Epoch 160/300, trend Loss: 0.0451 | 0.0116
Epoch 161/300, trend Loss: 0.0451 | 0.0116
Epoch 162/300, trend Loss: 0.0451 | 0.0116
Epoch 163/300, trend Loss: 0.0451 | 0.0116
Epoch 164/300, trend Loss: 0.0451 | 0.0116
Epoch 165/300, trend Loss: 0.0451 | 0.0116
Epoch 166/300, trend Loss: 0.0451 | 0.0116
Epoch 167/300, trend Loss: 0.0451 | 0.0116
Epoch 168/300, trend Loss: 0.0451 | 0.0116
Epoch 169/300, trend Loss: 0.0451 | 0.0116
Epoch 170/300, trend Loss: 0.0451 | 0.0116
Epoch 171/300, trend Loss: 0.0451 | 0.0116
Epoch 172/300, trend Loss: 0.0451 | 0.0116
Epoch 173/300, trend Loss: 0.0450 | 0.0116
Epoch 174/300, trend Loss: 0.0450 | 0.0116
Epoch 175/300, trend Loss: 0.0450 | 0.0116
Epoch 176/300, trend Loss: 0.0450 | 0.0116
Epoch 177/300, trend Loss: 0.0450 | 0.0116
Epoch 178/300, trend Loss: 0.0450 | 0.0116
Epoch 179/300, trend Loss: 0.0450 | 0.0116
Epoch 180/300, trend Loss: 0.0450 | 0.0116
Epoch 181/300, trend Loss: 0.0450 | 0.0116
Epoch 182/300, trend Loss: 0.0450 | 0.0117
Epoch 183/300, trend Loss: 0.0450 | 0.0117
Epoch 184/300, trend Loss: 0.0450 | 0.0117
Epoch 185/300, trend Loss: 0.0450 | 0.0117
Epoch 186/300, trend Loss: 0.0450 | 0.0117
Epoch 187/300, trend Loss: 0.0450 | 0.0117
Epoch 188/300, trend Loss: 0.0450 | 0.0117
Epoch 189/300, trend Loss: 0.0450 | 0.0117
Epoch 190/300, trend Loss: 0.0450 | 0.0117
Epoch 191/300, trend Loss: 0.0450 | 0.0117
Epoch 192/300, trend Loss: 0.0450 | 0.0117
Epoch 193/300, trend Loss: 0.0450 | 0.0117
Epoch 194/300, trend Loss: 0.0450 | 0.0117
Epoch 195/300, trend Loss: 0.0450 | 0.0117
Epoch 196/300, trend Loss: 0.0450 | 0.0117
Epoch 197/300, trend Loss: 0.0449 | 0.0117
Epoch 198/300, trend Loss: 0.0449 | 0.0117
Epoch 199/300, trend Loss: 0.0449 | 0.0117
Epoch 200/300, trend Loss: 0.0449 | 0.0117
Epoch 201/300, trend Loss: 0.0449 | 0.0117
Epoch 202/300, trend Loss: 0.0449 | 0.0117
Epoch 203/300, trend Loss: 0.0449 | 0.0117
Epoch 204/300, trend Loss: 0.0449 | 0.0117
Epoch 205/300, trend Loss: 0.0449 | 0.0117
Epoch 206/300, trend Loss: 0.0449 | 0.0117
Epoch 207/300, trend Loss: 0.0449 | 0.0117
Epoch 208/300, trend Loss: 0.0449 | 0.0117
Epoch 209/300, trend Loss: 0.0449 | 0.0117
Epoch 210/300, trend Loss: 0.0449 | 0.0117
Epoch 211/300, trend Loss: 0.0449 | 0.0117
Epoch 212/300, trend Loss: 0.0449 | 0.0117
Epoch 213/300, trend Loss: 0.0449 | 0.0117
Epoch 214/300, trend Loss: 0.0449 | 0.0117
Epoch 215/300, trend Loss: 0.0449 | 0.0117
Epoch 216/300, trend Loss: 0.0449 | 0.0117
Epoch 217/300, trend Loss: 0.0449 | 0.0117
Epoch 218/300, trend Loss: 0.0449 | 0.0117
Epoch 219/300, trend Loss: 0.0449 | 0.0117
Epoch 220/300, trend Loss: 0.0449 | 0.0117
Epoch 221/300, trend Loss: 0.0449 | 0.0117
Epoch 222/300, trend Loss: 0.0449 | 0.0117
Epoch 223/300, trend Loss: 0.0449 | 0.0117
Epoch 224/300, trend Loss: 0.0449 | 0.0117
Epoch 225/300, trend Loss: 0.0449 | 0.0117
Epoch 226/300, trend Loss: 0.0449 | 0.0117
Epoch 227/300, trend Loss: 0.0449 | 0.0117
Epoch 228/300, trend Loss: 0.0449 | 0.0117
Epoch 229/300, trend Loss: 0.0449 | 0.0117
Epoch 230/300, trend Loss: 0.0449 | 0.0117
Epoch 231/300, trend Loss: 0.0449 | 0.0117
Epoch 232/300, trend Loss: 0.0449 | 0.0117
Epoch 233/300, trend Loss: 0.0449 | 0.0117
Epoch 234/300, trend Loss: 0.0449 | 0.0117
Epoch 235/300, trend Loss: 0.0449 | 0.0117
Epoch 236/300, trend Loss: 0.0449 | 0.0117
Epoch 237/300, trend Loss: 0.0449 | 0.0117
Epoch 238/300, trend Loss: 0.0449 | 0.0117
Epoch 239/300, trend Loss: 0.0449 | 0.0117
Epoch 240/300, trend Loss: 0.0449 | 0.0117
Epoch 241/300, trend Loss: 0.0449 | 0.0117
Epoch 242/300, trend Loss: 0.0449 | 0.0117
Epoch 243/300, trend Loss: 0.0449 | 0.0117
Epoch 244/300, trend Loss: 0.0449 | 0.0117
Epoch 245/300, trend Loss: 0.0449 | 0.0117
Epoch 246/300, trend Loss: 0.0449 | 0.0117
Epoch 247/300, trend Loss: 0.0449 | 0.0117
Epoch 248/300, trend Loss: 0.0449 | 0.0117
Epoch 249/300, trend Loss: 0.0449 | 0.0117
Epoch 250/300, trend Loss: 0.0449 | 0.0117
Epoch 251/300, trend Loss: 0.0449 | 0.0117
Epoch 252/300, trend Loss: 0.0449 | 0.0117
Epoch 253/300, trend Loss: 0.0449 | 0.0117
Epoch 254/300, trend Loss: 0.0449 | 0.0117
Epoch 255/300, trend Loss: 0.0449 | 0.0117
Epoch 256/300, trend Loss: 0.0449 | 0.0117
Epoch 257/300, trend Loss: 0.0449 | 0.0117
Epoch 258/300, trend Loss: 0.0449 | 0.0117
Epoch 259/300, trend Loss: 0.0449 | 0.0117
Epoch 260/300, trend Loss: 0.0449 | 0.0117
Epoch 261/300, trend Loss: 0.0449 | 0.0117
Epoch 262/300, trend Loss: 0.0449 | 0.0117
Epoch 263/300, trend Loss: 0.0449 | 0.0117
Epoch 264/300, trend Loss: 0.0449 | 0.0117
Epoch 265/300, trend Loss: 0.0449 | 0.0117
Epoch 266/300, trend Loss: 0.0449 | 0.0117
Epoch 267/300, trend Loss: 0.0449 | 0.0117
Epoch 268/300, trend Loss: 0.0449 | 0.0117
Epoch 269/300, trend Loss: 0.0449 | 0.0117
Epoch 270/300, trend Loss: 0.0449 | 0.0117
Epoch 271/300, trend Loss: 0.0449 | 0.0117
Epoch 272/300, trend Loss: 0.0449 | 0.0117
Epoch 273/300, trend Loss: 0.0449 | 0.0117
Epoch 274/300, trend Loss: 0.0449 | 0.0117
Epoch 275/300, trend Loss: 0.0449 | 0.0117
Epoch 276/300, trend Loss: 0.0449 | 0.0117
Epoch 277/300, trend Loss: 0.0449 | 0.0117
Epoch 278/300, trend Loss: 0.0449 | 0.0117
Epoch 279/300, trend Loss: 0.0449 | 0.0117
Epoch 280/300, trend Loss: 0.0449 | 0.0117
Epoch 281/300, trend Loss: 0.0449 | 0.0117
Epoch 282/300, trend Loss: 0.0449 | 0.0117
Epoch 283/300, trend Loss: 0.0449 | 0.0117
Epoch 284/300, trend Loss: 0.0449 | 0.0117
Epoch 285/300, trend Loss: 0.0449 | 0.0117
Epoch 286/300, trend Loss: 0.0449 | 0.0117
Epoch 287/300, trend Loss: 0.0449 | 0.0117
Epoch 288/300, trend Loss: 0.0449 | 0.0117
Epoch 289/300, trend Loss: 0.0449 | 0.0117
Epoch 290/300, trend Loss: 0.0449 | 0.0117
Epoch 291/300, trend Loss: 0.0449 | 0.0117
Epoch 292/300, trend Loss: 0.0449 | 0.0117
Epoch 293/300, trend Loss: 0.0449 | 0.0117
Epoch 294/300, trend Loss: 0.0449 | 0.0117
Epoch 295/300, trend Loss: 0.0449 | 0.0117
Epoch 296/300, trend Loss: 0.0449 | 0.0117
Epoch 297/300, trend Loss: 0.0449 | 0.0117
Epoch 298/300, trend Loss: 0.0449 | 0.0117
Epoch 299/300, trend Loss: 0.0449 | 0.0117
Epoch 300/300, trend Loss: 0.0449 | 0.0117
Training seasonal_0 component with params: {'observation_period_num': 7, 'train_rates': 0.9888345190812218, 'learning_rate': 0.0005065208855896543, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8671270107279989}
Epoch 1/300, seasonal_0 Loss: 0.2302 | 0.0999
Epoch 2/300, seasonal_0 Loss: 0.1166 | 0.0682
Epoch 3/300, seasonal_0 Loss: 0.1000 | 0.0801
Epoch 4/300, seasonal_0 Loss: 0.0981 | 0.0727
Epoch 5/300, seasonal_0 Loss: 0.0967 | 0.0684
Epoch 6/300, seasonal_0 Loss: 0.0958 | 0.0684
Epoch 7/300, seasonal_0 Loss: 0.0973 | 0.1182
Epoch 8/300, seasonal_0 Loss: 0.0995 | 0.0547
Epoch 9/300, seasonal_0 Loss: 0.0814 | 0.0524
Epoch 10/300, seasonal_0 Loss: 0.0779 | 0.0502
Epoch 11/300, seasonal_0 Loss: 0.0755 | 0.0486
Epoch 12/300, seasonal_0 Loss: 0.0727 | 0.0462
Epoch 13/300, seasonal_0 Loss: 0.0699 | 0.0410
Epoch 14/300, seasonal_0 Loss: 0.0682 | 0.0388
Epoch 15/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 16/300, seasonal_0 Loss: 0.0664 | 0.0359
Epoch 17/300, seasonal_0 Loss: 0.0659 | 0.0355
Epoch 18/300, seasonal_0 Loss: 0.0655 | 0.0353
Epoch 19/300, seasonal_0 Loss: 0.0650 | 0.0338
Epoch 20/300, seasonal_0 Loss: 0.0648 | 0.0361
Epoch 21/300, seasonal_0 Loss: 0.0643 | 0.0361
Epoch 22/300, seasonal_0 Loss: 0.0653 | 0.0343
Epoch 23/300, seasonal_0 Loss: 0.0660 | 0.0345
Epoch 24/300, seasonal_0 Loss: 0.0647 | 0.0355
Epoch 25/300, seasonal_0 Loss: 0.0634 | 0.0353
Epoch 26/300, seasonal_0 Loss: 0.0654 | 0.0378
Epoch 27/300, seasonal_0 Loss: 0.0631 | 0.0354
Epoch 28/300, seasonal_0 Loss: 0.0639 | 0.0362
Epoch 29/300, seasonal_0 Loss: 0.0756 | 0.0415
Epoch 30/300, seasonal_0 Loss: 0.0696 | 0.0399
Epoch 31/300, seasonal_0 Loss: 0.0698 | 0.0420
Epoch 32/300, seasonal_0 Loss: 0.0653 | 0.0420
Epoch 33/300, seasonal_0 Loss: 0.0646 | 0.0387
Epoch 34/300, seasonal_0 Loss: 0.0626 | 0.0350
Epoch 35/300, seasonal_0 Loss: 0.0615 | 0.0323
Epoch 36/300, seasonal_0 Loss: 0.0627 | 0.0319
Epoch 37/300, seasonal_0 Loss: 0.0634 | 0.0315
Epoch 38/300, seasonal_0 Loss: 0.0632 | 0.0315
Epoch 39/300, seasonal_0 Loss: 0.0617 | 0.0328
Epoch 40/300, seasonal_0 Loss: 0.0608 | 0.0295
Epoch 41/300, seasonal_0 Loss: 0.0632 | 0.0280
Epoch 42/300, seasonal_0 Loss: 0.0597 | 0.0246
Epoch 43/300, seasonal_0 Loss: 0.0594 | 0.0221
Epoch 44/300, seasonal_0 Loss: 0.0603 | 0.0220
Epoch 45/300, seasonal_0 Loss: 0.0570 | 0.0210
Epoch 46/300, seasonal_0 Loss: 0.0567 | 0.0200
Epoch 47/300, seasonal_0 Loss: 0.0563 | 0.0203
Epoch 48/300, seasonal_0 Loss: 0.0563 | 0.0206
Epoch 49/300, seasonal_0 Loss: 0.0562 | 0.0198
Epoch 50/300, seasonal_0 Loss: 0.0563 | 0.0212
Epoch 51/300, seasonal_0 Loss: 0.0562 | 0.0207
Epoch 52/300, seasonal_0 Loss: 0.0563 | 0.0207
Epoch 53/300, seasonal_0 Loss: 0.0564 | 0.0197
Epoch 54/300, seasonal_0 Loss: 0.0567 | 0.0213
Epoch 55/300, seasonal_0 Loss: 0.0573 | 0.0200
Epoch 56/300, seasonal_0 Loss: 0.0567 | 0.0216
Epoch 57/300, seasonal_0 Loss: 0.0561 | 0.0226
Epoch 58/300, seasonal_0 Loss: 0.0553 | 0.0217
Epoch 59/300, seasonal_0 Loss: 0.0550 | 0.0229
Epoch 60/300, seasonal_0 Loss: 0.0554 | 0.0237
Epoch 61/300, seasonal_0 Loss: 0.0557 | 0.0266
Epoch 62/300, seasonal_0 Loss: 0.0560 | 0.0270
Epoch 63/300, seasonal_0 Loss: 0.0555 | 0.0212
Epoch 64/300, seasonal_0 Loss: 0.0553 | 0.0242
Epoch 65/300, seasonal_0 Loss: 0.0548 | 0.0216
Epoch 66/300, seasonal_0 Loss: 0.0605 | 0.0257
Epoch 67/300, seasonal_0 Loss: 0.0584 | 0.0195
Epoch 68/300, seasonal_0 Loss: 0.0558 | 0.0203
Epoch 69/300, seasonal_0 Loss: 0.0555 | 0.0190
Epoch 70/300, seasonal_0 Loss: 0.0538 | 0.0182
Epoch 71/300, seasonal_0 Loss: 0.0542 | 0.0195
Epoch 72/300, seasonal_0 Loss: 0.0554 | 0.0207
Epoch 73/300, seasonal_0 Loss: 0.0564 | 0.0232
Epoch 74/300, seasonal_0 Loss: 0.0584 | 0.0245
Epoch 75/300, seasonal_0 Loss: 0.0641 | 0.0304
Epoch 76/300, seasonal_0 Loss: 0.0582 | 0.0298
Epoch 77/300, seasonal_0 Loss: 0.0580 | 0.0291
Epoch 78/300, seasonal_0 Loss: 0.0567 | 0.0251
Epoch 79/300, seasonal_0 Loss: 0.0567 | 0.0223
Epoch 80/300, seasonal_0 Loss: 0.0567 | 0.0215
Epoch 81/300, seasonal_0 Loss: 0.0567 | 0.0230
Epoch 82/300, seasonal_0 Loss: 0.0562 | 0.0229
Epoch 83/300, seasonal_0 Loss: 0.0562 | 0.0185
Epoch 84/300, seasonal_0 Loss: 0.0548 | 0.0170
Epoch 85/300, seasonal_0 Loss: 0.0534 | 0.0172
Epoch 86/300, seasonal_0 Loss: 0.0533 | 0.0157
Epoch 87/300, seasonal_0 Loss: 0.0525 | 0.0149
Epoch 88/300, seasonal_0 Loss: 0.0522 | 0.0146
Epoch 89/300, seasonal_0 Loss: 0.0519 | 0.0146
Epoch 90/300, seasonal_0 Loss: 0.0516 | 0.0147
Epoch 91/300, seasonal_0 Loss: 0.0515 | 0.0146
Epoch 92/300, seasonal_0 Loss: 0.0509 | 0.0148
Epoch 93/300, seasonal_0 Loss: 0.0506 | 0.0146
Epoch 94/300, seasonal_0 Loss: 0.0499 | 0.0148
Epoch 95/300, seasonal_0 Loss: 0.0496 | 0.0147
Epoch 96/300, seasonal_0 Loss: 0.0497 | 0.0148
Epoch 97/300, seasonal_0 Loss: 0.0498 | 0.0160
Epoch 98/300, seasonal_0 Loss: 0.0501 | 0.0168
Epoch 99/300, seasonal_0 Loss: 0.0502 | 0.0175
Epoch 100/300, seasonal_0 Loss: 0.0500 | 0.0178
Epoch 101/300, seasonal_0 Loss: 0.0497 | 0.0174
Epoch 102/300, seasonal_0 Loss: 0.0495 | 0.0170
Epoch 103/300, seasonal_0 Loss: 0.0495 | 0.0172
Epoch 104/300, seasonal_0 Loss: 0.0495 | 0.0166
Epoch 105/300, seasonal_0 Loss: 0.0493 | 0.0160
Epoch 106/300, seasonal_0 Loss: 0.0492 | 0.0157
Epoch 107/300, seasonal_0 Loss: 0.0491 | 0.0156
Epoch 108/300, seasonal_0 Loss: 0.0490 | 0.0157
Epoch 109/300, seasonal_0 Loss: 0.0490 | 0.0161
Epoch 110/300, seasonal_0 Loss: 0.0490 | 0.0168
Epoch 111/300, seasonal_0 Loss: 0.0490 | 0.0175
Epoch 112/300, seasonal_0 Loss: 0.0489 | 0.0179
Epoch 113/300, seasonal_0 Loss: 0.0488 | 0.0181
Epoch 114/300, seasonal_0 Loss: 0.0487 | 0.0182
Epoch 115/300, seasonal_0 Loss: 0.0487 | 0.0184
Epoch 116/300, seasonal_0 Loss: 0.0486 | 0.0187
Epoch 117/300, seasonal_0 Loss: 0.0487 | 0.0190
Epoch 118/300, seasonal_0 Loss: 0.0487 | 0.0192
Epoch 119/300, seasonal_0 Loss: 0.0489 | 0.0199
Epoch 120/300, seasonal_0 Loss: 0.0490 | 0.0213
Epoch 121/300, seasonal_0 Loss: 0.0491 | 0.0198
Epoch 122/300, seasonal_0 Loss: 0.0493 | 0.0183
Epoch 123/300, seasonal_0 Loss: 0.0494 | 0.0177
Epoch 124/300, seasonal_0 Loss: 0.0500 | 0.0173
Epoch 125/300, seasonal_0 Loss: 0.0511 | 0.0179
Epoch 126/300, seasonal_0 Loss: 0.0512 | 0.0179
Epoch 127/300, seasonal_0 Loss: 0.0510 | 0.0182
Epoch 128/300, seasonal_0 Loss: 0.0501 | 0.0183
Epoch 129/300, seasonal_0 Loss: 0.0491 | 0.0185
Epoch 130/300, seasonal_0 Loss: 0.0484 | 0.0176
Epoch 131/300, seasonal_0 Loss: 0.0482 | 0.0173
Epoch 132/300, seasonal_0 Loss: 0.0479 | 0.0171
Epoch 133/300, seasonal_0 Loss: 0.0479 | 0.0170
Epoch 134/300, seasonal_0 Loss: 0.0478 | 0.0169
Epoch 135/300, seasonal_0 Loss: 0.0477 | 0.0170
Epoch 136/300, seasonal_0 Loss: 0.0477 | 0.0171
Epoch 137/300, seasonal_0 Loss: 0.0476 | 0.0172
Epoch 138/300, seasonal_0 Loss: 0.0476 | 0.0173
Epoch 139/300, seasonal_0 Loss: 0.0476 | 0.0173
Epoch 140/300, seasonal_0 Loss: 0.0475 | 0.0172
Epoch 141/300, seasonal_0 Loss: 0.0475 | 0.0173
Epoch 142/300, seasonal_0 Loss: 0.0475 | 0.0173
Epoch 143/300, seasonal_0 Loss: 0.0475 | 0.0175
Epoch 144/300, seasonal_0 Loss: 0.0475 | 0.0176
Epoch 145/300, seasonal_0 Loss: 0.0475 | 0.0177
Epoch 146/300, seasonal_0 Loss: 0.0474 | 0.0178
Epoch 147/300, seasonal_0 Loss: 0.0473 | 0.0178
Epoch 148/300, seasonal_0 Loss: 0.0472 | 0.0178
Epoch 149/300, seasonal_0 Loss: 0.0471 | 0.0178
Epoch 150/300, seasonal_0 Loss: 0.0470 | 0.0178
Epoch 151/300, seasonal_0 Loss: 0.0470 | 0.0180
Epoch 152/300, seasonal_0 Loss: 0.0469 | 0.0181
Epoch 153/300, seasonal_0 Loss: 0.0469 | 0.0181
Epoch 154/300, seasonal_0 Loss: 0.0469 | 0.0180
Epoch 155/300, seasonal_0 Loss: 0.0468 | 0.0178
Epoch 156/300, seasonal_0 Loss: 0.0468 | 0.0177
Epoch 157/300, seasonal_0 Loss: 0.0468 | 0.0177
Epoch 158/300, seasonal_0 Loss: 0.0468 | 0.0177
Epoch 159/300, seasonal_0 Loss: 0.0468 | 0.0179
Epoch 160/300, seasonal_0 Loss: 0.0467 | 0.0179
Epoch 161/300, seasonal_0 Loss: 0.0467 | 0.0180
Epoch 162/300, seasonal_0 Loss: 0.0466 | 0.0181
Epoch 163/300, seasonal_0 Loss: 0.0466 | 0.0181
Epoch 164/300, seasonal_0 Loss: 0.0466 | 0.0180
Epoch 165/300, seasonal_0 Loss: 0.0465 | 0.0180
Epoch 166/300, seasonal_0 Loss: 0.0465 | 0.0178
Epoch 167/300, seasonal_0 Loss: 0.0466 | 0.0175
Epoch 168/300, seasonal_0 Loss: 0.0467 | 0.0172
Epoch 169/300, seasonal_0 Loss: 0.0472 | 0.0167
Epoch 170/300, seasonal_0 Loss: 0.0474 | 0.0165
Epoch 171/300, seasonal_0 Loss: 0.0470 | 0.0169
Epoch 172/300, seasonal_0 Loss: 0.0467 | 0.0172
Epoch 173/300, seasonal_0 Loss: 0.0465 | 0.0173
Epoch 174/300, seasonal_0 Loss: 0.0465 | 0.0173
Epoch 175/300, seasonal_0 Loss: 0.0465 | 0.0173
Epoch 176/300, seasonal_0 Loss: 0.0465 | 0.0173
Epoch 177/300, seasonal_0 Loss: 0.0464 | 0.0173
Epoch 178/300, seasonal_0 Loss: 0.0464 | 0.0173
Epoch 179/300, seasonal_0 Loss: 0.0463 | 0.0173
Epoch 180/300, seasonal_0 Loss: 0.0463 | 0.0173
Epoch 181/300, seasonal_0 Loss: 0.0462 | 0.0174
Epoch 182/300, seasonal_0 Loss: 0.0462 | 0.0174
Epoch 183/300, seasonal_0 Loss: 0.0462 | 0.0174
Epoch 184/300, seasonal_0 Loss: 0.0462 | 0.0173
Epoch 185/300, seasonal_0 Loss: 0.0461 | 0.0173
Epoch 186/300, seasonal_0 Loss: 0.0461 | 0.0173
Epoch 187/300, seasonal_0 Loss: 0.0461 | 0.0173
Epoch 188/300, seasonal_0 Loss: 0.0461 | 0.0172
Epoch 189/300, seasonal_0 Loss: 0.0460 | 0.0172
Epoch 190/300, seasonal_0 Loss: 0.0460 | 0.0171
Epoch 191/300, seasonal_0 Loss: 0.0460 | 0.0171
Epoch 192/300, seasonal_0 Loss: 0.0460 | 0.0171
Epoch 193/300, seasonal_0 Loss: 0.0460 | 0.0171
Epoch 194/300, seasonal_0 Loss: 0.0460 | 0.0170
Epoch 195/300, seasonal_0 Loss: 0.0459 | 0.0170
Epoch 196/300, seasonal_0 Loss: 0.0459 | 0.0170
Epoch 197/300, seasonal_0 Loss: 0.0459 | 0.0169
Epoch 198/300, seasonal_0 Loss: 0.0459 | 0.0169
Epoch 199/300, seasonal_0 Loss: 0.0459 | 0.0169
Epoch 200/300, seasonal_0 Loss: 0.0459 | 0.0169
Epoch 201/300, seasonal_0 Loss: 0.0459 | 0.0168
Epoch 202/300, seasonal_0 Loss: 0.0458 | 0.0168
Epoch 203/300, seasonal_0 Loss: 0.0458 | 0.0168
Epoch 204/300, seasonal_0 Loss: 0.0458 | 0.0168
Epoch 205/300, seasonal_0 Loss: 0.0458 | 0.0168
Epoch 206/300, seasonal_0 Loss: 0.0458 | 0.0168
Epoch 207/300, seasonal_0 Loss: 0.0458 | 0.0167
Epoch 208/300, seasonal_0 Loss: 0.0458 | 0.0167
Epoch 209/300, seasonal_0 Loss: 0.0458 | 0.0167
Epoch 210/300, seasonal_0 Loss: 0.0458 | 0.0167
Epoch 211/300, seasonal_0 Loss: 0.0457 | 0.0167
Epoch 212/300, seasonal_0 Loss: 0.0457 | 0.0167
Epoch 213/300, seasonal_0 Loss: 0.0457 | 0.0167
Epoch 214/300, seasonal_0 Loss: 0.0457 | 0.0167
Epoch 215/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 216/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 217/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 218/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 219/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 220/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 221/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 222/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 223/300, seasonal_0 Loss: 0.0457 | 0.0166
Epoch 224/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 225/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 226/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 227/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 228/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 229/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 230/300, seasonal_0 Loss: 0.0456 | 0.0166
Epoch 231/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 232/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 233/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 234/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 235/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 236/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 237/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 238/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 239/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 240/300, seasonal_0 Loss: 0.0456 | 0.0165
Epoch 241/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 242/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 243/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 244/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 245/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 246/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 247/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 248/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 249/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 250/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 251/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 252/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 253/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 254/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 255/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 256/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 257/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 258/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 259/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 260/300, seasonal_0 Loss: 0.0455 | 0.0165
Epoch 261/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 262/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 263/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 264/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 265/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 266/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 267/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 268/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 269/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 270/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 271/300, seasonal_0 Loss: 0.0455 | 0.0164
Epoch 272/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 273/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 274/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 275/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 276/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 277/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 278/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 279/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 280/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 281/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 282/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 283/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 284/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 285/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 286/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 287/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 288/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 289/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 290/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 291/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 292/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 293/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 294/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 295/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 296/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 297/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 298/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 299/300, seasonal_0 Loss: 0.0454 | 0.0164
Epoch 300/300, seasonal_0 Loss: 0.0454 | 0.0164
Training seasonal_1 component with params: {'observation_period_num': 15, 'train_rates': 0.9136040382929589, 'learning_rate': 0.0005554523333265239, 'batch_size': 91, 'step_size': 10, 'gamma': 0.9777060915216246}
Epoch 1/300, seasonal_1 Loss: 0.2507 | 0.1616
Epoch 2/300, seasonal_1 Loss: 0.1310 | 0.1040
Epoch 3/300, seasonal_1 Loss: 0.1142 | 0.0886
Epoch 4/300, seasonal_1 Loss: 0.1065 | 0.1227
Epoch 5/300, seasonal_1 Loss: 0.1046 | 0.1968
Epoch 6/300, seasonal_1 Loss: 0.1031 | 0.1533
Epoch 7/300, seasonal_1 Loss: 0.1257 | 0.0850
Epoch 8/300, seasonal_1 Loss: 0.1078 | 0.0730
Epoch 9/300, seasonal_1 Loss: 0.1001 | 0.0766
Epoch 10/300, seasonal_1 Loss: 0.1056 | 0.0690
Epoch 11/300, seasonal_1 Loss: 0.0987 | 0.0656
Epoch 12/300, seasonal_1 Loss: 0.0980 | 0.0626
Epoch 13/300, seasonal_1 Loss: 0.1001 | 0.0740
Epoch 14/300, seasonal_1 Loss: 0.0966 | 0.0594
Epoch 15/300, seasonal_1 Loss: 0.1017 | 0.0577
Epoch 16/300, seasonal_1 Loss: 0.0927 | 0.0576
Epoch 17/300, seasonal_1 Loss: 0.0933 | 0.0614
Epoch 18/300, seasonal_1 Loss: 0.0926 | 0.0701
Epoch 19/300, seasonal_1 Loss: 0.0880 | 0.0515
Epoch 20/300, seasonal_1 Loss: 0.0823 | 0.0496
Epoch 21/300, seasonal_1 Loss: 0.0829 | 0.0525
Epoch 22/300, seasonal_1 Loss: 0.0833 | 0.0493
Epoch 23/300, seasonal_1 Loss: 0.0881 | 0.0528
Epoch 24/300, seasonal_1 Loss: 0.0900 | 0.0508
Epoch 25/300, seasonal_1 Loss: 0.0979 | 0.0519
Epoch 26/300, seasonal_1 Loss: 0.0793 | 0.0448
Epoch 27/300, seasonal_1 Loss: 0.0793 | 0.0425
Epoch 28/300, seasonal_1 Loss: 0.0732 | 0.0495
Epoch 29/300, seasonal_1 Loss: 0.0796 | 0.0520
Epoch 30/300, seasonal_1 Loss: 0.0837 | 0.0480
Epoch 31/300, seasonal_1 Loss: 0.0826 | 0.0592
Epoch 32/300, seasonal_1 Loss: 0.1015 | 0.0707
Epoch 33/300, seasonal_1 Loss: 0.1047 | 0.0599
Epoch 34/300, seasonal_1 Loss: 0.0767 | 0.0433
Epoch 35/300, seasonal_1 Loss: 0.0775 | 0.0729
Epoch 36/300, seasonal_1 Loss: 0.0887 | 0.0617
Epoch 37/300, seasonal_1 Loss: 0.0780 | 0.1377
Epoch 38/300, seasonal_1 Loss: 0.0869 | 0.1528
Epoch 39/300, seasonal_1 Loss: 0.0702 | 0.0643
Epoch 40/300, seasonal_1 Loss: 0.0685 | 0.0495
Epoch 41/300, seasonal_1 Loss: 0.0824 | 0.0453
Epoch 42/300, seasonal_1 Loss: 0.0747 | 0.0406
Epoch 43/300, seasonal_1 Loss: 0.0684 | 0.0405
Epoch 44/300, seasonal_1 Loss: 0.0653 | 0.0540
Epoch 45/300, seasonal_1 Loss: 0.0661 | 0.0470
Epoch 46/300, seasonal_1 Loss: 0.0625 | 0.0470
Epoch 47/300, seasonal_1 Loss: 0.0626 | 0.0459
Epoch 48/300, seasonal_1 Loss: 0.0628 | 0.0387
Epoch 49/300, seasonal_1 Loss: 0.0610 | 0.0363
Epoch 50/300, seasonal_1 Loss: 0.0621 | 0.0367
Epoch 51/300, seasonal_1 Loss: 0.0605 | 0.0340
Epoch 52/300, seasonal_1 Loss: 0.0599 | 0.0325
Epoch 53/300, seasonal_1 Loss: 0.0592 | 0.0379
Epoch 54/300, seasonal_1 Loss: 0.0589 | 0.0345
Epoch 55/300, seasonal_1 Loss: 0.0570 | 0.0388
Epoch 56/300, seasonal_1 Loss: 0.0570 | 0.0342
Epoch 57/300, seasonal_1 Loss: 0.0564 | 0.0382
Epoch 58/300, seasonal_1 Loss: 0.0565 | 0.0357
Epoch 59/300, seasonal_1 Loss: 0.0580 | 0.0386
Epoch 60/300, seasonal_1 Loss: 0.0610 | 0.0408
Epoch 61/300, seasonal_1 Loss: 0.0557 | 0.0334
Epoch 62/300, seasonal_1 Loss: 0.0597 | 0.0404
Epoch 63/300, seasonal_1 Loss: 0.0565 | 0.0320
Epoch 64/300, seasonal_1 Loss: 0.0592 | 0.0378
Epoch 65/300, seasonal_1 Loss: 0.0634 | 0.0400
Epoch 66/300, seasonal_1 Loss: 0.0607 | 0.0363
Epoch 67/300, seasonal_1 Loss: 0.0587 | 0.0360
Epoch 68/300, seasonal_1 Loss: 0.0558 | 0.0333
Epoch 69/300, seasonal_1 Loss: 0.0539 | 0.0355
Epoch 70/300, seasonal_1 Loss: 0.0543 | 0.0364
Epoch 71/300, seasonal_1 Loss: 0.0548 | 0.0362
Epoch 72/300, seasonal_1 Loss: 0.0550 | 0.0367
Epoch 73/300, seasonal_1 Loss: 0.0552 | 0.0364
Epoch 74/300, seasonal_1 Loss: 0.0559 | 0.0403
Epoch 75/300, seasonal_1 Loss: 0.0594 | 0.0392
Epoch 76/300, seasonal_1 Loss: 0.0572 | 0.0485
Epoch 77/300, seasonal_1 Loss: 0.0643 | 0.0527
Epoch 78/300, seasonal_1 Loss: 0.0617 | 0.0808
Epoch 79/300, seasonal_1 Loss: 0.0743 | 0.1101
Epoch 80/300, seasonal_1 Loss: 0.0640 | 0.0868
Epoch 81/300, seasonal_1 Loss: 0.0658 | 0.0716
Epoch 82/300, seasonal_1 Loss: 0.0618 | 0.0569
Epoch 83/300, seasonal_1 Loss: 0.0645 | 0.0416
Epoch 84/300, seasonal_1 Loss: 0.0648 | 0.0421
Epoch 85/300, seasonal_1 Loss: 0.0626 | 0.0390
Epoch 86/300, seasonal_1 Loss: 0.0567 | 0.0398
Epoch 87/300, seasonal_1 Loss: 0.0556 | 0.0438
Epoch 88/300, seasonal_1 Loss: 0.0546 | 0.0372
Epoch 89/300, seasonal_1 Loss: 0.0520 | 0.0416
Epoch 90/300, seasonal_1 Loss: 0.0518 | 0.0406
Epoch 91/300, seasonal_1 Loss: 0.0521 | 0.0416
Epoch 92/300, seasonal_1 Loss: 0.0509 | 0.0373
Epoch 93/300, seasonal_1 Loss: 0.0511 | 0.0347
Epoch 94/300, seasonal_1 Loss: 0.0526 | 0.0323
Epoch 95/300, seasonal_1 Loss: 0.0536 | 0.0326
Epoch 96/300, seasonal_1 Loss: 0.0531 | 0.0337
Epoch 97/300, seasonal_1 Loss: 0.0509 | 0.0349
Epoch 98/300, seasonal_1 Loss: 0.0498 | 0.0369
Epoch 99/300, seasonal_1 Loss: 0.0507 | 0.0363
Epoch 100/300, seasonal_1 Loss: 0.0507 | 0.0367
Epoch 101/300, seasonal_1 Loss: 0.0500 | 0.0370
Epoch 102/300, seasonal_1 Loss: 0.0501 | 0.0390
Epoch 103/300, seasonal_1 Loss: 0.0510 | 0.0382
Epoch 104/300, seasonal_1 Loss: 0.0492 | 0.0332
Epoch 105/300, seasonal_1 Loss: 0.0498 | 0.0312
Epoch 106/300, seasonal_1 Loss: 0.0505 | 0.0345
Epoch 107/300, seasonal_1 Loss: 0.0511 | 0.0359
Epoch 108/300, seasonal_1 Loss: 0.0518 | 0.0430
Epoch 109/300, seasonal_1 Loss: 0.0518 | 0.0329
Epoch 110/300, seasonal_1 Loss: 0.0491 | 0.0352
Epoch 111/300, seasonal_1 Loss: 0.0482 | 0.0337
Epoch 112/300, seasonal_1 Loss: 0.0475 | 0.0336
Epoch 113/300, seasonal_1 Loss: 0.0468 | 0.0349
Epoch 114/300, seasonal_1 Loss: 0.0479 | 0.0332
Epoch 115/300, seasonal_1 Loss: 0.0475 | 0.0396
Epoch 116/300, seasonal_1 Loss: 0.0487 | 0.0349
Epoch 117/300, seasonal_1 Loss: 0.0467 | 0.0388
Epoch 118/300, seasonal_1 Loss: 0.0468 | 0.0381
Epoch 119/300, seasonal_1 Loss: 0.0462 | 0.0369
Epoch 120/300, seasonal_1 Loss: 0.0462 | 0.0345
Epoch 121/300, seasonal_1 Loss: 0.0465 | 0.0353
Epoch 122/300, seasonal_1 Loss: 0.0471 | 0.0343
Epoch 123/300, seasonal_1 Loss: 0.0476 | 0.0353
Epoch 124/300, seasonal_1 Loss: 0.0487 | 0.0357
Epoch 125/300, seasonal_1 Loss: 0.0500 | 0.0345
Epoch 126/300, seasonal_1 Loss: 0.0487 | 0.0308
Epoch 127/300, seasonal_1 Loss: 0.0471 | 0.0322
Epoch 128/300, seasonal_1 Loss: 0.0453 | 0.0343
Epoch 129/300, seasonal_1 Loss: 0.0453 | 0.0351
Epoch 130/300, seasonal_1 Loss: 0.0452 | 0.0358
Epoch 131/300, seasonal_1 Loss: 0.0447 | 0.0352
Epoch 132/300, seasonal_1 Loss: 0.0444 | 0.0357
Epoch 133/300, seasonal_1 Loss: 0.0446 | 0.0359
Epoch 134/300, seasonal_1 Loss: 0.0447 | 0.0351
Epoch 135/300, seasonal_1 Loss: 0.0455 | 0.0355
Epoch 136/300, seasonal_1 Loss: 0.0466 | 0.0380
Epoch 137/300, seasonal_1 Loss: 0.0450 | 0.0347
Epoch 138/300, seasonal_1 Loss: 0.0468 | 0.0335
Epoch 139/300, seasonal_1 Loss: 0.0459 | 0.0352
Epoch 140/300, seasonal_1 Loss: 0.0456 | 0.0327
Epoch 141/300, seasonal_1 Loss: 0.0452 | 0.0317
Epoch 142/300, seasonal_1 Loss: 0.0445 | 0.0334
Epoch 143/300, seasonal_1 Loss: 0.0437 | 0.0361
Epoch 144/300, seasonal_1 Loss: 0.0441 | 0.0351
Epoch 145/300, seasonal_1 Loss: 0.0422 | 0.0343
Epoch 146/300, seasonal_1 Loss: 0.0412 | 0.0324
Epoch 147/300, seasonal_1 Loss: 0.0419 | 0.0365
Epoch 148/300, seasonal_1 Loss: 0.0453 | 0.0399
Epoch 149/300, seasonal_1 Loss: 0.0444 | 0.0334
Epoch 150/300, seasonal_1 Loss: 0.0407 | 0.0350
Epoch 151/300, seasonal_1 Loss: 0.0431 | 0.0338
Epoch 152/300, seasonal_1 Loss: 0.0414 | 0.0357
Epoch 153/300, seasonal_1 Loss: 0.0399 | 0.0347
Epoch 154/300, seasonal_1 Loss: 0.0394 | 0.0335
Epoch 155/300, seasonal_1 Loss: 0.0392 | 0.0333
Epoch 156/300, seasonal_1 Loss: 0.0420 | 0.0336
Epoch 157/300, seasonal_1 Loss: 0.0382 | 0.0362
Epoch 158/300, seasonal_1 Loss: 0.0385 | 0.0346
Epoch 159/300, seasonal_1 Loss: 0.0390 | 0.0369
Epoch 160/300, seasonal_1 Loss: 0.0404 | 0.0346
Epoch 161/300, seasonal_1 Loss: 0.0401 | 0.0333
Epoch 162/300, seasonal_1 Loss: 0.0415 | 0.0368
Epoch 163/300, seasonal_1 Loss: 0.0433 | 0.0346
Epoch 164/300, seasonal_1 Loss: 0.0427 | 0.0358
Epoch 165/300, seasonal_1 Loss: 0.0395 | 0.0318
Epoch 166/300, seasonal_1 Loss: 0.0437 | 0.0321
Epoch 167/300, seasonal_1 Loss: 0.0424 | 0.0332
Epoch 168/300, seasonal_1 Loss: 0.0416 | 0.0360
Epoch 169/300, seasonal_1 Loss: 0.0427 | 0.0359
Epoch 170/300, seasonal_1 Loss: 0.0431 | 0.0334
Epoch 171/300, seasonal_1 Loss: 0.0433 | 0.0383
Epoch 172/300, seasonal_1 Loss: 0.0444 | 0.0353
Epoch 173/300, seasonal_1 Loss: 0.0425 | 0.0368
Epoch 174/300, seasonal_1 Loss: 0.0426 | 0.0334
Epoch 175/300, seasonal_1 Loss: 0.0429 | 0.0350
Epoch 176/300, seasonal_1 Loss: 0.0440 | 0.0321
Epoch 177/300, seasonal_1 Loss: 0.0416 | 0.0345
Epoch 178/300, seasonal_1 Loss: 0.0409 | 0.0326
Epoch 179/300, seasonal_1 Loss: 0.0402 | 0.0349
Epoch 180/300, seasonal_1 Loss: 0.0404 | 0.0337
Epoch 181/300, seasonal_1 Loss: 0.0384 | 0.0341
Epoch 182/300, seasonal_1 Loss: 0.0386 | 0.0342
Epoch 183/300, seasonal_1 Loss: 0.0425 | 0.0312
Epoch 184/300, seasonal_1 Loss: 0.0407 | 0.0314
Epoch 185/300, seasonal_1 Loss: 0.0369 | 0.0308
Epoch 186/300, seasonal_1 Loss: 0.0364 | 0.0329
Epoch 187/300, seasonal_1 Loss: 0.0401 | 0.0351
Epoch 188/300, seasonal_1 Loss: 0.0392 | 0.0325
Epoch 189/300, seasonal_1 Loss: 0.0377 | 0.0306
Epoch 190/300, seasonal_1 Loss: 0.0409 | 0.0320
Epoch 191/300, seasonal_1 Loss: 0.0371 | 0.0332
Epoch 192/300, seasonal_1 Loss: 0.0360 | 0.0317
Epoch 193/300, seasonal_1 Loss: 0.0349 | 0.0318
Epoch 194/300, seasonal_1 Loss: 0.0397 | 0.0361
Epoch 195/300, seasonal_1 Loss: 0.0364 | 0.0364
Epoch 196/300, seasonal_1 Loss: 0.0357 | 0.0337
Epoch 197/300, seasonal_1 Loss: 0.0385 | 0.0322
Epoch 198/300, seasonal_1 Loss: 0.0328 | 0.0348
Epoch 199/300, seasonal_1 Loss: 0.0371 | 0.0353
Epoch 200/300, seasonal_1 Loss: 0.0334 | 0.0337
Epoch 201/300, seasonal_1 Loss: 0.0342 | 0.0351
Epoch 202/300, seasonal_1 Loss: 0.0328 | 0.0336
Epoch 203/300, seasonal_1 Loss: 0.0327 | 0.0337
Epoch 204/300, seasonal_1 Loss: 0.0315 | 0.0340
Epoch 205/300, seasonal_1 Loss: 0.0321 | 0.0333
Epoch 206/300, seasonal_1 Loss: 0.0352 | 0.0368
Epoch 207/300, seasonal_1 Loss: 0.0352 | 0.0342
Epoch 208/300, seasonal_1 Loss: 0.0319 | 0.0389
Epoch 209/300, seasonal_1 Loss: 0.0344 | 0.0333
Epoch 210/300, seasonal_1 Loss: 0.0334 | 0.0341
Epoch 211/300, seasonal_1 Loss: 0.0317 | 0.0390
Epoch 212/300, seasonal_1 Loss: 0.0310 | 0.0331
Epoch 213/300, seasonal_1 Loss: 0.0347 | 0.0363
Epoch 214/300, seasonal_1 Loss: 0.0337 | 0.0338
Epoch 215/300, seasonal_1 Loss: 0.0308 | 0.0342
Epoch 216/300, seasonal_1 Loss: 0.0310 | 0.0329
Epoch 217/300, seasonal_1 Loss: 0.0306 | 0.0344
Epoch 218/300, seasonal_1 Loss: 0.0304 | 0.0331
Epoch 219/300, seasonal_1 Loss: 0.0296 | 0.0332
Epoch 220/300, seasonal_1 Loss: 0.0319 | 0.0352
Epoch 221/300, seasonal_1 Loss: 0.0334 | 0.0339
Epoch 222/300, seasonal_1 Loss: 0.0391 | 0.0392
Epoch 223/300, seasonal_1 Loss: 0.0429 | 0.0324
Epoch 224/300, seasonal_1 Loss: 0.0406 | 0.0335
Epoch 225/300, seasonal_1 Loss: 0.0389 | 0.0356
Epoch 226/300, seasonal_1 Loss: 0.0362 | 0.0334
Epoch 227/300, seasonal_1 Loss: 0.0349 | 0.0342
Epoch 228/300, seasonal_1 Loss: 0.0355 | 0.0322
Epoch 229/300, seasonal_1 Loss: 0.0364 | 0.0322
Epoch 230/300, seasonal_1 Loss: 0.0363 | 0.0354
Epoch 231/300, seasonal_1 Loss: 0.0358 | 0.0341
Epoch 232/300, seasonal_1 Loss: 0.0331 | 0.0362
Epoch 233/300, seasonal_1 Loss: 0.0338 | 0.0329
Epoch 234/300, seasonal_1 Loss: 0.0362 | 0.0355
Epoch 235/300, seasonal_1 Loss: 0.0364 | 0.0359
Epoch 236/300, seasonal_1 Loss: 0.0350 | 0.0352
Epoch 237/300, seasonal_1 Loss: 0.0348 | 0.0354
Epoch 238/300, seasonal_1 Loss: 0.0339 | 0.0338
Epoch 239/300, seasonal_1 Loss: 0.0348 | 0.0342
Epoch 240/300, seasonal_1 Loss: 0.0333 | 0.0352
Epoch 241/300, seasonal_1 Loss: 0.0341 | 0.0343
Epoch 242/300, seasonal_1 Loss: 0.0334 | 0.0368
Epoch 243/300, seasonal_1 Loss: 0.0364 | 0.0347
Epoch 244/300, seasonal_1 Loss: 0.0342 | 0.0466
Epoch 245/300, seasonal_1 Loss: 0.0334 | 0.0387
Epoch 246/300, seasonal_1 Loss: 0.0327 | 0.0461
Epoch 247/300, seasonal_1 Loss: 0.0358 | 0.0381
Epoch 248/300, seasonal_1 Loss: 0.0348 | 0.0367
Epoch 249/300, seasonal_1 Loss: 0.0335 | 0.0371
Epoch 250/300, seasonal_1 Loss: 0.0350 | 0.0389
Epoch 251/300, seasonal_1 Loss: 0.0359 | 0.0349
Epoch 252/300, seasonal_1 Loss: 0.0321 | 0.0386
Epoch 253/300, seasonal_1 Loss: 0.0313 | 0.0399
Epoch 254/300, seasonal_1 Loss: 0.0297 | 0.0389
Epoch 255/300, seasonal_1 Loss: 0.0289 | 0.0365
Epoch 256/300, seasonal_1 Loss: 0.0285 | 0.0339
Epoch 257/300, seasonal_1 Loss: 0.0278 | 0.0352
Epoch 258/300, seasonal_1 Loss: 0.0276 | 0.0340
Epoch 259/300, seasonal_1 Loss: 0.0285 | 0.0415
Epoch 260/300, seasonal_1 Loss: 0.0297 | 0.0375
Epoch 261/300, seasonal_1 Loss: 0.0281 | 0.0367
Epoch 262/300, seasonal_1 Loss: 0.0270 | 0.0332
Epoch 263/300, seasonal_1 Loss: 0.0265 | 0.0332
Epoch 264/300, seasonal_1 Loss: 0.0262 | 0.0333
Epoch 265/300, seasonal_1 Loss: 0.0292 | 0.0333
Epoch 266/300, seasonal_1 Loss: 0.0345 | 0.0398
Epoch 267/300, seasonal_1 Loss: 0.0319 | 0.0369
Epoch 268/300, seasonal_1 Loss: 0.0298 | 0.0359
Epoch 269/300, seasonal_1 Loss: 0.0296 | 0.0349
Epoch 270/300, seasonal_1 Loss: 0.0285 | 0.0318
Epoch 271/300, seasonal_1 Loss: 0.0354 | 0.0379
Epoch 272/300, seasonal_1 Loss: 0.0286 | 0.0342
Epoch 273/300, seasonal_1 Loss: 0.0274 | 0.0332
Epoch 274/300, seasonal_1 Loss: 0.0271 | 0.0330
Epoch 275/300, seasonal_1 Loss: 0.0265 | 0.0328
Epoch 276/300, seasonal_1 Loss: 0.0260 | 0.0330
Epoch 277/300, seasonal_1 Loss: 0.0258 | 0.0349
Epoch 278/300, seasonal_1 Loss: 0.0258 | 0.0357
Epoch 279/300, seasonal_1 Loss: 0.0260 | 0.0344
Epoch 280/300, seasonal_1 Loss: 0.0258 | 0.0341
Epoch 281/300, seasonal_1 Loss: 0.0253 | 0.0331
Epoch 282/300, seasonal_1 Loss: 0.0254 | 0.0340
Epoch 283/300, seasonal_1 Loss: 0.0252 | 0.0338
Epoch 284/300, seasonal_1 Loss: 0.0249 | 0.0352
Epoch 285/300, seasonal_1 Loss: 0.0249 | 0.0351
Epoch 286/300, seasonal_1 Loss: 0.0249 | 0.0357
Epoch 287/300, seasonal_1 Loss: 0.0249 | 0.0344
Epoch 288/300, seasonal_1 Loss: 0.0247 | 0.0345
Epoch 289/300, seasonal_1 Loss: 0.0247 | 0.0337
Epoch 290/300, seasonal_1 Loss: 0.0248 | 0.0343
Epoch 291/300, seasonal_1 Loss: 0.0248 | 0.0339
Epoch 292/300, seasonal_1 Loss: 0.0246 | 0.0345
Epoch 293/300, seasonal_1 Loss: 0.0248 | 0.0352
Epoch 294/300, seasonal_1 Loss: 0.0250 | 0.0357
Epoch 295/300, seasonal_1 Loss: 0.0250 | 0.0355
Epoch 296/300, seasonal_1 Loss: 0.0252 | 0.0353
Epoch 297/300, seasonal_1 Loss: 0.0248 | 0.0360
Epoch 298/300, seasonal_1 Loss: 0.0247 | 0.0347
Epoch 299/300, seasonal_1 Loss: 0.0248 | 0.0345
Epoch 300/300, seasonal_1 Loss: 0.0248 | 0.0340
Training seasonal_2 component with params: {'observation_period_num': 8, 'train_rates': 0.9741517312878949, 'learning_rate': 0.0002149535490773464, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9818500054033034}
Epoch 1/300, seasonal_2 Loss: 0.1698 | 0.1149
Epoch 2/300, seasonal_2 Loss: 0.1166 | 0.0812
Epoch 3/300, seasonal_2 Loss: 0.1045 | 0.0690
Epoch 4/300, seasonal_2 Loss: 0.0991 | 0.0686
Epoch 5/300, seasonal_2 Loss: 0.0952 | 0.0695
Epoch 6/300, seasonal_2 Loss: 0.0962 | 0.0660
Epoch 7/300, seasonal_2 Loss: 0.0952 | 0.0602
Epoch 8/300, seasonal_2 Loss: 0.0944 | 0.0596
Epoch 9/300, seasonal_2 Loss: 0.0899 | 0.0555
Epoch 10/300, seasonal_2 Loss: 0.0888 | 0.0544
Epoch 11/300, seasonal_2 Loss: 0.0916 | 0.0561
Epoch 12/300, seasonal_2 Loss: 0.0927 | 0.0520
Epoch 13/300, seasonal_2 Loss: 0.0894 | 0.0557
Epoch 14/300, seasonal_2 Loss: 0.0871 | 0.0500
Epoch 15/300, seasonal_2 Loss: 0.0777 | 0.0441
Epoch 16/300, seasonal_2 Loss: 0.0743 | 0.0426
Epoch 17/300, seasonal_2 Loss: 0.0740 | 0.0418
Epoch 18/300, seasonal_2 Loss: 0.0762 | 0.0435
Epoch 19/300, seasonal_2 Loss: 0.0803 | 0.0428
Epoch 20/300, seasonal_2 Loss: 0.0752 | 0.0401
Epoch 21/300, seasonal_2 Loss: 0.0738 | 0.0405
Epoch 22/300, seasonal_2 Loss: 0.0735 | 0.0389
Epoch 23/300, seasonal_2 Loss: 0.0726 | 0.0405
Epoch 24/300, seasonal_2 Loss: 0.0756 | 0.0493
Epoch 25/300, seasonal_2 Loss: 0.0747 | 0.0391
Epoch 26/300, seasonal_2 Loss: 0.0717 | 0.0406
Epoch 27/300, seasonal_2 Loss: 0.0710 | 0.0399
Epoch 28/300, seasonal_2 Loss: 0.0704 | 0.0389
Epoch 29/300, seasonal_2 Loss: 0.0691 | 0.0379
Epoch 30/300, seasonal_2 Loss: 0.0680 | 0.0371
Epoch 31/300, seasonal_2 Loss: 0.0668 | 0.0365
Epoch 32/300, seasonal_2 Loss: 0.0661 | 0.0359
Epoch 33/300, seasonal_2 Loss: 0.0655 | 0.0353
Epoch 34/300, seasonal_2 Loss: 0.0650 | 0.0345
Epoch 35/300, seasonal_2 Loss: 0.0643 | 0.0342
Epoch 36/300, seasonal_2 Loss: 0.0633 | 0.0332
Epoch 37/300, seasonal_2 Loss: 0.0621 | 0.0323
Epoch 38/300, seasonal_2 Loss: 0.0613 | 0.0314
Epoch 39/300, seasonal_2 Loss: 0.0610 | 0.0311
Epoch 40/300, seasonal_2 Loss: 0.0618 | 0.0325
Epoch 41/300, seasonal_2 Loss: 0.0644 | 0.0426
Epoch 42/300, seasonal_2 Loss: 0.0705 | 0.0382
Epoch 43/300, seasonal_2 Loss: 0.0694 | 0.0339
Epoch 44/300, seasonal_2 Loss: 0.0706 | 0.0357
Epoch 45/300, seasonal_2 Loss: 0.0627 | 0.0310
Epoch 46/300, seasonal_2 Loss: 0.0614 | 0.0308
Epoch 47/300, seasonal_2 Loss: 0.0607 | 0.0303
Epoch 48/300, seasonal_2 Loss: 0.0589 | 0.0298
Epoch 49/300, seasonal_2 Loss: 0.0584 | 0.0312
Epoch 50/300, seasonal_2 Loss: 0.0615 | 0.0294
Epoch 51/300, seasonal_2 Loss: 0.0585 | 0.0300
Epoch 52/300, seasonal_2 Loss: 0.0577 | 0.0304
Epoch 53/300, seasonal_2 Loss: 0.0573 | 0.0298
Epoch 54/300, seasonal_2 Loss: 0.0562 | 0.0274
Epoch 55/300, seasonal_2 Loss: 0.0563 | 0.0273
Epoch 56/300, seasonal_2 Loss: 0.0568 | 0.0284
Epoch 57/300, seasonal_2 Loss: 0.0629 | 0.0291
Epoch 58/300, seasonal_2 Loss: 0.0567 | 0.0252
Epoch 59/300, seasonal_2 Loss: 0.0558 | 0.0253
Epoch 60/300, seasonal_2 Loss: 0.0561 | 0.0267
Epoch 61/300, seasonal_2 Loss: 0.0557 | 0.0300
Epoch 62/300, seasonal_2 Loss: 0.0565 | 0.0296
Epoch 63/300, seasonal_2 Loss: 0.0582 | 0.0271
Epoch 64/300, seasonal_2 Loss: 0.0551 | 0.0256
Epoch 65/300, seasonal_2 Loss: 0.0547 | 0.0258
Epoch 66/300, seasonal_2 Loss: 0.0546 | 0.0251
Epoch 67/300, seasonal_2 Loss: 0.0541 | 0.0246
Epoch 68/300, seasonal_2 Loss: 0.0535 | 0.0230
Epoch 69/300, seasonal_2 Loss: 0.0547 | 0.0228
Epoch 70/300, seasonal_2 Loss: 0.0556 | 0.0252
Epoch 71/300, seasonal_2 Loss: 0.0539 | 0.0229
Epoch 72/300, seasonal_2 Loss: 0.0534 | 0.0224
Epoch 73/300, seasonal_2 Loss: 0.0534 | 0.0243
Epoch 74/300, seasonal_2 Loss: 0.0532 | 0.0240
Epoch 75/300, seasonal_2 Loss: 0.0535 | 0.0265
Epoch 76/300, seasonal_2 Loss: 0.0536 | 0.0223
Epoch 77/300, seasonal_2 Loss: 0.0536 | 0.0232
Epoch 78/300, seasonal_2 Loss: 0.0535 | 0.0239
Epoch 79/300, seasonal_2 Loss: 0.0545 | 0.0249
Epoch 80/300, seasonal_2 Loss: 0.0590 | 0.0235
Epoch 81/300, seasonal_2 Loss: 0.0635 | 0.0321
Epoch 82/300, seasonal_2 Loss: 0.0611 | 0.0288
Epoch 83/300, seasonal_2 Loss: 0.0587 | 0.0257
Epoch 84/300, seasonal_2 Loss: 0.0564 | 0.0228
Epoch 85/300, seasonal_2 Loss: 0.0544 | 0.0243
Epoch 86/300, seasonal_2 Loss: 0.0532 | 0.0243
Epoch 87/300, seasonal_2 Loss: 0.0530 | 0.0254
Epoch 88/300, seasonal_2 Loss: 0.0525 | 0.0223
Epoch 89/300, seasonal_2 Loss: 0.0521 | 0.0247
Epoch 90/300, seasonal_2 Loss: 0.0519 | 0.0256
Epoch 91/300, seasonal_2 Loss: 0.0521 | 0.0263
Epoch 92/300, seasonal_2 Loss: 0.0525 | 0.0270
Epoch 93/300, seasonal_2 Loss: 0.0529 | 0.0277
Epoch 94/300, seasonal_2 Loss: 0.0520 | 0.0298
Epoch 95/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 96/300, seasonal_2 Loss: 0.0514 | 0.0298
Epoch 97/300, seasonal_2 Loss: 0.0512 | 0.0282
Epoch 98/300, seasonal_2 Loss: 0.0515 | 0.0281
Epoch 99/300, seasonal_2 Loss: 0.0512 | 0.0271
Epoch 100/300, seasonal_2 Loss: 0.0520 | 0.0285
Epoch 101/300, seasonal_2 Loss: 0.0528 | 0.0291
Epoch 102/300, seasonal_2 Loss: 0.0545 | 0.0343
Epoch 103/300, seasonal_2 Loss: 0.0535 | 0.0307
Epoch 104/300, seasonal_2 Loss: 0.0529 | 0.0309
Epoch 105/300, seasonal_2 Loss: 0.0533 | 0.0285
Epoch 106/300, seasonal_2 Loss: 0.0547 | 0.0298
Epoch 107/300, seasonal_2 Loss: 0.0541 | 0.0289
Epoch 108/300, seasonal_2 Loss: 0.0572 | 0.0326
Epoch 109/300, seasonal_2 Loss: 0.0611 | 0.0454
Epoch 110/300, seasonal_2 Loss: 0.0617 | 0.0482
Epoch 111/300, seasonal_2 Loss: 0.0599 | 0.0367
Epoch 112/300, seasonal_2 Loss: 0.0577 | 0.0316
Epoch 113/300, seasonal_2 Loss: 0.0582 | 0.0283
Epoch 114/300, seasonal_2 Loss: 0.0568 | 0.0350
Epoch 115/300, seasonal_2 Loss: 0.0609 | 0.0358
Epoch 116/300, seasonal_2 Loss: 0.0647 | 0.0313
Epoch 117/300, seasonal_2 Loss: 0.0618 | 0.0307
Epoch 118/300, seasonal_2 Loss: 0.0658 | 0.0266
Epoch 119/300, seasonal_2 Loss: 0.0676 | 0.0302
Epoch 120/300, seasonal_2 Loss: 0.0758 | 0.0316
Epoch 121/300, seasonal_2 Loss: 0.0673 | 0.0736
Epoch 122/300, seasonal_2 Loss: 0.0787 | 0.1071
Epoch 123/300, seasonal_2 Loss: 0.0672 | 0.0642
Epoch 124/300, seasonal_2 Loss: 0.0633 | 0.0307
Epoch 125/300, seasonal_2 Loss: 0.0653 | 0.0353
Epoch 126/300, seasonal_2 Loss: 0.0593 | 0.0309
Epoch 127/300, seasonal_2 Loss: 0.0561 | 0.0265
Epoch 128/300, seasonal_2 Loss: 0.0532 | 0.0225
Epoch 129/300, seasonal_2 Loss: 0.0526 | 0.0241
Epoch 130/300, seasonal_2 Loss: 0.0527 | 0.0250
Epoch 131/300, seasonal_2 Loss: 0.0526 | 0.0260
Epoch 132/300, seasonal_2 Loss: 0.0517 | 0.0273
Epoch 133/300, seasonal_2 Loss: 0.0509 | 0.0281
Epoch 134/300, seasonal_2 Loss: 0.0502 | 0.0298
Epoch 135/300, seasonal_2 Loss: 0.0493 | 0.0312
Epoch 136/300, seasonal_2 Loss: 0.0489 | 0.0302
Epoch 137/300, seasonal_2 Loss: 0.0491 | 0.0287
Epoch 138/300, seasonal_2 Loss: 0.0499 | 0.0271
Epoch 139/300, seasonal_2 Loss: 0.0507 | 0.0268
Epoch 140/300, seasonal_2 Loss: 0.0511 | 0.0269
Epoch 141/300, seasonal_2 Loss: 0.0511 | 0.0281
Epoch 142/300, seasonal_2 Loss: 0.0508 | 0.0291
Epoch 143/300, seasonal_2 Loss: 0.0497 | 0.0297
Epoch 144/300, seasonal_2 Loss: 0.0486 | 0.0268
Epoch 145/300, seasonal_2 Loss: 0.0479 | 0.0250
Epoch 146/300, seasonal_2 Loss: 0.0480 | 0.0265
Epoch 147/300, seasonal_2 Loss: 0.0484 | 0.0266
Epoch 148/300, seasonal_2 Loss: 0.0485 | 0.0277
Epoch 149/300, seasonal_2 Loss: 0.0484 | 0.0294
Epoch 150/300, seasonal_2 Loss: 0.0480 | 0.0297
Epoch 151/300, seasonal_2 Loss: 0.0474 | 0.0287
Epoch 152/300, seasonal_2 Loss: 0.0471 | 0.0283
Epoch 153/300, seasonal_2 Loss: 0.0473 | 0.0277
Epoch 154/300, seasonal_2 Loss: 0.0478 | 0.0219
Epoch 155/300, seasonal_2 Loss: 0.0482 | 0.0230
Epoch 156/300, seasonal_2 Loss: 0.0479 | 0.0213
Epoch 157/300, seasonal_2 Loss: 0.0473 | 0.0245
Epoch 158/300, seasonal_2 Loss: 0.0466 | 0.0240
Epoch 159/300, seasonal_2 Loss: 0.0465 | 0.0252
Epoch 160/300, seasonal_2 Loss: 0.0465 | 0.0259
Epoch 161/300, seasonal_2 Loss: 0.0468 | 0.0267
Epoch 162/300, seasonal_2 Loss: 0.0471 | 0.0280
Epoch 163/300, seasonal_2 Loss: 0.0469 | 0.0267
Epoch 164/300, seasonal_2 Loss: 0.0466 | 0.0280
Epoch 165/300, seasonal_2 Loss: 0.0468 | 0.0282
Epoch 166/300, seasonal_2 Loss: 0.0469 | 0.0294
Epoch 167/300, seasonal_2 Loss: 0.0472 | 0.0289
Epoch 168/300, seasonal_2 Loss: 0.0475 | 0.0306
Epoch 169/300, seasonal_2 Loss: 0.0477 | 0.0304
Epoch 170/300, seasonal_2 Loss: 0.0480 | 0.0303
Epoch 171/300, seasonal_2 Loss: 0.0479 | 0.0301
Epoch 172/300, seasonal_2 Loss: 0.0476 | 0.0253
Epoch 173/300, seasonal_2 Loss: 0.0470 | 0.0237
Epoch 174/300, seasonal_2 Loss: 0.0473 | 0.0244
Epoch 175/300, seasonal_2 Loss: 0.0479 | 0.0263
Epoch 176/300, seasonal_2 Loss: 0.0476 | 0.0250
Epoch 177/300, seasonal_2 Loss: 0.0468 | 0.0281
Epoch 178/300, seasonal_2 Loss: 0.0467 | 0.0267
Epoch 179/300, seasonal_2 Loss: 0.0464 | 0.0286
Epoch 180/300, seasonal_2 Loss: 0.0460 | 0.0275
Epoch 181/300, seasonal_2 Loss: 0.0460 | 0.0268
Epoch 182/300, seasonal_2 Loss: 0.0462 | 0.0265
Epoch 183/300, seasonal_2 Loss: 0.0466 | 0.0290
Epoch 184/300, seasonal_2 Loss: 0.0474 | 0.0283
Epoch 185/300, seasonal_2 Loss: 0.0473 | 0.0298
Epoch 186/300, seasonal_2 Loss: 0.0462 | 0.0287
Epoch 187/300, seasonal_2 Loss: 0.0455 | 0.0286
Epoch 188/300, seasonal_2 Loss: 0.0456 | 0.0285
Epoch 189/300, seasonal_2 Loss: 0.0457 | 0.0278
Epoch 190/300, seasonal_2 Loss: 0.0454 | 0.0280
Epoch 191/300, seasonal_2 Loss: 0.0448 | 0.0290
Epoch 192/300, seasonal_2 Loss: 0.0447 | 0.0293
Epoch 193/300, seasonal_2 Loss: 0.0451 | 0.0285
Epoch 194/300, seasonal_2 Loss: 0.0453 | 0.0284
Epoch 195/300, seasonal_2 Loss: 0.0459 | 0.0282
Epoch 196/300, seasonal_2 Loss: 0.0464 | 0.0288
Epoch 197/300, seasonal_2 Loss: 0.0463 | 0.0284
Epoch 198/300, seasonal_2 Loss: 0.0462 | 0.0272
Epoch 199/300, seasonal_2 Loss: 0.0464 | 0.0283
Epoch 200/300, seasonal_2 Loss: 0.0499 | 0.0323
Epoch 201/300, seasonal_2 Loss: 0.0487 | 0.0321
Epoch 202/300, seasonal_2 Loss: 0.0495 | 0.0308
Epoch 203/300, seasonal_2 Loss: 0.0476 | 0.0279
Epoch 204/300, seasonal_2 Loss: 0.0456 | 0.0254
Epoch 205/300, seasonal_2 Loss: 0.0452 | 0.0272
Epoch 206/300, seasonal_2 Loss: 0.0449 | 0.0266
Epoch 207/300, seasonal_2 Loss: 0.0441 | 0.0263
Epoch 208/300, seasonal_2 Loss: 0.0437 | 0.0265
Epoch 209/300, seasonal_2 Loss: 0.0438 | 0.0290
Epoch 210/300, seasonal_2 Loss: 0.0430 | 0.0260
Epoch 211/300, seasonal_2 Loss: 0.0447 | 0.0318
Epoch 212/300, seasonal_2 Loss: 0.0438 | 0.0297
Epoch 213/300, seasonal_2 Loss: 0.0430 | 0.0295
Epoch 214/300, seasonal_2 Loss: 0.0429 | 0.0280
Epoch 215/300, seasonal_2 Loss: 0.0440 | 0.0342
Epoch 216/300, seasonal_2 Loss: 0.0423 | 0.0320
Epoch 217/300, seasonal_2 Loss: 0.0417 | 0.0302
Epoch 218/300, seasonal_2 Loss: 0.0410 | 0.0316
Epoch 219/300, seasonal_2 Loss: 0.0408 | 0.0314
Epoch 220/300, seasonal_2 Loss: 0.0457 | 0.0408
Epoch 221/300, seasonal_2 Loss: 0.0441 | 0.0348
Epoch 222/300, seasonal_2 Loss: 0.0469 | 0.0355
Epoch 223/300, seasonal_2 Loss: 0.0446 | 0.0276
Epoch 224/300, seasonal_2 Loss: 0.0439 | 0.0352
Epoch 225/300, seasonal_2 Loss: 0.0421 | 0.0316
Epoch 226/300, seasonal_2 Loss: 0.0407 | 0.0319
Epoch 227/300, seasonal_2 Loss: 0.0405 | 0.0308
Epoch 228/300, seasonal_2 Loss: 0.0406 | 0.0300
Epoch 229/300, seasonal_2 Loss: 0.0409 | 0.0288
Epoch 230/300, seasonal_2 Loss: 0.0420 | 0.0287
Epoch 231/300, seasonal_2 Loss: 0.0409 | 0.0308
Epoch 232/300, seasonal_2 Loss: 0.0422 | 0.0275
Epoch 233/300, seasonal_2 Loss: 0.0415 | 0.0285
Epoch 234/300, seasonal_2 Loss: 0.0408 | 0.0275
Epoch 235/300, seasonal_2 Loss: 0.0399 | 0.0289
Epoch 236/300, seasonal_2 Loss: 0.0392 | 0.0282
Epoch 237/300, seasonal_2 Loss: 0.0398 | 0.0295
Epoch 238/300, seasonal_2 Loss: 0.0404 | 0.0315
Epoch 239/300, seasonal_2 Loss: 0.0401 | 0.0266
Epoch 240/300, seasonal_2 Loss: 0.0409 | 0.0287
Epoch 241/300, seasonal_2 Loss: 0.0413 | 0.0327
Epoch 242/300, seasonal_2 Loss: 0.0405 | 0.0316
Epoch 243/300, seasonal_2 Loss: 0.0398 | 0.0343
Epoch 244/300, seasonal_2 Loss: 0.0471 | 0.0339
Epoch 245/300, seasonal_2 Loss: 0.0523 | 0.0341
Epoch 246/300, seasonal_2 Loss: 0.0495 | 0.0295
Epoch 247/300, seasonal_2 Loss: 0.0457 | 0.0242
Epoch 248/300, seasonal_2 Loss: 0.0408 | 0.0263
Epoch 249/300, seasonal_2 Loss: 0.0396 | 0.0244
Epoch 250/300, seasonal_2 Loss: 0.0392 | 0.0246
Epoch 251/300, seasonal_2 Loss: 0.0390 | 0.0238
Epoch 252/300, seasonal_2 Loss: 0.0380 | 0.0265
Epoch 253/300, seasonal_2 Loss: 0.0367 | 0.0274
Epoch 254/300, seasonal_2 Loss: 0.0377 | 0.0276
Epoch 255/300, seasonal_2 Loss: 0.0366 | 0.0305
Epoch 256/300, seasonal_2 Loss: 0.0364 | 0.0282
Epoch 257/300, seasonal_2 Loss: 0.0361 | 0.0303
Epoch 258/300, seasonal_2 Loss: 0.0400 | 0.0267
Epoch 259/300, seasonal_2 Loss: 0.0390 | 0.0298
Epoch 260/300, seasonal_2 Loss: 0.0378 | 0.0302
Epoch 261/300, seasonal_2 Loss: 0.0367 | 0.0310
Epoch 262/300, seasonal_2 Loss: 0.0370 | 0.0298
Epoch 263/300, seasonal_2 Loss: 0.0369 | 0.0319
Epoch 264/300, seasonal_2 Loss: 0.0365 | 0.0319
Epoch 265/300, seasonal_2 Loss: 0.0362 | 0.0320
Epoch 266/300, seasonal_2 Loss: 0.0357 | 0.0312
Epoch 267/300, seasonal_2 Loss: 0.0364 | 0.0319
Epoch 268/300, seasonal_2 Loss: 0.0358 | 0.0317
Epoch 269/300, seasonal_2 Loss: 0.0361 | 0.0305
Epoch 270/300, seasonal_2 Loss: 0.0364 | 0.0287
Epoch 271/300, seasonal_2 Loss: 0.0360 | 0.0315
Epoch 272/300, seasonal_2 Loss: 0.0355 | 0.0301
Epoch 273/300, seasonal_2 Loss: 0.0368 | 0.0305
Epoch 274/300, seasonal_2 Loss: 0.0349 | 0.0308
Epoch 275/300, seasonal_2 Loss: 0.0345 | 0.0317
Epoch 276/300, seasonal_2 Loss: 0.0343 | 0.0276
Epoch 277/300, seasonal_2 Loss: 0.0346 | 0.0295
Epoch 278/300, seasonal_2 Loss: 0.0396 | 0.0274
Epoch 279/300, seasonal_2 Loss: 0.0392 | 0.0305
Epoch 280/300, seasonal_2 Loss: 0.0381 | 0.0309
Epoch 281/300, seasonal_2 Loss: 0.0376 | 0.0294
Epoch 282/300, seasonal_2 Loss: 0.0367 | 0.0308
Epoch 283/300, seasonal_2 Loss: 0.0381 | 0.0303
Epoch 284/300, seasonal_2 Loss: 0.0372 | 0.0297
Epoch 285/300, seasonal_2 Loss: 0.0367 | 0.0301
Epoch 286/300, seasonal_2 Loss: 0.0359 | 0.0296
Epoch 287/300, seasonal_2 Loss: 0.0354 | 0.0286
Epoch 288/300, seasonal_2 Loss: 0.0352 | 0.0284
Epoch 289/300, seasonal_2 Loss: 0.0352 | 0.0284
Epoch 290/300, seasonal_2 Loss: 0.0354 | 0.0289
Epoch 291/300, seasonal_2 Loss: 0.0359 | 0.0296
Epoch 292/300, seasonal_2 Loss: 0.0361 | 0.0245
Epoch 293/300, seasonal_2 Loss: 0.0369 | 0.0275
Epoch 294/300, seasonal_2 Loss: 0.0375 | 0.0279
Epoch 295/300, seasonal_2 Loss: 0.0365 | 0.0278
Epoch 296/300, seasonal_2 Loss: 0.0365 | 0.0314
Epoch 297/300, seasonal_2 Loss: 0.0363 | 0.0278
Epoch 298/300, seasonal_2 Loss: 0.0352 | 0.0268
Epoch 299/300, seasonal_2 Loss: 0.0348 | 0.0242
Epoch 300/300, seasonal_2 Loss: 0.0347 | 0.0258
Training seasonal_3 component with params: {'observation_period_num': 21, 'train_rates': 0.9860033460337184, 'learning_rate': 0.0006475033857525854, 'batch_size': 130, 'step_size': 8, 'gamma': 0.821796322650269}
Epoch 1/300, seasonal_3 Loss: 0.3379 | 0.1733
Epoch 2/300, seasonal_3 Loss: 0.1951 | 0.1484
Epoch 3/300, seasonal_3 Loss: 0.1459 | 0.1086
Epoch 4/300, seasonal_3 Loss: 0.1550 | 0.1698
Epoch 5/300, seasonal_3 Loss: 0.1650 | 0.1029
Epoch 6/300, seasonal_3 Loss: 0.1829 | 0.1233
Epoch 7/300, seasonal_3 Loss: 0.2244 | 0.3454
Epoch 8/300, seasonal_3 Loss: 0.1951 | 0.2024
Epoch 9/300, seasonal_3 Loss: 0.1448 | 0.2162
Epoch 10/300, seasonal_3 Loss: 0.1475 | 0.2839
Epoch 11/300, seasonal_3 Loss: 0.1426 | 0.2269
Epoch 12/300, seasonal_3 Loss: 0.1152 | 0.0905
Epoch 13/300, seasonal_3 Loss: 0.1159 | 0.0751
Epoch 14/300, seasonal_3 Loss: 0.1136 | 0.0875
Epoch 15/300, seasonal_3 Loss: 0.1090 | 0.0818
Epoch 16/300, seasonal_3 Loss: 0.0922 | 0.0675
Epoch 17/300, seasonal_3 Loss: 0.0865 | 0.0511
Epoch 18/300, seasonal_3 Loss: 0.0884 | 0.0554
Epoch 19/300, seasonal_3 Loss: 0.0867 | 0.0475
Epoch 20/300, seasonal_3 Loss: 0.0869 | 0.0501
Epoch 21/300, seasonal_3 Loss: 0.0895 | 0.0493
Epoch 22/300, seasonal_3 Loss: 0.0879 | 0.0496
Epoch 23/300, seasonal_3 Loss: 0.0859 | 0.0519
Epoch 24/300, seasonal_3 Loss: 0.0825 | 0.0476
Epoch 25/300, seasonal_3 Loss: 0.0794 | 0.0443
Epoch 26/300, seasonal_3 Loss: 0.0802 | 0.0466
Epoch 27/300, seasonal_3 Loss: 0.0827 | 0.0491
Epoch 28/300, seasonal_3 Loss: 0.0804 | 0.0452
Epoch 29/300, seasonal_3 Loss: 0.0781 | 0.0448
Epoch 30/300, seasonal_3 Loss: 0.0747 | 0.0426
Epoch 31/300, seasonal_3 Loss: 0.0730 | 0.0413
Epoch 32/300, seasonal_3 Loss: 0.0733 | 0.0423
Epoch 33/300, seasonal_3 Loss: 0.0741 | 0.0447
Epoch 34/300, seasonal_3 Loss: 0.0736 | 0.0415
Epoch 35/300, seasonal_3 Loss: 0.0717 | 0.0391
Epoch 36/300, seasonal_3 Loss: 0.0706 | 0.0389
Epoch 37/300, seasonal_3 Loss: 0.0700 | 0.0398
Epoch 38/300, seasonal_3 Loss: 0.0691 | 0.0386
Epoch 39/300, seasonal_3 Loss: 0.0682 | 0.0374
Epoch 40/300, seasonal_3 Loss: 0.0674 | 0.0367
Epoch 41/300, seasonal_3 Loss: 0.0668 | 0.0360
Epoch 42/300, seasonal_3 Loss: 0.0664 | 0.0355
Epoch 43/300, seasonal_3 Loss: 0.0661 | 0.0350
Epoch 44/300, seasonal_3 Loss: 0.0657 | 0.0346
Epoch 45/300, seasonal_3 Loss: 0.0654 | 0.0345
Epoch 46/300, seasonal_3 Loss: 0.0652 | 0.0343
Epoch 47/300, seasonal_3 Loss: 0.0650 | 0.0341
Epoch 48/300, seasonal_3 Loss: 0.0648 | 0.0340
Epoch 49/300, seasonal_3 Loss: 0.0646 | 0.0339
Epoch 50/300, seasonal_3 Loss: 0.0644 | 0.0338
Epoch 51/300, seasonal_3 Loss: 0.0642 | 0.0339
Epoch 52/300, seasonal_3 Loss: 0.0641 | 0.0339
Epoch 53/300, seasonal_3 Loss: 0.0640 | 0.0338
Epoch 54/300, seasonal_3 Loss: 0.0638 | 0.0334
Epoch 55/300, seasonal_3 Loss: 0.0634 | 0.0332
Epoch 56/300, seasonal_3 Loss: 0.0632 | 0.0330
Epoch 57/300, seasonal_3 Loss: 0.0630 | 0.0328
Epoch 58/300, seasonal_3 Loss: 0.0628 | 0.0326
Epoch 59/300, seasonal_3 Loss: 0.0627 | 0.0324
Epoch 60/300, seasonal_3 Loss: 0.0625 | 0.0323
Epoch 61/300, seasonal_3 Loss: 0.0624 | 0.0322
Epoch 62/300, seasonal_3 Loss: 0.0623 | 0.0321
Epoch 63/300, seasonal_3 Loss: 0.0622 | 0.0320
Epoch 64/300, seasonal_3 Loss: 0.0621 | 0.0319
Epoch 65/300, seasonal_3 Loss: 0.0620 | 0.0318
Epoch 66/300, seasonal_3 Loss: 0.0619 | 0.0317
Epoch 67/300, seasonal_3 Loss: 0.0618 | 0.0316
Epoch 68/300, seasonal_3 Loss: 0.0617 | 0.0316
Epoch 69/300, seasonal_3 Loss: 0.0616 | 0.0315
Epoch 70/300, seasonal_3 Loss: 0.0616 | 0.0314
Epoch 71/300, seasonal_3 Loss: 0.0615 | 0.0313
Epoch 72/300, seasonal_3 Loss: 0.0614 | 0.0312
Epoch 73/300, seasonal_3 Loss: 0.0613 | 0.0312
Epoch 74/300, seasonal_3 Loss: 0.0613 | 0.0311
Epoch 75/300, seasonal_3 Loss: 0.0612 | 0.0310
Epoch 76/300, seasonal_3 Loss: 0.0612 | 0.0310
Epoch 77/300, seasonal_3 Loss: 0.0611 | 0.0309
Epoch 78/300, seasonal_3 Loss: 0.0610 | 0.0308
Epoch 79/300, seasonal_3 Loss: 0.0610 | 0.0308
Epoch 80/300, seasonal_3 Loss: 0.0609 | 0.0307
Epoch 81/300, seasonal_3 Loss: 0.0609 | 0.0306
Epoch 82/300, seasonal_3 Loss: 0.0608 | 0.0306
Epoch 83/300, seasonal_3 Loss: 0.0608 | 0.0305
Epoch 84/300, seasonal_3 Loss: 0.0608 | 0.0305
Epoch 85/300, seasonal_3 Loss: 0.0607 | 0.0304
Epoch 86/300, seasonal_3 Loss: 0.0607 | 0.0304
Epoch 87/300, seasonal_3 Loss: 0.0606 | 0.0303
Epoch 88/300, seasonal_3 Loss: 0.0606 | 0.0302
Epoch 89/300, seasonal_3 Loss: 0.0606 | 0.0302
Epoch 90/300, seasonal_3 Loss: 0.0605 | 0.0302
Epoch 91/300, seasonal_3 Loss: 0.0605 | 0.0301
Epoch 92/300, seasonal_3 Loss: 0.0605 | 0.0301
Epoch 93/300, seasonal_3 Loss: 0.0604 | 0.0300
Epoch 94/300, seasonal_3 Loss: 0.0604 | 0.0300
Epoch 95/300, seasonal_3 Loss: 0.0604 | 0.0299
Epoch 96/300, seasonal_3 Loss: 0.0604 | 0.0299
Epoch 97/300, seasonal_3 Loss: 0.0603 | 0.0299
Epoch 98/300, seasonal_3 Loss: 0.0603 | 0.0298
Epoch 99/300, seasonal_3 Loss: 0.0603 | 0.0298
Epoch 100/300, seasonal_3 Loss: 0.0603 | 0.0298
Epoch 101/300, seasonal_3 Loss: 0.0602 | 0.0297
Epoch 102/300, seasonal_3 Loss: 0.0602 | 0.0297
Epoch 103/300, seasonal_3 Loss: 0.0602 | 0.0297
Epoch 104/300, seasonal_3 Loss: 0.0602 | 0.0297
Epoch 105/300, seasonal_3 Loss: 0.0602 | 0.0296
Epoch 106/300, seasonal_3 Loss: 0.0602 | 0.0296
Epoch 107/300, seasonal_3 Loss: 0.0601 | 0.0296
Epoch 108/300, seasonal_3 Loss: 0.0601 | 0.0296
Epoch 109/300, seasonal_3 Loss: 0.0601 | 0.0295
Epoch 110/300, seasonal_3 Loss: 0.0601 | 0.0295
Epoch 111/300, seasonal_3 Loss: 0.0601 | 0.0295
Epoch 112/300, seasonal_3 Loss: 0.0601 | 0.0295
Epoch 113/300, seasonal_3 Loss: 0.0601 | 0.0294
Epoch 114/300, seasonal_3 Loss: 0.0600 | 0.0294
Epoch 115/300, seasonal_3 Loss: 0.0600 | 0.0294
Epoch 116/300, seasonal_3 Loss: 0.0600 | 0.0294
Epoch 117/300, seasonal_3 Loss: 0.0600 | 0.0294
Epoch 118/300, seasonal_3 Loss: 0.0600 | 0.0294
Epoch 119/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 120/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 121/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 122/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 123/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 124/300, seasonal_3 Loss: 0.0600 | 0.0293
Epoch 125/300, seasonal_3 Loss: 0.0599 | 0.0293
Epoch 126/300, seasonal_3 Loss: 0.0599 | 0.0293
Epoch 127/300, seasonal_3 Loss: 0.0599 | 0.0293
Epoch 128/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 129/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 130/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 131/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 132/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 133/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 134/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 135/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 136/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 137/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 138/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 139/300, seasonal_3 Loss: 0.0599 | 0.0292
Epoch 140/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 141/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 142/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 143/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 144/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 145/300, seasonal_3 Loss: 0.0599 | 0.0291
Epoch 146/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 147/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 148/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 149/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 150/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 151/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 152/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 153/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 154/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 155/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 156/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 157/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 158/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 159/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 160/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 161/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 162/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 163/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 164/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 165/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 166/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 167/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 168/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 169/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 170/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 171/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 172/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 173/300, seasonal_3 Loss: 0.0598 | 0.0291
Epoch 174/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 175/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 176/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 177/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 178/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 179/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 180/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 181/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 182/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 183/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 184/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 185/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 186/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 187/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 188/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 189/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 190/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 191/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 192/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 193/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 194/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 195/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 196/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 197/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 198/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 199/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 200/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 201/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 202/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 203/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 204/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 205/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 206/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 207/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 208/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 209/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 210/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 211/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 212/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 213/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 214/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 215/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 216/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 217/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 218/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 219/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 220/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 221/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 222/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 223/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 224/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 225/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 226/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 227/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 228/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 229/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 230/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 231/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 232/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 233/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 234/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 235/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 236/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 237/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 238/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 239/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 240/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 241/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 242/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 243/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 244/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 245/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 246/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 247/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 248/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 249/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 250/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 251/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 252/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 253/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 254/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 255/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 256/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 257/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 258/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 259/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 260/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 261/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 262/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 263/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 264/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 265/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 266/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 267/300, seasonal_3 Loss: 0.0598 | 0.0290
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.88160448450886, 'learning_rate': 0.00011461454332786872, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8652707023637851}
Epoch 1/300, resid Loss: 0.1823 | 0.0962
Epoch 2/300, resid Loss: 0.1141 | 0.0732
Epoch 3/300, resid Loss: 0.1025 | 0.0653
Epoch 4/300, resid Loss: 0.0967 | 0.0593
Epoch 5/300, resid Loss: 0.0936 | 0.0543
Epoch 6/300, resid Loss: 0.0905 | 0.0509
Epoch 7/300, resid Loss: 0.0874 | 0.0486
Epoch 8/300, resid Loss: 0.0854 | 0.0467
Epoch 9/300, resid Loss: 0.0835 | 0.0452
Epoch 10/300, resid Loss: 0.0815 | 0.0438
Epoch 11/300, resid Loss: 0.0800 | 0.0422
Epoch 12/300, resid Loss: 0.0784 | 0.0407
Epoch 13/300, resid Loss: 0.0769 | 0.0397
Epoch 14/300, resid Loss: 0.0760 | 0.0389
Epoch 15/300, resid Loss: 0.0751 | 0.0381
Epoch 16/300, resid Loss: 0.0741 | 0.0371
Epoch 17/300, resid Loss: 0.0732 | 0.0366
Epoch 18/300, resid Loss: 0.0724 | 0.0359
Epoch 19/300, resid Loss: 0.0714 | 0.0349
Epoch 20/300, resid Loss: 0.0704 | 0.0340
Epoch 21/300, resid Loss: 0.0694 | 0.0331
Epoch 22/300, resid Loss: 0.0686 | 0.0324
Epoch 23/300, resid Loss: 0.0681 | 0.0321
Epoch 24/300, resid Loss: 0.0678 | 0.0319
Epoch 25/300, resid Loss: 0.0674 | 0.0323
Epoch 26/300, resid Loss: 0.0671 | 0.0319
Epoch 27/300, resid Loss: 0.0668 | 0.0316
Epoch 28/300, resid Loss: 0.0665 | 0.0315
Epoch 29/300, resid Loss: 0.0663 | 0.0313
Epoch 30/300, resid Loss: 0.0660 | 0.0310
Epoch 31/300, resid Loss: 0.0657 | 0.0310
Epoch 32/300, resid Loss: 0.0655 | 0.0307
Epoch 33/300, resid Loss: 0.0653 | 0.0305
Epoch 34/300, resid Loss: 0.0650 | 0.0302
Epoch 35/300, resid Loss: 0.0648 | 0.0300
Epoch 36/300, resid Loss: 0.0646 | 0.0298
Epoch 37/300, resid Loss: 0.0644 | 0.0297
Epoch 38/300, resid Loss: 0.0642 | 0.0295
Epoch 39/300, resid Loss: 0.0640 | 0.0293
Epoch 40/300, resid Loss: 0.0638 | 0.0294
Epoch 41/300, resid Loss: 0.0636 | 0.0292
Epoch 42/300, resid Loss: 0.0635 | 0.0291
Epoch 43/300, resid Loss: 0.0632 | 0.0294
Epoch 44/300, resid Loss: 0.0631 | 0.0293
Epoch 45/300, resid Loss: 0.0630 | 0.0290
Epoch 46/300, resid Loss: 0.0628 | 0.0294
Epoch 47/300, resid Loss: 0.0627 | 0.0292
Epoch 48/300, resid Loss: 0.0625 | 0.0289
Epoch 49/300, resid Loss: 0.0624 | 0.0290
Epoch 50/300, resid Loss: 0.0623 | 0.0287
Epoch 51/300, resid Loss: 0.0621 | 0.0285
Epoch 52/300, resid Loss: 0.0620 | 0.0284
Epoch 53/300, resid Loss: 0.0619 | 0.0282
Epoch 54/300, resid Loss: 0.0618 | 0.0280
Epoch 55/300, resid Loss: 0.0617 | 0.0278
Epoch 56/300, resid Loss: 0.0616 | 0.0277
Epoch 57/300, resid Loss: 0.0615 | 0.0275
Epoch 58/300, resid Loss: 0.0614 | 0.0274
Epoch 59/300, resid Loss: 0.0614 | 0.0273
Epoch 60/300, resid Loss: 0.0613 | 0.0272
Epoch 61/300, resid Loss: 0.0612 | 0.0271
Epoch 62/300, resid Loss: 0.0612 | 0.0271
Epoch 63/300, resid Loss: 0.0611 | 0.0270
Epoch 64/300, resid Loss: 0.0611 | 0.0270
Epoch 65/300, resid Loss: 0.0610 | 0.0269
Epoch 66/300, resid Loss: 0.0610 | 0.0268
Epoch 67/300, resid Loss: 0.0609 | 0.0268
Epoch 68/300, resid Loss: 0.0609 | 0.0268
Epoch 69/300, resid Loss: 0.0608 | 0.0268
Epoch 70/300, resid Loss: 0.0608 | 0.0268
Epoch 71/300, resid Loss: 0.0607 | 0.0267
Epoch 72/300, resid Loss: 0.0607 | 0.0267
Epoch 73/300, resid Loss: 0.0606 | 0.0267
Epoch 74/300, resid Loss: 0.0606 | 0.0266
Epoch 75/300, resid Loss: 0.0606 | 0.0266
Epoch 76/300, resid Loss: 0.0606 | 0.0266
Epoch 77/300, resid Loss: 0.0605 | 0.0265
Epoch 78/300, resid Loss: 0.0605 | 0.0265
Epoch 79/300, resid Loss: 0.0605 | 0.0264
Epoch 80/300, resid Loss: 0.0604 | 0.0264
Epoch 81/300, resid Loss: 0.0604 | 0.0264
Epoch 82/300, resid Loss: 0.0604 | 0.0263
Epoch 83/300, resid Loss: 0.0604 | 0.0263
Epoch 84/300, resid Loss: 0.0603 | 0.0263
Epoch 85/300, resid Loss: 0.0603 | 0.0262
Epoch 86/300, resid Loss: 0.0603 | 0.0262
Epoch 87/300, resid Loss: 0.0603 | 0.0262
Epoch 88/300, resid Loss: 0.0603 | 0.0262
Epoch 89/300, resid Loss: 0.0602 | 0.0262
Epoch 90/300, resid Loss: 0.0602 | 0.0262
Epoch 91/300, resid Loss: 0.0602 | 0.0261
Epoch 92/300, resid Loss: 0.0602 | 0.0261
Epoch 93/300, resid Loss: 0.0602 | 0.0261
Epoch 94/300, resid Loss: 0.0602 | 0.0261
Epoch 95/300, resid Loss: 0.0601 | 0.0261
Epoch 96/300, resid Loss: 0.0601 | 0.0261
Epoch 97/300, resid Loss: 0.0601 | 0.0261
Epoch 98/300, resid Loss: 0.0601 | 0.0261
Epoch 99/300, resid Loss: 0.0601 | 0.0261
Epoch 100/300, resid Loss: 0.0601 | 0.0261
Epoch 101/300, resid Loss: 0.0601 | 0.0261
Epoch 102/300, resid Loss: 0.0601 | 0.0261
Epoch 103/300, resid Loss: 0.0601 | 0.0261
Epoch 104/300, resid Loss: 0.0601 | 0.0261
Epoch 105/300, resid Loss: 0.0600 | 0.0261
Epoch 106/300, resid Loss: 0.0600 | 0.0261
Epoch 107/300, resid Loss: 0.0600 | 0.0261
Epoch 108/300, resid Loss: 0.0600 | 0.0261
Epoch 109/300, resid Loss: 0.0600 | 0.0261
Epoch 110/300, resid Loss: 0.0600 | 0.0261
Epoch 111/300, resid Loss: 0.0600 | 0.0261
Epoch 112/300, resid Loss: 0.0600 | 0.0261
Epoch 113/300, resid Loss: 0.0600 | 0.0261
Epoch 114/300, resid Loss: 0.0600 | 0.0260
Epoch 115/300, resid Loss: 0.0600 | 0.0261
Epoch 116/300, resid Loss: 0.0600 | 0.0260
Epoch 117/300, resid Loss: 0.0599 | 0.0260
Epoch 118/300, resid Loss: 0.0599 | 0.0260
Epoch 119/300, resid Loss: 0.0599 | 0.0260
Epoch 120/300, resid Loss: 0.0599 | 0.0260
Epoch 121/300, resid Loss: 0.0599 | 0.0260
Epoch 122/300, resid Loss: 0.0599 | 0.0260
Epoch 123/300, resid Loss: 0.0599 | 0.0260
Epoch 124/300, resid Loss: 0.0599 | 0.0260
Epoch 125/300, resid Loss: 0.0599 | 0.0260
Epoch 126/300, resid Loss: 0.0599 | 0.0260
Epoch 127/300, resid Loss: 0.0599 | 0.0260
Epoch 128/300, resid Loss: 0.0599 | 0.0260
Epoch 129/300, resid Loss: 0.0599 | 0.0260
Epoch 130/300, resid Loss: 0.0599 | 0.0260
Epoch 131/300, resid Loss: 0.0599 | 0.0260
Epoch 132/300, resid Loss: 0.0599 | 0.0260
Epoch 133/300, resid Loss: 0.0599 | 0.0260
Epoch 134/300, resid Loss: 0.0599 | 0.0260
Epoch 135/300, resid Loss: 0.0599 | 0.0260
Epoch 136/300, resid Loss: 0.0599 | 0.0260
Epoch 137/300, resid Loss: 0.0599 | 0.0260
Epoch 138/300, resid Loss: 0.0599 | 0.0260
Epoch 139/300, resid Loss: 0.0599 | 0.0260
Epoch 140/300, resid Loss: 0.0599 | 0.0260
Epoch 141/300, resid Loss: 0.0599 | 0.0260
Epoch 142/300, resid Loss: 0.0598 | 0.0260
Epoch 143/300, resid Loss: 0.0598 | 0.0260
Epoch 144/300, resid Loss: 0.0598 | 0.0260
Epoch 145/300, resid Loss: 0.0598 | 0.0260
Epoch 146/300, resid Loss: 0.0598 | 0.0260
Epoch 147/300, resid Loss: 0.0598 | 0.0260
Epoch 148/300, resid Loss: 0.0598 | 0.0260
Epoch 149/300, resid Loss: 0.0598 | 0.0260
Epoch 150/300, resid Loss: 0.0598 | 0.0260
Epoch 151/300, resid Loss: 0.0598 | 0.0260
Epoch 152/300, resid Loss: 0.0598 | 0.0260
Epoch 153/300, resid Loss: 0.0598 | 0.0260
Epoch 154/300, resid Loss: 0.0598 | 0.0260
Epoch 155/300, resid Loss: 0.0598 | 0.0260
Epoch 156/300, resid Loss: 0.0598 | 0.0260
Epoch 157/300, resid Loss: 0.0598 | 0.0260
Epoch 158/300, resid Loss: 0.0598 | 0.0260
Epoch 159/300, resid Loss: 0.0598 | 0.0260
Epoch 160/300, resid Loss: 0.0598 | 0.0260
Epoch 161/300, resid Loss: 0.0598 | 0.0260
Epoch 162/300, resid Loss: 0.0598 | 0.0260
Epoch 163/300, resid Loss: 0.0598 | 0.0260
Epoch 164/300, resid Loss: 0.0598 | 0.0260
Epoch 165/300, resid Loss: 0.0598 | 0.0260
Epoch 166/300, resid Loss: 0.0598 | 0.0260
Epoch 167/300, resid Loss: 0.0598 | 0.0260
Epoch 168/300, resid Loss: 0.0598 | 0.0260
Epoch 169/300, resid Loss: 0.0598 | 0.0260
Epoch 170/300, resid Loss: 0.0598 | 0.0260
Epoch 171/300, resid Loss: 0.0598 | 0.0260
Epoch 172/300, resid Loss: 0.0598 | 0.0260
Epoch 173/300, resid Loss: 0.0598 | 0.0260
Epoch 174/300, resid Loss: 0.0598 | 0.0260
Epoch 175/300, resid Loss: 0.0598 | 0.0260
Epoch 176/300, resid Loss: 0.0598 | 0.0260
Epoch 177/300, resid Loss: 0.0598 | 0.0260
Epoch 178/300, resid Loss: 0.0598 | 0.0260
Epoch 179/300, resid Loss: 0.0598 | 0.0260
Epoch 180/300, resid Loss: 0.0598 | 0.0260
Epoch 181/300, resid Loss: 0.0598 | 0.0260
Epoch 182/300, resid Loss: 0.0598 | 0.0260
Epoch 183/300, resid Loss: 0.0598 | 0.0260
Epoch 184/300, resid Loss: 0.0598 | 0.0260
Epoch 185/300, resid Loss: 0.0598 | 0.0260
Epoch 186/300, resid Loss: 0.0598 | 0.0260
Epoch 187/300, resid Loss: 0.0598 | 0.0260
Epoch 188/300, resid Loss: 0.0598 | 0.0260
Epoch 189/300, resid Loss: 0.0598 | 0.0260
Epoch 190/300, resid Loss: 0.0598 | 0.0260
Epoch 191/300, resid Loss: 0.0598 | 0.0260
Epoch 192/300, resid Loss: 0.0598 | 0.0260
Epoch 193/300, resid Loss: 0.0598 | 0.0260
Epoch 194/300, resid Loss: 0.0598 | 0.0260
Epoch 195/300, resid Loss: 0.0598 | 0.0260
Epoch 196/300, resid Loss: 0.0598 | 0.0260
Epoch 197/300, resid Loss: 0.0598 | 0.0260
Epoch 198/300, resid Loss: 0.0598 | 0.0260
Epoch 199/300, resid Loss: 0.0598 | 0.0260
Epoch 200/300, resid Loss: 0.0598 | 0.0260
Epoch 201/300, resid Loss: 0.0598 | 0.0260
Epoch 202/300, resid Loss: 0.0598 | 0.0260
Epoch 203/300, resid Loss: 0.0598 | 0.0260
Epoch 204/300, resid Loss: 0.0598 | 0.0260
Epoch 205/300, resid Loss: 0.0598 | 0.0260
Epoch 206/300, resid Loss: 0.0598 | 0.0260
Epoch 207/300, resid Loss: 0.0598 | 0.0260
Epoch 208/300, resid Loss: 0.0598 | 0.0260
Epoch 209/300, resid Loss: 0.0598 | 0.0260
Epoch 210/300, resid Loss: 0.0598 | 0.0260
Epoch 211/300, resid Loss: 0.0598 | 0.0260
Epoch 212/300, resid Loss: 0.0598 | 0.0260
Epoch 213/300, resid Loss: 0.0598 | 0.0260
Epoch 214/300, resid Loss: 0.0598 | 0.0260
Epoch 215/300, resid Loss: 0.0598 | 0.0260
Epoch 216/300, resid Loss: 0.0598 | 0.0260
Epoch 217/300, resid Loss: 0.0598 | 0.0260
Epoch 218/300, resid Loss: 0.0598 | 0.0260
Epoch 219/300, resid Loss: 0.0598 | 0.0260
Epoch 220/300, resid Loss: 0.0598 | 0.0260
Epoch 221/300, resid Loss: 0.0598 | 0.0260
Epoch 222/300, resid Loss: 0.0598 | 0.0260
Epoch 223/300, resid Loss: 0.0598 | 0.0260
Epoch 224/300, resid Loss: 0.0598 | 0.0260
Epoch 225/300, resid Loss: 0.0598 | 0.0260
Epoch 226/300, resid Loss: 0.0598 | 0.0260
Epoch 227/300, resid Loss: 0.0598 | 0.0260
Epoch 228/300, resid Loss: 0.0598 | 0.0260
Epoch 229/300, resid Loss: 0.0598 | 0.0260
Epoch 230/300, resid Loss: 0.0598 | 0.0260
Epoch 231/300, resid Loss: 0.0598 | 0.0260
Epoch 232/300, resid Loss: 0.0598 | 0.0260
Epoch 233/300, resid Loss: 0.0598 | 0.0260
Epoch 234/300, resid Loss: 0.0598 | 0.0260
Epoch 235/300, resid Loss: 0.0598 | 0.0260
Epoch 236/300, resid Loss: 0.0598 | 0.0260
Epoch 237/300, resid Loss: 0.0598 | 0.0260
Epoch 238/300, resid Loss: 0.0598 | 0.0260
Epoch 239/300, resid Loss: 0.0598 | 0.0260
Epoch 240/300, resid Loss: 0.0598 | 0.0260
Epoch 241/300, resid Loss: 0.0598 | 0.0260
Epoch 242/300, resid Loss: 0.0598 | 0.0260
Epoch 243/300, resid Loss: 0.0598 | 0.0260
Epoch 244/300, resid Loss: 0.0598 | 0.0260
Epoch 245/300, resid Loss: 0.0598 | 0.0260
Epoch 246/300, resid Loss: 0.0598 | 0.0260
Epoch 247/300, resid Loss: 0.0598 | 0.0260
Epoch 248/300, resid Loss: 0.0598 | 0.0260
Epoch 249/300, resid Loss: 0.0598 | 0.0260
Epoch 250/300, resid Loss: 0.0598 | 0.0260
Epoch 251/300, resid Loss: 0.0598 | 0.0260
Epoch 252/300, resid Loss: 0.0598 | 0.0260
Epoch 253/300, resid Loss: 0.0598 | 0.0260
Epoch 254/300, resid Loss: 0.0598 | 0.0260
Epoch 255/300, resid Loss: 0.0598 | 0.0260
Epoch 256/300, resid Loss: 0.0598 | 0.0260
Epoch 257/300, resid Loss: 0.0598 | 0.0260
Epoch 258/300, resid Loss: 0.0598 | 0.0260
Epoch 259/300, resid Loss: 0.0598 | 0.0260
Epoch 260/300, resid Loss: 0.0598 | 0.0260
Epoch 261/300, resid Loss: 0.0598 | 0.0260
Epoch 262/300, resid Loss: 0.0598 | 0.0260
Epoch 263/300, resid Loss: 0.0598 | 0.0260
Epoch 264/300, resid Loss: 0.0598 | 0.0260
Epoch 265/300, resid Loss: 0.0598 | 0.0260
Epoch 266/300, resid Loss: 0.0598 | 0.0260
Epoch 267/300, resid Loss: 0.0598 | 0.0260
Epoch 268/300, resid Loss: 0.0598 | 0.0260
Epoch 269/300, resid Loss: 0.0598 | 0.0260
Epoch 270/300, resid Loss: 0.0598 | 0.0260
Epoch 271/300, resid Loss: 0.0598 | 0.0260
Epoch 272/300, resid Loss: 0.0598 | 0.0260
Epoch 273/300, resid Loss: 0.0598 | 0.0260
Epoch 274/300, resid Loss: 0.0598 | 0.0260
Epoch 275/300, resid Loss: 0.0598 | 0.0260
Epoch 276/300, resid Loss: 0.0598 | 0.0260
Epoch 277/300, resid Loss: 0.0598 | 0.0260
Epoch 278/300, resid Loss: 0.0598 | 0.0260
Epoch 279/300, resid Loss: 0.0598 | 0.0260
Epoch 280/300, resid Loss: 0.0598 | 0.0260
Epoch 281/300, resid Loss: 0.0598 | 0.0260
Epoch 282/300, resid Loss: 0.0598 | 0.0260
Epoch 283/300, resid Loss: 0.0598 | 0.0260
Epoch 284/300, resid Loss: 0.0598 | 0.0260
Epoch 285/300, resid Loss: 0.0598 | 0.0260
Epoch 286/300, resid Loss: 0.0598 | 0.0260
Epoch 287/300, resid Loss: 0.0598 | 0.0260
Epoch 288/300, resid Loss: 0.0598 | 0.0260
Epoch 289/300, resid Loss: 0.0598 | 0.0260
Epoch 290/300, resid Loss: 0.0598 | 0.0260
Epoch 291/300, resid Loss: 0.0598 | 0.0260
Epoch 292/300, resid Loss: 0.0598 | 0.0260
Epoch 293/300, resid Loss: 0.0598 | 0.0260
Epoch 294/300, resid Loss: 0.0598 | 0.0260
Epoch 295/300, resid Loss: 0.0598 | 0.0260
Epoch 296/300, resid Loss: 0.0598 | 0.0260
Epoch 297/300, resid Loss: 0.0598 | 0.0260
Epoch 298/300, resid Loss: 0.0598 | 0.0260
Epoch 299/300, resid Loss: 0.0598 | 0.0260
Epoch 300/300, resid Loss: 0.0598 | 0.0260
Runtime (seconds): 2074.6059873104095
0.00017030802740725277
[215.31003]
[1.3924865]
[-0.02876204]
[4.8040824]
[-0.01950324]
[6.6455517]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 22.71583580970764
RMSE: 4.76611328125
MAE: 4.76611328125
R-squared: nan
[228.10388]
