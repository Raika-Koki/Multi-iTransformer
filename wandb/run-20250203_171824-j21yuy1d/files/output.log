ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 17:18:25,904][0m A new study created in memory with name: no-name-992f8a03-6db5-4503-a400-0ee84d8987af[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-03 17:22:38,898][0m Trial 0 finished with value: 0.0810799960430588 and parameters: {'observation_period_num': 49, 'train_rates': 0.6052710984285995, 'learning_rate': 8.581853911484273e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7807119254636962}. Best is trial 0 with value: 0.0810799960430588.[0m
[32m[I 2025-02-03 17:25:47,415][0m Trial 1 finished with value: 0.1326635594972793 and parameters: {'observation_period_num': 224, 'train_rates': 0.9005698422395452, 'learning_rate': 0.00013018104672062874, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8005654649934695}. Best is trial 0 with value: 0.0810799960430588.[0m
[32m[I 2025-02-03 17:26:47,170][0m Trial 2 finished with value: 0.37295647601090504 and parameters: {'observation_period_num': 188, 'train_rates': 0.6698278613090675, 'learning_rate': 5.876466389235307e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.8677025323988277}. Best is trial 0 with value: 0.0810799960430588.[0m
[32m[I 2025-02-03 17:28:42,581][0m Trial 3 finished with value: 0.06485531638320695 and parameters: {'observation_period_num': 79, 'train_rates': 0.8780717107406786, 'learning_rate': 0.00011823563427981011, 'batch_size': 48, 'step_size': 5, 'gamma': 0.7557246235118301}. Best is trial 3 with value: 0.06485531638320695.[0m
[32m[I 2025-02-03 17:29:36,002][0m Trial 4 finished with value: 0.5375887674086419 and parameters: {'observation_period_num': 231, 'train_rates': 0.6790173182109592, 'learning_rate': 4.134146900500829e-06, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8922397234953111}. Best is trial 3 with value: 0.06485531638320695.[0m
[32m[I 2025-02-03 17:30:07,216][0m Trial 5 finished with value: 0.47050901093199604 and parameters: {'observation_period_num': 137, 'train_rates': 0.6097877728171904, 'learning_rate': 0.0006268391495118076, 'batch_size': 141, 'step_size': 10, 'gamma': 0.9441647343541684}. Best is trial 3 with value: 0.06485531638320695.[0m
[32m[I 2025-02-03 17:31:50,228][0m Trial 6 finished with value: 0.10895514004979746 and parameters: {'observation_period_num': 111, 'train_rates': 0.9123879972093909, 'learning_rate': 0.0009429759815252963, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8144442270275662}. Best is trial 3 with value: 0.06485531638320695.[0m
[32m[I 2025-02-03 17:34:34,293][0m Trial 7 finished with value: 0.05227353303059595 and parameters: {'observation_period_num': 56, 'train_rates': 0.9243935246672547, 'learning_rate': 5.726898671115964e-05, 'batch_size': 35, 'step_size': 12, 'gamma': 0.8133650441097813}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:35:09,637][0m Trial 8 finished with value: 0.4721483588218689 and parameters: {'observation_period_num': 157, 'train_rates': 0.9810523079887599, 'learning_rate': 3.8043358414885335e-06, 'batch_size': 176, 'step_size': 7, 'gamma': 0.982063896353242}. Best is trial 7 with value: 0.05227353303059595.[0m
Early stopping at epoch 74
[32m[I 2025-02-03 17:35:30,469][0m Trial 9 finished with value: 2.3123703002929688 and parameters: {'observation_period_num': 167, 'train_rates': 0.9535245146859456, 'learning_rate': 6.88336743150818e-06, 'batch_size': 233, 'step_size': 1, 'gamma': 0.862085181333711}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:36:22,313][0m Trial 10 finished with value: 0.0762587493245906 and parameters: {'observation_period_num': 11, 'train_rates': 0.8083960891124983, 'learning_rate': 1.673218250101893e-05, 'batch_size': 107, 'step_size': 15, 'gamma': 0.8338775185169292}. Best is trial 7 with value: 0.05227353303059595.[0m
Early stopping at epoch 92
[32m[I 2025-02-03 17:37:06,867][0m Trial 11 finished with value: 0.13872853298614055 and parameters: {'observation_period_num': 80, 'train_rates': 0.8308722438797843, 'learning_rate': 0.00022870269114441457, 'batch_size': 119, 'step_size': 2, 'gamma': 0.758227148389586}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:39:09,970][0m Trial 12 finished with value: 0.0761017965374833 and parameters: {'observation_period_num': 72, 'train_rates': 0.8659352176087312, 'learning_rate': 2.3667817167495887e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.7508698453351316}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:40:07,532][0m Trial 13 finished with value: 1.5517538073667245 and parameters: {'observation_period_num': 14, 'train_rates': 0.7557858078837731, 'learning_rate': 1.1977826438381578e-06, 'batch_size': 90, 'step_size': 9, 'gamma': 0.7956387655418189}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:40:45,362][0m Trial 14 finished with value: 0.10108105200701036 and parameters: {'observation_period_num': 98, 'train_rates': 0.9272207110455459, 'learning_rate': 0.00022185679248294188, 'batch_size': 157, 'step_size': 4, 'gamma': 0.8305947053448366}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:41:15,715][0m Trial 15 finished with value: 0.09745088948345766 and parameters: {'observation_period_num': 41, 'train_rates': 0.8592507391662224, 'learning_rate': 4.149637062114921e-05, 'batch_size': 201, 'step_size': 13, 'gamma': 0.776307431179793}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:42:51,904][0m Trial 16 finished with value: 0.06817358253654175 and parameters: {'observation_period_num': 50, 'train_rates': 0.7686624430839742, 'learning_rate': 0.00028174453610382047, 'batch_size': 52, 'step_size': 3, 'gamma': 0.8991005888302138}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:43:43,550][0m Trial 17 finished with value: 0.23522207208655097 and parameters: {'observation_period_num': 112, 'train_rates': 0.8762242372853627, 'learning_rate': 1.2470716018779797e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.8410766523667709}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:46:10,561][0m Trial 18 finished with value: 0.0703616083452576 and parameters: {'observation_period_num': 79, 'train_rates': 0.9671510546723546, 'learning_rate': 0.00011864186809431569, 'batch_size': 40, 'step_size': 12, 'gamma': 0.7732896672472305}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:47:31,166][0m Trial 19 finished with value: 0.11030528125205265 and parameters: {'observation_period_num': 132, 'train_rates': 0.8349747850449523, 'learning_rate': 4.0594029982273416e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8120491736141257}. Best is trial 7 with value: 0.05227353303059595.[0m
[32m[I 2025-02-03 17:52:50,105][0m Trial 20 finished with value: 0.04956718693601894 and parameters: {'observation_period_num': 34, 'train_rates': 0.9361004098294984, 'learning_rate': 0.0004296767681690208, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9093962941488921}. Best is trial 20 with value: 0.04956718693601894.[0m
[32m[I 2025-02-03 17:55:58,813][0m Trial 21 finished with value: 0.04464115732416664 and parameters: {'observation_period_num': 26, 'train_rates': 0.9339686596502897, 'learning_rate': 0.0005155897154045203, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9201625420330889}. Best is trial 21 with value: 0.04464115732416664.[0m
[32m[I 2025-02-03 18:01:57,150][0m Trial 22 finished with value: 0.04213403076643035 and parameters: {'observation_period_num': 30, 'train_rates': 0.9355017402282599, 'learning_rate': 0.00048565811235321357, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9247873585995677}. Best is trial 22 with value: 0.04213403076643035.[0m
[32m[I 2025-02-03 18:05:01,478][0m Trial 23 finished with value: 0.04313673065629548 and parameters: {'observation_period_num': 26, 'train_rates': 0.9431280056717094, 'learning_rate': 0.00047320293505173964, 'batch_size': 32, 'step_size': 1, 'gamma': 0.9277803116763683}. Best is trial 22 with value: 0.04213403076643035.[0m
[32m[I 2025-02-03 18:06:07,759][0m Trial 24 finished with value: 0.03714189678430557 and parameters: {'observation_period_num': 5, 'train_rates': 0.9865785333711675, 'learning_rate': 0.0009604535136590349, 'batch_size': 95, 'step_size': 1, 'gamma': 0.9314262726338094}. Best is trial 24 with value: 0.03714189678430557.[0m
[32m[I 2025-02-03 18:07:16,668][0m Trial 25 finished with value: 0.035856787115335464 and parameters: {'observation_period_num': 27, 'train_rates': 0.9843141474400887, 'learning_rate': 0.0007855184336553985, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9494911810235032}. Best is trial 25 with value: 0.035856787115335464.[0m
[32m[I 2025-02-03 18:08:22,999][0m Trial 26 finished with value: 0.021241584792733192 and parameters: {'observation_period_num': 8, 'train_rates': 0.985793298323535, 'learning_rate': 0.0008605679610728868, 'batch_size': 95, 'step_size': 2, 'gamma': 0.9617782556153507}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:09:31,248][0m Trial 27 finished with value: 0.028773972764611244 and parameters: {'observation_period_num': 8, 'train_rates': 0.9866153639721159, 'learning_rate': 0.0009260177580962272, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9700131248746682}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:10:19,163][0m Trial 28 finished with value: 0.05127604305744171 and parameters: {'observation_period_num': 58, 'train_rates': 0.9790402038598333, 'learning_rate': 0.0002925787724321828, 'batch_size': 130, 'step_size': 3, 'gamma': 0.9838984543587539}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:10:59,764][0m Trial 29 finished with value: 0.03142805526513514 and parameters: {'observation_period_num': 7, 'train_rates': 0.9039054276749413, 'learning_rate': 0.0009551148816360749, 'batch_size': 148, 'step_size': 2, 'gamma': 0.9578094321964175}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:11:40,678][0m Trial 30 finished with value: 0.057412081093776345 and parameters: {'observation_period_num': 64, 'train_rates': 0.8952230319908857, 'learning_rate': 0.00017149110573941312, 'batch_size': 151, 'step_size': 3, 'gamma': 0.9668198364044076}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:12:40,872][0m Trial 31 finished with value: 0.038343724388290534 and parameters: {'observation_period_num': 5, 'train_rates': 0.962786847800935, 'learning_rate': 0.0009069368476536252, 'batch_size': 103, 'step_size': 2, 'gamma': 0.9556032234015547}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:13:16,326][0m Trial 32 finished with value: 0.04185870289802551 and parameters: {'observation_period_num': 19, 'train_rates': 0.957537525302357, 'learning_rate': 0.0006883476328607011, 'batch_size': 179, 'step_size': 3, 'gamma': 0.9680988427385112}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:14:01,525][0m Trial 33 finished with value: 0.05390235829429749 and parameters: {'observation_period_num': 43, 'train_rates': 0.9061404436890993, 'learning_rate': 0.00033445409853591904, 'batch_size': 133, 'step_size': 6, 'gamma': 0.946219625237691}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:15:14,633][0m Trial 34 finished with value: 0.14476822316646576 and parameters: {'observation_period_num': 200, 'train_rates': 0.9869651276041724, 'learning_rate': 0.000686954957254744, 'batch_size': 81, 'step_size': 2, 'gamma': 0.969515412291314}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:16:04,245][0m Trial 35 finished with value: 0.06182884328247767 and parameters: {'observation_period_num': 23, 'train_rates': 0.901291932436236, 'learning_rate': 0.00036246628793981526, 'batch_size': 121, 'step_size': 5, 'gamma': 0.9890242264816186}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:17:39,824][0m Trial 36 finished with value: 0.06689053829481352 and parameters: {'observation_period_num': 39, 'train_rates': 0.9541427629679035, 'learning_rate': 9.109106301837127e-05, 'batch_size': 63, 'step_size': 1, 'gamma': 0.9529472527477386}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:18:37,104][0m Trial 37 finished with value: 0.07059578840040935 and parameters: {'observation_period_num': 16, 'train_rates': 0.7110915690818613, 'learning_rate': 0.00018054732194196287, 'batch_size': 88, 'step_size': 6, 'gamma': 0.9443155414790434}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:19:17,432][0m Trial 38 finished with value: 0.11826325953006744 and parameters: {'observation_period_num': 48, 'train_rates': 0.987857088274556, 'learning_rate': 0.0009939009128078528, 'batch_size': 165, 'step_size': 5, 'gamma': 0.9621058590355401}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:20:32,925][0m Trial 39 finished with value: 0.06876873858510309 and parameters: {'observation_period_num': 95, 'train_rates': 0.8900616731829591, 'learning_rate': 0.0006430449474154342, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8808885042698933}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:21:00,903][0m Trial 40 finished with value: 0.17452301309046142 and parameters: {'observation_period_num': 238, 'train_rates': 0.8455820353696378, 'learning_rate': 0.0007085561529067475, 'batch_size': 205, 'step_size': 2, 'gamma': 0.9758216243595331}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:22:07,783][0m Trial 41 finished with value: 0.03467227518558502 and parameters: {'observation_period_num': 13, 'train_rates': 0.988032469250949, 'learning_rate': 0.0009356925594278194, 'batch_size': 96, 'step_size': 1, 'gamma': 0.9311993604059394}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:23:10,763][0m Trial 42 finished with value: 0.042380768805742264 and parameters: {'observation_period_num': 7, 'train_rates': 0.9684244623369426, 'learning_rate': 0.0005549419281558571, 'batch_size': 99, 'step_size': 1, 'gamma': 0.9338391309557157}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:23:54,969][0m Trial 43 finished with value: 0.04058219544690658 and parameters: {'observation_period_num': 32, 'train_rates': 0.9244132157074931, 'learning_rate': 0.00035939157858852825, 'batch_size': 141, 'step_size': 3, 'gamma': 0.9402806667453841}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:24:47,878][0m Trial 44 finished with value: 0.043724496734060654 and parameters: {'observation_period_num': 19, 'train_rates': 0.9503486846025276, 'learning_rate': 0.0007450677570929852, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9151018614381181}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:26:02,709][0m Trial 45 finished with value: 0.06820729374885559 and parameters: {'observation_period_num': 64, 'train_rates': 0.9721278989817742, 'learning_rate': 0.0009836251969309606, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9572720783642049}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:26:38,485][0m Trial 46 finished with value: 0.1273795824880912 and parameters: {'observation_period_num': 152, 'train_rates': 0.6178615837789847, 'learning_rate': 0.0004456408670375795, 'batch_size': 127, 'step_size': 3, 'gamma': 0.9073922712099203}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:28:10,829][0m Trial 47 finished with value: 0.5005836486816406 and parameters: {'observation_period_num': 212, 'train_rates': 0.9899805812197131, 'learning_rate': 1.4582911243979431e-06, 'batch_size': 62, 'step_size': 2, 'gamma': 0.975611535143238}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:28:36,235][0m Trial 48 finished with value: 0.04332224231298643 and parameters: {'observation_period_num': 14, 'train_rates': 0.9110347964366127, 'learning_rate': 0.00025386373698297704, 'batch_size': 252, 'step_size': 4, 'gamma': 0.9397502981623389}. Best is trial 26 with value: 0.021241584792733192.[0m
[32m[I 2025-02-03 18:29:27,230][0m Trial 49 finished with value: 0.10834178861638682 and parameters: {'observation_period_num': 37, 'train_rates': 0.8060366521060354, 'learning_rate': 0.00016283600705408336, 'batch_size': 111, 'step_size': 1, 'gamma': 0.8858126463540837}. Best is trial 26 with value: 0.021241584792733192.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 18:29:27,240][0m A new study created in memory with name: no-name-2cd9877e-6298-429e-8d3a-a59580f8138b[0m
[32m[I 2025-02-03 18:30:22,839][0m Trial 0 finished with value: 0.12551880082381622 and parameters: {'observation_period_num': 128, 'train_rates': 0.841450915228204, 'learning_rate': 0.0006771030914283709, 'batch_size': 97, 'step_size': 12, 'gamma': 0.8500249433059188}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:31:18,054][0m Trial 1 finished with value: 0.13536095573670334 and parameters: {'observation_period_num': 202, 'train_rates': 0.8692484892516851, 'learning_rate': 4.631882714783054e-05, 'batch_size': 97, 'step_size': 12, 'gamma': 0.8884457094814627}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:31:44,283][0m Trial 2 finished with value: 0.6790056560785716 and parameters: {'observation_period_num': 174, 'train_rates': 0.8112823654462138, 'learning_rate': 8.838353096398883e-06, 'batch_size': 211, 'step_size': 3, 'gamma': 0.8557765247542732}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:34:00,069][0m Trial 3 finished with value: 0.14285968180666578 and parameters: {'observation_period_num': 218, 'train_rates': 0.9315211548282705, 'learning_rate': 2.7367546784764697e-05, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8349606283899191}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:34:43,015][0m Trial 4 finished with value: 0.28262691171123433 and parameters: {'observation_period_num': 219, 'train_rates': 0.6195442535651595, 'learning_rate': 3.975275130057995e-05, 'batch_size': 100, 'step_size': 5, 'gamma': 0.9869925055411654}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:35:08,057][0m Trial 5 finished with value: 0.19470740346074442 and parameters: {'observation_period_num': 207, 'train_rates': 0.742684564969714, 'learning_rate': 0.0005390671281634334, 'batch_size': 223, 'step_size': 11, 'gamma': 0.9195283638337922}. Best is trial 0 with value: 0.12551880082381622.[0m
[32m[I 2025-02-03 18:35:55,256][0m Trial 6 finished with value: 0.09486280606602723 and parameters: {'observation_period_num': 124, 'train_rates': 0.8688964956357765, 'learning_rate': 0.0002675788459076782, 'batch_size': 121, 'step_size': 7, 'gamma': 0.9510826980246103}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:36:41,331][0m Trial 7 finished with value: 0.5580951153457939 and parameters: {'observation_period_num': 110, 'train_rates': 0.9459961815123418, 'learning_rate': 2.531695561547723e-06, 'batch_size': 128, 'step_size': 13, 'gamma': 0.7703544090541041}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:37:03,707][0m Trial 8 finished with value: 1.8899415089005351 and parameters: {'observation_period_num': 42, 'train_rates': 0.7614640251449418, 'learning_rate': 1.6703844466767654e-06, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8021862544443293}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:39:23,594][0m Trial 9 finished with value: 0.1283781684285872 and parameters: {'observation_period_num': 9, 'train_rates': 0.7542149454493793, 'learning_rate': 3.099311030736143e-06, 'batch_size': 36, 'step_size': 9, 'gamma': 0.8750050719143427}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:39:53,772][0m Trial 10 finished with value: 0.10508815835932069 and parameters: {'observation_period_num': 78, 'train_rates': 0.6552182077330273, 'learning_rate': 0.00018663503675516628, 'batch_size': 166, 'step_size': 15, 'gamma': 0.9751524336841256}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:40:21,651][0m Trial 11 finished with value: 0.1306835209395899 and parameters: {'observation_period_num': 79, 'train_rates': 0.6329976105382936, 'learning_rate': 0.00017618671901412796, 'batch_size': 174, 'step_size': 15, 'gamma': 0.9785849009267734}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:40:53,582][0m Trial 12 finished with value: 0.12381109146342328 and parameters: {'observation_period_num': 68, 'train_rates': 0.7043253789663467, 'learning_rate': 0.00017463874945852957, 'batch_size': 160, 'step_size': 1, 'gamma': 0.9348179367463952}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:41:32,258][0m Trial 13 finished with value: 0.17116292543930584 and parameters: {'observation_period_num': 157, 'train_rates': 0.8917000479027798, 'learning_rate': 0.0002056881770099508, 'batch_size': 147, 'step_size': 9, 'gamma': 0.9556892827699581}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:41:59,762][0m Trial 14 finished with value: 0.11640630922020467 and parameters: {'observation_period_num': 96, 'train_rates': 0.6823082816202122, 'learning_rate': 9.346979497266946e-05, 'batch_size': 189, 'step_size': 15, 'gamma': 0.9100196031176638}. Best is trial 6 with value: 0.09486280606602723.[0m
[32m[I 2025-02-03 18:43:34,415][0m Trial 15 finished with value: 0.03347114473581314 and parameters: {'observation_period_num': 42, 'train_rates': 0.9787096223372829, 'learning_rate': 0.0009686758589989683, 'batch_size': 64, 'step_size': 7, 'gamma': 0.9499487732220393}. Best is trial 15 with value: 0.03347114473581314.[0m
[32m[I 2025-02-03 18:45:07,474][0m Trial 16 finished with value: 0.03803032413125038 and parameters: {'observation_period_num': 22, 'train_rates': 0.9745600887004179, 'learning_rate': 0.0007814292360505832, 'batch_size': 65, 'step_size': 7, 'gamma': 0.9462604253206739}. Best is trial 15 with value: 0.03347114473581314.[0m
[32m[I 2025-02-03 18:46:43,291][0m Trial 17 finished with value: 0.03262337249251875 and parameters: {'observation_period_num': 7, 'train_rates': 0.9658364970062293, 'learning_rate': 0.0005670250489359773, 'batch_size': 64, 'step_size': 5, 'gamma': 0.9005004586604695}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:48:18,978][0m Trial 18 finished with value: 0.035193637013435364 and parameters: {'observation_period_num': 44, 'train_rates': 0.9810870661717125, 'learning_rate': 0.00041451703597622844, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9024431227096336}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:52:36,674][0m Trial 19 finished with value: 0.039146119624791864 and parameters: {'observation_period_num': 41, 'train_rates': 0.9139913143825421, 'learning_rate': 0.0008788195016246871, 'batch_size': 22, 'step_size': 1, 'gamma': 0.82048375177899}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:54:05,922][0m Trial 20 finished with value: 0.0771728903055191 and parameters: {'observation_period_num': 10, 'train_rates': 0.9863569440301518, 'learning_rate': 1.5931142862368447e-05, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8827319133274828}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:55:40,421][0m Trial 21 finished with value: 0.048373526824054436 and parameters: {'observation_period_num': 44, 'train_rates': 0.9540259321235818, 'learning_rate': 0.0003590999425105615, 'batch_size': 64, 'step_size': 5, 'gamma': 0.9081646738485029}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:56:47,430][0m Trial 22 finished with value: 0.13012797516755392 and parameters: {'observation_period_num': 248, 'train_rates': 0.9097426296087561, 'learning_rate': 7.872330984289122e-05, 'batch_size': 81, 'step_size': 4, 'gamma': 0.9002559683333473}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 18:59:08,762][0m Trial 23 finished with value: 0.1605871617794037 and parameters: {'observation_period_num': 63, 'train_rates': 0.9865945508718292, 'learning_rate': 0.0004517330762586302, 'batch_size': 42, 'step_size': 6, 'gamma': 0.9307389402316133}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:01:07,081][0m Trial 24 finished with value: 0.04901399014339055 and parameters: {'observation_period_num': 31, 'train_rates': 0.9500983737866646, 'learning_rate': 8.657303469556773e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8599032802898605}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:01:55,324][0m Trial 25 finished with value: 0.05493180245450399 and parameters: {'observation_period_num': 58, 'train_rates': 0.822234644634408, 'learning_rate': 0.00035501553895409015, 'batch_size': 114, 'step_size': 3, 'gamma': 0.8964817436554865}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:07:41,550][0m Trial 26 finished with value: 0.03751247202207942 and parameters: {'observation_period_num': 5, 'train_rates': 0.8867427988129984, 'learning_rate': 0.0008360900744323145, 'batch_size': 16, 'step_size': 2, 'gamma': 0.959746738947161}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:08:50,988][0m Trial 27 finished with value: 0.0376463970885827 and parameters: {'observation_period_num': 28, 'train_rates': 0.9246309027621893, 'learning_rate': 0.0009626427273147668, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8756966885298465}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:10:31,717][0m Trial 28 finished with value: 0.07527406416325406 and parameters: {'observation_period_num': 86, 'train_rates': 0.9693311198705656, 'learning_rate': 0.00012518256921581684, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9229833809931076}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:11:23,856][0m Trial 29 finished with value: 0.05811482323270266 and parameters: {'observation_period_num': 55, 'train_rates': 0.8479728516711071, 'learning_rate': 0.0005139168633814192, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9388661493935526}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:12:36,903][0m Trial 30 finished with value: 0.08105082369663498 and parameters: {'observation_period_num': 107, 'train_rates': 0.9537023907654869, 'learning_rate': 0.00031888805651290864, 'batch_size': 82, 'step_size': 10, 'gamma': 0.9653866797813888}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:16:58,607][0m Trial 31 finished with value: 0.037463882114165105 and parameters: {'observation_period_num': 5, 'train_rates': 0.8765090188523157, 'learning_rate': 0.0009834881655098593, 'batch_size': 21, 'step_size': 2, 'gamma': 0.9600177185194487}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:19:48,757][0m Trial 32 finished with value: 0.04269622519852653 and parameters: {'observation_period_num': 24, 'train_rates': 0.894092581637138, 'learning_rate': 0.0006092870653235087, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9191435258548138}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:21:33,111][0m Trial 33 finished with value: 0.04306305153295398 and parameters: {'observation_period_num': 21, 'train_rates': 0.860961905913914, 'learning_rate': 0.0005741928229403318, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8932468853994219}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:24:57,480][0m Trial 34 finished with value: 0.04356002503323673 and parameters: {'observation_period_num': 47, 'train_rates': 0.9306527009574979, 'learning_rate': 0.0009993301274278498, 'batch_size': 28, 'step_size': 2, 'gamma': 0.8455205896885487}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:25:55,839][0m Trial 35 finished with value: 0.20942465329059848 and parameters: {'observation_period_num': 155, 'train_rates': 0.8069670057498051, 'learning_rate': 0.00027786192427361746, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9646534047683428}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:26:55,368][0m Trial 36 finished with value: 0.13291636109352112 and parameters: {'observation_period_num': 5, 'train_rates': 0.9669221104109922, 'learning_rate': 8.695323006002366e-06, 'batch_size': 104, 'step_size': 5, 'gamma': 0.861598582719399}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:28:15,341][0m Trial 37 finished with value: 0.050452462815660605 and parameters: {'observation_period_num': 31, 'train_rates': 0.8323912428635976, 'learning_rate': 0.000437420888621361, 'batch_size': 69, 'step_size': 4, 'gamma': 0.9827163317535378}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:30:16,863][0m Trial 38 finished with value: 0.04958293479843152 and parameters: {'observation_period_num': 38, 'train_rates': 0.9339665151488874, 'learning_rate': 0.0006665708404562276, 'batch_size': 48, 'step_size': 3, 'gamma': 0.9136833621645091}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:35:44,188][0m Trial 39 finished with value: 0.03374335628339193 and parameters: {'observation_period_num': 16, 'train_rates': 0.9019046353777722, 'learning_rate': 3.4929351385371216e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9438874029081629}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:36:23,409][0m Trial 40 finished with value: 0.21737717881798743 and parameters: {'observation_period_num': 134, 'train_rates': 0.7869934514339632, 'learning_rate': 2.362512705213216e-05, 'batch_size': 138, 'step_size': 8, 'gamma': 0.9275686865522871}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:41:56,156][0m Trial 41 finished with value: 0.03856130763888359 and parameters: {'observation_period_num': 16, 'train_rates': 0.9897965802337259, 'learning_rate': 5.700215366770674e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9437083553506376}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:44:38,302][0m Trial 42 finished with value: 0.06384777091443539 and parameters: {'observation_period_num': 18, 'train_rates': 0.9011493812707077, 'learning_rate': 6.221196800993746e-06, 'batch_size': 35, 'step_size': 6, 'gamma': 0.9711134276809689}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:46:43,863][0m Trial 43 finished with value: 0.06802727643287543 and parameters: {'observation_period_num': 52, 'train_rates': 0.8750089311416379, 'learning_rate': 0.00024264565916795643, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9890951525793685}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:47:44,753][0m Trial 44 finished with value: 0.07056261071314414 and parameters: {'observation_period_num': 71, 'train_rates': 0.9375010781978889, 'learning_rate': 0.00013549880772531674, 'batch_size': 97, 'step_size': 5, 'gamma': 0.7567881794120888}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:51:08,911][0m Trial 45 finished with value: 0.19004773166106673 and parameters: {'observation_period_num': 35, 'train_rates': 0.9664291318992694, 'learning_rate': 1.2304572036781536e-06, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9517035681857602}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:52:31,141][0m Trial 46 finished with value: 0.039343863877370884 and parameters: {'observation_period_num': 9, 'train_rates': 0.9251694989670034, 'learning_rate': 4.4531299266177626e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.8867128825918493}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:52:57,461][0m Trial 47 finished with value: 0.043291124301669724 and parameters: {'observation_period_num': 16, 'train_rates': 0.8559145975729174, 'learning_rate': 0.0004640165957225572, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9369566935839232}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:54:43,066][0m Trial 48 finished with value: 0.06049347890556351 and parameters: {'observation_period_num': 50, 'train_rates': 0.8749478499087978, 'learning_rate': 3.0435967803838432e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9091524654522092}. Best is trial 17 with value: 0.03262337249251875.[0m
[32m[I 2025-02-03 19:58:16,682][0m Trial 49 finished with value: 0.0687797011632938 and parameters: {'observation_period_num': 88, 'train_rates': 0.9108470453585653, 'learning_rate': 2.013774999145511e-05, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9485293393846178}. Best is trial 17 with value: 0.03262337249251875.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 19:58:16,692][0m A new study created in memory with name: no-name-ce2e086d-0c39-4cad-b0df-664b70ba6750[0m
[32m[I 2025-02-03 19:59:20,768][0m Trial 0 finished with value: 0.8053352629266134 and parameters: {'observation_period_num': 248, 'train_rates': 0.7726943532872226, 'learning_rate': 1.1318304517751412e-06, 'batch_size': 75, 'step_size': 9, 'gamma': 0.9368950033523054}. Best is trial 0 with value: 0.8053352629266134.[0m
[32m[I 2025-02-03 19:59:43,779][0m Trial 1 finished with value: 1.6209409445456942 and parameters: {'observation_period_num': 248, 'train_rates': 0.6721837071383695, 'learning_rate': 4.534159409376336e-06, 'batch_size': 214, 'step_size': 1, 'gamma': 0.9410285942058003}. Best is trial 0 with value: 0.8053352629266134.[0m
[32m[I 2025-02-03 20:00:44,490][0m Trial 2 finished with value: 0.9445555806159973 and parameters: {'observation_period_num': 126, 'train_rates': 0.9695295985096866, 'learning_rate': 1.203553500104979e-06, 'batch_size': 98, 'step_size': 9, 'gamma': 0.7584595881540587}. Best is trial 0 with value: 0.8053352629266134.[0m
[32m[I 2025-02-03 20:01:10,086][0m Trial 3 finished with value: 0.0821765538958539 and parameters: {'observation_period_num': 20, 'train_rates': 0.6557051091566064, 'learning_rate': 0.00032525125433264954, 'batch_size': 212, 'step_size': 4, 'gamma': 0.8056424895141681}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:02:20,711][0m Trial 4 finished with value: 0.09099487215280533 and parameters: {'observation_period_num': 183, 'train_rates': 0.9710133937704033, 'learning_rate': 0.0002814146298085369, 'batch_size': 83, 'step_size': 8, 'gamma': 0.768376806298593}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:03:08,195][0m Trial 5 finished with value: 0.36762373070243326 and parameters: {'observation_period_num': 177, 'train_rates': 0.688042902600069, 'learning_rate': 2.3452394427672962e-05, 'batch_size': 100, 'step_size': 6, 'gamma': 0.8204381975376575}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:03:34,376][0m Trial 6 finished with value: 0.09969815193007221 and parameters: {'observation_period_num': 83, 'train_rates': 0.691556933528153, 'learning_rate': 0.0003106254522187907, 'batch_size': 205, 'step_size': 5, 'gamma': 0.8686056125494763}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:03:55,735][0m Trial 7 finished with value: 0.5830186746752857 and parameters: {'observation_period_num': 96, 'train_rates': 0.6212024222790529, 'learning_rate': 5.042297696439935e-06, 'batch_size': 241, 'step_size': 9, 'gamma': 0.9147157475942289}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:04:39,320][0m Trial 8 finished with value: 0.15972893130827465 and parameters: {'observation_period_num': 95, 'train_rates': 0.6104812614446752, 'learning_rate': 0.0001518514241498698, 'batch_size': 107, 'step_size': 7, 'gamma': 0.9744454371223681}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:05:14,026][0m Trial 9 finished with value: 0.6565888240785882 and parameters: {'observation_period_num': 181, 'train_rates': 0.854353802118383, 'learning_rate': 2.6191231162712437e-06, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8873057830631497}. Best is trial 3 with value: 0.0821765538958539.[0m
[32m[I 2025-02-03 20:05:49,638][0m Trial 10 finished with value: 0.07456604068304275 and parameters: {'observation_period_num': 18, 'train_rates': 0.8052207544640066, 'learning_rate': 5.156476159632221e-05, 'batch_size': 164, 'step_size': 14, 'gamma': 0.8267331002326412}. Best is trial 10 with value: 0.07456604068304275.[0m
[32m[I 2025-02-03 20:06:24,619][0m Trial 11 finished with value: 0.060578367018953284 and parameters: {'observation_period_num': 12, 'train_rates': 0.8080354980608246, 'learning_rate': 5.7408971642083066e-05, 'batch_size': 168, 'step_size': 15, 'gamma': 0.8216714302980724}. Best is trial 11 with value: 0.060578367018953284.[0m
[32m[I 2025-02-03 20:09:00,382][0m Trial 12 finished with value: 0.03863870003705571 and parameters: {'observation_period_num': 13, 'train_rates': 0.8396427094858782, 'learning_rate': 4.784776425299193e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8399395523396411}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:09:43,789][0m Trial 13 finished with value: 0.09116287078332463 and parameters: {'observation_period_num': 40, 'train_rates': 0.8879213964291319, 'learning_rate': 2.7664462068016236e-05, 'batch_size': 140, 'step_size': 15, 'gamma': 0.8548611052859683}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:14:15,692][0m Trial 14 finished with value: 0.09395818268770681 and parameters: {'observation_period_num': 60, 'train_rates': 0.7647402801131347, 'learning_rate': 7.268495493187146e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7996051828978952}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:17:23,562][0m Trial 15 finished with value: 0.05372484081557819 and parameters: {'observation_period_num': 5, 'train_rates': 0.8932364260850824, 'learning_rate': 1.2271837319187767e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8524397872148874}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:21:34,301][0m Trial 16 finished with value: 0.06449765508337844 and parameters: {'observation_period_num': 59, 'train_rates': 0.9040914464795506, 'learning_rate': 1.0058166354824658e-05, 'batch_size': 22, 'step_size': 12, 'gamma': 0.8970492139679718}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:23:20,868][0m Trial 17 finished with value: 0.07529754855338185 and parameters: {'observation_period_num': 50, 'train_rates': 0.9102281351149305, 'learning_rate': 1.5435550159880198e-05, 'batch_size': 53, 'step_size': 12, 'gamma': 0.852917259042566}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:25:06,214][0m Trial 18 finished with value: 0.09802081149233424 and parameters: {'observation_period_num': 141, 'train_rates': 0.8364877243261556, 'learning_rate': 0.00012169683162256551, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7873156453868243}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:26:55,988][0m Trial 19 finished with value: 0.11466902728301125 and parameters: {'observation_period_num': 6, 'train_rates': 0.7265111892235878, 'learning_rate': 9.11754397908153e-06, 'batch_size': 45, 'step_size': 11, 'gamma': 0.843859878539265}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:27:41,399][0m Trial 20 finished with value: 0.10092124369192483 and parameters: {'observation_period_num': 121, 'train_rates': 0.8597654637023726, 'learning_rate': 0.0008749903200797259, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8815141867934505}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:28:11,818][0m Trial 21 finished with value: 0.08999973506547133 and parameters: {'observation_period_num': 33, 'train_rates': 0.8238294496979041, 'learning_rate': 4.291469652928732e-05, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8341125231200633}. Best is trial 12 with value: 0.03863870003705571.[0m
[32m[I 2025-02-03 20:29:44,113][0m Trial 22 finished with value: 0.03798306480714 and parameters: {'observation_period_num': 5, 'train_rates': 0.9314124308430736, 'learning_rate': 8.042938687400072e-05, 'batch_size': 65, 'step_size': 15, 'gamma': 0.8142026456047169}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:31:19,606][0m Trial 23 finished with value: 0.08987215773335525 and parameters: {'observation_period_num': 75, 'train_rates': 0.9415799339691451, 'learning_rate': 0.00011083731764403356, 'batch_size': 62, 'step_size': 14, 'gamma': 0.7855250920122877}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:34:20,089][0m Trial 24 finished with value: 0.051903727330007955 and parameters: {'observation_period_num': 36, 'train_rates': 0.9311615525485759, 'learning_rate': 1.77102990320526e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8657843949821038}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:37:05,853][0m Trial 25 finished with value: 0.050014635577877574 and parameters: {'observation_period_num': 34, 'train_rates': 0.9359905165254091, 'learning_rate': 2.4132678648834078e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8680061889629976}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:38:48,887][0m Trial 26 finished with value: 0.04819269850850105 and parameters: {'observation_period_num': 29, 'train_rates': 0.9887153009976406, 'learning_rate': 3.6315064139629555e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9059494525900178}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:40:18,043][0m Trial 27 finished with value: 0.2111506164073944 and parameters: {'observation_period_num': 69, 'train_rates': 0.9894230624338262, 'learning_rate': 0.00019011108005205718, 'batch_size': 68, 'step_size': 15, 'gamma': 0.9133353140883289}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:41:31,086][0m Trial 28 finished with value: 0.09054765634630856 and parameters: {'observation_period_num': 27, 'train_rates': 0.9610445814480588, 'learning_rate': 8.13547213026486e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.9899055258298113}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:42:41,653][0m Trial 29 finished with value: 0.07747350456742584 and parameters: {'observation_period_num': 47, 'train_rates': 0.772564737071096, 'learning_rate': 4.144987147144771e-05, 'batch_size': 74, 'step_size': 13, 'gamma': 0.9429722243450078}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:43:26,614][0m Trial 30 finished with value: 0.23925679388330945 and parameters: {'observation_period_num': 235, 'train_rates': 0.8766951891189486, 'learning_rate': 3.475373385880637e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9182426568008896}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:45:44,351][0m Trial 31 finished with value: 0.050300558363752704 and parameters: {'observation_period_num': 27, 'train_rates': 0.9329466937887746, 'learning_rate': 2.262535458631007e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8788120411180909}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:48:26,608][0m Trial 32 finished with value: 0.049574900418519974 and parameters: {'observation_period_num': 47, 'train_rates': 0.989012468237118, 'learning_rate': 9.201425553725855e-05, 'batch_size': 37, 'step_size': 2, 'gamma': 0.8969118711300351}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:50:07,686][0m Trial 33 finished with value: 0.09713809192180634 and parameters: {'observation_period_num': 51, 'train_rates': 0.9898052161802392, 'learning_rate': 8.911282406114117e-05, 'batch_size': 61, 'step_size': 1, 'gamma': 0.9301708821916878}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:55:49,800][0m Trial 34 finished with value: 0.0485603459660084 and parameters: {'observation_period_num': 20, 'train_rates': 0.9578514294012399, 'learning_rate': 0.0006262216590127705, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9006037847804313}. Best is trial 22 with value: 0.03798306480714.[0m
[32m[I 2025-02-03 20:57:02,171][0m Trial 35 finished with value: 0.03627711940895428 and parameters: {'observation_period_num': 16, 'train_rates': 0.9590640048866931, 'learning_rate': 0.0007857562616438256, 'batch_size': 85, 'step_size': 4, 'gamma': 0.9021341255230515}. Best is trial 35 with value: 0.03627711940895428.[0m
[32m[I 2025-02-03 20:58:07,095][0m Trial 36 finished with value: 0.03589573559852747 and parameters: {'observation_period_num': 17, 'train_rates': 0.9114933359027958, 'learning_rate': 0.0005152403848311492, 'batch_size': 90, 'step_size': 4, 'gamma': 0.9614177718030043}. Best is trial 36 with value: 0.03589573559852747.[0m
[32m[I 2025-02-03 20:59:10,358][0m Trial 37 finished with value: 0.031601176097989084 and parameters: {'observation_period_num': 5, 'train_rates': 0.9152115690472944, 'learning_rate': 0.00048167493317498354, 'batch_size': 94, 'step_size': 4, 'gamma': 0.9619418228544955}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:00:13,412][0m Trial 38 finished with value: 0.11017031828915724 and parameters: {'observation_period_num': 109, 'train_rates': 0.9160865119566923, 'learning_rate': 0.0004816928079101821, 'batch_size': 91, 'step_size': 4, 'gamma': 0.957016760852582}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:01:08,220][0m Trial 39 finished with value: 0.11976303455750804 and parameters: {'observation_period_num': 152, 'train_rates': 0.9571045668170894, 'learning_rate': 0.00042895186924774855, 'batch_size': 108, 'step_size': 5, 'gamma': 0.9570491488505948}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:01:53,017][0m Trial 40 finished with value: 0.03817980596538407 and parameters: {'observation_period_num': 5, 'train_rates': 0.8771673190670828, 'learning_rate': 0.00022745574479043996, 'batch_size': 134, 'step_size': 3, 'gamma': 0.9684747227079421}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:02:34,055][0m Trial 41 finished with value: 0.041973128014213434 and parameters: {'observation_period_num': 5, 'train_rates': 0.8765655077566777, 'learning_rate': 0.00021170523407231312, 'batch_size': 145, 'step_size': 3, 'gamma': 0.9694919971883522}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:03:35,295][0m Trial 42 finished with value: 0.039278247859328985 and parameters: {'observation_period_num': 19, 'train_rates': 0.9044983556943254, 'learning_rate': 0.0009458708320078181, 'batch_size': 95, 'step_size': 5, 'gamma': 0.9870374098308041}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:04:28,411][0m Trial 43 finished with value: 0.03826365285576918 and parameters: {'observation_period_num': 19, 'train_rates': 0.8669668846193377, 'learning_rate': 0.00025782850215717385, 'batch_size': 110, 'step_size': 4, 'gamma': 0.9368049338956519}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:05:41,475][0m Trial 44 finished with value: 0.042457963986623236 and parameters: {'observation_period_num': 18, 'train_rates': 0.9221786766727442, 'learning_rate': 0.0006227236163562962, 'batch_size': 81, 'step_size': 6, 'gamma': 0.953648762701226}. Best is trial 37 with value: 0.031601176097989084.[0m
Early stopping at epoch 93
[32m[I 2025-02-03 21:06:28,058][0m Trial 45 finished with value: 0.1307518118387693 and parameters: {'observation_period_num': 84, 'train_rates': 0.9463828982312399, 'learning_rate': 0.0004278293520112917, 'batch_size': 124, 'step_size': 2, 'gamma': 0.7502139871924924}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:07:42,028][0m Trial 46 finished with value: 0.10451620315461849 and parameters: {'observation_period_num': 197, 'train_rates': 0.8923800211048821, 'learning_rate': 0.0003345527602396088, 'batch_size': 74, 'step_size': 3, 'gamma': 0.9730469852920083}. Best is trial 37 with value: 0.031601176097989084.[0m
[32m[I 2025-02-03 21:08:41,831][0m Trial 47 finished with value: 0.028815506026148796 and parameters: {'observation_period_num': 6, 'train_rates': 0.9720425285742778, 'learning_rate': 0.0006151717938397003, 'batch_size': 103, 'step_size': 6, 'gamma': 0.9288865499055178}. Best is trial 47 with value: 0.028815506026148796.[0m
[32m[I 2025-02-03 21:09:42,086][0m Trial 48 finished with value: 0.07771093791849176 and parameters: {'observation_period_num': 64, 'train_rates': 0.950867905756878, 'learning_rate': 0.0006363326842532646, 'batch_size': 101, 'step_size': 7, 'gamma': 0.9225817603910523}. Best is trial 47 with value: 0.028815506026148796.[0m
[32m[I 2025-02-03 21:10:52,082][0m Trial 49 finished with value: 0.04521215707063675 and parameters: {'observation_period_num': 42, 'train_rates': 0.9743775682940627, 'learning_rate': 0.0003956264478231971, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9456199748381242}. Best is trial 47 with value: 0.028815506026148796.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 21:10:52,092][0m A new study created in memory with name: no-name-335c933b-c7bb-4cd3-8500-21e4e8edd0b3[0m
[32m[I 2025-02-03 21:11:29,548][0m Trial 0 finished with value: 0.15472522377967834 and parameters: {'observation_period_num': 233, 'train_rates': 0.9709570450863088, 'learning_rate': 0.00010007719016050732, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8750742952777789}. Best is trial 0 with value: 0.15472522377967834.[0m
[32m[I 2025-02-03 21:14:53,852][0m Trial 1 finished with value: 0.24389948656012722 and parameters: {'observation_period_num': 239, 'train_rates': 0.827868830104082, 'learning_rate': 0.0003524953328364209, 'batch_size': 24, 'step_size': 11, 'gamma': 0.7896700251421199}. Best is trial 0 with value: 0.15472522377967834.[0m
Early stopping at epoch 42
[32m[I 2025-02-03 21:15:08,017][0m Trial 2 finished with value: 0.8992913046190816 and parameters: {'observation_period_num': 173, 'train_rates': 0.7769409988902352, 'learning_rate': 2.8770403178243484e-05, 'batch_size': 168, 'step_size': 1, 'gamma': 0.7604496972518073}. Best is trial 0 with value: 0.15472522377967834.[0m
[32m[I 2025-02-03 21:15:44,133][0m Trial 3 finished with value: 0.040148328399766085 and parameters: {'observation_period_num': 34, 'train_rates': 0.9244193500101121, 'learning_rate': 0.0006934436724381587, 'batch_size': 171, 'step_size': 14, 'gamma': 0.8894730271265469}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:16:18,662][0m Trial 4 finished with value: 0.22731535759839144 and parameters: {'observation_period_num': 32, 'train_rates': 0.7174389197137739, 'learning_rate': 1.4760171183091812e-05, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8394230005570069}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:17:05,914][0m Trial 5 finished with value: 0.17887007898276613 and parameters: {'observation_period_num': 70, 'train_rates': 0.9328663565236567, 'learning_rate': 1.5191556303190463e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.8842822554972334}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:21:05,466][0m Trial 6 finished with value: 0.14753019892695277 and parameters: {'observation_period_num': 119, 'train_rates': 0.9224598271824356, 'learning_rate': 0.0003425122567112556, 'batch_size': 23, 'step_size': 13, 'gamma': 0.7507700171752779}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:21:51,322][0m Trial 7 finished with value: 1.2070469698815975 and parameters: {'observation_period_num': 158, 'train_rates': 0.7344634328348827, 'learning_rate': 1.2528693409986045e-06, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8284429512038975}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:22:12,196][0m Trial 8 finished with value: 1.3783163981346294 and parameters: {'observation_period_num': 222, 'train_rates': 0.6749576970926394, 'learning_rate': 1.115152035492757e-06, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9559883407798179}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:24:45,772][0m Trial 9 finished with value: 0.13936866187973868 and parameters: {'observation_period_num': 67, 'train_rates': 0.6164970192273457, 'learning_rate': 4.4732781419869205e-05, 'batch_size': 28, 'step_size': 11, 'gamma': 0.9758563871242436}. Best is trial 3 with value: 0.040148328399766085.[0m
[32m[I 2025-02-03 21:25:13,513][0m Trial 10 finished with value: 0.039942319877445696 and parameters: {'observation_period_num': 16, 'train_rates': 0.8775525389506476, 'learning_rate': 0.0006307175783347395, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9213763588541641}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:25:42,152][0m Trial 11 finished with value: 0.0642296596408805 and parameters: {'observation_period_num': 12, 'train_rates': 0.8672543578443539, 'learning_rate': 0.0009727142697545647, 'batch_size': 228, 'step_size': 15, 'gamma': 0.9229420721041913}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:26:12,473][0m Trial 12 finished with value: 0.13766003089477322 and parameters: {'observation_period_num': 55, 'train_rates': 0.8734441927317799, 'learning_rate': 0.0006192841091004731, 'batch_size': 203, 'step_size': 11, 'gamma': 0.9191611524330062}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:26:45,987][0m Trial 13 finished with value: 0.1543225198984146 and parameters: {'observation_period_num': 103, 'train_rates': 0.9848712059161089, 'learning_rate': 0.00016428553186310072, 'batch_size': 197, 'step_size': 15, 'gamma': 0.9086334264892023}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:27:58,017][0m Trial 14 finished with value: 0.05444150185212493 and parameters: {'observation_period_num': 27, 'train_rates': 0.9015653292885202, 'learning_rate': 0.0001469817141433769, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9497096977819443}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:28:26,490][0m Trial 15 finished with value: 0.5175080591392133 and parameters: {'observation_period_num': 92, 'train_rates': 0.8264734451282351, 'learning_rate': 5.36380626380259e-06, 'batch_size': 205, 'step_size': 8, 'gamma': 0.8314928373475662}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:28:50,172][0m Trial 16 finished with value: 0.045032839605350365 and parameters: {'observation_period_num': 8, 'train_rates': 0.8461553928474439, 'learning_rate': 0.0003127475032274832, 'batch_size': 252, 'step_size': 12, 'gamma': 0.8971374312804424}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:29:24,555][0m Trial 17 finished with value: 0.0904151052236557 and parameters: {'observation_period_num': 46, 'train_rates': 0.9404128091973201, 'learning_rate': 0.000933689105400316, 'batch_size': 184, 'step_size': 15, 'gamma': 0.8585180054467747}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:29:48,347][0m Trial 18 finished with value: 0.17463508178906115 and parameters: {'observation_period_num': 144, 'train_rates': 0.7859142421987869, 'learning_rate': 7.33211341594856e-05, 'batch_size': 228, 'step_size': 4, 'gamma': 0.9397895174058826}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:30:49,781][0m Trial 19 finished with value: 0.15862455528439873 and parameters: {'observation_period_num': 83, 'train_rates': 0.8952429310289413, 'learning_rate': 0.0002824295758659967, 'batch_size': 92, 'step_size': 9, 'gamma': 0.9879858617620112}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:31:17,921][0m Trial 20 finished with value: 0.14590921998023987 and parameters: {'observation_period_num': 194, 'train_rates': 0.967064508279855, 'learning_rate': 0.0005385903167270569, 'batch_size': 227, 'step_size': 13, 'gamma': 0.8619068792314424}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:31:42,862][0m Trial 21 finished with value: 0.047804618971550396 and parameters: {'observation_period_num': 7, 'train_rates': 0.8273563530762896, 'learning_rate': 0.00023047738706642138, 'batch_size': 253, 'step_size': 12, 'gamma': 0.8977581934593375}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:32:09,380][0m Trial 22 finished with value: 0.040449761879144674 and parameters: {'observation_period_num': 5, 'train_rates': 0.8583908549157163, 'learning_rate': 0.0004702033755965793, 'batch_size': 235, 'step_size': 14, 'gamma': 0.8931257942040198}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:32:37,496][0m Trial 23 finished with value: 0.06125661727498498 and parameters: {'observation_period_num': 37, 'train_rates': 0.8826438106850948, 'learning_rate': 0.0005066802016630937, 'batch_size': 219, 'step_size': 14, 'gamma': 0.9286538285002129}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:33:11,471][0m Trial 24 finished with value: 0.04197298056549496 and parameters: {'observation_period_num': 32, 'train_rates': 0.907812296089223, 'learning_rate': 0.000990128133303561, 'batch_size': 186, 'step_size': 14, 'gamma': 0.8863280262406065}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:33:52,685][0m Trial 25 finished with value: 0.0455514178292392 and parameters: {'observation_period_num': 7, 'train_rates': 0.8514161641581055, 'learning_rate': 0.0001503474095957221, 'batch_size': 140, 'step_size': 10, 'gamma': 0.8516621104858594}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:34:25,084][0m Trial 26 finished with value: 0.08896229767599291 and parameters: {'observation_period_num': 59, 'train_rates': 0.8045072648528323, 'learning_rate': 8.586099776580659e-05, 'batch_size': 175, 'step_size': 15, 'gamma': 0.7988591745667413}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:34:54,547][0m Trial 27 finished with value: 0.04609763249754906 and parameters: {'observation_period_num': 22, 'train_rates': 0.9464984893142332, 'learning_rate': 0.0005210857664996407, 'batch_size': 214, 'step_size': 12, 'gamma': 0.9068489284854016}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:35:16,575][0m Trial 28 finished with value: 0.6312611191110178 and parameters: {'observation_period_num': 116, 'train_rates': 0.7517278560973127, 'learning_rate': 4.077110074008826e-06, 'batch_size': 241, 'step_size': 13, 'gamma': 0.9649552381712223}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:35:56,095][0m Trial 29 finished with value: 0.08202411979436874 and parameters: {'observation_period_num': 47, 'train_rates': 0.9605690824984537, 'learning_rate': 5.386517026206912e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8739198565017536}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:36:26,416][0m Trial 30 finished with value: 0.08437333469933803 and parameters: {'observation_period_num': 84, 'train_rates': 0.8593405302198032, 'learning_rate': 0.00016345730393827414, 'batch_size': 192, 'step_size': 7, 'gamma': 0.8780865678324578}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:36:59,629][0m Trial 31 finished with value: 0.043827994948341734 and parameters: {'observation_period_num': 27, 'train_rates': 0.9141122385604887, 'learning_rate': 0.0009477600344653482, 'batch_size': 186, 'step_size': 14, 'gamma': 0.8918312086978432}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:37:27,320][0m Trial 32 finished with value: 0.0445143036456324 and parameters: {'observation_period_num': 42, 'train_rates': 0.9129057662239358, 'learning_rate': 0.0006507861682510829, 'batch_size': 237, 'step_size': 14, 'gamma': 0.9397235435097265}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:37:56,977][0m Trial 33 finished with value: 0.05063279149624018 and parameters: {'observation_period_num': 22, 'train_rates': 0.8892213690988775, 'learning_rate': 0.00037128304078137467, 'batch_size': 211, 'step_size': 14, 'gamma': 0.9113310230486944}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:38:29,856][0m Trial 34 finished with value: 0.06596239729471684 and parameters: {'observation_period_num': 72, 'train_rates': 0.8127918546204334, 'learning_rate': 0.0004314015420843996, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8714476695703468}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:39:09,106][0m Trial 35 finished with value: 0.050026603646299246 and parameters: {'observation_period_num': 35, 'train_rates': 0.8433627397108948, 'learning_rate': 0.0007389504674122654, 'batch_size': 151, 'step_size': 10, 'gamma': 0.8903111641563877}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:39:57,408][0m Trial 36 finished with value: 0.057476397522765656 and parameters: {'observation_period_num': 53, 'train_rates': 0.9366857823968545, 'learning_rate': 0.00027051250562536324, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8419358759314018}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:40:31,149][0m Trial 37 finished with value: 0.04095189160868233 and parameters: {'observation_period_num': 16, 'train_rates': 0.9132922544023693, 'learning_rate': 0.00022446455512610306, 'batch_size': 181, 'step_size': 13, 'gamma': 0.815250863459883}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:41:10,323][0m Trial 38 finished with value: 0.045124515891075134 and parameters: {'observation_period_num': 20, 'train_rates': 0.9532123757980415, 'learning_rate': 0.00023260787117884292, 'batch_size': 162, 'step_size': 13, 'gamma': 0.7896120541737864}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:42:02,320][0m Trial 39 finished with value: 0.3484537601470947 and parameters: {'observation_period_num': 191, 'train_rates': 0.9845711193300708, 'learning_rate': 2.8768231061392116e-05, 'batch_size': 118, 'step_size': 4, 'gamma': 0.8191250696303106}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:42:27,698][0m Trial 40 finished with value: 0.059400191247139286 and parameters: {'observation_period_num': 16, 'train_rates': 0.8748178162606374, 'learning_rate': 0.0001057213363548116, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8084441552058644}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:43:00,839][0m Trial 41 finished with value: 0.07042150073019522 and parameters: {'observation_period_num': 36, 'train_rates': 0.9234714414829015, 'learning_rate': 0.0007507921833525347, 'batch_size': 187, 'step_size': 14, 'gamma': 0.8828600657994042}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:43:40,508][0m Trial 42 finished with value: 0.05707803287527018 and parameters: {'observation_period_num': 63, 'train_rates': 0.901350825834756, 'learning_rate': 0.0004321986763218604, 'batch_size': 149, 'step_size': 13, 'gamma': 0.9303936903474984}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:44:13,665][0m Trial 43 finished with value: 0.04321042071465108 and parameters: {'observation_period_num': 5, 'train_rates': 0.7640887303606484, 'learning_rate': 0.0007325120477098692, 'batch_size': 171, 'step_size': 15, 'gamma': 0.7808179357571577}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:44:42,828][0m Trial 44 finished with value: 0.14783521400433836 and parameters: {'observation_period_num': 249, 'train_rates': 0.9208242910611377, 'learning_rate': 0.0009680417089487418, 'batch_size': 204, 'step_size': 14, 'gamma': 0.8490511359958208}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:45:12,668][0m Trial 45 finished with value: 0.05765563567789892 and parameters: {'observation_period_num': 27, 'train_rates': 0.8851020596099675, 'learning_rate': 0.0002034013534235796, 'batch_size': 215, 'step_size': 13, 'gamma': 0.7681893239893623}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:47:02,649][0m Trial 46 finished with value: 0.0734898155033239 and parameters: {'observation_period_num': 17, 'train_rates': 0.70950881786977, 'learning_rate': 0.00035947049939761243, 'batch_size': 44, 'step_size': 15, 'gamma': 0.865882010309418}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:47:33,152][0m Trial 47 finished with value: 0.1717983147681658 and parameters: {'observation_period_num': 48, 'train_rates': 0.8629546987824092, 'learning_rate': 1.3940587293953473e-05, 'batch_size': 195, 'step_size': 12, 'gamma': 0.9054438730263933}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:48:00,255][0m Trial 48 finished with value: 0.15179513663027347 and parameters: {'observation_period_num': 76, 'train_rates': 0.6039093374008598, 'learning_rate': 0.0005659842747522922, 'batch_size': 178, 'step_size': 11, 'gamma': 0.9137674598772674}. Best is trial 10 with value: 0.039942319877445696.[0m
[32m[I 2025-02-03 21:48:40,318][0m Trial 49 finished with value: 0.04638153030692866 and parameters: {'observation_period_num': 34, 'train_rates': 0.9078115954489463, 'learning_rate': 0.00038460091953856435, 'batch_size': 160, 'step_size': 14, 'gamma': 0.8834641335318155}. Best is trial 10 with value: 0.039942319877445696.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 21:48:40,328][0m A new study created in memory with name: no-name-0b4632ec-c201-4ed2-824b-fa0fad3d29ff[0m
[32m[I 2025-02-03 21:49:20,235][0m Trial 0 finished with value: 0.153767399689093 and parameters: {'observation_period_num': 34, 'train_rates': 0.9361234327587504, 'learning_rate': 9.337978322253457e-06, 'batch_size': 160, 'step_size': 6, 'gamma': 0.9720873271091544}. Best is trial 0 with value: 0.153767399689093.[0m
[32m[I 2025-02-03 21:51:24,608][0m Trial 1 finished with value: 0.09772312682392688 and parameters: {'observation_period_num': 32, 'train_rates': 0.7031306407328075, 'learning_rate': 0.0004233946116637046, 'batch_size': 39, 'step_size': 2, 'gamma': 0.8454679447683818}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:51:59,144][0m Trial 2 finished with value: 0.5522858782818443 and parameters: {'observation_period_num': 128, 'train_rates': 0.8655095458992033, 'learning_rate': 7.657869368575028e-06, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8125130031275416}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:52:28,857][0m Trial 3 finished with value: 0.8210535645484924 and parameters: {'observation_period_num': 252, 'train_rates': 0.9530723861954163, 'learning_rate': 2.348433007767949e-06, 'batch_size': 209, 'step_size': 11, 'gamma': 0.8004012557769202}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:53:33,006][0m Trial 4 finished with value: 0.8715764868380176 and parameters: {'observation_period_num': 34, 'train_rates': 0.6278115278147585, 'learning_rate': 1.0398046688616567e-06, 'batch_size': 71, 'step_size': 10, 'gamma': 0.8891954055634105}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:55:42,698][0m Trial 5 finished with value: 0.1563750704273928 and parameters: {'observation_period_num': 237, 'train_rates': 0.7465565796216772, 'learning_rate': 0.0007843354262830114, 'batch_size': 36, 'step_size': 3, 'gamma': 0.7538397413381019}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:56:45,358][0m Trial 6 finished with value: 0.126555642648919 and parameters: {'observation_period_num': 180, 'train_rates': 0.9474965938353569, 'learning_rate': 0.00044622327932581463, 'batch_size': 93, 'step_size': 15, 'gamma': 0.7973474771319666}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:57:17,894][0m Trial 7 finished with value: 0.13915836076042065 and parameters: {'observation_period_num': 125, 'train_rates': 0.895144164356009, 'learning_rate': 9.530995496499508e-05, 'batch_size': 185, 'step_size': 14, 'gamma': 0.7911150740034021}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:57:46,954][0m Trial 8 finished with value: 0.11484846287709198 and parameters: {'observation_period_num': 48, 'train_rates': 0.792036124678599, 'learning_rate': 4.668425383377653e-05, 'batch_size': 193, 'step_size': 4, 'gamma': 0.9820781795897648}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 21:58:14,468][0m Trial 9 finished with value: 0.17782774567604065 and parameters: {'observation_period_num': 166, 'train_rates': 0.9588059826498984, 'learning_rate': 7.168743485032938e-05, 'batch_size': 230, 'step_size': 8, 'gamma': 0.9567525958063887}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 22:02:33,721][0m Trial 10 finished with value: 0.12610718830727224 and parameters: {'observation_period_num': 80, 'train_rates': 0.6627463559816604, 'learning_rate': 0.00023895928165533, 'batch_size': 17, 'step_size': 1, 'gamma': 0.868015223308743}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 22:03:18,657][0m Trial 11 finished with value: 0.2350782573223114 and parameters: {'observation_period_num': 16, 'train_rates': 0.7566395155086635, 'learning_rate': 2.5650454577744285e-05, 'batch_size': 117, 'step_size': 1, 'gamma': 0.9012711998438357}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 22:03:39,054][0m Trial 12 finished with value: 0.13415775877497363 and parameters: {'observation_period_num': 77, 'train_rates': 0.6971234954742093, 'learning_rate': 0.00018508212221777202, 'batch_size': 253, 'step_size': 4, 'gamma': 0.9311536803059892}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 22:04:20,405][0m Trial 13 finished with value: 0.29020514487643767 and parameters: {'observation_period_num': 65, 'train_rates': 0.8207990427258861, 'learning_rate': 2.709465036914166e-05, 'batch_size': 133, 'step_size': 3, 'gamma': 0.8495570219201447}. Best is trial 1 with value: 0.09772312682392688.[0m
[32m[I 2025-02-03 22:05:27,618][0m Trial 14 finished with value: 0.0741093977128021 and parameters: {'observation_period_num': 56, 'train_rates': 0.8109917252231721, 'learning_rate': 6.710630116716687e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9886829552492113}. Best is trial 14 with value: 0.0741093977128021.[0m
[32m[I 2025-02-03 22:06:49,025][0m Trial 15 finished with value: 0.04853007985203972 and parameters: {'observation_period_num': 6, 'train_rates': 0.6881017692595994, 'learning_rate': 0.0009093786272155121, 'batch_size': 60, 'step_size': 7, 'gamma': 0.8320512906021822}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:07:44,831][0m Trial 16 finished with value: 0.05724778354815815 and parameters: {'observation_period_num': 5, 'train_rates': 0.6097161055041977, 'learning_rate': 0.0008357592705818364, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9290460039048034}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:08:58,845][0m Trial 17 finished with value: 0.0628559536666618 and parameters: {'observation_period_num': 6, 'train_rates': 0.6018906976357229, 'learning_rate': 0.0008005710734043099, 'batch_size': 61, 'step_size': 10, 'gamma': 0.9274914253647284}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:09:45,001][0m Trial 18 finished with value: 0.12687306442713164 and parameters: {'observation_period_num': 97, 'train_rates': 0.6511975610233895, 'learning_rate': 0.0009790194976129405, 'batch_size': 99, 'step_size': 12, 'gamma': 0.9181149955650303}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:10:28,963][0m Trial 19 finished with value: 0.12588201426962436 and parameters: {'observation_period_num': 94, 'train_rates': 0.6981475454867347, 'learning_rate': 0.00022514136743780503, 'batch_size': 116, 'step_size': 8, 'gamma': 0.8292409185687135}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:11:48,887][0m Trial 20 finished with value: 0.19725275039672852 and parameters: {'observation_period_num': 188, 'train_rates': 0.6000619075893927, 'learning_rate': 0.00037352694923089094, 'batch_size': 51, 'step_size': 6, 'gamma': 0.8732311800209779}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:13:03,306][0m Trial 21 finished with value: 0.05818827574665493 and parameters: {'observation_period_num': 6, 'train_rates': 0.605535322166067, 'learning_rate': 0.0009721325983236876, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9434395961985327}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:17:25,128][0m Trial 22 finished with value: 0.049578102515482836 and parameters: {'observation_period_num': 8, 'train_rates': 0.6474865619164739, 'learning_rate': 0.0005340165409354173, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9490589917038674}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:21:40,431][0m Trial 23 finished with value: 0.0728713379900777 and parameters: {'observation_period_num': 24, 'train_rates': 0.6654673964003773, 'learning_rate': 0.00014017600503090676, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9047911422928141}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:22:39,235][0m Trial 24 finished with value: 0.0866656977229914 and parameters: {'observation_period_num': 51, 'train_rates': 0.7252023611237661, 'learning_rate': 0.0004584837302940448, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9502228111503512}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:24:36,868][0m Trial 25 finished with value: 0.10366071632530303 and parameters: {'observation_period_num': 21, 'train_rates': 0.6456975678343531, 'learning_rate': 0.0003063066763001693, 'batch_size': 39, 'step_size': 9, 'gamma': 0.9655430756074578}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:25:22,337][0m Trial 26 finished with value: 0.14966855124780334 and parameters: {'observation_period_num': 110, 'train_rates': 0.6782289496621966, 'learning_rate': 0.000572401763235836, 'batch_size': 104, 'step_size': 6, 'gamma': 0.8783535429144933}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:27:29,263][0m Trial 27 finished with value: 0.12089683444509361 and parameters: {'observation_period_num': 44, 'train_rates': 0.6288771329074376, 'learning_rate': 0.00012788332884478457, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8458915206608991}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:28:52,121][0m Trial 28 finished with value: 0.18207135720544104 and parameters: {'observation_period_num': 154, 'train_rates': 0.7688419360654908, 'learning_rate': 0.0005588670966913402, 'batch_size': 59, 'step_size': 9, 'gamma': 0.77537371264195}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:33:35,624][0m Trial 29 finished with value: 0.06222004309342599 and parameters: {'observation_period_num': 7, 'train_rates': 0.7248959309404185, 'learning_rate': 0.00031357826263790737, 'batch_size': 17, 'step_size': 5, 'gamma': 0.91819188792191}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:34:10,752][0m Trial 30 finished with value: 0.2213610314007656 and parameters: {'observation_period_num': 34, 'train_rates': 0.6348047342874578, 'learning_rate': 1.71561138337174e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8277267699130519}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:35:14,372][0m Trial 31 finished with value: 0.04855051793379101 and parameters: {'observation_period_num': 6, 'train_rates': 0.6110166341383628, 'learning_rate': 0.0009580576664358371, 'batch_size': 71, 'step_size': 9, 'gamma': 0.943925632221088}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:36:20,891][0m Trial 32 finished with value: 0.08034247169876124 and parameters: {'observation_period_num': 26, 'train_rates': 0.6790366516080402, 'learning_rate': 0.0007072206599375857, 'batch_size': 73, 'step_size': 10, 'gamma': 0.9680902716639194}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:37:51,029][0m Trial 33 finished with value: 0.11436324825473061 and parameters: {'observation_period_num': 39, 'train_rates': 0.6230925694671198, 'learning_rate': 0.0005532462198446641, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9369366478502799}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:38:27,303][0m Trial 34 finished with value: 0.2888893638158319 and parameters: {'observation_period_num': 66, 'train_rates': 0.7214712712553122, 'learning_rate': 6.60628763182885e-06, 'batch_size': 146, 'step_size': 11, 'gamma': 0.9592444218376013}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:40:48,831][0m Trial 35 finished with value: 0.06642089450470069 and parameters: {'observation_period_num': 19, 'train_rates': 0.6190745186109601, 'learning_rate': 0.0009776966946360964, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9007493316841203}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:41:30,703][0m Trial 36 finished with value: 0.5007656496315176 and parameters: {'observation_period_num': 216, 'train_rates': 0.6790323251650273, 'learning_rate': 4.610996899081332e-06, 'batch_size': 112, 'step_size': 5, 'gamma': 0.9779476322552902}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:42:26,717][0m Trial 37 finished with value: 0.04930214193104126 and parameters: {'observation_period_num': 5, 'train_rates': 0.6395812396396197, 'learning_rate': 0.000329167145982967, 'batch_size': 83, 'step_size': 8, 'gamma': 0.9223210330394653}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:43:31,901][0m Trial 38 finished with value: 0.07720582663126438 and parameters: {'observation_period_num': 34, 'train_rates': 0.6370511725953303, 'learning_rate': 0.0003163553252759865, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8579886708885728}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:45:30,271][0m Trial 39 finished with value: 0.06566841388393564 and parameters: {'observation_period_num': 61, 'train_rates': 0.8819533205696234, 'learning_rate': 0.00016819502868379728, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8224754496555773}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:46:00,396][0m Trial 40 finished with value: 0.8612097177816473 and parameters: {'observation_period_num': 19, 'train_rates': 0.654958148932214, 'learning_rate': 1.2280437447043782e-06, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8920064456117673}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:46:54,948][0m Trial 41 finished with value: 0.05844475068941765 and parameters: {'observation_period_num': 9, 'train_rates': 0.6153959955676378, 'learning_rate': 0.0006021600836901163, 'batch_size': 86, 'step_size': 8, 'gamma': 0.9220149260772099}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:47:45,946][0m Trial 42 finished with value: 0.07835340502751828 and parameters: {'observation_period_num': 28, 'train_rates': 0.6451794306542471, 'learning_rate': 0.00041795672104566695, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9149467367720093}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:48:35,412][0m Trial 43 finished with value: 0.11303558200597763 and parameters: {'observation_period_num': 46, 'train_rates': 0.9788500526616457, 'learning_rate': 0.0007243055495239465, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9445015536378628}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:49:38,866][0m Trial 44 finished with value: 0.05503584881505932 and parameters: {'observation_period_num': 14, 'train_rates': 0.6693051842128023, 'learning_rate': 0.00042584622552820284, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9342278628253317}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:52:34,622][0m Trial 45 finished with value: 0.08358177830491748 and parameters: {'observation_period_num': 33, 'train_rates': 0.7002639866330443, 'learning_rate': 0.0002533713789322372, 'batch_size': 27, 'step_size': 6, 'gamma': 0.954932031239588}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:53:43,578][0m Trial 46 finished with value: 0.06754605826510515 and parameters: {'observation_period_num': 17, 'train_rates': 0.6706175260720761, 'learning_rate': 0.0003955295041952745, 'batch_size': 70, 'step_size': 9, 'gamma': 0.939323759270962}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:55:36,216][0m Trial 47 finished with value: 0.06580540645336655 and parameters: {'observation_period_num': 14, 'train_rates': 0.7108704111883537, 'learning_rate': 4.6790820146927054e-05, 'batch_size': 43, 'step_size': 4, 'gamma': 0.9078392704753173}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:56:25,396][0m Trial 48 finished with value: 0.156228355466062 and parameters: {'observation_period_num': 142, 'train_rates': 0.7468687436972966, 'learning_rate': 0.00010045893856847775, 'batch_size': 101, 'step_size': 10, 'gamma': 0.885204826050669}. Best is trial 15 with value: 0.04853007985203972.[0m
[32m[I 2025-02-03 22:58:05,401][0m Trial 49 finished with value: 0.07124788202345371 and parameters: {'observation_period_num': 81, 'train_rates': 0.9163487787844652, 'learning_rate': 0.00019388023760319713, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9741364502649652}. Best is trial 15 with value: 0.04853007985203972.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 22:58:05,412][0m A new study created in memory with name: no-name-0ebe277c-f08d-4972-b750-a0f63797d97f[0m
[32m[I 2025-02-03 22:58:50,693][0m Trial 0 finished with value: 0.18221785411908387 and parameters: {'observation_period_num': 59, 'train_rates': 0.692192055838195, 'learning_rate': 1.5204285906795592e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.9891871510339995}. Best is trial 0 with value: 0.18221785411908387.[0m
[32m[I 2025-02-03 23:00:30,142][0m Trial 1 finished with value: 0.0756677280049402 and parameters: {'observation_period_num': 17, 'train_rates': 0.60431904720656, 'learning_rate': 0.0009568176050806092, 'batch_size': 44, 'step_size': 15, 'gamma': 0.7965343812138178}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:05:16,551][0m Trial 2 finished with value: 0.17263278276173036 and parameters: {'observation_period_num': 211, 'train_rates': 0.951359599829473, 'learning_rate': 0.00011269907151740601, 'batch_size': 19, 'step_size': 2, 'gamma': 0.9283553414010185}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:07:08,787][0m Trial 3 finished with value: 0.07844577772276742 and parameters: {'observation_period_num': 46, 'train_rates': 0.7953583076111285, 'learning_rate': 0.00014221763019480767, 'batch_size': 46, 'step_size': 13, 'gamma': 0.7920927005044799}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:07:36,259][0m Trial 4 finished with value: 0.29770671848386 and parameters: {'observation_period_num': 170, 'train_rates': 0.7147122197500864, 'learning_rate': 4.1800761759175425e-05, 'batch_size': 181, 'step_size': 12, 'gamma': 0.9345905226987391}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:08:10,809][0m Trial 5 finished with value: 0.11576619904395963 and parameters: {'observation_period_num': 35, 'train_rates': 0.8757378161857599, 'learning_rate': 2.1758825907846295e-05, 'batch_size': 174, 'step_size': 2, 'gamma': 0.9802528702550721}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:08:48,195][0m Trial 6 finished with value: 0.1232975646853447 and parameters: {'observation_period_num': 59, 'train_rates': 0.9493202336724524, 'learning_rate': 5.557894883687833e-05, 'batch_size': 163, 'step_size': 14, 'gamma': 0.8573723491905136}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:09:24,950][0m Trial 7 finished with value: 0.2901987132505266 and parameters: {'observation_period_num': 194, 'train_rates': 0.6839576180069737, 'learning_rate': 0.0007359340814568038, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8104219697640103}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:10:09,689][0m Trial 8 finished with value: 0.7828450188040733 and parameters: {'observation_period_num': 191, 'train_rates': 0.7393787229755241, 'learning_rate': 2.4394018563481046e-06, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8291521892866399}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:11:41,816][0m Trial 9 finished with value: 0.3532906634765759 and parameters: {'observation_period_num': 50, 'train_rates': 0.9019928520460596, 'learning_rate': 3.9605433062946885e-06, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8587654900349201}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:12:01,000][0m Trial 10 finished with value: 0.18874521880962286 and parameters: {'observation_period_num': 115, 'train_rates': 0.6126557886581379, 'learning_rate': 0.0008818661010402965, 'batch_size': 253, 'step_size': 7, 'gamma': 0.7640976081069488}. Best is trial 1 with value: 0.0756677280049402.[0m
[32m[I 2025-02-03 23:14:56,507][0m Trial 11 finished with value: 0.04653282783702933 and parameters: {'observation_period_num': 7, 'train_rates': 0.8068291015411287, 'learning_rate': 0.00021667193866227674, 'batch_size': 30, 'step_size': 15, 'gamma': 0.7539014587669215}. Best is trial 11 with value: 0.04653282783702933.[0m
[32m[I 2025-02-03 23:16:15,371][0m Trial 12 finished with value: 0.042364330152305986 and parameters: {'observation_period_num': 6, 'train_rates': 0.814919466916558, 'learning_rate': 0.0003225962917820377, 'batch_size': 69, 'step_size': 15, 'gamma': 0.7519714967131529}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:17:21,503][0m Trial 13 finished with value: 0.22615738772233648 and parameters: {'observation_period_num': 252, 'train_rates': 0.8174473539660793, 'learning_rate': 0.00025826772180230815, 'batch_size': 76, 'step_size': 9, 'gamma': 0.7594562979007629}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:21:19,099][0m Trial 14 finished with value: 0.12384169301608713 and parameters: {'observation_period_num': 105, 'train_rates': 0.8558937514435391, 'learning_rate': 0.00027381984218762765, 'batch_size': 22, 'step_size': 15, 'gamma': 0.7541537444207504}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:22:22,915][0m Trial 15 finished with value: 0.04912849780789182 and parameters: {'observation_period_num': 8, 'train_rates': 0.778411598678923, 'learning_rate': 0.00031116146479538264, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8309121552266295}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:23:23,740][0m Trial 16 finished with value: 0.10120438283924893 and parameters: {'observation_period_num': 89, 'train_rates': 0.8374591460903822, 'learning_rate': 0.00010096993854806637, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9008183794616812}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:25:08,571][0m Trial 17 finished with value: 0.2542713171059496 and parameters: {'observation_period_num': 143, 'train_rates': 0.746648229692956, 'learning_rate': 0.0003893295832984975, 'batch_size': 46, 'step_size': 15, 'gamma': 0.7769702512500184}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:25:36,026][0m Trial 18 finished with value: 0.5481049632943625 and parameters: {'observation_period_num': 82, 'train_rates': 0.8992904410645597, 'learning_rate': 1.2873848643077058e-05, 'batch_size': 222, 'step_size': 4, 'gamma': 0.8214668634400802}. Best is trial 12 with value: 0.042364330152305986.[0m
[32m[I 2025-02-03 23:31:47,190][0m Trial 19 finished with value: 0.02802162869449924 and parameters: {'observation_period_num': 5, 'train_rates': 0.9887437529260938, 'learning_rate': 7.03924902616613e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7842891224561274}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:33:17,338][0m Trial 20 finished with value: 0.49354711174964905 and parameters: {'observation_period_num': 144, 'train_rates': 0.9895196413788693, 'learning_rate': 6.724571970526558e-06, 'batch_size': 65, 'step_size': 8, 'gamma': 0.7900808066390532}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:36:51,440][0m Trial 21 finished with value: 0.06050865812595263 and parameters: {'observation_period_num': 5, 'train_rates': 0.6546947682285882, 'learning_rate': 6.804568130795133e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.7528480657589305}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:39:03,387][0m Trial 22 finished with value: 0.07013180647701436 and parameters: {'observation_period_num': 31, 'train_rates': 0.7766061159869054, 'learning_rate': 0.00019136528615354968, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7789324920160027}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:40:42,159][0m Trial 23 finished with value: 0.047782020158555406 and parameters: {'observation_period_num': 23, 'train_rates': 0.925464704559709, 'learning_rate': 0.0004887939923059266, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7750332423938842}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:46:01,372][0m Trial 24 finished with value: 0.09608094092465637 and parameters: {'observation_period_num': 76, 'train_rates': 0.8187584037761202, 'learning_rate': 9.775318005307478e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8030995895021371}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:47:03,218][0m Trial 25 finished with value: 0.08087441119020933 and parameters: {'observation_period_num': 6, 'train_rates': 0.8761532245463675, 'learning_rate': 3.05713637070661e-05, 'batch_size': 94, 'step_size': 6, 'gamma': 0.8411180363241952}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:49:16,426][0m Trial 26 finished with value: 0.110003120965024 and parameters: {'observation_period_num': 35, 'train_rates': 0.7471162662607339, 'learning_rate': 0.0005241019461457893, 'batch_size': 37, 'step_size': 14, 'gamma': 0.7525408577466486}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:50:00,984][0m Trial 27 finished with value: 0.1066325306892395 and parameters: {'observation_period_num': 68, 'train_rates': 0.9570120286812303, 'learning_rate': 0.00017422494055560227, 'batch_size': 142, 'step_size': 4, 'gamma': 0.88819234593019}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:51:30,582][0m Trial 28 finished with value: 0.058708682987425063 and parameters: {'observation_period_num': 26, 'train_rates': 0.8462927282936605, 'learning_rate': 6.658715254711289e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8124921319101795}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:52:16,527][0m Trial 29 finished with value: 0.29114349967267317 and parameters: {'observation_period_num': 49, 'train_rates': 0.684474136071394, 'learning_rate': 1.1562273310875585e-05, 'batch_size': 110, 'step_size': 12, 'gamma': 0.7734133552901956}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:54:46,508][0m Trial 30 finished with value: 0.9307815450610537 and parameters: {'observation_period_num': 104, 'train_rates': 0.8028956580678148, 'learning_rate': 1.0739369790631296e-06, 'batch_size': 34, 'step_size': 9, 'gamma': 0.7845472510324913}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:56:28,830][0m Trial 31 finished with value: 0.04764483035740694 and parameters: {'observation_period_num': 22, 'train_rates': 0.92813213113722, 'learning_rate': 0.0004815067791263076, 'batch_size': 57, 'step_size': 14, 'gamma': 0.7729187731191525}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:58:28,501][0m Trial 32 finished with value: 0.04913612827658653 and parameters: {'observation_period_num': 16, 'train_rates': 0.9869472288736307, 'learning_rate': 0.0005499331937853698, 'batch_size': 51, 'step_size': 14, 'gamma': 0.7673875671908863}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-03 23:59:51,587][0m Trial 33 finished with value: 0.03902523516088116 and parameters: {'observation_period_num': 19, 'train_rates': 0.9668930801749868, 'learning_rate': 0.0002092057654414279, 'batch_size': 74, 'step_size': 15, 'gamma': 0.803148615613295}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:02:42,611][0m Trial 34 finished with value: 0.052540881413162925 and parameters: {'observation_period_num': 38, 'train_rates': 0.9652355554156472, 'learning_rate': 0.0001867710806056443, 'batch_size': 35, 'step_size': 15, 'gamma': 0.7972452820673152}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:03:58,502][0m Trial 35 finished with value: 0.06357525497185232 and parameters: {'observation_period_num': 57, 'train_rates': 0.9264128378465283, 'learning_rate': 0.00013245198597147917, 'batch_size': 77, 'step_size': 12, 'gamma': 0.7908355607069172}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:04:52,876][0m Trial 36 finished with value: 0.07166406057060581 and parameters: {'observation_period_num': 17, 'train_rates': 0.8758408208337544, 'learning_rate': 3.2775031201089046e-05, 'batch_size': 108, 'step_size': 15, 'gamma': 0.807638580700396}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:08:36,018][0m Trial 37 finished with value: 0.04804703524630321 and parameters: {'observation_period_num': 5, 'train_rates': 0.9743190140945351, 'learning_rate': 9.005633481710928e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.9686738176905997}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:09:55,709][0m Trial 38 finished with value: 0.08468095681001973 and parameters: {'observation_period_num': 42, 'train_rates': 0.9410036963456329, 'learning_rate': 4.290910682008823e-05, 'batch_size': 74, 'step_size': 14, 'gamma': 0.750272015167263}. Best is trial 19 with value: 0.02802162869449924.[0m
Early stopping at epoch 57
[32m[I 2025-02-04 00:10:20,372][0m Trial 39 finished with value: 0.2769679714906113 and parameters: {'observation_period_num': 64, 'train_rates': 0.7183224859399999, 'learning_rate': 0.00024120987204011372, 'batch_size': 127, 'step_size': 1, 'gamma': 0.7886744767178931}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:11:18,103][0m Trial 40 finished with value: 0.08390626730397344 and parameters: {'observation_period_num': 26, 'train_rates': 0.9073037163619345, 'learning_rate': 2.2008073779974084e-05, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8441938971313544}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:13:10,641][0m Trial 41 finished with value: 0.0433535864124599 and parameters: {'observation_period_num': 19, 'train_rates': 0.9297756550854095, 'learning_rate': 0.0003872510887952617, 'batch_size': 52, 'step_size': 14, 'gamma': 0.7673156233792583}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:15:17,012][0m Trial 42 finished with value: 0.04436916910651091 and parameters: {'observation_period_num': 15, 'train_rates': 0.9484967311862367, 'learning_rate': 0.00035069325058521904, 'batch_size': 47, 'step_size': 15, 'gamma': 0.7630999760795322}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:17:24,202][0m Trial 43 finished with value: 0.10800518778062636 and parameters: {'observation_period_num': 45, 'train_rates': 0.9469516566300873, 'learning_rate': 0.0007118754052159379, 'batch_size': 46, 'step_size': 14, 'gamma': 0.8184816181523854}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:18:53,399][0m Trial 44 finished with value: 0.04056477166810318 and parameters: {'observation_period_num': 18, 'train_rates': 0.9740861367545018, 'learning_rate': 0.000382286520557102, 'batch_size': 69, 'step_size': 13, 'gamma': 0.7653453907546013}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:20:20,129][0m Trial 45 finished with value: 0.05455571736651237 and parameters: {'observation_period_num': 33, 'train_rates': 0.9660666415760963, 'learning_rate': 0.00014590989503538712, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7993530296197435}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:20:51,552][0m Trial 46 finished with value: 0.07026880234479904 and parameters: {'observation_period_num': 53, 'train_rates': 0.977375353388416, 'learning_rate': 0.0009779875767739253, 'batch_size': 203, 'step_size': 13, 'gamma': 0.7660852180618349}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:22:00,838][0m Trial 47 finished with value: 0.03623644510904948 and parameters: {'observation_period_num': 16, 'train_rates': 0.9141860680348085, 'learning_rate': 0.00038688186471598924, 'batch_size': 84, 'step_size': 12, 'gamma': 0.7831817535836062}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:22:41,285][0m Trial 48 finished with value: 0.12298216195557062 and parameters: {'observation_period_num': 177, 'train_rates': 0.9085704580653485, 'learning_rate': 0.0007142361458499755, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8796851216808462}. Best is trial 19 with value: 0.02802162869449924.[0m
[32m[I 2025-02-04 00:23:29,766][0m Trial 49 finished with value: 0.05520865478013691 and parameters: {'observation_period_num': 40, 'train_rates': 0.8825922257164084, 'learning_rate': 0.0003056591671839141, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9132188450629273}. Best is trial 19 with value: 0.02802162869449924.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_iTransformer_VMD.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 8, 'train_rates': 0.985793298323535, 'learning_rate': 0.0008605679610728868, 'batch_size': 95, 'step_size': 2, 'gamma': 0.9617782556153507}
Epoch 1/300, trend Loss: 0.3498 | 0.1381
Epoch 2/300, trend Loss: 0.1173 | 0.0971
Epoch 3/300, trend Loss: 0.1004 | 0.1124
Epoch 4/300, trend Loss: 0.0895 | 0.0785
Epoch 5/300, trend Loss: 0.0945 | 0.1592
Epoch 6/300, trend Loss: 0.0893 | 0.0780
Epoch 7/300, trend Loss: 0.0847 | 0.0743
Epoch 8/300, trend Loss: 0.0771 | 0.0651
Epoch 9/300, trend Loss: 0.0740 | 0.0640
Epoch 10/300, trend Loss: 0.0692 | 0.0674
Epoch 11/300, trend Loss: 0.0659 | 0.0690
Epoch 12/300, trend Loss: 0.0661 | 0.0739
Epoch 13/300, trend Loss: 0.0670 | 0.0888
Epoch 14/300, trend Loss: 0.0637 | 0.1088
Epoch 15/300, trend Loss: 0.0670 | 0.1180
Epoch 16/300, trend Loss: 0.0952 | 0.1360
Epoch 17/300, trend Loss: 0.0796 | 0.0908
Epoch 18/300, trend Loss: 0.0736 | 0.0685
Epoch 19/300, trend Loss: 0.0592 | 0.0661
Epoch 20/300, trend Loss: 0.0752 | 0.0707
Epoch 21/300, trend Loss: 0.0977 | 0.1104
Epoch 22/300, trend Loss: 0.0995 | 0.2130
Epoch 23/300, trend Loss: 0.0762 | 0.1944
Epoch 24/300, trend Loss: 0.0742 | 0.0855
Epoch 25/300, trend Loss: 0.0596 | 0.0560
Epoch 26/300, trend Loss: 0.0477 | 0.0456
Epoch 27/300, trend Loss: 0.0449 | 0.0440
Epoch 28/300, trend Loss: 0.0450 | 0.0435
Epoch 29/300, trend Loss: 0.0433 | 0.0426
Epoch 30/300, trend Loss: 0.0409 | 0.0411
Epoch 31/300, trend Loss: 0.0400 | 0.0406
Epoch 32/300, trend Loss: 0.0390 | 0.0402
Epoch 33/300, trend Loss: 0.0384 | 0.0395
Epoch 34/300, trend Loss: 0.0383 | 0.0392
Epoch 35/300, trend Loss: 0.0385 | 0.0391
Epoch 36/300, trend Loss: 0.0383 | 0.0395
Epoch 37/300, trend Loss: 0.0374 | 0.0394
Epoch 38/300, trend Loss: 0.0362 | 0.0400
Epoch 39/300, trend Loss: 0.0347 | 0.0403
Epoch 40/300, trend Loss: 0.0338 | 0.0375
Epoch 41/300, trend Loss: 0.0335 | 0.0373
Epoch 42/300, trend Loss: 0.0336 | 0.0374
Epoch 43/300, trend Loss: 0.0334 | 0.0378
Epoch 44/300, trend Loss: 0.0329 | 0.0376
Epoch 45/300, trend Loss: 0.0328 | 0.0369
Epoch 46/300, trend Loss: 0.0331 | 0.0370
Epoch 47/300, trend Loss: 0.0339 | 0.0371
Epoch 48/300, trend Loss: 0.0338 | 0.0368
Epoch 49/300, trend Loss: 0.0323 | 0.0362
Epoch 50/300, trend Loss: 0.0319 | 0.0352
Epoch 51/300, trend Loss: 0.0316 | 0.0346
Epoch 52/300, trend Loss: 0.0315 | 0.0341
Epoch 53/300, trend Loss: 0.0316 | 0.0338
Epoch 54/300, trend Loss: 0.0315 | 0.0337
Epoch 55/300, trend Loss: 0.0312 | 0.0335
Epoch 56/300, trend Loss: 0.0309 | 0.0331
Epoch 57/300, trend Loss: 0.0307 | 0.0328
Epoch 58/300, trend Loss: 0.0305 | 0.0325
Epoch 59/300, trend Loss: 0.0304 | 0.0321
Epoch 60/300, trend Loss: 0.0303 | 0.0317
Epoch 61/300, trend Loss: 0.0303 | 0.0313
Epoch 62/300, trend Loss: 0.0303 | 0.0312
Epoch 63/300, trend Loss: 0.0304 | 0.0312
Epoch 64/300, trend Loss: 0.0303 | 0.0316
Epoch 65/300, trend Loss: 0.0301 | 0.0317
Epoch 66/300, trend Loss: 0.0297 | 0.0315
Epoch 67/300, trend Loss: 0.0295 | 0.0314
Epoch 68/300, trend Loss: 0.0293 | 0.0313
Epoch 69/300, trend Loss: 0.0292 | 0.0314
Epoch 70/300, trend Loss: 0.0291 | 0.0315
Epoch 71/300, trend Loss: 0.0290 | 0.0315
Epoch 72/300, trend Loss: 0.0289 | 0.0316
Epoch 73/300, trend Loss: 0.0288 | 0.0316
Epoch 74/300, trend Loss: 0.0287 | 0.0316
Epoch 75/300, trend Loss: 0.0286 | 0.0316
Epoch 76/300, trend Loss: 0.0285 | 0.0315
Epoch 77/300, trend Loss: 0.0284 | 0.0314
Epoch 78/300, trend Loss: 0.0283 | 0.0313
Epoch 79/300, trend Loss: 0.0282 | 0.0313
Epoch 80/300, trend Loss: 0.0281 | 0.0312
Epoch 81/300, trend Loss: 0.0281 | 0.0311
Epoch 82/300, trend Loss: 0.0280 | 0.0311
Epoch 83/300, trend Loss: 0.0280 | 0.0311
Epoch 84/300, trend Loss: 0.0279 | 0.0311
Epoch 85/300, trend Loss: 0.0279 | 0.0311
Epoch 86/300, trend Loss: 0.0279 | 0.0310
Epoch 87/300, trend Loss: 0.0279 | 0.0308
Epoch 88/300, trend Loss: 0.0278 | 0.0306
Epoch 89/300, trend Loss: 0.0278 | 0.0305
Epoch 90/300, trend Loss: 0.0277 | 0.0303
Epoch 91/300, trend Loss: 0.0277 | 0.0303
Epoch 92/300, trend Loss: 0.0277 | 0.0303
Epoch 93/300, trend Loss: 0.0277 | 0.0303
Epoch 94/300, trend Loss: 0.0277 | 0.0304
Epoch 95/300, trend Loss: 0.0277 | 0.0306
Epoch 96/300, trend Loss: 0.0277 | 0.0308
Epoch 97/300, trend Loss: 0.0277 | 0.0311
Epoch 98/300, trend Loss: 0.0278 | 0.0314
Epoch 99/300, trend Loss: 0.0279 | 0.0318
Epoch 100/300, trend Loss: 0.0280 | 0.0321
Epoch 101/300, trend Loss: 0.0281 | 0.0322
Epoch 102/300, trend Loss: 0.0283 | 0.0321
Epoch 103/300, trend Loss: 0.0283 | 0.0315
Epoch 104/300, trend Loss: 0.0280 | 0.0308
Epoch 105/300, trend Loss: 0.0277 | 0.0303
Epoch 106/300, trend Loss: 0.0274 | 0.0300
Epoch 107/300, trend Loss: 0.0273 | 0.0298
Epoch 108/300, trend Loss: 0.0272 | 0.0296
Epoch 109/300, trend Loss: 0.0271 | 0.0295
Epoch 110/300, trend Loss: 0.0270 | 0.0294
Epoch 111/300, trend Loss: 0.0270 | 0.0293
Epoch 112/300, trend Loss: 0.0269 | 0.0292
Epoch 113/300, trend Loss: 0.0269 | 0.0291
Epoch 114/300, trend Loss: 0.0268 | 0.0290
Epoch 115/300, trend Loss: 0.0268 | 0.0289
Epoch 116/300, trend Loss: 0.0268 | 0.0288
Epoch 117/300, trend Loss: 0.0268 | 0.0288
Epoch 118/300, trend Loss: 0.0267 | 0.0287
Epoch 119/300, trend Loss: 0.0267 | 0.0287
Epoch 120/300, trend Loss: 0.0267 | 0.0286
Epoch 121/300, trend Loss: 0.0267 | 0.0286
Epoch 122/300, trend Loss: 0.0267 | 0.0285
Epoch 123/300, trend Loss: 0.0266 | 0.0285
Epoch 124/300, trend Loss: 0.0266 | 0.0285
Epoch 125/300, trend Loss: 0.0266 | 0.0284
Epoch 126/300, trend Loss: 0.0266 | 0.0284
Epoch 127/300, trend Loss: 0.0266 | 0.0284
Epoch 128/300, trend Loss: 0.0265 | 0.0283
Epoch 129/300, trend Loss: 0.0265 | 0.0283
Epoch 130/300, trend Loss: 0.0265 | 0.0283
Epoch 131/300, trend Loss: 0.0265 | 0.0283
Epoch 132/300, trend Loss: 0.0265 | 0.0283
Epoch 133/300, trend Loss: 0.0265 | 0.0282
Epoch 134/300, trend Loss: 0.0264 | 0.0282
Epoch 135/300, trend Loss: 0.0264 | 0.0282
Epoch 136/300, trend Loss: 0.0264 | 0.0282
Epoch 137/300, trend Loss: 0.0264 | 0.0282
Epoch 138/300, trend Loss: 0.0264 | 0.0281
Epoch 139/300, trend Loss: 0.0264 | 0.0281
Epoch 140/300, trend Loss: 0.0264 | 0.0281
Epoch 141/300, trend Loss: 0.0264 | 0.0281
Epoch 142/300, trend Loss: 0.0264 | 0.0281
Epoch 143/300, trend Loss: 0.0263 | 0.0281
Epoch 144/300, trend Loss: 0.0263 | 0.0281
Epoch 145/300, trend Loss: 0.0263 | 0.0280
Epoch 146/300, trend Loss: 0.0263 | 0.0280
Epoch 147/300, trend Loss: 0.0263 | 0.0280
Epoch 148/300, trend Loss: 0.0263 | 0.0280
Epoch 149/300, trend Loss: 0.0263 | 0.0280
Epoch 150/300, trend Loss: 0.0263 | 0.0280
Epoch 151/300, trend Loss: 0.0263 | 0.0280
Epoch 152/300, trend Loss: 0.0263 | 0.0280
Epoch 153/300, trend Loss: 0.0263 | 0.0279
Epoch 154/300, trend Loss: 0.0263 | 0.0279
Epoch 155/300, trend Loss: 0.0262 | 0.0279
Epoch 156/300, trend Loss: 0.0262 | 0.0279
Epoch 157/300, trend Loss: 0.0262 | 0.0279
Epoch 158/300, trend Loss: 0.0262 | 0.0279
Epoch 159/300, trend Loss: 0.0262 | 0.0279
Epoch 160/300, trend Loss: 0.0262 | 0.0279
Epoch 161/300, trend Loss: 0.0262 | 0.0279
Epoch 162/300, trend Loss: 0.0262 | 0.0279
Epoch 163/300, trend Loss: 0.0262 | 0.0279
Epoch 164/300, trend Loss: 0.0262 | 0.0279
Epoch 165/300, trend Loss: 0.0262 | 0.0278
Epoch 166/300, trend Loss: 0.0262 | 0.0278
Epoch 167/300, trend Loss: 0.0262 | 0.0278
Epoch 168/300, trend Loss: 0.0262 | 0.0278
Epoch 169/300, trend Loss: 0.0262 | 0.0278
Epoch 170/300, trend Loss: 0.0262 | 0.0278
Epoch 171/300, trend Loss: 0.0262 | 0.0278
Epoch 172/300, trend Loss: 0.0262 | 0.0278
Epoch 173/300, trend Loss: 0.0262 | 0.0278
Epoch 174/300, trend Loss: 0.0262 | 0.0278
Epoch 175/300, trend Loss: 0.0262 | 0.0278
Epoch 176/300, trend Loss: 0.0262 | 0.0278
Epoch 177/300, trend Loss: 0.0261 | 0.0278
Epoch 178/300, trend Loss: 0.0261 | 0.0278
Epoch 179/300, trend Loss: 0.0261 | 0.0278
Epoch 180/300, trend Loss: 0.0261 | 0.0278
Epoch 181/300, trend Loss: 0.0261 | 0.0278
Epoch 182/300, trend Loss: 0.0261 | 0.0278
Epoch 183/300, trend Loss: 0.0261 | 0.0278
Epoch 184/300, trend Loss: 0.0261 | 0.0278
Epoch 185/300, trend Loss: 0.0261 | 0.0278
Epoch 186/300, trend Loss: 0.0261 | 0.0277
Epoch 187/300, trend Loss: 0.0261 | 0.0277
Epoch 188/300, trend Loss: 0.0261 | 0.0277
Epoch 189/300, trend Loss: 0.0261 | 0.0277
Epoch 190/300, trend Loss: 0.0261 | 0.0277
Epoch 191/300, trend Loss: 0.0261 | 0.0277
Epoch 192/300, trend Loss: 0.0261 | 0.0277
Epoch 193/300, trend Loss: 0.0261 | 0.0277
Epoch 194/300, trend Loss: 0.0261 | 0.0277
Epoch 195/300, trend Loss: 0.0261 | 0.0277
Epoch 196/300, trend Loss: 0.0261 | 0.0277
Epoch 197/300, trend Loss: 0.0261 | 0.0277
Epoch 198/300, trend Loss: 0.0261 | 0.0277
Epoch 199/300, trend Loss: 0.0261 | 0.0277
Epoch 200/300, trend Loss: 0.0261 | 0.0277
Epoch 201/300, trend Loss: 0.0261 | 0.0277
Epoch 202/300, trend Loss: 0.0261 | 0.0277
Epoch 203/300, trend Loss: 0.0261 | 0.0277
Epoch 204/300, trend Loss: 0.0261 | 0.0277
Epoch 205/300, trend Loss: 0.0261 | 0.0277
Epoch 206/300, trend Loss: 0.0261 | 0.0277
Epoch 207/300, trend Loss: 0.0261 | 0.0277
Epoch 208/300, trend Loss: 0.0261 | 0.0277
Epoch 209/300, trend Loss: 0.0261 | 0.0277
Epoch 210/300, trend Loss: 0.0261 | 0.0277
Epoch 211/300, trend Loss: 0.0261 | 0.0277
Epoch 212/300, trend Loss: 0.0261 | 0.0277
Epoch 213/300, trend Loss: 0.0261 | 0.0277
Epoch 214/300, trend Loss: 0.0261 | 0.0277
Epoch 215/300, trend Loss: 0.0261 | 0.0277
Epoch 216/300, trend Loss: 0.0261 | 0.0277
Epoch 217/300, trend Loss: 0.0261 | 0.0277
Epoch 218/300, trend Loss: 0.0261 | 0.0277
Epoch 219/300, trend Loss: 0.0261 | 0.0277
Epoch 220/300, trend Loss: 0.0261 | 0.0277
Epoch 221/300, trend Loss: 0.0261 | 0.0277
Epoch 222/300, trend Loss: 0.0261 | 0.0277
Epoch 223/300, trend Loss: 0.0261 | 0.0277
Epoch 224/300, trend Loss: 0.0261 | 0.0277
Epoch 225/300, trend Loss: 0.0261 | 0.0277
Epoch 226/300, trend Loss: 0.0261 | 0.0277
Epoch 227/300, trend Loss: 0.0261 | 0.0277
Epoch 228/300, trend Loss: 0.0261 | 0.0277
Epoch 229/300, trend Loss: 0.0261 | 0.0277
Epoch 230/300, trend Loss: 0.0261 | 0.0277
Epoch 231/300, trend Loss: 0.0261 | 0.0277
Epoch 232/300, trend Loss: 0.0261 | 0.0277
Epoch 233/300, trend Loss: 0.0261 | 0.0277
Epoch 234/300, trend Loss: 0.0261 | 0.0277
Epoch 235/300, trend Loss: 0.0261 | 0.0277
Epoch 236/300, trend Loss: 0.0261 | 0.0277
Epoch 237/300, trend Loss: 0.0261 | 0.0277
Epoch 238/300, trend Loss: 0.0261 | 0.0277
Epoch 239/300, trend Loss: 0.0261 | 0.0277
Epoch 240/300, trend Loss: 0.0261 | 0.0277
Epoch 241/300, trend Loss: 0.0261 | 0.0277
Epoch 242/300, trend Loss: 0.0261 | 0.0277
Epoch 243/300, trend Loss: 0.0261 | 0.0277
Epoch 244/300, trend Loss: 0.0261 | 0.0277
Epoch 245/300, trend Loss: 0.0261 | 0.0277
Epoch 246/300, trend Loss: 0.0261 | 0.0277
Epoch 247/300, trend Loss: 0.0261 | 0.0277
Epoch 248/300, trend Loss: 0.0261 | 0.0277
Epoch 249/300, trend Loss: 0.0261 | 0.0277
Epoch 250/300, trend Loss: 0.0261 | 0.0277
Epoch 251/300, trend Loss: 0.0261 | 0.0277
Epoch 252/300, trend Loss: 0.0261 | 0.0277
Epoch 253/300, trend Loss: 0.0261 | 0.0277
Epoch 254/300, trend Loss: 0.0261 | 0.0277
Epoch 255/300, trend Loss: 0.0261 | 0.0277
Epoch 256/300, trend Loss: 0.0261 | 0.0277
Epoch 257/300, trend Loss: 0.0261 | 0.0277
Epoch 258/300, trend Loss: 0.0261 | 0.0277
Epoch 259/300, trend Loss: 0.0261 | 0.0277
Epoch 260/300, trend Loss: 0.0261 | 0.0277
Epoch 261/300, trend Loss: 0.0261 | 0.0277
Epoch 262/300, trend Loss: 0.0261 | 0.0277
Epoch 263/300, trend Loss: 0.0261 | 0.0277
Epoch 264/300, trend Loss: 0.0261 | 0.0277
Epoch 265/300, trend Loss: 0.0261 | 0.0277
Epoch 266/300, trend Loss: 0.0261 | 0.0277
Epoch 267/300, trend Loss: 0.0261 | 0.0277
Epoch 268/300, trend Loss: 0.0261 | 0.0277
Epoch 269/300, trend Loss: 0.0261 | 0.0277
Epoch 270/300, trend Loss: 0.0261 | 0.0277
Epoch 271/300, trend Loss: 0.0261 | 0.0277
Epoch 272/300, trend Loss: 0.0261 | 0.0277
Epoch 273/300, trend Loss: 0.0261 | 0.0277
Epoch 274/300, trend Loss: 0.0261 | 0.0277
Epoch 275/300, trend Loss: 0.0261 | 0.0277
Epoch 276/300, trend Loss: 0.0261 | 0.0277
Epoch 277/300, trend Loss: 0.0261 | 0.0277
Epoch 278/300, trend Loss: 0.0261 | 0.0277
Epoch 279/300, trend Loss: 0.0261 | 0.0277
Epoch 280/300, trend Loss: 0.0261 | 0.0277
Epoch 281/300, trend Loss: 0.0261 | 0.0277
Epoch 282/300, trend Loss: 0.0261 | 0.0277
Epoch 283/300, trend Loss: 0.0261 | 0.0277
Epoch 284/300, trend Loss: 0.0261 | 0.0277
Epoch 285/300, trend Loss: 0.0261 | 0.0277
Epoch 286/300, trend Loss: 0.0261 | 0.0277
Epoch 287/300, trend Loss: 0.0261 | 0.0277
Epoch 288/300, trend Loss: 0.0261 | 0.0277
Epoch 289/300, trend Loss: 0.0261 | 0.0277
Epoch 290/300, trend Loss: 0.0261 | 0.0277
Epoch 291/300, trend Loss: 0.0261 | 0.0277
Epoch 292/300, trend Loss: 0.0261 | 0.0277
Epoch 293/300, trend Loss: 0.0261 | 0.0277
Epoch 294/300, trend Loss: 0.0261 | 0.0277
Epoch 295/300, trend Loss: 0.0261 | 0.0277
Epoch 296/300, trend Loss: 0.0261 | 0.0277
Epoch 297/300, trend Loss: 0.0261 | 0.0277
Epoch 298/300, trend Loss: 0.0261 | 0.0277
Epoch 299/300, trend Loss: 0.0261 | 0.0277
Epoch 300/300, trend Loss: 0.0261 | 0.0277
Training seasonal_0 component with params: {'observation_period_num': 7, 'train_rates': 0.9658364970062293, 'learning_rate': 0.0005670250489359773, 'batch_size': 64, 'step_size': 5, 'gamma': 0.9005004586604695}
Epoch 1/300, seasonal_0 Loss: 0.2643 | 0.1164
Epoch 2/300, seasonal_0 Loss: 0.1021 | 0.0808
Epoch 3/300, seasonal_0 Loss: 0.0838 | 0.0668
Epoch 4/300, seasonal_0 Loss: 0.0757 | 0.0779
Epoch 5/300, seasonal_0 Loss: 0.0778 | 0.0723
Epoch 6/300, seasonal_0 Loss: 0.0668 | 0.0601
Epoch 7/300, seasonal_0 Loss: 0.0603 | 0.0574
Epoch 8/300, seasonal_0 Loss: 0.0562 | 0.0554
Epoch 9/300, seasonal_0 Loss: 0.0524 | 0.0521
Epoch 10/300, seasonal_0 Loss: 0.0501 | 0.0507
Epoch 11/300, seasonal_0 Loss: 0.0474 | 0.0496
Epoch 12/300, seasonal_0 Loss: 0.0457 | 0.0491
Epoch 13/300, seasonal_0 Loss: 0.0448 | 0.0497
Epoch 14/300, seasonal_0 Loss: 0.0458 | 0.0535
Epoch 15/300, seasonal_0 Loss: 0.0492 | 0.0519
Epoch 16/300, seasonal_0 Loss: 0.0450 | 0.0475
Epoch 17/300, seasonal_0 Loss: 0.0421 | 0.0466
Epoch 18/300, seasonal_0 Loss: 0.0395 | 0.0433
Epoch 19/300, seasonal_0 Loss: 0.0399 | 0.0459
Epoch 20/300, seasonal_0 Loss: 0.0390 | 0.0430
Epoch 21/300, seasonal_0 Loss: 0.0386 | 0.0449
Epoch 22/300, seasonal_0 Loss: 0.0375 | 0.0424
Epoch 23/300, seasonal_0 Loss: 0.0365 | 0.0428
Epoch 24/300, seasonal_0 Loss: 0.0358 | 0.0418
Epoch 25/300, seasonal_0 Loss: 0.0353 | 0.0418
Epoch 26/300, seasonal_0 Loss: 0.0345 | 0.0408
Epoch 27/300, seasonal_0 Loss: 0.0342 | 0.0413
Epoch 28/300, seasonal_0 Loss: 0.0338 | 0.0399
Epoch 29/300, seasonal_0 Loss: 0.0332 | 0.0402
Epoch 30/300, seasonal_0 Loss: 0.0331 | 0.0398
Epoch 31/300, seasonal_0 Loss: 0.0340 | 0.0407
Epoch 32/300, seasonal_0 Loss: 0.0341 | 0.0401
Epoch 33/300, seasonal_0 Loss: 0.0337 | 0.0394
Epoch 34/300, seasonal_0 Loss: 0.0335 | 0.0382
Epoch 35/300, seasonal_0 Loss: 0.0334 | 0.0381
Epoch 36/300, seasonal_0 Loss: 0.0341 | 0.0391
Epoch 37/300, seasonal_0 Loss: 0.0334 | 0.0381
Epoch 38/300, seasonal_0 Loss: 0.0322 | 0.0360
Epoch 39/300, seasonal_0 Loss: 0.0314 | 0.0361
Epoch 40/300, seasonal_0 Loss: 0.0311 | 0.0357
Epoch 41/300, seasonal_0 Loss: 0.0307 | 0.0349
Epoch 42/300, seasonal_0 Loss: 0.0300 | 0.0340
Epoch 43/300, seasonal_0 Loss: 0.0293 | 0.0329
Epoch 44/300, seasonal_0 Loss: 0.0292 | 0.0327
Epoch 45/300, seasonal_0 Loss: 0.0290 | 0.0324
Epoch 46/300, seasonal_0 Loss: 0.0289 | 0.0323
Epoch 47/300, seasonal_0 Loss: 0.0288 | 0.0321
Epoch 48/300, seasonal_0 Loss: 0.0285 | 0.0318
Epoch 49/300, seasonal_0 Loss: 0.0281 | 0.0320
Epoch 50/300, seasonal_0 Loss: 0.0279 | 0.0317
Epoch 51/300, seasonal_0 Loss: 0.0275 | 0.0320
Epoch 52/300, seasonal_0 Loss: 0.0274 | 0.0319
Epoch 53/300, seasonal_0 Loss: 0.0272 | 0.0316
Epoch 54/300, seasonal_0 Loss: 0.0271 | 0.0314
Epoch 55/300, seasonal_0 Loss: 0.0270 | 0.0311
Epoch 56/300, seasonal_0 Loss: 0.0269 | 0.0309
Epoch 57/300, seasonal_0 Loss: 0.0268 | 0.0307
Epoch 58/300, seasonal_0 Loss: 0.0268 | 0.0307
Epoch 59/300, seasonal_0 Loss: 0.0268 | 0.0313
Epoch 60/300, seasonal_0 Loss: 0.0269 | 0.0313
Epoch 61/300, seasonal_0 Loss: 0.0268 | 0.0309
Epoch 62/300, seasonal_0 Loss: 0.0266 | 0.0304
Epoch 63/300, seasonal_0 Loss: 0.0264 | 0.0304
Epoch 64/300, seasonal_0 Loss: 0.0261 | 0.0299
Epoch 65/300, seasonal_0 Loss: 0.0259 | 0.0298
Epoch 66/300, seasonal_0 Loss: 0.0259 | 0.0297
Epoch 67/300, seasonal_0 Loss: 0.0257 | 0.0298
Epoch 68/300, seasonal_0 Loss: 0.0258 | 0.0298
Epoch 69/300, seasonal_0 Loss: 0.0255 | 0.0305
Epoch 70/300, seasonal_0 Loss: 0.0258 | 0.0306
Epoch 71/300, seasonal_0 Loss: 0.0254 | 0.0318
Epoch 72/300, seasonal_0 Loss: 0.0258 | 0.0326
Epoch 73/300, seasonal_0 Loss: 0.0251 | 0.0317
Epoch 74/300, seasonal_0 Loss: 0.0256 | 0.0334
Epoch 75/300, seasonal_0 Loss: 0.0252 | 0.0303
Epoch 76/300, seasonal_0 Loss: 0.0254 | 0.0301
Epoch 77/300, seasonal_0 Loss: 0.0261 | 0.0302
Epoch 78/300, seasonal_0 Loss: 0.0265 | 0.0307
Epoch 79/300, seasonal_0 Loss: 0.0270 | 0.0342
Epoch 80/300, seasonal_0 Loss: 0.0271 | 0.0333
Epoch 81/300, seasonal_0 Loss: 0.0264 | 0.0321
Epoch 82/300, seasonal_0 Loss: 0.0257 | 0.0306
Epoch 83/300, seasonal_0 Loss: 0.0250 | 0.0300
Epoch 84/300, seasonal_0 Loss: 0.0247 | 0.0301
Epoch 85/300, seasonal_0 Loss: 0.0246 | 0.0298
Epoch 86/300, seasonal_0 Loss: 0.0245 | 0.0298
Epoch 87/300, seasonal_0 Loss: 0.0244 | 0.0295
Epoch 88/300, seasonal_0 Loss: 0.0242 | 0.0294
Epoch 89/300, seasonal_0 Loss: 0.0242 | 0.0294
Epoch 90/300, seasonal_0 Loss: 0.0241 | 0.0293
Epoch 91/300, seasonal_0 Loss: 0.0239 | 0.0292
Epoch 92/300, seasonal_0 Loss: 0.0238 | 0.0291
Epoch 93/300, seasonal_0 Loss: 0.0237 | 0.0290
Epoch 94/300, seasonal_0 Loss: 0.0236 | 0.0290
Epoch 95/300, seasonal_0 Loss: 0.0236 | 0.0290
Epoch 96/300, seasonal_0 Loss: 0.0236 | 0.0290
Epoch 97/300, seasonal_0 Loss: 0.0235 | 0.0289
Epoch 98/300, seasonal_0 Loss: 0.0235 | 0.0289
Epoch 99/300, seasonal_0 Loss: 0.0235 | 0.0289
Epoch 100/300, seasonal_0 Loss: 0.0235 | 0.0289
Epoch 101/300, seasonal_0 Loss: 0.0235 | 0.0290
Epoch 102/300, seasonal_0 Loss: 0.0235 | 0.0290
Epoch 103/300, seasonal_0 Loss: 0.0236 | 0.0290
Epoch 104/300, seasonal_0 Loss: 0.0236 | 0.0290
Epoch 105/300, seasonal_0 Loss: 0.0236 | 0.0291
Epoch 106/300, seasonal_0 Loss: 0.0237 | 0.0292
Epoch 107/300, seasonal_0 Loss: 0.0238 | 0.0292
Epoch 108/300, seasonal_0 Loss: 0.0238 | 0.0293
Epoch 109/300, seasonal_0 Loss: 0.0238 | 0.0294
Epoch 110/300, seasonal_0 Loss: 0.0239 | 0.0294
Epoch 111/300, seasonal_0 Loss: 0.0239 | 0.0294
Epoch 112/300, seasonal_0 Loss: 0.0238 | 0.0294
Epoch 113/300, seasonal_0 Loss: 0.0237 | 0.0293
Epoch 114/300, seasonal_0 Loss: 0.0236 | 0.0292
Epoch 115/300, seasonal_0 Loss: 0.0236 | 0.0292
Epoch 116/300, seasonal_0 Loss: 0.0235 | 0.0291
Epoch 117/300, seasonal_0 Loss: 0.0234 | 0.0291
Epoch 118/300, seasonal_0 Loss: 0.0233 | 0.0290
Epoch 119/300, seasonal_0 Loss: 0.0233 | 0.0290
Epoch 120/300, seasonal_0 Loss: 0.0232 | 0.0290
Epoch 121/300, seasonal_0 Loss: 0.0232 | 0.0289
Epoch 122/300, seasonal_0 Loss: 0.0231 | 0.0289
Epoch 123/300, seasonal_0 Loss: 0.0231 | 0.0289
Epoch 124/300, seasonal_0 Loss: 0.0231 | 0.0288
Epoch 125/300, seasonal_0 Loss: 0.0231 | 0.0288
Epoch 126/300, seasonal_0 Loss: 0.0230 | 0.0288
Epoch 127/300, seasonal_0 Loss: 0.0230 | 0.0288
Epoch 128/300, seasonal_0 Loss: 0.0230 | 0.0288
Epoch 129/300, seasonal_0 Loss: 0.0230 | 0.0287
Epoch 130/300, seasonal_0 Loss: 0.0230 | 0.0287
Epoch 131/300, seasonal_0 Loss: 0.0230 | 0.0287
Epoch 132/300, seasonal_0 Loss: 0.0229 | 0.0287
Epoch 133/300, seasonal_0 Loss: 0.0229 | 0.0287
Epoch 134/300, seasonal_0 Loss: 0.0229 | 0.0287
Epoch 135/300, seasonal_0 Loss: 0.0229 | 0.0287
Epoch 136/300, seasonal_0 Loss: 0.0229 | 0.0287
Epoch 137/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 138/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 139/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 140/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 141/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 142/300, seasonal_0 Loss: 0.0229 | 0.0286
Epoch 143/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 144/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 145/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 146/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 147/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 148/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 149/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 150/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 151/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 152/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 153/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 154/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 155/300, seasonal_0 Loss: 0.0228 | 0.0286
Epoch 156/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 157/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 158/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 159/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 160/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 161/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 162/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 163/300, seasonal_0 Loss: 0.0228 | 0.0285
Epoch 164/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 165/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 166/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 167/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 168/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 169/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 170/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 171/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 172/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 173/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 174/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 175/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 176/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 177/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 178/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 179/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 180/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 181/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 182/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 183/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 184/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 185/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 186/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 187/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 188/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 189/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 190/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 191/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 192/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 193/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 194/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 195/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 196/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 197/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 198/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 199/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 200/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 201/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 202/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 203/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 204/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 205/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 206/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 207/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 208/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 209/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 210/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 211/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 212/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 213/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 214/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 215/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 216/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 217/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 218/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 219/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 220/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 221/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 222/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 223/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 224/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 225/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 226/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 227/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 228/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 229/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 230/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 231/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 232/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 233/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 234/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 235/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 236/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 237/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 238/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 239/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 240/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 241/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 242/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 243/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 244/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 245/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 246/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 247/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 248/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 249/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 250/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 251/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 252/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 253/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 254/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 255/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 256/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 257/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 258/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 259/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 260/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 261/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 262/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 263/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 264/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 265/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 266/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 267/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 268/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 269/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 270/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 271/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 272/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 273/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 274/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 275/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 276/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 277/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 278/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 279/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 280/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 281/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 282/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 283/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 284/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 285/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 286/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 287/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 288/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 289/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 290/300, seasonal_0 Loss: 0.0227 | 0.0285
Epoch 291/300, seasonal_0 Loss: 0.0227 | 0.0285
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.9720425285742778, 'learning_rate': 0.0006151717938397003, 'batch_size': 103, 'step_size': 6, 'gamma': 0.9288865499055178}
Epoch 1/300, seasonal_1 Loss: 0.3108 | 0.1251
Epoch 2/300, seasonal_1 Loss: 0.1138 | 0.1016
Epoch 3/300, seasonal_1 Loss: 0.0921 | 0.0753
Epoch 4/300, seasonal_1 Loss: 0.0838 | 0.0728
Epoch 5/300, seasonal_1 Loss: 0.0820 | 0.0733
Epoch 6/300, seasonal_1 Loss: 0.0903 | 0.0724
Epoch 7/300, seasonal_1 Loss: 0.0863 | 0.0714
Epoch 8/300, seasonal_1 Loss: 0.0743 | 0.0663
Epoch 9/300, seasonal_1 Loss: 0.0699 | 0.0636
Epoch 10/300, seasonal_1 Loss: 0.0719 | 0.0666
Epoch 11/300, seasonal_1 Loss: 0.0703 | 0.0636
Epoch 12/300, seasonal_1 Loss: 0.0674 | 0.0626
Epoch 13/300, seasonal_1 Loss: 0.0639 | 0.0624
Epoch 14/300, seasonal_1 Loss: 0.0594 | 0.0581
Epoch 15/300, seasonal_1 Loss: 0.0545 | 0.0551
Epoch 16/300, seasonal_1 Loss: 0.0514 | 0.0527
Epoch 17/300, seasonal_1 Loss: 0.0496 | 0.0523
Epoch 18/300, seasonal_1 Loss: 0.0485 | 0.0498
Epoch 19/300, seasonal_1 Loss: 0.0483 | 0.0487
Epoch 20/300, seasonal_1 Loss: 0.0474 | 0.0494
Epoch 21/300, seasonal_1 Loss: 0.0482 | 0.0534
Epoch 22/300, seasonal_1 Loss: 0.0467 | 0.0503
Epoch 23/300, seasonal_1 Loss: 0.0451 | 0.0469
Epoch 24/300, seasonal_1 Loss: 0.0481 | 0.0478
Epoch 25/300, seasonal_1 Loss: 0.0469 | 0.0522
Epoch 26/300, seasonal_1 Loss: 0.0432 | 0.0446
Epoch 27/300, seasonal_1 Loss: 0.0434 | 0.0458
Epoch 28/300, seasonal_1 Loss: 0.0459 | 0.0536
Epoch 29/300, seasonal_1 Loss: 0.0526 | 0.0558
Epoch 30/300, seasonal_1 Loss: 0.0552 | 0.0608
Epoch 31/300, seasonal_1 Loss: 0.0526 | 0.0563
Epoch 32/300, seasonal_1 Loss: 0.0629 | 0.0632
Epoch 33/300, seasonal_1 Loss: 0.0606 | 0.0646
Epoch 34/300, seasonal_1 Loss: 0.0653 | 0.0510
Epoch 35/300, seasonal_1 Loss: 0.0629 | 0.1108
Epoch 36/300, seasonal_1 Loss: 0.0587 | 0.0837
Epoch 37/300, seasonal_1 Loss: 0.0561 | 0.0815
Epoch 38/300, seasonal_1 Loss: 0.0501 | 0.0439
Epoch 39/300, seasonal_1 Loss: 0.0406 | 0.0446
Epoch 40/300, seasonal_1 Loss: 0.0405 | 0.0390
Epoch 41/300, seasonal_1 Loss: 0.0380 | 0.0382
Epoch 42/300, seasonal_1 Loss: 0.0362 | 0.0378
Epoch 43/300, seasonal_1 Loss: 0.0356 | 0.0380
Epoch 44/300, seasonal_1 Loss: 0.0358 | 0.0378
Epoch 45/300, seasonal_1 Loss: 0.0367 | 0.0388
Epoch 46/300, seasonal_1 Loss: 0.0378 | 0.0392
Epoch 47/300, seasonal_1 Loss: 0.0381 | 0.0384
Epoch 48/300, seasonal_1 Loss: 0.0366 | 0.0382
Epoch 49/300, seasonal_1 Loss: 0.0356 | 0.0382
Epoch 50/300, seasonal_1 Loss: 0.0348 | 0.0368
Epoch 51/300, seasonal_1 Loss: 0.0337 | 0.0362
Epoch 52/300, seasonal_1 Loss: 0.0330 | 0.0358
Epoch 53/300, seasonal_1 Loss: 0.0328 | 0.0357
Epoch 54/300, seasonal_1 Loss: 0.0327 | 0.0360
Epoch 55/300, seasonal_1 Loss: 0.0327 | 0.0372
Epoch 56/300, seasonal_1 Loss: 0.0327 | 0.0368
Epoch 57/300, seasonal_1 Loss: 0.0319 | 0.0363
Epoch 58/300, seasonal_1 Loss: 0.0313 | 0.0364
Epoch 59/300, seasonal_1 Loss: 0.0312 | 0.0367
Epoch 60/300, seasonal_1 Loss: 0.0312 | 0.0369
Epoch 61/300, seasonal_1 Loss: 0.0316 | 0.0375
Epoch 62/300, seasonal_1 Loss: 0.0320 | 0.0373
Epoch 63/300, seasonal_1 Loss: 0.0310 | 0.0361
Epoch 64/300, seasonal_1 Loss: 0.0298 | 0.0351
Epoch 65/300, seasonal_1 Loss: 0.0294 | 0.0352
Epoch 66/300, seasonal_1 Loss: 0.0291 | 0.0349
Epoch 67/300, seasonal_1 Loss: 0.0289 | 0.0351
Epoch 68/300, seasonal_1 Loss: 0.0289 | 0.0345
Epoch 69/300, seasonal_1 Loss: 0.0285 | 0.0344
Epoch 70/300, seasonal_1 Loss: 0.0280 | 0.0342
Epoch 71/300, seasonal_1 Loss: 0.0277 | 0.0343
Epoch 72/300, seasonal_1 Loss: 0.0274 | 0.0344
Epoch 73/300, seasonal_1 Loss: 0.0272 | 0.0342
Epoch 74/300, seasonal_1 Loss: 0.0271 | 0.0342
Epoch 75/300, seasonal_1 Loss: 0.0270 | 0.0341
Epoch 76/300, seasonal_1 Loss: 0.0269 | 0.0337
Epoch 77/300, seasonal_1 Loss: 0.0267 | 0.0337
Epoch 78/300, seasonal_1 Loss: 0.0265 | 0.0336
Epoch 79/300, seasonal_1 Loss: 0.0263 | 0.0331
Epoch 80/300, seasonal_1 Loss: 0.0261 | 0.0327
Epoch 81/300, seasonal_1 Loss: 0.0259 | 0.0325
Epoch 82/300, seasonal_1 Loss: 0.0258 | 0.0321
Epoch 83/300, seasonal_1 Loss: 0.0256 | 0.0319
Epoch 84/300, seasonal_1 Loss: 0.0255 | 0.0317
Epoch 85/300, seasonal_1 Loss: 0.0254 | 0.0313
Epoch 86/300, seasonal_1 Loss: 0.0252 | 0.0310
Epoch 87/300, seasonal_1 Loss: 0.0251 | 0.0308
Epoch 88/300, seasonal_1 Loss: 0.0249 | 0.0304
Epoch 89/300, seasonal_1 Loss: 0.0247 | 0.0301
Epoch 90/300, seasonal_1 Loss: 0.0246 | 0.0299
Epoch 91/300, seasonal_1 Loss: 0.0245 | 0.0296
Epoch 92/300, seasonal_1 Loss: 0.0244 | 0.0296
Epoch 93/300, seasonal_1 Loss: 0.0243 | 0.0295
Epoch 94/300, seasonal_1 Loss: 0.0243 | 0.0287
Epoch 95/300, seasonal_1 Loss: 0.0246 | 0.0286
Epoch 96/300, seasonal_1 Loss: 0.0263 | 0.0274
Epoch 97/300, seasonal_1 Loss: 0.0282 | 0.0300
Epoch 98/300, seasonal_1 Loss: 0.0252 | 0.0294
Epoch 99/300, seasonal_1 Loss: 0.0242 | 0.0266
Epoch 100/300, seasonal_1 Loss: 0.0239 | 0.0269
Epoch 101/300, seasonal_1 Loss: 0.0238 | 0.0267
Epoch 102/300, seasonal_1 Loss: 0.0240 | 0.0266
Epoch 103/300, seasonal_1 Loss: 0.0248 | 0.0259
Epoch 104/300, seasonal_1 Loss: 0.0253 | 0.0272
Epoch 105/300, seasonal_1 Loss: 0.0239 | 0.0263
Epoch 106/300, seasonal_1 Loss: 0.0236 | 0.0259
Epoch 107/300, seasonal_1 Loss: 0.0237 | 0.0259
Epoch 108/300, seasonal_1 Loss: 0.0238 | 0.0260
Epoch 109/300, seasonal_1 Loss: 0.0239 | 0.0258
Epoch 110/300, seasonal_1 Loss: 0.0235 | 0.0258
Epoch 111/300, seasonal_1 Loss: 0.0233 | 0.0256
Epoch 112/300, seasonal_1 Loss: 0.0232 | 0.0253
Epoch 113/300, seasonal_1 Loss: 0.0232 | 0.0251
Epoch 114/300, seasonal_1 Loss: 0.0232 | 0.0250
Epoch 115/300, seasonal_1 Loss: 0.0231 | 0.0249
Epoch 116/300, seasonal_1 Loss: 0.0232 | 0.0249
Epoch 117/300, seasonal_1 Loss: 0.0232 | 0.0250
Epoch 118/300, seasonal_1 Loss: 0.0231 | 0.0249
Epoch 119/300, seasonal_1 Loss: 0.0231 | 0.0249
Epoch 120/300, seasonal_1 Loss: 0.0230 | 0.0250
Epoch 121/300, seasonal_1 Loss: 0.0229 | 0.0251
Epoch 122/300, seasonal_1 Loss: 0.0229 | 0.0252
Epoch 123/300, seasonal_1 Loss: 0.0229 | 0.0254
Epoch 124/300, seasonal_1 Loss: 0.0229 | 0.0254
Epoch 125/300, seasonal_1 Loss: 0.0230 | 0.0254
Epoch 126/300, seasonal_1 Loss: 0.0230 | 0.0256
Epoch 127/300, seasonal_1 Loss: 0.0232 | 0.0259
Epoch 128/300, seasonal_1 Loss: 0.0234 | 0.0264
Epoch 129/300, seasonal_1 Loss: 0.0238 | 0.0273
Epoch 130/300, seasonal_1 Loss: 0.0245 | 0.0295
Epoch 131/300, seasonal_1 Loss: 0.0253 | 0.0289
Epoch 132/300, seasonal_1 Loss: 0.0251 | 0.0280
Epoch 133/300, seasonal_1 Loss: 0.0241 | 0.0269
Epoch 134/300, seasonal_1 Loss: 0.0232 | 0.0266
Epoch 135/300, seasonal_1 Loss: 0.0230 | 0.0265
Epoch 136/300, seasonal_1 Loss: 0.0229 | 0.0266
Epoch 137/300, seasonal_1 Loss: 0.0229 | 0.0266
Epoch 138/300, seasonal_1 Loss: 0.0229 | 0.0266
Epoch 139/300, seasonal_1 Loss: 0.0229 | 0.0267
Epoch 140/300, seasonal_1 Loss: 0.0229 | 0.0268
Epoch 141/300, seasonal_1 Loss: 0.0228 | 0.0267
Epoch 142/300, seasonal_1 Loss: 0.0227 | 0.0265
Epoch 143/300, seasonal_1 Loss: 0.0226 | 0.0263
Epoch 144/300, seasonal_1 Loss: 0.0225 | 0.0261
Epoch 145/300, seasonal_1 Loss: 0.0223 | 0.0260
Epoch 146/300, seasonal_1 Loss: 0.0223 | 0.0259
Epoch 147/300, seasonal_1 Loss: 0.0222 | 0.0259
Epoch 148/300, seasonal_1 Loss: 0.0222 | 0.0258
Epoch 149/300, seasonal_1 Loss: 0.0222 | 0.0258
Epoch 150/300, seasonal_1 Loss: 0.0221 | 0.0257
Epoch 151/300, seasonal_1 Loss: 0.0221 | 0.0257
Epoch 152/300, seasonal_1 Loss: 0.0221 | 0.0256
Epoch 153/300, seasonal_1 Loss: 0.0220 | 0.0255
Epoch 154/300, seasonal_1 Loss: 0.0220 | 0.0254
Epoch 155/300, seasonal_1 Loss: 0.0220 | 0.0253
Epoch 156/300, seasonal_1 Loss: 0.0220 | 0.0252
Epoch 157/300, seasonal_1 Loss: 0.0219 | 0.0251
Epoch 158/300, seasonal_1 Loss: 0.0220 | 0.0250
Epoch 159/300, seasonal_1 Loss: 0.0220 | 0.0248
Epoch 160/300, seasonal_1 Loss: 0.0221 | 0.0247
Epoch 161/300, seasonal_1 Loss: 0.0223 | 0.0247
Epoch 162/300, seasonal_1 Loss: 0.0226 | 0.0248
Epoch 163/300, seasonal_1 Loss: 0.0227 | 0.0248
Epoch 164/300, seasonal_1 Loss: 0.0224 | 0.0248
Epoch 165/300, seasonal_1 Loss: 0.0222 | 0.0248
Epoch 166/300, seasonal_1 Loss: 0.0220 | 0.0247
Epoch 167/300, seasonal_1 Loss: 0.0220 | 0.0247
Epoch 168/300, seasonal_1 Loss: 0.0219 | 0.0247
Epoch 169/300, seasonal_1 Loss: 0.0219 | 0.0247
Epoch 170/300, seasonal_1 Loss: 0.0219 | 0.0247
Epoch 171/300, seasonal_1 Loss: 0.0218 | 0.0247
Epoch 172/300, seasonal_1 Loss: 0.0218 | 0.0247
Epoch 173/300, seasonal_1 Loss: 0.0218 | 0.0247
Epoch 174/300, seasonal_1 Loss: 0.0218 | 0.0247
Epoch 175/300, seasonal_1 Loss: 0.0217 | 0.0246
Epoch 176/300, seasonal_1 Loss: 0.0217 | 0.0246
Epoch 177/300, seasonal_1 Loss: 0.0217 | 0.0246
Epoch 178/300, seasonal_1 Loss: 0.0217 | 0.0246
Epoch 179/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 180/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 181/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 182/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 183/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 184/300, seasonal_1 Loss: 0.0216 | 0.0246
Epoch 185/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 186/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 187/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 188/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 189/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 190/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 191/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 192/300, seasonal_1 Loss: 0.0215 | 0.0246
Epoch 193/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 194/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 195/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 196/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 197/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 198/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 199/300, seasonal_1 Loss: 0.0214 | 0.0246
Epoch 200/300, seasonal_1 Loss: 0.0214 | 0.0247
Epoch 201/300, seasonal_1 Loss: 0.0214 | 0.0247
Epoch 202/300, seasonal_1 Loss: 0.0214 | 0.0247
Epoch 203/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 204/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 205/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 206/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 207/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 208/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 209/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 210/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 211/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 212/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 213/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 214/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 215/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 216/300, seasonal_1 Loss: 0.0213 | 0.0247
Epoch 217/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 218/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 219/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 220/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 221/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 222/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 223/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 224/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 225/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 226/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 227/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 228/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 229/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 230/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 231/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 232/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 233/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 234/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 235/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 236/300, seasonal_1 Loss: 0.0212 | 0.0247
Epoch 237/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 238/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 239/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 240/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 241/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 242/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 243/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 244/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 245/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 246/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 247/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 248/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 249/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 250/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 251/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 252/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 253/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 254/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 255/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 256/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 257/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 258/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 259/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 260/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 261/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 262/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 263/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 264/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 265/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 266/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 267/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 268/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 269/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 270/300, seasonal_1 Loss: 0.0211 | 0.0247
Epoch 271/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 272/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 273/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 274/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 275/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 276/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 277/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 278/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 279/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 280/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 281/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 282/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 283/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 284/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 285/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 286/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 287/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 288/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 289/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 290/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 291/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 292/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 293/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 294/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 295/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 296/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 297/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 298/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 299/300, seasonal_1 Loss: 0.0210 | 0.0247
Epoch 300/300, seasonal_1 Loss: 0.0210 | 0.0247
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.8775525389506476, 'learning_rate': 0.0006307175783347395, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9213763588541641}
Epoch 1/300, seasonal_2 Loss: 0.4470 | 0.1914
Epoch 2/300, seasonal_2 Loss: 0.1754 | 0.1954
Epoch 3/300, seasonal_2 Loss: 0.1349 | 0.2645
Epoch 4/300, seasonal_2 Loss: 0.1448 | 0.2994
Epoch 5/300, seasonal_2 Loss: 0.1768 | 0.1601
Epoch 6/300, seasonal_2 Loss: 0.1711 | 0.1041
Epoch 7/300, seasonal_2 Loss: 0.1189 | 0.3605
Epoch 8/300, seasonal_2 Loss: 0.1506 | 0.1913
Epoch 9/300, seasonal_2 Loss: 0.1634 | 0.1697
Epoch 10/300, seasonal_2 Loss: 0.1060 | 0.0878
Epoch 11/300, seasonal_2 Loss: 0.0843 | 0.1111
Epoch 12/300, seasonal_2 Loss: 0.0812 | 0.0855
Epoch 13/300, seasonal_2 Loss: 0.0788 | 0.0794
Epoch 14/300, seasonal_2 Loss: 0.0731 | 0.0743
Epoch 15/300, seasonal_2 Loss: 0.0662 | 0.0723
Epoch 16/300, seasonal_2 Loss: 0.0652 | 0.0672
Epoch 17/300, seasonal_2 Loss: 0.0677 | 0.0741
Epoch 18/300, seasonal_2 Loss: 0.0687 | 0.0716
Epoch 19/300, seasonal_2 Loss: 0.0630 | 0.0624
Epoch 20/300, seasonal_2 Loss: 0.0609 | 0.0600
Epoch 21/300, seasonal_2 Loss: 0.0704 | 0.0719
Epoch 22/300, seasonal_2 Loss: 0.0925 | 0.0738
Epoch 23/300, seasonal_2 Loss: 0.1106 | 0.0701
Epoch 24/300, seasonal_2 Loss: 0.0947 | 0.0654
Epoch 25/300, seasonal_2 Loss: 0.0659 | 0.0619
Epoch 26/300, seasonal_2 Loss: 0.0791 | 0.0742
Epoch 27/300, seasonal_2 Loss: 0.0597 | 0.1264
Epoch 28/300, seasonal_2 Loss: 0.0839 | 0.0685
Epoch 29/300, seasonal_2 Loss: 0.0655 | 0.0910
Epoch 30/300, seasonal_2 Loss: 0.0599 | 0.0564
Epoch 31/300, seasonal_2 Loss: 0.0517 | 0.0573
Epoch 32/300, seasonal_2 Loss: 0.0521 | 0.0755
Epoch 33/300, seasonal_2 Loss: 0.0496 | 0.0543
Epoch 34/300, seasonal_2 Loss: 0.0458 | 0.0660
Epoch 35/300, seasonal_2 Loss: 0.0483 | 0.0486
Epoch 36/300, seasonal_2 Loss: 0.0457 | 0.0519
Epoch 37/300, seasonal_2 Loss: 0.0476 | 0.0763
Epoch 38/300, seasonal_2 Loss: 0.0494 | 0.0546
Epoch 39/300, seasonal_2 Loss: 0.0485 | 0.0506
Epoch 40/300, seasonal_2 Loss: 0.0471 | 0.0488
Epoch 41/300, seasonal_2 Loss: 0.0475 | 0.0719
Epoch 42/300, seasonal_2 Loss: 0.0500 | 0.0478
Epoch 43/300, seasonal_2 Loss: 0.0439 | 0.0528
Epoch 44/300, seasonal_2 Loss: 0.0467 | 0.0471
Epoch 45/300, seasonal_2 Loss: 0.0473 | 0.0449
Epoch 46/300, seasonal_2 Loss: 0.0464 | 0.0439
Epoch 47/300, seasonal_2 Loss: 0.0448 | 0.0740
Epoch 48/300, seasonal_2 Loss: 0.0462 | 0.0480
Epoch 49/300, seasonal_2 Loss: 0.0426 | 0.0443
Epoch 50/300, seasonal_2 Loss: 0.0412 | 0.0469
Epoch 51/300, seasonal_2 Loss: 0.0418 | 0.0601
Epoch 52/300, seasonal_2 Loss: 0.0402 | 0.0485
Epoch 53/300, seasonal_2 Loss: 0.0406 | 0.0451
Epoch 54/300, seasonal_2 Loss: 0.0388 | 0.0435
Epoch 55/300, seasonal_2 Loss: 0.0374 | 0.0563
Epoch 56/300, seasonal_2 Loss: 0.0421 | 0.0432
Epoch 57/300, seasonal_2 Loss: 0.0408 | 0.0647
Epoch 58/300, seasonal_2 Loss: 0.0399 | 0.0438
Epoch 59/300, seasonal_2 Loss: 0.0360 | 0.0401
Epoch 60/300, seasonal_2 Loss: 0.0352 | 0.0407
Epoch 61/300, seasonal_2 Loss: 0.0346 | 0.0450
Epoch 62/300, seasonal_2 Loss: 0.0333 | 0.0392
Epoch 63/300, seasonal_2 Loss: 0.0328 | 0.0385
Epoch 64/300, seasonal_2 Loss: 0.0329 | 0.0406
Epoch 65/300, seasonal_2 Loss: 0.0331 | 0.0443
Epoch 66/300, seasonal_2 Loss: 0.0349 | 0.0395
Epoch 67/300, seasonal_2 Loss: 0.0353 | 0.0441
Epoch 68/300, seasonal_2 Loss: 0.0337 | 0.0448
Epoch 69/300, seasonal_2 Loss: 0.0359 | 0.0383
Epoch 70/300, seasonal_2 Loss: 0.0354 | 0.0433
Epoch 71/300, seasonal_2 Loss: 0.0359 | 0.0413
Epoch 72/300, seasonal_2 Loss: 0.0363 | 0.0400
Epoch 73/300, seasonal_2 Loss: 0.0370 | 0.0527
Epoch 74/300, seasonal_2 Loss: 0.0351 | 0.0430
Epoch 75/300, seasonal_2 Loss: 0.0356 | 0.0384
Epoch 76/300, seasonal_2 Loss: 0.0327 | 0.0399
Epoch 77/300, seasonal_2 Loss: 0.0318 | 0.0375
Epoch 78/300, seasonal_2 Loss: 0.0302 | 0.0380
Epoch 79/300, seasonal_2 Loss: 0.0294 | 0.0367
Epoch 80/300, seasonal_2 Loss: 0.0291 | 0.0390
Epoch 81/300, seasonal_2 Loss: 0.0289 | 0.0370
Epoch 82/300, seasonal_2 Loss: 0.0288 | 0.0369
Epoch 83/300, seasonal_2 Loss: 0.0289 | 0.0375
Epoch 84/300, seasonal_2 Loss: 0.0291 | 0.0375
Epoch 85/300, seasonal_2 Loss: 0.0296 | 0.0363
Epoch 86/300, seasonal_2 Loss: 0.0298 | 0.0386
Epoch 87/300, seasonal_2 Loss: 0.0295 | 0.0365
Epoch 88/300, seasonal_2 Loss: 0.0286 | 0.0361
Epoch 89/300, seasonal_2 Loss: 0.0282 | 0.0377
Epoch 90/300, seasonal_2 Loss: 0.0293 | 0.0394
Epoch 91/300, seasonal_2 Loss: 0.0315 | 0.0368
Epoch 92/300, seasonal_2 Loss: 0.0302 | 0.0409
Epoch 93/300, seasonal_2 Loss: 0.0285 | 0.0364
Epoch 94/300, seasonal_2 Loss: 0.0282 | 0.0352
Epoch 95/300, seasonal_2 Loss: 0.0283 | 0.0364
Epoch 96/300, seasonal_2 Loss: 0.0277 | 0.0391
Epoch 97/300, seasonal_2 Loss: 0.0282 | 0.0352
Epoch 98/300, seasonal_2 Loss: 0.0284 | 0.0398
Epoch 99/300, seasonal_2 Loss: 0.0274 | 0.0395
Epoch 100/300, seasonal_2 Loss: 0.0286 | 0.0361
Epoch 101/300, seasonal_2 Loss: 0.0286 | 0.0427
Epoch 102/300, seasonal_2 Loss: 0.0285 | 0.0383
Epoch 103/300, seasonal_2 Loss: 0.0310 | 0.0380
Epoch 104/300, seasonal_2 Loss: 0.0350 | 0.0480
Epoch 105/300, seasonal_2 Loss: 0.0320 | 0.0391
Epoch 106/300, seasonal_2 Loss: 0.0292 | 0.0358
Epoch 107/300, seasonal_2 Loss: 0.0280 | 0.0390
Epoch 108/300, seasonal_2 Loss: 0.0265 | 0.0356
Epoch 109/300, seasonal_2 Loss: 0.0261 | 0.0352
Epoch 110/300, seasonal_2 Loss: 0.0264 | 0.0365
Epoch 111/300, seasonal_2 Loss: 0.0273 | 0.0357
Epoch 112/300, seasonal_2 Loss: 0.0266 | 0.0361
Epoch 113/300, seasonal_2 Loss: 0.0256 | 0.0354
Epoch 114/300, seasonal_2 Loss: 0.0263 | 0.0353
Epoch 115/300, seasonal_2 Loss: 0.0260 | 0.0358
Epoch 116/300, seasonal_2 Loss: 0.0248 | 0.0351
Epoch 117/300, seasonal_2 Loss: 0.0243 | 0.0359
Epoch 118/300, seasonal_2 Loss: 0.0247 | 0.0352
Epoch 119/300, seasonal_2 Loss: 0.0248 | 0.0360
Epoch 120/300, seasonal_2 Loss: 0.0250 | 0.0349
Epoch 121/300, seasonal_2 Loss: 0.0242 | 0.0349
Epoch 122/300, seasonal_2 Loss: 0.0248 | 0.0347
Epoch 123/300, seasonal_2 Loss: 0.0260 | 0.0372
Epoch 124/300, seasonal_2 Loss: 0.0252 | 0.0341
Epoch 125/300, seasonal_2 Loss: 0.0240 | 0.0354
Epoch 126/300, seasonal_2 Loss: 0.0240 | 0.0349
Epoch 127/300, seasonal_2 Loss: 0.0239 | 0.0344
Epoch 128/300, seasonal_2 Loss: 0.0243 | 0.0348
Epoch 129/300, seasonal_2 Loss: 0.0239 | 0.0360
Epoch 130/300, seasonal_2 Loss: 0.0237 | 0.0332
Epoch 131/300, seasonal_2 Loss: 0.0235 | 0.0361
Epoch 132/300, seasonal_2 Loss: 0.0290 | 0.0386
Epoch 133/300, seasonal_2 Loss: 0.0264 | 0.0350
Epoch 134/300, seasonal_2 Loss: 0.0248 | 0.0358
Epoch 135/300, seasonal_2 Loss: 0.0233 | 0.0342
Epoch 136/300, seasonal_2 Loss: 0.0228 | 0.0335
Epoch 137/300, seasonal_2 Loss: 0.0222 | 0.0344
Epoch 138/300, seasonal_2 Loss: 0.0220 | 0.0336
Epoch 139/300, seasonal_2 Loss: 0.0225 | 0.0340
Epoch 140/300, seasonal_2 Loss: 0.0226 | 0.0346
Epoch 141/300, seasonal_2 Loss: 0.0222 | 0.0337
Epoch 142/300, seasonal_2 Loss: 0.0217 | 0.0337
Epoch 143/300, seasonal_2 Loss: 0.0215 | 0.0340
Epoch 144/300, seasonal_2 Loss: 0.0221 | 0.0338
Epoch 145/300, seasonal_2 Loss: 0.0234 | 0.0345
Epoch 146/300, seasonal_2 Loss: 0.0218 | 0.0351
Epoch 147/300, seasonal_2 Loss: 0.0211 | 0.0326
Epoch 148/300, seasonal_2 Loss: 0.0211 | 0.0332
Epoch 149/300, seasonal_2 Loss: 0.0209 | 0.0334
Epoch 150/300, seasonal_2 Loss: 0.0206 | 0.0323
Epoch 151/300, seasonal_2 Loss: 0.0204 | 0.0338
Epoch 152/300, seasonal_2 Loss: 0.0203 | 0.0328
Epoch 153/300, seasonal_2 Loss: 0.0206 | 0.0322
Epoch 154/300, seasonal_2 Loss: 0.0218 | 0.0357
Epoch 155/300, seasonal_2 Loss: 0.0240 | 0.0332
Epoch 156/300, seasonal_2 Loss: 0.0212 | 0.0337
Epoch 157/300, seasonal_2 Loss: 0.0226 | 0.0333
Epoch 158/300, seasonal_2 Loss: 0.0214 | 0.0366
Epoch 159/300, seasonal_2 Loss: 0.0210 | 0.0322
Epoch 160/300, seasonal_2 Loss: 0.0200 | 0.0323
Epoch 161/300, seasonal_2 Loss: 0.0198 | 0.0322
Epoch 162/300, seasonal_2 Loss: 0.0195 | 0.0323
Epoch 163/300, seasonal_2 Loss: 0.0197 | 0.0326
Epoch 164/300, seasonal_2 Loss: 0.0206 | 0.0331
Epoch 165/300, seasonal_2 Loss: 0.0205 | 0.0330
Epoch 166/300, seasonal_2 Loss: 0.0202 | 0.0325
Epoch 167/300, seasonal_2 Loss: 0.0201 | 0.0331
Epoch 168/300, seasonal_2 Loss: 0.0215 | 0.0332
Epoch 169/300, seasonal_2 Loss: 0.0202 | 0.0340
Epoch 170/300, seasonal_2 Loss: 0.0200 | 0.0321
Epoch 171/300, seasonal_2 Loss: 0.0193 | 0.0325
Epoch 172/300, seasonal_2 Loss: 0.0189 | 0.0323
Epoch 173/300, seasonal_2 Loss: 0.0186 | 0.0320
Epoch 174/300, seasonal_2 Loss: 0.0187 | 0.0329
Epoch 175/300, seasonal_2 Loss: 0.0190 | 0.0321
Epoch 176/300, seasonal_2 Loss: 0.0188 | 0.0321
Epoch 177/300, seasonal_2 Loss: 0.0183 | 0.0319
Epoch 178/300, seasonal_2 Loss: 0.0184 | 0.0320
Epoch 179/300, seasonal_2 Loss: 0.0184 | 0.0317
Epoch 180/300, seasonal_2 Loss: 0.0182 | 0.0322
Epoch 181/300, seasonal_2 Loss: 0.0181 | 0.0322
Epoch 182/300, seasonal_2 Loss: 0.0184 | 0.0325
Epoch 183/300, seasonal_2 Loss: 0.0186 | 0.0319
Epoch 184/300, seasonal_2 Loss: 0.0182 | 0.0322
Epoch 185/300, seasonal_2 Loss: 0.0179 | 0.0318
Epoch 186/300, seasonal_2 Loss: 0.0179 | 0.0320
Epoch 187/300, seasonal_2 Loss: 0.0178 | 0.0319
Epoch 188/300, seasonal_2 Loss: 0.0178 | 0.0323
Epoch 189/300, seasonal_2 Loss: 0.0179 | 0.0323
Epoch 190/300, seasonal_2 Loss: 0.0181 | 0.0321
Epoch 191/300, seasonal_2 Loss: 0.0177 | 0.0320
Epoch 192/300, seasonal_2 Loss: 0.0175 | 0.0320
Epoch 193/300, seasonal_2 Loss: 0.0175 | 0.0318
Epoch 194/300, seasonal_2 Loss: 0.0174 | 0.0322
Epoch 195/300, seasonal_2 Loss: 0.0174 | 0.0322
Epoch 196/300, seasonal_2 Loss: 0.0176 | 0.0323
Epoch 197/300, seasonal_2 Loss: 0.0175 | 0.0320
Epoch 198/300, seasonal_2 Loss: 0.0172 | 0.0322
Epoch 199/300, seasonal_2 Loss: 0.0171 | 0.0319
Epoch 200/300, seasonal_2 Loss: 0.0171 | 0.0322
Epoch 201/300, seasonal_2 Loss: 0.0170 | 0.0321
Epoch 202/300, seasonal_2 Loss: 0.0170 | 0.0324
Epoch 203/300, seasonal_2 Loss: 0.0171 | 0.0323
Epoch 204/300, seasonal_2 Loss: 0.0171 | 0.0322
Epoch 205/300, seasonal_2 Loss: 0.0169 | 0.0322
Epoch 206/300, seasonal_2 Loss: 0.0168 | 0.0322
Epoch 207/300, seasonal_2 Loss: 0.0168 | 0.0321
Epoch 208/300, seasonal_2 Loss: 0.0167 | 0.0323
Epoch 209/300, seasonal_2 Loss: 0.0166 | 0.0322
Epoch 210/300, seasonal_2 Loss: 0.0166 | 0.0323
Epoch 211/300, seasonal_2 Loss: 0.0166 | 0.0323
Epoch 212/300, seasonal_2 Loss: 0.0167 | 0.0324
Epoch 213/300, seasonal_2 Loss: 0.0166 | 0.0322
Epoch 214/300, seasonal_2 Loss: 0.0164 | 0.0324
Epoch 215/300, seasonal_2 Loss: 0.0164 | 0.0322
Epoch 216/300, seasonal_2 Loss: 0.0164 | 0.0324
Epoch 217/300, seasonal_2 Loss: 0.0163 | 0.0323
Epoch 218/300, seasonal_2 Loss: 0.0163 | 0.0324
Epoch 219/300, seasonal_2 Loss: 0.0162 | 0.0323
Epoch 220/300, seasonal_2 Loss: 0.0162 | 0.0325
Epoch 221/300, seasonal_2 Loss: 0.0163 | 0.0324
Epoch 222/300, seasonal_2 Loss: 0.0164 | 0.0325
Epoch 223/300, seasonal_2 Loss: 0.0162 | 0.0325
Epoch 224/300, seasonal_2 Loss: 0.0161 | 0.0326
Epoch 225/300, seasonal_2 Loss: 0.0161 | 0.0325
Epoch 226/300, seasonal_2 Loss: 0.0161 | 0.0326
Epoch 227/300, seasonal_2 Loss: 0.0160 | 0.0325
Epoch 228/300, seasonal_2 Loss: 0.0160 | 0.0326
Epoch 229/300, seasonal_2 Loss: 0.0160 | 0.0325
Epoch 230/300, seasonal_2 Loss: 0.0161 | 0.0326
Epoch 231/300, seasonal_2 Loss: 0.0161 | 0.0324
Epoch 232/300, seasonal_2 Loss: 0.0158 | 0.0327
Epoch 233/300, seasonal_2 Loss: 0.0158 | 0.0327
Epoch 234/300, seasonal_2 Loss: 0.0159 | 0.0327
Epoch 235/300, seasonal_2 Loss: 0.0158 | 0.0326
Epoch 236/300, seasonal_2 Loss: 0.0157 | 0.0327
Epoch 237/300, seasonal_2 Loss: 0.0157 | 0.0326
Epoch 238/300, seasonal_2 Loss: 0.0159 | 0.0327
Epoch 239/300, seasonal_2 Loss: 0.0157 | 0.0327
Epoch 240/300, seasonal_2 Loss: 0.0156 | 0.0330
Epoch 241/300, seasonal_2 Loss: 0.0157 | 0.0328
Epoch 242/300, seasonal_2 Loss: 0.0156 | 0.0328
Epoch 243/300, seasonal_2 Loss: 0.0155 | 0.0327
Epoch 244/300, seasonal_2 Loss: 0.0156 | 0.0329
Epoch 245/300, seasonal_2 Loss: 0.0157 | 0.0327
Epoch 246/300, seasonal_2 Loss: 0.0154 | 0.0329
Epoch 247/300, seasonal_2 Loss: 0.0155 | 0.0329
Epoch 248/300, seasonal_2 Loss: 0.0155 | 0.0329
Epoch 249/300, seasonal_2 Loss: 0.0154 | 0.0329
Epoch 250/300, seasonal_2 Loss: 0.0154 | 0.0329
Epoch 251/300, seasonal_2 Loss: 0.0154 | 0.0327
Epoch 252/300, seasonal_2 Loss: 0.0152 | 0.0331
Epoch 253/300, seasonal_2 Loss: 0.0153 | 0.0330
Epoch 254/300, seasonal_2 Loss: 0.0152 | 0.0331
Epoch 255/300, seasonal_2 Loss: 0.0151 | 0.0331
Epoch 256/300, seasonal_2 Loss: 0.0152 | 0.0330
Epoch 257/300, seasonal_2 Loss: 0.0151 | 0.0330
Epoch 258/300, seasonal_2 Loss: 0.0150 | 0.0332
Epoch 259/300, seasonal_2 Loss: 0.0151 | 0.0332
Epoch 260/300, seasonal_2 Loss: 0.0150 | 0.0332
Epoch 261/300, seasonal_2 Loss: 0.0150 | 0.0331
Epoch 262/300, seasonal_2 Loss: 0.0150 | 0.0331
Epoch 263/300, seasonal_2 Loss: 0.0149 | 0.0331
Epoch 264/300, seasonal_2 Loss: 0.0149 | 0.0334
Epoch 265/300, seasonal_2 Loss: 0.0149 | 0.0333
Epoch 266/300, seasonal_2 Loss: 0.0148 | 0.0333
Epoch 267/300, seasonal_2 Loss: 0.0148 | 0.0332
Epoch 268/300, seasonal_2 Loss: 0.0149 | 0.0332
Epoch 269/300, seasonal_2 Loss: 0.0147 | 0.0333
Epoch 270/300, seasonal_2 Loss: 0.0148 | 0.0334
Epoch 271/300, seasonal_2 Loss: 0.0148 | 0.0334
Epoch 272/300, seasonal_2 Loss: 0.0147 | 0.0334
Epoch 273/300, seasonal_2 Loss: 0.0148 | 0.0333
Epoch 274/300, seasonal_2 Loss: 0.0147 | 0.0333
Epoch 275/300, seasonal_2 Loss: 0.0146 | 0.0335
Epoch 276/300, seasonal_2 Loss: 0.0147 | 0.0335
Epoch 277/300, seasonal_2 Loss: 0.0146 | 0.0335
Epoch 278/300, seasonal_2 Loss: 0.0146 | 0.0334
Epoch 279/300, seasonal_2 Loss: 0.0147 | 0.0333
Epoch 280/300, seasonal_2 Loss: 0.0145 | 0.0335
Epoch 281/300, seasonal_2 Loss: 0.0146 | 0.0335
Epoch 282/300, seasonal_2 Loss: 0.0145 | 0.0336
Epoch 283/300, seasonal_2 Loss: 0.0145 | 0.0335
Epoch 284/300, seasonal_2 Loss: 0.0145 | 0.0334
Epoch 285/300, seasonal_2 Loss: 0.0144 | 0.0335
Epoch 286/300, seasonal_2 Loss: 0.0144 | 0.0336
Epoch 287/300, seasonal_2 Loss: 0.0144 | 0.0336
Epoch 288/300, seasonal_2 Loss: 0.0144 | 0.0336
Epoch 289/300, seasonal_2 Loss: 0.0144 | 0.0335
Epoch 290/300, seasonal_2 Loss: 0.0143 | 0.0336
Epoch 291/300, seasonal_2 Loss: 0.0143 | 0.0336
Epoch 292/300, seasonal_2 Loss: 0.0142 | 0.0337
Epoch 293/300, seasonal_2 Loss: 0.0142 | 0.0337
Epoch 294/300, seasonal_2 Loss: 0.0143 | 0.0336
Epoch 295/300, seasonal_2 Loss: 0.0142 | 0.0337
Epoch 296/300, seasonal_2 Loss: 0.0142 | 0.0337
Epoch 297/300, seasonal_2 Loss: 0.0141 | 0.0338
Epoch 298/300, seasonal_2 Loss: 0.0141 | 0.0337
Epoch 299/300, seasonal_2 Loss: 0.0141 | 0.0337
Epoch 300/300, seasonal_2 Loss: 0.0141 | 0.0338
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.6881017692595994, 'learning_rate': 0.0009093786272155121, 'batch_size': 60, 'step_size': 7, 'gamma': 0.8320512906021822}
Epoch 1/300, seasonal_3 Loss: 0.3641 | 0.2668
Epoch 2/300, seasonal_3 Loss: 0.1130 | 0.1908
Epoch 3/300, seasonal_3 Loss: 0.0955 | 0.1574
Epoch 4/300, seasonal_3 Loss: 0.0833 | 0.1223
Epoch 5/300, seasonal_3 Loss: 0.0776 | 0.1309
Epoch 6/300, seasonal_3 Loss: 0.0778 | 0.1472
Epoch 7/300, seasonal_3 Loss: 0.0771 | 0.1271
Epoch 8/300, seasonal_3 Loss: 0.0685 | 0.1255
Epoch 9/300, seasonal_3 Loss: 0.0636 | 0.1436
Epoch 10/300, seasonal_3 Loss: 0.0634 | 0.1117
Epoch 11/300, seasonal_3 Loss: 0.0564 | 0.0832
Epoch 12/300, seasonal_3 Loss: 0.0479 | 0.0875
Epoch 13/300, seasonal_3 Loss: 0.0471 | 0.0655
Epoch 14/300, seasonal_3 Loss: 0.0450 | 0.1050
Epoch 15/300, seasonal_3 Loss: 0.0532 | 0.0853
Epoch 16/300, seasonal_3 Loss: 0.0455 | 0.0747
Epoch 17/300, seasonal_3 Loss: 0.0403 | 0.0534
Epoch 18/300, seasonal_3 Loss: 0.0373 | 0.0527
Epoch 19/300, seasonal_3 Loss: 0.0360 | 0.0513
Epoch 20/300, seasonal_3 Loss: 0.0350 | 0.0530
Epoch 21/300, seasonal_3 Loss: 0.0349 | 0.0546
Epoch 22/300, seasonal_3 Loss: 0.0344 | 0.0630
Epoch 23/300, seasonal_3 Loss: 0.0364 | 0.0717
Epoch 24/300, seasonal_3 Loss: 0.0362 | 0.0753
Epoch 25/300, seasonal_3 Loss: 0.0407 | 0.0745
Epoch 26/300, seasonal_3 Loss: 0.0342 | 0.0480
Epoch 27/300, seasonal_3 Loss: 0.0320 | 0.0478
Epoch 28/300, seasonal_3 Loss: 0.0314 | 0.0547
Epoch 29/300, seasonal_3 Loss: 0.0313 | 0.0487
Epoch 30/300, seasonal_3 Loss: 0.0314 | 0.0600
Epoch 31/300, seasonal_3 Loss: 0.0326 | 0.0544
Epoch 32/300, seasonal_3 Loss: 0.0319 | 0.0602
Epoch 33/300, seasonal_3 Loss: 0.0325 | 0.0519
Epoch 34/300, seasonal_3 Loss: 0.0306 | 0.0526
Epoch 35/300, seasonal_3 Loss: 0.0300 | 0.0484
Epoch 36/300, seasonal_3 Loss: 0.0290 | 0.0511
Epoch 37/300, seasonal_3 Loss: 0.0284 | 0.0461
Epoch 38/300, seasonal_3 Loss: 0.0281 | 0.0517
Epoch 39/300, seasonal_3 Loss: 0.0276 | 0.0463
Epoch 40/300, seasonal_3 Loss: 0.0272 | 0.0483
Epoch 41/300, seasonal_3 Loss: 0.0266 | 0.0450
Epoch 42/300, seasonal_3 Loss: 0.0264 | 0.0485
Epoch 43/300, seasonal_3 Loss: 0.0259 | 0.0448
Epoch 44/300, seasonal_3 Loss: 0.0256 | 0.0456
Epoch 45/300, seasonal_3 Loss: 0.0253 | 0.0445
Epoch 46/300, seasonal_3 Loss: 0.0251 | 0.0452
Epoch 47/300, seasonal_3 Loss: 0.0248 | 0.0444
Epoch 48/300, seasonal_3 Loss: 0.0249 | 0.0444
Epoch 49/300, seasonal_3 Loss: 0.0247 | 0.0448
Epoch 50/300, seasonal_3 Loss: 0.0245 | 0.0477
Epoch 51/300, seasonal_3 Loss: 0.0244 | 0.0469
Epoch 52/300, seasonal_3 Loss: 0.0242 | 0.0469
Epoch 53/300, seasonal_3 Loss: 0.0241 | 0.0469
Epoch 54/300, seasonal_3 Loss: 0.0240 | 0.0453
Epoch 55/300, seasonal_3 Loss: 0.0240 | 0.0452
Epoch 56/300, seasonal_3 Loss: 0.0239 | 0.0449
Epoch 57/300, seasonal_3 Loss: 0.0240 | 0.0431
Epoch 58/300, seasonal_3 Loss: 0.0243 | 0.0424
Epoch 59/300, seasonal_3 Loss: 0.0241 | 0.0420
Epoch 60/300, seasonal_3 Loss: 0.0240 | 0.0418
Epoch 61/300, seasonal_3 Loss: 0.0239 | 0.0417
Epoch 62/300, seasonal_3 Loss: 0.0239 | 0.0418
Epoch 63/300, seasonal_3 Loss: 0.0237 | 0.0418
Epoch 64/300, seasonal_3 Loss: 0.0237 | 0.0421
Epoch 65/300, seasonal_3 Loss: 0.0237 | 0.0420
Epoch 66/300, seasonal_3 Loss: 0.0236 | 0.0420
Epoch 67/300, seasonal_3 Loss: 0.0235 | 0.0420
Epoch 68/300, seasonal_3 Loss: 0.0235 | 0.0421
Epoch 69/300, seasonal_3 Loss: 0.0236 | 0.0422
Epoch 70/300, seasonal_3 Loss: 0.0235 | 0.0423
Epoch 71/300, seasonal_3 Loss: 0.0235 | 0.0420
Epoch 72/300, seasonal_3 Loss: 0.0235 | 0.0422
Epoch 73/300, seasonal_3 Loss: 0.0234 | 0.0422
Epoch 74/300, seasonal_3 Loss: 0.0232 | 0.0423
Epoch 75/300, seasonal_3 Loss: 0.0232 | 0.0416
Epoch 76/300, seasonal_3 Loss: 0.0232 | 0.0418
Epoch 77/300, seasonal_3 Loss: 0.0231 | 0.0419
Epoch 78/300, seasonal_3 Loss: 0.0230 | 0.0416
Epoch 79/300, seasonal_3 Loss: 0.0230 | 0.0417
Epoch 80/300, seasonal_3 Loss: 0.0230 | 0.0419
Epoch 81/300, seasonal_3 Loss: 0.0229 | 0.0421
Epoch 82/300, seasonal_3 Loss: 0.0228 | 0.0422
Epoch 83/300, seasonal_3 Loss: 0.0228 | 0.0422
Epoch 84/300, seasonal_3 Loss: 0.0227 | 0.0422
Epoch 85/300, seasonal_3 Loss: 0.0227 | 0.0427
Epoch 86/300, seasonal_3 Loss: 0.0226 | 0.0427
Epoch 87/300, seasonal_3 Loss: 0.0224 | 0.0425
Epoch 88/300, seasonal_3 Loss: 0.0223 | 0.0423
Epoch 89/300, seasonal_3 Loss: 0.0223 | 0.0427
Epoch 90/300, seasonal_3 Loss: 0.0222 | 0.0425
Epoch 91/300, seasonal_3 Loss: 0.0221 | 0.0422
Epoch 92/300, seasonal_3 Loss: 0.0221 | 0.0426
Epoch 93/300, seasonal_3 Loss: 0.0220 | 0.0423
Epoch 94/300, seasonal_3 Loss: 0.0220 | 0.0421
Epoch 95/300, seasonal_3 Loss: 0.0219 | 0.0419
Epoch 96/300, seasonal_3 Loss: 0.0219 | 0.0422
Epoch 97/300, seasonal_3 Loss: 0.0218 | 0.0420
Epoch 98/300, seasonal_3 Loss: 0.0218 | 0.0418
Epoch 99/300, seasonal_3 Loss: 0.0218 | 0.0419
Epoch 100/300, seasonal_3 Loss: 0.0217 | 0.0417
Epoch 101/300, seasonal_3 Loss: 0.0217 | 0.0416
Epoch 102/300, seasonal_3 Loss: 0.0217 | 0.0415
Epoch 103/300, seasonal_3 Loss: 0.0217 | 0.0415
Epoch 104/300, seasonal_3 Loss: 0.0216 | 0.0414
Epoch 105/300, seasonal_3 Loss: 0.0216 | 0.0413
Epoch 106/300, seasonal_3 Loss: 0.0216 | 0.0413
Epoch 107/300, seasonal_3 Loss: 0.0216 | 0.0412
Epoch 108/300, seasonal_3 Loss: 0.0215 | 0.0412
Epoch 109/300, seasonal_3 Loss: 0.0215 | 0.0412
Epoch 110/300, seasonal_3 Loss: 0.0215 | 0.0412
Epoch 111/300, seasonal_3 Loss: 0.0215 | 0.0411
Epoch 112/300, seasonal_3 Loss: 0.0215 | 0.0411
Epoch 113/300, seasonal_3 Loss: 0.0215 | 0.0411
Epoch 114/300, seasonal_3 Loss: 0.0214 | 0.0411
Epoch 115/300, seasonal_3 Loss: 0.0214 | 0.0411
Epoch 116/300, seasonal_3 Loss: 0.0214 | 0.0411
Epoch 117/300, seasonal_3 Loss: 0.0214 | 0.0411
Epoch 118/300, seasonal_3 Loss: 0.0214 | 0.0411
Epoch 119/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 120/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 121/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 122/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 123/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 124/300, seasonal_3 Loss: 0.0214 | 0.0410
Epoch 125/300, seasonal_3 Loss: 0.0213 | 0.0410
Epoch 126/300, seasonal_3 Loss: 0.0213 | 0.0410
Epoch 127/300, seasonal_3 Loss: 0.0213 | 0.0410
Epoch 128/300, seasonal_3 Loss: 0.0213 | 0.0410
Epoch 129/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 130/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 131/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 132/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 133/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 134/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 135/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 136/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 137/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 138/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 139/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 140/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 141/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 142/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 143/300, seasonal_3 Loss: 0.0213 | 0.0409
Epoch 144/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 145/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 146/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 147/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 148/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 149/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 150/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 151/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 152/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 153/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 154/300, seasonal_3 Loss: 0.0213 | 0.0408
Epoch 155/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 156/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 157/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 158/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 159/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 160/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 161/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 162/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 163/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 164/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 165/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 166/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 167/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 168/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 169/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 170/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 171/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 172/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 173/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 174/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 175/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 176/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 177/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 178/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 179/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 180/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 181/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 182/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 183/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 184/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 185/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 186/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 187/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 188/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 189/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 190/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 191/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 192/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 193/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 194/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 195/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 196/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 197/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 198/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 199/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 200/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 201/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 202/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 203/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 204/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 205/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 206/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 207/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 208/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 209/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 210/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 211/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 212/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 213/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 214/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 215/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 216/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 217/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 218/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 219/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 220/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 221/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 222/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 223/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 224/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 225/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 226/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 227/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 228/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 229/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 230/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 231/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 232/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 233/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 234/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 235/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 236/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 237/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 238/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 239/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 240/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 241/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 242/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 243/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 244/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 245/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 246/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 247/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 248/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 249/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 250/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 251/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 252/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 253/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 254/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 255/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 256/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 257/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 258/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 259/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 260/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 261/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 262/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 263/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 264/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 265/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 266/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 267/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 268/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 269/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 270/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 271/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 272/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 273/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 274/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 275/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 276/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 277/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 278/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 279/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 280/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 281/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 282/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 283/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 284/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 285/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 286/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 287/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 288/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 289/300, seasonal_3 Loss: 0.0212 | 0.0408
Epoch 290/300, seasonal_3 Loss: 0.0212 | 0.0408
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9887437529260938, 'learning_rate': 7.03924902616613e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7842891224561274}
Epoch 1/300, resid Loss: 0.3720 | 0.1963
Epoch 2/300, resid Loss: 0.1218 | 0.1092
Epoch 3/300, resid Loss: 0.0948 | 0.0887
Epoch 4/300, resid Loss: 0.0847 | 0.0800
Epoch 5/300, resid Loss: 0.0783 | 0.0743
Epoch 6/300, resid Loss: 0.0741 | 0.0713
Epoch 7/300, resid Loss: 0.0706 | 0.0691
Epoch 8/300, resid Loss: 0.0674 | 0.0672
Epoch 9/300, resid Loss: 0.0640 | 0.0648
Epoch 10/300, resid Loss: 0.0613 | 0.0633
Epoch 11/300, resid Loss: 0.0588 | 0.0617
Epoch 12/300, resid Loss: 0.0568 | 0.0601
Epoch 13/300, resid Loss: 0.0550 | 0.0588
Epoch 14/300, resid Loss: 0.0538 | 0.0578
Epoch 15/300, resid Loss: 0.0528 | 0.0568
Epoch 16/300, resid Loss: 0.0519 | 0.0557
Epoch 17/300, resid Loss: 0.0508 | 0.0545
Epoch 18/300, resid Loss: 0.0501 | 0.0539
Epoch 19/300, resid Loss: 0.0495 | 0.0534
Epoch 20/300, resid Loss: 0.0489 | 0.0528
Epoch 21/300, resid Loss: 0.0482 | 0.0518
Epoch 22/300, resid Loss: 0.0477 | 0.0514
Epoch 23/300, resid Loss: 0.0473 | 0.0510
Epoch 24/300, resid Loss: 0.0469 | 0.0507
Epoch 25/300, resid Loss: 0.0463 | 0.0500
Epoch 26/300, resid Loss: 0.0460 | 0.0498
Epoch 27/300, resid Loss: 0.0457 | 0.0495
Epoch 28/300, resid Loss: 0.0454 | 0.0493
Epoch 29/300, resid Loss: 0.0450 | 0.0489
Epoch 30/300, resid Loss: 0.0448 | 0.0487
Epoch 31/300, resid Loss: 0.0445 | 0.0485
Epoch 32/300, resid Loss: 0.0443 | 0.0484
Epoch 33/300, resid Loss: 0.0440 | 0.0482
Epoch 34/300, resid Loss: 0.0438 | 0.0480
Epoch 35/300, resid Loss: 0.0436 | 0.0479
Epoch 36/300, resid Loss: 0.0435 | 0.0478
Epoch 37/300, resid Loss: 0.0432 | 0.0476
Epoch 38/300, resid Loss: 0.0431 | 0.0475
Epoch 39/300, resid Loss: 0.0429 | 0.0474
Epoch 40/300, resid Loss: 0.0428 | 0.0473
Epoch 41/300, resid Loss: 0.0426 | 0.0472
Epoch 42/300, resid Loss: 0.0425 | 0.0471
Epoch 43/300, resid Loss: 0.0424 | 0.0470
Epoch 44/300, resid Loss: 0.0423 | 0.0470
Epoch 45/300, resid Loss: 0.0421 | 0.0468
Epoch 46/300, resid Loss: 0.0420 | 0.0468
Epoch 47/300, resid Loss: 0.0420 | 0.0467
Epoch 48/300, resid Loss: 0.0419 | 0.0466
Epoch 49/300, resid Loss: 0.0418 | 0.0466
Epoch 50/300, resid Loss: 0.0417 | 0.0465
Epoch 51/300, resid Loss: 0.0416 | 0.0464
Epoch 52/300, resid Loss: 0.0415 | 0.0464
Epoch 53/300, resid Loss: 0.0414 | 0.0464
Epoch 54/300, resid Loss: 0.0414 | 0.0463
Epoch 55/300, resid Loss: 0.0413 | 0.0463
Epoch 56/300, resid Loss: 0.0412 | 0.0462
Epoch 57/300, resid Loss: 0.0412 | 0.0462
Epoch 58/300, resid Loss: 0.0411 | 0.0462
Epoch 59/300, resid Loss: 0.0411 | 0.0461
Epoch 60/300, resid Loss: 0.0410 | 0.0461
Epoch 61/300, resid Loss: 0.0410 | 0.0460
Epoch 62/300, resid Loss: 0.0409 | 0.0460
Epoch 63/300, resid Loss: 0.0409 | 0.0460
Epoch 64/300, resid Loss: 0.0408 | 0.0460
Epoch 65/300, resid Loss: 0.0408 | 0.0459
Epoch 66/300, resid Loss: 0.0408 | 0.0459
Epoch 67/300, resid Loss: 0.0407 | 0.0459
Epoch 68/300, resid Loss: 0.0407 | 0.0458
Epoch 69/300, resid Loss: 0.0407 | 0.0458
Epoch 70/300, resid Loss: 0.0407 | 0.0458
Epoch 71/300, resid Loss: 0.0406 | 0.0458
Epoch 72/300, resid Loss: 0.0406 | 0.0458
Epoch 73/300, resid Loss: 0.0406 | 0.0458
Epoch 74/300, resid Loss: 0.0406 | 0.0458
Epoch 75/300, resid Loss: 0.0405 | 0.0458
Epoch 76/300, resid Loss: 0.0405 | 0.0457
Epoch 77/300, resid Loss: 0.0405 | 0.0458
Epoch 78/300, resid Loss: 0.0405 | 0.0457
Epoch 79/300, resid Loss: 0.0405 | 0.0457
Epoch 80/300, resid Loss: 0.0404 | 0.0457
Epoch 81/300, resid Loss: 0.0404 | 0.0457
Epoch 82/300, resid Loss: 0.0404 | 0.0457
Epoch 83/300, resid Loss: 0.0404 | 0.0457
Epoch 84/300, resid Loss: 0.0404 | 0.0457
Epoch 85/300, resid Loss: 0.0404 | 0.0457
Epoch 86/300, resid Loss: 0.0404 | 0.0457
Epoch 87/300, resid Loss: 0.0404 | 0.0457
Epoch 88/300, resid Loss: 0.0403 | 0.0457
Epoch 89/300, resid Loss: 0.0403 | 0.0457
Epoch 90/300, resid Loss: 0.0403 | 0.0457
Epoch 91/300, resid Loss: 0.0403 | 0.0457
Epoch 92/300, resid Loss: 0.0403 | 0.0457
Epoch 93/300, resid Loss: 0.0403 | 0.0457
Epoch 94/300, resid Loss: 0.0403 | 0.0457
Epoch 95/300, resid Loss: 0.0403 | 0.0457
Epoch 96/300, resid Loss: 0.0403 | 0.0457
Epoch 97/300, resid Loss: 0.0403 | 0.0457
Epoch 98/300, resid Loss: 0.0403 | 0.0457
Epoch 99/300, resid Loss: 0.0403 | 0.0457
Epoch 100/300, resid Loss: 0.0402 | 0.0457
Epoch 101/300, resid Loss: 0.0402 | 0.0457
Epoch 102/300, resid Loss: 0.0402 | 0.0457
Epoch 103/300, resid Loss: 0.0402 | 0.0457
Epoch 104/300, resid Loss: 0.0402 | 0.0457
Epoch 105/300, resid Loss: 0.0402 | 0.0457
Epoch 106/300, resid Loss: 0.0402 | 0.0457
Epoch 107/300, resid Loss: 0.0402 | 0.0457
Epoch 108/300, resid Loss: 0.0402 | 0.0457
Epoch 109/300, resid Loss: 0.0402 | 0.0457
Epoch 110/300, resid Loss: 0.0402 | 0.0457
Epoch 111/300, resid Loss: 0.0402 | 0.0457
Epoch 112/300, resid Loss: 0.0402 | 0.0457
Epoch 113/300, resid Loss: 0.0402 | 0.0457
Epoch 114/300, resid Loss: 0.0402 | 0.0457
Epoch 115/300, resid Loss: 0.0402 | 0.0457
Epoch 116/300, resid Loss: 0.0402 | 0.0457
Epoch 117/300, resid Loss: 0.0402 | 0.0457
Epoch 118/300, resid Loss: 0.0402 | 0.0457
Epoch 119/300, resid Loss: 0.0402 | 0.0457
Epoch 120/300, resid Loss: 0.0402 | 0.0457
Epoch 121/300, resid Loss: 0.0402 | 0.0457
Epoch 122/300, resid Loss: 0.0402 | 0.0457
Epoch 123/300, resid Loss: 0.0402 | 0.0457
Epoch 124/300, resid Loss: 0.0402 | 0.0457
Epoch 125/300, resid Loss: 0.0402 | 0.0457
Epoch 126/300, resid Loss: 0.0402 | 0.0457
Epoch 127/300, resid Loss: 0.0402 | 0.0457
Epoch 128/300, resid Loss: 0.0402 | 0.0457
Epoch 129/300, resid Loss: 0.0402 | 0.0457
Epoch 130/300, resid Loss: 0.0402 | 0.0457
Epoch 131/300, resid Loss: 0.0402 | 0.0457
Epoch 132/300, resid Loss: 0.0402 | 0.0457
Epoch 133/300, resid Loss: 0.0402 | 0.0457
Epoch 134/300, resid Loss: 0.0402 | 0.0457
Epoch 135/300, resid Loss: 0.0402 | 0.0457
Epoch 136/300, resid Loss: 0.0402 | 0.0457
Epoch 137/300, resid Loss: 0.0402 | 0.0457
Epoch 138/300, resid Loss: 0.0402 | 0.0457
Epoch 139/300, resid Loss: 0.0402 | 0.0457
Epoch 140/300, resid Loss: 0.0402 | 0.0457
Epoch 141/300, resid Loss: 0.0402 | 0.0457
Epoch 142/300, resid Loss: 0.0402 | 0.0457
Epoch 143/300, resid Loss: 0.0402 | 0.0457
Epoch 144/300, resid Loss: 0.0402 | 0.0457
Epoch 145/300, resid Loss: 0.0402 | 0.0457
Epoch 146/300, resid Loss: 0.0402 | 0.0457
Epoch 147/300, resid Loss: 0.0402 | 0.0457
Epoch 148/300, resid Loss: 0.0402 | 0.0457
Epoch 149/300, resid Loss: 0.0402 | 0.0457
Epoch 150/300, resid Loss: 0.0402 | 0.0457
Epoch 151/300, resid Loss: 0.0402 | 0.0457
Epoch 152/300, resid Loss: 0.0402 | 0.0457
Epoch 153/300, resid Loss: 0.0402 | 0.0457
Epoch 154/300, resid Loss: 0.0402 | 0.0457
Epoch 155/300, resid Loss: 0.0402 | 0.0457
Epoch 156/300, resid Loss: 0.0402 | 0.0457
Epoch 157/300, resid Loss: 0.0402 | 0.0457
Epoch 158/300, resid Loss: 0.0402 | 0.0457
Epoch 159/300, resid Loss: 0.0402 | 0.0457
Epoch 160/300, resid Loss: 0.0402 | 0.0457
Epoch 161/300, resid Loss: 0.0402 | 0.0457
Epoch 162/300, resid Loss: 0.0402 | 0.0457
Epoch 163/300, resid Loss: 0.0402 | 0.0457
Epoch 164/300, resid Loss: 0.0402 | 0.0457
Epoch 165/300, resid Loss: 0.0402 | 0.0457
Epoch 166/300, resid Loss: 0.0402 | 0.0457
Epoch 167/300, resid Loss: 0.0402 | 0.0457
Epoch 168/300, resid Loss: 0.0402 | 0.0457
Epoch 169/300, resid Loss: 0.0402 | 0.0457
Epoch 170/300, resid Loss: 0.0402 | 0.0457
Epoch 171/300, resid Loss: 0.0402 | 0.0457
Epoch 172/300, resid Loss: 0.0402 | 0.0457
Epoch 173/300, resid Loss: 0.0402 | 0.0457
Epoch 174/300, resid Loss: 0.0402 | 0.0457
Epoch 175/300, resid Loss: 0.0402 | 0.0457
Epoch 176/300, resid Loss: 0.0402 | 0.0457
Epoch 177/300, resid Loss: 0.0402 | 0.0457
Epoch 178/300, resid Loss: 0.0402 | 0.0457
Epoch 179/300, resid Loss: 0.0402 | 0.0457
Epoch 180/300, resid Loss: 0.0402 | 0.0457
Epoch 181/300, resid Loss: 0.0402 | 0.0457
Epoch 182/300, resid Loss: 0.0402 | 0.0457
Epoch 183/300, resid Loss: 0.0402 | 0.0457
Epoch 184/300, resid Loss: 0.0402 | 0.0457
Epoch 185/300, resid Loss: 0.0402 | 0.0457
Epoch 186/300, resid Loss: 0.0402 | 0.0457
Epoch 187/300, resid Loss: 0.0402 | 0.0457
Epoch 188/300, resid Loss: 0.0402 | 0.0457
Epoch 189/300, resid Loss: 0.0402 | 0.0457
Epoch 190/300, resid Loss: 0.0402 | 0.0457
Epoch 191/300, resid Loss: 0.0402 | 0.0457
Epoch 192/300, resid Loss: 0.0402 | 0.0457
Epoch 193/300, resid Loss: 0.0402 | 0.0457
Epoch 194/300, resid Loss: 0.0402 | 0.0457
Epoch 195/300, resid Loss: 0.0402 | 0.0457
Epoch 196/300, resid Loss: 0.0402 | 0.0457
Epoch 197/300, resid Loss: 0.0402 | 0.0457
Epoch 198/300, resid Loss: 0.0402 | 0.0457
Epoch 199/300, resid Loss: 0.0402 | 0.0457
Epoch 200/300, resid Loss: 0.0402 | 0.0457
Epoch 201/300, resid Loss: 0.0402 | 0.0457
Epoch 202/300, resid Loss: 0.0402 | 0.0457
Epoch 203/300, resid Loss: 0.0402 | 0.0457
Epoch 204/300, resid Loss: 0.0402 | 0.0457
Epoch 205/300, resid Loss: 0.0402 | 0.0457
Epoch 206/300, resid Loss: 0.0402 | 0.0457
Epoch 207/300, resid Loss: 0.0402 | 0.0457
Epoch 208/300, resid Loss: 0.0402 | 0.0457
Epoch 209/300, resid Loss: 0.0402 | 0.0457
Epoch 210/300, resid Loss: 0.0402 | 0.0457
Epoch 211/300, resid Loss: 0.0402 | 0.0457
Epoch 212/300, resid Loss: 0.0402 | 0.0457
Early stopping for resid
Runtime (seconds): 1757.916249513626
0.0008605679610728868
[116.3575]
[2.9762764]
[0.15940271]
[-0.3443232]
[-0.00270706]
[0.32099986]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 5.395681914174929
RMSE: 2.3228607177734375
MAE: 2.3228607177734375
R-squared: nan
[119.46714]
