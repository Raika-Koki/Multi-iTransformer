ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 14:12:01,409][0m A new study created in memory with name: no-name-737bff0d-af8b-4868-b399-6eb47caed004[0m
[32m[I 2025-01-06 14:12:27,489][0m Trial 0 finished with value: 2.531571388244629 and parameters: {'observation_period_num': 101, 'train_rates': 0.9020599524202217, 'learning_rate': 1.8374304235515086e-06, 'batch_size': 250, 'step_size': 14, 'gamma': 0.8218675232323811}. Best is trial 0 with value: 2.531571388244629.[0m
[32m[I 2025-01-06 14:12:57,726][0m Trial 1 finished with value: 0.17226178844769796 and parameters: {'observation_period_num': 84, 'train_rates': 0.8537843597654529, 'learning_rate': 0.00010304001138131628, 'batch_size': 197, 'step_size': 6, 'gamma': 0.8016662477322644}. Best is trial 1 with value: 0.17226178844769796.[0m
[32m[I 2025-01-06 14:17:44,966][0m Trial 2 finished with value: 0.09660537767384021 and parameters: {'observation_period_num': 202, 'train_rates': 0.9176138980143891, 'learning_rate': 3.909858203141391e-05, 'batch_size': 18, 'step_size': 2, 'gamma': 0.931600786494129}. Best is trial 2 with value: 0.09660537767384021.[0m
Early stopping at epoch 65
[32m[I 2025-01-06 14:18:21,817][0m Trial 3 finished with value: 0.3460267677523259 and parameters: {'observation_period_num': 126, 'train_rates': 0.8751748862625207, 'learning_rate': 0.0001560003915564344, 'batch_size': 102, 'step_size': 1, 'gamma': 0.81474420469232}. Best is trial 2 with value: 0.09660537767384021.[0m
[32m[I 2025-01-06 14:19:16,495][0m Trial 4 finished with value: 0.2280630186049625 and parameters: {'observation_period_num': 54, 'train_rates': 0.6584779829241234, 'learning_rate': 0.0001528094837320347, 'batch_size': 86, 'step_size': 1, 'gamma': 0.9639563078099569}. Best is trial 2 with value: 0.09660537767384021.[0m
[32m[I 2025-01-06 14:19:58,942][0m Trial 5 finished with value: 0.5286261013435452 and parameters: {'observation_period_num': 87, 'train_rates': 0.894342866786322, 'learning_rate': 9.848721186613613e-06, 'batch_size': 137, 'step_size': 13, 'gamma': 0.7868117880397568}. Best is trial 2 with value: 0.09660537767384021.[0m
[32m[I 2025-01-06 14:20:23,827][0m Trial 6 finished with value: 0.19469167980863605 and parameters: {'observation_period_num': 16, 'train_rates': 0.736424023375392, 'learning_rate': 0.00015648082213997268, 'batch_size': 235, 'step_size': 13, 'gamma': 0.8380860977813152}. Best is trial 2 with value: 0.09660537767384021.[0m
[32m[I 2025-01-06 14:26:17,250][0m Trial 7 finished with value: 0.07246512399306373 and parameters: {'observation_period_num': 68, 'train_rates': 0.97846152754912, 'learning_rate': 4.973736524035018e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9326675212268787}. Best is trial 7 with value: 0.07246512399306373.[0m
[32m[I 2025-01-06 14:26:40,795][0m Trial 8 finished with value: 1.005029953102197 and parameters: {'observation_period_num': 160, 'train_rates': 0.6144921938809151, 'learning_rate': 1.4870699087728731e-06, 'batch_size': 204, 'step_size': 15, 'gamma': 0.8458226754190632}. Best is trial 7 with value: 0.07246512399306373.[0m
[32m[I 2025-01-06 14:27:18,263][0m Trial 9 finished with value: 1.117059505341911 and parameters: {'observation_period_num': 242, 'train_rates': 0.7406437843888141, 'learning_rate': 6.941949809190095e-06, 'batch_size': 129, 'step_size': 3, 'gamma': 0.8545932416358981}. Best is trial 7 with value: 0.07246512399306373.[0m
[32m[I 2025-01-06 14:31:24,087][0m Trial 10 finished with value: 0.03145431997437103 and parameters: {'observation_period_num': 15, 'train_rates': 0.9827880633794893, 'learning_rate': 0.0005959111900686167, 'batch_size': 23, 'step_size': 10, 'gamma': 0.9059704860871896}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:36:48,565][0m Trial 11 finished with value: 0.04013438358209854 and parameters: {'observation_period_num': 7, 'train_rates': 0.9856763457378555, 'learning_rate': 0.0009831599672026144, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9100834269246971}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:38:36,316][0m Trial 12 finished with value: 0.04121706634759903 and parameters: {'observation_period_num': 5, 'train_rates': 0.9896696989965346, 'learning_rate': 0.0009309207849800891, 'batch_size': 57, 'step_size': 10, 'gamma': 0.8960003620071316}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:40:03,570][0m Trial 13 finished with value: 0.05522271788454261 and parameters: {'observation_period_num': 33, 'train_rates': 0.8014530291022592, 'learning_rate': 0.0009034710980197526, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8935028237926832}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:41:56,915][0m Trial 14 finished with value: 0.0475605068765334 and parameters: {'observation_period_num': 40, 'train_rates': 0.948293880728636, 'learning_rate': 0.00039601678105610777, 'batch_size': 51, 'step_size': 11, 'gamma': 0.7522589222010836}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:44:08,350][0m Trial 15 finished with value: 0.09050443510472517 and parameters: {'observation_period_num': 154, 'train_rates': 0.8351141452594466, 'learning_rate': 0.0004188451151988593, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9882375367310714}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:45:17,390][0m Trial 16 finished with value: 0.03702240186658773 and parameters: {'observation_period_num': 9, 'train_rates': 0.9439608481502029, 'learning_rate': 0.0004141484079104514, 'batch_size': 86, 'step_size': 7, 'gamma': 0.9032388432829481}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:46:16,843][0m Trial 17 finished with value: 0.05051702668424696 and parameters: {'observation_period_num': 38, 'train_rates': 0.9341665589684705, 'learning_rate': 0.0004048039844346897, 'batch_size': 98, 'step_size': 7, 'gamma': 0.878733681836131}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:46:55,197][0m Trial 18 finished with value: 0.7145871711349904 and parameters: {'observation_period_num': 108, 'train_rates': 0.7585691524245618, 'learning_rate': 1.621126352688597e-05, 'batch_size': 143, 'step_size': 4, 'gamma': 0.9297736609540694}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:48:09,630][0m Trial 19 finished with value: 0.06878866804524875 and parameters: {'observation_period_num': 58, 'train_rates': 0.9462034682032698, 'learning_rate': 8.222455549445022e-05, 'batch_size': 79, 'step_size': 8, 'gamma': 0.9585112229160508}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:48:43,270][0m Trial 20 finished with value: 0.11146050211391412 and parameters: {'observation_period_num': 159, 'train_rates': 0.8169536612434194, 'learning_rate': 0.0003495316965980227, 'batch_size': 167, 'step_size': 5, 'gamma': 0.8702856079958589}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:51:41,187][0m Trial 21 finished with value: 0.03367789564654231 and parameters: {'observation_period_num': 5, 'train_rates': 0.986422828504854, 'learning_rate': 0.0008468790427214205, 'batch_size': 34, 'step_size': 11, 'gamma': 0.9070587823887425}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:54:10,211][0m Trial 22 finished with value: 0.06437172262190745 and parameters: {'observation_period_num': 25, 'train_rates': 0.9605041042407333, 'learning_rate': 0.0002622603562472387, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9115450453280752}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:55:28,507][0m Trial 23 finished with value: 0.0517236837082439 and parameters: {'observation_period_num': 52, 'train_rates': 0.9226493092971668, 'learning_rate': 0.0005525435237775126, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8861618911144552}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:56:21,058][0m Trial 24 finished with value: 0.04342852835607594 and parameters: {'observation_period_num': 24, 'train_rates': 0.8761606491276166, 'learning_rate': 0.0002560549606284348, 'batch_size': 109, 'step_size': 8, 'gamma': 0.9224969595744512}. Best is trial 10 with value: 0.03145431997437103.[0m
[32m[I 2025-01-06 14:58:30,903][0m Trial 25 finished with value: 0.026704643187778338 and parameters: {'observation_period_num': 5, 'train_rates': 0.9597915636103903, 'learning_rate': 0.0006642964513775525, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8639121757908413}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:01:12,061][0m Trial 26 finished with value: 0.07130696624517441 and parameters: {'observation_period_num': 70, 'train_rates': 0.965057232920262, 'learning_rate': 0.0006864266448323939, 'batch_size': 36, 'step_size': 9, 'gamma': 0.8628716294062615}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:03:56,235][0m Trial 27 finished with value: 0.04862099885940552 and parameters: {'observation_period_num': 39, 'train_rates': 0.988178964576802, 'learning_rate': 0.00023234992027916987, 'batch_size': 37, 'step_size': 11, 'gamma': 0.9467588575880611}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:05:02,491][0m Trial 28 finished with value: 0.25850655038914416 and parameters: {'observation_period_num': 186, 'train_rates': 0.6972072017355551, 'learning_rate': 7.662579347554035e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8771487878515655}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:06:48,199][0m Trial 29 finished with value: 0.17094273979847247 and parameters: {'observation_period_num': 110, 'train_rates': 0.8858907562242553, 'learning_rate': 1.990600849739456e-05, 'batch_size': 50, 'step_size': 12, 'gamma': 0.8413860409117068}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:07:38,588][0m Trial 30 finished with value: 0.48793645050581985 and parameters: {'observation_period_num': 86, 'train_rates': 0.9116801837081342, 'learning_rate': 4.635098751414153e-06, 'batch_size': 114, 'step_size': 10, 'gamma': 0.8204502222539161}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:08:50,422][0m Trial 31 finished with value: 0.040599271233351726 and parameters: {'observation_period_num': 16, 'train_rates': 0.9461557015249635, 'learning_rate': 0.0005862829684638915, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9046408985602147}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:12:09,885][0m Trial 32 finished with value: 0.03243573221825916 and parameters: {'observation_period_num': 13, 'train_rates': 0.9555601199251802, 'learning_rate': 0.0006072229102726944, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9141199748257467}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:15:21,549][0m Trial 33 finished with value: 0.06766190258999362 and parameters: {'observation_period_num': 28, 'train_rates': 0.9105199591231443, 'learning_rate': 0.0006289969073690802, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9497167398032329}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:20:26,639][0m Trial 34 finished with value: 0.21300977881138142 and parameters: {'observation_period_num': 54, 'train_rates': 0.8430846859874803, 'learning_rate': 2.6762768930789156e-06, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9185287890488206}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:22:16,764][0m Trial 35 finished with value: 0.038537678671510596 and parameters: {'observation_period_num': 20, 'train_rates': 0.9679096517308259, 'learning_rate': 0.00019873380370575357, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9379248502124999}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:25:26,066][0m Trial 36 finished with value: 0.05094733666111198 and parameters: {'observation_period_num': 44, 'train_rates': 0.9256971807674853, 'learning_rate': 0.00011055494488033771, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8622888584132614}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:26:43,906][0m Trial 37 finished with value: 0.08626196180833852 and parameters: {'observation_period_num': 71, 'train_rates': 0.8702778481579626, 'learning_rate': 0.0006821301958989922, 'batch_size': 69, 'step_size': 5, 'gamma': 0.9766418074145169}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:29:50,083][0m Trial 38 finished with value: 0.10066287510097027 and parameters: {'observation_period_num': 250, 'train_rates': 0.9632106662823076, 'learning_rate': 0.0003003658208084155, 'batch_size': 29, 'step_size': 10, 'gamma': 0.800321714524044}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:30:18,283][0m Trial 39 finished with value: 0.07386446120596979 and parameters: {'observation_period_num': 5, 'train_rates': 0.89192515762519, 'learning_rate': 4.7708684010346334e-05, 'batch_size': 251, 'step_size': 13, 'gamma': 0.8890650532898412}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:30:53,906][0m Trial 40 finished with value: 0.10950902240796828 and parameters: {'observation_period_num': 129, 'train_rates': 0.8994820741448127, 'learning_rate': 0.00014829647455947122, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8323060460534286}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:32:57,995][0m Trial 41 finished with value: 0.030444342146317162 and parameters: {'observation_period_num': 16, 'train_rates': 0.9388044283656346, 'learning_rate': 0.0005074944380225202, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9015538223301273}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:35:01,916][0m Trial 42 finished with value: 0.035302811967475076 and parameters: {'observation_period_num': 21, 'train_rates': 0.9713331526792714, 'learning_rate': 0.0005234290097935769, 'batch_size': 48, 'step_size': 7, 'gamma': 0.920428208662717}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:38:36,381][0m Trial 43 finished with value: 0.035878686189363546 and parameters: {'observation_period_num': 15, 'train_rates': 0.9298093892240094, 'learning_rate': 0.0007777428170058557, 'batch_size': 26, 'step_size': 5, 'gamma': 0.8988942715188594}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:40:12,366][0m Trial 44 finished with value: 0.054668380063958466 and parameters: {'observation_period_num': 30, 'train_rates': 0.9562738849233887, 'learning_rate': 0.0001836926437430121, 'batch_size': 61, 'step_size': 9, 'gamma': 0.876518983474909}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:42:23,548][0m Trial 45 finished with value: 0.056496262550354004 and parameters: {'observation_period_num': 48, 'train_rates': 0.9884822972230645, 'learning_rate': 0.0009982520176970442, 'batch_size': 45, 'step_size': 8, 'gamma': 0.886356118013799}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:46:48,619][0m Trial 46 finished with value: 0.04974472544073774 and parameters: {'observation_period_num': 13, 'train_rates': 0.8576720914139916, 'learning_rate': 0.00047449468673711065, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8527466095843184}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:47:50,479][0m Trial 47 finished with value: 0.06185763975248692 and parameters: {'observation_period_num': 62, 'train_rates': 0.9352042952947789, 'learning_rate': 0.00031972158243339836, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9112682728069061}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:49:28,380][0m Trial 48 finished with value: 0.0453590918580691 and parameters: {'observation_period_num': 6, 'train_rates': 0.9745468528283688, 'learning_rate': 0.0007962462394599824, 'batch_size': 62, 'step_size': 10, 'gamma': 0.932802388552007}. Best is trial 25 with value: 0.026704643187778338.[0m
[32m[I 2025-01-06 15:49:50,294][0m Trial 49 finished with value: 0.4277186135394715 and parameters: {'observation_period_num': 211, 'train_rates': 0.6137012597667286, 'learning_rate': 0.0001345735938505, 'batch_size': 230, 'step_size': 2, 'gamma': 0.9432792974533467}. Best is trial 25 with value: 0.026704643187778338.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 15:49:50,305][0m A new study created in memory with name: no-name-57ced171-9401-42d4-91f3-8886e802d8ff[0m
[32m[I 2025-01-06 15:51:24,716][0m Trial 0 finished with value: 0.09627378046993286 and parameters: {'observation_period_num': 123, 'train_rates': 0.8687208571972791, 'learning_rate': 0.00016009078817113618, 'batch_size': 211, 'step_size': 12, 'gamma': 0.9682868123836601}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 15:52:41,707][0m Trial 1 finished with value: 0.1857351812397658 and parameters: {'observation_period_num': 45, 'train_rates': 0.7252547393824101, 'learning_rate': 0.00021781146600619145, 'batch_size': 203, 'step_size': 6, 'gamma': 0.7736637405942185}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 15:54:04,353][0m Trial 2 finished with value: 0.2810911488049296 and parameters: {'observation_period_num': 168, 'train_rates': 0.7388154648987876, 'learning_rate': 0.000141886905219445, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8699327555860271}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 15:58:20,990][0m Trial 3 finished with value: 0.20999953837692736 and parameters: {'observation_period_num': 118, 'train_rates': 0.6472678257198352, 'learning_rate': 0.0003567864828573113, 'batch_size': 28, 'step_size': 8, 'gamma': 0.7988297576295745}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 15:59:40,066][0m Trial 4 finished with value: 0.13747548941108917 and parameters: {'observation_period_num': 5, 'train_rates': 0.6336421800754282, 'learning_rate': 0.00013224761312468259, 'batch_size': 201, 'step_size': 8, 'gamma': 0.7620641427139928}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 16:01:18,279][0m Trial 5 finished with value: 0.21298065807652586 and parameters: {'observation_period_num': 75, 'train_rates': 0.7815003196670136, 'learning_rate': 0.0008687934085667237, 'batch_size': 130, 'step_size': 4, 'gamma': 0.8701510877419631}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 16:02:58,309][0m Trial 6 finished with value: 0.10362256652095751 and parameters: {'observation_period_num': 167, 'train_rates': 0.7965976738247887, 'learning_rate': 0.0002772449642549187, 'batch_size': 115, 'step_size': 12, 'gamma': 0.8810929871211629}. Best is trial 0 with value: 0.09627378046993286.[0m
[32m[I 2025-01-06 16:05:56,398][0m Trial 7 finished with value: 0.05215440549852937 and parameters: {'observation_period_num': 19, 'train_rates': 0.9180117859061319, 'learning_rate': 5.3165715953281664e-06, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9817652335910327}. Best is trial 7 with value: 0.05215440549852937.[0m
[32m[I 2025-01-06 16:07:36,761][0m Trial 8 finished with value: 0.07737990220387776 and parameters: {'observation_period_num': 98, 'train_rates': 0.945629006888629, 'learning_rate': 5.280164649347253e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.7926718073784917}. Best is trial 7 with value: 0.05215440549852937.[0m
[32m[I 2025-01-06 16:09:03,591][0m Trial 9 finished with value: 0.29762017929725215 and parameters: {'observation_period_num': 201, 'train_rates': 0.7344925086190699, 'learning_rate': 0.00017934534383279917, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9227869737552212}. Best is trial 7 with value: 0.05215440549852937.[0m
[32m[I 2025-01-06 16:17:18,122][0m Trial 10 finished with value: 0.10604562654214747 and parameters: {'observation_period_num': 247, 'train_rates': 0.9750805063120129, 'learning_rate': 2.5960263912780016e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9854939212646118}. Best is trial 7 with value: 0.05215440549852937.[0m
Early stopping at epoch 57
[32m[I 2025-01-06 16:18:36,861][0m Trial 11 finished with value: 0.26291796565055847 and parameters: {'observation_period_num': 70, 'train_rates': 0.9773121068986621, 'learning_rate': 9.492999313056646e-06, 'batch_size': 76, 'step_size': 1, 'gamma': 0.8297423657622575}. Best is trial 7 with value: 0.05215440549852937.[0m
[32m[I 2025-01-06 16:21:04,623][0m Trial 12 finished with value: 0.04760273405146308 and parameters: {'observation_period_num': 13, 'train_rates': 0.8884467282122659, 'learning_rate': 2.2379882696905715e-05, 'batch_size': 74, 'step_size': 15, 'gamma': 0.9309253430683597}. Best is trial 12 with value: 0.04760273405146308.[0m
[32m[I 2025-01-06 16:23:46,113][0m Trial 13 finished with value: 0.04154044630246478 and parameters: {'observation_period_num': 10, 'train_rates': 0.8868602382463512, 'learning_rate': 1.191680213199595e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.9417049386646806}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:25:47,356][0m Trial 14 finished with value: 0.05709559861300648 and parameters: {'observation_period_num': 41, 'train_rates': 0.8815231747596382, 'learning_rate': 2.0636212068655165e-05, 'batch_size': 82, 'step_size': 15, 'gamma': 0.9284150723858459}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:27:43,306][0m Trial 15 finished with value: 0.053631294202017836 and parameters: {'observation_period_num': 36, 'train_rates': 0.8469401451010665, 'learning_rate': 3.202893054938033e-05, 'batch_size': 88, 'step_size': 13, 'gamma': 0.9389738063982396}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:30:14,804][0m Trial 16 finished with value: 0.06626544882398677 and parameters: {'observation_period_num': 74, 'train_rates': 0.8258488150720529, 'learning_rate': 9.857137079145302e-06, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9045275013583841}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:31:47,248][0m Trial 17 finished with value: 0.1111401884240399 and parameters: {'observation_period_num': 10, 'train_rates': 0.9128467122785586, 'learning_rate': 1.678070055065961e-06, 'batch_size': 170, 'step_size': 14, 'gamma': 0.9554488084796265}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:33:35,987][0m Trial 18 finished with value: 0.055912852518318235 and parameters: {'observation_period_num': 64, 'train_rates': 0.9109138420681395, 'learning_rate': 3.760387779550941e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9089225432324218}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:36:23,013][0m Trial 19 finished with value: 0.07323895457497781 and parameters: {'observation_period_num': 99, 'train_rates': 0.8210875738725759, 'learning_rate': 1.0878444162812642e-05, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8427541622367802}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:38:06,816][0m Trial 20 finished with value: 0.0499051442144769 and parameters: {'observation_period_num': 31, 'train_rates': 0.8670195991089004, 'learning_rate': 7.34249508692472e-05, 'batch_size': 169, 'step_size': 5, 'gamma': 0.9512160363765847}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:39:52,841][0m Trial 21 finished with value: 0.05566304938893923 and parameters: {'observation_period_num': 29, 'train_rates': 0.8785480245784335, 'learning_rate': 7.33740660243563e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9555761781999382}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:42:01,015][0m Trial 22 finished with value: 0.06924820658597318 and parameters: {'observation_period_num': 49, 'train_rates': 0.937600373734212, 'learning_rate': 1.8765452482507657e-05, 'batch_size': 166, 'step_size': 3, 'gamma': 0.9473148664561701}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:43:46,683][0m Trial 23 finished with value: 0.08177585577642596 and parameters: {'observation_period_num': 6, 'train_rates': 0.8619182689809571, 'learning_rate': 3.6020073204573705e-06, 'batch_size': 167, 'step_size': 6, 'gamma': 0.9008003684093615}. Best is trial 13 with value: 0.04154044630246478.[0m
[32m[I 2025-01-06 16:48:10,110][0m Trial 24 finished with value: 0.03846107593399152 and parameters: {'observation_period_num': 30, 'train_rates': 0.893865866821741, 'learning_rate': 6.270732903724764e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9673405948440336}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 16:52:27,360][0m Trial 25 finished with value: 0.06547588645480573 and parameters: {'observation_period_num': 58, 'train_rates': 0.94486505819683, 'learning_rate': 1.7293237126390106e-05, 'batch_size': 37, 'step_size': 1, 'gamma': 0.9716854230455064}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 16:54:32,816][0m Trial 26 finished with value: 0.24076398895230405 and parameters: {'observation_period_num': 25, 'train_rates': 0.7651796632640872, 'learning_rate': 4.971320868130181e-06, 'batch_size': 68, 'step_size': 2, 'gamma': 0.9243173502034305}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 16:58:22,836][0m Trial 27 finished with value: 0.16024057217398469 and parameters: {'observation_period_num': 89, 'train_rates': 0.8289195952748358, 'learning_rate': 1.1838192465419998e-06, 'batch_size': 38, 'step_size': 10, 'gamma': 0.8923327680016895}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 17:00:38,300][0m Trial 28 finished with value: 0.08743333329374973 and parameters: {'observation_period_num': 149, 'train_rates': 0.8981082557682403, 'learning_rate': 6.672413437099716e-05, 'batch_size': 99, 'step_size': 14, 'gamma': 0.9685889765276592}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 17:09:06,782][0m Trial 29 finished with value: 0.12990554650291872 and parameters: {'observation_period_num': 125, 'train_rates': 0.8523651717825044, 'learning_rate': 2.3618136986308557e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9884782039368969}. Best is trial 24 with value: 0.03846107593399152.[0m
[32m[I 2025-01-06 17:13:16,879][0m Trial 30 finished with value: 0.03433865415198462 and parameters: {'observation_period_num': 20, 'train_rates': 0.98574645833138, 'learning_rate': 1.2612189687041193e-05, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9339254686918922}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:17:12,102][0m Trial 31 finished with value: 0.045073844089701366 and parameters: {'observation_period_num': 20, 'train_rates': 0.96225966433249, 'learning_rate': 1.266788875100345e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9344568606900066}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:20:57,501][0m Trial 32 finished with value: 0.07561615219822636 and parameters: {'observation_period_num': 51, 'train_rates': 0.9814340030485961, 'learning_rate': 7.119540760699792e-06, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9155247573643371}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:25:47,671][0m Trial 33 finished with value: 0.04115118134788351 and parameters: {'observation_period_num': 25, 'train_rates': 0.9411954609047303, 'learning_rate': 1.3155957826695449e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.9647348336168254}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:31:17,753][0m Trial 34 finished with value: 0.05343296162330257 and parameters: {'observation_period_num': 41, 'train_rates': 0.9323881158675331, 'learning_rate': 4.944330125511829e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.965367747563628}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:34:05,571][0m Trial 35 finished with value: 0.07231830688208107 and parameters: {'observation_period_num': 54, 'train_rates': 0.9561955388342772, 'learning_rate': 0.0001082938850587407, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9679247181406598}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:40:15,289][0m Trial 36 finished with value: 0.07594441385431723 and parameters: {'observation_period_num': 84, 'train_rates': 0.9846994773488175, 'learning_rate': 6.42537620140139e-06, 'batch_size': 26, 'step_size': 7, 'gamma': 0.9425410055589764}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:41:52,697][0m Trial 37 finished with value: 0.21337040218005654 and parameters: {'observation_period_num': 26, 'train_rates': 0.6868253999142844, 'learning_rate': 3.1311039223316223e-06, 'batch_size': 93, 'step_size': 11, 'gamma': 0.8514555250489587}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:44:17,029][0m Trial 38 finished with value: 0.07596758360098183 and parameters: {'observation_period_num': 107, 'train_rates': 0.9168130898346368, 'learning_rate': 1.2871298493367843e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9745695949080855}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:46:09,561][0m Trial 39 finished with value: 0.043721589093495695 and parameters: {'observation_period_num': 7, 'train_rates': 0.9620345918963, 'learning_rate': 0.0008597158834278268, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8825780141302545}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:55:27,162][0m Trial 40 finished with value: 0.08124076789961411 and parameters: {'observation_period_num': 142, 'train_rates': 0.925964442097196, 'learning_rate': 3.6765848559495915e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9590863949378767}. Best is trial 30 with value: 0.03433865415198462.[0m
[32m[I 2025-01-06 17:57:53,879][0m Trial 41 finished with value: 0.034148340619455174 and parameters: {'observation_period_num': 7, 'train_rates': 0.9570966630688225, 'learning_rate': 0.0006828972698269891, 'batch_size': 116, 'step_size': 9, 'gamma': 0.881701989769117}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:01:27,272][0m Trial 42 finished with value: 0.17205666681100806 and parameters: {'observation_period_num': 18, 'train_rates': 0.6040769165850889, 'learning_rate': 0.00035594468316095876, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8676417367559406}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:04:21,353][0m Trial 43 finished with value: 0.05272586293080274 and parameters: {'observation_period_num': 31, 'train_rates': 0.9013094007204698, 'learning_rate': 7.145430530200621e-06, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9445903205314943}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:06:47,519][0m Trial 44 finished with value: 0.05835367710978696 and parameters: {'observation_period_num': 42, 'train_rates': 0.9548192957606867, 'learning_rate': 0.0007943952931326514, 'batch_size': 118, 'step_size': 8, 'gamma': 0.9155379409644879}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:08:50,393][0m Trial 45 finished with value: 0.11888454854488373 and parameters: {'observation_period_num': 225, 'train_rates': 0.9893790695039871, 'learning_rate': 0.0006616127906518043, 'batch_size': 150, 'step_size': 5, 'gamma': 0.8157008752310446}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:12:17,208][0m Trial 46 finished with value: 0.03861676279417018 and parameters: {'observation_period_num': 17, 'train_rates': 0.9339835294349792, 'learning_rate': 0.0005717959402659206, 'batch_size': 49, 'step_size': 12, 'gamma': 0.7821212145751035}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:14:04,188][0m Trial 47 finished with value: 0.10377228260040283 and parameters: {'observation_period_num': 181, 'train_rates': 0.9662021051113671, 'learning_rate': 0.0005241656371029928, 'batch_size': 199, 'step_size': 11, 'gamma': 0.7699043396254274}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:20:20,649][0m Trial 48 finished with value: 0.03596980022217231 and parameters: {'observation_period_num': 18, 'train_rates': 0.9357391994943206, 'learning_rate': 0.0002574339741302211, 'batch_size': 26, 'step_size': 8, 'gamma': 0.7922196826128142}. Best is trial 41 with value: 0.034148340619455174.[0m
[32m[I 2025-01-06 18:23:47,776][0m Trial 49 finished with value: 0.06457462929681225 and parameters: {'observation_period_num': 64, 'train_rates': 0.9268833961600583, 'learning_rate': 0.00023447351612331698, 'batch_size': 49, 'step_size': 12, 'gamma': 0.7520661821560292}. Best is trial 41 with value: 0.034148340619455174.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 18:23:47,786][0m A new study created in memory with name: no-name-ca367a50-7902-49dc-962b-4d8931a78faf[0m
[32m[I 2025-01-06 18:25:25,243][0m Trial 0 finished with value: 0.24409418227703203 and parameters: {'observation_period_num': 146, 'train_rates': 0.6393038806932382, 'learning_rate': 0.0005428662102075509, 'batch_size': 233, 'step_size': 10, 'gamma': 0.8980064521907902}. Best is trial 0 with value: 0.24409418227703203.[0m
[32m[I 2025-01-06 18:26:59,410][0m Trial 1 finished with value: 0.065968968530451 and parameters: {'observation_period_num': 95, 'train_rates': 0.8453727763917643, 'learning_rate': 0.00017113152211106034, 'batch_size': 231, 'step_size': 13, 'gamma': 0.8033935987137553}. Best is trial 1 with value: 0.065968968530451.[0m
[32m[I 2025-01-06 18:33:36,674][0m Trial 2 finished with value: 0.4285618614127614 and parameters: {'observation_period_num': 224, 'train_rates': 0.6411652184563948, 'learning_rate': 1.0710210839472625e-06, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8837044543974522}. Best is trial 1 with value: 0.065968968530451.[0m
[32m[I 2025-01-06 18:35:37,550][0m Trial 3 finished with value: 0.17409380374380426 and parameters: {'observation_period_num': 71, 'train_rates': 0.683121721568758, 'learning_rate': 0.0001243984390162121, 'batch_size': 89, 'step_size': 12, 'gamma': 0.7539605120635663}. Best is trial 1 with value: 0.065968968530451.[0m
[32m[I 2025-01-06 18:37:20,834][0m Trial 4 finished with value: 0.335604533702696 and parameters: {'observation_period_num': 195, 'train_rates': 0.7110284614444148, 'learning_rate': 7.791717947037855e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8539082503397143}. Best is trial 1 with value: 0.065968968530451.[0m
Early stopping at epoch 61
[32m[I 2025-01-06 18:38:39,566][0m Trial 5 finished with value: 1.259904876402799 and parameters: {'observation_period_num': 241, 'train_rates': 0.6794690293404131, 'learning_rate': 2.479903827138863e-06, 'batch_size': 108, 'step_size': 1, 'gamma': 0.8183658657005914}. Best is trial 1 with value: 0.065968968530451.[0m
[32m[I 2025-01-06 18:41:02,866][0m Trial 6 finished with value: 0.04102468768158122 and parameters: {'observation_period_num': 26, 'train_rates': 0.8098014014494239, 'learning_rate': 0.00017880657355772876, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8182566697270969}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:43:31,988][0m Trial 7 finished with value: 0.07210410995916887 and parameters: {'observation_period_num': 166, 'train_rates': 0.8976088972317725, 'learning_rate': 6.657763062760088e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.8391494370329916}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:45:53,273][0m Trial 8 finished with value: 0.10444150091455387 and parameters: {'observation_period_num': 185, 'train_rates': 0.8869369748486378, 'learning_rate': 0.00012683173378227276, 'batch_size': 73, 'step_size': 14, 'gamma': 0.8113881874851094}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:49:59,741][0m Trial 9 finished with value: 0.10268946183033478 and parameters: {'observation_period_num': 142, 'train_rates': 0.8751835215502073, 'learning_rate': 0.0008899976855728714, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8810807756903789}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:52:02,054][0m Trial 10 finished with value: 0.057264044880867004 and parameters: {'observation_period_num': 24, 'train_rates': 0.9872772255697804, 'learning_rate': 1.6746712277061068e-05, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9426257601447903}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:54:07,469][0m Trial 11 finished with value: 0.05117727816104889 and parameters: {'observation_period_num': 7, 'train_rates': 0.9786777647865144, 'learning_rate': 1.6600381009277964e-05, 'batch_size': 173, 'step_size': 7, 'gamma': 0.9694705750324054}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:56:09,185][0m Trial 12 finished with value: 0.18822301102654151 and parameters: {'observation_period_num': 5, 'train_rates': 0.7738210668861947, 'learning_rate': 1.9421119957423602e-05, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9728216229412577}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 18:58:30,040][0m Trial 13 finished with value: 0.10022500902414322 and parameters: {'observation_period_num': 62, 'train_rates': 0.9711151137596049, 'learning_rate': 4.5244368567427096e-05, 'batch_size': 160, 'step_size': 10, 'gamma': 0.9269674438711581}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:00:38,134][0m Trial 14 finished with value: 0.13772334356780175 and parameters: {'observation_period_num': 42, 'train_rates': 0.7882655206163547, 'learning_rate': 6.947497719581222e-06, 'batch_size': 207, 'step_size': 5, 'gamma': 0.9893263867847994}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:03:02,021][0m Trial 15 finished with value: 0.06196686632931232 and parameters: {'observation_period_num': 100, 'train_rates': 0.929930202278741, 'learning_rate': 0.00030246861328372737, 'batch_size': 142, 'step_size': 9, 'gamma': 0.7754608624087281}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:04:47,615][0m Trial 16 finished with value: 0.051850000250207584 and parameters: {'observation_period_num': 11, 'train_rates': 0.8310275734887614, 'learning_rate': 3.0750269350616926e-05, 'batch_size': 200, 'step_size': 5, 'gamma': 0.917145500479513}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:06:30,545][0m Trial 17 finished with value: 0.25708498244282457 and parameters: {'observation_period_num': 49, 'train_rates': 0.7373075535772456, 'learning_rate': 8.29016534158639e-06, 'batch_size': 131, 'step_size': 2, 'gamma': 0.9512361378997594}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:09:26,567][0m Trial 18 finished with value: 0.0791343566090028 and parameters: {'observation_period_num': 95, 'train_rates': 0.9346791567614157, 'learning_rate': 6.953987654223291e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.851692412622798}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:11:36,518][0m Trial 19 finished with value: 0.0445134540517631 and parameters: {'observation_period_num': 35, 'train_rates': 0.8206915994354443, 'learning_rate': 0.0003013440488837715, 'batch_size': 91, 'step_size': 8, 'gamma': 0.7928503081169211}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:13:32,041][0m Trial 20 finished with value: 0.06977851330846935 and parameters: {'observation_period_num': 119, 'train_rates': 0.8369825839811662, 'learning_rate': 0.00030787464381634136, 'batch_size': 96, 'step_size': 8, 'gamma': 0.7836482520436988}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:15:12,623][0m Trial 21 finished with value: 0.17320680626716775 and parameters: {'observation_period_num': 29, 'train_rates': 0.7582413037546953, 'learning_rate': 0.00025855206514355835, 'batch_size': 142, 'step_size': 7, 'gamma': 0.8363038735742702}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:17:17,676][0m Trial 22 finished with value: 0.06713359440666633 and parameters: {'observation_period_num': 67, 'train_rates': 0.8107470992785162, 'learning_rate': 0.0005829088092944974, 'batch_size': 89, 'step_size': 9, 'gamma': 0.786225713154519}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:21:12,991][0m Trial 23 finished with value: 0.04675032926468074 and parameters: {'observation_period_num': 38, 'train_rates': 0.9326067982112702, 'learning_rate': 6.997141210997107e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.7504322852686113}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:24:48,056][0m Trial 24 finished with value: 0.05507934764611192 and parameters: {'observation_period_num': 40, 'train_rates': 0.8684935105340231, 'learning_rate': 0.00010686765524991328, 'batch_size': 46, 'step_size': 5, 'gamma': 0.7596172241784069}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:32:51,623][0m Trial 25 finished with value: 0.06747150896893193 and parameters: {'observation_period_num': 82, 'train_rates': 0.9165354102803481, 'learning_rate': 0.00024758159486258955, 'batch_size': 19, 'step_size': 3, 'gamma': 0.796043411014348}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:35:21,317][0m Trial 26 finished with value: 0.056139937837299925 and parameters: {'observation_period_num': 51, 'train_rates': 0.8065038498430812, 'learning_rate': 7.207280049519637e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7683951497305197}. Best is trial 6 with value: 0.04102468768158122.[0m
[32m[I 2025-01-06 19:39:58,001][0m Trial 27 finished with value: 0.03960947997524942 and parameters: {'observation_period_num': 28, 'train_rates': 0.948136658462941, 'learning_rate': 0.0005082980604011545, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8182604742885604}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 19:41:29,441][0m Trial 28 finished with value: 0.26334667002612894 and parameters: {'observation_period_num': 117, 'train_rates': 0.7439602492239143, 'learning_rate': 0.0006372417336329301, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8244808186872844}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 19:43:48,821][0m Trial 29 finished with value: 0.19740469941551783 and parameters: {'observation_period_num': 26, 'train_rates': 0.6293694803305089, 'learning_rate': 0.0004327173827497152, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8625250126777186}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 19:45:50,678][0m Trial 30 finished with value: 0.09632872162680876 and parameters: {'observation_period_num': 80, 'train_rates': 0.8546504993757909, 'learning_rate': 0.0009507545344614453, 'batch_size': 79, 'step_size': 11, 'gamma': 0.8338850178633863}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 19:50:49,347][0m Trial 31 finished with value: 0.04081241673196268 and parameters: {'observation_period_num': 27, 'train_rates': 0.9415736842903304, 'learning_rate': 0.00018180801957772377, 'batch_size': 32, 'step_size': 9, 'gamma': 0.7924237554781699}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 19:56:06,189][0m Trial 32 finished with value: 0.04227854177631714 and parameters: {'observation_period_num': 19, 'train_rates': 0.9541701935114298, 'learning_rate': 0.000180888174192933, 'batch_size': 31, 'step_size': 9, 'gamma': 0.801959015412144}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 20:01:11,203][0m Trial 33 finished with value: 0.09048242648260309 and parameters: {'observation_period_num': 57, 'train_rates': 0.9625451399590947, 'learning_rate': 0.00015440625856552638, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8042640189729661}. Best is trial 27 with value: 0.03960947997524942.[0m
[32m[I 2025-01-06 20:07:17,102][0m Trial 34 finished with value: 0.037903483465520874 and parameters: {'observation_period_num': 18, 'train_rates': 0.9511471991025027, 'learning_rate': 0.00017557017798392244, 'batch_size': 26, 'step_size': 11, 'gamma': 0.8101943309739283}. Best is trial 34 with value: 0.037903483465520874.[0m
[32m[I 2025-01-06 20:13:38,873][0m Trial 35 finished with value: 0.031553584989491734 and parameters: {'observation_period_num': 18, 'train_rates': 0.9042602902877797, 'learning_rate': 0.0004147506175565505, 'batch_size': 24, 'step_size': 11, 'gamma': 0.8239217881832612}. Best is trial 35 with value: 0.031553584989491734.[0m
[32m[I 2025-01-06 20:20:57,456][0m Trial 36 finished with value: 0.03081286745145917 and parameters: {'observation_period_num': 5, 'train_rates': 0.9091825794569509, 'learning_rate': 0.00047140202488809885, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8486716085303738}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:28:16,035][0m Trial 37 finished with value: 0.032272268229952224 and parameters: {'observation_period_num': 15, 'train_rates': 0.9072607549953039, 'learning_rate': 0.0004441708944534464, 'batch_size': 21, 'step_size': 12, 'gamma': 0.849543100051985}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:36:39,641][0m Trial 38 finished with value: 0.031796434127232605 and parameters: {'observation_period_num': 15, 'train_rates': 0.9076838512887744, 'learning_rate': 0.0003802654483854656, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8874267050063066}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:44:28,679][0m Trial 39 finished with value: 0.14970945298325183 and parameters: {'observation_period_num': 221, 'train_rates': 0.9055380933067664, 'learning_rate': 0.0004045855625740663, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8971777929337208}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:46:22,277][0m Trial 40 finished with value: 0.03467106245758981 and parameters: {'observation_period_num': 5, 'train_rates': 0.8573650916945199, 'learning_rate': 0.0007598316643275057, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8701756422918667}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:49:06,064][0m Trial 41 finished with value: 0.035054939007499314 and parameters: {'observation_period_num': 6, 'train_rates': 0.860076484309974, 'learning_rate': 0.0008217739263732198, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8755763481014559}. Best is trial 36 with value: 0.03081286745145917.[0m
[32m[I 2025-01-06 20:50:31,177][0m Trial 42 finished with value: 0.030041939200776996 and parameters: {'observation_period_num': 17, 'train_rates': 0.8877681061763335, 'learning_rate': 0.000674404552077101, 'batch_size': 254, 'step_size': 12, 'gamma': 0.8941227859745525}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 20:52:49,418][0m Trial 43 finished with value: 0.07037426685949541 and parameters: {'observation_period_num': 45, 'train_rates': 0.8905942834576736, 'learning_rate': 0.0004653013770659679, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8985817346013972}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:01:56,156][0m Trial 44 finished with value: 0.03652848605598722 and parameters: {'observation_period_num': 17, 'train_rates': 0.8807478541247057, 'learning_rate': 0.0009993478770201744, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8922190152337721}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:03:39,590][0m Trial 45 finished with value: 0.054255161570151325 and parameters: {'observation_period_num': 59, 'train_rates': 0.914798618583162, 'learning_rate': 0.00039709467050959204, 'batch_size': 229, 'step_size': 12, 'gamma': 0.852714165029413}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:07:00,089][0m Trial 46 finished with value: 0.036443184305737904 and parameters: {'observation_period_num': 14, 'train_rates': 0.9026528917736198, 'learning_rate': 0.0006484982864669661, 'batch_size': 45, 'step_size': 15, 'gamma': 0.912742688531584}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:08:50,017][0m Trial 47 finished with value: 0.0955251614915 and parameters: {'observation_period_num': 160, 'train_rates': 0.9195564776007049, 'learning_rate': 0.0001126560633456096, 'batch_size': 193, 'step_size': 13, 'gamma': 0.8621601390929668}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:10:42,116][0m Trial 48 finished with value: 0.22482109121654345 and parameters: {'observation_period_num': 76, 'train_rates': 0.6003229820252165, 'learning_rate': 0.00023008971131178535, 'batch_size': 65, 'step_size': 11, 'gamma': 0.8871603711886658}. Best is trial 42 with value: 0.030041939200776996.[0m
[32m[I 2025-01-06 21:16:47,208][0m Trial 49 finished with value: 0.06198512672344384 and parameters: {'observation_period_num': 35, 'train_rates': 0.8815093403289198, 'learning_rate': 1.8978446630122729e-06, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9069672594149009}. Best is trial 42 with value: 0.030041939200776996.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 21:16:47,218][0m A new study created in memory with name: no-name-02690c93-635c-47d6-90e1-545c9767c906[0m
[32m[I 2025-01-06 21:18:56,270][0m Trial 0 finished with value: 0.11753123304023042 and parameters: {'observation_period_num': 67, 'train_rates': 0.8210028947001207, 'learning_rate': 0.0006820767676079542, 'batch_size': 103, 'step_size': 13, 'gamma': 0.8962339604451223}. Best is trial 0 with value: 0.11753123304023042.[0m
[32m[I 2025-01-06 21:20:38,439][0m Trial 1 finished with value: 0.2475894310484987 and parameters: {'observation_period_num': 116, 'train_rates': 0.6430136936602707, 'learning_rate': 0.0008425988218951877, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8461114531304834}. Best is trial 0 with value: 0.11753123304023042.[0m
Early stopping at epoch 74
[32m[I 2025-01-06 21:21:55,156][0m Trial 2 finished with value: 0.15437152205218732 and parameters: {'observation_period_num': 179, 'train_rates': 0.8976174562156221, 'learning_rate': 9.249169141303665e-05, 'batch_size': 113, 'step_size': 1, 'gamma': 0.8368637225846924}. Best is trial 0 with value: 0.11753123304023042.[0m
[32m[I 2025-01-06 21:23:28,253][0m Trial 3 finished with value: 0.09306665994588612 and parameters: {'observation_period_num': 59, 'train_rates': 0.8932327976758166, 'learning_rate': 9.612985982869939e-06, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8630208295930191}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:24:57,639][0m Trial 4 finished with value: 0.3306884160408607 and parameters: {'observation_period_num': 214, 'train_rates': 0.8340907923634779, 'learning_rate': 1.1932768529779584e-06, 'batch_size': 147, 'step_size': 6, 'gamma': 0.939457194920838}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:26:22,536][0m Trial 5 finished with value: 0.37233102029445125 and parameters: {'observation_period_num': 66, 'train_rates': 0.7878693029878329, 'learning_rate': 1.0786786087268532e-06, 'batch_size': 232, 'step_size': 6, 'gamma': 0.9746105282207488}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:28:20,940][0m Trial 6 finished with value: 0.7507040500640869 and parameters: {'observation_period_num': 116, 'train_rates': 0.94958484262671, 'learning_rate': 1.0278561130656187e-06, 'batch_size': 214, 'step_size': 3, 'gamma': 0.9034001937581505}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:30:06,883][0m Trial 7 finished with value: 0.3232037054155679 and parameters: {'observation_period_num': 242, 'train_rates': 0.6989452798106665, 'learning_rate': 5.02974965574037e-05, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7662203091876483}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:35:49,418][0m Trial 8 finished with value: 0.11175884916023775 and parameters: {'observation_period_num': 236, 'train_rates': 0.988162201767983, 'learning_rate': 0.000189502018542874, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8579247812593243}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:37:32,276][0m Trial 9 finished with value: 0.17991083169855723 and parameters: {'observation_period_num': 213, 'train_rates': 0.7756495719273402, 'learning_rate': 1.8675462534520867e-05, 'batch_size': 96, 'step_size': 7, 'gamma': 0.9619513750015333}. Best is trial 3 with value: 0.09306665994588612.[0m
[32m[I 2025-01-06 21:39:09,193][0m Trial 10 finished with value: 0.05601225152793454 and parameters: {'observation_period_num': 6, 'train_rates': 0.888786376000419, 'learning_rate': 8.231919698104852e-06, 'batch_size': 172, 'step_size': 15, 'gamma': 0.8018419489651508}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:40:48,244][0m Trial 11 finished with value: 0.057738345713378526 and parameters: {'observation_period_num': 5, 'train_rates': 0.8941434301410766, 'learning_rate': 8.505442408384552e-06, 'batch_size': 178, 'step_size': 15, 'gamma': 0.7895042896443744}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:42:18,585][0m Trial 12 finished with value: 0.0820703098683485 and parameters: {'observation_period_num': 10, 'train_rates': 0.8857792180983177, 'learning_rate': 4.159231818989618e-06, 'batch_size': 173, 'step_size': 15, 'gamma': 0.778607120165854}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:43:54,553][0m Trial 13 finished with value: 0.07101579718497576 and parameters: {'observation_period_num': 9, 'train_rates': 0.9387142539273642, 'learning_rate': 5.3556316916664115e-06, 'batch_size': 171, 'step_size': 12, 'gamma': 0.8076993216410896}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:45:21,753][0m Trial 14 finished with value: 0.06517520645182195 and parameters: {'observation_period_num': 36, 'train_rates': 0.8569470385469944, 'learning_rate': 1.9835717927974694e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8055327434887942}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:46:58,641][0m Trial 15 finished with value: 0.36994660469776686 and parameters: {'observation_period_num': 91, 'train_rates': 0.7604764430062647, 'learning_rate': 4.040100247507619e-06, 'batch_size': 179, 'step_size': 12, 'gamma': 0.7507004165108799}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:49:40,836][0m Trial 16 finished with value: 0.09230434988789699 and parameters: {'observation_period_num': 158, 'train_rates': 0.9391787691295075, 'learning_rate': 1.016332288013065e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8063618445789922}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:51:25,220][0m Trial 17 finished with value: 0.16206106543540955 and parameters: {'observation_period_num': 34, 'train_rates': 0.9813282891109565, 'learning_rate': 2.6744484019690793e-06, 'batch_size': 139, 'step_size': 11, 'gamma': 0.7880420211744105}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:53:02,881][0m Trial 18 finished with value: 0.177733382137504 and parameters: {'observation_period_num': 38, 'train_rates': 0.7272472829023807, 'learning_rate': 4.6190008372463465e-05, 'batch_size': 187, 'step_size': 15, 'gamma': 0.8237116486450065}. Best is trial 10 with value: 0.05601225152793454.[0m
[32m[I 2025-01-06 21:54:38,607][0m Trial 19 finished with value: 0.05592468279144151 and parameters: {'observation_period_num': 5, 'train_rates': 0.8760180505788111, 'learning_rate': 1.0118560771199982e-05, 'batch_size': 152, 'step_size': 11, 'gamma': 0.788547877560489}. Best is trial 19 with value: 0.05592468279144151.[0m
[32m[I 2025-01-06 21:56:14,855][0m Trial 20 finished with value: 0.06478684895199623 and parameters: {'observation_period_num': 88, 'train_rates': 0.8529477592792475, 'learning_rate': 2.5886129517760987e-05, 'batch_size': 150, 'step_size': 10, 'gamma': 0.882219659200042}. Best is trial 19 with value: 0.05592468279144151.[0m
[32m[I 2025-01-06 21:58:00,682][0m Trial 21 finished with value: 0.05390722154437816 and parameters: {'observation_period_num': 5, 'train_rates': 0.9107366673617366, 'learning_rate': 1.0045692558014725e-05, 'batch_size': 129, 'step_size': 14, 'gamma': 0.7895107123738235}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 21:59:56,217][0m Trial 22 finished with value: 0.2300357482991777 and parameters: {'observation_period_num': 30, 'train_rates': 0.9242937319195305, 'learning_rate': 2.18157666952866e-06, 'batch_size': 125, 'step_size': 13, 'gamma': 0.756945567657676}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 22:01:50,275][0m Trial 23 finished with value: 0.06081058635926334 and parameters: {'observation_period_num': 52, 'train_rates': 0.8116418741593474, 'learning_rate': 1.2319721487329682e-05, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8254538705225472}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 22:03:21,944][0m Trial 24 finished with value: 0.08379891175258009 and parameters: {'observation_period_num': 21, 'train_rates': 0.8652381943904, 'learning_rate': 6.241005840866597e-06, 'batch_size': 145, 'step_size': 11, 'gamma': 0.7750490960729437}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 22:05:00,881][0m Trial 25 finished with value: 0.0795125886797905 and parameters: {'observation_period_num': 83, 'train_rates': 0.958679674849412, 'learning_rate': 4.148536445299718e-05, 'batch_size': 159, 'step_size': 14, 'gamma': 0.7973839144218068}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 22:06:52,071][0m Trial 26 finished with value: 0.22307907322399045 and parameters: {'observation_period_num': 151, 'train_rates': 0.9118210373085142, 'learning_rate': 2.537334677729222e-06, 'batch_size': 127, 'step_size': 12, 'gamma': 0.8170708040340408}. Best is trial 21 with value: 0.05390722154437816.[0m
[32m[I 2025-01-06 22:09:09,149][0m Trial 27 finished with value: 0.053736134626384144 and parameters: {'observation_period_num': 43, 'train_rates': 0.8697740274813257, 'learning_rate': 1.5436601954530742e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8380913491639693}. Best is trial 27 with value: 0.053736134626384144.[0m
[32m[I 2025-01-06 22:11:27,818][0m Trial 28 finished with value: 0.052183776520765744 and parameters: {'observation_period_num': 47, 'train_rates': 0.8435603892227459, 'learning_rate': 0.00012646336797165933, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8396048877744124}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:13:54,128][0m Trial 29 finished with value: 0.09296287464787621 and parameters: {'observation_period_num': 54, 'train_rates': 0.8230163751196367, 'learning_rate': 0.00022859619781342993, 'batch_size': 71, 'step_size': 9, 'gamma': 0.9093568863799194}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:18:21,961][0m Trial 30 finished with value: 0.07449299263210714 and parameters: {'observation_period_num': 79, 'train_rates': 0.8388089542161217, 'learning_rate': 0.00037941089831715276, 'batch_size': 33, 'step_size': 4, 'gamma': 0.8431551638850092}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:21:28,106][0m Trial 31 finished with value: 0.06084231437051584 and parameters: {'observation_period_num': 23, 'train_rates': 0.8729374056663847, 'learning_rate': 8.529512000360413e-05, 'batch_size': 52, 'step_size': 10, 'gamma': 0.8714567096541076}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:23:28,693][0m Trial 32 finished with value: 0.18459376437063832 and parameters: {'observation_period_num': 46, 'train_rates': 0.6049594592104064, 'learning_rate': 1.5703954951629358e-05, 'batch_size': 83, 'step_size': 10, 'gamma': 0.8350279181609579}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:25:29,796][0m Trial 33 finished with value: 0.07533737376063794 and parameters: {'observation_period_num': 105, 'train_rates': 0.8051526114359444, 'learning_rate': 3.369668438762743e-05, 'batch_size': 115, 'step_size': 8, 'gamma': 0.854202084014692}. Best is trial 28 with value: 0.052183776520765744.[0m
[32m[I 2025-01-06 22:28:07,003][0m Trial 34 finished with value: 0.03493492263555527 and parameters: {'observation_period_num': 25, 'train_rates': 0.914831907401213, 'learning_rate': 8.526414220783496e-05, 'batch_size': 99, 'step_size': 11, 'gamma': 0.8740884146418254}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:30:37,455][0m Trial 35 finished with value: 0.053485109423827226 and parameters: {'observation_period_num': 72, 'train_rates': 0.9176143104631026, 'learning_rate': 7.88593383107014e-05, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8810563397644675}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:33:01,392][0m Trial 36 finished with value: 0.0731201772620012 and parameters: {'observation_period_num': 69, 'train_rates': 0.9615924284762735, 'learning_rate': 0.00012468299570242105, 'batch_size': 92, 'step_size': 8, 'gamma': 0.886938135463077}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:35:36,614][0m Trial 37 finished with value: 0.05383833349077848 and parameters: {'observation_period_num': 60, 'train_rates': 0.8425032770665143, 'learning_rate': 7.198029268924171e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8724898010879684}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:38:00,979][0m Trial 38 finished with value: 0.10963599127199915 and parameters: {'observation_period_num': 136, 'train_rates': 0.9202678150336578, 'learning_rate': 0.0005443892064744582, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9099224933357193}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:46:09,360][0m Trial 39 finished with value: 0.23293369716786325 and parameters: {'observation_period_num': 101, 'train_rates': 0.7527419082411124, 'learning_rate': 0.00017433566263703312, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9251212215975048}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:47:52,721][0m Trial 40 finished with value: 0.16616778664997425 and parameters: {'observation_period_num': 67, 'train_rates': 0.6784085132066857, 'learning_rate': 6.535898052375859e-05, 'batch_size': 111, 'step_size': 9, 'gamma': 0.8522898297210363}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:50:47,325][0m Trial 41 finished with value: 0.056557486928222975 and parameters: {'observation_period_num': 60, 'train_rates': 0.8441172221601937, 'learning_rate': 8.577802497462867e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8718264771182884}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:54:07,775][0m Trial 42 finished with value: 0.0865218751054716 and parameters: {'observation_period_num': 45, 'train_rates': 0.835366386379222, 'learning_rate': 0.00012856934949146762, 'batch_size': 45, 'step_size': 10, 'gamma': 0.891718618743332}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:56:23,911][0m Trial 43 finished with value: 0.07609235558291556 and parameters: {'observation_period_num': 74, 'train_rates': 0.9015484321697463, 'learning_rate': 6.800576849210997e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.8774892543155535}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 22:58:43,009][0m Trial 44 finished with value: 0.17956904101432586 and parameters: {'observation_period_num': 26, 'train_rates': 0.7824060649328667, 'learning_rate': 0.0001259252192466869, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8645171687984858}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 23:02:10,794][0m Trial 45 finished with value: 0.06285873357863987 and parameters: {'observation_period_num': 57, 'train_rates': 0.8240704745492518, 'learning_rate': 0.0002133172277388005, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8373860700714515}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 23:04:04,364][0m Trial 46 finished with value: 0.09228269378064384 and parameters: {'observation_period_num': 117, 'train_rates': 0.7999285830834794, 'learning_rate': 0.00027761673216392146, 'batch_size': 100, 'step_size': 7, 'gamma': 0.9001087681757625}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 23:06:09,160][0m Trial 47 finished with value: 0.05909673548192965 and parameters: {'observation_period_num': 44, 'train_rates': 0.8697911122128328, 'learning_rate': 2.572696527207427e-05, 'batch_size': 79, 'step_size': 11, 'gamma': 0.9272240523006333}. Best is trial 34 with value: 0.03493492263555527.[0m
[32m[I 2025-01-06 23:08:22,320][0m Trial 48 finished with value: 0.031127101115100544 and parameters: {'observation_period_num': 17, 'train_rates': 0.8896248267181686, 'learning_rate': 6.491009699002938e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8627617187175196}. Best is trial 48 with value: 0.031127101115100544.[0m
[32m[I 2025-01-06 23:10:08,258][0m Trial 49 finished with value: 0.04071499736115919 and parameters: {'observation_period_num': 20, 'train_rates': 0.9390440682031806, 'learning_rate': 3.680917195698267e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8617299694343259}. Best is trial 48 with value: 0.031127101115100544.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-06 23:10:08,268][0m A new study created in memory with name: no-name-c6b8b9e7-c279-4430-9e5c-20d951042373[0m
[32m[I 2025-01-06 23:12:50,034][0m Trial 0 finished with value: 0.13828041788615983 and parameters: {'observation_period_num': 122, 'train_rates': 0.7957891891924129, 'learning_rate': 3.2920639954061876e-06, 'batch_size': 52, 'step_size': 11, 'gamma': 0.8400575993217757}. Best is trial 0 with value: 0.13828041788615983.[0m
[32m[I 2025-01-06 23:14:18,815][0m Trial 1 finished with value: 0.0894278734922409 and parameters: {'observation_period_num': 64, 'train_rates': 0.9377152823833815, 'learning_rate': 0.0002861187241469358, 'batch_size': 189, 'step_size': 10, 'gamma': 0.9007998904455401}. Best is trial 1 with value: 0.0894278734922409.[0m
[32m[I 2025-01-06 23:15:31,658][0m Trial 2 finished with value: 0.2806694234184853 and parameters: {'observation_period_num': 153, 'train_rates': 0.7252471806786731, 'learning_rate': 2.8064492931690506e-05, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8105201077126593}. Best is trial 1 with value: 0.0894278734922409.[0m
[32m[I 2025-01-06 23:16:49,605][0m Trial 3 finished with value: 0.6100011611559305 and parameters: {'observation_period_num': 205, 'train_rates': 0.6980449711445984, 'learning_rate': 1.379463798303908e-06, 'batch_size': 115, 'step_size': 6, 'gamma': 0.9460493004270298}. Best is trial 1 with value: 0.0894278734922409.[0m
[32m[I 2025-01-06 23:19:45,089][0m Trial 4 finished with value: 0.06417020581374865 and parameters: {'observation_period_num': 112, 'train_rates': 0.8361221784132901, 'learning_rate': 5.577716553383008e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.8116164210730754}. Best is trial 4 with value: 0.06417020581374865.[0m
[32m[I 2025-01-06 23:21:17,804][0m Trial 5 finished with value: 0.1766781508388485 and parameters: {'observation_period_num': 245, 'train_rates': 0.7940696047590043, 'learning_rate': 2.330140795071192e-05, 'batch_size': 238, 'step_size': 14, 'gamma': 0.8496131572679466}. Best is trial 4 with value: 0.06417020581374865.[0m
[32m[I 2025-01-06 23:23:02,578][0m Trial 6 finished with value: 0.640919887792256 and parameters: {'observation_period_num': 207, 'train_rates': 0.7422902147464929, 'learning_rate': 1.4588659735495004e-06, 'batch_size': 93, 'step_size': 9, 'gamma': 0.7597806172046102}. Best is trial 4 with value: 0.06417020581374865.[0m
[32m[I 2025-01-06 23:26:46,775][0m Trial 7 finished with value: 0.21675982154926376 and parameters: {'observation_period_num': 42, 'train_rates': 0.7513470546768439, 'learning_rate': 2.95147198790725e-06, 'batch_size': 37, 'step_size': 5, 'gamma': 0.961154995773954}. Best is trial 4 with value: 0.06417020581374865.[0m
[32m[I 2025-01-06 23:28:29,667][0m Trial 8 finished with value: 0.21975348649893778 and parameters: {'observation_period_num': 152, 'train_rates': 0.9157918229227358, 'learning_rate': 1.944950980279326e-06, 'batch_size': 139, 'step_size': 13, 'gamma': 0.9207584541964409}. Best is trial 4 with value: 0.06417020581374865.[0m
[32m[I 2025-01-06 23:30:19,130][0m Trial 9 finished with value: 0.04388083145022392 and parameters: {'observation_period_num': 59, 'train_rates': 0.9748990216841231, 'learning_rate': 0.0001804575160996961, 'batch_size': 94, 'step_size': 12, 'gamma': 0.7961801909178696}. Best is trial 9 with value: 0.04388083145022392.[0m
Early stopping at epoch 47
[32m[I 2025-01-06 23:31:07,771][0m Trial 10 finished with value: 0.06805480271577835 and parameters: {'observation_period_num': 9, 'train_rates': 0.9845485589476484, 'learning_rate': 0.0009654415415968665, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7532649023235821}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:32:44,571][0m Trial 11 finished with value: 0.30593882962478014 and parameters: {'observation_period_num': 93, 'train_rates': 0.6067765600301905, 'learning_rate': 0.000105265119682324, 'batch_size': 76, 'step_size': 2, 'gamma': 0.8178921565548327}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:40:03,546][0m Trial 12 finished with value: 0.0791778130862946 and parameters: {'observation_period_num': 93, 'train_rates': 0.8606106964136122, 'learning_rate': 5.44719940677675e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.7934604135571556}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:43:04,133][0m Trial 13 finished with value: 0.053185208581446354 and parameters: {'observation_period_num': 47, 'train_rates': 0.8681326984300874, 'learning_rate': 0.00018211473118736737, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8781686818561582}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:45:26,090][0m Trial 14 finished with value: 0.08234783595070144 and parameters: {'observation_period_num': 8, 'train_rates': 0.8974026732467498, 'learning_rate': 0.0002858593849051428, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8824915082467316}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:47:52,120][0m Trial 15 finished with value: 0.049069978296756744 and parameters: {'observation_period_num': 51, 'train_rates': 0.9770191335240727, 'learning_rate': 0.00019469431279640917, 'batch_size': 123, 'step_size': 8, 'gamma': 0.8611217990334822}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:50:16,393][0m Trial 16 finished with value: 0.09323157370090485 and parameters: {'observation_period_num': 69, 'train_rates': 0.9822003165242295, 'learning_rate': 0.0009855500907031416, 'batch_size': 147, 'step_size': 8, 'gamma': 0.853361625468469}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:52:26,672][0m Trial 17 finished with value: 0.09606108069419861 and parameters: {'observation_period_num': 36, 'train_rates': 0.9542735596745905, 'learning_rate': 1.0278242660193555e-05, 'batch_size': 175, 'step_size': 8, 'gamma': 0.7782082198051156}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:54:56,043][0m Trial 18 finished with value: 0.06719104494912768 and parameters: {'observation_period_num': 70, 'train_rates': 0.9024709539903497, 'learning_rate': 0.0004691911866779439, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9813776009499733}. Best is trial 9 with value: 0.04388083145022392.[0m
[32m[I 2025-01-06 23:57:07,717][0m Trial 19 finished with value: 0.043339907590832026 and parameters: {'observation_period_num': 23, 'train_rates': 0.9498351361470292, 'learning_rate': 0.00011654703685789656, 'batch_size': 114, 'step_size': 10, 'gamma': 0.9096900320932237}. Best is trial 19 with value: 0.043339907590832026.[0m
[32m[I 2025-01-06 23:58:41,295][0m Trial 20 finished with value: 0.18311059528675216 and parameters: {'observation_period_num': 26, 'train_rates': 0.6445676496566761, 'learning_rate': 1.1082054849546996e-05, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9119262385196546}. Best is trial 19 with value: 0.043339907590832026.[0m
[32m[I 2025-01-07 00:00:50,371][0m Trial 21 finished with value: 0.06001892035276118 and parameters: {'observation_period_num': 58, 'train_rates': 0.9511967235099466, 'learning_rate': 0.00011488505520695383, 'batch_size': 116, 'step_size': 9, 'gamma': 0.9257329558671105}. Best is trial 19 with value: 0.043339907590832026.[0m
[32m[I 2025-01-07 00:03:02,711][0m Trial 22 finished with value: 0.030204299837350845 and parameters: {'observation_period_num': 20, 'train_rates': 0.9803507984693185, 'learning_rate': 8.825352304488835e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8901025916688151}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:05:13,159][0m Trial 23 finished with value: 0.03715358152985573 and parameters: {'observation_period_num': 23, 'train_rates': 0.9386219009059948, 'learning_rate': 5.706163212370226e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.8903381960959991}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:07:09,596][0m Trial 24 finished with value: 0.040206983847462616 and parameters: {'observation_period_num': 22, 'train_rates': 0.921502898707406, 'learning_rate': 7.138049693221223e-05, 'batch_size': 168, 'step_size': 7, 'gamma': 0.8956976107418567}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:08:55,302][0m Trial 25 finished with value: 0.039413128160100126 and parameters: {'observation_period_num': 5, 'train_rates': 0.9209103585321512, 'learning_rate': 5.4836287761680446e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.8903994154803202}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:10:33,040][0m Trial 26 finished with value: 0.05280612379732266 and parameters: {'observation_period_num': 7, 'train_rates': 0.8797396526966959, 'learning_rate': 1.4977284068286181e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9383045954030247}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:12:12,631][0m Trial 27 finished with value: 0.07397407995393643 and parameters: {'observation_period_num': 82, 'train_rates': 0.8369659984910462, 'learning_rate': 4.374119224876751e-05, 'batch_size': 138, 'step_size': 3, 'gamma': 0.8810497962729603}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:13:47,353][0m Trial 28 finished with value: 0.05534537509083748 and parameters: {'observation_period_num': 28, 'train_rates': 0.9334885602398773, 'learning_rate': 1.947271351415178e-05, 'batch_size': 214, 'step_size': 7, 'gamma': 0.8893304709370513}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:16:06,872][0m Trial 29 finished with value: 0.11601203866636177 and parameters: {'observation_period_num': 120, 'train_rates': 0.8026080546702958, 'learning_rate': 7.602564346225071e-06, 'batch_size': 61, 'step_size': 10, 'gamma': 0.8304795613809618}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:17:50,379][0m Trial 30 finished with value: 0.047097612195868596 and parameters: {'observation_period_num': 5, 'train_rates': 0.8927061617524359, 'learning_rate': 3.7599280009247794e-05, 'batch_size': 182, 'step_size': 7, 'gamma': 0.867069377633675}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:19:22,184][0m Trial 31 finished with value: 0.04035811708030473 and parameters: {'observation_period_num': 26, 'train_rates': 0.9145193112816207, 'learning_rate': 6.980815534113769e-05, 'batch_size': 159, 'step_size': 7, 'gamma': 0.8909258856864959}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:20:51,844][0m Trial 32 finished with value: 0.03663602548883046 and parameters: {'observation_period_num': 21, 'train_rates': 0.9329331670435874, 'learning_rate': 8.003512695625347e-05, 'batch_size': 167, 'step_size': 9, 'gamma': 0.9015967080490311}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:22:26,758][0m Trial 33 finished with value: 0.060473401099443436 and parameters: {'observation_period_num': 41, 'train_rates': 0.9530302504969752, 'learning_rate': 3.3126175978074284e-05, 'batch_size': 196, 'step_size': 9, 'gamma': 0.9055052906710671}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:24:05,559][0m Trial 34 finished with value: 0.03651461377739906 and parameters: {'observation_period_num': 18, 'train_rates': 0.9386515425909242, 'learning_rate': 0.00010102118358475892, 'batch_size': 220, 'step_size': 11, 'gamma': 0.9359673213473418}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:25:49,351][0m Trial 35 finished with value: 0.08559955613610382 and parameters: {'observation_period_num': 146, 'train_rates': 0.8454547135167828, 'learning_rate': 0.00034724834023325306, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9527516131768899}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:27:27,842][0m Trial 36 finished with value: 0.07292983680963516 and parameters: {'observation_period_num': 79, 'train_rates': 0.9607588534547156, 'learning_rate': 9.093404809697847e-05, 'batch_size': 216, 'step_size': 11, 'gamma': 0.9289361696574261}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:28:49,928][0m Trial 37 finished with value: 0.12927639733206495 and parameters: {'observation_period_num': 188, 'train_rates': 0.7995786381543525, 'learning_rate': 0.00017106166046801355, 'batch_size': 235, 'step_size': 12, 'gamma': 0.9714746638836445}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:31:17,034][0m Trial 38 finished with value: 0.08799524226832012 and parameters: {'observation_period_num': 100, 'train_rates': 0.9340378424868238, 'learning_rate': 3.015664521227985e-05, 'batch_size': 75, 'step_size': 10, 'gamma': 0.9403482444599176}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:33:03,571][0m Trial 39 finished with value: 0.04178647674797546 and parameters: {'observation_period_num': 18, 'train_rates': 0.8856682520983757, 'learning_rate': 0.0004784737759950387, 'batch_size': 124, 'step_size': 9, 'gamma': 0.9189974874375537}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:35:05,878][0m Trial 40 finished with value: 0.04505085200071335 and parameters: {'observation_period_num': 39, 'train_rates': 0.988044283496028, 'learning_rate': 7.538539670559285e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8394648735502293}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:36:55,389][0m Trial 41 finished with value: 0.03848180987618186 and parameters: {'observation_period_num': 14, 'train_rates': 0.9291656316861728, 'learning_rate': 5.0991544852352146e-05, 'batch_size': 157, 'step_size': 9, 'gamma': 0.8977786754977841}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:38:49,281][0m Trial 42 finished with value: 0.06825118511915207 and parameters: {'observation_period_num': 36, 'train_rates': 0.9642496633559081, 'learning_rate': 0.00013519219575856533, 'batch_size': 151, 'step_size': 11, 'gamma': 0.902200580134304}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:40:32,376][0m Trial 43 finished with value: 0.06370212137699127 and parameters: {'observation_period_num': 50, 'train_rates': 0.9334997146954387, 'learning_rate': 4.536899751937186e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8712024432916088}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:42:24,704][0m Trial 44 finished with value: 0.0977270690491423 and parameters: {'observation_period_num': 249, 'train_rates': 0.9053794257289459, 'learning_rate': 2.6795426960617076e-05, 'batch_size': 131, 'step_size': 13, 'gamma': 0.9337668187067647}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:44:10,255][0m Trial 45 finished with value: 0.04275163454569389 and parameters: {'observation_period_num': 17, 'train_rates': 0.8179774924523451, 'learning_rate': 8.349185116957058e-05, 'batch_size': 189, 'step_size': 5, 'gamma': 0.9585443709064153}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:45:53,338][0m Trial 46 finished with value: 0.21549929096931364 and parameters: {'observation_period_num': 60, 'train_rates': 0.7732015262530236, 'learning_rate': 0.0002544669252673446, 'batch_size': 165, 'step_size': 12, 'gamma': 0.9181478143561653}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:48:13,413][0m Trial 47 finished with value: 0.04019294193300305 and parameters: {'observation_period_num': 33, 'train_rates': 0.8706804122417915, 'learning_rate': 5.748157413376263e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9003050573221997}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:50:17,941][0m Trial 48 finished with value: 0.14046195149421692 and parameters: {'observation_period_num': 183, 'train_rates': 0.9685758154148337, 'learning_rate': 2.0379382561183398e-05, 'batch_size': 148, 'step_size': 8, 'gamma': 0.8536918261259758}. Best is trial 22 with value: 0.030204299837350845.[0m
[32m[I 2025-01-07 00:52:40,757][0m Trial 49 finished with value: 0.03728554956614971 and parameters: {'observation_period_num': 16, 'train_rates': 0.9404975421956548, 'learning_rate': 0.0001498372287783756, 'batch_size': 85, 'step_size': 14, 'gamma': 0.87463602402768}. Best is trial 22 with value: 0.030204299837350845.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 00:52:40,767][0m A new study created in memory with name: no-name-63609522-b6fa-473a-860c-3c3c90ddb69a[0m
[32m[I 2025-01-07 00:54:20,455][0m Trial 0 finished with value: 0.19064220786094666 and parameters: {'observation_period_num': 13, 'train_rates': 0.9707612916234544, 'learning_rate': 1.9292510481663625e-06, 'batch_size': 186, 'step_size': 10, 'gamma': 0.88150522728373}. Best is trial 0 with value: 0.19064220786094666.[0m
[32m[I 2025-01-07 00:56:02,873][0m Trial 1 finished with value: 0.1743171827056402 and parameters: {'observation_period_num': 51, 'train_rates': 0.8585539506893294, 'learning_rate': 3.674517463127877e-05, 'batch_size': 202, 'step_size': 1, 'gamma': 0.8535767639779348}. Best is trial 1 with value: 0.1743171827056402.[0m
Early stopping at epoch 94
[32m[I 2025-01-07 00:57:54,679][0m Trial 2 finished with value: 0.15612477174784878 and parameters: {'observation_period_num': 124, 'train_rates': 0.8217026348326374, 'learning_rate': 9.95906037743545e-05, 'batch_size': 143, 'step_size': 2, 'gamma': 0.7593270083838501}. Best is trial 2 with value: 0.15612477174784878.[0m
[32m[I 2025-01-07 00:59:53,879][0m Trial 3 finished with value: 0.205870990951856 and parameters: {'observation_period_num': 52, 'train_rates': 0.7268297148059288, 'learning_rate': 3.661205535728711e-05, 'batch_size': 132, 'step_size': 1, 'gamma': 0.9708750930303105}. Best is trial 2 with value: 0.15612477174784878.[0m
[32m[I 2025-01-07 01:04:15,273][0m Trial 4 finished with value: 0.2439120708565627 and parameters: {'observation_period_num': 144, 'train_rates': 0.7008176644078683, 'learning_rate': 6.253913217800021e-05, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9255402475994967}. Best is trial 2 with value: 0.15612477174784878.[0m
[32m[I 2025-01-07 01:06:47,741][0m Trial 5 finished with value: 0.05703483149409294 and parameters: {'observation_period_num': 21, 'train_rates': 0.9657937525037354, 'learning_rate': 2.1558311224861118e-05, 'batch_size': 107, 'step_size': 7, 'gamma': 0.867591174759095}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:08:38,104][0m Trial 6 finished with value: 0.12620893848380915 and parameters: {'observation_period_num': 106, 'train_rates': 0.7900328730561519, 'learning_rate': 1.1764581497715346e-05, 'batch_size': 252, 'step_size': 15, 'gamma': 0.964591079581779}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:10:39,015][0m Trial 7 finished with value: 0.12940098345279694 and parameters: {'observation_period_num': 166, 'train_rates': 0.9415732323140026, 'learning_rate': 2.1601612150099842e-05, 'batch_size': 230, 'step_size': 10, 'gamma': 0.7832761380730537}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:12:21,476][0m Trial 8 finished with value: 0.4089327822618749 and parameters: {'observation_period_num': 99, 'train_rates': 0.7726721142885842, 'learning_rate': 1.1247787118569565e-05, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8280146877332613}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:14:37,218][0m Trial 9 finished with value: 0.3423289695925552 and parameters: {'observation_period_num': 144, 'train_rates': 0.6934272986496967, 'learning_rate': 4.037168736283341e-06, 'batch_size': 61, 'step_size': 11, 'gamma': 0.7881651290633255}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:16:25,604][0m Trial 10 finished with value: 0.2921772431106488 and parameters: {'observation_period_num': 214, 'train_rates': 0.6284100288188086, 'learning_rate': 0.0007470861418750107, 'batch_size': 93, 'step_size': 6, 'gamma': 0.8960161587132932}. Best is trial 5 with value: 0.05703483149409294.[0m
[32m[I 2025-01-07 01:18:58,298][0m Trial 11 finished with value: 0.05326354031761487 and parameters: {'observation_period_num': 6, 'train_rates': 0.8982803200244224, 'learning_rate': 7.3902853001376545e-06, 'batch_size': 130, 'step_size': 14, 'gamma': 0.9736388110879705}. Best is trial 11 with value: 0.05326354031761487.[0m
[32m[I 2025-01-07 01:21:21,322][0m Trial 12 finished with value: 0.03354983125533258 and parameters: {'observation_period_num': 20, 'train_rates': 0.9027233331806938, 'learning_rate': 0.0001882470216234829, 'batch_size': 124, 'step_size': 15, 'gamma': 0.9231152191525884}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:23:42,914][0m Trial 13 finished with value: 0.06876294025734289 and parameters: {'observation_period_num': 64, 'train_rates': 0.8842414573248732, 'learning_rate': 0.00027800391986289937, 'batch_size': 169, 'step_size': 15, 'gamma': 0.9323364988814407}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:26:03,212][0m Trial 14 finished with value: 0.06230688844598941 and parameters: {'observation_period_num': 16, 'train_rates': 0.8859134022165114, 'learning_rate': 0.00021684408633935214, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9879582373805716}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:27:46,983][0m Trial 15 finished with value: 0.18757115635606977 and parameters: {'observation_period_num': 250, 'train_rates': 0.9069730023660949, 'learning_rate': 4.470877716071342e-06, 'batch_size': 140, 'step_size': 13, 'gamma': 0.9346153034622291}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:30:46,143][0m Trial 16 finished with value: 0.15373047840622095 and parameters: {'observation_period_num': 72, 'train_rates': 0.9232315727657533, 'learning_rate': 1.173164967417486e-06, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9092426498793896}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:32:44,105][0m Trial 17 finished with value: 0.06733378134087094 and parameters: {'observation_period_num': 34, 'train_rates': 0.8463926136938149, 'learning_rate': 0.0009567656630343239, 'batch_size': 116, 'step_size': 15, 'gamma': 0.9546528676995525}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:34:32,672][0m Trial 18 finished with value: 0.13813507556915283 and parameters: {'observation_period_num': 87, 'train_rates': 0.9891971694406425, 'learning_rate': 0.00015025983863907287, 'batch_size': 166, 'step_size': 9, 'gamma': 0.9868317851363548}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:36:56,081][0m Trial 19 finished with value: 0.05181572712817282 and parameters: {'observation_period_num': 8, 'train_rates': 0.8382293826624247, 'learning_rate': 6.375261165661539e-06, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9466495559639335}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:39:38,377][0m Trial 20 finished with value: 0.1846775674120203 and parameters: {'observation_period_num': 36, 'train_rates': 0.7577890607181369, 'learning_rate': 0.00042338668084442754, 'batch_size': 68, 'step_size': 5, 'gamma': 0.9104257693168077}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:43:39,817][0m Trial 21 finished with value: 0.05267274889395178 and parameters: {'observation_period_num': 5, 'train_rates': 0.8213433210525181, 'learning_rate': 4.578594101439473e-06, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9512521361958546}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:46:58,278][0m Trial 22 finished with value: 0.08239004981270842 and parameters: {'observation_period_num': 38, 'train_rates': 0.8177333984571127, 'learning_rate': 2.702595494999298e-06, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9459300532242857}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:54:55,646][0m Trial 23 finished with value: 0.0999277160446129 and parameters: {'observation_period_num': 72, 'train_rates': 0.8522755484996118, 'learning_rate': 7.642637206875207e-06, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9122204857793519}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 01:57:11,219][0m Trial 24 finished with value: 0.07165346341379454 and parameters: {'observation_period_num': 5, 'train_rates': 0.8161697508438299, 'learning_rate': 1.0615986114347164e-06, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9491587408594369}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:01:19,193][0m Trial 25 finished with value: 0.05721862965463838 and parameters: {'observation_period_num': 31, 'train_rates': 0.8649887231129638, 'learning_rate': 5.890348221166739e-05, 'batch_size': 37, 'step_size': 7, 'gamma': 0.888534038488338}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:03:21,711][0m Trial 26 finished with value: 0.34661662590331377 and parameters: {'observation_period_num': 49, 'train_rates': 0.7379776493465143, 'learning_rate': 4.843977762087588e-06, 'batch_size': 74, 'step_size': 3, 'gamma': 0.8344438606874057}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:05:08,401][0m Trial 27 finished with value: 0.2907202038371447 and parameters: {'observation_period_num': 185, 'train_rates': 0.9255522334417616, 'learning_rate': 2.258639370802592e-06, 'batch_size': 110, 'step_size': 5, 'gamma': 0.9219762130822583}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:10:32,527][0m Trial 28 finished with value: 0.04425941029563546 and parameters: {'observation_period_num': 25, 'train_rates': 0.8293959656974278, 'learning_rate': 1.0068757941151235e-05, 'batch_size': 27, 'step_size': 3, 'gamma': 0.942832432081947}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:12:23,589][0m Trial 29 finished with value: 0.10972028474013011 and parameters: {'observation_period_num': 86, 'train_rates': 0.954228009551745, 'learning_rate': 1.440470113516577e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8732286775515637}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:19:01,083][0m Trial 30 finished with value: 0.051933002037809775 and parameters: {'observation_period_num': 19, 'train_rates': 0.7895869248944993, 'learning_rate': 6.581386627779735e-05, 'batch_size': 21, 'step_size': 7, 'gamma': 0.8934795999248215}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:27:11,748][0m Trial 31 finished with value: 0.05024386562633173 and parameters: {'observation_period_num': 27, 'train_rates': 0.7973543760826151, 'learning_rate': 7.664417169133645e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.889583631793771}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:35:55,516][0m Trial 32 finished with value: 0.13603547986211448 and parameters: {'observation_period_num': 55, 'train_rates': 0.8400003200846549, 'learning_rate': 0.00011795535355209104, 'batch_size': 16, 'step_size': 9, 'gamma': 0.934226297347779}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:38:35,806][0m Trial 33 finished with value: 0.04530603718485324 and parameters: {'observation_period_num': 27, 'train_rates': 0.7979899758727662, 'learning_rate': 2.5950260806610253e-05, 'batch_size': 54, 'step_size': 6, 'gamma': 0.847603613955206}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:41:11,472][0m Trial 34 finished with value: 0.1886825630754015 and parameters: {'observation_period_num': 41, 'train_rates': 0.7647843825220076, 'learning_rate': 2.517350578175472e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8497253085252097}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:42:30,464][0m Trial 35 finished with value: 0.0652629312430227 and parameters: {'observation_period_num': 27, 'train_rates': 0.8023141142494488, 'learning_rate': 4.074351249536472e-05, 'batch_size': 200, 'step_size': 6, 'gamma': 0.8268145248157022}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:46:56,844][0m Trial 36 finished with value: 0.1814025521237394 and parameters: {'observation_period_num': 58, 'train_rates': 0.71735880020343, 'learning_rate': 7.837579531999551e-05, 'batch_size': 29, 'step_size': 2, 'gamma': 0.8562751989018752}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:48:25,268][0m Trial 37 finished with value: 0.26134746606037407 and parameters: {'observation_period_num': 124, 'train_rates': 0.7534084182291755, 'learning_rate': 4.011165695586561e-05, 'batch_size': 155, 'step_size': 10, 'gamma': 0.807136778991656}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:51:56,854][0m Trial 38 finished with value: 0.07443280068978872 and parameters: {'observation_period_num': 45, 'train_rates': 0.8746510117353681, 'learning_rate': 0.0001692307638505699, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8771346679174754}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:56:12,337][0m Trial 39 finished with value: 0.18047618296529566 and parameters: {'observation_period_num': 23, 'train_rates': 0.7849374181590681, 'learning_rate': 1.4717442097224042e-05, 'batch_size': 33, 'step_size': 6, 'gamma': 0.8577853473676057}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 02:58:26,970][0m Trial 40 finished with value: 0.18517333828545643 and parameters: {'observation_period_num': 78, 'train_rates': 0.6928591576049095, 'learning_rate': 0.0004439617700433917, 'batch_size': 57, 'step_size': 1, 'gamma': 0.9040817569952943}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:00:27,907][0m Trial 41 finished with value: 0.0612741259272828 and parameters: {'observation_period_num': 19, 'train_rates': 0.8340596350166635, 'learning_rate': 7.845165224389973e-06, 'batch_size': 80, 'step_size': 4, 'gamma': 0.9187412923988069}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:02:43,162][0m Trial 42 finished with value: 0.05635266253490662 and parameters: {'observation_period_num': 26, 'train_rates': 0.8015374528880652, 'learning_rate': 2.8566839013042192e-05, 'batch_size': 67, 'step_size': 7, 'gamma': 0.9632600418893305}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:04:45,555][0m Trial 43 finished with value: 0.05655680837480289 and parameters: {'observation_period_num': 105, 'train_rates': 0.8640862437565718, 'learning_rate': 9.334384154388482e-05, 'batch_size': 122, 'step_size': 6, 'gamma': 0.884637978070075}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:06:48,189][0m Trial 44 finished with value: 0.04258234930610416 and parameters: {'observation_period_num': 15, 'train_rates': 0.8314328422157191, 'learning_rate': 5.1728043411243814e-05, 'batch_size': 100, 'step_size': 2, 'gamma': 0.9413199066991739}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:08:28,263][0m Trial 45 finished with value: 0.25280172418464314 and parameters: {'observation_period_num': 61, 'train_rates': 0.771922730871493, 'learning_rate': 4.514998139787187e-05, 'batch_size': 104, 'step_size': 2, 'gamma': 0.8446796108169228}. Best is trial 12 with value: 0.03354983125533258.[0m
Early stopping at epoch 45
[32m[I 2025-01-07 03:09:08,285][0m Trial 46 finished with value: 0.4083678942679161 and parameters: {'observation_period_num': 45, 'train_rates': 0.6576335498834062, 'learning_rate': 1.7916862233789896e-05, 'batch_size': 133, 'step_size': 1, 'gamma': 0.7560727940852063}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:14:10,552][0m Trial 47 finished with value: 0.03886973429471254 and parameters: {'observation_period_num': 18, 'train_rates': 0.8094047355182096, 'learning_rate': 0.00012921866058830949, 'batch_size': 28, 'step_size': 2, 'gamma': 0.8668911783290884}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:15:52,982][0m Trial 48 finished with value: 0.05235823672551375 and parameters: {'observation_period_num': 15, 'train_rates': 0.8893456756864243, 'learning_rate': 0.0003065116997350072, 'batch_size': 148, 'step_size': 2, 'gamma': 0.8137325397359728}. Best is trial 12 with value: 0.03354983125533258.[0m
[32m[I 2025-01-07 03:18:50,091][0m Trial 49 finished with value: 0.08392496626356463 and parameters: {'observation_period_num': 176, 'train_rates': 0.916962244618247, 'learning_rate': 0.0001157904967585113, 'batch_size': 50, 'step_size': 3, 'gamma': 0.8671767441459031}. Best is trial 12 with value: 0.03354983125533258.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.9597915636103903, 'learning_rate': 0.0006642964513775525, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8639121757908413}
Epoch 1/300, trend Loss: 0.2450 | 0.1051
Epoch 2/300, trend Loss: 0.1318 | 0.0746
Epoch 3/300, trend Loss: 0.1169 | 0.0765
Epoch 4/300, trend Loss: 0.1150 | 0.0660
Epoch 5/300, trend Loss: 0.1188 | 0.0794
Epoch 6/300, trend Loss: 0.1143 | 0.0638
Epoch 7/300, trend Loss: 0.1019 | 0.0638
Epoch 8/300, trend Loss: 0.0990 | 0.0598
Epoch 9/300, trend Loss: 0.0962 | 0.0561
Epoch 10/300, trend Loss: 0.0936 | 0.0534
Epoch 11/300, trend Loss: 0.0915 | 0.0518
Epoch 12/300, trend Loss: 0.0900 | 0.0511
Epoch 13/300, trend Loss: 0.0882 | 0.0500
Epoch 14/300, trend Loss: 0.0862 | 0.0482
Epoch 15/300, trend Loss: 0.0845 | 0.0480
Epoch 16/300, trend Loss: 0.0829 | 0.0460
Epoch 17/300, trend Loss: 0.0808 | 0.0439
Epoch 18/300, trend Loss: 0.0797 | 0.0436
Epoch 19/300, trend Loss: 0.0790 | 0.0434
Epoch 20/300, trend Loss: 0.0776 | 0.0417
Epoch 21/300, trend Loss: 0.0762 | 0.0405
Epoch 22/300, trend Loss: 0.0753 | 0.0396
Epoch 23/300, trend Loss: 0.0745 | 0.0387
Epoch 24/300, trend Loss: 0.0740 | 0.0392
Epoch 25/300, trend Loss: 0.0737 | 0.0386
Epoch 26/300, trend Loss: 0.0730 | 0.0378
Epoch 27/300, trend Loss: 0.0726 | 0.0374
Epoch 28/300, trend Loss: 0.0722 | 0.0380
Epoch 29/300, trend Loss: 0.0718 | 0.0379
Epoch 30/300, trend Loss: 0.0713 | 0.0376
Epoch 31/300, trend Loss: 0.0709 | 0.0372
Epoch 32/300, trend Loss: 0.0706 | 0.0368
Epoch 33/300, trend Loss: 0.0702 | 0.0360
Epoch 34/300, trend Loss: 0.0699 | 0.0357
Epoch 35/300, trend Loss: 0.0695 | 0.0352
Epoch 36/300, trend Loss: 0.0692 | 0.0349
Epoch 37/300, trend Loss: 0.0688 | 0.0347
Epoch 38/300, trend Loss: 0.0685 | 0.0344
Epoch 39/300, trend Loss: 0.0683 | 0.0341
Epoch 40/300, trend Loss: 0.0680 | 0.0338
Epoch 41/300, trend Loss: 0.0677 | 0.0336
Epoch 42/300, trend Loss: 0.0674 | 0.0336
Epoch 43/300, trend Loss: 0.0672 | 0.0336
Epoch 44/300, trend Loss: 0.0669 | 0.0336
Epoch 45/300, trend Loss: 0.0667 | 0.0335
Epoch 46/300, trend Loss: 0.0665 | 0.0339
Epoch 47/300, trend Loss: 0.0663 | 0.0341
Epoch 48/300, trend Loss: 0.0662 | 0.0341
Epoch 49/300, trend Loss: 0.0661 | 0.0341
Epoch 50/300, trend Loss: 0.0660 | 0.0340
Epoch 51/300, trend Loss: 0.0661 | 0.0328
Epoch 52/300, trend Loss: 0.0661 | 0.0328
Epoch 53/300, trend Loss: 0.0661 | 0.0326
Epoch 54/300, trend Loss: 0.0659 | 0.0326
Epoch 55/300, trend Loss: 0.0657 | 0.0341
Epoch 56/300, trend Loss: 0.0656 | 0.0337
Epoch 57/300, trend Loss: 0.0655 | 0.0335
Epoch 58/300, trend Loss: 0.0659 | 0.0337
Epoch 59/300, trend Loss: 0.0665 | 0.0346
Epoch 60/300, trend Loss: 0.0675 | 0.0365
Epoch 61/300, trend Loss: 0.0676 | 0.0363
Epoch 62/300, trend Loss: 0.0669 | 0.0361
Epoch 63/300, trend Loss: 0.0663 | 0.0362
Epoch 64/300, trend Loss: 0.0660 | 0.0360
Epoch 65/300, trend Loss: 0.0659 | 0.0355
Epoch 66/300, trend Loss: 0.0657 | 0.0336
Epoch 67/300, trend Loss: 0.0653 | 0.0320
Epoch 68/300, trend Loss: 0.0650 | 0.0315
Epoch 69/300, trend Loss: 0.0647 | 0.0316
Epoch 70/300, trend Loss: 0.0641 | 0.0314
Epoch 71/300, trend Loss: 0.0636 | 0.0312
Epoch 72/300, trend Loss: 0.0633 | 0.0310
Epoch 73/300, trend Loss: 0.0630 | 0.0310
Epoch 74/300, trend Loss: 0.0629 | 0.0309
Epoch 75/300, trend Loss: 0.0628 | 0.0308
Epoch 76/300, trend Loss: 0.0627 | 0.0308
Epoch 77/300, trend Loss: 0.0626 | 0.0307
Epoch 78/300, trend Loss: 0.0625 | 0.0307
Epoch 79/300, trend Loss: 0.0624 | 0.0307
Epoch 80/300, trend Loss: 0.0624 | 0.0306
Epoch 81/300, trend Loss: 0.0623 | 0.0306
Epoch 82/300, trend Loss: 0.0623 | 0.0306
Epoch 83/300, trend Loss: 0.0622 | 0.0305
Epoch 84/300, trend Loss: 0.0622 | 0.0305
Epoch 85/300, trend Loss: 0.0621 | 0.0305
Epoch 86/300, trend Loss: 0.0621 | 0.0305
Epoch 87/300, trend Loss: 0.0620 | 0.0304
Epoch 88/300, trend Loss: 0.0620 | 0.0304
Epoch 89/300, trend Loss: 0.0619 | 0.0304
Epoch 90/300, trend Loss: 0.0619 | 0.0304
Epoch 91/300, trend Loss: 0.0619 | 0.0304
Epoch 92/300, trend Loss: 0.0619 | 0.0304
Epoch 93/300, trend Loss: 0.0618 | 0.0303
Epoch 94/300, trend Loss: 0.0618 | 0.0303
Epoch 95/300, trend Loss: 0.0617 | 0.0303
Epoch 96/300, trend Loss: 0.0617 | 0.0303
Epoch 97/300, trend Loss: 0.0617 | 0.0303
Epoch 98/300, trend Loss: 0.0616 | 0.0303
Epoch 99/300, trend Loss: 0.0615 | 0.0303
Epoch 100/300, trend Loss: 0.0615 | 0.0303
Epoch 101/300, trend Loss: 0.0614 | 0.0303
Epoch 102/300, trend Loss: 0.0614 | 0.0303
Epoch 103/300, trend Loss: 0.0613 | 0.0302
Epoch 104/300, trend Loss: 0.0613 | 0.0302
Epoch 105/300, trend Loss: 0.0612 | 0.0302
Epoch 106/300, trend Loss: 0.0612 | 0.0302
Epoch 107/300, trend Loss: 0.0612 | 0.0302
Epoch 108/300, trend Loss: 0.0611 | 0.0301
Epoch 109/300, trend Loss: 0.0611 | 0.0301
Epoch 110/300, trend Loss: 0.0610 | 0.0301
Epoch 111/300, trend Loss: 0.0610 | 0.0300
Epoch 112/300, trend Loss: 0.0610 | 0.0300
Epoch 113/300, trend Loss: 0.0609 | 0.0300
Epoch 114/300, trend Loss: 0.0609 | 0.0300
Epoch 115/300, trend Loss: 0.0608 | 0.0300
Epoch 116/300, trend Loss: 0.0608 | 0.0300
Epoch 117/300, trend Loss: 0.0608 | 0.0299
Epoch 118/300, trend Loss: 0.0607 | 0.0300
Epoch 119/300, trend Loss: 0.0607 | 0.0300
Epoch 120/300, trend Loss: 0.0607 | 0.0299
Epoch 121/300, trend Loss: 0.0606 | 0.0299
Epoch 122/300, trend Loss: 0.0606 | 0.0299
Epoch 123/300, trend Loss: 0.0606 | 0.0299
Epoch 124/300, trend Loss: 0.0606 | 0.0299
Epoch 125/300, trend Loss: 0.0606 | 0.0299
Epoch 126/300, trend Loss: 0.0605 | 0.0299
Epoch 127/300, trend Loss: 0.0605 | 0.0299
Epoch 128/300, trend Loss: 0.0605 | 0.0299
Epoch 129/300, trend Loss: 0.0605 | 0.0299
Epoch 130/300, trend Loss: 0.0605 | 0.0299
Epoch 131/300, trend Loss: 0.0605 | 0.0299
Epoch 132/300, trend Loss: 0.0604 | 0.0299
Epoch 133/300, trend Loss: 0.0604 | 0.0298
Epoch 134/300, trend Loss: 0.0604 | 0.0298
Epoch 135/300, trend Loss: 0.0604 | 0.0298
Epoch 136/300, trend Loss: 0.0604 | 0.0298
Epoch 137/300, trend Loss: 0.0604 | 0.0298
Epoch 138/300, trend Loss: 0.0604 | 0.0298
Epoch 139/300, trend Loss: 0.0604 | 0.0298
Epoch 140/300, trend Loss: 0.0604 | 0.0298
Epoch 141/300, trend Loss: 0.0604 | 0.0298
Epoch 142/300, trend Loss: 0.0603 | 0.0298
Epoch 143/300, trend Loss: 0.0603 | 0.0298
Epoch 144/300, trend Loss: 0.0603 | 0.0298
Epoch 145/300, trend Loss: 0.0603 | 0.0298
Epoch 146/300, trend Loss: 0.0603 | 0.0298
Epoch 147/300, trend Loss: 0.0603 | 0.0297
Epoch 148/300, trend Loss: 0.0603 | 0.0297
Epoch 149/300, trend Loss: 0.0603 | 0.0297
Epoch 150/300, trend Loss: 0.0603 | 0.0297
Epoch 151/300, trend Loss: 0.0603 | 0.0297
Epoch 152/300, trend Loss: 0.0603 | 0.0297
Epoch 153/300, trend Loss: 0.0602 | 0.0297
Epoch 154/300, trend Loss: 0.0602 | 0.0297
Epoch 155/300, trend Loss: 0.0602 | 0.0297
Epoch 156/300, trend Loss: 0.0602 | 0.0297
Epoch 157/300, trend Loss: 0.0602 | 0.0297
Epoch 158/300, trend Loss: 0.0602 | 0.0296
Epoch 159/300, trend Loss: 0.0602 | 0.0296
Epoch 160/300, trend Loss: 0.0602 | 0.0296
Epoch 161/300, trend Loss: 0.0601 | 0.0296
Epoch 162/300, trend Loss: 0.0601 | 0.0296
Epoch 163/300, trend Loss: 0.0601 | 0.0296
Epoch 164/300, trend Loss: 0.0601 | 0.0296
Epoch 165/300, trend Loss: 0.0601 | 0.0296
Epoch 166/300, trend Loss: 0.0601 | 0.0296
Epoch 167/300, trend Loss: 0.0601 | 0.0296
Epoch 168/300, trend Loss: 0.0601 | 0.0296
Epoch 169/300, trend Loss: 0.0601 | 0.0296
Epoch 170/300, trend Loss: 0.0601 | 0.0296
Epoch 171/300, trend Loss: 0.0601 | 0.0296
Epoch 172/300, trend Loss: 0.0601 | 0.0296
Epoch 173/300, trend Loss: 0.0601 | 0.0296
Epoch 174/300, trend Loss: 0.0601 | 0.0296
Epoch 175/300, trend Loss: 0.0601 | 0.0296
Epoch 176/300, trend Loss: 0.0601 | 0.0295
Epoch 177/300, trend Loss: 0.0600 | 0.0295
Epoch 178/300, trend Loss: 0.0600 | 0.0295
Epoch 179/300, trend Loss: 0.0600 | 0.0295
Epoch 180/300, trend Loss: 0.0600 | 0.0295
Epoch 181/300, trend Loss: 0.0600 | 0.0295
Epoch 182/300, trend Loss: 0.0600 | 0.0295
Epoch 183/300, trend Loss: 0.0600 | 0.0295
Epoch 184/300, trend Loss: 0.0600 | 0.0295
Epoch 185/300, trend Loss: 0.0600 | 0.0295
Epoch 186/300, trend Loss: 0.0600 | 0.0295
Epoch 187/300, trend Loss: 0.0600 | 0.0295
Epoch 188/300, trend Loss: 0.0600 | 0.0295
Epoch 189/300, trend Loss: 0.0600 | 0.0295
Epoch 190/300, trend Loss: 0.0600 | 0.0295
Epoch 191/300, trend Loss: 0.0600 | 0.0295
Epoch 192/300, trend Loss: 0.0600 | 0.0295
Epoch 193/300, trend Loss: 0.0600 | 0.0295
Epoch 194/300, trend Loss: 0.0600 | 0.0295
Epoch 195/300, trend Loss: 0.0600 | 0.0295
Epoch 196/300, trend Loss: 0.0600 | 0.0295
Epoch 197/300, trend Loss: 0.0600 | 0.0295
Epoch 198/300, trend Loss: 0.0600 | 0.0295
Epoch 199/300, trend Loss: 0.0600 | 0.0295
Epoch 200/300, trend Loss: 0.0600 | 0.0295
Epoch 201/300, trend Loss: 0.0600 | 0.0295
Epoch 202/300, trend Loss: 0.0600 | 0.0295
Epoch 203/300, trend Loss: 0.0600 | 0.0295
Epoch 204/300, trend Loss: 0.0600 | 0.0295
Epoch 205/300, trend Loss: 0.0600 | 0.0295
Epoch 206/300, trend Loss: 0.0600 | 0.0295
Epoch 207/300, trend Loss: 0.0600 | 0.0295
Epoch 208/300, trend Loss: 0.0600 | 0.0295
Epoch 209/300, trend Loss: 0.0600 | 0.0295
Epoch 210/300, trend Loss: 0.0600 | 0.0295
Epoch 211/300, trend Loss: 0.0600 | 0.0295
Epoch 212/300, trend Loss: 0.0600 | 0.0295
Epoch 213/300, trend Loss: 0.0600 | 0.0295
Epoch 214/300, trend Loss: 0.0600 | 0.0295
Epoch 215/300, trend Loss: 0.0600 | 0.0295
Epoch 216/300, trend Loss: 0.0600 | 0.0295
Epoch 217/300, trend Loss: 0.0600 | 0.0295
Epoch 218/300, trend Loss: 0.0600 | 0.0295
Epoch 219/300, trend Loss: 0.0600 | 0.0295
Epoch 220/300, trend Loss: 0.0600 | 0.0295
Epoch 221/300, trend Loss: 0.0600 | 0.0295
Epoch 222/300, trend Loss: 0.0600 | 0.0295
Epoch 223/300, trend Loss: 0.0600 | 0.0295
Epoch 224/300, trend Loss: 0.0600 | 0.0295
Epoch 225/300, trend Loss: 0.0600 | 0.0295
Epoch 226/300, trend Loss: 0.0600 | 0.0295
Epoch 227/300, trend Loss: 0.0600 | 0.0295
Epoch 228/300, trend Loss: 0.0600 | 0.0295
Epoch 229/300, trend Loss: 0.0600 | 0.0295
Epoch 230/300, trend Loss: 0.0600 | 0.0295
Epoch 231/300, trend Loss: 0.0599 | 0.0295
Epoch 232/300, trend Loss: 0.0599 | 0.0295
Epoch 233/300, trend Loss: 0.0599 | 0.0295
Epoch 234/300, trend Loss: 0.0599 | 0.0295
Epoch 235/300, trend Loss: 0.0599 | 0.0295
Epoch 236/300, trend Loss: 0.0599 | 0.0295
Epoch 237/300, trend Loss: 0.0599 | 0.0295
Epoch 238/300, trend Loss: 0.0599 | 0.0295
Epoch 239/300, trend Loss: 0.0599 | 0.0295
Epoch 240/300, trend Loss: 0.0599 | 0.0295
Epoch 241/300, trend Loss: 0.0599 | 0.0295
Epoch 242/300, trend Loss: 0.0599 | 0.0295
Epoch 243/300, trend Loss: 0.0599 | 0.0295
Epoch 244/300, trend Loss: 0.0599 | 0.0295
Epoch 245/300, trend Loss: 0.0599 | 0.0295
Epoch 246/300, trend Loss: 0.0599 | 0.0295
Epoch 247/300, trend Loss: 0.0599 | 0.0295
Epoch 248/300, trend Loss: 0.0599 | 0.0295
Epoch 249/300, trend Loss: 0.0599 | 0.0295
Epoch 250/300, trend Loss: 0.0599 | 0.0295
Epoch 251/300, trend Loss: 0.0599 | 0.0295
Epoch 252/300, trend Loss: 0.0599 | 0.0295
Epoch 253/300, trend Loss: 0.0599 | 0.0295
Epoch 254/300, trend Loss: 0.0599 | 0.0295
Epoch 255/300, trend Loss: 0.0599 | 0.0295
Epoch 256/300, trend Loss: 0.0599 | 0.0295
Epoch 257/300, trend Loss: 0.0599 | 0.0295
Epoch 258/300, trend Loss: 0.0599 | 0.0295
Epoch 259/300, trend Loss: 0.0599 | 0.0295
Epoch 260/300, trend Loss: 0.0599 | 0.0295
Epoch 261/300, trend Loss: 0.0599 | 0.0295
Epoch 262/300, trend Loss: 0.0599 | 0.0295
Epoch 263/300, trend Loss: 0.0599 | 0.0295
Epoch 264/300, trend Loss: 0.0599 | 0.0295
Epoch 265/300, trend Loss: 0.0599 | 0.0295
Epoch 266/300, trend Loss: 0.0599 | 0.0295
Epoch 267/300, trend Loss: 0.0599 | 0.0295
Epoch 268/300, trend Loss: 0.0599 | 0.0295
Epoch 269/300, trend Loss: 0.0599 | 0.0295
Epoch 270/300, trend Loss: 0.0599 | 0.0295
Epoch 271/300, trend Loss: 0.0599 | 0.0295
Epoch 272/300, trend Loss: 0.0599 | 0.0295
Epoch 273/300, trend Loss: 0.0599 | 0.0295
Epoch 274/300, trend Loss: 0.0599 | 0.0295
Epoch 275/300, trend Loss: 0.0599 | 0.0295
Epoch 276/300, trend Loss: 0.0599 | 0.0295
Epoch 277/300, trend Loss: 0.0599 | 0.0295
Epoch 278/300, trend Loss: 0.0599 | 0.0295
Epoch 279/300, trend Loss: 0.0599 | 0.0295
Epoch 280/300, trend Loss: 0.0599 | 0.0295
Epoch 281/300, trend Loss: 0.0599 | 0.0295
Epoch 282/300, trend Loss: 0.0599 | 0.0295
Epoch 283/300, trend Loss: 0.0599 | 0.0295
Epoch 284/300, trend Loss: 0.0599 | 0.0295
Epoch 285/300, trend Loss: 0.0599 | 0.0295
Epoch 286/300, trend Loss: 0.0599 | 0.0295
Epoch 287/300, trend Loss: 0.0599 | 0.0295
Epoch 288/300, trend Loss: 0.0599 | 0.0295
Epoch 289/300, trend Loss: 0.0599 | 0.0295
Epoch 290/300, trend Loss: 0.0599 | 0.0295
Epoch 291/300, trend Loss: 0.0599 | 0.0295
Epoch 292/300, trend Loss: 0.0599 | 0.0295
Epoch 293/300, trend Loss: 0.0599 | 0.0295
Epoch 294/300, trend Loss: 0.0599 | 0.0295
Epoch 295/300, trend Loss: 0.0599 | 0.0295
Epoch 296/300, trend Loss: 0.0599 | 0.0295
Epoch 297/300, trend Loss: 0.0599 | 0.0295
Epoch 298/300, trend Loss: 0.0599 | 0.0295
Epoch 299/300, trend Loss: 0.0599 | 0.0295
Epoch 300/300, trend Loss: 0.0599 | 0.0295
Training seasonal_0 component with params: {'observation_period_num': 7, 'train_rates': 0.9570966630688225, 'learning_rate': 0.0006828972698269891, 'batch_size': 116, 'step_size': 9, 'gamma': 0.881701989769117}
Epoch 1/300, seasonal_0 Loss: 2.1496 | 1.3649
Epoch 2/300, seasonal_0 Loss: 0.7074 | 0.6402
Epoch 3/300, seasonal_0 Loss: 0.5220 | 0.4101
Epoch 4/300, seasonal_0 Loss: 0.4399 | 0.3235
Epoch 5/300, seasonal_0 Loss: 0.3946 | 0.3201
Epoch 6/300, seasonal_0 Loss: 0.3923 | 0.3018
Epoch 7/300, seasonal_0 Loss: 0.3460 | 0.2816
Epoch 8/300, seasonal_0 Loss: 0.2879 | 0.2190
Epoch 9/300, seasonal_0 Loss: 0.2363 | 0.1873
Epoch 10/300, seasonal_0 Loss: 0.1637 | 0.0991
Epoch 11/300, seasonal_0 Loss: 0.1637 | 0.0961
Epoch 12/300, seasonal_0 Loss: 0.1445 | 0.1120
Epoch 13/300, seasonal_0 Loss: 0.1475 | 0.1018
Epoch 14/300, seasonal_0 Loss: 0.1476 | 0.1373
Epoch 15/300, seasonal_0 Loss: 0.1525 | 0.0966
Epoch 16/300, seasonal_0 Loss: 0.1610 | 0.1101
Epoch 17/300, seasonal_0 Loss: 0.1526 | 0.0946
Epoch 18/300, seasonal_0 Loss: 0.1380 | 0.0845
Epoch 19/300, seasonal_0 Loss: 0.1702 | 0.1016
Epoch 20/300, seasonal_0 Loss: 0.1515 | 0.0954
Epoch 21/300, seasonal_0 Loss: 0.1794 | 0.1246
Epoch 22/300, seasonal_0 Loss: 0.1739 | 0.1019
Epoch 23/300, seasonal_0 Loss: 0.1488 | 0.0916
Epoch 24/300, seasonal_0 Loss: 0.1510 | 0.1029
Epoch 25/300, seasonal_0 Loss: 0.1925 | 0.1435
Epoch 26/300, seasonal_0 Loss: 0.2023 | 0.0832
Epoch 27/300, seasonal_0 Loss: 0.1710 | 0.1018
Epoch 28/300, seasonal_0 Loss: 0.1698 | 0.0982
Epoch 29/300, seasonal_0 Loss: 0.1471 | 0.0940
Epoch 30/300, seasonal_0 Loss: 0.1341 | 0.0750
Epoch 31/300, seasonal_0 Loss: 0.1484 | 0.0887
Epoch 32/300, seasonal_0 Loss: 0.1668 | 0.0985
Epoch 33/300, seasonal_0 Loss: 0.1349 | 0.0874
Epoch 34/300, seasonal_0 Loss: 0.1283 | 0.0848
Epoch 35/300, seasonal_0 Loss: 0.1201 | 0.0911
Epoch 36/300, seasonal_0 Loss: 0.1301 | 0.0818
Epoch 37/300, seasonal_0 Loss: 0.1290 | 0.0831
Epoch 38/300, seasonal_0 Loss: 0.1268 | 0.0710
Epoch 39/300, seasonal_0 Loss: 0.1301 | 0.0912
Epoch 40/300, seasonal_0 Loss: 0.1440 | 0.0705
Epoch 41/300, seasonal_0 Loss: 0.1283 | 0.0858
Epoch 42/300, seasonal_0 Loss: 0.1186 | 0.0837
Epoch 43/300, seasonal_0 Loss: 0.1428 | 0.0982
Epoch 44/300, seasonal_0 Loss: 0.1371 | 0.0966
Epoch 45/300, seasonal_0 Loss: 0.1125 | 0.0721
Epoch 46/300, seasonal_0 Loss: 0.1150 | 0.0993
Epoch 47/300, seasonal_0 Loss: 0.1039 | 0.0608
Epoch 48/300, seasonal_0 Loss: 0.0986 | 0.0631
Epoch 49/300, seasonal_0 Loss: 0.0931 | 0.0595
Epoch 50/300, seasonal_0 Loss: 0.1058 | 0.0600
Epoch 51/300, seasonal_0 Loss: 0.1083 | 0.0575
Epoch 52/300, seasonal_0 Loss: 0.1083 | 0.0723
Epoch 53/300, seasonal_0 Loss: 0.1126 | 0.0617
Epoch 54/300, seasonal_0 Loss: 0.0973 | 0.0512
Epoch 55/300, seasonal_0 Loss: 0.0975 | 0.0527
Epoch 56/300, seasonal_0 Loss: 0.0927 | 0.0494
Epoch 57/300, seasonal_0 Loss: 0.0854 | 0.0511
Epoch 58/300, seasonal_0 Loss: 0.0997 | 0.0531
Epoch 59/300, seasonal_0 Loss: 0.0983 | 0.0500
Epoch 60/300, seasonal_0 Loss: 0.0848 | 0.0459
Epoch 61/300, seasonal_0 Loss: 0.0859 | 0.0490
Epoch 62/300, seasonal_0 Loss: 0.0905 | 0.0669
Epoch 63/300, seasonal_0 Loss: 0.0985 | 0.0603
Epoch 64/300, seasonal_0 Loss: 0.0930 | 0.0711
Epoch 65/300, seasonal_0 Loss: 0.0985 | 0.0580
Epoch 66/300, seasonal_0 Loss: 0.0832 | 0.0571
Epoch 67/300, seasonal_0 Loss: 0.1015 | 0.0578
Epoch 68/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 69/300, seasonal_0 Loss: 0.1009 | 0.0590
Epoch 70/300, seasonal_0 Loss: 0.0971 | 0.0556
Epoch 71/300, seasonal_0 Loss: 0.0903 | 0.0496
Epoch 72/300, seasonal_0 Loss: 0.0766 | 0.0464
Epoch 73/300, seasonal_0 Loss: 0.0842 | 0.0493
Epoch 74/300, seasonal_0 Loss: 0.0874 | 0.0456
Epoch 75/300, seasonal_0 Loss: 0.0918 | 0.0663
Epoch 76/300, seasonal_0 Loss: 0.0868 | 0.0792
Epoch 77/300, seasonal_0 Loss: 0.0848 | 0.0565
Epoch 78/300, seasonal_0 Loss: 0.0872 | 0.0568
Epoch 79/300, seasonal_0 Loss: 0.0932 | 0.0486
Epoch 80/300, seasonal_0 Loss: 0.0979 | 0.0492
Epoch 81/300, seasonal_0 Loss: 0.0803 | 0.0450
Epoch 82/300, seasonal_0 Loss: 0.0780 | 0.0522
Epoch 83/300, seasonal_0 Loss: 0.0811 | 0.0553
Epoch 84/300, seasonal_0 Loss: 0.0795 | 0.0556
Epoch 85/300, seasonal_0 Loss: 0.0715 | 0.0428
Epoch 86/300, seasonal_0 Loss: 0.0706 | 0.0435
Epoch 87/300, seasonal_0 Loss: 0.0722 | 0.0404
Epoch 88/300, seasonal_0 Loss: 0.0689 | 0.0425
Epoch 89/300, seasonal_0 Loss: 0.0654 | 0.0416
Epoch 90/300, seasonal_0 Loss: 0.0661 | 0.0449
Epoch 91/300, seasonal_0 Loss: 0.0648 | 0.0392
Epoch 92/300, seasonal_0 Loss: 0.0637 | 0.0387
Epoch 93/300, seasonal_0 Loss: 0.0636 | 0.0386
Epoch 94/300, seasonal_0 Loss: 0.0634 | 0.0404
Epoch 95/300, seasonal_0 Loss: 0.0643 | 0.0404
Epoch 96/300, seasonal_0 Loss: 0.0651 | 0.0445
Epoch 97/300, seasonal_0 Loss: 0.0657 | 0.0409
Epoch 98/300, seasonal_0 Loss: 0.0643 | 0.0409
Epoch 99/300, seasonal_0 Loss: 0.0636 | 0.0404
Epoch 100/300, seasonal_0 Loss: 0.0627 | 0.0401
Epoch 101/300, seasonal_0 Loss: 0.0625 | 0.0398
Epoch 102/300, seasonal_0 Loss: 0.0635 | 0.0440
Epoch 103/300, seasonal_0 Loss: 0.0648 | 0.0418
Epoch 104/300, seasonal_0 Loss: 0.0637 | 0.0405
Epoch 105/300, seasonal_0 Loss: 0.0625 | 0.0394
Epoch 106/300, seasonal_0 Loss: 0.0621 | 0.0395
Epoch 107/300, seasonal_0 Loss: 0.0612 | 0.0386
Epoch 108/300, seasonal_0 Loss: 0.0608 | 0.0380
Epoch 109/300, seasonal_0 Loss: 0.0607 | 0.0381
Epoch 110/300, seasonal_0 Loss: 0.0607 | 0.0385
Epoch 111/300, seasonal_0 Loss: 0.0606 | 0.0372
Epoch 112/300, seasonal_0 Loss: 0.0600 | 0.0376
Epoch 113/300, seasonal_0 Loss: 0.0595 | 0.0373
Epoch 114/300, seasonal_0 Loss: 0.0593 | 0.0379
Epoch 115/300, seasonal_0 Loss: 0.0597 | 0.0377
Epoch 116/300, seasonal_0 Loss: 0.0617 | 0.0436
Epoch 117/300, seasonal_0 Loss: 0.0628 | 0.0406
Epoch 118/300, seasonal_0 Loss: 0.0616 | 0.0395
Epoch 119/300, seasonal_0 Loss: 0.0602 | 0.0384
Epoch 120/300, seasonal_0 Loss: 0.0589 | 0.0378
Epoch 121/300, seasonal_0 Loss: 0.0581 | 0.0374
Epoch 122/300, seasonal_0 Loss: 0.0555 | 0.0362
Epoch 123/300, seasonal_0 Loss: 0.0534 | 0.0371
Epoch 124/300, seasonal_0 Loss: 0.0568 | 0.0376
Epoch 125/300, seasonal_0 Loss: 0.0538 | 0.0383
Epoch 126/300, seasonal_0 Loss: 0.0581 | 0.0342
Epoch 127/300, seasonal_0 Loss: 0.0699 | 0.0434
Epoch 128/300, seasonal_0 Loss: 0.0669 | 0.0354
Epoch 129/300, seasonal_0 Loss: 0.0642 | 0.0353
Epoch 130/300, seasonal_0 Loss: 0.0617 | 0.0349
Epoch 131/300, seasonal_0 Loss: 0.0607 | 0.0349
Epoch 132/300, seasonal_0 Loss: 0.0599 | 0.0346
Epoch 133/300, seasonal_0 Loss: 0.0589 | 0.0342
Epoch 134/300, seasonal_0 Loss: 0.0582 | 0.0349
Epoch 135/300, seasonal_0 Loss: 0.0579 | 0.0353
Epoch 136/300, seasonal_0 Loss: 0.0576 | 0.0351
Epoch 137/300, seasonal_0 Loss: 0.0574 | 0.0344
Epoch 138/300, seasonal_0 Loss: 0.0573 | 0.0340
Epoch 139/300, seasonal_0 Loss: 0.0573 | 0.0340
Epoch 140/300, seasonal_0 Loss: 0.0571 | 0.0345
Epoch 141/300, seasonal_0 Loss: 0.0570 | 0.0348
Epoch 142/300, seasonal_0 Loss: 0.0570 | 0.0350
Epoch 143/300, seasonal_0 Loss: 0.0570 | 0.0347
Epoch 144/300, seasonal_0 Loss: 0.0567 | 0.0342
Epoch 145/300, seasonal_0 Loss: 0.0566 | 0.0336
Epoch 146/300, seasonal_0 Loss: 0.0565 | 0.0338
Epoch 147/300, seasonal_0 Loss: 0.0563 | 0.0342
Epoch 148/300, seasonal_0 Loss: 0.0563 | 0.0345
Epoch 149/300, seasonal_0 Loss: 0.0563 | 0.0344
Epoch 150/300, seasonal_0 Loss: 0.0562 | 0.0341
Epoch 151/300, seasonal_0 Loss: 0.0561 | 0.0336
Epoch 152/300, seasonal_0 Loss: 0.0560 | 0.0335
Epoch 153/300, seasonal_0 Loss: 0.0559 | 0.0338
Epoch 154/300, seasonal_0 Loss: 0.0559 | 0.0341
Epoch 155/300, seasonal_0 Loss: 0.0559 | 0.0342
Epoch 156/300, seasonal_0 Loss: 0.0558 | 0.0340
Epoch 157/300, seasonal_0 Loss: 0.0557 | 0.0336
Epoch 158/300, seasonal_0 Loss: 0.0557 | 0.0334
Epoch 159/300, seasonal_0 Loss: 0.0556 | 0.0335
Epoch 160/300, seasonal_0 Loss: 0.0556 | 0.0338
Epoch 161/300, seasonal_0 Loss: 0.0555 | 0.0340
Epoch 162/300, seasonal_0 Loss: 0.0555 | 0.0339
Epoch 163/300, seasonal_0 Loss: 0.0555 | 0.0337
Epoch 164/300, seasonal_0 Loss: 0.0554 | 0.0334
Epoch 165/300, seasonal_0 Loss: 0.0554 | 0.0333
Epoch 166/300, seasonal_0 Loss: 0.0553 | 0.0335
Epoch 167/300, seasonal_0 Loss: 0.0553 | 0.0337
Epoch 168/300, seasonal_0 Loss: 0.0553 | 0.0338
Epoch 169/300, seasonal_0 Loss: 0.0552 | 0.0337
Epoch 170/300, seasonal_0 Loss: 0.0552 | 0.0334
Epoch 171/300, seasonal_0 Loss: 0.0551 | 0.0332
Epoch 172/300, seasonal_0 Loss: 0.0551 | 0.0333
Epoch 173/300, seasonal_0 Loss: 0.0550 | 0.0335
Epoch 174/300, seasonal_0 Loss: 0.0550 | 0.0336
Epoch 175/300, seasonal_0 Loss: 0.0550 | 0.0336
Epoch 176/300, seasonal_0 Loss: 0.0550 | 0.0334
Epoch 177/300, seasonal_0 Loss: 0.0549 | 0.0332
Epoch 178/300, seasonal_0 Loss: 0.0549 | 0.0331
Epoch 179/300, seasonal_0 Loss: 0.0548 | 0.0332
Epoch 180/300, seasonal_0 Loss: 0.0548 | 0.0333
Epoch 181/300, seasonal_0 Loss: 0.0548 | 0.0334
Epoch 182/300, seasonal_0 Loss: 0.0548 | 0.0333
Epoch 183/300, seasonal_0 Loss: 0.0547 | 0.0331
Epoch 184/300, seasonal_0 Loss: 0.0547 | 0.0331
Epoch 185/300, seasonal_0 Loss: 0.0547 | 0.0331
Epoch 186/300, seasonal_0 Loss: 0.0546 | 0.0332
Epoch 187/300, seasonal_0 Loss: 0.0546 | 0.0332
Epoch 188/300, seasonal_0 Loss: 0.0546 | 0.0332
Epoch 189/300, seasonal_0 Loss: 0.0546 | 0.0331
Epoch 190/300, seasonal_0 Loss: 0.0545 | 0.0330
Epoch 191/300, seasonal_0 Loss: 0.0545 | 0.0330
Epoch 192/300, seasonal_0 Loss: 0.0545 | 0.0330
Epoch 193/300, seasonal_0 Loss: 0.0545 | 0.0331
Epoch 194/300, seasonal_0 Loss: 0.0545 | 0.0330
Epoch 195/300, seasonal_0 Loss: 0.0544 | 0.0330
Epoch 196/300, seasonal_0 Loss: 0.0544 | 0.0329
Epoch 197/300, seasonal_0 Loss: 0.0544 | 0.0329
Epoch 198/300, seasonal_0 Loss: 0.0544 | 0.0329
Epoch 199/300, seasonal_0 Loss: 0.0543 | 0.0329
Epoch 200/300, seasonal_0 Loss: 0.0543 | 0.0329
Epoch 201/300, seasonal_0 Loss: 0.0543 | 0.0329
Epoch 202/300, seasonal_0 Loss: 0.0543 | 0.0329
Epoch 203/300, seasonal_0 Loss: 0.0543 | 0.0329
Epoch 204/300, seasonal_0 Loss: 0.0542 | 0.0329
Epoch 205/300, seasonal_0 Loss: 0.0542 | 0.0328
Epoch 206/300, seasonal_0 Loss: 0.0542 | 0.0328
Epoch 207/300, seasonal_0 Loss: 0.0542 | 0.0328
Epoch 208/300, seasonal_0 Loss: 0.0542 | 0.0328
Epoch 209/300, seasonal_0 Loss: 0.0542 | 0.0328
Epoch 210/300, seasonal_0 Loss: 0.0541 | 0.0328
Epoch 211/300, seasonal_0 Loss: 0.0541 | 0.0328
Epoch 212/300, seasonal_0 Loss: 0.0541 | 0.0327
Epoch 213/300, seasonal_0 Loss: 0.0541 | 0.0327
Epoch 214/300, seasonal_0 Loss: 0.0541 | 0.0327
Epoch 215/300, seasonal_0 Loss: 0.0541 | 0.0327
Epoch 216/300, seasonal_0 Loss: 0.0541 | 0.0327
Epoch 217/300, seasonal_0 Loss: 0.0540 | 0.0327
Epoch 218/300, seasonal_0 Loss: 0.0540 | 0.0327
Epoch 219/300, seasonal_0 Loss: 0.0540 | 0.0327
Epoch 220/300, seasonal_0 Loss: 0.0540 | 0.0327
Epoch 221/300, seasonal_0 Loss: 0.0540 | 0.0326
Epoch 222/300, seasonal_0 Loss: 0.0540 | 0.0326
Epoch 223/300, seasonal_0 Loss: 0.0540 | 0.0326
Epoch 224/300, seasonal_0 Loss: 0.0540 | 0.0326
Epoch 225/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 226/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 227/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 228/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 229/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 230/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 231/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 232/300, seasonal_0 Loss: 0.0539 | 0.0326
Epoch 233/300, seasonal_0 Loss: 0.0538 | 0.0326
Epoch 234/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 235/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 236/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 237/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 238/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 239/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 240/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 241/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 242/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 243/300, seasonal_0 Loss: 0.0538 | 0.0325
Epoch 244/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 245/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 246/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 247/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 248/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 249/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 250/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 251/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 252/300, seasonal_0 Loss: 0.0537 | 0.0325
Epoch 253/300, seasonal_0 Loss: 0.0537 | 0.0324
Epoch 254/300, seasonal_0 Loss: 0.0537 | 0.0324
Epoch 255/300, seasonal_0 Loss: 0.0537 | 0.0324
Epoch 256/300, seasonal_0 Loss: 0.0537 | 0.0324
Epoch 257/300, seasonal_0 Loss: 0.0537 | 0.0324
Epoch 258/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 259/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 260/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 261/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 262/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 263/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 264/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 265/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 266/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 267/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 268/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 269/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 270/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 271/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 272/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 273/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 274/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 275/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 276/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 277/300, seasonal_0 Loss: 0.0536 | 0.0324
Epoch 278/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 279/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 280/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 281/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 282/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 283/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 284/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 285/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 286/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 287/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 288/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 289/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 290/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 291/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 292/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 293/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 294/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 295/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 296/300, seasonal_0 Loss: 0.0535 | 0.0324
Epoch 297/300, seasonal_0 Loss: 0.0535 | 0.0323
Epoch 298/300, seasonal_0 Loss: 0.0535 | 0.0323
Epoch 299/300, seasonal_0 Loss: 0.0535 | 0.0323
Epoch 300/300, seasonal_0 Loss: 0.0535 | 0.0323
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8877681061763335, 'learning_rate': 0.000674404552077101, 'batch_size': 254, 'step_size': 12, 'gamma': 0.8941227859745525}
Epoch 1/300, seasonal_1 Loss: 3.7871 | 1.1781
Epoch 2/300, seasonal_1 Loss: 0.5350 | 0.5360
Epoch 3/300, seasonal_1 Loss: 0.3977 | 0.2753
Epoch 4/300, seasonal_1 Loss: 0.4566 | 0.2943
Epoch 5/300, seasonal_1 Loss: 0.5156 | 0.4551
Epoch 6/300, seasonal_1 Loss: 0.6261 | 0.8284
Epoch 7/300, seasonal_1 Loss: 0.6471 | 0.3648
Epoch 8/300, seasonal_1 Loss: 0.4927 | 0.3520
Epoch 9/300, seasonal_1 Loss: 0.3983 | 0.2314
Epoch 10/300, seasonal_1 Loss: 0.3079 | 0.2575
Epoch 11/300, seasonal_1 Loss: 0.3052 | 0.2811
Epoch 12/300, seasonal_1 Loss: 0.3414 | 0.1655
Epoch 13/300, seasonal_1 Loss: 0.2775 | 0.1758
Epoch 14/300, seasonal_1 Loss: 0.2649 | 0.1618
Epoch 15/300, seasonal_1 Loss: 0.2387 | 0.1412
Epoch 16/300, seasonal_1 Loss: 0.2222 | 0.2744
Epoch 17/300, seasonal_1 Loss: 0.2385 | 0.1904
Epoch 18/300, seasonal_1 Loss: 0.2123 | 0.1417
Epoch 19/300, seasonal_1 Loss: 0.2659 | 0.1172
Epoch 20/300, seasonal_1 Loss: 0.2050 | 0.1283
Epoch 21/300, seasonal_1 Loss: 0.1445 | 0.1278
Epoch 22/300, seasonal_1 Loss: 0.1766 | 0.1974
Epoch 23/300, seasonal_1 Loss: 0.2046 | 0.1615
Epoch 24/300, seasonal_1 Loss: 0.1912 | 0.0798
Epoch 25/300, seasonal_1 Loss: 0.1655 | 0.0976
Epoch 26/300, seasonal_1 Loss: 0.1495 | 0.0998
Epoch 27/300, seasonal_1 Loss: 0.1295 | 0.0994
Epoch 28/300, seasonal_1 Loss: 0.1435 | 0.1512
Epoch 29/300, seasonal_1 Loss: 0.1241 | 0.0662
Epoch 30/300, seasonal_1 Loss: 0.1366 | 0.0652
Epoch 31/300, seasonal_1 Loss: 0.1508 | 0.1054
Epoch 32/300, seasonal_1 Loss: 0.1499 | 0.0840
Epoch 33/300, seasonal_1 Loss: 0.1150 | 0.0560
Epoch 34/300, seasonal_1 Loss: 0.1125 | 0.0899
Epoch 35/300, seasonal_1 Loss: 0.1090 | 0.1132
Epoch 36/300, seasonal_1 Loss: 0.1029 | 0.0684
Epoch 37/300, seasonal_1 Loss: 0.1027 | 0.0598
Epoch 38/300, seasonal_1 Loss: 0.1025 | 0.0591
Epoch 39/300, seasonal_1 Loss: 0.1002 | 0.0671
Epoch 40/300, seasonal_1 Loss: 0.0951 | 0.0835
Epoch 41/300, seasonal_1 Loss: 0.1020 | 0.0952
Epoch 42/300, seasonal_1 Loss: 0.0970 | 0.0588
Epoch 43/300, seasonal_1 Loss: 0.0963 | 0.0609
Epoch 44/300, seasonal_1 Loss: 0.0963 | 0.0483
Epoch 45/300, seasonal_1 Loss: 0.0928 | 0.0846
Epoch 46/300, seasonal_1 Loss: 0.0860 | 0.0590
Epoch 47/300, seasonal_1 Loss: 0.0956 | 0.0881
Epoch 48/300, seasonal_1 Loss: 0.0910 | 0.0545
Epoch 49/300, seasonal_1 Loss: 0.0949 | 0.0538
Epoch 50/300, seasonal_1 Loss: 0.0911 | 0.0571
Epoch 51/300, seasonal_1 Loss: 0.0847 | 0.0846
Epoch 52/300, seasonal_1 Loss: 0.0854 | 0.0572
Epoch 53/300, seasonal_1 Loss: 0.0885 | 0.0559
Epoch 54/300, seasonal_1 Loss: 0.0839 | 0.0462
Epoch 55/300, seasonal_1 Loss: 0.0786 | 0.0629
Epoch 56/300, seasonal_1 Loss: 0.0750 | 0.0638
Epoch 57/300, seasonal_1 Loss: 0.0758 | 0.0490
Epoch 58/300, seasonal_1 Loss: 0.0761 | 0.0742
Epoch 59/300, seasonal_1 Loss: 0.0788 | 0.0398
Epoch 60/300, seasonal_1 Loss: 0.0761 | 0.0515
Epoch 61/300, seasonal_1 Loss: 0.0770 | 0.0673
Epoch 62/300, seasonal_1 Loss: 0.0765 | 0.0474
Epoch 63/300, seasonal_1 Loss: 0.0765 | 0.0745
Epoch 64/300, seasonal_1 Loss: 0.0843 | 0.0401
Epoch 65/300, seasonal_1 Loss: 0.0823 | 0.0668
Epoch 66/300, seasonal_1 Loss: 0.0802 | 0.0400
Epoch 67/300, seasonal_1 Loss: 0.0806 | 0.0749
Epoch 68/300, seasonal_1 Loss: 0.0886 | 0.0482
Epoch 69/300, seasonal_1 Loss: 0.0861 | 0.0623
Epoch 70/300, seasonal_1 Loss: 0.0882 | 0.0492
Epoch 71/300, seasonal_1 Loss: 0.0941 | 0.0944
Epoch 72/300, seasonal_1 Loss: 0.1034 | 0.0891
Epoch 73/300, seasonal_1 Loss: 0.0987 | 0.0525
Epoch 74/300, seasonal_1 Loss: 0.0885 | 0.0615
Epoch 75/300, seasonal_1 Loss: 0.0806 | 0.0475
Epoch 76/300, seasonal_1 Loss: 0.0735 | 0.0564
Epoch 77/300, seasonal_1 Loss: 0.0717 | 0.0507
Epoch 78/300, seasonal_1 Loss: 0.0700 | 0.0459
Epoch 79/300, seasonal_1 Loss: 0.0684 | 0.0564
Epoch 80/300, seasonal_1 Loss: 0.0713 | 0.0490
Epoch 81/300, seasonal_1 Loss: 0.0725 | 0.0587
Epoch 82/300, seasonal_1 Loss: 0.0759 | 0.0560
Epoch 83/300, seasonal_1 Loss: 0.0720 | 0.0514
Epoch 84/300, seasonal_1 Loss: 0.0715 | 0.0637
Epoch 85/300, seasonal_1 Loss: 0.0693 | 0.0529
Epoch 86/300, seasonal_1 Loss: 0.0696 | 0.0569
Epoch 87/300, seasonal_1 Loss: 0.0666 | 0.0441
Epoch 88/300, seasonal_1 Loss: 0.0623 | 0.0525
Epoch 89/300, seasonal_1 Loss: 0.0623 | 0.0506
Epoch 90/300, seasonal_1 Loss: 0.0628 | 0.0500
Epoch 91/300, seasonal_1 Loss: 0.0638 | 0.0468
Epoch 92/300, seasonal_1 Loss: 0.0617 | 0.0478
Epoch 93/300, seasonal_1 Loss: 0.0612 | 0.0480
Epoch 94/300, seasonal_1 Loss: 0.0622 | 0.0471
Epoch 95/300, seasonal_1 Loss: 0.0614 | 0.0441
Epoch 96/300, seasonal_1 Loss: 0.0610 | 0.0441
Epoch 97/300, seasonal_1 Loss: 0.0603 | 0.0472
Epoch 98/300, seasonal_1 Loss: 0.0605 | 0.0434
Epoch 99/300, seasonal_1 Loss: 0.0609 | 0.0456
Epoch 100/300, seasonal_1 Loss: 0.0602 | 0.0453
Epoch 101/300, seasonal_1 Loss: 0.0598 | 0.0457
Epoch 102/300, seasonal_1 Loss: 0.0603 | 0.0451
Epoch 103/300, seasonal_1 Loss: 0.0602 | 0.0448
Epoch 104/300, seasonal_1 Loss: 0.0599 | 0.0443
Epoch 105/300, seasonal_1 Loss: 0.0605 | 0.0452
Epoch 106/300, seasonal_1 Loss: 0.0598 | 0.0457
Epoch 107/300, seasonal_1 Loss: 0.0598 | 0.0455
Epoch 108/300, seasonal_1 Loss: 0.0607 | 0.0436
Epoch 109/300, seasonal_1 Loss: 0.0595 | 0.0441
Epoch 110/300, seasonal_1 Loss: 0.0598 | 0.0451
Epoch 111/300, seasonal_1 Loss: 0.0600 | 0.0459
Epoch 112/300, seasonal_1 Loss: 0.0597 | 0.0454
Epoch 113/300, seasonal_1 Loss: 0.0600 | 0.0447
Epoch 114/300, seasonal_1 Loss: 0.0596 | 0.0436
Epoch 115/300, seasonal_1 Loss: 0.0594 | 0.0447
Epoch 116/300, seasonal_1 Loss: 0.0589 | 0.0451
Epoch 117/300, seasonal_1 Loss: 0.0587 | 0.0448
Epoch 118/300, seasonal_1 Loss: 0.0586 | 0.0442
Epoch 119/300, seasonal_1 Loss: 0.0588 | 0.0447
Epoch 120/300, seasonal_1 Loss: 0.0585 | 0.0440
Epoch 121/300, seasonal_1 Loss: 0.0589 | 0.0443
Epoch 122/300, seasonal_1 Loss: 0.0585 | 0.0460
Epoch 123/300, seasonal_1 Loss: 0.0586 | 0.0448
Epoch 124/300, seasonal_1 Loss: 0.0591 | 0.0460
Epoch 125/300, seasonal_1 Loss: 0.0594 | 0.0450
Epoch 126/300, seasonal_1 Loss: 0.0598 | 0.0476
Epoch 127/300, seasonal_1 Loss: 0.0588 | 0.0445
Epoch 128/300, seasonal_1 Loss: 0.0583 | 0.0486
Epoch 129/300, seasonal_1 Loss: 0.0578 | 0.0436
Epoch 130/300, seasonal_1 Loss: 0.0570 | 0.0457
Epoch 131/300, seasonal_1 Loss: 0.0569 | 0.0443
Epoch 132/300, seasonal_1 Loss: 0.0564 | 0.0454
Epoch 133/300, seasonal_1 Loss: 0.0560 | 0.0431
Epoch 134/300, seasonal_1 Loss: 0.0556 | 0.0452
Epoch 135/300, seasonal_1 Loss: 0.0556 | 0.0427
Epoch 136/300, seasonal_1 Loss: 0.0553 | 0.0448
Epoch 137/300, seasonal_1 Loss: 0.0553 | 0.0434
Epoch 138/300, seasonal_1 Loss: 0.0550 | 0.0443
Epoch 139/300, seasonal_1 Loss: 0.0554 | 0.0428
Epoch 140/300, seasonal_1 Loss: 0.0553 | 0.0435
Epoch 141/300, seasonal_1 Loss: 0.0553 | 0.0431
Epoch 142/300, seasonal_1 Loss: 0.0547 | 0.0441
Epoch 143/300, seasonal_1 Loss: 0.0545 | 0.0423
Epoch 144/300, seasonal_1 Loss: 0.0544 | 0.0424
Epoch 145/300, seasonal_1 Loss: 0.0541 | 0.0421
Epoch 146/300, seasonal_1 Loss: 0.0540 | 0.0427
Epoch 147/300, seasonal_1 Loss: 0.0538 | 0.0422
Epoch 148/300, seasonal_1 Loss: 0.0539 | 0.0417
Epoch 149/300, seasonal_1 Loss: 0.0537 | 0.0417
Epoch 150/300, seasonal_1 Loss: 0.0537 | 0.0421
Epoch 151/300, seasonal_1 Loss: 0.0534 | 0.0422
Epoch 152/300, seasonal_1 Loss: 0.0534 | 0.0415
Epoch 153/300, seasonal_1 Loss: 0.0533 | 0.0412
Epoch 154/300, seasonal_1 Loss: 0.0532 | 0.0417
Epoch 155/300, seasonal_1 Loss: 0.0531 | 0.0418
Epoch 156/300, seasonal_1 Loss: 0.0530 | 0.0413
Epoch 157/300, seasonal_1 Loss: 0.0532 | 0.0409
Epoch 158/300, seasonal_1 Loss: 0.0528 | 0.0412
Epoch 159/300, seasonal_1 Loss: 0.0529 | 0.0414
Epoch 160/300, seasonal_1 Loss: 0.0527 | 0.0412
Epoch 161/300, seasonal_1 Loss: 0.0528 | 0.0408
Epoch 162/300, seasonal_1 Loss: 0.0527 | 0.0408
Epoch 163/300, seasonal_1 Loss: 0.0527 | 0.0411
Epoch 164/300, seasonal_1 Loss: 0.0524 | 0.0412
Epoch 165/300, seasonal_1 Loss: 0.0525 | 0.0406
Epoch 166/300, seasonal_1 Loss: 0.0524 | 0.0404
Epoch 167/300, seasonal_1 Loss: 0.0523 | 0.0409
Epoch 168/300, seasonal_1 Loss: 0.0522 | 0.0409
Epoch 169/300, seasonal_1 Loss: 0.0521 | 0.0404
Epoch 170/300, seasonal_1 Loss: 0.0521 | 0.0402
Epoch 171/300, seasonal_1 Loss: 0.0520 | 0.0407
Epoch 172/300, seasonal_1 Loss: 0.0519 | 0.0406
Epoch 173/300, seasonal_1 Loss: 0.0518 | 0.0403
Epoch 174/300, seasonal_1 Loss: 0.0518 | 0.0400
Epoch 175/300, seasonal_1 Loss: 0.0517 | 0.0404
Epoch 176/300, seasonal_1 Loss: 0.0516 | 0.0405
Epoch 177/300, seasonal_1 Loss: 0.0515 | 0.0402
Epoch 178/300, seasonal_1 Loss: 0.0515 | 0.0399
Epoch 179/300, seasonal_1 Loss: 0.0514 | 0.0402
Epoch 180/300, seasonal_1 Loss: 0.0514 | 0.0404
Epoch 181/300, seasonal_1 Loss: 0.0512 | 0.0401
Epoch 182/300, seasonal_1 Loss: 0.0512 | 0.0398
Epoch 183/300, seasonal_1 Loss: 0.0511 | 0.0400
Epoch 184/300, seasonal_1 Loss: 0.0510 | 0.0403
Epoch 185/300, seasonal_1 Loss: 0.0509 | 0.0400
Epoch 186/300, seasonal_1 Loss: 0.0509 | 0.0397
Epoch 187/300, seasonal_1 Loss: 0.0507 | 0.0398
Epoch 188/300, seasonal_1 Loss: 0.0507 | 0.0401
Epoch 189/300, seasonal_1 Loss: 0.0505 | 0.0398
Epoch 190/300, seasonal_1 Loss: 0.0504 | 0.0396
Epoch 191/300, seasonal_1 Loss: 0.0502 | 0.0397
Epoch 192/300, seasonal_1 Loss: 0.0501 | 0.0399
Epoch 193/300, seasonal_1 Loss: 0.0498 | 0.0396
Epoch 194/300, seasonal_1 Loss: 0.0496 | 0.0394
Epoch 195/300, seasonal_1 Loss: 0.0493 | 0.0395
Epoch 196/300, seasonal_1 Loss: 0.0490 | 0.0397
Epoch 197/300, seasonal_1 Loss: 0.0486 | 0.0394
Epoch 198/300, seasonal_1 Loss: 0.0481 | 0.0393
Epoch 199/300, seasonal_1 Loss: 0.0475 | 0.0393
Epoch 200/300, seasonal_1 Loss: 0.0470 | 0.0394
Epoch 201/300, seasonal_1 Loss: 0.0464 | 0.0391
Epoch 202/300, seasonal_1 Loss: 0.0459 | 0.0391
Epoch 203/300, seasonal_1 Loss: 0.0456 | 0.0390
Epoch 204/300, seasonal_1 Loss: 0.0453 | 0.0395
Epoch 205/300, seasonal_1 Loss: 0.0451 | 0.0384
Epoch 206/300, seasonal_1 Loss: 0.0447 | 0.0400
Epoch 207/300, seasonal_1 Loss: 0.0444 | 0.0384
Epoch 208/300, seasonal_1 Loss: 0.0441 | 0.0397
Epoch 209/300, seasonal_1 Loss: 0.0438 | 0.0389
Epoch 210/300, seasonal_1 Loss: 0.0436 | 0.0391
Epoch 211/300, seasonal_1 Loss: 0.0434 | 0.0391
Epoch 212/300, seasonal_1 Loss: 0.0433 | 0.0392
Epoch 213/300, seasonal_1 Loss: 0.0431 | 0.0391
Epoch 214/300, seasonal_1 Loss: 0.0430 | 0.0391
Epoch 215/300, seasonal_1 Loss: 0.0429 | 0.0392
Epoch 216/300, seasonal_1 Loss: 0.0428 | 0.0392
Epoch 217/300, seasonal_1 Loss: 0.0427 | 0.0391
Epoch 218/300, seasonal_1 Loss: 0.0426 | 0.0391
Epoch 219/300, seasonal_1 Loss: 0.0425 | 0.0392
Epoch 220/300, seasonal_1 Loss: 0.0424 | 0.0391
Epoch 221/300, seasonal_1 Loss: 0.0423 | 0.0391
Epoch 222/300, seasonal_1 Loss: 0.0423 | 0.0391
Epoch 223/300, seasonal_1 Loss: 0.0422 | 0.0391
Epoch 224/300, seasonal_1 Loss: 0.0421 | 0.0391
Epoch 225/300, seasonal_1 Loss: 0.0421 | 0.0391
Epoch 226/300, seasonal_1 Loss: 0.0420 | 0.0391
Epoch 227/300, seasonal_1 Loss: 0.0420 | 0.0391
Epoch 228/300, seasonal_1 Loss: 0.0419 | 0.0391
Epoch 229/300, seasonal_1 Loss: 0.0419 | 0.0391
Epoch 230/300, seasonal_1 Loss: 0.0418 | 0.0391
Epoch 231/300, seasonal_1 Loss: 0.0418 | 0.0391
Epoch 232/300, seasonal_1 Loss: 0.0417 | 0.0390
Epoch 233/300, seasonal_1 Loss: 0.0417 | 0.0390
Epoch 234/300, seasonal_1 Loss: 0.0416 | 0.0390
Epoch 235/300, seasonal_1 Loss: 0.0416 | 0.0390
Epoch 236/300, seasonal_1 Loss: 0.0415 | 0.0390
Epoch 237/300, seasonal_1 Loss: 0.0415 | 0.0390
Epoch 238/300, seasonal_1 Loss: 0.0415 | 0.0390
Epoch 239/300, seasonal_1 Loss: 0.0414 | 0.0390
Epoch 240/300, seasonal_1 Loss: 0.0414 | 0.0390
Epoch 241/300, seasonal_1 Loss: 0.0413 | 0.0390
Epoch 242/300, seasonal_1 Loss: 0.0413 | 0.0389
Epoch 243/300, seasonal_1 Loss: 0.0413 | 0.0389
Epoch 244/300, seasonal_1 Loss: 0.0412 | 0.0389
Epoch 245/300, seasonal_1 Loss: 0.0412 | 0.0389
Epoch 246/300, seasonal_1 Loss: 0.0412 | 0.0389
Epoch 247/300, seasonal_1 Loss: 0.0411 | 0.0389
Epoch 248/300, seasonal_1 Loss: 0.0411 | 0.0389
Epoch 249/300, seasonal_1 Loss: 0.0411 | 0.0389
Epoch 250/300, seasonal_1 Loss: 0.0411 | 0.0389
Epoch 251/300, seasonal_1 Loss: 0.0410 | 0.0389
Epoch 252/300, seasonal_1 Loss: 0.0410 | 0.0388
Epoch 253/300, seasonal_1 Loss: 0.0410 | 0.0388
Epoch 254/300, seasonal_1 Loss: 0.0409 | 0.0388
Epoch 255/300, seasonal_1 Loss: 0.0409 | 0.0388
Epoch 256/300, seasonal_1 Loss: 0.0409 | 0.0388
Epoch 257/300, seasonal_1 Loss: 0.0409 | 0.0388
Epoch 258/300, seasonal_1 Loss: 0.0408 | 0.0388
Epoch 259/300, seasonal_1 Loss: 0.0408 | 0.0388
Epoch 260/300, seasonal_1 Loss: 0.0408 | 0.0388
Epoch 261/300, seasonal_1 Loss: 0.0408 | 0.0388
Epoch 262/300, seasonal_1 Loss: 0.0408 | 0.0387
Epoch 263/300, seasonal_1 Loss: 0.0407 | 0.0387
Epoch 264/300, seasonal_1 Loss: 0.0407 | 0.0387
Epoch 265/300, seasonal_1 Loss: 0.0407 | 0.0387
Epoch 266/300, seasonal_1 Loss: 0.0407 | 0.0387
Epoch 267/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 268/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 269/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 270/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 271/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 272/300, seasonal_1 Loss: 0.0406 | 0.0387
Epoch 273/300, seasonal_1 Loss: 0.0405 | 0.0387
Epoch 274/300, seasonal_1 Loss: 0.0405 | 0.0386
Epoch 275/300, seasonal_1 Loss: 0.0405 | 0.0386
Epoch 276/300, seasonal_1 Loss: 0.0405 | 0.0386
Epoch 277/300, seasonal_1 Loss: 0.0405 | 0.0386
Epoch 278/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 279/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 280/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 281/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 282/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 283/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 284/300, seasonal_1 Loss: 0.0404 | 0.0386
Epoch 285/300, seasonal_1 Loss: 0.0403 | 0.0386
Epoch 286/300, seasonal_1 Loss: 0.0403 | 0.0386
Epoch 287/300, seasonal_1 Loss: 0.0403 | 0.0386
Epoch 288/300, seasonal_1 Loss: 0.0403 | 0.0385
Epoch 289/300, seasonal_1 Loss: 0.0403 | 0.0385
Epoch 290/300, seasonal_1 Loss: 0.0403 | 0.0385
Epoch 291/300, seasonal_1 Loss: 0.0403 | 0.0385
Epoch 292/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 293/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 294/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 295/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 296/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 297/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 298/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 299/300, seasonal_1 Loss: 0.0402 | 0.0385
Epoch 300/300, seasonal_1 Loss: 0.0401 | 0.0385
Training seasonal_2 component with params: {'observation_period_num': 17, 'train_rates': 0.8896248267181686, 'learning_rate': 6.491009699002938e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8627617187175196}
Epoch 1/300, seasonal_2 Loss: 0.5188 | 0.2048
Epoch 2/300, seasonal_2 Loss: 0.2235 | 0.1616
Epoch 3/300, seasonal_2 Loss: 0.1464 | 0.0910
Epoch 4/300, seasonal_2 Loss: 0.1346 | 0.1151
Epoch 5/300, seasonal_2 Loss: 0.1261 | 0.1096
Epoch 6/300, seasonal_2 Loss: 0.1299 | 0.2406
Epoch 7/300, seasonal_2 Loss: 0.1259 | 0.2093
Epoch 8/300, seasonal_2 Loss: 0.1425 | 0.2442
Epoch 9/300, seasonal_2 Loss: 0.1301 | 0.2420
Epoch 10/300, seasonal_2 Loss: 0.1375 | 0.1289
Epoch 11/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 12/300, seasonal_2 Loss: 0.1685 | 0.1030
Epoch 13/300, seasonal_2 Loss: 0.1880 | 0.3349
Epoch 14/300, seasonal_2 Loss: 0.1565 | 0.1626
Epoch 15/300, seasonal_2 Loss: 0.1505 | 0.1479
Epoch 16/300, seasonal_2 Loss: 0.1468 | 0.0959
Epoch 17/300, seasonal_2 Loss: 0.1229 | 0.0703
Epoch 18/300, seasonal_2 Loss: 0.1042 | 0.0774
Epoch 19/300, seasonal_2 Loss: 0.1031 | 0.0876
Epoch 20/300, seasonal_2 Loss: 0.1080 | 0.0735
Epoch 21/300, seasonal_2 Loss: 0.1017 | 0.0755
Epoch 22/300, seasonal_2 Loss: 0.1042 | 0.0796
Epoch 23/300, seasonal_2 Loss: 0.1067 | 0.0771
Epoch 24/300, seasonal_2 Loss: 0.1244 | 0.0802
Epoch 25/300, seasonal_2 Loss: 0.1218 | 0.0751
Epoch 26/300, seasonal_2 Loss: 0.1141 | 0.0696
Epoch 27/300, seasonal_2 Loss: 0.1019 | 0.0727
Epoch 28/300, seasonal_2 Loss: 0.1013 | 0.0873
Epoch 29/300, seasonal_2 Loss: 0.1084 | 0.0784
Epoch 30/300, seasonal_2 Loss: 0.1015 | 0.0743
Epoch 31/300, seasonal_2 Loss: 0.0955 | 0.0704
Epoch 32/300, seasonal_2 Loss: 0.0957 | 0.0765
Epoch 33/300, seasonal_2 Loss: 0.1002 | 0.0866
Epoch 34/300, seasonal_2 Loss: 0.0982 | 0.0691
Epoch 35/300, seasonal_2 Loss: 0.0967 | 0.0538
Epoch 36/300, seasonal_2 Loss: 0.0872 | 0.0536
Epoch 37/300, seasonal_2 Loss: 0.0894 | 0.0663
Epoch 38/300, seasonal_2 Loss: 0.0933 | 0.0623
Epoch 39/300, seasonal_2 Loss: 0.0866 | 0.0541
Epoch 40/300, seasonal_2 Loss: 0.0948 | 0.0576
Epoch 41/300, seasonal_2 Loss: 0.0822 | 0.0587
Epoch 42/300, seasonal_2 Loss: 0.0794 | 0.0595
Epoch 43/300, seasonal_2 Loss: 0.0770 | 0.0562
Epoch 44/300, seasonal_2 Loss: 0.0727 | 0.0525
Epoch 45/300, seasonal_2 Loss: 0.0723 | 0.0537
Epoch 46/300, seasonal_2 Loss: 0.0735 | 0.0544
Epoch 47/300, seasonal_2 Loss: 0.0717 | 0.0514
Epoch 48/300, seasonal_2 Loss: 0.0714 | 0.0512
Epoch 49/300, seasonal_2 Loss: 0.0711 | 0.0512
Epoch 50/300, seasonal_2 Loss: 0.0709 | 0.0513
Epoch 51/300, seasonal_2 Loss: 0.0698 | 0.0512
Epoch 52/300, seasonal_2 Loss: 0.0690 | 0.0501
Epoch 53/300, seasonal_2 Loss: 0.0689 | 0.0495
Epoch 54/300, seasonal_2 Loss: 0.0704 | 0.0507
Epoch 55/300, seasonal_2 Loss: 0.0709 | 0.0510
Epoch 56/300, seasonal_2 Loss: 0.0705 | 0.0500
Epoch 57/300, seasonal_2 Loss: 0.0694 | 0.0494
Epoch 58/300, seasonal_2 Loss: 0.0712 | 0.0519
Epoch 59/300, seasonal_2 Loss: 0.0700 | 0.0508
Epoch 60/300, seasonal_2 Loss: 0.0674 | 0.0504
Epoch 61/300, seasonal_2 Loss: 0.0686 | 0.0543
Epoch 62/300, seasonal_2 Loss: 0.0677 | 0.0512
Epoch 63/300, seasonal_2 Loss: 0.0672 | 0.0489
Epoch 64/300, seasonal_2 Loss: 0.0667 | 0.0480
Epoch 65/300, seasonal_2 Loss: 0.0655 | 0.0481
Epoch 66/300, seasonal_2 Loss: 0.0657 | 0.0491
Epoch 67/300, seasonal_2 Loss: 0.0671 | 0.0495
Epoch 68/300, seasonal_2 Loss: 0.0671 | 0.0492
Epoch 69/300, seasonal_2 Loss: 0.0653 | 0.0495
Epoch 70/300, seasonal_2 Loss: 0.0650 | 0.0492
Epoch 71/300, seasonal_2 Loss: 0.0665 | 0.0482
Epoch 72/300, seasonal_2 Loss: 0.0676 | 0.0484
Epoch 73/300, seasonal_2 Loss: 0.0652 | 0.0488
Epoch 74/300, seasonal_2 Loss: 0.0640 | 0.0494
Epoch 75/300, seasonal_2 Loss: 0.0659 | 0.0493
Epoch 76/300, seasonal_2 Loss: 0.0662 | 0.0492
Epoch 77/300, seasonal_2 Loss: 0.0646 | 0.0479
Epoch 78/300, seasonal_2 Loss: 0.0641 | 0.0463
Epoch 79/300, seasonal_2 Loss: 0.0654 | 0.0467
Epoch 80/300, seasonal_2 Loss: 0.0634 | 0.0483
Epoch 81/300, seasonal_2 Loss: 0.0626 | 0.0506
Epoch 82/300, seasonal_2 Loss: 0.0644 | 0.0496
Epoch 83/300, seasonal_2 Loss: 0.0637 | 0.0476
Epoch 84/300, seasonal_2 Loss: 0.0623 | 0.0454
Epoch 85/300, seasonal_2 Loss: 0.0633 | 0.0449
Epoch 86/300, seasonal_2 Loss: 0.0633 | 0.0469
Epoch 87/300, seasonal_2 Loss: 0.0615 | 0.0499
Epoch 88/300, seasonal_2 Loss: 0.0621 | 0.0495
Epoch 89/300, seasonal_2 Loss: 0.0617 | 0.0474
Epoch 90/300, seasonal_2 Loss: 0.0607 | 0.0456
Epoch 91/300, seasonal_2 Loss: 0.0609 | 0.0443
Epoch 92/300, seasonal_2 Loss: 0.0607 | 0.0458
Epoch 93/300, seasonal_2 Loss: 0.0598 | 0.0477
Epoch 94/300, seasonal_2 Loss: 0.0597 | 0.0477
Epoch 95/300, seasonal_2 Loss: 0.0596 | 0.0465
Epoch 96/300, seasonal_2 Loss: 0.0594 | 0.0451
Epoch 97/300, seasonal_2 Loss: 0.0592 | 0.0448
Epoch 98/300, seasonal_2 Loss: 0.0591 | 0.0456
Epoch 99/300, seasonal_2 Loss: 0.0588 | 0.0463
Epoch 100/300, seasonal_2 Loss: 0.0587 | 0.0462
Epoch 101/300, seasonal_2 Loss: 0.0586 | 0.0456
Epoch 102/300, seasonal_2 Loss: 0.0585 | 0.0452
Epoch 103/300, seasonal_2 Loss: 0.0583 | 0.0452
Epoch 104/300, seasonal_2 Loss: 0.0582 | 0.0453
Epoch 105/300, seasonal_2 Loss: 0.0581 | 0.0454
Epoch 106/300, seasonal_2 Loss: 0.0580 | 0.0453
Epoch 107/300, seasonal_2 Loss: 0.0579 | 0.0452
Epoch 108/300, seasonal_2 Loss: 0.0578 | 0.0451
Epoch 109/300, seasonal_2 Loss: 0.0577 | 0.0450
Epoch 110/300, seasonal_2 Loss: 0.0576 | 0.0450
Epoch 111/300, seasonal_2 Loss: 0.0575 | 0.0448
Epoch 112/300, seasonal_2 Loss: 0.0574 | 0.0448
Epoch 113/300, seasonal_2 Loss: 0.0574 | 0.0448
Epoch 114/300, seasonal_2 Loss: 0.0573 | 0.0448
Epoch 115/300, seasonal_2 Loss: 0.0572 | 0.0447
Epoch 116/300, seasonal_2 Loss: 0.0571 | 0.0446
Epoch 117/300, seasonal_2 Loss: 0.0570 | 0.0445
Epoch 118/300, seasonal_2 Loss: 0.0569 | 0.0445
Epoch 119/300, seasonal_2 Loss: 0.0569 | 0.0445
Epoch 120/300, seasonal_2 Loss: 0.0568 | 0.0444
Epoch 121/300, seasonal_2 Loss: 0.0567 | 0.0443
Epoch 122/300, seasonal_2 Loss: 0.0566 | 0.0443
Epoch 123/300, seasonal_2 Loss: 0.0566 | 0.0443
Epoch 124/300, seasonal_2 Loss: 0.0565 | 0.0442
Epoch 125/300, seasonal_2 Loss: 0.0564 | 0.0442
Epoch 126/300, seasonal_2 Loss: 0.0563 | 0.0441
Epoch 127/300, seasonal_2 Loss: 0.0563 | 0.0441
Epoch 128/300, seasonal_2 Loss: 0.0562 | 0.0440
Epoch 129/300, seasonal_2 Loss: 0.0561 | 0.0440
Epoch 130/300, seasonal_2 Loss: 0.0561 | 0.0440
Epoch 131/300, seasonal_2 Loss: 0.0560 | 0.0439
Epoch 132/300, seasonal_2 Loss: 0.0560 | 0.0439
Epoch 133/300, seasonal_2 Loss: 0.0559 | 0.0439
Epoch 134/300, seasonal_2 Loss: 0.0558 | 0.0438
Epoch 135/300, seasonal_2 Loss: 0.0558 | 0.0438
Epoch 136/300, seasonal_2 Loss: 0.0557 | 0.0437
Epoch 137/300, seasonal_2 Loss: 0.0557 | 0.0437
Epoch 138/300, seasonal_2 Loss: 0.0556 | 0.0437
Epoch 139/300, seasonal_2 Loss: 0.0556 | 0.0437
Epoch 140/300, seasonal_2 Loss: 0.0555 | 0.0437
Epoch 141/300, seasonal_2 Loss: 0.0554 | 0.0436
Epoch 142/300, seasonal_2 Loss: 0.0554 | 0.0436
Epoch 143/300, seasonal_2 Loss: 0.0554 | 0.0436
Epoch 144/300, seasonal_2 Loss: 0.0553 | 0.0436
Epoch 145/300, seasonal_2 Loss: 0.0553 | 0.0435
Epoch 146/300, seasonal_2 Loss: 0.0552 | 0.0435
Epoch 147/300, seasonal_2 Loss: 0.0552 | 0.0435
Epoch 148/300, seasonal_2 Loss: 0.0551 | 0.0435
Epoch 149/300, seasonal_2 Loss: 0.0551 | 0.0435
Epoch 150/300, seasonal_2 Loss: 0.0550 | 0.0434
Epoch 151/300, seasonal_2 Loss: 0.0550 | 0.0434
Epoch 152/300, seasonal_2 Loss: 0.0550 | 0.0434
Epoch 153/300, seasonal_2 Loss: 0.0549 | 0.0434
Epoch 154/300, seasonal_2 Loss: 0.0549 | 0.0434
Epoch 155/300, seasonal_2 Loss: 0.0548 | 0.0434
Epoch 156/300, seasonal_2 Loss: 0.0548 | 0.0433
Epoch 157/300, seasonal_2 Loss: 0.0548 | 0.0433
Epoch 158/300, seasonal_2 Loss: 0.0547 | 0.0433
Epoch 159/300, seasonal_2 Loss: 0.0547 | 0.0433
Epoch 160/300, seasonal_2 Loss: 0.0547 | 0.0433
Epoch 161/300, seasonal_2 Loss: 0.0546 | 0.0433
Epoch 162/300, seasonal_2 Loss: 0.0546 | 0.0433
Epoch 163/300, seasonal_2 Loss: 0.0546 | 0.0433
Epoch 164/300, seasonal_2 Loss: 0.0546 | 0.0433
Epoch 165/300, seasonal_2 Loss: 0.0545 | 0.0433
Epoch 166/300, seasonal_2 Loss: 0.0545 | 0.0432
Epoch 167/300, seasonal_2 Loss: 0.0545 | 0.0432
Epoch 168/300, seasonal_2 Loss: 0.0544 | 0.0432
Epoch 169/300, seasonal_2 Loss: 0.0544 | 0.0432
Epoch 170/300, seasonal_2 Loss: 0.0544 | 0.0432
Epoch 171/300, seasonal_2 Loss: 0.0544 | 0.0432
Epoch 172/300, seasonal_2 Loss: 0.0543 | 0.0432
Epoch 173/300, seasonal_2 Loss: 0.0543 | 0.0432
Epoch 174/300, seasonal_2 Loss: 0.0543 | 0.0432
Epoch 175/300, seasonal_2 Loss: 0.0543 | 0.0432
Epoch 176/300, seasonal_2 Loss: 0.0543 | 0.0432
Epoch 177/300, seasonal_2 Loss: 0.0542 | 0.0432
Epoch 178/300, seasonal_2 Loss: 0.0542 | 0.0432
Epoch 179/300, seasonal_2 Loss: 0.0542 | 0.0432
Epoch 180/300, seasonal_2 Loss: 0.0542 | 0.0432
Epoch 181/300, seasonal_2 Loss: 0.0542 | 0.0431
Epoch 182/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 183/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 184/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 185/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 186/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 187/300, seasonal_2 Loss: 0.0541 | 0.0431
Epoch 188/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 189/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 190/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 191/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 192/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 193/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 194/300, seasonal_2 Loss: 0.0540 | 0.0431
Epoch 195/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 196/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 197/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 198/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 199/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 200/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 201/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 202/300, seasonal_2 Loss: 0.0539 | 0.0431
Epoch 203/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 204/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 205/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 206/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 207/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 208/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 209/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 210/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 211/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 212/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 213/300, seasonal_2 Loss: 0.0538 | 0.0431
Epoch 214/300, seasonal_2 Loss: 0.0537 | 0.0431
Epoch 215/300, seasonal_2 Loss: 0.0537 | 0.0431
Epoch 216/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 217/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 218/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 219/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 220/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 221/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 222/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 223/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 224/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 225/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 226/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 227/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 228/300, seasonal_2 Loss: 0.0537 | 0.0430
Epoch 229/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 230/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 231/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 232/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 233/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 234/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 235/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 236/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 237/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 238/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 239/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 240/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 241/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 242/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 243/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 244/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 245/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 246/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 247/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 248/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 249/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 250/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 251/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 252/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 253/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 254/300, seasonal_2 Loss: 0.0536 | 0.0430
Epoch 255/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 256/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 257/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 258/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 259/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 260/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 261/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 262/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 263/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 264/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 265/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 266/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 267/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 268/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 269/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 270/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 271/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 272/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 273/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 274/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 275/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 276/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 277/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 278/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 279/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 280/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 281/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 282/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 283/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 284/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 285/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 286/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 287/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 288/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 289/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 290/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 291/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 292/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 293/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 294/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 295/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 296/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 297/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 298/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 299/300, seasonal_2 Loss: 0.0535 | 0.0430
Epoch 300/300, seasonal_2 Loss: 0.0535 | 0.0430
Training seasonal_3 component with params: {'observation_period_num': 20, 'train_rates': 0.9803507984693185, 'learning_rate': 8.825352304488835e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8901025916688151}
Epoch 1/300, seasonal_3 Loss: 0.8409 | 0.1673
Epoch 2/300, seasonal_3 Loss: 0.2257 | 0.1607
Epoch 3/300, seasonal_3 Loss: 0.1927 | 0.1265
Epoch 4/300, seasonal_3 Loss: 0.1723 | 0.0873
Epoch 5/300, seasonal_3 Loss: 0.1432 | 0.1005
Epoch 6/300, seasonal_3 Loss: 0.1502 | 0.1305
Epoch 7/300, seasonal_3 Loss: 0.1450 | 0.1287
Epoch 8/300, seasonal_3 Loss: 0.1403 | 0.0879
Epoch 9/300, seasonal_3 Loss: 0.1397 | 0.0763
Epoch 10/300, seasonal_3 Loss: 0.1600 | 0.0861
Epoch 11/300, seasonal_3 Loss: 0.1666 | 0.0709
Epoch 12/300, seasonal_3 Loss: 0.1790 | 0.1631
Epoch 13/300, seasonal_3 Loss: 0.1758 | 0.0781
Epoch 14/300, seasonal_3 Loss: 0.2010 | 0.0679
Epoch 15/300, seasonal_3 Loss: 0.2054 | 0.0992
Epoch 16/300, seasonal_3 Loss: 0.1974 | 0.1063
Epoch 17/300, seasonal_3 Loss: 0.2183 | 0.0752
Epoch 18/300, seasonal_3 Loss: 0.1963 | 0.1004
Epoch 19/300, seasonal_3 Loss: 0.2005 | 0.2570
Epoch 20/300, seasonal_3 Loss: 0.1520 | 0.2885
Epoch 21/300, seasonal_3 Loss: 0.1501 | 0.0850
Epoch 22/300, seasonal_3 Loss: 0.1134 | 0.0772
Epoch 23/300, seasonal_3 Loss: 0.1104 | 0.1003
Epoch 24/300, seasonal_3 Loss: 0.1365 | 0.1345
Epoch 25/300, seasonal_3 Loss: 0.1222 | 0.0636
Epoch 26/300, seasonal_3 Loss: 0.1157 | 0.0603
Epoch 27/300, seasonal_3 Loss: 0.1070 | 0.0535
Epoch 28/300, seasonal_3 Loss: 0.1322 | 0.0865
Epoch 29/300, seasonal_3 Loss: 0.1442 | 0.1596
Epoch 30/300, seasonal_3 Loss: 0.1111 | 0.0982
Epoch 31/300, seasonal_3 Loss: 0.1562 | 0.0567
Epoch 32/300, seasonal_3 Loss: 0.1381 | 0.0534
Epoch 33/300, seasonal_3 Loss: 0.1204 | 0.0916
Epoch 34/300, seasonal_3 Loss: 0.1052 | 0.0957
Epoch 35/300, seasonal_3 Loss: 0.1068 | 0.0513
Epoch 36/300, seasonal_3 Loss: 0.0887 | 0.0449
Epoch 37/300, seasonal_3 Loss: 0.0944 | 0.0518
Epoch 38/300, seasonal_3 Loss: 0.0817 | 0.0479
Epoch 39/300, seasonal_3 Loss: 0.0806 | 0.0472
Epoch 40/300, seasonal_3 Loss: 0.0776 | 0.0479
Epoch 41/300, seasonal_3 Loss: 0.0763 | 0.0480
Epoch 42/300, seasonal_3 Loss: 0.0767 | 0.0394
Epoch 43/300, seasonal_3 Loss: 0.0751 | 0.0371
Epoch 44/300, seasonal_3 Loss: 0.0735 | 0.0408
Epoch 45/300, seasonal_3 Loss: 0.0732 | 0.0482
Epoch 46/300, seasonal_3 Loss: 0.0730 | 0.0417
Epoch 47/300, seasonal_3 Loss: 0.0718 | 0.0362
Epoch 48/300, seasonal_3 Loss: 0.0709 | 0.0354
Epoch 49/300, seasonal_3 Loss: 0.0697 | 0.0404
Epoch 50/300, seasonal_3 Loss: 0.0701 | 0.0391
Epoch 51/300, seasonal_3 Loss: 0.0688 | 0.0353
Epoch 52/300, seasonal_3 Loss: 0.0683 | 0.0352
Epoch 53/300, seasonal_3 Loss: 0.0678 | 0.0371
Epoch 54/300, seasonal_3 Loss: 0.0674 | 0.0360
Epoch 55/300, seasonal_3 Loss: 0.0671 | 0.0341
Epoch 56/300, seasonal_3 Loss: 0.0666 | 0.0339
Epoch 57/300, seasonal_3 Loss: 0.0664 | 0.0344
Epoch 58/300, seasonal_3 Loss: 0.0660 | 0.0338
Epoch 59/300, seasonal_3 Loss: 0.0658 | 0.0339
Epoch 60/300, seasonal_3 Loss: 0.0656 | 0.0345
Epoch 61/300, seasonal_3 Loss: 0.0654 | 0.0339
Epoch 62/300, seasonal_3 Loss: 0.0650 | 0.0326
Epoch 63/300, seasonal_3 Loss: 0.0650 | 0.0321
Epoch 64/300, seasonal_3 Loss: 0.0651 | 0.0323
Epoch 65/300, seasonal_3 Loss: 0.0646 | 0.0324
Epoch 66/300, seasonal_3 Loss: 0.0644 | 0.0333
Epoch 67/300, seasonal_3 Loss: 0.0649 | 0.0342
Epoch 68/300, seasonal_3 Loss: 0.0646 | 0.0329
Epoch 69/300, seasonal_3 Loss: 0.0638 | 0.0312
Epoch 70/300, seasonal_3 Loss: 0.0645 | 0.0311
Epoch 71/300, seasonal_3 Loss: 0.0642 | 0.0313
Epoch 72/300, seasonal_3 Loss: 0.0634 | 0.0324
Epoch 73/300, seasonal_3 Loss: 0.0639 | 0.0331
Epoch 74/300, seasonal_3 Loss: 0.0632 | 0.0316
Epoch 75/300, seasonal_3 Loss: 0.0628 | 0.0306
Epoch 76/300, seasonal_3 Loss: 0.0629 | 0.0306
Epoch 77/300, seasonal_3 Loss: 0.0624 | 0.0310
Epoch 78/300, seasonal_3 Loss: 0.0623 | 0.0314
Epoch 79/300, seasonal_3 Loss: 0.0621 | 0.0309
Epoch 80/300, seasonal_3 Loss: 0.0619 | 0.0304
Epoch 81/300, seasonal_3 Loss: 0.0618 | 0.0303
Epoch 82/300, seasonal_3 Loss: 0.0616 | 0.0305
Epoch 83/300, seasonal_3 Loss: 0.0615 | 0.0306
Epoch 84/300, seasonal_3 Loss: 0.0614 | 0.0304
Epoch 85/300, seasonal_3 Loss: 0.0612 | 0.0301
Epoch 86/300, seasonal_3 Loss: 0.0611 | 0.0300
Epoch 87/300, seasonal_3 Loss: 0.0610 | 0.0300
Epoch 88/300, seasonal_3 Loss: 0.0609 | 0.0301
Epoch 89/300, seasonal_3 Loss: 0.0608 | 0.0300
Epoch 90/300, seasonal_3 Loss: 0.0607 | 0.0299
Epoch 91/300, seasonal_3 Loss: 0.0606 | 0.0298
Epoch 92/300, seasonal_3 Loss: 0.0605 | 0.0298
Epoch 93/300, seasonal_3 Loss: 0.0604 | 0.0297
Epoch 94/300, seasonal_3 Loss: 0.0603 | 0.0297
Epoch 95/300, seasonal_3 Loss: 0.0602 | 0.0296
Epoch 96/300, seasonal_3 Loss: 0.0601 | 0.0296
Epoch 97/300, seasonal_3 Loss: 0.0601 | 0.0295
Epoch 98/300, seasonal_3 Loss: 0.0600 | 0.0295
Epoch 99/300, seasonal_3 Loss: 0.0599 | 0.0294
Epoch 100/300, seasonal_3 Loss: 0.0598 | 0.0294
Epoch 101/300, seasonal_3 Loss: 0.0597 | 0.0293
Epoch 102/300, seasonal_3 Loss: 0.0597 | 0.0293
Epoch 103/300, seasonal_3 Loss: 0.0596 | 0.0293
Epoch 104/300, seasonal_3 Loss: 0.0595 | 0.0292
Epoch 105/300, seasonal_3 Loss: 0.0595 | 0.0292
Epoch 106/300, seasonal_3 Loss: 0.0594 | 0.0292
Epoch 107/300, seasonal_3 Loss: 0.0593 | 0.0291
Epoch 108/300, seasonal_3 Loss: 0.0593 | 0.0291
Epoch 109/300, seasonal_3 Loss: 0.0592 | 0.0291
Epoch 110/300, seasonal_3 Loss: 0.0592 | 0.0290
Epoch 111/300, seasonal_3 Loss: 0.0591 | 0.0290
Epoch 112/300, seasonal_3 Loss: 0.0590 | 0.0290
Epoch 113/300, seasonal_3 Loss: 0.0590 | 0.0289
Epoch 114/300, seasonal_3 Loss: 0.0589 | 0.0289
Epoch 115/300, seasonal_3 Loss: 0.0589 | 0.0289
Epoch 116/300, seasonal_3 Loss: 0.0588 | 0.0289
Epoch 117/300, seasonal_3 Loss: 0.0588 | 0.0288
Epoch 118/300, seasonal_3 Loss: 0.0587 | 0.0288
Epoch 119/300, seasonal_3 Loss: 0.0587 | 0.0288
Epoch 120/300, seasonal_3 Loss: 0.0586 | 0.0288
Epoch 121/300, seasonal_3 Loss: 0.0586 | 0.0288
Epoch 122/300, seasonal_3 Loss: 0.0586 | 0.0287
Epoch 123/300, seasonal_3 Loss: 0.0585 | 0.0287
Epoch 124/300, seasonal_3 Loss: 0.0585 | 0.0287
Epoch 125/300, seasonal_3 Loss: 0.0584 | 0.0287
Epoch 126/300, seasonal_3 Loss: 0.0584 | 0.0287
Epoch 127/300, seasonal_3 Loss: 0.0584 | 0.0286
Epoch 128/300, seasonal_3 Loss: 0.0583 | 0.0286
Epoch 129/300, seasonal_3 Loss: 0.0583 | 0.0286
Epoch 130/300, seasonal_3 Loss: 0.0583 | 0.0286
Epoch 131/300, seasonal_3 Loss: 0.0582 | 0.0286
Epoch 132/300, seasonal_3 Loss: 0.0582 | 0.0286
Epoch 133/300, seasonal_3 Loss: 0.0581 | 0.0285
Epoch 134/300, seasonal_3 Loss: 0.0581 | 0.0285
Epoch 135/300, seasonal_3 Loss: 0.0581 | 0.0285
Epoch 136/300, seasonal_3 Loss: 0.0581 | 0.0285
Epoch 137/300, seasonal_3 Loss: 0.0580 | 0.0285
Epoch 138/300, seasonal_3 Loss: 0.0580 | 0.0285
Epoch 139/300, seasonal_3 Loss: 0.0580 | 0.0285
Epoch 140/300, seasonal_3 Loss: 0.0579 | 0.0285
Epoch 141/300, seasonal_3 Loss: 0.0579 | 0.0284
Epoch 142/300, seasonal_3 Loss: 0.0579 | 0.0284
Epoch 143/300, seasonal_3 Loss: 0.0579 | 0.0284
Epoch 144/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 145/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 146/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 147/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 148/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 149/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 150/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 151/300, seasonal_3 Loss: 0.0577 | 0.0283
Epoch 152/300, seasonal_3 Loss: 0.0577 | 0.0283
Epoch 153/300, seasonal_3 Loss: 0.0577 | 0.0283
Epoch 154/300, seasonal_3 Loss: 0.0576 | 0.0283
Epoch 155/300, seasonal_3 Loss: 0.0576 | 0.0283
Epoch 156/300, seasonal_3 Loss: 0.0576 | 0.0283
Epoch 157/300, seasonal_3 Loss: 0.0576 | 0.0283
Epoch 158/300, seasonal_3 Loss: 0.0576 | 0.0283
Epoch 159/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 160/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 161/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 162/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 163/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 164/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 165/300, seasonal_3 Loss: 0.0575 | 0.0282
Epoch 166/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 167/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 168/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 169/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 170/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 171/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 172/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 173/300, seasonal_3 Loss: 0.0574 | 0.0282
Epoch 174/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 175/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 176/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 177/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 178/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 179/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 180/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 181/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 182/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 183/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 184/300, seasonal_3 Loss: 0.0573 | 0.0282
Epoch 185/300, seasonal_3 Loss: 0.0572 | 0.0282
Epoch 186/300, seasonal_3 Loss: 0.0572 | 0.0282
Epoch 187/300, seasonal_3 Loss: 0.0572 | 0.0282
Epoch 188/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 189/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 190/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 191/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 192/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 193/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 194/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 195/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 196/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 197/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 198/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 199/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 200/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 201/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 202/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 203/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 204/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 205/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 206/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 207/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 208/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 209/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 210/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 211/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 212/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 213/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 214/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 215/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 216/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 217/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 218/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 219/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 220/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 221/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 222/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 223/300, seasonal_3 Loss: 0.0571 | 0.0281
Epoch 224/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 225/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 226/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 227/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 228/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 229/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 230/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 231/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 232/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 233/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 234/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 235/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 236/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 237/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 238/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 239/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 240/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 241/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 242/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 243/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 244/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 245/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 246/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 247/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 248/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 249/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 250/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 251/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 252/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 253/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 254/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 255/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 256/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 257/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 258/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 259/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 260/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 261/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 262/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 263/300, seasonal_3 Loss: 0.0570 | 0.0281
Epoch 264/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 265/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 266/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 267/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 268/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 269/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 270/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 271/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 272/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 273/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 274/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 275/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 276/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 277/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 278/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 279/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 280/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 281/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 282/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 283/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 284/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 285/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 286/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 287/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 288/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 289/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 290/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 291/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 292/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 293/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 294/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 295/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 296/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 297/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 298/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 299/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 300/300, seasonal_3 Loss: 0.0570 | 0.0280
Training resid component with params: {'observation_period_num': 20, 'train_rates': 0.9027233331806938, 'learning_rate': 0.0001882470216234829, 'batch_size': 124, 'step_size': 15, 'gamma': 0.9231152191525884}
Epoch 1/300, resid Loss: 1.0266 | 0.3072
Epoch 2/300, resid Loss: 0.2656 | 0.2379
Epoch 3/300, resid Loss: 0.2816 | 0.2086
Epoch 4/300, resid Loss: 0.2520 | 0.2013
Epoch 5/300, resid Loss: 0.2067 | 0.1194
Epoch 6/300, resid Loss: 0.2170 | 0.1576
Epoch 7/300, resid Loss: 0.2178 | 0.1582
Epoch 8/300, resid Loss: 0.2737 | 0.1458
Epoch 9/300, resid Loss: 0.2280 | 0.1102
Epoch 10/300, resid Loss: 0.1776 | 0.1047
Epoch 11/300, resid Loss: 0.1801 | 0.1955
Epoch 12/300, resid Loss: 0.2090 | 0.1868
Epoch 13/300, resid Loss: 0.2458 | 0.1218
Epoch 14/300, resid Loss: 0.3701 | 0.1744
Epoch 15/300, resid Loss: 0.1716 | 0.0815
Epoch 16/300, resid Loss: 0.1875 | 0.0853
Epoch 17/300, resid Loss: 0.2075 | 0.0797
Epoch 18/300, resid Loss: 0.2282 | 0.0933
Epoch 19/300, resid Loss: 0.2619 | 0.1308
Epoch 20/300, resid Loss: 0.2589 | 0.1953
Epoch 21/300, resid Loss: 0.2217 | 0.1452
Epoch 22/300, resid Loss: 0.2505 | 0.0936
Epoch 23/300, resid Loss: 0.2040 | 0.1447
Epoch 24/300, resid Loss: 0.2025 | 0.1520
Epoch 25/300, resid Loss: 0.1989 | 0.1734
Epoch 26/300, resid Loss: 0.2057 | 0.0965
Epoch 27/300, resid Loss: 0.1599 | 0.1147
Epoch 28/300, resid Loss: 0.1488 | 0.1405
Epoch 29/300, resid Loss: 0.1697 | 0.0795
Epoch 30/300, resid Loss: 0.1673 | 0.1018
Epoch 31/300, resid Loss: 0.1442 | 0.0983
Epoch 32/300, resid Loss: 0.1879 | 0.1075
Epoch 33/300, resid Loss: 0.2068 | 0.1862
Epoch 34/300, resid Loss: 0.1685 | 0.2242
Epoch 35/300, resid Loss: 0.1669 | 0.1185
Epoch 36/300, resid Loss: 0.1673 | 0.0742
Epoch 37/300, resid Loss: 0.1412 | 0.0657
Epoch 38/300, resid Loss: 0.1570 | 0.1006
Epoch 39/300, resid Loss: 0.1667 | 0.0704
Epoch 40/300, resid Loss: 0.1710 | 0.0613
Epoch 41/300, resid Loss: 0.1425 | 0.0694
Epoch 42/300, resid Loss: 0.1193 | 0.0576
Epoch 43/300, resid Loss: 0.1325 | 0.0747
Epoch 44/300, resid Loss: 0.1383 | 0.1133
Epoch 45/300, resid Loss: 0.1709 | 0.2948
Epoch 46/300, resid Loss: 0.1590 | 0.0961
Epoch 47/300, resid Loss: 0.1262 | 0.0577
Epoch 48/300, resid Loss: 0.2033 | 0.0502
Epoch 49/300, resid Loss: 0.0948 | 0.0482
Epoch 50/300, resid Loss: 0.0899 | 0.0758
Epoch 51/300, resid Loss: 0.0953 | 0.1009
Epoch 52/300, resid Loss: 0.1011 | 0.0624
Epoch 53/300, resid Loss: 0.0939 | 0.0519
Epoch 54/300, resid Loss: 0.0944 | 0.0600
Epoch 55/300, resid Loss: 0.0902 | 0.0564
Epoch 56/300, resid Loss: 0.0975 | 0.0506
Epoch 57/300, resid Loss: 0.1072 | 0.0613
Epoch 58/300, resid Loss: 0.1029 | 0.1178
Epoch 59/300, resid Loss: 0.0902 | 0.0932
Epoch 60/300, resid Loss: 0.1038 | 0.0480
Epoch 61/300, resid Loss: 0.1041 | 0.0626
Epoch 62/300, resid Loss: 0.1044 | 0.0585
Epoch 63/300, resid Loss: 0.0927 | 0.0634
Epoch 64/300, resid Loss: 0.1088 | 0.0493
Epoch 65/300, resid Loss: 0.0920 | 0.0697
Epoch 66/300, resid Loss: 0.0902 | 0.0484
Epoch 67/300, resid Loss: 0.0993 | 0.0392
Epoch 68/300, resid Loss: 0.0855 | 0.0556
Epoch 69/300, resid Loss: 0.0862 | 0.1075
Epoch 70/300, resid Loss: 0.1045 | 0.0900
Epoch 71/300, resid Loss: 0.0853 | 0.0455
Epoch 72/300, resid Loss: 0.0826 | 0.0476
Epoch 73/300, resid Loss: 0.0840 | 0.0515
Epoch 74/300, resid Loss: 0.0764 | 0.0619
Epoch 75/300, resid Loss: 0.0842 | 0.0528
Epoch 76/300, resid Loss: 0.0814 | 0.0756
Epoch 77/300, resid Loss: 0.0733 | 0.0432
Epoch 78/300, resid Loss: 0.0797 | 0.0385
Epoch 79/300, resid Loss: 0.0710 | 0.0558
Epoch 80/300, resid Loss: 0.0654 | 0.0642
Epoch 81/300, resid Loss: 0.0674 | 0.0573
Epoch 82/300, resid Loss: 0.0645 | 0.0449
Epoch 83/300, resid Loss: 0.0645 | 0.0482
Epoch 84/300, resid Loss: 0.0689 | 0.0429
Epoch 85/300, resid Loss: 0.0719 | 0.0486
Epoch 86/300, resid Loss: 0.0670 | 0.0577
Epoch 87/300, resid Loss: 0.0645 | 0.0553
Epoch 88/300, resid Loss: 0.0615 | 0.0438
Epoch 89/300, resid Loss: 0.0611 | 0.0418
Epoch 90/300, resid Loss: 0.0595 | 0.0415
Epoch 91/300, resid Loss: 0.0591 | 0.0556
Epoch 92/300, resid Loss: 0.0607 | 0.0437
Epoch 93/300, resid Loss: 0.0593 | 0.0417
Epoch 94/300, resid Loss: 0.0584 | 0.0458
Epoch 95/300, resid Loss: 0.0579 | 0.0406
Epoch 96/300, resid Loss: 0.0575 | 0.0439
Epoch 97/300, resid Loss: 0.0587 | 0.0501
Epoch 98/300, resid Loss: 0.0592 | 0.0525
Epoch 99/300, resid Loss: 0.0605 | 0.0371
Epoch 100/300, resid Loss: 0.0580 | 0.0367
Epoch 101/300, resid Loss: 0.0582 | 0.0499
Epoch 102/300, resid Loss: 0.0590 | 0.0609
Epoch 103/300, resid Loss: 0.0608 | 0.0496
Epoch 104/300, resid Loss: 0.0581 | 0.0438
Epoch 105/300, resid Loss: 0.0607 | 0.0410
Epoch 106/300, resid Loss: 0.0646 | 0.0349
Epoch 107/300, resid Loss: 0.0642 | 0.0505
Epoch 108/300, resid Loss: 0.0693 | 0.0653
Epoch 109/300, resid Loss: 0.0724 | 0.0732
Epoch 110/300, resid Loss: 0.0639 | 0.0372
Epoch 111/300, resid Loss: 0.0623 | 0.0404
Epoch 112/300, resid Loss: 0.0585 | 0.0386
Epoch 113/300, resid Loss: 0.0579 | 0.0602
Epoch 114/300, resid Loss: 0.0623 | 0.0493
Epoch 115/300, resid Loss: 0.0624 | 0.0461
Epoch 116/300, resid Loss: 0.0627 | 0.0410
Epoch 117/300, resid Loss: 0.0638 | 0.0387
Epoch 118/300, resid Loss: 0.0583 | 0.0508
Epoch 119/300, resid Loss: 0.0622 | 0.0702
Epoch 120/300, resid Loss: 0.0618 | 0.0462
Epoch 121/300, resid Loss: 0.0613 | 0.0364
Epoch 122/300, resid Loss: 0.0582 | 0.0510
Epoch 123/300, resid Loss: 0.0543 | 0.0483
Epoch 124/300, resid Loss: 0.0607 | 0.0551
Epoch 125/300, resid Loss: 0.0573 | 0.0420
Epoch 126/300, resid Loss: 0.0580 | 0.0423
Epoch 127/300, resid Loss: 0.0567 | 0.0399
Epoch 128/300, resid Loss: 0.0551 | 0.0468
Epoch 129/300, resid Loss: 0.0555 | 0.0546
Epoch 130/300, resid Loss: 0.0553 | 0.0469
Epoch 131/300, resid Loss: 0.0534 | 0.0376
Epoch 132/300, resid Loss: 0.0517 | 0.0454
Epoch 133/300, resid Loss: 0.0493 | 0.0512
Epoch 134/300, resid Loss: 0.0517 | 0.0604
Epoch 135/300, resid Loss: 0.0481 | 0.0431
Epoch 136/300, resid Loss: 0.0487 | 0.0391
Epoch 137/300, resid Loss: 0.0474 | 0.0409
Epoch 138/300, resid Loss: 0.0579 | 0.0538
Epoch 139/300, resid Loss: 0.0572 | 0.0526
Epoch 140/300, resid Loss: 0.0584 | 0.0449
Epoch 141/300, resid Loss: 0.0555 | 0.0352
Epoch 142/300, resid Loss: 0.0514 | 0.0429
Epoch 143/300, resid Loss: 0.0466 | 0.0491
Epoch 144/300, resid Loss: 0.0545 | 0.0541
Epoch 145/300, resid Loss: 0.0449 | 0.0415
Epoch 146/300, resid Loss: 0.0452 | 0.0446
Epoch 147/300, resid Loss: 0.0433 | 0.0402
Epoch 148/300, resid Loss: 0.0436 | 0.0484
Epoch 149/300, resid Loss: 0.0425 | 0.0476
Epoch 150/300, resid Loss: 0.0424 | 0.0471
Epoch 151/300, resid Loss: 0.0414 | 0.0376
Epoch 152/300, resid Loss: 0.0409 | 0.0395
Epoch 153/300, resid Loss: 0.0397 | 0.0505
Epoch 154/300, resid Loss: 0.0412 | 0.0573
Epoch 155/300, resid Loss: 0.0398 | 0.0393
Epoch 156/300, resid Loss: 0.0402 | 0.0379
Epoch 157/300, resid Loss: 0.0400 | 0.0441
Epoch 158/300, resid Loss: 0.0394 | 0.0520
Epoch 159/300, resid Loss: 0.0420 | 0.0504
Epoch 160/300, resid Loss: 0.0402 | 0.0390
Epoch 161/300, resid Loss: 0.0419 | 0.0390
Epoch 162/300, resid Loss: 0.0400 | 0.0441
Epoch 163/300, resid Loss: 0.0420 | 0.0528
Epoch 164/300, resid Loss: 0.0422 | 0.0505
Epoch 165/300, resid Loss: 0.0421 | 0.0419
Epoch 166/300, resid Loss: 0.0419 | 0.0384
Epoch 167/300, resid Loss: 0.0404 | 0.0461
Epoch 168/300, resid Loss: 0.0402 | 0.0607
Epoch 169/300, resid Loss: 0.0422 | 0.0498
Epoch 170/300, resid Loss: 0.0403 | 0.0387
Epoch 171/300, resid Loss: 0.0382 | 0.0459
Epoch 172/300, resid Loss: 0.0371 | 0.0522
Epoch 173/300, resid Loss: 0.0388 | 0.0489
Epoch 174/300, resid Loss: 0.0375 | 0.0401
Epoch 175/300, resid Loss: 0.0373 | 0.0427
Epoch 176/300, resid Loss: 0.0365 | 0.0459
Epoch 177/300, resid Loss: 0.0378 | 0.0500
Epoch 178/300, resid Loss: 0.0374 | 0.0459
Epoch 179/300, resid Loss: 0.0372 | 0.0417
Epoch 180/300, resid Loss: 0.0370 | 0.0425
Epoch 181/300, resid Loss: 0.0370 | 0.0499
Epoch 182/300, resid Loss: 0.0381 | 0.0520
Epoch 183/300, resid Loss: 0.0375 | 0.0441
Epoch 184/300, resid Loss: 0.0378 | 0.0395
Epoch 185/300, resid Loss: 0.0371 | 0.0473
Epoch 186/300, resid Loss: 0.0375 | 0.0565
Epoch 187/300, resid Loss: 0.0390 | 0.0501
Epoch 188/300, resid Loss: 0.0382 | 0.0385
Epoch 189/300, resid Loss: 0.0378 | 0.0447
Epoch 190/300, resid Loss: 0.0358 | 0.0550
Epoch 191/300, resid Loss: 0.0379 | 0.0482
Epoch 192/300, resid Loss: 0.0360 | 0.0415
Epoch 193/300, resid Loss: 0.0370 | 0.0442
Epoch 194/300, resid Loss: 0.0352 | 0.0484
Epoch 195/300, resid Loss: 0.0376 | 0.0519
Epoch 196/300, resid Loss: 0.0366 | 0.0453
Epoch 197/300, resid Loss: 0.0376 | 0.0438
Epoch 198/300, resid Loss: 0.0354 | 0.0462
Epoch 199/300, resid Loss: 0.0368 | 0.0518
Epoch 200/300, resid Loss: 0.0359 | 0.0492
Epoch 201/300, resid Loss: 0.0379 | 0.0454
Epoch 202/300, resid Loss: 0.0357 | 0.0448
Epoch 203/300, resid Loss: 0.0363 | 0.0498
Epoch 204/300, resid Loss: 0.0358 | 0.0533
Epoch 205/300, resid Loss: 0.0351 | 0.0458
Epoch 206/300, resid Loss: 0.0348 | 0.0420
Epoch 207/300, resid Loss: 0.0339 | 0.0526
Epoch 208/300, resid Loss: 0.0343 | 0.0529
Epoch 209/300, resid Loss: 0.0340 | 0.0425
Epoch 210/300, resid Loss: 0.0342 | 0.0437
Epoch 211/300, resid Loss: 0.0335 | 0.0540
Epoch 212/300, resid Loss: 0.0333 | 0.0475
Epoch 213/300, resid Loss: 0.0328 | 0.0441
Epoch 214/300, resid Loss: 0.0322 | 0.0481
Epoch 215/300, resid Loss: 0.0324 | 0.0494
Epoch 216/300, resid Loss: 0.0324 | 0.0463
Epoch 217/300, resid Loss: 0.0326 | 0.0464
Epoch 218/300, resid Loss: 0.0321 | 0.0489
Epoch 219/300, resid Loss: 0.0325 | 0.0476
Epoch 220/300, resid Loss: 0.0321 | 0.0451
Epoch 221/300, resid Loss: 0.0320 | 0.0477
Epoch 222/300, resid Loss: 0.0319 | 0.0501
Epoch 223/300, resid Loss: 0.0318 | 0.0460
Epoch 224/300, resid Loss: 0.0322 | 0.0442
Epoch 225/300, resid Loss: 0.0318 | 0.0490
Epoch 226/300, resid Loss: 0.0323 | 0.0505
Epoch 227/300, resid Loss: 0.0320 | 0.0443
Epoch 228/300, resid Loss: 0.0320 | 0.0456
Epoch 229/300, resid Loss: 0.0316 | 0.0504
Epoch 230/300, resid Loss: 0.0316 | 0.0481
Epoch 231/300, resid Loss: 0.0316 | 0.0444
Epoch 232/300, resid Loss: 0.0312 | 0.0481
Epoch 233/300, resid Loss: 0.0313 | 0.0494
Epoch 234/300, resid Loss: 0.0313 | 0.0461
Epoch 235/300, resid Loss: 0.0308 | 0.0471
Epoch 236/300, resid Loss: 0.0305 | 0.0497
Epoch 237/300, resid Loss: 0.0302 | 0.0472
Epoch 238/300, resid Loss: 0.0304 | 0.0462
Epoch 239/300, resid Loss: 0.0300 | 0.0497
Epoch 240/300, resid Loss: 0.0301 | 0.0499
Epoch 241/300, resid Loss: 0.0300 | 0.0453
Epoch 242/300, resid Loss: 0.0301 | 0.0473
Epoch 243/300, resid Loss: 0.0299 | 0.0510
Epoch 244/300, resid Loss: 0.0303 | 0.0486
Epoch 245/300, resid Loss: 0.0301 | 0.0450
Epoch 246/300, resid Loss: 0.0303 | 0.0483
Epoch 247/300, resid Loss: 0.0303 | 0.0503
Epoch 248/300, resid Loss: 0.0309 | 0.0479
Epoch 249/300, resid Loss: 0.0307 | 0.0473
Epoch 250/300, resid Loss: 0.0307 | 0.0479
Epoch 251/300, resid Loss: 0.0304 | 0.0481
Epoch 252/300, resid Loss: 0.0304 | 0.0491
Epoch 253/300, resid Loss: 0.0300 | 0.0494
Epoch 254/300, resid Loss: 0.0296 | 0.0470
Epoch 255/300, resid Loss: 0.0300 | 0.0475
Epoch 256/300, resid Loss: 0.0295 | 0.0512
Epoch 257/300, resid Loss: 0.0299 | 0.0516
Epoch 258/300, resid Loss: 0.0295 | 0.0454
Epoch 259/300, resid Loss: 0.0296 | 0.0477
Epoch 260/300, resid Loss: 0.0297 | 0.0536
Epoch 261/300, resid Loss: 0.0294 | 0.0504
Epoch 262/300, resid Loss: 0.0299 | 0.0446
Epoch 263/300, resid Loss: 0.0296 | 0.0477
Epoch 264/300, resid Loss: 0.0295 | 0.0536
Epoch 265/300, resid Loss: 0.0293 | 0.0479
Epoch 266/300, resid Loss: 0.0289 | 0.0469
Epoch 267/300, resid Loss: 0.0286 | 0.0494
Epoch 268/300, resid Loss: 0.0285 | 0.0507
Epoch 269/300, resid Loss: 0.0286 | 0.0491
Epoch 270/300, resid Loss: 0.0285 | 0.0487
Epoch 271/300, resid Loss: 0.0288 | 0.0491
Epoch 272/300, resid Loss: 0.0286 | 0.0495
Epoch 273/300, resid Loss: 0.0286 | 0.0511
Epoch 274/300, resid Loss: 0.0284 | 0.0494
Epoch 275/300, resid Loss: 0.0284 | 0.0477
Epoch 276/300, resid Loss: 0.0285 | 0.0497
Epoch 277/300, resid Loss: 0.0283 | 0.0530
Epoch 278/300, resid Loss: 0.0284 | 0.0490
Epoch 279/300, resid Loss: 0.0285 | 0.0463
Epoch 280/300, resid Loss: 0.0284 | 0.0520
Epoch 281/300, resid Loss: 0.0287 | 0.0527
Epoch 282/300, resid Loss: 0.0283 | 0.0466
Epoch 283/300, resid Loss: 0.0294 | 0.0474
Epoch 284/300, resid Loss: 0.0288 | 0.0536
Epoch 285/300, resid Loss: 0.0303 | 0.0504
Epoch 286/300, resid Loss: 0.0289 | 0.0468
Epoch 287/300, resid Loss: 0.0299 | 0.0492
Epoch 288/300, resid Loss: 0.0287 | 0.0516
Epoch 289/300, resid Loss: 0.0293 | 0.0499
Epoch 290/300, resid Loss: 0.0286 | 0.0484
Epoch 291/300, resid Loss: 0.0286 | 0.0497
Epoch 292/300, resid Loss: 0.0286 | 0.0520
Epoch 293/300, resid Loss: 0.0285 | 0.0506
Epoch 294/300, resid Loss: 0.0286 | 0.0485
Epoch 295/300, resid Loss: 0.0280 | 0.0504
Epoch 296/300, resid Loss: 0.0283 | 0.0523
Epoch 297/300, resid Loss: 0.0277 | 0.0487
Epoch 298/300, resid Loss: 0.0280 | 0.0490
Epoch 299/300, resid Loss: 0.0276 | 0.0521
Epoch 300/300, resid Loss: 0.0278 | 0.0510
Runtime (seconds): 7293.807292461395
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[161.9351]
[-3.2646024]
[2.5590262]
[13.904796]
[1.7852066]
[17.808126]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 45.19000396504998
RMSE: 6.72235107421875
MAE: 6.72235107421875
R-squared: nan
[194.72765]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
