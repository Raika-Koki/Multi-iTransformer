ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-31 00:14:12,793][0m A new study created in memory with name: no-name-66c65ad8-af50-4f28-b08f-8b00f4acc3e2[0m
[32m[I 2025-01-31 00:16:01,327][0m Trial 0 finished with value: 0.1621364468091527 and parameters: {'observation_period_num': 157, 'train_rates': 0.6468739045108898, 'learning_rate': 0.0003587938997591148, 'batch_size': 39, 'step_size': 11, 'gamma': 0.7580767404804813}. Best is trial 0 with value: 0.1621364468091527.[0m
[32m[I 2025-01-31 00:17:13,488][0m Trial 1 finished with value: 0.23065560402454072 and parameters: {'observation_period_num': 205, 'train_rates': 0.9231241739166851, 'learning_rate': 0.0001757288710609214, 'batch_size': 75, 'step_size': 4, 'gamma': 0.9203610402335688}. Best is trial 0 with value: 0.1621364468091527.[0m
[32m[I 2025-01-31 00:17:40,446][0m Trial 2 finished with value: 0.29261813966201733 and parameters: {'observation_period_num': 153, 'train_rates': 0.8896402839892046, 'learning_rate': 2.0818168484679848e-05, 'batch_size': 218, 'step_size': 14, 'gamma': 0.7886618279146411}. Best is trial 0 with value: 0.1621364468091527.[0m
[32m[I 2025-01-31 00:18:04,817][0m Trial 3 finished with value: 0.2703065872192383 and parameters: {'observation_period_num': 124, 'train_rates': 0.9494375686321718, 'learning_rate': 0.00036328471556305636, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8526610341425755}. Best is trial 0 with value: 0.1621364468091527.[0m
[32m[I 2025-01-31 00:18:37,815][0m Trial 4 finished with value: 0.13330143166449962 and parameters: {'observation_period_num': 120, 'train_rates': 0.6275362714641197, 'learning_rate': 0.00013705483504550272, 'batch_size': 137, 'step_size': 4, 'gamma': 0.9500866983997491}. Best is trial 4 with value: 0.13330143166449962.[0m
[32m[I 2025-01-31 00:21:18,807][0m Trial 5 finished with value: 0.3243013005535882 and parameters: {'observation_period_num': 19, 'train_rates': 0.7731538878160056, 'learning_rate': 1.0057964719109888e-06, 'batch_size': 31, 'step_size': 4, 'gamma': 0.838151026724673}. Best is trial 4 with value: 0.13330143166449962.[0m
[32m[I 2025-01-31 00:21:47,907][0m Trial 6 finished with value: 0.2611828148365021 and parameters: {'observation_period_num': 70, 'train_rates': 0.9454444289480859, 'learning_rate': 0.00023826305861370095, 'batch_size': 221, 'step_size': 10, 'gamma': 0.809224912016557}. Best is trial 4 with value: 0.13330143166449962.[0m
[32m[I 2025-01-31 00:22:10,372][0m Trial 7 finished with value: 0.23653461386079658 and parameters: {'observation_period_num': 188, 'train_rates': 0.8682504630004166, 'learning_rate': 3.4166892129881076e-05, 'batch_size': 240, 'step_size': 5, 'gamma': 0.9447228791504673}. Best is trial 4 with value: 0.13330143166449962.[0m
[32m[I 2025-01-31 00:23:04,861][0m Trial 8 finished with value: 0.10177256035921205 and parameters: {'observation_period_num': 47, 'train_rates': 0.8062607896052056, 'learning_rate': 0.00027471546357985514, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8995058836007653}. Best is trial 8 with value: 0.10177256035921205.[0m
[32m[I 2025-01-31 00:23:41,252][0m Trial 9 finished with value: 0.42799124642023967 and parameters: {'observation_period_num': 232, 'train_rates': 0.9306406552818839, 'learning_rate': 1.2870494218288324e-05, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8464146933937989}. Best is trial 8 with value: 0.10177256035921205.[0m
[32m[I 2025-01-31 00:24:34,782][0m Trial 10 finished with value: 0.09663489439657756 and parameters: {'observation_period_num': 24, 'train_rates': 0.7610006708871344, 'learning_rate': 0.0009240394664721368, 'batch_size': 94, 'step_size': 15, 'gamma': 0.9096531616479122}. Best is trial 10 with value: 0.09663489439657756.[0m
[32m[I 2025-01-31 00:25:30,243][0m Trial 11 finished with value: 0.09052028410511909 and parameters: {'observation_period_num': 18, 'train_rates': 0.7631337832818772, 'learning_rate': 0.0006300048394657082, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8950936808256236}. Best is trial 11 with value: 0.09052028410511909.[0m
[32m[I 2025-01-31 00:26:22,726][0m Trial 12 finished with value: 0.12626266051618645 and parameters: {'observation_period_num': 75, 'train_rates': 0.7217146114466327, 'learning_rate': 0.0009020227813811016, 'batch_size': 92, 'step_size': 13, 'gamma': 0.9865545931545495}. Best is trial 11 with value: 0.09052028410511909.[0m
[32m[I 2025-01-31 00:26:53,495][0m Trial 13 finished with value: 0.10080353613094005 and parameters: {'observation_period_num': 25, 'train_rates': 0.7107960208819799, 'learning_rate': 0.0009855180024323376, 'batch_size': 172, 'step_size': 1, 'gamma': 0.8946502538603267}. Best is trial 11 with value: 0.09052028410511909.[0m
[32m[I 2025-01-31 00:28:16,377][0m Trial 14 finished with value: 0.08243034815310564 and parameters: {'observation_period_num': 5, 'train_rates': 0.8138308425880747, 'learning_rate': 6.985639038613882e-05, 'batch_size': 64, 'step_size': 13, 'gamma': 0.8806775629779937}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:29:43,326][0m Trial 15 finished with value: 0.08979234441943552 and parameters: {'observation_period_num': 6, 'train_rates': 0.8228728656016857, 'learning_rate': 5.856424668691659e-05, 'batch_size': 62, 'step_size': 13, 'gamma': 0.8727783409271536}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:31:17,469][0m Trial 16 finished with value: 0.1375050585074064 and parameters: {'observation_period_num': 95, 'train_rates': 0.8521326687880876, 'learning_rate': 5.130179291277741e-05, 'batch_size': 56, 'step_size': 9, 'gamma': 0.8717194629525046}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:36:38,930][0m Trial 17 finished with value: 0.11964747200560089 and parameters: {'observation_period_num': 55, 'train_rates': 0.82919700948333, 'learning_rate': 8.892216727780696e-06, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8690618683506944}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:37:20,772][0m Trial 18 finished with value: 0.15062344411019132 and parameters: {'observation_period_num': 96, 'train_rates': 0.7120701929614962, 'learning_rate': 7.713159205143196e-05, 'batch_size': 118, 'step_size': 7, 'gamma': 0.8200542539824397}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:38:53,487][0m Trial 19 finished with value: 0.19738266180491493 and parameters: {'observation_period_num': 47, 'train_rates': 0.8139494656997568, 'learning_rate': 4.7423021807695965e-06, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8772195488823293}. Best is trial 14 with value: 0.08243034815310564.[0m
[32m[I 2025-01-31 00:39:46,272][0m Trial 20 finished with value: 0.0385613888502121 and parameters: {'observation_period_num': 14, 'train_rates': 0.9882822592893774, 'learning_rate': 8.806475988691994e-05, 'batch_size': 118, 'step_size': 12, 'gamma': 0.9577657322522101}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:40:21,623][0m Trial 21 finished with value: 0.12698328805833878 and parameters: {'observation_period_num': 6, 'train_rates': 0.8853423102338416, 'learning_rate': 9.965034277749803e-05, 'batch_size': 167, 'step_size': 12, 'gamma': 0.9887435271675483}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:41:58,457][0m Trial 22 finished with value: 0.051376406103372574 and parameters: {'observation_period_num': 36, 'train_rates': 0.9791355133053066, 'learning_rate': 4.401649792785891e-05, 'batch_size': 62, 'step_size': 13, 'gamma': 0.9377539761321924}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:42:51,351][0m Trial 23 finished with value: 0.05539649352431297 and parameters: {'observation_period_num': 39, 'train_rates': 0.9870956369569097, 'learning_rate': 3.075433107502277e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9543619734804337}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:43:47,400][0m Trial 24 finished with value: 0.057259030640125275 and parameters: {'observation_period_num': 42, 'train_rates': 0.9865439245114754, 'learning_rate': 2.6710096228609703e-05, 'batch_size': 114, 'step_size': 10, 'gamma': 0.9604668691117435}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:44:21,901][0m Trial 25 finished with value: 0.27420079708099365 and parameters: {'observation_period_num': 75, 'train_rates': 0.9885525729731561, 'learning_rate': 5.8820638795103474e-06, 'batch_size': 182, 'step_size': 7, 'gamma': 0.9306368537016501}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:45:12,589][0m Trial 26 finished with value: 0.3538127541542053 and parameters: {'observation_period_num': 99, 'train_rates': 0.9632923150549424, 'learning_rate': 3.711510297613966e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.9636019541526227}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:45:55,545][0m Trial 27 finished with value: 0.19623379038189942 and parameters: {'observation_period_num': 36, 'train_rates': 0.9093707958264203, 'learning_rate': 1.3667896551396694e-05, 'batch_size': 133, 'step_size': 9, 'gamma': 0.9720301317056199}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:46:34,579][0m Trial 28 finished with value: 0.640787661075592 and parameters: {'observation_period_num': 63, 'train_rates': 0.965222322964722, 'learning_rate': 2.0352042600178497e-06, 'batch_size': 155, 'step_size': 10, 'gamma': 0.9336009770381891}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:47:08,998][0m Trial 29 finished with value: 0.0972290113568306 and parameters: {'observation_period_num': 88, 'train_rates': 0.9843481897419137, 'learning_rate': 0.00011880466887881572, 'batch_size': 186, 'step_size': 12, 'gamma': 0.9423473655403513}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:48:20,763][0m Trial 30 finished with value: 0.20725642162349384 and parameters: {'observation_period_num': 146, 'train_rates': 0.9100356466671722, 'learning_rate': 1.8336361775971524e-05, 'batch_size': 76, 'step_size': 14, 'gamma': 0.974942793276287}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:49:15,578][0m Trial 31 finished with value: 0.08059781044721603 and parameters: {'observation_period_num': 49, 'train_rates': 0.9886002479495581, 'learning_rate': 2.5461246275594372e-05, 'batch_size': 113, 'step_size': 10, 'gamma': 0.9612338091962751}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:50:02,388][0m Trial 32 finished with value: 0.2857837378978729 and parameters: {'observation_period_num': 35, 'train_rates': 0.9640666801875105, 'learning_rate': 3.999468737573225e-05, 'batch_size': 129, 'step_size': 11, 'gamma': 0.923540634267143}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:50:56,052][0m Trial 33 finished with value: 0.20382771301682634 and parameters: {'observation_period_num': 33, 'train_rates': 0.9309188537144387, 'learning_rate': 2.5356121955003268e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.9500406336015245}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:53:24,041][0m Trial 34 finished with value: 0.35915731734463147 and parameters: {'observation_period_num': 179, 'train_rates': 0.9498046478272859, 'learning_rate': 1.5926262222109484e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.7529006234877254}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:54:02,522][0m Trial 35 finished with value: 0.15730480730850646 and parameters: {'observation_period_num': 60, 'train_rates': 0.9027187514476607, 'learning_rate': 0.00016882595283919036, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9168054640140596}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:55:15,620][0m Trial 36 finished with value: 0.39164102503231596 and parameters: {'observation_period_num': 108, 'train_rates': 0.9704855384402821, 'learning_rate': 8.732191577771584e-05, 'batch_size': 81, 'step_size': 14, 'gamma': 0.9597380566617117}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:56:11,108][0m Trial 37 finished with value: 0.2859741965485685 and parameters: {'observation_period_num': 39, 'train_rates': 0.9386276869032832, 'learning_rate': 8.512982528125538e-06, 'batch_size': 106, 'step_size': 7, 'gamma': 0.9788811996837331}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:56:43,163][0m Trial 38 finished with value: 0.16329885087907314 and parameters: {'observation_period_num': 79, 'train_rates': 0.6131191158244238, 'learning_rate': 4.949852509897157e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9376349094827983}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:57:41,787][0m Trial 39 finished with value: 0.1137165605750677 and parameters: {'observation_period_num': 20, 'train_rates': 0.6626957740343059, 'learning_rate': 2.6655498857315917e-05, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9603119056337234}. Best is trial 20 with value: 0.0385613888502121.[0m
Early stopping at epoch 56
[32m[I 2025-01-31 00:58:07,416][0m Trial 40 finished with value: 0.32586965371261944 and parameters: {'observation_period_num': 118, 'train_rates': 0.883581743702632, 'learning_rate': 0.0001841472066775976, 'batch_size': 126, 'step_size': 1, 'gamma': 0.7782707331172373}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 00:59:04,978][0m Trial 41 finished with value: 0.061558205634355545 and parameters: {'observation_period_num': 51, 'train_rates': 0.9865655869574994, 'learning_rate': 2.3597894775972145e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.9586248059019201}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:00:01,770][0m Trial 42 finished with value: 0.287655949060406 and parameters: {'observation_period_num': 63, 'train_rates': 0.951587010267639, 'learning_rate': 3.9305318205858504e-05, 'batch_size': 102, 'step_size': 8, 'gamma': 0.9494334745716057}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:00:44,586][0m Trial 43 finished with value: 0.094326913356781 and parameters: {'observation_period_num': 47, 'train_rates': 0.9773951368723118, 'learning_rate': 2.04527400179865e-05, 'batch_size': 146, 'step_size': 11, 'gamma': 0.9700792495491329}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:02:40,854][0m Trial 44 finished with value: 0.18818155701526187 and parameters: {'observation_period_num': 17, 'train_rates': 0.9252890993013375, 'learning_rate': 9.941001937345546e-06, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9078479657339603}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:03:27,367][0m Trial 45 finished with value: 0.32771573180244085 and parameters: {'observation_period_num': 232, 'train_rates': 0.9538393066969926, 'learning_rate': 0.00039270270957568666, 'batch_size': 123, 'step_size': 10, 'gamma': 0.9223192604561676}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:04:38,802][0m Trial 46 finished with value: 0.35247254371643066 and parameters: {'observation_period_num': 29, 'train_rates': 0.9739641106342293, 'learning_rate': 3.0139513528242733e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.954654702638541}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:05:07,351][0m Trial 47 finished with value: 0.258554607629776 and parameters: {'observation_period_num': 133, 'train_rates': 0.9349700573781596, 'learning_rate': 6.291539385447307e-05, 'batch_size': 203, 'step_size': 12, 'gamma': 0.9800727001840095}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:05:40,171][0m Trial 48 finished with value: 0.2295157504229506 and parameters: {'observation_period_num': 251, 'train_rates': 0.865735536202219, 'learning_rate': 0.00012476378217030654, 'batch_size': 160, 'step_size': 14, 'gamma': 0.9363906684118946}. Best is trial 20 with value: 0.0385613888502121.[0m
[32m[I 2025-01-31 01:06:39,173][0m Trial 49 finished with value: 0.1808877444461636 and parameters: {'observation_period_num': 44, 'train_rates': 0.9210408597258585, 'learning_rate': 4.563253635828782e-05, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9448291830752422}. Best is trial 20 with value: 0.0385613888502121.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-31 01:06:39,184][0m A new study created in memory with name: no-name-1131d547-2215-43a1-bc11-0abfebfe24c8[0m
[32m[I 2025-01-31 01:07:07,498][0m Trial 0 finished with value: 0.2468563387330794 and parameters: {'observation_period_num': 128, 'train_rates': 0.893305314222042, 'learning_rate': 0.00011622334922628532, 'batch_size': 198, 'step_size': 3, 'gamma': 0.839613677358429}. Best is trial 0 with value: 0.2468563387330794.[0m
[32m[I 2025-01-31 01:07:35,521][0m Trial 1 finished with value: 0.19933184540720397 and parameters: {'observation_period_num': 184, 'train_rates': 0.6977153114407147, 'learning_rate': 0.0007762045764041518, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9575838106800162}. Best is trial 1 with value: 0.19933184540720397.[0m
[32m[I 2025-01-31 01:07:56,946][0m Trial 2 finished with value: 0.15750252065082934 and parameters: {'observation_period_num': 92, 'train_rates': 0.6939035283058549, 'learning_rate': 0.00044318396299686334, 'batch_size': 237, 'step_size': 3, 'gamma': 0.7653628264847334}. Best is trial 2 with value: 0.15750252065082934.[0m
[32m[I 2025-01-31 01:08:32,992][0m Trial 3 finished with value: 0.32533249258995056 and parameters: {'observation_period_num': 218, 'train_rates': 0.9519577101504897, 'learning_rate': 6.597699579887976e-05, 'batch_size': 160, 'step_size': 13, 'gamma': 0.8595585174092593}. Best is trial 2 with value: 0.15750252065082934.[0m
[32m[I 2025-01-31 01:08:57,359][0m Trial 4 finished with value: 0.39875003695487976 and parameters: {'observation_period_num': 249, 'train_rates': 0.9246733182449638, 'learning_rate': 1.4967021402631192e-05, 'batch_size': 249, 'step_size': 14, 'gamma': 0.9207305329475306}. Best is trial 2 with value: 0.15750252065082934.[0m
[32m[I 2025-01-31 01:09:36,166][0m Trial 5 finished with value: 0.4927419809943651 and parameters: {'observation_period_num': 128, 'train_rates': 0.9112276195629182, 'learning_rate': 3.570167823200301e-06, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9828493489078277}. Best is trial 2 with value: 0.15750252065082934.[0m
[32m[I 2025-01-31 01:10:05,012][0m Trial 6 finished with value: 0.25563823927953405 and parameters: {'observation_period_num': 130, 'train_rates': 0.8176242332246758, 'learning_rate': 1.0716200054005324e-05, 'batch_size': 181, 'step_size': 14, 'gamma': 0.9504954656462762}. Best is trial 2 with value: 0.15750252065082934.[0m
[32m[I 2025-01-31 01:11:31,013][0m Trial 7 finished with value: 0.11851522064692266 and parameters: {'observation_period_num': 15, 'train_rates': 0.8739879694171548, 'learning_rate': 3.5689046829133734e-05, 'batch_size': 63, 'step_size': 15, 'gamma': 0.8380833763224808}. Best is trial 7 with value: 0.11851522064692266.[0m
[32m[I 2025-01-31 01:11:55,674][0m Trial 8 finished with value: 0.38120143711566923 and parameters: {'observation_period_num': 160, 'train_rates': 0.8781875399598391, 'learning_rate': 1.2565573776697613e-05, 'batch_size': 238, 'step_size': 15, 'gamma': 0.7555476353105957}. Best is trial 7 with value: 0.11851522064692266.[0m
[32m[I 2025-01-31 01:12:28,870][0m Trial 9 finished with value: 0.2968780016899109 and parameters: {'observation_period_num': 160, 'train_rates': 0.6867341622769667, 'learning_rate': 0.0009378754691405232, 'batch_size': 140, 'step_size': 15, 'gamma': 0.9496320647638914}. Best is trial 7 with value: 0.11851522064692266.[0m
[32m[I 2025-01-31 01:14:03,648][0m Trial 10 finished with value: 0.2578523957356197 and parameters: {'observation_period_num': 19, 'train_rates': 0.7932163181389403, 'learning_rate': 1.5726521363322813e-06, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8130863256523204}. Best is trial 7 with value: 0.11851522064692266.[0m
Early stopping at epoch 53
[32m[I 2025-01-31 01:15:00,674][0m Trial 11 finished with value: 0.15376815288724385 and parameters: {'observation_period_num': 22, 'train_rates': 0.6193319008677516, 'learning_rate': 0.0002216696325696131, 'batch_size': 41, 'step_size': 1, 'gamma': 0.751367782571729}. Best is trial 7 with value: 0.11851522064692266.[0m
Early stopping at epoch 83
[32m[I 2025-01-31 01:16:21,295][0m Trial 12 finished with value: 0.1415419877160084 and parameters: {'observation_period_num': 14, 'train_rates': 0.6095807751625614, 'learning_rate': 0.00016198245585945155, 'batch_size': 44, 'step_size': 1, 'gamma': 0.8047927995561732}. Best is trial 7 with value: 0.11851522064692266.[0m
[32m[I 2025-01-31 01:17:32,681][0m Trial 13 finished with value: 0.11730884148132759 and parameters: {'observation_period_num': 64, 'train_rates': 0.805338366140357, 'learning_rate': 4.245437797569777e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8134409104210355}. Best is trial 13 with value: 0.11730884148132759.[0m
[32m[I 2025-01-31 01:18:29,699][0m Trial 14 finished with value: 0.11559040150041407 and parameters: {'observation_period_num': 63, 'train_rates': 0.7913275504595257, 'learning_rate': 3.660521072736079e-05, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8932526317761433}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:19:22,528][0m Trial 15 finished with value: 0.1190839406207998 and parameters: {'observation_period_num': 66, 'train_rates': 0.7791182852175583, 'learning_rate': 4.060069444136715e-05, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9078843477724189}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:20:14,078][0m Trial 16 finished with value: 0.27601687952143245 and parameters: {'observation_period_num': 64, 'train_rates': 0.8311879719883879, 'learning_rate': 6.797331892197388e-06, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8907415567487103}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:21:05,979][0m Trial 17 finished with value: 0.15014607667687072 and parameters: {'observation_period_num': 61, 'train_rates': 0.7378367874942934, 'learning_rate': 2.2831781551923137e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.7923589614271745}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:22:07,534][0m Trial 18 finished with value: 0.12105082908402319 and parameters: {'observation_period_num': 89, 'train_rates': 0.7592162180073596, 'learning_rate': 8.043894780845042e-05, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8800514333177114}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:22:52,723][0m Trial 19 finished with value: 0.3568345313590276 and parameters: {'observation_period_num': 46, 'train_rates': 0.8449466910297652, 'learning_rate': 3.6722440373670204e-06, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8559681443453642}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:26:30,562][0m Trial 20 finished with value: 0.1276533194031233 and parameters: {'observation_period_num': 97, 'train_rates': 0.7300104125579215, 'learning_rate': 0.0002567218329189959, 'batch_size': 21, 'step_size': 12, 'gamma': 0.7839824811072461}. Best is trial 14 with value: 0.11559040150041407.[0m
[32m[I 2025-01-31 01:27:43,273][0m Trial 21 finished with value: 0.1155097516356436 and parameters: {'observation_period_num': 34, 'train_rates': 0.8325888534188672, 'learning_rate': 4.057400015040034e-05, 'batch_size': 73, 'step_size': 11, 'gamma': 0.8299379664793081}. Best is trial 21 with value: 0.1155097516356436.[0m
[32m[I 2025-01-31 01:28:55,591][0m Trial 22 finished with value: 0.1226671163359998 and parameters: {'observation_period_num': 42, 'train_rates': 0.8538662968653133, 'learning_rate': 5.475197426420545e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8246256230884116}. Best is trial 21 with value: 0.1155097516356436.[0m
[32m[I 2025-01-31 01:33:31,021][0m Trial 23 finished with value: 0.06315775061162507 and parameters: {'observation_period_num': 48, 'train_rates': 0.9859973163689069, 'learning_rate': 2.40943664310899e-05, 'batch_size': 21, 'step_size': 8, 'gamma': 0.835698425550331}. Best is trial 23 with value: 0.06315775061162507.[0m
[32m[I 2025-01-31 01:37:21,939][0m Trial 24 finished with value: 0.04615491388055185 and parameters: {'observation_period_num': 37, 'train_rates': 0.9835390887107553, 'learning_rate': 2.2492669107258797e-05, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8995124516941069}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 01:42:40,647][0m Trial 25 finished with value: 0.07216362468898296 and parameters: {'observation_period_num': 38, 'train_rates': 0.9754375125273269, 'learning_rate': 2.2007587194069265e-05, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8644962166064715}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 01:48:06,405][0m Trial 26 finished with value: 0.04853220247933941 and parameters: {'observation_period_num': 7, 'train_rates': 0.9895949981385482, 'learning_rate': 2.0939639791585803e-05, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9185572095626642}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 01:50:58,623][0m Trial 27 finished with value: 0.15350834670520963 and parameters: {'observation_period_num': 84, 'train_rates': 0.9782966469214851, 'learning_rate': 7.43281344865393e-06, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9241772305417242}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 01:56:31,846][0m Trial 28 finished with value: 0.23183405715047467 and parameters: {'observation_period_num': 13, 'train_rates': 0.9521422050566054, 'learning_rate': 5.074397453231888e-06, 'batch_size': 17, 'step_size': 8, 'gamma': 0.912447419394572}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 01:58:17,071][0m Trial 29 finished with value: 0.2456575582568356 and parameters: {'observation_period_num': 113, 'train_rates': 0.9390954377252856, 'learning_rate': 0.00011044598572412741, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9338553994593364}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:01:11,693][0m Trial 30 finished with value: 0.06062048457160189 and parameters: {'observation_period_num': 5, 'train_rates': 0.9875856205695956, 'learning_rate': 2.038912511681112e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.848469816933396}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:04:21,560][0m Trial 31 finished with value: 0.05557793865187301 and parameters: {'observation_period_num': 5, 'train_rates': 0.9880538950921722, 'learning_rate': 2.1179033601400506e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8470520931965865}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:07:03,911][0m Trial 32 finished with value: 0.16366960777995293 and parameters: {'observation_period_num': 5, 'train_rates': 0.9094667701677714, 'learning_rate': 1.6456707664331126e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.8498324949373703}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:08:50,833][0m Trial 33 finished with value: 0.33255388633867283 and parameters: {'observation_period_num': 29, 'train_rates': 0.9613941331723226, 'learning_rate': 9.11226591902892e-06, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8798274685682145}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:11:44,449][0m Trial 34 finished with value: 0.06259631067514419 and parameters: {'observation_period_num': 6, 'train_rates': 0.9866603413444807, 'learning_rate': 1.5280635386967276e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.8734478777863213}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:13:55,141][0m Trial 35 finished with value: 0.17716497284285898 and parameters: {'observation_period_num': 27, 'train_rates': 0.9273053785476169, 'learning_rate': 7.946590463406214e-05, 'batch_size': 43, 'step_size': 3, 'gamma': 0.8959148666122004}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:14:45,760][0m Trial 36 finished with value: 0.2864036560058594 and parameters: {'observation_period_num': 50, 'train_rates': 0.9599922115578792, 'learning_rate': 2.5391707929149212e-05, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9827713738020321}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:15:14,455][0m Trial 37 finished with value: 0.7243591135945814 and parameters: {'observation_period_num': 206, 'train_rates': 0.9050752516648761, 'learning_rate': 2.6445550171048362e-06, 'batch_size': 201, 'step_size': 7, 'gamma': 0.851607076100223}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:18:44,260][0m Trial 38 finished with value: 0.20096806128110203 and parameters: {'observation_period_num': 6, 'train_rates': 0.9407842464425412, 'learning_rate': 1.700217931602027e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.9642557064950933}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:19:12,906][0m Trial 39 finished with value: 0.23525239247828722 and parameters: {'observation_period_num': 33, 'train_rates': 0.8904001029697848, 'learning_rate': 9.932012343795718e-06, 'batch_size': 211, 'step_size': 9, 'gamma': 0.9394276450439548}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:20:45,164][0m Trial 40 finished with value: 0.3900818852006748 and parameters: {'observation_period_num': 248, 'train_rates': 0.9655984063136801, 'learning_rate': 5.598136308354633e-05, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9030722053404345}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:23:55,602][0m Trial 41 finished with value: 0.05997285407942694 and parameters: {'observation_period_num': 5, 'train_rates': 0.9877151868369256, 'learning_rate': 1.4851716742141563e-05, 'batch_size': 31, 'step_size': 6, 'gamma': 0.8754159654942588}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:26:01,388][0m Trial 42 finished with value: 0.07914353162050247 and parameters: {'observation_period_num': 21, 'train_rates': 0.9899718060600646, 'learning_rate': 1.183943008176415e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8686738644833417}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:28:58,174][0m Trial 43 finished with value: 0.2047254606855638 and parameters: {'observation_period_num': 22, 'train_rates': 0.9325261314054379, 'learning_rate': 2.5297216954603502e-05, 'batch_size': 32, 'step_size': 4, 'gamma': 0.8442418181440332}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:30:23,986][0m Trial 44 finished with value: 0.3258599155935748 and parameters: {'observation_period_num': 7, 'train_rates': 0.9508165282657962, 'learning_rate': 5.859214847162783e-06, 'batch_size': 67, 'step_size': 5, 'gamma': 0.8845572040168591}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:31:00,132][0m Trial 45 finished with value: 0.2243144609804811 and parameters: {'observation_period_num': 28, 'train_rates': 0.92079404779218, 'learning_rate': 1.9421302093853245e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.920037975428397}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:36:44,083][0m Trial 46 finished with value: 0.11525107394246493 and parameters: {'observation_period_num': 162, 'train_rates': 0.9758346791456356, 'learning_rate': 1.2712127575749875e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8586664742953659}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:39:00,776][0m Trial 47 finished with value: 0.3007817541559537 and parameters: {'observation_period_num': 16, 'train_rates': 0.9635216592730198, 'learning_rate': 3.285406417182818e-05, 'batch_size': 42, 'step_size': 3, 'gamma': 0.8209295120530566}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:41:28,586][0m Trial 48 finished with value: 0.17999297598658975 and parameters: {'observation_period_num': 79, 'train_rates': 0.6530592745802464, 'learning_rate': 8.852635436518434e-06, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8991499995071307}. Best is trial 24 with value: 0.04615491388055185.[0m
[32m[I 2025-01-31 02:42:08,908][0m Trial 49 finished with value: 0.22108785510063172 and parameters: {'observation_period_num': 51, 'train_rates': 0.94321808368748, 'learning_rate': 0.0004899542210451822, 'batch_size': 151, 'step_size': 7, 'gamma': 0.8419837634479866}. Best is trial 24 with value: 0.04615491388055185.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-31 02:42:08,919][0m A new study created in memory with name: no-name-fbfec694-99d5-414e-a8aa-6583d55d8280[0m
[32m[I 2025-01-31 02:44:27,254][0m Trial 0 finished with value: 0.15893864093327914 and parameters: {'observation_period_num': 102, 'train_rates': 0.8724572369975536, 'learning_rate': 0.0009581380152081898, 'batch_size': 38, 'step_size': 9, 'gamma': 0.954657851548032}. Best is trial 0 with value: 0.15893864093327914.[0m
[32m[I 2025-01-31 02:44:49,556][0m Trial 1 finished with value: 0.23122316715023328 and parameters: {'observation_period_num': 250, 'train_rates': 0.6058069811301693, 'learning_rate': 0.00016285388355865158, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8884159970634}. Best is trial 0 with value: 0.15893864093327914.[0m
[32m[I 2025-01-31 02:45:17,773][0m Trial 2 finished with value: 0.31925888075973047 and parameters: {'observation_period_num': 175, 'train_rates': 0.7030392730873424, 'learning_rate': 8.831319788879575e-06, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9406078556069749}. Best is trial 0 with value: 0.15893864093327914.[0m
[32m[I 2025-01-31 02:45:46,334][0m Trial 3 finished with value: 0.9824387243418526 and parameters: {'observation_period_num': 147, 'train_rates': 0.6640213148633802, 'learning_rate': 1.425359577811472e-06, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9519708939274504}. Best is trial 0 with value: 0.15893864093327914.[0m
[32m[I 2025-01-31 02:46:35,070][0m Trial 4 finished with value: 0.18987322533039297 and parameters: {'observation_period_num': 129, 'train_rates': 0.815376454834468, 'learning_rate': 1.8510291520813397e-05, 'batch_size': 106, 'step_size': 14, 'gamma': 0.7996307488059293}. Best is trial 0 with value: 0.15893864093327914.[0m
[32m[I 2025-01-31 02:47:01,352][0m Trial 5 finished with value: 0.13780333627991884 and parameters: {'observation_period_num': 180, 'train_rates': 0.7026597932817921, 'learning_rate': 0.0008587136283309207, 'batch_size': 186, 'step_size': 8, 'gamma': 0.7844655293838149}. Best is trial 5 with value: 0.13780333627991884.[0m
[32m[I 2025-01-31 02:47:34,546][0m Trial 6 finished with value: 0.1865425070990687 and parameters: {'observation_period_num': 130, 'train_rates': 0.633380483758677, 'learning_rate': 5.8228438388351295e-05, 'batch_size': 134, 'step_size': 4, 'gamma': 0.8633218676061716}. Best is trial 5 with value: 0.13780333627991884.[0m
[32m[I 2025-01-31 02:48:27,129][0m Trial 7 finished with value: 0.2318552277996007 and parameters: {'observation_period_num': 214, 'train_rates': 0.6046924605543457, 'learning_rate': 4.1374094789564856e-05, 'batch_size': 76, 'step_size': 4, 'gamma': 0.9549813538553136}. Best is trial 5 with value: 0.13780333627991884.[0m
[32m[I 2025-01-31 02:48:53,715][0m Trial 8 finished with value: 0.3411563038825989 and parameters: {'observation_period_num': 57, 'train_rates': 0.9855962151851548, 'learning_rate': 3.7464534265793505e-06, 'batch_size': 248, 'step_size': 13, 'gamma': 0.7903205836880985}. Best is trial 5 with value: 0.13780333627991884.[0m
[32m[I 2025-01-31 02:49:18,344][0m Trial 9 finished with value: 0.7636932584669129 and parameters: {'observation_period_num': 64, 'train_rates': 0.8801221363055913, 'learning_rate': 2.929816934648163e-06, 'batch_size': 236, 'step_size': 13, 'gamma': 0.8282062059724695}. Best is trial 5 with value: 0.13780333627991884.[0m
Early stopping at epoch 52
[32m[I 2025-01-31 02:49:32,437][0m Trial 10 finished with value: 0.1427027092822928 and parameters: {'observation_period_num': 12, 'train_rates': 0.7322506995557932, 'learning_rate': 0.0007818098189745969, 'batch_size': 199, 'step_size': 1, 'gamma': 0.7620583349193187}. Best is trial 5 with value: 0.13780333627991884.[0m
Early stopping at epoch 96
[32m[I 2025-01-31 02:49:59,056][0m Trial 11 finished with value: 0.10924533423006375 and parameters: {'observation_period_num': 16, 'train_rates': 0.7459017361503905, 'learning_rate': 0.0009887620965930422, 'batch_size': 197, 'step_size': 2, 'gamma': 0.7522048622036352}. Best is trial 11 with value: 0.10924533423006375.[0m
Early stopping at epoch 47
[32m[I 2025-01-31 02:50:15,270][0m Trial 12 finished with value: 0.3347378091956781 and parameters: {'observation_period_num': 181, 'train_rates': 0.7411581309091747, 'learning_rate': 0.0002549383608807268, 'batch_size': 147, 'step_size': 1, 'gamma': 0.7524286496281987}. Best is trial 11 with value: 0.10924533423006375.[0m
[32m[I 2025-01-31 02:50:40,309][0m Trial 13 finished with value: 0.08882728238197768 and parameters: {'observation_period_num': 20, 'train_rates': 0.7879680760451905, 'learning_rate': 0.0002953447256410626, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8264754759822998}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:51:05,568][0m Trial 14 finished with value: 0.09301602903886792 and parameters: {'observation_period_num': 6, 'train_rates': 0.8008983364458978, 'learning_rate': 0.0002569321412730528, 'batch_size': 224, 'step_size': 10, 'gamma': 0.8446115493936692}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:51:30,511][0m Trial 15 finished with value: 0.11332683966399353 and parameters: {'observation_period_num': 48, 'train_rates': 0.8158748583870512, 'learning_rate': 0.00015505033626974434, 'batch_size': 223, 'step_size': 10, 'gamma': 0.8550336878349922}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:51:53,075][0m Trial 16 finished with value: 0.15035021128739126 and parameters: {'observation_period_num': 90, 'train_rates': 0.8819243091646649, 'learning_rate': 0.000319302051853777, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9018692535516379}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:52:18,474][0m Trial 17 finished with value: 0.10195439743498962 and parameters: {'observation_period_num': 34, 'train_rates': 0.7903191056815202, 'learning_rate': 7.886847801810537e-05, 'batch_size': 218, 'step_size': 15, 'gamma': 0.8295267577793928}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:53:03,413][0m Trial 18 finished with value: 0.2918507645769817 and parameters: {'observation_period_num': 84, 'train_rates': 0.9287438226030728, 'learning_rate': 1.7371762167261493e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8239291897082758}. Best is trial 13 with value: 0.08882728238197768.[0m
[32m[I 2025-01-31 02:53:37,950][0m Trial 19 finished with value: 0.07945931159725814 and parameters: {'observation_period_num': 9, 'train_rates': 0.7818393464983381, 'learning_rate': 0.00035956057339686263, 'batch_size': 160, 'step_size': 6, 'gamma': 0.9166836612302044}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:54:35,969][0m Trial 20 finished with value: 0.10995629863335496 and parameters: {'observation_period_num': 41, 'train_rates': 0.8447943075978307, 'learning_rate': 0.0004377349898072748, 'batch_size': 93, 'step_size': 5, 'gamma': 0.9141613395958841}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:55:09,810][0m Trial 21 finished with value: 0.08639879916544731 and parameters: {'observation_period_num': 7, 'train_rates': 0.77718583550629, 'learning_rate': 0.00011536701402987261, 'batch_size': 157, 'step_size': 8, 'gamma': 0.9215064571882213}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:55:41,869][0m Trial 22 finished with value: 0.09492894381995351 and parameters: {'observation_period_num': 27, 'train_rates': 0.7705280515968309, 'learning_rate': 0.00010279512422729316, 'batch_size': 161, 'step_size': 8, 'gamma': 0.9281904751810384}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:56:22,372][0m Trial 23 finished with value: 0.11489123522002728 and parameters: {'observation_period_num': 73, 'train_rates': 0.7649187103282671, 'learning_rate': 0.0004248793124222497, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9831537666599772}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:56:57,478][0m Trial 24 finished with value: 0.10184914788656067 and parameters: {'observation_period_num': 6, 'train_rates': 0.8453032553115224, 'learning_rate': 0.00016094616407595709, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8899226180106781}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:57:41,265][0m Trial 25 finished with value: 0.08734164032118046 and parameters: {'observation_period_num': 32, 'train_rates': 0.6967136793639525, 'learning_rate': 0.0001003261878488375, 'batch_size': 112, 'step_size': 8, 'gamma': 0.8783258216114823}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:58:55,858][0m Trial 26 finished with value: 0.1728937152734737 and parameters: {'observation_period_num': 42, 'train_rates': 0.6999245883003824, 'learning_rate': 2.4959060092927483e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8854146696975377}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 02:59:36,159][0m Trial 27 finished with value: 0.10768543270726999 and parameters: {'observation_period_num': 104, 'train_rates': 0.6629592139676556, 'learning_rate': 9.735949794105242e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.9176801998225953}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:00:07,498][0m Trial 28 finished with value: 0.10392267132022728 and parameters: {'observation_period_num': 31, 'train_rates': 0.6715250864285086, 'learning_rate': 4.691264275293813e-05, 'batch_size': 151, 'step_size': 6, 'gamma': 0.9846300785658108}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:02:12,653][0m Trial 29 finished with value: 0.16123507138003002 and parameters: {'observation_period_num': 105, 'train_rates': 0.8416621235220679, 'learning_rate': 0.0005765423809875099, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8754480377821348}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:03:02,805][0m Trial 30 finished with value: 0.09921320455510226 and parameters: {'observation_period_num': 62, 'train_rates': 0.7219535956535705, 'learning_rate': 0.00012864859579554515, 'batch_size': 97, 'step_size': 7, 'gamma': 0.9614556511841261}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:03:32,302][0m Trial 31 finished with value: 0.08832560205056493 and parameters: {'observation_period_num': 24, 'train_rates': 0.775493600694652, 'learning_rate': 0.0002289607463371817, 'batch_size': 187, 'step_size': 9, 'gamma': 0.9091319901488708}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:04:03,309][0m Trial 32 finished with value: 0.09507352777338221 and parameters: {'observation_period_num': 26, 'train_rates': 0.7468912627352884, 'learning_rate': 6.973088970708578e-05, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9123068465536616}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:04:30,800][0m Trial 33 finished with value: 0.10497347909589579 and parameters: {'observation_period_num': 51, 'train_rates': 0.7659837011119763, 'learning_rate': 0.00018915963209350882, 'batch_size': 187, 'step_size': 5, 'gamma': 0.9347955590230717}. Best is trial 19 with value: 0.07945931159725814.[0m
[32m[I 2025-01-31 03:05:05,231][0m Trial 34 finished with value: 0.07397441527102483 and parameters: {'observation_period_num': 5, 'train_rates': 0.7140109728209583, 'learning_rate': 0.00020335232478945983, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9030131574826116}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:05:45,134][0m Trial 35 finished with value: 0.10684735261155331 and parameters: {'observation_period_num': 7, 'train_rates': 0.6783126985151178, 'learning_rate': 3.150366715807063e-05, 'batch_size': 121, 'step_size': 5, 'gamma': 0.8946966353651455}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:06:44,196][0m Trial 36 finished with value: 0.09418407859598278 and parameters: {'observation_period_num': 37, 'train_rates': 0.7164787036155942, 'learning_rate': 0.000542378329815971, 'batch_size': 82, 'step_size': 7, 'gamma': 0.8775933485011845}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:07:15,630][0m Trial 37 finished with value: 0.18679459415139782 and parameters: {'observation_period_num': 245, 'train_rates': 0.639060189519081, 'learning_rate': 0.00011684542862756528, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9256335924418801}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:07:46,101][0m Trial 38 finished with value: 0.15390727256619652 and parameters: {'observation_period_num': 153, 'train_rates': 0.6929184244167822, 'learning_rate': 5.741968950691557e-05, 'batch_size': 151, 'step_size': 8, 'gamma': 0.944875134316045}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:08:13,105][0m Trial 39 finished with value: 0.2781804311534633 and parameters: {'observation_period_num': 76, 'train_rates': 0.640299032228449, 'learning_rate': 9.906994190769616e-06, 'batch_size': 171, 'step_size': 11, 'gamma': 0.9682435923593579}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:08:56,822][0m Trial 40 finished with value: 0.08290066218929376 and parameters: {'observation_period_num': 17, 'train_rates': 0.715451681068383, 'learning_rate': 0.0003990842560679112, 'batch_size': 113, 'step_size': 4, 'gamma': 0.8571842660394549}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:09:40,492][0m Trial 41 finished with value: 0.07639102089509063 and parameters: {'observation_period_num': 17, 'train_rates': 0.7167829673608048, 'learning_rate': 0.0003346386364639, 'batch_size': 112, 'step_size': 4, 'gamma': 0.862277612509977}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:10:19,475][0m Trial 42 finished with value: 0.08113173705000189 and parameters: {'observation_period_num': 17, 'train_rates': 0.7217072446976088, 'learning_rate': 0.00038165549795577693, 'batch_size': 126, 'step_size': 4, 'gamma': 0.8624881587880988}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:10:58,545][0m Trial 43 finished with value: 0.08312827140661867 and parameters: {'observation_period_num': 19, 'train_rates': 0.7143690108154575, 'learning_rate': 0.0006774847674469649, 'batch_size': 124, 'step_size': 3, 'gamma': 0.8669255467902867}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:11:48,088][0m Trial 44 finished with value: 0.10215692362267612 and parameters: {'observation_period_num': 49, 'train_rates': 0.7511440796678012, 'learning_rate': 0.0003883020598956356, 'batch_size': 103, 'step_size': 4, 'gamma': 0.8494234737952711}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:13:02,104][0m Trial 45 finished with value: 0.08181852413110033 and parameters: {'observation_period_num': 18, 'train_rates': 0.7282921150397851, 'learning_rate': 0.0004245080673437897, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8105611406220109}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:16:25,192][0m Trial 46 finished with value: 0.09782991134676651 and parameters: {'observation_period_num': 61, 'train_rates': 0.7341178414679386, 'learning_rate': 0.0005885434401741769, 'batch_size': 23, 'step_size': 3, 'gamma': 0.7762558524239566}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:17:33,756][0m Trial 47 finished with value: 0.08493971702071929 and parameters: {'observation_period_num': 17, 'train_rates': 0.6830844146034327, 'learning_rate': 0.0008934831675972258, 'batch_size': 68, 'step_size': 2, 'gamma': 0.8398061422114953}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:18:58,853][0m Trial 48 finished with value: 0.2666373457506065 and parameters: {'observation_period_num': 196, 'train_rates': 0.6533801536649604, 'learning_rate': 0.00021650667552053506, 'batch_size': 49, 'step_size': 2, 'gamma': 0.8017763660620787}. Best is trial 34 with value: 0.07397441527102483.[0m
[32m[I 2025-01-31 03:19:48,721][0m Trial 49 finished with value: 0.09408824611205517 and parameters: {'observation_period_num': 42, 'train_rates': 0.6155435217109643, 'learning_rate': 0.00034312905363263844, 'batch_size': 88, 'step_size': 5, 'gamma': 0.8091612648055324}. Best is trial 34 with value: 0.07397441527102483.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-31 03:19:48,731][0m A new study created in memory with name: no-name-2d671baa-f3ee-4288-8691-ca9ac5441f5b[0m
[32m[I 2025-01-31 03:24:00,818][0m Trial 0 finished with value: 0.23582533464686017 and parameters: {'observation_period_num': 120, 'train_rates': 0.622412972526085, 'learning_rate': 7.559035548973326e-06, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8050427551911105}. Best is trial 0 with value: 0.23582533464686017.[0m
[32m[I 2025-01-31 03:24:31,216][0m Trial 1 finished with value: 0.2845694183101846 and parameters: {'observation_period_num': 155, 'train_rates': 0.6808787299732787, 'learning_rate': 1.7035269053641785e-05, 'batch_size': 156, 'step_size': 12, 'gamma': 0.8485278310204075}. Best is trial 0 with value: 0.23582533464686017.[0m
[32m[I 2025-01-31 03:25:03,385][0m Trial 2 finished with value: 0.13579767131241116 and parameters: {'observation_period_num': 25, 'train_rates': 0.884740152863521, 'learning_rate': 6.484869779752224e-05, 'batch_size': 177, 'step_size': 13, 'gamma': 0.8778703173296132}. Best is trial 2 with value: 0.13579767131241116.[0m
[32m[I 2025-01-31 03:25:26,580][0m Trial 3 finished with value: 0.10851163027911365 and parameters: {'observation_period_num': 56, 'train_rates': 0.8071210568115125, 'learning_rate': 0.0003868627792940309, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9557131960503047}. Best is trial 3 with value: 0.10851163027911365.[0m
[32m[I 2025-01-31 03:25:51,579][0m Trial 4 finished with value: 0.6196012486725809 and parameters: {'observation_period_num': 72, 'train_rates': 0.6051362271573675, 'learning_rate': 1.973130786634247e-06, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9194253523121116}. Best is trial 3 with value: 0.10851163027911365.[0m
[32m[I 2025-01-31 03:26:13,954][0m Trial 5 finished with value: 0.19222833882729398 and parameters: {'observation_period_num': 135, 'train_rates': 0.8485496067630047, 'learning_rate': 0.0003682829010187596, 'batch_size': 239, 'step_size': 3, 'gamma': 0.7633174649583049}. Best is trial 3 with value: 0.10851163027911365.[0m
[32m[I 2025-01-31 03:26:44,159][0m Trial 6 finished with value: 0.29996541142463684 and parameters: {'observation_period_num': 89, 'train_rates': 0.9592691352116891, 'learning_rate': 0.0008729144652779544, 'batch_size': 207, 'step_size': 11, 'gamma': 0.8928705268612174}. Best is trial 3 with value: 0.10851163027911365.[0m
[32m[I 2025-01-31 03:27:17,557][0m Trial 7 finished with value: 0.1380634374681272 and parameters: {'observation_period_num': 130, 'train_rates': 0.8316720738941334, 'learning_rate': 0.0005323031401010376, 'batch_size': 157, 'step_size': 11, 'gamma': 0.9019953785987542}. Best is trial 3 with value: 0.10851163027911365.[0m
[32m[I 2025-01-31 03:28:15,544][0m Trial 8 finished with value: 0.07480635736137628 and parameters: {'observation_period_num': 6, 'train_rates': 0.7286412279030541, 'learning_rate': 0.00011318378612943035, 'batch_size': 85, 'step_size': 8, 'gamma': 0.982405484334268}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:29:43,481][0m Trial 9 finished with value: 0.5575761359017883 and parameters: {'observation_period_num': 156, 'train_rates': 0.7288985394442812, 'learning_rate': 1.7553515085855196e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8404755499630806}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:30:35,016][0m Trial 10 finished with value: 0.18050914885236896 and parameters: {'observation_period_num': 242, 'train_rates': 0.7359094010205265, 'learning_rate': 8.237413126252127e-05, 'batch_size': 89, 'step_size': 7, 'gamma': 0.9826405567601296}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:31:21,900][0m Trial 11 finished with value: 0.08129405642490416 and parameters: {'observation_period_num': 19, 'train_rates': 0.769533643184072, 'learning_rate': 0.00018752266914980913, 'batch_size': 108, 'step_size': 6, 'gamma': 0.9828614709181075}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:32:09,183][0m Trial 12 finished with value: 0.07910042177340966 and parameters: {'observation_period_num': 5, 'train_rates': 0.7424866809685375, 'learning_rate': 0.0001128178457709245, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9889433636315389}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:32:55,173][0m Trial 13 finished with value: 0.11652830860285616 and parameters: {'observation_period_num': 9, 'train_rates': 0.6814770907308985, 'learning_rate': 3.0209242853940535e-05, 'batch_size': 104, 'step_size': 2, 'gamma': 0.9410399449708093}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:33:59,038][0m Trial 14 finished with value: 0.09251906894220521 and parameters: {'observation_period_num': 48, 'train_rates': 0.6844431433217043, 'learning_rate': 0.00011775857669850584, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9486122707462882}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:34:39,239][0m Trial 15 finished with value: 0.24616356505458378 and parameters: {'observation_period_num': 215, 'train_rates': 0.7729789252107552, 'learning_rate': 1.1720672000625933e-05, 'batch_size': 120, 'step_size': 9, 'gamma': 0.9875745188339219}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:36:19,581][0m Trial 16 finished with value: 0.16827236823025782 and parameters: {'observation_period_num': 90, 'train_rates': 0.8903505527449984, 'learning_rate': 4.510240321855789e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.9236675379735998}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:36:58,857][0m Trial 17 finished with value: 0.08579239536290455 and parameters: {'observation_period_num': 5, 'train_rates': 0.7279736340863521, 'learning_rate': 0.00019037316867771554, 'batch_size': 130, 'step_size': 1, 'gamma': 0.9637045583960753}. Best is trial 8 with value: 0.07480635736137628.[0m
[32m[I 2025-01-31 03:41:48,839][0m Trial 18 finished with value: 0.058273430448025465 and parameters: {'observation_period_num': 52, 'train_rates': 0.9889648789937507, 'learning_rate': 2.4810551694002915e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.9302092778840485}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 03:46:53,928][0m Trial 19 finished with value: 0.06778661624805347 and parameters: {'observation_period_num': 40, 'train_rates': 0.9875601538202957, 'learning_rate': 4.709265155066931e-06, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9228694691664578}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 03:52:50,050][0m Trial 20 finished with value: 0.4502699511406356 and parameters: {'observation_period_num': 46, 'train_rates': 0.9750541855812223, 'learning_rate': 4.710988431475901e-06, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8615514707489853}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 03:55:03,775][0m Trial 21 finished with value: 0.25527032697573304 and parameters: {'observation_period_num': 36, 'train_rates': 0.9233735148755571, 'learning_rate': 3.854166167236406e-06, 'batch_size': 42, 'step_size': 8, 'gamma': 0.9221853258219718}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 03:57:44,579][0m Trial 22 finished with value: 0.0985160619020462 and parameters: {'observation_period_num': 72, 'train_rates': 0.9887675229380656, 'learning_rate': 2.653880752924682e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9008642554650554}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 03:59:01,648][0m Trial 23 finished with value: 0.23959212154150009 and parameters: {'observation_period_num': 66, 'train_rates': 0.9377296202877825, 'learning_rate': 1.5000769069347013e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9375599270779368}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:00:21,203][0m Trial 24 finished with value: 0.6812841445207596 and parameters: {'observation_period_num': 98, 'train_rates': 0.9046651505068051, 'learning_rate': 1.1808358487429527e-06, 'batch_size': 68, 'step_size': 6, 'gamma': 0.9645046619923799}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:03:10,141][0m Trial 25 finished with value: 0.14375178854821657 and parameters: {'observation_period_num': 32, 'train_rates': 0.8596547454556218, 'learning_rate': 5.5756031842752875e-06, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9277446493475395}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:04:47,118][0m Trial 26 finished with value: 0.3716847083295684 and parameters: {'observation_period_num': 32, 'train_rates': 0.9431725390345067, 'learning_rate': 2.921390655502388e-06, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8886185657555725}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:05:44,117][0m Trial 27 finished with value: 0.1911404038954705 and parameters: {'observation_period_num': 107, 'train_rates': 0.8097119213583416, 'learning_rate': 8.712587594050992e-06, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9663559881054146}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:10:27,547][0m Trial 28 finished with value: 0.3162091946009026 and parameters: {'observation_period_num': 60, 'train_rates': 0.9678982130367111, 'learning_rate': 5.334190991604907e-05, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9090457749588206}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:12:35,496][0m Trial 29 finished with value: 0.20196683515877834 and parameters: {'observation_period_num': 202, 'train_rates': 0.6502505190402746, 'learning_rate': 2.3157932104629133e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8127209442718419}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:13:40,782][0m Trial 30 finished with value: 0.18002250790596008 and parameters: {'observation_period_num': 116, 'train_rates': 0.9885876280964229, 'learning_rate': 1.0482556890191512e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.8718301874527178}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:14:16,957][0m Trial 31 finished with value: 0.0835935029708132 and parameters: {'observation_period_num': 7, 'train_rates': 0.7585873129381291, 'learning_rate': 0.00016772697468383875, 'batch_size': 147, 'step_size': 3, 'gamma': 0.9742117062203456}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:14:58,397][0m Trial 32 finished with value: 0.0855386747507488 and parameters: {'observation_period_num': 26, 'train_rates': 0.7094198832844556, 'learning_rate': 0.00010967132230704266, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9480987589248453}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:15:31,760][0m Trial 33 finished with value: 0.16692901107244848 and parameters: {'observation_period_num': 47, 'train_rates': 0.6588402222859647, 'learning_rate': 4.156612699716233e-05, 'batch_size': 139, 'step_size': 4, 'gamma': 0.9893262647256834}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:16:26,715][0m Trial 34 finished with value: 0.09540822966198616 and parameters: {'observation_period_num': 18, 'train_rates': 0.7926046698181555, 'learning_rate': 0.0002590518251538672, 'batch_size': 93, 'step_size': 1, 'gamma': 0.9548884115190965}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:21:06,088][0m Trial 35 finished with value: 0.11589081225120965 and parameters: {'observation_period_num': 76, 'train_rates': 0.7039744880494182, 'learning_rate': 7.30534896705613e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9715918782446047}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:21:36,233][0m Trial 36 finished with value: 0.3204436506657453 and parameters: {'observation_period_num': 42, 'train_rates': 0.7553393988768117, 'learning_rate': 1.704039538647124e-05, 'batch_size': 173, 'step_size': 3, 'gamma': 0.838197902635289}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:23:19,755][0m Trial 37 finished with value: 0.08146239874960653 and parameters: {'observation_period_num': 19, 'train_rates': 0.6333450763226245, 'learning_rate': 0.00010629933661678045, 'batch_size': 42, 'step_size': 6, 'gamma': 0.7636971197426968}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:23:49,158][0m Trial 38 finished with value: 0.08340293843238081 and parameters: {'observation_period_num': 5, 'train_rates': 0.792987916093422, 'learning_rate': 0.0003661433189215021, 'batch_size': 193, 'step_size': 12, 'gamma': 0.9363452464218337}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:25:18,291][0m Trial 39 finished with value: 0.16487315039812905 and parameters: {'observation_period_num': 150, 'train_rates': 0.8615244805867459, 'learning_rate': 0.0009172564679629026, 'batch_size': 58, 'step_size': 2, 'gamma': 0.9130930967249133}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:26:27,156][0m Trial 40 finished with value: 0.12105912358222028 and parameters: {'observation_period_num': 57, 'train_rates': 0.8346110477522499, 'learning_rate': 3.7341258580626604e-05, 'batch_size': 77, 'step_size': 14, 'gamma': 0.8849801446211206}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:27:16,332][0m Trial 41 finished with value: 0.08325240933152317 and parameters: {'observation_period_num': 20, 'train_rates': 0.7755703578247819, 'learning_rate': 0.00017303652737795647, 'batch_size': 107, 'step_size': 6, 'gamma': 0.9792070052478492}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:27:54,747][0m Trial 42 finished with value: 0.08843025428159211 and parameters: {'observation_period_num': 23, 'train_rates': 0.7418405779434921, 'learning_rate': 0.00026650906673810273, 'batch_size': 130, 'step_size': 7, 'gamma': 0.9600473222223379}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:28:25,197][0m Trial 43 finished with value: 0.10535978216757165 and parameters: {'observation_period_num': 37, 'train_rates': 0.7097952431888769, 'learning_rate': 6.711922595607478e-05, 'batch_size': 162, 'step_size': 5, 'gamma': 0.9759883841095341}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:29:14,928][0m Trial 44 finished with value: 0.09445615038275719 and parameters: {'observation_period_num': 21, 'train_rates': 0.820852251644361, 'learning_rate': 0.0005720627189494239, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9520048627918962}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:30:00,371][0m Trial 45 finished with value: 0.10124885969860263 and parameters: {'observation_period_num': 53, 'train_rates': 0.7626537758239111, 'learning_rate': 0.0001541035772476556, 'batch_size': 113, 'step_size': 8, 'gamma': 0.9898161994214155}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:30:26,409][0m Trial 46 finished with value: 0.25187695026397705 and parameters: {'observation_period_num': 84, 'train_rates': 0.9442611679298014, 'learning_rate': 0.0006155651089499199, 'batch_size': 229, 'step_size': 10, 'gamma': 0.9309999078284396}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:31:07,401][0m Trial 47 finished with value: 0.07366400784847879 and parameters: {'observation_period_num': 15, 'train_rates': 0.7282094759375067, 'learning_rate': 0.00025183152654780665, 'batch_size': 123, 'step_size': 4, 'gamma': 0.9472706592069489}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:31:45,165][0m Trial 48 finished with value: 0.13139763218646794 and parameters: {'observation_period_num': 183, 'train_rates': 0.6878411632488498, 'learning_rate': 0.00025245959213187615, 'batch_size': 123, 'step_size': 3, 'gamma': 0.943812446453957}. Best is trial 18 with value: 0.058273430448025465.[0m
[32m[I 2025-01-31 04:34:38,709][0m Trial 49 finished with value: 0.07393717137536754 and parameters: {'observation_period_num': 12, 'train_rates': 0.7229373650603468, 'learning_rate': 9.748440248087556e-05, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9126599678280105}. Best is trial 18 with value: 0.058273430448025465.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-31 04:34:38,719][0m A new study created in memory with name: no-name-76dcbaa1-b589-4e2c-a720-15fc5fa25141[0m
[32m[I 2025-01-31 04:36:19,516][0m Trial 0 finished with value: 0.5773305540030258 and parameters: {'observation_period_num': 250, 'train_rates': 0.6444608970745902, 'learning_rate': 1.4725915608743792e-06, 'batch_size': 40, 'step_size': 8, 'gamma': 0.7539101243350286}. Best is trial 0 with value: 0.5773305540030258.[0m
[32m[I 2025-01-31 04:36:57,377][0m Trial 1 finished with value: 0.20658916234970093 and parameters: {'observation_period_num': 152, 'train_rates': 0.9765984394614421, 'learning_rate': 3.737753508977811e-05, 'batch_size': 159, 'step_size': 3, 'gamma': 0.8915819675297283}. Best is trial 1 with value: 0.20658916234970093.[0m
[32m[I 2025-01-31 04:37:56,194][0m Trial 2 finished with value: 0.08893418889796993 and parameters: {'observation_period_num': 23, 'train_rates': 0.8118806773222601, 'learning_rate': 0.00023105726369088942, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8202058486049565}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:38:15,660][0m Trial 3 finished with value: 0.17606107425781514 and parameters: {'observation_period_num': 215, 'train_rates': 0.7155866735969711, 'learning_rate': 0.0003309756333695212, 'batch_size': 254, 'step_size': 15, 'gamma': 0.7700056826597652}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:38:53,022][0m Trial 4 finished with value: 0.16093271336069814 and parameters: {'observation_period_num': 194, 'train_rates': 0.6869718586548116, 'learning_rate': 0.0006431467661974363, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7627353938865338}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:39:15,076][0m Trial 5 finished with value: 0.10903998859025336 and parameters: {'observation_period_num': 21, 'train_rates': 0.6846085503505268, 'learning_rate': 0.0002530841328754756, 'batch_size': 224, 'step_size': 1, 'gamma': 0.9260747918169102}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:39:59,557][0m Trial 6 finished with value: 0.3685162365436554 and parameters: {'observation_period_num': 45, 'train_rates': 0.9682679777555248, 'learning_rate': 0.0004356162567492913, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9811018602689792}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:40:37,306][0m Trial 7 finished with value: 0.34198471903800964 and parameters: {'observation_period_num': 99, 'train_rates': 0.9890905345517789, 'learning_rate': 3.337273720720998e-06, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8741716832117955}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:41:52,996][0m Trial 8 finished with value: 0.4867902560833773 and parameters: {'observation_period_num': 79, 'train_rates': 0.8868212521319814, 'learning_rate': 2.238504039605394e-06, 'batch_size': 71, 'step_size': 3, 'gamma': 0.9261484410887453}. Best is trial 2 with value: 0.08893418889796993.[0m
Early stopping at epoch 58
[32m[I 2025-01-31 04:42:17,159][0m Trial 9 finished with value: 1.2933091706153885 and parameters: {'observation_period_num': 193, 'train_rates': 0.8330510513814, 'learning_rate': 1.3684537850296816e-06, 'batch_size': 133, 'step_size': 1, 'gamma': 0.81732673640476}. Best is trial 2 with value: 0.08893418889796993.[0m
[32m[I 2025-01-31 04:46:34,564][0m Trial 10 finished with value: 0.0729214748334106 and parameters: {'observation_period_num': 7, 'train_rates': 0.7738061365863265, 'learning_rate': 6.814371349377379e-05, 'batch_size': 19, 'step_size': 11, 'gamma': 0.8197290416644879}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 04:50:01,715][0m Trial 11 finished with value: 0.07646646586088639 and parameters: {'observation_period_num': 6, 'train_rates': 0.7840698108908254, 'learning_rate': 6.127798682116793e-05, 'batch_size': 24, 'step_size': 11, 'gamma': 0.8194887098387033}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 04:55:00,849][0m Trial 12 finished with value: 0.11749817608995775 and parameters: {'observation_period_num': 63, 'train_rates': 0.7654630036569469, 'learning_rate': 4.1058402760523655e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8220328809838623}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 04:59:57,439][0m Trial 13 finished with value: 0.11316075263064745 and parameters: {'observation_period_num': 6, 'train_rates': 0.8804459654220513, 'learning_rate': 1.6219646636536734e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8445590279234778}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:01:12,209][0m Trial 14 finished with value: 0.13493835243543223 and parameters: {'observation_period_num': 119, 'train_rates': 0.760431386085775, 'learning_rate': 9.60740972727116e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.7939361977126266}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:03:08,462][0m Trial 15 finished with value: 0.172887742816006 and parameters: {'observation_period_num': 59, 'train_rates': 0.8526248365458822, 'learning_rate': 1.0435246397489943e-05, 'batch_size': 45, 'step_size': 6, 'gamma': 0.8495855560874624}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:03:53,852][0m Trial 16 finished with value: 0.17281959896259333 and parameters: {'observation_period_num': 149, 'train_rates': 0.6061135603352774, 'learning_rate': 6.597313187608481e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.8024077261767967}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:04:21,445][0m Trial 17 finished with value: 0.10637858179871908 and parameters: {'observation_period_num': 39, 'train_rates': 0.770815081947543, 'learning_rate': 0.0001184842390278124, 'batch_size': 196, 'step_size': 15, 'gamma': 0.8522753172041926}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:05:54,040][0m Trial 18 finished with value: 0.16359662684630982 and parameters: {'observation_period_num': 90, 'train_rates': 0.726253093328793, 'learning_rate': 1.4115752645456055e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8993062598496108}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:06:53,069][0m Trial 19 finished with value: 0.2541901800664551 and parameters: {'observation_period_num': 10, 'train_rates': 0.9243745317317285, 'learning_rate': 6.013103016596465e-06, 'batch_size': 100, 'step_size': 13, 'gamma': 0.7850413173940594}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:09:37,121][0m Trial 20 finished with value: 0.16964874690198628 and parameters: {'observation_period_num': 112, 'train_rates': 0.8132772980941813, 'learning_rate': 0.00015256356112971708, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8345880984783169}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:10:42,509][0m Trial 21 finished with value: 0.09650055049122244 and parameters: {'observation_period_num': 35, 'train_rates': 0.8108793685341995, 'learning_rate': 0.0009517015367218664, 'batch_size': 80, 'step_size': 12, 'gamma': 0.808021309018752}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:12:08,896][0m Trial 22 finished with value: 0.07344722479214634 and parameters: {'observation_period_num': 5, 'train_rates': 0.7868281471276188, 'learning_rate': 0.00021142128959745517, 'batch_size': 59, 'step_size': 14, 'gamma': 0.8583984678365807}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:13:38,939][0m Trial 23 finished with value: 0.07829867252419072 and parameters: {'observation_period_num': 6, 'train_rates': 0.7369940554579972, 'learning_rate': 6.368791066267414e-05, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8696375602477634}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:16:12,034][0m Trial 24 finished with value: 0.1224412820871928 and parameters: {'observation_period_num': 60, 'train_rates': 0.8505588856951567, 'learning_rate': 2.2562717036303205e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.8675335996831077}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:21:02,001][0m Trial 25 finished with value: 0.08597250211368068 and parameters: {'observation_period_num': 31, 'train_rates': 0.7889231765491834, 'learning_rate': 6.183213330070668e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7841869332463529}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:21:45,482][0m Trial 26 finished with value: 0.10072676438885184 and parameters: {'observation_period_num': 70, 'train_rates': 0.6969936013819692, 'learning_rate': 0.00016137363786257214, 'batch_size': 109, 'step_size': 14, 'gamma': 0.8330623840378787}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:22:54,193][0m Trial 27 finished with value: 0.11441664692142914 and parameters: {'observation_period_num': 49, 'train_rates': 0.750442493217474, 'learning_rate': 3.063785338397347e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8970552674458375}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:24:20,251][0m Trial 28 finished with value: 0.08437854148483623 and parameters: {'observation_period_num': 23, 'train_rates': 0.7888051737498523, 'learning_rate': 9.279813071956773e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9273154172311114}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:26:12,026][0m Trial 29 finished with value: 0.16527381189734644 and parameters: {'observation_period_num': 149, 'train_rates': 0.6577500578899222, 'learning_rate': 0.0004915788072139346, 'batch_size': 38, 'step_size': 8, 'gamma': 0.9895288773255462}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:28:56,524][0m Trial 30 finished with value: 0.22578341477991162 and parameters: {'observation_period_num': 246, 'train_rates': 0.8696275951338291, 'learning_rate': 0.00018956326412697316, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8352431923074282}. Best is trial 10 with value: 0.0729214748334106.[0m
[32m[I 2025-01-31 05:30:37,400][0m Trial 31 finished with value: 0.07102358713746071 and parameters: {'observation_period_num': 5, 'train_rates': 0.7288715274148763, 'learning_rate': 5.9799780652114716e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8780685769136342}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:32:34,674][0m Trial 32 finished with value: 0.07199905728910087 and parameters: {'observation_period_num': 7, 'train_rates': 0.7179405890204533, 'learning_rate': 4.741509537061971e-05, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8608364057425796}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:34:18,090][0m Trial 33 finished with value: 0.09695528629659252 and parameters: {'observation_period_num': 22, 'train_rates': 0.6395005530014695, 'learning_rate': 3.9865021694297304e-05, 'batch_size': 42, 'step_size': 15, 'gamma': 0.88111897306591}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:35:19,089][0m Trial 34 finished with value: 0.09803078790933001 and parameters: {'observation_period_num': 20, 'train_rates': 0.7147175213320563, 'learning_rate': 3.0372028486195753e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.9133268027920644}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:36:34,285][0m Trial 35 finished with value: 0.09198734670802576 and parameters: {'observation_period_num': 47, 'train_rates': 0.6575339595063459, 'learning_rate': 0.00028994254344249853, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8567482371918009}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:38:09,205][0m Trial 36 finished with value: 0.08689521821023724 and parameters: {'observation_period_num': 31, 'train_rates': 0.7344601526685858, 'learning_rate': 0.00011343117284305496, 'batch_size': 50, 'step_size': 14, 'gamma': 0.9580722448775398}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:38:53,477][0m Trial 37 finished with value: 0.15562304036666727 and parameters: {'observation_period_num': 81, 'train_rates': 0.6895302671666181, 'learning_rate': 2.1954239807495964e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.8839353361515253}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:39:23,576][0m Trial 38 finished with value: 0.09771131046479374 and parameters: {'observation_period_num': 18, 'train_rates': 0.7063357180683495, 'learning_rate': 4.5892338654418073e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8623425923434159}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:39:44,809][0m Trial 39 finished with value: 0.4314044261150223 and parameters: {'observation_period_num': 175, 'train_rates': 0.7498380980504767, 'learning_rate': 7.391008634328989e-06, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9089607644606023}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:42:15,442][0m Trial 40 finished with value: 0.11091889480491364 and parameters: {'observation_period_num': 47, 'train_rates': 0.8214754868687779, 'learning_rate': 0.00022660465022408013, 'batch_size': 34, 'step_size': 12, 'gamma': 0.7544928487463954}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:45:34,949][0m Trial 41 finished with value: 0.07601243877665155 and parameters: {'observation_period_num': 7, 'train_rates': 0.7861338224737914, 'learning_rate': 6.336107053007072e-05, 'batch_size': 25, 'step_size': 9, 'gamma': 0.8418163494301674}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:47:34,680][0m Trial 42 finished with value: 0.08316235735654627 and parameters: {'observation_period_num': 17, 'train_rates': 0.8016358531338047, 'learning_rate': 8.33258902423234e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.8406548004274551}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:48:36,331][0m Trial 43 finished with value: 0.08978127695025842 and parameters: {'observation_period_num': 28, 'train_rates': 0.7772044196046809, 'learning_rate': 0.00012963574310525692, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8806209943181638}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:49:51,152][0m Trial 44 finished with value: 0.07820127773077898 and parameters: {'observation_period_num': 5, 'train_rates': 0.7412437468177685, 'learning_rate': 4.909588489885364e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8260609393874498}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:52:51,118][0m Trial 45 finished with value: 0.0732122525060303 and parameters: {'observation_period_num': 14, 'train_rates': 0.6783439447699345, 'learning_rate': 0.0004054074868672782, 'batch_size': 25, 'step_size': 9, 'gamma': 0.811959618203472}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:57:10,827][0m Trial 46 finished with value: 0.08731567113990099 and parameters: {'observation_period_num': 41, 'train_rates': 0.6808319094436992, 'learning_rate': 0.0004332857215101098, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8103493088457794}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:57:44,048][0m Trial 47 finished with value: 0.0963934731155823 and parameters: {'observation_period_num': 55, 'train_rates': 0.6736191413540585, 'learning_rate': 0.0003669273799959643, 'batch_size': 144, 'step_size': 10, 'gamma': 0.7712531945473502}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 05:59:30,590][0m Trial 48 finished with value: 0.07353034175317642 and parameters: {'observation_period_num': 15, 'train_rates': 0.7235643194473996, 'learning_rate': 0.0005750426793992017, 'batch_size': 45, 'step_size': 15, 'gamma': 0.8572306173890326}. Best is trial 31 with value: 0.07102358713746071.[0m
[32m[I 2025-01-31 06:02:06,753][0m Trial 49 finished with value: 0.09001165146937753 and parameters: {'observation_period_num': 34, 'train_rates': 0.6252288831672037, 'learning_rate': 0.0002166380114062381, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8882813958674028}. Best is trial 31 with value: 0.07102358713746071.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-31 06:02:06,763][0m A new study created in memory with name: no-name-24ea7816-658c-4e37-aa84-1c3e6fa2b4a6[0m
[32m[I 2025-01-31 06:03:29,277][0m Trial 0 finished with value: 0.2218277424836134 and parameters: {'observation_period_num': 121, 'train_rates': 0.6588400663974715, 'learning_rate': 3.055592727057521e-05, 'batch_size': 53, 'step_size': 3, 'gamma': 0.8609329494498636}. Best is trial 0 with value: 0.2218277424836134.[0m
[32m[I 2025-01-31 06:04:39,400][0m Trial 1 finished with value: 0.11791445314884186 and parameters: {'observation_period_num': 236, 'train_rates': 0.9804057987764848, 'learning_rate': 0.0008323111107757726, 'batch_size': 79, 'step_size': 9, 'gamma': 0.853401711887589}. Best is trial 1 with value: 0.11791445314884186.[0m
[32m[I 2025-01-31 06:05:23,629][0m Trial 2 finished with value: 0.48487791419029236 and parameters: {'observation_period_num': 227, 'train_rates': 0.9711289729646329, 'learning_rate': 5.7532236090152024e-05, 'batch_size': 130, 'step_size': 4, 'gamma': 0.9044004518271417}. Best is trial 1 with value: 0.11791445314884186.[0m
[32m[I 2025-01-31 06:05:46,622][0m Trial 3 finished with value: 0.10124838836935476 and parameters: {'observation_period_num': 28, 'train_rates': 0.8279677033684734, 'learning_rate': 0.0006892995336993908, 'batch_size': 248, 'step_size': 4, 'gamma': 0.927507668821397}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:08:36,053][0m Trial 4 finished with value: 0.14839269871484 and parameters: {'observation_period_num': 61, 'train_rates': 0.6452092178227208, 'learning_rate': 9.150895002226872e-06, 'batch_size': 25, 'step_size': 10, 'gamma': 0.7745343398787892}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:09:02,510][0m Trial 5 finished with value: 0.11680487686339601 and parameters: {'observation_period_num': 77, 'train_rates': 0.6857113719801062, 'learning_rate': 0.00012321623453675291, 'batch_size': 181, 'step_size': 9, 'gamma': 0.7829120167547258}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:09:39,749][0m Trial 6 finished with value: 0.1924663955652261 and parameters: {'observation_period_num': 198, 'train_rates': 0.8567602930966203, 'learning_rate': 0.00010809045331712973, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9049593444839079}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:10:15,214][0m Trial 7 finished with value: 0.1900093567104979 and parameters: {'observation_period_num': 181, 'train_rates': 0.8062047831171774, 'learning_rate': 4.376912894719799e-05, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8069655633000624}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:10:52,906][0m Trial 8 finished with value: 0.5292690362249102 and parameters: {'observation_period_num': 148, 'train_rates': 0.7752441804407075, 'learning_rate': 4.156544662463931e-06, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8028990207239604}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:11:52,101][0m Trial 9 finished with value: 0.1392504560809604 and parameters: {'observation_period_num': 47, 'train_rates': 0.8809287763849023, 'learning_rate': 0.0008764081056886816, 'batch_size': 92, 'step_size': 1, 'gamma': 0.8821881204337888}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:12:13,921][0m Trial 10 finished with value: 0.9860857648035225 and parameters: {'observation_period_num': 9, 'train_rates': 0.7374308656751943, 'learning_rate': 1.2380711996260354e-06, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9723922624299164}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:12:35,443][0m Trial 11 finished with value: 0.12854634682621274 and parameters: {'observation_period_num': 86, 'train_rates': 0.6946214333320702, 'learning_rate': 0.00024804663231910813, 'batch_size': 239, 'step_size': 14, 'gamma': 0.9639624217482737}. Best is trial 3 with value: 0.10124838836935476.[0m
[32m[I 2025-01-31 06:13:00,128][0m Trial 12 finished with value: 0.07029012971454196 and parameters: {'observation_period_num': 9, 'train_rates': 0.6176895872064401, 'learning_rate': 0.0003100602505933823, 'batch_size': 200, 'step_size': 7, 'gamma': 0.9355932803353859}. Best is trial 12 with value: 0.07029012971454196.[0m
[32m[I 2025-01-31 06:13:22,655][0m Trial 13 finished with value: 0.07747668167057463 and parameters: {'observation_period_num': 8, 'train_rates': 0.6037481044900171, 'learning_rate': 0.00034191472621404595, 'batch_size': 212, 'step_size': 6, 'gamma': 0.9421317847119164}. Best is trial 12 with value: 0.07029012971454196.[0m
[32m[I 2025-01-31 06:13:44,397][0m Trial 14 finished with value: 0.06915617926082646 and parameters: {'observation_period_num': 8, 'train_rates': 0.6176346006970737, 'learning_rate': 0.000332007832103998, 'batch_size': 204, 'step_size': 7, 'gamma': 0.928631274216935}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:14:06,653][0m Trial 15 finished with value: 0.13658760059657762 and parameters: {'observation_period_num': 109, 'train_rates': 0.6012452419765666, 'learning_rate': 0.0002390926353608916, 'batch_size': 190, 'step_size': 12, 'gamma': 0.9852126848196784}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:14:35,008][0m Trial 16 finished with value: 0.17910071902654387 and parameters: {'observation_period_num': 40, 'train_rates': 0.7282541222490961, 'learning_rate': 1.4224982482495233e-05, 'batch_size': 180, 'step_size': 7, 'gamma': 0.9404300252654092}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:14:57,509][0m Trial 17 finished with value: 0.29854539189396834 and parameters: {'observation_period_num': 89, 'train_rates': 0.6421360302796149, 'learning_rate': 0.00010677006046576104, 'batch_size': 211, 'step_size': 1, 'gamma': 0.9067899230240306}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:15:29,549][0m Trial 18 finished with value: 0.09001313083331679 and parameters: {'observation_period_num': 27, 'train_rates': 0.7553931747960878, 'learning_rate': 0.0004958864607988744, 'batch_size': 164, 'step_size': 11, 'gamma': 0.9544181041562877}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:15:52,712][0m Trial 19 finished with value: 0.13599999203799595 and parameters: {'observation_period_num': 147, 'train_rates': 0.7004074722694201, 'learning_rate': 0.00017493092362366427, 'batch_size': 213, 'step_size': 8, 'gamma': 0.8404979479549272}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:16:13,068][0m Trial 20 finished with value: 0.23691048527933273 and parameters: {'observation_period_num': 63, 'train_rates': 0.6190977597921926, 'learning_rate': 2.413747905347761e-05, 'batch_size': 226, 'step_size': 6, 'gamma': 0.8828657626946982}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:16:35,163][0m Trial 21 finished with value: 0.06978802613654342 and parameters: {'observation_period_num': 8, 'train_rates': 0.6198472416667703, 'learning_rate': 0.00039815758344747415, 'batch_size': 205, 'step_size': 6, 'gamma': 0.929485027633826}. Best is trial 14 with value: 0.06915617926082646.[0m
[32m[I 2025-01-31 06:16:59,473][0m Trial 22 finished with value: 0.06610818728804588 and parameters: {'observation_period_num': 7, 'train_rates': 0.6554212425693922, 'learning_rate': 0.00040988410476031626, 'batch_size': 197, 'step_size': 7, 'gamma': 0.9266831805134134}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:17:28,218][0m Trial 23 finished with value: 0.08894454846622864 and parameters: {'observation_period_num': 32, 'train_rates': 0.6634676607376456, 'learning_rate': 0.0004798631335596242, 'batch_size': 163, 'step_size': 3, 'gamma': 0.9223206613851986}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:17:58,333][0m Trial 24 finished with value: 0.08694929461607821 and parameters: {'observation_period_num': 5, 'train_rates': 0.6824453141963278, 'learning_rate': 7.077218480359785e-05, 'batch_size': 163, 'step_size': 5, 'gamma': 0.9104936070558168}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:18:21,820][0m Trial 25 finished with value: 0.09959011214319617 and parameters: {'observation_period_num': 49, 'train_rates': 0.7243347216066832, 'learning_rate': 0.0004678413626644014, 'batch_size': 231, 'step_size': 8, 'gamma': 0.8894365429159112}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:18:45,964][0m Trial 26 finished with value: 0.0913446524393246 and parameters: {'observation_period_num': 23, 'train_rates': 0.6352730802375226, 'learning_rate': 0.00017164401391095294, 'batch_size': 196, 'step_size': 6, 'gamma': 0.9869491731011066}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:19:38,281][0m Trial 27 finished with value: 0.23064038479650342 and parameters: {'observation_period_num': 55, 'train_rates': 0.9363484602194763, 'learning_rate': 0.0004637642577584576, 'batch_size': 110, 'step_size': 9, 'gamma': 0.9520873977501169}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:20:00,674][0m Trial 28 finished with value: 0.1071753550848677 and parameters: {'observation_period_num': 73, 'train_rates': 0.661906831700703, 'learning_rate': 0.0009564716455170942, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8377264121592385}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:20:26,937][0m Trial 29 finished with value: 0.321967479967254 and parameters: {'observation_period_num': 102, 'train_rates': 0.6623175378794665, 'learning_rate': 3.4764551177730784e-05, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8653164685659387}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:20:58,749][0m Trial 30 finished with value: 0.08522505639943966 and parameters: {'observation_period_num': 19, 'train_rates': 0.6334672776450635, 'learning_rate': 0.0001725910113292079, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9234095586314552}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:21:21,233][0m Trial 31 finished with value: 0.06919762694493287 and parameters: {'observation_period_num': 6, 'train_rates': 0.6100593369880388, 'learning_rate': 0.00030014097931541317, 'batch_size': 202, 'step_size': 7, 'gamma': 0.9373409751522922}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:21:43,750][0m Trial 32 finished with value: 0.13837353923978904 and parameters: {'observation_period_num': 40, 'train_rates': 0.6043755087847648, 'learning_rate': 0.0003051057871926011, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9488547382915121}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:22:08,053][0m Trial 33 finished with value: 0.074669206470196 and parameters: {'observation_period_num': 19, 'train_rates': 0.6309946110286678, 'learning_rate': 0.000624733757408393, 'batch_size': 190, 'step_size': 8, 'gamma': 0.7504465616034438}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:22:29,350][0m Trial 34 finished with value: 0.10494747757911682 and parameters: {'observation_period_num': 30, 'train_rates': 0.6741241821615034, 'learning_rate': 8.317772533226413e-05, 'batch_size': 240, 'step_size': 6, 'gamma': 0.8923666599219147}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:22:53,171][0m Trial 35 finished with value: 0.07327036035426876 and parameters: {'observation_period_num': 5, 'train_rates': 0.70997552582418, 'learning_rate': 0.0006734525392391579, 'batch_size': 219, 'step_size': 4, 'gamma': 0.920109553963726}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:24:11,176][0m Trial 36 finished with value: 0.13204093044943618 and parameters: {'observation_period_num': 42, 'train_rates': 0.649891566081285, 'learning_rate': 0.00018512785693253518, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9673348080969024}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:24:49,488][0m Trial 37 finished with value: 0.10394973750396148 and parameters: {'observation_period_num': 65, 'train_rates': 0.6202793319389711, 'learning_rate': 0.00035715431615597417, 'batch_size': 118, 'step_size': 8, 'gamma': 0.930717869246825}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:25:13,889][0m Trial 38 finished with value: 0.17827913201321507 and parameters: {'observation_period_num': 238, 'train_rates': 0.6513085371148026, 'learning_rate': 0.00012263895652644459, 'batch_size': 181, 'step_size': 10, 'gamma': 0.897782995813591}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:25:33,186][0m Trial 39 finished with value: 0.2089486762881279 and parameters: {'observation_period_num': 209, 'train_rates': 0.6792028078470573, 'learning_rate': 5.6317063031221494e-05, 'batch_size': 242, 'step_size': 4, 'gamma': 0.9586417629700954}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:26:09,263][0m Trial 40 finished with value: 0.07991329326105663 and parameters: {'observation_period_num': 19, 'train_rates': 0.7912872900739844, 'learning_rate': 0.0009885828272841852, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9100162568289252}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:26:33,740][0m Trial 41 finished with value: 0.07621258278103436 and parameters: {'observation_period_num': 18, 'train_rates': 0.6175551004930846, 'learning_rate': 0.0002904576424090859, 'batch_size': 198, 'step_size': 7, 'gamma': 0.936485696593569}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:26:56,288][0m Trial 42 finished with value: 0.09528730222362249 and parameters: {'observation_period_num': 36, 'train_rates': 0.6014643834051869, 'learning_rate': 0.00022697183882005177, 'batch_size': 204, 'step_size': 5, 'gamma': 0.932714120064958}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:27:20,681][0m Trial 43 finished with value: 0.08166367291371421 and parameters: {'observation_period_num': 13, 'train_rates': 0.6218578676704428, 'learning_rate': 0.0006416413964769072, 'batch_size': 187, 'step_size': 9, 'gamma': 0.97483537123238}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:30:38,123][0m Trial 44 finished with value: 0.1995960512830244 and parameters: {'observation_period_num': 133, 'train_rates': 0.6448562351390644, 'learning_rate': 0.0003827048836043236, 'batch_size': 21, 'step_size': 6, 'gamma': 0.9125826615354873}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:31:02,427][0m Trial 45 finished with value: 0.8017626614656729 and parameters: {'observation_period_num': 176, 'train_rates': 0.8407094061103446, 'learning_rate': 2.1745264516257867e-06, 'batch_size': 229, 'step_size': 7, 'gamma': 0.943074978510626}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:31:22,672][0m Trial 46 finished with value: 0.12208946117310114 and parameters: {'observation_period_num': 53, 'train_rates': 0.6151410122299374, 'learning_rate': 0.00013018137410910035, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8746261646976728}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:31:50,868][0m Trial 47 finished with value: 0.07487458112040751 and parameters: {'observation_period_num': 5, 'train_rates': 0.6669949074095571, 'learning_rate': 0.00023659512160018745, 'batch_size': 175, 'step_size': 7, 'gamma': 0.8537157284175705}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:32:21,182][0m Trial 48 finished with value: 0.496101051568985 and parameters: {'observation_period_num': 29, 'train_rates': 0.947734000096678, 'learning_rate': 5.617464384475838e-06, 'batch_size': 200, 'step_size': 2, 'gamma': 0.9751739298468215}. Best is trial 22 with value: 0.06610818728804588.[0m
[32m[I 2025-01-31 06:32:49,606][0m Trial 49 finished with value: 0.12917152030515097 and parameters: {'observation_period_num': 15, 'train_rates': 0.9011511770509758, 'learning_rate': 0.0007042315628457433, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9196521041542215}. Best is trial 22 with value: 0.06610818728804588.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 14, 'train_rates': 0.9882822592893774, 'learning_rate': 8.806475988691994e-05, 'batch_size': 118, 'step_size': 12, 'gamma': 0.9577657322522101}
Epoch 1/300, trend Loss: 0.6437 | 0.3214
Epoch 2/300, trend Loss: 0.2971 | 0.2410
Epoch 3/300, trend Loss: 0.2380 | 0.2348
Epoch 4/300, trend Loss: 0.1908 | 0.1430
Epoch 5/300, trend Loss: 0.1637 | 0.1245
Epoch 6/300, trend Loss: 0.1565 | 0.1264
Epoch 7/300, trend Loss: 0.1484 | 0.1199
Epoch 8/300, trend Loss: 0.1418 | 0.1090
Epoch 9/300, trend Loss: 0.1361 | 0.1009
Epoch 10/300, trend Loss: 0.1314 | 0.0947
Epoch 11/300, trend Loss: 0.1275 | 0.0890
Epoch 12/300, trend Loss: 0.1245 | 0.0836
Epoch 13/300, trend Loss: 0.1224 | 0.0790
Epoch 14/300, trend Loss: 0.1206 | 0.0746
Epoch 15/300, trend Loss: 0.1193 | 0.0706
Epoch 16/300, trend Loss: 0.1188 | 0.0672
Epoch 17/300, trend Loss: 0.1206 | 0.0659
Epoch 18/300, trend Loss: 0.1270 | 0.0724
Epoch 19/300, trend Loss: 0.1394 | 0.1300
Epoch 20/300, trend Loss: 0.1427 | 0.1901
Epoch 21/300, trend Loss: 0.1358 | 0.0959
Epoch 22/300, trend Loss: 0.1372 | 0.0777
Epoch 23/300, trend Loss: 0.1419 | 0.0723
Epoch 24/300, trend Loss: 0.1354 | 0.0608
Epoch 25/300, trend Loss: 0.1454 | 0.0627
Epoch 26/300, trend Loss: 0.1515 | 0.0672
Epoch 27/300, trend Loss: 0.1372 | 0.0637
Epoch 28/300, trend Loss: 0.1146 | 0.0579
Epoch 29/300, trend Loss: 0.1163 | 0.0610
Epoch 30/300, trend Loss: 0.1189 | 0.0644
Epoch 31/300, trend Loss: 0.1115 | 0.0594
Epoch 32/300, trend Loss: 0.1045 | 0.0565
Epoch 33/300, trend Loss: 0.1035 | 0.0557
Epoch 34/300, trend Loss: 0.1029 | 0.0559
Epoch 35/300, trend Loss: 0.1023 | 0.0562
Epoch 36/300, trend Loss: 0.1014 | 0.0563
Epoch 37/300, trend Loss: 0.1004 | 0.0562
Epoch 38/300, trend Loss: 0.0994 | 0.0558
Epoch 39/300, trend Loss: 0.0987 | 0.0556
Epoch 40/300, trend Loss: 0.0981 | 0.0554
Epoch 41/300, trend Loss: 0.0976 | 0.0552
Epoch 42/300, trend Loss: 0.0972 | 0.0550
Epoch 43/300, trend Loss: 0.0971 | 0.0550
Epoch 44/300, trend Loss: 0.0974 | 0.0556
Epoch 45/300, trend Loss: 0.0981 | 0.0577
Epoch 46/300, trend Loss: 0.0991 | 0.0627
Epoch 47/300, trend Loss: 0.0994 | 0.0704
Epoch 48/300, trend Loss: 0.0980 | 0.0750
Epoch 49/300, trend Loss: 0.0959 | 0.0696
Epoch 50/300, trend Loss: 0.0980 | 0.0565
Epoch 51/300, trend Loss: 0.1078 | 0.0641
Epoch 52/300, trend Loss: 0.1126 | 0.0684
Epoch 53/300, trend Loss: 0.1058 | 0.0601
Epoch 54/300, trend Loss: 0.0959 | 0.0543
Epoch 55/300, trend Loss: 0.0939 | 0.0524
Epoch 56/300, trend Loss: 0.1011 | 0.0564
Epoch 57/300, trend Loss: 0.1042 | 0.0601
Epoch 58/300, trend Loss: 0.1013 | 0.0598
Epoch 59/300, trend Loss: 0.0946 | 0.0576
Epoch 60/300, trend Loss: 0.0902 | 0.0539
Epoch 61/300, trend Loss: 0.0908 | 0.0505
Epoch 62/300, trend Loss: 0.0941 | 0.0525
Epoch 63/300, trend Loss: 0.0947 | 0.0551
Epoch 64/300, trend Loss: 0.0906 | 0.0505
Epoch 65/300, trend Loss: 0.0878 | 0.0475
Epoch 66/300, trend Loss: 0.0886 | 0.0473
Epoch 67/300, trend Loss: 0.0900 | 0.0493
Epoch 68/300, trend Loss: 0.0901 | 0.0515
Epoch 69/300, trend Loss: 0.0885 | 0.0518
Epoch 70/300, trend Loss: 0.0863 | 0.0492
Epoch 71/300, trend Loss: 0.0855 | 0.0459
Epoch 72/300, trend Loss: 0.0873 | 0.0452
Epoch 73/300, trend Loss: 0.0905 | 0.0493
Epoch 74/300, trend Loss: 0.0904 | 0.0495
Epoch 75/300, trend Loss: 0.0866 | 0.0449
Epoch 76/300, trend Loss: 0.0852 | 0.0430
Epoch 77/300, trend Loss: 0.0880 | 0.0464
Epoch 78/300, trend Loss: 0.0914 | 0.0537
Epoch 79/300, trend Loss: 0.0919 | 0.0568
Epoch 80/300, trend Loss: 0.0884 | 0.0480
Epoch 81/300, trend Loss: 0.0845 | 0.0431
Epoch 82/300, trend Loss: 0.0846 | 0.0441
Epoch 83/300, trend Loss: 0.0858 | 0.0461
Epoch 84/300, trend Loss: 0.0843 | 0.0443
Epoch 85/300, trend Loss: 0.0822 | 0.0418
Epoch 86/300, trend Loss: 0.0819 | 0.0419
Epoch 87/300, trend Loss: 0.0830 | 0.0439
Epoch 88/300, trend Loss: 0.0841 | 0.0453
Epoch 89/300, trend Loss: 0.0846 | 0.0448
Epoch 90/300, trend Loss: 0.0851 | 0.0442
Epoch 91/300, trend Loss: 0.0859 | 0.0432
Epoch 92/300, trend Loss: 0.0851 | 0.0416
Epoch 93/300, trend Loss: 0.0835 | 0.0413
Epoch 94/300, trend Loss: 0.0850 | 0.0426
Epoch 95/300, trend Loss: 0.0897 | 0.0450
Epoch 96/300, trend Loss: 0.0948 | 0.0464
Epoch 97/300, trend Loss: 0.0933 | 0.0452
Epoch 98/300, trend Loss: 0.0854 | 0.0440
Epoch 99/300, trend Loss: 0.0901 | 0.0478
Epoch 100/300, trend Loss: 0.0880 | 0.0462
Epoch 101/300, trend Loss: 0.0833 | 0.0423
Epoch 102/300, trend Loss: 0.0794 | 0.0399
Epoch 103/300, trend Loss: 0.0784 | 0.0390
Epoch 104/300, trend Loss: 0.0782 | 0.0385
Epoch 105/300, trend Loss: 0.0781 | 0.0382
Epoch 106/300, trend Loss: 0.0779 | 0.0380
Epoch 107/300, trend Loss: 0.0776 | 0.0378
Epoch 108/300, trend Loss: 0.0773 | 0.0378
Epoch 109/300, trend Loss: 0.0770 | 0.0380
Epoch 110/300, trend Loss: 0.0769 | 0.0384
Epoch 111/300, trend Loss: 0.0768 | 0.0387
Epoch 112/300, trend Loss: 0.0766 | 0.0387
Epoch 113/300, trend Loss: 0.0763 | 0.0385
Epoch 114/300, trend Loss: 0.0760 | 0.0379
Epoch 115/300, trend Loss: 0.0758 | 0.0373
Epoch 116/300, trend Loss: 0.0758 | 0.0369
Epoch 117/300, trend Loss: 0.0759 | 0.0370
Epoch 118/300, trend Loss: 0.0759 | 0.0373
Epoch 119/300, trend Loss: 0.0758 | 0.0376
Epoch 120/300, trend Loss: 0.0755 | 0.0377
Epoch 121/300, trend Loss: 0.0753 | 0.0378
Epoch 122/300, trend Loss: 0.0756 | 0.0387
Epoch 123/300, trend Loss: 0.0763 | 0.0407
Epoch 124/300, trend Loss: 0.0772 | 0.0434
Epoch 125/300, trend Loss: 0.0775 | 0.0453
Epoch 126/300, trend Loss: 0.0769 | 0.0441
Epoch 127/300, trend Loss: 0.0761 | 0.0400
Epoch 128/300, trend Loss: 0.0761 | 0.0365
Epoch 129/300, trend Loss: 0.0777 | 0.0375
Epoch 130/300, trend Loss: 0.0791 | 0.0409
Epoch 131/300, trend Loss: 0.0780 | 0.0400
Epoch 132/300, trend Loss: 0.0767 | 0.0392
Epoch 133/300, trend Loss: 0.0768 | 0.0419
Epoch 134/300, trend Loss: 0.0766 | 0.0430
Epoch 135/300, trend Loss: 0.0752 | 0.0407
Epoch 136/300, trend Loss: 0.0741 | 0.0383
Epoch 137/300, trend Loss: 0.0748 | 0.0378
Epoch 138/300, trend Loss: 0.0773 | 0.0395
Epoch 139/300, trend Loss: 0.0816 | 0.0421
Epoch 140/300, trend Loss: 0.0841 | 0.0420
Epoch 141/300, trend Loss: 0.0802 | 0.0398
Epoch 142/300, trend Loss: 0.0803 | 0.0398
Epoch 143/300, trend Loss: 0.0801 | 0.0414
Epoch 144/300, trend Loss: 0.0790 | 0.0403
Epoch 145/300, trend Loss: 0.0764 | 0.0384
Epoch 146/300, trend Loss: 0.0737 | 0.0376
Epoch 147/300, trend Loss: 0.0731 | 0.0375
Epoch 148/300, trend Loss: 0.0731 | 0.0372
Epoch 149/300, trend Loss: 0.0730 | 0.0368
Epoch 150/300, trend Loss: 0.0726 | 0.0366
Epoch 151/300, trend Loss: 0.0721 | 0.0366
Epoch 152/300, trend Loss: 0.0719 | 0.0368
Epoch 153/300, trend Loss: 0.0718 | 0.0373
Epoch 154/300, trend Loss: 0.0717 | 0.0375
Epoch 155/300, trend Loss: 0.0715 | 0.0374
Epoch 156/300, trend Loss: 0.0713 | 0.0371
Epoch 157/300, trend Loss: 0.0712 | 0.0368
Epoch 158/300, trend Loss: 0.0711 | 0.0366
Epoch 159/300, trend Loss: 0.0711 | 0.0366
Epoch 160/300, trend Loss: 0.0710 | 0.0366
Epoch 161/300, trend Loss: 0.0709 | 0.0367
Epoch 162/300, trend Loss: 0.0707 | 0.0368
Epoch 163/300, trend Loss: 0.0707 | 0.0370
Epoch 164/300, trend Loss: 0.0706 | 0.0372
Epoch 165/300, trend Loss: 0.0705 | 0.0372
Epoch 166/300, trend Loss: 0.0703 | 0.0370
Epoch 167/300, trend Loss: 0.0701 | 0.0365
Epoch 168/300, trend Loss: 0.0700 | 0.0359
Epoch 169/300, trend Loss: 0.0700 | 0.0354
Epoch 170/300, trend Loss: 0.0702 | 0.0353
Epoch 171/300, trend Loss: 0.0704 | 0.0356
Epoch 172/300, trend Loss: 0.0706 | 0.0359
Epoch 173/300, trend Loss: 0.0706 | 0.0362
Epoch 174/300, trend Loss: 0.0705 | 0.0364
Epoch 175/300, trend Loss: 0.0703 | 0.0369
Epoch 176/300, trend Loss: 0.0699 | 0.0373
Epoch 177/300, trend Loss: 0.0697 | 0.0375
Epoch 178/300, trend Loss: 0.0703 | 0.0375
Epoch 179/300, trend Loss: 0.0724 | 0.0377
Epoch 180/300, trend Loss: 0.0757 | 0.0379
Epoch 181/300, trend Loss: 0.0769 | 0.0371
Epoch 182/300, trend Loss: 0.0742 | 0.0360
Epoch 183/300, trend Loss: 0.0743 | 0.0359
Epoch 184/300, trend Loss: 0.0732 | 0.0362
Epoch 185/300, trend Loss: 0.0727 | 0.0378
Epoch 186/300, trend Loss: 0.0724 | 0.0408
Epoch 187/300, trend Loss: 0.0703 | 0.0415
Epoch 188/300, trend Loss: 0.0690 | 0.0360
Epoch 189/300, trend Loss: 0.0690 | 0.0342
Epoch 190/300, trend Loss: 0.0692 | 0.0353
Epoch 191/300, trend Loss: 0.0685 | 0.0349
Epoch 192/300, trend Loss: 0.0679 | 0.0348
Epoch 193/300, trend Loss: 0.0678 | 0.0364
Epoch 194/300, trend Loss: 0.0678 | 0.0359
Epoch 195/300, trend Loss: 0.0674 | 0.0344
Epoch 196/300, trend Loss: 0.0672 | 0.0339
Epoch 197/300, trend Loss: 0.0671 | 0.0339
Epoch 198/300, trend Loss: 0.0670 | 0.0340
Epoch 199/300, trend Loss: 0.0668 | 0.0342
Epoch 200/300, trend Loss: 0.0667 | 0.0343
Epoch 201/300, trend Loss: 0.0667 | 0.0343
Epoch 202/300, trend Loss: 0.0665 | 0.0340
Epoch 203/300, trend Loss: 0.0664 | 0.0337
Epoch 204/300, trend Loss: 0.0663 | 0.0335
Epoch 205/300, trend Loss: 0.0662 | 0.0335
Epoch 206/300, trend Loss: 0.0662 | 0.0335
Epoch 207/300, trend Loss: 0.0661 | 0.0336
Epoch 208/300, trend Loss: 0.0660 | 0.0336
Epoch 209/300, trend Loss: 0.0659 | 0.0336
Epoch 210/300, trend Loss: 0.0658 | 0.0335
Epoch 211/300, trend Loss: 0.0657 | 0.0334
Epoch 212/300, trend Loss: 0.0657 | 0.0332
Epoch 213/300, trend Loss: 0.0656 | 0.0331
Epoch 214/300, trend Loss: 0.0655 | 0.0331
Epoch 215/300, trend Loss: 0.0654 | 0.0331
Epoch 216/300, trend Loss: 0.0654 | 0.0331
Epoch 217/300, trend Loss: 0.0653 | 0.0331
Epoch 218/300, trend Loss: 0.0652 | 0.0332
Epoch 219/300, trend Loss: 0.0652 | 0.0331
Epoch 220/300, trend Loss: 0.0651 | 0.0330
Epoch 221/300, trend Loss: 0.0650 | 0.0329
Epoch 222/300, trend Loss: 0.0649 | 0.0327
Epoch 223/300, trend Loss: 0.0649 | 0.0326
Epoch 224/300, trend Loss: 0.0648 | 0.0325
Epoch 225/300, trend Loss: 0.0647 | 0.0324
Epoch 226/300, trend Loss: 0.0646 | 0.0323
Epoch 227/300, trend Loss: 0.0646 | 0.0323
Epoch 228/300, trend Loss: 0.0645 | 0.0323
Epoch 229/300, trend Loss: 0.0646 | 0.0322
Epoch 230/300, trend Loss: 0.0647 | 0.0321
Epoch 231/300, trend Loss: 0.0649 | 0.0320
Epoch 232/300, trend Loss: 0.0652 | 0.0317
Epoch 233/300, trend Loss: 0.0659 | 0.0317
Epoch 234/300, trend Loss: 0.0669 | 0.0321
Epoch 235/300, trend Loss: 0.0677 | 0.0325
Epoch 236/300, trend Loss: 0.0670 | 0.0319
Epoch 237/300, trend Loss: 0.0664 | 0.0326
Epoch 238/300, trend Loss: 0.0705 | 0.0357
Epoch 239/300, trend Loss: 0.0747 | 0.0360
Epoch 240/300, trend Loss: 0.0707 | 0.0330
Epoch 241/300, trend Loss: 0.0683 | 0.0315
Epoch 242/300, trend Loss: 0.0671 | 0.0318
Epoch 243/300, trend Loss: 0.0656 | 0.0332
Epoch 244/300, trend Loss: 0.0648 | 0.0336
Epoch 245/300, trend Loss: 0.0642 | 0.0323
Epoch 246/300, trend Loss: 0.0641 | 0.0311
Epoch 247/300, trend Loss: 0.0639 | 0.0310
Epoch 248/300, trend Loss: 0.0637 | 0.0311
Epoch 249/300, trend Loss: 0.0636 | 0.0310
Epoch 250/300, trend Loss: 0.0635 | 0.0313
Epoch 251/300, trend Loss: 0.0635 | 0.0316
Epoch 252/300, trend Loss: 0.0635 | 0.0315
Epoch 253/300, trend Loss: 0.0633 | 0.0310
Epoch 254/300, trend Loss: 0.0633 | 0.0306
Epoch 255/300, trend Loss: 0.0632 | 0.0305
Epoch 256/300, trend Loss: 0.0632 | 0.0305
Epoch 257/300, trend Loss: 0.0630 | 0.0304
Epoch 258/300, trend Loss: 0.0630 | 0.0305
Epoch 259/300, trend Loss: 0.0629 | 0.0305
Epoch 260/300, trend Loss: 0.0629 | 0.0304
Epoch 261/300, trend Loss: 0.0628 | 0.0302
Epoch 262/300, trend Loss: 0.0627 | 0.0300
Epoch 263/300, trend Loss: 0.0627 | 0.0298
Epoch 264/300, trend Loss: 0.0626 | 0.0298
Epoch 265/300, trend Loss: 0.0626 | 0.0298
Epoch 266/300, trend Loss: 0.0625 | 0.0299
Epoch 267/300, trend Loss: 0.0625 | 0.0299
Epoch 268/300, trend Loss: 0.0624 | 0.0299
Epoch 269/300, trend Loss: 0.0624 | 0.0298
Epoch 270/300, trend Loss: 0.0623 | 0.0296
Epoch 271/300, trend Loss: 0.0622 | 0.0294
Epoch 272/300, trend Loss: 0.0622 | 0.0292
Epoch 273/300, trend Loss: 0.0621 | 0.0291
Epoch 274/300, trend Loss: 0.0621 | 0.0290
Epoch 275/300, trend Loss: 0.0620 | 0.0290
Epoch 276/300, trend Loss: 0.0619 | 0.0290
Epoch 277/300, trend Loss: 0.0619 | 0.0290
Epoch 278/300, trend Loss: 0.0619 | 0.0290
Epoch 279/300, trend Loss: 0.0618 | 0.0289
Epoch 280/300, trend Loss: 0.0617 | 0.0288
Epoch 281/300, trend Loss: 0.0617 | 0.0286
Epoch 282/300, trend Loss: 0.0616 | 0.0284
Epoch 283/300, trend Loss: 0.0616 | 0.0283
Epoch 284/300, trend Loss: 0.0616 | 0.0283
Epoch 285/300, trend Loss: 0.0616 | 0.0284
Epoch 286/300, trend Loss: 0.0615 | 0.0285
Epoch 287/300, trend Loss: 0.0615 | 0.0287
Epoch 288/300, trend Loss: 0.0616 | 0.0290
Epoch 289/300, trend Loss: 0.0617 | 0.0293
Epoch 290/300, trend Loss: 0.0617 | 0.0294
Epoch 291/300, trend Loss: 0.0616 | 0.0291
Epoch 292/300, trend Loss: 0.0614 | 0.0286
Epoch 293/300, trend Loss: 0.0612 | 0.0281
Epoch 294/300, trend Loss: 0.0612 | 0.0276
Epoch 295/300, trend Loss: 0.0613 | 0.0271
Epoch 296/300, trend Loss: 0.0616 | 0.0269
Epoch 297/300, trend Loss: 0.0621 | 0.0268
Epoch 298/300, trend Loss: 0.0627 | 0.0272
Epoch 299/300, trend Loss: 0.0631 | 0.0276
Epoch 300/300, trend Loss: 0.0623 | 0.0279
Training seasonal_0 component with params: {'observation_period_num': 37, 'train_rates': 0.9835390887107553, 'learning_rate': 2.2492669107258797e-05, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8995124516941069}
Epoch 1/300, seasonal_0 Loss: 0.5151 | 0.3294
Epoch 2/300, seasonal_0 Loss: 0.2945 | 0.2181
Epoch 3/300, seasonal_0 Loss: 0.2260 | 0.1745
Epoch 4/300, seasonal_0 Loss: 0.1949 | 0.1487
Epoch 5/300, seasonal_0 Loss: 0.1767 | 0.1331
Epoch 6/300, seasonal_0 Loss: 0.1640 | 0.1202
Epoch 7/300, seasonal_0 Loss: 0.1543 | 0.1100
Epoch 8/300, seasonal_0 Loss: 0.1468 | 0.1030
Epoch 9/300, seasonal_0 Loss: 0.1413 | 0.0969
Epoch 10/300, seasonal_0 Loss: 0.1369 | 0.0919
Epoch 11/300, seasonal_0 Loss: 0.1333 | 0.0879
Epoch 12/300, seasonal_0 Loss: 0.1303 | 0.0851
Epoch 13/300, seasonal_0 Loss: 0.1280 | 0.0824
Epoch 14/300, seasonal_0 Loss: 0.1258 | 0.0801
Epoch 15/300, seasonal_0 Loss: 0.1239 | 0.0786
Epoch 16/300, seasonal_0 Loss: 0.1222 | 0.0769
Epoch 17/300, seasonal_0 Loss: 0.1206 | 0.0754
Epoch 18/300, seasonal_0 Loss: 0.1191 | 0.0739
Epoch 19/300, seasonal_0 Loss: 0.1176 | 0.0729
Epoch 20/300, seasonal_0 Loss: 0.1164 | 0.0719
Epoch 21/300, seasonal_0 Loss: 0.1151 | 0.0709
Epoch 22/300, seasonal_0 Loss: 0.1139 | 0.0698
Epoch 23/300, seasonal_0 Loss: 0.1128 | 0.0692
Epoch 24/300, seasonal_0 Loss: 0.1118 | 0.0684
Epoch 25/300, seasonal_0 Loss: 0.1108 | 0.0677
Epoch 26/300, seasonal_0 Loss: 0.1098 | 0.0664
Epoch 27/300, seasonal_0 Loss: 0.1089 | 0.0660
Epoch 28/300, seasonal_0 Loss: 0.1080 | 0.0655
Epoch 29/300, seasonal_0 Loss: 0.1072 | 0.0642
Epoch 30/300, seasonal_0 Loss: 0.1064 | 0.0639
Epoch 31/300, seasonal_0 Loss: 0.1057 | 0.0635
Epoch 32/300, seasonal_0 Loss: 0.1050 | 0.0631
Epoch 33/300, seasonal_0 Loss: 0.1042 | 0.0619
Epoch 34/300, seasonal_0 Loss: 0.1036 | 0.0616
Epoch 35/300, seasonal_0 Loss: 0.1030 | 0.0613
Epoch 36/300, seasonal_0 Loss: 0.1024 | 0.0606
Epoch 37/300, seasonal_0 Loss: 0.1018 | 0.0603
Epoch 38/300, seasonal_0 Loss: 0.1013 | 0.0600
Epoch 39/300, seasonal_0 Loss: 0.1008 | 0.0598
Epoch 40/300, seasonal_0 Loss: 0.1002 | 0.0594
Epoch 41/300, seasonal_0 Loss: 0.0998 | 0.0591
Epoch 42/300, seasonal_0 Loss: 0.0993 | 0.0589
Epoch 43/300, seasonal_0 Loss: 0.0988 | 0.0587
Epoch 44/300, seasonal_0 Loss: 0.0984 | 0.0585
Epoch 45/300, seasonal_0 Loss: 0.0980 | 0.0583
Epoch 46/300, seasonal_0 Loss: 0.0977 | 0.0581
Epoch 47/300, seasonal_0 Loss: 0.0972 | 0.0580
Epoch 48/300, seasonal_0 Loss: 0.0969 | 0.0578
Epoch 49/300, seasonal_0 Loss: 0.0965 | 0.0576
Epoch 50/300, seasonal_0 Loss: 0.0961 | 0.0575
Epoch 51/300, seasonal_0 Loss: 0.0958 | 0.0573
Epoch 52/300, seasonal_0 Loss: 0.0956 | 0.0572
Epoch 53/300, seasonal_0 Loss: 0.0953 | 0.0571
Epoch 54/300, seasonal_0 Loss: 0.0949 | 0.0569
Epoch 55/300, seasonal_0 Loss: 0.0947 | 0.0568
Epoch 56/300, seasonal_0 Loss: 0.0944 | 0.0567
Epoch 57/300, seasonal_0 Loss: 0.0941 | 0.0566
Epoch 58/300, seasonal_0 Loss: 0.0939 | 0.0565
Epoch 59/300, seasonal_0 Loss: 0.0937 | 0.0564
Epoch 60/300, seasonal_0 Loss: 0.0934 | 0.0563
Epoch 61/300, seasonal_0 Loss: 0.0932 | 0.0562
Epoch 62/300, seasonal_0 Loss: 0.0930 | 0.0561
Epoch 63/300, seasonal_0 Loss: 0.0928 | 0.0560
Epoch 64/300, seasonal_0 Loss: 0.0926 | 0.0560
Epoch 65/300, seasonal_0 Loss: 0.0924 | 0.0559
Epoch 66/300, seasonal_0 Loss: 0.0922 | 0.0558
Epoch 67/300, seasonal_0 Loss: 0.0921 | 0.0557
Epoch 68/300, seasonal_0 Loss: 0.0919 | 0.0557
Epoch 69/300, seasonal_0 Loss: 0.0917 | 0.0556
Epoch 70/300, seasonal_0 Loss: 0.0916 | 0.0556
Epoch 71/300, seasonal_0 Loss: 0.0914 | 0.0556
Epoch 72/300, seasonal_0 Loss: 0.0913 | 0.0555
Epoch 73/300, seasonal_0 Loss: 0.0912 | 0.0555
Epoch 74/300, seasonal_0 Loss: 0.0910 | 0.0554
Epoch 75/300, seasonal_0 Loss: 0.0909 | 0.0555
Epoch 76/300, seasonal_0 Loss: 0.0908 | 0.0554
Epoch 77/300, seasonal_0 Loss: 0.0907 | 0.0554
Epoch 78/300, seasonal_0 Loss: 0.0906 | 0.0555
Epoch 79/300, seasonal_0 Loss: 0.0905 | 0.0554
Epoch 80/300, seasonal_0 Loss: 0.0904 | 0.0553
Epoch 81/300, seasonal_0 Loss: 0.0903 | 0.0552
Epoch 82/300, seasonal_0 Loss: 0.0902 | 0.0552
Epoch 83/300, seasonal_0 Loss: 0.0901 | 0.0552
Epoch 84/300, seasonal_0 Loss: 0.0900 | 0.0551
Epoch 85/300, seasonal_0 Loss: 0.0899 | 0.0549
Epoch 86/300, seasonal_0 Loss: 0.0899 | 0.0548
Epoch 87/300, seasonal_0 Loss: 0.0898 | 0.0547
Epoch 88/300, seasonal_0 Loss: 0.0897 | 0.0546
Epoch 89/300, seasonal_0 Loss: 0.0896 | 0.0544
Epoch 90/300, seasonal_0 Loss: 0.0896 | 0.0543
Epoch 91/300, seasonal_0 Loss: 0.0895 | 0.0543
Epoch 92/300, seasonal_0 Loss: 0.0894 | 0.0542
Epoch 93/300, seasonal_0 Loss: 0.0894 | 0.0542
Epoch 94/300, seasonal_0 Loss: 0.0893 | 0.0541
Epoch 95/300, seasonal_0 Loss: 0.0892 | 0.0541
Epoch 96/300, seasonal_0 Loss: 0.0891 | 0.0543
Epoch 97/300, seasonal_0 Loss: 0.0891 | 0.0543
Epoch 98/300, seasonal_0 Loss: 0.0890 | 0.0543
Epoch 99/300, seasonal_0 Loss: 0.0889 | 0.0545
Epoch 100/300, seasonal_0 Loss: 0.0889 | 0.0545
Epoch 101/300, seasonal_0 Loss: 0.0888 | 0.0545
Epoch 102/300, seasonal_0 Loss: 0.0887 | 0.0545
Epoch 103/300, seasonal_0 Loss: 0.0887 | 0.0547
Epoch 104/300, seasonal_0 Loss: 0.0886 | 0.0547
Epoch 105/300, seasonal_0 Loss: 0.0886 | 0.0547
Epoch 106/300, seasonal_0 Loss: 0.0885 | 0.0549
Epoch 107/300, seasonal_0 Loss: 0.0884 | 0.0549
Epoch 108/300, seasonal_0 Loss: 0.0884 | 0.0548
Epoch 109/300, seasonal_0 Loss: 0.0883 | 0.0548
Epoch 110/300, seasonal_0 Loss: 0.0883 | 0.0549
Epoch 111/300, seasonal_0 Loss: 0.0882 | 0.0549
Epoch 112/300, seasonal_0 Loss: 0.0882 | 0.0549
Epoch 113/300, seasonal_0 Loss: 0.0882 | 0.0550
Epoch 114/300, seasonal_0 Loss: 0.0881 | 0.0549
Epoch 115/300, seasonal_0 Loss: 0.0881 | 0.0549
Epoch 116/300, seasonal_0 Loss: 0.0880 | 0.0549
Epoch 117/300, seasonal_0 Loss: 0.0880 | 0.0549
Epoch 118/300, seasonal_0 Loss: 0.0880 | 0.0549
Epoch 119/300, seasonal_0 Loss: 0.0879 | 0.0549
Epoch 120/300, seasonal_0 Loss: 0.0879 | 0.0549
Epoch 121/300, seasonal_0 Loss: 0.0878 | 0.0549
Epoch 122/300, seasonal_0 Loss: 0.0878 | 0.0549
Epoch 123/300, seasonal_0 Loss: 0.0878 | 0.0548
Epoch 124/300, seasonal_0 Loss: 0.0877 | 0.0549
Epoch 125/300, seasonal_0 Loss: 0.0877 | 0.0548
Epoch 126/300, seasonal_0 Loss: 0.0877 | 0.0548
Epoch 127/300, seasonal_0 Loss: 0.0877 | 0.0548
Epoch 128/300, seasonal_0 Loss: 0.0876 | 0.0548
Epoch 129/300, seasonal_0 Loss: 0.0876 | 0.0548
Epoch 130/300, seasonal_0 Loss: 0.0876 | 0.0548
Epoch 131/300, seasonal_0 Loss: 0.0876 | 0.0548
Epoch 132/300, seasonal_0 Loss: 0.0875 | 0.0548
Epoch 133/300, seasonal_0 Loss: 0.0875 | 0.0547
Epoch 134/300, seasonal_0 Loss: 0.0875 | 0.0547
Epoch 135/300, seasonal_0 Loss: 0.0875 | 0.0547
Epoch 136/300, seasonal_0 Loss: 0.0874 | 0.0547
Epoch 137/300, seasonal_0 Loss: 0.0874 | 0.0547
Epoch 138/300, seasonal_0 Loss: 0.0874 | 0.0547
Epoch 139/300, seasonal_0 Loss: 0.0874 | 0.0547
Epoch 140/300, seasonal_0 Loss: 0.0874 | 0.0547
Epoch 141/300, seasonal_0 Loss: 0.0873 | 0.0547
Epoch 142/300, seasonal_0 Loss: 0.0873 | 0.0547
Epoch 143/300, seasonal_0 Loss: 0.0873 | 0.0546
Epoch 144/300, seasonal_0 Loss: 0.0873 | 0.0546
Epoch 145/300, seasonal_0 Loss: 0.0873 | 0.0546
Epoch 146/300, seasonal_0 Loss: 0.0873 | 0.0546
Epoch 147/300, seasonal_0 Loss: 0.0873 | 0.0546
Epoch 148/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 149/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 150/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 151/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 152/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 153/300, seasonal_0 Loss: 0.0872 | 0.0546
Epoch 154/300, seasonal_0 Loss: 0.0872 | 0.0545
Epoch 155/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 156/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 157/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 158/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 159/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 160/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 161/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 162/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 163/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 164/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 165/300, seasonal_0 Loss: 0.0871 | 0.0545
Epoch 166/300, seasonal_0 Loss: 0.0870 | 0.0545
Epoch 167/300, seasonal_0 Loss: 0.0870 | 0.0545
Epoch 168/300, seasonal_0 Loss: 0.0870 | 0.0545
Epoch 169/300, seasonal_0 Loss: 0.0870 | 0.0545
Epoch 170/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 171/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 172/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 173/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 174/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 175/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 176/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 177/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 178/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 179/300, seasonal_0 Loss: 0.0870 | 0.0544
Epoch 180/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 181/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 182/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 183/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 184/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 185/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 186/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 187/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 188/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 189/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 190/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 191/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 192/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 193/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 194/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 195/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 196/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 197/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 198/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 199/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 200/300, seasonal_0 Loss: 0.0869 | 0.0544
Epoch 201/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 202/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 203/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 204/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 205/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 206/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 207/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 208/300, seasonal_0 Loss: 0.0869 | 0.0543
Epoch 209/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 210/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 211/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 212/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 213/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 214/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 215/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 216/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 217/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 218/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 219/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 220/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 221/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 222/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 223/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 224/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 225/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 226/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 227/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 228/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 229/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 230/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 231/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 232/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 233/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 234/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 235/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 236/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 237/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 238/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 239/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 240/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 241/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 242/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 243/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 244/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 245/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 246/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 247/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 248/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 249/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 250/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 251/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 252/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 253/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 254/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 255/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 256/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 257/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 258/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 259/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 260/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 261/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 262/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 263/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 264/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 265/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 266/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 267/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 268/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 269/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 270/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 271/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 272/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 273/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 274/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 275/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 276/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 277/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 278/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 279/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 280/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 281/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 282/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 283/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 284/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 285/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 286/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 287/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 288/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 289/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 290/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 291/300, seasonal_0 Loss: 0.0868 | 0.0543
Epoch 292/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 293/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 294/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 295/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 296/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 297/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 298/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 299/300, seasonal_0 Loss: 0.0868 | 0.0544
Epoch 300/300, seasonal_0 Loss: 0.0868 | 0.0544
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.7140109728209583, 'learning_rate': 0.00020335232478945983, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9030131574826116}
Epoch 1/300, seasonal_1 Loss: 0.9846 | 0.6245
Epoch 2/300, seasonal_1 Loss: 0.2883 | 0.3640
Epoch 3/300, seasonal_1 Loss: 0.2574 | 0.2237
Epoch 4/300, seasonal_1 Loss: 0.1766 | 0.1842
Epoch 5/300, seasonal_1 Loss: 0.1891 | 0.1885
Epoch 6/300, seasonal_1 Loss: 0.1696 | 0.1643
Epoch 7/300, seasonal_1 Loss: 0.1495 | 0.1525
Epoch 8/300, seasonal_1 Loss: 0.1501 | 0.1460
Epoch 9/300, seasonal_1 Loss: 0.1425 | 0.1413
Epoch 10/300, seasonal_1 Loss: 0.1375 | 0.1365
Epoch 11/300, seasonal_1 Loss: 0.1375 | 0.1313
Epoch 12/300, seasonal_1 Loss: 0.1334 | 0.1279
Epoch 13/300, seasonal_1 Loss: 0.1286 | 0.1282
Epoch 14/300, seasonal_1 Loss: 0.1274 | 0.1287
Epoch 15/300, seasonal_1 Loss: 0.1264 | 0.1229
Epoch 16/300, seasonal_1 Loss: 0.1231 | 0.1191
Epoch 17/300, seasonal_1 Loss: 0.1206 | 0.1165
Epoch 18/300, seasonal_1 Loss: 0.1201 | 0.1146
Epoch 19/300, seasonal_1 Loss: 0.1188 | 0.1117
Epoch 20/300, seasonal_1 Loss: 0.1162 | 0.1101
Epoch 21/300, seasonal_1 Loss: 0.1143 | 0.1098
Epoch 22/300, seasonal_1 Loss: 0.1137 | 0.1087
Epoch 23/300, seasonal_1 Loss: 0.1130 | 0.1074
Epoch 24/300, seasonal_1 Loss: 0.1117 | 0.1059
Epoch 25/300, seasonal_1 Loss: 0.1107 | 0.1050
Epoch 26/300, seasonal_1 Loss: 0.1101 | 0.1037
Epoch 27/300, seasonal_1 Loss: 0.1095 | 0.1027
Epoch 28/300, seasonal_1 Loss: 0.1085 | 0.1018
Epoch 29/300, seasonal_1 Loss: 0.1077 | 0.1011
Epoch 30/300, seasonal_1 Loss: 0.1072 | 0.1005
Epoch 31/300, seasonal_1 Loss: 0.1068 | 0.0999
Epoch 32/300, seasonal_1 Loss: 0.1063 | 0.0993
Epoch 33/300, seasonal_1 Loss: 0.1059 | 0.0986
Epoch 34/300, seasonal_1 Loss: 0.1055 | 0.0979
Epoch 35/300, seasonal_1 Loss: 0.1051 | 0.0973
Epoch 36/300, seasonal_1 Loss: 0.1047 | 0.0968
Epoch 37/300, seasonal_1 Loss: 0.1043 | 0.0963
Epoch 38/300, seasonal_1 Loss: 0.1040 | 0.0958
Epoch 39/300, seasonal_1 Loss: 0.1036 | 0.0954
Epoch 40/300, seasonal_1 Loss: 0.1033 | 0.0949
Epoch 41/300, seasonal_1 Loss: 0.1030 | 0.0945
Epoch 42/300, seasonal_1 Loss: 0.1027 | 0.0941
Epoch 43/300, seasonal_1 Loss: 0.1024 | 0.0937
Epoch 44/300, seasonal_1 Loss: 0.1021 | 0.0933
Epoch 45/300, seasonal_1 Loss: 0.1019 | 0.0930
Epoch 46/300, seasonal_1 Loss: 0.1016 | 0.0927
Epoch 47/300, seasonal_1 Loss: 0.1013 | 0.0924
Epoch 48/300, seasonal_1 Loss: 0.1011 | 0.0920
Epoch 49/300, seasonal_1 Loss: 0.1008 | 0.0918
Epoch 50/300, seasonal_1 Loss: 0.1005 | 0.0914
Epoch 51/300, seasonal_1 Loss: 0.1003 | 0.0912
Epoch 52/300, seasonal_1 Loss: 0.1000 | 0.0909
Epoch 53/300, seasonal_1 Loss: 0.0998 | 0.0906
Epoch 54/300, seasonal_1 Loss: 0.0995 | 0.0904
Epoch 55/300, seasonal_1 Loss: 0.0993 | 0.0901
Epoch 56/300, seasonal_1 Loss: 0.0990 | 0.0899
Epoch 57/300, seasonal_1 Loss: 0.0987 | 0.0896
Epoch 58/300, seasonal_1 Loss: 0.0985 | 0.0894
Epoch 59/300, seasonal_1 Loss: 0.0983 | 0.0892
Epoch 60/300, seasonal_1 Loss: 0.0980 | 0.0889
Epoch 61/300, seasonal_1 Loss: 0.0978 | 0.0887
Epoch 62/300, seasonal_1 Loss: 0.0976 | 0.0885
Epoch 63/300, seasonal_1 Loss: 0.0973 | 0.0883
Epoch 64/300, seasonal_1 Loss: 0.0971 | 0.0880
Epoch 65/300, seasonal_1 Loss: 0.0969 | 0.0879
Epoch 66/300, seasonal_1 Loss: 0.0967 | 0.0876
Epoch 67/300, seasonal_1 Loss: 0.0965 | 0.0874
Epoch 68/300, seasonal_1 Loss: 0.0963 | 0.0872
Epoch 69/300, seasonal_1 Loss: 0.0961 | 0.0870
Epoch 70/300, seasonal_1 Loss: 0.0959 | 0.0868
Epoch 71/300, seasonal_1 Loss: 0.0957 | 0.0867
Epoch 72/300, seasonal_1 Loss: 0.0955 | 0.0864
Epoch 73/300, seasonal_1 Loss: 0.0954 | 0.0863
Epoch 74/300, seasonal_1 Loss: 0.0952 | 0.0861
Epoch 75/300, seasonal_1 Loss: 0.0950 | 0.0859
Epoch 76/300, seasonal_1 Loss: 0.0949 | 0.0858
Epoch 77/300, seasonal_1 Loss: 0.0947 | 0.0856
Epoch 78/300, seasonal_1 Loss: 0.0946 | 0.0854
Epoch 79/300, seasonal_1 Loss: 0.0944 | 0.0853
Epoch 80/300, seasonal_1 Loss: 0.0943 | 0.0851
Epoch 81/300, seasonal_1 Loss: 0.0941 | 0.0850
Epoch 82/300, seasonal_1 Loss: 0.0940 | 0.0848
Epoch 83/300, seasonal_1 Loss: 0.0939 | 0.0847
Epoch 84/300, seasonal_1 Loss: 0.0938 | 0.0845
Epoch 85/300, seasonal_1 Loss: 0.0936 | 0.0844
Epoch 86/300, seasonal_1 Loss: 0.0935 | 0.0843
Epoch 87/300, seasonal_1 Loss: 0.0934 | 0.0841
Epoch 88/300, seasonal_1 Loss: 0.0933 | 0.0840
Epoch 89/300, seasonal_1 Loss: 0.0932 | 0.0839
Epoch 90/300, seasonal_1 Loss: 0.0931 | 0.0838
Epoch 91/300, seasonal_1 Loss: 0.0930 | 0.0836
Epoch 92/300, seasonal_1 Loss: 0.0929 | 0.0835
Epoch 93/300, seasonal_1 Loss: 0.0928 | 0.0834
Epoch 94/300, seasonal_1 Loss: 0.0927 | 0.0833
Epoch 95/300, seasonal_1 Loss: 0.0926 | 0.0832
Epoch 96/300, seasonal_1 Loss: 0.0925 | 0.0831
Epoch 97/300, seasonal_1 Loss: 0.0924 | 0.0830
Epoch 98/300, seasonal_1 Loss: 0.0923 | 0.0829
Epoch 99/300, seasonal_1 Loss: 0.0922 | 0.0828
Epoch 100/300, seasonal_1 Loss: 0.0922 | 0.0827
Epoch 101/300, seasonal_1 Loss: 0.0921 | 0.0826
Epoch 102/300, seasonal_1 Loss: 0.0920 | 0.0826
Epoch 103/300, seasonal_1 Loss: 0.0919 | 0.0825
Epoch 104/300, seasonal_1 Loss: 0.0918 | 0.0824
Epoch 105/300, seasonal_1 Loss: 0.0918 | 0.0823
Epoch 106/300, seasonal_1 Loss: 0.0917 | 0.0822
Epoch 107/300, seasonal_1 Loss: 0.0916 | 0.0822
Epoch 108/300, seasonal_1 Loss: 0.0916 | 0.0821
Epoch 109/300, seasonal_1 Loss: 0.0915 | 0.0820
Epoch 110/300, seasonal_1 Loss: 0.0914 | 0.0820
Epoch 111/300, seasonal_1 Loss: 0.0914 | 0.0819
Epoch 112/300, seasonal_1 Loss: 0.0913 | 0.0818
Epoch 113/300, seasonal_1 Loss: 0.0913 | 0.0818
Epoch 114/300, seasonal_1 Loss: 0.0912 | 0.0817
Epoch 115/300, seasonal_1 Loss: 0.0912 | 0.0817
Epoch 116/300, seasonal_1 Loss: 0.0911 | 0.0816
Epoch 117/300, seasonal_1 Loss: 0.0911 | 0.0815
Epoch 118/300, seasonal_1 Loss: 0.0910 | 0.0815
Epoch 119/300, seasonal_1 Loss: 0.0910 | 0.0814
Epoch 120/300, seasonal_1 Loss: 0.0909 | 0.0814
Epoch 121/300, seasonal_1 Loss: 0.0909 | 0.0813
Epoch 122/300, seasonal_1 Loss: 0.0908 | 0.0813
Epoch 123/300, seasonal_1 Loss: 0.0908 | 0.0813
Epoch 124/300, seasonal_1 Loss: 0.0907 | 0.0812
Epoch 125/300, seasonal_1 Loss: 0.0907 | 0.0812
Epoch 126/300, seasonal_1 Loss: 0.0907 | 0.0811
Epoch 127/300, seasonal_1 Loss: 0.0906 | 0.0811
Epoch 128/300, seasonal_1 Loss: 0.0906 | 0.0811
Epoch 129/300, seasonal_1 Loss: 0.0905 | 0.0810
Epoch 130/300, seasonal_1 Loss: 0.0905 | 0.0810
Epoch 131/300, seasonal_1 Loss: 0.0905 | 0.0809
Epoch 132/300, seasonal_1 Loss: 0.0904 | 0.0809
Epoch 133/300, seasonal_1 Loss: 0.0904 | 0.0809
Epoch 134/300, seasonal_1 Loss: 0.0904 | 0.0809
Epoch 135/300, seasonal_1 Loss: 0.0903 | 0.0808
Epoch 136/300, seasonal_1 Loss: 0.0903 | 0.0808
Epoch 137/300, seasonal_1 Loss: 0.0903 | 0.0808
Epoch 138/300, seasonal_1 Loss: 0.0903 | 0.0807
Epoch 139/300, seasonal_1 Loss: 0.0902 | 0.0807
Epoch 140/300, seasonal_1 Loss: 0.0902 | 0.0807
Epoch 141/300, seasonal_1 Loss: 0.0902 | 0.0807
Epoch 142/300, seasonal_1 Loss: 0.0902 | 0.0806
Epoch 143/300, seasonal_1 Loss: 0.0901 | 0.0806
Epoch 144/300, seasonal_1 Loss: 0.0901 | 0.0806
Epoch 145/300, seasonal_1 Loss: 0.0901 | 0.0806
Epoch 146/300, seasonal_1 Loss: 0.0901 | 0.0806
Epoch 147/300, seasonal_1 Loss: 0.0900 | 0.0805
Epoch 148/300, seasonal_1 Loss: 0.0900 | 0.0805
Epoch 149/300, seasonal_1 Loss: 0.0900 | 0.0805
Epoch 150/300, seasonal_1 Loss: 0.0900 | 0.0805
Epoch 151/300, seasonal_1 Loss: 0.0900 | 0.0805
Epoch 152/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 153/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 154/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 155/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 156/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 157/300, seasonal_1 Loss: 0.0899 | 0.0804
Epoch 158/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 159/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 160/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 161/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 162/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 163/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 164/300, seasonal_1 Loss: 0.0898 | 0.0803
Epoch 165/300, seasonal_1 Loss: 0.0897 | 0.0803
Epoch 166/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 167/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 168/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 169/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 170/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 171/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 172/300, seasonal_1 Loss: 0.0897 | 0.0802
Epoch 173/300, seasonal_1 Loss: 0.0896 | 0.0802
Epoch 174/300, seasonal_1 Loss: 0.0896 | 0.0802
Epoch 175/300, seasonal_1 Loss: 0.0896 | 0.0802
Epoch 176/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 177/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 178/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 179/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 180/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 181/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 182/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 183/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 184/300, seasonal_1 Loss: 0.0896 | 0.0801
Epoch 185/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 186/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 187/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 188/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 189/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 190/300, seasonal_1 Loss: 0.0895 | 0.0801
Epoch 191/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 192/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 193/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 194/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 195/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 196/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 197/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 198/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 199/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 200/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 201/300, seasonal_1 Loss: 0.0895 | 0.0800
Epoch 202/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 203/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 204/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 205/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 206/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 207/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 208/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 209/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 210/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 211/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 212/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 213/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 214/300, seasonal_1 Loss: 0.0894 | 0.0800
Epoch 215/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 216/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 217/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 218/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 219/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 220/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 221/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 222/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 223/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 224/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 225/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 226/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 227/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 228/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 229/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 230/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 231/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 232/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 233/300, seasonal_1 Loss: 0.0894 | 0.0799
Epoch 234/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 235/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 236/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 237/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 238/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 239/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 240/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 241/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 242/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 243/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 244/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 245/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 246/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 247/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 248/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 249/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 250/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 251/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 252/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 253/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 254/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 255/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 256/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 257/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 258/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 259/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 260/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 261/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 262/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 263/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 264/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 265/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 266/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 267/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 268/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 269/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 270/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 271/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 272/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 273/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 274/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 275/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 276/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 277/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 278/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 279/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 280/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 281/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 282/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 283/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 284/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 285/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 286/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 287/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 288/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 289/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 290/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 291/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 292/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 293/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 294/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 295/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 296/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 297/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 298/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 299/300, seasonal_1 Loss: 0.0893 | 0.0799
Epoch 300/300, seasonal_1 Loss: 0.0893 | 0.0799
Training seasonal_2 component with params: {'observation_period_num': 52, 'train_rates': 0.9889648789937507, 'learning_rate': 2.4810551694002915e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.9302092778840485}
Epoch 1/300, seasonal_2 Loss: 0.3330 | 0.2316
Epoch 2/300, seasonal_2 Loss: 0.2305 | 0.1706
Epoch 3/300, seasonal_2 Loss: 0.1933 | 0.1367
Epoch 4/300, seasonal_2 Loss: 0.1708 | 0.1182
Epoch 5/300, seasonal_2 Loss: 0.1562 | 0.1075
Epoch 6/300, seasonal_2 Loss: 0.1458 | 0.0988
Epoch 7/300, seasonal_2 Loss: 0.1388 | 0.0927
Epoch 8/300, seasonal_2 Loss: 0.1334 | 0.0877
Epoch 9/300, seasonal_2 Loss: 0.1291 | 0.0834
Epoch 10/300, seasonal_2 Loss: 0.1255 | 0.0808
Epoch 11/300, seasonal_2 Loss: 0.1224 | 0.0770
Epoch 12/300, seasonal_2 Loss: 0.1198 | 0.0752
Epoch 13/300, seasonal_2 Loss: 0.1175 | 0.0734
Epoch 14/300, seasonal_2 Loss: 0.1154 | 0.0696
Epoch 15/300, seasonal_2 Loss: 0.1135 | 0.0682
Epoch 16/300, seasonal_2 Loss: 0.1118 | 0.0666
Epoch 17/300, seasonal_2 Loss: 0.1102 | 0.0658
Epoch 18/300, seasonal_2 Loss: 0.1087 | 0.0651
Epoch 19/300, seasonal_2 Loss: 0.1073 | 0.0655
Epoch 20/300, seasonal_2 Loss: 0.1060 | 0.0652
Epoch 21/300, seasonal_2 Loss: 0.1048 | 0.0655
Epoch 22/300, seasonal_2 Loss: 0.1038 | 0.0652
Epoch 23/300, seasonal_2 Loss: 0.1028 | 0.0650
Epoch 24/300, seasonal_2 Loss: 0.1019 | 0.0643
Epoch 25/300, seasonal_2 Loss: 0.1011 | 0.0640
Epoch 26/300, seasonal_2 Loss: 0.1003 | 0.0627
Epoch 27/300, seasonal_2 Loss: 0.0996 | 0.0624
Epoch 28/300, seasonal_2 Loss: 0.0988 | 0.0621
Epoch 29/300, seasonal_2 Loss: 0.0981 | 0.0612
Epoch 30/300, seasonal_2 Loss: 0.0974 | 0.0609
Epoch 31/300, seasonal_2 Loss: 0.0967 | 0.0605
Epoch 32/300, seasonal_2 Loss: 0.0961 | 0.0603
Epoch 33/300, seasonal_2 Loss: 0.0954 | 0.0600
Epoch 34/300, seasonal_2 Loss: 0.0948 | 0.0600
Epoch 35/300, seasonal_2 Loss: 0.0942 | 0.0598
Epoch 36/300, seasonal_2 Loss: 0.0936 | 0.0599
Epoch 37/300, seasonal_2 Loss: 0.0931 | 0.0598
Epoch 38/300, seasonal_2 Loss: 0.0925 | 0.0596
Epoch 39/300, seasonal_2 Loss: 0.0920 | 0.0597
Epoch 40/300, seasonal_2 Loss: 0.0915 | 0.0596
Epoch 41/300, seasonal_2 Loss: 0.0910 | 0.0597
Epoch 42/300, seasonal_2 Loss: 0.0906 | 0.0596
Epoch 43/300, seasonal_2 Loss: 0.0902 | 0.0595
Epoch 44/300, seasonal_2 Loss: 0.0898 | 0.0595
Epoch 45/300, seasonal_2 Loss: 0.0894 | 0.0594
Epoch 46/300, seasonal_2 Loss: 0.0890 | 0.0593
Epoch 47/300, seasonal_2 Loss: 0.0886 | 0.0593
Epoch 48/300, seasonal_2 Loss: 0.0883 | 0.0592
Epoch 49/300, seasonal_2 Loss: 0.0880 | 0.0590
Epoch 50/300, seasonal_2 Loss: 0.0877 | 0.0589
Epoch 51/300, seasonal_2 Loss: 0.0873 | 0.0587
Epoch 52/300, seasonal_2 Loss: 0.0871 | 0.0586
Epoch 53/300, seasonal_2 Loss: 0.0868 | 0.0586
Epoch 54/300, seasonal_2 Loss: 0.0865 | 0.0583
Epoch 55/300, seasonal_2 Loss: 0.0863 | 0.0582
Epoch 56/300, seasonal_2 Loss: 0.0860 | 0.0580
Epoch 57/300, seasonal_2 Loss: 0.0858 | 0.0579
Epoch 58/300, seasonal_2 Loss: 0.0856 | 0.0579
Epoch 59/300, seasonal_2 Loss: 0.0853 | 0.0576
Epoch 60/300, seasonal_2 Loss: 0.0851 | 0.0576
Epoch 61/300, seasonal_2 Loss: 0.0849 | 0.0573
Epoch 62/300, seasonal_2 Loss: 0.0847 | 0.0573
Epoch 63/300, seasonal_2 Loss: 0.0845 | 0.0572
Epoch 64/300, seasonal_2 Loss: 0.0843 | 0.0570
Epoch 65/300, seasonal_2 Loss: 0.0842 | 0.0570
Epoch 66/300, seasonal_2 Loss: 0.0840 | 0.0568
Epoch 67/300, seasonal_2 Loss: 0.0838 | 0.0567
Epoch 68/300, seasonal_2 Loss: 0.0837 | 0.0567
Epoch 69/300, seasonal_2 Loss: 0.0835 | 0.0566
Epoch 70/300, seasonal_2 Loss: 0.0834 | 0.0565
Epoch 71/300, seasonal_2 Loss: 0.0832 | 0.0564
Epoch 72/300, seasonal_2 Loss: 0.0831 | 0.0563
Epoch 73/300, seasonal_2 Loss: 0.0830 | 0.0563
Epoch 74/300, seasonal_2 Loss: 0.0828 | 0.0562
Epoch 75/300, seasonal_2 Loss: 0.0827 | 0.0561
Epoch 76/300, seasonal_2 Loss: 0.0826 | 0.0560
Epoch 77/300, seasonal_2 Loss: 0.0825 | 0.0560
Epoch 78/300, seasonal_2 Loss: 0.0824 | 0.0559
Epoch 79/300, seasonal_2 Loss: 0.0822 | 0.0558
Epoch 80/300, seasonal_2 Loss: 0.0821 | 0.0558
Epoch 81/300, seasonal_2 Loss: 0.0820 | 0.0557
Epoch 82/300, seasonal_2 Loss: 0.0819 | 0.0557
Epoch 83/300, seasonal_2 Loss: 0.0818 | 0.0557
Epoch 84/300, seasonal_2 Loss: 0.0817 | 0.0556
Epoch 85/300, seasonal_2 Loss: 0.0817 | 0.0555
Epoch 86/300, seasonal_2 Loss: 0.0816 | 0.0555
Epoch 87/300, seasonal_2 Loss: 0.0815 | 0.0554
Epoch 88/300, seasonal_2 Loss: 0.0814 | 0.0554
Epoch 89/300, seasonal_2 Loss: 0.0813 | 0.0553
Epoch 90/300, seasonal_2 Loss: 0.0812 | 0.0553
Epoch 91/300, seasonal_2 Loss: 0.0812 | 0.0553
Epoch 92/300, seasonal_2 Loss: 0.0811 | 0.0552
Epoch 93/300, seasonal_2 Loss: 0.0810 | 0.0552
Epoch 94/300, seasonal_2 Loss: 0.0810 | 0.0552
Epoch 95/300, seasonal_2 Loss: 0.0809 | 0.0551
Epoch 96/300, seasonal_2 Loss: 0.0808 | 0.0551
Epoch 97/300, seasonal_2 Loss: 0.0808 | 0.0551
Epoch 98/300, seasonal_2 Loss: 0.0807 | 0.0550
Epoch 99/300, seasonal_2 Loss: 0.0806 | 0.0550
Epoch 100/300, seasonal_2 Loss: 0.0806 | 0.0550
Epoch 101/300, seasonal_2 Loss: 0.0805 | 0.0550
Epoch 102/300, seasonal_2 Loss: 0.0805 | 0.0549
Epoch 103/300, seasonal_2 Loss: 0.0804 | 0.0549
Epoch 104/300, seasonal_2 Loss: 0.0804 | 0.0549
Epoch 105/300, seasonal_2 Loss: 0.0803 | 0.0549
Epoch 106/300, seasonal_2 Loss: 0.0803 | 0.0549
Epoch 107/300, seasonal_2 Loss: 0.0802 | 0.0549
Epoch 108/300, seasonal_2 Loss: 0.0802 | 0.0548
Epoch 109/300, seasonal_2 Loss: 0.0801 | 0.0548
Epoch 110/300, seasonal_2 Loss: 0.0801 | 0.0548
Epoch 111/300, seasonal_2 Loss: 0.0801 | 0.0548
Epoch 112/300, seasonal_2 Loss: 0.0800 | 0.0548
Epoch 113/300, seasonal_2 Loss: 0.0800 | 0.0548
Epoch 114/300, seasonal_2 Loss: 0.0800 | 0.0547
Epoch 115/300, seasonal_2 Loss: 0.0799 | 0.0547
Epoch 116/300, seasonal_2 Loss: 0.0799 | 0.0547
Epoch 117/300, seasonal_2 Loss: 0.0798 | 0.0547
Epoch 118/300, seasonal_2 Loss: 0.0798 | 0.0546
Epoch 119/300, seasonal_2 Loss: 0.0798 | 0.0546
Epoch 120/300, seasonal_2 Loss: 0.0798 | 0.0546
Epoch 121/300, seasonal_2 Loss: 0.0797 | 0.0545
Epoch 122/300, seasonal_2 Loss: 0.0797 | 0.0545
Epoch 123/300, seasonal_2 Loss: 0.0797 | 0.0545
Epoch 124/300, seasonal_2 Loss: 0.0796 | 0.0544
Epoch 125/300, seasonal_2 Loss: 0.0796 | 0.0544
Epoch 126/300, seasonal_2 Loss: 0.0796 | 0.0544
Epoch 127/300, seasonal_2 Loss: 0.0795 | 0.0544
Epoch 128/300, seasonal_2 Loss: 0.0795 | 0.0544
Epoch 129/300, seasonal_2 Loss: 0.0795 | 0.0544
Epoch 130/300, seasonal_2 Loss: 0.0795 | 0.0544
Epoch 131/300, seasonal_2 Loss: 0.0794 | 0.0544
Epoch 132/300, seasonal_2 Loss: 0.0794 | 0.0543
Epoch 133/300, seasonal_2 Loss: 0.0794 | 0.0543
Epoch 134/300, seasonal_2 Loss: 0.0794 | 0.0543
Epoch 135/300, seasonal_2 Loss: 0.0793 | 0.0543
Epoch 136/300, seasonal_2 Loss: 0.0793 | 0.0543
Epoch 137/300, seasonal_2 Loss: 0.0793 | 0.0543
Epoch 138/300, seasonal_2 Loss: 0.0793 | 0.0543
Epoch 139/300, seasonal_2 Loss: 0.0792 | 0.0543
Epoch 140/300, seasonal_2 Loss: 0.0792 | 0.0543
Epoch 141/300, seasonal_2 Loss: 0.0792 | 0.0543
Epoch 142/300, seasonal_2 Loss: 0.0792 | 0.0543
Epoch 143/300, seasonal_2 Loss: 0.0792 | 0.0542
Epoch 144/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 145/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 146/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 147/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 148/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 149/300, seasonal_2 Loss: 0.0791 | 0.0542
Epoch 150/300, seasonal_2 Loss: 0.0790 | 0.0542
Epoch 151/300, seasonal_2 Loss: 0.0790 | 0.0542
Epoch 152/300, seasonal_2 Loss: 0.0790 | 0.0542
Epoch 153/300, seasonal_2 Loss: 0.0790 | 0.0542
Epoch 154/300, seasonal_2 Loss: 0.0790 | 0.0542
Epoch 155/300, seasonal_2 Loss: 0.0790 | 0.0541
Epoch 156/300, seasonal_2 Loss: 0.0790 | 0.0541
Epoch 157/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 158/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 159/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 160/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 161/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 162/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 163/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 164/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 165/300, seasonal_2 Loss: 0.0789 | 0.0541
Epoch 166/300, seasonal_2 Loss: 0.0788 | 0.0541
Epoch 167/300, seasonal_2 Loss: 0.0788 | 0.0541
Epoch 168/300, seasonal_2 Loss: 0.0788 | 0.0541
Epoch 169/300, seasonal_2 Loss: 0.0788 | 0.0541
Epoch 170/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 171/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 172/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 173/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 174/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 175/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 176/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 177/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 178/300, seasonal_2 Loss: 0.0788 | 0.0540
Epoch 179/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 180/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 181/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 182/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 183/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 184/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 185/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 186/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 187/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 188/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 189/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 190/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 191/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 192/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 193/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 194/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 195/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 196/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 197/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 198/300, seasonal_2 Loss: 0.0787 | 0.0540
Epoch 199/300, seasonal_2 Loss: 0.0786 | 0.0540
Epoch 200/300, seasonal_2 Loss: 0.0786 | 0.0540
Epoch 201/300, seasonal_2 Loss: 0.0786 | 0.0540
Epoch 202/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 203/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 204/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 205/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 206/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 207/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 208/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 209/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 210/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 211/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 212/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 213/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 214/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 215/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 216/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 217/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 218/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 219/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 220/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 221/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 222/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 223/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 224/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 225/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 226/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 227/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 228/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 229/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 230/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 231/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 232/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 233/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 234/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 235/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 236/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 237/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 238/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 239/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 240/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 241/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 242/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 243/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 244/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 245/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 246/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 247/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 248/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 249/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 250/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 251/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 252/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 253/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 254/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 255/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 256/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 257/300, seasonal_2 Loss: 0.0786 | 0.0539
Epoch 258/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 259/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 260/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 261/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 262/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 263/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 264/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 265/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 266/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 267/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 268/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 269/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 270/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 271/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 272/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 273/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 274/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 275/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 276/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 277/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 278/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 279/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 280/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 281/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 282/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 283/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 284/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 285/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 286/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 287/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 288/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 289/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 290/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 291/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 292/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 293/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 294/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 295/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 296/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 297/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 298/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 299/300, seasonal_2 Loss: 0.0785 | 0.0539
Epoch 300/300, seasonal_2 Loss: 0.0785 | 0.0539
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.7288715274148763, 'learning_rate': 5.9799780652114716e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8780685769136342}
Epoch 1/300, seasonal_3 Loss: 0.3672 | 0.2630
Epoch 2/300, seasonal_3 Loss: 0.1887 | 0.1681
Epoch 3/300, seasonal_3 Loss: 0.1478 | 0.1456
Epoch 4/300, seasonal_3 Loss: 0.1393 | 0.1358
Epoch 5/300, seasonal_3 Loss: 0.1331 | 0.1290
Epoch 6/300, seasonal_3 Loss: 0.1281 | 0.1246
Epoch 7/300, seasonal_3 Loss: 0.1242 | 0.1217
Epoch 8/300, seasonal_3 Loss: 0.1214 | 0.1197
Epoch 9/300, seasonal_3 Loss: 0.1200 | 0.1184
Epoch 10/300, seasonal_3 Loss: 0.1194 | 0.1173
Epoch 11/300, seasonal_3 Loss: 0.1191 | 0.1159
Epoch 12/300, seasonal_3 Loss: 0.1186 | 0.1141
Epoch 13/300, seasonal_3 Loss: 0.1175 | 0.1119
Epoch 14/300, seasonal_3 Loss: 0.1160 | 0.1097
Epoch 15/300, seasonal_3 Loss: 0.1143 | 0.1066
Epoch 16/300, seasonal_3 Loss: 0.1126 | 0.1046
Epoch 17/300, seasonal_3 Loss: 0.1104 | 0.1033
Epoch 18/300, seasonal_3 Loss: 0.1088 | 0.1024
Epoch 19/300, seasonal_3 Loss: 0.1075 | 0.1016
Epoch 20/300, seasonal_3 Loss: 0.1064 | 0.1009
Epoch 21/300, seasonal_3 Loss: 0.1054 | 0.1003
Epoch 22/300, seasonal_3 Loss: 0.1045 | 0.0991
Epoch 23/300, seasonal_3 Loss: 0.1038 | 0.0989
Epoch 24/300, seasonal_3 Loss: 0.1028 | 0.0984
Epoch 25/300, seasonal_3 Loss: 0.1020 | 0.0977
Epoch 26/300, seasonal_3 Loss: 0.1012 | 0.0969
Epoch 27/300, seasonal_3 Loss: 0.1004 | 0.0961
Epoch 28/300, seasonal_3 Loss: 0.0995 | 0.0952
Epoch 29/300, seasonal_3 Loss: 0.0983 | 0.0939
Epoch 30/300, seasonal_3 Loss: 0.0977 | 0.0936
Epoch 31/300, seasonal_3 Loss: 0.0968 | 0.0929
Epoch 32/300, seasonal_3 Loss: 0.0960 | 0.0921
Epoch 33/300, seasonal_3 Loss: 0.0954 | 0.0913
Epoch 34/300, seasonal_3 Loss: 0.0948 | 0.0904
Epoch 35/300, seasonal_3 Loss: 0.0942 | 0.0896
Epoch 36/300, seasonal_3 Loss: 0.0935 | 0.0893
Epoch 37/300, seasonal_3 Loss: 0.0931 | 0.0887
Epoch 38/300, seasonal_3 Loss: 0.0925 | 0.0879
Epoch 39/300, seasonal_3 Loss: 0.0918 | 0.0871
Epoch 40/300, seasonal_3 Loss: 0.0912 | 0.0864
Epoch 41/300, seasonal_3 Loss: 0.0907 | 0.0857
Epoch 42/300, seasonal_3 Loss: 0.0902 | 0.0851
Epoch 43/300, seasonal_3 Loss: 0.0896 | 0.0848
Epoch 44/300, seasonal_3 Loss: 0.0892 | 0.0843
Epoch 45/300, seasonal_3 Loss: 0.0888 | 0.0838
Epoch 46/300, seasonal_3 Loss: 0.0884 | 0.0833
Epoch 47/300, seasonal_3 Loss: 0.0880 | 0.0830
Epoch 48/300, seasonal_3 Loss: 0.0877 | 0.0826
Epoch 49/300, seasonal_3 Loss: 0.0874 | 0.0823
Epoch 50/300, seasonal_3 Loss: 0.0870 | 0.0822
Epoch 51/300, seasonal_3 Loss: 0.0867 | 0.0819
Epoch 52/300, seasonal_3 Loss: 0.0863 | 0.0816
Epoch 53/300, seasonal_3 Loss: 0.0860 | 0.0813
Epoch 54/300, seasonal_3 Loss: 0.0858 | 0.0811
Epoch 55/300, seasonal_3 Loss: 0.0855 | 0.0809
Epoch 56/300, seasonal_3 Loss: 0.0853 | 0.0807
Epoch 57/300, seasonal_3 Loss: 0.0849 | 0.0806
Epoch 58/300, seasonal_3 Loss: 0.0847 | 0.0803
Epoch 59/300, seasonal_3 Loss: 0.0844 | 0.0802
Epoch 60/300, seasonal_3 Loss: 0.0842 | 0.0800
Epoch 61/300, seasonal_3 Loss: 0.0840 | 0.0798
Epoch 62/300, seasonal_3 Loss: 0.0838 | 0.0796
Epoch 63/300, seasonal_3 Loss: 0.0836 | 0.0795
Epoch 64/300, seasonal_3 Loss: 0.0833 | 0.0794
Epoch 65/300, seasonal_3 Loss: 0.0831 | 0.0792
Epoch 66/300, seasonal_3 Loss: 0.0829 | 0.0790
Epoch 67/300, seasonal_3 Loss: 0.0828 | 0.0789
Epoch 68/300, seasonal_3 Loss: 0.0826 | 0.0787
Epoch 69/300, seasonal_3 Loss: 0.0824 | 0.0786
Epoch 70/300, seasonal_3 Loss: 0.0823 | 0.0784
Epoch 71/300, seasonal_3 Loss: 0.0821 | 0.0782
Epoch 72/300, seasonal_3 Loss: 0.0819 | 0.0781
Epoch 73/300, seasonal_3 Loss: 0.0818 | 0.0779
Epoch 74/300, seasonal_3 Loss: 0.0816 | 0.0778
Epoch 75/300, seasonal_3 Loss: 0.0815 | 0.0777
Epoch 76/300, seasonal_3 Loss: 0.0813 | 0.0775
Epoch 77/300, seasonal_3 Loss: 0.0812 | 0.0774
Epoch 78/300, seasonal_3 Loss: 0.0810 | 0.0772
Epoch 79/300, seasonal_3 Loss: 0.0809 | 0.0771
Epoch 80/300, seasonal_3 Loss: 0.0807 | 0.0770
Epoch 81/300, seasonal_3 Loss: 0.0806 | 0.0768
Epoch 82/300, seasonal_3 Loss: 0.0805 | 0.0767
Epoch 83/300, seasonal_3 Loss: 0.0803 | 0.0766
Epoch 84/300, seasonal_3 Loss: 0.0802 | 0.0764
Epoch 85/300, seasonal_3 Loss: 0.0801 | 0.0763
Epoch 86/300, seasonal_3 Loss: 0.0799 | 0.0762
Epoch 87/300, seasonal_3 Loss: 0.0798 | 0.0760
Epoch 88/300, seasonal_3 Loss: 0.0797 | 0.0759
Epoch 89/300, seasonal_3 Loss: 0.0795 | 0.0758
Epoch 90/300, seasonal_3 Loss: 0.0794 | 0.0756
Epoch 91/300, seasonal_3 Loss: 0.0793 | 0.0755
Epoch 92/300, seasonal_3 Loss: 0.0792 | 0.0754
Epoch 93/300, seasonal_3 Loss: 0.0790 | 0.0753
Epoch 94/300, seasonal_3 Loss: 0.0790 | 0.0752
Epoch 95/300, seasonal_3 Loss: 0.0788 | 0.0750
Epoch 96/300, seasonal_3 Loss: 0.0787 | 0.0749
Epoch 97/300, seasonal_3 Loss: 0.0786 | 0.0748
Epoch 98/300, seasonal_3 Loss: 0.0785 | 0.0747
Epoch 99/300, seasonal_3 Loss: 0.0784 | 0.0746
Epoch 100/300, seasonal_3 Loss: 0.0783 | 0.0745
Epoch 101/300, seasonal_3 Loss: 0.0783 | 0.0744
Epoch 102/300, seasonal_3 Loss: 0.0782 | 0.0743
Epoch 103/300, seasonal_3 Loss: 0.0781 | 0.0743
Epoch 104/300, seasonal_3 Loss: 0.0780 | 0.0742
Epoch 105/300, seasonal_3 Loss: 0.0779 | 0.0741
Epoch 106/300, seasonal_3 Loss: 0.0779 | 0.0740
Epoch 107/300, seasonal_3 Loss: 0.0778 | 0.0740
Epoch 108/300, seasonal_3 Loss: 0.0777 | 0.0739
Epoch 109/300, seasonal_3 Loss: 0.0776 | 0.0739
Epoch 110/300, seasonal_3 Loss: 0.0775 | 0.0738
Epoch 111/300, seasonal_3 Loss: 0.0774 | 0.0738
Epoch 112/300, seasonal_3 Loss: 0.0773 | 0.0738
Epoch 113/300, seasonal_3 Loss: 0.0772 | 0.0737
Epoch 114/300, seasonal_3 Loss: 0.0772 | 0.0737
Epoch 115/300, seasonal_3 Loss: 0.0771 | 0.0737
Epoch 116/300, seasonal_3 Loss: 0.0771 | 0.0737
Epoch 117/300, seasonal_3 Loss: 0.0770 | 0.0736
Epoch 118/300, seasonal_3 Loss: 0.0769 | 0.0736
Epoch 119/300, seasonal_3 Loss: 0.0768 | 0.0735
Epoch 120/300, seasonal_3 Loss: 0.0768 | 0.0735
Epoch 121/300, seasonal_3 Loss: 0.0768 | 0.0735
Epoch 122/300, seasonal_3 Loss: 0.0767 | 0.0734
Epoch 123/300, seasonal_3 Loss: 0.0766 | 0.0734
Epoch 124/300, seasonal_3 Loss: 0.0766 | 0.0734
Epoch 125/300, seasonal_3 Loss: 0.0765 | 0.0733
Epoch 126/300, seasonal_3 Loss: 0.0765 | 0.0733
Epoch 127/300, seasonal_3 Loss: 0.0764 | 0.0733
Epoch 128/300, seasonal_3 Loss: 0.0764 | 0.0733
Epoch 129/300, seasonal_3 Loss: 0.0764 | 0.0733
Epoch 130/300, seasonal_3 Loss: 0.0763 | 0.0732
Epoch 131/300, seasonal_3 Loss: 0.0763 | 0.0732
Epoch 132/300, seasonal_3 Loss: 0.0762 | 0.0732
Epoch 133/300, seasonal_3 Loss: 0.0762 | 0.0731
Epoch 134/300, seasonal_3 Loss: 0.0761 | 0.0732
Epoch 135/300, seasonal_3 Loss: 0.0761 | 0.0731
Epoch 136/300, seasonal_3 Loss: 0.0761 | 0.0731
Epoch 137/300, seasonal_3 Loss: 0.0760 | 0.0731
Epoch 138/300, seasonal_3 Loss: 0.0760 | 0.0730
Epoch 139/300, seasonal_3 Loss: 0.0759 | 0.0730
Epoch 140/300, seasonal_3 Loss: 0.0759 | 0.0729
Epoch 141/300, seasonal_3 Loss: 0.0758 | 0.0730
Epoch 142/300, seasonal_3 Loss: 0.0758 | 0.0729
Epoch 143/300, seasonal_3 Loss: 0.0758 | 0.0729
Epoch 144/300, seasonal_3 Loss: 0.0757 | 0.0728
Epoch 145/300, seasonal_3 Loss: 0.0757 | 0.0728
Epoch 146/300, seasonal_3 Loss: 0.0756 | 0.0727
Epoch 147/300, seasonal_3 Loss: 0.0756 | 0.0727
Epoch 148/300, seasonal_3 Loss: 0.0756 | 0.0726
Epoch 149/300, seasonal_3 Loss: 0.0755 | 0.0726
Epoch 150/300, seasonal_3 Loss: 0.0755 | 0.0725
Epoch 151/300, seasonal_3 Loss: 0.0755 | 0.0725
Epoch 152/300, seasonal_3 Loss: 0.0755 | 0.0725
Epoch 153/300, seasonal_3 Loss: 0.0754 | 0.0724
Epoch 154/300, seasonal_3 Loss: 0.0754 | 0.0724
Epoch 155/300, seasonal_3 Loss: 0.0754 | 0.0724
Epoch 156/300, seasonal_3 Loss: 0.0754 | 0.0723
Epoch 157/300, seasonal_3 Loss: 0.0753 | 0.0723
Epoch 158/300, seasonal_3 Loss: 0.0753 | 0.0723
Epoch 159/300, seasonal_3 Loss: 0.0752 | 0.0722
Epoch 160/300, seasonal_3 Loss: 0.0752 | 0.0722
Epoch 161/300, seasonal_3 Loss: 0.0752 | 0.0722
Epoch 162/300, seasonal_3 Loss: 0.0751 | 0.0722
Epoch 163/300, seasonal_3 Loss: 0.0751 | 0.0721
Epoch 164/300, seasonal_3 Loss: 0.0751 | 0.0721
Epoch 165/300, seasonal_3 Loss: 0.0751 | 0.0721
Epoch 166/300, seasonal_3 Loss: 0.0750 | 0.0721
Epoch 167/300, seasonal_3 Loss: 0.0750 | 0.0720
Epoch 168/300, seasonal_3 Loss: 0.0750 | 0.0720
Epoch 169/300, seasonal_3 Loss: 0.0750 | 0.0720
Epoch 170/300, seasonal_3 Loss: 0.0750 | 0.0720
Epoch 171/300, seasonal_3 Loss: 0.0749 | 0.0720
Epoch 172/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 173/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 174/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 175/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 176/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 177/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 178/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 179/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 180/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 181/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 182/300, seasonal_3 Loss: 0.0748 | 0.0718
Epoch 183/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 184/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 185/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 186/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 187/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 188/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 189/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 190/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 191/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 192/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 193/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 194/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 195/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 196/300, seasonal_3 Loss: 0.0749 | 0.0718
Epoch 197/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 198/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 199/300, seasonal_3 Loss: 0.0749 | 0.0719
Epoch 200/300, seasonal_3 Loss: 0.0748 | 0.0719
Epoch 201/300, seasonal_3 Loss: 0.0748 | 0.0718
Epoch 202/300, seasonal_3 Loss: 0.0747 | 0.0718
Epoch 203/300, seasonal_3 Loss: 0.0747 | 0.0718
Epoch 204/300, seasonal_3 Loss: 0.0746 | 0.0718
Epoch 205/300, seasonal_3 Loss: 0.0746 | 0.0718
Epoch 206/300, seasonal_3 Loss: 0.0746 | 0.0718
Epoch 207/300, seasonal_3 Loss: 0.0745 | 0.0718
Epoch 208/300, seasonal_3 Loss: 0.0745 | 0.0718
Epoch 209/300, seasonal_3 Loss: 0.0744 | 0.0718
Epoch 210/300, seasonal_3 Loss: 0.0744 | 0.0718
Epoch 211/300, seasonal_3 Loss: 0.0744 | 0.0718
Epoch 212/300, seasonal_3 Loss: 0.0744 | 0.0718
Epoch 213/300, seasonal_3 Loss: 0.0744 | 0.0717
Epoch 214/300, seasonal_3 Loss: 0.0744 | 0.0717
Epoch 215/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 216/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 217/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 218/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 219/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 220/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 221/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 222/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 223/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 224/300, seasonal_3 Loss: 0.0743 | 0.0717
Epoch 225/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 226/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 227/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 228/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 229/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 230/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 231/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 232/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 233/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 234/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 235/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 236/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 237/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 238/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 239/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 240/300, seasonal_3 Loss: 0.0742 | 0.0716
Epoch 241/300, seasonal_3 Loss: 0.0741 | 0.0716
Epoch 242/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 243/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 244/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 245/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 246/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 247/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 248/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 249/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 250/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 251/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 252/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 253/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 254/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 255/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 256/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 257/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 258/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 259/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 260/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 261/300, seasonal_3 Loss: 0.0741 | 0.0715
Epoch 262/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 263/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 264/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 265/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 266/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 267/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 268/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 269/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 270/300, seasonal_3 Loss: 0.0740 | 0.0715
Epoch 271/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 272/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 273/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 274/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 275/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 276/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 277/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 278/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 279/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 280/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 281/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 282/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 283/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 284/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 285/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 286/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 287/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 288/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 289/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 290/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 291/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 292/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 293/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 294/300, seasonal_3 Loss: 0.0740 | 0.0714
Epoch 295/300, seasonal_3 Loss: 0.0739 | 0.0714
Epoch 296/300, seasonal_3 Loss: 0.0739 | 0.0714
Epoch 297/300, seasonal_3 Loss: 0.0739 | 0.0714
Epoch 298/300, seasonal_3 Loss: 0.0739 | 0.0714
Epoch 299/300, seasonal_3 Loss: 0.0739 | 0.0714
Epoch 300/300, seasonal_3 Loss: 0.0739 | 0.0714
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.6554212425693922, 'learning_rate': 0.00040988410476031626, 'batch_size': 197, 'step_size': 7, 'gamma': 0.9266831805134134}
Epoch 1/300, resid Loss: 0.7299 | 0.3873
Epoch 2/300, resid Loss: 0.2608 | 0.2187
Epoch 3/300, resid Loss: 0.2057 | 0.2175
Epoch 4/300, resid Loss: 0.1671 | 0.1607
Epoch 5/300, resid Loss: 0.1588 | 0.1482
Epoch 6/300, resid Loss: 0.1451 | 0.1407
Epoch 7/300, resid Loss: 0.1385 | 0.1324
Epoch 8/300, resid Loss: 0.1323 | 0.1336
Epoch 9/300, resid Loss: 0.1273 | 0.1402
Epoch 10/300, resid Loss: 0.1239 | 0.1422
Epoch 11/300, resid Loss: 0.1209 | 0.1338
Epoch 12/300, resid Loss: 0.1191 | 0.1149
Epoch 13/300, resid Loss: 0.1180 | 0.1082
Epoch 14/300, resid Loss: 0.1161 | 0.1040
Epoch 15/300, resid Loss: 0.1136 | 0.1111
Epoch 16/300, resid Loss: 0.1128 | 0.1201
Epoch 17/300, resid Loss: 0.1140 | 0.1204
Epoch 18/300, resid Loss: 0.1132 | 0.0997
Epoch 19/300, resid Loss: 0.1103 | 0.0979
Epoch 20/300, resid Loss: 0.1110 | 0.1149
Epoch 21/300, resid Loss: 0.1158 | 0.0944
Epoch 22/300, resid Loss: 0.1164 | 0.1385
Epoch 23/300, resid Loss: 0.1350 | 0.1646
Epoch 24/300, resid Loss: 0.1280 | 0.1062
Epoch 25/300, resid Loss: 0.1182 | 0.1066
Epoch 26/300, resid Loss: 0.1188 | 0.1133
Epoch 27/300, resid Loss: 0.1239 | 0.1029
Epoch 28/300, resid Loss: 0.1129 | 0.1184
Epoch 29/300, resid Loss: 0.1136 | 0.0944
Epoch 30/300, resid Loss: 0.1121 | 0.0978
Epoch 31/300, resid Loss: 0.1087 | 0.0917
Epoch 32/300, resid Loss: 0.1093 | 0.0930
Epoch 33/300, resid Loss: 0.1212 | 0.1244
Epoch 34/300, resid Loss: 0.1126 | 0.1152
Epoch 35/300, resid Loss: 0.1099 | 0.0923
Epoch 36/300, resid Loss: 0.1063 | 0.0884
Epoch 37/300, resid Loss: 0.1042 | 0.0853
Epoch 38/300, resid Loss: 0.1067 | 0.0917
Epoch 39/300, resid Loss: 0.1073 | 0.0913
Epoch 40/300, resid Loss: 0.1119 | 0.1097
Epoch 41/300, resid Loss: 0.1026 | 0.0894
Epoch 42/300, resid Loss: 0.1002 | 0.0875
Epoch 43/300, resid Loss: 0.1000 | 0.0823
Epoch 44/300, resid Loss: 0.1000 | 0.0861
Epoch 45/300, resid Loss: 0.1026 | 0.0897
Epoch 46/300, resid Loss: 0.0997 | 0.0856
Epoch 47/300, resid Loss: 0.0988 | 0.0815
Epoch 48/300, resid Loss: 0.0964 | 0.0810
Epoch 49/300, resid Loss: 0.0960 | 0.0789
Epoch 50/300, resid Loss: 0.0953 | 0.0798
Epoch 51/300, resid Loss: 0.0953 | 0.0783
Epoch 52/300, resid Loss: 0.0950 | 0.0782
Epoch 53/300, resid Loss: 0.0948 | 0.0783
Epoch 54/300, resid Loss: 0.0944 | 0.0777
Epoch 55/300, resid Loss: 0.0942 | 0.0780
Epoch 56/300, resid Loss: 0.0939 | 0.0771
Epoch 57/300, resid Loss: 0.0936 | 0.0775
Epoch 58/300, resid Loss: 0.0934 | 0.0764
Epoch 59/300, resid Loss: 0.0931 | 0.0767
Epoch 60/300, resid Loss: 0.0930 | 0.0759
Epoch 61/300, resid Loss: 0.0926 | 0.0764
Epoch 62/300, resid Loss: 0.0925 | 0.0755
Epoch 63/300, resid Loss: 0.0922 | 0.0759
Epoch 64/300, resid Loss: 0.0921 | 0.0751
Epoch 65/300, resid Loss: 0.0918 | 0.0756
Epoch 66/300, resid Loss: 0.0917 | 0.0748
Epoch 67/300, resid Loss: 0.0914 | 0.0751
Epoch 68/300, resid Loss: 0.0913 | 0.0745
Epoch 69/300, resid Loss: 0.0910 | 0.0749
Epoch 70/300, resid Loss: 0.0910 | 0.0742
Epoch 71/300, resid Loss: 0.0907 | 0.0746
Epoch 72/300, resid Loss: 0.0906 | 0.0740
Epoch 73/300, resid Loss: 0.0904 | 0.0742
Epoch 74/300, resid Loss: 0.0904 | 0.0738
Epoch 75/300, resid Loss: 0.0901 | 0.0739
Epoch 76/300, resid Loss: 0.0900 | 0.0736
Epoch 77/300, resid Loss: 0.0898 | 0.0736
Epoch 78/300, resid Loss: 0.0898 | 0.0734
Epoch 79/300, resid Loss: 0.0896 | 0.0734
Epoch 80/300, resid Loss: 0.0895 | 0.0732
Epoch 81/300, resid Loss: 0.0893 | 0.0731
Epoch 82/300, resid Loss: 0.0892 | 0.0730
Epoch 83/300, resid Loss: 0.0891 | 0.0729
Epoch 84/300, resid Loss: 0.0890 | 0.0728
Epoch 85/300, resid Loss: 0.0888 | 0.0727
Epoch 86/300, resid Loss: 0.0887 | 0.0726
Epoch 87/300, resid Loss: 0.0886 | 0.0725
Epoch 88/300, resid Loss: 0.0885 | 0.0725
Epoch 89/300, resid Loss: 0.0884 | 0.0724
Epoch 90/300, resid Loss: 0.0883 | 0.0723
Epoch 91/300, resid Loss: 0.0882 | 0.0722
Epoch 92/300, resid Loss: 0.0881 | 0.0722
Epoch 93/300, resid Loss: 0.0879 | 0.0721
Epoch 94/300, resid Loss: 0.0879 | 0.0720
Epoch 95/300, resid Loss: 0.0877 | 0.0719
Epoch 96/300, resid Loss: 0.0876 | 0.0719
Epoch 97/300, resid Loss: 0.0875 | 0.0718
Epoch 98/300, resid Loss: 0.0874 | 0.0717
Epoch 99/300, resid Loss: 0.0873 | 0.0717
Epoch 100/300, resid Loss: 0.0873 | 0.0716
Epoch 101/300, resid Loss: 0.0872 | 0.0715
Epoch 102/300, resid Loss: 0.0871 | 0.0715
Epoch 103/300, resid Loss: 0.0870 | 0.0714
Epoch 104/300, resid Loss: 0.0869 | 0.0714
Epoch 105/300, resid Loss: 0.0868 | 0.0713
Epoch 106/300, resid Loss: 0.0867 | 0.0713
Epoch 107/300, resid Loss: 0.0866 | 0.0712
Epoch 108/300, resid Loss: 0.0866 | 0.0712
Epoch 109/300, resid Loss: 0.0865 | 0.0712
Epoch 110/300, resid Loss: 0.0864 | 0.0711
Epoch 111/300, resid Loss: 0.0863 | 0.0711
Epoch 112/300, resid Loss: 0.0862 | 0.0710
Epoch 113/300, resid Loss: 0.0862 | 0.0710
Epoch 114/300, resid Loss: 0.0861 | 0.0709
Epoch 115/300, resid Loss: 0.0860 | 0.0709
Epoch 116/300, resid Loss: 0.0860 | 0.0709
Epoch 117/300, resid Loss: 0.0859 | 0.0708
Epoch 118/300, resid Loss: 0.0858 | 0.0708
Epoch 119/300, resid Loss: 0.0858 | 0.0707
Epoch 120/300, resid Loss: 0.0857 | 0.0707
Epoch 121/300, resid Loss: 0.0856 | 0.0707
Epoch 122/300, resid Loss: 0.0856 | 0.0706
Epoch 123/300, resid Loss: 0.0855 | 0.0706
Epoch 124/300, resid Loss: 0.0855 | 0.0706
Epoch 125/300, resid Loss: 0.0854 | 0.0705
Epoch 126/300, resid Loss: 0.0853 | 0.0705
Epoch 127/300, resid Loss: 0.0853 | 0.0705
Epoch 128/300, resid Loss: 0.0852 | 0.0704
Epoch 129/300, resid Loss: 0.0852 | 0.0704
Epoch 130/300, resid Loss: 0.0851 | 0.0704
Epoch 131/300, resid Loss: 0.0851 | 0.0704
Epoch 132/300, resid Loss: 0.0850 | 0.0703
Epoch 133/300, resid Loss: 0.0850 | 0.0703
Epoch 134/300, resid Loss: 0.0849 | 0.0703
Epoch 135/300, resid Loss: 0.0849 | 0.0702
Epoch 136/300, resid Loss: 0.0848 | 0.0702
Epoch 137/300, resid Loss: 0.0848 | 0.0702
Epoch 138/300, resid Loss: 0.0847 | 0.0701
Epoch 139/300, resid Loss: 0.0847 | 0.0701
Epoch 140/300, resid Loss: 0.0846 | 0.0701
Epoch 141/300, resid Loss: 0.0846 | 0.0701
Epoch 142/300, resid Loss: 0.0845 | 0.0700
Epoch 143/300, resid Loss: 0.0845 | 0.0700
Epoch 144/300, resid Loss: 0.0845 | 0.0700
Epoch 145/300, resid Loss: 0.0844 | 0.0700
Epoch 146/300, resid Loss: 0.0844 | 0.0699
Epoch 147/300, resid Loss: 0.0843 | 0.0699
Epoch 148/300, resid Loss: 0.0843 | 0.0699
Epoch 149/300, resid Loss: 0.0843 | 0.0699
Epoch 150/300, resid Loss: 0.0842 | 0.0698
Epoch 151/300, resid Loss: 0.0842 | 0.0698
Epoch 152/300, resid Loss: 0.0842 | 0.0698
Epoch 153/300, resid Loss: 0.0841 | 0.0698
Epoch 154/300, resid Loss: 0.0841 | 0.0697
Epoch 155/300, resid Loss: 0.0840 | 0.0697
Epoch 156/300, resid Loss: 0.0840 | 0.0697
Epoch 157/300, resid Loss: 0.0840 | 0.0697
Epoch 158/300, resid Loss: 0.0839 | 0.0696
Epoch 159/300, resid Loss: 0.0839 | 0.0696
Epoch 160/300, resid Loss: 0.0839 | 0.0696
Epoch 161/300, resid Loss: 0.0839 | 0.0696
Epoch 162/300, resid Loss: 0.0838 | 0.0695
Epoch 163/300, resid Loss: 0.0838 | 0.0695
Epoch 164/300, resid Loss: 0.0838 | 0.0695
Epoch 165/300, resid Loss: 0.0837 | 0.0695
Epoch 166/300, resid Loss: 0.0837 | 0.0695
Epoch 167/300, resid Loss: 0.0837 | 0.0694
Epoch 168/300, resid Loss: 0.0836 | 0.0694
Epoch 169/300, resid Loss: 0.0836 | 0.0694
Epoch 170/300, resid Loss: 0.0836 | 0.0694
Epoch 171/300, resid Loss: 0.0836 | 0.0694
Epoch 172/300, resid Loss: 0.0835 | 0.0693
Epoch 173/300, resid Loss: 0.0835 | 0.0693
Epoch 174/300, resid Loss: 0.0835 | 0.0693
Epoch 175/300, resid Loss: 0.0835 | 0.0693
Epoch 176/300, resid Loss: 0.0834 | 0.0693
Epoch 177/300, resid Loss: 0.0834 | 0.0693
Epoch 178/300, resid Loss: 0.0834 | 0.0692
Epoch 179/300, resid Loss: 0.0834 | 0.0692
Epoch 180/300, resid Loss: 0.0834 | 0.0692
Epoch 181/300, resid Loss: 0.0833 | 0.0692
Epoch 182/300, resid Loss: 0.0833 | 0.0692
Epoch 183/300, resid Loss: 0.0833 | 0.0691
Epoch 184/300, resid Loss: 0.0833 | 0.0691
Epoch 185/300, resid Loss: 0.0832 | 0.0691
Epoch 186/300, resid Loss: 0.0832 | 0.0691
Epoch 187/300, resid Loss: 0.0832 | 0.0691
Epoch 188/300, resid Loss: 0.0832 | 0.0691
Epoch 189/300, resid Loss: 0.0832 | 0.0691
Epoch 190/300, resid Loss: 0.0832 | 0.0690
Epoch 191/300, resid Loss: 0.0831 | 0.0690
Epoch 192/300, resid Loss: 0.0831 | 0.0690
Epoch 193/300, resid Loss: 0.0831 | 0.0690
Epoch 194/300, resid Loss: 0.0831 | 0.0690
Epoch 195/300, resid Loss: 0.0831 | 0.0690
Epoch 196/300, resid Loss: 0.0830 | 0.0690
Epoch 197/300, resid Loss: 0.0830 | 0.0689
Epoch 198/300, resid Loss: 0.0830 | 0.0689
Epoch 199/300, resid Loss: 0.0830 | 0.0689
Epoch 200/300, resid Loss: 0.0830 | 0.0689
Epoch 201/300, resid Loss: 0.0830 | 0.0689
Epoch 202/300, resid Loss: 0.0830 | 0.0689
Epoch 203/300, resid Loss: 0.0829 | 0.0689
Epoch 204/300, resid Loss: 0.0829 | 0.0689
Epoch 205/300, resid Loss: 0.0829 | 0.0688
Epoch 206/300, resid Loss: 0.0829 | 0.0688
Epoch 207/300, resid Loss: 0.0829 | 0.0688
Epoch 208/300, resid Loss: 0.0829 | 0.0688
Epoch 209/300, resid Loss: 0.0829 | 0.0688
Epoch 210/300, resid Loss: 0.0828 | 0.0688
Epoch 211/300, resid Loss: 0.0828 | 0.0688
Epoch 212/300, resid Loss: 0.0828 | 0.0688
Epoch 213/300, resid Loss: 0.0828 | 0.0688
Epoch 214/300, resid Loss: 0.0828 | 0.0687
Epoch 215/300, resid Loss: 0.0828 | 0.0687
Epoch 216/300, resid Loss: 0.0828 | 0.0687
Epoch 217/300, resid Loss: 0.0828 | 0.0687
Epoch 218/300, resid Loss: 0.0828 | 0.0687
Epoch 219/300, resid Loss: 0.0827 | 0.0687
Epoch 220/300, resid Loss: 0.0827 | 0.0687
Epoch 221/300, resid Loss: 0.0827 | 0.0687
Epoch 222/300, resid Loss: 0.0827 | 0.0687
Epoch 223/300, resid Loss: 0.0827 | 0.0687
Epoch 224/300, resid Loss: 0.0827 | 0.0686
Epoch 225/300, resid Loss: 0.0827 | 0.0686
Epoch 226/300, resid Loss: 0.0827 | 0.0686
Epoch 227/300, resid Loss: 0.0827 | 0.0686
Epoch 228/300, resid Loss: 0.0827 | 0.0686
Epoch 229/300, resid Loss: 0.0827 | 0.0686
Epoch 230/300, resid Loss: 0.0826 | 0.0686
Epoch 231/300, resid Loss: 0.0826 | 0.0686
Epoch 232/300, resid Loss: 0.0826 | 0.0686
Epoch 233/300, resid Loss: 0.0826 | 0.0686
Epoch 234/300, resid Loss: 0.0826 | 0.0686
Epoch 235/300, resid Loss: 0.0826 | 0.0686
Epoch 236/300, resid Loss: 0.0826 | 0.0686
Epoch 237/300, resid Loss: 0.0826 | 0.0686
Epoch 238/300, resid Loss: 0.0826 | 0.0685
Epoch 239/300, resid Loss: 0.0826 | 0.0685
Epoch 240/300, resid Loss: 0.0826 | 0.0685
Epoch 241/300, resid Loss: 0.0826 | 0.0685
Epoch 242/300, resid Loss: 0.0826 | 0.0685
Epoch 243/300, resid Loss: 0.0825 | 0.0685
Epoch 244/300, resid Loss: 0.0825 | 0.0685
Epoch 245/300, resid Loss: 0.0825 | 0.0685
Epoch 246/300, resid Loss: 0.0825 | 0.0685
Epoch 247/300, resid Loss: 0.0825 | 0.0685
Epoch 248/300, resid Loss: 0.0825 | 0.0685
Epoch 249/300, resid Loss: 0.0825 | 0.0685
Epoch 250/300, resid Loss: 0.0825 | 0.0685
Epoch 251/300, resid Loss: 0.0825 | 0.0685
Epoch 252/300, resid Loss: 0.0825 | 0.0685
Epoch 253/300, resid Loss: 0.0825 | 0.0685
Epoch 254/300, resid Loss: 0.0825 | 0.0685
Epoch 255/300, resid Loss: 0.0825 | 0.0684
Epoch 256/300, resid Loss: 0.0825 | 0.0684
Epoch 257/300, resid Loss: 0.0825 | 0.0684
Epoch 258/300, resid Loss: 0.0825 | 0.0684
Epoch 259/300, resid Loss: 0.0825 | 0.0684
Epoch 260/300, resid Loss: 0.0825 | 0.0684
Epoch 261/300, resid Loss: 0.0824 | 0.0684
Epoch 262/300, resid Loss: 0.0824 | 0.0684
Epoch 263/300, resid Loss: 0.0824 | 0.0684
Epoch 264/300, resid Loss: 0.0824 | 0.0684
Epoch 265/300, resid Loss: 0.0824 | 0.0684
Epoch 266/300, resid Loss: 0.0824 | 0.0684
Epoch 267/300, resid Loss: 0.0824 | 0.0684
Epoch 268/300, resid Loss: 0.0824 | 0.0684
Epoch 269/300, resid Loss: 0.0824 | 0.0684
Epoch 270/300, resid Loss: 0.0824 | 0.0684
Epoch 271/300, resid Loss: 0.0824 | 0.0684
Epoch 272/300, resid Loss: 0.0824 | 0.0684
Epoch 273/300, resid Loss: 0.0824 | 0.0684
Epoch 274/300, resid Loss: 0.0824 | 0.0684
Epoch 275/300, resid Loss: 0.0824 | 0.0684
Epoch 276/300, resid Loss: 0.0824 | 0.0684
Epoch 277/300, resid Loss: 0.0824 | 0.0684
Epoch 278/300, resid Loss: 0.0824 | 0.0684
Epoch 279/300, resid Loss: 0.0824 | 0.0684
Epoch 280/300, resid Loss: 0.0824 | 0.0684
Epoch 281/300, resid Loss: 0.0824 | 0.0684
Epoch 282/300, resid Loss: 0.0824 | 0.0683
Epoch 283/300, resid Loss: 0.0824 | 0.0683
Epoch 284/300, resid Loss: 0.0824 | 0.0683
Epoch 285/300, resid Loss: 0.0824 | 0.0683
Epoch 286/300, resid Loss: 0.0824 | 0.0683
Epoch 287/300, resid Loss: 0.0824 | 0.0683
Epoch 288/300, resid Loss: 0.0824 | 0.0683
Epoch 289/300, resid Loss: 0.0823 | 0.0683
Epoch 290/300, resid Loss: 0.0823 | 0.0683
Epoch 291/300, resid Loss: 0.0823 | 0.0683
Epoch 292/300, resid Loss: 0.0823 | 0.0683
Epoch 293/300, resid Loss: 0.0823 | 0.0683
Epoch 294/300, resid Loss: 0.0823 | 0.0683
Epoch 295/300, resid Loss: 0.0823 | 0.0683
Epoch 296/300, resid Loss: 0.0823 | 0.0683
Epoch 297/300, resid Loss: 0.0823 | 0.0683
Epoch 298/300, resid Loss: 0.0823 | 0.0683
Epoch 299/300, resid Loss: 0.0823 | 0.0683
Epoch 300/300, resid Loss: 0.0823 | 0.0683
Runtime (seconds): 2187.2488367557526
8.806475988691994e-05
[101.28027]
[5.7535214]
[-1.7495893]
[0.31304553]
[-4.7642107]
[-3.149465]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 17.27580931619741
RMSE: 4.1564178466796875
MAE: 4.1564178466796875
R-squared: nan
[97.68358]
