[32m[I 2025-01-07 09:23:45,585][0m A new study created in memory with name: no-name-e028fcb8-9214-405b-808a-178e2ba29f34[0m
[32m[I 2025-01-07 09:29:18,250][0m Trial 0 finished with value: 0.3686094126105309 and parameters: {'observation_period_num': 236, 'train_rates': 0.816727344477465, 'learning_rate': 0.00016353818182918253, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9127047358173161}. Best is trial 0 with value: 0.3686094126105309.[0m
[32m[I 2025-01-07 09:30:33,811][0m Trial 1 finished with value: 0.16277890115723773 and parameters: {'observation_period_num': 58, 'train_rates': 0.919099018745825, 'learning_rate': 0.00035666213151399576, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9377768544329684}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:31:22,661][0m Trial 2 finished with value: 1.6820147081594887 and parameters: {'observation_period_num': 25, 'train_rates': 0.7609164449271009, 'learning_rate': 0.0008728890399384122, 'batch_size': 78, 'step_size': 4, 'gamma': 0.9573744104287556}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:31:53,128][0m Trial 3 finished with value: 0.28216768924139396 and parameters: {'observation_period_num': 22, 'train_rates': 0.831226937343356, 'learning_rate': 0.000333186569124616, 'batch_size': 212, 'step_size': 5, 'gamma': 0.9073757055617531}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:37:28,654][0m Trial 4 finished with value: 0.5642986771713107 and parameters: {'observation_period_num': 247, 'train_rates': 0.7750786862293457, 'learning_rate': 0.000126431536592848, 'batch_size': 134, 'step_size': 14, 'gamma': 0.836804022561477}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:37:48,205][0m Trial 5 finished with value: 0.4407578578736997 and parameters: {'observation_period_num': 6, 'train_rates': 0.8202326967117027, 'learning_rate': 0.0007944966824316913, 'batch_size': 191, 'step_size': 3, 'gamma': 0.9475940399473554}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:42:28,726][0m Trial 6 finished with value: 1.392261146656616 and parameters: {'observation_period_num': 207, 'train_rates': 0.7948213999518863, 'learning_rate': 1.2981937235763847e-06, 'batch_size': 210, 'step_size': 10, 'gamma': 0.860313591814531}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:47:20,114][0m Trial 7 finished with value: 0.31839517575043896 and parameters: {'observation_period_num': 192, 'train_rates': 0.883045399979403, 'learning_rate': 4.530439400331321e-06, 'batch_size': 24, 'step_size': 9, 'gamma': 0.8859343250784903}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:49:11,418][0m Trial 8 finished with value: 0.2237240895628929 and parameters: {'observation_period_num': 86, 'train_rates': 0.8917431441563569, 'learning_rate': 0.00044795755439872606, 'batch_size': 156, 'step_size': 14, 'gamma': 0.8849865595031946}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:51:11,029][0m Trial 9 finished with value: 0.17024756968021393 and parameters: {'observation_period_num': 92, 'train_rates': 0.924431592085678, 'learning_rate': 7.910205491934171e-05, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9170011317042691}. Best is trial 1 with value: 0.16277890115723773.[0m
Early stopping at epoch 59
[32m[I 2025-01-07 09:52:45,289][0m Trial 10 finished with value: 1.3390593218895726 and parameters: {'observation_period_num': 141, 'train_rates': 0.6341877291233631, 'learning_rate': 1.9684987385719898e-05, 'batch_size': 102, 'step_size': 1, 'gamma': 0.7981800418448486}. Best is trial 1 with value: 0.16277890115723773.[0m
[32m[I 2025-01-07 09:54:33,039][0m Trial 11 finished with value: 0.1609724760055542 and parameters: {'observation_period_num': 80, 'train_rates': 0.9839378692968672, 'learning_rate': 5.278742634983823e-05, 'batch_size': 251, 'step_size': 6, 'gamma': 0.9840302919117356}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 09:56:19,392][0m Trial 12 finished with value: 0.2462134063243866 and parameters: {'observation_period_num': 78, 'train_rates': 0.9873625302033634, 'learning_rate': 1.9160718866583122e-05, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9853806293932037}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 09:59:27,959][0m Trial 13 finished with value: 0.2391531765460968 and parameters: {'observation_period_num': 130, 'train_rates': 0.9765631505447547, 'learning_rate': 4.7575588178192814e-05, 'batch_size': 160, 'step_size': 1, 'gamma': 0.9572871504207725}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:01:04,140][0m Trial 14 finished with value: 0.16190950486522454 and parameters: {'observation_period_num': 57, 'train_rates': 0.9374957269432433, 'learning_rate': 9.426761699730654e-06, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9828299961670369}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:02:25,071][0m Trial 15 finished with value: 0.7939059142150274 and parameters: {'observation_period_num': 50, 'train_rates': 0.7081265367230376, 'learning_rate': 6.4044155984308874e-06, 'batch_size': 41, 'step_size': 7, 'gamma': 0.982610190912183}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:06:25,260][0m Trial 16 finished with value: 0.27292193387235913 and parameters: {'observation_period_num': 161, 'train_rates': 0.9500501296345556, 'learning_rate': 7.609939850897783e-06, 'batch_size': 61, 'step_size': 12, 'gamma': 0.753397633306973}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:08:44,803][0m Trial 17 finished with value: 0.6992473637735521 and parameters: {'observation_period_num': 107, 'train_rates': 0.8706866316637392, 'learning_rate': 1.9762136776517798e-06, 'batch_size': 98, 'step_size': 11, 'gamma': 0.9867184291192792}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:09:49,637][0m Trial 18 finished with value: 0.23914024804109407 and parameters: {'observation_period_num': 47, 'train_rates': 0.9447207881513449, 'learning_rate': 2.7627404730625975e-05, 'batch_size': 118, 'step_size': 6, 'gamma': 0.8221125236428267}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:11:47,780][0m Trial 19 finished with value: 1.0710194434591855 and parameters: {'observation_period_num': 105, 'train_rates': 0.7282395616354828, 'learning_rate': 1.156296120019936e-05, 'batch_size': 188, 'step_size': 4, 'gamma': 0.9324016741729604}. Best is trial 11 with value: 0.1609724760055542.[0m
[32m[I 2025-01-07 10:15:32,773][0m Trial 20 finished with value: 0.11486007049679756 and parameters: {'observation_period_num': 67, 'train_rates': 0.989757945994361, 'learning_rate': 5.1902436885015726e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9630102937331789}. Best is trial 20 with value: 0.11486007049679756.[0m
[32m[I 2025-01-07 10:19:08,636][0m Trial 21 finished with value: 0.11744539101015437 and parameters: {'observation_period_num': 69, 'train_rates': 0.9887842476172406, 'learning_rate': 5.421021058018093e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9667732632420302}. Best is trial 20 with value: 0.11486007049679756.[0m
[32m[I 2025-01-07 10:22:51,299][0m Trial 22 finished with value: 0.10271676369877748 and parameters: {'observation_period_num': 106, 'train_rates': 0.9851294575872503, 'learning_rate': 5.4284352538292996e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9646528346139415}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:26:13,951][0m Trial 23 finished with value: 0.13040460192638895 and parameters: {'observation_period_num': 111, 'train_rates': 0.9599920169770656, 'learning_rate': 0.00010683612005818356, 'batch_size': 20, 'step_size': 8, 'gamma': 0.9595817783652382}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:30:00,924][0m Trial 24 finished with value: 0.19699995929165504 and parameters: {'observation_period_num': 159, 'train_rates': 0.8958274590628977, 'learning_rate': 4.914181423553532e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.9270258840128341}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:31:34,368][0m Trial 25 finished with value: 0.37852592755328207 and parameters: {'observation_period_num': 68, 'train_rates': 0.8591088243244183, 'learning_rate': 0.00018019434894541258, 'batch_size': 75, 'step_size': 8, 'gamma': 0.8938154633232247}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:34:44,055][0m Trial 26 finished with value: 0.14209158129607263 and parameters: {'observation_period_num': 40, 'train_rates': 0.9137326437316916, 'learning_rate': 3.0458926995367873e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.9639661222559043}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:37:03,022][0m Trial 27 finished with value: 0.9077604619201255 and parameters: {'observation_period_num': 124, 'train_rates': 0.6356903029863534, 'learning_rate': 8.407597462697041e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9434658366427167}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:37:58,375][0m Trial 28 finished with value: 0.12060622870922089 and parameters: {'observation_period_num': 30, 'train_rates': 0.9897184309937254, 'learning_rate': 0.00022430858248481423, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8590807147938138}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:42:00,293][0m Trial 29 finished with value: 0.13119364037699655 and parameters: {'observation_period_num': 154, 'train_rates': 0.9615295816929699, 'learning_rate': 1.6021861711113595e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9102929383390986}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:43:23,183][0m Trial 30 finished with value: 0.8372906500657316 and parameters: {'observation_period_num': 68, 'train_rates': 0.6743377313865713, 'learning_rate': 6.733544920937776e-05, 'batch_size': 62, 'step_size': 5, 'gamma': 0.9642745343248934}. Best is trial 22 with value: 0.10271676369877748.[0m
[32m[I 2025-01-07 10:47:35,079][0m Trial 31 finished with value: 0.08790283033560062 and parameters: {'observation_period_num': 32, 'train_rates': 0.9803216685694176, 'learning_rate': 0.00018540923031875578, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8595535928004019}. Best is trial 31 with value: 0.08790283033560062.[0m
[32m[I 2025-01-07 10:49:41,050][0m Trial 32 finished with value: 0.08666467222205379 and parameters: {'observation_period_num': 7, 'train_rates': 0.9616263608729723, 'learning_rate': 0.00014056466497531708, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8405021376166915}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:51:40,231][0m Trial 33 finished with value: 0.10412563860979902 and parameters: {'observation_period_num': 5, 'train_rates': 0.9196799000136289, 'learning_rate': 0.00022384392560455493, 'batch_size': 33, 'step_size': 8, 'gamma': 0.8387578811709051}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:53:25,486][0m Trial 34 finished with value: 0.3549598215613514 and parameters: {'observation_period_num': 10, 'train_rates': 0.8485633087730962, 'learning_rate': 0.0002884539157535763, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8327686286012707}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:54:40,073][0m Trial 35 finished with value: 0.39928765553567147 and parameters: {'observation_period_num': 22, 'train_rates': 0.9144391512050857, 'learning_rate': 0.00048572681537268454, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8015823399239604}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:55:38,081][0m Trial 36 finished with value: 0.12502557884815127 and parameters: {'observation_period_num': 34, 'train_rates': 0.926685546763104, 'learning_rate': 0.00013580368451993006, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8543143613804588}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:56:24,913][0m Trial 37 finished with value: 0.32407920022981357 and parameters: {'observation_period_num': 7, 'train_rates': 0.9618355969975627, 'learning_rate': 0.0006274513808112561, 'batch_size': 91, 'step_size': 3, 'gamma': 0.8087616750893536}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 10:58:12,460][0m Trial 38 finished with value: 0.1182550264110092 and parameters: {'observation_period_num': 18, 'train_rates': 0.9112194325377458, 'learning_rate': 0.00017722394638520147, 'batch_size': 36, 'step_size': 8, 'gamma': 0.781651762308604}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:02:33,149][0m Trial 39 finished with value: 0.3281922993452653 and parameters: {'observation_period_num': 192, 'train_rates': 0.8342751207395449, 'learning_rate': 0.0002678337729466337, 'batch_size': 48, 'step_size': 5, 'gamma': 0.8430069005502239}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:03:05,236][0m Trial 40 finished with value: 0.35940721489134286 and parameters: {'observation_period_num': 18, 'train_rates': 0.8003890811836617, 'learning_rate': 0.00011091280009781784, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8649937176023819}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:07:13,399][0m Trial 41 finished with value: 0.625495120565942 and parameters: {'observation_period_num': 42, 'train_rates': 0.9679839593972887, 'learning_rate': 0.0003864510647955567, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8758684548206245}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:09:20,971][0m Trial 42 finished with value: 0.10452060742219063 and parameters: {'observation_period_num': 30, 'train_rates': 0.9400185221571189, 'learning_rate': 3.797882741376643e-05, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8469547365157751}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:11:28,752][0m Trial 43 finished with value: 0.10988487718118863 and parameters: {'observation_period_num': 29, 'train_rates': 0.9401016406510776, 'learning_rate': 0.00021415644828616618, 'batch_size': 31, 'step_size': 9, 'gamma': 0.8248430851636863}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:13:40,540][0m Trial 44 finished with value: 0.11426635534561354 and parameters: {'observation_period_num': 7, 'train_rates': 0.8983563558542893, 'learning_rate': 0.00014796533581470914, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8490955943165753}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:19:32,905][0m Trial 45 finished with value: 0.14972562342882156 and parameters: {'observation_period_num': 231, 'train_rates': 0.9300348816451585, 'learning_rate': 9.002116579555629e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.873834017586942}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:21:00,448][0m Trial 46 finished with value: 0.11981320523080372 and parameters: {'observation_period_num': 57, 'train_rates': 0.956737991295, 'learning_rate': 3.659482849930596e-05, 'batch_size': 53, 'step_size': 15, 'gamma': 0.8911669208475037}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:21:47,525][0m Trial 47 finished with value: 0.5062251483316476 and parameters: {'observation_period_num': 19, 'train_rates': 0.8798521577489454, 'learning_rate': 0.0005996161916425502, 'batch_size': 86, 'step_size': 6, 'gamma': 0.8169017262581549}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:24:11,961][0m Trial 48 finished with value: 0.1025021265853535 and parameters: {'observation_period_num': 38, 'train_rates': 0.9476721605915873, 'learning_rate': 3.692691076670271e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8401553253259084}. Best is trial 32 with value: 0.08666467222205379.[0m
[32m[I 2025-01-07 11:26:28,166][0m Trial 49 finished with value: 0.21590348543264928 and parameters: {'observation_period_num': 90, 'train_rates': 0.972880699473057, 'learning_rate': 0.0003364436570102183, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8350837903686961}. Best is trial 32 with value: 0.08666467222205379.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.4707 | 0.6100
Epoch 2/300, Loss: 0.4569 | 0.5898
Epoch 3/300, Loss: 0.4506 | 0.4838
Epoch 4/300, Loss: 0.3622 | 0.4824
Epoch 5/300, Loss: 0.3308 | 0.5461
Epoch 6/300, Loss: 0.3011 | 0.3835
Epoch 7/300, Loss: 0.2664 | 0.3472
Epoch 8/300, Loss: 0.2673 | 0.3113
Epoch 9/300, Loss: 0.2515 | 0.3099
Epoch 10/300, Loss: 0.2373 | 0.3309
Epoch 11/300, Loss: 0.2241 | 0.2709
Epoch 12/300, Loss: 0.2414 | 0.2574
Epoch 13/300, Loss: 0.2516 | 0.3059
Epoch 14/300, Loss: 0.2095 | 0.2874
Epoch 15/300, Loss: 0.2286 | 0.2962
Epoch 16/300, Loss: 0.1894 | 0.2354
Epoch 17/300, Loss: 0.1777 | 0.2483
Epoch 18/300, Loss: 0.1714 | 0.2040
Epoch 19/300, Loss: 0.1584 | 0.1973
Epoch 20/300, Loss: 0.1508 | 0.2284
Epoch 21/300, Loss: 0.1426 | 0.1824
Epoch 22/300, Loss: 0.1391 | 0.1706
Epoch 23/300, Loss: 0.1348 | 0.1915
Epoch 24/300, Loss: 0.1318 | 0.1642
Epoch 25/300, Loss: 0.1323 | 0.1765
Epoch 26/300, Loss: 0.1332 | 0.1721
Epoch 27/300, Loss: 0.1291 | 0.1477
Epoch 28/300, Loss: 0.1307 | 0.1748
Epoch 29/300, Loss: 0.1240 | 0.1532
Epoch 30/300, Loss: 0.1194 | 0.1387
Epoch 31/300, Loss: 0.1183 | 0.1511
Epoch 32/300, Loss: 0.1125 | 0.1472
Epoch 33/300, Loss: 0.1106 | 0.1305
Epoch 34/300, Loss: 0.1089 | 0.1317
Epoch 35/300, Loss: 0.1065 | 0.1343
Epoch 36/300, Loss: 0.1055 | 0.1238
Epoch 37/300, Loss: 0.1037 | 0.1210
Epoch 38/300, Loss: 0.1025 | 0.1227
Epoch 39/300, Loss: 0.1016 | 0.1177
Epoch 40/300, Loss: 0.1013 | 0.1140
Epoch 41/300, Loss: 0.0995 | 0.1143
Epoch 42/300, Loss: 0.0996 | 0.1108
Epoch 43/300, Loss: 0.0980 | 0.1118
Epoch 44/300, Loss: 0.0972 | 0.1075
Epoch 45/300, Loss: 0.0959 | 0.1045
Epoch 46/300, Loss: 0.0954 | 0.1083
Epoch 47/300, Loss: 0.0951 | 0.1049
Epoch 48/300, Loss: 0.0939 | 0.1012
Epoch 49/300, Loss: 0.0942 | 0.1030
Epoch 50/300, Loss: 0.0928 | 0.1016
Epoch 51/300, Loss: 0.0929 | 0.0985
Epoch 52/300, Loss: 0.0918 | 0.0994
Epoch 53/300, Loss: 0.0914 | 0.0989
Epoch 54/300, Loss: 0.0911 | 0.0961
Epoch 55/300, Loss: 0.0909 | 0.0961
Epoch 56/300, Loss: 0.0901 | 0.0975
Epoch 57/300, Loss: 0.0890 | 0.0951
Epoch 58/300, Loss: 0.0888 | 0.0937
Epoch 59/300, Loss: 0.0886 | 0.0945
Epoch 60/300, Loss: 0.0889 | 0.0937
Epoch 61/300, Loss: 0.0878 | 0.0932
Epoch 62/300, Loss: 0.0879 | 0.0930
Epoch 63/300, Loss: 0.0873 | 0.0914
Epoch 64/300, Loss: 0.0871 | 0.0909
Epoch 65/300, Loss: 0.0875 | 0.0909
Epoch 66/300, Loss: 0.0867 | 0.0901
Epoch 67/300, Loss: 0.0867 | 0.0897
Epoch 68/300, Loss: 0.0856 | 0.0890
Epoch 69/300, Loss: 0.0855 | 0.0891
Epoch 70/300, Loss: 0.0861 | 0.0896
Epoch 71/300, Loss: 0.0859 | 0.0884
Epoch 72/300, Loss: 0.0852 | 0.0881
Epoch 73/300, Loss: 0.0849 | 0.0882
Epoch 74/300, Loss: 0.0846 | 0.0883
Epoch 75/300, Loss: 0.0846 | 0.0879
Epoch 76/300, Loss: 0.0845 | 0.0875
Epoch 77/300, Loss: 0.0844 | 0.0872
Epoch 78/300, Loss: 0.0837 | 0.0870
Epoch 79/300, Loss: 0.0842 | 0.0865
Epoch 80/300, Loss: 0.0838 | 0.0865
Epoch 81/300, Loss: 0.0836 | 0.0864
Epoch 82/300, Loss: 0.0847 | 0.0861
Epoch 83/300, Loss: 0.0839 | 0.0863
Epoch 84/300, Loss: 0.0830 | 0.0858
Epoch 85/300, Loss: 0.0838 | 0.0857
Epoch 86/300, Loss: 0.0830 | 0.0854
Epoch 87/300, Loss: 0.0832 | 0.0853
Epoch 88/300, Loss: 0.0829 | 0.0853
Epoch 89/300, Loss: 0.0829 | 0.0852
Epoch 90/300, Loss: 0.0829 | 0.0851
Epoch 91/300, Loss: 0.0832 | 0.0849
Epoch 92/300, Loss: 0.0820 | 0.0848
Epoch 93/300, Loss: 0.0833 | 0.0848
Epoch 94/300, Loss: 0.0826 | 0.0846
Epoch 95/300, Loss: 0.0824 | 0.0842
Epoch 96/300, Loss: 0.0823 | 0.0840
Epoch 97/300, Loss: 0.0826 | 0.0841
Epoch 98/300, Loss: 0.0834 | 0.0842
Epoch 99/300, Loss: 0.0822 | 0.0842
Epoch 100/300, Loss: 0.0827 | 0.0840
Epoch 101/300, Loss: 0.0819 | 0.0841
Epoch 102/300, Loss: 0.0824 | 0.0840
Epoch 103/300, Loss: 0.0824 | 0.0840
Epoch 104/300, Loss: 0.0821 | 0.0838
Epoch 105/300, Loss: 0.0826 | 0.0839
Epoch 106/300, Loss: 0.0819 | 0.0839
Epoch 107/300, Loss: 0.0825 | 0.0838
Epoch 108/300, Loss: 0.0817 | 0.0838
Epoch 109/300, Loss: 0.0816 | 0.0838
Epoch 110/300, Loss: 0.0819 | 0.0837
Epoch 111/300, Loss: 0.0824 | 0.0838
Epoch 112/300, Loss: 0.0820 | 0.0838
Epoch 113/300, Loss: 0.0815 | 0.0838
Epoch 114/300, Loss: 0.0816 | 0.0836
Epoch 115/300, Loss: 0.0817 | 0.0836
Epoch 116/300, Loss: 0.0814 | 0.0836
Epoch 117/300, Loss: 0.0814 | 0.0836
Epoch 118/300, Loss: 0.0824 | 0.0836
Epoch 119/300, Loss: 0.0818 | 0.0835
Epoch 120/300, Loss: 0.0814 | 0.0833
Epoch 121/300, Loss: 0.0816 | 0.0832
Epoch 122/300, Loss: 0.0816 | 0.0832
Epoch 123/300, Loss: 0.0816 | 0.0832
Epoch 124/300, Loss: 0.0823 | 0.0833
Epoch 125/300, Loss: 0.0811 | 0.0833
Epoch 126/300, Loss: 0.0815 | 0.0833
Epoch 127/300, Loss: 0.0816 | 0.0832
Epoch 128/300, Loss: 0.0815 | 0.0832
Epoch 129/300, Loss: 0.0812 | 0.0832
Epoch 130/300, Loss: 0.0826 | 0.0832
Epoch 131/300, Loss: 0.0810 | 0.0832
Epoch 132/300, Loss: 0.0817 | 0.0832
Epoch 133/300, Loss: 0.0812 | 0.0832
Epoch 134/300, Loss: 0.0818 | 0.0831
Epoch 135/300, Loss: 0.0814 | 0.0831
Epoch 136/300, Loss: 0.0816 | 0.0832
Epoch 137/300, Loss: 0.0819 | 0.0832
Epoch 138/300, Loss: 0.0815 | 0.0832
Epoch 139/300, Loss: 0.0816 | 0.0831
Epoch 140/300, Loss: 0.0812 | 0.0831
Epoch 141/300, Loss: 0.0819 | 0.0831
Epoch 142/300, Loss: 0.0815 | 0.0831
Epoch 143/300, Loss: 0.0818 | 0.0831
Epoch 144/300, Loss: 0.0815 | 0.0830
Epoch 145/300, Loss: 0.0817 | 0.0830
Epoch 146/300, Loss: 0.0814 | 0.0830
Epoch 147/300, Loss: 0.0817 | 0.0830
Epoch 148/300, Loss: 0.0818 | 0.0830
Epoch 149/300, Loss: 0.0818 | 0.0831
Epoch 150/300, Loss: 0.0816 | 0.0831
Epoch 151/300, Loss: 0.0817 | 0.0830
Epoch 152/300, Loss: 0.0814 | 0.0830
Epoch 153/300, Loss: 0.0816 | 0.0830
Epoch 154/300, Loss: 0.0821 | 0.0830
Epoch 155/300, Loss: 0.0814 | 0.0830
Epoch 156/300, Loss: 0.0817 | 0.0830
Epoch 157/300, Loss: 0.0813 | 0.0830
Epoch 158/300, Loss: 0.0815 | 0.0830
Epoch 159/300, Loss: 0.0816 | 0.0830
Epoch 160/300, Loss: 0.0817 | 0.0830
Epoch 161/300, Loss: 0.0816 | 0.0830
Epoch 162/300, Loss: 0.0822 | 0.0830
Epoch 163/300, Loss: 0.0815 | 0.0830
Epoch 164/300, Loss: 0.0818 | 0.0830
Epoch 165/300, Loss: 0.0816 | 0.0830
Epoch 166/300, Loss: 0.0814 | 0.0830
Epoch 167/300, Loss: 0.0816 | 0.0830
Epoch 168/300, Loss: 0.0808 | 0.0830
Epoch 169/300, Loss: 0.0807 | 0.0830
Epoch 170/300, Loss: 0.0815 | 0.0830
Epoch 171/300, Loss: 0.0816 | 0.0830
Epoch 172/300, Loss: 0.0817 | 0.0830
Epoch 173/300, Loss: 0.0815 | 0.0830
Epoch 174/300, Loss: 0.0811 | 0.0830
Epoch 175/300, Loss: 0.0816 | 0.0830
Epoch 176/300, Loss: 0.0814 | 0.0830
Epoch 177/300, Loss: 0.0815 | 0.0830
Epoch 178/300, Loss: 0.0819 | 0.0830
Epoch 179/300, Loss: 0.0814 | 0.0830
Epoch 180/300, Loss: 0.0814 | 0.0830
Epoch 181/300, Loss: 0.0820 | 0.0830
Epoch 182/300, Loss: 0.0825 | 0.0830
Epoch 183/300, Loss: 0.0811 | 0.0830
Epoch 184/300, Loss: 0.0815 | 0.0830
Epoch 185/300, Loss: 0.0813 | 0.0830
Epoch 186/300, Loss: 0.0814 | 0.0830
Epoch 187/300, Loss: 0.0813 | 0.0830
Epoch 188/300, Loss: 0.0817 | 0.0830
Epoch 189/300, Loss: 0.0812 | 0.0830
Epoch 190/300, Loss: 0.0820 | 0.0830
Epoch 191/300, Loss: 0.0815 | 0.0830
Epoch 192/300, Loss: 0.0815 | 0.0830
Epoch 193/300, Loss: 0.0813 | 0.0830
Epoch 194/300, Loss: 0.0810 | 0.0830
Epoch 195/300, Loss: 0.0810 | 0.0830
Epoch 196/300, Loss: 0.0819 | 0.0830
Epoch 197/300, Loss: 0.0819 | 0.0830
Epoch 198/300, Loss: 0.0817 | 0.0830
Epoch 199/300, Loss: 0.0809 | 0.0830
Epoch 200/300, Loss: 0.0814 | 0.0830
Epoch 201/300, Loss: 0.0817 | 0.0830
Epoch 202/300, Loss: 0.0822 | 0.0830
Epoch 203/300, Loss: 0.0820 | 0.0830
Epoch 204/300, Loss: 0.0811 | 0.0830
Epoch 205/300, Loss: 0.0807 | 0.0829
Epoch 206/300, Loss: 0.0817 | 0.0829
Epoch 207/300, Loss: 0.0815 | 0.0829
Epoch 208/300, Loss: 0.0815 | 0.0829
Epoch 209/300, Loss: 0.0814 | 0.0829
Epoch 210/300, Loss: 0.0814 | 0.0829
Epoch 211/300, Loss: 0.0812 | 0.0829
Epoch 212/300, Loss: 0.0816 | 0.0829
Epoch 213/300, Loss: 0.0820 | 0.0829
Epoch 214/300, Loss: 0.0811 | 0.0829
Epoch 215/300, Loss: 0.0816 | 0.0829
Epoch 216/300, Loss: 0.0816 | 0.0829
Epoch 217/300, Loss: 0.0811 | 0.0829
Epoch 218/300, Loss: 0.0811 | 0.0829
Epoch 219/300, Loss: 0.0813 | 0.0829
Epoch 220/300, Loss: 0.0815 | 0.0829
Epoch 221/300, Loss: 0.0810 | 0.0829
Epoch 222/300, Loss: 0.0814 | 0.0829
Epoch 223/300, Loss: 0.0814 | 0.0829
Epoch 224/300, Loss: 0.0817 | 0.0829
Epoch 225/300, Loss: 0.0809 | 0.0829
Epoch 226/300, Loss: 0.0818 | 0.0829
Epoch 227/300, Loss: 0.0824 | 0.0829
Epoch 228/300, Loss: 0.0810 | 0.0829
Epoch 229/300, Loss: 0.0812 | 0.0829
Epoch 230/300, Loss: 0.0816 | 0.0829
Epoch 231/300, Loss: 0.0812 | 0.0829
Epoch 232/300, Loss: 0.0816 | 0.0829
Epoch 233/300, Loss: 0.0816 | 0.0829
Epoch 234/300, Loss: 0.0815 | 0.0829
Epoch 235/300, Loss: 0.0818 | 0.0829
Epoch 236/300, Loss: 0.0817 | 0.0829
Epoch 237/300, Loss: 0.0814 | 0.0829
Epoch 238/300, Loss: 0.0811 | 0.0829
Epoch 239/300, Loss: 0.0813 | 0.0829
Epoch 240/300, Loss: 0.0811 | 0.0829
Epoch 241/300, Loss: 0.0816 | 0.0829
Epoch 242/300, Loss: 0.0811 | 0.0829
Epoch 243/300, Loss: 0.0822 | 0.0829
Epoch 244/300, Loss: 0.0814 | 0.0829
Epoch 245/300, Loss: 0.0810 | 0.0829
Epoch 246/300, Loss: 0.0818 | 0.0829
Epoch 247/300, Loss: 0.0822 | 0.0829
Epoch 248/300, Loss: 0.0815 | 0.0829
Epoch 249/300, Loss: 0.0811 | 0.0829
Epoch 250/300, Loss: 0.0815 | 0.0829
Epoch 251/300, Loss: 0.0816 | 0.0829
Epoch 252/300, Loss: 0.0809 | 0.0829
Epoch 253/300, Loss: 0.0817 | 0.0829
Epoch 254/300, Loss: 0.0807 | 0.0829
Epoch 255/300, Loss: 0.0812 | 0.0829
Epoch 256/300, Loss: 0.0822 | 0.0829
Epoch 257/300, Loss: 0.0811 | 0.0829
Epoch 258/300, Loss: 0.0813 | 0.0829
Epoch 259/300, Loss: 0.0813 | 0.0829
Epoch 260/300, Loss: 0.0815 | 0.0829
Epoch 261/300, Loss: 0.0811 | 0.0829
Epoch 262/300, Loss: 0.0803 | 0.0829
Epoch 263/300, Loss: 0.0812 | 0.0829
Epoch 264/300, Loss: 0.0818 | 0.0829
Epoch 265/300, Loss: 0.0816 | 0.0829
Epoch 266/300, Loss: 0.0812 | 0.0829
Epoch 267/300, Loss: 0.0807 | 0.0829
Epoch 268/300, Loss: 0.0812 | 0.0829
Epoch 269/300, Loss: 0.0815 | 0.0829
Epoch 270/300, Loss: 0.0819 | 0.0829
Epoch 271/300, Loss: 0.0815 | 0.0829
Epoch 272/300, Loss: 0.0813 | 0.0829
Epoch 273/300, Loss: 0.0811 | 0.0829
Epoch 274/300, Loss: 0.0819 | 0.0829
Epoch 275/300, Loss: 0.0813 | 0.0829
Early stopping
Runtime (seconds): 347.3323905467987
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1573.4024796634912
RMSE: 39.6661376953125
MAE: 39.6661376953125
R-squared: nan
[189.30386]
