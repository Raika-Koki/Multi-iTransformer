[32m[I 2025-01-07 03:53:46,541][0m A new study created in memory with name: no-name-5566b709-fa3e-42bc-9c5d-a8a008425de3[0m
[32m[I 2025-01-07 03:58:13,457][0m Trial 0 finished with value: 1.2937118943762664 and parameters: {'observation_period_num': 230, 'train_rates': 0.6212845633830677, 'learning_rate': 8.380470287703743e-06, 'batch_size': 248, 'step_size': 13, 'gamma': 0.9771368733094973}. Best is trial 0 with value: 1.2937118943762664.[0m
[32m[I 2025-01-07 04:03:49,337][0m Trial 1 finished with value: 0.701158763821592 and parameters: {'observation_period_num': 230, 'train_rates': 0.8579322436686347, 'learning_rate': 1.7287474743464277e-06, 'batch_size': 70, 'step_size': 12, 'gamma': 0.9410452914893429}. Best is trial 1 with value: 0.701158763821592.[0m
[32m[I 2025-01-07 04:08:49,755][0m Trial 2 finished with value: 0.8828528915438203 and parameters: {'observation_period_num': 247, 'train_rates': 0.6560951971065846, 'learning_rate': 0.0002141291268229251, 'batch_size': 185, 'step_size': 4, 'gamma': 0.8834363897339789}. Best is trial 1 with value: 0.701158763821592.[0m
[32m[I 2025-01-07 04:10:20,358][0m Trial 3 finished with value: 0.46195781519884394 and parameters: {'observation_period_num': 74, 'train_rates': 0.817872311248586, 'learning_rate': 4.6974132405389246e-05, 'batch_size': 201, 'step_size': 1, 'gamma': 0.9635762871918871}. Best is trial 3 with value: 0.46195781519884394.[0m
[32m[I 2025-01-07 04:16:00,565][0m Trial 4 finished with value: 0.13560821115970612 and parameters: {'observation_period_num': 219, 'train_rates': 0.9699404946298276, 'learning_rate': 0.0002609455382391932, 'batch_size': 145, 'step_size': 3, 'gamma': 0.8986562755737615}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:21:10,218][0m Trial 5 finished with value: 0.9125886048387551 and parameters: {'observation_period_num': 236, 'train_rates': 0.651776709312908, 'learning_rate': 2.682979162768204e-06, 'batch_size': 21, 'step_size': 12, 'gamma': 0.9875490016921087}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:24:35,546][0m Trial 6 finished with value: 0.6137356898152685 and parameters: {'observation_period_num': 147, 'train_rates': 0.8233763471191109, 'learning_rate': 2.158263967101314e-06, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9194862790072338}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:29:34,176][0m Trial 7 finished with value: 0.15749932825565338 and parameters: {'observation_period_num': 199, 'train_rates': 0.9310921431984007, 'learning_rate': 0.0001653378771860791, 'batch_size': 239, 'step_size': 9, 'gamma': 0.8474419813986334}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:30:14,895][0m Trial 8 finished with value: 0.598686230407369 and parameters: {'observation_period_num': 21, 'train_rates': 0.8108624666704495, 'learning_rate': 5.223890583342055e-06, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9687902423748718}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:31:32,380][0m Trial 9 finished with value: 0.19504249281216163 and parameters: {'observation_period_num': 58, 'train_rates': 0.9338565228740495, 'learning_rate': 1.4452735382348587e-05, 'batch_size': 128, 'step_size': 15, 'gamma': 0.9646433422824857}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:35:35,737][0m Trial 10 finished with value: 0.5097026228904724 and parameters: {'observation_period_num': 164, 'train_rates': 0.9813279416471927, 'learning_rate': 0.0008780285599257941, 'batch_size': 173, 'step_size': 2, 'gamma': 0.7535225233794689}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:40:09,228][0m Trial 11 finished with value: 0.1994740515947342 and parameters: {'observation_period_num': 191, 'train_rates': 0.9189573360172139, 'learning_rate': 0.0001064730285170321, 'batch_size': 241, 'step_size': 8, 'gamma': 0.8170545117104153}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:45:10,888][0m Trial 12 finished with value: 0.158499076962471 and parameters: {'observation_period_num': 194, 'train_rates': 0.9828849586286639, 'learning_rate': 0.00044084833915103747, 'batch_size': 87, 'step_size': 6, 'gamma': 0.8462534988671092}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:47:45,547][0m Trial 13 finished with value: 0.19214320691879108 and parameters: {'observation_period_num': 120, 'train_rates': 0.9012112533320852, 'learning_rate': 7.811131623355502e-05, 'batch_size': 158, 'step_size': 10, 'gamma': 0.8844133210783915}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:51:53,470][0m Trial 14 finished with value: 0.7542505807215625 and parameters: {'observation_period_num': 195, 'train_rates': 0.7445305491386182, 'learning_rate': 0.00021508916953899614, 'batch_size': 215, 'step_size': 5, 'gamma': 0.8103875457839883}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:54:14,711][0m Trial 15 finished with value: 1.0239574588200075 and parameters: {'observation_period_num': 124, 'train_rates': 0.7395651618200041, 'learning_rate': 3.3452233765326775e-05, 'batch_size': 153, 'step_size': 3, 'gamma': 0.8542401141366375}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 04:58:11,460][0m Trial 16 finished with value: 0.8242548948349657 and parameters: {'observation_period_num': 176, 'train_rates': 0.8670844627507864, 'learning_rate': 0.0009076877517136871, 'batch_size': 227, 'step_size': 9, 'gamma': 0.914208816137199}. Best is trial 4 with value: 0.13560821115970612.[0m
[32m[I 2025-01-07 05:00:28,182][0m Trial 17 finished with value: 0.12689753286080596 and parameters: {'observation_period_num': 100, 'train_rates': 0.9434936044685618, 'learning_rate': 0.0001836776412991194, 'batch_size': 91, 'step_size': 10, 'gamma': 0.8165682941137922}. Best is trial 17 with value: 0.12689753286080596.[0m
[32m[I 2025-01-07 05:02:45,577][0m Trial 18 finished with value: 0.1964188647379569 and parameters: {'observation_period_num': 99, 'train_rates': 0.9619430653348309, 'learning_rate': 0.00036088801847402544, 'batch_size': 92, 'step_size': 6, 'gamma': 0.7667289289141354}. Best is trial 17 with value: 0.12689753286080596.[0m
[32m[I 2025-01-07 05:03:54,358][0m Trial 19 finished with value: 0.1643763741149622 and parameters: {'observation_period_num': 10, 'train_rates': 0.8791535996426987, 'learning_rate': 5.793440735230897e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8048352297061375}. Best is trial 17 with value: 0.12689753286080596.[0m
[32m[I 2025-01-07 05:05:34,658][0m Trial 20 finished with value: 0.6847781886283616 and parameters: {'observation_period_num': 84, 'train_rates': 0.7690371277409229, 'learning_rate': 1.961249903775491e-05, 'batch_size': 117, 'step_size': 15, 'gamma': 0.7896239472382818}. Best is trial 17 with value: 0.12689753286080596.[0m
[32m[I 2025-01-07 05:11:01,998][0m Trial 21 finished with value: 0.11893511568285098 and parameters: {'observation_period_num': 210, 'train_rates': 0.9432536639459841, 'learning_rate': 0.00012304025910022553, 'batch_size': 51, 'step_size': 9, 'gamma': 0.8302750145575345}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:14:53,891][0m Trial 22 finished with value: 0.878966121278571 and parameters: {'observation_period_num': 154, 'train_rates': 0.9400273005413967, 'learning_rate': 0.00048483702065025137, 'batch_size': 49, 'step_size': 10, 'gamma': 0.8295318391624377}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:20:43,025][0m Trial 23 finished with value: 0.15607282519340515 and parameters: {'observation_period_num': 222, 'train_rates': 0.9890958204028203, 'learning_rate': 0.00013789068863899917, 'batch_size': 96, 'step_size': 8, 'gamma': 0.8753069850615836}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:22:59,191][0m Trial 24 finished with value: 0.19373369176509017 and parameters: {'observation_period_num': 104, 'train_rates': 0.8969443333831437, 'learning_rate': 0.00028793264671612246, 'batch_size': 146, 'step_size': 7, 'gamma': 0.7871999320005604}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:26:21,042][0m Trial 25 finished with value: 0.13007979714251183 and parameters: {'observation_period_num': 138, 'train_rates': 0.9456988527724511, 'learning_rate': 9.564639307040039e-05, 'batch_size': 74, 'step_size': 4, 'gamma': 0.9039445272491662}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:27:20,371][0m Trial 26 finished with value: 0.2621818666502965 and parameters: {'observation_period_num': 40, 'train_rates': 0.8453403591303107, 'learning_rate': 8.841678872264728e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.828248172825902}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:30:16,327][0m Trial 27 finished with value: 0.20344462085502546 and parameters: {'observation_period_num': 127, 'train_rates': 0.8969189901295487, 'learning_rate': 2.8422571815202257e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.8586724501206746}. Best is trial 21 with value: 0.11893511568285098.[0m
[32m[I 2025-01-07 05:33:42,874][0m Trial 28 finished with value: 0.11442398956843784 and parameters: {'observation_period_num': 104, 'train_rates': 0.9389143083867573, 'learning_rate': 5.18699692566647e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9372913390205712}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:37:25,117][0m Trial 29 finished with value: 0.14757596199789386 and parameters: {'observation_period_num': 106, 'train_rates': 0.9112202854376599, 'learning_rate': 1.2436866411312514e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9315724324988555}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:39:04,510][0m Trial 30 finished with value: 0.6955907235347244 and parameters: {'observation_period_num': 69, 'train_rates': 0.7018274909895027, 'learning_rate': 5.176109174958872e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.7841587192648669}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:42:37,670][0m Trial 31 finished with value: 0.12500106512683712 and parameters: {'observation_period_num': 144, 'train_rates': 0.9483999053302646, 'learning_rate': 0.00013841019724530998, 'batch_size': 57, 'step_size': 11, 'gamma': 0.9507724177461369}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:44:50,983][0m Trial 32 finished with value: 0.1558340282763465 and parameters: {'observation_period_num': 91, 'train_rates': 0.9590362840226159, 'learning_rate': 0.0001305429066597001, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9484878489072889}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:47:35,746][0m Trial 33 finished with value: 0.28722535635604235 and parameters: {'observation_period_num': 114, 'train_rates': 0.8502650308791612, 'learning_rate': 3.348764189109359e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9513474042247634}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:48:43,356][0m Trial 34 finished with value: 0.22673365505861826 and parameters: {'observation_period_num': 49, 'train_rates': 0.880790774049919, 'learning_rate': 7.307895514902921e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9339657748554315}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:52:49,308][0m Trial 35 finished with value: 0.12719795659301789 and parameters: {'observation_period_num': 169, 'train_rates': 0.9547519695319431, 'learning_rate': 0.00017657238658288439, 'batch_size': 104, 'step_size': 8, 'gamma': 0.8306839720822393}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:56:11,492][0m Trial 36 finished with value: 0.9600366275361244 and parameters: {'observation_period_num': 139, 'train_rates': 0.9170396101547157, 'learning_rate': 0.000521908506156372, 'batch_size': 49, 'step_size': 13, 'gamma': 0.9831998908690227}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 05:57:53,040][0m Trial 37 finished with value: 0.31539964875416776 and parameters: {'observation_period_num': 77, 'train_rates': 0.8343750608537875, 'learning_rate': 4.575819037346242e-05, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8932062505410786}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:02:47,493][0m Trial 38 finished with value: 1.0875650931856056 and parameters: {'observation_period_num': 242, 'train_rates': 0.6180537029098314, 'learning_rate': 0.00027029447740370226, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8673981267829558}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:08:43,118][0m Trial 39 finished with value: 0.17044377016524473 and parameters: {'observation_period_num': 213, 'train_rates': 0.9651860719062538, 'learning_rate': 0.00013506580452866788, 'batch_size': 23, 'step_size': 9, 'gamma': 0.9192810357620234}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:11:58,523][0m Trial 40 finished with value: 0.6809832161945822 and parameters: {'observation_period_num': 152, 'train_rates': 0.796129035460781, 'learning_rate': 7.516746609750572e-06, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8393151218521442}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:16:18,316][0m Trial 41 finished with value: 0.14211915545675852 and parameters: {'observation_period_num': 177, 'train_rates': 0.9476467334023643, 'learning_rate': 0.00018077629092096888, 'batch_size': 106, 'step_size': 8, 'gamma': 0.7990792508626545}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:20:21,223][0m Trial 42 finished with value: 0.1676932008053775 and parameters: {'observation_period_num': 170, 'train_rates': 0.923800327276306, 'learning_rate': 0.00021624043507762757, 'batch_size': 130, 'step_size': 8, 'gamma': 0.8251588501559711}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:25:36,551][0m Trial 43 finished with value: 0.1462128609418869 and parameters: {'observation_period_num': 206, 'train_rates': 0.9640950137544592, 'learning_rate': 6.433233340589525e-05, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8377841001550225}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:30:11,074][0m Trial 44 finished with value: 0.4340271395773975 and parameters: {'observation_period_num': 184, 'train_rates': 0.9415788656200229, 'learning_rate': 0.000651901699413137, 'batch_size': 57, 'step_size': 11, 'gamma': 0.8175868061623991}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:33:47,226][0m Trial 45 finished with value: 0.27471274304530613 and parameters: {'observation_period_num': 160, 'train_rates': 0.879035519537513, 'learning_rate': 0.00010730698982103151, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9757070299295172}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:37:02,728][0m Trial 46 finished with value: 0.17952227592468262 and parameters: {'observation_period_num': 132, 'train_rates': 0.9779080540487484, 'learning_rate': 0.00033467008966444776, 'batch_size': 83, 'step_size': 7, 'gamma': 0.9514818862466099}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:42:59,054][0m Trial 47 finished with value: 0.14633944980343994 and parameters: {'observation_period_num': 230, 'train_rates': 0.9283794204057085, 'learning_rate': 0.00018277782324279367, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8652365353104153}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:45:37,711][0m Trial 48 finished with value: 1.0230198544989 and parameters: {'observation_period_num': 144, 'train_rates': 0.641415806987907, 'learning_rate': 4.516727177352609e-05, 'batch_size': 108, 'step_size': 13, 'gamma': 0.7710038884419909}. Best is trial 28 with value: 0.11442398956843784.[0m
[32m[I 2025-01-07 06:47:50,925][0m Trial 49 finished with value: 0.10987493983150398 and parameters: {'observation_period_num': 94, 'train_rates': 0.9525070141633981, 'learning_rate': 0.00013744765772568478, 'batch_size': 61, 'step_size': 8, 'gamma': 0.8464984876242603}. Best is trial 49 with value: 0.10987493983150398.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7212 | 0.9364
Epoch 2/300, Loss: 0.7111 | 0.6437
Epoch 3/300, Loss: 0.5574 | 0.6625
Epoch 4/300, Loss: 0.5349 | 0.5255
Epoch 5/300, Loss: 0.4697 | 0.4412
Epoch 6/300, Loss: 0.4007 | 0.4924
Epoch 7/300, Loss: 0.3525 | 0.3905
Epoch 8/300, Loss: 0.3546 | 0.3662
Epoch 9/300, Loss: 0.3385 | 0.3363
Epoch 10/300, Loss: 0.3091 | 0.3066
Epoch 11/300, Loss: 0.3133 | 0.3439
Epoch 12/300, Loss: 0.3121 | 0.3104
Epoch 13/300, Loss: 0.3215 | 0.2900
Epoch 14/300, Loss: 0.3274 | 0.3442
Epoch 15/300, Loss: 0.3020 | 0.3190
Epoch 16/300, Loss: 0.2643 | 0.3070
Epoch 17/300, Loss: 0.2842 | 0.2987
Epoch 18/300, Loss: 0.2506 | 0.2713
Epoch 19/300, Loss: 0.2218 | 0.2668
Epoch 20/300, Loss: 0.2228 | 0.2570
Epoch 21/300, Loss: 0.2015 | 0.2458
Epoch 22/300, Loss: 0.1932 | 0.2439
Epoch 23/300, Loss: 0.1906 | 0.2363
Epoch 24/300, Loss: 0.1849 | 0.2321
Epoch 25/300, Loss: 0.1822 | 0.2294
Epoch 26/300, Loss: 0.1749 | 0.2254
Epoch 27/300, Loss: 0.1711 | 0.2214
Epoch 28/300, Loss: 0.1699 | 0.2205
Epoch 29/300, Loss: 0.1682 | 0.2145
Epoch 30/300, Loss: 0.1662 | 0.2143
Epoch 31/300, Loss: 0.1628 | 0.2104
Epoch 32/300, Loss: 0.1616 | 0.2086
Epoch 33/300, Loss: 0.1600 | 0.2054
Epoch 34/300, Loss: 0.1592 | 0.2040
Epoch 35/300, Loss: 0.1578 | 0.2015
Epoch 36/300, Loss: 0.1571 | 0.1989
Epoch 37/300, Loss: 0.1554 | 0.1971
Epoch 38/300, Loss: 0.1557 | 0.1961
Epoch 39/300, Loss: 0.1536 | 0.1938
Epoch 40/300, Loss: 0.1531 | 0.1916
Epoch 41/300, Loss: 0.1517 | 0.1895
Epoch 42/300, Loss: 0.1518 | 0.1881
Epoch 43/300, Loss: 0.1509 | 0.1868
Epoch 44/300, Loss: 0.1504 | 0.1858
Epoch 45/300, Loss: 0.1494 | 0.1834
Epoch 46/300, Loss: 0.1489 | 0.1822
Epoch 47/300, Loss: 0.1484 | 0.1809
Epoch 48/300, Loss: 0.1479 | 0.1794
Epoch 49/300, Loss: 0.1472 | 0.1786
Epoch 50/300, Loss: 0.1457 | 0.1768
Epoch 51/300, Loss: 0.1461 | 0.1761
Epoch 52/300, Loss: 0.1453 | 0.1753
Epoch 53/300, Loss: 0.1453 | 0.1739
Epoch 54/300, Loss: 0.1451 | 0.1732
Epoch 55/300, Loss: 0.1436 | 0.1711
Epoch 56/300, Loss: 0.1429 | 0.1700
Epoch 57/300, Loss: 0.1427 | 0.1696
Epoch 58/300, Loss: 0.1423 | 0.1685
Epoch 59/300, Loss: 0.1420 | 0.1673
Epoch 60/300, Loss: 0.1411 | 0.1665
Epoch 61/300, Loss: 0.1409 | 0.1660
Epoch 62/300, Loss: 0.1408 | 0.1649
Epoch 63/300, Loss: 0.1403 | 0.1643
Epoch 64/300, Loss: 0.1393 | 0.1636
Epoch 65/300, Loss: 0.1395 | 0.1627
Epoch 66/300, Loss: 0.1390 | 0.1618
Epoch 67/300, Loss: 0.1383 | 0.1606
Epoch 68/300, Loss: 0.1387 | 0.1605
Epoch 69/300, Loss: 0.1381 | 0.1597
Epoch 70/300, Loss: 0.1379 | 0.1594
Epoch 71/300, Loss: 0.1377 | 0.1588
Epoch 72/300, Loss: 0.1373 | 0.1579
Epoch 73/300, Loss: 0.1369 | 0.1576
Epoch 74/300, Loss: 0.1367 | 0.1569
Epoch 75/300, Loss: 0.1368 | 0.1564
Epoch 76/300, Loss: 0.1366 | 0.1559
Epoch 77/300, Loss: 0.1365 | 0.1556
Epoch 78/300, Loss: 0.1357 | 0.1550
Epoch 79/300, Loss: 0.1353 | 0.1545
Epoch 80/300, Loss: 0.1356 | 0.1543
Epoch 81/300, Loss: 0.1353 | 0.1538
Epoch 82/300, Loss: 0.1351 | 0.1535
Epoch 83/300, Loss: 0.1350 | 0.1533
Epoch 84/300, Loss: 0.1343 | 0.1527
Epoch 85/300, Loss: 0.1348 | 0.1524
Epoch 86/300, Loss: 0.1346 | 0.1523
Epoch 87/300, Loss: 0.1346 | 0.1519
Epoch 88/300, Loss: 0.1339 | 0.1517
Epoch 89/300, Loss: 0.1336 | 0.1510
Epoch 90/300, Loss: 0.1337 | 0.1508
Epoch 91/300, Loss: 0.1334 | 0.1506
Epoch 92/300, Loss: 0.1331 | 0.1502
Epoch 93/300, Loss: 0.1334 | 0.1497
Epoch 94/300, Loss: 0.1334 | 0.1496
Epoch 95/300, Loss: 0.1335 | 0.1494
Epoch 96/300, Loss: 0.1327 | 0.1492
Epoch 97/300, Loss: 0.1320 | 0.1489
Epoch 98/300, Loss: 0.1328 | 0.1487
Epoch 99/300, Loss: 0.1322 | 0.1484
Epoch 100/300, Loss: 0.1330 | 0.1483
Epoch 101/300, Loss: 0.1324 | 0.1480
Epoch 102/300, Loss: 0.1320 | 0.1479
Epoch 103/300, Loss: 0.1319 | 0.1476
Epoch 104/300, Loss: 0.1325 | 0.1476
Epoch 105/300, Loss: 0.1322 | 0.1475
Epoch 106/300, Loss: 0.1317 | 0.1473
Epoch 107/300, Loss: 0.1317 | 0.1471
Epoch 108/300, Loss: 0.1314 | 0.1467
Epoch 109/300, Loss: 0.1318 | 0.1464
Epoch 110/300, Loss: 0.1319 | 0.1462
Epoch 111/300, Loss: 0.1314 | 0.1462
Epoch 112/300, Loss: 0.1315 | 0.1460
Epoch 113/300, Loss: 0.1313 | 0.1458
Epoch 114/300, Loss: 0.1311 | 0.1457
Epoch 115/300, Loss: 0.1314 | 0.1456
Epoch 116/300, Loss: 0.1310 | 0.1455
Epoch 117/300, Loss: 0.1316 | 0.1455
Epoch 118/300, Loss: 0.1311 | 0.1454
Epoch 119/300, Loss: 0.1306 | 0.1453
Epoch 120/300, Loss: 0.1309 | 0.1453
Epoch 121/300, Loss: 0.1308 | 0.1451
Epoch 122/300, Loss: 0.1306 | 0.1450
Epoch 123/300, Loss: 0.1306 | 0.1450
Epoch 124/300, Loss: 0.1308 | 0.1450
Epoch 125/300, Loss: 0.1307 | 0.1449
Epoch 126/300, Loss: 0.1308 | 0.1447
Epoch 127/300, Loss: 0.1304 | 0.1445
Epoch 128/300, Loss: 0.1305 | 0.1445
Epoch 129/300, Loss: 0.1306 | 0.1444
Epoch 130/300, Loss: 0.1307 | 0.1444
Epoch 131/300, Loss: 0.1309 | 0.1444
Epoch 132/300, Loss: 0.1310 | 0.1443
Epoch 133/300, Loss: 0.1302 | 0.1443
Epoch 134/300, Loss: 0.1300 | 0.1442
Epoch 135/300, Loss: 0.1307 | 0.1441
Epoch 136/300, Loss: 0.1301 | 0.1440
Epoch 137/300, Loss: 0.1299 | 0.1439
Epoch 138/300, Loss: 0.1304 | 0.1439
Epoch 139/300, Loss: 0.1300 | 0.1438
Epoch 140/300, Loss: 0.1308 | 0.1437
Epoch 141/300, Loss: 0.1304 | 0.1437
Epoch 142/300, Loss: 0.1306 | 0.1437
Epoch 143/300, Loss: 0.1301 | 0.1436
Epoch 144/300, Loss: 0.1297 | 0.1436
Epoch 145/300, Loss: 0.1310 | 0.1436
Epoch 146/300, Loss: 0.1305 | 0.1435
Epoch 147/300, Loss: 0.1299 | 0.1435
Epoch 148/300, Loss: 0.1303 | 0.1434
Epoch 149/300, Loss: 0.1298 | 0.1434
Epoch 150/300, Loss: 0.1298 | 0.1434
Epoch 151/300, Loss: 0.1299 | 0.1433
Epoch 152/300, Loss: 0.1300 | 0.1433
Epoch 153/300, Loss: 0.1305 | 0.1433
Epoch 154/300, Loss: 0.1299 | 0.1433
Epoch 155/300, Loss: 0.1304 | 0.1433
Epoch 156/300, Loss: 0.1297 | 0.1432
Epoch 157/300, Loss: 0.1303 | 0.1433
Epoch 158/300, Loss: 0.1302 | 0.1432
Epoch 159/300, Loss: 0.1300 | 0.1432
Epoch 160/300, Loss: 0.1302 | 0.1432
Epoch 161/300, Loss: 0.1293 | 0.1432
Epoch 162/300, Loss: 0.1299 | 0.1432
Epoch 163/300, Loss: 0.1296 | 0.1432
Epoch 164/300, Loss: 0.1304 | 0.1431
Epoch 165/300, Loss: 0.1302 | 0.1431
Epoch 166/300, Loss: 0.1294 | 0.1431
Epoch 167/300, Loss: 0.1300 | 0.1431
Epoch 168/300, Loss: 0.1298 | 0.1431
Epoch 169/300, Loss: 0.1299 | 0.1431
Epoch 170/300, Loss: 0.1298 | 0.1431
Epoch 171/300, Loss: 0.1300 | 0.1430
Epoch 172/300, Loss: 0.1301 | 0.1430
Epoch 173/300, Loss: 0.1298 | 0.1430
Epoch 174/300, Loss: 0.1298 | 0.1430
Epoch 175/300, Loss: 0.1295 | 0.1430
Epoch 176/300, Loss: 0.1297 | 0.1430
Epoch 177/300, Loss: 0.1297 | 0.1430
Epoch 178/300, Loss: 0.1304 | 0.1430
Epoch 179/300, Loss: 0.1298 | 0.1430
Epoch 180/300, Loss: 0.1297 | 0.1430
Epoch 181/300, Loss: 0.1302 | 0.1430
Epoch 182/300, Loss: 0.1300 | 0.1429
Epoch 183/300, Loss: 0.1299 | 0.1429
Epoch 184/300, Loss: 0.1298 | 0.1429
Epoch 185/300, Loss: 0.1299 | 0.1429
Epoch 186/300, Loss: 0.1296 | 0.1429
Epoch 187/300, Loss: 0.1298 | 0.1429
Epoch 188/300, Loss: 0.1295 | 0.1429
Epoch 189/300, Loss: 0.1297 | 0.1429
Epoch 190/300, Loss: 0.1301 | 0.1429
Epoch 191/300, Loss: 0.1304 | 0.1429
Epoch 192/300, Loss: 0.1300 | 0.1429
Epoch 193/300, Loss: 0.1298 | 0.1429
Epoch 194/300, Loss: 0.1294 | 0.1429
Epoch 195/300, Loss: 0.1301 | 0.1429
Epoch 196/300, Loss: 0.1295 | 0.1429
Epoch 197/300, Loss: 0.1291 | 0.1429
Epoch 198/300, Loss: 0.1301 | 0.1429
Epoch 199/300, Loss: 0.1295 | 0.1428
Epoch 200/300, Loss: 0.1300 | 0.1428
Epoch 201/300, Loss: 0.1296 | 0.1428
Epoch 202/300, Loss: 0.1298 | 0.1428
Epoch 203/300, Loss: 0.1293 | 0.1428
Epoch 204/300, Loss: 0.1293 | 0.1428
Epoch 205/300, Loss: 0.1296 | 0.1428
Epoch 206/300, Loss: 0.1298 | 0.1428
Epoch 207/300, Loss: 0.1297 | 0.1428
Epoch 208/300, Loss: 0.1293 | 0.1428
Epoch 209/300, Loss: 0.1292 | 0.1428
Epoch 210/300, Loss: 0.1299 | 0.1428
Epoch 211/300, Loss: 0.1298 | 0.1428
Epoch 212/300, Loss: 0.1299 | 0.1428
Epoch 213/300, Loss: 0.1302 | 0.1428
Epoch 214/300, Loss: 0.1299 | 0.1428
Epoch 215/300, Loss: 0.1303 | 0.1428
Epoch 216/300, Loss: 0.1304 | 0.1428
Epoch 217/300, Loss: 0.1292 | 0.1428
Epoch 218/300, Loss: 0.1302 | 0.1428
Epoch 219/300, Loss: 0.1296 | 0.1428
Epoch 220/300, Loss: 0.1303 | 0.1428
Epoch 221/300, Loss: 0.1297 | 0.1428
Epoch 222/300, Loss: 0.1298 | 0.1428
Epoch 223/300, Loss: 0.1293 | 0.1428
Epoch 224/300, Loss: 0.1299 | 0.1428
Epoch 225/300, Loss: 0.1299 | 0.1428
Epoch 226/300, Loss: 0.1306 | 0.1428
Epoch 227/300, Loss: 0.1300 | 0.1428
Epoch 228/300, Loss: 0.1291 | 0.1428
Epoch 229/300, Loss: 0.1301 | 0.1428
Epoch 230/300, Loss: 0.1294 | 0.1428
Epoch 231/300, Loss: 0.1304 | 0.1428
Epoch 232/300, Loss: 0.1303 | 0.1428
Epoch 233/300, Loss: 0.1295 | 0.1428
Epoch 234/300, Loss: 0.1296 | 0.1428
Epoch 235/300, Loss: 0.1302 | 0.1428
Epoch 236/300, Loss: 0.1292 | 0.1428
Epoch 237/300, Loss: 0.1302 | 0.1428
Epoch 238/300, Loss: 0.1298 | 0.1428
Epoch 239/300, Loss: 0.1292 | 0.1428
Epoch 240/300, Loss: 0.1300 | 0.1428
Epoch 241/300, Loss: 0.1305 | 0.1428
Epoch 242/300, Loss: 0.1296 | 0.1428
Epoch 243/300, Loss: 0.1290 | 0.1428
Epoch 244/300, Loss: 0.1298 | 0.1428
Epoch 245/300, Loss: 0.1297 | 0.1428
Epoch 246/300, Loss: 0.1293 | 0.1428
Epoch 247/300, Loss: 0.1298 | 0.1428
Epoch 248/300, Loss: 0.1301 | 0.1428
Epoch 249/300, Loss: 0.1300 | 0.1428
Epoch 250/300, Loss: 0.1294 | 0.1428
Epoch 251/300, Loss: 0.1294 | 0.1428
Epoch 252/300, Loss: 0.1290 | 0.1428
Epoch 253/300, Loss: 0.1299 | 0.1428
Epoch 254/300, Loss: 0.1294 | 0.1428
Epoch 255/300, Loss: 0.1300 | 0.1428
Epoch 256/300, Loss: 0.1295 | 0.1428
Epoch 257/300, Loss: 0.1297 | 0.1428
Epoch 258/300, Loss: 0.1295 | 0.1428
Epoch 259/300, Loss: 0.1299 | 0.1428
Epoch 260/300, Loss: 0.1297 | 0.1428
Epoch 261/300, Loss: 0.1297 | 0.1428
Epoch 262/300, Loss: 0.1298 | 0.1428
Epoch 263/300, Loss: 0.1296 | 0.1428
Epoch 264/300, Loss: 0.1296 | 0.1428
Epoch 265/300, Loss: 0.1299 | 0.1428
Epoch 266/300, Loss: 0.1291 | 0.1428
Epoch 267/300, Loss: 0.1295 | 0.1428
Epoch 268/300, Loss: 0.1300 | 0.1428
Epoch 269/300, Loss: 0.1297 | 0.1428
Epoch 270/300, Loss: 0.1296 | 0.1428
Epoch 271/300, Loss: 0.1297 | 0.1428
Epoch 272/300, Loss: 0.1294 | 0.1428
Epoch 273/300, Loss: 0.1291 | 0.1428
Epoch 274/300, Loss: 0.1294 | 0.1428
Epoch 275/300, Loss: 0.1294 | 0.1428
Epoch 276/300, Loss: 0.1300 | 0.1428
Epoch 277/300, Loss: 0.1297 | 0.1428
Epoch 278/300, Loss: 0.1295 | 0.1428
Epoch 279/300, Loss: 0.1298 | 0.1428
Epoch 280/300, Loss: 0.1294 | 0.1428
Epoch 281/300, Loss: 0.1296 | 0.1428
Epoch 282/300, Loss: 0.1301 | 0.1428
Epoch 283/300, Loss: 0.1296 | 0.1428
Epoch 284/300, Loss: 0.1303 | 0.1428
Epoch 285/300, Loss: 0.1293 | 0.1428
Epoch 286/300, Loss: 0.1303 | 0.1428
Epoch 287/300, Loss: 0.1298 | 0.1428
Epoch 288/300, Loss: 0.1299 | 0.1428
Epoch 289/300, Loss: 0.1300 | 0.1428
Epoch 290/300, Loss: 0.1293 | 0.1428
Epoch 291/300, Loss: 0.1297 | 0.1428
Epoch 292/300, Loss: 0.1295 | 0.1428
Epoch 293/300, Loss: 0.1294 | 0.1428
Epoch 294/300, Loss: 0.1297 | 0.1428
Epoch 295/300, Loss: 0.1300 | 0.1428
Epoch 296/300, Loss: 0.1299 | 0.1428
Epoch 297/300, Loss: 0.1296 | 0.1428
Epoch 298/300, Loss: 0.1296 | 0.1428
Epoch 299/300, Loss: 0.1295 | 0.1428
Epoch 300/300, Loss: 0.1304 | 0.1428
Runtime (seconds): 396.65826511383057
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1473.8482890243176
RMSE: 38.39073181152344
MAE: 38.39073181152344
R-squared: nan
[186.64926]
