[32m[I 2025-01-06 07:45:10,661][0m A new study created in memory with name: no-name-c11e0950-730d-437d-89a2-5115a92f084b[0m
[32m[I 2025-01-06 07:46:03,937][0m Trial 0 finished with value: 0.6748642443727564 and parameters: {'observation_period_num': 47, 'train_rates': 0.76801679481451, 'learning_rate': 4.220153878953307e-05, 'batch_size': 187, 'step_size': 15, 'gamma': 0.9460225896725148}. Best is trial 0 with value: 0.6748642443727564.[0m
[32m[I 2025-01-06 07:48:49,426][0m Trial 1 finished with value: 1.2226344632341506 and parameters: {'observation_period_num': 130, 'train_rates': 0.8003274387619105, 'learning_rate': 0.000785531951708731, 'batch_size': 87, 'step_size': 8, 'gamma': 0.7896555890952486}. Best is trial 0 with value: 0.6748642443727564.[0m
[32m[I 2025-01-06 07:50:30,634][0m Trial 2 finished with value: 0.289035673234977 and parameters: {'observation_period_num': 80, 'train_rates': 0.9112999994803048, 'learning_rate': 1.6503601164033736e-05, 'batch_size': 250, 'step_size': 6, 'gamma': 0.962218561751005}. Best is trial 2 with value: 0.289035673234977.[0m
[32m[I 2025-01-06 07:54:05,028][0m Trial 3 finished with value: 0.2771900029200588 and parameters: {'observation_period_num': 143, 'train_rates': 0.9301090053342138, 'learning_rate': 2.398987833979636e-06, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9790808089909236}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 07:59:06,312][0m Trial 4 finished with value: 0.8378857430070639 and parameters: {'observation_period_num': 249, 'train_rates': 0.6660617215751913, 'learning_rate': 0.00048419859652256206, 'batch_size': 113, 'step_size': 9, 'gamma': 0.7840199544471473}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:00:49,383][0m Trial 5 finished with value: 1.0083612203598022 and parameters: {'observation_period_num': 77, 'train_rates': 0.9460403684170906, 'learning_rate': 1.5795158142658145e-06, 'batch_size': 241, 'step_size': 14, 'gamma': 0.7774431575907226}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:02:18,644][0m Trial 6 finished with value: 1.6434926825388037 and parameters: {'observation_period_num': 55, 'train_rates': 0.6866876993426725, 'learning_rate': 1.793067537769386e-06, 'batch_size': 42, 'step_size': 2, 'gamma': 0.8334747445518169}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:03:26,410][0m Trial 7 finished with value: 0.7893339576186315 and parameters: {'observation_period_num': 63, 'train_rates': 0.7102382339359563, 'learning_rate': 3.656269659199996e-05, 'batch_size': 180, 'step_size': 5, 'gamma': 0.9309176128617378}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:05:56,847][0m Trial 8 finished with value: 0.8914505596195317 and parameters: {'observation_period_num': 112, 'train_rates': 0.9027035205584633, 'learning_rate': 1.826946596051999e-06, 'batch_size': 58, 'step_size': 5, 'gamma': 0.7622887184016425}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:06:41,761][0m Trial 9 finished with value: 0.6325511872513682 and parameters: {'observation_period_num': 38, 'train_rates': 0.7595537960629738, 'learning_rate': 0.00015225453007209459, 'batch_size': 209, 'step_size': 4, 'gamma': 0.8331242726151598}. Best is trial 3 with value: 0.2771900029200588.[0m
[32m[I 2025-01-06 08:12:07,483][0m Trial 10 finished with value: 0.1327658681308522 and parameters: {'observation_period_num': 188, 'train_rates': 0.9880624992285393, 'learning_rate': 6.7786916629955225e-06, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9041579156726827}. Best is trial 10 with value: 0.1327658681308522.[0m
[32m[I 2025-01-06 08:17:13,796][0m Trial 11 finished with value: 0.13193116911820002 and parameters: {'observation_period_num': 176, 'train_rates': 0.9875248245583617, 'learning_rate': 7.908764238062089e-06, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9063390364560685}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:22:36,804][0m Trial 12 finished with value: 0.15037263593366068 and parameters: {'observation_period_num': 191, 'train_rates': 0.9889757913526535, 'learning_rate': 6.315405621232633e-06, 'batch_size': 21, 'step_size': 11, 'gamma': 0.9113997765862646}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:26:57,267][0m Trial 13 finished with value: 0.4621418443799929 and parameters: {'observation_period_num': 190, 'train_rates': 0.8580272265857813, 'learning_rate': 9.94324927934768e-06, 'batch_size': 73, 'step_size': 11, 'gamma': 0.8917365263325641}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:31:28,717][0m Trial 14 finished with value: 0.5568739209146727 and parameters: {'observation_period_num': 194, 'train_rates': 0.8481863831588462, 'learning_rate': 6.051915797172958e-06, 'batch_size': 137, 'step_size': 13, 'gamma': 0.8689477888674849}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:36:24,412][0m Trial 15 finished with value: 0.9639859274655831 and parameters: {'observation_period_num': 239, 'train_rates': 0.6056515339947479, 'learning_rate': 8.569381466170995e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8591056744681902}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:37:06,731][0m Trial 16 finished with value: 0.1635769009590149 and parameters: {'observation_period_num': 5, 'train_rates': 0.9857798098932277, 'learning_rate': 1.6519663466082386e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9044109332817372}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:40:44,463][0m Trial 17 finished with value: 0.43601039787341034 and parameters: {'observation_period_num': 160, 'train_rates': 0.8632728415091023, 'learning_rate': 4.206622778163284e-06, 'batch_size': 61, 'step_size': 15, 'gamma': 0.9282495848810304}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:46:23,012][0m Trial 18 finished with value: 0.22720496356487274 and parameters: {'observation_period_num': 219, 'train_rates': 0.9553311188710488, 'learning_rate': 1.309886928968417e-05, 'batch_size': 143, 'step_size': 13, 'gamma': 0.8423794185750623}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:50:32,745][0m Trial 19 finished with value: 0.7113665719189959 and parameters: {'observation_period_num': 163, 'train_rates': 0.8213433181815554, 'learning_rate': 1.048613114531946e-06, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8899180364237759}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:52:59,803][0m Trial 20 finished with value: 0.32619203741733843 and parameters: {'observation_period_num': 106, 'train_rates': 0.8906683927706736, 'learning_rate': 3.523964543860032e-06, 'batch_size': 48, 'step_size': 7, 'gamma': 0.9816288062805256}. Best is trial 11 with value: 0.13193116911820002.[0m
[32m[I 2025-01-06 08:58:42,573][0m Trial 21 finished with value: 0.12363243309987916 and parameters: {'observation_period_num': 194, 'train_rates': 0.9871459815245689, 'learning_rate': 8.312892728279732e-06, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9111796060424485}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:04:13,970][0m Trial 22 finished with value: 0.21017350835128895 and parameters: {'observation_period_num': 214, 'train_rates': 0.9626710552342944, 'learning_rate': 8.394795593222817e-06, 'batch_size': 77, 'step_size': 10, 'gamma': 0.9262545176278393}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:08:40,758][0m Trial 23 finished with value: 0.13748511672019958 and parameters: {'observation_period_num': 171, 'train_rates': 0.9889318708413262, 'learning_rate': 2.0169423687686175e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8832166400965396}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:14:35,467][0m Trial 24 finished with value: 0.17088112074841735 and parameters: {'observation_period_num': 216, 'train_rates': 0.9322729903548141, 'learning_rate': 4.16436300110263e-06, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9541414901520127}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:18:40,538][0m Trial 25 finished with value: 0.2087064606279231 and parameters: {'observation_period_num': 177, 'train_rates': 0.8889898765423685, 'learning_rate': 5.881855513069673e-05, 'batch_size': 113, 'step_size': 10, 'gamma': 0.9104771701423249}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:22:16,709][0m Trial 26 finished with value: 0.15695922262966633 and parameters: {'observation_period_num': 146, 'train_rates': 0.9602592459745214, 'learning_rate': 2.529110874052516e-05, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8512028530507365}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:27:33,100][0m Trial 27 finished with value: 0.1925860849196601 and parameters: {'observation_period_num': 203, 'train_rates': 0.9192906738924541, 'learning_rate': 9.946394880117455e-06, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8707214031097201}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:33:39,604][0m Trial 28 finished with value: 0.4213548233466489 and parameters: {'observation_period_num': 234, 'train_rates': 0.958859006426167, 'learning_rate': 3.360300877389388e-06, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9415002573405682}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:36:35,486][0m Trial 29 finished with value: 0.8465507519672592 and parameters: {'observation_period_num': 148, 'train_rates': 0.7318860137654101, 'learning_rate': 3.01370097115421e-05, 'batch_size': 184, 'step_size': 15, 'gamma': 0.901613770579906}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:40:35,004][0m Trial 30 finished with value: 0.7944560538638722 and parameters: {'observation_period_num': 182, 'train_rates': 0.8254418704193278, 'learning_rate': 6.024848012059062e-06, 'batch_size': 159, 'step_size': 14, 'gamma': 0.87615677078491}. Best is trial 21 with value: 0.12363243309987916.[0m
[32m[I 2025-01-06 09:44:54,462][0m Trial 31 finished with value: 0.12055345609784127 and parameters: {'observation_period_num': 167, 'train_rates': 0.982102439480644, 'learning_rate': 1.725368087844175e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8835626401735607}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 09:50:23,245][0m Trial 32 finished with value: 0.14312532258794664 and parameters: {'observation_period_num': 203, 'train_rates': 0.9659362250555495, 'learning_rate': 1.170871530386465e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.9230113659929592}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 09:53:08,381][0m Trial 33 finished with value: 0.1314664117751583 and parameters: {'observation_period_num': 118, 'train_rates': 0.9345542010999358, 'learning_rate': 6.106238878210852e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8946059827620408}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 09:56:17,574][0m Trial 34 finished with value: 0.13603668655809117 and parameters: {'observation_period_num': 129, 'train_rates': 0.9360733436463816, 'learning_rate': 0.00014685155113511805, 'batch_size': 52, 'step_size': 9, 'gamma': 0.9428459605082182}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 09:58:45,729][0m Trial 35 finished with value: 0.1906683464933719 and parameters: {'observation_period_num': 110, 'train_rates': 0.9212532816148186, 'learning_rate': 5.188331371387093e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8156179967130847}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 10:02:04,413][0m Trial 36 finished with value: 0.17258399333272662 and parameters: {'observation_period_num': 134, 'train_rates': 0.8758986528662118, 'learning_rate': 9.484695171517468e-05, 'batch_size': 32, 'step_size': 8, 'gamma': 0.8082935101044946}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 10:04:10,112][0m Trial 37 finished with value: 0.14511330854036136 and parameters: {'observation_period_num': 89, 'train_rates': 0.9417319034048136, 'learning_rate': 2.1832311828256396e-05, 'batch_size': 70, 'step_size': 10, 'gamma': 0.9688202758182}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 10:08:12,914][0m Trial 38 finished with value: 0.4582848459123129 and parameters: {'observation_period_num': 157, 'train_rates': 0.9703777884950293, 'learning_rate': 0.0004447625952101103, 'batch_size': 51, 'step_size': 11, 'gamma': 0.8904005168097202}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 10:10:57,691][0m Trial 39 finished with value: 0.34194477551123675 and parameters: {'observation_period_num': 124, 'train_rates': 0.9099278300671342, 'learning_rate': 4.3757885740040416e-05, 'batch_size': 117, 'step_size': 1, 'gamma': 0.9142184700502316}. Best is trial 31 with value: 0.12055345609784127.[0m
[32m[I 2025-01-06 10:15:22,220][0m Trial 40 finished with value: 0.09176577840532575 and parameters: {'observation_period_num': 169, 'train_rates': 0.9750054816190366, 'learning_rate': 8.311833041485514e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8566811865061682}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:19:45,286][0m Trial 41 finished with value: 0.10980982997932949 and parameters: {'observation_period_num': 170, 'train_rates': 0.9735398549882011, 'learning_rate': 7.549640671046055e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8598008286863084}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:23:12,479][0m Trial 42 finished with value: 0.1082095425979347 and parameters: {'observation_period_num': 140, 'train_rates': 0.944393979670098, 'learning_rate': 0.00016800146814879705, 'batch_size': 47, 'step_size': 8, 'gamma': 0.8551574935352363}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:27:28,997][0m Trial 43 finished with value: 0.12333467054751611 and parameters: {'observation_period_num': 167, 'train_rates': 0.9446389834213114, 'learning_rate': 0.000247989295038129, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8572443013754476}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:30:58,214][0m Trial 44 finished with value: 0.19669300087278696 and parameters: {'observation_period_num': 140, 'train_rates': 0.901489206113443, 'learning_rate': 0.000272369973100051, 'batch_size': 33, 'step_size': 6, 'gamma': 0.8280282709068303}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:34:29,547][0m Trial 45 finished with value: 0.5575173151985503 and parameters: {'observation_period_num': 166, 'train_rates': 0.791825000505673, 'learning_rate': 0.00025055126253061406, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8616045955161677}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:38:09,011][0m Trial 46 finished with value: 0.5342240929603577 and parameters: {'observation_period_num': 154, 'train_rates': 0.9451793275405045, 'learning_rate': 0.0009734365791073542, 'batch_size': 220, 'step_size': 4, 'gamma': 0.8478639159647389}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:42:26,295][0m Trial 47 finished with value: 0.10828497601144108 and parameters: {'observation_period_num': 173, 'train_rates': 0.9708877571186246, 'learning_rate': 0.00013893012618285946, 'batch_size': 64, 'step_size': 8, 'gamma': 0.8017786147083601}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:46:57,042][0m Trial 48 finished with value: 0.1278578070657594 and parameters: {'observation_period_num': 178, 'train_rates': 0.9748809009073781, 'learning_rate': 0.00014015570492849225, 'batch_size': 66, 'step_size': 6, 'gamma': 0.798452013740115}. Best is trial 40 with value: 0.09176577840532575.[0m
[32m[I 2025-01-06 10:49:14,167][0m Trial 49 finished with value: 0.14853689074516296 and parameters: {'observation_period_num': 99, 'train_rates': 0.9723512709661719, 'learning_rate': 9.314941686674209e-05, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7509228868996785}. Best is trial 40 with value: 0.09176577840532575.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.6465 | 0.6745
Epoch 2/300, Loss: 0.5745 | 0.6639
Epoch 3/300, Loss: 0.4189 | 0.4922
Epoch 4/300, Loss: 0.3724 | 0.3692
Epoch 5/300, Loss: 0.3598 | 0.3800
Epoch 6/300, Loss: 0.3072 | 0.3401
Epoch 7/300, Loss: 0.3067 | 0.2938
Epoch 8/300, Loss: 0.2723 | 0.2740
Epoch 9/300, Loss: 0.2822 | 0.3087
Epoch 10/300, Loss: 0.2502 | 0.2699
Epoch 11/300, Loss: 0.2272 | 0.2302
Epoch 12/300, Loss: 0.2130 | 0.3043
Epoch 13/300, Loss: 0.2171 | 0.2599
Epoch 14/300, Loss: 0.2139 | 0.2165
Epoch 15/300, Loss: 0.2185 | 0.2392
Epoch 16/300, Loss: 0.2113 | 0.2267
Epoch 17/300, Loss: 0.1958 | 0.2220
Epoch 18/300, Loss: 0.1937 | 0.2188
Epoch 19/300, Loss: 0.2009 | 0.1990
Epoch 20/300, Loss: 0.1958 | 0.2050
Epoch 21/300, Loss: 0.1955 | 0.2006
Epoch 22/300, Loss: 0.2015 | 0.1938
Epoch 23/300, Loss: 0.1936 | 0.1918
Epoch 24/300, Loss: 0.1953 | 0.1898
Epoch 25/300, Loss: 0.1774 | 0.1735
Epoch 26/300, Loss: 0.1660 | 0.1846
Epoch 27/300, Loss: 0.1648 | 0.1840
Epoch 28/300, Loss: 0.1594 | 0.1685
Epoch 29/300, Loss: 0.1568 | 0.1608
Epoch 30/300, Loss: 0.1556 | 0.1590
Epoch 31/300, Loss: 0.1509 | 0.1591
Epoch 32/300, Loss: 0.1496 | 0.1548
Epoch 33/300, Loss: 0.1488 | 0.1496
Epoch 34/300, Loss: 0.1466 | 0.1477
Epoch 35/300, Loss: 0.1452 | 0.1494
Epoch 36/300, Loss: 0.1451 | 0.1456
Epoch 37/300, Loss: 0.1430 | 0.1394
Epoch 38/300, Loss: 0.1429 | 0.1469
Epoch 39/300, Loss: 0.1415 | 0.1394
Epoch 40/300, Loss: 0.1415 | 0.1380
Epoch 41/300, Loss: 0.1400 | 0.1407
Epoch 42/300, Loss: 0.1410 | 0.1366
Epoch 43/300, Loss: 0.1392 | 0.1362
Epoch 44/300, Loss: 0.1366 | 0.1308
Epoch 45/300, Loss: 0.1369 | 0.1320
Epoch 46/300, Loss: 0.1349 | 0.1290
Epoch 47/300, Loss: 0.1333 | 0.1281
Epoch 48/300, Loss: 0.1328 | 0.1253
Epoch 49/300, Loss: 0.1322 | 0.1254
Epoch 50/300, Loss: 0.1320 | 0.1304
Epoch 51/300, Loss: 0.1313 | 0.1220
Epoch 52/300, Loss: 0.1301 | 0.1227
Epoch 53/300, Loss: 0.1300 | 0.1298
Epoch 54/300, Loss: 0.1293 | 0.1192
Epoch 55/300, Loss: 0.1282 | 0.1186
Epoch 56/300, Loss: 0.1280 | 0.1232
Epoch 57/300, Loss: 0.1264 | 0.1184
Epoch 58/300, Loss: 0.1265 | 0.1159
Epoch 59/300, Loss: 0.1259 | 0.1196
Epoch 60/300, Loss: 0.1252 | 0.1202
Epoch 61/300, Loss: 0.1247 | 0.1140
Epoch 62/300, Loss: 0.1235 | 0.1169
Epoch 63/300, Loss: 0.1237 | 0.1189
Epoch 64/300, Loss: 0.1226 | 0.1135
Epoch 65/300, Loss: 0.1219 | 0.1127
Epoch 66/300, Loss: 0.1218 | 0.1167
Epoch 67/300, Loss: 0.1220 | 0.1143
Epoch 68/300, Loss: 0.1210 | 0.1120
Epoch 69/300, Loss: 0.1206 | 0.1131
Epoch 70/300, Loss: 0.1200 | 0.1121
Epoch 71/300, Loss: 0.1201 | 0.1108
Epoch 72/300, Loss: 0.1193 | 0.1107
Epoch 73/300, Loss: 0.1197 | 0.1114
Epoch 74/300, Loss: 0.1198 | 0.1098
Epoch 75/300, Loss: 0.1190 | 0.1099
Epoch 76/300, Loss: 0.1188 | 0.1098
Epoch 77/300, Loss: 0.1182 | 0.1097
Epoch 78/300, Loss: 0.1180 | 0.1092
Epoch 79/300, Loss: 0.1179 | 0.1090
Epoch 80/300, Loss: 0.1178 | 0.1084
Epoch 81/300, Loss: 0.1167 | 0.1086
Epoch 82/300, Loss: 0.1171 | 0.1084
Epoch 83/300, Loss: 0.1164 | 0.1080
Epoch 84/300, Loss: 0.1167 | 0.1077
Epoch 85/300, Loss: 0.1166 | 0.1079
Epoch 86/300, Loss: 0.1163 | 0.1075
Epoch 87/300, Loss: 0.1154 | 0.1072
Epoch 88/300, Loss: 0.1158 | 0.1069
Epoch 89/300, Loss: 0.1151 | 0.1071
Epoch 90/300, Loss: 0.1149 | 0.1070
Epoch 91/300, Loss: 0.1154 | 0.1064
Epoch 92/300, Loss: 0.1153 | 0.1065
Epoch 93/300, Loss: 0.1150 | 0.1065
Epoch 94/300, Loss: 0.1145 | 0.1065
Epoch 95/300, Loss: 0.1145 | 0.1060
Epoch 96/300, Loss: 0.1144 | 0.1061
Epoch 97/300, Loss: 0.1147 | 0.1057
Epoch 98/300, Loss: 0.1140 | 0.1059
Epoch 99/300, Loss: 0.1141 | 0.1058
Epoch 100/300, Loss: 0.1147 | 0.1055
Epoch 101/300, Loss: 0.1135 | 0.1056
Epoch 102/300, Loss: 0.1136 | 0.1058
Epoch 103/300, Loss: 0.1135 | 0.1050
Epoch 104/300, Loss: 0.1137 | 0.1053
Epoch 105/300, Loss: 0.1134 | 0.1047
Epoch 106/300, Loss: 0.1130 | 0.1050
Epoch 107/300, Loss: 0.1129 | 0.1052
Epoch 108/300, Loss: 0.1128 | 0.1051
Epoch 109/300, Loss: 0.1134 | 0.1048
Epoch 110/300, Loss: 0.1130 | 0.1049
Epoch 111/300, Loss: 0.1122 | 0.1051
Epoch 112/300, Loss: 0.1133 | 0.1048
Epoch 113/300, Loss: 0.1130 | 0.1047
Epoch 114/300, Loss: 0.1133 | 0.1048
Epoch 115/300, Loss: 0.1123 | 0.1045
Epoch 116/300, Loss: 0.1128 | 0.1043
Epoch 117/300, Loss: 0.1124 | 0.1042
Epoch 118/300, Loss: 0.1124 | 0.1042
Epoch 119/300, Loss: 0.1119 | 0.1041
Epoch 120/300, Loss: 0.1120 | 0.1040
Epoch 121/300, Loss: 0.1124 | 0.1039
Epoch 122/300, Loss: 0.1120 | 0.1040
Epoch 123/300, Loss: 0.1121 | 0.1040
Epoch 124/300, Loss: 0.1119 | 0.1039
Epoch 125/300, Loss: 0.1122 | 0.1037
Epoch 126/300, Loss: 0.1117 | 0.1037
Epoch 127/300, Loss: 0.1115 | 0.1037
Epoch 128/300, Loss: 0.1119 | 0.1036
Epoch 129/300, Loss: 0.1115 | 0.1036
Epoch 130/300, Loss: 0.1120 | 0.1036
Epoch 131/300, Loss: 0.1118 | 0.1036
Epoch 132/300, Loss: 0.1116 | 0.1036
Epoch 133/300, Loss: 0.1112 | 0.1035
Epoch 134/300, Loss: 0.1124 | 0.1035
Epoch 135/300, Loss: 0.1112 | 0.1035
Epoch 136/300, Loss: 0.1114 | 0.1035
Epoch 137/300, Loss: 0.1111 | 0.1034
Epoch 138/300, Loss: 0.1116 | 0.1033
Epoch 139/300, Loss: 0.1112 | 0.1034
Epoch 140/300, Loss: 0.1114 | 0.1034
Epoch 141/300, Loss: 0.1116 | 0.1033
Epoch 142/300, Loss: 0.1115 | 0.1033
Epoch 143/300, Loss: 0.1109 | 0.1035
Epoch 144/300, Loss: 0.1109 | 0.1036
Epoch 145/300, Loss: 0.1114 | 0.1035
Epoch 146/300, Loss: 0.1117 | 0.1034
Epoch 147/300, Loss: 0.1107 | 0.1033
Epoch 148/300, Loss: 0.1109 | 0.1033
Epoch 149/300, Loss: 0.1111 | 0.1033
Epoch 150/300, Loss: 0.1108 | 0.1033
Epoch 151/300, Loss: 0.1104 | 0.1033
Epoch 152/300, Loss: 0.1106 | 0.1033
Epoch 153/300, Loss: 0.1117 | 0.1033
Epoch 154/300, Loss: 0.1106 | 0.1034
Epoch 155/300, Loss: 0.1112 | 0.1033
Epoch 156/300, Loss: 0.1112 | 0.1033
Epoch 157/300, Loss: 0.1111 | 0.1033
Epoch 158/300, Loss: 0.1110 | 0.1033
Epoch 159/300, Loss: 0.1113 | 0.1033
Epoch 160/300, Loss: 0.1107 | 0.1033
Epoch 161/300, Loss: 0.1105 | 0.1032
Epoch 162/300, Loss: 0.1108 | 0.1032
Epoch 163/300, Loss: 0.1104 | 0.1033
Epoch 164/300, Loss: 0.1108 | 0.1032
Epoch 165/300, Loss: 0.1110 | 0.1031
Epoch 166/300, Loss: 0.1106 | 0.1031
Epoch 167/300, Loss: 0.1104 | 0.1031
Epoch 168/300, Loss: 0.1117 | 0.1031
Epoch 169/300, Loss: 0.1106 | 0.1031
Epoch 170/300, Loss: 0.1108 | 0.1031
Epoch 171/300, Loss: 0.1111 | 0.1031
Epoch 172/300, Loss: 0.1104 | 0.1031
Epoch 173/300, Loss: 0.1105 | 0.1031
Epoch 174/300, Loss: 0.1113 | 0.1031
Epoch 175/300, Loss: 0.1106 | 0.1031
Epoch 176/300, Loss: 0.1107 | 0.1030
Epoch 177/300, Loss: 0.1110 | 0.1030
Epoch 178/300, Loss: 0.1108 | 0.1030
Epoch 179/300, Loss: 0.1106 | 0.1030
Epoch 180/300, Loss: 0.1108 | 0.1030
Epoch 181/300, Loss: 0.1108 | 0.1030
Epoch 182/300, Loss: 0.1104 | 0.1030
Epoch 183/300, Loss: 0.1105 | 0.1029
Epoch 184/300, Loss: 0.1102 | 0.1030
Epoch 185/300, Loss: 0.1110 | 0.1030
Epoch 186/300, Loss: 0.1109 | 0.1030
Epoch 187/300, Loss: 0.1108 | 0.1030
Epoch 188/300, Loss: 0.1114 | 0.1030
Epoch 189/300, Loss: 0.1108 | 0.1030
Epoch 190/300, Loss: 0.1109 | 0.1030
Epoch 191/300, Loss: 0.1103 | 0.1030
Epoch 192/300, Loss: 0.1101 | 0.1030
Epoch 193/300, Loss: 0.1102 | 0.1030
Epoch 194/300, Loss: 0.1103 | 0.1030
Epoch 195/300, Loss: 0.1108 | 0.1030
Epoch 196/300, Loss: 0.1107 | 0.1030
Epoch 197/300, Loss: 0.1103 | 0.1030
Epoch 198/300, Loss: 0.1103 | 0.1030
Epoch 199/300, Loss: 0.1105 | 0.1030
Epoch 200/300, Loss: 0.1110 | 0.1030
Epoch 201/300, Loss: 0.1111 | 0.1030
Epoch 202/300, Loss: 0.1108 | 0.1030
Epoch 203/300, Loss: 0.1103 | 0.1030
Epoch 204/300, Loss: 0.1107 | 0.1030
Epoch 205/300, Loss: 0.1109 | 0.1030
Epoch 206/300, Loss: 0.1107 | 0.1030
Epoch 207/300, Loss: 0.1107 | 0.1030
Epoch 208/300, Loss: 0.1105 | 0.1030
Epoch 209/300, Loss: 0.1108 | 0.1030
Epoch 210/300, Loss: 0.1096 | 0.1030
Epoch 211/300, Loss: 0.1111 | 0.1030
Epoch 212/300, Loss: 0.1104 | 0.1030
Epoch 213/300, Loss: 0.1103 | 0.1030
Epoch 214/300, Loss: 0.1104 | 0.1030
Epoch 215/300, Loss: 0.1103 | 0.1030
Epoch 216/300, Loss: 0.1107 | 0.1030
Epoch 217/300, Loss: 0.1103 | 0.1030
Epoch 218/300, Loss: 0.1103 | 0.1030
Epoch 219/300, Loss: 0.1101 | 0.1030
Epoch 220/300, Loss: 0.1102 | 0.1030
Epoch 221/300, Loss: 0.1109 | 0.1030
Epoch 222/300, Loss: 0.1099 | 0.1030
Epoch 223/300, Loss: 0.1103 | 0.1030
Epoch 224/300, Loss: 0.1110 | 0.1030
Epoch 225/300, Loss: 0.1103 | 0.1030
Epoch 226/300, Loss: 0.1107 | 0.1029
Epoch 227/300, Loss: 0.1104 | 0.1029
Epoch 228/300, Loss: 0.1103 | 0.1029
Epoch 229/300, Loss: 0.1105 | 0.1029
Epoch 230/300, Loss: 0.1107 | 0.1029
Epoch 231/300, Loss: 0.1104 | 0.1029
Epoch 232/300, Loss: 0.1114 | 0.1029
Epoch 233/300, Loss: 0.1106 | 0.1029
Epoch 234/300, Loss: 0.1107 | 0.1029
Epoch 235/300, Loss: 0.1110 | 0.1029
Epoch 236/300, Loss: 0.1107 | 0.1029
Epoch 237/300, Loss: 0.1104 | 0.1029
Epoch 238/300, Loss: 0.1104 | 0.1029
Epoch 239/300, Loss: 0.1113 | 0.1029
Epoch 240/300, Loss: 0.1106 | 0.1029
Epoch 241/300, Loss: 0.1107 | 0.1029
Epoch 242/300, Loss: 0.1105 | 0.1029
Epoch 243/300, Loss: 0.1103 | 0.1029
Epoch 244/300, Loss: 0.1111 | 0.1029
Epoch 245/300, Loss: 0.1108 | 0.1029
Epoch 246/300, Loss: 0.1103 | 0.1029
Epoch 247/300, Loss: 0.1106 | 0.1029
Epoch 248/300, Loss: 0.1102 | 0.1029
Epoch 249/300, Loss: 0.1110 | 0.1029
Epoch 250/300, Loss: 0.1099 | 0.1029
Epoch 251/300, Loss: 0.1108 | 0.1029
Epoch 252/300, Loss: 0.1107 | 0.1029
Epoch 253/300, Loss: 0.1110 | 0.1029
Epoch 254/300, Loss: 0.1104 | 0.1029
Epoch 255/300, Loss: 0.1104 | 0.1029
Epoch 256/300, Loss: 0.1104 | 0.1029
Epoch 257/300, Loss: 0.1101 | 0.1029
Epoch 258/300, Loss: 0.1103 | 0.1029
Epoch 259/300, Loss: 0.1100 | 0.1029
Epoch 260/300, Loss: 0.1104 | 0.1029
Epoch 261/300, Loss: 0.1112 | 0.1029
Epoch 262/300, Loss: 0.1110 | 0.1029
Epoch 263/300, Loss: 0.1111 | 0.1029
Epoch 264/300, Loss: 0.1103 | 0.1029
Epoch 265/300, Loss: 0.1106 | 0.1029
Epoch 266/300, Loss: 0.1104 | 0.1029
Epoch 267/300, Loss: 0.1104 | 0.1029
Epoch 268/300, Loss: 0.1107 | 0.1029
Epoch 269/300, Loss: 0.1105 | 0.1029
Epoch 270/300, Loss: 0.1097 | 0.1029
Epoch 271/300, Loss: 0.1098 | 0.1029
Epoch 272/300, Loss: 0.1097 | 0.1029
Epoch 273/300, Loss: 0.1104 | 0.1029
Epoch 274/300, Loss: 0.1110 | 0.1029
Epoch 275/300, Loss: 0.1105 | 0.1029
Epoch 276/300, Loss: 0.1106 | 0.1029
Epoch 277/300, Loss: 0.1108 | 0.1029
Epoch 278/300, Loss: 0.1104 | 0.1029
Epoch 279/300, Loss: 0.1106 | 0.1029
Epoch 280/300, Loss: 0.1107 | 0.1029
Epoch 281/300, Loss: 0.1107 | 0.1029
Epoch 282/300, Loss: 0.1104 | 0.1029
Epoch 283/300, Loss: 0.1106 | 0.1029
Epoch 284/300, Loss: 0.1104 | 0.1029
Epoch 285/300, Loss: 0.1106 | 0.1029
Epoch 286/300, Loss: 0.1109 | 0.1029
Epoch 287/300, Loss: 0.1107 | 0.1029
Epoch 288/300, Loss: 0.1102 | 0.1029
Epoch 289/300, Loss: 0.1106 | 0.1029
Epoch 290/300, Loss: 0.1104 | 0.1029
Epoch 291/300, Loss: 0.1100 | 0.1029
Epoch 292/300, Loss: 0.1106 | 0.1029
Epoch 293/300, Loss: 0.1099 | 0.1029
Epoch 294/300, Loss: 0.1102 | 0.1029
Epoch 295/300, Loss: 0.1110 | 0.1029
Epoch 296/300, Loss: 0.1104 | 0.1029
Epoch 297/300, Loss: 0.1108 | 0.1029
Epoch 298/300, Loss: 0.1102 | 0.1029
Epoch 299/300, Loss: 0.1102 | 0.1029
Epoch 300/300, Loss: 0.1106 | 0.1029
Runtime (seconds): 789.3695170879364
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 467.99244056479074
RMSE: 21.633132934570312
MAE: 21.633132934570312
R-squared: nan
[186.25687]
