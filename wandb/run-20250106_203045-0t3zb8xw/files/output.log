[32m[I 2025-01-06 20:30:46,712][0m A new study created in memory with name: no-name-3349232e-9658-4353-8204-9767ebad603e[0m
[32m[I 2025-01-06 20:32:46,050][0m Trial 0 finished with value: 0.5655228918284373 and parameters: {'observation_period_num': 97, 'train_rates': 0.8399209263396321, 'learning_rate': 1.2029132558408253e-05, 'batch_size': 173, 'step_size': 12, 'gamma': 0.8636198890003739}. Best is trial 0 with value: 0.5655228918284373.[0m
[32m[I 2025-01-06 20:33:20,599][0m Trial 1 finished with value: 0.7295315724641159 and parameters: {'observation_period_num': 18, 'train_rates': 0.7288447543850538, 'learning_rate': 9.747767646602487e-06, 'batch_size': 109, 'step_size': 8, 'gamma': 0.9123539904379658}. Best is trial 0 with value: 0.5655228918284373.[0m
[32m[I 2025-01-06 20:39:34,816][0m Trial 2 finished with value: 0.7746142148971558 and parameters: {'observation_period_num': 231, 'train_rates': 0.9858095938144427, 'learning_rate': 1.3400340324374549e-05, 'batch_size': 241, 'step_size': 3, 'gamma': 0.8293639855475198}. Best is trial 0 with value: 0.5655228918284373.[0m
[32m[I 2025-01-06 20:41:16,835][0m Trial 3 finished with value: 0.21872633695602417 and parameters: {'observation_period_num': 75, 'train_rates': 0.9503286922652223, 'learning_rate': 0.00019239136854561087, 'batch_size': 240, 'step_size': 11, 'gamma': 0.9806716747338101}. Best is trial 3 with value: 0.21872633695602417.[0m
[32m[I 2025-01-06 20:43:01,039][0m Trial 4 finished with value: 0.7928948039593904 and parameters: {'observation_period_num': 78, 'train_rates': 0.8805070232834356, 'learning_rate': 2.6790705750600377e-06, 'batch_size': 84, 'step_size': 2, 'gamma': 0.9634752472934853}. Best is trial 3 with value: 0.21872633695602417.[0m
[32m[I 2025-01-06 20:45:18,326][0m Trial 5 finished with value: 1.0630015809446165 and parameters: {'observation_period_num': 124, 'train_rates': 0.6912786111173465, 'learning_rate': 3.3736665953839643e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.916627204708315}. Best is trial 3 with value: 0.21872633695602417.[0m
Early stopping at epoch 79
[32m[I 2025-01-06 20:47:35,003][0m Trial 6 finished with value: 1.2200854929318106 and parameters: {'observation_period_num': 133, 'train_rates': 0.717798883900497, 'learning_rate': 6.966393553695391e-06, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8576061667102841}. Best is trial 3 with value: 0.21872633695602417.[0m
[32m[I 2025-01-06 20:52:35,020][0m Trial 7 finished with value: 0.7047612001729566 and parameters: {'observation_period_num': 214, 'train_rates': 0.8277141886922181, 'learning_rate': 7.989313975131038e-06, 'batch_size': 132, 'step_size': 7, 'gamma': 0.8619064913021346}. Best is trial 3 with value: 0.21872633695602417.[0m
[32m[I 2025-01-06 20:54:13,490][0m Trial 8 finished with value: 0.42602330446243286 and parameters: {'observation_period_num': 71, 'train_rates': 0.9702418497064487, 'learning_rate': 6.623254715896467e-06, 'batch_size': 198, 'step_size': 15, 'gamma': 0.959104160917407}. Best is trial 3 with value: 0.21872633695602417.[0m
[32m[I 2025-01-06 20:56:17,347][0m Trial 9 finished with value: 0.08652326837182045 and parameters: {'observation_period_num': 74, 'train_rates': 0.9792627169008337, 'learning_rate': 0.0001485056900120888, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9534132387321717}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 20:59:42,064][0m Trial 10 finished with value: 0.6412254357800901 and parameters: {'observation_period_num': 6, 'train_rates': 0.8956803371965341, 'learning_rate': 0.0005433089138840022, 'batch_size': 20, 'step_size': 5, 'gamma': 0.7691360614324547}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:02:40,078][0m Trial 11 finished with value: 1.010100165414766 and parameters: {'observation_period_num': 165, 'train_rates': 0.6134050694132331, 'learning_rate': 0.00018588493319484126, 'batch_size': 247, 'step_size': 10, 'gamma': 0.9897086613892239}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:03:45,634][0m Trial 12 finished with value: 0.18684046241370114 and parameters: {'observation_period_num': 50, 'train_rates': 0.9320093523492715, 'learning_rate': 0.00010393642675140928, 'batch_size': 172, 'step_size': 5, 'gamma': 0.9266466322360692}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:04:39,857][0m Trial 13 finished with value: 0.19238521006642556 and parameters: {'observation_period_num': 41, 'train_rates': 0.9162485113744806, 'learning_rate': 6.327701375915576e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9249579900718007}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:05:45,426][0m Trial 14 finished with value: 0.17071112925591675 and parameters: {'observation_period_num': 44, 'train_rates': 0.9291878569668255, 'learning_rate': 5.0953412981989404e-05, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9347030636884798}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:06:40,466][0m Trial 15 finished with value: 0.4068340947422549 and parameters: {'observation_period_num': 37, 'train_rates': 0.7697178411787395, 'learning_rate': 3.497514388081623e-05, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8942406001880352}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:10:11,736][0m Trial 16 finished with value: 2.449952769453508 and parameters: {'observation_period_num': 116, 'train_rates': 0.8557450749726919, 'learning_rate': 0.000999866659715641, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9600847613159891}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:13:34,889][0m Trial 17 finished with value: 1.358300552439334 and parameters: {'observation_period_num': 160, 'train_rates': 0.7847369731925412, 'learning_rate': 1.206195623563947e-06, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8101404224783907}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:15:12,472][0m Trial 18 finished with value: 0.10567363944588876 and parameters: {'observation_period_num': 60, 'train_rates': 0.9832056415784309, 'learning_rate': 3.126770496008861e-05, 'batch_size': 47, 'step_size': 4, 'gamma': 0.9457119499971796}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:17:40,922][0m Trial 19 finished with value: 0.1909454065434476 and parameters: {'observation_period_num': 97, 'train_rates': 0.9835858198675915, 'learning_rate': 2.1841779915880548e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8891397168698147}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:21:29,151][0m Trial 20 finished with value: 0.25265340640153255 and parameters: {'observation_period_num': 164, 'train_rates': 0.876014226636017, 'learning_rate': 0.00030889925389942255, 'batch_size': 96, 'step_size': 7, 'gamma': 0.9475954936041657}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:22:55,678][0m Trial 21 finished with value: 0.14646385669235198 and parameters: {'observation_period_num': 56, 'train_rates': 0.9349026864693347, 'learning_rate': 6.584366055342522e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.9407833310848127}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:24:32,666][0m Trial 22 finished with value: 0.1611224604504449 and parameters: {'observation_period_num': 59, 'train_rates': 0.9520460733738595, 'learning_rate': 9.484333544613044e-05, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8951656194325937}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:26:09,612][0m Trial 23 finished with value: 0.18122369911935593 and parameters: {'observation_period_num': 27, 'train_rates': 0.9079300569237643, 'learning_rate': 3.417774129518782e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9444502927059425}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:28:44,613][0m Trial 24 finished with value: 0.15672913525486365 and parameters: {'observation_period_num': 97, 'train_rates': 0.9555661253984551, 'learning_rate': 0.00010962264209589242, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9712626089153871}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:30:14,515][0m Trial 25 finished with value: 0.15212921798229218 and parameters: {'observation_period_num': 61, 'train_rates': 0.9856927364492181, 'learning_rate': 2.1521557216852974e-05, 'batch_size': 65, 'step_size': 7, 'gamma': 0.944753609712164}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:32:07,179][0m Trial 26 finished with value: 0.2791692434381623 and parameters: {'observation_period_num': 89, 'train_rates': 0.8167719830836637, 'learning_rate': 0.00022280495373235465, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9073302401488799}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:34:41,453][0m Trial 27 finished with value: 0.1817997251608357 and parameters: {'observation_period_num': 110, 'train_rates': 0.9435348705572333, 'learning_rate': 6.338164924061556e-05, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8802558339297921}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:38:04,434][0m Trial 28 finished with value: 0.20132605944361007 and parameters: {'observation_period_num': 146, 'train_rates': 0.8907177613910362, 'learning_rate': 0.0003914702712794014, 'batch_size': 88, 'step_size': 4, 'gamma': 0.8397142313975121}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:42:22,489][0m Trial 29 finished with value: 0.5155482537606183 and parameters: {'observation_period_num': 190, 'train_rates': 0.8527957558003281, 'learning_rate': 1.7475755509318167e-05, 'batch_size': 138, 'step_size': 8, 'gamma': 0.7503781516554393}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:42:50,116][0m Trial 30 finished with value: 0.7388080555773291 and parameters: {'observation_period_num': 22, 'train_rates': 0.6173986849055159, 'learning_rate': 4.5496659850970975e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9898215372487932}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:44:20,182][0m Trial 31 finished with value: 0.12247516214847565 and parameters: {'observation_period_num': 59, 'train_rates': 0.9897455686415919, 'learning_rate': 2.6546708526289932e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9424147779880983}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:46:33,733][0m Trial 32 finished with value: 0.14954850866514094 and parameters: {'observation_period_num': 62, 'train_rates': 0.9650025758822108, 'learning_rate': 8.49184954393257e-05, 'batch_size': 30, 'step_size': 8, 'gamma': 0.9415498750827436}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:47:51,340][0m Trial 33 finished with value: 0.10191837226891 and parameters: {'observation_period_num': 10, 'train_rates': 0.922178358807886, 'learning_rate': 0.00015024527197057496, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9086059450336552}. Best is trial 9 with value: 0.08652326837182045.[0m
[32m[I 2025-01-06 21:49:58,201][0m Trial 34 finished with value: 0.0527558975986072 and parameters: {'observation_period_num': 10, 'train_rates': 0.9834160426808255, 'learning_rate': 0.00015716077200717473, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9089254945076785}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 21:51:49,873][0m Trial 35 finished with value: 0.1027316677631164 and parameters: {'observation_period_num': 5, 'train_rates': 0.9669984966302514, 'learning_rate': 0.00014597505516604313, 'batch_size': 36, 'step_size': 2, 'gamma': 0.905984856975865}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 21:53:55,007][0m Trial 36 finished with value: 0.106129883043468 and parameters: {'observation_period_num': 6, 'train_rates': 0.9189965127261029, 'learning_rate': 0.00013772874356997569, 'batch_size': 31, 'step_size': 2, 'gamma': 0.9075310924932302}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 21:54:46,283][0m Trial 37 finished with value: 0.5398585249569791 and parameters: {'observation_period_num': 30, 'train_rates': 0.9589687826294773, 'learning_rate': 0.0006398693938694635, 'batch_size': 84, 'step_size': 1, 'gamma': 0.8736437451647113}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 21:57:05,946][0m Trial 38 finished with value: 0.09358625851419507 and parameters: {'observation_period_num': 17, 'train_rates': 0.9668705102492677, 'learning_rate': 0.00016306126415382158, 'batch_size': 29, 'step_size': 2, 'gamma': 0.916157791832338}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:00:42,179][0m Trial 39 finished with value: 0.177215496508773 and parameters: {'observation_period_num': 20, 'train_rates': 0.8780391484760859, 'learning_rate': 0.0002386726132308348, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9164765655255003}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:05:45,854][0m Trial 40 finished with value: 0.7021828016927165 and parameters: {'observation_period_num': 251, 'train_rates': 0.6568252045694934, 'learning_rate': 0.0003860953536285426, 'batch_size': 118, 'step_size': 2, 'gamma': 0.8523428391541554}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:07:50,818][0m Trial 41 finished with value: 0.06278284210004384 and parameters: {'observation_period_num': 12, 'train_rates': 0.973358355124204, 'learning_rate': 0.00014481052631185805, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9010735411386614}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:10:12,030][0m Trial 42 finished with value: 0.10449230896179067 and parameters: {'observation_period_num': 14, 'train_rates': 0.9419175109820213, 'learning_rate': 0.00016436194906289259, 'batch_size': 28, 'step_size': 3, 'gamma': 0.9233852532636809}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:11:00,390][0m Trial 43 finished with value: 0.2218458652496338 and parameters: {'observation_period_num': 34, 'train_rates': 0.9700001471870008, 'learning_rate': 0.0002749047166247355, 'batch_size': 202, 'step_size': 1, 'gamma': 0.8996685730485988}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:11:50,987][0m Trial 44 finished with value: 0.2506479641138497 and parameters: {'observation_period_num': 15, 'train_rates': 0.900012763341173, 'learning_rate': 0.0005649261390401924, 'batch_size': 79, 'step_size': 2, 'gamma': 0.8800238596240444}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:13:57,282][0m Trial 45 finished with value: 0.15599744650535285 and parameters: {'observation_period_num': 83, 'train_rates': 0.9222761684335705, 'learning_rate': 0.00012853756648543378, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9285796949043572}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:15:12,582][0m Trial 46 finished with value: 0.16186412491581656 and parameters: {'observation_period_num': 45, 'train_rates': 0.9471752621133286, 'learning_rate': 0.00019387276751478381, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9162041238927314}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:17:21,708][0m Trial 47 finished with value: 0.7902064402955027 and parameters: {'observation_period_num': 30, 'train_rates': 0.7317454744921968, 'learning_rate': 7.761268400874883e-05, 'batch_size': 25, 'step_size': 5, 'gamma': 0.9710441573148766}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:20:33,670][0m Trial 48 finished with value: 0.17637314610183238 and parameters: {'observation_period_num': 74, 'train_rates': 0.9725082856297703, 'learning_rate': 0.0003005720488868532, 'batch_size': 21, 'step_size': 2, 'gamma': 0.8112227412324104}. Best is trial 34 with value: 0.0527558975986072.[0m
[32m[I 2025-01-06 22:21:27,665][0m Trial 49 finished with value: 0.6757281659876259 and parameters: {'observation_period_num': 18, 'train_rates': 0.9300707665619337, 'learning_rate': 0.0007540637716907486, 'batch_size': 75, 'step_size': 3, 'gamma': 0.8847071157655453}. Best is trial 34 with value: 0.0527558975986072.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.5098 | 0.5862
Epoch 2/300, Loss: 0.4613 | 0.5340
Epoch 3/300, Loss: 0.4512 | 0.4659
Epoch 4/300, Loss: 0.3761 | 0.3833
Epoch 5/300, Loss: 0.3017 | 0.3668
Epoch 6/300, Loss: 0.2811 | 0.3512
Epoch 7/300, Loss: 0.3003 | 0.3096
Epoch 8/300, Loss: 0.2559 | 0.2555
Epoch 9/300, Loss: 0.2367 | 0.2603
Epoch 10/300, Loss: 0.2131 | 0.2197
Epoch 11/300, Loss: 0.1976 | 0.2053
Epoch 12/300, Loss: 0.1827 | 0.2398
Epoch 13/300, Loss: 0.1684 | 0.1930
Epoch 14/300, Loss: 0.1617 | 0.1788
Epoch 15/300, Loss: 0.1577 | 0.1706
Epoch 16/300, Loss: 0.1549 | 0.1812
Epoch 17/300, Loss: 0.1461 | 0.1612
Epoch 18/300, Loss: 0.1411 | 0.1561
Epoch 19/300, Loss: 0.1340 | 0.1512
Epoch 20/300, Loss: 0.1301 | 0.1455
Epoch 21/300, Loss: 0.1260 | 0.1368
Epoch 22/300, Loss: 0.1218 | 0.1358
Epoch 23/300, Loss: 0.1190 | 0.1322
Epoch 24/300, Loss: 0.1158 | 0.1259
Epoch 25/300, Loss: 0.1128 | 0.1231
Epoch 26/300, Loss: 0.1111 | 0.1213
Epoch 27/300, Loss: 0.1100 | 0.1178
Epoch 28/300, Loss: 0.1083 | 0.1145
Epoch 29/300, Loss: 0.1064 | 0.1124
Epoch 30/300, Loss: 0.1043 | 0.1110
Epoch 31/300, Loss: 0.1030 | 0.1077
Epoch 32/300, Loss: 0.1021 | 0.1050
Epoch 33/300, Loss: 0.1008 | 0.1040
Epoch 34/300, Loss: 0.0998 | 0.1009
Epoch 35/300, Loss: 0.0987 | 0.0999
Epoch 36/300, Loss: 0.0979 | 0.0985
Epoch 37/300, Loss: 0.0969 | 0.0972
Epoch 38/300, Loss: 0.0958 | 0.0956
Epoch 39/300, Loss: 0.0949 | 0.0949
Epoch 40/300, Loss: 0.0952 | 0.0931
Epoch 41/300, Loss: 0.0944 | 0.0916
Epoch 42/300, Loss: 0.0937 | 0.0908
Epoch 43/300, Loss: 0.0931 | 0.0898
Epoch 44/300, Loss: 0.0917 | 0.0889
Epoch 45/300, Loss: 0.0918 | 0.0884
Epoch 46/300, Loss: 0.0912 | 0.0870
Epoch 47/300, Loss: 0.0910 | 0.0859
Epoch 48/300, Loss: 0.0901 | 0.0851
Epoch 49/300, Loss: 0.0898 | 0.0849
Epoch 50/300, Loss: 0.0900 | 0.0838
Epoch 51/300, Loss: 0.0895 | 0.0833
Epoch 52/300, Loss: 0.0884 | 0.0825
Epoch 53/300, Loss: 0.0890 | 0.0821
Epoch 54/300, Loss: 0.0885 | 0.0817
Epoch 55/300, Loss: 0.0883 | 0.0814
Epoch 56/300, Loss: 0.0880 | 0.0810
Epoch 57/300, Loss: 0.0872 | 0.0807
Epoch 58/300, Loss: 0.0876 | 0.0802
Epoch 59/300, Loss: 0.0873 | 0.0798
Epoch 60/300, Loss: 0.0874 | 0.0800
Epoch 61/300, Loss: 0.0867 | 0.0795
Epoch 62/300, Loss: 0.0862 | 0.0792
Epoch 63/300, Loss: 0.0862 | 0.0790
Epoch 64/300, Loss: 0.0868 | 0.0790
Epoch 65/300, Loss: 0.0864 | 0.0785
Epoch 66/300, Loss: 0.0858 | 0.0782
Epoch 67/300, Loss: 0.0857 | 0.0782
Epoch 68/300, Loss: 0.0858 | 0.0781
Epoch 69/300, Loss: 0.0854 | 0.0778
Epoch 70/300, Loss: 0.0861 | 0.0772
Epoch 71/300, Loss: 0.0857 | 0.0774
Epoch 72/300, Loss: 0.0856 | 0.0774
Epoch 73/300, Loss: 0.0862 | 0.0771
Epoch 74/300, Loss: 0.0849 | 0.0770
Epoch 75/300, Loss: 0.0853 | 0.0770
Epoch 76/300, Loss: 0.0850 | 0.0768
Epoch 77/300, Loss: 0.0850 | 0.0766
Epoch 78/300, Loss: 0.0849 | 0.0764
Epoch 79/300, Loss: 0.0847 | 0.0763
Epoch 80/300, Loss: 0.0852 | 0.0762
Epoch 81/300, Loss: 0.0850 | 0.0762
Epoch 82/300, Loss: 0.0846 | 0.0761
Epoch 83/300, Loss: 0.0849 | 0.0760
Epoch 84/300, Loss: 0.0845 | 0.0760
Epoch 85/300, Loss: 0.0847 | 0.0759
Epoch 86/300, Loss: 0.0845 | 0.0759
Epoch 87/300, Loss: 0.0847 | 0.0758
Epoch 88/300, Loss: 0.0852 | 0.0757
Epoch 89/300, Loss: 0.0842 | 0.0757
Epoch 90/300, Loss: 0.0843 | 0.0757
Epoch 91/300, Loss: 0.0843 | 0.0756
Epoch 92/300, Loss: 0.0851 | 0.0755
Epoch 93/300, Loss: 0.0847 | 0.0755
Epoch 94/300, Loss: 0.0844 | 0.0754
Epoch 95/300, Loss: 0.0839 | 0.0754
Epoch 96/300, Loss: 0.0841 | 0.0754
Epoch 97/300, Loss: 0.0845 | 0.0754
Epoch 98/300, Loss: 0.0841 | 0.0753
Epoch 99/300, Loss: 0.0845 | 0.0753
Epoch 100/300, Loss: 0.0848 | 0.0753
Epoch 101/300, Loss: 0.0837 | 0.0752
Epoch 102/300, Loss: 0.0841 | 0.0752
Epoch 103/300, Loss: 0.0847 | 0.0752
Epoch 104/300, Loss: 0.0837 | 0.0751
Epoch 105/300, Loss: 0.0837 | 0.0751
Epoch 106/300, Loss: 0.0845 | 0.0751
Epoch 107/300, Loss: 0.0841 | 0.0751
Epoch 108/300, Loss: 0.0840 | 0.0751
Epoch 109/300, Loss: 0.0843 | 0.0751
Epoch 110/300, Loss: 0.0837 | 0.0751
Epoch 111/300, Loss: 0.0839 | 0.0751
Epoch 112/300, Loss: 0.0843 | 0.0750
Epoch 113/300, Loss: 0.0842 | 0.0750
Epoch 114/300, Loss: 0.0846 | 0.0750
Epoch 115/300, Loss: 0.0838 | 0.0750
Epoch 116/300, Loss: 0.0840 | 0.0750
Epoch 117/300, Loss: 0.0840 | 0.0750
Epoch 118/300, Loss: 0.0841 | 0.0750
Epoch 119/300, Loss: 0.0842 | 0.0750
Epoch 120/300, Loss: 0.0839 | 0.0749
Epoch 121/300, Loss: 0.0844 | 0.0749
Epoch 122/300, Loss: 0.0839 | 0.0749
Epoch 123/300, Loss: 0.0840 | 0.0749
Epoch 124/300, Loss: 0.0835 | 0.0749
Epoch 125/300, Loss: 0.0841 | 0.0749
Epoch 126/300, Loss: 0.0838 | 0.0749
Epoch 127/300, Loss: 0.0840 | 0.0749
Epoch 128/300, Loss: 0.0844 | 0.0749
Epoch 129/300, Loss: 0.0839 | 0.0749
Epoch 130/300, Loss: 0.0843 | 0.0749
Epoch 131/300, Loss: 0.0842 | 0.0749
Epoch 132/300, Loss: 0.0838 | 0.0749
Epoch 133/300, Loss: 0.0837 | 0.0749
Epoch 134/300, Loss: 0.0840 | 0.0749
Epoch 135/300, Loss: 0.0838 | 0.0749
Epoch 136/300, Loss: 0.0841 | 0.0749
Epoch 137/300, Loss: 0.0836 | 0.0749
Epoch 138/300, Loss: 0.0843 | 0.0749
Epoch 139/300, Loss: 0.0838 | 0.0749
Epoch 140/300, Loss: 0.0842 | 0.0749
Epoch 141/300, Loss: 0.0842 | 0.0749
Epoch 142/300, Loss: 0.0841 | 0.0749
Epoch 143/300, Loss: 0.0839 | 0.0749
Epoch 144/300, Loss: 0.0841 | 0.0749
Epoch 145/300, Loss: 0.0839 | 0.0749
Epoch 146/300, Loss: 0.0840 | 0.0749
Epoch 147/300, Loss: 0.0837 | 0.0749
Epoch 148/300, Loss: 0.0843 | 0.0749
Epoch 149/300, Loss: 0.0846 | 0.0749
Epoch 150/300, Loss: 0.0833 | 0.0749
Epoch 151/300, Loss: 0.0843 | 0.0749
Epoch 152/300, Loss: 0.0843 | 0.0749
Epoch 153/300, Loss: 0.0836 | 0.0749
Epoch 154/300, Loss: 0.0841 | 0.0749
Epoch 155/300, Loss: 0.0841 | 0.0749
Epoch 156/300, Loss: 0.0843 | 0.0749
Epoch 157/300, Loss: 0.0840 | 0.0749
Epoch 158/300, Loss: 0.0841 | 0.0749
Epoch 159/300, Loss: 0.0841 | 0.0749
Epoch 160/300, Loss: 0.0841 | 0.0749
Epoch 161/300, Loss: 0.0843 | 0.0749
Epoch 162/300, Loss: 0.0841 | 0.0749
Epoch 163/300, Loss: 0.0843 | 0.0749
Epoch 164/300, Loss: 0.0839 | 0.0749
Epoch 165/300, Loss: 0.0835 | 0.0749
Epoch 166/300, Loss: 0.0840 | 0.0749
Epoch 167/300, Loss: 0.0840 | 0.0749
Epoch 168/300, Loss: 0.0841 | 0.0749
Epoch 169/300, Loss: 0.0843 | 0.0749
Epoch 170/300, Loss: 0.0846 | 0.0749
Epoch 171/300, Loss: 0.0837 | 0.0749
Epoch 172/300, Loss: 0.0842 | 0.0749
Epoch 173/300, Loss: 0.0839 | 0.0749
Epoch 174/300, Loss: 0.0832 | 0.0749
Epoch 175/300, Loss: 0.0841 | 0.0749
Epoch 176/300, Loss: 0.0839 | 0.0749
Epoch 177/300, Loss: 0.0840 | 0.0749
Epoch 178/300, Loss: 0.0837 | 0.0749
Epoch 179/300, Loss: 0.0839 | 0.0749
Epoch 180/300, Loss: 0.0839 | 0.0749
Epoch 181/300, Loss: 0.0840 | 0.0749
Epoch 182/300, Loss: 0.0843 | 0.0749
Epoch 183/300, Loss: 0.0841 | 0.0749
Epoch 184/300, Loss: 0.0838 | 0.0749
Epoch 185/300, Loss: 0.0838 | 0.0749
Epoch 186/300, Loss: 0.0836 | 0.0749
Epoch 187/300, Loss: 0.0845 | 0.0749
Epoch 188/300, Loss: 0.0841 | 0.0749
Epoch 189/300, Loss: 0.0842 | 0.0749
Epoch 190/300, Loss: 0.0841 | 0.0749
Epoch 191/300, Loss: 0.0838 | 0.0749
Epoch 192/300, Loss: 0.0834 | 0.0749
Epoch 193/300, Loss: 0.0843 | 0.0749
Epoch 194/300, Loss: 0.0836 | 0.0749
Epoch 195/300, Loss: 0.0841 | 0.0749
Epoch 196/300, Loss: 0.0838 | 0.0749
Epoch 197/300, Loss: 0.0841 | 0.0749
Epoch 198/300, Loss: 0.0836 | 0.0749
Epoch 199/300, Loss: 0.0837 | 0.0749
Epoch 200/300, Loss: 0.0844 | 0.0749
Epoch 201/300, Loss: 0.0841 | 0.0749
Epoch 202/300, Loss: 0.0832 | 0.0749
Epoch 203/300, Loss: 0.0845 | 0.0749
Epoch 204/300, Loss: 0.0841 | 0.0749
Epoch 205/300, Loss: 0.0838 | 0.0749
Epoch 206/300, Loss: 0.0843 | 0.0749
Epoch 207/300, Loss: 0.0845 | 0.0749
Epoch 208/300, Loss: 0.0841 | 0.0749
Epoch 209/300, Loss: 0.0835 | 0.0749
Epoch 210/300, Loss: 0.0838 | 0.0749
Early stopping
Runtime (seconds): 265.5033338069916
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 469.7210931277368
RMSE: 21.673049926757812
MAE: 21.673049926757812
R-squared: nan
[221.16695]
