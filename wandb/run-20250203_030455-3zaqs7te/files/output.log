ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 03:04:57,353][0m A new study created in memory with name: no-name-6801f7a5-cfea-47ef-9f2c-cc70be61a8a6[0m
[32m[I 2025-02-03 03:05:24,619][0m Trial 0 finished with value: 1.325856642056537 and parameters: {'observation_period_num': 18, 'train_rates': 0.8096872036504152, 'learning_rate': 1.869433509960675e-06, 'batch_size': 150, 'step_size': 10, 'gamma': 0.7789454745360858}. Best is trial 0 with value: 1.325856642056537.[0m
[32m[I 2025-02-03 03:06:00,821][0m Trial 1 finished with value: 1.026918175990867 and parameters: {'observation_period_num': 40, 'train_rates': 0.7535223276774003, 'learning_rate': 2.0293392985429304e-05, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8477126489447868}. Best is trial 1 with value: 1.026918175990867.[0m
[32m[I 2025-02-03 03:08:51,004][0m Trial 2 finished with value: 0.388324838745169 and parameters: {'observation_period_num': 159, 'train_rates': 0.7891664872214849, 'learning_rate': 5.5128099525984254e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9175914246422864}. Best is trial 2 with value: 0.388324838745169.[0m
[32m[I 2025-02-03 03:11:21,870][0m Trial 3 finished with value: 0.09515994579293006 and parameters: {'observation_period_num': 18, 'train_rates': 0.9562184342713078, 'learning_rate': 0.0001232665294002515, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8752014863564357}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:13:33,535][0m Trial 4 finished with value: 0.9120412261730776 and parameters: {'observation_period_num': 130, 'train_rates': 0.772014072997308, 'learning_rate': 0.00010386548194152725, 'batch_size': 123, 'step_size': 2, 'gamma': 0.862126646315898}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:16:02,176][0m Trial 5 finished with value: 0.2118819609687135 and parameters: {'observation_period_num': 52, 'train_rates': 0.9873230358553458, 'learning_rate': 0.000388293449353188, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8929819121457222}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:18:47,427][0m Trial 6 finished with value: 0.7486363713194508 and parameters: {'observation_period_num': 153, 'train_rates': 0.7882671866435002, 'learning_rate': 2.4626037638501176e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.795323157941518}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:20:05,088][0m Trial 7 finished with value: 1.129910621431566 and parameters: {'observation_period_num': 79, 'train_rates': 0.6331218524000848, 'learning_rate': 4.694327400930799e-06, 'batch_size': 51, 'step_size': 10, 'gamma': 0.9503718447199496}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:21:39,281][0m Trial 8 finished with value: 0.40608999133110046 and parameters: {'observation_period_num': 97, 'train_rates': 0.8073429001815071, 'learning_rate': 2.5594143922896932e-05, 'batch_size': 110, 'step_size': 13, 'gamma': 0.9249712166597965}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:22:29,999][0m Trial 9 finished with value: 0.20349540561437607 and parameters: {'observation_period_num': 41, 'train_rates': 0.8820400855502559, 'learning_rate': 6.517517418247354e-05, 'batch_size': 86, 'step_size': 9, 'gamma': 0.9728226754731393}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:28:07,704][0m Trial 10 finished with value: 0.15714585781097412 and parameters: {'observation_period_num': 247, 'train_rates': 0.9841700422143045, 'learning_rate': 0.000588154547449799, 'batch_size': 174, 'step_size': 15, 'gamma': 0.8271384874845592}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:33:36,035][0m Trial 11 finished with value: 0.38126036524772644 and parameters: {'observation_period_num': 241, 'train_rates': 0.9882744826768252, 'learning_rate': 0.0009315477702936855, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8277123971134677}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:38:39,331][0m Trial 12 finished with value: 0.1407470102271726 and parameters: {'observation_period_num': 244, 'train_rates': 0.9085354297766697, 'learning_rate': 0.00024287613272278163, 'batch_size': 236, 'step_size': 15, 'gamma': 0.8157919482340696}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:42:31,475][0m Trial 13 finished with value: 0.2922104328222897 and parameters: {'observation_period_num': 198, 'train_rates': 0.8999681173657024, 'learning_rate': 0.00015800421186871466, 'batch_size': 249, 'step_size': 5, 'gamma': 0.7625170543593474}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:46:21,824][0m Trial 14 finished with value: 0.14157754318280655 and parameters: {'observation_period_num': 193, 'train_rates': 0.9043867575439823, 'learning_rate': 0.00033013110923703766, 'batch_size': 244, 'step_size': 13, 'gamma': 0.8938851451172147}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:48:08,900][0m Trial 15 finished with value: 0.22133378655586428 and parameters: {'observation_period_num': 94, 'train_rates': 0.9281565922160494, 'learning_rate': 0.00018498750912593286, 'batch_size': 64, 'step_size': 2, 'gamma': 0.8090126151207139}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:52:04,761][0m Trial 16 finished with value: 0.5656137702793911 and parameters: {'observation_period_num': 197, 'train_rates': 0.8736787250685212, 'learning_rate': 9.444806413885432e-06, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8772762539753934}. Best is trial 3 with value: 0.09515994579293006.[0m
[32m[I 2025-02-03 03:55:39,252][0m Trial 17 finished with value: 0.07726108130202236 and parameters: {'observation_period_num': 8, 'train_rates': 0.9435997685240318, 'learning_rate': 0.0001902181604655992, 'batch_size': 18, 'step_size': 4, 'gamma': 0.839658720454682}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 03:58:01,237][0m Trial 18 finished with value: 0.8817459116425465 and parameters: {'observation_period_num': 6, 'train_rates': 0.7069408435317868, 'learning_rate': 7.397850003510448e-05, 'batch_size': 22, 'step_size': 5, 'gamma': 0.8530254911084477}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 03:59:20,943][0m Trial 19 finished with value: 0.2659182968228144 and parameters: {'observation_period_num': 62, 'train_rates': 0.8414286100254766, 'learning_rate': 4.3338561717483675e-05, 'batch_size': 46, 'step_size': 3, 'gamma': 0.9103077892067507}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:00:15,701][0m Trial 20 finished with value: 0.15364290249924506 and parameters: {'observation_period_num': 26, 'train_rates': 0.9470459321648942, 'learning_rate': 0.00013256553128405694, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8380296410468999}. Best is trial 17 with value: 0.07726108130202236.[0m
Early stopping at epoch 62
[32m[I 2025-02-03 04:00:34,201][0m Trial 21 finished with value: 0.3466469466686249 and parameters: {'observation_period_num': 8, 'train_rates': 0.9356722884976547, 'learning_rate': 0.00026632052297911895, 'batch_size': 152, 'step_size': 1, 'gamma': 0.8057694770929515}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:03:39,636][0m Trial 22 finished with value: 1.5855136782571069 and parameters: {'observation_period_num': 119, 'train_rates': 0.8520969596722774, 'learning_rate': 0.0007688118630097156, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8793201767293518}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:08:31,645][0m Trial 23 finished with value: 0.12445241212844849 and parameters: {'observation_period_num': 224, 'train_rates': 0.9516664937263044, 'learning_rate': 0.00023922637392035975, 'batch_size': 48, 'step_size': 14, 'gamma': 0.8187615055880189}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:10:15,114][0m Trial 24 finished with value: 0.31976575960419074 and parameters: {'observation_period_num': 70, 'train_rates': 0.9538187866641096, 'learning_rate': 0.0004697373823977236, 'batch_size': 41, 'step_size': 8, 'gamma': 0.7811836645942511}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:13:43,656][0m Trial 25 finished with value: 0.12538455104505694 and parameters: {'observation_period_num': 170, 'train_rates': 0.9601166427730237, 'learning_rate': 0.00010345315974675088, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8586122843925482}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:18:11,277][0m Trial 26 finished with value: 0.21141775388309023 and parameters: {'observation_period_num': 216, 'train_rates': 0.8584373279342904, 'learning_rate': 0.0002756621840156962, 'batch_size': 37, 'step_size': 13, 'gamma': 0.7533611142989013}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:18:59,853][0m Trial 27 finished with value: 0.4138067018985748 and parameters: {'observation_period_num': 30, 'train_rates': 0.9231465215843869, 'learning_rate': 1.1719645945068034e-05, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8350339851604838}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:21:56,240][0m Trial 28 finished with value: 1.0498583730875466 and parameters: {'observation_period_num': 129, 'train_rates': 0.6122452387827756, 'learning_rate': 9.332534191088653e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.787612277404639}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:23:04,387][0m Trial 29 finished with value: 0.9727963861730909 and parameters: {'observation_period_num': 22, 'train_rates': 0.8345030556020305, 'learning_rate': 1.0198117172852316e-06, 'batch_size': 57, 'step_size': 14, 'gamma': 0.8730501455371708}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:24:54,317][0m Trial 30 finished with value: 0.11657126427144933 and parameters: {'observation_period_num': 47, 'train_rates': 0.9544665421781091, 'learning_rate': 0.00019061740847417974, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9340945359134748}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:26:54,069][0m Trial 31 finished with value: 0.1296483725309372 and parameters: {'observation_period_num': 39, 'train_rates': 0.9648776049652703, 'learning_rate': 0.00017988155190246387, 'batch_size': 34, 'step_size': 11, 'gamma': 0.984251757444369}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:27:53,071][0m Trial 32 finished with value: 0.21039392926006495 and parameters: {'observation_period_num': 54, 'train_rates': 0.8886531753969388, 'learning_rate': 3.693551622710856e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.9496124401781261}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:29:17,060][0m Trial 33 finished with value: 1.4867252171976535 and parameters: {'observation_period_num': 6, 'train_rates': 0.7191238076085172, 'learning_rate': 0.0005467603903530523, 'batch_size': 38, 'step_size': 11, 'gamma': 0.9321091688562669}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:30:21,189][0m Trial 34 finished with value: 0.12105094889799754 and parameters: {'observation_period_num': 42, 'train_rates': 0.9652200770217383, 'learning_rate': 0.00020010088404232834, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8469320591078674}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:31:24,524][0m Trial 35 finished with value: 0.12946211349690098 and parameters: {'observation_period_num': 44, 'train_rates': 0.9702117893434102, 'learning_rate': 0.00013205175421034687, 'batch_size': 70, 'step_size': 9, 'gamma': 0.897963830670419}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:33:45,187][0m Trial 36 finished with value: 0.12003872750719566 and parameters: {'observation_period_num': 22, 'train_rates': 0.923857481919684, 'learning_rate': 6.933058232266793e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8448523670525}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:34:17,389][0m Trial 37 finished with value: 0.1756379615906711 and parameters: {'observation_period_num': 20, 'train_rates': 0.9254726516100529, 'learning_rate': 4.879800532975417e-05, 'batch_size': 136, 'step_size': 7, 'gamma': 0.9420252557365191}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:36:30,357][0m Trial 38 finished with value: 0.15683671321640624 and parameters: {'observation_period_num': 82, 'train_rates': 0.9183614881476726, 'learning_rate': 1.680544620229918e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9099782598044854}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:37:11,546][0m Trial 39 finished with value: 0.34822502219426243 and parameters: {'observation_period_num': 31, 'train_rates': 0.8210137098773422, 'learning_rate': 8.843408055268564e-05, 'batch_size': 98, 'step_size': 6, 'gamma': 0.8674119747592303}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:39:17,838][0m Trial 40 finished with value: 0.674250969707549 and parameters: {'observation_period_num': 55, 'train_rates': 0.7530852506290657, 'learning_rate': 3.3310261732259236e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9695101840668643}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:40:39,518][0m Trial 41 finished with value: 0.1380796215337951 and parameters: {'observation_period_num': 17, 'train_rates': 0.9720674673702688, 'learning_rate': 0.00039155185341713944, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8431681792170274}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:43:43,396][0m Trial 42 finished with value: 0.09953694717913139 and parameters: {'observation_period_num': 39, 'train_rates': 0.9438666830991129, 'learning_rate': 6.42668764716061e-05, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8562622743515907}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:47:03,167][0m Trial 43 finished with value: 0.10251935135797073 and parameters: {'observation_period_num': 72, 'train_rates': 0.9367379784607166, 'learning_rate': 5.754345671725372e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8866488345206254}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:50:50,851][0m Trial 44 finished with value: 0.09740940142761577 and parameters: {'observation_period_num': 111, 'train_rates': 0.9460621773429116, 'learning_rate': 5.365861520674883e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8860078872636123}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:54:35,723][0m Trial 45 finished with value: 0.18541963398456573 and parameters: {'observation_period_num': 113, 'train_rates': 0.8774273435276598, 'learning_rate': 2.8607649104114523e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8891450643366313}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 04:59:05,439][0m Trial 46 finished with value: 0.11260469905899827 and parameters: {'observation_period_num': 147, 'train_rates': 0.939966717920095, 'learning_rate': 2.1442962202671075e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8862480203101002}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 05:00:51,512][0m Trial 47 finished with value: 0.15670347213745117 and parameters: {'observation_period_num': 82, 'train_rates': 0.9875631773114177, 'learning_rate': 5.521983770491743e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8617323587348363}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 05:03:27,040][0m Trial 48 finished with value: 0.2127843935923143 and parameters: {'observation_period_num': 97, 'train_rates': 0.9036873143680008, 'learning_rate': 6.364584544641009e-06, 'batch_size': 26, 'step_size': 14, 'gamma': 0.899743151747154}. Best is trial 17 with value: 0.07726108130202236.[0m
[32m[I 2025-02-03 05:04:42,105][0m Trial 49 finished with value: 0.1738389351238043 and parameters: {'observation_period_num': 71, 'train_rates': 0.8916563539720685, 'learning_rate': 0.00012689014080150918, 'batch_size': 194, 'step_size': 13, 'gamma': 0.8679936791265699}. Best is trial 17 with value: 0.07726108130202236.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 05:04:42,112][0m A new study created in memory with name: no-name-58d973e6-19d8-46c4-8538-1d36cf0379fa[0m
[32m[I 2025-02-03 05:06:16,900][0m Trial 0 finished with value: 1.2379140814933642 and parameters: {'observation_period_num': 91, 'train_rates': 0.7003486172841061, 'learning_rate': 1.4084716261243147e-06, 'batch_size': 43, 'step_size': 5, 'gamma': 0.8924277415052904}. Best is trial 0 with value: 1.2379140814933642.[0m
[32m[I 2025-02-03 05:10:23,882][0m Trial 1 finished with value: 1.4151558657509042 and parameters: {'observation_period_num': 248, 'train_rates': 0.6631921329180732, 'learning_rate': 2.96705395254394e-06, 'batch_size': 219, 'step_size': 4, 'gamma': 0.862318329407495}. Best is trial 0 with value: 1.2379140814933642.[0m
[32m[I 2025-02-03 05:10:54,007][0m Trial 2 finished with value: 0.8155518907555787 and parameters: {'observation_period_num': 5, 'train_rates': 0.852884129555507, 'learning_rate': 4.082720291847875e-06, 'batch_size': 146, 'step_size': 11, 'gamma': 0.7537623275444969}. Best is trial 2 with value: 0.8155518907555787.[0m
[32m[I 2025-02-03 05:15:35,976][0m Trial 3 finished with value: 1.145516037940979 and parameters: {'observation_period_num': 220, 'train_rates': 0.9804507862422696, 'learning_rate': 2.3140336394200385e-06, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8099148711385181}. Best is trial 2 with value: 0.8155518907555787.[0m
[32m[I 2025-02-03 05:16:33,547][0m Trial 4 finished with value: 0.19680560822204007 and parameters: {'observation_period_num': 51, 'train_rates': 0.9187860165665218, 'learning_rate': 3.916870685844247e-05, 'batch_size': 117, 'step_size': 10, 'gamma': 0.9793557485880094}. Best is trial 4 with value: 0.19680560822204007.[0m
[32m[I 2025-02-03 05:19:17,128][0m Trial 5 finished with value: 0.17984470051701598 and parameters: {'observation_period_num': 143, 'train_rates': 0.9151135002323258, 'learning_rate': 0.0006368022087592476, 'batch_size': 237, 'step_size': 4, 'gamma': 0.8833925245803367}. Best is trial 5 with value: 0.17984470051701598.[0m
[32m[I 2025-02-03 05:22:20,290][0m Trial 6 finished with value: 1.0149313443025965 and parameters: {'observation_period_num': 190, 'train_rates': 0.7157751315911816, 'learning_rate': 0.000162506530504532, 'batch_size': 226, 'step_size': 3, 'gamma': 0.8877087181270451}. Best is trial 5 with value: 0.17984470051701598.[0m
[32m[I 2025-02-03 05:22:39,467][0m Trial 7 finished with value: 1.3416927833685883 and parameters: {'observation_period_num': 18, 'train_rates': 0.6337564029233502, 'learning_rate': 3.436504157905312e-06, 'batch_size': 245, 'step_size': 9, 'gamma': 0.8015676327732316}. Best is trial 5 with value: 0.17984470051701598.[0m
Early stopping at epoch 94
[32m[I 2025-02-03 05:24:20,480][0m Trial 8 finished with value: 0.981543760870132 and parameters: {'observation_period_num': 120, 'train_rates': 0.7122610307069539, 'learning_rate': 0.0001649319325014116, 'batch_size': 246, 'step_size': 1, 'gamma': 0.8774639517957106}. Best is trial 5 with value: 0.17984470051701598.[0m
Early stopping at epoch 86
[32m[I 2025-02-03 05:27:49,853][0m Trial 9 finished with value: 0.8984728215754717 and parameters: {'observation_period_num': 220, 'train_rates': 0.7769659073881646, 'learning_rate': 0.00013330853566252815, 'batch_size': 196, 'step_size': 1, 'gamma': 0.8561937294274571}. Best is trial 5 with value: 0.17984470051701598.[0m
[32m[I 2025-02-03 05:30:59,676][0m Trial 10 finished with value: 1.0122025064439162 and parameters: {'observation_period_num': 159, 'train_rates': 0.8715657889470236, 'learning_rate': 0.0008537810646444292, 'batch_size': 60, 'step_size': 15, 'gamma': 0.9489932309612191}. Best is trial 5 with value: 0.17984470051701598.[0m
[32m[I 2025-02-03 05:32:26,836][0m Trial 11 finished with value: 0.17233087122440338 and parameters: {'observation_period_num': 75, 'train_rates': 0.9714174414949245, 'learning_rate': 2.7350006303488358e-05, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9873258249059688}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:34:16,973][0m Trial 12 finished with value: 0.2684960961341858 and parameters: {'observation_period_num': 93, 'train_rates': 0.9848362712489722, 'learning_rate': 1.4371797979822472e-05, 'batch_size': 86, 'step_size': 7, 'gamma': 0.9368411845533063}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:37:02,652][0m Trial 13 finished with value: 0.23665746050862085 and parameters: {'observation_period_num': 147, 'train_rates': 0.9195608329464731, 'learning_rate': 0.0007394366934634783, 'batch_size': 181, 'step_size': 7, 'gamma': 0.9858404143932797}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:38:20,568][0m Trial 14 finished with value: 0.24445524285821355 and parameters: {'observation_period_num': 68, 'train_rates': 0.9233214377070176, 'learning_rate': 2.1142813150791725e-05, 'batch_size': 91, 'step_size': 6, 'gamma': 0.9270586920405923}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:40:24,172][0m Trial 15 finished with value: 0.37450690802774933 and parameters: {'observation_period_num': 124, 'train_rates': 0.8117646933475424, 'learning_rate': 5.2051696331266865e-05, 'batch_size': 118, 'step_size': 8, 'gamma': 0.8164188616810896}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:43:52,199][0m Trial 16 finished with value: 0.3033986985683441 and parameters: {'observation_period_num': 173, 'train_rates': 0.9511549765466445, 'learning_rate': 1.0691234331680718e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.9172366603069051}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:47:17,119][0m Trial 17 finished with value: 0.3168723970944524 and parameters: {'observation_period_num': 42, 'train_rates': 0.8602455268946365, 'learning_rate': 0.0003432563184884046, 'batch_size': 19, 'step_size': 13, 'gamma': 0.959006483885091}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:49:00,505][0m Trial 18 finished with value: 0.8706678657052134 and parameters: {'observation_period_num': 105, 'train_rates': 0.8184519650105796, 'learning_rate': 7.517680894617231e-06, 'batch_size': 122, 'step_size': 5, 'gamma': 0.8330543829796787}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:50:26,536][0m Trial 19 finished with value: 0.34597875392064453 and parameters: {'observation_period_num': 80, 'train_rates': 0.8888125446351351, 'learning_rate': 7.010365230117452e-05, 'batch_size': 93, 'step_size': 3, 'gamma': 0.7653983445515423}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:52:45,171][0m Trial 20 finished with value: 0.5635695095412129 and parameters: {'observation_period_num': 140, 'train_rates': 0.7648265241217227, 'learning_rate': 0.00035423515255070373, 'batch_size': 199, 'step_size': 9, 'gamma': 0.9043496080620301}. Best is trial 11 with value: 0.17233087122440338.[0m
[32m[I 2025-02-03 05:53:44,379][0m Trial 21 finished with value: 0.15195113782724623 and parameters: {'observation_period_num': 54, 'train_rates': 0.9377008551193257, 'learning_rate': 4.3680605176141345e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9833295627663705}. Best is trial 21 with value: 0.15195113782724623.[0m
[32m[I 2025-02-03 05:54:47,710][0m Trial 22 finished with value: 0.156709186452672 and parameters: {'observation_period_num': 42, 'train_rates': 0.9544237904694856, 'learning_rate': 2.3465415462622292e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.960315388604996}. Best is trial 21 with value: 0.15195113782724623.[0m
[32m[I 2025-02-03 05:55:49,991][0m Trial 23 finished with value: 0.1435117988487867 and parameters: {'observation_period_num': 39, 'train_rates': 0.9566301424432906, 'learning_rate': 2.9414707423654832e-05, 'batch_size': 71, 'step_size': 8, 'gamma': 0.966514780503275}. Best is trial 23 with value: 0.1435117988487867.[0m
[32m[I 2025-02-03 05:57:00,739][0m Trial 24 finished with value: 0.1110183273292618 and parameters: {'observation_period_num': 30, 'train_rates': 0.9446396377860254, 'learning_rate': 8.560269887436295e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9631191612198436}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 05:59:01,521][0m Trial 25 finished with value: 0.113501293020589 and parameters: {'observation_period_num': 21, 'train_rates': 0.9405435722198815, 'learning_rate': 8.344427817188517e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.9686880975294185}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:02:40,461][0m Trial 26 finished with value: 0.13471662709398455 and parameters: {'observation_period_num': 26, 'train_rates': 0.8952166541502662, 'learning_rate': 7.542151515637102e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.9445803149924734}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:06:06,176][0m Trial 27 finished with value: 0.1506081218696484 and parameters: {'observation_period_num': 25, 'train_rates': 0.8937488013547217, 'learning_rate': 9.607503176358516e-05, 'batch_size': 18, 'step_size': 14, 'gamma': 0.9417810882974085}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:07:44,688][0m Trial 28 finished with value: 0.16018829113579425 and parameters: {'observation_period_num': 7, 'train_rates': 0.8972458069925883, 'learning_rate': 0.0002588049632380561, 'batch_size': 40, 'step_size': 13, 'gamma': 0.9144273473437227}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:09:22,050][0m Trial 29 finished with value: 0.20589865397796955 and parameters: {'observation_period_num': 27, 'train_rates': 0.839621246776918, 'learning_rate': 8.206944974361887e-05, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9027740455054101}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:10:36,713][0m Trial 30 finished with value: 0.8185395333524615 and parameters: {'observation_period_num': 68, 'train_rates': 0.7459705704313421, 'learning_rate': 0.0002473951442929164, 'batch_size': 53, 'step_size': 12, 'gamma': 0.9639040898141498}. Best is trial 24 with value: 0.1110183273292618.[0m
[32m[I 2025-02-03 06:12:34,235][0m Trial 31 finished with value: 0.10780212574678918 and parameters: {'observation_period_num': 32, 'train_rates': 0.9529283013612364, 'learning_rate': 6.180860015408453e-05, 'batch_size': 34, 'step_size': 11, 'gamma': 0.9694393338870197}. Best is trial 31 with value: 0.10780212574678918.[0m
[32m[I 2025-02-03 06:14:30,815][0m Trial 32 finished with value: 0.10624731684985914 and parameters: {'observation_period_num': 20, 'train_rates': 0.9482018595019792, 'learning_rate': 6.084372146665323e-05, 'batch_size': 34, 'step_size': 11, 'gamma': 0.9461837995499365}. Best is trial 32 with value: 0.10624731684985914.[0m
[32m[I 2025-02-03 06:16:20,338][0m Trial 33 finished with value: 0.10675163742373972 and parameters: {'observation_period_num': 9, 'train_rates': 0.9422872975123342, 'learning_rate': 0.00011824115092813245, 'batch_size': 36, 'step_size': 11, 'gamma': 0.9740704857246696}. Best is trial 32 with value: 0.10624731684985914.[0m
[32m[I 2025-02-03 06:17:33,497][0m Trial 34 finished with value: 0.10360093560555707 and parameters: {'observation_period_num': 7, 'train_rates': 0.9688446261875494, 'learning_rate': 0.00011728096611327674, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9276161736921833}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:19:38,822][0m Trial 35 finished with value: 0.1414298713207245 and parameters: {'observation_period_num': 9, 'train_rates': 0.9898119542111978, 'learning_rate': 0.0001241598266783148, 'batch_size': 32, 'step_size': 11, 'gamma': 0.9227983552197289}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:20:34,264][0m Trial 36 finished with value: 0.11023851594712475 and parameters: {'observation_period_num': 7, 'train_rates': 0.965716892099826, 'learning_rate': 4.9253463552585644e-05, 'batch_size': 74, 'step_size': 11, 'gamma': 0.932213487953103}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:22:05,688][0m Trial 37 finished with value: 0.12523097544908524 and parameters: {'observation_period_num': 56, 'train_rates': 0.9285825770862968, 'learning_rate': 0.00021406149794831646, 'batch_size': 45, 'step_size': 12, 'gamma': 0.9517772724997116}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:23:10,928][0m Trial 38 finished with value: 0.890248857650095 and parameters: {'observation_period_num': 37, 'train_rates': 0.6737628634398161, 'learning_rate': 5.353138347530261e-05, 'batch_size': 50, 'step_size': 11, 'gamma': 0.9745925070974523}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:25:17,291][0m Trial 39 finished with value: 0.30035556549796016 and parameters: {'observation_period_num': 16, 'train_rates': 0.8377058647039586, 'learning_rate': 0.00034580713814618075, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8657625691071185}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:26:17,190][0m Trial 40 finished with value: 0.9992084216354484 and parameters: {'observation_period_num': 59, 'train_rates': 0.9081050576697237, 'learning_rate': 1.0379958671337586e-06, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9894334900058476}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:27:23,737][0m Trial 41 finished with value: 0.11216717239688424 and parameters: {'observation_period_num': 6, 'train_rates': 0.9713406972179719, 'learning_rate': 4.290048491411933e-05, 'batch_size': 61, 'step_size': 11, 'gamma': 0.9327603097258385}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:28:15,971][0m Trial 42 finished with value: 0.10427625122524443 and parameters: {'observation_period_num': 16, 'train_rates': 0.9644877901420034, 'learning_rate': 0.0001214482987200768, 'batch_size': 80, 'step_size': 11, 'gamma': 0.8949135653072794}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:29:09,603][0m Trial 43 finished with value: 0.12604713200152606 and parameters: {'observation_period_num': 19, 'train_rates': 0.9706498568978255, 'learning_rate': 0.00012376373416086973, 'batch_size': 77, 'step_size': 12, 'gamma': 0.9035164899744698}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:30:38,193][0m Trial 44 finished with value: 0.22838154435157776 and parameters: {'observation_period_num': 34, 'train_rates': 0.9887937060122219, 'learning_rate': 0.0005037786805827126, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8883667328223409}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:31:29,513][0m Trial 45 finished with value: 0.22308459853868923 and parameters: {'observation_period_num': 44, 'train_rates': 0.8723492767889213, 'learning_rate': 0.0001586744599366161, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9491774899854988}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:36:29,874][0m Trial 46 finished with value: 0.12810449857026973 and parameters: {'observation_period_num': 217, 'train_rates': 0.9315224240642894, 'learning_rate': 1.85864839988833e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9747651642197007}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:41:59,012][0m Trial 47 finished with value: 0.13162589650380283 and parameters: {'observation_period_num': 250, 'train_rates': 0.906422941641936, 'learning_rate': 0.00019812644969766186, 'batch_size': 59, 'step_size': 11, 'gamma': 0.8500233691004063}. Best is trial 34 with value: 0.10360093560555707.[0m
[32m[I 2025-02-03 06:44:22,666][0m Trial 48 finished with value: 0.09841655548346245 and parameters: {'observation_period_num': 17, 'train_rates': 0.9597906513130275, 'learning_rate': 6.41794912887292e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8741560057977434}. Best is trial 48 with value: 0.09841655548346245.[0m
[32m[I 2025-02-03 06:44:44,460][0m Trial 49 finished with value: 0.9393702680236552 and parameters: {'observation_period_num': 16, 'train_rates': 0.6262026399224884, 'learning_rate': 0.00010897146224342679, 'batch_size': 158, 'step_size': 14, 'gamma': 0.8732158118746531}. Best is trial 48 with value: 0.09841655548346245.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 06:44:44,467][0m A new study created in memory with name: no-name-bb9fe7df-f49d-4506-abbc-0155ad38be12[0m
[32m[I 2025-02-03 06:45:48,164][0m Trial 0 finished with value: 1.5351117342195855 and parameters: {'observation_period_num': 67, 'train_rates': 0.7790649633602067, 'learning_rate': 1.0965941573320516e-06, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9897059938811826}. Best is trial 0 with value: 1.5351117342195855.[0m
[32m[I 2025-02-03 06:48:06,964][0m Trial 1 finished with value: 1.1280927370380422 and parameters: {'observation_period_num': 145, 'train_rates': 0.6812017428151247, 'learning_rate': 1.2427345134464015e-06, 'batch_size': 161, 'step_size': 11, 'gamma': 0.9580153328152803}. Best is trial 1 with value: 1.1280927370380422.[0m
[32m[I 2025-02-03 06:53:26,982][0m Trial 2 finished with value: 0.19524778425693512 and parameters: {'observation_period_num': 244, 'train_rates': 0.9788168933683468, 'learning_rate': 0.00013998761103464524, 'batch_size': 244, 'step_size': 12, 'gamma': 0.7629297734255069}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 06:55:13,623][0m Trial 3 finished with value: 0.7863868896237715 and parameters: {'observation_period_num': 120, 'train_rates': 0.724220566404381, 'learning_rate': 7.942921133097316e-05, 'batch_size': 185, 'step_size': 8, 'gamma': 0.9097559891671948}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 06:59:39,788][0m Trial 4 finished with value: 0.6366042860399801 and parameters: {'observation_period_num': 211, 'train_rates': 0.8859619344664975, 'learning_rate': 4.300841165024237e-06, 'batch_size': 71, 'step_size': 8, 'gamma': 0.7862559422653066}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:01:28,488][0m Trial 5 finished with value: 1.2356806782227527 and parameters: {'observation_period_num': 113, 'train_rates': 0.7497283995443369, 'learning_rate': 2.022517283717211e-06, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9006959298256006}. Best is trial 2 with value: 0.19524778425693512.[0m
Early stopping at epoch 48
[32m[I 2025-02-03 07:02:20,757][0m Trial 6 finished with value: 1.0049823801169235 and parameters: {'observation_period_num': 100, 'train_rates': 0.8439235652809277, 'learning_rate': 2.303845263647481e-05, 'batch_size': 77, 'step_size': 1, 'gamma': 0.7607820417194849}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:04:25,032][0m Trial 7 finished with value: 1.0432428458264766 and parameters: {'observation_period_num': 142, 'train_rates': 0.6270721757635485, 'learning_rate': 0.00011896688247205009, 'batch_size': 228, 'step_size': 3, 'gamma': 0.9378372223464828}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:07:42,047][0m Trial 8 finished with value: 0.9339137854839585 and parameters: {'observation_period_num': 204, 'train_rates': 0.6642334316131967, 'learning_rate': 5.558960400780039e-05, 'batch_size': 162, 'step_size': 11, 'gamma': 0.8111620131593658}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:12:12,106][0m Trial 9 finished with value: 0.5186417102813721 and parameters: {'observation_period_num': 214, 'train_rates': 0.9636306618400017, 'learning_rate': 7.841514211697987e-06, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8410494987489614}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:12:50,694][0m Trial 10 finished with value: 0.32734501361846924 and parameters: {'observation_period_num': 6, 'train_rates': 0.9783390250778274, 'learning_rate': 0.00065147035712874, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8545920029720437}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:13:30,021][0m Trial 11 finished with value: 0.45411771535873413 and parameters: {'observation_period_num': 18, 'train_rates': 0.9825824828302387, 'learning_rate': 0.0009452512447458358, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8506491989226586}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:19:16,504][0m Trial 12 finished with value: 0.3671407507910273 and parameters: {'observation_period_num': 251, 'train_rates': 0.9109235722997113, 'learning_rate': 0.00046378612730761556, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8111138423864744}. Best is trial 2 with value: 0.19524778425693512.[0m
[32m[I 2025-02-03 07:19:52,494][0m Trial 13 finished with value: 0.152253267841529 and parameters: {'observation_period_num': 14, 'train_rates': 0.9233494353645518, 'learning_rate': 0.00027791807600999495, 'batch_size': 115, 'step_size': 13, 'gamma': 0.7542449824565169}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:20:39,812][0m Trial 14 finished with value: 0.18401810493611048 and parameters: {'observation_period_num': 47, 'train_rates': 0.9074321996703071, 'learning_rate': 0.00022190494670896747, 'batch_size': 220, 'step_size': 13, 'gamma': 0.7508275281677518}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:21:28,219][0m Trial 15 finished with value: 0.23306472181464752 and parameters: {'observation_period_num': 49, 'train_rates': 0.8477641855110549, 'learning_rate': 0.000288306141400002, 'batch_size': 206, 'step_size': 13, 'gamma': 0.7501294078449898}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:22:18,301][0m Trial 16 finished with value: 0.1845467032691245 and parameters: {'observation_period_num': 48, 'train_rates': 0.9149537116963111, 'learning_rate': 0.00024720784171495896, 'batch_size': 140, 'step_size': 13, 'gamma': 0.7882515865656912}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:23:42,277][0m Trial 17 finished with value: 0.4410140782831207 and parameters: {'observation_period_num': 79, 'train_rates': 0.8247228493349531, 'learning_rate': 2.495973287641365e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8126142798181976}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:25:24,336][0m Trial 18 finished with value: 0.182107930458509 and parameters: {'observation_period_num': 31, 'train_rates': 0.8888475883904378, 'learning_rate': 0.00024885877397664777, 'batch_size': 45, 'step_size': 13, 'gamma': 0.7842742428539833}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:29:35,438][0m Trial 19 finished with value: 0.17118747763120595 and parameters: {'observation_period_num': 24, 'train_rates': 0.8700396333463909, 'learning_rate': 4.763292176405832e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.786402909946527}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:33:37,476][0m Trial 20 finished with value: 0.18019629905452836 and parameters: {'observation_period_num': 169, 'train_rates': 0.9365312109579599, 'learning_rate': 1.2607818147780933e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8816707488966606}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:37:39,662][0m Trial 21 finished with value: 0.21959547506224725 and parameters: {'observation_period_num': 177, 'train_rates': 0.9219944932467681, 'learning_rate': 1.0952833426579985e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8790623571358592}. Best is trial 13 with value: 0.152253267841529.[0m
[32m[I 2025-02-03 07:41:02,637][0m Trial 22 finished with value: 0.14843242652714253 and parameters: {'observation_period_num': 162, 'train_rates': 0.9499559035631256, 'learning_rate': 4.822172254656333e-05, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8840728779633491}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:42:48,846][0m Trial 23 finished with value: 0.3034423333544707 and parameters: {'observation_period_num': 91, 'train_rates': 0.8644022169488992, 'learning_rate': 4.375936783591175e-05, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8324365301669856}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:44:08,738][0m Trial 24 finished with value: 0.3006203914014231 and parameters: {'observation_period_num': 6, 'train_rates': 0.7980191295997608, 'learning_rate': 8.186491946840571e-05, 'batch_size': 47, 'step_size': 3, 'gamma': 0.913882633965913}. Best is trial 22 with value: 0.14843242652714253.[0m
Early stopping at epoch 96
[32m[I 2025-02-03 07:47:29,864][0m Trial 25 finished with value: 0.4317841301573084 and parameters: {'observation_period_num': 170, 'train_rates': 0.9494051544889421, 'learning_rate': 3.603550284149796e-05, 'batch_size': 58, 'step_size': 2, 'gamma': 0.7762187298630278}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:48:43,007][0m Trial 26 finished with value: 0.3386609352589971 and parameters: {'observation_period_num': 66, 'train_rates': 0.8747586176828442, 'learning_rate': 2.0082344107219432e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.868630035296578}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:50:32,884][0m Trial 27 finished with value: 0.3026934757991864 and parameters: {'observation_period_num': 28, 'train_rates': 0.8126278173335404, 'learning_rate': 0.0001354692411845763, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8216349662975205}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:53:23,235][0m Trial 28 finished with value: 0.16832775401848335 and parameters: {'observation_period_num': 141, 'train_rates': 0.9519985677879543, 'learning_rate': 6.292280001271595e-05, 'batch_size': 65, 'step_size': 6, 'gamma': 0.7981344542527069}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:56:17,499][0m Trial 29 finished with value: 0.17307084032767256 and parameters: {'observation_period_num': 143, 'train_rates': 0.9495201734400109, 'learning_rate': 0.0004429538236973035, 'batch_size': 91, 'step_size': 7, 'gamma': 0.9825650206821261}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 07:59:33,241][0m Trial 30 finished with value: 0.601411017516773 and parameters: {'observation_period_num': 188, 'train_rates': 0.758462970914547, 'learning_rate': 7.534487859048955e-05, 'batch_size': 65, 'step_size': 5, 'gamma': 0.8001793725012464}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 08:02:39,734][0m Trial 31 finished with value: 0.1557246034808934 and parameters: {'observation_period_num': 147, 'train_rates': 0.9277432218889596, 'learning_rate': 5.022462250107701e-05, 'batch_size': 37, 'step_size': 7, 'gamma': 0.7680373789083229}. Best is trial 22 with value: 0.14843242652714253.[0m
[32m[I 2025-02-03 08:05:48,562][0m Trial 32 finished with value: 0.14402584964112872 and parameters: {'observation_period_num': 154, 'train_rates': 0.9438760091752999, 'learning_rate': 0.00011619073841891686, 'batch_size': 59, 'step_size': 7, 'gamma': 0.7680572339810373}. Best is trial 32 with value: 0.14402584964112872.[0m
[32m[I 2025-02-03 08:08:40,059][0m Trial 33 finished with value: 0.15456461967881194 and parameters: {'observation_period_num': 129, 'train_rates': 0.9313628401592919, 'learning_rate': 0.00015796216829967952, 'batch_size': 36, 'step_size': 7, 'gamma': 0.768564247120845}. Best is trial 32 with value: 0.14402584964112872.[0m
[32m[I 2025-02-03 08:11:54,529][0m Trial 34 finished with value: 0.1888878494501114 and parameters: {'observation_period_num': 159, 'train_rates': 0.9863213293411558, 'learning_rate': 0.0001701564780732655, 'batch_size': 166, 'step_size': 9, 'gamma': 0.770012351099659}. Best is trial 32 with value: 0.14402584964112872.[0m
[32m[I 2025-02-03 08:13:57,379][0m Trial 35 finished with value: 0.20706212630978338 and parameters: {'observation_period_num': 116, 'train_rates': 0.8954674832412897, 'learning_rate': 9.767786549473084e-05, 'batch_size': 88, 'step_size': 9, 'gamma': 0.9311198671909411}. Best is trial 32 with value: 0.14402584964112872.[0m
[32m[I 2025-02-03 08:16:31,511][0m Trial 36 finished with value: 0.13595139980316162 and parameters: {'observation_period_num': 130, 'train_rates': 0.9613739207498658, 'learning_rate': 0.0003287913123190972, 'batch_size': 123, 'step_size': 8, 'gamma': 0.8932489641267987}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:18:22,756][0m Trial 37 finished with value: 0.13720281422138214 and parameters: {'observation_period_num': 102, 'train_rates': 0.9630310656602377, 'learning_rate': 0.0004682401219159552, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8954644121865505}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:20:42,480][0m Trial 38 finished with value: 0.13856376707553864 and parameters: {'observation_period_num': 128, 'train_rates': 0.9635951723587783, 'learning_rate': 0.0004434651933318306, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8961541412460832}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:22:38,027][0m Trial 39 finished with value: 0.15511877834796906 and parameters: {'observation_period_num': 101, 'train_rates': 0.969435997068391, 'learning_rate': 0.00048483627644546254, 'batch_size': 145, 'step_size': 8, 'gamma': 0.9043563558302851}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:24:24,177][0m Trial 40 finished with value: 1.2189535551251622 and parameters: {'observation_period_num': 122, 'train_rates': 0.6919234458099566, 'learning_rate': 0.0007788558088502013, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9242288700210589}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:26:15,964][0m Trial 41 finished with value: 0.13620246946811676 and parameters: {'observation_period_num': 103, 'train_rates': 0.9610080353865902, 'learning_rate': 0.00036973744993949167, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8911096590321689}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:28:10,772][0m Trial 42 finished with value: 0.1391848623752594 and parameters: {'observation_period_num': 101, 'train_rates': 0.9649666293792406, 'learning_rate': 0.0003635787522158245, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8926365326913949}. Best is trial 36 with value: 0.13595139980316162.[0m
[32m[I 2025-02-03 08:30:05,940][0m Trial 43 finished with value: 0.12726907432079315 and parameters: {'observation_period_num': 106, 'train_rates': 0.9594807283195983, 'learning_rate': 0.0003710192989328447, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8923596555998821}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:32:38,608][0m Trial 44 finished with value: 0.20569410920143127 and parameters: {'observation_period_num': 130, 'train_rates': 0.9848612399336198, 'learning_rate': 0.0006053601844987432, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8659075497563452}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:34:17,182][0m Trial 45 finished with value: 0.3661556541919708 and parameters: {'observation_period_num': 87, 'train_rates': 0.9636623900518395, 'learning_rate': 0.0008419480989839336, 'batch_size': 130, 'step_size': 10, 'gamma': 0.9521464164020904}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:36:13,152][0m Trial 46 finished with value: 0.16511229802383465 and parameters: {'observation_period_num': 109, 'train_rates': 0.9061399847275478, 'learning_rate': 0.000345410207236452, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8952031992078777}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:37:11,667][0m Trial 47 finished with value: 1.268495863601164 and parameters: {'observation_period_num': 70, 'train_rates': 0.6133675535702586, 'learning_rate': 0.000588352579675627, 'batch_size': 153, 'step_size': 12, 'gamma': 0.9153239361227032}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:39:55,257][0m Trial 48 finished with value: 0.16679121553897858 and parameters: {'observation_period_num': 131, 'train_rates': 0.9896583671065481, 'learning_rate': 0.00020414086183075973, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8596357118924063}. Best is trial 43 with value: 0.12726907432079315.[0m
[32m[I 2025-02-03 08:41:59,973][0m Trial 49 finished with value: 0.530454158782959 and parameters: {'observation_period_num': 109, 'train_rates': 0.968372745214724, 'learning_rate': 2.997058550311789e-06, 'batch_size': 106, 'step_size': 12, 'gamma': 0.9446250702398566}. Best is trial 43 with value: 0.12726907432079315.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 08:41:59,979][0m A new study created in memory with name: no-name-651bc7a8-878e-4002-b131-140e82d0c9b4[0m
[32m[I 2025-02-03 08:46:48,976][0m Trial 0 finished with value: 0.6276881050694649 and parameters: {'observation_period_num': 250, 'train_rates': 0.7728118193992374, 'learning_rate': 0.0002309899539272651, 'batch_size': 86, 'step_size': 14, 'gamma': 0.9391008541860814}. Best is trial 0 with value: 0.6276881050694649.[0m
[32m[I 2025-02-03 08:49:05,767][0m Trial 1 finished with value: 1.6360076370284609 and parameters: {'observation_period_num': 122, 'train_rates': 0.7398759223214182, 'learning_rate': 0.0007981050666788817, 'batch_size': 29, 'step_size': 5, 'gamma': 0.9381802624999985}. Best is trial 0 with value: 0.6276881050694649.[0m
[32m[I 2025-02-03 08:50:45,252][0m Trial 2 finished with value: 1.3944986624890063 and parameters: {'observation_period_num': 108, 'train_rates': 0.7080508185364264, 'learning_rate': 2.343217365914771e-06, 'batch_size': 92, 'step_size': 15, 'gamma': 0.7798201165142766}. Best is trial 0 with value: 0.6276881050694649.[0m
[32m[I 2025-02-03 08:51:56,978][0m Trial 3 finished with value: 1.2904947985750916 and parameters: {'observation_period_num': 73, 'train_rates': 0.6811564979075002, 'learning_rate': 0.0008345286840466106, 'batch_size': 83, 'step_size': 7, 'gamma': 0.7703922284683318}. Best is trial 0 with value: 0.6276881050694649.[0m
[32m[I 2025-02-03 08:53:39,906][0m Trial 4 finished with value: 0.511475982702323 and parameters: {'observation_period_num': 101, 'train_rates': 0.8620208707309001, 'learning_rate': 0.0001287589369244126, 'batch_size': 230, 'step_size': 2, 'gamma': 0.7966857118018839}. Best is trial 4 with value: 0.511475982702323.[0m
[32m[I 2025-02-03 08:54:27,327][0m Trial 5 finished with value: 0.4988189580421599 and parameters: {'observation_period_num': 52, 'train_rates': 0.7829127997813649, 'learning_rate': 0.0008691319139205332, 'batch_size': 251, 'step_size': 14, 'gamma': 0.9260708634586079}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 08:56:43,122][0m Trial 6 finished with value: 1.1130607657761076 and parameters: {'observation_period_num': 28, 'train_rates': 0.6622385980460342, 'learning_rate': 0.000217755357144644, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8663035068626942}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 08:58:45,371][0m Trial 7 finished with value: 0.6993264921686866 and parameters: {'observation_period_num': 109, 'train_rates': 0.9537255978431904, 'learning_rate': 2.22235010507577e-06, 'batch_size': 129, 'step_size': 7, 'gamma': 0.9696792907456396}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 09:00:21,150][0m Trial 8 finished with value: 0.6136729440554208 and parameters: {'observation_period_num': 104, 'train_rates': 0.7889734167911667, 'learning_rate': 3.643568173258725e-05, 'batch_size': 209, 'step_size': 14, 'gamma': 0.7520689534774169}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 09:01:17,311][0m Trial 9 finished with value: 0.9666077521877672 and parameters: {'observation_period_num': 66, 'train_rates': 0.655030952318727, 'learning_rate': 0.0005150191917062817, 'batch_size': 201, 'step_size': 11, 'gamma': 0.8907467286844266}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 09:04:45,170][0m Trial 10 finished with value: 0.5454465857477431 and parameters: {'observation_period_num': 181, 'train_rates': 0.8726461872649982, 'learning_rate': 1.1599832240426368e-05, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8467641424285864}. Best is trial 5 with value: 0.4988189580421599.[0m
[32m[I 2025-02-03 09:05:03,787][0m Trial 11 finished with value: 0.44666141567280027 and parameters: {'observation_period_num': 5, 'train_rates': 0.8702239451755297, 'learning_rate': 7.317822876790987e-05, 'batch_size': 256, 'step_size': 3, 'gamma': 0.8207139107600114}. Best is trial 11 with value: 0.44666141567280027.[0m
Early stopping at epoch 63
[32m[I 2025-02-03 09:05:16,714][0m Trial 12 finished with value: 0.9566841032356024 and parameters: {'observation_period_num': 14, 'train_rates': 0.8585479558273161, 'learning_rate': 4.9163052413167356e-05, 'batch_size': 243, 'step_size': 1, 'gamma': 0.8225184994213683}. Best is trial 11 with value: 0.44666141567280027.[0m
[32m[I 2025-02-03 09:06:03,153][0m Trial 13 finished with value: 0.5300055742263794 and parameters: {'observation_period_num': 44, 'train_rates': 0.9518030238733843, 'learning_rate': 9.93065758315946e-06, 'batch_size': 251, 'step_size': 4, 'gamma': 0.9043157965688112}. Best is trial 11 with value: 0.44666141567280027.[0m
[32m[I 2025-02-03 09:06:26,546][0m Trial 14 finished with value: 0.40185344806136863 and parameters: {'observation_period_num': 10, 'train_rates': 0.8223370683668915, 'learning_rate': 8.660517027805259e-05, 'batch_size': 175, 'step_size': 4, 'gamma': 0.8196819856801072}. Best is trial 14 with value: 0.40185344806136863.[0m
[32m[I 2025-02-03 09:06:53,779][0m Trial 15 finished with value: 0.35227600165775846 and parameters: {'observation_period_num': 5, 'train_rates': 0.9028612161340539, 'learning_rate': 8.288656255127895e-05, 'batch_size': 166, 'step_size': 4, 'gamma': 0.8213008539331085}. Best is trial 15 with value: 0.35227600165775846.[0m
[32m[I 2025-02-03 09:09:16,049][0m Trial 16 finished with value: 1.2017463875345735 and parameters: {'observation_period_num': 163, 'train_rates': 0.6117860590959948, 'learning_rate': 1.5256845638739347e-05, 'batch_size': 154, 'step_size': 5, 'gamma': 0.8328236670326845}. Best is trial 15 with value: 0.35227600165775846.[0m
[32m[I 2025-02-03 09:12:19,914][0m Trial 17 finished with value: 0.31581792606304643 and parameters: {'observation_period_num': 163, 'train_rates': 0.9166331646951678, 'learning_rate': 0.00010027023685772159, 'batch_size': 177, 'step_size': 6, 'gamma': 0.7968686653863106}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:15:53,466][0m Trial 18 finished with value: 0.3450429439544678 and parameters: {'observation_period_num': 177, 'train_rates': 0.9817018547366547, 'learning_rate': 2.3435485819049874e-05, 'batch_size': 124, 'step_size': 8, 'gamma': 0.794504805245623}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:20:07,934][0m Trial 19 finished with value: 0.6606706976890564 and parameters: {'observation_period_num': 200, 'train_rates': 0.9884920484359178, 'learning_rate': 5.219296500425295e-06, 'batch_size': 113, 'step_size': 9, 'gamma': 0.7927898398801438}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:23:18,321][0m Trial 20 finished with value: 0.32374057173728943 and parameters: {'observation_period_num': 152, 'train_rates': 0.9899029624521543, 'learning_rate': 1.7105183890026412e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7502562043919722}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:26:28,923][0m Trial 21 finished with value: 0.31705594512086416 and parameters: {'observation_period_num': 151, 'train_rates': 0.9292476430042291, 'learning_rate': 2.182262671751276e-05, 'batch_size': 54, 'step_size': 7, 'gamma': 0.7515086908868749}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:29:35,123][0m Trial 22 finished with value: 0.7968991608233065 and parameters: {'observation_period_num': 157, 'train_rates': 0.9205933791781484, 'learning_rate': 4.677045641720539e-06, 'batch_size': 64, 'step_size': 6, 'gamma': 0.7549592455461627}. Best is trial 17 with value: 0.31581792606304643.[0m
[32m[I 2025-02-03 09:32:34,713][0m Trial 23 finished with value: 0.3112817739398734 and parameters: {'observation_period_num': 143, 'train_rates': 0.9269348922863389, 'learning_rate': 2.2868612458088424e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.769099875957257}. Best is trial 23 with value: 0.3112817739398734.[0m
[32m[I 2025-02-03 09:37:22,086][0m Trial 24 finished with value: 0.23028978451485058 and parameters: {'observation_period_num': 217, 'train_rates': 0.9186390959364169, 'learning_rate': 3.06381972319778e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.7779832273257455}. Best is trial 24 with value: 0.23028978451485058.[0m
[32m[I 2025-02-03 09:42:27,676][0m Trial 25 finished with value: 0.21943154606845353 and parameters: {'observation_period_num': 219, 'train_rates': 0.8337148314837659, 'learning_rate': 4.552785254884707e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7772987519411536}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 09:47:01,097][0m Trial 26 finished with value: 0.293842986127616 and parameters: {'observation_period_num': 223, 'train_rates': 0.8337352549819781, 'learning_rate': 3.47790551443839e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.7754900073507949}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 09:51:51,750][0m Trial 27 finished with value: 0.2546282651871132 and parameters: {'observation_period_num': 231, 'train_rates': 0.8238402339561032, 'learning_rate': 4.477442666571034e-05, 'batch_size': 35, 'step_size': 12, 'gamma': 0.8502113890401367}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 09:57:34,164][0m Trial 28 finished with value: 0.24116917641882876 and parameters: {'observation_period_num': 252, 'train_rates': 0.8325857158848768, 'learning_rate': 5.2286628026351356e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8605460451021779}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:02:49,158][0m Trial 29 finished with value: 0.718171648834784 and parameters: {'observation_period_num': 251, 'train_rates': 0.7487502666650995, 'learning_rate': 0.00018547229339501477, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9793212411468214}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:07:33,795][0m Trial 30 finished with value: 0.3034500405192375 and parameters: {'observation_period_num': 230, 'train_rates': 0.8885968351499013, 'learning_rate': 0.0003801497740749135, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8899377529660333}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:12:13,892][0m Trial 31 finished with value: 0.30206871164636595 and parameters: {'observation_period_num': 230, 'train_rates': 0.8174559631626652, 'learning_rate': 5.209883732850391e-05, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8542168203715145}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:16:36,004][0m Trial 32 finished with value: 0.5880442658985803 and parameters: {'observation_period_num': 213, 'train_rates': 0.7559869032123507, 'learning_rate': 5.159562575715751e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.8761297189504132}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:21:31,439][0m Trial 33 finished with value: 0.31770956158288755 and parameters: {'observation_period_num': 243, 'train_rates': 0.8111839155061136, 'learning_rate': 3.8922493370424794e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8418668844331789}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:25:22,379][0m Trial 34 finished with value: 0.605317081998876 and parameters: {'observation_period_num': 197, 'train_rates': 0.8371297707445023, 'learning_rate': 7.471239717583122e-06, 'batch_size': 105, 'step_size': 10, 'gamma': 0.9204146026262886}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:28:57,688][0m Trial 35 finished with value: 0.9889445868917147 and parameters: {'observation_period_num': 205, 'train_rates': 0.7186355396695571, 'learning_rate': 0.00015863297562560946, 'batch_size': 70, 'step_size': 15, 'gamma': 0.8052297535736729}. Best is trial 25 with value: 0.21943154606845353.[0m
[32m[I 2025-02-03 10:34:25,251][0m Trial 36 finished with value: 0.21867627652329955 and parameters: {'observation_period_num': 237, 'train_rates': 0.8500156651139277, 'learning_rate': 6.827304774368626e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.865823824568634}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 10:39:36,843][0m Trial 37 finished with value: 0.9138651012021873 and parameters: {'observation_period_num': 242, 'train_rates': 0.7722664492390063, 'learning_rate': 0.0003061101048578468, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8692808781684744}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 10:43:11,188][0m Trial 38 finished with value: 1.0627882480621338 and parameters: {'observation_period_num': 186, 'train_rates': 0.8887459272790127, 'learning_rate': 1.0488386380002304e-06, 'batch_size': 91, 'step_size': 9, 'gamma': 0.953157829612564}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 10:47:39,611][0m Trial 39 finished with value: 0.31014647550989677 and parameters: {'observation_period_num': 221, 'train_rates': 0.7993939330336061, 'learning_rate': 0.00012016605680755238, 'batch_size': 29, 'step_size': 13, 'gamma': 0.7854454297667565}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 10:52:47,326][0m Trial 40 finished with value: 0.24036999872605788 and parameters: {'observation_period_num': 241, 'train_rates': 0.8503894546821485, 'learning_rate': 6.801253420678295e-05, 'batch_size': 46, 'step_size': 11, 'gamma': 0.805896597542038}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 10:57:53,312][0m Trial 41 finished with value: 0.23552204000418175 and parameters: {'observation_period_num': 248, 'train_rates': 0.8426254871945487, 'learning_rate': 6.227826866894427e-05, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8077452270473157}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 11:02:13,887][0m Trial 42 finished with value: 0.23348542242436796 and parameters: {'observation_period_num': 216, 'train_rates': 0.8511882218989805, 'learning_rate': 6.886626100623862e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.8088493120157607}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 11:06:34,007][0m Trial 43 finished with value: 0.33682061017120146 and parameters: {'observation_period_num': 212, 'train_rates': 0.883741434243432, 'learning_rate': 3.1145275292261786e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.7652161188629678}. Best is trial 36 with value: 0.21867627652329955.[0m
[32m[I 2025-02-03 11:10:25,500][0m Trial 44 finished with value: 0.20906982817514627 and parameters: {'observation_period_num': 192, 'train_rates': 0.8434707963838703, 'learning_rate': 0.0001354549294845049, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8106771071327684}. Best is trial 44 with value: 0.20906982817514627.[0m
[32m[I 2025-02-03 11:14:03,882][0m Trial 45 finished with value: 0.34748103460040664 and parameters: {'observation_period_num': 194, 'train_rates': 0.8023299953595497, 'learning_rate': 0.00012978950081627623, 'batch_size': 83, 'step_size': 8, 'gamma': 0.7846193322207391}. Best is trial 44 with value: 0.20906982817514627.[0m
[32m[I 2025-02-03 11:18:34,072][0m Trial 46 finished with value: 0.3085182971600196 and parameters: {'observation_period_num': 214, 'train_rates': 0.8622486680656293, 'learning_rate': 2.78676936142715e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.8372419250243676}. Best is trial 44 with value: 0.20906982817514627.[0m
[32m[I 2025-02-03 11:20:39,358][0m Trial 47 finished with value: 0.529317380972089 and parameters: {'observation_period_num': 126, 'train_rates': 0.7755495046026268, 'learning_rate': 0.0002972316135240612, 'batch_size': 70, 'step_size': 8, 'gamma': 0.7620187371651296}. Best is trial 44 with value: 0.20906982817514627.[0m
[32m[I 2025-02-03 11:23:01,010][0m Trial 48 finished with value: 0.20014347081744907 and parameters: {'observation_period_num': 89, 'train_rates': 0.8947915643735478, 'learning_rate': 0.00010866410533477106, 'batch_size': 27, 'step_size': 10, 'gamma': 0.7793386018946219}. Best is trial 48 with value: 0.20014347081744907.[0m
[32m[I 2025-02-03 11:25:22,448][0m Trial 49 finished with value: 0.1196346401322818 and parameters: {'observation_period_num': 87, 'train_rates': 0.950804806338593, 'learning_rate': 0.00010847916181310959, 'batch_size': 28, 'step_size': 15, 'gamma': 0.7762769483181596}. Best is trial 49 with value: 0.1196346401322818.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 11:25:22,454][0m A new study created in memory with name: no-name-edffcae1-a26a-4cb5-9ae6-03309fdb5790[0m
[32m[I 2025-02-03 11:27:41,410][0m Trial 0 finished with value: 1.244563994875976 and parameters: {'observation_period_num': 147, 'train_rates': 0.7006638596175496, 'learning_rate': 4.166329749967062e-06, 'batch_size': 129, 'step_size': 6, 'gamma': 0.9638097657111334}. Best is trial 0 with value: 1.244563994875976.[0m
[32m[I 2025-02-03 11:30:08,422][0m Trial 1 finished with value: 1.0146160899937007 and parameters: {'observation_period_num': 145, 'train_rates': 0.7086310369340252, 'learning_rate': 4.4281910866188654e-06, 'batch_size': 77, 'step_size': 12, 'gamma': 0.8812579949598045}. Best is trial 1 with value: 1.0146160899937007.[0m
[32m[I 2025-02-03 11:33:08,149][0m Trial 2 finished with value: 1.623919748756838 and parameters: {'observation_period_num': 190, 'train_rates': 0.6910058625754056, 'learning_rate': 6.840619319446499e-06, 'batch_size': 237, 'step_size': 7, 'gamma': 0.833128145689164}. Best is trial 1 with value: 1.0146160899937007.[0m
[32m[I 2025-02-03 11:35:48,476][0m Trial 3 finished with value: 0.3632668790952215 and parameters: {'observation_period_num': 30, 'train_rates': 0.8911823773591335, 'learning_rate': 5.360123337133043e-06, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8109392845836982}. Best is trial 3 with value: 0.3632668790952215.[0m
[32m[I 2025-02-03 11:37:37,652][0m Trial 4 finished with value: 1.041812624979718 and parameters: {'observation_period_num': 123, 'train_rates': 0.6864981977079195, 'learning_rate': 5.708333478273894e-05, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8526646703783256}. Best is trial 3 with value: 0.3632668790952215.[0m
[32m[I 2025-02-03 11:39:45,850][0m Trial 5 finished with value: 0.5140604536970764 and parameters: {'observation_period_num': 121, 'train_rates': 0.8634958609428545, 'learning_rate': 6.691694993352827e-06, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9061849639247861}. Best is trial 3 with value: 0.3632668790952215.[0m
[32m[I 2025-02-03 11:44:09,574][0m Trial 6 finished with value: 0.3554018400998896 and parameters: {'observation_period_num': 228, 'train_rates': 0.7893243613273982, 'learning_rate': 0.0001918633818125969, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8212212374222221}. Best is trial 6 with value: 0.3554018400998896.[0m
[32m[I 2025-02-03 11:47:30,443][0m Trial 7 finished with value: 1.1478622290526401 and parameters: {'observation_period_num': 200, 'train_rates': 0.7160670408574388, 'learning_rate': 0.0008794649838671803, 'batch_size': 167, 'step_size': 4, 'gamma': 0.9077307539960823}. Best is trial 6 with value: 0.3554018400998896.[0m
[32m[I 2025-02-03 11:48:29,468][0m Trial 8 finished with value: 0.42706206840846467 and parameters: {'observation_period_num': 51, 'train_rates': 0.8126257363297524, 'learning_rate': 9.370461160654596e-06, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9131571364948265}. Best is trial 6 with value: 0.3554018400998896.[0m
[32m[I 2025-02-03 11:49:07,106][0m Trial 9 finished with value: 0.31957396960004847 and parameters: {'observation_period_num': 6, 'train_rates': 0.8565133255321434, 'learning_rate': 0.000623982822515177, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8240043650981198}. Best is trial 9 with value: 0.31957396960004847.[0m
Early stopping at epoch 49
[32m[I 2025-02-03 11:49:21,525][0m Trial 10 finished with value: 0.5432897210121155 and parameters: {'observation_period_num': 6, 'train_rates': 0.9774091913939886, 'learning_rate': 0.000884966500542344, 'batch_size': 183, 'step_size': 1, 'gamma': 0.7517631300143023}. Best is trial 9 with value: 0.31957396960004847.[0m
[32m[I 2025-02-03 11:54:28,795][0m Trial 11 finished with value: 0.6416955805158356 and parameters: {'observation_period_num': 232, 'train_rates': 0.7962526548145397, 'learning_rate': 0.0001720381734967933, 'batch_size': 18, 'step_size': 15, 'gamma': 0.78487729666475}. Best is trial 9 with value: 0.31957396960004847.[0m
[32m[I 2025-02-03 11:55:41,790][0m Trial 12 finished with value: 1.081606413970954 and parameters: {'observation_period_num': 83, 'train_rates': 0.6001527420769404, 'learning_rate': 0.00020475667816659347, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8004119248076266}. Best is trial 9 with value: 0.31957396960004847.[0m
[32m[I 2025-02-03 12:00:44,185][0m Trial 13 finished with value: 0.19720662017799404 and parameters: {'observation_period_num': 239, 'train_rates': 0.8924416786944559, 'learning_rate': 0.00021960569038324842, 'batch_size': 114, 'step_size': 5, 'gamma': 0.7633885874609163}. Best is trial 13 with value: 0.19720662017799404.[0m
[32m[I 2025-02-03 12:02:10,840][0m Trial 14 finished with value: 0.3620530391855193 and parameters: {'observation_period_num': 82, 'train_rates': 0.9293874645870446, 'learning_rate': 4.989847188828947e-05, 'batch_size': 157, 'step_size': 5, 'gamma': 0.7712076450553269}. Best is trial 13 with value: 0.19720662017799404.[0m
Early stopping at epoch 75
[32m[I 2025-02-03 12:04:33,355][0m Trial 15 finished with value: 1.4408257478400122 and parameters: {'observation_period_num': 171, 'train_rates': 0.8580135608179402, 'learning_rate': 1.058169512985001e-06, 'batch_size': 123, 'step_size': 2, 'gamma': 0.7597926008903915}. Best is trial 13 with value: 0.19720662017799404.[0m
[32m[I 2025-02-03 12:09:48,447][0m Trial 16 finished with value: 0.1489098221063614 and parameters: {'observation_period_num': 246, 'train_rates': 0.9420112698650825, 'learning_rate': 0.00039685796003276333, 'batch_size': 202, 'step_size': 4, 'gamma': 0.846892405296241}. Best is trial 16 with value: 0.1489098221063614.[0m
[32m[I 2025-02-03 12:15:08,745][0m Trial 17 finished with value: 0.14857995510101318 and parameters: {'observation_period_num': 244, 'train_rates': 0.9834059480042635, 'learning_rate': 0.00034107382660227226, 'batch_size': 192, 'step_size': 6, 'gamma': 0.9891374324729496}. Best is trial 17 with value: 0.14857995510101318.[0m
[32m[I 2025-02-03 12:19:15,211][0m Trial 18 finished with value: 0.19878776371479034 and parameters: {'observation_period_num': 196, 'train_rates': 0.989077373959952, 'learning_rate': 0.0004488143630893174, 'batch_size': 209, 'step_size': 1, 'gamma': 0.9606044971377962}. Best is trial 17 with value: 0.14857995510101318.[0m
[32m[I 2025-02-03 12:24:29,799][0m Trial 19 finished with value: 0.16941213607788086 and parameters: {'observation_period_num': 246, 'train_rates': 0.9366215379436544, 'learning_rate': 7.94605702755649e-05, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9350421358584656}. Best is trial 17 with value: 0.14857995510101318.[0m
[32m[I 2025-02-03 12:28:53,720][0m Trial 20 finished with value: 0.10797877609729767 and parameters: {'observation_period_num': 216, 'train_rates': 0.9404379805723854, 'learning_rate': 0.0003633459231758779, 'batch_size': 196, 'step_size': 10, 'gamma': 0.9856811621505901}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:33:18,103][0m Trial 21 finished with value: 0.11810667812824249 and parameters: {'observation_period_num': 214, 'train_rates': 0.9498248012575099, 'learning_rate': 0.000447074700812475, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9876595849065114}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:37:52,703][0m Trial 22 finished with value: 0.23703138530254364 and parameters: {'observation_period_num': 218, 'train_rates': 0.9602031156468511, 'learning_rate': 2.2162919076000668e-05, 'batch_size': 175, 'step_size': 11, 'gamma': 0.9887865168034367}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:42:14,213][0m Trial 23 finished with value: 0.1451993879050978 and parameters: {'observation_period_num': 215, 'train_rates': 0.9095999696684316, 'learning_rate': 0.00011576839076413175, 'batch_size': 195, 'step_size': 14, 'gamma': 0.988697674252034}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:45:21,723][0m Trial 24 finished with value: 0.15079372761578397 and parameters: {'observation_period_num': 168, 'train_rates': 0.9063066869498158, 'learning_rate': 0.00013228772435664222, 'batch_size': 151, 'step_size': 15, 'gamma': 0.9608341848902229}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:49:21,260][0m Trial 25 finished with value: 0.26198578870042843 and parameters: {'observation_period_num': 212, 'train_rates': 0.8284128986151176, 'learning_rate': 8.503979264156257e-05, 'batch_size': 224, 'step_size': 13, 'gamma': 0.940963924007493}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:52:47,005][0m Trial 26 finished with value: 0.29756452631221586 and parameters: {'observation_period_num': 179, 'train_rates': 0.9176591180730458, 'learning_rate': 1.8079711890893793e-05, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9752223960494547}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:56:31,558][0m Trial 27 finished with value: 0.5499241704955423 and parameters: {'observation_period_num': 212, 'train_rates': 0.7622148459918372, 'learning_rate': 0.00011597434497021044, 'batch_size': 252, 'step_size': 10, 'gamma': 0.9351938695105741}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 12:59:43,294][0m Trial 28 finished with value: 0.12067952752113342 and parameters: {'observation_period_num': 164, 'train_rates': 0.9585370577410265, 'learning_rate': 0.0003167154343184712, 'batch_size': 206, 'step_size': 13, 'gamma': 0.9502895912077027}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:02:44,745][0m Trial 29 finished with value: 0.12023001909255981 and parameters: {'observation_period_num': 155, 'train_rates': 0.9546450973006865, 'learning_rate': 0.00035137941415489796, 'batch_size': 217, 'step_size': 11, 'gamma': 0.9488357415639983}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:05:23,406][0m Trial 30 finished with value: 0.17708271597662278 and parameters: {'observation_period_num': 146, 'train_rates': 0.878411269170408, 'learning_rate': 0.0005835315154005171, 'batch_size': 221, 'step_size': 10, 'gamma': 0.9709889468478953}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:08:30,500][0m Trial 31 finished with value: 0.12534813582897186 and parameters: {'observation_period_num': 161, 'train_rates': 0.9560021822801842, 'learning_rate': 0.00033047085733895875, 'batch_size': 210, 'step_size': 12, 'gamma': 0.949175105523145}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:10:25,692][0m Trial 32 finished with value: 0.13059350848197937 and parameters: {'observation_period_num': 108, 'train_rates': 0.9545826559143565, 'learning_rate': 0.000264298461027957, 'batch_size': 182, 'step_size': 11, 'gamma': 0.9218272117439518}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:13:28,398][0m Trial 33 finished with value: 0.1881660521030426 and parameters: {'observation_period_num': 153, 'train_rates': 0.9668470774568018, 'learning_rate': 0.0009898442571349287, 'batch_size': 239, 'step_size': 13, 'gamma': 0.8893956361642742}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:17:02,284][0m Trial 34 finished with value: 0.1357274055480957 and parameters: {'observation_period_num': 181, 'train_rates': 0.9320264303961336, 'learning_rate': 0.0006020821112010283, 'batch_size': 235, 'step_size': 9, 'gamma': 0.952257403071159}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:19:31,474][0m Trial 35 finished with value: 0.1804268905654498 and parameters: {'observation_period_num': 134, 'train_rates': 0.8975255344623133, 'learning_rate': 0.0004354481675908, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9752250788403241}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:23:22,511][0m Trial 36 finished with value: 0.6812853217124939 and parameters: {'observation_period_num': 191, 'train_rates': 0.9551369861732598, 'learning_rate': 2.6766025051714145e-06, 'batch_size': 134, 'step_size': 12, 'gamma': 0.8717945918923549}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:25:49,114][0m Trial 37 finished with value: 0.13644547591155226 and parameters: {'observation_period_num': 132, 'train_rates': 0.9221672993395624, 'learning_rate': 0.0002829933000248175, 'batch_size': 219, 'step_size': 9, 'gamma': 0.8916182705651247}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:27:10,979][0m Trial 38 finished with value: 1.0296509218368914 and parameters: {'observation_period_num': 103, 'train_rates': 0.6168392871770607, 'learning_rate': 3.5915033589393715e-05, 'batch_size': 234, 'step_size': 13, 'gamma': 0.9243675653753}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:31:07,339][0m Trial 39 finished with value: 0.16662919690359884 and parameters: {'observation_period_num': 204, 'train_rates': 0.878099129822292, 'learning_rate': 0.0001494001430882838, 'batch_size': 194, 'step_size': 10, 'gamma': 0.9727631389421674}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:34:28,490][0m Trial 40 finished with value: 0.27489726953472654 and parameters: {'observation_period_num': 186, 'train_rates': 0.847141950989494, 'learning_rate': 0.0006515863005740713, 'batch_size': 206, 'step_size': 8, 'gamma': 0.9507066300380582}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:37:29,117][0m Trial 41 finished with value: 0.1254856139421463 and parameters: {'observation_period_num': 157, 'train_rates': 0.9474202957929584, 'learning_rate': 0.00028512491648442084, 'batch_size': 213, 'step_size': 12, 'gamma': 0.9545719127099597}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:40:48,112][0m Trial 42 finished with value: 0.144896000623703 and parameters: {'observation_period_num': 169, 'train_rates': 0.97097568286088, 'learning_rate': 0.0004285709759739001, 'batch_size': 185, 'step_size': 11, 'gamma': 0.9411241512972568}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:43:55,095][0m Trial 43 finished with value: 0.352603554725647 and parameters: {'observation_period_num': 157, 'train_rates': 0.9879651178296287, 'learning_rate': 0.000717998051939509, 'batch_size': 168, 'step_size': 12, 'gamma': 0.9793304345423556}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:48:38,117][0m Trial 44 finished with value: 0.11662202328443527 and parameters: {'observation_period_num': 226, 'train_rates': 0.945262735952854, 'learning_rate': 0.00022221795091566593, 'batch_size': 231, 'step_size': 14, 'gamma': 0.9656856803693881}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:52:40,693][0m Trial 45 finished with value: 1.0505360270931512 and parameters: {'observation_period_num': 228, 'train_rates': 0.734701584156301, 'learning_rate': 0.0002073689395378889, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9683726113953081}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 13:57:11,345][0m Trial 46 finished with value: 0.1826348453760147 and parameters: {'observation_period_num': 224, 'train_rates': 0.9196113418363625, 'learning_rate': 7.156475647181402e-05, 'batch_size': 242, 'step_size': 13, 'gamma': 0.9238018082691868}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 14:01:17,111][0m Trial 47 finished with value: 0.16843476271027646 and parameters: {'observation_period_num': 203, 'train_rates': 0.8848705221998431, 'learning_rate': 0.0005068732971798221, 'batch_size': 228, 'step_size': 14, 'gamma': 0.9641280704787101}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 14:05:26,486][0m Trial 48 finished with value: 1.1342894439011404 and parameters: {'observation_period_num': 252, 'train_rates': 0.6482771288856042, 'learning_rate': 0.00023766367054303176, 'batch_size': 247, 'step_size': 15, 'gamma': 0.9040351011293536}. Best is trial 20 with value: 0.10797877609729767.[0m
[32m[I 2025-02-03 14:07:50,324][0m Trial 49 finished with value: 0.13614627718925476 and parameters: {'observation_period_num': 118, 'train_rates': 0.9407320885549473, 'learning_rate': 0.00018250196315079632, 'batch_size': 40, 'step_size': 8, 'gamma': 0.9821457937265762}. Best is trial 20 with value: 0.10797877609729767.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 14:07:50,331][0m A new study created in memory with name: no-name-52aacb86-2dbe-4fa0-9b21-c40241a5b650[0m
[32m[I 2025-02-03 14:09:19,952][0m Trial 0 finished with value: 0.41788721499064113 and parameters: {'observation_period_num': 33, 'train_rates': 0.948385109767862, 'learning_rate': 2.9588252826450215e-06, 'batch_size': 48, 'step_size': 6, 'gamma': 0.9140593629315321}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:10:47,427][0m Trial 1 finished with value: 1.24664209867669 and parameters: {'observation_period_num': 24, 'train_rates': 0.687018990590826, 'learning_rate': 0.0003762166734332963, 'batch_size': 40, 'step_size': 4, 'gamma': 0.7756646957745303}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:13:16,285][0m Trial 2 finished with value: 1.2366166785960235 and parameters: {'observation_period_num': 166, 'train_rates': 0.6437607765183193, 'learning_rate': 6.334706028401054e-06, 'batch_size': 126, 'step_size': 9, 'gamma': 0.9670607037026249}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:13:51,746][0m Trial 3 finished with value: 0.7499405140431907 and parameters: {'observation_period_num': 26, 'train_rates': 0.7657104266047704, 'learning_rate': 0.0001515599191965409, 'batch_size': 113, 'step_size': 7, 'gamma': 0.8109403593228852}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:15:11,189][0m Trial 4 finished with value: 1.2166007197662896 and parameters: {'observation_period_num': 83, 'train_rates': 0.7157202943598058, 'learning_rate': 5.10001835078838e-06, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9692459874614606}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:16:18,638][0m Trial 5 finished with value: 0.7455814132553702 and parameters: {'observation_period_num': 38, 'train_rates': 0.7128816014177406, 'learning_rate': 1.46772842118213e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.9769980380166625}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:17:40,712][0m Trial 6 finished with value: 0.5575555381628702 and parameters: {'observation_period_num': 81, 'train_rates': 0.8524405888223905, 'learning_rate': 1.3827730948579462e-05, 'batch_size': 150, 'step_size': 7, 'gamma': 0.7994240201883898}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:18:31,771][0m Trial 7 finished with value: 0.4210485931899812 and parameters: {'observation_period_num': 8, 'train_rates': 0.9023798674072074, 'learning_rate': 9.496461071673795e-06, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8128633745369114}. Best is trial 0 with value: 0.41788721499064113.[0m
[32m[I 2025-02-03 14:18:57,976][0m Trial 8 finished with value: 0.16488433127170024 and parameters: {'observation_period_num': 14, 'train_rates': 0.906137757897013, 'learning_rate': 0.00020795437873658, 'batch_size': 189, 'step_size': 9, 'gamma': 0.762860517737805}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:21:54,755][0m Trial 9 finished with value: 1.234903012552569 and parameters: {'observation_period_num': 159, 'train_rates': 0.8446970460628114, 'learning_rate': 1.0033668416674953e-06, 'batch_size': 120, 'step_size': 14, 'gamma': 0.8761591856389626}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:27:30,286][0m Trial 10 finished with value: 0.23478636145591736 and parameters: {'observation_period_num': 252, 'train_rates': 0.9887045325506301, 'learning_rate': 7.899153746521952e-05, 'batch_size': 232, 'step_size': 10, 'gamma': 0.7605573715461458}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:32:34,609][0m Trial 11 finished with value: 0.24568063020706177 and parameters: {'observation_period_num': 235, 'train_rates': 0.9785306668158491, 'learning_rate': 8.626182738772513e-05, 'batch_size': 224, 'step_size': 10, 'gamma': 0.7595091889516103}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:37:18,467][0m Trial 12 finished with value: 0.22960379719734192 and parameters: {'observation_period_num': 225, 'train_rates': 0.9139306180771524, 'learning_rate': 6.525843095386316e-05, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8425901709438528}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:41:12,667][0m Trial 13 finished with value: 0.18983737629401934 and parameters: {'observation_period_num': 200, 'train_rates': 0.8961368634831107, 'learning_rate': 0.0006304426369811402, 'batch_size': 189, 'step_size': 11, 'gamma': 0.8649310990196551}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:44:52,165][0m Trial 14 finished with value: 0.3324617545926169 and parameters: {'observation_period_num': 194, 'train_rates': 0.8386247263555954, 'learning_rate': 0.0008651778944611143, 'batch_size': 180, 'step_size': 15, 'gamma': 0.8827572926811525}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:46:30,589][0m Trial 15 finished with value: 0.29927972371163575 and parameters: {'observation_period_num': 99, 'train_rates': 0.798482403265483, 'learning_rate': 0.00034970986120258035, 'batch_size': 187, 'step_size': 8, 'gamma': 0.9133086746519897}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:48:54,847][0m Trial 16 finished with value: 0.2864257051240678 and parameters: {'observation_period_num': 132, 'train_rates': 0.894453147795469, 'learning_rate': 0.0009401075308814533, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9367776327746317}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:52:31,574][0m Trial 17 finished with value: 0.5063928331012901 and parameters: {'observation_period_num': 195, 'train_rates': 0.7833306611005769, 'learning_rate': 0.00028721634741428774, 'batch_size': 157, 'step_size': 11, 'gamma': 0.8482107510582053}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:54:44,200][0m Trial 18 finished with value: 0.563098132610321 and parameters: {'observation_period_num': 126, 'train_rates': 0.935982761137553, 'learning_rate': 3.8380535827318274e-05, 'batch_size': 208, 'step_size': 2, 'gamma': 0.851043759887198}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:55:46,009][0m Trial 19 finished with value: 0.2060134691235266 and parameters: {'observation_period_num': 64, 'train_rates': 0.8686715488495007, 'learning_rate': 0.0001723989218507437, 'batch_size': 165, 'step_size': 13, 'gamma': 0.7895248219950944}. Best is trial 8 with value: 0.16488433127170024.[0m
[32m[I 2025-02-03 14:59:36,308][0m Trial 20 finished with value: 0.13258209824562073 and parameters: {'observation_period_num': 189, 'train_rates': 0.9476206040700943, 'learning_rate': 0.000547938927251604, 'batch_size': 211, 'step_size': 9, 'gamma': 0.8340550271428604}. Best is trial 20 with value: 0.13258209824562073.[0m
[32m[I 2025-02-03 15:03:22,196][0m Trial 21 finished with value: 0.13471898436546326 and parameters: {'observation_period_num': 187, 'train_rates': 0.9460299729082325, 'learning_rate': 0.0005830354746092284, 'batch_size': 248, 'step_size': 9, 'gamma': 0.8275301096511631}. Best is trial 20 with value: 0.13258209824562073.[0m
[32m[I 2025-02-03 15:06:28,021][0m Trial 22 finished with value: 0.12177031487226486 and parameters: {'observation_period_num': 162, 'train_rates': 0.9542532897279636, 'learning_rate': 0.0004778478138179691, 'batch_size': 256, 'step_size': 9, 'gamma': 0.8264498373490333}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:09:44,760][0m Trial 23 finished with value: 0.13552269339561462 and parameters: {'observation_period_num': 171, 'train_rates': 0.9471413125318154, 'learning_rate': 0.000517543059673886, 'batch_size': 253, 'step_size': 8, 'gamma': 0.8298023873901547}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:12:27,625][0m Trial 24 finished with value: 0.17376957833766937 and parameters: {'observation_period_num': 141, 'train_rates': 0.9631495332564526, 'learning_rate': 0.0001243100206376797, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8262865505662278}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:16:11,103][0m Trial 25 finished with value: 0.17324109375476837 and parameters: {'observation_period_num': 181, 'train_rates': 0.9890632969208701, 'learning_rate': 0.0005118265173659903, 'batch_size': 216, 'step_size': 4, 'gamma': 0.8924374571647453}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:20:32,553][0m Trial 26 finished with value: 0.35252031683921814 and parameters: {'observation_period_num': 209, 'train_rates': 0.933077656993211, 'learning_rate': 3.908881098506544e-05, 'batch_size': 241, 'step_size': 7, 'gamma': 0.826534304164755}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:24:16,267][0m Trial 27 finished with value: 0.2375774861885661 and parameters: {'observation_period_num': 152, 'train_rates': 0.8816969104006389, 'learning_rate': 0.0002570429485349064, 'batch_size': 19, 'step_size': 10, 'gamma': 0.7873606091668104}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:26:12,623][0m Trial 28 finished with value: 0.680532519371455 and parameters: {'observation_period_num': 115, 'train_rates': 0.8153591400176078, 'learning_rate': 0.000985269511824287, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8055032931420114}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:30:52,836][0m Trial 29 finished with value: 0.12951773405075073 and parameters: {'observation_period_num': 217, 'train_rates': 0.9656175939470382, 'learning_rate': 0.0005239879817385178, 'batch_size': 246, 'step_size': 6, 'gamma': 0.861617754203469}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:35:36,361][0m Trial 30 finished with value: 0.3382679522037506 and parameters: {'observation_period_num': 219, 'train_rates': 0.9596719968811729, 'learning_rate': 2.327770093087684e-05, 'batch_size': 226, 'step_size': 6, 'gamma': 0.8979148762169262}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:39:03,270][0m Trial 31 finished with value: 0.15610332787036896 and parameters: {'observation_period_num': 180, 'train_rates': 0.9317129871140298, 'learning_rate': 0.0005074246172613378, 'batch_size': 254, 'step_size': 6, 'gamma': 0.8618210190974823}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:44:17,428][0m Trial 32 finished with value: 0.14622792601585388 and parameters: {'observation_period_num': 242, 'train_rates': 0.9650419734371481, 'learning_rate': 0.0003774164800984149, 'batch_size': 243, 'step_size': 5, 'gamma': 0.8281922725990856}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:47:31,669][0m Trial 33 finished with value: 0.987613047664978 and parameters: {'observation_period_num': 212, 'train_rates': 0.6102316122688948, 'learning_rate': 0.000623273481061355, 'batch_size': 218, 'step_size': 8, 'gamma': 0.8444140827973162}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:51:02,098][0m Trial 34 finished with value: 0.19534043967723846 and parameters: {'observation_period_num': 184, 'train_rates': 0.9300697949212571, 'learning_rate': 0.00012187252245029322, 'batch_size': 240, 'step_size': 9, 'gamma': 0.8549193611756538}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:53:56,252][0m Trial 35 finished with value: 0.13571369647979736 and parameters: {'observation_period_num': 152, 'train_rates': 0.9557251018678538, 'learning_rate': 0.000340635966877647, 'batch_size': 203, 'step_size': 7, 'gamma': 0.8182020233515851}. Best is trial 22 with value: 0.12177031487226486.[0m
Early stopping at epoch 54
[32m[I 2025-02-03 15:55:26,956][0m Trial 36 finished with value: 0.8924791724593552 and parameters: {'observation_period_num': 169, 'train_rates': 0.7381443007991582, 'learning_rate': 0.0002575287596489017, 'batch_size': 254, 'step_size': 1, 'gamma': 0.7793277866281592}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 15:59:20,284][0m Trial 37 finished with value: 0.24277977742762727 and parameters: {'observation_period_num': 202, 'train_rates': 0.8719244525771241, 'learning_rate': 0.0006950695300224738, 'batch_size': 227, 'step_size': 5, 'gamma': 0.8382936322224892}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:04:02,189][0m Trial 38 finished with value: 0.17881185904277108 and parameters: {'observation_period_num': 229, 'train_rates': 0.9171964776934811, 'learning_rate': 0.00041055140886417255, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9419287363760785}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:06:16,481][0m Trial 39 finished with value: 1.3884352105180013 and parameters: {'observation_period_num': 143, 'train_rates': 0.6545144198409636, 'learning_rate': 2.6846673436844976e-06, 'batch_size': 134, 'step_size': 10, 'gamma': 0.7948053206841652}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:10:00,946][0m Trial 40 finished with value: 0.1863822489976883 and parameters: {'observation_period_num': 188, 'train_rates': 0.9707739803934399, 'learning_rate': 0.0001951520496585825, 'batch_size': 244, 'step_size': 4, 'gamma': 0.8698178198256492}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:13:11,705][0m Trial 41 finished with value: 0.13427093625068665 and parameters: {'observation_period_num': 166, 'train_rates': 0.949239346522495, 'learning_rate': 0.0004915562192726406, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8322519358213218}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:16:24,396][0m Trial 42 finished with value: 0.15933987498283386 and parameters: {'observation_period_num': 168, 'train_rates': 0.9421436404318906, 'learning_rate': 0.0007326760685038837, 'batch_size': 233, 'step_size': 9, 'gamma': 0.816204458864087}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:20:56,356][0m Trial 43 finished with value: 0.14756686985492706 and parameters: {'observation_period_num': 213, 'train_rates': 0.9778646853443795, 'learning_rate': 0.0004702156274480063, 'batch_size': 217, 'step_size': 7, 'gamma': 0.8052644849370743}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:23:52,401][0m Trial 44 finished with value: 0.17806215584278107 and parameters: {'observation_period_num': 158, 'train_rates': 0.9153797463715072, 'learning_rate': 0.00022482779145192503, 'batch_size': 245, 'step_size': 9, 'gamma': 0.8367473934371888}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:27:22,389][0m Trial 45 finished with value: 0.15107306838035583 and parameters: {'observation_period_num': 175, 'train_rates': 0.9512301852763997, 'learning_rate': 0.0001585352307530106, 'batch_size': 228, 'step_size': 8, 'gamma': 0.8774162163464961}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:31:18,127][0m Trial 46 finished with value: 0.22041856586558264 and parameters: {'observation_period_num': 202, 'train_rates': 0.8845560320331586, 'learning_rate': 0.0006861207126412248, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8594708209836314}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:33:28,319][0m Trial 47 finished with value: 0.21013640970625777 and parameters: {'observation_period_num': 117, 'train_rates': 0.9172534980614173, 'learning_rate': 0.00010157151411566965, 'batch_size': 107, 'step_size': 6, 'gamma': 0.7734068734955853}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:37:33,444][0m Trial 48 finished with value: 0.18368466198444366 and parameters: {'observation_period_num': 189, 'train_rates': 0.9891809078019553, 'learning_rate': 0.00030686844175217473, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8129458263347779}. Best is trial 22 with value: 0.12177031487226486.[0m
[32m[I 2025-02-03 16:40:25,313][0m Trial 49 finished with value: 0.24289960226098312 and parameters: {'observation_period_num': 161, 'train_rates': 0.8526242362643774, 'learning_rate': 0.00039534409236303655, 'batch_size': 235, 'step_size': 11, 'gamma': 0.8356097675902455}. Best is trial 22 with value: 0.12177031487226486.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_BA_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 8, 'train_rates': 0.9435997685240318, 'learning_rate': 0.0001902181604655992, 'batch_size': 18, 'step_size': 4, 'gamma': 0.839658720454682}
Epoch 1/300, trend Loss: 0.5286 | 0.3882
Epoch 2/300, trend Loss: 0.4388 | 0.3293
Epoch 3/300, trend Loss: 0.4162 | 0.3177
Epoch 4/300, trend Loss: 0.3911 | 0.2968
Epoch 5/300, trend Loss: 0.4066 | 0.2749
Epoch 6/300, trend Loss: 0.3507 | 0.2435
Epoch 7/300, trend Loss: 0.3166 | 0.2458
Epoch 8/300, trend Loss: 0.2698 | 0.2306
Epoch 9/300, trend Loss: 0.2542 | 0.1961
Epoch 10/300, trend Loss: 0.2435 | 0.1958
Epoch 11/300, trend Loss: 0.2303 | 0.1914
Epoch 12/300, trend Loss: 0.2182 | 0.1880
Epoch 13/300, trend Loss: 0.2135 | 0.1735
Epoch 14/300, trend Loss: 0.2001 | 0.1683
Epoch 15/300, trend Loss: 0.1919 | 0.1656
Epoch 16/300, trend Loss: 0.1819 | 0.1656
Epoch 17/300, trend Loss: 0.1812 | 0.1753
Epoch 18/300, trend Loss: 0.1733 | 0.1521
Epoch 19/300, trend Loss: 0.1728 | 0.1418
Epoch 20/300, trend Loss: 0.1642 | 0.1383
Epoch 21/300, trend Loss: 0.1595 | 0.1416
Epoch 22/300, trend Loss: 0.1574 | 0.1401
Epoch 23/300, trend Loss: 0.1516 | 0.1313
Epoch 24/300, trend Loss: 0.1467 | 0.1244
Epoch 25/300, trend Loss: 0.1433 | 0.1283
Epoch 26/300, trend Loss: 0.1408 | 0.1268
Epoch 27/300, trend Loss: 0.1367 | 0.1213
Epoch 28/300, trend Loss: 0.1349 | 0.1156
Epoch 29/300, trend Loss: 0.1329 | 0.1169
Epoch 30/300, trend Loss: 0.1312 | 0.1185
Epoch 31/300, trend Loss: 0.1294 | 0.1170
Epoch 32/300, trend Loss: 0.1278 | 0.1132
Epoch 33/300, trend Loss: 0.1268 | 0.1121
Epoch 34/300, trend Loss: 0.1249 | 0.1133
Epoch 35/300, trend Loss: 0.1245 | 0.1128
Epoch 36/300, trend Loss: 0.1234 | 0.1110
Epoch 37/300, trend Loss: 0.1224 | 0.1103
Epoch 38/300, trend Loss: 0.1218 | 0.1091
Epoch 39/300, trend Loss: 0.1213 | 0.1094
Epoch 40/300, trend Loss: 0.1204 | 0.1089
Epoch 41/300, trend Loss: 0.1196 | 0.1082
Epoch 42/300, trend Loss: 0.1192 | 0.1081
Epoch 43/300, trend Loss: 0.1189 | 0.1080
Epoch 44/300, trend Loss: 0.1185 | 0.1070
Epoch 45/300, trend Loss: 0.1177 | 0.1063
Epoch 46/300, trend Loss: 0.1178 | 0.1069
Epoch 47/300, trend Loss: 0.1169 | 0.1063
Epoch 48/300, trend Loss: 0.1172 | 0.1058
Epoch 49/300, trend Loss: 0.1170 | 0.1054
Epoch 50/300, trend Loss: 0.1163 | 0.1048
Epoch 51/300, trend Loss: 0.1162 | 0.1051
Epoch 52/300, trend Loss: 0.1164 | 0.1050
Epoch 53/300, trend Loss: 0.1152 | 0.1048
Epoch 54/300, trend Loss: 0.1157 | 0.1045
Epoch 55/300, trend Loss: 0.1148 | 0.1044
Epoch 56/300, trend Loss: 0.1151 | 0.1044
Epoch 57/300, trend Loss: 0.1151 | 0.1044
Epoch 58/300, trend Loss: 0.1141 | 0.1041
Epoch 59/300, trend Loss: 0.1147 | 0.1038
Epoch 60/300, trend Loss: 0.1140 | 0.1038
Epoch 61/300, trend Loss: 0.1145 | 0.1037
Epoch 62/300, trend Loss: 0.1151 | 0.1036
Epoch 63/300, trend Loss: 0.1147 | 0.1035
Epoch 64/300, trend Loss: 0.1144 | 0.1035
Epoch 65/300, trend Loss: 0.1145 | 0.1034
Epoch 66/300, trend Loss: 0.1134 | 0.1033
Epoch 67/300, trend Loss: 0.1140 | 0.1033
Epoch 68/300, trend Loss: 0.1135 | 0.1032
Epoch 69/300, trend Loss: 0.1137 | 0.1032
Epoch 70/300, trend Loss: 0.1136 | 0.1033
Epoch 71/300, trend Loss: 0.1136 | 0.1033
Epoch 72/300, trend Loss: 0.1132 | 0.1031
Epoch 73/300, trend Loss: 0.1137 | 0.1030
Epoch 74/300, trend Loss: 0.1142 | 0.1030
Epoch 75/300, trend Loss: 0.1140 | 0.1030
Epoch 76/300, trend Loss: 0.1131 | 0.1030
Epoch 77/300, trend Loss: 0.1137 | 0.1029
Epoch 78/300, trend Loss: 0.1139 | 0.1029
Epoch 79/300, trend Loss: 0.1133 | 0.1029
Epoch 80/300, trend Loss: 0.1134 | 0.1029
Epoch 81/300, trend Loss: 0.1133 | 0.1029
Epoch 82/300, trend Loss: 0.1134 | 0.1029
Epoch 83/300, trend Loss: 0.1131 | 0.1029
Epoch 84/300, trend Loss: 0.1137 | 0.1029
Epoch 85/300, trend Loss: 0.1133 | 0.1029
Epoch 86/300, trend Loss: 0.1131 | 0.1029
Epoch 87/300, trend Loss: 0.1136 | 0.1029
Epoch 88/300, trend Loss: 0.1132 | 0.1029
Epoch 89/300, trend Loss: 0.1134 | 0.1029
Epoch 90/300, trend Loss: 0.1131 | 0.1029
Epoch 91/300, trend Loss: 0.1131 | 0.1029
Epoch 92/300, trend Loss: 0.1132 | 0.1029
Epoch 93/300, trend Loss: 0.1128 | 0.1029
Epoch 94/300, trend Loss: 0.1139 | 0.1029
Epoch 95/300, trend Loss: 0.1133 | 0.1029
Epoch 96/300, trend Loss: 0.1131 | 0.1029
Epoch 97/300, trend Loss: 0.1136 | 0.1029
Epoch 98/300, trend Loss: 0.1128 | 0.1028
Epoch 99/300, trend Loss: 0.1130 | 0.1028
Epoch 100/300, trend Loss: 0.1137 | 0.1028
Epoch 101/300, trend Loss: 0.1134 | 0.1028
Epoch 102/300, trend Loss: 0.1136 | 0.1028
Epoch 103/300, trend Loss: 0.1132 | 0.1028
Epoch 104/300, trend Loss: 0.1131 | 0.1028
Epoch 105/300, trend Loss: 0.1126 | 0.1028
Epoch 106/300, trend Loss: 0.1135 | 0.1028
Epoch 107/300, trend Loss: 0.1130 | 0.1028
Epoch 108/300, trend Loss: 0.1129 | 0.1028
Epoch 109/300, trend Loss: 0.1128 | 0.1028
Epoch 110/300, trend Loss: 0.1136 | 0.1028
Epoch 111/300, trend Loss: 0.1123 | 0.1028
Epoch 112/300, trend Loss: 0.1136 | 0.1028
Epoch 113/300, trend Loss: 0.1131 | 0.1028
Epoch 114/300, trend Loss: 0.1131 | 0.1028
Epoch 115/300, trend Loss: 0.1127 | 0.1028
Epoch 116/300, trend Loss: 0.1136 | 0.1028
Epoch 117/300, trend Loss: 0.1130 | 0.1028
Epoch 118/300, trend Loss: 0.1142 | 0.1028
Epoch 119/300, trend Loss: 0.1134 | 0.1028
Epoch 120/300, trend Loss: 0.1133 | 0.1028
Epoch 121/300, trend Loss: 0.1135 | 0.1028
Epoch 122/300, trend Loss: 0.1129 | 0.1028
Epoch 123/300, trend Loss: 0.1123 | 0.1028
Epoch 124/300, trend Loss: 0.1134 | 0.1028
Epoch 125/300, trend Loss: 0.1131 | 0.1028
Epoch 126/300, trend Loss: 0.1141 | 0.1028
Epoch 127/300, trend Loss: 0.1127 | 0.1028
Epoch 128/300, trend Loss: 0.1133 | 0.1028
Epoch 129/300, trend Loss: 0.1129 | 0.1028
Epoch 130/300, trend Loss: 0.1134 | 0.1028
Epoch 131/300, trend Loss: 0.1134 | 0.1028
Epoch 132/300, trend Loss: 0.1128 | 0.1028
Epoch 133/300, trend Loss: 0.1133 | 0.1028
Epoch 134/300, trend Loss: 0.1132 | 0.1028
Epoch 135/300, trend Loss: 0.1135 | 0.1028
Epoch 136/300, trend Loss: 0.1130 | 0.1028
Epoch 137/300, trend Loss: 0.1137 | 0.1028
Epoch 138/300, trend Loss: 0.1134 | 0.1028
Epoch 139/300, trend Loss: 0.1135 | 0.1028
Epoch 140/300, trend Loss: 0.1139 | 0.1028
Epoch 141/300, trend Loss: 0.1137 | 0.1028
Epoch 142/300, trend Loss: 0.1131 | 0.1028
Epoch 143/300, trend Loss: 0.1134 | 0.1028
Epoch 144/300, trend Loss: 0.1125 | 0.1028
Epoch 145/300, trend Loss: 0.1135 | 0.1028
Epoch 146/300, trend Loss: 0.1132 | 0.1028
Epoch 147/300, trend Loss: 0.1136 | 0.1028
Epoch 148/300, trend Loss: 0.1137 | 0.1028
Epoch 149/300, trend Loss: 0.1133 | 0.1028
Epoch 150/300, trend Loss: 0.1130 | 0.1028
Epoch 151/300, trend Loss: 0.1140 | 0.1028
Epoch 152/300, trend Loss: 0.1124 | 0.1028
Epoch 153/300, trend Loss: 0.1134 | 0.1028
Epoch 154/300, trend Loss: 0.1135 | 0.1028
Epoch 155/300, trend Loss: 0.1137 | 0.1028
Epoch 156/300, trend Loss: 0.1130 | 0.1028
Epoch 157/300, trend Loss: 0.1130 | 0.1028
Epoch 158/300, trend Loss: 0.1132 | 0.1028
Epoch 159/300, trend Loss: 0.1134 | 0.1028
Epoch 160/300, trend Loss: 0.1134 | 0.1028
Epoch 161/300, trend Loss: 0.1128 | 0.1028
Epoch 162/300, trend Loss: 0.1131 | 0.1028
Epoch 163/300, trend Loss: 0.1130 | 0.1028
Epoch 164/300, trend Loss: 0.1128 | 0.1028
Epoch 165/300, trend Loss: 0.1137 | 0.1028
Epoch 166/300, trend Loss: 0.1131 | 0.1028
Epoch 167/300, trend Loss: 0.1136 | 0.1028
Epoch 168/300, trend Loss: 0.1128 | 0.1028
Epoch 169/300, trend Loss: 0.1130 | 0.1028
Epoch 170/300, trend Loss: 0.1129 | 0.1028
Epoch 171/300, trend Loss: 0.1129 | 0.1028
Epoch 172/300, trend Loss: 0.1131 | 0.1028
Epoch 173/300, trend Loss: 0.1130 | 0.1028
Epoch 174/300, trend Loss: 0.1128 | 0.1028
Epoch 175/300, trend Loss: 0.1140 | 0.1028
Epoch 176/300, trend Loss: 0.1133 | 0.1028
Epoch 177/300, trend Loss: 0.1130 | 0.1028
Epoch 178/300, trend Loss: 0.1130 | 0.1028
Epoch 179/300, trend Loss: 0.1138 | 0.1028
Epoch 180/300, trend Loss: 0.1135 | 0.1028
Epoch 181/300, trend Loss: 0.1138 | 0.1028
Epoch 182/300, trend Loss: 0.1134 | 0.1028
Epoch 183/300, trend Loss: 0.1126 | 0.1028
Epoch 184/300, trend Loss: 0.1130 | 0.1028
Epoch 185/300, trend Loss: 0.1137 | 0.1028
Epoch 186/300, trend Loss: 0.1133 | 0.1028
Epoch 187/300, trend Loss: 0.1128 | 0.1028
Epoch 188/300, trend Loss: 0.1134 | 0.1028
Epoch 189/300, trend Loss: 0.1128 | 0.1028
Epoch 190/300, trend Loss: 0.1132 | 0.1028
Epoch 191/300, trend Loss: 0.1127 | 0.1028
Epoch 192/300, trend Loss: 0.1130 | 0.1028
Epoch 193/300, trend Loss: 0.1132 | 0.1028
Epoch 194/300, trend Loss: 0.1144 | 0.1028
Epoch 195/300, trend Loss: 0.1133 | 0.1028
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 17, 'train_rates': 0.9597906513130275, 'learning_rate': 6.41794912887292e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8741560057977434}
Epoch 1/300, seasonal_0 Loss: 0.8550 | 0.6664
Epoch 2/300, seasonal_0 Loss: 0.6203 | 0.4684
Epoch 3/300, seasonal_0 Loss: 0.4479 | 0.3895
Epoch 4/300, seasonal_0 Loss: 0.3839 | 0.3347
Epoch 5/300, seasonal_0 Loss: 0.3493 | 0.3046
Epoch 6/300, seasonal_0 Loss: 0.3130 | 0.2767
Epoch 7/300, seasonal_0 Loss: 0.2960 | 0.2820
Epoch 8/300, seasonal_0 Loss: 0.2742 | 0.2483
Epoch 9/300, seasonal_0 Loss: 0.2608 | 0.2252
Epoch 10/300, seasonal_0 Loss: 0.2457 | 0.2271
Epoch 11/300, seasonal_0 Loss: 0.2518 | 0.2232
Epoch 12/300, seasonal_0 Loss: 0.2404 | 0.2083
Epoch 13/300, seasonal_0 Loss: 0.2414 | 0.2074
Epoch 14/300, seasonal_0 Loss: 0.2388 | 0.1898
Epoch 15/300, seasonal_0 Loss: 0.2311 | 0.1867
Epoch 16/300, seasonal_0 Loss: 0.2166 | 0.1949
Epoch 17/300, seasonal_0 Loss: 0.2226 | 0.2015
Epoch 18/300, seasonal_0 Loss: 0.2082 | 0.1898
Epoch 19/300, seasonal_0 Loss: 0.1976 | 0.1645
Epoch 20/300, seasonal_0 Loss: 0.1874 | 0.1561
Epoch 21/300, seasonal_0 Loss: 0.1832 | 0.1755
Epoch 22/300, seasonal_0 Loss: 0.1780 | 0.1709
Epoch 23/300, seasonal_0 Loss: 0.1753 | 0.1518
Epoch 24/300, seasonal_0 Loss: 0.1701 | 0.1407
Epoch 25/300, seasonal_0 Loss: 0.1702 | 0.1548
Epoch 26/300, seasonal_0 Loss: 0.1644 | 0.1558
Epoch 27/300, seasonal_0 Loss: 0.1645 | 0.1445
Epoch 28/300, seasonal_0 Loss: 0.1584 | 0.1352
Epoch 29/300, seasonal_0 Loss: 0.1572 | 0.1356
Epoch 30/300, seasonal_0 Loss: 0.1544 | 0.1496
Epoch 31/300, seasonal_0 Loss: 0.1564 | 0.1438
Epoch 32/300, seasonal_0 Loss: 0.1552 | 0.1368
Epoch 33/300, seasonal_0 Loss: 0.1525 | 0.1262
Epoch 34/300, seasonal_0 Loss: 0.1467 | 0.1325
Epoch 35/300, seasonal_0 Loss: 0.1464 | 0.1448
Epoch 36/300, seasonal_0 Loss: 0.1460 | 0.1326
Epoch 37/300, seasonal_0 Loss: 0.1466 | 0.1195
Epoch 38/300, seasonal_0 Loss: 0.1408 | 0.1237
Epoch 39/300, seasonal_0 Loss: 0.1399 | 0.1324
Epoch 40/300, seasonal_0 Loss: 0.1352 | 0.1238
Epoch 41/300, seasonal_0 Loss: 0.1349 | 0.1195
Epoch 42/300, seasonal_0 Loss: 0.1342 | 0.1199
Epoch 43/300, seasonal_0 Loss: 0.1342 | 0.1300
Epoch 44/300, seasonal_0 Loss: 0.1300 | 0.1208
Epoch 45/300, seasonal_0 Loss: 0.1304 | 0.1172
Epoch 46/300, seasonal_0 Loss: 0.1278 | 0.1169
Epoch 47/300, seasonal_0 Loss: 0.1314 | 0.1285
Epoch 48/300, seasonal_0 Loss: 0.1265 | 0.1322
Epoch 49/300, seasonal_0 Loss: 0.1272 | 0.1138
Epoch 50/300, seasonal_0 Loss: 0.1238 | 0.1115
Epoch 51/300, seasonal_0 Loss: 0.1238 | 0.1181
Epoch 52/300, seasonal_0 Loss: 0.1221 | 0.1229
Epoch 53/300, seasonal_0 Loss: 0.1189 | 0.1140
Epoch 54/300, seasonal_0 Loss: 0.1188 | 0.1109
Epoch 55/300, seasonal_0 Loss: 0.1154 | 0.1060
Epoch 56/300, seasonal_0 Loss: 0.1153 | 0.1153
Epoch 57/300, seasonal_0 Loss: 0.1119 | 0.1115
Epoch 58/300, seasonal_0 Loss: 0.1121 | 0.1092
Epoch 59/300, seasonal_0 Loss: 0.1102 | 0.1062
Epoch 60/300, seasonal_0 Loss: 0.1101 | 0.1054
Epoch 61/300, seasonal_0 Loss: 0.1097 | 0.1109
Epoch 62/300, seasonal_0 Loss: 0.1094 | 0.1127
Epoch 63/300, seasonal_0 Loss: 0.1075 | 0.1080
Epoch 64/300, seasonal_0 Loss: 0.1074 | 0.1003
Epoch 65/300, seasonal_0 Loss: 0.1061 | 0.1021
Epoch 66/300, seasonal_0 Loss: 0.1059 | 0.1089
Epoch 67/300, seasonal_0 Loss: 0.1061 | 0.1130
Epoch 68/300, seasonal_0 Loss: 0.1041 | 0.1031
Epoch 69/300, seasonal_0 Loss: 0.1039 | 0.0987
Epoch 70/300, seasonal_0 Loss: 0.1025 | 0.1005
Epoch 71/300, seasonal_0 Loss: 0.1027 | 0.1073
Epoch 72/300, seasonal_0 Loss: 0.1008 | 0.1102
Epoch 73/300, seasonal_0 Loss: 0.1018 | 0.1008
Epoch 74/300, seasonal_0 Loss: 0.1009 | 0.0964
Epoch 75/300, seasonal_0 Loss: 0.0984 | 0.0999
Epoch 76/300, seasonal_0 Loss: 0.0985 | 0.1060
Epoch 77/300, seasonal_0 Loss: 0.0983 | 0.1058
Epoch 78/300, seasonal_0 Loss: 0.0973 | 0.0996
Epoch 79/300, seasonal_0 Loss: 0.0971 | 0.0970
Epoch 80/300, seasonal_0 Loss: 0.0964 | 0.0989
Epoch 81/300, seasonal_0 Loss: 0.0961 | 0.1027
Epoch 82/300, seasonal_0 Loss: 0.0953 | 0.1018
Epoch 83/300, seasonal_0 Loss: 0.0950 | 0.0995
Epoch 84/300, seasonal_0 Loss: 0.0945 | 0.0978
Epoch 85/300, seasonal_0 Loss: 0.0940 | 0.0987
Epoch 86/300, seasonal_0 Loss: 0.0932 | 0.0997
Epoch 87/300, seasonal_0 Loss: 0.0924 | 0.0996
Epoch 88/300, seasonal_0 Loss: 0.0922 | 0.1001
Epoch 89/300, seasonal_0 Loss: 0.0916 | 0.0983
Epoch 90/300, seasonal_0 Loss: 0.0913 | 0.0963
Epoch 91/300, seasonal_0 Loss: 0.0914 | 0.0982
Epoch 92/300, seasonal_0 Loss: 0.0905 | 0.0993
Epoch 93/300, seasonal_0 Loss: 0.0907 | 0.0992
Epoch 94/300, seasonal_0 Loss: 0.0896 | 0.0949
Epoch 95/300, seasonal_0 Loss: 0.0892 | 0.0951
Epoch 96/300, seasonal_0 Loss: 0.0893 | 0.0984
Epoch 97/300, seasonal_0 Loss: 0.0891 | 0.0961
Epoch 98/300, seasonal_0 Loss: 0.0884 | 0.0966
Epoch 99/300, seasonal_0 Loss: 0.0883 | 0.0961
Epoch 100/300, seasonal_0 Loss: 0.0880 | 0.0965
Epoch 101/300, seasonal_0 Loss: 0.0875 | 0.0976
Epoch 102/300, seasonal_0 Loss: 0.0874 | 0.0974
Epoch 103/300, seasonal_0 Loss: 0.0876 | 0.0946
Epoch 104/300, seasonal_0 Loss: 0.0865 | 0.0949
Epoch 105/300, seasonal_0 Loss: 0.0865 | 0.0961
Epoch 106/300, seasonal_0 Loss: 0.0868 | 0.0964
Epoch 107/300, seasonal_0 Loss: 0.0856 | 0.0959
Epoch 108/300, seasonal_0 Loss: 0.0858 | 0.0959
Epoch 109/300, seasonal_0 Loss: 0.0858 | 0.0963
Epoch 110/300, seasonal_0 Loss: 0.0854 | 0.0961
Epoch 111/300, seasonal_0 Loss: 0.0851 | 0.0954
Epoch 112/300, seasonal_0 Loss: 0.0848 | 0.0961
Epoch 113/300, seasonal_0 Loss: 0.0844 | 0.0947
Epoch 114/300, seasonal_0 Loss: 0.0842 | 0.0950
Epoch 115/300, seasonal_0 Loss: 0.0843 | 0.0951
Epoch 116/300, seasonal_0 Loss: 0.0837 | 0.0945
Epoch 117/300, seasonal_0 Loss: 0.0841 | 0.0943
Epoch 118/300, seasonal_0 Loss: 0.0843 | 0.0953
Epoch 119/300, seasonal_0 Loss: 0.0832 | 0.0941
Epoch 120/300, seasonal_0 Loss: 0.0833 | 0.0945
Epoch 121/300, seasonal_0 Loss: 0.0828 | 0.0954
Epoch 122/300, seasonal_0 Loss: 0.0834 | 0.0946
Epoch 123/300, seasonal_0 Loss: 0.0826 | 0.0949
Epoch 124/300, seasonal_0 Loss: 0.0830 | 0.0936
Epoch 125/300, seasonal_0 Loss: 0.0828 | 0.0939
Epoch 126/300, seasonal_0 Loss: 0.0825 | 0.0943
Epoch 127/300, seasonal_0 Loss: 0.0818 | 0.0932
Epoch 128/300, seasonal_0 Loss: 0.0820 | 0.0939
Epoch 129/300, seasonal_0 Loss: 0.0819 | 0.0938
Epoch 130/300, seasonal_0 Loss: 0.0817 | 0.0941
Epoch 131/300, seasonal_0 Loss: 0.0820 | 0.0936
Epoch 132/300, seasonal_0 Loss: 0.0813 | 0.0930
Epoch 133/300, seasonal_0 Loss: 0.0815 | 0.0939
Epoch 134/300, seasonal_0 Loss: 0.0816 | 0.0940
Epoch 135/300, seasonal_0 Loss: 0.0818 | 0.0939
Epoch 136/300, seasonal_0 Loss: 0.0809 | 0.0930
Epoch 137/300, seasonal_0 Loss: 0.0806 | 0.0939
Epoch 138/300, seasonal_0 Loss: 0.0810 | 0.0933
Epoch 139/300, seasonal_0 Loss: 0.0807 | 0.0931
Epoch 140/300, seasonal_0 Loss: 0.0803 | 0.0940
Epoch 141/300, seasonal_0 Loss: 0.0807 | 0.0940
Epoch 142/300, seasonal_0 Loss: 0.0810 | 0.0936
Epoch 143/300, seasonal_0 Loss: 0.0803 | 0.0937
Epoch 144/300, seasonal_0 Loss: 0.0800 | 0.0937
Epoch 145/300, seasonal_0 Loss: 0.0799 | 0.0933
Epoch 146/300, seasonal_0 Loss: 0.0796 | 0.0934
Epoch 147/300, seasonal_0 Loss: 0.0793 | 0.0932
Epoch 148/300, seasonal_0 Loss: 0.0800 | 0.0928
Epoch 149/300, seasonal_0 Loss: 0.0796 | 0.0931
Epoch 150/300, seasonal_0 Loss: 0.0798 | 0.0938
Epoch 151/300, seasonal_0 Loss: 0.0794 | 0.0939
Epoch 152/300, seasonal_0 Loss: 0.0791 | 0.0933
Epoch 153/300, seasonal_0 Loss: 0.0792 | 0.0933
Epoch 154/300, seasonal_0 Loss: 0.0789 | 0.0930
Epoch 155/300, seasonal_0 Loss: 0.0791 | 0.0929
Epoch 156/300, seasonal_0 Loss: 0.0794 | 0.0931
Epoch 157/300, seasonal_0 Loss: 0.0791 | 0.0931
Epoch 158/300, seasonal_0 Loss: 0.0783 | 0.0929
Epoch 159/300, seasonal_0 Loss: 0.0788 | 0.0934
Epoch 160/300, seasonal_0 Loss: 0.0792 | 0.0935
Epoch 161/300, seasonal_0 Loss: 0.0784 | 0.0934
Epoch 162/300, seasonal_0 Loss: 0.0786 | 0.0929
Epoch 163/300, seasonal_0 Loss: 0.0786 | 0.0927
Epoch 164/300, seasonal_0 Loss: 0.0788 | 0.0928
Epoch 165/300, seasonal_0 Loss: 0.0780 | 0.0931
Epoch 166/300, seasonal_0 Loss: 0.0784 | 0.0930
Epoch 167/300, seasonal_0 Loss: 0.0780 | 0.0924
Epoch 168/300, seasonal_0 Loss: 0.0784 | 0.0928
Epoch 169/300, seasonal_0 Loss: 0.0780 | 0.0925
Epoch 170/300, seasonal_0 Loss: 0.0786 | 0.0930
Epoch 171/300, seasonal_0 Loss: 0.0782 | 0.0926
Epoch 172/300, seasonal_0 Loss: 0.0776 | 0.0920
Epoch 173/300, seasonal_0 Loss: 0.0780 | 0.0926
Epoch 174/300, seasonal_0 Loss: 0.0780 | 0.0930
Epoch 175/300, seasonal_0 Loss: 0.0782 | 0.0925
Epoch 176/300, seasonal_0 Loss: 0.0781 | 0.0926
Epoch 177/300, seasonal_0 Loss: 0.0777 | 0.0921
Epoch 178/300, seasonal_0 Loss: 0.0777 | 0.0921
Epoch 179/300, seasonal_0 Loss: 0.0772 | 0.0920
Epoch 180/300, seasonal_0 Loss: 0.0776 | 0.0929
Epoch 181/300, seasonal_0 Loss: 0.0773 | 0.0927
Epoch 182/300, seasonal_0 Loss: 0.0775 | 0.0925
Epoch 183/300, seasonal_0 Loss: 0.0775 | 0.0923
Epoch 184/300, seasonal_0 Loss: 0.0775 | 0.0921
Epoch 185/300, seasonal_0 Loss: 0.0773 | 0.0921
Epoch 186/300, seasonal_0 Loss: 0.0771 | 0.0923
Epoch 187/300, seasonal_0 Loss: 0.0773 | 0.0923
Epoch 188/300, seasonal_0 Loss: 0.0770 | 0.0926
Epoch 189/300, seasonal_0 Loss: 0.0777 | 0.0925
Epoch 190/300, seasonal_0 Loss: 0.0776 | 0.0925
Epoch 191/300, seasonal_0 Loss: 0.0770 | 0.0925
Epoch 192/300, seasonal_0 Loss: 0.0773 | 0.0925
Epoch 193/300, seasonal_0 Loss: 0.0770 | 0.0922
Epoch 194/300, seasonal_0 Loss: 0.0777 | 0.0922
Epoch 195/300, seasonal_0 Loss: 0.0772 | 0.0920
Epoch 196/300, seasonal_0 Loss: 0.0776 | 0.0923
Epoch 197/300, seasonal_0 Loss: 0.0772 | 0.0921
Epoch 198/300, seasonal_0 Loss: 0.0774 | 0.0921
Epoch 199/300, seasonal_0 Loss: 0.0772 | 0.0920
Epoch 200/300, seasonal_0 Loss: 0.0773 | 0.0923
Epoch 201/300, seasonal_0 Loss: 0.0770 | 0.0923
Epoch 202/300, seasonal_0 Loss: 0.0771 | 0.0920
Epoch 203/300, seasonal_0 Loss: 0.0767 | 0.0920
Epoch 204/300, seasonal_0 Loss: 0.0766 | 0.0918
Epoch 205/300, seasonal_0 Loss: 0.0762 | 0.0919
Epoch 206/300, seasonal_0 Loss: 0.0770 | 0.0919
Epoch 207/300, seasonal_0 Loss: 0.0768 | 0.0919
Epoch 208/300, seasonal_0 Loss: 0.0771 | 0.0918
Epoch 209/300, seasonal_0 Loss: 0.0765 | 0.0922
Epoch 210/300, seasonal_0 Loss: 0.0766 | 0.0920
Epoch 211/300, seasonal_0 Loss: 0.0763 | 0.0921
Epoch 212/300, seasonal_0 Loss: 0.0767 | 0.0920
Epoch 213/300, seasonal_0 Loss: 0.0771 | 0.0919
Epoch 214/300, seasonal_0 Loss: 0.0768 | 0.0919
Epoch 215/300, seasonal_0 Loss: 0.0768 | 0.0919
Epoch 216/300, seasonal_0 Loss: 0.0769 | 0.0920
Epoch 217/300, seasonal_0 Loss: 0.0767 | 0.0920
Epoch 218/300, seasonal_0 Loss: 0.0762 | 0.0919
Epoch 219/300, seasonal_0 Loss: 0.0765 | 0.0921
Epoch 220/300, seasonal_0 Loss: 0.0763 | 0.0921
Epoch 221/300, seasonal_0 Loss: 0.0768 | 0.0923
Epoch 222/300, seasonal_0 Loss: 0.0765 | 0.0922
Epoch 223/300, seasonal_0 Loss: 0.0768 | 0.0924
Epoch 224/300, seasonal_0 Loss: 0.0766 | 0.0925
Epoch 225/300, seasonal_0 Loss: 0.0767 | 0.0924
Epoch 226/300, seasonal_0 Loss: 0.0767 | 0.0922
Epoch 227/300, seasonal_0 Loss: 0.0762 | 0.0922
Epoch 228/300, seasonal_0 Loss: 0.0764 | 0.0921
Epoch 229/300, seasonal_0 Loss: 0.0765 | 0.0921
Epoch 230/300, seasonal_0 Loss: 0.0765 | 0.0923
Epoch 231/300, seasonal_0 Loss: 0.0764 | 0.0924
Epoch 232/300, seasonal_0 Loss: 0.0763 | 0.0924
Epoch 233/300, seasonal_0 Loss: 0.0767 | 0.0923
Epoch 234/300, seasonal_0 Loss: 0.0764 | 0.0922
Epoch 235/300, seasonal_0 Loss: 0.0765 | 0.0923
Epoch 236/300, seasonal_0 Loss: 0.0758 | 0.0923
Epoch 237/300, seasonal_0 Loss: 0.0766 | 0.0922
Epoch 238/300, seasonal_0 Loss: 0.0768 | 0.0923
Epoch 239/300, seasonal_0 Loss: 0.0763 | 0.0922
Epoch 240/300, seasonal_0 Loss: 0.0766 | 0.0920
Epoch 241/300, seasonal_0 Loss: 0.0762 | 0.0919
Epoch 242/300, seasonal_0 Loss: 0.0759 | 0.0920
Epoch 243/300, seasonal_0 Loss: 0.0765 | 0.0920
Epoch 244/300, seasonal_0 Loss: 0.0765 | 0.0922
Epoch 245/300, seasonal_0 Loss: 0.0767 | 0.0922
Epoch 246/300, seasonal_0 Loss: 0.0761 | 0.0921
Epoch 247/300, seasonal_0 Loss: 0.0766 | 0.0920
Epoch 248/300, seasonal_0 Loss: 0.0762 | 0.0920
Epoch 249/300, seasonal_0 Loss: 0.0761 | 0.0920
Epoch 250/300, seasonal_0 Loss: 0.0761 | 0.0920
Epoch 251/300, seasonal_0 Loss: 0.0763 | 0.0921
Epoch 252/300, seasonal_0 Loss: 0.0767 | 0.0921
Epoch 253/300, seasonal_0 Loss: 0.0765 | 0.0920
Epoch 254/300, seasonal_0 Loss: 0.0765 | 0.0920
Epoch 255/300, seasonal_0 Loss: 0.0764 | 0.0920
Epoch 256/300, seasonal_0 Loss: 0.0758 | 0.0919
Epoch 257/300, seasonal_0 Loss: 0.0760 | 0.0918
Epoch 258/300, seasonal_0 Loss: 0.0763 | 0.0918
Epoch 259/300, seasonal_0 Loss: 0.0763 | 0.0918
Epoch 260/300, seasonal_0 Loss: 0.0755 | 0.0918
Epoch 261/300, seasonal_0 Loss: 0.0761 | 0.0917
Epoch 262/300, seasonal_0 Loss: 0.0766 | 0.0917
Epoch 263/300, seasonal_0 Loss: 0.0765 | 0.0917
Epoch 264/300, seasonal_0 Loss: 0.0761 | 0.0918
Epoch 265/300, seasonal_0 Loss: 0.0762 | 0.0918
Epoch 266/300, seasonal_0 Loss: 0.0764 | 0.0919
Epoch 267/300, seasonal_0 Loss: 0.0766 | 0.0919
Epoch 268/300, seasonal_0 Loss: 0.0759 | 0.0919
Epoch 269/300, seasonal_0 Loss: 0.0760 | 0.0919
Epoch 270/300, seasonal_0 Loss: 0.0758 | 0.0919
Epoch 271/300, seasonal_0 Loss: 0.0763 | 0.0919
Epoch 272/300, seasonal_0 Loss: 0.0764 | 0.0918
Epoch 273/300, seasonal_0 Loss: 0.0759 | 0.0919
Epoch 274/300, seasonal_0 Loss: 0.0765 | 0.0919
Epoch 275/300, seasonal_0 Loss: 0.0761 | 0.0918
Epoch 276/300, seasonal_0 Loss: 0.0758 | 0.0919
Epoch 277/300, seasonal_0 Loss: 0.0760 | 0.0918
Epoch 278/300, seasonal_0 Loss: 0.0764 | 0.0918
Epoch 279/300, seasonal_0 Loss: 0.0758 | 0.0918
Epoch 280/300, seasonal_0 Loss: 0.0762 | 0.0918
Epoch 281/300, seasonal_0 Loss: 0.0766 | 0.0918
Epoch 282/300, seasonal_0 Loss: 0.0765 | 0.0918
Epoch 283/300, seasonal_0 Loss: 0.0769 | 0.0918
Epoch 284/300, seasonal_0 Loss: 0.0766 | 0.0918
Epoch 285/300, seasonal_0 Loss: 0.0756 | 0.0918
Epoch 286/300, seasonal_0 Loss: 0.0760 | 0.0918
Epoch 287/300, seasonal_0 Loss: 0.0762 | 0.0919
Epoch 288/300, seasonal_0 Loss: 0.0759 | 0.0919
Epoch 289/300, seasonal_0 Loss: 0.0757 | 0.0919
Epoch 290/300, seasonal_0 Loss: 0.0759 | 0.0918
Epoch 291/300, seasonal_0 Loss: 0.0761 | 0.0918
Epoch 292/300, seasonal_0 Loss: 0.0765 | 0.0918
Epoch 293/300, seasonal_0 Loss: 0.0760 | 0.0917
Epoch 294/300, seasonal_0 Loss: 0.0762 | 0.0917
Epoch 295/300, seasonal_0 Loss: 0.0761 | 0.0918
Epoch 296/300, seasonal_0 Loss: 0.0758 | 0.0918
Epoch 297/300, seasonal_0 Loss: 0.0761 | 0.0918
Epoch 298/300, seasonal_0 Loss: 0.0765 | 0.0917
Epoch 299/300, seasonal_0 Loss: 0.0763 | 0.0917
Epoch 300/300, seasonal_0 Loss: 0.0757 | 0.0917
Training seasonal_1 component with params: {'observation_period_num': 106, 'train_rates': 0.9594807283195983, 'learning_rate': 0.0003710192989328447, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8923596555998821}
Epoch 1/300, seasonal_1 Loss: 1.1131 | 1.5081
Epoch 2/300, seasonal_1 Loss: 0.9630 | 0.9737
Epoch 3/300, seasonal_1 Loss: 0.7702 | 0.6964
Epoch 4/300, seasonal_1 Loss: 0.6947 | 0.6100
Epoch 5/300, seasonal_1 Loss: 0.6471 | 0.6037
Epoch 6/300, seasonal_1 Loss: 0.5515 | 0.5078
Epoch 7/300, seasonal_1 Loss: 0.4903 | 0.4452
Epoch 8/300, seasonal_1 Loss: 0.4626 | 0.4462
Epoch 9/300, seasonal_1 Loss: 0.4528 | 0.4071
Epoch 10/300, seasonal_1 Loss: 0.6012 | 0.4386
Epoch 11/300, seasonal_1 Loss: 0.5266 | 0.4962
Epoch 12/300, seasonal_1 Loss: 0.4607 | 0.3958
Epoch 13/300, seasonal_1 Loss: 0.5054 | 0.3923
Epoch 14/300, seasonal_1 Loss: 0.5503 | 0.3857
Epoch 15/300, seasonal_1 Loss: 0.4106 | 0.3788
Epoch 16/300, seasonal_1 Loss: 0.3826 | 0.3663
Epoch 17/300, seasonal_1 Loss: 0.3628 | 0.3621
Epoch 18/300, seasonal_1 Loss: 0.3428 | 0.3123
Epoch 19/300, seasonal_1 Loss: 0.3059 | 0.2995
Epoch 20/300, seasonal_1 Loss: 0.3194 | 0.2826
Epoch 21/300, seasonal_1 Loss: 0.2929 | 0.2766
Epoch 22/300, seasonal_1 Loss: 0.2871 | 0.2911
Epoch 23/300, seasonal_1 Loss: 0.2912 | 0.2616
Epoch 24/300, seasonal_1 Loss: 0.2926 | 0.2984
Epoch 25/300, seasonal_1 Loss: 0.2920 | 0.2549
Epoch 26/300, seasonal_1 Loss: 0.2754 | 0.2814
Epoch 27/300, seasonal_1 Loss: 0.2731 | 0.2575
Epoch 28/300, seasonal_1 Loss: 0.2713 | 0.2575
Epoch 29/300, seasonal_1 Loss: 0.2490 | 0.2519
Epoch 30/300, seasonal_1 Loss: 0.2536 | 0.2368
Epoch 31/300, seasonal_1 Loss: 0.2412 | 0.2549
Epoch 32/300, seasonal_1 Loss: 0.2454 | 0.2213
Epoch 33/300, seasonal_1 Loss: 0.2370 | 0.2461
Epoch 34/300, seasonal_1 Loss: 0.2323 | 0.2145
Epoch 35/300, seasonal_1 Loss: 0.2306 | 0.2199
Epoch 36/300, seasonal_1 Loss: 0.2303 | 0.2210
Epoch 37/300, seasonal_1 Loss: 0.2173 | 0.2207
Epoch 38/300, seasonal_1 Loss: 0.2133 | 0.2054
Epoch 39/300, seasonal_1 Loss: 0.2075 | 0.2120
Epoch 40/300, seasonal_1 Loss: 0.2000 | 0.1952
Epoch 41/300, seasonal_1 Loss: 0.1946 | 0.2071
Epoch 42/300, seasonal_1 Loss: 0.1940 | 0.1927
Epoch 43/300, seasonal_1 Loss: 0.1916 | 0.1997
Epoch 44/300, seasonal_1 Loss: 0.1938 | 0.1912
Epoch 45/300, seasonal_1 Loss: 0.1863 | 0.1933
Epoch 46/300, seasonal_1 Loss: 0.1846 | 0.1922
Epoch 47/300, seasonal_1 Loss: 0.1784 | 0.1868
Epoch 48/300, seasonal_1 Loss: 0.1762 | 0.1869
Epoch 49/300, seasonal_1 Loss: 0.1730 | 0.1860
Epoch 50/300, seasonal_1 Loss: 0.1712 | 0.1823
Epoch 51/300, seasonal_1 Loss: 0.1692 | 0.1824
Epoch 52/300, seasonal_1 Loss: 0.1676 | 0.1817
Epoch 53/300, seasonal_1 Loss: 0.1672 | 0.1776
Epoch 54/300, seasonal_1 Loss: 0.1678 | 0.1802
Epoch 55/300, seasonal_1 Loss: 0.1684 | 0.1749
Epoch 56/300, seasonal_1 Loss: 0.1680 | 0.1798
Epoch 57/300, seasonal_1 Loss: 0.1665 | 0.1743
Epoch 58/300, seasonal_1 Loss: 0.1631 | 0.1756
Epoch 59/300, seasonal_1 Loss: 0.1637 | 0.1738
Epoch 60/300, seasonal_1 Loss: 0.1630 | 0.1735
Epoch 61/300, seasonal_1 Loss: 0.1609 | 0.1695
Epoch 62/300, seasonal_1 Loss: 0.1586 | 0.1729
Epoch 63/300, seasonal_1 Loss: 0.1573 | 0.1673
Epoch 64/300, seasonal_1 Loss: 0.1555 | 0.1706
Epoch 65/300, seasonal_1 Loss: 0.1547 | 0.1671
Epoch 66/300, seasonal_1 Loss: 0.1533 | 0.1673
Epoch 67/300, seasonal_1 Loss: 0.1518 | 0.1651
Epoch 68/300, seasonal_1 Loss: 0.1501 | 0.1657
Epoch 69/300, seasonal_1 Loss: 0.1491 | 0.1644
Epoch 70/300, seasonal_1 Loss: 0.1481 | 0.1631
Epoch 71/300, seasonal_1 Loss: 0.1475 | 0.1634
Epoch 72/300, seasonal_1 Loss: 0.1462 | 0.1615
Epoch 73/300, seasonal_1 Loss: 0.1455 | 0.1623
Epoch 74/300, seasonal_1 Loss: 0.1442 | 0.1589
Epoch 75/300, seasonal_1 Loss: 0.1435 | 0.1595
Epoch 76/300, seasonal_1 Loss: 0.1439 | 0.1588
Epoch 77/300, seasonal_1 Loss: 0.1429 | 0.1578
Epoch 78/300, seasonal_1 Loss: 0.1417 | 0.1610
Epoch 79/300, seasonal_1 Loss: 0.1403 | 0.1555
Epoch 80/300, seasonal_1 Loss: 0.1402 | 0.1578
Epoch 81/300, seasonal_1 Loss: 0.1394 | 0.1550
Epoch 82/300, seasonal_1 Loss: 0.1394 | 0.1567
Epoch 83/300, seasonal_1 Loss: 0.1390 | 0.1531
Epoch 84/300, seasonal_1 Loss: 0.1374 | 0.1561
Epoch 85/300, seasonal_1 Loss: 0.1370 | 0.1524
Epoch 86/300, seasonal_1 Loss: 0.1361 | 0.1539
Epoch 87/300, seasonal_1 Loss: 0.1358 | 0.1523
Epoch 88/300, seasonal_1 Loss: 0.1347 | 0.1524
Epoch 89/300, seasonal_1 Loss: 0.1342 | 0.1520
Epoch 90/300, seasonal_1 Loss: 0.1336 | 0.1509
Epoch 91/300, seasonal_1 Loss: 0.1330 | 0.1507
Epoch 92/300, seasonal_1 Loss: 0.1329 | 0.1495
Epoch 93/300, seasonal_1 Loss: 0.1317 | 0.1508
Epoch 94/300, seasonal_1 Loss: 0.1309 | 0.1480
Epoch 95/300, seasonal_1 Loss: 0.1310 | 0.1492
Epoch 96/300, seasonal_1 Loss: 0.1291 | 0.1476
Epoch 97/300, seasonal_1 Loss: 0.1290 | 0.1480
Epoch 98/300, seasonal_1 Loss: 0.1291 | 0.1469
Epoch 99/300, seasonal_1 Loss: 0.1290 | 0.1467
Epoch 100/300, seasonal_1 Loss: 0.1274 | 0.1461
Epoch 101/300, seasonal_1 Loss: 0.1269 | 0.1453
Epoch 102/300, seasonal_1 Loss: 0.1271 | 0.1452
Epoch 103/300, seasonal_1 Loss: 0.1259 | 0.1447
Epoch 104/300, seasonal_1 Loss: 0.1256 | 0.1441
Epoch 105/300, seasonal_1 Loss: 0.1250 | 0.1439
Epoch 106/300, seasonal_1 Loss: 0.1248 | 0.1435
Epoch 107/300, seasonal_1 Loss: 0.1234 | 0.1433
Epoch 108/300, seasonal_1 Loss: 0.1241 | 0.1436
Epoch 109/300, seasonal_1 Loss: 0.1244 | 0.1419
Epoch 110/300, seasonal_1 Loss: 0.1240 | 0.1419
Epoch 111/300, seasonal_1 Loss: 0.1234 | 0.1422
Epoch 112/300, seasonal_1 Loss: 0.1238 | 0.1421
Epoch 113/300, seasonal_1 Loss: 0.1221 | 0.1420
Epoch 114/300, seasonal_1 Loss: 0.1220 | 0.1414
Epoch 115/300, seasonal_1 Loss: 0.1217 | 0.1403
Epoch 116/300, seasonal_1 Loss: 0.1213 | 0.1410
Epoch 117/300, seasonal_1 Loss: 0.1213 | 0.1404
Epoch 118/300, seasonal_1 Loss: 0.1213 | 0.1402
Epoch 119/300, seasonal_1 Loss: 0.1210 | 0.1410
Epoch 120/300, seasonal_1 Loss: 0.1208 | 0.1409
Epoch 121/300, seasonal_1 Loss: 0.1202 | 0.1392
Epoch 122/300, seasonal_1 Loss: 0.1198 | 0.1400
Epoch 123/300, seasonal_1 Loss: 0.1196 | 0.1387
Epoch 124/300, seasonal_1 Loss: 0.1201 | 0.1395
Epoch 125/300, seasonal_1 Loss: 0.1187 | 0.1392
Epoch 126/300, seasonal_1 Loss: 0.1189 | 0.1384
Epoch 127/300, seasonal_1 Loss: 0.1187 | 0.1390
Epoch 128/300, seasonal_1 Loss: 0.1185 | 0.1383
Epoch 129/300, seasonal_1 Loss: 0.1181 | 0.1382
Epoch 130/300, seasonal_1 Loss: 0.1180 | 0.1378
Epoch 131/300, seasonal_1 Loss: 0.1180 | 0.1385
Epoch 132/300, seasonal_1 Loss: 0.1173 | 0.1380
Epoch 133/300, seasonal_1 Loss: 0.1174 | 0.1386
Epoch 134/300, seasonal_1 Loss: 0.1179 | 0.1375
Epoch 135/300, seasonal_1 Loss: 0.1168 | 0.1369
Epoch 136/300, seasonal_1 Loss: 0.1161 | 0.1373
Epoch 137/300, seasonal_1 Loss: 0.1162 | 0.1369
Epoch 138/300, seasonal_1 Loss: 0.1159 | 0.1368
Epoch 139/300, seasonal_1 Loss: 0.1161 | 0.1366
Epoch 140/300, seasonal_1 Loss: 0.1167 | 0.1373
Epoch 141/300, seasonal_1 Loss: 0.1165 | 0.1368
Epoch 142/300, seasonal_1 Loss: 0.1148 | 0.1369
Epoch 143/300, seasonal_1 Loss: 0.1154 | 0.1369
Epoch 144/300, seasonal_1 Loss: 0.1150 | 0.1360
Epoch 145/300, seasonal_1 Loss: 0.1150 | 0.1362
Epoch 146/300, seasonal_1 Loss: 0.1146 | 0.1356
Epoch 147/300, seasonal_1 Loss: 0.1152 | 0.1355
Epoch 148/300, seasonal_1 Loss: 0.1153 | 0.1362
Epoch 149/300, seasonal_1 Loss: 0.1145 | 0.1358
Epoch 150/300, seasonal_1 Loss: 0.1142 | 0.1363
Epoch 151/300, seasonal_1 Loss: 0.1138 | 0.1348
Epoch 152/300, seasonal_1 Loss: 0.1140 | 0.1352
Epoch 153/300, seasonal_1 Loss: 0.1129 | 0.1350
Epoch 154/300, seasonal_1 Loss: 0.1136 | 0.1350
Epoch 155/300, seasonal_1 Loss: 0.1128 | 0.1354
Epoch 156/300, seasonal_1 Loss: 0.1130 | 0.1349
Epoch 157/300, seasonal_1 Loss: 0.1128 | 0.1352
Epoch 158/300, seasonal_1 Loss: 0.1137 | 0.1356
Epoch 159/300, seasonal_1 Loss: 0.1124 | 0.1349
Epoch 160/300, seasonal_1 Loss: 0.1130 | 0.1349
Epoch 161/300, seasonal_1 Loss: 0.1126 | 0.1345
Epoch 162/300, seasonal_1 Loss: 0.1122 | 0.1340
Epoch 163/300, seasonal_1 Loss: 0.1125 | 0.1344
Epoch 164/300, seasonal_1 Loss: 0.1125 | 0.1346
Epoch 165/300, seasonal_1 Loss: 0.1124 | 0.1349
Epoch 166/300, seasonal_1 Loss: 0.1128 | 0.1344
Epoch 167/300, seasonal_1 Loss: 0.1123 | 0.1345
Epoch 168/300, seasonal_1 Loss: 0.1121 | 0.1347
Epoch 169/300, seasonal_1 Loss: 0.1129 | 0.1341
Epoch 170/300, seasonal_1 Loss: 0.1118 | 0.1340
Epoch 171/300, seasonal_1 Loss: 0.1110 | 0.1341
Epoch 172/300, seasonal_1 Loss: 0.1113 | 0.1336
Epoch 173/300, seasonal_1 Loss: 0.1114 | 0.1338
Epoch 174/300, seasonal_1 Loss: 0.1121 | 0.1340
Epoch 175/300, seasonal_1 Loss: 0.1110 | 0.1340
Epoch 176/300, seasonal_1 Loss: 0.1109 | 0.1337
Epoch 177/300, seasonal_1 Loss: 0.1107 | 0.1343
Epoch 178/300, seasonal_1 Loss: 0.1114 | 0.1342
Epoch 179/300, seasonal_1 Loss: 0.1106 | 0.1340
Epoch 180/300, seasonal_1 Loss: 0.1115 | 0.1340
Epoch 181/300, seasonal_1 Loss: 0.1110 | 0.1340
Epoch 182/300, seasonal_1 Loss: 0.1101 | 0.1337
Epoch 183/300, seasonal_1 Loss: 0.1113 | 0.1339
Epoch 184/300, seasonal_1 Loss: 0.1107 | 0.1337
Epoch 185/300, seasonal_1 Loss: 0.1103 | 0.1334
Epoch 186/300, seasonal_1 Loss: 0.1099 | 0.1337
Epoch 187/300, seasonal_1 Loss: 0.1101 | 0.1338
Epoch 188/300, seasonal_1 Loss: 0.1101 | 0.1336
Epoch 189/300, seasonal_1 Loss: 0.1100 | 0.1336
Epoch 190/300, seasonal_1 Loss: 0.1102 | 0.1336
Epoch 191/300, seasonal_1 Loss: 0.1100 | 0.1331
Epoch 192/300, seasonal_1 Loss: 0.1096 | 0.1335
Epoch 193/300, seasonal_1 Loss: 0.1099 | 0.1338
Epoch 194/300, seasonal_1 Loss: 0.1104 | 0.1337
Epoch 195/300, seasonal_1 Loss: 0.1102 | 0.1334
Epoch 196/300, seasonal_1 Loss: 0.1099 | 0.1328
Epoch 197/300, seasonal_1 Loss: 0.1098 | 0.1330
Epoch 198/300, seasonal_1 Loss: 0.1099 | 0.1330
Epoch 199/300, seasonal_1 Loss: 0.1089 | 0.1325
Epoch 200/300, seasonal_1 Loss: 0.1091 | 0.1327
Epoch 201/300, seasonal_1 Loss: 0.1095 | 0.1330
Epoch 202/300, seasonal_1 Loss: 0.1095 | 0.1332
Epoch 203/300, seasonal_1 Loss: 0.1100 | 0.1331
Epoch 204/300, seasonal_1 Loss: 0.1097 | 0.1328
Epoch 205/300, seasonal_1 Loss: 0.1093 | 0.1329
Epoch 206/300, seasonal_1 Loss: 0.1088 | 0.1330
Epoch 207/300, seasonal_1 Loss: 0.1083 | 0.1330
Epoch 208/300, seasonal_1 Loss: 0.1089 | 0.1330
Epoch 209/300, seasonal_1 Loss: 0.1081 | 0.1328
Epoch 210/300, seasonal_1 Loss: 0.1087 | 0.1322
Epoch 211/300, seasonal_1 Loss: 0.1094 | 0.1322
Epoch 212/300, seasonal_1 Loss: 0.1086 | 0.1326
Epoch 213/300, seasonal_1 Loss: 0.1090 | 0.1328
Epoch 214/300, seasonal_1 Loss: 0.1082 | 0.1328
Epoch 215/300, seasonal_1 Loss: 0.1081 | 0.1326
Epoch 216/300, seasonal_1 Loss: 0.1088 | 0.1326
Epoch 217/300, seasonal_1 Loss: 0.1082 | 0.1328
Epoch 218/300, seasonal_1 Loss: 0.1087 | 0.1330
Epoch 219/300, seasonal_1 Loss: 0.1087 | 0.1331
Epoch 220/300, seasonal_1 Loss: 0.1086 | 0.1328
Epoch 221/300, seasonal_1 Loss: 0.1083 | 0.1326
Epoch 222/300, seasonal_1 Loss: 0.1075 | 0.1327
Epoch 223/300, seasonal_1 Loss: 0.1084 | 0.1328
Epoch 224/300, seasonal_1 Loss: 0.1083 | 0.1328
Epoch 225/300, seasonal_1 Loss: 0.1086 | 0.1327
Epoch 226/300, seasonal_1 Loss: 0.1083 | 0.1330
Epoch 227/300, seasonal_1 Loss: 0.1080 | 0.1330
Epoch 228/300, seasonal_1 Loss: 0.1085 | 0.1327
Epoch 229/300, seasonal_1 Loss: 0.1079 | 0.1322
Epoch 230/300, seasonal_1 Loss: 0.1075 | 0.1320
Epoch 231/300, seasonal_1 Loss: 0.1079 | 0.1320
Epoch 232/300, seasonal_1 Loss: 0.1073 | 0.1322
Epoch 233/300, seasonal_1 Loss: 0.1081 | 0.1323
Epoch 234/300, seasonal_1 Loss: 0.1095 | 0.1326
Epoch 235/300, seasonal_1 Loss: 0.1076 | 0.1327
Epoch 236/300, seasonal_1 Loss: 0.1083 | 0.1325
Epoch 237/300, seasonal_1 Loss: 0.1077 | 0.1326
Epoch 238/300, seasonal_1 Loss: 0.1085 | 0.1328
Epoch 239/300, seasonal_1 Loss: 0.1081 | 0.1329
Epoch 240/300, seasonal_1 Loss: 0.1074 | 0.1327
Epoch 241/300, seasonal_1 Loss: 0.1075 | 0.1324
Epoch 242/300, seasonal_1 Loss: 0.1074 | 0.1325
Epoch 243/300, seasonal_1 Loss: 0.1072 | 0.1324
Epoch 244/300, seasonal_1 Loss: 0.1072 | 0.1325
Epoch 245/300, seasonal_1 Loss: 0.1083 | 0.1326
Epoch 246/300, seasonal_1 Loss: 0.1069 | 0.1325
Epoch 247/300, seasonal_1 Loss: 0.1082 | 0.1325
Epoch 248/300, seasonal_1 Loss: 0.1077 | 0.1327
Epoch 249/300, seasonal_1 Loss: 0.1078 | 0.1327
Epoch 250/300, seasonal_1 Loss: 0.1071 | 0.1326
Epoch 251/300, seasonal_1 Loss: 0.1079 | 0.1325
Epoch 252/300, seasonal_1 Loss: 0.1068 | 0.1324
Epoch 253/300, seasonal_1 Loss: 0.1069 | 0.1324
Epoch 254/300, seasonal_1 Loss: 0.1073 | 0.1325
Epoch 255/300, seasonal_1 Loss: 0.1073 | 0.1324
Epoch 256/300, seasonal_1 Loss: 0.1064 | 0.1323
Epoch 257/300, seasonal_1 Loss: 0.1077 | 0.1322
Epoch 258/300, seasonal_1 Loss: 0.1081 | 0.1322
Epoch 259/300, seasonal_1 Loss: 0.1075 | 0.1323
Epoch 260/300, seasonal_1 Loss: 0.1074 | 0.1324
Epoch 261/300, seasonal_1 Loss: 0.1078 | 0.1324
Epoch 262/300, seasonal_1 Loss: 0.1069 | 0.1324
Epoch 263/300, seasonal_1 Loss: 0.1068 | 0.1324
Epoch 264/300, seasonal_1 Loss: 0.1078 | 0.1323
Epoch 265/300, seasonal_1 Loss: 0.1074 | 0.1323
Epoch 266/300, seasonal_1 Loss: 0.1077 | 0.1323
Epoch 267/300, seasonal_1 Loss: 0.1071 | 0.1323
Epoch 268/300, seasonal_1 Loss: 0.1070 | 0.1324
Epoch 269/300, seasonal_1 Loss: 0.1066 | 0.1322
Epoch 270/300, seasonal_1 Loss: 0.1070 | 0.1321
Epoch 271/300, seasonal_1 Loss: 0.1072 | 0.1320
Epoch 272/300, seasonal_1 Loss: 0.1069 | 0.1320
Epoch 273/300, seasonal_1 Loss: 0.1080 | 0.1321
Epoch 274/300, seasonal_1 Loss: 0.1076 | 0.1323
Epoch 275/300, seasonal_1 Loss: 0.1072 | 0.1323
Epoch 276/300, seasonal_1 Loss: 0.1076 | 0.1322
Epoch 277/300, seasonal_1 Loss: 0.1071 | 0.1321
Epoch 278/300, seasonal_1 Loss: 0.1072 | 0.1320
Epoch 279/300, seasonal_1 Loss: 0.1073 | 0.1320
Epoch 280/300, seasonal_1 Loss: 0.1067 | 0.1320
Epoch 281/300, seasonal_1 Loss: 0.1070 | 0.1320
Epoch 282/300, seasonal_1 Loss: 0.1067 | 0.1320
Epoch 283/300, seasonal_1 Loss: 0.1066 | 0.1321
Epoch 284/300, seasonal_1 Loss: 0.1072 | 0.1321
Epoch 285/300, seasonal_1 Loss: 0.1074 | 0.1322
Epoch 286/300, seasonal_1 Loss: 0.1076 | 0.1322
Epoch 287/300, seasonal_1 Loss: 0.1071 | 0.1322
Epoch 288/300, seasonal_1 Loss: 0.1075 | 0.1322
Epoch 289/300, seasonal_1 Loss: 0.1074 | 0.1321
Epoch 290/300, seasonal_1 Loss: 0.1068 | 0.1321
Epoch 291/300, seasonal_1 Loss: 0.1066 | 0.1321
Epoch 292/300, seasonal_1 Loss: 0.1072 | 0.1321
Epoch 293/300, seasonal_1 Loss: 0.1069 | 0.1320
Epoch 294/300, seasonal_1 Loss: 0.1072 | 0.1320
Epoch 295/300, seasonal_1 Loss: 0.1067 | 0.1320
Epoch 296/300, seasonal_1 Loss: 0.1078 | 0.1320
Epoch 297/300, seasonal_1 Loss: 0.1070 | 0.1320
Epoch 298/300, seasonal_1 Loss: 0.1074 | 0.1319
Epoch 299/300, seasonal_1 Loss: 0.1076 | 0.1319
Epoch 300/300, seasonal_1 Loss: 0.1071 | 0.1318
Training seasonal_2 component with params: {'observation_period_num': 87, 'train_rates': 0.950804806338593, 'learning_rate': 0.00010847916181310959, 'batch_size': 28, 'step_size': 15, 'gamma': 0.7762769483181596}
Epoch 1/300, seasonal_2 Loss: 0.8049 | 0.6668
Epoch 2/300, seasonal_2 Loss: 0.6964 | 0.5888
Epoch 3/300, seasonal_2 Loss: 0.5214 | 0.5300
Epoch 4/300, seasonal_2 Loss: 0.4361 | 0.4465
Epoch 5/300, seasonal_2 Loss: 0.4083 | 0.4990
Epoch 6/300, seasonal_2 Loss: 0.3812 | 0.3929
Epoch 7/300, seasonal_2 Loss: 0.3445 | 0.3349
Epoch 8/300, seasonal_2 Loss: 0.3079 | 0.4168
Epoch 9/300, seasonal_2 Loss: 0.3077 | 0.3232
Epoch 10/300, seasonal_2 Loss: 0.2789 | 0.2860
Epoch 11/300, seasonal_2 Loss: 0.2650 | 0.2852
Epoch 12/300, seasonal_2 Loss: 0.2535 | 0.2605
Epoch 13/300, seasonal_2 Loss: 0.2459 | 0.2613
Epoch 14/300, seasonal_2 Loss: 0.2527 | 0.2619
Epoch 15/300, seasonal_2 Loss: 0.2540 | 0.2521
Epoch 16/300, seasonal_2 Loss: 0.2445 | 0.2366
Epoch 17/300, seasonal_2 Loss: 0.2375 | 0.2210
Epoch 18/300, seasonal_2 Loss: 0.2312 | 0.2337
Epoch 19/300, seasonal_2 Loss: 0.2211 | 0.2314
Epoch 20/300, seasonal_2 Loss: 0.2134 | 0.2164
Epoch 21/300, seasonal_2 Loss: 0.2198 | 0.2015
Epoch 22/300, seasonal_2 Loss: 0.2160 | 0.2414
Epoch 23/300, seasonal_2 Loss: 0.2063 | 0.2294
Epoch 24/300, seasonal_2 Loss: 0.2015 | 0.2096
Epoch 25/300, seasonal_2 Loss: 0.1954 | 0.1939
Epoch 26/300, seasonal_2 Loss: 0.1850 | 0.1895
Epoch 27/300, seasonal_2 Loss: 0.1781 | 0.1856
Epoch 28/300, seasonal_2 Loss: 0.1725 | 0.1809
Epoch 29/300, seasonal_2 Loss: 0.1734 | 0.1760
Epoch 30/300, seasonal_2 Loss: 0.1690 | 0.1826
Epoch 31/300, seasonal_2 Loss: 0.1680 | 0.1802
Epoch 32/300, seasonal_2 Loss: 0.1635 | 0.1615
Epoch 33/300, seasonal_2 Loss: 0.1628 | 0.1678
Epoch 34/300, seasonal_2 Loss: 0.1570 | 0.1672
Epoch 35/300, seasonal_2 Loss: 0.1555 | 0.1592
Epoch 36/300, seasonal_2 Loss: 0.1545 | 0.1641
Epoch 37/300, seasonal_2 Loss: 0.1551 | 0.1645
Epoch 38/300, seasonal_2 Loss: 0.1546 | 0.1586
Epoch 39/300, seasonal_2 Loss: 0.1558 | 0.1656
Epoch 40/300, seasonal_2 Loss: 0.1545 | 0.1704
Epoch 41/300, seasonal_2 Loss: 0.1506 | 0.1529
Epoch 42/300, seasonal_2 Loss: 0.1492 | 0.1504
Epoch 43/300, seasonal_2 Loss: 0.1480 | 0.1669
Epoch 44/300, seasonal_2 Loss: 0.1456 | 0.1537
Epoch 45/300, seasonal_2 Loss: 0.1436 | 0.1424
Epoch 46/300, seasonal_2 Loss: 0.1440 | 0.1495
Epoch 47/300, seasonal_2 Loss: 0.1405 | 0.1498
Epoch 48/300, seasonal_2 Loss: 0.1385 | 0.1431
Epoch 49/300, seasonal_2 Loss: 0.1366 | 0.1381
Epoch 50/300, seasonal_2 Loss: 0.1360 | 0.1433
Epoch 51/300, seasonal_2 Loss: 0.1336 | 0.1460
Epoch 52/300, seasonal_2 Loss: 0.1342 | 0.1364
Epoch 53/300, seasonal_2 Loss: 0.1318 | 0.1345
Epoch 54/300, seasonal_2 Loss: 0.1333 | 0.1439
Epoch 55/300, seasonal_2 Loss: 0.1309 | 0.1440
Epoch 56/300, seasonal_2 Loss: 0.1292 | 0.1369
Epoch 57/300, seasonal_2 Loss: 0.1287 | 0.1378
Epoch 58/300, seasonal_2 Loss: 0.1290 | 0.1411
Epoch 59/300, seasonal_2 Loss: 0.1276 | 0.1393
Epoch 60/300, seasonal_2 Loss: 0.1276 | 0.1369
Epoch 61/300, seasonal_2 Loss: 0.1269 | 0.1349
Epoch 62/300, seasonal_2 Loss: 0.1268 | 0.1380
Epoch 63/300, seasonal_2 Loss: 0.1249 | 0.1353
Epoch 64/300, seasonal_2 Loss: 0.1242 | 0.1346
Epoch 65/300, seasonal_2 Loss: 0.1228 | 0.1344
Epoch 66/300, seasonal_2 Loss: 0.1226 | 0.1319
Epoch 67/300, seasonal_2 Loss: 0.1213 | 0.1327
Epoch 68/300, seasonal_2 Loss: 0.1200 | 0.1319
Epoch 69/300, seasonal_2 Loss: 0.1204 | 0.1329
Epoch 70/300, seasonal_2 Loss: 0.1197 | 0.1319
Epoch 71/300, seasonal_2 Loss: 0.1194 | 0.1306
Epoch 72/300, seasonal_2 Loss: 0.1184 | 0.1313
Epoch 73/300, seasonal_2 Loss: 0.1179 | 0.1321
Epoch 74/300, seasonal_2 Loss: 0.1173 | 0.1319
Epoch 75/300, seasonal_2 Loss: 0.1171 | 0.1323
Epoch 76/300, seasonal_2 Loss: 0.1173 | 0.1298
Epoch 77/300, seasonal_2 Loss: 0.1167 | 0.1300
Epoch 78/300, seasonal_2 Loss: 0.1158 | 0.1313
Epoch 79/300, seasonal_2 Loss: 0.1151 | 0.1304
Epoch 80/300, seasonal_2 Loss: 0.1152 | 0.1301
Epoch 81/300, seasonal_2 Loss: 0.1147 | 0.1295
Epoch 82/300, seasonal_2 Loss: 0.1150 | 0.1301
Epoch 83/300, seasonal_2 Loss: 0.1147 | 0.1295
Epoch 84/300, seasonal_2 Loss: 0.1140 | 0.1299
Epoch 85/300, seasonal_2 Loss: 0.1135 | 0.1284
Epoch 86/300, seasonal_2 Loss: 0.1128 | 0.1290
Epoch 87/300, seasonal_2 Loss: 0.1133 | 0.1282
Epoch 88/300, seasonal_2 Loss: 0.1133 | 0.1285
Epoch 89/300, seasonal_2 Loss: 0.1124 | 0.1274
Epoch 90/300, seasonal_2 Loss: 0.1128 | 0.1288
Epoch 91/300, seasonal_2 Loss: 0.1121 | 0.1279
Epoch 92/300, seasonal_2 Loss: 0.1113 | 0.1293
Epoch 93/300, seasonal_2 Loss: 0.1117 | 0.1278
Epoch 94/300, seasonal_2 Loss: 0.1110 | 0.1290
Epoch 95/300, seasonal_2 Loss: 0.1111 | 0.1284
Epoch 96/300, seasonal_2 Loss: 0.1108 | 0.1278
Epoch 97/300, seasonal_2 Loss: 0.1113 | 0.1285
Epoch 98/300, seasonal_2 Loss: 0.1110 | 0.1287
Epoch 99/300, seasonal_2 Loss: 0.1103 | 0.1277
Epoch 100/300, seasonal_2 Loss: 0.1103 | 0.1280
Epoch 101/300, seasonal_2 Loss: 0.1103 | 0.1283
Epoch 102/300, seasonal_2 Loss: 0.1099 | 0.1285
Epoch 103/300, seasonal_2 Loss: 0.1095 | 0.1275
Epoch 104/300, seasonal_2 Loss: 0.1098 | 0.1279
Epoch 105/300, seasonal_2 Loss: 0.1097 | 0.1276
Epoch 106/300, seasonal_2 Loss: 0.1098 | 0.1274
Epoch 107/300, seasonal_2 Loss: 0.1085 | 0.1278
Epoch 108/300, seasonal_2 Loss: 0.1092 | 0.1276
Epoch 109/300, seasonal_2 Loss: 0.1092 | 0.1266
Epoch 110/300, seasonal_2 Loss: 0.1085 | 0.1264
Epoch 111/300, seasonal_2 Loss: 0.1084 | 0.1268
Epoch 112/300, seasonal_2 Loss: 0.1082 | 0.1269
Epoch 113/300, seasonal_2 Loss: 0.1087 | 0.1270
Epoch 114/300, seasonal_2 Loss: 0.1081 | 0.1270
Epoch 115/300, seasonal_2 Loss: 0.1088 | 0.1271
Epoch 116/300, seasonal_2 Loss: 0.1091 | 0.1270
Epoch 117/300, seasonal_2 Loss: 0.1075 | 0.1265
Epoch 118/300, seasonal_2 Loss: 0.1080 | 0.1269
Epoch 119/300, seasonal_2 Loss: 0.1086 | 0.1272
Epoch 120/300, seasonal_2 Loss: 0.1087 | 0.1270
Epoch 121/300, seasonal_2 Loss: 0.1076 | 0.1262
Epoch 122/300, seasonal_2 Loss: 0.1078 | 0.1260
Epoch 123/300, seasonal_2 Loss: 0.1077 | 0.1252
Epoch 124/300, seasonal_2 Loss: 0.1076 | 0.1264
Epoch 125/300, seasonal_2 Loss: 0.1066 | 0.1264
Epoch 126/300, seasonal_2 Loss: 0.1067 | 0.1261
Epoch 127/300, seasonal_2 Loss: 0.1072 | 0.1263
Epoch 128/300, seasonal_2 Loss: 0.1076 | 0.1265
Epoch 129/300, seasonal_2 Loss: 0.1076 | 0.1266
Epoch 130/300, seasonal_2 Loss: 0.1072 | 0.1265
Epoch 131/300, seasonal_2 Loss: 0.1067 | 0.1261
Epoch 132/300, seasonal_2 Loss: 0.1069 | 0.1262
Epoch 133/300, seasonal_2 Loss: 0.1062 | 0.1268
Epoch 134/300, seasonal_2 Loss: 0.1075 | 0.1267
Epoch 135/300, seasonal_2 Loss: 0.1065 | 0.1260
Epoch 136/300, seasonal_2 Loss: 0.1066 | 0.1256
Epoch 137/300, seasonal_2 Loss: 0.1068 | 0.1262
Epoch 138/300, seasonal_2 Loss: 0.1067 | 0.1265
Epoch 139/300, seasonal_2 Loss: 0.1066 | 0.1268
Epoch 140/300, seasonal_2 Loss: 0.1063 | 0.1267
Epoch 141/300, seasonal_2 Loss: 0.1062 | 0.1270
Epoch 142/300, seasonal_2 Loss: 0.1061 | 0.1267
Epoch 143/300, seasonal_2 Loss: 0.1063 | 0.1265
Epoch 144/300, seasonal_2 Loss: 0.1058 | 0.1262
Epoch 145/300, seasonal_2 Loss: 0.1061 | 0.1262
Epoch 146/300, seasonal_2 Loss: 0.1060 | 0.1259
Epoch 147/300, seasonal_2 Loss: 0.1063 | 0.1263
Epoch 148/300, seasonal_2 Loss: 0.1066 | 0.1264
Epoch 149/300, seasonal_2 Loss: 0.1063 | 0.1262
Epoch 150/300, seasonal_2 Loss: 0.1059 | 0.1264
Epoch 151/300, seasonal_2 Loss: 0.1065 | 0.1263
Epoch 152/300, seasonal_2 Loss: 0.1065 | 0.1262
Epoch 153/300, seasonal_2 Loss: 0.1061 | 0.1259
Epoch 154/300, seasonal_2 Loss: 0.1054 | 0.1256
Epoch 155/300, seasonal_2 Loss: 0.1065 | 0.1260
Epoch 156/300, seasonal_2 Loss: 0.1058 | 0.1263
Epoch 157/300, seasonal_2 Loss: 0.1056 | 0.1263
Epoch 158/300, seasonal_2 Loss: 0.1059 | 0.1261
Epoch 159/300, seasonal_2 Loss: 0.1060 | 0.1261
Epoch 160/300, seasonal_2 Loss: 0.1052 | 0.1261
Epoch 161/300, seasonal_2 Loss: 0.1057 | 0.1259
Epoch 162/300, seasonal_2 Loss: 0.1065 | 0.1259
Epoch 163/300, seasonal_2 Loss: 0.1061 | 0.1259
Epoch 164/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 165/300, seasonal_2 Loss: 0.1053 | 0.1256
Epoch 166/300, seasonal_2 Loss: 0.1059 | 0.1258
Epoch 167/300, seasonal_2 Loss: 0.1065 | 0.1256
Epoch 168/300, seasonal_2 Loss: 0.1060 | 0.1255
Epoch 169/300, seasonal_2 Loss: 0.1059 | 0.1255
Epoch 170/300, seasonal_2 Loss: 0.1053 | 0.1256
Epoch 171/300, seasonal_2 Loss: 0.1057 | 0.1258
Epoch 172/300, seasonal_2 Loss: 0.1056 | 0.1260
Epoch 173/300, seasonal_2 Loss: 0.1060 | 0.1261
Epoch 174/300, seasonal_2 Loss: 0.1050 | 0.1262
Epoch 175/300, seasonal_2 Loss: 0.1049 | 0.1261
Epoch 176/300, seasonal_2 Loss: 0.1059 | 0.1260
Epoch 177/300, seasonal_2 Loss: 0.1055 | 0.1259
Epoch 178/300, seasonal_2 Loss: 0.1054 | 0.1259
Epoch 179/300, seasonal_2 Loss: 0.1055 | 0.1259
Epoch 180/300, seasonal_2 Loss: 0.1049 | 0.1259
Epoch 181/300, seasonal_2 Loss: 0.1054 | 0.1258
Epoch 182/300, seasonal_2 Loss: 0.1053 | 0.1258
Epoch 183/300, seasonal_2 Loss: 0.1053 | 0.1258
Epoch 184/300, seasonal_2 Loss: 0.1061 | 0.1256
Epoch 185/300, seasonal_2 Loss: 0.1053 | 0.1256
Epoch 186/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 187/300, seasonal_2 Loss: 0.1055 | 0.1257
Epoch 188/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 189/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 190/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 191/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 192/300, seasonal_2 Loss: 0.1055 | 0.1257
Epoch 193/300, seasonal_2 Loss: 0.1058 | 0.1256
Epoch 194/300, seasonal_2 Loss: 0.1059 | 0.1256
Epoch 195/300, seasonal_2 Loss: 0.1058 | 0.1256
Epoch 196/300, seasonal_2 Loss: 0.1055 | 0.1257
Epoch 197/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 198/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 199/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 200/300, seasonal_2 Loss: 0.1053 | 0.1258
Epoch 201/300, seasonal_2 Loss: 0.1057 | 0.1258
Epoch 202/300, seasonal_2 Loss: 0.1060 | 0.1258
Epoch 203/300, seasonal_2 Loss: 0.1049 | 0.1258
Epoch 204/300, seasonal_2 Loss: 0.1050 | 0.1258
Epoch 205/300, seasonal_2 Loss: 0.1052 | 0.1258
Epoch 206/300, seasonal_2 Loss: 0.1056 | 0.1258
Epoch 207/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 208/300, seasonal_2 Loss: 0.1045 | 0.1257
Epoch 209/300, seasonal_2 Loss: 0.1046 | 0.1257
Epoch 210/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 211/300, seasonal_2 Loss: 0.1046 | 0.1257
Epoch 212/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 213/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 214/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 215/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 216/300, seasonal_2 Loss: 0.1047 | 0.1257
Epoch 217/300, seasonal_2 Loss: 0.1048 | 0.1257
Epoch 218/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 219/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 220/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 221/300, seasonal_2 Loss: 0.1049 | 0.1257
Epoch 222/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 223/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 224/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 225/300, seasonal_2 Loss: 0.1056 | 0.1258
Epoch 226/300, seasonal_2 Loss: 0.1055 | 0.1258
Epoch 227/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 228/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 229/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 230/300, seasonal_2 Loss: 0.1048 | 0.1257
Epoch 231/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 232/300, seasonal_2 Loss: 0.1062 | 0.1257
Epoch 233/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 234/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 235/300, seasonal_2 Loss: 0.1055 | 0.1256
Epoch 236/300, seasonal_2 Loss: 0.1053 | 0.1256
Epoch 237/300, seasonal_2 Loss: 0.1050 | 0.1256
Epoch 238/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 239/300, seasonal_2 Loss: 0.1044 | 0.1257
Epoch 240/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 241/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 242/300, seasonal_2 Loss: 0.1050 | 0.1256
Epoch 243/300, seasonal_2 Loss: 0.1055 | 0.1256
Epoch 244/300, seasonal_2 Loss: 0.1049 | 0.1256
Epoch 245/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 246/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 247/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 248/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 249/300, seasonal_2 Loss: 0.1049 | 0.1256
Epoch 250/300, seasonal_2 Loss: 0.1050 | 0.1256
Epoch 251/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 252/300, seasonal_2 Loss: 0.1048 | 0.1257
Epoch 253/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 254/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 255/300, seasonal_2 Loss: 0.1066 | 0.1257
Epoch 256/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 257/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 258/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 259/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 260/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 261/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 262/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 263/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 264/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 265/300, seasonal_2 Loss: 0.1046 | 0.1257
Epoch 266/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 267/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 268/300, seasonal_2 Loss: 0.1047 | 0.1257
Epoch 269/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 270/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 271/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 272/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 273/300, seasonal_2 Loss: 0.1053 | 0.1257
Epoch 274/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 275/300, seasonal_2 Loss: 0.1048 | 0.1257
Epoch 276/300, seasonal_2 Loss: 0.1049 | 0.1257
Epoch 277/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 278/300, seasonal_2 Loss: 0.1055 | 0.1257
Epoch 279/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 280/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 281/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 282/300, seasonal_2 Loss: 0.1048 | 0.1257
Epoch 283/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 284/300, seasonal_2 Loss: 0.1046 | 0.1257
Epoch 285/300, seasonal_2 Loss: 0.1051 | 0.1257
Epoch 286/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 287/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 288/300, seasonal_2 Loss: 0.1057 | 0.1257
Epoch 289/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 290/300, seasonal_2 Loss: 0.1056 | 0.1257
Epoch 291/300, seasonal_2 Loss: 0.1050 | 0.1257
Epoch 292/300, seasonal_2 Loss: 0.1058 | 0.1257
Epoch 293/300, seasonal_2 Loss: 0.1052 | 0.1257
Epoch 294/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 295/300, seasonal_2 Loss: 0.1054 | 0.1257
Epoch 296/300, seasonal_2 Loss: 0.1049 | 0.1257
