ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 09:41:03,868][0m A new study created in memory with name: no-name-e3f39b94-0ad0-48ac-957d-3fdcaf77253b[0m
[32m[I 2025-02-06 09:42:01,261][0m Trial 0 finished with value: 0.80204377765443 and parameters: {'observation_period_num': 57, 'train_rates': 0.8763429503348237, 'learning_rate': 1.099786867980885e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8639586653121922}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:44:36,571][0m Trial 1 finished with value: 1.2104928569200841 and parameters: {'observation_period_num': 168, 'train_rates': 0.7316887087536705, 'learning_rate': 8.946630733952656e-05, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8069448083739355}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:45:53,701][0m Trial 2 finished with value: 0.984500033729667 and parameters: {'observation_period_num': 40, 'train_rates': 0.8504584727560732, 'learning_rate': 2.5280618373094626e-06, 'batch_size': 59, 'step_size': 11, 'gamma': 0.878596681874009}. Best is trial 0 with value: 0.80204377765443.[0m
Early stopping at epoch 73
[32m[I 2025-02-06 09:46:26,962][0m Trial 3 finished with value: 1.0709512255345173 and parameters: {'observation_period_num': 51, 'train_rates': 0.7056858567152956, 'learning_rate': 0.0005909898591647191, 'batch_size': 199, 'step_size': 1, 'gamma': 0.8285812278046804}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:51:17,841][0m Trial 4 finished with value: 1.1858525276184082 and parameters: {'observation_period_num': 231, 'train_rates': 0.9584926029001524, 'learning_rate': 3.132083529196559e-05, 'batch_size': 205, 'step_size': 1, 'gamma': 0.9372709838495928}. Best is trial 0 with value: 0.80204377765443.[0m
Early stopping at epoch 97
[32m[I 2025-02-06 09:55:09,605][0m Trial 5 finished with value: 1.2076700463764287 and parameters: {'observation_period_num': 229, 'train_rates': 0.6981061751807429, 'learning_rate': 4.508123893365104e-05, 'batch_size': 96, 'step_size': 2, 'gamma': 0.7734732165154259}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:56:26,424][0m Trial 6 finished with value: 1.0759161350090123 and parameters: {'observation_period_num': 87, 'train_rates': 0.6698423777863005, 'learning_rate': 1.4063333252832306e-05, 'batch_size': 118, 'step_size': 7, 'gamma': 0.9332831129174385}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:58:04,827][0m Trial 7 finished with value: 2.5640487574890956 and parameters: {'observation_period_num': 102, 'train_rates': 0.8957818490328113, 'learning_rate': 2.3225902018753407e-06, 'batch_size': 229, 'step_size': 2, 'gamma': 0.8258201616438205}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:58:33,755][0m Trial 8 finished with value: 1.0674643308132679 and parameters: {'observation_period_num': 23, 'train_rates': 0.6584418812243636, 'learning_rate': 0.0004403932774619637, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9453162135097837}. Best is trial 0 with value: 0.80204377765443.[0m
[32m[I 2025-02-06 09:59:58,427][0m Trial 9 finished with value: 0.3148453890622317 and parameters: {'observation_period_num': 83, 'train_rates': 0.873405191540634, 'learning_rate': 0.00027287342211785014, 'batch_size': 136, 'step_size': 10, 'gamma': 0.7876927827772388}. Best is trial 9 with value: 0.3148453890622317.[0m
[32m[I 2025-02-06 10:03:15,176][0m Trial 10 finished with value: 0.5448885148986667 and parameters: {'observation_period_num': 165, 'train_rates': 0.7891427998600871, 'learning_rate': 0.00017654750053621655, 'batch_size': 26, 'step_size': 10, 'gamma': 0.7584416932781564}. Best is trial 9 with value: 0.3148453890622317.[0m
[32m[I 2025-02-06 10:07:19,136][0m Trial 11 finished with value: 0.5033900173767558 and parameters: {'observation_period_num': 152, 'train_rates': 0.8109140769887442, 'learning_rate': 0.00017227041744203903, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7513794898948024}. Best is trial 9 with value: 0.3148453890622317.[0m
[32m[I 2025-02-06 10:11:11,925][0m Trial 12 finished with value: 0.5295872749327734 and parameters: {'observation_period_num': 139, 'train_rates': 0.8075685181980821, 'learning_rate': 0.00018395554174317386, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7518200595566193}. Best is trial 9 with value: 0.3148453890622317.[0m
[32m[I 2025-02-06 10:14:50,051][0m Trial 13 finished with value: 0.16107091307640076 and parameters: {'observation_period_num': 179, 'train_rates': 0.9893505701192629, 'learning_rate': 0.0002570231311590525, 'batch_size': 94, 'step_size': 9, 'gamma': 0.7892656128578837}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:19:12,168][0m Trial 14 finished with value: 0.8978376623163832 and parameters: {'observation_period_num': 207, 'train_rates': 0.9658084396827309, 'learning_rate': 0.0009059859747160457, 'batch_size': 85, 'step_size': 5, 'gamma': 0.7980118757840301}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:23:20,864][0m Trial 15 finished with value: 0.1961185783147812 and parameters: {'observation_period_num': 195, 'train_rates': 0.9895439824001568, 'learning_rate': 0.00036910694454754333, 'batch_size': 64, 'step_size': 8, 'gamma': 0.8685457642039145}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:27:07,260][0m Trial 16 finished with value: 0.25543144604046186 and parameters: {'observation_period_num': 186, 'train_rates': 0.93977507418596, 'learning_rate': 7.919678264029292e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.9833623986007006}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:30:16,546][0m Trial 17 finished with value: 1.6852707215433766 and parameters: {'observation_period_num': 198, 'train_rates': 0.6085246949970784, 'learning_rate': 0.0008664728457490013, 'batch_size': 57, 'step_size': 5, 'gamma': 0.8770610773540473}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:32:28,189][0m Trial 18 finished with value: 1.5149998664855957 and parameters: {'observation_period_num': 122, 'train_rates': 0.9864052999840441, 'learning_rate': 1.0344375470931105e-06, 'batch_size': 104, 'step_size': 15, 'gamma': 0.850223506250402}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:36:53,252][0m Trial 19 finished with value: 0.2683017379212602 and parameters: {'observation_period_num': 214, 'train_rates': 0.9219921714699625, 'learning_rate': 0.0004061696352422415, 'batch_size': 76, 'step_size': 13, 'gamma': 0.904092514054224}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:41:57,940][0m Trial 20 finished with value: 0.2875450606058751 and parameters: {'observation_period_num': 243, 'train_rates': 0.91751146612211, 'learning_rate': 0.00010217806322450936, 'batch_size': 115, 'step_size': 8, 'gamma': 0.9012335173854105}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:45:51,322][0m Trial 21 finished with value: 0.24112817645072937 and parameters: {'observation_period_num': 187, 'train_rates': 0.9450164671946159, 'learning_rate': 5.429579375590285e-05, 'batch_size': 51, 'step_size': 8, 'gamma': 0.98517571607016}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:49:52,614][0m Trial 22 finished with value: 0.1801757514476776 and parameters: {'observation_period_num': 184, 'train_rates': 0.986907271454749, 'learning_rate': 2.4509357069715628e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.9631392369738426}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:53:38,291][0m Trial 23 finished with value: 0.18502981960773468 and parameters: {'observation_period_num': 176, 'train_rates': 0.9870750724397226, 'learning_rate': 2.0025703177750022e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.9612976829014793}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:57:20,593][0m Trial 24 finished with value: 0.24013923108577728 and parameters: {'observation_period_num': 169, 'train_rates': 0.9896759698785139, 'learning_rate': 1.705280095958998e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.961192735919681}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 10:59:43,831][0m Trial 25 finished with value: 0.5060623102572375 and parameters: {'observation_period_num': 121, 'train_rates': 0.903721251814211, 'learning_rate': 7.677899348756713e-06, 'batch_size': 40, 'step_size': 12, 'gamma': 0.9091643180876139}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:02:38,732][0m Trial 26 finished with value: 0.4605771216727395 and parameters: {'observation_period_num': 149, 'train_rates': 0.9534950845283338, 'learning_rate': 2.3692741956105925e-05, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9529610638096867}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:06:04,638][0m Trial 27 finished with value: 0.6231726932923948 and parameters: {'observation_period_num': 175, 'train_rates': 0.8492949378461191, 'learning_rate': 6.146920523862577e-06, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9669339410549571}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:09:54,296][0m Trial 28 finished with value: 1.2454788315907979 and parameters: {'observation_period_num': 214, 'train_rates': 0.7654209934870144, 'learning_rate': 4.352234308086646e-06, 'batch_size': 162, 'step_size': 9, 'gamma': 0.9241944082941089}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:12:21,631][0m Trial 29 finished with value: 0.9432886291194607 and parameters: {'observation_period_num': 137, 'train_rates': 0.8817604826141917, 'learning_rate': 1.985160171199449e-05, 'batch_size': 252, 'step_size': 4, 'gamma': 0.8497652480711056}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:15:17,642][0m Trial 30 finished with value: 0.7128395479709948 and parameters: {'observation_period_num': 155, 'train_rates': 0.9284641751808531, 'learning_rate': 9.632120756104803e-06, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9715694210995344}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:19:05,260][0m Trial 31 finished with value: 0.21741743532704635 and parameters: {'observation_period_num': 190, 'train_rates': 0.9744925267731183, 'learning_rate': 5.557982124803756e-05, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8912702323795342}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:22:40,136][0m Trial 32 finished with value: 0.27830401062965393 and parameters: {'observation_period_num': 178, 'train_rates': 0.9873093828168504, 'learning_rate': 3.598013666847377e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9201405557052695}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:27:08,109][0m Trial 33 finished with value: 0.2091969419093359 and parameters: {'observation_period_num': 199, 'train_rates': 0.9619532014279498, 'learning_rate': 0.00010504305828112735, 'batch_size': 34, 'step_size': 7, 'gamma': 0.8590410205353752}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:32:27,619][0m Trial 34 finished with value: 0.2477507634923376 and parameters: {'observation_period_num': 248, 'train_rates': 0.9359331180410386, 'learning_rate': 0.00035077654967804946, 'batch_size': 51, 'step_size': 11, 'gamma': 0.8337514809786909}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:36:41,528][0m Trial 35 finished with value: 0.37729278039232733 and parameters: {'observation_period_num': 215, 'train_rates': 0.8506777410556936, 'learning_rate': 0.00026259715856426176, 'batch_size': 71, 'step_size': 9, 'gamma': 0.8102796646369007}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:41:37,080][0m Trial 36 finished with value: 0.4435374438762665 and parameters: {'observation_period_num': 229, 'train_rates': 0.9641782229730433, 'learning_rate': 3.0426320737985063e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8889884005616688}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:44:49,123][0m Trial 37 finished with value: 0.5666145659002121 and parameters: {'observation_period_num': 163, 'train_rates': 0.8995897884851315, 'learning_rate': 1.2242993331527054e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.7758643325051564}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:46:45,866][0m Trial 38 finished with value: 0.25460439920425415 and parameters: {'observation_period_num': 105, 'train_rates': 0.9894232289580035, 'learning_rate': 7.385129001209867e-05, 'batch_size': 128, 'step_size': 10, 'gamma': 0.8141208746286176}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:50:10,456][0m Trial 39 finished with value: 0.28775715827941895 and parameters: {'observation_period_num': 177, 'train_rates': 0.9494934105998761, 'learning_rate': 0.0001341260534173485, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9418456050719385}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 11:55:43,668][0m Trial 40 finished with value: 0.5960904474441822 and parameters: {'observation_period_num': 235, 'train_rates': 0.9714407565725405, 'learning_rate': 0.0005297831302926428, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8647649539213577}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:00:06,567][0m Trial 41 finished with value: 0.22212814539670944 and parameters: {'observation_period_num': 198, 'train_rates': 0.9625055651127821, 'learning_rate': 0.0002450849500735068, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8550429758310474}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:04:35,045][0m Trial 42 finished with value: 0.6012841564157735 and parameters: {'observation_period_num': 201, 'train_rates': 0.9165603110166318, 'learning_rate': 0.0007117216685527223, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8420542859553903}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:09:24,952][0m Trial 43 finished with value: 0.2713713262011023 and parameters: {'observation_period_num': 222, 'train_rates': 0.9692214109988643, 'learning_rate': 0.00011642860029815349, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8736826713130743}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:13:24,764][0m Trial 44 finished with value: 0.30517615639526424 and parameters: {'observation_period_num': 189, 'train_rates': 0.9526304047894005, 'learning_rate': 4.075380372410021e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.8214373661461599}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:16:05,231][0m Trial 45 finished with value: 0.9477135639096517 and parameters: {'observation_period_num': 163, 'train_rates': 0.7460222847491438, 'learning_rate': 2.763200091310909e-05, 'batch_size': 189, 'step_size': 7, 'gamma': 0.9191996672991375}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:20:36,736][0m Trial 46 finished with value: 0.20656050510447602 and parameters: {'observation_period_num': 204, 'train_rates': 0.9367565384542084, 'learning_rate': 0.00016032444809204507, 'batch_size': 27, 'step_size': 10, 'gamma': 0.7955722593198526}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:21:40,853][0m Trial 47 finished with value: 0.21223589446809557 and parameters: {'observation_period_num': 11, 'train_rates': 0.93581727021857, 'learning_rate': 0.0003119372768951987, 'batch_size': 78, 'step_size': 10, 'gamma': 0.7837259130987673}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:26:17,610][0m Trial 48 finished with value: 0.3409341089646606 and parameters: {'observation_period_num': 206, 'train_rates': 0.8765098435879641, 'learning_rate': 0.00018921411844456313, 'batch_size': 21, 'step_size': 12, 'gamma': 0.793324918359575}. Best is trial 13 with value: 0.16107091307640076.[0m
[32m[I 2025-02-06 12:29:56,448][0m Trial 49 finished with value: 0.31700697953884416 and parameters: {'observation_period_num': 182, 'train_rates': 0.9766888025131857, 'learning_rate': 0.0005085686174362726, 'batch_size': 61, 'step_size': 10, 'gamma': 0.7601502335052583}. Best is trial 13 with value: 0.16107091307640076.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-06 12:29:56,455][0m A new study created in memory with name: no-name-a22905ba-aaae-4aa5-b058-4ac000b07598[0m
[32m[I 2025-02-06 12:31:52,433][0m Trial 0 finished with value: 0.8923156712688295 and parameters: {'observation_period_num': 126, 'train_rates': 0.6870718972673401, 'learning_rate': 0.0003969328018219531, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9007161134655097}. Best is trial 0 with value: 0.8923156712688295.[0m
[32m[I 2025-02-06 12:32:41,992][0m Trial 1 finished with value: 1.238373314775906 and parameters: {'observation_period_num': 27, 'train_rates': 0.6644983470597173, 'learning_rate': 0.0005274425750414822, 'batch_size': 72, 'step_size': 15, 'gamma': 0.7679077677290448}. Best is trial 0 with value: 0.8923156712688295.[0m
[32m[I 2025-02-06 12:34:48,738][0m Trial 2 finished with value: 1.6706707289569267 and parameters: {'observation_period_num': 125, 'train_rates': 0.9016330503969388, 'learning_rate': 1.5747231958829837e-06, 'batch_size': 200, 'step_size': 14, 'gamma': 0.7697549687613267}. Best is trial 0 with value: 0.8923156712688295.[0m
[32m[I 2025-02-06 12:37:04,652][0m Trial 3 finished with value: 0.259742357720763 and parameters: {'observation_period_num': 122, 'train_rates': 0.9583228683804126, 'learning_rate': 0.00015395500199207938, 'batch_size': 71, 'step_size': 9, 'gamma': 0.9511943298127954}. Best is trial 3 with value: 0.259742357720763.[0m
Early stopping at epoch 96
[32m[I 2025-02-06 12:39:05,646][0m Trial 4 finished with value: 2.5356046936728736 and parameters: {'observation_period_num': 42, 'train_rates': 0.969890478380037, 'learning_rate': 0.0007102436570502006, 'batch_size': 40, 'step_size': 1, 'gamma': 0.8652830666747878}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:44:25,982][0m Trial 5 finished with value: 0.2938008087021964 and parameters: {'observation_period_num': 249, 'train_rates': 0.9224987895686013, 'learning_rate': 7.680019779832398e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.8298006044067185}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:48:16,339][0m Trial 6 finished with value: 1.771396993728174 and parameters: {'observation_period_num': 217, 'train_rates': 0.747794744584485, 'learning_rate': 1.8389472526611782e-06, 'batch_size': 182, 'step_size': 5, 'gamma': 0.7860016070218024}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:48:43,704][0m Trial 7 finished with value: 1.5032993149362217 and parameters: {'observation_period_num': 27, 'train_rates': 0.6909375074082428, 'learning_rate': 5.60875277400175e-06, 'batch_size': 251, 'step_size': 5, 'gamma': 0.9757728562208143}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:49:28,302][0m Trial 8 finished with value: 0.4053791086442645 and parameters: {'observation_period_num': 48, 'train_rates': 0.8546423621651562, 'learning_rate': 0.0001709391118741685, 'batch_size': 221, 'step_size': 11, 'gamma': 0.8572414841081439}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:54:36,976][0m Trial 9 finished with value: 0.40539100019345703 and parameters: {'observation_period_num': 251, 'train_rates': 0.8452232651184538, 'learning_rate': 9.726129899906801e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.9789352901921606}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 12:57:46,814][0m Trial 10 finished with value: 0.9135740995407104 and parameters: {'observation_period_num': 162, 'train_rates': 0.9807304747824426, 'learning_rate': 1.3313054913105804e-05, 'batch_size': 133, 'step_size': 1, 'gamma': 0.925445235737948}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:00:57,380][0m Trial 11 finished with value: 0.4055678458923989 and parameters: {'observation_period_num': 174, 'train_rates': 0.915676122780874, 'learning_rate': 5.8346100878500344e-05, 'batch_size': 132, 'step_size': 12, 'gamma': 0.8245106662598742}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:02:28,553][0m Trial 12 finished with value: 0.4077627103437077 and parameters: {'observation_period_num': 83, 'train_rates': 0.9159715918910962, 'learning_rate': 2.7687088257268228e-05, 'batch_size': 99, 'step_size': 10, 'gamma': 0.922737457090618}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:05:18,513][0m Trial 13 finished with value: 0.6241286242008209 and parameters: {'observation_period_num': 172, 'train_rates': 0.7844031010337889, 'learning_rate': 0.00018863057227479506, 'batch_size': 108, 'step_size': 12, 'gamma': 0.825116678579804}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:07:48,396][0m Trial 14 finished with value: 0.40294017559952205 and parameters: {'observation_period_num': 86, 'train_rates': 0.8496671758209522, 'learning_rate': 3.233009436494866e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8129568877428828}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:12:11,372][0m Trial 15 finished with value: 0.29026854038238525 and parameters: {'observation_period_num': 218, 'train_rates': 0.9434177535125259, 'learning_rate': 0.00021617833248074796, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8927155039943895}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:15:21,590][0m Trial 16 finished with value: 1.0731837261703228 and parameters: {'observation_period_num': 205, 'train_rates': 0.6196307432357614, 'learning_rate': 0.0002764493483123108, 'batch_size': 168, 'step_size': 14, 'gamma': 0.9441904791987675}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:17:13,456][0m Trial 17 finished with value: 0.46512383222579956 and parameters: {'observation_period_num': 108, 'train_rates': 0.9620898067648325, 'learning_rate': 0.0008884945561895456, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8927312272544912}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:21:33,160][0m Trial 18 finished with value: 0.3230798779583046 and parameters: {'observation_period_num': 152, 'train_rates': 0.8765105803692003, 'learning_rate': 0.00012725745947094472, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9513319317662798}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:25:08,828][0m Trial 19 finished with value: 0.5480192263692731 and parameters: {'observation_period_num': 202, 'train_rates': 0.8104603339242406, 'learning_rate': 4.167375100103308e-05, 'batch_size': 115, 'step_size': 9, 'gamma': 0.8908823792032492}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:26:42,947][0m Trial 20 finished with value: 0.7170429825782776 and parameters: {'observation_period_num': 89, 'train_rates': 0.9453508925767183, 'learning_rate': 1.3651854928575618e-05, 'batch_size': 161, 'step_size': 13, 'gamma': 0.9895712971359559}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:32:02,844][0m Trial 21 finished with value: 0.30264305151425874 and parameters: {'observation_period_num': 247, 'train_rates': 0.933031907734443, 'learning_rate': 7.002583594263988e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8465260250501981}. Best is trial 3 with value: 0.259742357720763.[0m
[32m[I 2025-02-06 13:36:41,817][0m Trial 22 finished with value: 0.25864888817370935 and parameters: {'observation_period_num': 234, 'train_rates': 0.8903396506753377, 'learning_rate': 0.0002697043922649807, 'batch_size': 89, 'step_size': 12, 'gamma': 0.7982309848075457}. Best is trial 22 with value: 0.25864888817370935.[0m
[32m[I 2025-02-06 13:41:31,112][0m Trial 23 finished with value: 0.21710415184497833 and parameters: {'observation_period_num': 221, 'train_rates': 0.9858649757516564, 'learning_rate': 0.0002846103715129531, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9112900987938711}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 13:45:20,318][0m Trial 24 finished with value: 2.138662099838257 and parameters: {'observation_period_num': 188, 'train_rates': 0.984663299431059, 'learning_rate': 0.0009421156234277579, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9534966068316872}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 13:49:59,866][0m Trial 25 finished with value: 0.33722457872784656 and parameters: {'observation_period_num': 229, 'train_rates': 0.8903793870075385, 'learning_rate': 0.0003997292566225436, 'batch_size': 91, 'step_size': 8, 'gamma': 0.9242434868994925}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 13:52:34,197][0m Trial 26 finished with value: 0.44269752733860956 and parameters: {'observation_period_num': 146, 'train_rates': 0.8120583377139782, 'learning_rate': 0.00030282871297629676, 'batch_size': 59, 'step_size': 10, 'gamma': 0.7974389339950924}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 13:56:14,117][0m Trial 27 finished with value: 0.30885806679725647 and parameters: {'observation_period_num': 183, 'train_rates': 0.984985145171936, 'learning_rate': 0.00011277880301995086, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7527558701577723}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:01:06,939][0m Trial 28 finished with value: 0.620410242360779 and parameters: {'observation_period_num': 231, 'train_rates': 0.8719262017335412, 'learning_rate': 0.0005259623022602119, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9087906723005379}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:03:19,209][0m Trial 29 finished with value: 0.30238996008558877 and parameters: {'observation_period_num': 122, 'train_rates': 0.9555318723403474, 'learning_rate': 0.00030214172253288055, 'batch_size': 89, 'step_size': 7, 'gamma': 0.8833486222800491}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:05:46,471][0m Trial 30 finished with value: 0.7338416755199433 and parameters: {'observation_period_num': 141, 'train_rates': 0.7620000970052286, 'learning_rate': 0.00014789473854252965, 'batch_size': 69, 'step_size': 11, 'gamma': 0.9069859956124604}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:10:13,910][0m Trial 31 finished with value: 0.27817874619873556 and parameters: {'observation_period_num': 224, 'train_rates': 0.9399565192420707, 'learning_rate': 0.00022642417050349056, 'batch_size': 150, 'step_size': 13, 'gamma': 0.8748209017853563}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:14:16,406][0m Trial 32 finished with value: 0.2825635017564633 and parameters: {'observation_period_num': 198, 'train_rates': 0.9462101413448686, 'learning_rate': 0.000495313574347581, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8754686307327025}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:19:01,110][0m Trial 33 finished with value: 0.3016169243263748 and parameters: {'observation_period_num': 230, 'train_rates': 0.9028101306237226, 'learning_rate': 0.0002729902631983077, 'batch_size': 76, 'step_size': 14, 'gamma': 0.9389279003228119}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:21:14,155][0m Trial 34 finished with value: 0.7744165062904358 and parameters: {'observation_period_num': 106, 'train_rates': 0.9899361348584222, 'learning_rate': 0.0006305942697498745, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8525969694228827}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:25:46,461][0m Trial 35 finished with value: 0.2835158109664917 and parameters: {'observation_period_num': 217, 'train_rates': 0.9648005599124262, 'learning_rate': 0.00043149513936804743, 'batch_size': 147, 'step_size': 11, 'gamma': 0.8691764110670512}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:26:57,425][0m Trial 36 finished with value: 0.3222751042177511 and parameters: {'observation_period_num': 67, 'train_rates': 0.9257398892464951, 'learning_rate': 9.431594897777957e-05, 'batch_size': 190, 'step_size': 10, 'gamma': 0.9641206307575334}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:31:54,421][0m Trial 37 finished with value: 0.28680848515306423 and parameters: {'observation_period_num': 242, 'train_rates': 0.8871936553958457, 'learning_rate': 0.00018764775578780649, 'batch_size': 125, 'step_size': 13, 'gamma': 0.8374439600743542}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:35:46,045][0m Trial 38 finished with value: 1.444252108944986 and parameters: {'observation_period_num': 193, 'train_rates': 0.903778183437793, 'learning_rate': 1.0724648779773704e-06, 'batch_size': 85, 'step_size': 15, 'gamma': 0.9126415112046488}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:37:37,271][0m Trial 39 finished with value: 0.8400483257744623 and parameters: {'observation_period_num': 128, 'train_rates': 0.7070704477268339, 'learning_rate': 0.00034660218168832904, 'batch_size': 143, 'step_size': 12, 'gamma': 0.7840548184082129}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:41:24,365][0m Trial 40 finished with value: 0.5664662011280781 and parameters: {'observation_period_num': 210, 'train_rates': 0.8266993429371173, 'learning_rate': 4.6064861945517116e-05, 'batch_size': 222, 'step_size': 9, 'gamma': 0.9358452489718168}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:46:21,548][0m Trial 41 finished with value: 0.3671446924931125 and parameters: {'observation_period_num': 238, 'train_rates': 0.9442989019952938, 'learning_rate': 0.0005578946138462169, 'batch_size': 102, 'step_size': 15, 'gamma': 0.8744206004041751}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:51:08,475][0m Trial 42 finished with value: 0.2833307145671411 and parameters: {'observation_period_num': 223, 'train_rates': 0.959924472904178, 'learning_rate': 0.00047652779917502864, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8783185215768805}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:55:06,607][0m Trial 43 finished with value: 0.2812548875808716 and parameters: {'observation_period_num': 197, 'train_rates': 0.9315389095638452, 'learning_rate': 0.0002441620577390339, 'batch_size': 112, 'step_size': 15, 'gamma': 0.8578305917032271}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 14:58:39,689][0m Trial 44 finished with value: 0.2539163604745724 and parameters: {'observation_period_num': 183, 'train_rates': 0.9270659499295884, 'learning_rate': 0.00022419150337917558, 'batch_size': 118, 'step_size': 14, 'gamma': 0.857692942631366}. Best is trial 23 with value: 0.21710415184497833.[0m
[32m[I 2025-02-06 15:02:05,767][0m Trial 45 finished with value: 0.21608732732725733 and parameters: {'observation_period_num': 167, 'train_rates': 0.9709798883461263, 'learning_rate': 0.0001535899274256522, 'batch_size': 62, 'step_size': 12, 'gamma': 0.8076212979560224}. Best is trial 45 with value: 0.21608732732725733.[0m
[32m[I 2025-02-06 15:05:27,827][0m Trial 46 finished with value: 0.18417437171394174 and parameters: {'observation_period_num': 163, 'train_rates': 0.9764042024704181, 'learning_rate': 7.873569585185744e-05, 'batch_size': 57, 'step_size': 12, 'gamma': 0.8132111979496619}. Best is trial 46 with value: 0.18417437171394174.[0m
[32m[I 2025-02-06 15:09:01,132][0m Trial 47 finished with value: 0.2417949140071869 and parameters: {'observation_period_num': 172, 'train_rates': 0.9691263049068981, 'learning_rate': 8.778200082908868e-05, 'batch_size': 43, 'step_size': 12, 'gamma': 0.810235693196825}. Best is trial 46 with value: 0.18417437171394174.[0m
[32m[I 2025-02-06 15:13:27,352][0m Trial 48 finished with value: 0.15837558064805835 and parameters: {'observation_period_num': 170, 'train_rates': 0.9727977559365747, 'learning_rate': 8.245615925140233e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.815065307098686}. Best is trial 48 with value: 0.15837558064805835.[0m
[32m[I 2025-02-06 15:18:21,096][0m Trial 49 finished with value: 0.19465482391809164 and parameters: {'observation_period_num': 164, 'train_rates': 0.9728042288206828, 'learning_rate': 2.2978276202072474e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8123785928889861}. Best is trial 48 with value: 0.15837558064805835.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-06 15:18:21,104][0m A new study created in memory with name: no-name-b723bc4f-575b-4e3a-9185-433cac0e65f6[0m
[32m[I 2025-02-06 15:19:03,338][0m Trial 0 finished with value: 0.97613225580169 and parameters: {'observation_period_num': 44, 'train_rates': 0.7742501994445516, 'learning_rate': 2.5838200475454914e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8331502914138487}. Best is trial 0 with value: 0.97613225580169.[0m
[32m[I 2025-02-06 15:21:32,689][0m Trial 1 finished with value: 2.075719019745004 and parameters: {'observation_period_num': 172, 'train_rates': 0.6238101613813181, 'learning_rate': 1.0206648090098833e-06, 'batch_size': 94, 'step_size': 1, 'gamma': 0.9737105182190748}. Best is trial 0 with value: 0.97613225580169.[0m
[32m[I 2025-02-06 15:22:58,249][0m Trial 2 finished with value: 1.1560252554276411 and parameters: {'observation_period_num': 89, 'train_rates': 0.6557341182719697, 'learning_rate': 0.00026164047294286087, 'batch_size': 58, 'step_size': 15, 'gamma': 0.9403298173065369}. Best is trial 0 with value: 0.97613225580169.[0m
[32m[I 2025-02-06 15:23:47,061][0m Trial 3 finished with value: 0.3108560585513197 and parameters: {'observation_period_num': 20, 'train_rates': 0.9210254760289857, 'learning_rate': 4.5166203421008446e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.9598396287260341}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:27:13,627][0m Trial 4 finished with value: 0.9219920516216836 and parameters: {'observation_period_num': 160, 'train_rates': 0.7370028800123174, 'learning_rate': 1.7615326353547542e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7791208632855682}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:30:36,345][0m Trial 5 finished with value: 1.4018617723820477 and parameters: {'observation_period_num': 201, 'train_rates': 0.6317917201963816, 'learning_rate': 3.6927425629807657e-06, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8719218570806307}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:33:05,152][0m Trial 6 finished with value: 1.4254898567564953 and parameters: {'observation_period_num': 169, 'train_rates': 0.6178879956292338, 'learning_rate': 1.090990569986373e-05, 'batch_size': 125, 'step_size': 9, 'gamma': 0.9213963066368712}. Best is trial 3 with value: 0.3108560585513197.[0m
Early stopping at epoch 53
[32m[I 2025-02-06 15:34:29,229][0m Trial 7 finished with value: 1.1624115336892065 and parameters: {'observation_period_num': 163, 'train_rates': 0.7080582370707222, 'learning_rate': 0.00028735461549850424, 'batch_size': 181, 'step_size': 1, 'gamma': 0.7652323276109154}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:37:01,906][0m Trial 8 finished with value: 0.6780742712333253 and parameters: {'observation_period_num': 149, 'train_rates': 0.8149277194777356, 'learning_rate': 5.6347020551085906e-05, 'batch_size': 202, 'step_size': 8, 'gamma': 0.7938821023948935}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:40:31,854][0m Trial 9 finished with value: 2.0010960205741553 and parameters: {'observation_period_num': 193, 'train_rates': 0.8170196629944118, 'learning_rate': 2.0659238248667205e-06, 'batch_size': 197, 'step_size': 3, 'gamma': 0.8601649441088279}. Best is trial 3 with value: 0.3108560585513197.[0m
[32m[I 2025-02-06 15:41:30,848][0m Trial 10 finished with value: 0.22896783241947877 and parameters: {'observation_period_num': 13, 'train_rates': 0.9652417257642486, 'learning_rate': 0.00011592557740945963, 'batch_size': 88, 'step_size': 15, 'gamma': 0.9827430962645546}. Best is trial 10 with value: 0.22896783241947877.[0m
[32m[I 2025-02-06 15:42:31,660][0m Trial 11 finished with value: 0.17813467979431152 and parameters: {'observation_period_num': 15, 'train_rates': 0.9812477930522198, 'learning_rate': 9.2199734947747e-05, 'batch_size': 86, 'step_size': 15, 'gamma': 0.9880782776739725}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:44:10,106][0m Trial 12 finished with value: 0.8127620816230774 and parameters: {'observation_period_num': 83, 'train_rates': 0.9812491676820565, 'learning_rate': 0.0009051892652922203, 'batch_size': 76, 'step_size': 15, 'gamma': 0.9829405798723457}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:49:05,141][0m Trial 13 finished with value: 0.2943640718093285 and parameters: {'observation_period_num': 241, 'train_rates': 0.904252366631024, 'learning_rate': 0.00012428940542689884, 'batch_size': 241, 'step_size': 12, 'gamma': 0.9181043780305874}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:49:45,070][0m Trial 14 finished with value: 0.18632061779499054 and parameters: {'observation_period_num': 14, 'train_rates': 0.9854135554616452, 'learning_rate': 0.0001181688922583371, 'batch_size': 136, 'step_size': 12, 'gamma': 0.9005336738083946}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:50:45,521][0m Trial 15 finished with value: 0.4640797661469046 and parameters: {'observation_period_num': 64, 'train_rates': 0.8829208426916181, 'learning_rate': 0.0007305520213233483, 'batch_size': 138, 'step_size': 12, 'gamma': 0.8898912036564313}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:52:41,743][0m Trial 16 finished with value: 0.5642676530164831 and parameters: {'observation_period_num': 115, 'train_rates': 0.8504878283440558, 'learning_rate': 0.00011152713234740943, 'batch_size': 121, 'step_size': 6, 'gamma': 0.8237644091889288}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:53:24,534][0m Trial 17 finished with value: 0.901680052280426 and parameters: {'observation_period_num': 39, 'train_rates': 0.9418938372847603, 'learning_rate': 7.851476689724955e-06, 'batch_size': 176, 'step_size': 13, 'gamma': 0.9053764615342763}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:53:57,183][0m Trial 18 finished with value: 0.3178148865699768 and parameters: {'observation_period_num': 5, 'train_rates': 0.9835454974998056, 'learning_rate': 0.0003566677228001661, 'batch_size': 162, 'step_size': 11, 'gamma': 0.950432415290283}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:55:47,852][0m Trial 19 finished with value: 0.4839889207940217 and parameters: {'observation_period_num': 116, 'train_rates': 0.869284422253957, 'learning_rate': 6.878709048772756e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8468487473376922}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:56:38,668][0m Trial 20 finished with value: 0.29978672300066267 and parameters: {'observation_period_num': 43, 'train_rates': 0.9399235955902272, 'learning_rate': 0.00020410297616626908, 'batch_size': 111, 'step_size': 5, 'gamma': 0.8887518237305188}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:57:43,818][0m Trial 21 finished with value: 0.20954135060310364 and parameters: {'observation_period_num': 5, 'train_rates': 0.9833831996410392, 'learning_rate': 0.00010606536711282637, 'batch_size': 80, 'step_size': 15, 'gamma': 0.9891129225187656}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 15:58:57,104][0m Trial 22 finished with value: 0.2973358374607714 and parameters: {'observation_period_num': 27, 'train_rates': 0.9461720468926818, 'learning_rate': 3.1044265840838956e-05, 'batch_size': 68, 'step_size': 14, 'gamma': 0.9453564675045552}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:02:04,586][0m Trial 23 finished with value: 0.24205934598404147 and parameters: {'observation_period_num': 65, 'train_rates': 0.9113168817758613, 'learning_rate': 8.264280439718913e-05, 'batch_size': 25, 'step_size': 14, 'gamma': 0.9310257591473697}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:03:51,061][0m Trial 24 finished with value: 0.4949716031551361 and parameters: {'observation_period_num': 5, 'train_rates': 0.9881532533108903, 'learning_rate': 0.00048337067216885226, 'batch_size': 47, 'step_size': 11, 'gamma': 0.9645098357748646}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:04:54,357][0m Trial 25 finished with value: 0.3088018792240243 and parameters: {'observation_period_num': 64, 'train_rates': 0.947451191540833, 'learning_rate': 0.0001663202047102885, 'batch_size': 137, 'step_size': 14, 'gamma': 0.96426305152198}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:05:54,992][0m Trial 26 finished with value: 0.3188926721657361 and parameters: {'observation_period_num': 34, 'train_rates': 0.8862454694338046, 'learning_rate': 4.4409121138851935e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8126255681059411}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:07:32,956][0m Trial 27 finished with value: 0.5986525455246801 and parameters: {'observation_period_num': 95, 'train_rates': 0.8552177606637581, 'learning_rate': 1.8259119092366688e-05, 'batch_size': 106, 'step_size': 15, 'gamma': 0.9886560383700725}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:09:07,576][0m Trial 28 finished with value: 0.21322985263220195 and parameters: {'observation_period_num': 56, 'train_rates': 0.9600253563391475, 'learning_rate': 8.457171187082292e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.897396602571118}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:09:35,995][0m Trial 29 finished with value: 0.70582015202704 and parameters: {'observation_period_num': 23, 'train_rates': 0.7691078236539354, 'learning_rate': 0.00044682051095771126, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8664133288221036}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:10:25,697][0m Trial 30 finished with value: 0.6345746762774609 and parameters: {'observation_period_num': 47, 'train_rates': 0.925758092852493, 'learning_rate': 2.244039293878614e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.915704370691232}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:11:53,240][0m Trial 31 finished with value: 0.2671216035173053 and parameters: {'observation_period_num': 52, 'train_rates': 0.9640930021838632, 'learning_rate': 8.401212219087356e-05, 'batch_size': 58, 'step_size': 11, 'gamma': 0.8980695910499127}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:13:49,694][0m Trial 32 finished with value: 0.21021617211327695 and parameters: {'observation_period_num': 28, 'train_rates': 0.9655401787795597, 'learning_rate': 0.0001778725124302715, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8797543661071981}. Best is trial 11 with value: 0.17813467979431152.[0m
[32m[I 2025-02-06 16:16:07,222][0m Trial 33 finished with value: 0.12399747222661972 and parameters: {'observation_period_num': 26, 'train_rates': 0.9879121583785551, 'learning_rate': 0.00016027549988584385, 'batch_size': 36, 'step_size': 10, 'gamma': 0.8824744636105338}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:17:37,228][0m Trial 34 finished with value: 0.29498979449272156 and parameters: {'observation_period_num': 80, 'train_rates': 0.9852775948917116, 'learning_rate': 3.495088719097681e-05, 'batch_size': 85, 'step_size': 13, 'gamma': 0.8517426026223495}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:20:06,568][0m Trial 35 finished with value: 0.2813184523582459 and parameters: {'observation_period_num': 18, 'train_rates': 0.9234339814538446, 'learning_rate': 0.00022808229865560624, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9327319306684684}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:21:03,604][0m Trial 36 finished with value: 1.0073707436521848 and parameters: {'observation_period_num': 5, 'train_rates': 0.6745100963501186, 'learning_rate': 0.00015628967307114078, 'batch_size': 68, 'step_size': 15, 'gamma': 0.9742565868858263}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:21:53,500][0m Trial 37 finished with value: 0.3388244221287389 and parameters: {'observation_period_num': 36, 'train_rates': 0.8939290420156469, 'learning_rate': 5.2165478652481094e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.837857639928029}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:22:35,806][0m Trial 38 finished with value: 0.2737149079307985 and parameters: {'observation_period_num': 21, 'train_rates': 0.928838594335061, 'learning_rate': 0.00026993184296065354, 'batch_size': 119, 'step_size': 8, 'gamma': 0.9560436272519753}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:26:35,689][0m Trial 39 finished with value: 0.28784145627702984 and parameters: {'observation_period_num': 139, 'train_rates': 0.965304746593995, 'learning_rate': 1.19388944032239e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9715599045855895}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:27:47,794][0m Trial 40 finished with value: 0.9157703933683602 and parameters: {'observation_period_num': 73, 'train_rates': 0.7432478030558743, 'learning_rate': 0.000574554898406245, 'batch_size': 95, 'step_size': 14, 'gamma': 0.9372747529046302}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:30:06,284][0m Trial 41 finished with value: 0.2364783090353012 and parameters: {'observation_period_num': 29, 'train_rates': 0.9659923412766519, 'learning_rate': 0.00015649598696711886, 'batch_size': 36, 'step_size': 10, 'gamma': 0.8816629622268936}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:31:24,366][0m Trial 42 finished with value: 0.15051770210266113 and parameters: {'observation_period_num': 16, 'train_rates': 0.9890722133464314, 'learning_rate': 0.00011024150045781378, 'batch_size': 67, 'step_size': 7, 'gamma': 0.87783735498719}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:32:43,709][0m Trial 43 finished with value: 0.17564228177070618 and parameters: {'observation_period_num': 17, 'train_rates': 0.9835989379307891, 'learning_rate': 0.00032430590852339464, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9075183455058361}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:34:04,351][0m Trial 44 finished with value: 0.161285400390625 and parameters: {'observation_period_num': 15, 'train_rates': 0.9893175210416123, 'learning_rate': 0.00032542843448323303, 'batch_size': 63, 'step_size': 4, 'gamma': 0.9075260527134826}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:35:25,012][0m Trial 45 finished with value: 0.3349786126613617 and parameters: {'observation_period_num': 47, 'train_rates': 0.9485448260671895, 'learning_rate': 0.00035680427552923247, 'batch_size': 62, 'step_size': 4, 'gamma': 0.8718668642970867}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:37:18,769][0m Trial 46 finished with value: 0.23163613307383635 and parameters: {'observation_period_num': 98, 'train_rates': 0.9099575781573844, 'learning_rate': 0.00033320260344819034, 'batch_size': 52, 'step_size': 3, 'gamma': 0.909694960627572}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:42:57,498][0m Trial 47 finished with value: 0.25813659108602083 and parameters: {'observation_period_num': 252, 'train_rates': 0.9712451430510137, 'learning_rate': 0.00025060018871695864, 'batch_size': 70, 'step_size': 7, 'gamma': 0.9256718209901275}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:47:27,106][0m Trial 48 finished with value: 1.1501137030293926 and parameters: {'observation_period_num': 16, 'train_rates': 0.8321491180813237, 'learning_rate': 0.0006668888784023602, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7591089468286367}. Best is trial 33 with value: 0.12399747222661972.[0m
[32m[I 2025-02-06 16:51:23,008][0m Trial 49 finished with value: 0.4458980335129632 and parameters: {'observation_period_num': 190, 'train_rates': 0.9350186112630006, 'learning_rate': 6.0852062317301424e-05, 'batch_size': 32, 'step_size': 2, 'gamma': 0.8613026734595531}. Best is trial 33 with value: 0.12399747222661972.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-06 16:51:23,015][0m A new study created in memory with name: no-name-4f415518-0ae0-4e17-a569-9b8530e7ca4e[0m
[32m[I 2025-02-06 16:54:48,792][0m Trial 0 finished with value: 1.3074823097459922 and parameters: {'observation_period_num': 219, 'train_rates': 0.6166507276840143, 'learning_rate': 1.84269043861007e-05, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9213228796026274}. Best is trial 0 with value: 1.3074823097459922.[0m
[32m[I 2025-02-06 16:55:18,336][0m Trial 1 finished with value: 0.8703655247487202 and parameters: {'observation_period_num': 23, 'train_rates': 0.6904920663639023, 'learning_rate': 0.0005619130900023416, 'batch_size': 137, 'step_size': 4, 'gamma': 0.8790690874675264}. Best is trial 1 with value: 0.8703655247487202.[0m
[32m[I 2025-02-06 16:58:31,597][0m Trial 2 finished with value: 1.7608634002328178 and parameters: {'observation_period_num': 183, 'train_rates': 0.8346407853372237, 'learning_rate': 4.277205082720016e-06, 'batch_size': 212, 'step_size': 2, 'gamma': 0.8942771267915486}. Best is trial 1 with value: 0.8703655247487202.[0m
[32m[I 2025-02-06 16:59:35,352][0m Trial 3 finished with value: 0.6136746901874395 and parameters: {'observation_period_num': 33, 'train_rates': 0.7794336961662789, 'learning_rate': 0.00012079954149577424, 'batch_size': 67, 'step_size': 6, 'gamma': 0.7963819982705473}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:03:45,792][0m Trial 4 finished with value: 0.8461864691931531 and parameters: {'observation_period_num': 245, 'train_rates': 0.6795900894385116, 'learning_rate': 0.0003138326552494894, 'batch_size': 225, 'step_size': 8, 'gamma': 0.8150025380707419}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:04:35,347][0m Trial 5 finished with value: 1.1090548133123213 and parameters: {'observation_period_num': 31, 'train_rates': 0.6495333675801118, 'learning_rate': 0.000583303274419779, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8463838783526468}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:06:32,053][0m Trial 6 finished with value: 1.6005401188412685 and parameters: {'observation_period_num': 124, 'train_rates': 0.8407461650207061, 'learning_rate': 4.715501238563532e-06, 'batch_size': 225, 'step_size': 8, 'gamma': 0.8103381231083772}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:07:57,086][0m Trial 7 finished with value: 1.1855408517534243 and parameters: {'observation_period_num': 65, 'train_rates': 0.8119396452244452, 'learning_rate': 3.0349064855909512e-06, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8430026830566084}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:12:10,651][0m Trial 8 finished with value: 0.9784465099178385 and parameters: {'observation_period_num': 250, 'train_rates': 0.6626590969664006, 'learning_rate': 4.8953531657261907e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.8568752844227395}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:13:43,669][0m Trial 9 finished with value: 0.9966537725371247 and parameters: {'observation_period_num': 113, 'train_rates': 0.6292151375737106, 'learning_rate': 4.8220680367646325e-05, 'batch_size': 177, 'step_size': 13, 'gamma': 0.7864975501239649}. Best is trial 3 with value: 0.6136746901874395.[0m
[32m[I 2025-02-06 17:16:39,616][0m Trial 10 finished with value: 0.29516936293462426 and parameters: {'observation_period_num': 75, 'train_rates': 0.9717837147013089, 'learning_rate': 0.000141096334922666, 'batch_size': 28, 'step_size': 13, 'gamma': 0.9793096310637825}. Best is trial 10 with value: 0.29516936293462426.[0m
[32m[I 2025-02-06 17:20:13,393][0m Trial 11 finished with value: 0.19501990354978122 and parameters: {'observation_period_num': 72, 'train_rates': 0.9774597713816928, 'learning_rate': 0.00012017462699480984, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9882791057643175}. Best is trial 11 with value: 0.19501990354978122.[0m
[32m[I 2025-02-06 17:25:15,680][0m Trial 12 finished with value: 0.1683259388575187 and parameters: {'observation_period_num': 88, 'train_rates': 0.9775674782409541, 'learning_rate': 0.00016705765422249743, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9844422763026982}. Best is trial 12 with value: 0.1683259388575187.[0m
[32m[I 2025-02-06 17:29:28,925][0m Trial 13 finished with value: 0.2524166455323046 and parameters: {'observation_period_num': 89, 'train_rates': 0.9618206596558179, 'learning_rate': 1.7666067061587234e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9872052048642735}. Best is trial 12 with value: 0.1683259388575187.[0m
[32m[I 2025-02-06 17:32:47,009][0m Trial 14 finished with value: 0.26369739705253215 and parameters: {'observation_period_num': 161, 'train_rates': 0.9097828255525013, 'learning_rate': 0.00015212428825695927, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9495972237832346}. Best is trial 12 with value: 0.1683259388575187.[0m
[32m[I 2025-02-06 17:35:30,327][0m Trial 15 finished with value: 0.3025674895020842 and parameters: {'observation_period_num': 150, 'train_rates': 0.9066504910906313, 'learning_rate': 0.0002394706543632704, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9469101614907052}. Best is trial 12 with value: 0.1683259388575187.[0m
[32m[I 2025-02-06 17:37:09,060][0m Trial 16 finished with value: 0.5321448894817967 and parameters: {'observation_period_num': 96, 'train_rates': 0.9153538683078037, 'learning_rate': 0.0009731468735373194, 'batch_size': 161, 'step_size': 11, 'gamma': 0.955544018523308}. Best is trial 12 with value: 0.1683259388575187.[0m
[32m[I 2025-02-06 17:41:43,934][0m Trial 17 finished with value: 0.12564057697142875 and parameters: {'observation_period_num': 57, 'train_rates': 0.9810066371025419, 'learning_rate': 9.617839851595133e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9088965030470848}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:43:08,937][0m Trial 18 finished with value: 0.7672129674118107 and parameters: {'observation_period_num': 53, 'train_rates': 0.7540320628425498, 'learning_rate': 5.315418434120799e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9107649460909146}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:43:54,830][0m Trial 19 finished with value: 1.3301521577733628 and parameters: {'observation_period_num': 10, 'train_rates': 0.8724493348739512, 'learning_rate': 1.3012033211586114e-06, 'batch_size': 106, 'step_size': 13, 'gamma': 0.9244317400876543}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:45:15,294][0m Trial 20 finished with value: 0.6307811108103559 and parameters: {'observation_period_num': 47, 'train_rates': 0.9441928194094162, 'learning_rate': 1.1119271956334664e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.7575739166790129}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:49:20,390][0m Trial 21 finished with value: 0.13993826174006171 and parameters: {'observation_period_num': 98, 'train_rates': 0.9831312341282606, 'learning_rate': 8.751068988961964e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.97665007659463}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:51:30,498][0m Trial 22 finished with value: 0.17580944109470287 and parameters: {'observation_period_num': 102, 'train_rates': 0.9836083439231814, 'learning_rate': 5.776474204092284e-05, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9674002083246486}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:54:55,829][0m Trial 23 finished with value: 0.328996157792748 and parameters: {'observation_period_num': 144, 'train_rates': 0.934956609045122, 'learning_rate': 0.00025786631608243474, 'batch_size': 23, 'step_size': 11, 'gamma': 0.9403515817340344}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 17:56:29,627][0m Trial 24 finished with value: 0.3161875588052413 and parameters: {'observation_period_num': 87, 'train_rates': 0.8814979792657203, 'learning_rate': 8.48045151551748e-05, 'batch_size': 80, 'step_size': 14, 'gamma': 0.8957411172028975}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:01:19,857][0m Trial 25 finished with value: 0.22321029638220197 and parameters: {'observation_period_num': 124, 'train_rates': 0.9425447187825596, 'learning_rate': 2.6580564572875724e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9703829945616279}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:02:10,963][0m Trial 26 finished with value: 0.2661096425510423 and parameters: {'observation_period_num': 56, 'train_rates': 0.8814235636311629, 'learning_rate': 0.00036230857107659507, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9311192500616521}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:04:02,983][0m Trial 27 finished with value: 0.9416276231829489 and parameters: {'observation_period_num': 105, 'train_rates': 0.7269949034787457, 'learning_rate': 8.50843032387409e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9650911685943794}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:07:44,167][0m Trial 28 finished with value: 0.34094302065960774 and parameters: {'observation_period_num': 183, 'train_rates': 0.9446895223720332, 'learning_rate': 2.9441427983849198e-05, 'batch_size': 61, 'step_size': 12, 'gamma': 0.909657300359165}. Best is trial 17 with value: 0.12564057697142875.[0m
Early stopping at epoch 88
[32m[I 2025-02-06 18:08:29,509][0m Trial 29 finished with value: 1.0966496347523422 and parameters: {'observation_period_num': 5, 'train_rates': 0.8585669795447518, 'learning_rate': 2.0131203515785077e-05, 'batch_size': 93, 'step_size': 1, 'gamma': 0.8755156164912379}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:10:52,815][0m Trial 30 finished with value: 0.2539680304902571 and parameters: {'observation_period_num': 79, 'train_rates': 0.9815155897021173, 'learning_rate': 1.0938326276634673e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9326847085250666}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:13:04,974][0m Trial 31 finished with value: 0.16222721338272095 and parameters: {'observation_period_num': 105, 'train_rates': 0.9882094113712081, 'learning_rate': 7.187837758550893e-05, 'batch_size': 45, 'step_size': 14, 'gamma': 0.9580833667800113}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:15:23,849][0m Trial 32 finished with value: 0.14661037921905518 and parameters: {'observation_period_num': 116, 'train_rates': 0.9874054674392165, 'learning_rate': 0.00020057025595765298, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9585643844914449}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:18:02,808][0m Trial 33 finished with value: 0.19881731271743774 and parameters: {'observation_period_num': 138, 'train_rates': 0.9894069261568299, 'learning_rate': 8.300569886008755e-05, 'batch_size': 120, 'step_size': 13, 'gamma': 0.9564273686364042}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:21:19,835][0m Trial 34 finished with value: 0.4076471847628579 and parameters: {'observation_period_num': 164, 'train_rates': 0.929444088283779, 'learning_rate': 0.0004874871987242851, 'batch_size': 52, 'step_size': 14, 'gamma': 0.90915975462232}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:23:34,259][0m Trial 35 finished with value: 0.2340589259135521 and parameters: {'observation_period_num': 117, 'train_rates': 0.9587290079146326, 'learning_rate': 8.515592019964357e-05, 'batch_size': 71, 'step_size': 15, 'gamma': 0.9697397664523217}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:26:01,876][0m Trial 36 finished with value: 0.25370581654736596 and parameters: {'observation_period_num': 134, 'train_rates': 0.9028200469591872, 'learning_rate': 0.00018785758145891768, 'batch_size': 148, 'step_size': 13, 'gamma': 0.8883104217309425}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:26:54,988][0m Trial 37 finished with value: 0.3747087442499446 and parameters: {'observation_period_num': 39, 'train_rates': 0.9564623205227545, 'learning_rate': 3.5640233680831613e-05, 'batch_size': 97, 'step_size': 11, 'gamma': 0.9406437509643727}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:28:20,627][0m Trial 38 finished with value: 0.3476479363880949 and parameters: {'observation_period_num': 63, 'train_rates': 0.9251774458752691, 'learning_rate': 0.0003691987987183144, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9198688567268202}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:32:19,830][0m Trial 39 finished with value: 0.4698710490971111 and parameters: {'observation_period_num': 204, 'train_rates': 0.8125141824419087, 'learning_rate': 0.00011134518205554223, 'batch_size': 33, 'step_size': 4, 'gamma': 0.8593132919475226}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:33:53,514][0m Trial 40 finished with value: 1.0551834684731138 and parameters: {'observation_period_num': 107, 'train_rates': 0.7055183599593489, 'learning_rate': 0.0008371494996768214, 'batch_size': 184, 'step_size': 14, 'gamma': 0.9604897200807221}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:39:01,698][0m Trial 41 finished with value: 0.15196220865172724 and parameters: {'observation_period_num': 85, 'train_rates': 0.9892552318577169, 'learning_rate': 0.0001773695831983287, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9793172708065135}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:40:52,301][0m Trial 42 finished with value: 0.2322603803873062 and parameters: {'observation_period_num': 22, 'train_rates': 0.9575444485130901, 'learning_rate': 0.00020592648596198896, 'batch_size': 45, 'step_size': 15, 'gamma': 0.9780963392957533}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:42:43,611][0m Trial 43 finished with value: 1.0359384364166317 and parameters: {'observation_period_num': 79, 'train_rates': 0.6027680316832545, 'learning_rate': 7.010493478204537e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.97366440867337}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:44:55,811][0m Trial 44 finished with value: 0.24981998596140134 and parameters: {'observation_period_num': 115, 'train_rates': 0.9672840768382839, 'learning_rate': 0.00012865559928142847, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9462377901838901}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:47:41,216][0m Trial 45 finished with value: 0.17467838153243065 and parameters: {'observation_period_num': 64, 'train_rates': 0.9863724586789308, 'learning_rate': 3.9282810376161765e-05, 'batch_size': 30, 'step_size': 13, 'gamma': 0.9888780051357402}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:49:43,018][0m Trial 46 finished with value: 0.16157841682434082 and parameters: {'observation_period_num': 98, 'train_rates': 0.9890063565894052, 'learning_rate': 0.00029382020369958186, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8353114467965839}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:54:41,559][0m Trial 47 finished with value: 1.051910156751201 and parameters: {'observation_period_num': 88, 'train_rates': 0.9524149948277915, 'learning_rate': 0.0005497853826990813, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8297228817722715}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:57:16,772][0m Trial 48 finished with value: 0.2650956078654244 and parameters: {'observation_period_num': 130, 'train_rates': 0.9703372324014451, 'learning_rate': 0.00030674091177847163, 'batch_size': 65, 'step_size': 15, 'gamma': 0.8268896728019183}. Best is trial 17 with value: 0.12564057697142875.[0m
[32m[I 2025-02-06 18:58:46,166][0m Trial 49 finished with value: 0.2988714932611114 and parameters: {'observation_period_num': 42, 'train_rates': 0.9218148256002807, 'learning_rate': 0.000425823394781756, 'batch_size': 54, 'step_size': 13, 'gamma': 0.7903494168045032}. Best is trial 17 with value: 0.12564057697142875.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-06 18:58:46,174][0m A new study created in memory with name: no-name-a370ffd1-5028-4ed7-834f-eeba48ec1aeb[0m
[32m[I 2025-02-06 19:03:35,200][0m Trial 0 finished with value: 0.21604425480878686 and parameters: {'observation_period_num': 211, 'train_rates': 0.9087767485263024, 'learning_rate': 0.00013162823252754066, 'batch_size': 23, 'step_size': 12, 'gamma': 0.9287625384918291}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:05:10,171][0m Trial 1 finished with value: 0.31186185280481976 and parameters: {'observation_period_num': 95, 'train_rates': 0.9055297220103522, 'learning_rate': 4.898395294951301e-05, 'batch_size': 255, 'step_size': 11, 'gamma': 0.9772009447278764}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:06:46,435][0m Trial 2 finished with value: 1.1489570564031601 and parameters: {'observation_period_num': 88, 'train_rates': 0.825509168347474, 'learning_rate': 3.900494459326204e-06, 'batch_size': 55, 'step_size': 4, 'gamma': 0.8195898403294698}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:08:33,130][0m Trial 3 finished with value: 0.9706605344443412 and parameters: {'observation_period_num': 123, 'train_rates': 0.6845713964350755, 'learning_rate': 0.0005135568557437994, 'batch_size': 144, 'step_size': 3, 'gamma': 0.9559846146031137}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:09:49,829][0m Trial 4 finished with value: 1.2008616551233748 and parameters: {'observation_period_num': 93, 'train_rates': 0.6516833678777041, 'learning_rate': 0.0009828995047726416, 'batch_size': 249, 'step_size': 15, 'gamma': 0.8406894565515937}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:12:36,788][0m Trial 5 finished with value: 0.9713694527476273 and parameters: {'observation_period_num': 168, 'train_rates': 0.6023609924781367, 'learning_rate': 2.5487844097027357e-05, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9707544062584922}. Best is trial 0 with value: 0.21604425480878686.[0m
Early stopping at epoch 79
[32m[I 2025-02-06 19:15:14,429][0m Trial 6 finished with value: 0.7296801419531713 and parameters: {'observation_period_num': 188, 'train_rates': 0.8016927836733904, 'learning_rate': 0.00010501916730736003, 'batch_size': 63, 'step_size': 1, 'gamma': 0.8071145008596057}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:17:57,874][0m Trial 7 finished with value: 0.8014563338526137 and parameters: {'observation_period_num': 159, 'train_rates': 0.8186600209514523, 'learning_rate': 3.4740803194074106e-05, 'batch_size': 250, 'step_size': 6, 'gamma': 0.8788424191923134}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:20:36,352][0m Trial 8 finished with value: 1.3337361807606034 and parameters: {'observation_period_num': 45, 'train_rates': 0.6301266784448515, 'learning_rate': 3.1593406723993893e-06, 'batch_size': 22, 'step_size': 8, 'gamma': 0.789597067276762}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:21:16,072][0m Trial 9 finished with value: 1.1145409019880517 and parameters: {'observation_period_num': 47, 'train_rates': 0.60062061142257, 'learning_rate': 0.0003288280994294324, 'batch_size': 135, 'step_size': 10, 'gamma': 0.8118197500363238}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:26:33,510][0m Trial 10 finished with value: 0.7319565415382385 and parameters: {'observation_period_num': 241, 'train_rates': 0.979257707265809, 'learning_rate': 7.664458248166845e-06, 'batch_size': 112, 'step_size': 15, 'gamma': 0.908855529299488}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:31:53,717][0m Trial 11 finished with value: 0.2975224256515503 and parameters: {'observation_period_num': 252, 'train_rates': 0.9334445211373051, 'learning_rate': 9.277768965543246e-05, 'batch_size': 191, 'step_size': 11, 'gamma': 0.9332855826433603}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:37:00,054][0m Trial 12 finished with value: 0.25070953139891994 and parameters: {'observation_period_num': 244, 'train_rates': 0.9184797224710317, 'learning_rate': 0.0001609259079905643, 'batch_size': 186, 'step_size': 12, 'gamma': 0.9072185462342259}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:41:01,343][0m Trial 13 finished with value: 0.3077009511582645 and parameters: {'observation_period_num': 208, 'train_rates': 0.8821444796663704, 'learning_rate': 0.0002160050996456548, 'batch_size': 166, 'step_size': 13, 'gamma': 0.9051370192072944}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:44:48,415][0m Trial 14 finished with value: 0.7950371799436775 and parameters: {'observation_period_num': 219, 'train_rates': 0.7293926905083733, 'learning_rate': 0.00014822297345440236, 'batch_size': 213, 'step_size': 13, 'gamma': 0.87013935514697}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:49:26,208][0m Trial 15 finished with value: 0.40749067068099976 and parameters: {'observation_period_num': 217, 'train_rates': 0.9805455811324172, 'learning_rate': 1.600243868413861e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9289463135346528}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:52:04,683][0m Trial 16 finished with value: 1.8493214050928752 and parameters: {'observation_period_num': 148, 'train_rates': 0.865458006336881, 'learning_rate': 1.0808665971876387e-06, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8990516819109762}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:55:44,297][0m Trial 17 finished with value: 0.5217866006435132 and parameters: {'observation_period_num': 192, 'train_rates': 0.9262798044480665, 'learning_rate': 6.738204451850283e-05, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7665964821960204}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 19:59:48,273][0m Trial 18 finished with value: 1.0058463866948726 and parameters: {'observation_period_num': 232, 'train_rates': 0.7535854118770621, 'learning_rate': 0.0005706698347530836, 'batch_size': 86, 'step_size': 12, 'gamma': 0.845615133366038}. Best is trial 0 with value: 0.21604425480878686.[0m
[32m[I 2025-02-06 20:00:12,972][0m Trial 19 finished with value: 0.21529926359653473 and parameters: {'observation_period_num': 11, 'train_rates': 0.9504385477217979, 'learning_rate': 0.00020780068723321024, 'batch_size': 224, 'step_size': 10, 'gamma': 0.9445866002589044}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:01:39,445][0m Trial 20 finished with value: 0.6374637524354948 and parameters: {'observation_period_num': 17, 'train_rates': 0.8511345185882996, 'learning_rate': 0.0002894763791245282, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9477300917469073}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:04:10,868][0m Trial 21 finished with value: 0.274347186088562 and parameters: {'observation_period_num': 133, 'train_rates': 0.9561614546306697, 'learning_rate': 0.00015792204832819335, 'batch_size': 223, 'step_size': 11, 'gamma': 0.9244132618951633}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:07:45,224][0m Trial 22 finished with value: 0.24117148220539092 and parameters: {'observation_period_num': 189, 'train_rates': 0.9061546398004428, 'learning_rate': 0.000397174451475753, 'batch_size': 182, 'step_size': 14, 'gamma': 0.9863497725805735}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:11:06,831][0m Trial 23 finished with value: 0.40270498519943604 and parameters: {'observation_period_num': 181, 'train_rates': 0.8882098595139818, 'learning_rate': 0.0008923425890965911, 'batch_size': 231, 'step_size': 14, 'gamma': 0.9864016387293407}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:11:38,123][0m Trial 24 finished with value: 0.2658458650112152 and parameters: {'observation_period_num': 7, 'train_rates': 0.9490207069867511, 'learning_rate': 0.0003640609757289969, 'batch_size': 171, 'step_size': 14, 'gamma': 0.9596336865304737}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:15:27,525][0m Trial 25 finished with value: 0.5964657474914795 and parameters: {'observation_period_num': 201, 'train_rates': 0.8527128819225905, 'learning_rate': 0.000475788016297692, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9440589338392048}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:16:20,533][0m Trial 26 finished with value: 0.7597132481709875 and parameters: {'observation_period_num': 62, 'train_rates': 0.7603778943221877, 'learning_rate': 8.035898159062983e-05, 'batch_size': 224, 'step_size': 12, 'gamma': 0.9627893248533556}. Best is trial 19 with value: 0.21529926359653473.[0m
[32m[I 2025-02-06 20:18:30,173][0m Trial 27 finished with value: 0.2033480852842331 and parameters: {'observation_period_num': 124, 'train_rates': 0.9868785534337718, 'learning_rate': 0.00023570056730386606, 'batch_size': 188, 'step_size': 9, 'gamma': 0.9799180747634838}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:20:48,102][0m Trial 28 finished with value: 0.3077991306781769 and parameters: {'observation_period_num': 123, 'train_rates': 0.9841588103282362, 'learning_rate': 4.8407211378599383e-05, 'batch_size': 129, 'step_size': 6, 'gamma': 0.8869148467788549}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:22:39,068][0m Trial 29 finished with value: 0.25286439061164856 and parameters: {'observation_period_num': 106, 'train_rates': 0.9557289338083794, 'learning_rate': 0.00022538909062983246, 'batch_size': 208, 'step_size': 8, 'gamma': 0.9756187632901999}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:23:55,007][0m Trial 30 finished with value: 0.42425104379653933 and parameters: {'observation_period_num': 75, 'train_rates': 0.8975702571447014, 'learning_rate': 4.976079919503834e-05, 'batch_size': 236, 'step_size': 10, 'gamma': 0.9205640233281345}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:26:58,506][0m Trial 31 finished with value: 0.33001438246832954 and parameters: {'observation_period_num': 168, 'train_rates': 0.9193248176102575, 'learning_rate': 0.0006654092379451869, 'batch_size': 181, 'step_size': 11, 'gamma': 0.9853636931430234}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:27:38,833][0m Trial 32 finished with value: 0.24106615781784058 and parameters: {'observation_period_num': 34, 'train_rates': 0.9611263679719756, 'learning_rate': 0.00012050256547721588, 'batch_size': 155, 'step_size': 9, 'gamma': 0.9414820978194015}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:28:14,413][0m Trial 33 finished with value: 0.24089838564395905 and parameters: {'observation_period_num': 28, 'train_rates': 0.9575814728164644, 'learning_rate': 0.0001055016880988809, 'batch_size': 152, 'step_size': 9, 'gamma': 0.9421834748056317}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:28:58,400][0m Trial 34 finished with value: 0.23395434296204268 and parameters: {'observation_period_num': 25, 'train_rates': 0.9444196561710724, 'learning_rate': 0.00022244403703803152, 'batch_size': 117, 'step_size': 7, 'gamma': 0.9572348128091432}. Best is trial 27 with value: 0.2033480852842331.[0m
[32m[I 2025-02-06 20:31:12,075][0m Trial 35 finished with value: 0.16883844137191772 and parameters: {'observation_period_num': 106, 'train_rates': 0.9893391739618191, 'learning_rate': 0.00022799324653885987, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9660586637902551}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:33:31,350][0m Trial 36 finished with value: 0.1979228942150093 and parameters: {'observation_period_num': 106, 'train_rates': 0.9708907154543936, 'learning_rate': 5.9378303988306376e-05, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9693633579274453}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:35:49,318][0m Trial 37 finished with value: 0.2191762924194336 and parameters: {'observation_period_num': 106, 'train_rates': 0.9880585944730552, 'learning_rate': 1.4675045406379883e-05, 'batch_size': 40, 'step_size': 5, 'gamma': 0.9662475251051497}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:37:28,359][0m Trial 38 finished with value: 0.34210647728251314 and parameters: {'observation_period_num': 83, 'train_rates': 0.9696960714409572, 'learning_rate': 2.3715663191462178e-05, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9740790195598306}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:40:15,779][0m Trial 39 finished with value: 0.2602540772212179 and parameters: {'observation_period_num': 138, 'train_rates': 0.9326344015306297, 'learning_rate': 6.043443732267114e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.8532429072788625}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:44:08,752][0m Trial 40 finished with value: 0.6989365658318236 and parameters: {'observation_period_num': 107, 'train_rates': 0.7081822306142386, 'learning_rate': 4.079910191050257e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.949846852854704}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:46:16,143][0m Trial 41 finished with value: 0.17914016544818878 and parameters: {'observation_period_num': 71, 'train_rates': 0.988590917452763, 'learning_rate': 0.00024267738942461173, 'batch_size': 40, 'step_size': 7, 'gamma': 0.9692513447778507}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:48:35,821][0m Trial 42 finished with value: 0.2926339975425175 and parameters: {'observation_period_num': 69, 'train_rates': 0.9856672825724322, 'learning_rate': 0.00024966811674299346, 'batch_size': 36, 'step_size': 7, 'gamma': 0.9895982693446951}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:50:16,927][0m Trial 43 finished with value: 0.2604567560157335 and parameters: {'observation_period_num': 92, 'train_rates': 0.9395868062699286, 'learning_rate': 0.00018304651591906715, 'batch_size': 72, 'step_size': 6, 'gamma': 0.9692133159239027}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:53:06,070][0m Trial 44 finished with value: 0.3397240169567637 and parameters: {'observation_period_num': 115, 'train_rates': 0.9647619717478673, 'learning_rate': 0.000314961326554665, 'batch_size': 29, 'step_size': 5, 'gamma': 0.9736310562566176}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:56:02,183][0m Trial 45 finished with value: 0.727756679058075 and parameters: {'observation_period_num': 144, 'train_rates': 0.9897205764979438, 'learning_rate': 0.0007981333780350978, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9572373155940517}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:57:08,659][0m Trial 46 finished with value: 0.22125280445272272 and parameters: {'observation_period_num': 55, 'train_rates': 0.9660459152760313, 'learning_rate': 0.000129360570891611, 'batch_size': 87, 'step_size': 4, 'gamma': 0.9156934392264631}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 20:58:33,160][0m Trial 47 finished with value: 0.22447232042367643 and parameters: {'observation_period_num': 84, 'train_rates': 0.9094401287990591, 'learning_rate': 0.0004462483413394079, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9362550162097667}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 21:00:41,322][0m Trial 48 finished with value: 0.3094169795513153 and parameters: {'observation_period_num': 121, 'train_rates': 0.9398276115746796, 'learning_rate': 8.750845195086229e-05, 'batch_size': 241, 'step_size': 7, 'gamma': 0.9807392349860248}. Best is trial 35 with value: 0.16883844137191772.[0m
[32m[I 2025-02-06 21:03:02,579][0m Trial 49 finished with value: 1.1862416908191566 and parameters: {'observation_period_num': 155, 'train_rates': 0.6573496581501684, 'learning_rate': 2.6921564077361116e-05, 'batch_size': 197, 'step_size': 3, 'gamma': 0.9520371659990079}. Best is trial 35 with value: 0.16883844137191772.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 21:03:02,586][0m A new study created in memory with name: no-name-f2ea5784-d5aa-4c3f-946f-51a324d0111a[0m
[32m[I 2025-02-06 21:04:17,175][0m Trial 0 finished with value: 0.951877930547109 and parameters: {'observation_period_num': 89, 'train_rates': 0.629497577419274, 'learning_rate': 0.0006100621179991916, 'batch_size': 166, 'step_size': 6, 'gamma': 0.8176610101986479}. Best is trial 0 with value: 0.951877930547109.[0m
[32m[I 2025-02-06 21:05:12,422][0m Trial 1 finished with value: 0.9944904165836687 and parameters: {'observation_period_num': 26, 'train_rates': 0.604737160653248, 'learning_rate': 4.393058611843629e-05, 'batch_size': 64, 'step_size': 11, 'gamma': 0.7941293831732585}. Best is trial 0 with value: 0.951877930547109.[0m
[32m[I 2025-02-06 21:09:21,450][0m Trial 2 finished with value: 1.0521000838995236 and parameters: {'observation_period_num': 246, 'train_rates': 0.6556173065324281, 'learning_rate': 1.8419798873979475e-05, 'batch_size': 44, 'step_size': 7, 'gamma': 0.7940894227057743}. Best is trial 0 with value: 0.951877930547109.[0m
[32m[I 2025-02-06 21:13:14,769][0m Trial 3 finished with value: 0.8260641983326744 and parameters: {'observation_period_num': 216, 'train_rates': 0.7206091903535157, 'learning_rate': 3.3656725681708014e-05, 'batch_size': 33, 'step_size': 13, 'gamma': 0.7953752840480539}. Best is trial 3 with value: 0.8260641983326744.[0m
[32m[I 2025-02-06 21:14:05,255][0m Trial 4 finished with value: 1.1049147480625217 and parameters: {'observation_period_num': 57, 'train_rates': 0.6741323936095116, 'learning_rate': 0.00041358679951543765, 'batch_size': 116, 'step_size': 7, 'gamma': 0.9705519692688157}. Best is trial 3 with value: 0.8260641983326744.[0m
[32m[I 2025-02-06 21:14:30,293][0m Trial 5 finished with value: 1.0096072573234522 and parameters: {'observation_period_num': 19, 'train_rates': 0.6788972169921668, 'learning_rate': 5.565837945349692e-05, 'batch_size': 173, 'step_size': 8, 'gamma': 0.7511029648686677}. Best is trial 3 with value: 0.8260641983326744.[0m
[32m[I 2025-02-06 21:15:56,643][0m Trial 6 finished with value: 1.2613751888275146 and parameters: {'observation_period_num': 82, 'train_rates': 0.9379288568216453, 'learning_rate': 1.5332127910829367e-05, 'batch_size': 206, 'step_size': 4, 'gamma': 0.7726818822159148}. Best is trial 3 with value: 0.8260641983326744.[0m
[32m[I 2025-02-06 21:18:00,856][0m Trial 7 finished with value: 0.9490754440413016 and parameters: {'observation_period_num': 125, 'train_rates': 0.6185707333532365, 'learning_rate': 4.526191793946184e-05, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8956220874173996}. Best is trial 3 with value: 0.8260641983326744.[0m
[32m[I 2025-02-06 21:20:24,309][0m Trial 8 finished with value: 0.7661459612359333 and parameters: {'observation_period_num': 140, 'train_rates': 0.8174196926860573, 'learning_rate': 5.506831107855056e-05, 'batch_size': 165, 'step_size': 5, 'gamma': 0.8163576071446813}. Best is trial 8 with value: 0.7661459612359333.[0m
[32m[I 2025-02-06 21:24:57,425][0m Trial 9 finished with value: 0.7238369947475083 and parameters: {'observation_period_num': 235, 'train_rates': 0.8440984202255378, 'learning_rate': 0.0008662166241603894, 'batch_size': 161, 'step_size': 10, 'gamma': 0.946036307711072}. Best is trial 9 with value: 0.7238369947475083.[0m
[32m[I 2025-02-06 21:28:18,023][0m Trial 10 finished with value: 1.812834532211664 and parameters: {'observation_period_num': 188, 'train_rates': 0.8775528639087925, 'learning_rate': 1.911994505738455e-06, 'batch_size': 230, 'step_size': 2, 'gamma': 0.9820167060310506}. Best is trial 9 with value: 0.7238369947475083.[0m
[32m[I 2025-02-06 21:31:04,880][0m Trial 11 finished with value: 0.4659516750447518 and parameters: {'observation_period_num': 166, 'train_rates': 0.7974227912963571, 'learning_rate': 0.00017811563033053344, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8726522791402683}. Best is trial 11 with value: 0.4659516750447518.[0m
[32m[I 2025-02-06 21:34:14,814][0m Trial 12 finished with value: 0.5562092268957006 and parameters: {'observation_period_num': 177, 'train_rates': 0.8117173749021014, 'learning_rate': 0.00022685839502607557, 'batch_size': 107, 'step_size': 11, 'gamma': 0.9027016399880449}. Best is trial 11 with value: 0.4659516750447518.[0m
[32m[I 2025-02-06 21:37:01,655][0m Trial 13 finished with value: 0.7469679736801194 and parameters: {'observation_period_num': 164, 'train_rates': 0.7641492340517759, 'learning_rate': 0.0002391556726686086, 'batch_size': 96, 'step_size': 15, 'gamma': 0.8734504324531127}. Best is trial 11 with value: 0.4659516750447518.[0m
[32m[I 2025-02-06 21:40:18,208][0m Trial 14 finished with value: 0.7999316409547278 and parameters: {'observation_period_num': 189, 'train_rates': 0.7621916171846209, 'learning_rate': 0.00016548381380717065, 'batch_size': 87, 'step_size': 11, 'gamma': 0.9171531582046238}. Best is trial 11 with value: 0.4659516750447518.[0m
[32m[I 2025-02-06 21:42:44,961][0m Trial 15 finished with value: 0.3338372224747245 and parameters: {'observation_period_num': 134, 'train_rates': 0.9051679713363457, 'learning_rate': 0.00013144216704622498, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8546475710669045}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:44:47,311][0m Trial 16 finished with value: 0.38859378646133813 and parameters: {'observation_period_num': 116, 'train_rates': 0.944893176245444, 'learning_rate': 0.00011406428456625135, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8524806385320796}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:46:54,981][0m Trial 17 finished with value: 0.8843206167221069 and parameters: {'observation_period_num': 117, 'train_rates': 0.971272647746186, 'learning_rate': 6.896703378747957e-06, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8445457752437311}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:48:31,785][0m Trial 18 finished with value: 0.34015071283291726 and parameters: {'observation_period_num': 96, 'train_rates': 0.90415113686176, 'learning_rate': 0.00010803071141307742, 'batch_size': 196, 'step_size': 13, 'gamma': 0.8471491914791148}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:49:53,529][0m Trial 19 finished with value: 0.42164858363862534 and parameters: {'observation_period_num': 81, 'train_rates': 0.9030145569684448, 'learning_rate': 9.525035887125994e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.835774275497291}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:50:44,989][0m Trial 20 finished with value: 0.8957782413648523 and parameters: {'observation_period_num': 51, 'train_rates': 0.9049493522517096, 'learning_rate': 6.483933532588908e-06, 'batch_size': 196, 'step_size': 13, 'gamma': 0.928554905734647}. Best is trial 15 with value: 0.3338372224747245.[0m
[32m[I 2025-02-06 21:52:50,984][0m Trial 21 finished with value: 0.2305607944726944 and parameters: {'observation_period_num': 115, 'train_rates': 0.9739714941569599, 'learning_rate': 9.424323092396406e-05, 'batch_size': 136, 'step_size': 14, 'gamma': 0.8524460912375784}. Best is trial 21 with value: 0.2305607944726944.[0m
[32m[I 2025-02-06 21:54:36,789][0m Trial 22 finished with value: 0.2208799570798874 and parameters: {'observation_period_num': 97, 'train_rates': 0.9861256554452222, 'learning_rate': 9.261386599149838e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8614255462425905}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 21:57:31,498][0m Trial 23 finished with value: 0.51700758934021 and parameters: {'observation_period_num': 149, 'train_rates': 0.9871651507892335, 'learning_rate': 1.8706778265722088e-05, 'batch_size': 141, 'step_size': 14, 'gamma': 0.8847324876809657}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 21:59:27,744][0m Trial 24 finished with value: 0.4378357529640198 and parameters: {'observation_period_num': 103, 'train_rates': 0.9468288435131268, 'learning_rate': 0.0004288021571729753, 'batch_size': 76, 'step_size': 12, 'gamma': 0.8249109325054144}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:00:27,337][0m Trial 25 finished with value: 0.2891966700553894 and parameters: {'observation_period_num': 55, 'train_rates': 0.9868089171295071, 'learning_rate': 7.02438813309853e-05, 'batch_size': 140, 'step_size': 9, 'gamma': 0.8672577068455345}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:01:23,951][0m Trial 26 finished with value: 0.2872867286205292 and parameters: {'observation_period_num': 51, 'train_rates': 0.9899483584853483, 'learning_rate': 7.412501986752841e-05, 'batch_size': 153, 'step_size': 9, 'gamma': 0.8704585077846869}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:02:21,182][0m Trial 27 finished with value: 0.7610037326812744 and parameters: {'observation_period_num': 58, 'train_rates': 0.9588741515555854, 'learning_rate': 2.808191826345053e-05, 'batch_size': 186, 'step_size': 9, 'gamma': 0.9031298864041772}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:03:01,445][0m Trial 28 finished with value: 0.22320739924907684 and parameters: {'observation_period_num': 34, 'train_rates': 0.9870300507942616, 'learning_rate': 0.0003338365747941425, 'batch_size': 155, 'step_size': 3, 'gamma': 0.8832319927591371}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:03:35,764][0m Trial 29 finished with value: 0.4735820881058188 and parameters: {'observation_period_num': 34, 'train_rates': 0.8546225151993795, 'learning_rate': 0.00036343091228028136, 'batch_size': 214, 'step_size': 4, 'gamma': 0.8219634546544871}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:04:05,571][0m Trial 30 finished with value: 0.5562486334161444 and parameters: {'observation_period_num': 6, 'train_rates': 0.9385533420592507, 'learning_rate': 0.0002805797547204828, 'batch_size': 176, 'step_size': 1, 'gamma': 0.9317944960627321}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:04:53,787][0m Trial 31 finished with value: 0.25237420201301575 and parameters: {'observation_period_num': 43, 'train_rates': 0.9857041445464522, 'learning_rate': 0.0006432269451548265, 'batch_size': 160, 'step_size': 3, 'gamma': 0.882781428835211}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:06:14,375][0m Trial 32 finished with value: 0.4983034133911133 and parameters: {'observation_period_num': 73, 'train_rates': 0.9643676454483251, 'learning_rate': 0.0007121845092287168, 'batch_size': 154, 'step_size': 3, 'gamma': 0.88726918749066}. Best is trial 22 with value: 0.2208799570798874.[0m
Early stopping at epoch 88
[32m[I 2025-02-06 22:06:53,271][0m Trial 33 finished with value: 0.713129107768719 and parameters: {'observation_period_num': 40, 'train_rates': 0.9154512723497559, 'learning_rate': 0.0005220586704242074, 'batch_size': 151, 'step_size': 1, 'gamma': 0.8607573338198891}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:07:23,111][0m Trial 34 finished with value: 0.8347279067550387 and parameters: {'observation_period_num': 14, 'train_rates': 0.9239747303023873, 'learning_rate': 0.0009565285309783201, 'batch_size': 180, 'step_size': 6, 'gamma': 0.805808841677876}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:08:38,155][0m Trial 35 finished with value: 0.4091337385509992 and parameters: {'observation_period_num': 71, 'train_rates': 0.8708738515333498, 'learning_rate': 0.0002889392807863909, 'batch_size': 120, 'step_size': 3, 'gamma': 0.8828513269922942}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:10:34,562][0m Trial 36 finished with value: 0.6376801911861666 and parameters: {'observation_period_num': 102, 'train_rates': 0.9674603376851162, 'learning_rate': 0.0005506467829906671, 'batch_size': 60, 'step_size': 6, 'gamma': 0.9173922316549692}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:11:22,847][0m Trial 37 finished with value: 1.011121158028992 and parameters: {'observation_period_num': 35, 'train_rates': 0.9272029847619516, 'learning_rate': 3.143085154373267e-05, 'batch_size': 105, 'step_size': 2, 'gamma': 0.8378571108012797}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:11:55,274][0m Trial 38 finished with value: 0.3783215582370758 and parameters: {'observation_period_num': 26, 'train_rates': 0.9623288305176068, 'learning_rate': 0.0001715929409072445, 'batch_size': 167, 'step_size': 5, 'gamma': 0.898124087097293}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:13:09,455][0m Trial 39 finished with value: 0.412418871111161 and parameters: {'observation_period_num': 70, 'train_rates': 0.9489328241378303, 'learning_rate': 0.0003617067636784, 'batch_size': 131, 'step_size': 7, 'gamma': 0.9496219170914002}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:14:39,333][0m Trial 40 finished with value: 0.46952639479155933 and parameters: {'observation_period_num': 89, 'train_rates': 0.8860116139766977, 'learning_rate': 0.0006558403426562995, 'batch_size': 189, 'step_size': 4, 'gamma': 0.7812894460093756}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:15:35,867][0m Trial 41 finished with value: 0.22844190895557404 and parameters: {'observation_period_num': 51, 'train_rates': 0.9888267564742239, 'learning_rate': 8.581800508896648e-05, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8702278667932765}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:16:12,981][0m Trial 42 finished with value: 0.22329457104206085 and parameters: {'observation_period_num': 23, 'train_rates': 0.9857507270393671, 'learning_rate': 8.176269574886925e-05, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8622907632212433}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:16:49,862][0m Trial 43 finished with value: 0.38562577962875366 and parameters: {'observation_period_num': 20, 'train_rates': 0.973918881141409, 'learning_rate': 4.521029594269454e-05, 'batch_size': 147, 'step_size': 14, 'gamma': 0.8328934591148981}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:17:15,166][0m Trial 44 finished with value: 0.7868006495856479 and parameters: {'observation_period_num': 7, 'train_rates': 0.7021419469442132, 'learning_rate': 7.26315597040071e-05, 'batch_size': 170, 'step_size': 14, 'gamma': 0.8590602205133437}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:17:57,701][0m Trial 45 finished with value: 0.5703925974477971 and parameters: {'observation_period_num': 27, 'train_rates': 0.9327132427670792, 'learning_rate': 2.3650485019656405e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.8648663039883038}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:22:09,934][0m Trial 46 finished with value: 0.2523756297372228 and parameters: {'observation_period_num': 113, 'train_rates': 0.951210047660233, 'learning_rate': 4.029496464834365e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8039037148884972}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:23:00,033][0m Trial 47 finished with value: 0.8315522017686263 and parameters: {'observation_period_num': 64, 'train_rates': 0.6418312168907001, 'learning_rate': 8.534433509768083e-05, 'batch_size': 124, 'step_size': 14, 'gamma': 0.9100506820033927}. Best is trial 22 with value: 0.2208799570798874.[0m
[32m[I 2025-02-06 22:26:00,583][0m Trial 48 finished with value: 0.21831941604614258 and parameters: {'observation_period_num': 151, 'train_rates': 0.9745053919367286, 'learning_rate': 0.00013902055071722392, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8773915949274665}. Best is trial 48 with value: 0.21831941604614258.[0m
[32m[I 2025-02-06 22:28:07,603][0m Trial 49 finished with value: 1.0235360697031661 and parameters: {'observation_period_num': 150, 'train_rates': 0.6009692117268741, 'learning_rate': 0.00014170074616325876, 'batch_size': 105, 'step_size': 15, 'gamma': 0.8914530826912247}. Best is trial 48 with value: 0.21831941604614258.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 179, 'train_rates': 0.9893505701192629, 'learning_rate': 0.0002570231311590525, 'batch_size': 94, 'step_size': 9, 'gamma': 0.7892656128578837}
Epoch 1/300, trend Loss: 0.8302 | 1.1396
Epoch 2/300, trend Loss: 0.7572 | 0.9557
Epoch 3/300, trend Loss: 0.5952 | 0.8559
Epoch 4/300, trend Loss: 0.6678 | 0.8166
Epoch 5/300, trend Loss: 0.5730 | 0.7344
Epoch 6/300, trend Loss: 0.5872 | 0.7239
Epoch 7/300, trend Loss: 0.5145 | 0.6915
Epoch 8/300, trend Loss: 0.4968 | 0.6164
Epoch 9/300, trend Loss: 0.4061 | 0.5894
Epoch 10/300, trend Loss: 0.3801 | 0.5381
Epoch 11/300, trend Loss: 0.3612 | 0.5179
Epoch 12/300, trend Loss: 0.3451 | 0.4858
Epoch 13/300, trend Loss: 0.3297 | 0.4614
Epoch 14/300, trend Loss: 0.3499 | 0.4565
Epoch 15/300, trend Loss: 0.3126 | 0.4549
Epoch 16/300, trend Loss: 0.2889 | 0.4163
Epoch 17/300, trend Loss: 0.2783 | 0.4060
Epoch 18/300, trend Loss: 0.2939 | 0.4000
Epoch 19/300, trend Loss: 0.2786 | 0.3857
Epoch 20/300, trend Loss: 0.2672 | 0.3736
Epoch 21/300, trend Loss: 0.2502 | 0.3652
Epoch 22/300, trend Loss: 0.2537 | 0.3596
Epoch 23/300, trend Loss: 0.2492 | 0.3530
Epoch 24/300, trend Loss: 0.2495 | 0.3471
Epoch 25/300, trend Loss: 0.2407 | 0.3363
Epoch 26/300, trend Loss: 0.2325 | 0.3300
Epoch 27/300, trend Loss: 0.2245 | 0.3204
Epoch 28/300, trend Loss: 0.2189 | 0.3160
Epoch 29/300, trend Loss: 0.2179 | 0.3107
Epoch 30/300, trend Loss: 0.2127 | 0.3069
Epoch 31/300, trend Loss: 0.2123 | 0.3033
Epoch 32/300, trend Loss: 0.2075 | 0.2979
Epoch 33/300, trend Loss: 0.2068 | 0.2943
Epoch 34/300, trend Loss: 0.2054 | 0.2912
Epoch 35/300, trend Loss: 0.2026 | 0.2873
Epoch 36/300, trend Loss: 0.2007 | 0.2851
Epoch 37/300, trend Loss: 0.1999 | 0.2813
Epoch 38/300, trend Loss: 0.1979 | 0.2793
Epoch 39/300, trend Loss: 0.1967 | 0.2770
Epoch 40/300, trend Loss: 0.1939 | 0.2743
Epoch 41/300, trend Loss: 0.1923 | 0.2724
Epoch 42/300, trend Loss: 0.1916 | 0.2700
Epoch 43/300, trend Loss: 0.1905 | 0.2680
Epoch 44/300, trend Loss: 0.1902 | 0.2665
Epoch 45/300, trend Loss: 0.1883 | 0.2642
Epoch 46/300, trend Loss: 0.1879 | 0.2624
Epoch 47/300, trend Loss: 0.1870 | 0.2609
Epoch 48/300, trend Loss: 0.1857 | 0.2598
Epoch 49/300, trend Loss: 0.1853 | 0.2583
Epoch 50/300, trend Loss: 0.1845 | 0.2569
Epoch 51/300, trend Loss: 0.1851 | 0.2558
Epoch 52/300, trend Loss: 0.1838 | 0.2542
Epoch 53/300, trend Loss: 0.1829 | 0.2528
Epoch 54/300, trend Loss: 0.1829 | 0.2520
Epoch 55/300, trend Loss: 0.1819 | 0.2513
Epoch 56/300, trend Loss: 0.1825 | 0.2503
Epoch 57/300, trend Loss: 0.1818 | 0.2497
Epoch 58/300, trend Loss: 0.1808 | 0.2487
Epoch 59/300, trend Loss: 0.1806 | 0.2477
Epoch 60/300, trend Loss: 0.1795 | 0.2470
Epoch 61/300, trend Loss: 0.1797 | 0.2463
Epoch 62/300, trend Loss: 0.1793 | 0.2455
Epoch 63/300, trend Loss: 0.1791 | 0.2447
Epoch 64/300, trend Loss: 0.1782 | 0.2440
Epoch 65/300, trend Loss: 0.1772 | 0.2440
Epoch 66/300, trend Loss: 0.1779 | 0.2435
Epoch 67/300, trend Loss: 0.1775 | 0.2429
Epoch 68/300, trend Loss: 0.1772 | 0.2421
Epoch 69/300, trend Loss: 0.1775 | 0.2415
Epoch 70/300, trend Loss: 0.1772 | 0.2412
Epoch 71/300, trend Loss: 0.1767 | 0.2410
Epoch 72/300, trend Loss: 0.1770 | 0.2405
Epoch 73/300, trend Loss: 0.1755 | 0.2401
Epoch 74/300, trend Loss: 0.1759 | 0.2396
Epoch 75/300, trend Loss: 0.1749 | 0.2392
Epoch 76/300, trend Loss: 0.1761 | 0.2389
Epoch 77/300, trend Loss: 0.1752 | 0.2384
Epoch 78/300, trend Loss: 0.1750 | 0.2380
Epoch 79/300, trend Loss: 0.1756 | 0.2376
Epoch 80/300, trend Loss: 0.1754 | 0.2374
Epoch 81/300, trend Loss: 0.1746 | 0.2372
Epoch 82/300, trend Loss: 0.1745 | 0.2370
Epoch 83/300, trend Loss: 0.1743 | 0.2367
Epoch 84/300, trend Loss: 0.1738 | 0.2364
Epoch 85/300, trend Loss: 0.1742 | 0.2364
Epoch 86/300, trend Loss: 0.1744 | 0.2361
Epoch 87/300, trend Loss: 0.1742 | 0.2360
Epoch 88/300, trend Loss: 0.1736 | 0.2358
Epoch 89/300, trend Loss: 0.1742 | 0.2357
Epoch 90/300, trend Loss: 0.1738 | 0.2356
Epoch 91/300, trend Loss: 0.1735 | 0.2354
Epoch 92/300, trend Loss: 0.1731 | 0.2351
Epoch 93/300, trend Loss: 0.1739 | 0.2349
Epoch 94/300, trend Loss: 0.1734 | 0.2347
Epoch 95/300, trend Loss: 0.1732 | 0.2346
Epoch 96/300, trend Loss: 0.1732 | 0.2345
Epoch 97/300, trend Loss: 0.1732 | 0.2344
Epoch 98/300, trend Loss: 0.1726 | 0.2342
Epoch 99/300, trend Loss: 0.1729 | 0.2341
Epoch 100/300, trend Loss: 0.1732 | 0.2340
Epoch 101/300, trend Loss: 0.1724 | 0.2339
Epoch 102/300, trend Loss: 0.1725 | 0.2338
Epoch 103/300, trend Loss: 0.1726 | 0.2338
Epoch 104/300, trend Loss: 0.1724 | 0.2337
Epoch 105/300, trend Loss: 0.1728 | 0.2336
Epoch 106/300, trend Loss: 0.1719 | 0.2335
Epoch 107/300, trend Loss: 0.1717 | 0.2334
Epoch 108/300, trend Loss: 0.1724 | 0.2333
Epoch 109/300, trend Loss: 0.1724 | 0.2332
Epoch 110/300, trend Loss: 0.1723 | 0.2331
Epoch 111/300, trend Loss: 0.1716 | 0.2331
Epoch 112/300, trend Loss: 0.1722 | 0.2330
Epoch 113/300, trend Loss: 0.1720 | 0.2329
Epoch 114/300, trend Loss: 0.1723 | 0.2329
Epoch 115/300, trend Loss: 0.1725 | 0.2328
Epoch 116/300, trend Loss: 0.1725 | 0.2328
Epoch 117/300, trend Loss: 0.1730 | 0.2327
Epoch 118/300, trend Loss: 0.1724 | 0.2327
Epoch 119/300, trend Loss: 0.1717 | 0.2327
Epoch 120/300, trend Loss: 0.1723 | 0.2326
Epoch 121/300, trend Loss: 0.1723 | 0.2325
Epoch 122/300, trend Loss: 0.1724 | 0.2325
Epoch 123/300, trend Loss: 0.1717 | 0.2325
Epoch 124/300, trend Loss: 0.1725 | 0.2325
Epoch 125/300, trend Loss: 0.1720 | 0.2324
Epoch 126/300, trend Loss: 0.1717 | 0.2323
Epoch 127/300, trend Loss: 0.1719 | 0.2323
Epoch 128/300, trend Loss: 0.1721 | 0.2323
Epoch 129/300, trend Loss: 0.1725 | 0.2323
Epoch 130/300, trend Loss: 0.1714 | 0.2323
Epoch 131/300, trend Loss: 0.1717 | 0.2322
Epoch 132/300, trend Loss: 0.1723 | 0.2322
Epoch 133/300, trend Loss: 0.1715 | 0.2322
Epoch 134/300, trend Loss: 0.1717 | 0.2321
Epoch 135/300, trend Loss: 0.1724 | 0.2321
Epoch 136/300, trend Loss: 0.1725 | 0.2321
Epoch 137/300, trend Loss: 0.1719 | 0.2321
Epoch 138/300, trend Loss: 0.1711 | 0.2320
Epoch 139/300, trend Loss: 0.1721 | 0.2320
Epoch 140/300, trend Loss: 0.1722 | 0.2320
Epoch 141/300, trend Loss: 0.1718 | 0.2320
Epoch 142/300, trend Loss: 0.1719 | 0.2320
Epoch 143/300, trend Loss: 0.1724 | 0.2320
Epoch 144/300, trend Loss: 0.1722 | 0.2319
Epoch 145/300, trend Loss: 0.1725 | 0.2319
Epoch 146/300, trend Loss: 0.1721 | 0.2319
Epoch 147/300, trend Loss: 0.1722 | 0.2319
Epoch 148/300, trend Loss: 0.1721 | 0.2319
Epoch 149/300, trend Loss: 0.1720 | 0.2319
Epoch 150/300, trend Loss: 0.1716 | 0.2319
Epoch 151/300, trend Loss: 0.1715 | 0.2319
Epoch 152/300, trend Loss: 0.1718 | 0.2319
Epoch 153/300, trend Loss: 0.1714 | 0.2319
Epoch 154/300, trend Loss: 0.1726 | 0.2319
Epoch 155/300, trend Loss: 0.1720 | 0.2318
Epoch 156/300, trend Loss: 0.1712 | 0.2318
Epoch 157/300, trend Loss: 0.1718 | 0.2318
Epoch 158/300, trend Loss: 0.1717 | 0.2318
Epoch 159/300, trend Loss: 0.1714 | 0.2318
Epoch 160/300, trend Loss: 0.1719 | 0.2318
Epoch 161/300, trend Loss: 0.1717 | 0.2318
Epoch 162/300, trend Loss: 0.1713 | 0.2318
Epoch 163/300, trend Loss: 0.1718 | 0.2318
Epoch 164/300, trend Loss: 0.1716 | 0.2318
Epoch 165/300, trend Loss: 0.1713 | 0.2318
Epoch 166/300, trend Loss: 0.1721 | 0.2318
Epoch 167/300, trend Loss: 0.1721 | 0.2318
Epoch 168/300, trend Loss: 0.1718 | 0.2318
Epoch 169/300, trend Loss: 0.1712 | 0.2318
Epoch 170/300, trend Loss: 0.1723 | 0.2317
Epoch 171/300, trend Loss: 0.1706 | 0.2317
Epoch 172/300, trend Loss: 0.1719 | 0.2317
Epoch 173/300, trend Loss: 0.1713 | 0.2317
Epoch 174/300, trend Loss: 0.1713 | 0.2317
Epoch 175/300, trend Loss: 0.1712 | 0.2317
Epoch 176/300, trend Loss: 0.1712 | 0.2317
Epoch 177/300, trend Loss: 0.1709 | 0.2317
Epoch 178/300, trend Loss: 0.1723 | 0.2317
Epoch 179/300, trend Loss: 0.1713 | 0.2317
Epoch 180/300, trend Loss: 0.1718 | 0.2317
Epoch 181/300, trend Loss: 0.1726 | 0.2317
Epoch 182/300, trend Loss: 0.1718 | 0.2317
Epoch 183/300, trend Loss: 0.1711 | 0.2317
Epoch 184/300, trend Loss: 0.1719 | 0.2317
Epoch 185/300, trend Loss: 0.1722 | 0.2317
Epoch 186/300, trend Loss: 0.1713 | 0.2317
Epoch 187/300, trend Loss: 0.1721 | 0.2317
Epoch 188/300, trend Loss: 0.1717 | 0.2317
Epoch 189/300, trend Loss: 0.1717 | 0.2317
Epoch 190/300, trend Loss: 0.1721 | 0.2317
Epoch 191/300, trend Loss: 0.1723 | 0.2317
Epoch 192/300, trend Loss: 0.1727 | 0.2317
Epoch 193/300, trend Loss: 0.1721 | 0.2317
Epoch 194/300, trend Loss: 0.1720 | 0.2317
Epoch 195/300, trend Loss: 0.1716 | 0.2317
Epoch 196/300, trend Loss: 0.1719 | 0.2317
Epoch 197/300, trend Loss: 0.1712 | 0.2317
Epoch 198/300, trend Loss: 0.1714 | 0.2317
Epoch 199/300, trend Loss: 0.1719 | 0.2317
Epoch 200/300, trend Loss: 0.1714 | 0.2317
Epoch 201/300, trend Loss: 0.1717 | 0.2317
Epoch 202/300, trend Loss: 0.1719 | 0.2317
Epoch 203/300, trend Loss: 0.1722 | 0.2317
Epoch 204/300, trend Loss: 0.1718 | 0.2317
Epoch 205/300, trend Loss: 0.1717 | 0.2317
Epoch 206/300, trend Loss: 0.1723 | 0.2317
Epoch 207/300, trend Loss: 0.1717 | 0.2317
Epoch 208/300, trend Loss: 0.1717 | 0.2317
Epoch 209/300, trend Loss: 0.1717 | 0.2317
Epoch 210/300, trend Loss: 0.1715 | 0.2317
Epoch 211/300, trend Loss: 0.1714 | 0.2317
Epoch 212/300, trend Loss: 0.1717 | 0.2317
Epoch 213/300, trend Loss: 0.1715 | 0.2317
Epoch 214/300, trend Loss: 0.1719 | 0.2317
Epoch 215/300, trend Loss: 0.1714 | 0.2317
Epoch 216/300, trend Loss: 0.1720 | 0.2317
Epoch 217/300, trend Loss: 0.1721 | 0.2317
Epoch 218/300, trend Loss: 0.1707 | 0.2317
Epoch 219/300, trend Loss: 0.1721 | 0.2317
Epoch 220/300, trend Loss: 0.1721 | 0.2317
Epoch 221/300, trend Loss: 0.1714 | 0.2317
Epoch 222/300, trend Loss: 0.1715 | 0.2317
Epoch 223/300, trend Loss: 0.1719 | 0.2317
Epoch 224/300, trend Loss: 0.1725 | 0.2317
Epoch 225/300, trend Loss: 0.1720 | 0.2317
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 170, 'train_rates': 0.9727977559365747, 'learning_rate': 8.245615925140233e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.815065307098686}
Epoch 1/300, seasonal_0 Loss: 0.5920 | 1.0123
Epoch 2/300, seasonal_0 Loss: 0.4473 | 0.7923
Epoch 3/300, seasonal_0 Loss: 0.3836 | 0.6984
Epoch 4/300, seasonal_0 Loss: 0.3292 | 0.6222
Epoch 5/300, seasonal_0 Loss: 0.2968 | 0.5886
Epoch 6/300, seasonal_0 Loss: 0.2940 | 0.4804
Epoch 7/300, seasonal_0 Loss: 0.2978 | 0.5351
Epoch 8/300, seasonal_0 Loss: 0.2861 | 0.4267
Epoch 9/300, seasonal_0 Loss: 0.2602 | 0.4609
Epoch 10/300, seasonal_0 Loss: 0.2494 | 0.4361
Epoch 11/300, seasonal_0 Loss: 0.2438 | 0.4375
Epoch 12/300, seasonal_0 Loss: 0.2287 | 0.3674
Epoch 13/300, seasonal_0 Loss: 0.2261 | 0.3451
Epoch 14/300, seasonal_0 Loss: 0.2298 | 0.4082
Epoch 15/300, seasonal_0 Loss: 0.2326 | 0.3746
Epoch 16/300, seasonal_0 Loss: 0.2311 | 0.3944
Epoch 17/300, seasonal_0 Loss: 0.2276 | 0.3324
Epoch 18/300, seasonal_0 Loss: 0.2201 | 0.3930
Epoch 19/300, seasonal_0 Loss: 0.2155 | 0.3157
Epoch 20/300, seasonal_0 Loss: 0.2027 | 0.3245
Epoch 21/300, seasonal_0 Loss: 0.1935 | 0.2992
Epoch 22/300, seasonal_0 Loss: 0.1897 | 0.2900
Epoch 23/300, seasonal_0 Loss: 0.1879 | 0.3072
Epoch 24/300, seasonal_0 Loss: 0.1844 | 0.3082
Epoch 25/300, seasonal_0 Loss: 0.1858 | 0.2948
Epoch 26/300, seasonal_0 Loss: 0.1803 | 0.2669
Epoch 27/300, seasonal_0 Loss: 0.1793 | 0.2638
Epoch 28/300, seasonal_0 Loss: 0.1753 | 0.3176
Epoch 29/300, seasonal_0 Loss: 0.1706 | 0.2902
Epoch 30/300, seasonal_0 Loss: 0.1701 | 0.2702
Epoch 31/300, seasonal_0 Loss: 0.1695 | 0.2569
Epoch 32/300, seasonal_0 Loss: 0.1650 | 0.2466
Epoch 33/300, seasonal_0 Loss: 0.1646 | 0.2866
Epoch 34/300, seasonal_0 Loss: 0.1631 | 0.2763
Epoch 35/300, seasonal_0 Loss: 0.1614 | 0.2683
Epoch 36/300, seasonal_0 Loss: 0.1626 | 0.2436
Epoch 37/300, seasonal_0 Loss: 0.1618 | 0.2341
Epoch 38/300, seasonal_0 Loss: 0.1560 | 0.2730
Epoch 39/300, seasonal_0 Loss: 0.1542 | 0.2628
Epoch 40/300, seasonal_0 Loss: 0.1525 | 0.2500
Epoch 41/300, seasonal_0 Loss: 0.1522 | 0.2322
Epoch 42/300, seasonal_0 Loss: 0.1509 | 0.2273
Epoch 43/300, seasonal_0 Loss: 0.1483 | 0.2436
Epoch 44/300, seasonal_0 Loss: 0.1470 | 0.2466
Epoch 45/300, seasonal_0 Loss: 0.1449 | 0.2461
Epoch 46/300, seasonal_0 Loss: 0.1440 | 0.2305
Epoch 47/300, seasonal_0 Loss: 0.1437 | 0.2206
Epoch 48/300, seasonal_0 Loss: 0.1427 | 0.2181
Epoch 49/300, seasonal_0 Loss: 0.1411 | 0.2296
Epoch 50/300, seasonal_0 Loss: 0.1407 | 0.2319
Epoch 51/300, seasonal_0 Loss: 0.1393 | 0.2328
Epoch 52/300, seasonal_0 Loss: 0.1384 | 0.2254
Epoch 53/300, seasonal_0 Loss: 0.1386 | 0.2124
Epoch 54/300, seasonal_0 Loss: 0.1376 | 0.2067
Epoch 55/300, seasonal_0 Loss: 0.1364 | 0.2140
Epoch 56/300, seasonal_0 Loss: 0.1369 | 0.2270
Epoch 57/300, seasonal_0 Loss: 0.1364 | 0.2343
Epoch 58/300, seasonal_0 Loss: 0.1346 | 0.2177
Epoch 59/300, seasonal_0 Loss: 0.1358 | 0.2044
Epoch 60/300, seasonal_0 Loss: 0.1349 | 0.2011
Epoch 61/300, seasonal_0 Loss: 0.1345 | 0.2142
Epoch 62/300, seasonal_0 Loss: 0.1337 | 0.2280
Epoch 63/300, seasonal_0 Loss: 0.1328 | 0.2195
Epoch 64/300, seasonal_0 Loss: 0.1322 | 0.2097
Epoch 65/300, seasonal_0 Loss: 0.1313 | 0.2018
Epoch 66/300, seasonal_0 Loss: 0.1312 | 0.2022
Epoch 67/300, seasonal_0 Loss: 0.1303 | 0.2130
Epoch 68/300, seasonal_0 Loss: 0.1300 | 0.2106
Epoch 69/300, seasonal_0 Loss: 0.1300 | 0.2091
Epoch 70/300, seasonal_0 Loss: 0.1286 | 0.2044
Epoch 71/300, seasonal_0 Loss: 0.1284 | 0.2023
Epoch 72/300, seasonal_0 Loss: 0.1288 | 0.2053
Epoch 73/300, seasonal_0 Loss: 0.1280 | 0.2045
Epoch 74/300, seasonal_0 Loss: 0.1282 | 0.2024
Epoch 75/300, seasonal_0 Loss: 0.1273 | 0.2011
Epoch 76/300, seasonal_0 Loss: 0.1267 | 0.2029
Epoch 77/300, seasonal_0 Loss: 0.1271 | 0.2028
Epoch 78/300, seasonal_0 Loss: 0.1259 | 0.2000
Epoch 79/300, seasonal_0 Loss: 0.1260 | 0.2015
Epoch 80/300, seasonal_0 Loss: 0.1261 | 0.2029
Epoch 81/300, seasonal_0 Loss: 0.1263 | 0.2035
Epoch 82/300, seasonal_0 Loss: 0.1258 | 0.1991
Epoch 83/300, seasonal_0 Loss: 0.1254 | 0.1994
Epoch 84/300, seasonal_0 Loss: 0.1249 | 0.1994
Epoch 85/300, seasonal_0 Loss: 0.1243 | 0.2004
Epoch 86/300, seasonal_0 Loss: 0.1244 | 0.1972
Epoch 87/300, seasonal_0 Loss: 0.1239 | 0.1996
Epoch 88/300, seasonal_0 Loss: 0.1237 | 0.1997
Epoch 89/300, seasonal_0 Loss: 0.1241 | 0.1993
Epoch 90/300, seasonal_0 Loss: 0.1237 | 0.1974
Epoch 91/300, seasonal_0 Loss: 0.1237 | 0.1976
Epoch 92/300, seasonal_0 Loss: 0.1240 | 0.1980
Epoch 93/300, seasonal_0 Loss: 0.1237 | 0.1971
Epoch 94/300, seasonal_0 Loss: 0.1229 | 0.1971
Epoch 95/300, seasonal_0 Loss: 0.1233 | 0.1969
Epoch 96/300, seasonal_0 Loss: 0.1236 | 0.1985
Epoch 97/300, seasonal_0 Loss: 0.1228 | 0.1975
Epoch 98/300, seasonal_0 Loss: 0.1219 | 0.1969
Epoch 99/300, seasonal_0 Loss: 0.1224 | 0.1941
Epoch 100/300, seasonal_0 Loss: 0.1223 | 0.1963
Epoch 101/300, seasonal_0 Loss: 0.1224 | 0.1957
Epoch 102/300, seasonal_0 Loss: 0.1214 | 0.1965
Epoch 103/300, seasonal_0 Loss: 0.1213 | 0.1957
Epoch 104/300, seasonal_0 Loss: 0.1220 | 0.1957
Epoch 105/300, seasonal_0 Loss: 0.1215 | 0.1959
Epoch 106/300, seasonal_0 Loss: 0.1217 | 0.1949
Epoch 107/300, seasonal_0 Loss: 0.1221 | 0.1953
Epoch 108/300, seasonal_0 Loss: 0.1218 | 0.1948
Epoch 109/300, seasonal_0 Loss: 0.1213 | 0.1954
Epoch 110/300, seasonal_0 Loss: 0.1221 | 0.1950
Epoch 111/300, seasonal_0 Loss: 0.1208 | 0.1953
Epoch 112/300, seasonal_0 Loss: 0.1207 | 0.1957
Epoch 113/300, seasonal_0 Loss: 0.1218 | 0.1938
Epoch 114/300, seasonal_0 Loss: 0.1206 | 0.1941
Epoch 115/300, seasonal_0 Loss: 0.1209 | 0.1941
Epoch 116/300, seasonal_0 Loss: 0.1213 | 0.1941
Epoch 117/300, seasonal_0 Loss: 0.1214 | 0.1936
Epoch 118/300, seasonal_0 Loss: 0.1204 | 0.1936
Epoch 119/300, seasonal_0 Loss: 0.1206 | 0.1948
Epoch 120/300, seasonal_0 Loss: 0.1207 | 0.1933
Epoch 121/300, seasonal_0 Loss: 0.1213 | 0.1936
Epoch 122/300, seasonal_0 Loss: 0.1200 | 0.1932
Epoch 123/300, seasonal_0 Loss: 0.1204 | 0.1934
Epoch 124/300, seasonal_0 Loss: 0.1203 | 0.1937
Epoch 125/300, seasonal_0 Loss: 0.1197 | 0.1945
Epoch 126/300, seasonal_0 Loss: 0.1197 | 0.1936
Epoch 127/300, seasonal_0 Loss: 0.1200 | 0.1932
Epoch 128/300, seasonal_0 Loss: 0.1196 | 0.1930
Epoch 129/300, seasonal_0 Loss: 0.1195 | 0.1933
Epoch 130/300, seasonal_0 Loss: 0.1193 | 0.1935
Epoch 131/300, seasonal_0 Loss: 0.1204 | 0.1924
Epoch 132/300, seasonal_0 Loss: 0.1203 | 0.1924
Epoch 133/300, seasonal_0 Loss: 0.1204 | 0.1920
Epoch 134/300, seasonal_0 Loss: 0.1203 | 0.1917
Epoch 135/300, seasonal_0 Loss: 0.1205 | 0.1916
Epoch 136/300, seasonal_0 Loss: 0.1191 | 0.1916
Epoch 137/300, seasonal_0 Loss: 0.1191 | 0.1921
Epoch 138/300, seasonal_0 Loss: 0.1201 | 0.1924
Epoch 139/300, seasonal_0 Loss: 0.1196 | 0.1918
Epoch 140/300, seasonal_0 Loss: 0.1192 | 0.1921
Epoch 141/300, seasonal_0 Loss: 0.1196 | 0.1922
Epoch 142/300, seasonal_0 Loss: 0.1194 | 0.1915
Epoch 143/300, seasonal_0 Loss: 0.1191 | 0.1918
Epoch 144/300, seasonal_0 Loss: 0.1201 | 0.1921
Epoch 145/300, seasonal_0 Loss: 0.1199 | 0.1922
Epoch 146/300, seasonal_0 Loss: 0.1196 | 0.1922
Epoch 147/300, seasonal_0 Loss: 0.1202 | 0.1922
Epoch 148/300, seasonal_0 Loss: 0.1185 | 0.1919
Epoch 149/300, seasonal_0 Loss: 0.1198 | 0.1918
Epoch 150/300, seasonal_0 Loss: 0.1193 | 0.1920
Epoch 151/300, seasonal_0 Loss: 0.1191 | 0.1919
Epoch 152/300, seasonal_0 Loss: 0.1197 | 0.1918
Epoch 153/300, seasonal_0 Loss: 0.1198 | 0.1922
Epoch 154/300, seasonal_0 Loss: 0.1193 | 0.1920
Epoch 155/300, seasonal_0 Loss: 0.1194 | 0.1918
Epoch 156/300, seasonal_0 Loss: 0.1192 | 0.1915
Epoch 157/300, seasonal_0 Loss: 0.1191 | 0.1915
Epoch 158/300, seasonal_0 Loss: 0.1197 | 0.1917
Epoch 159/300, seasonal_0 Loss: 0.1191 | 0.1916
Epoch 160/300, seasonal_0 Loss: 0.1193 | 0.1914
Epoch 161/300, seasonal_0 Loss: 0.1191 | 0.1913
Epoch 162/300, seasonal_0 Loss: 0.1195 | 0.1911
Epoch 163/300, seasonal_0 Loss: 0.1194 | 0.1912
Epoch 164/300, seasonal_0 Loss: 0.1193 | 0.1912
Epoch 165/300, seasonal_0 Loss: 0.1186 | 0.1913
Epoch 166/300, seasonal_0 Loss: 0.1192 | 0.1914
Epoch 167/300, seasonal_0 Loss: 0.1189 | 0.1913
Epoch 168/300, seasonal_0 Loss: 0.1197 | 0.1914
Epoch 169/300, seasonal_0 Loss: 0.1199 | 0.1916
Epoch 170/300, seasonal_0 Loss: 0.1196 | 0.1915
Epoch 171/300, seasonal_0 Loss: 0.1189 | 0.1917
Epoch 172/300, seasonal_0 Loss: 0.1194 | 0.1916
Epoch 173/300, seasonal_0 Loss: 0.1185 | 0.1914
Epoch 174/300, seasonal_0 Loss: 0.1197 | 0.1913
Epoch 175/300, seasonal_0 Loss: 0.1198 | 0.1912
Epoch 176/300, seasonal_0 Loss: 0.1188 | 0.1911
Epoch 177/300, seasonal_0 Loss: 0.1187 | 0.1913
Epoch 178/300, seasonal_0 Loss: 0.1197 | 0.1913
Epoch 179/300, seasonal_0 Loss: 0.1197 | 0.1913
Epoch 180/300, seasonal_0 Loss: 0.1195 | 0.1914
Epoch 181/300, seasonal_0 Loss: 0.1198 | 0.1914
Epoch 182/300, seasonal_0 Loss: 0.1193 | 0.1913
Epoch 183/300, seasonal_0 Loss: 0.1192 | 0.1911
Epoch 184/300, seasonal_0 Loss: 0.1192 | 0.1910
Epoch 185/300, seasonal_0 Loss: 0.1183 | 0.1911
Epoch 186/300, seasonal_0 Loss: 0.1191 | 0.1910
Epoch 187/300, seasonal_0 Loss: 0.1194 | 0.1910
Epoch 188/300, seasonal_0 Loss: 0.1186 | 0.1911
Epoch 189/300, seasonal_0 Loss: 0.1194 | 0.1910
Epoch 190/300, seasonal_0 Loss: 0.1194 | 0.1911
Epoch 191/300, seasonal_0 Loss: 0.1190 | 0.1910
Epoch 192/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 193/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 194/300, seasonal_0 Loss: 0.1194 | 0.1907
Epoch 195/300, seasonal_0 Loss: 0.1191 | 0.1907
Epoch 196/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 197/300, seasonal_0 Loss: 0.1194 | 0.1907
Epoch 198/300, seasonal_0 Loss: 0.1188 | 0.1907
Epoch 199/300, seasonal_0 Loss: 0.1196 | 0.1907
Epoch 200/300, seasonal_0 Loss: 0.1184 | 0.1907
Epoch 201/300, seasonal_0 Loss: 0.1191 | 0.1907
Epoch 202/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 203/300, seasonal_0 Loss: 0.1185 | 0.1908
Epoch 204/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 205/300, seasonal_0 Loss: 0.1194 | 0.1909
Epoch 206/300, seasonal_0 Loss: 0.1192 | 0.1909
Epoch 207/300, seasonal_0 Loss: 0.1193 | 0.1909
Epoch 208/300, seasonal_0 Loss: 0.1182 | 0.1909
Epoch 209/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 210/300, seasonal_0 Loss: 0.1189 | 0.1909
Epoch 211/300, seasonal_0 Loss: 0.1193 | 0.1909
Epoch 212/300, seasonal_0 Loss: 0.1198 | 0.1909
Epoch 213/300, seasonal_0 Loss: 0.1185 | 0.1909
Epoch 214/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 215/300, seasonal_0 Loss: 0.1189 | 0.1909
Epoch 216/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 217/300, seasonal_0 Loss: 0.1186 | 0.1909
Epoch 218/300, seasonal_0 Loss: 0.1183 | 0.1909
Epoch 219/300, seasonal_0 Loss: 0.1196 | 0.1909
Epoch 220/300, seasonal_0 Loss: 0.1193 | 0.1909
Epoch 221/300, seasonal_0 Loss: 0.1195 | 0.1909
Epoch 222/300, seasonal_0 Loss: 0.1187 | 0.1909
Epoch 223/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 224/300, seasonal_0 Loss: 0.1185 | 0.1909
Epoch 225/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 226/300, seasonal_0 Loss: 0.1191 | 0.1909
Epoch 227/300, seasonal_0 Loss: 0.1191 | 0.1909
Epoch 228/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 229/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 230/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 231/300, seasonal_0 Loss: 0.1179 | 0.1908
Epoch 232/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 233/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 234/300, seasonal_0 Loss: 0.1196 | 0.1909
Epoch 235/300, seasonal_0 Loss: 0.1186 | 0.1909
Epoch 236/300, seasonal_0 Loss: 0.1191 | 0.1909
Epoch 237/300, seasonal_0 Loss: 0.1190 | 0.1909
Epoch 238/300, seasonal_0 Loss: 0.1185 | 0.1909
Epoch 239/300, seasonal_0 Loss: 0.1191 | 0.1909
Epoch 240/300, seasonal_0 Loss: 0.1185 | 0.1909
Epoch 241/300, seasonal_0 Loss: 0.1192 | 0.1909
Epoch 242/300, seasonal_0 Loss: 0.1181 | 0.1909
Epoch 243/300, seasonal_0 Loss: 0.1183 | 0.1909
Epoch 244/300, seasonal_0 Loss: 0.1196 | 0.1908
Epoch 245/300, seasonal_0 Loss: 0.1184 | 0.1908
Epoch 246/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 247/300, seasonal_0 Loss: 0.1195 | 0.1908
Epoch 248/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 249/300, seasonal_0 Loss: 0.1191 | 0.1908
Epoch 250/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 251/300, seasonal_0 Loss: 0.1186 | 0.1908
Epoch 252/300, seasonal_0 Loss: 0.1194 | 0.1908
Epoch 253/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 254/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 255/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 256/300, seasonal_0 Loss: 0.1188 | 0.1908
Epoch 257/300, seasonal_0 Loss: 0.1191 | 0.1908
Epoch 258/300, seasonal_0 Loss: 0.1191 | 0.1908
Epoch 259/300, seasonal_0 Loss: 0.1193 | 0.1908
Epoch 260/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 261/300, seasonal_0 Loss: 0.1189 | 0.1908
Epoch 262/300, seasonal_0 Loss: 0.1184 | 0.1908
Epoch 263/300, seasonal_0 Loss: 0.1198 | 0.1908
Epoch 264/300, seasonal_0 Loss: 0.1185 | 0.1908
Epoch 265/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 266/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 267/300, seasonal_0 Loss: 0.1185 | 0.1908
Epoch 268/300, seasonal_0 Loss: 0.1183 | 0.1908
Epoch 269/300, seasonal_0 Loss: 0.1191 | 0.1908
Epoch 270/300, seasonal_0 Loss: 0.1180 | 0.1908
Epoch 271/300, seasonal_0 Loss: 0.1195 | 0.1908
Epoch 272/300, seasonal_0 Loss: 0.1179 | 0.1908
Epoch 273/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 274/300, seasonal_0 Loss: 0.1184 | 0.1908
Epoch 275/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 276/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 277/300, seasonal_0 Loss: 0.1196 | 0.1908
Epoch 278/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 279/300, seasonal_0 Loss: 0.1195 | 0.1908
Epoch 280/300, seasonal_0 Loss: 0.1191 | 0.1908
Epoch 281/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 282/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 283/300, seasonal_0 Loss: 0.1185 | 0.1908
Epoch 284/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 285/300, seasonal_0 Loss: 0.1194 | 0.1908
Epoch 286/300, seasonal_0 Loss: 0.1187 | 0.1908
Epoch 287/300, seasonal_0 Loss: 0.1188 | 0.1908
Epoch 288/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 289/300, seasonal_0 Loss: 0.1189 | 0.1908
Epoch 290/300, seasonal_0 Loss: 0.1196 | 0.1908
Epoch 291/300, seasonal_0 Loss: 0.1183 | 0.1908
Epoch 292/300, seasonal_0 Loss: 0.1188 | 0.1908
Epoch 293/300, seasonal_0 Loss: 0.1184 | 0.1908
Epoch 294/300, seasonal_0 Loss: 0.1190 | 0.1908
Epoch 295/300, seasonal_0 Loss: 0.1188 | 0.1908
Epoch 296/300, seasonal_0 Loss: 0.1185 | 0.1908
Epoch 297/300, seasonal_0 Loss: 0.1192 | 0.1908
Epoch 298/300, seasonal_0 Loss: 0.1189 | 0.1908
Epoch 299/300, seasonal_0 Loss: 0.1184 | 0.1908
Epoch 300/300, seasonal_0 Loss: 0.1184 | 0.1908
Training seasonal_1 component with params: {'observation_period_num': 26, 'train_rates': 0.9879121583785551, 'learning_rate': 0.00016027549988584385, 'batch_size': 36, 'step_size': 10, 'gamma': 0.8824744636105338}
Epoch 1/300, seasonal_1 Loss: 0.7277 | 0.9409
Epoch 2/300, seasonal_1 Loss: 0.6465 | 0.8214
Epoch 3/300, seasonal_1 Loss: 0.5711 | 0.7143
Epoch 4/300, seasonal_1 Loss: 0.5278 | 0.6430
Epoch 5/300, seasonal_1 Loss: 0.4745 | 0.5679
Epoch 6/300, seasonal_1 Loss: 0.3992 | 0.5439
Epoch 7/300, seasonal_1 Loss: 0.3775 | 0.5891
Epoch 8/300, seasonal_1 Loss: 0.3560 | 0.4946
Epoch 9/300, seasonal_1 Loss: 0.3811 | 0.5200
Epoch 10/300, seasonal_1 Loss: 0.3369 | 0.4313
Epoch 11/300, seasonal_1 Loss: 0.2957 | 0.4264
Epoch 12/300, seasonal_1 Loss: 0.2811 | 0.4023
Epoch 13/300, seasonal_1 Loss: 0.2606 | 0.3872
Epoch 14/300, seasonal_1 Loss: 0.2660 | 0.3843
Epoch 15/300, seasonal_1 Loss: 0.2670 | 0.3502
Epoch 16/300, seasonal_1 Loss: 0.2827 | 0.3681
Epoch 17/300, seasonal_1 Loss: 0.3271 | 0.4274
Epoch 18/300, seasonal_1 Loss: 0.3030 | 0.3972
Epoch 19/300, seasonal_1 Loss: 0.2699 | 0.3477
Epoch 20/300, seasonal_1 Loss: 0.2374 | 0.3305
Epoch 21/300, seasonal_1 Loss: 0.2409 | 0.3297
Epoch 22/300, seasonal_1 Loss: 0.2281 | 0.3187
Epoch 23/300, seasonal_1 Loss: 0.2294 | 0.3306
Epoch 24/300, seasonal_1 Loss: 0.2414 | 0.3244
Epoch 25/300, seasonal_1 Loss: 0.2122 | 0.2930
Epoch 26/300, seasonal_1 Loss: 0.2114 | 0.2890
Epoch 27/300, seasonal_1 Loss: 0.2083 | 0.3537
Epoch 28/300, seasonal_1 Loss: 0.1973 | 0.3003
Epoch 29/300, seasonal_1 Loss: 0.2030 | 0.2851
Epoch 30/300, seasonal_1 Loss: 0.2169 | 0.2842
Epoch 31/300, seasonal_1 Loss: 0.2119 | 0.3183
Epoch 32/300, seasonal_1 Loss: 0.1935 | 0.2989
Epoch 33/300, seasonal_1 Loss: 0.1920 | 0.2791
Epoch 34/300, seasonal_1 Loss: 0.1885 | 0.2931
Epoch 35/300, seasonal_1 Loss: 0.1770 | 0.2463
Epoch 36/300, seasonal_1 Loss: 0.1744 | 0.2484
Epoch 37/300, seasonal_1 Loss: 0.1764 | 0.2543
Epoch 38/300, seasonal_1 Loss: 0.1667 | 0.2383
Epoch 39/300, seasonal_1 Loss: 0.1620 | 0.2421
Epoch 40/300, seasonal_1 Loss: 0.1649 | 0.2610
Epoch 41/300, seasonal_1 Loss: 0.1608 | 0.2500
Epoch 42/300, seasonal_1 Loss: 0.1599 | 0.2179
Epoch 43/300, seasonal_1 Loss: 0.1592 | 0.2240
Epoch 44/300, seasonal_1 Loss: 0.1590 | 0.2494
Epoch 45/300, seasonal_1 Loss: 0.1552 | 0.2382
Epoch 46/300, seasonal_1 Loss: 0.1541 | 0.2143
Epoch 47/300, seasonal_1 Loss: 0.1511 | 0.2213
Epoch 48/300, seasonal_1 Loss: 0.1495 | 0.2070
Epoch 49/300, seasonal_1 Loss: 0.1483 | 0.2266
Epoch 50/300, seasonal_1 Loss: 0.1482 | 0.2085
Epoch 51/300, seasonal_1 Loss: 0.1500 | 0.2058
Epoch 52/300, seasonal_1 Loss: 0.1453 | 0.1950
Epoch 53/300, seasonal_1 Loss: 0.1529 | 0.2065
Epoch 54/300, seasonal_1 Loss: 0.1421 | 0.1919
Epoch 55/300, seasonal_1 Loss: 0.1473 | 0.1882
Epoch 56/300, seasonal_1 Loss: 0.1457 | 0.1757
Epoch 57/300, seasonal_1 Loss: 0.1431 | 0.1984
Epoch 58/300, seasonal_1 Loss: 0.1361 | 0.1835
Epoch 59/300, seasonal_1 Loss: 0.1368 | 0.1716
Epoch 60/300, seasonal_1 Loss: 0.1343 | 0.1734
Epoch 61/300, seasonal_1 Loss: 0.1315 | 0.1894
Epoch 62/300, seasonal_1 Loss: 0.1290 | 0.1760
Epoch 63/300, seasonal_1 Loss: 0.1280 | 0.1608
Epoch 64/300, seasonal_1 Loss: 0.1274 | 0.1626
Epoch 65/300, seasonal_1 Loss: 0.1265 | 0.1839
Epoch 66/300, seasonal_1 Loss: 0.1249 | 0.1770
Epoch 67/300, seasonal_1 Loss: 0.1248 | 0.1540
Epoch 68/300, seasonal_1 Loss: 0.1245 | 0.1518
Epoch 69/300, seasonal_1 Loss: 0.1275 | 0.1784
Epoch 70/300, seasonal_1 Loss: 0.1238 | 0.1921
Epoch 71/300, seasonal_1 Loss: 0.1247 | 0.1510
Epoch 72/300, seasonal_1 Loss: 0.1234 | 0.1487
Epoch 73/300, seasonal_1 Loss: 0.1226 | 0.1647
Epoch 74/300, seasonal_1 Loss: 0.1216 | 0.1735
Epoch 75/300, seasonal_1 Loss: 0.1195 | 0.1485
Epoch 76/300, seasonal_1 Loss: 0.1186 | 0.1432
Epoch 77/300, seasonal_1 Loss: 0.1177 | 0.1564
Epoch 78/300, seasonal_1 Loss: 0.1161 | 0.1607
Epoch 79/300, seasonal_1 Loss: 0.1164 | 0.1444
Epoch 80/300, seasonal_1 Loss: 0.1156 | 0.1426
Epoch 81/300, seasonal_1 Loss: 0.1154 | 0.1504
Epoch 82/300, seasonal_1 Loss: 0.1162 | 0.1632
Epoch 83/300, seasonal_1 Loss: 0.1144 | 0.1438
Epoch 84/300, seasonal_1 Loss: 0.1137 | 0.1378
Epoch 85/300, seasonal_1 Loss: 0.1138 | 0.1513
Epoch 86/300, seasonal_1 Loss: 0.1134 | 0.1533
Epoch 87/300, seasonal_1 Loss: 0.1116 | 0.1401
Epoch 88/300, seasonal_1 Loss: 0.1108 | 0.1360
Epoch 89/300, seasonal_1 Loss: 0.1110 | 0.1420
Epoch 90/300, seasonal_1 Loss: 0.1107 | 0.1424
Epoch 91/300, seasonal_1 Loss: 0.1092 | 0.1368
Epoch 92/300, seasonal_1 Loss: 0.1089 | 0.1365
Epoch 93/300, seasonal_1 Loss: 0.1093 | 0.1400
Epoch 94/300, seasonal_1 Loss: 0.1082 | 0.1399
Epoch 95/300, seasonal_1 Loss: 0.1078 | 0.1388
Epoch 96/300, seasonal_1 Loss: 0.1070 | 0.1355
Epoch 97/300, seasonal_1 Loss: 0.1077 | 0.1394
Epoch 98/300, seasonal_1 Loss: 0.1070 | 0.1395
Epoch 99/300, seasonal_1 Loss: 0.1071 | 0.1374
Epoch 100/300, seasonal_1 Loss: 0.1061 | 0.1326
Epoch 101/300, seasonal_1 Loss: 0.1062 | 0.1352
Epoch 102/300, seasonal_1 Loss: 0.1062 | 0.1350
Epoch 103/300, seasonal_1 Loss: 0.1061 | 0.1349
Epoch 104/300, seasonal_1 Loss: 0.1052 | 0.1359
Epoch 105/300, seasonal_1 Loss: 0.1047 | 0.1321
Epoch 106/300, seasonal_1 Loss: 0.1053 | 0.1327
Epoch 107/300, seasonal_1 Loss: 0.1039 | 0.1339
Epoch 108/300, seasonal_1 Loss: 0.1045 | 0.1319
Epoch 109/300, seasonal_1 Loss: 0.1039 | 0.1311
Epoch 110/300, seasonal_1 Loss: 0.1041 | 0.1303
Epoch 111/300, seasonal_1 Loss: 0.1026 | 0.1339
Epoch 112/300, seasonal_1 Loss: 0.1028 | 0.1317
Epoch 113/300, seasonal_1 Loss: 0.1029 | 0.1365
Epoch 114/300, seasonal_1 Loss: 0.1019 | 0.1327
Epoch 115/300, seasonal_1 Loss: 0.1022 | 0.1310
Epoch 116/300, seasonal_1 Loss: 0.1020 | 0.1305
Epoch 117/300, seasonal_1 Loss: 0.1013 | 0.1312
Epoch 118/300, seasonal_1 Loss: 0.1012 | 0.1313
Epoch 119/300, seasonal_1 Loss: 0.1014 | 0.1303
Epoch 120/300, seasonal_1 Loss: 0.1011 | 0.1296
Epoch 121/300, seasonal_1 Loss: 0.1017 | 0.1307
Epoch 122/300, seasonal_1 Loss: 0.1000 | 0.1297
Epoch 123/300, seasonal_1 Loss: 0.1002 | 0.1291
Epoch 124/300, seasonal_1 Loss: 0.1002 | 0.1311
Epoch 125/300, seasonal_1 Loss: 0.1006 | 0.1293
Epoch 126/300, seasonal_1 Loss: 0.1003 | 0.1290
Epoch 127/300, seasonal_1 Loss: 0.1001 | 0.1295
Epoch 128/300, seasonal_1 Loss: 0.1004 | 0.1273
Epoch 129/300, seasonal_1 Loss: 0.0996 | 0.1276
Epoch 130/300, seasonal_1 Loss: 0.0992 | 0.1289
Epoch 131/300, seasonal_1 Loss: 0.0994 | 0.1282
Epoch 132/300, seasonal_1 Loss: 0.0998 | 0.1274
Epoch 133/300, seasonal_1 Loss: 0.0988 | 0.1267
Epoch 134/300, seasonal_1 Loss: 0.0991 | 0.1277
Epoch 135/300, seasonal_1 Loss: 0.0989 | 0.1275
Epoch 136/300, seasonal_1 Loss: 0.0986 | 0.1282
Epoch 137/300, seasonal_1 Loss: 0.0984 | 0.1265
Epoch 138/300, seasonal_1 Loss: 0.0982 | 0.1277
Epoch 139/300, seasonal_1 Loss: 0.0990 | 0.1274
Epoch 140/300, seasonal_1 Loss: 0.0981 | 0.1266
Epoch 141/300, seasonal_1 Loss: 0.0980 | 0.1271
Epoch 142/300, seasonal_1 Loss: 0.0976 | 0.1264
Epoch 143/300, seasonal_1 Loss: 0.0984 | 0.1268
Epoch 144/300, seasonal_1 Loss: 0.0967 | 0.1270
Epoch 145/300, seasonal_1 Loss: 0.0972 | 0.1268
Epoch 146/300, seasonal_1 Loss: 0.0971 | 0.1249
Epoch 147/300, seasonal_1 Loss: 0.0978 | 0.1266
Epoch 148/300, seasonal_1 Loss: 0.0971 | 0.1267
Epoch 149/300, seasonal_1 Loss: 0.0975 | 0.1260
Epoch 150/300, seasonal_1 Loss: 0.0967 | 0.1252
Epoch 151/300, seasonal_1 Loss: 0.0972 | 0.1256
Epoch 152/300, seasonal_1 Loss: 0.0972 | 0.1263
Epoch 153/300, seasonal_1 Loss: 0.0964 | 0.1266
Epoch 154/300, seasonal_1 Loss: 0.0972 | 0.1258
Epoch 155/300, seasonal_1 Loss: 0.0974 | 0.1245
Epoch 156/300, seasonal_1 Loss: 0.0970 | 0.1246
Epoch 157/300, seasonal_1 Loss: 0.0969 | 0.1253
Epoch 158/300, seasonal_1 Loss: 0.0967 | 0.1254
Epoch 159/300, seasonal_1 Loss: 0.0961 | 0.1251
Epoch 160/300, seasonal_1 Loss: 0.0968 | 0.1249
Epoch 161/300, seasonal_1 Loss: 0.0962 | 0.1248
Epoch 162/300, seasonal_1 Loss: 0.0961 | 0.1250
Epoch 163/300, seasonal_1 Loss: 0.0958 | 0.1237
Epoch 164/300, seasonal_1 Loss: 0.0964 | 0.1254
Epoch 165/300, seasonal_1 Loss: 0.0961 | 0.1241
Epoch 166/300, seasonal_1 Loss: 0.0963 | 0.1247
Epoch 167/300, seasonal_1 Loss: 0.0958 | 0.1255
Epoch 168/300, seasonal_1 Loss: 0.0962 | 0.1248
Epoch 169/300, seasonal_1 Loss: 0.0960 | 0.1245
Epoch 170/300, seasonal_1 Loss: 0.0957 | 0.1236
Epoch 171/300, seasonal_1 Loss: 0.0953 | 0.1232
Epoch 172/300, seasonal_1 Loss: 0.0958 | 0.1242
Epoch 173/300, seasonal_1 Loss: 0.0954 | 0.1245
Epoch 174/300, seasonal_1 Loss: 0.0958 | 0.1254
Epoch 175/300, seasonal_1 Loss: 0.0964 | 0.1246
Epoch 176/300, seasonal_1 Loss: 0.0957 | 0.1250
Epoch 177/300, seasonal_1 Loss: 0.0955 | 0.1250
Epoch 178/300, seasonal_1 Loss: 0.0960 | 0.1248
Epoch 179/300, seasonal_1 Loss: 0.0950 | 0.1247
Epoch 180/300, seasonal_1 Loss: 0.0956 | 0.1244
Epoch 181/300, seasonal_1 Loss: 0.0954 | 0.1245
Epoch 182/300, seasonal_1 Loss: 0.0959 | 0.1241
Epoch 183/300, seasonal_1 Loss: 0.0946 | 0.1246
Epoch 184/300, seasonal_1 Loss: 0.0949 | 0.1245
Epoch 185/300, seasonal_1 Loss: 0.0948 | 0.1243
Epoch 186/300, seasonal_1 Loss: 0.0949 | 0.1245
Epoch 187/300, seasonal_1 Loss: 0.0953 | 0.1239
Epoch 188/300, seasonal_1 Loss: 0.0950 | 0.1240
Epoch 189/300, seasonal_1 Loss: 0.0943 | 0.1238
Epoch 190/300, seasonal_1 Loss: 0.0952 | 0.1240
Epoch 191/300, seasonal_1 Loss: 0.0947 | 0.1241
Epoch 192/300, seasonal_1 Loss: 0.0950 | 0.1240
Epoch 193/300, seasonal_1 Loss: 0.0952 | 0.1236
Epoch 194/300, seasonal_1 Loss: 0.0960 | 0.1235
Epoch 195/300, seasonal_1 Loss: 0.0948 | 0.1233
Epoch 196/300, seasonal_1 Loss: 0.0944 | 0.1235
Epoch 197/300, seasonal_1 Loss: 0.0939 | 0.1234
Epoch 198/300, seasonal_1 Loss: 0.0950 | 0.1242
Epoch 199/300, seasonal_1 Loss: 0.0944 | 0.1242
Epoch 200/300, seasonal_1 Loss: 0.0944 | 0.1240
Epoch 201/300, seasonal_1 Loss: 0.0944 | 0.1236
Epoch 202/300, seasonal_1 Loss: 0.0949 | 0.1236
Epoch 203/300, seasonal_1 Loss: 0.0950 | 0.1233
Epoch 204/300, seasonal_1 Loss: 0.0946 | 0.1234
Epoch 205/300, seasonal_1 Loss: 0.0949 | 0.1236
Epoch 206/300, seasonal_1 Loss: 0.0941 | 0.1233
Epoch 207/300, seasonal_1 Loss: 0.0937 | 0.1233
Epoch 208/300, seasonal_1 Loss: 0.0939 | 0.1238
Epoch 209/300, seasonal_1 Loss: 0.0941 | 0.1241
Epoch 210/300, seasonal_1 Loss: 0.0951 | 0.1242
Epoch 211/300, seasonal_1 Loss: 0.0944 | 0.1244
Epoch 212/300, seasonal_1 Loss: 0.0929 | 0.1242
Epoch 213/300, seasonal_1 Loss: 0.0948 | 0.1240
Epoch 214/300, seasonal_1 Loss: 0.0940 | 0.1235
Epoch 215/300, seasonal_1 Loss: 0.0944 | 0.1233
Epoch 216/300, seasonal_1 Loss: 0.0943 | 0.1234
Epoch 217/300, seasonal_1 Loss: 0.0944 | 0.1233
Epoch 218/300, seasonal_1 Loss: 0.0948 | 0.1233
Epoch 219/300, seasonal_1 Loss: 0.0939 | 0.1232
Epoch 220/300, seasonal_1 Loss: 0.0941 | 0.1232
Epoch 221/300, seasonal_1 Loss: 0.0947 | 0.1236
Epoch 222/300, seasonal_1 Loss: 0.0947 | 0.1235
Epoch 223/300, seasonal_1 Loss: 0.0930 | 0.1234
Epoch 224/300, seasonal_1 Loss: 0.0948 | 0.1236
Epoch 225/300, seasonal_1 Loss: 0.0944 | 0.1238
Epoch 226/300, seasonal_1 Loss: 0.0943 | 0.1237
Epoch 227/300, seasonal_1 Loss: 0.0943 | 0.1235
Epoch 228/300, seasonal_1 Loss: 0.0951 | 0.1232
Epoch 229/300, seasonal_1 Loss: 0.0941 | 0.1230
Epoch 230/300, seasonal_1 Loss: 0.0942 | 0.1230
Epoch 231/300, seasonal_1 Loss: 0.0934 | 0.1231
Epoch 232/300, seasonal_1 Loss: 0.0946 | 0.1232
Epoch 233/300, seasonal_1 Loss: 0.0940 | 0.1232
Epoch 234/300, seasonal_1 Loss: 0.0939 | 0.1232
Epoch 235/300, seasonal_1 Loss: 0.0943 | 0.1232
Epoch 236/300, seasonal_1 Loss: 0.0943 | 0.1232
Epoch 237/300, seasonal_1 Loss: 0.0939 | 0.1232
Epoch 238/300, seasonal_1 Loss: 0.0944 | 0.1231
Epoch 239/300, seasonal_1 Loss: 0.0940 | 0.1232
Epoch 240/300, seasonal_1 Loss: 0.0942 | 0.1231
Epoch 241/300, seasonal_1 Loss: 0.0941 | 0.1230
Epoch 242/300, seasonal_1 Loss: 0.0946 | 0.1232
Epoch 243/300, seasonal_1 Loss: 0.0944 | 0.1234
Epoch 244/300, seasonal_1 Loss: 0.0937 | 0.1235
Epoch 245/300, seasonal_1 Loss: 0.0942 | 0.1235
Epoch 246/300, seasonal_1 Loss: 0.0941 | 0.1235
Epoch 247/300, seasonal_1 Loss: 0.0943 | 0.1238
Epoch 248/300, seasonal_1 Loss: 0.0948 | 0.1238
Epoch 249/300, seasonal_1 Loss: 0.0943 | 0.1238
Epoch 250/300, seasonal_1 Loss: 0.0939 | 0.1236
Epoch 251/300, seasonal_1 Loss: 0.0941 | 0.1235
Epoch 252/300, seasonal_1 Loss: 0.0946 | 0.1234
Epoch 253/300, seasonal_1 Loss: 0.0945 | 0.1234
Epoch 254/300, seasonal_1 Loss: 0.0944 | 0.1235
Epoch 255/300, seasonal_1 Loss: 0.0934 | 0.1234
Epoch 256/300, seasonal_1 Loss: 0.0943 | 0.1234
Epoch 257/300, seasonal_1 Loss: 0.0936 | 0.1235
Epoch 258/300, seasonal_1 Loss: 0.0934 | 0.1234
Epoch 259/300, seasonal_1 Loss: 0.0935 | 0.1234
Epoch 260/300, seasonal_1 Loss: 0.0950 | 0.1235
Epoch 261/300, seasonal_1 Loss: 0.0935 | 0.1235
Epoch 262/300, seasonal_1 Loss: 0.0931 | 0.1236
Epoch 263/300, seasonal_1 Loss: 0.0943 | 0.1236
Epoch 264/300, seasonal_1 Loss: 0.0930 | 0.1235
Epoch 265/300, seasonal_1 Loss: 0.0935 | 0.1234
Epoch 266/300, seasonal_1 Loss: 0.0938 | 0.1234
Epoch 267/300, seasonal_1 Loss: 0.0934 | 0.1234
Epoch 268/300, seasonal_1 Loss: 0.0947 | 0.1234
Epoch 269/300, seasonal_1 Loss: 0.0940 | 0.1233
Epoch 270/300, seasonal_1 Loss: 0.0937 | 0.1232
Epoch 271/300, seasonal_1 Loss: 0.0940 | 0.1231
Epoch 272/300, seasonal_1 Loss: 0.0938 | 0.1231
Epoch 273/300, seasonal_1 Loss: 0.0944 | 0.1231
Epoch 274/300, seasonal_1 Loss: 0.0943 | 0.1230
Epoch 275/300, seasonal_1 Loss: 0.0933 | 0.1230
Epoch 276/300, seasonal_1 Loss: 0.0935 | 0.1230
Epoch 277/300, seasonal_1 Loss: 0.0939 | 0.1230
Epoch 278/300, seasonal_1 Loss: 0.0937 | 0.1229
Epoch 279/300, seasonal_1 Loss: 0.0937 | 0.1230
Epoch 280/300, seasonal_1 Loss: 0.0936 | 0.1230
Epoch 281/300, seasonal_1 Loss: 0.0939 | 0.1229
Epoch 282/300, seasonal_1 Loss: 0.0937 | 0.1229
Epoch 283/300, seasonal_1 Loss: 0.0944 | 0.1229
Epoch 284/300, seasonal_1 Loss: 0.0936 | 0.1229
Epoch 285/300, seasonal_1 Loss: 0.0935 | 0.1229
Epoch 286/300, seasonal_1 Loss: 0.0932 | 0.1229
Epoch 287/300, seasonal_1 Loss: 0.0928 | 0.1229
Epoch 288/300, seasonal_1 Loss: 0.0937 | 0.1229
Epoch 289/300, seasonal_1 Loss: 0.0935 | 0.1230
Epoch 290/300, seasonal_1 Loss: 0.0940 | 0.1230
Epoch 291/300, seasonal_1 Loss: 0.0936 | 0.1231
Epoch 292/300, seasonal_1 Loss: 0.0942 | 0.1231
Epoch 293/300, seasonal_1 Loss: 0.0936 | 0.1231
Epoch 294/300, seasonal_1 Loss: 0.0939 | 0.1231
Epoch 295/300, seasonal_1 Loss: 0.0938 | 0.1231
Epoch 296/300, seasonal_1 Loss: 0.0932 | 0.1231
Epoch 297/300, seasonal_1 Loss: 0.0940 | 0.1231
Epoch 298/300, seasonal_1 Loss: 0.0930 | 0.1231
Epoch 299/300, seasonal_1 Loss: 0.0936 | 0.1231
Epoch 300/300, seasonal_1 Loss: 0.0943 | 0.1231
Training seasonal_2 component with params: {'observation_period_num': 57, 'train_rates': 0.9810066371025419, 'learning_rate': 9.617839851595133e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9088965030470848}
Epoch 1/300, seasonal_2 Loss: 0.6560 | 0.8976
Epoch 2/300, seasonal_2 Loss: 0.5020 | 0.6844
Epoch 3/300, seasonal_2 Loss: 0.4065 | 0.6053
Epoch 4/300, seasonal_2 Loss: 0.3489 | 0.5917
Epoch 5/300, seasonal_2 Loss: 0.3279 | 0.6294
Epoch 6/300, seasonal_2 Loss: 0.3112 | 0.4009
Epoch 7/300, seasonal_2 Loss: 0.2688 | 0.3648
Epoch 8/300, seasonal_2 Loss: 0.2546 | 0.3505
Epoch 9/300, seasonal_2 Loss: 0.2661 | 0.3821
Epoch 10/300, seasonal_2 Loss: 0.2647 | 0.4124
Epoch 11/300, seasonal_2 Loss: 0.2613 | 0.4720
Epoch 12/300, seasonal_2 Loss: 0.2627 | 0.3702
Epoch 13/300, seasonal_2 Loss: 0.2574 | 0.4585
Epoch 14/300, seasonal_2 Loss: 0.2848 | 0.4682
Epoch 15/300, seasonal_2 Loss: 0.3240 | 0.3501
Epoch 16/300, seasonal_2 Loss: 0.2947 | 0.6039
Epoch 17/300, seasonal_2 Loss: 0.2742 | 0.3543
Epoch 18/300, seasonal_2 Loss: 0.2494 | 0.2911
Epoch 19/300, seasonal_2 Loss: 0.2418 | 0.2886
Epoch 20/300, seasonal_2 Loss: 0.2187 | 0.3647
Epoch 21/300, seasonal_2 Loss: 0.2068 | 0.3263
Epoch 22/300, seasonal_2 Loss: 0.2056 | 0.2591
Epoch 23/300, seasonal_2 Loss: 0.1988 | 0.2349
Epoch 24/300, seasonal_2 Loss: 0.2043 | 0.3378
Epoch 25/300, seasonal_2 Loss: 0.2036 | 0.2667
Epoch 26/300, seasonal_2 Loss: 0.1970 | 0.2419
Epoch 27/300, seasonal_2 Loss: 0.1981 | 0.2551
Epoch 28/300, seasonal_2 Loss: 0.1959 | 0.2372
Epoch 29/300, seasonal_2 Loss: 0.1974 | 0.2560
Epoch 30/300, seasonal_2 Loss: 0.1914 | 0.2823
Epoch 31/300, seasonal_2 Loss: 0.1920 | 0.2448
Epoch 32/300, seasonal_2 Loss: 0.1949 | 0.2507
Epoch 33/300, seasonal_2 Loss: 0.1970 | 0.2219
Epoch 34/300, seasonal_2 Loss: 0.1920 | 0.2155
Epoch 35/300, seasonal_2 Loss: 0.1870 | 0.2625
Epoch 36/300, seasonal_2 Loss: 0.1906 | 0.2778
Epoch 37/300, seasonal_2 Loss: 0.1834 | 0.2253
Epoch 38/300, seasonal_2 Loss: 0.1751 | 0.2095
Epoch 39/300, seasonal_2 Loss: 0.1765 | 0.2751
Epoch 40/300, seasonal_2 Loss: 0.1663 | 0.2589
Epoch 41/300, seasonal_2 Loss: 0.1632 | 0.2314
Epoch 42/300, seasonal_2 Loss: 0.1571 | 0.2005
Epoch 43/300, seasonal_2 Loss: 0.1589 | 0.2642
Epoch 44/300, seasonal_2 Loss: 0.1604 | 0.2788
Epoch 45/300, seasonal_2 Loss: 0.1576 | 0.2169
Epoch 46/300, seasonal_2 Loss: 0.1588 | 0.2252
Epoch 47/300, seasonal_2 Loss: 0.1581 | 0.1972
Epoch 48/300, seasonal_2 Loss: 0.1584 | 0.3074
Epoch 49/300, seasonal_2 Loss: 0.1538 | 0.2271
Epoch 50/300, seasonal_2 Loss: 0.1449 | 0.1962
Epoch 51/300, seasonal_2 Loss: 0.1445 | 0.1666
Epoch 52/300, seasonal_2 Loss: 0.1396 | 0.1897
Epoch 53/300, seasonal_2 Loss: 0.1433 | 0.2236
Epoch 54/300, seasonal_2 Loss: 0.1455 | 0.2029
Epoch 55/300, seasonal_2 Loss: 0.1418 | 0.1889
Epoch 56/300, seasonal_2 Loss: 0.1458 | 0.1756
Epoch 57/300, seasonal_2 Loss: 0.1412 | 0.2030
Epoch 58/300, seasonal_2 Loss: 0.1465 | 0.2204
Epoch 59/300, seasonal_2 Loss: 0.1380 | 0.2050
Epoch 60/300, seasonal_2 Loss: 0.1430 | 0.1757
Epoch 61/300, seasonal_2 Loss: 0.1440 | 0.1649
Epoch 62/300, seasonal_2 Loss: 0.1482 | 0.2591
Epoch 63/300, seasonal_2 Loss: 0.1367 | 0.2083
Epoch 64/300, seasonal_2 Loss: 0.1272 | 0.1720
Epoch 65/300, seasonal_2 Loss: 0.1242 | 0.1561
Epoch 66/300, seasonal_2 Loss: 0.1235 | 0.1823
Epoch 67/300, seasonal_2 Loss: 0.1221 | 0.1868
Epoch 68/300, seasonal_2 Loss: 0.1202 | 0.1577
Epoch 69/300, seasonal_2 Loss: 0.1178 | 0.1451
Epoch 70/300, seasonal_2 Loss: 0.1166 | 0.1527
Epoch 71/300, seasonal_2 Loss: 0.1185 | 0.1845
Epoch 72/300, seasonal_2 Loss: 0.1205 | 0.1557
Epoch 73/300, seasonal_2 Loss: 0.1226 | 0.1665
Epoch 74/300, seasonal_2 Loss: 0.1259 | 0.1785
Epoch 75/300, seasonal_2 Loss: 0.1251 | 0.1920
Epoch 76/300, seasonal_2 Loss: 0.1296 | 0.1626
Epoch 77/300, seasonal_2 Loss: 0.1317 | 0.1828
Epoch 78/300, seasonal_2 Loss: 0.1233 | 0.1956
Epoch 79/300, seasonal_2 Loss: 0.1188 | 0.1950
Epoch 80/300, seasonal_2 Loss: 0.1157 | 0.1673
Epoch 81/300, seasonal_2 Loss: 0.1117 | 0.1535
Epoch 82/300, seasonal_2 Loss: 0.1111 | 0.1493
Epoch 83/300, seasonal_2 Loss: 0.1097 | 0.1952
Epoch 84/300, seasonal_2 Loss: 0.1123 | 0.1797
Epoch 85/300, seasonal_2 Loss: 0.1088 | 0.1459
Epoch 86/300, seasonal_2 Loss: 0.1100 | 0.1467
Epoch 87/300, seasonal_2 Loss: 0.1088 | 0.1984
Epoch 88/300, seasonal_2 Loss: 0.1082 | 0.1680
Epoch 89/300, seasonal_2 Loss: 0.1059 | 0.1452
Epoch 90/300, seasonal_2 Loss: 0.1069 | 0.1440
Epoch 91/300, seasonal_2 Loss: 0.1046 | 0.1636
Epoch 92/300, seasonal_2 Loss: 0.1053 | 0.1727
Epoch 93/300, seasonal_2 Loss: 0.1024 | 0.1478
Epoch 94/300, seasonal_2 Loss: 0.1015 | 0.1556
Epoch 95/300, seasonal_2 Loss: 0.0999 | 0.1376
Epoch 96/300, seasonal_2 Loss: 0.0991 | 0.1304
Epoch 97/300, seasonal_2 Loss: 0.0996 | 0.1562
Epoch 98/300, seasonal_2 Loss: 0.0995 | 0.1481
Epoch 99/300, seasonal_2 Loss: 0.0995 | 0.1302
Epoch 100/300, seasonal_2 Loss: 0.1004 | 0.1443
Epoch 101/300, seasonal_2 Loss: 0.1000 | 0.1394
Epoch 102/300, seasonal_2 Loss: 0.0975 | 0.1403
Epoch 103/300, seasonal_2 Loss: 0.0968 | 0.1346
Epoch 104/300, seasonal_2 Loss: 0.0958 | 0.1574
Epoch 105/300, seasonal_2 Loss: 0.0952 | 0.1346
Epoch 106/300, seasonal_2 Loss: 0.0955 | 0.1257
Epoch 107/300, seasonal_2 Loss: 0.0941 | 0.1449
Epoch 108/300, seasonal_2 Loss: 0.0944 | 0.1548
Epoch 109/300, seasonal_2 Loss: 0.0927 | 0.1449
Epoch 110/300, seasonal_2 Loss: 0.0918 | 0.1309
Epoch 111/300, seasonal_2 Loss: 0.0914 | 0.1361
Epoch 112/300, seasonal_2 Loss: 0.0909 | 0.1349
Epoch 113/300, seasonal_2 Loss: 0.0895 | 0.1284
Epoch 114/300, seasonal_2 Loss: 0.0900 | 0.1452
Epoch 115/300, seasonal_2 Loss: 0.0901 | 0.1524
Epoch 116/300, seasonal_2 Loss: 0.0890 | 0.1294
Epoch 117/300, seasonal_2 Loss: 0.0894 | 0.1223
Epoch 118/300, seasonal_2 Loss: 0.0882 | 0.1487
Epoch 119/300, seasonal_2 Loss: 0.0887 | 0.1541
Epoch 120/300, seasonal_2 Loss: 0.0883 | 0.1232
Epoch 121/300, seasonal_2 Loss: 0.0886 | 0.1224
Epoch 122/300, seasonal_2 Loss: 0.0871 | 0.1401
Epoch 123/300, seasonal_2 Loss: 0.0871 | 0.1462
Epoch 124/300, seasonal_2 Loss: 0.0860 | 0.1300
Epoch 125/300, seasonal_2 Loss: 0.0854 | 0.1287
Epoch 126/300, seasonal_2 Loss: 0.0851 | 0.1245
Epoch 127/300, seasonal_2 Loss: 0.0840 | 0.1233
Epoch 128/300, seasonal_2 Loss: 0.0835 | 0.1309
Epoch 129/300, seasonal_2 Loss: 0.0844 | 0.1431
Epoch 130/300, seasonal_2 Loss: 0.0838 | 0.1328
Epoch 131/300, seasonal_2 Loss: 0.0831 | 0.1155
Epoch 132/300, seasonal_2 Loss: 0.0835 | 0.1271
Epoch 133/300, seasonal_2 Loss: 0.0843 | 0.1442
Epoch 134/300, seasonal_2 Loss: 0.0827 | 0.1318
Epoch 135/300, seasonal_2 Loss: 0.0820 | 0.1257
Epoch 136/300, seasonal_2 Loss: 0.0819 | 0.1262
Epoch 137/300, seasonal_2 Loss: 0.0805 | 0.1259
Epoch 138/300, seasonal_2 Loss: 0.0805 | 0.1259
Epoch 139/300, seasonal_2 Loss: 0.0801 | 0.1297
Epoch 140/300, seasonal_2 Loss: 0.0794 | 0.1332
Epoch 141/300, seasonal_2 Loss: 0.0796 | 0.1194
Epoch 142/300, seasonal_2 Loss: 0.0783 | 0.1203
Epoch 143/300, seasonal_2 Loss: 0.0783 | 0.1342
Epoch 144/300, seasonal_2 Loss: 0.0780 | 0.1356
Epoch 145/300, seasonal_2 Loss: 0.0781 | 0.1223
Epoch 146/300, seasonal_2 Loss: 0.0777 | 0.1193
Epoch 147/300, seasonal_2 Loss: 0.0771 | 0.1235
Epoch 148/300, seasonal_2 Loss: 0.0764 | 0.1393
Epoch 149/300, seasonal_2 Loss: 0.0769 | 0.1315
Epoch 150/300, seasonal_2 Loss: 0.0762 | 0.1191
Epoch 151/300, seasonal_2 Loss: 0.0765 | 0.1195
Epoch 152/300, seasonal_2 Loss: 0.0755 | 0.1219
Epoch 153/300, seasonal_2 Loss: 0.0759 | 0.1299
Epoch 154/300, seasonal_2 Loss: 0.0754 | 0.1273
Epoch 155/300, seasonal_2 Loss: 0.0750 | 0.1265
Epoch 156/300, seasonal_2 Loss: 0.0746 | 0.1202
Epoch 157/300, seasonal_2 Loss: 0.0747 | 0.1180
Epoch 158/300, seasonal_2 Loss: 0.0748 | 0.1320
Epoch 159/300, seasonal_2 Loss: 0.0750 | 0.1276
Epoch 160/300, seasonal_2 Loss: 0.0744 | 0.1212
Epoch 161/300, seasonal_2 Loss: 0.0735 | 0.1225
Epoch 162/300, seasonal_2 Loss: 0.0733 | 0.1218
Epoch 163/300, seasonal_2 Loss: 0.0733 | 0.1231
Epoch 164/300, seasonal_2 Loss: 0.0735 | 0.1198
Epoch 165/300, seasonal_2 Loss: 0.0727 | 0.1268
Epoch 166/300, seasonal_2 Loss: 0.0732 | 0.1246
Epoch 167/300, seasonal_2 Loss: 0.0734 | 0.1209
Epoch 168/300, seasonal_2 Loss: 0.0721 | 0.1154
Epoch 169/300, seasonal_2 Loss: 0.0719 | 0.1234
Epoch 170/300, seasonal_2 Loss: 0.0721 | 0.1255
Epoch 171/300, seasonal_2 Loss: 0.0719 | 0.1304
Epoch 172/300, seasonal_2 Loss: 0.0715 | 0.1225
Epoch 173/300, seasonal_2 Loss: 0.0724 | 0.1193
Epoch 174/300, seasonal_2 Loss: 0.0707 | 0.1283
Epoch 175/300, seasonal_2 Loss: 0.0716 | 0.1274
Epoch 176/300, seasonal_2 Loss: 0.0715 | 0.1189
Epoch 177/300, seasonal_2 Loss: 0.0712 | 0.1220
Epoch 178/300, seasonal_2 Loss: 0.0711 | 0.1235
Epoch 179/300, seasonal_2 Loss: 0.0714 | 0.1200
Epoch 180/300, seasonal_2 Loss: 0.0703 | 0.1284
Epoch 181/300, seasonal_2 Loss: 0.0701 | 0.1291
Epoch 182/300, seasonal_2 Loss: 0.0701 | 0.1180
Epoch 183/300, seasonal_2 Loss: 0.0701 | 0.1206
Epoch 184/300, seasonal_2 Loss: 0.0706 | 0.1224
Epoch 185/300, seasonal_2 Loss: 0.0699 | 0.1301
Epoch 186/300, seasonal_2 Loss: 0.0701 | 0.1242
Epoch 187/300, seasonal_2 Loss: 0.0698 | 0.1254
Epoch 188/300, seasonal_2 Loss: 0.0693 | 0.1256
Epoch 189/300, seasonal_2 Loss: 0.0693 | 0.1264
Epoch 190/300, seasonal_2 Loss: 0.0691 | 0.1187
Epoch 191/300, seasonal_2 Loss: 0.0687 | 0.1247
Epoch 192/300, seasonal_2 Loss: 0.0690 | 0.1325
Epoch 193/300, seasonal_2 Loss: 0.0693 | 0.1264
Epoch 194/300, seasonal_2 Loss: 0.0690 | 0.1189
Epoch 195/300, seasonal_2 Loss: 0.0686 | 0.1229
Epoch 196/300, seasonal_2 Loss: 0.0692 | 0.1239
Epoch 197/300, seasonal_2 Loss: 0.0683 | 0.1229
Epoch 198/300, seasonal_2 Loss: 0.0680 | 0.1220
Epoch 199/300, seasonal_2 Loss: 0.0687 | 0.1248
Epoch 200/300, seasonal_2 Loss: 0.0680 | 0.1244
Epoch 201/300, seasonal_2 Loss: 0.0679 | 0.1179
Epoch 202/300, seasonal_2 Loss: 0.0677 | 0.1243
Epoch 203/300, seasonal_2 Loss: 0.0677 | 0.1226
Epoch 204/300, seasonal_2 Loss: 0.0685 | 0.1240
Epoch 205/300, seasonal_2 Loss: 0.0679 | 0.1232
Epoch 206/300, seasonal_2 Loss: 0.0673 | 0.1247
Epoch 207/300, seasonal_2 Loss: 0.0682 | 0.1250
Epoch 208/300, seasonal_2 Loss: 0.0676 | 0.1235
Epoch 209/300, seasonal_2 Loss: 0.0667 | 0.1226
Epoch 210/300, seasonal_2 Loss: 0.0672 | 0.1252
Epoch 211/300, seasonal_2 Loss: 0.0674 | 0.1231
Epoch 212/300, seasonal_2 Loss: 0.0669 | 0.1205
Epoch 213/300, seasonal_2 Loss: 0.0671 | 0.1226
Epoch 214/300, seasonal_2 Loss: 0.0669 | 0.1226
Epoch 215/300, seasonal_2 Loss: 0.0666 | 0.1226
Epoch 216/300, seasonal_2 Loss: 0.0674 | 0.1228
Epoch 217/300, seasonal_2 Loss: 0.0672 | 0.1271
Epoch 218/300, seasonal_2 Loss: 0.0670 | 0.1249
Epoch 219/300, seasonal_2 Loss: 0.0667 | 0.1259
Epoch 220/300, seasonal_2 Loss: 0.0666 | 0.1269
Epoch 221/300, seasonal_2 Loss: 0.0670 | 0.1238
Epoch 222/300, seasonal_2 Loss: 0.0665 | 0.1249
Epoch 223/300, seasonal_2 Loss: 0.0659 | 0.1262
Epoch 224/300, seasonal_2 Loss: 0.0663 | 0.1266
Epoch 225/300, seasonal_2 Loss: 0.0655 | 0.1232
Epoch 226/300, seasonal_2 Loss: 0.0655 | 0.1244
Epoch 227/300, seasonal_2 Loss: 0.0665 | 0.1243
Epoch 228/300, seasonal_2 Loss: 0.0650 | 0.1236
Epoch 229/300, seasonal_2 Loss: 0.0655 | 0.1246
Epoch 230/300, seasonal_2 Loss: 0.0655 | 0.1231
Epoch 231/300, seasonal_2 Loss: 0.0657 | 0.1225
Epoch 232/300, seasonal_2 Loss: 0.0659 | 0.1244
Epoch 233/300, seasonal_2 Loss: 0.0652 | 0.1277
Epoch 234/300, seasonal_2 Loss: 0.0658 | 0.1253
Epoch 235/300, seasonal_2 Loss: 0.0660 | 0.1222
Epoch 236/300, seasonal_2 Loss: 0.0657 | 0.1238
Epoch 237/300, seasonal_2 Loss: 0.0653 | 0.1218
Epoch 238/300, seasonal_2 Loss: 0.0655 | 0.1214
Epoch 239/300, seasonal_2 Loss: 0.0649 | 0.1238
Epoch 240/300, seasonal_2 Loss: 0.0654 | 0.1233
Epoch 241/300, seasonal_2 Loss: 0.0656 | 0.1251
Epoch 242/300, seasonal_2 Loss: 0.0654 | 0.1233
Epoch 243/300, seasonal_2 Loss: 0.0651 | 0.1238
Epoch 244/300, seasonal_2 Loss: 0.0646 | 0.1255
Epoch 245/300, seasonal_2 Loss: 0.0651 | 0.1255
Epoch 246/300, seasonal_2 Loss: 0.0645 | 0.1230
Epoch 247/300, seasonal_2 Loss: 0.0650 | 0.1261
Epoch 248/300, seasonal_2 Loss: 0.0645 | 0.1252
Epoch 249/300, seasonal_2 Loss: 0.0651 | 0.1265
Epoch 250/300, seasonal_2 Loss: 0.0644 | 0.1281
Epoch 251/300, seasonal_2 Loss: 0.0647 | 0.1266
Epoch 252/300, seasonal_2 Loss: 0.0643 | 0.1251
Epoch 253/300, seasonal_2 Loss: 0.0645 | 0.1251
Epoch 254/300, seasonal_2 Loss: 0.0649 | 0.1246
Epoch 255/300, seasonal_2 Loss: 0.0649 | 0.1257
Epoch 256/300, seasonal_2 Loss: 0.0647 | 0.1224
Epoch 257/300, seasonal_2 Loss: 0.0639 | 0.1248
Epoch 258/300, seasonal_2 Loss: 0.0648 | 0.1253
Epoch 259/300, seasonal_2 Loss: 0.0646 | 0.1242
Epoch 260/300, seasonal_2 Loss: 0.0645 | 0.1213
Epoch 261/300, seasonal_2 Loss: 0.0644 | 0.1223
Epoch 262/300, seasonal_2 Loss: 0.0649 | 0.1241
Epoch 263/300, seasonal_2 Loss: 0.0641 | 0.1262
Epoch 264/300, seasonal_2 Loss: 0.0642 | 0.1255
Epoch 265/300, seasonal_2 Loss: 0.0641 | 0.1240
Epoch 266/300, seasonal_2 Loss: 0.0638 | 0.1236
Epoch 267/300, seasonal_2 Loss: 0.0643 | 0.1239
Epoch 268/300, seasonal_2 Loss: 0.0636 | 0.1230
Epoch 269/300, seasonal_2 Loss: 0.0640 | 0.1225
Epoch 270/300, seasonal_2 Loss: 0.0638 | 0.1238
Epoch 271/300, seasonal_2 Loss: 0.0632 | 0.1244
Epoch 272/300, seasonal_2 Loss: 0.0639 | 0.1243
Epoch 273/300, seasonal_2 Loss: 0.0638 | 0.1261
Epoch 274/300, seasonal_2 Loss: 0.0636 | 0.1256
Epoch 275/300, seasonal_2 Loss: 0.0636 | 0.1231
Epoch 276/300, seasonal_2 Loss: 0.0641 | 0.1265
Epoch 277/300, seasonal_2 Loss: 0.0637 | 0.1258
Epoch 278/300, seasonal_2 Loss: 0.0632 | 0.1243
Epoch 279/300, seasonal_2 Loss: 0.0636 | 0.1227
Epoch 280/300, seasonal_2 Loss: 0.0635 | 0.1236
Epoch 281/300, seasonal_2 Loss: 0.0641 | 0.1224
Epoch 282/300, seasonal_2 Loss: 0.0631 | 0.1232
Epoch 283/300, seasonal_2 Loss: 0.0639 | 0.1243
Epoch 284/300, seasonal_2 Loss: 0.0632 | 0.1234
Epoch 285/300, seasonal_2 Loss: 0.0636 | 0.1249
Epoch 286/300, seasonal_2 Loss: 0.0630 | 0.1248
Epoch 287/300, seasonal_2 Loss: 0.0626 | 0.1245
Epoch 288/300, seasonal_2 Loss: 0.0631 | 0.1259
Epoch 289/300, seasonal_2 Loss: 0.0630 | 0.1249
Epoch 290/300, seasonal_2 Loss: 0.0635 | 0.1249
Epoch 291/300, seasonal_2 Loss: 0.0634 | 0.1259
Epoch 292/300, seasonal_2 Loss: 0.0636 | 0.1256
Epoch 293/300, seasonal_2 Loss: 0.0635 | 0.1253
Epoch 294/300, seasonal_2 Loss: 0.0628 | 0.1258
Epoch 295/300, seasonal_2 Loss: 0.0628 | 0.1241
Epoch 296/300, seasonal_2 Loss: 0.0633 | 0.1229
Epoch 297/300, seasonal_2 Loss: 0.0632 | 0.1223
Epoch 298/300, seasonal_2 Loss: 0.0632 | 0.1232
Epoch 299/300, seasonal_2 Loss: 0.0634 | 0.1228
Epoch 300/300, seasonal_2 Loss: 0.0632 | 0.1224
Training seasonal_3 component with params: {'observation_period_num': 106, 'train_rates': 0.9893391739618191, 'learning_rate': 0.00022799324653885987, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9660586637902551}
Epoch 1/300, seasonal_3 Loss: 0.7827 | 0.9905
Epoch 2/300, seasonal_3 Loss: 0.7378 | 0.8722
Epoch 3/300, seasonal_3 Loss: 0.6174 | 0.7588
Epoch 4/300, seasonal_3 Loss: 0.5272 | 0.6302
Epoch 5/300, seasonal_3 Loss: 0.6678 | 0.7090
Epoch 6/300, seasonal_3 Loss: 0.6771 | 0.7461
Epoch 7/300, seasonal_3 Loss: 0.5363 | 0.6622
Epoch 8/300, seasonal_3 Loss: 0.4448 | 0.5772
Epoch 9/300, seasonal_3 Loss: 0.3997 | 0.5562
Epoch 10/300, seasonal_3 Loss: 0.3794 | 0.5419
Epoch 11/300, seasonal_3 Loss: 0.3571 | 0.4797
Epoch 12/300, seasonal_3 Loss: 0.4420 | 0.4750
Epoch 13/300, seasonal_3 Loss: 0.4434 | 0.5160
Epoch 14/300, seasonal_3 Loss: 0.4058 | 0.4728
Epoch 15/300, seasonal_3 Loss: 0.3982 | 0.4638
Epoch 16/300, seasonal_3 Loss: 0.3822 | 0.5006
Epoch 17/300, seasonal_3 Loss: 0.3426 | 0.5244
Epoch 18/300, seasonal_3 Loss: 0.3510 | 0.3776
Epoch 19/300, seasonal_3 Loss: 0.3535 | 0.4331
Epoch 20/300, seasonal_3 Loss: 0.3024 | 0.4051
Epoch 21/300, seasonal_3 Loss: 0.2912 | 0.3860
Epoch 22/300, seasonal_3 Loss: 0.2895 | 0.4188
Epoch 23/300, seasonal_3 Loss: 0.3092 | 0.3618
Epoch 24/300, seasonal_3 Loss: 0.2779 | 0.3545
Epoch 25/300, seasonal_3 Loss: 0.2939 | 0.3888
Epoch 26/300, seasonal_3 Loss: 0.2461 | 0.3349
Epoch 27/300, seasonal_3 Loss: 0.2650 | 0.3824
Epoch 28/300, seasonal_3 Loss: 0.2490 | 0.3378
Epoch 29/300, seasonal_3 Loss: 0.2367 | 0.2977
Epoch 30/300, seasonal_3 Loss: 0.2427 | 0.3851
Epoch 31/300, seasonal_3 Loss: 0.2358 | 0.3081
Epoch 32/300, seasonal_3 Loss: 0.2401 | 0.3164
Epoch 33/300, seasonal_3 Loss: 0.2367 | 0.3102
Epoch 34/300, seasonal_3 Loss: 0.2687 | 0.3404
Epoch 35/300, seasonal_3 Loss: 0.2694 | 0.2870
Epoch 36/300, seasonal_3 Loss: 0.2508 | 0.2906
Epoch 37/300, seasonal_3 Loss: 0.2340 | 0.3485
Epoch 38/300, seasonal_3 Loss: 0.2209 | 0.3155
Epoch 39/300, seasonal_3 Loss: 0.2358 | 0.3024
Epoch 40/300, seasonal_3 Loss: 0.2495 | 0.3883
Epoch 41/300, seasonal_3 Loss: 0.2079 | 0.3564
Epoch 42/300, seasonal_3 Loss: 0.2030 | 0.3196
Epoch 43/300, seasonal_3 Loss: 0.2214 | 0.3245
Epoch 44/300, seasonal_3 Loss: 0.1962 | 0.3103
Epoch 45/300, seasonal_3 Loss: 0.1801 | 0.2859
Epoch 46/300, seasonal_3 Loss: 0.1791 | 0.2663
Epoch 47/300, seasonal_3 Loss: 0.1782 | 0.3314
Epoch 48/300, seasonal_3 Loss: 0.1764 | 0.2547
Epoch 49/300, seasonal_3 Loss: 0.1821 | 0.2518
Epoch 50/300, seasonal_3 Loss: 0.2016 | 0.3209
Epoch 51/300, seasonal_3 Loss: 0.1810 | 0.2825
Epoch 52/300, seasonal_3 Loss: 0.1920 | 0.2829
Epoch 53/300, seasonal_3 Loss: 0.2157 | 0.3222
Epoch 54/300, seasonal_3 Loss: 0.1928 | 0.2843
Epoch 55/300, seasonal_3 Loss: 0.2087 | 0.2917
Epoch 56/300, seasonal_3 Loss: 0.2426 | 0.2863
Epoch 57/300, seasonal_3 Loss: 0.2251 | 0.2852
Epoch 58/300, seasonal_3 Loss: 0.2101 | 0.3426
Epoch 59/300, seasonal_3 Loss: 0.1965 | 0.2986
Epoch 60/300, seasonal_3 Loss: 0.2093 | 0.2583
Epoch 61/300, seasonal_3 Loss: 0.1893 | 0.3198
Epoch 62/300, seasonal_3 Loss: 0.1857 | 0.2815
Epoch 63/300, seasonal_3 Loss: 0.1774 | 0.2746
Epoch 64/300, seasonal_3 Loss: 0.1873 | 0.3025
Epoch 65/300, seasonal_3 Loss: 0.1713 | 0.3177
Epoch 66/300, seasonal_3 Loss: 0.1588 | 0.2722
Epoch 67/300, seasonal_3 Loss: 0.1554 | 0.3049
Epoch 68/300, seasonal_3 Loss: 0.1530 | 0.2408
Epoch 69/300, seasonal_3 Loss: 0.1515 | 0.2763
Epoch 70/300, seasonal_3 Loss: 0.1545 | 0.2526
Epoch 71/300, seasonal_3 Loss: 0.1504 | 0.2486
Epoch 72/300, seasonal_3 Loss: 0.1513 | 0.2853
Epoch 73/300, seasonal_3 Loss: 0.1441 | 0.2517
Epoch 74/300, seasonal_3 Loss: 0.1405 | 0.2424
Epoch 75/300, seasonal_3 Loss: 0.1398 | 0.2404
Epoch 76/300, seasonal_3 Loss: 0.1371 | 0.2258
Epoch 77/300, seasonal_3 Loss: 0.1340 | 0.2231
Epoch 78/300, seasonal_3 Loss: 0.1348 | 0.2528
Epoch 79/300, seasonal_3 Loss: 0.1356 | 0.1949
Epoch 80/300, seasonal_3 Loss: 0.1371 | 0.2428
Epoch 81/300, seasonal_3 Loss: 0.1405 | 0.2486
Epoch 82/300, seasonal_3 Loss: 0.1403 | 0.2028
Epoch 83/300, seasonal_3 Loss: 0.1422 | 0.2521
Epoch 84/300, seasonal_3 Loss: 0.1618 | 0.2310
Epoch 85/300, seasonal_3 Loss: 0.1497 | 0.2377
Epoch 86/300, seasonal_3 Loss: 0.1438 | 0.2309
Epoch 87/300, seasonal_3 Loss: 0.1534 | 0.2863
Epoch 88/300, seasonal_3 Loss: 0.1462 | 0.2359
Epoch 89/300, seasonal_3 Loss: 0.1449 | 0.1987
Epoch 90/300, seasonal_3 Loss: 0.1629 | 0.2495
Epoch 91/300, seasonal_3 Loss: 0.1506 | 0.2064
Epoch 92/300, seasonal_3 Loss: 0.1392 | 0.2175
Epoch 93/300, seasonal_3 Loss: 0.1548 | 0.2065
Epoch 94/300, seasonal_3 Loss: 0.1424 | 0.2026
Epoch 95/300, seasonal_3 Loss: 0.1383 | 0.2805
Epoch 96/300, seasonal_3 Loss: 0.1332 | 0.2073
Epoch 97/300, seasonal_3 Loss: 0.1308 | 0.2054
Epoch 98/300, seasonal_3 Loss: 0.1262 | 0.1955
Epoch 99/300, seasonal_3 Loss: 0.1206 | 0.1931
Epoch 100/300, seasonal_3 Loss: 0.1203 | 0.1938
Epoch 101/300, seasonal_3 Loss: 0.1205 | 0.1807
Epoch 102/300, seasonal_3 Loss: 0.1175 | 0.1816
Epoch 103/300, seasonal_3 Loss: 0.1174 | 0.1801
Epoch 104/300, seasonal_3 Loss: 0.1163 | 0.1761
Epoch 105/300, seasonal_3 Loss: 0.1160 | 0.1758
Epoch 106/300, seasonal_3 Loss: 0.1155 | 0.1710
Epoch 107/300, seasonal_3 Loss: 0.1146 | 0.1791
Epoch 108/300, seasonal_3 Loss: 0.1148 | 0.1530
Epoch 109/300, seasonal_3 Loss: 0.1146 | 0.1794
Epoch 110/300, seasonal_3 Loss: 0.1136 | 0.1583
Epoch 111/300, seasonal_3 Loss: 0.1131 | 0.1516
Epoch 112/300, seasonal_3 Loss: 0.1107 | 0.1686
Epoch 113/300, seasonal_3 Loss: 0.1089 | 0.1557
Epoch 114/300, seasonal_3 Loss: 0.1093 | 0.1528
Epoch 115/300, seasonal_3 Loss: 0.1108 | 0.1472
Epoch 116/300, seasonal_3 Loss: 0.1084 | 0.1520
Epoch 117/300, seasonal_3 Loss: 0.1087 | 0.1564
Epoch 118/300, seasonal_3 Loss: 0.1104 | 0.1356
Epoch 119/300, seasonal_3 Loss: 0.1161 | 0.2125
Epoch 120/300, seasonal_3 Loss: 0.1225 | 0.1514
Epoch 121/300, seasonal_3 Loss: 0.1223 | 0.1597
Epoch 122/300, seasonal_3 Loss: 0.1183 | 0.1729
Epoch 123/300, seasonal_3 Loss: 0.1216 | 0.1538
Epoch 124/300, seasonal_3 Loss: 0.1204 | 0.1562
Epoch 125/300, seasonal_3 Loss: 0.1148 | 0.1665
Epoch 126/300, seasonal_3 Loss: 0.1102 | 0.1514
Epoch 127/300, seasonal_3 Loss: 0.1078 | 0.1460
Epoch 128/300, seasonal_3 Loss: 0.1056 | 0.1464
Epoch 129/300, seasonal_3 Loss: 0.1036 | 0.1369
Epoch 130/300, seasonal_3 Loss: 0.1049 | 0.1443
Epoch 131/300, seasonal_3 Loss: 0.1040 | 0.1393
Epoch 132/300, seasonal_3 Loss: 0.1012 | 0.1313
Epoch 133/300, seasonal_3 Loss: 0.0993 | 0.1381
Epoch 134/300, seasonal_3 Loss: 0.0976 | 0.1313
Epoch 135/300, seasonal_3 Loss: 0.0984 | 0.1267
Epoch 136/300, seasonal_3 Loss: 0.0984 | 0.1416
Epoch 137/300, seasonal_3 Loss: 0.0974 | 0.1274
Epoch 138/300, seasonal_3 Loss: 0.0969 | 0.1303
Epoch 139/300, seasonal_3 Loss: 0.0974 | 0.1268
Epoch 140/300, seasonal_3 Loss: 0.0965 | 0.1258
Epoch 141/300, seasonal_3 Loss: 0.0964 | 0.1270
Epoch 142/300, seasonal_3 Loss: 0.0949 | 0.1294
Epoch 143/300, seasonal_3 Loss: 0.0946 | 0.1205
Epoch 144/300, seasonal_3 Loss: 0.0952 | 0.1293
Epoch 145/300, seasonal_3 Loss: 0.0952 | 0.1276
Epoch 146/300, seasonal_3 Loss: 0.0936 | 0.1227
Epoch 147/300, seasonal_3 Loss: 0.0934 | 0.1319
Epoch 148/300, seasonal_3 Loss: 0.0938 | 0.1217
Epoch 149/300, seasonal_3 Loss: 0.0926 | 0.1185
Epoch 150/300, seasonal_3 Loss: 0.0925 | 0.1304
Epoch 151/300, seasonal_3 Loss: 0.0925 | 0.1207
Epoch 152/300, seasonal_3 Loss: 0.0924 | 0.1325
Epoch 153/300, seasonal_3 Loss: 0.0920 | 0.1214
Epoch 154/300, seasonal_3 Loss: 0.0918 | 0.1185
Epoch 155/300, seasonal_3 Loss: 0.0922 | 0.1332
Epoch 156/300, seasonal_3 Loss: 0.0920 | 0.1163
Epoch 157/300, seasonal_3 Loss: 0.0928 | 0.1228
Epoch 158/300, seasonal_3 Loss: 0.0926 | 0.1275
Epoch 159/300, seasonal_3 Loss: 0.0910 | 0.1182
Epoch 160/300, seasonal_3 Loss: 0.0888 | 0.1221
Epoch 161/300, seasonal_3 Loss: 0.0889 | 0.1239
Epoch 162/300, seasonal_3 Loss: 0.0889 | 0.1166
Epoch 163/300, seasonal_3 Loss: 0.0879 | 0.1103
Epoch 164/300, seasonal_3 Loss: 0.0882 | 0.1311
Epoch 165/300, seasonal_3 Loss: 0.0884 | 0.1154
Epoch 166/300, seasonal_3 Loss: 0.0877 | 0.1139
Epoch 167/300, seasonal_3 Loss: 0.0867 | 0.1226
Epoch 168/300, seasonal_3 Loss: 0.0887 | 0.1156
Epoch 169/300, seasonal_3 Loss: 0.0882 | 0.1107
Epoch 170/300, seasonal_3 Loss: 0.0874 | 0.1257
Epoch 171/300, seasonal_3 Loss: 0.0882 | 0.1090
Epoch 172/300, seasonal_3 Loss: 0.0866 | 0.1166
Epoch 173/300, seasonal_3 Loss: 0.0854 | 0.1184
Epoch 174/300, seasonal_3 Loss: 0.0857 | 0.1120
Epoch 175/300, seasonal_3 Loss: 0.0850 | 0.1198
Epoch 176/300, seasonal_3 Loss: 0.0848 | 0.1149
Epoch 177/300, seasonal_3 Loss: 0.0841 | 0.1121
Epoch 178/300, seasonal_3 Loss: 0.0829 | 0.1210
Epoch 179/300, seasonal_3 Loss: 0.0829 | 0.1093
Epoch 180/300, seasonal_3 Loss: 0.0826 | 0.1131
Epoch 181/300, seasonal_3 Loss: 0.0832 | 0.1189
Epoch 182/300, seasonal_3 Loss: 0.0830 | 0.1072
Epoch 183/300, seasonal_3 Loss: 0.0833 | 0.1183
Epoch 184/300, seasonal_3 Loss: 0.0828 | 0.1137
Epoch 185/300, seasonal_3 Loss: 0.0821 | 0.1095
Epoch 186/300, seasonal_3 Loss: 0.0822 | 0.1178
Epoch 187/300, seasonal_3 Loss: 0.0821 | 0.1140
Epoch 188/300, seasonal_3 Loss: 0.0821 | 0.1086
Epoch 189/300, seasonal_3 Loss: 0.0826 | 0.1184
Epoch 190/300, seasonal_3 Loss: 0.0816 | 0.1081
Epoch 191/300, seasonal_3 Loss: 0.0799 | 0.1140
Epoch 192/300, seasonal_3 Loss: 0.0801 | 0.1162
Epoch 193/300, seasonal_3 Loss: 0.0801 | 0.1078
Epoch 194/300, seasonal_3 Loss: 0.0795 | 0.1160
Epoch 195/300, seasonal_3 Loss: 0.0803 | 0.1048
Epoch 196/300, seasonal_3 Loss: 0.0790 | 0.1080
Epoch 197/300, seasonal_3 Loss: 0.0791 | 0.1134
Epoch 198/300, seasonal_3 Loss: 0.0798 | 0.1071
Epoch 199/300, seasonal_3 Loss: 0.0803 | 0.1101
Epoch 200/300, seasonal_3 Loss: 0.0795 | 0.1123
Epoch 201/300, seasonal_3 Loss: 0.0791 | 0.1047
Epoch 202/300, seasonal_3 Loss: 0.0797 | 0.1112
Epoch 203/300, seasonal_3 Loss: 0.0793 | 0.1087
Epoch 204/300, seasonal_3 Loss: 0.0792 | 0.1127
Epoch 205/300, seasonal_3 Loss: 0.0785 | 0.1128
Epoch 206/300, seasonal_3 Loss: 0.0791 | 0.1075
Epoch 207/300, seasonal_3 Loss: 0.0780 | 0.1084
Epoch 208/300, seasonal_3 Loss: 0.0777 | 0.1097
Epoch 209/300, seasonal_3 Loss: 0.0774 | 0.1072
Epoch 210/300, seasonal_3 Loss: 0.0778 | 0.1134
Epoch 211/300, seasonal_3 Loss: 0.0781 | 0.1071
Epoch 212/300, seasonal_3 Loss: 0.0771 | 0.1105
Epoch 213/300, seasonal_3 Loss: 0.0777 | 0.1091
Epoch 214/300, seasonal_3 Loss: 0.0773 | 0.1073
Epoch 215/300, seasonal_3 Loss: 0.0761 | 0.1071
Epoch 216/300, seasonal_3 Loss: 0.0756 | 0.1070
Epoch 217/300, seasonal_3 Loss: 0.0760 | 0.1062
Epoch 218/300, seasonal_3 Loss: 0.0753 | 0.1111
Epoch 219/300, seasonal_3 Loss: 0.0757 | 0.1063
Epoch 220/300, seasonal_3 Loss: 0.0747 | 0.1044
Epoch 221/300, seasonal_3 Loss: 0.0760 | 0.1082
Epoch 222/300, seasonal_3 Loss: 0.0749 | 0.1073
Epoch 223/300, seasonal_3 Loss: 0.0749 | 0.1062
Epoch 224/300, seasonal_3 Loss: 0.0752 | 0.1085
Epoch 225/300, seasonal_3 Loss: 0.0744 | 0.1075
Epoch 226/300, seasonal_3 Loss: 0.0748 | 0.1058
Epoch 227/300, seasonal_3 Loss: 0.0735 | 0.1061
Epoch 228/300, seasonal_3 Loss: 0.0737 | 0.1057
Epoch 229/300, seasonal_3 Loss: 0.0743 | 0.1070
Epoch 230/300, seasonal_3 Loss: 0.0735 | 0.1060
Epoch 231/300, seasonal_3 Loss: 0.0734 | 0.1059
Epoch 232/300, seasonal_3 Loss: 0.0740 | 0.1080
Epoch 233/300, seasonal_3 Loss: 0.0735 | 0.1074
Epoch 234/300, seasonal_3 Loss: 0.0740 | 0.1055
Epoch 235/300, seasonal_3 Loss: 0.0738 | 0.1063
Epoch 236/300, seasonal_3 Loss: 0.0730 | 0.1058
Epoch 237/300, seasonal_3 Loss: 0.0723 | 0.1057
Epoch 238/300, seasonal_3 Loss: 0.0729 | 0.1049
Epoch 239/300, seasonal_3 Loss: 0.0730 | 0.1068
Epoch 240/300, seasonal_3 Loss: 0.0723 | 0.1052
Epoch 241/300, seasonal_3 Loss: 0.0734 | 0.1055
Epoch 242/300, seasonal_3 Loss: 0.0724 | 0.1093
Epoch 243/300, seasonal_3 Loss: 0.0724 | 0.1080
Epoch 244/300, seasonal_3 Loss: 0.0727 | 0.1084
Epoch 245/300, seasonal_3 Loss: 0.0725 | 0.1060
Epoch 246/300, seasonal_3 Loss: 0.0726 | 0.1039
Epoch 247/300, seasonal_3 Loss: 0.0732 | 0.1091
Epoch 248/300, seasonal_3 Loss: 0.0717 | 0.1041
Epoch 249/300, seasonal_3 Loss: 0.0724 | 0.1056
Epoch 250/300, seasonal_3 Loss: 0.0719 | 0.1043
Epoch 251/300, seasonal_3 Loss: 0.0706 | 0.1049
Epoch 252/300, seasonal_3 Loss: 0.0724 | 0.1068
Epoch 253/300, seasonal_3 Loss: 0.0706 | 0.1062
Epoch 254/300, seasonal_3 Loss: 0.0716 | 0.1025
Epoch 255/300, seasonal_3 Loss: 0.0714 | 0.1036
Epoch 256/300, seasonal_3 Loss: 0.0713 | 0.1043
Epoch 257/300, seasonal_3 Loss: 0.0712 | 0.1045
Epoch 258/300, seasonal_3 Loss: 0.0708 | 0.1058
Epoch 259/300, seasonal_3 Loss: 0.0705 | 0.1033
Epoch 260/300, seasonal_3 Loss: 0.0713 | 0.1054
Epoch 261/300, seasonal_3 Loss: 0.0703 | 0.1036
Epoch 262/300, seasonal_3 Loss: 0.0699 | 0.1016
Epoch 263/300, seasonal_3 Loss: 0.0701 | 0.1036
Epoch 264/300, seasonal_3 Loss: 0.0706 | 0.1031
Epoch 265/300, seasonal_3 Loss: 0.0702 | 0.1065
Epoch 266/300, seasonal_3 Loss: 0.0701 | 0.1055
Epoch 267/300, seasonal_3 Loss: 0.0706 | 0.1064
Epoch 268/300, seasonal_3 Loss: 0.0703 | 0.1031
Epoch 269/300, seasonal_3 Loss: 0.0705 | 0.1040
Epoch 270/300, seasonal_3 Loss: 0.0699 | 0.1046
Epoch 271/300, seasonal_3 Loss: 0.0702 | 0.1049
Epoch 272/300, seasonal_3 Loss: 0.0699 | 0.1057
Epoch 273/300, seasonal_3 Loss: 0.0703 | 0.1059
Epoch 274/300, seasonal_3 Loss: 0.0693 | 0.1043
Epoch 275/300, seasonal_3 Loss: 0.0697 | 0.1060
Epoch 276/300, seasonal_3 Loss: 0.0699 | 0.1054
Epoch 277/300, seasonal_3 Loss: 0.0695 | 0.1045
Epoch 278/300, seasonal_3 Loss: 0.0698 | 0.1042
Epoch 279/300, seasonal_3 Loss: 0.0690 | 0.1045
Epoch 280/300, seasonal_3 Loss: 0.0693 | 0.1062
Epoch 281/300, seasonal_3 Loss: 0.0692 | 0.1057
Epoch 282/300, seasonal_3 Loss: 0.0692 | 0.1052
Epoch 283/300, seasonal_3 Loss: 0.0698 | 0.1055
Epoch 284/300, seasonal_3 Loss: 0.0693 | 0.1043
Epoch 285/300, seasonal_3 Loss: 0.0695 | 0.1055
Epoch 286/300, seasonal_3 Loss: 0.0693 | 0.1048
Epoch 287/300, seasonal_3 Loss: 0.0690 | 0.1059
Epoch 288/300, seasonal_3 Loss: 0.0694 | 0.1066
Epoch 289/300, seasonal_3 Loss: 0.0689 | 0.1053
Epoch 290/300, seasonal_3 Loss: 0.0691 | 0.1062
Epoch 291/300, seasonal_3 Loss: 0.0680 | 0.1062
Epoch 292/300, seasonal_3 Loss: 0.0699 | 0.1063
Epoch 293/300, seasonal_3 Loss: 0.0681 | 0.1056
Epoch 294/300, seasonal_3 Loss: 0.0678 | 0.1059
Epoch 295/300, seasonal_3 Loss: 0.0682 | 0.1053
Epoch 296/300, seasonal_3 Loss: 0.0680 | 0.1053
Epoch 297/300, seasonal_3 Loss: 0.0683 | 0.1051
Epoch 298/300, seasonal_3 Loss: 0.0688 | 0.1058
Epoch 299/300, seasonal_3 Loss: 0.0683 | 0.1065
Epoch 300/300, seasonal_3 Loss: 0.0682 | 0.1050
Training resid component with params: {'observation_period_num': 151, 'train_rates': 0.9745053919367286, 'learning_rate': 0.00013902055071722392, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8773915949274665}
Epoch 1/300, resid Loss: 1.0927 | 2.1648
Epoch 2/300, resid Loss: 0.7979 | 1.2841
Epoch 3/300, resid Loss: 0.5914 | 1.0464
Epoch 4/300, resid Loss: 0.5346 | 0.9456
Epoch 5/300, resid Loss: 0.4899 | 0.8492
Epoch 6/300, resid Loss: 0.5373 | 0.8989
Epoch 7/300, resid Loss: 0.5740 | 0.8321
Epoch 8/300, resid Loss: 0.4731 | 0.7775
Epoch 9/300, resid Loss: 0.3908 | 0.7249
Epoch 10/300, resid Loss: 0.3676 | 0.6994
Epoch 11/300, resid Loss: 0.3494 | 0.6488
Epoch 12/300, resid Loss: 0.3317 | 0.6353
Epoch 13/300, resid Loss: 0.3225 | 0.5930
Epoch 14/300, resid Loss: 0.3074 | 0.5808
Epoch 15/300, resid Loss: 0.2949 | 0.5454
Epoch 16/300, resid Loss: 0.3285 | 0.5354
Epoch 17/300, resid Loss: 0.3159 | 0.5326
Epoch 18/300, resid Loss: 0.3153 | 0.4869
Epoch 19/300, resid Loss: 0.3243 | 0.5561
Epoch 20/300, resid Loss: 0.2760 | 0.4946
Epoch 21/300, resid Loss: 0.2658 | 0.4954
Epoch 22/300, resid Loss: 0.2670 | 0.4697
Epoch 23/300, resid Loss: 0.2641 | 0.4722
Epoch 24/300, resid Loss: 0.2624 | 0.4423
Epoch 25/300, resid Loss: 0.2359 | 0.4395
Epoch 26/300, resid Loss: 0.2310 | 0.4202
Epoch 27/300, resid Loss: 0.2262 | 0.4117
Epoch 28/300, resid Loss: 0.2245 | 0.4182
Epoch 29/300, resid Loss: 0.2207 | 0.3954
Epoch 30/300, resid Loss: 0.2239 | 0.3873
Epoch 31/300, resid Loss: 0.2166 | 0.3735
Epoch 32/300, resid Loss: 0.2255 | 0.4022
Epoch 33/300, resid Loss: 0.2182 | 0.3711
Epoch 34/300, resid Loss: 0.2279 | 0.3813
Epoch 35/300, resid Loss: 0.2162 | 0.3714
Epoch 36/300, resid Loss: 0.2432 | 0.3687
Epoch 37/300, resid Loss: 0.2260 | 0.3549
Epoch 38/300, resid Loss: 0.2114 | 0.3609
Epoch 39/300, resid Loss: 0.2195 | 0.3514
Epoch 40/300, resid Loss: 0.2126 | 0.3413
Epoch 41/300, resid Loss: 0.2102 | 0.3573
Epoch 42/300, resid Loss: 0.2126 | 0.3431
Epoch 43/300, resid Loss: 0.2105 | 0.3318
Epoch 44/300, resid Loss: 0.2119 | 0.3471
Epoch 45/300, resid Loss: 0.2031 | 0.3299
Epoch 46/300, resid Loss: 0.2025 | 0.3433
Epoch 47/300, resid Loss: 0.2023 | 0.3160
Epoch 48/300, resid Loss: 0.1933 | 0.3285
Epoch 49/300, resid Loss: 0.1895 | 0.3127
Epoch 50/300, resid Loss: 0.1892 | 0.3176
Epoch 51/300, resid Loss: 0.1885 | 0.3012
Epoch 52/300, resid Loss: 0.1889 | 0.3119
Epoch 53/300, resid Loss: 0.1829 | 0.2970
Epoch 54/300, resid Loss: 0.1810 | 0.2997
Epoch 55/300, resid Loss: 0.1774 | 0.2961
Epoch 56/300, resid Loss: 0.1748 | 0.2931
Epoch 57/300, resid Loss: 0.1713 | 0.2901
Epoch 58/300, resid Loss: 0.1711 | 0.2869
Epoch 59/300, resid Loss: 0.1693 | 0.2808
Epoch 60/300, resid Loss: 0.1685 | 0.2829
Epoch 61/300, resid Loss: 0.1697 | 0.2777
Epoch 62/300, resid Loss: 0.1680 | 0.2729
Epoch 63/300, resid Loss: 0.1665 | 0.2740
Epoch 64/300, resid Loss: 0.1657 | 0.2695
Epoch 65/300, resid Loss: 0.1653 | 0.2662
Epoch 66/300, resid Loss: 0.1661 | 0.2659
Epoch 67/300, resid Loss: 0.1654 | 0.2639
Epoch 68/300, resid Loss: 0.1651 | 0.2581
Epoch 69/300, resid Loss: 0.1662 | 0.2642
Epoch 70/300, resid Loss: 0.1656 | 0.2541
Epoch 71/300, resid Loss: 0.1644 | 0.2557
Epoch 72/300, resid Loss: 0.1629 | 0.2568
Epoch 73/300, resid Loss: 0.1637 | 0.2541
Epoch 74/300, resid Loss: 0.1649 | 0.2474
Epoch 75/300, resid Loss: 0.1628 | 0.2602
Epoch 76/300, resid Loss: 0.1654 | 0.2461
Epoch 77/300, resid Loss: 0.1640 | 0.2488
Epoch 78/300, resid Loss: 0.1622 | 0.2584
Epoch 79/300, resid Loss: 0.1697 | 0.2469
Epoch 80/300, resid Loss: 0.1698 | 0.2464
Epoch 81/300, resid Loss: 0.1647 | 0.2613
Epoch 82/300, resid Loss: 0.1678 | 0.2452
Epoch 83/300, resid Loss: 0.1692 | 0.2428
Epoch 84/300, resid Loss: 0.1638 | 0.2487
Epoch 85/300, resid Loss: 0.1591 | 0.2420
Epoch 86/300, resid Loss: 0.1590 | 0.2387
Epoch 87/300, resid Loss: 0.1595 | 0.2396
Epoch 88/300, resid Loss: 0.1545 | 0.2368
Epoch 89/300, resid Loss: 0.1525 | 0.2372
Epoch 90/300, resid Loss: 0.1535 | 0.2354
Epoch 91/300, resid Loss: 0.1525 | 0.2336
Epoch 92/300, resid Loss: 0.1510 | 0.2326
Epoch 93/300, resid Loss: 0.1499 | 0.2313
Epoch 94/300, resid Loss: 0.1491 | 0.2293
Epoch 95/300, resid Loss: 0.1486 | 0.2281
Epoch 96/300, resid Loss: 0.1487 | 0.2264
Epoch 97/300, resid Loss: 0.1478 | 0.2264
Epoch 98/300, resid Loss: 0.1480 | 0.2249
Epoch 99/300, resid Loss: 0.1471 | 0.2244
Epoch 100/300, resid Loss: 0.1473 | 0.2250
Epoch 101/300, resid Loss: 0.1466 | 0.2235
Epoch 102/300, resid Loss: 0.1462 | 0.2230
Epoch 103/300, resid Loss: 0.1461 | 0.2224
Epoch 104/300, resid Loss: 0.1458 | 0.2203
Epoch 105/300, resid Loss: 0.1449 | 0.2186
Epoch 106/300, resid Loss: 0.1439 | 0.2176
Epoch 107/300, resid Loss: 0.1441 | 0.2171
Epoch 108/300, resid Loss: 0.1445 | 0.2138
Epoch 109/300, resid Loss: 0.1432 | 0.2163
Epoch 110/300, resid Loss: 0.1430 | 0.2134
Epoch 111/300, resid Loss: 0.1427 | 0.2134
Epoch 112/300, resid Loss: 0.1426 | 0.2131
Epoch 113/300, resid Loss: 0.1419 | 0.2126
Epoch 114/300, resid Loss: 0.1429 | 0.2113
Epoch 115/300, resid Loss: 0.1428 | 0.2114
Epoch 116/300, resid Loss: 0.1425 | 0.2097
Epoch 117/300, resid Loss: 0.1419 | 0.2084
Epoch 118/300, resid Loss: 0.1413 | 0.2088
Epoch 119/300, resid Loss: 0.1412 | 0.2094
Epoch 120/300, resid Loss: 0.1404 | 0.2076
Epoch 121/300, resid Loss: 0.1396 | 0.2070
Epoch 122/300, resid Loss: 0.1401 | 0.2047
Epoch 123/300, resid Loss: 0.1400 | 0.2052
Epoch 124/300, resid Loss: 0.1396 | 0.2041
Epoch 125/300, resid Loss: 0.1388 | 0.2043
Epoch 126/300, resid Loss: 0.1388 | 0.2043
Epoch 127/300, resid Loss: 0.1385 | 0.2029
Epoch 128/300, resid Loss: 0.1384 | 0.2027
Epoch 129/300, resid Loss: 0.1383 | 0.2026
Epoch 130/300, resid Loss: 0.1374 | 0.2021
Epoch 131/300, resid Loss: 0.1380 | 0.2017
Epoch 132/300, resid Loss: 0.1377 | 0.2000
Epoch 133/300, resid Loss: 0.1372 | 0.1992
Epoch 134/300, resid Loss: 0.1365 | 0.1999
Epoch 135/300, resid Loss: 0.1373 | 0.1983
Epoch 136/300, resid Loss: 0.1373 | 0.1983
Epoch 137/300, resid Loss: 0.1365 | 0.1983
Epoch 138/300, resid Loss: 0.1363 | 0.1966
Epoch 139/300, resid Loss: 0.1362 | 0.1969
Epoch 140/300, resid Loss: 0.1354 | 0.1961
Epoch 141/300, resid Loss: 0.1361 | 0.1966
Epoch 142/300, resid Loss: 0.1359 | 0.1964
Epoch 143/300, resid Loss: 0.1359 | 0.1954
Epoch 144/300, resid Loss: 0.1354 | 0.1949
Epoch 145/300, resid Loss: 0.1352 | 0.1952
Epoch 146/300, resid Loss: 0.1353 | 0.1936
Epoch 147/300, resid Loss: 0.1347 | 0.1941
Epoch 148/300, resid Loss: 0.1348 | 0.1926
Epoch 149/300, resid Loss: 0.1352 | 0.1926
Epoch 150/300, resid Loss: 0.1350 | 0.1942
Epoch 151/300, resid Loss: 0.1346 | 0.1936
Epoch 152/300, resid Loss: 0.1340 | 0.1927
Epoch 153/300, resid Loss: 0.1342 | 0.1919
Epoch 154/300, resid Loss: 0.1340 | 0.1924
Epoch 155/300, resid Loss: 0.1337 | 0.1922
Epoch 156/300, resid Loss: 0.1331 | 0.1910
Epoch 157/300, resid Loss: 0.1330 | 0.1908
Epoch 158/300, resid Loss: 0.1332 | 0.1903
Epoch 159/300, resid Loss: 0.1331 | 0.1891
Epoch 160/300, resid Loss: 0.1333 | 0.1905
Epoch 161/300, resid Loss: 0.1321 | 0.1897
Epoch 162/300, resid Loss: 0.1323 | 0.1888
Epoch 163/300, resid Loss: 0.1325 | 0.1885
Epoch 164/300, resid Loss: 0.1329 | 0.1873
Epoch 165/300, resid Loss: 0.1321 | 0.1870
Epoch 166/300, resid Loss: 0.1323 | 0.1883
Epoch 167/300, resid Loss: 0.1320 | 0.1873
Epoch 168/300, resid Loss: 0.1321 | 0.1868
Epoch 169/300, resid Loss: 0.1313 | 0.1876
Epoch 170/300, resid Loss: 0.1316 | 0.1877
Epoch 171/300, resid Loss: 0.1314 | 0.1871
Epoch 172/300, resid Loss: 0.1309 | 0.1882
Epoch 173/300, resid Loss: 0.1316 | 0.1878
Epoch 174/300, resid Loss: 0.1307 | 0.1876
Epoch 175/300, resid Loss: 0.1300 | 0.1856
Epoch 176/300, resid Loss: 0.1310 | 0.1864
Epoch 177/300, resid Loss: 0.1305 | 0.1862
Epoch 178/300, resid Loss: 0.1315 | 0.1857
Epoch 179/300, resid Loss: 0.1310 | 0.1853
Epoch 180/300, resid Loss: 0.1305 | 0.1856
Epoch 181/300, resid Loss: 0.1308 | 0.1850
Epoch 182/300, resid Loss: 0.1295 | 0.1855
Epoch 183/300, resid Loss: 0.1302 | 0.1850
Epoch 184/300, resid Loss: 0.1298 | 0.1844
Epoch 185/300, resid Loss: 0.1301 | 0.1853
Epoch 186/300, resid Loss: 0.1288 | 0.1843
Epoch 187/300, resid Loss: 0.1298 | 0.1846
Epoch 188/300, resid Loss: 0.1291 | 0.1833
Epoch 189/300, resid Loss: 0.1300 | 0.1831
Epoch 190/300, resid Loss: 0.1297 | 0.1828
Epoch 191/300, resid Loss: 0.1297 | 0.1820
Epoch 192/300, resid Loss: 0.1287 | 0.1823
Epoch 193/300, resid Loss: 0.1291 | 0.1825
Epoch 194/300, resid Loss: 0.1285 | 0.1830
Epoch 195/300, resid Loss: 0.1286 | 0.1835
Epoch 196/300, resid Loss: 0.1295 | 0.1827
Epoch 197/300, resid Loss: 0.1283 | 0.1825
Epoch 198/300, resid Loss: 0.1290 | 0.1828
Epoch 199/300, resid Loss: 0.1293 | 0.1826
Epoch 200/300, resid Loss: 0.1285 | 0.1823
Epoch 201/300, resid Loss: 0.1284 | 0.1820
Epoch 202/300, resid Loss: 0.1283 | 0.1818
Epoch 203/300, resid Loss: 0.1288 | 0.1812
Epoch 204/300, resid Loss: 0.1283 | 0.1811
Epoch 205/300, resid Loss: 0.1282 | 0.1818
Epoch 206/300, resid Loss: 0.1286 | 0.1811
Epoch 207/300, resid Loss: 0.1283 | 0.1812
Epoch 208/300, resid Loss: 0.1278 | 0.1816
Epoch 209/300, resid Loss: 0.1277 | 0.1808
Epoch 210/300, resid Loss: 0.1282 | 0.1812
Epoch 211/300, resid Loss: 0.1272 | 0.1804
Epoch 212/300, resid Loss: 0.1279 | 0.1801
Epoch 213/300, resid Loss: 0.1282 | 0.1807
Epoch 214/300, resid Loss: 0.1274 | 0.1809
Epoch 215/300, resid Loss: 0.1272 | 0.1799
Epoch 216/300, resid Loss: 0.1272 | 0.1803
Epoch 217/300, resid Loss: 0.1276 | 0.1801
Epoch 218/300, resid Loss: 0.1269 | 0.1799
Epoch 219/300, resid Loss: 0.1278 | 0.1796
Epoch 220/300, resid Loss: 0.1276 | 0.1794
Epoch 221/300, resid Loss: 0.1273 | 0.1797
Epoch 222/300, resid Loss: 0.1276 | 0.1791
Epoch 223/300, resid Loss: 0.1272 | 0.1789
Epoch 224/300, resid Loss: 0.1275 | 0.1792
Epoch 225/300, resid Loss: 0.1271 | 0.1791
Epoch 226/300, resid Loss: 0.1267 | 0.1785
Epoch 227/300, resid Loss: 0.1269 | 0.1790
Epoch 228/300, resid Loss: 0.1271 | 0.1794
Epoch 229/300, resid Loss: 0.1261 | 0.1794
Epoch 230/300, resid Loss: 0.1268 | 0.1788
Epoch 231/300, resid Loss: 0.1264 | 0.1786
Epoch 232/300, resid Loss: 0.1271 | 0.1787
Epoch 233/300, resid Loss: 0.1278 | 0.1783
Epoch 234/300, resid Loss: 0.1268 | 0.1778
Epoch 235/300, resid Loss: 0.1269 | 0.1781
Epoch 236/300, resid Loss: 0.1267 | 0.1784
Epoch 237/300, resid Loss: 0.1262 | 0.1778
Epoch 238/300, resid Loss: 0.1265 | 0.1774
Epoch 239/300, resid Loss: 0.1268 | 0.1775
Epoch 240/300, resid Loss: 0.1264 | 0.1776
Epoch 241/300, resid Loss: 0.1261 | 0.1773
Epoch 242/300, resid Loss: 0.1276 | 0.1769
Epoch 243/300, resid Loss: 0.1264 | 0.1769
Epoch 244/300, resid Loss: 0.1266 | 0.1770
Epoch 245/300, resid Loss: 0.1264 | 0.1769
Epoch 246/300, resid Loss: 0.1264 | 0.1776
Epoch 247/300, resid Loss: 0.1269 | 0.1774
Epoch 248/300, resid Loss: 0.1265 | 0.1781
Epoch 249/300, resid Loss: 0.1258 | 0.1775
Epoch 250/300, resid Loss: 0.1258 | 0.1772
Epoch 251/300, resid Loss: 0.1264 | 0.1770
Epoch 252/300, resid Loss: 0.1255 | 0.1767
Epoch 253/300, resid Loss: 0.1255 | 0.1767
Epoch 254/300, resid Loss: 0.1255 | 0.1763
Epoch 255/300, resid Loss: 0.1263 | 0.1766
Epoch 256/300, resid Loss: 0.1254 | 0.1769
Epoch 257/300, resid Loss: 0.1259 | 0.1768
Epoch 258/300, resid Loss: 0.1258 | 0.1768
Epoch 259/300, resid Loss: 0.1249 | 0.1767
Epoch 260/300, resid Loss: 0.1263 | 0.1767
Epoch 261/300, resid Loss: 0.1259 | 0.1770
Epoch 262/300, resid Loss: 0.1255 | 0.1769
Epoch 263/300, resid Loss: 0.1264 | 0.1759
Epoch 264/300, resid Loss: 0.1253 | 0.1759
Epoch 265/300, resid Loss: 0.1260 | 0.1764
Epoch 266/300, resid Loss: 0.1254 | 0.1758
Epoch 267/300, resid Loss: 0.1256 | 0.1760
Epoch 268/300, resid Loss: 0.1255 | 0.1764
Epoch 269/300, resid Loss: 0.1246 | 0.1764
Epoch 270/300, resid Loss: 0.1254 | 0.1763
Epoch 271/300, resid Loss: 0.1255 | 0.1761
Epoch 272/300, resid Loss: 0.1250 | 0.1761
Epoch 273/300, resid Loss: 0.1257 | 0.1760
Epoch 274/300, resid Loss: 0.1254 | 0.1762
Epoch 275/300, resid Loss: 0.1251 | 0.1763
Epoch 276/300, resid Loss: 0.1254 | 0.1760
Epoch 277/300, resid Loss: 0.1251 | 0.1760
Epoch 278/300, resid Loss: 0.1254 | 0.1758
Epoch 279/300, resid Loss: 0.1258 | 0.1757
Epoch 280/300, resid Loss: 0.1251 | 0.1756
Epoch 281/300, resid Loss: 0.1256 | 0.1759
Epoch 282/300, resid Loss: 0.1253 | 0.1760
Epoch 283/300, resid Loss: 0.1255 | 0.1760
Epoch 284/300, resid Loss: 0.1257 | 0.1759
Epoch 285/300, resid Loss: 0.1251 | 0.1757
Epoch 286/300, resid Loss: 0.1255 | 0.1758
Epoch 287/300, resid Loss: 0.1248 | 0.1755
Epoch 288/300, resid Loss: 0.1252 | 0.1754
Epoch 289/300, resid Loss: 0.1244 | 0.1754
Epoch 290/300, resid Loss: 0.1254 | 0.1753
Epoch 291/300, resid Loss: 0.1253 | 0.1757
Epoch 292/300, resid Loss: 0.1247 | 0.1757
Epoch 293/300, resid Loss: 0.1244 | 0.1757
Epoch 294/300, resid Loss: 0.1246 | 0.1755
Epoch 295/300, resid Loss: 0.1244 | 0.1755
Epoch 296/300, resid Loss: 0.1252 | 0.1754
Epoch 297/300, resid Loss: 0.1246 | 0.1752
Epoch 298/300, resid Loss: 0.1256 | 0.1752
Epoch 299/300, resid Loss: 0.1250 | 0.1754
Epoch 300/300, resid Loss: 0.1250 | 0.1755
Runtime (seconds): 3426.0423061847687
0.0002570231311590525
[140.4585]
[-1.501484]
[-5.08681]
[7.8255854]
[1.7189966]
[3.7606907]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 549.9467272686306
RMSE: 23.450942993164062
MAE: 23.450942993164062
R-squared: nan
[147.17549]
