ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 10:29:41,477][0m A new study created in memory with name: no-name-97aa462c-33dc-4239-b2be-00a5356d0269[0m
[32m[I 2025-01-07 10:31:20,150][0m Trial 0 finished with value: 0.21065539918570134 and parameters: {'observation_period_num': 127, 'train_rates': 0.8731738914760132, 'learning_rate': 1.2203020570486715e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7965145921179497}. Best is trial 0 with value: 0.21065539918570134.[0m
[32m[I 2025-01-07 10:34:29,015][0m Trial 1 finished with value: 0.1731093037329123 and parameters: {'observation_period_num': 151, 'train_rates': 0.7510323963808925, 'learning_rate': 0.0004594684060960292, 'batch_size': 25, 'step_size': 6, 'gamma': 0.7608484659902993}. Best is trial 1 with value: 0.1731093037329123.[0m
[32m[I 2025-01-07 10:35:04,779][0m Trial 2 finished with value: 0.11840992636894912 and parameters: {'observation_period_num': 204, 'train_rates': 0.6762663042690514, 'learning_rate': 0.0009596639249795673, 'batch_size': 172, 'step_size': 4, 'gamma': 0.8586036009274595}. Best is trial 2 with value: 0.11840992636894912.[0m
[32m[I 2025-01-07 10:35:45,183][0m Trial 3 finished with value: 0.33483956426489075 and parameters: {'observation_period_num': 100, 'train_rates': 0.6098343058996882, 'learning_rate': 2.0896251784078783e-05, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9330473230874261}. Best is trial 2 with value: 0.11840992636894912.[0m
[32m[I 2025-01-07 10:36:15,880][0m Trial 4 finished with value: 0.10270565200825127 and parameters: {'observation_period_num': 114, 'train_rates': 0.8276711679440762, 'learning_rate': 7.94457310701915e-05, 'batch_size': 200, 'step_size': 10, 'gamma': 0.8667577564657234}. Best is trial 4 with value: 0.10270565200825127.[0m
[32m[I 2025-01-07 10:36:44,583][0m Trial 5 finished with value: 0.6270922056327646 and parameters: {'observation_period_num': 86, 'train_rates': 0.7098731366828789, 'learning_rate': 3.3547752490916074e-06, 'batch_size': 226, 'step_size': 15, 'gamma': 0.8287558698382785}. Best is trial 4 with value: 0.10270565200825127.[0m
[32m[I 2025-01-07 10:40:44,436][0m Trial 6 finished with value: 0.14676332510347093 and parameters: {'observation_period_num': 66, 'train_rates': 0.7464416022937661, 'learning_rate': 6.159524350188775e-06, 'batch_size': 20, 'step_size': 9, 'gamma': 0.847882662051009}. Best is trial 4 with value: 0.10270565200825127.[0m
[32m[I 2025-01-07 10:41:25,270][0m Trial 7 finished with value: 1.3415006399154663 and parameters: {'observation_period_num': 119, 'train_rates': 0.9222665032856261, 'learning_rate': 2.1274875616845556e-06, 'batch_size': 221, 'step_size': 3, 'gamma': 0.8180594678054944}. Best is trial 4 with value: 0.10270565200825127.[0m
[32m[I 2025-01-07 10:42:08,204][0m Trial 8 finished with value: 0.3553019993159236 and parameters: {'observation_period_num': 8, 'train_rates': 0.8836726591851982, 'learning_rate': 3.272444559239928e-06, 'batch_size': 252, 'step_size': 14, 'gamma': 0.8122285660056859}. Best is trial 4 with value: 0.10270565200825127.[0m
[32m[I 2025-01-07 10:44:17,498][0m Trial 9 finished with value: 0.08908455229323843 and parameters: {'observation_period_num': 32, 'train_rates': 0.6064805871399506, 'learning_rate': 0.0005813062665778604, 'batch_size': 35, 'step_size': 2, 'gamma': 0.9115503380916821}. Best is trial 9 with value: 0.08908455229323843.[0m
[32m[I 2025-01-07 10:45:20,003][0m Trial 10 finished with value: 0.03807782009243965 and parameters: {'observation_period_num': 10, 'train_rates': 0.9821297881802269, 'learning_rate': 0.00012966974255862929, 'batch_size': 101, 'step_size': 1, 'gamma': 0.979392964114567}. Best is trial 10 with value: 0.03807782009243965.[0m
[32m[I 2025-01-07 10:46:26,907][0m Trial 11 finished with value: 0.03742563825041529 and parameters: {'observation_period_num': 8, 'train_rates': 0.9574954958633919, 'learning_rate': 0.00013139306662639398, 'batch_size': 92, 'step_size': 1, 'gamma': 0.9870521761390583}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:47:34,484][0m Trial 12 finished with value: 0.05552932247519493 and parameters: {'observation_period_num': 43, 'train_rates': 0.9880911055292078, 'learning_rate': 9.957364541284405e-05, 'batch_size': 98, 'step_size': 1, 'gamma': 0.9823462586691853}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:48:58,657][0m Trial 13 finished with value: 0.17048396170139313 and parameters: {'observation_period_num': 249, 'train_rates': 0.9851917343793161, 'learning_rate': 0.0001373738306999148, 'batch_size': 77, 'step_size': 6, 'gamma': 0.9889954563559206}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:49:55,537][0m Trial 14 finished with value: 0.039290094592918955 and parameters: {'observation_period_num': 8, 'train_rates': 0.9349408815014131, 'learning_rate': 0.00022316956654127627, 'batch_size': 136, 'step_size': 5, 'gamma': 0.945574436903035}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:50:38,838][0m Trial 15 finished with value: 0.37715954628101617 and parameters: {'observation_period_num': 51, 'train_rates': 0.9408694849484456, 'learning_rate': 3.260926649027873e-05, 'batch_size': 166, 'step_size': 1, 'gamma': 0.9015421711746566}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:51:37,458][0m Trial 16 finished with value: 0.12746985416327203 and parameters: {'observation_period_num': 173, 'train_rates': 0.8288306502197493, 'learning_rate': 5.116342774884654e-05, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9546346069635233}. Best is trial 11 with value: 0.03742563825041529.[0m
[32m[I 2025-01-07 10:52:46,279][0m Trial 17 finished with value: 0.027172359657852658 and parameters: {'observation_period_num': 5, 'train_rates': 0.8892589918714747, 'learning_rate': 0.00020995956511775144, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8981119351193323}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 10:54:10,715][0m Trial 18 finished with value: 0.04827728967296432 and parameters: {'observation_period_num': 75, 'train_rates': 0.8782640943720585, 'learning_rate': 0.0002608950097612913, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8988397328074782}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 10:54:51,849][0m Trial 19 finished with value: 0.040762523361300054 and parameters: {'observation_period_num': 33, 'train_rates': 0.8334064359026722, 'learning_rate': 0.00031818713081163403, 'batch_size': 149, 'step_size': 3, 'gamma': 0.8874133824275865}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 10:56:44,142][0m Trial 20 finished with value: 0.5901029761020954 and parameters: {'observation_period_num': 159, 'train_rates': 0.9072677444944945, 'learning_rate': 1.0263147408405262e-06, 'batch_size': 48, 'step_size': 12, 'gamma': 0.937254616110645}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 10:57:50,094][0m Trial 21 finished with value: 0.054831288289278746 and parameters: {'observation_period_num': 20, 'train_rates': 0.9620406162573476, 'learning_rate': 0.00014354669603995556, 'batch_size': 91, 'step_size': 1, 'gamma': 0.9641757809367372}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 10:58:44,164][0m Trial 22 finished with value: 0.048224468385944 and parameters: {'observation_period_num': 6, 'train_rates': 0.9562212408093091, 'learning_rate': 6.17487794050407e-05, 'batch_size': 113, 'step_size': 3, 'gamma': 0.9706483339106498}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:00:01,461][0m Trial 23 finished with value: 0.04982942865207428 and parameters: {'observation_period_num': 61, 'train_rates': 0.915054466948498, 'learning_rate': 0.00016596225180591273, 'batch_size': 75, 'step_size': 2, 'gamma': 0.9293021879235643}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:00:51,326][0m Trial 24 finished with value: 0.057597219944000244 and parameters: {'observation_period_num': 32, 'train_rates': 0.9670072039285192, 'learning_rate': 5.1037840044102584e-05, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9893886122989318}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:02:28,540][0m Trial 25 finished with value: 0.0954651175238562 and parameters: {'observation_period_num': 45, 'train_rates': 0.8548578850430213, 'learning_rate': 2.889921696556539e-05, 'batch_size': 56, 'step_size': 1, 'gamma': 0.95818130707165}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:03:26,996][0m Trial 26 finished with value: 0.05549434219964412 and parameters: {'observation_period_num': 86, 'train_rates': 0.7926738532141775, 'learning_rate': 0.0003978965760256544, 'batch_size': 89, 'step_size': 4, 'gamma': 0.9211376427739194}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:04:19,795][0m Trial 27 finished with value: 0.04375646665993363 and parameters: {'observation_period_num': 25, 'train_rates': 0.8988279495338877, 'learning_rate': 0.0008160676522055279, 'batch_size': 114, 'step_size': 8, 'gamma': 0.9724103970441998}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:05:01,485][0m Trial 28 finished with value: 0.13538973033428192 and parameters: {'observation_period_num': 53, 'train_rates': 0.9861466986837736, 'learning_rate': 9.521790427708081e-05, 'batch_size': 154, 'step_size': 2, 'gamma': 0.8799478963011235}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:06:48,904][0m Trial 29 finished with value: 0.059049849814557014 and parameters: {'observation_period_num': 5, 'train_rates': 0.8567768516634618, 'learning_rate': 1.80068037262353e-05, 'batch_size': 51, 'step_size': 5, 'gamma': 0.775055193414648}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:08:02,533][0m Trial 30 finished with value: 0.08202967903306407 and parameters: {'observation_period_num': 138, 'train_rates': 0.9450001421187065, 'learning_rate': 0.00020657357605550697, 'batch_size': 76, 'step_size': 3, 'gamma': 0.8434274756640847}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:08:48,359][0m Trial 31 finished with value: 0.03605323533217112 and parameters: {'observation_period_num': 18, 'train_rates': 0.9305503145045841, 'learning_rate': 0.00024065094529545538, 'batch_size': 136, 'step_size': 5, 'gamma': 0.947069220181469}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:09:32,280][0m Trial 32 finished with value: 0.03352421285076575 and parameters: {'observation_period_num': 19, 'train_rates': 0.9252079839630044, 'learning_rate': 0.000594121492135908, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9465697699355144}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:10:06,329][0m Trial 33 finished with value: 0.030031953006982803 and parameters: {'observation_period_num': 20, 'train_rates': 0.9102703201173554, 'learning_rate': 0.0006017133072485849, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9462239596829984}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:10:36,671][0m Trial 34 finished with value: 0.04323622501974729 and parameters: {'observation_period_num': 24, 'train_rates': 0.7962629815696012, 'learning_rate': 0.0006360720370772413, 'batch_size': 183, 'step_size': 7, 'gamma': 0.948300510476372}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:11:11,188][0m Trial 35 finished with value: 0.07231979126610406 and parameters: {'observation_period_num': 39, 'train_rates': 0.9015985801858366, 'learning_rate': 0.0005124026903300872, 'batch_size': 182, 'step_size': 8, 'gamma': 0.919011037114527}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:11:50,193][0m Trial 36 finished with value: 0.1153929523880715 and parameters: {'observation_period_num': 103, 'train_rates': 0.8711818894163, 'learning_rate': 0.0009457319259081424, 'batch_size': 148, 'step_size': 11, 'gamma': 0.9393610908959241}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:12:23,400][0m Trial 37 finished with value: 0.06698345003732994 and parameters: {'observation_period_num': 70, 'train_rates': 0.9305222386512599, 'learning_rate': 0.00033374855797298404, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9039330768970584}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:12:53,327][0m Trial 38 finished with value: 0.14186775249403877 and parameters: {'observation_period_num': 224, 'train_rates': 0.7968228368200123, 'learning_rate': 0.00045315574478493187, 'batch_size': 209, 'step_size': 9, 'gamma': 0.8874184077882341}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:13:30,110][0m Trial 39 finished with value: 0.058491704787345646 and parameters: {'observation_period_num': 82, 'train_rates': 0.8854544320914208, 'learning_rate': 0.0006778570842787135, 'batch_size': 165, 'step_size': 6, 'gamma': 0.9254523805271998}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:14:07,160][0m Trial 40 finished with value: 0.24613923002684632 and parameters: {'observation_period_num': 21, 'train_rates': 0.6679063658185117, 'learning_rate': 9.23694324359644e-06, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8636295858715035}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:14:55,748][0m Trial 41 finished with value: 0.07753299681090675 and parameters: {'observation_period_num': 22, 'train_rates': 0.9229926185033267, 'learning_rate': 0.00026499255327106983, 'batch_size': 123, 'step_size': 9, 'gamma': 0.9611714399868324}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:15:26,601][0m Trial 42 finished with value: 0.12779486179351807 and parameters: {'observation_period_num': 59, 'train_rates': 0.9551367173606002, 'learning_rate': 0.00040897494423212954, 'batch_size': 237, 'step_size': 10, 'gamma': 0.9458126527355433}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:16:19,803][0m Trial 43 finished with value: 0.027923541681768447 and parameters: {'observation_period_num': 18, 'train_rates': 0.8953942176396502, 'learning_rate': 0.0001892791408131733, 'batch_size': 111, 'step_size': 8, 'gamma': 0.9725438704219506}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:17:03,051][0m Trial 44 finished with value: 0.03684944645552845 and parameters: {'observation_period_num': 38, 'train_rates': 0.8441439292801001, 'learning_rate': 0.00019475423560148976, 'batch_size': 139, 'step_size': 7, 'gamma': 0.9723984916726278}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:17:56,553][0m Trial 45 finished with value: 0.03254208975960432 and parameters: {'observation_period_num': 17, 'train_rates': 0.8839799835859181, 'learning_rate': 0.0006215712321801618, 'batch_size': 116, 'step_size': 8, 'gamma': 0.9112581250049908}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:18:50,992][0m Trial 46 finished with value: 0.05257748952596494 and parameters: {'observation_period_num': 48, 'train_rates': 0.8956974435788141, 'learning_rate': 0.000769456326137326, 'batch_size': 108, 'step_size': 8, 'gamma': 0.9113576205260968}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:21:35,375][0m Trial 47 finished with value: 0.0300222810869681 and parameters: {'observation_period_num': 16, 'train_rates': 0.8664483852062204, 'learning_rate': 0.0005447792710453123, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8771905315889966}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:24:14,059][0m Trial 48 finished with value: 0.1514904069447378 and parameters: {'observation_period_num': 185, 'train_rates': 0.8148477315406059, 'learning_rate': 0.0003288681435237343, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8473326098421969}. Best is trial 17 with value: 0.027172359657852658.[0m
[32m[I 2025-01-07 11:25:45,319][0m Trial 49 finished with value: 0.06418337799494958 and parameters: {'observation_period_num': 33, 'train_rates': 0.8655579192946639, 'learning_rate': 0.0004841329101507971, 'batch_size': 61, 'step_size': 10, 'gamma': 0.8715418322360693}. Best is trial 17 with value: 0.027172359657852658.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 11:25:45,329][0m A new study created in memory with name: no-name-99dc002b-01fe-4289-b6e0-0a7e388f2ca4[0m
[32m[I 2025-01-07 11:26:22,306][0m Trial 0 finished with value: 1.0266506551509222 and parameters: {'observation_period_num': 214, 'train_rates': 0.7435895324479014, 'learning_rate': 1.44289371690095e-06, 'batch_size': 138, 'step_size': 8, 'gamma': 0.8640943035777989}. Best is trial 0 with value: 1.0266506551509222.[0m
[32m[I 2025-01-07 11:26:56,854][0m Trial 1 finished with value: 0.3015940189361572 and parameters: {'observation_period_num': 117, 'train_rates': 0.9620493295588826, 'learning_rate': 1.983555747204846e-05, 'batch_size': 235, 'step_size': 4, 'gamma': 0.9465609907623157}. Best is trial 1 with value: 0.3015940189361572.[0m
[32m[I 2025-01-07 11:27:38,564][0m Trial 2 finished with value: 0.1906292049781136 and parameters: {'observation_period_num': 36, 'train_rates': 0.7635855549733827, 'learning_rate': 3.698637314664768e-05, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8792622577691631}. Best is trial 2 with value: 0.1906292049781136.[0m
[32m[I 2025-01-07 11:28:27,664][0m Trial 3 finished with value: 0.980663973515428 and parameters: {'observation_period_num': 198, 'train_rates': 0.723601082396616, 'learning_rate': 2.3624612626940204e-06, 'batch_size': 138, 'step_size': 2, 'gamma': 0.8890723201245752}. Best is trial 2 with value: 0.1906292049781136.[0m
[32m[I 2025-01-07 11:29:19,613][0m Trial 4 finished with value: 0.08538409762974805 and parameters: {'observation_period_num': 126, 'train_rates': 0.9441031147722058, 'learning_rate': 0.0007838701542576582, 'batch_size': 148, 'step_size': 9, 'gamma': 0.7944866686205325}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:31:33,434][0m Trial 5 finished with value: 0.09898974254216883 and parameters: {'observation_period_num': 26, 'train_rates': 0.7908404698114646, 'learning_rate': 8.102082314589223e-06, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9088720763088128}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:32:31,767][0m Trial 6 finished with value: 0.10356195472407764 and parameters: {'observation_period_num': 91, 'train_rates': 0.7247574435127729, 'learning_rate': 0.00028713886072289025, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8613659527188728}. Best is trial 4 with value: 0.08538409762974805.[0m
Early stopping at epoch 63
[32m[I 2025-01-07 11:32:55,457][0m Trial 7 finished with value: 1.1662824969125072 and parameters: {'observation_period_num': 80, 'train_rates': 0.7910940353661127, 'learning_rate': 1.3534242063120204e-06, 'batch_size': 181, 'step_size': 1, 'gamma': 0.8556912751795747}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:33:53,992][0m Trial 8 finished with value: 0.5569686050768252 and parameters: {'observation_period_num': 240, 'train_rates': 0.9304718319048784, 'learning_rate': 2.8499785855014947e-06, 'batch_size': 98, 'step_size': 15, 'gamma': 0.884466785157021}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:34:51,667][0m Trial 9 finished with value: 0.10239184145749222 and parameters: {'observation_period_num': 80, 'train_rates': 0.6929740025293011, 'learning_rate': 0.00036882013629368273, 'batch_size': 102, 'step_size': 3, 'gamma': 0.9610498891402062}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:35:36,795][0m Trial 10 finished with value: 0.15293219048302273 and parameters: {'observation_period_num': 159, 'train_rates': 0.8888634481261644, 'learning_rate': 0.000895283282901083, 'batch_size': 211, 'step_size': 13, 'gamma': 0.774585740075096}. Best is trial 4 with value: 0.08538409762974805.[0m
[32m[I 2025-01-07 11:38:52,533][0m Trial 11 finished with value: 0.0743707950572072 and parameters: {'observation_period_num': 19, 'train_rates': 0.8657579271494086, 'learning_rate': 1.844034314398164e-05, 'batch_size': 28, 'step_size': 11, 'gamma': 0.75617859036069}. Best is trial 11 with value: 0.0743707950572072.[0m
[32m[I 2025-01-07 11:41:09,331][0m Trial 12 finished with value: 0.052922513261063016 and parameters: {'observation_period_num': 9, 'train_rates': 0.8648484263393874, 'learning_rate': 7.444828745719565e-05, 'batch_size': 41, 'step_size': 11, 'gamma': 0.7502593972585639}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:46:45,186][0m Trial 13 finished with value: 0.05316821747113273 and parameters: {'observation_period_num': 5, 'train_rates': 0.8707172131930357, 'learning_rate': 7.367882214204107e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7579498976359296}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:48:08,464][0m Trial 14 finished with value: 0.09551747024304023 and parameters: {'observation_period_num': 7, 'train_rates': 0.6280624101794718, 'learning_rate': 7.467893679071536e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8093996002662373}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:53:41,416][0m Trial 15 finished with value: 0.07918543887921377 and parameters: {'observation_period_num': 56, 'train_rates': 0.8715208681300347, 'learning_rate': 0.0001042730701265367, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8193848064660354}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:55:14,040][0m Trial 16 finished with value: 0.06931041261840208 and parameters: {'observation_period_num': 63, 'train_rates': 0.8311127326184218, 'learning_rate': 0.0001016109792237071, 'batch_size': 65, 'step_size': 15, 'gamma': 0.7685010593511374}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:56:51,137][0m Trial 17 finished with value: 0.0919496511930449 and parameters: {'observation_period_num': 163, 'train_rates': 0.8365270630310934, 'learning_rate': 0.00020607978168520408, 'batch_size': 63, 'step_size': 6, 'gamma': 0.750512905384573}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 11:57:57,269][0m Trial 18 finished with value: 0.08569935740638966 and parameters: {'observation_period_num': 46, 'train_rates': 0.9104391211847029, 'learning_rate': 4.712793366148288e-05, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8317162282066792}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:00:48,636][0m Trial 19 finished with value: 0.08688873797655106 and parameters: {'observation_period_num': 6, 'train_rates': 0.987970960581733, 'learning_rate': 7.664255035618964e-06, 'batch_size': 38, 'step_size': 9, 'gamma': 0.7945222038139546}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:02:03,485][0m Trial 20 finished with value: 0.06976374712777081 and parameters: {'observation_period_num': 104, 'train_rates': 0.8519542784797132, 'learning_rate': 0.00016862309050108727, 'batch_size': 81, 'step_size': 10, 'gamma': 0.7832711005352772}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:03:49,286][0m Trial 21 finished with value: 0.07216750873067906 and parameters: {'observation_period_num': 59, 'train_rates': 0.8191169627369796, 'learning_rate': 8.58095143573664e-05, 'batch_size': 57, 'step_size': 15, 'gamma': 0.7707039767774413}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:09:41,981][0m Trial 22 finished with value: 0.07354353031102759 and parameters: {'observation_period_num': 65, 'train_rates': 0.9054325794765543, 'learning_rate': 0.00012458312617756346, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7536154037992138}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:11:20,779][0m Trial 23 finished with value: 0.07011065720835796 and parameters: {'observation_period_num': 25, 'train_rates': 0.8193801921807916, 'learning_rate': 4.652197178409647e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8352565335701232}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:13:32,014][0m Trial 24 finished with value: 0.07272381063844603 and parameters: {'observation_period_num': 5, 'train_rates': 0.8412051747143545, 'learning_rate': 0.00044749288392043627, 'batch_size': 44, 'step_size': 13, 'gamma': 0.7717946577111106}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:14:46,803][0m Trial 25 finished with value: 0.10182746695724831 and parameters: {'observation_period_num': 42, 'train_rates': 0.8879310745849349, 'learning_rate': 1.93263941065989e-05, 'batch_size': 86, 'step_size': 11, 'gamma': 0.8070320984029786}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:15:41,229][0m Trial 26 finished with value: 0.15403172373771667 and parameters: {'observation_period_num': 144, 'train_rates': 0.7988095704515912, 'learning_rate': 6.919412105457239e-05, 'batch_size': 117, 'step_size': 6, 'gamma': 0.7865542501464649}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:17:19,422][0m Trial 27 finished with value: 0.18111123005363428 and parameters: {'observation_period_num': 71, 'train_rates': 0.925903604609384, 'learning_rate': 1.0883767037866004e-05, 'batch_size': 69, 'step_size': 14, 'gamma': 0.7639881241533863}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:19:54,594][0m Trial 28 finished with value: 0.06770940410329941 and parameters: {'observation_period_num': 35, 'train_rates': 0.8713560491677212, 'learning_rate': 0.0001570560102390712, 'batch_size': 38, 'step_size': 10, 'gamma': 0.8373815262787744}. Best is trial 12 with value: 0.052922513261063016.[0m
[32m[I 2025-01-07 12:23:38,141][0m Trial 29 finished with value: 0.04342335549204848 and parameters: {'observation_period_num': 24, 'train_rates': 0.9709211066160284, 'learning_rate': 0.00015840946362171565, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9130564804790825}. Best is trial 29 with value: 0.04342335549204848.[0m
[32m[I 2025-01-07 12:29:59,532][0m Trial 30 finished with value: 0.047792913569580944 and parameters: {'observation_period_num': 24, 'train_rates': 0.9859585636431712, 'learning_rate': 0.00022381119416365147, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9204088098420098}. Best is trial 29 with value: 0.04342335549204848.[0m
[32m[I 2025-01-07 12:35:56,074][0m Trial 31 finished with value: 0.02777345066091844 and parameters: {'observation_period_num': 22, 'train_rates': 0.9858843112930379, 'learning_rate': 0.0002264264507039241, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9211312499234133}. Best is trial 31 with value: 0.02777345066091844.[0m
[32m[I 2025-01-07 12:39:37,261][0m Trial 32 finished with value: 0.022309385291818116 and parameters: {'observation_period_num': 22, 'train_rates': 0.9877648868604361, 'learning_rate': 0.0002611511446315947, 'batch_size': 29, 'step_size': 8, 'gamma': 0.9263027157541567}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:43:36,012][0m Trial 33 finished with value: 0.03555759748754402 and parameters: {'observation_period_num': 49, 'train_rates': 0.9836125823544873, 'learning_rate': 0.0005319193310251174, 'batch_size': 25, 'step_size': 7, 'gamma': 0.9161310524898529}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:46:59,081][0m Trial 34 finished with value: 0.06329292343819842 and parameters: {'observation_period_num': 38, 'train_rates': 0.9653446791890582, 'learning_rate': 0.000529611203679518, 'batch_size': 30, 'step_size': 5, 'gamma': 0.9338792685109257}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:48:01,544][0m Trial 35 finished with value: 0.0725623071193695 and parameters: {'observation_period_num': 47, 'train_rates': 0.9596144481666566, 'learning_rate': 0.0006587303750198996, 'batch_size': 170, 'step_size': 7, 'gamma': 0.9815286919719652}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:48:56,537][0m Trial 36 finished with value: 0.14500266313552856 and parameters: {'observation_period_num': 103, 'train_rates': 0.9485168436141027, 'learning_rate': 0.0003281807936313314, 'batch_size': 249, 'step_size': 7, 'gamma': 0.9030547718407542}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:49:55,353][0m Trial 37 finished with value: 0.10026490688323975 and parameters: {'observation_period_num': 200, 'train_rates': 0.9706250041598548, 'learning_rate': 0.0009774307948300805, 'batch_size': 127, 'step_size': 9, 'gamma': 0.944830599723396}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:51:52,613][0m Trial 38 finished with value: 0.04943930629927378 and parameters: {'observation_period_num': 31, 'train_rates': 0.9335475785067257, 'learning_rate': 0.00025816136569939164, 'batch_size': 51, 'step_size': 8, 'gamma': 0.9021127337161863}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:55:13,727][0m Trial 39 finished with value: 0.024717122180069367 and parameters: {'observation_period_num': 20, 'train_rates': 0.9861385348726915, 'learning_rate': 0.000499080508273532, 'batch_size': 32, 'step_size': 5, 'gamma': 0.9217562153557907}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 12:56:28,807][0m Trial 40 finished with value: 0.08722230046987534 and parameters: {'observation_period_num': 81, 'train_rates': 0.947161100886327, 'learning_rate': 0.0005545107952551332, 'batch_size': 76, 'step_size': 4, 'gamma': 0.929865244415455}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:00:07,835][0m Trial 41 finished with value: 0.02558872802183032 and parameters: {'observation_period_num': 20, 'train_rates': 0.9878536554137406, 'learning_rate': 0.00039734233995478526, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9144093678662801}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:03:36,614][0m Trial 42 finished with value: 0.05015110992120974 and parameters: {'observation_period_num': 50, 'train_rates': 0.988951568785568, 'learning_rate': 0.00035568621498485394, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8936856223641488}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:05:34,104][0m Trial 43 finished with value: 0.06518411037135631 and parameters: {'observation_period_num': 20, 'train_rates': 0.9200334790728937, 'learning_rate': 0.0006633517139086775, 'batch_size': 49, 'step_size': 5, 'gamma': 0.9537426066360398}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:06:06,239][0m Trial 44 finished with value: 0.058062512427568436 and parameters: {'observation_period_num': 16, 'train_rates': 0.9538669975229837, 'learning_rate': 0.0004302383882443638, 'batch_size': 210, 'step_size': 5, 'gamma': 0.8688678500353172}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:09:28,556][0m Trial 45 finished with value: 0.04066352954466049 and parameters: {'observation_period_num': 37, 'train_rates': 0.9751413888763361, 'learning_rate': 0.0002886823652096799, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9734606875639201}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:11:34,409][0m Trial 46 finished with value: 0.053671561471075595 and parameters: {'observation_period_num': 18, 'train_rates': 0.9344036695057264, 'learning_rate': 0.0007374954352578142, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9241031564926682}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:12:24,556][0m Trial 47 finished with value: 0.24657081784743842 and parameters: {'observation_period_num': 249, 'train_rates': 0.6739800912061175, 'learning_rate': 0.00047315821118749886, 'batch_size': 93, 'step_size': 7, 'gamma': 0.8811016465727093}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:15:11,821][0m Trial 48 finished with value: 0.08151640655363307 and parameters: {'observation_period_num': 52, 'train_rates': 0.9005028787750643, 'learning_rate': 0.0001958157044133993, 'batch_size': 34, 'step_size': 6, 'gamma': 0.946559356061069}. Best is trial 32 with value: 0.022309385291818116.[0m
[32m[I 2025-01-07 13:15:58,306][0m Trial 49 finished with value: 0.08770429055289466 and parameters: {'observation_period_num': 32, 'train_rates': 0.7582438812167669, 'learning_rate': 0.00028486604924148404, 'batch_size': 154, 'step_size': 1, 'gamma': 0.9367997306349275}. Best is trial 32 with value: 0.022309385291818116.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 13:15:58,316][0m A new study created in memory with name: no-name-6d2e492d-dab1-4f55-a080-ebb49eb71351[0m
[32m[I 2025-01-07 13:18:27,890][0m Trial 0 finished with value: 0.17652962095895958 and parameters: {'observation_period_num': 163, 'train_rates': 0.6172165055664999, 'learning_rate': 0.0004987976559797027, 'batch_size': 28, 'step_size': 3, 'gamma': 0.9878259004091163}. Best is trial 0 with value: 0.17652962095895958.[0m
Early stopping at epoch 92
[32m[I 2025-01-07 13:19:15,280][0m Trial 1 finished with value: 0.40273676538264785 and parameters: {'observation_period_num': 27, 'train_rates': 0.8796183412715015, 'learning_rate': 1.925889790841775e-05, 'batch_size': 151, 'step_size': 2, 'gamma': 0.7760946172826101}. Best is trial 0 with value: 0.17652962095895958.[0m
[32m[I 2025-01-07 13:19:50,302][0m Trial 2 finished with value: 0.44527678310765256 and parameters: {'observation_period_num': 138, 'train_rates': 0.8003064167227889, 'learning_rate': 6.966782105969052e-06, 'batch_size': 183, 'step_size': 6, 'gamma': 0.9040976443263811}. Best is trial 0 with value: 0.17652962095895958.[0m
[32m[I 2025-01-07 13:21:05,823][0m Trial 3 finished with value: 0.42521196215698037 and parameters: {'observation_period_num': 85, 'train_rates': 0.7911633712591941, 'learning_rate': 4.931390927924353e-06, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8729390463954232}. Best is trial 0 with value: 0.17652962095895958.[0m
[32m[I 2025-01-07 13:22:02,555][0m Trial 4 finished with value: 0.03701283189433592 and parameters: {'observation_period_num': 20, 'train_rates': 0.917171973085647, 'learning_rate': 0.00032741566575572535, 'batch_size': 106, 'step_size': 6, 'gamma': 0.8302271246114112}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:22:39,339][0m Trial 5 finished with value: 0.1630233583327766 and parameters: {'observation_period_num': 158, 'train_rates': 0.84719735111803, 'learning_rate': 0.0005356094714762991, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9448480965437601}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:23:07,022][0m Trial 6 finished with value: 0.32339072041949735 and parameters: {'observation_period_num': 57, 'train_rates': 0.8253018641936628, 'learning_rate': 7.0540318290346384e-06, 'batch_size': 229, 'step_size': 12, 'gamma': 0.8163790851867185}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:23:37,410][0m Trial 7 finished with value: 0.04824171708412253 and parameters: {'observation_period_num': 58, 'train_rates': 0.8804996596618067, 'learning_rate': 0.0002720243320926098, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9083182492717864}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:24:29,608][0m Trial 8 finished with value: 0.335593874566257 and parameters: {'observation_period_num': 82, 'train_rates': 0.9555404021543278, 'learning_rate': 6.0788171870662625e-06, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8859580768504121}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:25:26,124][0m Trial 9 finished with value: 0.8663948774337769 and parameters: {'observation_period_num': 81, 'train_rates': 0.9477861893327518, 'learning_rate': 1.8912633712757863e-06, 'batch_size': 181, 'step_size': 14, 'gamma': 0.8386859058886071}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:26:41,971][0m Trial 10 finished with value: 0.16975131117973197 and parameters: {'observation_period_num': 241, 'train_rates': 0.7290515702773844, 'learning_rate': 0.00010741231193220834, 'batch_size': 94, 'step_size': 8, 'gamma': 0.7521822623200427}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:27:30,273][0m Trial 11 finished with value: 0.04610337506725181 and parameters: {'observation_period_num': 12, 'train_rates': 0.8906860984721089, 'learning_rate': 0.000153064400051067, 'batch_size': 244, 'step_size': 6, 'gamma': 0.9292123776441941}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:28:25,433][0m Trial 12 finished with value: 0.055341560393571854 and parameters: {'observation_period_num': 7, 'train_rates': 0.9868713236500609, 'learning_rate': 0.00011786874822525078, 'batch_size': 251, 'step_size': 7, 'gamma': 0.8271535397665588}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:30:26,210][0m Trial 13 finished with value: 0.03715754595525082 and parameters: {'observation_period_num': 16, 'train_rates': 0.9155700299391291, 'learning_rate': 7.400166494731436e-05, 'batch_size': 57, 'step_size': 10, 'gamma': 0.9468161747281263}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:32:26,926][0m Trial 14 finished with value: 0.1228232267694395 and parameters: {'observation_period_num': 206, 'train_rates': 0.7335871501434977, 'learning_rate': 4.373923071041938e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9882328805795256}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:33:48,700][0m Trial 15 finished with value: 0.08722913276026215 and parameters: {'observation_period_num': 47, 'train_rates': 0.9288141973562783, 'learning_rate': 0.0009453728753428972, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8439997729754377}. Best is trial 4 with value: 0.03701283189433592.[0m
Early stopping at epoch 68
[32m[I 2025-01-07 13:34:20,402][0m Trial 16 finished with value: 0.5100328814619249 and parameters: {'observation_period_num': 103, 'train_rates': 0.7361248000195896, 'learning_rate': 5.165992304661468e-05, 'batch_size': 118, 'step_size': 1, 'gamma': 0.7950695517846168}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:36:09,484][0m Trial 17 finished with value: 0.05027485113110273 and parameters: {'observation_period_num': 33, 'train_rates': 0.9153743682406975, 'learning_rate': 2.7768061270538945e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9526803859997413}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:37:18,130][0m Trial 18 finished with value: 0.11742918938398361 and parameters: {'observation_period_num': 122, 'train_rates': 0.9826819827572497, 'learning_rate': 0.0002562399481861091, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8584113516088913}. Best is trial 4 with value: 0.03701283189433592.[0m
[32m[I 2025-01-07 13:40:54,928][0m Trial 19 finished with value: 0.03341876134580122 and parameters: {'observation_period_num': 8, 'train_rates': 0.6546622550950503, 'learning_rate': 6.324168278568484e-05, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8027684413718548}. Best is trial 19 with value: 0.03341876134580122.[0m
[32m[I 2025-01-07 13:44:35,363][0m Trial 20 finished with value: 0.13647477700664604 and parameters: {'observation_period_num': 45, 'train_rates': 0.6009111904678144, 'learning_rate': 1.8506983657837243e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8038549935725176}. Best is trial 19 with value: 0.03341876134580122.[0m
[32m[I 2025-01-07 13:46:20,712][0m Trial 21 finished with value: 0.04432086977995273 and parameters: {'observation_period_num': 15, 'train_rates': 0.6730767810296439, 'learning_rate': 7.439844376897768e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7638877693043593}. Best is trial 19 with value: 0.03341876134580122.[0m
[32m[I 2025-01-07 13:47:19,759][0m Trial 22 finished with value: 0.03280086542519046 and parameters: {'observation_period_num': 7, 'train_rates': 0.6387800615440151, 'learning_rate': 0.0002218766751959904, 'batch_size': 80, 'step_size': 15, 'gamma': 0.7889332963194743}. Best is trial 22 with value: 0.03280086542519046.[0m
[32m[I 2025-01-07 13:48:15,137][0m Trial 23 finished with value: 0.04690826028647137 and parameters: {'observation_period_num': 35, 'train_rates': 0.6689877753231287, 'learning_rate': 0.0002441322012428801, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7893139853822458}. Best is trial 22 with value: 0.03280086542519046.[0m
[32m[I 2025-01-07 13:48:57,026][0m Trial 24 finished with value: 0.06379774133757175 and parameters: {'observation_period_num': 67, 'train_rates': 0.6671391431986552, 'learning_rate': 0.000552957632959961, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8178305442473833}. Best is trial 22 with value: 0.03280086542519046.[0m
[32m[I 2025-01-07 13:49:33,706][0m Trial 25 finished with value: 0.06208333065951934 and parameters: {'observation_period_num': 33, 'train_rates': 0.635536991489878, 'learning_rate': 0.0001561054147731776, 'batch_size': 136, 'step_size': 13, 'gamma': 0.7836573249956883}. Best is trial 22 with value: 0.03280086542519046.[0m
[32m[I 2025-01-07 13:50:33,393][0m Trial 26 finished with value: 0.02989270319225663 and parameters: {'observation_period_num': 6, 'train_rates': 0.6958712928400423, 'learning_rate': 0.00031520795993351204, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8078821809944344}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 13:51:37,023][0m Trial 27 finished with value: 0.09074464885445384 and parameters: {'observation_period_num': 107, 'train_rates': 0.7025101223715513, 'learning_rate': 0.00017854525236958945, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8056329144933249}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 13:53:44,956][0m Trial 28 finished with value: 0.03326542359788142 and parameters: {'observation_period_num': 8, 'train_rates': 0.6379881619480355, 'learning_rate': 0.000817278551303874, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7678410450656367}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 13:55:47,772][0m Trial 29 finished with value: 0.1632306750196253 and parameters: {'observation_period_num': 180, 'train_rates': 0.6258093649447352, 'learning_rate': 0.0007027546021506057, 'batch_size': 34, 'step_size': 15, 'gamma': 0.7681204523894755}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 13:57:04,521][0m Trial 30 finished with value: 0.10311485727529965 and parameters: {'observation_period_num': 70, 'train_rates': 0.6927359028016905, 'learning_rate': 0.0009268210620349311, 'batch_size': 61, 'step_size': 14, 'gamma': 0.7628172301084984}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 13:59:51,266][0m Trial 31 finished with value: 0.0380903523611395 and parameters: {'observation_period_num': 7, 'train_rates': 0.6455226771697007, 'learning_rate': 0.00040075849142297786, 'batch_size': 27, 'step_size': 13, 'gamma': 0.7972623319203376}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 14:04:10,986][0m Trial 32 finished with value: 0.1299236744383107 and parameters: {'observation_period_num': 44, 'train_rates': 0.6049287882819342, 'learning_rate': 0.0003948980292979633, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7772830904011819}. Best is trial 26 with value: 0.02989270319225663.[0m
[32m[I 2025-01-07 14:05:59,554][0m Trial 33 finished with value: 0.02861506853020933 and parameters: {'observation_period_num': 5, 'train_rates': 0.6455842433315105, 'learning_rate': 0.0006222650001541163, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7517703082774607}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:07:45,046][0m Trial 34 finished with value: 0.0454462833491594 and parameters: {'observation_period_num': 28, 'train_rates': 0.7706575915222698, 'learning_rate': 0.0006530171555763342, 'batch_size': 48, 'step_size': 11, 'gamma': 0.752765757228737}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:08:46,441][0m Trial 35 finished with value: 0.05223200319523222 and parameters: {'observation_period_num': 27, 'train_rates': 0.6959419804612434, 'learning_rate': 0.0009847142889997645, 'batch_size': 80, 'step_size': 14, 'gamma': 0.7769243622121259}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:09:52,001][0m Trial 36 finished with value: 0.08631245031843691 and parameters: {'observation_period_num': 50, 'train_rates': 0.6241669820718422, 'learning_rate': 0.0003635028255900068, 'batch_size': 68, 'step_size': 11, 'gamma': 0.7505308280106336}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:11:53,713][0m Trial 37 finished with value: 0.10006007832199545 and parameters: {'observation_period_num': 22, 'train_rates': 0.7094107793832318, 'learning_rate': 0.0005024594106069177, 'batch_size': 39, 'step_size': 15, 'gamma': 0.774248606037039}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:12:44,588][0m Trial 38 finished with value: 0.19787197694355665 and parameters: {'observation_period_num': 68, 'train_rates': 0.7617775004491558, 'learning_rate': 1.2406295321389926e-05, 'batch_size': 102, 'step_size': 13, 'gamma': 0.7862806880722506}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:13:59,799][0m Trial 39 finished with value: 0.04231669293606982 and parameters: {'observation_period_num': 21, 'train_rates': 0.6466013837246273, 'learning_rate': 0.00022691992560236224, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8612359547805696}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:14:34,934][0m Trial 40 finished with value: 1.5835650307520972 and parameters: {'observation_period_num': 40, 'train_rates': 0.6762137122978277, 'learning_rate': 1.0090692981765246e-06, 'batch_size': 151, 'step_size': 3, 'gamma': 0.8104017378501462}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:17:41,713][0m Trial 41 finished with value: 0.044952890464271214 and parameters: {'observation_period_num': 8, 'train_rates': 0.65440717432039, 'learning_rate': 0.00033651911758452515, 'batch_size': 24, 'step_size': 14, 'gamma': 0.7637480304980386}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:19:54,443][0m Trial 42 finished with value: 0.06154556271941525 and parameters: {'observation_period_num': 8, 'train_rates': 0.6163765073521025, 'learning_rate': 0.0006998057501153865, 'batch_size': 33, 'step_size': 13, 'gamma': 0.823764704475761}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:21:27,757][0m Trial 43 finished with value: 0.04704748673714919 and parameters: {'observation_period_num': 24, 'train_rates': 0.6553243184247683, 'learning_rate': 9.552285770790299e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.8438649382730533}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:23:55,384][0m Trial 44 finished with value: 0.02921148701979753 and parameters: {'observation_period_num': 5, 'train_rates': 0.6835113241968268, 'learning_rate': 0.0001935453331765814, 'batch_size': 31, 'step_size': 14, 'gamma': 0.7979621826388978}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:25:08,694][0m Trial 45 finished with value: 0.05883854255080223 and parameters: {'observation_period_num': 58, 'train_rates': 0.7189226743205146, 'learning_rate': 0.0002060313071283706, 'batch_size': 67, 'step_size': 14, 'gamma': 0.788753713608235}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:25:37,519][0m Trial 46 finished with value: 0.13553306593062125 and parameters: {'observation_period_num': 158, 'train_rates': 0.6866958849523656, 'learning_rate': 0.00013439348994964359, 'batch_size': 186, 'step_size': 15, 'gamma': 0.8884782215429113}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:26:31,344][0m Trial 47 finished with value: 0.05105603292850762 and parameters: {'observation_period_num': 18, 'train_rates': 0.6378366998180514, 'learning_rate': 0.0004602439559553506, 'batch_size': 88, 'step_size': 14, 'gamma': 0.7594306437037102}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:28:57,594][0m Trial 48 finished with value: 0.06804181303572558 and parameters: {'observation_period_num': 35, 'train_rates': 0.8307648079298429, 'learning_rate': 0.0003075079688263664, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8361098286849493}. Best is trial 33 with value: 0.02861506853020933.[0m
[32m[I 2025-01-07 14:29:41,562][0m Trial 49 finished with value: 0.05034486245225977 and parameters: {'observation_period_num': 56, 'train_rates': 0.7672134032916619, 'learning_rate': 0.0006533072151708661, 'batch_size': 133, 'step_size': 11, 'gamma': 0.8128573593008969}. Best is trial 33 with value: 0.02861506853020933.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 14:29:41,572][0m A new study created in memory with name: no-name-df2c6617-a0a6-41b8-9455-d533d67964fd[0m
[32m[I 2025-01-07 14:30:21,399][0m Trial 0 finished with value: 0.0847676363415443 and parameters: {'observation_period_num': 112, 'train_rates': 0.8539152253910107, 'learning_rate': 0.00017466279685639753, 'batch_size': 240, 'step_size': 14, 'gamma': 0.8845216336131957}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:33:27,916][0m Trial 1 finished with value: 0.18593026796005174 and parameters: {'observation_period_num': 178, 'train_rates': 0.7054176669959185, 'learning_rate': 2.732038924751892e-05, 'batch_size': 24, 'step_size': 4, 'gamma': 0.8315718770329985}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:33:59,916][0m Trial 2 finished with value: 0.35205948430390266 and parameters: {'observation_period_num': 218, 'train_rates': 0.8315695682741281, 'learning_rate': 8.607683416648431e-06, 'batch_size': 250, 'step_size': 15, 'gamma': 0.890108825136649}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:34:36,713][0m Trial 3 finished with value: 0.14807140827178955 and parameters: {'observation_period_num': 215, 'train_rates': 0.9535277440262493, 'learning_rate': 0.0004764911386956838, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8689657120618723}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:35:07,769][0m Trial 4 finished with value: 1.0583562534139876 and parameters: {'observation_period_num': 246, 'train_rates': 0.6191907060421667, 'learning_rate': 1.8278088935598852e-06, 'batch_size': 200, 'step_size': 12, 'gamma': 0.8714918933774545}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:35:46,294][0m Trial 5 finished with value: 0.7312190150184551 and parameters: {'observation_period_num': 100, 'train_rates': 0.6675854079076705, 'learning_rate': 3.145420950863496e-06, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8621201956488834}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:36:27,026][0m Trial 6 finished with value: 0.31226835929668323 and parameters: {'observation_period_num': 103, 'train_rates': 0.655228346936828, 'learning_rate': 6.8550777351819054e-06, 'batch_size': 211, 'step_size': 13, 'gamma': 0.9459192545041707}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:37:35,604][0m Trial 7 finished with value: 0.3625056294411638 and parameters: {'observation_period_num': 68, 'train_rates': 0.7627808769684445, 'learning_rate': 5.361336273133516e-06, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9386053318544463}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:38:13,984][0m Trial 8 finished with value: 0.14617899546014054 and parameters: {'observation_period_num': 156, 'train_rates': 0.7565460950425872, 'learning_rate': 0.0009059994374882419, 'batch_size': 218, 'step_size': 15, 'gamma': 0.8400244492689879}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:38:51,654][0m Trial 9 finished with value: 0.32603899162748584 and parameters: {'observation_period_num': 62, 'train_rates': 0.6182852241040929, 'learning_rate': 5.577461431693672e-06, 'batch_size': 255, 'step_size': 15, 'gamma': 0.9373859673761598}. Best is trial 0 with value: 0.0847676363415443.[0m
[32m[I 2025-01-07 14:39:47,542][0m Trial 10 finished with value: 0.040816722778787076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9037833213362487, 'learning_rate': 0.000143038647423993, 'batch_size': 122, 'step_size': 8, 'gamma': 0.7874335523015903}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:40:41,445][0m Trial 11 finished with value: 0.04449757515060856 and parameters: {'observation_period_num': 7, 'train_rates': 0.9122608725643515, 'learning_rate': 0.00015996614466412608, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7763047485074331}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:41:36,568][0m Trial 12 finished with value: 0.062396034944889156 and parameters: {'observation_period_num': 14, 'train_rates': 0.9416099341515188, 'learning_rate': 9.209348093499893e-05, 'batch_size': 116, 'step_size': 7, 'gamma': 0.7507417183174031}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:42:32,422][0m Trial 13 finished with value: 0.05187654153073096 and parameters: {'observation_period_num': 14, 'train_rates': 0.8914961283100937, 'learning_rate': 9.48466420796606e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.7655739986583872}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:44:07,160][0m Trial 14 finished with value: 0.07034076005220413 and parameters: {'observation_period_num': 47, 'train_rates': 0.9833133538710597, 'learning_rate': 2.9494581756858882e-05, 'batch_size': 69, 'step_size': 10, 'gamma': 0.7901060519629441}. Best is trial 10 with value: 0.040816722778787076.[0m
Early stopping at epoch 63
[32m[I 2025-01-07 14:44:38,126][0m Trial 15 finished with value: 0.08277702375513608 and parameters: {'observation_period_num': 6, 'train_rates': 0.8965158408014975, 'learning_rate': 0.00027772063054591997, 'batch_size': 153, 'step_size': 1, 'gamma': 0.7940091597996459}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:46:02,437][0m Trial 16 finished with value: 0.043598738717307677 and parameters: {'observation_period_num': 31, 'train_rates': 0.8849272547063614, 'learning_rate': 8.127758282274092e-05, 'batch_size': 81, 'step_size': 10, 'gamma': 0.8032125625983525}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:47:38,207][0m Trial 17 finished with value: 0.05450766169657019 and parameters: {'observation_period_num': 45, 'train_rates': 0.8356374633739208, 'learning_rate': 4.5014352846990815e-05, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8107048848366692}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:51:38,718][0m Trial 18 finished with value: 0.052146986278694234 and parameters: {'observation_period_num': 41, 'train_rates': 0.7978666159483093, 'learning_rate': 1.7608572140766413e-05, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8168878934218429}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:52:37,413][0m Trial 19 finished with value: 0.05629953572169487 and parameters: {'observation_period_num': 81, 'train_rates': 0.8659172526467244, 'learning_rate': 6.791344313886196e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9060785746788828}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:54:51,970][0m Trial 20 finished with value: 0.10104124671356245 and parameters: {'observation_period_num': 140, 'train_rates': 0.9845414222533176, 'learning_rate': 0.000507372397250153, 'batch_size': 43, 'step_size': 5, 'gamma': 0.8394105944020573}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:55:36,544][0m Trial 21 finished with value: 0.08231725819365826 and parameters: {'observation_period_num': 25, 'train_rates': 0.9410913827443076, 'learning_rate': 0.0001792022423738499, 'batch_size': 141, 'step_size': 8, 'gamma': 0.9862814987792066}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:56:29,806][0m Trial 22 finished with value: 0.04421859138227742 and parameters: {'observation_period_num': 33, 'train_rates': 0.9009677767687247, 'learning_rate': 0.00018882298918150757, 'batch_size': 111, 'step_size': 7, 'gamma': 0.7786723169297528}. Best is trial 10 with value: 0.040816722778787076.[0m
[32m[I 2025-01-07 14:57:34,975][0m Trial 23 finished with value: 0.03649150664971127 and parameters: {'observation_period_num': 30, 'train_rates': 0.9159000556031455, 'learning_rate': 0.0002847367746087278, 'batch_size': 92, 'step_size': 11, 'gamma': 0.7956471989094615}. Best is trial 23 with value: 0.03649150664971127.[0m
[32m[I 2025-01-07 14:59:17,810][0m Trial 24 finished with value: 0.05438402109167588 and parameters: {'observation_period_num': 65, 'train_rates': 0.798283017784225, 'learning_rate': 0.0003452536230485646, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8009101031334329}. Best is trial 23 with value: 0.03649150664971127.[0m
[32m[I 2025-01-07 15:00:19,675][0m Trial 25 finished with value: 0.06719753464912986 and parameters: {'observation_period_num': 77, 'train_rates': 0.8700873848348534, 'learning_rate': 6.018918928436862e-05, 'batch_size': 92, 'step_size': 12, 'gamma': 0.7606866946934091}. Best is trial 23 with value: 0.03649150664971127.[0m
[32m[I 2025-01-07 15:00:56,187][0m Trial 26 finished with value: 0.03445640508154476 and parameters: {'observation_period_num': 30, 'train_rates': 0.9261229666749138, 'learning_rate': 0.0006961065284747594, 'batch_size': 175, 'step_size': 9, 'gamma': 0.823449133347969}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:01:32,419][0m Trial 27 finished with value: 0.04428607037412786 and parameters: {'observation_period_num': 51, 'train_rates': 0.9263837959194327, 'learning_rate': 0.0009634379952810518, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8223153311158824}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:02:08,294][0m Trial 28 finished with value: 0.06973455846309662 and parameters: {'observation_period_num': 80, 'train_rates': 0.9615334829024548, 'learning_rate': 0.000494805574880686, 'batch_size': 174, 'step_size': 8, 'gamma': 0.8498810986243617}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:02:45,439][0m Trial 29 finished with value: 0.06801961159900478 and parameters: {'observation_period_num': 121, 'train_rates': 0.8376612748335425, 'learning_rate': 0.00013936305467544862, 'batch_size': 150, 'step_size': 13, 'gamma': 0.7745906498245364}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:03:20,579][0m Trial 30 finished with value: 0.04694612578234889 and parameters: {'observation_period_num': 25, 'train_rates': 0.9249299084557572, 'learning_rate': 0.00027350069108237273, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8250376652872908}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:04:05,645][0m Trial 31 finished with value: 0.035325436173258604 and parameters: {'observation_period_num': 29, 'train_rates': 0.8735541971646344, 'learning_rate': 0.0006684081050424495, 'batch_size': 130, 'step_size': 11, 'gamma': 0.8049113957488883}. Best is trial 26 with value: 0.03445640508154476.[0m
[32m[I 2025-01-07 15:04:48,848][0m Trial 32 finished with value: 0.0319076203797237 and parameters: {'observation_period_num': 20, 'train_rates': 0.8587064892386436, 'learning_rate': 0.0006365719535765252, 'batch_size': 138, 'step_size': 11, 'gamma': 0.7902409533329344}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:05:31,903][0m Trial 33 finished with value: 0.03883003225460004 and parameters: {'observation_period_num': 55, 'train_rates': 0.8646076388659973, 'learning_rate': 0.0006489344075885495, 'batch_size': 136, 'step_size': 11, 'gamma': 0.8064319314837779}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:06:10,210][0m Trial 34 finished with value: 0.03575463630814015 and parameters: {'observation_period_num': 27, 'train_rates': 0.8166619883637674, 'learning_rate': 0.0007684452890498047, 'batch_size': 157, 'step_size': 13, 'gamma': 0.850332211179225}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:06:41,228][0m Trial 35 finished with value: 0.11769250798083487 and parameters: {'observation_period_num': 171, 'train_rates': 0.8190277973735749, 'learning_rate': 0.0006971366092743323, 'batch_size': 229, 'step_size': 14, 'gamma': 0.8893101498080942}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:07:17,670][0m Trial 36 finished with value: 0.06890497062386584 and parameters: {'observation_period_num': 89, 'train_rates': 0.7507023053138687, 'learning_rate': 0.00039654858986803515, 'batch_size': 164, 'step_size': 13, 'gamma': 0.847722821378088}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:07:52,453][0m Trial 37 finished with value: 0.16683020034144003 and parameters: {'observation_period_num': 214, 'train_rates': 0.728477317497778, 'learning_rate': 0.000660333044825373, 'batch_size': 192, 'step_size': 14, 'gamma': 0.8617990513987606}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:08:37,079][0m Trial 38 finished with value: 0.49051899769069107 and parameters: {'observation_period_num': 22, 'train_rates': 0.8184052538015282, 'learning_rate': 1.044481181176844e-06, 'batch_size': 136, 'step_size': 12, 'gamma': 0.8311385096476879}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:09:17,337][0m Trial 39 finished with value: 0.10617138617968287 and parameters: {'observation_period_num': 97, 'train_rates': 0.7853692828463167, 'learning_rate': 0.0009936303070022284, 'batch_size': 155, 'step_size': 9, 'gamma': 0.8816893607227886}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:09:51,388][0m Trial 40 finished with value: 0.10798947536756122 and parameters: {'observation_period_num': 41, 'train_rates': 0.8486815638184153, 'learning_rate': 1.3660322652641675e-05, 'batch_size': 178, 'step_size': 12, 'gamma': 0.9058693968064466}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:10:45,696][0m Trial 41 finished with value: 0.0331769389951639 and parameters: {'observation_period_num': 34, 'train_rates': 0.8807553337251199, 'learning_rate': 0.00026207469077336163, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8156160317781338}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:11:38,504][0m Trial 42 finished with value: 0.07089214865118265 and parameters: {'observation_period_num': 60, 'train_rates': 0.8522758495262132, 'learning_rate': 0.0006138914780001086, 'batch_size': 107, 'step_size': 11, 'gamma': 0.8351009774938816}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:12:23,192][0m Trial 43 finished with value: 0.03212602060223215 and parameters: {'observation_period_num': 18, 'train_rates': 0.8830084546104436, 'learning_rate': 0.00044775596498199024, 'batch_size': 135, 'step_size': 13, 'gamma': 0.8159829408064917}. Best is trial 32 with value: 0.0319076203797237.[0m
[32m[I 2025-01-07 15:13:07,842][0m Trial 44 finished with value: 0.02959455179246091 and parameters: {'observation_period_num': 19, 'train_rates': 0.8769150642910872, 'learning_rate': 0.0004388346776949093, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8141949405634111}. Best is trial 44 with value: 0.02959455179246091.[0m
[32m[I 2025-01-07 15:14:05,715][0m Trial 45 finished with value: 0.039167222334071994 and parameters: {'observation_period_num': 15, 'train_rates': 0.959243037276523, 'learning_rate': 0.0003891483775184964, 'batch_size': 105, 'step_size': 12, 'gamma': 0.8181355125202652}. Best is trial 44 with value: 0.02959455179246091.[0m
[32m[I 2025-01-07 15:14:46,280][0m Trial 46 finished with value: 0.031864094876629466 and parameters: {'observation_period_num': 16, 'train_rates': 0.8845296914495395, 'learning_rate': 0.00023345642192257015, 'batch_size': 145, 'step_size': 14, 'gamma': 0.7820130600264832}. Best is trial 44 with value: 0.02959455179246091.[0m
[32m[I 2025-01-07 15:15:28,481][0m Trial 47 finished with value: 0.03248772926241251 and parameters: {'observation_period_num': 14, 'train_rates': 0.8888021624206562, 'learning_rate': 0.00026435756755403497, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7812032230444025}. Best is trial 44 with value: 0.02959455179246091.[0m
[32m[I 2025-01-07 15:16:11,365][0m Trial 48 finished with value: 0.03415480172522714 and parameters: {'observation_period_num': 16, 'train_rates': 0.8909107957658985, 'learning_rate': 0.00019643704093881995, 'batch_size': 144, 'step_size': 15, 'gamma': 0.7809094337028534}. Best is trial 44 with value: 0.02959455179246091.[0m
[32m[I 2025-01-07 15:16:48,190][0m Trial 49 finished with value: 0.040987217756302936 and parameters: {'observation_period_num': 6, 'train_rates': 0.8513776109181446, 'learning_rate': 0.00012692007605168138, 'batch_size': 163, 'step_size': 14, 'gamma': 0.7605831224731336}. Best is trial 44 with value: 0.02959455179246091.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 15:16:48,201][0m A new study created in memory with name: no-name-a94d4cfe-1068-455a-928d-45e2c7159e9d[0m
[32m[I 2025-01-07 15:17:21,417][0m Trial 0 finished with value: 0.12061688713846162 and parameters: {'observation_period_num': 164, 'train_rates': 0.7713412990910447, 'learning_rate': 0.0003864825403330471, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8953901619702632}. Best is trial 0 with value: 0.12061688713846162.[0m
[32m[I 2025-01-07 15:17:48,605][0m Trial 1 finished with value: 0.6100119689167373 and parameters: {'observation_period_num': 148, 'train_rates': 0.6660828646273554, 'learning_rate': 1.0216847094379637e-05, 'batch_size': 233, 'step_size': 8, 'gamma': 0.8631696335992731}. Best is trial 0 with value: 0.12061688713846162.[0m
[32m[I 2025-01-07 15:19:39,102][0m Trial 2 finished with value: 0.434894415310451 and parameters: {'observation_period_num': 134, 'train_rates': 0.8137763946378095, 'learning_rate': 1.5366666995323185e-06, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9500239852460908}. Best is trial 0 with value: 0.12061688713846162.[0m
[32m[I 2025-01-07 15:20:04,390][0m Trial 3 finished with value: 0.5514480295833251 and parameters: {'observation_period_num': 128, 'train_rates': 0.6041216967261576, 'learning_rate': 6.65595009941204e-06, 'batch_size': 199, 'step_size': 15, 'gamma': 0.9400227622688528}. Best is trial 0 with value: 0.12061688713846162.[0m
[32m[I 2025-01-07 15:20:34,350][0m Trial 4 finished with value: 0.09096431924720828 and parameters: {'observation_period_num': 54, 'train_rates': 0.8466499448186712, 'learning_rate': 0.00015435174796257062, 'batch_size': 225, 'step_size': 11, 'gamma': 0.917138042505832}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:21:33,058][0m Trial 5 finished with value: 0.3788211454484987 and parameters: {'observation_period_num': 36, 'train_rates': 0.6836140745061635, 'learning_rate': 1.20652234541164e-06, 'batch_size': 82, 'step_size': 11, 'gamma': 0.9838023401137646}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:22:07,034][0m Trial 6 finished with value: 0.23349526367482776 and parameters: {'observation_period_num': 185, 'train_rates': 0.725970494326473, 'learning_rate': 3.5435143002241764e-05, 'batch_size': 166, 'step_size': 14, 'gamma': 0.8703312142019636}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:22:43,243][0m Trial 7 finished with value: 0.19033785164356232 and parameters: {'observation_period_num': 144, 'train_rates': 0.9569449753288346, 'learning_rate': 4.047372408288037e-05, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9163349855998819}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:23:14,317][0m Trial 8 finished with value: 0.13224514448266092 and parameters: {'observation_period_num': 232, 'train_rates': 0.6336433004832813, 'learning_rate': 0.0008138507459223687, 'batch_size': 149, 'step_size': 9, 'gamma': 0.8118505459701656}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:23:44,432][0m Trial 9 finished with value: 0.4248887345842693 and parameters: {'observation_period_num': 10, 'train_rates': 0.8750191351411261, 'learning_rate': 3.572390136181285e-06, 'batch_size': 221, 'step_size': 2, 'gamma': 0.8694167119374416}. Best is trial 4 with value: 0.09096431924720828.[0m
[32m[I 2025-01-07 15:24:47,956][0m Trial 10 finished with value: 0.08625613152980804 and parameters: {'observation_period_num': 83, 'train_rates': 0.9864019707332262, 'learning_rate': 0.00016049001019052985, 'batch_size': 97, 'step_size': 6, 'gamma': 0.7544693181064686}. Best is trial 10 with value: 0.08625613152980804.[0m
[32m[I 2025-01-07 15:25:52,485][0m Trial 11 finished with value: 0.11234503984451294 and parameters: {'observation_period_num': 69, 'train_rates': 0.9886573829013257, 'learning_rate': 0.00015191864673370632, 'batch_size': 96, 'step_size': 4, 'gamma': 0.7612156386788858}. Best is trial 10 with value: 0.08625613152980804.[0m
[32m[I 2025-01-07 15:26:46,597][0m Trial 12 finished with value: 0.07745335731296445 and parameters: {'observation_period_num': 81, 'train_rates': 0.8954890220761201, 'learning_rate': 0.00014965089155367827, 'batch_size': 109, 'step_size': 5, 'gamma': 0.7540529409231984}. Best is trial 12 with value: 0.07745335731296445.[0m
[32m[I 2025-01-07 15:27:46,269][0m Trial 13 finished with value: 0.10752678434054057 and parameters: {'observation_period_num': 91, 'train_rates': 0.9215749439635041, 'learning_rate': 0.00011085399530944167, 'batch_size': 102, 'step_size': 5, 'gamma': 0.7573474102815817}. Best is trial 12 with value: 0.07745335731296445.[0m
Early stopping at epoch 86
[32m[I 2025-01-07 15:32:05,968][0m Trial 14 finished with value: 0.11747139513602556 and parameters: {'observation_period_num': 99, 'train_rates': 0.9053444529216595, 'learning_rate': 7.705605550686346e-05, 'batch_size': 18, 'step_size': 1, 'gamma': 0.8029206061795525}. Best is trial 12 with value: 0.07745335731296445.[0m
[32m[I 2025-01-07 15:32:55,868][0m Trial 15 finished with value: 0.06934667158426729 and parameters: {'observation_period_num': 97, 'train_rates': 0.9446293296256596, 'learning_rate': 0.00041523818444281276, 'batch_size': 122, 'step_size': 5, 'gamma': 0.800094571682092}. Best is trial 15 with value: 0.06934667158426729.[0m
[32m[I 2025-01-07 15:33:37,620][0m Trial 16 finished with value: 0.07782634889537637 and parameters: {'observation_period_num': 115, 'train_rates': 0.9227007118343223, 'learning_rate': 0.0004595477799910717, 'batch_size': 138, 'step_size': 3, 'gamma': 0.8050657257454764}. Best is trial 15 with value: 0.06934667158426729.[0m
[32m[I 2025-01-07 15:35:06,098][0m Trial 17 finished with value: 0.034644739844938786 and parameters: {'observation_period_num': 32, 'train_rates': 0.8570016101941498, 'learning_rate': 0.0003305211368701386, 'batch_size': 63, 'step_size': 5, 'gamma': 0.7847786483061591}. Best is trial 17 with value: 0.034644739844938786.[0m
[32m[I 2025-01-07 15:36:47,942][0m Trial 18 finished with value: 0.02699513493244313 and parameters: {'observation_period_num': 12, 'train_rates': 0.8357436620366449, 'learning_rate': 0.0004045453387011886, 'batch_size': 56, 'step_size': 3, 'gamma': 0.8314533689908394}. Best is trial 18 with value: 0.02699513493244313.[0m
Early stopping at epoch 86
[32m[I 2025-01-07 15:38:11,285][0m Trial 19 finished with value: 0.05034774669145342 and parameters: {'observation_period_num': 6, 'train_rates': 0.7836434134591248, 'learning_rate': 0.0009663812274164088, 'batch_size': 62, 'step_size': 1, 'gamma': 0.8279825848059353}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:43:04,121][0m Trial 20 finished with value: 0.07145554904535317 and parameters: {'observation_period_num': 34, 'train_rates': 0.8388776174848911, 'learning_rate': 1.672909130687161e-05, 'batch_size': 18, 'step_size': 3, 'gamma': 0.8337869742907066}. Best is trial 18 with value: 0.02699513493244313.[0m
Early stopping at epoch 90
[32m[I 2025-01-07 15:44:32,457][0m Trial 21 finished with value: 0.04609103230062035 and parameters: {'observation_period_num': 5, 'train_rates': 0.7690770806392407, 'learning_rate': 0.000994877026277738, 'batch_size': 61, 'step_size': 1, 'gamma': 0.8359127777087421}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:46:12,907][0m Trial 22 finished with value: 0.045828138675350925 and parameters: {'observation_period_num': 30, 'train_rates': 0.7449411735275114, 'learning_rate': 0.00029640028640419414, 'batch_size': 54, 'step_size': 3, 'gamma': 0.7833638948136452}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:48:06,929][0m Trial 23 finished with value: 0.059386422967069606 and parameters: {'observation_period_num': 37, 'train_rates': 0.7316036833989175, 'learning_rate': 0.0002699752986982972, 'batch_size': 47, 'step_size': 3, 'gamma': 0.779310442173282}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:49:21,246][0m Trial 24 finished with value: 0.07509907908028081 and parameters: {'observation_period_num': 49, 'train_rates': 0.8556109499245256, 'learning_rate': 5.8959106026226585e-05, 'batch_size': 74, 'step_size': 4, 'gamma': 0.782159632326551}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:51:11,794][0m Trial 25 finished with value: 0.03799612412965575 and parameters: {'observation_period_num': 25, 'train_rates': 0.8091807732970314, 'learning_rate': 0.0005602775880255282, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8532929756170387}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:53:35,130][0m Trial 26 finished with value: 0.06540310244050587 and parameters: {'observation_period_num': 61, 'train_rates': 0.8223005808751951, 'learning_rate': 0.0005873193336299105, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8494011631095872}. Best is trial 18 with value: 0.02699513493244313.[0m
[32m[I 2025-01-07 15:54:56,372][0m Trial 27 finished with value: 0.024468637808846932 and parameters: {'observation_period_num': 15, 'train_rates': 0.8042050780175394, 'learning_rate': 0.0002489861450357806, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8878208985881009}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 15:56:10,210][0m Trial 28 finished with value: 0.15823838524400613 and parameters: {'observation_period_num': 251, 'train_rates': 0.8852565545415514, 'learning_rate': 0.00021340884830310956, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8861511290014591}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 15:57:05,612][0m Trial 29 finished with value: 0.14127536314922767 and parameters: {'observation_period_num': 188, 'train_rates': 0.7591358453724492, 'learning_rate': 9.161877063082585e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9025722402444198}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:00:14,742][0m Trial 30 finished with value: 0.03733941626136865 and parameters: {'observation_period_num': 20, 'train_rates': 0.7905797184667576, 'learning_rate': 5.8828977915852435e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8199414951553671}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:03:03,181][0m Trial 31 finished with value: 0.04914365093345228 and parameters: {'observation_period_num': 19, 'train_rates': 0.7885012106220564, 'learning_rate': 2.0544904864050386e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8263427031520966}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:06:21,908][0m Trial 32 finished with value: 0.10897494114434986 and parameters: {'observation_period_num': 47, 'train_rates': 0.8620095092751316, 'learning_rate': 0.00025951394294807315, 'batch_size': 27, 'step_size': 7, 'gamma': 0.8207639314981138}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:07:48,018][0m Trial 33 finished with value: 0.05404211996582323 and parameters: {'observation_period_num': 21, 'train_rates': 0.8301292482698006, 'learning_rate': 5.424831270565623e-05, 'batch_size': 70, 'step_size': 4, 'gamma': 0.8503940407612269}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:10:15,373][0m Trial 34 finished with value: 0.05128540307030839 and parameters: {'observation_period_num': 46, 'train_rates': 0.7971390293823397, 'learning_rate': 0.00035168588671837583, 'batch_size': 36, 'step_size': 9, 'gamma': 0.8858097474765044}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:10:48,700][0m Trial 35 finished with value: 0.2808202274790093 and parameters: {'observation_period_num': 70, 'train_rates': 0.696495352027547, 'learning_rate': 2.3745388876827594e-05, 'batch_size': 179, 'step_size': 7, 'gamma': 0.7866427653669059}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:12:17,827][0m Trial 36 finished with value: 0.029580150696006283 and parameters: {'observation_period_num': 18, 'train_rates': 0.8274383231582607, 'learning_rate': 0.0002237875667244541, 'batch_size': 62, 'step_size': 6, 'gamma': 0.9500077441478129}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:13:32,370][0m Trial 37 finished with value: 0.09671664788843469 and parameters: {'observation_period_num': 61, 'train_rates': 0.8702910006928347, 'learning_rate': 0.0006492692426786919, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9557490474296524}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:14:58,402][0m Trial 38 finished with value: 0.047944082455201584 and parameters: {'observation_period_num': 42, 'train_rates': 0.8186908415039167, 'learning_rate': 0.0002181931559368453, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9654272292107461}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:16:36,146][0m Trial 39 finished with value: 0.08915139991661598 and parameters: {'observation_period_num': 170, 'train_rates': 0.8437882715571552, 'learning_rate': 0.0004283774573506001, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9196171148874033}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:17:27,731][0m Trial 40 finished with value: 0.12493350741589013 and parameters: {'observation_period_num': 15, 'train_rates': 0.7100710427896204, 'learning_rate': 5.7831752873042015e-06, 'batch_size': 116, 'step_size': 8, 'gamma': 0.9867489872688778}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:19:39,332][0m Trial 41 finished with value: 0.03323901320911807 and parameters: {'observation_period_num': 27, 'train_rates': 0.7955757746687535, 'learning_rate': 0.00011482663310583096, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9392656734579398}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:20:08,325][0m Trial 42 finished with value: 0.04821642689246068 and parameters: {'observation_period_num': 26, 'train_rates': 0.7628272812964622, 'learning_rate': 0.00011289799483605144, 'batch_size': 250, 'step_size': 6, 'gamma': 0.9414821981459635}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:22:03,624][0m Trial 43 finished with value: 0.027371610514819622 and parameters: {'observation_period_num': 11, 'train_rates': 0.8178740519850478, 'learning_rate': 0.00020166302287002338, 'batch_size': 46, 'step_size': 8, 'gamma': 0.9302814077071044}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:23:52,555][0m Trial 44 finished with value: 0.025427110251551335 and parameters: {'observation_period_num': 7, 'train_rates': 0.8027807564118562, 'learning_rate': 0.00015708162290260344, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9282400695202643}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:25:10,990][0m Trial 45 finished with value: 0.026072685382093952 and parameters: {'observation_period_num': 5, 'train_rates': 0.8159013708552539, 'learning_rate': 0.00019357921003901996, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9234966600729205}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:26:22,855][0m Trial 46 finished with value: 0.025310130225955893 and parameters: {'observation_period_num': 6, 'train_rates': 0.8151256710202026, 'learning_rate': 0.00014930187807086457, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9273458146809014}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:27:17,918][0m Trial 47 finished with value: 0.03129686000341883 and parameters: {'observation_period_num': 6, 'train_rates': 0.6538785006494551, 'learning_rate': 0.0001442609416636432, 'batch_size': 92, 'step_size': 11, 'gamma': 0.9076967557242379}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:28:22,806][0m Trial 48 finished with value: 0.07182564194131967 and parameters: {'observation_period_num': 56, 'train_rates': 0.7459803699621987, 'learning_rate': 8.326183979091499e-05, 'batch_size': 81, 'step_size': 10, 'gamma': 0.8890729553167649}. Best is trial 27 with value: 0.024468637808846932.[0m
[32m[I 2025-01-07 16:29:21,978][0m Trial 49 finished with value: 0.11252138325396706 and parameters: {'observation_period_num': 206, 'train_rates': 0.8023244806643398, 'learning_rate': 0.00016934873148087082, 'batch_size': 132, 'step_size': 14, 'gamma': 0.8766208575779432}. Best is trial 27 with value: 0.024468637808846932.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 16:29:21,992][0m A new study created in memory with name: no-name-e3cc5457-cc7a-4154-8876-5606aa04fc33[0m
[32m[I 2025-01-07 16:30:05,287][0m Trial 0 finished with value: 0.20622532427928608 and parameters: {'observation_period_num': 124, 'train_rates': 0.6590404442253458, 'learning_rate': 2.040192187005853e-05, 'batch_size': 194, 'step_size': 13, 'gamma': 0.9836043013508295}. Best is trial 0 with value: 0.20622532427928608.[0m
[32m[I 2025-01-07 16:31:55,276][0m Trial 1 finished with value: 0.4621746807407761 and parameters: {'observation_period_num': 68, 'train_rates': 0.7543069360728134, 'learning_rate': 1.3825757347486966e-06, 'batch_size': 48, 'step_size': 6, 'gamma': 0.959253467860484}. Best is trial 0 with value: 0.20622532427928608.[0m
[32m[I 2025-01-07 16:32:39,560][0m Trial 2 finished with value: 0.11022447342179832 and parameters: {'observation_period_num': 157, 'train_rates': 0.8636084543126078, 'learning_rate': 0.0001629185253977867, 'batch_size': 150, 'step_size': 8, 'gamma': 0.7726920548131011}. Best is trial 2 with value: 0.11022447342179832.[0m
[32m[I 2025-01-07 16:33:26,032][0m Trial 3 finished with value: 0.25691859441853704 and parameters: {'observation_period_num': 184, 'train_rates': 0.8483394930294154, 'learning_rate': 9.533293980303468e-06, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9467767145875585}. Best is trial 2 with value: 0.11022447342179832.[0m
[32m[I 2025-01-07 16:34:01,379][0m Trial 4 finished with value: 0.06856133507670097 and parameters: {'observation_period_num': 103, 'train_rates': 0.8143342678678517, 'learning_rate': 0.00045382688489974896, 'batch_size': 220, 'step_size': 9, 'gamma': 0.8195117429420552}. Best is trial 4 with value: 0.06856133507670097.[0m
[32m[I 2025-01-07 16:34:42,101][0m Trial 5 finished with value: 0.06720663351270388 and parameters: {'observation_period_num': 93, 'train_rates': 0.9074632561996834, 'learning_rate': 0.0008557541128631902, 'batch_size': 149, 'step_size': 4, 'gamma': 0.7769511379087346}. Best is trial 5 with value: 0.06720663351270388.[0m
[32m[I 2025-01-07 16:35:15,296][0m Trial 6 finished with value: 1.5105210542678833 and parameters: {'observation_period_num': 244, 'train_rates': 0.9292028944218879, 'learning_rate': 1.8371587061170753e-06, 'batch_size': 251, 'step_size': 9, 'gamma': 0.7822484645922047}. Best is trial 5 with value: 0.06720663351270388.[0m
[32m[I 2025-01-07 16:36:19,303][0m Trial 7 finished with value: 0.1530836580181413 and parameters: {'observation_period_num': 73, 'train_rates': 0.6021428981539494, 'learning_rate': 0.00012916562462068155, 'batch_size': 68, 'step_size': 14, 'gamma': 0.7524519279063842}. Best is trial 5 with value: 0.06720663351270388.[0m
[32m[I 2025-01-07 16:36:55,795][0m Trial 8 finished with value: 0.8875280182174697 and parameters: {'observation_period_num': 114, 'train_rates': 0.8161009933435749, 'learning_rate': 1.9429623544471515e-06, 'batch_size': 223, 'step_size': 11, 'gamma': 0.823031996099285}. Best is trial 5 with value: 0.06720663351270388.[0m
[32m[I 2025-01-07 16:37:32,821][0m Trial 9 finished with value: 0.30021216009631224 and parameters: {'observation_period_num': 62, 'train_rates': 0.805198111066169, 'learning_rate': 4.077460891247971e-05, 'batch_size': 203, 'step_size': 2, 'gamma': 0.818368080863886}. Best is trial 5 with value: 0.06720663351270388.[0m
[32m[I 2025-01-07 16:38:38,066][0m Trial 10 finished with value: 0.035223569720983505 and parameters: {'observation_period_num': 21, 'train_rates': 0.9715675430410117, 'learning_rate': 0.000655913624855929, 'batch_size': 97, 'step_size': 2, 'gamma': 0.8856488488811688}. Best is trial 10 with value: 0.035223569720983505.[0m
[32m[I 2025-01-07 16:39:46,006][0m Trial 11 finished with value: 0.03855940327048302 and parameters: {'observation_period_num': 6, 'train_rates': 0.9834687926320799, 'learning_rate': 0.0008983691190197661, 'batch_size': 94, 'step_size': 2, 'gamma': 0.8885907902406676}. Best is trial 10 with value: 0.035223569720983505.[0m
[32m[I 2025-01-07 16:40:53,093][0m Trial 12 finished with value: 0.06059783697128296 and parameters: {'observation_period_num': 14, 'train_rates': 0.9834625009362307, 'learning_rate': 0.0002980120685754137, 'batch_size': 97, 'step_size': 1, 'gamma': 0.8969767440616243}. Best is trial 10 with value: 0.035223569720983505.[0m
[32m[I 2025-01-07 16:41:53,193][0m Trial 13 finished with value: 0.029635684564709663 and parameters: {'observation_period_num': 5, 'train_rates': 0.979025897955739, 'learning_rate': 0.0009616859319551283, 'batch_size': 106, 'step_size': 4, 'gamma': 0.8937769318988896}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:42:49,474][0m Trial 14 finished with value: 0.05177947177894806 and parameters: {'observation_period_num': 37, 'train_rates': 0.922066013788645, 'learning_rate': 8.273852429320227e-05, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9125384919503357}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:46:27,428][0m Trial 15 finished with value: 0.07152534949388283 and parameters: {'observation_period_num': 38, 'train_rates': 0.7336348767199575, 'learning_rate': 0.000336909297642236, 'batch_size': 22, 'step_size': 4, 'gamma': 0.851661656384349}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:47:25,330][0m Trial 16 finished with value: 0.29831247742359457 and parameters: {'observation_period_num': 34, 'train_rates': 0.9554759549071202, 'learning_rate': 5.711912660889007e-06, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8622663650215466}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:48:46,523][0m Trial 17 finished with value: 0.09612050501575807 and parameters: {'observation_period_num': 159, 'train_rates': 0.888866103203004, 'learning_rate': 5.525998926417906e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9277058862929759}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:49:34,351][0m Trial 18 finished with value: 0.23368331789970398 and parameters: {'observation_period_num': 234, 'train_rates': 0.9602146170454079, 'learning_rate': 0.0005061493107918335, 'batch_size': 127, 'step_size': 1, 'gamma': 0.8798475180328313}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:50:07,446][0m Trial 19 finished with value: 0.04234148790033496 and parameters: {'observation_period_num': 8, 'train_rates': 0.7505142330073274, 'learning_rate': 0.00017752840347919457, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9198697967976199}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:51:23,826][0m Trial 20 finished with value: 0.050296264427496976 and parameters: {'observation_period_num': 50, 'train_rates': 0.8628014084381478, 'learning_rate': 0.0009640889581086485, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8467751230203723}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:52:29,162][0m Trial 21 finished with value: 0.03504585102200508 and parameters: {'observation_period_num': 6, 'train_rates': 0.9707949148846741, 'learning_rate': 0.00088482014915277, 'batch_size': 98, 'step_size': 2, 'gamma': 0.8875778681900968}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:53:28,395][0m Trial 22 finished with value: 0.06614149410343735 and parameters: {'observation_period_num': 24, 'train_rates': 0.9425749083938385, 'learning_rate': 0.0005315734272341794, 'batch_size': 108, 'step_size': 1, 'gamma': 0.9016475498461695}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:54:44,851][0m Trial 23 finished with value: 0.0802641436457634 and parameters: {'observation_period_num': 85, 'train_rates': 0.9870090188591899, 'learning_rate': 0.0002530068431034488, 'batch_size': 81, 'step_size': 3, 'gamma': 0.870912280937963}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:56:57,210][0m Trial 24 finished with value: 0.07917308397462211 and parameters: {'observation_period_num': 48, 'train_rates': 0.9005692866999988, 'learning_rate': 0.0006033905869063379, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9382113365346018}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:57:51,964][0m Trial 25 finished with value: 0.06687947155176839 and parameters: {'observation_period_num': 6, 'train_rates': 0.9464372617392772, 'learning_rate': 0.0002480041401278084, 'batch_size': 126, 'step_size': 2, 'gamma': 0.8384443024364912}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 16:58:43,481][0m Trial 26 finished with value: 0.052093300968408585 and parameters: {'observation_period_num': 27, 'train_rates': 0.8844910262356085, 'learning_rate': 9.513150316748246e-05, 'batch_size': 114, 'step_size': 4, 'gamma': 0.8739423013539097}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:00:49,489][0m Trial 27 finished with value: 0.09029617408911388 and parameters: {'observation_period_num': 51, 'train_rates': 0.961023267592316, 'learning_rate': 2.1086733042168332e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9024623645041262}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:02:03,536][0m Trial 28 finished with value: 0.03601789873262698 and parameters: {'observation_period_num': 19, 'train_rates': 0.9252456672428109, 'learning_rate': 0.0009474203379399936, 'batch_size': 83, 'step_size': 3, 'gamma': 0.8890216330177227}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:02:42,845][0m Trial 29 finished with value: 0.20201388535020653 and parameters: {'observation_period_num': 80, 'train_rates': 0.8407536657087781, 'learning_rate': 0.00039137631133735854, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9819636582151524}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:03:23,323][0m Trial 30 finished with value: 0.5053088829593427 and parameters: {'observation_period_num': 136, 'train_rates': 0.6774280077243493, 'learning_rate': 9.181878742386437e-06, 'batch_size': 136, 'step_size': 2, 'gamma': 0.9638022299081617}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:04:30,776][0m Trial 31 finished with value: 0.03951138840988278 and parameters: {'observation_period_num': 25, 'train_rates': 0.9317121515468497, 'learning_rate': 0.0006815980078414154, 'batch_size': 91, 'step_size': 3, 'gamma': 0.8857599785179218}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:05:44,385][0m Trial 32 finished with value: 0.036960162222385406 and parameters: {'observation_period_num': 58, 'train_rates': 0.9896512375556188, 'learning_rate': 0.0009995535528342025, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8604838462091028}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:07:34,277][0m Trial 33 finished with value: 0.03761604506718485 and parameters: {'observation_period_num': 21, 'train_rates': 0.9611948715752012, 'learning_rate': 0.0005984960558262152, 'batch_size': 54, 'step_size': 1, 'gamma': 0.9160350914132553}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:08:27,167][0m Trial 34 finished with value: 0.08775397866695851 and parameters: {'observation_period_num': 216, 'train_rates': 0.9189722628064567, 'learning_rate': 0.0002026814018466336, 'batch_size': 106, 'step_size': 4, 'gamma': 0.9371297224985424}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:11:05,724][0m Trial 35 finished with value: 0.06695978492902223 and parameters: {'observation_period_num': 39, 'train_rates': 0.8736783976313122, 'learning_rate': 0.00037743198839222994, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8932849983331125}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:11:54,716][0m Trial 36 finished with value: 0.031577520072460175 and parameters: {'observation_period_num': 17, 'train_rates': 0.9676445725049155, 'learning_rate': 0.000693914775491298, 'batch_size': 138, 'step_size': 2, 'gamma': 0.9553358664657318}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:12:43,895][0m Trial 37 finished with value: 0.05350456014275551 and parameters: {'observation_period_num': 5, 'train_rates': 0.9692953207468671, 'learning_rate': 0.00013139751772243245, 'batch_size': 152, 'step_size': 2, 'gamma': 0.968382963694409}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:13:28,518][0m Trial 38 finished with value: 0.05923468660977152 and parameters: {'observation_period_num': 71, 'train_rates': 0.9063324167646021, 'learning_rate': 0.0006603177867627823, 'batch_size': 138, 'step_size': 1, 'gamma': 0.9456023861982004}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:14:06,128][0m Trial 39 finished with value: 0.1070662374799337 and parameters: {'observation_period_num': 95, 'train_rates': 0.785479793289046, 'learning_rate': 0.00042854259127774374, 'batch_size': 168, 'step_size': 10, 'gamma': 0.9869041718184148}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:14:49,944][0m Trial 40 finished with value: 0.11113634043854735 and parameters: {'observation_period_num': 134, 'train_rates': 0.6988557669630941, 'learning_rate': 0.00027766091039076966, 'batch_size': 140, 'step_size': 6, 'gamma': 0.8060906602116483}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:15:47,694][0m Trial 41 finished with value: 0.03813056158833206 and parameters: {'observation_period_num': 19, 'train_rates': 0.9401259582768159, 'learning_rate': 0.0006999731857986747, 'batch_size': 118, 'step_size': 3, 'gamma': 0.9062315369309244}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:17:31,158][0m Trial 42 finished with value: 0.04270271426614593 and parameters: {'observation_period_num': 31, 'train_rates': 0.9711595335162265, 'learning_rate': 0.0009937551152737448, 'batch_size': 61, 'step_size': 4, 'gamma': 0.9250615012480863}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:18:48,594][0m Trial 43 finished with value: 0.0392505265237612 and parameters: {'observation_period_num': 14, 'train_rates': 0.929075452572578, 'learning_rate': 0.0004870271444301491, 'batch_size': 83, 'step_size': 2, 'gamma': 0.8814276800615773}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:19:59,653][0m Trial 44 finished with value: 0.044637590646743774 and parameters: {'observation_period_num': 43, 'train_rates': 0.9698221085217954, 'learning_rate': 0.000739705811253532, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8636305894810837}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:21:03,826][0m Trial 45 finished with value: 0.07298512377247617 and parameters: {'observation_period_num': 17, 'train_rates': 0.6222703164166912, 'learning_rate': 0.00037142231228348477, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8382491667170285}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:22:08,232][0m Trial 46 finished with value: 0.06018447554111481 and parameters: {'observation_period_num': 61, 'train_rates': 0.9136071287938312, 'learning_rate': 0.0008432476536597332, 'batch_size': 102, 'step_size': 2, 'gamma': 0.9524701728626956}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:23:24,197][0m Trial 47 finished with value: 0.6888581618185966 and parameters: {'observation_period_num': 27, 'train_rates': 0.9470672192984045, 'learning_rate': 1.1216666335952583e-06, 'batch_size': 89, 'step_size': 7, 'gamma': 0.8887078358119683}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:24:18,196][0m Trial 48 finished with value: 0.08634145557880402 and parameters: {'observation_period_num': 117, 'train_rates': 0.975757032584733, 'learning_rate': 0.00021153253618132222, 'batch_size': 158, 'step_size': 4, 'gamma': 0.9102705903790015}. Best is trial 13 with value: 0.029635684564709663.[0m
[32m[I 2025-01-07 17:25:11,510][0m Trial 49 finished with value: 0.18144805169030398 and parameters: {'observation_period_num': 186, 'train_rates': 0.8281562703024716, 'learning_rate': 0.00012858365876303646, 'batch_size': 115, 'step_size': 1, 'gamma': 0.9322561397870791}. Best is trial 13 with value: 0.029635684564709663.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8892589918714747, 'learning_rate': 0.00020995956511775144, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8981119351193323}
Epoch 1/300, trend Loss: 0.3621 | 0.2420
Epoch 2/300, trend Loss: 0.1502 | 0.1345
Epoch 3/300, trend Loss: 0.1330 | 0.1020
Epoch 4/300, trend Loss: 0.1221 | 0.0832
Epoch 5/300, trend Loss: 0.1267 | 0.0949
Epoch 6/300, trend Loss: 0.1308 | 0.1100
Epoch 7/300, trend Loss: 0.1293 | 0.1266
Epoch 8/300, trend Loss: 0.1187 | 0.0998
Epoch 9/300, trend Loss: 0.1090 | 0.0689
Epoch 10/300, trend Loss: 0.1159 | 0.0795
Epoch 11/300, trend Loss: 0.1231 | 0.0894
Epoch 12/300, trend Loss: 0.1119 | 0.0791
Epoch 13/300, trend Loss: 0.1016 | 0.0860
Epoch 14/300, trend Loss: 0.0984 | 0.0739
Epoch 15/300, trend Loss: 0.0946 | 0.0675
Epoch 16/300, trend Loss: 0.0925 | 0.0631
Epoch 17/300, trend Loss: 0.0912 | 0.0595
Epoch 18/300, trend Loss: 0.0902 | 0.0569
Epoch 19/300, trend Loss: 0.0895 | 0.0543
Epoch 20/300, trend Loss: 0.0889 | 0.0528
Epoch 21/300, trend Loss: 0.0884 | 0.0513
Epoch 22/300, trend Loss: 0.0878 | 0.0506
Epoch 23/300, trend Loss: 0.0873 | 0.0498
Epoch 24/300, trend Loss: 0.0867 | 0.0492
Epoch 25/300, trend Loss: 0.0861 | 0.0486
Epoch 26/300, trend Loss: 0.0855 | 0.0481
Epoch 27/300, trend Loss: 0.0849 | 0.0476
Epoch 28/300, trend Loss: 0.0844 | 0.0471
Epoch 29/300, trend Loss: 0.0839 | 0.0467
Epoch 30/300, trend Loss: 0.0835 | 0.0462
Epoch 31/300, trend Loss: 0.0830 | 0.0458
Epoch 32/300, trend Loss: 0.0826 | 0.0455
Epoch 33/300, trend Loss: 0.0823 | 0.0451
Epoch 34/300, trend Loss: 0.0819 | 0.0448
Epoch 35/300, trend Loss: 0.0816 | 0.0445
Epoch 36/300, trend Loss: 0.0813 | 0.0442
Epoch 37/300, trend Loss: 0.0810 | 0.0439
Epoch 38/300, trend Loss: 0.0808 | 0.0436
Epoch 39/300, trend Loss: 0.0805 | 0.0434
Epoch 40/300, trend Loss: 0.0803 | 0.0431
Epoch 41/300, trend Loss: 0.0801 | 0.0428
Epoch 42/300, trend Loss: 0.0799 | 0.0426
Epoch 43/300, trend Loss: 0.0796 | 0.0423
Epoch 44/300, trend Loss: 0.0794 | 0.0421
Epoch 45/300, trend Loss: 0.0792 | 0.0419
Epoch 46/300, trend Loss: 0.0790 | 0.0417
Epoch 47/300, trend Loss: 0.0788 | 0.0415
Epoch 48/300, trend Loss: 0.0786 | 0.0413
Epoch 49/300, trend Loss: 0.0784 | 0.0411
Epoch 50/300, trend Loss: 0.0782 | 0.0409
Epoch 51/300, trend Loss: 0.0780 | 0.0408
Epoch 52/300, trend Loss: 0.0778 | 0.0406
Epoch 53/300, trend Loss: 0.0776 | 0.0405
Epoch 54/300, trend Loss: 0.0775 | 0.0403
Epoch 55/300, trend Loss: 0.0773 | 0.0402
Epoch 56/300, trend Loss: 0.0772 | 0.0400
Epoch 57/300, trend Loss: 0.0771 | 0.0399
Epoch 58/300, trend Loss: 0.0769 | 0.0398
Epoch 59/300, trend Loss: 0.0768 | 0.0396
Epoch 60/300, trend Loss: 0.0767 | 0.0395
Epoch 61/300, trend Loss: 0.0766 | 0.0394
Epoch 62/300, trend Loss: 0.0765 | 0.0393
Epoch 63/300, trend Loss: 0.0764 | 0.0392
Epoch 64/300, trend Loss: 0.0763 | 0.0391
Epoch 65/300, trend Loss: 0.0762 | 0.0390
Epoch 66/300, trend Loss: 0.0761 | 0.0389
Epoch 67/300, trend Loss: 0.0760 | 0.0388
Epoch 68/300, trend Loss: 0.0760 | 0.0388
Epoch 69/300, trend Loss: 0.0759 | 0.0387
Epoch 70/300, trend Loss: 0.0758 | 0.0386
Epoch 71/300, trend Loss: 0.0758 | 0.0386
Epoch 72/300, trend Loss: 0.0757 | 0.0385
Epoch 73/300, trend Loss: 0.0757 | 0.0384
Epoch 74/300, trend Loss: 0.0756 | 0.0384
Epoch 75/300, trend Loss: 0.0756 | 0.0383
Epoch 76/300, trend Loss: 0.0755 | 0.0383
Epoch 77/300, trend Loss: 0.0755 | 0.0382
Epoch 78/300, trend Loss: 0.0754 | 0.0382
Epoch 79/300, trend Loss: 0.0754 | 0.0381
Epoch 80/300, trend Loss: 0.0753 | 0.0381
Epoch 81/300, trend Loss: 0.0753 | 0.0381
Epoch 82/300, trend Loss: 0.0753 | 0.0380
Epoch 83/300, trend Loss: 0.0752 | 0.0380
Epoch 84/300, trend Loss: 0.0752 | 0.0380
Epoch 85/300, trend Loss: 0.0752 | 0.0379
Epoch 86/300, trend Loss: 0.0752 | 0.0379
Epoch 87/300, trend Loss: 0.0751 | 0.0379
Epoch 88/300, trend Loss: 0.0751 | 0.0378
Epoch 89/300, trend Loss: 0.0751 | 0.0378
Epoch 90/300, trend Loss: 0.0751 | 0.0378
Epoch 91/300, trend Loss: 0.0750 | 0.0378
Epoch 92/300, trend Loss: 0.0750 | 0.0378
Epoch 93/300, trend Loss: 0.0750 | 0.0377
Epoch 94/300, trend Loss: 0.0750 | 0.0377
Epoch 95/300, trend Loss: 0.0750 | 0.0377
Epoch 96/300, trend Loss: 0.0749 | 0.0377
Epoch 97/300, trend Loss: 0.0749 | 0.0377
Epoch 98/300, trend Loss: 0.0749 | 0.0376
Epoch 99/300, trend Loss: 0.0749 | 0.0376
Epoch 100/300, trend Loss: 0.0749 | 0.0376
Epoch 101/300, trend Loss: 0.0749 | 0.0376
Epoch 102/300, trend Loss: 0.0749 | 0.0376
Epoch 103/300, trend Loss: 0.0748 | 0.0376
Epoch 104/300, trend Loss: 0.0748 | 0.0376
Epoch 105/300, trend Loss: 0.0748 | 0.0376
Epoch 106/300, trend Loss: 0.0748 | 0.0375
Epoch 107/300, trend Loss: 0.0748 | 0.0375
Epoch 108/300, trend Loss: 0.0748 | 0.0375
Epoch 109/300, trend Loss: 0.0748 | 0.0375
Epoch 110/300, trend Loss: 0.0748 | 0.0375
Epoch 111/300, trend Loss: 0.0748 | 0.0375
Epoch 112/300, trend Loss: 0.0748 | 0.0375
Epoch 113/300, trend Loss: 0.0748 | 0.0375
Epoch 114/300, trend Loss: 0.0747 | 0.0375
Epoch 115/300, trend Loss: 0.0747 | 0.0375
Epoch 116/300, trend Loss: 0.0747 | 0.0375
Epoch 117/300, trend Loss: 0.0747 | 0.0375
Epoch 118/300, trend Loss: 0.0747 | 0.0374
Epoch 119/300, trend Loss: 0.0747 | 0.0374
Epoch 120/300, trend Loss: 0.0747 | 0.0374
Epoch 121/300, trend Loss: 0.0747 | 0.0374
Epoch 122/300, trend Loss: 0.0747 | 0.0374
Epoch 123/300, trend Loss: 0.0747 | 0.0374
Epoch 124/300, trend Loss: 0.0747 | 0.0374
Epoch 125/300, trend Loss: 0.0747 | 0.0374
Epoch 126/300, trend Loss: 0.0747 | 0.0374
Epoch 127/300, trend Loss: 0.0747 | 0.0374
Epoch 128/300, trend Loss: 0.0747 | 0.0374
Epoch 129/300, trend Loss: 0.0747 | 0.0374
Epoch 130/300, trend Loss: 0.0747 | 0.0374
Epoch 131/300, trend Loss: 0.0747 | 0.0374
Epoch 132/300, trend Loss: 0.0747 | 0.0374
Epoch 133/300, trend Loss: 0.0747 | 0.0374
Epoch 134/300, trend Loss: 0.0747 | 0.0374
Epoch 135/300, trend Loss: 0.0747 | 0.0374
Epoch 136/300, trend Loss: 0.0747 | 0.0374
Epoch 137/300, trend Loss: 0.0747 | 0.0374
Epoch 138/300, trend Loss: 0.0747 | 0.0374
Epoch 139/300, trend Loss: 0.0747 | 0.0374
Epoch 140/300, trend Loss: 0.0746 | 0.0374
Epoch 141/300, trend Loss: 0.0746 | 0.0374
Epoch 142/300, trend Loss: 0.0746 | 0.0374
Epoch 143/300, trend Loss: 0.0746 | 0.0374
Epoch 144/300, trend Loss: 0.0746 | 0.0374
Epoch 145/300, trend Loss: 0.0746 | 0.0374
Epoch 146/300, trend Loss: 0.0746 | 0.0374
Epoch 147/300, trend Loss: 0.0746 | 0.0374
Epoch 148/300, trend Loss: 0.0746 | 0.0374
Epoch 149/300, trend Loss: 0.0746 | 0.0374
Epoch 150/300, trend Loss: 0.0746 | 0.0374
Epoch 151/300, trend Loss: 0.0746 | 0.0374
Epoch 152/300, trend Loss: 0.0746 | 0.0374
Epoch 153/300, trend Loss: 0.0746 | 0.0374
Epoch 154/300, trend Loss: 0.0746 | 0.0374
Epoch 155/300, trend Loss: 0.0746 | 0.0373
Epoch 156/300, trend Loss: 0.0746 | 0.0373
Epoch 157/300, trend Loss: 0.0746 | 0.0373
Epoch 158/300, trend Loss: 0.0746 | 0.0373
Epoch 159/300, trend Loss: 0.0746 | 0.0373
Epoch 160/300, trend Loss: 0.0746 | 0.0373
Epoch 161/300, trend Loss: 0.0746 | 0.0373
Epoch 162/300, trend Loss: 0.0746 | 0.0373
Epoch 163/300, trend Loss: 0.0746 | 0.0373
Epoch 164/300, trend Loss: 0.0746 | 0.0373
Epoch 165/300, trend Loss: 0.0746 | 0.0373
Epoch 166/300, trend Loss: 0.0746 | 0.0373
Epoch 167/300, trend Loss: 0.0746 | 0.0373
Epoch 168/300, trend Loss: 0.0746 | 0.0373
Epoch 169/300, trend Loss: 0.0746 | 0.0373
Epoch 170/300, trend Loss: 0.0746 | 0.0373
Epoch 171/300, trend Loss: 0.0746 | 0.0373
Epoch 172/300, trend Loss: 0.0746 | 0.0373
Epoch 173/300, trend Loss: 0.0746 | 0.0373
Epoch 174/300, trend Loss: 0.0746 | 0.0373
Epoch 175/300, trend Loss: 0.0746 | 0.0373
Epoch 176/300, trend Loss: 0.0746 | 0.0373
Epoch 177/300, trend Loss: 0.0746 | 0.0373
Epoch 178/300, trend Loss: 0.0746 | 0.0373
Epoch 179/300, trend Loss: 0.0746 | 0.0373
Epoch 180/300, trend Loss: 0.0746 | 0.0373
Epoch 181/300, trend Loss: 0.0746 | 0.0373
Epoch 182/300, trend Loss: 0.0746 | 0.0373
Epoch 183/300, trend Loss: 0.0746 | 0.0373
Epoch 184/300, trend Loss: 0.0746 | 0.0373
Epoch 185/300, trend Loss: 0.0746 | 0.0373
Epoch 186/300, trend Loss: 0.0746 | 0.0373
Epoch 187/300, trend Loss: 0.0746 | 0.0373
Epoch 188/300, trend Loss: 0.0746 | 0.0373
Epoch 189/300, trend Loss: 0.0746 | 0.0373
Epoch 190/300, trend Loss: 0.0746 | 0.0373
Epoch 191/300, trend Loss: 0.0746 | 0.0373
Epoch 192/300, trend Loss: 0.0746 | 0.0373
Epoch 193/300, trend Loss: 0.0746 | 0.0373
Epoch 194/300, trend Loss: 0.0746 | 0.0373
Epoch 195/300, trend Loss: 0.0746 | 0.0373
Epoch 196/300, trend Loss: 0.0746 | 0.0373
Epoch 197/300, trend Loss: 0.0746 | 0.0373
Epoch 198/300, trend Loss: 0.0746 | 0.0373
Epoch 199/300, trend Loss: 0.0746 | 0.0373
Epoch 200/300, trend Loss: 0.0746 | 0.0373
Epoch 201/300, trend Loss: 0.0746 | 0.0373
Epoch 202/300, trend Loss: 0.0746 | 0.0373
Epoch 203/300, trend Loss: 0.0746 | 0.0373
Epoch 204/300, trend Loss: 0.0746 | 0.0373
Epoch 205/300, trend Loss: 0.0746 | 0.0373
Epoch 206/300, trend Loss: 0.0746 | 0.0373
Epoch 207/300, trend Loss: 0.0746 | 0.0373
Epoch 208/300, trend Loss: 0.0746 | 0.0373
Epoch 209/300, trend Loss: 0.0746 | 0.0373
Epoch 210/300, trend Loss: 0.0746 | 0.0373
Epoch 211/300, trend Loss: 0.0746 | 0.0373
Epoch 212/300, trend Loss: 0.0746 | 0.0373
Epoch 213/300, trend Loss: 0.0746 | 0.0373
Epoch 214/300, trend Loss: 0.0746 | 0.0373
Epoch 215/300, trend Loss: 0.0746 | 0.0373
Epoch 216/300, trend Loss: 0.0746 | 0.0373
Epoch 217/300, trend Loss: 0.0746 | 0.0373
Epoch 218/300, trend Loss: 0.0746 | 0.0373
Epoch 219/300, trend Loss: 0.0746 | 0.0373
Epoch 220/300, trend Loss: 0.0746 | 0.0373
Epoch 221/300, trend Loss: 0.0746 | 0.0373
Epoch 222/300, trend Loss: 0.0746 | 0.0373
Epoch 223/300, trend Loss: 0.0746 | 0.0373
Epoch 224/300, trend Loss: 0.0746 | 0.0373
Epoch 225/300, trend Loss: 0.0746 | 0.0373
Epoch 226/300, trend Loss: 0.0746 | 0.0373
Epoch 227/300, trend Loss: 0.0746 | 0.0373
Epoch 228/300, trend Loss: 0.0746 | 0.0373
Epoch 229/300, trend Loss: 0.0746 | 0.0373
Epoch 230/300, trend Loss: 0.0746 | 0.0373
Epoch 231/300, trend Loss: 0.0746 | 0.0373
Epoch 232/300, trend Loss: 0.0746 | 0.0373
Epoch 233/300, trend Loss: 0.0746 | 0.0373
Epoch 234/300, trend Loss: 0.0746 | 0.0373
Epoch 235/300, trend Loss: 0.0746 | 0.0373
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 22, 'train_rates': 0.9877648868604361, 'learning_rate': 0.0002611511446315947, 'batch_size': 29, 'step_size': 8, 'gamma': 0.9263027157541567}
Epoch 1/300, seasonal_0 Loss: 0.1822 | 0.1329
Epoch 2/300, seasonal_0 Loss: 0.1277 | 0.1046
Epoch 3/300, seasonal_0 Loss: 0.1159 | 0.0925
Epoch 4/300, seasonal_0 Loss: 0.1088 | 0.0772
Epoch 5/300, seasonal_0 Loss: 0.1008 | 0.0703
Epoch 6/300, seasonal_0 Loss: 0.1004 | 0.0673
Epoch 7/300, seasonal_0 Loss: 0.1004 | 0.0899
Epoch 8/300, seasonal_0 Loss: 0.0963 | 0.0763
Epoch 9/300, seasonal_0 Loss: 0.0877 | 0.0686
Epoch 10/300, seasonal_0 Loss: 0.0826 | 0.0650
Epoch 11/300, seasonal_0 Loss: 0.0788 | 0.0604
Epoch 12/300, seasonal_0 Loss: 0.0761 | 0.0575
Epoch 13/300, seasonal_0 Loss: 0.0739 | 0.0573
Epoch 14/300, seasonal_0 Loss: 0.0716 | 0.0539
Epoch 15/300, seasonal_0 Loss: 0.0696 | 0.0499
Epoch 16/300, seasonal_0 Loss: 0.0676 | 0.0455
Epoch 17/300, seasonal_0 Loss: 0.0655 | 0.0396
Epoch 18/300, seasonal_0 Loss: 0.0638 | 0.0367
Epoch 19/300, seasonal_0 Loss: 0.0624 | 0.0361
Epoch 20/300, seasonal_0 Loss: 0.0612 | 0.0357
Epoch 21/300, seasonal_0 Loss: 0.0602 | 0.0370
Epoch 22/300, seasonal_0 Loss: 0.0596 | 0.0384
Epoch 23/300, seasonal_0 Loss: 0.0590 | 0.0363
Epoch 24/300, seasonal_0 Loss: 0.0583 | 0.0378
Epoch 25/300, seasonal_0 Loss: 0.0580 | 0.0389
Epoch 26/300, seasonal_0 Loss: 0.0572 | 0.0456
Epoch 27/300, seasonal_0 Loss: 0.0564 | 0.0498
Epoch 28/300, seasonal_0 Loss: 0.0563 | 0.0516
Epoch 29/300, seasonal_0 Loss: 0.0556 | 0.0494
Epoch 30/300, seasonal_0 Loss: 0.0547 | 0.0465
Epoch 31/300, seasonal_0 Loss: 0.0536 | 0.0437
Epoch 32/300, seasonal_0 Loss: 0.0520 | 0.0418
Epoch 33/300, seasonal_0 Loss: 0.0512 | 0.0400
Epoch 34/300, seasonal_0 Loss: 0.0502 | 0.0389
Epoch 35/300, seasonal_0 Loss: 0.0499 | 0.0307
Epoch 36/300, seasonal_0 Loss: 0.0494 | 0.0287
Epoch 37/300, seasonal_0 Loss: 0.0488 | 0.0309
Epoch 38/300, seasonal_0 Loss: 0.0497 | 0.0273
Epoch 39/300, seasonal_0 Loss: 0.0495 | 0.0252
Epoch 40/300, seasonal_0 Loss: 0.0479 | 0.0260
Epoch 41/300, seasonal_0 Loss: 0.0474 | 0.0250
Epoch 42/300, seasonal_0 Loss: 0.0480 | 0.0279
Epoch 43/300, seasonal_0 Loss: 0.0474 | 0.0241
Epoch 44/300, seasonal_0 Loss: 0.0458 | 0.0235
Epoch 45/300, seasonal_0 Loss: 0.0455 | 0.0272
Epoch 46/300, seasonal_0 Loss: 0.0455 | 0.0330
Epoch 47/300, seasonal_0 Loss: 0.0453 | 0.0405
Epoch 48/300, seasonal_0 Loss: 0.0451 | 0.0458
Epoch 49/300, seasonal_0 Loss: 0.0452 | 0.0388
Epoch 50/300, seasonal_0 Loss: 0.0463 | 0.0288
Epoch 51/300, seasonal_0 Loss: 0.0470 | 0.0218
Epoch 52/300, seasonal_0 Loss: 0.0448 | 0.0219
Epoch 53/300, seasonal_0 Loss: 0.0438 | 0.0264
Epoch 54/300, seasonal_0 Loss: 0.0434 | 0.0252
Epoch 55/300, seasonal_0 Loss: 0.0426 | 0.0253
Epoch 56/300, seasonal_0 Loss: 0.0424 | 0.0231
Epoch 57/300, seasonal_0 Loss: 0.0414 | 0.0245
Epoch 58/300, seasonal_0 Loss: 0.0403 | 0.0241
Epoch 59/300, seasonal_0 Loss: 0.0391 | 0.0199
Epoch 60/300, seasonal_0 Loss: 0.0431 | 0.0217
Epoch 61/300, seasonal_0 Loss: 0.0384 | 0.0207
Epoch 62/300, seasonal_0 Loss: 0.0382 | 0.0189
Epoch 63/300, seasonal_0 Loss: 0.0428 | 0.0242
Epoch 64/300, seasonal_0 Loss: 0.0391 | 0.0198
Epoch 65/300, seasonal_0 Loss: 0.0376 | 0.0220
Epoch 66/300, seasonal_0 Loss: 0.0363 | 0.0209
Epoch 67/300, seasonal_0 Loss: 0.0355 | 0.0199
Epoch 68/300, seasonal_0 Loss: 0.0349 | 0.0196
Epoch 69/300, seasonal_0 Loss: 0.0343 | 0.0204
Epoch 70/300, seasonal_0 Loss: 0.0339 | 0.0202
Epoch 71/300, seasonal_0 Loss: 0.0334 | 0.0208
Epoch 72/300, seasonal_0 Loss: 0.0332 | 0.0204
Epoch 73/300, seasonal_0 Loss: 0.0333 | 0.0235
Epoch 74/300, seasonal_0 Loss: 0.0341 | 0.0202
Epoch 75/300, seasonal_0 Loss: 0.0420 | 0.0292
Epoch 76/300, seasonal_0 Loss: 0.0370 | 0.0267
Epoch 77/300, seasonal_0 Loss: 0.0359 | 0.0237
Epoch 78/300, seasonal_0 Loss: 0.0348 | 0.0236
Epoch 79/300, seasonal_0 Loss: 0.0347 | 0.0219
Epoch 80/300, seasonal_0 Loss: 0.0340 | 0.0231
Epoch 81/300, seasonal_0 Loss: 0.0343 | 0.0218
Epoch 82/300, seasonal_0 Loss: 0.0340 | 0.0221
Epoch 83/300, seasonal_0 Loss: 0.0345 | 0.0220
Epoch 84/300, seasonal_0 Loss: 0.0338 | 0.0269
Epoch 85/300, seasonal_0 Loss: 0.0342 | 0.0303
Epoch 86/300, seasonal_0 Loss: 0.0337 | 0.0445
Epoch 87/300, seasonal_0 Loss: 0.0329 | 0.0387
Epoch 88/300, seasonal_0 Loss: 0.0333 | 0.0380
Epoch 89/300, seasonal_0 Loss: 0.0324 | 0.0279
Epoch 90/300, seasonal_0 Loss: 0.0318 | 0.0291
Epoch 91/300, seasonal_0 Loss: 0.0313 | 0.0270
Epoch 92/300, seasonal_0 Loss: 0.0309 | 0.0275
Epoch 93/300, seasonal_0 Loss: 0.0310 | 0.0258
Epoch 94/300, seasonal_0 Loss: 0.0309 | 0.0268
Epoch 95/300, seasonal_0 Loss: 0.0307 | 0.0247
Epoch 96/300, seasonal_0 Loss: 0.0302 | 0.0241
Epoch 97/300, seasonal_0 Loss: 0.0299 | 0.0244
Epoch 98/300, seasonal_0 Loss: 0.0295 | 0.0237
Epoch 99/300, seasonal_0 Loss: 0.0303 | 0.0245
Epoch 100/300, seasonal_0 Loss: 0.0354 | 0.0264
Epoch 101/300, seasonal_0 Loss: 0.0319 | 0.0245
Epoch 102/300, seasonal_0 Loss: 0.0302 | 0.0229
Epoch 103/300, seasonal_0 Loss: 0.0295 | 0.0232
Epoch 104/300, seasonal_0 Loss: 0.0292 | 0.0232
Epoch 105/300, seasonal_0 Loss: 0.0290 | 0.0232
Epoch 106/300, seasonal_0 Loss: 0.0288 | 0.0244
Epoch 107/300, seasonal_0 Loss: 0.0296 | 0.0232
Epoch 108/300, seasonal_0 Loss: 0.0288 | 0.0226
Epoch 109/300, seasonal_0 Loss: 0.0286 | 0.0240
Epoch 110/300, seasonal_0 Loss: 0.0287 | 0.0232
Epoch 111/300, seasonal_0 Loss: 0.0292 | 0.0245
Epoch 112/300, seasonal_0 Loss: 0.0289 | 0.0234
Epoch 113/300, seasonal_0 Loss: 0.0292 | 0.0231
Epoch 114/300, seasonal_0 Loss: 0.0291 | 0.0216
Epoch 115/300, seasonal_0 Loss: 0.0283 | 0.0227
Epoch 116/300, seasonal_0 Loss: 0.0279 | 0.0226
Epoch 117/300, seasonal_0 Loss: 0.0275 | 0.0218
Epoch 118/300, seasonal_0 Loss: 0.0278 | 0.0250
Epoch 119/300, seasonal_0 Loss: 0.0281 | 0.0224
Epoch 120/300, seasonal_0 Loss: 0.0280 | 0.0229
Epoch 121/300, seasonal_0 Loss: 0.0281 | 0.0210
Epoch 122/300, seasonal_0 Loss: 0.0280 | 0.0239
Epoch 123/300, seasonal_0 Loss: 0.0278 | 0.0224
Epoch 124/300, seasonal_0 Loss: 0.0273 | 0.0221
Epoch 125/300, seasonal_0 Loss: 0.0273 | 0.0219
Epoch 126/300, seasonal_0 Loss: 0.0271 | 0.0222
Epoch 127/300, seasonal_0 Loss: 0.0267 | 0.0223
Epoch 128/300, seasonal_0 Loss: 0.0267 | 0.0218
Epoch 129/300, seasonal_0 Loss: 0.0266 | 0.0236
Epoch 130/300, seasonal_0 Loss: 0.0268 | 0.0220
Epoch 131/300, seasonal_0 Loss: 0.0259 | 0.0216
Epoch 132/300, seasonal_0 Loss: 0.0256 | 0.0215
Epoch 133/300, seasonal_0 Loss: 0.0265 | 0.0219
Epoch 134/300, seasonal_0 Loss: 0.0257 | 0.0218
Epoch 135/300, seasonal_0 Loss: 0.0258 | 0.0217
Epoch 136/300, seasonal_0 Loss: 0.0255 | 0.0218
Epoch 137/300, seasonal_0 Loss: 0.0252 | 0.0214
Epoch 138/300, seasonal_0 Loss: 0.0249 | 0.0215
Epoch 139/300, seasonal_0 Loss: 0.0246 | 0.0215
Epoch 140/300, seasonal_0 Loss: 0.0251 | 0.0211
Epoch 141/300, seasonal_0 Loss: 0.0252 | 0.0214
Epoch 142/300, seasonal_0 Loss: 0.0247 | 0.0239
Epoch 143/300, seasonal_0 Loss: 0.0259 | 0.0221
Epoch 144/300, seasonal_0 Loss: 0.0250 | 0.0215
Epoch 145/300, seasonal_0 Loss: 0.0245 | 0.0216
Epoch 146/300, seasonal_0 Loss: 0.0243 | 0.0216
Epoch 147/300, seasonal_0 Loss: 0.0245 | 0.0213
Epoch 148/300, seasonal_0 Loss: 0.0245 | 0.0217
Epoch 149/300, seasonal_0 Loss: 0.0241 | 0.0224
Epoch 150/300, seasonal_0 Loss: 0.0240 | 0.0215
Epoch 151/300, seasonal_0 Loss: 0.0240 | 0.0228
Epoch 152/300, seasonal_0 Loss: 0.0244 | 0.0211
Epoch 153/300, seasonal_0 Loss: 0.0283 | 0.0223
Epoch 154/300, seasonal_0 Loss: 0.0246 | 0.0225
Epoch 155/300, seasonal_0 Loss: 0.0240 | 0.0224
Epoch 156/300, seasonal_0 Loss: 0.0238 | 0.0219
Epoch 157/300, seasonal_0 Loss: 0.0241 | 0.0226
Epoch 158/300, seasonal_0 Loss: 0.0236 | 0.0227
Epoch 159/300, seasonal_0 Loss: 0.0235 | 0.0222
Epoch 160/300, seasonal_0 Loss: 0.0240 | 0.0235
Epoch 161/300, seasonal_0 Loss: 0.0243 | 0.0229
Epoch 162/300, seasonal_0 Loss: 0.0244 | 0.0223
Epoch 163/300, seasonal_0 Loss: 0.0241 | 0.0225
Epoch 164/300, seasonal_0 Loss: 0.0242 | 0.0226
Epoch 165/300, seasonal_0 Loss: 0.0240 | 0.0221
Epoch 166/300, seasonal_0 Loss: 0.0241 | 0.0221
Epoch 167/300, seasonal_0 Loss: 0.0237 | 0.0222
Epoch 168/300, seasonal_0 Loss: 0.0233 | 0.0219
Epoch 169/300, seasonal_0 Loss: 0.0234 | 0.0226
Epoch 170/300, seasonal_0 Loss: 0.0238 | 0.0219
Epoch 171/300, seasonal_0 Loss: 0.0253 | 0.0238
Epoch 172/300, seasonal_0 Loss: 0.0238 | 0.0227
Epoch 173/300, seasonal_0 Loss: 0.0233 | 0.0223
Epoch 174/300, seasonal_0 Loss: 0.0231 | 0.0223
Epoch 175/300, seasonal_0 Loss: 0.0237 | 0.0221
Epoch 176/300, seasonal_0 Loss: 0.0234 | 0.0221
Epoch 177/300, seasonal_0 Loss: 0.0233 | 0.0222
Epoch 178/300, seasonal_0 Loss: 0.0228 | 0.0220
Epoch 179/300, seasonal_0 Loss: 0.0227 | 0.0223
Epoch 180/300, seasonal_0 Loss: 0.0234 | 0.0220
Epoch 181/300, seasonal_0 Loss: 0.0229 | 0.0222
Epoch 182/300, seasonal_0 Loss: 0.0226 | 0.0217
Epoch 183/300, seasonal_0 Loss: 0.0229 | 0.0220
Epoch 184/300, seasonal_0 Loss: 0.0227 | 0.0226
Epoch 185/300, seasonal_0 Loss: 0.0228 | 0.0217
Epoch 186/300, seasonal_0 Loss: 0.0231 | 0.0214
Epoch 187/300, seasonal_0 Loss: 0.0235 | 0.0227
Epoch 188/300, seasonal_0 Loss: 0.0226 | 0.0223
Epoch 189/300, seasonal_0 Loss: 0.0224 | 0.0222
Epoch 190/300, seasonal_0 Loss: 0.0223 | 0.0221
Epoch 191/300, seasonal_0 Loss: 0.0222 | 0.0221
Epoch 192/300, seasonal_0 Loss: 0.0222 | 0.0221
Epoch 193/300, seasonal_0 Loss: 0.0222 | 0.0221
Epoch 194/300, seasonal_0 Loss: 0.0221 | 0.0221
Epoch 195/300, seasonal_0 Loss: 0.0221 | 0.0221
Epoch 196/300, seasonal_0 Loss: 0.0221 | 0.0221
Epoch 197/300, seasonal_0 Loss: 0.0221 | 0.0221
Epoch 198/300, seasonal_0 Loss: 0.0220 | 0.0221
Epoch 199/300, seasonal_0 Loss: 0.0220 | 0.0221
Epoch 200/300, seasonal_0 Loss: 0.0220 | 0.0221
Epoch 201/300, seasonal_0 Loss: 0.0220 | 0.0221
Epoch 202/300, seasonal_0 Loss: 0.0219 | 0.0221
Epoch 203/300, seasonal_0 Loss: 0.0219 | 0.0222
Epoch 204/300, seasonal_0 Loss: 0.0219 | 0.0221
Epoch 205/300, seasonal_0 Loss: 0.0219 | 0.0222
Epoch 206/300, seasonal_0 Loss: 0.0219 | 0.0221
Epoch 207/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 208/300, seasonal_0 Loss: 0.0218 | 0.0221
Epoch 209/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 210/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 211/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 212/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 213/300, seasonal_0 Loss: 0.0217 | 0.0222
Epoch 214/300, seasonal_0 Loss: 0.0217 | 0.0221
Epoch 215/300, seasonal_0 Loss: 0.0217 | 0.0223
Epoch 216/300, seasonal_0 Loss: 0.0217 | 0.0220
Epoch 217/300, seasonal_0 Loss: 0.0220 | 0.0227
Epoch 218/300, seasonal_0 Loss: 0.0224 | 0.0217
Epoch 219/300, seasonal_0 Loss: 0.0222 | 0.0221
Epoch 220/300, seasonal_0 Loss: 0.0220 | 0.0223
Epoch 221/300, seasonal_0 Loss: 0.0218 | 0.0222
Epoch 222/300, seasonal_0 Loss: 0.0217 | 0.0224
Epoch 223/300, seasonal_0 Loss: 0.0216 | 0.0223
Epoch 224/300, seasonal_0 Loss: 0.0216 | 0.0223
Epoch 225/300, seasonal_0 Loss: 0.0216 | 0.0222
Epoch 226/300, seasonal_0 Loss: 0.0216 | 0.0222
Epoch 227/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 228/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 229/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 230/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 231/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 232/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 233/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 234/300, seasonal_0 Loss: 0.0215 | 0.0222
Epoch 235/300, seasonal_0 Loss: 0.0214 | 0.0222
Epoch 236/300, seasonal_0 Loss: 0.0214 | 0.0222
Epoch 237/300, seasonal_0 Loss: 0.0214 | 0.0222
Epoch 238/300, seasonal_0 Loss: 0.0214 | 0.0222
Epoch 239/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 240/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 241/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 242/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 243/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 244/300, seasonal_0 Loss: 0.0214 | 0.0223
Epoch 245/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 246/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 247/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 248/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 249/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 250/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 251/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 252/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 253/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 254/300, seasonal_0 Loss: 0.0213 | 0.0223
Epoch 255/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 256/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 257/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 258/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 259/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 260/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 261/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 262/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 263/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 264/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 265/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 266/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 267/300, seasonal_0 Loss: 0.0212 | 0.0223
Epoch 268/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 269/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 270/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 271/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 272/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 273/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 274/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 275/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 276/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 277/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 278/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 279/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 280/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 281/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 282/300, seasonal_0 Loss: 0.0211 | 0.0223
Epoch 283/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 284/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 285/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 286/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 287/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 288/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 289/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 290/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 291/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 292/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 293/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 294/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 295/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 296/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 297/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 298/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 299/300, seasonal_0 Loss: 0.0210 | 0.0223
Epoch 300/300, seasonal_0 Loss: 0.0210 | 0.0223
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.6455842433315105, 'learning_rate': 0.0006222650001541163, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7517703082774607}
Epoch 1/300, seasonal_1 Loss: 0.2371 | 0.2559
Epoch 2/300, seasonal_1 Loss: 0.1336 | 0.1556
Epoch 3/300, seasonal_1 Loss: 0.1194 | 0.0694
Epoch 4/300, seasonal_1 Loss: 0.1109 | 0.1095
Epoch 5/300, seasonal_1 Loss: 0.1105 | 0.0698
Epoch 6/300, seasonal_1 Loss: 0.1121 | 0.0667
Epoch 7/300, seasonal_1 Loss: 0.1060 | 0.0571
Epoch 8/300, seasonal_1 Loss: 0.1031 | 0.0535
Epoch 9/300, seasonal_1 Loss: 0.0974 | 0.0534
Epoch 10/300, seasonal_1 Loss: 0.0932 | 0.0568
Epoch 11/300, seasonal_1 Loss: 0.0911 | 0.0507
Epoch 12/300, seasonal_1 Loss: 0.0888 | 0.0487
Epoch 13/300, seasonal_1 Loss: 0.0874 | 0.0469
Epoch 14/300, seasonal_1 Loss: 0.0863 | 0.0452
Epoch 15/300, seasonal_1 Loss: 0.0853 | 0.0442
Epoch 16/300, seasonal_1 Loss: 0.0843 | 0.0430
Epoch 17/300, seasonal_1 Loss: 0.0834 | 0.0420
Epoch 18/300, seasonal_1 Loss: 0.0822 | 0.0413
Epoch 19/300, seasonal_1 Loss: 0.0823 | 0.0396
Epoch 20/300, seasonal_1 Loss: 0.0818 | 0.0397
Epoch 21/300, seasonal_1 Loss: 0.0809 | 0.0374
Epoch 22/300, seasonal_1 Loss: 0.0796 | 0.0376
Epoch 23/300, seasonal_1 Loss: 0.0786 | 0.0372
Epoch 24/300, seasonal_1 Loss: 0.0777 | 0.0369
Epoch 25/300, seasonal_1 Loss: 0.0771 | 0.0361
Epoch 26/300, seasonal_1 Loss: 0.0765 | 0.0360
Epoch 27/300, seasonal_1 Loss: 0.0762 | 0.0355
Epoch 28/300, seasonal_1 Loss: 0.0758 | 0.0354
Epoch 29/300, seasonal_1 Loss: 0.0752 | 0.0366
Epoch 30/300, seasonal_1 Loss: 0.0750 | 0.0357
Epoch 31/300, seasonal_1 Loss: 0.0744 | 0.0360
Epoch 32/300, seasonal_1 Loss: 0.0740 | 0.0355
Epoch 33/300, seasonal_1 Loss: 0.0736 | 0.0343
Epoch 34/300, seasonal_1 Loss: 0.0727 | 0.0364
Epoch 35/300, seasonal_1 Loss: 0.0725 | 0.0344
Epoch 36/300, seasonal_1 Loss: 0.0720 | 0.0339
Epoch 37/300, seasonal_1 Loss: 0.0715 | 0.0332
Epoch 38/300, seasonal_1 Loss: 0.0710 | 0.0325
Epoch 39/300, seasonal_1 Loss: 0.0707 | 0.0330
Epoch 40/300, seasonal_1 Loss: 0.0700 | 0.0331
Epoch 41/300, seasonal_1 Loss: 0.0698 | 0.0337
Epoch 42/300, seasonal_1 Loss: 0.0696 | 0.0336
Epoch 43/300, seasonal_1 Loss: 0.0695 | 0.0326
Epoch 44/300, seasonal_1 Loss: 0.0693 | 0.0325
Epoch 45/300, seasonal_1 Loss: 0.0687 | 0.0326
Epoch 46/300, seasonal_1 Loss: 0.0687 | 0.0312
Epoch 47/300, seasonal_1 Loss: 0.0685 | 0.0327
Epoch 48/300, seasonal_1 Loss: 0.0684 | 0.0310
Epoch 49/300, seasonal_1 Loss: 0.0683 | 0.0326
Epoch 50/300, seasonal_1 Loss: 0.0682 | 0.0308
Epoch 51/300, seasonal_1 Loss: 0.0679 | 0.0326
Epoch 52/300, seasonal_1 Loss: 0.0677 | 0.0305
Epoch 53/300, seasonal_1 Loss: 0.0675 | 0.0329
Epoch 54/300, seasonal_1 Loss: 0.0675 | 0.0299
Epoch 55/300, seasonal_1 Loss: 0.0675 | 0.0343
Epoch 56/300, seasonal_1 Loss: 0.0675 | 0.0299
Epoch 57/300, seasonal_1 Loss: 0.0675 | 0.0349
Epoch 58/300, seasonal_1 Loss: 0.0679 | 0.0302
Epoch 59/300, seasonal_1 Loss: 0.0677 | 0.0357
Epoch 60/300, seasonal_1 Loss: 0.0681 | 0.0302
Epoch 61/300, seasonal_1 Loss: 0.0675 | 0.0354
Epoch 62/300, seasonal_1 Loss: 0.0674 | 0.0303
Epoch 63/300, seasonal_1 Loss: 0.0668 | 0.0322
Epoch 64/300, seasonal_1 Loss: 0.0666 | 0.0303
Epoch 65/300, seasonal_1 Loss: 0.0662 | 0.0321
Epoch 66/300, seasonal_1 Loss: 0.0661 | 0.0304
Epoch 67/300, seasonal_1 Loss: 0.0656 | 0.0312
Epoch 68/300, seasonal_1 Loss: 0.0654 | 0.0308
Epoch 69/300, seasonal_1 Loss: 0.0652 | 0.0312
Epoch 70/300, seasonal_1 Loss: 0.0650 | 0.0309
Epoch 71/300, seasonal_1 Loss: 0.0648 | 0.0312
Epoch 72/300, seasonal_1 Loss: 0.0645 | 0.0310
Epoch 73/300, seasonal_1 Loss: 0.0643 | 0.0310
Epoch 74/300, seasonal_1 Loss: 0.0642 | 0.0310
Epoch 75/300, seasonal_1 Loss: 0.0639 | 0.0310
Epoch 76/300, seasonal_1 Loss: 0.0637 | 0.0309
Epoch 77/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 78/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 79/300, seasonal_1 Loss: 0.0632 | 0.0305
Epoch 80/300, seasonal_1 Loss: 0.0630 | 0.0304
Epoch 81/300, seasonal_1 Loss: 0.0628 | 0.0302
Epoch 82/300, seasonal_1 Loss: 0.0626 | 0.0301
Epoch 83/300, seasonal_1 Loss: 0.0625 | 0.0300
Epoch 84/300, seasonal_1 Loss: 0.0623 | 0.0303
Epoch 85/300, seasonal_1 Loss: 0.0625 | 0.0301
Epoch 86/300, seasonal_1 Loss: 0.0623 | 0.0300
Epoch 87/300, seasonal_1 Loss: 0.0621 | 0.0299
Epoch 88/300, seasonal_1 Loss: 0.0618 | 0.0298
Epoch 89/300, seasonal_1 Loss: 0.0616 | 0.0300
Epoch 90/300, seasonal_1 Loss: 0.0615 | 0.0300
Epoch 91/300, seasonal_1 Loss: 0.0614 | 0.0300
Epoch 92/300, seasonal_1 Loss: 0.0612 | 0.0300
Epoch 93/300, seasonal_1 Loss: 0.0611 | 0.0301
Epoch 94/300, seasonal_1 Loss: 0.0610 | 0.0301
Epoch 95/300, seasonal_1 Loss: 0.0609 | 0.0303
Epoch 96/300, seasonal_1 Loss: 0.0608 | 0.0303
Epoch 97/300, seasonal_1 Loss: 0.0607 | 0.0303
Epoch 98/300, seasonal_1 Loss: 0.0607 | 0.0303
Epoch 99/300, seasonal_1 Loss: 0.0606 | 0.0303
Epoch 100/300, seasonal_1 Loss: 0.0605 | 0.0304
Epoch 101/300, seasonal_1 Loss: 0.0605 | 0.0304
Epoch 102/300, seasonal_1 Loss: 0.0604 | 0.0304
Epoch 103/300, seasonal_1 Loss: 0.0604 | 0.0304
Epoch 104/300, seasonal_1 Loss: 0.0603 | 0.0305
Epoch 105/300, seasonal_1 Loss: 0.0603 | 0.0305
Epoch 106/300, seasonal_1 Loss: 0.0602 | 0.0305
Epoch 107/300, seasonal_1 Loss: 0.0602 | 0.0305
Epoch 108/300, seasonal_1 Loss: 0.0601 | 0.0305
Epoch 109/300, seasonal_1 Loss: 0.0601 | 0.0306
Epoch 110/300, seasonal_1 Loss: 0.0601 | 0.0306
Epoch 111/300, seasonal_1 Loss: 0.0600 | 0.0306
Epoch 112/300, seasonal_1 Loss: 0.0600 | 0.0306
Epoch 113/300, seasonal_1 Loss: 0.0600 | 0.0306
Epoch 114/300, seasonal_1 Loss: 0.0600 | 0.0307
Epoch 115/300, seasonal_1 Loss: 0.0599 | 0.0307
Epoch 116/300, seasonal_1 Loss: 0.0599 | 0.0307
Epoch 117/300, seasonal_1 Loss: 0.0599 | 0.0307
Epoch 118/300, seasonal_1 Loss: 0.0599 | 0.0307
Epoch 119/300, seasonal_1 Loss: 0.0599 | 0.0308
Epoch 120/300, seasonal_1 Loss: 0.0598 | 0.0308
Epoch 121/300, seasonal_1 Loss: 0.0598 | 0.0308
Epoch 122/300, seasonal_1 Loss: 0.0598 | 0.0308
Epoch 123/300, seasonal_1 Loss: 0.0598 | 0.0309
Epoch 124/300, seasonal_1 Loss: 0.0598 | 0.0309
Epoch 125/300, seasonal_1 Loss: 0.0598 | 0.0309
Epoch 126/300, seasonal_1 Loss: 0.0597 | 0.0309
Epoch 127/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 128/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 129/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 130/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 131/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 132/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 133/300, seasonal_1 Loss: 0.0597 | 0.0311
Epoch 134/300, seasonal_1 Loss: 0.0597 | 0.0311
Epoch 135/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 136/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 137/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 138/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 139/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 140/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 141/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 142/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 143/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 144/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 145/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 146/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 147/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 148/300, seasonal_1 Loss: 0.0596 | 0.0312
Epoch 149/300, seasonal_1 Loss: 0.0596 | 0.0313
Epoch 150/300, seasonal_1 Loss: 0.0596 | 0.0313
Epoch 151/300, seasonal_1 Loss: 0.0596 | 0.0313
Epoch 152/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 153/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 154/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 155/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 156/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 157/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 158/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 159/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 160/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 161/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 162/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 163/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 164/300, seasonal_1 Loss: 0.0595 | 0.0313
Epoch 165/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 166/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 167/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 168/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 169/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 170/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 171/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 172/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 173/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 174/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 175/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 176/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 177/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 178/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 179/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 180/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 181/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 182/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 183/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 184/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 185/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 186/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 187/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 188/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 189/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 190/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 191/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 192/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 193/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 194/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 195/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 196/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 197/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 198/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 199/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 200/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 201/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 202/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 203/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 204/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 205/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 206/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 207/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 208/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 209/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 210/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 211/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 212/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 213/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 214/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 215/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 216/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 217/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 218/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 219/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 220/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 221/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 222/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 223/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 224/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 225/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 226/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 227/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 228/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 229/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 230/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 231/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 232/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 233/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 234/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 235/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 236/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 237/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 238/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 239/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 240/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 241/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 242/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 243/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 244/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 245/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 246/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 247/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 248/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 249/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 250/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 251/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 252/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 253/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 254/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 255/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 256/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 257/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 258/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 259/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 260/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 261/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 262/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 263/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 264/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 265/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 266/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 267/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 268/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 269/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 270/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 271/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 272/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 273/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 274/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 275/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 276/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 277/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 278/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 279/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 280/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 281/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 282/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 283/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 284/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 285/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 286/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 287/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 288/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 289/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 290/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 291/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 292/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 293/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 294/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 295/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 296/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 297/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 298/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 299/300, seasonal_1 Loss: 0.0595 | 0.0314
Epoch 300/300, seasonal_1 Loss: 0.0595 | 0.0314
Training seasonal_2 component with params: {'observation_period_num': 19, 'train_rates': 0.8769150642910872, 'learning_rate': 0.0004388346776949093, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8141949405634111}
Epoch 1/300, seasonal_2 Loss: 1.1051 | 0.3158
Epoch 2/300, seasonal_2 Loss: 0.2647 | 0.3359
Epoch 3/300, seasonal_2 Loss: 0.2431 | 0.5332
Epoch 4/300, seasonal_2 Loss: 0.2139 | 0.3694
Epoch 5/300, seasonal_2 Loss: 0.1739 | 0.1690
Epoch 6/300, seasonal_2 Loss: 0.1623 | 0.1518
Epoch 7/300, seasonal_2 Loss: 0.1661 | 0.1403
Epoch 8/300, seasonal_2 Loss: 0.1439 | 0.1390
Epoch 9/300, seasonal_2 Loss: 0.1307 | 0.1363
Epoch 10/300, seasonal_2 Loss: 0.1199 | 0.1010
Epoch 11/300, seasonal_2 Loss: 0.1139 | 0.0833
Epoch 12/300, seasonal_2 Loss: 0.1271 | 0.1035
Epoch 13/300, seasonal_2 Loss: 0.1391 | 0.0862
Epoch 14/300, seasonal_2 Loss: 0.1124 | 0.1175
Epoch 15/300, seasonal_2 Loss: 0.1130 | 0.1145
Epoch 16/300, seasonal_2 Loss: 0.1060 | 0.0761
Epoch 17/300, seasonal_2 Loss: 0.0995 | 0.0636
Epoch 18/300, seasonal_2 Loss: 0.1081 | 0.0757
Epoch 19/300, seasonal_2 Loss: 0.1118 | 0.0653
Epoch 20/300, seasonal_2 Loss: 0.1013 | 0.0721
Epoch 21/300, seasonal_2 Loss: 0.1021 | 0.0674
Epoch 22/300, seasonal_2 Loss: 0.0981 | 0.0587
Epoch 23/300, seasonal_2 Loss: 0.1003 | 0.0633
Epoch 24/300, seasonal_2 Loss: 0.1046 | 0.0695
Epoch 25/300, seasonal_2 Loss: 0.1049 | 0.0650
Epoch 26/300, seasonal_2 Loss: 0.1023 | 0.0595
Epoch 27/300, seasonal_2 Loss: 0.0989 | 0.0580
Epoch 28/300, seasonal_2 Loss: 0.0983 | 0.0751
Epoch 29/300, seasonal_2 Loss: 0.1030 | 0.1236
Epoch 30/300, seasonal_2 Loss: 0.1136 | 0.0970
Epoch 31/300, seasonal_2 Loss: 0.1165 | 0.0748
Epoch 32/300, seasonal_2 Loss: 0.1008 | 0.0629
Epoch 33/300, seasonal_2 Loss: 0.0871 | 0.0530
Epoch 34/300, seasonal_2 Loss: 0.0869 | 0.0537
Epoch 35/300, seasonal_2 Loss: 0.0857 | 0.0535
Epoch 36/300, seasonal_2 Loss: 0.0837 | 0.0534
Epoch 37/300, seasonal_2 Loss: 0.0833 | 0.0545
Epoch 38/300, seasonal_2 Loss: 0.0825 | 0.0571
Epoch 39/300, seasonal_2 Loss: 0.0821 | 0.0565
Epoch 40/300, seasonal_2 Loss: 0.0817 | 0.0510
Epoch 41/300, seasonal_2 Loss: 0.0812 | 0.0501
Epoch 42/300, seasonal_2 Loss: 0.0811 | 0.0497
Epoch 43/300, seasonal_2 Loss: 0.0807 | 0.0494
Epoch 44/300, seasonal_2 Loss: 0.0803 | 0.0521
Epoch 45/300, seasonal_2 Loss: 0.0800 | 0.0525
Epoch 46/300, seasonal_2 Loss: 0.0797 | 0.0487
Epoch 47/300, seasonal_2 Loss: 0.0792 | 0.0479
Epoch 48/300, seasonal_2 Loss: 0.0791 | 0.0479
Epoch 49/300, seasonal_2 Loss: 0.0788 | 0.0486
Epoch 50/300, seasonal_2 Loss: 0.0785 | 0.0487
Epoch 51/300, seasonal_2 Loss: 0.0783 | 0.0478
Epoch 52/300, seasonal_2 Loss: 0.0780 | 0.0470
Epoch 53/300, seasonal_2 Loss: 0.0778 | 0.0468
Epoch 54/300, seasonal_2 Loss: 0.0777 | 0.0470
Epoch 55/300, seasonal_2 Loss: 0.0775 | 0.0469
Epoch 56/300, seasonal_2 Loss: 0.0772 | 0.0466
Epoch 57/300, seasonal_2 Loss: 0.0771 | 0.0463
Epoch 58/300, seasonal_2 Loss: 0.0769 | 0.0461
Epoch 59/300, seasonal_2 Loss: 0.0768 | 0.0461
Epoch 60/300, seasonal_2 Loss: 0.0766 | 0.0459
Epoch 61/300, seasonal_2 Loss: 0.0764 | 0.0457
Epoch 62/300, seasonal_2 Loss: 0.0763 | 0.0456
Epoch 63/300, seasonal_2 Loss: 0.0761 | 0.0455
Epoch 64/300, seasonal_2 Loss: 0.0760 | 0.0453
Epoch 65/300, seasonal_2 Loss: 0.0758 | 0.0452
Epoch 66/300, seasonal_2 Loss: 0.0757 | 0.0450
Epoch 67/300, seasonal_2 Loss: 0.0755 | 0.0450
Epoch 68/300, seasonal_2 Loss: 0.0754 | 0.0448
Epoch 69/300, seasonal_2 Loss: 0.0753 | 0.0447
Epoch 70/300, seasonal_2 Loss: 0.0752 | 0.0446
Epoch 71/300, seasonal_2 Loss: 0.0751 | 0.0445
Epoch 72/300, seasonal_2 Loss: 0.0749 | 0.0444
Epoch 73/300, seasonal_2 Loss: 0.0748 | 0.0443
Epoch 74/300, seasonal_2 Loss: 0.0747 | 0.0442
Epoch 75/300, seasonal_2 Loss: 0.0746 | 0.0441
Epoch 76/300, seasonal_2 Loss: 0.0745 | 0.0440
Epoch 77/300, seasonal_2 Loss: 0.0744 | 0.0439
Epoch 78/300, seasonal_2 Loss: 0.0743 | 0.0438
Epoch 79/300, seasonal_2 Loss: 0.0742 | 0.0437
Epoch 80/300, seasonal_2 Loss: 0.0741 | 0.0436
Epoch 81/300, seasonal_2 Loss: 0.0740 | 0.0435
Epoch 82/300, seasonal_2 Loss: 0.0739 | 0.0434
Epoch 83/300, seasonal_2 Loss: 0.0738 | 0.0433
Epoch 84/300, seasonal_2 Loss: 0.0737 | 0.0433
Epoch 85/300, seasonal_2 Loss: 0.0736 | 0.0432
Epoch 86/300, seasonal_2 Loss: 0.0735 | 0.0431
Epoch 87/300, seasonal_2 Loss: 0.0734 | 0.0430
Epoch 88/300, seasonal_2 Loss: 0.0734 | 0.0430
Epoch 89/300, seasonal_2 Loss: 0.0733 | 0.0429
Epoch 90/300, seasonal_2 Loss: 0.0732 | 0.0428
Epoch 91/300, seasonal_2 Loss: 0.0731 | 0.0428
Epoch 92/300, seasonal_2 Loss: 0.0731 | 0.0427
Epoch 93/300, seasonal_2 Loss: 0.0730 | 0.0426
Epoch 94/300, seasonal_2 Loss: 0.0729 | 0.0425
Epoch 95/300, seasonal_2 Loss: 0.0728 | 0.0425
Epoch 96/300, seasonal_2 Loss: 0.0728 | 0.0424
Epoch 97/300, seasonal_2 Loss: 0.0727 | 0.0424
Epoch 98/300, seasonal_2 Loss: 0.0727 | 0.0423
Epoch 99/300, seasonal_2 Loss: 0.0726 | 0.0422
Epoch 100/300, seasonal_2 Loss: 0.0725 | 0.0422
Epoch 101/300, seasonal_2 Loss: 0.0725 | 0.0421
Epoch 102/300, seasonal_2 Loss: 0.0724 | 0.0421
Epoch 103/300, seasonal_2 Loss: 0.0724 | 0.0420
Epoch 104/300, seasonal_2 Loss: 0.0723 | 0.0420
Epoch 105/300, seasonal_2 Loss: 0.0723 | 0.0419
Epoch 106/300, seasonal_2 Loss: 0.0722 | 0.0419
Epoch 107/300, seasonal_2 Loss: 0.0722 | 0.0418
Epoch 108/300, seasonal_2 Loss: 0.0721 | 0.0418
Epoch 109/300, seasonal_2 Loss: 0.0721 | 0.0417
Epoch 110/300, seasonal_2 Loss: 0.0720 | 0.0417
Epoch 111/300, seasonal_2 Loss: 0.0720 | 0.0416
Epoch 112/300, seasonal_2 Loss: 0.0720 | 0.0416
Epoch 113/300, seasonal_2 Loss: 0.0719 | 0.0415
Epoch 114/300, seasonal_2 Loss: 0.0719 | 0.0415
Epoch 115/300, seasonal_2 Loss: 0.0718 | 0.0415
Epoch 116/300, seasonal_2 Loss: 0.0718 | 0.0414
Epoch 117/300, seasonal_2 Loss: 0.0718 | 0.0414
Epoch 118/300, seasonal_2 Loss: 0.0717 | 0.0413
Epoch 119/300, seasonal_2 Loss: 0.0717 | 0.0413
Epoch 120/300, seasonal_2 Loss: 0.0717 | 0.0413
Epoch 121/300, seasonal_2 Loss: 0.0716 | 0.0412
Epoch 122/300, seasonal_2 Loss: 0.0716 | 0.0412
Epoch 123/300, seasonal_2 Loss: 0.0716 | 0.0412
Epoch 124/300, seasonal_2 Loss: 0.0715 | 0.0412
Epoch 125/300, seasonal_2 Loss: 0.0715 | 0.0411
Epoch 126/300, seasonal_2 Loss: 0.0715 | 0.0411
Epoch 127/300, seasonal_2 Loss: 0.0715 | 0.0411
Epoch 128/300, seasonal_2 Loss: 0.0714 | 0.0410
Epoch 129/300, seasonal_2 Loss: 0.0714 | 0.0410
Epoch 130/300, seasonal_2 Loss: 0.0714 | 0.0410
Epoch 131/300, seasonal_2 Loss: 0.0714 | 0.0410
Epoch 132/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 133/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 134/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 135/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 136/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 137/300, seasonal_2 Loss: 0.0712 | 0.0408
Epoch 138/300, seasonal_2 Loss: 0.0712 | 0.0408
Epoch 139/300, seasonal_2 Loss: 0.0712 | 0.0408
Epoch 140/300, seasonal_2 Loss: 0.0712 | 0.0408
Epoch 141/300, seasonal_2 Loss: 0.0712 | 0.0408
Epoch 142/300, seasonal_2 Loss: 0.0712 | 0.0407
Epoch 143/300, seasonal_2 Loss: 0.0711 | 0.0407
Epoch 144/300, seasonal_2 Loss: 0.0711 | 0.0407
Epoch 145/300, seasonal_2 Loss: 0.0711 | 0.0407
Epoch 146/300, seasonal_2 Loss: 0.0711 | 0.0407
Epoch 147/300, seasonal_2 Loss: 0.0711 | 0.0407
Epoch 148/300, seasonal_2 Loss: 0.0711 | 0.0406
Epoch 149/300, seasonal_2 Loss: 0.0711 | 0.0406
Epoch 150/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 151/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 152/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 153/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 154/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 155/300, seasonal_2 Loss: 0.0710 | 0.0406
Epoch 156/300, seasonal_2 Loss: 0.0710 | 0.0405
Epoch 157/300, seasonal_2 Loss: 0.0710 | 0.0405
Epoch 158/300, seasonal_2 Loss: 0.0710 | 0.0405
Epoch 159/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 160/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 161/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 162/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 163/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 164/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 165/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 166/300, seasonal_2 Loss: 0.0709 | 0.0405
Epoch 167/300, seasonal_2 Loss: 0.0709 | 0.0404
Epoch 168/300, seasonal_2 Loss: 0.0709 | 0.0404
Epoch 169/300, seasonal_2 Loss: 0.0709 | 0.0404
Epoch 170/300, seasonal_2 Loss: 0.0709 | 0.0404
Epoch 171/300, seasonal_2 Loss: 0.0709 | 0.0404
Epoch 172/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 173/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 174/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 175/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 176/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 177/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 178/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 179/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 180/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 181/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 182/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 183/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 184/300, seasonal_2 Loss: 0.0708 | 0.0404
Epoch 185/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 186/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 187/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 188/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 189/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 190/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 191/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 192/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 193/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 194/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 195/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 196/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 197/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 198/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 199/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 200/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 201/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 202/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 203/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 204/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 205/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 206/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 207/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 208/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 209/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 210/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 211/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 212/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 213/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 214/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 215/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 216/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 217/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 218/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 219/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 220/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 221/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 222/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 223/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 224/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 225/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 226/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 227/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 228/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 229/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 230/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 231/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 232/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 233/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 234/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 235/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 236/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 237/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 238/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 239/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 240/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 241/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 242/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 243/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 244/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 245/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 246/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 247/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 248/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 249/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 250/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 251/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 252/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 253/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 254/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 255/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 256/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 257/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 258/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 259/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 260/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 261/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 262/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 263/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 264/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 265/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 266/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 267/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 268/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 269/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 270/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 271/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 272/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 273/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 274/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 275/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 276/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 277/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 278/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 279/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 280/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 281/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 282/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 283/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 284/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 285/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 286/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 287/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 288/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 289/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 290/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 291/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 292/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 293/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 294/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 295/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 296/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 297/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 298/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 299/300, seasonal_2 Loss: 0.0707 | 0.0402
Epoch 300/300, seasonal_2 Loss: 0.0707 | 0.0402
Training seasonal_3 component with params: {'observation_period_num': 15, 'train_rates': 0.8042050780175394, 'learning_rate': 0.0002489861450357806, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8878208985881009}
Epoch 1/300, seasonal_3 Loss: 0.2832 | 0.1807
Epoch 2/300, seasonal_3 Loss: 0.1577 | 0.1177
Epoch 3/300, seasonal_3 Loss: 0.1554 | 0.1173
Epoch 4/300, seasonal_3 Loss: 0.1336 | 0.0967
Epoch 5/300, seasonal_3 Loss: 0.1191 | 0.0883
Epoch 6/300, seasonal_3 Loss: 0.1128 | 0.0868
Epoch 7/300, seasonal_3 Loss: 0.1116 | 0.0848
Epoch 8/300, seasonal_3 Loss: 0.1120 | 0.0680
Epoch 9/300, seasonal_3 Loss: 0.1140 | 0.0655
Epoch 10/300, seasonal_3 Loss: 0.1134 | 0.0639
Epoch 11/300, seasonal_3 Loss: 0.1038 | 0.0584
Epoch 12/300, seasonal_3 Loss: 0.0994 | 0.0576
Epoch 13/300, seasonal_3 Loss: 0.1025 | 0.0568
Epoch 14/300, seasonal_3 Loss: 0.1004 | 0.0538
Epoch 15/300, seasonal_3 Loss: 0.0962 | 0.0517
Epoch 16/300, seasonal_3 Loss: 0.0957 | 0.0504
Epoch 17/300, seasonal_3 Loss: 0.0937 | 0.0483
Epoch 18/300, seasonal_3 Loss: 0.0914 | 0.0463
Epoch 19/300, seasonal_3 Loss: 0.0884 | 0.0446
Epoch 20/300, seasonal_3 Loss: 0.0866 | 0.0449
Epoch 21/300, seasonal_3 Loss: 0.0865 | 0.0530
Epoch 22/300, seasonal_3 Loss: 0.0877 | 0.0502
Epoch 23/300, seasonal_3 Loss: 0.0865 | 0.0473
Epoch 24/300, seasonal_3 Loss: 0.0843 | 0.0477
Epoch 25/300, seasonal_3 Loss: 0.0820 | 0.0447
Epoch 26/300, seasonal_3 Loss: 0.0828 | 0.0432
Epoch 27/300, seasonal_3 Loss: 0.0866 | 0.0443
Epoch 28/300, seasonal_3 Loss: 0.0901 | 0.0428
Epoch 29/300, seasonal_3 Loss: 0.0800 | 0.0395
Epoch 30/300, seasonal_3 Loss: 0.0762 | 0.0366
Epoch 31/300, seasonal_3 Loss: 0.0751 | 0.0365
Epoch 32/300, seasonal_3 Loss: 0.0745 | 0.0357
Epoch 33/300, seasonal_3 Loss: 0.0740 | 0.0352
Epoch 34/300, seasonal_3 Loss: 0.0735 | 0.0348
Epoch 35/300, seasonal_3 Loss: 0.0729 | 0.0344
Epoch 36/300, seasonal_3 Loss: 0.0723 | 0.0340
Epoch 37/300, seasonal_3 Loss: 0.0718 | 0.0336
Epoch 38/300, seasonal_3 Loss: 0.0714 | 0.0332
Epoch 39/300, seasonal_3 Loss: 0.0709 | 0.0328
Epoch 40/300, seasonal_3 Loss: 0.0705 | 0.0325
Epoch 41/300, seasonal_3 Loss: 0.0702 | 0.0322
Epoch 42/300, seasonal_3 Loss: 0.0698 | 0.0319
Epoch 43/300, seasonal_3 Loss: 0.0694 | 0.0316
Epoch 44/300, seasonal_3 Loss: 0.0691 | 0.0314
Epoch 45/300, seasonal_3 Loss: 0.0687 | 0.0312
Epoch 46/300, seasonal_3 Loss: 0.0684 | 0.0310
Epoch 47/300, seasonal_3 Loss: 0.0681 | 0.0308
Epoch 48/300, seasonal_3 Loss: 0.0679 | 0.0307
Epoch 49/300, seasonal_3 Loss: 0.0676 | 0.0306
Epoch 50/300, seasonal_3 Loss: 0.0675 | 0.0304
Epoch 51/300, seasonal_3 Loss: 0.0673 | 0.0303
Epoch 52/300, seasonal_3 Loss: 0.0671 | 0.0302
Epoch 53/300, seasonal_3 Loss: 0.0669 | 0.0302
Epoch 54/300, seasonal_3 Loss: 0.0668 | 0.0301
Epoch 55/300, seasonal_3 Loss: 0.0666 | 0.0300
Epoch 56/300, seasonal_3 Loss: 0.0665 | 0.0300
Epoch 57/300, seasonal_3 Loss: 0.0663 | 0.0300
Epoch 58/300, seasonal_3 Loss: 0.0662 | 0.0300
Epoch 59/300, seasonal_3 Loss: 0.0661 | 0.0299
Epoch 60/300, seasonal_3 Loss: 0.0660 | 0.0299
Epoch 61/300, seasonal_3 Loss: 0.0659 | 0.0301
Epoch 62/300, seasonal_3 Loss: 0.0658 | 0.0300
Epoch 63/300, seasonal_3 Loss: 0.0657 | 0.0300
Epoch 64/300, seasonal_3 Loss: 0.0656 | 0.0302
Epoch 65/300, seasonal_3 Loss: 0.0656 | 0.0301
Epoch 66/300, seasonal_3 Loss: 0.0656 | 0.0300
Epoch 67/300, seasonal_3 Loss: 0.0656 | 0.0300
Epoch 68/300, seasonal_3 Loss: 0.0656 | 0.0298
Epoch 69/300, seasonal_3 Loss: 0.0656 | 0.0297
Epoch 70/300, seasonal_3 Loss: 0.0656 | 0.0296
Epoch 71/300, seasonal_3 Loss: 0.0655 | 0.0294
Epoch 72/300, seasonal_3 Loss: 0.0655 | 0.0294
Epoch 73/300, seasonal_3 Loss: 0.0652 | 0.0294
Epoch 74/300, seasonal_3 Loss: 0.0650 | 0.0294
Epoch 75/300, seasonal_3 Loss: 0.0647 | 0.0296
Epoch 76/300, seasonal_3 Loss: 0.0646 | 0.0296
Epoch 77/300, seasonal_3 Loss: 0.0644 | 0.0296
Epoch 78/300, seasonal_3 Loss: 0.0643 | 0.0297
Epoch 79/300, seasonal_3 Loss: 0.0642 | 0.0296
Epoch 80/300, seasonal_3 Loss: 0.0641 | 0.0294
Epoch 81/300, seasonal_3 Loss: 0.0640 | 0.0292
Epoch 82/300, seasonal_3 Loss: 0.0640 | 0.0289
Epoch 83/300, seasonal_3 Loss: 0.0640 | 0.0288
Epoch 84/300, seasonal_3 Loss: 0.0641 | 0.0287
Epoch 85/300, seasonal_3 Loss: 0.0642 | 0.0285
Epoch 86/300, seasonal_3 Loss: 0.0643 | 0.0285
Epoch 87/300, seasonal_3 Loss: 0.0644 | 0.0284
Epoch 88/300, seasonal_3 Loss: 0.0643 | 0.0284
Epoch 89/300, seasonal_3 Loss: 0.0644 | 0.0282
Epoch 90/300, seasonal_3 Loss: 0.0644 | 0.0283
Epoch 91/300, seasonal_3 Loss: 0.0641 | 0.0284
Epoch 92/300, seasonal_3 Loss: 0.0640 | 0.0286
Epoch 93/300, seasonal_3 Loss: 0.0640 | 0.0288
Epoch 94/300, seasonal_3 Loss: 0.0641 | 0.0291
Epoch 95/300, seasonal_3 Loss: 0.0645 | 0.0295
Epoch 96/300, seasonal_3 Loss: 0.0651 | 0.0300
Epoch 97/300, seasonal_3 Loss: 0.0661 | 0.0295
Epoch 98/300, seasonal_3 Loss: 0.0662 | 0.0295
Epoch 99/300, seasonal_3 Loss: 0.0657 | 0.0295
Epoch 100/300, seasonal_3 Loss: 0.0647 | 0.0295
Epoch 101/300, seasonal_3 Loss: 0.0638 | 0.0293
Epoch 102/300, seasonal_3 Loss: 0.0632 | 0.0291
Epoch 103/300, seasonal_3 Loss: 0.0630 | 0.0288
Epoch 104/300, seasonal_3 Loss: 0.0628 | 0.0287
Epoch 105/300, seasonal_3 Loss: 0.0627 | 0.0286
Epoch 106/300, seasonal_3 Loss: 0.0627 | 0.0284
Epoch 107/300, seasonal_3 Loss: 0.0626 | 0.0284
Epoch 108/300, seasonal_3 Loss: 0.0626 | 0.0283
Epoch 109/300, seasonal_3 Loss: 0.0625 | 0.0283
Epoch 110/300, seasonal_3 Loss: 0.0625 | 0.0282
Epoch 111/300, seasonal_3 Loss: 0.0625 | 0.0282
Epoch 112/300, seasonal_3 Loss: 0.0624 | 0.0281
Epoch 113/300, seasonal_3 Loss: 0.0624 | 0.0280
Epoch 114/300, seasonal_3 Loss: 0.0623 | 0.0280
Epoch 115/300, seasonal_3 Loss: 0.0623 | 0.0280
Epoch 116/300, seasonal_3 Loss: 0.0623 | 0.0279
Epoch 117/300, seasonal_3 Loss: 0.0623 | 0.0279
Epoch 118/300, seasonal_3 Loss: 0.0622 | 0.0278
Epoch 119/300, seasonal_3 Loss: 0.0622 | 0.0278
Epoch 120/300, seasonal_3 Loss: 0.0622 | 0.0278
Epoch 121/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 122/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 123/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 124/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 125/300, seasonal_3 Loss: 0.0621 | 0.0276
Epoch 126/300, seasonal_3 Loss: 0.0620 | 0.0276
Epoch 127/300, seasonal_3 Loss: 0.0620 | 0.0276
Epoch 128/300, seasonal_3 Loss: 0.0620 | 0.0276
Epoch 129/300, seasonal_3 Loss: 0.0620 | 0.0275
Epoch 130/300, seasonal_3 Loss: 0.0620 | 0.0275
Epoch 131/300, seasonal_3 Loss: 0.0619 | 0.0275
Epoch 132/300, seasonal_3 Loss: 0.0619 | 0.0275
Epoch 133/300, seasonal_3 Loss: 0.0619 | 0.0275
Epoch 134/300, seasonal_3 Loss: 0.0619 | 0.0274
Epoch 135/300, seasonal_3 Loss: 0.0619 | 0.0274
Epoch 136/300, seasonal_3 Loss: 0.0619 | 0.0274
Epoch 137/300, seasonal_3 Loss: 0.0618 | 0.0274
Epoch 138/300, seasonal_3 Loss: 0.0618 | 0.0274
Epoch 139/300, seasonal_3 Loss: 0.0618 | 0.0273
Epoch 140/300, seasonal_3 Loss: 0.0618 | 0.0273
Epoch 141/300, seasonal_3 Loss: 0.0618 | 0.0273
Epoch 142/300, seasonal_3 Loss: 0.0618 | 0.0273
Epoch 143/300, seasonal_3 Loss: 0.0618 | 0.0273
Epoch 144/300, seasonal_3 Loss: 0.0617 | 0.0273
Epoch 145/300, seasonal_3 Loss: 0.0617 | 0.0273
Epoch 146/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 147/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 148/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 149/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 150/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 151/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 152/300, seasonal_3 Loss: 0.0617 | 0.0272
Epoch 153/300, seasonal_3 Loss: 0.0616 | 0.0272
Epoch 154/300, seasonal_3 Loss: 0.0616 | 0.0272
Epoch 155/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 156/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 157/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 158/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 159/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 160/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 161/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 162/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 163/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 164/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 165/300, seasonal_3 Loss: 0.0616 | 0.0271
Epoch 166/300, seasonal_3 Loss: 0.0615 | 0.0271
Epoch 167/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 168/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 169/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 170/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 171/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 172/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 173/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 174/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 175/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 176/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 177/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 178/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 179/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 180/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 181/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 182/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 183/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 184/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 185/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 186/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 187/300, seasonal_3 Loss: 0.0615 | 0.0270
Epoch 188/300, seasonal_3 Loss: 0.0614 | 0.0270
Epoch 189/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 190/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 191/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 192/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 193/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 194/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 195/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 196/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 197/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 198/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 199/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 200/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 201/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 202/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 203/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 204/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 205/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 206/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 207/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 208/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 209/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 210/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 211/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 212/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 213/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 214/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 215/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 216/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 217/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 218/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 219/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 220/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 221/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 222/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 223/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 224/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 225/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 226/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 227/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 228/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 229/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 230/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 231/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 232/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 233/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 234/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 235/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 236/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 237/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 238/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 239/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 240/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 241/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 242/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 243/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 244/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 245/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 246/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 247/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 248/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 249/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 250/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 251/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 252/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 253/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 254/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 255/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 256/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 257/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 258/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 259/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 260/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 261/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 262/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 263/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 264/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 265/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 266/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 267/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 268/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 269/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 270/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 271/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 272/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 273/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 274/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 275/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 276/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 277/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 278/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 279/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 280/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 281/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 282/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 283/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 284/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 285/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 286/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 287/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 288/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 289/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 290/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 291/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 292/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 293/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 294/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 295/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 296/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 297/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 298/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 299/300, seasonal_3 Loss: 0.0614 | 0.0269
Epoch 300/300, seasonal_3 Loss: 0.0614 | 0.0269
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.979025897955739, 'learning_rate': 0.0009616859319551283, 'batch_size': 106, 'step_size': 4, 'gamma': 0.8937769318988896}
Epoch 1/300, resid Loss: 0.3439 | 0.1546
Epoch 2/300, resid Loss: 0.1248 | 0.1032
Epoch 3/300, resid Loss: 0.1038 | 0.0716
Epoch 4/300, resid Loss: 0.0984 | 0.0736
Epoch 5/300, resid Loss: 0.0961 | 0.0721
Epoch 6/300, resid Loss: 0.0917 | 0.0629
Epoch 7/300, resid Loss: 0.0882 | 0.0600
Epoch 8/300, resid Loss: 0.0877 | 0.0585
Epoch 9/300, resid Loss: 0.0892 | 0.0595
Epoch 10/300, resid Loss: 0.0893 | 0.0573
Epoch 11/300, resid Loss: 0.0890 | 0.0671
Epoch 12/300, resid Loss: 0.1041 | 0.1156
Epoch 13/300, resid Loss: 0.1046 | 0.0661
Epoch 14/300, resid Loss: 0.0938 | 0.0562
Epoch 15/300, resid Loss: 0.0861 | 0.0597
Epoch 16/300, resid Loss: 0.0860 | 0.0556
Epoch 17/300, resid Loss: 0.0832 | 0.0546
Epoch 18/300, resid Loss: 0.0809 | 0.0533
Epoch 19/300, resid Loss: 0.0802 | 0.0511
Epoch 20/300, resid Loss: 0.0804 | 0.0499
Epoch 21/300, resid Loss: 0.0813 | 0.0514
Epoch 22/300, resid Loss: 0.0827 | 0.0541
Epoch 23/300, resid Loss: 0.0823 | 0.0581
Epoch 24/300, resid Loss: 0.0805 | 0.0578
Epoch 25/300, resid Loss: 0.0786 | 0.0599
Epoch 26/300, resid Loss: 0.0866 | 0.0511
Epoch 27/300, resid Loss: 0.0981 | 0.0835
Epoch 28/300, resid Loss: 0.0876 | 0.0632
Epoch 29/300, resid Loss: 0.0809 | 0.0668
Epoch 30/300, resid Loss: 0.0805 | 0.0570
Epoch 31/300, resid Loss: 0.0765 | 0.0536
Epoch 32/300, resid Loss: 0.0758 | 0.0504
Epoch 33/300, resid Loss: 0.0781 | 0.0471
Epoch 34/300, resid Loss: 0.0813 | 0.0475
Epoch 35/300, resid Loss: 0.0844 | 0.0579
Epoch 36/300, resid Loss: 0.0830 | 0.0641
Epoch 37/300, resid Loss: 0.0779 | 0.0554
Epoch 38/300, resid Loss: 0.0737 | 0.0432
Epoch 39/300, resid Loss: 0.0717 | 0.0450
Epoch 40/300, resid Loss: 0.0694 | 0.0438
Epoch 41/300, resid Loss: 0.0678 | 0.0432
Epoch 42/300, resid Loss: 0.0679 | 0.0433
Epoch 43/300, resid Loss: 0.0679 | 0.0434
Epoch 44/300, resid Loss: 0.0678 | 0.0432
Epoch 45/300, resid Loss: 0.0675 | 0.0433
Epoch 46/300, resid Loss: 0.0670 | 0.0428
Epoch 47/300, resid Loss: 0.0662 | 0.0424
Epoch 48/300, resid Loss: 0.0655 | 0.0420
Epoch 49/300, resid Loss: 0.0651 | 0.0419
Epoch 50/300, resid Loss: 0.0649 | 0.0418
Epoch 51/300, resid Loss: 0.0649 | 0.0417
Epoch 52/300, resid Loss: 0.0649 | 0.0416
Epoch 53/300, resid Loss: 0.0647 | 0.0414
Epoch 54/300, resid Loss: 0.0645 | 0.0413
Epoch 55/300, resid Loss: 0.0643 | 0.0412
Epoch 56/300, resid Loss: 0.0641 | 0.0411
Epoch 57/300, resid Loss: 0.0640 | 0.0411
Epoch 58/300, resid Loss: 0.0639 | 0.0410
Epoch 59/300, resid Loss: 0.0638 | 0.0409
Epoch 60/300, resid Loss: 0.0637 | 0.0409
Epoch 61/300, resid Loss: 0.0636 | 0.0408
Epoch 62/300, resid Loss: 0.0635 | 0.0408
Epoch 63/300, resid Loss: 0.0635 | 0.0407
Epoch 64/300, resid Loss: 0.0634 | 0.0407
Epoch 65/300, resid Loss: 0.0633 | 0.0406
Epoch 66/300, resid Loss: 0.0633 | 0.0406
Epoch 67/300, resid Loss: 0.0632 | 0.0405
Epoch 68/300, resid Loss: 0.0632 | 0.0405
Epoch 69/300, resid Loss: 0.0631 | 0.0405
Epoch 70/300, resid Loss: 0.0631 | 0.0404
Epoch 71/300, resid Loss: 0.0630 | 0.0404
Epoch 72/300, resid Loss: 0.0630 | 0.0403
Epoch 73/300, resid Loss: 0.0629 | 0.0403
Epoch 74/300, resid Loss: 0.0629 | 0.0403
Epoch 75/300, resid Loss: 0.0628 | 0.0402
Epoch 76/300, resid Loss: 0.0628 | 0.0402
Epoch 77/300, resid Loss: 0.0628 | 0.0402
Epoch 78/300, resid Loss: 0.0627 | 0.0402
Epoch 79/300, resid Loss: 0.0627 | 0.0401
Epoch 80/300, resid Loss: 0.0627 | 0.0401
Epoch 81/300, resid Loss: 0.0626 | 0.0401
Epoch 82/300, resid Loss: 0.0626 | 0.0400
Epoch 83/300, resid Loss: 0.0626 | 0.0400
Epoch 84/300, resid Loss: 0.0625 | 0.0400
Epoch 85/300, resid Loss: 0.0625 | 0.0400
Epoch 86/300, resid Loss: 0.0625 | 0.0400
Epoch 87/300, resid Loss: 0.0625 | 0.0399
Epoch 88/300, resid Loss: 0.0624 | 0.0399
Epoch 89/300, resid Loss: 0.0624 | 0.0399
Epoch 90/300, resid Loss: 0.0624 | 0.0399
Epoch 91/300, resid Loss: 0.0624 | 0.0399
Epoch 92/300, resid Loss: 0.0624 | 0.0399
Epoch 93/300, resid Loss: 0.0623 | 0.0398
Epoch 94/300, resid Loss: 0.0623 | 0.0398
Epoch 95/300, resid Loss: 0.0623 | 0.0398
Epoch 96/300, resid Loss: 0.0623 | 0.0398
Epoch 97/300, resid Loss: 0.0623 | 0.0398
Epoch 98/300, resid Loss: 0.0623 | 0.0398
Epoch 99/300, resid Loss: 0.0623 | 0.0398
Epoch 100/300, resid Loss: 0.0622 | 0.0397
Epoch 101/300, resid Loss: 0.0622 | 0.0397
Epoch 102/300, resid Loss: 0.0622 | 0.0397
Epoch 103/300, resid Loss: 0.0622 | 0.0397
Epoch 104/300, resid Loss: 0.0622 | 0.0397
Epoch 105/300, resid Loss: 0.0622 | 0.0397
Epoch 106/300, resid Loss: 0.0622 | 0.0397
Epoch 107/300, resid Loss: 0.0622 | 0.0397
Epoch 108/300, resid Loss: 0.0622 | 0.0397
Epoch 109/300, resid Loss: 0.0622 | 0.0397
Epoch 110/300, resid Loss: 0.0621 | 0.0397
Epoch 111/300, resid Loss: 0.0621 | 0.0396
Epoch 112/300, resid Loss: 0.0621 | 0.0396
Epoch 113/300, resid Loss: 0.0621 | 0.0396
Epoch 114/300, resid Loss: 0.0621 | 0.0396
Epoch 115/300, resid Loss: 0.0621 | 0.0396
Epoch 116/300, resid Loss: 0.0621 | 0.0396
Epoch 117/300, resid Loss: 0.0621 | 0.0396
Epoch 118/300, resid Loss: 0.0621 | 0.0396
Epoch 119/300, resid Loss: 0.0621 | 0.0396
Epoch 120/300, resid Loss: 0.0621 | 0.0396
Epoch 121/300, resid Loss: 0.0621 | 0.0396
Epoch 122/300, resid Loss: 0.0621 | 0.0396
Epoch 123/300, resid Loss: 0.0621 | 0.0396
Epoch 124/300, resid Loss: 0.0621 | 0.0396
Epoch 125/300, resid Loss: 0.0621 | 0.0396
Epoch 126/300, resid Loss: 0.0621 | 0.0396
Epoch 127/300, resid Loss: 0.0621 | 0.0396
Epoch 128/300, resid Loss: 0.0621 | 0.0396
Epoch 129/300, resid Loss: 0.0620 | 0.0396
Epoch 130/300, resid Loss: 0.0620 | 0.0396
Epoch 131/300, resid Loss: 0.0620 | 0.0396
Epoch 132/300, resid Loss: 0.0620 | 0.0396
Epoch 133/300, resid Loss: 0.0620 | 0.0396
Epoch 134/300, resid Loss: 0.0620 | 0.0396
Epoch 135/300, resid Loss: 0.0620 | 0.0396
Epoch 136/300, resid Loss: 0.0620 | 0.0396
Epoch 137/300, resid Loss: 0.0620 | 0.0395
Epoch 138/300, resid Loss: 0.0620 | 0.0395
Epoch 139/300, resid Loss: 0.0620 | 0.0395
Epoch 140/300, resid Loss: 0.0620 | 0.0395
Epoch 141/300, resid Loss: 0.0620 | 0.0395
Epoch 142/300, resid Loss: 0.0620 | 0.0395
Epoch 143/300, resid Loss: 0.0620 | 0.0395
Epoch 144/300, resid Loss: 0.0620 | 0.0395
Epoch 145/300, resid Loss: 0.0620 | 0.0395
Epoch 146/300, resid Loss: 0.0620 | 0.0395
Epoch 147/300, resid Loss: 0.0620 | 0.0395
Epoch 148/300, resid Loss: 0.0620 | 0.0395
Epoch 149/300, resid Loss: 0.0620 | 0.0395
Epoch 150/300, resid Loss: 0.0620 | 0.0395
Epoch 151/300, resid Loss: 0.0620 | 0.0395
Epoch 152/300, resid Loss: 0.0620 | 0.0395
Epoch 153/300, resid Loss: 0.0620 | 0.0395
Epoch 154/300, resid Loss: 0.0620 | 0.0395
Epoch 155/300, resid Loss: 0.0620 | 0.0395
Epoch 156/300, resid Loss: 0.0620 | 0.0395
Epoch 157/300, resid Loss: 0.0620 | 0.0395
Epoch 158/300, resid Loss: 0.0620 | 0.0395
Epoch 159/300, resid Loss: 0.0620 | 0.0395
Epoch 160/300, resid Loss: 0.0620 | 0.0395
Epoch 161/300, resid Loss: 0.0620 | 0.0395
Epoch 162/300, resid Loss: 0.0620 | 0.0395
Epoch 163/300, resid Loss: 0.0620 | 0.0395
Epoch 164/300, resid Loss: 0.0620 | 0.0395
Epoch 165/300, resid Loss: 0.0620 | 0.0395
Epoch 166/300, resid Loss: 0.0620 | 0.0395
Epoch 167/300, resid Loss: 0.0620 | 0.0395
Epoch 168/300, resid Loss: 0.0620 | 0.0395
Epoch 169/300, resid Loss: 0.0620 | 0.0395
Epoch 170/300, resid Loss: 0.0620 | 0.0395
Epoch 171/300, resid Loss: 0.0620 | 0.0395
Epoch 172/300, resid Loss: 0.0620 | 0.0395
Epoch 173/300, resid Loss: 0.0620 | 0.0395
Epoch 174/300, resid Loss: 0.0620 | 0.0395
Epoch 175/300, resid Loss: 0.0620 | 0.0395
Epoch 176/300, resid Loss: 0.0620 | 0.0395
Epoch 177/300, resid Loss: 0.0620 | 0.0395
Epoch 178/300, resid Loss: 0.0620 | 0.0395
Epoch 179/300, resid Loss: 0.0620 | 0.0395
Epoch 180/300, resid Loss: 0.0620 | 0.0395
Epoch 181/300, resid Loss: 0.0620 | 0.0395
Epoch 182/300, resid Loss: 0.0620 | 0.0395
Epoch 183/300, resid Loss: 0.0620 | 0.0395
Epoch 184/300, resid Loss: 0.0620 | 0.0395
Epoch 185/300, resid Loss: 0.0620 | 0.0395
Epoch 186/300, resid Loss: 0.0620 | 0.0395
Epoch 187/300, resid Loss: 0.0620 | 0.0395
Epoch 188/300, resid Loss: 0.0620 | 0.0395
Epoch 189/300, resid Loss: 0.0620 | 0.0395
Epoch 190/300, resid Loss: 0.0620 | 0.0395
Epoch 191/300, resid Loss: 0.0620 | 0.0395
Epoch 192/300, resid Loss: 0.0620 | 0.0395
Epoch 193/300, resid Loss: 0.0620 | 0.0395
Epoch 194/300, resid Loss: 0.0620 | 0.0395
Epoch 195/300, resid Loss: 0.0620 | 0.0395
Epoch 196/300, resid Loss: 0.0620 | 0.0395
Epoch 197/300, resid Loss: 0.0620 | 0.0395
Epoch 198/300, resid Loss: 0.0620 | 0.0395
Epoch 199/300, resid Loss: 0.0620 | 0.0395
Epoch 200/300, resid Loss: 0.0620 | 0.0395
Epoch 201/300, resid Loss: 0.0620 | 0.0395
Epoch 202/300, resid Loss: 0.0620 | 0.0395
Epoch 203/300, resid Loss: 0.0620 | 0.0395
Epoch 204/300, resid Loss: 0.0620 | 0.0395
Epoch 205/300, resid Loss: 0.0620 | 0.0395
Epoch 206/300, resid Loss: 0.0620 | 0.0395
Epoch 207/300, resid Loss: 0.0620 | 0.0395
Epoch 208/300, resid Loss: 0.0620 | 0.0395
Epoch 209/300, resid Loss: 0.0620 | 0.0395
Epoch 210/300, resid Loss: 0.0620 | 0.0395
Epoch 211/300, resid Loss: 0.0620 | 0.0395
Epoch 212/300, resid Loss: 0.0620 | 0.0395
Epoch 213/300, resid Loss: 0.0620 | 0.0395
Epoch 214/300, resid Loss: 0.0620 | 0.0395
Epoch 215/300, resid Loss: 0.0620 | 0.0395
Epoch 216/300, resid Loss: 0.0620 | 0.0395
Epoch 217/300, resid Loss: 0.0620 | 0.0395
Epoch 218/300, resid Loss: 0.0620 | 0.0395
Epoch 219/300, resid Loss: 0.0620 | 0.0395
Epoch 220/300, resid Loss: 0.0620 | 0.0395
Epoch 221/300, resid Loss: 0.0620 | 0.0395
Epoch 222/300, resid Loss: 0.0620 | 0.0395
Epoch 223/300, resid Loss: 0.0620 | 0.0395
Epoch 224/300, resid Loss: 0.0620 | 0.0395
Epoch 225/300, resid Loss: 0.0620 | 0.0395
Epoch 226/300, resid Loss: 0.0620 | 0.0395
Epoch 227/300, resid Loss: 0.0620 | 0.0395
Epoch 228/300, resid Loss: 0.0620 | 0.0395
Epoch 229/300, resid Loss: 0.0620 | 0.0395
Epoch 230/300, resid Loss: 0.0620 | 0.0395
Early stopping for resid
Runtime (seconds): 1547.705405473709
0.00020995956511775144
[217.56668]
[0.03664499]
[-3.4289901]
[3.431423]
[0.43677604]
[10.978527]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 34.9151586452499
RMSE: 5.908905029296875
MAE: 5.908905029296875
R-squared: nan
[229.02109]
