ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-02 06:46:11,839][0m A new study created in memory with name: no-name-a1dd77c4-26c2-46b0-ac06-a249ee6ea48d[0m
[32m[I 2025-02-02 06:46:40,423][0m Trial 0 finished with value: 0.19948046006378398 and parameters: {'observation_period_num': 72, 'train_rates': 0.6211051497477198, 'learning_rate': 1.9957164546565276e-05, 'batch_size': 112, 'step_size': 3, 'gamma': 0.9445419699944452}. Best is trial 0 with value: 0.19948046006378398.[0m
[32m[I 2025-02-02 06:47:07,516][0m Trial 1 finished with value: 0.6649564454772255 and parameters: {'observation_period_num': 178, 'train_rates': 0.7027065254485535, 'learning_rate': 3.7539996563601097e-06, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8256493529727826}. Best is trial 0 with value: 0.19948046006378398.[0m
[32m[I 2025-02-02 06:47:34,299][0m Trial 2 finished with value: 0.25363824786314 and parameters: {'observation_period_num': 155, 'train_rates': 0.7264774057761667, 'learning_rate': 8.272340773244354e-05, 'batch_size': 163, 'step_size': 1, 'gamma': 0.9443652124009873}. Best is trial 0 with value: 0.19948046006378398.[0m
[32m[I 2025-02-02 06:48:05,472][0m Trial 3 finished with value: 0.2868958202455818 and parameters: {'observation_period_num': 181, 'train_rates': 0.6043983513887584, 'learning_rate': 4.5543730898182825e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8793332895527598}. Best is trial 0 with value: 0.19948046006378398.[0m
[32m[I 2025-02-02 06:48:34,759][0m Trial 4 finished with value: 0.17499810791221157 and parameters: {'observation_period_num': 161, 'train_rates': 0.8753706249648211, 'learning_rate': 0.0005290016312320235, 'batch_size': 169, 'step_size': 4, 'gamma': 0.7764388635371009}. Best is trial 4 with value: 0.17499810791221157.[0m
[32m[I 2025-02-02 06:49:51,067][0m Trial 5 finished with value: 0.1118595670474498 and parameters: {'observation_period_num': 67, 'train_rates': 0.6462582852650639, 'learning_rate': 0.00037800228825173005, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9071350396145398}. Best is trial 5 with value: 0.1118595670474498.[0m
[32m[I 2025-02-02 06:50:40,215][0m Trial 6 finished with value: 0.26357213458083323 and parameters: {'observation_period_num': 54, 'train_rates': 0.7397110494496102, 'learning_rate': 3.398671099145148e-05, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8902165057329383}. Best is trial 5 with value: 0.1118595670474498.[0m
[32m[I 2025-02-02 06:51:10,178][0m Trial 7 finished with value: 0.39580858480639575 and parameters: {'observation_period_num': 150, 'train_rates': 0.9125140447909539, 'learning_rate': 1.0760002925621963e-05, 'batch_size': 194, 'step_size': 10, 'gamma': 0.8874743402705312}. Best is trial 5 with value: 0.1118595670474498.[0m
[32m[I 2025-02-02 06:52:17,670][0m Trial 8 finished with value: 0.13123087953416662 and parameters: {'observation_period_num': 57, 'train_rates': 0.6848874786723733, 'learning_rate': 3.9171006760080886e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.7719474758579312}. Best is trial 5 with value: 0.1118595670474498.[0m
[32m[I 2025-02-02 06:52:52,007][0m Trial 9 finished with value: 0.15483337364956776 and parameters: {'observation_period_num': 121, 'train_rates': 0.8625484030291464, 'learning_rate': 3.995955662528087e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9468163388716525}. Best is trial 5 with value: 0.1118595670474498.[0m
[32m[I 2025-02-02 06:55:28,770][0m Trial 10 finished with value: 0.0749854815857751 and parameters: {'observation_period_num': 251, 'train_rates': 0.9872917657626628, 'learning_rate': 0.0008088262167310116, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9828320053774495}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 06:58:31,057][0m Trial 11 finished with value: 0.1557798690122107 and parameters: {'observation_period_num': 249, 'train_rates': 0.9748216772185577, 'learning_rate': 0.0006366895770762856, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9849952737409606}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:00:48,716][0m Trial 12 finished with value: 0.08816917861717712 and parameters: {'observation_period_num': 6, 'train_rates': 0.8054187407741231, 'learning_rate': 0.00018486429731335593, 'batch_size': 25, 'step_size': 15, 'gamma': 0.9818483428415076}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:01:15,641][0m Trial 13 finished with value: 0.09123961759884638 and parameters: {'observation_period_num': 10, 'train_rates': 0.7897946965645395, 'learning_rate': 0.0002195464239723609, 'batch_size': 248, 'step_size': 15, 'gamma': 0.9884007409666018}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:02:09,580][0m Trial 14 finished with value: 0.09707604348659515 and parameters: {'observation_period_num': 251, 'train_rates': 0.9871510402873792, 'learning_rate': 0.00018312449517287428, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9605871922873747}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:05:25,114][0m Trial 15 finished with value: 0.08073019861595201 and parameters: {'observation_period_num': 10, 'train_rates': 0.8101349819554112, 'learning_rate': 0.0009921453389884368, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8364930747289281}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:06:36,400][0m Trial 16 finished with value: 0.29775085477601915 and parameters: {'observation_period_num': 214, 'train_rates': 0.9312998755334568, 'learning_rate': 0.0009120087254986339, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8386559982306417}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:07:15,360][0m Trial 17 finished with value: 0.7112162261113634 and parameters: {'observation_period_num': 103, 'train_rates': 0.8079205063476748, 'learning_rate': 1.251110219510596e-06, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8379183566584724}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:10:07,103][0m Trial 18 finished with value: 0.21695078539641263 and parameters: {'observation_period_num': 208, 'train_rates': 0.8532926748295617, 'learning_rate': 9.960320240698186e-05, 'batch_size': 20, 'step_size': 8, 'gamma': 0.8004330633921498}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:10:37,916][0m Trial 19 finished with value: 0.20300334692001343 and parameters: {'observation_period_num': 33, 'train_rates': 0.9401924089210137, 'learning_rate': 0.0009516052180672242, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8568816021849984}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:11:09,610][0m Trial 20 finished with value: 0.12537631259997167 and parameters: {'observation_period_num': 103, 'train_rates': 0.7848215285223167, 'learning_rate': 0.00034755209983604004, 'batch_size': 131, 'step_size': 8, 'gamma': 0.9208999389417285}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:14:54,590][0m Trial 21 finished with value: 0.10411633222053449 and parameters: {'observation_period_num': 21, 'train_rates': 0.8260791546104723, 'learning_rate': 0.0002165219740227639, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9667998846687318}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:16:28,083][0m Trial 22 finished with value: 0.08566673855175791 and parameters: {'observation_period_num': 36, 'train_rates': 0.7477847972300726, 'learning_rate': 0.00012092012395339682, 'batch_size': 40, 'step_size': 14, 'gamma': 0.9085906355796123}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:17:47,251][0m Trial 23 finished with value: 0.08352221234040803 and parameters: {'observation_period_num': 31, 'train_rates': 0.7548414095726679, 'learning_rate': 9.041208367094639e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.9186572215034698}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:18:33,794][0m Trial 24 finished with value: 0.18927400739028535 and parameters: {'observation_period_num': 85, 'train_rates': 0.7674826314454606, 'learning_rate': 1.4770388166302631e-05, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8112402007931839}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:19:52,152][0m Trial 25 finished with value: 0.18572195865332133 and parameters: {'observation_period_num': 41, 'train_rates': 0.8920051321523174, 'learning_rate': 6.811693624820539e-06, 'batch_size': 53, 'step_size': 14, 'gamma': 0.9252219874316289}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:21:16,976][0m Trial 26 finished with value: 0.21914486269201708 and parameters: {'observation_period_num': 225, 'train_rates': 0.6804348355320057, 'learning_rate': 0.0004872433750572445, 'batch_size': 38, 'step_size': 12, 'gamma': 0.864543503765614}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:22:15,189][0m Trial 27 finished with value: 0.12759564536158993 and parameters: {'observation_period_num': 125, 'train_rates': 0.8349101345818298, 'learning_rate': 6.920252004316223e-05, 'batch_size': 68, 'step_size': 14, 'gamma': 0.8527048639912694}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:22:59,848][0m Trial 28 finished with value: 0.3667925749631489 and parameters: {'observation_period_num': 86, 'train_rates': 0.9644922107526239, 'learning_rate': 0.0003065894945520942, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9250053202249244}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:24:28,418][0m Trial 29 finished with value: 0.14234840539014507 and parameters: {'observation_period_num': 22, 'train_rates': 0.6582231881433385, 'learning_rate': 0.000962734643480203, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9466867065336689}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:25:00,417][0m Trial 30 finished with value: 0.08952619596395527 and parameters: {'observation_period_num': 52, 'train_rates': 0.7098630790914415, 'learning_rate': 0.000631115023274667, 'batch_size': 110, 'step_size': 14, 'gamma': 0.7539055493784621}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:26:11,662][0m Trial 31 finished with value: 0.09193213094364514 and parameters: {'observation_period_num': 38, 'train_rates': 0.7735275102715446, 'learning_rate': 0.00012173053445705002, 'batch_size': 48, 'step_size': 14, 'gamma': 0.9036100065155076}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:27:55,324][0m Trial 32 finished with value: 0.08898064186391623 and parameters: {'observation_period_num': 24, 'train_rates': 0.7487055988527732, 'learning_rate': 0.0001366401087335763, 'batch_size': 32, 'step_size': 13, 'gamma': 0.9094884324740787}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:28:51,666][0m Trial 33 finished with value: 0.13008185085992735 and parameters: {'observation_period_num': 69, 'train_rates': 0.7498068245745185, 'learning_rate': 2.3490900535039646e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.8729571155689597}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:29:37,253][0m Trial 34 finished with value: 0.07756291219836794 and parameters: {'observation_period_num': 5, 'train_rates': 0.7230776340457121, 'learning_rate': 6.469436415505805e-05, 'batch_size': 83, 'step_size': 11, 'gamma': 0.9387716297034601}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:30:22,512][0m Trial 35 finished with value: 0.1674005069177259 and parameters: {'observation_period_num': 187, 'train_rates': 0.7135923632783071, 'learning_rate': 6.76739392303026e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9631613405822157}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:30:49,504][0m Trial 36 finished with value: 0.20056213239828746 and parameters: {'observation_period_num': 6, 'train_rates': 0.6284800462874551, 'learning_rate': 3.5539565136980148e-06, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9339497400188143}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:34:20,749][0m Trial 37 finished with value: 0.08144841114891337 and parameters: {'observation_period_num': 20, 'train_rates': 0.6793841371164742, 'learning_rate': 5.4874272903088836e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9703294724579894}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:35:57,000][0m Trial 38 finished with value: 0.13645538721942796 and parameters: {'observation_period_num': 51, 'train_rates': 0.6851458612810888, 'learning_rate': 1.8523001722386775e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.9711548816076084}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:36:42,457][0m Trial 39 finished with value: 0.13655773048167644 and parameters: {'observation_period_num': 145, 'train_rates': 0.6395589025773614, 'learning_rate': 5.65430787736782e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.9548012040546164}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:37:06,514][0m Trial 40 finished with value: 0.4475053448553667 and parameters: {'observation_period_num': 163, 'train_rates': 0.6119932876449972, 'learning_rate': 7.01075480182672e-06, 'batch_size': 180, 'step_size': 3, 'gamma': 0.9770020935170323}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:39:13,989][0m Trial 41 finished with value: 0.07539926567023583 and parameters: {'observation_period_num': 19, 'train_rates': 0.7217418140887106, 'learning_rate': 5.2041722874966387e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.9428953314633212}. Best is trial 10 with value: 0.0749854815857751.[0m
[32m[I 2025-02-02 07:41:11,627][0m Trial 42 finished with value: 0.06928334628382772 and parameters: {'observation_period_num': 16, 'train_rates': 0.6659674472424055, 'learning_rate': 5.776884749770517e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.938888273377653}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:42:50,693][0m Trial 43 finished with value: 0.11168412210441861 and parameters: {'observation_period_num': 15, 'train_rates': 0.6553379131294946, 'learning_rate': 3.0396197637237662e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9371012194694299}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:43:30,605][0m Trial 44 finished with value: 0.06978562453016177 and parameters: {'observation_period_num': 5, 'train_rates': 0.7262108684136603, 'learning_rate': 0.0004400998837021268, 'batch_size': 82, 'step_size': 9, 'gamma': 0.8952245916011436}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:44:02,654][0m Trial 45 finished with value: 0.12625683984596736 and parameters: {'observation_period_num': 47, 'train_rates': 0.722756521059387, 'learning_rate': 2.6631020722367422e-05, 'batch_size': 110, 'step_size': 10, 'gamma': 0.8932593980992133}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:44:30,923][0m Trial 46 finished with value: 0.10045877575177309 and parameters: {'observation_period_num': 65, 'train_rates': 0.7261019238290932, 'learning_rate': 0.0004998085799586096, 'batch_size': 149, 'step_size': 9, 'gamma': 0.9366230575356933}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:45:07,338][0m Trial 47 finished with value: 0.16214328466572608 and parameters: {'observation_period_num': 138, 'train_rates': 0.6924139554520653, 'learning_rate': 0.00028301401573167963, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9467529941113015}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:45:35,317][0m Trial 48 finished with value: 0.11925704651118073 and parameters: {'observation_period_num': 81, 'train_rates': 0.6668264031572043, 'learning_rate': 3.8437909443958026e-05, 'batch_size': 123, 'step_size': 10, 'gamma': 0.9561021047440154}. Best is trial 42 with value: 0.06928334628382772.[0m
[32m[I 2025-02-02 07:46:10,451][0m Trial 49 finished with value: 0.1911624656582645 and parameters: {'observation_period_num': 238, 'train_rates': 0.7019293272201428, 'learning_rate': 0.0006613617911935448, 'batch_size': 95, 'step_size': 11, 'gamma': 0.8957074736600674}. Best is trial 42 with value: 0.06928334628382772.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-02 07:46:10,458][0m A new study created in memory with name: no-name-bff4a02a-e53e-4933-bbeb-1b959142b9e5[0m
[32m[I 2025-02-02 07:46:40,791][0m Trial 0 finished with value: 0.32796525955200195 and parameters: {'observation_period_num': 193, 'train_rates': 0.9558702363373115, 'learning_rate': 0.00047026171058591067, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9001792967460966}. Best is trial 0 with value: 0.32796525955200195.[0m
[32m[I 2025-02-02 07:47:28,033][0m Trial 1 finished with value: 0.2980513847874899 and parameters: {'observation_period_num': 236, 'train_rates': 0.9497847668696049, 'learning_rate': 0.00011446790196698896, 'batch_size': 84, 'step_size': 8, 'gamma': 0.8099845311927679}. Best is trial 1 with value: 0.2980513847874899.[0m
[32m[I 2025-02-02 07:47:59,195][0m Trial 2 finished with value: 0.44397106766700745 and parameters: {'observation_period_num': 104, 'train_rates': 0.9785288249744553, 'learning_rate': 5.156735170094343e-06, 'batch_size': 227, 'step_size': 15, 'gamma': 0.8467934821121976}. Best is trial 1 with value: 0.2980513847874899.[0m
[32m[I 2025-02-02 07:49:02,365][0m Trial 3 finished with value: 0.299755866606007 and parameters: {'observation_period_num': 234, 'train_rates': 0.7430028981636924, 'learning_rate': 6.880208793955552e-06, 'batch_size': 46, 'step_size': 12, 'gamma': 0.8471979583357477}. Best is trial 1 with value: 0.2980513847874899.[0m
[32m[I 2025-02-02 07:51:15,318][0m Trial 4 finished with value: 0.07854232057474853 and parameters: {'observation_period_num': 8, 'train_rates': 0.738067519191119, 'learning_rate': 0.0001571159722474044, 'batch_size': 23, 'step_size': 11, 'gamma': 0.8857081924786031}. Best is trial 4 with value: 0.07854232057474853.[0m
[32m[I 2025-02-02 07:51:47,913][0m Trial 5 finished with value: 0.05324200168251991 and parameters: {'observation_period_num': 49, 'train_rates': 0.9812210116025188, 'learning_rate': 0.00016331834340772313, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8692557219863031}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:52:33,802][0m Trial 6 finished with value: 0.21225739379671857 and parameters: {'observation_period_num': 64, 'train_rates': 0.933958394329583, 'learning_rate': 6.938042947995707e-05, 'batch_size': 81, 'step_size': 6, 'gamma': 0.933982992006999}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:53:06,331][0m Trial 7 finished with value: 0.18975322914597456 and parameters: {'observation_period_num': 233, 'train_rates': 0.8797539472884572, 'learning_rate': 0.0003472982313626472, 'batch_size': 110, 'step_size': 2, 'gamma': 0.9188151064254451}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:54:31,398][0m Trial 8 finished with value: 0.1127506864102346 and parameters: {'observation_period_num': 35, 'train_rates': 0.8703620019141836, 'learning_rate': 0.0004879744165625596, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9351901638455133}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:55:08,546][0m Trial 9 finished with value: 0.9146035891378969 and parameters: {'observation_period_num': 140, 'train_rates': 0.6055631543307578, 'learning_rate': 1.1118489125053554e-06, 'batch_size': 73, 'step_size': 8, 'gamma': 0.7639556847802037}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:55:36,745][0m Trial 10 finished with value: 0.17540277037668142 and parameters: {'observation_period_num': 111, 'train_rates': 0.8061237057282512, 'learning_rate': 3.218353856052867e-05, 'batch_size': 167, 'step_size': 15, 'gamma': 0.977790113064049}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:56:03,128][0m Trial 11 finished with value: 0.08597378879352365 and parameters: {'observation_period_num': 9, 'train_rates': 0.7079672256681951, 'learning_rate': 0.0001393232490128226, 'batch_size': 161, 'step_size': 11, 'gamma': 0.8765224091314225}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:56:27,217][0m Trial 12 finished with value: 0.18230340038854803 and parameters: {'observation_period_num': 58, 'train_rates': 0.6657926900943227, 'learning_rate': 3.26333471133345e-05, 'batch_size': 254, 'step_size': 12, 'gamma': 0.8158339745213283}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:56:57,374][0m Trial 13 finished with value: 0.08466450764593403 and parameters: {'observation_period_num': 6, 'train_rates': 0.7819032857829722, 'learning_rate': 0.0008504708171700859, 'batch_size': 142, 'step_size': 10, 'gamma': 0.8783334828128743}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:57:27,163][0m Trial 14 finished with value: 0.12875364719187482 and parameters: {'observation_period_num': 68, 'train_rates': 0.8421209345747325, 'learning_rate': 0.000155361632290327, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9886152386353191}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 07:57:59,509][0m Trial 15 finished with value: 0.22293622883710457 and parameters: {'observation_period_num': 35, 'train_rates': 0.6778048440920518, 'learning_rate': 8.986993367854232e-06, 'batch_size': 116, 'step_size': 10, 'gamma': 0.8362922599565984}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:01:17,062][0m Trial 16 finished with value: 0.15194068705155092 and parameters: {'observation_period_num': 151, 'train_rates': 0.7487057650450601, 'learning_rate': 5.581638423406236e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7643352362933115}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:01:47,731][0m Trial 17 finished with value: 0.1762876695405275 and parameters: {'observation_period_num': 89, 'train_rates': 0.9035621402985801, 'learning_rate': 0.00023127131066923073, 'batch_size': 177, 'step_size': 9, 'gamma': 0.899628012612112}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:02:18,822][0m Trial 18 finished with value: 0.13766202583681536 and parameters: {'observation_period_num': 32, 'train_rates': 0.8206210771290083, 'learning_rate': 1.9998043190391216e-05, 'batch_size': 142, 'step_size': 6, 'gamma': 0.962139476195104}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:02:47,602][0m Trial 19 finished with value: 0.15888149275512134 and parameters: {'observation_period_num': 80, 'train_rates': 0.6216312850392636, 'learning_rate': 7.213494660699754e-05, 'batch_size': 116, 'step_size': 13, 'gamma': 0.7945520907846848}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:03:13,411][0m Trial 20 finished with value: 0.09712799080571938 and parameters: {'observation_period_num': 36, 'train_rates': 0.7762435761862992, 'learning_rate': 0.0009744188551740912, 'batch_size': 252, 'step_size': 11, 'gamma': 0.8645099357741763}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:03:43,986][0m Trial 21 finished with value: 0.08069452141598413 and parameters: {'observation_period_num': 15, 'train_rates': 0.7674625847237255, 'learning_rate': 0.0007225580268850493, 'batch_size': 144, 'step_size': 10, 'gamma': 0.8813632648845147}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:04:12,505][0m Trial 22 finished with value: 0.08686896172570593 and parameters: {'observation_period_num': 11, 'train_rates': 0.724759358353133, 'learning_rate': 0.0002503521568387519, 'batch_size': 191, 'step_size': 7, 'gamma': 0.9000142573621269}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:04:40,213][0m Trial 23 finished with value: 0.09133779705287172 and parameters: {'observation_period_num': 43, 'train_rates': 0.6782347047885547, 'learning_rate': 0.0005180869673629508, 'batch_size': 153, 'step_size': 10, 'gamma': 0.8693699771650469}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:05:13,324][0m Trial 24 finished with value: 0.08850803971290588 and parameters: {'observation_period_num': 20, 'train_rates': 0.764899343321232, 'learning_rate': 0.0002274267759945534, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8913307079575321}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:05:54,832][0m Trial 25 finished with value: 0.11920352232340882 and parameters: {'observation_period_num': 60, 'train_rates': 0.8304207887043148, 'learning_rate': 0.00011910072621394963, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9166019002298121}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:06:19,181][0m Trial 26 finished with value: 0.14961440591039238 and parameters: {'observation_period_num': 165, 'train_rates': 0.7060862454757756, 'learning_rate': 0.0006609504409113419, 'batch_size': 219, 'step_size': 9, 'gamma': 0.8541813566202151}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:06:44,003][0m Trial 27 finished with value: 0.2482933655078275 and parameters: {'observation_period_num': 48, 'train_rates': 0.64440068848188, 'learning_rate': 1.6581625175388244e-05, 'batch_size': 182, 'step_size': 14, 'gamma': 0.8293252412767511}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:08:02,502][0m Trial 28 finished with value: 0.14662820232602267 and parameters: {'observation_period_num': 86, 'train_rates': 0.8638865706297862, 'learning_rate': 0.0003219764509282862, 'batch_size': 43, 'step_size': 12, 'gamma': 0.9469790846688354}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:08:28,935][0m Trial 29 finished with value: 0.15689404426718787 and parameters: {'observation_period_num': 192, 'train_rates': 0.7986413770468506, 'learning_rate': 0.0003960476286413761, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8858202581304664}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:09:01,703][0m Trial 30 finished with value: 0.17469607280418936 and parameters: {'observation_period_num': 27, 'train_rates': 0.9229399241652845, 'learning_rate': 5.274410888977024e-05, 'batch_size': 136, 'step_size': 9, 'gamma': 0.9145742459980027}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:09:31,225][0m Trial 31 finished with value: 0.08095977019051528 and parameters: {'observation_period_num': 8, 'train_rates': 0.7779089242860939, 'learning_rate': 0.0009256142524550839, 'batch_size': 152, 'step_size': 10, 'gamma': 0.8807235698488226}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:09:58,546][0m Trial 32 finished with value: 0.08590740161350524 and parameters: {'observation_period_num': 21, 'train_rates': 0.7430466544828254, 'learning_rate': 0.0009438980485088627, 'batch_size': 170, 'step_size': 11, 'gamma': 0.8598206209119525}. Best is trial 5 with value: 0.05324200168251991.[0m
[32m[I 2025-02-02 08:11:04,740][0m Trial 33 finished with value: 0.038524139672517776 and parameters: {'observation_period_num': 9, 'train_rates': 0.9795067990142872, 'learning_rate': 0.0006184661001500915, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8849017799214686}. Best is trial 33 with value: 0.038524139672517776.[0m
[32m[I 2025-02-02 08:14:56,298][0m Trial 34 finished with value: 0.04219760694023636 and parameters: {'observation_period_num': 47, 'train_rates': 0.9878129858162826, 'learning_rate': 8.949855302561874e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8359868636717748}. Best is trial 33 with value: 0.038524139672517776.[0m
[32m[I 2025-02-02 08:18:51,829][0m Trial 35 finished with value: 0.42212320922257063 and parameters: {'observation_period_num': 115, 'train_rates': 0.9731386182148698, 'learning_rate': 9.7444525283359e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.828703873019706}. Best is trial 33 with value: 0.038524139672517776.[0m
[32m[I 2025-02-02 08:21:12,321][0m Trial 36 finished with value: 0.03507604375481606 and parameters: {'observation_period_num': 49, 'train_rates': 0.988103519295398, 'learning_rate': 0.0001853235005004823, 'batch_size': 28, 'step_size': 6, 'gamma': 0.7962495614845033}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:22:17,984][0m Trial 37 finished with value: 0.09137814491987228 and parameters: {'observation_period_num': 74, 'train_rates': 0.9876365184877259, 'learning_rate': 9.944064468160311e-05, 'batch_size': 61, 'step_size': 7, 'gamma': 0.7876211824413254}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:23:25,808][0m Trial 38 finished with value: 0.2338827686740997 and parameters: {'observation_period_num': 50, 'train_rates': 0.9515186691219277, 'learning_rate': 0.00018453067118250898, 'batch_size': 57, 'step_size': 3, 'gamma': 0.7914895135193136}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:25:23,586][0m Trial 39 finished with value: 0.6370295121389277 and parameters: {'observation_period_num': 93, 'train_rates': 0.9703043981430611, 'learning_rate': 2.742244205684035e-06, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8047501028831496}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:27:15,914][0m Trial 40 finished with value: 0.2521579460158867 and parameters: {'observation_period_num': 127, 'train_rates': 0.9316844252648444, 'learning_rate': 0.0003134840544078735, 'batch_size': 31, 'step_size': 5, 'gamma': 0.8487854008496053}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:28:23,158][0m Trial 41 finished with value: 0.0601796880364418 and parameters: {'observation_period_num': 48, 'train_rates': 0.9898586026134528, 'learning_rate': 0.0001743836918080057, 'batch_size': 57, 'step_size': 8, 'gamma': 0.7517023436557232}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:29:28,612][0m Trial 42 finished with value: 0.26181917225033785 and parameters: {'observation_period_num': 49, 'train_rates': 0.9583180300511461, 'learning_rate': 4.591760595810086e-05, 'batch_size': 56, 'step_size': 8, 'gamma': 0.7563352784225499}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:30:16,467][0m Trial 43 finished with value: 0.19378038716746826 and parameters: {'observation_period_num': 100, 'train_rates': 0.9079267407878252, 'learning_rate': 7.536192694925952e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.7755650369154891}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:30:57,445][0m Trial 44 finished with value: 0.06698951870203018 and parameters: {'observation_period_num': 61, 'train_rates': 0.9859688086471999, 'learning_rate': 0.00017625830971816495, 'batch_size': 96, 'step_size': 8, 'gamma': 0.8152221193276774}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:32:38,957][0m Trial 45 finished with value: 0.3279000376832896 and parameters: {'observation_period_num': 251, 'train_rates': 0.9463063239917086, 'learning_rate': 0.0004704099795888442, 'batch_size': 33, 'step_size': 6, 'gamma': 0.777409497814755}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:33:51,645][0m Trial 46 finished with value: 0.3538010385301378 and parameters: {'observation_period_num': 76, 'train_rates': 0.9686996183418155, 'learning_rate': 0.00011482639808715193, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8360370487406978}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:34:41,841][0m Trial 47 finished with value: 0.17023717602978833 and parameters: {'observation_period_num': 41, 'train_rates': 0.9153938104176708, 'learning_rate': 8.588392733715622e-05, 'batch_size': 72, 'step_size': 7, 'gamma': 0.806874843550702}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:37:05,082][0m Trial 48 finished with value: 0.03770437217768161 and parameters: {'observation_period_num': 24, 'train_rates': 0.9896341562780115, 'learning_rate': 0.0002649682703609873, 'batch_size': 26, 'step_size': 9, 'gamma': 0.7697219352636403}. Best is trial 36 with value: 0.03507604375481606.[0m
[32m[I 2025-02-02 08:39:33,498][0m Trial 49 finished with value: 0.12910011611444858 and parameters: {'observation_period_num': 26, 'train_rates': 0.8945844511457859, 'learning_rate': 0.0006224489848925915, 'batch_size': 23, 'step_size': 4, 'gamma': 0.8232586482500814}. Best is trial 36 with value: 0.03507604375481606.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-02 08:39:33,505][0m A new study created in memory with name: no-name-ba8d9a19-c794-480e-ac53-2a730a7c3bdd[0m
[32m[I 2025-02-02 08:41:10,542][0m Trial 0 finished with value: 0.13158304558752418 and parameters: {'observation_period_num': 24, 'train_rates': 0.8948550522386471, 'learning_rate': 2.784981043766946e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.9623302102738783}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:42:41,247][0m Trial 1 finished with value: 0.2957740125174706 and parameters: {'observation_period_num': 201, 'train_rates': 0.9434997406989859, 'learning_rate': 1.9247684791735365e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8166979962021544}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:43:11,504][0m Trial 2 finished with value: 0.14128134878816215 and parameters: {'observation_period_num': 23, 'train_rates': 0.9081609995786275, 'learning_rate': 0.0004911946261356974, 'batch_size': 191, 'step_size': 10, 'gamma': 0.8300976548378713}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:45:04,459][0m Trial 3 finished with value: 0.4437774289183635 and parameters: {'observation_period_num': 210, 'train_rates': 0.8112263921440204, 'learning_rate': 2.9648512787704542e-06, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8478493661962295}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:45:31,226][0m Trial 4 finished with value: 0.1342167250318558 and parameters: {'observation_period_num': 148, 'train_rates': 0.7795025877221714, 'learning_rate': 0.0004811794062601766, 'batch_size': 206, 'step_size': 11, 'gamma': 0.839399263392143}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:46:02,899][0m Trial 5 finished with value: 0.14821097640127973 and parameters: {'observation_period_num': 155, 'train_rates': 0.8240013157947075, 'learning_rate': 0.00014196859692046238, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9103730221134778}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:46:59,499][0m Trial 6 finished with value: 0.13628699223688043 and parameters: {'observation_period_num': 114, 'train_rates': 0.7704957446086109, 'learning_rate': 4.66562030398292e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9703855719935819}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:47:22,006][0m Trial 7 finished with value: 1.0179184398757284 and parameters: {'observation_period_num': 178, 'train_rates': 0.6114569399301882, 'learning_rate': 3.48681057096506e-06, 'batch_size': 228, 'step_size': 9, 'gamma': 0.8248853035066569}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:47:49,129][0m Trial 8 finished with value: 0.23459313380875885 and parameters: {'observation_period_num': 249, 'train_rates': 0.6364283447266936, 'learning_rate': 4.0001942823008554e-05, 'batch_size': 110, 'step_size': 3, 'gamma': 0.9510379742219173}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:48:19,737][0m Trial 9 finished with value: 0.17945297062397003 and parameters: {'observation_period_num': 193, 'train_rates': 0.9857128757993955, 'learning_rate': 0.0005633813603597151, 'batch_size': 244, 'step_size': 1, 'gamma': 0.8902332857839285}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:49:05,744][0m Trial 10 finished with value: 0.15124069624667832 and parameters: {'observation_period_num': 9, 'train_rates': 0.8734842699583937, 'learning_rate': 8.526661266917118e-06, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7653108155040125}. Best is trial 0 with value: 0.13158304558752418.[0m
[32m[I 2025-02-02 08:49:31,475][0m Trial 11 finished with value: 0.11695714469562445 and parameters: {'observation_period_num': 92, 'train_rates': 0.719637315744854, 'learning_rate': 9.899279053276867e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.924322431248191}. Best is trial 11 with value: 0.11695714469562445.[0m
[32m[I 2025-02-02 08:49:58,455][0m Trial 12 finished with value: 0.10912633896086392 and parameters: {'observation_period_num': 74, 'train_rates': 0.7074616194216851, 'learning_rate': 0.00011387376066625817, 'batch_size': 164, 'step_size': 13, 'gamma': 0.9389650003812232}. Best is trial 12 with value: 0.10912633896086392.[0m
[32m[I 2025-02-02 08:50:25,338][0m Trial 13 finished with value: 0.10790926878679695 and parameters: {'observation_period_num': 92, 'train_rates': 0.6976958034008448, 'learning_rate': 0.0001388053480224955, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9244374544245106}. Best is trial 13 with value: 0.10790926878679695.[0m
[32m[I 2025-02-02 08:50:51,910][0m Trial 14 finished with value: 0.10185046583752741 and parameters: {'observation_period_num': 72, 'train_rates': 0.6945655348516863, 'learning_rate': 0.0001783217589751439, 'batch_size': 148, 'step_size': 6, 'gamma': 0.989398657402172}. Best is trial 14 with value: 0.10185046583752741.[0m
[32m[I 2025-02-02 08:51:19,217][0m Trial 15 finished with value: 0.09966202858989665 and parameters: {'observation_period_num': 66, 'train_rates': 0.6820992557933293, 'learning_rate': 0.00022550124817415069, 'batch_size': 149, 'step_size': 6, 'gamma': 0.9882909942874758}. Best is trial 15 with value: 0.09966202858989665.[0m
[32m[I 2025-02-02 08:51:45,509][0m Trial 16 finished with value: 0.09730439708082086 and parameters: {'observation_period_num': 55, 'train_rates': 0.6535895415448942, 'learning_rate': 0.00028799624360334556, 'batch_size': 138, 'step_size': 7, 'gamma': 0.9865508860246164}. Best is trial 16 with value: 0.09730439708082086.[0m
[32m[I 2025-02-02 08:52:19,551][0m Trial 17 finished with value: 0.12792073714458205 and parameters: {'observation_period_num': 49, 'train_rates': 0.6490644978221439, 'learning_rate': 0.0009237742188335267, 'batch_size': 87, 'step_size': 6, 'gamma': 0.9840196823396764}. Best is trial 16 with value: 0.09730439708082086.[0m
[32m[I 2025-02-02 08:52:46,824][0m Trial 18 finished with value: 0.09814577882483616 and parameters: {'observation_period_num': 56, 'train_rates': 0.6538022732223839, 'learning_rate': 0.00028702612796057813, 'batch_size': 134, 'step_size': 7, 'gamma': 0.8878024773650733}. Best is trial 16 with value: 0.09730439708082086.[0m
[32m[I 2025-02-02 08:53:18,818][0m Trial 19 finished with value: 0.09397664255021701 and parameters: {'observation_period_num': 43, 'train_rates': 0.7481503791258675, 'learning_rate': 0.0003446240942223235, 'batch_size': 113, 'step_size': 8, 'gamma': 0.8754059190330538}. Best is trial 19 with value: 0.09397664255021701.[0m
[32m[I 2025-02-02 08:53:53,828][0m Trial 20 finished with value: 0.9038025932362074 and parameters: {'observation_period_num': 112, 'train_rates': 0.7317618835774303, 'learning_rate': 1.2465109499026346e-06, 'batch_size': 92, 'step_size': 4, 'gamma': 0.7898564391097653}. Best is trial 19 with value: 0.09397664255021701.[0m
[32m[I 2025-02-02 08:54:19,973][0m Trial 21 finished with value: 0.0909559852935929 and parameters: {'observation_period_num': 42, 'train_rates': 0.6036114272764723, 'learning_rate': 0.0002817707482764612, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8723400442081856}. Best is trial 21 with value: 0.0909559852935929.[0m
[32m[I 2025-02-02 08:54:47,537][0m Trial 22 finished with value: 0.09935534429389217 and parameters: {'observation_period_num': 41, 'train_rates': 0.6055116113978445, 'learning_rate': 0.0009626193428585382, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8677897827943447}. Best is trial 21 with value: 0.0909559852935929.[0m
[32m[I 2025-02-02 08:55:23,309][0m Trial 23 finished with value: 0.09805296642639091 and parameters: {'observation_period_num': 35, 'train_rates': 0.7511928457100723, 'learning_rate': 7.1353462841917e-05, 'batch_size': 100, 'step_size': 8, 'gamma': 0.8663979941428822}. Best is trial 21 with value: 0.0909559852935929.[0m
[32m[I 2025-02-02 08:56:11,460][0m Trial 24 finished with value: 0.06477421584976725 and parameters: {'observation_period_num': 7, 'train_rates': 0.6616920409899009, 'learning_rate': 0.00030025334533441444, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8975767544901151}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 08:56:54,530][0m Trial 25 finished with value: 0.09171971082815947 and parameters: {'observation_period_num': 6, 'train_rates': 0.6064699037922993, 'learning_rate': 0.0003995725450008003, 'batch_size': 70, 'step_size': 1, 'gamma': 0.889465645243938}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 08:57:36,815][0m Trial 26 finished with value: 0.1255166719842386 and parameters: {'observation_period_num': 7, 'train_rates': 0.6039342642633269, 'learning_rate': 6.215844860769287e-05, 'batch_size': 71, 'step_size': 1, 'gamma': 0.9027478194859884}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 08:58:25,976][0m Trial 27 finished with value: 0.07112573019046452 and parameters: {'observation_period_num': 7, 'train_rates': 0.6707828441873017, 'learning_rate': 0.0006461251455273003, 'batch_size': 65, 'step_size': 2, 'gamma': 0.8544852803429739}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 08:59:21,887][0m Trial 28 finished with value: 0.07648381727476106 and parameters: {'observation_period_num': 24, 'train_rates': 0.6731267192021253, 'learning_rate': 0.0008272113420577949, 'batch_size': 54, 'step_size': 3, 'gamma': 0.8538664635484647}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:00:26,499][0m Trial 29 finished with value: 0.07604334361689247 and parameters: {'observation_period_num': 23, 'train_rates': 0.6703530325063857, 'learning_rate': 0.0008200835566704429, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8083484209172782}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:02:47,711][0m Trial 30 finished with value: 0.11757282705557456 and parameters: {'observation_period_num': 30, 'train_rates': 0.6675824501846203, 'learning_rate': 2.469886308891218e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.8020961053062976}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:03:49,251][0m Trial 31 finished with value: 0.08250659925583452 and parameters: {'observation_period_num': 22, 'train_rates': 0.6797340191792958, 'learning_rate': 0.0007311084131653093, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8504803325533499}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:04:41,213][0m Trial 32 finished with value: 0.09769934591637036 and parameters: {'observation_period_num': 20, 'train_rates': 0.6351468336299434, 'learning_rate': 0.0006812753120620403, 'batch_size': 55, 'step_size': 2, 'gamma': 0.8026475228180301}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:06:09,489][0m Trial 33 finished with value: 0.07350879852837945 and parameters: {'observation_period_num': 22, 'train_rates': 0.7251611770084058, 'learning_rate': 0.0009384461870659719, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7579804853637597}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:07:33,457][0m Trial 34 finished with value: 0.06764034628213723 and parameters: {'observation_period_num': 5, 'train_rates': 0.7296422081664903, 'learning_rate': 0.0005139919290627617, 'batch_size': 39, 'step_size': 4, 'gamma': 0.7506724801120745}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:09:15,391][0m Trial 35 finished with value: 0.06812369496196131 and parameters: {'observation_period_num': 5, 'train_rates': 0.732151801699657, 'learning_rate': 0.00046772174854823787, 'batch_size': 32, 'step_size': 5, 'gamma': 0.7542464767050839}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:11:05,305][0m Trial 36 finished with value: 0.0880750418997973 and parameters: {'observation_period_num': 8, 'train_rates': 0.8381156354601742, 'learning_rate': 0.0005174030337201223, 'batch_size': 32, 'step_size': 5, 'gamma': 0.7771938887761394}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:12:56,802][0m Trial 37 finished with value: 0.07886615505467853 and parameters: {'observation_period_num': 5, 'train_rates': 0.7797565419670404, 'learning_rate': 0.00017570766212770555, 'batch_size': 30, 'step_size': 5, 'gamma': 0.7503829370403616}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:15:34,205][0m Trial 38 finished with value: 0.24546583503362918 and parameters: {'observation_period_num': 89, 'train_rates': 0.7435376743228796, 'learning_rate': 1.493018380433748e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.7810119293334297}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:16:22,499][0m Trial 39 finished with value: 0.1741744521080602 and parameters: {'observation_period_num': 232, 'train_rates': 0.7929276780823852, 'learning_rate': 0.0004358923651106534, 'batch_size': 65, 'step_size': 5, 'gamma': 0.8332062822698281}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:17:42,541][0m Trial 40 finished with value: 0.147979228496997 and parameters: {'observation_period_num': 131, 'train_rates': 0.7627691288160635, 'learning_rate': 0.00021873851439956304, 'batch_size': 38, 'step_size': 2, 'gamma': 0.7660007846125519}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:20:45,966][0m Trial 41 finished with value: 0.076928419350576 and parameters: {'observation_period_num': 22, 'train_rates': 0.715702828974139, 'learning_rate': 0.0005952231272121406, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7533712813361354}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:22:02,919][0m Trial 42 finished with value: 0.07258211947081948 and parameters: {'observation_period_num': 16, 'train_rates': 0.7336601530837458, 'learning_rate': 0.00045122798152676634, 'batch_size': 41, 'step_size': 4, 'gamma': 0.7654543305003901}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:23:01,072][0m Trial 43 finished with value: 0.09310072704979998 and parameters: {'observation_period_num': 34, 'train_rates': 0.8194755046597947, 'learning_rate': 0.00040046222651367606, 'batch_size': 60, 'step_size': 5, 'gamma': 0.7702420211697769}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:23:37,896][0m Trial 44 finished with value: 0.06903988898304415 and parameters: {'observation_period_num': 15, 'train_rates': 0.6277058087837272, 'learning_rate': 0.0005275978889173532, 'batch_size': 79, 'step_size': 4, 'gamma': 0.7887613445970357}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:24:13,603][0m Trial 45 finished with value: 0.10617912970483304 and parameters: {'observation_period_num': 58, 'train_rates': 0.6267699315929761, 'learning_rate': 0.0005666467915488654, 'batch_size': 81, 'step_size': 2, 'gamma': 0.7899451965788356}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:24:42,994][0m Trial 46 finished with value: 0.18824392578512944 and parameters: {'observation_period_num': 159, 'train_rates': 0.6254277898842087, 'learning_rate': 9.976989077359341e-05, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8176570073068022}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:25:24,878][0m Trial 47 finished with value: 0.11074277594137351 and parameters: {'observation_period_num': 34, 'train_rates': 0.6925875916080082, 'learning_rate': 0.0003224102660174316, 'batch_size': 74, 'step_size': 2, 'gamma': 0.7882665396836068}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:25:59,142][0m Trial 48 finished with value: 0.06546277432857851 and parameters: {'observation_period_num': 15, 'train_rates': 0.6471005076520862, 'learning_rate': 0.00021952184661692896, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9114470064077671}. Best is trial 24 with value: 0.06477421584976725.[0m
[32m[I 2025-02-02 09:26:45,358][0m Trial 49 finished with value: 0.10001244350073583 and parameters: {'observation_period_num': 15, 'train_rates': 0.8577198730955549, 'learning_rate': 0.00021764638999104195, 'batch_size': 83, 'step_size': 7, 'gamma': 0.9239053054339679}. Best is trial 24 with value: 0.06477421584976725.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-02 09:26:45,366][0m A new study created in memory with name: no-name-6eb349c3-c793-4c45-ab8f-6a2cc77ec8de[0m
[32m[I 2025-02-02 09:27:18,648][0m Trial 0 finished with value: 0.12043454311788082 and parameters: {'observation_period_num': 83, 'train_rates': 0.7854335182213683, 'learning_rate': 0.000338615588206963, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9701087358429148}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:27:49,827][0m Trial 1 finished with value: 0.5380198980603262 and parameters: {'observation_period_num': 168, 'train_rates': 0.9232252505249064, 'learning_rate': 4.773133469394131e-06, 'batch_size': 195, 'step_size': 9, 'gamma': 0.917063926938762}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:28:18,590][0m Trial 2 finished with value: 0.17439004642412412 and parameters: {'observation_period_num': 198, 'train_rates': 0.750244649649899, 'learning_rate': 5.133873640750351e-05, 'batch_size': 171, 'step_size': 6, 'gamma': 0.9461849074328694}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:29:23,233][0m Trial 3 finished with value: 0.12882841205392898 and parameters: {'observation_period_num': 64, 'train_rates': 0.8586702182159099, 'learning_rate': 4.0159562975502235e-05, 'batch_size': 57, 'step_size': 6, 'gamma': 0.916027625573318}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:29:47,266][0m Trial 4 finished with value: 1.1055452076264614 and parameters: {'observation_period_num': 101, 'train_rates': 0.6616705838612709, 'learning_rate': 3.31189732127441e-06, 'batch_size': 256, 'step_size': 1, 'gamma': 0.962388613453687}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:30:44,698][0m Trial 5 finished with value: 0.28695388760747786 and parameters: {'observation_period_num': 187, 'train_rates': 0.9429232292023154, 'learning_rate': 5.6836616797587115e-05, 'batch_size': 66, 'step_size': 2, 'gamma': 0.9664920532749418}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:31:33,789][0m Trial 6 finished with value: 0.39594937533140184 and parameters: {'observation_period_num': 132, 'train_rates': 0.9645294196691873, 'learning_rate': 6.604067706384711e-05, 'batch_size': 78, 'step_size': 7, 'gamma': 0.98262249789637}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:32:22,544][0m Trial 7 finished with value: 0.4073467216957575 and parameters: {'observation_period_num': 38, 'train_rates': 0.7312632964572725, 'learning_rate': 1.1795370443581615e-06, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9818229223036082}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:33:09,379][0m Trial 8 finished with value: 0.3793603470376063 and parameters: {'observation_period_num': 226, 'train_rates': 0.861773761686764, 'learning_rate': 3.900501517387357e-06, 'batch_size': 70, 'step_size': 10, 'gamma': 0.9252742849626263}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:33:39,542][0m Trial 9 finished with value: 0.5295837417557859 and parameters: {'observation_period_num': 241, 'train_rates': 0.7718318006360889, 'learning_rate': 3.7179349317819014e-06, 'batch_size': 112, 'step_size': 13, 'gamma': 0.7551302999360457}. Best is trial 0 with value: 0.12043454311788082.[0m
[32m[I 2025-02-02 09:34:05,543][0m Trial 10 finished with value: 0.06488908293840512 and parameters: {'observation_period_num': 6, 'train_rates': 0.6331038458659988, 'learning_rate': 0.0008603521926342401, 'batch_size': 136, 'step_size': 15, 'gamma': 0.8427944733935501}. Best is trial 10 with value: 0.06488908293840512.[0m
[32m[I 2025-02-02 09:34:32,245][0m Trial 11 finished with value: 0.06667342904251446 and parameters: {'observation_period_num': 15, 'train_rates': 0.6196671326130465, 'learning_rate': 0.0009834595084426579, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8185073099148955}. Best is trial 10 with value: 0.06488908293840512.[0m
[32m[I 2025-02-02 09:34:57,118][0m Trial 12 finished with value: 0.07218979494036915 and parameters: {'observation_period_num': 9, 'train_rates': 0.6000390182099433, 'learning_rate': 0.000876003487804099, 'batch_size': 176, 'step_size': 15, 'gamma': 0.8192047269452093}. Best is trial 10 with value: 0.06488908293840512.[0m
[32m[I 2025-02-02 09:37:31,809][0m Trial 13 finished with value: 0.06350164617917554 and parameters: {'observation_period_num': 6, 'train_rates': 0.6136198146533486, 'learning_rate': 0.0009774163957408462, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8415641919000277}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:40:11,377][0m Trial 14 finished with value: 0.11104008059749323 and parameters: {'observation_period_num': 49, 'train_rates': 0.6717346930758502, 'learning_rate': 0.0002207819377799221, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8577271236385065}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:42:44,902][0m Trial 15 finished with value: 0.18581493570574636 and parameters: {'observation_period_num': 126, 'train_rates': 0.6808437260582497, 'learning_rate': 0.00023076591454688038, 'batch_size': 18, 'step_size': 12, 'gamma': 0.858541245383936}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:43:09,825][0m Trial 16 finished with value: 0.08729850920137919 and parameters: {'observation_period_num': 34, 'train_rates': 0.7109029923955638, 'learning_rate': 0.0004044545256295635, 'batch_size': 247, 'step_size': 13, 'gamma': 0.8155543876196464}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:43:39,904][0m Trial 17 finished with value: 0.16743278040705004 and parameters: {'observation_period_num': 7, 'train_rates': 0.6351902487091957, 'learning_rate': 1.4501570056259774e-05, 'batch_size': 103, 'step_size': 11, 'gamma': 0.885919292819278}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:44:09,422][0m Trial 18 finished with value: 0.12464760162018157 and parameters: {'observation_period_num': 66, 'train_rates': 0.8425946066277432, 'learning_rate': 0.00012214472099006957, 'batch_size': 215, 'step_size': 14, 'gamma': 0.7854172205218468}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:45:11,850][0m Trial 19 finished with value: 0.20999118085675592 and parameters: {'observation_period_num': 104, 'train_rates': 0.6336758587988274, 'learning_rate': 1.6730658937688713e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8754942411535825}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:45:43,824][0m Trial 20 finished with value: 0.08708955233045128 and parameters: {'observation_period_num': 33, 'train_rates': 0.7011925554147127, 'learning_rate': 0.0005330225772841537, 'batch_size': 104, 'step_size': 15, 'gamma': 0.8305682705390872}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:46:09,018][0m Trial 21 finished with value: 0.06436043673671454 and parameters: {'observation_period_num': 7, 'train_rates': 0.6064355587459025, 'learning_rate': 0.0008046991104787499, 'batch_size': 151, 'step_size': 14, 'gamma': 0.8372436596044529}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:46:34,433][0m Trial 22 finished with value: 0.07489116010153334 and parameters: {'observation_period_num': 5, 'train_rates': 0.6003225419445937, 'learning_rate': 0.0005767111476087994, 'batch_size': 157, 'step_size': 13, 'gamma': 0.8463200013824183}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:46:59,239][0m Trial 23 finished with value: 0.14076613220903608 and parameters: {'observation_period_num': 29, 'train_rates': 0.6522819505522119, 'learning_rate': 0.0001064380057587262, 'batch_size': 220, 'step_size': 14, 'gamma': 0.7908663932415947}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:47:25,038][0m Trial 24 finished with value: 0.10427914818037946 and parameters: {'observation_period_num': 64, 'train_rates': 0.6943040384905159, 'learning_rate': 0.0009959675430781876, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8926086515105512}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:47:52,711][0m Trial 25 finished with value: 0.08761006063594688 and parameters: {'observation_period_num': 53, 'train_rates': 0.6461182645268965, 'learning_rate': 0.0002482270954750485, 'batch_size': 122, 'step_size': 14, 'gamma': 0.7913690447930841}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:48:32,598][0m Trial 26 finished with value: 0.11207750858215804 and parameters: {'observation_period_num': 84, 'train_rates': 0.8139540518214439, 'learning_rate': 0.00012907351938680763, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8426683302171214}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:49:46,695][0m Trial 27 finished with value: 0.08457401765044778 and parameters: {'observation_period_num': 24, 'train_rates': 0.6176708866705181, 'learning_rate': 0.0005751443930126572, 'batch_size': 37, 'step_size': 15, 'gamma': 0.8025358910443089}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:50:14,722][0m Trial 28 finished with value: 0.09598142597289372 and parameters: {'observation_period_num': 51, 'train_rates': 0.7361875827684186, 'learning_rate': 0.00034204310444153473, 'batch_size': 149, 'step_size': 11, 'gamma': 0.7706263949350759}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:50:40,319][0m Trial 29 finished with value: 0.10631590174017222 and parameters: {'observation_period_num': 83, 'train_rates': 0.6660997944637516, 'learning_rate': 0.0005857334842474536, 'batch_size': 189, 'step_size': 8, 'gamma': 0.8975116699395328}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:51:16,885][0m Trial 30 finished with value: 0.1706827664763809 and parameters: {'observation_period_num': 144, 'train_rates': 0.7923387499442547, 'learning_rate': 2.3751438807592188e-05, 'batch_size': 92, 'step_size': 13, 'gamma': 0.8675086011302902}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:51:43,493][0m Trial 31 finished with value: 0.0676753221532783 and parameters: {'observation_period_num': 17, 'train_rates': 0.6219531280200402, 'learning_rate': 0.0009947359969549293, 'batch_size': 134, 'step_size': 15, 'gamma': 0.8310456674873409}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:52:09,585][0m Trial 32 finished with value: 0.0745806524629437 and parameters: {'observation_period_num': 18, 'train_rates': 0.6038504190900187, 'learning_rate': 0.0003958688454324314, 'batch_size': 141, 'step_size': 14, 'gamma': 0.8116566967677699}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:52:36,686][0m Trial 33 finished with value: 0.08288630840433386 and parameters: {'observation_period_num': 41, 'train_rates': 0.6248147657434875, 'learning_rate': 0.0007160863074516659, 'batch_size': 122, 'step_size': 15, 'gamma': 0.8431689361381369}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:53:01,688][0m Trial 34 finished with value: 0.09084891681034069 and parameters: {'observation_period_num': 21, 'train_rates': 0.6432604144669627, 'learning_rate': 0.00018664922812763597, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8311059692265391}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:53:26,877][0m Trial 35 finished with value: 0.07040592996091802 and parameters: {'observation_period_num': 6, 'train_rates': 0.6837225244852285, 'learning_rate': 0.0003895990209141959, 'batch_size': 203, 'step_size': 14, 'gamma': 0.8521251299810454}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:53:57,109][0m Trial 36 finished with value: 0.32777875596109557 and parameters: {'observation_period_num': 64, 'train_rates': 0.9060918544534525, 'learning_rate': 8.192690749214477e-06, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8723016436153886}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:54:27,055][0m Trial 37 finished with value: 0.07788942553323185 and parameters: {'observation_period_num': 19, 'train_rates': 0.7230822959332253, 'learning_rate': 0.0006442123110243314, 'batch_size': 121, 'step_size': 5, 'gamma': 0.8062424577582472}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:54:53,977][0m Trial 38 finished with value: 0.14417706188844076 and parameters: {'observation_period_num': 80, 'train_rates': 0.6537024684486393, 'learning_rate': 8.129512601641329e-05, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8237077546626188}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:55:47,646][0m Trial 39 finished with value: 0.16587642730815708 and parameters: {'observation_period_num': 173, 'train_rates': 0.6153950838120077, 'learning_rate': 0.00029036769083216543, 'batch_size': 49, 'step_size': 14, 'gamma': 0.910935459378441}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:56:12,438][0m Trial 40 finished with value: 0.13979121771654743 and parameters: {'observation_period_num': 44, 'train_rates': 0.6721478647907375, 'learning_rate': 0.00016532690788637473, 'batch_size': 229, 'step_size': 9, 'gamma': 0.7734037566814359}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:56:38,880][0m Trial 41 finished with value: 0.07048760995001536 and parameters: {'observation_period_num': 19, 'train_rates': 0.6197408943643209, 'learning_rate': 0.0009370383888962867, 'batch_size': 132, 'step_size': 15, 'gamma': 0.8336613631940831}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:57:06,650][0m Trial 42 finished with value: 0.07120605076510292 and parameters: {'observation_period_num': 18, 'train_rates': 0.6346635310204584, 'learning_rate': 0.0009690204857122472, 'batch_size': 131, 'step_size': 15, 'gamma': 0.83906244744239}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:57:32,608][0m Trial 43 finished with value: 0.08669763540677333 and parameters: {'observation_period_num': 31, 'train_rates': 0.6175002469191638, 'learning_rate': 0.0004898405467107624, 'batch_size': 146, 'step_size': 15, 'gamma': 0.8240278335864714}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:58:05,222][0m Trial 44 finished with value: 0.07929878335864751 and parameters: {'observation_period_num': 6, 'train_rates': 0.7615547815880341, 'learning_rate': 0.0007688164353155932, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8635254350792846}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:58:30,811][0m Trial 45 finished with value: 0.5290730139042469 and parameters: {'observation_period_num': 40, 'train_rates': 0.660972396204535, 'learning_rate': 1.7315063873916653e-06, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8007675319898967}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:59:01,588][0m Trial 46 finished with value: 0.22714887338655967 and parameters: {'observation_period_num': 207, 'train_rates': 0.6064835941333349, 'learning_rate': 0.0007461945310054799, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9458705495274375}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 09:59:26,512][0m Trial 47 finished with value: 0.0769939548704853 and parameters: {'observation_period_num': 18, 'train_rates': 0.6341658098937557, 'learning_rate': 0.0004694471531095399, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8546101174964083}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 10:00:06,261][0m Trial 48 finished with value: 0.12554120097139077 and parameters: {'observation_period_num': 52, 'train_rates': 0.6907338130842783, 'learning_rate': 4.161559085469737e-05, 'batch_size': 76, 'step_size': 12, 'gamma': 0.880850580778536}. Best is trial 13 with value: 0.06350164617917554.[0m
[32m[I 2025-02-02 10:00:36,994][0m Trial 49 finished with value: 0.18475314766979542 and parameters: {'observation_period_num': 105, 'train_rates': 0.896771478116893, 'learning_rate': 0.000293647557102222, 'batch_size': 155, 'step_size': 15, 'gamma': 0.8177865246515658}. Best is trial 13 with value: 0.06350164617917554.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-02 10:00:37,002][0m A new study created in memory with name: no-name-a9c52b10-e6a9-48d4-9509-08b3644a36cc[0m
[32m[I 2025-02-02 10:01:09,880][0m Trial 0 finished with value: 0.942347537691348 and parameters: {'observation_period_num': 184, 'train_rates': 0.9227938830943992, 'learning_rate': 1.8870735502374484e-06, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7685770040802166}. Best is trial 0 with value: 0.942347537691348.[0m
[32m[I 2025-02-02 10:01:44,129][0m Trial 1 finished with value: 0.1053038677215025 and parameters: {'observation_period_num': 16, 'train_rates': 0.7788793567521389, 'learning_rate': 3.1581751207183035e-05, 'batch_size': 104, 'step_size': 14, 'gamma': 0.8270570166988978}. Best is trial 1 with value: 0.1053038677215025.[0m
[32m[I 2025-02-02 10:02:10,163][0m Trial 2 finished with value: 0.5146697405347957 and parameters: {'observation_period_num': 110, 'train_rates': 0.7747089524344397, 'learning_rate': 1.1724428999385427e-05, 'batch_size': 250, 'step_size': 4, 'gamma': 0.7972402246227952}. Best is trial 1 with value: 0.1053038677215025.[0m
[32m[I 2025-02-02 10:02:36,247][0m Trial 3 finished with value: 0.18921559541062874 and parameters: {'observation_period_num': 81, 'train_rates': 0.7393914330950109, 'learning_rate': 2.3499416029937744e-05, 'batch_size': 221, 'step_size': 11, 'gamma': 0.9329887722207578}. Best is trial 1 with value: 0.1053038677215025.[0m
[32m[I 2025-02-02 10:02:58,481][0m Trial 4 finished with value: 1.0611157475448236 and parameters: {'observation_period_num': 237, 'train_rates': 0.6223652195081943, 'learning_rate': 1.3121589477199403e-06, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9427576812219101}. Best is trial 1 with value: 0.1053038677215025.[0m
[32m[I 2025-02-02 10:03:55,653][0m Trial 5 finished with value: 0.3484708531557218 and parameters: {'observation_period_num': 171, 'train_rates': 0.7355124637253571, 'learning_rate': 1.5529862063721653e-05, 'batch_size': 52, 'step_size': 3, 'gamma': 0.7540094590084633}. Best is trial 1 with value: 0.1053038677215025.[0m
[32m[I 2025-02-02 10:04:29,414][0m Trial 6 finished with value: 0.06937263906002045 and parameters: {'observation_period_num': 139, 'train_rates': 0.9846595579806947, 'learning_rate': 0.0007149123438290095, 'batch_size': 137, 'step_size': 3, 'gamma': 0.8796701262853258}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:04:54,181][0m Trial 7 finished with value: 0.4671506110871102 and parameters: {'observation_period_num': 145, 'train_rates': 0.6457851632600999, 'learning_rate': 5.519251462722144e-06, 'batch_size': 143, 'step_size': 3, 'gamma': 0.984856372881976}. Best is trial 6 with value: 0.06937263906002045.[0m
Early stopping at epoch 55
[32m[I 2025-02-02 10:05:11,551][0m Trial 8 finished with value: 1.221098298766701 and parameters: {'observation_period_num': 190, 'train_rates': 0.7941549382222653, 'learning_rate': 4.587562369509594e-06, 'batch_size': 117, 'step_size': 1, 'gamma': 0.7987258803437972}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:05:40,214][0m Trial 9 finished with value: 0.12642652167539511 and parameters: {'observation_period_num': 17, 'train_rates': 0.8092832041666636, 'learning_rate': 3.163742164293045e-05, 'batch_size': 209, 'step_size': 14, 'gamma': 0.7753055805497697}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:07:33,913][0m Trial 10 finished with value: 0.0716763436794281 and parameters: {'observation_period_num': 73, 'train_rates': 0.9888730989232435, 'learning_rate': 0.0008317344298967251, 'batch_size': 33, 'step_size': 8, 'gamma': 0.8820438472138737}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:09:40,146][0m Trial 11 finished with value: 0.06942621695826638 and parameters: {'observation_period_num': 72, 'train_rates': 0.9818345068383767, 'learning_rate': 0.0009969170737748685, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8700898788012552}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:10:11,729][0m Trial 12 finished with value: 0.16861428067845813 and parameters: {'observation_period_num': 76, 'train_rates': 0.9180586643652895, 'learning_rate': 0.0008104957476110441, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8671207267844941}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:11:08,467][0m Trial 13 finished with value: 0.08563578873872757 and parameters: {'observation_period_num': 118, 'train_rates': 0.9835224838343702, 'learning_rate': 0.00025190494103635177, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8689605209429605}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:14:33,228][0m Trial 14 finished with value: 0.14512738212943077 and parameters: {'observation_period_num': 49, 'train_rates': 0.8829930373862276, 'learning_rate': 0.00011986504572361491, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9048444249275202}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:15:18,153][0m Trial 15 finished with value: 0.25312643729542433 and parameters: {'observation_period_num': 225, 'train_rates': 0.9305283456411314, 'learning_rate': 0.0002541205690398386, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8477178839292564}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:15:47,676][0m Trial 16 finished with value: 0.23378550571002318 and parameters: {'observation_period_num': 146, 'train_rates': 0.8731903011436013, 'learning_rate': 0.0002998151295892317, 'batch_size': 174, 'step_size': 1, 'gamma': 0.9079216240715422}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:16:19,296][0m Trial 17 finished with value: 0.2835961580276489 and parameters: {'observation_period_num': 101, 'train_rates': 0.9545609905589942, 'learning_rate': 8.578211163402307e-05, 'batch_size': 154, 'step_size': 10, 'gamma': 0.8273385561255048}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:17:01,282][0m Trial 18 finished with value: 0.12551964471561408 and parameters: {'observation_period_num': 48, 'train_rates': 0.8546255335485182, 'learning_rate': 0.0009685205233929017, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8965367093666268}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:17:30,046][0m Trial 19 finished with value: 0.14513644397325579 and parameters: {'observation_period_num': 153, 'train_rates': 0.8408469971087141, 'learning_rate': 0.0004136800520286989, 'batch_size': 191, 'step_size': 5, 'gamma': 0.9407904565503887}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:18:10,122][0m Trial 20 finished with value: 0.2834724549476693 and parameters: {'observation_period_num': 45, 'train_rates': 0.9579471551294063, 'learning_rate': 0.0001087671497985589, 'batch_size': 100, 'step_size': 2, 'gamma': 0.8449404538120169}. Best is trial 6 with value: 0.06937263906002045.[0m
[32m[I 2025-02-02 10:21:43,690][0m Trial 21 finished with value: 0.06616781279444695 and parameters: {'observation_period_num': 81, 'train_rates': 0.9872342212534561, 'learning_rate': 0.0006049246712672042, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8867204566050071}. Best is trial 21 with value: 0.06616781279444695.[0m
[32m[I 2025-02-02 10:23:21,578][0m Trial 22 finished with value: 0.22735565293475357 and parameters: {'observation_period_num': 92, 'train_rates': 0.9008532797779722, 'learning_rate': 0.00045522782544017183, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9213235514458981}. Best is trial 21 with value: 0.06616781279444695.[0m
[32m[I 2025-02-02 10:24:24,620][0m Trial 23 finished with value: 0.3313072286875902 and parameters: {'observation_period_num': 120, 'train_rates': 0.9602621195277309, 'learning_rate': 0.0005143593024673676, 'batch_size': 57, 'step_size': 12, 'gamma': 0.8843044864152663}. Best is trial 21 with value: 0.06616781279444695.[0m
[32m[I 2025-02-02 10:26:56,225][0m Trial 24 finished with value: 0.04512928053736687 and parameters: {'observation_period_num': 60, 'train_rates': 0.9878864078802193, 'learning_rate': 0.00015686191052045257, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8523988405864255}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:30:26,680][0m Trial 25 finished with value: 0.23702859379693694 and parameters: {'observation_period_num': 55, 'train_rates': 0.9401644396942207, 'learning_rate': 0.00016563371560986622, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9651019913286312}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:31:35,047][0m Trial 26 finished with value: 0.14177941045035486 and parameters: {'observation_period_num': 28, 'train_rates': 0.8901366674880785, 'learning_rate': 5.1593919450175846e-05, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8412912414442384}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:32:09,164][0m Trial 27 finished with value: 0.12985702874832714 and parameters: {'observation_period_num': 131, 'train_rates': 0.6859448003653389, 'learning_rate': 0.00019171259627803806, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8195811183401424}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:32:44,011][0m Trial 28 finished with value: 0.30307579040527344 and parameters: {'observation_period_num': 98, 'train_rates': 0.9601177114555733, 'learning_rate': 0.0005190101333573782, 'batch_size': 135, 'step_size': 3, 'gamma': 0.8546188709728074}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:33:18,174][0m Trial 29 finished with value: 0.20942684669682032 and parameters: {'observation_period_num': 181, 'train_rates': 0.9176028714932499, 'learning_rate': 7.156198214852328e-05, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8897744448674597}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:34:12,424][0m Trial 30 finished with value: 0.10625185817480087 and parameters: {'observation_period_num': 204, 'train_rates': 0.9888443616550175, 'learning_rate': 0.00017024750218567442, 'batch_size': 67, 'step_size': 5, 'gamma': 0.913565574899048}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:35:59,114][0m Trial 31 finished with value: 0.08707560043013285 and parameters: {'observation_period_num': 70, 'train_rates': 0.9784719345109554, 'learning_rate': 0.0006777296759609009, 'batch_size': 34, 'step_size': 7, 'gamma': 0.864151961982446}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:37:26,017][0m Trial 32 finished with value: 0.20624075285517252 and parameters: {'observation_period_num': 31, 'train_rates': 0.9379089089136949, 'learning_rate': 0.00035695723955860004, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8811167092368803}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:39:55,223][0m Trial 33 finished with value: 0.23334623387871453 and parameters: {'observation_period_num': 5, 'train_rates': 0.9649224453681742, 'learning_rate': 0.0006517623332535011, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8169633126724113}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:43:16,687][0m Trial 34 finished with value: 0.2045340844062695 and parameters: {'observation_period_num': 87, 'train_rates': 0.9125815001934777, 'learning_rate': 0.0008978593207533221, 'batch_size': 17, 'step_size': 9, 'gamma': 0.832315010675594}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:44:35,483][0m Trial 35 finished with value: 0.22041453980973788 and parameters: {'observation_period_num': 59, 'train_rates': 0.9398200008528095, 'learning_rate': 0.0002588617452905674, 'batch_size': 46, 'step_size': 7, 'gamma': 0.8558261785374895}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:45:17,849][0m Trial 36 finished with value: 0.39942798018455505 and parameters: {'observation_period_num': 132, 'train_rates': 0.9705017971720443, 'learning_rate': 0.0005666588582166106, 'batch_size': 89, 'step_size': 2, 'gamma': 0.92933875224845}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:45:48,462][0m Trial 37 finished with value: 0.11431368892447333 and parameters: {'observation_period_num': 102, 'train_rates': 0.7501865691862837, 'learning_rate': 0.00037634005815574706, 'batch_size': 118, 'step_size': 12, 'gamma': 0.8048253687573382}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:46:46,095][0m Trial 38 finished with value: 0.2643282793192978 and parameters: {'observation_period_num': 65, 'train_rates': 0.942549319679875, 'learning_rate': 4.336519281654655e-05, 'batch_size': 63, 'step_size': 2, 'gamma': 0.8945653517028328}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:47:15,409][0m Trial 39 finished with value: 0.26626443581318293 and parameters: {'observation_period_num': 86, 'train_rates': 0.8229019407453163, 'learning_rate': 1.1188302267582494e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8752057192786555}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:47:45,610][0m Trial 40 finished with value: 0.1205192431807518 and parameters: {'observation_period_num': 166, 'train_rates': 0.9899252965616633, 'learning_rate': 0.00015558956465032615, 'batch_size': 236, 'step_size': 6, 'gamma': 0.7790661994186207}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:49:35,224][0m Trial 41 finished with value: 0.3482561893640207 and parameters: {'observation_period_num': 34, 'train_rates': 0.973804927437702, 'learning_rate': 0.0009952198204104088, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8609470514676159}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:51:51,248][0m Trial 42 finished with value: 0.06054523925889622 and parameters: {'observation_period_num': 72, 'train_rates': 0.9887123628059248, 'learning_rate': 0.0006550263004263486, 'batch_size': 27, 'step_size': 8, 'gamma': 0.876925400939466}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:54:05,817][0m Trial 43 finished with value: 0.34055964084608215 and parameters: {'observation_period_num': 114, 'train_rates': 0.9507998780502249, 'learning_rate': 0.0006138643230048874, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8730767174329547}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:55:23,490][0m Trial 44 finished with value: 0.17622371771331155 and parameters: {'observation_period_num': 78, 'train_rates': 0.9068241982432138, 'learning_rate': 0.00034745014197331485, 'batch_size': 45, 'step_size': 7, 'gamma': 0.8972986654216794}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:56:14,551][0m Trial 45 finished with value: 0.5303177507418506 and parameters: {'observation_period_num': 65, 'train_rates': 0.9266392901486125, 'learning_rate': 1.0174786860306778e-06, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8759225082624229}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:58:13,860][0m Trial 46 finished with value: 0.2801999621085236 and parameters: {'observation_period_num': 40, 'train_rates': 0.6818068695641183, 'learning_rate': 2.7387230505081123e-06, 'batch_size': 25, 'step_size': 5, 'gamma': 0.8383545085387675}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:58:50,299][0m Trial 47 finished with value: 0.4525703191757202 and parameters: {'observation_period_num': 109, 'train_rates': 0.9745084327330091, 'learning_rate': 0.00023203330378844593, 'batch_size': 112, 'step_size': 3, 'gamma': 0.8497055843923822}. Best is trial 24 with value: 0.04512928053736687.[0m
[32m[I 2025-02-02 10:59:26,763][0m Trial 48 finished with value: 0.02425631508231163 and parameters: {'observation_period_num': 23, 'train_rates': 0.9891688445832926, 'learning_rate': 0.0006654625268226993, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9095385174594361}. Best is trial 48 with value: 0.02425631508231163.[0m
[32m[I 2025-02-02 10:59:59,429][0m Trial 49 finished with value: 0.062308505177497864 and parameters: {'observation_period_num': 8, 'train_rates': 0.9897237339992777, 'learning_rate': 1.942005468153057e-05, 'batch_size': 173, 'step_size': 9, 'gamma': 0.9158482634981834}. Best is trial 48 with value: 0.02425631508231163.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-02 10:59:59,436][0m A new study created in memory with name: no-name-8d7a6096-b9f5-4458-88c1-9a262e5569aa[0m
[32m[I 2025-02-02 11:00:34,266][0m Trial 0 finished with value: 0.14306857897580125 and parameters: {'observation_period_num': 59, 'train_rates': 0.8644441851438209, 'learning_rate': 4.576236019831369e-05, 'batch_size': 112, 'step_size': 5, 'gamma': 0.8856923879436602}. Best is trial 0 with value: 0.14306857897580125.[0m
[32m[I 2025-02-02 11:01:00,050][0m Trial 1 finished with value: 0.5061859068943855 and parameters: {'observation_period_num': 241, 'train_rates': 0.724148600126358, 'learning_rate': 4.059962517713948e-06, 'batch_size': 177, 'step_size': 12, 'gamma': 0.8701324069687927}. Best is trial 0 with value: 0.14306857897580125.[0m
[32m[I 2025-02-02 11:01:40,700][0m Trial 2 finished with value: 0.31077960700686297 and parameters: {'observation_period_num': 99, 'train_rates': 0.9297062654742476, 'learning_rate': 1.5874364594521848e-05, 'batch_size': 93, 'step_size': 6, 'gamma': 0.876642971487821}. Best is trial 0 with value: 0.14306857897580125.[0m
[32m[I 2025-02-02 11:02:03,572][0m Trial 3 finished with value: 1.7717534991539492 and parameters: {'observation_period_num': 146, 'train_rates': 0.6184493598548956, 'learning_rate': 1.7920755270340554e-06, 'batch_size': 237, 'step_size': 5, 'gamma': 0.9344711530518264}. Best is trial 0 with value: 0.14306857897580125.[0m
[32m[I 2025-02-02 11:02:27,414][0m Trial 4 finished with value: 0.3255152468275717 and parameters: {'observation_period_num': 94, 'train_rates': 0.6463539142949, 'learning_rate': 1.2916490037954588e-05, 'batch_size': 212, 'step_size': 15, 'gamma': 0.758779843166163}. Best is trial 0 with value: 0.14306857897580125.[0m
[32m[I 2025-02-02 11:02:50,817][0m Trial 5 finished with value: 0.07664195321621778 and parameters: {'observation_period_num': 9, 'train_rates': 0.6250258838195091, 'learning_rate': 0.0007565270360830328, 'batch_size': 237, 'step_size': 6, 'gamma': 0.8031029006492061}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:03:24,188][0m Trial 6 finished with value: 0.9634003358116316 and parameters: {'observation_period_num': 224, 'train_rates': 0.664286744643428, 'learning_rate': 1.427197140522027e-06, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8578694703074675}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:04:13,648][0m Trial 7 finished with value: 0.4080579185831374 and parameters: {'observation_period_num': 112, 'train_rates': 0.9517037214715375, 'learning_rate': 1.4828847979657481e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.7583271825300909}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:04:37,765][0m Trial 8 finished with value: 0.2841717678670988 and parameters: {'observation_period_num': 93, 'train_rates': 0.6491892973645883, 'learning_rate': 1.5057080011493795e-05, 'batch_size': 248, 'step_size': 12, 'gamma': 0.8181144053043525}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:05:08,558][0m Trial 9 finished with value: 0.7827393412590027 and parameters: {'observation_period_num': 135, 'train_rates': 0.9574817800783864, 'learning_rate': 2.715743487435735e-06, 'batch_size': 174, 'step_size': 6, 'gamma': 0.9731960090176005}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:07:22,966][0m Trial 10 finished with value: 0.0787185848352514 and parameters: {'observation_period_num': 8, 'train_rates': 0.7633802253570562, 'learning_rate': 0.000731570208603633, 'batch_size': 23, 'step_size': 1, 'gamma': 0.8151359498308528}. Best is trial 5 with value: 0.07664195321621778.[0m
[32m[I 2025-02-02 11:09:39,666][0m Trial 11 finished with value: 0.0737975509182538 and parameters: {'observation_period_num': 12, 'train_rates': 0.7631684866379914, 'learning_rate': 0.0009884096959338145, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8055835702287463}. Best is trial 11 with value: 0.0737975509182538.[0m
Early stopping at epoch 98
[32m[I 2025-02-02 11:13:01,405][0m Trial 12 finished with value: 0.08071455111106236 and parameters: {'observation_period_num': 13, 'train_rates': 0.8019800871669402, 'learning_rate': 0.000597959782031805, 'batch_size': 16, 'step_size': 1, 'gamma': 0.8049454318359934}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:13:27,365][0m Trial 13 finished with value: 0.1278929088128359 and parameters: {'observation_period_num': 42, 'train_rates': 0.7089298930712951, 'learning_rate': 0.00018142197870542041, 'batch_size': 163, 'step_size': 3, 'gamma': 0.785576658836199}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:13:55,738][0m Trial 14 finished with value: 0.14848466486227316 and parameters: {'observation_period_num': 185, 'train_rates': 0.8238268519895594, 'learning_rate': 0.00019022737824219633, 'batch_size': 134, 'step_size': 9, 'gamma': 0.8419960280717642}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:15:00,493][0m Trial 15 finished with value: 0.12704761256527966 and parameters: {'observation_period_num': 55, 'train_rates': 0.8749814145723673, 'learning_rate': 0.00023474119711054374, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9106373277617694}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:15:27,395][0m Trial 16 finished with value: 0.20126127983842576 and parameters: {'observation_period_num': 32, 'train_rates': 0.7125711321018361, 'learning_rate': 7.077440985747714e-05, 'batch_size': 200, 'step_size': 3, 'gamma': 0.783731431084976}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:15:58,426][0m Trial 17 finished with value: 0.10662154028567758 and parameters: {'observation_period_num': 67, 'train_rates': 0.747386442876169, 'learning_rate': 0.0004176413247191443, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8356924331263413}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:17:06,310][0m Trial 18 finished with value: 0.14828623221447207 and parameters: {'observation_period_num': 169, 'train_rates': 0.8426716289219642, 'learning_rate': 0.0009700341014351818, 'batch_size': 51, 'step_size': 2, 'gamma': 0.7899649500120957}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:17:31,008][0m Trial 19 finished with value: 0.09096132361123131 and parameters: {'observation_period_num': 5, 'train_rates': 0.6092032292592542, 'learning_rate': 0.00011538142289759349, 'batch_size': 217, 'step_size': 8, 'gamma': 0.8333050478797317}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:18:02,011][0m Trial 20 finished with value: 0.12907223387344463 and parameters: {'observation_period_num': 74, 'train_rates': 0.6787526961454035, 'learning_rate': 0.0003451389399658941, 'batch_size': 110, 'step_size': 4, 'gamma': 0.7793122971339256}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:21:33,594][0m Trial 21 finished with value: 0.08697855282580615 and parameters: {'observation_period_num': 25, 'train_rates': 0.7775729845216356, 'learning_rate': 0.0009294753184021486, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8202349640165115}. Best is trial 11 with value: 0.0737975509182538.[0m
Early stopping at epoch 91
[32m[I 2025-02-02 11:22:47,377][0m Trial 22 finished with value: 0.10278736332796777 and parameters: {'observation_period_num': 36, 'train_rates': 0.7869226588338555, 'learning_rate': 0.0005313970127862013, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8066644753995253}. Best is trial 11 with value: 0.0737975509182538.[0m
[32m[I 2025-02-02 11:24:31,427][0m Trial 23 finished with value: 0.07135370086410991 and parameters: {'observation_period_num': 10, 'train_rates': 0.74811080809927, 'learning_rate': 0.0009207298141960786, 'batch_size': 35, 'step_size': 2, 'gamma': 0.8502553261924184}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:26:03,249][0m Trial 24 finished with value: 0.10567438870766876 and parameters: {'observation_period_num': 42, 'train_rates': 0.6880652465915902, 'learning_rate': 0.00030944808411769337, 'batch_size': 38, 'step_size': 3, 'gamma': 0.8522188969141605}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:26:57,278][0m Trial 25 finished with value: 0.08608050436463231 and parameters: {'observation_period_num': 24, 'train_rates': 0.7389731569955327, 'learning_rate': 0.00012282212347714807, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8982067866165239}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:27:28,397][0m Trial 26 finished with value: 0.11540294749755056 and parameters: {'observation_period_num': 74, 'train_rates': 0.8186392121647531, 'learning_rate': 0.0004990391999144276, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9246457809906848}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:29:18,157][0m Trial 27 finished with value: 0.13446359519536297 and parameters: {'observation_period_num': 19, 'train_rates': 0.8979575994056939, 'learning_rate': 0.0009517602158647738, 'batch_size': 35, 'step_size': 11, 'gamma': 0.7982662836614777}. Best is trial 23 with value: 0.07135370086410991.[0m
Early stopping at epoch 95
[32m[I 2025-02-02 11:29:53,406][0m Trial 28 finished with value: 0.18808509121154823 and parameters: {'observation_period_num': 45, 'train_rates': 0.7583492482904939, 'learning_rate': 7.704472889647905e-05, 'batch_size': 108, 'step_size': 2, 'gamma': 0.7668607223060745}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:31:05,378][0m Trial 29 finished with value: 0.07796353846788406 and parameters: {'observation_period_num': 57, 'train_rates': 0.9871811987338763, 'learning_rate': 3.1749350984979033e-05, 'batch_size': 64, 'step_size': 5, 'gamma': 0.8284942226537303}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:31:44,388][0m Trial 30 finished with value: 0.09983910464535078 and parameters: {'observation_period_num': 55, 'train_rates': 0.6951778380331473, 'learning_rate': 0.000302287840492213, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8558716183700489}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:32:47,303][0m Trial 31 finished with value: 0.13407878960732003 and parameters: {'observation_period_num': 6, 'train_rates': 0.8553706472226055, 'learning_rate': 3.214690508329244e-05, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8272130331526866}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:34:39,660][0m Trial 32 finished with value: 0.24822517934926722 and parameters: {'observation_period_num': 23, 'train_rates': 0.91190569054037, 'learning_rate': 5.028542716449914e-06, 'batch_size': 36, 'step_size': 5, 'gamma': 0.8882604469441009}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:35:49,652][0m Trial 33 finished with value: 0.12772496044635773 and parameters: {'observation_period_num': 62, 'train_rates': 0.9852199119021683, 'learning_rate': 7.061578998043859e-06, 'batch_size': 59, 'step_size': 7, 'gamma': 0.8714444966678817}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:37:19,138][0m Trial 34 finished with value: 0.2023740781884249 and parameters: {'observation_period_num': 81, 'train_rates': 0.7316885892567749, 'learning_rate': 3.884377468223808e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8031151929752208}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:37:53,027][0m Trial 35 finished with value: 0.09284913389835246 and parameters: {'observation_period_num': 49, 'train_rates': 0.6296028782599669, 'learning_rate': 0.0006426478010933632, 'batch_size': 88, 'step_size': 6, 'gamma': 0.846239160187445}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:39:58,405][0m Trial 36 finished with value: 0.14825759935507013 and parameters: {'observation_period_num': 30, 'train_rates': 0.8885397580522528, 'learning_rate': 7.646684853924401e-06, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8641201473833406}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:40:21,710][0m Trial 37 finished with value: 0.29112452678941353 and parameters: {'observation_period_num': 113, 'train_rates': 0.6010726066821422, 'learning_rate': 7.218292140320747e-05, 'batch_size': 232, 'step_size': 4, 'gamma': 0.774849994133073}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:40:45,957][0m Trial 38 finished with value: 0.3548007504829508 and parameters: {'observation_period_num': 234, 'train_rates': 0.6668192860470259, 'learning_rate': 2.5810953260030606e-05, 'batch_size': 196, 'step_size': 7, 'gamma': 0.880295564555149}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:41:14,140][0m Trial 39 finished with value: 0.14160607422856117 and parameters: {'observation_period_num': 87, 'train_rates': 0.8060298521862648, 'learning_rate': 0.000131295739623727, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8271674813901079}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:41:51,808][0m Trial 40 finished with value: 0.11992194063576131 and parameters: {'observation_period_num': 113, 'train_rates': 0.632293022992976, 'learning_rate': 0.00023074210449577828, 'batch_size': 81, 'step_size': 5, 'gamma': 0.978934455931887}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:44:24,932][0m Trial 41 finished with value: 0.07470003082273777 and parameters: {'observation_period_num': 12, 'train_rates': 0.7678638993270344, 'learning_rate': 0.000710226300815694, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8133216998120005}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:45:46,298][0m Trial 42 finished with value: 0.08475687564212896 and parameters: {'observation_period_num': 16, 'train_rates': 0.769872391188165, 'learning_rate': 0.00042043933595190507, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8161013286747524}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:49:12,222][0m Trial 43 finished with value: 0.08649028831472 and parameters: {'observation_period_num': 6, 'train_rates': 0.827112041756876, 'learning_rate': 0.0006760792323836619, 'batch_size': 17, 'step_size': 3, 'gamma': 0.7952557768892756}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:51:17,896][0m Trial 44 finished with value: 0.6607955751552651 and parameters: {'observation_period_num': 33, 'train_rates': 0.7186677439460932, 'learning_rate': 1.0321466668516752e-06, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8406574367113001}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:51:51,318][0m Trial 45 finished with value: 0.08313430198615482 and parameters: {'observation_period_num': 19, 'train_rates': 0.7953338014763178, 'learning_rate': 0.0007214053749118955, 'batch_size': 122, 'step_size': 6, 'gamma': 0.7678701124965539}. Best is trial 23 with value: 0.07135370086410991.[0m
Early stopping at epoch 70
[32m[I 2025-02-02 11:52:36,877][0m Trial 46 finished with value: 0.2656942172211477 and parameters: {'observation_period_num': 150, 'train_rates': 0.9380626497660385, 'learning_rate': 0.0009992405286600589, 'batch_size': 61, 'step_size': 1, 'gamma': 0.8134187130190355}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:53:51,036][0m Trial 47 finished with value: 0.24902388595106187 and parameters: {'observation_period_num': 39, 'train_rates': 0.7508960571816135, 'learning_rate': 2.742454541333691e-06, 'batch_size': 45, 'step_size': 4, 'gamma': 0.9606290752809614}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:54:31,316][0m Trial 48 finished with value: 0.11477882447850575 and parameters: {'observation_period_num': 16, 'train_rates': 0.6557838709641927, 'learning_rate': 0.00044053462913694265, 'batch_size': 77, 'step_size': 3, 'gamma': 0.7502413596384997}. Best is trial 23 with value: 0.07135370086410991.[0m
[32m[I 2025-02-02 11:55:05,137][0m Trial 49 finished with value: 0.13426798670082737 and parameters: {'observation_period_num': 55, 'train_rates': 0.6991199676560564, 'learning_rate': 1.8563533766973856e-05, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8266877203523048}. Best is trial 23 with value: 0.07135370086410991.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 16, 'train_rates': 0.6659674472424055, 'learning_rate': 5.776884749770517e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.938888273377653}
Epoch 1/300, trend Loss: 0.2419 | 0.1966
Epoch 2/300, trend Loss: 0.1631 | 0.1595
Epoch 3/300, trend Loss: 0.1493 | 0.1386
Epoch 4/300, trend Loss: 0.1401 | 0.1279
Epoch 5/300, trend Loss: 0.1335 | 0.1239
Epoch 6/300, trend Loss: 0.1287 | 0.1227
Epoch 7/300, trend Loss: 0.1252 | 0.1223
Epoch 8/300, trend Loss: 0.1226 | 0.1212
Epoch 9/300, trend Loss: 0.1203 | 0.1201
Epoch 10/300, trend Loss: 0.1181 | 0.1193
Epoch 11/300, trend Loss: 0.1159 | 0.1187
Epoch 12/300, trend Loss: 0.1139 | 0.1184
Epoch 13/300, trend Loss: 0.1119 | 0.1184
Epoch 14/300, trend Loss: 0.1097 | 0.1174
Epoch 15/300, trend Loss: 0.1078 | 0.1155
Epoch 16/300, trend Loss: 0.1060 | 0.1132
Epoch 17/300, trend Loss: 0.1041 | 0.1106
Epoch 18/300, trend Loss: 0.1023 | 0.1085
Epoch 19/300, trend Loss: 0.1006 | 0.1054
Epoch 20/300, trend Loss: 0.0989 | 0.1025
Epoch 21/300, trend Loss: 0.0973 | 0.1003
Epoch 22/300, trend Loss: 0.0958 | 0.0985
Epoch 23/300, trend Loss: 0.0943 | 0.0963
Epoch 24/300, trend Loss: 0.0931 | 0.0950
Epoch 25/300, trend Loss: 0.0919 | 0.0935
Epoch 26/300, trend Loss: 0.0907 | 0.0920
Epoch 27/300, trend Loss: 0.0896 | 0.0901
Epoch 28/300, trend Loss: 0.0886 | 0.0883
Epoch 29/300, trend Loss: 0.0877 | 0.0865
Epoch 30/300, trend Loss: 0.0869 | 0.0852
Epoch 31/300, trend Loss: 0.0862 | 0.0840
Epoch 32/300, trend Loss: 0.0856 | 0.0829
Epoch 33/300, trend Loss: 0.0850 | 0.0818
Epoch 34/300, trend Loss: 0.0844 | 0.0810
Epoch 35/300, trend Loss: 0.0839 | 0.0803
Epoch 36/300, trend Loss: 0.0835 | 0.0797
Epoch 37/300, trend Loss: 0.0831 | 0.0793
Epoch 38/300, trend Loss: 0.0827 | 0.0789
Epoch 39/300, trend Loss: 0.0823 | 0.0785
Epoch 40/300, trend Loss: 0.0819 | 0.0782
Epoch 41/300, trend Loss: 0.0816 | 0.0779
Epoch 42/300, trend Loss: 0.0813 | 0.0776
Epoch 43/300, trend Loss: 0.0809 | 0.0774
Epoch 44/300, trend Loss: 0.0806 | 0.0771
Epoch 45/300, trend Loss: 0.0802 | 0.0769
Epoch 46/300, trend Loss: 0.0799 | 0.0765
Epoch 47/300, trend Loss: 0.0795 | 0.0761
Epoch 48/300, trend Loss: 0.0791 | 0.0757
Epoch 49/300, trend Loss: 0.0786 | 0.0752
Epoch 50/300, trend Loss: 0.0780 | 0.0747
Epoch 51/300, trend Loss: 0.0775 | 0.0740
Epoch 52/300, trend Loss: 0.0770 | 0.0735
Epoch 53/300, trend Loss: 0.0765 | 0.0731
Epoch 54/300, trend Loss: 0.0760 | 0.0727
Epoch 55/300, trend Loss: 0.0755 | 0.0724
Epoch 56/300, trend Loss: 0.0751 | 0.0718
Epoch 57/300, trend Loss: 0.0747 | 0.0715
Epoch 58/300, trend Loss: 0.0743 | 0.0712
Epoch 59/300, trend Loss: 0.0740 | 0.0710
Epoch 60/300, trend Loss: 0.0737 | 0.0708
Epoch 61/300, trend Loss: 0.0734 | 0.0706
Epoch 62/300, trend Loss: 0.0731 | 0.0702
Epoch 63/300, trend Loss: 0.0728 | 0.0701
Epoch 64/300, trend Loss: 0.0726 | 0.0700
Epoch 65/300, trend Loss: 0.0724 | 0.0700
Epoch 66/300, trend Loss: 0.0721 | 0.0698
Epoch 67/300, trend Loss: 0.0718 | 0.0696
Epoch 68/300, trend Loss: 0.0716 | 0.0696
Epoch 69/300, trend Loss: 0.0714 | 0.0694
Epoch 70/300, trend Loss: 0.0712 | 0.0695
Epoch 71/300, trend Loss: 0.0709 | 0.0694
Epoch 72/300, trend Loss: 0.0707 | 0.0693
Epoch 73/300, trend Loss: 0.0704 | 0.0692
Epoch 74/300, trend Loss: 0.0702 | 0.0691
Epoch 75/300, trend Loss: 0.0700 | 0.0691
Epoch 76/300, trend Loss: 0.0697 | 0.0691
Epoch 77/300, trend Loss: 0.0695 | 0.0690
Epoch 78/300, trend Loss: 0.0692 | 0.0689
Epoch 79/300, trend Loss: 0.0690 | 0.0689
Epoch 80/300, trend Loss: 0.0688 | 0.0687
Epoch 81/300, trend Loss: 0.0686 | 0.0689
Epoch 82/300, trend Loss: 0.0684 | 0.0687
Epoch 83/300, trend Loss: 0.0682 | 0.0687
Epoch 84/300, trend Loss: 0.0680 | 0.0686
Epoch 85/300, trend Loss: 0.0678 | 0.0683
Epoch 86/300, trend Loss: 0.0676 | 0.0685
Epoch 87/300, trend Loss: 0.0674 | 0.0683
Epoch 88/300, trend Loss: 0.0672 | 0.0682
Epoch 89/300, trend Loss: 0.0670 | 0.0682
Epoch 90/300, trend Loss: 0.0668 | 0.0679
Epoch 91/300, trend Loss: 0.0666 | 0.0681
Epoch 92/300, trend Loss: 0.0664 | 0.0680
Epoch 93/300, trend Loss: 0.0663 | 0.0678
Epoch 94/300, trend Loss: 0.0661 | 0.0682
Epoch 95/300, trend Loss: 0.0659 | 0.0676
Epoch 96/300, trend Loss: 0.0658 | 0.0680
Epoch 97/300, trend Loss: 0.0656 | 0.0680
Epoch 98/300, trend Loss: 0.0655 | 0.0675
Epoch 99/300, trend Loss: 0.0653 | 0.0684
Epoch 100/300, trend Loss: 0.0652 | 0.0672
Epoch 101/300, trend Loss: 0.0651 | 0.0697
Epoch 102/300, trend Loss: 0.0653 | 0.0683
Epoch 103/300, trend Loss: 0.0653 | 0.0749
Epoch 104/300, trend Loss: 0.0668 | 0.0822
Epoch 105/300, trend Loss: 0.0668 | 0.0889
Epoch 106/300, trend Loss: 0.0696 | 0.0877
Epoch 107/300, trend Loss: 0.0662 | 0.0719
Epoch 108/300, trend Loss: 0.0654 | 0.0683
Epoch 109/300, trend Loss: 0.0646 | 0.0761
Epoch 110/300, trend Loss: 0.0658 | 0.0753
Epoch 111/300, trend Loss: 0.0649 | 0.0774
Epoch 112/300, trend Loss: 0.0660 | 0.0748
Epoch 113/300, trend Loss: 0.0646 | 0.0748
Epoch 114/300, trend Loss: 0.0653 | 0.0722
Epoch 115/300, trend Loss: 0.0642 | 0.0737
Epoch 116/300, trend Loss: 0.0649 | 0.0713
Epoch 117/300, trend Loss: 0.0640 | 0.0723
Epoch 118/300, trend Loss: 0.0644 | 0.0693
Epoch 119/300, trend Loss: 0.0637 | 0.0715
Epoch 120/300, trend Loss: 0.0641 | 0.0688
Epoch 121/300, trend Loss: 0.0635 | 0.0712
Epoch 122/300, trend Loss: 0.0639 | 0.0680
Epoch 123/300, trend Loss: 0.0633 | 0.0705
Epoch 124/300, trend Loss: 0.0636 | 0.0675
Epoch 125/300, trend Loss: 0.0632 | 0.0702
Epoch 126/300, trend Loss: 0.0635 | 0.0674
Epoch 127/300, trend Loss: 0.0631 | 0.0703
Epoch 128/300, trend Loss: 0.0633 | 0.0669
Epoch 129/300, trend Loss: 0.0629 | 0.0699
Epoch 130/300, trend Loss: 0.0631 | 0.0666
Epoch 131/300, trend Loss: 0.0628 | 0.0699
Epoch 132/300, trend Loss: 0.0629 | 0.0666
Epoch 133/300, trend Loss: 0.0626 | 0.0701
Epoch 134/300, trend Loss: 0.0627 | 0.0659
Epoch 135/300, trend Loss: 0.0624 | 0.0698
Epoch 136/300, trend Loss: 0.0625 | 0.0658
Epoch 137/300, trend Loss: 0.0622 | 0.0700
Epoch 138/300, trend Loss: 0.0623 | 0.0658
Epoch 139/300, trend Loss: 0.0621 | 0.0700
Epoch 140/300, trend Loss: 0.0621 | 0.0655
Epoch 141/300, trend Loss: 0.0619 | 0.0699
Epoch 142/300, trend Loss: 0.0619 | 0.0654
Epoch 143/300, trend Loss: 0.0617 | 0.0701
Epoch 144/300, trend Loss: 0.0617 | 0.0653
Epoch 145/300, trend Loss: 0.0615 | 0.0697
Epoch 146/300, trend Loss: 0.0615 | 0.0652
Epoch 147/300, trend Loss: 0.0613 | 0.0697
Epoch 148/300, trend Loss: 0.0613 | 0.0653
Epoch 149/300, trend Loss: 0.0611 | 0.0700
Epoch 150/300, trend Loss: 0.0611 | 0.0652
Epoch 151/300, trend Loss: 0.0609 | 0.0696
Epoch 152/300, trend Loss: 0.0609 | 0.0652
Epoch 153/300, trend Loss: 0.0607 | 0.0696
Epoch 154/300, trend Loss: 0.0606 | 0.0652
Epoch 155/300, trend Loss: 0.0604 | 0.0694
Epoch 156/300, trend Loss: 0.0604 | 0.0651
Epoch 157/300, trend Loss: 0.0602 | 0.0693
Epoch 158/300, trend Loss: 0.0601 | 0.0651
Epoch 159/300, trend Loss: 0.0600 | 0.0694
Epoch 160/300, trend Loss: 0.0599 | 0.0651
Epoch 161/300, trend Loss: 0.0597 | 0.0693
Epoch 162/300, trend Loss: 0.0596 | 0.0651
Epoch 163/300, trend Loss: 0.0594 | 0.0693
Epoch 164/300, trend Loss: 0.0593 | 0.0652
Epoch 165/300, trend Loss: 0.0591 | 0.0694
Epoch 166/300, trend Loss: 0.0590 | 0.0652
Epoch 167/300, trend Loss: 0.0588 | 0.0692
Epoch 168/300, trend Loss: 0.0587 | 0.0652
Epoch 169/300, trend Loss: 0.0585 | 0.0693
Epoch 170/300, trend Loss: 0.0583 | 0.0653
Epoch 171/300, trend Loss: 0.0581 | 0.0696
Epoch 172/300, trend Loss: 0.0579 | 0.0654
Epoch 173/300, trend Loss: 0.0577 | 0.0695
Epoch 174/300, trend Loss: 0.0575 | 0.0654
Epoch 175/300, trend Loss: 0.0572 | 0.0696
Epoch 176/300, trend Loss: 0.0570 | 0.0655
Epoch 177/300, trend Loss: 0.0567 | 0.0697
Epoch 178/300, trend Loss: 0.0565 | 0.0656
Epoch 179/300, trend Loss: 0.0562 | 0.0698
Epoch 180/300, trend Loss: 0.0559 | 0.0657
Epoch 181/300, trend Loss: 0.0556 | 0.0700
Epoch 182/300, trend Loss: 0.0554 | 0.0658
Epoch 183/300, trend Loss: 0.0550 | 0.0700
Epoch 184/300, trend Loss: 0.0548 | 0.0659
Epoch 185/300, trend Loss: 0.0545 | 0.0701
Epoch 186/300, trend Loss: 0.0542 | 0.0660
Epoch 187/300, trend Loss: 0.0540 | 0.0703
Epoch 188/300, trend Loss: 0.0537 | 0.0661
Epoch 189/300, trend Loss: 0.0534 | 0.0701
Epoch 190/300, trend Loss: 0.0532 | 0.0661
Epoch 191/300, trend Loss: 0.0530 | 0.0701
Epoch 192/300, trend Loss: 0.0528 | 0.0662
Epoch 193/300, trend Loss: 0.0525 | 0.0703
Epoch 194/300, trend Loss: 0.0523 | 0.0662
Epoch 195/300, trend Loss: 0.0521 | 0.0701
Epoch 196/300, trend Loss: 0.0519 | 0.0663
Epoch 197/300, trend Loss: 0.0517 | 0.0702
Epoch 198/300, trend Loss: 0.0516 | 0.0663
Epoch 199/300, trend Loss: 0.0514 | 0.0702
Epoch 200/300, trend Loss: 0.0512 | 0.0664
Epoch 201/300, trend Loss: 0.0511 | 0.0701
Epoch 202/300, trend Loss: 0.0509 | 0.0664
Epoch 203/300, trend Loss: 0.0508 | 0.0702
Epoch 204/300, trend Loss: 0.0507 | 0.0664
Epoch 205/300, trend Loss: 0.0505 | 0.0701
Epoch 206/300, trend Loss: 0.0504 | 0.0665
Epoch 207/300, trend Loss: 0.0503 | 0.0700
Epoch 208/300, trend Loss: 0.0502 | 0.0665
Epoch 209/300, trend Loss: 0.0501 | 0.0701
Epoch 210/300, trend Loss: 0.0499 | 0.0666
Epoch 211/300, trend Loss: 0.0498 | 0.0698
Epoch 212/300, trend Loss: 0.0497 | 0.0666
Epoch 213/300, trend Loss: 0.0496 | 0.0698
Epoch 214/300, trend Loss: 0.0496 | 0.0666
Epoch 215/300, trend Loss: 0.0495 | 0.0699
Epoch 216/300, trend Loss: 0.0494 | 0.0667
Epoch 217/300, trend Loss: 0.0493 | 0.0697
Epoch 218/300, trend Loss: 0.0492 | 0.0667
Epoch 219/300, trend Loss: 0.0491 | 0.0697
Epoch 220/300, trend Loss: 0.0491 | 0.0667
Epoch 221/300, trend Loss: 0.0490 | 0.0696
Epoch 222/300, trend Loss: 0.0489 | 0.0669
Epoch 223/300, trend Loss: 0.0489 | 0.0696
Epoch 224/300, trend Loss: 0.0488 | 0.0669
Epoch 225/300, trend Loss: 0.0487 | 0.0696
Epoch 226/300, trend Loss: 0.0487 | 0.0669
Epoch 227/300, trend Loss: 0.0486 | 0.0695
Epoch 228/300, trend Loss: 0.0486 | 0.0670
Epoch 229/300, trend Loss: 0.0485 | 0.0695
Epoch 230/300, trend Loss: 0.0485 | 0.0670
Epoch 231/300, trend Loss: 0.0484 | 0.0695
Epoch 232/300, trend Loss: 0.0483 | 0.0671
Epoch 233/300, trend Loss: 0.0483 | 0.0694
Epoch 234/300, trend Loss: 0.0482 | 0.0671
Epoch 235/300, trend Loss: 0.0482 | 0.0693
Epoch 236/300, trend Loss: 0.0482 | 0.0671
Epoch 237/300, trend Loss: 0.0481 | 0.0694
Epoch 238/300, trend Loss: 0.0481 | 0.0672
Epoch 239/300, trend Loss: 0.0480 | 0.0693
Epoch 240/300, trend Loss: 0.0480 | 0.0672
Epoch 241/300, trend Loss: 0.0479 | 0.0692
Epoch 242/300, trend Loss: 0.0479 | 0.0672
Epoch 243/300, trend Loss: 0.0479 | 0.0692
Epoch 244/300, trend Loss: 0.0478 | 0.0673
Epoch 245/300, trend Loss: 0.0478 | 0.0691
Epoch 246/300, trend Loss: 0.0477 | 0.0673
Epoch 247/300, trend Loss: 0.0477 | 0.0691
Epoch 248/300, trend Loss: 0.0477 | 0.0673
Epoch 249/300, trend Loss: 0.0476 | 0.0691
Epoch 250/300, trend Loss: 0.0476 | 0.0674
Epoch 251/300, trend Loss: 0.0476 | 0.0691
Epoch 252/300, trend Loss: 0.0475 | 0.0674
Epoch 253/300, trend Loss: 0.0475 | 0.0691
Epoch 254/300, trend Loss: 0.0475 | 0.0675
Epoch 255/300, trend Loss: 0.0475 | 0.0690
Epoch 256/300, trend Loss: 0.0474 | 0.0675
Epoch 257/300, trend Loss: 0.0474 | 0.0689
Epoch 258/300, trend Loss: 0.0474 | 0.0675
Epoch 259/300, trend Loss: 0.0473 | 0.0689
Epoch 260/300, trend Loss: 0.0473 | 0.0676
Epoch 261/300, trend Loss: 0.0473 | 0.0689
Epoch 262/300, trend Loss: 0.0473 | 0.0676
Epoch 263/300, trend Loss: 0.0472 | 0.0689
Epoch 264/300, trend Loss: 0.0472 | 0.0676
Epoch 265/300, trend Loss: 0.0472 | 0.0688
Epoch 266/300, trend Loss: 0.0471 | 0.0677
Epoch 267/300, trend Loss: 0.0471 | 0.0688
Epoch 268/300, trend Loss: 0.0471 | 0.0677
Epoch 269/300, trend Loss: 0.0471 | 0.0688
Epoch 270/300, trend Loss: 0.0471 | 0.0677
Epoch 271/300, trend Loss: 0.0470 | 0.0688
Epoch 272/300, trend Loss: 0.0470 | 0.0678
Epoch 273/300, trend Loss: 0.0470 | 0.0688
Epoch 274/300, trend Loss: 0.0470 | 0.0678
Epoch 275/300, trend Loss: 0.0469 | 0.0688
Epoch 276/300, trend Loss: 0.0469 | 0.0679
Epoch 277/300, trend Loss: 0.0469 | 0.0688
Epoch 278/300, trend Loss: 0.0469 | 0.0679
Epoch 279/300, trend Loss: 0.0468 | 0.0688
Epoch 280/300, trend Loss: 0.0468 | 0.0680
Epoch 281/300, trend Loss: 0.0468 | 0.0688
Epoch 282/300, trend Loss: 0.0468 | 0.0681
Epoch 283/300, trend Loss: 0.0468 | 0.0688
Epoch 284/300, trend Loss: 0.0467 | 0.0681
Epoch 285/300, trend Loss: 0.0467 | 0.0688
Epoch 286/300, trend Loss: 0.0467 | 0.0681
Epoch 287/300, trend Loss: 0.0467 | 0.0688
Epoch 288/300, trend Loss: 0.0467 | 0.0682
Epoch 289/300, trend Loss: 0.0466 | 0.0688
Epoch 290/300, trend Loss: 0.0466 | 0.0683
Epoch 291/300, trend Loss: 0.0466 | 0.0688
Epoch 292/300, trend Loss: 0.0466 | 0.0683
Epoch 293/300, trend Loss: 0.0466 | 0.0688
Epoch 294/300, trend Loss: 0.0465 | 0.0684
Epoch 295/300, trend Loss: 0.0465 | 0.0688
Epoch 296/300, trend Loss: 0.0465 | 0.0684
Epoch 297/300, trend Loss: 0.0465 | 0.0688
Epoch 298/300, trend Loss: 0.0465 | 0.0685
Epoch 299/300, trend Loss: 0.0465 | 0.0688
Epoch 300/300, trend Loss: 0.0464 | 0.0685
Training seasonal_0 component with params: {'observation_period_num': 49, 'train_rates': 0.988103519295398, 'learning_rate': 0.0001853235005004823, 'batch_size': 28, 'step_size': 6, 'gamma': 0.7962495614845033}
Epoch 1/300, seasonal_0 Loss: 0.2800 | 0.2059
Epoch 2/300, seasonal_0 Loss: 0.1707 | 0.1232
Epoch 3/300, seasonal_0 Loss: 0.1481 | 0.1012
Epoch 4/300, seasonal_0 Loss: 0.1290 | 0.0924
Epoch 5/300, seasonal_0 Loss: 0.1182 | 0.0802
Epoch 6/300, seasonal_0 Loss: 0.1109 | 0.0741
Epoch 7/300, seasonal_0 Loss: 0.1046 | 0.0790
Epoch 8/300, seasonal_0 Loss: 0.0991 | 0.0719
Epoch 9/300, seasonal_0 Loss: 0.0952 | 0.0675
Epoch 10/300, seasonal_0 Loss: 0.0920 | 0.0665
Epoch 11/300, seasonal_0 Loss: 0.0893 | 0.0646
Epoch 12/300, seasonal_0 Loss: 0.0872 | 0.0634
Epoch 13/300, seasonal_0 Loss: 0.0856 | 0.0621
Epoch 14/300, seasonal_0 Loss: 0.0842 | 0.0613
Epoch 15/300, seasonal_0 Loss: 0.0829 | 0.0607
Epoch 16/300, seasonal_0 Loss: 0.0818 | 0.0599
Epoch 17/300, seasonal_0 Loss: 0.0814 | 0.0610
Epoch 18/300, seasonal_0 Loss: 0.0801 | 0.0600
Epoch 19/300, seasonal_0 Loss: 0.0795 | 0.0573
Epoch 20/300, seasonal_0 Loss: 0.0793 | 0.0584
Epoch 21/300, seasonal_0 Loss: 0.0780 | 0.0574
Epoch 22/300, seasonal_0 Loss: 0.0770 | 0.0558
Epoch 23/300, seasonal_0 Loss: 0.0764 | 0.0555
Epoch 24/300, seasonal_0 Loss: 0.0757 | 0.0551
Epoch 25/300, seasonal_0 Loss: 0.0750 | 0.0549
Epoch 26/300, seasonal_0 Loss: 0.0746 | 0.0547
Epoch 27/300, seasonal_0 Loss: 0.0739 | 0.0543
Epoch 28/300, seasonal_0 Loss: 0.0734 | 0.0536
Epoch 29/300, seasonal_0 Loss: 0.0729 | 0.0532
Epoch 30/300, seasonal_0 Loss: 0.0724 | 0.0529
Epoch 31/300, seasonal_0 Loss: 0.0721 | 0.0523
Epoch 32/300, seasonal_0 Loss: 0.0717 | 0.0520
Epoch 33/300, seasonal_0 Loss: 0.0713 | 0.0518
Epoch 34/300, seasonal_0 Loss: 0.0711 | 0.0514
Epoch 35/300, seasonal_0 Loss: 0.0708 | 0.0513
Epoch 36/300, seasonal_0 Loss: 0.0705 | 0.0512
Epoch 37/300, seasonal_0 Loss: 0.0702 | 0.0512
Epoch 38/300, seasonal_0 Loss: 0.0700 | 0.0511
Epoch 39/300, seasonal_0 Loss: 0.0697 | 0.0511
Epoch 40/300, seasonal_0 Loss: 0.0695 | 0.0510
Epoch 41/300, seasonal_0 Loss: 0.0692 | 0.0510
Epoch 42/300, seasonal_0 Loss: 0.0690 | 0.0509
Epoch 43/300, seasonal_0 Loss: 0.0688 | 0.0508
Epoch 44/300, seasonal_0 Loss: 0.0686 | 0.0507
Epoch 45/300, seasonal_0 Loss: 0.0685 | 0.0506
Epoch 46/300, seasonal_0 Loss: 0.0683 | 0.0504
Epoch 47/300, seasonal_0 Loss: 0.0682 | 0.0503
Epoch 48/300, seasonal_0 Loss: 0.0680 | 0.0503
Epoch 49/300, seasonal_0 Loss: 0.0679 | 0.0500
Epoch 50/300, seasonal_0 Loss: 0.0678 | 0.0500
Epoch 51/300, seasonal_0 Loss: 0.0677 | 0.0499
Epoch 52/300, seasonal_0 Loss: 0.0676 | 0.0498
Epoch 53/300, seasonal_0 Loss: 0.0675 | 0.0498
Epoch 54/300, seasonal_0 Loss: 0.0674 | 0.0498
Epoch 55/300, seasonal_0 Loss: 0.0673 | 0.0496
Epoch 56/300, seasonal_0 Loss: 0.0673 | 0.0496
Epoch 57/300, seasonal_0 Loss: 0.0672 | 0.0496
Epoch 58/300, seasonal_0 Loss: 0.0672 | 0.0496
Epoch 59/300, seasonal_0 Loss: 0.0671 | 0.0496
Epoch 60/300, seasonal_0 Loss: 0.0671 | 0.0496
Epoch 61/300, seasonal_0 Loss: 0.0670 | 0.0499
Epoch 62/300, seasonal_0 Loss: 0.0670 | 0.0499
Epoch 63/300, seasonal_0 Loss: 0.0669 | 0.0499
Epoch 64/300, seasonal_0 Loss: 0.0669 | 0.0500
Epoch 65/300, seasonal_0 Loss: 0.0668 | 0.0500
Epoch 66/300, seasonal_0 Loss: 0.0668 | 0.0499
Epoch 67/300, seasonal_0 Loss: 0.0668 | 0.0497
Epoch 68/300, seasonal_0 Loss: 0.0667 | 0.0497
Epoch 69/300, seasonal_0 Loss: 0.0667 | 0.0496
Epoch 70/300, seasonal_0 Loss: 0.0666 | 0.0494
Epoch 71/300, seasonal_0 Loss: 0.0666 | 0.0494
Epoch 72/300, seasonal_0 Loss: 0.0666 | 0.0494
Epoch 73/300, seasonal_0 Loss: 0.0665 | 0.0493
Epoch 74/300, seasonal_0 Loss: 0.0665 | 0.0493
Epoch 75/300, seasonal_0 Loss: 0.0665 | 0.0493
Epoch 76/300, seasonal_0 Loss: 0.0665 | 0.0492
Epoch 77/300, seasonal_0 Loss: 0.0665 | 0.0492
Epoch 78/300, seasonal_0 Loss: 0.0664 | 0.0492
Epoch 79/300, seasonal_0 Loss: 0.0664 | 0.0492
Epoch 80/300, seasonal_0 Loss: 0.0664 | 0.0492
Epoch 81/300, seasonal_0 Loss: 0.0664 | 0.0492
Epoch 82/300, seasonal_0 Loss: 0.0664 | 0.0492
Epoch 83/300, seasonal_0 Loss: 0.0664 | 0.0491
Epoch 84/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 85/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 86/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 87/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 88/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 89/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 90/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 91/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 92/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 93/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 94/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 95/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 96/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 97/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 98/300, seasonal_0 Loss: 0.0663 | 0.0491
Epoch 99/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 100/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 101/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 102/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 103/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 104/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 105/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 106/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 107/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 108/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 109/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 110/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 111/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 112/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 113/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 114/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 115/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 116/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 117/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 118/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 119/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 120/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 121/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 122/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 123/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 124/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 125/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 126/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 127/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 128/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 129/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 130/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 131/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 132/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 133/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 134/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 135/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 136/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 137/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 138/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 139/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 140/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 141/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 142/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 143/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 144/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 145/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 146/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 147/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 148/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 149/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 150/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 151/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 152/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 153/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 154/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 155/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 156/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 157/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 158/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 159/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 160/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 161/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 162/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 163/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 164/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 165/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 166/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 167/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 168/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 169/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 170/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 171/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 172/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 173/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 174/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 175/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 176/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 177/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 178/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 179/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 180/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 181/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 182/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 183/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 184/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 185/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 186/300, seasonal_0 Loss: 0.0662 | 0.0490
Epoch 187/300, seasonal_0 Loss: 0.0662 | 0.0490
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.6616920409899009, 'learning_rate': 0.00030025334533441444, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8975767544901151}
Epoch 1/300, seasonal_1 Loss: 0.2305 | 0.1560
Epoch 2/300, seasonal_1 Loss: 0.1439 | 0.1463
Epoch 3/300, seasonal_1 Loss: 0.1333 | 0.1386
Epoch 4/300, seasonal_1 Loss: 0.1273 | 0.1527
Epoch 5/300, seasonal_1 Loss: 0.1220 | 0.1315
Epoch 6/300, seasonal_1 Loss: 0.1169 | 0.1182
Epoch 7/300, seasonal_1 Loss: 0.1134 | 0.1035
Epoch 8/300, seasonal_1 Loss: 0.1108 | 0.1033
Epoch 9/300, seasonal_1 Loss: 0.1086 | 0.0976
Epoch 10/300, seasonal_1 Loss: 0.1072 | 0.0949
Epoch 11/300, seasonal_1 Loss: 0.1057 | 0.0890
Epoch 12/300, seasonal_1 Loss: 0.1042 | 0.0859
Epoch 13/300, seasonal_1 Loss: 0.1020 | 0.0838
Epoch 14/300, seasonal_1 Loss: 0.0995 | 0.0823
Epoch 15/300, seasonal_1 Loss: 0.0964 | 0.0854
Epoch 16/300, seasonal_1 Loss: 0.0943 | 0.0802
Epoch 17/300, seasonal_1 Loss: 0.0934 | 0.0953
Epoch 18/300, seasonal_1 Loss: 0.0948 | 0.0980
Epoch 19/300, seasonal_1 Loss: 0.0985 | 0.0929
Epoch 20/300, seasonal_1 Loss: 0.0959 | 0.0899
Epoch 21/300, seasonal_1 Loss: 0.0981 | 0.1032
Epoch 22/300, seasonal_1 Loss: 0.1015 | 0.0831
Epoch 23/300, seasonal_1 Loss: 0.0928 | 0.0758
Epoch 24/300, seasonal_1 Loss: 0.0898 | 0.0748
Epoch 25/300, seasonal_1 Loss: 0.0888 | 0.0775
Epoch 26/300, seasonal_1 Loss: 0.0885 | 0.0745
Epoch 27/300, seasonal_1 Loss: 0.0883 | 0.0861
Epoch 28/300, seasonal_1 Loss: 0.0910 | 0.0773
Epoch 29/300, seasonal_1 Loss: 0.0890 | 0.0852
Epoch 30/300, seasonal_1 Loss: 0.0906 | 0.0758
Epoch 31/300, seasonal_1 Loss: 0.0879 | 0.0791
Epoch 32/300, seasonal_1 Loss: 0.0882 | 0.0739
Epoch 33/300, seasonal_1 Loss: 0.0868 | 0.0772
Epoch 34/300, seasonal_1 Loss: 0.0870 | 0.0729
Epoch 35/300, seasonal_1 Loss: 0.0860 | 0.0757
Epoch 36/300, seasonal_1 Loss: 0.0860 | 0.0723
Epoch 37/300, seasonal_1 Loss: 0.0853 | 0.0746
Epoch 38/300, seasonal_1 Loss: 0.0852 | 0.0721
Epoch 39/300, seasonal_1 Loss: 0.0847 | 0.0738
Epoch 40/300, seasonal_1 Loss: 0.0846 | 0.0720
Epoch 41/300, seasonal_1 Loss: 0.0843 | 0.0733
Epoch 42/300, seasonal_1 Loss: 0.0841 | 0.0721
Epoch 43/300, seasonal_1 Loss: 0.0839 | 0.0730
Epoch 44/300, seasonal_1 Loss: 0.0838 | 0.0723
Epoch 45/300, seasonal_1 Loss: 0.0837 | 0.0727
Epoch 46/300, seasonal_1 Loss: 0.0836 | 0.0724
Epoch 47/300, seasonal_1 Loss: 0.0834 | 0.0725
Epoch 48/300, seasonal_1 Loss: 0.0833 | 0.0723
Epoch 49/300, seasonal_1 Loss: 0.0831 | 0.0722
Epoch 50/300, seasonal_1 Loss: 0.0830 | 0.0721
Epoch 51/300, seasonal_1 Loss: 0.0828 | 0.0720
Epoch 52/300, seasonal_1 Loss: 0.0827 | 0.0720
Epoch 53/300, seasonal_1 Loss: 0.0826 | 0.0720
Epoch 54/300, seasonal_1 Loss: 0.0826 | 0.0719
Epoch 55/300, seasonal_1 Loss: 0.0825 | 0.0719
Epoch 56/300, seasonal_1 Loss: 0.0824 | 0.0719
Epoch 57/300, seasonal_1 Loss: 0.0823 | 0.0718
Epoch 58/300, seasonal_1 Loss: 0.0822 | 0.0718
Epoch 59/300, seasonal_1 Loss: 0.0821 | 0.0716
Epoch 60/300, seasonal_1 Loss: 0.0820 | 0.0716
Epoch 61/300, seasonal_1 Loss: 0.0819 | 0.0714
Epoch 62/300, seasonal_1 Loss: 0.0818 | 0.0714
Epoch 63/300, seasonal_1 Loss: 0.0817 | 0.0713
Epoch 64/300, seasonal_1 Loss: 0.0816 | 0.0713
Epoch 65/300, seasonal_1 Loss: 0.0816 | 0.0712
Epoch 66/300, seasonal_1 Loss: 0.0815 | 0.0712
Epoch 67/300, seasonal_1 Loss: 0.0814 | 0.0711
Epoch 68/300, seasonal_1 Loss: 0.0814 | 0.0711
Epoch 69/300, seasonal_1 Loss: 0.0813 | 0.0711
Epoch 70/300, seasonal_1 Loss: 0.0813 | 0.0711
Epoch 71/300, seasonal_1 Loss: 0.0812 | 0.0710
Epoch 72/300, seasonal_1 Loss: 0.0812 | 0.0710
Epoch 73/300, seasonal_1 Loss: 0.0811 | 0.0710
Epoch 74/300, seasonal_1 Loss: 0.0811 | 0.0710
Epoch 75/300, seasonal_1 Loss: 0.0811 | 0.0710
Epoch 76/300, seasonal_1 Loss: 0.0810 | 0.0710
Epoch 77/300, seasonal_1 Loss: 0.0810 | 0.0710
Epoch 78/300, seasonal_1 Loss: 0.0810 | 0.0710
Epoch 79/300, seasonal_1 Loss: 0.0809 | 0.0710
Epoch 80/300, seasonal_1 Loss: 0.0809 | 0.0711
Epoch 81/300, seasonal_1 Loss: 0.0809 | 0.0711
Epoch 82/300, seasonal_1 Loss: 0.0808 | 0.0711
Epoch 83/300, seasonal_1 Loss: 0.0808 | 0.0711
Epoch 84/300, seasonal_1 Loss: 0.0808 | 0.0711
Epoch 85/300, seasonal_1 Loss: 0.0807 | 0.0711
Epoch 86/300, seasonal_1 Loss: 0.0807 | 0.0711
Epoch 87/300, seasonal_1 Loss: 0.0807 | 0.0711
Epoch 88/300, seasonal_1 Loss: 0.0807 | 0.0711
Epoch 89/300, seasonal_1 Loss: 0.0807 | 0.0711
Epoch 90/300, seasonal_1 Loss: 0.0806 | 0.0711
Epoch 91/300, seasonal_1 Loss: 0.0806 | 0.0711
Epoch 92/300, seasonal_1 Loss: 0.0806 | 0.0711
Epoch 93/300, seasonal_1 Loss: 0.0806 | 0.0711
Epoch 94/300, seasonal_1 Loss: 0.0806 | 0.0710
Epoch 95/300, seasonal_1 Loss: 0.0806 | 0.0710
Epoch 96/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 97/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 98/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 99/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 100/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 101/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 102/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 103/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 104/300, seasonal_1 Loss: 0.0805 | 0.0710
Epoch 105/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 106/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 107/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 108/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 109/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 110/300, seasonal_1 Loss: 0.0804 | 0.0710
Epoch 111/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 112/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 113/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 114/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 115/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 116/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 117/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 118/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 119/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 120/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 121/300, seasonal_1 Loss: 0.0804 | 0.0709
Epoch 122/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 123/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 124/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 125/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 126/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 127/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 128/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 129/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 130/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 131/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 132/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 133/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 134/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 135/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 136/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 137/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 138/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 139/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 140/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 141/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 142/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 143/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 144/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 145/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 146/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 147/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 148/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 149/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 150/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 151/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 152/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 153/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 154/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 155/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 156/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 157/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 158/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 159/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 160/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 161/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 162/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 163/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 164/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 165/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 166/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 167/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 168/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 169/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 170/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 171/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 172/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 173/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 174/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 175/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 176/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 177/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 178/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 179/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 180/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 181/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 182/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 183/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 184/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 185/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 186/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 187/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 188/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 189/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 190/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 191/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 192/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 193/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 194/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 195/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 196/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 197/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 198/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 199/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 200/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 201/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 202/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 203/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 204/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 205/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 206/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 207/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 208/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 209/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 210/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 211/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 212/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 213/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 214/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 215/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 216/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 217/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 218/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 219/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 220/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 221/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 222/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 223/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 224/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 225/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 226/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 227/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 228/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 229/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 230/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 231/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 232/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 233/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 234/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 235/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 236/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 237/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 238/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 239/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 240/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 241/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 242/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 243/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 244/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 245/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 246/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 247/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 248/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 249/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 250/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 251/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 252/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 253/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 254/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 255/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 256/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 257/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 258/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 259/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 260/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 261/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 262/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 263/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 264/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 265/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 266/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 267/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 268/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 269/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 270/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 271/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 272/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 273/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 274/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 275/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 276/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 277/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 278/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 279/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 280/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 281/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 282/300, seasonal_1 Loss: 0.0803 | 0.0709
Epoch 283/300, seasonal_1 Loss: 0.0803 | 0.0709
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.6136198146533486, 'learning_rate': 0.0009774163957408462, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8415641919000277}
Epoch 1/300, seasonal_2 Loss: 0.2075 | 0.1449
Epoch 2/300, seasonal_2 Loss: 0.1353 | 0.2555
Epoch 3/300, seasonal_2 Loss: 0.1268 | 0.1268
Epoch 4/300, seasonal_2 Loss: 0.1166 | 0.1834
Epoch 5/300, seasonal_2 Loss: 0.1091 | 0.1113
Epoch 6/300, seasonal_2 Loss: 0.1021 | 0.1021
Epoch 7/300, seasonal_2 Loss: 0.0989 | 0.2246
Epoch 8/300, seasonal_2 Loss: 0.1010 | 0.1805
Epoch 9/300, seasonal_2 Loss: 0.0994 | 0.0888
Epoch 10/300, seasonal_2 Loss: 0.0972 | 0.0966
Epoch 11/300, seasonal_2 Loss: 0.0922 | 0.0833
Epoch 12/300, seasonal_2 Loss: 0.0912 | 0.0736
Epoch 13/300, seasonal_2 Loss: 0.0883 | 0.0746
Epoch 14/300, seasonal_2 Loss: 0.0892 | 0.0904
Epoch 15/300, seasonal_2 Loss: 0.0861 | 0.0738
Epoch 16/300, seasonal_2 Loss: 0.0847 | 0.0722
Epoch 17/300, seasonal_2 Loss: 0.0864 | 0.0820
Epoch 18/300, seasonal_2 Loss: 0.0840 | 0.0722
Epoch 19/300, seasonal_2 Loss: 0.0831 | 0.0740
Epoch 20/300, seasonal_2 Loss: 0.0848 | 0.0890
Epoch 21/300, seasonal_2 Loss: 0.0834 | 0.0763
Epoch 22/300, seasonal_2 Loss: 0.0846 | 0.0835
Epoch 23/300, seasonal_2 Loss: 0.0812 | 0.0682
Epoch 24/300, seasonal_2 Loss: 0.0806 | 0.0804
Epoch 25/300, seasonal_2 Loss: 0.0790 | 0.0771
Epoch 26/300, seasonal_2 Loss: 0.0808 | 0.0815
Epoch 27/300, seasonal_2 Loss: 0.0805 | 0.0661
Epoch 28/300, seasonal_2 Loss: 0.0790 | 0.1257
Epoch 29/300, seasonal_2 Loss: 0.0795 | 0.0932
Epoch 30/300, seasonal_2 Loss: 0.0807 | 0.0731
Epoch 31/300, seasonal_2 Loss: 0.0757 | 0.0950
Epoch 32/300, seasonal_2 Loss: 0.0771 | 0.0702
Epoch 33/300, seasonal_2 Loss: 0.0746 | 0.0647
Epoch 34/300, seasonal_2 Loss: 0.0732 | 0.1285
Epoch 35/300, seasonal_2 Loss: 0.0754 | 0.0712
Epoch 36/300, seasonal_2 Loss: 0.0744 | 0.1647
Epoch 37/300, seasonal_2 Loss: 0.0756 | 0.0846
Epoch 38/300, seasonal_2 Loss: 0.0751 | 0.0709
Epoch 39/300, seasonal_2 Loss: 0.0733 | 0.0961
Epoch 40/300, seasonal_2 Loss: 0.0720 | 0.0716
Epoch 41/300, seasonal_2 Loss: 0.0757 | 0.2209
Epoch 42/300, seasonal_2 Loss: 0.0740 | 0.0627
Epoch 43/300, seasonal_2 Loss: 0.0689 | 0.0680
Epoch 44/300, seasonal_2 Loss: 0.0677 | 0.0615
Epoch 45/300, seasonal_2 Loss: 0.0672 | 0.0838
Epoch 46/300, seasonal_2 Loss: 0.0669 | 0.0708
Epoch 47/300, seasonal_2 Loss: 0.0702 | 0.1488
Epoch 48/300, seasonal_2 Loss: 0.0717 | 0.0663
Epoch 49/300, seasonal_2 Loss: 0.0663 | 0.0599
Epoch 50/300, seasonal_2 Loss: 0.0648 | 0.0817
Epoch 51/300, seasonal_2 Loss: 0.0650 | 0.0636
Epoch 52/300, seasonal_2 Loss: 0.0643 | 0.0696
Epoch 53/300, seasonal_2 Loss: 0.0655 | 0.0627
Epoch 54/300, seasonal_2 Loss: 0.0647 | 0.0629
Epoch 55/300, seasonal_2 Loss: 0.0633 | 0.0612
Epoch 56/300, seasonal_2 Loss: 0.0630 | 0.0644
Epoch 57/300, seasonal_2 Loss: 0.0617 | 0.0622
Epoch 58/300, seasonal_2 Loss: 0.0656 | 0.0701
Epoch 59/300, seasonal_2 Loss: 0.0662 | 0.0621
Epoch 60/300, seasonal_2 Loss: 0.0638 | 0.0810
Epoch 61/300, seasonal_2 Loss: 0.0620 | 0.0630
Epoch 62/300, seasonal_2 Loss: 0.0601 | 0.0635
Epoch 63/300, seasonal_2 Loss: 0.0576 | 0.0626
Epoch 64/300, seasonal_2 Loss: 0.0635 | 0.0701
Epoch 65/300, seasonal_2 Loss: 0.0619 | 0.0624
Epoch 66/300, seasonal_2 Loss: 0.0615 | 0.0637
Epoch 67/300, seasonal_2 Loss: 0.0605 | 0.0634
Epoch 68/300, seasonal_2 Loss: 0.0605 | 0.0628
Epoch 69/300, seasonal_2 Loss: 0.0599 | 0.0639
Epoch 70/300, seasonal_2 Loss: 0.0599 | 0.0635
Epoch 71/300, seasonal_2 Loss: 0.0606 | 0.0625
Epoch 72/300, seasonal_2 Loss: 0.0617 | 0.0639
Epoch 73/300, seasonal_2 Loss: 0.0609 | 0.0644
Epoch 74/300, seasonal_2 Loss: 0.0598 | 0.0636
Epoch 75/300, seasonal_2 Loss: 0.0572 | 0.0621
Epoch 76/300, seasonal_2 Loss: 0.0604 | 0.0621
Epoch 77/300, seasonal_2 Loss: 0.0594 | 0.0624
Epoch 78/300, seasonal_2 Loss: 0.0590 | 0.0627
Epoch 79/300, seasonal_2 Loss: 0.0587 | 0.0628
Epoch 80/300, seasonal_2 Loss: 0.0584 | 0.0631
Epoch 81/300, seasonal_2 Loss: 0.0582 | 0.0636
Epoch 82/300, seasonal_2 Loss: 0.0577 | 0.0636
Epoch 83/300, seasonal_2 Loss: 0.0576 | 0.0641
Epoch 84/300, seasonal_2 Loss: 0.0547 | 0.0648
Epoch 85/300, seasonal_2 Loss: 0.0507 | 0.0644
Epoch 86/300, seasonal_2 Loss: 0.0582 | 0.0636
Epoch 87/300, seasonal_2 Loss: 0.0508 | 0.0646
Epoch 88/300, seasonal_2 Loss: 0.0488 | 0.0635
Epoch 89/300, seasonal_2 Loss: 0.0514 | 0.0666
Epoch 90/300, seasonal_2 Loss: 0.0484 | 0.0635
Epoch 91/300, seasonal_2 Loss: 0.0454 | 0.0635
Epoch 92/300, seasonal_2 Loss: 0.0446 | 0.0634
Epoch 93/300, seasonal_2 Loss: 0.0443 | 0.0638
Epoch 94/300, seasonal_2 Loss: 0.0439 | 0.0635
Epoch 95/300, seasonal_2 Loss: 0.0437 | 0.0643
Epoch 96/300, seasonal_2 Loss: 0.0433 | 0.0636
Epoch 97/300, seasonal_2 Loss: 0.0433 | 0.0649
Epoch 98/300, seasonal_2 Loss: 0.0429 | 0.0638
Epoch 99/300, seasonal_2 Loss: 0.0429 | 0.0655
Epoch 100/300, seasonal_2 Loss: 0.0426 | 0.0641
Epoch 101/300, seasonal_2 Loss: 0.0426 | 0.0664
Epoch 102/300, seasonal_2 Loss: 0.0423 | 0.0646
Epoch 103/300, seasonal_2 Loss: 0.0422 | 0.0658
Epoch 104/300, seasonal_2 Loss: 0.0418 | 0.0645
Epoch 105/300, seasonal_2 Loss: 0.0417 | 0.0656
Epoch 106/300, seasonal_2 Loss: 0.0414 | 0.0648
Epoch 107/300, seasonal_2 Loss: 0.0413 | 0.0657
Epoch 108/300, seasonal_2 Loss: 0.0410 | 0.0651
Epoch 109/300, seasonal_2 Loss: 0.0409 | 0.0659
Epoch 110/300, seasonal_2 Loss: 0.0407 | 0.0653
Epoch 111/300, seasonal_2 Loss: 0.0406 | 0.0662
Epoch 112/300, seasonal_2 Loss: 0.0404 | 0.0656
Epoch 113/300, seasonal_2 Loss: 0.0403 | 0.0666
Epoch 114/300, seasonal_2 Loss: 0.0401 | 0.0658
Epoch 115/300, seasonal_2 Loss: 0.0401 | 0.0670
Epoch 116/300, seasonal_2 Loss: 0.0398 | 0.0660
Epoch 117/300, seasonal_2 Loss: 0.0398 | 0.0673
Epoch 118/300, seasonal_2 Loss: 0.0396 | 0.0663
Epoch 119/300, seasonal_2 Loss: 0.0395 | 0.0677
Epoch 120/300, seasonal_2 Loss: 0.0393 | 0.0666
Epoch 121/300, seasonal_2 Loss: 0.0393 | 0.0677
Epoch 122/300, seasonal_2 Loss: 0.0391 | 0.0670
Epoch 123/300, seasonal_2 Loss: 0.0390 | 0.0678
Epoch 124/300, seasonal_2 Loss: 0.0389 | 0.0672
Epoch 125/300, seasonal_2 Loss: 0.0388 | 0.0679
Epoch 126/300, seasonal_2 Loss: 0.0386 | 0.0675
Epoch 127/300, seasonal_2 Loss: 0.0386 | 0.0682
Epoch 128/300, seasonal_2 Loss: 0.0385 | 0.0679
Epoch 129/300, seasonal_2 Loss: 0.0384 | 0.0686
Epoch 130/300, seasonal_2 Loss: 0.0383 | 0.0685
Epoch 131/300, seasonal_2 Loss: 0.0383 | 0.0694
Epoch 132/300, seasonal_2 Loss: 0.0382 | 0.0692
Epoch 133/300, seasonal_2 Loss: 0.0382 | 0.0695
Epoch 134/300, seasonal_2 Loss: 0.0380 | 0.0689
Epoch 135/300, seasonal_2 Loss: 0.0379 | 0.0693
Epoch 136/300, seasonal_2 Loss: 0.0377 | 0.0689
Epoch 137/300, seasonal_2 Loss: 0.0377 | 0.0692
Epoch 138/300, seasonal_2 Loss: 0.0375 | 0.0688
Epoch 139/300, seasonal_2 Loss: 0.0375 | 0.0691
Epoch 140/300, seasonal_2 Loss: 0.0373 | 0.0689
Epoch 141/300, seasonal_2 Loss: 0.0373 | 0.0692
Epoch 142/300, seasonal_2 Loss: 0.0372 | 0.0690
Epoch 143/300, seasonal_2 Loss: 0.0371 | 0.0694
Epoch 144/300, seasonal_2 Loss: 0.0370 | 0.0692
Epoch 145/300, seasonal_2 Loss: 0.0370 | 0.0695
Epoch 146/300, seasonal_2 Loss: 0.0369 | 0.0694
Epoch 147/300, seasonal_2 Loss: 0.0368 | 0.0696
Epoch 148/300, seasonal_2 Loss: 0.0368 | 0.0696
Epoch 149/300, seasonal_2 Loss: 0.0367 | 0.0698
Epoch 150/300, seasonal_2 Loss: 0.0367 | 0.0697
Epoch 151/300, seasonal_2 Loss: 0.0366 | 0.0700
Epoch 152/300, seasonal_2 Loss: 0.0365 | 0.0699
Epoch 153/300, seasonal_2 Loss: 0.0365 | 0.0700
Epoch 154/300, seasonal_2 Loss: 0.0364 | 0.0700
Epoch 155/300, seasonal_2 Loss: 0.0364 | 0.0702
Epoch 156/300, seasonal_2 Loss: 0.0363 | 0.0702
Epoch 157/300, seasonal_2 Loss: 0.0363 | 0.0703
Epoch 158/300, seasonal_2 Loss: 0.0362 | 0.0703
Epoch 159/300, seasonal_2 Loss: 0.0362 | 0.0704
Epoch 160/300, seasonal_2 Loss: 0.0362 | 0.0704
Epoch 161/300, seasonal_2 Loss: 0.0361 | 0.0705
Epoch 162/300, seasonal_2 Loss: 0.0361 | 0.0705
Epoch 163/300, seasonal_2 Loss: 0.0360 | 0.0706
Epoch 164/300, seasonal_2 Loss: 0.0360 | 0.0706
Epoch 165/300, seasonal_2 Loss: 0.0360 | 0.0707
Epoch 166/300, seasonal_2 Loss: 0.0359 | 0.0707
Epoch 167/300, seasonal_2 Loss: 0.0359 | 0.0708
Epoch 168/300, seasonal_2 Loss: 0.0358 | 0.0709
Epoch 169/300, seasonal_2 Loss: 0.0358 | 0.0709
Epoch 170/300, seasonal_2 Loss: 0.0358 | 0.0710
Epoch 171/300, seasonal_2 Loss: 0.0358 | 0.0710
Epoch 172/300, seasonal_2 Loss: 0.0357 | 0.0711
Epoch 173/300, seasonal_2 Loss: 0.0357 | 0.0712
Epoch 174/300, seasonal_2 Loss: 0.0357 | 0.0712
Epoch 175/300, seasonal_2 Loss: 0.0357 | 0.0713
Epoch 176/300, seasonal_2 Loss: 0.0356 | 0.0713
Epoch 177/300, seasonal_2 Loss: 0.0356 | 0.0714
Epoch 178/300, seasonal_2 Loss: 0.0356 | 0.0714
Epoch 179/300, seasonal_2 Loss: 0.0356 | 0.0715
Epoch 180/300, seasonal_2 Loss: 0.0355 | 0.0716
Epoch 181/300, seasonal_2 Loss: 0.0355 | 0.0716
Epoch 182/300, seasonal_2 Loss: 0.0355 | 0.0716
Epoch 183/300, seasonal_2 Loss: 0.0355 | 0.0717
Epoch 184/300, seasonal_2 Loss: 0.0354 | 0.0717
Epoch 185/300, seasonal_2 Loss: 0.0354 | 0.0718
Epoch 186/300, seasonal_2 Loss: 0.0354 | 0.0718
Epoch 187/300, seasonal_2 Loss: 0.0354 | 0.0719
Epoch 188/300, seasonal_2 Loss: 0.0354 | 0.0719
Epoch 189/300, seasonal_2 Loss: 0.0353 | 0.0719
Epoch 190/300, seasonal_2 Loss: 0.0353 | 0.0720
Epoch 191/300, seasonal_2 Loss: 0.0353 | 0.0720
Epoch 192/300, seasonal_2 Loss: 0.0353 | 0.0720
Epoch 193/300, seasonal_2 Loss: 0.0352 | 0.0721
Epoch 194/300, seasonal_2 Loss: 0.0352 | 0.0721
Epoch 195/300, seasonal_2 Loss: 0.0352 | 0.0721
Epoch 196/300, seasonal_2 Loss: 0.0352 | 0.0722
Epoch 197/300, seasonal_2 Loss: 0.0352 | 0.0722
Epoch 198/300, seasonal_2 Loss: 0.0352 | 0.0722
Epoch 199/300, seasonal_2 Loss: 0.0351 | 0.0723
Epoch 200/300, seasonal_2 Loss: 0.0351 | 0.0723
Epoch 201/300, seasonal_2 Loss: 0.0351 | 0.0723
Epoch 202/300, seasonal_2 Loss: 0.0351 | 0.0723
Epoch 203/300, seasonal_2 Loss: 0.0351 | 0.0723
Epoch 204/300, seasonal_2 Loss: 0.0351 | 0.0724
Epoch 205/300, seasonal_2 Loss: 0.0350 | 0.0724
Epoch 206/300, seasonal_2 Loss: 0.0350 | 0.0724
Epoch 207/300, seasonal_2 Loss: 0.0350 | 0.0724
Epoch 208/300, seasonal_2 Loss: 0.0350 | 0.0725
Epoch 209/300, seasonal_2 Loss: 0.0350 | 0.0725
Epoch 210/300, seasonal_2 Loss: 0.0350 | 0.0725
Epoch 211/300, seasonal_2 Loss: 0.0350 | 0.0725
Epoch 212/300, seasonal_2 Loss: 0.0350 | 0.0725
Epoch 213/300, seasonal_2 Loss: 0.0349 | 0.0726
Epoch 214/300, seasonal_2 Loss: 0.0349 | 0.0726
Epoch 215/300, seasonal_2 Loss: 0.0349 | 0.0726
Epoch 216/300, seasonal_2 Loss: 0.0349 | 0.0726
Epoch 217/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 218/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 219/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 220/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 221/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 222/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 223/300, seasonal_2 Loss: 0.0349 | 0.0727
Epoch 224/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 225/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 226/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 227/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 228/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 229/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 230/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 231/300, seasonal_2 Loss: 0.0348 | 0.0728
Epoch 232/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 233/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 234/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 235/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 236/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 237/300, seasonal_2 Loss: 0.0348 | 0.0729
Epoch 238/300, seasonal_2 Loss: 0.0347 | 0.0729
Epoch 239/300, seasonal_2 Loss: 0.0347 | 0.0729
Epoch 240/300, seasonal_2 Loss: 0.0347 | 0.0729
Epoch 241/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 242/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 243/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 244/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 245/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 246/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 247/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 248/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 249/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 250/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 251/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 252/300, seasonal_2 Loss: 0.0347 | 0.0730
Epoch 253/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 254/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 255/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 256/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 257/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 258/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 259/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 260/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 261/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 262/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 263/300, seasonal_2 Loss: 0.0347 | 0.0731
Epoch 264/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 265/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 266/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 267/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 268/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 269/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 270/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 271/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 272/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 273/300, seasonal_2 Loss: 0.0346 | 0.0731
Epoch 274/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 275/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 276/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 277/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 278/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 279/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 280/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 281/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 282/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 283/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 284/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 285/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 286/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 287/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 288/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 289/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 290/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 291/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 292/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 293/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 294/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 295/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 296/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 297/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 298/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 299/300, seasonal_2 Loss: 0.0346 | 0.0732
Epoch 300/300, seasonal_2 Loss: 0.0346 | 0.0732
Training seasonal_3 component with params: {'observation_period_num': 23, 'train_rates': 0.9891688445832926, 'learning_rate': 0.0006654625268226993, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9095385174594361}
Epoch 1/300, seasonal_3 Loss: 0.4018 | 0.2409
Epoch 2/300, seasonal_3 Loss: 0.2728 | 0.1361
Epoch 3/300, seasonal_3 Loss: 0.1708 | 0.1545
Epoch 4/300, seasonal_3 Loss: 0.1568 | 0.0917
Epoch 5/300, seasonal_3 Loss: 0.1463 | 0.2202
Epoch 6/300, seasonal_3 Loss: 0.1654 | 0.0978
Epoch 7/300, seasonal_3 Loss: 0.1347 | 0.0722
Epoch 8/300, seasonal_3 Loss: 0.1268 | 0.0905
Epoch 9/300, seasonal_3 Loss: 0.1241 | 0.0893
Epoch 10/300, seasonal_3 Loss: 0.1254 | 0.1130
Epoch 11/300, seasonal_3 Loss: 0.1382 | 0.1035
Epoch 12/300, seasonal_3 Loss: 0.1359 | 0.0798
Epoch 13/300, seasonal_3 Loss: 0.1221 | 0.0809
Epoch 14/300, seasonal_3 Loss: 0.1098 | 0.0796
Epoch 15/300, seasonal_3 Loss: 0.1052 | 0.0928
Epoch 16/300, seasonal_3 Loss: 0.1099 | 0.1004
Epoch 17/300, seasonal_3 Loss: 0.1138 | 0.0975
Epoch 18/300, seasonal_3 Loss: 0.1136 | 0.1166
Epoch 19/300, seasonal_3 Loss: 0.1342 | 0.1100
Epoch 20/300, seasonal_3 Loss: 0.1485 | 0.1043
Epoch 21/300, seasonal_3 Loss: 0.1427 | 0.0937
Epoch 22/300, seasonal_3 Loss: 0.1411 | 0.0755
Epoch 23/300, seasonal_3 Loss: 0.1295 | 0.0718
Epoch 24/300, seasonal_3 Loss: 0.1342 | 0.1613
Epoch 25/300, seasonal_3 Loss: 0.1667 | 0.3164
Epoch 26/300, seasonal_3 Loss: 0.1573 | 0.1005
Epoch 27/300, seasonal_3 Loss: 0.1148 | 0.0686
Epoch 28/300, seasonal_3 Loss: 0.0939 | 0.0566
Epoch 29/300, seasonal_3 Loss: 0.0988 | 0.0610
Epoch 30/300, seasonal_3 Loss: 0.1168 | 0.0546
Epoch 31/300, seasonal_3 Loss: 0.1170 | 0.0568
Epoch 32/300, seasonal_3 Loss: 0.1182 | 0.0604
Epoch 33/300, seasonal_3 Loss: 0.0966 | 0.0614
Epoch 34/300, seasonal_3 Loss: 0.0914 | 0.0613
Epoch 35/300, seasonal_3 Loss: 0.0901 | 0.0571
Epoch 36/300, seasonal_3 Loss: 0.0954 | 0.0752
Epoch 37/300, seasonal_3 Loss: 0.0939 | 0.1025
Epoch 38/300, seasonal_3 Loss: 0.0889 | 0.0498
Epoch 39/300, seasonal_3 Loss: 0.0798 | 0.0479
Epoch 40/300, seasonal_3 Loss: 0.0795 | 0.0476
Epoch 41/300, seasonal_3 Loss: 0.0801 | 0.0479
Epoch 42/300, seasonal_3 Loss: 0.0786 | 0.0489
Epoch 43/300, seasonal_3 Loss: 0.0770 | 0.0470
Epoch 44/300, seasonal_3 Loss: 0.0769 | 0.0519
Epoch 45/300, seasonal_3 Loss: 0.0810 | 0.0645
Epoch 46/300, seasonal_3 Loss: 0.0794 | 0.0615
Epoch 47/300, seasonal_3 Loss: 0.0775 | 0.0494
Epoch 48/300, seasonal_3 Loss: 0.0748 | 0.0446
Epoch 49/300, seasonal_3 Loss: 0.0749 | 0.0422
Epoch 50/300, seasonal_3 Loss: 0.0767 | 0.0434
Epoch 51/300, seasonal_3 Loss: 0.0763 | 0.0426
Epoch 52/300, seasonal_3 Loss: 0.0749 | 0.0445
Epoch 53/300, seasonal_3 Loss: 0.0775 | 0.0514
Epoch 54/300, seasonal_3 Loss: 0.0759 | 0.0518
Epoch 55/300, seasonal_3 Loss: 0.0729 | 0.0435
Epoch 56/300, seasonal_3 Loss: 0.0698 | 0.0394
Epoch 57/300, seasonal_3 Loss: 0.0680 | 0.0380
Epoch 58/300, seasonal_3 Loss: 0.0682 | 0.0377
Epoch 59/300, seasonal_3 Loss: 0.0683 | 0.0378
Epoch 60/300, seasonal_3 Loss: 0.0681 | 0.0388
Epoch 61/300, seasonal_3 Loss: 0.0681 | 0.0401
Epoch 62/300, seasonal_3 Loss: 0.0682 | 0.0419
Epoch 63/300, seasonal_3 Loss: 0.0676 | 0.0420
Epoch 64/300, seasonal_3 Loss: 0.0666 | 0.0390
Epoch 65/300, seasonal_3 Loss: 0.0656 | 0.0358
Epoch 66/300, seasonal_3 Loss: 0.0647 | 0.0341
Epoch 67/300, seasonal_3 Loss: 0.0644 | 0.0331
Epoch 68/300, seasonal_3 Loss: 0.0642 | 0.0324
Epoch 69/300, seasonal_3 Loss: 0.0640 | 0.0320
Epoch 70/300, seasonal_3 Loss: 0.0637 | 0.0316
Epoch 71/300, seasonal_3 Loss: 0.0635 | 0.0318
Epoch 72/300, seasonal_3 Loss: 0.0640 | 0.0334
Epoch 73/300, seasonal_3 Loss: 0.0648 | 0.0362
Epoch 74/300, seasonal_3 Loss: 0.0656 | 0.0360
Epoch 75/300, seasonal_3 Loss: 0.0649 | 0.0332
Epoch 76/300, seasonal_3 Loss: 0.0632 | 0.0293
Epoch 77/300, seasonal_3 Loss: 0.0625 | 0.0272
Epoch 78/300, seasonal_3 Loss: 0.0636 | 0.0271
Epoch 79/300, seasonal_3 Loss: 0.0652 | 0.0284
Epoch 80/300, seasonal_3 Loss: 0.0653 | 0.0280
Epoch 81/300, seasonal_3 Loss: 0.0639 | 0.0271
Epoch 82/300, seasonal_3 Loss: 0.0626 | 0.0280
Epoch 83/300, seasonal_3 Loss: 0.0623 | 0.0297
Epoch 84/300, seasonal_3 Loss: 0.0620 | 0.0295
Epoch 85/300, seasonal_3 Loss: 0.0612 | 0.0274
Epoch 86/300, seasonal_3 Loss: 0.0602 | 0.0255
Epoch 87/300, seasonal_3 Loss: 0.0597 | 0.0243
Epoch 88/300, seasonal_3 Loss: 0.0597 | 0.0240
Epoch 89/300, seasonal_3 Loss: 0.0597 | 0.0239
Epoch 90/300, seasonal_3 Loss: 0.0595 | 0.0237
Epoch 91/300, seasonal_3 Loss: 0.0591 | 0.0237
Epoch 92/300, seasonal_3 Loss: 0.0589 | 0.0238
Epoch 93/300, seasonal_3 Loss: 0.0587 | 0.0241
Epoch 94/300, seasonal_3 Loss: 0.0585 | 0.0241
Epoch 95/300, seasonal_3 Loss: 0.0583 | 0.0238
Epoch 96/300, seasonal_3 Loss: 0.0581 | 0.0233
Epoch 97/300, seasonal_3 Loss: 0.0579 | 0.0228
Epoch 98/300, seasonal_3 Loss: 0.0577 | 0.0226
Epoch 99/300, seasonal_3 Loss: 0.0576 | 0.0224
Epoch 100/300, seasonal_3 Loss: 0.0575 | 0.0223
Epoch 101/300, seasonal_3 Loss: 0.0574 | 0.0223
Epoch 102/300, seasonal_3 Loss: 0.0573 | 0.0224
Epoch 103/300, seasonal_3 Loss: 0.0573 | 0.0224
Epoch 104/300, seasonal_3 Loss: 0.0571 | 0.0224
Epoch 105/300, seasonal_3 Loss: 0.0570 | 0.0221
Epoch 106/300, seasonal_3 Loss: 0.0568 | 0.0218
Epoch 107/300, seasonal_3 Loss: 0.0569 | 0.0216
Epoch 108/300, seasonal_3 Loss: 0.0579 | 0.0233
Epoch 109/300, seasonal_3 Loss: 0.0600 | 0.0222
Epoch 110/300, seasonal_3 Loss: 0.0576 | 0.0224
Epoch 111/300, seasonal_3 Loss: 0.0563 | 0.0229
Epoch 112/300, seasonal_3 Loss: 0.0561 | 0.0231
Epoch 113/300, seasonal_3 Loss: 0.0560 | 0.0235
Epoch 114/300, seasonal_3 Loss: 0.0560 | 0.0232
Epoch 115/300, seasonal_3 Loss: 0.0558 | 0.0227
Epoch 116/300, seasonal_3 Loss: 0.0553 | 0.0225
Epoch 117/300, seasonal_3 Loss: 0.0550 | 0.0224
Epoch 118/300, seasonal_3 Loss: 0.0547 | 0.0225
Epoch 119/300, seasonal_3 Loss: 0.0546 | 0.0226
Epoch 120/300, seasonal_3 Loss: 0.0544 | 0.0226
Epoch 121/300, seasonal_3 Loss: 0.0542 | 0.0227
Epoch 122/300, seasonal_3 Loss: 0.0541 | 0.0228
Epoch 123/300, seasonal_3 Loss: 0.0540 | 0.0229
Epoch 124/300, seasonal_3 Loss: 0.0539 | 0.0230
Epoch 125/300, seasonal_3 Loss: 0.0538 | 0.0230
Epoch 126/300, seasonal_3 Loss: 0.0537 | 0.0229
Epoch 127/300, seasonal_3 Loss: 0.0535 | 0.0226
Epoch 128/300, seasonal_3 Loss: 0.0534 | 0.0224
Epoch 129/300, seasonal_3 Loss: 0.0533 | 0.0221
Epoch 130/300, seasonal_3 Loss: 0.0534 | 0.0219
Epoch 131/300, seasonal_3 Loss: 0.0537 | 0.0218
Epoch 132/300, seasonal_3 Loss: 0.0542 | 0.0222
Epoch 133/300, seasonal_3 Loss: 0.0543 | 0.0220
Epoch 134/300, seasonal_3 Loss: 0.0535 | 0.0215
Epoch 135/300, seasonal_3 Loss: 0.0529 | 0.0215
Epoch 136/300, seasonal_3 Loss: 0.0533 | 0.0222
Epoch 137/300, seasonal_3 Loss: 0.0534 | 0.0227
Epoch 138/300, seasonal_3 Loss: 0.0529 | 0.0226
Epoch 139/300, seasonal_3 Loss: 0.0524 | 0.0223
Epoch 140/300, seasonal_3 Loss: 0.0522 | 0.0220
Epoch 141/300, seasonal_3 Loss: 0.0522 | 0.0217
Epoch 142/300, seasonal_3 Loss: 0.0522 | 0.0215
Epoch 143/300, seasonal_3 Loss: 0.0522 | 0.0214
Epoch 144/300, seasonal_3 Loss: 0.0521 | 0.0214
Epoch 145/300, seasonal_3 Loss: 0.0519 | 0.0214
Epoch 146/300, seasonal_3 Loss: 0.0518 | 0.0214
Epoch 147/300, seasonal_3 Loss: 0.0516 | 0.0215
Epoch 148/300, seasonal_3 Loss: 0.0516 | 0.0216
Epoch 149/300, seasonal_3 Loss: 0.0515 | 0.0218
Epoch 150/300, seasonal_3 Loss: 0.0515 | 0.0221
Epoch 151/300, seasonal_3 Loss: 0.0514 | 0.0221
Epoch 152/300, seasonal_3 Loss: 0.0513 | 0.0220
Epoch 153/300, seasonal_3 Loss: 0.0512 | 0.0219
Epoch 154/300, seasonal_3 Loss: 0.0511 | 0.0217
Epoch 155/300, seasonal_3 Loss: 0.0511 | 0.0216
Epoch 156/300, seasonal_3 Loss: 0.0511 | 0.0215
Epoch 157/300, seasonal_3 Loss: 0.0510 | 0.0214
Epoch 158/300, seasonal_3 Loss: 0.0510 | 0.0214
Epoch 159/300, seasonal_3 Loss: 0.0510 | 0.0214
Epoch 160/300, seasonal_3 Loss: 0.0509 | 0.0214
Epoch 161/300, seasonal_3 Loss: 0.0507 | 0.0215
Epoch 162/300, seasonal_3 Loss: 0.0507 | 0.0216
Epoch 163/300, seasonal_3 Loss: 0.0506 | 0.0218
Epoch 164/300, seasonal_3 Loss: 0.0506 | 0.0219
Epoch 165/300, seasonal_3 Loss: 0.0505 | 0.0219
Epoch 166/300, seasonal_3 Loss: 0.0505 | 0.0219
Epoch 167/300, seasonal_3 Loss: 0.0504 | 0.0218
Epoch 168/300, seasonal_3 Loss: 0.0504 | 0.0217
Epoch 169/300, seasonal_3 Loss: 0.0503 | 0.0216
Epoch 170/300, seasonal_3 Loss: 0.0503 | 0.0216
Epoch 171/300, seasonal_3 Loss: 0.0503 | 0.0215
Epoch 172/300, seasonal_3 Loss: 0.0502 | 0.0215
Epoch 173/300, seasonal_3 Loss: 0.0502 | 0.0215
Epoch 174/300, seasonal_3 Loss: 0.0501 | 0.0216
Epoch 175/300, seasonal_3 Loss: 0.0501 | 0.0217
Epoch 176/300, seasonal_3 Loss: 0.0500 | 0.0217
Epoch 177/300, seasonal_3 Loss: 0.0500 | 0.0218
Epoch 178/300, seasonal_3 Loss: 0.0499 | 0.0218
Epoch 179/300, seasonal_3 Loss: 0.0499 | 0.0218
Epoch 180/300, seasonal_3 Loss: 0.0499 | 0.0218
Epoch 181/300, seasonal_3 Loss: 0.0498 | 0.0218
Epoch 182/300, seasonal_3 Loss: 0.0498 | 0.0218
Epoch 183/300, seasonal_3 Loss: 0.0498 | 0.0217
Epoch 184/300, seasonal_3 Loss: 0.0497 | 0.0217
Epoch 185/300, seasonal_3 Loss: 0.0497 | 0.0217
Epoch 186/300, seasonal_3 Loss: 0.0497 | 0.0217
Epoch 187/300, seasonal_3 Loss: 0.0496 | 0.0218
Epoch 188/300, seasonal_3 Loss: 0.0496 | 0.0218
Epoch 189/300, seasonal_3 Loss: 0.0496 | 0.0218
Epoch 190/300, seasonal_3 Loss: 0.0495 | 0.0218
Epoch 191/300, seasonal_3 Loss: 0.0495 | 0.0219
Epoch 192/300, seasonal_3 Loss: 0.0495 | 0.0219
Epoch 193/300, seasonal_3 Loss: 0.0494 | 0.0219
Epoch 194/300, seasonal_3 Loss: 0.0494 | 0.0219
Epoch 195/300, seasonal_3 Loss: 0.0494 | 0.0219
Epoch 196/300, seasonal_3 Loss: 0.0494 | 0.0219
Epoch 197/300, seasonal_3 Loss: 0.0493 | 0.0219
Epoch 198/300, seasonal_3 Loss: 0.0493 | 0.0219
Epoch 199/300, seasonal_3 Loss: 0.0493 | 0.0219
Epoch 200/300, seasonal_3 Loss: 0.0493 | 0.0219
Epoch 201/300, seasonal_3 Loss: 0.0492 | 0.0219
Epoch 202/300, seasonal_3 Loss: 0.0492 | 0.0219
Epoch 203/300, seasonal_3 Loss: 0.0492 | 0.0219
Epoch 204/300, seasonal_3 Loss: 0.0492 | 0.0220
Epoch 205/300, seasonal_3 Loss: 0.0491 | 0.0220
Epoch 206/300, seasonal_3 Loss: 0.0491 | 0.0220
Epoch 207/300, seasonal_3 Loss: 0.0491 | 0.0220
Epoch 208/300, seasonal_3 Loss: 0.0491 | 0.0220
Epoch 209/300, seasonal_3 Loss: 0.0490 | 0.0220
Epoch 210/300, seasonal_3 Loss: 0.0490 | 0.0220
Epoch 211/300, seasonal_3 Loss: 0.0490 | 0.0220
Epoch 212/300, seasonal_3 Loss: 0.0490 | 0.0220
Epoch 213/300, seasonal_3 Loss: 0.0490 | 0.0220
Epoch 214/300, seasonal_3 Loss: 0.0489 | 0.0220
Epoch 215/300, seasonal_3 Loss: 0.0489 | 0.0221
Epoch 216/300, seasonal_3 Loss: 0.0489 | 0.0221
Epoch 217/300, seasonal_3 Loss: 0.0489 | 0.0221
Epoch 218/300, seasonal_3 Loss: 0.0489 | 0.0221
Epoch 219/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 220/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 221/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 222/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 223/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 224/300, seasonal_3 Loss: 0.0488 | 0.0221
Epoch 225/300, seasonal_3 Loss: 0.0487 | 0.0221
Epoch 226/300, seasonal_3 Loss: 0.0487 | 0.0221
Epoch 227/300, seasonal_3 Loss: 0.0487 | 0.0221
Epoch 228/300, seasonal_3 Loss: 0.0487 | 0.0222
Epoch 229/300, seasonal_3 Loss: 0.0487 | 0.0222
Epoch 230/300, seasonal_3 Loss: 0.0487 | 0.0222
Epoch 231/300, seasonal_3 Loss: 0.0487 | 0.0222
Epoch 232/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 233/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 234/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 235/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 236/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 237/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 238/300, seasonal_3 Loss: 0.0486 | 0.0222
Epoch 239/300, seasonal_3 Loss: 0.0485 | 0.0222
Epoch 240/300, seasonal_3 Loss: 0.0485 | 0.0222
Epoch 241/300, seasonal_3 Loss: 0.0485 | 0.0222
Epoch 242/300, seasonal_3 Loss: 0.0485 | 0.0222
Epoch 243/300, seasonal_3 Loss: 0.0485 | 0.0223
Epoch 244/300, seasonal_3 Loss: 0.0485 | 0.0223
Epoch 245/300, seasonal_3 Loss: 0.0485 | 0.0223
Epoch 246/300, seasonal_3 Loss: 0.0485 | 0.0223
Epoch 247/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 248/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 249/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 250/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 251/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 252/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 253/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 254/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 255/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 256/300, seasonal_3 Loss: 0.0484 | 0.0223
Epoch 257/300, seasonal_3 Loss: 0.0483 | 0.0223
Epoch 258/300, seasonal_3 Loss: 0.0483 | 0.0223
Epoch 259/300, seasonal_3 Loss: 0.0483 | 0.0223
Epoch 260/300, seasonal_3 Loss: 0.0483 | 0.0223
Epoch 261/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 262/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 263/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 264/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 265/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 266/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 267/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 268/300, seasonal_3 Loss: 0.0483 | 0.0224
Epoch 269/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 270/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 271/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 272/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 273/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 274/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 275/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 276/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 277/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 278/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 279/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 280/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 281/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 282/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 283/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 284/300, seasonal_3 Loss: 0.0482 | 0.0224
Epoch 285/300, seasonal_3 Loss: 0.0481 | 0.0224
Epoch 286/300, seasonal_3 Loss: 0.0481 | 0.0224
Epoch 287/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 288/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 289/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 290/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 291/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 292/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 293/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 294/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 295/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 296/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 297/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 298/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 299/300, seasonal_3 Loss: 0.0481 | 0.0225
Epoch 300/300, seasonal_3 Loss: 0.0481 | 0.0225
Training resid component with params: {'observation_period_num': 10, 'train_rates': 0.74811080809927, 'learning_rate': 0.0009207298141960786, 'batch_size': 35, 'step_size': 2, 'gamma': 0.8502553261924184}
Epoch 1/300, resid Loss: 0.2418 | 0.1451
Epoch 2/300, resid Loss: 0.1301 | 0.1617
Epoch 3/300, resid Loss: 0.1242 | 0.1293
Epoch 4/300, resid Loss: 0.1127 | 0.1073
Epoch 5/300, resid Loss: 0.1043 | 0.0975
Epoch 6/300, resid Loss: 0.0970 | 0.0934
Epoch 7/300, resid Loss: 0.0919 | 0.0902
Epoch 8/300, resid Loss: 0.0914 | 0.0908
Epoch 9/300, resid Loss: 0.0897 | 0.0894
Epoch 10/300, resid Loss: 0.0845 | 0.0853
Epoch 11/300, resid Loss: 0.0825 | 0.0841
Epoch 12/300, resid Loss: 0.0811 | 0.0826
Epoch 13/300, resid Loss: 0.0800 | 0.0815
Epoch 14/300, resid Loss: 0.0792 | 0.0806
Epoch 15/300, resid Loss: 0.0785 | 0.0797
Epoch 16/300, resid Loss: 0.0780 | 0.0793
Epoch 17/300, resid Loss: 0.0774 | 0.0786
Epoch 18/300, resid Loss: 0.0769 | 0.0783
Epoch 19/300, resid Loss: 0.0765 | 0.0775
Epoch 20/300, resid Loss: 0.0762 | 0.0775
Epoch 21/300, resid Loss: 0.0759 | 0.0768
Epoch 22/300, resid Loss: 0.0757 | 0.0768
Epoch 23/300, resid Loss: 0.0755 | 0.0764
Epoch 24/300, resid Loss: 0.0753 | 0.0763
Epoch 25/300, resid Loss: 0.0751 | 0.0762
Epoch 26/300, resid Loss: 0.0750 | 0.0760
Epoch 27/300, resid Loss: 0.0749 | 0.0758
Epoch 28/300, resid Loss: 0.0748 | 0.0757
Epoch 29/300, resid Loss: 0.0747 | 0.0756
Epoch 30/300, resid Loss: 0.0746 | 0.0756
Epoch 31/300, resid Loss: 0.0745 | 0.0755
Epoch 32/300, resid Loss: 0.0744 | 0.0755
Epoch 33/300, resid Loss: 0.0743 | 0.0755
Epoch 34/300, resid Loss: 0.0743 | 0.0755
Epoch 35/300, resid Loss: 0.0742 | 0.0755
Epoch 36/300, resid Loss: 0.0741 | 0.0754
Epoch 37/300, resid Loss: 0.0741 | 0.0754
Epoch 38/300, resid Loss: 0.0741 | 0.0754
Epoch 39/300, resid Loss: 0.0740 | 0.0754
Epoch 40/300, resid Loss: 0.0740 | 0.0754
Epoch 41/300, resid Loss: 0.0740 | 0.0754
Epoch 42/300, resid Loss: 0.0740 | 0.0754
Epoch 43/300, resid Loss: 0.0739 | 0.0754
Epoch 44/300, resid Loss: 0.0739 | 0.0754
Epoch 45/300, resid Loss: 0.0739 | 0.0754
Epoch 46/300, resid Loss: 0.0739 | 0.0754
Epoch 47/300, resid Loss: 0.0739 | 0.0754
Epoch 48/300, resid Loss: 0.0739 | 0.0754
Epoch 49/300, resid Loss: 0.0739 | 0.0754
Epoch 50/300, resid Loss: 0.0739 | 0.0754
Epoch 51/300, resid Loss: 0.0739 | 0.0754
Epoch 52/300, resid Loss: 0.0739 | 0.0754
Epoch 53/300, resid Loss: 0.0739 | 0.0754
Epoch 54/300, resid Loss: 0.0739 | 0.0754
Epoch 55/300, resid Loss: 0.0739 | 0.0754
Epoch 56/300, resid Loss: 0.0739 | 0.0754
Epoch 57/300, resid Loss: 0.0739 | 0.0754
Epoch 58/300, resid Loss: 0.0739 | 0.0754
Epoch 59/300, resid Loss: 0.0739 | 0.0754
Epoch 60/300, resid Loss: 0.0739 | 0.0754
Epoch 61/300, resid Loss: 0.0739 | 0.0754
Epoch 62/300, resid Loss: 0.0739 | 0.0754
Epoch 63/300, resid Loss: 0.0739 | 0.0754
Epoch 64/300, resid Loss: 0.0739 | 0.0754
Epoch 65/300, resid Loss: 0.0739 | 0.0754
Epoch 66/300, resid Loss: 0.0739 | 0.0754
Epoch 67/300, resid Loss: 0.0739 | 0.0754
Epoch 68/300, resid Loss: 0.0739 | 0.0754
Epoch 69/300, resid Loss: 0.0739 | 0.0754
Epoch 70/300, resid Loss: 0.0739 | 0.0754
Epoch 71/300, resid Loss: 0.0739 | 0.0754
Epoch 72/300, resid Loss: 0.0739 | 0.0754
Epoch 73/300, resid Loss: 0.0739 | 0.0754
Epoch 74/300, resid Loss: 0.0739 | 0.0754
Epoch 75/300, resid Loss: 0.0739 | 0.0754
Epoch 76/300, resid Loss: 0.0739 | 0.0754
Epoch 77/300, resid Loss: 0.0739 | 0.0754
Epoch 78/300, resid Loss: 0.0739 | 0.0754
Epoch 79/300, resid Loss: 0.0739 | 0.0754
Epoch 80/300, resid Loss: 0.0739 | 0.0754
Epoch 81/300, resid Loss: 0.0739 | 0.0754
Epoch 82/300, resid Loss: 0.0739 | 0.0754
Epoch 83/300, resid Loss: 0.0739 | 0.0754
Epoch 84/300, resid Loss: 0.0739 | 0.0754
Epoch 85/300, resid Loss: 0.0739 | 0.0754
Epoch 86/300, resid Loss: 0.0739 | 0.0754
Epoch 87/300, resid Loss: 0.0739 | 0.0754
Epoch 88/300, resid Loss: 0.0739 | 0.0754
Epoch 89/300, resid Loss: 0.0739 | 0.0754
Epoch 90/300, resid Loss: 0.0739 | 0.0754
Epoch 91/300, resid Loss: 0.0739 | 0.0754
Epoch 92/300, resid Loss: 0.0739 | 0.0754
Epoch 93/300, resid Loss: 0.0739 | 0.0754
Epoch 94/300, resid Loss: 0.0739 | 0.0754
Epoch 95/300, resid Loss: 0.0739 | 0.0754
Epoch 96/300, resid Loss: 0.0739 | 0.0754
Epoch 97/300, resid Loss: 0.0739 | 0.0754
Epoch 98/300, resid Loss: 0.0739 | 0.0754
Epoch 99/300, resid Loss: 0.0739 | 0.0754
Epoch 100/300, resid Loss: 0.0739 | 0.0754
Epoch 101/300, resid Loss: 0.0739 | 0.0754
Epoch 102/300, resid Loss: 0.0739 | 0.0754
Epoch 103/300, resid Loss: 0.0739 | 0.0754
Epoch 104/300, resid Loss: 0.0739 | 0.0754
Epoch 105/300, resid Loss: 0.0739 | 0.0754
Epoch 106/300, resid Loss: 0.0739 | 0.0754
Epoch 107/300, resid Loss: 0.0739 | 0.0754
Epoch 108/300, resid Loss: 0.0739 | 0.0754
Epoch 109/300, resid Loss: 0.0739 | 0.0754
Epoch 110/300, resid Loss: 0.0739 | 0.0754
Epoch 111/300, resid Loss: 0.0739 | 0.0754
Epoch 112/300, resid Loss: 0.0739 | 0.0754
Epoch 113/300, resid Loss: 0.0739 | 0.0754
Epoch 114/300, resid Loss: 0.0739 | 0.0754
Epoch 115/300, resid Loss: 0.0739 | 0.0754
Epoch 116/300, resid Loss: 0.0739 | 0.0754
Epoch 117/300, resid Loss: 0.0739 | 0.0754
Epoch 118/300, resid Loss: 0.0739 | 0.0754
Epoch 119/300, resid Loss: 0.0739 | 0.0754
Epoch 120/300, resid Loss: 0.0739 | 0.0754
Epoch 121/300, resid Loss: 0.0739 | 0.0754
Epoch 122/300, resid Loss: 0.0739 | 0.0754
Epoch 123/300, resid Loss: 0.0739 | 0.0754
Epoch 124/300, resid Loss: 0.0739 | 0.0754
Epoch 125/300, resid Loss: 0.0739 | 0.0754
Epoch 126/300, resid Loss: 0.0739 | 0.0754
Epoch 127/300, resid Loss: 0.0739 | 0.0754
Epoch 128/300, resid Loss: 0.0739 | 0.0754
Early stopping for resid
Runtime (seconds): 1398.7977130413055
5.776884749770517e-05
[99.86273]
[5.5489063]
[-1.8369464]
[0.46002385]
[-4.511174]
[-3.291368]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 31.447715058515314
RMSE: 5.607826232910156
MAE: 5.607826232910156
R-squared: nan
[96.23217]
