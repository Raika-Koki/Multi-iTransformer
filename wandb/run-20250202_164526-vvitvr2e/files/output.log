ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-02 16:45:28,818][0m A new study created in memory with name: no-name-95525a20-aba9-484d-9c90-97f0a4df2cb7[0m
[32m[I 2025-02-02 16:49:01,254][0m Trial 0 finished with value: 1.1952965997702238 and parameters: {'observation_period_num': 228, 'train_rates': 0.6117358271736395, 'learning_rate': 0.0006477896157887528, 'batch_size': 249, 'step_size': 12, 'gamma': 0.8702263506684635}. Best is trial 0 with value: 1.1952965997702238.[0m
[32m[I 2025-02-02 16:50:37,210][0m Trial 1 finished with value: 1.2102453482624953 and parameters: {'observation_period_num': 105, 'train_rates': 0.770339341019975, 'learning_rate': 7.2889745446121685e-06, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8377667861669066}. Best is trial 0 with value: 1.1952965997702238.[0m
[32m[I 2025-02-02 16:55:00,340][0m Trial 2 finished with value: 1.6964779428515975 and parameters: {'observation_period_num': 239, 'train_rates': 0.730542398212168, 'learning_rate': 1.9336931738471493e-06, 'batch_size': 127, 'step_size': 6, 'gamma': 0.863405312797536}. Best is trial 0 with value: 1.1952965997702238.[0m
[32m[I 2025-02-02 16:56:52,770][0m Trial 3 finished with value: 0.7596709310160994 and parameters: {'observation_period_num': 101, 'train_rates': 0.9262087796549345, 'learning_rate': 0.0009811856106126389, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8107438305395943}. Best is trial 3 with value: 0.7596709310160994.[0m
[32m[I 2025-02-02 16:59:12,529][0m Trial 4 finished with value: 0.9850383546930799 and parameters: {'observation_period_num': 149, 'train_rates': 0.6983488515806844, 'learning_rate': 2.9945111587726244e-05, 'batch_size': 229, 'step_size': 1, 'gamma': 0.9794434108529521}. Best is trial 3 with value: 0.7596709310160994.[0m
[32m[I 2025-02-02 16:59:52,912][0m Trial 5 finished with value: 0.8374571498536966 and parameters: {'observation_period_num': 11, 'train_rates': 0.6687558784405103, 'learning_rate': 2.2377141357591278e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9797743356776931}. Best is trial 3 with value: 0.7596709310160994.[0m
[32m[I 2025-02-02 17:03:12,241][0m Trial 6 finished with value: 0.5696224222580591 and parameters: {'observation_period_num': 192, 'train_rates': 0.7967583818197961, 'learning_rate': 0.00021635507465470947, 'batch_size': 119, 'step_size': 6, 'gamma': 0.917014231357434}. Best is trial 6 with value: 0.5696224222580591.[0m
[32m[I 2025-02-02 17:05:17,279][0m Trial 7 finished with value: 1.5373126266986026 and parameters: {'observation_period_num': 129, 'train_rates': 0.6984638942107292, 'learning_rate': 3.0105541666872787e-06, 'batch_size': 156, 'step_size': 4, 'gamma': 0.8523012461897322}. Best is trial 6 with value: 0.5696224222580591.[0m
[32m[I 2025-02-02 17:10:22,148][0m Trial 8 finished with value: 0.16368615627288818 and parameters: {'observation_period_num': 234, 'train_rates': 0.9769133179632867, 'learning_rate': 0.0004187787742161568, 'batch_size': 176, 'step_size': 15, 'gamma': 0.8767307072502162}. Best is trial 8 with value: 0.16368615627288818.[0m
[32m[I 2025-02-02 17:14:54,958][0m Trial 9 finished with value: 0.7512575906144986 and parameters: {'observation_period_num': 231, 'train_rates': 0.8282589536679812, 'learning_rate': 0.0002128326326277052, 'batch_size': 246, 'step_size': 1, 'gamma': 0.9110590770061822}. Best is trial 8 with value: 0.16368615627288818.[0m
[32m[I 2025-02-02 17:19:42,886][0m Trial 10 finished with value: 0.10735239524666856 and parameters: {'observation_period_num': 35, 'train_rates': 0.985963944123243, 'learning_rate': 0.00011887663373591572, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7516318067191714}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:24:08,711][0m Trial 11 finished with value: 0.12964182011783124 and parameters: {'observation_period_num': 24, 'train_rates': 0.9865778593799979, 'learning_rate': 0.00013786229716759036, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7765747846171183}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:28:24,086][0m Trial 12 finished with value: 0.23510093034969437 and parameters: {'observation_period_num': 11, 'train_rates': 0.8961844758185383, 'learning_rate': 7.108149470837146e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7501364174937641}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:32:09,803][0m Trial 13 finished with value: 0.117121603577695 and parameters: {'observation_period_num': 61, 'train_rates': 0.9837878006671686, 'learning_rate': 7.951560579500094e-05, 'batch_size': 21, 'step_size': 12, 'gamma': 0.7539242180845837}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:33:36,006][0m Trial 14 finished with value: 0.26736319655334795 and parameters: {'observation_period_num': 57, 'train_rates': 0.8935657178155049, 'learning_rate': 6.931063838987463e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.7954886908907364}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:35:00,937][0m Trial 15 finished with value: 0.34861913389263893 and parameters: {'observation_period_num': 60, 'train_rates': 0.9375824385238698, 'learning_rate': 1.2977280285701383e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.7501834452775301}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:36:19,725][0m Trial 16 finished with value: 0.3656097560197906 and parameters: {'observation_period_num': 56, 'train_rates': 0.8440693876893286, 'learning_rate': 6.852006838875287e-05, 'batch_size': 54, 'step_size': 10, 'gamma': 0.7821199910238549}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:37:53,020][0m Trial 17 finished with value: 0.1666444816045313 and parameters: {'observation_period_num': 83, 'train_rates': 0.9480870742159762, 'learning_rate': 0.00011351798959110489, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8226726124347595}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:39:45,789][0m Trial 18 finished with value: 0.5571342083994902 and parameters: {'observation_period_num': 38, 'train_rates': 0.8771054565009306, 'learning_rate': 7.036007029295764e-06, 'batch_size': 38, 'step_size': 10, 'gamma': 0.7700266710392943}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:42:47,027][0m Trial 19 finished with value: 0.17362536489963531 and parameters: {'observation_period_num': 160, 'train_rates': 0.9443319860559489, 'learning_rate': 0.00035823998585124184, 'batch_size': 201, 'step_size': 14, 'gamma': 0.8051565595353845}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:44:25,069][0m Trial 20 finished with value: 0.40772520380031657 and parameters: {'observation_period_num': 94, 'train_rates': 0.8550936730982978, 'learning_rate': 4.720227873451396e-05, 'batch_size': 74, 'step_size': 10, 'gamma': 0.9431708871020552}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:47:00,051][0m Trial 21 finished with value: 0.12064807237805547 and parameters: {'observation_period_num': 29, 'train_rates': 0.9876087165609746, 'learning_rate': 0.00016791810294218044, 'batch_size': 31, 'step_size': 15, 'gamma': 0.7734820225352582}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:49:22,990][0m Trial 22 finished with value: 0.11209811758799632 and parameters: {'observation_period_num': 32, 'train_rates': 0.9794121107297314, 'learning_rate': 0.00018353956779296337, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7674105622871298}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:51:22,444][0m Trial 23 finished with value: 0.28970899335185035 and parameters: {'observation_period_num': 74, 'train_rates': 0.9154821102369322, 'learning_rate': 0.0003054895226540004, 'batch_size': 38, 'step_size': 13, 'gamma': 0.7542903698176304}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:52:32,207][0m Trial 24 finished with value: 0.1513280658810227 and parameters: {'observation_period_num': 46, 'train_rates': 0.9630796588383224, 'learning_rate': 0.00010736182839304881, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7922885591734552}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:55:31,979][0m Trial 25 finished with value: 0.20563166129092375 and parameters: {'observation_period_num': 122, 'train_rates': 0.9578775875301112, 'learning_rate': 1.7307751048774007e-05, 'batch_size': 26, 'step_size': 13, 'gamma': 0.8242422644724003}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:56:53,775][0m Trial 26 finished with value: 0.31545927297127874 and parameters: {'observation_period_num': 73, 'train_rates': 0.9077211758756807, 'learning_rate': 3.701199299534692e-05, 'batch_size': 105, 'step_size': 14, 'gamma': 0.7651853030571997}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:58:08,779][0m Trial 27 finished with value: 0.28924527345593815 and parameters: {'observation_period_num': 7, 'train_rates': 0.8713014417225329, 'learning_rate': 5.193598632653789e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.790121366113906}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 17:58:48,168][0m Trial 28 finished with value: 0.2301314356473257 and parameters: {'observation_period_num': 33, 'train_rates': 0.9274269555459204, 'learning_rate': 0.00010695623074287855, 'batch_size': 146, 'step_size': 11, 'gamma': 0.765059302495406}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:02:48,085][0m Trial 29 finished with value: 1.39472496509552 and parameters: {'observation_period_num': 185, 'train_rates': 0.9863348310142059, 'learning_rate': 0.0008443508571685758, 'batch_size': 40, 'step_size': 11, 'gamma': 0.808969074359773}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:05:51,049][0m Trial 30 finished with value: 1.250516679753428 and parameters: {'observation_period_num': 46, 'train_rates': 0.6436162825017717, 'learning_rate': 0.0004032256847495547, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8843785545748722}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:08:10,679][0m Trial 31 finished with value: 0.14013521804621346 and parameters: {'observation_period_num': 24, 'train_rates': 0.9872874666937144, 'learning_rate': 0.00019620575872626058, 'batch_size': 34, 'step_size': 15, 'gamma': 0.7721332715066617}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:10:00,689][0m Trial 32 finished with value: 0.15087349938623834 and parameters: {'observation_period_num': 26, 'train_rates': 0.9662421148561007, 'learning_rate': 0.00015821689451931163, 'batch_size': 43, 'step_size': 15, 'gamma': 0.7848889591494439}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:11:13,165][0m Trial 33 finished with value: 0.5403838738014823 and parameters: {'observation_period_num': 64, 'train_rates': 0.9473614733413558, 'learning_rate': 0.0006298650400780034, 'batch_size': 71, 'step_size': 12, 'gamma': 0.7606198010763646}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:13:12,199][0m Trial 34 finished with value: 0.5878596617322829 and parameters: {'observation_period_num': 42, 'train_rates': 0.801946072771919, 'learning_rate': 8.396876460052202e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8372409169833254}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:14:35,987][0m Trial 35 finished with value: 0.7802579801390142 and parameters: {'observation_period_num': 84, 'train_rates': 0.7485955107979454, 'learning_rate': 0.00022961799304400298, 'batch_size': 60, 'step_size': 12, 'gamma': 0.7776098389168292}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:15:35,567][0m Trial 36 finished with value: 0.3292947312635658 and parameters: {'observation_period_num': 17, 'train_rates': 0.9231197460301106, 'learning_rate': 0.0005820397580105013, 'batch_size': 82, 'step_size': 8, 'gamma': 0.8012034235407484}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:18:25,928][0m Trial 37 finished with value: 0.15038445466443112 and parameters: {'observation_period_num': 121, 'train_rates': 0.9664386012439383, 'learning_rate': 3.0296379928930238e-05, 'batch_size': 28, 'step_size': 13, 'gamma': 0.8180819831732213}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:20:23,052][0m Trial 38 finished with value: 0.16476576030254364 and parameters: {'observation_period_num': 106, 'train_rates': 0.96362020127491, 'learning_rate': 0.0001455564720604291, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8375677957589334}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:21:03,712][0m Trial 39 finished with value: 0.2250800905551048 and parameters: {'observation_period_num': 34, 'train_rates': 0.9358249110939649, 'learning_rate': 0.0002712487163004837, 'batch_size': 125, 'step_size': 15, 'gamma': 0.763181359828985}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:22:03,042][0m Trial 40 finished with value: 0.3206643760204315 and parameters: {'observation_period_num': 47, 'train_rates': 0.9879329585207095, 'learning_rate': 4.8929169692959864e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.7844603317790861}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:26:24,155][0m Trial 41 finished with value: 0.11014300818954195 and parameters: {'observation_period_num': 18, 'train_rates': 0.988358584520929, 'learning_rate': 0.0001431120698354618, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7732909717198972}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:30:56,684][0m Trial 42 finished with value: 0.11787763941619131 and parameters: {'observation_period_num': 21, 'train_rates': 0.969407373068516, 'learning_rate': 0.00016437474909922198, 'batch_size': 17, 'step_size': 14, 'gamma': 0.7610238961230897}. Best is trial 10 with value: 0.10735239524666856.[0m
[32m[I 2025-02-02 18:35:50,418][0m Trial 43 finished with value: 0.09007120731918292 and parameters: {'observation_period_num': 6, 'train_rates': 0.9705950714146946, 'learning_rate': 9.011730237716253e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7510806385844074}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:37:25,588][0m Trial 44 finished with value: 0.19935190509936027 and parameters: {'observation_period_num': 5, 'train_rates': 0.90824045756921, 'learning_rate': 8.56115947058736e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.7529831642685502}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:40:38,085][0m Trial 45 finished with value: 0.43710945452315897 and parameters: {'observation_period_num': 15, 'train_rates': 0.9463522805980713, 'learning_rate': 1.761132410487324e-06, 'batch_size': 24, 'step_size': 13, 'gamma': 0.9649854530957446}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:43:57,736][0m Trial 46 finished with value: 1.0489537526166974 and parameters: {'observation_period_num': 52, 'train_rates': 0.6013788026575237, 'learning_rate': 2.4603244544252427e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8575866982460167}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:44:27,031][0m Trial 47 finished with value: 0.19573748111724854 and parameters: {'observation_period_num': 6, 'train_rates': 0.9706292734551397, 'learning_rate': 5.7894280564382983e-05, 'batch_size': 179, 'step_size': 15, 'gamma': 0.7936097626888249}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:45:46,811][0m Trial 48 finished with value: 1.3651481469472249 and parameters: {'observation_period_num': 68, 'train_rates': 0.8878567321755687, 'learning_rate': 1.1049330285926765e-06, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7505661371531849}. Best is trial 43 with value: 0.09007120731918292.[0m
[32m[I 2025-02-02 18:50:20,076][0m Trial 49 finished with value: 0.8690325764915611 and parameters: {'observation_period_num': 249, 'train_rates': 0.7078772429543305, 'learning_rate': 0.00011418146884568916, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8998477246224601}. Best is trial 43 with value: 0.09007120731918292.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-02 18:50:20,083][0m A new study created in memory with name: no-name-67aaa18b-74cf-4bb8-864c-43a259cf7eb2[0m
[32m[I 2025-02-02 18:52:10,628][0m Trial 0 finished with value: 0.8809283370933225 and parameters: {'observation_period_num': 114, 'train_rates': 0.6505164791140465, 'learning_rate': 1.5491928121624275e-05, 'batch_size': 37, 'step_size': 15, 'gamma': 0.9594764548747549}. Best is trial 0 with value: 0.8809283370933225.[0m
[32m[I 2025-02-02 18:56:35,373][0m Trial 1 finished with value: 1.1844811366088148 and parameters: {'observation_period_num': 251, 'train_rates': 0.693582278808148, 'learning_rate': 0.0009352942232738455, 'batch_size': 74, 'step_size': 3, 'gamma': 0.9376678988263358}. Best is trial 0 with value: 0.8809283370933225.[0m
[32m[I 2025-02-02 19:00:27,297][0m Trial 2 finished with value: 0.7216845758703371 and parameters: {'observation_period_num': 209, 'train_rates': 0.7799029761052856, 'learning_rate': 0.0004342087169107401, 'batch_size': 176, 'step_size': 10, 'gamma': 0.9089493497511109}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:01:51,203][0m Trial 3 finished with value: 1.3053467572023885 and parameters: {'observation_period_num': 104, 'train_rates': 0.6371913053987368, 'learning_rate': 1.7901496317105487e-05, 'batch_size': 235, 'step_size': 12, 'gamma': 0.8326192482083263}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:02:45,695][0m Trial 4 finished with value: 1.2272670269012451 and parameters: {'observation_period_num': 51, 'train_rates': 0.9497268696861398, 'learning_rate': 2.499767918070134e-06, 'batch_size': 186, 'step_size': 2, 'gamma': 0.8781305573176188}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:04:22,922][0m Trial 5 finished with value: 1.8760943220745971 and parameters: {'observation_period_num': 107, 'train_rates': 0.7162275939294576, 'learning_rate': 7.975464784584022e-06, 'batch_size': 229, 'step_size': 3, 'gamma': 0.7553274247072971}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:07:27,198][0m Trial 6 finished with value: 1.4920181365796399 and parameters: {'observation_period_num': 166, 'train_rates': 0.8754664441697864, 'learning_rate': 1.351208714708741e-06, 'batch_size': 105, 'step_size': 14, 'gamma': 0.7708862655075903}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:08:18,765][0m Trial 7 finished with value: 0.7579308917018819 and parameters: {'observation_period_num': 20, 'train_rates': 0.6809933308006743, 'learning_rate': 0.00022772771896570058, 'batch_size': 73, 'step_size': 5, 'gamma': 0.8439007375718468}. Best is trial 2 with value: 0.7216845758703371.[0m
[32m[I 2025-02-02 19:10:19,407][0m Trial 8 finished with value: 0.1601727455854416 and parameters: {'observation_period_num': 109, 'train_rates': 0.9805213828798066, 'learning_rate': 0.00023811080355126978, 'batch_size': 216, 'step_size': 5, 'gamma': 0.9869877100503879}. Best is trial 8 with value: 0.1601727455854416.[0m
[32m[I 2025-02-02 19:12:35,142][0m Trial 9 finished with value: 0.15454502403736115 and parameters: {'observation_period_num': 126, 'train_rates': 0.9638999442861529, 'learning_rate': 0.00029440171894885007, 'batch_size': 198, 'step_size': 14, 'gamma': 0.8867167608725388}. Best is trial 9 with value: 0.15454502403736115.[0m
[32m[I 2025-02-02 19:15:47,633][0m Trial 10 finished with value: 0.3543283964196841 and parameters: {'observation_period_num': 172, 'train_rates': 0.8922342901649907, 'learning_rate': 7.991150343437871e-05, 'batch_size': 145, 'step_size': 9, 'gamma': 0.8097913432208309}. Best is trial 9 with value: 0.15454502403736115.[0m
[32m[I 2025-02-02 19:17:01,020][0m Trial 11 finished with value: 0.1511862874031067 and parameters: {'observation_period_num': 68, 'train_rates': 0.98046139218329, 'learning_rate': 8.685330814922627e-05, 'batch_size': 195, 'step_size': 6, 'gamma': 0.989899405259335}. Best is trial 11 with value: 0.1511862874031067.[0m
[32m[I 2025-02-02 19:17:41,884][0m Trial 12 finished with value: 0.3380193973796955 and parameters: {'observation_period_num': 40, 'train_rates': 0.9054862110848759, 'learning_rate': 6.949846395012737e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.9179433243418066}. Best is trial 11 with value: 0.1511862874031067.[0m
[32m[I 2025-02-02 19:18:49,252][0m Trial 13 finished with value: 0.5394820133845012 and parameters: {'observation_period_num': 70, 'train_rates': 0.8336436569160202, 'learning_rate': 7.017608980867788e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9872281469585705}. Best is trial 11 with value: 0.1511862874031067.[0m
[32m[I 2025-02-02 19:21:57,267][0m Trial 14 finished with value: 0.1451328843832016 and parameters: {'observation_period_num': 153, 'train_rates': 0.9763975413326856, 'learning_rate': 0.0001897003113783757, 'batch_size': 138, 'step_size': 7, 'gamma': 0.8831381827193499}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:24:37,981][0m Trial 15 finished with value: 0.7439854977961861 and parameters: {'observation_period_num': 160, 'train_rates': 0.7800876162430515, 'learning_rate': 9.9145072356676e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.854414421240134}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:26:07,410][0m Trial 16 finished with value: 0.457858130412224 and parameters: {'observation_period_num': 84, 'train_rates': 0.9322389136042398, 'learning_rate': 3.806546833930065e-05, 'batch_size': 112, 'step_size': 5, 'gamma': 0.7979388721170603}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:28:48,199][0m Trial 17 finished with value: 0.8378283518728952 and parameters: {'observation_period_num': 149, 'train_rates': 0.8552126679454439, 'learning_rate': 0.0009474805944630683, 'batch_size': 157, 'step_size': 8, 'gamma': 0.954875401309448}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:33:10,259][0m Trial 18 finished with value: 0.3242219388484955 and parameters: {'observation_period_num': 205, 'train_rates': 0.9877008139315336, 'learning_rate': 0.00013072796403416182, 'batch_size': 107, 'step_size': 1, 'gamma': 0.9055636874640828}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:34:28,979][0m Trial 19 finished with value: 0.448869108888838 and parameters: {'observation_period_num': 75, 'train_rates': 0.9219856694565484, 'learning_rate': 3.766742050169407e-05, 'batch_size': 206, 'step_size': 6, 'gamma': 0.9321229188433175}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:34:56,041][0m Trial 20 finished with value: 0.8696640002345699 and parameters: {'observation_period_num': 9, 'train_rates': 0.8091716897414669, 'learning_rate': 5.353163881642369e-06, 'batch_size': 160, 'step_size': 10, 'gamma': 0.9636753996798676}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:37:31,076][0m Trial 21 finished with value: 0.1582174301147461 and parameters: {'observation_period_num': 135, 'train_rates': 0.9610691890725749, 'learning_rate': 0.00033735182294166675, 'batch_size': 191, 'step_size': 12, 'gamma': 0.8823931306461812}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:41:15,822][0m Trial 22 finished with value: 0.1842588633298874 and parameters: {'observation_period_num': 191, 'train_rates': 0.9412901459921523, 'learning_rate': 0.0001585172909956138, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8890094249151719}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:44:05,720][0m Trial 23 finished with value: 0.1772117167711258 and parameters: {'observation_period_num': 137, 'train_rates': 0.9867471214516678, 'learning_rate': 0.0005147738377085538, 'batch_size': 118, 'step_size': 6, 'gamma': 0.8691177978233328}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:45:36,090][0m Trial 24 finished with value: 0.3955035866016433 and parameters: {'observation_period_num': 88, 'train_rates': 0.9119513883301206, 'learning_rate': 0.00018752533190784465, 'batch_size': 165, 'step_size': 4, 'gamma': 0.808589827820035}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:46:29,828][0m Trial 25 finished with value: 0.15909035503864288 and parameters: {'observation_period_num': 53, 'train_rates': 0.959904501440282, 'learning_rate': 0.000443462955577212, 'batch_size': 247, 'step_size': 8, 'gamma': 0.8589931555770837}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:48:36,197][0m Trial 26 finished with value: 0.48353760858267053 and parameters: {'observation_period_num': 125, 'train_rates': 0.8722015099241497, 'learning_rate': 3.28827308449937e-05, 'batch_size': 219, 'step_size': 13, 'gamma': 0.830458636152135}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:52:31,371][0m Trial 27 finished with value: 0.1971099896997702 and parameters: {'observation_period_num': 191, 'train_rates': 0.9558540865234576, 'learning_rate': 5.399697969020417e-05, 'batch_size': 81, 'step_size': 10, 'gamma': 0.8989624892364352}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:56:48,238][0m Trial 28 finished with value: 0.509278967837307 and parameters: {'observation_period_num': 224, 'train_rates': 0.8433396924757447, 'learning_rate': 1.874443685869146e-05, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9329858229704395}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 19:59:04,675][0m Trial 29 finished with value: 0.3064943597280125 and parameters: {'observation_period_num': 118, 'train_rates': 0.8967514344337127, 'learning_rate': 0.00011744916386751594, 'batch_size': 41, 'step_size': 14, 'gamma': 0.9691505286927539}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:02:02,017][0m Trial 30 finished with value: 0.7910424753673179 and parameters: {'observation_period_num': 179, 'train_rates': 0.7391708914073702, 'learning_rate': 0.0002961008943678862, 'batch_size': 203, 'step_size': 15, 'gamma': 0.9236173775669096}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:04:43,207][0m Trial 31 finished with value: 0.2218569815158844 and parameters: {'observation_period_num': 141, 'train_rates': 0.952385678867454, 'learning_rate': 0.0003483399707535719, 'batch_size': 187, 'step_size': 15, 'gamma': 0.8858443577565251}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:07:46,073][0m Trial 32 finished with value: 0.19729574024677277 and parameters: {'observation_period_num': 156, 'train_rates': 0.9686853566068669, 'learning_rate': 0.0005552793633186503, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9484041370111432}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:10:17,599][0m Trial 33 finished with value: 0.2974223922572884 and parameters: {'observation_period_num': 131, 'train_rates': 0.9278111292898428, 'learning_rate': 0.0006280133383975776, 'batch_size': 181, 'step_size': 7, 'gamma': 0.8925641517781261}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:12:02,671][0m Trial 34 finished with value: 0.18502351641654968 and parameters: {'observation_period_num': 97, 'train_rates': 0.988263695139009, 'learning_rate': 0.00016538701595290384, 'batch_size': 214, 'step_size': 12, 'gamma': 0.8695223385711917}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:17:33,669][0m Trial 35 finished with value: 0.1696213036775589 and parameters: {'observation_period_num': 252, 'train_rates': 0.9638180186185759, 'learning_rate': 0.0002795800516414509, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9086215598157958}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:22:18,062][0m Trial 36 finished with value: 0.22226964640948507 and parameters: {'observation_period_num': 227, 'train_rates': 0.9340339750837555, 'learning_rate': 0.0003542214326335792, 'batch_size': 169, 'step_size': 14, 'gamma': 0.8314740084623262}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:24:54,670][0m Trial 37 finished with value: 0.36758172963604785 and parameters: {'observation_period_num': 144, 'train_rates': 0.8827129236032066, 'learning_rate': 0.000634872545134657, 'batch_size': 190, 'step_size': 4, 'gamma': 0.8567009565381027}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:26:33,051][0m Trial 38 finished with value: 0.9962936051830447 and parameters: {'observation_period_num': 120, 'train_rates': 0.6097013815407093, 'learning_rate': 5.1446557296290606e-05, 'batch_size': 88, 'step_size': 9, 'gamma': 0.8759679682070177}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:27:23,546][0m Trial 39 finished with value: 0.8228095886107333 and parameters: {'observation_period_num': 59, 'train_rates': 0.7378064127922928, 'learning_rate': 0.00010167832553392303, 'batch_size': 238, 'step_size': 13, 'gamma': 0.8426814029058407}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:28:01,309][0m Trial 40 finished with value: 0.32034216015255407 and parameters: {'observation_period_num': 29, 'train_rates': 0.9122970045836788, 'learning_rate': 0.00019246955297545128, 'batch_size': 131, 'step_size': 6, 'gamma': 0.7754315099003202}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:28:53,572][0m Trial 41 finished with value: 0.3796844780445099 and parameters: {'observation_period_num': 49, 'train_rates': 0.9562305912635554, 'learning_rate': 0.0007917350629368418, 'batch_size': 240, 'step_size': 8, 'gamma': 0.8603320036578963}. Best is trial 14 with value: 0.1451328843832016.[0m
[32m[I 2025-02-02 20:30:33,858][0m Trial 42 finished with value: 0.14003179967403412 and parameters: {'observation_period_num': 94, 'train_rates': 0.9739503871172471, 'learning_rate': 0.00042423777436945555, 'batch_size': 204, 'step_size': 7, 'gamma': 0.88043291394095}. Best is trial 42 with value: 0.14003179967403412.[0m
[32m[I 2025-02-02 20:32:25,730][0m Trial 43 finished with value: 0.13184893131256104 and parameters: {'observation_period_num': 101, 'train_rates': 0.9760620940225511, 'learning_rate': 0.00039126491326857426, 'batch_size': 203, 'step_size': 7, 'gamma': 0.9192531887938383}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:34:04,807][0m Trial 44 finished with value: 0.15022912621498108 and parameters: {'observation_period_num': 92, 'train_rates': 0.9761938787624839, 'learning_rate': 0.00026184930859071335, 'batch_size': 221, 'step_size': 6, 'gamma': 0.9423100804144099}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:36:46,905][0m Trial 45 finished with value: 0.15881494787477313 and parameters: {'observation_period_num': 94, 'train_rates': 0.9780953098284527, 'learning_rate': 0.00022216637201069958, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9760034444171716}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:38:37,746][0m Trial 46 finished with value: 0.4001481235027313 and parameters: {'observation_period_num': 106, 'train_rates': 0.9431059824865035, 'learning_rate': 2.2735312069131738e-05, 'batch_size': 220, 'step_size': 4, 'gamma': 0.9462825581853332}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:39:57,839][0m Trial 47 finished with value: 0.14049498736858368 and parameters: {'observation_period_num': 71, 'train_rates': 0.9751877825036644, 'learning_rate': 0.0004574394162163203, 'batch_size': 209, 'step_size': 5, 'gamma': 0.9200851324971588}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:41:20,695][0m Trial 48 finished with value: 0.27581035483040306 and parameters: {'observation_period_num': 77, 'train_rates': 0.9207851909606072, 'learning_rate': 0.0007438489588889871, 'batch_size': 227, 'step_size': 3, 'gamma': 0.9194559270165665}. Best is trial 43 with value: 0.13184893131256104.[0m
[32m[I 2025-02-02 20:42:27,392][0m Trial 49 finished with value: 0.14176274836063385 and parameters: {'observation_period_num': 63, 'train_rates': 0.9743182376049773, 'learning_rate': 0.0004615440792140932, 'batch_size': 207, 'step_size': 5, 'gamma': 0.9391707774243473}. Best is trial 43 with value: 0.13184893131256104.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-02 20:42:27,399][0m A new study created in memory with name: no-name-ad82b790-6f66-41e5-b4b2-235b0ffdd0f8[0m
[32m[I 2025-02-02 20:43:23,941][0m Trial 0 finished with value: 1.0215554276331789 and parameters: {'observation_period_num': 56, 'train_rates': 0.6170333981199904, 'learning_rate': 0.0001598391685478695, 'batch_size': 63, 'step_size': 2, 'gamma': 0.774995313512773}. Best is trial 0 with value: 1.0215554276331789.[0m
[32m[I 2025-02-02 20:44:23,653][0m Trial 1 finished with value: 1.3650646155880344 and parameters: {'observation_period_num': 41, 'train_rates': 0.9467691577001032, 'learning_rate': 1.1002897946339016e-06, 'batch_size': 79, 'step_size': 2, 'gamma': 0.91119210693269}. Best is trial 0 with value: 1.0215554276331789.[0m
[32m[I 2025-02-02 20:46:08,367][0m Trial 2 finished with value: 0.5698201060295105 and parameters: {'observation_period_num': 98, 'train_rates': 0.9697095526142142, 'learning_rate': 8.249028212679151e-06, 'batch_size': 235, 'step_size': 14, 'gamma': 0.8867863057451906}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 20:47:44,737][0m Trial 3 finished with value: 1.0733905698942101 and parameters: {'observation_period_num': 102, 'train_rates': 0.6366818247197016, 'learning_rate': 0.00011211535392819194, 'batch_size': 42, 'step_size': 15, 'gamma': 0.9125582123419111}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 20:49:26,452][0m Trial 4 finished with value: 0.803338885307312 and parameters: {'observation_period_num': 92, 'train_rates': 0.9872556726579147, 'learning_rate': 2.7929312383080934e-06, 'batch_size': 115, 'step_size': 8, 'gamma': 0.8873075178701986}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 20:53:50,475][0m Trial 5 finished with value: 0.7004604348451784 and parameters: {'observation_period_num': 214, 'train_rates': 0.8154556610025834, 'learning_rate': 1.1762798054607906e-05, 'batch_size': 25, 'step_size': 2, 'gamma': 0.9381409442299855}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 20:54:35,535][0m Trial 6 finished with value: 0.6106876835778907 and parameters: {'observation_period_num': 12, 'train_rates': 0.7613715807744328, 'learning_rate': 3.647625483849922e-05, 'batch_size': 91, 'step_size': 10, 'gamma': 0.9272514450316086}. Best is trial 2 with value: 0.5698201060295105.[0m
Early stopping at epoch 70
[32m[I 2025-02-02 20:57:40,991][0m Trial 7 finished with value: 1.320706095130677 and parameters: {'observation_period_num': 226, 'train_rates': 0.8390095356862688, 'learning_rate': 2.372650551254422e-06, 'batch_size': 221, 'step_size': 1, 'gamma': 0.8817120391472613}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 21:01:03,826][0m Trial 8 finished with value: 1.6248462572792508 and parameters: {'observation_period_num': 198, 'train_rates': 0.7260128730246056, 'learning_rate': 3.410959270960128e-06, 'batch_size': 116, 'step_size': 7, 'gamma': 0.8324697134235951}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 21:03:19,338][0m Trial 9 finished with value: 1.5372588283645245 and parameters: {'observation_period_num': 144, 'train_rates': 0.6744653188862103, 'learning_rate': 1.767317774056645e-06, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8546215418648204}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 21:06:13,455][0m Trial 10 finished with value: 0.5874600981672605 and parameters: {'observation_period_num': 160, 'train_rates': 0.8968369217475591, 'learning_rate': 1.9330398786747625e-05, 'batch_size': 251, 'step_size': 12, 'gamma': 0.9825397432014571}. Best is trial 2 with value: 0.5698201060295105.[0m
[32m[I 2025-02-02 21:09:07,493][0m Trial 11 finished with value: 0.3669251955177889 and parameters: {'observation_period_num': 151, 'train_rates': 0.9110717207060371, 'learning_rate': 0.0009521079095512535, 'batch_size': 244, 'step_size': 12, 'gamma': 0.9840548984458498}. Best is trial 11 with value: 0.3669251955177889.[0m
[32m[I 2025-02-02 21:12:18,119][0m Trial 12 finished with value: 0.5529495310365108 and parameters: {'observation_period_num': 172, 'train_rates': 0.8975768113553555, 'learning_rate': 0.0009466877921720339, 'batch_size': 184, 'step_size': 12, 'gamma': 0.981167570323381}. Best is trial 11 with value: 0.3669251955177889.[0m
[32m[I 2025-02-02 21:15:38,967][0m Trial 13 finished with value: 0.7062977957003045 and parameters: {'observation_period_num': 180, 'train_rates': 0.8810077475306045, 'learning_rate': 0.0008761943255510155, 'batch_size': 182, 'step_size': 11, 'gamma': 0.9784745332268404}. Best is trial 11 with value: 0.3669251955177889.[0m
[32m[I 2025-02-02 21:18:05,948][0m Trial 14 finished with value: 0.7578087753575781 and parameters: {'observation_period_num': 129, 'train_rates': 0.9024637604255848, 'learning_rate': 0.0008212360680934025, 'batch_size': 177, 'step_size': 10, 'gamma': 0.9588842210574293}. Best is trial 11 with value: 0.3669251955177889.[0m
[32m[I 2025-02-02 21:23:01,303][0m Trial 15 finished with value: 0.33619462259610494 and parameters: {'observation_period_num': 245, 'train_rates': 0.8614636123343054, 'learning_rate': 0.00024885420456745457, 'batch_size': 194, 'step_size': 5, 'gamma': 0.9559248683927937}. Best is trial 15 with value: 0.33619462259610494.[0m
[32m[I 2025-02-02 21:27:55,459][0m Trial 16 finished with value: 0.3787325663024215 and parameters: {'observation_period_num': 248, 'train_rates': 0.8438778137463114, 'learning_rate': 0.0002764015183638172, 'batch_size': 208, 'step_size': 6, 'gamma': 0.949990065369167}. Best is trial 15 with value: 0.33619462259610494.[0m
[32m[I 2025-02-02 21:32:19,197][0m Trial 17 finished with value: 0.7207954431908845 and parameters: {'observation_period_num': 240, 'train_rates': 0.764997283105617, 'learning_rate': 0.0003712829058107234, 'batch_size': 153, 'step_size': 4, 'gamma': 0.8061088009392321}. Best is trial 15 with value: 0.33619462259610494.[0m
[32m[I 2025-02-02 21:36:20,865][0m Trial 18 finished with value: 0.30093321204185486 and parameters: {'observation_period_num': 198, 'train_rates': 0.9388486321294739, 'learning_rate': 6.179170922992192e-05, 'batch_size': 252, 'step_size': 4, 'gamma': 0.9599868309173241}. Best is trial 18 with value: 0.30093321204185486.[0m
[32m[I 2025-02-02 21:40:28,432][0m Trial 19 finished with value: 0.28880974650382996 and parameters: {'observation_period_num': 201, 'train_rates': 0.9394195514833306, 'learning_rate': 5.6045873340952145e-05, 'batch_size': 210, 'step_size': 5, 'gamma': 0.9567227616358032}. Best is trial 19 with value: 0.28880974650382996.[0m
[32m[I 2025-02-02 21:44:38,644][0m Trial 20 finished with value: 0.3253868531659653 and parameters: {'observation_period_num': 205, 'train_rates': 0.9335970297835117, 'learning_rate': 5.602596494211859e-05, 'batch_size': 154, 'step_size': 4, 'gamma': 0.9128290162166098}. Best is trial 19 with value: 0.28880974650382996.[0m
[32m[I 2025-02-02 21:48:43,000][0m Trial 21 finished with value: 0.28469892085334403 and parameters: {'observation_period_num': 195, 'train_rates': 0.9412927836553203, 'learning_rate': 5.663417056944055e-05, 'batch_size': 148, 'step_size': 4, 'gamma': 0.9155701344761275}. Best is trial 21 with value: 0.28469892085334403.[0m
[32m[I 2025-02-02 21:52:49,055][0m Trial 22 finished with value: 0.2342761605978012 and parameters: {'observation_period_num': 195, 'train_rates': 0.9560050517385508, 'learning_rate': 8.386368989185033e-05, 'batch_size': 214, 'step_size': 4, 'gamma': 0.9282542474647204}. Best is trial 22 with value: 0.2342761605978012.[0m
[32m[I 2025-02-02 21:56:20,345][0m Trial 23 finished with value: 0.1820044070482254 and parameters: {'observation_period_num': 179, 'train_rates': 0.9579278064729293, 'learning_rate': 9.282227115899043e-05, 'batch_size': 159, 'step_size': 8, 'gamma': 0.923241802324117}. Best is trial 23 with value: 0.1820044070482254.[0m
[32m[I 2025-02-02 21:59:52,746][0m Trial 24 finished with value: 0.16123680770397186 and parameters: {'observation_period_num': 177, 'train_rates': 0.9819274909340108, 'learning_rate': 0.00010430178875781789, 'batch_size': 146, 'step_size': 8, 'gamma': 0.8527420764844237}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:03:13,399][0m Trial 25 finished with value: 0.1726115494966507 and parameters: {'observation_period_num': 168, 'train_rates': 0.9836840124791455, 'learning_rate': 0.00010903744067324282, 'batch_size': 125, 'step_size': 8, 'gamma': 0.8537566869432192}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:05:33,370][0m Trial 26 finished with value: 0.17706803977489471 and parameters: {'observation_period_num': 124, 'train_rates': 0.9895027653114962, 'learning_rate': 0.00014305360746835614, 'batch_size': 118, 'step_size': 8, 'gamma': 0.8460092321997736}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:07:38,306][0m Trial 27 finished with value: 0.17490535974502563 and parameters: {'observation_period_num': 112, 'train_rates': 0.9884384776391975, 'learning_rate': 0.00016799848578427466, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8462652179863137}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:10:01,692][0m Trial 28 finished with value: 0.1844383329153061 and parameters: {'observation_period_num': 123, 'train_rates': 0.9884928794224638, 'learning_rate': 0.0004420219777828811, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8173751294874847}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:11:24,204][0m Trial 29 finished with value: 0.29424355380603523 and parameters: {'observation_period_num': 73, 'train_rates': 0.9154088879646913, 'learning_rate': 0.00019119979988824749, 'batch_size': 103, 'step_size': 7, 'gamma': 0.7649683938107692}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:14:15,721][0m Trial 30 finished with value: 0.25799446604972665 and parameters: {'observation_period_num': 138, 'train_rates': 0.9695135424066627, 'learning_rate': 2.7840419679919307e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.7820724452906211}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:16:17,204][0m Trial 31 finished with value: 0.18112216889858246 and parameters: {'observation_period_num': 110, 'train_rates': 0.9880116487816094, 'learning_rate': 0.00013577658780405356, 'batch_size': 129, 'step_size': 8, 'gamma': 0.8495323819922819}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:17:45,929][0m Trial 32 finished with value: 0.1711963415145874 and parameters: {'observation_period_num': 77, 'train_rates': 0.9669875668543174, 'learning_rate': 0.00022164431428683512, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8614298433815575}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:18:59,249][0m Trial 33 finished with value: 0.20117563754320145 and parameters: {'observation_period_num': 61, 'train_rates': 0.9642660144073586, 'learning_rate': 0.00044457337274098524, 'batch_size': 73, 'step_size': 6, 'gamma': 0.868997692301426}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:20:29,480][0m Trial 34 finished with value: 0.352016537136466 and parameters: {'observation_period_num': 88, 'train_rates': 0.870474950276431, 'learning_rate': 0.00019768404657947395, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8283312615043731}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:21:06,808][0m Trial 35 finished with value: 1.0406829811805902 and parameters: {'observation_period_num': 45, 'train_rates': 0.6003680541009363, 'learning_rate': 9.358962728209293e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.870989423467259}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:24:11,476][0m Trial 36 finished with value: 0.2375863213354433 and parameters: {'observation_period_num': 162, 'train_rates': 0.923823823144251, 'learning_rate': 0.00031547474001359006, 'batch_size': 165, 'step_size': 6, 'gamma': 0.894695487039471}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:24:59,900][0m Trial 37 finished with value: 0.38602951169013977 and parameters: {'observation_period_num': 21, 'train_rates': 0.9679278951170381, 'learning_rate': 0.0005536780395373586, 'batch_size': 101, 'step_size': 9, 'gamma': 0.7969247328036416}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:26:33,037][0m Trial 38 finished with value: 0.19351305937872523 and parameters: {'observation_period_num': 80, 'train_rates': 0.9607606919888061, 'learning_rate': 3.699726136512879e-05, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8655603020970608}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:28:21,212][0m Trial 39 finished with value: 0.5879541994577431 and parameters: {'observation_period_num': 109, 'train_rates': 0.7958428601204668, 'learning_rate': 0.00015152596429808182, 'batch_size': 126, 'step_size': 7, 'gamma': 0.8996703796268035}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:29:19,809][0m Trial 40 finished with value: 1.3040994207931917 and parameters: {'observation_period_num': 66, 'train_rates': 0.6827312079262441, 'learning_rate': 1.0191359150606918e-05, 'batch_size': 144, 'step_size': 9, 'gamma': 0.8379358727998087}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:31:26,727][0m Trial 41 finished with value: 0.1917133778333664 and parameters: {'observation_period_num': 114, 'train_rates': 0.9898986866919909, 'learning_rate': 0.0001249408615706442, 'batch_size': 117, 'step_size': 8, 'gamma': 0.8491726851174716}. Best is trial 24 with value: 0.16123680770397186.[0m
[32m[I 2025-02-02 22:33:07,689][0m Trial 42 finished with value: 0.13718347251415253 and parameters: {'observation_period_num': 92, 'train_rates': 0.9751189159989856, 'learning_rate': 0.00020580345409222357, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8590511584912979}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:34:51,601][0m Trial 43 finished with value: 0.1721955562452618 and parameters: {'observation_period_num': 96, 'train_rates': 0.9513522768602055, 'learning_rate': 0.0002119063045239725, 'batch_size': 96, 'step_size': 10, 'gamma': 0.8803429313358542}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:36:35,005][0m Trial 44 finished with value: 0.19527109209715465 and parameters: {'observation_period_num': 96, 'train_rates': 0.927118651172522, 'learning_rate': 0.00021157408452995039, 'batch_size': 87, 'step_size': 10, 'gamma': 0.8807692855240402}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:38:13,684][0m Trial 45 finished with value: 0.6195103484433848 and parameters: {'observation_period_num': 88, 'train_rates': 0.9503410952083241, 'learning_rate': 0.0006086758248900434, 'batch_size': 72, 'step_size': 11, 'gamma': 0.8611369731534952}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:39:55,987][0m Trial 46 finished with value: 0.2942168530894489 and parameters: {'observation_period_num': 54, 'train_rates': 0.8869596866495726, 'learning_rate': 7.034366252245956e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.823648417744169}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:42:42,120][0m Trial 47 finished with value: 0.19317927956581116 and parameters: {'observation_period_num': 139, 'train_rates': 0.9720029213455289, 'learning_rate': 4.059866351305488e-05, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8741805561052244}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:43:26,210][0m Trial 48 finished with value: 0.37730416785115783 and parameters: {'observation_period_num': 34, 'train_rates': 0.9212326948915305, 'learning_rate': 2.2018333401232015e-05, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8894894687886671}. Best is trial 42 with value: 0.13718347251415253.[0m
[32m[I 2025-02-02 22:46:35,359][0m Trial 49 finished with value: 0.22243671119213104 and parameters: {'observation_period_num': 159, 'train_rates': 0.9493098120347019, 'learning_rate': 0.0002756213401018983, 'batch_size': 166, 'step_size': 14, 'gamma': 0.9036351428847332}. Best is trial 42 with value: 0.13718347251415253.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-02 22:46:35,367][0m A new study created in memory with name: no-name-f668e25e-aee0-4075-a2e9-ba196ca967ff[0m
[32m[I 2025-02-02 22:49:24,739][0m Trial 0 finished with value: 0.4857118289058025 and parameters: {'observation_period_num': 155, 'train_rates': 0.8607822460361778, 'learning_rate': 0.0005768689825169057, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8068288672197017}. Best is trial 0 with value: 0.4857118289058025.[0m
[32m[I 2025-02-02 22:52:31,580][0m Trial 1 finished with value: 0.4491688634668078 and parameters: {'observation_period_num': 158, 'train_rates': 0.9375094452003342, 'learning_rate': 9.534453541713358e-06, 'batch_size': 85, 'step_size': 13, 'gamma': 0.896453604082139}. Best is trial 1 with value: 0.4491688634668078.[0m
[32m[I 2025-02-02 22:54:17,878][0m Trial 2 finished with value: 1.3512464837167273 and parameters: {'observation_period_num': 127, 'train_rates': 0.6588018608486029, 'learning_rate': 1.596182910440439e-05, 'batch_size': 234, 'step_size': 15, 'gamma': 0.8993123987388896}. Best is trial 1 with value: 0.4491688634668078.[0m
[32m[I 2025-02-02 22:56:55,016][0m Trial 3 finished with value: 1.0893982544337233 and parameters: {'observation_period_num': 157, 'train_rates': 0.7389778080849196, 'learning_rate': 3.3504865032998705e-05, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8928615381457594}. Best is trial 1 with value: 0.4491688634668078.[0m
[32m[I 2025-02-02 22:57:44,467][0m Trial 4 finished with value: 1.0325731230118582 and parameters: {'observation_period_num': 55, 'train_rates': 0.6575757795015118, 'learning_rate': 1.687036313831224e-05, 'batch_size': 114, 'step_size': 13, 'gamma': 0.911735640669401}. Best is trial 1 with value: 0.4491688634668078.[0m
[32m[I 2025-02-02 22:59:49,881][0m Trial 5 finished with value: 0.2506387531757355 and parameters: {'observation_period_num': 116, 'train_rates': 0.9670480199577166, 'learning_rate': 2.695913161458118e-05, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8015424243150008}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:02:41,813][0m Trial 6 finished with value: 0.980266125570286 and parameters: {'observation_period_num': 12, 'train_rates': 0.6961599951695996, 'learning_rate': 0.00021206776094996144, 'batch_size': 21, 'step_size': 13, 'gamma': 0.7929409428529609}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:07:36,356][0m Trial 7 finished with value: 0.4869453122457826 and parameters: {'observation_period_num': 223, 'train_rates': 0.8782588637884303, 'learning_rate': 3.646439919150107e-06, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9375014062171655}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:12:40,447][0m Trial 8 finished with value: 0.25307485405887875 and parameters: {'observation_period_num': 230, 'train_rates': 0.9357159462325385, 'learning_rate': 0.0002489299037452186, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8397753902696358}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:13:17,083][0m Trial 9 finished with value: 0.8819389939308167 and parameters: {'observation_period_num': 31, 'train_rates': 0.9859754449084345, 'learning_rate': 3.068467117667644e-06, 'batch_size': 221, 'step_size': 3, 'gamma': 0.9757697499764026}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:14:42,666][0m Trial 10 finished with value: 0.6456328756449854 and parameters: {'observation_period_num': 88, 'train_rates': 0.8102370246279821, 'learning_rate': 5.205125457229144e-05, 'batch_size': 178, 'step_size': 9, 'gamma': 0.7548987515172633}. Best is trial 5 with value: 0.2506387531757355.[0m
[32m[I 2025-02-02 23:19:54,763][0m Trial 11 finished with value: 0.16240891402072094 and parameters: {'observation_period_num': 234, 'train_rates': 0.9654989111686586, 'learning_rate': 0.0001469235945656219, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8355638840033268}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:24:00,258][0m Trial 12 finished with value: 0.16839224100112915 and parameters: {'observation_period_num': 194, 'train_rates': 0.9842325099691401, 'learning_rate': 0.00013724230794356595, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8435875382352682}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:28:01,895][0m Trial 13 finished with value: 0.32178956988084056 and parameters: {'observation_period_num': 204, 'train_rates': 0.9005818127356157, 'learning_rate': 0.00010609439637474661, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8498419240092772}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:31:31,148][0m Trial 14 finished with value: 1.5289546807607015 and parameters: {'observation_period_num': 191, 'train_rates': 0.7936457906569231, 'learning_rate': 0.0007983418423152033, 'batch_size': 56, 'step_size': 5, 'gamma': 0.849233964337201}. Best is trial 11 with value: 0.16240891402072094.[0m
Early stopping at epoch 70
[32m[I 2025-02-02 23:34:10,725][0m Trial 15 finished with value: 0.8232955552719452 and parameters: {'observation_period_num': 196, 'train_rates': 0.8350518988248121, 'learning_rate': 0.00011898967630038242, 'batch_size': 90, 'step_size': 1, 'gamma': 0.82526799183001}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:38:43,438][0m Trial 16 finished with value: 0.23326729928408194 and parameters: {'observation_period_num': 224, 'train_rates': 0.9201769436970717, 'learning_rate': 0.00033303423596242496, 'batch_size': 179, 'step_size': 10, 'gamma': 0.7698615031745563}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:44:20,898][0m Trial 17 finished with value: 0.17522764205932617 and parameters: {'observation_period_num': 248, 'train_rates': 0.98724132059207, 'learning_rate': 7.452881777973385e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8705694253663786}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:47:20,858][0m Trial 18 finished with value: 1.7803167830106883 and parameters: {'observation_period_num': 180, 'train_rates': 0.7622870042447771, 'learning_rate': 1.169295700959871e-06, 'batch_size': 197, 'step_size': 11, 'gamma': 0.8684585216096422}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:48:42,298][0m Trial 19 finished with value: 0.876047501882756 and parameters: {'observation_period_num': 99, 'train_rates': 0.6014205462481113, 'learning_rate': 0.00017242031607107394, 'batch_size': 250, 'step_size': 8, 'gamma': 0.817295815146744}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:54:12,426][0m Trial 20 finished with value: 0.2552802860736847 and parameters: {'observation_period_num': 250, 'train_rates': 0.9524854214957483, 'learning_rate': 0.00037524153721017956, 'batch_size': 157, 'step_size': 10, 'gamma': 0.9400161552450739}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-02 23:59:56,934][0m Trial 21 finished with value: 0.17549964785575867 and parameters: {'observation_period_num': 252, 'train_rates': 0.9857339681187891, 'learning_rate': 6.774004600900795e-05, 'batch_size': 73, 'step_size': 6, 'gamma': 0.8706215882588282}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-03 00:04:15,325][0m Trial 22 finished with value: 0.33273793682721164 and parameters: {'observation_period_num': 213, 'train_rates': 0.8975584411349921, 'learning_rate': 7.189428288174061e-05, 'batch_size': 115, 'step_size': 6, 'gamma': 0.8721725229119457}. Best is trial 11 with value: 0.16240891402072094.[0m
[32m[I 2025-02-03 00:09:44,613][0m Trial 23 finished with value: 0.15683734481052686 and parameters: {'observation_period_num': 245, 'train_rates': 0.9659034302688089, 'learning_rate': 0.00012101793300551583, 'batch_size': 55, 'step_size': 8, 'gamma': 0.8335030206795978}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:13:10,145][0m Trial 24 finished with value: 0.22848507857475525 and parameters: {'observation_period_num': 170, 'train_rates': 0.9158908642338638, 'learning_rate': 0.0001379259452700951, 'batch_size': 46, 'step_size': 8, 'gamma': 0.7872439750747355}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:17:32,733][0m Trial 25 finished with value: 0.44630417959185864 and parameters: {'observation_period_num': 222, 'train_rates': 0.8599279191285412, 'learning_rate': 0.000565373431300922, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8400133154825944}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:22:38,007][0m Trial 26 finished with value: 0.34777364134788513 and parameters: {'observation_period_num': 233, 'train_rates': 0.9545780487984352, 'learning_rate': 3.8462034379876494e-05, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8260119901537024}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:26:49,438][0m Trial 27 finished with value: 0.22225366532802582 and parameters: {'observation_period_num': 202, 'train_rates': 0.9542796640417243, 'learning_rate': 0.0003380587172123788, 'batch_size': 164, 'step_size': 7, 'gamma': 0.7801243355171777}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:30:23,146][0m Trial 28 finished with value: 0.3237693807472884 and parameters: {'observation_period_num': 183, 'train_rates': 0.8906478056427436, 'learning_rate': 0.00010855540422219943, 'batch_size': 88, 'step_size': 11, 'gamma': 0.8160059758407459}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:33:06,267][0m Trial 29 finished with value: 0.5155431642647712 and parameters: {'observation_period_num': 149, 'train_rates': 0.8672843448683826, 'learning_rate': 0.0009650040717539804, 'batch_size': 138, 'step_size': 4, 'gamma': 0.855010374507937}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:37:27,725][0m Trial 30 finished with value: 0.45573635117427724 and parameters: {'observation_period_num': 209, 'train_rates': 0.8381144515427559, 'learning_rate': 0.000188335484151019, 'batch_size': 36, 'step_size': 12, 'gamma': 0.8064087711649134}. Best is trial 23 with value: 0.15683734481052686.[0m
[32m[I 2025-02-03 00:43:01,444][0m Trial 31 finished with value: 0.14842580258846283 and parameters: {'observation_period_num': 239, 'train_rates': 0.9821808595343902, 'learning_rate': 7.632680080876045e-05, 'batch_size': 62, 'step_size': 6, 'gamma': 0.8744345326380887}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 00:48:04,312][0m Trial 32 finished with value: 0.21858428222964507 and parameters: {'observation_period_num': 234, 'train_rates': 0.9272654515648598, 'learning_rate': 6.023748315055135e-05, 'batch_size': 68, 'step_size': 8, 'gamma': 0.8882762233566169}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 00:53:26,954][0m Trial 33 finished with value: 0.21694164113564926 and parameters: {'observation_period_num': 240, 'train_rates': 0.9716363920008918, 'learning_rate': 2.3675929378889527e-05, 'batch_size': 49, 'step_size': 5, 'gamma': 0.8337393208118021}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 00:58:05,111][0m Trial 34 finished with value: 0.22836524197781408 and parameters: {'observation_period_num': 218, 'train_rates': 0.9459674000636296, 'learning_rate': 4.411387212283419e-05, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9121080876823412}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:01:31,287][0m Trial 35 finished with value: 0.23367410898208618 and parameters: {'observation_period_num': 170, 'train_rates': 0.968252892529076, 'learning_rate': 0.0004950025706635971, 'batch_size': 100, 'step_size': 9, 'gamma': 0.8828806111674867}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:04:04,857][0m Trial 36 finished with value: 0.7508408163414627 and parameters: {'observation_period_num': 136, 'train_rates': 0.913418887984857, 'learning_rate': 1.1791637785252353e-05, 'batch_size': 126, 'step_size': 2, 'gamma': 0.9046199036740842}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:09:33,042][0m Trial 37 finished with value: 0.15479964039781513 and parameters: {'observation_period_num': 243, 'train_rates': 0.9374686025175669, 'learning_rate': 9.220234865660985e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8581187702218551}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:15:24,766][0m Trial 38 finished with value: 0.2254679525035551 and parameters: {'observation_period_num': 243, 'train_rates': 0.9360513548294407, 'learning_rate': 2.3067409618779123e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.8599939935807146}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:17:46,286][0m Trial 39 finished with value: 0.18913031966899926 and parameters: {'observation_period_num': 69, 'train_rates': 0.9632982244776701, 'learning_rate': 0.000245359235617448, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9189016730356614}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:22:02,479][0m Trial 40 finished with value: 0.8814398057636668 and parameters: {'observation_period_num': 235, 'train_rates': 0.7304461034285056, 'learning_rate': 8.021580948915112e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8808006986840162}. Best is trial 31 with value: 0.14842580258846283.[0m
[32m[I 2025-02-03 01:26:52,002][0m Trial 41 finished with value: 0.13306918618131858 and parameters: {'observation_period_num': 214, 'train_rates': 0.977773839543957, 'learning_rate': 0.00014684515690295606, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8370419728201278}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:31:40,153][0m Trial 42 finished with value: 0.17349823818701068 and parameters: {'observation_period_num': 214, 'train_rates': 0.9401009483184387, 'learning_rate': 9.966045083651937e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.8307766189879685}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:37:24,646][0m Trial 43 finished with value: 0.14583799697277022 and parameters: {'observation_period_num': 252, 'train_rates': 0.9698227844912373, 'learning_rate': 0.0001663603625256137, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8161408322365672}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:43:05,780][0m Trial 44 finished with value: 0.1617192741897371 and parameters: {'observation_period_num': 249, 'train_rates': 0.9735073438670839, 'learning_rate': 0.00024237714069545553, 'batch_size': 47, 'step_size': 7, 'gamma': 0.8131901616394708}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:48:11,801][0m Trial 45 finished with value: 0.23056340217590332 and parameters: {'observation_period_num': 227, 'train_rates': 0.9879297363327909, 'learning_rate': 4.655303649880746e-05, 'batch_size': 55, 'step_size': 5, 'gamma': 0.8015843558094347}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:53:33,211][0m Trial 46 finished with value: 0.1598536805111981 and parameters: {'observation_period_num': 237, 'train_rates': 0.9379157641867786, 'learning_rate': 9.326681013437188e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8582017900801635}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 01:59:05,977][0m Trial 47 finished with value: 0.32541982419937254 and parameters: {'observation_period_num': 252, 'train_rates': 0.9072107327487612, 'learning_rate': 2.8543941423881418e-05, 'batch_size': 41, 'step_size': 4, 'gamma': 0.8936088114168399}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 02:03:45,394][0m Trial 48 finished with value: 0.2582482224439873 and parameters: {'observation_period_num': 223, 'train_rates': 0.9254051085422823, 'learning_rate': 0.00017701383356443406, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8464443648005723}. Best is trial 41 with value: 0.13306918618131858.[0m
[32m[I 2025-02-03 02:08:42,689][0m Trial 49 finished with value: 0.14879585293113676 and parameters: {'observation_period_num': 205, 'train_rates': 0.9573817154337709, 'learning_rate': 0.0001414290928503639, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9889792990504408}. Best is trial 41 with value: 0.13306918618131858.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 02:08:42,696][0m A new study created in memory with name: no-name-004b2144-dcf3-486d-b73b-46342210b881[0m
[32m[I 2025-02-03 02:09:28,736][0m Trial 0 finished with value: 0.33243927280459784 and parameters: {'observation_period_num': 45, 'train_rates': 0.9224779742824013, 'learning_rate': 7.812032506665548e-05, 'batch_size': 220, 'step_size': 7, 'gamma': 0.8126706296046412}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:12:24,611][0m Trial 1 finished with value: 2.0155436708376957 and parameters: {'observation_period_num': 187, 'train_rates': 0.6474676347441697, 'learning_rate': 5.5779346669462685e-06, 'batch_size': 185, 'step_size': 3, 'gamma': 0.756267276122242}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:15:49,868][0m Trial 2 finished with value: 1.391866397857666 and parameters: {'observation_period_num': 207, 'train_rates': 0.6865541314098097, 'learning_rate': 1.112436172768902e-05, 'batch_size': 204, 'step_size': 8, 'gamma': 0.7944503126651623}. Best is trial 0 with value: 0.33243927280459784.[0m
Early stopping at epoch 47
[32m[I 2025-02-03 02:16:36,646][0m Trial 3 finished with value: 2.286286666713078 and parameters: {'observation_period_num': 109, 'train_rates': 0.6774409132035326, 'learning_rate': 1.211047730051459e-05, 'batch_size': 221, 'step_size': 1, 'gamma': 0.7993141320813747}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:18:29,013][0m Trial 4 finished with value: 0.752483069896698 and parameters: {'observation_period_num': 78, 'train_rates': 0.8643748589443347, 'learning_rate': 0.0005887742630888265, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9423830634646495}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:18:57,147][0m Trial 5 finished with value: 0.7723105065284237 and parameters: {'observation_period_num': 18, 'train_rates': 0.8944079920194804, 'learning_rate': 3.9978150876283874e-05, 'batch_size': 179, 'step_size': 3, 'gamma': 0.7974828994531062}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:21:22,775][0m Trial 6 finished with value: 0.7185282146526595 and parameters: {'observation_period_num': 149, 'train_rates': 0.7051365691234301, 'learning_rate': 5.5165659208753154e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9481568168072547}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:21:55,964][0m Trial 7 finished with value: 0.6396421627195893 and parameters: {'observation_period_num': 30, 'train_rates': 0.824182008190027, 'learning_rate': 1.7106283045332386e-05, 'batch_size': 169, 'step_size': 10, 'gamma': 0.8622417562901991}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:25:59,207][0m Trial 8 finished with value: 1.0832984740767246 and parameters: {'observation_period_num': 251, 'train_rates': 0.6270125529713331, 'learning_rate': 0.0002877594581612911, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9728966995502273}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:28:43,441][0m Trial 9 finished with value: 0.9802033029335159 and parameters: {'observation_period_num': 188, 'train_rates': 0.6162046045570363, 'learning_rate': 0.0001046129296742827, 'batch_size': 248, 'step_size': 14, 'gamma': 0.921514124355195}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:30:05,358][0m Trial 10 finished with value: 1.053514838218689 and parameters: {'observation_period_num': 72, 'train_rates': 0.9882024859886945, 'learning_rate': 1.7419950798889812e-06, 'batch_size': 109, 'step_size': 8, 'gamma': 0.8654166231581956}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:30:47,546][0m Trial 11 finished with value: 0.5110956241687139 and parameters: {'observation_period_num': 9, 'train_rates': 0.796267292642294, 'learning_rate': 0.00014609700830205, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8642567005325974}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:31:25,662][0m Trial 12 finished with value: 0.7075570192018444 and parameters: {'observation_period_num': 6, 'train_rates': 0.7689216257286315, 'learning_rate': 0.00017686615843092742, 'batch_size': 109, 'step_size': 11, 'gamma': 0.8382477724767332}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:32:27,323][0m Trial 13 finished with value: 0.7688737511634827 and parameters: {'observation_period_num': 55, 'train_rates': 0.963251966218049, 'learning_rate': 0.0009347942839160097, 'batch_size': 121, 'step_size': 6, 'gamma': 0.9028132415999294}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:33:26,422][0m Trial 14 finished with value: 0.7908344548012732 and parameters: {'observation_period_num': 48, 'train_rates': 0.7543475031722575, 'learning_rate': 0.00011731812262565288, 'batch_size': 67, 'step_size': 12, 'gamma': 0.8319520188499503}. Best is trial 0 with value: 0.33243927280459784.[0m
[32m[I 2025-02-03 02:35:18,155][0m Trial 15 finished with value: 0.2611503005027771 and parameters: {'observation_period_num': 110, 'train_rates': 0.9114324575658312, 'learning_rate': 0.0003561777385569567, 'batch_size': 253, 'step_size': 6, 'gamma': 0.8846685297182799}. Best is trial 15 with value: 0.2611503005027771.[0m
[32m[I 2025-02-03 02:37:15,441][0m Trial 16 finished with value: 0.2767235040664673 and parameters: {'observation_period_num': 114, 'train_rates': 0.9205049518556642, 'learning_rate': 0.0004167797070623378, 'batch_size': 247, 'step_size': 5, 'gamma': 0.8908282812834863}. Best is trial 15 with value: 0.2611503005027771.[0m
[32m[I 2025-02-03 02:39:43,551][0m Trial 17 finished with value: 0.21820640563964844 and parameters: {'observation_period_num': 132, 'train_rates': 0.9322895811548912, 'learning_rate': 0.0003563308179034142, 'batch_size': 248, 'step_size': 5, 'gamma': 0.9131579679065336}. Best is trial 17 with value: 0.21820640563964844.[0m
[32m[I 2025-02-03 02:42:23,987][0m Trial 18 finished with value: 0.40991268791169 and parameters: {'observation_period_num': 145, 'train_rates': 0.851622581885173, 'learning_rate': 0.00031020954378989336, 'batch_size': 253, 'step_size': 1, 'gamma': 0.985122540566735}. Best is trial 17 with value: 0.21820640563964844.[0m
[32m[I 2025-02-03 02:44:01,084][0m Trial 19 finished with value: 0.671586275100708 and parameters: {'observation_period_num': 92, 'train_rates': 0.9462913748291286, 'learning_rate': 0.0009816262322867812, 'batch_size': 221, 'step_size': 9, 'gamma': 0.8960407060125316}. Best is trial 17 with value: 0.21820640563964844.[0m
[32m[I 2025-02-03 02:46:41,052][0m Trial 20 finished with value: 0.5268896314604529 and parameters: {'observation_period_num': 147, 'train_rates': 0.8760191086989838, 'learning_rate': 3.071013967852472e-05, 'batch_size': 148, 'step_size': 6, 'gamma': 0.9251391854590825}. Best is trial 17 with value: 0.21820640563964844.[0m
[32m[I 2025-02-03 02:48:37,979][0m Trial 21 finished with value: 0.287987893425537 and parameters: {'observation_period_num': 114, 'train_rates': 0.9094742749678005, 'learning_rate': 0.0003957557932699871, 'batch_size': 250, 'step_size': 5, 'gamma': 0.8894509946761364}. Best is trial 17 with value: 0.21820640563964844.[0m
[32m[I 2025-02-03 02:50:48,680][0m Trial 22 finished with value: 0.21501272916793823 and parameters: {'observation_period_num': 124, 'train_rates': 0.9319072650790927, 'learning_rate': 0.0002591310976657259, 'batch_size': 237, 'step_size': 4, 'gamma': 0.8845310964235101}. Best is trial 22 with value: 0.21501272916793823.[0m
[32m[I 2025-02-03 02:53:33,752][0m Trial 23 finished with value: 0.18497340381145477 and parameters: {'observation_period_num': 137, 'train_rates': 0.9845230727839808, 'learning_rate': 0.00022644117729577094, 'batch_size': 208, 'step_size': 3, 'gamma': 0.9188857814694609}. Best is trial 23 with value: 0.18497340381145477.[0m
[32m[I 2025-02-03 02:56:53,733][0m Trial 24 finished with value: 0.203459694981575 and parameters: {'observation_period_num': 168, 'train_rates': 0.9874683525114901, 'learning_rate': 0.00018122058031833262, 'batch_size': 198, 'step_size': 3, 'gamma': 0.9193497843858897}. Best is trial 23 with value: 0.18497340381145477.[0m
[32m[I 2025-02-03 03:00:38,233][0m Trial 25 finished with value: 0.33542123436927795 and parameters: {'observation_period_num': 181, 'train_rates': 0.9899647137633067, 'learning_rate': 6.737826848132469e-05, 'batch_size': 197, 'step_size': 2, 'gamma': 0.9471488450760907}. Best is trial 23 with value: 0.18497340381145477.[0m
[32m[I 2025-02-03 03:04:45,875][0m Trial 26 finished with value: 0.16776925325393677 and parameters: {'observation_period_num': 165, 'train_rates': 0.962644962874279, 'learning_rate': 0.00020592956781757178, 'batch_size': 203, 'step_size': 3, 'gamma': 0.963318140369514}. Best is trial 26 with value: 0.16776925325393677.[0m
[32m[I 2025-02-03 03:08:07,513][0m Trial 27 finished with value: 0.18318986892700195 and parameters: {'observation_period_num': 172, 'train_rates': 0.9640152250591663, 'learning_rate': 0.0002074479046273214, 'batch_size': 201, 'step_size': 2, 'gamma': 0.9612516561537101}. Best is trial 26 with value: 0.16776925325393677.[0m
[32m[I 2025-02-03 03:12:37,147][0m Trial 28 finished with value: 0.2277088612318039 and parameters: {'observation_period_num': 216, 'train_rates': 0.9620248016501891, 'learning_rate': 0.0006573214589975379, 'batch_size': 167, 'step_size': 2, 'gamma': 0.9713893621637452}. Best is trial 26 with value: 0.16776925325393677.[0m
[32m[I 2025-02-03 03:15:48,037][0m Trial 29 finished with value: 0.25211960077285767 and parameters: {'observation_period_num': 163, 'train_rates': 0.9577311044925856, 'learning_rate': 8.724336648719615e-05, 'batch_size': 225, 'step_size': 2, 'gamma': 0.9690575308900911}. Best is trial 26 with value: 0.16776925325393677.[0m
[32m[I 2025-02-03 03:20:06,099][0m Trial 30 finished with value: 1.11110802350846 and parameters: {'observation_period_num': 217, 'train_rates': 0.8346551492325134, 'learning_rate': 3.0082989367126326e-05, 'batch_size': 207, 'step_size': 1, 'gamma': 0.9373834128146895}. Best is trial 26 with value: 0.16776925325393677.[0m
[32m[I 2025-02-03 03:23:24,374][0m Trial 31 finished with value: 0.1549530327320099 and parameters: {'observation_period_num': 168, 'train_rates': 0.984351423310229, 'learning_rate': 0.00018931844816329907, 'batch_size': 193, 'step_size': 3, 'gamma': 0.9593378536823582}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:26:44,073][0m Trial 32 finished with value: 0.16777734458446503 and parameters: {'observation_period_num': 170, 'train_rates': 0.965820682231285, 'learning_rate': 0.0001630081321604396, 'batch_size': 186, 'step_size': 4, 'gamma': 0.9560000277533756}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:30:40,623][0m Trial 33 finished with value: 0.2174399495124817 and parameters: {'observation_period_num': 194, 'train_rates': 0.9463782533834566, 'learning_rate': 0.00013041866005137205, 'batch_size': 182, 'step_size': 4, 'gamma': 0.984811277352842}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:33:55,081][0m Trial 34 finished with value: 0.4072967266690904 and parameters: {'observation_period_num': 167, 'train_rates': 0.9008373187208598, 'learning_rate': 5.666093107330021e-05, 'batch_size': 140, 'step_size': 2, 'gamma': 0.9579613190424273}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:37:46,465][0m Trial 35 finished with value: 0.3344223337095292 and parameters: {'observation_period_num': 200, 'train_rates': 0.8892797755047253, 'learning_rate': 0.0005422828895586843, 'batch_size': 192, 'step_size': 7, 'gamma': 0.7663572131971734}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:41:16,528][0m Trial 36 finished with value: 0.8272092938423157 and parameters: {'observation_period_num': 177, 'train_rates': 0.9677818463879221, 'learning_rate': 5.388335498441917e-06, 'batch_size': 173, 'step_size': 4, 'gamma': 0.9573459470165149}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:44:16,988][0m Trial 37 finished with value: 0.20513851940631866 and parameters: {'observation_period_num': 157, 'train_rates': 0.9372195033003254, 'learning_rate': 0.00021172411362034726, 'batch_size': 230, 'step_size': 1, 'gamma': 0.988764910206618}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:48:29,339][0m Trial 38 finished with value: 0.9663133668396278 and parameters: {'observation_period_num': 238, 'train_rates': 0.7206984037418245, 'learning_rate': 4.4413951598668526e-05, 'batch_size': 158, 'step_size': 3, 'gamma': 0.9619487773635194}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:52:32,225][0m Trial 39 finished with value: 0.3234843023287983 and parameters: {'observation_period_num': 207, 'train_rates': 0.8859507887134701, 'learning_rate': 8.842380726645056e-05, 'batch_size': 216, 'step_size': 4, 'gamma': 0.9393760490891742}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:55:50,100][0m Trial 40 finished with value: 0.3475367090918801 and parameters: {'observation_period_num': 174, 'train_rates': 0.9209557075154057, 'learning_rate': 0.00064806479385538, 'batch_size': 184, 'step_size': 2, 'gamma': 0.9332473284887973}. Best is trial 31 with value: 0.1549530327320099.[0m
[32m[I 2025-02-03 03:58:42,085][0m Trial 41 finished with value: 0.1408453732728958 and parameters: {'observation_period_num': 146, 'train_rates': 0.97638315430517, 'learning_rate': 0.00021339223263146682, 'batch_size': 209, 'step_size': 3, 'gamma': 0.9521369356707362}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:01:47,920][0m Trial 42 finished with value: 0.23569521307945251 and parameters: {'observation_period_num': 158, 'train_rates': 0.9687714738092207, 'learning_rate': 0.0001462866464712589, 'batch_size': 213, 'step_size': 3, 'gamma': 0.9531284222249495}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:05:35,857][0m Trial 43 finished with value: 0.3973783552646637 and parameters: {'observation_period_num': 189, 'train_rates': 0.946919822675009, 'learning_rate': 1.7401674105351737e-05, 'batch_size': 192, 'step_size': 5, 'gamma': 0.9731192148215942}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:08:19,300][0m Trial 44 finished with value: 0.30193448066711426 and parameters: {'observation_period_num': 142, 'train_rates': 0.9673032292728353, 'learning_rate': 0.0002176080785226458, 'batch_size': 235, 'step_size': 1, 'gamma': 0.9329646900165891}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:10:35,440][0m Trial 45 finished with value: 0.2081724852323532 and parameters: {'observation_period_num': 125, 'train_rates': 0.9722267313750615, 'learning_rate': 0.0005247170658848968, 'batch_size': 170, 'step_size': 3, 'gamma': 0.9759449338117974}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:13:16,523][0m Trial 46 finished with value: 0.5227229196836453 and parameters: {'observation_period_num': 152, 'train_rates': 0.8557741721237971, 'learning_rate': 0.00010125718353202039, 'batch_size': 204, 'step_size': 4, 'gamma': 0.9452586896781104}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:15:58,202][0m Trial 47 finished with value: 0.865949263277742 and parameters: {'observation_period_num': 173, 'train_rates': 0.6509670375897917, 'learning_rate': 0.00014618674777501687, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9061482434999455}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:17:33,522][0m Trial 48 finished with value: 0.28271499276161194 and parameters: {'observation_period_num': 90, 'train_rates': 0.9476601993228704, 'learning_rate': 7.180455134043948e-05, 'batch_size': 187, 'step_size': 2, 'gamma': 0.9621810119304565}. Best is trial 41 with value: 0.1408453732728958.[0m
[32m[I 2025-02-03 04:21:30,218][0m Trial 49 finished with value: 1.690812273068471 and parameters: {'observation_period_num': 198, 'train_rates': 0.9196235307182149, 'learning_rate': 1.017242388835431e-06, 'batch_size': 178, 'step_size': 3, 'gamma': 0.8510625611136853}. Best is trial 41 with value: 0.1408453732728958.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 04:21:30,226][0m A new study created in memory with name: no-name-ce45a974-c34e-4a67-826d-d0c07bda144b[0m
[32m[I 2025-02-03 04:24:55,310][0m Trial 0 finished with value: 1.315915340319612 and parameters: {'observation_period_num': 188, 'train_rates': 0.8075269895298339, 'learning_rate': 3.1094707501619997e-06, 'batch_size': 58, 'step_size': 3, 'gamma': 0.8968498427238757}. Best is trial 0 with value: 1.315915340319612.[0m
[32m[I 2025-02-03 04:25:12,739][0m Trial 1 finished with value: 0.8170586686449003 and parameters: {'observation_period_num': 12, 'train_rates': 0.6649957655837447, 'learning_rate': 0.0004168058608067135, 'batch_size': 219, 'step_size': 4, 'gamma': 0.7793923470935283}. Best is trial 1 with value: 0.8170586686449003.[0m
[32m[I 2025-02-03 04:29:28,811][0m Trial 2 finished with value: 0.3958948476748033 and parameters: {'observation_period_num': 209, 'train_rates': 0.8997441802182138, 'learning_rate': 0.000815761854972706, 'batch_size': 195, 'step_size': 12, 'gamma': 0.9670413719728774}. Best is trial 2 with value: 0.3958948476748033.[0m
[32m[I 2025-02-03 04:34:14,140][0m Trial 3 finished with value: 0.20968898765321048 and parameters: {'observation_period_num': 220, 'train_rates': 0.9613251199896478, 'learning_rate': 0.000353105159671504, 'batch_size': 74, 'step_size': 13, 'gamma': 0.7792119523861889}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:36:57,919][0m Trial 4 finished with value: 0.6015725134141383 and parameters: {'observation_period_num': 160, 'train_rates': 0.8072101189008217, 'learning_rate': 0.0005238623213270218, 'batch_size': 166, 'step_size': 6, 'gamma': 0.8406471171617285}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:37:21,002][0m Trial 5 finished with value: 0.8700119105096047 and parameters: {'observation_period_num': 17, 'train_rates': 0.7164116372186456, 'learning_rate': 0.00016032025363522693, 'batch_size': 188, 'step_size': 3, 'gamma': 0.8340108841870026}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:40:11,034][0m Trial 6 finished with value: 0.9131493972064891 and parameters: {'observation_period_num': 178, 'train_rates': 0.6234927318208475, 'learning_rate': 7.960922288613052e-05, 'batch_size': 41, 'step_size': 3, 'gamma': 0.9389955708944248}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:44:19,272][0m Trial 7 finished with value: 1.2471199549477676 and parameters: {'observation_period_num': 227, 'train_rates': 0.7657766913083666, 'learning_rate': 3.658735569658722e-05, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9107899683048286}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:46:56,065][0m Trial 8 finished with value: 1.1674805425557515 and parameters: {'observation_period_num': 152, 'train_rates': 0.7873223548476588, 'learning_rate': 3.399713767600451e-06, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8954005005275638}. Best is trial 3 with value: 0.20968898765321048.[0m
Early stopping at epoch 99
[32m[I 2025-02-03 04:49:42,933][0m Trial 9 finished with value: 1.357045333612241 and parameters: {'observation_period_num': 151, 'train_rates': 0.8613322874364504, 'learning_rate': 8.957112780159503e-06, 'batch_size': 175, 'step_size': 1, 'gamma': 0.8980711640809336}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:51:26,506][0m Trial 10 finished with value: 0.33999133110046387 and parameters: {'observation_period_num': 92, 'train_rates': 0.9798883617080157, 'learning_rate': 1.632237027329568e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.7736956028995128}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:53:04,974][0m Trial 11 finished with value: 0.4309587776660919 and parameters: {'observation_period_num': 88, 'train_rates': 0.9867614832683723, 'learning_rate': 1.1507386133567637e-05, 'batch_size': 98, 'step_size': 10, 'gamma': 0.7551926358570212}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 04:54:49,289][0m Trial 12 finished with value: 0.32271599769592285 and parameters: {'observation_period_num': 90, 'train_rates': 0.9794455898221065, 'learning_rate': 2.2254343306084028e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8054770118188024}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 05:00:47,194][0m Trial 13 finished with value: 0.2632610160477307 and parameters: {'observation_period_num': 252, 'train_rates': 0.9095402578732512, 'learning_rate': 0.00011938012972366455, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8116335321901263}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 05:06:10,168][0m Trial 14 finished with value: 0.28032833177924615 and parameters: {'observation_period_num': 245, 'train_rates': 0.9038152193663068, 'learning_rate': 0.00021226731177089668, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8099006397566201}. Best is trial 3 with value: 0.20968898765321048.[0m
[32m[I 2025-02-03 05:11:42,369][0m Trial 15 finished with value: 0.1964673536884434 and parameters: {'observation_period_num': 246, 'train_rates': 0.9248368222053015, 'learning_rate': 7.914631677407338e-05, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8579320121060078}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:16:08,568][0m Trial 16 finished with value: 0.2871544361114502 and parameters: {'observation_period_num': 216, 'train_rates': 0.9348063855365639, 'learning_rate': 6.323611806774815e-05, 'batch_size': 253, 'step_size': 13, 'gamma': 0.8591899089087529}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:18:13,467][0m Trial 17 finished with value: 0.3537586891617855 and parameters: {'observation_period_num': 116, 'train_rates': 0.8738119253253439, 'learning_rate': 0.0002930861483516647, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8676848399021647}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:21:42,749][0m Trial 18 finished with value: 0.6412504903349866 and parameters: {'observation_period_num': 190, 'train_rates': 0.8396332753992675, 'learning_rate': 4.507865631214791e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.7530725130043935}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:25:19,310][0m Trial 19 finished with value: 1.1246023082093104 and parameters: {'observation_period_num': 51, 'train_rates': 0.9489430080413894, 'learning_rate': 0.0007507877439344572, 'batch_size': 18, 'step_size': 14, 'gamma': 0.988462110398303}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:30:11,694][0m Trial 20 finished with value: 0.2523776950030927 and parameters: {'observation_period_num': 230, 'train_rates': 0.9447550368940378, 'learning_rate': 0.0001080535776051922, 'batch_size': 143, 'step_size': 11, 'gamma': 0.841009281178178}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:35:01,037][0m Trial 21 finished with value: 1.6297846561984013 and parameters: {'observation_period_num': 229, 'train_rates': 0.9442303043289141, 'learning_rate': 1.0639811085002483e-06, 'batch_size': 148, 'step_size': 11, 'gamma': 0.8365725849330732}. Best is trial 15 with value: 0.1964673536884434.[0m
[32m[I 2025-02-03 05:39:15,833][0m Trial 22 finished with value: 0.1934953464993409 and parameters: {'observation_period_num': 205, 'train_rates': 0.9389239180868495, 'learning_rate': 0.00010483334961800736, 'batch_size': 93, 'step_size': 14, 'gamma': 0.7895135332554796}. Best is trial 22 with value: 0.1934953464993409.[0m
[32m[I 2025-02-03 05:43:23,789][0m Trial 23 finished with value: 0.36673119739324106 and parameters: {'observation_period_num': 203, 'train_rates': 0.870317894168483, 'learning_rate': 0.00025556518189023375, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7804526010005289}. Best is trial 22 with value: 0.1934953464993409.[0m
[32m[I 2025-02-03 05:46:47,110][0m Trial 24 finished with value: 0.2111179748514913 and parameters: {'observation_period_num': 170, 'train_rates': 0.9239110073625847, 'learning_rate': 6.360055800384692e-05, 'batch_size': 55, 'step_size': 13, 'gamma': 0.7930392257317898}. Best is trial 22 with value: 0.1934953464993409.[0m
[32m[I 2025-02-03 05:51:57,089][0m Trial 25 finished with value: 0.4343050877763846 and parameters: {'observation_period_num': 252, 'train_rates': 0.8415642264665848, 'learning_rate': 0.0001821473639785829, 'batch_size': 66, 'step_size': 14, 'gamma': 0.8205531954345325}. Best is trial 22 with value: 0.1934953464993409.[0m
[32m[I 2025-02-03 05:54:29,004][0m Trial 26 finished with value: 0.16674687365690868 and parameters: {'observation_period_num': 130, 'train_rates': 0.9577436199911334, 'learning_rate': 0.00033127150386787235, 'batch_size': 112, 'step_size': 14, 'gamma': 0.7905197678127541}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 05:56:22,996][0m Trial 27 finished with value: 0.42148721946610346 and parameters: {'observation_period_num': 110, 'train_rates': 0.8892721433583564, 'learning_rate': 2.812944937924189e-05, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8573698997174944}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 05:58:33,759][0m Trial 28 finished with value: 0.7961539636578476 and parameters: {'observation_period_num': 133, 'train_rates': 0.7371319492042596, 'learning_rate': 9.129143499551849e-05, 'batch_size': 100, 'step_size': 8, 'gamma': 0.9274707480145438}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:02:11,820][0m Trial 29 finished with value: 0.50942565608132 and parameters: {'observation_period_num': 191, 'train_rates': 0.8392271566681714, 'learning_rate': 0.00015299224997924104, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8846428285663379}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:06:12,477][0m Trial 30 finished with value: 0.2876417846418917 and parameters: {'observation_period_num': 199, 'train_rates': 0.9186666006996236, 'learning_rate': 0.0005367995475359363, 'batch_size': 133, 'step_size': 12, 'gamma': 0.7991215970260007}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:10:57,930][0m Trial 31 finished with value: 0.20333288537292946 and parameters: {'observation_period_num': 221, 'train_rates': 0.9550715973215829, 'learning_rate': 0.00033789470613022996, 'batch_size': 75, 'step_size': 14, 'gamma': 0.769501559551666}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:16:01,990][0m Trial 32 finished with value: 0.17587678303772752 and parameters: {'observation_period_num': 235, 'train_rates': 0.9516433538992718, 'learning_rate': 0.00035217431670970124, 'batch_size': 111, 'step_size': 14, 'gamma': 0.7660140117990063}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:21:23,778][0m Trial 33 finished with value: 0.27802836894989014 and parameters: {'observation_period_num': 244, 'train_rates': 0.9648171540940187, 'learning_rate': 0.0005607250260001247, 'batch_size': 109, 'step_size': 5, 'gamma': 0.7906533108269713}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:22:27,035][0m Trial 34 finished with value: 0.25191162832795755 and parameters: {'observation_period_num': 62, 'train_rates': 0.9243597784975406, 'learning_rate': 0.00013548533115413904, 'batch_size': 134, 'step_size': 12, 'gamma': 0.7595468465652778}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:26:49,700][0m Trial 35 finished with value: 0.26504290103912354 and parameters: {'observation_period_num': 207, 'train_rates': 0.9889502710710437, 'learning_rate': 0.0009290211252931779, 'batch_size': 154, 'step_size': 14, 'gamma': 0.8231783627619206}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:31:41,804][0m Trial 36 finished with value: 0.3502862223304144 and parameters: {'observation_period_num': 237, 'train_rates': 0.8884009023528711, 'learning_rate': 5.452046861677612e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.786109995409358}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:35:09,250][0m Trial 37 finished with value: 0.21624968821803728 and parameters: {'observation_period_num': 172, 'train_rates': 0.9657686498520874, 'learning_rate': 0.000424162249789055, 'batch_size': 83, 'step_size': 13, 'gamma': 0.7647539970628386}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:37:39,042][0m Trial 38 finished with value: 0.30468479843538426 and parameters: {'observation_period_num': 135, 'train_rates': 0.9003997130345465, 'learning_rate': 0.00021458549892483362, 'batch_size': 163, 'step_size': 15, 'gamma': 0.8197861973644937}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:39:19,660][0m Trial 39 finished with value: 0.8009446067245383 and parameters: {'observation_period_num': 27, 'train_rates': 0.6881358310195589, 'learning_rate': 8.437026164504994e-05, 'batch_size': 32, 'step_size': 12, 'gamma': 0.9579696889246962}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:43:45,435][0m Trial 40 finished with value: 0.27378515094176104 and parameters: {'observation_period_num': 214, 'train_rates': 0.9365126320823493, 'learning_rate': 0.0003873484035158716, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8489206053093223}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:48:46,419][0m Trial 41 finished with value: 0.2395782563695334 and parameters: {'observation_period_num': 225, 'train_rates': 0.9512981388630776, 'learning_rate': 0.00029012936574598284, 'batch_size': 73, 'step_size': 14, 'gamma': 0.7749209887744921}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:53:55,761][0m Trial 42 finished with value: 0.16892319917678833 and parameters: {'observation_period_num': 237, 'train_rates': 0.9625791498098556, 'learning_rate': 0.0003697468014270604, 'batch_size': 110, 'step_size': 14, 'gamma': 0.769250800250316}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 06:58:52,722][0m Trial 43 finished with value: 0.2946504718571339 and parameters: {'observation_period_num': 235, 'train_rates': 0.9222374107617172, 'learning_rate': 0.0005450845693171204, 'batch_size': 111, 'step_size': 13, 'gamma': 0.7509929629761956}. Best is trial 26 with value: 0.16674687365690868.[0m
[32m[I 2025-02-03 07:04:15,042][0m Trial 44 finished with value: 0.14823217689990997 and parameters: {'observation_period_num': 239, 'train_rates': 0.9706047478926179, 'learning_rate': 0.00018245554330405971, 'batch_size': 89, 'step_size': 15, 'gamma': 0.7834205846160324}. Best is trial 44 with value: 0.14823217689990997.[0m
[32m[I 2025-02-03 07:07:53,943][0m Trial 45 finished with value: 0.4924939274787903 and parameters: {'observation_period_num': 179, 'train_rates': 0.968358771853207, 'learning_rate': 0.0007432770397877399, 'batch_size': 90, 'step_size': 14, 'gamma': 0.7994768656397926}. Best is trial 44 with value: 0.14823217689990997.[0m
[32m[I 2025-02-03 07:10:14,339][0m Trial 46 finished with value: 0.999957392003104 and parameters: {'observation_period_num': 156, 'train_rates': 0.6327933573808688, 'learning_rate': 0.00020664452899321931, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7823834983032857}. Best is trial 44 with value: 0.14823217689990997.[0m
[32m[I 2025-02-03 07:14:30,365][0m Trial 47 finished with value: 0.16869822144508362 and parameters: {'observation_period_num': 197, 'train_rates': 0.9731055756086172, 'learning_rate': 0.00013866413075379645, 'batch_size': 93, 'step_size': 10, 'gamma': 0.7667506033347496}. Best is trial 44 with value: 0.14823217689990997.[0m
[32m[I 2025-02-03 07:17:24,877][0m Trial 48 finished with value: 0.16080564260482788 and parameters: {'observation_period_num': 143, 'train_rates': 0.9739095306914727, 'learning_rate': 0.0004033467791183447, 'batch_size': 118, 'step_size': 9, 'gamma': 0.7700975949645631}. Best is trial 44 with value: 0.14823217689990997.[0m
[32m[I 2025-02-03 07:20:11,864][0m Trial 49 finished with value: 0.19551508128643036 and parameters: {'observation_period_num': 144, 'train_rates': 0.9741114206471022, 'learning_rate': 0.0001566697752038159, 'batch_size': 144, 'step_size': 8, 'gamma': 0.7688315800080308}. Best is trial 44 with value: 0.14823217689990997.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_PFE_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.9705950714146946, 'learning_rate': 9.011730237716253e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7510806385844074}
Epoch 1/300, trend Loss: 0.5623 | 0.4796
Epoch 2/300, trend Loss: 0.4473 | 0.4300
Epoch 3/300, trend Loss: 0.3735 | 0.3749
Epoch 4/300, trend Loss: 0.3108 | 0.3480
Epoch 5/300, trend Loss: 0.2752 | 0.3229
Epoch 6/300, trend Loss: 0.2480 | 0.2700
Epoch 7/300, trend Loss: 0.2290 | 0.2459
Epoch 8/300, trend Loss: 0.2201 | 0.2449
Epoch 9/300, trend Loss: 0.2065 | 0.2154
Epoch 10/300, trend Loss: 0.1914 | 0.1965
Epoch 11/300, trend Loss: 0.1835 | 0.2020
Epoch 12/300, trend Loss: 0.1771 | 0.1831
Epoch 13/300, trend Loss: 0.1721 | 0.1715
Epoch 14/300, trend Loss: 0.1630 | 0.1760
Epoch 15/300, trend Loss: 0.1606 | 0.1693
Epoch 16/300, trend Loss: 0.1578 | 0.1618
Epoch 17/300, trend Loss: 0.1503 | 0.1558
Epoch 18/300, trend Loss: 0.1488 | 0.1572
Epoch 19/300, trend Loss: 0.1465 | 0.1510
Epoch 20/300, trend Loss: 0.1407 | 0.1553
Epoch 21/300, trend Loss: 0.1386 | 0.1534
Epoch 22/300, trend Loss: 0.1393 | 0.1515
Epoch 23/300, trend Loss: 0.1359 | 0.1556
Epoch 24/300, trend Loss: 0.1312 | 0.1449
Epoch 25/300, trend Loss: 0.1300 | 0.1348
Epoch 26/300, trend Loss: 0.1268 | 0.1424
Epoch 27/300, trend Loss: 0.1251 | 0.1437
Epoch 28/300, trend Loss: 0.1267 | 0.1472
Epoch 29/300, trend Loss: 0.1236 | 0.1641
Epoch 30/300, trend Loss: 0.1191 | 0.1465
Epoch 31/300, trend Loss: 0.1160 | 0.1321
Epoch 32/300, trend Loss: 0.1147 | 0.1287
Epoch 33/300, trend Loss: 0.1129 | 0.1381
Epoch 34/300, trend Loss: 0.1128 | 0.1377
Epoch 35/300, trend Loss: 0.1124 | 0.1396
Epoch 36/300, trend Loss: 0.1104 | 0.1447
Epoch 37/300, trend Loss: 0.1107 | 0.1262
Epoch 38/300, trend Loss: 0.1087 | 0.1242
Epoch 39/300, trend Loss: 0.1085 | 0.1289
Epoch 40/300, trend Loss: 0.1071 | 0.1328
Epoch 41/300, trend Loss: 0.1067 | 0.1412
Epoch 42/300, trend Loss: 0.1059 | 0.1336
Epoch 43/300, trend Loss: 0.1058 | 0.1249
Epoch 44/300, trend Loss: 0.1029 | 0.1304
Epoch 45/300, trend Loss: 0.1029 | 0.1240
Epoch 46/300, trend Loss: 0.1013 | 0.1266
Epoch 47/300, trend Loss: 0.0988 | 0.1237
Epoch 48/300, trend Loss: 0.0986 | 0.1191
Epoch 49/300, trend Loss: 0.0978 | 0.1209
Epoch 50/300, trend Loss: 0.0970 | 0.1201
Epoch 51/300, trend Loss: 0.0963 | 0.1222
Epoch 52/300, trend Loss: 0.0956 | 0.1190
Epoch 53/300, trend Loss: 0.0949 | 0.1203
Epoch 54/300, trend Loss: 0.0942 | 0.1182
Epoch 55/300, trend Loss: 0.0946 | 0.1175
Epoch 56/300, trend Loss: 0.0946 | 0.1161
Epoch 57/300, trend Loss: 0.0931 | 0.1153
Epoch 58/300, trend Loss: 0.0928 | 0.1154
Epoch 59/300, trend Loss: 0.0922 | 0.1166
Epoch 60/300, trend Loss: 0.0917 | 0.1152
Epoch 61/300, trend Loss: 0.0915 | 0.1166
Epoch 62/300, trend Loss: 0.0913 | 0.1139
Epoch 63/300, trend Loss: 0.0911 | 0.1139
Epoch 64/300, trend Loss: 0.0907 | 0.1120
Epoch 65/300, trend Loss: 0.0903 | 0.1171
Epoch 66/300, trend Loss: 0.0903 | 0.1129
Epoch 67/300, trend Loss: 0.0896 | 0.1171
Epoch 68/300, trend Loss: 0.0899 | 0.1164
Epoch 69/300, trend Loss: 0.0896 | 0.1144
Epoch 70/300, trend Loss: 0.0895 | 0.1145
Epoch 71/300, trend Loss: 0.0891 | 0.1123
Epoch 72/300, trend Loss: 0.0888 | 0.1128
Epoch 73/300, trend Loss: 0.0880 | 0.1159
Epoch 74/300, trend Loss: 0.0888 | 0.1132
Epoch 75/300, trend Loss: 0.0881 | 0.1127
Epoch 76/300, trend Loss: 0.0880 | 0.1109
Epoch 77/300, trend Loss: 0.0885 | 0.1132
Epoch 78/300, trend Loss: 0.0879 | 0.1125
Epoch 79/300, trend Loss: 0.0876 | 0.1106
Epoch 80/300, trend Loss: 0.0880 | 0.1120
Epoch 81/300, trend Loss: 0.0878 | 0.1101
Epoch 82/300, trend Loss: 0.0872 | 0.1139
Epoch 83/300, trend Loss: 0.0871 | 0.1126
Epoch 84/300, trend Loss: 0.0869 | 0.1116
Epoch 85/300, trend Loss: 0.0867 | 0.1111
Epoch 86/300, trend Loss: 0.0867 | 0.1122
Epoch 87/300, trend Loss: 0.0862 | 0.1112
Epoch 88/300, trend Loss: 0.0864 | 0.1111
Epoch 89/300, trend Loss: 0.0863 | 0.1099
Epoch 90/300, trend Loss: 0.0863 | 0.1105
Epoch 91/300, trend Loss: 0.0860 | 0.1106
Epoch 92/300, trend Loss: 0.0857 | 0.1120
Epoch 93/300, trend Loss: 0.0865 | 0.1123
Epoch 94/300, trend Loss: 0.0865 | 0.1111
Epoch 95/300, trend Loss: 0.0863 | 0.1115
Epoch 96/300, trend Loss: 0.0855 | 0.1123
Epoch 97/300, trend Loss: 0.0857 | 0.1101
Epoch 98/300, trend Loss: 0.0860 | 0.1111
Epoch 99/300, trend Loss: 0.0858 | 0.1114
Epoch 100/300, trend Loss: 0.0856 | 0.1104
Epoch 101/300, trend Loss: 0.0861 | 0.1102
Epoch 102/300, trend Loss: 0.0855 | 0.1096
Epoch 103/300, trend Loss: 0.0854 | 0.1106
Epoch 104/300, trend Loss: 0.0851 | 0.1104
Epoch 105/300, trend Loss: 0.0857 | 0.1102
Epoch 106/300, trend Loss: 0.0861 | 0.1099
Epoch 107/300, trend Loss: 0.0853 | 0.1101
Epoch 108/300, trend Loss: 0.0857 | 0.1104
Epoch 109/300, trend Loss: 0.0855 | 0.1105
Epoch 110/300, trend Loss: 0.0853 | 0.1107
Epoch 111/300, trend Loss: 0.0851 | 0.1107
Epoch 112/300, trend Loss: 0.0854 | 0.1106
Epoch 113/300, trend Loss: 0.0847 | 0.1106
Epoch 114/300, trend Loss: 0.0855 | 0.1106
Epoch 115/300, trend Loss: 0.0852 | 0.1095
Epoch 116/300, trend Loss: 0.0854 | 0.1099
Epoch 117/300, trend Loss: 0.0852 | 0.1098
Epoch 118/300, trend Loss: 0.0849 | 0.1094
Epoch 119/300, trend Loss: 0.0849 | 0.1092
Epoch 120/300, trend Loss: 0.0851 | 0.1091
Epoch 121/300, trend Loss: 0.0844 | 0.1098
Epoch 122/300, trend Loss: 0.0847 | 0.1101
Epoch 123/300, trend Loss: 0.0849 | 0.1101
Epoch 124/300, trend Loss: 0.0846 | 0.1107
Epoch 125/300, trend Loss: 0.0848 | 0.1107
Epoch 126/300, trend Loss: 0.0845 | 0.1099
Epoch 127/300, trend Loss: 0.0855 | 0.1102
Epoch 128/300, trend Loss: 0.0845 | 0.1104
Epoch 129/300, trend Loss: 0.0852 | 0.1106
Epoch 130/300, trend Loss: 0.0847 | 0.1105
Epoch 131/300, trend Loss: 0.0848 | 0.1106
Epoch 132/300, trend Loss: 0.0847 | 0.1104
Epoch 133/300, trend Loss: 0.0849 | 0.1102
Epoch 134/300, trend Loss: 0.0847 | 0.1099
Epoch 135/300, trend Loss: 0.0849 | 0.1100
Epoch 136/300, trend Loss: 0.0846 | 0.1103
Epoch 137/300, trend Loss: 0.0848 | 0.1107
Epoch 138/300, trend Loss: 0.0847 | 0.1104
Epoch 139/300, trend Loss: 0.0843 | 0.1103
Epoch 140/300, trend Loss: 0.0845 | 0.1102
Epoch 141/300, trend Loss: 0.0847 | 0.1100
Epoch 142/300, trend Loss: 0.0839 | 0.1100
Epoch 143/300, trend Loss: 0.0850 | 0.1101
Epoch 144/300, trend Loss: 0.0850 | 0.1103
Epoch 145/300, trend Loss: 0.0848 | 0.1104
Epoch 146/300, trend Loss: 0.0850 | 0.1105
Epoch 147/300, trend Loss: 0.0844 | 0.1104
Epoch 148/300, trend Loss: 0.0845 | 0.1104
Epoch 149/300, trend Loss: 0.0838 | 0.1103
Epoch 150/300, trend Loss: 0.0847 | 0.1102
Epoch 151/300, trend Loss: 0.0840 | 0.1099
Epoch 152/300, trend Loss: 0.0841 | 0.1098
Epoch 153/300, trend Loss: 0.0842 | 0.1097
Epoch 154/300, trend Loss: 0.0840 | 0.1099
Epoch 155/300, trend Loss: 0.0846 | 0.1098
Epoch 156/300, trend Loss: 0.0845 | 0.1098
Epoch 157/300, trend Loss: 0.0843 | 0.1099
Epoch 158/300, trend Loss: 0.0847 | 0.1100
Epoch 159/300, trend Loss: 0.0844 | 0.1099
Epoch 160/300, trend Loss: 0.0842 | 0.1099
Epoch 161/300, trend Loss: 0.0849 | 0.1098
Epoch 162/300, trend Loss: 0.0842 | 0.1099
Epoch 163/300, trend Loss: 0.0839 | 0.1099
Epoch 164/300, trend Loss: 0.0843 | 0.1099
Epoch 165/300, trend Loss: 0.0846 | 0.1099
Epoch 166/300, trend Loss: 0.0842 | 0.1101
Epoch 167/300, trend Loss: 0.0840 | 0.1100
Epoch 168/300, trend Loss: 0.0844 | 0.1101
Epoch 169/300, trend Loss: 0.0837 | 0.1101
Epoch 170/300, trend Loss: 0.0846 | 0.1100
Epoch 171/300, trend Loss: 0.0846 | 0.1100
Epoch 172/300, trend Loss: 0.0839 | 0.1099
Epoch 173/300, trend Loss: 0.0838 | 0.1098
Epoch 174/300, trend Loss: 0.0847 | 0.1097
Epoch 175/300, trend Loss: 0.0850 | 0.1097
Epoch 176/300, trend Loss: 0.0842 | 0.1097
Epoch 177/300, trend Loss: 0.0846 | 0.1097
Epoch 178/300, trend Loss: 0.0850 | 0.1097
Epoch 179/300, trend Loss: 0.0839 | 0.1097
Epoch 180/300, trend Loss: 0.0839 | 0.1097
Epoch 181/300, trend Loss: 0.0846 | 0.1097
Epoch 182/300, trend Loss: 0.0840 | 0.1097
Epoch 183/300, trend Loss: 0.0848 | 0.1097
Epoch 184/300, trend Loss: 0.0848 | 0.1097
Epoch 185/300, trend Loss: 0.0845 | 0.1097
Epoch 186/300, trend Loss: 0.0842 | 0.1097
Epoch 187/300, trend Loss: 0.0843 | 0.1097
Epoch 188/300, trend Loss: 0.0842 | 0.1097
Epoch 189/300, trend Loss: 0.0845 | 0.1097
Epoch 190/300, trend Loss: 0.0839 | 0.1097
Epoch 191/300, trend Loss: 0.0842 | 0.1097
Epoch 192/300, trend Loss: 0.0840 | 0.1097
Epoch 193/300, trend Loss: 0.0845 | 0.1097
Epoch 194/300, trend Loss: 0.0854 | 0.1097
Epoch 195/300, trend Loss: 0.0846 | 0.1097
Epoch 196/300, trend Loss: 0.0848 | 0.1097
Epoch 197/300, trend Loss: 0.0845 | 0.1098
Epoch 198/300, trend Loss: 0.0848 | 0.1098
Epoch 199/300, trend Loss: 0.0846 | 0.1098
Epoch 200/300, trend Loss: 0.0841 | 0.1098
Epoch 201/300, trend Loss: 0.0841 | 0.1097
Epoch 202/300, trend Loss: 0.0844 | 0.1098
Epoch 203/300, trend Loss: 0.0844 | 0.1098
Epoch 204/300, trend Loss: 0.0844 | 0.1097
Epoch 205/300, trend Loss: 0.0839 | 0.1097
Epoch 206/300, trend Loss: 0.0846 | 0.1097
Epoch 207/300, trend Loss: 0.0841 | 0.1098
Epoch 208/300, trend Loss: 0.0848 | 0.1097
Epoch 209/300, trend Loss: 0.0850 | 0.1097
Epoch 210/300, trend Loss: 0.0843 | 0.1097
Epoch 211/300, trend Loss: 0.0848 | 0.1097
Epoch 212/300, trend Loss: 0.0841 | 0.1097
Epoch 213/300, trend Loss: 0.0840 | 0.1097
Epoch 214/300, trend Loss: 0.0840 | 0.1097
Epoch 215/300, trend Loss: 0.0841 | 0.1098
Epoch 216/300, trend Loss: 0.0842 | 0.1098
Epoch 217/300, trend Loss: 0.0844 | 0.1098
Epoch 218/300, trend Loss: 0.0846 | 0.1098
Epoch 219/300, trend Loss: 0.0844 | 0.1098
Epoch 220/300, trend Loss: 0.0842 | 0.1098
Epoch 221/300, trend Loss: 0.0845 | 0.1098
Epoch 222/300, trend Loss: 0.0842 | 0.1098
Epoch 223/300, trend Loss: 0.0847 | 0.1097
Epoch 224/300, trend Loss: 0.0855 | 0.1097
Epoch 225/300, trend Loss: 0.0842 | 0.1097
Epoch 226/300, trend Loss: 0.0845 | 0.1097
Epoch 227/300, trend Loss: 0.0845 | 0.1097
Epoch 228/300, trend Loss: 0.0839 | 0.1097
Epoch 229/300, trend Loss: 0.0843 | 0.1097
Epoch 230/300, trend Loss: 0.0850 | 0.1097
Epoch 231/300, trend Loss: 0.0846 | 0.1097
Epoch 232/300, trend Loss: 0.0852 | 0.1097
Epoch 233/300, trend Loss: 0.0841 | 0.1097
Epoch 234/300, trend Loss: 0.0844 | 0.1097
Epoch 235/300, trend Loss: 0.0847 | 0.1098
Epoch 236/300, trend Loss: 0.0840 | 0.1098
Epoch 237/300, trend Loss: 0.0838 | 0.1097
Epoch 238/300, trend Loss: 0.0842 | 0.1097
Epoch 239/300, trend Loss: 0.0841 | 0.1097
Epoch 240/300, trend Loss: 0.0844 | 0.1097
Epoch 241/300, trend Loss: 0.0844 | 0.1097
Epoch 242/300, trend Loss: 0.0846 | 0.1097
Epoch 243/300, trend Loss: 0.0840 | 0.1097
Epoch 244/300, trend Loss: 0.0846 | 0.1097
Epoch 245/300, trend Loss: 0.0841 | 0.1097
Epoch 246/300, trend Loss: 0.0841 | 0.1097
Epoch 247/300, trend Loss: 0.0848 | 0.1097
Epoch 248/300, trend Loss: 0.0847 | 0.1097
Epoch 249/300, trend Loss: 0.0845 | 0.1097
Epoch 250/300, trend Loss: 0.0846 | 0.1097
Epoch 251/300, trend Loss: 0.0833 | 0.1097
Epoch 252/300, trend Loss: 0.0845 | 0.1097
Epoch 253/300, trend Loss: 0.0844 | 0.1097
Epoch 254/300, trend Loss: 0.0839 | 0.1097
Epoch 255/300, trend Loss: 0.0844 | 0.1097
Epoch 256/300, trend Loss: 0.0841 | 0.1097
Epoch 257/300, trend Loss: 0.0845 | 0.1097
Epoch 258/300, trend Loss: 0.0844 | 0.1097
Epoch 259/300, trend Loss: 0.0841 | 0.1097
Epoch 260/300, trend Loss: 0.0840 | 0.1097
Epoch 261/300, trend Loss: 0.0845 | 0.1097
Epoch 262/300, trend Loss: 0.0840 | 0.1097
Epoch 263/300, trend Loss: 0.0845 | 0.1097
Epoch 264/300, trend Loss: 0.0842 | 0.1097
Epoch 265/300, trend Loss: 0.0843 | 0.1097
Epoch 266/300, trend Loss: 0.0845 | 0.1097
Epoch 267/300, trend Loss: 0.0841 | 0.1097
Epoch 268/300, trend Loss: 0.0844 | 0.1097
Epoch 269/300, trend Loss: 0.0847 | 0.1097
Epoch 270/300, trend Loss: 0.0850 | 0.1097
Epoch 271/300, trend Loss: 0.0841 | 0.1097
Epoch 272/300, trend Loss: 0.0840 | 0.1097
Epoch 273/300, trend Loss: 0.0836 | 0.1097
Epoch 274/300, trend Loss: 0.0846 | 0.1097
Epoch 275/300, trend Loss: 0.0849 | 0.1097
Epoch 276/300, trend Loss: 0.0849 | 0.1097
Epoch 277/300, trend Loss: 0.0846 | 0.1097
Epoch 278/300, trend Loss: 0.0843 | 0.1097
Epoch 279/300, trend Loss: 0.0845 | 0.1097
Epoch 280/300, trend Loss: 0.0845 | 0.1097
Epoch 281/300, trend Loss: 0.0843 | 0.1097
Epoch 282/300, trend Loss: 0.0844 | 0.1097
Epoch 283/300, trend Loss: 0.0836 | 0.1097
Epoch 284/300, trend Loss: 0.0840 | 0.1097
Epoch 285/300, trend Loss: 0.0847 | 0.1097
Epoch 286/300, trend Loss: 0.0845 | 0.1097
Epoch 287/300, trend Loss: 0.0840 | 0.1097
Epoch 288/300, trend Loss: 0.0845 | 0.1097
Epoch 289/300, trend Loss: 0.0840 | 0.1097
Epoch 290/300, trend Loss: 0.0841 | 0.1097
Epoch 291/300, trend Loss: 0.0844 | 0.1097
Epoch 292/300, trend Loss: 0.0838 | 0.1097
Epoch 293/300, trend Loss: 0.0841 | 0.1097
Epoch 294/300, trend Loss: 0.0842 | 0.1097
Epoch 295/300, trend Loss: 0.0841 | 0.1097
Epoch 296/300, trend Loss: 0.0846 | 0.1097
Epoch 297/300, trend Loss: 0.0842 | 0.1097
Epoch 298/300, trend Loss: 0.0845 | 0.1097
Epoch 299/300, trend Loss: 0.0844 | 0.1097
Epoch 300/300, trend Loss: 0.0841 | 0.1097
Training seasonal_0 component with params: {'observation_period_num': 101, 'train_rates': 0.9760620940225511, 'learning_rate': 0.00039126491326857426, 'batch_size': 203, 'step_size': 7, 'gamma': 0.9192531887938383}
Epoch 1/300, seasonal_0 Loss: 1.1722 | 1.4253
Epoch 2/300, seasonal_0 Loss: 0.9091 | 0.8141
Epoch 3/300, seasonal_0 Loss: 0.7078 | 0.7468
Epoch 4/300, seasonal_0 Loss: 0.6944 | 0.5919
Epoch 5/300, seasonal_0 Loss: 0.9371 | 1.0435
Epoch 6/300, seasonal_0 Loss: 0.5990 | 0.5546
Epoch 7/300, seasonal_0 Loss: 0.5523 | 0.5253
Epoch 8/300, seasonal_0 Loss: 0.5013 | 0.5174
Epoch 9/300, seasonal_0 Loss: 0.4435 | 0.4910
Epoch 10/300, seasonal_0 Loss: 0.4351 | 0.5221
Epoch 11/300, seasonal_0 Loss: 0.4250 | 0.4289
Epoch 12/300, seasonal_0 Loss: 0.4635 | 0.4796
Epoch 13/300, seasonal_0 Loss: 0.4106 | 0.3975
Epoch 14/300, seasonal_0 Loss: 0.3707 | 0.4035
Epoch 15/300, seasonal_0 Loss: 0.4333 | 0.5103
Epoch 16/300, seasonal_0 Loss: 0.3862 | 0.4246
Epoch 17/300, seasonal_0 Loss: 0.3670 | 0.3758
Epoch 18/300, seasonal_0 Loss: 0.3945 | 0.3783
Epoch 19/300, seasonal_0 Loss: 0.3548 | 0.3524
Epoch 20/300, seasonal_0 Loss: 0.3699 | 0.3348
Epoch 21/300, seasonal_0 Loss: 0.3291 | 0.3549
Epoch 22/300, seasonal_0 Loss: 0.2948 | 0.3256
Epoch 23/300, seasonal_0 Loss: 0.2806 | 0.3241
Epoch 24/300, seasonal_0 Loss: 0.2779 | 0.3054
Epoch 25/300, seasonal_0 Loss: 0.2865 | 0.2964
Epoch 26/300, seasonal_0 Loss: 0.2762 | 0.2890
Epoch 27/300, seasonal_0 Loss: 0.2684 | 0.2829
Epoch 28/300, seasonal_0 Loss: 0.2552 | 0.2740
Epoch 29/300, seasonal_0 Loss: 0.2485 | 0.2718
Epoch 30/300, seasonal_0 Loss: 0.2416 | 0.2593
Epoch 31/300, seasonal_0 Loss: 0.2365 | 0.2553
Epoch 32/300, seasonal_0 Loss: 0.2322 | 0.2454
Epoch 33/300, seasonal_0 Loss: 0.2295 | 0.2421
Epoch 34/300, seasonal_0 Loss: 0.2273 | 0.2346
Epoch 35/300, seasonal_0 Loss: 0.2247 | 0.2363
Epoch 36/300, seasonal_0 Loss: 0.2230 | 0.2263
Epoch 37/300, seasonal_0 Loss: 0.2202 | 0.2288
Epoch 38/300, seasonal_0 Loss: 0.2163 | 0.2206
Epoch 39/300, seasonal_0 Loss: 0.2161 | 0.2166
Epoch 40/300, seasonal_0 Loss: 0.2156 | 0.2199
Epoch 41/300, seasonal_0 Loss: 0.2177 | 0.2138
Epoch 42/300, seasonal_0 Loss: 0.2142 | 0.2175
Epoch 43/300, seasonal_0 Loss: 0.2117 | 0.2098
Epoch 44/300, seasonal_0 Loss: 0.2077 | 0.2111
Epoch 45/300, seasonal_0 Loss: 0.2047 | 0.2011
Epoch 46/300, seasonal_0 Loss: 0.2016 | 0.2021
Epoch 47/300, seasonal_0 Loss: 0.2004 | 0.1988
Epoch 48/300, seasonal_0 Loss: 0.2000 | 0.1959
Epoch 49/300, seasonal_0 Loss: 0.1980 | 0.1971
Epoch 50/300, seasonal_0 Loss: 0.1969 | 0.1927
Epoch 51/300, seasonal_0 Loss: 0.1946 | 0.1928
Epoch 52/300, seasonal_0 Loss: 0.1923 | 0.1883
Epoch 53/300, seasonal_0 Loss: 0.1907 | 0.1884
Epoch 54/300, seasonal_0 Loss: 0.1889 | 0.1859
Epoch 55/300, seasonal_0 Loss: 0.1879 | 0.1837
Epoch 56/300, seasonal_0 Loss: 0.1873 | 0.1851
Epoch 57/300, seasonal_0 Loss: 0.1858 | 0.1807
Epoch 58/300, seasonal_0 Loss: 0.1851 | 0.1835
Epoch 59/300, seasonal_0 Loss: 0.1834 | 0.1796
Epoch 60/300, seasonal_0 Loss: 0.1823 | 0.1782
Epoch 61/300, seasonal_0 Loss: 0.1815 | 0.1777
Epoch 62/300, seasonal_0 Loss: 0.1802 | 0.1766
Epoch 63/300, seasonal_0 Loss: 0.1800 | 0.1762
Epoch 64/300, seasonal_0 Loss: 0.1789 | 0.1743
Epoch 65/300, seasonal_0 Loss: 0.1788 | 0.1730
Epoch 66/300, seasonal_0 Loss: 0.1770 | 0.1729
Epoch 67/300, seasonal_0 Loss: 0.1761 | 0.1710
Epoch 68/300, seasonal_0 Loss: 0.1759 | 0.1712
Epoch 69/300, seasonal_0 Loss: 0.1753 | 0.1689
Epoch 70/300, seasonal_0 Loss: 0.1744 | 0.1701
Epoch 71/300, seasonal_0 Loss: 0.1734 | 0.1678
Epoch 72/300, seasonal_0 Loss: 0.1732 | 0.1674
Epoch 73/300, seasonal_0 Loss: 0.1718 | 0.1669
Epoch 74/300, seasonal_0 Loss: 0.1717 | 0.1658
Epoch 75/300, seasonal_0 Loss: 0.1717 | 0.1642
Epoch 76/300, seasonal_0 Loss: 0.1703 | 0.1648
Epoch 77/300, seasonal_0 Loss: 0.1698 | 0.1640
Epoch 78/300, seasonal_0 Loss: 0.1685 | 0.1632
Epoch 79/300, seasonal_0 Loss: 0.1690 | 0.1622
Epoch 80/300, seasonal_0 Loss: 0.1679 | 0.1612
Epoch 81/300, seasonal_0 Loss: 0.1675 | 0.1606
Epoch 82/300, seasonal_0 Loss: 0.1665 | 0.1608
Epoch 83/300, seasonal_0 Loss: 0.1668 | 0.1593
Epoch 84/300, seasonal_0 Loss: 0.1663 | 0.1589
Epoch 85/300, seasonal_0 Loss: 0.1651 | 0.1580
Epoch 86/300, seasonal_0 Loss: 0.1654 | 0.1576
Epoch 87/300, seasonal_0 Loss: 0.1648 | 0.1573
Epoch 88/300, seasonal_0 Loss: 0.1641 | 0.1573
Epoch 89/300, seasonal_0 Loss: 0.1632 | 0.1571
Epoch 90/300, seasonal_0 Loss: 0.1633 | 0.1568
Epoch 91/300, seasonal_0 Loss: 0.1629 | 0.1557
Epoch 92/300, seasonal_0 Loss: 0.1629 | 0.1548
Epoch 93/300, seasonal_0 Loss: 0.1620 | 0.1541
Epoch 94/300, seasonal_0 Loss: 0.1612 | 0.1550
Epoch 95/300, seasonal_0 Loss: 0.1614 | 0.1541
Epoch 96/300, seasonal_0 Loss: 0.1608 | 0.1538
Epoch 97/300, seasonal_0 Loss: 0.1599 | 0.1527
Epoch 98/300, seasonal_0 Loss: 0.1607 | 0.1529
Epoch 99/300, seasonal_0 Loss: 0.1601 | 0.1525
Epoch 100/300, seasonal_0 Loss: 0.1587 | 0.1524
Epoch 101/300, seasonal_0 Loss: 0.1578 | 0.1515
Epoch 102/300, seasonal_0 Loss: 0.1581 | 0.1509
Epoch 103/300, seasonal_0 Loss: 0.1583 | 0.1509
Epoch 104/300, seasonal_0 Loss: 0.1576 | 0.1512
Epoch 105/300, seasonal_0 Loss: 0.1577 | 0.1505
Epoch 106/300, seasonal_0 Loss: 0.1582 | 0.1502
Epoch 107/300, seasonal_0 Loss: 0.1566 | 0.1499
Epoch 108/300, seasonal_0 Loss: 0.1558 | 0.1492
Epoch 109/300, seasonal_0 Loss: 0.1558 | 0.1490
Epoch 110/300, seasonal_0 Loss: 0.1559 | 0.1487
Epoch 111/300, seasonal_0 Loss: 0.1558 | 0.1483
Epoch 112/300, seasonal_0 Loss: 0.1557 | 0.1484
Epoch 113/300, seasonal_0 Loss: 0.1549 | 0.1475
Epoch 114/300, seasonal_0 Loss: 0.1544 | 0.1476
Epoch 115/300, seasonal_0 Loss: 0.1542 | 0.1474
Epoch 116/300, seasonal_0 Loss: 0.1547 | 0.1471
Epoch 117/300, seasonal_0 Loss: 0.1543 | 0.1470
Epoch 118/300, seasonal_0 Loss: 0.1535 | 0.1471
Epoch 119/300, seasonal_0 Loss: 0.1534 | 0.1469
Epoch 120/300, seasonal_0 Loss: 0.1533 | 0.1465
Epoch 121/300, seasonal_0 Loss: 0.1534 | 0.1460
Epoch 122/300, seasonal_0 Loss: 0.1524 | 0.1462
Epoch 123/300, seasonal_0 Loss: 0.1534 | 0.1458
Epoch 124/300, seasonal_0 Loss: 0.1524 | 0.1453
Epoch 125/300, seasonal_0 Loss: 0.1524 | 0.1452
Epoch 126/300, seasonal_0 Loss: 0.1525 | 0.1451
Epoch 127/300, seasonal_0 Loss: 0.1525 | 0.1453
Epoch 128/300, seasonal_0 Loss: 0.1518 | 0.1448
Epoch 129/300, seasonal_0 Loss: 0.1519 | 0.1448
Epoch 130/300, seasonal_0 Loss: 0.1516 | 0.1443
Epoch 131/300, seasonal_0 Loss: 0.1506 | 0.1442
Epoch 132/300, seasonal_0 Loss: 0.1516 | 0.1443
Epoch 133/300, seasonal_0 Loss: 0.1513 | 0.1440
Epoch 134/300, seasonal_0 Loss: 0.1509 | 0.1439
Epoch 135/300, seasonal_0 Loss: 0.1514 | 0.1442
Epoch 136/300, seasonal_0 Loss: 0.1501 | 0.1438
Epoch 137/300, seasonal_0 Loss: 0.1500 | 0.1436
Epoch 138/300, seasonal_0 Loss: 0.1506 | 0.1431
Epoch 139/300, seasonal_0 Loss: 0.1498 | 0.1431
Epoch 140/300, seasonal_0 Loss: 0.1503 | 0.1428
Epoch 141/300, seasonal_0 Loss: 0.1501 | 0.1426
Epoch 142/300, seasonal_0 Loss: 0.1499 | 0.1425
Epoch 143/300, seasonal_0 Loss: 0.1494 | 0.1420
Epoch 144/300, seasonal_0 Loss: 0.1497 | 0.1418
Epoch 145/300, seasonal_0 Loss: 0.1497 | 0.1417
Epoch 146/300, seasonal_0 Loss: 0.1498 | 0.1414
Epoch 147/300, seasonal_0 Loss: 0.1483 | 0.1418
Epoch 148/300, seasonal_0 Loss: 0.1493 | 0.1419
Epoch 149/300, seasonal_0 Loss: 0.1486 | 0.1418
Epoch 150/300, seasonal_0 Loss: 0.1484 | 0.1416
Epoch 151/300, seasonal_0 Loss: 0.1489 | 0.1412
Epoch 152/300, seasonal_0 Loss: 0.1485 | 0.1412
Epoch 153/300, seasonal_0 Loss: 0.1479 | 0.1411
Epoch 154/300, seasonal_0 Loss: 0.1489 | 0.1411
Epoch 155/300, seasonal_0 Loss: 0.1482 | 0.1410
Epoch 156/300, seasonal_0 Loss: 0.1480 | 0.1410
Epoch 157/300, seasonal_0 Loss: 0.1484 | 0.1408
Epoch 158/300, seasonal_0 Loss: 0.1487 | 0.1406
Epoch 159/300, seasonal_0 Loss: 0.1475 | 0.1405
Epoch 160/300, seasonal_0 Loss: 0.1477 | 0.1404
Epoch 161/300, seasonal_0 Loss: 0.1473 | 0.1404
Epoch 162/300, seasonal_0 Loss: 0.1478 | 0.1406
Epoch 163/300, seasonal_0 Loss: 0.1476 | 0.1404
Epoch 164/300, seasonal_0 Loss: 0.1466 | 0.1403
Epoch 165/300, seasonal_0 Loss: 0.1467 | 0.1399
Epoch 166/300, seasonal_0 Loss: 0.1476 | 0.1397
Epoch 167/300, seasonal_0 Loss: 0.1473 | 0.1396
Epoch 168/300, seasonal_0 Loss: 0.1472 | 0.1395
Epoch 169/300, seasonal_0 Loss: 0.1472 | 0.1396
Epoch 170/300, seasonal_0 Loss: 0.1470 | 0.1397
Epoch 171/300, seasonal_0 Loss: 0.1469 | 0.1396
Epoch 172/300, seasonal_0 Loss: 0.1465 | 0.1395
Epoch 173/300, seasonal_0 Loss: 0.1469 | 0.1394
Epoch 174/300, seasonal_0 Loss: 0.1469 | 0.1395
Epoch 175/300, seasonal_0 Loss: 0.1463 | 0.1393
Epoch 176/300, seasonal_0 Loss: 0.1469 | 0.1392
Epoch 177/300, seasonal_0 Loss: 0.1464 | 0.1392
Epoch 178/300, seasonal_0 Loss: 0.1464 | 0.1393
Epoch 179/300, seasonal_0 Loss: 0.1463 | 0.1395
Epoch 180/300, seasonal_0 Loss: 0.1458 | 0.1395
Epoch 181/300, seasonal_0 Loss: 0.1463 | 0.1393
Epoch 182/300, seasonal_0 Loss: 0.1457 | 0.1391
Epoch 183/300, seasonal_0 Loss: 0.1463 | 0.1389
Epoch 184/300, seasonal_0 Loss: 0.1456 | 0.1388
Epoch 185/300, seasonal_0 Loss: 0.1466 | 0.1389
Epoch 186/300, seasonal_0 Loss: 0.1467 | 0.1389
Epoch 187/300, seasonal_0 Loss: 0.1464 | 0.1390
Epoch 188/300, seasonal_0 Loss: 0.1461 | 0.1389
Epoch 189/300, seasonal_0 Loss: 0.1452 | 0.1388
Epoch 190/300, seasonal_0 Loss: 0.1458 | 0.1385
Epoch 191/300, seasonal_0 Loss: 0.1459 | 0.1384
Epoch 192/300, seasonal_0 Loss: 0.1449 | 0.1384
Epoch 193/300, seasonal_0 Loss: 0.1457 | 0.1385
Epoch 194/300, seasonal_0 Loss: 0.1447 | 0.1385
Epoch 195/300, seasonal_0 Loss: 0.1460 | 0.1383
Epoch 196/300, seasonal_0 Loss: 0.1453 | 0.1382
Epoch 197/300, seasonal_0 Loss: 0.1448 | 0.1383
Epoch 198/300, seasonal_0 Loss: 0.1445 | 0.1383
Epoch 199/300, seasonal_0 Loss: 0.1453 | 0.1382
Epoch 200/300, seasonal_0 Loss: 0.1449 | 0.1381
Epoch 201/300, seasonal_0 Loss: 0.1454 | 0.1380
Epoch 202/300, seasonal_0 Loss: 0.1454 | 0.1380
Epoch 203/300, seasonal_0 Loss: 0.1454 | 0.1381
Epoch 204/300, seasonal_0 Loss: 0.1450 | 0.1382
Epoch 205/300, seasonal_0 Loss: 0.1442 | 0.1381
Epoch 206/300, seasonal_0 Loss: 0.1447 | 0.1379
Epoch 207/300, seasonal_0 Loss: 0.1449 | 0.1380
Epoch 208/300, seasonal_0 Loss: 0.1446 | 0.1380
Epoch 209/300, seasonal_0 Loss: 0.1447 | 0.1379
Epoch 210/300, seasonal_0 Loss: 0.1440 | 0.1379
Epoch 211/300, seasonal_0 Loss: 0.1441 | 0.1379
Epoch 212/300, seasonal_0 Loss: 0.1449 | 0.1379
Epoch 213/300, seasonal_0 Loss: 0.1445 | 0.1379
Epoch 214/300, seasonal_0 Loss: 0.1458 | 0.1378
Epoch 215/300, seasonal_0 Loss: 0.1450 | 0.1378
Epoch 216/300, seasonal_0 Loss: 0.1446 | 0.1377
Epoch 217/300, seasonal_0 Loss: 0.1445 | 0.1377
Epoch 218/300, seasonal_0 Loss: 0.1447 | 0.1376
Epoch 219/300, seasonal_0 Loss: 0.1444 | 0.1377
Epoch 220/300, seasonal_0 Loss: 0.1444 | 0.1377
Epoch 221/300, seasonal_0 Loss: 0.1447 | 0.1376
Epoch 222/300, seasonal_0 Loss: 0.1444 | 0.1376
Epoch 223/300, seasonal_0 Loss: 0.1443 | 0.1376
Epoch 224/300, seasonal_0 Loss: 0.1445 | 0.1375
Epoch 225/300, seasonal_0 Loss: 0.1444 | 0.1374
Epoch 226/300, seasonal_0 Loss: 0.1441 | 0.1374
Epoch 227/300, seasonal_0 Loss: 0.1445 | 0.1374
Epoch 228/300, seasonal_0 Loss: 0.1442 | 0.1374
Epoch 229/300, seasonal_0 Loss: 0.1447 | 0.1374
Epoch 230/300, seasonal_0 Loss: 0.1436 | 0.1374
Epoch 231/300, seasonal_0 Loss: 0.1441 | 0.1374
Epoch 232/300, seasonal_0 Loss: 0.1446 | 0.1374
Epoch 233/300, seasonal_0 Loss: 0.1445 | 0.1374
Epoch 234/300, seasonal_0 Loss: 0.1442 | 0.1374
Epoch 235/300, seasonal_0 Loss: 0.1437 | 0.1374
Epoch 236/300, seasonal_0 Loss: 0.1432 | 0.1374
Epoch 237/300, seasonal_0 Loss: 0.1438 | 0.1373
Epoch 238/300, seasonal_0 Loss: 0.1439 | 0.1372
Epoch 239/300, seasonal_0 Loss: 0.1437 | 0.1371
Epoch 240/300, seasonal_0 Loss: 0.1435 | 0.1371
Epoch 241/300, seasonal_0 Loss: 0.1439 | 0.1372
Epoch 242/300, seasonal_0 Loss: 0.1445 | 0.1372
Epoch 243/300, seasonal_0 Loss: 0.1436 | 0.1371
Epoch 244/300, seasonal_0 Loss: 0.1440 | 0.1371
Epoch 245/300, seasonal_0 Loss: 0.1436 | 0.1371
Epoch 246/300, seasonal_0 Loss: 0.1440 | 0.1371
Epoch 247/300, seasonal_0 Loss: 0.1444 | 0.1371
Epoch 248/300, seasonal_0 Loss: 0.1438 | 0.1371
Epoch 249/300, seasonal_0 Loss: 0.1436 | 0.1371
Epoch 250/300, seasonal_0 Loss: 0.1443 | 0.1371
Epoch 251/300, seasonal_0 Loss: 0.1441 | 0.1371
Epoch 252/300, seasonal_0 Loss: 0.1436 | 0.1370
Epoch 253/300, seasonal_0 Loss: 0.1438 | 0.1370
Epoch 254/300, seasonal_0 Loss: 0.1439 | 0.1370
Epoch 255/300, seasonal_0 Loss: 0.1437 | 0.1370
Epoch 256/300, seasonal_0 Loss: 0.1435 | 0.1371
Epoch 257/300, seasonal_0 Loss: 0.1438 | 0.1371
Epoch 258/300, seasonal_0 Loss: 0.1443 | 0.1371
Epoch 259/300, seasonal_0 Loss: 0.1440 | 0.1370
Epoch 260/300, seasonal_0 Loss: 0.1431 | 0.1370
Epoch 261/300, seasonal_0 Loss: 0.1440 | 0.1370
Epoch 262/300, seasonal_0 Loss: 0.1436 | 0.1370
Epoch 263/300, seasonal_0 Loss: 0.1438 | 0.1370
Epoch 264/300, seasonal_0 Loss: 0.1442 | 0.1370
Epoch 265/300, seasonal_0 Loss: 0.1432 | 0.1370
Epoch 266/300, seasonal_0 Loss: 0.1444 | 0.1369
Epoch 267/300, seasonal_0 Loss: 0.1436 | 0.1370
Epoch 268/300, seasonal_0 Loss: 0.1431 | 0.1370
Epoch 269/300, seasonal_0 Loss: 0.1445 | 0.1370
Epoch 270/300, seasonal_0 Loss: 0.1435 | 0.1370
Epoch 271/300, seasonal_0 Loss: 0.1433 | 0.1370
Epoch 272/300, seasonal_0 Loss: 0.1433 | 0.1369
Epoch 273/300, seasonal_0 Loss: 0.1434 | 0.1369
Epoch 274/300, seasonal_0 Loss: 0.1437 | 0.1369
Epoch 275/300, seasonal_0 Loss: 0.1435 | 0.1369
Epoch 276/300, seasonal_0 Loss: 0.1442 | 0.1369
Epoch 277/300, seasonal_0 Loss: 0.1439 | 0.1368
Epoch 278/300, seasonal_0 Loss: 0.1435 | 0.1368
Epoch 279/300, seasonal_0 Loss: 0.1436 | 0.1368
Epoch 280/300, seasonal_0 Loss: 0.1434 | 0.1368
Epoch 281/300, seasonal_0 Loss: 0.1433 | 0.1368
Epoch 282/300, seasonal_0 Loss: 0.1435 | 0.1368
Epoch 283/300, seasonal_0 Loss: 0.1432 | 0.1367
Epoch 284/300, seasonal_0 Loss: 0.1434 | 0.1367
Epoch 285/300, seasonal_0 Loss: 0.1437 | 0.1367
Epoch 286/300, seasonal_0 Loss: 0.1438 | 0.1367
Epoch 287/300, seasonal_0 Loss: 0.1441 | 0.1367
Epoch 288/300, seasonal_0 Loss: 0.1427 | 0.1367
Epoch 289/300, seasonal_0 Loss: 0.1437 | 0.1367
Epoch 290/300, seasonal_0 Loss: 0.1438 | 0.1367
Epoch 291/300, seasonal_0 Loss: 0.1431 | 0.1366
Epoch 292/300, seasonal_0 Loss: 0.1436 | 0.1366
Epoch 293/300, seasonal_0 Loss: 0.1436 | 0.1366
Epoch 294/300, seasonal_0 Loss: 0.1435 | 0.1366
Epoch 295/300, seasonal_0 Loss: 0.1436 | 0.1366
Epoch 296/300, seasonal_0 Loss: 0.1436 | 0.1366
Epoch 297/300, seasonal_0 Loss: 0.1433 | 0.1366
Epoch 298/300, seasonal_0 Loss: 0.1429 | 0.1366
Epoch 299/300, seasonal_0 Loss: 0.1433 | 0.1366
Epoch 300/300, seasonal_0 Loss: 0.1439 | 0.1366
Training seasonal_1 component with params: {'observation_period_num': 92, 'train_rates': 0.9751189159989856, 'learning_rate': 0.00020580345409222357, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8590511584912979}
Epoch 1/300, seasonal_1 Loss: 1.0496 | 0.9564
Epoch 2/300, seasonal_1 Loss: 0.8184 | 0.6621
Epoch 3/300, seasonal_1 Loss: 0.6748 | 0.6104
Epoch 4/300, seasonal_1 Loss: 0.6005 | 0.4660
Epoch 5/300, seasonal_1 Loss: 0.5167 | 0.4876
Epoch 6/300, seasonal_1 Loss: 0.4840 | 0.4048
Epoch 7/300, seasonal_1 Loss: 0.4794 | 0.4883
Epoch 8/300, seasonal_1 Loss: 0.4796 | 0.4229
Epoch 9/300, seasonal_1 Loss: 0.4938 | 0.4770
Epoch 10/300, seasonal_1 Loss: 0.4417 | 0.3676
Epoch 11/300, seasonal_1 Loss: 0.5331 | 0.5210
Epoch 12/300, seasonal_1 Loss: 0.4667 | 0.3798
Epoch 13/300, seasonal_1 Loss: 0.4172 | 0.3328
Epoch 14/300, seasonal_1 Loss: 0.4024 | 0.3284
Epoch 15/300, seasonal_1 Loss: 0.3443 | 0.2929
Epoch 16/300, seasonal_1 Loss: 0.3331 | 0.2875
Epoch 17/300, seasonal_1 Loss: 0.3226 | 0.2777
Epoch 18/300, seasonal_1 Loss: 0.2988 | 0.2528
Epoch 19/300, seasonal_1 Loss: 0.2988 | 0.2601
Epoch 20/300, seasonal_1 Loss: 0.2895 | 0.2421
Epoch 21/300, seasonal_1 Loss: 0.2900 | 0.2595
Epoch 22/300, seasonal_1 Loss: 0.2706 | 0.2312
Epoch 23/300, seasonal_1 Loss: 0.2615 | 0.2337
Epoch 24/300, seasonal_1 Loss: 0.2577 | 0.2245
Epoch 25/300, seasonal_1 Loss: 0.2503 | 0.2165
Epoch 26/300, seasonal_1 Loss: 0.2485 | 0.2171
Epoch 27/300, seasonal_1 Loss: 0.2412 | 0.2109
Epoch 28/300, seasonal_1 Loss: 0.2380 | 0.2065
Epoch 29/300, seasonal_1 Loss: 0.2358 | 0.2070
Epoch 30/300, seasonal_1 Loss: 0.2323 | 0.1991
Epoch 31/300, seasonal_1 Loss: 0.2298 | 0.2010
Epoch 32/300, seasonal_1 Loss: 0.2260 | 0.1961
Epoch 33/300, seasonal_1 Loss: 0.2238 | 0.1948
Epoch 34/300, seasonal_1 Loss: 0.2221 | 0.1954
Epoch 35/300, seasonal_1 Loss: 0.2198 | 0.1905
Epoch 36/300, seasonal_1 Loss: 0.2192 | 0.1904
Epoch 37/300, seasonal_1 Loss: 0.2164 | 0.1884
Epoch 38/300, seasonal_1 Loss: 0.2152 | 0.1855
Epoch 39/300, seasonal_1 Loss: 0.2139 | 0.1856
Epoch 40/300, seasonal_1 Loss: 0.2112 | 0.1827
Epoch 41/300, seasonal_1 Loss: 0.2107 | 0.1818
Epoch 42/300, seasonal_1 Loss: 0.2085 | 0.1808
Epoch 43/300, seasonal_1 Loss: 0.2084 | 0.1796
Epoch 44/300, seasonal_1 Loss: 0.2073 | 0.1789
Epoch 45/300, seasonal_1 Loss: 0.2065 | 0.1779
Epoch 46/300, seasonal_1 Loss: 0.2048 | 0.1770
Epoch 47/300, seasonal_1 Loss: 0.2038 | 0.1764
Epoch 48/300, seasonal_1 Loss: 0.2031 | 0.1749
Epoch 49/300, seasonal_1 Loss: 0.2019 | 0.1752
Epoch 50/300, seasonal_1 Loss: 0.2017 | 0.1738
Epoch 51/300, seasonal_1 Loss: 0.2000 | 0.1738
Epoch 52/300, seasonal_1 Loss: 0.1997 | 0.1726
Epoch 53/300, seasonal_1 Loss: 0.1983 | 0.1720
Epoch 54/300, seasonal_1 Loss: 0.1977 | 0.1719
Epoch 55/300, seasonal_1 Loss: 0.1967 | 0.1714
Epoch 56/300, seasonal_1 Loss: 0.1966 | 0.1717
Epoch 57/300, seasonal_1 Loss: 0.1954 | 0.1708
Epoch 58/300, seasonal_1 Loss: 0.1945 | 0.1692
Epoch 59/300, seasonal_1 Loss: 0.1946 | 0.1694
Epoch 60/300, seasonal_1 Loss: 0.1931 | 0.1696
Epoch 61/300, seasonal_1 Loss: 0.1934 | 0.1690
Epoch 62/300, seasonal_1 Loss: 0.1922 | 0.1688
Epoch 63/300, seasonal_1 Loss: 0.1936 | 0.1678
Epoch 64/300, seasonal_1 Loss: 0.1916 | 0.1674
Epoch 65/300, seasonal_1 Loss: 0.1906 | 0.1670
Epoch 66/300, seasonal_1 Loss: 0.1905 | 0.1665
Epoch 67/300, seasonal_1 Loss: 0.1902 | 0.1664
Epoch 68/300, seasonal_1 Loss: 0.1900 | 0.1662
Epoch 69/300, seasonal_1 Loss: 0.1886 | 0.1650
Epoch 70/300, seasonal_1 Loss: 0.1895 | 0.1646
Epoch 71/300, seasonal_1 Loss: 0.1884 | 0.1645
Epoch 72/300, seasonal_1 Loss: 0.1880 | 0.1650
Epoch 73/300, seasonal_1 Loss: 0.1878 | 0.1643
Epoch 74/300, seasonal_1 Loss: 0.1874 | 0.1637
Epoch 75/300, seasonal_1 Loss: 0.1867 | 0.1639
Epoch 76/300, seasonal_1 Loss: 0.1870 | 0.1636
Epoch 77/300, seasonal_1 Loss: 0.1856 | 0.1634
Epoch 78/300, seasonal_1 Loss: 0.1868 | 0.1633
Epoch 79/300, seasonal_1 Loss: 0.1858 | 0.1626
Epoch 80/300, seasonal_1 Loss: 0.1863 | 0.1629
Epoch 81/300, seasonal_1 Loss: 0.1861 | 0.1624
Epoch 82/300, seasonal_1 Loss: 0.1861 | 0.1623
Epoch 83/300, seasonal_1 Loss: 0.1859 | 0.1621
Epoch 84/300, seasonal_1 Loss: 0.1841 | 0.1622
Epoch 85/300, seasonal_1 Loss: 0.1843 | 0.1617
Epoch 86/300, seasonal_1 Loss: 0.1844 | 0.1613
Epoch 87/300, seasonal_1 Loss: 0.1840 | 0.1610
Epoch 88/300, seasonal_1 Loss: 0.1838 | 0.1606
Epoch 89/300, seasonal_1 Loss: 0.1832 | 0.1606
Epoch 90/300, seasonal_1 Loss: 0.1838 | 0.1607
Epoch 91/300, seasonal_1 Loss: 0.1832 | 0.1606
Epoch 92/300, seasonal_1 Loss: 0.1827 | 0.1605
Epoch 93/300, seasonal_1 Loss: 0.1824 | 0.1607
Epoch 94/300, seasonal_1 Loss: 0.1829 | 0.1604
Epoch 95/300, seasonal_1 Loss: 0.1824 | 0.1604
Epoch 96/300, seasonal_1 Loss: 0.1831 | 0.1603
Epoch 97/300, seasonal_1 Loss: 0.1822 | 0.1602
Epoch 98/300, seasonal_1 Loss: 0.1830 | 0.1597
Epoch 99/300, seasonal_1 Loss: 0.1814 | 0.1599
Epoch 100/300, seasonal_1 Loss: 0.1816 | 0.1599
Epoch 101/300, seasonal_1 Loss: 0.1811 | 0.1598
Epoch 102/300, seasonal_1 Loss: 0.1816 | 0.1597
Epoch 103/300, seasonal_1 Loss: 0.1809 | 0.1595
Epoch 104/300, seasonal_1 Loss: 0.1812 | 0.1594
Epoch 105/300, seasonal_1 Loss: 0.1808 | 0.1593
Epoch 106/300, seasonal_1 Loss: 0.1806 | 0.1590
Epoch 107/300, seasonal_1 Loss: 0.1807 | 0.1590
Epoch 108/300, seasonal_1 Loss: 0.1805 | 0.1590
Epoch 109/300, seasonal_1 Loss: 0.1801 | 0.1591
Epoch 110/300, seasonal_1 Loss: 0.1802 | 0.1590
Epoch 111/300, seasonal_1 Loss: 0.1813 | 0.1587
Epoch 112/300, seasonal_1 Loss: 0.1796 | 0.1587
Epoch 113/300, seasonal_1 Loss: 0.1802 | 0.1589
Epoch 114/300, seasonal_1 Loss: 0.1806 | 0.1587
Epoch 115/300, seasonal_1 Loss: 0.1806 | 0.1587
Epoch 116/300, seasonal_1 Loss: 0.1798 | 0.1588
Epoch 117/300, seasonal_1 Loss: 0.1796 | 0.1587
Epoch 118/300, seasonal_1 Loss: 0.1792 | 0.1586
Epoch 119/300, seasonal_1 Loss: 0.1809 | 0.1586
Epoch 120/300, seasonal_1 Loss: 0.1791 | 0.1586
Epoch 121/300, seasonal_1 Loss: 0.1793 | 0.1587
Epoch 122/300, seasonal_1 Loss: 0.1793 | 0.1586
Epoch 123/300, seasonal_1 Loss: 0.1795 | 0.1584
Epoch 124/300, seasonal_1 Loss: 0.1795 | 0.1581
Epoch 125/300, seasonal_1 Loss: 0.1795 | 0.1583
Epoch 126/300, seasonal_1 Loss: 0.1796 | 0.1582
Epoch 127/300, seasonal_1 Loss: 0.1789 | 0.1582
Epoch 128/300, seasonal_1 Loss: 0.1794 | 0.1582
Epoch 129/300, seasonal_1 Loss: 0.1784 | 0.1582
Epoch 130/300, seasonal_1 Loss: 0.1798 | 0.1580
Epoch 131/300, seasonal_1 Loss: 0.1795 | 0.1580
Epoch 132/300, seasonal_1 Loss: 0.1791 | 0.1580
Epoch 133/300, seasonal_1 Loss: 0.1789 | 0.1580
Epoch 134/300, seasonal_1 Loss: 0.1789 | 0.1581
Epoch 135/300, seasonal_1 Loss: 0.1800 | 0.1580
Epoch 136/300, seasonal_1 Loss: 0.1785 | 0.1579
Epoch 137/300, seasonal_1 Loss: 0.1789 | 0.1578
Epoch 138/300, seasonal_1 Loss: 0.1787 | 0.1578
Epoch 139/300, seasonal_1 Loss: 0.1791 | 0.1579
Epoch 140/300, seasonal_1 Loss: 0.1788 | 0.1579
Epoch 141/300, seasonal_1 Loss: 0.1781 | 0.1579
Epoch 142/300, seasonal_1 Loss: 0.1781 | 0.1578
Epoch 143/300, seasonal_1 Loss: 0.1791 | 0.1579
Epoch 144/300, seasonal_1 Loss: 0.1785 | 0.1579
Epoch 145/300, seasonal_1 Loss: 0.1780 | 0.1578
Epoch 146/300, seasonal_1 Loss: 0.1787 | 0.1578
Epoch 147/300, seasonal_1 Loss: 0.1782 | 0.1577
Epoch 148/300, seasonal_1 Loss: 0.1784 | 0.1576
Epoch 149/300, seasonal_1 Loss: 0.1780 | 0.1575
Epoch 150/300, seasonal_1 Loss: 0.1790 | 0.1575
Epoch 151/300, seasonal_1 Loss: 0.1782 | 0.1576
Epoch 152/300, seasonal_1 Loss: 0.1776 | 0.1576
Epoch 153/300, seasonal_1 Loss: 0.1792 | 0.1575
Epoch 154/300, seasonal_1 Loss: 0.1777 | 0.1576
Epoch 155/300, seasonal_1 Loss: 0.1781 | 0.1576
Epoch 156/300, seasonal_1 Loss: 0.1784 | 0.1577
Epoch 157/300, seasonal_1 Loss: 0.1782 | 0.1577
Epoch 158/300, seasonal_1 Loss: 0.1780 | 0.1576
Epoch 159/300, seasonal_1 Loss: 0.1786 | 0.1576
Epoch 160/300, seasonal_1 Loss: 0.1784 | 0.1576
Epoch 161/300, seasonal_1 Loss: 0.1783 | 0.1575
Epoch 162/300, seasonal_1 Loss: 0.1777 | 0.1575
Epoch 163/300, seasonal_1 Loss: 0.1781 | 0.1575
Epoch 164/300, seasonal_1 Loss: 0.1777 | 0.1574
Epoch 165/300, seasonal_1 Loss: 0.1781 | 0.1574
Epoch 166/300, seasonal_1 Loss: 0.1781 | 0.1574
Epoch 167/300, seasonal_1 Loss: 0.1783 | 0.1574
Epoch 168/300, seasonal_1 Loss: 0.1783 | 0.1573
Epoch 169/300, seasonal_1 Loss: 0.1782 | 0.1573
Epoch 170/300, seasonal_1 Loss: 0.1788 | 0.1574
Epoch 171/300, seasonal_1 Loss: 0.1775 | 0.1573
Epoch 172/300, seasonal_1 Loss: 0.1776 | 0.1573
Epoch 173/300, seasonal_1 Loss: 0.1779 | 0.1573
Epoch 174/300, seasonal_1 Loss: 0.1777 | 0.1573
Epoch 175/300, seasonal_1 Loss: 0.1777 | 0.1573
Epoch 176/300, seasonal_1 Loss: 0.1778 | 0.1573
Epoch 177/300, seasonal_1 Loss: 0.1775 | 0.1574
Epoch 178/300, seasonal_1 Loss: 0.1779 | 0.1574
Epoch 179/300, seasonal_1 Loss: 0.1773 | 0.1574
Epoch 180/300, seasonal_1 Loss: 0.1777 | 0.1574
Epoch 181/300, seasonal_1 Loss: 0.1789 | 0.1574
Epoch 182/300, seasonal_1 Loss: 0.1779 | 0.1574
Epoch 183/300, seasonal_1 Loss: 0.1784 | 0.1574
Epoch 184/300, seasonal_1 Loss: 0.1776 | 0.1574
Epoch 185/300, seasonal_1 Loss: 0.1787 | 0.1573
Epoch 186/300, seasonal_1 Loss: 0.1775 | 0.1573
Epoch 187/300, seasonal_1 Loss: 0.1785 | 0.1573
Epoch 188/300, seasonal_1 Loss: 0.1780 | 0.1573
Epoch 189/300, seasonal_1 Loss: 0.1778 | 0.1573
Epoch 190/300, seasonal_1 Loss: 0.1776 | 0.1573
Epoch 191/300, seasonal_1 Loss: 0.1780 | 0.1573
Epoch 192/300, seasonal_1 Loss: 0.1771 | 0.1573
Epoch 193/300, seasonal_1 Loss: 0.1770 | 0.1573
Epoch 194/300, seasonal_1 Loss: 0.1776 | 0.1573
Epoch 195/300, seasonal_1 Loss: 0.1783 | 0.1573
Epoch 196/300, seasonal_1 Loss: 0.1778 | 0.1573
Epoch 197/300, seasonal_1 Loss: 0.1774 | 0.1573
Epoch 198/300, seasonal_1 Loss: 0.1772 | 0.1573
Epoch 199/300, seasonal_1 Loss: 0.1772 | 0.1573
Epoch 200/300, seasonal_1 Loss: 0.1776 | 0.1573
Epoch 201/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 202/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 203/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 204/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 205/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 206/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 207/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 208/300, seasonal_1 Loss: 0.1783 | 0.1572
Epoch 209/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 210/300, seasonal_1 Loss: 0.1773 | 0.1572
Epoch 211/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 212/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 213/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 214/300, seasonal_1 Loss: 0.1783 | 0.1572
Epoch 215/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 216/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 217/300, seasonal_1 Loss: 0.1767 | 0.1572
Epoch 218/300, seasonal_1 Loss: 0.1782 | 0.1572
Epoch 219/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 220/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 221/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 222/300, seasonal_1 Loss: 0.1787 | 0.1572
Epoch 223/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 224/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 225/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 226/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 227/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 228/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 229/300, seasonal_1 Loss: 0.1768 | 0.1572
Epoch 230/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 231/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 232/300, seasonal_1 Loss: 0.1786 | 0.1572
Epoch 233/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 234/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 235/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 236/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 237/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 238/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 239/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 240/300, seasonal_1 Loss: 0.1773 | 0.1572
Epoch 241/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 242/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 243/300, seasonal_1 Loss: 0.1773 | 0.1572
Epoch 244/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 245/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 246/300, seasonal_1 Loss: 0.1783 | 0.1572
Epoch 247/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 248/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 249/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 250/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 251/300, seasonal_1 Loss: 0.1771 | 0.1572
Epoch 252/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 253/300, seasonal_1 Loss: 0.1768 | 0.1572
Epoch 254/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 255/300, seasonal_1 Loss: 0.1773 | 0.1572
Epoch 256/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 257/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 258/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 259/300, seasonal_1 Loss: 0.1780 | 0.1572
Epoch 260/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 261/300, seasonal_1 Loss: 0.1773 | 0.1572
Epoch 262/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 263/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 264/300, seasonal_1 Loss: 0.1779 | 0.1572
Epoch 265/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 266/300, seasonal_1 Loss: 0.1769 | 0.1572
Epoch 267/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 268/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 269/300, seasonal_1 Loss: 0.1783 | 0.1572
Epoch 270/300, seasonal_1 Loss: 0.1769 | 0.1572
Epoch 271/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 272/300, seasonal_1 Loss: 0.1785 | 0.1572
Epoch 273/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 274/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 275/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 276/300, seasonal_1 Loss: 0.1770 | 0.1572
Epoch 277/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 278/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 279/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 280/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 281/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 282/300, seasonal_1 Loss: 0.1768 | 0.1572
Epoch 283/300, seasonal_1 Loss: 0.1782 | 0.1572
Epoch 284/300, seasonal_1 Loss: 0.1771 | 0.1572
Epoch 285/300, seasonal_1 Loss: 0.1774 | 0.1572
Epoch 286/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 287/300, seasonal_1 Loss: 0.1770 | 0.1572
Epoch 288/300, seasonal_1 Loss: 0.1782 | 0.1572
Epoch 289/300, seasonal_1 Loss: 0.1778 | 0.1572
Epoch 290/300, seasonal_1 Loss: 0.1782 | 0.1572
Epoch 291/300, seasonal_1 Loss: 0.1776 | 0.1572
Epoch 292/300, seasonal_1 Loss: 0.1775 | 0.1571
Epoch 293/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 294/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 295/300, seasonal_1 Loss: 0.1769 | 0.1572
Epoch 296/300, seasonal_1 Loss: 0.1775 | 0.1572
Epoch 297/300, seasonal_1 Loss: 0.1772 | 0.1572
Epoch 298/300, seasonal_1 Loss: 0.1781 | 0.1572
Epoch 299/300, seasonal_1 Loss: 0.1777 | 0.1572
Epoch 300/300, seasonal_1 Loss: 0.1774 | 0.1572
Training seasonal_2 component with params: {'observation_period_num': 214, 'train_rates': 0.977773839543957, 'learning_rate': 0.00014684515690295606, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8370419728201278}
Epoch 1/300, seasonal_2 Loss: 0.7868 | 0.6075
Epoch 2/300, seasonal_2 Loss: 0.7209 | 0.5189
Epoch 3/300, seasonal_2 Loss: 0.6015 | 0.4860
Epoch 4/300, seasonal_2 Loss: 0.4979 | 0.4297
Epoch 5/300, seasonal_2 Loss: 0.4434 | 0.3803
Epoch 6/300, seasonal_2 Loss: 0.4017 | 0.3935
Epoch 7/300, seasonal_2 Loss: 0.3590 | 0.3707
Epoch 8/300, seasonal_2 Loss: 0.4011 | 0.3533
Epoch 9/300, seasonal_2 Loss: 0.3710 | 0.3240
Epoch 10/300, seasonal_2 Loss: 0.3299 | 0.3559
Epoch 11/300, seasonal_2 Loss: 0.2986 | 0.3129
Epoch 12/300, seasonal_2 Loss: 0.3187 | 0.3065
Epoch 13/300, seasonal_2 Loss: 0.3053 | 0.3605
Epoch 14/300, seasonal_2 Loss: 0.3154 | 0.2801
Epoch 15/300, seasonal_2 Loss: 0.2931 | 0.2707
Epoch 16/300, seasonal_2 Loss: 0.3000 | 0.2554
Epoch 17/300, seasonal_2 Loss: 0.2830 | 0.2820
Epoch 18/300, seasonal_2 Loss: 0.2573 | 0.2770
Epoch 19/300, seasonal_2 Loss: 0.2393 | 0.2150
Epoch 20/300, seasonal_2 Loss: 0.2271 | 0.2069
Epoch 21/300, seasonal_2 Loss: 0.2230 | 0.2252
Epoch 22/300, seasonal_2 Loss: 0.2150 | 0.1991
Epoch 23/300, seasonal_2 Loss: 0.2113 | 0.1974
Epoch 24/300, seasonal_2 Loss: 0.2080 | 0.1978
Epoch 25/300, seasonal_2 Loss: 0.2026 | 0.1886
Epoch 26/300, seasonal_2 Loss: 0.1995 | 0.1842
Epoch 27/300, seasonal_2 Loss: 0.1967 | 0.1836
Epoch 28/300, seasonal_2 Loss: 0.1931 | 0.1812
Epoch 29/300, seasonal_2 Loss: 0.1912 | 0.1751
Epoch 30/300, seasonal_2 Loss: 0.1900 | 0.1753
Epoch 31/300, seasonal_2 Loss: 0.1867 | 0.1762
Epoch 32/300, seasonal_2 Loss: 0.1860 | 0.1699
Epoch 33/300, seasonal_2 Loss: 0.1835 | 0.1713
Epoch 34/300, seasonal_2 Loss: 0.1825 | 0.1708
Epoch 35/300, seasonal_2 Loss: 0.1810 | 0.1670
Epoch 36/300, seasonal_2 Loss: 0.1787 | 0.1656
Epoch 37/300, seasonal_2 Loss: 0.1775 | 0.1665
Epoch 38/300, seasonal_2 Loss: 0.1756 | 0.1621
Epoch 39/300, seasonal_2 Loss: 0.1749 | 0.1606
Epoch 40/300, seasonal_2 Loss: 0.1730 | 0.1652
Epoch 41/300, seasonal_2 Loss: 0.1719 | 0.1611
Epoch 42/300, seasonal_2 Loss: 0.1716 | 0.1591
Epoch 43/300, seasonal_2 Loss: 0.1712 | 0.1600
Epoch 44/300, seasonal_2 Loss: 0.1700 | 0.1585
Epoch 45/300, seasonal_2 Loss: 0.1686 | 0.1575
Epoch 46/300, seasonal_2 Loss: 0.1678 | 0.1575
Epoch 47/300, seasonal_2 Loss: 0.1666 | 0.1562
Epoch 48/300, seasonal_2 Loss: 0.1661 | 0.1565
Epoch 49/300, seasonal_2 Loss: 0.1647 | 0.1542
Epoch 50/300, seasonal_2 Loss: 0.1648 | 0.1538
Epoch 51/300, seasonal_2 Loss: 0.1636 | 0.1545
Epoch 52/300, seasonal_2 Loss: 0.1638 | 0.1526
Epoch 53/300, seasonal_2 Loss: 0.1636 | 0.1538
Epoch 54/300, seasonal_2 Loss: 0.1611 | 0.1523
Epoch 55/300, seasonal_2 Loss: 0.1621 | 0.1514
Epoch 56/300, seasonal_2 Loss: 0.1615 | 0.1511
Epoch 57/300, seasonal_2 Loss: 0.1608 | 0.1515
Epoch 58/300, seasonal_2 Loss: 0.1597 | 0.1505
Epoch 59/300, seasonal_2 Loss: 0.1598 | 0.1498
Epoch 60/300, seasonal_2 Loss: 0.1587 | 0.1514
Epoch 61/300, seasonal_2 Loss: 0.1583 | 0.1492
Epoch 62/300, seasonal_2 Loss: 0.1583 | 0.1497
Epoch 63/300, seasonal_2 Loss: 0.1579 | 0.1493
Epoch 64/300, seasonal_2 Loss: 0.1575 | 0.1489
Epoch 65/300, seasonal_2 Loss: 0.1568 | 0.1484
Epoch 66/300, seasonal_2 Loss: 0.1570 | 0.1483
Epoch 67/300, seasonal_2 Loss: 0.1569 | 0.1483
Epoch 68/300, seasonal_2 Loss: 0.1557 | 0.1488
Epoch 69/300, seasonal_2 Loss: 0.1561 | 0.1477
Epoch 70/300, seasonal_2 Loss: 0.1560 | 0.1480
Epoch 71/300, seasonal_2 Loss: 0.1559 | 0.1475
Epoch 72/300, seasonal_2 Loss: 0.1548 | 0.1476
Epoch 73/300, seasonal_2 Loss: 0.1551 | 0.1480
Epoch 74/300, seasonal_2 Loss: 0.1548 | 0.1470
Epoch 75/300, seasonal_2 Loss: 0.1547 | 0.1466
Epoch 76/300, seasonal_2 Loss: 0.1546 | 0.1474
Epoch 77/300, seasonal_2 Loss: 0.1543 | 0.1470
Epoch 78/300, seasonal_2 Loss: 0.1533 | 0.1468
Epoch 79/300, seasonal_2 Loss: 0.1542 | 0.1463
Epoch 80/300, seasonal_2 Loss: 0.1542 | 0.1460
Epoch 81/300, seasonal_2 Loss: 0.1539 | 0.1458
Epoch 82/300, seasonal_2 Loss: 0.1526 | 0.1458
Epoch 83/300, seasonal_2 Loss: 0.1524 | 0.1452
Epoch 84/300, seasonal_2 Loss: 0.1520 | 0.1452
Epoch 85/300, seasonal_2 Loss: 0.1531 | 0.1454
Epoch 86/300, seasonal_2 Loss: 0.1533 | 0.1455
Epoch 87/300, seasonal_2 Loss: 0.1527 | 0.1455
Epoch 88/300, seasonal_2 Loss: 0.1527 | 0.1455
Epoch 89/300, seasonal_2 Loss: 0.1523 | 0.1454
Epoch 90/300, seasonal_2 Loss: 0.1519 | 0.1453
Epoch 91/300, seasonal_2 Loss: 0.1526 | 0.1452
Epoch 92/300, seasonal_2 Loss: 0.1529 | 0.1453
Epoch 93/300, seasonal_2 Loss: 0.1522 | 0.1449
Epoch 94/300, seasonal_2 Loss: 0.1520 | 0.1450
Epoch 95/300, seasonal_2 Loss: 0.1521 | 0.1447
Epoch 96/300, seasonal_2 Loss: 0.1525 | 0.1447
Epoch 97/300, seasonal_2 Loss: 0.1525 | 0.1450
Epoch 98/300, seasonal_2 Loss: 0.1516 | 0.1452
Epoch 99/300, seasonal_2 Loss: 0.1523 | 0.1451
Epoch 100/300, seasonal_2 Loss: 0.1524 | 0.1450
Epoch 101/300, seasonal_2 Loss: 0.1513 | 0.1450
Epoch 102/300, seasonal_2 Loss: 0.1518 | 0.1452
Epoch 103/300, seasonal_2 Loss: 0.1511 | 0.1451
Epoch 104/300, seasonal_2 Loss: 0.1516 | 0.1450
Epoch 105/300, seasonal_2 Loss: 0.1518 | 0.1452
Epoch 106/300, seasonal_2 Loss: 0.1518 | 0.1450
Epoch 107/300, seasonal_2 Loss: 0.1511 | 0.1448
Epoch 108/300, seasonal_2 Loss: 0.1516 | 0.1447
Epoch 109/300, seasonal_2 Loss: 0.1517 | 0.1447
Epoch 110/300, seasonal_2 Loss: 0.1510 | 0.1446
Epoch 111/300, seasonal_2 Loss: 0.1518 | 0.1446
Epoch 112/300, seasonal_2 Loss: 0.1508 | 0.1446
Epoch 113/300, seasonal_2 Loss: 0.1512 | 0.1446
Epoch 114/300, seasonal_2 Loss: 0.1511 | 0.1445
Epoch 115/300, seasonal_2 Loss: 0.1510 | 0.1444
Epoch 116/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 117/300, seasonal_2 Loss: 0.1518 | 0.1442
Epoch 118/300, seasonal_2 Loss: 0.1509 | 0.1442
Epoch 119/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 120/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 121/300, seasonal_2 Loss: 0.1509 | 0.1442
Epoch 122/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 123/300, seasonal_2 Loss: 0.1517 | 0.1445
Epoch 124/300, seasonal_2 Loss: 0.1497 | 0.1445
Epoch 125/300, seasonal_2 Loss: 0.1513 | 0.1445
Epoch 126/300, seasonal_2 Loss: 0.1519 | 0.1445
Epoch 127/300, seasonal_2 Loss: 0.1521 | 0.1445
Epoch 128/300, seasonal_2 Loss: 0.1510 | 0.1445
Epoch 129/300, seasonal_2 Loss: 0.1505 | 0.1444
Epoch 130/300, seasonal_2 Loss: 0.1507 | 0.1444
Epoch 131/300, seasonal_2 Loss: 0.1508 | 0.1444
Epoch 132/300, seasonal_2 Loss: 0.1506 | 0.1445
Epoch 133/300, seasonal_2 Loss: 0.1503 | 0.1444
Epoch 134/300, seasonal_2 Loss: 0.1501 | 0.1444
Epoch 135/300, seasonal_2 Loss: 0.1513 | 0.1444
Epoch 136/300, seasonal_2 Loss: 0.1514 | 0.1444
Epoch 137/300, seasonal_2 Loss: 0.1508 | 0.1444
Epoch 138/300, seasonal_2 Loss: 0.1505 | 0.1444
Epoch 139/300, seasonal_2 Loss: 0.1513 | 0.1444
Epoch 140/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 141/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 142/300, seasonal_2 Loss: 0.1513 | 0.1443
Epoch 143/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 144/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 145/300, seasonal_2 Loss: 0.1499 | 0.1443
Epoch 146/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 147/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 148/300, seasonal_2 Loss: 0.1512 | 0.1443
Epoch 149/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 150/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 151/300, seasonal_2 Loss: 0.1515 | 0.1443
Epoch 152/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 153/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 154/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 155/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 156/300, seasonal_2 Loss: 0.1515 | 0.1443
Epoch 157/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 158/300, seasonal_2 Loss: 0.1500 | 0.1443
Epoch 159/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 160/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 161/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 162/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 163/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 164/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 165/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 166/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 167/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 168/300, seasonal_2 Loss: 0.1513 | 0.1443
Epoch 169/300, seasonal_2 Loss: 0.1515 | 0.1443
Epoch 170/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 171/300, seasonal_2 Loss: 0.1502 | 0.1443
Epoch 172/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 173/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 174/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 175/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 176/300, seasonal_2 Loss: 0.1512 | 0.1443
Epoch 177/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 178/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 179/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 180/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 181/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 182/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 183/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 184/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 185/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 186/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 187/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 188/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 189/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 190/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 191/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 192/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 193/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 194/300, seasonal_2 Loss: 0.1514 | 0.1443
Epoch 195/300, seasonal_2 Loss: 0.1513 | 0.1443
Epoch 196/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 197/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 198/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 199/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 200/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 201/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 202/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 203/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 204/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 205/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 206/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 207/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 208/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 209/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 210/300, seasonal_2 Loss: 0.1513 | 0.1443
Epoch 211/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 212/300, seasonal_2 Loss: 0.1502 | 0.1443
Epoch 213/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 214/300, seasonal_2 Loss: 0.1500 | 0.1443
Epoch 215/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 216/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 217/300, seasonal_2 Loss: 0.1505 | 0.1443
Epoch 218/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 219/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 220/300, seasonal_2 Loss: 0.1507 | 0.1443
Epoch 221/300, seasonal_2 Loss: 0.1515 | 0.1443
Epoch 222/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 223/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 224/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 225/300, seasonal_2 Loss: 0.1501 | 0.1443
Epoch 226/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 227/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 228/300, seasonal_2 Loss: 0.1504 | 0.1443
Epoch 229/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 230/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 231/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 232/300, seasonal_2 Loss: 0.1500 | 0.1443
Epoch 233/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 234/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 235/300, seasonal_2 Loss: 0.1513 | 0.1443
Epoch 236/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 237/300, seasonal_2 Loss: 0.1512 | 0.1443
Epoch 238/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 239/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 240/300, seasonal_2 Loss: 0.1506 | 0.1443
Epoch 241/300, seasonal_2 Loss: 0.1503 | 0.1443
Epoch 242/300, seasonal_2 Loss: 0.1515 | 0.1443
Epoch 243/300, seasonal_2 Loss: 0.1514 | 0.1443
Epoch 244/300, seasonal_2 Loss: 0.1511 | 0.1443
Epoch 245/300, seasonal_2 Loss: 0.1509 | 0.1443
Epoch 246/300, seasonal_2 Loss: 0.1510 | 0.1443
Epoch 247/300, seasonal_2 Loss: 0.1508 | 0.1443
Epoch 248/300, seasonal_2 Loss: 0.1505 | 0.1443
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 146, 'train_rates': 0.97638315430517, 'learning_rate': 0.00021339223263146682, 'batch_size': 209, 'step_size': 3, 'gamma': 0.9521369356707362}
Epoch 1/300, seasonal_3 Loss: 1.2726 | 1.6494
Epoch 2/300, seasonal_3 Loss: 0.8361 | 0.9322
Epoch 3/300, seasonal_3 Loss: 0.6612 | 0.6706
Epoch 4/300, seasonal_3 Loss: 0.5783 | 0.5531
Epoch 5/300, seasonal_3 Loss: 0.5287 | 0.5051
Epoch 6/300, seasonal_3 Loss: 0.5031 | 0.4803
Epoch 7/300, seasonal_3 Loss: 0.5211 | 0.6223
Epoch 8/300, seasonal_3 Loss: 0.4975 | 0.4562
Epoch 9/300, seasonal_3 Loss: 0.4794 | 0.4855
Epoch 10/300, seasonal_3 Loss: 0.4005 | 0.4167
Epoch 11/300, seasonal_3 Loss: 0.4006 | 0.4377
Epoch 12/300, seasonal_3 Loss: 0.3647 | 0.3720
Epoch 13/300, seasonal_3 Loss: 0.3723 | 0.3714
Epoch 14/300, seasonal_3 Loss: 0.3419 | 0.3619
Epoch 15/300, seasonal_3 Loss: 0.3388 | 0.3558
Epoch 16/300, seasonal_3 Loss: 0.3189 | 0.3331
Epoch 17/300, seasonal_3 Loss: 0.3060 | 0.3237
Epoch 18/300, seasonal_3 Loss: 0.2881 | 0.3120
Epoch 19/300, seasonal_3 Loss: 0.2780 | 0.3035
Epoch 20/300, seasonal_3 Loss: 0.2704 | 0.2971
Epoch 21/300, seasonal_3 Loss: 0.2660 | 0.2901
Epoch 22/300, seasonal_3 Loss: 0.2609 | 0.2848
Epoch 23/300, seasonal_3 Loss: 0.2581 | 0.2805
Epoch 24/300, seasonal_3 Loss: 0.2548 | 0.2724
Epoch 25/300, seasonal_3 Loss: 0.2513 | 0.2688
Epoch 26/300, seasonal_3 Loss: 0.2467 | 0.2615
Epoch 27/300, seasonal_3 Loss: 0.2430 | 0.2606
Epoch 28/300, seasonal_3 Loss: 0.2403 | 0.2563
Epoch 29/300, seasonal_3 Loss: 0.2379 | 0.2535
Epoch 30/300, seasonal_3 Loss: 0.2366 | 0.2473
Epoch 31/300, seasonal_3 Loss: 0.2329 | 0.2443
Epoch 32/300, seasonal_3 Loss: 0.2298 | 0.2420
Epoch 33/300, seasonal_3 Loss: 0.2273 | 0.2388
Epoch 34/300, seasonal_3 Loss: 0.2248 | 0.2359
Epoch 35/300, seasonal_3 Loss: 0.2229 | 0.2322
Epoch 36/300, seasonal_3 Loss: 0.2217 | 0.2309
Epoch 37/300, seasonal_3 Loss: 0.2196 | 0.2287
Epoch 38/300, seasonal_3 Loss: 0.2181 | 0.2264
Epoch 39/300, seasonal_3 Loss: 0.2160 | 0.2243
Epoch 40/300, seasonal_3 Loss: 0.2151 | 0.2215
Epoch 41/300, seasonal_3 Loss: 0.2141 | 0.2206
Epoch 42/300, seasonal_3 Loss: 0.2122 | 0.2183
Epoch 43/300, seasonal_3 Loss: 0.2110 | 0.2184
Epoch 44/300, seasonal_3 Loss: 0.2104 | 0.2164
Epoch 45/300, seasonal_3 Loss: 0.2078 | 0.2146
Epoch 46/300, seasonal_3 Loss: 0.2080 | 0.2131
Epoch 47/300, seasonal_3 Loss: 0.2070 | 0.2118
Epoch 48/300, seasonal_3 Loss: 0.2053 | 0.2112
Epoch 49/300, seasonal_3 Loss: 0.2044 | 0.2095
Epoch 50/300, seasonal_3 Loss: 0.2041 | 0.2082
Epoch 51/300, seasonal_3 Loss: 0.2027 | 0.2076
Epoch 52/300, seasonal_3 Loss: 0.2016 | 0.2066
Epoch 53/300, seasonal_3 Loss: 0.2008 | 0.2057
Epoch 54/300, seasonal_3 Loss: 0.2006 | 0.2038
Epoch 55/300, seasonal_3 Loss: 0.1996 | 0.2018
Epoch 56/300, seasonal_3 Loss: 0.1996 | 0.2019
Epoch 57/300, seasonal_3 Loss: 0.1988 | 0.2017
Epoch 58/300, seasonal_3 Loss: 0.1977 | 0.2009
Epoch 59/300, seasonal_3 Loss: 0.1977 | 0.2009
Epoch 60/300, seasonal_3 Loss: 0.1962 | 0.1982
Epoch 61/300, seasonal_3 Loss: 0.1962 | 0.1981
Epoch 62/300, seasonal_3 Loss: 0.1948 | 0.1972
Epoch 63/300, seasonal_3 Loss: 0.1949 | 0.1967
Epoch 64/300, seasonal_3 Loss: 0.1949 | 0.1961
Epoch 65/300, seasonal_3 Loss: 0.1944 | 0.1955
Epoch 66/300, seasonal_3 Loss: 0.1936 | 0.1943
Epoch 67/300, seasonal_3 Loss: 0.1929 | 0.1943
Epoch 68/300, seasonal_3 Loss: 0.1923 | 0.1939
Epoch 69/300, seasonal_3 Loss: 0.1921 | 0.1935
Epoch 70/300, seasonal_3 Loss: 0.1919 | 0.1925
Epoch 71/300, seasonal_3 Loss: 0.1913 | 0.1920
Epoch 72/300, seasonal_3 Loss: 0.1908 | 0.1922
Epoch 73/300, seasonal_3 Loss: 0.1905 | 0.1916
Epoch 74/300, seasonal_3 Loss: 0.1904 | 0.1912
Epoch 75/300, seasonal_3 Loss: 0.1898 | 0.1901
Epoch 76/300, seasonal_3 Loss: 0.1897 | 0.1897
Epoch 77/300, seasonal_3 Loss: 0.1892 | 0.1897
Epoch 78/300, seasonal_3 Loss: 0.1887 | 0.1893
Epoch 79/300, seasonal_3 Loss: 0.1890 | 0.1890
Epoch 80/300, seasonal_3 Loss: 0.1885 | 0.1891
Epoch 81/300, seasonal_3 Loss: 0.1882 | 0.1886
Epoch 82/300, seasonal_3 Loss: 0.1873 | 0.1882
Epoch 83/300, seasonal_3 Loss: 0.1877 | 0.1882
Epoch 84/300, seasonal_3 Loss: 0.1865 | 0.1878
Epoch 85/300, seasonal_3 Loss: 0.1870 | 0.1876
Epoch 86/300, seasonal_3 Loss: 0.1873 | 0.1873
Epoch 87/300, seasonal_3 Loss: 0.1863 | 0.1867
Epoch 88/300, seasonal_3 Loss: 0.1865 | 0.1865
Epoch 89/300, seasonal_3 Loss: 0.1861 | 0.1862
Epoch 90/300, seasonal_3 Loss: 0.1856 | 0.1866
Epoch 91/300, seasonal_3 Loss: 0.1857 | 0.1860
Epoch 92/300, seasonal_3 Loss: 0.1855 | 0.1858
Epoch 93/300, seasonal_3 Loss: 0.1858 | 0.1850
Epoch 94/300, seasonal_3 Loss: 0.1844 | 0.1843
Epoch 95/300, seasonal_3 Loss: 0.1850 | 0.1835
Epoch 96/300, seasonal_3 Loss: 0.1843 | 0.1835
Epoch 97/300, seasonal_3 Loss: 0.1840 | 0.1836
Epoch 98/300, seasonal_3 Loss: 0.1837 | 0.1833
Epoch 99/300, seasonal_3 Loss: 0.1846 | 0.1831
Epoch 100/300, seasonal_3 Loss: 0.1836 | 0.1833
Epoch 101/300, seasonal_3 Loss: 0.1838 | 0.1828
Epoch 102/300, seasonal_3 Loss: 0.1839 | 0.1829
Epoch 103/300, seasonal_3 Loss: 0.1834 | 0.1827
Epoch 104/300, seasonal_3 Loss: 0.1829 | 0.1827
Epoch 105/300, seasonal_3 Loss: 0.1837 | 0.1824
Epoch 106/300, seasonal_3 Loss: 0.1828 | 0.1821
Epoch 107/300, seasonal_3 Loss: 0.1833 | 0.1818
Epoch 108/300, seasonal_3 Loss: 0.1831 | 0.1816
Epoch 109/300, seasonal_3 Loss: 0.1824 | 0.1816
Epoch 110/300, seasonal_3 Loss: 0.1822 | 0.1818
Epoch 111/300, seasonal_3 Loss: 0.1821 | 0.1818
Epoch 112/300, seasonal_3 Loss: 0.1822 | 0.1821
Epoch 113/300, seasonal_3 Loss: 0.1824 | 0.1820
Epoch 114/300, seasonal_3 Loss: 0.1821 | 0.1817
Epoch 115/300, seasonal_3 Loss: 0.1817 | 0.1816
Epoch 116/300, seasonal_3 Loss: 0.1816 | 0.1816
Epoch 117/300, seasonal_3 Loss: 0.1819 | 0.1816
Epoch 118/300, seasonal_3 Loss: 0.1821 | 0.1814
Epoch 119/300, seasonal_3 Loss: 0.1818 | 0.1811
Epoch 120/300, seasonal_3 Loss: 0.1813 | 0.1808
Epoch 121/300, seasonal_3 Loss: 0.1814 | 0.1808
Epoch 122/300, seasonal_3 Loss: 0.1817 | 0.1805
Epoch 123/300, seasonal_3 Loss: 0.1816 | 0.1804
Epoch 124/300, seasonal_3 Loss: 0.1821 | 0.1805
Epoch 125/300, seasonal_3 Loss: 0.1808 | 0.1805
Epoch 126/300, seasonal_3 Loss: 0.1814 | 0.1804
Epoch 127/300, seasonal_3 Loss: 0.1806 | 0.1802
Epoch 128/300, seasonal_3 Loss: 0.1816 | 0.1801
Epoch 129/300, seasonal_3 Loss: 0.1813 | 0.1801
Epoch 130/300, seasonal_3 Loss: 0.1808 | 0.1800
Epoch 131/300, seasonal_3 Loss: 0.1805 | 0.1799
Epoch 132/300, seasonal_3 Loss: 0.1811 | 0.1799
Epoch 133/300, seasonal_3 Loss: 0.1804 | 0.1798
Epoch 134/300, seasonal_3 Loss: 0.1809 | 0.1798
Epoch 135/300, seasonal_3 Loss: 0.1808 | 0.1797
Epoch 136/300, seasonal_3 Loss: 0.1800 | 0.1795
Epoch 137/300, seasonal_3 Loss: 0.1813 | 0.1795
Epoch 138/300, seasonal_3 Loss: 0.1807 | 0.1796
Epoch 139/300, seasonal_3 Loss: 0.1807 | 0.1796
Epoch 140/300, seasonal_3 Loss: 0.1808 | 0.1795
Epoch 141/300, seasonal_3 Loss: 0.1805 | 0.1793
Epoch 142/300, seasonal_3 Loss: 0.1806 | 0.1793
Epoch 143/300, seasonal_3 Loss: 0.1806 | 0.1791
Epoch 144/300, seasonal_3 Loss: 0.1799 | 0.1791
Epoch 145/300, seasonal_3 Loss: 0.1805 | 0.1792
Epoch 146/300, seasonal_3 Loss: 0.1805 | 0.1792
Epoch 147/300, seasonal_3 Loss: 0.1794 | 0.1791
Epoch 148/300, seasonal_3 Loss: 0.1803 | 0.1790
Epoch 149/300, seasonal_3 Loss: 0.1799 | 0.1790
Epoch 150/300, seasonal_3 Loss: 0.1802 | 0.1789
Epoch 151/300, seasonal_3 Loss: 0.1802 | 0.1787
Epoch 152/300, seasonal_3 Loss: 0.1799 | 0.1786
Epoch 153/300, seasonal_3 Loss: 0.1797 | 0.1785
Epoch 154/300, seasonal_3 Loss: 0.1800 | 0.1784
Epoch 155/300, seasonal_3 Loss: 0.1792 | 0.1784
Epoch 156/300, seasonal_3 Loss: 0.1802 | 0.1785
Epoch 157/300, seasonal_3 Loss: 0.1791 | 0.1784
Epoch 158/300, seasonal_3 Loss: 0.1796 | 0.1784
Epoch 159/300, seasonal_3 Loss: 0.1801 | 0.1784
Epoch 160/300, seasonal_3 Loss: 0.1804 | 0.1784
Epoch 161/300, seasonal_3 Loss: 0.1800 | 0.1784
Epoch 162/300, seasonal_3 Loss: 0.1798 | 0.1784
Epoch 163/300, seasonal_3 Loss: 0.1791 | 0.1783
Epoch 164/300, seasonal_3 Loss: 0.1798 | 0.1782
Epoch 165/300, seasonal_3 Loss: 0.1792 | 0.1782
Epoch 166/300, seasonal_3 Loss: 0.1798 | 0.1783
Epoch 167/300, seasonal_3 Loss: 0.1797 | 0.1782
Epoch 168/300, seasonal_3 Loss: 0.1793 | 0.1782
Epoch 169/300, seasonal_3 Loss: 0.1791 | 0.1781
Epoch 170/300, seasonal_3 Loss: 0.1799 | 0.1781
Epoch 171/300, seasonal_3 Loss: 0.1794 | 0.1781
Epoch 172/300, seasonal_3 Loss: 0.1794 | 0.1781
Epoch 173/300, seasonal_3 Loss: 0.1794 | 0.1780
Epoch 174/300, seasonal_3 Loss: 0.1790 | 0.1780
Epoch 175/300, seasonal_3 Loss: 0.1796 | 0.1780
Epoch 176/300, seasonal_3 Loss: 0.1798 | 0.1780
Epoch 177/300, seasonal_3 Loss: 0.1791 | 0.1779
Epoch 178/300, seasonal_3 Loss: 0.1788 | 0.1778
Epoch 179/300, seasonal_3 Loss: 0.1796 | 0.1778
Epoch 180/300, seasonal_3 Loss: 0.1794 | 0.1777
Epoch 181/300, seasonal_3 Loss: 0.1795 | 0.1777
Epoch 182/300, seasonal_3 Loss: 0.1787 | 0.1777
Epoch 183/300, seasonal_3 Loss: 0.1793 | 0.1777
Epoch 184/300, seasonal_3 Loss: 0.1786 | 0.1777
Epoch 185/300, seasonal_3 Loss: 0.1791 | 0.1777
Epoch 186/300, seasonal_3 Loss: 0.1794 | 0.1778
Epoch 187/300, seasonal_3 Loss: 0.1784 | 0.1778
Epoch 188/300, seasonal_3 Loss: 0.1788 | 0.1778
Epoch 189/300, seasonal_3 Loss: 0.1787 | 0.1778
Epoch 190/300, seasonal_3 Loss: 0.1793 | 0.1778
Epoch 191/300, seasonal_3 Loss: 0.1791 | 0.1778
Epoch 192/300, seasonal_3 Loss: 0.1793 | 0.1777
Epoch 193/300, seasonal_3 Loss: 0.1783 | 0.1777
Epoch 194/300, seasonal_3 Loss: 0.1792 | 0.1777
Epoch 195/300, seasonal_3 Loss: 0.1792 | 0.1777
Epoch 196/300, seasonal_3 Loss: 0.1790 | 0.1777
Epoch 197/300, seasonal_3 Loss: 0.1789 | 0.1776
Epoch 198/300, seasonal_3 Loss: 0.1790 | 0.1776
Epoch 199/300, seasonal_3 Loss: 0.1788 | 0.1776
Epoch 200/300, seasonal_3 Loss: 0.1792 | 0.1776
Epoch 201/300, seasonal_3 Loss: 0.1789 | 0.1776
Epoch 202/300, seasonal_3 Loss: 0.1789 | 0.1776
Epoch 203/300, seasonal_3 Loss: 0.1791 | 0.1776
Epoch 204/300, seasonal_3 Loss: 0.1790 | 0.1776
Epoch 205/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 206/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 207/300, seasonal_3 Loss: 0.1786 | 0.1775
Epoch 208/300, seasonal_3 Loss: 0.1792 | 0.1775
Epoch 209/300, seasonal_3 Loss: 0.1794 | 0.1775
Epoch 210/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 211/300, seasonal_3 Loss: 0.1797 | 0.1775
Epoch 212/300, seasonal_3 Loss: 0.1797 | 0.1775
Epoch 213/300, seasonal_3 Loss: 0.1793 | 0.1775
Epoch 214/300, seasonal_3 Loss: 0.1794 | 0.1775
Epoch 215/300, seasonal_3 Loss: 0.1785 | 0.1775
Epoch 216/300, seasonal_3 Loss: 0.1788 | 0.1775
Epoch 217/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 218/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 219/300, seasonal_3 Loss: 0.1782 | 0.1775
Epoch 220/300, seasonal_3 Loss: 0.1791 | 0.1775
Epoch 221/300, seasonal_3 Loss: 0.1792 | 0.1775
Epoch 222/300, seasonal_3 Loss: 0.1784 | 0.1775
Epoch 223/300, seasonal_3 Loss: 0.1786 | 0.1775
Epoch 224/300, seasonal_3 Loss: 0.1784 | 0.1775
Epoch 225/300, seasonal_3 Loss: 0.1789 | 0.1775
Epoch 226/300, seasonal_3 Loss: 0.1790 | 0.1775
Epoch 227/300, seasonal_3 Loss: 0.1785 | 0.1775
Epoch 228/300, seasonal_3 Loss: 0.1783 | 0.1775
Epoch 229/300, seasonal_3 Loss: 0.1784 | 0.1775
Epoch 230/300, seasonal_3 Loss: 0.1794 | 0.1775
Epoch 231/300, seasonal_3 Loss: 0.1786 | 0.1775
Epoch 232/300, seasonal_3 Loss: 0.1788 | 0.1775
Epoch 233/300, seasonal_3 Loss: 0.1793 | 0.1775
Epoch 234/300, seasonal_3 Loss: 0.1783 | 0.1775
Epoch 235/300, seasonal_3 Loss: 0.1787 | 0.1775
Epoch 236/300, seasonal_3 Loss: 0.1793 | 0.1775
Epoch 237/300, seasonal_3 Loss: 0.1784 | 0.1775
Epoch 238/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 239/300, seasonal_3 Loss: 0.1795 | 0.1774
Epoch 240/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 241/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 242/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 243/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 244/300, seasonal_3 Loss: 0.1792 | 0.1774
Epoch 245/300, seasonal_3 Loss: 0.1786 | 0.1774
Epoch 246/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 247/300, seasonal_3 Loss: 0.1782 | 0.1774
Epoch 248/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 249/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 250/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 251/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 252/300, seasonal_3 Loss: 0.1793 | 0.1774
Epoch 253/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 254/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 255/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 256/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 257/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 258/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 259/300, seasonal_3 Loss: 0.1785 | 0.1774
Epoch 260/300, seasonal_3 Loss: 0.1791 | 0.1774
Epoch 261/300, seasonal_3 Loss: 0.1792 | 0.1774
Epoch 262/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 263/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 264/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 265/300, seasonal_3 Loss: 0.1787 | 0.1774
Epoch 266/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 267/300, seasonal_3 Loss: 0.1780 | 0.1774
Epoch 268/300, seasonal_3 Loss: 0.1794 | 0.1774
Epoch 269/300, seasonal_3 Loss: 0.1785 | 0.1774
Epoch 270/300, seasonal_3 Loss: 0.1782 | 0.1774
Epoch 271/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 272/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 273/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 274/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 275/300, seasonal_3 Loss: 0.1782 | 0.1774
Epoch 276/300, seasonal_3 Loss: 0.1793 | 0.1774
Epoch 277/300, seasonal_3 Loss: 0.1784 | 0.1774
Epoch 278/300, seasonal_3 Loss: 0.1794 | 0.1774
Epoch 279/300, seasonal_3 Loss: 0.1786 | 0.1774
Epoch 280/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 281/300, seasonal_3 Loss: 0.1791 | 0.1774
Epoch 282/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 283/300, seasonal_3 Loss: 0.1785 | 0.1774
Epoch 284/300, seasonal_3 Loss: 0.1786 | 0.1774
Epoch 285/300, seasonal_3 Loss: 0.1782 | 0.1774
Epoch 286/300, seasonal_3 Loss: 0.1785 | 0.1774
Epoch 287/300, seasonal_3 Loss: 0.1778 | 0.1774
Epoch 288/300, seasonal_3 Loss: 0.1795 | 0.1774
Epoch 289/300, seasonal_3 Loss: 0.1782 | 0.1774
Epoch 290/300, seasonal_3 Loss: 0.1790 | 0.1774
Epoch 291/300, seasonal_3 Loss: 0.1785 | 0.1774
Epoch 292/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 293/300, seasonal_3 Loss: 0.1789 | 0.1774
Epoch 294/300, seasonal_3 Loss: 0.1793 | 0.1774
Epoch 295/300, seasonal_3 Loss: 0.1787 | 0.1774
Epoch 296/300, seasonal_3 Loss: 0.1793 | 0.1774
Epoch 297/300, seasonal_3 Loss: 0.1786 | 0.1774
Epoch 298/300, seasonal_3 Loss: 0.1788 | 0.1774
Epoch 299/300, seasonal_3 Loss: 0.1787 | 0.1774
Epoch 300/300, seasonal_3 Loss: 0.1785 | 0.1774
Training resid component with params: {'observation_period_num': 239, 'train_rates': 0.9706047478926179, 'learning_rate': 0.00018245554330405971, 'batch_size': 89, 'step_size': 15, 'gamma': 0.7834205846160324}
Epoch 1/300, resid Loss: 1.0675 | 1.2933
Epoch 2/300, resid Loss: 0.8543 | 0.7549
Epoch 3/300, resid Loss: 0.6986 | 0.7353
Epoch 4/300, resid Loss: 0.6093 | 0.6269
Epoch 5/300, resid Loss: 0.5442 | 0.5418
Epoch 6/300, resid Loss: 0.5537 | 0.6278
Epoch 7/300, resid Loss: 0.5124 | 0.4630
Epoch 8/300, resid Loss: 0.4728 | 0.4749
Epoch 9/300, resid Loss: 0.5201 | 0.5768
Epoch 10/300, resid Loss: 0.4738 | 0.4316
Epoch 11/300, resid Loss: 0.3968 | 0.4392
Epoch 12/300, resid Loss: 0.3963 | 0.3828
Epoch 13/300, resid Loss: 0.3662 | 0.3754
Epoch 14/300, resid Loss: 0.3367 | 0.3283
Epoch 15/300, resid Loss: 0.3500 | 0.3418
Epoch 16/300, resid Loss: 0.3109 | 0.2927
Epoch 17/300, resid Loss: 0.2857 | 0.2833
Epoch 18/300, resid Loss: 0.2776 | 0.2836
Epoch 19/300, resid Loss: 0.2676 | 0.2627
Epoch 20/300, resid Loss: 0.2583 | 0.2624
Epoch 21/300, resid Loss: 0.2552 | 0.2699
Epoch 22/300, resid Loss: 0.2586 | 0.2478
Epoch 23/300, resid Loss: 0.2643 | 0.2375
Epoch 24/300, resid Loss: 0.2589 | 0.2834
Epoch 25/300, resid Loss: 0.2725 | 0.2497
Epoch 26/300, resid Loss: 0.2519 | 0.2318
Epoch 27/300, resid Loss: 0.2573 | 0.2190
Epoch 28/300, resid Loss: 0.2735 | 0.2306
Epoch 29/300, resid Loss: 0.2391 | 0.2261
Epoch 30/300, resid Loss: 0.2333 | 0.2367
Epoch 31/300, resid Loss: 0.2361 | 0.2155
Epoch 32/300, resid Loss: 0.2234 | 0.2092
Epoch 33/300, resid Loss: 0.2174 | 0.2063
Epoch 34/300, resid Loss: 0.2162 | 0.2046
Epoch 35/300, resid Loss: 0.2136 | 0.1974
Epoch 36/300, resid Loss: 0.2122 | 0.2171
Epoch 37/300, resid Loss: 0.2110 | 0.1960
Epoch 38/300, resid Loss: 0.2087 | 0.1958
Epoch 39/300, resid Loss: 0.2077 | 0.1934
Epoch 40/300, resid Loss: 0.2011 | 0.1871
Epoch 41/300, resid Loss: 0.1997 | 0.1861
Epoch 42/300, resid Loss: 0.1977 | 0.1898
Epoch 43/300, resid Loss: 0.1959 | 0.1810
Epoch 44/300, resid Loss: 0.1952 | 0.1880
Epoch 45/300, resid Loss: 0.1949 | 0.1826
Epoch 46/300, resid Loss: 0.1926 | 0.1780
Epoch 47/300, resid Loss: 0.1895 | 0.1810
Epoch 48/300, resid Loss: 0.1894 | 0.1776
Epoch 49/300, resid Loss: 0.1876 | 0.1774
Epoch 50/300, resid Loss: 0.1863 | 0.1763
Epoch 51/300, resid Loss: 0.1852 | 0.1745
Epoch 52/300, resid Loss: 0.1839 | 0.1733
Epoch 53/300, resid Loss: 0.1837 | 0.1734
Epoch 54/300, resid Loss: 0.1822 | 0.1712
Epoch 55/300, resid Loss: 0.1811 | 0.1727
Epoch 56/300, resid Loss: 0.1806 | 0.1712
Epoch 57/300, resid Loss: 0.1791 | 0.1694
Epoch 58/300, resid Loss: 0.1792 | 0.1714
Epoch 59/300, resid Loss: 0.1777 | 0.1689
Epoch 60/300, resid Loss: 0.1771 | 0.1683
Epoch 61/300, resid Loss: 0.1764 | 0.1683
Epoch 62/300, resid Loss: 0.1755 | 0.1673
Epoch 63/300, resid Loss: 0.1748 | 0.1674
Epoch 64/300, resid Loss: 0.1741 | 0.1673
Epoch 65/300, resid Loss: 0.1734 | 0.1654
Epoch 66/300, resid Loss: 0.1725 | 0.1663
Epoch 67/300, resid Loss: 0.1727 | 0.1657
Epoch 68/300, resid Loss: 0.1720 | 0.1641
Epoch 69/300, resid Loss: 0.1709 | 0.1644
Epoch 70/300, resid Loss: 0.1711 | 0.1642
Epoch 71/300, resid Loss: 0.1708 | 0.1630
Epoch 72/300, resid Loss: 0.1704 | 0.1633
Epoch 73/300, resid Loss: 0.1693 | 0.1629
Epoch 74/300, resid Loss: 0.1690 | 0.1617
Epoch 75/300, resid Loss: 0.1693 | 0.1623
Epoch 76/300, resid Loss: 0.1684 | 0.1612
Epoch 77/300, resid Loss: 0.1689 | 0.1610
Epoch 78/300, resid Loss: 0.1678 | 0.1608
Epoch 79/300, resid Loss: 0.1674 | 0.1605
Epoch 80/300, resid Loss: 0.1666 | 0.1600
Epoch 81/300, resid Loss: 0.1663 | 0.1600
Epoch 82/300, resid Loss: 0.1662 | 0.1602
Epoch 83/300, resid Loss: 0.1659 | 0.1588
Epoch 84/300, resid Loss: 0.1661 | 0.1590
Epoch 85/300, resid Loss: 0.1654 | 0.1588
Epoch 86/300, resid Loss: 0.1644 | 0.1591
Epoch 87/300, resid Loss: 0.1640 | 0.1583
Epoch 88/300, resid Loss: 0.1644 | 0.1578
Epoch 89/300, resid Loss: 0.1640 | 0.1581
Epoch 90/300, resid Loss: 0.1637 | 0.1581
Epoch 91/300, resid Loss: 0.1630 | 0.1576
Epoch 92/300, resid Loss: 0.1637 | 0.1581
Epoch 93/300, resid Loss: 0.1624 | 0.1574
Epoch 94/300, resid Loss: 0.1629 | 0.1580
Epoch 95/300, resid Loss: 0.1622 | 0.1574
Epoch 96/300, resid Loss: 0.1614 | 0.1575
Epoch 97/300, resid Loss: 0.1622 | 0.1571
Epoch 98/300, resid Loss: 0.1623 | 0.1571
Epoch 99/300, resid Loss: 0.1611 | 0.1573
Epoch 100/300, resid Loss: 0.1614 | 0.1575
Epoch 101/300, resid Loss: 0.1605 | 0.1575
Epoch 102/300, resid Loss: 0.1608 | 0.1577
Epoch 103/300, resid Loss: 0.1610 | 0.1570
Epoch 104/300, resid Loss: 0.1606 | 0.1563
Epoch 105/300, resid Loss: 0.1605 | 0.1560
Epoch 106/300, resid Loss: 0.1603 | 0.1557
Epoch 107/300, resid Loss: 0.1601 | 0.1560
Epoch 108/300, resid Loss: 0.1605 | 0.1557
Epoch 109/300, resid Loss: 0.1589 | 0.1557
Epoch 110/300, resid Loss: 0.1592 | 0.1554
Epoch 111/300, resid Loss: 0.1591 | 0.1552
Epoch 112/300, resid Loss: 0.1590 | 0.1551
Epoch 113/300, resid Loss: 0.1591 | 0.1552
Epoch 114/300, resid Loss: 0.1581 | 0.1557
Epoch 115/300, resid Loss: 0.1582 | 0.1555
Epoch 116/300, resid Loss: 0.1588 | 0.1556
Epoch 117/300, resid Loss: 0.1580 | 0.1551
Epoch 118/300, resid Loss: 0.1585 | 0.1549
Epoch 119/300, resid Loss: 0.1586 | 0.1548
Epoch 120/300, resid Loss: 0.1580 | 0.1547
Epoch 121/300, resid Loss: 0.1580 | 0.1547
Epoch 122/300, resid Loss: 0.1579 | 0.1545
Epoch 123/300, resid Loss: 0.1575 | 0.1546
Epoch 124/300, resid Loss: 0.1570 | 0.1545
Epoch 125/300, resid Loss: 0.1580 | 0.1546
Epoch 126/300, resid Loss: 0.1577 | 0.1546
Epoch 127/300, resid Loss: 0.1573 | 0.1548
Epoch 128/300, resid Loss: 0.1576 | 0.1547
Epoch 129/300, resid Loss: 0.1571 | 0.1544
Epoch 130/300, resid Loss: 0.1572 | 0.1544
Epoch 131/300, resid Loss: 0.1573 | 0.1543
Epoch 132/300, resid Loss: 0.1565 | 0.1542
Epoch 133/300, resid Loss: 0.1564 | 0.1537
Epoch 134/300, resid Loss: 0.1568 | 0.1537
Epoch 135/300, resid Loss: 0.1566 | 0.1537
Epoch 136/300, resid Loss: 0.1566 | 0.1537
Epoch 137/300, resid Loss: 0.1572 | 0.1538
Epoch 138/300, resid Loss: 0.1568 | 0.1536
Epoch 139/300, resid Loss: 0.1563 | 0.1537
Epoch 140/300, resid Loss: 0.1565 | 0.1539
Epoch 141/300, resid Loss: 0.1565 | 0.1538
Epoch 142/300, resid Loss: 0.1570 | 0.1534
Epoch 143/300, resid Loss: 0.1567 | 0.1532
Epoch 144/300, resid Loss: 0.1560 | 0.1534
Epoch 145/300, resid Loss: 0.1558 | 0.1533
Epoch 146/300, resid Loss: 0.1551 | 0.1534
Epoch 147/300, resid Loss: 0.1563 | 0.1532
Epoch 148/300, resid Loss: 0.1551 | 0.1532
Epoch 149/300, resid Loss: 0.1556 | 0.1533
Epoch 150/300, resid Loss: 0.1560 | 0.1531
Epoch 151/300, resid Loss: 0.1557 | 0.1530
Epoch 152/300, resid Loss: 0.1557 | 0.1529
Epoch 153/300, resid Loss: 0.1565 | 0.1529
Epoch 154/300, resid Loss: 0.1557 | 0.1529
Epoch 155/300, resid Loss: 0.1554 | 0.1529
Epoch 156/300, resid Loss: 0.1562 | 0.1529
Epoch 157/300, resid Loss: 0.1550 | 0.1529
Epoch 158/300, resid Loss: 0.1557 | 0.1530
Epoch 159/300, resid Loss: 0.1554 | 0.1530
Epoch 160/300, resid Loss: 0.1552 | 0.1529
Epoch 161/300, resid Loss: 0.1544 | 0.1528
Epoch 162/300, resid Loss: 0.1558 | 0.1527
Epoch 163/300, resid Loss: 0.1561 | 0.1526
Epoch 164/300, resid Loss: 0.1552 | 0.1526
Epoch 165/300, resid Loss: 0.1553 | 0.1525
Epoch 166/300, resid Loss: 0.1552 | 0.1526
Epoch 167/300, resid Loss: 0.1555 | 0.1527
Epoch 168/300, resid Loss: 0.1549 | 0.1527
Epoch 169/300, resid Loss: 0.1547 | 0.1526
Epoch 170/300, resid Loss: 0.1545 | 0.1526
Epoch 171/300, resid Loss: 0.1545 | 0.1527
Epoch 172/300, resid Loss: 0.1549 | 0.1527
Epoch 173/300, resid Loss: 0.1548 | 0.1526
Epoch 174/300, resid Loss: 0.1559 | 0.1525
Epoch 175/300, resid Loss: 0.1543 | 0.1524
Epoch 176/300, resid Loss: 0.1550 | 0.1524
Epoch 177/300, resid Loss: 0.1549 | 0.1524
Epoch 178/300, resid Loss: 0.1551 | 0.1524
Epoch 179/300, resid Loss: 0.1557 | 0.1524
Epoch 180/300, resid Loss: 0.1558 | 0.1524
Epoch 181/300, resid Loss: 0.1540 | 0.1525
Epoch 182/300, resid Loss: 0.1539 | 0.1525
Epoch 183/300, resid Loss: 0.1546 | 0.1525
Epoch 184/300, resid Loss: 0.1551 | 0.1526
Epoch 185/300, resid Loss: 0.1541 | 0.1525
Epoch 186/300, resid Loss: 0.1549 | 0.1525
Epoch 187/300, resid Loss: 0.1542 | 0.1525
Epoch 188/300, resid Loss: 0.1549 | 0.1524
Epoch 189/300, resid Loss: 0.1546 | 0.1524
Epoch 190/300, resid Loss: 0.1547 | 0.1523
Epoch 191/300, resid Loss: 0.1548 | 0.1522
Epoch 192/300, resid Loss: 0.1545 | 0.1522
Epoch 193/300, resid Loss: 0.1546 | 0.1522
Epoch 194/300, resid Loss: 0.1548 | 0.1523
Epoch 195/300, resid Loss: 0.1548 | 0.1523
Epoch 196/300, resid Loss: 0.1543 | 0.1524
Epoch 197/300, resid Loss: 0.1550 | 0.1523
Epoch 198/300, resid Loss: 0.1553 | 0.1523
Epoch 199/300, resid Loss: 0.1544 | 0.1523
Epoch 200/300, resid Loss: 0.1547 | 0.1523
Epoch 201/300, resid Loss: 0.1549 | 0.1523
Epoch 202/300, resid Loss: 0.1546 | 0.1523
Epoch 203/300, resid Loss: 0.1543 | 0.1523
Epoch 204/300, resid Loss: 0.1542 | 0.1523
Epoch 205/300, resid Loss: 0.1537 | 0.1523
Epoch 206/300, resid Loss: 0.1541 | 0.1523
Epoch 207/300, resid Loss: 0.1556 | 0.1523
Epoch 208/300, resid Loss: 0.1545 | 0.1522
Epoch 209/300, resid Loss: 0.1541 | 0.1523
Epoch 210/300, resid Loss: 0.1545 | 0.1523
Epoch 211/300, resid Loss: 0.1545 | 0.1522
Epoch 212/300, resid Loss: 0.1545 | 0.1522
Epoch 213/300, resid Loss: 0.1544 | 0.1522
Epoch 214/300, resid Loss: 0.1539 | 0.1522
Epoch 215/300, resid Loss: 0.1546 | 0.1522
Epoch 216/300, resid Loss: 0.1543 | 0.1521
Epoch 217/300, resid Loss: 0.1542 | 0.1522
Epoch 218/300, resid Loss: 0.1545 | 0.1522
Epoch 219/300, resid Loss: 0.1547 | 0.1521
Epoch 220/300, resid Loss: 0.1550 | 0.1521
Epoch 221/300, resid Loss: 0.1547 | 0.1522
Epoch 222/300, resid Loss: 0.1544 | 0.1522
Epoch 223/300, resid Loss: 0.1549 | 0.1522
Epoch 224/300, resid Loss: 0.1538 | 0.1522
Epoch 225/300, resid Loss: 0.1549 | 0.1521
Epoch 226/300, resid Loss: 0.1545 | 0.1521
Epoch 227/300, resid Loss: 0.1540 | 0.1521
Epoch 228/300, resid Loss: 0.1548 | 0.1521
Epoch 229/300, resid Loss: 0.1542 | 0.1521
Epoch 230/300, resid Loss: 0.1544 | 0.1521
Epoch 231/300, resid Loss: 0.1546 | 0.1521
Epoch 232/300, resid Loss: 0.1549 | 0.1521
Epoch 233/300, resid Loss: 0.1544 | 0.1521
Epoch 234/300, resid Loss: 0.1545 | 0.1521
Epoch 235/300, resid Loss: 0.1542 | 0.1521
Epoch 236/300, resid Loss: 0.1545 | 0.1521
Epoch 237/300, resid Loss: 0.1546 | 0.1521
Epoch 238/300, resid Loss: 0.1542 | 0.1521
Epoch 239/300, resid Loss: 0.1536 | 0.1521
Epoch 240/300, resid Loss: 0.1545 | 0.1520
Epoch 241/300, resid Loss: 0.1545 | 0.1520
Epoch 242/300, resid Loss: 0.1542 | 0.1520
Epoch 243/300, resid Loss: 0.1541 | 0.1520
Epoch 244/300, resid Loss: 0.1548 | 0.1520
Epoch 245/300, resid Loss: 0.1534 | 0.1520
Epoch 246/300, resid Loss: 0.1536 | 0.1520
Epoch 247/300, resid Loss: 0.1543 | 0.1520
Epoch 248/300, resid Loss: 0.1543 | 0.1520
Epoch 249/300, resid Loss: 0.1550 | 0.1520
Epoch 250/300, resid Loss: 0.1533 | 0.1520
Epoch 251/300, resid Loss: 0.1545 | 0.1520
Epoch 252/300, resid Loss: 0.1538 | 0.1520
Epoch 253/300, resid Loss: 0.1547 | 0.1520
Epoch 254/300, resid Loss: 0.1543 | 0.1520
Epoch 255/300, resid Loss: 0.1539 | 0.1520
Epoch 256/300, resid Loss: 0.1541 | 0.1520
Epoch 257/300, resid Loss: 0.1547 | 0.1520
Epoch 258/300, resid Loss: 0.1541 | 0.1520
Epoch 259/300, resid Loss: 0.1538 | 0.1520
Epoch 260/300, resid Loss: 0.1544 | 0.1520
Epoch 261/300, resid Loss: 0.1545 | 0.1520
Epoch 262/300, resid Loss: 0.1539 | 0.1520
Epoch 263/300, resid Loss: 0.1546 | 0.1520
Epoch 264/300, resid Loss: 0.1546 | 0.1520
Epoch 265/300, resid Loss: 0.1541 | 0.1521
Epoch 266/300, resid Loss: 0.1543 | 0.1521
Epoch 267/300, resid Loss: 0.1542 | 0.1521
Epoch 268/300, resid Loss: 0.1540 | 0.1520
Epoch 269/300, resid Loss: 0.1544 | 0.1521
Epoch 270/300, resid Loss: 0.1542 | 0.1521
Epoch 271/300, resid Loss: 0.1550 | 0.1521
Epoch 272/300, resid Loss: 0.1555 | 0.1521
Epoch 273/300, resid Loss: 0.1536 | 0.1521
Epoch 274/300, resid Loss: 0.1544 | 0.1521
Epoch 275/300, resid Loss: 0.1543 | 0.1521
Epoch 276/300, resid Loss: 0.1542 | 0.1521
Epoch 277/300, resid Loss: 0.1548 | 0.1521
Epoch 278/300, resid Loss: 0.1545 | 0.1521
Epoch 279/300, resid Loss: 0.1546 | 0.1521
Epoch 280/300, resid Loss: 0.1548 | 0.1521
Epoch 281/300, resid Loss: 0.1538 | 0.1521
Epoch 282/300, resid Loss: 0.1540 | 0.1521
Epoch 283/300, resid Loss: 0.1539 | 0.1521
Epoch 284/300, resid Loss: 0.1541 | 0.1521
Epoch 285/300, resid Loss: 0.1543 | 0.1521
Epoch 286/300, resid Loss: 0.1551 | 0.1521
Epoch 287/300, resid Loss: 0.1539 | 0.1521
Epoch 288/300, resid Loss: 0.1549 | 0.1521
Epoch 289/300, resid Loss: 0.1543 | 0.1521
Epoch 290/300, resid Loss: 0.1551 | 0.1521
Epoch 291/300, resid Loss: 0.1543 | 0.1521
Epoch 292/300, resid Loss: 0.1539 | 0.1521
Epoch 293/300, resid Loss: 0.1540 | 0.1521
Epoch 294/300, resid Loss: 0.1541 | 0.1521
Epoch 295/300, resid Loss: 0.1546 | 0.1521
Epoch 296/300, resid Loss: 0.1537 | 0.1521
Epoch 297/300, resid Loss: 0.1545 | 0.1521
Epoch 298/300, resid Loss: 0.1541 | 0.1521
Epoch 299/300, resid Loss: 0.1546 | 0.1521
Epoch 300/300, resid Loss: 0.1541 | 0.1521
Runtime (seconds): 3686.348508834839
9.011730237716253e-05
[34.696056]
[0.7989336]
[0.01973067]
[0.45816857]
[-3.7363899]
[-5.094079]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.6464748041507846
RMSE: 1.9095745086669922
MAE: 1.9095745086669922
R-squared: nan
[27.142422]
