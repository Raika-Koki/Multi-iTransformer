[32m[I 2025-02-04 01:07:23,173][0m A new study created in memory with name: no-name-746e955b-9431-4c06-949f-81c424954384[0m
[32m[I 2025-02-04 01:07:49,964][0m Trial 0 finished with value: 0.20430809259414673 and parameters: {'observation_period_num': 42, 'train_rates': 0.9288682572090208, 'learning_rate': 1.2430530770665676e-05, 'batch_size': 244, 'step_size': 8, 'gamma': 0.9337423744147123}. Best is trial 0 with value: 0.20430809259414673.[0m
[32m[I 2025-02-04 01:08:18,695][0m Trial 1 finished with value: 1.045023430225461 and parameters: {'observation_period_num': 82, 'train_rates': 0.7011794142556187, 'learning_rate': 2.7078514330950975e-06, 'batch_size': 180, 'step_size': 8, 'gamma': 0.8190954866163701}. Best is trial 0 with value: 0.20430809259414673.[0m
[32m[I 2025-02-04 01:09:01,499][0m Trial 2 finished with value: 0.9737442870573564 and parameters: {'observation_period_num': 16, 'train_rates': 0.813303764535281, 'learning_rate': 1.782351767311506e-06, 'batch_size': 126, 'step_size': 5, 'gamma': 0.7883213697520102}. Best is trial 0 with value: 0.20430809259414673.[0m
[32m[I 2025-02-04 01:09:42,981][0m Trial 3 finished with value: 0.21512508392333984 and parameters: {'observation_period_num': 201, 'train_rates': 0.9600279156905205, 'learning_rate': 4.064613298079302e-05, 'batch_size': 145, 'step_size': 5, 'gamma': 0.796179408278498}. Best is trial 0 with value: 0.20430809259414673.[0m
[32m[I 2025-02-04 01:10:35,083][0m Trial 4 finished with value: 0.15667875842816795 and parameters: {'observation_period_num': 103, 'train_rates': 0.7654300908550992, 'learning_rate': 0.0005995671788027742, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9263772878251946}. Best is trial 4 with value: 0.15667875842816795.[0m
[32m[I 2025-02-04 01:12:02,584][0m Trial 5 finished with value: 0.46429892407881246 and parameters: {'observation_period_num': 241, 'train_rates': 0.9730531457259122, 'learning_rate': 5.493059906403956e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.918644787880358}. Best is trial 4 with value: 0.15667875842816795.[0m
[32m[I 2025-02-04 01:12:55,783][0m Trial 6 finished with value: 0.14201897382736206 and parameters: {'observation_period_num': 173, 'train_rates': 0.9873325514546354, 'learning_rate': 5.875577586706566e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.8765911343192273}. Best is trial 6 with value: 0.14201897382736206.[0m
[32m[I 2025-02-04 01:13:24,601][0m Trial 7 finished with value: 0.19384832989494755 and parameters: {'observation_period_num': 228, 'train_rates': 0.728122985531555, 'learning_rate': 5.200148728864217e-05, 'batch_size': 168, 'step_size': 4, 'gamma': 0.9883417877725766}. Best is trial 6 with value: 0.14201897382736206.[0m
[32m[I 2025-02-04 01:13:54,656][0m Trial 8 finished with value: 0.1640472998744563 and parameters: {'observation_period_num': 243, 'train_rates': 0.7552051339378671, 'learning_rate': 0.0006999558107132603, 'batch_size': 169, 'step_size': 9, 'gamma': 0.7986586678383019}. Best is trial 6 with value: 0.14201897382736206.[0m
Early stopping at epoch 46
[32m[I 2025-02-04 01:14:05,777][0m Trial 9 finished with value: 1.0738539134436937 and parameters: {'observation_period_num': 70, 'train_rates': 0.7327367991581366, 'learning_rate': 2.7928329499832487e-06, 'batch_size': 243, 'step_size': 1, 'gamma': 0.7805588707748189}. Best is trial 6 with value: 0.14201897382736206.[0m
[32m[I 2025-02-04 01:17:18,984][0m Trial 10 finished with value: 0.1493378497934544 and parameters: {'observation_period_num': 164, 'train_rates': 0.8739221070449414, 'learning_rate': 0.00013735722239614223, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8610311244147968}. Best is trial 6 with value: 0.14201897382736206.[0m
[32m[I 2025-02-04 01:20:17,319][0m Trial 11 finished with value: 0.11792909697686112 and parameters: {'observation_period_num': 165, 'train_rates': 0.870647185385887, 'learning_rate': 0.0001149369269564335, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8573662284570183}. Best is trial 11 with value: 0.11792909697686112.[0m
[32m[I 2025-02-04 01:24:47,314][0m Trial 12 finished with value: 0.1845832566090857 and parameters: {'observation_period_num': 152, 'train_rates': 0.8656201255108879, 'learning_rate': 0.00014655886995324616, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8619902302494045}. Best is trial 11 with value: 0.11792909697686112.[0m
[32m[I 2025-02-04 01:25:42,888][0m Trial 13 finished with value: 0.217981638692653 and parameters: {'observation_period_num': 187, 'train_rates': 0.6132660869087133, 'learning_rate': 0.0001323775083149215, 'batch_size': 76, 'step_size': 12, 'gamma': 0.8895477422723719}. Best is trial 11 with value: 0.11792909697686112.[0m
[32m[I 2025-02-04 01:27:20,855][0m Trial 14 finished with value: 0.10328292454264766 and parameters: {'observation_period_num': 131, 'train_rates': 0.9006529277431762, 'learning_rate': 2.5603900061158026e-05, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8381819686588728}. Best is trial 14 with value: 0.10328292454264766.[0m
[32m[I 2025-02-04 01:29:10,125][0m Trial 15 finished with value: 0.1786686106333657 and parameters: {'observation_period_num': 131, 'train_rates': 0.8887739800069965, 'learning_rate': 1.3312937877173733e-05, 'batch_size': 50, 'step_size': 10, 'gamma': 0.824853920470625}. Best is trial 14 with value: 0.10328292454264766.[0m
[32m[I 2025-02-04 01:31:05,791][0m Trial 16 finished with value: 0.1383712811654328 and parameters: {'observation_period_num': 124, 'train_rates': 0.8198392979192641, 'learning_rate': 1.699901360255864e-05, 'batch_size': 44, 'step_size': 14, 'gamma': 0.7505468669792257}. Best is trial 14 with value: 0.10328292454264766.[0m
[32m[I 2025-02-04 01:32:11,508][0m Trial 17 finished with value: 0.1011677739972418 and parameters: {'observation_period_num': 140, 'train_rates': 0.9063522548264947, 'learning_rate': 0.00032580222842653944, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8371748427852224}. Best is trial 17 with value: 0.1011677739972418.[0m
[32m[I 2025-02-04 01:33:15,122][0m Trial 18 finished with value: 0.08543543602327933 and parameters: {'observation_period_num': 127, 'train_rates': 0.9178668032301045, 'learning_rate': 0.0002675808482966798, 'batch_size': 89, 'step_size': 10, 'gamma': 0.8283060814721966}. Best is trial 18 with value: 0.08543543602327933.[0m
[32m[I 2025-02-04 01:34:19,286][0m Trial 19 finished with value: 0.08902849517762661 and parameters: {'observation_period_num': 96, 'train_rates': 0.9268722585388255, 'learning_rate': 0.00042352192240555667, 'batch_size': 89, 'step_size': 10, 'gamma': 0.895921350029389}. Best is trial 18 with value: 0.08543543602327933.[0m
[32m[I 2025-02-04 01:34:49,926][0m Trial 20 finished with value: 0.11949891597032547 and parameters: {'observation_period_num': 77, 'train_rates': 0.9350704236853837, 'learning_rate': 0.0003643691366295415, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9694510917053373}. Best is trial 18 with value: 0.08543543602327933.[0m
[32m[I 2025-02-04 01:35:50,045][0m Trial 21 finished with value: 0.07500006619529052 and parameters: {'observation_period_num': 107, 'train_rates': 0.9205614082538153, 'learning_rate': 0.00030491110361967387, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8947516892113078}. Best is trial 21 with value: 0.07500006619529052.[0m
[32m[I 2025-02-04 01:36:44,035][0m Trial 22 finished with value: 0.10113559347210509 and parameters: {'observation_period_num': 105, 'train_rates': 0.8382935819994644, 'learning_rate': 0.000994846111219759, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8983228833730915}. Best is trial 21 with value: 0.07500006619529052.[0m
[32m[I 2025-02-04 01:37:25,078][0m Trial 23 finished with value: 0.07371459180972853 and parameters: {'observation_period_num': 104, 'train_rates': 0.9368097961310503, 'learning_rate': 0.00029736409885203137, 'batch_size': 145, 'step_size': 7, 'gamma': 0.9462048427930075}. Best is trial 23 with value: 0.07371459180972853.[0m
[32m[I 2025-02-04 01:38:08,573][0m Trial 24 finished with value: 0.053216710686683655 and parameters: {'observation_period_num': 49, 'train_rates': 0.9523977963419504, 'learning_rate': 0.0002292222334847155, 'batch_size': 145, 'step_size': 7, 'gamma': 0.9588021176006437}. Best is trial 24 with value: 0.053216710686683655.[0m
[32m[I 2025-02-04 01:38:49,690][0m Trial 25 finished with value: 0.05398949980735779 and parameters: {'observation_period_num': 49, 'train_rates': 0.9538747864456594, 'learning_rate': 0.0002448148433329468, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9575019336139196}. Best is trial 24 with value: 0.053216710686683655.[0m
[32m[I 2025-02-04 01:39:32,470][0m Trial 26 finished with value: 0.08540981262922287 and parameters: {'observation_period_num': 48, 'train_rates': 0.954874412716282, 'learning_rate': 7.537337030618338e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.9535436355400178}. Best is trial 24 with value: 0.053216710686683655.[0m
[32m[I 2025-02-04 01:40:05,473][0m Trial 27 finished with value: 0.04621689021587372 and parameters: {'observation_period_num': 5, 'train_rates': 0.9859118372129413, 'learning_rate': 0.0002093570806673505, 'batch_size': 207, 'step_size': 3, 'gamma': 0.9466169606182986}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:40:36,652][0m Trial 28 finished with value: 0.04769285023212433 and parameters: {'observation_period_num': 7, 'train_rates': 0.9836148353585653, 'learning_rate': 0.00018846266462403135, 'batch_size': 212, 'step_size': 3, 'gamma': 0.9888224436136498}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:41:07,813][0m Trial 29 finished with value: 0.07023311406373978 and parameters: {'observation_period_num': 9, 'train_rates': 0.9875681022511108, 'learning_rate': 7.997878533074222e-05, 'batch_size': 219, 'step_size': 3, 'gamma': 0.9864183166404304}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:41:27,750][0m Trial 30 finished with value: 0.09161771161202988 and parameters: {'observation_period_num': 27, 'train_rates': 0.6557588631882565, 'learning_rate': 0.00019370351845643253, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9716207807159417}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:42:00,520][0m Trial 31 finished with value: 0.07439455389976501 and parameters: {'observation_period_num': 48, 'train_rates': 0.9582547662751768, 'learning_rate': 0.00021243468306604682, 'batch_size': 197, 'step_size': 2, 'gamma': 0.9485830192765119}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:42:29,612][0m Trial 32 finished with value: 0.06652189791202545 and parameters: {'observation_period_num': 30, 'train_rates': 0.9494781362736808, 'learning_rate': 0.000683166069672949, 'batch_size': 227, 'step_size': 4, 'gamma': 0.9708976919931948}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:43:05,361][0m Trial 33 finished with value: 0.06404568254947662 and parameters: {'observation_period_num': 61, 'train_rates': 0.9775898199009002, 'learning_rate': 0.00018887686710255595, 'batch_size': 187, 'step_size': 6, 'gamma': 0.9129561541939503}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:43:43,992][0m Trial 34 finished with value: 0.056802261620759964 and parameters: {'observation_period_num': 25, 'train_rates': 0.9632285521110082, 'learning_rate': 0.00010072534704131117, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9355283774796604}. Best is trial 27 with value: 0.04621689021587372.[0m
[32m[I 2025-02-04 01:44:28,459][0m Trial 35 finished with value: 0.0365267051863057 and parameters: {'observation_period_num': 12, 'train_rates': 0.8485365051990803, 'learning_rate': 0.000455422697749108, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9652504495370855}. Best is trial 35 with value: 0.0365267051863057.[0m
[32m[I 2025-02-04 01:45:11,364][0m Trial 36 finished with value: 0.038447799010217686 and parameters: {'observation_period_num': 6, 'train_rates': 0.7951523196417746, 'learning_rate': 0.00045252676587545314, 'batch_size': 127, 'step_size': 8, 'gamma': 0.9775007418767403}. Best is trial 35 with value: 0.0365267051863057.[0m
[32m[I 2025-02-04 01:45:53,973][0m Trial 37 finished with value: 0.03633093167739505 and parameters: {'observation_period_num': 7, 'train_rates': 0.7920378133929074, 'learning_rate': 0.00047978670546504245, 'batch_size': 125, 'step_size': 8, 'gamma': 0.9340320323396318}. Best is trial 37 with value: 0.03633093167739505.[0m
[32m[I 2025-02-04 01:46:38,855][0m Trial 38 finished with value: 0.050619885300260944 and parameters: {'observation_period_num': 19, 'train_rates': 0.7987877560187641, 'learning_rate': 0.0005462904108288576, 'batch_size': 118, 'step_size': 8, 'gamma': 0.9400455929055349}. Best is trial 37 with value: 0.03633093167739505.[0m
[32m[I 2025-02-04 01:47:20,254][0m Trial 39 finished with value: 0.05984828836464007 and parameters: {'observation_period_num': 35, 'train_rates': 0.7762709083328916, 'learning_rate': 0.0004885173727287022, 'batch_size': 126, 'step_size': 9, 'gamma': 0.9223307667528525}. Best is trial 37 with value: 0.03633093167739505.[0m
[32m[I 2025-02-04 01:48:04,326][0m Trial 40 finished with value: 0.03556377406554868 and parameters: {'observation_period_num': 5, 'train_rates': 0.8300375541412511, 'learning_rate': 0.0007277484788176399, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9316922471906258}. Best is trial 40 with value: 0.03556377406554868.[0m
[32m[I 2025-02-04 01:48:57,192][0m Trial 41 finished with value: 0.0316195549177272 and parameters: {'observation_period_num': 6, 'train_rates': 0.8435981698532012, 'learning_rate': 0.0009712789025079728, 'batch_size': 108, 'step_size': 8, 'gamma': 0.9133853704322381}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:49:48,939][0m Trial 42 finished with value: 0.04253460151021895 and parameters: {'observation_period_num': 18, 'train_rates': 0.8436511194867468, 'learning_rate': 0.0009823517214278118, 'batch_size': 111, 'step_size': 8, 'gamma': 0.9131943529174978}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:50:31,851][0m Trial 43 finished with value: 0.05191593313095521 and parameters: {'observation_period_num': 35, 'train_rates': 0.8156401007843812, 'learning_rate': 0.0007202743029323804, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9324913504064342}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:51:21,457][0m Trial 44 finished with value: 0.07882750788973075 and parameters: {'observation_period_num': 61, 'train_rates': 0.7855066513365558, 'learning_rate': 0.0008006984477654004, 'batch_size': 106, 'step_size': 9, 'gamma': 0.9766169666523395}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:52:08,265][0m Trial 45 finished with value: 0.04022458220956362 and parameters: {'observation_period_num': 18, 'train_rates': 0.8446120245413848, 'learning_rate': 0.0004932130513031094, 'batch_size': 123, 'step_size': 7, 'gamma': 0.9092164028858178}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:52:41,492][0m Trial 46 finished with value: 0.4141911865088352 and parameters: {'observation_period_num': 7, 'train_rates': 0.7501413539840972, 'learning_rate': 5.365657999302729e-06, 'batch_size': 163, 'step_size': 8, 'gamma': 0.8759591740164978}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:53:22,531][0m Trial 47 finished with value: 0.05261026002052757 and parameters: {'observation_period_num': 32, 'train_rates': 0.8031835281152508, 'learning_rate': 0.00045853786243968474, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9259063620664774}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:53:56,106][0m Trial 48 finished with value: 0.0957455652044017 and parameters: {'observation_period_num': 61, 'train_rates': 0.7084549898214605, 'learning_rate': 0.0006111343059495202, 'batch_size': 153, 'step_size': 9, 'gamma': 0.9596364011695513}. Best is trial 41 with value: 0.0316195549177272.[0m
[32m[I 2025-02-04 01:55:12,570][0m Trial 49 finished with value: 0.06164359246477759 and parameters: {'observation_period_num': 41, 'train_rates': 0.832573162911736, 'learning_rate': 0.0008384039629158277, 'batch_size': 70, 'step_size': 11, 'gamma': 0.9358901096628811}. Best is trial 41 with value: 0.0316195549177272.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4666 | 0.1530
Epoch 2/300, Loss: 0.1213 | 0.1497
Epoch 3/300, Loss: 0.1055 | 0.1882
Epoch 4/300, Loss: 0.0870 | 0.1697
Epoch 5/300, Loss: 0.0834 | 0.1652
Epoch 6/300, Loss: 0.0951 | 0.1131
Epoch 7/300, Loss: 0.0856 | 0.0804
Epoch 8/300, Loss: 0.0865 | 0.0808
Epoch 9/300, Loss: 0.0858 | 0.1551
Epoch 10/300, Loss: 0.0908 | 0.1178
Epoch 11/300, Loss: 0.0802 | 0.2033
Epoch 12/300, Loss: 0.0757 | 0.0706
Epoch 13/300, Loss: 0.0587 | 0.0630
Epoch 14/300, Loss: 0.0580 | 0.0669
Epoch 15/300, Loss: 0.0675 | 0.0669
Epoch 16/300, Loss: 0.0602 | 0.0615
Epoch 17/300, Loss: 0.0642 | 0.0642
Epoch 18/300, Loss: 0.0718 | 0.0685
Epoch 19/300, Loss: 0.0570 | 0.0528
Epoch 20/300, Loss: 0.0527 | 0.0592
Epoch 21/300, Loss: 0.0479 | 0.0570
Epoch 22/300, Loss: 0.0473 | 0.0502
Epoch 23/300, Loss: 0.0431 | 0.0540
Epoch 24/300, Loss: 0.0457 | 0.0702
Epoch 25/300, Loss: 0.0500 | 0.0537
Epoch 26/300, Loss: 0.0517 | 0.0567
Epoch 27/300, Loss: 0.0508 | 0.0721
Epoch 28/300, Loss: 0.0496 | 0.0524
Epoch 29/300, Loss: 0.0475 | 0.0549
Epoch 30/300, Loss: 0.0444 | 0.0610
Epoch 31/300, Loss: 0.0427 | 0.0456
Epoch 32/300, Loss: 0.0400 | 0.0481
Epoch 33/300, Loss: 0.0395 | 0.0499
Epoch 34/300, Loss: 0.0376 | 0.0436
Epoch 35/300, Loss: 0.0382 | 0.0474
Epoch 36/300, Loss: 0.0397 | 0.0503
Epoch 37/300, Loss: 0.0369 | 0.0425
Epoch 38/300, Loss: 0.0378 | 0.0485
Epoch 39/300, Loss: 0.0399 | 0.0531
Epoch 40/300, Loss: 0.0404 | 0.0458
Epoch 41/300, Loss: 0.0386 | 0.0479
Epoch 42/300, Loss: 0.0399 | 0.0498
Epoch 43/300, Loss: 0.0392 | 0.0474
Epoch 44/300, Loss: 0.0389 | 0.0474
Epoch 45/300, Loss: 0.0407 | 0.0433
Epoch 46/300, Loss: 0.0354 | 0.0431
Epoch 47/300, Loss: 0.0376 | 0.0480
Epoch 48/300, Loss: 0.0403 | 0.0449
Epoch 49/300, Loss: 0.0369 | 0.0412
Epoch 50/300, Loss: 0.0357 | 0.0441
Epoch 51/300, Loss: 0.0374 | 0.0409
Epoch 52/300, Loss: 0.0336 | 0.0395
Epoch 53/300, Loss: 0.0339 | 0.0412
Epoch 54/300, Loss: 0.0331 | 0.0400
Epoch 55/300, Loss: 0.0321 | 0.0409
Epoch 56/300, Loss: 0.0331 | 0.0393
Epoch 57/300, Loss: 0.0310 | 0.0389
Epoch 58/300, Loss: 0.0303 | 0.0395
Epoch 59/300, Loss: 0.0310 | 0.0387
Epoch 60/300, Loss: 0.0301 | 0.0383
Epoch 61/300, Loss: 0.0295 | 0.0378
Epoch 62/300, Loss: 0.0293 | 0.0372
Epoch 63/300, Loss: 0.0289 | 0.0366
Epoch 64/300, Loss: 0.0286 | 0.0364
Epoch 65/300, Loss: 0.0284 | 0.0361
Epoch 66/300, Loss: 0.0281 | 0.0358
Epoch 67/300, Loss: 0.0279 | 0.0356
Epoch 68/300, Loss: 0.0278 | 0.0354
Epoch 69/300, Loss: 0.0277 | 0.0353
Epoch 70/300, Loss: 0.0277 | 0.0353
Epoch 71/300, Loss: 0.0279 | 0.0355
Epoch 72/300, Loss: 0.0310 | 0.0393
Epoch 73/300, Loss: 0.0303 | 0.0377
Epoch 74/300, Loss: 0.0287 | 0.0355
Epoch 75/300, Loss: 0.0276 | 0.0343
Epoch 76/300, Loss: 0.0273 | 0.0353
Epoch 77/300, Loss: 0.0273 | 0.0348
Epoch 78/300, Loss: 0.0265 | 0.0355
Epoch 79/300, Loss: 0.0265 | 0.0347
Epoch 80/300, Loss: 0.0260 | 0.0341
Epoch 81/300, Loss: 0.0258 | 0.0341
Epoch 82/300, Loss: 0.0258 | 0.0343
Epoch 83/300, Loss: 0.0257 | 0.0346
Epoch 84/300, Loss: 0.0291 | 0.0391
Epoch 85/300, Loss: 0.0297 | 0.0380
Epoch 86/300, Loss: 0.0268 | 0.0351
Epoch 87/300, Loss: 0.0321 | 0.0394
Epoch 88/300, Loss: 0.0299 | 0.0397
Epoch 89/300, Loss: 0.0294 | 0.0381
Epoch 90/300, Loss: 0.0285 | 0.0381
Epoch 91/300, Loss: 0.0283 | 0.0378
Epoch 92/300, Loss: 0.0281 | 0.0376
Epoch 93/300, Loss: 0.0279 | 0.0376
Epoch 94/300, Loss: 0.0277 | 0.0372
Epoch 95/300, Loss: 0.0275 | 0.0369
Epoch 96/300, Loss: 0.0272 | 0.0361
Epoch 97/300, Loss: 0.0266 | 0.0347
Epoch 98/300, Loss: 0.0257 | 0.0341
Epoch 99/300, Loss: 0.0254 | 0.0340
Epoch 100/300, Loss: 0.0251 | 0.0336
Epoch 101/300, Loss: 0.0247 | 0.0336
Epoch 102/300, Loss: 0.0244 | 0.0330
Epoch 103/300, Loss: 0.0241 | 0.0328
Epoch 104/300, Loss: 0.0241 | 0.0327
Epoch 105/300, Loss: 0.0242 | 0.0324
Epoch 106/300, Loss: 0.0241 | 0.0324
Epoch 107/300, Loss: 0.0238 | 0.0324
Epoch 108/300, Loss: 0.0236 | 0.0323
Epoch 109/300, Loss: 0.0236 | 0.0322
Epoch 110/300, Loss: 0.0236 | 0.0323
Epoch 111/300, Loss: 0.0232 | 0.0323
Epoch 112/300, Loss: 0.0231 | 0.0323
Epoch 113/300, Loss: 0.0231 | 0.0323
Epoch 114/300, Loss: 0.0230 | 0.0324
Epoch 115/300, Loss: 0.0228 | 0.0323
Epoch 116/300, Loss: 0.0227 | 0.0323
Epoch 117/300, Loss: 0.0225 | 0.0323
Epoch 118/300, Loss: 0.0224 | 0.0322
Epoch 119/300, Loss: 0.0223 | 0.0322
Epoch 120/300, Loss: 0.0222 | 0.0321
Epoch 121/300, Loss: 0.0220 | 0.0320
Epoch 122/300, Loss: 0.0219 | 0.0319
Epoch 123/300, Loss: 0.0218 | 0.0319
Epoch 124/300, Loss: 0.0217 | 0.0318
Epoch 125/300, Loss: 0.0216 | 0.0318
Epoch 126/300, Loss: 0.0216 | 0.0317
Epoch 127/300, Loss: 0.0215 | 0.0317
Epoch 128/300, Loss: 0.0214 | 0.0317
Epoch 129/300, Loss: 0.0214 | 0.0316
Epoch 130/300, Loss: 0.0214 | 0.0316
Epoch 131/300, Loss: 0.0213 | 0.0316
Epoch 132/300, Loss: 0.0213 | 0.0315
Epoch 133/300, Loss: 0.0214 | 0.0315
Epoch 134/300, Loss: 0.0214 | 0.0314
Epoch 135/300, Loss: 0.0215 | 0.0314
Epoch 136/300, Loss: 0.0216 | 0.0314
Epoch 137/300, Loss: 0.0218 | 0.0316
Epoch 138/300, Loss: 0.0222 | 0.0319
Epoch 139/300, Loss: 0.0226 | 0.0322
Epoch 140/300, Loss: 0.0228 | 0.0321
Epoch 141/300, Loss: 0.0230 | 0.0318
Epoch 142/300, Loss: 0.0226 | 0.0317
Epoch 143/300, Loss: 0.0216 | 0.0322
Epoch 144/300, Loss: 0.0213 | 0.0318
Epoch 145/300, Loss: 0.0213 | 0.0318
Epoch 146/300, Loss: 0.0213 | 0.0316
Epoch 147/300, Loss: 0.0211 | 0.0319
Epoch 148/300, Loss: 0.0209 | 0.0320
Epoch 149/300, Loss: 0.0207 | 0.0319
Epoch 150/300, Loss: 0.0207 | 0.0319
Epoch 151/300, Loss: 0.0207 | 0.0319
Epoch 152/300, Loss: 0.0207 | 0.0318
Epoch 153/300, Loss: 0.0206 | 0.0318
Epoch 154/300, Loss: 0.0206 | 0.0318
Epoch 155/300, Loss: 0.0206 | 0.0318
Epoch 156/300, Loss: 0.0205 | 0.0317
Epoch 157/300, Loss: 0.0204 | 0.0316
Epoch 158/300, Loss: 0.0204 | 0.0316
Epoch 159/300, Loss: 0.0204 | 0.0316
Epoch 160/300, Loss: 0.0203 | 0.0316
Epoch 161/300, Loss: 0.0203 | 0.0315
Epoch 162/300, Loss: 0.0203 | 0.0315
Epoch 163/300, Loss: 0.0203 | 0.0315
Epoch 164/300, Loss: 0.0203 | 0.0314
Epoch 165/300, Loss: 0.0203 | 0.0314
Epoch 166/300, Loss: 0.0203 | 0.0313
Epoch 167/300, Loss: 0.0203 | 0.0313
Epoch 168/300, Loss: 0.0203 | 0.0313
Epoch 169/300, Loss: 0.0203 | 0.0313
Epoch 170/300, Loss: 0.0202 | 0.0313
Epoch 171/300, Loss: 0.0202 | 0.0313
Epoch 172/300, Loss: 0.0201 | 0.0313
Epoch 173/300, Loss: 0.0201 | 0.0313
Epoch 174/300, Loss: 0.0200 | 0.0313
Epoch 175/300, Loss: 0.0200 | 0.0313
Epoch 176/300, Loss: 0.0199 | 0.0313
Epoch 177/300, Loss: 0.0199 | 0.0313
Epoch 178/300, Loss: 0.0199 | 0.0313
Epoch 179/300, Loss: 0.0198 | 0.0313
Epoch 180/300, Loss: 0.0198 | 0.0313
Epoch 181/300, Loss: 0.0198 | 0.0313
Epoch 182/300, Loss: 0.0197 | 0.0313
Epoch 183/300, Loss: 0.0197 | 0.0313
Epoch 184/300, Loss: 0.0197 | 0.0313
Epoch 185/300, Loss: 0.0197 | 0.0313
Epoch 186/300, Loss: 0.0196 | 0.0313
Epoch 187/300, Loss: 0.0196 | 0.0313
Epoch 188/300, Loss: 0.0196 | 0.0313
Epoch 189/300, Loss: 0.0196 | 0.0313
Epoch 190/300, Loss: 0.0196 | 0.0313
Epoch 191/300, Loss: 0.0196 | 0.0313
Epoch 192/300, Loss: 0.0196 | 0.0313
Epoch 193/300, Loss: 0.0195 | 0.0313
Epoch 194/300, Loss: 0.0195 | 0.0313
Epoch 195/300, Loss: 0.0195 | 0.0313
Epoch 196/300, Loss: 0.0195 | 0.0313
Epoch 197/300, Loss: 0.0195 | 0.0313
Epoch 198/300, Loss: 0.0195 | 0.0313
Epoch 199/300, Loss: 0.0195 | 0.0313
Epoch 200/300, Loss: 0.0195 | 0.0313
Epoch 201/300, Loss: 0.0194 | 0.0313
Epoch 202/300, Loss: 0.0194 | 0.0313
Epoch 203/300, Loss: 0.0194 | 0.0313
Epoch 204/300, Loss: 0.0194 | 0.0313
Epoch 205/300, Loss: 0.0194 | 0.0313
Epoch 206/300, Loss: 0.0194 | 0.0313
Epoch 207/300, Loss: 0.0194 | 0.0313
Epoch 208/300, Loss: 0.0194 | 0.0313
Epoch 209/300, Loss: 0.0193 | 0.0313
Epoch 210/300, Loss: 0.0193 | 0.0313
Epoch 211/300, Loss: 0.0193 | 0.0313
Epoch 212/300, Loss: 0.0193 | 0.0313
Epoch 213/300, Loss: 0.0193 | 0.0313
Epoch 214/300, Loss: 0.0193 | 0.0313
Epoch 215/300, Loss: 0.0193 | 0.0313
Epoch 216/300, Loss: 0.0193 | 0.0313
Epoch 217/300, Loss: 0.0193 | 0.0313
Epoch 218/300, Loss: 0.0193 | 0.0313
Epoch 219/300, Loss: 0.0193 | 0.0313
Epoch 220/300, Loss: 0.0192 | 0.0313
Epoch 221/300, Loss: 0.0192 | 0.0313
Epoch 222/300, Loss: 0.0192 | 0.0313
Epoch 223/300, Loss: 0.0192 | 0.0313
Epoch 224/300, Loss: 0.0192 | 0.0313
Epoch 225/300, Loss: 0.0192 | 0.0313
Epoch 226/300, Loss: 0.0192 | 0.0313
Epoch 227/300, Loss: 0.0192 | 0.0313
Epoch 228/300, Loss: 0.0192 | 0.0313
Epoch 229/300, Loss: 0.0192 | 0.0313
Epoch 230/300, Loss: 0.0192 | 0.0313
Epoch 231/300, Loss: 0.0192 | 0.0313
Epoch 232/300, Loss: 0.0192 | 0.0313
Epoch 233/300, Loss: 0.0191 | 0.0313
Epoch 234/300, Loss: 0.0191 | 0.0313
Epoch 235/300, Loss: 0.0191 | 0.0313
Epoch 236/300, Loss: 0.0191 | 0.0313
Epoch 237/300, Loss: 0.0191 | 0.0313
Epoch 238/300, Loss: 0.0191 | 0.0313
Epoch 239/300, Loss: 0.0191 | 0.0313
Epoch 240/300, Loss: 0.0191 | 0.0313
Epoch 241/300, Loss: 0.0191 | 0.0313
Epoch 242/300, Loss: 0.0191 | 0.0313
Epoch 243/300, Loss: 0.0191 | 0.0313
Epoch 244/300, Loss: 0.0191 | 0.0313
Epoch 245/300, Loss: 0.0191 | 0.0313
Epoch 246/300, Loss: 0.0191 | 0.0313
Epoch 247/300, Loss: 0.0191 | 0.0313
Epoch 248/300, Loss: 0.0191 | 0.0313
Epoch 249/300, Loss: 0.0191 | 0.0313
Epoch 250/300, Loss: 0.0190 | 0.0313
Epoch 251/300, Loss: 0.0190 | 0.0313
Epoch 252/300, Loss: 0.0190 | 0.0313
Epoch 253/300, Loss: 0.0190 | 0.0313
Epoch 254/300, Loss: 0.0190 | 0.0313
Epoch 255/300, Loss: 0.0190 | 0.0313
Epoch 256/300, Loss: 0.0190 | 0.0313
Epoch 257/300, Loss: 0.0190 | 0.0313
Epoch 258/300, Loss: 0.0190 | 0.0313
Epoch 259/300, Loss: 0.0190 | 0.0313
Epoch 260/300, Loss: 0.0190 | 0.0313
Epoch 261/300, Loss: 0.0190 | 0.0313
Epoch 262/300, Loss: 0.0190 | 0.0312
Epoch 263/300, Loss: 0.0190 | 0.0312
Epoch 264/300, Loss: 0.0190 | 0.0312
Epoch 265/300, Loss: 0.0190 | 0.0312
Epoch 266/300, Loss: 0.0190 | 0.0312
Epoch 267/300, Loss: 0.0190 | 0.0312
Epoch 268/300, Loss: 0.0190 | 0.0312
Epoch 269/300, Loss: 0.0190 | 0.0312
Epoch 270/300, Loss: 0.0190 | 0.0312
Epoch 271/300, Loss: 0.0190 | 0.0312
Epoch 272/300, Loss: 0.0190 | 0.0312
Epoch 273/300, Loss: 0.0190 | 0.0312
Epoch 274/300, Loss: 0.0190 | 0.0312
Epoch 275/300, Loss: 0.0190 | 0.0312
Epoch 276/300, Loss: 0.0189 | 0.0312
Epoch 277/300, Loss: 0.0189 | 0.0312
Epoch 278/300, Loss: 0.0189 | 0.0312
Epoch 279/300, Loss: 0.0189 | 0.0312
Epoch 280/300, Loss: 0.0189 | 0.0312
Epoch 281/300, Loss: 0.0189 | 0.0312
Epoch 282/300, Loss: 0.0189 | 0.0312
Epoch 283/300, Loss: 0.0189 | 0.0312
Epoch 284/300, Loss: 0.0189 | 0.0312
Epoch 285/300, Loss: 0.0189 | 0.0312
Epoch 286/300, Loss: 0.0189 | 0.0312
Epoch 287/300, Loss: 0.0189 | 0.0312
Epoch 288/300, Loss: 0.0189 | 0.0312
Epoch 289/300, Loss: 0.0189 | 0.0312
Epoch 290/300, Loss: 0.0189 | 0.0312
Epoch 291/300, Loss: 0.0189 | 0.0312
Epoch 292/300, Loss: 0.0189 | 0.0312
Epoch 293/300, Loss: 0.0189 | 0.0312
Epoch 294/300, Loss: 0.0189 | 0.0312
Epoch 295/300, Loss: 0.0189 | 0.0312
Epoch 296/300, Loss: 0.0189 | 0.0312
Epoch 297/300, Loss: 0.0189 | 0.0312
Epoch 298/300, Loss: 0.0189 | 0.0312
Epoch 299/300, Loss: 0.0189 | 0.0312
Epoch 300/300, Loss: 0.0189 | 0.0312
Runtime (seconds): 159.0594127178192
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 11.298424184555188
RMSE: 3.3613128662109375
MAE: 3.3613128662109375
R-squared: nan
[117.64131]
