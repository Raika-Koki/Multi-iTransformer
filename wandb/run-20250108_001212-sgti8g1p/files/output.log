[32m[I 2025-01-08 00:12:16,518][0m A new study created in memory with name: no-name-f715793c-311c-41e0-af89-103738fa9cb3[0m
[32m[I 2025-01-08 00:14:53,456][0m Trial 0 finished with value: 0.2319385050243846 and parameters: {'observation_period_num': 53, 'train_rates': 0.8469279784614951, 'learning_rate': 0.00016982363338397698, 'batch_size': 23, 'step_size': 3, 'gamma': 0.8667953262487768}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:19:41,148][0m Trial 1 finished with value: 0.48399413717548234 and parameters: {'observation_period_num': 205, 'train_rates': 0.8403944641293514, 'learning_rate': 2.374939535531271e-05, 'batch_size': 253, 'step_size': 7, 'gamma': 0.946040100162072}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:23:20,455][0m Trial 2 finished with value: 0.9843911601708281 and parameters: {'observation_period_num': 193, 'train_rates': 0.6126212666564319, 'learning_rate': 0.0002446682540950039, 'batch_size': 196, 'step_size': 2, 'gamma': 0.9122876794900578}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:25:14,292][0m Trial 3 finished with value: 1.033750213717177 and parameters: {'observation_period_num': 110, 'train_rates': 0.6230505318938419, 'learning_rate': 3.0153747843829578e-05, 'batch_size': 177, 'step_size': 12, 'gamma': 0.8918384432442368}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:28:34,008][0m Trial 4 finished with value: 0.45131598449703575 and parameters: {'observation_period_num': 151, 'train_rates': 0.7554847917216124, 'learning_rate': 9.502990885942813e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.879999890378255}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:30:41,038][0m Trial 5 finished with value: 0.7887654314275647 and parameters: {'observation_period_num': 96, 'train_rates': 0.9364339921230583, 'learning_rate': 8.456681560844278e-06, 'batch_size': 142, 'step_size': 5, 'gamma': 0.7542135673722419}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:33:39,301][0m Trial 6 finished with value: 1.0557873523988734 and parameters: {'observation_period_num': 160, 'train_rates': 0.6509262653140321, 'learning_rate': 0.0007732714296252783, 'batch_size': 253, 'step_size': 11, 'gamma': 0.8275173635185689}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:36:23,305][0m Trial 7 finished with value: 0.7390848552826608 and parameters: {'observation_period_num': 145, 'train_rates': 0.6424882408973254, 'learning_rate': 3.670790010104382e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.8858644869370391}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:42:08,850][0m Trial 8 finished with value: 0.48254321191286176 and parameters: {'observation_period_num': 239, 'train_rates': 0.7882592377048976, 'learning_rate': 2.490870121690743e-05, 'batch_size': 33, 'step_size': 1, 'gamma': 0.9576564964110497}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:45:45,626][0m Trial 9 finished with value: 0.9283188115901104 and parameters: {'observation_period_num': 192, 'train_rates': 0.6498592177213948, 'learning_rate': 0.0006983631418801846, 'batch_size': 226, 'step_size': 8, 'gamma': 0.8757612178605895}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:46:31,369][0m Trial 10 finished with value: 1.8186322450637817 and parameters: {'observation_period_num': 13, 'train_rates': 0.9772448086870973, 'learning_rate': 1.0024344838144435e-06, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8159806591638773}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:49:34,152][0m Trial 11 finished with value: 0.4854058633056119 and parameters: {'observation_period_num': 54, 'train_rates': 0.7695076307231122, 'learning_rate': 0.00016161516203731278, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8388548740530387}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:50:50,354][0m Trial 12 finished with value: 0.23945855481881545 and parameters: {'observation_period_num': 50, 'train_rates': 0.8636137968503791, 'learning_rate': 0.00014073948515546457, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7885806998159091}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:52:04,298][0m Trial 13 finished with value: 0.2660858402164971 and parameters: {'observation_period_num': 56, 'train_rates': 0.8735565584449332, 'learning_rate': 0.00023709155919995924, 'batch_size': 83, 'step_size': 4, 'gamma': 0.7672788420500213}. Best is trial 0 with value: 0.2319385050243846.[0m
[32m[I 2025-01-08 00:53:08,511][0m Trial 14 finished with value: 0.18065116413765483 and parameters: {'observation_period_num': 12, 'train_rates': 0.8784279008161154, 'learning_rate': 7.841415006563293e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.7911499134032988}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 00:53:35,391][0m Trial 15 finished with value: 1.4855788036505935 and parameters: {'observation_period_num': 6, 'train_rates': 0.7095371741421679, 'learning_rate': 3.0081132329500616e-06, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7978133440655937}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 00:55:39,928][0m Trial 16 finished with value: 0.21616007728749012 and parameters: {'observation_period_num': 86, 'train_rates': 0.9138165695250777, 'learning_rate': 6.701826317929692e-05, 'batch_size': 52, 'step_size': 3, 'gamma': 0.8504412692856158}. Best is trial 14 with value: 0.18065116413765483.[0m
Early stopping at epoch 74
[32m[I 2025-01-08 00:57:07,265][0m Trial 17 finished with value: 0.45547486159761075 and parameters: {'observation_period_num': 86, 'train_rates': 0.9183985731789811, 'learning_rate': 6.408548425775894e-05, 'batch_size': 122, 'step_size': 1, 'gamma': 0.8444574779622611}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 00:58:13,910][0m Trial 18 finished with value: 0.35446139176686603 and parameters: {'observation_period_num': 36, 'train_rates': 0.9083026898458049, 'learning_rate': 9.49837374843911e-06, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8040109742586119}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 01:00:13,997][0m Trial 19 finished with value: 0.2298307567834854 and parameters: {'observation_period_num': 84, 'train_rates': 0.986916974527029, 'learning_rate': 9.872408447467857e-06, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9876442175375499}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 01:02:49,709][0m Trial 20 finished with value: 0.3467087859252714 and parameters: {'observation_period_num': 126, 'train_rates': 0.8138141728180099, 'learning_rate': 0.0004353761076975728, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8534152468123412}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 01:04:46,283][0m Trial 21 finished with value: 0.2683786153793335 and parameters: {'observation_period_num': 81, 'train_rates': 0.9731043393881088, 'learning_rate': 1.1032769086073392e-05, 'batch_size': 98, 'step_size': 6, 'gamma': 0.9858756213049066}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 01:05:40,499][0m Trial 22 finished with value: 0.19091855942392025 and parameters: {'observation_period_num': 26, 'train_rates': 0.9501080368269798, 'learning_rate': 5.3859529872039915e-05, 'batch_size': 76, 'step_size': 6, 'gamma': 0.916590814501445}. Best is trial 14 with value: 0.18065116413765483.[0m
[32m[I 2025-01-08 01:06:39,559][0m Trial 23 finished with value: 0.14297612245987962 and parameters: {'observation_period_num': 29, 'train_rates': 0.8899966169870213, 'learning_rate': 6.324407359130397e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.9084177493443198}. Best is trial 23 with value: 0.14297612245987962.[0m
[32m[I 2025-01-08 01:07:40,496][0m Trial 24 finished with value: 0.18333555895973136 and parameters: {'observation_period_num': 27, 'train_rates': 0.9452264857194709, 'learning_rate': 5.1572315020712966e-05, 'batch_size': 67, 'step_size': 9, 'gamma': 0.9148310903012447}. Best is trial 23 with value: 0.14297612245987962.[0m
[32m[I 2025-01-08 01:08:38,542][0m Trial 25 finished with value: 0.19779539403692975 and parameters: {'observation_period_num': 28, 'train_rates': 0.883636583368086, 'learning_rate': 1.9479671191421955e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.9145800408692432}. Best is trial 23 with value: 0.14297612245987962.[0m
[32m[I 2025-01-08 01:10:16,414][0m Trial 26 finished with value: 0.11896641566374591 and parameters: {'observation_period_num': 13, 'train_rates': 0.891739592443982, 'learning_rate': 0.00010079850389410729, 'batch_size': 39, 'step_size': 13, 'gamma': 0.937475811532838}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:11:44,992][0m Trial 27 finished with value: 0.12342438106425106 and parameters: {'observation_period_num': 5, 'train_rates': 0.892308228511762, 'learning_rate': 0.00010227735121383266, 'batch_size': 43, 'step_size': 15, 'gamma': 0.9404833939335154}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:13:22,216][0m Trial 28 finished with value: 0.6362058160388294 and parameters: {'observation_period_num': 40, 'train_rates': 0.8144996468207428, 'learning_rate': 0.0004723165858355471, 'batch_size': 37, 'step_size': 15, 'gamma': 0.9443507201738722}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:15:13,830][0m Trial 29 finished with value: 0.35678443278708566 and parameters: {'observation_period_num': 68, 'train_rates': 0.8337666031840785, 'learning_rate': 0.00011358942456246786, 'batch_size': 34, 'step_size': 13, 'gamma': 0.9332834704037288}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:18:37,200][0m Trial 30 finished with value: 0.13893779797987504 and parameters: {'observation_period_num': 6, 'train_rates': 0.9075555567518749, 'learning_rate': 0.0002232539532960114, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9741132028809557}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:22:05,827][0m Trial 31 finished with value: 0.18362746051322656 and parameters: {'observation_period_num': 5, 'train_rates': 0.8993579481449151, 'learning_rate': 0.00023510618080404937, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9676136134261083}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:23:46,197][0m Trial 32 finished with value: 0.5002321413269749 and parameters: {'observation_period_num': 25, 'train_rates': 0.8352042926088197, 'learning_rate': 0.0003707474388812965, 'batch_size': 36, 'step_size': 13, 'gamma': 0.9653444016526469}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:25:38,275][0m Trial 33 finished with value: 0.2896217554807663 and parameters: {'observation_period_num': 66, 'train_rates': 0.8596390925698711, 'learning_rate': 0.00018073774061012232, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9340352938205466}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:29:30,904][0m Trial 34 finished with value: 0.17611959157660365 and parameters: {'observation_period_num': 45, 'train_rates': 0.8932449345028668, 'learning_rate': 0.00011300622234444385, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8995727510034817}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:31:05,201][0m Trial 35 finished with value: 0.14573369372416944 and parameters: {'observation_period_num': 18, 'train_rates': 0.9312088931830127, 'learning_rate': 4.110913358459269e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9320763592070483}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:31:56,868][0m Trial 36 finished with value: 0.29728554166380633 and parameters: {'observation_period_num': 38, 'train_rates': 0.8444432749022937, 'learning_rate': 1.7278649134173866e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.9508708626302704}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:32:50,803][0m Trial 37 finished with value: 0.208120801619121 and parameters: {'observation_period_num': 5, 'train_rates': 0.9530252141684435, 'learning_rate': 0.00033818710630998967, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9023442181735899}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:35:10,344][0m Trial 38 finished with value: 0.6660910606994044 and parameters: {'observation_period_num': 116, 'train_rates': 0.7262502049120397, 'learning_rate': 9.20117006151641e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9747973070958404}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:36:46,119][0m Trial 39 finished with value: 0.18671892528180722 and parameters: {'observation_period_num': 71, 'train_rates': 0.9257342390862495, 'learning_rate': 0.00019382044186547827, 'batch_size': 176, 'step_size': 11, 'gamma': 0.9438262185375869}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:37:16,539][0m Trial 40 finished with value: 0.9623472233463947 and parameters: {'observation_period_num': 22, 'train_rates': 0.8196736886567643, 'learning_rate': 0.0009540950332251713, 'batch_size': 216, 'step_size': 11, 'gamma': 0.9270542925036096}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:38:50,050][0m Trial 41 finished with value: 0.1377316108606394 and parameters: {'observation_period_num': 16, 'train_rates': 0.9304989705684272, 'learning_rate': 3.56337731892344e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9586861634878683}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:41:21,773][0m Trial 42 finished with value: 0.12846614521438793 and parameters: {'observation_period_num': 34, 'train_rates': 0.9598615439587094, 'learning_rate': 3.193218932666279e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9551670969361414}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:43:53,920][0m Trial 43 finished with value: 0.16573423669854206 and parameters: {'observation_period_num': 18, 'train_rates': 0.9546366617757102, 'learning_rate': 3.771873935833327e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9597989822797154}. Best is trial 26 with value: 0.11896641566374591.[0m
[32m[I 2025-01-08 01:48:43,852][0m Trial 44 finished with value: 0.11568503754168022 and parameters: {'observation_period_num': 177, 'train_rates': 0.9706422732130692, 'learning_rate': 1.5565514275848938e-05, 'batch_size': 26, 'step_size': 15, 'gamma': 0.9516114146616432}. Best is trial 44 with value: 0.11568503754168022.[0m
[32m[I 2025-01-08 01:53:04,504][0m Trial 45 finished with value: 0.19627800656528008 and parameters: {'observation_period_num': 168, 'train_rates': 0.9708917845061438, 'learning_rate': 6.271003545816642e-06, 'batch_size': 43, 'step_size': 13, 'gamma': 0.9549055035480674}. Best is trial 44 with value: 0.11568503754168022.[0m
[32m[I 2025-01-08 01:59:55,666][0m Trial 46 finished with value: 0.11984567488860158 and parameters: {'observation_period_num': 245, 'train_rates': 0.9625196607701497, 'learning_rate': 2.603362487576576e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.9240616796651026}. Best is trial 44 with value: 0.11568503754168022.[0m
[32m[I 2025-01-08 02:06:52,376][0m Trial 47 finished with value: 0.13894290339599535 and parameters: {'observation_period_num': 249, 'train_rates': 0.962263872745032, 'learning_rate': 1.665199901125643e-05, 'batch_size': 28, 'step_size': 15, 'gamma': 0.9238456105481042}. Best is trial 44 with value: 0.11568503754168022.[0m
[32m[I 2025-01-08 02:13:03,266][0m Trial 48 finished with value: 0.20847728668925275 and parameters: {'observation_period_num': 224, 'train_rates': 0.9698403723337629, 'learning_rate': 5.091608421740803e-06, 'batch_size': 26, 'step_size': 12, 'gamma': 0.8669046346755453}. Best is trial 44 with value: 0.11568503754168022.[0m
[32m[I 2025-01-08 02:17:55,911][0m Trial 49 finished with value: 0.1377226859331131 and parameters: {'observation_period_num': 185, 'train_rates': 0.98946770425876, 'learning_rate': 1.9756039438138948e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.8863487339820505}. Best is trial 44 with value: 0.11568503754168022.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7732 | 1.1213
Epoch 2/300, Loss: 0.5213 | 0.8699
Epoch 3/300, Loss: 0.4151 | 0.7317
Epoch 4/300, Loss: 0.3506 | 0.6328
Epoch 5/300, Loss: 0.3043 | 0.5611
Epoch 6/300, Loss: 0.2726 | 0.5119
Epoch 7/300, Loss: 0.2502 | 0.4718
Epoch 8/300, Loss: 0.2337 | 0.4375
Epoch 9/300, Loss: 0.2209 | 0.4108
Epoch 10/300, Loss: 0.2108 | 0.3887
Epoch 11/300, Loss: 0.2033 | 0.3692
Epoch 12/300, Loss: 0.1967 | 0.3516
Epoch 13/300, Loss: 0.1907 | 0.3332
Epoch 14/300, Loss: 0.1860 | 0.3192
Epoch 15/300, Loss: 0.1821 | 0.3046
Epoch 16/300, Loss: 0.1786 | 0.2943
Epoch 17/300, Loss: 0.1752 | 0.2822
Epoch 18/300, Loss: 0.1728 | 0.2718
Epoch 19/300, Loss: 0.1713 | 0.2658
Epoch 20/300, Loss: 0.1684 | 0.2601
Epoch 21/300, Loss: 0.1658 | 0.2493
Epoch 22/300, Loss: 0.1649 | 0.2414
Epoch 23/300, Loss: 0.1635 | 0.2357
Epoch 24/300, Loss: 0.1640 | 0.2319
Epoch 25/300, Loss: 0.1626 | 0.2249
Epoch 26/300, Loss: 0.1609 | 0.2195
Epoch 27/300, Loss: 0.1598 | 0.2149
Epoch 28/300, Loss: 0.1600 | 0.2119
Epoch 29/300, Loss: 0.1624 | 0.2213
Epoch 30/300, Loss: 0.1622 | 0.2149
Epoch 31/300, Loss: 0.1586 | 0.2048
Epoch 32/300, Loss: 0.1588 | 0.2083
Epoch 33/300, Loss: 0.1574 | 0.1933
Epoch 34/300, Loss: 0.1544 | 0.1951
Epoch 35/300, Loss: 0.1540 | 0.1904
Epoch 36/300, Loss: 0.1527 | 0.1844
Epoch 37/300, Loss: 0.1507 | 0.1848
Epoch 38/300, Loss: 0.1508 | 0.1941
Epoch 39/300, Loss: 0.1494 | 0.1807
Epoch 40/300, Loss: 0.1484 | 0.1789
Epoch 41/300, Loss: 0.1468 | 0.1756
Epoch 42/300, Loss: 0.1447 | 0.1786
Epoch 43/300, Loss: 0.1425 | 0.1712
Epoch 44/300, Loss: 0.1437 | 0.1669
Epoch 45/300, Loss: 0.1416 | 0.1675
Epoch 46/300, Loss: 0.1421 | 0.1735
Epoch 47/300, Loss: 0.1394 | 0.1640
Epoch 48/300, Loss: 0.1394 | 0.1614
Epoch 49/300, Loss: 0.1384 | 0.1588
Epoch 50/300, Loss: 0.1377 | 0.1646
Epoch 51/300, Loss: 0.1376 | 0.1593
Epoch 52/300, Loss: 0.1370 | 0.1556
Epoch 53/300, Loss: 0.1374 | 0.1545
Epoch 54/300, Loss: 0.1361 | 0.1591
Epoch 55/300, Loss: 0.1364 | 0.1569
Epoch 56/300, Loss: 0.1357 | 0.1501
Epoch 57/300, Loss: 0.1358 | 0.1469
Epoch 58/300, Loss: 0.1333 | 0.1503
Epoch 59/300, Loss: 0.1334 | 0.1535
Epoch 60/300, Loss: 0.1317 | 0.1457
Epoch 61/300, Loss: 0.1328 | 0.1443
Epoch 62/300, Loss: 0.1306 | 0.1471
Epoch 63/300, Loss: 0.1311 | 0.1486
Epoch 64/300, Loss: 0.1290 | 0.1401
Epoch 65/300, Loss: 0.1303 | 0.1379
Epoch 66/300, Loss: 0.1272 | 0.1381
Epoch 67/300, Loss: 0.1273 | 0.1434
Epoch 68/300, Loss: 0.1261 | 0.1372
Epoch 69/300, Loss: 0.1259 | 0.1343
Epoch 70/300, Loss: 0.1252 | 0.1321
Epoch 71/300, Loss: 0.1240 | 0.1371
Epoch 72/300, Loss: 0.1240 | 0.1353
Epoch 73/300, Loss: 0.1239 | 0.1308
Epoch 74/300, Loss: 0.1236 | 0.1321
Epoch 75/300, Loss: 0.1235 | 0.1384
Epoch 76/300, Loss: 0.1232 | 0.1324
Epoch 77/300, Loss: 0.1222 | 0.1272
Epoch 78/300, Loss: 0.1224 | 0.1260
Epoch 79/300, Loss: 0.1206 | 0.1274
Epoch 80/300, Loss: 0.1218 | 0.1301
Epoch 81/300, Loss: 0.1216 | 0.1258
Epoch 82/300, Loss: 0.1206 | 0.1238
Epoch 83/300, Loss: 0.1205 | 0.1231
Epoch 84/300, Loss: 0.1211 | 0.1271
Epoch 85/300, Loss: 0.1191 | 0.1243
Epoch 86/300, Loss: 0.1189 | 0.1203
Epoch 87/300, Loss: 0.1171 | 0.1208
Epoch 88/300, Loss: 0.1180 | 0.1251
Epoch 89/300, Loss: 0.1171 | 0.1223
Epoch 90/300, Loss: 0.1165 | 0.1186
Epoch 91/300, Loss: 0.1164 | 0.1170
Epoch 92/300, Loss: 0.1156 | 0.1206
Epoch 93/300, Loss: 0.1156 | 0.1217
Epoch 94/300, Loss: 0.1146 | 0.1176
Epoch 95/300, Loss: 0.1135 | 0.1159
Epoch 96/300, Loss: 0.1138 | 0.1167
Epoch 97/300, Loss: 0.1141 | 0.1172
Epoch 98/300, Loss: 0.1135 | 0.1182
Epoch 99/300, Loss: 0.1140 | 0.1151
Epoch 100/300, Loss: 0.1138 | 0.1140
Epoch 101/300, Loss: 0.1126 | 0.1166
Epoch 102/300, Loss: 0.1155 | 0.1143
Epoch 103/300, Loss: 0.1114 | 0.1124
Epoch 104/300, Loss: 0.1125 | 0.1119
Epoch 105/300, Loss: 0.1110 | 0.1133
Epoch 106/300, Loss: 0.1120 | 0.1136
Epoch 107/300, Loss: 0.1110 | 0.1135
Epoch 108/300, Loss: 0.1099 | 0.1103
Epoch 109/300, Loss: 0.1101 | 0.1103
Epoch 110/300, Loss: 0.1084 | 0.1108
Epoch 111/300, Loss: 0.1094 | 0.1106
Epoch 112/300, Loss: 0.1086 | 0.1093
Epoch 113/300, Loss: 0.1088 | 0.1085
Epoch 114/300, Loss: 0.1073 | 0.1091
Epoch 115/300, Loss: 0.1073 | 0.1108
Epoch 116/300, Loss: 0.1073 | 0.1094
Epoch 117/300, Loss: 0.1071 | 0.1077
Epoch 118/300, Loss: 0.1070 | 0.1063
Epoch 119/300, Loss: 0.1059 | 0.1079
Epoch 120/300, Loss: 0.1062 | 0.1090
Epoch 121/300, Loss: 0.1055 | 0.1082
Epoch 122/300, Loss: 0.1050 | 0.1070
Epoch 123/300, Loss: 0.1046 | 0.1082
Epoch 124/300, Loss: 0.1052 | 0.1082
Epoch 125/300, Loss: 0.1053 | 0.1058
Epoch 126/300, Loss: 0.1040 | 0.1051
Epoch 127/300, Loss: 0.1049 | 0.1049
Epoch 128/300, Loss: 0.1042 | 0.1063
Epoch 129/300, Loss: 0.1055 | 0.1096
Epoch 130/300, Loss: 0.1051 | 0.1072
Epoch 131/300, Loss: 0.1046 | 0.1050
Epoch 132/300, Loss: 0.1043 | 0.1057
Epoch 133/300, Loss: 0.1040 | 0.1048
Epoch 134/300, Loss: 0.1041 | 0.1051
Epoch 135/300, Loss: 0.1028 | 0.1044
Epoch 136/300, Loss: 0.1038 | 0.1035
Epoch 137/300, Loss: 0.1029 | 0.1043
Epoch 138/300, Loss: 0.1014 | 0.1041
Epoch 139/300, Loss: 0.1011 | 0.1029
Epoch 140/300, Loss: 0.1006 | 0.1030
Epoch 141/300, Loss: 0.1000 | 0.1035
Epoch 142/300, Loss: 0.1001 | 0.1032
Epoch 143/300, Loss: 0.1002 | 0.1020
Epoch 144/300, Loss: 0.0997 | 0.1008
Epoch 145/300, Loss: 0.0998 | 0.1017
Epoch 146/300, Loss: 0.0992 | 0.1028
Epoch 147/300, Loss: 0.0990 | 0.1021
Epoch 148/300, Loss: 0.0988 | 0.1019
Epoch 149/300, Loss: 0.0988 | 0.1013
Epoch 150/300, Loss: 0.0991 | 0.1016
Epoch 151/300, Loss: 0.0983 | 0.1027
Epoch 152/300, Loss: 0.0983 | 0.1014
Epoch 153/300, Loss: 0.0986 | 0.1006
Epoch 154/300, Loss: 0.0973 | 0.1006
Epoch 155/300, Loss: 0.0971 | 0.1009
Epoch 156/300, Loss: 0.0972 | 0.1019
Epoch 157/300, Loss: 0.0966 | 0.1022
Epoch 158/300, Loss: 0.0969 | 0.1018
Epoch 159/300, Loss: 0.0964 | 0.1019
Epoch 160/300, Loss: 0.0972 | 0.1004
Epoch 161/300, Loss: 0.0965 | 0.0994
Epoch 162/300, Loss: 0.0963 | 0.0990
Epoch 163/300, Loss: 0.0959 | 0.0989
Epoch 164/300, Loss: 0.0959 | 0.1000
Epoch 165/300, Loss: 0.0981 | 0.1014
Epoch 166/300, Loss: 0.0966 | 0.0994
Epoch 167/300, Loss: 0.0967 | 0.0988
Epoch 168/300, Loss: 0.0975 | 0.0996
Epoch 169/300, Loss: 0.0971 | 0.0998
Epoch 170/300, Loss: 0.0967 | 0.0980
Epoch 171/300, Loss: 0.0956 | 0.0985
Epoch 172/300, Loss: 0.0952 | 0.0984
Epoch 173/300, Loss: 0.0957 | 0.0985
Epoch 174/300, Loss: 0.0956 | 0.0980
Epoch 175/300, Loss: 0.0959 | 0.0969
Epoch 176/300, Loss: 0.0946 | 0.0979
Epoch 177/300, Loss: 0.0947 | 0.0986
Epoch 178/300, Loss: 0.0937 | 0.0988
Epoch 179/300, Loss: 0.0940 | 0.0988
Epoch 180/300, Loss: 0.0940 | 0.0981
Epoch 181/300, Loss: 0.0936 | 0.0979
Epoch 182/300, Loss: 0.0928 | 0.0987
Epoch 183/300, Loss: 0.0929 | 0.0993
Epoch 184/300, Loss: 0.0926 | 0.0991
Epoch 185/300, Loss: 0.0930 | 0.0984
Epoch 186/300, Loss: 0.0927 | 0.0970
Epoch 187/300, Loss: 0.0916 | 0.0967
Epoch 188/300, Loss: 0.0924 | 0.0967
Epoch 189/300, Loss: 0.0924 | 0.0980
Epoch 190/300, Loss: 0.0923 | 0.0978
Epoch 191/300, Loss: 0.0924 | 0.0963
Epoch 192/300, Loss: 0.0923 | 0.0960
Epoch 193/300, Loss: 0.0916 | 0.0959
Epoch 194/300, Loss: 0.0911 | 0.0960
Epoch 195/300, Loss: 0.0909 | 0.0958
Epoch 196/300, Loss: 0.0906 | 0.0965
Epoch 197/300, Loss: 0.0912 | 0.0964
Epoch 198/300, Loss: 0.0903 | 0.0959
Epoch 199/300, Loss: 0.0906 | 0.0952
Epoch 200/300, Loss: 0.0913 | 0.0947
Epoch 201/300, Loss: 0.0904 | 0.0955
Epoch 202/300, Loss: 0.0905 | 0.0964
Epoch 203/300, Loss: 0.0907 | 0.0986
Epoch 204/300, Loss: 0.0897 | 0.0968
Epoch 205/300, Loss: 0.0903 | 0.0958
Epoch 206/300, Loss: 0.0898 | 0.0958
Epoch 207/300, Loss: 0.0894 | 0.0955
Epoch 208/300, Loss: 0.0892 | 0.0965
Epoch 209/300, Loss: 0.0889 | 0.0992
Epoch 210/300, Loss: 0.0896 | 0.0978
Epoch 211/300, Loss: 0.0894 | 0.0973
Epoch 212/300, Loss: 0.0892 | 0.0952
Epoch 213/300, Loss: 0.0887 | 0.0948
Epoch 214/300, Loss: 0.0894 | 0.0951
Epoch 215/300, Loss: 0.0905 | 0.0963
Epoch 216/300, Loss: 0.0882 | 0.0963
Epoch 217/300, Loss: 0.0899 | 0.0951
Epoch 218/300, Loss: 0.0891 | 0.0942
Epoch 219/300, Loss: 0.0882 | 0.0937
Epoch 220/300, Loss: 0.0877 | 0.0947
Epoch 221/300, Loss: 0.0876 | 0.0952
Epoch 222/300, Loss: 0.0883 | 0.0942
Epoch 223/300, Loss: 0.0873 | 0.0938
Epoch 224/300, Loss: 0.0874 | 0.0939
Epoch 225/300, Loss: 0.0870 | 0.0934
Epoch 226/300, Loss: 0.0871 | 0.0941
Epoch 227/300, Loss: 0.0869 | 0.0942
Epoch 228/300, Loss: 0.0866 | 0.0946
Epoch 229/300, Loss: 0.0870 | 0.0941
Epoch 230/300, Loss: 0.0860 | 0.0943
Epoch 231/300, Loss: 0.0858 | 0.0935
Epoch 232/300, Loss: 0.0865 | 0.0933
Epoch 233/300, Loss: 0.0862 | 0.0946
Epoch 234/300, Loss: 0.0858 | 0.0956
Epoch 235/300, Loss: 0.0860 | 0.0955
Epoch 236/300, Loss: 0.0867 | 0.0953
Epoch 237/300, Loss: 0.0861 | 0.0938
Epoch 238/300, Loss: 0.0863 | 0.0935
Epoch 239/300, Loss: 0.0867 | 0.0946
Epoch 240/300, Loss: 0.0872 | 0.0969
Epoch 241/300, Loss: 0.0872 | 0.0945
Epoch 242/300, Loss: 0.0864 | 0.0929
Epoch 243/300, Loss: 0.0861 | 0.0922
Epoch 244/300, Loss: 0.0854 | 0.0926
Epoch 245/300, Loss: 0.0858 | 0.0943
Epoch 246/300, Loss: 0.0848 | 0.0936
Epoch 247/300, Loss: 0.0847 | 0.0924
Epoch 248/300, Loss: 0.0848 | 0.0924
Epoch 249/300, Loss: 0.0852 | 0.0924
Epoch 250/300, Loss: 0.0850 | 0.0927
Epoch 251/300, Loss: 0.0843 | 0.0929
Epoch 252/300, Loss: 0.0848 | 0.0935
Epoch 253/300, Loss: 0.0839 | 0.0928
Epoch 254/300, Loss: 0.0838 | 0.0927
Epoch 255/300, Loss: 0.0846 | 0.0930
Epoch 256/300, Loss: 0.0841 | 0.0929
Epoch 257/300, Loss: 0.0837 | 0.0926
Epoch 258/300, Loss: 0.0837 | 0.0940
Epoch 259/300, Loss: 0.0836 | 0.0953
Epoch 260/300, Loss: 0.0835 | 0.0943
Epoch 261/300, Loss: 0.0835 | 0.0936
Epoch 262/300, Loss: 0.0828 | 0.0929
Epoch 263/300, Loss: 0.0842 | 0.0937
Epoch 264/300, Loss: 0.0827 | 0.0940
Epoch 265/300, Loss: 0.0830 | 0.0933
Epoch 266/300, Loss: 0.0835 | 0.0927
Epoch 267/300, Loss: 0.0838 | 0.0928
Epoch 268/300, Loss: 0.0830 | 0.0937
Epoch 269/300, Loss: 0.0832 | 0.0946
Epoch 270/300, Loss: 0.0831 | 0.0925
Epoch 271/300, Loss: 0.0828 | 0.0920
Epoch 272/300, Loss: 0.0825 | 0.0918
Epoch 273/300, Loss: 0.0823 | 0.0919
Epoch 274/300, Loss: 0.0823 | 0.0932
Epoch 275/300, Loss: 0.0818 | 0.0917
Epoch 276/300, Loss: 0.0826 | 0.0912
Epoch 277/300, Loss: 0.0825 | 0.0908
Epoch 278/300, Loss: 0.0817 | 0.0909
Epoch 279/300, Loss: 0.0812 | 0.0918
Epoch 280/300, Loss: 0.0814 | 0.0915
Epoch 281/300, Loss: 0.0818 | 0.0926
Epoch 282/300, Loss: 0.0821 | 0.0913
Epoch 283/300, Loss: 0.0816 | 0.0917
Epoch 284/300, Loss: 0.0822 | 0.0911
Epoch 285/300, Loss: 0.0816 | 0.0912
Epoch 286/300, Loss: 0.0829 | 0.0924
Epoch 287/300, Loss: 0.0817 | 0.0935
Epoch 288/300, Loss: 0.0816 | 0.0939
Epoch 289/300, Loss: 0.0816 | 0.0935
Epoch 290/300, Loss: 0.0819 | 0.0915
Epoch 291/300, Loss: 0.0816 | 0.0914
Epoch 292/300, Loss: 0.0820 | 0.0921
Epoch 293/300, Loss: 0.0810 | 0.0929
Epoch 294/300, Loss: 0.0809 | 0.0948
Epoch 295/300, Loss: 0.0813 | 0.0927
Epoch 296/300, Loss: 0.0815 | 0.0917
Epoch 297/300, Loss: 0.0810 | 0.0911
Epoch 298/300, Loss: 0.0803 | 0.0920
Epoch 299/300, Loss: 0.0808 | 0.0918
Epoch 300/300, Loss: 0.0805 | 0.0924
Runtime (seconds): 870.9762229919434
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 275.2375980189536
RMSE: 16.590286254882812
MAE: 16.590286254882812
R-squared: nan
[238.67972]
