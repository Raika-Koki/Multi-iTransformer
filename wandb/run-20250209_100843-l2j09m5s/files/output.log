[32m[I 2025-02-09 10:08:48,119][0m A new study created in memory with name: no-name-9056641e-ef8f-4433-b9db-b1175f904298[0m
[32m[I 2025-02-09 10:09:16,431][0m Trial 0 finished with value: 0.2826731045988916 and parameters: {'observation_period_num': 125, 'train_rates': 0.6344328532772702, 'learning_rate': 0.0002611045628531163, 'batch_size': 177, 'step_size': 3, 'gamma': 0.9755688981243065}. Best is trial 0 with value: 0.2826731045988916.[0m
[32m[I 2025-02-09 10:09:44,447][0m Trial 1 finished with value: 0.2440123341249344 and parameters: {'observation_period_num': 50, 'train_rates': 0.8196879792966634, 'learning_rate': 3.1394925783269196e-05, 'batch_size': 219, 'step_size': 3, 'gamma': 0.8568686264849349}. Best is trial 1 with value: 0.2440123341249344.[0m
[32m[I 2025-02-09 10:10:10,495][0m Trial 2 finished with value: 0.21159834711804892 and parameters: {'observation_period_num': 136, 'train_rates': 0.6123392818471154, 'learning_rate': 0.000308904615101077, 'batch_size': 179, 'step_size': 5, 'gamma': 0.982620530454225}. Best is trial 2 with value: 0.21159834711804892.[0m
[32m[I 2025-02-09 10:11:11,058][0m Trial 3 finished with value: 0.30109409067560644 and parameters: {'observation_period_num': 8, 'train_rates': 0.8866349185954889, 'learning_rate': 1.4342141863205064e-06, 'batch_size': 98, 'step_size': 12, 'gamma': 0.7712577111936177}. Best is trial 2 with value: 0.21159834711804892.[0m
[32m[I 2025-02-09 10:12:44,907][0m Trial 4 finished with value: 0.3437231987818608 and parameters: {'observation_period_num': 48, 'train_rates': 0.7084897600097656, 'learning_rate': 4.917312261728884e-06, 'batch_size': 51, 'step_size': 7, 'gamma': 0.8645450370828078}. Best is trial 2 with value: 0.21159834711804892.[0m
[32m[I 2025-02-09 10:13:28,937][0m Trial 5 finished with value: 0.6639966359810149 and parameters: {'observation_period_num': 27, 'train_rates': 0.6157816024328201, 'learning_rate': 3.376464751190788e-06, 'batch_size': 113, 'step_size': 7, 'gamma': 0.8840702997957341}. Best is trial 2 with value: 0.21159834711804892.[0m
Early stopping at epoch 57
[32m[I 2025-02-09 10:13:58,128][0m Trial 6 finished with value: 0.5121483366838177 and parameters: {'observation_period_num': 225, 'train_rates': 0.8498914184572469, 'learning_rate': 5.905508198428598e-05, 'batch_size': 110, 'step_size': 1, 'gamma': 0.8036368455902728}. Best is trial 2 with value: 0.21159834711804892.[0m
[32m[I 2025-02-09 10:15:40,120][0m Trial 7 finished with value: 0.18624391195339127 and parameters: {'observation_period_num': 61, 'train_rates': 0.7356890917581029, 'learning_rate': 0.0003537202644630829, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9643008085536007}. Best is trial 7 with value: 0.18624391195339127.[0m
[32m[I 2025-02-09 10:16:52,809][0m Trial 8 finished with value: 0.2707715920215152 and parameters: {'observation_period_num': 204, 'train_rates': 0.757176453604923, 'learning_rate': 0.0003188090631907999, 'batch_size': 68, 'step_size': 1, 'gamma': 0.9191948523498863}. Best is trial 7 with value: 0.18624391195339127.[0m
[32m[I 2025-02-09 10:17:13,326][0m Trial 9 finished with value: 0.45036889060959623 and parameters: {'observation_period_num': 210, 'train_rates': 0.670024803157928, 'learning_rate': 3.510411792945031e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9388900230343397}. Best is trial 7 with value: 0.18624391195339127.[0m
[32m[I 2025-02-09 10:22:21,892][0m Trial 10 finished with value: 0.12355392915196717 and parameters: {'observation_period_num': 103, 'train_rates': 0.9781212209605827, 'learning_rate': 0.0008783572306340914, 'batch_size': 19, 'step_size': 12, 'gamma': 0.9260675958607998}. Best is trial 10 with value: 0.12355392915196717.[0m
[32m[I 2025-02-09 10:27:01,108][0m Trial 11 finished with value: 0.11831938238306479 and parameters: {'observation_period_num': 89, 'train_rates': 0.9661231054385397, 'learning_rate': 0.0008375341939238662, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9325453334231655}. Best is trial 11 with value: 0.11831938238306479.[0m
[32m[I 2025-02-09 10:32:10,325][0m Trial 12 finished with value: 0.15577348737747637 and parameters: {'observation_period_num': 115, 'train_rates': 0.9734832247306912, 'learning_rate': 0.0009179965822327637, 'batch_size': 19, 'step_size': 14, 'gamma': 0.9111743972534319}. Best is trial 11 with value: 0.11831938238306479.[0m
[32m[I 2025-02-09 10:37:04,372][0m Trial 13 finished with value: 0.11741851930910686 and parameters: {'observation_period_num': 97, 'train_rates': 0.9818646941037504, 'learning_rate': 0.0008946424074008986, 'batch_size': 20, 'step_size': 11, 'gamma': 0.9366604418533467}. Best is trial 13 with value: 0.11741851930910686.[0m
[32m[I 2025-02-09 10:38:19,471][0m Trial 14 finished with value: 0.15730731235623854 and parameters: {'observation_period_num': 166, 'train_rates': 0.915296079703124, 'learning_rate': 9.871038141494881e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8358920028537984}. Best is trial 13 with value: 0.11741851930910686.[0m
[32m[I 2025-02-09 10:44:14,980][0m Trial 15 finished with value: 0.08848507884813815 and parameters: {'observation_period_num': 84, 'train_rates': 0.9329036408536696, 'learning_rate': 1.1901105610054818e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8957176755405962}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:44:58,537][0m Trial 16 finished with value: 0.23988803305963832 and parameters: {'observation_period_num': 76, 'train_rates': 0.9133312024623239, 'learning_rate': 1.3858701336267775e-05, 'batch_size': 140, 'step_size': 15, 'gamma': 0.8987317038803728}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:46:34,326][0m Trial 17 finished with value: 0.2606100187437171 and parameters: {'observation_period_num': 151, 'train_rates': 0.8644138644517497, 'learning_rate': 9.91635034000058e-06, 'batch_size': 56, 'step_size': 10, 'gamma': 0.8298908804864281}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:47:14,116][0m Trial 18 finished with value: 0.24182937388920653 and parameters: {'observation_period_num': 160, 'train_rates': 0.936281293102789, 'learning_rate': 0.00011883837821910369, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9478509757586386}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:48:08,179][0m Trial 19 finished with value: 0.30881170143607917 and parameters: {'observation_period_num': 252, 'train_rates': 0.8095399944196123, 'learning_rate': 1.3791356900213307e-05, 'batch_size': 96, 'step_size': 15, 'gamma': 0.896516801021601}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:50:44,651][0m Trial 20 finished with value: 0.6479236094248777 and parameters: {'observation_period_num': 89, 'train_rates': 0.9405824035607303, 'learning_rate': 1.0149508257398488e-06, 'batch_size': 37, 'step_size': 9, 'gamma': 0.9513096058903888}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:56:52,740][0m Trial 21 finished with value: 0.12397772404882643 and parameters: {'observation_period_num': 84, 'train_rates': 0.9878945408485711, 'learning_rate': 0.000625955312849244, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9351348690271305}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 10:59:34,798][0m Trial 22 finished with value: 0.19174472713957028 and parameters: {'observation_period_num': 101, 'train_rates': 0.9493688037987456, 'learning_rate': 0.0001550302781572634, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8781927390900656}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 11:00:52,351][0m Trial 23 finished with value: 0.12399461360828933 and parameters: {'observation_period_num': 65, 'train_rates': 0.8982510171536866, 'learning_rate': 0.0004783744694917892, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9056282957433829}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 11:03:34,450][0m Trial 24 finished with value: 0.09869933058902369 and parameters: {'observation_period_num': 30, 'train_rates': 0.9604037702832862, 'learning_rate': 6.187669212927373e-06, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9596899297619423}. Best is trial 15 with value: 0.08848507884813815.[0m
[32m[I 2025-02-09 11:05:58,183][0m Trial 25 finished with value: 0.07282875283735539 and parameters: {'observation_period_num': 29, 'train_rates': 0.8710932896199123, 'learning_rate': 4.902233855837875e-06, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9898674200390203}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:08:17,379][0m Trial 26 finished with value: 0.10623016892771128 and parameters: {'observation_period_num': 27, 'train_rates': 0.8351338892192897, 'learning_rate': 3.887910518633209e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9628382635069584}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:09:25,255][0m Trial 27 finished with value: 0.08915547559658686 and parameters: {'observation_period_num': 7, 'train_rates': 0.8748951225987552, 'learning_rate': 6.444851625625872e-06, 'batch_size': 85, 'step_size': 12, 'gamma': 0.9877067038546928}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:10:30,929][0m Trial 28 finished with value: 0.15971903532274126 and parameters: {'observation_period_num': 14, 'train_rates': 0.8726611278867628, 'learning_rate': 2.8715156838443925e-06, 'batch_size': 87, 'step_size': 15, 'gamma': 0.9831697206327851}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:11:54,887][0m Trial 29 finished with value: 0.4069418671303952 and parameters: {'observation_period_num': 43, 'train_rates': 0.7714011946689348, 'learning_rate': 1.8891352019967076e-06, 'batch_size': 63, 'step_size': 14, 'gamma': 0.9885558609611025}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:12:40,720][0m Trial 30 finished with value: 0.07429436163511127 and parameters: {'observation_period_num': 6, 'train_rates': 0.7863791409978043, 'learning_rate': 7.4628917195361256e-06, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9697953787554808}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:13:24,660][0m Trial 31 finished with value: 0.24071728726289304 and parameters: {'observation_period_num': 7, 'train_rates': 0.7833157326541909, 'learning_rate': 7.1548057657718225e-06, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9709711736382798}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:13:58,749][0m Trial 32 finished with value: 0.08477893872286683 and parameters: {'observation_period_num': 35, 'train_rates': 0.8118080222475967, 'learning_rate': 2.4901438538229704e-05, 'batch_size': 166, 'step_size': 8, 'gamma': 0.9716992675954735}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:14:29,659][0m Trial 33 finished with value: 0.11570804697942218 and parameters: {'observation_period_num': 64, 'train_rates': 0.8114909904393172, 'learning_rate': 2.5127985485052207e-05, 'batch_size': 184, 'step_size': 8, 'gamma': 0.9713565605618434}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:15:04,590][0m Trial 34 finished with value: 0.11187356283851699 and parameters: {'observation_period_num': 40, 'train_rates': 0.8278667771952138, 'learning_rate': 1.9189062172517384e-05, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9516384592423942}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:15:31,269][0m Trial 35 finished with value: 0.1995398979728597 and parameters: {'observation_period_num': 25, 'train_rates': 0.7353819310668896, 'learning_rate': 4.867335084776449e-05, 'batch_size': 206, 'step_size': 8, 'gamma': 0.8453272899574329}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:16:06,280][0m Trial 36 finished with value: 0.3201485633145313 and parameters: {'observation_period_num': 57, 'train_rates': 0.7989926977836018, 'learning_rate': 1.0737534005789138e-05, 'batch_size': 158, 'step_size': 9, 'gamma': 0.7992831508411691}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:16:36,804][0m Trial 37 finished with value: 0.24340003577329344 and parameters: {'observation_period_num': 35, 'train_rates': 0.8409956911233868, 'learning_rate': 2.4233231227548945e-05, 'batch_size': 200, 'step_size': 7, 'gamma': 0.7604272683813127}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:17:01,800][0m Trial 38 finished with value: 1.0051498026042789 and parameters: {'observation_period_num': 19, 'train_rates': 0.6900581077205297, 'learning_rate': 2.2644234998138764e-06, 'batch_size': 228, 'step_size': 6, 'gamma': 0.9749077760642735}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:17:47,321][0m Trial 39 finished with value: 0.2613709808225698 and parameters: {'observation_period_num': 50, 'train_rates': 0.8539212163867589, 'learning_rate': 8.931175758123956e-06, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8113911238482243}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:18:19,016][0m Trial 40 finished with value: 0.5219768455295251 and parameters: {'observation_period_num': 133, 'train_rates': 0.744606597243346, 'learning_rate': 1.5162207548936337e-05, 'batch_size': 175, 'step_size': 4, 'gamma': 0.8629209337525926}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:19:11,546][0m Trial 41 finished with value: 0.12222668466295704 and parameters: {'observation_period_num': 6, 'train_rates': 0.8801062081575921, 'learning_rate': 4.435239504163668e-06, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9871756491186647}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:20:10,717][0m Trial 42 finished with value: 0.12893912995275264 and parameters: {'observation_period_num': 19, 'train_rates': 0.8938375867705175, 'learning_rate': 5.9124191204867935e-06, 'batch_size': 102, 'step_size': 11, 'gamma': 0.9893569869234745}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:20:52,477][0m Trial 43 finished with value: 0.07737603723468267 and parameters: {'observation_period_num': 42, 'train_rates': 0.9249155755021488, 'learning_rate': 3.821437757372939e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9748499536284768}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:21:35,370][0m Trial 44 finished with value: 0.11109191974711163 and parameters: {'observation_period_num': 70, 'train_rates': 0.904370883208341, 'learning_rate': 5.661494688380065e-05, 'batch_size': 140, 'step_size': 9, 'gamma': 0.9583569531609648}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:22:09,386][0m Trial 45 finished with value: 0.13826930313371122 and parameters: {'observation_period_num': 54, 'train_rates': 0.9241682460380631, 'learning_rate': 3.6342258753244144e-05, 'batch_size': 189, 'step_size': 10, 'gamma': 0.9697029081422528}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:22:44,762][0m Trial 46 finished with value: 0.20153297835754025 and parameters: {'observation_period_num': 42, 'train_rates': 0.7811537953242977, 'learning_rate': 8.239409836177539e-05, 'batch_size': 156, 'step_size': 7, 'gamma': 0.9165289979606612}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:23:14,885][0m Trial 47 finished with value: 0.34327355384884584 and parameters: {'observation_period_num': 78, 'train_rates': 0.6479536819507921, 'learning_rate': 1.8961308858182428e-05, 'batch_size': 169, 'step_size': 8, 'gamma': 0.9229983617857775}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:23:42,097][0m Trial 48 finished with value: 0.09765909040910267 and parameters: {'observation_period_num': 34, 'train_rates': 0.8540736003760473, 'learning_rate': 3.933230741176489e-05, 'batch_size': 228, 'step_size': 6, 'gamma': 0.9448092227731223}. Best is trial 25 with value: 0.07282875283735539.[0m
[32m[I 2025-02-09 11:24:16,169][0m Trial 49 finished with value: 0.28391946879104585 and parameters: {'observation_period_num': 120, 'train_rates': 0.7115893122604534, 'learning_rate': 2.4007559015696084e-05, 'batch_size': 147, 'step_size': 11, 'gamma': 0.977473930537939}. Best is trial 25 with value: 0.07282875283735539.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5502 | 1.0805
Epoch 2/300, Loss: 0.4700 | 0.8428
Epoch 3/300, Loss: 0.4196 | 0.7285
Epoch 4/300, Loss: 0.3804 | 0.6491
Epoch 5/300, Loss: 0.3481 | 0.5865
Epoch 6/300, Loss: 0.3216 | 0.5360
Epoch 7/300, Loss: 0.3000 | 0.4956
Epoch 8/300, Loss: 0.2822 | 0.4627
Epoch 9/300, Loss: 0.2674 | 0.4355
Epoch 10/300, Loss: 0.2548 | 0.4127
Epoch 11/300, Loss: 0.2439 | 0.3932
Epoch 12/300, Loss: 0.2344 | 0.3762
Epoch 13/300, Loss: 0.2259 | 0.3610
Epoch 14/300, Loss: 0.2183 | 0.3473
Epoch 15/300, Loss: 0.2114 | 0.3346
Epoch 16/300, Loss: 0.2052 | 0.3229
Epoch 17/300, Loss: 0.1995 | 0.3119
Epoch 18/300, Loss: 0.1942 | 0.3016
Epoch 19/300, Loss: 0.1894 | 0.2919
Epoch 20/300, Loss: 0.1850 | 0.2827
Epoch 21/300, Loss: 0.1808 | 0.2740
Epoch 22/300, Loss: 0.1770 | 0.2657
Epoch 23/300, Loss: 0.1735 | 0.2578
Epoch 24/300, Loss: 0.1702 | 0.2503
Epoch 25/300, Loss: 0.1671 | 0.2431
Epoch 26/300, Loss: 0.1642 | 0.2362
Epoch 27/300, Loss: 0.1615 | 0.2296
Epoch 28/300, Loss: 0.1589 | 0.2232
Epoch 29/300, Loss: 0.1565 | 0.2171
Epoch 30/300, Loss: 0.1543 | 0.2113
Epoch 31/300, Loss: 0.1522 | 0.2056
Epoch 32/300, Loss: 0.1502 | 0.2003
Epoch 33/300, Loss: 0.1483 | 0.1951
Epoch 34/300, Loss: 0.1465 | 0.1900
Epoch 35/300, Loss: 0.1448 | 0.1853
Epoch 36/300, Loss: 0.1432 | 0.1807
Epoch 37/300, Loss: 0.1416 | 0.1762
Epoch 38/300, Loss: 0.1402 | 0.1720
Epoch 39/300, Loss: 0.1388 | 0.1679
Epoch 40/300, Loss: 0.1374 | 0.1639
Epoch 41/300, Loss: 0.1362 | 0.1602
Epoch 42/300, Loss: 0.1350 | 0.1565
Epoch 43/300, Loss: 0.1338 | 0.1531
Epoch 44/300, Loss: 0.1327 | 0.1497
Epoch 45/300, Loss: 0.1317 | 0.1466
Epoch 46/300, Loss: 0.1306 | 0.1435
Epoch 47/300, Loss: 0.1297 | 0.1406
Epoch 48/300, Loss: 0.1287 | 0.1378
Epoch 49/300, Loss: 0.1278 | 0.1351
Epoch 50/300, Loss: 0.1270 | 0.1325
Epoch 51/300, Loss: 0.1261 | 0.1301
Epoch 52/300, Loss: 0.1253 | 0.1277
Epoch 53/300, Loss: 0.1245 | 0.1255
Epoch 54/300, Loss: 0.1237 | 0.1233
Epoch 55/300, Loss: 0.1230 | 0.1212
Epoch 56/300, Loss: 0.1223 | 0.1193
Epoch 57/300, Loss: 0.1216 | 0.1173
Epoch 58/300, Loss: 0.1209 | 0.1155
Epoch 59/300, Loss: 0.1202 | 0.1137
Epoch 60/300, Loss: 0.1196 | 0.1120
Epoch 61/300, Loss: 0.1190 | 0.1104
Epoch 62/300, Loss: 0.1183 | 0.1088
Epoch 63/300, Loss: 0.1177 | 0.1073
Epoch 64/300, Loss: 0.1171 | 0.1059
Epoch 65/300, Loss: 0.1166 | 0.1045
Epoch 66/300, Loss: 0.1160 | 0.1031
Epoch 67/300, Loss: 0.1154 | 0.1018
Epoch 68/300, Loss: 0.1149 | 0.1005
Epoch 69/300, Loss: 0.1144 | 0.0993
Epoch 70/300, Loss: 0.1138 | 0.0982
Epoch 71/300, Loss: 0.1133 | 0.0970
Epoch 72/300, Loss: 0.1128 | 0.0959
Epoch 73/300, Loss: 0.1123 | 0.0949
Epoch 74/300, Loss: 0.1118 | 0.0939
Epoch 75/300, Loss: 0.1113 | 0.0929
Epoch 76/300, Loss: 0.1109 | 0.0920
Epoch 77/300, Loss: 0.1104 | 0.0911
Epoch 78/300, Loss: 0.1099 | 0.0902
Epoch 79/300, Loss: 0.1095 | 0.0894
Epoch 80/300, Loss: 0.1091 | 0.0886
Epoch 81/300, Loss: 0.1086 | 0.0878
Epoch 82/300, Loss: 0.1082 | 0.0870
Epoch 83/300, Loss: 0.1078 | 0.0863
Epoch 84/300, Loss: 0.1074 | 0.0856
Epoch 85/300, Loss: 0.1070 | 0.0850
Epoch 86/300, Loss: 0.1066 | 0.0844
Epoch 87/300, Loss: 0.1062 | 0.0838
Epoch 88/300, Loss: 0.1058 | 0.0832
Epoch 89/300, Loss: 0.1054 | 0.0827
Epoch 90/300, Loss: 0.1051 | 0.0821
Epoch 91/300, Loss: 0.1047 | 0.0816
Epoch 92/300, Loss: 0.1043 | 0.0811
Epoch 93/300, Loss: 0.1040 | 0.0807
Epoch 94/300, Loss: 0.1036 | 0.0802
Epoch 95/300, Loss: 0.1033 | 0.0798
Epoch 96/300, Loss: 0.1029 | 0.0794
Epoch 97/300, Loss: 0.1026 | 0.0789
Epoch 98/300, Loss: 0.1022 | 0.0785
Epoch 99/300, Loss: 0.1019 | 0.0782
Epoch 100/300, Loss: 0.1015 | 0.0778
Epoch 101/300, Loss: 0.1012 | 0.0774
Epoch 102/300, Loss: 0.1009 | 0.0771
Epoch 103/300, Loss: 0.1005 | 0.0767
Epoch 104/300, Loss: 0.1002 | 0.0764
Epoch 105/300, Loss: 0.0999 | 0.0761
Epoch 106/300, Loss: 0.0996 | 0.0758
Epoch 107/300, Loss: 0.0993 | 0.0755
Epoch 108/300, Loss: 0.0990 | 0.0752
Epoch 109/300, Loss: 0.0987 | 0.0749
Epoch 110/300, Loss: 0.0984 | 0.0747
Epoch 111/300, Loss: 0.0981 | 0.0744
Epoch 112/300, Loss: 0.0978 | 0.0742
Epoch 113/300, Loss: 0.0975 | 0.0740
Epoch 114/300, Loss: 0.0973 | 0.0738
Epoch 115/300, Loss: 0.0970 | 0.0736
Epoch 116/300, Loss: 0.0967 | 0.0734
Epoch 117/300, Loss: 0.0964 | 0.0732
Epoch 118/300, Loss: 0.0962 | 0.0730
Epoch 119/300, Loss: 0.0959 | 0.0728
Epoch 120/300, Loss: 0.0956 | 0.0727
Epoch 121/300, Loss: 0.0954 | 0.0725
Epoch 122/300, Loss: 0.0951 | 0.0724
Epoch 123/300, Loss: 0.0949 | 0.0722
Epoch 124/300, Loss: 0.0946 | 0.0720
Epoch 125/300, Loss: 0.0944 | 0.0719
Epoch 126/300, Loss: 0.0941 | 0.0717
Epoch 127/300, Loss: 0.0939 | 0.0715
Epoch 128/300, Loss: 0.0936 | 0.0714
Epoch 129/300, Loss: 0.0934 | 0.0713
Epoch 130/300, Loss: 0.0932 | 0.0711
Epoch 131/300, Loss: 0.0929 | 0.0710
Epoch 132/300, Loss: 0.0927 | 0.0708
Epoch 133/300, Loss: 0.0925 | 0.0707
Epoch 134/300, Loss: 0.0923 | 0.0706
Epoch 135/300, Loss: 0.0921 | 0.0704
Epoch 136/300, Loss: 0.0918 | 0.0703
Epoch 137/300, Loss: 0.0916 | 0.0702
Epoch 138/300, Loss: 0.0914 | 0.0701
Epoch 139/300, Loss: 0.0912 | 0.0700
Epoch 140/300, Loss: 0.0910 | 0.0699
Epoch 141/300, Loss: 0.0908 | 0.0697
Epoch 142/300, Loss: 0.0906 | 0.0696
Epoch 143/300, Loss: 0.0904 | 0.0695
Epoch 144/300, Loss: 0.0902 | 0.0694
Epoch 145/300, Loss: 0.0900 | 0.0693
Epoch 146/300, Loss: 0.0898 | 0.0692
Epoch 147/300, Loss: 0.0897 | 0.0691
Epoch 148/300, Loss: 0.0895 | 0.0689
Epoch 149/300, Loss: 0.0893 | 0.0688
Epoch 150/300, Loss: 0.0891 | 0.0688
Epoch 151/300, Loss: 0.0889 | 0.0686
Epoch 152/300, Loss: 0.0888 | 0.0685
Epoch 153/300, Loss: 0.0886 | 0.0684
Epoch 154/300, Loss: 0.0884 | 0.0682
Epoch 155/300, Loss: 0.0882 | 0.0682
Epoch 156/300, Loss: 0.0881 | 0.0680
Epoch 157/300, Loss: 0.0879 | 0.0679
Epoch 158/300, Loss: 0.0878 | 0.0678
Epoch 159/300, Loss: 0.0876 | 0.0676
Epoch 160/300, Loss: 0.0874 | 0.0675
Epoch 161/300, Loss: 0.0873 | 0.0675
Epoch 162/300, Loss: 0.0871 | 0.0673
Epoch 163/300, Loss: 0.0870 | 0.0672
Epoch 164/300, Loss: 0.0868 | 0.0671
Epoch 165/300, Loss: 0.0867 | 0.0669
Epoch 166/300, Loss: 0.0865 | 0.0669
Epoch 167/300, Loss: 0.0864 | 0.0667
Epoch 168/300, Loss: 0.0863 | 0.0666
Epoch 169/300, Loss: 0.0861 | 0.0665
Epoch 170/300, Loss: 0.0860 | 0.0664
Epoch 171/300, Loss: 0.0858 | 0.0662
Epoch 172/300, Loss: 0.0857 | 0.0662
Epoch 173/300, Loss: 0.0856 | 0.0660
Epoch 174/300, Loss: 0.0854 | 0.0659
Epoch 175/300, Loss: 0.0853 | 0.0658
Epoch 176/300, Loss: 0.0852 | 0.0656
Epoch 177/300, Loss: 0.0850 | 0.0656
Epoch 178/300, Loss: 0.0849 | 0.0654
Epoch 179/300, Loss: 0.0848 | 0.0653
Epoch 180/300, Loss: 0.0847 | 0.0652
Epoch 181/300, Loss: 0.0845 | 0.0650
Epoch 182/300, Loss: 0.0844 | 0.0649
Epoch 183/300, Loss: 0.0843 | 0.0648
Epoch 184/300, Loss: 0.0842 | 0.0647
Epoch 185/300, Loss: 0.0841 | 0.0645
Epoch 186/300, Loss: 0.0840 | 0.0644
Epoch 187/300, Loss: 0.0838 | 0.0643
Epoch 188/300, Loss: 0.0837 | 0.0642
Epoch 189/300, Loss: 0.0836 | 0.0641
Epoch 190/300, Loss: 0.0835 | 0.0639
Epoch 191/300, Loss: 0.0834 | 0.0638
Epoch 192/300, Loss: 0.0833 | 0.0636
Epoch 193/300, Loss: 0.0832 | 0.0635
Epoch 194/300, Loss: 0.0831 | 0.0634
Epoch 195/300, Loss: 0.0830 | 0.0633
Epoch 196/300, Loss: 0.0829 | 0.0632
Epoch 197/300, Loss: 0.0828 | 0.0630
Epoch 198/300, Loss: 0.0827 | 0.0629
Epoch 199/300, Loss: 0.0826 | 0.0628
Epoch 200/300, Loss: 0.0825 | 0.0627
Epoch 201/300, Loss: 0.0824 | 0.0626
Epoch 202/300, Loss: 0.0823 | 0.0624
Epoch 203/300, Loss: 0.0822 | 0.0623
Epoch 204/300, Loss: 0.0821 | 0.0622
Epoch 205/300, Loss: 0.0820 | 0.0621
Epoch 206/300, Loss: 0.0819 | 0.0620
Epoch 207/300, Loss: 0.0818 | 0.0618
Epoch 208/300, Loss: 0.0817 | 0.0617
Epoch 209/300, Loss: 0.0816 | 0.0616
Epoch 210/300, Loss: 0.0815 | 0.0615
Epoch 211/300, Loss: 0.0814 | 0.0614
Epoch 212/300, Loss: 0.0813 | 0.0612
Epoch 213/300, Loss: 0.0813 | 0.0611
Epoch 214/300, Loss: 0.0812 | 0.0609
Epoch 215/300, Loss: 0.0811 | 0.0608
Epoch 216/300, Loss: 0.0810 | 0.0608
Epoch 217/300, Loss: 0.0809 | 0.0606
Epoch 218/300, Loss: 0.0808 | 0.0605
Epoch 219/300, Loss: 0.0807 | 0.0604
Epoch 220/300, Loss: 0.0807 | 0.0602
Epoch 221/300, Loss: 0.0806 | 0.0602
Epoch 222/300, Loss: 0.0805 | 0.0600
Epoch 223/300, Loss: 0.0804 | 0.0599
Epoch 224/300, Loss: 0.0803 | 0.0598
Epoch 225/300, Loss: 0.0803 | 0.0597
Epoch 226/300, Loss: 0.0802 | 0.0595
Epoch 227/300, Loss: 0.0801 | 0.0595
Epoch 228/300, Loss: 0.0800 | 0.0594
Epoch 229/300, Loss: 0.0800 | 0.0592
Epoch 230/300, Loss: 0.0799 | 0.0591
Epoch 231/300, Loss: 0.0798 | 0.0590
Epoch 232/300, Loss: 0.0797 | 0.0589
Epoch 233/300, Loss: 0.0797 | 0.0588
Epoch 234/300, Loss: 0.0796 | 0.0587
Epoch 235/300, Loss: 0.0795 | 0.0586
Epoch 236/300, Loss: 0.0794 | 0.0584
Epoch 237/300, Loss: 0.0794 | 0.0583
Epoch 238/300, Loss: 0.0793 | 0.0583
Epoch 239/300, Loss: 0.0792 | 0.0581
Epoch 240/300, Loss: 0.0791 | 0.0580
Epoch 241/300, Loss: 0.0791 | 0.0579
Epoch 242/300, Loss: 0.0790 | 0.0578
Epoch 243/300, Loss: 0.0789 | 0.0577
Epoch 244/300, Loss: 0.0789 | 0.0576
Epoch 245/300, Loss: 0.0788 | 0.0575
Epoch 246/300, Loss: 0.0787 | 0.0574
Epoch 247/300, Loss: 0.0787 | 0.0573
Epoch 248/300, Loss: 0.0786 | 0.0571
Epoch 249/300, Loss: 0.0785 | 0.0571
Epoch 250/300, Loss: 0.0785 | 0.0570
Epoch 251/300, Loss: 0.0784 | 0.0569
Epoch 252/300, Loss: 0.0783 | 0.0568
Epoch 253/300, Loss: 0.0783 | 0.0566
Epoch 254/300, Loss: 0.0782 | 0.0566
Epoch 255/300, Loss: 0.0781 | 0.0565
Epoch 256/300, Loss: 0.0781 | 0.0564
Epoch 257/300, Loss: 0.0780 | 0.0563
Epoch 258/300, Loss: 0.0779 | 0.0562
Epoch 259/300, Loss: 0.0779 | 0.0560
Epoch 260/300, Loss: 0.0778 | 0.0560
Epoch 261/300, Loss: 0.0778 | 0.0559
Epoch 262/300, Loss: 0.0777 | 0.0558
Epoch 263/300, Loss: 0.0776 | 0.0557
Epoch 264/300, Loss: 0.0776 | 0.0556
Epoch 265/300, Loss: 0.0775 | 0.0555
Epoch 266/300, Loss: 0.0774 | 0.0554
Epoch 267/300, Loss: 0.0774 | 0.0553
Epoch 268/300, Loss: 0.0773 | 0.0552
Epoch 269/300, Loss: 0.0773 | 0.0551
Epoch 270/300, Loss: 0.0772 | 0.0550
Epoch 271/300, Loss: 0.0771 | 0.0550
Epoch 272/300, Loss: 0.0771 | 0.0549
Epoch 273/300, Loss: 0.0770 | 0.0548
Epoch 274/300, Loss: 0.0770 | 0.0547
Epoch 275/300, Loss: 0.0769 | 0.0546
Epoch 276/300, Loss: 0.0769 | 0.0545
Epoch 277/300, Loss: 0.0768 | 0.0544
Epoch 278/300, Loss: 0.0767 | 0.0543
Epoch 279/300, Loss: 0.0767 | 0.0542
Epoch 280/300, Loss: 0.0766 | 0.0541
Epoch 281/300, Loss: 0.0766 | 0.0540
Epoch 282/300, Loss: 0.0765 | 0.0540
Epoch 283/300, Loss: 0.0765 | 0.0539
Epoch 284/300, Loss: 0.0764 | 0.0538
Epoch 285/300, Loss: 0.0764 | 0.0537
Epoch 286/300, Loss: 0.0763 | 0.0536
Epoch 287/300, Loss: 0.0763 | 0.0536
Epoch 288/300, Loss: 0.0762 | 0.0535
Epoch 289/300, Loss: 0.0761 | 0.0534
Epoch 290/300, Loss: 0.0761 | 0.0533
Epoch 291/300, Loss: 0.0760 | 0.0532
Epoch 292/300, Loss: 0.0760 | 0.0531
Epoch 293/300, Loss: 0.0759 | 0.0531
Epoch 294/300, Loss: 0.0759 | 0.0530
Epoch 295/300, Loss: 0.0758 | 0.0529
Epoch 296/300, Loss: 0.0758 | 0.0528
Epoch 297/300, Loss: 0.0757 | 0.0527
Epoch 298/300, Loss: 0.0757 | 0.0527
Epoch 299/300, Loss: 0.0756 | 0.0526
Epoch 300/300, Loss: 0.0756 | 0.0526
Runtime (seconds): 430.60702753067017
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1029.2557500603143
RMSE: 32.08201599121094
MAE: 32.08201599121094
R-squared: nan
[219.22202]
