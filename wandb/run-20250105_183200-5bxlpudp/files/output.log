ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 18:32:01,068][0m A new study created in memory with name: no-name-917c86e8-187b-4a24-936c-4c7e48a2591b[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-05 18:32:33,704][0m Trial 0 finished with value: 0.09604329260764644 and parameters: {'observation_period_num': 84, 'train_rates': 0.6877179683073517, 'learning_rate': 0.0005534838362197154, 'batch_size': 161, 'step_size': 9, 'gamma': 0.9129175573079017}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:32:56,789][0m Trial 1 finished with value: 0.9383307112977396 and parameters: {'observation_period_num': 150, 'train_rates': 0.7817321787947743, 'learning_rate': 3.601046097227297e-06, 'batch_size': 226, 'step_size': 8, 'gamma': 0.8026235617391506}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:33:27,614][0m Trial 2 finished with value: 0.39948045510166097 and parameters: {'observation_period_num': 221, 'train_rates': 0.9031397450622423, 'learning_rate': 1.2630216030775658e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.8055228396386513}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:33:54,766][0m Trial 3 finished with value: 0.2092282791671 and parameters: {'observation_period_num': 178, 'train_rates': 0.6711331720430398, 'learning_rate': 0.000729038987383009, 'batch_size': 231, 'step_size': 14, 'gamma': 0.8499997943190283}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:34:24,094][0m Trial 4 finished with value: 0.8849882260086883 and parameters: {'observation_period_num': 230, 'train_rates': 0.7756089281430589, 'learning_rate': 2.5404013776971277e-06, 'batch_size': 194, 'step_size': 15, 'gamma': 0.751745542536572}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:34:59,062][0m Trial 5 finished with value: 0.1039434710926399 and parameters: {'observation_period_num': 100, 'train_rates': 0.8274519062613508, 'learning_rate': 0.0007559773611621748, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8496707502520309}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:36:59,197][0m Trial 6 finished with value: 0.12548065830573993 and parameters: {'observation_period_num': 128, 'train_rates': 0.6176794312275515, 'learning_rate': 0.0005078371123891196, 'batch_size': 34, 'step_size': 6, 'gamma': 0.7787079240190542}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:39:10,945][0m Trial 7 finished with value: 0.5702076044736457 and parameters: {'observation_period_num': 5, 'train_rates': 0.7025978651086924, 'learning_rate': 1.2261633478335979e-06, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8638243350089894}. Best is trial 0 with value: 0.09604329260764644.[0m
[32m[I 2025-01-05 18:39:40,513][0m Trial 8 finished with value: 0.06430283789123807 and parameters: {'observation_period_num': 21, 'train_rates': 0.7016646792343575, 'learning_rate': 7.85364193484851e-05, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8224841399295684}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:41:02,101][0m Trial 9 finished with value: 0.12094998359680176 and parameters: {'observation_period_num': 239, 'train_rates': 0.9768932813837703, 'learning_rate': 0.000138499324892452, 'batch_size': 73, 'step_size': 8, 'gamma': 0.9845863148994911}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:42:00,182][0m Trial 10 finished with value: 0.0945815539259589 and parameters: {'observation_period_num': 12, 'train_rates': 0.8489129406030527, 'learning_rate': 4.890601914843448e-05, 'batch_size': 118, 'step_size': 1, 'gamma': 0.9148709668546853}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:43:06,974][0m Trial 11 finished with value: 0.07840830545083448 and parameters: {'observation_period_num': 7, 'train_rates': 0.8733402329634219, 'learning_rate': 5.398380183168105e-05, 'batch_size': 102, 'step_size': 1, 'gamma': 0.9180932682454681}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:44:11,994][0m Trial 12 finished with value: 0.13210675573047204 and parameters: {'observation_period_num': 50, 'train_rates': 0.9183454122452723, 'learning_rate': 7.390736063569312e-05, 'batch_size': 104, 'step_size': 1, 'gamma': 0.9280542971823693}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:45:21,562][0m Trial 13 finished with value: 0.08830931291386888 and parameters: {'observation_period_num': 39, 'train_rates': 0.7460517807088447, 'learning_rate': 1.735666451075388e-05, 'batch_size': 84, 'step_size': 4, 'gamma': 0.96600575408507}. Best is trial 8 with value: 0.06430283789123807.[0m
[32m[I 2025-01-05 18:46:09,624][0m Trial 14 finished with value: 0.0527158730170306 and parameters: {'observation_period_num': 58, 'train_rates': 0.870772711883604, 'learning_rate': 0.00017270859430607363, 'batch_size': 151, 'step_size': 4, 'gamma': 0.8843193431751687}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:46:40,315][0m Trial 15 finished with value: 0.14324268623473232 and parameters: {'observation_period_num': 66, 'train_rates': 0.6018212474708403, 'learning_rate': 0.00016754055882229894, 'batch_size': 160, 'step_size': 4, 'gamma': 0.8224293399117486}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:47:16,811][0m Trial 16 finished with value: 0.05565543845295906 and parameters: {'observation_period_num': 40, 'train_rates': 0.9828034302406264, 'learning_rate': 0.00020702568917228194, 'batch_size': 193, 'step_size': 6, 'gamma': 0.8838578855035568}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:47:52,693][0m Trial 17 finished with value: 0.12576556205749512 and parameters: {'observation_period_num': 104, 'train_rates': 0.9822248552313544, 'learning_rate': 0.00019273812043673374, 'batch_size': 194, 'step_size': 10, 'gamma': 0.884538586710972}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:48:37,967][0m Trial 18 finished with value: 0.05600314984870393 and parameters: {'observation_period_num': 69, 'train_rates': 0.9434403543779878, 'learning_rate': 0.00023835420100675478, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8799339620322181}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:49:19,640][0m Trial 19 finished with value: 0.15186509551815175 and parameters: {'observation_period_num': 40, 'train_rates': 0.92578737401272, 'learning_rate': 2.193918672624824e-05, 'batch_size': 206, 'step_size': 3, 'gamma': 0.9553824494553557}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:49:53,619][0m Trial 20 finished with value: 0.10502558399219902 and parameters: {'observation_period_num': 137, 'train_rates': 0.8781250985611745, 'learning_rate': 0.000319083855421083, 'batch_size': 252, 'step_size': 11, 'gamma': 0.896963711771786}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:50:44,595][0m Trial 21 finished with value: 0.06274955719709396 and parameters: {'observation_period_num': 73, 'train_rates': 0.9546060396965022, 'learning_rate': 0.00031049664515426123, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8775749300597121}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:51:32,986][0m Trial 22 finished with value: 0.07871011709606887 and parameters: {'observation_period_num': 54, 'train_rates': 0.9488757756612689, 'learning_rate': 0.00012917425092752143, 'batch_size': 145, 'step_size': 7, 'gamma': 0.8570596081558004}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:52:10,733][0m Trial 23 finished with value: 0.08290272243098758 and parameters: {'observation_period_num': 95, 'train_rates': 0.8239766875664194, 'learning_rate': 0.0002635050342836161, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8942649200512198}. Best is trial 14 with value: 0.0527158730170306.[0m
[32m[I 2025-01-05 18:53:05,375][0m Trial 24 finished with value: 0.04214312963477977 and parameters: {'observation_period_num': 36, 'train_rates': 0.894601146503732, 'learning_rate': 0.00010660985943526072, 'batch_size': 122, 'step_size': 5, 'gamma': 0.9397881606739551}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 18:53:56,193][0m Trial 25 finished with value: 0.053038674374390096 and parameters: {'observation_period_num': 30, 'train_rates': 0.8915586564140239, 'learning_rate': 3.159670571942089e-05, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9363021107205709}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 18:54:47,893][0m Trial 26 finished with value: 0.14905845721562702 and parameters: {'observation_period_num': 27, 'train_rates': 0.8871672370138054, 'learning_rate': 9.109216857221439e-06, 'batch_size': 122, 'step_size': 5, 'gamma': 0.9436757199157153}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 18:56:08,784][0m Trial 27 finished with value: 0.05657838769424122 and parameters: {'observation_period_num': 31, 'train_rates': 0.8474855195466497, 'learning_rate': 4.094606780213253e-05, 'batch_size': 70, 'step_size': 2, 'gamma': 0.9389139538133456}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 18:56:56,910][0m Trial 28 finished with value: 0.06562829924176537 and parameters: {'observation_period_num': 55, 'train_rates': 0.8290212290510939, 'learning_rate': 3.178625901660504e-05, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9786870176516957}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 18:58:40,372][0m Trial 29 finished with value: 0.05076191740110517 and parameters: {'observation_period_num': 81, 'train_rates': 0.8607772318713194, 'learning_rate': 8.590457097655859e-05, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9078026230078348}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:00:14,965][0m Trial 30 finished with value: 0.06401977252389054 and parameters: {'observation_period_num': 83, 'train_rates': 0.858096697531894, 'learning_rate': 9.352802979644479e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9004495118254491}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:04:59,923][0m Trial 31 finished with value: 0.10543423131374376 and parameters: {'observation_period_num': 110, 'train_rates': 0.9016305312189687, 'learning_rate': 0.00010074854484117096, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9357564174347832}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:05:59,346][0m Trial 32 finished with value: 0.2187716213042197 and parameters: {'observation_period_num': 81, 'train_rates': 0.7965991122773095, 'learning_rate': 7.177498862215628e-06, 'batch_size': 97, 'step_size': 7, 'gamma': 0.9612465900398174}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:07:41,777][0m Trial 33 finished with value: 0.05098652682089619 and parameters: {'observation_period_num': 59, 'train_rates': 0.8677371435932861, 'learning_rate': 3.464470737211766e-05, 'batch_size': 55, 'step_size': 5, 'gamma': 0.9204617794619904}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:09:14,582][0m Trial 34 finished with value: 0.1274168892508972 and parameters: {'observation_period_num': 173, 'train_rates': 0.7919444642718786, 'learning_rate': 0.00042126248382421365, 'batch_size': 56, 'step_size': 7, 'gamma': 0.9075339551543905}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:11:21,129][0m Trial 35 finished with value: 0.07022981005819871 and parameters: {'observation_period_num': 119, 'train_rates': 0.8675743076916417, 'learning_rate': 6.729439701540853e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9192778388703863}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:12:32,098][0m Trial 36 finished with value: 0.16941413853479467 and parameters: {'observation_period_num': 59, 'train_rates': 0.9207381471301367, 'learning_rate': 2.476049279156081e-05, 'batch_size': 84, 'step_size': 4, 'gamma': 0.8354686421844297}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:16:48,602][0m Trial 37 finished with value: 0.1339404632415727 and parameters: {'observation_period_num': 88, 'train_rates': 0.8133251674013098, 'learning_rate': 0.00010643040062639168, 'batch_size': 20, 'step_size': 9, 'gamma': 0.9507449467930308}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:18:25,305][0m Trial 38 finished with value: 0.1556371691237603 and parameters: {'observation_period_num': 160, 'train_rates': 0.7595828182679552, 'learning_rate': 0.000997614990357895, 'batch_size': 51, 'step_size': 5, 'gamma': 0.8662799441585447}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:18:59,735][0m Trial 39 finished with value: 0.2268929631498299 and parameters: {'observation_period_num': 76, 'train_rates': 0.8414696759979232, 'learning_rate': 1.2904532668137828e-05, 'batch_size': 173, 'step_size': 13, 'gamma': 0.9048296463796216}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:20:19,020][0m Trial 40 finished with value: 0.05054281245339924 and parameters: {'observation_period_num': 50, 'train_rates': 0.804439241622875, 'learning_rate': 5.85702357803526e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.92674333608178}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:21:29,726][0m Trial 41 finished with value: 0.2195266556015331 and parameters: {'observation_period_num': 216, 'train_rates': 0.8015115823116865, 'learning_rate': 3.425809861248319e-05, 'batch_size': 73, 'step_size': 2, 'gamma': 0.9262141762020393}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:24:02,195][0m Trial 42 finished with value: 0.05960423438508305 and parameters: {'observation_period_num': 50, 'train_rates': 0.7605959266460703, 'learning_rate': 5.6614536356780784e-05, 'batch_size': 33, 'step_size': 3, 'gamma': 0.8919500609600003}. Best is trial 24 with value: 0.04214312963477977.[0m
[32m[I 2025-01-05 19:24:59,864][0m Trial 43 finished with value: 0.037240918787737665 and parameters: {'observation_period_num': 18, 'train_rates': 0.7340178460381248, 'learning_rate': 0.00015232656498093876, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9720083966273434}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:25:54,859][0m Trial 44 finished with value: 0.049044713920672535 and parameters: {'observation_period_num': 15, 'train_rates': 0.6646278348678258, 'learning_rate': 4.543027798903945e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9738905045923224}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:26:52,756][0m Trial 45 finished with value: 0.05205581329138465 and parameters: {'observation_period_num': 20, 'train_rates': 0.6460555308926232, 'learning_rate': 0.00012464390331572386, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9727696048262598}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:27:48,730][0m Trial 46 finished with value: 0.04081128493284173 and parameters: {'observation_period_num': 15, 'train_rates': 0.7254963240949842, 'learning_rate': 7.401462335507806e-05, 'batch_size': 96, 'step_size': 2, 'gamma': 0.988586098309607}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:28:45,327][0m Trial 47 finished with value: 0.04474262856401321 and parameters: {'observation_period_num': 17, 'train_rates': 0.7206889902222717, 'learning_rate': 5.0320866702561e-05, 'batch_size': 94, 'step_size': 2, 'gamma': 0.9852686463195846}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:29:34,511][0m Trial 48 finished with value: 0.05262198415383899 and parameters: {'observation_period_num': 15, 'train_rates': 0.7239999212837305, 'learning_rate': 4.419995583346406e-05, 'batch_size': 109, 'step_size': 2, 'gamma': 0.9889271800158802}. Best is trial 43 with value: 0.037240918787737665.[0m
[32m[I 2025-01-05 19:30:29,975][0m Trial 49 finished with value: 0.09056606889942533 and parameters: {'observation_period_num': 5, 'train_rates': 0.6764849759313891, 'learning_rate': 1.8347997936751263e-05, 'batch_size': 93, 'step_size': 1, 'gamma': 0.9714711974349314}. Best is trial 43 with value: 0.037240918787737665.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 19:30:29,985][0m A new study created in memory with name: no-name-05b1914c-30e2-45d5-8c76-b4f0db9caf9e[0m
[32m[I 2025-01-05 19:32:49,440][0m Trial 0 finished with value: 0.48009807460788473 and parameters: {'observation_period_num': 169, 'train_rates': 0.7111322602760571, 'learning_rate': 6.755419785069791e-06, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8042298048833452}. Best is trial 0 with value: 0.48009807460788473.[0m
[32m[I 2025-01-05 19:33:14,738][0m Trial 1 finished with value: 0.790749742434575 and parameters: {'observation_period_num': 157, 'train_rates': 0.688283480504987, 'learning_rate': 2.6316620290135597e-06, 'batch_size': 246, 'step_size': 4, 'gamma': 0.9788960336833002}. Best is trial 0 with value: 0.48009807460788473.[0m
[32m[I 2025-01-05 19:33:54,887][0m Trial 2 finished with value: 0.1302920853315814 and parameters: {'observation_period_num': 40, 'train_rates': 0.7256742427437373, 'learning_rate': 4.585017525809775e-05, 'batch_size': 132, 'step_size': 10, 'gamma': 0.862281120425714}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:34:51,233][0m Trial 3 finished with value: 0.18147903822717212 and parameters: {'observation_period_num': 227, 'train_rates': 0.684151369578225, 'learning_rate': 0.0003821590939507825, 'batch_size': 82, 'step_size': 3, 'gamma': 0.9304872407743026}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:35:22,314][0m Trial 4 finished with value: 0.61968709111597 and parameters: {'observation_period_num': 105, 'train_rates': 0.8907634505512659, 'learning_rate': 2.011334226991508e-06, 'batch_size': 220, 'step_size': 5, 'gamma': 0.9675340938588315}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:39:20,503][0m Trial 5 finished with value: 0.18521372249493231 and parameters: {'observation_period_num': 191, 'train_rates': 0.8822864458653752, 'learning_rate': 1.4379454422647332e-05, 'batch_size': 22, 'step_size': 6, 'gamma': 0.7764549940768101}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:39:59,907][0m Trial 6 finished with value: 0.6371267437934875 and parameters: {'observation_period_num': 93, 'train_rates': 0.9754390043665151, 'learning_rate': 1.5804578903310284e-06, 'batch_size': 192, 'step_size': 15, 'gamma': 0.8455286969060105}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:40:38,276][0m Trial 7 finished with value: 0.7510962533500959 and parameters: {'observation_period_num': 51, 'train_rates': 0.6347290793480388, 'learning_rate': 2.268248583577583e-06, 'batch_size': 141, 'step_size': 9, 'gamma': 0.7603201078083552}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:41:26,169][0m Trial 8 finished with value: 0.7111567164722242 and parameters: {'observation_period_num': 156, 'train_rates': 0.9186790751277791, 'learning_rate': 1.532319509919529e-06, 'batch_size': 219, 'step_size': 9, 'gamma': 0.759142285669524}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:45:22,881][0m Trial 9 finished with value: 0.28539519506621536 and parameters: {'observation_period_num': 217, 'train_rates': 0.603116211507565, 'learning_rate': 2.970027250143591e-05, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9078005074521038}. Best is trial 2 with value: 0.1302920853315814.[0m
[32m[I 2025-01-05 19:46:09,425][0m Trial 10 finished with value: 0.06327962477538096 and parameters: {'observation_period_num': 11, 'train_rates': 0.7999437395500695, 'learning_rate': 0.00016510071900972066, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8565336458175375}. Best is trial 10 with value: 0.06327962477538096.[0m
[32m[I 2025-01-05 19:46:57,006][0m Trial 11 finished with value: 0.06955946793663846 and parameters: {'observation_period_num': 6, 'train_rates': 0.7762441111227348, 'learning_rate': 0.0001609828636189927, 'batch_size': 133, 'step_size': 13, 'gamma': 0.8535041114397848}. Best is trial 10 with value: 0.06327962477538096.[0m
[32m[I 2025-01-05 19:47:41,643][0m Trial 12 finished with value: 0.058861047978822294 and parameters: {'observation_period_num': 11, 'train_rates': 0.8066903175693002, 'learning_rate': 0.00023115754965744674, 'batch_size': 136, 'step_size': 14, 'gamma': 0.8309714976650402}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:48:42,248][0m Trial 13 finished with value: 0.06946611773742045 and parameters: {'observation_period_num': 9, 'train_rates': 0.8025441957351634, 'learning_rate': 0.0009945564701265511, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8181641986630696}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:49:18,244][0m Trial 14 finished with value: 0.0788124488420127 and parameters: {'observation_period_num': 57, 'train_rates': 0.8123237674330657, 'learning_rate': 0.00011786346724977653, 'batch_size': 171, 'step_size': 15, 'gamma': 0.9022869990904882}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:50:19,836][0m Trial 15 finished with value: 0.06634626373283582 and parameters: {'observation_period_num': 83, 'train_rates': 0.8435034175972843, 'learning_rate': 0.00017969227574962703, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8196788954799621}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:50:57,351][0m Trial 16 finished with value: 0.0755182634827427 and parameters: {'observation_period_num': 35, 'train_rates': 0.7586394693180467, 'learning_rate': 0.0005117223853081584, 'batch_size': 156, 'step_size': 13, 'gamma': 0.8842722679205243}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:51:49,006][0m Trial 17 finished with value: 0.10806246833843097 and parameters: {'observation_period_num': 125, 'train_rates': 0.8387097665361477, 'learning_rate': 5.244711307849199e-05, 'batch_size': 112, 'step_size': 11, 'gamma': 0.7988901382419867}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:53:13,182][0m Trial 18 finished with value: 0.08857567433829176 and parameters: {'observation_period_num': 74, 'train_rates': 0.7464154148808182, 'learning_rate': 9.503113907340335e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8312230032441852}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:53:52,432][0m Trial 19 finished with value: 0.0995304062962532 and parameters: {'observation_period_num': 23, 'train_rates': 0.9786695252064841, 'learning_rate': 0.0003248659681269271, 'batch_size': 178, 'step_size': 1, 'gamma': 0.8777013440691407}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:54:41,317][0m Trial 20 finished with value: 0.1946774781518016 and parameters: {'observation_period_num': 123, 'train_rates': 0.8493445283437155, 'learning_rate': 0.0009410488188121733, 'batch_size': 117, 'step_size': 11, 'gamma': 0.9274966601239785}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:55:43,406][0m Trial 21 finished with value: 0.062510184943676 and parameters: {'observation_period_num': 75, 'train_rates': 0.8329612992351012, 'learning_rate': 0.00021131096295582536, 'batch_size': 91, 'step_size': 13, 'gamma': 0.8331727694927021}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:57:07,012][0m Trial 22 finished with value: 0.06813886780596039 and parameters: {'observation_period_num': 65, 'train_rates': 0.809245574188363, 'learning_rate': 0.0002430863868729142, 'batch_size': 64, 'step_size': 14, 'gamma': 0.8374380252038744}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:58:03,452][0m Trial 23 finished with value: 0.06988583740511144 and parameters: {'observation_period_num': 26, 'train_rates': 0.911929506980276, 'learning_rate': 7.916608180234053e-05, 'batch_size': 109, 'step_size': 13, 'gamma': 0.7984441427375134}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 19:58:42,892][0m Trial 24 finished with value: 0.06913926922961285 and parameters: {'observation_period_num': 6, 'train_rates': 0.7807128935737517, 'learning_rate': 0.0005467029825527234, 'batch_size': 153, 'step_size': 15, 'gamma': 0.8639607677884574}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:00:07,573][0m Trial 25 finished with value: 0.07682979858275187 and parameters: {'observation_period_num': 43, 'train_rates': 0.85823410679687, 'learning_rate': 2.6800155989946713e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.8882326748264988}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:01:05,625][0m Trial 26 finished with value: 0.07526752663155396 and parameters: {'observation_period_num': 110, 'train_rates': 0.9411820375606783, 'learning_rate': 0.00022217023059607112, 'batch_size': 106, 'step_size': 14, 'gamma': 0.7788850298092531}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:03:03,537][0m Trial 27 finished with value: 0.06452530015561238 and parameters: {'observation_period_num': 25, 'train_rates': 0.8268727861947707, 'learning_rate': 6.217054120498664e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.8262245914132863}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:03:49,271][0m Trial 28 finished with value: 0.1605735272642166 and parameters: {'observation_period_num': 252, 'train_rates': 0.8833987244859592, 'learning_rate': 0.000118583384298626, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8476410316795492}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:04:24,679][0m Trial 29 finished with value: 0.269274549966585 and parameters: {'observation_period_num': 64, 'train_rates': 0.7298160015219695, 'learning_rate': 1.837432760906347e-05, 'batch_size': 151, 'step_size': 7, 'gamma': 0.8139571316786349}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:06:23,908][0m Trial 30 finished with value: 0.36478868434285705 and parameters: {'observation_period_num': 86, 'train_rates': 0.7567125660674991, 'learning_rate': 6.297428007168264e-06, 'batch_size': 42, 'step_size': 8, 'gamma': 0.7828382378313182}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:08:26,891][0m Trial 31 finished with value: 0.06842158740518647 and parameters: {'observation_period_num': 21, 'train_rates': 0.8225006777270248, 'learning_rate': 7.195979391510151e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8323872893291109}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:09:45,344][0m Trial 32 finished with value: 0.07304401648403767 and parameters: {'observation_period_num': 29, 'train_rates': 0.7898887110145103, 'learning_rate': 5.9481729970504855e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.8047664415074199}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:11:36,684][0m Trial 33 finished with value: 0.12347662988496044 and parameters: {'observation_period_num': 47, 'train_rates': 0.6991850303929117, 'learning_rate': 0.00014495126596500386, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8624701105411623}. Best is trial 12 with value: 0.058861047978822294.[0m
[32m[I 2025-01-05 20:13:10,753][0m Trial 34 finished with value: 0.058645304818363754 and parameters: {'observation_period_num': 20, 'train_rates': 0.826018371029121, 'learning_rate': 0.00034079700306912964, 'batch_size': 96, 'step_size': 12, 'gamma': 0.8269731077249067}. Best is trial 34 with value: 0.058645304818363754.[0m
[32m[I 2025-01-05 20:14:27,101][0m Trial 35 finished with value: 0.049915015198818344 and parameters: {'observation_period_num': 14, 'train_rates': 0.8704271336866358, 'learning_rate': 0.0003478936498117362, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8494384666475375}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:16:18,477][0m Trial 36 finished with value: 0.06246821128583084 and parameters: {'observation_period_num': 39, 'train_rates': 0.8740426422204939, 'learning_rate': 0.0005379886439757665, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8096931013762877}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:17:51,649][0m Trial 37 finished with value: 0.06857488434466105 and parameters: {'observation_period_num': 41, 'train_rates': 0.8702944486467149, 'learning_rate': 0.0005937395126061522, 'batch_size': 103, 'step_size': 10, 'gamma': 0.7922983436833378}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:19:26,272][0m Trial 38 finished with value: 0.08987017410727682 and parameters: {'observation_period_num': 54, 'train_rates': 0.907341414988057, 'learning_rate': 0.0003573908851878959, 'batch_size': 143, 'step_size': 7, 'gamma': 0.9867069624284837}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:21:10,961][0m Trial 39 finished with value: 0.1612129881978035 and parameters: {'observation_period_num': 178, 'train_rates': 0.943776366328941, 'learning_rate': 0.0006363803353733308, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8120339108925063}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:23:00,631][0m Trial 40 finished with value: 0.05609127912404282 and parameters: {'observation_period_num': 17, 'train_rates': 0.8664440716682914, 'learning_rate': 0.00027646588077483977, 'batch_size': 97, 'step_size': 8, 'gamma': 0.840536567045131}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:24:33,537][0m Trial 41 finished with value: 0.05834479693595956 and parameters: {'observation_period_num': 33, 'train_rates': 0.8735758661203732, 'learning_rate': 0.0003277020471140048, 'batch_size': 97, 'step_size': 8, 'gamma': 0.8425556098591528}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:26:11,014][0m Trial 42 finished with value: 0.05443709249598811 and parameters: {'observation_period_num': 16, 'train_rates': 0.8988593685636682, 'learning_rate': 0.00024495602058101915, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8443216150905084}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:27:46,584][0m Trial 43 finished with value: 0.056949227410774905 and parameters: {'observation_period_num': 19, 'train_rates': 0.8983984979949965, 'learning_rate': 0.00032070722047393874, 'batch_size': 121, 'step_size': 6, 'gamma': 0.844015976440383}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:29:20,681][0m Trial 44 finished with value: 0.0562800637435639 and parameters: {'observation_period_num': 33, 'train_rates': 0.9403716347236097, 'learning_rate': 0.00030505927352553463, 'batch_size': 125, 'step_size': 6, 'gamma': 0.8449642446598996}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:30:58,978][0m Trial 45 finished with value: 0.3322173548745109 and parameters: {'observation_period_num': 17, 'train_rates': 0.9514403472138524, 'learning_rate': 7.080659881095056e-06, 'batch_size': 119, 'step_size': 4, 'gamma': 0.8729316393857411}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:32:18,713][0m Trial 46 finished with value: 0.061510107506598743 and parameters: {'observation_period_num': 5, 'train_rates': 0.9288427327324112, 'learning_rate': 0.0002769538112348315, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8525739811288559}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:33:37,772][0m Trial 47 finished with value: 0.1539502381136193 and parameters: {'observation_period_num': 150, 'train_rates': 0.8978538417769851, 'learning_rate': 0.0007605623483549285, 'batch_size': 249, 'step_size': 6, 'gamma': 0.8679035122325446}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:34:54,728][0m Trial 48 finished with value: 0.0538887083530426 and parameters: {'observation_period_num': 52, 'train_rates': 0.9650609492168762, 'learning_rate': 0.0004394861967236125, 'batch_size': 161, 'step_size': 5, 'gamma': 0.8975316993784422}. Best is trial 35 with value: 0.049915015198818344.[0m
[32m[I 2025-01-05 20:36:17,390][0m Trial 49 finished with value: 0.06305404007434845 and parameters: {'observation_period_num': 102, 'train_rates': 0.9841245748441633, 'learning_rate': 0.00044877313248757914, 'batch_size': 168, 'step_size': 4, 'gamma': 0.9590066903363906}. Best is trial 35 with value: 0.049915015198818344.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 20:36:17,400][0m A new study created in memory with name: no-name-2013a909-0c08-462e-a9ae-e6a175e39998[0m
[32m[I 2025-01-05 20:40:01,812][0m Trial 0 finished with value: 0.023616208171006292 and parameters: {'observation_period_num': 13, 'train_rates': 0.9782848377586508, 'learning_rate': 0.00013617271938485957, 'batch_size': 30, 'step_size': 9, 'gamma': 0.7803631435511014}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:41:27,464][0m Trial 1 finished with value: 0.12741595426040564 and parameters: {'observation_period_num': 83, 'train_rates': 0.6521063158094337, 'learning_rate': 3.17545041629939e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8427131153096955}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:43:46,376][0m Trial 2 finished with value: 0.5177957136784831 and parameters: {'observation_period_num': 184, 'train_rates': 0.9552927081328959, 'learning_rate': 1.5221404016039795e-06, 'batch_size': 51, 'step_size': 12, 'gamma': 0.8848466569673883}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:45:09,742][0m Trial 3 finished with value: 0.07019105592131307 and parameters: {'observation_period_num': 62, 'train_rates': 0.7322962197826626, 'learning_rate': 7.909737683623585e-05, 'batch_size': 142, 'step_size': 11, 'gamma': 0.7931761682529039}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:46:18,580][0m Trial 4 finished with value: 0.1414134319078818 and parameters: {'observation_period_num': 51, 'train_rates': 0.6453125074352573, 'learning_rate': 1.1486193874314372e-05, 'batch_size': 114, 'step_size': 7, 'gamma': 0.9826359404912642}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:47:47,966][0m Trial 5 finished with value: 1.0762629508972168 and parameters: {'observation_period_num': 79, 'train_rates': 0.9340745417905337, 'learning_rate': 1.1397684872585674e-06, 'batch_size': 192, 'step_size': 14, 'gamma': 0.9645394943137657}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:49:39,177][0m Trial 6 finished with value: 0.4395936037235926 and parameters: {'observation_period_num': 151, 'train_rates': 0.8261306270187003, 'learning_rate': 5.019610461166198e-06, 'batch_size': 62, 'step_size': 4, 'gamma': 0.8475929131250762}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:51:05,032][0m Trial 7 finished with value: 0.14030649112583993 and parameters: {'observation_period_num': 95, 'train_rates': 0.7898463621262232, 'learning_rate': 2.4990201246391055e-05, 'batch_size': 182, 'step_size': 15, 'gamma': 0.85845715536172}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:52:00,943][0m Trial 8 finished with value: 0.2210535501935994 and parameters: {'observation_period_num': 171, 'train_rates': 0.6685618262793226, 'learning_rate': 0.00020356507582222118, 'batch_size': 239, 'step_size': 9, 'gamma': 0.9574505195259321}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:55:25,610][0m Trial 9 finished with value: 0.039491213612905404 and parameters: {'observation_period_num': 25, 'train_rates': 0.7562808481739707, 'learning_rate': 6.751368084513158e-05, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8697921039252559}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 20:59:43,472][0m Trial 10 finished with value: 0.13798681128553406 and parameters: {'observation_period_num': 232, 'train_rates': 0.8675606152072031, 'learning_rate': 0.0007762424922057037, 'batch_size': 23, 'step_size': 3, 'gamma': 0.7672761013009156}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 21:04:57,442][0m Trial 11 finished with value: 0.035684620538218456 and parameters: {'observation_period_num': 15, 'train_rates': 0.7455299128014529, 'learning_rate': 0.00017469538060165256, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9045280862870504}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 21:06:44,295][0m Trial 12 finished with value: 0.030404542727579534 and parameters: {'observation_period_num': 12, 'train_rates': 0.8795102736142237, 'learning_rate': 0.0003632377895350867, 'batch_size': 97, 'step_size': 7, 'gamma': 0.9155449248336551}. Best is trial 0 with value: 0.023616208171006292.[0m
[32m[I 2025-01-05 21:08:28,178][0m Trial 13 finished with value: 0.01627248525619507 and parameters: {'observation_period_num': 8, 'train_rates': 0.9879719816512463, 'learning_rate': 0.000703155254653984, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9375957401128853}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:09:34,508][0m Trial 14 finished with value: 0.081436887383461 and parameters: {'observation_period_num': 121, 'train_rates': 0.9850854356835783, 'learning_rate': 0.0005514256993379774, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8113174420863797}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:11:23,951][0m Trial 15 finished with value: 0.044549665261879035 and parameters: {'observation_period_num': 46, 'train_rates': 0.9030822994418451, 'learning_rate': 0.0009425278451371909, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9301929383784803}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:12:40,326][0m Trial 16 finished with value: 0.08224836736917496 and parameters: {'observation_period_num': 114, 'train_rates': 0.9871781123514696, 'learning_rate': 0.00021277335764779222, 'batch_size': 118, 'step_size': 10, 'gamma': 0.7528189862963329}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:14:04,068][0m Trial 17 finished with value: 0.0681148722436693 and parameters: {'observation_period_num': 37, 'train_rates': 0.9230342429767289, 'learning_rate': 8.981971223425656e-05, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8040278870897203}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:15:55,630][0m Trial 18 finished with value: 0.11803116671089987 and parameters: {'observation_period_num': 236, 'train_rates': 0.8483393153408736, 'learning_rate': 0.0004026349953027812, 'batch_size': 48, 'step_size': 2, 'gamma': 0.8256745431362762}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:17:07,054][0m Trial 19 finished with value: 0.05547296547597939 and parameters: {'observation_period_num': 8, 'train_rates': 0.9516852315612955, 'learning_rate': 3.4910143757454145e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9401750887914699}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:18:30,045][0m Trial 20 finished with value: 0.059399030509704534 and parameters: {'observation_period_num': 59, 'train_rates': 0.9000838013090257, 'learning_rate': 0.00014369883204901038, 'batch_size': 84, 'step_size': 5, 'gamma': 0.7850284908217011}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:19:50,063][0m Trial 21 finished with value: 0.02768962549422174 and parameters: {'observation_period_num': 5, 'train_rates': 0.8723756615208845, 'learning_rate': 0.00035613618541315543, 'batch_size': 99, 'step_size': 7, 'gamma': 0.9096250956516121}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:22:04,257][0m Trial 22 finished with value: 0.02926097810268402 and parameters: {'observation_period_num': 33, 'train_rates': 0.9866986709182524, 'learning_rate': 0.0003339132728599561, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8896238525732524}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:23:37,627][0m Trial 23 finished with value: 0.028622815299594234 and parameters: {'observation_period_num': 5, 'train_rates': 0.8087892720075425, 'learning_rate': 0.0009846967270942056, 'batch_size': 70, 'step_size': 9, 'gamma': 0.9290821872170905}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:25:13,814][0m Trial 24 finished with value: 0.03973533109658294 and parameters: {'observation_period_num': 28, 'train_rates': 0.9538269333399066, 'learning_rate': 0.0005084821061007072, 'batch_size': 130, 'step_size': 5, 'gamma': 0.892412603369604}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:26:31,589][0m Trial 25 finished with value: 0.06971825433363876 and parameters: {'observation_period_num': 68, 'train_rates': 0.9138745004573927, 'learning_rate': 0.00011743999258706708, 'batch_size': 161, 'step_size': 8, 'gamma': 0.9548754220497063}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:28:18,294][0m Trial 26 finished with value: 0.07421055595937878 and parameters: {'observation_period_num': 96, 'train_rates': 0.8743831769317872, 'learning_rate': 0.0002908347942365127, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9107313187450948}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:30:43,658][0m Trial 27 finished with value: 0.1319231771189591 and parameters: {'observation_period_num': 209, 'train_rates': 0.9578702868215923, 'learning_rate': 5.371856846373334e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.9864365259460149}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:31:59,993][0m Trial 28 finished with value: 0.047348955524149944 and parameters: {'observation_period_num': 42, 'train_rates': 0.8427222915963866, 'learning_rate': 0.0005884562044106714, 'batch_size': 236, 'step_size': 3, 'gamma': 0.8741400268437269}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:33:09,160][0m Trial 29 finished with value: 0.2861453890800476 and parameters: {'observation_period_num': 139, 'train_rates': 0.929093370457591, 'learning_rate': 1.7019925548101926e-05, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8325956792706479}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:34:39,704][0m Trial 30 finished with value: 0.1565091085380019 and parameters: {'observation_period_num': 80, 'train_rates': 0.6152936840827553, 'learning_rate': 0.0002534547203957655, 'batch_size': 80, 'step_size': 6, 'gamma': 0.9432710486155309}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:36:40,293][0m Trial 31 finished with value: 0.028446178931694526 and parameters: {'observation_period_num': 6, 'train_rates': 0.791030289130228, 'learning_rate': 0.0008362545114206846, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9242783286768842}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:38:25,785][0m Trial 32 finished with value: 0.04663505504300835 and parameters: {'observation_period_num': 23, 'train_rates': 0.7768881927902336, 'learning_rate': 0.0006498927255105544, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9231676131686183}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:41:18,472][0m Trial 33 finished with value: 0.077237816618605 and parameters: {'observation_period_num': 5, 'train_rates': 0.7016520185016659, 'learning_rate': 0.00045428755943230554, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9415814623868244}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:43:28,696][0m Trial 34 finished with value: 0.046846351657922454 and parameters: {'observation_period_num': 22, 'train_rates': 0.9646440447293315, 'learning_rate': 0.00011883847258558444, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8931644479428373}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:44:55,895][0m Trial 35 finished with value: 0.08195173723485835 and parameters: {'observation_period_num': 53, 'train_rates': 0.8178040633789199, 'learning_rate': 0.0007052504765562405, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9685733955347675}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:46:21,892][0m Trial 36 finished with value: 0.07505574355452833 and parameters: {'observation_period_num': 69, 'train_rates': 0.7044160953237517, 'learning_rate': 0.00027142250860881143, 'batch_size': 92, 'step_size': 7, 'gamma': 0.9035680152183853}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:47:56,202][0m Trial 37 finished with value: 0.306430379152298 and parameters: {'observation_period_num': 38, 'train_rates': 0.8455848102225784, 'learning_rate': 2.959019341484407e-06, 'batch_size': 123, 'step_size': 11, 'gamma': 0.8741005036054639}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:51:04,998][0m Trial 38 finished with value: 0.06390053766780977 and parameters: {'observation_period_num': 19, 'train_rates': 0.7691154722943551, 'learning_rate': 6.29519210776574e-06, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9724075831038583}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:52:53,612][0m Trial 39 finished with value: 0.06035976721596839 and parameters: {'observation_period_num': 51, 'train_rates': 0.7970517252643894, 'learning_rate': 4.898037185487956e-05, 'batch_size': 58, 'step_size': 4, 'gamma': 0.8593452925449971}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:54:37,151][0m Trial 40 finished with value: 0.0895952133461833 and parameters: {'observation_period_num': 100, 'train_rates': 0.887927226997774, 'learning_rate': 0.0003917366432443585, 'batch_size': 82, 'step_size': 11, 'gamma': 0.9224976390437989}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:56:29,342][0m Trial 41 finished with value: 0.024913132056676373 and parameters: {'observation_period_num': 8, 'train_rates': 0.8159293976540967, 'learning_rate': 0.0009688231211991792, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9275079918943381}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:57:58,529][0m Trial 42 finished with value: 0.04860255352475427 and parameters: {'observation_period_num': 30, 'train_rates': 0.7176883465497248, 'learning_rate': 0.000986143295408913, 'batch_size': 105, 'step_size': 7, 'gamma': 0.9480791396835265}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 21:59:46,413][0m Trial 43 finished with value: 0.0299665205893837 and parameters: {'observation_period_num': 15, 'train_rates': 0.8328360428592086, 'learning_rate': 0.0006942511974571815, 'batch_size': 70, 'step_size': 10, 'gamma': 0.9015475856947727}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:05:34,480][0m Trial 44 finished with value: 0.15283275540555802 and parameters: {'observation_period_num': 163, 'train_rates': 0.8564574221304899, 'learning_rate': 0.00018407054051912386, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9327089699680368}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:08:11,412][0m Trial 45 finished with value: 0.02693240165684624 and parameters: {'observation_period_num': 6, 'train_rates': 0.8041405208797163, 'learning_rate': 0.000501149458751619, 'batch_size': 40, 'step_size': 12, 'gamma': 0.9148280146291543}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:11:26,527][0m Trial 46 finished with value: 0.04550564401476077 and parameters: {'observation_period_num': 19, 'train_rates': 0.9391750603698469, 'learning_rate': 0.00046417309365054805, 'batch_size': 36, 'step_size': 14, 'gamma': 0.9139896989979148}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:15:19,782][0m Trial 47 finished with value: 0.038393065333366394 and parameters: {'observation_period_num': 44, 'train_rates': 0.9722991558088175, 'learning_rate': 0.00023207734516839684, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8828249236499358}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:17:40,433][0m Trial 48 finished with value: 0.06143266750144693 and parameters: {'observation_period_num': 31, 'train_rates': 0.7539798815142115, 'learning_rate': 0.0003179482354620825, 'batch_size': 44, 'step_size': 15, 'gamma': 0.9575852860121832}. Best is trial 13 with value: 0.01627248525619507.[0m
[32m[I 2025-01-05 22:18:51,358][0m Trial 49 finished with value: 0.03960036754663756 and parameters: {'observation_period_num': 15, 'train_rates': 0.8177290962462932, 'learning_rate': 9.422461967412297e-05, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8526471487606293}. Best is trial 13 with value: 0.01627248525619507.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 22:18:51,368][0m A new study created in memory with name: no-name-853b40af-036a-41b7-b2c0-ad70acc878bd[0m
[32m[I 2025-01-05 22:21:32,587][0m Trial 0 finished with value: 0.12933969408273696 and parameters: {'observation_period_num': 176, 'train_rates': 0.864939617633318, 'learning_rate': 0.0007368642820317534, 'batch_size': 45, 'step_size': 2, 'gamma': 0.760227145118633}. Best is trial 0 with value: 0.12933969408273696.[0m
[32m[I 2025-01-05 22:22:56,337][0m Trial 1 finished with value: 0.47010404959212254 and parameters: {'observation_period_num': 198, 'train_rates': 0.95032361389864, 'learning_rate': 4.0862439860307905e-06, 'batch_size': 132, 'step_size': 10, 'gamma': 0.942602312755196}. Best is trial 0 with value: 0.12933969408273696.[0m
[32m[I 2025-01-05 22:24:15,092][0m Trial 2 finished with value: 0.10722973630871883 and parameters: {'observation_period_num': 196, 'train_rates': 0.9063548523454006, 'learning_rate': 0.0002916224311567107, 'batch_size': 162, 'step_size': 11, 'gamma': 0.8766934949650019}. Best is trial 2 with value: 0.10722973630871883.[0m
[32m[I 2025-01-05 22:25:56,434][0m Trial 3 finished with value: 0.09692658440997967 and parameters: {'observation_period_num': 187, 'train_rates': 0.9626952195767543, 'learning_rate': 0.00018123181158149966, 'batch_size': 82, 'step_size': 10, 'gamma': 0.784841259639392}. Best is trial 3 with value: 0.09692658440997967.[0m
[32m[I 2025-01-05 22:27:26,836][0m Trial 4 finished with value: 0.1159533141686703 and parameters: {'observation_period_num': 31, 'train_rates': 0.7857880618162504, 'learning_rate': 6.308371321942736e-06, 'batch_size': 103, 'step_size': 15, 'gamma': 0.9756189506894243}. Best is trial 3 with value: 0.09692658440997967.[0m
[32m[I 2025-01-05 22:28:32,328][0m Trial 5 finished with value: 0.1200669091759306 and parameters: {'observation_period_num': 155, 'train_rates': 0.6461359331349877, 'learning_rate': 0.0007245414772517126, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8425072508649677}. Best is trial 3 with value: 0.09692658440997967.[0m
[32m[I 2025-01-05 22:29:55,480][0m Trial 6 finished with value: 0.637148140736346 and parameters: {'observation_period_num': 101, 'train_rates': 0.8887093291315722, 'learning_rate': 2.036066566231741e-06, 'batch_size': 129, 'step_size': 9, 'gamma': 0.9319505899097622}. Best is trial 3 with value: 0.09692658440997967.[0m
Early stopping at epoch 71
[32m[I 2025-01-05 22:30:45,269][0m Trial 7 finished with value: 0.8349907499733971 and parameters: {'observation_period_num': 115, 'train_rates': 0.9196009012848017, 'learning_rate': 1.341746488386745e-06, 'batch_size': 183, 'step_size': 1, 'gamma': 0.8821287935355739}. Best is trial 3 with value: 0.09692658440997967.[0m
[32m[I 2025-01-05 22:31:42,613][0m Trial 8 finished with value: 0.0783281736566971 and parameters: {'observation_period_num': 107, 'train_rates': 0.8133217677676311, 'learning_rate': 0.0008990989141719118, 'batch_size': 140, 'step_size': 14, 'gamma': 0.8362231383928479}. Best is trial 8 with value: 0.0783281736566971.[0m
[32m[I 2025-01-05 22:33:21,931][0m Trial 9 finished with value: 0.5897783678153465 and parameters: {'observation_period_num': 124, 'train_rates': 0.6308960160560008, 'learning_rate': 1.4499536490172039e-06, 'batch_size': 63, 'step_size': 13, 'gamma': 0.9812596095232308}. Best is trial 8 with value: 0.0783281736566971.[0m
[32m[I 2025-01-05 22:34:10,884][0m Trial 10 finished with value: 0.2115009924052322 and parameters: {'observation_period_num': 63, 'train_rates': 0.7512710936334708, 'learning_rate': 3.533246585315289e-05, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8257730986182318}. Best is trial 8 with value: 0.0783281736566971.[0m
[32m[I 2025-01-05 22:35:50,782][0m Trial 11 finished with value: 0.10355749726295471 and parameters: {'observation_period_num': 238, 'train_rates': 0.9813074476147224, 'learning_rate': 0.00010511329710351618, 'batch_size': 76, 'step_size': 15, 'gamma': 0.7775292044495118}. Best is trial 8 with value: 0.0783281736566971.[0m
[32m[I 2025-01-05 22:40:05,078][0m Trial 12 finished with value: 0.15338076678341786 and parameters: {'observation_period_num': 243, 'train_rates': 0.7375274243042438, 'learning_rate': 0.00013261502456846444, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8031260529228774}. Best is trial 8 with value: 0.0783281736566971.[0m
[32m[I 2025-01-05 22:41:06,267][0m Trial 13 finished with value: 0.06377570534294302 and parameters: {'observation_period_num': 73, 'train_rates': 0.8321379624776831, 'learning_rate': 0.00025595099425581224, 'batch_size': 202, 'step_size': 7, 'gamma': 0.7990143314498259}. Best is trial 13 with value: 0.06377570534294302.[0m
[32m[I 2025-01-05 22:42:13,744][0m Trial 14 finished with value: 0.13801923846837624 and parameters: {'observation_period_num': 74, 'train_rates': 0.8336532343347759, 'learning_rate': 3.9849995165950864e-05, 'batch_size': 215, 'step_size': 6, 'gamma': 0.8414119796501395}. Best is trial 13 with value: 0.06377570534294302.[0m
[32m[I 2025-01-05 22:43:32,212][0m Trial 15 finished with value: 0.05457903920120783 and parameters: {'observation_period_num': 5, 'train_rates': 0.7001220525718637, 'learning_rate': 0.00041669872250077233, 'batch_size': 201, 'step_size': 4, 'gamma': 0.8147336937196794}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:44:32,283][0m Trial 16 finished with value: 0.21007397479481169 and parameters: {'observation_period_num': 8, 'train_rates': 0.6944408321055016, 'learning_rate': 1.4941854035131355e-05, 'batch_size': 208, 'step_size': 4, 'gamma': 0.7520432048458485}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:45:43,609][0m Trial 17 finished with value: 0.07728848083151711 and parameters: {'observation_period_num': 46, 'train_rates': 0.6901976750912907, 'learning_rate': 0.0003193831662589148, 'batch_size': 242, 'step_size': 4, 'gamma': 0.8089654314345405}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:46:46,699][0m Trial 18 finished with value: 0.07607715574667809 and parameters: {'observation_period_num': 11, 'train_rates': 0.6006699238344823, 'learning_rate': 9.47092420052376e-05, 'batch_size': 187, 'step_size': 8, 'gamma': 0.9039680456545465}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:47:54,692][0m Trial 19 finished with value: 0.09359589920324438 and parameters: {'observation_period_num': 76, 'train_rates': 0.7637705920373837, 'learning_rate': 0.00038230956067959976, 'batch_size': 220, 'step_size': 3, 'gamma': 0.801444820625582}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:49:04,909][0m Trial 20 finished with value: 0.15282453683500302 and parameters: {'observation_period_num': 39, 'train_rates': 0.717672570096734, 'learning_rate': 1.9246536653804547e-05, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8562213978550665}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:50:04,512][0m Trial 21 finished with value: 0.08053930103457138 and parameters: {'observation_period_num': 5, 'train_rates': 0.6072457388902789, 'learning_rate': 8.685580270731149e-05, 'batch_size': 191, 'step_size': 8, 'gamma': 0.9035545143026587}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:51:01,261][0m Trial 22 finished with value: 0.07424832083352793 and parameters: {'observation_period_num': 18, 'train_rates': 0.663610452129859, 'learning_rate': 6.711757291403577e-05, 'batch_size': 228, 'step_size': 5, 'gamma': 0.860355375614884}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:52:01,845][0m Trial 23 finished with value: 0.12483754075054392 and parameters: {'observation_period_num': 28, 'train_rates': 0.6567170541060415, 'learning_rate': 4.787433300112446e-05, 'batch_size': 229, 'step_size': 4, 'gamma': 0.8198051683366466}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:52:48,528][0m Trial 24 finished with value: 0.09878018544332401 and parameters: {'observation_period_num': 58, 'train_rates': 0.6805984360806191, 'learning_rate': 0.0002071499713624357, 'batch_size': 256, 'step_size': 5, 'gamma': 0.7796487935039006}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:53:33,091][0m Trial 25 finished with value: 0.09576951577720871 and parameters: {'observation_period_num': 86, 'train_rates': 0.7973140993186044, 'learning_rate': 0.00043343775570882945, 'batch_size': 203, 'step_size': 2, 'gamma': 0.8611162182050875}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:54:53,043][0m Trial 26 finished with value: 0.05510070360424113 and parameters: {'observation_period_num': 25, 'train_rates': 0.8355380139243493, 'learning_rate': 7.441312690413781e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.8910981637198085}. Best is trial 15 with value: 0.05457903920120783.[0m
[32m[I 2025-01-05 22:56:04,259][0m Trial 27 finished with value: 0.04207004114812866 and parameters: {'observation_period_num': 50, 'train_rates': 0.8216483709524606, 'learning_rate': 0.0002158934982063486, 'batch_size': 165, 'step_size': 7, 'gamma': 0.8887837343026863}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 22:57:17,371][0m Trial 28 finished with value: 0.27118614241985384 and parameters: {'observation_period_num': 47, 'train_rates': 0.847681346066331, 'learning_rate': 1.696567477217428e-05, 'batch_size': 165, 'step_size': 3, 'gamma': 0.89725464123529}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 22:58:35,265][0m Trial 29 finished with value: 0.21661521572815745 and parameters: {'observation_period_num': 145, 'train_rates': 0.8783555247277767, 'learning_rate': 0.00015538478069438186, 'batch_size': 168, 'step_size': 1, 'gamma': 0.9242352454753489}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:00:02,023][0m Trial 30 finished with value: 0.04822676460135658 and parameters: {'observation_period_num': 29, 'train_rates': 0.8551772919866496, 'learning_rate': 0.0005730470314611159, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9510801959175654}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:01:07,990][0m Trial 31 finished with value: 0.04903344172635116 and parameters: {'observation_period_num': 26, 'train_rates': 0.8687378148523777, 'learning_rate': 0.0006031578741745889, 'batch_size': 117, 'step_size': 7, 'gamma': 0.9492424730593211}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:02:35,733][0m Trial 32 finished with value: 0.06113724985179749 and parameters: {'observation_period_num': 52, 'train_rates': 0.8706197622758737, 'learning_rate': 0.0005563560602332906, 'batch_size': 113, 'step_size': 9, 'gamma': 0.9625560078570803}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:03:52,297][0m Trial 33 finished with value: 0.05657157193557675 and parameters: {'observation_period_num': 34, 'train_rates': 0.913956846281332, 'learning_rate': 0.0005795154925773886, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9391245696135954}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:05:15,444][0m Trial 34 finished with value: 0.04782678852277684 and parameters: {'observation_period_num': 21, 'train_rates': 0.9411937017368547, 'learning_rate': 0.000997698527225309, 'batch_size': 124, 'step_size': 8, 'gamma': 0.9567456074411957}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:06:43,310][0m Trial 35 finished with value: 0.11404831504279916 and parameters: {'observation_period_num': 91, 'train_rates': 0.9462098021633316, 'learning_rate': 0.0008557729420305276, 'batch_size': 98, 'step_size': 10, 'gamma': 0.9545826323695179}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:08:02,641][0m Trial 36 finished with value: 0.043245600955037586 and parameters: {'observation_period_num': 23, 'train_rates': 0.9392576439967515, 'learning_rate': 0.000999074542733936, 'batch_size': 125, 'step_size': 9, 'gamma': 0.9221767189318655}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:09:22,038][0m Trial 37 finished with value: 0.05611426071466311 and parameters: {'observation_period_num': 36, 'train_rates': 0.9371812407622787, 'learning_rate': 0.0009489885619975913, 'batch_size': 146, 'step_size': 9, 'gamma': 0.9185373006255085}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:10:39,398][0m Trial 38 finished with value: 0.3004751205444336 and parameters: {'observation_period_num': 215, 'train_rates': 0.9851048159890033, 'learning_rate': 0.0003008484701425382, 'batch_size': 128, 'step_size': 11, 'gamma': 0.9896466239671476}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:12:28,684][0m Trial 39 finished with value: 0.0885381809389982 and parameters: {'observation_period_num': 63, 'train_rates': 0.8944222982855641, 'learning_rate': 0.0009844772560163061, 'batch_size': 93, 'step_size': 10, 'gamma': 0.9660507487082605}. Best is trial 27 with value: 0.04207004114812866.[0m
[32m[I 2025-01-05 23:13:41,525][0m Trial 40 finished with value: 0.03963770344853401 and parameters: {'observation_period_num': 21, 'train_rates': 0.9628113887044799, 'learning_rate': 0.0002238292292963187, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9140432219782495}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:15:04,739][0m Trial 41 finished with value: 0.041553396731615067 and parameters: {'observation_period_num': 21, 'train_rates': 0.9617314245395137, 'learning_rate': 0.00019947718786672761, 'batch_size': 155, 'step_size': 11, 'gamma': 0.9173833627907827}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:16:08,540][0m Trial 42 finished with value: 0.051232337951660156 and parameters: {'observation_period_num': 21, 'train_rates': 0.9623688223538058, 'learning_rate': 0.00017269383035207415, 'batch_size': 152, 'step_size': 12, 'gamma': 0.9119313466387584}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:17:10,696][0m Trial 43 finished with value: 0.048424773224412576 and parameters: {'observation_period_num': 44, 'train_rates': 0.9248059761735477, 'learning_rate': 0.00021483660311574546, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8815068927464006}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:18:31,476][0m Trial 44 finished with value: 0.2650037705898285 and parameters: {'observation_period_num': 18, 'train_rates': 0.966027473737434, 'learning_rate': 3.8527091345892095e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9345247587367486}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:19:33,762][0m Trial 45 finished with value: 0.0874332094060782 and parameters: {'observation_period_num': 56, 'train_rates': 0.9376734491067994, 'learning_rate': 2.5765937718449027e-05, 'batch_size': 141, 'step_size': 12, 'gamma': 0.9245494955517708}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:20:49,062][0m Trial 46 finished with value: 0.04054801166057587 and parameters: {'observation_period_num': 15, 'train_rates': 0.9644814955442738, 'learning_rate': 0.00012291294147115912, 'batch_size': 125, 'step_size': 13, 'gamma': 0.8739181921566493}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:21:48,403][0m Trial 47 finished with value: 0.0823390781879425 and parameters: {'observation_period_num': 142, 'train_rates': 0.9675066008952354, 'learning_rate': 0.00011978134049964763, 'batch_size': 176, 'step_size': 14, 'gamma': 0.8722065481309739}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:23:36,028][0m Trial 48 finished with value: 0.07068717344934217 and parameters: {'observation_period_num': 39, 'train_rates': 0.8996300752553908, 'learning_rate': 6.257636140893019e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.9104536674933761}. Best is trial 40 with value: 0.03963770344853401.[0m
[32m[I 2025-01-05 23:24:33,797][0m Trial 49 finished with value: 0.06769916415214539 and parameters: {'observation_period_num': 67, 'train_rates': 0.9876789896303532, 'learning_rate': 0.00023725062349897992, 'batch_size': 153, 'step_size': 13, 'gamma': 0.8886480325736266}. Best is trial 40 with value: 0.03963770344853401.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 23:24:33,807][0m A new study created in memory with name: no-name-ad7cf4af-177e-466f-a4e3-4dbaeec8a897[0m
[32m[I 2025-01-05 23:25:57,238][0m Trial 0 finished with value: 0.05181482037514206 and parameters: {'observation_period_num': 24, 'train_rates': 0.8321996109952423, 'learning_rate': 6.358335791239563e-05, 'batch_size': 212, 'step_size': 14, 'gamma': 0.8121417713110888}. Best is trial 0 with value: 0.05181482037514206.[0m
[32m[I 2025-01-05 23:27:19,006][0m Trial 1 finished with value: 0.6814041233740712 and parameters: {'observation_period_num': 68, 'train_rates': 0.7073397085150832, 'learning_rate': 1.6643719398614707e-06, 'batch_size': 89, 'step_size': 6, 'gamma': 0.8867310486937111}. Best is trial 0 with value: 0.05181482037514206.[0m
[32m[I 2025-01-05 23:29:05,654][0m Trial 2 finished with value: 0.08143643581105355 and parameters: {'observation_period_num': 103, 'train_rates': 0.9534015429650735, 'learning_rate': 0.0005863675962285226, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9262484308985193}. Best is trial 0 with value: 0.05181482037514206.[0m
[32m[I 2025-01-05 23:30:08,144][0m Trial 3 finished with value: 0.10112572961989261 and parameters: {'observation_period_num': 84, 'train_rates': 0.6683003074352594, 'learning_rate': 5.6188530166437435e-05, 'batch_size': 228, 'step_size': 14, 'gamma': 0.9649628840372897}. Best is trial 0 with value: 0.05181482037514206.[0m
[32m[I 2025-01-05 23:33:50,266][0m Trial 4 finished with value: 0.439095191292043 and parameters: {'observation_period_num': 206, 'train_rates': 0.8071108336793302, 'learning_rate': 1.985282856085291e-06, 'batch_size': 26, 'step_size': 6, 'gamma': 0.8629166373622885}. Best is trial 0 with value: 0.05181482037514206.[0m
[32m[I 2025-01-05 23:35:35,691][0m Trial 5 finished with value: 0.04906584944715918 and parameters: {'observation_period_num': 13, 'train_rates': 0.7553522674744082, 'learning_rate': 5.7789187458433194e-05, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8233390996453736}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:36:39,290][0m Trial 6 finished with value: 0.10032181312026603 and parameters: {'observation_period_num': 157, 'train_rates': 0.745177941189151, 'learning_rate': 0.0003932080189865203, 'batch_size': 180, 'step_size': 8, 'gamma': 0.8184574409644875}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:38:07,656][0m Trial 7 finished with value: 0.29275570900470566 and parameters: {'observation_period_num': 111, 'train_rates': 0.7511219830272625, 'learning_rate': 1.9909800605098593e-05, 'batch_size': 113, 'step_size': 8, 'gamma': 0.7684210678422162}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:44:27,248][0m Trial 8 finished with value: 0.061571093119737755 and parameters: {'observation_period_num': 52, 'train_rates': 0.9550410686455493, 'learning_rate': 2.188130161222405e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.917538847906638}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:45:55,045][0m Trial 9 finished with value: 0.5107366442680359 and parameters: {'observation_period_num': 148, 'train_rates': 0.9723843727369295, 'learning_rate': 4.843631421716038e-06, 'batch_size': 247, 'step_size': 13, 'gamma': 0.829571860357803}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:46:58,822][0m Trial 10 finished with value: 0.06824274930247845 and parameters: {'observation_period_num': 13, 'train_rates': 0.6131396500854456, 'learning_rate': 0.00017028912474981243, 'batch_size': 160, 'step_size': 11, 'gamma': 0.7524909218308937}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:48:22,292][0m Trial 11 finished with value: 0.054159307676666185 and parameters: {'observation_period_num': 12, 'train_rates': 0.8377068245059691, 'learning_rate': 8.262626127599928e-05, 'batch_size': 198, 'step_size': 11, 'gamma': 0.8114670717959935}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:50:32,193][0m Trial 12 finished with value: 0.2648957996521814 and parameters: {'observation_period_num': 42, 'train_rates': 0.8720382106007308, 'learning_rate': 1.1469241311402563e-05, 'batch_size': 61, 'step_size': 3, 'gamma': 0.7941761633776777}. Best is trial 5 with value: 0.04906584944715918.[0m
[32m[I 2025-01-05 23:52:06,491][0m Trial 13 finished with value: 0.032283608728351815 and parameters: {'observation_period_num': 5, 'train_rates': 0.8838169065309208, 'learning_rate': 0.00015305586366833564, 'batch_size': 124, 'step_size': 11, 'gamma': 0.854381970495337}. Best is trial 13 with value: 0.032283608728351815.[0m
[32m[I 2025-01-05 23:53:23,301][0m Trial 14 finished with value: 0.1163055755912441 and parameters: {'observation_period_num': 241, 'train_rates': 0.8924027955154221, 'learning_rate': 0.0002226504953563221, 'batch_size': 140, 'step_size': 10, 'gamma': 0.8592955107178967}. Best is trial 13 with value: 0.032283608728351815.[0m
[32m[I 2025-01-05 23:55:05,311][0m Trial 15 finished with value: 0.025825289889208732 and parameters: {'observation_period_num': 5, 'train_rates': 0.9056214239746089, 'learning_rate': 0.0009510689268301915, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8445617972345013}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-05 23:56:18,645][0m Trial 16 finished with value: 0.06002297781079131 and parameters: {'observation_period_num': 47, 'train_rates': 0.9064471286907643, 'learning_rate': 0.0009717825283223446, 'batch_size': 121, 'step_size': 10, 'gamma': 0.8981451951807387}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-05 23:57:52,240][0m Trial 17 finished with value: 0.1163977423346922 and parameters: {'observation_period_num': 184, 'train_rates': 0.9120136159482122, 'learning_rate': 0.0002016654674449498, 'batch_size': 145, 'step_size': 12, 'gamma': 0.8526025911027977}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-05 23:59:12,710][0m Trial 18 finished with value: 0.06277688232989147 and parameters: {'observation_period_num': 80, 'train_rates': 0.8586716953025826, 'learning_rate': 0.0009882585986640696, 'batch_size': 112, 'step_size': 15, 'gamma': 0.978412345805723}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:00:48,606][0m Trial 19 finished with value: 0.03680687110069432 and parameters: {'observation_period_num': 36, 'train_rates': 0.9188491448333254, 'learning_rate': 0.0003717836917466697, 'batch_size': 168, 'step_size': 9, 'gamma': 0.9403716675290762}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:03:16,992][0m Trial 20 finished with value: 0.05692391264049903 and parameters: {'observation_period_num': 127, 'train_rates': 0.7964431959463094, 'learning_rate': 0.00011885459588589444, 'batch_size': 43, 'step_size': 4, 'gamma': 0.8887439364935467}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:04:47,938][0m Trial 21 finished with value: 0.045472603774181115 and parameters: {'observation_period_num': 36, 'train_rates': 0.92614634488986, 'learning_rate': 0.00041373830440833434, 'batch_size': 154, 'step_size': 9, 'gamma': 0.9416238379873034}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:06:12,599][0m Trial 22 finished with value: 0.04614581813414891 and parameters: {'observation_period_num': 61, 'train_rates': 0.9325661702205127, 'learning_rate': 0.0003298430123745379, 'batch_size': 166, 'step_size': 9, 'gamma': 0.8463798085210229}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:07:35,724][0m Trial 23 finished with value: 0.028880192735683884 and parameters: {'observation_period_num': 6, 'train_rates': 0.877703167385125, 'learning_rate': 0.0007610750229559157, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8782818071758985}. Best is trial 15 with value: 0.025825289889208732.[0m
[32m[I 2025-01-06 00:09:23,546][0m Trial 24 finished with value: 0.01829608529806137 and parameters: {'observation_period_num': 6, 'train_rates': 0.9856472836766001, 'learning_rate': 0.0006805468475422376, 'batch_size': 103, 'step_size': 12, 'gamma': 0.879742445948611}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:11:06,941][0m Trial 25 finished with value: 0.03999061977899656 and parameters: {'observation_period_num': 32, 'train_rates': 0.9579808106620916, 'learning_rate': 0.0006441659356048146, 'batch_size': 102, 'step_size': 13, 'gamma': 0.903946307272704}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:12:14,537][0m Trial 26 finished with value: 0.05399074761805182 and parameters: {'observation_period_num': 84, 'train_rates': 0.8635209387565598, 'learning_rate': 0.0006678832031975252, 'batch_size': 191, 'step_size': 12, 'gamma': 0.8688099382101002}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:13:58,534][0m Trial 27 finished with value: 0.053983598947525024 and parameters: {'observation_period_num': 25, 'train_rates': 0.9878390700432731, 'learning_rate': 0.0009624666244472284, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8794769773456032}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:15:22,237][0m Trial 28 finished with value: 0.06067144125699997 and parameters: {'observation_period_num': 59, 'train_rates': 0.984012440720381, 'learning_rate': 0.0002714149716528434, 'batch_size': 132, 'step_size': 12, 'gamma': 0.8362739878335795}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:16:31,877][0m Trial 29 finished with value: 0.03428260131623741 and parameters: {'observation_period_num': 24, 'train_rates': 0.8153418269372583, 'learning_rate': 0.0005356274471002334, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7901326819942037}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:18:28,377][0m Trial 30 finished with value: 0.029940113129909397 and parameters: {'observation_period_num': 19, 'train_rates': 0.8389264591195078, 'learning_rate': 0.00010313178314640909, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9105392255831144}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:20:42,850][0m Trial 31 finished with value: 0.02329017296053317 and parameters: {'observation_period_num': 6, 'train_rates': 0.8339124890728966, 'learning_rate': 0.00011541363083223461, 'batch_size': 57, 'step_size': 10, 'gamma': 0.9127701141007605}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:23:20,371][0m Trial 32 finished with value: 0.030510336260222114 and parameters: {'observation_period_num': 5, 'train_rates': 0.8476943374634732, 'learning_rate': 0.0005853295298968589, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8785286175978269}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:24:58,353][0m Trial 33 finished with value: 0.042379711632888145 and parameters: {'observation_period_num': 28, 'train_rates': 0.7830826217491026, 'learning_rate': 3.8277840003681406e-05, 'batch_size': 81, 'step_size': 14, 'gamma': 0.8917543491124702}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:26:38,566][0m Trial 34 finished with value: 0.09066101482936315 and parameters: {'observation_period_num': 77, 'train_rates': 0.9416744969304631, 'learning_rate': 0.0002863521413402526, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9283606581406882}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:28:30,568][0m Trial 35 finished with value: 0.20398082882165908 and parameters: {'observation_period_num': 96, 'train_rates': 0.8881893500026662, 'learning_rate': 7.134896697752122e-06, 'batch_size': 68, 'step_size': 10, 'gamma': 0.8720569186147153}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:30:52,782][0m Trial 36 finished with value: 0.056234404723220895 and parameters: {'observation_period_num': 45, 'train_rates': 0.8161793491714187, 'learning_rate': 0.0005060558554010098, 'batch_size': 42, 'step_size': 7, 'gamma': 0.9571479074009269}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:32:04,984][0m Trial 37 finished with value: 0.026723732334644205 and parameters: {'observation_period_num': 6, 'train_rates': 0.7817971156359961, 'learning_rate': 0.0007228521251225422, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9216901978865545}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:33:25,992][0m Trial 38 finished with value: 0.0695049107961052 and parameters: {'observation_period_num': 61, 'train_rates': 0.7209913675402593, 'learning_rate': 3.751807899576263e-05, 'batch_size': 84, 'step_size': 14, 'gamma': 0.9300114607760308}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:34:58,498][0m Trial 39 finished with value: 0.785821629660671 and parameters: {'observation_period_num': 21, 'train_rates': 0.7688824230247351, 'learning_rate': 2.55289982174233e-06, 'batch_size': 110, 'step_size': 1, 'gamma': 0.9162006774399688}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:36:15,309][0m Trial 40 finished with value: 0.16472265520897755 and parameters: {'observation_period_num': 247, 'train_rates': 0.6736483676522491, 'learning_rate': 6.568025119255308e-05, 'batch_size': 124, 'step_size': 8, 'gamma': 0.9526482724570449}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:37:34,781][0m Trial 41 finished with value: 0.026199169203291142 and parameters: {'observation_period_num': 6, 'train_rates': 0.8257624786725438, 'learning_rate': 0.0007307853523185758, 'batch_size': 150, 'step_size': 12, 'gamma': 0.898430103388217}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:38:56,799][0m Trial 42 finished with value: 0.034019838305791535 and parameters: {'observation_period_num': 19, 'train_rates': 0.7837337400224351, 'learning_rate': 0.0004451532956608637, 'batch_size': 137, 'step_size': 13, 'gamma': 0.9110628053263142}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:40:31,664][0m Trial 43 finished with value: 0.029279030365587183 and parameters: {'observation_period_num': 5, 'train_rates': 0.8210108746292568, 'learning_rate': 0.0007317505167944461, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8986983427245884}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:42:00,526][0m Trial 44 finished with value: 0.04907156977500708 and parameters: {'observation_period_num': 31, 'train_rates': 0.7168223508666433, 'learning_rate': 0.00013867244916209223, 'batch_size': 104, 'step_size': 9, 'gamma': 0.9249792029371631}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:43:46,900][0m Trial 45 finished with value: 0.06426605374598113 and parameters: {'observation_period_num': 50, 'train_rates': 0.7477215094046709, 'learning_rate': 0.000295821900245321, 'batch_size': 76, 'step_size': 12, 'gamma': 0.8420995701549676}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:45:04,887][0m Trial 46 finished with value: 0.030640194407936666 and parameters: {'observation_period_num': 18, 'train_rates': 0.799719078972005, 'learning_rate': 0.00048423455901107777, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9380154641028412}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:49:00,606][0m Trial 47 finished with value: 0.04630199674589523 and parameters: {'observation_period_num': 38, 'train_rates': 0.7713109311233342, 'learning_rate': 0.0007729587972049631, 'batch_size': 26, 'step_size': 13, 'gamma': 0.8630412268644055}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:50:41,644][0m Trial 48 finished with value: 0.6196586489677429 and parameters: {'observation_period_num': 175, 'train_rates': 0.9689164853689073, 'learning_rate': 1.0404547062698605e-06, 'batch_size': 94, 'step_size': 11, 'gamma': 0.9898966236376979}. Best is trial 24 with value: 0.01829608529806137.[0m
[32m[I 2025-01-06 00:52:16,650][0m Trial 49 finished with value: 0.0313434924922928 and parameters: {'observation_period_num': 14, 'train_rates': 0.8287511629647629, 'learning_rate': 0.00021146840261045079, 'batch_size': 118, 'step_size': 8, 'gamma': 0.814406642342978}. Best is trial 24 with value: 0.01829608529806137.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 00:52:16,660][0m A new study created in memory with name: no-name-fbd36445-1938-4470-a9bd-af7bc9f4d486[0m
[32m[I 2025-01-06 00:53:29,420][0m Trial 0 finished with value: 0.13123888062236885 and parameters: {'observation_period_num': 50, 'train_rates': 0.7730705814820129, 'learning_rate': 0.00016979811343851168, 'batch_size': 222, 'step_size': 3, 'gamma': 0.7574026300715941}. Best is trial 0 with value: 0.13123888062236885.[0m
[32m[I 2025-01-06 00:56:48,539][0m Trial 1 finished with value: 0.15623560959462962 and parameters: {'observation_period_num': 224, 'train_rates': 0.8673413287426536, 'learning_rate': 0.0006525398372724647, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8731756432173502}. Best is trial 0 with value: 0.13123888062236885.[0m
[32m[I 2025-01-06 00:58:15,078][0m Trial 2 finished with value: 0.05861418170844043 and parameters: {'observation_period_num': 40, 'train_rates': 0.7543739042319261, 'learning_rate': 0.0003134849661996693, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9543021066006409}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 00:59:26,557][0m Trial 3 finished with value: 0.08356392346322536 and parameters: {'observation_period_num': 103, 'train_rates': 0.8525301536509872, 'learning_rate': 0.00041849832780220737, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8096428461834896}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:00:50,310][0m Trial 4 finished with value: 0.14035778134800983 and parameters: {'observation_period_num': 147, 'train_rates': 0.8367980616751638, 'learning_rate': 0.0001023850128631464, 'batch_size': 226, 'step_size': 12, 'gamma': 0.923340490443195}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:02:03,281][0m Trial 5 finished with value: 0.24682114936664273 and parameters: {'observation_period_num': 121, 'train_rates': 0.7293941050882891, 'learning_rate': 0.00020974158324177155, 'batch_size': 174, 'step_size': 12, 'gamma': 0.9192986985890511}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:04:03,539][0m Trial 6 finished with value: 0.46339315098264944 and parameters: {'observation_period_num': 113, 'train_rates': 0.7569783486178411, 'learning_rate': 2.567234910420351e-06, 'batch_size': 57, 'step_size': 2, 'gamma': 0.977709433110989}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:05:24,619][0m Trial 7 finished with value: 0.8322934508323669 and parameters: {'observation_period_num': 13, 'train_rates': 0.9793501850327102, 'learning_rate': 1.4278159762010524e-06, 'batch_size': 126, 'step_size': 15, 'gamma': 0.8654559987354823}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:07:31,400][0m Trial 8 finished with value: 0.39675147286037993 and parameters: {'observation_period_num': 10, 'train_rates': 0.8441620281228461, 'learning_rate': 1.0136113203575456e-06, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8119223220509765}. Best is trial 2 with value: 0.05861418170844043.[0m
Early stopping at epoch 75
[32m[I 2025-01-06 01:08:38,285][0m Trial 9 finished with value: 1.407212412848099 and parameters: {'observation_period_num': 147, 'train_rates': 0.7316976368114784, 'learning_rate': 3.5015480452799198e-06, 'batch_size': 184, 'step_size': 1, 'gamma': 0.860790455928796}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:09:51,828][0m Trial 10 finished with value: 0.4590051372756632 and parameters: {'observation_period_num': 58, 'train_rates': 0.6159235223896408, 'learning_rate': 2.7250455869784472e-05, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9795851459587945}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:11:01,702][0m Trial 11 finished with value: 0.11058744043111801 and parameters: {'observation_period_num': 74, 'train_rates': 0.9455612532653862, 'learning_rate': 0.0007882380347950495, 'batch_size': 240, 'step_size': 9, 'gamma': 0.7998037270383478}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:12:18,049][0m Trial 12 finished with value: 0.35464585999166387 and parameters: {'observation_period_num': 196, 'train_rates': 0.6705515735874366, 'learning_rate': 2.8286517736045404e-05, 'batch_size': 182, 'step_size': 11, 'gamma': 0.8117225851131367}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:13:29,387][0m Trial 13 finished with value: 0.10914196086385321 and parameters: {'observation_period_num': 93, 'train_rates': 0.8948464404945831, 'learning_rate': 7.132712645719568e-05, 'batch_size': 251, 'step_size': 6, 'gamma': 0.9245993644898962}. Best is trial 2 with value: 0.05861418170844043.[0m
[32m[I 2025-01-06 01:14:52,864][0m Trial 14 finished with value: 0.046524121063676746 and parameters: {'observation_period_num': 43, 'train_rates': 0.8111329321072915, 'learning_rate': 0.0003622611662046325, 'batch_size': 201, 'step_size': 7, 'gamma': 0.7730293927934256}. Best is trial 14 with value: 0.046524121063676746.[0m
[32m[I 2025-01-06 01:16:07,234][0m Trial 15 finished with value: 0.37668410243518347 and parameters: {'observation_period_num': 40, 'train_rates': 0.679310969046074, 'learning_rate': 1.0480320834699199e-05, 'batch_size': 156, 'step_size': 5, 'gamma': 0.7560112784912818}. Best is trial 14 with value: 0.046524121063676746.[0m
[32m[I 2025-01-06 01:17:14,245][0m Trial 16 finished with value: 0.04030969300995702 and parameters: {'observation_period_num': 33, 'train_rates': 0.8030937551711059, 'learning_rate': 0.000291556029292591, 'batch_size': 203, 'step_size': 7, 'gamma': 0.9015367330901679}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:18:46,480][0m Trial 17 finished with value: 0.053060397414422075 and parameters: {'observation_period_num': 10, 'train_rates': 0.8059829660269412, 'learning_rate': 5.4248704494251476e-05, 'batch_size': 149, 'step_size': 7, 'gamma': 0.8933425258519307}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:20:11,177][0m Trial 18 finished with value: 0.38458233062676556 and parameters: {'observation_period_num': 166, 'train_rates': 0.9090749834145938, 'learning_rate': 1.0230928765706064e-05, 'batch_size': 98, 'step_size': 5, 'gamma': 0.8437776943190436}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:21:33,290][0m Trial 19 finished with value: 0.13750135588763965 and parameters: {'observation_period_num': 80, 'train_rates': 0.8243963155775393, 'learning_rate': 0.00014276564963133608, 'batch_size': 205, 'step_size': 4, 'gamma': 0.7778900890733321}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:22:29,467][0m Trial 20 finished with value: 0.06315358181248655 and parameters: {'observation_period_num': 33, 'train_rates': 0.68825172718173, 'learning_rate': 0.0009634104617362858, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8374404519444604}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:23:51,376][0m Trial 21 finished with value: 0.05997442517240169 and parameters: {'observation_period_num': 15, 'train_rates': 0.7992025129057685, 'learning_rate': 5.341556445326968e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.897927463310838}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:25:17,933][0m Trial 22 finished with value: 0.045493754900020104 and parameters: {'observation_period_num': 58, 'train_rates': 0.8013928995661845, 'learning_rate': 0.0003644926230358233, 'batch_size': 146, 'step_size': 7, 'gamma': 0.8861919619281801}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:26:30,076][0m Trial 23 finished with value: 0.06821774933431905 and parameters: {'observation_period_num': 66, 'train_rates': 0.8833140552087896, 'learning_rate': 0.000341080119208313, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8927883931645412}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:27:50,929][0m Trial 24 finished with value: 0.09801625741187052 and parameters: {'observation_period_num': 85, 'train_rates': 0.7919167855686232, 'learning_rate': 0.0005083525226705679, 'batch_size': 140, 'step_size': 7, 'gamma': 0.946952135629133}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:29:30,925][0m Trial 25 finished with value: 0.04038823227110867 and parameters: {'observation_period_num': 28, 'train_rates': 0.9270592452040758, 'learning_rate': 0.000320146662965674, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8360312792033173}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:31:08,518][0m Trial 26 finished with value: 0.04353328795832956 and parameters: {'observation_period_num': 30, 'train_rates': 0.9199134377520535, 'learning_rate': 0.0002256929099439588, 'batch_size': 107, 'step_size': 5, 'gamma': 0.8403287991043367}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:32:52,332][0m Trial 27 finished with value: 0.04369720272146738 and parameters: {'observation_period_num': 25, 'train_rates': 0.9289251632098368, 'learning_rate': 0.00020883027990264213, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8456294594999635}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:34:18,576][0m Trial 28 finished with value: 0.17150487106735424 and parameters: {'observation_period_num': 250, 'train_rates': 0.9565276838447553, 'learning_rate': 9.627959062481904e-05, 'batch_size': 110, 'step_size': 5, 'gamma': 0.838106172751812}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:36:04,467][0m Trial 29 finished with value: 0.04063870757818222 and parameters: {'observation_period_num': 27, 'train_rates': 0.9869002666568158, 'learning_rate': 0.00016145087527542977, 'batch_size': 81, 'step_size': 4, 'gamma': 0.8269100192001222}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:37:53,214][0m Trial 30 finished with value: 0.05679354816675186 and parameters: {'observation_period_num': 50, 'train_rates': 0.9779125330698195, 'learning_rate': 0.00013976813337610283, 'batch_size': 71, 'step_size': 3, 'gamma': 0.8238979910477943}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:39:28,334][0m Trial 31 finished with value: 0.054851845790480455 and parameters: {'observation_period_num': 28, 'train_rates': 0.9266774611178032, 'learning_rate': 0.00023333397900418987, 'batch_size': 78, 'step_size': 3, 'gamma': 0.7866815992318213}. Best is trial 16 with value: 0.04030969300995702.[0m
[32m[I 2025-01-06 01:43:01,246][0m Trial 32 finished with value: 0.025187363848090172 and parameters: {'observation_period_num': 27, 'train_rates': 0.9889334090181356, 'learning_rate': 0.0006735979974378459, 'batch_size': 33, 'step_size': 4, 'gamma': 0.857387435944638}. Best is trial 32 with value: 0.025187363848090172.[0m
[32m[I 2025-01-06 01:47:00,868][0m Trial 33 finished with value: 0.028561893083593425 and parameters: {'observation_period_num': 50, 'train_rates': 0.9885030911924537, 'learning_rate': 0.0006173938636686451, 'batch_size': 31, 'step_size': 2, 'gamma': 0.8735489584875297}. Best is trial 32 with value: 0.025187363848090172.[0m
[32m[I 2025-01-06 01:50:36,741][0m Trial 34 finished with value: 0.06203755841783758 and parameters: {'observation_period_num': 52, 'train_rates': 0.9634754252278682, 'learning_rate': 0.0006070716636674063, 'batch_size': 32, 'step_size': 1, 'gamma': 0.8722512212739674}. Best is trial 32 with value: 0.025187363848090172.[0m
[32m[I 2025-01-06 01:55:57,364][0m Trial 35 finished with value: 0.02343457532641234 and parameters: {'observation_period_num': 5, 'train_rates': 0.8768792230859861, 'learning_rate': 0.000607045115825463, 'batch_size': 20, 'step_size': 2, 'gamma': 0.8573671033404323}. Best is trial 35 with value: 0.02343457532641234.[0m
[32m[I 2025-01-06 02:00:53,525][0m Trial 36 finished with value: 0.025161361806500672 and parameters: {'observation_period_num': 7, 'train_rates': 0.875508519025721, 'learning_rate': 0.0005496295148677986, 'batch_size': 21, 'step_size': 2, 'gamma': 0.8591484308793704}. Best is trial 35 with value: 0.02343457532641234.[0m
[32m[I 2025-01-06 02:05:25,523][0m Trial 37 finished with value: 0.02366676933475231 and parameters: {'observation_period_num': 8, 'train_rates': 0.8710836028208707, 'learning_rate': 0.0006351441906487686, 'batch_size': 22, 'step_size': 2, 'gamma': 0.8522360212938324}. Best is trial 35 with value: 0.02343457532641234.[0m
[32m[I 2025-01-06 02:09:39,313][0m Trial 38 finished with value: 0.022560735812617673 and parameters: {'observation_period_num': 5, 'train_rates': 0.8627613199218886, 'learning_rate': 0.0009171296532336576, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8565352682392651}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:15:42,976][0m Trial 39 finished with value: 0.02503144481989938 and parameters: {'observation_period_num': 7, 'train_rates': 0.8664516610940647, 'learning_rate': 0.0004766461610357546, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8541025062387843}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:18:01,959][0m Trial 40 finished with value: 0.034525186820349225 and parameters: {'observation_period_num': 5, 'train_rates': 0.8556402251112439, 'learning_rate': 0.0009921896648936192, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8783674029115575}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:23:14,167][0m Trial 41 finished with value: 0.026468451496773432 and parameters: {'observation_period_num': 7, 'train_rates': 0.8690540568160118, 'learning_rate': 0.00046939880141955365, 'batch_size': 18, 'step_size': 2, 'gamma': 0.8568435528390355}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:29:11,021][0m Trial 42 finished with value: 0.03282304629276359 and parameters: {'observation_period_num': 18, 'train_rates': 0.8754748734373714, 'learning_rate': 0.0004778281620251493, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8535599601627514}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:31:44,378][0m Trial 43 finished with value: 0.02407103996377001 and parameters: {'observation_period_num': 5, 'train_rates': 0.8325819417473282, 'learning_rate': 0.0007879513550715371, 'batch_size': 44, 'step_size': 2, 'gamma': 0.911187592568351}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:34:07,840][0m Trial 44 finished with value: 0.042912476998451465 and parameters: {'observation_period_num': 18, 'train_rates': 0.8426739952373704, 'learning_rate': 0.0008090342158017324, 'batch_size': 47, 'step_size': 3, 'gamma': 0.9283175129109572}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:36:10,507][0m Trial 45 finished with value: 0.034707928645681556 and parameters: {'observation_period_num': 18, 'train_rates': 0.7702999429716975, 'learning_rate': 0.0009573856236209259, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9094989282920004}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:37:40,403][0m Trial 46 finished with value: 0.10065495896283999 and parameters: {'observation_period_num': 134, 'train_rates': 0.828381568670327, 'learning_rate': 0.0007181599134341381, 'batch_size': 61, 'step_size': 3, 'gamma': 0.88569195452276}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:40:18,867][0m Trial 47 finished with value: 0.04380706227430613 and parameters: {'observation_period_num': 40, 'train_rates': 0.8960981149835826, 'learning_rate': 0.0004284972026297531, 'batch_size': 39, 'step_size': 2, 'gamma': 0.9364351984923638}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:44:02,145][0m Trial 48 finished with value: 0.1045436275596837 and parameters: {'observation_period_num': 188, 'train_rates': 0.8577718514835546, 'learning_rate': 0.00027500703743796654, 'batch_size': 25, 'step_size': 1, 'gamma': 0.9091246657073894}. Best is trial 38 with value: 0.022560735812617673.[0m
[32m[I 2025-01-06 02:45:53,855][0m Trial 49 finished with value: 0.06269184228980948 and parameters: {'observation_period_num': 68, 'train_rates': 0.8271101457247018, 'learning_rate': 0.000714513616364863, 'batch_size': 58, 'step_size': 3, 'gamma': 0.8189193132673677}. Best is trial 38 with value: 0.022560735812617673.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 18, 'train_rates': 0.7340178460381248, 'learning_rate': 0.00015232656498093876, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9720083966273434}
Epoch 1/300, trend Loss: 0.2859 | 0.2933
Epoch 2/300, trend Loss: 0.1773 | 0.2157
Epoch 3/300, trend Loss: 0.1962 | 0.2397
Epoch 4/300, trend Loss: 0.1493 | 0.1631
Epoch 5/300, trend Loss: 0.1421 | 0.1603
Epoch 6/300, trend Loss: 0.1285 | 0.1245
Epoch 7/300, trend Loss: 0.1242 | 0.1170
Epoch 8/300, trend Loss: 0.1178 | 0.0962
Epoch 9/300, trend Loss: 0.1174 | 0.0942
Epoch 10/300, trend Loss: 0.1124 | 0.0858
Epoch 11/300, trend Loss: 0.1106 | 0.0897
Epoch 12/300, trend Loss: 0.1073 | 0.0865
Epoch 13/300, trend Loss: 0.1046 | 0.0953
Epoch 14/300, trend Loss: 0.1041 | 0.0985
Epoch 15/300, trend Loss: 0.1021 | 0.1091
Epoch 16/300, trend Loss: 0.1017 | 0.1209
Epoch 17/300, trend Loss: 0.1028 | 0.1244
Epoch 18/300, trend Loss: 0.1025 | 0.1373
Epoch 19/300, trend Loss: 0.1054 | 0.1239
Epoch 20/300, trend Loss: 0.1040 | 0.1150
Epoch 21/300, trend Loss: 0.1002 | 0.1057
Epoch 22/300, trend Loss: 0.0956 | 0.1034
Epoch 23/300, trend Loss: 0.0947 | 0.1027
Epoch 24/300, trend Loss: 0.0935 | 0.0916
Epoch 25/300, trend Loss: 0.0921 | 0.0828
Epoch 26/300, trend Loss: 0.0909 | 0.0732
Epoch 27/300, trend Loss: 0.0899 | 0.0660
Epoch 28/300, trend Loss: 0.0891 | 0.0614
Epoch 29/300, trend Loss: 0.0883 | 0.0599
Epoch 30/300, trend Loss: 0.0873 | 0.0637
Epoch 31/300, trend Loss: 0.0864 | 0.0750
Epoch 32/300, trend Loss: 0.0864 | 0.0798
Epoch 33/300, trend Loss: 0.0877 | 0.0708
Epoch 34/300, trend Loss: 0.0895 | 0.0613
Epoch 35/300, trend Loss: 0.0891 | 0.0581
Epoch 36/300, trend Loss: 0.0865 | 0.0558
Epoch 37/300, trend Loss: 0.0842 | 0.0581
Epoch 38/300, trend Loss: 0.0831 | 0.0564
Epoch 39/300, trend Loss: 0.0823 | 0.0522
Epoch 40/300, trend Loss: 0.0819 | 0.0500
Epoch 41/300, trend Loss: 0.0812 | 0.0483
Epoch 42/300, trend Loss: 0.0807 | 0.0482
Epoch 43/300, trend Loss: 0.0801 | 0.0473
Epoch 44/300, trend Loss: 0.0797 | 0.0481
Epoch 45/300, trend Loss: 0.0792 | 0.0481
Epoch 46/300, trend Loss: 0.0788 | 0.0509
Epoch 47/300, trend Loss: 0.0786 | 0.0496
Epoch 48/300, trend Loss: 0.0783 | 0.0493
Epoch 49/300, trend Loss: 0.0781 | 0.0450
Epoch 50/300, trend Loss: 0.0776 | 0.0434
Epoch 51/300, trend Loss: 0.0776 | 0.0454
Epoch 52/300, trend Loss: 0.0797 | 0.0768
Epoch 53/300, trend Loss: 0.0910 | 0.0543
Epoch 54/300, trend Loss: 0.0844 | 0.1374
Epoch 55/300, trend Loss: 0.0897 | 0.0521
Epoch 56/300, trend Loss: 0.0826 | 0.1148
Epoch 57/300, trend Loss: 0.0892 | 0.0543
Epoch 58/300, trend Loss: 0.0783 | 0.0617
Epoch 59/300, trend Loss: 0.0775 | 0.0474
Epoch 60/300, trend Loss: 0.0758 | 0.0729
Epoch 61/300, trend Loss: 0.0761 | 0.0499
Epoch 62/300, trend Loss: 0.0754 | 0.0665
Epoch 63/300, trend Loss: 0.0750 | 0.0475
Epoch 64/300, trend Loss: 0.0745 | 0.0582
Epoch 65/300, trend Loss: 0.0740 | 0.0458
Epoch 66/300, trend Loss: 0.0736 | 0.0516
Epoch 67/300, trend Loss: 0.0731 | 0.0445
Epoch 68/300, trend Loss: 0.0728 | 0.0480
Epoch 69/300, trend Loss: 0.0725 | 0.0439
Epoch 70/300, trend Loss: 0.0722 | 0.0461
Epoch 71/300, trend Loss: 0.0720 | 0.0433
Epoch 72/300, trend Loss: 0.0718 | 0.0445
Epoch 73/300, trend Loss: 0.0716 | 0.0428
Epoch 74/300, trend Loss: 0.0714 | 0.0431
Epoch 75/300, trend Loss: 0.0711 | 0.0421
Epoch 76/300, trend Loss: 0.0709 | 0.0419
Epoch 77/300, trend Loss: 0.0707 | 0.0414
Epoch 78/300, trend Loss: 0.0705 | 0.0409
Epoch 79/300, trend Loss: 0.0703 | 0.0406
Epoch 80/300, trend Loss: 0.0702 | 0.0400
Epoch 81/300, trend Loss: 0.0700 | 0.0397
Epoch 82/300, trend Loss: 0.0698 | 0.0390
Epoch 83/300, trend Loss: 0.0697 | 0.0387
Epoch 84/300, trend Loss: 0.0695 | 0.0382
Epoch 85/300, trend Loss: 0.0694 | 0.0379
Epoch 86/300, trend Loss: 0.0693 | 0.0374
Epoch 87/300, trend Loss: 0.0692 | 0.0372
Epoch 88/300, trend Loss: 0.0691 | 0.0370
Epoch 89/300, trend Loss: 0.0691 | 0.0369
Epoch 90/300, trend Loss: 0.0692 | 0.0370
Epoch 91/300, trend Loss: 0.0692 | 0.0369
Epoch 92/300, trend Loss: 0.0693 | 0.0369
Epoch 93/300, trend Loss: 0.0691 | 0.0366
Epoch 94/300, trend Loss: 0.0690 | 0.0364
Epoch 95/300, trend Loss: 0.0686 | 0.0361
Epoch 96/300, trend Loss: 0.0683 | 0.0360
Epoch 97/300, trend Loss: 0.0679 | 0.0360
Epoch 98/300, trend Loss: 0.0678 | 0.0358
Epoch 99/300, trend Loss: 0.0678 | 0.0358
Epoch 100/300, trend Loss: 0.0679 | 0.0355
Epoch 101/300, trend Loss: 0.0678 | 0.0353
Epoch 102/300, trend Loss: 0.0676 | 0.0350
Epoch 103/300, trend Loss: 0.0672 | 0.0348
Epoch 104/300, trend Loss: 0.0667 | 0.0346
Epoch 105/300, trend Loss: 0.0664 | 0.0345
Epoch 106/300, trend Loss: 0.0662 | 0.0346
Epoch 107/300, trend Loss: 0.0662 | 0.0346
Epoch 108/300, trend Loss: 0.0661 | 0.0346
Epoch 109/300, trend Loss: 0.0661 | 0.0346
Epoch 110/300, trend Loss: 0.0660 | 0.0347
Epoch 111/300, trend Loss: 0.0659 | 0.0347
Epoch 112/300, trend Loss: 0.0657 | 0.0347
Epoch 113/300, trend Loss: 0.0656 | 0.0347
Epoch 114/300, trend Loss: 0.0655 | 0.0347
Epoch 115/300, trend Loss: 0.0654 | 0.0347
Epoch 116/300, trend Loss: 0.0653 | 0.0347
Epoch 117/300, trend Loss: 0.0653 | 0.0347
Epoch 118/300, trend Loss: 0.0653 | 0.0348
Epoch 119/300, trend Loss: 0.0652 | 0.0348
Epoch 120/300, trend Loss: 0.0652 | 0.0349
Epoch 121/300, trend Loss: 0.0652 | 0.0350
Epoch 122/300, trend Loss: 0.0652 | 0.0352
Epoch 123/300, trend Loss: 0.0650 | 0.0352
Epoch 124/300, trend Loss: 0.0648 | 0.0352
Epoch 125/300, trend Loss: 0.0645 | 0.0352
Epoch 126/300, trend Loss: 0.0643 | 0.0350
Epoch 127/300, trend Loss: 0.0642 | 0.0348
Epoch 128/300, trend Loss: 0.0641 | 0.0345
Epoch 129/300, trend Loss: 0.0641 | 0.0343
Epoch 130/300, trend Loss: 0.0640 | 0.0340
Epoch 131/300, trend Loss: 0.0639 | 0.0338
Epoch 132/300, trend Loss: 0.0638 | 0.0336
Epoch 133/300, trend Loss: 0.0637 | 0.0334
Epoch 134/300, trend Loss: 0.0635 | 0.0333
Epoch 135/300, trend Loss: 0.0634 | 0.0333
Epoch 136/300, trend Loss: 0.0633 | 0.0333
Epoch 137/300, trend Loss: 0.0632 | 0.0333
Epoch 138/300, trend Loss: 0.0631 | 0.0333
Epoch 139/300, trend Loss: 0.0630 | 0.0332
Epoch 140/300, trend Loss: 0.0630 | 0.0332
Epoch 141/300, trend Loss: 0.0629 | 0.0332
Epoch 142/300, trend Loss: 0.0629 | 0.0332
Epoch 143/300, trend Loss: 0.0628 | 0.0331
Epoch 144/300, trend Loss: 0.0627 | 0.0331
Epoch 145/300, trend Loss: 0.0627 | 0.0331
Epoch 146/300, trend Loss: 0.0626 | 0.0330
Epoch 147/300, trend Loss: 0.0625 | 0.0330
Epoch 148/300, trend Loss: 0.0625 | 0.0329
Epoch 149/300, trend Loss: 0.0624 | 0.0329
Epoch 150/300, trend Loss: 0.0623 | 0.0329
Epoch 151/300, trend Loss: 0.0623 | 0.0328
Epoch 152/300, trend Loss: 0.0622 | 0.0328
Epoch 153/300, trend Loss: 0.0621 | 0.0328
Epoch 154/300, trend Loss: 0.0621 | 0.0327
Epoch 155/300, trend Loss: 0.0620 | 0.0327
Epoch 156/300, trend Loss: 0.0620 | 0.0327
Epoch 157/300, trend Loss: 0.0619 | 0.0327
Epoch 158/300, trend Loss: 0.0619 | 0.0327
Epoch 159/300, trend Loss: 0.0618 | 0.0326
Epoch 160/300, trend Loss: 0.0618 | 0.0326
Epoch 161/300, trend Loss: 0.0617 | 0.0326
Epoch 162/300, trend Loss: 0.0617 | 0.0326
Epoch 163/300, trend Loss: 0.0616 | 0.0326
Epoch 164/300, trend Loss: 0.0616 | 0.0325
Epoch 165/300, trend Loss: 0.0615 | 0.0325
Epoch 166/300, trend Loss: 0.0615 | 0.0325
Epoch 167/300, trend Loss: 0.0614 | 0.0325
Epoch 168/300, trend Loss: 0.0614 | 0.0325
Epoch 169/300, trend Loss: 0.0613 | 0.0324
Epoch 170/300, trend Loss: 0.0613 | 0.0324
Epoch 171/300, trend Loss: 0.0612 | 0.0324
Epoch 172/300, trend Loss: 0.0612 | 0.0324
Epoch 173/300, trend Loss: 0.0611 | 0.0324
Epoch 174/300, trend Loss: 0.0611 | 0.0324
Epoch 175/300, trend Loss: 0.0610 | 0.0324
Epoch 176/300, trend Loss: 0.0610 | 0.0324
Epoch 177/300, trend Loss: 0.0609 | 0.0324
Epoch 178/300, trend Loss: 0.0609 | 0.0323
Epoch 179/300, trend Loss: 0.0609 | 0.0323
Epoch 180/300, trend Loss: 0.0608 | 0.0323
Epoch 181/300, trend Loss: 0.0608 | 0.0323
Epoch 182/300, trend Loss: 0.0608 | 0.0323
Epoch 183/300, trend Loss: 0.0607 | 0.0323
Epoch 184/300, trend Loss: 0.0607 | 0.0323
Epoch 185/300, trend Loss: 0.0606 | 0.0323
Epoch 186/300, trend Loss: 0.0606 | 0.0323
Epoch 187/300, trend Loss: 0.0606 | 0.0323
Epoch 188/300, trend Loss: 0.0605 | 0.0323
Epoch 189/300, trend Loss: 0.0605 | 0.0323
Epoch 190/300, trend Loss: 0.0604 | 0.0323
Epoch 191/300, trend Loss: 0.0604 | 0.0323
Epoch 192/300, trend Loss: 0.0604 | 0.0323
Epoch 193/300, trend Loss: 0.0603 | 0.0323
Epoch 194/300, trend Loss: 0.0603 | 0.0323
Epoch 195/300, trend Loss: 0.0603 | 0.0323
Epoch 196/300, trend Loss: 0.0602 | 0.0323
Epoch 197/300, trend Loss: 0.0602 | 0.0323
Epoch 198/300, trend Loss: 0.0602 | 0.0323
Epoch 199/300, trend Loss: 0.0601 | 0.0323
Epoch 200/300, trend Loss: 0.0601 | 0.0323
Epoch 201/300, trend Loss: 0.0601 | 0.0323
Epoch 202/300, trend Loss: 0.0601 | 0.0323
Epoch 203/300, trend Loss: 0.0600 | 0.0323
Epoch 204/300, trend Loss: 0.0600 | 0.0323
Epoch 205/300, trend Loss: 0.0600 | 0.0323
Epoch 206/300, trend Loss: 0.0599 | 0.0323
Epoch 207/300, trend Loss: 0.0599 | 0.0323
Epoch 208/300, trend Loss: 0.0599 | 0.0323
Epoch 209/300, trend Loss: 0.0599 | 0.0324
Epoch 210/300, trend Loss: 0.0598 | 0.0324
Epoch 211/300, trend Loss: 0.0598 | 0.0324
Epoch 212/300, trend Loss: 0.0598 | 0.0324
Epoch 213/300, trend Loss: 0.0598 | 0.0324
Epoch 214/300, trend Loss: 0.0597 | 0.0324
Epoch 215/300, trend Loss: 0.0597 | 0.0324
Epoch 216/300, trend Loss: 0.0597 | 0.0325
Epoch 217/300, trend Loss: 0.0597 | 0.0325
Epoch 218/300, trend Loss: 0.0597 | 0.0325
Epoch 219/300, trend Loss: 0.0596 | 0.0325
Epoch 220/300, trend Loss: 0.0596 | 0.0325
Epoch 221/300, trend Loss: 0.0596 | 0.0325
Epoch 222/300, trend Loss: 0.0596 | 0.0325
Epoch 223/300, trend Loss: 0.0595 | 0.0325
Epoch 224/300, trend Loss: 0.0595 | 0.0325
Epoch 225/300, trend Loss: 0.0595 | 0.0324
Epoch 226/300, trend Loss: 0.0595 | 0.0324
Epoch 227/300, trend Loss: 0.0595 | 0.0324
Epoch 228/300, trend Loss: 0.0594 | 0.0324
Epoch 229/300, trend Loss: 0.0594 | 0.0324
Epoch 230/300, trend Loss: 0.0594 | 0.0324
Epoch 231/300, trend Loss: 0.0594 | 0.0323
Epoch 232/300, trend Loss: 0.0594 | 0.0323
Epoch 233/300, trend Loss: 0.0593 | 0.0323
Epoch 234/300, trend Loss: 0.0593 | 0.0323
Epoch 235/300, trend Loss: 0.0593 | 0.0323
Epoch 236/300, trend Loss: 0.0593 | 0.0323
Epoch 237/300, trend Loss: 0.0592 | 0.0323
Epoch 238/300, trend Loss: 0.0592 | 0.0323
Epoch 239/300, trend Loss: 0.0592 | 0.0323
Epoch 240/300, trend Loss: 0.0592 | 0.0324
Epoch 241/300, trend Loss: 0.0592 | 0.0324
Epoch 242/300, trend Loss: 0.0591 | 0.0324
Epoch 243/300, trend Loss: 0.0591 | 0.0324
Epoch 244/300, trend Loss: 0.0591 | 0.0323
Epoch 245/300, trend Loss: 0.0591 | 0.0323
Epoch 246/300, trend Loss: 0.0591 | 0.0323
Epoch 247/300, trend Loss: 0.0591 | 0.0323
Epoch 248/300, trend Loss: 0.0590 | 0.0323
Epoch 249/300, trend Loss: 0.0590 | 0.0323
Epoch 250/300, trend Loss: 0.0590 | 0.0323
Epoch 251/300, trend Loss: 0.0590 | 0.0323
Epoch 252/300, trend Loss: 0.0589 | 0.0323
Epoch 253/300, trend Loss: 0.0589 | 0.0323
Epoch 254/300, trend Loss: 0.0589 | 0.0323
Epoch 255/300, trend Loss: 0.0589 | 0.0324
Epoch 256/300, trend Loss: 0.0589 | 0.0324
Epoch 257/300, trend Loss: 0.0589 | 0.0324
Epoch 258/300, trend Loss: 0.0588 | 0.0324
Epoch 259/300, trend Loss: 0.0588 | 0.0324
Epoch 260/300, trend Loss: 0.0588 | 0.0324
Epoch 261/300, trend Loss: 0.0588 | 0.0324
Epoch 262/300, trend Loss: 0.0588 | 0.0324
Epoch 263/300, trend Loss: 0.0588 | 0.0324
Epoch 264/300, trend Loss: 0.0588 | 0.0324
Epoch 265/300, trend Loss: 0.0588 | 0.0324
Epoch 266/300, trend Loss: 0.0587 | 0.0324
Epoch 267/300, trend Loss: 0.0587 | 0.0324
Epoch 268/300, trend Loss: 0.0587 | 0.0324
Epoch 269/300, trend Loss: 0.0587 | 0.0324
Epoch 270/300, trend Loss: 0.0587 | 0.0324
Epoch 271/300, trend Loss: 0.0587 | 0.0324
Epoch 272/300, trend Loss: 0.0587 | 0.0324
Epoch 273/300, trend Loss: 0.0587 | 0.0324
Epoch 274/300, trend Loss: 0.0586 | 0.0324
Epoch 275/300, trend Loss: 0.0586 | 0.0324
Epoch 276/300, trend Loss: 0.0586 | 0.0324
Epoch 277/300, trend Loss: 0.0586 | 0.0324
Epoch 278/300, trend Loss: 0.0586 | 0.0324
Epoch 279/300, trend Loss: 0.0586 | 0.0324
Epoch 280/300, trend Loss: 0.0586 | 0.0324
Epoch 281/300, trend Loss: 0.0586 | 0.0324
Epoch 282/300, trend Loss: 0.0586 | 0.0324
Epoch 283/300, trend Loss: 0.0585 | 0.0324
Epoch 284/300, trend Loss: 0.0585 | 0.0324
Epoch 285/300, trend Loss: 0.0585 | 0.0324
Epoch 286/300, trend Loss: 0.0585 | 0.0324
Epoch 287/300, trend Loss: 0.0585 | 0.0324
Epoch 288/300, trend Loss: 0.0585 | 0.0324
Epoch 289/300, trend Loss: 0.0585 | 0.0324
Epoch 290/300, trend Loss: 0.0585 | 0.0324
Epoch 291/300, trend Loss: 0.0585 | 0.0324
Epoch 292/300, trend Loss: 0.0585 | 0.0324
Epoch 293/300, trend Loss: 0.0585 | 0.0324
Epoch 294/300, trend Loss: 0.0584 | 0.0324
Epoch 295/300, trend Loss: 0.0584 | 0.0325
Epoch 296/300, trend Loss: 0.0584 | 0.0325
Epoch 297/300, trend Loss: 0.0584 | 0.0325
Epoch 298/300, trend Loss: 0.0584 | 0.0325
Epoch 299/300, trend Loss: 0.0584 | 0.0325
Epoch 300/300, trend Loss: 0.0584 | 0.0325
Training seasonal_0 component with params: {'observation_period_num': 14, 'train_rates': 0.8704271336866358, 'learning_rate': 0.0003478936498117362, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8494384666475375}
Epoch 1/300, seasonal_0 Loss: 0.3376 | 0.2339
Epoch 2/300, seasonal_0 Loss: 0.1750 | 0.1651
Epoch 3/300, seasonal_0 Loss: 0.1803 | 0.2141
Epoch 4/300, seasonal_0 Loss: 0.1506 | 0.1783
Epoch 5/300, seasonal_0 Loss: 0.1469 | 0.1968
Epoch 6/300, seasonal_0 Loss: 0.1510 | 0.2121
Epoch 7/300, seasonal_0 Loss: 0.1381 | 0.1717
Epoch 8/300, seasonal_0 Loss: 0.1384 | 0.1078
Epoch 9/300, seasonal_0 Loss: 0.1357 | 0.0931
Epoch 10/300, seasonal_0 Loss: 0.1311 | 0.1051
Epoch 11/300, seasonal_0 Loss: 0.1247 | 0.0862
Epoch 12/300, seasonal_0 Loss: 0.1111 | 0.0777
Epoch 13/300, seasonal_0 Loss: 0.1116 | 0.0911
Epoch 14/300, seasonal_0 Loss: 0.1184 | 0.0918
Epoch 15/300, seasonal_0 Loss: 0.1098 | 0.0726
Epoch 16/300, seasonal_0 Loss: 0.1006 | 0.0706
Epoch 17/300, seasonal_0 Loss: 0.0993 | 0.0694
Epoch 18/300, seasonal_0 Loss: 0.0979 | 0.0693
Epoch 19/300, seasonal_0 Loss: 0.0977 | 0.0713
Epoch 20/300, seasonal_0 Loss: 0.1001 | 0.0769
Epoch 21/300, seasonal_0 Loss: 0.1006 | 0.0735
Epoch 22/300, seasonal_0 Loss: 0.0987 | 0.0699
Epoch 23/300, seasonal_0 Loss: 0.0959 | 0.0662
Epoch 24/300, seasonal_0 Loss: 0.0917 | 0.0653
Epoch 25/300, seasonal_0 Loss: 0.0928 | 0.0801
Epoch 26/300, seasonal_0 Loss: 0.0965 | 0.0843
Epoch 27/300, seasonal_0 Loss: 0.0945 | 0.0776
Epoch 28/300, seasonal_0 Loss: 0.0897 | 0.0739
Epoch 29/300, seasonal_0 Loss: 0.0903 | 0.0667
Epoch 30/300, seasonal_0 Loss: 0.0885 | 0.0619
Epoch 31/300, seasonal_0 Loss: 0.0873 | 0.0602
Epoch 32/300, seasonal_0 Loss: 0.0855 | 0.0596
Epoch 33/300, seasonal_0 Loss: 0.0836 | 0.0597
Epoch 34/300, seasonal_0 Loss: 0.0824 | 0.0595
Epoch 35/300, seasonal_0 Loss: 0.0822 | 0.0664
Epoch 36/300, seasonal_0 Loss: 0.0823 | 0.0657
Epoch 37/300, seasonal_0 Loss: 0.0822 | 0.0614
Epoch 38/300, seasonal_0 Loss: 0.0801 | 0.0600
Epoch 39/300, seasonal_0 Loss: 0.0798 | 0.0603
Epoch 40/300, seasonal_0 Loss: 0.0788 | 0.0592
Epoch 41/300, seasonal_0 Loss: 0.0785 | 0.0584
Epoch 42/300, seasonal_0 Loss: 0.0780 | 0.0576
Epoch 43/300, seasonal_0 Loss: 0.0781 | 0.0575
Epoch 44/300, seasonal_0 Loss: 0.0782 | 0.0574
Epoch 45/300, seasonal_0 Loss: 0.0785 | 0.0576
Epoch 46/300, seasonal_0 Loss: 0.0783 | 0.0577
Epoch 47/300, seasonal_0 Loss: 0.0778 | 0.0563
Epoch 48/300, seasonal_0 Loss: 0.0759 | 0.0564
Epoch 49/300, seasonal_0 Loss: 0.0751 | 0.0571
Epoch 50/300, seasonal_0 Loss: 0.0748 | 0.0571
Epoch 51/300, seasonal_0 Loss: 0.0743 | 0.0563
Epoch 52/300, seasonal_0 Loss: 0.0736 | 0.0554
Epoch 53/300, seasonal_0 Loss: 0.0730 | 0.0551
Epoch 54/300, seasonal_0 Loss: 0.0728 | 0.0553
Epoch 55/300, seasonal_0 Loss: 0.0729 | 0.0556
Epoch 56/300, seasonal_0 Loss: 0.0730 | 0.0557
Epoch 57/300, seasonal_0 Loss: 0.0729 | 0.0551
Epoch 58/300, seasonal_0 Loss: 0.0722 | 0.0549
Epoch 59/300, seasonal_0 Loss: 0.0718 | 0.0550
Epoch 60/300, seasonal_0 Loss: 0.0715 | 0.0550
Epoch 61/300, seasonal_0 Loss: 0.0713 | 0.0549
Epoch 62/300, seasonal_0 Loss: 0.0711 | 0.0547
Epoch 63/300, seasonal_0 Loss: 0.0709 | 0.0548
Epoch 64/300, seasonal_0 Loss: 0.0708 | 0.0550
Epoch 65/300, seasonal_0 Loss: 0.0706 | 0.0550
Epoch 66/300, seasonal_0 Loss: 0.0706 | 0.0554
Epoch 67/300, seasonal_0 Loss: 0.0706 | 0.0550
Epoch 68/300, seasonal_0 Loss: 0.0706 | 0.0548
Epoch 69/300, seasonal_0 Loss: 0.0706 | 0.0547
Epoch 70/300, seasonal_0 Loss: 0.0707 | 0.0545
Epoch 71/300, seasonal_0 Loss: 0.0710 | 0.0544
Epoch 72/300, seasonal_0 Loss: 0.0717 | 0.0546
Epoch 73/300, seasonal_0 Loss: 0.0729 | 0.0549
Epoch 74/300, seasonal_0 Loss: 0.0740 | 0.0543
Epoch 75/300, seasonal_0 Loss: 0.0745 | 0.0537
Epoch 76/300, seasonal_0 Loss: 0.0751 | 0.0541
Epoch 77/300, seasonal_0 Loss: 0.0753 | 0.0544
Epoch 78/300, seasonal_0 Loss: 0.0731 | 0.0539
Epoch 79/300, seasonal_0 Loss: 0.0716 | 0.0534
Epoch 80/300, seasonal_0 Loss: 0.0714 | 0.0542
Epoch 81/300, seasonal_0 Loss: 0.0710 | 0.0547
Epoch 82/300, seasonal_0 Loss: 0.0698 | 0.0541
Epoch 83/300, seasonal_0 Loss: 0.0685 | 0.0534
Epoch 84/300, seasonal_0 Loss: 0.0680 | 0.0529
Epoch 85/300, seasonal_0 Loss: 0.0679 | 0.0526
Epoch 86/300, seasonal_0 Loss: 0.0679 | 0.0524
Epoch 87/300, seasonal_0 Loss: 0.0679 | 0.0524
Epoch 88/300, seasonal_0 Loss: 0.0677 | 0.0525
Epoch 89/300, seasonal_0 Loss: 0.0675 | 0.0524
Epoch 90/300, seasonal_0 Loss: 0.0674 | 0.0524
Epoch 91/300, seasonal_0 Loss: 0.0673 | 0.0525
Epoch 92/300, seasonal_0 Loss: 0.0672 | 0.0524
Epoch 93/300, seasonal_0 Loss: 0.0671 | 0.0524
Epoch 94/300, seasonal_0 Loss: 0.0670 | 0.0523
Epoch 95/300, seasonal_0 Loss: 0.0669 | 0.0522
Epoch 96/300, seasonal_0 Loss: 0.0668 | 0.0522
Epoch 97/300, seasonal_0 Loss: 0.0667 | 0.0521
Epoch 98/300, seasonal_0 Loss: 0.0667 | 0.0521
Epoch 99/300, seasonal_0 Loss: 0.0666 | 0.0520
Epoch 100/300, seasonal_0 Loss: 0.0666 | 0.0520
Epoch 101/300, seasonal_0 Loss: 0.0665 | 0.0520
Epoch 102/300, seasonal_0 Loss: 0.0664 | 0.0520
Epoch 103/300, seasonal_0 Loss: 0.0664 | 0.0520
Epoch 104/300, seasonal_0 Loss: 0.0663 | 0.0519
Epoch 105/300, seasonal_0 Loss: 0.0662 | 0.0519
Epoch 106/300, seasonal_0 Loss: 0.0662 | 0.0519
Epoch 107/300, seasonal_0 Loss: 0.0661 | 0.0518
Epoch 108/300, seasonal_0 Loss: 0.0661 | 0.0518
Epoch 109/300, seasonal_0 Loss: 0.0660 | 0.0518
Epoch 110/300, seasonal_0 Loss: 0.0660 | 0.0517
Epoch 111/300, seasonal_0 Loss: 0.0659 | 0.0517
Epoch 112/300, seasonal_0 Loss: 0.0659 | 0.0517
Epoch 113/300, seasonal_0 Loss: 0.0658 | 0.0517
Epoch 114/300, seasonal_0 Loss: 0.0658 | 0.0516
Epoch 115/300, seasonal_0 Loss: 0.0657 | 0.0516
Epoch 116/300, seasonal_0 Loss: 0.0657 | 0.0516
Epoch 117/300, seasonal_0 Loss: 0.0656 | 0.0516
Epoch 118/300, seasonal_0 Loss: 0.0656 | 0.0515
Epoch 119/300, seasonal_0 Loss: 0.0656 | 0.0515
Epoch 120/300, seasonal_0 Loss: 0.0655 | 0.0515
Epoch 121/300, seasonal_0 Loss: 0.0655 | 0.0515
Epoch 122/300, seasonal_0 Loss: 0.0654 | 0.0515
Epoch 123/300, seasonal_0 Loss: 0.0654 | 0.0514
Epoch 124/300, seasonal_0 Loss: 0.0654 | 0.0514
Epoch 125/300, seasonal_0 Loss: 0.0653 | 0.0514
Epoch 126/300, seasonal_0 Loss: 0.0653 | 0.0514
Epoch 127/300, seasonal_0 Loss: 0.0653 | 0.0514
Epoch 128/300, seasonal_0 Loss: 0.0652 | 0.0514
Epoch 129/300, seasonal_0 Loss: 0.0652 | 0.0513
Epoch 130/300, seasonal_0 Loss: 0.0652 | 0.0513
Epoch 131/300, seasonal_0 Loss: 0.0651 | 0.0513
Epoch 132/300, seasonal_0 Loss: 0.0651 | 0.0513
Epoch 133/300, seasonal_0 Loss: 0.0651 | 0.0513
Epoch 134/300, seasonal_0 Loss: 0.0651 | 0.0513
Epoch 135/300, seasonal_0 Loss: 0.0650 | 0.0513
Epoch 136/300, seasonal_0 Loss: 0.0650 | 0.0512
Epoch 137/300, seasonal_0 Loss: 0.0650 | 0.0512
Epoch 138/300, seasonal_0 Loss: 0.0650 | 0.0512
Epoch 139/300, seasonal_0 Loss: 0.0649 | 0.0512
Epoch 140/300, seasonal_0 Loss: 0.0649 | 0.0512
Epoch 141/300, seasonal_0 Loss: 0.0649 | 0.0512
Epoch 142/300, seasonal_0 Loss: 0.0649 | 0.0512
Epoch 143/300, seasonal_0 Loss: 0.0649 | 0.0512
Epoch 144/300, seasonal_0 Loss: 0.0648 | 0.0512
Epoch 145/300, seasonal_0 Loss: 0.0648 | 0.0511
Epoch 146/300, seasonal_0 Loss: 0.0648 | 0.0511
Epoch 147/300, seasonal_0 Loss: 0.0648 | 0.0511
Epoch 148/300, seasonal_0 Loss: 0.0648 | 0.0511
Epoch 149/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 150/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 151/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 152/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 153/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 154/300, seasonal_0 Loss: 0.0647 | 0.0511
Epoch 155/300, seasonal_0 Loss: 0.0646 | 0.0511
Epoch 156/300, seasonal_0 Loss: 0.0646 | 0.0511
Epoch 157/300, seasonal_0 Loss: 0.0646 | 0.0510
Epoch 158/300, seasonal_0 Loss: 0.0646 | 0.0510
Epoch 159/300, seasonal_0 Loss: 0.0646 | 0.0510
Epoch 160/300, seasonal_0 Loss: 0.0646 | 0.0510
Epoch 161/300, seasonal_0 Loss: 0.0646 | 0.0510
Epoch 162/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 163/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 164/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 165/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 166/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 167/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 168/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 169/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 170/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 171/300, seasonal_0 Loss: 0.0645 | 0.0510
Epoch 172/300, seasonal_0 Loss: 0.0644 | 0.0510
Epoch 173/300, seasonal_0 Loss: 0.0644 | 0.0510
Epoch 174/300, seasonal_0 Loss: 0.0644 | 0.0510
Epoch 175/300, seasonal_0 Loss: 0.0644 | 0.0510
Epoch 176/300, seasonal_0 Loss: 0.0644 | 0.0510
Epoch 177/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 178/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 179/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 180/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 181/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 182/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 183/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 184/300, seasonal_0 Loss: 0.0644 | 0.0509
Epoch 185/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 186/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 187/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 188/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 189/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 190/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 191/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 192/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 193/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 194/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 195/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 196/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 197/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 198/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 199/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 200/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 201/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 202/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 203/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 204/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 205/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 206/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 207/300, seasonal_0 Loss: 0.0643 | 0.0509
Epoch 208/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 209/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 210/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 211/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 212/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 213/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 214/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 215/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 216/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 217/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 218/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 219/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 220/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 221/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 222/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 223/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 224/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 225/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 226/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 227/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 228/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 229/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 230/300, seasonal_0 Loss: 0.0642 | 0.0509
Epoch 231/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 232/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 233/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 234/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 235/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 236/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 237/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 238/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 239/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 240/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 241/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 242/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 243/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 244/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 245/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 246/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 247/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 248/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 249/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 250/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 251/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 252/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 253/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 254/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 255/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 256/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 257/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 258/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 259/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 260/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 261/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 262/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 263/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 264/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 265/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 266/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 267/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 268/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 269/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 270/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 271/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 272/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 273/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 274/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 275/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 276/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 277/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 278/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 279/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 280/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 281/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 282/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 283/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 284/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 285/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 286/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 287/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 288/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 289/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 290/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 291/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 292/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 293/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 294/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 295/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 296/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 297/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 298/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 299/300, seasonal_0 Loss: 0.0642 | 0.0508
Epoch 300/300, seasonal_0 Loss: 0.0642 | 0.0508
Training seasonal_1 component with params: {'observation_period_num': 8, 'train_rates': 0.9879719816512463, 'learning_rate': 0.000703155254653984, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9375957401128853}
Epoch 1/300, seasonal_1 Loss: 0.3640 | 0.1348
Epoch 2/300, seasonal_1 Loss: 0.1255 | 0.0862
Epoch 3/300, seasonal_1 Loss: 0.1090 | 0.0791
Epoch 4/300, seasonal_1 Loss: 0.1037 | 0.0640
Epoch 5/300, seasonal_1 Loss: 0.0991 | 0.0616
Epoch 6/300, seasonal_1 Loss: 0.0917 | 0.0605
Epoch 7/300, seasonal_1 Loss: 0.0887 | 0.0547
Epoch 8/300, seasonal_1 Loss: 0.0845 | 0.0522
Epoch 9/300, seasonal_1 Loss: 0.0813 | 0.0492
Epoch 10/300, seasonal_1 Loss: 0.0794 | 0.0469
Epoch 11/300, seasonal_1 Loss: 0.0783 | 0.0461
Epoch 12/300, seasonal_1 Loss: 0.0767 | 0.0460
Epoch 13/300, seasonal_1 Loss: 0.0753 | 0.0447
Epoch 14/300, seasonal_1 Loss: 0.0757 | 0.0405
Epoch 15/300, seasonal_1 Loss: 0.0741 | 0.0389
Epoch 16/300, seasonal_1 Loss: 0.0731 | 0.0392
Epoch 17/300, seasonal_1 Loss: 0.0775 | 0.0399
Epoch 18/300, seasonal_1 Loss: 0.0713 | 0.0405
Epoch 19/300, seasonal_1 Loss: 0.0758 | 0.0383
Epoch 20/300, seasonal_1 Loss: 0.0720 | 0.0380
Epoch 21/300, seasonal_1 Loss: 0.0736 | 0.0487
Epoch 22/300, seasonal_1 Loss: 0.0692 | 0.0400
Epoch 23/300, seasonal_1 Loss: 0.0788 | 0.0508
Epoch 24/300, seasonal_1 Loss: 0.0787 | 0.0424
Epoch 25/300, seasonal_1 Loss: 0.0815 | 0.0378
Epoch 26/300, seasonal_1 Loss: 0.0915 | 0.0535
Epoch 27/300, seasonal_1 Loss: 0.0823 | 0.0374
Epoch 28/300, seasonal_1 Loss: 0.0724 | 0.0399
Epoch 29/300, seasonal_1 Loss: 0.0694 | 0.0361
Epoch 30/300, seasonal_1 Loss: 0.0680 | 0.0336
Epoch 31/300, seasonal_1 Loss: 0.0674 | 0.0337
Epoch 32/300, seasonal_1 Loss: 0.0721 | 0.0357
Epoch 33/300, seasonal_1 Loss: 0.0735 | 0.0369
Epoch 34/300, seasonal_1 Loss: 0.0720 | 0.0318
Epoch 35/300, seasonal_1 Loss: 0.0680 | 0.0312
Epoch 36/300, seasonal_1 Loss: 0.0663 | 0.0354
Epoch 37/300, seasonal_1 Loss: 0.0631 | 0.0308
Epoch 38/300, seasonal_1 Loss: 0.0620 | 0.0289
Epoch 39/300, seasonal_1 Loss: 0.0614 | 0.0281
Epoch 40/300, seasonal_1 Loss: 0.0620 | 0.0283
Epoch 41/300, seasonal_1 Loss: 0.0628 | 0.0277
Epoch 42/300, seasonal_1 Loss: 0.0638 | 0.0284
Epoch 43/300, seasonal_1 Loss: 0.0656 | 0.0295
Epoch 44/300, seasonal_1 Loss: 0.0670 | 0.0332
Epoch 45/300, seasonal_1 Loss: 0.0640 | 0.0283
Epoch 46/300, seasonal_1 Loss: 0.0656 | 0.0305
Epoch 47/300, seasonal_1 Loss: 0.0631 | 0.0258
Epoch 48/300, seasonal_1 Loss: 0.0633 | 0.0279
Epoch 49/300, seasonal_1 Loss: 0.0676 | 0.0332
Epoch 50/300, seasonal_1 Loss: 0.0639 | 0.0261
Epoch 51/300, seasonal_1 Loss: 0.0632 | 0.0238
Epoch 52/300, seasonal_1 Loss: 0.0651 | 0.0248
Epoch 53/300, seasonal_1 Loss: 0.0636 | 0.0196
Epoch 54/300, seasonal_1 Loss: 0.0707 | 0.0208
Epoch 55/300, seasonal_1 Loss: 0.0771 | 0.0239
Epoch 56/300, seasonal_1 Loss: 0.0802 | 0.0309
Epoch 57/300, seasonal_1 Loss: 0.0871 | 0.0329
Epoch 58/300, seasonal_1 Loss: 0.0866 | 0.0320
Epoch 59/300, seasonal_1 Loss: 0.0957 | 0.0683
Epoch 60/300, seasonal_1 Loss: 0.0881 | 0.1277
Epoch 61/300, seasonal_1 Loss: 0.0850 | 0.0605
Epoch 62/300, seasonal_1 Loss: 0.0793 | 0.0325
Epoch 63/300, seasonal_1 Loss: 0.0719 | 0.0235
Epoch 64/300, seasonal_1 Loss: 0.0650 | 0.0233
Epoch 65/300, seasonal_1 Loss: 0.0580 | 0.0179
Epoch 66/300, seasonal_1 Loss: 0.0573 | 0.0179
Epoch 67/300, seasonal_1 Loss: 0.0562 | 0.0180
Epoch 68/300, seasonal_1 Loss: 0.0553 | 0.0174
Epoch 69/300, seasonal_1 Loss: 0.0546 | 0.0171
Epoch 70/300, seasonal_1 Loss: 0.0543 | 0.0172
Epoch 71/300, seasonal_1 Loss: 0.0542 | 0.0169
Epoch 72/300, seasonal_1 Loss: 0.0538 | 0.0166
Epoch 73/300, seasonal_1 Loss: 0.0534 | 0.0165
Epoch 74/300, seasonal_1 Loss: 0.0531 | 0.0164
Epoch 75/300, seasonal_1 Loss: 0.0528 | 0.0164
Epoch 76/300, seasonal_1 Loss: 0.0526 | 0.0166
Epoch 77/300, seasonal_1 Loss: 0.0526 | 0.0167
Epoch 78/300, seasonal_1 Loss: 0.0524 | 0.0167
Epoch 79/300, seasonal_1 Loss: 0.0524 | 0.0170
Epoch 80/300, seasonal_1 Loss: 0.0524 | 0.0171
Epoch 81/300, seasonal_1 Loss: 0.0524 | 0.0171
Epoch 82/300, seasonal_1 Loss: 0.0525 | 0.0170
Epoch 83/300, seasonal_1 Loss: 0.0527 | 0.0169
Epoch 84/300, seasonal_1 Loss: 0.0530 | 0.0168
Epoch 85/300, seasonal_1 Loss: 0.0538 | 0.0167
Epoch 86/300, seasonal_1 Loss: 0.0555 | 0.0187
Epoch 87/300, seasonal_1 Loss: 0.0563 | 0.0173
Epoch 88/300, seasonal_1 Loss: 0.0545 | 0.0169
Epoch 89/300, seasonal_1 Loss: 0.0547 | 0.0190
Epoch 90/300, seasonal_1 Loss: 0.0541 | 0.0197
Epoch 91/300, seasonal_1 Loss: 0.0541 | 0.0208
Epoch 92/300, seasonal_1 Loss: 0.0537 | 0.0198
Epoch 93/300, seasonal_1 Loss: 0.0532 | 0.0196
Epoch 94/300, seasonal_1 Loss: 0.0528 | 0.0180
Epoch 95/300, seasonal_1 Loss: 0.0522 | 0.0172
Epoch 96/300, seasonal_1 Loss: 0.0517 | 0.0163
Epoch 97/300, seasonal_1 Loss: 0.0515 | 0.0157
Epoch 98/300, seasonal_1 Loss: 0.0514 | 0.0152
Epoch 99/300, seasonal_1 Loss: 0.0515 | 0.0149
Epoch 100/300, seasonal_1 Loss: 0.0522 | 0.0149
Epoch 101/300, seasonal_1 Loss: 0.0544 | 0.0184
Epoch 102/300, seasonal_1 Loss: 0.0577 | 0.0218
Epoch 103/300, seasonal_1 Loss: 0.0557 | 0.0194
Epoch 104/300, seasonal_1 Loss: 0.0533 | 0.0175
Epoch 105/300, seasonal_1 Loss: 0.0517 | 0.0148
Epoch 106/300, seasonal_1 Loss: 0.0507 | 0.0147
Epoch 107/300, seasonal_1 Loss: 0.0504 | 0.0146
Epoch 108/300, seasonal_1 Loss: 0.0504 | 0.0149
Epoch 109/300, seasonal_1 Loss: 0.0506 | 0.0151
Epoch 110/300, seasonal_1 Loss: 0.0509 | 0.0151
Epoch 111/300, seasonal_1 Loss: 0.0508 | 0.0152
Epoch 112/300, seasonal_1 Loss: 0.0507 | 0.0152
Epoch 113/300, seasonal_1 Loss: 0.0507 | 0.0154
Epoch 114/300, seasonal_1 Loss: 0.0507 | 0.0156
Epoch 115/300, seasonal_1 Loss: 0.0510 | 0.0164
Epoch 116/300, seasonal_1 Loss: 0.0517 | 0.0184
Epoch 117/300, seasonal_1 Loss: 0.0530 | 0.0194
Epoch 118/300, seasonal_1 Loss: 0.0550 | 0.0190
Epoch 119/300, seasonal_1 Loss: 0.0548 | 0.0172
Epoch 120/300, seasonal_1 Loss: 0.0527 | 0.0164
Epoch 121/300, seasonal_1 Loss: 0.0512 | 0.0166
Epoch 122/300, seasonal_1 Loss: 0.0498 | 0.0163
Epoch 123/300, seasonal_1 Loss: 0.0492 | 0.0162
Epoch 124/300, seasonal_1 Loss: 0.0493 | 0.0161
Epoch 125/300, seasonal_1 Loss: 0.0495 | 0.0158
Epoch 126/300, seasonal_1 Loss: 0.0496 | 0.0153
Epoch 127/300, seasonal_1 Loss: 0.0496 | 0.0148
Epoch 128/300, seasonal_1 Loss: 0.0493 | 0.0144
Epoch 129/300, seasonal_1 Loss: 0.0489 | 0.0142
Epoch 130/300, seasonal_1 Loss: 0.0485 | 0.0144
Epoch 131/300, seasonal_1 Loss: 0.0482 | 0.0150
Epoch 132/300, seasonal_1 Loss: 0.0481 | 0.0161
Epoch 133/300, seasonal_1 Loss: 0.0483 | 0.0172
Epoch 134/300, seasonal_1 Loss: 0.0486 | 0.0173
Epoch 135/300, seasonal_1 Loss: 0.0486 | 0.0165
Epoch 136/300, seasonal_1 Loss: 0.0483 | 0.0162
Epoch 137/300, seasonal_1 Loss: 0.0479 | 0.0165
Epoch 138/300, seasonal_1 Loss: 0.0477 | 0.0168
Epoch 139/300, seasonal_1 Loss: 0.0476 | 0.0167
Epoch 140/300, seasonal_1 Loss: 0.0476 | 0.0164
Epoch 141/300, seasonal_1 Loss: 0.0475 | 0.0161
Epoch 142/300, seasonal_1 Loss: 0.0475 | 0.0158
Epoch 143/300, seasonal_1 Loss: 0.0474 | 0.0156
Epoch 144/300, seasonal_1 Loss: 0.0473 | 0.0156
Epoch 145/300, seasonal_1 Loss: 0.0473 | 0.0156
Epoch 146/300, seasonal_1 Loss: 0.0472 | 0.0158
Epoch 147/300, seasonal_1 Loss: 0.0472 | 0.0159
Epoch 148/300, seasonal_1 Loss: 0.0472 | 0.0161
Epoch 149/300, seasonal_1 Loss: 0.0472 | 0.0162
Epoch 150/300, seasonal_1 Loss: 0.0473 | 0.0162
Epoch 151/300, seasonal_1 Loss: 0.0473 | 0.0162
Epoch 152/300, seasonal_1 Loss: 0.0473 | 0.0160
Epoch 153/300, seasonal_1 Loss: 0.0472 | 0.0161
Epoch 154/300, seasonal_1 Loss: 0.0470 | 0.0162
Epoch 155/300, seasonal_1 Loss: 0.0470 | 0.0162
Epoch 156/300, seasonal_1 Loss: 0.0470 | 0.0159
Epoch 157/300, seasonal_1 Loss: 0.0470 | 0.0155
Epoch 158/300, seasonal_1 Loss: 0.0470 | 0.0150
Epoch 159/300, seasonal_1 Loss: 0.0470 | 0.0148
Epoch 160/300, seasonal_1 Loss: 0.0470 | 0.0146
Epoch 161/300, seasonal_1 Loss: 0.0469 | 0.0146
Epoch 162/300, seasonal_1 Loss: 0.0468 | 0.0149
Epoch 163/300, seasonal_1 Loss: 0.0467 | 0.0151
Epoch 164/300, seasonal_1 Loss: 0.0467 | 0.0154
Epoch 165/300, seasonal_1 Loss: 0.0467 | 0.0155
Epoch 166/300, seasonal_1 Loss: 0.0468 | 0.0156
Epoch 167/300, seasonal_1 Loss: 0.0468 | 0.0155
Epoch 168/300, seasonal_1 Loss: 0.0467 | 0.0154
Epoch 169/300, seasonal_1 Loss: 0.0467 | 0.0155
Epoch 170/300, seasonal_1 Loss: 0.0466 | 0.0155
Epoch 171/300, seasonal_1 Loss: 0.0465 | 0.0155
Epoch 172/300, seasonal_1 Loss: 0.0465 | 0.0154
Epoch 173/300, seasonal_1 Loss: 0.0465 | 0.0152
Epoch 174/300, seasonal_1 Loss: 0.0465 | 0.0150
Epoch 175/300, seasonal_1 Loss: 0.0465 | 0.0149
Epoch 176/300, seasonal_1 Loss: 0.0464 | 0.0149
Epoch 177/300, seasonal_1 Loss: 0.0464 | 0.0149
Epoch 178/300, seasonal_1 Loss: 0.0463 | 0.0150
Epoch 179/300, seasonal_1 Loss: 0.0463 | 0.0150
Epoch 180/300, seasonal_1 Loss: 0.0463 | 0.0151
Epoch 181/300, seasonal_1 Loss: 0.0463 | 0.0151
Epoch 182/300, seasonal_1 Loss: 0.0463 | 0.0151
Epoch 183/300, seasonal_1 Loss: 0.0463 | 0.0151
Epoch 184/300, seasonal_1 Loss: 0.0462 | 0.0151
Epoch 185/300, seasonal_1 Loss: 0.0462 | 0.0151
Epoch 186/300, seasonal_1 Loss: 0.0462 | 0.0150
Epoch 187/300, seasonal_1 Loss: 0.0461 | 0.0150
Epoch 188/300, seasonal_1 Loss: 0.0461 | 0.0149
Epoch 189/300, seasonal_1 Loss: 0.0461 | 0.0149
Epoch 190/300, seasonal_1 Loss: 0.0461 | 0.0148
Epoch 191/300, seasonal_1 Loss: 0.0461 | 0.0147
Epoch 192/300, seasonal_1 Loss: 0.0460 | 0.0147
Epoch 193/300, seasonal_1 Loss: 0.0460 | 0.0147
Epoch 194/300, seasonal_1 Loss: 0.0460 | 0.0147
Epoch 195/300, seasonal_1 Loss: 0.0460 | 0.0147
Epoch 196/300, seasonal_1 Loss: 0.0459 | 0.0147
Epoch 197/300, seasonal_1 Loss: 0.0459 | 0.0147
Epoch 198/300, seasonal_1 Loss: 0.0459 | 0.0147
Epoch 199/300, seasonal_1 Loss: 0.0459 | 0.0147
Epoch 200/300, seasonal_1 Loss: 0.0459 | 0.0147
Epoch 201/300, seasonal_1 Loss: 0.0459 | 0.0146
Epoch 202/300, seasonal_1 Loss: 0.0458 | 0.0146
Epoch 203/300, seasonal_1 Loss: 0.0458 | 0.0146
Epoch 204/300, seasonal_1 Loss: 0.0458 | 0.0146
Epoch 205/300, seasonal_1 Loss: 0.0458 | 0.0145
Epoch 206/300, seasonal_1 Loss: 0.0458 | 0.0145
Epoch 207/300, seasonal_1 Loss: 0.0458 | 0.0145
Epoch 208/300, seasonal_1 Loss: 0.0458 | 0.0145
Epoch 209/300, seasonal_1 Loss: 0.0457 | 0.0145
Epoch 210/300, seasonal_1 Loss: 0.0457 | 0.0145
Epoch 211/300, seasonal_1 Loss: 0.0457 | 0.0145
Epoch 212/300, seasonal_1 Loss: 0.0457 | 0.0144
Epoch 213/300, seasonal_1 Loss: 0.0457 | 0.0144
Epoch 214/300, seasonal_1 Loss: 0.0457 | 0.0144
Epoch 215/300, seasonal_1 Loss: 0.0457 | 0.0144
Epoch 216/300, seasonal_1 Loss: 0.0456 | 0.0144
Epoch 217/300, seasonal_1 Loss: 0.0456 | 0.0144
Epoch 218/300, seasonal_1 Loss: 0.0456 | 0.0144
Epoch 219/300, seasonal_1 Loss: 0.0456 | 0.0144
Epoch 220/300, seasonal_1 Loss: 0.0456 | 0.0144
Epoch 221/300, seasonal_1 Loss: 0.0456 | 0.0143
Epoch 222/300, seasonal_1 Loss: 0.0456 | 0.0143
Epoch 223/300, seasonal_1 Loss: 0.0456 | 0.0143
Epoch 224/300, seasonal_1 Loss: 0.0456 | 0.0143
Epoch 225/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 226/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 227/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 228/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 229/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 230/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 231/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 232/300, seasonal_1 Loss: 0.0455 | 0.0143
Epoch 233/300, seasonal_1 Loss: 0.0455 | 0.0142
Epoch 234/300, seasonal_1 Loss: 0.0455 | 0.0142
Epoch 235/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 236/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 237/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 238/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 239/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 240/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 241/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 242/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 243/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 244/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 245/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 246/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 247/300, seasonal_1 Loss: 0.0454 | 0.0142
Epoch 248/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 249/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 250/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 251/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 252/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 253/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 254/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 255/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 256/300, seasonal_1 Loss: 0.0453 | 0.0142
Epoch 257/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 258/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 259/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 260/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 261/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 262/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 263/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 264/300, seasonal_1 Loss: 0.0453 | 0.0141
Epoch 265/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 266/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 267/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 268/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 269/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 270/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 271/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 272/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 273/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 274/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 275/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 276/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 277/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 278/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 279/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 280/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 281/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 282/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 283/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 284/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 285/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 286/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 287/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 288/300, seasonal_1 Loss: 0.0452 | 0.0141
Epoch 289/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 290/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 291/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 292/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 293/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 294/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 295/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 296/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 297/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 298/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 299/300, seasonal_1 Loss: 0.0451 | 0.0141
Epoch 300/300, seasonal_1 Loss: 0.0451 | 0.0141
Training seasonal_2 component with params: {'observation_period_num': 21, 'train_rates': 0.9628113887044799, 'learning_rate': 0.0002238292292963187, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9140432219782495}
Epoch 1/300, seasonal_2 Loss: 0.3217 | 0.2926
Epoch 2/300, seasonal_2 Loss: 0.2343 | 0.2733
Epoch 3/300, seasonal_2 Loss: 0.3043 | 0.7528
Epoch 4/300, seasonal_2 Loss: 0.2303 | 0.2766
Epoch 5/300, seasonal_2 Loss: 0.1685 | 0.2307
Epoch 6/300, seasonal_2 Loss: 0.1624 | 0.2168
Epoch 7/300, seasonal_2 Loss: 0.1741 | 0.2144
Epoch 8/300, seasonal_2 Loss: 0.1703 | 0.2047
Epoch 9/300, seasonal_2 Loss: 0.1214 | 0.1765
Epoch 10/300, seasonal_2 Loss: 0.1204 | 0.1715
Epoch 11/300, seasonal_2 Loss: 0.1177 | 0.1567
Epoch 12/300, seasonal_2 Loss: 0.1062 | 0.1493
Epoch 13/300, seasonal_2 Loss: 0.1066 | 0.1321
Epoch 14/300, seasonal_2 Loss: 0.1104 | 0.1182
Epoch 15/300, seasonal_2 Loss: 0.1150 | 0.1173
Epoch 16/300, seasonal_2 Loss: 0.1072 | 0.1140
Epoch 17/300, seasonal_2 Loss: 0.1219 | 0.1988
Epoch 18/300, seasonal_2 Loss: 0.1322 | 0.3579
Epoch 19/300, seasonal_2 Loss: 0.1165 | 0.1196
Epoch 20/300, seasonal_2 Loss: 0.1078 | 0.1103
Epoch 21/300, seasonal_2 Loss: 0.0976 | 0.1193
Epoch 22/300, seasonal_2 Loss: 0.1046 | 0.0979
Epoch 23/300, seasonal_2 Loss: 0.1029 | 0.0935
Epoch 24/300, seasonal_2 Loss: 0.0947 | 0.0906
Epoch 25/300, seasonal_2 Loss: 0.0904 | 0.0909
Epoch 26/300, seasonal_2 Loss: 0.0971 | 0.0917
Epoch 27/300, seasonal_2 Loss: 0.0958 | 0.0798
Epoch 28/300, seasonal_2 Loss: 0.0894 | 0.0770
Epoch 29/300, seasonal_2 Loss: 0.0893 | 0.0765
Epoch 30/300, seasonal_2 Loss: 0.0869 | 0.0808
Epoch 31/300, seasonal_2 Loss: 0.0835 | 0.0753
Epoch 32/300, seasonal_2 Loss: 0.0847 | 0.0704
Epoch 33/300, seasonal_2 Loss: 0.0875 | 0.0761
Epoch 34/300, seasonal_2 Loss: 0.0845 | 0.0670
Epoch 35/300, seasonal_2 Loss: 0.0852 | 0.0739
Epoch 36/300, seasonal_2 Loss: 0.0881 | 0.0948
Epoch 37/300, seasonal_2 Loss: 0.0851 | 0.1057
Epoch 38/300, seasonal_2 Loss: 0.0820 | 0.0758
Epoch 39/300, seasonal_2 Loss: 0.0872 | 0.0751
Epoch 40/300, seasonal_2 Loss: 0.0880 | 0.0644
Epoch 41/300, seasonal_2 Loss: 0.0860 | 0.0839
Epoch 42/300, seasonal_2 Loss: 0.0865 | 0.0693
Epoch 43/300, seasonal_2 Loss: 0.0822 | 0.0663
Epoch 44/300, seasonal_2 Loss: 0.0865 | 0.0681
Epoch 45/300, seasonal_2 Loss: 0.0828 | 0.0713
Epoch 46/300, seasonal_2 Loss: 0.0835 | 0.0836
Epoch 47/300, seasonal_2 Loss: 0.0806 | 0.0664
Epoch 48/300, seasonal_2 Loss: 0.0817 | 0.0632
Epoch 49/300, seasonal_2 Loss: 0.0788 | 0.0592
Epoch 50/300, seasonal_2 Loss: 0.0797 | 0.0625
Epoch 51/300, seasonal_2 Loss: 0.0784 | 0.0619
Epoch 52/300, seasonal_2 Loss: 0.0757 | 0.0582
Epoch 53/300, seasonal_2 Loss: 0.0768 | 0.0596
Epoch 54/300, seasonal_2 Loss: 0.0754 | 0.0556
Epoch 55/300, seasonal_2 Loss: 0.0756 | 0.0605
Epoch 56/300, seasonal_2 Loss: 0.0746 | 0.0614
Epoch 57/300, seasonal_2 Loss: 0.0739 | 0.0554
Epoch 58/300, seasonal_2 Loss: 0.0734 | 0.0555
Epoch 59/300, seasonal_2 Loss: 0.0726 | 0.0534
Epoch 60/300, seasonal_2 Loss: 0.0727 | 0.0541
Epoch 61/300, seasonal_2 Loss: 0.0722 | 0.0550
Epoch 62/300, seasonal_2 Loss: 0.0713 | 0.0533
Epoch 63/300, seasonal_2 Loss: 0.0712 | 0.0524
Epoch 64/300, seasonal_2 Loss: 0.0710 | 0.0522
Epoch 65/300, seasonal_2 Loss: 0.0707 | 0.0512
Epoch 66/300, seasonal_2 Loss: 0.0707 | 0.0520
Epoch 67/300, seasonal_2 Loss: 0.0703 | 0.0526
Epoch 68/300, seasonal_2 Loss: 0.0699 | 0.0508
Epoch 69/300, seasonal_2 Loss: 0.0697 | 0.0507
Epoch 70/300, seasonal_2 Loss: 0.0692 | 0.0500
Epoch 71/300, seasonal_2 Loss: 0.0689 | 0.0495
Epoch 72/300, seasonal_2 Loss: 0.0690 | 0.0507
Epoch 73/300, seasonal_2 Loss: 0.0687 | 0.0510
Epoch 74/300, seasonal_2 Loss: 0.0683 | 0.0489
Epoch 75/300, seasonal_2 Loss: 0.0684 | 0.0498
Epoch 76/300, seasonal_2 Loss: 0.0681 | 0.0489
Epoch 77/300, seasonal_2 Loss: 0.0680 | 0.0481
Epoch 78/300, seasonal_2 Loss: 0.0683 | 0.0494
Epoch 79/300, seasonal_2 Loss: 0.0675 | 0.0482
Epoch 80/300, seasonal_2 Loss: 0.0673 | 0.0474
Epoch 81/300, seasonal_2 Loss: 0.0673 | 0.0474
Epoch 82/300, seasonal_2 Loss: 0.0667 | 0.0461
Epoch 83/300, seasonal_2 Loss: 0.0669 | 0.0473
Epoch 84/300, seasonal_2 Loss: 0.0668 | 0.0489
Epoch 85/300, seasonal_2 Loss: 0.0662 | 0.0454
Epoch 86/300, seasonal_2 Loss: 0.0662 | 0.0464
Epoch 87/300, seasonal_2 Loss: 0.0659 | 0.0455
Epoch 88/300, seasonal_2 Loss: 0.0657 | 0.0448
Epoch 89/300, seasonal_2 Loss: 0.0660 | 0.0459
Epoch 90/300, seasonal_2 Loss: 0.0651 | 0.0436
Epoch 91/300, seasonal_2 Loss: 0.0649 | 0.0441
Epoch 92/300, seasonal_2 Loss: 0.0654 | 0.0437
Epoch 93/300, seasonal_2 Loss: 0.0662 | 0.0435
Epoch 94/300, seasonal_2 Loss: 0.0672 | 0.0445
Epoch 95/300, seasonal_2 Loss: 0.0675 | 0.0440
Epoch 96/300, seasonal_2 Loss: 0.0682 | 0.0450
Epoch 97/300, seasonal_2 Loss: 0.0687 | 0.0452
Epoch 98/300, seasonal_2 Loss: 0.0666 | 0.0433
Epoch 99/300, seasonal_2 Loss: 0.0642 | 0.0414
Epoch 100/300, seasonal_2 Loss: 0.0656 | 0.0436
Epoch 101/300, seasonal_2 Loss: 0.0673 | 0.0437
Epoch 102/300, seasonal_2 Loss: 0.0650 | 0.0413
Epoch 103/300, seasonal_2 Loss: 0.0635 | 0.0421
Epoch 104/300, seasonal_2 Loss: 0.0639 | 0.0417
Epoch 105/300, seasonal_2 Loss: 0.0631 | 0.0404
Epoch 106/300, seasonal_2 Loss: 0.0625 | 0.0400
Epoch 107/300, seasonal_2 Loss: 0.0624 | 0.0398
Epoch 108/300, seasonal_2 Loss: 0.0622 | 0.0396
Epoch 109/300, seasonal_2 Loss: 0.0620 | 0.0394
Epoch 110/300, seasonal_2 Loss: 0.0619 | 0.0393
Epoch 111/300, seasonal_2 Loss: 0.0617 | 0.0391
Epoch 112/300, seasonal_2 Loss: 0.0616 | 0.0390
Epoch 113/300, seasonal_2 Loss: 0.0615 | 0.0388
Epoch 114/300, seasonal_2 Loss: 0.0614 | 0.0387
Epoch 115/300, seasonal_2 Loss: 0.0613 | 0.0385
Epoch 116/300, seasonal_2 Loss: 0.0612 | 0.0384
Epoch 117/300, seasonal_2 Loss: 0.0611 | 0.0382
Epoch 118/300, seasonal_2 Loss: 0.0610 | 0.0381
Epoch 119/300, seasonal_2 Loss: 0.0609 | 0.0380
Epoch 120/300, seasonal_2 Loss: 0.0608 | 0.0379
Epoch 121/300, seasonal_2 Loss: 0.0607 | 0.0377
Epoch 122/300, seasonal_2 Loss: 0.0606 | 0.0376
Epoch 123/300, seasonal_2 Loss: 0.0606 | 0.0375
Epoch 124/300, seasonal_2 Loss: 0.0605 | 0.0374
Epoch 125/300, seasonal_2 Loss: 0.0604 | 0.0373
Epoch 126/300, seasonal_2 Loss: 0.0603 | 0.0372
Epoch 127/300, seasonal_2 Loss: 0.0602 | 0.0371
Epoch 128/300, seasonal_2 Loss: 0.0601 | 0.0370
Epoch 129/300, seasonal_2 Loss: 0.0601 | 0.0369
Epoch 130/300, seasonal_2 Loss: 0.0600 | 0.0368
Epoch 131/300, seasonal_2 Loss: 0.0599 | 0.0367
Epoch 132/300, seasonal_2 Loss: 0.0598 | 0.0366
Epoch 133/300, seasonal_2 Loss: 0.0598 | 0.0366
Epoch 134/300, seasonal_2 Loss: 0.0597 | 0.0365
Epoch 135/300, seasonal_2 Loss: 0.0596 | 0.0364
Epoch 136/300, seasonal_2 Loss: 0.0595 | 0.0363
Epoch 137/300, seasonal_2 Loss: 0.0595 | 0.0363
Epoch 138/300, seasonal_2 Loss: 0.0594 | 0.0362
Epoch 139/300, seasonal_2 Loss: 0.0593 | 0.0361
Epoch 140/300, seasonal_2 Loss: 0.0593 | 0.0361
Epoch 141/300, seasonal_2 Loss: 0.0592 | 0.0360
Epoch 142/300, seasonal_2 Loss: 0.0591 | 0.0359
Epoch 143/300, seasonal_2 Loss: 0.0591 | 0.0359
Epoch 144/300, seasonal_2 Loss: 0.0590 | 0.0358
Epoch 145/300, seasonal_2 Loss: 0.0590 | 0.0357
Epoch 146/300, seasonal_2 Loss: 0.0589 | 0.0357
Epoch 147/300, seasonal_2 Loss: 0.0588 | 0.0356
Epoch 148/300, seasonal_2 Loss: 0.0588 | 0.0356
Epoch 149/300, seasonal_2 Loss: 0.0587 | 0.0355
Epoch 150/300, seasonal_2 Loss: 0.0587 | 0.0355
Epoch 151/300, seasonal_2 Loss: 0.0586 | 0.0354
Epoch 152/300, seasonal_2 Loss: 0.0586 | 0.0354
Epoch 153/300, seasonal_2 Loss: 0.0585 | 0.0353
Epoch 154/300, seasonal_2 Loss: 0.0584 | 0.0353
Epoch 155/300, seasonal_2 Loss: 0.0584 | 0.0352
Epoch 156/300, seasonal_2 Loss: 0.0583 | 0.0352
Epoch 157/300, seasonal_2 Loss: 0.0583 | 0.0352
Epoch 158/300, seasonal_2 Loss: 0.0582 | 0.0351
Epoch 159/300, seasonal_2 Loss: 0.0582 | 0.0351
Epoch 160/300, seasonal_2 Loss: 0.0581 | 0.0350
Epoch 161/300, seasonal_2 Loss: 0.0581 | 0.0350
Epoch 162/300, seasonal_2 Loss: 0.0580 | 0.0350
Epoch 163/300, seasonal_2 Loss: 0.0580 | 0.0349
Epoch 164/300, seasonal_2 Loss: 0.0579 | 0.0349
Epoch 165/300, seasonal_2 Loss: 0.0579 | 0.0348
Epoch 166/300, seasonal_2 Loss: 0.0579 | 0.0348
Epoch 167/300, seasonal_2 Loss: 0.0578 | 0.0348
Epoch 168/300, seasonal_2 Loss: 0.0578 | 0.0347
Epoch 169/300, seasonal_2 Loss: 0.0577 | 0.0347
Epoch 170/300, seasonal_2 Loss: 0.0577 | 0.0347
Epoch 171/300, seasonal_2 Loss: 0.0576 | 0.0346
Epoch 172/300, seasonal_2 Loss: 0.0576 | 0.0346
Epoch 173/300, seasonal_2 Loss: 0.0576 | 0.0346
Epoch 174/300, seasonal_2 Loss: 0.0575 | 0.0346
Epoch 175/300, seasonal_2 Loss: 0.0575 | 0.0345
Epoch 176/300, seasonal_2 Loss: 0.0574 | 0.0345
Epoch 177/300, seasonal_2 Loss: 0.0574 | 0.0345
Epoch 178/300, seasonal_2 Loss: 0.0574 | 0.0344
Epoch 179/300, seasonal_2 Loss: 0.0573 | 0.0344
Epoch 180/300, seasonal_2 Loss: 0.0573 | 0.0344
Epoch 181/300, seasonal_2 Loss: 0.0573 | 0.0343
Epoch 182/300, seasonal_2 Loss: 0.0572 | 0.0343
Epoch 183/300, seasonal_2 Loss: 0.0572 | 0.0343
Epoch 184/300, seasonal_2 Loss: 0.0571 | 0.0343
Epoch 185/300, seasonal_2 Loss: 0.0571 | 0.0342
Epoch 186/300, seasonal_2 Loss: 0.0571 | 0.0342
Epoch 187/300, seasonal_2 Loss: 0.0570 | 0.0342
Epoch 188/300, seasonal_2 Loss: 0.0570 | 0.0342
Epoch 189/300, seasonal_2 Loss: 0.0570 | 0.0341
Epoch 190/300, seasonal_2 Loss: 0.0569 | 0.0341
Epoch 191/300, seasonal_2 Loss: 0.0569 | 0.0341
Epoch 192/300, seasonal_2 Loss: 0.0569 | 0.0341
Epoch 193/300, seasonal_2 Loss: 0.0569 | 0.0340
Epoch 194/300, seasonal_2 Loss: 0.0568 | 0.0340
Epoch 195/300, seasonal_2 Loss: 0.0568 | 0.0340
Epoch 196/300, seasonal_2 Loss: 0.0568 | 0.0340
Epoch 197/300, seasonal_2 Loss: 0.0567 | 0.0339
Epoch 198/300, seasonal_2 Loss: 0.0567 | 0.0339
Epoch 199/300, seasonal_2 Loss: 0.0567 | 0.0339
Epoch 200/300, seasonal_2 Loss: 0.0566 | 0.0339
Epoch 201/300, seasonal_2 Loss: 0.0566 | 0.0339
Epoch 202/300, seasonal_2 Loss: 0.0566 | 0.0338
Epoch 203/300, seasonal_2 Loss: 0.0566 | 0.0338
Epoch 204/300, seasonal_2 Loss: 0.0565 | 0.0338
Epoch 205/300, seasonal_2 Loss: 0.0565 | 0.0338
Epoch 206/300, seasonal_2 Loss: 0.0565 | 0.0338
Epoch 207/300, seasonal_2 Loss: 0.0565 | 0.0337
Epoch 208/300, seasonal_2 Loss: 0.0564 | 0.0337
Epoch 209/300, seasonal_2 Loss: 0.0564 | 0.0337
Epoch 210/300, seasonal_2 Loss: 0.0564 | 0.0337
Epoch 211/300, seasonal_2 Loss: 0.0564 | 0.0337
Epoch 212/300, seasonal_2 Loss: 0.0563 | 0.0336
Epoch 213/300, seasonal_2 Loss: 0.0563 | 0.0336
Epoch 214/300, seasonal_2 Loss: 0.0563 | 0.0336
Epoch 215/300, seasonal_2 Loss: 0.0563 | 0.0336
Epoch 216/300, seasonal_2 Loss: 0.0562 | 0.0336
Epoch 217/300, seasonal_2 Loss: 0.0562 | 0.0336
Epoch 218/300, seasonal_2 Loss: 0.0562 | 0.0335
Epoch 219/300, seasonal_2 Loss: 0.0562 | 0.0335
Epoch 220/300, seasonal_2 Loss: 0.0562 | 0.0335
Epoch 221/300, seasonal_2 Loss: 0.0561 | 0.0335
Epoch 222/300, seasonal_2 Loss: 0.0561 | 0.0335
Epoch 223/300, seasonal_2 Loss: 0.0561 | 0.0335
Epoch 224/300, seasonal_2 Loss: 0.0561 | 0.0334
Epoch 225/300, seasonal_2 Loss: 0.0561 | 0.0334
Epoch 226/300, seasonal_2 Loss: 0.0560 | 0.0334
Epoch 227/300, seasonal_2 Loss: 0.0560 | 0.0334
Epoch 228/300, seasonal_2 Loss: 0.0560 | 0.0334
Epoch 229/300, seasonal_2 Loss: 0.0560 | 0.0334
Epoch 230/300, seasonal_2 Loss: 0.0560 | 0.0333
Epoch 231/300, seasonal_2 Loss: 0.0560 | 0.0333
Epoch 232/300, seasonal_2 Loss: 0.0559 | 0.0333
Epoch 233/300, seasonal_2 Loss: 0.0559 | 0.0333
Epoch 234/300, seasonal_2 Loss: 0.0559 | 0.0333
Epoch 235/300, seasonal_2 Loss: 0.0559 | 0.0333
Epoch 236/300, seasonal_2 Loss: 0.0559 | 0.0333
Epoch 237/300, seasonal_2 Loss: 0.0560 | 0.0333
Epoch 238/300, seasonal_2 Loss: 0.0560 | 0.0332
Epoch 239/300, seasonal_2 Loss: 0.0561 | 0.0332
Epoch 240/300, seasonal_2 Loss: 0.0563 | 0.0333
Epoch 241/300, seasonal_2 Loss: 0.0567 | 0.0333
Epoch 242/300, seasonal_2 Loss: 0.0572 | 0.0339
Epoch 243/300, seasonal_2 Loss: 0.0575 | 0.0340
Epoch 244/300, seasonal_2 Loss: 0.0576 | 0.0367
Epoch 245/300, seasonal_2 Loss: 0.0573 | 0.0345
Epoch 246/300, seasonal_2 Loss: 0.0568 | 0.0361
Epoch 247/300, seasonal_2 Loss: 0.0564 | 0.0339
Epoch 248/300, seasonal_2 Loss: 0.0561 | 0.0343
Epoch 249/300, seasonal_2 Loss: 0.0559 | 0.0334
Epoch 250/300, seasonal_2 Loss: 0.0557 | 0.0334
Epoch 251/300, seasonal_2 Loss: 0.0557 | 0.0331
Epoch 252/300, seasonal_2 Loss: 0.0556 | 0.0331
Epoch 253/300, seasonal_2 Loss: 0.0556 | 0.0331
Epoch 254/300, seasonal_2 Loss: 0.0556 | 0.0331
Epoch 255/300, seasonal_2 Loss: 0.0556 | 0.0331
Epoch 256/300, seasonal_2 Loss: 0.0556 | 0.0330
Epoch 257/300, seasonal_2 Loss: 0.0556 | 0.0330
Epoch 258/300, seasonal_2 Loss: 0.0556 | 0.0330
Epoch 259/300, seasonal_2 Loss: 0.0556 | 0.0330
Epoch 260/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 261/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 262/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 263/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 264/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 265/300, seasonal_2 Loss: 0.0555 | 0.0330
Epoch 266/300, seasonal_2 Loss: 0.0555 | 0.0329
Epoch 267/300, seasonal_2 Loss: 0.0555 | 0.0329
Epoch 268/300, seasonal_2 Loss: 0.0555 | 0.0329
Epoch 269/300, seasonal_2 Loss: 0.0555 | 0.0329
Epoch 270/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 271/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 272/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 273/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 274/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 275/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 276/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 277/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 278/300, seasonal_2 Loss: 0.0554 | 0.0329
Epoch 279/300, seasonal_2 Loss: 0.0554 | 0.0328
Epoch 280/300, seasonal_2 Loss: 0.0554 | 0.0328
Epoch 281/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 282/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 283/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 284/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 285/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 286/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 287/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 288/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 289/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 290/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 291/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 292/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 293/300, seasonal_2 Loss: 0.0553 | 0.0328
Epoch 294/300, seasonal_2 Loss: 0.0552 | 0.0328
Epoch 295/300, seasonal_2 Loss: 0.0552 | 0.0327
Epoch 296/300, seasonal_2 Loss: 0.0552 | 0.0327
Epoch 297/300, seasonal_2 Loss: 0.0552 | 0.0327
Epoch 298/300, seasonal_2 Loss: 0.0552 | 0.0327
Epoch 299/300, seasonal_2 Loss: 0.0552 | 0.0327
Epoch 300/300, seasonal_2 Loss: 0.0552 | 0.0327
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.9856472836766001, 'learning_rate': 0.0006805468475422376, 'batch_size': 103, 'step_size': 12, 'gamma': 0.879742445948611}
Epoch 1/300, seasonal_3 Loss: 0.2973 | 0.0886
Epoch 2/300, seasonal_3 Loss: 0.1213 | 0.0578
Epoch 3/300, seasonal_3 Loss: 0.1057 | 0.0571
Epoch 4/300, seasonal_3 Loss: 0.1029 | 0.0583
Epoch 5/300, seasonal_3 Loss: 0.0950 | 0.0508
Epoch 6/300, seasonal_3 Loss: 0.0971 | 0.0564
Epoch 7/300, seasonal_3 Loss: 0.0918 | 0.0502
Epoch 8/300, seasonal_3 Loss: 0.0990 | 0.0539
Epoch 9/300, seasonal_3 Loss: 0.0879 | 0.0452
Epoch 10/300, seasonal_3 Loss: 0.0804 | 0.0402
Epoch 11/300, seasonal_3 Loss: 0.0773 | 0.0385
Epoch 12/300, seasonal_3 Loss: 0.0770 | 0.0395
Epoch 13/300, seasonal_3 Loss: 0.0793 | 0.0374
Epoch 14/300, seasonal_3 Loss: 0.0806 | 0.0369
Epoch 15/300, seasonal_3 Loss: 0.0762 | 0.0378
Epoch 16/300, seasonal_3 Loss: 0.0748 | 0.0375
Epoch 17/300, seasonal_3 Loss: 0.0730 | 0.0354
Epoch 18/300, seasonal_3 Loss: 0.0703 | 0.0337
Epoch 19/300, seasonal_3 Loss: 0.0688 | 0.0327
Epoch 20/300, seasonal_3 Loss: 0.0681 | 0.0328
Epoch 21/300, seasonal_3 Loss: 0.0673 | 0.0332
Epoch 22/300, seasonal_3 Loss: 0.0669 | 0.0330
Epoch 23/300, seasonal_3 Loss: 0.0665 | 0.0329
Epoch 24/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 25/300, seasonal_3 Loss: 0.0668 | 0.0337
Epoch 26/300, seasonal_3 Loss: 0.0679 | 0.0360
Epoch 27/300, seasonal_3 Loss: 0.0659 | 0.0323
Epoch 28/300, seasonal_3 Loss: 0.0667 | 0.0352
Epoch 29/300, seasonal_3 Loss: 0.0679 | 0.0396
Epoch 30/300, seasonal_3 Loss: 0.0656 | 0.0329
Epoch 31/300, seasonal_3 Loss: 0.0647 | 0.0344
Epoch 32/300, seasonal_3 Loss: 0.0643 | 0.0354
Epoch 33/300, seasonal_3 Loss: 0.0636 | 0.0348
Epoch 34/300, seasonal_3 Loss: 0.0631 | 0.0319
Epoch 35/300, seasonal_3 Loss: 0.0629 | 0.0328
Epoch 36/300, seasonal_3 Loss: 0.0629 | 0.0329
Epoch 37/300, seasonal_3 Loss: 0.0641 | 0.0318
Epoch 38/300, seasonal_3 Loss: 0.0644 | 0.0311
Epoch 39/300, seasonal_3 Loss: 0.0632 | 0.0313
Epoch 40/300, seasonal_3 Loss: 0.0620 | 0.0282
Epoch 41/300, seasonal_3 Loss: 0.0611 | 0.0268
Epoch 42/300, seasonal_3 Loss: 0.0609 | 0.0296
Epoch 43/300, seasonal_3 Loss: 0.0608 | 0.0248
Epoch 44/300, seasonal_3 Loss: 0.0597 | 0.0225
Epoch 45/300, seasonal_3 Loss: 0.0598 | 0.0260
Epoch 46/300, seasonal_3 Loss: 0.0609 | 0.0249
Epoch 47/300, seasonal_3 Loss: 0.0599 | 0.0255
Epoch 48/300, seasonal_3 Loss: 0.0584 | 0.0215
Epoch 49/300, seasonal_3 Loss: 0.0588 | 0.0248
Epoch 50/300, seasonal_3 Loss: 0.0591 | 0.0243
Epoch 51/300, seasonal_3 Loss: 0.0605 | 0.0348
Epoch 52/300, seasonal_3 Loss: 0.0626 | 0.0307
Epoch 53/300, seasonal_3 Loss: 0.0628 | 0.0311
Epoch 54/300, seasonal_3 Loss: 0.0598 | 0.0330
Epoch 55/300, seasonal_3 Loss: 0.0603 | 0.0235
Epoch 56/300, seasonal_3 Loss: 0.0591 | 0.0240
Epoch 57/300, seasonal_3 Loss: 0.0639 | 0.0275
Epoch 58/300, seasonal_3 Loss: 0.0663 | 0.0276
Epoch 59/300, seasonal_3 Loss: 0.0661 | 0.0273
Epoch 60/300, seasonal_3 Loss: 0.0579 | 0.0212
Epoch 61/300, seasonal_3 Loss: 0.0583 | 0.0220
Epoch 62/300, seasonal_3 Loss: 0.0595 | 0.0212
Epoch 63/300, seasonal_3 Loss: 0.0590 | 0.0227
Epoch 64/300, seasonal_3 Loss: 0.0577 | 0.0226
Epoch 65/300, seasonal_3 Loss: 0.0574 | 0.0253
Epoch 66/300, seasonal_3 Loss: 0.0593 | 0.0211
Epoch 67/300, seasonal_3 Loss: 0.0594 | 0.0234
Epoch 68/300, seasonal_3 Loss: 0.0581 | 0.0217
Epoch 69/300, seasonal_3 Loss: 0.0625 | 0.0232
Epoch 70/300, seasonal_3 Loss: 0.0625 | 0.0200
Epoch 71/300, seasonal_3 Loss: 0.0641 | 0.0218
Epoch 72/300, seasonal_3 Loss: 0.0639 | 0.0239
Epoch 73/300, seasonal_3 Loss: 0.0625 | 0.0279
Epoch 74/300, seasonal_3 Loss: 0.0593 | 0.0213
Epoch 75/300, seasonal_3 Loss: 0.0576 | 0.0202
Epoch 76/300, seasonal_3 Loss: 0.0561 | 0.0189
Epoch 77/300, seasonal_3 Loss: 0.0557 | 0.0204
Epoch 78/300, seasonal_3 Loss: 0.0560 | 0.0199
Epoch 79/300, seasonal_3 Loss: 0.0572 | 0.0209
Epoch 80/300, seasonal_3 Loss: 0.0607 | 0.0198
Epoch 81/300, seasonal_3 Loss: 0.0635 | 0.0199
Epoch 82/300, seasonal_3 Loss: 0.0677 | 0.0230
Epoch 83/300, seasonal_3 Loss: 0.0685 | 0.0254
Epoch 84/300, seasonal_3 Loss: 0.0816 | 0.0521
Epoch 85/300, seasonal_3 Loss: 0.0891 | 0.1455
Epoch 86/300, seasonal_3 Loss: 0.0762 | 0.0192
Epoch 87/300, seasonal_3 Loss: 0.0580 | 0.0202
Epoch 88/300, seasonal_3 Loss: 0.0578 | 0.0193
Epoch 89/300, seasonal_3 Loss: 0.0564 | 0.0194
Epoch 90/300, seasonal_3 Loss: 0.0530 | 0.0178
Epoch 91/300, seasonal_3 Loss: 0.0523 | 0.0173
Epoch 92/300, seasonal_3 Loss: 0.0520 | 0.0170
Epoch 93/300, seasonal_3 Loss: 0.0518 | 0.0166
Epoch 94/300, seasonal_3 Loss: 0.0515 | 0.0164
Epoch 95/300, seasonal_3 Loss: 0.0513 | 0.0163
Epoch 96/300, seasonal_3 Loss: 0.0511 | 0.0163
Epoch 97/300, seasonal_3 Loss: 0.0510 | 0.0163
Epoch 98/300, seasonal_3 Loss: 0.0510 | 0.0165
Epoch 99/300, seasonal_3 Loss: 0.0513 | 0.0171
Epoch 100/300, seasonal_3 Loss: 0.0521 | 0.0177
Epoch 101/300, seasonal_3 Loss: 0.0526 | 0.0178
Epoch 102/300, seasonal_3 Loss: 0.0529 | 0.0175
Epoch 103/300, seasonal_3 Loss: 0.0529 | 0.0171
Epoch 104/300, seasonal_3 Loss: 0.0527 | 0.0174
Epoch 105/300, seasonal_3 Loss: 0.0525 | 0.0180
Epoch 106/300, seasonal_3 Loss: 0.0523 | 0.0177
Epoch 107/300, seasonal_3 Loss: 0.0517 | 0.0179
Epoch 108/300, seasonal_3 Loss: 0.0512 | 0.0178
Epoch 109/300, seasonal_3 Loss: 0.0507 | 0.0176
Epoch 110/300, seasonal_3 Loss: 0.0503 | 0.0173
Epoch 111/300, seasonal_3 Loss: 0.0502 | 0.0169
Epoch 112/300, seasonal_3 Loss: 0.0503 | 0.0166
Epoch 113/300, seasonal_3 Loss: 0.0505 | 0.0163
Epoch 114/300, seasonal_3 Loss: 0.0506 | 0.0159
Epoch 115/300, seasonal_3 Loss: 0.0508 | 0.0151
Epoch 116/300, seasonal_3 Loss: 0.0507 | 0.0154
Epoch 117/300, seasonal_3 Loss: 0.0506 | 0.0170
Epoch 118/300, seasonal_3 Loss: 0.0503 | 0.0168
Epoch 119/300, seasonal_3 Loss: 0.0499 | 0.0172
Epoch 120/300, seasonal_3 Loss: 0.0498 | 0.0172
Epoch 121/300, seasonal_3 Loss: 0.0497 | 0.0168
Epoch 122/300, seasonal_3 Loss: 0.0496 | 0.0169
Epoch 123/300, seasonal_3 Loss: 0.0495 | 0.0169
Epoch 124/300, seasonal_3 Loss: 0.0495 | 0.0167
Epoch 125/300, seasonal_3 Loss: 0.0494 | 0.0167
Epoch 126/300, seasonal_3 Loss: 0.0493 | 0.0166
Epoch 127/300, seasonal_3 Loss: 0.0492 | 0.0165
Epoch 128/300, seasonal_3 Loss: 0.0491 | 0.0162
Epoch 129/300, seasonal_3 Loss: 0.0491 | 0.0160
Epoch 130/300, seasonal_3 Loss: 0.0490 | 0.0157
Epoch 131/300, seasonal_3 Loss: 0.0490 | 0.0155
Epoch 132/300, seasonal_3 Loss: 0.0490 | 0.0154
Epoch 133/300, seasonal_3 Loss: 0.0490 | 0.0153
Epoch 134/300, seasonal_3 Loss: 0.0489 | 0.0153
Epoch 135/300, seasonal_3 Loss: 0.0489 | 0.0155
Epoch 136/300, seasonal_3 Loss: 0.0489 | 0.0162
Epoch 137/300, seasonal_3 Loss: 0.0489 | 0.0166
Epoch 138/300, seasonal_3 Loss: 0.0489 | 0.0167
Epoch 139/300, seasonal_3 Loss: 0.0490 | 0.0167
Epoch 140/300, seasonal_3 Loss: 0.0489 | 0.0161
Epoch 141/300, seasonal_3 Loss: 0.0487 | 0.0157
Epoch 142/300, seasonal_3 Loss: 0.0486 | 0.0155
Epoch 143/300, seasonal_3 Loss: 0.0486 | 0.0154
Epoch 144/300, seasonal_3 Loss: 0.0486 | 0.0153
Epoch 145/300, seasonal_3 Loss: 0.0486 | 0.0152
Epoch 146/300, seasonal_3 Loss: 0.0485 | 0.0152
Epoch 147/300, seasonal_3 Loss: 0.0485 | 0.0151
Epoch 148/300, seasonal_3 Loss: 0.0485 | 0.0151
Epoch 149/300, seasonal_3 Loss: 0.0485 | 0.0158
Epoch 150/300, seasonal_3 Loss: 0.0485 | 0.0165
Epoch 151/300, seasonal_3 Loss: 0.0485 | 0.0167
Epoch 152/300, seasonal_3 Loss: 0.0485 | 0.0163
Epoch 153/300, seasonal_3 Loss: 0.0484 | 0.0160
Epoch 154/300, seasonal_3 Loss: 0.0483 | 0.0158
Epoch 155/300, seasonal_3 Loss: 0.0483 | 0.0158
Epoch 156/300, seasonal_3 Loss: 0.0482 | 0.0156
Epoch 157/300, seasonal_3 Loss: 0.0482 | 0.0155
Epoch 158/300, seasonal_3 Loss: 0.0482 | 0.0154
Epoch 159/300, seasonal_3 Loss: 0.0481 | 0.0154
Epoch 160/300, seasonal_3 Loss: 0.0481 | 0.0153
Epoch 161/300, seasonal_3 Loss: 0.0481 | 0.0153
Epoch 162/300, seasonal_3 Loss: 0.0481 | 0.0154
Epoch 163/300, seasonal_3 Loss: 0.0481 | 0.0153
Epoch 164/300, seasonal_3 Loss: 0.0481 | 0.0155
Epoch 165/300, seasonal_3 Loss: 0.0481 | 0.0165
Epoch 166/300, seasonal_3 Loss: 0.0481 | 0.0168
Epoch 167/300, seasonal_3 Loss: 0.0481 | 0.0162
Epoch 168/300, seasonal_3 Loss: 0.0480 | 0.0162
Epoch 169/300, seasonal_3 Loss: 0.0480 | 0.0161
Epoch 170/300, seasonal_3 Loss: 0.0479 | 0.0159
Epoch 171/300, seasonal_3 Loss: 0.0479 | 0.0158
Epoch 172/300, seasonal_3 Loss: 0.0478 | 0.0157
Epoch 173/300, seasonal_3 Loss: 0.0478 | 0.0156
Epoch 174/300, seasonal_3 Loss: 0.0478 | 0.0156
Epoch 175/300, seasonal_3 Loss: 0.0478 | 0.0155
Epoch 176/300, seasonal_3 Loss: 0.0478 | 0.0155
Epoch 177/300, seasonal_3 Loss: 0.0478 | 0.0155
Epoch 178/300, seasonal_3 Loss: 0.0477 | 0.0154
Epoch 179/300, seasonal_3 Loss: 0.0477 | 0.0155
Epoch 180/300, seasonal_3 Loss: 0.0478 | 0.0156
Epoch 181/300, seasonal_3 Loss: 0.0478 | 0.0162
Epoch 182/300, seasonal_3 Loss: 0.0478 | 0.0169
Epoch 183/300, seasonal_3 Loss: 0.0478 | 0.0163
Epoch 184/300, seasonal_3 Loss: 0.0477 | 0.0162
Epoch 185/300, seasonal_3 Loss: 0.0476 | 0.0162
Epoch 186/300, seasonal_3 Loss: 0.0476 | 0.0161
Epoch 187/300, seasonal_3 Loss: 0.0475 | 0.0161
Epoch 188/300, seasonal_3 Loss: 0.0475 | 0.0160
Epoch 189/300, seasonal_3 Loss: 0.0475 | 0.0159
Epoch 190/300, seasonal_3 Loss: 0.0475 | 0.0159
Epoch 191/300, seasonal_3 Loss: 0.0475 | 0.0158
Epoch 192/300, seasonal_3 Loss: 0.0474 | 0.0157
Epoch 193/300, seasonal_3 Loss: 0.0474 | 0.0157
Epoch 194/300, seasonal_3 Loss: 0.0474 | 0.0156
Epoch 195/300, seasonal_3 Loss: 0.0474 | 0.0156
Epoch 196/300, seasonal_3 Loss: 0.0474 | 0.0155
Epoch 197/300, seasonal_3 Loss: 0.0474 | 0.0155
Epoch 198/300, seasonal_3 Loss: 0.0475 | 0.0157
Epoch 199/300, seasonal_3 Loss: 0.0475 | 0.0165
Epoch 200/300, seasonal_3 Loss: 0.0474 | 0.0168
Epoch 201/300, seasonal_3 Loss: 0.0474 | 0.0163
Epoch 202/300, seasonal_3 Loss: 0.0473 | 0.0162
Epoch 203/300, seasonal_3 Loss: 0.0473 | 0.0161
Epoch 204/300, seasonal_3 Loss: 0.0473 | 0.0160
Epoch 205/300, seasonal_3 Loss: 0.0472 | 0.0160
Epoch 206/300, seasonal_3 Loss: 0.0472 | 0.0159
Epoch 207/300, seasonal_3 Loss: 0.0472 | 0.0158
Epoch 208/300, seasonal_3 Loss: 0.0472 | 0.0158
Epoch 209/300, seasonal_3 Loss: 0.0472 | 0.0157
Epoch 210/300, seasonal_3 Loss: 0.0472 | 0.0157
Epoch 211/300, seasonal_3 Loss: 0.0472 | 0.0157
Epoch 212/300, seasonal_3 Loss: 0.0473 | 0.0162
Epoch 213/300, seasonal_3 Loss: 0.0472 | 0.0166
Epoch 214/300, seasonal_3 Loss: 0.0472 | 0.0164
Epoch 215/300, seasonal_3 Loss: 0.0471 | 0.0162
Epoch 216/300, seasonal_3 Loss: 0.0471 | 0.0161
Epoch 217/300, seasonal_3 Loss: 0.0471 | 0.0160
Epoch 218/300, seasonal_3 Loss: 0.0471 | 0.0159
Epoch 219/300, seasonal_3 Loss: 0.0471 | 0.0159
Epoch 220/300, seasonal_3 Loss: 0.0471 | 0.0158
Epoch 221/300, seasonal_3 Loss: 0.0471 | 0.0157
Epoch 222/300, seasonal_3 Loss: 0.0471 | 0.0158
Epoch 223/300, seasonal_3 Loss: 0.0471 | 0.0160
Epoch 224/300, seasonal_3 Loss: 0.0470 | 0.0163
Epoch 225/300, seasonal_3 Loss: 0.0470 | 0.0164
Epoch 226/300, seasonal_3 Loss: 0.0470 | 0.0162
Epoch 227/300, seasonal_3 Loss: 0.0470 | 0.0161
Epoch 228/300, seasonal_3 Loss: 0.0470 | 0.0160
Epoch 229/300, seasonal_3 Loss: 0.0470 | 0.0159
Epoch 230/300, seasonal_3 Loss: 0.0470 | 0.0159
Epoch 231/300, seasonal_3 Loss: 0.0470 | 0.0159
Epoch 232/300, seasonal_3 Loss: 0.0469 | 0.0160
Epoch 233/300, seasonal_3 Loss: 0.0469 | 0.0161
Epoch 234/300, seasonal_3 Loss: 0.0469 | 0.0162
Epoch 235/300, seasonal_3 Loss: 0.0469 | 0.0162
Epoch 236/300, seasonal_3 Loss: 0.0469 | 0.0161
Epoch 237/300, seasonal_3 Loss: 0.0469 | 0.0160
Epoch 238/300, seasonal_3 Loss: 0.0469 | 0.0160
Epoch 239/300, seasonal_3 Loss: 0.0469 | 0.0160
Epoch 240/300, seasonal_3 Loss: 0.0469 | 0.0160
Epoch 241/300, seasonal_3 Loss: 0.0468 | 0.0160
Epoch 242/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 243/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 244/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 245/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 246/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 247/300, seasonal_3 Loss: 0.0468 | 0.0160
Epoch 248/300, seasonal_3 Loss: 0.0468 | 0.0160
Epoch 249/300, seasonal_3 Loss: 0.0468 | 0.0160
Epoch 250/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 251/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 252/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 253/300, seasonal_3 Loss: 0.0468 | 0.0161
Epoch 254/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 255/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 256/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 257/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 258/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 259/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 260/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 261/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 262/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 263/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 264/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 265/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 266/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 267/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 268/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 269/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 270/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 271/300, seasonal_3 Loss: 0.0467 | 0.0161
Epoch 272/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 273/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 274/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 275/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 276/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 277/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 278/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 279/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 280/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 281/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 282/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 283/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 284/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 285/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 286/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 287/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 288/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 289/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 290/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 291/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 292/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 293/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 294/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 295/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 296/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 297/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 298/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 299/300, seasonal_3 Loss: 0.0466 | 0.0161
Epoch 300/300, seasonal_3 Loss: 0.0466 | 0.0161
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8627613199218886, 'learning_rate': 0.0009171296532336576, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8565352682392651}
Epoch 1/300, resid Loss: 0.1890 | 0.0718
Epoch 2/300, resid Loss: 0.1071 | 0.0689
Epoch 3/300, resid Loss: 0.0966 | 0.0524
Epoch 4/300, resid Loss: 0.0881 | 0.0471
Epoch 5/300, resid Loss: 0.0842 | 0.0465
Epoch 6/300, resid Loss: 0.0804 | 0.0431
Epoch 7/300, resid Loss: 0.0776 | 0.0410
Epoch 8/300, resid Loss: 0.0751 | 0.0398
Epoch 9/300, resid Loss: 0.0729 | 0.0387
Epoch 10/300, resid Loss: 0.0711 | 0.0389
Epoch 11/300, resid Loss: 0.0698 | 0.0427
Epoch 12/300, resid Loss: 0.0687 | 0.0447
Epoch 13/300, resid Loss: 0.0679 | 0.0420
Epoch 14/300, resid Loss: 0.0670 | 0.0357
Epoch 15/300, resid Loss: 0.0662 | 0.0309
Epoch 16/300, resid Loss: 0.0653 | 0.0291
Epoch 17/300, resid Loss: 0.0645 | 0.0287
Epoch 18/300, resid Loss: 0.0639 | 0.0288
Epoch 19/300, resid Loss: 0.0634 | 0.0292
Epoch 20/300, resid Loss: 0.0631 | 0.0292
Epoch 21/300, resid Loss: 0.0627 | 0.0286
Epoch 22/300, resid Loss: 0.0624 | 0.0283
Epoch 23/300, resid Loss: 0.0621 | 0.0281
Epoch 24/300, resid Loss: 0.0619 | 0.0277
Epoch 25/300, resid Loss: 0.0617 | 0.0271
Epoch 26/300, resid Loss: 0.0615 | 0.0265
Epoch 27/300, resid Loss: 0.0613 | 0.0262
Epoch 28/300, resid Loss: 0.0611 | 0.0261
Epoch 29/300, resid Loss: 0.0610 | 0.0261
Epoch 30/300, resid Loss: 0.0608 | 0.0261
Epoch 31/300, resid Loss: 0.0607 | 0.0261
Epoch 32/300, resid Loss: 0.0606 | 0.0261
Epoch 33/300, resid Loss: 0.0605 | 0.0261
Epoch 34/300, resid Loss: 0.0604 | 0.0262
Epoch 35/300, resid Loss: 0.0603 | 0.0262
Epoch 36/300, resid Loss: 0.0602 | 0.0262
Epoch 37/300, resid Loss: 0.0602 | 0.0262
Epoch 38/300, resid Loss: 0.0601 | 0.0262
Epoch 39/300, resid Loss: 0.0601 | 0.0262
Epoch 40/300, resid Loss: 0.0601 | 0.0263
Epoch 41/300, resid Loss: 0.0600 | 0.0263
Epoch 42/300, resid Loss: 0.0600 | 0.0263
Epoch 43/300, resid Loss: 0.0600 | 0.0263
Epoch 44/300, resid Loss: 0.0600 | 0.0263
Epoch 45/300, resid Loss: 0.0599 | 0.0263
Epoch 46/300, resid Loss: 0.0599 | 0.0263
Epoch 47/300, resid Loss: 0.0599 | 0.0263
Epoch 48/300, resid Loss: 0.0599 | 0.0263
Epoch 49/300, resid Loss: 0.0599 | 0.0263
Epoch 50/300, resid Loss: 0.0599 | 0.0263
Epoch 51/300, resid Loss: 0.0599 | 0.0263
Epoch 52/300, resid Loss: 0.0599 | 0.0263
Epoch 53/300, resid Loss: 0.0598 | 0.0263
Epoch 54/300, resid Loss: 0.0598 | 0.0263
Epoch 55/300, resid Loss: 0.0598 | 0.0263
Epoch 56/300, resid Loss: 0.0598 | 0.0263
Epoch 57/300, resid Loss: 0.0598 | 0.0263
Epoch 58/300, resid Loss: 0.0598 | 0.0263
Epoch 59/300, resid Loss: 0.0598 | 0.0263
Epoch 60/300, resid Loss: 0.0598 | 0.0263
Epoch 61/300, resid Loss: 0.0598 | 0.0263
Epoch 62/300, resid Loss: 0.0598 | 0.0263
Epoch 63/300, resid Loss: 0.0598 | 0.0263
Epoch 64/300, resid Loss: 0.0598 | 0.0263
Epoch 65/300, resid Loss: 0.0598 | 0.0263
Epoch 66/300, resid Loss: 0.0598 | 0.0263
Epoch 67/300, resid Loss: 0.0598 | 0.0263
Epoch 68/300, resid Loss: 0.0598 | 0.0263
Epoch 69/300, resid Loss: 0.0598 | 0.0263
Epoch 70/300, resid Loss: 0.0598 | 0.0263
Epoch 71/300, resid Loss: 0.0598 | 0.0263
Epoch 72/300, resid Loss: 0.0598 | 0.0263
Epoch 73/300, resid Loss: 0.0598 | 0.0263
Epoch 74/300, resid Loss: 0.0598 | 0.0263
Epoch 75/300, resid Loss: 0.0598 | 0.0263
Epoch 76/300, resid Loss: 0.0598 | 0.0263
Epoch 77/300, resid Loss: 0.0598 | 0.0263
Epoch 78/300, resid Loss: 0.0598 | 0.0263
Epoch 79/300, resid Loss: 0.0598 | 0.0263
Epoch 80/300, resid Loss: 0.0598 | 0.0263
Epoch 81/300, resid Loss: 0.0598 | 0.0263
Epoch 82/300, resid Loss: 0.0598 | 0.0263
Epoch 83/300, resid Loss: 0.0598 | 0.0263
Epoch 84/300, resid Loss: 0.0598 | 0.0263
Epoch 85/300, resid Loss: 0.0598 | 0.0263
Epoch 86/300, resid Loss: 0.0598 | 0.0263
Epoch 87/300, resid Loss: 0.0598 | 0.0263
Epoch 88/300, resid Loss: 0.0598 | 0.0263
Epoch 89/300, resid Loss: 0.0598 | 0.0263
Epoch 90/300, resid Loss: 0.0598 | 0.0263
Epoch 91/300, resid Loss: 0.0598 | 0.0263
Epoch 92/300, resid Loss: 0.0598 | 0.0263
Epoch 93/300, resid Loss: 0.0598 | 0.0263
Epoch 94/300, resid Loss: 0.0598 | 0.0263
Epoch 95/300, resid Loss: 0.0598 | 0.0263
Epoch 96/300, resid Loss: 0.0598 | 0.0263
Epoch 97/300, resid Loss: 0.0598 | 0.0263
Epoch 98/300, resid Loss: 0.0598 | 0.0263
Epoch 99/300, resid Loss: 0.0598 | 0.0263
Epoch 100/300, resid Loss: 0.0598 | 0.0263
Epoch 101/300, resid Loss: 0.0598 | 0.0263
Epoch 102/300, resid Loss: 0.0598 | 0.0263
Epoch 103/300, resid Loss: 0.0598 | 0.0263
Epoch 104/300, resid Loss: 0.0598 | 0.0263
Epoch 105/300, resid Loss: 0.0598 | 0.0263
Epoch 106/300, resid Loss: 0.0598 | 0.0263
Epoch 107/300, resid Loss: 0.0598 | 0.0263
Epoch 108/300, resid Loss: 0.0598 | 0.0263
Epoch 109/300, resid Loss: 0.0598 | 0.0263
Epoch 110/300, resid Loss: 0.0598 | 0.0263
Epoch 111/300, resid Loss: 0.0598 | 0.0263
Epoch 112/300, resid Loss: 0.0598 | 0.0263
Epoch 113/300, resid Loss: 0.0598 | 0.0263
Epoch 114/300, resid Loss: 0.0598 | 0.0263
Epoch 115/300, resid Loss: 0.0598 | 0.0263
Epoch 116/300, resid Loss: 0.0598 | 0.0263
Epoch 117/300, resid Loss: 0.0598 | 0.0263
Epoch 118/300, resid Loss: 0.0598 | 0.0263
Epoch 119/300, resid Loss: 0.0598 | 0.0263
Epoch 120/300, resid Loss: 0.0598 | 0.0263
Epoch 121/300, resid Loss: 0.0598 | 0.0263
Epoch 122/300, resid Loss: 0.0598 | 0.0263
Epoch 123/300, resid Loss: 0.0598 | 0.0263
Epoch 124/300, resid Loss: 0.0598 | 0.0263
Epoch 125/300, resid Loss: 0.0598 | 0.0263
Epoch 126/300, resid Loss: 0.0598 | 0.0263
Epoch 127/300, resid Loss: 0.0598 | 0.0263
Epoch 128/300, resid Loss: 0.0598 | 0.0263
Epoch 129/300, resid Loss: 0.0598 | 0.0263
Epoch 130/300, resid Loss: 0.0598 | 0.0263
Epoch 131/300, resid Loss: 0.0598 | 0.0263
Epoch 132/300, resid Loss: 0.0598 | 0.0263
Epoch 133/300, resid Loss: 0.0598 | 0.0263
Early stopping for resid
Runtime (seconds): 1531.1635932922363
0.00015232656498093876
[210.14005]
[0.96932137]
[-4.2677965]
[3.2755537]
[0.9667125]
[9.396439]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 153.50527892098762
RMSE: 12.389724731445312
MAE: 12.389724731445312
R-squared: nan
[220.48027]
