[32m[I 2025-01-04 16:37:56,566][0m A new study created in memory with name: no-name-cb5bda49-c8fc-4a84-8eee-38c850db2eef[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-04 16:41:53,321][0m Trial 0 finished with value: 0.030494107234682957 and parameters: {'observation_period_num': 23, 'train_rates': 0.9083481569243887, 'learning_rate': 9.535585979982325e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.760614884698275}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:46:09,262][0m Trial 1 finished with value: 0.10928812250494957 and parameters: {'observation_period_num': 228, 'train_rates': 0.9875851716621392, 'learning_rate': 4.8284645064982604e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9100167986218339}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:47:24,147][0m Trial 2 finished with value: 0.07274005795461182 and parameters: {'observation_period_num': 64, 'train_rates': 0.685259011327263, 'learning_rate': 0.00020983430750014224, 'batch_size': 103, 'step_size': 1, 'gamma': 0.975753863650395}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:48:20,550][0m Trial 3 finished with value: 0.14641500335275306 and parameters: {'observation_period_num': 117, 'train_rates': 0.7180420812799686, 'learning_rate': 3.6684471289748394e-05, 'batch_size': 133, 'step_size': 2, 'gamma': 0.9710170224359288}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:49:07,785][0m Trial 4 finished with value: 0.20693411561888983 and parameters: {'observation_period_num': 142, 'train_rates': 0.6734459597695313, 'learning_rate': 2.8802131356185894e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.9857304725473686}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:50:13,837][0m Trial 5 finished with value: 0.4436217598477752 and parameters: {'observation_period_num': 50, 'train_rates': 0.612777019791186, 'learning_rate': 3.2889255244515107e-06, 'batch_size': 103, 'step_size': 4, 'gamma': 0.875553564771431}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:54:40,030][0m Trial 6 finished with value: 0.2705940663121467 and parameters: {'observation_period_num': 173, 'train_rates': 0.9847784695727759, 'learning_rate': 1.6943434892632635e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9085489360164941}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:55:25,814][0m Trial 7 finished with value: 0.6908962485156481 and parameters: {'observation_period_num': 191, 'train_rates': 0.7424974686624438, 'learning_rate': 3.5137322987237913e-06, 'batch_size': 150, 'step_size': 10, 'gamma': 0.8577755122365931}. Best is trial 0 with value: 0.030494107234682957.[0m
Early stopping at epoch 44
[32m[I 2025-01-04 16:55:54,923][0m Trial 8 finished with value: 1.1398479459874185 and parameters: {'observation_period_num': 164, 'train_rates': 0.690257435426078, 'learning_rate': 5.0975682307111564e-06, 'batch_size': 121, 'step_size': 1, 'gamma': 0.773845179490859}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:57:52,937][0m Trial 9 finished with value: 0.1329784298805814 and parameters: {'observation_period_num': 123, 'train_rates': 0.8656617609799708, 'learning_rate': 0.0007241441271840873, 'batch_size': 47, 'step_size': 6, 'gamma': 0.7862372740246856}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:58:47,932][0m Trial 10 finished with value: 0.03630556463115457 and parameters: {'observation_period_num': 10, 'train_rates': 0.8594937181100508, 'learning_rate': 0.00017874612907786566, 'batch_size': 196, 'step_size': 15, 'gamma': 0.8197371228616751}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 16:59:48,644][0m Trial 11 finished with value: 0.03542945675494135 and parameters: {'observation_period_num': 11, 'train_rates': 0.8686242126879095, 'learning_rate': 0.00016726851767883742, 'batch_size': 209, 'step_size': 15, 'gamma': 0.8109526447103379}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:00:42,160][0m Trial 12 finished with value: 0.047859185250701416 and parameters: {'observation_period_num': 5, 'train_rates': 0.8936722041108714, 'learning_rate': 0.00017304014804966214, 'batch_size': 248, 'step_size': 12, 'gamma': 0.7512103035852307}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:01:26,480][0m Trial 13 finished with value: 0.054127760344584074 and parameters: {'observation_period_num': 59, 'train_rates': 0.8063819433163347, 'learning_rate': 0.000689432493529695, 'batch_size': 177, 'step_size': 13, 'gamma': 0.8096420890649095}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:02:35,257][0m Trial 14 finished with value: 0.059740751966016706 and parameters: {'observation_period_num': 86, 'train_rates': 0.9174720472808277, 'learning_rate': 8.312100448814891e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8297160514096967}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:03:15,203][0m Trial 15 finished with value: 0.21988683704355455 and parameters: {'observation_period_num': 32, 'train_rates': 0.7954207474420977, 'learning_rate': 1.6187873330991298e-05, 'batch_size': 208, 'step_size': 14, 'gamma': 0.750360457665753}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:03:57,597][0m Trial 16 finished with value: 0.07255932688713074 and parameters: {'observation_period_num': 92, 'train_rates': 0.942496877692554, 'learning_rate': 0.0003877652529342646, 'batch_size': 165, 'step_size': 10, 'gamma': 0.7935179606880548}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:05:12,825][0m Trial 17 finished with value: 0.09931368020600975 and parameters: {'observation_period_num': 43, 'train_rates': 0.8235303706721313, 'learning_rate': 1.164285598313068e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8484965231892083}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:06:11,849][0m Trial 18 finished with value: 0.08454284374445876 and parameters: {'observation_period_num': 25, 'train_rates': 0.8473884588048256, 'learning_rate': 8.694350424262119e-05, 'batch_size': 213, 'step_size': 7, 'gamma': 0.77195990944728}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:07:06,550][0m Trial 19 finished with value: 0.12298386543989182 and parameters: {'observation_period_num': 85, 'train_rates': 0.9380657332055774, 'learning_rate': 7.70848898785119e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8026894994046191}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:07:57,690][0m Trial 20 finished with value: 0.14958338289515777 and parameters: {'observation_period_num': 234, 'train_rates': 0.7658853589983293, 'learning_rate': 0.00034186603106875176, 'batch_size': 172, 'step_size': 11, 'gamma': 0.8376842180569215}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:08:55,248][0m Trial 21 finished with value: 0.03652027930963684 and parameters: {'observation_period_num': 11, 'train_rates': 0.8742341344891429, 'learning_rate': 0.00019977059548080892, 'batch_size': 194, 'step_size': 15, 'gamma': 0.8245080795325169}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:09:52,036][0m Trial 22 finished with value: 0.03942133062136122 and parameters: {'observation_period_num': 23, 'train_rates': 0.842298234978976, 'learning_rate': 0.00017548910550863617, 'batch_size': 214, 'step_size': 15, 'gamma': 0.8794561848156972}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:10:49,098][0m Trial 23 finished with value: 0.07386203472103392 and parameters: {'observation_period_num': 66, 'train_rates': 0.9031884851804, 'learning_rate': 0.00012234663561140437, 'batch_size': 192, 'step_size': 13, 'gamma': 0.8162263284753383}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:11:45,730][0m Trial 24 finished with value: 0.04495691880583763 and parameters: {'observation_period_num': 5, 'train_rates': 0.9487092581135452, 'learning_rate': 0.0003039611730593024, 'batch_size': 235, 'step_size': 13, 'gamma': 0.7773305363864141}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:12:40,016][0m Trial 25 finished with value: 0.05943518012157969 and parameters: {'observation_period_num': 36, 'train_rates': 0.8732214096845892, 'learning_rate': 5.267867644403611e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.7639409907989608}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:13:27,172][0m Trial 26 finished with value: 0.03465049385237015 and parameters: {'observation_period_num': 23, 'train_rates': 0.8322309650190792, 'learning_rate': 0.00048186605802429144, 'batch_size': 190, 'step_size': 15, 'gamma': 0.7970292782733032}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:14:09,394][0m Trial 27 finished with value: 0.06880266543856521 and parameters: {'observation_period_num': 105, 'train_rates': 0.7685528453364276, 'learning_rate': 0.0007286065752275237, 'batch_size': 226, 'step_size': 11, 'gamma': 0.795241024349872}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:15:12,358][0m Trial 28 finished with value: 0.06156531460944888 and parameters: {'observation_period_num': 70, 'train_rates': 0.8214047088883234, 'learning_rate': 0.0009634578551252528, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8909816183918798}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:17:49,716][0m Trial 29 finished with value: 0.0778103382029432 and parameters: {'observation_period_num': 38, 'train_rates': 0.9679692128058257, 'learning_rate': 0.0004954181125129785, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8483812937396061}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:19:15,036][0m Trial 30 finished with value: 0.17735453711047194 and parameters: {'observation_period_num': 251, 'train_rates': 0.9138073601936673, 'learning_rate': 1.821875184164928e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9405029314893318}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:20:12,274][0m Trial 31 finished with value: 0.039112040553420914 and parameters: {'observation_period_num': 18, 'train_rates': 0.8441188921215278, 'learning_rate': 0.00011605536710753388, 'batch_size': 188, 'step_size': 15, 'gamma': 0.8064396454742092}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:20:59,328][0m Trial 32 finished with value: 0.04013420568487371 and parameters: {'observation_period_num': 50, 'train_rates': 0.8856687162085302, 'learning_rate': 0.0002741603483922879, 'batch_size': 157, 'step_size': 15, 'gamma': 0.8171131495324592}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:21:47,830][0m Trial 33 finished with value: 0.07780733391426611 and parameters: {'observation_period_num': 20, 'train_rates': 0.8589982734316981, 'learning_rate': 5.0197978534741274e-05, 'batch_size': 202, 'step_size': 13, 'gamma': 0.7851810256957102}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:22:32,448][0m Trial 34 finished with value: 0.062102799572176855 and parameters: {'observation_period_num': 74, 'train_rates': 0.774389653819733, 'learning_rate': 0.0001308174927163909, 'batch_size': 133, 'step_size': 14, 'gamma': 0.7631696165542795}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:23:18,493][0m Trial 35 finished with value: 0.05304793785104439 and parameters: {'observation_period_num': 52, 'train_rates': 0.8262379587062827, 'learning_rate': 0.0004459420015946235, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8355932810413319}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:24:06,900][0m Trial 36 finished with value: 0.2519321452487599 and parameters: {'observation_period_num': 143, 'train_rates': 0.9219460048992215, 'learning_rate': 2.889642527370177e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.8612157193496123}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:27:30,046][0m Trial 37 finished with value: 0.11210272042647652 and parameters: {'observation_period_num': 34, 'train_rates': 0.60592767568942, 'learning_rate': 0.00021924588371746462, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7985566865856063}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:28:39,392][0m Trial 38 finished with value: 0.08025625276138477 and parameters: {'observation_period_num': 17, 'train_rates': 0.8908101977385625, 'learning_rate': 7.149491203540417e-05, 'batch_size': 124, 'step_size': 3, 'gamma': 0.820904124902291}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:29:24,331][0m Trial 39 finished with value: 0.2750671410012519 and parameters: {'observation_period_num': 193, 'train_rates': 0.7160379394464644, 'learning_rate': 0.0005300235024116749, 'batch_size': 240, 'step_size': 14, 'gamma': 0.947806343580026}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:30:18,758][0m Trial 40 finished with value: 0.18156477808952332 and parameters: {'observation_period_num': 49, 'train_rates': 0.9717017870294554, 'learning_rate': 3.842662372884979e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.7832666758400311}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:31:21,242][0m Trial 41 finished with value: 0.033683631131744954 and parameters: {'observation_period_num': 8, 'train_rates': 0.8713399719087944, 'learning_rate': 0.00024802698151026937, 'batch_size': 199, 'step_size': 15, 'gamma': 0.8243363373345849}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:32:20,615][0m Trial 42 finished with value: 0.03761588595807552 and parameters: {'observation_period_num': 27, 'train_rates': 0.8627169521847253, 'learning_rate': 0.00025021024171051077, 'batch_size': 201, 'step_size': 15, 'gamma': 0.8444452349976166}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:33:15,135][0m Trial 43 finished with value: 0.04179675295336606 and parameters: {'observation_period_num': 8, 'train_rates': 0.8018099132420822, 'learning_rate': 0.0001441052778865064, 'batch_size': 224, 'step_size': 15, 'gamma': 0.7645534092544413}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:33:59,300][0m Trial 44 finished with value: 0.04210115120481656 and parameters: {'observation_period_num': 15, 'train_rates': 0.6498993562386006, 'learning_rate': 0.0002929490839550993, 'batch_size': 178, 'step_size': 14, 'gamma': 0.7917025837311337}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:34:55,925][0m Trial 45 finished with value: 0.06979652549299968 and parameters: {'observation_period_num': 60, 'train_rates': 0.8833161769391658, 'learning_rate': 0.00010724580026682393, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8125850494706158}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:35:57,592][0m Trial 46 finished with value: 0.042072304990142584 and parameters: {'observation_period_num': 29, 'train_rates': 0.9042032655985415, 'learning_rate': 0.00018531137892258882, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8650459704354748}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:36:44,940][0m Trial 47 finished with value: 0.05164084631281021 and parameters: {'observation_period_num': 38, 'train_rates': 0.8394340524841155, 'learning_rate': 0.00038608214660734366, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8289319675118689}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:37:22,847][0m Trial 48 finished with value: 0.7284226786677738 and parameters: {'observation_period_num': 5, 'train_rates': 0.8137720602638314, 'learning_rate': 1.2281605310710724e-06, 'batch_size': 245, 'step_size': 14, 'gamma': 0.7565659231709034}. Best is trial 0 with value: 0.030494107234682957.[0m
[32m[I 2025-01-04 17:38:13,185][0m Trial 49 finished with value: 0.05760507297876117 and parameters: {'observation_period_num': 48, 'train_rates': 0.7804967026538202, 'learning_rate': 6.317327736437898e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.7773331224356869}. Best is trial 0 with value: 0.030494107234682957.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AAPL_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2015 | 0.1589
Epoch 2/300, Loss: 0.1286 | 0.1332
Epoch 3/300, Loss: 0.1139 | 0.1121
Epoch 4/300, Loss: 0.1069 | 0.0948
Epoch 5/300, Loss: 0.1010 | 0.0868
Epoch 6/300, Loss: 0.0966 | 0.0837
Epoch 7/300, Loss: 0.0919 | 0.0826
Epoch 8/300, Loss: 0.0866 | 0.0686
Epoch 9/300, Loss: 0.0846 | 0.0658
Epoch 10/300, Loss: 0.0845 | 0.0548
Epoch 11/300, Loss: 0.0839 | 0.0503
Epoch 12/300, Loss: 0.0811 | 0.0518
Epoch 13/300, Loss: 0.0786 | 0.0552
Epoch 14/300, Loss: 0.0767 | 0.0549
Epoch 15/300, Loss: 0.0758 | 0.0552
Epoch 16/300, Loss: 0.0749 | 0.0541
Epoch 17/300, Loss: 0.0743 | 0.0525
Epoch 18/300, Loss: 0.0736 | 0.0509
Epoch 19/300, Loss: 0.0728 | 0.0493
Epoch 20/300, Loss: 0.0719 | 0.0478
Epoch 21/300, Loss: 0.0704 | 0.0462
Epoch 22/300, Loss: 0.0698 | 0.0454
Epoch 23/300, Loss: 0.0692 | 0.0444
Epoch 24/300, Loss: 0.0686 | 0.0434
Epoch 25/300, Loss: 0.0679 | 0.0425
Epoch 26/300, Loss: 0.0673 | 0.0417
Epoch 27/300, Loss: 0.0664 | 0.0399
Epoch 28/300, Loss: 0.0662 | 0.0393
Epoch 29/300, Loss: 0.0659 | 0.0388
Epoch 30/300, Loss: 0.0655 | 0.0383
Epoch 31/300, Loss: 0.0651 | 0.0378
Epoch 32/300, Loss: 0.0647 | 0.0373
Epoch 33/300, Loss: 0.0642 | 0.0368
Epoch 34/300, Loss: 0.0637 | 0.0372
Epoch 35/300, Loss: 0.0634 | 0.0366
Epoch 36/300, Loss: 0.0630 | 0.0362
Epoch 37/300, Loss: 0.0627 | 0.0358
Epoch 38/300, Loss: 0.0624 | 0.0355
Epoch 39/300, Loss: 0.0621 | 0.0352
Epoch 40/300, Loss: 0.0617 | 0.0356
Epoch 41/300, Loss: 0.0614 | 0.0351
Epoch 42/300, Loss: 0.0612 | 0.0347
Epoch 43/300, Loss: 0.0609 | 0.0344
Epoch 44/300, Loss: 0.0607 | 0.0341
Epoch 45/300, Loss: 0.0604 | 0.0338
Epoch 46/300, Loss: 0.0602 | 0.0336
Epoch 47/300, Loss: 0.0599 | 0.0342
Epoch 48/300, Loss: 0.0598 | 0.0341
Epoch 49/300, Loss: 0.0596 | 0.0339
Epoch 50/300, Loss: 0.0595 | 0.0336
Epoch 51/300, Loss: 0.0593 | 0.0334
Epoch 52/300, Loss: 0.0591 | 0.0332
Epoch 53/300, Loss: 0.0589 | 0.0335
Epoch 54/300, Loss: 0.0589 | 0.0334
Epoch 55/300, Loss: 0.0587 | 0.0332
Epoch 56/300, Loss: 0.0586 | 0.0329
Epoch 57/300, Loss: 0.0585 | 0.0326
Epoch 58/300, Loss: 0.0583 | 0.0323
Epoch 59/300, Loss: 0.0582 | 0.0321
Epoch 60/300, Loss: 0.0581 | 0.0319
Epoch 61/300, Loss: 0.0579 | 0.0318
Epoch 62/300, Loss: 0.0578 | 0.0318
Epoch 63/300, Loss: 0.0577 | 0.0318
Epoch 64/300, Loss: 0.0576 | 0.0318
Epoch 65/300, Loss: 0.0575 | 0.0318
Epoch 66/300, Loss: 0.0575 | 0.0320
Epoch 67/300, Loss: 0.0574 | 0.0320
Epoch 68/300, Loss: 0.0573 | 0.0320
Epoch 69/300, Loss: 0.0572 | 0.0319
Epoch 70/300, Loss: 0.0571 | 0.0319
Epoch 71/300, Loss: 0.0570 | 0.0319
Epoch 72/300, Loss: 0.0569 | 0.0318
Epoch 73/300, Loss: 0.0568 | 0.0319
Epoch 74/300, Loss: 0.0568 | 0.0319
Epoch 75/300, Loss: 0.0567 | 0.0319
Epoch 76/300, Loss: 0.0567 | 0.0318
Epoch 77/300, Loss: 0.0566 | 0.0318
Epoch 78/300, Loss: 0.0565 | 0.0318
Epoch 79/300, Loss: 0.0565 | 0.0320
Epoch 80/300, Loss: 0.0564 | 0.0320
Epoch 81/300, Loss: 0.0564 | 0.0319
Epoch 82/300, Loss: 0.0563 | 0.0319
Epoch 83/300, Loss: 0.0563 | 0.0319
Epoch 84/300, Loss: 0.0562 | 0.0319
Epoch 85/300, Loss: 0.0562 | 0.0318
Epoch 86/300, Loss: 0.0561 | 0.0323
Epoch 87/300, Loss: 0.0561 | 0.0323
Epoch 88/300, Loss: 0.0560 | 0.0323
Epoch 89/300, Loss: 0.0560 | 0.0323
Epoch 90/300, Loss: 0.0559 | 0.0322
Epoch 91/300, Loss: 0.0559 | 0.0322
Epoch 92/300, Loss: 0.0559 | 0.0325
Epoch 93/300, Loss: 0.0558 | 0.0324
Epoch 94/300, Loss: 0.0558 | 0.0324
Epoch 95/300, Loss: 0.0558 | 0.0323
Epoch 96/300, Loss: 0.0557 | 0.0323
Epoch 97/300, Loss: 0.0557 | 0.0323
Epoch 98/300, Loss: 0.0557 | 0.0322
Epoch 99/300, Loss: 0.0557 | 0.0319
Epoch 100/300, Loss: 0.0556 | 0.0319
Epoch 101/300, Loss: 0.0556 | 0.0319
Epoch 102/300, Loss: 0.0556 | 0.0319
Epoch 103/300, Loss: 0.0556 | 0.0319
Epoch 104/300, Loss: 0.0555 | 0.0319
Epoch 105/300, Loss: 0.0555 | 0.0315
Epoch 106/300, Loss: 0.0555 | 0.0315
Epoch 107/300, Loss: 0.0555 | 0.0316
Epoch 108/300, Loss: 0.0554 | 0.0316
Epoch 109/300, Loss: 0.0554 | 0.0316
Epoch 110/300, Loss: 0.0554 | 0.0316
Epoch 111/300, Loss: 0.0554 | 0.0316
Epoch 112/300, Loss: 0.0554 | 0.0314
Epoch 113/300, Loss: 0.0553 | 0.0315
Epoch 114/300, Loss: 0.0553 | 0.0315
Epoch 115/300, Loss: 0.0553 | 0.0315
Epoch 116/300, Loss: 0.0553 | 0.0315
Epoch 117/300, Loss: 0.0553 | 0.0315
Epoch 118/300, Loss: 0.0553 | 0.0314
Epoch 119/300, Loss: 0.0552 | 0.0314
Epoch 120/300, Loss: 0.0552 | 0.0314
Epoch 121/300, Loss: 0.0552 | 0.0314
Epoch 122/300, Loss: 0.0552 | 0.0314
Epoch 123/300, Loss: 0.0552 | 0.0314
Epoch 124/300, Loss: 0.0552 | 0.0314
Epoch 125/300, Loss: 0.0552 | 0.0314
Epoch 126/300, Loss: 0.0552 | 0.0314
Epoch 127/300, Loss: 0.0552 | 0.0314
Epoch 128/300, Loss: 0.0552 | 0.0314
Epoch 129/300, Loss: 0.0552 | 0.0314
Epoch 130/300, Loss: 0.0551 | 0.0314
Epoch 131/300, Loss: 0.0551 | 0.0314
Epoch 132/300, Loss: 0.0551 | 0.0314
Epoch 133/300, Loss: 0.0551 | 0.0314
Epoch 134/300, Loss: 0.0551 | 0.0314
Epoch 135/300, Loss: 0.0551 | 0.0314
Epoch 136/300, Loss: 0.0551 | 0.0314
Epoch 137/300, Loss: 0.0551 | 0.0314
Epoch 138/300, Loss: 0.0551 | 0.0314
Epoch 139/300, Loss: 0.0551 | 0.0314
Epoch 140/300, Loss: 0.0551 | 0.0314
Epoch 141/300, Loss: 0.0551 | 0.0314
Epoch 142/300, Loss: 0.0551 | 0.0314
Epoch 143/300, Loss: 0.0551 | 0.0314
Epoch 144/300, Loss: 0.0551 | 0.0314
Epoch 145/300, Loss: 0.0550 | 0.0314
Epoch 146/300, Loss: 0.0550 | 0.0314
Epoch 147/300, Loss: 0.0550 | 0.0314
Epoch 148/300, Loss: 0.0550 | 0.0314
Epoch 149/300, Loss: 0.0550 | 0.0314
Epoch 150/300, Loss: 0.0550 | 0.0314
Epoch 151/300, Loss: 0.0550 | 0.0314
Epoch 152/300, Loss: 0.0550 | 0.0314
Epoch 153/300, Loss: 0.0550 | 0.0314
Epoch 154/300, Loss: 0.0550 | 0.0314
Epoch 155/300, Loss: 0.0550 | 0.0314
Epoch 156/300, Loss: 0.0550 | 0.0314
Epoch 157/300, Loss: 0.0550 | 0.0314
Epoch 158/300, Loss: 0.0550 | 0.0313
Epoch 159/300, Loss: 0.0550 | 0.0313
Epoch 160/300, Loss: 0.0550 | 0.0313
Epoch 161/300, Loss: 0.0550 | 0.0313
Epoch 162/300, Loss: 0.0550 | 0.0313
Epoch 163/300, Loss: 0.0550 | 0.0313
Epoch 164/300, Loss: 0.0550 | 0.0313
Epoch 165/300, Loss: 0.0550 | 0.0313
Epoch 166/300, Loss: 0.0550 | 0.0313
Epoch 167/300, Loss: 0.0550 | 0.0313
Epoch 168/300, Loss: 0.0550 | 0.0313
Epoch 169/300, Loss: 0.0550 | 0.0313
Epoch 170/300, Loss: 0.0550 | 0.0313
Epoch 171/300, Loss: 0.0550 | 0.0313
Epoch 172/300, Loss: 0.0550 | 0.0313
Epoch 173/300, Loss: 0.0550 | 0.0313
Epoch 174/300, Loss: 0.0550 | 0.0313
Epoch 175/300, Loss: 0.0550 | 0.0313
Epoch 176/300, Loss: 0.0550 | 0.0313
Epoch 177/300, Loss: 0.0550 | 0.0313
Epoch 178/300, Loss: 0.0550 | 0.0313
Epoch 179/300, Loss: 0.0550 | 0.0313
Epoch 180/300, Loss: 0.0550 | 0.0313
Epoch 181/300, Loss: 0.0550 | 0.0313
Epoch 182/300, Loss: 0.0550 | 0.0313
Epoch 183/300, Loss: 0.0550 | 0.0313
Epoch 184/300, Loss: 0.0550 | 0.0313
Epoch 185/300, Loss: 0.0550 | 0.0313
Epoch 186/300, Loss: 0.0550 | 0.0313
Epoch 187/300, Loss: 0.0550 | 0.0313
Epoch 188/300, Loss: 0.0550 | 0.0313
Epoch 189/300, Loss: 0.0550 | 0.0313
Epoch 190/300, Loss: 0.0550 | 0.0313
Epoch 191/300, Loss: 0.0550 | 0.0313
Epoch 192/300, Loss: 0.0550 | 0.0313
Epoch 193/300, Loss: 0.0550 | 0.0313
Epoch 194/300, Loss: 0.0550 | 0.0313
Epoch 195/300, Loss: 0.0550 | 0.0313
Epoch 196/300, Loss: 0.0550 | 0.0313
Epoch 197/300, Loss: 0.0550 | 0.0313
Epoch 198/300, Loss: 0.0550 | 0.0313
Epoch 199/300, Loss: 0.0550 | 0.0313
Epoch 200/300, Loss: 0.0550 | 0.0313
Epoch 201/300, Loss: 0.0550 | 0.0313
Epoch 202/300, Loss: 0.0550 | 0.0313
Epoch 203/300, Loss: 0.0550 | 0.0313
Epoch 204/300, Loss: 0.0550 | 0.0313
Epoch 205/300, Loss: 0.0550 | 0.0313
Epoch 206/300, Loss: 0.0550 | 0.0313
Epoch 207/300, Loss: 0.0550 | 0.0313
Epoch 208/300, Loss: 0.0550 | 0.0313
Epoch 209/300, Loss: 0.0550 | 0.0313
Epoch 210/300, Loss: 0.0550 | 0.0313
Epoch 211/300, Loss: 0.0550 | 0.0313
Epoch 212/300, Loss: 0.0550 | 0.0313
Epoch 213/300, Loss: 0.0550 | 0.0313
Epoch 214/300, Loss: 0.0550 | 0.0313
Epoch 215/300, Loss: 0.0550 | 0.0313
Epoch 216/300, Loss: 0.0550 | 0.0313
Epoch 217/300, Loss: 0.0550 | 0.0313
Epoch 218/300, Loss: 0.0550 | 0.0313
Epoch 219/300, Loss: 0.0550 | 0.0313
Epoch 220/300, Loss: 0.0550 | 0.0313
Epoch 221/300, Loss: 0.0550 | 0.0313
Epoch 222/300, Loss: 0.0550 | 0.0313
Epoch 223/300, Loss: 0.0550 | 0.0313
Epoch 224/300, Loss: 0.0550 | 0.0313
Epoch 225/300, Loss: 0.0550 | 0.0313
Epoch 226/300, Loss: 0.0550 | 0.0313
Epoch 227/300, Loss: 0.0550 | 0.0313
Epoch 228/300, Loss: 0.0550 | 0.0313
Epoch 229/300, Loss: 0.0550 | 0.0313
Epoch 230/300, Loss: 0.0550 | 0.0313
Epoch 231/300, Loss: 0.0550 | 0.0313
Epoch 232/300, Loss: 0.0550 | 0.0313
Epoch 233/300, Loss: 0.0550 | 0.0313
Epoch 234/300, Loss: 0.0550 | 0.0313
Epoch 235/300, Loss: 0.0550 | 0.0313
Epoch 236/300, Loss: 0.0550 | 0.0313
Epoch 237/300, Loss: 0.0550 | 0.0313
Epoch 238/300, Loss: 0.0550 | 0.0313
Epoch 239/300, Loss: 0.0550 | 0.0313
Epoch 240/300, Loss: 0.0550 | 0.0313
Epoch 241/300, Loss: 0.0550 | 0.0313
Epoch 242/300, Loss: 0.0550 | 0.0313
Epoch 243/300, Loss: 0.0550 | 0.0313
Epoch 244/300, Loss: 0.0550 | 0.0313
Epoch 245/300, Loss: 0.0550 | 0.0313
Epoch 246/300, Loss: 0.0550 | 0.0313
Epoch 247/300, Loss: 0.0550 | 0.0313
Epoch 248/300, Loss: 0.0550 | 0.0313
Epoch 249/300, Loss: 0.0550 | 0.0313
Epoch 250/300, Loss: 0.0550 | 0.0313
Epoch 251/300, Loss: 0.0550 | 0.0313
Epoch 252/300, Loss: 0.0550 | 0.0313
Epoch 253/300, Loss: 0.0550 | 0.0313
Epoch 254/300, Loss: 0.0550 | 0.0313
Epoch 255/300, Loss: 0.0550 | 0.0313
Epoch 256/300, Loss: 0.0550 | 0.0313
Epoch 257/300, Loss: 0.0550 | 0.0313
Epoch 258/300, Loss: 0.0550 | 0.0313
Epoch 259/300, Loss: 0.0550 | 0.0313
Epoch 260/300, Loss: 0.0550 | 0.0313
Epoch 261/300, Loss: 0.0550 | 0.0313
Epoch 262/300, Loss: 0.0550 | 0.0313
Epoch 263/300, Loss: 0.0550 | 0.0313
Epoch 264/300, Loss: 0.0550 | 0.0313
Epoch 265/300, Loss: 0.0550 | 0.0313
Epoch 266/300, Loss: 0.0550 | 0.0313
Epoch 267/300, Loss: 0.0550 | 0.0313
Epoch 268/300, Loss: 0.0550 | 0.0313
Epoch 269/300, Loss: 0.0550 | 0.0313
Epoch 270/300, Loss: 0.0550 | 0.0313
Epoch 271/300, Loss: 0.0550 | 0.0313
Epoch 272/300, Loss: 0.0550 | 0.0313
Epoch 273/300, Loss: 0.0550 | 0.0313
Epoch 274/300, Loss: 0.0550 | 0.0313
Epoch 275/300, Loss: 0.0550 | 0.0313
Epoch 276/300, Loss: 0.0550 | 0.0313
Epoch 277/300, Loss: 0.0550 | 0.0313
Epoch 278/300, Loss: 0.0550 | 0.0313
Epoch 279/300, Loss: 0.0550 | 0.0313
Epoch 280/300, Loss: 0.0550 | 0.0313
Epoch 281/300, Loss: 0.0550 | 0.0313
Epoch 282/300, Loss: 0.0550 | 0.0313
Epoch 283/300, Loss: 0.0550 | 0.0313
Epoch 284/300, Loss: 0.0550 | 0.0313
Epoch 285/300, Loss: 0.0550 | 0.0313
Epoch 286/300, Loss: 0.0550 | 0.0313
Epoch 287/300, Loss: 0.0550 | 0.0313
Epoch 288/300, Loss: 0.0550 | 0.0313
Epoch 289/300, Loss: 0.0550 | 0.0313
Epoch 290/300, Loss: 0.0550 | 0.0313
Epoch 291/300, Loss: 0.0550 | 0.0313
Epoch 292/300, Loss: 0.0550 | 0.0313
Epoch 293/300, Loss: 0.0550 | 0.0313
Epoch 294/300, Loss: 0.0550 | 0.0313
Epoch 295/300, Loss: 0.0550 | 0.0313
Epoch 296/300, Loss: 0.0550 | 0.0313
Epoch 297/300, Loss: 0.0550 | 0.0313
Epoch 298/300, Loss: 0.0550 | 0.0313
Epoch 299/300, Loss: 0.0550 | 0.0313
Epoch 300/300, Loss: 0.0550 | 0.0313
Runtime (seconds): 672.9441504478455
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 18.447009271243587
RMSE: 4.2949981689453125
MAE: 4.2949981689453125
R-squared: nan
[225.575]
