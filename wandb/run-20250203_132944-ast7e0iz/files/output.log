[32m[I 2025-02-03 13:29:49,169][0m A new study created in memory with name: no-name-43e3b537-f8bc-4579-9d61-0acc0af00828[0m
[32m[I 2025-02-03 13:32:07,196][0m Trial 0 finished with value: 0.046961719670127446 and parameters: {'observation_period_num': 54, 'train_rates': 0.8269526469618095, 'learning_rate': 6.885114632184662e-05, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9072488277240108}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:33:02,725][0m Trial 1 finished with value: 0.4134016213505073 and parameters: {'observation_period_num': 60, 'train_rates': 0.7850118733257855, 'learning_rate': 3.1020112032507355e-06, 'batch_size': 97, 'step_size': 4, 'gamma': 0.771002688981868}. Best is trial 0 with value: 0.046961719670127446.[0m
Early stopping at epoch 60
[32m[I 2025-02-03 13:33:18,470][0m Trial 2 finished with value: 0.30957255274417694 and parameters: {'observation_period_num': 237, 'train_rates': 0.7871559671229922, 'learning_rate': 5.209870607177096e-05, 'batch_size': 227, 'step_size': 1, 'gamma': 0.8150394536260377}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:33:53,470][0m Trial 3 finished with value: 0.04773379992177744 and parameters: {'observation_period_num': 67, 'train_rates': 0.6378783245503534, 'learning_rate': 0.0008765386829210175, 'batch_size': 137, 'step_size': 6, 'gamma': 0.9174248017882249}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:34:21,873][0m Trial 4 finished with value: 0.10255946561131278 and parameters: {'observation_period_num': 156, 'train_rates': 0.8205614603133305, 'learning_rate': 0.0003039198406535159, 'batch_size': 197, 'step_size': 2, 'gamma': 0.8013192870501787}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:35:31,710][0m Trial 5 finished with value: 0.06218547852295439 and parameters: {'observation_period_num': 182, 'train_rates': 0.8593067378339985, 'learning_rate': 9.201717383486692e-05, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8871106041488721}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:35:57,177][0m Trial 6 finished with value: 0.048410934377724014 and parameters: {'observation_period_num': 106, 'train_rates': 0.8756550997769044, 'learning_rate': 0.00040803157241682574, 'batch_size': 239, 'step_size': 7, 'gamma': 0.7951614667917472}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:36:26,418][0m Trial 7 finished with value: 0.4147908688556029 and parameters: {'observation_period_num': 223, 'train_rates': 0.8873389199036643, 'learning_rate': 4.761748293289697e-06, 'batch_size': 187, 'step_size': 9, 'gamma': 0.8829039452444666}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:36:53,924][0m Trial 8 finished with value: 0.5715841021974264 and parameters: {'observation_period_num': 67, 'train_rates': 0.7999333094925145, 'learning_rate': 1.1180024566954847e-06, 'batch_size': 199, 'step_size': 8, 'gamma': 0.8192994307531182}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:37:31,167][0m Trial 9 finished with value: 0.06271973716956313 and parameters: {'observation_period_num': 48, 'train_rates': 0.9041558699255435, 'learning_rate': 3.529434512394153e-05, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8245108691756715}. Best is trial 0 with value: 0.046961719670127446.[0m
[32m[I 2025-02-03 13:42:48,048][0m Trial 10 finished with value: 0.03671373263181698 and parameters: {'observation_period_num': 6, 'train_rates': 0.9790613172725132, 'learning_rate': 0.00013554347603526688, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9889944436650495}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:48:20,059][0m Trial 11 finished with value: 0.05237301366160745 and parameters: {'observation_period_num': 16, 'train_rates': 0.9846007836114217, 'learning_rate': 0.00014896106539219489, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9891969589449575}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:51:31,973][0m Trial 12 finished with value: 0.03811923830399235 and parameters: {'observation_period_num': 5, 'train_rates': 0.686280676760915, 'learning_rate': 1.4163949098532443e-05, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9866840699808688}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:52:42,799][0m Trial 13 finished with value: 0.05456769963238756 and parameters: {'observation_period_num': 11, 'train_rates': 0.6836703010413653, 'learning_rate': 1.1787331009579073e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9822159259436953}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:55:52,164][0m Trial 14 finished with value: 0.06634412266011608 and parameters: {'observation_period_num': 114, 'train_rates': 0.714347049124181, 'learning_rate': 1.762367131002259e-05, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9496151395478039}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:56:39,715][0m Trial 15 finished with value: 0.08361688095514905 and parameters: {'observation_period_num': 5, 'train_rates': 0.6044155051526758, 'learning_rate': 1.6124776301754786e-05, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9541764697555729}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:58:29,943][0m Trial 16 finished with value: 0.10204610228538513 and parameters: {'observation_period_num': 90, 'train_rates': 0.9825152712177774, 'learning_rate': 7.063191633862948e-06, 'batch_size': 54, 'step_size': 10, 'gamma': 0.9488324908615143}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 13:59:16,179][0m Trial 17 finished with value: 0.039375953731082734 and parameters: {'observation_period_num': 32, 'train_rates': 0.727214378148612, 'learning_rate': 0.0001755017112373363, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8508417773195347}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:01:11,939][0m Trial 18 finished with value: 0.20134541972891076 and parameters: {'observation_period_num': 145, 'train_rates': 0.9453788283211939, 'learning_rate': 1.3248232369376153e-06, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9699520140004038}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:04:48,451][0m Trial 19 finished with value: 0.14634006023301896 and parameters: {'observation_period_num': 194, 'train_rates': 0.7437418716907831, 'learning_rate': 2.7884689326027925e-05, 'batch_size': 21, 'step_size': 13, 'gamma': 0.922941928981498}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:05:24,494][0m Trial 20 finished with value: 0.048338769269841056 and parameters: {'observation_period_num': 86, 'train_rates': 0.6710891595254814, 'learning_rate': 0.0008840247406797448, 'batch_size': 135, 'step_size': 14, 'gamma': 0.7505908385024963}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:06:12,346][0m Trial 21 finished with value: 0.040705098804585504 and parameters: {'observation_period_num': 31, 'train_rates': 0.7474945478345127, 'learning_rate': 0.00014088917031755953, 'batch_size': 106, 'step_size': 15, 'gamma': 0.844454013390507}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:07:22,440][0m Trial 22 finished with value: 0.037413120931707136 and parameters: {'observation_period_num': 32, 'train_rates': 0.7105777526367019, 'learning_rate': 0.00030122580735003813, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8636142637363082}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:08:29,635][0m Trial 23 finished with value: 0.03890308669204684 and parameters: {'observation_period_num': 36, 'train_rates': 0.6811205722337519, 'learning_rate': 0.00048479562080262355, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8615723481385107}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:10:33,342][0m Trial 24 finished with value: 0.05201345893655908 and parameters: {'observation_period_num': 23, 'train_rates': 0.6405321240520861, 'learning_rate': 0.00021203280752718715, 'batch_size': 36, 'step_size': 14, 'gamma': 0.8993243419558734}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:12:02,165][0m Trial 25 finished with value: 0.04967698824288868 and parameters: {'observation_period_num': 44, 'train_rates': 0.7109733088715918, 'learning_rate': 4.78677842063246e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9397714897950096}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:13:09,510][0m Trial 26 finished with value: 0.05316465074040133 and parameters: {'observation_period_num': 5, 'train_rates': 0.7619507675846402, 'learning_rate': 0.00010155929662476405, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9676625999637867}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:15:34,861][0m Trial 27 finished with value: 0.054652563027911265 and parameters: {'observation_period_num': 84, 'train_rates': 0.9386531426489396, 'learning_rate': 0.00024065969640403522, 'batch_size': 39, 'step_size': 11, 'gamma': 0.8679727736918721}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:19:41,088][0m Trial 28 finished with value: 0.04146312671603963 and parameters: {'observation_period_num': 24, 'train_rates': 0.6060665827138758, 'learning_rate': 0.0004934247746556548, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9230674695974472}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:21:57,735][0m Trial 29 finished with value: 0.0447764142196711 and parameters: {'observation_period_num': 54, 'train_rates': 0.8350042391706949, 'learning_rate': 6.614195767190679e-05, 'batch_size': 38, 'step_size': 15, 'gamma': 0.9025659556430173}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:23:11,113][0m Trial 30 finished with value: 0.13431632625205175 and parameters: {'observation_period_num': 130, 'train_rates': 0.6533867566419872, 'learning_rate': 2.655230769176286e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9345525183827376}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:24:10,262][0m Trial 31 finished with value: 0.05933012611577104 and parameters: {'observation_period_num': 33, 'train_rates': 0.687530766636785, 'learning_rate': 0.0005294691737431969, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8601758772034808}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:26:24,462][0m Trial 32 finished with value: 0.057811073555809556 and parameters: {'observation_period_num': 42, 'train_rates': 0.6861821763903958, 'learning_rate': 0.00034018693509966266, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8833533689243726}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:27:20,868][0m Trial 33 finished with value: 0.052187886595264237 and parameters: {'observation_period_num': 63, 'train_rates': 0.7102660818307306, 'learning_rate': 0.0006810540046136876, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8452282198085728}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:27:59,275][0m Trial 34 finished with value: 0.1898872129150204 and parameters: {'observation_period_num': 19, 'train_rates': 0.6595874269255911, 'learning_rate': 6.849199699125047e-06, 'batch_size': 126, 'step_size': 5, 'gamma': 0.831998678998826}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:29:15,322][0m Trial 35 finished with value: 0.08733246297001125 and parameters: {'observation_period_num': 41, 'train_rates': 0.7711497606846931, 'learning_rate': 9.639785992872825e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.9737244831078073}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:29:54,303][0m Trial 36 finished with value: 0.07520928259213615 and parameters: {'observation_period_num': 72, 'train_rates': 0.6336064842138366, 'learning_rate': 0.0002971348654480556, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8018189275891404}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:31:14,733][0m Trial 37 finished with value: 0.0434246650520947 and parameters: {'observation_period_num': 54, 'train_rates': 0.7979160254886946, 'learning_rate': 5.005710609135352e-05, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8923059567603807}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:33:03,217][0m Trial 38 finished with value: 0.09851910528100905 and parameters: {'observation_period_num': 18, 'train_rates': 0.7304984906097182, 'learning_rate': 2.244769752893083e-06, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8699732820921782}. Best is trial 10 with value: 0.03671373263181698.[0m
[32m[I 2025-02-03 14:35:55,604][0m Trial 39 finished with value: 0.023384105817915717 and parameters: {'observation_period_num': 5, 'train_rates': 0.8344408404703466, 'learning_rate': 0.0005687590436568785, 'batch_size': 31, 'step_size': 9, 'gamma': 0.8563151738157466}. Best is trial 39 with value: 0.023384105817915717.[0m
[32m[I 2025-02-03 14:38:56,164][0m Trial 40 finished with value: 0.02258197654269098 and parameters: {'observation_period_num': 12, 'train_rates': 0.8355997080650817, 'learning_rate': 0.000678000856616685, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9117399831244177}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:41:47,569][0m Trial 41 finished with value: 0.024191704382448564 and parameters: {'observation_period_num': 11, 'train_rates': 0.8165293186892305, 'learning_rate': 0.0009545733626024394, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9087593729399676}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:44:44,391][0m Trial 42 finished with value: 0.028882256455042145 and parameters: {'observation_period_num': 20, 'train_rates': 0.8314711028914854, 'learning_rate': 0.000960776384150068, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9106836739683665}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:47:37,101][0m Trial 43 finished with value: 0.029134314405538413 and parameters: {'observation_period_num': 17, 'train_rates': 0.841162976753596, 'learning_rate': 0.0007130383750356122, 'batch_size': 31, 'step_size': 6, 'gamma': 0.9099215406858455}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:50:28,272][0m Trial 44 finished with value: 0.031097396568311644 and parameters: {'observation_period_num': 24, 'train_rates': 0.8444414391758879, 'learning_rate': 0.0007087791319846244, 'batch_size': 31, 'step_size': 6, 'gamma': 0.9101764660735464}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:50:58,661][0m Trial 45 finished with value: 0.061590724238535254 and parameters: {'observation_period_num': 244, 'train_rates': 0.8189109166223921, 'learning_rate': 0.000702528119954786, 'batch_size': 178, 'step_size': 7, 'gamma': 0.9100524267191972}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:51:26,485][0m Trial 46 finished with value: 0.029095722923611667 and parameters: {'observation_period_num': 15, 'train_rates': 0.8691946267452245, 'learning_rate': 0.000977983091374658, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8802026360285014}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:51:53,715][0m Trial 47 finished with value: 0.05113938250301918 and parameters: {'observation_period_num': 168, 'train_rates': 0.8737589904440064, 'learning_rate': 0.0009904897524473692, 'batch_size': 204, 'step_size': 8, 'gamma': 0.8793533710431809}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:52:18,301][0m Trial 48 finished with value: 0.03604459666839525 and parameters: {'observation_period_num': 74, 'train_rates': 0.814044717968018, 'learning_rate': 0.00048441313892551557, 'batch_size': 243, 'step_size': 7, 'gamma': 0.8950822532391782}. Best is trial 40 with value: 0.02258197654269098.[0m
[32m[I 2025-02-03 14:52:42,196][0m Trial 49 finished with value: 0.037856503283789894 and parameters: {'observation_period_num': 51, 'train_rates': 0.8580674426201981, 'learning_rate': 0.00037034770327901644, 'batch_size': 254, 'step_size': 4, 'gamma': 0.9327402246921555}. Best is trial 40 with value: 0.02258197654269098.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1747 | 0.1003
Epoch 2/300, Loss: 0.1025 | 0.0857
Epoch 3/300, Loss: 0.0965 | 0.0696
Epoch 4/300, Loss: 0.0846 | 0.0576
Epoch 5/300, Loss: 0.0711 | 0.0493
Epoch 6/300, Loss: 0.0570 | 0.0489
Epoch 7/300, Loss: 0.0503 | 0.0450
Epoch 8/300, Loss: 0.0428 | 0.0408
Epoch 9/300, Loss: 0.0402 | 0.0420
Epoch 10/300, Loss: 0.0470 | 0.0391
Epoch 11/300, Loss: 0.0429 | 0.0383
Epoch 12/300, Loss: 0.0369 | 0.0355
Epoch 13/300, Loss: 0.0353 | 0.0385
Epoch 14/300, Loss: 0.0470 | 0.0428
Epoch 15/300, Loss: 0.0376 | 0.0371
Epoch 16/300, Loss: 0.0308 | 0.0338
Epoch 17/300, Loss: 0.0324 | 0.0365
Epoch 18/300, Loss: 0.0292 | 0.0355
Epoch 19/300, Loss: 0.0290 | 0.0301
Epoch 20/300, Loss: 0.0316 | 0.0362
Epoch 21/300, Loss: 0.0317 | 0.0298
Epoch 22/300, Loss: 0.0280 | 0.0309
Epoch 23/300, Loss: 0.0267 | 0.0315
Epoch 24/300, Loss: 0.0255 | 0.0285
Epoch 25/300, Loss: 0.0289 | 0.0318
Epoch 26/300, Loss: 0.0246 | 0.0287
Epoch 27/300, Loss: 0.0254 | 0.0347
Epoch 28/300, Loss: 0.0259 | 0.0294
Epoch 29/300, Loss: 0.0232 | 0.0297
Epoch 30/300, Loss: 0.0231 | 0.0300
Epoch 31/300, Loss: 0.0240 | 0.0252
Epoch 32/300, Loss: 0.0214 | 0.0279
Epoch 33/300, Loss: 0.0196 | 0.0242
Epoch 34/300, Loss: 0.0196 | 0.0265
Epoch 35/300, Loss: 0.0201 | 0.0276
Epoch 36/300, Loss: 0.0187 | 0.0242
Epoch 37/300, Loss: 0.0185 | 0.0262
Epoch 38/300, Loss: 0.0174 | 0.0231
Epoch 39/300, Loss: 0.0176 | 0.0236
Epoch 40/300, Loss: 0.0164 | 0.0277
Epoch 41/300, Loss: 0.0161 | 0.0238
Epoch 42/300, Loss: 0.0160 | 0.0244
Epoch 43/300, Loss: 0.0158 | 0.0236
Epoch 44/300, Loss: 0.0155 | 0.0235
Epoch 45/300, Loss: 0.0151 | 0.0247
Epoch 46/300, Loss: 0.0163 | 0.0247
Epoch 47/300, Loss: 0.0163 | 0.0243
Epoch 48/300, Loss: 0.0144 | 0.0241
Epoch 49/300, Loss: 0.0140 | 0.0233
Epoch 50/300, Loss: 0.0138 | 0.0230
Epoch 51/300, Loss: 0.0133 | 0.0236
Epoch 52/300, Loss: 0.0136 | 0.0245
Epoch 53/300, Loss: 0.0133 | 0.0250
Epoch 54/300, Loss: 0.0131 | 0.0254
Epoch 55/300, Loss: 0.0135 | 0.0261
Epoch 56/300, Loss: 0.0129 | 0.0276
Epoch 57/300, Loss: 0.0132 | 0.0235
Epoch 58/300, Loss: 0.0142 | 0.0274
Epoch 59/300, Loss: 0.0133 | 0.0237
Epoch 60/300, Loss: 0.0125 | 0.0254
Epoch 61/300, Loss: 0.0122 | 0.0244
Epoch 62/300, Loss: 0.0124 | 0.0239
Epoch 63/300, Loss: 0.0124 | 0.0236
Epoch 64/300, Loss: 0.0124 | 0.0255
Epoch 65/300, Loss: 0.0123 | 0.0239
Epoch 66/300, Loss: 0.0125 | 0.0264
Epoch 67/300, Loss: 0.0122 | 0.0238
Epoch 68/300, Loss: 0.0116 | 0.0252
Epoch 69/300, Loss: 0.0114 | 0.0233
Epoch 70/300, Loss: 0.0123 | 0.0276
Epoch 71/300, Loss: 0.0123 | 0.0266
Epoch 72/300, Loss: 0.0124 | 0.0295
Epoch 73/300, Loss: 0.0159 | 0.0302
Epoch 74/300, Loss: 0.0131 | 0.0305
Epoch 75/300, Loss: 0.0139 | 0.0267
Epoch 76/300, Loss: 0.0120 | 0.0263
Epoch 77/300, Loss: 0.0120 | 0.0253
Epoch 78/300, Loss: 0.0116 | 0.0244
Epoch 79/300, Loss: 0.0122 | 0.0239
Epoch 80/300, Loss: 0.0123 | 0.0240
Epoch 81/300, Loss: 0.0109 | 0.0228
Epoch 82/300, Loss: 0.0114 | 0.0234
Epoch 83/300, Loss: 0.0107 | 0.0232
Epoch 84/300, Loss: 0.0106 | 0.0234
Epoch 85/300, Loss: 0.0104 | 0.0235
Epoch 86/300, Loss: 0.0099 | 0.0233
Epoch 87/300, Loss: 0.0098 | 0.0233
Epoch 88/300, Loss: 0.0098 | 0.0232
Epoch 89/300, Loss: 0.0096 | 0.0235
Epoch 90/300, Loss: 0.0095 | 0.0235
Epoch 91/300, Loss: 0.0093 | 0.0234
Epoch 92/300, Loss: 0.0093 | 0.0236
Epoch 93/300, Loss: 0.0091 | 0.0235
Epoch 94/300, Loss: 0.0090 | 0.0238
Epoch 95/300, Loss: 0.0089 | 0.0237
Epoch 96/300, Loss: 0.0087 | 0.0239
Epoch 97/300, Loss: 0.0087 | 0.0239
Epoch 98/300, Loss: 0.0085 | 0.0237
Epoch 99/300, Loss: 0.0085 | 0.0240
Epoch 100/300, Loss: 0.0083 | 0.0240
Epoch 101/300, Loss: 0.0082 | 0.0241
Epoch 102/300, Loss: 0.0081 | 0.0241
Epoch 103/300, Loss: 0.0081 | 0.0243
Epoch 104/300, Loss: 0.0080 | 0.0242
Epoch 105/300, Loss: 0.0079 | 0.0245
Epoch 106/300, Loss: 0.0078 | 0.0245
Epoch 107/300, Loss: 0.0078 | 0.0247
Epoch 108/300, Loss: 0.0077 | 0.0245
Epoch 109/300, Loss: 0.0076 | 0.0246
Epoch 110/300, Loss: 0.0075 | 0.0246
Epoch 111/300, Loss: 0.0074 | 0.0246
Epoch 112/300, Loss: 0.0074 | 0.0247
Epoch 113/300, Loss: 0.0073 | 0.0245
Epoch 114/300, Loss: 0.0073 | 0.0247
Epoch 115/300, Loss: 0.0071 | 0.0246
Epoch 116/300, Loss: 0.0071 | 0.0246
Epoch 117/300, Loss: 0.0071 | 0.0247
Epoch 118/300, Loss: 0.0070 | 0.0248
Epoch 119/300, Loss: 0.0069 | 0.0252
Epoch 120/300, Loss: 0.0070 | 0.0255
Epoch 121/300, Loss: 0.0068 | 0.0256
Epoch 122/300, Loss: 0.0068 | 0.0255
Epoch 123/300, Loss: 0.0067 | 0.0257
Epoch 124/300, Loss: 0.0067 | 0.0257
Epoch 125/300, Loss: 0.0066 | 0.0259
Epoch 126/300, Loss: 0.0066 | 0.0258
Epoch 127/300, Loss: 0.0066 | 0.0260
Epoch 128/300, Loss: 0.0066 | 0.0259
Epoch 129/300, Loss: 0.0065 | 0.0260
Epoch 130/300, Loss: 0.0065 | 0.0259
Epoch 131/300, Loss: 0.0065 | 0.0260
Epoch 132/300, Loss: 0.0065 | 0.0259
Epoch 133/300, Loss: 0.0064 | 0.0259
Epoch 134/300, Loss: 0.0064 | 0.0259
Epoch 135/300, Loss: 0.0063 | 0.0258
Epoch 136/300, Loss: 0.0062 | 0.0258
Epoch 137/300, Loss: 0.0062 | 0.0257
Epoch 138/300, Loss: 0.0062 | 0.0257
Epoch 139/300, Loss: 0.0061 | 0.0256
Epoch 140/300, Loss: 0.0061 | 0.0256
Epoch 141/300, Loss: 0.0061 | 0.0256
Epoch 142/300, Loss: 0.0061 | 0.0256
Epoch 143/300, Loss: 0.0060 | 0.0256
Epoch 144/300, Loss: 0.0060 | 0.0256
Epoch 145/300, Loss: 0.0060 | 0.0256
Epoch 146/300, Loss: 0.0060 | 0.0257
Epoch 147/300, Loss: 0.0060 | 0.0257
Epoch 148/300, Loss: 0.0060 | 0.0257
Epoch 149/300, Loss: 0.0060 | 0.0258
Epoch 150/300, Loss: 0.0060 | 0.0258
Epoch 151/300, Loss: 0.0060 | 0.0258
Epoch 152/300, Loss: 0.0059 | 0.0258
Epoch 153/300, Loss: 0.0059 | 0.0258
Epoch 154/300, Loss: 0.0059 | 0.0258
Epoch 155/300, Loss: 0.0058 | 0.0258
Epoch 156/300, Loss: 0.0058 | 0.0259
Epoch 157/300, Loss: 0.0058 | 0.0259
Epoch 158/300, Loss: 0.0057 | 0.0259
Epoch 159/300, Loss: 0.0057 | 0.0259
Epoch 160/300, Loss: 0.0056 | 0.0260
Epoch 161/300, Loss: 0.0056 | 0.0260
Epoch 162/300, Loss: 0.0055 | 0.0260
Epoch 163/300, Loss: 0.0055 | 0.0260
Epoch 164/300, Loss: 0.0055 | 0.0260
Epoch 165/300, Loss: 0.0054 | 0.0260
Epoch 166/300, Loss: 0.0054 | 0.0260
Epoch 167/300, Loss: 0.0054 | 0.0260
Epoch 168/300, Loss: 0.0054 | 0.0261
Epoch 169/300, Loss: 0.0054 | 0.0261
Epoch 170/300, Loss: 0.0053 | 0.0261
Epoch 171/300, Loss: 0.0053 | 0.0261
Epoch 172/300, Loss: 0.0053 | 0.0261
Epoch 173/300, Loss: 0.0053 | 0.0261
Epoch 174/300, Loss: 0.0053 | 0.0261
Epoch 175/300, Loss: 0.0053 | 0.0261
Epoch 176/300, Loss: 0.0052 | 0.0261
Epoch 177/300, Loss: 0.0052 | 0.0261
Epoch 178/300, Loss: 0.0052 | 0.0262
Epoch 179/300, Loss: 0.0052 | 0.0262
Epoch 180/300, Loss: 0.0052 | 0.0262
Epoch 181/300, Loss: 0.0052 | 0.0262
Epoch 182/300, Loss: 0.0052 | 0.0262
Epoch 183/300, Loss: 0.0051 | 0.0262
Epoch 184/300, Loss: 0.0051 | 0.0262
Epoch 185/300, Loss: 0.0051 | 0.0262
Epoch 186/300, Loss: 0.0051 | 0.0262
Epoch 187/300, Loss: 0.0051 | 0.0262
Epoch 188/300, Loss: 0.0051 | 0.0262
Epoch 189/300, Loss: 0.0051 | 0.0262
Epoch 190/300, Loss: 0.0051 | 0.0262
Epoch 191/300, Loss: 0.0050 | 0.0262
Epoch 192/300, Loss: 0.0050 | 0.0263
Epoch 193/300, Loss: 0.0050 | 0.0263
Epoch 194/300, Loss: 0.0050 | 0.0263
Epoch 195/300, Loss: 0.0050 | 0.0263
Epoch 196/300, Loss: 0.0050 | 0.0263
Epoch 197/300, Loss: 0.0050 | 0.0263
Epoch 198/300, Loss: 0.0050 | 0.0263
Epoch 199/300, Loss: 0.0050 | 0.0263
Epoch 200/300, Loss: 0.0050 | 0.0263
Epoch 201/300, Loss: 0.0050 | 0.0263
Epoch 202/300, Loss: 0.0050 | 0.0263
Epoch 203/300, Loss: 0.0049 | 0.0263
Epoch 204/300, Loss: 0.0049 | 0.0263
Epoch 205/300, Loss: 0.0049 | 0.0263
Epoch 206/300, Loss: 0.0049 | 0.0263
Epoch 207/300, Loss: 0.0049 | 0.0263
Epoch 208/300, Loss: 0.0049 | 0.0263
Epoch 209/300, Loss: 0.0049 | 0.0263
Epoch 210/300, Loss: 0.0049 | 0.0263
Epoch 211/300, Loss: 0.0049 | 0.0263
Epoch 212/300, Loss: 0.0049 | 0.0263
Epoch 213/300, Loss: 0.0049 | 0.0263
Epoch 214/300, Loss: 0.0049 | 0.0263
Epoch 215/300, Loss: 0.0049 | 0.0264
Epoch 216/300, Loss: 0.0049 | 0.0264
Epoch 217/300, Loss: 0.0049 | 0.0264
Epoch 218/300, Loss: 0.0049 | 0.0264
Epoch 219/300, Loss: 0.0048 | 0.0264
Epoch 220/300, Loss: 0.0048 | 0.0264
Epoch 221/300, Loss: 0.0048 | 0.0264
Epoch 222/300, Loss: 0.0048 | 0.0264
Epoch 223/300, Loss: 0.0048 | 0.0264
Epoch 224/300, Loss: 0.0048 | 0.0264
Epoch 225/300, Loss: 0.0048 | 0.0264
Epoch 226/300, Loss: 0.0048 | 0.0264
Epoch 227/300, Loss: 0.0048 | 0.0264
Epoch 228/300, Loss: 0.0048 | 0.0264
Epoch 229/300, Loss: 0.0048 | 0.0265
Epoch 230/300, Loss: 0.0048 | 0.0265
Epoch 231/300, Loss: 0.0048 | 0.0265
Epoch 232/300, Loss: 0.0048 | 0.0265
Epoch 233/300, Loss: 0.0048 | 0.0265
Epoch 234/300, Loss: 0.0048 | 0.0265
Epoch 235/300, Loss: 0.0048 | 0.0265
Epoch 236/300, Loss: 0.0048 | 0.0265
Epoch 237/300, Loss: 0.0048 | 0.0265
Epoch 238/300, Loss: 0.0048 | 0.0265
Epoch 239/300, Loss: 0.0048 | 0.0265
Epoch 240/300, Loss: 0.0048 | 0.0265
Epoch 241/300, Loss: 0.0048 | 0.0265
Epoch 242/300, Loss: 0.0048 | 0.0265
Epoch 243/300, Loss: 0.0047 | 0.0265
Epoch 244/300, Loss: 0.0047 | 0.0265
Epoch 245/300, Loss: 0.0047 | 0.0265
Epoch 246/300, Loss: 0.0047 | 0.0265
Epoch 247/300, Loss: 0.0047 | 0.0265
Epoch 248/300, Loss: 0.0047 | 0.0265
Epoch 249/300, Loss: 0.0047 | 0.0265
Epoch 250/300, Loss: 0.0047 | 0.0265
Epoch 251/300, Loss: 0.0047 | 0.0265
Epoch 252/300, Loss: 0.0047 | 0.0265
Epoch 253/300, Loss: 0.0047 | 0.0265
Epoch 254/300, Loss: 0.0047 | 0.0265
Epoch 255/300, Loss: 0.0047 | 0.0265
Epoch 256/300, Loss: 0.0047 | 0.0265
Epoch 257/300, Loss: 0.0047 | 0.0265
Epoch 258/300, Loss: 0.0047 | 0.0265
Epoch 259/300, Loss: 0.0047 | 0.0265
Epoch 260/300, Loss: 0.0047 | 0.0265
Epoch 261/300, Loss: 0.0047 | 0.0265
Epoch 262/300, Loss: 0.0047 | 0.0265
Epoch 263/300, Loss: 0.0047 | 0.0265
Epoch 264/300, Loss: 0.0047 | 0.0265
Epoch 265/300, Loss: 0.0047 | 0.0265
Epoch 266/300, Loss: 0.0047 | 0.0266
Epoch 267/300, Loss: 0.0047 | 0.0266
Epoch 268/300, Loss: 0.0047 | 0.0266
Epoch 269/300, Loss: 0.0047 | 0.0266
Epoch 270/300, Loss: 0.0047 | 0.0266
Epoch 271/300, Loss: 0.0047 | 0.0266
Epoch 272/300, Loss: 0.0047 | 0.0266
Epoch 273/300, Loss: 0.0047 | 0.0266
Epoch 274/300, Loss: 0.0047 | 0.0266
Epoch 275/300, Loss: 0.0047 | 0.0266
Epoch 276/300, Loss: 0.0047 | 0.0266
Epoch 277/300, Loss: 0.0047 | 0.0266
Epoch 278/300, Loss: 0.0047 | 0.0266
Epoch 279/300, Loss: 0.0047 | 0.0266
Epoch 280/300, Loss: 0.0047 | 0.0266
Epoch 281/300, Loss: 0.0047 | 0.0266
Epoch 282/300, Loss: 0.0047 | 0.0266
Epoch 283/300, Loss: 0.0047 | 0.0266
Epoch 284/300, Loss: 0.0047 | 0.0266
Epoch 285/300, Loss: 0.0047 | 0.0266
Epoch 286/300, Loss: 0.0047 | 0.0266
Epoch 287/300, Loss: 0.0047 | 0.0266
Epoch 288/300, Loss: 0.0047 | 0.0266
Epoch 289/300, Loss: 0.0047 | 0.0266
Epoch 290/300, Loss: 0.0047 | 0.0266
Epoch 291/300, Loss: 0.0047 | 0.0266
Epoch 292/300, Loss: 0.0047 | 0.0266
Epoch 293/300, Loss: 0.0047 | 0.0266
Epoch 294/300, Loss: 0.0047 | 0.0266
Epoch 295/300, Loss: 0.0047 | 0.0266
Epoch 296/300, Loss: 0.0047 | 0.0266
Epoch 297/300, Loss: 0.0047 | 0.0266
Epoch 298/300, Loss: 0.0047 | 0.0266
Epoch 299/300, Loss: 0.0047 | 0.0266
Epoch 300/300, Loss: 0.0047 | 0.0266
Runtime (seconds): 548.4861114025116
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 4.555009515024722
RMSE: 2.134246826171875
MAE: 2.134246826171875
R-squared: nan
[156.14575]
