ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2024-12-30 02:51:38,097][0m A new study created in memory with name: no-name-317711dc-3cdc-4142-82b9-c6f529da0082[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[32m[I 2024-12-30 02:54:28,091][0m Trial 0 finished with value: 0.8312454704718519 and parameters: {'observation_period_num': 215, 'train_rates': 0.6499136576781817, 'learning_rate': 1.1965916041576599e-06, 'batch_size': 202, 'step_size': 12, 'gamma': 0.9740564583090711}. Best is trial 0 with value: 0.8312454704718519.[0m
[32m[I 2024-12-30 03:00:55,749][0m Trial 1 finished with value: 0.5136375872603742 and parameters: {'observation_period_num': 13, 'train_rates': 0.6052370487827604, 'learning_rate': 0.0004764623402653452, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7891088451855907}. Best is trial 1 with value: 0.5136375872603742.[0m
[32m[I 2024-12-30 03:03:57,016][0m Trial 2 finished with value: 0.39845083021082417 and parameters: {'observation_period_num': 247, 'train_rates': 0.726221735175133, 'learning_rate': 0.0007970987743185611, 'batch_size': 206, 'step_size': 1, 'gamma': 0.9236169945182181}. Best is trial 2 with value: 0.39845083021082417.[0m
[32m[I 2024-12-30 03:07:35,456][0m Trial 3 finished with value: 0.2510251370026691 and parameters: {'observation_period_num': 18, 'train_rates': 0.7321404554821885, 'learning_rate': 4.781919659711839e-06, 'batch_size': 111, 'step_size': 5, 'gamma': 0.7728594028469307}. Best is trial 3 with value: 0.2510251370026691.[0m
[32m[I 2024-12-30 03:10:42,931][0m Trial 4 finished with value: 0.36891231405355535 and parameters: {'observation_period_num': 102, 'train_rates': 0.6788407153478859, 'learning_rate': 1.1128134379408184e-05, 'batch_size': 154, 'step_size': 2, 'gamma': 0.885961745907862}. Best is trial 3 with value: 0.2510251370026691.[0m
[32m[I 2024-12-30 03:14:12,952][0m Trial 5 finished with value: 0.24432732008247876 and parameters: {'observation_period_num': 42, 'train_rates': 0.7768906782644691, 'learning_rate': 0.0006946827338859451, 'batch_size': 142, 'step_size': 15, 'gamma': 0.9108024275263099}. Best is trial 5 with value: 0.24432732008247876.[0m
[32m[I 2024-12-30 03:18:35,930][0m Trial 6 finished with value: 0.04149517518487864 and parameters: {'observation_period_num': 14, 'train_rates': 0.8221024631253029, 'learning_rate': 1.89417747404024e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.7796399472014476}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:22:05,018][0m Trial 7 finished with value: 0.2903830040551411 and parameters: {'observation_period_num': 93, 'train_rates': 0.7726331721186604, 'learning_rate': 7.97130348189353e-05, 'batch_size': 116, 'step_size': 3, 'gamma': 0.8465964636080361}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:25:10,504][0m Trial 8 finished with value: 0.3134711389017736 and parameters: {'observation_period_num': 38, 'train_rates': 0.6611238314603672, 'learning_rate': 0.0009178857659690775, 'batch_size': 179, 'step_size': 13, 'gamma': 0.7819015691847875}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:28:06,527][0m Trial 9 finished with value: 0.3831816146108126 and parameters: {'observation_period_num': 193, 'train_rates': 0.6618675915658526, 'learning_rate': 0.0005553389624387814, 'batch_size': 195, 'step_size': 6, 'gamma': 0.7680971328877273}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:34:47,640][0m Trial 10 finished with value: 0.11772977575963857 and parameters: {'observation_period_num': 147, 'train_rates': 0.9124478225805576, 'learning_rate': 4.6386092557312325e-05, 'batch_size': 50, 'step_size': 10, 'gamma': 0.8385710185804803}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:45:28,858][0m Trial 11 finished with value: 0.15312993109643042 and parameters: {'observation_period_num': 155, 'train_rates': 0.8975672175636988, 'learning_rate': 6.338171000576091e-05, 'batch_size': 31, 'step_size': 10, 'gamma': 0.8327764580149863}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:50:17,670][0m Trial 12 finished with value: 0.10744163114577532 and parameters: {'observation_period_num': 141, 'train_rates': 0.8971243057895858, 'learning_rate': 2.023463499413359e-05, 'batch_size': 74, 'step_size': 10, 'gamma': 0.8132441966871716}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:54:48,531][0m Trial 13 finished with value: 0.08027728508295479 and parameters: {'observation_period_num': 81, 'train_rates': 0.8533594645941125, 'learning_rate': 1.4065148005888414e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.8111879558052336}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 03:59:15,981][0m Trial 14 finished with value: 0.11140069332426396 and parameters: {'observation_period_num': 70, 'train_rates': 0.851054279344322, 'learning_rate': 4.628535496178783e-06, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8020124812432182}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:03:16,413][0m Trial 15 finished with value: 0.09256856212404067 and parameters: {'observation_period_num': 66, 'train_rates': 0.8326460581611884, 'learning_rate': 0.00017829247236916422, 'batch_size': 97, 'step_size': 15, 'gamma': 0.7580694471004629}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:19:58,481][0m Trial 16 finished with value: 0.13342613719403743 and parameters: {'observation_period_num': 110, 'train_rates': 0.9647897936066102, 'learning_rate': 1.5252873830954089e-05, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8632499399715305}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:23:23,926][0m Trial 17 finished with value: 0.18040955190592484 and parameters: {'observation_period_num': 68, 'train_rates': 0.8366875810986109, 'learning_rate': 4.4885615551389355e-06, 'batch_size': 243, 'step_size': 8, 'gamma': 0.8177197828373477}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:29:03,493][0m Trial 18 finished with value: 0.07378327109150051 and parameters: {'observation_period_num': 5, 'train_rates': 0.9672120284699417, 'learning_rate': 1.1687060579255141e-06, 'batch_size': 68, 'step_size': 13, 'gamma': 0.7503073377875937}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:35:27,993][0m Trial 19 finished with value: 0.12596086368841283 and parameters: {'observation_period_num': 34, 'train_rates': 0.9768729409293498, 'learning_rate': 1.305671926973078e-06, 'batch_size': 59, 'step_size': 13, 'gamma': 0.7542404344462158}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:39:34,285][0m Trial 20 finished with value: 0.05397643834270167 and parameters: {'observation_period_num': 5, 'train_rates': 0.9386014007439377, 'learning_rate': 2.398530706429531e-06, 'batch_size': 121, 'step_size': 14, 'gamma': 0.9727172459928884}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:43:45,935][0m Trial 21 finished with value: 0.06052461420137858 and parameters: {'observation_period_num': 8, 'train_rates': 0.9340212322740047, 'learning_rate': 2.199720242778783e-06, 'batch_size': 118, 'step_size': 14, 'gamma': 0.9753217611471856}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:47:52,381][0m Trial 22 finished with value: 0.09346713617126992 and parameters: {'observation_period_num': 42, 'train_rates': 0.9353607533102203, 'learning_rate': 2.4940869533760064e-06, 'batch_size': 128, 'step_size': 15, 'gamma': 0.9864385048809519}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:51:48,930][0m Trial 23 finished with value: 0.05316093484205859 and parameters: {'observation_period_num': 24, 'train_rates': 0.93298304849974, 'learning_rate': 7.627613904275407e-06, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9556355883593742}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:55:32,359][0m Trial 24 finished with value: 0.08794666612651986 and parameters: {'observation_period_num': 53, 'train_rates': 0.8770484715504823, 'learning_rate': 7.491431327738885e-06, 'batch_size': 161, 'step_size': 12, 'gamma': 0.944066439815946}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 04:59:32,161][0m Trial 25 finished with value: 0.054630306209363634 and parameters: {'observation_period_num': 25, 'train_rates': 0.8055438540770782, 'learning_rate': 2.638719375959305e-05, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9427781008013281}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 05:03:35,742][0m Trial 26 finished with value: 0.07232150915596221 and parameters: {'observation_period_num': 55, 'train_rates': 0.9375731003832001, 'learning_rate': 8.307267452535154e-06, 'batch_size': 142, 'step_size': 14, 'gamma': 0.9541433447961666}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 05:07:00,520][0m Trial 27 finished with value: 0.22611308411548012 and parameters: {'observation_period_num': 124, 'train_rates': 0.8042032446753548, 'learning_rate': 2.7748321813081843e-06, 'batch_size': 165, 'step_size': 14, 'gamma': 0.8993657555856516}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 05:10:57,597][0m Trial 28 finished with value: 0.04212893173098564 and parameters: {'observation_period_num': 27, 'train_rates': 0.9866021382438629, 'learning_rate': 0.000154714567074932, 'batch_size': 230, 'step_size': 11, 'gamma': 0.9251305436472405}. Best is trial 6 with value: 0.04149517518487864.[0m
[32m[I 2024-12-30 05:14:47,107][0m Trial 29 finished with value: 0.1705327033996582 and parameters: {'observation_period_num': 179, 'train_rates': 0.985320858801652, 'learning_rate': 0.00014888745350373356, 'batch_size': 247, 'step_size': 12, 'gamma': 0.9326307416733822}. Best is trial 6 with value: 0.04149517518487864.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2024-12-30 05:14:47,112][0m A new study created in memory with name: no-name-2b9842e8-61b0-4d11-833d-6cd7c29d6d4f[0m
[32m[I 2024-12-30 05:17:59,163][0m Trial 0 finished with value: 0.32097455825547333 and parameters: {'observation_period_num': 142, 'train_rates': 0.6831927066465703, 'learning_rate': 0.0008381681345307543, 'batch_size': 212, 'step_size': 10, 'gamma': 0.9657376753542508}. Best is trial 0 with value: 0.32097455825547333.[0m
[32m[I 2024-12-30 05:20:55,385][0m Trial 1 finished with value: 0.493172664662731 and parameters: {'observation_period_num': 169, 'train_rates': 0.6200915745276181, 'learning_rate': 0.00012661488576231573, 'batch_size': 212, 'step_size': 12, 'gamma': 0.825339701808558}. Best is trial 0 with value: 0.32097455825547333.[0m
[32m[I 2024-12-30 05:23:53,245][0m Trial 2 finished with value: 0.32622598143334086 and parameters: {'observation_period_num': 153, 'train_rates': 0.6964021026552516, 'learning_rate': 9.54140691722771e-05, 'batch_size': 243, 'step_size': 11, 'gamma': 0.7910857238325013}. Best is trial 0 with value: 0.32097455825547333.[0m
[32m[I 2024-12-30 05:27:48,703][0m Trial 3 finished with value: 0.35981824070308066 and parameters: {'observation_period_num': 169, 'train_rates': 0.7682191705596312, 'learning_rate': 1.4848331518895061e-05, 'batch_size': 86, 'step_size': 7, 'gamma': 0.833614240405434}. Best is trial 0 with value: 0.32097455825547333.[0m
[32m[I 2024-12-30 05:30:49,253][0m Trial 4 finished with value: 0.22631775598584686 and parameters: {'observation_period_num': 7, 'train_rates': 0.6465684997624368, 'learning_rate': 1.0995609618971425e-06, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9262311300004993}. Best is trial 4 with value: 0.22631775598584686.[0m
[32m[I 2024-12-30 05:35:21,918][0m Trial 5 finished with value: 0.11165272559110935 and parameters: {'observation_period_num': 103, 'train_rates': 0.8258698055661758, 'learning_rate': 0.0001862250750667558, 'batch_size': 76, 'step_size': 8, 'gamma': 0.7710700731613618}. Best is trial 5 with value: 0.11165272559110935.[0m
[32m[I 2024-12-30 05:40:05,229][0m Trial 6 finished with value: 0.3297346093152699 and parameters: {'observation_period_num': 150, 'train_rates': 0.8297005449368033, 'learning_rate': 1.729434762353148e-06, 'batch_size': 75, 'step_size': 3, 'gamma': 0.9014408352175229}. Best is trial 5 with value: 0.11165272559110935.[0m
[32m[I 2024-12-30 05:43:43,963][0m Trial 7 finished with value: 0.08013692298500809 and parameters: {'observation_period_num': 31, 'train_rates': 0.8668478970421216, 'learning_rate': 0.000764286066182096, 'batch_size': 215, 'step_size': 4, 'gamma': 0.961316729370759}. Best is trial 7 with value: 0.08013692298500809.[0m
Early stopping at epoch 85
[32m[I 2024-12-30 05:47:06,115][0m Trial 8 finished with value: 0.19720146254352902 and parameters: {'observation_period_num': 220, 'train_rates': 0.8311189075416292, 'learning_rate': 0.00011736844623385817, 'batch_size': 102, 'step_size': 1, 'gamma': 0.855010838484551}. Best is trial 7 with value: 0.08013692298500809.[0m
[32m[I 2024-12-30 05:50:22,454][0m Trial 9 finished with value: 0.49922868216479266 and parameters: {'observation_period_num': 193, 'train_rates': 0.7542081283212938, 'learning_rate': 6.6645450463239524e-06, 'batch_size': 188, 'step_size': 12, 'gamma': 0.7700686846498499}. Best is trial 7 with value: 0.08013692298500809.[0m
[32m[I 2024-12-30 05:54:26,278][0m Trial 10 finished with value: 0.06567563861608505 and parameters: {'observation_period_num': 23, 'train_rates': 0.9521353978142657, 'learning_rate': 0.0009378508771565122, 'batch_size': 151, 'step_size': 5, 'gamma': 0.9893872700958178}. Best is trial 10 with value: 0.06567563861608505.[0m
[32m[I 2024-12-30 05:58:34,646][0m Trial 11 finished with value: 0.051050711423158646 and parameters: {'observation_period_num': 16, 'train_rates': 0.9583470252230343, 'learning_rate': 0.0008063897760470949, 'batch_size': 145, 'step_size': 4, 'gamma': 0.9888153836389085}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:02:37,242][0m Trial 12 finished with value: 0.059452854096889496 and parameters: {'observation_period_num': 51, 'train_rates': 0.9802341835668505, 'learning_rate': 0.0003840868010138375, 'batch_size': 152, 'step_size': 15, 'gamma': 0.9875288395369802}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:06:44,274][0m Trial 13 finished with value: 0.12154947221279144 and parameters: {'observation_period_num': 80, 'train_rates': 0.9879547132484249, 'learning_rate': 0.0002801645743827505, 'batch_size': 137, 'step_size': 15, 'gamma': 0.9090691557651968}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:10:29,715][0m Trial 14 finished with value: 0.10025346202728076 and parameters: {'observation_period_num': 57, 'train_rates': 0.9187852663757579, 'learning_rate': 3.658821146889081e-05, 'batch_size': 168, 'step_size': 15, 'gamma': 0.9491898867214873}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:22:29,483][0m Trial 15 finished with value: 0.06253466514103553 and parameters: {'observation_period_num': 56, 'train_rates': 0.905766798075489, 'learning_rate': 0.00032470414770448334, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9873687840026104}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:26:41,599][0m Trial 16 finished with value: 0.12294425815343857 and parameters: {'observation_period_num': 100, 'train_rates': 0.9838164277955599, 'learning_rate': 4.035988735650729e-05, 'batch_size': 115, 'step_size': 2, 'gamma': 0.8856459829276933}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:30:32,827][0m Trial 17 finished with value: 0.09811074069635135 and parameters: {'observation_period_num': 48, 'train_rates': 0.9122830875725433, 'learning_rate': 0.00039684739502283457, 'batch_size': 162, 'step_size': 5, 'gamma': 0.9472828270295188}. Best is trial 11 with value: 0.051050711423158646.[0m
[32m[I 2024-12-30 06:34:30,016][0m Trial 18 finished with value: 0.03682366001574113 and parameters: {'observation_period_num': 6, 'train_rates': 0.9363323972799562, 'learning_rate': 7.369791430155322e-05, 'batch_size': 186, 'step_size': 9, 'gamma': 0.931068329628434}. Best is trial 18 with value: 0.03682366001574113.[0m
[32m[I 2024-12-30 06:37:55,309][0m Trial 19 finished with value: 0.19579551747212043 and parameters: {'observation_period_num': 251, 'train_rates': 0.8791432386070274, 'learning_rate': 9.683564049678834e-06, 'batch_size': 188, 'step_size': 9, 'gamma': 0.9327810694334211}. Best is trial 18 with value: 0.03682366001574113.[0m
[32m[I 2024-12-30 06:46:06,579][0m Trial 20 finished with value: 0.05036762279440772 and parameters: {'observation_period_num': 12, 'train_rates': 0.9458865895269312, 'learning_rate': 3.2913097978205738e-06, 'batch_size': 45, 'step_size': 6, 'gamma': 0.9195212181123719}. Best is trial 18 with value: 0.03682366001574113.[0m
[32m[I 2024-12-30 06:58:20,464][0m Trial 21 finished with value: 0.035887865215001334 and parameters: {'observation_period_num': 10, 'train_rates': 0.9434195722422634, 'learning_rate': 6.544776772204642e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.916341210826651}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:20:04,452][0m Trial 22 finished with value: 0.07572437750483021 and parameters: {'observation_period_num': 82, 'train_rates': 0.9281112960666414, 'learning_rate': 6.25929589312625e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8788504036262028}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:27:53,668][0m Trial 23 finished with value: 0.06676504139749097 and parameters: {'observation_period_num': 29, 'train_rates': 0.8783021854292836, 'learning_rate': 2.0309782336677613e-05, 'batch_size': 48, 'step_size': 8, 'gamma': 0.9135623225780755}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:35:19,277][0m Trial 24 finished with value: 0.04341882525617818 and parameters: {'observation_period_num': 12, 'train_rates': 0.9434977225285937, 'learning_rate': 5.335123888906238e-06, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8999805109572733}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:42:34,760][0m Trial 25 finished with value: 0.06804315148674821 and parameters: {'observation_period_num': 40, 'train_rates': 0.8610392274476256, 'learning_rate': 4.743971850125433e-06, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8585157154334803}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:48:49,800][0m Trial 26 finished with value: 0.10558340142310506 and parameters: {'observation_period_num': 86, 'train_rates': 0.9006286648194954, 'learning_rate': 2.0352039122535446e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8998024691257045}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 07:52:41,602][0m Trial 27 finished with value: 0.23760162474679164 and parameters: {'observation_period_num': 71, 'train_rates': 0.7873479686840268, 'learning_rate': 6.500442177150766e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.937711245192087}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 08:13:10,605][0m Trial 28 finished with value: 0.03727842797805746 and parameters: {'observation_period_num': 11, 'train_rates': 0.943366045762122, 'learning_rate': 5.750039083834037e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8824953852283711}. Best is trial 21 with value: 0.035887865215001334.[0m
[32m[I 2024-12-30 08:27:41,823][0m Trial 29 finished with value: 0.2748018300050435 and parameters: {'observation_period_num': 124, 'train_rates': 0.6940584983028263, 'learning_rate': 4.655603915449768e-05, 'batch_size': 19, 'step_size': 3, 'gamma': 0.8693274465878147}. Best is trial 21 with value: 0.035887865215001334.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2024-12-30 08:27:41,828][0m A new study created in memory with name: no-name-3a936333-ed45-47ab-a5a2-8f045e1b9b4b[0m
[32m[I 2024-12-30 08:31:08,534][0m Trial 0 finished with value: 0.24339519980791452 and parameters: {'observation_period_num': 210, 'train_rates': 0.8647177640148388, 'learning_rate': 9.06074659565672e-06, 'batch_size': 174, 'step_size': 6, 'gamma': 0.8554284734351925}. Best is trial 0 with value: 0.24339519980791452.[0m
[32m[I 2024-12-30 08:34:30,874][0m Trial 1 finished with value: 0.16227820216945657 and parameters: {'observation_period_num': 144, 'train_rates': 0.8395988002194663, 'learning_rate': 8.194495748943864e-06, 'batch_size': 243, 'step_size': 3, 'gamma': 0.9565259743800454}. Best is trial 1 with value: 0.16227820216945657.[0m
[32m[I 2024-12-30 08:42:33,731][0m Trial 2 finished with value: 0.2748258449439378 and parameters: {'observation_period_num': 119, 'train_rates': 0.6940822191070245, 'learning_rate': 0.0001491250763264693, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8088195252922491}. Best is trial 1 with value: 0.16227820216945657.[0m
[32m[I 2024-12-30 08:45:37,901][0m Trial 3 finished with value: 0.38738954237182366 and parameters: {'observation_period_num': 138, 'train_rates': 0.6495696810982624, 'learning_rate': 3.821527763222735e-06, 'batch_size': 136, 'step_size': 12, 'gamma': 0.9400586167871995}. Best is trial 1 with value: 0.16227820216945657.[0m
[32m[I 2024-12-30 08:48:35,968][0m Trial 4 finished with value: 0.38734291655471526 and parameters: {'observation_period_num': 141, 'train_rates': 0.7033518836246043, 'learning_rate': 1.2530313170965725e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8592078924736182}. Best is trial 1 with value: 0.16227820216945657.[0m
[32m[I 2024-12-30 08:52:38,625][0m Trial 5 finished with value: 0.16295211039351176 and parameters: {'observation_period_num': 13, 'train_rates': 0.7307685820081837, 'learning_rate': 7.817125694629453e-05, 'batch_size': 91, 'step_size': 14, 'gamma': 0.8101625369830456}. Best is trial 1 with value: 0.16227820216945657.[0m
[32m[I 2024-12-30 08:56:28,953][0m Trial 6 finished with value: 0.1034466449840827 and parameters: {'observation_period_num': 102, 'train_rates': 0.9235883594148653, 'learning_rate': 0.00011948412303701245, 'batch_size': 148, 'step_size': 9, 'gamma': 0.9190257008604488}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:00:25,121][0m Trial 7 finished with value: 0.43373093008995056 and parameters: {'observation_period_num': 62, 'train_rates': 0.9729523453872048, 'learning_rate': 1.9234936578262794e-06, 'batch_size': 182, 'step_size': 3, 'gamma': 0.8488060026330441}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:16:21,424][0m Trial 8 finished with value: 0.18255194572799177 and parameters: {'observation_period_num': 232, 'train_rates': 0.8468980638711644, 'learning_rate': 5.758341887645413e-06, 'batch_size': 19, 'step_size': 15, 'gamma': 0.764194230226407}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:20:16,199][0m Trial 9 finished with value: 0.16658091864415578 and parameters: {'observation_period_num': 32, 'train_rates': 0.6387335397006059, 'learning_rate': 1.1006300696792494e-05, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9529428065101907}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:24:11,179][0m Trial 10 finished with value: 0.12680567800998688 and parameters: {'observation_period_num': 82, 'train_rates': 0.9759116311166655, 'learning_rate': 0.000782167618887097, 'batch_size': 195, 'step_size': 7, 'gamma': 0.9131365243704819}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:28:07,678][0m Trial 11 finished with value: 0.10982699692249298 and parameters: {'observation_period_num': 82, 'train_rates': 0.9865469960205357, 'learning_rate': 0.0009829196959298635, 'batch_size': 202, 'step_size': 8, 'gamma': 0.9157447096471497}. Best is trial 6 with value: 0.1034466449840827.[0m
[32m[I 2024-12-30 09:31:57,042][0m Trial 12 finished with value: 0.09206358028378914 and parameters: {'observation_period_num': 93, 'train_rates': 0.9136499820105574, 'learning_rate': 0.0007890304544694795, 'batch_size': 139, 'step_size': 9, 'gamma': 0.9093170411781246}. Best is trial 12 with value: 0.09206358028378914.[0m
[32m[I 2024-12-30 09:35:44,654][0m Trial 13 finished with value: 0.11979082857605315 and parameters: {'observation_period_num': 173, 'train_rates': 0.9011386250598579, 'learning_rate': 0.00025171469731278466, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9879893562384182}. Best is trial 12 with value: 0.09206358028378914.[0m
[32m[I 2024-12-30 09:39:36,064][0m Trial 14 finished with value: 0.08494903921496634 and parameters: {'observation_period_num': 99, 'train_rates': 0.910324770587714, 'learning_rate': 4.9950067512556067e-05, 'batch_size': 140, 'step_size': 5, 'gamma': 0.8977785351182752}. Best is trial 14 with value: 0.08494903921496634.[0m
[32m[I 2024-12-30 09:43:29,025][0m Trial 15 finished with value: 0.08241645125267298 and parameters: {'observation_period_num': 46, 'train_rates': 0.7933471931971384, 'learning_rate': 3.6672521580824704e-05, 'batch_size': 101, 'step_size': 5, 'gamma': 0.8807591219733462}. Best is trial 15 with value: 0.08241645125267298.[0m
[32m[I 2024-12-30 09:47:45,734][0m Trial 16 finished with value: 0.2788378130996002 and parameters: {'observation_period_num': 50, 'train_rates': 0.7799739080814818, 'learning_rate': 3.140809917187948e-05, 'batch_size': 89, 'step_size': 1, 'gamma': 0.8884831658446137}. Best is trial 15 with value: 0.08241645125267298.[0m
[32m[I 2024-12-30 09:53:12,655][0m Trial 17 finished with value: 0.04691828538768536 and parameters: {'observation_period_num': 17, 'train_rates': 0.793253869243469, 'learning_rate': 4.176353471339515e-05, 'batch_size': 61, 'step_size': 5, 'gamma': 0.8264602579821321}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 09:59:38,436][0m Trial 18 finished with value: 0.1759526254966878 and parameters: {'observation_period_num': 5, 'train_rates': 0.7783827502200225, 'learning_rate': 2.7915367190716553e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.813392554563173}. Best is trial 17 with value: 0.04691828538768536.[0m
Early stopping at epoch 68
[32m[I 2024-12-30 10:03:12,562][0m Trial 19 finished with value: 0.08455458208789891 and parameters: {'observation_period_num': 41, 'train_rates': 0.8003301185676549, 'learning_rate': 0.0002752144968009068, 'batch_size': 67, 'step_size': 1, 'gamma': 0.7576947282837243}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:06:24,450][0m Trial 20 finished with value: 0.22664884461865403 and parameters: {'observation_period_num': 23, 'train_rates': 0.6005618595150298, 'learning_rate': 2.078865385483235e-05, 'batch_size': 106, 'step_size': 6, 'gamma': 0.8336323785008133}. Best is trial 17 with value: 0.04691828538768536.[0m
Early stopping at epoch 72
[32m[I 2024-12-30 10:10:05,230][0m Trial 21 finished with value: 0.09312986308795577 and parameters: {'observation_period_num': 54, 'train_rates': 0.8068237852698162, 'learning_rate': 0.0003391227055228206, 'batch_size': 70, 'step_size': 1, 'gamma': 0.7607780520704001}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:15:41,789][0m Trial 22 finished with value: 0.20486979874766478 and parameters: {'observation_period_num': 39, 'train_rates': 0.7558136350340897, 'learning_rate': 6.786419003562687e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.7859963845351164}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:19:29,065][0m Trial 23 finished with value: 0.08197833666903379 and parameters: {'observation_period_num': 73, 'train_rates': 0.8123108614309793, 'learning_rate': 0.0002690910481662843, 'batch_size': 106, 'step_size': 2, 'gamma': 0.8806876707234442}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:23:14,429][0m Trial 24 finished with value: 0.07578153106552815 and parameters: {'observation_period_num': 70, 'train_rates': 0.8211817471218941, 'learning_rate': 5.482598460792229e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.8786690212861097}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:26:57,940][0m Trial 25 finished with value: 0.08960712711533945 and parameters: {'observation_period_num': 64, 'train_rates': 0.8266762327152322, 'learning_rate': 0.000135186474904951, 'batch_size': 121, 'step_size': 2, 'gamma': 0.8346437064112431}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:30:41,900][0m Trial 26 finished with value: 0.09363345610304068 and parameters: {'observation_period_num': 74, 'train_rates': 0.876398154764135, 'learning_rate': 0.00046572275399881446, 'batch_size': 161, 'step_size': 4, 'gamma': 0.8699406865056245}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:34:17,680][0m Trial 27 finished with value: 0.324589954833638 and parameters: {'observation_period_num': 121, 'train_rates': 0.7485749751945683, 'learning_rate': 1.8677499641209043e-05, 'batch_size': 115, 'step_size': 5, 'gamma': 0.8294260860532894}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:41:50,166][0m Trial 28 finished with value: 0.13558737558773515 and parameters: {'observation_period_num': 166, 'train_rates': 0.8272833202255986, 'learning_rate': 7.765746729578038e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8735953741149239}. Best is trial 17 with value: 0.04691828538768536.[0m
[32m[I 2024-12-30 10:46:22,210][0m Trial 29 finished with value: 0.05248990251916891 and parameters: {'observation_period_num': 22, 'train_rates': 0.8827795505426178, 'learning_rate': 0.00014525990675592638, 'batch_size': 91, 'step_size': 6, 'gamma': 0.8596426281236983}. Best is trial 17 with value: 0.04691828538768536.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2024-12-30 10:46:22,215][0m A new study created in memory with name: no-name-4da2dec1-9682-46af-a22c-e5a4dc2e79dd[0m
Early stopping at epoch 84
[32m[I 2024-12-30 10:49:53,539][0m Trial 0 finished with value: 0.5724219083786011 and parameters: {'observation_period_num': 191, 'train_rates': 0.841890828829112, 'learning_rate': 1.1049819852326232e-06, 'batch_size': 87, 'step_size': 2, 'gamma': 0.7753295124643862}. Best is trial 0 with value: 0.5724219083786011.[0m
[32m[I 2024-12-30 10:58:29,655][0m Trial 1 finished with value: 0.36261180543982996 and parameters: {'observation_period_num': 220, 'train_rates': 0.6850555158237137, 'learning_rate': 1.0840725164884341e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.9394377454974834}. Best is trial 1 with value: 0.36261180543982996.[0m
[32m[I 2024-12-30 11:02:50,150][0m Trial 2 finished with value: 0.07128430157899857 and parameters: {'observation_period_num': 34, 'train_rates': 0.9655174250642709, 'learning_rate': 0.0007623377815449649, 'batch_size': 106, 'step_size': 14, 'gamma': 0.9653878989813797}. Best is trial 2 with value: 0.07128430157899857.[0m
[32m[I 2024-12-30 11:05:41,550][0m Trial 3 finished with value: 0.5352952559628794 and parameters: {'observation_period_num': 167, 'train_rates': 0.6423048953353958, 'learning_rate': 1.7008824515897815e-05, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8767670437526685}. Best is trial 2 with value: 0.07128430157899857.[0m
[32m[I 2024-12-30 11:23:07,721][0m Trial 4 finished with value: 0.10874091944664963 and parameters: {'observation_period_num': 86, 'train_rates': 0.9152815657060838, 'learning_rate': 1.9983200457168925e-05, 'batch_size': 20, 'step_size': 8, 'gamma': 0.7665907012151814}. Best is trial 2 with value: 0.07128430157899857.[0m
[32m[I 2024-12-30 11:26:36,153][0m Trial 5 finished with value: 0.17001634093716894 and parameters: {'observation_period_num': 135, 'train_rates': 0.7939401090788822, 'learning_rate': 6.283631321201683e-05, 'batch_size': 144, 'step_size': 15, 'gamma': 0.8926184252329141}. Best is trial 2 with value: 0.07128430157899857.[0m
[32m[I 2024-12-30 11:31:50,526][0m Trial 6 finished with value: 0.2420917544431886 and parameters: {'observation_period_num': 94, 'train_rates': 0.6810820365753358, 'learning_rate': 0.0002349525179581979, 'batch_size': 53, 'step_size': 9, 'gamma': 0.7917817964644744}. Best is trial 2 with value: 0.07128430157899857.[0m
[32m[I 2024-12-30 11:35:42,096][0m Trial 7 finished with value: 0.036842404320688533 and parameters: {'observation_period_num': 8, 'train_rates': 0.931192630481117, 'learning_rate': 6.22675633292613e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.7869654206158495}. Best is trial 7 with value: 0.036842404320688533.[0m
[32m[I 2024-12-30 11:38:42,723][0m Trial 8 finished with value: 0.35784995682006016 and parameters: {'observation_period_num': 212, 'train_rates': 0.6967215616916144, 'learning_rate': 0.00048826150092100366, 'batch_size': 199, 'step_size': 6, 'gamma': 0.9721601438850871}. Best is trial 7 with value: 0.036842404320688533.[0m
[32m[I 2024-12-30 11:42:14,652][0m Trial 9 finished with value: 0.18497559428215027 and parameters: {'observation_period_num': 229, 'train_rates': 0.9344358575074363, 'learning_rate': 1.4052479893040012e-05, 'batch_size': 238, 'step_size': 7, 'gamma': 0.8814362484207894}. Best is trial 7 with value: 0.036842404320688533.[0m
[32m[I 2024-12-30 11:45:56,831][0m Trial 10 finished with value: 0.05220976129894247 and parameters: {'observation_period_num': 31, 'train_rates': 0.8364022875212833, 'learning_rate': 9.016013283583935e-05, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8220633456822074}. Best is trial 7 with value: 0.036842404320688533.[0m
[32m[I 2024-12-30 11:49:36,505][0m Trial 11 finished with value: 0.03868335464171001 and parameters: {'observation_period_num': 11, 'train_rates': 0.844716204971777, 'learning_rate': 9.383068348033754e-05, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8221854398275136}. Best is trial 7 with value: 0.036842404320688533.[0m
[32m[I 2024-12-30 11:53:23,470][0m Trial 12 finished with value: 0.036265853001216874 and parameters: {'observation_period_num': 6, 'train_rates': 0.8844636033918444, 'learning_rate': 0.00013102074237674412, 'batch_size': 177, 'step_size': 12, 'gamma': 0.8287261052406978}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 11:57:03,862][0m Trial 13 finished with value: 0.10822672125052761 and parameters: {'observation_period_num': 59, 'train_rates': 0.8973075222409406, 'learning_rate': 4.749817764250462e-06, 'batch_size': 252, 'step_size': 12, 'gamma': 0.827640609627825}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:01:03,346][0m Trial 14 finished with value: 0.09441731870174408 and parameters: {'observation_period_num': 68, 'train_rates': 0.9852290823095543, 'learning_rate': 0.0002229288803311278, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8399187315193735}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:04:49,354][0m Trial 15 finished with value: 0.1726701998187553 and parameters: {'observation_period_num': 5, 'train_rates': 0.7690085358406468, 'learning_rate': 5.660010457619765e-05, 'batch_size': 118, 'step_size': 13, 'gamma': 0.755062760624163}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:08:22,981][0m Trial 16 finished with value: 0.1152124778113582 and parameters: {'observation_period_num': 136, 'train_rates': 0.8823895063655155, 'learning_rate': 0.00019623329127786706, 'batch_size': 219, 'step_size': 15, 'gamma': 0.7997248545038913}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:11:48,125][0m Trial 17 finished with value: 0.2567972088420151 and parameters: {'observation_period_num': 41, 'train_rates': 0.742930349481455, 'learning_rate': 6.765024655765938e-06, 'batch_size': 177, 'step_size': 10, 'gamma': 0.8505971465197311}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:15:45,475][0m Trial 18 finished with value: 0.0812701149671166 and parameters: {'observation_period_num': 110, 'train_rates': 0.9428710805164991, 'learning_rate': 5.066378858338514e-05, 'batch_size': 136, 'step_size': 5, 'gamma': 0.9165058847806395}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:19:23,566][0m Trial 19 finished with value: 0.08351578154625036 and parameters: {'observation_period_num': 58, 'train_rates': 0.8647755286380945, 'learning_rate': 3.319623549882406e-05, 'batch_size': 178, 'step_size': 12, 'gamma': 0.7989442961668176}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:22:17,658][0m Trial 20 finished with value: 0.14049612066430234 and parameters: {'observation_period_num': 5, 'train_rates': 0.6054377729088755, 'learning_rate': 0.00013682469178009338, 'batch_size': 227, 'step_size': 8, 'gamma': 0.8557332922080788}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:25:52,380][0m Trial 21 finished with value: 0.040116952232471324 and parameters: {'observation_period_num': 21, 'train_rates': 0.8314733643147489, 'learning_rate': 0.000101872025980317, 'batch_size': 168, 'step_size': 11, 'gamma': 0.8179326907767054}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:29:51,142][0m Trial 22 finished with value: 0.043581862424740374 and parameters: {'observation_period_num': 6, 'train_rates': 0.8799156528263236, 'learning_rate': 0.0003946986497060425, 'batch_size': 142, 'step_size': 13, 'gamma': 0.784375426498695}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:33:21,384][0m Trial 23 finished with value: 0.07319322764791929 and parameters: {'observation_period_num': 47, 'train_rates': 0.8106595636845921, 'learning_rate': 3.617465584228989e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.8078459099830005}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:37:22,379][0m Trial 24 finished with value: 0.052319784249578206 and parameters: {'observation_period_num': 26, 'train_rates': 0.9185953037213741, 'learning_rate': 0.0001369837850082993, 'batch_size': 156, 'step_size': 12, 'gamma': 0.8377458251129958}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:41:11,231][0m Trial 25 finished with value: 0.08652597181225756 and parameters: {'observation_period_num': 79, 'train_rates': 0.8570864921881863, 'learning_rate': 0.00035032794151594414, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8591692894682477}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:46:01,281][0m Trial 26 finished with value: 0.11931481701071545 and parameters: {'observation_period_num': 105, 'train_rates': 0.9672663816105795, 'learning_rate': 7.689554330629647e-05, 'batch_size': 83, 'step_size': 11, 'gamma': 0.7505634473483002}. Best is trial 12 with value: 0.036265853001216874.[0m
[32m[I 2024-12-30 12:49:51,363][0m Trial 27 finished with value: 0.03598401153267915 and parameters: {'observation_period_num': 20, 'train_rates': 0.9038493802150087, 'learning_rate': 0.0007536950053643158, 'batch_size': 212, 'step_size': 9, 'gamma': 0.8107415679438772}. Best is trial 27 with value: 0.03598401153267915.[0m
[32m[I 2024-12-30 12:53:42,126][0m Trial 28 finished with value: 0.061765801161527634 and parameters: {'observation_period_num': 44, 'train_rates': 0.9407689522849536, 'learning_rate': 0.0006687438153039427, 'batch_size': 215, 'step_size': 9, 'gamma': 0.7766038891882324}. Best is trial 27 with value: 0.03598401153267915.[0m
Early stopping at epoch 84
[32m[I 2024-12-30 12:56:41,975][0m Trial 29 finished with value: 1.4813716654344038 and parameters: {'observation_period_num': 187, 'train_rates': 0.9001796495545356, 'learning_rate': 1.3715936333663729e-06, 'batch_size': 241, 'step_size': 2, 'gamma': 0.7756003183248086}. Best is trial 27 with value: 0.03598401153267915.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2024-12-30 12:56:41,980][0m A new study created in memory with name: no-name-fe582bd0-c85e-48c5-b26b-d6b236c1a544[0m
[32m[I 2024-12-30 12:59:57,352][0m Trial 0 finished with value: 0.29108896385878325 and parameters: {'observation_period_num': 127, 'train_rates': 0.755503266899374, 'learning_rate': 0.0008660462824162684, 'batch_size': 215, 'step_size': 14, 'gamma': 0.8494523084229586}. Best is trial 0 with value: 0.29108896385878325.[0m
[32m[I 2024-12-30 13:02:58,140][0m Trial 1 finished with value: 0.45017945466928144 and parameters: {'observation_period_num': 171, 'train_rates': 0.7125619850805308, 'learning_rate': 1.1378211518067574e-05, 'batch_size': 248, 'step_size': 4, 'gamma': 0.9224710635770632}. Best is trial 0 with value: 0.29108896385878325.[0m
[32m[I 2024-12-30 13:06:38,053][0m Trial 2 finished with value: 0.10192106001369365 and parameters: {'observation_period_num': 120, 'train_rates': 0.8909516267551324, 'learning_rate': 0.000995755200450375, 'batch_size': 185, 'step_size': 2, 'gamma': 0.8360126282104429}. Best is trial 2 with value: 0.10192106001369365.[0m
[32m[I 2024-12-30 13:10:23,027][0m Trial 3 finished with value: 0.08981215643627634 and parameters: {'observation_period_num': 83, 'train_rates': 0.9101104004500077, 'learning_rate': 7.7825961049474e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.9086222706113964}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:14:02,967][0m Trial 4 finished with value: 0.18400494754314423 and parameters: {'observation_period_num': 164, 'train_rates': 0.9393513775577707, 'learning_rate': 5.86557630652083e-06, 'batch_size': 232, 'step_size': 7, 'gamma': 0.905907575116565}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:27:05,131][0m Trial 5 finished with value: 0.11093597023086693 and parameters: {'observation_period_num': 212, 'train_rates': 0.9039563652257923, 'learning_rate': 0.0001749670383350689, 'batch_size': 25, 'step_size': 2, 'gamma': 0.8104381106970264}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:40:31,266][0m Trial 6 finished with value: 0.13978776345317512 and parameters: {'observation_period_num': 111, 'train_rates': 0.7941800047589055, 'learning_rate': 5.540971592652629e-05, 'batch_size': 23, 'step_size': 12, 'gamma': 0.9455441465414235}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:44:17,350][0m Trial 7 finished with value: 0.12169527262449265 and parameters: {'observation_period_num': 105, 'train_rates': 0.9368845445182418, 'learning_rate': 3.118631692608485e-05, 'batch_size': 198, 'step_size': 2, 'gamma': 0.9813705952303875}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:47:52,667][0m Trial 8 finished with value: 0.14480294870536128 and parameters: {'observation_period_num': 16, 'train_rates': 0.6573414072891487, 'learning_rate': 0.0004289199045660412, 'batch_size': 93, 'step_size': 4, 'gamma': 0.8238171504483938}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:51:25,665][0m Trial 9 finished with value: 0.11375116292507417 and parameters: {'observation_period_num': 107, 'train_rates': 0.8360346240897528, 'learning_rate': 0.0001602310552999455, 'batch_size': 181, 'step_size': 15, 'gamma': 0.9312426825408996}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:55:44,731][0m Trial 10 finished with value: 0.18903926014900208 and parameters: {'observation_period_num': 40, 'train_rates': 0.9808090340839355, 'learning_rate': 1.0356166712397145e-06, 'batch_size': 112, 'step_size': 10, 'gamma': 0.7735094025790178}. Best is trial 3 with value: 0.08981215643627634.[0m
[32m[I 2024-12-30 13:59:28,742][0m Trial 11 finished with value: 0.06938171481029896 and parameters: {'observation_period_num': 64, 'train_rates': 0.8626661772427943, 'learning_rate': 0.00019256133843654818, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8754024363704432}. Best is trial 11 with value: 0.06938171481029896.[0m
[32m[I 2024-12-30 14:03:08,266][0m Trial 12 finished with value: 0.06298952495798152 and parameters: {'observation_period_num': 62, 'train_rates': 0.8368948812057442, 'learning_rate': 9.364309899894478e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.8762315892836351}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:06:51,597][0m Trial 13 finished with value: 0.07739970363352609 and parameters: {'observation_period_num': 59, 'train_rates': 0.8404689663179937, 'learning_rate': 0.00022286677252428353, 'batch_size': 148, 'step_size': 8, 'gamma': 0.8762677412130644}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:10:53,719][0m Trial 14 finished with value: 0.06965705569622652 and parameters: {'observation_period_num': 55, 'train_rates': 0.8348866030887242, 'learning_rate': 1.6604716027117652e-05, 'batch_size': 108, 'step_size': 10, 'gamma': 0.8744583245136085}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:14:53,403][0m Trial 15 finished with value: 0.27520009285971486 and parameters: {'observation_period_num': 12, 'train_rates': 0.606189485531369, 'learning_rate': 6.208290445612965e-05, 'batch_size': 73, 'step_size': 6, 'gamma': 0.7916004839958909}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:18:20,511][0m Trial 16 finished with value: 0.256122388104175 and parameters: {'observation_period_num': 79, 'train_rates': 0.7699118479361089, 'learning_rate': 0.00041478752163159915, 'batch_size': 140, 'step_size': 9, 'gamma': 0.8708589510484145}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:23:39,450][0m Trial 17 finished with value: 0.12520169061422348 and parameters: {'observation_period_num': 164, 'train_rates': 0.8648174912120951, 'learning_rate': 0.00010293612932337412, 'batch_size': 63, 'step_size': 12, 'gamma': 0.7556513494379533}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:27:07,499][0m Trial 18 finished with value: 0.19616913260605834 and parameters: {'observation_period_num': 38, 'train_rates': 0.726403474670474, 'learning_rate': 2.924781726388239e-05, 'batch_size': 129, 'step_size': 5, 'gamma': 0.9687039290337766}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:30:39,788][0m Trial 19 finished with value: 0.13059572366679587 and parameters: {'observation_period_num': 74, 'train_rates': 0.8139218800370347, 'learning_rate': 3.3698003445859675e-06, 'batch_size': 156, 'step_size': 12, 'gamma': 0.8532175848133089}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:36:27,828][0m Trial 20 finished with value: 0.08367427438497543 and parameters: {'observation_period_num': 144, 'train_rates': 0.9885770018759956, 'learning_rate': 0.0003551518478419666, 'batch_size': 66, 'step_size': 7, 'gamma': 0.888016092565889}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:40:23,824][0m Trial 21 finished with value: 0.06390634397494382 and parameters: {'observation_period_num': 46, 'train_rates': 0.8598090705132957, 'learning_rate': 1.2831417120032748e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.8911229592010582}. Best is trial 12 with value: 0.06298952495798152.[0m
[32m[I 2024-12-30 14:44:17,743][0m Trial 22 finished with value: 0.04792458636530221 and parameters: {'observation_period_num': 33, 'train_rates': 0.8603999334232927, 'learning_rate': 1.7603244570286414e-05, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8864607795968951}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 14:47:57,021][0m Trial 23 finished with value: 0.057914486626954115 and parameters: {'observation_period_num': 33, 'train_rates': 0.7992681578738, 'learning_rate': 1.452140740080042e-05, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8934167586097863}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 14:52:10,362][0m Trial 24 finished with value: 0.18423838509031218 and parameters: {'observation_period_num': 5, 'train_rates': 0.7850190314016933, 'learning_rate': 7.140872877153513e-06, 'batch_size': 94, 'step_size': 11, 'gamma': 0.9458510378744427}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 14:55:36,624][0m Trial 25 finished with value: 0.18758941160710266 and parameters: {'observation_period_num': 28, 'train_rates': 0.7316132677382688, 'learning_rate': 2.093993264145571e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.8536766688712037}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 15:00:07,709][0m Trial 26 finished with value: 0.057632506970549 and parameters: {'observation_period_num': 29, 'train_rates': 0.8242270755239968, 'learning_rate': 3.129624020604556e-06, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8988359830112068}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 15:06:35,275][0m Trial 27 finished with value: 0.19391185711411868 and parameters: {'observation_period_num': 27, 'train_rates': 0.6788510313933729, 'learning_rate': 2.144478193022039e-06, 'batch_size': 44, 'step_size': 13, 'gamma': 0.904339556705932}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 15:10:36,764][0m Trial 28 finished with value: 0.23586871484877714 and parameters: {'observation_period_num': 226, 'train_rates': 0.8058871063576049, 'learning_rate': 3.1981936901883703e-06, 'batch_size': 88, 'step_size': 13, 'gamma': 0.9357140492545117}. Best is trial 22 with value: 0.04792458636530221.[0m
[32m[I 2024-12-30 15:14:44,815][0m Trial 29 finished with value: 0.41775846247396065 and parameters: {'observation_period_num': 91, 'train_rates': 0.7765987316806823, 'learning_rate': 1.0915626382769519e-06, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8481200744056482}. Best is trial 22 with value: 0.04792458636530221.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2024-12-30 15:14:44,820][0m A new study created in memory with name: no-name-cdb2d4b9-1cb7-4589-bc97-da9a5d479a42[0m
[32m[I 2024-12-30 15:18:27,922][0m Trial 0 finished with value: 0.11420270800590515 and parameters: {'observation_period_num': 175, 'train_rates': 0.9725301538903182, 'learning_rate': 0.00032048785988489267, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8985594054939599}. Best is trial 0 with value: 0.11420270800590515.[0m
[32m[I 2024-12-30 15:21:23,923][0m Trial 1 finished with value: 0.5274848106126078 and parameters: {'observation_period_num': 69, 'train_rates': 0.6011167891723415, 'learning_rate': 1.0925949767791743e-06, 'batch_size': 157, 'step_size': 11, 'gamma': 0.913316845749736}. Best is trial 0 with value: 0.11420270800590515.[0m
Early stopping at epoch 63
[32m[I 2024-12-30 15:23:38,772][0m Trial 2 finished with value: 0.3091919975278629 and parameters: {'observation_period_num': 165, 'train_rates': 0.7987373099510817, 'learning_rate': 4.874855291478238e-05, 'batch_size': 135, 'step_size': 1, 'gamma': 0.8077731284110147}. Best is trial 0 with value: 0.11420270800590515.[0m
[32m[I 2024-12-30 15:26:49,887][0m Trial 3 finished with value: 0.3716608149454906 and parameters: {'observation_period_num': 208, 'train_rates': 0.7178532260639509, 'learning_rate': 6.347774351781812e-05, 'batch_size': 134, 'step_size': 6, 'gamma': 0.8054627240866306}. Best is trial 0 with value: 0.11420270800590515.[0m
[32m[I 2024-12-30 15:30:29,486][0m Trial 4 finished with value: 0.10464487734034714 and parameters: {'observation_period_num': 65, 'train_rates': 0.8171604037031973, 'learning_rate': 0.00021436056572292334, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8289039991761998}. Best is trial 4 with value: 0.10464487734034714.[0m
[32m[I 2024-12-30 15:34:00,139][0m Trial 5 finished with value: 0.10028969895118361 and parameters: {'observation_period_num': 134, 'train_rates': 0.8322879813894535, 'learning_rate': 0.00012372659353156016, 'batch_size': 159, 'step_size': 4, 'gamma': 0.8665457996350694}. Best is trial 5 with value: 0.10028969895118361.[0m
[32m[I 2024-12-30 15:37:49,672][0m Trial 6 finished with value: 0.08563371840255712 and parameters: {'observation_period_num': 32, 'train_rates': 0.8949131613455017, 'learning_rate': 6.0713242138140515e-06, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8560455359794901}. Best is trial 6 with value: 0.08563371840255712.[0m
[32m[I 2024-12-30 15:40:53,886][0m Trial 7 finished with value: 0.4510422276763954 and parameters: {'observation_period_num': 89, 'train_rates': 0.6468044706826843, 'learning_rate': 3.0808111537745103e-06, 'batch_size': 200, 'step_size': 7, 'gamma': 0.7639646444528119}. Best is trial 6 with value: 0.08563371840255712.[0m
[32m[I 2024-12-30 15:45:01,897][0m Trial 8 finished with value: 0.18555506070454916 and parameters: {'observation_period_num': 242, 'train_rates': 0.8988746852889214, 'learning_rate': 0.00012124848281787219, 'batch_size': 91, 'step_size': 7, 'gamma': 0.8969202626026505}. Best is trial 6 with value: 0.08563371840255712.[0m
[32m[I 2024-12-30 15:48:49,738][0m Trial 9 finished with value: 0.13422104716300964 and parameters: {'observation_period_num': 93, 'train_rates': 0.9808673260220399, 'learning_rate': 2.4378060491855424e-05, 'batch_size': 242, 'step_size': 12, 'gamma': 0.9254329465138789}. Best is trial 6 with value: 0.08563371840255712.[0m
[32m[I 2024-12-30 16:08:29,832][0m Trial 10 finished with value: 0.09879882413893938 and parameters: {'observation_period_num': 48, 'train_rates': 0.8965311716196469, 'learning_rate': 9.01927970311776e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9800554062835586}. Best is trial 6 with value: 0.08563371840255712.[0m
[32m[I 2024-12-30 16:24:37,989][0m Trial 11 finished with value: 0.032421076534823935 and parameters: {'observation_period_num': 5, 'train_rates': 0.8876291712995726, 'learning_rate': 9.842250568398566e-06, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9846904717498514}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 16:43:30,561][0m Trial 12 finished with value: 0.05566018869259716 and parameters: {'observation_period_num': 13, 'train_rates': 0.8842974898134168, 'learning_rate': 8.754921904828098e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.983896142946401}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:00:30,108][0m Trial 13 finished with value: 0.1794381943350883 and parameters: {'observation_period_num': 8, 'train_rates': 0.7377766263071015, 'learning_rate': 1.192771493615461e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9664779385603708}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:06:06,370][0m Trial 14 finished with value: 0.05904962941638082 and parameters: {'observation_period_num': 11, 'train_rates': 0.8611033278676372, 'learning_rate': 1.5696660019189524e-06, 'batch_size': 60, 'step_size': 13, 'gamma': 0.9564103716191803}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:12:08,649][0m Trial 15 finished with value: 0.036380124026133366 and parameters: {'observation_period_num': 5, 'train_rates': 0.9346445127344344, 'learning_rate': 0.0008256236466811789, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9895744945939942}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:17:39,031][0m Trial 16 finished with value: 0.08736803998448411 and parameters: {'observation_period_num': 99, 'train_rates': 0.9485464953814446, 'learning_rate': 0.0008617015570794091, 'batch_size': 65, 'step_size': 13, 'gamma': 0.9401602482703935}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:24:22,734][0m Trial 17 finished with value: 0.07055182182702466 and parameters: {'observation_period_num': 39, 'train_rates': 0.9286525608171925, 'learning_rate': 0.0007670068559566169, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9410615499263798}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:27:58,005][0m Trial 18 finished with value: 0.3364151039444808 and parameters: {'observation_period_num': 136, 'train_rates': 0.7486994396783313, 'learning_rate': 1.6528136734991366e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.9835051806965563}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:37:12,459][0m Trial 19 finished with value: 0.07890871222372409 and parameters: {'observation_period_num': 58, 'train_rates': 0.9346244135094117, 'learning_rate': 3.933147726432701e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9521722683975865}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:41:26,461][0m Trial 20 finished with value: 0.12079859056123873 and parameters: {'observation_period_num': 112, 'train_rates': 0.8552475284999579, 'learning_rate': 3.671611452321919e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.8814332977502326}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:50:46,117][0m Trial 21 finished with value: 0.0333815644616666 and parameters: {'observation_period_num': 7, 'train_rates': 0.8823595402886347, 'learning_rate': 6.26990596520628e-06, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9889853181908689}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 17:57:24,889][0m Trial 22 finished with value: 0.20954132154355384 and parameters: {'observation_period_num': 31, 'train_rates': 0.7759761236684857, 'learning_rate': 3.390911695059455e-06, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9662003331007163}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:02:48,745][0m Trial 23 finished with value: 0.05942059220636592 and parameters: {'observation_period_num': 26, 'train_rates': 0.930133834343859, 'learning_rate': 1.6740872512925203e-05, 'batch_size': 72, 'step_size': 14, 'gamma': 0.9868176903190354}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:11:36,188][0m Trial 24 finished with value: 0.044060592599418184 and parameters: {'observation_period_num': 10, 'train_rates': 0.8512969196520318, 'learning_rate': 2.2728769720093968e-06, 'batch_size': 38, 'step_size': 14, 'gamma': 0.9275663824444377}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:16:43,252][0m Trial 25 finished with value: 0.09886278212070465 and parameters: {'observation_period_num': 72, 'train_rates': 0.989137405845028, 'learning_rate': 4.6943495687581795e-06, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9682880518703928}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:20:51,783][0m Trial 26 finished with value: 0.09327474975725636 and parameters: {'observation_period_num': 45, 'train_rates': 0.911668120278343, 'learning_rate': 0.00043422813997819894, 'batch_size': 108, 'step_size': 15, 'gamma': 0.9399876498349647}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:31:09,626][0m Trial 27 finished with value: 0.04253831505775452 and parameters: {'observation_period_num': 23, 'train_rates': 0.9519781742001963, 'learning_rate': 1.8977101133868222e-05, 'batch_size': 35, 'step_size': 11, 'gamma': 0.7506554224634487}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:41:17,015][0m Trial 28 finished with value: 0.10974995231037367 and parameters: {'observation_period_num': 49, 'train_rates': 0.8763828762671826, 'learning_rate': 7.165224602551958e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9896692044927516}. Best is trial 11 with value: 0.032421076534823935.[0m
[32m[I 2024-12-30 18:46:12,459][0m Trial 29 finished with value: 0.1607734746846956 and parameters: {'observation_period_num': 158, 'train_rates': 0.9651103923674318, 'learning_rate': 7.003528591468262e-06, 'batch_size': 78, 'step_size': 9, 'gamma': 0.9138557215139997}. Best is trial 11 with value: 0.032421076534823935.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 14, 'train_rates': 0.8221024631253029, 'learning_rate': 1.89417747404024e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.7796399472014476}
Epoch 1/300, trend Loss: 0.3610 | 0.2259
Epoch 2/300, trend Loss: 0.2066 | 0.1448
Epoch 3/300, trend Loss: 0.2819 | 0.1287
Epoch 4/300, trend Loss: 0.1865 | 0.1475
Epoch 5/300, trend Loss: 0.1543 | 0.1115
Epoch 6/300, trend Loss: 0.1605 | 0.1066
Epoch 7/300, trend Loss: 0.1820 | 0.0890
Epoch 8/300, trend Loss: 0.1434 | 0.1043
Epoch 9/300, trend Loss: 0.1364 | 0.0953
Epoch 10/300, trend Loss: 0.1284 | 0.0750
Epoch 11/300, trend Loss: 0.1238 | 0.0743
Epoch 12/300, trend Loss: 0.1188 | 0.0746
Epoch 13/300, trend Loss: 0.1170 | 0.0767
Epoch 14/300, trend Loss: 0.1151 | 0.0739
Epoch 15/300, trend Loss: 0.1132 | 0.0683
Epoch 16/300, trend Loss: 0.1129 | 0.0681
Epoch 17/300, trend Loss: 0.1108 | 0.0678
Epoch 18/300, trend Loss: 0.1095 | 0.0680
Epoch 19/300, trend Loss: 0.1093 | 0.0661
Epoch 20/300, trend Loss: 0.1083 | 0.0639
Epoch 21/300, trend Loss: 0.1070 | 0.0640
Epoch 22/300, trend Loss: 0.1057 | 0.0631
Epoch 23/300, trend Loss: 0.1047 | 0.0631
Epoch 24/300, trend Loss: 0.1039 | 0.0621
Epoch 25/300, trend Loss: 0.1029 | 0.0606
Epoch 26/300, trend Loss: 0.1022 | 0.0604
Epoch 27/300, trend Loss: 0.1018 | 0.0602
Epoch 28/300, trend Loss: 0.1010 | 0.0602
Epoch 29/300, trend Loss: 0.1003 | 0.0595
Epoch 30/300, trend Loss: 0.0996 | 0.0584
Epoch 31/300, trend Loss: 0.0992 | 0.0580
Epoch 32/300, trend Loss: 0.0986 | 0.0578
Epoch 33/300, trend Loss: 0.0979 | 0.0575
Epoch 34/300, trend Loss: 0.0976 | 0.0571
Epoch 35/300, trend Loss: 0.0973 | 0.0565
Epoch 36/300, trend Loss: 0.0969 | 0.0561
Epoch 37/300, trend Loss: 0.0962 | 0.0560
Epoch 38/300, trend Loss: 0.0957 | 0.0559
Epoch 39/300, trend Loss: 0.0953 | 0.0557
Epoch 40/300, trend Loss: 0.0949 | 0.0555
Epoch 41/300, trend Loss: 0.0946 | 0.0550
Epoch 42/300, trend Loss: 0.0942 | 0.0549
Epoch 43/300, trend Loss: 0.0939 | 0.0547
Epoch 44/300, trend Loss: 0.0936 | 0.0544
Epoch 45/300, trend Loss: 0.0933 | 0.0541
Epoch 46/300, trend Loss: 0.0930 | 0.0538
Epoch 47/300, trend Loss: 0.0927 | 0.0536
Epoch 48/300, trend Loss: 0.0924 | 0.0535
Epoch 49/300, trend Loss: 0.0921 | 0.0534
Epoch 50/300, trend Loss: 0.0918 | 0.0533
Epoch 51/300, trend Loss: 0.0916 | 0.0532
Epoch 52/300, trend Loss: 0.0913 | 0.0530
Epoch 53/300, trend Loss: 0.0911 | 0.0529
Epoch 54/300, trend Loss: 0.0909 | 0.0526
Epoch 55/300, trend Loss: 0.0907 | 0.0524
Epoch 56/300, trend Loss: 0.0905 | 0.0522
Epoch 57/300, trend Loss: 0.0903 | 0.0521
Epoch 58/300, trend Loss: 0.0901 | 0.0520
Epoch 59/300, trend Loss: 0.0898 | 0.0519
Epoch 60/300, trend Loss: 0.0896 | 0.0519
Epoch 61/300, trend Loss: 0.0894 | 0.0518
Epoch 62/300, trend Loss: 0.0893 | 0.0517
Epoch 63/300, trend Loss: 0.0891 | 0.0515
Epoch 64/300, trend Loss: 0.0889 | 0.0513
Epoch 65/300, trend Loss: 0.0888 | 0.0512
Epoch 66/300, trend Loss: 0.0886 | 0.0510
Epoch 67/300, trend Loss: 0.0885 | 0.0510
Epoch 68/300, trend Loss: 0.0883 | 0.0510
Epoch 69/300, trend Loss: 0.0882 | 0.0509
Epoch 70/300, trend Loss: 0.0881 | 0.0508
Epoch 71/300, trend Loss: 0.0879 | 0.0507
Epoch 72/300, trend Loss: 0.0878 | 0.0506
Epoch 73/300, trend Loss: 0.0877 | 0.0505
Epoch 74/300, trend Loss: 0.0876 | 0.0504
Epoch 75/300, trend Loss: 0.0874 | 0.0504
Epoch 76/300, trend Loss: 0.0873 | 0.0503
Epoch 77/300, trend Loss: 0.0872 | 0.0502
Epoch 78/300, trend Loss: 0.0871 | 0.0502
Epoch 79/300, trend Loss: 0.0870 | 0.0501
Epoch 80/300, trend Loss: 0.0869 | 0.0500
Epoch 81/300, trend Loss: 0.0868 | 0.0500
Epoch 82/300, trend Loss: 0.0868 | 0.0499
Epoch 83/300, trend Loss: 0.0867 | 0.0499
Epoch 84/300, trend Loss: 0.0866 | 0.0498
Epoch 85/300, trend Loss: 0.0865 | 0.0497
Epoch 86/300, trend Loss: 0.0864 | 0.0497
Epoch 87/300, trend Loss: 0.0863 | 0.0496
Epoch 88/300, trend Loss: 0.0863 | 0.0496
Epoch 89/300, trend Loss: 0.0862 | 0.0496
Epoch 90/300, trend Loss: 0.0861 | 0.0495
Epoch 91/300, trend Loss: 0.0861 | 0.0495
Epoch 92/300, trend Loss: 0.0860 | 0.0494
Epoch 93/300, trend Loss: 0.0859 | 0.0494
Epoch 94/300, trend Loss: 0.0859 | 0.0493
Epoch 95/300, trend Loss: 0.0858 | 0.0493
Epoch 96/300, trend Loss: 0.0858 | 0.0493
Epoch 97/300, trend Loss: 0.0857 | 0.0492
Epoch 98/300, trend Loss: 0.0857 | 0.0492
Epoch 99/300, trend Loss: 0.0856 | 0.0492
Epoch 100/300, trend Loss: 0.0856 | 0.0491
Epoch 101/300, trend Loss: 0.0855 | 0.0491
Epoch 102/300, trend Loss: 0.0855 | 0.0491
Epoch 103/300, trend Loss: 0.0854 | 0.0490
Epoch 104/300, trend Loss: 0.0854 | 0.0490
Epoch 105/300, trend Loss: 0.0853 | 0.0490
Epoch 106/300, trend Loss: 0.0853 | 0.0490
Epoch 107/300, trend Loss: 0.0853 | 0.0489
Epoch 108/300, trend Loss: 0.0852 | 0.0489
Epoch 109/300, trend Loss: 0.0852 | 0.0489
Epoch 110/300, trend Loss: 0.0852 | 0.0489
Epoch 111/300, trend Loss: 0.0851 | 0.0488
Epoch 112/300, trend Loss: 0.0851 | 0.0488
Epoch 113/300, trend Loss: 0.0851 | 0.0488
Epoch 114/300, trend Loss: 0.0850 | 0.0488
Epoch 115/300, trend Loss: 0.0850 | 0.0488
Epoch 116/300, trend Loss: 0.0850 | 0.0487
Epoch 117/300, trend Loss: 0.0850 | 0.0487
Epoch 118/300, trend Loss: 0.0849 | 0.0487
Epoch 119/300, trend Loss: 0.0849 | 0.0487
Epoch 120/300, trend Loss: 0.0849 | 0.0487
Epoch 121/300, trend Loss: 0.0849 | 0.0487
Epoch 122/300, trend Loss: 0.0848 | 0.0486
Epoch 123/300, trend Loss: 0.0848 | 0.0486
Epoch 124/300, trend Loss: 0.0848 | 0.0486
Epoch 125/300, trend Loss: 0.0848 | 0.0486
Epoch 126/300, trend Loss: 0.0848 | 0.0486
Epoch 127/300, trend Loss: 0.0847 | 0.0486
Epoch 128/300, trend Loss: 0.0847 | 0.0486
Epoch 129/300, trend Loss: 0.0847 | 0.0486
Epoch 130/300, trend Loss: 0.0847 | 0.0485
Epoch 131/300, trend Loss: 0.0847 | 0.0485
Epoch 132/300, trend Loss: 0.0846 | 0.0485
Epoch 133/300, trend Loss: 0.0846 | 0.0485
Epoch 134/300, trend Loss: 0.0846 | 0.0485
Epoch 135/300, trend Loss: 0.0846 | 0.0485
Epoch 136/300, trend Loss: 0.0846 | 0.0485
Epoch 137/300, trend Loss: 0.0846 | 0.0485
Epoch 138/300, trend Loss: 0.0846 | 0.0485
Epoch 139/300, trend Loss: 0.0845 | 0.0485
Epoch 140/300, trend Loss: 0.0845 | 0.0484
Epoch 141/300, trend Loss: 0.0845 | 0.0484
Epoch 142/300, trend Loss: 0.0845 | 0.0484
Epoch 143/300, trend Loss: 0.0845 | 0.0484
Epoch 144/300, trend Loss: 0.0845 | 0.0484
Epoch 145/300, trend Loss: 0.0845 | 0.0484
Epoch 146/300, trend Loss: 0.0845 | 0.0484
Epoch 147/300, trend Loss: 0.0845 | 0.0484
Epoch 148/300, trend Loss: 0.0845 | 0.0484
Epoch 149/300, trend Loss: 0.0844 | 0.0484
Epoch 150/300, trend Loss: 0.0844 | 0.0484
Epoch 151/300, trend Loss: 0.0844 | 0.0484
Epoch 152/300, trend Loss: 0.0844 | 0.0484
Epoch 153/300, trend Loss: 0.0844 | 0.0484
Epoch 154/300, trend Loss: 0.0844 | 0.0484
Epoch 155/300, trend Loss: 0.0844 | 0.0484
Epoch 156/300, trend Loss: 0.0844 | 0.0483
Epoch 157/300, trend Loss: 0.0844 | 0.0483
Epoch 158/300, trend Loss: 0.0844 | 0.0483
Epoch 159/300, trend Loss: 0.0844 | 0.0483
Epoch 160/300, trend Loss: 0.0844 | 0.0483
Epoch 161/300, trend Loss: 0.0844 | 0.0483
Epoch 162/300, trend Loss: 0.0844 | 0.0483
Epoch 163/300, trend Loss: 0.0843 | 0.0483
Epoch 164/300, trend Loss: 0.0843 | 0.0483
Epoch 165/300, trend Loss: 0.0843 | 0.0483
Epoch 166/300, trend Loss: 0.0843 | 0.0483
Epoch 167/300, trend Loss: 0.0843 | 0.0483
Epoch 168/300, trend Loss: 0.0843 | 0.0483
Epoch 169/300, trend Loss: 0.0843 | 0.0483
Epoch 170/300, trend Loss: 0.0843 | 0.0483
Epoch 171/300, trend Loss: 0.0843 | 0.0483
Epoch 172/300, trend Loss: 0.0843 | 0.0483
Epoch 173/300, trend Loss: 0.0843 | 0.0483
Epoch 174/300, trend Loss: 0.0843 | 0.0483
Epoch 175/300, trend Loss: 0.0843 | 0.0483
Epoch 176/300, trend Loss: 0.0843 | 0.0483
Epoch 177/300, trend Loss: 0.0843 | 0.0483
Epoch 178/300, trend Loss: 0.0843 | 0.0483
Epoch 179/300, trend Loss: 0.0843 | 0.0483
Epoch 180/300, trend Loss: 0.0843 | 0.0483
Epoch 181/300, trend Loss: 0.0843 | 0.0483
Epoch 182/300, trend Loss: 0.0843 | 0.0483
Epoch 183/300, trend Loss: 0.0843 | 0.0483
Epoch 184/300, trend Loss: 0.0843 | 0.0483
Epoch 185/300, trend Loss: 0.0843 | 0.0483
Epoch 186/300, trend Loss: 0.0843 | 0.0483
Epoch 187/300, trend Loss: 0.0843 | 0.0483
Epoch 188/300, trend Loss: 0.0843 | 0.0483
Epoch 189/300, trend Loss: 0.0843 | 0.0483
Epoch 190/300, trend Loss: 0.0843 | 0.0483
Epoch 191/300, trend Loss: 0.0843 | 0.0483
Epoch 192/300, trend Loss: 0.0842 | 0.0483
Epoch 193/300, trend Loss: 0.0842 | 0.0482
Epoch 194/300, trend Loss: 0.0842 | 0.0482
Epoch 195/300, trend Loss: 0.0842 | 0.0482
Epoch 196/300, trend Loss: 0.0842 | 0.0482
Epoch 197/300, trend Loss: 0.0842 | 0.0482
Epoch 198/300, trend Loss: 0.0842 | 0.0482
Epoch 199/300, trend Loss: 0.0842 | 0.0482
Epoch 200/300, trend Loss: 0.0842 | 0.0482
Epoch 201/300, trend Loss: 0.0842 | 0.0482
Epoch 202/300, trend Loss: 0.0842 | 0.0482
Epoch 203/300, trend Loss: 0.0842 | 0.0482
Epoch 204/300, trend Loss: 0.0842 | 0.0482
Epoch 205/300, trend Loss: 0.0842 | 0.0482
Epoch 206/300, trend Loss: 0.0842 | 0.0482
Epoch 207/300, trend Loss: 0.0842 | 0.0482
Epoch 208/300, trend Loss: 0.0842 | 0.0482
Epoch 209/300, trend Loss: 0.0842 | 0.0482
Epoch 210/300, trend Loss: 0.0842 | 0.0482
Epoch 211/300, trend Loss: 0.0842 | 0.0482
Epoch 212/300, trend Loss: 0.0842 | 0.0482
Epoch 213/300, trend Loss: 0.0842 | 0.0482
Epoch 214/300, trend Loss: 0.0842 | 0.0482
Epoch 215/300, trend Loss: 0.0842 | 0.0482
Epoch 216/300, trend Loss: 0.0842 | 0.0482
Epoch 217/300, trend Loss: 0.0842 | 0.0482
Epoch 218/300, trend Loss: 0.0842 | 0.0482
Epoch 219/300, trend Loss: 0.0842 | 0.0482
Epoch 220/300, trend Loss: 0.0842 | 0.0482
Epoch 221/300, trend Loss: 0.0842 | 0.0482
Epoch 222/300, trend Loss: 0.0842 | 0.0482
Epoch 223/300, trend Loss: 0.0842 | 0.0482
Epoch 224/300, trend Loss: 0.0842 | 0.0482
Epoch 225/300, trend Loss: 0.0842 | 0.0482
Epoch 226/300, trend Loss: 0.0842 | 0.0482
Epoch 227/300, trend Loss: 0.0842 | 0.0482
Epoch 228/300, trend Loss: 0.0842 | 0.0482
Epoch 229/300, trend Loss: 0.0842 | 0.0482
Epoch 230/300, trend Loss: 0.0842 | 0.0482
Epoch 231/300, trend Loss: 0.0842 | 0.0482
Epoch 232/300, trend Loss: 0.0842 | 0.0482
Epoch 233/300, trend Loss: 0.0842 | 0.0482
Epoch 234/300, trend Loss: 0.0842 | 0.0482
Epoch 235/300, trend Loss: 0.0842 | 0.0482
Epoch 236/300, trend Loss: 0.0842 | 0.0482
Epoch 237/300, trend Loss: 0.0842 | 0.0482
Epoch 238/300, trend Loss: 0.0842 | 0.0482
Epoch 239/300, trend Loss: 0.0842 | 0.0482
Epoch 240/300, trend Loss: 0.0842 | 0.0482
Epoch 241/300, trend Loss: 0.0842 | 0.0482
Epoch 242/300, trend Loss: 0.0842 | 0.0482
Epoch 243/300, trend Loss: 0.0842 | 0.0482
Epoch 244/300, trend Loss: 0.0842 | 0.0482
Epoch 245/300, trend Loss: 0.0842 | 0.0482
Epoch 246/300, trend Loss: 0.0842 | 0.0482
Epoch 247/300, trend Loss: 0.0842 | 0.0482
Epoch 248/300, trend Loss: 0.0842 | 0.0482
Epoch 249/300, trend Loss: 0.0842 | 0.0482
Epoch 250/300, trend Loss: 0.0842 | 0.0482
Epoch 251/300, trend Loss: 0.0842 | 0.0482
Epoch 252/300, trend Loss: 0.0842 | 0.0482
Epoch 253/300, trend Loss: 0.0842 | 0.0482
Epoch 254/300, trend Loss: 0.0842 | 0.0482
Epoch 255/300, trend Loss: 0.0842 | 0.0482
Epoch 256/300, trend Loss: 0.0842 | 0.0482
Epoch 257/300, trend Loss: 0.0842 | 0.0482
Epoch 258/300, trend Loss: 0.0842 | 0.0482
Epoch 259/300, trend Loss: 0.0842 | 0.0482
Epoch 260/300, trend Loss: 0.0842 | 0.0482
Epoch 261/300, trend Loss: 0.0842 | 0.0482
Epoch 262/300, trend Loss: 0.0842 | 0.0482
Epoch 263/300, trend Loss: 0.0842 | 0.0482
Epoch 264/300, trend Loss: 0.0842 | 0.0482
Epoch 265/300, trend Loss: 0.0842 | 0.0482
Epoch 266/300, trend Loss: 0.0842 | 0.0482
Epoch 267/300, trend Loss: 0.0842 | 0.0482
Epoch 268/300, trend Loss: 0.0842 | 0.0482
Epoch 269/300, trend Loss: 0.0842 | 0.0482
Epoch 270/300, trend Loss: 0.0842 | 0.0482
Epoch 271/300, trend Loss: 0.0842 | 0.0482
Epoch 272/300, trend Loss: 0.0842 | 0.0482
Epoch 273/300, trend Loss: 0.0842 | 0.0482
Epoch 274/300, trend Loss: 0.0842 | 0.0482
Epoch 275/300, trend Loss: 0.0842 | 0.0482
Epoch 276/300, trend Loss: 0.0842 | 0.0482
Epoch 277/300, trend Loss: 0.0842 | 0.0482
Epoch 278/300, trend Loss: 0.0842 | 0.0482
Epoch 279/300, trend Loss: 0.0842 | 0.0482
Epoch 280/300, trend Loss: 0.0842 | 0.0482
Epoch 281/300, trend Loss: 0.0842 | 0.0482
Epoch 282/300, trend Loss: 0.0842 | 0.0482
Epoch 283/300, trend Loss: 0.0842 | 0.0482
Epoch 284/300, trend Loss: 0.0842 | 0.0482
Epoch 285/300, trend Loss: 0.0842 | 0.0482
Epoch 286/300, trend Loss: 0.0842 | 0.0482
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 10, 'train_rates': 0.9434195722422634, 'learning_rate': 6.544776772204642e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.916341210826651}
Epoch 1/300, seasonal_0 Loss: 0.1993 | 0.1721
Epoch 2/300, seasonal_0 Loss: 0.1367 | 0.0898
Epoch 3/300, seasonal_0 Loss: 0.1159 | 0.0770
Epoch 4/300, seasonal_0 Loss: 0.1091 | 0.0859
Epoch 5/300, seasonal_0 Loss: 0.1024 | 0.0785
Epoch 6/300, seasonal_0 Loss: 0.0976 | 0.0694
Epoch 7/300, seasonal_0 Loss: 0.0940 | 0.0703
Epoch 8/300, seasonal_0 Loss: 0.0937 | 0.0692
Epoch 9/300, seasonal_0 Loss: 0.0964 | 0.0695
Epoch 10/300, seasonal_0 Loss: 0.0921 | 0.0556
Epoch 11/300, seasonal_0 Loss: 0.0866 | 0.0483
Epoch 12/300, seasonal_0 Loss: 0.0834 | 0.0454
Epoch 13/300, seasonal_0 Loss: 0.0803 | 0.0433
Epoch 14/300, seasonal_0 Loss: 0.0783 | 0.0426
Epoch 15/300, seasonal_0 Loss: 0.0764 | 0.0425
Epoch 16/300, seasonal_0 Loss: 0.0748 | 0.0420
Epoch 17/300, seasonal_0 Loss: 0.0736 | 0.0415
Epoch 18/300, seasonal_0 Loss: 0.0723 | 0.0411
Epoch 19/300, seasonal_0 Loss: 0.0719 | 0.0436
Epoch 20/300, seasonal_0 Loss: 0.0712 | 0.0422
Epoch 21/300, seasonal_0 Loss: 0.0697 | 0.0403
Epoch 22/300, seasonal_0 Loss: 0.0689 | 0.0411
Epoch 23/300, seasonal_0 Loss: 0.0686 | 0.0388
Epoch 24/300, seasonal_0 Loss: 0.0667 | 0.0412
Epoch 25/300, seasonal_0 Loss: 0.0678 | 0.0430
Epoch 26/300, seasonal_0 Loss: 0.0654 | 0.0402
Epoch 27/300, seasonal_0 Loss: 0.0651 | 0.0404
Epoch 28/300, seasonal_0 Loss: 0.0643 | 0.0405
Epoch 29/300, seasonal_0 Loss: 0.0641 | 0.0444
Epoch 30/300, seasonal_0 Loss: 0.0641 | 0.0337
Epoch 31/300, seasonal_0 Loss: 0.0629 | 0.0390
Epoch 32/300, seasonal_0 Loss: 0.0612 | 0.0347
Epoch 33/300, seasonal_0 Loss: 0.0598 | 0.0350
Epoch 34/300, seasonal_0 Loss: 0.0595 | 0.0350
Epoch 35/300, seasonal_0 Loss: 0.0593 | 0.0346
Epoch 36/300, seasonal_0 Loss: 0.0590 | 0.0346
Epoch 37/300, seasonal_0 Loss: 0.0587 | 0.0347
Epoch 38/300, seasonal_0 Loss: 0.0583 | 0.0348
Epoch 39/300, seasonal_0 Loss: 0.0580 | 0.0355
Epoch 40/300, seasonal_0 Loss: 0.0579 | 0.0401
Epoch 41/300, seasonal_0 Loss: 0.0586 | 0.0398
Epoch 42/300, seasonal_0 Loss: 0.0576 | 0.0359
Epoch 43/300, seasonal_0 Loss: 0.0573 | 0.0357
Epoch 44/300, seasonal_0 Loss: 0.0571 | 0.0345
Epoch 45/300, seasonal_0 Loss: 0.0573 | 0.0349
Epoch 46/300, seasonal_0 Loss: 0.0576 | 0.0340
Epoch 47/300, seasonal_0 Loss: 0.0564 | 0.0331
Epoch 48/300, seasonal_0 Loss: 0.0559 | 0.0334
Epoch 49/300, seasonal_0 Loss: 0.0555 | 0.0342
Epoch 50/300, seasonal_0 Loss: 0.0555 | 0.0349
Epoch 51/300, seasonal_0 Loss: 0.0558 | 0.0347
Epoch 52/300, seasonal_0 Loss: 0.0553 | 0.0342
Epoch 53/300, seasonal_0 Loss: 0.0557 | 0.0350
Epoch 54/300, seasonal_0 Loss: 0.0562 | 0.0361
Epoch 55/300, seasonal_0 Loss: 0.0568 | 0.0386
Epoch 56/300, seasonal_0 Loss: 0.0552 | 0.0393
Epoch 57/300, seasonal_0 Loss: 0.0549 | 0.0397
Epoch 58/300, seasonal_0 Loss: 0.0573 | 0.0397
Epoch 59/300, seasonal_0 Loss: 0.0555 | 0.0387
Epoch 60/300, seasonal_0 Loss: 0.0533 | 0.0400
Epoch 61/300, seasonal_0 Loss: 0.0516 | 0.0436
Epoch 62/300, seasonal_0 Loss: 0.0500 | 0.0469
Epoch 63/300, seasonal_0 Loss: 0.0485 | 0.0467
Epoch 64/300, seasonal_0 Loss: 0.0471 | 0.0476
Epoch 65/300, seasonal_0 Loss: 0.0462 | 0.0428
Epoch 66/300, seasonal_0 Loss: 0.0452 | 0.0438
Epoch 67/300, seasonal_0 Loss: 0.0446 | 0.0403
Epoch 68/300, seasonal_0 Loss: 0.0439 | 0.0417
Epoch 69/300, seasonal_0 Loss: 0.0434 | 0.0388
Epoch 70/300, seasonal_0 Loss: 0.0428 | 0.0394
Epoch 71/300, seasonal_0 Loss: 0.0424 | 0.0375
Epoch 72/300, seasonal_0 Loss: 0.0420 | 0.0381
Epoch 73/300, seasonal_0 Loss: 0.0416 | 0.0363
Epoch 74/300, seasonal_0 Loss: 0.0413 | 0.0370
Epoch 75/300, seasonal_0 Loss: 0.0410 | 0.0357
Epoch 76/300, seasonal_0 Loss: 0.0407 | 0.0361
Epoch 77/300, seasonal_0 Loss: 0.0405 | 0.0352
Epoch 78/300, seasonal_0 Loss: 0.0402 | 0.0357
Epoch 79/300, seasonal_0 Loss: 0.0400 | 0.0349
Epoch 80/300, seasonal_0 Loss: 0.0398 | 0.0354
Epoch 81/300, seasonal_0 Loss: 0.0396 | 0.0348
Epoch 82/300, seasonal_0 Loss: 0.0394 | 0.0352
Epoch 83/300, seasonal_0 Loss: 0.0393 | 0.0347
Epoch 84/300, seasonal_0 Loss: 0.0391 | 0.0351
Epoch 85/300, seasonal_0 Loss: 0.0390 | 0.0346
Epoch 86/300, seasonal_0 Loss: 0.0389 | 0.0351
Epoch 87/300, seasonal_0 Loss: 0.0387 | 0.0346
Epoch 88/300, seasonal_0 Loss: 0.0386 | 0.0350
Epoch 89/300, seasonal_0 Loss: 0.0385 | 0.0345
Epoch 90/300, seasonal_0 Loss: 0.0384 | 0.0351
Epoch 91/300, seasonal_0 Loss: 0.0383 | 0.0344
Epoch 92/300, seasonal_0 Loss: 0.0382 | 0.0351
Epoch 93/300, seasonal_0 Loss: 0.0381 | 0.0342
Epoch 94/300, seasonal_0 Loss: 0.0381 | 0.0352
Epoch 95/300, seasonal_0 Loss: 0.0380 | 0.0344
Epoch 96/300, seasonal_0 Loss: 0.0379 | 0.0351
Epoch 97/300, seasonal_0 Loss: 0.0378 | 0.0345
Epoch 98/300, seasonal_0 Loss: 0.0376 | 0.0349
Epoch 99/300, seasonal_0 Loss: 0.0375 | 0.0345
Epoch 100/300, seasonal_0 Loss: 0.0373 | 0.0347
Epoch 101/300, seasonal_0 Loss: 0.0372 | 0.0346
Epoch 102/300, seasonal_0 Loss: 0.0371 | 0.0347
Epoch 103/300, seasonal_0 Loss: 0.0370 | 0.0346
Epoch 104/300, seasonal_0 Loss: 0.0369 | 0.0346
Epoch 105/300, seasonal_0 Loss: 0.0368 | 0.0346
Epoch 106/300, seasonal_0 Loss: 0.0367 | 0.0347
Epoch 107/300, seasonal_0 Loss: 0.0367 | 0.0347
Epoch 108/300, seasonal_0 Loss: 0.0366 | 0.0347
Epoch 109/300, seasonal_0 Loss: 0.0365 | 0.0348
Epoch 110/300, seasonal_0 Loss: 0.0364 | 0.0347
Epoch 111/300, seasonal_0 Loss: 0.0364 | 0.0347
Epoch 112/300, seasonal_0 Loss: 0.0363 | 0.0348
Epoch 113/300, seasonal_0 Loss: 0.0362 | 0.0348
Epoch 114/300, seasonal_0 Loss: 0.0362 | 0.0348
Epoch 115/300, seasonal_0 Loss: 0.0361 | 0.0348
Epoch 116/300, seasonal_0 Loss: 0.0361 | 0.0348
Epoch 117/300, seasonal_0 Loss: 0.0361 | 0.0347
Epoch 118/300, seasonal_0 Loss: 0.0360 | 0.0347
Epoch 119/300, seasonal_0 Loss: 0.0360 | 0.0346
Epoch 120/300, seasonal_0 Loss: 0.0359 | 0.0346
Epoch 121/300, seasonal_0 Loss: 0.0359 | 0.0345
Epoch 122/300, seasonal_0 Loss: 0.0358 | 0.0346
Epoch 123/300, seasonal_0 Loss: 0.0358 | 0.0346
Epoch 124/300, seasonal_0 Loss: 0.0357 | 0.0345
Epoch 125/300, seasonal_0 Loss: 0.0357 | 0.0345
Epoch 126/300, seasonal_0 Loss: 0.0356 | 0.0345
Epoch 127/300, seasonal_0 Loss: 0.0356 | 0.0345
Epoch 128/300, seasonal_0 Loss: 0.0356 | 0.0345
Epoch 129/300, seasonal_0 Loss: 0.0355 | 0.0345
Epoch 130/300, seasonal_0 Loss: 0.0355 | 0.0345
Epoch 131/300, seasonal_0 Loss: 0.0354 | 0.0345
Epoch 132/300, seasonal_0 Loss: 0.0354 | 0.0345
Epoch 133/300, seasonal_0 Loss: 0.0353 | 0.0345
Epoch 134/300, seasonal_0 Loss: 0.0353 | 0.0345
Epoch 135/300, seasonal_0 Loss: 0.0352 | 0.0345
Epoch 136/300, seasonal_0 Loss: 0.0352 | 0.0345
Epoch 137/300, seasonal_0 Loss: 0.0351 | 0.0345
Epoch 138/300, seasonal_0 Loss: 0.0351 | 0.0345
Epoch 139/300, seasonal_0 Loss: 0.0350 | 0.0345
Epoch 140/300, seasonal_0 Loss: 0.0350 | 0.0345
Epoch 141/300, seasonal_0 Loss: 0.0350 | 0.0345
Epoch 142/300, seasonal_0 Loss: 0.0349 | 0.0345
Epoch 143/300, seasonal_0 Loss: 0.0349 | 0.0345
Epoch 144/300, seasonal_0 Loss: 0.0348 | 0.0345
Epoch 145/300, seasonal_0 Loss: 0.0348 | 0.0345
Epoch 146/300, seasonal_0 Loss: 0.0348 | 0.0345
Epoch 147/300, seasonal_0 Loss: 0.0348 | 0.0345
Epoch 148/300, seasonal_0 Loss: 0.0347 | 0.0346
Epoch 149/300, seasonal_0 Loss: 0.0347 | 0.0346
Epoch 150/300, seasonal_0 Loss: 0.0347 | 0.0346
Epoch 151/300, seasonal_0 Loss: 0.0347 | 0.0346
Epoch 152/300, seasonal_0 Loss: 0.0346 | 0.0346
Epoch 153/300, seasonal_0 Loss: 0.0346 | 0.0346
Epoch 154/300, seasonal_0 Loss: 0.0346 | 0.0346
Epoch 155/300, seasonal_0 Loss: 0.0346 | 0.0347
Epoch 156/300, seasonal_0 Loss: 0.0346 | 0.0347
Epoch 157/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 158/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 159/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 160/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 161/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 162/300, seasonal_0 Loss: 0.0345 | 0.0347
Epoch 163/300, seasonal_0 Loss: 0.0344 | 0.0347
Epoch 164/300, seasonal_0 Loss: 0.0344 | 0.0347
Epoch 165/300, seasonal_0 Loss: 0.0344 | 0.0347
Epoch 166/300, seasonal_0 Loss: 0.0344 | 0.0347
Epoch 167/300, seasonal_0 Loss: 0.0344 | 0.0347
Epoch 168/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 169/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 170/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 171/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 172/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 173/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 174/300, seasonal_0 Loss: 0.0343 | 0.0347
Epoch 175/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 176/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 177/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 178/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 179/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 180/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 181/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 182/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 183/300, seasonal_0 Loss: 0.0342 | 0.0347
Epoch 184/300, seasonal_0 Loss: 0.0341 | 0.0347
Epoch 185/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 186/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 187/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 188/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 189/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 190/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 191/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 192/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 193/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 194/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 195/300, seasonal_0 Loss: 0.0341 | 0.0348
Epoch 196/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 197/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 198/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 199/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 200/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 201/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 202/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 203/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 204/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 205/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 206/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 207/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 208/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 209/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 210/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 211/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 212/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 213/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 214/300, seasonal_0 Loss: 0.0340 | 0.0348
Epoch 215/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 216/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 217/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 218/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 219/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 220/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 221/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 222/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 223/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 224/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 225/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 226/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 227/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 228/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 229/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 230/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 231/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 232/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 233/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 234/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 235/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 236/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 237/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 238/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 239/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 240/300, seasonal_0 Loss: 0.0339 | 0.0348
Epoch 241/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 242/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 243/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 244/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 245/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 246/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 247/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 248/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 249/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 250/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 251/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 252/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 253/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 254/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 255/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 256/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 257/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 258/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 259/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 260/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 261/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 262/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 263/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 264/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 265/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 266/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 267/300, seasonal_0 Loss: 0.0339 | 0.0349
Epoch 268/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 269/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 270/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 271/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 272/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 273/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 274/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 275/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 276/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 277/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 278/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 279/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 280/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 281/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 282/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 283/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 284/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 285/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 286/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 287/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 288/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 289/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 290/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 291/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 292/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 293/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 294/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 295/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 296/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 297/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 298/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 299/300, seasonal_0 Loss: 0.0338 | 0.0349
Epoch 300/300, seasonal_0 Loss: 0.0338 | 0.0349
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.793253869243469, 'learning_rate': 4.176353471339515e-05, 'batch_size': 61, 'step_size': 5, 'gamma': 0.8264602579821321}
Epoch 1/300, seasonal_1 Loss: 0.3578 | 0.2117
Epoch 2/300, seasonal_1 Loss: 0.1595 | 0.1121
Epoch 3/300, seasonal_1 Loss: 0.1456 | 0.0936
Epoch 4/300, seasonal_1 Loss: 0.1313 | 0.0855
Epoch 5/300, seasonal_1 Loss: 0.1249 | 0.0781
Epoch 6/300, seasonal_1 Loss: 0.1215 | 0.0762
Epoch 7/300, seasonal_1 Loss: 0.1192 | 0.0734
Epoch 8/300, seasonal_1 Loss: 0.1155 | 0.0705
Epoch 9/300, seasonal_1 Loss: 0.1124 | 0.0756
Epoch 10/300, seasonal_1 Loss: 0.1107 | 0.0738
Epoch 11/300, seasonal_1 Loss: 0.1106 | 0.0823
Epoch 12/300, seasonal_1 Loss: 0.1115 | 0.0810
Epoch 13/300, seasonal_1 Loss: 0.1272 | 0.0748
Epoch 14/300, seasonal_1 Loss: 0.1218 | 0.0782
Epoch 15/300, seasonal_1 Loss: 0.1258 | 0.0663
Epoch 16/300, seasonal_1 Loss: 0.1098 | 0.0699
Epoch 17/300, seasonal_1 Loss: 0.1054 | 0.0751
Epoch 18/300, seasonal_1 Loss: 0.1055 | 0.0718
Epoch 19/300, seasonal_1 Loss: 0.1039 | 0.0646
Epoch 20/300, seasonal_1 Loss: 0.1012 | 0.0604
Epoch 21/300, seasonal_1 Loss: 0.1010 | 0.0598
Epoch 22/300, seasonal_1 Loss: 0.0992 | 0.0595
Epoch 23/300, seasonal_1 Loss: 0.0980 | 0.0592
Epoch 24/300, seasonal_1 Loss: 0.0962 | 0.0584
Epoch 25/300, seasonal_1 Loss: 0.0954 | 0.0583
Epoch 26/300, seasonal_1 Loss: 0.0950 | 0.0580
Epoch 27/300, seasonal_1 Loss: 0.0946 | 0.0577
Epoch 28/300, seasonal_1 Loss: 0.0943 | 0.0574
Epoch 29/300, seasonal_1 Loss: 0.0937 | 0.0573
Epoch 30/300, seasonal_1 Loss: 0.0933 | 0.0571
Epoch 31/300, seasonal_1 Loss: 0.0929 | 0.0571
Epoch 32/300, seasonal_1 Loss: 0.0926 | 0.0569
Epoch 33/300, seasonal_1 Loss: 0.0923 | 0.0568
Epoch 34/300, seasonal_1 Loss: 0.0920 | 0.0567
Epoch 35/300, seasonal_1 Loss: 0.0918 | 0.0566
Epoch 36/300, seasonal_1 Loss: 0.0916 | 0.0565
Epoch 37/300, seasonal_1 Loss: 0.0914 | 0.0564
Epoch 38/300, seasonal_1 Loss: 0.0912 | 0.0563
Epoch 39/300, seasonal_1 Loss: 0.0910 | 0.0562
Epoch 40/300, seasonal_1 Loss: 0.0909 | 0.0561
Epoch 41/300, seasonal_1 Loss: 0.0907 | 0.0561
Epoch 42/300, seasonal_1 Loss: 0.0905 | 0.0560
Epoch 43/300, seasonal_1 Loss: 0.0904 | 0.0559
Epoch 44/300, seasonal_1 Loss: 0.0902 | 0.0559
Epoch 45/300, seasonal_1 Loss: 0.0901 | 0.0558
Epoch 46/300, seasonal_1 Loss: 0.0900 | 0.0558
Epoch 47/300, seasonal_1 Loss: 0.0899 | 0.0557
Epoch 48/300, seasonal_1 Loss: 0.0898 | 0.0556
Epoch 49/300, seasonal_1 Loss: 0.0897 | 0.0556
Epoch 50/300, seasonal_1 Loss: 0.0896 | 0.0555
Epoch 51/300, seasonal_1 Loss: 0.0895 | 0.0555
Epoch 52/300, seasonal_1 Loss: 0.0895 | 0.0554
Epoch 53/300, seasonal_1 Loss: 0.0894 | 0.0554
Epoch 54/300, seasonal_1 Loss: 0.0893 | 0.0554
Epoch 55/300, seasonal_1 Loss: 0.0893 | 0.0553
Epoch 56/300, seasonal_1 Loss: 0.0892 | 0.0553
Epoch 57/300, seasonal_1 Loss: 0.0892 | 0.0553
Epoch 58/300, seasonal_1 Loss: 0.0891 | 0.0552
Epoch 59/300, seasonal_1 Loss: 0.0891 | 0.0552
Epoch 60/300, seasonal_1 Loss: 0.0890 | 0.0552
Epoch 61/300, seasonal_1 Loss: 0.0890 | 0.0552
Epoch 62/300, seasonal_1 Loss: 0.0889 | 0.0551
Epoch 63/300, seasonal_1 Loss: 0.0889 | 0.0551
Epoch 64/300, seasonal_1 Loss: 0.0889 | 0.0551
Epoch 65/300, seasonal_1 Loss: 0.0888 | 0.0551
Epoch 66/300, seasonal_1 Loss: 0.0888 | 0.0551
Epoch 67/300, seasonal_1 Loss: 0.0888 | 0.0550
Epoch 68/300, seasonal_1 Loss: 0.0887 | 0.0550
Epoch 69/300, seasonal_1 Loss: 0.0887 | 0.0550
Epoch 70/300, seasonal_1 Loss: 0.0887 | 0.0550
Epoch 71/300, seasonal_1 Loss: 0.0887 | 0.0550
Epoch 72/300, seasonal_1 Loss: 0.0887 | 0.0550
Epoch 73/300, seasonal_1 Loss: 0.0886 | 0.0550
Epoch 74/300, seasonal_1 Loss: 0.0886 | 0.0550
Epoch 75/300, seasonal_1 Loss: 0.0886 | 0.0550
Epoch 76/300, seasonal_1 Loss: 0.0886 | 0.0549
Epoch 77/300, seasonal_1 Loss: 0.0886 | 0.0549
Epoch 78/300, seasonal_1 Loss: 0.0886 | 0.0549
Epoch 79/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 80/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 81/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 82/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 83/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 84/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 85/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 86/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 87/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 88/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 89/300, seasonal_1 Loss: 0.0885 | 0.0549
Epoch 90/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 91/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 92/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 93/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 94/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 95/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 96/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 97/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 98/300, seasonal_1 Loss: 0.0884 | 0.0549
Epoch 99/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 100/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 101/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 102/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 103/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 104/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 105/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 106/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 107/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 108/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 109/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 110/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 111/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 112/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 113/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 114/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 115/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 116/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 117/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 118/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 119/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 120/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 121/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 122/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 123/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 124/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 125/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 126/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 127/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 128/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 129/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 130/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 131/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 132/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 133/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 134/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 135/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 136/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 137/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 138/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 139/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 140/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 141/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 142/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 143/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 144/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 145/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 146/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 147/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 148/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 149/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 150/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 151/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 152/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 153/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 154/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 155/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 156/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 157/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 158/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 159/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 160/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 161/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 162/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 163/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 164/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 165/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 166/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 167/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 168/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 169/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 170/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 171/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 172/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 173/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 174/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 175/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 176/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 177/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 178/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 179/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 180/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 181/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 182/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 183/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 184/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 185/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 186/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 187/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 188/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 189/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 190/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 191/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 192/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 193/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 194/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 195/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 196/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 197/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 198/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 199/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 200/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 201/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 202/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 203/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 204/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 205/300, seasonal_1 Loss: 0.0884 | 0.0548
Epoch 206/300, seasonal_1 Loss: 0.0884 | 0.0548
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 20, 'train_rates': 0.9038493802150087, 'learning_rate': 0.0007536950053643158, 'batch_size': 212, 'step_size': 9, 'gamma': 0.8107415679438772}
Epoch 1/300, seasonal_2 Loss: 1.5565 | 0.5325
Epoch 2/300, seasonal_2 Loss: 0.3674 | 0.2030
Epoch 3/300, seasonal_2 Loss: 0.4810 | 0.2780
Epoch 4/300, seasonal_2 Loss: 0.4884 | 0.2670
Epoch 5/300, seasonal_2 Loss: 0.5549 | 0.2113
Epoch 6/300, seasonal_2 Loss: 0.3087 | 0.1693
Epoch 7/300, seasonal_2 Loss: 0.1995 | 0.1142
Epoch 8/300, seasonal_2 Loss: 0.1916 | 0.1851
Epoch 9/300, seasonal_2 Loss: 0.1625 | 0.1012
Epoch 10/300, seasonal_2 Loss: 0.1665 | 0.1071
Epoch 11/300, seasonal_2 Loss: 0.1633 | 0.1015
Epoch 12/300, seasonal_2 Loss: 0.1880 | 0.2315
Epoch 13/300, seasonal_2 Loss: 0.2832 | 0.2023
Epoch 14/300, seasonal_2 Loss: 0.2192 | 0.1582
Epoch 15/300, seasonal_2 Loss: 0.2293 | 0.1969
Epoch 16/300, seasonal_2 Loss: 0.2114 | 0.1553
Epoch 17/300, seasonal_2 Loss: 0.1665 | 0.1023
Epoch 18/300, seasonal_2 Loss: 0.1705 | 0.0936
Epoch 19/300, seasonal_2 Loss: 0.1768 | 0.3082
Epoch 20/300, seasonal_2 Loss: 0.2370 | 0.2083
Epoch 21/300, seasonal_2 Loss: 0.1983 | 0.1413
Epoch 22/300, seasonal_2 Loss: 0.1774 | 0.1255
Epoch 23/300, seasonal_2 Loss: 0.1539 | 0.1553
Epoch 24/300, seasonal_2 Loss: 0.1315 | 0.1107
Epoch 25/300, seasonal_2 Loss: 0.1256 | 0.0947
Epoch 26/300, seasonal_2 Loss: 0.1323 | 0.0976
Epoch 27/300, seasonal_2 Loss: 0.1344 | 0.0998
Epoch 28/300, seasonal_2 Loss: 0.1429 | 0.1388
Epoch 29/300, seasonal_2 Loss: 0.1349 | 0.0913
Epoch 30/300, seasonal_2 Loss: 0.1298 | 0.1158
Epoch 31/300, seasonal_2 Loss: 0.1310 | 0.0915
Epoch 32/300, seasonal_2 Loss: 0.1441 | 0.0922
Epoch 33/300, seasonal_2 Loss: 0.1337 | 0.0946
Epoch 34/300, seasonal_2 Loss: 0.1308 | 0.1434
Epoch 35/300, seasonal_2 Loss: 0.1346 | 0.0941
Epoch 36/300, seasonal_2 Loss: 0.1090 | 0.1041
Epoch 37/300, seasonal_2 Loss: 0.1028 | 0.0892
Epoch 38/300, seasonal_2 Loss: 0.0999 | 0.0963
Epoch 39/300, seasonal_2 Loss: 0.0963 | 0.0864
Epoch 40/300, seasonal_2 Loss: 0.0942 | 0.0849
Epoch 41/300, seasonal_2 Loss: 0.0922 | 0.0905
Epoch 42/300, seasonal_2 Loss: 0.0938 | 0.0878
Epoch 43/300, seasonal_2 Loss: 0.0957 | 0.0762
Epoch 44/300, seasonal_2 Loss: 0.0940 | 0.0932
Epoch 45/300, seasonal_2 Loss: 0.0891 | 0.0747
Epoch 46/300, seasonal_2 Loss: 0.0899 | 0.0866
Epoch 47/300, seasonal_2 Loss: 0.0890 | 0.0768
Epoch 48/300, seasonal_2 Loss: 0.0909 | 0.0817
Epoch 49/300, seasonal_2 Loss: 0.0863 | 0.0740
Epoch 50/300, seasonal_2 Loss: 0.0842 | 0.0792
Epoch 51/300, seasonal_2 Loss: 0.0822 | 0.0722
Epoch 52/300, seasonal_2 Loss: 0.0809 | 0.0763
Epoch 53/300, seasonal_2 Loss: 0.0797 | 0.0701
Epoch 54/300, seasonal_2 Loss: 0.0789 | 0.0727
Epoch 55/300, seasonal_2 Loss: 0.0781 | 0.0695
Epoch 56/300, seasonal_2 Loss: 0.0775 | 0.0714
Epoch 57/300, seasonal_2 Loss: 0.0770 | 0.0685
Epoch 58/300, seasonal_2 Loss: 0.0765 | 0.0700
Epoch 59/300, seasonal_2 Loss: 0.0761 | 0.0682
Epoch 60/300, seasonal_2 Loss: 0.0757 | 0.0684
Epoch 61/300, seasonal_2 Loss: 0.0754 | 0.0674
Epoch 62/300, seasonal_2 Loss: 0.0751 | 0.0677
Epoch 63/300, seasonal_2 Loss: 0.0748 | 0.0668
Epoch 64/300, seasonal_2 Loss: 0.0745 | 0.0665
Epoch 65/300, seasonal_2 Loss: 0.0743 | 0.0660
Epoch 66/300, seasonal_2 Loss: 0.0743 | 0.0659
Epoch 67/300, seasonal_2 Loss: 0.0739 | 0.0657
Epoch 68/300, seasonal_2 Loss: 0.0740 | 0.0655
Epoch 69/300, seasonal_2 Loss: 0.0739 | 0.0647
Epoch 70/300, seasonal_2 Loss: 0.0736 | 0.0643
Epoch 71/300, seasonal_2 Loss: 0.0740 | 0.0640
Epoch 72/300, seasonal_2 Loss: 0.0731 | 0.0645
Epoch 73/300, seasonal_2 Loss: 0.0736 | 0.0644
Epoch 74/300, seasonal_2 Loss: 0.0727 | 0.0630
Epoch 75/300, seasonal_2 Loss: 0.0727 | 0.0625
Epoch 76/300, seasonal_2 Loss: 0.0723 | 0.0632
Epoch 77/300, seasonal_2 Loss: 0.0721 | 0.0631
Epoch 78/300, seasonal_2 Loss: 0.0719 | 0.0622
Epoch 79/300, seasonal_2 Loss: 0.0718 | 0.0620
Epoch 80/300, seasonal_2 Loss: 0.0716 | 0.0622
Epoch 81/300, seasonal_2 Loss: 0.0715 | 0.0620
Epoch 82/300, seasonal_2 Loss: 0.0714 | 0.0616
Epoch 83/300, seasonal_2 Loss: 0.0713 | 0.0615
Epoch 84/300, seasonal_2 Loss: 0.0712 | 0.0614
Epoch 85/300, seasonal_2 Loss: 0.0711 | 0.0612
Epoch 86/300, seasonal_2 Loss: 0.0710 | 0.0611
Epoch 87/300, seasonal_2 Loss: 0.0709 | 0.0609
Epoch 88/300, seasonal_2 Loss: 0.0708 | 0.0608
Epoch 89/300, seasonal_2 Loss: 0.0707 | 0.0607
Epoch 90/300, seasonal_2 Loss: 0.0706 | 0.0606
Epoch 91/300, seasonal_2 Loss: 0.0705 | 0.0604
Epoch 92/300, seasonal_2 Loss: 0.0705 | 0.0603
Epoch 93/300, seasonal_2 Loss: 0.0704 | 0.0602
Epoch 94/300, seasonal_2 Loss: 0.0703 | 0.0601
Epoch 95/300, seasonal_2 Loss: 0.0703 | 0.0600
Epoch 96/300, seasonal_2 Loss: 0.0702 | 0.0599
Epoch 97/300, seasonal_2 Loss: 0.0701 | 0.0598
Epoch 98/300, seasonal_2 Loss: 0.0701 | 0.0598
Epoch 99/300, seasonal_2 Loss: 0.0700 | 0.0597
Epoch 100/300, seasonal_2 Loss: 0.0700 | 0.0596
Epoch 101/300, seasonal_2 Loss: 0.0699 | 0.0595
Epoch 102/300, seasonal_2 Loss: 0.0699 | 0.0595
Epoch 103/300, seasonal_2 Loss: 0.0698 | 0.0594
Epoch 104/300, seasonal_2 Loss: 0.0698 | 0.0593
Epoch 105/300, seasonal_2 Loss: 0.0697 | 0.0593
Epoch 106/300, seasonal_2 Loss: 0.0697 | 0.0592
Epoch 107/300, seasonal_2 Loss: 0.0697 | 0.0591
Epoch 108/300, seasonal_2 Loss: 0.0696 | 0.0591
Epoch 109/300, seasonal_2 Loss: 0.0696 | 0.0590
Epoch 110/300, seasonal_2 Loss: 0.0695 | 0.0590
Epoch 111/300, seasonal_2 Loss: 0.0695 | 0.0589
Epoch 112/300, seasonal_2 Loss: 0.0695 | 0.0589
Epoch 113/300, seasonal_2 Loss: 0.0695 | 0.0588
Epoch 114/300, seasonal_2 Loss: 0.0694 | 0.0588
Epoch 115/300, seasonal_2 Loss: 0.0694 | 0.0587
Epoch 116/300, seasonal_2 Loss: 0.0694 | 0.0587
Epoch 117/300, seasonal_2 Loss: 0.0693 | 0.0587
Epoch 118/300, seasonal_2 Loss: 0.0693 | 0.0586
Epoch 119/300, seasonal_2 Loss: 0.0693 | 0.0586
Epoch 120/300, seasonal_2 Loss: 0.0693 | 0.0586
Epoch 121/300, seasonal_2 Loss: 0.0692 | 0.0585
Epoch 122/300, seasonal_2 Loss: 0.0692 | 0.0585
Epoch 123/300, seasonal_2 Loss: 0.0692 | 0.0585
Epoch 124/300, seasonal_2 Loss: 0.0692 | 0.0584
Epoch 125/300, seasonal_2 Loss: 0.0692 | 0.0584
Epoch 126/300, seasonal_2 Loss: 0.0691 | 0.0584
Epoch 127/300, seasonal_2 Loss: 0.0691 | 0.0584
Epoch 128/300, seasonal_2 Loss: 0.0691 | 0.0583
Epoch 129/300, seasonal_2 Loss: 0.0691 | 0.0583
Epoch 130/300, seasonal_2 Loss: 0.0691 | 0.0583
Epoch 131/300, seasonal_2 Loss: 0.0691 | 0.0583
Epoch 132/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 133/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 134/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 135/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 136/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 137/300, seasonal_2 Loss: 0.0690 | 0.0582
Epoch 138/300, seasonal_2 Loss: 0.0690 | 0.0581
Epoch 139/300, seasonal_2 Loss: 0.0690 | 0.0581
Epoch 140/300, seasonal_2 Loss: 0.0689 | 0.0581
Epoch 141/300, seasonal_2 Loss: 0.0689 | 0.0581
Epoch 142/300, seasonal_2 Loss: 0.0689 | 0.0581
Epoch 143/300, seasonal_2 Loss: 0.0689 | 0.0581
Epoch 144/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 145/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 146/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 147/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 148/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 149/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 150/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 151/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 152/300, seasonal_2 Loss: 0.0689 | 0.0580
Epoch 153/300, seasonal_2 Loss: 0.0688 | 0.0580
Epoch 154/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 155/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 156/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 157/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 158/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 159/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 160/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 161/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 162/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 163/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 164/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 165/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 166/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 167/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 168/300, seasonal_2 Loss: 0.0688 | 0.0579
Epoch 169/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 170/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 171/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 172/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 173/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 174/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 175/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 176/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 177/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 178/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 179/300, seasonal_2 Loss: 0.0688 | 0.0578
Epoch 180/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 181/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 182/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 183/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 184/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 185/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 186/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 187/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 188/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 189/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 190/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 191/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 192/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 193/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 194/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 195/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 196/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 197/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 198/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 199/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 200/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 201/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 202/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 203/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 204/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 205/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 206/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 207/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 208/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 209/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 210/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 211/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 212/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 213/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 214/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 215/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 216/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 217/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 218/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 219/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 220/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 221/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 222/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 223/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 224/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 225/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 226/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 227/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 228/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 229/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 230/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 231/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 232/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 233/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 234/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 235/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 236/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 237/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 238/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 239/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 240/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 241/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 242/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 243/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 244/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 245/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 246/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 247/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 248/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 249/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 250/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 251/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 252/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 253/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 254/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 255/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 256/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 257/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 258/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 259/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 260/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 261/300, seasonal_2 Loss: 0.0687 | 0.0578
Epoch 262/300, seasonal_2 Loss: 0.0687 | 0.0578
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 33, 'train_rates': 0.8603999334232927, 'learning_rate': 1.7603244570286414e-05, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8864607795968951}
Epoch 1/300, seasonal_3 Loss: 0.8994 | 0.4356
Epoch 2/300, seasonal_3 Loss: 0.3844 | 0.2588
Epoch 3/300, seasonal_3 Loss: 0.3587 | 0.2324
Epoch 4/300, seasonal_3 Loss: 0.2450 | 0.1800
Epoch 5/300, seasonal_3 Loss: 0.2209 | 0.2338
Epoch 6/300, seasonal_3 Loss: 0.2071 | 0.1639
Epoch 7/300, seasonal_3 Loss: 0.2113 | 0.1485
Epoch 8/300, seasonal_3 Loss: 0.2021 | 0.1287
Epoch 9/300, seasonal_3 Loss: 0.1808 | 0.1855
Epoch 10/300, seasonal_3 Loss: 0.1764 | 0.1310
Epoch 11/300, seasonal_3 Loss: 0.1640 | 0.1073
Epoch 12/300, seasonal_3 Loss: 0.1761 | 0.1085
Epoch 13/300, seasonal_3 Loss: 0.1695 | 0.1031
Epoch 14/300, seasonal_3 Loss: 0.1462 | 0.1077
Epoch 15/300, seasonal_3 Loss: 0.1493 | 0.1308
Epoch 16/300, seasonal_3 Loss: 0.1425 | 0.1073
Epoch 17/300, seasonal_3 Loss: 0.1518 | 0.0972
Epoch 18/300, seasonal_3 Loss: 0.1485 | 0.0912
Epoch 19/300, seasonal_3 Loss: 0.1370 | 0.1319
Epoch 20/300, seasonal_3 Loss: 0.1386 | 0.1057
Epoch 21/300, seasonal_3 Loss: 0.1315 | 0.0838
Epoch 22/300, seasonal_3 Loss: 0.1389 | 0.0853
Epoch 23/300, seasonal_3 Loss: 0.1275 | 0.0931
Epoch 24/300, seasonal_3 Loss: 0.1246 | 0.0853
Epoch 25/300, seasonal_3 Loss: 0.1222 | 0.0811
Epoch 26/300, seasonal_3 Loss: 0.1220 | 0.0819
Epoch 27/300, seasonal_3 Loss: 0.1205 | 0.0782
Epoch 28/300, seasonal_3 Loss: 0.1175 | 0.0782
Epoch 29/300, seasonal_3 Loss: 0.1162 | 0.0818
Epoch 30/300, seasonal_3 Loss: 0.1151 | 0.0779
Epoch 31/300, seasonal_3 Loss: 0.1148 | 0.0736
Epoch 32/300, seasonal_3 Loss: 0.1133 | 0.0741
Epoch 33/300, seasonal_3 Loss: 0.1120 | 0.0776
Epoch 34/300, seasonal_3 Loss: 0.1110 | 0.0735
Epoch 35/300, seasonal_3 Loss: 0.1105 | 0.0707
Epoch 36/300, seasonal_3 Loss: 0.1099 | 0.0707
Epoch 37/300, seasonal_3 Loss: 0.1087 | 0.0727
Epoch 38/300, seasonal_3 Loss: 0.1080 | 0.0707
Epoch 39/300, seasonal_3 Loss: 0.1072 | 0.0686
Epoch 40/300, seasonal_3 Loss: 0.1069 | 0.0684
Epoch 41/300, seasonal_3 Loss: 0.1060 | 0.0683
Epoch 42/300, seasonal_3 Loss: 0.1053 | 0.0680
Epoch 43/300, seasonal_3 Loss: 0.1047 | 0.0672
Epoch 44/300, seasonal_3 Loss: 0.1042 | 0.0665
Epoch 45/300, seasonal_3 Loss: 0.1038 | 0.0658
Epoch 46/300, seasonal_3 Loss: 0.1031 | 0.0655
Epoch 47/300, seasonal_3 Loss: 0.1026 | 0.0658
Epoch 48/300, seasonal_3 Loss: 0.1021 | 0.0650
Epoch 49/300, seasonal_3 Loss: 0.1017 | 0.0642
Epoch 50/300, seasonal_3 Loss: 0.1012 | 0.0638
Epoch 51/300, seasonal_3 Loss: 0.1007 | 0.0640
Epoch 52/300, seasonal_3 Loss: 0.1003 | 0.0636
Epoch 53/300, seasonal_3 Loss: 0.0999 | 0.0629
Epoch 54/300, seasonal_3 Loss: 0.0995 | 0.0626
Epoch 55/300, seasonal_3 Loss: 0.0991 | 0.0625
Epoch 56/300, seasonal_3 Loss: 0.0987 | 0.0622
Epoch 57/300, seasonal_3 Loss: 0.0984 | 0.0618
Epoch 58/300, seasonal_3 Loss: 0.0980 | 0.0616
Epoch 59/300, seasonal_3 Loss: 0.0977 | 0.0613
Epoch 60/300, seasonal_3 Loss: 0.0974 | 0.0610
Epoch 61/300, seasonal_3 Loss: 0.0970 | 0.0608
Epoch 62/300, seasonal_3 Loss: 0.0967 | 0.0606
Epoch 63/300, seasonal_3 Loss: 0.0964 | 0.0603
Epoch 64/300, seasonal_3 Loss: 0.0961 | 0.0601
Epoch 65/300, seasonal_3 Loss: 0.0958 | 0.0599
Epoch 66/300, seasonal_3 Loss: 0.0956 | 0.0597
Epoch 67/300, seasonal_3 Loss: 0.0953 | 0.0595
Epoch 68/300, seasonal_3 Loss: 0.0950 | 0.0593
Epoch 69/300, seasonal_3 Loss: 0.0948 | 0.0591
Epoch 70/300, seasonal_3 Loss: 0.0945 | 0.0589
Epoch 71/300, seasonal_3 Loss: 0.0943 | 0.0588
Epoch 72/300, seasonal_3 Loss: 0.0940 | 0.0586
Epoch 73/300, seasonal_3 Loss: 0.0938 | 0.0584
Epoch 74/300, seasonal_3 Loss: 0.0936 | 0.0583
Epoch 75/300, seasonal_3 Loss: 0.0934 | 0.0581
Epoch 76/300, seasonal_3 Loss: 0.0931 | 0.0580
Epoch 77/300, seasonal_3 Loss: 0.0929 | 0.0578
Epoch 78/300, seasonal_3 Loss: 0.0927 | 0.0577
Epoch 79/300, seasonal_3 Loss: 0.0925 | 0.0576
Epoch 80/300, seasonal_3 Loss: 0.0923 | 0.0574
Epoch 81/300, seasonal_3 Loss: 0.0921 | 0.0573
Epoch 82/300, seasonal_3 Loss: 0.0920 | 0.0572
Epoch 83/300, seasonal_3 Loss: 0.0918 | 0.0571
Epoch 84/300, seasonal_3 Loss: 0.0916 | 0.0569
Epoch 85/300, seasonal_3 Loss: 0.0914 | 0.0568
Epoch 86/300, seasonal_3 Loss: 0.0913 | 0.0567
Epoch 87/300, seasonal_3 Loss: 0.0911 | 0.0566
Epoch 88/300, seasonal_3 Loss: 0.0909 | 0.0565
Epoch 89/300, seasonal_3 Loss: 0.0908 | 0.0564
Epoch 90/300, seasonal_3 Loss: 0.0906 | 0.0563
Epoch 91/300, seasonal_3 Loss: 0.0905 | 0.0562
Epoch 92/300, seasonal_3 Loss: 0.0903 | 0.0561
Epoch 93/300, seasonal_3 Loss: 0.0902 | 0.0560
Epoch 94/300, seasonal_3 Loss: 0.0900 | 0.0559
Epoch 95/300, seasonal_3 Loss: 0.0899 | 0.0558
Epoch 96/300, seasonal_3 Loss: 0.0898 | 0.0557
Epoch 97/300, seasonal_3 Loss: 0.0896 | 0.0556
Epoch 98/300, seasonal_3 Loss: 0.0895 | 0.0556
Epoch 99/300, seasonal_3 Loss: 0.0894 | 0.0555
Epoch 100/300, seasonal_3 Loss: 0.0893 | 0.0554
Epoch 101/300, seasonal_3 Loss: 0.0891 | 0.0553
Epoch 102/300, seasonal_3 Loss: 0.0890 | 0.0552
Epoch 103/300, seasonal_3 Loss: 0.0889 | 0.0552
Epoch 104/300, seasonal_3 Loss: 0.0888 | 0.0551
Epoch 105/300, seasonal_3 Loss: 0.0887 | 0.0550
Epoch 106/300, seasonal_3 Loss: 0.0886 | 0.0550
Epoch 107/300, seasonal_3 Loss: 0.0885 | 0.0549
Epoch 108/300, seasonal_3 Loss: 0.0884 | 0.0548
Epoch 109/300, seasonal_3 Loss: 0.0883 | 0.0548
Epoch 110/300, seasonal_3 Loss: 0.0882 | 0.0547
Epoch 111/300, seasonal_3 Loss: 0.0881 | 0.0546
Epoch 112/300, seasonal_3 Loss: 0.0880 | 0.0546
Epoch 113/300, seasonal_3 Loss: 0.0879 | 0.0545
Epoch 114/300, seasonal_3 Loss: 0.0878 | 0.0545
Epoch 115/300, seasonal_3 Loss: 0.0877 | 0.0544
Epoch 116/300, seasonal_3 Loss: 0.0876 | 0.0544
Epoch 117/300, seasonal_3 Loss: 0.0875 | 0.0543
Epoch 118/300, seasonal_3 Loss: 0.0875 | 0.0543
Epoch 119/300, seasonal_3 Loss: 0.0874 | 0.0542
Epoch 120/300, seasonal_3 Loss: 0.0873 | 0.0542
Epoch 121/300, seasonal_3 Loss: 0.0872 | 0.0541
Epoch 122/300, seasonal_3 Loss: 0.0871 | 0.0541
Epoch 123/300, seasonal_3 Loss: 0.0871 | 0.0540
Epoch 124/300, seasonal_3 Loss: 0.0870 | 0.0540
Epoch 125/300, seasonal_3 Loss: 0.0869 | 0.0539
Epoch 126/300, seasonal_3 Loss: 0.0869 | 0.0539
Epoch 127/300, seasonal_3 Loss: 0.0868 | 0.0539
Epoch 128/300, seasonal_3 Loss: 0.0867 | 0.0538
Epoch 129/300, seasonal_3 Loss: 0.0867 | 0.0538
Epoch 130/300, seasonal_3 Loss: 0.0866 | 0.0537
Epoch 131/300, seasonal_3 Loss: 0.0865 | 0.0537
Epoch 132/300, seasonal_3 Loss: 0.0865 | 0.0537
Epoch 133/300, seasonal_3 Loss: 0.0864 | 0.0536
Epoch 134/300, seasonal_3 Loss: 0.0864 | 0.0536
Epoch 135/300, seasonal_3 Loss: 0.0863 | 0.0535
Epoch 136/300, seasonal_3 Loss: 0.0862 | 0.0535
Epoch 137/300, seasonal_3 Loss: 0.0862 | 0.0535
Epoch 138/300, seasonal_3 Loss: 0.0861 | 0.0535
Epoch 139/300, seasonal_3 Loss: 0.0861 | 0.0534
Epoch 140/300, seasonal_3 Loss: 0.0860 | 0.0534
Epoch 141/300, seasonal_3 Loss: 0.0860 | 0.0534
Epoch 142/300, seasonal_3 Loss: 0.0859 | 0.0533
Epoch 143/300, seasonal_3 Loss: 0.0859 | 0.0533
Epoch 144/300, seasonal_3 Loss: 0.0858 | 0.0533
Epoch 145/300, seasonal_3 Loss: 0.0858 | 0.0532
Epoch 146/300, seasonal_3 Loss: 0.0857 | 0.0532
Epoch 147/300, seasonal_3 Loss: 0.0857 | 0.0532
Epoch 148/300, seasonal_3 Loss: 0.0856 | 0.0532
Epoch 149/300, seasonal_3 Loss: 0.0856 | 0.0531
Epoch 150/300, seasonal_3 Loss: 0.0856 | 0.0531
Epoch 151/300, seasonal_3 Loss: 0.0855 | 0.0531
Epoch 152/300, seasonal_3 Loss: 0.0855 | 0.0531
Epoch 153/300, seasonal_3 Loss: 0.0854 | 0.0530
Epoch 154/300, seasonal_3 Loss: 0.0854 | 0.0530
Epoch 155/300, seasonal_3 Loss: 0.0854 | 0.0530
Epoch 156/300, seasonal_3 Loss: 0.0853 | 0.0530
Epoch 157/300, seasonal_3 Loss: 0.0853 | 0.0530
Epoch 158/300, seasonal_3 Loss: 0.0852 | 0.0529
Epoch 159/300, seasonal_3 Loss: 0.0852 | 0.0529
Epoch 160/300, seasonal_3 Loss: 0.0852 | 0.0529
Epoch 161/300, seasonal_3 Loss: 0.0851 | 0.0529
Epoch 162/300, seasonal_3 Loss: 0.0851 | 0.0529
Epoch 163/300, seasonal_3 Loss: 0.0851 | 0.0528
Epoch 164/300, seasonal_3 Loss: 0.0850 | 0.0528
Epoch 165/300, seasonal_3 Loss: 0.0850 | 0.0528
Epoch 166/300, seasonal_3 Loss: 0.0850 | 0.0528
Epoch 167/300, seasonal_3 Loss: 0.0850 | 0.0528
Epoch 168/300, seasonal_3 Loss: 0.0849 | 0.0527
Epoch 169/300, seasonal_3 Loss: 0.0849 | 0.0527
Epoch 170/300, seasonal_3 Loss: 0.0849 | 0.0527
Epoch 171/300, seasonal_3 Loss: 0.0848 | 0.0527
Epoch 172/300, seasonal_3 Loss: 0.0848 | 0.0527
Epoch 173/300, seasonal_3 Loss: 0.0848 | 0.0527
Epoch 174/300, seasonal_3 Loss: 0.0848 | 0.0527
Epoch 175/300, seasonal_3 Loss: 0.0847 | 0.0526
Epoch 176/300, seasonal_3 Loss: 0.0847 | 0.0526
Epoch 177/300, seasonal_3 Loss: 0.0847 | 0.0526
Epoch 178/300, seasonal_3 Loss: 0.0847 | 0.0526
Epoch 179/300, seasonal_3 Loss: 0.0846 | 0.0526
Epoch 180/300, seasonal_3 Loss: 0.0846 | 0.0526
Epoch 181/300, seasonal_3 Loss: 0.0846 | 0.0526
Epoch 182/300, seasonal_3 Loss: 0.0846 | 0.0525
Epoch 183/300, seasonal_3 Loss: 0.0846 | 0.0525
Epoch 184/300, seasonal_3 Loss: 0.0845 | 0.0525
Epoch 185/300, seasonal_3 Loss: 0.0845 | 0.0525
Epoch 186/300, seasonal_3 Loss: 0.0845 | 0.0525
Epoch 187/300, seasonal_3 Loss: 0.0845 | 0.0525
Epoch 188/300, seasonal_3 Loss: 0.0844 | 0.0525
Epoch 189/300, seasonal_3 Loss: 0.0844 | 0.0525
Epoch 190/300, seasonal_3 Loss: 0.0844 | 0.0525
Epoch 191/300, seasonal_3 Loss: 0.0844 | 0.0524
Epoch 192/300, seasonal_3 Loss: 0.0844 | 0.0524
Epoch 193/300, seasonal_3 Loss: 0.0844 | 0.0524
Epoch 194/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 195/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 196/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 197/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 198/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 199/300, seasonal_3 Loss: 0.0843 | 0.0524
Epoch 200/300, seasonal_3 Loss: 0.0842 | 0.0524
Epoch 201/300, seasonal_3 Loss: 0.0842 | 0.0524
Epoch 202/300, seasonal_3 Loss: 0.0842 | 0.0523
Epoch 203/300, seasonal_3 Loss: 0.0842 | 0.0523
Epoch 204/300, seasonal_3 Loss: 0.0842 | 0.0523
Epoch 205/300, seasonal_3 Loss: 0.0842 | 0.0523
Epoch 206/300, seasonal_3 Loss: 0.0842 | 0.0523
Epoch 207/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 208/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 209/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 210/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 211/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 212/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 213/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 214/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 215/300, seasonal_3 Loss: 0.0841 | 0.0523
Epoch 216/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 217/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 218/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 219/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 220/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 221/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 222/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 223/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 224/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 225/300, seasonal_3 Loss: 0.0840 | 0.0522
Epoch 226/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 227/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 228/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 229/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 230/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 231/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 232/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 233/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 234/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 235/300, seasonal_3 Loss: 0.0839 | 0.0522
Epoch 236/300, seasonal_3 Loss: 0.0839 | 0.0521
Epoch 237/300, seasonal_3 Loss: 0.0839 | 0.0521
Epoch 238/300, seasonal_3 Loss: 0.0839 | 0.0521
Epoch 239/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 240/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 241/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 242/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 243/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 244/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 245/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 246/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 247/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 248/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 249/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 250/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 251/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 252/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 253/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 254/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 255/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 256/300, seasonal_3 Loss: 0.0838 | 0.0521
Epoch 257/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 258/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 259/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 260/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 261/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 262/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 263/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 264/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 265/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 266/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 267/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 268/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 269/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 270/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 271/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 272/300, seasonal_3 Loss: 0.0837 | 0.0521
Epoch 273/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 274/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 275/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 276/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 277/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 278/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 279/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 280/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 281/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 282/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 283/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 284/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 285/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 286/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 287/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 288/300, seasonal_3 Loss: 0.0837 | 0.0520
Epoch 289/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 290/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 291/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 292/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 293/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 294/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 295/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 296/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 297/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 298/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 299/300, seasonal_3 Loss: 0.0836 | 0.0520
Epoch 300/300, seasonal_3 Loss: 0.0836 | 0.0520
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8876291712995726, 'learning_rate': 9.842250568398566e-06, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9846904717498514}
Epoch 1/300, resid Loss: 0.3247 | 0.1087
Epoch 2/300, resid Loss: 0.1349 | 0.0911
Epoch 3/300, resid Loss: 0.1221 | 0.0862
Epoch 4/300, resid Loss: 0.1160 | 0.0823
Epoch 5/300, resid Loss: 0.1124 | 0.0816
Epoch 6/300, resid Loss: 0.1096 | 0.0827
Epoch 7/300, resid Loss: 0.1072 | 0.0843
Epoch 8/300, resid Loss: 0.1051 | 0.0861
Epoch 9/300, resid Loss: 0.1033 | 0.0881
Epoch 10/300, resid Loss: 0.1016 | 0.0888
Epoch 11/300, resid Loss: 0.1001 | 0.0880
Epoch 12/300, resid Loss: 0.0987 | 0.0849
Epoch 13/300, resid Loss: 0.0974 | 0.0793
Epoch 14/300, resid Loss: 0.0958 | 0.0738
Epoch 15/300, resid Loss: 0.0937 | 0.0708
Epoch 16/300, resid Loss: 0.0916 | 0.0685
Epoch 17/300, resid Loss: 0.0898 | 0.0671
Epoch 18/300, resid Loss: 0.0884 | 0.0658
Epoch 19/300, resid Loss: 0.0871 | 0.0645
Epoch 20/300, resid Loss: 0.0860 | 0.0631
Epoch 21/300, resid Loss: 0.0850 | 0.0617
Epoch 22/300, resid Loss: 0.0840 | 0.0602
Epoch 23/300, resid Loss: 0.0832 | 0.0584
Epoch 24/300, resid Loss: 0.0824 | 0.0562
Epoch 25/300, resid Loss: 0.0817 | 0.0544
Epoch 26/300, resid Loss: 0.0810 | 0.0523
Epoch 27/300, resid Loss: 0.0804 | 0.0503
Epoch 28/300, resid Loss: 0.0799 | 0.0483
Epoch 29/300, resid Loss: 0.0793 | 0.0466
Epoch 30/300, resid Loss: 0.0788 | 0.0452
Epoch 31/300, resid Loss: 0.0783 | 0.0441
Epoch 32/300, resid Loss: 0.0778 | 0.0432
Epoch 33/300, resid Loss: 0.0773 | 0.0424
Epoch 34/300, resid Loss: 0.0768 | 0.0417
Epoch 35/300, resid Loss: 0.0762 | 0.0412
Epoch 36/300, resid Loss: 0.0757 | 0.0407
Epoch 37/300, resid Loss: 0.0752 | 0.0402
Epoch 38/300, resid Loss: 0.0747 | 0.0396
Epoch 39/300, resid Loss: 0.0742 | 0.0390
Epoch 40/300, resid Loss: 0.0737 | 0.0384
Epoch 41/300, resid Loss: 0.0732 | 0.0378
Epoch 42/300, resid Loss: 0.0728 | 0.0372
Epoch 43/300, resid Loss: 0.0723 | 0.0366
Epoch 44/300, resid Loss: 0.0719 | 0.0360
Epoch 45/300, resid Loss: 0.0715 | 0.0355
Epoch 46/300, resid Loss: 0.0710 | 0.0350
Epoch 47/300, resid Loss: 0.0706 | 0.0346
Epoch 48/300, resid Loss: 0.0702 | 0.0344
Epoch 49/300, resid Loss: 0.0698 | 0.0342
Epoch 50/300, resid Loss: 0.0694 | 0.0341
Epoch 51/300, resid Loss: 0.0691 | 0.0340
Epoch 52/300, resid Loss: 0.0687 | 0.0340
Epoch 53/300, resid Loss: 0.0684 | 0.0341
Epoch 54/300, resid Loss: 0.0681 | 0.0341
Epoch 55/300, resid Loss: 0.0678 | 0.0342
Epoch 56/300, resid Loss: 0.0676 | 0.0342
Epoch 57/300, resid Loss: 0.0673 | 0.0343
Epoch 58/300, resid Loss: 0.0671 | 0.0344
Epoch 59/300, resid Loss: 0.0668 | 0.0345
Epoch 60/300, resid Loss: 0.0666 | 0.0346
Epoch 61/300, resid Loss: 0.0664 | 0.0347
Epoch 62/300, resid Loss: 0.0661 | 0.0347
Epoch 63/300, resid Loss: 0.0659 | 0.0348
Epoch 64/300, resid Loss: 0.0657 | 0.0348
Epoch 65/300, resid Loss: 0.0655 | 0.0348
Epoch 66/300, resid Loss: 0.0652 | 0.0347
Epoch 67/300, resid Loss: 0.0650 | 0.0347
Epoch 68/300, resid Loss: 0.0647 | 0.0346
Epoch 69/300, resid Loss: 0.0645 | 0.0345
Epoch 70/300, resid Loss: 0.0643 | 0.0343
Epoch 71/300, resid Loss: 0.0640 | 0.0343
Epoch 72/300, resid Loss: 0.0638 | 0.0342
Epoch 73/300, resid Loss: 0.0636 | 0.0341
Epoch 74/300, resid Loss: 0.0633 | 0.0341
Epoch 75/300, resid Loss: 0.0631 | 0.0341
Epoch 76/300, resid Loss: 0.0629 | 0.0341
Epoch 77/300, resid Loss: 0.0626 | 0.0341
Epoch 78/300, resid Loss: 0.0624 | 0.0341
Epoch 79/300, resid Loss: 0.0622 | 0.0341
Epoch 80/300, resid Loss: 0.0620 | 0.0341
Epoch 81/300, resid Loss: 0.0617 | 0.0341
Epoch 82/300, resid Loss: 0.0615 | 0.0341
Epoch 83/300, resid Loss: 0.0613 | 0.0340
Epoch 84/300, resid Loss: 0.0611 | 0.0337
Epoch 85/300, resid Loss: 0.0609 | 0.0335
Epoch 86/300, resid Loss: 0.0607 | 0.0333
Epoch 87/300, resid Loss: 0.0605 | 0.0331
Epoch 88/300, resid Loss: 0.0603 | 0.0329
Epoch 89/300, resid Loss: 0.0601 | 0.0328
Epoch 90/300, resid Loss: 0.0599 | 0.0327
Epoch 91/300, resid Loss: 0.0597 | 0.0325
Epoch 92/300, resid Loss: 0.0596 | 0.0323
Epoch 93/300, resid Loss: 0.0595 | 0.0314
Epoch 94/300, resid Loss: 0.0594 | 0.0303
Epoch 95/300, resid Loss: 0.0593 | 0.0300
Epoch 96/300, resid Loss: 0.0590 | 0.0298
Epoch 97/300, resid Loss: 0.0588 | 0.0296
Epoch 98/300, resid Loss: 0.0585 | 0.0294
Epoch 99/300, resid Loss: 0.0583 | 0.0293
Epoch 100/300, resid Loss: 0.0581 | 0.0292
Epoch 101/300, resid Loss: 0.0578 | 0.0292
Epoch 102/300, resid Loss: 0.0576 | 0.0293
Epoch 103/300, resid Loss: 0.0573 | 0.0294
Epoch 104/300, resid Loss: 0.0571 | 0.0296
Epoch 105/300, resid Loss: 0.0568 | 0.0298
Epoch 106/300, resid Loss: 0.0565 | 0.0301
Epoch 107/300, resid Loss: 0.0563 | 0.0304
Epoch 108/300, resid Loss: 0.0560 | 0.0306
Epoch 109/300, resid Loss: 0.0557 | 0.0309
Epoch 110/300, resid Loss: 0.0555 | 0.0313
Epoch 111/300, resid Loss: 0.0552 | 0.0317
Epoch 112/300, resid Loss: 0.0550 | 0.0322
Epoch 113/300, resid Loss: 0.0547 | 0.0327
Epoch 114/300, resid Loss: 0.0544 | 0.0335
Epoch 115/300, resid Loss: 0.0541 | 0.0344
Epoch 116/300, resid Loss: 0.0538 | 0.0354
Epoch 117/300, resid Loss: 0.0535 | 0.0365
Epoch 118/300, resid Loss: 0.0531 | 0.0378
Epoch 119/300, resid Loss: 0.0527 | 0.0391
Epoch 120/300, resid Loss: 0.0523 | 0.0403
Epoch 121/300, resid Loss: 0.0517 | 0.0411
Epoch 122/300, resid Loss: 0.0511 | 0.0420
Epoch 123/300, resid Loss: 0.0504 | 0.0422
Epoch 124/300, resid Loss: 0.0497 | 0.0417
Epoch 125/300, resid Loss: 0.0491 | 0.0407
Epoch 126/300, resid Loss: 0.0485 | 0.0404
Epoch 127/300, resid Loss: 0.0480 | 0.0386
Epoch 128/300, resid Loss: 0.0477 | 0.0395
Epoch 129/300, resid Loss: 0.0473 | 0.0373
Epoch 130/300, resid Loss: 0.0470 | 0.0395
Epoch 131/300, resid Loss: 0.0467 | 0.0370
Epoch 132/300, resid Loss: 0.0465 | 0.0402
Epoch 133/300, resid Loss: 0.0462 | 0.0371
Epoch 134/300, resid Loss: 0.0461 | 0.0406
Epoch 135/300, resid Loss: 0.0457 | 0.0372
Epoch 136/300, resid Loss: 0.0456 | 0.0400
Epoch 137/300, resid Loss: 0.0453 | 0.0373
Epoch 138/300, resid Loss: 0.0451 | 0.0397
Epoch 139/300, resid Loss: 0.0449 | 0.0375
Epoch 140/300, resid Loss: 0.0447 | 0.0399
Epoch 141/300, resid Loss: 0.0446 | 0.0375
Epoch 142/300, resid Loss: 0.0444 | 0.0405
Epoch 143/300, resid Loss: 0.0442 | 0.0373
Epoch 144/300, resid Loss: 0.0441 | 0.0417
Epoch 145/300, resid Loss: 0.0439 | 0.0371
Epoch 146/300, resid Loss: 0.0438 | 0.0446
Epoch 147/300, resid Loss: 0.0437 | 0.0366
Epoch 148/300, resid Loss: 0.0436 | 0.0470
Epoch 149/300, resid Loss: 0.0435 | 0.0368
Epoch 150/300, resid Loss: 0.0434 | 0.0442
Epoch 151/300, resid Loss: 0.0433 | 0.0382
Epoch 152/300, resid Loss: 0.0431 | 0.0511
Epoch 153/300, resid Loss: 0.0432 | 0.0388
Epoch 154/300, resid Loss: 0.0428 | 0.0521
Epoch 155/300, resid Loss: 0.0429 | 0.0389
Epoch 156/300, resid Loss: 0.0425 | 0.0505
Epoch 157/300, resid Loss: 0.0426 | 0.0389
Epoch 158/300, resid Loss: 0.0423 | 0.0499
Epoch 159/300, resid Loss: 0.0424 | 0.0391
Epoch 160/300, resid Loss: 0.0421 | 0.0492
Epoch 161/300, resid Loss: 0.0421 | 0.0394
Epoch 162/300, resid Loss: 0.0419 | 0.0499
Epoch 163/300, resid Loss: 0.0420 | 0.0400
Epoch 164/300, resid Loss: 0.0417 | 0.0499
Epoch 165/300, resid Loss: 0.0418 | 0.0406
Epoch 166/300, resid Loss: 0.0417 | 0.0498
Epoch 167/300, resid Loss: 0.0418 | 0.0410
Epoch 168/300, resid Loss: 0.0418 | 0.0498
Epoch 169/300, resid Loss: 0.0420 | 0.0416
Epoch 170/300, resid Loss: 0.0420 | 0.0506
Epoch 171/300, resid Loss: 0.0422 | 0.0424
Epoch 172/300, resid Loss: 0.0418 | 0.0503
Epoch 173/300, resid Loss: 0.0418 | 0.0383
Epoch 174/300, resid Loss: 0.0415 | 0.0390
Epoch 175/300, resid Loss: 0.0413 | 0.0373
Epoch 176/300, resid Loss: 0.0407 | 0.0404
Epoch 177/300, resid Loss: 0.0405 | 0.0372
Epoch 178/300, resid Loss: 0.0402 | 0.0414
Epoch 179/300, resid Loss: 0.0401 | 0.0373
Epoch 180/300, resid Loss: 0.0399 | 0.0423
Epoch 181/300, resid Loss: 0.0398 | 0.0371
Epoch 182/300, resid Loss: 0.0397 | 0.0423
Epoch 183/300, resid Loss: 0.0396 | 0.0369
Epoch 184/300, resid Loss: 0.0394 | 0.0428
Epoch 185/300, resid Loss: 0.0394 | 0.0370
Epoch 186/300, resid Loss: 0.0392 | 0.0432
Epoch 187/300, resid Loss: 0.0392 | 0.0372
Epoch 188/300, resid Loss: 0.0390 | 0.0433
Epoch 189/300, resid Loss: 0.0390 | 0.0374
Epoch 190/300, resid Loss: 0.0388 | 0.0430
Epoch 191/300, resid Loss: 0.0387 | 0.0376
Epoch 192/300, resid Loss: 0.0386 | 0.0432
Epoch 193/300, resid Loss: 0.0385 | 0.0378
Epoch 194/300, resid Loss: 0.0384 | 0.0432
Epoch 195/300, resid Loss: 0.0383 | 0.0380
Epoch 196/300, resid Loss: 0.0382 | 0.0431
Epoch 197/300, resid Loss: 0.0381 | 0.0382
Epoch 198/300, resid Loss: 0.0380 | 0.0429
Epoch 199/300, resid Loss: 0.0379 | 0.0385
Epoch 200/300, resid Loss: 0.0378 | 0.0428
Epoch 201/300, resid Loss: 0.0377 | 0.0387
Epoch 202/300, resid Loss: 0.0376 | 0.0426
Epoch 203/300, resid Loss: 0.0375 | 0.0390
Epoch 204/300, resid Loss: 0.0374 | 0.0424
Epoch 205/300, resid Loss: 0.0374 | 0.0392
Epoch 206/300, resid Loss: 0.0373 | 0.0422
Epoch 207/300, resid Loss: 0.0372 | 0.0395
Epoch 208/300, resid Loss: 0.0371 | 0.0422
Epoch 209/300, resid Loss: 0.0371 | 0.0397
Epoch 210/300, resid Loss: 0.0369 | 0.0421
Epoch 211/300, resid Loss: 0.0369 | 0.0396
Epoch 212/300, resid Loss: 0.0368 | 0.0415
Epoch 213/300, resid Loss: 0.0368 | 0.0394
Epoch 214/300, resid Loss: 0.0367 | 0.0409
Epoch 215/300, resid Loss: 0.0366 | 0.0387
Epoch 216/300, resid Loss: 0.0365 | 0.0399
Epoch 217/300, resid Loss: 0.0365 | 0.0374
Epoch 218/300, resid Loss: 0.0363 | 0.0385
Epoch 219/300, resid Loss: 0.0363 | 0.0359
Epoch 220/300, resid Loss: 0.0363 | 0.0366
Epoch 221/300, resid Loss: 0.0363 | 0.0353
Epoch 222/300, resid Loss: 0.0363 | 0.0359
Epoch 223/300, resid Loss: 0.0362 | 0.0358
Epoch 224/300, resid Loss: 0.0361 | 0.0364
Epoch 225/300, resid Loss: 0.0361 | 0.0358
Epoch 226/300, resid Loss: 0.0361 | 0.0366
Epoch 227/300, resid Loss: 0.0359 | 0.0365
Epoch 228/300, resid Loss: 0.0358 | 0.0386
Epoch 229/300, resid Loss: 0.0357 | 0.0379
Epoch 230/300, resid Loss: 0.0355 | 0.0398
Epoch 231/300, resid Loss: 0.0354 | 0.0386
Epoch 232/300, resid Loss: 0.0353 | 0.0408
Epoch 233/300, resid Loss: 0.0353 | 0.0399
Epoch 234/300, resid Loss: 0.0353 | 0.0423
Epoch 235/300, resid Loss: 0.0352 | 0.0407
Epoch 236/300, resid Loss: 0.0351 | 0.0424
Epoch 237/300, resid Loss: 0.0350 | 0.0395
Epoch 238/300, resid Loss: 0.0350 | 0.0406
Epoch 239/300, resid Loss: 0.0350 | 0.0375
Epoch 240/300, resid Loss: 0.0350 | 0.0386
Epoch 241/300, resid Loss: 0.0348 | 0.0374
Epoch 242/300, resid Loss: 0.0348 | 0.0391
Epoch 243/300, resid Loss: 0.0346 | 0.0371
Epoch 244/300, resid Loss: 0.0347 | 0.0387
Epoch 245/300, resid Loss: 0.0347 | 0.0384
Epoch 246/300, resid Loss: 0.0346 | 0.0412
Epoch 247/300, resid Loss: 0.0345 | 0.0406
Epoch 248/300, resid Loss: 0.0344 | 0.0420
Epoch 249/300, resid Loss: 0.0343 | 0.0405
Epoch 250/300, resid Loss: 0.0342 | 0.0420
Epoch 251/300, resid Loss: 0.0341 | 0.0407
Epoch 252/300, resid Loss: 0.0341 | 0.0422
Epoch 253/300, resid Loss: 0.0340 | 0.0408
Epoch 254/300, resid Loss: 0.0339 | 0.0421
Epoch 255/300, resid Loss: 0.0338 | 0.0404
Epoch 256/300, resid Loss: 0.0338 | 0.0420
Epoch 257/300, resid Loss: 0.0338 | 0.0401
Epoch 258/300, resid Loss: 0.0337 | 0.0420
Epoch 259/300, resid Loss: 0.0336 | 0.0401
Epoch 260/300, resid Loss: 0.0336 | 0.0421
Epoch 261/300, resid Loss: 0.0335 | 0.0404
Epoch 262/300, resid Loss: 0.0335 | 0.0426
Epoch 263/300, resid Loss: 0.0334 | 0.0415
Epoch 264/300, resid Loss: 0.0334 | 0.0436
Epoch 265/300, resid Loss: 0.0334 | 0.0422
Epoch 266/300, resid Loss: 0.0332 | 0.0434
Epoch 267/300, resid Loss: 0.0332 | 0.0423
Epoch 268/300, resid Loss: 0.0331 | 0.0436
Epoch 269/300, resid Loss: 0.0331 | 0.0430
Epoch 270/300, resid Loss: 0.0330 | 0.0439
Epoch 271/300, resid Loss: 0.0330 | 0.0424
Epoch 272/300, resid Loss: 0.0330 | 0.0423
Epoch 273/300, resid Loss: 0.0329 | 0.0408
Epoch 274/300, resid Loss: 0.0329 | 0.0407
Epoch 275/300, resid Loss: 0.0328 | 0.0401
Epoch 276/300, resid Loss: 0.0327 | 0.0414
Epoch 277/300, resid Loss: 0.0326 | 0.0412
Epoch 278/300, resid Loss: 0.0326 | 0.0419
Epoch 279/300, resid Loss: 0.0329 | 0.0424
Epoch 280/300, resid Loss: 0.0329 | 0.0455
Epoch 281/300, resid Loss: 0.0329 | 0.0450
Epoch 282/300, resid Loss: 0.0327 | 0.0461
Epoch 283/300, resid Loss: 0.0326 | 0.0456
Epoch 284/300, resid Loss: 0.0326 | 0.0479
Epoch 285/300, resid Loss: 0.0326 | 0.0471
Epoch 286/300, resid Loss: 0.0326 | 0.0488
Epoch 287/300, resid Loss: 0.0325 | 0.0476
Epoch 288/300, resid Loss: 0.0324 | 0.0488
Epoch 289/300, resid Loss: 0.0324 | 0.0476
Epoch 290/300, resid Loss: 0.0324 | 0.0479
Epoch 291/300, resid Loss: 0.0323 | 0.0464
Epoch 292/300, resid Loss: 0.0323 | 0.0453
Epoch 293/300, resid Loss: 0.0322 | 0.0431
Epoch 294/300, resid Loss: 0.0321 | 0.0415
Epoch 295/300, resid Loss: 0.0321 | 0.0401
Epoch 296/300, resid Loss: 0.0320 | 0.0396
Epoch 297/300, resid Loss: 0.0319 | 0.0390
Epoch 298/300, resid Loss: 0.0319 | 0.0389
Epoch 299/300, resid Loss: 0.0319 | 0.0387
Epoch 300/300, resid Loss: 0.0318 | 0.0391
Runtime (seconds): 7877.850166082382
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:678: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:679: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:680: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:681: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:682: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:683: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[157.07322929]
[-3.6517987]
[4.17346153]
[11.18248249]
[4.90909282]
[22.92193566]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 109.86379669526214
RMSE: 10.48159323267518
MAE: 10.48159323267518
R-squared: nan
[196.60840311]
/data/student/k2110261/Multi-iTransformer/roop_optuna.py:725: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1]
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/roop_optuna.py", line 737, in <module>
    plt.plot(predicted_dates, close_data[-output_date:-1].values, color='black', label='learning data')
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/pyplot.py", line 3794, in plot
    return gca().plot(
           ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_axes.py", line 1779, in plot
    lines = [*self._get_lines(self, *args, data=data, **kwargs)]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_base.py", line 296, in __call__
    yield from self._plot_args(
               ^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_base.py", line 486, in _plot_args
    raise ValueError(f"x and y must have same first dimension, but "
ValueError: x and y must have same first dimension, but have shapes (10,) and (9,)
