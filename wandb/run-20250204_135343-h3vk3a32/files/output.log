[32m[I 2025-02-04 13:53:49,611][0m A new study created in memory with name: no-name-330ccc6b-0712-47ab-a822-a3fdc20ab2c6[0m
[32m[I 2025-02-04 13:54:14,634][0m Trial 0 finished with value: 0.1926601308843364 and parameters: {'observation_period_num': 147, 'train_rates': 0.886072277520835, 'learning_rate': 0.0005617283266408453, 'batch_size': 228, 'step_size': 1, 'gamma': 0.8978687205565983}. Best is trial 0 with value: 0.1926601308843364.[0m
[32m[I 2025-02-04 13:54:39,967][0m Trial 1 finished with value: 0.4635821287670443 and parameters: {'observation_period_num': 149, 'train_rates': 0.9123755042316863, 'learning_rate': 4.017198741242897e-06, 'batch_size': 238, 'step_size': 3, 'gamma': 0.9855527166539028}. Best is trial 0 with value: 0.1926601308843364.[0m
[32m[I 2025-02-04 13:56:04,607][0m Trial 2 finished with value: 0.5152901316320659 and parameters: {'observation_period_num': 196, 'train_rates': 0.945861771224185, 'learning_rate': 4.777896601723688e-06, 'batch_size': 67, 'step_size': 4, 'gamma': 0.7556714832021247}. Best is trial 0 with value: 0.1926601308843364.[0m
[32m[I 2025-02-04 13:56:40,172][0m Trial 3 finished with value: 0.05061769415495804 and parameters: {'observation_period_num': 33, 'train_rates': 0.8767169821009224, 'learning_rate': 0.000315537931406228, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8778468515733633}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 13:57:12,637][0m Trial 4 finished with value: 0.09190271675688069 and parameters: {'observation_period_num': 104, 'train_rates': 0.7948649857888024, 'learning_rate': 0.00011340651111241105, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8363733220365767}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 13:59:11,285][0m Trial 5 finished with value: 0.08429038504490981 and parameters: {'observation_period_num': 86, 'train_rates': 0.9488933230311459, 'learning_rate': 0.0006928193633118783, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8683790619079252}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:00:04,128][0m Trial 6 finished with value: 0.7584038525204875 and parameters: {'observation_period_num': 27, 'train_rates': 0.7529871798296587, 'learning_rate': 1.2424227477260641e-06, 'batch_size': 100, 'step_size': 15, 'gamma': 0.836107291078034}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:00:33,925][0m Trial 7 finished with value: 0.7124569566695245 and parameters: {'observation_period_num': 151, 'train_rates': 0.8064105729418175, 'learning_rate': 3.0103024589589353e-06, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8794623529465144}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:01:05,422][0m Trial 8 finished with value: 0.2104484615664856 and parameters: {'observation_period_num': 249, 'train_rates': 0.8875380955672905, 'learning_rate': 4.6384696633371074e-05, 'batch_size': 173, 'step_size': 15, 'gamma': 0.8705460369740907}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:02:34,047][0m Trial 9 finished with value: 0.5488905493668684 and parameters: {'observation_period_num': 112, 'train_rates': 0.8223861905310668, 'learning_rate': 2.969513569099579e-06, 'batch_size': 59, 'step_size': 9, 'gamma': 0.789866949238266}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:03:12,772][0m Trial 10 finished with value: 0.05102010428341882 and parameters: {'observation_period_num': 5, 'train_rates': 0.6342104360452218, 'learning_rate': 0.0001671733705988053, 'batch_size': 125, 'step_size': 12, 'gamma': 0.9464880304188119}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:03:52,640][0m Trial 11 finished with value: 0.07597271462870857 and parameters: {'observation_period_num': 11, 'train_rates': 0.6010062134698723, 'learning_rate': 0.00016807064202142288, 'batch_size': 114, 'step_size': 12, 'gamma': 0.9522356437608436}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:04:25,246][0m Trial 12 finished with value: 0.10072313321170917 and parameters: {'observation_period_num': 54, 'train_rates': 0.6134352236143379, 'learning_rate': 0.00020225058307826973, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9311343742930326}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:04:49,927][0m Trial 13 finished with value: 0.11738111072131112 and parameters: {'observation_period_num': 55, 'train_rates': 0.6904591588796394, 'learning_rate': 3.143315078807047e-05, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9314148406125153}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:05:32,265][0m Trial 14 finished with value: 0.10784423164182404 and parameters: {'observation_period_num': 48, 'train_rates': 0.6967901755205741, 'learning_rate': 3.244340873578088e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9579903946393121}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:06:05,818][0m Trial 15 finished with value: 0.0652479771427756 and parameters: {'observation_period_num': 13, 'train_rates': 0.6750343888194694, 'learning_rate': 0.00033506691647296373, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9107021495048165}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:07:07,737][0m Trial 16 finished with value: 0.0682810684793616 and parameters: {'observation_period_num': 69, 'train_rates': 0.8490803391121202, 'learning_rate': 6.6225273430976e-05, 'batch_size': 90, 'step_size': 10, 'gamma': 0.8325034757505123}. Best is trial 3 with value: 0.05061769415495804.[0m
[32m[I 2025-02-04 14:11:55,916][0m Trial 17 finished with value: 0.049684498755430635 and parameters: {'observation_period_num': 6, 'train_rates': 0.7476347175922328, 'learning_rate': 1.5313651479368066e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9899118427571808}. Best is trial 17 with value: 0.049684498755430635.[0m
[32m[I 2025-02-04 14:16:54,129][0m Trial 18 finished with value: 0.1668868995165523 and parameters: {'observation_period_num': 37, 'train_rates': 0.7307353013151665, 'learning_rate': 1.1351201734239262e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9856775097862774}. Best is trial 17 with value: 0.049684498755430635.[0m
[32m[I 2025-02-04 14:20:48,222][0m Trial 19 finished with value: 0.15941742139643636 and parameters: {'observation_period_num': 79, 'train_rates': 0.9802275811121175, 'learning_rate': 1.4044982724799348e-05, 'batch_size': 25, 'step_size': 6, 'gamma': 0.7921134411418037}. Best is trial 17 with value: 0.049684498755430635.[0m
[32m[I 2025-02-04 14:21:12,248][0m Trial 20 finished with value: 0.5731926155416933 and parameters: {'observation_period_num': 199, 'train_rates': 0.7365077263363059, 'learning_rate': 1.1724675131134564e-05, 'batch_size': 204, 'step_size': 8, 'gamma': 0.811012990158023}. Best is trial 17 with value: 0.049684498755430635.[0m
[32m[I 2025-02-04 14:22:09,128][0m Trial 21 finished with value: 0.04913847083417027 and parameters: {'observation_period_num': 5, 'train_rates': 0.6550571022664431, 'learning_rate': 0.0009513436018320211, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9512732532467476}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:23:15,035][0m Trial 22 finished with value: 0.07010668826883423 and parameters: {'observation_period_num': 28, 'train_rates': 0.7768115668181037, 'learning_rate': 0.0009288782644777433, 'batch_size': 79, 'step_size': 14, 'gamma': 0.9704109890384861}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:25:33,261][0m Trial 23 finished with value: 0.05833555926890022 and parameters: {'observation_period_num': 30, 'train_rates': 0.8432194714431349, 'learning_rate': 0.00044227915716468633, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9128896986717752}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:26:04,855][0m Trial 24 finished with value: 0.06585950011464015 and parameters: {'observation_period_num': 7, 'train_rates': 0.655060385582704, 'learning_rate': 0.000313385061422622, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9891440245636818}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:26:53,905][0m Trial 25 finished with value: 0.08612320618260474 and parameters: {'observation_period_num': 68, 'train_rates': 0.7106052992941406, 'learning_rate': 8.354823911427816e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9622719334286886}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:28:34,574][0m Trial 26 finished with value: 0.23693294981928364 and parameters: {'observation_period_num': 43, 'train_rates': 0.6553495369569554, 'learning_rate': 0.000935969670178434, 'batch_size': 45, 'step_size': 14, 'gamma': 0.9324458808900794}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:29:39,682][0m Trial 27 finished with value: 0.13715904133079312 and parameters: {'observation_period_num': 100, 'train_rates': 0.7651808357103851, 'learning_rate': 2.8325216021326648e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8873816127499424}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:30:10,650][0m Trial 28 finished with value: 0.09977885464142108 and parameters: {'observation_period_num': 18, 'train_rates': 0.8723040051921418, 'learning_rate': 1.8818164111922553e-05, 'batch_size': 191, 'step_size': 7, 'gamma': 0.8601374351897491}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:30:38,585][0m Trial 29 finished with value: 0.3860498411136691 and parameters: {'observation_period_num': 175, 'train_rates': 0.9145819492691792, 'learning_rate': 7.96514474009373e-06, 'batch_size': 219, 'step_size': 10, 'gamma': 0.896016566340429}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:33:17,876][0m Trial 30 finished with value: 0.15543689277936829 and parameters: {'observation_period_num': 129, 'train_rates': 0.7200766148224235, 'learning_rate': 0.0005674447020554712, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9701801370556732}. Best is trial 21 with value: 0.04913847083417027.[0m
[32m[I 2025-02-04 14:33:52,893][0m Trial 31 finished with value: 0.04822005597461553 and parameters: {'observation_period_num': 6, 'train_rates': 0.633077466902865, 'learning_rate': 0.00024517184012751023, 'batch_size': 138, 'step_size': 12, 'gamma': 0.9424834155122637}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:34:28,231][0m Trial 32 finished with value: 0.07710681045847742 and parameters: {'observation_period_num': 25, 'train_rates': 0.6307418109564985, 'learning_rate': 0.0002747567549094448, 'batch_size': 133, 'step_size': 13, 'gamma': 0.9148860352325647}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:34:48,384][0m Trial 33 finished with value: 0.1114022357299203 and parameters: {'observation_period_num': 42, 'train_rates': 0.66103998861112, 'learning_rate': 0.0005333686968929842, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9412813135259115}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:35:57,490][0m Trial 34 finished with value: 0.07322570366196182 and parameters: {'observation_period_num': 20, 'train_rates': 0.633151761722917, 'learning_rate': 0.0004266270680026715, 'batch_size': 66, 'step_size': 11, 'gamma': 0.9707392871223472}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:36:29,109][0m Trial 35 finished with value: 0.0800176914297528 and parameters: {'observation_period_num': 59, 'train_rates': 0.682391973914597, 'learning_rate': 0.00010938535936884859, 'batch_size': 153, 'step_size': 15, 'gamma': 0.9767038132955324}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:37:08,454][0m Trial 36 finished with value: 0.12882109500210862 and parameters: {'observation_period_num': 5, 'train_rates': 0.9218582744548294, 'learning_rate': 5.908578638131551e-06, 'batch_size': 160, 'step_size': 13, 'gamma': 0.8540690568631248}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:37:57,085][0m Trial 37 finished with value: 0.07145426856974761 and parameters: {'observation_period_num': 35, 'train_rates': 0.7486489525383262, 'learning_rate': 0.0007065909927407497, 'batch_size': 105, 'step_size': 1, 'gamma': 0.9030149867630364}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:38:31,143][0m Trial 38 finished with value: 0.1092323346845693 and parameters: {'observation_period_num': 93, 'train_rates': 0.8851148913814567, 'learning_rate': 0.0002127588920025947, 'batch_size': 179, 'step_size': 9, 'gamma': 0.9234903726169728}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:39:13,229][0m Trial 39 finished with value: 0.08456109689358808 and parameters: {'observation_period_num': 73, 'train_rates': 0.8000076968872185, 'learning_rate': 4.984909634891455e-05, 'batch_size': 133, 'step_size': 12, 'gamma': 0.9446194203405431}. Best is trial 31 with value: 0.04822005597461553.[0m
[32m[I 2025-02-04 14:40:41,187][0m Trial 40 finished with value: 0.04188542958866594 and parameters: {'observation_period_num': 20, 'train_rates': 0.8268734431780809, 'learning_rate': 0.00012868453834289438, 'batch_size': 61, 'step_size': 4, 'gamma': 0.9552356731610655}. Best is trial 40 with value: 0.04188542958866594.[0m
[32m[I 2025-02-04 14:42:22,249][0m Trial 41 finished with value: 0.04367919509271974 and parameters: {'observation_period_num': 21, 'train_rates': 0.8252840585593032, 'learning_rate': 0.0001319576962308142, 'batch_size': 52, 'step_size': 3, 'gamma': 0.9591118321116661}. Best is trial 40 with value: 0.04188542958866594.[0m
[32m[I 2025-02-04 14:44:00,862][0m Trial 42 finished with value: 0.0349555596791372 and parameters: {'observation_period_num': 16, 'train_rates': 0.8310873273368429, 'learning_rate': 0.00014129175570302187, 'batch_size': 55, 'step_size': 3, 'gamma': 0.9601017125192325}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:45:35,872][0m Trial 43 finished with value: 0.047328181725506686 and parameters: {'observation_period_num': 21, 'train_rates': 0.8271637862939043, 'learning_rate': 0.00011168212443509637, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9577233048138563}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:47:11,531][0m Trial 44 finished with value: 0.049074852711943735 and parameters: {'observation_period_num': 23, 'train_rates': 0.830902429800743, 'learning_rate': 0.0001205290295729416, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9578309558220892}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:48:54,229][0m Trial 45 finished with value: 0.06911674750807845 and parameters: {'observation_period_num': 47, 'train_rates': 0.8615155666572649, 'learning_rate': 0.0001350050251262109, 'batch_size': 53, 'step_size': 3, 'gamma': 0.7522204759997241}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:50:16,445][0m Trial 46 finished with value: 0.06456431428470262 and parameters: {'observation_period_num': 58, 'train_rates': 0.8170691263756916, 'learning_rate': 7.536443368809007e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9410503743864223}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:52:23,403][0m Trial 47 finished with value: 0.15556847264314444 and parameters: {'observation_period_num': 235, 'train_rates': 0.8013909971298734, 'learning_rate': 0.0002312430104754476, 'batch_size': 38, 'step_size': 2, 'gamma': 0.9237565367138582}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:53:23,518][0m Trial 48 finished with value: 0.07460631485681356 and parameters: {'observation_period_num': 34, 'train_rates': 0.7807061530930904, 'learning_rate': 5.0458611295991064e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.9758673124568006}. Best is trial 42 with value: 0.0349555596791372.[0m
[32m[I 2025-02-04 14:56:04,072][0m Trial 49 finished with value: 0.04020504219340383 and parameters: {'observation_period_num': 19, 'train_rates': 0.8330958915665009, 'learning_rate': 8.997859862183378e-05, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9610454646567944}. Best is trial 42 with value: 0.0349555596791372.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.2192 | 0.2037
Epoch 2/300, Loss: 0.1539 | 0.1936
Epoch 3/300, Loss: 0.1062 | 0.1313
Epoch 4/300, Loss: 0.0916 | 0.1036
Epoch 5/300, Loss: 0.0826 | 0.0856
Epoch 6/300, Loss: 0.0778 | 0.0763
Epoch 7/300, Loss: 0.0757 | 0.0756
Epoch 8/300, Loss: 0.0761 | 0.0786
Epoch 9/300, Loss: 0.0758 | 0.0748
Epoch 10/300, Loss: 0.0720 | 0.0705
Epoch 11/300, Loss: 0.0681 | 0.0695
Epoch 12/300, Loss: 0.0665 | 0.0690
Epoch 13/300, Loss: 0.0653 | 0.0678
Epoch 14/300, Loss: 0.0638 | 0.0678
Epoch 15/300, Loss: 0.0632 | 0.0671
Epoch 16/300, Loss: 0.0622 | 0.0665
Epoch 17/300, Loss: 0.0617 | 0.0679
Epoch 18/300, Loss: 0.0618 | 0.0703
Epoch 19/300, Loss: 0.0604 | 0.0720
Epoch 20/300, Loss: 0.0589 | 0.0694
Epoch 21/300, Loss: 0.0563 | 0.0675
Epoch 22/300, Loss: 0.0544 | 0.0680
Epoch 23/300, Loss: 0.0540 | 0.0675
Epoch 24/300, Loss: 0.0532 | 0.0660
Epoch 25/300, Loss: 0.0519 | 0.0657
Epoch 26/300, Loss: 0.0511 | 0.0653
Epoch 27/300, Loss: 0.0504 | 0.0654
Epoch 28/300, Loss: 0.0501 | 0.0653
Epoch 29/300, Loss: 0.0495 | 0.0639
Epoch 30/300, Loss: 0.0486 | 0.0629
Epoch 31/300, Loss: 0.0480 | 0.0597
Epoch 32/300, Loss: 0.0466 | 0.0549
Epoch 33/300, Loss: 0.0455 | 0.0510
Epoch 34/300, Loss: 0.0443 | 0.0496
Epoch 35/300, Loss: 0.0422 | 0.0482
Epoch 36/300, Loss: 0.0406 | 0.0473
Epoch 37/300, Loss: 0.0400 | 0.0464
Epoch 38/300, Loss: 0.0399 | 0.0463
Epoch 39/300, Loss: 0.0392 | 0.0456
Epoch 40/300, Loss: 0.0386 | 0.0451
Epoch 41/300, Loss: 0.0383 | 0.0450
Epoch 42/300, Loss: 0.0382 | 0.0448
Epoch 43/300, Loss: 0.0377 | 0.0444
Epoch 44/300, Loss: 0.0374 | 0.0441
Epoch 45/300, Loss: 0.0372 | 0.0440
Epoch 46/300, Loss: 0.0370 | 0.0438
Epoch 47/300, Loss: 0.0367 | 0.0436
Epoch 48/300, Loss: 0.0364 | 0.0433
Epoch 49/300, Loss: 0.0363 | 0.0433
Epoch 50/300, Loss: 0.0362 | 0.0431
Epoch 51/300, Loss: 0.0359 | 0.0429
Epoch 52/300, Loss: 0.0358 | 0.0428
Epoch 53/300, Loss: 0.0357 | 0.0427
Epoch 54/300, Loss: 0.0356 | 0.0426
Epoch 55/300, Loss: 0.0355 | 0.0425
Epoch 56/300, Loss: 0.0354 | 0.0424
Epoch 57/300, Loss: 0.0353 | 0.0424
Epoch 58/300, Loss: 0.0353 | 0.0424
Epoch 59/300, Loss: 0.0353 | 0.0424
Epoch 60/300, Loss: 0.0353 | 0.0425
Epoch 61/300, Loss: 0.0355 | 0.0426
Epoch 62/300, Loss: 0.0355 | 0.0426
Epoch 63/300, Loss: 0.0355 | 0.0426
Epoch 64/300, Loss: 0.0353 | 0.0424
Epoch 65/300, Loss: 0.0348 | 0.0422
Epoch 66/300, Loss: 0.0343 | 0.0419
Epoch 67/300, Loss: 0.0337 | 0.0416
Epoch 68/300, Loss: 0.0332 | 0.0414
Epoch 69/300, Loss: 0.0327 | 0.0412
Epoch 70/300, Loss: 0.0322 | 0.0410
Epoch 71/300, Loss: 0.0319 | 0.0409
Epoch 72/300, Loss: 0.0316 | 0.0407
Epoch 73/300, Loss: 0.0313 | 0.0406
Epoch 74/300, Loss: 0.0311 | 0.0405
Epoch 75/300, Loss: 0.0309 | 0.0404
Epoch 76/300, Loss: 0.0308 | 0.0403
Epoch 77/300, Loss: 0.0307 | 0.0402
Epoch 78/300, Loss: 0.0305 | 0.0401
Epoch 79/300, Loss: 0.0304 | 0.0400
Epoch 80/300, Loss: 0.0303 | 0.0399
Epoch 81/300, Loss: 0.0302 | 0.0398
Epoch 82/300, Loss: 0.0301 | 0.0397
Epoch 83/300, Loss: 0.0300 | 0.0397
Epoch 84/300, Loss: 0.0299 | 0.0396
Epoch 85/300, Loss: 0.0298 | 0.0395
Epoch 86/300, Loss: 0.0298 | 0.0395
Epoch 87/300, Loss: 0.0297 | 0.0394
Epoch 88/300, Loss: 0.0296 | 0.0394
Epoch 89/300, Loss: 0.0295 | 0.0394
Epoch 90/300, Loss: 0.0294 | 0.0393
Epoch 91/300, Loss: 0.0294 | 0.0393
Epoch 92/300, Loss: 0.0293 | 0.0392
Epoch 93/300, Loss: 0.0292 | 0.0392
Epoch 94/300, Loss: 0.0291 | 0.0392
Epoch 95/300, Loss: 0.0291 | 0.0391
Epoch 96/300, Loss: 0.0290 | 0.0391
Epoch 97/300, Loss: 0.0290 | 0.0391
Epoch 98/300, Loss: 0.0289 | 0.0390
Epoch 99/300, Loss: 0.0288 | 0.0390
Epoch 100/300, Loss: 0.0288 | 0.0390
Epoch 101/300, Loss: 0.0287 | 0.0390
Epoch 102/300, Loss: 0.0287 | 0.0389
Epoch 103/300, Loss: 0.0286 | 0.0389
Epoch 104/300, Loss: 0.0285 | 0.0389
Epoch 105/300, Loss: 0.0285 | 0.0388
Epoch 106/300, Loss: 0.0284 | 0.0388
Epoch 107/300, Loss: 0.0283 | 0.0388
Epoch 108/300, Loss: 0.0283 | 0.0388
Epoch 109/300, Loss: 0.0282 | 0.0388
Epoch 110/300, Loss: 0.0281 | 0.0387
Epoch 111/300, Loss: 0.0281 | 0.0387
Epoch 112/300, Loss: 0.0280 | 0.0387
Epoch 113/300, Loss: 0.0280 | 0.0387
Epoch 114/300, Loss: 0.0279 | 0.0387
Epoch 115/300, Loss: 0.0279 | 0.0387
Epoch 116/300, Loss: 0.0278 | 0.0387
Epoch 117/300, Loss: 0.0278 | 0.0386
Epoch 118/300, Loss: 0.0277 | 0.0386
Epoch 119/300, Loss: 0.0277 | 0.0386
Epoch 120/300, Loss: 0.0276 | 0.0386
Epoch 121/300, Loss: 0.0276 | 0.0386
Epoch 122/300, Loss: 0.0276 | 0.0386
Epoch 123/300, Loss: 0.0275 | 0.0385
Epoch 124/300, Loss: 0.0275 | 0.0385
Epoch 125/300, Loss: 0.0275 | 0.0385
Epoch 126/300, Loss: 0.0274 | 0.0385
Epoch 127/300, Loss: 0.0274 | 0.0385
Epoch 128/300, Loss: 0.0274 | 0.0385
Epoch 129/300, Loss: 0.0273 | 0.0385
Epoch 130/300, Loss: 0.0273 | 0.0385
Epoch 131/300, Loss: 0.0273 | 0.0384
Epoch 132/300, Loss: 0.0272 | 0.0384
Epoch 133/300, Loss: 0.0272 | 0.0384
Epoch 134/300, Loss: 0.0272 | 0.0384
Epoch 135/300, Loss: 0.0272 | 0.0384
Epoch 136/300, Loss: 0.0271 | 0.0384
Epoch 137/300, Loss: 0.0271 | 0.0384
Epoch 138/300, Loss: 0.0271 | 0.0384
Epoch 139/300, Loss: 0.0271 | 0.0384
Epoch 140/300, Loss: 0.0271 | 0.0383
Epoch 141/300, Loss: 0.0270 | 0.0383
Epoch 142/300, Loss: 0.0270 | 0.0383
Epoch 143/300, Loss: 0.0270 | 0.0383
Epoch 144/300, Loss: 0.0270 | 0.0383
Epoch 145/300, Loss: 0.0270 | 0.0383
Epoch 146/300, Loss: 0.0269 | 0.0383
Epoch 147/300, Loss: 0.0269 | 0.0383
Epoch 148/300, Loss: 0.0269 | 0.0383
Epoch 149/300, Loss: 0.0269 | 0.0383
Epoch 150/300, Loss: 0.0269 | 0.0383
Epoch 151/300, Loss: 0.0269 | 0.0383
Epoch 152/300, Loss: 0.0269 | 0.0382
Epoch 153/300, Loss: 0.0268 | 0.0382
Epoch 154/300, Loss: 0.0268 | 0.0382
Epoch 155/300, Loss: 0.0268 | 0.0382
Epoch 156/300, Loss: 0.0268 | 0.0382
Epoch 157/300, Loss: 0.0268 | 0.0382
Epoch 158/300, Loss: 0.0268 | 0.0382
Epoch 159/300, Loss: 0.0267 | 0.0382
Epoch 160/300, Loss: 0.0267 | 0.0382
Epoch 161/300, Loss: 0.0267 | 0.0381
Epoch 162/300, Loss: 0.0267 | 0.0381
Epoch 163/300, Loss: 0.0267 | 0.0381
Epoch 164/300, Loss: 0.0267 | 0.0381
Epoch 165/300, Loss: 0.0267 | 0.0381
Epoch 166/300, Loss: 0.0266 | 0.0381
Epoch 167/300, Loss: 0.0266 | 0.0381
Epoch 168/300, Loss: 0.0266 | 0.0381
Epoch 169/300, Loss: 0.0266 | 0.0381
Epoch 170/300, Loss: 0.0266 | 0.0381
Epoch 171/300, Loss: 0.0266 | 0.0381
Epoch 172/300, Loss: 0.0266 | 0.0381
Epoch 173/300, Loss: 0.0265 | 0.0381
Epoch 174/300, Loss: 0.0265 | 0.0381
Epoch 175/300, Loss: 0.0265 | 0.0381
Epoch 176/300, Loss: 0.0265 | 0.0380
Epoch 177/300, Loss: 0.0265 | 0.0380
Epoch 178/300, Loss: 0.0265 | 0.0380
Epoch 179/300, Loss: 0.0265 | 0.0380
Epoch 180/300, Loss: 0.0265 | 0.0380
Epoch 181/300, Loss: 0.0265 | 0.0380
Epoch 182/300, Loss: 0.0265 | 0.0380
Epoch 183/300, Loss: 0.0264 | 0.0380
Epoch 184/300, Loss: 0.0264 | 0.0380
Epoch 185/300, Loss: 0.0264 | 0.0380
Epoch 186/300, Loss: 0.0264 | 0.0380
Epoch 187/300, Loss: 0.0264 | 0.0380
Epoch 188/300, Loss: 0.0264 | 0.0380
Epoch 189/300, Loss: 0.0264 | 0.0380
Epoch 190/300, Loss: 0.0264 | 0.0380
Epoch 191/300, Loss: 0.0264 | 0.0380
Epoch 192/300, Loss: 0.0264 | 0.0380
Epoch 193/300, Loss: 0.0264 | 0.0380
Epoch 194/300, Loss: 0.0264 | 0.0380
Epoch 195/300, Loss: 0.0264 | 0.0380
Epoch 196/300, Loss: 0.0263 | 0.0380
Epoch 197/300, Loss: 0.0263 | 0.0380
Epoch 198/300, Loss: 0.0263 | 0.0380
Epoch 199/300, Loss: 0.0263 | 0.0380
Epoch 200/300, Loss: 0.0263 | 0.0380
Epoch 201/300, Loss: 0.0263 | 0.0380
Epoch 202/300, Loss: 0.0263 | 0.0380
Epoch 203/300, Loss: 0.0263 | 0.0380
Epoch 204/300, Loss: 0.0263 | 0.0380
Epoch 205/300, Loss: 0.0263 | 0.0380
Epoch 206/300, Loss: 0.0263 | 0.0380
Epoch 207/300, Loss: 0.0263 | 0.0380
Epoch 208/300, Loss: 0.0263 | 0.0380
Epoch 209/300, Loss: 0.0263 | 0.0380
Epoch 210/300, Loss: 0.0263 | 0.0380
Epoch 211/300, Loss: 0.0263 | 0.0380
Epoch 212/300, Loss: 0.0263 | 0.0380
Epoch 213/300, Loss: 0.0263 | 0.0380
Epoch 214/300, Loss: 0.0263 | 0.0380
Epoch 215/300, Loss: 0.0263 | 0.0380
Epoch 216/300, Loss: 0.0263 | 0.0380
Epoch 217/300, Loss: 0.0263 | 0.0380
Epoch 218/300, Loss: 0.0262 | 0.0380
Epoch 219/300, Loss: 0.0262 | 0.0380
Epoch 220/300, Loss: 0.0262 | 0.0380
Epoch 221/300, Loss: 0.0262 | 0.0380
Epoch 222/300, Loss: 0.0262 | 0.0380
Epoch 223/300, Loss: 0.0262 | 0.0380
Epoch 224/300, Loss: 0.0262 | 0.0380
Epoch 225/300, Loss: 0.0262 | 0.0379
Epoch 226/300, Loss: 0.0262 | 0.0379
Epoch 227/300, Loss: 0.0262 | 0.0379
Epoch 228/300, Loss: 0.0262 | 0.0379
Epoch 229/300, Loss: 0.0262 | 0.0379
Epoch 230/300, Loss: 0.0262 | 0.0379
Epoch 231/300, Loss: 0.0262 | 0.0379
Epoch 232/300, Loss: 0.0262 | 0.0379
Epoch 233/300, Loss: 0.0262 | 0.0379
Epoch 234/300, Loss: 0.0262 | 0.0379
Epoch 235/300, Loss: 0.0262 | 0.0379
Epoch 236/300, Loss: 0.0262 | 0.0379
Epoch 237/300, Loss: 0.0262 | 0.0379
Epoch 238/300, Loss: 0.0262 | 0.0379
Epoch 239/300, Loss: 0.0262 | 0.0379
Epoch 240/300, Loss: 0.0262 | 0.0379
Epoch 241/300, Loss: 0.0262 | 0.0379
Epoch 242/300, Loss: 0.0262 | 0.0379
Epoch 243/300, Loss: 0.0262 | 0.0379
Epoch 244/300, Loss: 0.0262 | 0.0379
Epoch 245/300, Loss: 0.0262 | 0.0379
Epoch 246/300, Loss: 0.0262 | 0.0379
Epoch 247/300, Loss: 0.0262 | 0.0379
Epoch 248/300, Loss: 0.0262 | 0.0379
Epoch 249/300, Loss: 0.0262 | 0.0379
Epoch 250/300, Loss: 0.0262 | 0.0379
Epoch 251/300, Loss: 0.0262 | 0.0379
Epoch 252/300, Loss: 0.0262 | 0.0379
Epoch 253/300, Loss: 0.0262 | 0.0379
Epoch 254/300, Loss: 0.0262 | 0.0379
Epoch 255/300, Loss: 0.0262 | 0.0379
Epoch 256/300, Loss: 0.0262 | 0.0379
Epoch 257/300, Loss: 0.0262 | 0.0379
Epoch 258/300, Loss: 0.0262 | 0.0379
Epoch 259/300, Loss: 0.0262 | 0.0379
Epoch 260/300, Loss: 0.0262 | 0.0379
Epoch 261/300, Loss: 0.0262 | 0.0379
Epoch 262/300, Loss: 0.0262 | 0.0379
Epoch 263/300, Loss: 0.0262 | 0.0379
Epoch 264/300, Loss: 0.0262 | 0.0379
Epoch 265/300, Loss: 0.0262 | 0.0379
Epoch 266/300, Loss: 0.0262 | 0.0379
Epoch 267/300, Loss: 0.0262 | 0.0379
Epoch 268/300, Loss: 0.0262 | 0.0379
Epoch 269/300, Loss: 0.0262 | 0.0379
Epoch 270/300, Loss: 0.0262 | 0.0379
Epoch 271/300, Loss: 0.0262 | 0.0379
Epoch 272/300, Loss: 0.0262 | 0.0379
Epoch 273/300, Loss: 0.0262 | 0.0379
Epoch 274/300, Loss: 0.0262 | 0.0379
Epoch 275/300, Loss: 0.0262 | 0.0379
Epoch 276/300, Loss: 0.0262 | 0.0379
Epoch 277/300, Loss: 0.0262 | 0.0379
Epoch 278/300, Loss: 0.0262 | 0.0379
Epoch 279/300, Loss: 0.0262 | 0.0379
Epoch 280/300, Loss: 0.0262 | 0.0379
Epoch 281/300, Loss: 0.0262 | 0.0379
Epoch 282/300, Loss: 0.0262 | 0.0379
Epoch 283/300, Loss: 0.0262 | 0.0379
Epoch 284/300, Loss: 0.0262 | 0.0379
Epoch 285/300, Loss: 0.0261 | 0.0379
Epoch 286/300, Loss: 0.0261 | 0.0379
Epoch 287/300, Loss: 0.0261 | 0.0379
Epoch 288/300, Loss: 0.0261 | 0.0379
Epoch 289/300, Loss: 0.0261 | 0.0379
Epoch 290/300, Loss: 0.0261 | 0.0379
Epoch 291/300, Loss: 0.0261 | 0.0379
Epoch 292/300, Loss: 0.0261 | 0.0379
Epoch 293/300, Loss: 0.0261 | 0.0379
Epoch 294/300, Loss: 0.0261 | 0.0379
Epoch 295/300, Loss: 0.0261 | 0.0379
Epoch 296/300, Loss: 0.0261 | 0.0379
Epoch 297/300, Loss: 0.0261 | 0.0379
Epoch 298/300, Loss: 0.0261 | 0.0379
Epoch 299/300, Loss: 0.0261 | 0.0379
Epoch 300/300, Loss: 0.0261 | 0.0379
Runtime (seconds): 290.4916636943817
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1.4410731979296543
RMSE: 1.2004470825195312
MAE: 1.2004470825195312
R-squared: nan
[106.71045]
