[32m[I 2025-02-09 02:46:13,922][0m A new study created in memory with name: no-name-49c7bd0a-0bf0-4823-a6ad-a83b0fd0c7d1[0m
[32m[I 2025-02-09 02:46:40,807][0m Trial 0 finished with value: 0.7349493655061938 and parameters: {'observation_period_num': 139, 'train_rates': 0.7690516331044034, 'learning_rate': 2.8378447968001302e-06, 'batch_size': 203, 'step_size': 8, 'gamma': 0.914053750392794}. Best is trial 0 with value: 0.7349493655061938.[0m
[32m[I 2025-02-09 02:48:04,842][0m Trial 1 finished with value: 0.6202155605510429 and parameters: {'observation_period_num': 169, 'train_rates': 0.7711701766643966, 'learning_rate': 7.598860081073859e-06, 'batch_size': 60, 'step_size': 3, 'gamma': 0.852436456943342}. Best is trial 1 with value: 0.6202155605510429.[0m
[32m[I 2025-02-09 02:48:38,749][0m Trial 2 finished with value: 0.7335317730903625 and parameters: {'observation_period_num': 201, 'train_rates': 0.9789835837757126, 'learning_rate': 2.3144672399332886e-05, 'batch_size': 187, 'step_size': 2, 'gamma': 0.8601466033674127}. Best is trial 1 with value: 0.6202155605510429.[0m
[32m[I 2025-02-09 02:49:15,666][0m Trial 3 finished with value: 0.05891504467525332 and parameters: {'observation_period_num': 52, 'train_rates': 0.8064096720803096, 'learning_rate': 0.000845610836957443, 'batch_size': 149, 'step_size': 8, 'gamma': 0.7945851379965929}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:49:42,325][0m Trial 4 finished with value: 0.27292410516329396 and parameters: {'observation_period_num': 180, 'train_rates': 0.7679331028124996, 'learning_rate': 0.00036918323066847687, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9128508241568903}. Best is trial 3 with value: 0.05891504467525332.[0m
Early stopping at epoch 50
[32m[I 2025-02-09 02:51:17,658][0m Trial 5 finished with value: 0.5206223054977747 and parameters: {'observation_period_num': 107, 'train_rates': 0.896160181720814, 'learning_rate': 1.5056988184499975e-05, 'batch_size': 30, 'step_size': 1, 'gamma': 0.787444631334058}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:52:17,695][0m Trial 6 finished with value: 1.1235743595598624 and parameters: {'observation_period_num': 182, 'train_rates': 0.8875109604110043, 'learning_rate': 1.4971810461936666e-06, 'batch_size': 90, 'step_size': 4, 'gamma': 0.8510202653848358}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:52:51,476][0m Trial 7 finished with value: 0.9527088882935107 and parameters: {'observation_period_num': 198, 'train_rates': 0.7161047699853625, 'learning_rate': 3.826972211963768e-06, 'batch_size': 144, 'step_size': 3, 'gamma': 0.9139363306557773}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:53:40,603][0m Trial 8 finished with value: 0.2038548119190861 and parameters: {'observation_period_num': 78, 'train_rates': 0.7094351571637015, 'learning_rate': 9.057085850432126e-05, 'batch_size': 99, 'step_size': 5, 'gamma': 0.8548741437567298}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:55:23,420][0m Trial 9 finished with value: 0.19741754419094806 and parameters: {'observation_period_num': 18, 'train_rates': 0.8139877882169027, 'learning_rate': 2.077657044793819e-06, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8768308805019421}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:55:44,269][0m Trial 10 finished with value: 0.12279720429878135 and parameters: {'observation_period_num': 14, 'train_rates': 0.6169361476512188, 'learning_rate': 0.000868018012151195, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9875461833776443}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:56:04,751][0m Trial 11 finished with value: 0.12015565541926129 and parameters: {'observation_period_num': 12, 'train_rates': 0.6119514330139271, 'learning_rate': 0.0007742241671212893, 'batch_size': 249, 'step_size': 15, 'gamma': 0.7509492912536496}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:56:25,711][0m Trial 12 finished with value: 0.7460264923307631 and parameters: {'observation_period_num': 58, 'train_rates': 0.6179001322916717, 'learning_rate': 0.00014073868454981276, 'batch_size': 256, 'step_size': 14, 'gamma': 0.752253724923227}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:56:59,761][0m Trial 13 finished with value: 0.16315099282581716 and parameters: {'observation_period_num': 51, 'train_rates': 0.6808503101087781, 'learning_rate': 0.0009214329416354042, 'batch_size': 147, 'step_size': 12, 'gamma': 0.7924516573828918}. Best is trial 3 with value: 0.05891504467525332.[0m
[32m[I 2025-02-09 02:57:27,737][0m Trial 14 finished with value: 0.0539721971154213 and parameters: {'observation_period_num': 6, 'train_rates': 0.8331654497534415, 'learning_rate': 0.00020903859175749062, 'batch_size': 221, 'step_size': 9, 'gamma': 0.7660231237955872}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 02:58:02,937][0m Trial 15 finished with value: 0.21786375807740757 and parameters: {'observation_period_num': 232, 'train_rates': 0.863296733097871, 'learning_rate': 0.00013593097630659742, 'batch_size': 166, 'step_size': 8, 'gamma': 0.8045674951277592}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 02:58:54,979][0m Trial 16 finished with value: 0.08129941994052924 and parameters: {'observation_period_num': 92, 'train_rates': 0.827790821459954, 'learning_rate': 0.00030277220929705964, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8161391471123002}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 02:59:24,255][0m Trial 17 finished with value: 0.1410728394985199 and parameters: {'observation_period_num': 37, 'train_rates': 0.9515807907229085, 'learning_rate': 5.439763610895709e-05, 'batch_size': 227, 'step_size': 11, 'gamma': 0.77122214376866}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 02:59:58,314][0m Trial 18 finished with value: 0.09887766606573548 and parameters: {'observation_period_num': 119, 'train_rates': 0.854295276389555, 'learning_rate': 0.0003199337838066796, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8317174215086497}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 03:00:28,154][0m Trial 19 finished with value: 0.24805869162082672 and parameters: {'observation_period_num': 69, 'train_rates': 0.9349881664312347, 'learning_rate': 4.8906202751683575e-05, 'batch_size': 213, 'step_size': 6, 'gamma': 0.7761129106008233}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 03:00:54,397][0m Trial 20 finished with value: 0.05481625054459516 and parameters: {'observation_period_num': 37, 'train_rates': 0.8004693715761735, 'learning_rate': 0.00042659199468532237, 'batch_size': 227, 'step_size': 12, 'gamma': 0.8231254568697844}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 03:01:20,214][0m Trial 21 finished with value: 0.057570679667028224 and parameters: {'observation_period_num': 38, 'train_rates': 0.8003073270296411, 'learning_rate': 0.0004107298002017028, 'batch_size': 225, 'step_size': 12, 'gamma': 0.8193646266352332}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 03:01:44,324][0m Trial 22 finished with value: 0.18435990068356733 and parameters: {'observation_period_num': 35, 'train_rates': 0.7506264536768898, 'learning_rate': 0.00020379621314570002, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8258084468661461}. Best is trial 14 with value: 0.0539721971154213.[0m
[32m[I 2025-02-09 03:02:11,681][0m Trial 23 finished with value: 0.052613710449939524 and parameters: {'observation_period_num': 36, 'train_rates': 0.8545259850906186, 'learning_rate': 0.0004984212876514005, 'batch_size': 229, 'step_size': 11, 'gamma': 0.8862631632446661}. Best is trial 23 with value: 0.052613710449939524.[0m
[32m[I 2025-02-09 03:02:46,216][0m Trial 24 finished with value: 0.037201940041521324 and parameters: {'observation_period_num': 6, 'train_rates': 0.8466718461447285, 'learning_rate': 0.00046385476641856217, 'batch_size': 180, 'step_size': 10, 'gamma': 0.8890505928202375}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:03:18,234][0m Trial 25 finished with value: 0.05272649696578423 and parameters: {'observation_period_num': 5, 'train_rates': 0.8543213806365323, 'learning_rate': 7.8432636303011e-05, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8912647964859715}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:03:55,042][0m Trial 26 finished with value: 0.04877512732939966 and parameters: {'observation_period_num': 5, 'train_rates': 0.9098301094171646, 'learning_rate': 8.02544682172015e-05, 'batch_size': 176, 'step_size': 10, 'gamma': 0.8906649432844359}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:04:42,174][0m Trial 27 finished with value: 0.11187619054917156 and parameters: {'observation_period_num': 81, 'train_rates': 0.8877357510875368, 'learning_rate': 2.5849593013623154e-05, 'batch_size': 129, 'step_size': 10, 'gamma': 0.9464514315250258}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:05:17,548][0m Trial 28 finished with value: 0.04651429309246409 and parameters: {'observation_period_num': 28, 'train_rates': 0.9204382410436692, 'learning_rate': 0.0005007736402118739, 'batch_size': 172, 'step_size': 11, 'gamma': 0.8902044727277804}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:05:54,710][0m Trial 29 finished with value: 0.37226951712428935 and parameters: {'observation_period_num': 136, 'train_rates': 0.9298115621891772, 'learning_rate': 1.2978544674691697e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.9453874334456818}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:06:26,242][0m Trial 30 finished with value: 0.16041533924915172 and parameters: {'observation_period_num': 103, 'train_rates': 0.9164485358260979, 'learning_rate': 0.0002022757795641631, 'batch_size': 200, 'step_size': 13, 'gamma': 0.9337191114083264}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:07:20,832][0m Trial 31 finished with value: 0.04421456158161163 and parameters: {'observation_period_num': 24, 'train_rates': 0.9661825641408012, 'learning_rate': 0.0005289151037731339, 'batch_size': 119, 'step_size': 11, 'gamma': 0.8926819509222852}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:08:10,803][0m Trial 32 finished with value: 0.06606535613536835 and parameters: {'observation_period_num': 24, 'train_rates': 0.985279139969746, 'learning_rate': 0.0005682473261640618, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8988640840275715}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:09:04,729][0m Trial 33 finished with value: 0.04954451460297368 and parameters: {'observation_period_num': 23, 'train_rates': 0.9601918643137564, 'learning_rate': 0.00013808816521415724, 'batch_size': 115, 'step_size': 13, 'gamma': 0.8716948433117097}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:09:42,909][0m Trial 34 finished with value: 0.11744438778555048 and parameters: {'observation_period_num': 61, 'train_rates': 0.9060919004291655, 'learning_rate': 0.0005941051494701592, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9270903282921653}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:11:03,788][0m Trial 35 finished with value: 0.06596378204615219 and parameters: {'observation_period_num': 27, 'train_rates': 0.9537656894020884, 'learning_rate': 0.00028809768360720927, 'batch_size': 75, 'step_size': 10, 'gamma': 0.9015088909377893}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:11:39,143][0m Trial 36 finished with value: 0.3081512749195099 and parameters: {'observation_period_num': 156, 'train_rates': 0.9703139420149132, 'learning_rate': 3.7772306366211856e-05, 'batch_size': 183, 'step_size': 11, 'gamma': 0.8455184051651545}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:12:12,438][0m Trial 37 finished with value: 0.10112297975529845 and parameters: {'observation_period_num': 49, 'train_rates': 0.9296927986439084, 'learning_rate': 7.642176939928673e-05, 'batch_size': 191, 'step_size': 9, 'gamma': 0.8644163304548838}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:12:59,095][0m Trial 38 finished with value: 0.5783336183733561 and parameters: {'observation_period_num': 248, 'train_rates': 0.8722895373125351, 'learning_rate': 4.223765874234668e-06, 'batch_size': 117, 'step_size': 7, 'gamma': 0.8840224540986849}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:13:43,135][0m Trial 39 finished with value: 0.2648585118538092 and parameters: {'observation_period_num': 71, 'train_rates': 0.9174803085602531, 'learning_rate': 1.562374220261223e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9063745425223216}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:14:20,421][0m Trial 40 finished with value: 0.08233052492141724 and parameters: {'observation_period_num': 48, 'train_rates': 0.9869306223204375, 'learning_rate': 0.0006354853009482968, 'batch_size': 174, 'step_size': 8, 'gamma': 0.9234910871703889}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:15:13,824][0m Trial 41 finished with value: 0.05218330893053937 and parameters: {'observation_period_num': 23, 'train_rates': 0.9551819782569723, 'learning_rate': 0.00012283347312739418, 'batch_size': 118, 'step_size': 13, 'gamma': 0.8705176212248009}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:16:21,585][0m Trial 42 finished with value: 0.04575859968151365 and parameters: {'observation_period_num': 21, 'train_rates': 0.9648156938754539, 'learning_rate': 0.00023792978423393802, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8727788689674915}. Best is trial 24 with value: 0.037201940041521324.[0m
[32m[I 2025-02-09 03:17:25,656][0m Trial 43 finished with value: 0.035386107141538614 and parameters: {'observation_period_num': 7, 'train_rates': 0.8938679519494246, 'learning_rate': 0.00024168369077548434, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8409420928930037}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:18:38,509][0m Trial 44 finished with value: 0.04311346490593518 and parameters: {'observation_period_num': 17, 'train_rates': 0.8861029021306971, 'learning_rate': 0.00024447392498897185, 'batch_size': 82, 'step_size': 14, 'gamma': 0.857821417507189}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:20:07,040][0m Trial 45 finished with value: 0.04570662640035152 and parameters: {'observation_period_num': 16, 'train_rates': 0.8910905405795131, 'learning_rate': 0.00024381822261253542, 'batch_size': 65, 'step_size': 14, 'gamma': 0.8415476081002029}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:22:00,307][0m Trial 46 finished with value: 0.039523293609240596 and parameters: {'observation_period_num': 13, 'train_rates': 0.8798292591373403, 'learning_rate': 0.0001700831993273129, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8409362453000027}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:24:00,199][0m Trial 47 finished with value: 0.3397246362597105 and parameters: {'observation_period_num': 13, 'train_rates': 0.775249638121937, 'learning_rate': 1.0131944103365977e-06, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8589066526400555}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:25:06,714][0m Trial 48 finished with value: 0.05446800439335907 and parameters: {'observation_period_num': 5, 'train_rates': 0.8362039411262875, 'learning_rate': 0.0009734960426295055, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8454874385376463}. Best is trial 43 with value: 0.035386107141538614.[0m
[32m[I 2025-02-09 03:27:23,478][0m Trial 49 finished with value: 0.06988853459080605 and parameters: {'observation_period_num': 48, 'train_rates': 0.8763835879374053, 'learning_rate': 0.00016732541848893756, 'batch_size': 41, 'step_size': 14, 'gamma': 0.8355779920982449}. Best is trial 43 with value: 0.035386107141538614.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2568 | 0.1762
Epoch 2/300, Loss: 0.1490 | 0.1266
Epoch 3/300, Loss: 0.1652 | 0.1441
Epoch 4/300, Loss: 0.1811 | 0.2044
Epoch 5/300, Loss: 0.1732 | 0.2486
Epoch 6/300, Loss: 0.1423 | 0.1635
Epoch 7/300, Loss: 0.1239 | 0.1076
Epoch 8/300, Loss: 0.1209 | 0.0829
Epoch 9/300, Loss: 0.1283 | 0.1015
Epoch 10/300, Loss: 0.1141 | 0.0717
Epoch 11/300, Loss: 0.1019 | 0.0678
Epoch 12/300, Loss: 0.1041 | 0.0742
Epoch 13/300, Loss: 0.1044 | 0.0715
Epoch 14/300, Loss: 0.1002 | 0.0652
Epoch 15/300, Loss: 0.0973 | 0.0637
Epoch 16/300, Loss: 0.0954 | 0.0636
Epoch 17/300, Loss: 0.0927 | 0.0587
Epoch 18/300, Loss: 0.0892 | 0.0559
Epoch 19/300, Loss: 0.0872 | 0.0544
Epoch 20/300, Loss: 0.0863 | 0.0557
Epoch 21/300, Loss: 0.0874 | 0.0637
Epoch 22/300, Loss: 0.0903 | 0.0765
Epoch 23/300, Loss: 0.0910 | 0.0789
Epoch 24/300, Loss: 0.0901 | 0.0627
Epoch 25/300, Loss: 0.0886 | 0.0489
Epoch 26/300, Loss: 0.0846 | 0.0529
Epoch 27/300, Loss: 0.0856 | 0.0594
Epoch 28/300, Loss: 0.0886 | 0.0577
Epoch 29/300, Loss: 0.0879 | 0.0495
Epoch 30/300, Loss: 0.0848 | 0.0486
Epoch 31/300, Loss: 0.0840 | 0.0587
Epoch 32/300, Loss: 0.0830 | 0.0476
Epoch 33/300, Loss: 0.0796 | 0.0449
Epoch 34/300, Loss: 0.0778 | 0.0471
Epoch 35/300, Loss: 0.0766 | 0.0467
Epoch 36/300, Loss: 0.0774 | 0.0463
Epoch 37/300, Loss: 0.0795 | 0.0457
Epoch 38/300, Loss: 0.0810 | 0.0457
Epoch 39/300, Loss: 0.0808 | 0.0494
Epoch 40/300, Loss: 0.0796 | 0.0502
Epoch 41/300, Loss: 0.0768 | 0.0466
Epoch 42/300, Loss: 0.0765 | 0.0431
Epoch 43/300, Loss: 0.0771 | 0.0426
Epoch 44/300, Loss: 0.0780 | 0.0425
Epoch 45/300, Loss: 0.0780 | 0.0423
Epoch 46/300, Loss: 0.0772 | 0.0420
Epoch 47/300, Loss: 0.0754 | 0.0417
Epoch 48/300, Loss: 0.0742 | 0.0420
Epoch 49/300, Loss: 0.0748 | 0.0425
Epoch 50/300, Loss: 0.0747 | 0.0424
Epoch 51/300, Loss: 0.0735 | 0.0421
Epoch 52/300, Loss: 0.0725 | 0.0413
Epoch 53/300, Loss: 0.0720 | 0.0402
Epoch 54/300, Loss: 0.0717 | 0.0394
Epoch 55/300, Loss: 0.0715 | 0.0390
Epoch 56/300, Loss: 0.0714 | 0.0388
Epoch 57/300, Loss: 0.0714 | 0.0387
Epoch 58/300, Loss: 0.0712 | 0.0386
Epoch 59/300, Loss: 0.0711 | 0.0384
Epoch 60/300, Loss: 0.0709 | 0.0383
Epoch 61/300, Loss: 0.0706 | 0.0383
Epoch 62/300, Loss: 0.0703 | 0.0384
Epoch 63/300, Loss: 0.0698 | 0.0386
Epoch 64/300, Loss: 0.0695 | 0.0386
Epoch 65/300, Loss: 0.0694 | 0.0384
Epoch 66/300, Loss: 0.0692 | 0.0380
Epoch 67/300, Loss: 0.0690 | 0.0377
Epoch 68/300, Loss: 0.0689 | 0.0373
Epoch 69/300, Loss: 0.0688 | 0.0373
Epoch 70/300, Loss: 0.0689 | 0.0371
Epoch 71/300, Loss: 0.0690 | 0.0370
Epoch 72/300, Loss: 0.0694 | 0.0371
Epoch 73/300, Loss: 0.0699 | 0.0376
Epoch 74/300, Loss: 0.0706 | 0.0381
Epoch 75/300, Loss: 0.0708 | 0.0367
Epoch 76/300, Loss: 0.0697 | 0.0355
Epoch 77/300, Loss: 0.0686 | 0.0367
Epoch 78/300, Loss: 0.0682 | 0.0366
Epoch 79/300, Loss: 0.0680 | 0.0358
Epoch 80/300, Loss: 0.0676 | 0.0356
Epoch 81/300, Loss: 0.0673 | 0.0356
Epoch 82/300, Loss: 0.0671 | 0.0354
Epoch 83/300, Loss: 0.0671 | 0.0353
Epoch 84/300, Loss: 0.0671 | 0.0354
Epoch 85/300, Loss: 0.0671 | 0.0356
Epoch 86/300, Loss: 0.0667 | 0.0354
Epoch 87/300, Loss: 0.0663 | 0.0353
Epoch 88/300, Loss: 0.0661 | 0.0352
Epoch 89/300, Loss: 0.0659 | 0.0351
Epoch 90/300, Loss: 0.0658 | 0.0350
Epoch 91/300, Loss: 0.0657 | 0.0351
Epoch 92/300, Loss: 0.0656 | 0.0350
Epoch 93/300, Loss: 0.0656 | 0.0348
Epoch 94/300, Loss: 0.0656 | 0.0346
Epoch 95/300, Loss: 0.0655 | 0.0345
Epoch 96/300, Loss: 0.0655 | 0.0344
Epoch 97/300, Loss: 0.0654 | 0.0343
Epoch 98/300, Loss: 0.0653 | 0.0341
Epoch 99/300, Loss: 0.0652 | 0.0340
Epoch 100/300, Loss: 0.0652 | 0.0340
Epoch 101/300, Loss: 0.0649 | 0.0339
Epoch 102/300, Loss: 0.0648 | 0.0339
Epoch 103/300, Loss: 0.0646 | 0.0338
Epoch 104/300, Loss: 0.0644 | 0.0337
Epoch 105/300, Loss: 0.0643 | 0.0336
Epoch 106/300, Loss: 0.0642 | 0.0339
Epoch 107/300, Loss: 0.0641 | 0.0339
Epoch 108/300, Loss: 0.0641 | 0.0338
Epoch 109/300, Loss: 0.0640 | 0.0337
Epoch 110/300, Loss: 0.0639 | 0.0336
Epoch 111/300, Loss: 0.0638 | 0.0335
Epoch 112/300, Loss: 0.0638 | 0.0334
Epoch 113/300, Loss: 0.0637 | 0.0333
Epoch 114/300, Loss: 0.0636 | 0.0337
Epoch 115/300, Loss: 0.0636 | 0.0336
Epoch 116/300, Loss: 0.0635 | 0.0335
Epoch 117/300, Loss: 0.0634 | 0.0333
Epoch 118/300, Loss: 0.0634 | 0.0332
Epoch 119/300, Loss: 0.0633 | 0.0331
Epoch 120/300, Loss: 0.0632 | 0.0330
Epoch 121/300, Loss: 0.0631 | 0.0332
Epoch 122/300, Loss: 0.0631 | 0.0332
Epoch 123/300, Loss: 0.0630 | 0.0331
Epoch 124/300, Loss: 0.0630 | 0.0331
Epoch 125/300, Loss: 0.0630 | 0.0330
Epoch 126/300, Loss: 0.0630 | 0.0330
Epoch 127/300, Loss: 0.0630 | 0.0330
Epoch 128/300, Loss: 0.0630 | 0.0329
Epoch 129/300, Loss: 0.0630 | 0.0325
Epoch 130/300, Loss: 0.0630 | 0.0325
Epoch 131/300, Loss: 0.0630 | 0.0325
Epoch 132/300, Loss: 0.0630 | 0.0326
Epoch 133/300, Loss: 0.0629 | 0.0327
Epoch 134/300, Loss: 0.0629 | 0.0328
Epoch 135/300, Loss: 0.0628 | 0.0328
Epoch 136/300, Loss: 0.0629 | 0.0331
Epoch 137/300, Loss: 0.0628 | 0.0329
Epoch 138/300, Loss: 0.0630 | 0.0328
Epoch 139/300, Loss: 0.0631 | 0.0326
Epoch 140/300, Loss: 0.0633 | 0.0324
Epoch 141/300, Loss: 0.0637 | 0.0322
Epoch 142/300, Loss: 0.0642 | 0.0325
Epoch 143/300, Loss: 0.0648 | 0.0325
Epoch 144/300, Loss: 0.0647 | 0.0331
Epoch 145/300, Loss: 0.0643 | 0.0335
Epoch 146/300, Loss: 0.0647 | 0.0338
Epoch 147/300, Loss: 0.0643 | 0.0336
Epoch 148/300, Loss: 0.0637 | 0.0332
Epoch 149/300, Loss: 0.0631 | 0.0328
Epoch 150/300, Loss: 0.0626 | 0.0325
Epoch 151/300, Loss: 0.0624 | 0.0323
Epoch 152/300, Loss: 0.0623 | 0.0322
Epoch 153/300, Loss: 0.0622 | 0.0320
Epoch 154/300, Loss: 0.0621 | 0.0320
Epoch 155/300, Loss: 0.0621 | 0.0319
Epoch 156/300, Loss: 0.0621 | 0.0319
Epoch 157/300, Loss: 0.0621 | 0.0319
Epoch 158/300, Loss: 0.0621 | 0.0318
Epoch 159/300, Loss: 0.0621 | 0.0318
Epoch 160/300, Loss: 0.0621 | 0.0317
Epoch 161/300, Loss: 0.0620 | 0.0317
Epoch 162/300, Loss: 0.0620 | 0.0317
Epoch 163/300, Loss: 0.0620 | 0.0317
Epoch 164/300, Loss: 0.0619 | 0.0318
Epoch 165/300, Loss: 0.0619 | 0.0318
Epoch 166/300, Loss: 0.0619 | 0.0319
Epoch 167/300, Loss: 0.0619 | 0.0319
Epoch 168/300, Loss: 0.0618 | 0.0319
Epoch 169/300, Loss: 0.0618 | 0.0319
Epoch 170/300, Loss: 0.0618 | 0.0319
Epoch 171/300, Loss: 0.0618 | 0.0318
Epoch 172/300, Loss: 0.0617 | 0.0318
Epoch 173/300, Loss: 0.0617 | 0.0318
Epoch 174/300, Loss: 0.0617 | 0.0318
Epoch 175/300, Loss: 0.0617 | 0.0317
Epoch 176/300, Loss: 0.0616 | 0.0317
Epoch 177/300, Loss: 0.0616 | 0.0317
Epoch 178/300, Loss: 0.0616 | 0.0317
Epoch 179/300, Loss: 0.0616 | 0.0316
Epoch 180/300, Loss: 0.0616 | 0.0316
Epoch 181/300, Loss: 0.0616 | 0.0316
Epoch 182/300, Loss: 0.0616 | 0.0316
Epoch 183/300, Loss: 0.0615 | 0.0317
Epoch 184/300, Loss: 0.0615 | 0.0317
Epoch 185/300, Loss: 0.0615 | 0.0317
Epoch 186/300, Loss: 0.0615 | 0.0317
Epoch 187/300, Loss: 0.0615 | 0.0317
Epoch 188/300, Loss: 0.0615 | 0.0316
Epoch 189/300, Loss: 0.0614 | 0.0317
Epoch 190/300, Loss: 0.0614 | 0.0317
Epoch 191/300, Loss: 0.0614 | 0.0316
Epoch 192/300, Loss: 0.0614 | 0.0316
Epoch 193/300, Loss: 0.0614 | 0.0316
Epoch 194/300, Loss: 0.0614 | 0.0316
Epoch 195/300, Loss: 0.0614 | 0.0316
Epoch 196/300, Loss: 0.0614 | 0.0316
Epoch 197/300, Loss: 0.0614 | 0.0316
Epoch 198/300, Loss: 0.0613 | 0.0316
Epoch 199/300, Loss: 0.0613 | 0.0316
Epoch 200/300, Loss: 0.0613 | 0.0316
Epoch 201/300, Loss: 0.0613 | 0.0316
Epoch 202/300, Loss: 0.0613 | 0.0316
Epoch 203/300, Loss: 0.0613 | 0.0316
Epoch 204/300, Loss: 0.0613 | 0.0316
Epoch 205/300, Loss: 0.0613 | 0.0316
Epoch 206/300, Loss: 0.0613 | 0.0316
Epoch 207/300, Loss: 0.0613 | 0.0316
Epoch 208/300, Loss: 0.0612 | 0.0316
Epoch 209/300, Loss: 0.0612 | 0.0316
Epoch 210/300, Loss: 0.0612 | 0.0315
Epoch 211/300, Loss: 0.0612 | 0.0316
Epoch 212/300, Loss: 0.0612 | 0.0316
Epoch 213/300, Loss: 0.0612 | 0.0315
Epoch 214/300, Loss: 0.0612 | 0.0315
Epoch 215/300, Loss: 0.0612 | 0.0315
Epoch 216/300, Loss: 0.0612 | 0.0315
Epoch 217/300, Loss: 0.0612 | 0.0315
Epoch 218/300, Loss: 0.0612 | 0.0315
Epoch 219/300, Loss: 0.0612 | 0.0315
Epoch 220/300, Loss: 0.0612 | 0.0315
Epoch 221/300, Loss: 0.0612 | 0.0315
Epoch 222/300, Loss: 0.0611 | 0.0315
Epoch 223/300, Loss: 0.0611 | 0.0315
Epoch 224/300, Loss: 0.0611 | 0.0315
Epoch 225/300, Loss: 0.0611 | 0.0315
Epoch 226/300, Loss: 0.0611 | 0.0315
Epoch 227/300, Loss: 0.0611 | 0.0315
Epoch 228/300, Loss: 0.0611 | 0.0315
Epoch 229/300, Loss: 0.0611 | 0.0315
Epoch 230/300, Loss: 0.0611 | 0.0315
Epoch 231/300, Loss: 0.0611 | 0.0315
Epoch 232/300, Loss: 0.0611 | 0.0315
Epoch 233/300, Loss: 0.0611 | 0.0315
Epoch 234/300, Loss: 0.0611 | 0.0315
Epoch 235/300, Loss: 0.0611 | 0.0315
Epoch 236/300, Loss: 0.0611 | 0.0315
Epoch 237/300, Loss: 0.0611 | 0.0315
Epoch 238/300, Loss: 0.0611 | 0.0315
Epoch 239/300, Loss: 0.0611 | 0.0315
Epoch 240/300, Loss: 0.0610 | 0.0315
Epoch 241/300, Loss: 0.0610 | 0.0315
Epoch 242/300, Loss: 0.0610 | 0.0315
Epoch 243/300, Loss: 0.0610 | 0.0315
Epoch 244/300, Loss: 0.0610 | 0.0315
Epoch 245/300, Loss: 0.0610 | 0.0315
Epoch 246/300, Loss: 0.0610 | 0.0315
Epoch 247/300, Loss: 0.0610 | 0.0315
Epoch 248/300, Loss: 0.0610 | 0.0315
Epoch 249/300, Loss: 0.0610 | 0.0315
Epoch 250/300, Loss: 0.0610 | 0.0315
Epoch 251/300, Loss: 0.0610 | 0.0315
Epoch 252/300, Loss: 0.0610 | 0.0315
Epoch 253/300, Loss: 0.0610 | 0.0315
Epoch 254/300, Loss: 0.0610 | 0.0315
Epoch 255/300, Loss: 0.0610 | 0.0315
Epoch 256/300, Loss: 0.0610 | 0.0315
Epoch 257/300, Loss: 0.0610 | 0.0315
Epoch 258/300, Loss: 0.0610 | 0.0315
Epoch 259/300, Loss: 0.0610 | 0.0315
Epoch 260/300, Loss: 0.0610 | 0.0315
Epoch 261/300, Loss: 0.0610 | 0.0315
Epoch 262/300, Loss: 0.0610 | 0.0315
Epoch 263/300, Loss: 0.0610 | 0.0314
Epoch 264/300, Loss: 0.0610 | 0.0315
Epoch 265/300, Loss: 0.0610 | 0.0314
Epoch 266/300, Loss: 0.0610 | 0.0314
Epoch 267/300, Loss: 0.0610 | 0.0314
Epoch 268/300, Loss: 0.0610 | 0.0314
Epoch 269/300, Loss: 0.0610 | 0.0314
Epoch 270/300, Loss: 0.0610 | 0.0314
Epoch 271/300, Loss: 0.0610 | 0.0314
Epoch 272/300, Loss: 0.0609 | 0.0314
Epoch 273/300, Loss: 0.0609 | 0.0314
Epoch 274/300, Loss: 0.0609 | 0.0314
Epoch 275/300, Loss: 0.0609 | 0.0314
Epoch 276/300, Loss: 0.0609 | 0.0314
Epoch 277/300, Loss: 0.0609 | 0.0314
Epoch 278/300, Loss: 0.0609 | 0.0314
Epoch 279/300, Loss: 0.0609 | 0.0314
Epoch 280/300, Loss: 0.0609 | 0.0314
Epoch 281/300, Loss: 0.0609 | 0.0314
Epoch 282/300, Loss: 0.0609 | 0.0314
Epoch 283/300, Loss: 0.0609 | 0.0314
Epoch 284/300, Loss: 0.0609 | 0.0314
Epoch 285/300, Loss: 0.0609 | 0.0314
Epoch 286/300, Loss: 0.0609 | 0.0314
Epoch 287/300, Loss: 0.0609 | 0.0314
Epoch 288/300, Loss: 0.0609 | 0.0314
Epoch 289/300, Loss: 0.0609 | 0.0314
Epoch 290/300, Loss: 0.0609 | 0.0314
Epoch 291/300, Loss: 0.0609 | 0.0314
Epoch 292/300, Loss: 0.0609 | 0.0314
Epoch 293/300, Loss: 0.0609 | 0.0314
Epoch 294/300, Loss: 0.0609 | 0.0314
Epoch 295/300, Loss: 0.0609 | 0.0314
Epoch 296/300, Loss: 0.0609 | 0.0314
Epoch 297/300, Loss: 0.0609 | 0.0314
Epoch 298/300, Loss: 0.0609 | 0.0314
Epoch 299/300, Loss: 0.0609 | 0.0314
Epoch 300/300, Loss: 0.0609 | 0.0314
Runtime (seconds): 189.68695259094238
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 145.1327119397465
RMSE: 12.047103881835938
MAE: 12.047103881835938
R-squared: nan
[199.1871]
