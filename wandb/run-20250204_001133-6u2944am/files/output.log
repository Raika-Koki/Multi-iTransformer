ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-04 00:11:40,938][0m A new study created in memory with name: no-name-a5877b0a-4725-4ff9-94dd-1829cd8a9428[0m
[32m[I 2025-02-04 00:12:09,149][0m Trial 0 finished with value: 1.2090051174163818 and parameters: {'observation_period_num': 241, 'train_rates': 0.9735227387531298, 'learning_rate': 1.4054523035481066e-06, 'batch_size': 222, 'step_size': 4, 'gamma': 0.8424557004100288}. Best is trial 0 with value: 1.2090051174163818.[0m
[32m[I 2025-02-04 00:12:41,662][0m Trial 1 finished with value: 0.2522900494454001 and parameters: {'observation_period_num': 99, 'train_rates': 0.6533201457237581, 'learning_rate': 9.582435084025888e-06, 'batch_size': 149, 'step_size': 9, 'gamma': 0.9520278220508538}. Best is trial 1 with value: 0.2522900494454001.[0m
[32m[I 2025-02-04 00:13:05,580][0m Trial 2 finished with value: 0.15227209903143732 and parameters: {'observation_period_num': 221, 'train_rates': 0.6678345490825535, 'learning_rate': 0.0008018365046434825, 'batch_size': 221, 'step_size': 3, 'gamma': 0.8558869142859531}. Best is trial 2 with value: 0.15227209903143732.[0m
[32m[I 2025-02-04 00:14:50,154][0m Trial 3 finished with value: 0.37032175201218664 and parameters: {'observation_period_num': 197, 'train_rates': 0.6311525466775099, 'learning_rate': 1.3012531654658927e-05, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9174706975537021}. Best is trial 2 with value: 0.15227209903143732.[0m
[32m[I 2025-02-04 00:15:11,945][0m Trial 4 finished with value: 0.2404877787981278 and parameters: {'observation_period_num': 218, 'train_rates': 0.8290147362294279, 'learning_rate': 2.8111045962771338e-05, 'batch_size': 255, 'step_size': 11, 'gamma': 0.9249195686581912}. Best is trial 2 with value: 0.15227209903143732.[0m
[32m[I 2025-02-04 00:15:34,326][0m Trial 5 finished with value: 0.16018837553666052 and parameters: {'observation_period_num': 249, 'train_rates': 0.7541933195049353, 'learning_rate': 0.00032457905531541324, 'batch_size': 242, 'step_size': 10, 'gamma': 0.8070934667606897}. Best is trial 2 with value: 0.15227209903143732.[0m
[32m[I 2025-02-04 00:18:11,902][0m Trial 6 finished with value: 0.1620606207267731 and parameters: {'observation_period_num': 59, 'train_rates': 0.923771368357722, 'learning_rate': 4.34950152234597e-06, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8646432193053375}. Best is trial 2 with value: 0.15227209903143732.[0m
[32m[I 2025-02-04 00:18:50,837][0m Trial 7 finished with value: 0.09736035019159317 and parameters: {'observation_period_num': 10, 'train_rates': 0.9559341464943747, 'learning_rate': 1.668509968984483e-05, 'batch_size': 168, 'step_size': 9, 'gamma': 0.8535684630995423}. Best is trial 7 with value: 0.09736035019159317.[0m
[32m[I 2025-02-04 00:20:08,504][0m Trial 8 finished with value: 0.036505300393129915 and parameters: {'observation_period_num': 20, 'train_rates': 0.8558367496071695, 'learning_rate': 0.0005696124223336788, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8129116048113032}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:21:02,953][0m Trial 9 finished with value: 0.40983164476841805 and parameters: {'observation_period_num': 203, 'train_rates': 0.6318406114353502, 'learning_rate': 9.262680160157273e-06, 'batch_size': 81, 'step_size': 5, 'gamma': 0.8697697252355612}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:22:02,492][0m Trial 10 finished with value: 0.08905308490785041 and parameters: {'observation_period_num': 143, 'train_rates': 0.8562094112578764, 'learning_rate': 0.00011617118330405893, 'batch_size': 95, 'step_size': 14, 'gamma': 0.7516791252478151}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:23:01,563][0m Trial 11 finished with value: 0.08339370945468545 and parameters: {'observation_period_num': 147, 'train_rates': 0.8576384505272506, 'learning_rate': 0.0001272212383674109, 'batch_size': 93, 'step_size': 15, 'gamma': 0.7502940820876287}. Best is trial 8 with value: 0.036505300393129915.[0m
Early stopping at epoch 52
[32m[I 2025-02-04 00:23:35,174][0m Trial 12 finished with value: 0.31219171090204206 and parameters: {'observation_period_num': 151, 'train_rates': 0.8912867158735901, 'learning_rate': 0.00010490820146129115, 'batch_size': 91, 'step_size': 1, 'gamma': 0.7767135987603138}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:24:18,473][0m Trial 13 finished with value: 0.11351879303646237 and parameters: {'observation_period_num': 92, 'train_rates': 0.7773408842164111, 'learning_rate': 9.066206724527905e-05, 'batch_size': 124, 'step_size': 6, 'gamma': 0.8038769964649379}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:29:18,220][0m Trial 14 finished with value: 0.06630350256258724 and parameters: {'observation_period_num': 40, 'train_rates': 0.7202243599308329, 'learning_rate': 0.0006875813355433964, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7526542589540917}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:34:17,622][0m Trial 15 finished with value: 0.047762420006637274 and parameters: {'observation_period_num': 6, 'train_rates': 0.7100515140244661, 'learning_rate': 0.0009871140809279441, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8089154705617114}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:35:50,141][0m Trial 16 finished with value: 0.03983773906072502 and parameters: {'observation_period_num': 5, 'train_rates': 0.715239736563672, 'learning_rate': 0.0003562439289889952, 'batch_size': 54, 'step_size': 7, 'gamma': 0.8026342272460648}. Best is trial 8 with value: 0.036505300393129915.[0m
Early stopping at epoch 63
[32m[I 2025-02-04 00:36:45,627][0m Trial 17 finished with value: 0.11420418661070096 and parameters: {'observation_period_num': 45, 'train_rates': 0.7823897273270208, 'learning_rate': 0.0002738863936614, 'batch_size': 61, 'step_size': 1, 'gamma': 0.7910353661985574}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:37:32,214][0m Trial 18 finished with value: 0.0779540237594158 and parameters: {'observation_period_num': 82, 'train_rates': 0.8200112987311217, 'learning_rate': 0.00029268700766511596, 'batch_size': 121, 'step_size': 3, 'gamma': 0.8289483922906894}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:38:53,126][0m Trial 19 finished with value: 0.09177616060427997 and parameters: {'observation_period_num': 25, 'train_rates': 0.7171389446420785, 'learning_rate': 5.0565198877437164e-05, 'batch_size': 63, 'step_size': 6, 'gamma': 0.8954061392134824}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:39:29,146][0m Trial 20 finished with value: 0.27147994657851154 and parameters: {'observation_period_num': 67, 'train_rates': 0.8886451806009297, 'learning_rate': 0.0003968242305052564, 'batch_size': 177, 'step_size': 13, 'gamma': 0.9866519925729286}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:42:26,124][0m Trial 21 finished with value: 0.052420720190421474 and parameters: {'observation_period_num': 7, 'train_rates': 0.7021472380679729, 'learning_rate': 0.0009606391547550122, 'batch_size': 27, 'step_size': 7, 'gamma': 0.8289785664481009}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:43:48,319][0m Trial 22 finished with value: 0.09399108418512865 and parameters: {'observation_period_num': 27, 'train_rates': 0.68739191312412, 'learning_rate': 0.0005448234968072276, 'batch_size': 58, 'step_size': 7, 'gamma': 0.817344019116381}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:45:21,734][0m Trial 23 finished with value: 0.06721834221319672 and parameters: {'observation_period_num': 7, 'train_rates': 0.6017417062544169, 'learning_rate': 0.0001907764990056507, 'batch_size': 47, 'step_size': 5, 'gamma': 0.7803196026694922}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:46:28,798][0m Trial 24 finished with value: 0.1548819246961445 and parameters: {'observation_period_num': 111, 'train_rates': 0.7425360171087932, 'learning_rate': 0.0004304042913063115, 'batch_size': 73, 'step_size': 8, 'gamma': 0.7918719355879903}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:51:02,977][0m Trial 25 finished with value: 0.05071896674304173 and parameters: {'observation_period_num': 35, 'train_rates': 0.8110677328449398, 'learning_rate': 5.4982932288719324e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8324034564177979}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:51:50,250][0m Trial 26 finished with value: 0.09298299077679129 and parameters: {'observation_period_num': 64, 'train_rates': 0.747017413689877, 'learning_rate': 0.0009700674838565334, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8817100855468621}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:53:38,014][0m Trial 27 finished with value: 0.057568052656248085 and parameters: {'observation_period_num': 21, 'train_rates': 0.6807761546795066, 'learning_rate': 0.00019517310435230337, 'batch_size': 44, 'step_size': 7, 'gamma': 0.769827473855981}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:56:09,301][0m Trial 28 finished with value: 0.09276168047813926 and parameters: {'observation_period_num': 49, 'train_rates': 0.7740848666622105, 'learning_rate': 0.0005558048877371258, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8139815465477143}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:57:39,224][0m Trial 29 finished with value: 0.07850848138332367 and parameters: {'observation_period_num': 78, 'train_rates': 0.9838505315503758, 'learning_rate': 0.0002411575343990671, 'batch_size': 67, 'step_size': 4, 'gamma': 0.8439468262772729}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 00:58:36,457][0m Trial 30 finished with value: 0.863351771997851 and parameters: {'observation_period_num': 170, 'train_rates': 0.7218440230210734, 'learning_rate': 1.1157318211973738e-06, 'batch_size': 81, 'step_size': 12, 'gamma': 0.7981216098140664}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 01:03:55,256][0m Trial 31 finished with value: 0.05082410101777133 and parameters: {'observation_period_num': 37, 'train_rates': 0.8089957303072606, 'learning_rate': 5.21147585803031e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8313739665523311}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 01:09:27,829][0m Trial 32 finished with value: 0.09373643271642249 and parameters: {'observation_period_num': 23, 'train_rates': 0.8522167913017664, 'learning_rate': 4.157989825164701e-06, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8387163994029846}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 01:11:17,301][0m Trial 33 finished with value: 0.044928770584213004 and parameters: {'observation_period_num': 5, 'train_rates': 0.894434866655283, 'learning_rate': 6.085651475214828e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8213515502449859}. Best is trial 8 with value: 0.036505300393129915.[0m
[32m[I 2025-02-04 01:13:01,057][0m Trial 34 finished with value: 0.027581290652354557 and parameters: {'observation_period_num': 6, 'train_rates': 0.9287378374277016, 'learning_rate': 0.0005582732056514692, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8203869803270176}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:14:51,114][0m Trial 35 finished with value: 0.07633965920943481 and parameters: {'observation_period_num': 113, 'train_rates': 0.9407217465337169, 'learning_rate': 0.00014704589129158757, 'batch_size': 52, 'step_size': 8, 'gamma': 0.8488796605232539}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:15:45,960][0m Trial 36 finished with value: 0.04896974651015192 and parameters: {'observation_period_num': 54, 'train_rates': 0.9044978629409163, 'learning_rate': 0.0005701912661273691, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8197679703552205}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:16:26,075][0m Trial 37 finished with value: 0.13188559838517785 and parameters: {'observation_period_num': 19, 'train_rates': 0.9220038478765271, 'learning_rate': 2.0921412129542897e-05, 'batch_size': 154, 'step_size': 10, 'gamma': 0.7709703343429548}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:17:41,964][0m Trial 38 finished with value: 0.04975997883081436 and parameters: {'observation_period_num': 31, 'train_rates': 0.957454185194151, 'learning_rate': 7.629393372679315e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8938642947390325}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:18:11,148][0m Trial 39 finished with value: 0.054461244082088626 and parameters: {'observation_period_num': 18, 'train_rates': 0.8765789332231609, 'learning_rate': 0.00040336586858276944, 'batch_size': 202, 'step_size': 3, 'gamma': 0.8601311570515981}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:18:51,544][0m Trial 40 finished with value: 0.17637755713473496 and parameters: {'observation_period_num': 70, 'train_rates': 0.8450077926175954, 'learning_rate': 3.255123673075596e-05, 'batch_size': 139, 'step_size': 8, 'gamma': 0.7855298983741714}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:21:32,078][0m Trial 41 finished with value: 0.034546603415427465 and parameters: {'observation_period_num': 8, 'train_rates': 0.9239772460363938, 'learning_rate': 0.0007247062002611308, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8071240153302262}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:24:13,867][0m Trial 42 finished with value: 0.03737616471268914 and parameters: {'observation_period_num': 8, 'train_rates': 0.9178466762922645, 'learning_rate': 0.0007054285812860885, 'batch_size': 36, 'step_size': 8, 'gamma': 0.800790248684128}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:26:52,867][0m Trial 43 finished with value: 0.036310920255933785 and parameters: {'observation_period_num': 16, 'train_rates': 0.9221862214683952, 'learning_rate': 0.0007105634120346709, 'batch_size': 36, 'step_size': 11, 'gamma': 0.7998298581289752}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:29:52,045][0m Trial 44 finished with value: 0.08601189653078715 and parameters: {'observation_period_num': 49, 'train_rates': 0.9298717520225844, 'learning_rate': 0.0007250316450089921, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7965362361667722}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:32:09,303][0m Trial 45 finished with value: 0.11297199193467485 and parameters: {'observation_period_num': 232, 'train_rates': 0.9655620513755507, 'learning_rate': 0.000663526972221353, 'batch_size': 40, 'step_size': 10, 'gamma': 0.766547127501265}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:35:11,150][0m Trial 46 finished with value: 0.04828525354283127 and parameters: {'observation_period_num': 34, 'train_rates': 0.9157667658002456, 'learning_rate': 0.0004761410460355732, 'batch_size': 31, 'step_size': 9, 'gamma': 0.8090423620785128}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:37:27,914][0m Trial 47 finished with value: 0.03530640193394252 and parameters: {'observation_period_num': 18, 'train_rates': 0.9403363754844912, 'learning_rate': 0.00017923050989026668, 'batch_size': 43, 'step_size': 13, 'gamma': 0.7836804954917045}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:38:33,707][0m Trial 48 finished with value: 0.13199387245798763 and parameters: {'observation_period_num': 174, 'train_rates': 0.9477752307305326, 'learning_rate': 0.00017876294548831701, 'batch_size': 85, 'step_size': 13, 'gamma': 0.7827330230254923}. Best is trial 34 with value: 0.027581290652354557.[0m
[32m[I 2025-02-04 01:39:57,908][0m Trial 49 finished with value: 0.0348858322711691 and parameters: {'observation_period_num': 15, 'train_rates': 0.8714916387581164, 'learning_rate': 0.000249690106994684, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7636021033562732}. Best is trial 34 with value: 0.027581290652354557.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-04 01:39:57,919][0m A new study created in memory with name: no-name-7dc0eff3-4f26-4844-a5d4-95ce95311b21[0m
[32m[I 2025-02-04 01:40:27,121][0m Trial 0 finished with value: 0.15851926748977282 and parameters: {'observation_period_num': 171, 'train_rates': 0.6567588339069227, 'learning_rate': 0.00035350298470487635, 'batch_size': 165, 'step_size': 7, 'gamma': 0.9457451773996586}. Best is trial 0 with value: 0.15851926748977282.[0m
[32m[I 2025-02-04 01:40:50,597][0m Trial 1 finished with value: 0.6011016412382633 and parameters: {'observation_period_num': 169, 'train_rates': 0.8375271172928436, 'learning_rate': 4.576321229250354e-06, 'batch_size': 235, 'step_size': 15, 'gamma': 0.884590615885885}. Best is trial 0 with value: 0.15851926748977282.[0m
[32m[I 2025-02-04 01:43:11,486][0m Trial 2 finished with value: 0.36903085631708943 and parameters: {'observation_period_num': 140, 'train_rates': 0.8235514077942476, 'learning_rate': 5.703215697850321e-06, 'batch_size': 36, 'step_size': 1, 'gamma': 0.9701545921866546}. Best is trial 0 with value: 0.15851926748977282.[0m
[32m[I 2025-02-04 01:43:34,297][0m Trial 3 finished with value: 0.14335038954122062 and parameters: {'observation_period_num': 144, 'train_rates': 0.7100871759855627, 'learning_rate': 0.00010057245812899458, 'batch_size': 240, 'step_size': 14, 'gamma': 0.7870109106302962}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:44:52,209][0m Trial 4 finished with value: 0.1780600885723538 and parameters: {'observation_period_num': 141, 'train_rates': 0.7657437920195993, 'learning_rate': 4.268520713915062e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.759921012591159}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:45:22,985][0m Trial 5 finished with value: 0.15835975526886828 and parameters: {'observation_period_num': 134, 'train_rates': 0.7226907779303933, 'learning_rate': 4.3976353206018605e-05, 'batch_size': 161, 'step_size': 12, 'gamma': 0.8825346442753526}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:45:47,792][0m Trial 6 finished with value: 0.35554573633777564 and parameters: {'observation_period_num': 130, 'train_rates': 0.7627432247334297, 'learning_rate': 1.592834273680775e-05, 'batch_size': 238, 'step_size': 13, 'gamma': 0.8704779700824454}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:46:12,657][0m Trial 7 finished with value: 0.7583248940629697 and parameters: {'observation_period_num': 126, 'train_rates': 0.6142740756897106, 'learning_rate': 2.671809141202824e-06, 'batch_size': 190, 'step_size': 10, 'gamma': 0.9667979836065623}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:49:00,483][0m Trial 8 finished with value: 0.4842393565468672 and parameters: {'observation_period_num': 150, 'train_rates': 0.6343731831003742, 'learning_rate': 4.852406035769876e-05, 'batch_size': 25, 'step_size': 12, 'gamma': 0.9247193709830208}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:49:37,019][0m Trial 9 finished with value: 0.43903961485956083 and parameters: {'observation_period_num': 132, 'train_rates': 0.8151956324762742, 'learning_rate': 4.3245931264579525e-06, 'batch_size': 144, 'step_size': 7, 'gamma': 0.944986851406451}. Best is trial 3 with value: 0.14335038954122062.[0m
[32m[I 2025-02-04 01:50:33,096][0m Trial 10 finished with value: 0.05378696322441101 and parameters: {'observation_period_num': 53, 'train_rates': 0.965475365048056, 'learning_rate': 0.0008857903310991665, 'batch_size': 110, 'step_size': 3, 'gamma': 0.7847280330116149}. Best is trial 10 with value: 0.05378696322441101.[0m
Early stopping at epoch 98
[32m[I 2025-02-04 01:51:40,640][0m Trial 11 finished with value: 0.07014785706996918 and parameters: {'observation_period_num': 52, 'train_rates': 0.989262640989168, 'learning_rate': 0.0009744637957385961, 'batch_size': 93, 'step_size': 2, 'gamma': 0.7755240178491515}. Best is trial 10 with value: 0.05378696322441101.[0m
[32m[I 2025-02-04 01:52:43,865][0m Trial 12 finished with value: 0.051477476954460144 and parameters: {'observation_period_num': 42, 'train_rates': 0.9754552041255744, 'learning_rate': 0.0009433624823896338, 'batch_size': 96, 'step_size': 2, 'gamma': 0.8182668085833044}. Best is trial 12 with value: 0.051477476954460144.[0m
[32m[I 2025-02-04 01:53:45,347][0m Trial 13 finished with value: 0.028850477188825607 and parameters: {'observation_period_num': 9, 'train_rates': 0.9787259874347463, 'learning_rate': 0.000957277784736993, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8198445687159657}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 01:54:46,104][0m Trial 14 finished with value: 0.04900008231449667 and parameters: {'observation_period_num': 7, 'train_rates': 0.8951699882320022, 'learning_rate': 0.00023773768345127676, 'batch_size': 95, 'step_size': 4, 'gamma': 0.8340238541977916}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 01:56:12,095][0m Trial 15 finished with value: 0.04212877369556294 and parameters: {'observation_period_num': 13, 'train_rates': 0.9147361257156107, 'learning_rate': 0.00021391118255900386, 'batch_size': 68, 'step_size': 4, 'gamma': 0.827042801976151}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 01:57:50,647][0m Trial 16 finished with value: 0.09559260149087224 and parameters: {'observation_period_num': 250, 'train_rates': 0.9068627491708057, 'learning_rate': 0.00025385615279823414, 'batch_size': 55, 'step_size': 5, 'gamma': 0.8265425846765725}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 01:59:15,714][0m Trial 17 finished with value: 0.050479403312007586 and parameters: {'observation_period_num': 15, 'train_rates': 0.897883167724198, 'learning_rate': 0.00010668427570886416, 'batch_size': 67, 'step_size': 5, 'gamma': 0.8429505408730735}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:00:07,709][0m Trial 18 finished with value: 0.9904308880810963 and parameters: {'observation_period_num': 91, 'train_rates': 0.9332772045150135, 'learning_rate': 1.0557533164266738e-06, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8123193000794404}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:00:50,563][0m Trial 19 finished with value: 0.056685625135305506 and parameters: {'observation_period_num': 82, 'train_rates': 0.8598838719233514, 'learning_rate': 0.0004371686931223326, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8489629082786864}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:02:04,496][0m Trial 20 finished with value: 0.05811593611724675 and parameters: {'observation_period_num': 31, 'train_rates': 0.9342943864758584, 'learning_rate': 0.0001538516811157206, 'batch_size': 79, 'step_size': 4, 'gamma': 0.802036269149628}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:04:02,411][0m Trial 21 finished with value: 0.04125337825427132 and parameters: {'observation_period_num': 5, 'train_rates': 0.8845490408883385, 'learning_rate': 0.0002131926668491655, 'batch_size': 48, 'step_size': 4, 'gamma': 0.8491576453698083}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:06:12,928][0m Trial 22 finished with value: 0.09093451629890952 and parameters: {'observation_period_num': 71, 'train_rates': 0.9403402104712556, 'learning_rate': 0.0004935843625417945, 'batch_size': 44, 'step_size': 6, 'gamma': 0.8551879771635008}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:10:16,003][0m Trial 23 finished with value: 0.10887982971527997 and parameters: {'observation_period_num': 25, 'train_rates': 0.8548505224146019, 'learning_rate': 1.5814822392193283e-05, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9059006706409595}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:11:39,990][0m Trial 24 finished with value: 0.05653333468806176 and parameters: {'observation_period_num': 8, 'train_rates': 0.8788256592452331, 'learning_rate': 8.46194798959674e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.8600384937676542}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:13:48,087][0m Trial 25 finished with value: 0.07702706815028677 and parameters: {'observation_period_num': 97, 'train_rates': 0.9486399436080779, 'learning_rate': 0.00018461277692033106, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8016910713118052}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:15:01,213][0m Trial 26 finished with value: 0.05368323407218426 and parameters: {'observation_period_num': 64, 'train_rates': 0.9180528589402275, 'learning_rate': 0.0005161100523893215, 'batch_size': 78, 'step_size': 9, 'gamma': 0.7527455542677518}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:15:50,038][0m Trial 27 finished with value: 0.1112167826423081 and parameters: {'observation_period_num': 33, 'train_rates': 0.8728093914060776, 'learning_rate': 1.813459146033082e-05, 'batch_size': 118, 'step_size': 6, 'gamma': 0.8998205852059186}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:16:58,695][0m Trial 28 finished with value: 0.10118731720881029 and parameters: {'observation_period_num': 236, 'train_rates': 0.9597224388110879, 'learning_rate': 0.00031818233551884694, 'batch_size': 84, 'step_size': 4, 'gamma': 0.8331344796340459}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:17:25,437][0m Trial 29 finished with value: 0.13316323132818317 and parameters: {'observation_period_num': 199, 'train_rates': 0.7975638535929561, 'learning_rate': 0.0005613404867168467, 'batch_size': 202, 'step_size': 8, 'gamma': 0.8712676905343522}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:18:13,655][0m Trial 30 finished with value: 0.1260197013616562 and parameters: {'observation_period_num': 108, 'train_rates': 0.9867927004515629, 'learning_rate': 0.00013891047376071963, 'batch_size': 133, 'step_size': 6, 'gamma': 0.8095159610617856}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:19:12,777][0m Trial 31 finished with value: 0.05009653650730559 and parameters: {'observation_period_num': 11, 'train_rates': 0.8950336234803081, 'learning_rate': 0.00024286984085790955, 'batch_size': 99, 'step_size': 4, 'gamma': 0.835309021972633}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:20:11,472][0m Trial 32 finished with value: 0.09194695112133815 and parameters: {'observation_period_num': 5, 'train_rates': 0.9180746308336895, 'learning_rate': 6.19881496162484e-05, 'batch_size': 103, 'step_size': 2, 'gamma': 0.8248854652406117}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:22:40,383][0m Trial 33 finished with value: 0.047659621660073266 and parameters: {'observation_period_num': 22, 'train_rates': 0.8377586913631503, 'learning_rate': 0.000313305251390205, 'batch_size': 36, 'step_size': 5, 'gamma': 0.8403039670913317}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:25:10,020][0m Trial 34 finished with value: 0.059599872955606 and parameters: {'observation_period_num': 26, 'train_rates': 0.8347153695444821, 'learning_rate': 0.0006696925438720499, 'batch_size': 36, 'step_size': 5, 'gamma': 0.7920637245214136}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:26:52,039][0m Trial 35 finished with value: 0.05171081757866214 and parameters: {'observation_period_num': 44, 'train_rates': 0.8468040286892, 'learning_rate': 0.0003670965904342335, 'batch_size': 53, 'step_size': 3, 'gamma': 0.8619842022616777}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:29:34,922][0m Trial 36 finished with value: 0.11944697255935144 and parameters: {'observation_period_num': 22, 'train_rates': 0.8141680025071345, 'learning_rate': 2.3810976312471065e-05, 'batch_size': 32, 'step_size': 1, 'gamma': 0.8889805544703703}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:30:58,759][0m Trial 37 finished with value: 0.054596944046872 and parameters: {'observation_period_num': 66, 'train_rates': 0.8739167181732151, 'learning_rate': 0.00032907370286943094, 'batch_size': 65, 'step_size': 6, 'gamma': 0.7674924006741628}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:35:03,865][0m Trial 38 finished with value: 0.06637685597801758 and parameters: {'observation_period_num': 37, 'train_rates': 0.761850618041277, 'learning_rate': 0.0006698039991874698, 'batch_size': 20, 'step_size': 8, 'gamma': 0.8485770499731705}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:36:40,128][0m Trial 39 finished with value: 0.1772783887742616 and parameters: {'observation_period_num': 185, 'train_rates': 0.6931278372529277, 'learning_rate': 7.011413018449788e-05, 'batch_size': 47, 'step_size': 7, 'gamma': 0.8753581407980634}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:37:13,757][0m Trial 40 finished with value: 0.11129029433474877 and parameters: {'observation_period_num': 53, 'train_rates': 0.7410325821561143, 'learning_rate': 0.00011076729274156175, 'batch_size': 156, 'step_size': 4, 'gamma': 0.7937362841970058}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:38:27,365][0m Trial 41 finished with value: 0.05011529728238072 and parameters: {'observation_period_num': 18, 'train_rates': 0.8856303953631071, 'learning_rate': 0.00020946982893179107, 'batch_size': 78, 'step_size': 4, 'gamma': 0.8370663871142083}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:39:35,126][0m Trial 42 finished with value: 0.04320894638481348 and parameters: {'observation_period_num': 7, 'train_rates': 0.9221441370699159, 'learning_rate': 0.00027278320403021473, 'batch_size': 87, 'step_size': 5, 'gamma': 0.8176116365557291}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:41:13,638][0m Trial 43 finished with value: 0.05293513309159037 and parameters: {'observation_period_num': 22, 'train_rates': 0.9259214947348984, 'learning_rate': 0.00013169773771361837, 'batch_size': 60, 'step_size': 5, 'gamma': 0.8147187921879793}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:44:16,734][0m Trial 44 finished with value: 0.04916444312391886 and parameters: {'observation_period_num': 41, 'train_rates': 0.9541808291230195, 'learning_rate': 0.0003406108616459668, 'batch_size': 32, 'step_size': 2, 'gamma': 0.825317584329237}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:45:23,446][0m Trial 45 finished with value: 0.03936705381170777 and parameters: {'observation_period_num': 18, 'train_rates': 0.8351703247257211, 'learning_rate': 0.0007158157007464516, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8057142234782608}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:46:35,398][0m Trial 46 finished with value: 0.0364777905245622 and parameters: {'observation_period_num': 5, 'train_rates': 0.9706581299468854, 'learning_rate': 0.0007327320274382321, 'batch_size': 85, 'step_size': 7, 'gamma': 0.7782159958009682}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:47:25,442][0m Trial 47 finished with value: 0.04210938513278961 and parameters: {'observation_period_num': 48, 'train_rates': 0.9668057248925737, 'learning_rate': 0.0006642857108127618, 'batch_size': 122, 'step_size': 11, 'gamma': 0.7794955151973375}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:48:00,442][0m Trial 48 finished with value: 0.041972141712903976 and parameters: {'observation_period_num': 51, 'train_rates': 0.9724820826586239, 'learning_rate': 0.0007198036717116159, 'batch_size': 177, 'step_size': 11, 'gamma': 0.7798635267655114}. Best is trial 13 with value: 0.028850477188825607.[0m
[32m[I 2025-02-04 02:48:33,643][0m Trial 49 finished with value: 0.0721912756562233 and parameters: {'observation_period_num': 57, 'train_rates': 0.9795452536648168, 'learning_rate': 0.0009650869369729438, 'batch_size': 190, 'step_size': 15, 'gamma': 0.7679881639574367}. Best is trial 13 with value: 0.028850477188825607.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-04 02:48:33,654][0m A new study created in memory with name: no-name-45f08bfa-f665-4d26-a97a-579752baaca5[0m
[32m[I 2025-02-04 02:48:51,827][0m Trial 0 finished with value: 0.23363067186842676 and parameters: {'observation_period_num': 208, 'train_rates': 0.6441236985251011, 'learning_rate': 3.7455680524550624e-05, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9704880143943738}. Best is trial 0 with value: 0.23363067186842676.[0m
[32m[I 2025-02-04 02:50:11,997][0m Trial 1 finished with value: 0.10183624469898117 and parameters: {'observation_period_num': 177, 'train_rates': 0.8721658491426509, 'learning_rate': 0.0005140473203850421, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8801084126452932}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:50:44,474][0m Trial 2 finished with value: 0.14342494705467873 and parameters: {'observation_period_num': 138, 'train_rates': 0.707573303584246, 'learning_rate': 0.0001003812807781763, 'batch_size': 164, 'step_size': 14, 'gamma': 0.9802373605039859}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:52:38,763][0m Trial 3 finished with value: 0.20367279493290444 and parameters: {'observation_period_num': 127, 'train_rates': 0.877930344879128, 'learning_rate': 8.128489671354353e-06, 'batch_size': 48, 'step_size': 6, 'gamma': 0.8872678748178229}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:53:05,971][0m Trial 4 finished with value: 0.7994836807520681 and parameters: {'observation_period_num': 216, 'train_rates': 0.6771360190236652, 'learning_rate': 5.9904486596891975e-06, 'batch_size': 183, 'step_size': 3, 'gamma': 0.7975198943271812}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:54:34,841][0m Trial 5 finished with value: 0.21328251192957423 and parameters: {'observation_period_num': 153, 'train_rates': 0.6330935484555527, 'learning_rate': 5.049721960251793e-05, 'batch_size': 48, 'step_size': 8, 'gamma': 0.9231182634274286}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:55:26,252][0m Trial 6 finished with value: 0.4287830786527814 and parameters: {'observation_period_num': 55, 'train_rates': 0.9489488020726575, 'learning_rate': 4.583279459013313e-06, 'batch_size': 119, 'step_size': 3, 'gamma': 0.898495551001933}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:56:12,540][0m Trial 7 finished with value: 0.6631706986162398 and parameters: {'observation_period_num': 226, 'train_rates': 0.8087162866667138, 'learning_rate': 6.6328729555786996e-06, 'batch_size': 115, 'step_size': 3, 'gamma': 0.8645587802403857}. Best is trial 1 with value: 0.10183624469898117.[0m
[32m[I 2025-02-04 02:57:54,276][0m Trial 8 finished with value: 0.09833718650043011 and parameters: {'observation_period_num': 200, 'train_rates': 0.9420863913901882, 'learning_rate': 0.0003442778957020372, 'batch_size': 55, 'step_size': 4, 'gamma': 0.7735884641561173}. Best is trial 8 with value: 0.09833718650043011.[0m
[32m[I 2025-02-04 02:58:55,129][0m Trial 9 finished with value: 0.25255280939188407 and parameters: {'observation_period_num': 116, 'train_rates': 0.7017540599395262, 'learning_rate': 8.378691427010322e-06, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9834826779402462}. Best is trial 8 with value: 0.09833718650043011.[0m
[32m[I 2025-02-04 03:04:44,025][0m Trial 10 finished with value: 0.04331248269780823 and parameters: {'observation_period_num': 7, 'train_rates': 0.9845317487004596, 'learning_rate': 0.0009946713933341302, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7510128421595417}. Best is trial 10 with value: 0.04331248269780823.[0m
[32m[I 2025-02-04 03:09:26,721][0m Trial 11 finished with value: 0.03815536662936211 and parameters: {'observation_period_num': 6, 'train_rates': 0.9745798023394578, 'learning_rate': 0.000682184143612922, 'batch_size': 21, 'step_size': 11, 'gamma': 0.7519172295636412}. Best is trial 11 with value: 0.03815536662936211.[0m
[32m[I 2025-02-04 03:14:57,664][0m Trial 12 finished with value: 0.029543196586401838 and parameters: {'observation_period_num': 6, 'train_rates': 0.9874286479799612, 'learning_rate': 0.00023807741349588215, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7514737115599698}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:19:22,740][0m Trial 13 finished with value: 0.03323241642948876 and parameters: {'observation_period_num': 9, 'train_rates': 0.8885177028958667, 'learning_rate': 0.00017198477258834435, 'batch_size': 21, 'step_size': 11, 'gamma': 0.814784933271051}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:20:21,295][0m Trial 14 finished with value: 0.052823187681761655 and parameters: {'observation_period_num': 64, 'train_rates': 0.8858328316173941, 'learning_rate': 0.00017752612196398859, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8229122080029956}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:20:55,288][0m Trial 15 finished with value: 0.061487406518747796 and parameters: {'observation_period_num': 47, 'train_rates': 0.8065813353464705, 'learning_rate': 0.00016080472543848943, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8315270866830744}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:21:22,424][0m Trial 16 finished with value: 0.1783631298556385 and parameters: {'observation_period_num': 87, 'train_rates': 0.9133508600963476, 'learning_rate': 1.7681269389372675e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.8038915094293372}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:26:08,869][0m Trial 17 finished with value: 0.04968287824111293 and parameters: {'observation_period_num': 31, 'train_rates': 0.8202000144823272, 'learning_rate': 8.678073314476803e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7838769004083586}. Best is trial 12 with value: 0.029543196586401838.[0m
Early stopping at epoch 82
[32m[I 2025-02-04 03:26:55,911][0m Trial 18 finished with value: 0.12232090561997656 and parameters: {'observation_period_num': 91, 'train_rates': 0.8398117446419213, 'learning_rate': 0.0002917639421548, 'batch_size': 99, 'step_size': 1, 'gamma': 0.8484381930600635}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:27:22,564][0m Trial 19 finished with value: 0.13904149416297187 and parameters: {'observation_period_num': 28, 'train_rates': 0.770042166528882, 'learning_rate': 1.9430726604216538e-05, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8179997935965816}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:29:31,240][0m Trial 20 finished with value: 0.43858349323272705 and parameters: {'observation_period_num': 77, 'train_rates': 0.9237848897322105, 'learning_rate': 1.1443764363187343e-06, 'batch_size': 44, 'step_size': 12, 'gamma': 0.7805626700562764}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:35:43,978][0m Trial 21 finished with value: 0.035535379605633875 and parameters: {'observation_period_num': 5, 'train_rates': 0.9884273824157606, 'learning_rate': 0.0008863665878854968, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7544967393353175}. Best is trial 12 with value: 0.029543196586401838.[0m
[32m[I 2025-02-04 03:38:47,519][0m Trial 22 finished with value: 0.027447390059630077 and parameters: {'observation_period_num': 35, 'train_rates': 0.9876759182467544, 'learning_rate': 0.00033532080877648107, 'batch_size': 33, 'step_size': 9, 'gamma': 0.763782920865035}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:39:52,818][0m Trial 23 finished with value: 0.06479065813462347 and parameters: {'observation_period_num': 34, 'train_rates': 0.7660381802062068, 'learning_rate': 0.0002546691121844045, 'batch_size': 79, 'step_size': 8, 'gamma': 0.7731644485855774}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:42:26,842][0m Trial 24 finished with value: 0.11001311136143548 and parameters: {'observation_period_num': 251, 'train_rates': 0.9093168647987446, 'learning_rate': 8.370696097094364e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.798591800373328}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:43:42,699][0m Trial 25 finished with value: 0.047272064777858114 and parameters: {'observation_period_num': 25, 'train_rates': 0.9535425148808122, 'learning_rate': 0.00041510676820482386, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8428906392764355}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:46:13,180][0m Trial 26 finished with value: 0.06703931325585535 and parameters: {'observation_period_num': 102, 'train_rates': 0.8582631885247382, 'learning_rate': 0.00016404501196964943, 'batch_size': 35, 'step_size': 9, 'gamma': 0.7704779999873841}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:47:43,468][0m Trial 27 finished with value: 0.06783234747126698 and parameters: {'observation_period_num': 47, 'train_rates': 0.9561570294712282, 'learning_rate': 6.247933926029261e-05, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8094227080754057}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:50:23,879][0m Trial 28 finished with value: 0.03955186529597943 and parameters: {'observation_period_num': 20, 'train_rates': 0.9059038631013232, 'learning_rate': 0.0002054359983959609, 'batch_size': 35, 'step_size': 13, 'gamma': 0.7888522119981597}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:51:05,153][0m Trial 29 finished with value: 0.15508451913633653 and parameters: {'observation_period_num': 68, 'train_rates': 0.9355769746752912, 'learning_rate': 3.14212654649888e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.7637265052597773}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:52:42,897][0m Trial 30 finished with value: 0.08886484518251588 and parameters: {'observation_period_num': 46, 'train_rates': 0.9612949835729332, 'learning_rate': 0.00012496656291726728, 'batch_size': 61, 'step_size': 13, 'gamma': 0.9323516920372275}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 03:58:55,761][0m Trial 31 finished with value: 0.0419308392283244 and parameters: {'observation_period_num': 7, 'train_rates': 0.9868528439438917, 'learning_rate': 0.0008812292795883232, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7503490379303371}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:02:46,465][0m Trial 32 finished with value: 0.040359913681944214 and parameters: {'observation_period_num': 16, 'train_rates': 0.9878634754790182, 'learning_rate': 0.0005391289668739539, 'batch_size': 26, 'step_size': 12, 'gamma': 0.7650418787175431}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:05:07,407][0m Trial 33 finished with value: 0.0663274095271723 and parameters: {'observation_period_num': 37, 'train_rates': 0.8925911965543228, 'learning_rate': 0.0005358597638455157, 'batch_size': 40, 'step_size': 10, 'gamma': 0.790144126048064}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:06:51,463][0m Trial 34 finished with value: 0.03713888704567869 and parameters: {'observation_period_num': 18, 'train_rates': 0.8541852346046482, 'learning_rate': 0.00034420372003643115, 'batch_size': 53, 'step_size': 7, 'gamma': 0.7580557345136757}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:09:39,168][0m Trial 35 finished with value: 0.07819417867791001 and parameters: {'observation_period_num': 60, 'train_rates': 0.9304935306591682, 'learning_rate': 0.0006843509695247277, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7832089130813374}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:10:42,978][0m Trial 36 finished with value: 0.08720371872186661 and parameters: {'observation_period_num': 156, 'train_rates': 0.9753112942009122, 'learning_rate': 0.00011332278166808854, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8061211061041171}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:11:57,365][0m Trial 37 finished with value: 0.03688470349549802 and parameters: {'observation_period_num': 6, 'train_rates': 0.7725280278067089, 'learning_rate': 0.0004397774017235522, 'batch_size': 71, 'step_size': 6, 'gamma': 0.8585489483353931}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:13:51,208][0m Trial 38 finished with value: 0.04751071633369315 and parameters: {'observation_period_num': 40, 'train_rates': 0.965301879917147, 'learning_rate': 0.00024925627519226123, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8343811986695452}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:17:02,229][0m Trial 39 finished with value: 0.1051827240634609 and parameters: {'observation_period_num': 177, 'train_rates': 0.9333305582102724, 'learning_rate': 0.0006984073871555636, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9500659733855191}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:17:24,841][0m Trial 40 finished with value: 0.10135602752367655 and parameters: {'observation_period_num': 20, 'train_rates': 0.7341411857818914, 'learning_rate': 4.1226956642420975e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.7680543463992615}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:18:38,841][0m Trial 41 finished with value: 0.0380381750259197 and parameters: {'observation_period_num': 5, 'train_rates': 0.7843909058619289, 'learning_rate': 0.0004226191442267024, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8655349288336485}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:20:13,058][0m Trial 42 finished with value: 0.05137388582928723 and parameters: {'observation_period_num': 15, 'train_rates': 0.6089978287973055, 'learning_rate': 0.0004503012141125974, 'batch_size': 47, 'step_size': 6, 'gamma': 0.888231655013025}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:23:06,710][0m Trial 43 finished with value: 0.08127085957919337 and parameters: {'observation_period_num': 32, 'train_rates': 0.7379187087820486, 'learning_rate': 0.0003003094842254578, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8729014526580263}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:23:51,525][0m Trial 44 finished with value: 0.059476325999466215 and parameters: {'observation_period_num': 54, 'train_rates': 0.8336753656622992, 'learning_rate': 0.0009586529341920799, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8533221696631568}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:28:31,657][0m Trial 45 finished with value: 0.05831719902676998 and parameters: {'observation_period_num': 7, 'train_rates': 0.6628642147313144, 'learning_rate': 0.00020820126179748935, 'batch_size': 16, 'step_size': 10, 'gamma': 0.913692004660162}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:30:05,247][0m Trial 46 finished with value: 0.048899418534711005 and parameters: {'observation_period_num': 25, 'train_rates': 0.8633385127719223, 'learning_rate': 0.00013398522582578444, 'batch_size': 59, 'step_size': 5, 'gamma': 0.8203076646014311}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:30:51,963][0m Trial 47 finished with value: 0.12665385956666908 and parameters: {'observation_period_num': 123, 'train_rates': 0.7403129234394479, 'learning_rate': 0.0006227604118273799, 'batch_size': 109, 'step_size': 11, 'gamma': 0.758731057406312}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:33:04,711][0m Trial 48 finished with value: 0.09661202132701874 and parameters: {'observation_period_num': 71, 'train_rates': 0.9697074752455299, 'learning_rate': 0.0003920625565978313, 'batch_size': 44, 'step_size': 7, 'gamma': 0.7947908906706335}. Best is trial 22 with value: 0.027447390059630077.[0m
[32m[I 2025-02-04 04:36:36,104][0m Trial 49 finished with value: 0.04545126939719578 and parameters: {'observation_period_num': 41, 'train_rates': 0.9455234832396133, 'learning_rate': 0.00023955495398743484, 'batch_size': 27, 'step_size': 4, 'gamma': 0.7774458312106901}. Best is trial 22 with value: 0.027447390059630077.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-04 04:36:36,115][0m A new study created in memory with name: no-name-6760661d-bcbe-4abf-969d-5de6e7b79b78[0m
[32m[I 2025-02-04 04:37:46,420][0m Trial 0 finished with value: 0.10189419419255263 and parameters: {'observation_period_num': 53, 'train_rates': 0.7437699178480393, 'learning_rate': 0.0005163027054179953, 'batch_size': 71, 'step_size': 14, 'gamma': 0.7835858183138512}. Best is trial 0 with value: 0.10189419419255263.[0m
[32m[I 2025-02-04 04:38:18,129][0m Trial 1 finished with value: 1.7070112122690737 and parameters: {'observation_period_num': 14, 'train_rates': 0.9311305339213862, 'learning_rate': 5.066146134573443e-06, 'batch_size': 202, 'step_size': 3, 'gamma': 0.7861641234792169}. Best is trial 0 with value: 0.10189419419255263.[0m
[32m[I 2025-02-04 04:38:42,942][0m Trial 2 finished with value: 0.13324849545091821 and parameters: {'observation_period_num': 111, 'train_rates': 0.7815644307317238, 'learning_rate': 6.116780899246546e-05, 'batch_size': 234, 'step_size': 15, 'gamma': 0.8733926691621234}. Best is trial 0 with value: 0.10189419419255263.[0m
[32m[I 2025-02-04 04:40:27,873][0m Trial 3 finished with value: 0.047130018239840865 and parameters: {'observation_period_num': 42, 'train_rates': 0.9396357300889921, 'learning_rate': 0.00017691163262020384, 'batch_size': 55, 'step_size': 1, 'gamma': 0.9615775558740592}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:41:17,255][0m Trial 4 finished with value: 0.5641601308697799 and parameters: {'observation_period_num': 155, 'train_rates': 0.6136780113063469, 'learning_rate': 3.2359418630093844e-06, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8725107658009909}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:41:47,364][0m Trial 5 finished with value: 0.5940207139405504 and parameters: {'observation_period_num': 245, 'train_rates': 0.6174344767344756, 'learning_rate': 5.185150887889143e-06, 'batch_size': 151, 'step_size': 11, 'gamma': 0.898677631401228}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:42:14,071][0m Trial 6 finished with value: 0.08517152810144046 and parameters: {'observation_period_num': 78, 'train_rates': 0.736910020002895, 'learning_rate': 0.00020321945560758997, 'batch_size': 201, 'step_size': 12, 'gamma': 0.8061543767409083}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:42:44,447][0m Trial 7 finished with value: 0.07930447769271953 and parameters: {'observation_period_num': 56, 'train_rates': 0.8459952327763873, 'learning_rate': 0.00011664116279754503, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9688786261667001}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:43:06,664][0m Trial 8 finished with value: 1.4041416828673159 and parameters: {'observation_period_num': 224, 'train_rates': 0.7003334482202892, 'learning_rate': 1.5186703363881951e-06, 'batch_size': 221, 'step_size': 10, 'gamma': 0.7954729659620923}. Best is trial 3 with value: 0.047130018239840865.[0m
Early stopping at epoch 58
[32m[I 2025-02-04 04:43:39,878][0m Trial 9 finished with value: 0.24724195644028954 and parameters: {'observation_period_num': 139, 'train_rates': 0.6093736176453273, 'learning_rate': 0.00018265047235689518, 'batch_size': 80, 'step_size': 1, 'gamma': 0.7736547473287688}. Best is trial 3 with value: 0.047130018239840865.[0m
[32m[I 2025-02-04 04:46:47,416][0m Trial 10 finished with value: 0.04112102968965547 and parameters: {'observation_period_num': 6, 'train_rates': 0.9799946960035852, 'learning_rate': 2.5012948928311156e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9807034203750249}. Best is trial 10 with value: 0.04112102968965547.[0m
[32m[I 2025-02-04 04:52:59,126][0m Trial 11 finished with value: 0.028334167581505892 and parameters: {'observation_period_num': 6, 'train_rates': 0.9862233049904742, 'learning_rate': 2.0120657236148504e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9860150154569425}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 04:58:10,778][0m Trial 12 finished with value: 0.04227795324880968 and parameters: {'observation_period_num': 7, 'train_rates': 0.9628688813347327, 'learning_rate': 2.4300084329374858e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9289584010882239}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:03:25,333][0m Trial 13 finished with value: 0.06333075663716779 and parameters: {'observation_period_num': 97, 'train_rates': 0.8761594601222142, 'learning_rate': 1.9068808820253e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9855240278269541}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:04:17,580][0m Trial 14 finished with value: 0.2803178131580353 and parameters: {'observation_period_num': 172, 'train_rates': 0.9811705097825318, 'learning_rate': 1.2888166553674563e-05, 'batch_size': 118, 'step_size': 4, 'gamma': 0.9307897253161858}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:06:25,111][0m Trial 15 finished with value: 0.037607331578245105 and parameters: {'observation_period_num': 6, 'train_rates': 0.8821022767260146, 'learning_rate': 4.2771554754576783e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.9364761084617564}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:07:16,122][0m Trial 16 finished with value: 0.052593562660869835 and parameters: {'observation_period_num': 37, 'train_rates': 0.8878933817360531, 'learning_rate': 5.541534316154402e-05, 'batch_size': 116, 'step_size': 9, 'gamma': 0.9406806364371243}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:09:01,335][0m Trial 17 finished with value: 0.23789675030443402 and parameters: {'observation_period_num': 192, 'train_rates': 0.8371791073156939, 'learning_rate': 1.1937374581703243e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.8355375576173811}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:09:40,063][0m Trial 18 finished with value: 0.06506844917582773 and parameters: {'observation_period_num': 98, 'train_rates': 0.9157306036240342, 'learning_rate': 0.0008786630083434699, 'batch_size': 155, 'step_size': 9, 'gamma': 0.9030195872725262}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:11:32,540][0m Trial 19 finished with value: 0.05350503224696292 and parameters: {'observation_period_num': 27, 'train_rates': 0.8333023207484272, 'learning_rate': 5.907576202959546e-05, 'batch_size': 48, 'step_size': 3, 'gamma': 0.9487178228208469}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:12:28,737][0m Trial 20 finished with value: 0.21632118904313377 and parameters: {'observation_period_num': 71, 'train_rates': 0.8957597293346956, 'learning_rate': 8.272099952852257e-06, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9079206397154838}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:15:44,332][0m Trial 21 finished with value: 0.029055981701889704 and parameters: {'observation_period_num': 12, 'train_rates': 0.9854367509379793, 'learning_rate': 2.9755487280556934e-05, 'batch_size': 31, 'step_size': 6, 'gamma': 0.9848754211849227}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:18:17,997][0m Trial 22 finished with value: 0.046544814850650935 and parameters: {'observation_period_num': 28, 'train_rates': 0.9397820782930822, 'learning_rate': 4.5565771935291765e-05, 'batch_size': 38, 'step_size': 8, 'gamma': 0.989594361179976}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:19:50,418][0m Trial 23 finished with value: 0.08828458189964294 and parameters: {'observation_period_num': 73, 'train_rates': 0.9883446490829073, 'learning_rate': 0.00010775602087659847, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9579798536532063}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:22:25,341][0m Trial 24 finished with value: 0.04086279302631339 and parameters: {'observation_period_num': 5, 'train_rates': 0.9505595017174605, 'learning_rate': 3.244787451580353e-05, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9632167814434655}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:26:11,701][0m Trial 25 finished with value: 0.056945839180396154 and parameters: {'observation_period_num': 35, 'train_rates': 0.8663208294876107, 'learning_rate': 1.492654257086351e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.9242679238295842}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:27:37,622][0m Trial 26 finished with value: 0.13246967255805445 and parameters: {'observation_period_num': 55, 'train_rates': 0.8049964538145231, 'learning_rate': 7.349470678726557e-06, 'batch_size': 61, 'step_size': 3, 'gamma': 0.9755631614755215}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:28:43,102][0m Trial 27 finished with value: 0.05224552454802611 and parameters: {'observation_period_num': 23, 'train_rates': 0.9207287040712907, 'learning_rate': 3.6163384709342995e-05, 'batch_size': 91, 'step_size': 5, 'gamma': 0.9461480081583208}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:34:28,774][0m Trial 28 finished with value: 0.04751505279863203 and parameters: {'observation_period_num': 48, 'train_rates': 0.8982372506106573, 'learning_rate': 7.897062288363887e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8389369195811267}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:35:55,027][0m Trial 29 finished with value: 0.28413826501683187 and parameters: {'observation_period_num': 62, 'train_rates': 0.9606587619107267, 'learning_rate': 2.067691508134187e-06, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9158875848523144}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:37:57,438][0m Trial 30 finished with value: 0.10942699836274214 and parameters: {'observation_period_num': 92, 'train_rates': 0.7842570302498689, 'learning_rate': 0.0003864879646678689, 'batch_size': 41, 'step_size': 6, 'gamma': 0.9735017628319191}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:40:33,319][0m Trial 31 finished with value: 0.043146283530137115 and parameters: {'observation_period_num': 17, 'train_rates': 0.9536656126770847, 'learning_rate': 3.304813381822437e-05, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9574440232841241}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:41:50,619][0m Trial 32 finished with value: 0.04800770189317446 and parameters: {'observation_period_num': 6, 'train_rates': 0.9145491874595528, 'learning_rate': 2.2143648844238552e-05, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9890526094504525}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:45:09,368][0m Trial 33 finished with value: 0.06265065277970973 and parameters: {'observation_period_num': 23, 'train_rates': 0.9571542171660585, 'learning_rate': 3.842693472740573e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.7528869954645439}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:47:04,024][0m Trial 34 finished with value: 0.04859050735831261 and parameters: {'observation_period_num': 41, 'train_rates': 0.9877227880236833, 'learning_rate': 8.995279457868611e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8867460271297231}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:47:39,102][0m Trial 35 finished with value: 0.1949869761326043 and parameters: {'observation_period_num': 117, 'train_rates': 0.928499516667796, 'learning_rate': 1.6565316863505166e-05, 'batch_size': 170, 'step_size': 4, 'gamma': 0.9632948319163125}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:48:36,270][0m Trial 36 finished with value: 0.0903799751296986 and parameters: {'observation_period_num': 16, 'train_rates': 0.9416460102370677, 'learning_rate': 7.020559345558294e-06, 'batch_size': 105, 'step_size': 10, 'gamma': 0.9456662887642411}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:50:21,156][0m Trial 37 finished with value: 0.06831449089018074 and parameters: {'observation_period_num': 5, 'train_rates': 0.9623788537696603, 'learning_rate': 1.0331099293777415e-05, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8523706902901034}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:52:37,585][0m Trial 38 finished with value: 0.2371817625481376 and parameters: {'observation_period_num': 45, 'train_rates': 0.6541046905826265, 'learning_rate': 3.3987804723993962e-06, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9692727187608496}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:53:00,980][0m Trial 39 finished with value: 0.05166015672493037 and parameters: {'observation_period_num': 30, 'train_rates': 0.8556366765070278, 'learning_rate': 0.0003452192523688972, 'batch_size': 251, 'step_size': 5, 'gamma': 0.8882033121832438}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:54:03,868][0m Trial 40 finished with value: 0.08165312627271892 and parameters: {'observation_period_num': 62, 'train_rates': 0.812946783835302, 'learning_rate': 0.00013141642086153933, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9562303561550461}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 05:57:14,695][0m Trial 41 finished with value: 0.030978452567368336 and parameters: {'observation_period_num': 15, 'train_rates': 0.9751806878788759, 'learning_rate': 3.151724945357789e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9788171360202085}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:01:08,232][0m Trial 42 finished with value: 0.037509051979415946 and parameters: {'observation_period_num': 17, 'train_rates': 0.9694808933041933, 'learning_rate': 2.6752971067575045e-05, 'batch_size': 25, 'step_size': 6, 'gamma': 0.9758688685924212}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:04:40,991][0m Trial 43 finished with value: 0.036095627697139254 and parameters: {'observation_period_num': 21, 'train_rates': 0.9721080179993827, 'learning_rate': 7.111938130818137e-05, 'batch_size': 28, 'step_size': 6, 'gamma': 0.9776419771871453}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:07:43,410][0m Trial 44 finished with value: 0.05931415729481598 and parameters: {'observation_period_num': 19, 'train_rates': 0.752896030595691, 'learning_rate': 2.4226909942232903e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.978878334227315}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:13:06,045][0m Trial 45 finished with value: 0.03758823568267482 and parameters: {'observation_period_num': 37, 'train_rates': 0.976166674092214, 'learning_rate': 4.88352418491706e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9759711727624341}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:16:42,912][0m Trial 46 finished with value: 0.0621040098287247 and parameters: {'observation_period_num': 51, 'train_rates': 0.9688554612730825, 'learning_rate': 7.67277836682356e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9891341822310009}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:18:21,986][0m Trial 47 finished with value: 0.12038920819759369 and parameters: {'observation_period_num': 146, 'train_rates': 0.9353675131906027, 'learning_rate': 1.7721150697377034e-05, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9679384641545815}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:18:53,483][0m Trial 48 finished with value: 0.08136417267431739 and parameters: {'observation_period_num': 85, 'train_rates': 0.9086458292657822, 'learning_rate': 0.0001498991990121156, 'batch_size': 190, 'step_size': 8, 'gamma': 0.9802218063242161}. Best is trial 11 with value: 0.028334167581505892.[0m
[32m[I 2025-02-04 06:21:01,897][0m Trial 49 finished with value: 0.04869891330599785 and parameters: {'observation_period_num': 19, 'train_rates': 0.9894658080328375, 'learning_rate': 2.8332154071255278e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9531038698119619}. Best is trial 11 with value: 0.028334167581505892.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-04 06:21:01,907][0m A new study created in memory with name: no-name-2bfba26d-1a24-42d3-a659-91e1ef2a21ba[0m
[32m[I 2025-02-04 06:21:34,306][0m Trial 0 finished with value: 0.11282759861732185 and parameters: {'observation_period_num': 77, 'train_rates': 0.7597418379932168, 'learning_rate': 4.2972851037989274e-05, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9385074393260903}. Best is trial 0 with value: 0.11282759861732185.[0m
[32m[I 2025-02-04 06:21:59,562][0m Trial 1 finished with value: 0.5450087481742808 and parameters: {'observation_period_num': 28, 'train_rates': 0.6407595025374238, 'learning_rate': 5.560730412396755e-06, 'batch_size': 200, 'step_size': 2, 'gamma': 0.889609288959447}. Best is trial 0 with value: 0.11282759861732185.[0m
[32m[I 2025-02-04 06:22:54,573][0m Trial 2 finished with value: 0.15648189497587284 and parameters: {'observation_period_num': 173, 'train_rates': 0.7686978352045983, 'learning_rate': 9.374041902345207e-05, 'batch_size': 91, 'step_size': 7, 'gamma': 0.7970576146475635}. Best is trial 0 with value: 0.11282759861732185.[0m
[32m[I 2025-02-04 06:23:20,864][0m Trial 3 finished with value: 0.1798550862333049 and parameters: {'observation_period_num': 32, 'train_rates': 0.7636864478623533, 'learning_rate': 2.0882595670120313e-05, 'batch_size': 216, 'step_size': 3, 'gamma': 0.9330271972343225}. Best is trial 0 with value: 0.11282759861732185.[0m
[32m[I 2025-02-04 06:24:13,826][0m Trial 4 finished with value: 0.19458617337446643 and parameters: {'observation_period_num': 221, 'train_rates': 0.9111238988548538, 'learning_rate': 1.1117528695063505e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9487364412913182}. Best is trial 0 with value: 0.11282759861732185.[0m
[32m[I 2025-02-04 06:28:14,048][0m Trial 5 finished with value: 0.10140027836039292 and parameters: {'observation_period_num': 154, 'train_rates': 0.9282146046755089, 'learning_rate': 0.0004170671680457963, 'batch_size': 23, 'step_size': 7, 'gamma': 0.9360609875877404}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:28:46,966][0m Trial 6 finished with value: 0.1532569285701303 and parameters: {'observation_period_num': 150, 'train_rates': 0.7574725435300562, 'learning_rate': 0.000962792137067936, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8374221076661059}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:29:19,260][0m Trial 7 finished with value: 0.13065062854775458 and parameters: {'observation_period_num': 130, 'train_rates': 0.7843329196914277, 'learning_rate': 7.25497097316046e-05, 'batch_size': 166, 'step_size': 12, 'gamma': 0.9475760237791309}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:29:43,354][0m Trial 8 finished with value: 1.6255601975652907 and parameters: {'observation_period_num': 40, 'train_rates': 0.814629036489622, 'learning_rate': 1.1726422868496773e-06, 'batch_size': 249, 'step_size': 6, 'gamma': 0.761411325208239}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:30:12,253][0m Trial 9 finished with value: 0.17169693112373352 and parameters: {'observation_period_num': 143, 'train_rates': 0.9682422183597583, 'learning_rate': 5.034435087119682e-05, 'batch_size': 220, 'step_size': 11, 'gamma': 0.8948385621258372}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:32:48,032][0m Trial 10 finished with value: 0.15556084443861465 and parameters: {'observation_period_num': 247, 'train_rates': 0.8888048209473732, 'learning_rate': 0.0007516053141140818, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8564140235367084}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:34:36,274][0m Trial 11 finished with value: 0.2659665210594153 and parameters: {'observation_period_num': 91, 'train_rates': 0.6709980263492272, 'learning_rate': 0.00025513858295187305, 'batch_size': 42, 'step_size': 15, 'gamma': 0.9783158745775398}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:35:42,653][0m Trial 12 finished with value: 0.12502507963082166 and parameters: {'observation_period_num': 87, 'train_rates': 0.856446908078079, 'learning_rate': 0.00020323332410493333, 'batch_size': 86, 'step_size': 5, 'gamma': 0.9870987190755013}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:36:21,223][0m Trial 13 finished with value: 0.12008535653973619 and parameters: {'observation_period_num': 85, 'train_rates': 0.7071861703569698, 'learning_rate': 0.00028603448284273287, 'batch_size': 133, 'step_size': 8, 'gamma': 0.9146963546874667}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:41:17,930][0m Trial 14 finished with value: 0.18477010551620932 and parameters: {'observation_period_num': 192, 'train_rates': 0.9818382637363774, 'learning_rate': 4.582516850045705e-06, 'batch_size': 19, 'step_size': 13, 'gamma': 0.9124368386309564}. Best is trial 5 with value: 0.10140027836039292.[0m
[32m[I 2025-02-04 06:42:45,182][0m Trial 15 finished with value: 0.09528118156331147 and parameters: {'observation_period_num': 103, 'train_rates': 0.8376678888603877, 'learning_rate': 2.2373686153378928e-05, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9586946623150712}. Best is trial 15 with value: 0.09528118156331147.[0m
[32m[I 2025-02-04 06:44:16,790][0m Trial 16 finished with value: 0.09359528247303175 and parameters: {'observation_period_num': 107, 'train_rates': 0.9279138302206468, 'learning_rate': 1.6949448470172873e-05, 'batch_size': 62, 'step_size': 4, 'gamma': 0.9717421673384411}. Best is trial 16 with value: 0.09359528247303175.[0m
[32m[I 2025-02-04 06:45:44,356][0m Trial 17 finished with value: 0.11267449385560124 and parameters: {'observation_period_num': 116, 'train_rates': 0.8439993303447759, 'learning_rate': 1.4250418297025971e-05, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9687967078503228}. Best is trial 16 with value: 0.09359528247303175.[0m
Early stopping at epoch 72
[32m[I 2025-02-04 06:46:51,251][0m Trial 18 finished with value: 2.649376316311993 and parameters: {'observation_period_num': 57, 'train_rates': 0.8635832129756694, 'learning_rate': 2.4512665567434213e-06, 'batch_size': 61, 'step_size': 1, 'gamma': 0.8371665517748785}. Best is trial 16 with value: 0.09359528247303175.[0m
[32m[I 2025-02-04 06:47:39,973][0m Trial 19 finished with value: 0.29263793473031113 and parameters: {'observation_period_num': 112, 'train_rates': 0.9447667069611916, 'learning_rate': 6.350173994366568e-06, 'batch_size': 124, 'step_size': 4, 'gamma': 0.9643767452585933}. Best is trial 16 with value: 0.09359528247303175.[0m
[32m[I 2025-02-04 06:48:52,180][0m Trial 20 finished with value: 0.05427261392691726 and parameters: {'observation_period_num': 5, 'train_rates': 0.8178876709255631, 'learning_rate': 2.676966824193975e-05, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9877462229862086}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:50:13,461][0m Trial 21 finished with value: 0.05965566484346276 and parameters: {'observation_period_num': 6, 'train_rates': 0.8146365267592426, 'learning_rate': 2.7123593174035624e-05, 'batch_size': 65, 'step_size': 1, 'gamma': 0.9819417243168209}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:51:17,827][0m Trial 22 finished with value: 0.06579345446669634 and parameters: {'observation_period_num': 9, 'train_rates': 0.8176635820999747, 'learning_rate': 2.7647742869398037e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9813752879228965}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:52:16,284][0m Trial 23 finished with value: 0.05912620875371176 and parameters: {'observation_period_num': 6, 'train_rates': 0.723698204390953, 'learning_rate': 3.5809659861914305e-05, 'batch_size': 86, 'step_size': 1, 'gamma': 0.9877007969375102}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:53:04,147][0m Trial 24 finished with value: 0.06325687125528316 and parameters: {'observation_period_num': 6, 'train_rates': 0.7259779315348145, 'learning_rate': 8.961911188304094e-05, 'batch_size': 110, 'step_size': 2, 'gamma': 0.9889299393143162}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:53:51,932][0m Trial 25 finished with value: 0.1369608524316799 and parameters: {'observation_period_num': 56, 'train_rates': 0.7115952778864196, 'learning_rate': 0.0001454842105230407, 'batch_size': 104, 'step_size': 1, 'gamma': 0.9190162899567917}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:54:48,601][0m Trial 26 finished with value: 0.3857818451161458 and parameters: {'observation_period_num': 19, 'train_rates': 0.6018414516333787, 'learning_rate': 1.011619730508998e-05, 'batch_size': 77, 'step_size': 2, 'gamma': 0.954583815739726}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:56:35,398][0m Trial 27 finished with value: 0.11294156479536845 and parameters: {'observation_period_num': 57, 'train_rates': 0.6793500546821976, 'learning_rate': 3.761681875947955e-05, 'batch_size': 43, 'step_size': 3, 'gamma': 0.877087739771109}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:57:13,473][0m Trial 28 finished with value: 0.23288474455342045 and parameters: {'observation_period_num': 42, 'train_rates': 0.8085372951901002, 'learning_rate': 5.359309456291249e-05, 'batch_size': 144, 'step_size': 3, 'gamma': 0.8039322234170564}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:57:55,949][0m Trial 29 finished with value: 0.30904072128674565 and parameters: {'observation_period_num': 68, 'train_rates': 0.7407059966148782, 'learning_rate': 2.8831852535399974e-05, 'batch_size': 122, 'step_size': 1, 'gamma': 0.9362608364840709}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:58:52,468][0m Trial 30 finished with value: 0.10788821415687964 and parameters: {'observation_period_num': 19, 'train_rates': 0.7903633977259195, 'learning_rate': 8.557560531357849e-06, 'batch_size': 94, 'step_size': 5, 'gamma': 0.9898481447532103}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 06:59:37,508][0m Trial 31 finished with value: 0.05762546046760427 and parameters: {'observation_period_num': 13, 'train_rates': 0.7175673910714573, 'learning_rate': 9.82350617752153e-05, 'batch_size': 111, 'step_size': 2, 'gamma': 0.9895231174589405}. Best is trial 20 with value: 0.05427261392691726.[0m
[32m[I 2025-02-04 07:00:42,809][0m Trial 32 finished with value: 0.05020484768243009 and parameters: {'observation_period_num': 6, 'train_rates': 0.6814512371763013, 'learning_rate': 0.00010825099246108619, 'batch_size': 73, 'step_size': 2, 'gamma': 0.9689131527172499}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:01:45,387][0m Trial 33 finished with value: 0.059969241280548126 and parameters: {'observation_period_num': 27, 'train_rates': 0.6785012530762358, 'learning_rate': 0.00013916932519123835, 'batch_size': 74, 'step_size': 2, 'gamma': 0.9686394795117828}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:02:33,396][0m Trial 34 finished with value: 0.216190387051681 and parameters: {'observation_period_num': 41, 'train_rates': 0.6513461820424128, 'learning_rate': 0.0001250097572048786, 'batch_size': 98, 'step_size': 3, 'gamma': 0.9580008894311832}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:03:13,402][0m Trial 35 finished with value: 0.5354210393736345 and parameters: {'observation_period_num': 25, 'train_rates': 0.6148330633722641, 'learning_rate': 6.708344440839899e-05, 'batch_size': 117, 'step_size': 2, 'gamma': 0.9451437714297583}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:03:49,604][0m Trial 36 finished with value: 0.09454188990719994 and parameters: {'observation_period_num': 39, 'train_rates': 0.7021147393895903, 'learning_rate': 4.418629712963571e-05, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9302644370976332}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:04:18,810][0m Trial 37 finished with value: 0.05053879063790901 and parameters: {'observation_period_num': 17, 'train_rates': 0.7455886450817334, 'learning_rate': 0.00036479552261242686, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8966231464318183}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:04:47,611][0m Trial 38 finished with value: 0.07338159801814088 and parameters: {'observation_period_num': 51, 'train_rates': 0.7522926943110387, 'learning_rate': 0.0004891827040114228, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8959307646172356}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:05:16,130][0m Trial 39 finished with value: 0.06318745906950847 and parameters: {'observation_period_num': 70, 'train_rates': 0.7706148220370317, 'learning_rate': 0.0003845173023901212, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8722694815669255}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:05:39,368][0m Trial 40 finished with value: 0.20100496684143232 and parameters: {'observation_period_num': 25, 'train_rates': 0.6313436086835272, 'learning_rate': 0.0001947192961650023, 'batch_size': 219, 'step_size': 7, 'gamma': 0.8488932982386264}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:06:12,717][0m Trial 41 finished with value: 0.06948966169013426 and parameters: {'observation_period_num': 13, 'train_rates': 0.7345959131428251, 'learning_rate': 0.00010692673165147362, 'batch_size': 162, 'step_size': 3, 'gamma': 0.9696124651742212}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:07:55,111][0m Trial 42 finished with value: 0.08731485695978783 and parameters: {'observation_period_num': 34, 'train_rates': 0.6933265548437879, 'learning_rate': 7.864067488102363e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.9464440585119277}. Best is trial 32 with value: 0.05020484768243009.[0m
[32m[I 2025-02-04 07:08:50,821][0m Trial 43 finished with value: 0.046637817274513646 and parameters: {'observation_period_num': 17, 'train_rates': 0.7225756876602079, 'learning_rate': 0.0006481049933478129, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8100464085974847}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:09:10,681][0m Trial 44 finished with value: 0.057657359230266075 and parameters: {'observation_period_num': 20, 'train_rates': 0.6535672239125407, 'learning_rate': 0.0006535061442196152, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8080678887022629}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:10:00,950][0m Trial 45 finished with value: 0.05432865276651563 and parameters: {'observation_period_num': 32, 'train_rates': 0.7821743286086731, 'learning_rate': 0.000909904657055568, 'batch_size': 108, 'step_size': 10, 'gamma': 0.7600525482152726}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:10:24,813][0m Trial 46 finished with value: 0.13219085170163047 and parameters: {'observation_period_num': 176, 'train_rates': 0.7764747696180392, 'learning_rate': 0.000938620647683986, 'batch_size': 230, 'step_size': 10, 'gamma': 0.7646525869950229}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:10:55,138][0m Trial 47 finished with value: 0.055535666182159216 and parameters: {'observation_period_num': 32, 'train_rates': 0.7547088630863515, 'learning_rate': 0.0005461073170462183, 'batch_size': 181, 'step_size': 11, 'gamma': 0.7813265623571569}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:11:23,266][0m Trial 48 finished with value: 0.06634625027137071 and parameters: {'observation_period_num': 49, 'train_rates': 0.7977398058517621, 'learning_rate': 0.00028578576287761605, 'batch_size': 203, 'step_size': 9, 'gamma': 0.7500368017756891}. Best is trial 43 with value: 0.046637817274513646.[0m
[32m[I 2025-02-04 07:12:05,771][0m Trial 49 finished with value: 0.054796291510099876 and parameters: {'observation_period_num': 75, 'train_rates': 0.8738105469439142, 'learning_rate': 0.0003695929519661978, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8217405462740038}. Best is trial 43 with value: 0.046637817274513646.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-04 07:12:05,782][0m A new study created in memory with name: no-name-d7faa2d4-7bf9-4133-83a6-5dbb3a583207[0m
[32m[I 2025-02-04 07:12:49,159][0m Trial 0 finished with value: 0.04832195975354027 and parameters: {'observation_period_num': 22, 'train_rates': 0.8369580317424605, 'learning_rate': 9.174440518308638e-05, 'batch_size': 132, 'step_size': 5, 'gamma': 0.9865889966993973}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:14:46,627][0m Trial 1 finished with value: 0.2558178775715378 and parameters: {'observation_period_num': 173, 'train_rates': 0.9047808191686653, 'learning_rate': 9.638227119877877e-06, 'batch_size': 46, 'step_size': 3, 'gamma': 0.8840022348066405}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:15:10,856][0m Trial 2 finished with value: 0.16599768985802096 and parameters: {'observation_period_num': 218, 'train_rates': 0.8121562939640342, 'learning_rate': 0.0004317276651104747, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9308122017104039}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:16:01,556][0m Trial 3 finished with value: 0.18414438296072555 and parameters: {'observation_period_num': 217, 'train_rates': 0.8154358186587677, 'learning_rate': 0.00024672771365930355, 'batch_size': 99, 'step_size': 15, 'gamma': 0.8710403695986988}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:16:55,692][0m Trial 4 finished with value: 0.297437426067308 and parameters: {'observation_period_num': 179, 'train_rates': 0.7825877765338856, 'learning_rate': 1.789437011466035e-05, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7617110449252907}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:17:22,766][0m Trial 5 finished with value: 0.9464145800735377 and parameters: {'observation_period_num': 183, 'train_rates': 0.9144435985433135, 'learning_rate': 1.2777312701763199e-06, 'batch_size': 215, 'step_size': 11, 'gamma': 0.8106909730090803}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:18:30,640][0m Trial 6 finished with value: 0.2518641739893871 and parameters: {'observation_period_num': 213, 'train_rates': 0.7708495351763626, 'learning_rate': 2.1762003471702715e-05, 'batch_size': 72, 'step_size': 11, 'gamma': 0.7717352678136037}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:18:56,809][0m Trial 7 finished with value: 0.19624224881972036 and parameters: {'observation_period_num': 219, 'train_rates': 0.7277206718278818, 'learning_rate': 9.95943824404547e-05, 'batch_size': 192, 'step_size': 14, 'gamma': 0.9512466361812242}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:19:44,229][0m Trial 8 finished with value: 0.0656462684580328 and parameters: {'observation_period_num': 83, 'train_rates': 0.8307940017499197, 'learning_rate': 0.00021333948694063842, 'batch_size': 116, 'step_size': 15, 'gamma': 0.829980241737619}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:20:36,848][0m Trial 9 finished with value: 0.0984264851148112 and parameters: {'observation_period_num': 5, 'train_rates': 0.6576357992444354, 'learning_rate': 3.5139989143579886e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8558504662401103}. Best is trial 0 with value: 0.04832195975354027.[0m
[32m[I 2025-02-04 07:21:18,290][0m Trial 10 finished with value: 0.02850021794438362 and parameters: {'observation_period_num': 10, 'train_rates': 0.9875366212038447, 'learning_rate': 0.0008211719816221531, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9817291017168376}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:21:57,064][0m Trial 11 finished with value: 0.04227079823613167 and parameters: {'observation_period_num': 18, 'train_rates': 0.9843950415293952, 'learning_rate': 0.0008641285172893735, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9876478868252225}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:22:36,709][0m Trial 12 finished with value: 0.11404762417078018 and parameters: {'observation_period_num': 59, 'train_rates': 0.9674288920393399, 'learning_rate': 0.0007305022293154753, 'batch_size': 157, 'step_size': 6, 'gamma': 0.9892438246352837}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:23:15,987][0m Trial 13 finished with value: 0.09583353996276855 and parameters: {'observation_period_num': 107, 'train_rates': 0.9798820319343804, 'learning_rate': 0.0009594610442619315, 'batch_size': 163, 'step_size': 1, 'gamma': 0.9408981332842992}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:23:49,453][0m Trial 14 finished with value: 0.42419462953694165 and parameters: {'observation_period_num': 39, 'train_rates': 0.912399434251585, 'learning_rate': 2.2638368893879107e-06, 'batch_size': 182, 'step_size': 8, 'gamma': 0.9090914672699633}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:28:22,641][0m Trial 15 finished with value: 0.0854601557724751 and parameters: {'observation_period_num': 61, 'train_rates': 0.9461595662778008, 'learning_rate': 0.00012316133258180817, 'batch_size': 21, 'step_size': 7, 'gamma': 0.9629026778573904}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:28:46,379][0m Trial 16 finished with value: 0.11019534781265683 and parameters: {'observation_period_num': 136, 'train_rates': 0.880285672195883, 'learning_rate': 0.0004143235849011532, 'batch_size': 251, 'step_size': 2, 'gamma': 0.9086044320322352}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:29:19,543][0m Trial 17 finished with value: 0.19381189865400722 and parameters: {'observation_period_num': 38, 'train_rates': 0.6039763548832426, 'learning_rate': 4.5051422056356525e-05, 'batch_size': 139, 'step_size': 8, 'gamma': 0.9715805586398739}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:29:52,937][0m Trial 18 finished with value: 0.14072853326797485 and parameters: {'observation_period_num': 5, 'train_rates': 0.9888595201808871, 'learning_rate': 5.762708441521495e-06, 'batch_size': 207, 'step_size': 9, 'gamma': 0.9120611315725594}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:30:27,537][0m Trial 19 finished with value: 0.10547681555718831 and parameters: {'observation_period_num': 120, 'train_rates': 0.8703595004343014, 'learning_rate': 0.0009928328904191288, 'batch_size': 165, 'step_size': 5, 'gamma': 0.9340817379762243}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:31:06,785][0m Trial 20 finished with value: 0.14837923645973206 and parameters: {'observation_period_num': 251, 'train_rates': 0.9488449980798495, 'learning_rate': 0.00040187950729102825, 'batch_size': 143, 'step_size': 6, 'gamma': 0.9657554890724589}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:31:49,119][0m Trial 21 finished with value: 0.07981991466029913 and parameters: {'observation_period_num': 31, 'train_rates': 0.720408606272964, 'learning_rate': 8.088893528548367e-05, 'batch_size': 123, 'step_size': 4, 'gamma': 0.9826200660372661}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:32:21,500][0m Trial 22 finished with value: 0.054542162166587235 and parameters: {'observation_period_num': 22, 'train_rates': 0.8517204174981147, 'learning_rate': 0.00015147122902768026, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9865476971438952}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:33:10,029][0m Trial 23 finished with value: 0.07365413572566182 and parameters: {'observation_period_num': 65, 'train_rates': 0.9293621884289605, 'learning_rate': 0.0005105935713854245, 'batch_size': 122, 'step_size': 4, 'gamma': 0.9555540515743596}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:33:50,216][0m Trial 24 finished with value: 0.06885220476233207 and parameters: {'observation_period_num': 88, 'train_rates': 0.8837074225744616, 'learning_rate': 6.389604069816681e-05, 'batch_size': 147, 'step_size': 7, 'gamma': 0.9729038986375141}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:34:16,567][0m Trial 25 finished with value: 0.06676312917653396 and parameters: {'observation_period_num': 18, 'train_rates': 0.7495876550298444, 'learning_rate': 0.00022369665308280011, 'batch_size': 207, 'step_size': 3, 'gamma': 0.9238688141256531}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:35:13,666][0m Trial 26 finished with value: 0.12569985409876458 and parameters: {'observation_period_num': 44, 'train_rates': 0.954305718698228, 'learning_rate': 0.0006033299136338532, 'batch_size': 110, 'step_size': 5, 'gamma': 0.9479400547786765}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:36:19,880][0m Trial 27 finished with value: 0.26942316312956943 and parameters: {'observation_period_num': 82, 'train_rates': 0.682867194948866, 'learning_rate': 0.0002696037268035654, 'batch_size': 71, 'step_size': 7, 'gamma': 0.9726774255831838}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:36:53,291][0m Trial 28 finished with value: 0.06816766393739124 and parameters: {'observation_period_num': 54, 'train_rates': 0.8516651774879419, 'learning_rate': 5.814844673046548e-05, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8925311529443348}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:37:32,980][0m Trial 29 finished with value: 0.39725711219238513 and parameters: {'observation_period_num': 152, 'train_rates': 0.9059732373873914, 'learning_rate': 1.1004193940997371e-05, 'batch_size': 148, 'step_size': 2, 'gamma': 0.8913479710265563}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:38:01,781][0m Trial 30 finished with value: 0.5242691040039062 and parameters: {'observation_period_num': 20, 'train_rates': 0.9362165237298766, 'learning_rate': 5.2677221735153055e-06, 'batch_size': 224, 'step_size': 5, 'gamma': 0.8466456941060341}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:38:33,530][0m Trial 31 finished with value: 0.04606683561192878 and parameters: {'observation_period_num': 19, 'train_rates': 0.8436694371462037, 'learning_rate': 0.00015324265586188723, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9897677994976253}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:39:03,968][0m Trial 32 finished with value: 0.045206471294164656 and parameters: {'observation_period_num': 7, 'train_rates': 0.8302600798434615, 'learning_rate': 0.00016094274634620963, 'batch_size': 193, 'step_size': 6, 'gamma': 0.9851163977525133}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:39:33,835][0m Trial 33 finished with value: 0.048795713926665485 and parameters: {'observation_period_num': 8, 'train_rates': 0.8044287991267949, 'learning_rate': 0.0001527214203259163, 'batch_size': 195, 'step_size': 6, 'gamma': 0.9886029351346953}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:40:01,330][0m Trial 34 finished with value: 0.04455055420047942 and parameters: {'observation_period_num': 23, 'train_rates': 0.8458216628790503, 'learning_rate': 0.0003181998825394532, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9603986765105533}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:40:28,921][0m Trial 35 finished with value: 0.0947284922003746 and parameters: {'observation_period_num': 32, 'train_rates': 0.988656696245818, 'learning_rate': 0.00031569122570443837, 'batch_size': 234, 'step_size': 9, 'gamma': 0.9578821422679971}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:40:47,918][0m Trial 36 finished with value: 0.057946342003853964 and parameters: {'observation_period_num': 50, 'train_rates': 0.781850219686129, 'learning_rate': 0.0006772060404808738, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9227292895362642}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:41:14,291][0m Trial 37 finished with value: 0.07866775172834213 and parameters: {'observation_period_num': 70, 'train_rates': 0.8197945744967615, 'learning_rate': 0.0003648804428758781, 'batch_size': 235, 'step_size': 8, 'gamma': 0.9761365502146918}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:41:43,594][0m Trial 38 finished with value: 0.045966920775105384 and parameters: {'observation_period_num': 24, 'train_rates': 0.8833941351240115, 'learning_rate': 0.0005892798485951351, 'batch_size': 199, 'step_size': 10, 'gamma': 0.9426040427785343}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:42:09,443][0m Trial 39 finished with value: 0.0673615038394928 and parameters: {'observation_period_num': 98, 'train_rates': 0.9646404964373433, 'learning_rate': 0.0008200804387425587, 'batch_size': 219, 'step_size': 13, 'gamma': 0.9607819365915782}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:42:42,567][0m Trial 40 finished with value: 0.03984414447132532 and parameters: {'observation_period_num': 13, 'train_rates': 0.9245058051943945, 'learning_rate': 0.00019440871329195312, 'batch_size': 170, 'step_size': 10, 'gamma': 0.8085763751792072}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:43:18,697][0m Trial 41 finished with value: 0.04354283597490509 and parameters: {'observation_period_num': 5, 'train_rates': 0.9332233682789711, 'learning_rate': 0.00019784326221792685, 'batch_size': 160, 'step_size': 10, 'gamma': 0.7905500423265714}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:44:02,261][0m Trial 42 finished with value: 0.039241014764859125 and parameters: {'observation_period_num': 31, 'train_rates': 0.9289328477139465, 'learning_rate': 0.0003134562241987758, 'batch_size': 132, 'step_size': 10, 'gamma': 0.7918213212816038}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:44:43,627][0m Trial 43 finished with value: 0.03808898478746414 and parameters: {'observation_period_num': 42, 'train_rates': 0.9702983887882878, 'learning_rate': 0.0004827392853911744, 'batch_size': 132, 'step_size': 11, 'gamma': 0.787228583705126}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:45:28,846][0m Trial 44 finished with value: 0.03840077295899391 and parameters: {'observation_period_num': 33, 'train_rates': 0.9681330353596154, 'learning_rate': 0.0004883928831077569, 'batch_size': 129, 'step_size': 11, 'gamma': 0.7924863216649223}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:46:23,393][0m Trial 45 finished with value: 0.048276547342538834 and parameters: {'observation_period_num': 45, 'train_rates': 0.9677641416483415, 'learning_rate': 0.00048179807338638553, 'batch_size': 105, 'step_size': 11, 'gamma': 0.7953785134924896}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:47:05,537][0m Trial 46 finished with value: 0.059452502188824974 and parameters: {'observation_period_num': 74, 'train_rates': 0.9217502986292168, 'learning_rate': 0.0001021077907805088, 'batch_size': 125, 'step_size': 12, 'gamma': 0.7505343454385436}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:48:00,827][0m Trial 47 finished with value: 0.045236333053670454 and parameters: {'observation_period_num': 34, 'train_rates': 0.8970030889675114, 'learning_rate': 0.0002437157950623958, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8202874347378004}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:48:48,529][0m Trial 48 finished with value: 0.10789578408002853 and parameters: {'observation_period_num': 53, 'train_rates': 0.9605245840377579, 'learning_rate': 2.3654219921642818e-05, 'batch_size': 132, 'step_size': 13, 'gamma': 0.7848012693826988}. Best is trial 10 with value: 0.02850021794438362.[0m
[32m[I 2025-02-04 07:50:14,146][0m Trial 49 finished with value: 0.03196329564640397 and parameters: {'observation_period_num': 31, 'train_rates': 0.9742723043856049, 'learning_rate': 0.000485004497698952, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8057570123298157}. Best is trial 10 with value: 0.02850021794438362.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.9287378374277016, 'learning_rate': 0.0005582732056514692, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8203869803270176}
Epoch 1/300, trend Loss: 0.1653 | 0.0965
Epoch 2/300, trend Loss: 0.0959 | 0.0790
Epoch 3/300, trend Loss: 0.0852 | 0.0675
Epoch 4/300, trend Loss: 0.0756 | 0.0620
Epoch 5/300, trend Loss: 0.0629 | 0.0625
Epoch 6/300, trend Loss: 0.0570 | 0.0509
Epoch 7/300, trend Loss: 0.0518 | 0.0509
Epoch 8/300, trend Loss: 0.0488 | 0.0477
Epoch 9/300, trend Loss: 0.0456 | 0.0442
Epoch 10/300, trend Loss: 0.0442 | 0.0428
Epoch 11/300, trend Loss: 0.0422 | 0.0409
Epoch 12/300, trend Loss: 0.0412 | 0.0396
Epoch 13/300, trend Loss: 0.0397 | 0.0390
Epoch 14/300, trend Loss: 0.0380 | 0.0371
Epoch 15/300, trend Loss: 0.0372 | 0.0362
Epoch 16/300, trend Loss: 0.0364 | 0.0353
Epoch 17/300, trend Loss: 0.0356 | 0.0362
Epoch 18/300, trend Loss: 0.0350 | 0.0361
Epoch 19/300, trend Loss: 0.0343 | 0.0351
Epoch 20/300, trend Loss: 0.0339 | 0.0346
Epoch 21/300, trend Loss: 0.0335 | 0.0347
Epoch 22/300, trend Loss: 0.0331 | 0.0345
Epoch 23/300, trend Loss: 0.0329 | 0.0339
Epoch 24/300, trend Loss: 0.0324 | 0.0336
Epoch 25/300, trend Loss: 0.0324 | 0.0329
Epoch 26/300, trend Loss: 0.0329 | 0.0340
Epoch 27/300, trend Loss: 0.0325 | 0.0335
Epoch 28/300, trend Loss: 0.0323 | 0.0335
Epoch 29/300, trend Loss: 0.0315 | 0.0329
Epoch 30/300, trend Loss: 0.0313 | 0.0325
Epoch 31/300, trend Loss: 0.0309 | 0.0325
Epoch 32/300, trend Loss: 0.0313 | 0.0325
Epoch 33/300, trend Loss: 0.0308 | 0.0328
Epoch 34/300, trend Loss: 0.0323 | 0.0354
Epoch 35/300, trend Loss: 0.0306 | 0.0320
Epoch 36/300, trend Loss: 0.0310 | 0.0341
Epoch 37/300, trend Loss: 0.0313 | 0.0335
Epoch 38/300, trend Loss: 0.0294 | 0.0334
Epoch 39/300, trend Loss: 0.0296 | 0.0332
Epoch 40/300, trend Loss: 0.0295 | 0.0337
Epoch 41/300, trend Loss: 0.0289 | 0.0321
Epoch 42/300, trend Loss: 0.0289 | 0.0317
Epoch 43/300, trend Loss: 0.0286 | 0.0318
Epoch 44/300, trend Loss: 0.0284 | 0.0316
Epoch 45/300, trend Loss: 0.0285 | 0.0301
Epoch 46/300, trend Loss: 0.0287 | 0.0306
Epoch 47/300, trend Loss: 0.0286 | 0.0321
Epoch 48/300, trend Loss: 0.0287 | 0.0322
Epoch 49/300, trend Loss: 0.0289 | 0.0321
Epoch 50/300, trend Loss: 0.0291 | 0.0321
Epoch 51/300, trend Loss: 0.0284 | 0.0346
Epoch 52/300, trend Loss: 0.0281 | 0.0379
Epoch 53/300, trend Loss: 0.0277 | 0.0477
Epoch 54/300, trend Loss: 0.0276 | 0.0459
Epoch 55/300, trend Loss: 0.0267 | 0.0364
Epoch 56/300, trend Loss: 0.0296 | 0.0312
Epoch 57/300, trend Loss: 0.0310 | 0.0360
Epoch 58/300, trend Loss: 0.0297 | 0.0361
Epoch 59/300, trend Loss: 0.0280 | 0.0304
Epoch 60/300, trend Loss: 0.0256 | 0.0284
Epoch 61/300, trend Loss: 0.0250 | 0.0288
Epoch 62/300, trend Loss: 0.0248 | 0.0281
Epoch 63/300, trend Loss: 0.0244 | 0.0279
Epoch 64/300, trend Loss: 0.0243 | 0.0280
Epoch 65/300, trend Loss: 0.0242 | 0.0277
Epoch 66/300, trend Loss: 0.0240 | 0.0274
Epoch 67/300, trend Loss: 0.0238 | 0.0273
Epoch 68/300, trend Loss: 0.0237 | 0.0273
Epoch 69/300, trend Loss: 0.0236 | 0.0271
Epoch 70/300, trend Loss: 0.0236 | 0.0269
Epoch 71/300, trend Loss: 0.0234 | 0.0269
Epoch 72/300, trend Loss: 0.0234 | 0.0269
Epoch 73/300, trend Loss: 0.0233 | 0.0267
Epoch 74/300, trend Loss: 0.0233 | 0.0266
Epoch 75/300, trend Loss: 0.0232 | 0.0266
Epoch 76/300, trend Loss: 0.0231 | 0.0266
Epoch 77/300, trend Loss: 0.0231 | 0.0265
Epoch 78/300, trend Loss: 0.0230 | 0.0264
Epoch 79/300, trend Loss: 0.0229 | 0.0265
Epoch 80/300, trend Loss: 0.0229 | 0.0264
Epoch 81/300, trend Loss: 0.0228 | 0.0264
Epoch 82/300, trend Loss: 0.0228 | 0.0263
Epoch 83/300, trend Loss: 0.0227 | 0.0264
Epoch 84/300, trend Loss: 0.0227 | 0.0263
Epoch 85/300, trend Loss: 0.0227 | 0.0263
Epoch 86/300, trend Loss: 0.0226 | 0.0263
Epoch 87/300, trend Loss: 0.0226 | 0.0263
Epoch 88/300, trend Loss: 0.0226 | 0.0262
Epoch 89/300, trend Loss: 0.0226 | 0.0262
Epoch 90/300, trend Loss: 0.0225 | 0.0262
Epoch 91/300, trend Loss: 0.0225 | 0.0262
Epoch 92/300, trend Loss: 0.0225 | 0.0262
Epoch 93/300, trend Loss: 0.0225 | 0.0262
Epoch 94/300, trend Loss: 0.0225 | 0.0262
Epoch 95/300, trend Loss: 0.0225 | 0.0262
Epoch 96/300, trend Loss: 0.0225 | 0.0262
Epoch 97/300, trend Loss: 0.0225 | 0.0262
Epoch 98/300, trend Loss: 0.0225 | 0.0263
Epoch 99/300, trend Loss: 0.0225 | 0.0263
Epoch 100/300, trend Loss: 0.0225 | 0.0263
Epoch 101/300, trend Loss: 0.0225 | 0.0262
Epoch 102/300, trend Loss: 0.0225 | 0.0263
Epoch 103/300, trend Loss: 0.0224 | 0.0262
Epoch 104/300, trend Loss: 0.0224 | 0.0262
Epoch 105/300, trend Loss: 0.0224 | 0.0261
Epoch 106/300, trend Loss: 0.0224 | 0.0261
Epoch 107/300, trend Loss: 0.0223 | 0.0261
Epoch 108/300, trend Loss: 0.0223 | 0.0261
Epoch 109/300, trend Loss: 0.0223 | 0.0261
Epoch 110/300, trend Loss: 0.0223 | 0.0260
Epoch 111/300, trend Loss: 0.0223 | 0.0260
Epoch 112/300, trend Loss: 0.0223 | 0.0260
Epoch 113/300, trend Loss: 0.0222 | 0.0260
Epoch 114/300, trend Loss: 0.0222 | 0.0260
Epoch 115/300, trend Loss: 0.0222 | 0.0260
Epoch 116/300, trend Loss: 0.0222 | 0.0260
Epoch 117/300, trend Loss: 0.0222 | 0.0259
Epoch 118/300, trend Loss: 0.0222 | 0.0259
Epoch 119/300, trend Loss: 0.0222 | 0.0259
Epoch 120/300, trend Loss: 0.0222 | 0.0259
Epoch 121/300, trend Loss: 0.0221 | 0.0259
Epoch 122/300, trend Loss: 0.0221 | 0.0259
Epoch 123/300, trend Loss: 0.0221 | 0.0259
Epoch 124/300, trend Loss: 0.0221 | 0.0259
Epoch 125/300, trend Loss: 0.0221 | 0.0259
Epoch 126/300, trend Loss: 0.0221 | 0.0259
Epoch 127/300, trend Loss: 0.0221 | 0.0259
Epoch 128/300, trend Loss: 0.0221 | 0.0259
Epoch 129/300, trend Loss: 0.0221 | 0.0259
Epoch 130/300, trend Loss: 0.0221 | 0.0259
Epoch 131/300, trend Loss: 0.0221 | 0.0259
Epoch 132/300, trend Loss: 0.0221 | 0.0259
Epoch 133/300, trend Loss: 0.0220 | 0.0259
Epoch 134/300, trend Loss: 0.0220 | 0.0258
Epoch 135/300, trend Loss: 0.0220 | 0.0258
Epoch 136/300, trend Loss: 0.0220 | 0.0258
Epoch 137/300, trend Loss: 0.0220 | 0.0258
Epoch 138/300, trend Loss: 0.0220 | 0.0258
Epoch 139/300, trend Loss: 0.0220 | 0.0258
Epoch 140/300, trend Loss: 0.0220 | 0.0258
Epoch 141/300, trend Loss: 0.0220 | 0.0258
Epoch 142/300, trend Loss: 0.0220 | 0.0258
Epoch 143/300, trend Loss: 0.0220 | 0.0258
Epoch 144/300, trend Loss: 0.0220 | 0.0258
Epoch 145/300, trend Loss: 0.0220 | 0.0258
Epoch 146/300, trend Loss: 0.0220 | 0.0258
Epoch 147/300, trend Loss: 0.0220 | 0.0258
Epoch 148/300, trend Loss: 0.0220 | 0.0258
Epoch 149/300, trend Loss: 0.0220 | 0.0258
Epoch 150/300, trend Loss: 0.0220 | 0.0258
Epoch 151/300, trend Loss: 0.0220 | 0.0258
Epoch 152/300, trend Loss: 0.0220 | 0.0258
Epoch 153/300, trend Loss: 0.0220 | 0.0258
Epoch 154/300, trend Loss: 0.0220 | 0.0258
Epoch 155/300, trend Loss: 0.0220 | 0.0258
Epoch 156/300, trend Loss: 0.0220 | 0.0258
Epoch 157/300, trend Loss: 0.0220 | 0.0258
Epoch 158/300, trend Loss: 0.0220 | 0.0258
Epoch 159/300, trend Loss: 0.0220 | 0.0258
Epoch 160/300, trend Loss: 0.0220 | 0.0258
Epoch 161/300, trend Loss: 0.0220 | 0.0258
Epoch 162/300, trend Loss: 0.0220 | 0.0258
Epoch 163/300, trend Loss: 0.0220 | 0.0258
Epoch 164/300, trend Loss: 0.0220 | 0.0258
Epoch 165/300, trend Loss: 0.0220 | 0.0258
Epoch 166/300, trend Loss: 0.0220 | 0.0258
Epoch 167/300, trend Loss: 0.0220 | 0.0258
Epoch 168/300, trend Loss: 0.0220 | 0.0258
Epoch 169/300, trend Loss: 0.0220 | 0.0258
Epoch 170/300, trend Loss: 0.0220 | 0.0258
Epoch 171/300, trend Loss: 0.0220 | 0.0258
Epoch 172/300, trend Loss: 0.0220 | 0.0258
Epoch 173/300, trend Loss: 0.0220 | 0.0258
Epoch 174/300, trend Loss: 0.0220 | 0.0258
Epoch 175/300, trend Loss: 0.0220 | 0.0258
Epoch 176/300, trend Loss: 0.0220 | 0.0258
Epoch 177/300, trend Loss: 0.0220 | 0.0258
Epoch 178/300, trend Loss: 0.0220 | 0.0258
Epoch 179/300, trend Loss: 0.0220 | 0.0258
Epoch 180/300, trend Loss: 0.0220 | 0.0258
Epoch 181/300, trend Loss: 0.0220 | 0.0258
Epoch 182/300, trend Loss: 0.0220 | 0.0258
Epoch 183/300, trend Loss: 0.0220 | 0.0258
Epoch 184/300, trend Loss: 0.0220 | 0.0258
Epoch 185/300, trend Loss: 0.0220 | 0.0258
Epoch 186/300, trend Loss: 0.0220 | 0.0258
Epoch 187/300, trend Loss: 0.0220 | 0.0258
Epoch 188/300, trend Loss: 0.0220 | 0.0258
Epoch 189/300, trend Loss: 0.0220 | 0.0258
Epoch 190/300, trend Loss: 0.0220 | 0.0258
Epoch 191/300, trend Loss: 0.0220 | 0.0258
Epoch 192/300, trend Loss: 0.0220 | 0.0258
Epoch 193/300, trend Loss: 0.0220 | 0.0258
Epoch 194/300, trend Loss: 0.0220 | 0.0258
Epoch 195/300, trend Loss: 0.0220 | 0.0258
Epoch 196/300, trend Loss: 0.0220 | 0.0258
Epoch 197/300, trend Loss: 0.0220 | 0.0258
Epoch 198/300, trend Loss: 0.0220 | 0.0258
Epoch 199/300, trend Loss: 0.0220 | 0.0258
Epoch 200/300, trend Loss: 0.0220 | 0.0258
Epoch 201/300, trend Loss: 0.0220 | 0.0258
Epoch 202/300, trend Loss: 0.0220 | 0.0258
Epoch 203/300, trend Loss: 0.0220 | 0.0258
Epoch 204/300, trend Loss: 0.0220 | 0.0258
Epoch 205/300, trend Loss: 0.0220 | 0.0258
Epoch 206/300, trend Loss: 0.0220 | 0.0258
Epoch 207/300, trend Loss: 0.0220 | 0.0258
Epoch 208/300, trend Loss: 0.0220 | 0.0258
Epoch 209/300, trend Loss: 0.0220 | 0.0258
Epoch 210/300, trend Loss: 0.0220 | 0.0258
Epoch 211/300, trend Loss: 0.0220 | 0.0258
Epoch 212/300, trend Loss: 0.0220 | 0.0258
Epoch 213/300, trend Loss: 0.0220 | 0.0258
Epoch 214/300, trend Loss: 0.0220 | 0.0258
Epoch 215/300, trend Loss: 0.0220 | 0.0258
Epoch 216/300, trend Loss: 0.0220 | 0.0258
Epoch 217/300, trend Loss: 0.0220 | 0.0258
Epoch 218/300, trend Loss: 0.0220 | 0.0258
Epoch 219/300, trend Loss: 0.0220 | 0.0258
Epoch 220/300, trend Loss: 0.0220 | 0.0258
Epoch 221/300, trend Loss: 0.0219 | 0.0258
Epoch 222/300, trend Loss: 0.0219 | 0.0258
Epoch 223/300, trend Loss: 0.0219 | 0.0258
Epoch 224/300, trend Loss: 0.0219 | 0.0258
Epoch 225/300, trend Loss: 0.0219 | 0.0258
Epoch 226/300, trend Loss: 0.0219 | 0.0258
Epoch 227/300, trend Loss: 0.0219 | 0.0258
Epoch 228/300, trend Loss: 0.0219 | 0.0258
Epoch 229/300, trend Loss: 0.0219 | 0.0258
Epoch 230/300, trend Loss: 0.0219 | 0.0258
Epoch 231/300, trend Loss: 0.0219 | 0.0258
Epoch 232/300, trend Loss: 0.0219 | 0.0258
Epoch 233/300, trend Loss: 0.0219 | 0.0258
Epoch 234/300, trend Loss: 0.0219 | 0.0258
Epoch 235/300, trend Loss: 0.0219 | 0.0258
Epoch 236/300, trend Loss: 0.0219 | 0.0258
Epoch 237/300, trend Loss: 0.0219 | 0.0258
Epoch 238/300, trend Loss: 0.0219 | 0.0258
Epoch 239/300, trend Loss: 0.0219 | 0.0258
Epoch 240/300, trend Loss: 0.0219 | 0.0258
Epoch 241/300, trend Loss: 0.0219 | 0.0258
Epoch 242/300, trend Loss: 0.0219 | 0.0258
Epoch 243/300, trend Loss: 0.0219 | 0.0258
Epoch 244/300, trend Loss: 0.0219 | 0.0258
Epoch 245/300, trend Loss: 0.0219 | 0.0258
Epoch 246/300, trend Loss: 0.0219 | 0.0258
Epoch 247/300, trend Loss: 0.0219 | 0.0258
Epoch 248/300, trend Loss: 0.0219 | 0.0258
Epoch 249/300, trend Loss: 0.0219 | 0.0258
Epoch 250/300, trend Loss: 0.0219 | 0.0258
Epoch 251/300, trend Loss: 0.0219 | 0.0258
Epoch 252/300, trend Loss: 0.0219 | 0.0258
Epoch 253/300, trend Loss: 0.0219 | 0.0258
Epoch 254/300, trend Loss: 0.0219 | 0.0258
Epoch 255/300, trend Loss: 0.0219 | 0.0258
Epoch 256/300, trend Loss: 0.0219 | 0.0258
Epoch 257/300, trend Loss: 0.0219 | 0.0258
Epoch 258/300, trend Loss: 0.0219 | 0.0258
Epoch 259/300, trend Loss: 0.0219 | 0.0258
Epoch 260/300, trend Loss: 0.0219 | 0.0258
Epoch 261/300, trend Loss: 0.0219 | 0.0258
Epoch 262/300, trend Loss: 0.0219 | 0.0258
Epoch 263/300, trend Loss: 0.0219 | 0.0258
Epoch 264/300, trend Loss: 0.0219 | 0.0258
Epoch 265/300, trend Loss: 0.0219 | 0.0258
Epoch 266/300, trend Loss: 0.0219 | 0.0258
Epoch 267/300, trend Loss: 0.0219 | 0.0258
Epoch 268/300, trend Loss: 0.0219 | 0.0258
Epoch 269/300, trend Loss: 0.0219 | 0.0258
Epoch 270/300, trend Loss: 0.0219 | 0.0258
Epoch 271/300, trend Loss: 0.0219 | 0.0258
Epoch 272/300, trend Loss: 0.0219 | 0.0258
Epoch 273/300, trend Loss: 0.0219 | 0.0258
Epoch 274/300, trend Loss: 0.0219 | 0.0258
Epoch 275/300, trend Loss: 0.0219 | 0.0258
Epoch 276/300, trend Loss: 0.0219 | 0.0258
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 9, 'train_rates': 0.9787259874347463, 'learning_rate': 0.000957277784736993, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8198445687159657}
Epoch 1/300, seasonal_0 Loss: 0.3371 | 0.1217
Epoch 2/300, seasonal_0 Loss: 0.1105 | 0.0962
Epoch 3/300, seasonal_0 Loss: 0.0969 | 0.0818
Epoch 4/300, seasonal_0 Loss: 0.0853 | 0.0782
Epoch 5/300, seasonal_0 Loss: 0.0889 | 0.0958
Epoch 6/300, seasonal_0 Loss: 0.0881 | 0.0884
Epoch 7/300, seasonal_0 Loss: 0.0897 | 0.0883
Epoch 8/300, seasonal_0 Loss: 0.0799 | 0.0855
Epoch 9/300, seasonal_0 Loss: 0.0728 | 0.0861
Epoch 10/300, seasonal_0 Loss: 0.0685 | 0.0828
Epoch 11/300, seasonal_0 Loss: 0.0633 | 0.0717
Epoch 12/300, seasonal_0 Loss: 0.0624 | 0.0696
Epoch 13/300, seasonal_0 Loss: 0.0634 | 0.0657
Epoch 14/300, seasonal_0 Loss: 0.0688 | 0.0625
Epoch 15/300, seasonal_0 Loss: 0.0823 | 0.1048
Epoch 16/300, seasonal_0 Loss: 0.0940 | 0.0768
Epoch 17/300, seasonal_0 Loss: 0.0872 | 0.2688
Epoch 18/300, seasonal_0 Loss: 0.0808 | 0.0612
Epoch 19/300, seasonal_0 Loss: 0.0618 | 0.0617
Epoch 20/300, seasonal_0 Loss: 0.0609 | 0.0536
Epoch 21/300, seasonal_0 Loss: 0.0523 | 0.0531
Epoch 22/300, seasonal_0 Loss: 0.0507 | 0.0538
Epoch 23/300, seasonal_0 Loss: 0.0506 | 0.0538
Epoch 24/300, seasonal_0 Loss: 0.0497 | 0.0525
Epoch 25/300, seasonal_0 Loss: 0.0483 | 0.0517
Epoch 26/300, seasonal_0 Loss: 0.0475 | 0.0510
Epoch 27/300, seasonal_0 Loss: 0.0470 | 0.0505
Epoch 28/300, seasonal_0 Loss: 0.0467 | 0.0500
Epoch 29/300, seasonal_0 Loss: 0.0464 | 0.0496
Epoch 30/300, seasonal_0 Loss: 0.0461 | 0.0493
Epoch 31/300, seasonal_0 Loss: 0.0459 | 0.0489
Epoch 32/300, seasonal_0 Loss: 0.0457 | 0.0487
Epoch 33/300, seasonal_0 Loss: 0.0457 | 0.0483
Epoch 34/300, seasonal_0 Loss: 0.0457 | 0.0481
Epoch 35/300, seasonal_0 Loss: 0.0457 | 0.0479
Epoch 36/300, seasonal_0 Loss: 0.0456 | 0.0477
Epoch 37/300, seasonal_0 Loss: 0.0454 | 0.0476
Epoch 38/300, seasonal_0 Loss: 0.0451 | 0.0475
Epoch 39/300, seasonal_0 Loss: 0.0448 | 0.0475
Epoch 40/300, seasonal_0 Loss: 0.0446 | 0.0475
Epoch 41/300, seasonal_0 Loss: 0.0445 | 0.0474
Epoch 42/300, seasonal_0 Loss: 0.0444 | 0.0474
Epoch 43/300, seasonal_0 Loss: 0.0443 | 0.0474
Epoch 44/300, seasonal_0 Loss: 0.0442 | 0.0473
Epoch 45/300, seasonal_0 Loss: 0.0442 | 0.0473
Epoch 46/300, seasonal_0 Loss: 0.0441 | 0.0472
Epoch 47/300, seasonal_0 Loss: 0.0441 | 0.0472
Epoch 48/300, seasonal_0 Loss: 0.0440 | 0.0472
Epoch 49/300, seasonal_0 Loss: 0.0440 | 0.0472
Epoch 50/300, seasonal_0 Loss: 0.0439 | 0.0471
Epoch 51/300, seasonal_0 Loss: 0.0439 | 0.0471
Epoch 52/300, seasonal_0 Loss: 0.0439 | 0.0471
Epoch 53/300, seasonal_0 Loss: 0.0439 | 0.0471
Epoch 54/300, seasonal_0 Loss: 0.0438 | 0.0470
Epoch 55/300, seasonal_0 Loss: 0.0438 | 0.0470
Epoch 56/300, seasonal_0 Loss: 0.0438 | 0.0470
Epoch 57/300, seasonal_0 Loss: 0.0438 | 0.0470
Epoch 58/300, seasonal_0 Loss: 0.0438 | 0.0470
Epoch 59/300, seasonal_0 Loss: 0.0437 | 0.0470
Epoch 60/300, seasonal_0 Loss: 0.0437 | 0.0470
Epoch 61/300, seasonal_0 Loss: 0.0437 | 0.0470
Epoch 62/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 63/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 64/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 65/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 66/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 67/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 68/300, seasonal_0 Loss: 0.0437 | 0.0469
Epoch 69/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 70/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 71/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 72/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 73/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 74/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 75/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 76/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 77/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 78/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 79/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 80/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 81/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 82/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 83/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 84/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 85/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 86/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 87/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 88/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 89/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 90/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 91/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 92/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 93/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 94/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 95/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 96/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 97/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 98/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 99/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 100/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 101/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 102/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 103/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 104/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 105/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 106/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 107/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 108/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 109/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 110/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 111/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 112/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 113/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 114/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 115/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 116/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 117/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 118/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 119/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 120/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 121/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 122/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 123/300, seasonal_0 Loss: 0.0436 | 0.0469
Epoch 124/300, seasonal_0 Loss: 0.0436 | 0.0469
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 35, 'train_rates': 0.9876759182467544, 'learning_rate': 0.00033532080877648107, 'batch_size': 33, 'step_size': 9, 'gamma': 0.763782920865035}
Epoch 1/300, seasonal_1 Loss: 0.1804 | 0.1338
Epoch 2/300, seasonal_1 Loss: 0.1014 | 0.1235
Epoch 3/300, seasonal_1 Loss: 0.0898 | 0.0953
Epoch 4/300, seasonal_1 Loss: 0.0828 | 0.1075
Epoch 5/300, seasonal_1 Loss: 0.0694 | 0.0921
Epoch 6/300, seasonal_1 Loss: 0.0665 | 0.0806
Epoch 7/300, seasonal_1 Loss: 0.0623 | 0.0733
Epoch 8/300, seasonal_1 Loss: 0.0617 | 0.0680
Epoch 9/300, seasonal_1 Loss: 0.0594 | 0.0733
Epoch 10/300, seasonal_1 Loss: 0.0534 | 0.0533
Epoch 11/300, seasonal_1 Loss: 0.0506 | 0.0515
Epoch 12/300, seasonal_1 Loss: 0.0475 | 0.0506
Epoch 13/300, seasonal_1 Loss: 0.0455 | 0.0502
Epoch 14/300, seasonal_1 Loss: 0.0438 | 0.0500
Epoch 15/300, seasonal_1 Loss: 0.0423 | 0.0524
Epoch 16/300, seasonal_1 Loss: 0.0424 | 0.0516
Epoch 17/300, seasonal_1 Loss: 0.0396 | 0.0530
Epoch 18/300, seasonal_1 Loss: 0.0374 | 0.0510
Epoch 19/300, seasonal_1 Loss: 0.0362 | 0.0485
Epoch 20/300, seasonal_1 Loss: 0.0367 | 0.0484
Epoch 21/300, seasonal_1 Loss: 0.0381 | 0.0488
Epoch 22/300, seasonal_1 Loss: 0.0396 | 0.0491
Epoch 23/300, seasonal_1 Loss: 0.0347 | 0.0476
Epoch 24/300, seasonal_1 Loss: 0.0336 | 0.0428
Epoch 25/300, seasonal_1 Loss: 0.0326 | 0.0418
Epoch 26/300, seasonal_1 Loss: 0.0322 | 0.0412
Epoch 27/300, seasonal_1 Loss: 0.0319 | 0.0403
Epoch 28/300, seasonal_1 Loss: 0.0323 | 0.0491
Epoch 29/300, seasonal_1 Loss: 0.0317 | 0.0479
Epoch 30/300, seasonal_1 Loss: 0.0308 | 0.0443
Epoch 31/300, seasonal_1 Loss: 0.0301 | 0.0415
Epoch 32/300, seasonal_1 Loss: 0.0294 | 0.0399
Epoch 33/300, seasonal_1 Loss: 0.0291 | 0.0369
Epoch 34/300, seasonal_1 Loss: 0.0288 | 0.0365
Epoch 35/300, seasonal_1 Loss: 0.0284 | 0.0364
Epoch 36/300, seasonal_1 Loss: 0.0281 | 0.0361
Epoch 37/300, seasonal_1 Loss: 0.0280 | 0.0370
Epoch 38/300, seasonal_1 Loss: 0.0280 | 0.0363
Epoch 39/300, seasonal_1 Loss: 0.0277 | 0.0360
Epoch 40/300, seasonal_1 Loss: 0.0275 | 0.0352
Epoch 41/300, seasonal_1 Loss: 0.0273 | 0.0343
Epoch 42/300, seasonal_1 Loss: 0.0272 | 0.0334
Epoch 43/300, seasonal_1 Loss: 0.0275 | 0.0351
Epoch 44/300, seasonal_1 Loss: 0.0273 | 0.0328
Epoch 45/300, seasonal_1 Loss: 0.0279 | 0.0388
Epoch 46/300, seasonal_1 Loss: 0.0278 | 0.0454
Epoch 47/300, seasonal_1 Loss: 0.0282 | 0.0640
Epoch 48/300, seasonal_1 Loss: 0.0276 | 0.0610
Epoch 49/300, seasonal_1 Loss: 0.0269 | 0.0605
Epoch 50/300, seasonal_1 Loss: 0.0263 | 0.0560
Epoch 51/300, seasonal_1 Loss: 0.0258 | 0.0575
Epoch 52/300, seasonal_1 Loss: 0.0255 | 0.0513
Epoch 53/300, seasonal_1 Loss: 0.0248 | 0.0452
Epoch 54/300, seasonal_1 Loss: 0.0243 | 0.0420
Epoch 55/300, seasonal_1 Loss: 0.0241 | 0.0375
Epoch 56/300, seasonal_1 Loss: 0.0239 | 0.0360
Epoch 57/300, seasonal_1 Loss: 0.0236 | 0.0341
Epoch 58/300, seasonal_1 Loss: 0.0234 | 0.0330
Epoch 59/300, seasonal_1 Loss: 0.0232 | 0.0322
Epoch 60/300, seasonal_1 Loss: 0.0231 | 0.0296
Epoch 61/300, seasonal_1 Loss: 0.0230 | 0.0295
Epoch 62/300, seasonal_1 Loss: 0.0228 | 0.0292
Epoch 63/300, seasonal_1 Loss: 0.0226 | 0.0290
Epoch 64/300, seasonal_1 Loss: 0.0226 | 0.0276
Epoch 65/300, seasonal_1 Loss: 0.0225 | 0.0277
Epoch 66/300, seasonal_1 Loss: 0.0223 | 0.0277
Epoch 67/300, seasonal_1 Loss: 0.0222 | 0.0276
Epoch 68/300, seasonal_1 Loss: 0.0221 | 0.0276
Epoch 69/300, seasonal_1 Loss: 0.0220 | 0.0268
Epoch 70/300, seasonal_1 Loss: 0.0220 | 0.0269
Epoch 71/300, seasonal_1 Loss: 0.0219 | 0.0269
Epoch 72/300, seasonal_1 Loss: 0.0218 | 0.0269
Epoch 73/300, seasonal_1 Loss: 0.0217 | 0.0264
Epoch 74/300, seasonal_1 Loss: 0.0216 | 0.0264
Epoch 75/300, seasonal_1 Loss: 0.0215 | 0.0265
Epoch 76/300, seasonal_1 Loss: 0.0214 | 0.0265
Epoch 77/300, seasonal_1 Loss: 0.0214 | 0.0265
Epoch 78/300, seasonal_1 Loss: 0.0213 | 0.0262
Epoch 79/300, seasonal_1 Loss: 0.0212 | 0.0262
Epoch 80/300, seasonal_1 Loss: 0.0212 | 0.0262
Epoch 81/300, seasonal_1 Loss: 0.0211 | 0.0262
Epoch 82/300, seasonal_1 Loss: 0.0211 | 0.0261
Epoch 83/300, seasonal_1 Loss: 0.0210 | 0.0261
Epoch 84/300, seasonal_1 Loss: 0.0210 | 0.0261
Epoch 85/300, seasonal_1 Loss: 0.0210 | 0.0261
Epoch 86/300, seasonal_1 Loss: 0.0209 | 0.0261
Epoch 87/300, seasonal_1 Loss: 0.0209 | 0.0261
Epoch 88/300, seasonal_1 Loss: 0.0208 | 0.0260
Epoch 89/300, seasonal_1 Loss: 0.0208 | 0.0260
Epoch 90/300, seasonal_1 Loss: 0.0208 | 0.0260
Epoch 91/300, seasonal_1 Loss: 0.0207 | 0.0260
Epoch 92/300, seasonal_1 Loss: 0.0207 | 0.0260
Epoch 93/300, seasonal_1 Loss: 0.0207 | 0.0260
Epoch 94/300, seasonal_1 Loss: 0.0207 | 0.0260
Epoch 95/300, seasonal_1 Loss: 0.0207 | 0.0260
Epoch 96/300, seasonal_1 Loss: 0.0206 | 0.0260
Epoch 97/300, seasonal_1 Loss: 0.0206 | 0.0259
Epoch 98/300, seasonal_1 Loss: 0.0206 | 0.0259
Epoch 99/300, seasonal_1 Loss: 0.0206 | 0.0259
Epoch 100/300, seasonal_1 Loss: 0.0206 | 0.0259
Epoch 101/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 102/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 103/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 104/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 105/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 106/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 107/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 108/300, seasonal_1 Loss: 0.0205 | 0.0259
Epoch 109/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 110/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 111/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 112/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 113/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 114/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 115/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 116/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 117/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 118/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 119/300, seasonal_1 Loss: 0.0204 | 0.0259
Epoch 120/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 121/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 122/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 123/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 124/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 125/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 126/300, seasonal_1 Loss: 0.0204 | 0.0258
Epoch 127/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 128/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 129/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 130/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 131/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 132/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 133/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 134/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 135/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 136/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 137/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 138/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 139/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 140/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 141/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 142/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 143/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 144/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 145/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 146/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 147/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 148/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 149/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 150/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 151/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 152/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 153/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 154/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 155/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 156/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 157/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 158/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 159/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 160/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 161/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 162/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 163/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 164/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 165/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 166/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 167/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 168/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 169/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 170/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 171/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 172/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 173/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 174/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 175/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 176/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 177/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 178/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 179/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 180/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 181/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 182/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 183/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 184/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 185/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 186/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 187/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 188/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 189/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 190/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 191/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 192/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 193/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 194/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 195/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 196/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 197/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 198/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 199/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 200/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 201/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 202/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 203/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 204/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 205/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 206/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 207/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 208/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 209/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 210/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 211/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 212/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 213/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 214/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 215/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 216/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 217/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 218/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 219/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 220/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 221/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 222/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 223/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 224/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 225/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 226/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 227/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 228/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 229/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 230/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 231/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 232/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 233/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 234/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 235/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 236/300, seasonal_1 Loss: 0.0203 | 0.0258
Epoch 237/300, seasonal_1 Loss: 0.0203 | 0.0258
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9862233049904742, 'learning_rate': 2.0120657236148504e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9860150154569425}
Epoch 1/300, seasonal_2 Loss: 0.1935 | 0.1447
Epoch 2/300, seasonal_2 Loss: 0.1070 | 0.0987
Epoch 3/300, seasonal_2 Loss: 0.0915 | 0.0794
Epoch 4/300, seasonal_2 Loss: 0.0827 | 0.0696
Epoch 5/300, seasonal_2 Loss: 0.0770 | 0.0651
Epoch 6/300, seasonal_2 Loss: 0.0735 | 0.0631
Epoch 7/300, seasonal_2 Loss: 0.0711 | 0.0622
Epoch 8/300, seasonal_2 Loss: 0.0692 | 0.0616
Epoch 9/300, seasonal_2 Loss: 0.0674 | 0.0611
Epoch 10/300, seasonal_2 Loss: 0.0659 | 0.0606
Epoch 11/300, seasonal_2 Loss: 0.0644 | 0.0601
Epoch 12/300, seasonal_2 Loss: 0.0630 | 0.0595
Epoch 13/300, seasonal_2 Loss: 0.0615 | 0.0589
Epoch 14/300, seasonal_2 Loss: 0.0601 | 0.0582
Epoch 15/300, seasonal_2 Loss: 0.0586 | 0.0574
Epoch 16/300, seasonal_2 Loss: 0.0571 | 0.0565
Epoch 17/300, seasonal_2 Loss: 0.0556 | 0.0555
Epoch 18/300, seasonal_2 Loss: 0.0542 | 0.0546
Epoch 19/300, seasonal_2 Loss: 0.0529 | 0.0539
Epoch 20/300, seasonal_2 Loss: 0.0518 | 0.0532
Epoch 21/300, seasonal_2 Loss: 0.0507 | 0.0525
Epoch 22/300, seasonal_2 Loss: 0.0497 | 0.0518
Epoch 23/300, seasonal_2 Loss: 0.0487 | 0.0510
Epoch 24/300, seasonal_2 Loss: 0.0477 | 0.0502
Epoch 25/300, seasonal_2 Loss: 0.0468 | 0.0493
Epoch 26/300, seasonal_2 Loss: 0.0459 | 0.0484
Epoch 27/300, seasonal_2 Loss: 0.0451 | 0.0475
Epoch 28/300, seasonal_2 Loss: 0.0444 | 0.0468
Epoch 29/300, seasonal_2 Loss: 0.0437 | 0.0461
Epoch 30/300, seasonal_2 Loss: 0.0431 | 0.0456
Epoch 31/300, seasonal_2 Loss: 0.0426 | 0.0451
Epoch 32/300, seasonal_2 Loss: 0.0420 | 0.0447
Epoch 33/300, seasonal_2 Loss: 0.0415 | 0.0443
Epoch 34/300, seasonal_2 Loss: 0.0409 | 0.0440
Epoch 35/300, seasonal_2 Loss: 0.0404 | 0.0436
Epoch 36/300, seasonal_2 Loss: 0.0398 | 0.0433
Epoch 37/300, seasonal_2 Loss: 0.0392 | 0.0429
Epoch 38/300, seasonal_2 Loss: 0.0386 | 0.0424
Epoch 39/300, seasonal_2 Loss: 0.0379 | 0.0419
Epoch 40/300, seasonal_2 Loss: 0.0374 | 0.0414
Epoch 41/300, seasonal_2 Loss: 0.0369 | 0.0408
Epoch 42/300, seasonal_2 Loss: 0.0365 | 0.0404
Epoch 43/300, seasonal_2 Loss: 0.0361 | 0.0399
Epoch 44/300, seasonal_2 Loss: 0.0358 | 0.0394
Epoch 45/300, seasonal_2 Loss: 0.0355 | 0.0390
Epoch 46/300, seasonal_2 Loss: 0.0352 | 0.0386
Epoch 47/300, seasonal_2 Loss: 0.0349 | 0.0383
Epoch 48/300, seasonal_2 Loss: 0.0346 | 0.0379
Epoch 49/300, seasonal_2 Loss: 0.0344 | 0.0376
Epoch 50/300, seasonal_2 Loss: 0.0342 | 0.0372
Epoch 51/300, seasonal_2 Loss: 0.0340 | 0.0369
Epoch 52/300, seasonal_2 Loss: 0.0337 | 0.0366
Epoch 53/300, seasonal_2 Loss: 0.0335 | 0.0363
Epoch 54/300, seasonal_2 Loss: 0.0333 | 0.0360
Epoch 55/300, seasonal_2 Loss: 0.0331 | 0.0357
Epoch 56/300, seasonal_2 Loss: 0.0330 | 0.0355
Epoch 57/300, seasonal_2 Loss: 0.0328 | 0.0352
Epoch 58/300, seasonal_2 Loss: 0.0326 | 0.0349
Epoch 59/300, seasonal_2 Loss: 0.0324 | 0.0347
Epoch 60/300, seasonal_2 Loss: 0.0323 | 0.0344
Epoch 61/300, seasonal_2 Loss: 0.0321 | 0.0341
Epoch 62/300, seasonal_2 Loss: 0.0320 | 0.0339
Epoch 63/300, seasonal_2 Loss: 0.0318 | 0.0337
Epoch 64/300, seasonal_2 Loss: 0.0317 | 0.0334
Epoch 65/300, seasonal_2 Loss: 0.0315 | 0.0331
Epoch 66/300, seasonal_2 Loss: 0.0314 | 0.0329
Epoch 67/300, seasonal_2 Loss: 0.0313 | 0.0326
Epoch 68/300, seasonal_2 Loss: 0.0312 | 0.0324
Epoch 69/300, seasonal_2 Loss: 0.0310 | 0.0322
Epoch 70/300, seasonal_2 Loss: 0.0309 | 0.0319
Epoch 71/300, seasonal_2 Loss: 0.0308 | 0.0317
Epoch 72/300, seasonal_2 Loss: 0.0307 | 0.0315
Epoch 73/300, seasonal_2 Loss: 0.0305 | 0.0313
Epoch 74/300, seasonal_2 Loss: 0.0304 | 0.0312
Epoch 75/300, seasonal_2 Loss: 0.0303 | 0.0310
Epoch 76/300, seasonal_2 Loss: 0.0302 | 0.0309
Epoch 77/300, seasonal_2 Loss: 0.0301 | 0.0308
Epoch 78/300, seasonal_2 Loss: 0.0300 | 0.0306
Epoch 79/300, seasonal_2 Loss: 0.0299 | 0.0305
Epoch 80/300, seasonal_2 Loss: 0.0298 | 0.0304
Epoch 81/300, seasonal_2 Loss: 0.0297 | 0.0302
Epoch 82/300, seasonal_2 Loss: 0.0296 | 0.0301
Epoch 83/300, seasonal_2 Loss: 0.0295 | 0.0300
Epoch 84/300, seasonal_2 Loss: 0.0294 | 0.0299
Epoch 85/300, seasonal_2 Loss: 0.0293 | 0.0298
Epoch 86/300, seasonal_2 Loss: 0.0292 | 0.0297
Epoch 87/300, seasonal_2 Loss: 0.0292 | 0.0295
Epoch 88/300, seasonal_2 Loss: 0.0291 | 0.0294
Epoch 89/300, seasonal_2 Loss: 0.0290 | 0.0293
Epoch 90/300, seasonal_2 Loss: 0.0289 | 0.0292
Epoch 91/300, seasonal_2 Loss: 0.0289 | 0.0291
Epoch 92/300, seasonal_2 Loss: 0.0288 | 0.0290
Epoch 93/300, seasonal_2 Loss: 0.0287 | 0.0289
Epoch 94/300, seasonal_2 Loss: 0.0286 | 0.0288
Epoch 95/300, seasonal_2 Loss: 0.0286 | 0.0286
Epoch 96/300, seasonal_2 Loss: 0.0285 | 0.0285
Epoch 97/300, seasonal_2 Loss: 0.0284 | 0.0284
Epoch 98/300, seasonal_2 Loss: 0.0284 | 0.0283
Epoch 99/300, seasonal_2 Loss: 0.0283 | 0.0282
Epoch 100/300, seasonal_2 Loss: 0.0282 | 0.0281
Epoch 101/300, seasonal_2 Loss: 0.0282 | 0.0279
Epoch 102/300, seasonal_2 Loss: 0.0281 | 0.0278
Epoch 103/300, seasonal_2 Loss: 0.0280 | 0.0277
Epoch 104/300, seasonal_2 Loss: 0.0280 | 0.0276
Epoch 105/300, seasonal_2 Loss: 0.0279 | 0.0274
Epoch 106/300, seasonal_2 Loss: 0.0278 | 0.0273
Epoch 107/300, seasonal_2 Loss: 0.0277 | 0.0272
Epoch 108/300, seasonal_2 Loss: 0.0277 | 0.0271
Epoch 109/300, seasonal_2 Loss: 0.0276 | 0.0269
Epoch 110/300, seasonal_2 Loss: 0.0275 | 0.0268
Epoch 111/300, seasonal_2 Loss: 0.0275 | 0.0266
Epoch 112/300, seasonal_2 Loss: 0.0274 | 0.0265
Epoch 113/300, seasonal_2 Loss: 0.0273 | 0.0264
Epoch 114/300, seasonal_2 Loss: 0.0273 | 0.0262
Epoch 115/300, seasonal_2 Loss: 0.0272 | 0.0261
Epoch 116/300, seasonal_2 Loss: 0.0271 | 0.0259
Epoch 117/300, seasonal_2 Loss: 0.0271 | 0.0258
Epoch 118/300, seasonal_2 Loss: 0.0270 | 0.0256
Epoch 119/300, seasonal_2 Loss: 0.0269 | 0.0255
Epoch 120/300, seasonal_2 Loss: 0.0268 | 0.0253
Epoch 121/300, seasonal_2 Loss: 0.0268 | 0.0251
Epoch 122/300, seasonal_2 Loss: 0.0267 | 0.0250
Epoch 123/300, seasonal_2 Loss: 0.0266 | 0.0248
Epoch 124/300, seasonal_2 Loss: 0.0265 | 0.0247
Epoch 125/300, seasonal_2 Loss: 0.0265 | 0.0245
Epoch 126/300, seasonal_2 Loss: 0.0264 | 0.0244
Epoch 127/300, seasonal_2 Loss: 0.0263 | 0.0242
Epoch 128/300, seasonal_2 Loss: 0.0262 | 0.0240
Epoch 129/300, seasonal_2 Loss: 0.0262 | 0.0239
Epoch 130/300, seasonal_2 Loss: 0.0261 | 0.0237
Epoch 131/300, seasonal_2 Loss: 0.0260 | 0.0236
Epoch 132/300, seasonal_2 Loss: 0.0259 | 0.0235
Epoch 133/300, seasonal_2 Loss: 0.0258 | 0.0233
Epoch 134/300, seasonal_2 Loss: 0.0258 | 0.0232
Epoch 135/300, seasonal_2 Loss: 0.0257 | 0.0231
Epoch 136/300, seasonal_2 Loss: 0.0256 | 0.0229
Epoch 137/300, seasonal_2 Loss: 0.0255 | 0.0228
Epoch 138/300, seasonal_2 Loss: 0.0255 | 0.0227
Epoch 139/300, seasonal_2 Loss: 0.0254 | 0.0226
Epoch 140/300, seasonal_2 Loss: 0.0253 | 0.0225
Epoch 141/300, seasonal_2 Loss: 0.0252 | 0.0224
Epoch 142/300, seasonal_2 Loss: 0.0252 | 0.0224
Epoch 143/300, seasonal_2 Loss: 0.0251 | 0.0223
Epoch 144/300, seasonal_2 Loss: 0.0250 | 0.0222
Epoch 145/300, seasonal_2 Loss: 0.0250 | 0.0222
Epoch 146/300, seasonal_2 Loss: 0.0249 | 0.0221
Epoch 147/300, seasonal_2 Loss: 0.0249 | 0.0221
Epoch 148/300, seasonal_2 Loss: 0.0248 | 0.0220
Epoch 149/300, seasonal_2 Loss: 0.0247 | 0.0220
Epoch 150/300, seasonal_2 Loss: 0.0247 | 0.0220
Epoch 151/300, seasonal_2 Loss: 0.0246 | 0.0220
Epoch 152/300, seasonal_2 Loss: 0.0246 | 0.0219
Epoch 153/300, seasonal_2 Loss: 0.0245 | 0.0219
Epoch 154/300, seasonal_2 Loss: 0.0245 | 0.0219
Epoch 155/300, seasonal_2 Loss: 0.0244 | 0.0219
Epoch 156/300, seasonal_2 Loss: 0.0244 | 0.0219
Epoch 157/300, seasonal_2 Loss: 0.0243 | 0.0220
Epoch 158/300, seasonal_2 Loss: 0.0243 | 0.0220
Epoch 159/300, seasonal_2 Loss: 0.0243 | 0.0220
Epoch 160/300, seasonal_2 Loss: 0.0242 | 0.0220
Epoch 161/300, seasonal_2 Loss: 0.0242 | 0.0220
Epoch 162/300, seasonal_2 Loss: 0.0241 | 0.0220
Epoch 163/300, seasonal_2 Loss: 0.0241 | 0.0221
Epoch 164/300, seasonal_2 Loss: 0.0241 | 0.0221
Epoch 165/300, seasonal_2 Loss: 0.0240 | 0.0221
Epoch 166/300, seasonal_2 Loss: 0.0240 | 0.0222
Epoch 167/300, seasonal_2 Loss: 0.0240 | 0.0222
Epoch 168/300, seasonal_2 Loss: 0.0239 | 0.0223
Epoch 169/300, seasonal_2 Loss: 0.0239 | 0.0224
Epoch 170/300, seasonal_2 Loss: 0.0239 | 0.0224
Epoch 171/300, seasonal_2 Loss: 0.0238 | 0.0225
Epoch 172/300, seasonal_2 Loss: 0.0238 | 0.0226
Epoch 173/300, seasonal_2 Loss: 0.0238 | 0.0227
Epoch 174/300, seasonal_2 Loss: 0.0237 | 0.0227
Epoch 175/300, seasonal_2 Loss: 0.0237 | 0.0228
Epoch 176/300, seasonal_2 Loss: 0.0237 | 0.0229
Epoch 177/300, seasonal_2 Loss: 0.0236 | 0.0229
Epoch 178/300, seasonal_2 Loss: 0.0236 | 0.0231
Epoch 179/300, seasonal_2 Loss: 0.0235 | 0.0231
Epoch 180/300, seasonal_2 Loss: 0.0235 | 0.0231
Epoch 181/300, seasonal_2 Loss: 0.0235 | 0.0233
Epoch 182/300, seasonal_2 Loss: 0.0234 | 0.0233
Epoch 183/300, seasonal_2 Loss: 0.0234 | 0.0233
Epoch 184/300, seasonal_2 Loss: 0.0234 | 0.0234
Epoch 185/300, seasonal_2 Loss: 0.0233 | 0.0235
Epoch 186/300, seasonal_2 Loss: 0.0233 | 0.0235
Epoch 187/300, seasonal_2 Loss: 0.0233 | 0.0236
Epoch 188/300, seasonal_2 Loss: 0.0232 | 0.0236
Epoch 189/300, seasonal_2 Loss: 0.0232 | 0.0236
Epoch 190/300, seasonal_2 Loss: 0.0232 | 0.0237
Epoch 191/300, seasonal_2 Loss: 0.0231 | 0.0237
Epoch 192/300, seasonal_2 Loss: 0.0231 | 0.0237
Epoch 193/300, seasonal_2 Loss: 0.0231 | 0.0238
Epoch 194/300, seasonal_2 Loss: 0.0230 | 0.0239
Epoch 195/300, seasonal_2 Loss: 0.0230 | 0.0239
Epoch 196/300, seasonal_2 Loss: 0.0230 | 0.0240
Epoch 197/300, seasonal_2 Loss: 0.0229 | 0.0240
Epoch 198/300, seasonal_2 Loss: 0.0229 | 0.0240
Epoch 199/300, seasonal_2 Loss: 0.0229 | 0.0241
Epoch 200/300, seasonal_2 Loss: 0.0229 | 0.0241
Epoch 201/300, seasonal_2 Loss: 0.0228 | 0.0241
Epoch 202/300, seasonal_2 Loss: 0.0228 | 0.0242
Epoch 203/300, seasonal_2 Loss: 0.0228 | 0.0243
Epoch 204/300, seasonal_2 Loss: 0.0227 | 0.0243
Epoch 205/300, seasonal_2 Loss: 0.0227 | 0.0244
Epoch 206/300, seasonal_2 Loss: 0.0227 | 0.0244
Epoch 207/300, seasonal_2 Loss: 0.0227 | 0.0244
Epoch 208/300, seasonal_2 Loss: 0.0226 | 0.0245
Epoch 209/300, seasonal_2 Loss: 0.0226 | 0.0245
Epoch 210/300, seasonal_2 Loss: 0.0226 | 0.0245
Epoch 211/300, seasonal_2 Loss: 0.0226 | 0.0245
Epoch 212/300, seasonal_2 Loss: 0.0225 | 0.0246
Epoch 213/300, seasonal_2 Loss: 0.0225 | 0.0246
Epoch 214/300, seasonal_2 Loss: 0.0225 | 0.0246
Epoch 215/300, seasonal_2 Loss: 0.0225 | 0.0246
Epoch 216/300, seasonal_2 Loss: 0.0224 | 0.0246
Epoch 217/300, seasonal_2 Loss: 0.0224 | 0.0246
Epoch 218/300, seasonal_2 Loss: 0.0224 | 0.0245
Epoch 219/300, seasonal_2 Loss: 0.0224 | 0.0245
Epoch 220/300, seasonal_2 Loss: 0.0224 | 0.0245
Epoch 221/300, seasonal_2 Loss: 0.0223 | 0.0244
Epoch 222/300, seasonal_2 Loss: 0.0223 | 0.0244
Epoch 223/300, seasonal_2 Loss: 0.0223 | 0.0243
Epoch 224/300, seasonal_2 Loss: 0.0223 | 0.0243
Epoch 225/300, seasonal_2 Loss: 0.0223 | 0.0242
Epoch 226/300, seasonal_2 Loss: 0.0222 | 0.0241
Epoch 227/300, seasonal_2 Loss: 0.0222 | 0.0240
Epoch 228/300, seasonal_2 Loss: 0.0222 | 0.0240
Epoch 229/300, seasonal_2 Loss: 0.0222 | 0.0239
Epoch 230/300, seasonal_2 Loss: 0.0222 | 0.0238
Epoch 231/300, seasonal_2 Loss: 0.0222 | 0.0237
Epoch 232/300, seasonal_2 Loss: 0.0221 | 0.0236
Epoch 233/300, seasonal_2 Loss: 0.0221 | 0.0236
Epoch 234/300, seasonal_2 Loss: 0.0221 | 0.0235
Epoch 235/300, seasonal_2 Loss: 0.0221 | 0.0234
Epoch 236/300, seasonal_2 Loss: 0.0221 | 0.0234
Epoch 237/300, seasonal_2 Loss: 0.0220 | 0.0233
Epoch 238/300, seasonal_2 Loss: 0.0220 | 0.0233
Epoch 239/300, seasonal_2 Loss: 0.0220 | 0.0232
Epoch 240/300, seasonal_2 Loss: 0.0220 | 0.0232
Epoch 241/300, seasonal_2 Loss: 0.0220 | 0.0232
Epoch 242/300, seasonal_2 Loss: 0.0219 | 0.0232
Epoch 243/300, seasonal_2 Loss: 0.0219 | 0.0232
Epoch 244/300, seasonal_2 Loss: 0.0219 | 0.0231
Epoch 245/300, seasonal_2 Loss: 0.0219 | 0.0231
Epoch 246/300, seasonal_2 Loss: 0.0219 | 0.0231
Epoch 247/300, seasonal_2 Loss: 0.0218 | 0.0231
Epoch 248/300, seasonal_2 Loss: 0.0218 | 0.0231
Epoch 249/300, seasonal_2 Loss: 0.0218 | 0.0231
Epoch 250/300, seasonal_2 Loss: 0.0218 | 0.0231
Epoch 251/300, seasonal_2 Loss: 0.0217 | 0.0231
Epoch 252/300, seasonal_2 Loss: 0.0217 | 0.0231
Epoch 253/300, seasonal_2 Loss: 0.0217 | 0.0231
Epoch 254/300, seasonal_2 Loss: 0.0217 | 0.0232
Epoch 255/300, seasonal_2 Loss: 0.0217 | 0.0232
Epoch 256/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 257/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 258/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 259/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 260/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 261/300, seasonal_2 Loss: 0.0216 | 0.0232
Epoch 262/300, seasonal_2 Loss: 0.0215 | 0.0232
Epoch 263/300, seasonal_2 Loss: 0.0215 | 0.0232
Epoch 264/300, seasonal_2 Loss: 0.0215 | 0.0232
Epoch 265/300, seasonal_2 Loss: 0.0215 | 0.0232
Epoch 266/300, seasonal_2 Loss: 0.0215 | 0.0232
Epoch 267/300, seasonal_2 Loss: 0.0214 | 0.0232
Epoch 268/300, seasonal_2 Loss: 0.0214 | 0.0231
Epoch 269/300, seasonal_2 Loss: 0.0214 | 0.0231
Epoch 270/300, seasonal_2 Loss: 0.0214 | 0.0231
Epoch 271/300, seasonal_2 Loss: 0.0214 | 0.0230
Epoch 272/300, seasonal_2 Loss: 0.0214 | 0.0230
Epoch 273/300, seasonal_2 Loss: 0.0213 | 0.0230
Epoch 274/300, seasonal_2 Loss: 0.0213 | 0.0229
Epoch 275/300, seasonal_2 Loss: 0.0213 | 0.0229
Epoch 276/300, seasonal_2 Loss: 0.0213 | 0.0228
Epoch 277/300, seasonal_2 Loss: 0.0213 | 0.0228
Epoch 278/300, seasonal_2 Loss: 0.0213 | 0.0227
Epoch 279/300, seasonal_2 Loss: 0.0212 | 0.0227
Epoch 280/300, seasonal_2 Loss: 0.0212 | 0.0226
Epoch 281/300, seasonal_2 Loss: 0.0212 | 0.0226
Epoch 282/300, seasonal_2 Loss: 0.0212 | 0.0226
Epoch 283/300, seasonal_2 Loss: 0.0212 | 0.0225
Epoch 284/300, seasonal_2 Loss: 0.0211 | 0.0225
Epoch 285/300, seasonal_2 Loss: 0.0211 | 0.0225
Epoch 286/300, seasonal_2 Loss: 0.0211 | 0.0224
Epoch 287/300, seasonal_2 Loss: 0.0211 | 0.0224
Epoch 288/300, seasonal_2 Loss: 0.0211 | 0.0224
Epoch 289/300, seasonal_2 Loss: 0.0211 | 0.0224
Epoch 290/300, seasonal_2 Loss: 0.0210 | 0.0224
Epoch 291/300, seasonal_2 Loss: 0.0210 | 0.0223
Epoch 292/300, seasonal_2 Loss: 0.0210 | 0.0223
Epoch 293/300, seasonal_2 Loss: 0.0210 | 0.0223
Epoch 294/300, seasonal_2 Loss: 0.0210 | 0.0223
Epoch 295/300, seasonal_2 Loss: 0.0209 | 0.0223
Epoch 296/300, seasonal_2 Loss: 0.0209 | 0.0223
Epoch 297/300, seasonal_2 Loss: 0.0209 | 0.0223
Epoch 298/300, seasonal_2 Loss: 0.0209 | 0.0223
Epoch 299/300, seasonal_2 Loss: 0.0209 | 0.0223
Epoch 300/300, seasonal_2 Loss: 0.0208 | 0.0223
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.7225756876602079, 'learning_rate': 0.0006481049933478129, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8100464085974847}
Epoch 1/300, seasonal_3 Loss: 0.3447 | 0.3900
Epoch 2/300, seasonal_3 Loss: 0.1287 | 0.2950
Epoch 3/300, seasonal_3 Loss: 0.1062 | 0.2246
Epoch 4/300, seasonal_3 Loss: 0.1003 | 0.2189
Epoch 5/300, seasonal_3 Loss: 0.0946 | 0.2272
Epoch 6/300, seasonal_3 Loss: 0.0919 | 0.2000
Epoch 7/300, seasonal_3 Loss: 0.0918 | 0.1753
Epoch 8/300, seasonal_3 Loss: 0.0966 | 0.1512
Epoch 9/300, seasonal_3 Loss: 0.0880 | 0.1166
Epoch 10/300, seasonal_3 Loss: 0.0788 | 0.1246
Epoch 11/300, seasonal_3 Loss: 0.0716 | 0.1044
Epoch 12/300, seasonal_3 Loss: 0.0692 | 0.1053
Epoch 13/300, seasonal_3 Loss: 0.0648 | 0.1085
Epoch 14/300, seasonal_3 Loss: 0.0610 | 0.1000
Epoch 15/300, seasonal_3 Loss: 0.0590 | 0.0988
Epoch 16/300, seasonal_3 Loss: 0.0557 | 0.1107
Epoch 17/300, seasonal_3 Loss: 0.0514 | 0.1147
Epoch 18/300, seasonal_3 Loss: 0.0481 | 0.1208
Epoch 19/300, seasonal_3 Loss: 0.0457 | 0.1550
Epoch 20/300, seasonal_3 Loss: 0.0447 | 0.1689
Epoch 21/300, seasonal_3 Loss: 0.0446 | 0.0988
Epoch 22/300, seasonal_3 Loss: 0.0432 | 0.1537
Epoch 23/300, seasonal_3 Loss: 0.0437 | 0.1054
Epoch 24/300, seasonal_3 Loss: 0.0432 | 0.1514
Epoch 25/300, seasonal_3 Loss: 0.0462 | 0.0766
Epoch 26/300, seasonal_3 Loss: 0.0439 | 0.1461
Epoch 27/300, seasonal_3 Loss: 0.0494 | 0.0768
Epoch 28/300, seasonal_3 Loss: 0.0420 | 0.0947
Epoch 29/300, seasonal_3 Loss: 0.0411 | 0.0791
Epoch 30/300, seasonal_3 Loss: 0.0395 | 0.1056
Epoch 31/300, seasonal_3 Loss: 0.0398 | 0.0803
Epoch 32/300, seasonal_3 Loss: 0.0391 | 0.1000
Epoch 33/300, seasonal_3 Loss: 0.0396 | 0.0822
Epoch 34/300, seasonal_3 Loss: 0.0390 | 0.0966
Epoch 35/300, seasonal_3 Loss: 0.0391 | 0.0872
Epoch 36/300, seasonal_3 Loss: 0.0384 | 0.0996
Epoch 37/300, seasonal_3 Loss: 0.0386 | 0.0885
Epoch 38/300, seasonal_3 Loss: 0.0383 | 0.0900
Epoch 39/300, seasonal_3 Loss: 0.0382 | 0.0876
Epoch 40/300, seasonal_3 Loss: 0.0381 | 0.0864
Epoch 41/300, seasonal_3 Loss: 0.0380 | 0.0867
Epoch 42/300, seasonal_3 Loss: 0.0376 | 0.0847
Epoch 43/300, seasonal_3 Loss: 0.0371 | 0.0861
Epoch 44/300, seasonal_3 Loss: 0.0366 | 0.0832
Epoch 45/300, seasonal_3 Loss: 0.0361 | 0.0830
Epoch 46/300, seasonal_3 Loss: 0.0359 | 0.0817
Epoch 47/300, seasonal_3 Loss: 0.0357 | 0.0816
Epoch 48/300, seasonal_3 Loss: 0.0356 | 0.0805
Epoch 49/300, seasonal_3 Loss: 0.0354 | 0.0807
Epoch 50/300, seasonal_3 Loss: 0.0352 | 0.0802
Epoch 51/300, seasonal_3 Loss: 0.0351 | 0.0799
Epoch 52/300, seasonal_3 Loss: 0.0350 | 0.0799
Epoch 53/300, seasonal_3 Loss: 0.0348 | 0.0798
Epoch 54/300, seasonal_3 Loss: 0.0347 | 0.0797
Epoch 55/300, seasonal_3 Loss: 0.0346 | 0.0796
Epoch 56/300, seasonal_3 Loss: 0.0346 | 0.0796
Epoch 57/300, seasonal_3 Loss: 0.0345 | 0.0796
Epoch 58/300, seasonal_3 Loss: 0.0344 | 0.0795
Epoch 59/300, seasonal_3 Loss: 0.0344 | 0.0796
Epoch 60/300, seasonal_3 Loss: 0.0343 | 0.0796
Epoch 61/300, seasonal_3 Loss: 0.0343 | 0.0795
Epoch 62/300, seasonal_3 Loss: 0.0342 | 0.0796
Epoch 63/300, seasonal_3 Loss: 0.0342 | 0.0796
Epoch 64/300, seasonal_3 Loss: 0.0341 | 0.0796
Epoch 65/300, seasonal_3 Loss: 0.0341 | 0.0796
Epoch 66/300, seasonal_3 Loss: 0.0341 | 0.0796
Epoch 67/300, seasonal_3 Loss: 0.0340 | 0.0796
Epoch 68/300, seasonal_3 Loss: 0.0340 | 0.0796
Epoch 69/300, seasonal_3 Loss: 0.0340 | 0.0796
Epoch 70/300, seasonal_3 Loss: 0.0339 | 0.0796
Epoch 71/300, seasonal_3 Loss: 0.0339 | 0.0796
Epoch 72/300, seasonal_3 Loss: 0.0339 | 0.0796
Epoch 73/300, seasonal_3 Loss: 0.0339 | 0.0796
Epoch 74/300, seasonal_3 Loss: 0.0339 | 0.0796
Epoch 75/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 76/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 77/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 78/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 79/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 80/300, seasonal_3 Loss: 0.0338 | 0.0796
Epoch 81/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 82/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 83/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 84/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 85/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 86/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 87/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 88/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 89/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 90/300, seasonal_3 Loss: 0.0337 | 0.0796
Epoch 91/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 92/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 93/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 94/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 95/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 96/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 97/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 98/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 99/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 100/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 101/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 102/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 103/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 104/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 105/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 106/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 107/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 108/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 109/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 110/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 111/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 112/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 113/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 114/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 115/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 116/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 117/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 118/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 119/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 120/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 121/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 122/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 123/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 124/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 125/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 126/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 127/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 128/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 129/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 130/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 131/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 132/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 133/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 134/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 135/300, seasonal_3 Loss: 0.0336 | 0.0796
Epoch 136/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 137/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 138/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 139/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 140/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 141/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 142/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 143/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 144/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 145/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 146/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 147/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 148/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 149/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 150/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 151/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 152/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 153/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 154/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 155/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 156/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 157/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 158/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 159/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 160/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 161/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 162/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 163/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 164/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 165/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 166/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 167/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 168/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 169/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 170/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 171/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 172/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 173/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 174/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 175/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 176/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 177/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 178/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 179/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 180/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 181/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 182/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 183/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 184/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 185/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 186/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 187/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 188/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 189/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 190/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 191/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 192/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 193/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 194/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 195/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 196/300, seasonal_3 Loss: 0.0335 | 0.0796
Epoch 197/300, seasonal_3 Loss: 0.0335 | 0.0796
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 10, 'train_rates': 0.9875366212038447, 'learning_rate': 0.0008211719816221531, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9817291017168376}
Epoch 1/300, resid Loss: 0.3839 | 0.1884
Epoch 2/300, resid Loss: 0.2233 | 0.3251
Epoch 3/300, resid Loss: 0.1111 | 0.1154
Epoch 4/300, resid Loss: 0.1196 | 0.1684
Epoch 5/300, resid Loss: 0.1093 | 0.1466
Epoch 6/300, resid Loss: 0.1391 | 0.1959
Epoch 7/300, resid Loss: 0.1215 | 0.3142
Epoch 8/300, resid Loss: 0.0905 | 0.1151
Epoch 9/300, resid Loss: 0.0725 | 0.0838
Epoch 10/300, resid Loss: 0.0654 | 0.0693
Epoch 11/300, resid Loss: 0.0616 | 0.0622
Epoch 12/300, resid Loss: 0.0595 | 0.0590
Epoch 13/300, resid Loss: 0.0569 | 0.0568
Epoch 14/300, resid Loss: 0.0553 | 0.0559
Epoch 15/300, resid Loss: 0.0569 | 0.0566
Epoch 16/300, resid Loss: 0.0601 | 0.0616
Epoch 17/300, resid Loss: 0.0618 | 0.0588
Epoch 18/300, resid Loss: 0.0900 | 0.1515
Epoch 19/300, resid Loss: 0.1088 | 0.1197
Epoch 20/300, resid Loss: 0.1012 | 0.2949
Epoch 21/300, resid Loss: 0.1141 | 0.0806
Epoch 22/300, resid Loss: 0.0627 | 0.0792
Epoch 23/300, resid Loss: 0.0597 | 0.0529
Epoch 24/300, resid Loss: 0.0512 | 0.0510
Epoch 25/300, resid Loss: 0.0601 | 0.0641
Epoch 26/300, resid Loss: 0.0557 | 0.0545
Epoch 27/300, resid Loss: 0.0464 | 0.0500
Epoch 28/300, resid Loss: 0.0520 | 0.0502
Epoch 29/300, resid Loss: 0.0492 | 0.0565
Epoch 30/300, resid Loss: 0.0428 | 0.0507
Epoch 31/300, resid Loss: 0.0405 | 0.0420
Epoch 32/300, resid Loss: 0.0403 | 0.0408
Epoch 33/300, resid Loss: 0.0427 | 0.0415
Epoch 34/300, resid Loss: 0.0490 | 0.0458
Epoch 35/300, resid Loss: 0.0618 | 0.0736
Epoch 36/300, resid Loss: 0.0550 | 0.0510
Epoch 37/300, resid Loss: 0.0424 | 0.0471
Epoch 38/300, resid Loss: 0.0436 | 0.0450
Epoch 39/300, resid Loss: 0.0514 | 0.0494
Epoch 40/300, resid Loss: 0.0467 | 0.0643
Epoch 41/300, resid Loss: 0.0397 | 0.0410
Epoch 42/300, resid Loss: 0.0367 | 0.0349
Epoch 43/300, resid Loss: 0.0353 | 0.0367
Epoch 44/300, resid Loss: 0.0354 | 0.0353
Epoch 45/300, resid Loss: 0.0342 | 0.0345
Epoch 46/300, resid Loss: 0.0346 | 0.0365
Epoch 47/300, resid Loss: 0.0398 | 0.0384
Epoch 48/300, resid Loss: 0.0452 | 0.0489
Epoch 49/300, resid Loss: 0.0372 | 0.0406
Epoch 50/300, resid Loss: 0.0346 | 0.0322
Epoch 51/300, resid Loss: 0.0329 | 0.0315
Epoch 52/300, resid Loss: 0.0334 | 0.0319
Epoch 53/300, resid Loss: 0.0348 | 0.0331
Epoch 54/300, resid Loss: 0.0433 | 0.0427
Epoch 55/300, resid Loss: 0.0402 | 0.0396
Epoch 56/300, resid Loss: 0.0364 | 0.0353
Epoch 57/300, resid Loss: 0.0401 | 0.0352
Epoch 58/300, resid Loss: 0.0387 | 0.0324
Epoch 59/300, resid Loss: 0.0325 | 0.0280
Epoch 60/300, resid Loss: 0.0309 | 0.0261
Epoch 61/300, resid Loss: 0.0313 | 0.0275
Epoch 62/300, resid Loss: 0.0308 | 0.0251
Epoch 63/300, resid Loss: 0.0300 | 0.0243
Epoch 64/300, resid Loss: 0.0293 | 0.0259
Epoch 65/300, resid Loss: 0.0302 | 0.0252
Epoch 66/300, resid Loss: 0.0299 | 0.0286
Epoch 67/300, resid Loss: 0.0337 | 0.0367
Epoch 68/300, resid Loss: 0.0308 | 0.0290
Epoch 69/300, resid Loss: 0.0316 | 0.0333
Epoch 70/300, resid Loss: 0.0293 | 0.0233
Epoch 71/300, resid Loss: 0.0293 | 0.0278
Epoch 72/300, resid Loss: 0.0298 | 0.0312
Epoch 73/300, resid Loss: 0.0318 | 0.0482
Epoch 74/300, resid Loss: 0.0402 | 0.0407
Epoch 75/300, resid Loss: 0.0370 | 0.0316
Epoch 76/300, resid Loss: 0.0358 | 0.0314
Epoch 77/300, resid Loss: 0.0349 | 0.0305
Epoch 78/300, resid Loss: 0.0295 | 0.0249
Epoch 79/300, resid Loss: 0.0287 | 0.0270
Epoch 80/300, resid Loss: 0.0278 | 0.0257
Epoch 81/300, resid Loss: 0.0275 | 0.0270
Epoch 82/300, resid Loss: 0.0276 | 0.0236
Epoch 83/300, resid Loss: 0.0274 | 0.0243
Epoch 84/300, resid Loss: 0.0297 | 0.0267
Epoch 85/300, resid Loss: 0.0287 | 0.0381
Epoch 86/300, resid Loss: 0.0313 | 0.0398
Epoch 87/300, resid Loss: 0.0310 | 0.0371
Epoch 88/300, resid Loss: 0.0299 | 0.0311
Epoch 89/300, resid Loss: 0.0294 | 0.0379
Epoch 90/300, resid Loss: 0.0271 | 0.0308
Epoch 91/300, resid Loss: 0.0263 | 0.0359
Epoch 92/300, resid Loss: 0.0274 | 0.0282
Epoch 93/300, resid Loss: 0.0280 | 0.0269
Epoch 94/300, resid Loss: 0.0260 | 0.0242
Epoch 95/300, resid Loss: 0.0287 | 0.0299
Epoch 96/300, resid Loss: 0.0283 | 0.0289
Epoch 97/300, resid Loss: 0.0276 | 0.0296
Epoch 98/300, resid Loss: 0.0278 | 0.0314
Epoch 99/300, resid Loss: 0.0260 | 0.0262
Epoch 100/300, resid Loss: 0.0257 | 0.0287
Epoch 101/300, resid Loss: 0.0257 | 0.0262
Epoch 102/300, resid Loss: 0.0248 | 0.0272
Epoch 103/300, resid Loss: 0.0254 | 0.0258
Epoch 104/300, resid Loss: 0.0248 | 0.0252
Epoch 105/300, resid Loss: 0.0235 | 0.0242
Epoch 106/300, resid Loss: 0.0220 | 0.0265
Epoch 107/300, resid Loss: 0.0220 | 0.0261
Epoch 108/300, resid Loss: 0.0222 | 0.0247
Epoch 109/300, resid Loss: 0.0217 | 0.0250
Epoch 110/300, resid Loss: 0.0217 | 0.0246
Epoch 111/300, resid Loss: 0.0215 | 0.0234
Epoch 112/300, resid Loss: 0.0215 | 0.0236
Epoch 113/300, resid Loss: 0.0209 | 0.0232
Epoch 114/300, resid Loss: 0.0207 | 0.0236
Epoch 115/300, resid Loss: 0.0214 | 0.0254
Epoch 116/300, resid Loss: 0.0215 | 0.0242
Epoch 117/300, resid Loss: 0.0218 | 0.0250
Epoch 118/300, resid Loss: 0.0200 | 0.0232
Epoch 119/300, resid Loss: 0.0206 | 0.0261
Epoch 120/300, resid Loss: 0.0215 | 0.0230
Epoch 121/300, resid Loss: 0.0218 | 0.0273
Epoch 122/300, resid Loss: 0.0232 | 0.0286
Epoch 123/300, resid Loss: 0.0226 | 0.0243
Epoch 124/300, resid Loss: 0.0233 | 0.0301
Epoch 125/300, resid Loss: 0.0217 | 0.0248
Epoch 126/300, resid Loss: 0.0245 | 0.0350
Epoch 127/300, resid Loss: 0.0233 | 0.0258
Epoch 128/300, resid Loss: 0.0237 | 0.0333
Epoch 129/300, resid Loss: 0.0242 | 0.0254
Epoch 130/300, resid Loss: 0.0220 | 0.0271
Epoch 131/300, resid Loss: 0.0204 | 0.0242
Epoch 132/300, resid Loss: 0.0205 | 0.0252
Epoch 133/300, resid Loss: 0.0213 | 0.0257
Epoch 134/300, resid Loss: 0.0215 | 0.0274
Epoch 135/300, resid Loss: 0.0202 | 0.0273
Epoch 136/300, resid Loss: 0.0206 | 0.0306
Epoch 137/300, resid Loss: 0.0237 | 0.0334
Epoch 138/300, resid Loss: 0.0234 | 0.0385
Epoch 139/300, resid Loss: 0.0275 | 0.0373
Epoch 140/300, resid Loss: 0.0283 | 0.0277
Epoch 141/300, resid Loss: 0.0253 | 0.0296
Epoch 142/300, resid Loss: 0.0223 | 0.0277
Epoch 143/300, resid Loss: 0.0214 | 0.0266
Epoch 144/300, resid Loss: 0.0204 | 0.0304
Epoch 145/300, resid Loss: 0.0207 | 0.0305
Epoch 146/300, resid Loss: 0.0212 | 0.0265
Epoch 147/300, resid Loss: 0.0188 | 0.0277
Epoch 148/300, resid Loss: 0.0203 | 0.0266
Epoch 149/300, resid Loss: 0.0191 | 0.0252
Epoch 150/300, resid Loss: 0.0177 | 0.0259
Epoch 151/300, resid Loss: 0.0175 | 0.0242
Epoch 152/300, resid Loss: 0.0178 | 0.0260
Epoch 153/300, resid Loss: 0.0168 | 0.0262
Epoch 154/300, resid Loss: 0.0176 | 0.0284
Epoch 155/300, resid Loss: 0.0170 | 0.0268
Epoch 156/300, resid Loss: 0.0166 | 0.0258
Epoch 157/300, resid Loss: 0.0168 | 0.0266
Epoch 158/300, resid Loss: 0.0176 | 0.0272
Epoch 159/300, resid Loss: 0.0169 | 0.0279
Epoch 160/300, resid Loss: 0.0172 | 0.0280
Epoch 161/300, resid Loss: 0.0182 | 0.0323
Epoch 162/300, resid Loss: 0.0182 | 0.0314
Epoch 163/300, resid Loss: 0.0186 | 0.0336
Epoch 164/300, resid Loss: 0.0187 | 0.0310
Epoch 165/300, resid Loss: 0.0181 | 0.0349
Epoch 166/300, resid Loss: 0.0178 | 0.0304
Epoch 167/300, resid Loss: 0.0169 | 0.0307
Epoch 168/300, resid Loss: 0.0167 | 0.0263
Epoch 169/300, resid Loss: 0.0157 | 0.0258
Epoch 170/300, resid Loss: 0.0155 | 0.0265
Epoch 171/300, resid Loss: 0.0154 | 0.0285
Epoch 172/300, resid Loss: 0.0168 | 0.0284
Epoch 173/300, resid Loss: 0.0167 | 0.0267
Epoch 174/300, resid Loss: 0.0152 | 0.0258
Epoch 175/300, resid Loss: 0.0155 | 0.0298
Epoch 176/300, resid Loss: 0.0171 | 0.0256
Epoch 177/300, resid Loss: 0.0145 | 0.0274
Epoch 178/300, resid Loss: 0.0148 | 0.0269
Epoch 179/300, resid Loss: 0.0145 | 0.0281
Epoch 180/300, resid Loss: 0.0139 | 0.0274
Epoch 181/300, resid Loss: 0.0141 | 0.0263
Epoch 182/300, resid Loss: 0.0140 | 0.0261
Epoch 183/300, resid Loss: 0.0133 | 0.0276
Epoch 184/300, resid Loss: 0.0130 | 0.0275
Epoch 185/300, resid Loss: 0.0131 | 0.0279
Epoch 186/300, resid Loss: 0.0128 | 0.0276
Epoch 187/300, resid Loss: 0.0127 | 0.0277
Epoch 188/300, resid Loss: 0.0127 | 0.0272
Epoch 189/300, resid Loss: 0.0126 | 0.0280
Epoch 190/300, resid Loss: 0.0126 | 0.0282
Epoch 191/300, resid Loss: 0.0125 | 0.0285
Epoch 192/300, resid Loss: 0.0129 | 0.0289
Epoch 193/300, resid Loss: 0.0134 | 0.0305
Epoch 194/300, resid Loss: 0.0139 | 0.0332
Epoch 195/300, resid Loss: 0.0142 | 0.0307
Epoch 196/300, resid Loss: 0.0145 | 0.0341
Epoch 197/300, resid Loss: 0.0142 | 0.0284
Epoch 198/300, resid Loss: 0.0136 | 0.0301
Epoch 199/300, resid Loss: 0.0148 | 0.0322
Epoch 200/300, resid Loss: 0.0160 | 0.0364
Epoch 201/300, resid Loss: 0.0160 | 0.0300
Epoch 202/300, resid Loss: 0.0168 | 0.0332
Epoch 203/300, resid Loss: 0.0185 | 0.0337
Epoch 204/300, resid Loss: 0.0229 | 0.0388
Epoch 205/300, resid Loss: 0.0285 | 0.0461
Epoch 206/300, resid Loss: 0.0356 | 0.0316
Epoch 207/300, resid Loss: 0.0221 | 0.0400
Epoch 208/300, resid Loss: 0.0300 | 0.0480
Epoch 209/300, resid Loss: 0.0452 | 0.0713
Epoch 210/300, resid Loss: 0.0447 | 0.0775
Epoch 211/300, resid Loss: 0.0558 | 0.0774
Epoch 212/300, resid Loss: 0.0517 | 0.0423
Epoch 213/300, resid Loss: 0.0489 | 0.0424
Epoch 214/300, resid Loss: 0.0375 | 0.0362
Epoch 215/300, resid Loss: 0.0348 | 0.0284
Epoch 216/300, resid Loss: 0.0289 | 0.0445
Epoch 217/300, resid Loss: 0.0305 | 0.0330
Epoch 218/300, resid Loss: 0.0201 | 0.0273
Epoch 219/300, resid Loss: 0.0182 | 0.0278
Epoch 220/300, resid Loss: 0.0156 | 0.0259
Epoch 221/300, resid Loss: 0.0142 | 0.0273
Epoch 222/300, resid Loss: 0.0133 | 0.0257
Epoch 223/300, resid Loss: 0.0128 | 0.0256
Epoch 224/300, resid Loss: 0.0128 | 0.0277
Epoch 225/300, resid Loss: 0.0127 | 0.0260
Epoch 226/300, resid Loss: 0.0124 | 0.0259
Epoch 227/300, resid Loss: 0.0123 | 0.0283
Epoch 228/300, resid Loss: 0.0124 | 0.0258
Epoch 229/300, resid Loss: 0.0118 | 0.0258
Epoch 230/300, resid Loss: 0.0120 | 0.0278
Epoch 231/300, resid Loss: 0.0118 | 0.0258
Epoch 232/300, resid Loss: 0.0113 | 0.0258
Epoch 233/300, resid Loss: 0.0115 | 0.0277
Epoch 234/300, resid Loss: 0.0112 | 0.0261
Epoch 235/300, resid Loss: 0.0108 | 0.0260
Epoch 236/300, resid Loss: 0.0110 | 0.0277
Epoch 237/300, resid Loss: 0.0108 | 0.0265
Epoch 238/300, resid Loss: 0.0106 | 0.0264
Epoch 239/300, resid Loss: 0.0107 | 0.0279
Epoch 240/300, resid Loss: 0.0106 | 0.0273
Epoch 241/300, resid Loss: 0.0106 | 0.0273
Epoch 242/300, resid Loss: 0.0109 | 0.0287
Epoch 243/300, resid Loss: 0.0109 | 0.0286
Epoch 244/300, resid Loss: 0.0115 | 0.0281
Epoch 245/300, resid Loss: 0.0117 | 0.0294
Epoch 246/300, resid Loss: 0.0116 | 0.0283
Epoch 247/300, resid Loss: 0.0116 | 0.0275
Epoch 248/300, resid Loss: 0.0112 | 0.0290
Epoch 249/300, resid Loss: 0.0108 | 0.0286
Epoch 250/300, resid Loss: 0.0106 | 0.0281
Epoch 251/300, resid Loss: 0.0110 | 0.0287
Epoch 252/300, resid Loss: 0.0104 | 0.0278
Epoch 253/300, resid Loss: 0.0099 | 0.0275
Epoch 254/300, resid Loss: 0.0100 | 0.0279
Epoch 255/300, resid Loss: 0.0097 | 0.0292
Epoch 256/300, resid Loss: 0.0098 | 0.0281
Epoch 257/300, resid Loss: 0.0096 | 0.0284
Epoch 258/300, resid Loss: 0.0094 | 0.0298
Epoch 259/300, resid Loss: 0.0094 | 0.0281
Epoch 260/300, resid Loss: 0.0090 | 0.0282
Epoch 261/300, resid Loss: 0.0090 | 0.0291
Epoch 262/300, resid Loss: 0.0091 | 0.0284
Epoch 263/300, resid Loss: 0.0090 | 0.0290
Epoch 264/300, resid Loss: 0.0091 | 0.0292
Epoch 265/300, resid Loss: 0.0090 | 0.0292
Epoch 266/300, resid Loss: 0.0090 | 0.0298
Epoch 267/300, resid Loss: 0.0091 | 0.0306
Epoch 268/300, resid Loss: 0.0088 | 0.0305
Epoch 269/300, resid Loss: 0.0090 | 0.0303
Epoch 270/300, resid Loss: 0.0093 | 0.0317
Epoch 271/300, resid Loss: 0.0097 | 0.0308
Epoch 272/300, resid Loss: 0.0099 | 0.0305
Epoch 273/300, resid Loss: 0.0096 | 0.0312
Epoch 274/300, resid Loss: 0.0095 | 0.0312
Epoch 275/300, resid Loss: 0.0092 | 0.0302
Epoch 276/300, resid Loss: 0.0096 | 0.0303
Epoch 277/300, resid Loss: 0.0099 | 0.0312
Epoch 278/300, resid Loss: 0.0102 | 0.0309
Epoch 279/300, resid Loss: 0.0107 | 0.0309
Epoch 280/300, resid Loss: 0.0101 | 0.0297
Epoch 281/300, resid Loss: 0.0090 | 0.0324
Epoch 282/300, resid Loss: 0.0091 | 0.0313
Epoch 283/300, resid Loss: 0.0095 | 0.0325
Epoch 284/300, resid Loss: 0.0094 | 0.0310
Epoch 285/300, resid Loss: 0.0088 | 0.0302
Epoch 286/300, resid Loss: 0.0087 | 0.0322
Epoch 287/300, resid Loss: 0.0087 | 0.0309
Epoch 288/300, resid Loss: 0.0083 | 0.0312
Epoch 289/300, resid Loss: 0.0083 | 0.0312
Epoch 290/300, resid Loss: 0.0081 | 0.0313
Epoch 291/300, resid Loss: 0.0081 | 0.0309
Epoch 292/300, resid Loss: 0.0077 | 0.0320
Epoch 293/300, resid Loss: 0.0078 | 0.0318
Epoch 294/300, resid Loss: 0.0078 | 0.0319
Epoch 295/300, resid Loss: 0.0083 | 0.0321
Epoch 296/300, resid Loss: 0.0079 | 0.0312
Epoch 297/300, resid Loss: 0.0078 | 0.0308
Epoch 298/300, resid Loss: 0.0080 | 0.0318
Epoch 299/300, resid Loss: 0.0079 | 0.0330
Epoch 300/300, resid Loss: 0.0078 | 0.0332
Runtime (seconds): 2131.446815252304
0.0005582732056514692
[109.63628]
[0.8356178]
[1.6107005]
[0.110806]
[5.739918]
[1.7822279]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.06474375264951959
RMSE: 0.25444793701171875
MAE: 0.25444793701171875
R-squared: nan
[119.71555]
