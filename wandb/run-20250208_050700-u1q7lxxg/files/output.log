[32m[I 2025-02-08 05:07:05,798][0m A new study created in memory with name: no-name-3c480eca-d4ae-4814-be09-308cbea93809[0m
[32m[I 2025-02-08 05:07:28,396][0m Trial 0 finished with value: 0.16927176450784617 and parameters: {'observation_period_num': 29, 'train_rates': 0.7009743436905356, 'learning_rate': 5.671346056370916e-05, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9833958691583936}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:07:58,184][0m Trial 1 finished with value: 0.513956663213552 and parameters: {'observation_period_num': 21, 'train_rates': 0.8308370821739947, 'learning_rate': 9.26412337182942e-06, 'batch_size': 205, 'step_size': 2, 'gamma': 0.9090341889441919}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:08:20,811][0m Trial 2 finished with value: 1.2300989806444387 and parameters: {'observation_period_num': 185, 'train_rates': 0.6942801600103664, 'learning_rate': 1.6752973468210057e-06, 'batch_size': 225, 'step_size': 14, 'gamma': 0.8526665543693538}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:12:18,287][0m Trial 3 finished with value: 0.49718093001929514 and parameters: {'observation_period_num': 241, 'train_rates': 0.9010334124344543, 'learning_rate': 2.26193592407395e-06, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8023179083621071}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:12:55,465][0m Trial 4 finished with value: 0.36383575201034546 and parameters: {'observation_period_num': 239, 'train_rates': 0.9782434240346369, 'learning_rate': 5.364851720170096e-05, 'batch_size': 159, 'step_size': 14, 'gamma': 0.7593833482062422}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:13:21,460][0m Trial 5 finished with value: 0.2501594139895308 and parameters: {'observation_period_num': 200, 'train_rates': 0.8835127406669012, 'learning_rate': 0.0002384800129716072, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8329698142394177}. Best is trial 0 with value: 0.16927176450784617.[0m
[32m[I 2025-02-08 05:14:55,523][0m Trial 6 finished with value: 0.10767368140361654 and parameters: {'observation_period_num': 116, 'train_rates': 0.7933472626366624, 'learning_rate': 0.00011709754021022504, 'batch_size': 55, 'step_size': 15, 'gamma': 0.8580768561055229}. Best is trial 6 with value: 0.10767368140361654.[0m
[32m[I 2025-02-08 05:15:32,166][0m Trial 7 finished with value: 0.1299698737553916 and parameters: {'observation_period_num': 206, 'train_rates': 0.8494992143955482, 'learning_rate': 0.0009406383874802774, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9180611379527328}. Best is trial 6 with value: 0.10767368140361654.[0m
[32m[I 2025-02-08 05:15:59,869][0m Trial 8 finished with value: 0.5279043114791482 and parameters: {'observation_period_num': 247, 'train_rates': 0.8928561726755984, 'learning_rate': 5.622053437628838e-06, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9726230738795081}. Best is trial 6 with value: 0.10767368140361654.[0m
[32m[I 2025-02-08 05:16:31,122][0m Trial 9 finished with value: 0.4697843208401041 and parameters: {'observation_period_num': 73, 'train_rates': 0.6995507590946275, 'learning_rate': 2.5446413453310627e-05, 'batch_size': 163, 'step_size': 4, 'gamma': 0.7687974494880966}. Best is trial 6 with value: 0.10767368140361654.[0m
[32m[I 2025-02-08 05:17:55,118][0m Trial 10 finished with value: 0.23662615073699672 and parameters: {'observation_period_num': 129, 'train_rates': 0.6200214270500428, 'learning_rate': 0.00025481158795767156, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8977894071718441}. Best is trial 6 with value: 0.10767368140361654.[0m
[32m[I 2025-02-08 05:18:55,545][0m Trial 11 finished with value: 0.10053190078448367 and parameters: {'observation_period_num': 127, 'train_rates': 0.7933179900353744, 'learning_rate': 0.0007087874037842343, 'batch_size': 86, 'step_size': 1, 'gamma': 0.9264600376500854}. Best is trial 11 with value: 0.10053190078448367.[0m
[32m[I 2025-02-08 05:19:53,691][0m Trial 12 finished with value: 0.30255181038821183 and parameters: {'observation_period_num': 115, 'train_rates': 0.7660643214249273, 'learning_rate': 0.0009214572537243323, 'batch_size': 85, 'step_size': 8, 'gamma': 0.948354499666121}. Best is trial 11 with value: 0.10053190078448367.[0m
[32m[I 2025-02-08 05:20:44,375][0m Trial 13 finished with value: 0.2375093120918677 and parameters: {'observation_period_num': 132, 'train_rates': 0.7608781726357535, 'learning_rate': 0.0001963185368783439, 'batch_size': 101, 'step_size': 15, 'gamma': 0.8713116965303679}. Best is trial 11 with value: 0.10053190078448367.[0m
[32m[I 2025-02-08 05:21:39,650][0m Trial 14 finished with value: 0.07982105419573644 and parameters: {'observation_period_num': 80, 'train_rates': 0.7899481408186951, 'learning_rate': 0.00012444373833401238, 'batch_size': 98, 'step_size': 5, 'gamma': 0.9447559521747348}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:22:25,776][0m Trial 15 finished with value: 0.2047511566078524 and parameters: {'observation_period_num': 72, 'train_rates': 0.7361651833761309, 'learning_rate': 0.00042702884972447045, 'batch_size': 116, 'step_size': 1, 'gamma': 0.9404791615930478}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:23:29,948][0m Trial 16 finished with value: 0.18369898789414213 and parameters: {'observation_period_num': 72, 'train_rates': 0.6069922083019077, 'learning_rate': 0.00010022682574751417, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9475132744374707}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:24:13,824][0m Trial 17 finished with value: 0.13981215880044456 and parameters: {'observation_period_num': 177, 'train_rates': 0.8176401276094255, 'learning_rate': 0.00045677355341658594, 'batch_size': 127, 'step_size': 4, 'gamma': 0.8996106704952468}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:26:47,507][0m Trial 18 finished with value: 0.21446981767969808 and parameters: {'observation_period_num': 154, 'train_rates': 0.955702500602585, 'learning_rate': 1.5874857868674983e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9287404080108514}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:27:14,998][0m Trial 19 finished with value: 0.2417369492418997 and parameters: {'observation_period_num': 93, 'train_rates': 0.6557445335900198, 'learning_rate': 0.0005376230025694766, 'batch_size': 186, 'step_size': 3, 'gamma': 0.9650516535642875}. Best is trial 14 with value: 0.07982105419573644.[0m
[32m[I 2025-02-08 05:28:11,193][0m Trial 20 finished with value: 0.05741484634458057 and parameters: {'observation_period_num': 53, 'train_rates': 0.7896724362597647, 'learning_rate': 0.00011356016344632108, 'batch_size': 95, 'step_size': 9, 'gamma': 0.8911063348002326}. Best is trial 20 with value: 0.05741484634458057.[0m
[32m[I 2025-02-08 05:29:09,711][0m Trial 21 finished with value: 0.05716842683439703 and parameters: {'observation_period_num': 48, 'train_rates': 0.7911472745014166, 'learning_rate': 9.230004481343139e-05, 'batch_size': 90, 'step_size': 10, 'gamma': 0.8820084901834128}. Best is trial 21 with value: 0.05716842683439703.[0m
[32m[I 2025-02-08 05:29:58,393][0m Trial 22 finished with value: 0.1883268751480693 and parameters: {'observation_period_num': 43, 'train_rates': 0.7452115293620135, 'learning_rate': 6.559875460149468e-05, 'batch_size': 110, 'step_size': 10, 'gamma': 0.8801423969298324}. Best is trial 21 with value: 0.05716842683439703.[0m
[32m[I 2025-02-08 05:31:06,385][0m Trial 23 finished with value: 0.07600923002329842 and parameters: {'observation_period_num': 46, 'train_rates': 0.8571280808039505, 'learning_rate': 3.051457485852285e-05, 'batch_size': 82, 'step_size': 9, 'gamma': 0.8268047083904962}. Best is trial 21 with value: 0.05716842683439703.[0m
[32m[I 2025-02-08 05:32:25,756][0m Trial 24 finished with value: 0.04823525794721269 and parameters: {'observation_period_num': 6, 'train_rates': 0.8715756285766137, 'learning_rate': 3.098671195350194e-05, 'batch_size': 71, 'step_size': 11, 'gamma': 0.829746759266953}. Best is trial 24 with value: 0.04823525794721269.[0m
[32m[I 2025-02-08 05:34:05,318][0m Trial 25 finished with value: 0.06362033718358284 and parameters: {'observation_period_num': 7, 'train_rates': 0.9297943810587679, 'learning_rate': 1.7910788728353994e-05, 'batch_size': 60, 'step_size': 12, 'gamma': 0.8047854381114518}. Best is trial 24 with value: 0.04823525794721269.[0m
[32m[I 2025-02-08 05:34:49,378][0m Trial 26 finished with value: 0.07679155575904516 and parameters: {'observation_period_num': 50, 'train_rates': 0.8632298361000353, 'learning_rate': 4.31595772910546e-05, 'batch_size': 134, 'step_size': 12, 'gamma': 0.8843351196280737}. Best is trial 24 with value: 0.04823525794721269.[0m
[32m[I 2025-02-08 05:37:26,350][0m Trial 27 finished with value: 0.03552029223966206 and parameters: {'observation_period_num': 7, 'train_rates': 0.8173373761920241, 'learning_rate': 9.043145953629448e-05, 'batch_size': 34, 'step_size': 11, 'gamma': 0.8391826286413399}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:41:12,186][0m Trial 28 finished with value: 0.06801492889175714 and parameters: {'observation_period_num': 16, 'train_rates': 0.829630109998246, 'learning_rate': 7.82802184465998e-06, 'batch_size': 24, 'step_size': 11, 'gamma': 0.837506384019423}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:43:47,208][0m Trial 29 finished with value: 0.04492295632122928 and parameters: {'observation_period_num': 34, 'train_rates': 0.9212366614487904, 'learning_rate': 7.461131627176404e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8054407260805176}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:46:44,472][0m Trial 30 finished with value: 0.05041058332295963 and parameters: {'observation_period_num': 31, 'train_rates': 0.92513720561889, 'learning_rate': 3.9504382309722244e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.793643350551577}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:49:07,725][0m Trial 31 finished with value: 0.05465804322595053 and parameters: {'observation_period_num': 31, 'train_rates': 0.9320983598055065, 'learning_rate': 4.2408583373698135e-05, 'batch_size': 41, 'step_size': 13, 'gamma': 0.7891289527674569}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:54:26,808][0m Trial 32 finished with value: 0.05123969759273378 and parameters: {'observation_period_num': 25, 'train_rates': 0.9204329668685823, 'learning_rate': 1.849331064042943e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8164878574855082}. Best is trial 27 with value: 0.03552029223966206.[0m
[32m[I 2025-02-08 05:56:53,251][0m Trial 33 finished with value: 0.03172175586223602 and parameters: {'observation_period_num': 9, 'train_rates': 0.9888446892594105, 'learning_rate': 6.878779447626293e-05, 'batch_size': 42, 'step_size': 13, 'gamma': 0.7865218958730735}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 05:58:25,717][0m Trial 34 finished with value: 0.04253589492291212 and parameters: {'observation_period_num': 8, 'train_rates': 0.9601846095500849, 'learning_rate': 6.828210106606201e-05, 'batch_size': 66, 'step_size': 11, 'gamma': 0.7776811943499697}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:00:32,952][0m Trial 35 finished with value: 0.053737472742795944 and parameters: {'observation_period_num': 19, 'train_rates': 0.9857182046855316, 'learning_rate': 6.948373264500812e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7744976356852651}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:02:02,351][0m Trial 36 finished with value: 0.04333401003839986 and parameters: {'observation_period_num': 7, 'train_rates': 0.9503296715305803, 'learning_rate': 0.00017732066642633693, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7516812518236355}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:03:35,557][0m Trial 37 finished with value: 0.0397468373099322 and parameters: {'observation_period_num': 10, 'train_rates': 0.9691128951938299, 'learning_rate': 0.00017222992690887987, 'batch_size': 67, 'step_size': 11, 'gamma': 0.7502412793035929}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:09:43,717][0m Trial 38 finished with value: 0.09211931082437623 and parameters: {'observation_period_num': 61, 'train_rates': 0.963960856991803, 'learning_rate': 0.00031064764834846747, 'batch_size': 16, 'step_size': 9, 'gamma': 0.781742433168128}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:11:32,488][0m Trial 39 finished with value: 0.055291350930929184 and parameters: {'observation_period_num': 20, 'train_rates': 0.985626360303571, 'learning_rate': 0.00015876943023164427, 'batch_size': 57, 'step_size': 11, 'gamma': 0.7667741703637567}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:14:46,729][0m Trial 40 finished with value: 0.243748109280271 and parameters: {'observation_period_num': 34, 'train_rates': 0.898832311482769, 'learning_rate': 2.8804089875342767e-06, 'batch_size': 29, 'step_size': 12, 'gamma': 0.7518561625707223}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:16:16,326][0m Trial 41 finished with value: 0.04110039124379353 and parameters: {'observation_period_num': 14, 'train_rates': 0.9509873805790031, 'learning_rate': 0.00016343831874308454, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7542201087882445}. Best is trial 33 with value: 0.03172175586223602.[0m
[32m[I 2025-02-08 06:17:49,401][0m Trial 42 finished with value: 0.028583553391245176 and parameters: {'observation_period_num': 5, 'train_rates': 0.964917270684939, 'learning_rate': 0.00032360537060310173, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7658035736877923}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:19:12,221][0m Trial 43 finished with value: 0.06139942479657603 and parameters: {'observation_period_num': 18, 'train_rates': 0.9697699567362654, 'learning_rate': 0.00028118560241602335, 'batch_size': 76, 'step_size': 14, 'gamma': 0.7621097716736385}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:21:22,164][0m Trial 44 finished with value: 0.03839915245771408 and parameters: {'observation_period_num': 37, 'train_rates': 0.9896892847461902, 'learning_rate': 0.00014787331293352553, 'batch_size': 48, 'step_size': 8, 'gamma': 0.7888613117873203}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:23:29,034][0m Trial 45 finished with value: 0.05289110817374235 and parameters: {'observation_period_num': 38, 'train_rates': 0.9442202363744754, 'learning_rate': 0.0003532212681224232, 'batch_size': 47, 'step_size': 8, 'gamma': 0.7912148430512088}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:25:34,268][0m Trial 46 finished with value: 0.08999133982828685 and parameters: {'observation_period_num': 61, 'train_rates': 0.9763164676829054, 'learning_rate': 0.00023226434907775662, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8504343336348716}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:28:51,647][0m Trial 47 finished with value: 0.23070137425967227 and parameters: {'observation_period_num': 222, 'train_rates': 0.9075236544904838, 'learning_rate': 0.00013918318553269917, 'batch_size': 27, 'step_size': 8, 'gamma': 0.8122427043724679}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:30:13,229][0m Trial 48 finished with value: 0.1869395974909669 and parameters: {'observation_period_num': 95, 'train_rates': 0.6736172832758128, 'learning_rate': 9.401698065173017e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.78157186107371}. Best is trial 42 with value: 0.028583553391245176.[0m
[32m[I 2025-02-08 06:30:42,022][0m Trial 49 finished with value: 0.047577328979969025 and parameters: {'observation_period_num': 5, 'train_rates': 0.9860293557656866, 'learning_rate': 0.0007102595243917329, 'batch_size': 255, 'step_size': 15, 'gamma': 0.7695430416665294}. Best is trial 42 with value: 0.028583553391245176.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2201 | 0.1185
Epoch 2/300, Loss: 0.1293 | 0.0932
Epoch 3/300, Loss: 0.1207 | 0.0893
Epoch 4/300, Loss: 0.1100 | 0.1057
Epoch 5/300, Loss: 0.1039 | 0.0988
Epoch 6/300, Loss: 0.0996 | 0.0875
Epoch 7/300, Loss: 0.0966 | 0.0672
Epoch 8/300, Loss: 0.0972 | 0.0599
Epoch 9/300, Loss: 0.0916 | 0.0550
Epoch 10/300, Loss: 0.0941 | 0.0576
Epoch 11/300, Loss: 0.0846 | 0.0521
Epoch 12/300, Loss: 0.0810 | 0.0562
Epoch 13/300, Loss: 0.0818 | 0.0546
Epoch 14/300, Loss: 0.0785 | 0.0549
Epoch 15/300, Loss: 0.0777 | 0.0534
Epoch 16/300, Loss: 0.0785 | 0.0531
Epoch 17/300, Loss: 0.0805 | 0.0539
Epoch 18/300, Loss: 0.0802 | 0.0509
Epoch 19/300, Loss: 0.0770 | 0.0497
Epoch 20/300, Loss: 0.0747 | 0.0450
Epoch 21/300, Loss: 0.0722 | 0.0435
Epoch 22/300, Loss: 0.0713 | 0.0429
Epoch 23/300, Loss: 0.0710 | 0.0420
Epoch 24/300, Loss: 0.0707 | 0.0415
Epoch 25/300, Loss: 0.0696 | 0.0405
Epoch 26/300, Loss: 0.0688 | 0.0400
Epoch 27/300, Loss: 0.0681 | 0.0396
Epoch 28/300, Loss: 0.0676 | 0.0391
Epoch 29/300, Loss: 0.0672 | 0.0387
Epoch 30/300, Loss: 0.0669 | 0.0384
Epoch 31/300, Loss: 0.0665 | 0.0378
Epoch 32/300, Loss: 0.0663 | 0.0377
Epoch 33/300, Loss: 0.0661 | 0.0376
Epoch 34/300, Loss: 0.0659 | 0.0373
Epoch 35/300, Loss: 0.0656 | 0.0368
Epoch 36/300, Loss: 0.0654 | 0.0363
Epoch 37/300, Loss: 0.0652 | 0.0376
Epoch 38/300, Loss: 0.0653 | 0.0363
Epoch 39/300, Loss: 0.0648 | 0.0356
Epoch 40/300, Loss: 0.0646 | 0.0354
Epoch 41/300, Loss: 0.0643 | 0.0354
Epoch 42/300, Loss: 0.0640 | 0.0357
Epoch 43/300, Loss: 0.0639 | 0.0356
Epoch 44/300, Loss: 0.0639 | 0.0356
Epoch 45/300, Loss: 0.0637 | 0.0354
Epoch 46/300, Loss: 0.0637 | 0.0355
Epoch 47/300, Loss: 0.0637 | 0.0357
Epoch 48/300, Loss: 0.0638 | 0.0357
Epoch 49/300, Loss: 0.0640 | 0.0362
Epoch 50/300, Loss: 0.0643 | 0.0364
Epoch 51/300, Loss: 0.0647 | 0.0369
Epoch 52/300, Loss: 0.0652 | 0.0373
Epoch 53/300, Loss: 0.0655 | 0.0373
Epoch 54/300, Loss: 0.0655 | 0.0371
Epoch 55/300, Loss: 0.0652 | 0.0374
Epoch 56/300, Loss: 0.0644 | 0.0370
Epoch 57/300, Loss: 0.0633 | 0.0367
Epoch 58/300, Loss: 0.0627 | 0.0361
Epoch 59/300, Loss: 0.0624 | 0.0357
Epoch 60/300, Loss: 0.0621 | 0.0354
Epoch 61/300, Loss: 0.0621 | 0.0358
Epoch 62/300, Loss: 0.0620 | 0.0358
Epoch 63/300, Loss: 0.0619 | 0.0357
Epoch 64/300, Loss: 0.0619 | 0.0356
Epoch 65/300, Loss: 0.0619 | 0.0354
Epoch 66/300, Loss: 0.0619 | 0.0352
Epoch 67/300, Loss: 0.0619 | 0.0348
Epoch 68/300, Loss: 0.0623 | 0.0348
Epoch 69/300, Loss: 0.0625 | 0.0348
Epoch 70/300, Loss: 0.0625 | 0.0348
Epoch 71/300, Loss: 0.0623 | 0.0348
Epoch 72/300, Loss: 0.0619 | 0.0348
Epoch 73/300, Loss: 0.0617 | 0.0345
Epoch 74/300, Loss: 0.0614 | 0.0346
Epoch 75/300, Loss: 0.0611 | 0.0345
Epoch 76/300, Loss: 0.0608 | 0.0345
Epoch 77/300, Loss: 0.0607 | 0.0344
Epoch 78/300, Loss: 0.0605 | 0.0343
Epoch 79/300, Loss: 0.0604 | 0.0345
Epoch 80/300, Loss: 0.0603 | 0.0344
Epoch 81/300, Loss: 0.0602 | 0.0344
Epoch 82/300, Loss: 0.0601 | 0.0343
Epoch 83/300, Loss: 0.0601 | 0.0342
Epoch 84/300, Loss: 0.0601 | 0.0342
Epoch 85/300, Loss: 0.0600 | 0.0344
Epoch 86/300, Loss: 0.0600 | 0.0344
Epoch 87/300, Loss: 0.0599 | 0.0344
Epoch 88/300, Loss: 0.0599 | 0.0344
Epoch 89/300, Loss: 0.0599 | 0.0344
Epoch 90/300, Loss: 0.0599 | 0.0344
Epoch 91/300, Loss: 0.0599 | 0.0342
Epoch 92/300, Loss: 0.0599 | 0.0342
Epoch 93/300, Loss: 0.0599 | 0.0341
Epoch 94/300, Loss: 0.0599 | 0.0341
Epoch 95/300, Loss: 0.0599 | 0.0340
Epoch 96/300, Loss: 0.0599 | 0.0340
Epoch 97/300, Loss: 0.0599 | 0.0340
Epoch 98/300, Loss: 0.0600 | 0.0340
Epoch 99/300, Loss: 0.0599 | 0.0340
Epoch 100/300, Loss: 0.0599 | 0.0340
Epoch 101/300, Loss: 0.0598 | 0.0340
Epoch 102/300, Loss: 0.0598 | 0.0339
Epoch 103/300, Loss: 0.0598 | 0.0340
Epoch 104/300, Loss: 0.0598 | 0.0339
Epoch 105/300, Loss: 0.0597 | 0.0339
Epoch 106/300, Loss: 0.0597 | 0.0338
Epoch 107/300, Loss: 0.0597 | 0.0338
Epoch 108/300, Loss: 0.0597 | 0.0337
Epoch 109/300, Loss: 0.0597 | 0.0337
Epoch 110/300, Loss: 0.0598 | 0.0337
Epoch 111/300, Loss: 0.0599 | 0.0337
Epoch 112/300, Loss: 0.0600 | 0.0337
Epoch 113/300, Loss: 0.0599 | 0.0337
Epoch 114/300, Loss: 0.0599 | 0.0337
Epoch 115/300, Loss: 0.0598 | 0.0336
Epoch 116/300, Loss: 0.0596 | 0.0336
Epoch 117/300, Loss: 0.0595 | 0.0337
Epoch 118/300, Loss: 0.0594 | 0.0337
Epoch 119/300, Loss: 0.0594 | 0.0337
Epoch 120/300, Loss: 0.0593 | 0.0337
Epoch 121/300, Loss: 0.0593 | 0.0337
Epoch 122/300, Loss: 0.0593 | 0.0337
Epoch 123/300, Loss: 0.0593 | 0.0337
Epoch 124/300, Loss: 0.0593 | 0.0337
Epoch 125/300, Loss: 0.0593 | 0.0337
Epoch 126/300, Loss: 0.0592 | 0.0337
Epoch 127/300, Loss: 0.0592 | 0.0337
Epoch 128/300, Loss: 0.0592 | 0.0337
Epoch 129/300, Loss: 0.0592 | 0.0337
Epoch 130/300, Loss: 0.0592 | 0.0337
Epoch 131/300, Loss: 0.0592 | 0.0337
Epoch 132/300, Loss: 0.0592 | 0.0337
Epoch 133/300, Loss: 0.0592 | 0.0337
Epoch 134/300, Loss: 0.0592 | 0.0337
Epoch 135/300, Loss: 0.0592 | 0.0337
Epoch 136/300, Loss: 0.0592 | 0.0337
Epoch 137/300, Loss: 0.0592 | 0.0337
Epoch 138/300, Loss: 0.0592 | 0.0337
Epoch 139/300, Loss: 0.0592 | 0.0337
Epoch 140/300, Loss: 0.0592 | 0.0337
Epoch 141/300, Loss: 0.0592 | 0.0337
Epoch 142/300, Loss: 0.0592 | 0.0337
Epoch 143/300, Loss: 0.0592 | 0.0337
Epoch 144/300, Loss: 0.0591 | 0.0337
Epoch 145/300, Loss: 0.0591 | 0.0337
Epoch 146/300, Loss: 0.0591 | 0.0337
Epoch 147/300, Loss: 0.0591 | 0.0337
Epoch 148/300, Loss: 0.0591 | 0.0337
Epoch 149/300, Loss: 0.0591 | 0.0337
Epoch 150/300, Loss: 0.0591 | 0.0337
Epoch 151/300, Loss: 0.0591 | 0.0337
Epoch 152/300, Loss: 0.0591 | 0.0337
Epoch 153/300, Loss: 0.0591 | 0.0337
Epoch 154/300, Loss: 0.0591 | 0.0337
Epoch 155/300, Loss: 0.0591 | 0.0337
Epoch 156/300, Loss: 0.0591 | 0.0337
Epoch 157/300, Loss: 0.0591 | 0.0337
Epoch 158/300, Loss: 0.0591 | 0.0337
Epoch 159/300, Loss: 0.0591 | 0.0337
Epoch 160/300, Loss: 0.0591 | 0.0337
Epoch 161/300, Loss: 0.0591 | 0.0337
Epoch 162/300, Loss: 0.0591 | 0.0337
Epoch 163/300, Loss: 0.0591 | 0.0337
Epoch 164/300, Loss: 0.0591 | 0.0337
Epoch 165/300, Loss: 0.0591 | 0.0337
Epoch 166/300, Loss: 0.0591 | 0.0337
Epoch 167/300, Loss: 0.0591 | 0.0337
Epoch 168/300, Loss: 0.0591 | 0.0337
Epoch 169/300, Loss: 0.0591 | 0.0337
Epoch 170/300, Loss: 0.0591 | 0.0337
Epoch 171/300, Loss: 0.0591 | 0.0337
Epoch 172/300, Loss: 0.0591 | 0.0337
Epoch 173/300, Loss: 0.0591 | 0.0337
Epoch 174/300, Loss: 0.0591 | 0.0337
Epoch 175/300, Loss: 0.0591 | 0.0337
Epoch 176/300, Loss: 0.0591 | 0.0337
Epoch 177/300, Loss: 0.0591 | 0.0337
Epoch 178/300, Loss: 0.0591 | 0.0337
Epoch 179/300, Loss: 0.0591 | 0.0337
Epoch 180/300, Loss: 0.0591 | 0.0337
Epoch 181/300, Loss: 0.0591 | 0.0337
Epoch 182/300, Loss: 0.0591 | 0.0337
Epoch 183/300, Loss: 0.0591 | 0.0337
Epoch 184/300, Loss: 0.0591 | 0.0337
Epoch 185/300, Loss: 0.0591 | 0.0337
Epoch 186/300, Loss: 0.0591 | 0.0337
Epoch 187/300, Loss: 0.0591 | 0.0337
Epoch 188/300, Loss: 0.0591 | 0.0337
Epoch 189/300, Loss: 0.0591 | 0.0337
Epoch 190/300, Loss: 0.0591 | 0.0337
Epoch 191/300, Loss: 0.0591 | 0.0337
Epoch 192/300, Loss: 0.0591 | 0.0337
Epoch 193/300, Loss: 0.0591 | 0.0337
Epoch 194/300, Loss: 0.0591 | 0.0337
Epoch 195/300, Loss: 0.0591 | 0.0337
Epoch 196/300, Loss: 0.0591 | 0.0337
Epoch 197/300, Loss: 0.0591 | 0.0337
Epoch 198/300, Loss: 0.0591 | 0.0337
Epoch 199/300, Loss: 0.0591 | 0.0337
Epoch 200/300, Loss: 0.0591 | 0.0337
Epoch 201/300, Loss: 0.0591 | 0.0337
Epoch 202/300, Loss: 0.0591 | 0.0337
Epoch 203/300, Loss: 0.0591 | 0.0337
Epoch 204/300, Loss: 0.0591 | 0.0337
Epoch 205/300, Loss: 0.0591 | 0.0337
Epoch 206/300, Loss: 0.0591 | 0.0337
Epoch 207/300, Loss: 0.0591 | 0.0337
Epoch 208/300, Loss: 0.0591 | 0.0337
Epoch 209/300, Loss: 0.0591 | 0.0337
Epoch 210/300, Loss: 0.0591 | 0.0337
Epoch 211/300, Loss: 0.0591 | 0.0337
Epoch 212/300, Loss: 0.0591 | 0.0337
Epoch 213/300, Loss: 0.0591 | 0.0337
Epoch 214/300, Loss: 0.0591 | 0.0337
Epoch 215/300, Loss: 0.0591 | 0.0337
Epoch 216/300, Loss: 0.0591 | 0.0337
Epoch 217/300, Loss: 0.0591 | 0.0337
Epoch 218/300, Loss: 0.0591 | 0.0337
Epoch 219/300, Loss: 0.0591 | 0.0337
Epoch 220/300, Loss: 0.0591 | 0.0337
Epoch 221/300, Loss: 0.0591 | 0.0337
Epoch 222/300, Loss: 0.0591 | 0.0337
Epoch 223/300, Loss: 0.0591 | 0.0337
Epoch 224/300, Loss: 0.0591 | 0.0337
Epoch 225/300, Loss: 0.0591 | 0.0337
Epoch 226/300, Loss: 0.0591 | 0.0337
Epoch 227/300, Loss: 0.0591 | 0.0337
Epoch 228/300, Loss: 0.0591 | 0.0337
Epoch 229/300, Loss: 0.0591 | 0.0337
Epoch 230/300, Loss: 0.0591 | 0.0337
Epoch 231/300, Loss: 0.0591 | 0.0337
Epoch 232/300, Loss: 0.0591 | 0.0337
Epoch 233/300, Loss: 0.0591 | 0.0337
Epoch 234/300, Loss: 0.0591 | 0.0337
Epoch 235/300, Loss: 0.0591 | 0.0337
Epoch 236/300, Loss: 0.0591 | 0.0337
Epoch 237/300, Loss: 0.0591 | 0.0337
Epoch 238/300, Loss: 0.0591 | 0.0337
Epoch 239/300, Loss: 0.0591 | 0.0337
Epoch 240/300, Loss: 0.0591 | 0.0337
Epoch 241/300, Loss: 0.0591 | 0.0337
Epoch 242/300, Loss: 0.0591 | 0.0337
Epoch 243/300, Loss: 0.0591 | 0.0337
Epoch 244/300, Loss: 0.0591 | 0.0337
Epoch 245/300, Loss: 0.0591 | 0.0337
Epoch 246/300, Loss: 0.0591 | 0.0337
Epoch 247/300, Loss: 0.0591 | 0.0337
Epoch 248/300, Loss: 0.0591 | 0.0337
Epoch 249/300, Loss: 0.0591 | 0.0337
Epoch 250/300, Loss: 0.0591 | 0.0337
Epoch 251/300, Loss: 0.0591 | 0.0337
Epoch 252/300, Loss: 0.0591 | 0.0337
Epoch 253/300, Loss: 0.0591 | 0.0337
Epoch 254/300, Loss: 0.0591 | 0.0337
Epoch 255/300, Loss: 0.0591 | 0.0337
Epoch 256/300, Loss: 0.0591 | 0.0337
Epoch 257/300, Loss: 0.0591 | 0.0337
Epoch 258/300, Loss: 0.0591 | 0.0337
Epoch 259/300, Loss: 0.0591 | 0.0337
Epoch 260/300, Loss: 0.0591 | 0.0337
Epoch 261/300, Loss: 0.0591 | 0.0337
Epoch 262/300, Loss: 0.0591 | 0.0337
Epoch 263/300, Loss: 0.0591 | 0.0337
Epoch 264/300, Loss: 0.0591 | 0.0337
Epoch 265/300, Loss: 0.0591 | 0.0337
Epoch 266/300, Loss: 0.0591 | 0.0337
Epoch 267/300, Loss: 0.0591 | 0.0337
Epoch 268/300, Loss: 0.0591 | 0.0337
Epoch 269/300, Loss: 0.0591 | 0.0337
Epoch 270/300, Loss: 0.0591 | 0.0337
Epoch 271/300, Loss: 0.0591 | 0.0337
Epoch 272/300, Loss: 0.0591 | 0.0337
Epoch 273/300, Loss: 0.0591 | 0.0337
Epoch 274/300, Loss: 0.0591 | 0.0337
Epoch 275/300, Loss: 0.0591 | 0.0337
Epoch 276/300, Loss: 0.0591 | 0.0337
Epoch 277/300, Loss: 0.0591 | 0.0337
Epoch 278/300, Loss: 0.0591 | 0.0337
Epoch 279/300, Loss: 0.0591 | 0.0337
Epoch 280/300, Loss: 0.0591 | 0.0337
Epoch 281/300, Loss: 0.0591 | 0.0337
Epoch 282/300, Loss: 0.0591 | 0.0337
Epoch 283/300, Loss: 0.0591 | 0.0337
Epoch 284/300, Loss: 0.0591 | 0.0337
Epoch 285/300, Loss: 0.0591 | 0.0337
Epoch 286/300, Loss: 0.0591 | 0.0337
Epoch 287/300, Loss: 0.0591 | 0.0337
Epoch 288/300, Loss: 0.0591 | 0.0337
Early stopping
Runtime (seconds): 268.3458983898163
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 308.62044452223927
RMSE: 17.567596435546875
MAE: 17.567596435546875
R-squared: nan
[205.12259]
