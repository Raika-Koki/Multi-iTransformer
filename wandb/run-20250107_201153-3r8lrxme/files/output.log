[32m[I 2025-01-07 20:11:54,427][0m A new study created in memory with name: no-name-13e48eed-f0b5-4664-8dec-a1597b4a6ec9[0m
[32m[I 2025-01-07 20:15:13,830][0m Trial 0 finished with value: 1.207865571387336 and parameters: {'observation_period_num': 159, 'train_rates': 0.7514670112640328, 'learning_rate': 0.0005841411443675361, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9304810746146721}. Best is trial 0 with value: 1.207865571387336.[0m
[32m[I 2025-01-07 20:16:37,533][0m Trial 1 finished with value: 1.9899228222248282 and parameters: {'observation_period_num': 71, 'train_rates': 0.7368050753180851, 'learning_rate': 1.853170566747153e-06, 'batch_size': 141, 'step_size': 3, 'gamma': 0.8571835148911486}. Best is trial 0 with value: 1.207865571387336.[0m
[32m[I 2025-01-07 20:19:32,810][0m Trial 2 finished with value: 0.5917412638664246 and parameters: {'observation_period_num': 124, 'train_rates': 0.9881723007726508, 'learning_rate': 7.712601181931854e-06, 'batch_size': 200, 'step_size': 8, 'gamma': 0.7848815449120194}. Best is trial 2 with value: 0.5917412638664246.[0m
[32m[I 2025-01-07 20:20:16,160][0m Trial 3 finished with value: 0.8694272435978971 and parameters: {'observation_period_num': 33, 'train_rates': 0.6125234388427502, 'learning_rate': 9.154134794757236e-05, 'batch_size': 79, 'step_size': 11, 'gamma': 0.9444589846392961}. Best is trial 2 with value: 0.5917412638664246.[0m
[32m[I 2025-01-07 20:21:21,327][0m Trial 4 finished with value: 0.23832331363572723 and parameters: {'observation_period_num': 41, 'train_rates': 0.8979030309962572, 'learning_rate': 0.00047817767653141426, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9071056654506975}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:23:46,627][0m Trial 5 finished with value: 1.8105798711803904 and parameters: {'observation_period_num': 125, 'train_rates': 0.7533273266351497, 'learning_rate': 1.6670194040877676e-06, 'batch_size': 158, 'step_size': 7, 'gamma': 0.7938425637637159}. Best is trial 4 with value: 0.23832331363572723.[0m
Early stopping at epoch 58
[32m[I 2025-01-07 20:25:49,852][0m Trial 6 finished with value: 0.7073479369282722 and parameters: {'observation_period_num': 131, 'train_rates': 0.949243807956681, 'learning_rate': 0.0005263219319058616, 'batch_size': 33, 'step_size': 1, 'gamma': 0.7575351154427865}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:30:59,528][0m Trial 7 finished with value: 1.1084358447658422 and parameters: {'observation_period_num': 239, 'train_rates': 0.6684006940218576, 'learning_rate': 4.214072051616576e-06, 'batch_size': 35, 'step_size': 9, 'gamma': 0.7808902962690227}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:36:26,745][0m Trial 8 finished with value: 1.0832176458911773 and parameters: {'observation_period_num': 237, 'train_rates': 0.7710299023735279, 'learning_rate': 0.0007030039067319262, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7986549804105006}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:37:14,080][0m Trial 9 finished with value: 1.097771286609627 and parameters: {'observation_period_num': 24, 'train_rates': 0.643344655246964, 'learning_rate': 0.00035582034586689417, 'batch_size': 79, 'step_size': 12, 'gamma': 0.9264558316495629}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:38:46,920][0m Trial 10 finished with value: 0.26240667446357446 and parameters: {'observation_period_num': 73, 'train_rates': 0.8727424184516501, 'learning_rate': 7.492645725888105e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9897856240164618}. Best is trial 4 with value: 0.23832331363572723.[0m
[32m[I 2025-01-07 20:40:16,735][0m Trial 11 finished with value: 0.21785115561492385 and parameters: {'observation_period_num': 70, 'train_rates': 0.8842360909999915, 'learning_rate': 6.0461983291425905e-05, 'batch_size': 247, 'step_size': 5, 'gamma': 0.9668715504630189}. Best is trial 11 with value: 0.21785115561492385.[0m
[32m[I 2025-01-07 20:41:42,961][0m Trial 12 finished with value: 0.5587618244372732 and parameters: {'observation_period_num': 67, 'train_rates': 0.8728841153385035, 'learning_rate': 1.767776372790966e-05, 'batch_size': 245, 'step_size': 4, 'gamma': 0.8790361174922151}. Best is trial 11 with value: 0.21785115561492385.[0m
[32m[I 2025-01-07 20:42:08,024][0m Trial 13 finished with value: 0.14916850406615462 and parameters: {'observation_period_num': 11, 'train_rates': 0.876303351575452, 'learning_rate': 0.0001772711047352395, 'batch_size': 171, 'step_size': 1, 'gamma': 0.9886891839550538}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:42:34,628][0m Trial 14 finished with value: 0.3189729641123516 and parameters: {'observation_period_num': 17, 'train_rates': 0.8332857125832698, 'learning_rate': 0.00011282964048807734, 'batch_size': 204, 'step_size': 1, 'gamma': 0.9758130112089277}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:44:25,080][0m Trial 15 finished with value: 0.3524234948430474 and parameters: {'observation_period_num': 90, 'train_rates': 0.8235354530136763, 'learning_rate': 3.052296734199197e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.9607359008651901}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:48:50,304][0m Trial 16 finished with value: 0.26899074284350816 and parameters: {'observation_period_num': 182, 'train_rates': 0.9258028024122655, 'learning_rate': 0.00013558651717176352, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8885638656222405}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:49:10,896][0m Trial 17 finished with value: 0.26718061262861303 and parameters: {'observation_period_num': 9, 'train_rates': 0.8159177405231133, 'learning_rate': 0.00021638728041433077, 'batch_size': 231, 'step_size': 5, 'gamma': 0.8441758023996566}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:50:22,428][0m Trial 18 finished with value: 0.17978763580322266 and parameters: {'observation_period_num': 49, 'train_rates': 0.9830446424176912, 'learning_rate': 1.6825887404881676e-05, 'batch_size': 114, 'step_size': 10, 'gamma': 0.9643400932380981}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:51:29,928][0m Trial 19 finished with value: 0.30430230498313904 and parameters: {'observation_period_num': 48, 'train_rates': 0.9725123175988262, 'learning_rate': 1.2932628580485545e-05, 'batch_size': 117, 'step_size': 10, 'gamma': 0.8332740058540625}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:53:53,057][0m Trial 20 finished with value: 0.21755691818319833 and parameters: {'observation_period_num': 107, 'train_rates': 0.9273612598508474, 'learning_rate': 4.381271303339556e-05, 'batch_size': 158, 'step_size': 12, 'gamma': 0.9422567800316525}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:56:01,209][0m Trial 21 finished with value: 0.22555145419123185 and parameters: {'observation_period_num': 96, 'train_rates': 0.9308962730755295, 'learning_rate': 3.682836517681541e-05, 'batch_size': 169, 'step_size': 12, 'gamma': 0.9892164975073535}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:56:41,915][0m Trial 22 finished with value: 0.1675194933338732 and parameters: {'observation_period_num': 6, 'train_rates': 0.9179371470396952, 'learning_rate': 3.449503018130239e-05, 'batch_size': 103, 'step_size': 15, 'gamma': 0.9525463188210959}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:57:20,517][0m Trial 23 finished with value: 0.17616617679595947 and parameters: {'observation_period_num': 6, 'train_rates': 0.9729650977705105, 'learning_rate': 2.163542565545435e-05, 'batch_size': 107, 'step_size': 15, 'gamma': 0.9066178059075025}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:57:50,129][0m Trial 24 finished with value: 0.23592240280575222 and parameters: {'observation_period_num': 6, 'train_rates': 0.8452691209531883, 'learning_rate': 0.00020547448197378768, 'batch_size': 137, 'step_size': 15, 'gamma': 0.9110816838924071}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:58:34,319][0m Trial 25 finished with value: 0.38573546943993403 and parameters: {'observation_period_num': 29, 'train_rates': 0.9015264110698533, 'learning_rate': 6.199526873954232e-06, 'batch_size': 107, 'step_size': 15, 'gamma': 0.9002350660712438}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 20:59:50,003][0m Trial 26 finished with value: 0.23943685730556388 and parameters: {'observation_period_num': 54, 'train_rates': 0.9474517137786692, 'learning_rate': 2.4379128172782153e-05, 'batch_size': 94, 'step_size': 13, 'gamma': 0.926670322178004}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:04:50,380][0m Trial 27 finished with value: 0.4264633178110387 and parameters: {'observation_period_num': 206, 'train_rates': 0.8566133337938384, 'learning_rate': 3.256383517549725e-06, 'batch_size': 51, 'step_size': 14, 'gamma': 0.9540233073906259}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:05:12,961][0m Trial 28 finished with value: 0.5639840106136704 and parameters: {'observation_period_num': 9, 'train_rates': 0.7947333730382248, 'learning_rate': 1.0136469173147604e-05, 'batch_size': 183, 'step_size': 15, 'gamma': 0.9435414718985664}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:09:18,505][0m Trial 29 finished with value: 2.633001945715035 and parameters: {'observation_period_num': 167, 'train_rates': 0.9599909931114563, 'learning_rate': 0.0009119445862752954, 'batch_size': 96, 'step_size': 13, 'gamma': 0.9267909496227014}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:10:00,984][0m Trial 30 finished with value: 0.1792823446208033 and parameters: {'observation_period_num': 29, 'train_rates': 0.9112006526441062, 'learning_rate': 5.143512585109576e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9802123688734052}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:10:41,520][0m Trial 31 finished with value: 0.17395688366081755 and parameters: {'observation_period_num': 26, 'train_rates': 0.920093421524454, 'learning_rate': 5.44584701371001e-05, 'batch_size': 141, 'step_size': 8, 'gamma': 0.9804016321990936}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:11:17,216][0m Trial 32 finished with value: 0.21953573990875566 and parameters: {'observation_period_num': 21, 'train_rates': 0.9521495265722533, 'learning_rate': 0.00018131210784005133, 'batch_size': 124, 'step_size': 13, 'gamma': 0.9708068124809561}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:11:44,230][0m Trial 33 finished with value: 1.2099431775031833 and parameters: {'observation_period_num': 7, 'train_rates': 0.9264734910032018, 'learning_rate': 1.0399730597766746e-06, 'batch_size': 153, 'step_size': 7, 'gamma': 0.9522100504372203}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:12:39,468][0m Trial 34 finished with value: 0.2801854786418733 and parameters: {'observation_period_num': 42, 'train_rates': 0.8924884135212946, 'learning_rate': 2.3347951898387054e-05, 'batch_size': 187, 'step_size': 14, 'gamma': 0.865562186165071}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:13:47,671][0m Trial 35 finished with value: 0.5530145158231398 and parameters: {'observation_period_num': 60, 'train_rates': 0.7051047481324887, 'learning_rate': 6.988794585849479e-05, 'batch_size': 92, 'step_size': 3, 'gamma': 0.9351528848597818}. Best is trial 13 with value: 0.14916850406615462.[0m
[32m[I 2025-01-07 21:15:13,764][0m Trial 36 finished with value: 0.08468711376190186 and parameters: {'observation_period_num': 35, 'train_rates': 0.984108534458865, 'learning_rate': 0.00010914061233355704, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9114890474215566}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:16:39,639][0m Trial 37 finished with value: 0.12311156839132309 and parameters: {'observation_period_num': 35, 'train_rates': 0.989959747047167, 'learning_rate': 0.0002983398861010317, 'batch_size': 57, 'step_size': 9, 'gamma': 0.9783072854830934}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:18:47,365][0m Trial 38 finished with value: 0.12472274154424667 and parameters: {'observation_period_num': 86, 'train_rates': 0.9868592985634747, 'learning_rate': 0.00034396131373499013, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9194837487171292}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:20:51,896][0m Trial 39 finished with value: 0.15659369257363406 and parameters: {'observation_period_num': 83, 'train_rates': 0.977138256270503, 'learning_rate': 0.00029554221212198934, 'batch_size': 57, 'step_size': 10, 'gamma': 0.9157847901520452}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:24:53,786][0m Trial 40 finished with value: 0.19752642139792442 and parameters: {'observation_period_num': 141, 'train_rates': 0.9874336414259632, 'learning_rate': 0.00038593054662958295, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8915052611767154}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:27:04,035][0m Trial 41 finished with value: 0.09123800694942474 and parameters: {'observation_period_num': 86, 'train_rates': 0.989456670142579, 'learning_rate': 0.00027148557406259696, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8751041269119842}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:28:32,248][0m Trial 42 finished with value: 0.16884438095301607 and parameters: {'observation_period_num': 37, 'train_rates': 0.9535132003860045, 'learning_rate': 0.00028319993588408695, 'batch_size': 46, 'step_size': 11, 'gamma': 0.822723678992771}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:31:09,640][0m Trial 43 finished with value: 0.77476966381073 and parameters: {'observation_period_num': 107, 'train_rates': 0.9899131441529841, 'learning_rate': 0.0006610985304145093, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8601741099005876}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:33:48,687][0m Trial 44 finished with value: 0.18858101124902374 and parameters: {'observation_period_num': 103, 'train_rates': 0.9643405380661133, 'learning_rate': 0.0001428111868518418, 'batch_size': 34, 'step_size': 11, 'gamma': 0.8761395464513462}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:37:52,136][0m Trial 45 finished with value: 0.8856691857681999 and parameters: {'observation_period_num': 80, 'train_rates': 0.9453986437093226, 'learning_rate': 0.0004228477393534778, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8467563781876494}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:40:42,108][0m Trial 46 finished with value: 0.19724925952713665 and parameters: {'observation_period_num': 121, 'train_rates': 0.9424233996172826, 'learning_rate': 0.00028104109299460637, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8944494992884807}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:42:10,204][0m Trial 47 finished with value: 0.09546620398759842 and parameters: {'observation_period_num': 58, 'train_rates': 0.9897750578302262, 'learning_rate': 9.45447239728128e-05, 'batch_size': 59, 'step_size': 11, 'gamma': 0.8805753552567986}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:43:51,838][0m Trial 48 finished with value: 0.12973105490207673 and parameters: {'observation_period_num': 68, 'train_rates': 0.9655859854148701, 'learning_rate': 9.665317492226891e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.8797116480404893}. Best is trial 36 with value: 0.08468711376190186.[0m
[32m[I 2025-01-07 21:45:40,085][0m Trial 49 finished with value: 1.0651224851608276 and parameters: {'observation_period_num': 60, 'train_rates': 0.9899156824144524, 'learning_rate': 0.0005462091930151043, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9189044619723293}. Best is trial 36 with value: 0.08468711376190186.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7236 | 0.9780
Epoch 2/300, Loss: 0.5671 | 0.6726
Epoch 3/300, Loss: 0.4203 | 0.5987
Epoch 4/300, Loss: 0.3952 | 0.5141
Epoch 5/300, Loss: 0.3325 | 0.4773
Epoch 6/300, Loss: 0.2886 | 0.4048
Epoch 7/300, Loss: 0.2937 | 0.3915
Epoch 8/300, Loss: 0.2877 | 0.3552
Epoch 9/300, Loss: 0.2627 | 0.3227
Epoch 10/300, Loss: 0.2915 | 0.3183
Epoch 11/300, Loss: 0.2628 | 0.2809
Epoch 12/300, Loss: 0.2372 | 0.2979
Epoch 13/300, Loss: 0.2096 | 0.2679
Epoch 14/300, Loss: 0.2099 | 0.2536
Epoch 15/300, Loss: 0.2126 | 0.2522
Epoch 16/300, Loss: 0.1925 | 0.2313
Epoch 17/300, Loss: 0.1860 | 0.2193
Epoch 18/300, Loss: 0.1879 | 0.2196
Epoch 19/300, Loss: 0.1674 | 0.2097
Epoch 20/300, Loss: 0.1647 | 0.2104
Epoch 21/300, Loss: 0.1719 | 0.1967
Epoch 22/300, Loss: 0.1655 | 0.2043
Epoch 23/300, Loss: 0.1605 | 0.1915
Epoch 24/300, Loss: 0.1674 | 0.1805
Epoch 25/300, Loss: 0.1579 | 0.1863
Epoch 26/300, Loss: 0.1686 | 0.1774
Epoch 27/300, Loss: 0.1622 | 0.1741
Epoch 28/300, Loss: 0.1573 | 0.1726
Epoch 29/300, Loss: 0.1748 | 0.1791
Epoch 30/300, Loss: 0.1663 | 0.1809
Epoch 31/300, Loss: 0.1726 | 0.1662
Epoch 32/300, Loss: 0.1788 | 0.1922
Epoch 33/300, Loss: 0.1582 | 0.1688
Epoch 34/300, Loss: 0.1557 | 0.1598
Epoch 35/300, Loss: 0.1607 | 0.1576
Epoch 36/300, Loss: 0.1406 | 0.1494
Epoch 37/300, Loss: 0.1341 | 0.1464
Epoch 38/300, Loss: 0.1326 | 0.1492
Epoch 39/300, Loss: 0.1292 | 0.1436
Epoch 40/300, Loss: 0.1295 | 0.1375
Epoch 41/300, Loss: 0.1335 | 0.1398
Epoch 42/300, Loss: 0.1262 | 0.1356
Epoch 43/300, Loss: 0.1269 | 0.1323
Epoch 44/300, Loss: 0.1288 | 0.1338
Epoch 45/300, Loss: 0.1221 | 0.1285
Epoch 46/300, Loss: 0.1230 | 0.1279
Epoch 47/300, Loss: 0.1186 | 0.1236
Epoch 48/300, Loss: 0.1172 | 0.1293
Epoch 49/300, Loss: 0.1194 | 0.1203
Epoch 50/300, Loss: 0.1174 | 0.1205
Epoch 51/300, Loss: 0.1117 | 0.1158
Epoch 52/300, Loss: 0.1110 | 0.1169
Epoch 53/300, Loss: 0.1075 | 0.1105
Epoch 54/300, Loss: 0.1063 | 0.1127
Epoch 55/300, Loss: 0.1090 | 0.1136
Epoch 56/300, Loss: 0.1076 | 0.1116
Epoch 57/300, Loss: 0.1065 | 0.1102
Epoch 58/300, Loss: 0.1098 | 0.1139
Epoch 59/300, Loss: 0.1077 | 0.1090
Epoch 60/300, Loss: 0.1061 | 0.1152
Epoch 61/300, Loss: 0.1082 | 0.1098
Epoch 62/300, Loss: 0.1084 | 0.1029
Epoch 63/300, Loss: 0.1070 | 0.1149
Epoch 64/300, Loss: 0.1030 | 0.1055
Epoch 65/300, Loss: 0.1038 | 0.1015
Epoch 66/300, Loss: 0.1050 | 0.1112
Epoch 67/300, Loss: 0.1025 | 0.1004
Epoch 68/300, Loss: 0.1009 | 0.1018
Epoch 69/300, Loss: 0.1025 | 0.1042
Epoch 70/300, Loss: 0.0984 | 0.1011
Epoch 71/300, Loss: 0.0977 | 0.1014
Epoch 72/300, Loss: 0.0969 | 0.0973
Epoch 73/300, Loss: 0.0959 | 0.0991
Epoch 74/300, Loss: 0.0953 | 0.0965
Epoch 75/300, Loss: 0.0930 | 0.0933
Epoch 76/300, Loss: 0.0923 | 0.0945
Epoch 77/300, Loss: 0.0931 | 0.0960
Epoch 78/300, Loss: 0.0910 | 0.0914
Epoch 79/300, Loss: 0.0909 | 0.0920
Epoch 80/300, Loss: 0.0916 | 0.0936
Epoch 81/300, Loss: 0.0899 | 0.0908
Epoch 82/300, Loss: 0.0888 | 0.0910
Epoch 83/300, Loss: 0.0888 | 0.0927
Epoch 84/300, Loss: 0.0879 | 0.0892
Epoch 85/300, Loss: 0.0882 | 0.0899
Epoch 86/300, Loss: 0.0874 | 0.0907
Epoch 87/300, Loss: 0.0868 | 0.0876
Epoch 88/300, Loss: 0.0864 | 0.0887
Epoch 89/300, Loss: 0.0864 | 0.0911
Epoch 90/300, Loss: 0.0861 | 0.0876
Epoch 91/300, Loss: 0.0868 | 0.0872
Epoch 92/300, Loss: 0.0854 | 0.0896
Epoch 93/300, Loss: 0.0860 | 0.0851
Epoch 94/300, Loss: 0.0845 | 0.0863
Epoch 95/300, Loss: 0.0835 | 0.0864
Epoch 96/300, Loss: 0.0833 | 0.0870
Epoch 97/300, Loss: 0.0832 | 0.0851
Epoch 98/300, Loss: 0.0822 | 0.0853
Epoch 99/300, Loss: 0.0815 | 0.0844
Epoch 100/300, Loss: 0.0817 | 0.0854
Epoch 101/300, Loss: 0.0814 | 0.0837
Epoch 102/300, Loss: 0.0811 | 0.0846
Epoch 103/300, Loss: 0.0815 | 0.0825
Epoch 104/300, Loss: 0.0802 | 0.0827
Epoch 105/300, Loss: 0.0802 | 0.0838
Epoch 106/300, Loss: 0.0792 | 0.0820
Epoch 107/300, Loss: 0.0793 | 0.0838
Epoch 108/300, Loss: 0.0795 | 0.0815
Epoch 109/300, Loss: 0.0783 | 0.0815
Epoch 110/300, Loss: 0.0787 | 0.0818
Epoch 111/300, Loss: 0.0781 | 0.0808
Epoch 112/300, Loss: 0.0779 | 0.0808
Epoch 113/300, Loss: 0.0774 | 0.0802
Epoch 114/300, Loss: 0.0768 | 0.0814
Epoch 115/300, Loss: 0.0768 | 0.0803
Epoch 116/300, Loss: 0.0768 | 0.0807
Epoch 117/300, Loss: 0.0765 | 0.0804
Epoch 118/300, Loss: 0.0765 | 0.0799
Epoch 119/300, Loss: 0.0764 | 0.0790
Epoch 120/300, Loss: 0.0757 | 0.0800
Epoch 121/300, Loss: 0.0757 | 0.0785
Epoch 122/300, Loss: 0.0754 | 0.0794
Epoch 123/300, Loss: 0.0751 | 0.0791
Epoch 124/300, Loss: 0.0753 | 0.0792
Epoch 125/300, Loss: 0.0751 | 0.0786
Epoch 126/300, Loss: 0.0751 | 0.0788
Epoch 127/300, Loss: 0.0741 | 0.0783
Epoch 128/300, Loss: 0.0737 | 0.0779
Epoch 129/300, Loss: 0.0740 | 0.0776
Epoch 130/300, Loss: 0.0736 | 0.0784
Epoch 131/300, Loss: 0.0728 | 0.0776
Epoch 132/300, Loss: 0.0732 | 0.0779
Epoch 133/300, Loss: 0.0731 | 0.0782
Epoch 134/300, Loss: 0.0734 | 0.0771
Epoch 135/300, Loss: 0.0735 | 0.0781
Epoch 136/300, Loss: 0.0725 | 0.0777
Epoch 137/300, Loss: 0.0720 | 0.0771
Epoch 138/300, Loss: 0.0721 | 0.0777
Epoch 139/300, Loss: 0.0718 | 0.0767
Epoch 140/300, Loss: 0.0716 | 0.0775
Epoch 141/300, Loss: 0.0715 | 0.0764
Epoch 142/300, Loss: 0.0712 | 0.0765
Epoch 143/300, Loss: 0.0709 | 0.0759
Epoch 144/300, Loss: 0.0709 | 0.0769
Epoch 145/300, Loss: 0.0707 | 0.0771
Epoch 146/300, Loss: 0.0704 | 0.0773
Epoch 147/300, Loss: 0.0705 | 0.0762
Epoch 148/300, Loss: 0.0703 | 0.0763
Epoch 149/300, Loss: 0.0702 | 0.0761
Epoch 150/300, Loss: 0.0703 | 0.0769
Epoch 151/300, Loss: 0.0706 | 0.0764
Epoch 152/300, Loss: 0.0694 | 0.0761
Epoch 153/300, Loss: 0.0705 | 0.0765
Epoch 154/300, Loss: 0.0698 | 0.0769
Epoch 155/300, Loss: 0.0701 | 0.0764
Epoch 156/300, Loss: 0.0701 | 0.0755
Epoch 157/300, Loss: 0.0694 | 0.0758
Epoch 158/300, Loss: 0.0686 | 0.0758
Epoch 159/300, Loss: 0.0688 | 0.0757
Epoch 160/300, Loss: 0.0693 | 0.0749
Epoch 161/300, Loss: 0.0688 | 0.0750
Epoch 162/300, Loss: 0.0683 | 0.0752
Epoch 163/300, Loss: 0.0688 | 0.0747
Epoch 164/300, Loss: 0.0675 | 0.0750
Epoch 165/300, Loss: 0.0679 | 0.0745
Epoch 166/300, Loss: 0.0680 | 0.0747
Epoch 167/300, Loss: 0.0678 | 0.0743
Epoch 168/300, Loss: 0.0678 | 0.0745
Epoch 169/300, Loss: 0.0682 | 0.0749
Epoch 170/300, Loss: 0.0683 | 0.0747
Epoch 171/300, Loss: 0.0683 | 0.0742
Epoch 172/300, Loss: 0.0679 | 0.0745
Epoch 173/300, Loss: 0.0670 | 0.0747
Epoch 174/300, Loss: 0.0672 | 0.0741
Epoch 175/300, Loss: 0.0674 | 0.0741
Epoch 176/300, Loss: 0.0674 | 0.0743
Epoch 177/300, Loss: 0.0676 | 0.0741
Epoch 178/300, Loss: 0.0672 | 0.0746
Epoch 179/300, Loss: 0.0667 | 0.0745
Epoch 180/300, Loss: 0.0675 | 0.0744
Epoch 181/300, Loss: 0.0663 | 0.0741
Epoch 182/300, Loss: 0.0676 | 0.0742
Epoch 183/300, Loss: 0.0665 | 0.0739
Epoch 184/300, Loss: 0.0660 | 0.0737
Epoch 185/300, Loss: 0.0666 | 0.0738
Epoch 186/300, Loss: 0.0663 | 0.0740
Epoch 187/300, Loss: 0.0668 | 0.0739
Epoch 188/300, Loss: 0.0666 | 0.0740
Epoch 189/300, Loss: 0.0663 | 0.0739
Epoch 190/300, Loss: 0.0659 | 0.0737
Epoch 191/300, Loss: 0.0662 | 0.0736
Epoch 192/300, Loss: 0.0659 | 0.0737
Epoch 193/300, Loss: 0.0662 | 0.0737
Epoch 194/300, Loss: 0.0661 | 0.0737
Epoch 195/300, Loss: 0.0660 | 0.0739
Epoch 196/300, Loss: 0.0656 | 0.0739
Epoch 197/300, Loss: 0.0659 | 0.0733
Epoch 198/300, Loss: 0.0656 | 0.0733
Epoch 199/300, Loss: 0.0655 | 0.0728
Epoch 200/300, Loss: 0.0660 | 0.0733
Epoch 201/300, Loss: 0.0653 | 0.0734
Epoch 202/300, Loss: 0.0657 | 0.0734
Epoch 203/300, Loss: 0.0650 | 0.0730
Epoch 204/300, Loss: 0.0651 | 0.0733
Epoch 205/300, Loss: 0.0653 | 0.0733
Epoch 206/300, Loss: 0.0656 | 0.0732
Epoch 207/300, Loss: 0.0653 | 0.0727
Epoch 208/300, Loss: 0.0650 | 0.0724
Epoch 209/300, Loss: 0.0654 | 0.0725
Epoch 210/300, Loss: 0.0647 | 0.0726
Epoch 211/300, Loss: 0.0655 | 0.0728
Epoch 212/300, Loss: 0.0647 | 0.0728
Epoch 213/300, Loss: 0.0656 | 0.0726
Epoch 214/300, Loss: 0.0649 | 0.0728
Epoch 215/300, Loss: 0.0648 | 0.0728
Epoch 216/300, Loss: 0.0645 | 0.0726
Epoch 217/300, Loss: 0.0654 | 0.0726
Epoch 218/300, Loss: 0.0650 | 0.0727
Epoch 219/300, Loss: 0.0647 | 0.0724
Epoch 220/300, Loss: 0.0647 | 0.0725
Epoch 221/300, Loss: 0.0644 | 0.0727
Epoch 222/300, Loss: 0.0644 | 0.0724
Epoch 223/300, Loss: 0.0645 | 0.0722
Epoch 224/300, Loss: 0.0645 | 0.0724
Epoch 225/300, Loss: 0.0645 | 0.0720
Epoch 226/300, Loss: 0.0647 | 0.0722
Epoch 227/300, Loss: 0.0646 | 0.0724
Epoch 228/300, Loss: 0.0647 | 0.0724
Epoch 229/300, Loss: 0.0644 | 0.0725
Epoch 230/300, Loss: 0.0641 | 0.0723
Epoch 231/300, Loss: 0.0646 | 0.0720
Epoch 232/300, Loss: 0.0643 | 0.0722
Epoch 233/300, Loss: 0.0648 | 0.0723
Epoch 234/300, Loss: 0.0640 | 0.0724
Epoch 235/300, Loss: 0.0636 | 0.0723
Epoch 236/300, Loss: 0.0643 | 0.0722
Epoch 237/300, Loss: 0.0639 | 0.0722
Epoch 238/300, Loss: 0.0637 | 0.0722
Epoch 239/300, Loss: 0.0644 | 0.0722
Epoch 240/300, Loss: 0.0636 | 0.0722
Epoch 241/300, Loss: 0.0640 | 0.0722
Epoch 242/300, Loss: 0.0641 | 0.0721
Epoch 243/300, Loss: 0.0644 | 0.0721
Epoch 244/300, Loss: 0.0639 | 0.0722
Epoch 245/300, Loss: 0.0642 | 0.0721
Epoch 246/300, Loss: 0.0643 | 0.0721
Epoch 247/300, Loss: 0.0636 | 0.0722
Epoch 248/300, Loss: 0.0643 | 0.0721
Epoch 249/300, Loss: 0.0644 | 0.0720
Epoch 250/300, Loss: 0.0642 | 0.0721
Epoch 251/300, Loss: 0.0639 | 0.0722
Epoch 252/300, Loss: 0.0639 | 0.0720
Epoch 253/300, Loss: 0.0635 | 0.0720
Epoch 254/300, Loss: 0.0643 | 0.0720
Epoch 255/300, Loss: 0.0636 | 0.0721
Epoch 256/300, Loss: 0.0641 | 0.0722
Epoch 257/300, Loss: 0.0637 | 0.0720
Epoch 258/300, Loss: 0.0635 | 0.0719
Epoch 259/300, Loss: 0.0636 | 0.0719
Epoch 260/300, Loss: 0.0635 | 0.0719
Epoch 261/300, Loss: 0.0637 | 0.0720
Epoch 262/300, Loss: 0.0637 | 0.0721
Epoch 263/300, Loss: 0.0639 | 0.0722
Epoch 264/300, Loss: 0.0633 | 0.0721
Epoch 265/300, Loss: 0.0640 | 0.0719
Epoch 266/300, Loss: 0.0637 | 0.0718
Epoch 267/300, Loss: 0.0631 | 0.0719
Epoch 268/300, Loss: 0.0638 | 0.0718
Epoch 269/300, Loss: 0.0632 | 0.0719
Epoch 270/300, Loss: 0.0640 | 0.0721
Epoch 271/300, Loss: 0.0636 | 0.0718
Epoch 272/300, Loss: 0.0640 | 0.0719
Epoch 273/300, Loss: 0.0635 | 0.0719
Epoch 274/300, Loss: 0.0637 | 0.0717
Epoch 275/300, Loss: 0.0638 | 0.0718
Epoch 276/300, Loss: 0.0639 | 0.0719
Epoch 277/300, Loss: 0.0637 | 0.0718
Epoch 278/300, Loss: 0.0637 | 0.0718
Epoch 279/300, Loss: 0.0636 | 0.0718
Epoch 280/300, Loss: 0.0634 | 0.0718
Epoch 281/300, Loss: 0.0636 | 0.0719
Epoch 282/300, Loss: 0.0635 | 0.0719
Epoch 283/300, Loss: 0.0637 | 0.0718
Epoch 284/300, Loss: 0.0634 | 0.0718
Epoch 285/300, Loss: 0.0636 | 0.0717
Epoch 286/300, Loss: 0.0635 | 0.0718
Epoch 287/300, Loss: 0.0634 | 0.0718
Epoch 288/300, Loss: 0.0633 | 0.0718
Epoch 289/300, Loss: 0.0642 | 0.0719
Epoch 290/300, Loss: 0.0633 | 0.0718
Epoch 291/300, Loss: 0.0635 | 0.0719
Epoch 292/300, Loss: 0.0629 | 0.0719
Epoch 293/300, Loss: 0.0632 | 0.0717
Epoch 294/300, Loss: 0.0634 | 0.0717
Epoch 295/300, Loss: 0.0635 | 0.0716
Epoch 296/300, Loss: 0.0631 | 0.0715
Epoch 297/300, Loss: 0.0632 | 0.0715
Epoch 298/300, Loss: 0.0632 | 0.0716
Epoch 299/300, Loss: 0.0636 | 0.0717
Epoch 300/300, Loss: 0.0628 | 0.0717
Runtime (seconds): 221.28877568244934
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 36.51802456588484
RMSE: 6.0430145263671875
MAE: 6.0430145263671875
R-squared: nan
[243.74698]
