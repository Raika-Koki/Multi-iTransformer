/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.13/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
[*********************100%***********************]  1 of 1 completed
(2776,)
(2776,)
(2776,)
{'AAPL': Date
2012-05-18 00:00:00+00:00     18.373452
2012-05-21 00:00:00+00:00     18.357676
2012-05-22 00:00:00+00:00     18.341910
2012-05-23 00:00:00+00:00     18.326156
2012-05-24 00:00:00+00:00     18.310412
                                ...
2023-05-24 00:00:00+00:00    161.422815
2023-05-25 00:00:00+00:00    161.480449
2023-05-26 00:00:00+00:00    161.538080
2023-05-30 00:00:00+00:00    161.595707
2023-05-31 00:00:00+00:00    161.653330
Name: trend, Length: 2776, dtype: float64}
{'AAPL': Date
2012-05-18 00:00:00+00:00    -2.037161
2012-05-21 00:00:00+00:00    -1.339885
2012-05-22 00:00:00+00:00    -1.369429
2012-05-23 00:00:00+00:00    -1.131669
2012-05-24 00:00:00+00:00    -1.129782
                               ...
2023-05-24 00:00:00+00:00    -6.710183
2023-05-25 00:00:00+00:00   -10.178105
2023-05-26 00:00:00+00:00   -10.052277
2023-05-30 00:00:00+00:00    -6.308040
2023-05-31 00:00:00+00:00    -3.033508
Name: season, Length: 2776, dtype: float64}
[*********************100%***********************]  1 of 1 completed
GOOGL
[*********************100%***********************]  1 of 1 completed
META
[*********************100%***********************]  1 of 1 completed
AMZN
[*********************100%***********************]  1 of 1 completed
MSFT
[32m[I 2024-11-01 22:51:24,442][0m A new study created in memory with name: no-name-038e4edb-e2e9-4be0-aeab-e287fcf80306[0m
/data/student/k2110261/Multi-iTransformer/optunademo.py:93: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.13/contextlib.py:109: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[32m[I 2024-11-01 22:51:25,576][0m Trial 0 finished with value: 0.3924754134659628 and parameters: {'learning_rate': 0.00031859314238972863, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8415970316015233, 'depth': 6, 'dim': 74}. Best is trial 0 with value: 0.3924754134659628.[0m
[32m[I 2024-11-01 22:51:25,576][0m A new study created in memory with name: no-name-e135becb-1076-4a44-8f24-de55c8e03fc6[0m
[32m[I 2024-11-01 22:51:26,359][0m Trial 0 finished with value: 1.3049702035281265 and parameters: {'learning_rate': 1.6703184334386682e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.7562445736057188, 'depth': 4, 'dim': 74}. Best is trial 0 with value: 1.3049702035281265.[0m
[32m[I 2024-11-01 22:51:26,359][0m A new study created in memory with name: no-name-48438b31-46bb-4e8a-9891-dc5dd35efa60[0m
[32m[I 2024-11-01 22:51:27,404][0m Trial 0 finished with value: 1.732019614102771 and parameters: {'learning_rate': 4.0473589043623276e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8988368988881641, 'depth': 3, 'dim': 153}. Best is trial 0 with value: 1.732019614102771.[0m
Best hyperparameters (trend): {'learning_rate': 0.00031859314238972863, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8415970316015233, 'depth': 6, 'dim': 74}
Best hyperparameters (seasonal): {'learning_rate': 1.6703184334386682e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.7562445736057188, 'depth': 4, 'dim': 74}
Best hyperparameters (resid): {'learning_rate': 4.0473589043623276e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8988368988881641, 'depth': 3, 'dim': 153}
Epoch 1/100, (Training | Validation) Trend Loss: 0.1564 | 0.4263, Seasonal Loss: 0.3913 | 1.6449, Residual Loss: 0.2788 | 0.9441
Epoch 2/100, (Training | Validation) Trend Loss: 0.7813 | 0.9193, Seasonal Loss: 0.2887 | 1.4469, Residual Loss: 0.1805 | 0.5829
Epoch 3/100, (Training | Validation) Trend Loss: 0.9129 | 0.9962, Seasonal Loss: 0.2557 | 1.3043, Residual Loss: 0.1225 | 0.5533
Epoch 4/100, (Training | Validation) Trend Loss: 0.7797 | 0.5743, Seasonal Loss: 0.2342 | 1.2045, Residual Loss: 0.1128 | 0.4797
Epoch 5/100, (Training | Validation) Trend Loss: 0.1384 | 0.4599, Seasonal Loss: 0.2179 | 1.1316, Residual Loss: 0.0966 | 0.4519
Epoch 6/100, (Training | Validation) Trend Loss: 0.2899 | 0.2421, Seasonal Loss: 0.2046 | 1.0756, Residual Loss: 0.0933 | 0.4223
Epoch 7/100, (Training | Validation) Trend Loss: 0.0715 | 0.3058, Seasonal Loss: 0.1932 | 1.0308, Residual Loss: 0.0871 | 0.4039
Epoch 8/100, (Training | Validation) Trend Loss: 0.1276 | 0.2011, Seasonal Loss: 0.1833 | 0.9934, Residual Loss: 0.0834 | 0.3839
Epoch 9/100, (Training | Validation) Trend Loss: 0.0421 | 0.1916, Seasonal Loss: 0.1746 | 0.9615, Residual Loss: 0.0784 | 0.3703
Epoch 10/100, (Training | Validation) Trend Loss: 0.0462 | 0.1689, Seasonal Loss: 0.1668 | 0.9334, Residual Loss: 0.0747 | 0.3550
Epoch 11/100, (Training | Validation) Trend Loss: 0.0321 | 0.1394, Seasonal Loss: 0.1599 | 0.9139, Residual Loss: 0.0696 | 0.3436
Epoch 12/100, (Training | Validation) Trend Loss: 0.0313 | 0.1340, Seasonal Loss: 0.1550 | 0.8958, Residual Loss: 0.0659 | 0.3306
Epoch 13/100, (Training | Validation) Trend Loss: 0.0430 | 0.1078, Seasonal Loss: 0.1506 | 0.8791, Residual Loss: 0.0609 | 0.3202
Epoch 14/100, (Training | Validation) Trend Loss: 0.0532 | 0.1700, Seasonal Loss: 0.1464 | 0.8634, Residual Loss: 0.0578 | 0.3088
Epoch 15/100, (Training | Validation) Trend Loss: 0.1146 | 0.1134, Seasonal Loss: 0.1426 | 0.8487, Residual Loss: 0.0531 | 0.2992
Epoch 16/100, (Training | Validation) Trend Loss: 0.0780 | 0.2557, Seasonal Loss: 0.1390 | 0.8348, Residual Loss: 0.0501 | 0.2875
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/optunademo.py", line 217, in <module>
    model_seasonal, train_loss_seasonal, valid_loss_seasonal = train(
                                                               ~~~~~^
        model_seasonal, train_data_seasonal, valid_data_seasonal, optimizer_seasonal, criterion, scheduler_seasonal, best_params_seasonal['batch_size'], observation_period_num)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 39, in train
    loss.backward()  # ÈÄÜ‰ºùÊí≠
    ~~~~~~~~~~~~~^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.13/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.13/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.13/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
KeyboardInterrupt
