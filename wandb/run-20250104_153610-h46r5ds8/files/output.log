ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 15:36:11,653][0m A new study created in memory with name: no-name-fcb062a5-f80c-4dff-928d-ef73ad79ed50[0m
[32m[I 2025-01-04 15:38:33,695][0m Trial 0 finished with value: 1.150121187759658 and parameters: {'observation_period_num': 129, 'train_rates': 0.6658868204442712, 'learning_rate': 2.349436179072727e-05, 'batch_size': 192, 'step_size': 8, 'gamma': 0.8177870819868043}. Best is trial 0 with value: 1.150121187759658.[0m
[32m[I 2025-01-04 15:40:17,872][0m Trial 1 finished with value: 1.257711173165092 and parameters: {'observation_period_num': 82, 'train_rates': 0.8828676044561385, 'learning_rate': 1.3032776075736233e-06, 'batch_size': 226, 'step_size': 9, 'gamma': 0.7694832241451826}. Best is trial 0 with value: 1.150121187759658.[0m
[32m[I 2025-01-04 15:41:09,866][0m Trial 2 finished with value: 0.5410278697512043 and parameters: {'observation_period_num': 37, 'train_rates': 0.8623920398852508, 'learning_rate': 1.0929979616439094e-05, 'batch_size': 92, 'step_size': 6, 'gamma': 0.7817907495805838}. Best is trial 2 with value: 0.5410278697512043.[0m
[32m[I 2025-01-04 15:43:02,868][0m Trial 3 finished with value: 1.016600827124413 and parameters: {'observation_period_num': 103, 'train_rates': 0.6815160249506211, 'learning_rate': 8.643022955453397e-06, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8488012995492243}. Best is trial 2 with value: 0.5410278697512043.[0m
[32m[I 2025-01-04 15:47:52,738][0m Trial 4 finished with value: 0.16522730353437823 and parameters: {'observation_period_num': 200, 'train_rates': 0.9118882936914279, 'learning_rate': 0.00014940901024175924, 'batch_size': 175, 'step_size': 5, 'gamma': 0.8718835966250564}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 15:50:05,758][0m Trial 5 finished with value: 0.7057304466989908 and parameters: {'observation_period_num': 113, 'train_rates': 0.752178137658708, 'learning_rate': 2.776243210357016e-05, 'batch_size': 111, 'step_size': 12, 'gamma': 0.8477706948216799}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 15:53:00,653][0m Trial 6 finished with value: 1.2088885241740346 and parameters: {'observation_period_num': 154, 'train_rates': 0.6752379291627553, 'learning_rate': 2.659175213718867e-05, 'batch_size': 112, 'step_size': 2, 'gamma': 0.7934880316767462}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 15:57:04,811][0m Trial 7 finished with value: 1.0311129025651442 and parameters: {'observation_period_num': 215, 'train_rates': 0.6065791488368139, 'learning_rate': 0.00046838626759923503, 'batch_size': 157, 'step_size': 4, 'gamma': 0.7541862955459533}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 15:57:36,083][0m Trial 8 finished with value: 0.3537272286027077 and parameters: {'observation_period_num': 23, 'train_rates': 0.8114415882757742, 'learning_rate': 6.198924901340726e-05, 'batch_size': 187, 'step_size': 7, 'gamma': 0.8150546452646701}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 15:58:31,575][0m Trial 9 finished with value: 0.9202584876852521 and parameters: {'observation_period_num': 47, 'train_rates': 0.837769385518153, 'learning_rate': 7.116445449595234e-06, 'batch_size': 256, 'step_size': 7, 'gamma': 0.7828683219272349}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 16:05:11,886][0m Trial 10 finished with value: 0.7874643310256626 and parameters: {'observation_period_num': 237, 'train_rates': 0.9832371805377413, 'learning_rate': 0.0009295554510070636, 'batch_size': 24, 'step_size': 2, 'gamma': 0.9480555196562991}. Best is trial 4 with value: 0.16522730353437823.[0m
[32m[I 2025-01-04 16:09:50,323][0m Trial 11 finished with value: 0.1317635029554367 and parameters: {'observation_period_num': 190, 'train_rates': 0.9429470243970028, 'learning_rate': 0.00013942726576667182, 'batch_size': 173, 'step_size': 10, 'gamma': 0.9119968223007177}. Best is trial 11 with value: 0.1317635029554367.[0m
[32m[I 2025-01-04 16:14:23,275][0m Trial 12 finished with value: 0.1179933175444603 and parameters: {'observation_period_num': 186, 'train_rates': 0.9564864710566967, 'learning_rate': 0.0001409470186357949, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9149476096378365}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:18:46,273][0m Trial 13 finished with value: 0.1200028583407402 and parameters: {'observation_period_num': 180, 'train_rates': 0.9722958873535196, 'learning_rate': 0.00017148347888569992, 'batch_size': 144, 'step_size': 11, 'gamma': 0.9316361253510846}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:22:46,288][0m Trial 14 finished with value: 0.13042211532592773 and parameters: {'observation_period_num': 164, 'train_rates': 0.9749257201237178, 'learning_rate': 0.00024654267093218203, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9882196230953151}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:28:57,303][0m Trial 15 finished with value: 0.1450487858733756 and parameters: {'observation_period_num': 243, 'train_rates': 0.9156322656199165, 'learning_rate': 7.153732258483333e-05, 'batch_size': 68, 'step_size': 15, 'gamma': 0.9157741545907828}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:32:28,057][0m Trial 16 finished with value: 0.6708662583277776 and parameters: {'observation_period_num': 170, 'train_rates': 0.7664860022786347, 'learning_rate': 0.0004091842871545722, 'batch_size': 214, 'step_size': 11, 'gamma': 0.91761237255192}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:35:50,457][0m Trial 17 finished with value: 0.15248765814162435 and parameters: {'observation_period_num': 142, 'train_rates': 0.9402447145831154, 'learning_rate': 7.876745540339219e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.9575282122530919}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:40:12,185][0m Trial 18 finished with value: 1.9905405258310251 and parameters: {'observation_period_num': 182, 'train_rates': 0.8953948240867415, 'learning_rate': 0.0007234103421332423, 'batch_size': 48, 'step_size': 10, 'gamma': 0.8973581913420527}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:45:45,244][0m Trial 19 finished with value: 0.12433043867349625 and parameters: {'observation_period_num': 212, 'train_rates': 0.9851320393728991, 'learning_rate': 0.0002104113554732069, 'batch_size': 114, 'step_size': 9, 'gamma': 0.9515130352832759}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:47:37,597][0m Trial 20 finished with value: 1.04528434623575 and parameters: {'observation_period_num': 91, 'train_rates': 0.8513178630861943, 'learning_rate': 2.0533273800997133e-06, 'batch_size': 211, 'step_size': 15, 'gamma': 0.884908172402841}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:53:12,282][0m Trial 21 finished with value: 0.1278710961341858 and parameters: {'observation_period_num': 216, 'train_rates': 0.9862603529453449, 'learning_rate': 0.00019911704709285255, 'batch_size': 127, 'step_size': 9, 'gamma': 0.9483255116157929}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 16:58:36,497][0m Trial 22 finished with value: 0.12659034132957458 and parameters: {'observation_period_num': 214, 'train_rates': 0.9543005000201319, 'learning_rate': 0.0003144825894544899, 'batch_size': 155, 'step_size': 11, 'gamma': 0.9742082733139423}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:03:12,831][0m Trial 23 finished with value: 0.13301127621105738 and parameters: {'observation_period_num': 189, 'train_rates': 0.9368511555352357, 'learning_rate': 0.00010532146569198994, 'batch_size': 117, 'step_size': 9, 'gamma': 0.9350699618255217}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:09:07,548][0m Trial 24 finished with value: 0.13576704683234392 and parameters: {'observation_period_num': 229, 'train_rates': 0.962439184039227, 'learning_rate': 0.0002871202096994653, 'batch_size': 91, 'step_size': 13, 'gamma': 0.9310072556422333}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:12:31,521][0m Trial 25 finished with value: 0.1754885782670771 and parameters: {'observation_period_num': 146, 'train_rates': 0.9167038487819832, 'learning_rate': 4.959421135561019e-05, 'batch_size': 157, 'step_size': 11, 'gamma': 0.972505944000639}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:19:21,422][0m Trial 26 finished with value: 0.463178813457489 and parameters: {'observation_period_num': 249, 'train_rates': 0.9892395638453596, 'learning_rate': 0.0005475280600341948, 'batch_size': 61, 'step_size': 8, 'gamma': 0.890436073021965}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:23:07,976][0m Trial 27 finished with value: 0.2623967010350454 and parameters: {'observation_period_num': 168, 'train_rates': 0.8796456730739238, 'learning_rate': 4.728888467769868e-05, 'batch_size': 136, 'step_size': 10, 'gamma': 0.929912303635564}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:27:45,663][0m Trial 28 finished with value: 0.3724089273853758 and parameters: {'observation_period_num': 204, 'train_rates': 0.8252411772848509, 'learning_rate': 0.00014139901423731443, 'batch_size': 105, 'step_size': 7, 'gamma': 0.8995977395445858}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:30:04,717][0m Trial 29 finished with value: 0.8365973327308893 and parameters: {'observation_period_num': 124, 'train_rates': 0.7285932196938459, 'learning_rate': 1.4556039280722007e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.9638716677233301}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:35:10,217][0m Trial 30 finished with value: 0.48976359639251443 and parameters: {'observation_period_num': 228, 'train_rates': 0.791099113823032, 'learning_rate': 0.00020443814744732007, 'batch_size': 172, 'step_size': 4, 'gamma': 0.8482971946524356}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:40:24,808][0m Trial 31 finished with value: 0.17518314465977788 and parameters: {'observation_period_num': 209, 'train_rates': 0.9444965788901871, 'learning_rate': 0.0003792255076343863, 'batch_size': 152, 'step_size': 11, 'gamma': 0.9862131338351497}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:44:50,263][0m Trial 32 finished with value: 0.16448436677455902 and parameters: {'observation_period_num': 180, 'train_rates': 0.9572341108222641, 'learning_rate': 0.00030299827595947827, 'batch_size': 125, 'step_size': 9, 'gamma': 0.9729892302782858}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:50:19,002][0m Trial 33 finished with value: 0.24947891700149372 and parameters: {'observation_period_num': 225, 'train_rates': 0.8895929176888335, 'learning_rate': 0.00010007920182290182, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9456220324243498}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:55:19,099][0m Trial 34 finished with value: 0.12195904552936554 and parameters: {'observation_period_num': 196, 'train_rates': 0.9646560216874914, 'learning_rate': 0.00018853119762990092, 'batch_size': 188, 'step_size': 13, 'gamma': 0.9682453927331689}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 17:59:51,140][0m Trial 35 finished with value: 0.15351177752017975 and parameters: {'observation_period_num': 192, 'train_rates': 0.9243521819116116, 'learning_rate': 0.00016158981973079086, 'batch_size': 237, 'step_size': 13, 'gamma': 0.8714902496840279}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:03:45,202][0m Trial 36 finished with value: 0.16286605596542358 and parameters: {'observation_period_num': 162, 'train_rates': 0.9646725919693127, 'learning_rate': 3.801126267725456e-05, 'batch_size': 200, 'step_size': 14, 'gamma': 0.936929780832909}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:06:23,163][0m Trial 37 finished with value: 0.3504863228553381 and parameters: {'observation_period_num': 128, 'train_rates': 0.8622519560598149, 'learning_rate': 0.00010674671289846739, 'batch_size': 184, 'step_size': 8, 'gamma': 0.9592613830492057}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:11:17,422][0m Trial 38 finished with value: 0.16306160655417462 and parameters: {'observation_period_num': 200, 'train_rates': 0.9082631284673931, 'learning_rate': 0.00020196869547758027, 'batch_size': 78, 'step_size': 14, 'gamma': 0.9083277829629891}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:12:23,451][0m Trial 39 finished with value: 1.054168045697546 and parameters: {'observation_period_num': 58, 'train_rates': 0.7088190215503688, 'learning_rate': 0.0006253702681168916, 'batch_size': 101, 'step_size': 12, 'gamma': 0.9222936823728279}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:16:02,780][0m Trial 40 finished with value: 0.24163007736206055 and parameters: {'observation_period_num': 152, 'train_rates': 0.9720273832925971, 'learning_rate': 1.7970705394206387e-05, 'batch_size': 170, 'step_size': 9, 'gamma': 0.8826558571266068}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:20:23,052][0m Trial 41 finished with value: 0.1632581651210785 and parameters: {'observation_period_num': 178, 'train_rates': 0.9519075819246182, 'learning_rate': 0.0003841125046083265, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9783828635003895}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:25:35,211][0m Trial 42 finished with value: 0.16261262955641984 and parameters: {'observation_period_num': 210, 'train_rates': 0.9264818716133161, 'learning_rate': 0.00025759434582252495, 'batch_size': 127, 'step_size': 11, 'gamma': 0.9628695329704443}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:29:45,029][0m Trial 43 finished with value: 1.1690543150947557 and parameters: {'observation_period_num': 219, 'train_rates': 0.6187513963751615, 'learning_rate': 0.0009057808109535351, 'batch_size': 165, 'step_size': 13, 'gamma': 0.9459029256452758}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:34:46,110][0m Trial 44 finished with value: 0.13564467430114746 and parameters: {'observation_period_num': 198, 'train_rates': 0.9632236738759272, 'learning_rate': 0.00012813905373135887, 'batch_size': 189, 'step_size': 6, 'gamma': 0.9713932067727367}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:40:32,704][0m Trial 45 finished with value: 0.20185189308041204 and parameters: {'observation_period_num': 236, 'train_rates': 0.8993809761794315, 'learning_rate': 8.579991586815062e-05, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8605299111441268}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:43:54,210][0m Trial 46 finished with value: 0.6398777961730957 and parameters: {'observation_period_num': 138, 'train_rates': 0.989807946274013, 'learning_rate': 3.759377940139845e-06, 'batch_size': 180, 'step_size': 10, 'gamma': 0.8239028860353821}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:47:48,607][0m Trial 47 finished with value: 0.3424044997766403 and parameters: {'observation_period_num': 173, 'train_rates': 0.8729778403467923, 'learning_rate': 0.00032338881190404446, 'batch_size': 208, 'step_size': 14, 'gamma': 0.9527995020516815}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:54:20,267][0m Trial 48 finished with value: 0.15869399905204773 and parameters: {'observation_period_num': 252, 'train_rates': 0.933446583942568, 'learning_rate': 6.025766092373191e-05, 'batch_size': 233, 'step_size': 9, 'gamma': 0.9842952586732139}. Best is trial 12 with value: 0.1179933175444603.[0m
[32m[I 2025-01-04 18:58:57,936][0m Trial 49 finished with value: 0.2771624028682709 and parameters: {'observation_period_num': 187, 'train_rates': 0.970176808814684, 'learning_rate': 0.0005211516637194268, 'batch_size': 118, 'step_size': 10, 'gamma': 0.9056297851532547}. Best is trial 12 with value: 0.1179933175444603.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 18:58:57,943][0m A new study created in memory with name: no-name-c7dd9225-ac74-4d5d-8846-c3536d7751bb[0m
[32m[I 2025-01-04 19:00:06,714][0m Trial 0 finished with value: 0.36972704396361394 and parameters: {'observation_period_num': 6, 'train_rates': 0.8220875877032943, 'learning_rate': 0.00031451875884599327, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8290281912709618}. Best is trial 0 with value: 0.36972704396361394.[0m
[32m[I 2025-01-04 19:01:06,277][0m Trial 1 finished with value: 1.4008240000768142 and parameters: {'observation_period_num': 58, 'train_rates': 0.6199268155356079, 'learning_rate': 1.9466785892659e-06, 'batch_size': 135, 'step_size': 13, 'gamma': 0.8909641129578283}. Best is trial 0 with value: 0.36972704396361394.[0m
[32m[I 2025-01-04 19:01:46,558][0m Trial 2 finished with value: 0.21998481671822254 and parameters: {'observation_period_num': 14, 'train_rates': 0.9332341142655394, 'learning_rate': 2.418190428093995e-05, 'batch_size': 115, 'step_size': 6, 'gamma': 0.8739725734392895}. Best is trial 2 with value: 0.21998481671822254.[0m
[32m[I 2025-01-04 19:03:34,130][0m Trial 3 finished with value: 0.1375773400068283 and parameters: {'observation_period_num': 79, 'train_rates': 0.9770261426413251, 'learning_rate': 0.00015717497282168784, 'batch_size': 176, 'step_size': 5, 'gamma': 0.9255880174433653}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:05:08,888][0m Trial 4 finished with value: 0.4376088766316276 and parameters: {'observation_period_num': 79, 'train_rates': 0.7904572043847252, 'learning_rate': 0.00014533697023511898, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9366258853800707}. Best is trial 3 with value: 0.1375773400068283.[0m
Early stopping at epoch 99
[32m[I 2025-01-04 19:08:52,423][0m Trial 5 finished with value: 0.881421757031636 and parameters: {'observation_period_num': 172, 'train_rates': 0.8209802884663091, 'learning_rate': 2.4832929639861336e-05, 'batch_size': 120, 'step_size': 2, 'gamma': 0.7816055686332981}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:10:24,653][0m Trial 6 finished with value: 1.3045410537468098 and parameters: {'observation_period_num': 10, 'train_rates': 0.8713764234196604, 'learning_rate': 1.5290188431721287e-06, 'batch_size': 43, 'step_size': 1, 'gamma': 0.9608757115066782}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:10:55,042][0m Trial 7 finished with value: 0.6041338608492559 and parameters: {'observation_period_num': 11, 'train_rates': 0.7296169952983784, 'learning_rate': 0.0002482784601679417, 'batch_size': 128, 'step_size': 10, 'gamma': 0.7607927227937351}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:12:58,863][0m Trial 8 finished with value: 1.4321327096872507 and parameters: {'observation_period_num': 120, 'train_rates': 0.6562458071519442, 'learning_rate': 1.3677278226622067e-06, 'batch_size': 160, 'step_size': 4, 'gamma': 0.8803092274745925}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:16:04,557][0m Trial 9 finished with value: 0.16326915751622376 and parameters: {'observation_period_num': 129, 'train_rates': 0.9367585444052723, 'learning_rate': 2.423715843576843e-05, 'batch_size': 90, 'step_size': 10, 'gamma': 0.951938141524653}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:22:13,969][0m Trial 10 finished with value: 0.1873832792043686 and parameters: {'observation_period_num': 233, 'train_rates': 0.9814966805786137, 'learning_rate': 0.0008490068039912736, 'batch_size': 231, 'step_size': 6, 'gamma': 0.8232126278145828}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:25:33,841][0m Trial 11 finished with value: 0.1597212255001068 and parameters: {'observation_period_num': 138, 'train_rates': 0.9849168904917335, 'learning_rate': 5.777615720862995e-05, 'batch_size': 206, 'step_size': 11, 'gamma': 0.9888907989893518}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:29:36,828][0m Trial 12 finished with value: 0.14727771282196045 and parameters: {'observation_period_num': 165, 'train_rates': 0.9843228502279564, 'learning_rate': 8.605246468573197e-05, 'batch_size': 213, 'step_size': 12, 'gamma': 0.9867242267932397}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:34:24,027][0m Trial 13 finished with value: 0.5576816986614186 and parameters: {'observation_period_num': 199, 'train_rates': 0.8995467331535587, 'learning_rate': 7.57242915102186e-06, 'batch_size': 191, 'step_size': 7, 'gamma': 0.9156244896235131}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:36:03,248][0m Trial 14 finished with value: 0.27017461860965114 and parameters: {'observation_period_num': 80, 'train_rates': 0.8840332774881858, 'learning_rate': 9.170395301422431e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9842064606988978}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:39:58,382][0m Trial 15 finished with value: 0.8281470537185669 and parameters: {'observation_period_num': 166, 'train_rates': 0.9404577588925048, 'learning_rate': 0.0009883744953437773, 'batch_size': 176, 'step_size': 15, 'gamma': 0.9187040693784319}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:46:37,213][0m Trial 16 finished with value: 0.2868212163448334 and parameters: {'observation_period_num': 249, 'train_rates': 0.9883326643228285, 'learning_rate': 9.612488349371269e-06, 'batch_size': 214, 'step_size': 4, 'gamma': 0.9690332299539345}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:48:31,613][0m Trial 17 finished with value: 0.7643624875280592 and parameters: {'observation_period_num': 102, 'train_rates': 0.7410918275438809, 'learning_rate': 0.00033669426093032743, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9274763499292156}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:49:28,817][0m Trial 18 finished with value: 0.3333076680382938 and parameters: {'observation_period_num': 45, 'train_rates': 0.8547957980788314, 'learning_rate': 6.703076923775794e-05, 'batch_size': 176, 'step_size': 4, 'gamma': 0.8976577506935085}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:53:11,812][0m Trial 19 finished with value: 0.15368899703025818 and parameters: {'observation_period_num': 158, 'train_rates': 0.9242236275023173, 'learning_rate': 0.0001543621829395766, 'batch_size': 225, 'step_size': 12, 'gamma': 0.8573488143199549}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 19:56:44,758][0m Trial 20 finished with value: 0.7673878184024324 and parameters: {'observation_period_num': 104, 'train_rates': 0.7525747663385768, 'learning_rate': 6.909073977279175e-06, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9538035563867167}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:00:43,095][0m Trial 21 finished with value: 0.16023342311382294 and parameters: {'observation_period_num': 167, 'train_rates': 0.9454034778829751, 'learning_rate': 0.00015726247252487922, 'batch_size': 227, 'step_size': 12, 'gamma': 0.8434932965243522}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:05:37,143][0m Trial 22 finished with value: 0.19197858818646135 and parameters: {'observation_period_num': 201, 'train_rates': 0.9159696297342649, 'learning_rate': 4.886063859917777e-05, 'batch_size': 192, 'step_size': 15, 'gamma': 0.8543782518318909}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:10:38,572][0m Trial 23 finished with value: 0.159807026386261 and parameters: {'observation_period_num': 200, 'train_rates': 0.9609400065062906, 'learning_rate': 0.00012707705794922744, 'batch_size': 234, 'step_size': 13, 'gamma': 0.8122681672890627}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:14:00,881][0m Trial 24 finished with value: 0.21614652562718903 and parameters: {'observation_period_num': 146, 'train_rates': 0.8972651692661211, 'learning_rate': 0.0004449870198155892, 'batch_size': 206, 'step_size': 11, 'gamma': 0.8994303951966829}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:18:02,593][0m Trial 25 finished with value: 0.3619578207711236 and parameters: {'observation_period_num': 181, 'train_rates': 0.8495198931975627, 'learning_rate': 0.0005110228767796547, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8639255223450522}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:20:31,074][0m Trial 26 finished with value: 0.16140778362751007 and parameters: {'observation_period_num': 109, 'train_rates': 0.9679195412974481, 'learning_rate': 0.00018032481799007692, 'batch_size': 190, 'step_size': 3, 'gamma': 0.9400163677642086}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:24:06,617][0m Trial 27 finished with value: 0.1924849901987419 and parameters: {'observation_period_num': 155, 'train_rates': 0.9148779409970649, 'learning_rate': 8.083036826305269e-05, 'batch_size': 238, 'step_size': 9, 'gamma': 0.9097228777831002}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:26:02,225][0m Trial 28 finished with value: 0.19140374660491943 and parameters: {'observation_period_num': 86, 'train_rates': 0.9588595682149239, 'learning_rate': 5.055289936450683e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.7941203702564146}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:27:01,255][0m Trial 29 finished with value: 0.5932713341712952 and parameters: {'observation_period_num': 49, 'train_rates': 0.8192064250547275, 'learning_rate': 1.4354869197841681e-05, 'batch_size': 213, 'step_size': 9, 'gamma': 0.8350657054784623}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:31:31,650][0m Trial 30 finished with value: 0.9502167805349488 and parameters: {'observation_period_num': 218, 'train_rates': 0.7036521845613964, 'learning_rate': 0.0002466115700225493, 'batch_size': 90, 'step_size': 5, 'gamma': 0.9714138644609076}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:34:53,271][0m Trial 31 finished with value: 0.1633421778678894 and parameters: {'observation_period_num': 139, 'train_rates': 0.9823390249339133, 'learning_rate': 4.073015582536318e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.9899114328136129}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:38:07,160][0m Trial 32 finished with value: 0.15185698866844177 and parameters: {'observation_period_num': 133, 'train_rates': 0.988773510749701, 'learning_rate': 9.711649567421785e-05, 'batch_size': 218, 'step_size': 11, 'gamma': 0.9739556144840265}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:41:43,089][0m Trial 33 finished with value: 0.17748153236295497 and parameters: {'observation_period_num': 153, 'train_rates': 0.9199846060469667, 'learning_rate': 0.00011244528931710033, 'batch_size': 223, 'step_size': 12, 'gamma': 0.9744934291903188}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:46:11,040][0m Trial 34 finished with value: 0.7385791540145874 and parameters: {'observation_period_num': 184, 'train_rates': 0.9574175321187296, 'learning_rate': 3.2753011170196824e-06, 'batch_size': 243, 'step_size': 14, 'gamma': 0.9445753727478816}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:48:48,312][0m Trial 35 finished with value: 0.1545886993408203 and parameters: {'observation_period_num': 120, 'train_rates': 0.9344046343964603, 'learning_rate': 0.0001988625941326951, 'batch_size': 196, 'step_size': 10, 'gamma': 0.8798471736854585}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:50:20,786][0m Trial 36 finished with value: 0.19582311809062958 and parameters: {'observation_period_num': 68, 'train_rates': 0.9666594528042735, 'learning_rate': 3.612402698999878e-05, 'batch_size': 179, 'step_size': 14, 'gamma': 0.9363942974476573}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:51:10,768][0m Trial 37 finished with value: 0.6038202436277701 and parameters: {'observation_period_num': 41, 'train_rates': 0.7817506587041596, 'learning_rate': 0.00010334072167246895, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9592562641873451}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:53:06,891][0m Trial 38 finished with value: 0.3167048352782859 and parameters: {'observation_period_num': 92, 'train_rates': 0.8683593006201206, 'learning_rate': 0.0003238843168501894, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8564098845329411}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:55:53,407][0m Trial 39 finished with value: 0.1742226756843073 and parameters: {'observation_period_num': 126, 'train_rates': 0.9208407014646799, 'learning_rate': 7.567066324831632e-05, 'batch_size': 218, 'step_size': 13, 'gamma': 0.9286225548069482}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:57:22,977][0m Trial 40 finished with value: 0.19822154939174652 and parameters: {'observation_period_num': 67, 'train_rates': 0.9490896441449855, 'learning_rate': 0.0005479576175544759, 'batch_size': 244, 'step_size': 2, 'gamma': 0.8859072251299217}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 20:59:55,128][0m Trial 41 finished with value: 0.14950889800534103 and parameters: {'observation_period_num': 115, 'train_rates': 0.9305393214298727, 'learning_rate': 0.00022157637385681123, 'batch_size': 196, 'step_size': 10, 'gamma': 0.8673137671444917}. Best is trial 3 with value: 0.1375773400068283.[0m
[32m[I 2025-01-04 21:02:30,183][0m Trial 42 finished with value: 0.1347799301147461 and parameters: {'observation_period_num': 114, 'train_rates': 0.9724456007311626, 'learning_rate': 0.00020878562574481514, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8710186161657933}. Best is trial 42 with value: 0.1347799301147461.[0m
[32m[I 2025-01-04 21:03:11,405][0m Trial 43 finished with value: 0.13056038320064545 and parameters: {'observation_period_num': 28, 'train_rates': 0.976012842544029, 'learning_rate': 0.00023837046847105135, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9790430714180891}. Best is trial 43 with value: 0.13056038320064545.[0m
[32m[I 2025-01-04 21:03:51,613][0m Trial 44 finished with value: 0.1294950246810913 and parameters: {'observation_period_num': 26, 'train_rates': 0.97171594816454, 'learning_rate': 0.00025484983658506644, 'batch_size': 136, 'step_size': 10, 'gamma': 0.9037911712803184}. Best is trial 44 with value: 0.1294950246810913.[0m
[32m[I 2025-01-04 21:04:28,166][0m Trial 45 finished with value: 0.41505080461502075 and parameters: {'observation_period_num': 13, 'train_rates': 0.9708482607243746, 'learning_rate': 0.000673067815917428, 'batch_size': 126, 'step_size': 7, 'gamma': 0.90518839931197}. Best is trial 44 with value: 0.1294950246810913.[0m
[32m[I 2025-01-04 21:05:07,414][0m Trial 46 finished with value: 0.843881504117073 and parameters: {'observation_period_num': 33, 'train_rates': 0.6571500984241783, 'learning_rate': 0.00029457936266989623, 'batch_size': 113, 'step_size': 8, 'gamma': 0.8934103957441866}. Best is trial 44 with value: 0.1294950246810913.[0m
[32m[I 2025-01-04 21:05:46,063][0m Trial 47 finished with value: 0.19643206328347132 and parameters: {'observation_period_num': 26, 'train_rates': 0.8949801452636458, 'learning_rate': 0.00037182161832316675, 'batch_size': 139, 'step_size': 9, 'gamma': 0.9192751055960147}. Best is trial 44 with value: 0.1294950246810913.[0m
[32m[I 2025-01-04 21:07:08,075][0m Trial 48 finished with value: 0.13664641976356506 and parameters: {'observation_period_num': 60, 'train_rates': 0.9750927089685546, 'learning_rate': 0.0001298467715595584, 'batch_size': 101, 'step_size': 11, 'gamma': 0.8748176277434051}. Best is trial 44 with value: 0.1294950246810913.[0m
[32m[I 2025-01-04 21:07:53,522][0m Trial 49 finished with value: 0.539168120252675 and parameters: {'observation_period_num': 24, 'train_rates': 0.950750601232693, 'learning_rate': 0.0007267818692955222, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8769710171067977}. Best is trial 44 with value: 0.1294950246810913.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 21:07:53,530][0m A new study created in memory with name: no-name-a8445423-1bcc-474b-8a42-7443906e8043[0m
[32m[I 2025-01-04 21:09:37,977][0m Trial 0 finished with value: 0.9573329966002648 and parameters: {'observation_period_num': 101, 'train_rates': 0.6177554140265015, 'learning_rate': 0.0001408191823015609, 'batch_size': 173, 'step_size': 6, 'gamma': 0.8670882240759147}. Best is trial 0 with value: 0.9573329966002648.[0m
[32m[I 2025-01-04 21:10:07,278][0m Trial 1 finished with value: 0.15231141448020935 and parameters: {'observation_period_num': 18, 'train_rates': 0.9659272451407266, 'learning_rate': 5.4375066587274174e-05, 'batch_size': 203, 'step_size': 2, 'gamma': 0.9825477435274222}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:11:06,963][0m Trial 2 finished with value: 0.16591934859752655 and parameters: {'observation_period_num': 42, 'train_rates': 0.9784787471854972, 'learning_rate': 4.789733712203876e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.7686136529804508}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:13:43,325][0m Trial 3 finished with value: 0.7538341067054055 and parameters: {'observation_period_num': 136, 'train_rates': 0.6953344749406214, 'learning_rate': 3.803124596126785e-05, 'batch_size': 124, 'step_size': 13, 'gamma': 0.9640824742150411}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:14:51,737][0m Trial 4 finished with value: 0.8642762363515213 and parameters: {'observation_period_num': 39, 'train_rates': 0.6702418678055823, 'learning_rate': 0.00014124449139266205, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9845975476245132}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:15:24,646][0m Trial 5 finished with value: 0.23337640274654736 and parameters: {'observation_period_num': 8, 'train_rates': 0.8618753491958469, 'learning_rate': 6.814663419348758e-05, 'batch_size': 148, 'step_size': 3, 'gamma': 0.9876063452676}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:16:38,859][0m Trial 6 finished with value: 1.3095038182065566 and parameters: {'observation_period_num': 58, 'train_rates': 0.8859119554733432, 'learning_rate': 1.3668528800492062e-06, 'batch_size': 147, 'step_size': 9, 'gamma': 0.7724900702059828}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:17:42,149][0m Trial 7 finished with value: 2.0301599508963974 and parameters: {'observation_period_num': 57, 'train_rates': 0.742874165506574, 'learning_rate': 1.4527325985194535e-06, 'batch_size': 229, 'step_size': 5, 'gamma': 0.824393483244867}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:21:57,337][0m Trial 8 finished with value: 1.3834439349675953 and parameters: {'observation_period_num': 217, 'train_rates': 0.6178547099233193, 'learning_rate': 3.2860358543021873e-06, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8371482652685991}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:23:51,593][0m Trial 9 finished with value: 1.5134400501847267 and parameters: {'observation_period_num': 95, 'train_rates': 0.8041968561208758, 'learning_rate': 1.9073994962676747e-06, 'batch_size': 125, 'step_size': 4, 'gamma': 0.7857000494536938}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:28:05,663][0m Trial 10 finished with value: 0.39595672488212585 and parameters: {'observation_period_num': 176, 'train_rates': 0.9596879385775321, 'learning_rate': 0.000995284057337067, 'batch_size': 249, 'step_size': 1, 'gamma': 0.9256874589071764}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:28:30,541][0m Trial 11 finished with value: 0.25793150067329407 and parameters: {'observation_period_num': 15, 'train_rates': 0.9523556790749391, 'learning_rate': 1.6367251544803027e-05, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9110237515974303}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:30:28,439][0m Trial 12 finished with value: 0.6710823774337769 and parameters: {'observation_period_num': 83, 'train_rates': 0.9842053638645811, 'learning_rate': 1.021438118339053e-05, 'batch_size': 87, 'step_size': 1, 'gamma': 0.9133748099537838}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:33:25,484][0m Trial 13 finished with value: 0.22883545522837295 and parameters: {'observation_period_num': 130, 'train_rates': 0.8971537029165698, 'learning_rate': 0.0003281183123439141, 'batch_size': 193, 'step_size': 7, 'gamma': 0.75716196150642}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:34:18,956][0m Trial 14 finished with value: 0.32655675447823707 and parameters: {'observation_period_num': 37, 'train_rates': 0.9219506478869345, 'learning_rate': 9.25210217695807e-06, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8147511175997907}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:39:58,716][0m Trial 15 finished with value: 0.48735707028568953 and parameters: {'observation_period_num': 242, 'train_rates': 0.8299514449224549, 'learning_rate': 2.594627269965393e-05, 'batch_size': 214, 'step_size': 15, 'gamma': 0.86966543645751}. Best is trial 1 with value: 0.15231141448020935.[0m
[32m[I 2025-01-04 21:43:11,995][0m Trial 16 finished with value: 0.07876238371762964 and parameters: {'observation_period_num': 5, 'train_rates': 0.9268170647427366, 'learning_rate': 7.201083388337451e-05, 'batch_size': 21, 'step_size': 3, 'gamma': 0.9423777074844274}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 21:47:29,494][0m Trial 17 finished with value: 0.12015051710962628 and parameters: {'observation_period_num': 157, 'train_rates': 0.9315710891237712, 'learning_rate': 0.00012010649225729635, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9496136368435552}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 21:51:24,027][0m Trial 18 finished with value: 1.0529311330694902 and parameters: {'observation_period_num': 165, 'train_rates': 0.7616563226349932, 'learning_rate': 0.00044246960385380194, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9392775491838757}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 21:55:37,055][0m Trial 19 finished with value: 0.33703398697483344 and parameters: {'observation_period_num': 174, 'train_rates': 0.8478743219019944, 'learning_rate': 0.00012535072942953588, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9469290898952727}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:00:37,049][0m Trial 20 finished with value: 0.19344521067531936 and parameters: {'observation_period_num': 205, 'train_rates': 0.908648846818751, 'learning_rate': 0.00032660654594353036, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8874074928072251}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:02:29,180][0m Trial 21 finished with value: 0.08406160697340966 and parameters: {'observation_period_num': 5, 'train_rates': 0.9374704818070606, 'learning_rate': 7.432518049801092e-05, 'batch_size': 37, 'step_size': 2, 'gamma': 0.9588906935527758}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:06:17,284][0m Trial 22 finished with value: 0.11070528626441956 and parameters: {'observation_period_num': 151, 'train_rates': 0.9392780776877612, 'learning_rate': 0.00011327808565383408, 'batch_size': 36, 'step_size': 2, 'gamma': 0.958005784482053}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:08:40,896][0m Trial 23 finished with value: 0.40950747239897106 and parameters: {'observation_period_num': 111, 'train_rates': 0.8771882673398339, 'learning_rate': 7.873430271900364e-05, 'batch_size': 87, 'step_size': 1, 'gamma': 0.897249753004139}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:10:37,724][0m Trial 24 finished with value: 0.1437629471684611 and parameters: {'observation_period_num': 76, 'train_rates': 0.9405265535203637, 'learning_rate': 2.3519632598722073e-05, 'batch_size': 45, 'step_size': 2, 'gamma': 0.9630537557673275}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:13:48,637][0m Trial 25 finished with value: 0.6076732859836789 and parameters: {'observation_period_num': 145, 'train_rates': 0.8191723335798252, 'learning_rate': 0.0002564519870915825, 'batch_size': 68, 'step_size': 5, 'gamma': 0.9653225874981487}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:18:55,037][0m Trial 26 finished with value: 0.8138514825060398 and parameters: {'observation_period_num': 202, 'train_rates': 0.9139363542926081, 'learning_rate': 0.000686565125876439, 'batch_size': 44, 'step_size': 2, 'gamma': 0.9304442690437722}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:21:52,921][0m Trial 27 finished with value: 0.11497600376605988 and parameters: {'observation_period_num': 115, 'train_rates': 0.9892418617099127, 'learning_rate': 0.0002053049267136779, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9600087365529566}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:23:19,058][0m Trial 28 finished with value: 0.6010995359369661 and parameters: {'observation_period_num': 71, 'train_rates': 0.7735745645855379, 'learning_rate': 8.18348255585396e-05, 'batch_size': 104, 'step_size': 5, 'gamma': 0.9206763518486806}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:24:12,135][0m Trial 29 finished with value: 0.27979425015483667 and parameters: {'observation_period_num': 26, 'train_rates': 0.857039369423553, 'learning_rate': 0.00018755451139141257, 'batch_size': 74, 'step_size': 6, 'gamma': 0.8580082912510136}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:26:34,187][0m Trial 30 finished with value: 0.2767369437538935 and parameters: {'observation_period_num': 94, 'train_rates': 0.9416943441628614, 'learning_rate': 1.3463207071535279e-05, 'batch_size': 35, 'step_size': 2, 'gamma': 0.8954696963271767}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:29:41,638][0m Trial 31 finished with value: 0.1163612706370132 and parameters: {'observation_period_num': 119, 'train_rates': 0.9850939440142265, 'learning_rate': 0.00019796381377174606, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9687247260748585}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:33:08,106][0m Trial 32 finished with value: 0.0866536945400257 and parameters: {'observation_period_num': 8, 'train_rates': 0.9572628227920328, 'learning_rate': 9.805217381300461e-05, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9509833858135152}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:36:33,144][0m Trial 33 finished with value: 0.10497505868238116 and parameters: {'observation_period_num': 23, 'train_rates': 0.9571089093643674, 'learning_rate': 3.9000094191773236e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.9388835529784878}. Best is trial 16 with value: 0.07876238371762964.[0m
[32m[I 2025-01-04 22:40:40,387][0m Trial 34 finished with value: 0.07520573263147236 and parameters: {'observation_period_num': 5, 'train_rates': 0.9618639406159732, 'learning_rate': 4.3468440312567085e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9379950099051507}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:44:29,134][0m Trial 35 finished with value: 0.149678563392615 and parameters: {'observation_period_num': 6, 'train_rates': 0.8930414379605673, 'learning_rate': 5.242294237500455e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9769128866552836}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:45:49,136][0m Trial 36 finished with value: 0.16592394047313266 and parameters: {'observation_period_num': 51, 'train_rates': 0.9690473593705896, 'learning_rate': 3.299690077837576e-05, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9069616706848149}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:46:33,458][0m Trial 37 finished with value: 0.17081355716776753 and parameters: {'observation_period_num': 30, 'train_rates': 0.9157142898957584, 'learning_rate': 5.9432198103298024e-05, 'batch_size': 107, 'step_size': 1, 'gamma': 0.9799956396722598}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:46:56,866][0m Trial 38 finished with value: 0.1823514392060682 and parameters: {'observation_period_num': 5, 'train_rates': 0.8770589715068825, 'learning_rate': 8.173918550384168e-05, 'batch_size': 171, 'step_size': 6, 'gamma': 0.9349448789090204}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:48:17,427][0m Trial 39 finished with value: 0.14217618107795715 and parameters: {'observation_period_num': 44, 'train_rates': 0.9677519462858721, 'learning_rate': 2.2871107484538873e-05, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9889965536102807}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:48:55,222][0m Trial 40 finished with value: 0.7385935770731865 and parameters: {'observation_period_num': 18, 'train_rates': 0.6068669454109188, 'learning_rate': 4.404437260848831e-05, 'batch_size': 79, 'step_size': 5, 'gamma': 0.9489793365571982}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:52:57,210][0m Trial 41 finished with value: 0.10720914519495434 and parameters: {'observation_period_num': 21, 'train_rates': 0.9541350799443271, 'learning_rate': 3.6853348985112395e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9388761483877609}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:54:54,404][0m Trial 42 finished with value: 0.1092488094807812 and parameters: {'observation_period_num': 32, 'train_rates': 0.9285171177148854, 'learning_rate': 5.607735183695034e-05, 'batch_size': 34, 'step_size': 4, 'gamma': 0.8824733036325505}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:57:23,014][0m Trial 43 finished with value: 0.1259720884940841 and parameters: {'observation_period_num': 18, 'train_rates': 0.9627064877113813, 'learning_rate': 0.0001155205268115593, 'batch_size': 27, 'step_size': 1, 'gamma': 0.9191160595674452}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:58:55,742][0m Trial 44 finished with value: 0.18502467025801078 and parameters: {'observation_period_num': 64, 'train_rates': 0.8993785723740907, 'learning_rate': 8.317627674881399e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.9740894412109241}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 22:59:55,143][0m Trial 45 finished with value: 0.6989577017158095 and parameters: {'observation_period_num': 45, 'train_rates': 0.7061846079649456, 'learning_rate': 4.219662179053921e-05, 'batch_size': 59, 'step_size': 2, 'gamma': 0.9508795402306526}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 23:02:17,713][0m Trial 46 finished with value: 0.12569455180736566 and parameters: {'observation_period_num': 6, 'train_rates': 0.9490387793427094, 'learning_rate': 1.3869153117335545e-05, 'batch_size': 28, 'step_size': 3, 'gamma': 0.927466744003727}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 23:03:46,222][0m Trial 47 finished with value: 0.16792950276718582 and parameters: {'observation_period_num': 31, 'train_rates': 0.9707113391308512, 'learning_rate': 2.615268882218742e-05, 'batch_size': 46, 'step_size': 4, 'gamma': 0.853574595790669}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 23:04:07,590][0m Trial 48 finished with value: 0.8715637176357515 and parameters: {'observation_period_num': 15, 'train_rates': 0.6508764754638027, 'learning_rate': 1.7585063289778734e-05, 'batch_size': 160, 'step_size': 11, 'gamma': 0.9068372932692925}. Best is trial 34 with value: 0.07520573263147236.[0m
[32m[I 2025-01-04 23:06:42,730][0m Trial 49 finished with value: 0.40768393723789703 and parameters: {'observation_period_num': 55, 'train_rates': 0.9247884313748935, 'learning_rate': 5.197476794436749e-06, 'batch_size': 25, 'step_size': 1, 'gamma': 0.942601551749582}. Best is trial 34 with value: 0.07520573263147236.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 23:06:42,737][0m A new study created in memory with name: no-name-83bb391e-f38a-4826-ba09-72820ec84f18[0m
[32m[I 2025-01-04 23:07:24,455][0m Trial 0 finished with value: 0.3696718992828165 and parameters: {'observation_period_num': 12, 'train_rates': 0.8950790869793673, 'learning_rate': 2.2132581536960213e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8232451355785767}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:11:21,011][0m Trial 1 finished with value: 1.059395473210519 and parameters: {'observation_period_num': 195, 'train_rates': 0.7017051888707224, 'learning_rate': 6.0295163791605744e-05, 'batch_size': 185, 'step_size': 3, 'gamma': 0.7544948472017925}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:11:46,666][0m Trial 2 finished with value: 1.0147174561905985 and parameters: {'observation_period_num': 20, 'train_rates': 0.6040162053397128, 'learning_rate': 0.000497038705996522, 'batch_size': 142, 'step_size': 2, 'gamma': 0.9293312781977191}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:15:23,536][0m Trial 3 finished with value: 0.8529339006564396 and parameters: {'observation_period_num': 185, 'train_rates': 0.6957373058543121, 'learning_rate': 0.00010332656887256801, 'batch_size': 165, 'step_size': 15, 'gamma': 0.9185339008563935}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:15:56,300][0m Trial 4 finished with value: 0.6324267862936219 and parameters: {'observation_period_num': 24, 'train_rates': 0.8108669627421354, 'learning_rate': 7.4306544543986146e-06, 'batch_size': 163, 'step_size': 8, 'gamma': 0.8794236310290736}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:19:02,857][0m Trial 5 finished with value: 1.581721993130853 and parameters: {'observation_period_num': 153, 'train_rates': 0.6917444850933109, 'learning_rate': 2.370040699701768e-06, 'batch_size': 49, 'step_size': 7, 'gamma': 0.7530483827040401}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:19:54,557][0m Trial 6 finished with value: 0.9168232594520596 and parameters: {'observation_period_num': 21, 'train_rates': 0.9043523233903623, 'learning_rate': 1.0502754171236698e-06, 'batch_size': 80, 'step_size': 15, 'gamma': 0.866904507728414}. Best is trial 0 with value: 0.3696718992828165.[0m
Early stopping at epoch 78
[32m[I 2025-01-04 23:20:52,448][0m Trial 7 finished with value: 1.0697485507449693 and parameters: {'observation_period_num': 35, 'train_rates': 0.665075068858568, 'learning_rate': 3.4556241620960855e-05, 'batch_size': 46, 'step_size': 1, 'gamma': 0.8311521289234067}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:26:11,321][0m Trial 8 finished with value: 0.9481147853499752 and parameters: {'observation_period_num': 251, 'train_rates': 0.6507216611702391, 'learning_rate': 1.2564343857701299e-05, 'batch_size': 24, 'step_size': 12, 'gamma': 0.7900446056382009}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:27:51,266][0m Trial 9 finished with value: 0.4560579900713008 and parameters: {'observation_period_num': 77, 'train_rates': 0.8404395198280123, 'learning_rate': 1.02985662025495e-05, 'batch_size': 79, 'step_size': 11, 'gamma': 0.7876877100251536}. Best is trial 0 with value: 0.3696718992828165.[0m
[32m[I 2025-01-04 23:30:01,290][0m Trial 10 finished with value: 0.13656796514987946 and parameters: {'observation_period_num': 94, 'train_rates': 0.9814519517950916, 'learning_rate': 0.00021796664032410536, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9860921195455801}. Best is trial 10 with value: 0.13656796514987946.[0m
[32m[I 2025-01-04 23:32:04,866][0m Trial 11 finished with value: 0.1793430894613266 and parameters: {'observation_period_num': 90, 'train_rates': 0.983498550004446, 'learning_rate': 0.0004595039100744149, 'batch_size': 219, 'step_size': 5, 'gamma': 0.9822402687640436}. Best is trial 10 with value: 0.13656796514987946.[0m
[32m[I 2025-01-04 23:34:13,076][0m Trial 12 finished with value: 0.43507733941078186 and parameters: {'observation_period_num': 96, 'train_rates': 0.9729468934937905, 'learning_rate': 0.0007748016413558031, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9884641750581173}. Best is trial 10 with value: 0.13656796514987946.[0m
[32m[I 2025-01-04 23:36:11,225][0m Trial 13 finished with value: 0.11973837018013 and parameters: {'observation_period_num': 87, 'train_rates': 0.9880666205665781, 'learning_rate': 0.00022269585273092574, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9848736071991231}. Best is trial 13 with value: 0.11973837018013.[0m
[32m[I 2025-01-04 23:39:13,667][0m Trial 14 finished with value: 0.14530061185359955 and parameters: {'observation_period_num': 130, 'train_rates': 0.9258832440892307, 'learning_rate': 0.00016870701552022127, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9509606303453519}. Best is trial 13 with value: 0.11973837018013.[0m
[32m[I 2025-01-04 23:40:20,977][0m Trial 15 finished with value: 0.6903424936312216 and parameters: {'observation_period_num': 61, 'train_rates': 0.7573612962587468, 'learning_rate': 0.00020956409517298703, 'batch_size': 215, 'step_size': 9, 'gamma': 0.9578514077172303}. Best is trial 13 with value: 0.11973837018013.[0m
[32m[I 2025-01-04 23:42:54,737][0m Trial 16 finished with value: 0.11600451171398163 and parameters: {'observation_period_num': 116, 'train_rates': 0.9448626790317728, 'learning_rate': 0.0002838365375042452, 'batch_size': 219, 'step_size': 10, 'gamma': 0.895595701248966}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:45:30,501][0m Trial 17 finished with value: 0.36103546092606553 and parameters: {'observation_period_num': 123, 'train_rates': 0.860214833646875, 'learning_rate': 9.63232908637879e-05, 'batch_size': 205, 'step_size': 10, 'gamma': 0.8894069966059516}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:48:33,937][0m Trial 18 finished with value: 0.3326149880886078 and parameters: {'observation_period_num': 131, 'train_rates': 0.9317903766661301, 'learning_rate': 0.0009143466962072244, 'batch_size': 229, 'step_size': 13, 'gamma': 0.9095355949221848}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:52:05,011][0m Trial 19 finished with value: 0.6563127888553132 and parameters: {'observation_period_num': 170, 'train_rates': 0.7667481351294614, 'learning_rate': 0.00032161052968462093, 'batch_size': 190, 'step_size': 8, 'gamma': 0.8608322513551643}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:53:18,668][0m Trial 20 finished with value: 0.12852504518296984 and parameters: {'observation_period_num': 56, 'train_rates': 0.9380141158377536, 'learning_rate': 5.1086729248997484e-05, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9457497359535098}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:54:24,011][0m Trial 21 finished with value: 0.1337422582338441 and parameters: {'observation_period_num': 48, 'train_rates': 0.9455073433529364, 'learning_rate': 4.404682026256229e-05, 'batch_size': 118, 'step_size': 10, 'gamma': 0.9531132824703543}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:55:51,845][0m Trial 22 finished with value: 0.223694949007746 and parameters: {'observation_period_num': 65, 'train_rates': 0.8840046844055286, 'learning_rate': 9.055957401690425e-05, 'batch_size': 124, 'step_size': 13, 'gamma': 0.9309098999772523}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:58:18,972][0m Trial 23 finished with value: 0.13879626989364624 and parameters: {'observation_period_num': 112, 'train_rates': 0.9430215591810058, 'learning_rate': 0.0001851171009957701, 'batch_size': 237, 'step_size': 9, 'gamma': 0.964370437741522}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-04 23:59:26,582][0m Trial 24 finished with value: 0.13631261885166168 and parameters: {'observation_period_num': 50, 'train_rates': 0.9545553166707755, 'learning_rate': 6.197152230851109e-05, 'batch_size': 151, 'step_size': 7, 'gamma': 0.8966954739928549}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:02:46,544][0m Trial 25 finished with value: 0.42132751195529855 and parameters: {'observation_period_num': 151, 'train_rates': 0.8541837052875313, 'learning_rate': 0.0003052389611605808, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9399920510955592}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:04:27,084][0m Trial 26 finished with value: 0.24500128902307078 and parameters: {'observation_period_num': 77, 'train_rates': 0.9174269711176181, 'learning_rate': 2.376819531583569e-05, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8490526473462885}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:06:56,424][0m Trial 27 finished with value: 0.12532736361026764 and parameters: {'observation_period_num': 108, 'train_rates': 0.9890696539726127, 'learning_rate': 0.00013605969354653772, 'batch_size': 121, 'step_size': 7, 'gamma': 0.9721930141949089}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:09:26,548][0m Trial 28 finished with value: 0.1493414342403412 and parameters: {'observation_period_num': 109, 'train_rates': 0.9635753762190514, 'learning_rate': 0.00012233228142661694, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9664038787505316}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:12:47,623][0m Trial 29 finished with value: 0.18167780199706954 and parameters: {'observation_period_num': 146, 'train_rates': 0.8939719203971852, 'learning_rate': 0.0005365794138178453, 'batch_size': 234, 'step_size': 4, 'gamma': 0.9707954281362339}. Best is trial 16 with value: 0.11600451171398163.[0m
[32m[I 2025-01-05 00:13:29,280][0m Trial 30 finished with value: 0.09863005578517914 and parameters: {'observation_period_num': 5, 'train_rates': 0.9835091841359993, 'learning_rate': 0.000315164729450472, 'batch_size': 105, 'step_size': 7, 'gamma': 0.9027917245602047}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:14:16,470][0m Trial 31 finished with value: 0.10617808997631073 and parameters: {'observation_period_num': 10, 'train_rates': 0.9841513167973999, 'learning_rate': 0.00036899637042125656, 'batch_size': 96, 'step_size': 7, 'gamma': 0.9010464684427556}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:15:01,844][0m Trial 32 finished with value: 0.10413839992804405 and parameters: {'observation_period_num': 9, 'train_rates': 0.9603842514170263, 'learning_rate': 0.0003228451231788404, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9086622918110324}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:15:50,527][0m Trial 33 finished with value: 0.13503691842851712 and parameters: {'observation_period_num': 7, 'train_rates': 0.9565327693213868, 'learning_rate': 0.0003658021219634588, 'batch_size': 89, 'step_size': 8, 'gamma': 0.907041822153258}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:16:53,179][0m Trial 34 finished with value: 0.6919017661701549 and parameters: {'observation_period_num': 5, 'train_rates': 0.8880458468377831, 'learning_rate': 0.0006580126852925533, 'batch_size': 63, 'step_size': 4, 'gamma': 0.8898913555049601}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:17:43,516][0m Trial 35 finished with value: 0.15289865118265153 and parameters: {'observation_period_num': 35, 'train_rates': 0.9143610983060713, 'learning_rate': 0.00030084014665743145, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9174185664547003}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:23:05,214][0m Trial 36 finished with value: 0.13282449543476105 and parameters: {'observation_period_num': 212, 'train_rates': 0.9631266550409588, 'learning_rate': 0.0004949935501232951, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8820909793700845}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:23:49,842][0m Trial 37 finished with value: 0.35541101273505077 and parameters: {'observation_period_num': 30, 'train_rates': 0.824748241897573, 'learning_rate': 8.317053348674972e-05, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8490603363974505}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:24:54,558][0m Trial 38 finished with value: 1.1242202480531904 and parameters: {'observation_period_num': 16, 'train_rates': 0.8676490991807645, 'learning_rate': 0.000993368195621426, 'batch_size': 67, 'step_size': 9, 'gamma': 0.9034040056472445}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:25:49,035][0m Trial 39 finished with value: 0.18604939941930587 and parameters: {'observation_period_num': 40, 'train_rates': 0.9103744777364998, 'learning_rate': 0.0003999325997854973, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9243683731606591}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:30:17,622][0m Trial 40 finished with value: 0.8464189580991759 and parameters: {'observation_period_num': 212, 'train_rates': 0.7430535189514443, 'learning_rate': 0.000608835737613782, 'batch_size': 166, 'step_size': 7, 'gamma': 0.8666431832491456}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:31:14,843][0m Trial 41 finished with value: 0.11272430419921875 and parameters: {'observation_period_num': 21, 'train_rates': 0.9844924102713516, 'learning_rate': 0.0002369625930082372, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9343075298702294}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:32:13,400][0m Trial 42 finished with value: 0.34417495360741246 and parameters: {'observation_period_num': 20, 'train_rates': 0.9692902076756749, 'learning_rate': 4.610497894924589e-06, 'batch_size': 84, 'step_size': 6, 'gamma': 0.9348989017669814}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:33:02,434][0m Trial 43 finished with value: 0.8998289253373488 and parameters: {'observation_period_num': 25, 'train_rates': 0.6099655415191696, 'learning_rate': 0.0002722136299950483, 'batch_size': 68, 'step_size': 8, 'gamma': 0.9148948179215795}. Best is trial 30 with value: 0.09863005578517914.[0m
[32m[I 2025-01-05 00:34:43,118][0m Trial 44 finished with value: 0.09355859199636861 and parameters: {'observation_period_num': 13, 'train_rates': 0.935385766699552, 'learning_rate': 0.00017319215956000322, 'batch_size': 43, 'step_size': 4, 'gamma': 0.8790404803864401}. Best is trial 44 with value: 0.09355859199636861.[0m
[32m[I 2025-01-05 00:36:24,047][0m Trial 45 finished with value: 0.1333736628293991 and parameters: {'observation_period_num': 12, 'train_rates': 0.9893511010690456, 'learning_rate': 0.00014134550197657372, 'batch_size': 43, 'step_size': 2, 'gamma': 0.8765490812837095}. Best is trial 44 with value: 0.09355859199636861.[0m
[32m[I 2025-01-05 00:39:34,693][0m Trial 46 finished with value: 0.1315669384267595 and parameters: {'observation_period_num': 42, 'train_rates': 0.9724264755147638, 'learning_rate': 0.00016800115816569994, 'batch_size': 22, 'step_size': 4, 'gamma': 0.8208078945948502}. Best is trial 44 with value: 0.09355859199636861.[0m
[32m[I 2025-01-05 00:41:22,632][0m Trial 47 finished with value: 0.729378025813643 and parameters: {'observation_period_num': 25, 'train_rates': 0.9308095472426876, 'learning_rate': 0.0004475971172235294, 'batch_size': 38, 'step_size': 5, 'gamma': 0.9258018958214652}. Best is trial 44 with value: 0.09355859199636861.[0m
[32m[I 2025-01-05 00:42:03,536][0m Trial 48 finished with value: 0.3344235786167132 and parameters: {'observation_period_num': 5, 'train_rates': 0.8006012547064841, 'learning_rate': 0.0002529776075779401, 'batch_size': 92, 'step_size': 3, 'gamma': 0.8570730291836405}. Best is trial 44 with value: 0.09355859199636861.[0m
[32m[I 2025-01-05 00:43:03,360][0m Trial 49 finished with value: 1.0589842526582036 and parameters: {'observation_period_num': 18, 'train_rates': 0.9534446580403043, 'learning_rate': 0.0006312032371952499, 'batch_size': 73, 'step_size': 6, 'gamma': 0.8803212264845125}. Best is trial 44 with value: 0.09355859199636861.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 00:43:03,368][0m A new study created in memory with name: no-name-61a75c30-fbdf-46f2-a2e1-caf823101c9e[0m
[32m[I 2025-01-05 00:46:20,138][0m Trial 0 finished with value: 0.7154254100033056 and parameters: {'observation_period_num': 159, 'train_rates': 0.7703584306334958, 'learning_rate': 0.00032029710069376765, 'batch_size': 242, 'step_size': 7, 'gamma': 0.9354509878157162}. Best is trial 0 with value: 0.7154254100033056.[0m
[32m[I 2025-01-05 00:47:12,347][0m Trial 1 finished with value: 1.8938686870947117 and parameters: {'observation_period_num': 40, 'train_rates': 0.9297323730466855, 'learning_rate': 2.1598336826391923e-06, 'batch_size': 184, 'step_size': 3, 'gamma': 0.7850258854521328}. Best is trial 0 with value: 0.7154254100033056.[0m
[32m[I 2025-01-05 00:53:31,823][0m Trial 2 finished with value: 0.5237758275899258 and parameters: {'observation_period_num': 247, 'train_rates': 0.8655897259406743, 'learning_rate': 0.00021088236489975176, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9650213815434503}. Best is trial 2 with value: 0.5237758275899258.[0m
[32m[I 2025-01-05 00:58:07,893][0m Trial 3 finished with value: 0.8145767200140306 and parameters: {'observation_period_num': 207, 'train_rates': 0.7935022191095316, 'learning_rate': 0.00031451469299389545, 'batch_size': 110, 'step_size': 8, 'gamma': 0.9655310942865214}. Best is trial 2 with value: 0.5237758275899258.[0m
[32m[I 2025-01-05 01:02:19,450][0m Trial 4 finished with value: 0.8993831929596273 and parameters: {'observation_period_num': 214, 'train_rates': 0.6234552753228526, 'learning_rate': 6.562517731594289e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.8066471529278768}. Best is trial 2 with value: 0.5237758275899258.[0m
[32m[I 2025-01-05 01:05:22,421][0m Trial 5 finished with value: 0.2165487471574582 and parameters: {'observation_period_num': 136, 'train_rates': 0.8857864091237064, 'learning_rate': 0.0004342883294575785, 'batch_size': 246, 'step_size': 4, 'gamma': 0.8264430054741874}. Best is trial 5 with value: 0.2165487471574582.[0m
[32m[I 2025-01-05 01:11:13,045][0m Trial 6 finished with value: 0.6708066249748625 and parameters: {'observation_period_num': 235, 'train_rates': 0.9076593754271665, 'learning_rate': 2.60111296332404e-06, 'batch_size': 140, 'step_size': 12, 'gamma': 0.9085810304626599}. Best is trial 5 with value: 0.2165487471574582.[0m
[32m[I 2025-01-05 01:12:02,707][0m Trial 7 finished with value: 0.1599940061569214 and parameters: {'observation_period_num': 36, 'train_rates': 0.9714788798781719, 'learning_rate': 4.97750038200171e-05, 'batch_size': 232, 'step_size': 7, 'gamma': 0.9234059350014435}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:12:53,068][0m Trial 8 finished with value: 0.18979489567076288 and parameters: {'observation_period_num': 38, 'train_rates': 0.8877228675424831, 'learning_rate': 0.000125770704705277, 'batch_size': 172, 'step_size': 2, 'gamma': 0.9492311916952194}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:13:48,831][0m Trial 9 finished with value: 0.30923082861866985 and parameters: {'observation_period_num': 18, 'train_rates': 0.8051673931078439, 'learning_rate': 0.00014473247191947288, 'batch_size': 64, 'step_size': 5, 'gamma': 0.7597992802667747}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:16:01,694][0m Trial 10 finished with value: 0.2710444927215576 and parameters: {'observation_period_num': 98, 'train_rates': 0.9831453929462409, 'learning_rate': 1.3142851933325701e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.8815172923939947}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:17:37,310][0m Trial 11 finished with value: 0.21362583339214325 and parameters: {'observation_period_num': 69, 'train_rates': 0.9848234451212324, 'learning_rate': 1.950748023478407e-05, 'batch_size': 187, 'step_size': 1, 'gamma': 0.9850973873456778}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:18:49,964][0m Trial 12 finished with value: 0.4762700650102577 and parameters: {'observation_period_num': 60, 'train_rates': 0.8449553213918978, 'learning_rate': 5.2818948787540654e-05, 'batch_size': 148, 'step_size': 1, 'gamma': 0.9197224511191372}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:19:06,417][0m Trial 13 finished with value: 0.830780274801321 and parameters: {'observation_period_num': 5, 'train_rates': 0.709069247822521, 'learning_rate': 0.000908190299891468, 'batch_size': 216, 'step_size': 10, 'gamma': 0.8539434693196659}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:21:10,369][0m Trial 14 finished with value: 0.46432446456346355 and parameters: {'observation_period_num': 94, 'train_rates': 0.9360900446025662, 'learning_rate': 8.517060697322323e-06, 'batch_size': 168, 'step_size': 5, 'gamma': 0.8830200396610783}. Best is trial 7 with value: 0.1599940061569214.[0m
[32m[I 2025-01-05 01:22:06,876][0m Trial 15 finished with value: 0.14377959828445877 and parameters: {'observation_period_num': 38, 'train_rates': 0.9527177679234528, 'learning_rate': 5.636396046367397e-05, 'batch_size': 110, 'step_size': 3, 'gamma': 0.9407746545957902}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:24:21,843][0m Trial 16 finished with value: 0.3502206321420341 and parameters: {'observation_period_num': 99, 'train_rates': 0.949392364172089, 'learning_rate': 5.829141892270863e-06, 'batch_size': 102, 'step_size': 6, 'gamma': 0.9225526522658}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:27:43,652][0m Trial 17 finished with value: 0.8510530035942793 and parameters: {'observation_period_num': 169, 'train_rates': 0.7126319182969978, 'learning_rate': 3.245527895805749e-05, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8937041881764689}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:29:28,724][0m Trial 18 finished with value: 0.1478298306465149 and parameters: {'observation_period_num': 75, 'train_rates': 0.9899571092324809, 'learning_rate': 6.916199795679984e-05, 'batch_size': 125, 'step_size': 14, 'gamma': 0.8556372910042377}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:31:02,365][0m Trial 19 finished with value: 0.3572983304038644 and parameters: {'observation_period_num': 75, 'train_rates': 0.8223241821846008, 'learning_rate': 2.3475684561726476e-05, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8499669883420636}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:33:29,607][0m Trial 20 finished with value: 0.7925103758383729 and parameters: {'observation_period_num': 125, 'train_rates': 0.7469621052961606, 'learning_rate': 9.643715592160982e-05, 'batch_size': 74, 'step_size': 13, 'gamma': 0.8552769678766623}. Best is trial 15 with value: 0.14377959828445877.[0m
[32m[I 2025-01-05 01:34:22,097][0m Trial 21 finished with value: 0.12660439312458038 and parameters: {'observation_period_num': 38, 'train_rates': 0.9576062096950083, 'learning_rate': 6.000512955404383e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9089225072497208}. Best is trial 21 with value: 0.12660439312458038.[0m
[32m[I 2025-01-05 01:35:26,767][0m Trial 22 finished with value: 0.1383364483577396 and parameters: {'observation_period_num': 49, 'train_rates': 0.9251274946361319, 'learning_rate': 8.461943859493409e-05, 'batch_size': 146, 'step_size': 13, 'gamma': 0.8310511444683114}. Best is trial 21 with value: 0.12660439312458038.[0m
[32m[I 2025-01-05 01:36:32,328][0m Trial 23 finished with value: 1.0873214924185846 and parameters: {'observation_period_num': 50, 'train_rates': 0.9177662377755026, 'learning_rate': 1.0135580096888836e-06, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8339581378868341}. Best is trial 21 with value: 0.12660439312458038.[0m
[32m[I 2025-01-05 01:37:23,830][0m Trial 24 finished with value: 0.10965309053659439 and parameters: {'observation_period_num': 16, 'train_rates': 0.9492073991581168, 'learning_rate': 3.488595000841005e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9000186882202611}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:38:12,654][0m Trial 25 finished with value: 0.2533351071810318 and parameters: {'observation_period_num': 16, 'train_rates': 0.8595325746108156, 'learning_rate': 3.93125071376398e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8957437114895572}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:40:35,242][0m Trial 26 finished with value: 0.1470538484600355 and parameters: {'observation_period_num': 21, 'train_rates': 0.9097775086301827, 'learning_rate': 1.089944903586065e-05, 'batch_size': 30, 'step_size': 13, 'gamma': 0.8746954514380895}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:41:02,935][0m Trial 27 finished with value: 0.2209082692861557 and parameters: {'observation_period_num': 5, 'train_rates': 0.9493195268076777, 'learning_rate': 1.931869305146418e-05, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8048865362585085}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:42:15,535][0m Trial 28 finished with value: 0.16597663247730673 and parameters: {'observation_period_num': 57, 'train_rates': 0.8869253757589772, 'learning_rate': 9.885113198711382e-05, 'batch_size': 133, 'step_size': 13, 'gamma': 0.9028274421552008}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:44:15,185][0m Trial 29 finished with value: 0.4592635342134879 and parameters: {'observation_period_num': 86, 'train_rates': 0.8368256978173059, 'learning_rate': 0.00018733408974182307, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8350913878266503}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:46:49,212][0m Trial 30 finished with value: 0.6168064080979213 and parameters: {'observation_period_num': 111, 'train_rates': 0.9574717898869592, 'learning_rate': 0.0005180545308498972, 'batch_size': 86, 'step_size': 14, 'gamma': 0.8682541781094286}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:47:34,800][0m Trial 31 finished with value: 0.14537852808185245 and parameters: {'observation_period_num': 29, 'train_rates': 0.9528093778076309, 'learning_rate': 2.9061208736127814e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.941594704466177}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:48:40,539][0m Trial 32 finished with value: 0.12813381125798096 and parameters: {'observation_period_num': 48, 'train_rates': 0.9238502045372583, 'learning_rate': 8.137557787435746e-05, 'batch_size': 96, 'step_size': 12, 'gamma': 0.9341099815281122}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:49:55,163][0m Trial 33 finished with value: 0.15254604883862835 and parameters: {'observation_period_num': 53, 'train_rates': 0.924043718876972, 'learning_rate': 9.043650222498881e-05, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9578563905220747}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:50:54,680][0m Trial 34 finished with value: 0.1792530567924443 and parameters: {'observation_period_num': 42, 'train_rates': 0.8955865193323133, 'learning_rate': 0.00018030456943056878, 'batch_size': 97, 'step_size': 14, 'gamma': 0.9139910473811717}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:54:22,959][0m Trial 35 finished with value: 0.38039281991970003 and parameters: {'observation_period_num': 149, 'train_rates': 0.8657404911678266, 'learning_rate': 0.0002828023783889373, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9315042375759346}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:58:37,513][0m Trial 36 finished with value: 0.15124840439910098 and parameters: {'observation_period_num': 178, 'train_rates': 0.9307183983501104, 'learning_rate': 4.1831193166461565e-05, 'batch_size': 133, 'step_size': 9, 'gamma': 0.9755372868946081}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 01:59:07,702][0m Trial 37 finished with value: 0.86287715096716 and parameters: {'observation_period_num': 25, 'train_rates': 0.6437952910636426, 'learning_rate': 7.823122242253962e-05, 'batch_size': 186, 'step_size': 12, 'gamma': 0.8076357181517394}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:00:26,000][0m Trial 38 finished with value: 0.6920482054506835 and parameters: {'observation_period_num': 65, 'train_rates': 0.7655144843545241, 'learning_rate': 0.0002388549631948283, 'batch_size': 165, 'step_size': 8, 'gamma': 0.7814450119814782}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:02:12,713][0m Trial 39 finished with value: 0.3360837581604326 and parameters: {'observation_period_num': 82, 'train_rates': 0.8679553895729819, 'learning_rate': 0.00012078111691220967, 'batch_size': 96, 'step_size': 13, 'gamma': 0.9003511048590692}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:03:22,043][0m Trial 40 finished with value: 0.24174478192414556 and parameters: {'observation_period_num': 48, 'train_rates': 0.9038593704196438, 'learning_rate': 4.822946239524231e-06, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9546184074055001}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:04:11,985][0m Trial 41 finished with value: 0.14270962246002689 and parameters: {'observation_period_num': 34, 'train_rates': 0.936260788834441, 'learning_rate': 6.208991330282095e-05, 'batch_size': 112, 'step_size': 3, 'gamma': 0.9400259877000076}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:04:57,260][0m Trial 42 finished with value: 0.14443399012088776 and parameters: {'observation_period_num': 30, 'train_rates': 0.96989763073687, 'learning_rate': 2.9020888415789926e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.934040453274674}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:05:27,045][0m Trial 43 finished with value: 0.11801461197102248 and parameters: {'observation_period_num': 16, 'train_rates': 0.931786808460127, 'learning_rate': 0.0001480163401717366, 'batch_size': 143, 'step_size': 14, 'gamma': 0.9665892749781144}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:05:57,599][0m Trial 44 finished with value: 0.13538364936743885 and parameters: {'observation_period_num': 18, 'train_rates': 0.9177066498105686, 'learning_rate': 0.0001385811387229932, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9762872391027025}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:09:26,925][0m Trial 45 finished with value: 1.8821310930781894 and parameters: {'observation_period_num': 11, 'train_rates': 0.9757532685065389, 'learning_rate': 0.00039148418121408193, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9702908764360045}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:09:56,885][0m Trial 46 finished with value: 0.21092281956745926 and parameters: {'observation_period_num': 20, 'train_rates': 0.8780669369541766, 'learning_rate': 0.0001421214376497942, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9878873619906442}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:10:22,778][0m Trial 47 finished with value: 0.16446115670347572 and parameters: {'observation_period_num': 8, 'train_rates': 0.9099481402011944, 'learning_rate': 0.0005972082639478726, 'batch_size': 198, 'step_size': 12, 'gamma': 0.9788619020654881}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:11:04,520][0m Trial 48 finished with value: 0.11859531221357551 and parameters: {'observation_period_num': 28, 'train_rates': 0.9370668147945601, 'learning_rate': 0.00016513326763624382, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9599354716494611}. Best is trial 24 with value: 0.10965309053659439.[0m
[32m[I 2025-01-05 02:16:31,593][0m Trial 49 finished with value: 0.14282874763011932 and parameters: {'observation_period_num': 212, 'train_rates': 0.9697009759432794, 'learning_rate': 0.0002145832429194469, 'batch_size': 161, 'step_size': 12, 'gamma': 0.9628853142032007}. Best is trial 24 with value: 0.10965309053659439.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 02:16:31,601][0m A new study created in memory with name: no-name-969001dc-db6f-4ba9-accf-037be45b881f[0m
[32m[I 2025-01-05 02:22:10,139][0m Trial 0 finished with value: 0.1429494470357895 and parameters: {'observation_period_num': 223, 'train_rates': 0.9434676295079314, 'learning_rate': 0.00014452099272023585, 'batch_size': 212, 'step_size': 12, 'gamma': 0.9479132915580298}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:25:53,532][0m Trial 1 finished with value: 2.371396649503562 and parameters: {'observation_period_num': 177, 'train_rates': 0.7643441937746153, 'learning_rate': 0.0007434317622210334, 'batch_size': 64, 'step_size': 3, 'gamma': 0.8683795270249771}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:30:24,588][0m Trial 2 finished with value: 0.7283188104629517 and parameters: {'observation_period_num': 184, 'train_rates': 0.9804250642278001, 'learning_rate': 1.0644923830394444e-05, 'batch_size': 196, 'step_size': 4, 'gamma': 0.7762006009156563}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:31:42,197][0m Trial 3 finished with value: 0.6955377462171238 and parameters: {'observation_period_num': 52, 'train_rates': 0.7086491007772964, 'learning_rate': 1.8364836452510045e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.7637678077259596}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:33:32,340][0m Trial 4 finished with value: 1.0690509010005642 and parameters: {'observation_period_num': 97, 'train_rates': 0.7411141641044288, 'learning_rate': 7.945420982631602e-06, 'batch_size': 136, 'step_size': 10, 'gamma': 0.7758725071153423}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:34:19,959][0m Trial 5 finished with value: 0.21656423529059116 and parameters: {'observation_period_num': 14, 'train_rates': 0.9035900179123859, 'learning_rate': 0.0004096258807968447, 'batch_size': 91, 'step_size': 8, 'gamma': 0.7562091889775151}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:36:10,544][0m Trial 6 finished with value: 0.7544564754387428 and parameters: {'observation_period_num': 97, 'train_rates': 0.7463264933719466, 'learning_rate': 9.924866511907743e-05, 'batch_size': 128, 'step_size': 9, 'gamma': 0.8735831061510986}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:42:24,258][0m Trial 7 finished with value: 0.2597019968088716 and parameters: {'observation_period_num': 250, 'train_rates': 0.8816528517131297, 'learning_rate': 4.75468713716326e-05, 'batch_size': 215, 'step_size': 12, 'gamma': 0.830082757798952}. Best is trial 0 with value: 0.1429494470357895.[0m
[32m[I 2025-01-05 02:47:22,768][0m Trial 8 finished with value: 0.11605154722929001 and parameters: {'observation_period_num': 196, 'train_rates': 0.974699097115745, 'learning_rate': 0.0004243432121126998, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9467229245377647}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 02:51:01,307][0m Trial 9 finished with value: 1.8554545389157588 and parameters: {'observation_period_num': 193, 'train_rates': 0.6125008400764881, 'learning_rate': 3.011595994601688e-06, 'batch_size': 212, 'step_size': 1, 'gamma': 0.9170980879538545}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 02:54:07,891][0m Trial 10 finished with value: 1.2620518335177306 and parameters: {'observation_period_num': 144, 'train_rates': 0.8438160471942507, 'learning_rate': 1.0764233757031203e-06, 'batch_size': 242, 'step_size': 6, 'gamma': 0.982485426485068}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:00:23,702][0m Trial 11 finished with value: 0.14482147991657257 and parameters: {'observation_period_num': 238, 'train_rates': 0.9881131413732847, 'learning_rate': 0.0002036260220935942, 'batch_size': 175, 'step_size': 14, 'gamma': 0.9656522738657694}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:05:47,197][0m Trial 12 finished with value: 0.14958402514457703 and parameters: {'observation_period_num': 216, 'train_rates': 0.9349200165514657, 'learning_rate': 0.00015757486118692739, 'batch_size': 250, 'step_size': 15, 'gamma': 0.9362528395440963}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:09:20,711][0m Trial 13 finished with value: 0.848235441713917 and parameters: {'observation_period_num': 150, 'train_rates': 0.9302226234673003, 'learning_rate': 0.000951406394100213, 'batch_size': 155, 'step_size': 6, 'gamma': 0.9214865117221912}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:14:17,706][0m Trial 14 finished with value: 0.3931671120635756 and parameters: {'observation_period_num': 215, 'train_rates': 0.8338966074828356, 'learning_rate': 8.580105586883725e-05, 'batch_size': 187, 'step_size': 7, 'gamma': 0.9508437459064902}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:19:42,483][0m Trial 15 finished with value: 0.11708454042673111 and parameters: {'observation_period_num': 214, 'train_rates': 0.952189223850998, 'learning_rate': 0.0003272451542693774, 'batch_size': 228, 'step_size': 13, 'gamma': 0.8869136820970855}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:23:23,834][0m Trial 16 finished with value: 0.306685046549015 and parameters: {'observation_period_num': 165, 'train_rates': 0.8604881367619728, 'learning_rate': 0.0002879066572323278, 'batch_size': 241, 'step_size': 4, 'gamma': 0.8757606390064562}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:26:46,929][0m Trial 17 finished with value: 0.46176984290316725 and parameters: {'observation_period_num': 123, 'train_rates': 0.7982530676265268, 'learning_rate': 3.991458787103626e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8995561359167833}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:30:43,544][0m Trial 18 finished with value: 0.900435863836641 and parameters: {'observation_period_num': 198, 'train_rates': 0.6671487028519686, 'learning_rate': 0.0005271343755549351, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8105643743479153}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:37:18,112][0m Trial 19 finished with value: 0.14069056510925293 and parameters: {'observation_period_num': 252, 'train_rates': 0.9599389831346549, 'learning_rate': 0.0003291821561022242, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8364062973290092}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:39:56,084][0m Trial 20 finished with value: 0.24530253620334366 and parameters: {'observation_period_num': 121, 'train_rates': 0.900774572894434, 'learning_rate': 6.90738870175727e-05, 'batch_size': 227, 'step_size': 3, 'gamma': 0.8941181297190706}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:46:20,428][0m Trial 21 finished with value: 0.14824813604354858 and parameters: {'observation_period_num': 246, 'train_rates': 0.9634202279887117, 'learning_rate': 0.0003574007429315556, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8444502057461407}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:52:07,211][0m Trial 22 finished with value: 0.14754009965275014 and parameters: {'observation_period_num': 225, 'train_rates': 0.9488435207889094, 'learning_rate': 0.00030164176648547487, 'batch_size': 103, 'step_size': 6, 'gamma': 0.8404589281542166}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 03:57:11,733][0m Trial 23 finished with value: 1.925167455082446 and parameters: {'observation_period_num': 204, 'train_rates': 0.9177878616246786, 'learning_rate': 0.0009221740942030688, 'batch_size': 74, 'step_size': 15, 'gamma': 0.8136273306245966}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:03:36,752][0m Trial 24 finished with value: 0.13154913485050201 and parameters: {'observation_period_num': 241, 'train_rates': 0.9853495418531872, 'learning_rate': 0.00020049538058038304, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8590535369271423}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:07:46,214][0m Trial 25 finished with value: 0.14744159579277039 and parameters: {'observation_period_num': 168, 'train_rates': 0.9882499605178368, 'learning_rate': 0.00017661126248205106, 'batch_size': 153, 'step_size': 11, 'gamma': 0.8966860268445894}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:13:21,237][0m Trial 26 finished with value: 0.2209335603800259 and parameters: {'observation_period_num': 230, 'train_rates': 0.8884165129089115, 'learning_rate': 0.0004847019384490476, 'batch_size': 170, 'step_size': 13, 'gamma': 0.860021436126069}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:17:56,757][0m Trial 27 finished with value: 0.3130280302196253 and parameters: {'observation_period_num': 203, 'train_rates': 0.8226453854706288, 'learning_rate': 0.00010873255253673895, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9827019034664592}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:21:25,566][0m Trial 28 finished with value: 0.34473306119279573 and parameters: {'observation_period_num': 155, 'train_rates': 0.8688682249678399, 'learning_rate': 1.9270437142723756e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.9157636876676774}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:27:18,025][0m Trial 29 finished with value: 0.12174387276172638 and parameters: {'observation_period_num': 229, 'train_rates': 0.9520698781811326, 'learning_rate': 0.00021367079032454597, 'batch_size': 230, 'step_size': 13, 'gamma': 0.9448488690870062}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:32:41,863][0m Trial 30 finished with value: 0.17285315692424774 and parameters: {'observation_period_num': 216, 'train_rates': 0.9266806460710193, 'learning_rate': 0.0005725095186415583, 'batch_size': 256, 'step_size': 14, 'gamma': 0.9491119744370542}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:38:51,981][0m Trial 31 finished with value: 0.13494634628295898 and parameters: {'observation_period_num': 236, 'train_rates': 0.9615710375189572, 'learning_rate': 0.0002370541192578212, 'batch_size': 224, 'step_size': 12, 'gamma': 0.9648729034929036}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:43:25,915][0m Trial 32 finished with value: 0.150239035487175 and parameters: {'observation_period_num': 186, 'train_rates': 0.9484244777959715, 'learning_rate': 0.00013509836232771614, 'batch_size': 196, 'step_size': 13, 'gamma': 0.9354640790754295}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:49:09,988][0m Trial 33 finished with value: 0.1682894229888916 and parameters: {'observation_period_num': 224, 'train_rates': 0.9710034804949905, 'learning_rate': 0.0006542097242407937, 'batch_size': 231, 'step_size': 12, 'gamma': 0.8847621429791701}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:53:35,554][0m Trial 34 finished with value: 0.18164469301700592 and parameters: {'observation_period_num': 178, 'train_rates': 0.9899239465926671, 'learning_rate': 7.421864687984216e-05, 'batch_size': 205, 'step_size': 10, 'gamma': 0.859065708085376}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 04:58:37,035][0m Trial 35 finished with value: 0.17846532896220446 and parameters: {'observation_period_num': 204, 'train_rates': 0.9089256728133739, 'learning_rate': 0.00021124615911334503, 'batch_size': 178, 'step_size': 5, 'gamma': 0.9638534870764587}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:04:38,032][0m Trial 36 finished with value: 0.23342859745025635 and parameters: {'observation_period_num': 233, 'train_rates': 0.9507887919783585, 'learning_rate': 2.559665896406107e-05, 'batch_size': 238, 'step_size': 14, 'gamma': 0.9343361140359473}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:09:55,762][0m Trial 37 finished with value: 0.14571356773376465 and parameters: {'observation_period_num': 213, 'train_rates': 0.9307795071925009, 'learning_rate': 0.00042415413835304337, 'batch_size': 216, 'step_size': 10, 'gamma': 0.911294211523009}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:10:54,908][0m Trial 38 finished with value: 0.9462741357143795 and parameters: {'observation_period_num': 48, 'train_rates': 0.7792438861060368, 'learning_rate': 6.914397290715738e-06, 'batch_size': 92, 'step_size': 7, 'gamma': 0.806015811065062}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:15:28,624][0m Trial 39 finished with value: 0.13605469465255737 and parameters: {'observation_period_num': 185, 'train_rates': 0.9707460670570501, 'learning_rate': 0.00013696537598580403, 'batch_size': 131, 'step_size': 10, 'gamma': 0.8546243578436251}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:21:25,229][0m Trial 40 finished with value: 0.47824689391816017 and parameters: {'observation_period_num': 242, 'train_rates': 0.8883784937247767, 'learning_rate': 4.46155270784945e-05, 'batch_size': 201, 'step_size': 3, 'gamma': 0.8227229214190444}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:27:37,424][0m Trial 41 finished with value: 0.13992109894752502 and parameters: {'observation_period_num': 235, 'train_rates': 0.9670128548126958, 'learning_rate': 0.0002438952896685058, 'batch_size': 227, 'step_size': 12, 'gamma': 0.9604632579022004}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:33:43,634][0m Trial 42 finished with value: 0.14682503044605255 and parameters: {'observation_period_num': 237, 'train_rates': 0.9483794530849435, 'learning_rate': 0.0002329954806355229, 'batch_size': 219, 'step_size': 12, 'gamma': 0.987939301664442}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:39:34,539][0m Trial 43 finished with value: 0.562720775604248 and parameters: {'observation_period_num': 223, 'train_rates': 0.9761563800131073, 'learning_rate': 0.0007573065987415734, 'batch_size': 223, 'step_size': 11, 'gamma': 0.9751038918084683}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:45:51,345][0m Trial 44 finished with value: 0.22645529020916333 and parameters: {'observation_period_num': 252, 'train_rates': 0.906607974524267, 'learning_rate': 6.159254012048598e-05, 'batch_size': 184, 'step_size': 13, 'gamma': 0.9276410197919792}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:50:25,666][0m Trial 45 finished with value: 0.18089237809181213 and parameters: {'observation_period_num': 192, 'train_rates': 0.93483538516982, 'learning_rate': 0.00013207808691677146, 'batch_size': 241, 'step_size': 11, 'gamma': 0.7897046364150359}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:54:19,629][0m Trial 46 finished with value: 1.127927585362066 and parameters: {'observation_period_num': 210, 'train_rates': 0.6056310757406875, 'learning_rate': 0.0004176960118673109, 'batch_size': 208, 'step_size': 14, 'gamma': 0.9422899994779385}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 05:56:02,894][0m Trial 47 finished with value: 0.7351836533002232 and parameters: {'observation_period_num': 93, 'train_rates': 0.7104979486498393, 'learning_rate': 0.00010482730493886878, 'batch_size': 143, 'step_size': 8, 'gamma': 0.9717559309941044}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 06:00:09,731][0m Trial 48 finished with value: 0.128749817609787 and parameters: {'observation_period_num': 169, 'train_rates': 0.9586776436736105, 'learning_rate': 0.0002877074237340171, 'batch_size': 252, 'step_size': 15, 'gamma': 0.8839583451615729}. Best is trial 8 with value: 0.11605154722929001.[0m
[32m[I 2025-01-05 06:03:10,337][0m Trial 49 finished with value: 0.9419050031755029 and parameters: {'observation_period_num': 162, 'train_rates': 0.6327939173965225, 'learning_rate': 0.000656532842069539, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9072606907432361}. Best is trial 8 with value: 0.11605154722929001.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 186, 'train_rates': 0.9564864710566967, 'learning_rate': 0.0001409470186357949, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9149476096378365}
Epoch 1/300, trend Loss: 0.9698 | 1.1651
Epoch 2/300, trend Loss: 0.6773 | 0.7329
Epoch 3/300, trend Loss: 0.5728 | 0.6192
Epoch 4/300, trend Loss: 0.4680 | 0.5242
Epoch 5/300, trend Loss: 0.4068 | 0.4517
Epoch 6/300, trend Loss: 0.3696 | 0.4089
Epoch 7/300, trend Loss: 0.3641 | 0.3557
Epoch 8/300, trend Loss: 0.3609 | 0.3715
Epoch 9/300, trend Loss: 0.3798 | 0.4305
Epoch 10/300, trend Loss: 0.3176 | 0.3369
Epoch 11/300, trend Loss: 0.4496 | 0.3344
Epoch 12/300, trend Loss: 0.3365 | 0.3448
Epoch 13/300, trend Loss: 0.2900 | 0.2969
Epoch 14/300, trend Loss: 0.2522 | 0.3086
Epoch 15/300, trend Loss: 0.2466 | 0.2786
Epoch 16/300, trend Loss: 0.2488 | 0.2707
Epoch 17/300, trend Loss: 0.2286 | 0.2635
Epoch 18/300, trend Loss: 0.2312 | 0.2497
Epoch 19/300, trend Loss: 0.2260 | 0.2555
Epoch 20/300, trend Loss: 0.2378 | 0.2400
Epoch 21/300, trend Loss: 0.2302 | 0.2503
Epoch 22/300, trend Loss: 0.2137 | 0.2306
Epoch 23/300, trend Loss: 0.2060 | 0.2227
Epoch 24/300, trend Loss: 0.1976 | 0.2198
Epoch 25/300, trend Loss: 0.2020 | 0.2137
Epoch 26/300, trend Loss: 0.2054 | 0.2176
Epoch 27/300, trend Loss: 0.2006 | 0.2077
Epoch 28/300, trend Loss: 0.1967 | 0.2052
Epoch 29/300, trend Loss: 0.1857 | 0.2012
Epoch 30/300, trend Loss: 0.1822 | 0.1913
Epoch 31/300, trend Loss: 0.1761 | 0.1951
Epoch 32/300, trend Loss: 0.1730 | 0.1863
Epoch 33/300, trend Loss: 0.1693 | 0.1891
Epoch 34/300, trend Loss: 0.1691 | 0.1828
Epoch 35/300, trend Loss: 0.1695 | 0.1833
Epoch 36/300, trend Loss: 0.1702 | 0.1806
Epoch 37/300, trend Loss: 0.1732 | 0.1772
Epoch 38/300, trend Loss: 0.1717 | 0.1802
Epoch 39/300, trend Loss: 0.1709 | 0.1709
Epoch 40/300, trend Loss: 0.1666 | 0.1779
Epoch 41/300, trend Loss: 0.1649 | 0.1678
Epoch 42/300, trend Loss: 0.1622 | 0.1727
Epoch 43/300, trend Loss: 0.1613 | 0.1681
Epoch 44/300, trend Loss: 0.1610 | 0.1663
Epoch 45/300, trend Loss: 0.1593 | 0.1676
Epoch 46/300, trend Loss: 0.1595 | 0.1615
Epoch 47/300, trend Loss: 0.1573 | 0.1676
Epoch 48/300, trend Loss: 0.1562 | 0.1588
Epoch 49/300, trend Loss: 0.1554 | 0.1646
Epoch 50/300, trend Loss: 0.1539 | 0.1575
Epoch 51/300, trend Loss: 0.1537 | 0.1602
Epoch 52/300, trend Loss: 0.1525 | 0.1568
Epoch 53/300, trend Loss: 0.1527 | 0.1570
Epoch 54/300, trend Loss: 0.1519 | 0.1566
Epoch 55/300, trend Loss: 0.1514 | 0.1536
Epoch 56/300, trend Loss: 0.1515 | 0.1558
Epoch 57/300, trend Loss: 0.1506 | 0.1513
Epoch 58/300, trend Loss: 0.1492 | 0.1543
Epoch 59/300, trend Loss: 0.1476 | 0.1498
Epoch 60/300, trend Loss: 0.1477 | 0.1521
Epoch 61/300, trend Loss: 0.1460 | 0.1489
Epoch 62/300, trend Loss: 0.1479 | 0.1494
Epoch 63/300, trend Loss: 0.1466 | 0.1481
Epoch 64/300, trend Loss: 0.1456 | 0.1472
Epoch 65/300, trend Loss: 0.1455 | 0.1475
Epoch 66/300, trend Loss: 0.1455 | 0.1444
Epoch 67/300, trend Loss: 0.1438 | 0.1470
Epoch 68/300, trend Loss: 0.1432 | 0.1435
Epoch 69/300, trend Loss: 0.1436 | 0.1456
Epoch 70/300, trend Loss: 0.1429 | 0.1412
Epoch 71/300, trend Loss: 0.1428 | 0.1444
Epoch 72/300, trend Loss: 0.1422 | 0.1403
Epoch 73/300, trend Loss: 0.1413 | 0.1421
Epoch 74/300, trend Loss: 0.1406 | 0.1392
Epoch 75/300, trend Loss: 0.1401 | 0.1413
Epoch 76/300, trend Loss: 0.1391 | 0.1385
Epoch 77/300, trend Loss: 0.1388 | 0.1397
Epoch 78/300, trend Loss: 0.1384 | 0.1380
Epoch 79/300, trend Loss: 0.1377 | 0.1382
Epoch 80/300, trend Loss: 0.1374 | 0.1373
Epoch 81/300, trend Loss: 0.1368 | 0.1365
Epoch 82/300, trend Loss: 0.1361 | 0.1365
Epoch 83/300, trend Loss: 0.1357 | 0.1355
Epoch 84/300, trend Loss: 0.1355 | 0.1354
Epoch 85/300, trend Loss: 0.1360 | 0.1344
Epoch 86/300, trend Loss: 0.1351 | 0.1337
Epoch 87/300, trend Loss: 0.1351 | 0.1329
Epoch 88/300, trend Loss: 0.1346 | 0.1328
Epoch 89/300, trend Loss: 0.1341 | 0.1320
Epoch 90/300, trend Loss: 0.1339 | 0.1324
Epoch 91/300, trend Loss: 0.1347 | 0.1309
Epoch 92/300, trend Loss: 0.1345 | 0.1326
Epoch 93/300, trend Loss: 0.1345 | 0.1308
Epoch 94/300, trend Loss: 0.1354 | 0.1322
Epoch 95/300, trend Loss: 0.1355 | 0.1301
Epoch 96/300, trend Loss: 0.1357 | 0.1313
Epoch 97/300, trend Loss: 0.1341 | 0.1284
Epoch 98/300, trend Loss: 0.1331 | 0.1297
Epoch 99/300, trend Loss: 0.1327 | 0.1281
Epoch 100/300, trend Loss: 0.1312 | 0.1298
Epoch 101/300, trend Loss: 0.1311 | 0.1277
Epoch 102/300, trend Loss: 0.1315 | 0.1283
Epoch 103/300, trend Loss: 0.1302 | 0.1267
Epoch 104/300, trend Loss: 0.1306 | 0.1279
Epoch 105/300, trend Loss: 0.1295 | 0.1260
Epoch 106/300, trend Loss: 0.1296 | 0.1271
Epoch 107/300, trend Loss: 0.1287 | 0.1256
Epoch 108/300, trend Loss: 0.1284 | 0.1262
Epoch 109/300, trend Loss: 0.1287 | 0.1250
Epoch 110/300, trend Loss: 0.1277 | 0.1256
Epoch 111/300, trend Loss: 0.1281 | 0.1248
Epoch 112/300, trend Loss: 0.1274 | 0.1243
Epoch 113/300, trend Loss: 0.1271 | 0.1234
Epoch 114/300, trend Loss: 0.1273 | 0.1240
Epoch 115/300, trend Loss: 0.1268 | 0.1236
Epoch 116/300, trend Loss: 0.1270 | 0.1233
Epoch 117/300, trend Loss: 0.1267 | 0.1226
Epoch 118/300, trend Loss: 0.1261 | 0.1226
Epoch 119/300, trend Loss: 0.1263 | 0.1223
Epoch 120/300, trend Loss: 0.1259 | 0.1225
Epoch 121/300, trend Loss: 0.1257 | 0.1221
Epoch 122/300, trend Loss: 0.1251 | 0.1218
Epoch 123/300, trend Loss: 0.1252 | 0.1220
Epoch 124/300, trend Loss: 0.1253 | 0.1215
Epoch 125/300, trend Loss: 0.1252 | 0.1208
Epoch 126/300, trend Loss: 0.1246 | 0.1212
Epoch 127/300, trend Loss: 0.1243 | 0.1206
Epoch 128/300, trend Loss: 0.1242 | 0.1212
Epoch 129/300, trend Loss: 0.1237 | 0.1206
Epoch 130/300, trend Loss: 0.1238 | 0.1213
Epoch 131/300, trend Loss: 0.1232 | 0.1201
Epoch 132/300, trend Loss: 0.1244 | 0.1206
Epoch 133/300, trend Loss: 0.1241 | 0.1200
Epoch 134/300, trend Loss: 0.1242 | 0.1207
Epoch 135/300, trend Loss: 0.1238 | 0.1197
Epoch 136/300, trend Loss: 0.1229 | 0.1199
Epoch 137/300, trend Loss: 0.1233 | 0.1192
Epoch 138/300, trend Loss: 0.1229 | 0.1197
Epoch 139/300, trend Loss: 0.1229 | 0.1189
Epoch 140/300, trend Loss: 0.1221 | 0.1197
Epoch 141/300, trend Loss: 0.1219 | 0.1186
Epoch 142/300, trend Loss: 0.1226 | 0.1191
Epoch 143/300, trend Loss: 0.1225 | 0.1185
Epoch 144/300, trend Loss: 0.1224 | 0.1184
Epoch 145/300, trend Loss: 0.1222 | 0.1188
Epoch 146/300, trend Loss: 0.1212 | 0.1180
Epoch 147/300, trend Loss: 0.1217 | 0.1182
Epoch 148/300, trend Loss: 0.1213 | 0.1179
Epoch 149/300, trend Loss: 0.1215 | 0.1174
Epoch 150/300, trend Loss: 0.1208 | 0.1176
Epoch 151/300, trend Loss: 0.1212 | 0.1176
Epoch 152/300, trend Loss: 0.1208 | 0.1177
Epoch 153/300, trend Loss: 0.1213 | 0.1174
Epoch 154/300, trend Loss: 0.1211 | 0.1171
Epoch 155/300, trend Loss: 0.1210 | 0.1176
Epoch 156/300, trend Loss: 0.1209 | 0.1166
Epoch 157/300, trend Loss: 0.1203 | 0.1166
Epoch 158/300, trend Loss: 0.1200 | 0.1167
Epoch 159/300, trend Loss: 0.1203 | 0.1167
Epoch 160/300, trend Loss: 0.1204 | 0.1167
Epoch 161/300, trend Loss: 0.1200 | 0.1169
Epoch 162/300, trend Loss: 0.1203 | 0.1164
Epoch 163/300, trend Loss: 0.1204 | 0.1161
Epoch 164/300, trend Loss: 0.1194 | 0.1164
Epoch 165/300, trend Loss: 0.1197 | 0.1167
Epoch 166/300, trend Loss: 0.1194 | 0.1162
Epoch 167/300, trend Loss: 0.1196 | 0.1162
Epoch 168/300, trend Loss: 0.1196 | 0.1161
Epoch 169/300, trend Loss: 0.1192 | 0.1163
Epoch 170/300, trend Loss: 0.1194 | 0.1162
Epoch 171/300, trend Loss: 0.1191 | 0.1161
Epoch 172/300, trend Loss: 0.1186 | 0.1159
Epoch 173/300, trend Loss: 0.1189 | 0.1157
Epoch 174/300, trend Loss: 0.1190 | 0.1155
Epoch 175/300, trend Loss: 0.1179 | 0.1157
Epoch 176/300, trend Loss: 0.1189 | 0.1156
Epoch 177/300, trend Loss: 0.1188 | 0.1155
Epoch 178/300, trend Loss: 0.1182 | 0.1153
Epoch 179/300, trend Loss: 0.1190 | 0.1154
Epoch 180/300, trend Loss: 0.1182 | 0.1151
Epoch 181/300, trend Loss: 0.1186 | 0.1146
Epoch 182/300, trend Loss: 0.1186 | 0.1147
Epoch 183/300, trend Loss: 0.1181 | 0.1148
Epoch 184/300, trend Loss: 0.1179 | 0.1145
Epoch 185/300, trend Loss: 0.1182 | 0.1148
Epoch 186/300, trend Loss: 0.1183 | 0.1147
Epoch 187/300, trend Loss: 0.1179 | 0.1144
Epoch 188/300, trend Loss: 0.1175 | 0.1146
Epoch 189/300, trend Loss: 0.1179 | 0.1147
Epoch 190/300, trend Loss: 0.1181 | 0.1148
Epoch 191/300, trend Loss: 0.1178 | 0.1148
Epoch 192/300, trend Loss: 0.1174 | 0.1147
Epoch 193/300, trend Loss: 0.1175 | 0.1147
Epoch 194/300, trend Loss: 0.1179 | 0.1144
Epoch 195/300, trend Loss: 0.1176 | 0.1141
Epoch 196/300, trend Loss: 0.1177 | 0.1140
Epoch 197/300, trend Loss: 0.1176 | 0.1144
Epoch 198/300, trend Loss: 0.1176 | 0.1141
Epoch 199/300, trend Loss: 0.1177 | 0.1140
Epoch 200/300, trend Loss: 0.1170 | 0.1139
Epoch 201/300, trend Loss: 0.1163 | 0.1140
Epoch 202/300, trend Loss: 0.1171 | 0.1142
Epoch 203/300, trend Loss: 0.1170 | 0.1137
Epoch 204/300, trend Loss: 0.1168 | 0.1141
Epoch 205/300, trend Loss: 0.1162 | 0.1141
Epoch 206/300, trend Loss: 0.1169 | 0.1140
Epoch 207/300, trend Loss: 0.1170 | 0.1138
Epoch 208/300, trend Loss: 0.1175 | 0.1139
Epoch 209/300, trend Loss: 0.1165 | 0.1136
Epoch 210/300, trend Loss: 0.1161 | 0.1135
Epoch 211/300, trend Loss: 0.1173 | 0.1135
Epoch 212/300, trend Loss: 0.1168 | 0.1135
Epoch 213/300, trend Loss: 0.1165 | 0.1136
Epoch 214/300, trend Loss: 0.1164 | 0.1136
Epoch 215/300, trend Loss: 0.1163 | 0.1135
Epoch 216/300, trend Loss: 0.1165 | 0.1131
Epoch 217/300, trend Loss: 0.1163 | 0.1131
Epoch 218/300, trend Loss: 0.1163 | 0.1131
Epoch 219/300, trend Loss: 0.1152 | 0.1131
Epoch 220/300, trend Loss: 0.1164 | 0.1130
Epoch 221/300, trend Loss: 0.1164 | 0.1130
Epoch 222/300, trend Loss: 0.1162 | 0.1131
Epoch 223/300, trend Loss: 0.1162 | 0.1130
Epoch 224/300, trend Loss: 0.1159 | 0.1130
Epoch 225/300, trend Loss: 0.1152 | 0.1128
Epoch 226/300, trend Loss: 0.1158 | 0.1128
Epoch 227/300, trend Loss: 0.1161 | 0.1129
Epoch 228/300, trend Loss: 0.1157 | 0.1128
Epoch 229/300, trend Loss: 0.1156 | 0.1126
Epoch 230/300, trend Loss: 0.1160 | 0.1128
Epoch 231/300, trend Loss: 0.1155 | 0.1128
Epoch 232/300, trend Loss: 0.1157 | 0.1128
Epoch 233/300, trend Loss: 0.1153 | 0.1128
Epoch 234/300, trend Loss: 0.1155 | 0.1127
Epoch 235/300, trend Loss: 0.1155 | 0.1127
Epoch 236/300, trend Loss: 0.1156 | 0.1126
Epoch 237/300, trend Loss: 0.1158 | 0.1125
Epoch 238/300, trend Loss: 0.1153 | 0.1125
Epoch 239/300, trend Loss: 0.1154 | 0.1125
Epoch 240/300, trend Loss: 0.1157 | 0.1122
Epoch 241/300, trend Loss: 0.1149 | 0.1121
Epoch 242/300, trend Loss: 0.1152 | 0.1122
Epoch 243/300, trend Loss: 0.1151 | 0.1124
Epoch 244/300, trend Loss: 0.1156 | 0.1124
Epoch 245/300, trend Loss: 0.1154 | 0.1127
Epoch 246/300, trend Loss: 0.1146 | 0.1125
Epoch 247/300, trend Loss: 0.1156 | 0.1123
Epoch 248/300, trend Loss: 0.1149 | 0.1125
Epoch 249/300, trend Loss: 0.1155 | 0.1123
Epoch 250/300, trend Loss: 0.1153 | 0.1121
Epoch 251/300, trend Loss: 0.1148 | 0.1120
Epoch 252/300, trend Loss: 0.1153 | 0.1119
Epoch 253/300, trend Loss: 0.1146 | 0.1118
Epoch 254/300, trend Loss: 0.1146 | 0.1120
Epoch 255/300, trend Loss: 0.1152 | 0.1119
Epoch 256/300, trend Loss: 0.1146 | 0.1121
Epoch 257/300, trend Loss: 0.1153 | 0.1121
Epoch 258/300, trend Loss: 0.1143 | 0.1122
Epoch 259/300, trend Loss: 0.1146 | 0.1122
Epoch 260/300, trend Loss: 0.1144 | 0.1120
Epoch 261/300, trend Loss: 0.1148 | 0.1122
Epoch 262/300, trend Loss: 0.1146 | 0.1120
Epoch 263/300, trend Loss: 0.1148 | 0.1120
Epoch 264/300, trend Loss: 0.1146 | 0.1119
Epoch 265/300, trend Loss: 0.1148 | 0.1118
Epoch 266/300, trend Loss: 0.1145 | 0.1117
Epoch 267/300, trend Loss: 0.1146 | 0.1117
Epoch 268/300, trend Loss: 0.1148 | 0.1119
Epoch 269/300, trend Loss: 0.1148 | 0.1118
Epoch 270/300, trend Loss: 0.1148 | 0.1118
Epoch 271/300, trend Loss: 0.1139 | 0.1117
Epoch 272/300, trend Loss: 0.1142 | 0.1117
Epoch 273/300, trend Loss: 0.1148 | 0.1117
Epoch 274/300, trend Loss: 0.1148 | 0.1116
Epoch 275/300, trend Loss: 0.1144 | 0.1116
Epoch 276/300, trend Loss: 0.1148 | 0.1116
Epoch 277/300, trend Loss: 0.1140 | 0.1115
Epoch 278/300, trend Loss: 0.1148 | 0.1116
Epoch 279/300, trend Loss: 0.1144 | 0.1116
Epoch 280/300, trend Loss: 0.1147 | 0.1117
Epoch 281/300, trend Loss: 0.1147 | 0.1119
Epoch 282/300, trend Loss: 0.1139 | 0.1119
Epoch 283/300, trend Loss: 0.1139 | 0.1119
Epoch 284/300, trend Loss: 0.1146 | 0.1117
Epoch 285/300, trend Loss: 0.1142 | 0.1116
Epoch 286/300, trend Loss: 0.1137 | 0.1115
Epoch 287/300, trend Loss: 0.1144 | 0.1114
Epoch 288/300, trend Loss: 0.1150 | 0.1116
Epoch 289/300, trend Loss: 0.1144 | 0.1116
Epoch 290/300, trend Loss: 0.1137 | 0.1116
Epoch 291/300, trend Loss: 0.1139 | 0.1116
Epoch 292/300, trend Loss: 0.1143 | 0.1117
Epoch 293/300, trend Loss: 0.1141 | 0.1116
Epoch 294/300, trend Loss: 0.1143 | 0.1117
Epoch 295/300, trend Loss: 0.1144 | 0.1118
Epoch 296/300, trend Loss: 0.1143 | 0.1117
Epoch 297/300, trend Loss: 0.1138 | 0.1116
Epoch 298/300, trend Loss: 0.1141 | 0.1115
Epoch 299/300, trend Loss: 0.1140 | 0.1115
Epoch 300/300, trend Loss: 0.1138 | 0.1114
Training seasonal_0 component with params: {'observation_period_num': 26, 'train_rates': 0.97171594816454, 'learning_rate': 0.00025484983658506644, 'batch_size': 136, 'step_size': 10, 'gamma': 0.9037911712803184}
Epoch 1/300, seasonal_0 Loss: 0.8572 | 0.8969
Epoch 2/300, seasonal_0 Loss: 0.8436 | 0.6539
Epoch 3/300, seasonal_0 Loss: 0.6138 | 0.7282
Epoch 4/300, seasonal_0 Loss: 0.5970 | 0.6088
Epoch 5/300, seasonal_0 Loss: 0.5727 | 0.6710
Epoch 6/300, seasonal_0 Loss: 0.4550 | 0.4865
Epoch 7/300, seasonal_0 Loss: 0.3846 | 0.4354
Epoch 8/300, seasonal_0 Loss: 0.5070 | 0.4536
Epoch 9/300, seasonal_0 Loss: 0.4802 | 0.4369
Epoch 10/300, seasonal_0 Loss: 0.5166 | 0.5599
Epoch 11/300, seasonal_0 Loss: 0.4293 | 0.4217
Epoch 12/300, seasonal_0 Loss: 0.6707 | 0.5105
Epoch 13/300, seasonal_0 Loss: 0.4579 | 0.4376
Epoch 14/300, seasonal_0 Loss: 0.4973 | 0.4846
Epoch 15/300, seasonal_0 Loss: 0.4935 | 0.5069
Epoch 16/300, seasonal_0 Loss: 0.4334 | 0.3931
Epoch 17/300, seasonal_0 Loss: 0.3465 | 0.3465
Epoch 18/300, seasonal_0 Loss: 0.3443 | 0.3893
Epoch 19/300, seasonal_0 Loss: 0.3258 | 0.4534
Epoch 20/300, seasonal_0 Loss: 0.3185 | 0.3358
Epoch 21/300, seasonal_0 Loss: 0.2704 | 0.3344
Epoch 22/300, seasonal_0 Loss: 0.2494 | 0.3067
Epoch 23/300, seasonal_0 Loss: 0.2325 | 0.3120
Epoch 24/300, seasonal_0 Loss: 0.2322 | 0.3028
Epoch 25/300, seasonal_0 Loss: 0.2303 | 0.3068
Epoch 26/300, seasonal_0 Loss: 0.2248 | 0.2940
Epoch 27/300, seasonal_0 Loss: 0.2231 | 0.2830
Epoch 28/300, seasonal_0 Loss: 0.2226 | 0.2833
Epoch 29/300, seasonal_0 Loss: 0.2479 | 0.2769
Epoch 30/300, seasonal_0 Loss: 0.2284 | 0.3053
Epoch 31/300, seasonal_0 Loss: 0.2342 | 0.2826
Epoch 32/300, seasonal_0 Loss: 0.2133 | 0.2929
Epoch 33/300, seasonal_0 Loss: 0.1883 | 0.2606
Epoch 34/300, seasonal_0 Loss: 0.1792 | 0.2502
Epoch 35/300, seasonal_0 Loss: 0.1748 | 0.2476
Epoch 36/300, seasonal_0 Loss: 0.1720 | 0.2402
Epoch 37/300, seasonal_0 Loss: 0.1686 | 0.2391
Epoch 38/300, seasonal_0 Loss: 0.1662 | 0.2339
Epoch 39/300, seasonal_0 Loss: 0.1634 | 0.2281
Epoch 40/300, seasonal_0 Loss: 0.1617 | 0.2301
Epoch 41/300, seasonal_0 Loss: 0.1625 | 0.2212
Epoch 42/300, seasonal_0 Loss: 0.1624 | 0.2283
Epoch 43/300, seasonal_0 Loss: 0.1651 | 0.2141
Epoch 44/300, seasonal_0 Loss: 0.1627 | 0.2235
Epoch 45/300, seasonal_0 Loss: 0.1648 | 0.2132
Epoch 46/300, seasonal_0 Loss: 0.1615 | 0.2204
Epoch 47/300, seasonal_0 Loss: 0.1630 | 0.2185
Epoch 48/300, seasonal_0 Loss: 0.1667 | 0.2188
Epoch 49/300, seasonal_0 Loss: 0.1638 | 0.2210
Epoch 50/300, seasonal_0 Loss: 0.1643 | 0.2030
Epoch 51/300, seasonal_0 Loss: 0.1557 | 0.2145
Epoch 52/300, seasonal_0 Loss: 0.1513 | 0.1975
Epoch 53/300, seasonal_0 Loss: 0.1487 | 0.2028
Epoch 54/300, seasonal_0 Loss: 0.1469 | 0.2000
Epoch 55/300, seasonal_0 Loss: 0.1492 | 0.1958
Epoch 56/300, seasonal_0 Loss: 0.1508 | 0.2052
Epoch 57/300, seasonal_0 Loss: 0.1542 | 0.1892
Epoch 58/300, seasonal_0 Loss: 0.1478 | 0.2021
Epoch 59/300, seasonal_0 Loss: 0.1467 | 0.1865
Epoch 60/300, seasonal_0 Loss: 0.1425 | 0.1939
Epoch 61/300, seasonal_0 Loss: 0.1429 | 0.1908
Epoch 62/300, seasonal_0 Loss: 0.1441 | 0.1828
Epoch 63/300, seasonal_0 Loss: 0.1409 | 0.1925
Epoch 64/300, seasonal_0 Loss: 0.1395 | 0.1759
Epoch 65/300, seasonal_0 Loss: 0.1365 | 0.1891
Epoch 66/300, seasonal_0 Loss: 0.1336 | 0.1761
Epoch 67/300, seasonal_0 Loss: 0.1306 | 0.1804
Epoch 68/300, seasonal_0 Loss: 0.1306 | 0.1778
Epoch 69/300, seasonal_0 Loss: 0.1301 | 0.1731
Epoch 70/300, seasonal_0 Loss: 0.1297 | 0.1789
Epoch 71/300, seasonal_0 Loss: 0.1310 | 0.1690
Epoch 72/300, seasonal_0 Loss: 0.1296 | 0.1777
Epoch 73/300, seasonal_0 Loss: 0.1298 | 0.1684
Epoch 74/300, seasonal_0 Loss: 0.1273 | 0.1761
Epoch 75/300, seasonal_0 Loss: 0.1272 | 0.1658
Epoch 76/300, seasonal_0 Loss: 0.1272 | 0.1725
Epoch 77/300, seasonal_0 Loss: 0.1276 | 0.1650
Epoch 78/300, seasonal_0 Loss: 0.1241 | 0.1657
Epoch 79/300, seasonal_0 Loss: 0.1219 | 0.1632
Epoch 80/300, seasonal_0 Loss: 0.1205 | 0.1607
Epoch 81/300, seasonal_0 Loss: 0.1192 | 0.1615
Epoch 82/300, seasonal_0 Loss: 0.1186 | 0.1575
Epoch 83/300, seasonal_0 Loss: 0.1171 | 0.1571
Epoch 84/300, seasonal_0 Loss: 0.1172 | 0.1562
Epoch 85/300, seasonal_0 Loss: 0.1163 | 0.1554
Epoch 86/300, seasonal_0 Loss: 0.1167 | 0.1557
Epoch 87/300, seasonal_0 Loss: 0.1163 | 0.1530
Epoch 88/300, seasonal_0 Loss: 0.1150 | 0.1545
Epoch 89/300, seasonal_0 Loss: 0.1148 | 0.1511
Epoch 90/300, seasonal_0 Loss: 0.1145 | 0.1512
Epoch 91/300, seasonal_0 Loss: 0.1140 | 0.1502
Epoch 92/300, seasonal_0 Loss: 0.1141 | 0.1484
Epoch 93/300, seasonal_0 Loss: 0.1131 | 0.1493
Epoch 94/300, seasonal_0 Loss: 0.1128 | 0.1477
Epoch 95/300, seasonal_0 Loss: 0.1114 | 0.1473
Epoch 96/300, seasonal_0 Loss: 0.1112 | 0.1440
Epoch 97/300, seasonal_0 Loss: 0.1104 | 0.1473
Epoch 98/300, seasonal_0 Loss: 0.1104 | 0.1432
Epoch 99/300, seasonal_0 Loss: 0.1094 | 0.1434
Epoch 100/300, seasonal_0 Loss: 0.1090 | 0.1421
Epoch 101/300, seasonal_0 Loss: 0.1084 | 0.1408
Epoch 102/300, seasonal_0 Loss: 0.1086 | 0.1411
Epoch 103/300, seasonal_0 Loss: 0.1076 | 0.1413
Epoch 104/300, seasonal_0 Loss: 0.1075 | 0.1397
Epoch 105/300, seasonal_0 Loss: 0.1071 | 0.1390
Epoch 106/300, seasonal_0 Loss: 0.1066 | 0.1389
Epoch 107/300, seasonal_0 Loss: 0.1067 | 0.1379
Epoch 108/300, seasonal_0 Loss: 0.1064 | 0.1378
Epoch 109/300, seasonal_0 Loss: 0.1062 | 0.1372
Epoch 110/300, seasonal_0 Loss: 0.1061 | 0.1368
Epoch 111/300, seasonal_0 Loss: 0.1045 | 0.1364
Epoch 112/300, seasonal_0 Loss: 0.1042 | 0.1351
Epoch 113/300, seasonal_0 Loss: 0.1044 | 0.1350
Epoch 114/300, seasonal_0 Loss: 0.1045 | 0.1361
Epoch 115/300, seasonal_0 Loss: 0.1041 | 0.1349
Epoch 116/300, seasonal_0 Loss: 0.1050 | 0.1348
Epoch 117/300, seasonal_0 Loss: 0.1042 | 0.1339
Epoch 118/300, seasonal_0 Loss: 0.1035 | 0.1332
Epoch 119/300, seasonal_0 Loss: 0.1032 | 0.1327
Epoch 120/300, seasonal_0 Loss: 0.1022 | 0.1319
Epoch 121/300, seasonal_0 Loss: 0.1028 | 0.1318
Epoch 122/300, seasonal_0 Loss: 0.1022 | 0.1310
Epoch 123/300, seasonal_0 Loss: 0.1013 | 0.1305
Epoch 124/300, seasonal_0 Loss: 0.1017 | 0.1302
Epoch 125/300, seasonal_0 Loss: 0.1009 | 0.1293
Epoch 126/300, seasonal_0 Loss: 0.1010 | 0.1299
Epoch 127/300, seasonal_0 Loss: 0.1007 | 0.1287
Epoch 128/300, seasonal_0 Loss: 0.1009 | 0.1287
Epoch 129/300, seasonal_0 Loss: 0.1000 | 0.1286
Epoch 130/300, seasonal_0 Loss: 0.0998 | 0.1281
Epoch 131/300, seasonal_0 Loss: 0.0998 | 0.1272
Epoch 132/300, seasonal_0 Loss: 0.0994 | 0.1277
Epoch 133/300, seasonal_0 Loss: 0.0993 | 0.1273
Epoch 134/300, seasonal_0 Loss: 0.0988 | 0.1273
Epoch 135/300, seasonal_0 Loss: 0.0990 | 0.1265
Epoch 136/300, seasonal_0 Loss: 0.0982 | 0.1268
Epoch 137/300, seasonal_0 Loss: 0.0989 | 0.1271
Epoch 138/300, seasonal_0 Loss: 0.0988 | 0.1273
Epoch 139/300, seasonal_0 Loss: 0.0973 | 0.1272
Epoch 140/300, seasonal_0 Loss: 0.0975 | 0.1264
Epoch 141/300, seasonal_0 Loss: 0.0974 | 0.1248
Epoch 142/300, seasonal_0 Loss: 0.0976 | 0.1251
Epoch 143/300, seasonal_0 Loss: 0.0977 | 0.1249
Epoch 144/300, seasonal_0 Loss: 0.0972 | 0.1249
Epoch 145/300, seasonal_0 Loss: 0.0967 | 0.1247
Epoch 146/300, seasonal_0 Loss: 0.0976 | 0.1243
Epoch 147/300, seasonal_0 Loss: 0.0966 | 0.1239
Epoch 148/300, seasonal_0 Loss: 0.0966 | 0.1241
Epoch 149/300, seasonal_0 Loss: 0.0959 | 0.1236
Epoch 150/300, seasonal_0 Loss: 0.0958 | 0.1241
Epoch 151/300, seasonal_0 Loss: 0.0958 | 0.1241
Epoch 152/300, seasonal_0 Loss: 0.0964 | 0.1243
Epoch 153/300, seasonal_0 Loss: 0.0958 | 0.1244
Epoch 154/300, seasonal_0 Loss: 0.0956 | 0.1233
Epoch 155/300, seasonal_0 Loss: 0.0954 | 0.1227
Epoch 156/300, seasonal_0 Loss: 0.0959 | 0.1231
Epoch 157/300, seasonal_0 Loss: 0.0954 | 0.1231
Epoch 158/300, seasonal_0 Loss: 0.0958 | 0.1225
Epoch 159/300, seasonal_0 Loss: 0.0947 | 0.1226
Epoch 160/300, seasonal_0 Loss: 0.0946 | 0.1227
Epoch 161/300, seasonal_0 Loss: 0.0947 | 0.1220
Epoch 162/300, seasonal_0 Loss: 0.0949 | 0.1217
Epoch 163/300, seasonal_0 Loss: 0.0944 | 0.1214
Epoch 164/300, seasonal_0 Loss: 0.0947 | 0.1219
Epoch 165/300, seasonal_0 Loss: 0.0947 | 0.1211
Epoch 166/300, seasonal_0 Loss: 0.0945 | 0.1210
Epoch 167/300, seasonal_0 Loss: 0.0936 | 0.1212
Epoch 168/300, seasonal_0 Loss: 0.0937 | 0.1217
Epoch 169/300, seasonal_0 Loss: 0.0942 | 0.1212
Epoch 170/300, seasonal_0 Loss: 0.0940 | 0.1210
Epoch 171/300, seasonal_0 Loss: 0.0939 | 0.1212
Epoch 172/300, seasonal_0 Loss: 0.0940 | 0.1214
Epoch 173/300, seasonal_0 Loss: 0.0933 | 0.1212
Epoch 174/300, seasonal_0 Loss: 0.0930 | 0.1210
Epoch 175/300, seasonal_0 Loss: 0.0931 | 0.1211
Epoch 176/300, seasonal_0 Loss: 0.0935 | 0.1207
Epoch 177/300, seasonal_0 Loss: 0.0932 | 0.1202
Epoch 178/300, seasonal_0 Loss: 0.0928 | 0.1199
Epoch 179/300, seasonal_0 Loss: 0.0931 | 0.1198
Epoch 180/300, seasonal_0 Loss: 0.0929 | 0.1198
Epoch 181/300, seasonal_0 Loss: 0.0926 | 0.1198
Epoch 182/300, seasonal_0 Loss: 0.0929 | 0.1197
Epoch 183/300, seasonal_0 Loss: 0.0924 | 0.1194
Epoch 184/300, seasonal_0 Loss: 0.0926 | 0.1192
Epoch 185/300, seasonal_0 Loss: 0.0917 | 0.1190
Epoch 186/300, seasonal_0 Loss: 0.0923 | 0.1192
Epoch 187/300, seasonal_0 Loss: 0.0923 | 0.1194
Epoch 188/300, seasonal_0 Loss: 0.0921 | 0.1192
Epoch 189/300, seasonal_0 Loss: 0.0919 | 0.1191
Epoch 190/300, seasonal_0 Loss: 0.0919 | 0.1188
Epoch 191/300, seasonal_0 Loss: 0.0920 | 0.1185
Epoch 192/300, seasonal_0 Loss: 0.0923 | 0.1192
Epoch 193/300, seasonal_0 Loss: 0.0919 | 0.1190
Epoch 194/300, seasonal_0 Loss: 0.0918 | 0.1191
Epoch 195/300, seasonal_0 Loss: 0.0915 | 0.1186
Epoch 196/300, seasonal_0 Loss: 0.0916 | 0.1185
Epoch 197/300, seasonal_0 Loss: 0.0920 | 0.1184
Epoch 198/300, seasonal_0 Loss: 0.0919 | 0.1179
Epoch 199/300, seasonal_0 Loss: 0.0912 | 0.1180
Epoch 200/300, seasonal_0 Loss: 0.0918 | 0.1183
Epoch 201/300, seasonal_0 Loss: 0.0910 | 0.1182
Epoch 202/300, seasonal_0 Loss: 0.0915 | 0.1187
Epoch 203/300, seasonal_0 Loss: 0.0907 | 0.1187
Epoch 204/300, seasonal_0 Loss: 0.0910 | 0.1181
Epoch 205/300, seasonal_0 Loss: 0.0906 | 0.1177
Epoch 206/300, seasonal_0 Loss: 0.0911 | 0.1175
Epoch 207/300, seasonal_0 Loss: 0.0910 | 0.1177
Epoch 208/300, seasonal_0 Loss: 0.0907 | 0.1178
Epoch 209/300, seasonal_0 Loss: 0.0913 | 0.1176
Epoch 210/300, seasonal_0 Loss: 0.0910 | 0.1174
Epoch 211/300, seasonal_0 Loss: 0.0903 | 0.1175
Epoch 212/300, seasonal_0 Loss: 0.0907 | 0.1172
Epoch 213/300, seasonal_0 Loss: 0.0910 | 0.1173
Epoch 214/300, seasonal_0 Loss: 0.0907 | 0.1175
Epoch 215/300, seasonal_0 Loss: 0.0909 | 0.1177
Epoch 216/300, seasonal_0 Loss: 0.0907 | 0.1176
Epoch 217/300, seasonal_0 Loss: 0.0907 | 0.1175
Epoch 218/300, seasonal_0 Loss: 0.0919 | 0.1175
Epoch 219/300, seasonal_0 Loss: 0.0901 | 0.1177
Epoch 220/300, seasonal_0 Loss: 0.0905 | 0.1172
Epoch 221/300, seasonal_0 Loss: 0.0904 | 0.1169
Epoch 222/300, seasonal_0 Loss: 0.0902 | 0.1169
Epoch 223/300, seasonal_0 Loss: 0.0903 | 0.1166
Epoch 224/300, seasonal_0 Loss: 0.0900 | 0.1165
Epoch 225/300, seasonal_0 Loss: 0.0901 | 0.1166
Epoch 226/300, seasonal_0 Loss: 0.0904 | 0.1168
Epoch 227/300, seasonal_0 Loss: 0.0900 | 0.1167
Epoch 228/300, seasonal_0 Loss: 0.0895 | 0.1167
Epoch 229/300, seasonal_0 Loss: 0.0900 | 0.1167
Epoch 230/300, seasonal_0 Loss: 0.0897 | 0.1168
Epoch 231/300, seasonal_0 Loss: 0.0897 | 0.1169
Epoch 232/300, seasonal_0 Loss: 0.0898 | 0.1168
Epoch 233/300, seasonal_0 Loss: 0.0896 | 0.1166
Epoch 234/300, seasonal_0 Loss: 0.0893 | 0.1163
Epoch 235/300, seasonal_0 Loss: 0.0893 | 0.1164
Epoch 236/300, seasonal_0 Loss: 0.0893 | 0.1164
Epoch 237/300, seasonal_0 Loss: 0.0895 | 0.1164
Epoch 238/300, seasonal_0 Loss: 0.0895 | 0.1165
Epoch 239/300, seasonal_0 Loss: 0.0893 | 0.1165
Epoch 240/300, seasonal_0 Loss: 0.0898 | 0.1165
Epoch 241/300, seasonal_0 Loss: 0.0886 | 0.1162
Epoch 242/300, seasonal_0 Loss: 0.0893 | 0.1163
Epoch 243/300, seasonal_0 Loss: 0.0891 | 0.1163
Epoch 244/300, seasonal_0 Loss: 0.0899 | 0.1161
Epoch 245/300, seasonal_0 Loss: 0.0893 | 0.1158
Epoch 246/300, seasonal_0 Loss: 0.0895 | 0.1160
Epoch 247/300, seasonal_0 Loss: 0.0897 | 0.1158
Epoch 248/300, seasonal_0 Loss: 0.0890 | 0.1156
Epoch 249/300, seasonal_0 Loss: 0.0893 | 0.1159
Epoch 250/300, seasonal_0 Loss: 0.0889 | 0.1165
Epoch 251/300, seasonal_0 Loss: 0.0892 | 0.1164
Epoch 252/300, seasonal_0 Loss: 0.0892 | 0.1161
Epoch 253/300, seasonal_0 Loss: 0.0897 | 0.1159
Epoch 254/300, seasonal_0 Loss: 0.0892 | 0.1158
Epoch 255/300, seasonal_0 Loss: 0.0889 | 0.1158
Epoch 256/300, seasonal_0 Loss: 0.0888 | 0.1160
Epoch 257/300, seasonal_0 Loss: 0.0891 | 0.1159
Epoch 258/300, seasonal_0 Loss: 0.0890 | 0.1159
Epoch 259/300, seasonal_0 Loss: 0.0895 | 0.1161
Epoch 260/300, seasonal_0 Loss: 0.0888 | 0.1162
Epoch 261/300, seasonal_0 Loss: 0.0882 | 0.1161
Epoch 262/300, seasonal_0 Loss: 0.0886 | 0.1161
Epoch 263/300, seasonal_0 Loss: 0.0893 | 0.1160
Epoch 264/300, seasonal_0 Loss: 0.0900 | 0.1160
Epoch 265/300, seasonal_0 Loss: 0.0892 | 0.1162
Epoch 266/300, seasonal_0 Loss: 0.0897 | 0.1163
Epoch 267/300, seasonal_0 Loss: 0.0884 | 0.1164
Epoch 268/300, seasonal_0 Loss: 0.0884 | 0.1164
Epoch 269/300, seasonal_0 Loss: 0.0891 | 0.1164
Epoch 270/300, seasonal_0 Loss: 0.0891 | 0.1163
Epoch 271/300, seasonal_0 Loss: 0.0888 | 0.1161
Epoch 272/300, seasonal_0 Loss: 0.0882 | 0.1158
Epoch 273/300, seasonal_0 Loss: 0.0889 | 0.1156
Epoch 274/300, seasonal_0 Loss: 0.0876 | 0.1156
Epoch 275/300, seasonal_0 Loss: 0.0886 | 0.1155
Epoch 276/300, seasonal_0 Loss: 0.0884 | 0.1155
Epoch 277/300, seasonal_0 Loss: 0.0886 | 0.1155
Epoch 278/300, seasonal_0 Loss: 0.0880 | 0.1155
Epoch 279/300, seasonal_0 Loss: 0.0885 | 0.1156
Epoch 280/300, seasonal_0 Loss: 0.0882 | 0.1155
Epoch 281/300, seasonal_0 Loss: 0.0880 | 0.1155
Epoch 282/300, seasonal_0 Loss: 0.0891 | 0.1153
Epoch 283/300, seasonal_0 Loss: 0.0885 | 0.1154
Epoch 284/300, seasonal_0 Loss: 0.0890 | 0.1154
Epoch 285/300, seasonal_0 Loss: 0.0885 | 0.1153
Epoch 286/300, seasonal_0 Loss: 0.0887 | 0.1153
Epoch 287/300, seasonal_0 Loss: 0.0884 | 0.1153
Epoch 288/300, seasonal_0 Loss: 0.0882 | 0.1153
Epoch 289/300, seasonal_0 Loss: 0.0884 | 0.1153
Epoch 290/300, seasonal_0 Loss: 0.0886 | 0.1152
Epoch 291/300, seasonal_0 Loss: 0.0881 | 0.1152
Epoch 292/300, seasonal_0 Loss: 0.0876 | 0.1151
Epoch 293/300, seasonal_0 Loss: 0.0880 | 0.1152
Epoch 294/300, seasonal_0 Loss: 0.0879 | 0.1151
Epoch 295/300, seasonal_0 Loss: 0.0882 | 0.1151
Epoch 296/300, seasonal_0 Loss: 0.0884 | 0.1150
Epoch 297/300, seasonal_0 Loss: 0.0883 | 0.1151
Epoch 298/300, seasonal_0 Loss: 0.0887 | 0.1151
Epoch 299/300, seasonal_0 Loss: 0.0888 | 0.1152
Epoch 300/300, seasonal_0 Loss: 0.0886 | 0.1151
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.9618639406159732, 'learning_rate': 4.3468440312567085e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9379950099051507}
Epoch 1/300, seasonal_1 Loss: 0.5612 | 0.5565
Epoch 2/300, seasonal_1 Loss: 0.3694 | 0.4346
Epoch 3/300, seasonal_1 Loss: 0.2933 | 0.3718
Epoch 4/300, seasonal_1 Loss: 0.2407 | 0.3072
Epoch 5/300, seasonal_1 Loss: 0.2135 | 0.2421
Epoch 6/300, seasonal_1 Loss: 0.1966 | 0.2145
Epoch 7/300, seasonal_1 Loss: 0.1806 | 0.2004
Epoch 8/300, seasonal_1 Loss: 0.1702 | 0.2021
Epoch 9/300, seasonal_1 Loss: 0.1639 | 0.1878
Epoch 10/300, seasonal_1 Loss: 0.1583 | 0.1791
Epoch 11/300, seasonal_1 Loss: 0.1534 | 0.1695
Epoch 12/300, seasonal_1 Loss: 0.1505 | 0.1617
Epoch 13/300, seasonal_1 Loss: 0.1449 | 0.1616
Epoch 14/300, seasonal_1 Loss: 0.1428 | 0.1522
Epoch 15/300, seasonal_1 Loss: 0.1398 | 0.1463
Epoch 16/300, seasonal_1 Loss: 0.1382 | 0.1400
Epoch 17/300, seasonal_1 Loss: 0.1323 | 0.1403
Epoch 18/300, seasonal_1 Loss: 0.1290 | 0.1374
Epoch 19/300, seasonal_1 Loss: 0.1260 | 0.1312
Epoch 20/300, seasonal_1 Loss: 0.1240 | 0.1246
Epoch 21/300, seasonal_1 Loss: 0.1216 | 0.1254
Epoch 22/300, seasonal_1 Loss: 0.1206 | 0.1256
Epoch 23/300, seasonal_1 Loss: 0.1177 | 0.1201
Epoch 24/300, seasonal_1 Loss: 0.1161 | 0.1185
Epoch 25/300, seasonal_1 Loss: 0.1132 | 0.1178
Epoch 26/300, seasonal_1 Loss: 0.1110 | 0.1198
Epoch 27/300, seasonal_1 Loss: 0.1101 | 0.1190
Epoch 28/300, seasonal_1 Loss: 0.1088 | 0.1097
Epoch 29/300, seasonal_1 Loss: 0.1077 | 0.1034
Epoch 30/300, seasonal_1 Loss: 0.1064 | 0.1019
Epoch 31/300, seasonal_1 Loss: 0.1052 | 0.1116
Epoch 32/300, seasonal_1 Loss: 0.1037 | 0.1128
Epoch 33/300, seasonal_1 Loss: 0.1030 | 0.1037
Epoch 34/300, seasonal_1 Loss: 0.1025 | 0.0978
Epoch 35/300, seasonal_1 Loss: 0.1018 | 0.0996
Epoch 36/300, seasonal_1 Loss: 0.1006 | 0.1058
Epoch 37/300, seasonal_1 Loss: 0.1002 | 0.1059
Epoch 38/300, seasonal_1 Loss: 0.0980 | 0.0981
Epoch 39/300, seasonal_1 Loss: 0.0985 | 0.0943
Epoch 40/300, seasonal_1 Loss: 0.0979 | 0.0940
Epoch 41/300, seasonal_1 Loss: 0.0966 | 0.1003
Epoch 42/300, seasonal_1 Loss: 0.0962 | 0.1010
Epoch 43/300, seasonal_1 Loss: 0.0952 | 0.0956
Epoch 44/300, seasonal_1 Loss: 0.0951 | 0.0926
Epoch 45/300, seasonal_1 Loss: 0.0947 | 0.0914
Epoch 46/300, seasonal_1 Loss: 0.0937 | 0.0944
Epoch 47/300, seasonal_1 Loss: 0.0936 | 0.0949
Epoch 48/300, seasonal_1 Loss: 0.0925 | 0.0924
Epoch 49/300, seasonal_1 Loss: 0.0923 | 0.0915
Epoch 50/300, seasonal_1 Loss: 0.0921 | 0.0906
Epoch 51/300, seasonal_1 Loss: 0.0916 | 0.0921
Epoch 52/300, seasonal_1 Loss: 0.0913 | 0.0903
Epoch 53/300, seasonal_1 Loss: 0.0910 | 0.0905
Epoch 54/300, seasonal_1 Loss: 0.0909 | 0.0892
Epoch 55/300, seasonal_1 Loss: 0.0908 | 0.0903
Epoch 56/300, seasonal_1 Loss: 0.0905 | 0.0908
Epoch 57/300, seasonal_1 Loss: 0.0899 | 0.0891
Epoch 58/300, seasonal_1 Loss: 0.0892 | 0.0882
Epoch 59/300, seasonal_1 Loss: 0.0893 | 0.0877
Epoch 60/300, seasonal_1 Loss: 0.0894 | 0.0896
Epoch 61/300, seasonal_1 Loss: 0.0884 | 0.0891
Epoch 62/300, seasonal_1 Loss: 0.0889 | 0.0883
Epoch 63/300, seasonal_1 Loss: 0.0886 | 0.0879
Epoch 64/300, seasonal_1 Loss: 0.0882 | 0.0886
Epoch 65/300, seasonal_1 Loss: 0.0883 | 0.0882
Epoch 66/300, seasonal_1 Loss: 0.0876 | 0.0878
Epoch 67/300, seasonal_1 Loss: 0.0876 | 0.0874
Epoch 68/300, seasonal_1 Loss: 0.0876 | 0.0873
Epoch 69/300, seasonal_1 Loss: 0.0878 | 0.0877
Epoch 70/300, seasonal_1 Loss: 0.0875 | 0.0865
Epoch 71/300, seasonal_1 Loss: 0.0874 | 0.0870
Epoch 72/300, seasonal_1 Loss: 0.0872 | 0.0862
Epoch 73/300, seasonal_1 Loss: 0.0869 | 0.0866
Epoch 74/300, seasonal_1 Loss: 0.0871 | 0.0869
Epoch 75/300, seasonal_1 Loss: 0.0869 | 0.0864
Epoch 76/300, seasonal_1 Loss: 0.0864 | 0.0855
Epoch 77/300, seasonal_1 Loss: 0.0865 | 0.0859
Epoch 78/300, seasonal_1 Loss: 0.0862 | 0.0856
Epoch 79/300, seasonal_1 Loss: 0.0860 | 0.0849
Epoch 80/300, seasonal_1 Loss: 0.0863 | 0.0853
Epoch 81/300, seasonal_1 Loss: 0.0860 | 0.0853
Epoch 82/300, seasonal_1 Loss: 0.0861 | 0.0848
Epoch 83/300, seasonal_1 Loss: 0.0858 | 0.0858
Epoch 84/300, seasonal_1 Loss: 0.0858 | 0.0847
Epoch 85/300, seasonal_1 Loss: 0.0856 | 0.0847
Epoch 86/300, seasonal_1 Loss: 0.0865 | 0.0845
Epoch 87/300, seasonal_1 Loss: 0.0852 | 0.0854
Epoch 88/300, seasonal_1 Loss: 0.0855 | 0.0848
Epoch 89/300, seasonal_1 Loss: 0.0856 | 0.0847
Epoch 90/300, seasonal_1 Loss: 0.0847 | 0.0848
Epoch 91/300, seasonal_1 Loss: 0.0852 | 0.0851
Epoch 92/300, seasonal_1 Loss: 0.0855 | 0.0850
Epoch 93/300, seasonal_1 Loss: 0.0857 | 0.0846
Epoch 94/300, seasonal_1 Loss: 0.0850 | 0.0846
Epoch 95/300, seasonal_1 Loss: 0.0854 | 0.0844
Epoch 96/300, seasonal_1 Loss: 0.0849 | 0.0846
Epoch 97/300, seasonal_1 Loss: 0.0851 | 0.0848
Epoch 98/300, seasonal_1 Loss: 0.0853 | 0.0851
Epoch 99/300, seasonal_1 Loss: 0.0845 | 0.0844
Epoch 100/300, seasonal_1 Loss: 0.0852 | 0.0844
Epoch 101/300, seasonal_1 Loss: 0.0857 | 0.0844
Epoch 102/300, seasonal_1 Loss: 0.0852 | 0.0845
Epoch 103/300, seasonal_1 Loss: 0.0842 | 0.0845
Epoch 104/300, seasonal_1 Loss: 0.0841 | 0.0843
Epoch 105/300, seasonal_1 Loss: 0.0848 | 0.0845
Epoch 106/300, seasonal_1 Loss: 0.0848 | 0.0846
Epoch 107/300, seasonal_1 Loss: 0.0845 | 0.0845
Epoch 108/300, seasonal_1 Loss: 0.0850 | 0.0846
Epoch 109/300, seasonal_1 Loss: 0.0850 | 0.0844
Epoch 110/300, seasonal_1 Loss: 0.0850 | 0.0845
Epoch 111/300, seasonal_1 Loss: 0.0849 | 0.0846
Epoch 112/300, seasonal_1 Loss: 0.0847 | 0.0845
Epoch 113/300, seasonal_1 Loss: 0.0847 | 0.0843
Epoch 114/300, seasonal_1 Loss: 0.0838 | 0.0846
Epoch 115/300, seasonal_1 Loss: 0.0854 | 0.0844
Epoch 116/300, seasonal_1 Loss: 0.0851 | 0.0844
Epoch 117/300, seasonal_1 Loss: 0.0846 | 0.0844
Epoch 118/300, seasonal_1 Loss: 0.0848 | 0.0843
Epoch 119/300, seasonal_1 Loss: 0.0841 | 0.0843
Epoch 120/300, seasonal_1 Loss: 0.0849 | 0.0844
Epoch 121/300, seasonal_1 Loss: 0.0847 | 0.0843
Epoch 122/300, seasonal_1 Loss: 0.0843 | 0.0843
Epoch 123/300, seasonal_1 Loss: 0.0846 | 0.0843
Epoch 124/300, seasonal_1 Loss: 0.0845 | 0.0843
Epoch 125/300, seasonal_1 Loss: 0.0846 | 0.0844
Epoch 126/300, seasonal_1 Loss: 0.0841 | 0.0844
Epoch 127/300, seasonal_1 Loss: 0.0846 | 0.0842
Epoch 128/300, seasonal_1 Loss: 0.0842 | 0.0842
Epoch 129/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 130/300, seasonal_1 Loss: 0.0844 | 0.0842
Epoch 131/300, seasonal_1 Loss: 0.0842 | 0.0843
Epoch 132/300, seasonal_1 Loss: 0.0845 | 0.0844
Epoch 133/300, seasonal_1 Loss: 0.0845 | 0.0844
Epoch 134/300, seasonal_1 Loss: 0.0841 | 0.0843
Epoch 135/300, seasonal_1 Loss: 0.0844 | 0.0842
Epoch 136/300, seasonal_1 Loss: 0.0846 | 0.0842
Epoch 137/300, seasonal_1 Loss: 0.0847 | 0.0842
Epoch 138/300, seasonal_1 Loss: 0.0847 | 0.0842
Epoch 139/300, seasonal_1 Loss: 0.0842 | 0.0842
Epoch 140/300, seasonal_1 Loss: 0.0841 | 0.0841
Epoch 141/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 142/300, seasonal_1 Loss: 0.0846 | 0.0841
Epoch 143/300, seasonal_1 Loss: 0.0847 | 0.0841
Epoch 144/300, seasonal_1 Loss: 0.0842 | 0.0841
Epoch 145/300, seasonal_1 Loss: 0.0837 | 0.0841
Epoch 146/300, seasonal_1 Loss: 0.0843 | 0.0841
Epoch 147/300, seasonal_1 Loss: 0.0843 | 0.0841
Epoch 148/300, seasonal_1 Loss: 0.0846 | 0.0841
Epoch 149/300, seasonal_1 Loss: 0.0845 | 0.0841
Epoch 150/300, seasonal_1 Loss: 0.0843 | 0.0841
Epoch 151/300, seasonal_1 Loss: 0.0840 | 0.0841
Epoch 152/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 153/300, seasonal_1 Loss: 0.0838 | 0.0841
Epoch 154/300, seasonal_1 Loss: 0.0841 | 0.0841
Epoch 155/300, seasonal_1 Loss: 0.0836 | 0.0840
Epoch 156/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 157/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 158/300, seasonal_1 Loss: 0.0837 | 0.0840
Epoch 159/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 160/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 161/300, seasonal_1 Loss: 0.0836 | 0.0841
Epoch 162/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 163/300, seasonal_1 Loss: 0.0836 | 0.0840
Epoch 164/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 165/300, seasonal_1 Loss: 0.0839 | 0.0841
Epoch 166/300, seasonal_1 Loss: 0.0842 | 0.0841
Epoch 167/300, seasonal_1 Loss: 0.0842 | 0.0841
Epoch 168/300, seasonal_1 Loss: 0.0839 | 0.0841
Epoch 169/300, seasonal_1 Loss: 0.0838 | 0.0841
Epoch 170/300, seasonal_1 Loss: 0.0843 | 0.0841
Epoch 171/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 172/300, seasonal_1 Loss: 0.0841 | 0.0841
Epoch 173/300, seasonal_1 Loss: 0.0845 | 0.0841
Epoch 174/300, seasonal_1 Loss: 0.0840 | 0.0841
Epoch 175/300, seasonal_1 Loss: 0.0846 | 0.0841
Epoch 176/300, seasonal_1 Loss: 0.0841 | 0.0841
Epoch 177/300, seasonal_1 Loss: 0.0838 | 0.0841
Epoch 178/300, seasonal_1 Loss: 0.0843 | 0.0841
Epoch 179/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 180/300, seasonal_1 Loss: 0.0838 | 0.0841
Epoch 181/300, seasonal_1 Loss: 0.0841 | 0.0841
Epoch 182/300, seasonal_1 Loss: 0.0844 | 0.0841
Epoch 183/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 184/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 185/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 186/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 187/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 188/300, seasonal_1 Loss: 0.0838 | 0.0840
Epoch 189/300, seasonal_1 Loss: 0.0846 | 0.0840
Epoch 190/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 191/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 192/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 193/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 194/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 195/300, seasonal_1 Loss: 0.0849 | 0.0840
Epoch 196/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 197/300, seasonal_1 Loss: 0.0849 | 0.0840
Epoch 198/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 199/300, seasonal_1 Loss: 0.0838 | 0.0840
Epoch 200/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 201/300, seasonal_1 Loss: 0.0838 | 0.0840
Epoch 202/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 203/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 204/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 205/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 206/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 207/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 208/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 209/300, seasonal_1 Loss: 0.0851 | 0.0840
Epoch 210/300, seasonal_1 Loss: 0.0848 | 0.0840
Epoch 211/300, seasonal_1 Loss: 0.0851 | 0.0840
Epoch 212/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 213/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 214/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 215/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 216/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 217/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 218/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 219/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 220/300, seasonal_1 Loss: 0.0836 | 0.0840
Epoch 221/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 222/300, seasonal_1 Loss: 0.0838 | 0.0840
Epoch 223/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 224/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 225/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 226/300, seasonal_1 Loss: 0.0847 | 0.0840
Epoch 227/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 228/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 229/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 230/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 231/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 232/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 233/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 234/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 235/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 236/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 237/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 238/300, seasonal_1 Loss: 0.0837 | 0.0840
Epoch 239/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 240/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 241/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 242/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 243/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 244/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 245/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 246/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 247/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 248/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 249/300, seasonal_1 Loss: 0.0846 | 0.0840
Epoch 250/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 251/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 252/300, seasonal_1 Loss: 0.0848 | 0.0840
Epoch 253/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 254/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 255/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 256/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 257/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 258/300, seasonal_1 Loss: 0.0839 | 0.0840
Epoch 259/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 260/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 261/300, seasonal_1 Loss: 0.0846 | 0.0840
Epoch 262/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 263/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 264/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 265/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 266/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 267/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 268/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 269/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 270/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 271/300, seasonal_1 Loss: 0.0836 | 0.0840
Epoch 272/300, seasonal_1 Loss: 0.0849 | 0.0840
Epoch 273/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 274/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 275/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 276/300, seasonal_1 Loss: 0.0847 | 0.0840
Epoch 277/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 278/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 279/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 280/300, seasonal_1 Loss: 0.0843 | 0.0840
Epoch 281/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 282/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 283/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 284/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 285/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 286/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 287/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 288/300, seasonal_1 Loss: 0.0847 | 0.0840
Epoch 289/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 290/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 291/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 292/300, seasonal_1 Loss: 0.0837 | 0.0840
Epoch 293/300, seasonal_1 Loss: 0.0841 | 0.0840
Epoch 294/300, seasonal_1 Loss: 0.0845 | 0.0840
Epoch 295/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 296/300, seasonal_1 Loss: 0.0844 | 0.0840
Epoch 297/300, seasonal_1 Loss: 0.0842 | 0.0840
Epoch 298/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 299/300, seasonal_1 Loss: 0.0840 | 0.0840
Epoch 300/300, seasonal_1 Loss: 0.0838 | 0.0840
Training seasonal_2 component with params: {'observation_period_num': 13, 'train_rates': 0.935385766699552, 'learning_rate': 0.00017319215956000322, 'batch_size': 43, 'step_size': 4, 'gamma': 0.8790404803864401}
Epoch 1/300, seasonal_2 Loss: 0.6933 | 0.8001
Epoch 2/300, seasonal_2 Loss: 0.6698 | 0.8342
Epoch 3/300, seasonal_2 Loss: 0.5589 | 0.6610
Epoch 4/300, seasonal_2 Loss: 0.4832 | 0.6327
Epoch 5/300, seasonal_2 Loss: 0.4972 | 0.6920
Epoch 6/300, seasonal_2 Loss: 0.5264 | 0.6547
Epoch 7/300, seasonal_2 Loss: 0.4754 | 0.5288
Epoch 8/300, seasonal_2 Loss: 0.3868 | 0.5157
Epoch 9/300, seasonal_2 Loss: 0.3292 | 0.3957
Epoch 10/300, seasonal_2 Loss: 0.2808 | 0.3588
Epoch 11/300, seasonal_2 Loss: 0.3120 | 0.3221
Epoch 12/300, seasonal_2 Loss: 0.2615 | 0.3546
Epoch 13/300, seasonal_2 Loss: 0.2413 | 0.3021
Epoch 14/300, seasonal_2 Loss: 0.2328 | 0.2632
Epoch 15/300, seasonal_2 Loss: 0.2134 | 0.2673
Epoch 16/300, seasonal_2 Loss: 0.1951 | 0.2561
Epoch 17/300, seasonal_2 Loss: 0.1872 | 0.2340
Epoch 18/300, seasonal_2 Loss: 0.1807 | 0.2314
Epoch 19/300, seasonal_2 Loss: 0.1722 | 0.2233
Epoch 20/300, seasonal_2 Loss: 0.1703 | 0.2139
Epoch 21/300, seasonal_2 Loss: 0.1639 | 0.2169
Epoch 22/300, seasonal_2 Loss: 0.1587 | 0.2053
Epoch 23/300, seasonal_2 Loss: 0.1559 | 0.1998
Epoch 24/300, seasonal_2 Loss: 0.1525 | 0.1958
Epoch 25/300, seasonal_2 Loss: 0.1503 | 0.1891
Epoch 26/300, seasonal_2 Loss: 0.1471 | 0.1865
Epoch 27/300, seasonal_2 Loss: 0.1448 | 0.1841
Epoch 28/300, seasonal_2 Loss: 0.1431 | 0.1804
Epoch 29/300, seasonal_2 Loss: 0.1406 | 0.1784
Epoch 30/300, seasonal_2 Loss: 0.1396 | 0.1754
Epoch 31/300, seasonal_2 Loss: 0.1380 | 0.1733
Epoch 32/300, seasonal_2 Loss: 0.1363 | 0.1695
Epoch 33/300, seasonal_2 Loss: 0.1358 | 0.1683
Epoch 34/300, seasonal_2 Loss: 0.1342 | 0.1668
Epoch 35/300, seasonal_2 Loss: 0.1327 | 0.1639
Epoch 36/300, seasonal_2 Loss: 0.1322 | 0.1624
Epoch 37/300, seasonal_2 Loss: 0.1310 | 0.1617
Epoch 38/300, seasonal_2 Loss: 0.1307 | 0.1601
Epoch 39/300, seasonal_2 Loss: 0.1292 | 0.1574
Epoch 40/300, seasonal_2 Loss: 0.1288 | 0.1570
Epoch 41/300, seasonal_2 Loss: 0.1280 | 0.1565
Epoch 42/300, seasonal_2 Loss: 0.1280 | 0.1548
Epoch 43/300, seasonal_2 Loss: 0.1269 | 0.1526
Epoch 44/300, seasonal_2 Loss: 0.1265 | 0.1528
Epoch 45/300, seasonal_2 Loss: 0.1258 | 0.1514
Epoch 46/300, seasonal_2 Loss: 0.1257 | 0.1506
Epoch 47/300, seasonal_2 Loss: 0.1244 | 0.1495
Epoch 48/300, seasonal_2 Loss: 0.1245 | 0.1489
Epoch 49/300, seasonal_2 Loss: 0.1239 | 0.1480
Epoch 50/300, seasonal_2 Loss: 0.1237 | 0.1473
Epoch 51/300, seasonal_2 Loss: 0.1232 | 0.1467
Epoch 52/300, seasonal_2 Loss: 0.1223 | 0.1463
Epoch 53/300, seasonal_2 Loss: 0.1224 | 0.1455
Epoch 54/300, seasonal_2 Loss: 0.1216 | 0.1448
Epoch 55/300, seasonal_2 Loss: 0.1214 | 0.1439
Epoch 56/300, seasonal_2 Loss: 0.1210 | 0.1433
Epoch 57/300, seasonal_2 Loss: 0.1211 | 0.1427
Epoch 58/300, seasonal_2 Loss: 0.1213 | 0.1425
Epoch 59/300, seasonal_2 Loss: 0.1202 | 0.1423
Epoch 60/300, seasonal_2 Loss: 0.1202 | 0.1413
Epoch 61/300, seasonal_2 Loss: 0.1203 | 0.1409
Epoch 62/300, seasonal_2 Loss: 0.1199 | 0.1409
Epoch 63/300, seasonal_2 Loss: 0.1193 | 0.1409
Epoch 64/300, seasonal_2 Loss: 0.1193 | 0.1405
Epoch 65/300, seasonal_2 Loss: 0.1194 | 0.1401
Epoch 66/300, seasonal_2 Loss: 0.1186 | 0.1395
Epoch 67/300, seasonal_2 Loss: 0.1186 | 0.1391
Epoch 68/300, seasonal_2 Loss: 0.1188 | 0.1388
Epoch 69/300, seasonal_2 Loss: 0.1185 | 0.1388
Epoch 70/300, seasonal_2 Loss: 0.1185 | 0.1384
Epoch 71/300, seasonal_2 Loss: 0.1176 | 0.1381
Epoch 72/300, seasonal_2 Loss: 0.1181 | 0.1379
Epoch 73/300, seasonal_2 Loss: 0.1177 | 0.1378
Epoch 74/300, seasonal_2 Loss: 0.1178 | 0.1376
Epoch 75/300, seasonal_2 Loss: 0.1171 | 0.1374
Epoch 76/300, seasonal_2 Loss: 0.1174 | 0.1374
Epoch 77/300, seasonal_2 Loss: 0.1175 | 0.1372
Epoch 78/300, seasonal_2 Loss: 0.1179 | 0.1371
Epoch 79/300, seasonal_2 Loss: 0.1179 | 0.1370
Epoch 80/300, seasonal_2 Loss: 0.1168 | 0.1369
Epoch 81/300, seasonal_2 Loss: 0.1171 | 0.1368
Epoch 82/300, seasonal_2 Loss: 0.1172 | 0.1368
Epoch 83/300, seasonal_2 Loss: 0.1170 | 0.1365
Epoch 84/300, seasonal_2 Loss: 0.1170 | 0.1363
Epoch 85/300, seasonal_2 Loss: 0.1171 | 0.1362
Epoch 86/300, seasonal_2 Loss: 0.1169 | 0.1361
Epoch 87/300, seasonal_2 Loss: 0.1170 | 0.1361
Epoch 88/300, seasonal_2 Loss: 0.1168 | 0.1360
Epoch 89/300, seasonal_2 Loss: 0.1161 | 0.1360
Epoch 90/300, seasonal_2 Loss: 0.1164 | 0.1359
Epoch 91/300, seasonal_2 Loss: 0.1171 | 0.1357
Epoch 92/300, seasonal_2 Loss: 0.1162 | 0.1356
Epoch 93/300, seasonal_2 Loss: 0.1166 | 0.1356
Epoch 94/300, seasonal_2 Loss: 0.1156 | 0.1355
Epoch 95/300, seasonal_2 Loss: 0.1166 | 0.1354
Epoch 96/300, seasonal_2 Loss: 0.1163 | 0.1353
Epoch 97/300, seasonal_2 Loss: 0.1161 | 0.1353
Epoch 98/300, seasonal_2 Loss: 0.1163 | 0.1352
Epoch 99/300, seasonal_2 Loss: 0.1161 | 0.1352
Epoch 100/300, seasonal_2 Loss: 0.1166 | 0.1352
Epoch 101/300, seasonal_2 Loss: 0.1158 | 0.1352
Epoch 102/300, seasonal_2 Loss: 0.1162 | 0.1351
Epoch 103/300, seasonal_2 Loss: 0.1160 | 0.1351
Epoch 104/300, seasonal_2 Loss: 0.1162 | 0.1351
Epoch 105/300, seasonal_2 Loss: 0.1157 | 0.1350
Epoch 106/300, seasonal_2 Loss: 0.1169 | 0.1350
Epoch 107/300, seasonal_2 Loss: 0.1166 | 0.1349
Epoch 108/300, seasonal_2 Loss: 0.1160 | 0.1349
Epoch 109/300, seasonal_2 Loss: 0.1158 | 0.1349
Epoch 110/300, seasonal_2 Loss: 0.1165 | 0.1349
Epoch 111/300, seasonal_2 Loss: 0.1160 | 0.1349
Epoch 112/300, seasonal_2 Loss: 0.1158 | 0.1349
Epoch 113/300, seasonal_2 Loss: 0.1165 | 0.1348
Epoch 114/300, seasonal_2 Loss: 0.1165 | 0.1348
Epoch 115/300, seasonal_2 Loss: 0.1156 | 0.1348
Epoch 116/300, seasonal_2 Loss: 0.1162 | 0.1348
Epoch 117/300, seasonal_2 Loss: 0.1166 | 0.1348
Epoch 118/300, seasonal_2 Loss: 0.1162 | 0.1348
Epoch 119/300, seasonal_2 Loss: 0.1161 | 0.1347
Epoch 120/300, seasonal_2 Loss: 0.1158 | 0.1347
Epoch 121/300, seasonal_2 Loss: 0.1164 | 0.1347
Epoch 122/300, seasonal_2 Loss: 0.1158 | 0.1347
Epoch 123/300, seasonal_2 Loss: 0.1163 | 0.1347
Epoch 124/300, seasonal_2 Loss: 0.1163 | 0.1347
Epoch 125/300, seasonal_2 Loss: 0.1167 | 0.1347
Epoch 126/300, seasonal_2 Loss: 0.1163 | 0.1347
Epoch 127/300, seasonal_2 Loss: 0.1160 | 0.1347
Epoch 128/300, seasonal_2 Loss: 0.1161 | 0.1347
Epoch 129/300, seasonal_2 Loss: 0.1163 | 0.1347
Epoch 130/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 131/300, seasonal_2 Loss: 0.1154 | 0.1346
Epoch 132/300, seasonal_2 Loss: 0.1158 | 0.1347
Epoch 133/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 134/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 135/300, seasonal_2 Loss: 0.1157 | 0.1346
Epoch 136/300, seasonal_2 Loss: 0.1157 | 0.1346
Epoch 137/300, seasonal_2 Loss: 0.1164 | 0.1346
Epoch 138/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 139/300, seasonal_2 Loss: 0.1168 | 0.1346
Epoch 140/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 141/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 142/300, seasonal_2 Loss: 0.1167 | 0.1346
Epoch 143/300, seasonal_2 Loss: 0.1166 | 0.1346
Epoch 144/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 145/300, seasonal_2 Loss: 0.1155 | 0.1346
Epoch 146/300, seasonal_2 Loss: 0.1155 | 0.1346
Epoch 147/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 148/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 149/300, seasonal_2 Loss: 0.1160 | 0.1346
Epoch 150/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 151/300, seasonal_2 Loss: 0.1169 | 0.1346
Epoch 152/300, seasonal_2 Loss: 0.1160 | 0.1346
Epoch 153/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 154/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 155/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 156/300, seasonal_2 Loss: 0.1152 | 0.1346
Epoch 157/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 158/300, seasonal_2 Loss: 0.1153 | 0.1346
Epoch 159/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 160/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 161/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 162/300, seasonal_2 Loss: 0.1168 | 0.1346
Epoch 163/300, seasonal_2 Loss: 0.1155 | 0.1346
Epoch 164/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 165/300, seasonal_2 Loss: 0.1160 | 0.1346
Epoch 166/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 167/300, seasonal_2 Loss: 0.1153 | 0.1346
Epoch 168/300, seasonal_2 Loss: 0.1157 | 0.1346
Epoch 169/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 170/300, seasonal_2 Loss: 0.1156 | 0.1346
Epoch 171/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 172/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 173/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 174/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 175/300, seasonal_2 Loss: 0.1156 | 0.1346
Epoch 176/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 177/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 178/300, seasonal_2 Loss: 0.1160 | 0.1346
Epoch 179/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 180/300, seasonal_2 Loss: 0.1156 | 0.1346
Epoch 181/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 182/300, seasonal_2 Loss: 0.1164 | 0.1346
Epoch 183/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 184/300, seasonal_2 Loss: 0.1159 | 0.1346
Epoch 185/300, seasonal_2 Loss: 0.1160 | 0.1346
Epoch 186/300, seasonal_2 Loss: 0.1167 | 0.1346
Epoch 187/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 188/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 189/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 190/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 191/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 192/300, seasonal_2 Loss: 0.1164 | 0.1346
Epoch 193/300, seasonal_2 Loss: 0.1165 | 0.1346
Epoch 194/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 195/300, seasonal_2 Loss: 0.1157 | 0.1346
Epoch 196/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 197/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 198/300, seasonal_2 Loss: 0.1161 | 0.1346
Epoch 199/300, seasonal_2 Loss: 0.1158 | 0.1346
Epoch 200/300, seasonal_2 Loss: 0.1157 | 0.1346
Epoch 201/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 202/300, seasonal_2 Loss: 0.1164 | 0.1346
Epoch 203/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 204/300, seasonal_2 Loss: 0.1169 | 0.1346
Epoch 205/300, seasonal_2 Loss: 0.1164 | 0.1346
Epoch 206/300, seasonal_2 Loss: 0.1162 | 0.1346
Epoch 207/300, seasonal_2 Loss: 0.1163 | 0.1346
Epoch 208/300, seasonal_2 Loss: 0.1171 | 0.1346
Epoch 209/300, seasonal_2 Loss: 0.1165 | 0.1346
Epoch 210/300, seasonal_2 Loss: 0.1156 | 0.1346
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.9492073991581168, 'learning_rate': 3.488595000841005e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9000186882202611}
Epoch 1/300, seasonal_3 Loss: 1.0586 | 1.4628
Epoch 2/300, seasonal_3 Loss: 0.6694 | 0.9048
Epoch 3/300, seasonal_3 Loss: 0.5365 | 0.7153
Epoch 4/300, seasonal_3 Loss: 0.4501 | 0.6026
Epoch 5/300, seasonal_3 Loss: 0.3912 | 0.5313
Epoch 6/300, seasonal_3 Loss: 0.3504 | 0.4767
Epoch 7/300, seasonal_3 Loss: 0.3238 | 0.4443
Epoch 8/300, seasonal_3 Loss: 0.3042 | 0.4064
Epoch 9/300, seasonal_3 Loss: 0.2933 | 0.3827
Epoch 10/300, seasonal_3 Loss: 0.2811 | 0.3592
Epoch 11/300, seasonal_3 Loss: 0.2701 | 0.3393
Epoch 12/300, seasonal_3 Loss: 0.2549 | 0.3250
Epoch 13/300, seasonal_3 Loss: 0.2486 | 0.3104
Epoch 14/300, seasonal_3 Loss: 0.2417 | 0.2967
Epoch 15/300, seasonal_3 Loss: 0.2403 | 0.2895
Epoch 16/300, seasonal_3 Loss: 0.2418 | 0.2779
Epoch 17/300, seasonal_3 Loss: 0.2352 | 0.2753
Epoch 18/300, seasonal_3 Loss: 0.2246 | 0.2617
Epoch 19/300, seasonal_3 Loss: 0.2146 | 0.2584
Epoch 20/300, seasonal_3 Loss: 0.2097 | 0.2514
Epoch 21/300, seasonal_3 Loss: 0.2048 | 0.2460
Epoch 22/300, seasonal_3 Loss: 0.1990 | 0.2374
Epoch 23/300, seasonal_3 Loss: 0.1944 | 0.2358
Epoch 24/300, seasonal_3 Loss: 0.1898 | 0.2292
Epoch 25/300, seasonal_3 Loss: 0.1896 | 0.2252
Epoch 26/300, seasonal_3 Loss: 0.1856 | 0.2214
Epoch 27/300, seasonal_3 Loss: 0.1824 | 0.2184
Epoch 28/300, seasonal_3 Loss: 0.1819 | 0.2132
Epoch 29/300, seasonal_3 Loss: 0.1832 | 0.2119
Epoch 30/300, seasonal_3 Loss: 0.1861 | 0.2084
Epoch 31/300, seasonal_3 Loss: 0.1850 | 0.2052
Epoch 32/300, seasonal_3 Loss: 0.1798 | 0.2030
Epoch 33/300, seasonal_3 Loss: 0.1766 | 0.2001
Epoch 34/300, seasonal_3 Loss: 0.1745 | 0.1981
Epoch 35/300, seasonal_3 Loss: 0.1738 | 0.1949
Epoch 36/300, seasonal_3 Loss: 0.1719 | 0.1930
Epoch 37/300, seasonal_3 Loss: 0.1689 | 0.1902
Epoch 38/300, seasonal_3 Loss: 0.1691 | 0.1896
Epoch 39/300, seasonal_3 Loss: 0.1697 | 0.1859
Epoch 40/300, seasonal_3 Loss: 0.1696 | 0.1865
Epoch 41/300, seasonal_3 Loss: 0.1682 | 0.1826
Epoch 42/300, seasonal_3 Loss: 0.1654 | 0.1831
Epoch 43/300, seasonal_3 Loss: 0.1625 | 0.1793
Epoch 44/300, seasonal_3 Loss: 0.1602 | 0.1792
Epoch 45/300, seasonal_3 Loss: 0.1584 | 0.1754
Epoch 46/300, seasonal_3 Loss: 0.1575 | 0.1755
Epoch 47/300, seasonal_3 Loss: 0.1560 | 0.1737
Epoch 48/300, seasonal_3 Loss: 0.1544 | 0.1731
Epoch 49/300, seasonal_3 Loss: 0.1539 | 0.1718
Epoch 50/300, seasonal_3 Loss: 0.1533 | 0.1696
Epoch 51/300, seasonal_3 Loss: 0.1523 | 0.1678
Epoch 52/300, seasonal_3 Loss: 0.1511 | 0.1670
Epoch 53/300, seasonal_3 Loss: 0.1503 | 0.1658
Epoch 54/300, seasonal_3 Loss: 0.1496 | 0.1655
Epoch 55/300, seasonal_3 Loss: 0.1488 | 0.1637
Epoch 56/300, seasonal_3 Loss: 0.1484 | 0.1626
Epoch 57/300, seasonal_3 Loss: 0.1479 | 0.1608
Epoch 58/300, seasonal_3 Loss: 0.1476 | 0.1605
Epoch 59/300, seasonal_3 Loss: 0.1464 | 0.1599
Epoch 60/300, seasonal_3 Loss: 0.1460 | 0.1591
Epoch 61/300, seasonal_3 Loss: 0.1457 | 0.1581
Epoch 62/300, seasonal_3 Loss: 0.1452 | 0.1568
Epoch 63/300, seasonal_3 Loss: 0.1446 | 0.1556
Epoch 64/300, seasonal_3 Loss: 0.1444 | 0.1553
Epoch 65/300, seasonal_3 Loss: 0.1428 | 0.1535
Epoch 66/300, seasonal_3 Loss: 0.1433 | 0.1536
Epoch 67/300, seasonal_3 Loss: 0.1426 | 0.1520
Epoch 68/300, seasonal_3 Loss: 0.1423 | 0.1520
Epoch 69/300, seasonal_3 Loss: 0.1421 | 0.1514
Epoch 70/300, seasonal_3 Loss: 0.1416 | 0.1507
Epoch 71/300, seasonal_3 Loss: 0.1414 | 0.1494
Epoch 72/300, seasonal_3 Loss: 0.1409 | 0.1506
Epoch 73/300, seasonal_3 Loss: 0.1409 | 0.1481
Epoch 74/300, seasonal_3 Loss: 0.1400 | 0.1488
Epoch 75/300, seasonal_3 Loss: 0.1392 | 0.1472
Epoch 76/300, seasonal_3 Loss: 0.1389 | 0.1474
Epoch 77/300, seasonal_3 Loss: 0.1384 | 0.1460
Epoch 78/300, seasonal_3 Loss: 0.1378 | 0.1459
Epoch 79/300, seasonal_3 Loss: 0.1374 | 0.1449
Epoch 80/300, seasonal_3 Loss: 0.1370 | 0.1456
Epoch 81/300, seasonal_3 Loss: 0.1369 | 0.1439
Epoch 82/300, seasonal_3 Loss: 0.1368 | 0.1437
Epoch 83/300, seasonal_3 Loss: 0.1364 | 0.1431
Epoch 84/300, seasonal_3 Loss: 0.1360 | 0.1436
Epoch 85/300, seasonal_3 Loss: 0.1355 | 0.1424
Epoch 86/300, seasonal_3 Loss: 0.1347 | 0.1428
Epoch 87/300, seasonal_3 Loss: 0.1347 | 0.1410
Epoch 88/300, seasonal_3 Loss: 0.1342 | 0.1414
Epoch 89/300, seasonal_3 Loss: 0.1344 | 0.1402
Epoch 90/300, seasonal_3 Loss: 0.1337 | 0.1398
Epoch 91/300, seasonal_3 Loss: 0.1338 | 0.1397
Epoch 92/300, seasonal_3 Loss: 0.1334 | 0.1399
Epoch 93/300, seasonal_3 Loss: 0.1326 | 0.1391
Epoch 94/300, seasonal_3 Loss: 0.1331 | 0.1391
Epoch 95/300, seasonal_3 Loss: 0.1322 | 0.1379
Epoch 96/300, seasonal_3 Loss: 0.1319 | 0.1378
Epoch 97/300, seasonal_3 Loss: 0.1320 | 0.1375
Epoch 98/300, seasonal_3 Loss: 0.1316 | 0.1369
Epoch 99/300, seasonal_3 Loss: 0.1309 | 0.1371
Epoch 100/300, seasonal_3 Loss: 0.1313 | 0.1365
Epoch 101/300, seasonal_3 Loss: 0.1309 | 0.1360
Epoch 102/300, seasonal_3 Loss: 0.1306 | 0.1356
Epoch 103/300, seasonal_3 Loss: 0.1299 | 0.1355
Epoch 104/300, seasonal_3 Loss: 0.1306 | 0.1357
Epoch 105/300, seasonal_3 Loss: 0.1299 | 0.1350
Epoch 106/300, seasonal_3 Loss: 0.1302 | 0.1346
Epoch 107/300, seasonal_3 Loss: 0.1299 | 0.1343
Epoch 108/300, seasonal_3 Loss: 0.1294 | 0.1344
Epoch 109/300, seasonal_3 Loss: 0.1291 | 0.1346
Epoch 110/300, seasonal_3 Loss: 0.1288 | 0.1335
Epoch 111/300, seasonal_3 Loss: 0.1286 | 0.1335
Epoch 112/300, seasonal_3 Loss: 0.1284 | 0.1334
Epoch 113/300, seasonal_3 Loss: 0.1285 | 0.1330
Epoch 114/300, seasonal_3 Loss: 0.1279 | 0.1329
Epoch 115/300, seasonal_3 Loss: 0.1283 | 0.1327
Epoch 116/300, seasonal_3 Loss: 0.1273 | 0.1321
Epoch 117/300, seasonal_3 Loss: 0.1276 | 0.1316
Epoch 118/300, seasonal_3 Loss: 0.1275 | 0.1316
Epoch 119/300, seasonal_3 Loss: 0.1276 | 0.1315
Epoch 120/300, seasonal_3 Loss: 0.1266 | 0.1317
Epoch 121/300, seasonal_3 Loss: 0.1265 | 0.1312
Epoch 122/300, seasonal_3 Loss: 0.1268 | 0.1311
Epoch 123/300, seasonal_3 Loss: 0.1266 | 0.1306
Epoch 124/300, seasonal_3 Loss: 0.1266 | 0.1305
Epoch 125/300, seasonal_3 Loss: 0.1270 | 0.1303
Epoch 126/300, seasonal_3 Loss: 0.1260 | 0.1302
Epoch 127/300, seasonal_3 Loss: 0.1266 | 0.1301
Epoch 128/300, seasonal_3 Loss: 0.1262 | 0.1292
Epoch 129/300, seasonal_3 Loss: 0.1256 | 0.1292
Epoch 130/300, seasonal_3 Loss: 0.1258 | 0.1293
Epoch 131/300, seasonal_3 Loss: 0.1252 | 0.1293
Epoch 132/300, seasonal_3 Loss: 0.1250 | 0.1288
Epoch 133/300, seasonal_3 Loss: 0.1248 | 0.1290
Epoch 134/300, seasonal_3 Loss: 0.1250 | 0.1287
Epoch 135/300, seasonal_3 Loss: 0.1250 | 0.1283
Epoch 136/300, seasonal_3 Loss: 0.1249 | 0.1282
Epoch 137/300, seasonal_3 Loss: 0.1245 | 0.1282
Epoch 138/300, seasonal_3 Loss: 0.1247 | 0.1281
Epoch 139/300, seasonal_3 Loss: 0.1244 | 0.1278
Epoch 140/300, seasonal_3 Loss: 0.1241 | 0.1277
Epoch 141/300, seasonal_3 Loss: 0.1242 | 0.1275
Epoch 142/300, seasonal_3 Loss: 0.1244 | 0.1276
Epoch 143/300, seasonal_3 Loss: 0.1245 | 0.1274
Epoch 144/300, seasonal_3 Loss: 0.1236 | 0.1276
Epoch 145/300, seasonal_3 Loss: 0.1241 | 0.1273
Epoch 146/300, seasonal_3 Loss: 0.1236 | 0.1272
Epoch 147/300, seasonal_3 Loss: 0.1238 | 0.1268
Epoch 148/300, seasonal_3 Loss: 0.1237 | 0.1269
Epoch 149/300, seasonal_3 Loss: 0.1232 | 0.1270
Epoch 150/300, seasonal_3 Loss: 0.1234 | 0.1266
Epoch 151/300, seasonal_3 Loss: 0.1233 | 0.1266
Epoch 152/300, seasonal_3 Loss: 0.1237 | 0.1268
Epoch 153/300, seasonal_3 Loss: 0.1236 | 0.1265
Epoch 154/300, seasonal_3 Loss: 0.1231 | 0.1262
Epoch 155/300, seasonal_3 Loss: 0.1233 | 0.1260
Epoch 156/300, seasonal_3 Loss: 0.1227 | 0.1260
Epoch 157/300, seasonal_3 Loss: 0.1223 | 0.1263
Epoch 158/300, seasonal_3 Loss: 0.1231 | 0.1259
Epoch 159/300, seasonal_3 Loss: 0.1223 | 0.1256
Epoch 160/300, seasonal_3 Loss: 0.1229 | 0.1253
Epoch 161/300, seasonal_3 Loss: 0.1226 | 0.1257
Epoch 162/300, seasonal_3 Loss: 0.1222 | 0.1254
Epoch 163/300, seasonal_3 Loss: 0.1226 | 0.1249
Epoch 164/300, seasonal_3 Loss: 0.1227 | 0.1247
Epoch 165/300, seasonal_3 Loss: 0.1221 | 0.1249
Epoch 166/300, seasonal_3 Loss: 0.1226 | 0.1249
Epoch 167/300, seasonal_3 Loss: 0.1222 | 0.1249
Epoch 168/300, seasonal_3 Loss: 0.1224 | 0.1246
Epoch 169/300, seasonal_3 Loss: 0.1223 | 0.1245
Epoch 170/300, seasonal_3 Loss: 0.1216 | 0.1244
Epoch 171/300, seasonal_3 Loss: 0.1225 | 0.1243
Epoch 172/300, seasonal_3 Loss: 0.1218 | 0.1241
Epoch 173/300, seasonal_3 Loss: 0.1212 | 0.1241
Epoch 174/300, seasonal_3 Loss: 0.1224 | 0.1244
Epoch 175/300, seasonal_3 Loss: 0.1217 | 0.1241
Epoch 176/300, seasonal_3 Loss: 0.1214 | 0.1240
Epoch 177/300, seasonal_3 Loss: 0.1215 | 0.1238
Epoch 178/300, seasonal_3 Loss: 0.1211 | 0.1234
Epoch 179/300, seasonal_3 Loss: 0.1209 | 0.1234
Epoch 180/300, seasonal_3 Loss: 0.1214 | 0.1234
Epoch 181/300, seasonal_3 Loss: 0.1215 | 0.1234
Epoch 182/300, seasonal_3 Loss: 0.1208 | 0.1235
Epoch 183/300, seasonal_3 Loss: 0.1208 | 0.1234
Epoch 184/300, seasonal_3 Loss: 0.1210 | 0.1235
Epoch 185/300, seasonal_3 Loss: 0.1212 | 0.1234
Epoch 186/300, seasonal_3 Loss: 0.1206 | 0.1235
Epoch 187/300, seasonal_3 Loss: 0.1208 | 0.1234
Epoch 188/300, seasonal_3 Loss: 0.1208 | 0.1231
Epoch 189/300, seasonal_3 Loss: 0.1206 | 0.1229
Epoch 190/300, seasonal_3 Loss: 0.1207 | 0.1229
Epoch 191/300, seasonal_3 Loss: 0.1210 | 0.1229
Epoch 192/300, seasonal_3 Loss: 0.1210 | 0.1229
Epoch 193/300, seasonal_3 Loss: 0.1206 | 0.1229
Epoch 194/300, seasonal_3 Loss: 0.1209 | 0.1228
Epoch 195/300, seasonal_3 Loss: 0.1205 | 0.1228
Epoch 196/300, seasonal_3 Loss: 0.1206 | 0.1230
Epoch 197/300, seasonal_3 Loss: 0.1203 | 0.1231
Epoch 198/300, seasonal_3 Loss: 0.1208 | 0.1230
Epoch 199/300, seasonal_3 Loss: 0.1204 | 0.1228
Epoch 200/300, seasonal_3 Loss: 0.1209 | 0.1227
Epoch 201/300, seasonal_3 Loss: 0.1205 | 0.1227
Epoch 202/300, seasonal_3 Loss: 0.1202 | 0.1228
Epoch 203/300, seasonal_3 Loss: 0.1204 | 0.1228
Epoch 204/300, seasonal_3 Loss: 0.1206 | 0.1228
Epoch 205/300, seasonal_3 Loss: 0.1205 | 0.1225
Epoch 206/300, seasonal_3 Loss: 0.1205 | 0.1225
Epoch 207/300, seasonal_3 Loss: 0.1204 | 0.1225
Epoch 208/300, seasonal_3 Loss: 0.1206 | 0.1224
Epoch 209/300, seasonal_3 Loss: 0.1202 | 0.1224
Epoch 210/300, seasonal_3 Loss: 0.1198 | 0.1223
Epoch 211/300, seasonal_3 Loss: 0.1205 | 0.1222
Epoch 212/300, seasonal_3 Loss: 0.1199 | 0.1222
Epoch 213/300, seasonal_3 Loss: 0.1203 | 0.1222
Epoch 214/300, seasonal_3 Loss: 0.1204 | 0.1222
Epoch 215/300, seasonal_3 Loss: 0.1194 | 0.1222
Epoch 216/300, seasonal_3 Loss: 0.1196 | 0.1220
Epoch 217/300, seasonal_3 Loss: 0.1204 | 0.1219
Epoch 218/300, seasonal_3 Loss: 0.1192 | 0.1218
Epoch 219/300, seasonal_3 Loss: 0.1199 | 0.1217
Epoch 220/300, seasonal_3 Loss: 0.1195 | 0.1218
Epoch 221/300, seasonal_3 Loss: 0.1200 | 0.1218
Epoch 222/300, seasonal_3 Loss: 0.1205 | 0.1217
Epoch 223/300, seasonal_3 Loss: 0.1202 | 0.1218
Epoch 224/300, seasonal_3 Loss: 0.1199 | 0.1218
Epoch 225/300, seasonal_3 Loss: 0.1201 | 0.1218
Epoch 226/300, seasonal_3 Loss: 0.1201 | 0.1216
Epoch 227/300, seasonal_3 Loss: 0.1199 | 0.1215
Epoch 228/300, seasonal_3 Loss: 0.1200 | 0.1215
Epoch 229/300, seasonal_3 Loss: 0.1196 | 0.1216
Epoch 230/300, seasonal_3 Loss: 0.1194 | 0.1215
Epoch 231/300, seasonal_3 Loss: 0.1199 | 0.1215
Epoch 232/300, seasonal_3 Loss: 0.1195 | 0.1215
Epoch 233/300, seasonal_3 Loss: 0.1192 | 0.1216
Epoch 234/300, seasonal_3 Loss: 0.1194 | 0.1216
Epoch 235/300, seasonal_3 Loss: 0.1196 | 0.1216
Epoch 236/300, seasonal_3 Loss: 0.1198 | 0.1215
Epoch 237/300, seasonal_3 Loss: 0.1194 | 0.1214
Epoch 238/300, seasonal_3 Loss: 0.1195 | 0.1215
Epoch 239/300, seasonal_3 Loss: 0.1193 | 0.1214
Epoch 240/300, seasonal_3 Loss: 0.1192 | 0.1214
Epoch 241/300, seasonal_3 Loss: 0.1196 | 0.1214
Epoch 242/300, seasonal_3 Loss: 0.1200 | 0.1213
Epoch 243/300, seasonal_3 Loss: 0.1195 | 0.1213
Epoch 244/300, seasonal_3 Loss: 0.1193 | 0.1213
Epoch 245/300, seasonal_3 Loss: 0.1193 | 0.1213
Epoch 246/300, seasonal_3 Loss: 0.1191 | 0.1213
Epoch 247/300, seasonal_3 Loss: 0.1199 | 0.1213
Epoch 248/300, seasonal_3 Loss: 0.1197 | 0.1213
Epoch 249/300, seasonal_3 Loss: 0.1192 | 0.1213
Epoch 250/300, seasonal_3 Loss: 0.1195 | 0.1212
Epoch 251/300, seasonal_3 Loss: 0.1193 | 0.1212
Epoch 252/300, seasonal_3 Loss: 0.1195 | 0.1211
Epoch 253/300, seasonal_3 Loss: 0.1193 | 0.1211
Epoch 254/300, seasonal_3 Loss: 0.1191 | 0.1211
Epoch 255/300, seasonal_3 Loss: 0.1192 | 0.1210
Epoch 256/300, seasonal_3 Loss: 0.1196 | 0.1210
Epoch 257/300, seasonal_3 Loss: 0.1196 | 0.1210
Epoch 258/300, seasonal_3 Loss: 0.1191 | 0.1210
Epoch 259/300, seasonal_3 Loss: 0.1194 | 0.1210
Epoch 260/300, seasonal_3 Loss: 0.1199 | 0.1210
Epoch 261/300, seasonal_3 Loss: 0.1194 | 0.1209
Epoch 262/300, seasonal_3 Loss: 0.1192 | 0.1209
Epoch 263/300, seasonal_3 Loss: 0.1191 | 0.1210
Epoch 264/300, seasonal_3 Loss: 0.1197 | 0.1209
Epoch 265/300, seasonal_3 Loss: 0.1191 | 0.1209
Epoch 266/300, seasonal_3 Loss: 0.1193 | 0.1209
Epoch 267/300, seasonal_3 Loss: 0.1196 | 0.1209
Epoch 268/300, seasonal_3 Loss: 0.1193 | 0.1210
Epoch 269/300, seasonal_3 Loss: 0.1197 | 0.1210
Epoch 270/300, seasonal_3 Loss: 0.1190 | 0.1210
Epoch 271/300, seasonal_3 Loss: 0.1198 | 0.1210
Epoch 272/300, seasonal_3 Loss: 0.1190 | 0.1210
Epoch 273/300, seasonal_3 Loss: 0.1188 | 0.1210
Epoch 274/300, seasonal_3 Loss: 0.1194 | 0.1209
Epoch 275/300, seasonal_3 Loss: 0.1191 | 0.1209
Epoch 276/300, seasonal_3 Loss: 0.1191 | 0.1209
Epoch 277/300, seasonal_3 Loss: 0.1193 | 0.1209
Epoch 278/300, seasonal_3 Loss: 0.1201 | 0.1209
Epoch 279/300, seasonal_3 Loss: 0.1198 | 0.1209
Epoch 280/300, seasonal_3 Loss: 0.1189 | 0.1209
Epoch 281/300, seasonal_3 Loss: 0.1191 | 0.1209
Epoch 282/300, seasonal_3 Loss: 0.1193 | 0.1209
Epoch 283/300, seasonal_3 Loss: 0.1185 | 0.1208
Epoch 284/300, seasonal_3 Loss: 0.1193 | 0.1208
Epoch 285/300, seasonal_3 Loss: 0.1195 | 0.1208
Epoch 286/300, seasonal_3 Loss: 0.1194 | 0.1207
Epoch 287/300, seasonal_3 Loss: 0.1194 | 0.1207
Epoch 288/300, seasonal_3 Loss: 0.1196 | 0.1207
Epoch 289/300, seasonal_3 Loss: 0.1192 | 0.1207
Epoch 290/300, seasonal_3 Loss: 0.1186 | 0.1207
Epoch 291/300, seasonal_3 Loss: 0.1195 | 0.1207
Epoch 292/300, seasonal_3 Loss: 0.1190 | 0.1207
Epoch 293/300, seasonal_3 Loss: 0.1196 | 0.1207
Epoch 294/300, seasonal_3 Loss: 0.1190 | 0.1207
Epoch 295/300, seasonal_3 Loss: 0.1193 | 0.1207
Epoch 296/300, seasonal_3 Loss: 0.1182 | 0.1207
Epoch 297/300, seasonal_3 Loss: 0.1189 | 0.1207
Epoch 298/300, seasonal_3 Loss: 0.1189 | 0.1207
Epoch 299/300, seasonal_3 Loss: 0.1190 | 0.1207
Epoch 300/300, seasonal_3 Loss: 0.1187 | 0.1208
Training resid component with params: {'observation_period_num': 196, 'train_rates': 0.974699097115745, 'learning_rate': 0.0004243432121126998, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9467229245377647}
Epoch 1/300, resid Loss: 1.0617 | 1.0303
Epoch 2/300, resid Loss: 0.9909 | 0.8119
Epoch 3/300, resid Loss: 0.7217 | 0.6291
Epoch 4/300, resid Loss: 0.6097 | 0.6734
Epoch 5/300, resid Loss: 0.5960 | 0.6300
Epoch 6/300, resid Loss: 0.5362 | 0.5260
Epoch 7/300, resid Loss: 0.5176 | 0.4769
Epoch 8/300, resid Loss: 0.4966 | 0.6252
Epoch 9/300, resid Loss: 0.4713 | 0.4592
Epoch 10/300, resid Loss: 0.4698 | 0.4345
Epoch 11/300, resid Loss: 0.4432 | 0.4307
Epoch 12/300, resid Loss: 0.3953 | 0.3741
Epoch 13/300, resid Loss: 0.3863 | 0.3413
Epoch 14/300, resid Loss: 0.3345 | 0.3824
Epoch 15/300, resid Loss: 0.3139 | 0.3136
Epoch 16/300, resid Loss: 0.3204 | 0.3067
Epoch 17/300, resid Loss: 0.2884 | 0.2865
Epoch 18/300, resid Loss: 0.2793 | 0.2681
Epoch 19/300, resid Loss: 0.2442 | 0.2669
Epoch 20/300, resid Loss: 0.2394 | 0.2563
Epoch 21/300, resid Loss: 0.2605 | 0.2306
Epoch 22/300, resid Loss: 0.2441 | 0.2469
Epoch 23/300, resid Loss: 0.2563 | 0.2198
Epoch 24/300, resid Loss: 0.2421 | 0.3395
Epoch 25/300, resid Loss: 0.2394 | 0.2178
Epoch 26/300, resid Loss: 0.2177 | 0.2352
Epoch 27/300, resid Loss: 0.2061 | 0.2337
Epoch 28/300, resid Loss: 0.2023 | 0.1976
Epoch 29/300, resid Loss: 0.1949 | 0.2137
Epoch 30/300, resid Loss: 0.1958 | 0.1976
Epoch 31/300, resid Loss: 0.1950 | 0.1933
Epoch 32/300, resid Loss: 0.1963 | 0.1881
Epoch 33/300, resid Loss: 0.1982 | 0.2039
Epoch 34/300, resid Loss: 0.2007 | 0.1811
Epoch 35/300, resid Loss: 0.2001 | 0.1946
Epoch 36/300, resid Loss: 0.1892 | 0.1769
Epoch 37/300, resid Loss: 0.1845 | 0.1735
Epoch 38/300, resid Loss: 0.1754 | 0.1778
Epoch 39/300, resid Loss: 0.1697 | 0.1681
Epoch 40/300, resid Loss: 0.1660 | 0.1732
Epoch 41/300, resid Loss: 0.1650 | 0.1664
Epoch 42/300, resid Loss: 0.1632 | 0.1696
Epoch 43/300, resid Loss: 0.1612 | 0.1612
Epoch 44/300, resid Loss: 0.1587 | 0.1628
Epoch 45/300, resid Loss: 0.1561 | 0.1566
Epoch 46/300, resid Loss: 0.1529 | 0.1558
Epoch 47/300, resid Loss: 0.1517 | 0.1542
Epoch 48/300, resid Loss: 0.1499 | 0.1517
Epoch 49/300, resid Loss: 0.1485 | 0.1518
Epoch 50/300, resid Loss: 0.1484 | 0.1505
Epoch 51/300, resid Loss: 0.1468 | 0.1481
Epoch 52/300, resid Loss: 0.1457 | 0.1477
Epoch 53/300, resid Loss: 0.1455 | 0.1460
Epoch 54/300, resid Loss: 0.1450 | 0.1463
Epoch 55/300, resid Loss: 0.1450 | 0.1442
Epoch 56/300, resid Loss: 0.1454 | 0.1459
Epoch 57/300, resid Loss: 0.1462 | 0.1418
Epoch 58/300, resid Loss: 0.1463 | 0.1444
Epoch 59/300, resid Loss: 0.1454 | 0.1398
Epoch 60/300, resid Loss: 0.1445 | 0.1417
Epoch 61/300, resid Loss: 0.1416 | 0.1384
Epoch 62/300, resid Loss: 0.1393 | 0.1383
Epoch 63/300, resid Loss: 0.1384 | 0.1366
Epoch 64/300, resid Loss: 0.1376 | 0.1364
Epoch 65/300, resid Loss: 0.1364 | 0.1345
Epoch 66/300, resid Loss: 0.1365 | 0.1350
Epoch 67/300, resid Loss: 0.1354 | 0.1333
Epoch 68/300, resid Loss: 0.1351 | 0.1330
Epoch 69/300, resid Loss: 0.1346 | 0.1317
Epoch 70/300, resid Loss: 0.1339 | 0.1323
Epoch 71/300, resid Loss: 0.1325 | 0.1310
Epoch 72/300, resid Loss: 0.1333 | 0.1316
Epoch 73/300, resid Loss: 0.1319 | 0.1288
Epoch 74/300, resid Loss: 0.1310 | 0.1304
Epoch 75/300, resid Loss: 0.1321 | 0.1284
Epoch 76/300, resid Loss: 0.1310 | 0.1298
Epoch 77/300, resid Loss: 0.1305 | 0.1265
Epoch 78/300, resid Loss: 0.1297 | 0.1284
Epoch 79/300, resid Loss: 0.1286 | 0.1260
Epoch 80/300, resid Loss: 0.1280 | 0.1262
Epoch 81/300, resid Loss: 0.1280 | 0.1248
Epoch 82/300, resid Loss: 0.1267 | 0.1254
Epoch 83/300, resid Loss: 0.1265 | 0.1240
Epoch 84/300, resid Loss: 0.1260 | 0.1241
Epoch 85/300, resid Loss: 0.1258 | 0.1225
Epoch 86/300, resid Loss: 0.1249 | 0.1231
Epoch 87/300, resid Loss: 0.1249 | 0.1215
Epoch 88/300, resid Loss: 0.1244 | 0.1224
Epoch 89/300, resid Loss: 0.1239 | 0.1211
Epoch 90/300, resid Loss: 0.1236 | 0.1214
Epoch 91/300, resid Loss: 0.1237 | 0.1203
Epoch 92/300, resid Loss: 0.1235 | 0.1206
Epoch 93/300, resid Loss: 0.1228 | 0.1197
Epoch 94/300, resid Loss: 0.1223 | 0.1202
Epoch 95/300, resid Loss: 0.1222 | 0.1193
Epoch 96/300, resid Loss: 0.1221 | 0.1195
Epoch 97/300, resid Loss: 0.1215 | 0.1185
Epoch 98/300, resid Loss: 0.1214 | 0.1187
Epoch 99/300, resid Loss: 0.1210 | 0.1182
Epoch 100/300, resid Loss: 0.1206 | 0.1185
Epoch 101/300, resid Loss: 0.1200 | 0.1176
Epoch 102/300, resid Loss: 0.1193 | 0.1175
Epoch 103/300, resid Loss: 0.1192 | 0.1179
Epoch 104/300, resid Loss: 0.1190 | 0.1171
Epoch 105/300, resid Loss: 0.1195 | 0.1172
Epoch 106/300, resid Loss: 0.1190 | 0.1167
Epoch 107/300, resid Loss: 0.1190 | 0.1162
Epoch 108/300, resid Loss: 0.1184 | 0.1169
Epoch 109/300, resid Loss: 0.1188 | 0.1165
Epoch 110/300, resid Loss: 0.1183 | 0.1153
Epoch 111/300, resid Loss: 0.1179 | 0.1155
Epoch 112/300, resid Loss: 0.1174 | 0.1160
Epoch 113/300, resid Loss: 0.1178 | 0.1160
Epoch 114/300, resid Loss: 0.1178 | 0.1154
Epoch 115/300, resid Loss: 0.1173 | 0.1148
Epoch 116/300, resid Loss: 0.1166 | 0.1148
Epoch 117/300, resid Loss: 0.1166 | 0.1148
Epoch 118/300, resid Loss: 0.1162 | 0.1147
Epoch 119/300, resid Loss: 0.1160 | 0.1142
Epoch 120/300, resid Loss: 0.1167 | 0.1147
Epoch 121/300, resid Loss: 0.1153 | 0.1141
Epoch 122/300, resid Loss: 0.1153 | 0.1133
Epoch 123/300, resid Loss: 0.1157 | 0.1138
Epoch 124/300, resid Loss: 0.1148 | 0.1137
Epoch 125/300, resid Loss: 0.1151 | 0.1136
Epoch 126/300, resid Loss: 0.1154 | 0.1137
Epoch 127/300, resid Loss: 0.1149 | 0.1130
Epoch 128/300, resid Loss: 0.1146 | 0.1139
Epoch 129/300, resid Loss: 0.1148 | 0.1132
Epoch 130/300, resid Loss: 0.1140 | 0.1127
Epoch 131/300, resid Loss: 0.1143 | 0.1129
Epoch 132/300, resid Loss: 0.1135 | 0.1125
Epoch 133/300, resid Loss: 0.1141 | 0.1124
Epoch 134/300, resid Loss: 0.1135 | 0.1125
Epoch 135/300, resid Loss: 0.1135 | 0.1123
Epoch 136/300, resid Loss: 0.1132 | 0.1124
Epoch 137/300, resid Loss: 0.1136 | 0.1118
Epoch 138/300, resid Loss: 0.1128 | 0.1120
Epoch 139/300, resid Loss: 0.1127 | 0.1119
Epoch 140/300, resid Loss: 0.1126 | 0.1116
Epoch 141/300, resid Loss: 0.1125 | 0.1120
Epoch 142/300, resid Loss: 0.1117 | 0.1115
Epoch 143/300, resid Loss: 0.1128 | 0.1110
Epoch 144/300, resid Loss: 0.1122 | 0.1108
Epoch 145/300, resid Loss: 0.1116 | 0.1113
Epoch 146/300, resid Loss: 0.1122 | 0.1112
Epoch 147/300, resid Loss: 0.1115 | 0.1104
Epoch 148/300, resid Loss: 0.1110 | 0.1108
Epoch 149/300, resid Loss: 0.1117 | 0.1115
Epoch 150/300, resid Loss: 0.1120 | 0.1115
Epoch 151/300, resid Loss: 0.1109 | 0.1115
Epoch 152/300, resid Loss: 0.1116 | 0.1102
Epoch 153/300, resid Loss: 0.1112 | 0.1115
Epoch 154/300, resid Loss: 0.1112 | 0.1111
Epoch 155/300, resid Loss: 0.1105 | 0.1107
Epoch 156/300, resid Loss: 0.1111 | 0.1104
Epoch 157/300, resid Loss: 0.1109 | 0.1101
Epoch 158/300, resid Loss: 0.1108 | 0.1105
Epoch 159/300, resid Loss: 0.1106 | 0.1104
Epoch 160/300, resid Loss: 0.1103 | 0.1098
Epoch 161/300, resid Loss: 0.1106 | 0.1099
Epoch 162/300, resid Loss: 0.1105 | 0.1102
Epoch 163/300, resid Loss: 0.1101 | 0.1098
Epoch 164/300, resid Loss: 0.1104 | 0.1098
Epoch 165/300, resid Loss: 0.1100 | 0.1095
Epoch 166/300, resid Loss: 0.1096 | 0.1095
Epoch 167/300, resid Loss: 0.1099 | 0.1099
Epoch 168/300, resid Loss: 0.1104 | 0.1098
Epoch 169/300, resid Loss: 0.1094 | 0.1091
Epoch 170/300, resid Loss: 0.1089 | 0.1085
Epoch 171/300, resid Loss: 0.1095 | 0.1091
Epoch 172/300, resid Loss: 0.1097 | 0.1102
Epoch 173/300, resid Loss: 0.1089 | 0.1093
Epoch 174/300, resid Loss: 0.1095 | 0.1089
Epoch 175/300, resid Loss: 0.1097 | 0.1097
Epoch 176/300, resid Loss: 0.1090 | 0.1088
Epoch 177/300, resid Loss: 0.1099 | 0.1088
Epoch 178/300, resid Loss: 0.1096 | 0.1088
Epoch 179/300, resid Loss: 0.1086 | 0.1084
Epoch 180/300, resid Loss: 0.1090 | 0.1087
Epoch 181/300, resid Loss: 0.1090 | 0.1091
Epoch 182/300, resid Loss: 0.1087 | 0.1091
Epoch 183/300, resid Loss: 0.1091 | 0.1083
Epoch 184/300, resid Loss: 0.1085 | 0.1081
Epoch 185/300, resid Loss: 0.1083 | 0.1087
Epoch 186/300, resid Loss: 0.1081 | 0.1081
Epoch 187/300, resid Loss: 0.1086 | 0.1087
Epoch 188/300, resid Loss: 0.1077 | 0.1087
Epoch 189/300, resid Loss: 0.1088 | 0.1080
Epoch 190/300, resid Loss: 0.1079 | 0.1079
Epoch 191/300, resid Loss: 0.1083 | 0.1081
Epoch 192/300, resid Loss: 0.1083 | 0.1083
Epoch 193/300, resid Loss: 0.1083 | 0.1084
Epoch 194/300, resid Loss: 0.1081 | 0.1083
Epoch 195/300, resid Loss: 0.1082 | 0.1080
Epoch 196/300, resid Loss: 0.1080 | 0.1081
Epoch 197/300, resid Loss: 0.1080 | 0.1080
Epoch 198/300, resid Loss: 0.1078 | 0.1078
Epoch 199/300, resid Loss: 0.1080 | 0.1083
Epoch 200/300, resid Loss: 0.1077 | 0.1081
Epoch 201/300, resid Loss: 0.1083 | 0.1079
Epoch 202/300, resid Loss: 0.1074 | 0.1083
Epoch 203/300, resid Loss: 0.1072 | 0.1081
Epoch 204/300, resid Loss: 0.1069 | 0.1072
Epoch 205/300, resid Loss: 0.1074 | 0.1077
Epoch 206/300, resid Loss: 0.1077 | 0.1078
Epoch 207/300, resid Loss: 0.1074 | 0.1080
Epoch 208/300, resid Loss: 0.1071 | 0.1082
Epoch 209/300, resid Loss: 0.1070 | 0.1079
Epoch 210/300, resid Loss: 0.1078 | 0.1075
Epoch 211/300, resid Loss: 0.1073 | 0.1077
Epoch 212/300, resid Loss: 0.1072 | 0.1081
Epoch 213/300, resid Loss: 0.1073 | 0.1081
Epoch 214/300, resid Loss: 0.1073 | 0.1077
Epoch 215/300, resid Loss: 0.1068 | 0.1080
Epoch 216/300, resid Loss: 0.1068 | 0.1075
Epoch 217/300, resid Loss: 0.1069 | 0.1074
Epoch 218/300, resid Loss: 0.1072 | 0.1076
Epoch 219/300, resid Loss: 0.1070 | 0.1079
Epoch 220/300, resid Loss: 0.1067 | 0.1079
Epoch 221/300, resid Loss: 0.1075 | 0.1077
Epoch 222/300, resid Loss: 0.1069 | 0.1077
Epoch 223/300, resid Loss: 0.1073 | 0.1074
Epoch 224/300, resid Loss: 0.1076 | 0.1074
Epoch 225/300, resid Loss: 0.1065 | 0.1076
Epoch 226/300, resid Loss: 0.1068 | 0.1077
Epoch 227/300, resid Loss: 0.1069 | 0.1073
Epoch 228/300, resid Loss: 0.1066 | 0.1073
Epoch 229/300, resid Loss: 0.1069 | 0.1076
Epoch 230/300, resid Loss: 0.1070 | 0.1077
Epoch 231/300, resid Loss: 0.1068 | 0.1076
Epoch 232/300, resid Loss: 0.1066 | 0.1073
Epoch 233/300, resid Loss: 0.1070 | 0.1073
Epoch 234/300, resid Loss: 0.1064 | 0.1073
Epoch 235/300, resid Loss: 0.1065 | 0.1073
Epoch 236/300, resid Loss: 0.1060 | 0.1074
Epoch 237/300, resid Loss: 0.1059 | 0.1071
Epoch 238/300, resid Loss: 0.1059 | 0.1071
Epoch 239/300, resid Loss: 0.1061 | 0.1073
Epoch 240/300, resid Loss: 0.1059 | 0.1072
Epoch 241/300, resid Loss: 0.1069 | 0.1070
Epoch 242/300, resid Loss: 0.1070 | 0.1071
Epoch 243/300, resid Loss: 0.1059 | 0.1074
Epoch 244/300, resid Loss: 0.1065 | 0.1076
Epoch 245/300, resid Loss: 0.1061 | 0.1076
Epoch 246/300, resid Loss: 0.1057 | 0.1074
Epoch 247/300, resid Loss: 0.1058 | 0.1074
Epoch 248/300, resid Loss: 0.1058 | 0.1074
Epoch 249/300, resid Loss: 0.1059 | 0.1071
Epoch 250/300, resid Loss: 0.1058 | 0.1071
Epoch 251/300, resid Loss: 0.1056 | 0.1067
Epoch 252/300, resid Loss: 0.1062 | 0.1067
Epoch 253/300, resid Loss: 0.1059 | 0.1067
Epoch 254/300, resid Loss: 0.1066 | 0.1069
Epoch 255/300, resid Loss: 0.1071 | 0.1072
Epoch 256/300, resid Loss: 0.1068 | 0.1075
Epoch 257/300, resid Loss: 0.1060 | 0.1073
Epoch 258/300, resid Loss: 0.1060 | 0.1071
Epoch 259/300, resid Loss: 0.1057 | 0.1070
Epoch 260/300, resid Loss: 0.1066 | 0.1071
Epoch 261/300, resid Loss: 0.1051 | 0.1070
Epoch 262/300, resid Loss: 0.1056 | 0.1071
Epoch 263/300, resid Loss: 0.1056 | 0.1071
Epoch 264/300, resid Loss: 0.1061 | 0.1072
Epoch 265/300, resid Loss: 0.1060 | 0.1073
Epoch 266/300, resid Loss: 0.1059 | 0.1074
Epoch 267/300, resid Loss: 0.1054 | 0.1075
Epoch 268/300, resid Loss: 0.1056 | 0.1074
Epoch 269/300, resid Loss: 0.1055 | 0.1072
Epoch 270/300, resid Loss: 0.1058 | 0.1072
Epoch 271/300, resid Loss: 0.1057 | 0.1074
Epoch 272/300, resid Loss: 0.1063 | 0.1073
Epoch 273/300, resid Loss: 0.1056 | 0.1073
Epoch 274/300, resid Loss: 0.1057 | 0.1074
Epoch 275/300, resid Loss: 0.1059 | 0.1074
Epoch 276/300, resid Loss: 0.1062 | 0.1072
Epoch 277/300, resid Loss: 0.1059 | 0.1072
Epoch 278/300, resid Loss: 0.1050 | 0.1071
Epoch 279/300, resid Loss: 0.1057 | 0.1072
Epoch 280/300, resid Loss: 0.1054 | 0.1073
Epoch 281/300, resid Loss: 0.1060 | 0.1073
Epoch 282/300, resid Loss: 0.1054 | 0.1071
Epoch 283/300, resid Loss: 0.1054 | 0.1071
Epoch 284/300, resid Loss: 0.1050 | 0.1071
Epoch 285/300, resid Loss: 0.1058 | 0.1072
Epoch 286/300, resid Loss: 0.1058 | 0.1071
Epoch 287/300, resid Loss: 0.1062 | 0.1071
Epoch 288/300, resid Loss: 0.1050 | 0.1071
Epoch 289/300, resid Loss: 0.1062 | 0.1072
Epoch 290/300, resid Loss: 0.1055 | 0.1072
Epoch 291/300, resid Loss: 0.1054 | 0.1073
Epoch 292/300, resid Loss: 0.1053 | 0.1073
Epoch 293/300, resid Loss: 0.1056 | 0.1072
Epoch 294/300, resid Loss: 0.1055 | 0.1072
Epoch 295/300, resid Loss: 0.1051 | 0.1071
Epoch 296/300, resid Loss: 0.1055 | 0.1070
Epoch 297/300, resid Loss: 0.1048 | 0.1071
Epoch 298/300, resid Loss: 0.1065 | 0.1071
Epoch 299/300, resid Loss: 0.1056 | 0.1072
Epoch 300/300, resid Loss: 0.1057 | 0.1072
Runtime (seconds): 2912.0987837314606
0.0001409470186357949
[157.97867]
[-3.5673676]
[2.4845433]
[12.957321]
[-0.5198833]
[14.63296]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 570.9114348983858
RMSE: 23.893753051757812
MAE: 23.893753051757812
R-squared: nan
[183.96625]
File amzn_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
