[32m[I 2025-02-08 02:07:26,597][0m A new study created in memory with name: no-name-db6125bf-e0ff-43ea-bfeb-37e0b2e9adbe[0m
[32m[I 2025-02-08 02:07:54,554][0m Trial 0 finished with value: 0.31263216397282445 and parameters: {'observation_period_num': 238, 'train_rates': 0.656493763210834, 'learning_rate': 0.0001979576451279211, 'batch_size': 168, 'step_size': 6, 'gamma': 0.7773558173573655}. Best is trial 0 with value: 0.31263216397282445.[0m
[32m[I 2025-02-08 02:08:39,053][0m Trial 1 finished with value: 1.8894429556328616 and parameters: {'observation_period_num': 184, 'train_rates': 0.6054477979187006, 'learning_rate': 1.0204153092543087e-06, 'batch_size': 100, 'step_size': 5, 'gamma': 0.8987177937834939}. Best is trial 0 with value: 0.31263216397282445.[0m
[32m[I 2025-02-08 02:09:10,631][0m Trial 2 finished with value: 0.2317675594240427 and parameters: {'observation_period_num': 223, 'train_rates': 0.8965447570231306, 'learning_rate': 0.0006336252652044801, 'batch_size': 180, 'step_size': 5, 'gamma': 0.9651813206875256}. Best is trial 2 with value: 0.2317675594240427.[0m
[32m[I 2025-02-08 02:09:38,537][0m Trial 3 finished with value: 0.19027086666652135 and parameters: {'observation_period_num': 203, 'train_rates': 0.9100145858692086, 'learning_rate': 0.0009532996477478409, 'batch_size': 230, 'step_size': 3, 'gamma': 0.8509142756734349}. Best is trial 3 with value: 0.19027086666652135.[0m
[32m[I 2025-02-08 02:10:49,749][0m Trial 4 finished with value: 0.17258345137326783 and parameters: {'observation_period_num': 88, 'train_rates': 0.6733647455106548, 'learning_rate': 0.00014454637445280563, 'batch_size': 67, 'step_size': 14, 'gamma': 0.7910197176002634}. Best is trial 4 with value: 0.17258345137326783.[0m
[32m[I 2025-02-08 02:11:18,501][0m Trial 5 finished with value: 0.13852982223033905 and parameters: {'observation_period_num': 67, 'train_rates': 0.9839646838856847, 'learning_rate': 0.00026359510723889624, 'batch_size': 232, 'step_size': 14, 'gamma': 0.7558787894599526}. Best is trial 5 with value: 0.13852982223033905.[0m
[32m[I 2025-02-08 02:12:25,948][0m Trial 6 finished with value: 0.7869195816266602 and parameters: {'observation_period_num': 91, 'train_rates': 0.626734748216328, 'learning_rate': 1.7327566142481996e-06, 'batch_size': 66, 'step_size': 15, 'gamma': 0.7719415084168395}. Best is trial 5 with value: 0.13852982223033905.[0m
[32m[I 2025-02-08 02:13:25,850][0m Trial 7 finished with value: 0.08077056347322997 and parameters: {'observation_period_num': 56, 'train_rates': 0.8180602217393687, 'learning_rate': 0.00016564086151852941, 'batch_size': 92, 'step_size': 11, 'gamma': 0.8725621065579507}. Best is trial 7 with value: 0.08077056347322997.[0m
[32m[I 2025-02-08 02:14:00,855][0m Trial 8 finished with value: 0.5696871325351309 and parameters: {'observation_period_num': 230, 'train_rates': 0.7594734748608059, 'learning_rate': 9.384235400702556e-06, 'batch_size': 149, 'step_size': 14, 'gamma': 0.7786740407910663}. Best is trial 7 with value: 0.08077056347322997.[0m
[32m[I 2025-02-08 02:14:47,785][0m Trial 9 finished with value: 0.3818990326758507 and parameters: {'observation_period_num': 124, 'train_rates': 0.6856753361824749, 'learning_rate': 9.881672668329212e-05, 'batch_size': 103, 'step_size': 1, 'gamma': 0.9069013619755859}. Best is trial 7 with value: 0.08077056347322997.[0m
[32m[I 2025-02-08 02:19:35,906][0m Trial 10 finished with value: 0.03957110157723995 and parameters: {'observation_period_num': 10, 'train_rates': 0.8007287962361207, 'learning_rate': 2.3609653643132523e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8421400224135613}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:24:40,439][0m Trial 11 finished with value: 0.052020858115678505 and parameters: {'observation_period_num': 23, 'train_rates': 0.8048919785536239, 'learning_rate': 2.3842441783347023e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8424083164951497}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:30:00,674][0m Trial 12 finished with value: 0.16891211031926065 and parameters: {'observation_period_num': 6, 'train_rates': 0.7834448698116372, 'learning_rate': 1.81948238604391e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8308941801944306}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:34:57,127][0m Trial 13 finished with value: 0.061762192942202095 and parameters: {'observation_period_num': 9, 'train_rates': 0.8329426085147382, 'learning_rate': 6.388761989680682e-06, 'batch_size': 18, 'step_size': 9, 'gamma': 0.8249696384324586}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:36:46,220][0m Trial 14 finished with value: 0.17230398566067065 and parameters: {'observation_period_num': 34, 'train_rates': 0.7377329930302765, 'learning_rate': 4.412599092062764e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9318051370718469}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:38:19,978][0m Trial 15 finished with value: 0.1367159857402874 and parameters: {'observation_period_num': 145, 'train_rates': 0.8618989719518071, 'learning_rate': 3.624356169758298e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8127564440504433}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:40:29,545][0m Trial 16 finished with value: 0.3185443705033018 and parameters: {'observation_period_num': 39, 'train_rates': 0.726795472052354, 'learning_rate': 4.108751628037656e-06, 'batch_size': 38, 'step_size': 8, 'gamma': 0.8666523828456133}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:41:18,430][0m Trial 17 finished with value: 0.23903070390224457 and parameters: {'observation_period_num': 123, 'train_rates': 0.9521733126660195, 'learning_rate': 1.5123529743926992e-05, 'batch_size': 126, 'step_size': 12, 'gamma': 0.9897648346972192}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:42:26,366][0m Trial 18 finished with value: 0.06305193515428946 and parameters: {'observation_period_num': 26, 'train_rates': 0.8663238962513317, 'learning_rate': 4.685178319263749e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8923803421756437}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:44:53,851][0m Trial 19 finished with value: 0.09945491438962682 and parameters: {'observation_period_num': 84, 'train_rates': 0.8122663575075202, 'learning_rate': 1.8187471255964773e-05, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8487164190016919}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:45:21,950][0m Trial 20 finished with value: 0.7552674427428053 and parameters: {'observation_period_num': 56, 'train_rates': 0.7730155014614428, 'learning_rate': 3.6378898147988876e-06, 'batch_size': 195, 'step_size': 12, 'gamma': 0.8042000093899601}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:49:54,389][0m Trial 21 finished with value: 0.06281229758581507 and parameters: {'observation_period_num': 7, 'train_rates': 0.8376730916111695, 'learning_rate': 8.288691629196835e-06, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8272871939320127}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:55:12,158][0m Trial 22 finished with value: 0.072648493690299 and parameters: {'observation_period_num': 22, 'train_rates': 0.856233534718228, 'learning_rate': 5.420130559603577e-06, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8195884924861296}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:56:59,092][0m Trial 23 finished with value: 0.16922051134418506 and parameters: {'observation_period_num': 6, 'train_rates': 0.7191835084489843, 'learning_rate': 2.353239994976168e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.8554773505075438}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:58:11,172][0m Trial 24 finished with value: 0.06403131452569671 and parameters: {'observation_period_num': 48, 'train_rates': 0.7991969403624452, 'learning_rate': 6.921253599614374e-05, 'batch_size': 74, 'step_size': 7, 'gamma': 0.8798047993786896}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 02:59:01,919][0m Trial 25 finished with value: 0.3185214233527545 and parameters: {'observation_period_num': 73, 'train_rates': 0.9054334877047141, 'learning_rate': 8.66537880657197e-06, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8385435100803525}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:01:32,164][0m Trial 26 finished with value: 0.29128661565482616 and parameters: {'observation_period_num': 105, 'train_rates': 0.8260829314907898, 'learning_rate': 2.9312801748520746e-06, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9224695651141309}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:03:04,876][0m Trial 27 finished with value: 0.20349965016495525 and parameters: {'observation_period_num': 23, 'train_rates': 0.7620042082975242, 'learning_rate': 1.3956752616247406e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8123101739745926}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:06:18,311][0m Trial 28 finished with value: 0.11939516541909198 and parameters: {'observation_period_num': 145, 'train_rates': 0.8456219138793903, 'learning_rate': 2.6897201474508863e-05, 'batch_size': 27, 'step_size': 7, 'gamma': 0.7947362721649031}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:07:03,228][0m Trial 29 finished with value: 0.08423478049891335 and parameters: {'observation_period_num': 39, 'train_rates': 0.891465838072465, 'learning_rate': 7.364195458955165e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8432560297409514}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:08:33,830][0m Trial 30 finished with value: 0.38610859075248644 and parameters: {'observation_period_num': 63, 'train_rates': 0.6957177784517682, 'learning_rate': 6.0639097138089966e-06, 'batch_size': 54, 'step_size': 13, 'gamma': 0.8670922893340306}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:13:49,471][0m Trial 31 finished with value: 0.05133645795285702 and parameters: {'observation_period_num': 8, 'train_rates': 0.8327582051809211, 'learning_rate': 9.554598382251762e-06, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8298500486361916}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:17:01,119][0m Trial 32 finished with value: 0.07196346415369302 and parameters: {'observation_period_num': 21, 'train_rates': 0.8001206020147545, 'learning_rate': 1.1345183759384288e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8297860874347593}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:19:21,884][0m Trial 33 finished with value: 0.12344013037634831 and parameters: {'observation_period_num': 5, 'train_rates': 0.8809879024887736, 'learning_rate': 2.0053907525346034e-06, 'batch_size': 40, 'step_size': 10, 'gamma': 0.859438612671449}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:22:51,984][0m Trial 34 finished with value: 0.310045107284136 and parameters: {'observation_period_num': 178, 'train_rates': 0.9194411971449337, 'learning_rate': 6.712334309470705e-06, 'batch_size': 26, 'step_size': 6, 'gamma': 0.884709320198678}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:23:20,153][0m Trial 35 finished with value: 0.2994466002138568 and parameters: {'observation_period_num': 46, 'train_rates': 0.8318150269906788, 'learning_rate': 2.6443150926078253e-05, 'batch_size': 209, 'step_size': 5, 'gamma': 0.8077995070736954}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:24:32,743][0m Trial 36 finished with value: 1.0697829027466363 and parameters: {'observation_period_num': 25, 'train_rates': 0.9338552962863105, 'learning_rate': 1.2483519682068266e-06, 'batch_size': 82, 'step_size': 8, 'gamma': 0.7873507435058348}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:24:57,372][0m Trial 37 finished with value: 0.5903226697444915 and parameters: {'observation_period_num': 17, 'train_rates': 0.7486697343089511, 'learning_rate': 1.14494135244711e-05, 'batch_size': 246, 'step_size': 3, 'gamma': 0.8408912841363372}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:25:29,816][0m Trial 38 finished with value: 0.9209221117708781 and parameters: {'observation_period_num': 247, 'train_rates': 0.7880085453274028, 'learning_rate': 2.3535789520500187e-06, 'batch_size': 166, 'step_size': 8, 'gamma': 0.7529035077811567}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:26:59,591][0m Trial 39 finished with value: 0.07207585994704188 and parameters: {'observation_period_num': 38, 'train_rates': 0.8790377399538061, 'learning_rate': 2.016031195471846e-05, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8232676122284844}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:28:44,372][0m Trial 40 finished with value: 0.17117750610428295 and parameters: {'observation_period_num': 212, 'train_rates': 0.8128183662601696, 'learning_rate': 0.0002206147451424353, 'batch_size': 48, 'step_size': 13, 'gamma': 0.7765159302185103}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:33:02,640][0m Trial 41 finished with value: 0.0658669109555232 and parameters: {'observation_period_num': 11, 'train_rates': 0.8460701357938059, 'learning_rate': 7.1958524187500925e-06, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8299728708245516}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:37:32,045][0m Trial 42 finished with value: 0.09328770666845201 and parameters: {'observation_period_num': 31, 'train_rates': 0.8290113917648736, 'learning_rate': 4.672426915566779e-06, 'batch_size': 20, 'step_size': 10, 'gamma': 0.7993596660224696}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:40:21,882][0m Trial 43 finished with value: 0.0437121073467822 and parameters: {'observation_period_num': 15, 'train_rates': 0.841523857536336, 'learning_rate': 0.0005270929293904358, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8523604438263169}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:42:49,358][0m Trial 44 finished with value: 0.10608805639878884 and parameters: {'observation_period_num': 51, 'train_rates': 0.7862788007301499, 'learning_rate': 0.000551908652195975, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8504122717700824}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:44:03,970][0m Trial 45 finished with value: 0.051902434467214396 and parameters: {'observation_period_num': 17, 'train_rates': 0.8108403704691093, 'learning_rate': 0.0007063424418666919, 'batch_size': 72, 'step_size': 8, 'gamma': 0.862833079862303}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:44:48,391][0m Trial 46 finished with value: 0.1733171072299788 and parameters: {'observation_period_num': 66, 'train_rates': 0.6357089647738352, 'learning_rate': 0.0004045232079097294, 'batch_size': 105, 'step_size': 11, 'gamma': 0.9040965513865541}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:46:07,568][0m Trial 47 finished with value: 0.2400008085169899 and parameters: {'observation_period_num': 78, 'train_rates': 0.7708992518354133, 'learning_rate': 0.000956546024943261, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8740731891914089}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:47:19,498][0m Trial 48 finished with value: 0.09162656782271611 and parameters: {'observation_period_num': 103, 'train_rates': 0.8073861309369325, 'learning_rate': 0.00029906487790100236, 'batch_size': 75, 'step_size': 5, 'gamma': 0.8615210120485653}. Best is trial 10 with value: 0.03957110157723995.[0m
[32m[I 2025-02-08 03:49:16,415][0m Trial 49 finished with value: 0.053512757060243124 and parameters: {'observation_period_num': 17, 'train_rates': 0.8653162916354109, 'learning_rate': 0.0006596401662300529, 'batch_size': 48, 'step_size': 10, 'gamma': 0.8838640372859731}. Best is trial 10 with value: 0.03957110157723995.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2590 | 0.2094
Epoch 2/300, Loss: 0.1474 | 0.1644
Epoch 3/300, Loss: 0.1346 | 0.1385
Epoch 4/300, Loss: 0.1260 | 0.1175
Epoch 5/300, Loss: 0.1199 | 0.1014
Epoch 6/300, Loss: 0.1158 | 0.0908
Epoch 7/300, Loss: 0.1132 | 0.0849
Epoch 8/300, Loss: 0.1111 | 0.0807
Epoch 9/300, Loss: 0.1091 | 0.0776
Epoch 10/300, Loss: 0.1073 | 0.0751
Epoch 11/300, Loss: 0.1056 | 0.0723
Epoch 12/300, Loss: 0.1044 | 0.0707
Epoch 13/300, Loss: 0.1032 | 0.0692
Epoch 14/300, Loss: 0.1021 | 0.0678
Epoch 15/300, Loss: 0.1010 | 0.0665
Epoch 16/300, Loss: 0.1001 | 0.0648
Epoch 17/300, Loss: 0.0992 | 0.0639
Epoch 18/300, Loss: 0.0983 | 0.0630
Epoch 19/300, Loss: 0.0973 | 0.0621
Epoch 20/300, Loss: 0.0964 | 0.0612
Epoch 21/300, Loss: 0.0954 | 0.0606
Epoch 22/300, Loss: 0.0947 | 0.0598
Epoch 23/300, Loss: 0.0938 | 0.0590
Epoch 24/300, Loss: 0.0930 | 0.0581
Epoch 25/300, Loss: 0.0922 | 0.0573
Epoch 26/300, Loss: 0.0914 | 0.0565
Epoch 27/300, Loss: 0.0908 | 0.0557
Epoch 28/300, Loss: 0.0902 | 0.0550
Epoch 29/300, Loss: 0.0896 | 0.0543
Epoch 30/300, Loss: 0.0890 | 0.0537
Epoch 31/300, Loss: 0.0885 | 0.0531
Epoch 32/300, Loss: 0.0880 | 0.0526
Epoch 33/300, Loss: 0.0876 | 0.0521
Epoch 34/300, Loss: 0.0873 | 0.0516
Epoch 35/300, Loss: 0.0869 | 0.0511
Epoch 36/300, Loss: 0.0865 | 0.0509
Epoch 37/300, Loss: 0.0862 | 0.0505
Epoch 38/300, Loss: 0.0859 | 0.0501
Epoch 39/300, Loss: 0.0856 | 0.0497
Epoch 40/300, Loss: 0.0853 | 0.0493
Epoch 41/300, Loss: 0.0850 | 0.0491
Epoch 42/300, Loss: 0.0847 | 0.0488
Epoch 43/300, Loss: 0.0845 | 0.0485
Epoch 44/300, Loss: 0.0842 | 0.0482
Epoch 45/300, Loss: 0.0840 | 0.0479
Epoch 46/300, Loss: 0.0837 | 0.0476
Epoch 47/300, Loss: 0.0835 | 0.0473
Epoch 48/300, Loss: 0.0833 | 0.0471
Epoch 49/300, Loss: 0.0831 | 0.0469
Epoch 50/300, Loss: 0.0829 | 0.0466
Epoch 51/300, Loss: 0.0827 | 0.0464
Epoch 52/300, Loss: 0.0825 | 0.0462
Epoch 53/300, Loss: 0.0824 | 0.0460
Epoch 54/300, Loss: 0.0822 | 0.0458
Epoch 55/300, Loss: 0.0820 | 0.0456
Epoch 56/300, Loss: 0.0818 | 0.0454
Epoch 57/300, Loss: 0.0817 | 0.0453
Epoch 58/300, Loss: 0.0816 | 0.0451
Epoch 59/300, Loss: 0.0814 | 0.0450
Epoch 60/300, Loss: 0.0813 | 0.0448
Epoch 61/300, Loss: 0.0812 | 0.0447
Epoch 62/300, Loss: 0.0811 | 0.0446
Epoch 63/300, Loss: 0.0810 | 0.0445
Epoch 64/300, Loss: 0.0808 | 0.0443
Epoch 65/300, Loss: 0.0807 | 0.0442
Epoch 66/300, Loss: 0.0806 | 0.0442
Epoch 67/300, Loss: 0.0805 | 0.0441
Epoch 68/300, Loss: 0.0804 | 0.0440
Epoch 69/300, Loss: 0.0803 | 0.0439
Epoch 70/300, Loss: 0.0803 | 0.0438
Epoch 71/300, Loss: 0.0802 | 0.0437
Epoch 72/300, Loss: 0.0801 | 0.0436
Epoch 73/300, Loss: 0.0800 | 0.0435
Epoch 74/300, Loss: 0.0799 | 0.0435
Epoch 75/300, Loss: 0.0799 | 0.0434
Epoch 76/300, Loss: 0.0798 | 0.0434
Epoch 77/300, Loss: 0.0797 | 0.0433
Epoch 78/300, Loss: 0.0796 | 0.0432
Epoch 79/300, Loss: 0.0796 | 0.0432
Epoch 80/300, Loss: 0.0795 | 0.0431
Epoch 81/300, Loss: 0.0794 | 0.0431
Epoch 82/300, Loss: 0.0794 | 0.0430
Epoch 83/300, Loss: 0.0793 | 0.0430
Epoch 84/300, Loss: 0.0793 | 0.0429
Epoch 85/300, Loss: 0.0792 | 0.0429
Epoch 86/300, Loss: 0.0792 | 0.0429
Epoch 87/300, Loss: 0.0791 | 0.0428
Epoch 88/300, Loss: 0.0791 | 0.0428
Epoch 89/300, Loss: 0.0791 | 0.0427
Epoch 90/300, Loss: 0.0790 | 0.0427
Epoch 91/300, Loss: 0.0790 | 0.0427
Epoch 92/300, Loss: 0.0789 | 0.0427
Epoch 93/300, Loss: 0.0789 | 0.0426
Epoch 94/300, Loss: 0.0789 | 0.0426
Epoch 95/300, Loss: 0.0788 | 0.0426
Epoch 96/300, Loss: 0.0788 | 0.0425
Epoch 97/300, Loss: 0.0788 | 0.0425
Epoch 98/300, Loss: 0.0787 | 0.0425
Epoch 99/300, Loss: 0.0787 | 0.0425
Epoch 100/300, Loss: 0.0787 | 0.0424
Epoch 101/300, Loss: 0.0786 | 0.0424
Epoch 102/300, Loss: 0.0786 | 0.0424
Epoch 103/300, Loss: 0.0786 | 0.0424
Epoch 104/300, Loss: 0.0786 | 0.0424
Epoch 105/300, Loss: 0.0786 | 0.0423
Epoch 106/300, Loss: 0.0785 | 0.0423
Epoch 107/300, Loss: 0.0785 | 0.0423
Epoch 108/300, Loss: 0.0785 | 0.0423
Epoch 109/300, Loss: 0.0785 | 0.0423
Epoch 110/300, Loss: 0.0784 | 0.0422
Epoch 111/300, Loss: 0.0784 | 0.0422
Epoch 112/300, Loss: 0.0784 | 0.0422
Epoch 113/300, Loss: 0.0784 | 0.0422
Epoch 114/300, Loss: 0.0784 | 0.0422
Epoch 115/300, Loss: 0.0784 | 0.0422
Epoch 116/300, Loss: 0.0783 | 0.0422
Epoch 117/300, Loss: 0.0783 | 0.0421
Epoch 118/300, Loss: 0.0783 | 0.0421
Epoch 119/300, Loss: 0.0783 | 0.0421
Epoch 120/300, Loss: 0.0783 | 0.0421
Epoch 121/300, Loss: 0.0783 | 0.0421
Epoch 122/300, Loss: 0.0783 | 0.0421
Epoch 123/300, Loss: 0.0782 | 0.0421
Epoch 124/300, Loss: 0.0782 | 0.0421
Epoch 125/300, Loss: 0.0782 | 0.0421
Epoch 126/300, Loss: 0.0782 | 0.0420
Epoch 127/300, Loss: 0.0782 | 0.0420
Epoch 128/300, Loss: 0.0782 | 0.0420
Epoch 129/300, Loss: 0.0782 | 0.0420
Epoch 130/300, Loss: 0.0782 | 0.0420
Epoch 131/300, Loss: 0.0782 | 0.0420
Epoch 132/300, Loss: 0.0782 | 0.0420
Epoch 133/300, Loss: 0.0781 | 0.0420
Epoch 134/300, Loss: 0.0781 | 0.0420
Epoch 135/300, Loss: 0.0781 | 0.0420
Epoch 136/300, Loss: 0.0781 | 0.0420
Epoch 137/300, Loss: 0.0781 | 0.0420
Epoch 138/300, Loss: 0.0781 | 0.0420
Epoch 139/300, Loss: 0.0781 | 0.0420
Epoch 140/300, Loss: 0.0781 | 0.0420
Epoch 141/300, Loss: 0.0781 | 0.0419
Epoch 142/300, Loss: 0.0781 | 0.0419
Epoch 143/300, Loss: 0.0781 | 0.0419
Epoch 144/300, Loss: 0.0781 | 0.0419
Epoch 145/300, Loss: 0.0781 | 0.0419
Epoch 146/300, Loss: 0.0781 | 0.0419
Epoch 147/300, Loss: 0.0780 | 0.0419
Epoch 148/300, Loss: 0.0780 | 0.0419
Epoch 149/300, Loss: 0.0780 | 0.0419
Epoch 150/300, Loss: 0.0780 | 0.0419
Epoch 151/300, Loss: 0.0780 | 0.0419
Epoch 152/300, Loss: 0.0780 | 0.0419
Epoch 153/300, Loss: 0.0780 | 0.0419
Epoch 154/300, Loss: 0.0780 | 0.0419
Epoch 155/300, Loss: 0.0780 | 0.0419
Epoch 156/300, Loss: 0.0780 | 0.0419
Epoch 157/300, Loss: 0.0780 | 0.0419
Epoch 158/300, Loss: 0.0780 | 0.0419
Epoch 159/300, Loss: 0.0780 | 0.0419
Epoch 160/300, Loss: 0.0780 | 0.0419
Epoch 161/300, Loss: 0.0780 | 0.0419
Epoch 162/300, Loss: 0.0780 | 0.0419
Epoch 163/300, Loss: 0.0780 | 0.0419
Epoch 164/300, Loss: 0.0780 | 0.0419
Epoch 165/300, Loss: 0.0780 | 0.0419
Epoch 166/300, Loss: 0.0780 | 0.0419
Epoch 167/300, Loss: 0.0780 | 0.0419
Epoch 168/300, Loss: 0.0780 | 0.0419
Epoch 169/300, Loss: 0.0780 | 0.0419
Epoch 170/300, Loss: 0.0780 | 0.0419
Epoch 171/300, Loss: 0.0780 | 0.0419
Epoch 172/300, Loss: 0.0780 | 0.0419
Epoch 173/300, Loss: 0.0780 | 0.0419
Epoch 174/300, Loss: 0.0780 | 0.0419
Epoch 175/300, Loss: 0.0780 | 0.0419
Epoch 176/300, Loss: 0.0779 | 0.0419
Epoch 177/300, Loss: 0.0779 | 0.0419
Epoch 178/300, Loss: 0.0779 | 0.0419
Epoch 179/300, Loss: 0.0779 | 0.0419
Epoch 180/300, Loss: 0.0779 | 0.0419
Epoch 181/300, Loss: 0.0779 | 0.0419
Epoch 182/300, Loss: 0.0779 | 0.0419
Epoch 183/300, Loss: 0.0779 | 0.0419
Epoch 184/300, Loss: 0.0779 | 0.0419
Epoch 185/300, Loss: 0.0779 | 0.0419
Epoch 186/300, Loss: 0.0779 | 0.0419
Epoch 187/300, Loss: 0.0779 | 0.0419
Epoch 188/300, Loss: 0.0779 | 0.0419
Epoch 189/300, Loss: 0.0779 | 0.0419
Epoch 190/300, Loss: 0.0779 | 0.0419
Epoch 191/300, Loss: 0.0779 | 0.0419
Epoch 192/300, Loss: 0.0779 | 0.0419
Epoch 193/300, Loss: 0.0779 | 0.0419
Epoch 194/300, Loss: 0.0779 | 0.0419
Epoch 195/300, Loss: 0.0779 | 0.0419
Epoch 196/300, Loss: 0.0779 | 0.0419
Epoch 197/300, Loss: 0.0779 | 0.0419
Epoch 198/300, Loss: 0.0779 | 0.0419
Epoch 199/300, Loss: 0.0779 | 0.0419
Epoch 200/300, Loss: 0.0779 | 0.0419
Epoch 201/300, Loss: 0.0779 | 0.0419
Epoch 202/300, Loss: 0.0779 | 0.0419
Epoch 203/300, Loss: 0.0779 | 0.0419
Epoch 204/300, Loss: 0.0779 | 0.0419
Epoch 205/300, Loss: 0.0779 | 0.0419
Epoch 206/300, Loss: 0.0779 | 0.0419
Epoch 207/300, Loss: 0.0779 | 0.0419
Epoch 208/300, Loss: 0.0779 | 0.0419
Epoch 209/300, Loss: 0.0779 | 0.0419
Epoch 210/300, Loss: 0.0779 | 0.0419
Epoch 211/300, Loss: 0.0779 | 0.0419
Epoch 212/300, Loss: 0.0779 | 0.0419
Epoch 213/300, Loss: 0.0779 | 0.0419
Epoch 214/300, Loss: 0.0779 | 0.0419
Epoch 215/300, Loss: 0.0779 | 0.0419
Epoch 216/300, Loss: 0.0779 | 0.0419
Epoch 217/300, Loss: 0.0779 | 0.0419
Epoch 218/300, Loss: 0.0779 | 0.0419
Epoch 219/300, Loss: 0.0779 | 0.0419
Epoch 220/300, Loss: 0.0779 | 0.0419
Epoch 221/300, Loss: 0.0779 | 0.0419
Epoch 222/300, Loss: 0.0779 | 0.0419
Epoch 223/300, Loss: 0.0779 | 0.0419
Epoch 224/300, Loss: 0.0779 | 0.0419
Epoch 225/300, Loss: 0.0779 | 0.0419
Epoch 226/300, Loss: 0.0779 | 0.0419
Epoch 227/300, Loss: 0.0779 | 0.0418
Epoch 228/300, Loss: 0.0779 | 0.0418
Epoch 229/300, Loss: 0.0779 | 0.0418
Epoch 230/300, Loss: 0.0779 | 0.0418
Epoch 231/300, Loss: 0.0779 | 0.0418
Epoch 232/300, Loss: 0.0779 | 0.0418
Epoch 233/300, Loss: 0.0779 | 0.0418
Epoch 234/300, Loss: 0.0779 | 0.0418
Epoch 235/300, Loss: 0.0779 | 0.0418
Epoch 236/300, Loss: 0.0779 | 0.0418
Epoch 237/300, Loss: 0.0779 | 0.0418
Epoch 238/300, Loss: 0.0779 | 0.0418
Epoch 239/300, Loss: 0.0779 | 0.0418
Epoch 240/300, Loss: 0.0779 | 0.0418
Epoch 241/300, Loss: 0.0779 | 0.0418
Epoch 242/300, Loss: 0.0779 | 0.0418
Epoch 243/300, Loss: 0.0779 | 0.0418
Epoch 244/300, Loss: 0.0779 | 0.0418
Epoch 245/300, Loss: 0.0779 | 0.0418
Epoch 246/300, Loss: 0.0779 | 0.0418
Epoch 247/300, Loss: 0.0779 | 0.0418
Epoch 248/300, Loss: 0.0779 | 0.0418
Epoch 249/300, Loss: 0.0779 | 0.0418
Epoch 250/300, Loss: 0.0779 | 0.0418
Epoch 251/300, Loss: 0.0779 | 0.0418
Epoch 252/300, Loss: 0.0779 | 0.0418
Epoch 253/300, Loss: 0.0779 | 0.0418
Epoch 254/300, Loss: 0.0779 | 0.0418
Epoch 255/300, Loss: 0.0779 | 0.0418
Epoch 256/300, Loss: 0.0779 | 0.0418
Epoch 257/300, Loss: 0.0779 | 0.0418
Epoch 258/300, Loss: 0.0779 | 0.0418
Epoch 259/300, Loss: 0.0779 | 0.0418
Epoch 260/300, Loss: 0.0779 | 0.0418
Epoch 261/300, Loss: 0.0779 | 0.0418
Epoch 262/300, Loss: 0.0779 | 0.0418
Epoch 263/300, Loss: 0.0779 | 0.0418
Epoch 264/300, Loss: 0.0779 | 0.0418
Epoch 265/300, Loss: 0.0779 | 0.0418
Epoch 266/300, Loss: 0.0779 | 0.0418
Epoch 267/300, Loss: 0.0779 | 0.0418
Epoch 268/300, Loss: 0.0779 | 0.0418
Epoch 269/300, Loss: 0.0779 | 0.0418
Epoch 270/300, Loss: 0.0779 | 0.0418
Epoch 271/300, Loss: 0.0779 | 0.0418
Epoch 272/300, Loss: 0.0779 | 0.0418
Epoch 273/300, Loss: 0.0779 | 0.0418
Epoch 274/300, Loss: 0.0779 | 0.0418
Epoch 275/300, Loss: 0.0779 | 0.0418
Epoch 276/300, Loss: 0.0779 | 0.0418
Epoch 277/300, Loss: 0.0779 | 0.0418
Epoch 278/300, Loss: 0.0779 | 0.0418
Epoch 279/300, Loss: 0.0779 | 0.0418
Epoch 280/300, Loss: 0.0779 | 0.0418
Epoch 281/300, Loss: 0.0779 | 0.0418
Epoch 282/300, Loss: 0.0779 | 0.0418
Epoch 283/300, Loss: 0.0779 | 0.0418
Epoch 284/300, Loss: 0.0779 | 0.0418
Epoch 285/300, Loss: 0.0779 | 0.0418
Epoch 286/300, Loss: 0.0779 | 0.0418
Epoch 287/300, Loss: 0.0779 | 0.0418
Epoch 288/300, Loss: 0.0779 | 0.0418
Epoch 289/300, Loss: 0.0779 | 0.0418
Epoch 290/300, Loss: 0.0779 | 0.0418
Epoch 291/300, Loss: 0.0779 | 0.0418
Epoch 292/300, Loss: 0.0779 | 0.0418
Epoch 293/300, Loss: 0.0779 | 0.0418
Epoch 294/300, Loss: 0.0779 | 0.0418
Epoch 295/300, Loss: 0.0779 | 0.0418
Epoch 296/300, Loss: 0.0779 | 0.0418
Epoch 297/300, Loss: 0.0779 | 0.0418
Epoch 298/300, Loss: 0.0779 | 0.0418
Epoch 299/300, Loss: 0.0779 | 0.0418
Epoch 300/300, Loss: 0.0779 | 0.0418
Runtime (seconds): 878.5972719192505
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 376.70381750003435
RMSE: 19.408859252929688
MAE: 19.408859252929688
R-squared: nan
[205.41885]
