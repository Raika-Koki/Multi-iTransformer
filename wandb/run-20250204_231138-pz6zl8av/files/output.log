ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-04 23:11:47,035][0m A new study created in memory with name: no-name-9b1fa926-0f69-4af4-a698-b133a3b4e285[0m
[32m[I 2025-02-04 23:12:14,828][0m Trial 0 finished with value: 0.20192386209964752 and parameters: {'observation_period_num': 108, 'train_rates': 0.9200003221664214, 'learning_rate': 3.374721589386054e-05, 'batch_size': 230, 'step_size': 8, 'gamma': 0.9627674871761888}. Best is trial 0 with value: 0.20192386209964752.[0m
[32m[I 2025-02-04 23:12:42,410][0m Trial 1 finished with value: 0.24423950568588285 and parameters: {'observation_period_num': 66, 'train_rates': 0.6958241106023955, 'learning_rate': 6.234035032473855e-05, 'batch_size': 187, 'step_size': 5, 'gamma': 0.8414259071723297}. Best is trial 0 with value: 0.20192386209964752.[0m
[32m[I 2025-02-04 23:13:29,640][0m Trial 2 finished with value: 0.0561326604623061 and parameters: {'observation_period_num': 42, 'train_rates': 0.9510349992365539, 'learning_rate': 0.00022156932526982834, 'batch_size': 132, 'step_size': 8, 'gamma': 0.7904792821367607}. Best is trial 2 with value: 0.0561326604623061.[0m
Early stopping at epoch 49
[32m[I 2025-02-04 23:13:44,838][0m Trial 3 finished with value: 1.7330651277491314 and parameters: {'observation_period_num': 121, 'train_rates': 0.7084595594350963, 'learning_rate': 2.3350249763351197e-06, 'batch_size': 170, 'step_size': 1, 'gamma': 0.819370052326345}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:15:39,856][0m Trial 4 finished with value: 0.16905014112867126 and parameters: {'observation_period_num': 162, 'train_rates': 0.8254205285556231, 'learning_rate': 0.0008596278832259407, 'batch_size': 45, 'step_size': 6, 'gamma': 0.898545238614809}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:16:14,813][0m Trial 5 finished with value: 1.0785380601882935 and parameters: {'observation_period_num': 220, 'train_rates': 0.9662418143048903, 'learning_rate': 1.75547448747887e-06, 'batch_size': 182, 'step_size': 12, 'gamma': 0.7727183716719843}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:16:48,467][0m Trial 6 finished with value: 1.4262808948376422 and parameters: {'observation_period_num': 172, 'train_rates': 0.6603257476608473, 'learning_rate': 1.086359803317946e-06, 'batch_size': 149, 'step_size': 10, 'gamma': 0.829477485965755}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:17:48,471][0m Trial 7 finished with value: 0.18323427461212574 and parameters: {'observation_period_num': 13, 'train_rates': 0.6956240428399956, 'learning_rate': 1.05008636657038e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9548035561606376}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:18:32,970][0m Trial 8 finished with value: 0.496723973110099 and parameters: {'observation_period_num': 101, 'train_rates': 0.6184462006065, 'learning_rate': 2.6012886677762928e-05, 'batch_size': 103, 'step_size': 2, 'gamma': 0.785640216371491}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:20:39,400][0m Trial 9 finished with value: 0.38616097626743373 and parameters: {'observation_period_num': 121, 'train_rates': 0.7646922447217012, 'learning_rate': 4.8195449011747266e-05, 'batch_size': 40, 'step_size': 1, 'gamma': 0.8951533778830367}. Best is trial 2 with value: 0.0561326604623061.[0m
[32m[I 2025-02-04 23:21:35,651][0m Trial 10 finished with value: 0.030284698061086718 and parameters: {'observation_period_num': 6, 'train_rates': 0.8686324800561299, 'learning_rate': 0.00041362032381086464, 'batch_size': 104, 'step_size': 15, 'gamma': 0.7514834332399072}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:22:31,951][0m Trial 11 finished with value: 0.03083223779179642 and parameters: {'observation_period_num': 8, 'train_rates': 0.8752149545604873, 'learning_rate': 0.0004767703589028162, 'batch_size': 107, 'step_size': 15, 'gamma': 0.7572313767987116}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:23:43,117][0m Trial 12 finished with value: 0.03292181326130449 and parameters: {'observation_period_num': 7, 'train_rates': 0.8712933649021924, 'learning_rate': 0.0008329371834761317, 'batch_size': 85, 'step_size': 15, 'gamma': 0.7511726503992661}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:24:34,432][0m Trial 13 finished with value: 0.05642862859133043 and parameters: {'observation_period_num': 57, 'train_rates': 0.8688721904976139, 'learning_rate': 0.00026078930855450534, 'batch_size': 117, 'step_size': 15, 'gamma': 0.7556392524131793}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:25:55,824][0m Trial 14 finished with value: 0.05280074964820576 and parameters: {'observation_period_num': 35, 'train_rates': 0.8124230715993666, 'learning_rate': 0.0002529420381035973, 'batch_size': 68, 'step_size': 13, 'gamma': 0.8064095632483202}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:26:24,259][0m Trial 15 finished with value: 0.1358784823912043 and parameters: {'observation_period_num': 82, 'train_rates': 0.8959148240956766, 'learning_rate': 0.0004127017008212607, 'batch_size': 226, 'step_size': 13, 'gamma': 0.8546666553465007}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:31:02,703][0m Trial 16 finished with value: 0.39349630155733656 and parameters: {'observation_period_num': 243, 'train_rates': 0.7806013530668089, 'learning_rate': 0.00011009419218161634, 'batch_size': 17, 'step_size': 15, 'gamma': 0.877631727721823}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:31:46,790][0m Trial 17 finished with value: 0.14937132191473404 and parameters: {'observation_period_num': 30, 'train_rates': 0.8452629444701155, 'learning_rate': 8.815726628973421e-06, 'batch_size': 134, 'step_size': 11, 'gamma': 0.9311511408955858}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:32:44,553][0m Trial 18 finished with value: 0.11721108672874314 and parameters: {'observation_period_num': 159, 'train_rates': 0.924959081483604, 'learning_rate': 0.00012430177674091744, 'batch_size': 103, 'step_size': 13, 'gamma': 0.7952010290083342}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:34:02,548][0m Trial 19 finished with value: 0.209799283444452 and parameters: {'observation_period_num': 87, 'train_rates': 0.7578448922906145, 'learning_rate': 0.00047937365766232516, 'batch_size': 66, 'step_size': 14, 'gamma': 0.7686325077138165}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:34:45,251][0m Trial 20 finished with value: 0.050581466406583786 and parameters: {'observation_period_num': 9, 'train_rates': 0.9851301303253025, 'learning_rate': 0.00010600579033833194, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9839251538811231}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:35:56,628][0m Trial 21 finished with value: 0.03226204757991525 and parameters: {'observation_period_num': 6, 'train_rates': 0.8684486155575559, 'learning_rate': 0.0008637745580509754, 'batch_size': 80, 'step_size': 15, 'gamma': 0.7564622535963452}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:36:48,662][0m Trial 22 finished with value: 0.07189528350798018 and parameters: {'observation_period_num': 54, 'train_rates': 0.8588798401725397, 'learning_rate': 0.0005893532641301977, 'batch_size': 112, 'step_size': 14, 'gamma': 0.7712541038433203}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:38:00,937][0m Trial 23 finished with value: 0.04996057881106591 and parameters: {'observation_period_num': 25, 'train_rates': 0.9011117447124867, 'learning_rate': 0.0009178171442413705, 'batch_size': 81, 'step_size': 15, 'gamma': 0.75193015400529}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:39:39,836][0m Trial 24 finished with value: 0.06602688272979772 and parameters: {'observation_period_num': 72, 'train_rates': 0.8142158923423942, 'learning_rate': 0.0002724261835374654, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8127917652909313}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:43:14,103][0m Trial 25 finished with value: 0.08532679669416635 and parameters: {'observation_period_num': 39, 'train_rates': 0.8924459483939027, 'learning_rate': 0.0004688151556339544, 'batch_size': 26, 'step_size': 14, 'gamma': 0.7808244642461724}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:44:20,225][0m Trial 26 finished with value: 0.03674125232334648 and parameters: {'observation_period_num': 7, 'train_rates': 0.9383676038754374, 'learning_rate': 0.00015211210552760877, 'batch_size': 93, 'step_size': 9, 'gamma': 0.7992539967532221}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:45:39,556][0m Trial 27 finished with value: 0.06732092571348466 and parameters: {'observation_period_num': 51, 'train_rates': 0.8401505385690351, 'learning_rate': 0.00047851726467919726, 'batch_size': 71, 'step_size': 12, 'gamma': 0.7650122521182217}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:46:29,910][0m Trial 28 finished with value: 0.09573131465995816 and parameters: {'observation_period_num': 26, 'train_rates': 0.8789003836441499, 'learning_rate': 1.6108218396720226e-05, 'batch_size': 121, 'step_size': 14, 'gamma': 0.8403258974214511}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:48:07,157][0m Trial 29 finished with value: 0.15363026670469293 and parameters: {'observation_period_num': 194, 'train_rates': 0.9160874348562573, 'learning_rate': 6.591226986672446e-05, 'batch_size': 57, 'step_size': 6, 'gamma': 0.7857717438286578}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:48:33,791][0m Trial 30 finished with value: 0.09724739892888315 and parameters: {'observation_period_num': 100, 'train_rates': 0.7960068335851016, 'learning_rate': 0.00015751733062067965, 'batch_size': 210, 'step_size': 13, 'gamma': 0.7515991815800905}. Best is trial 10 with value: 0.030284698061086718.[0m
[32m[I 2025-02-04 23:49:36,882][0m Trial 31 finished with value: 0.025027773890727505 and parameters: {'observation_period_num': 6, 'train_rates': 0.8634069975137922, 'learning_rate': 0.0009757295617322969, 'batch_size': 92, 'step_size': 15, 'gamma': 0.753672879705485}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:50:35,670][0m Trial 32 finished with value: 0.0403979818780567 and parameters: {'observation_period_num': 22, 'train_rates': 0.8541225797046553, 'learning_rate': 0.0006713275771084204, 'batch_size': 98, 'step_size': 15, 'gamma': 0.7682954220493432}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:51:16,373][0m Trial 33 finished with value: 0.0368600100337062 and parameters: {'observation_period_num': 5, 'train_rates': 0.9134255047581564, 'learning_rate': 0.00034592705554745926, 'batch_size': 154, 'step_size': 14, 'gamma': 0.7783251406202552}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:51:40,682][0m Trial 34 finished with value: 0.06284206319593025 and parameters: {'observation_period_num': 48, 'train_rates': 0.8249236809148289, 'learning_rate': 0.0009493666966397333, 'batch_size': 256, 'step_size': 4, 'gamma': 0.801829449445199}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:52:32,011][0m Trial 35 finished with value: 0.12490799323175893 and parameters: {'observation_period_num': 67, 'train_rates': 0.9544870058840318, 'learning_rate': 0.0006346222188811319, 'batch_size': 122, 'step_size': 15, 'gamma': 0.8232070909603302}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:53:47,728][0m Trial 36 finished with value: 0.04053010294450326 and parameters: {'observation_period_num': 19, 'train_rates': 0.8899273132115871, 'learning_rate': 0.0002134323689594129, 'batch_size': 78, 'step_size': 7, 'gamma': 0.7613135827862414}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:54:27,602][0m Trial 37 finished with value: 0.04422054162367861 and parameters: {'observation_period_num': 40, 'train_rates': 0.8385614799649453, 'learning_rate': 0.00034907500012220055, 'batch_size': 144, 'step_size': 12, 'gamma': 0.7918262503480589}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:55:14,659][0m Trial 38 finished with value: 0.7935895545285346 and parameters: {'observation_period_num': 142, 'train_rates': 0.7465111924117931, 'learning_rate': 4.342203160508561e-06, 'batch_size': 108, 'step_size': 11, 'gamma': 0.7799739404192463}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:55:56,504][0m Trial 39 finished with value: 0.17070466056274078 and parameters: {'observation_period_num': 21, 'train_rates': 0.7351699700287523, 'learning_rate': 7.935696633114382e-05, 'batch_size': 134, 'step_size': 13, 'gamma': 0.8580714689920844}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:56:30,433][0m Trial 40 finished with value: 0.18782776990124295 and parameters: {'observation_period_num': 35, 'train_rates': 0.7876600161341945, 'learning_rate': 0.000660014117334487, 'batch_size': 169, 'step_size': 14, 'gamma': 0.7634953198567697}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:57:35,179][0m Trial 41 finished with value: 0.04180313458684647 and parameters: {'observation_period_num': 14, 'train_rates': 0.8624955596460645, 'learning_rate': 0.0009799657972779097, 'batch_size': 90, 'step_size': 15, 'gamma': 0.7506284073228287}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-04 23:58:45,037][0m Trial 42 finished with value: 0.028264955268241465 and parameters: {'observation_period_num': 5, 'train_rates': 0.8751033199918358, 'learning_rate': 0.0006824995353725055, 'batch_size': 85, 'step_size': 15, 'gamma': 0.7596809384635932}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:01:23,702][0m Trial 43 finished with value: 0.030058711549597925 and parameters: {'observation_period_num': 16, 'train_rates': 0.9273065259094093, 'learning_rate': 0.00019778902790029115, 'batch_size': 37, 'step_size': 14, 'gamma': 0.7732063285182471}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:04:05,591][0m Trial 44 finished with value: 0.043643643516440726 and parameters: {'observation_period_num': 25, 'train_rates': 0.9707028556546008, 'learning_rate': 0.00020654928323231127, 'batch_size': 38, 'step_size': 14, 'gamma': 0.7750885967543222}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:05:51,653][0m Trial 45 finished with value: 0.07027195798272783 and parameters: {'observation_period_num': 44, 'train_rates': 0.9515504914022728, 'learning_rate': 3.885845616266758e-05, 'batch_size': 57, 'step_size': 4, 'gamma': 0.9151957453565727}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:08:51,068][0m Trial 46 finished with value: 0.17168131378872134 and parameters: {'observation_period_num': 60, 'train_rates': 0.9270898169935502, 'learning_rate': 0.00032340528052836586, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8093711366336497}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:10:54,170][0m Trial 47 finished with value: 0.031444382016285716 and parameters: {'observation_period_num': 16, 'train_rates': 0.9055458142793456, 'learning_rate': 0.00018061949853214775, 'batch_size': 47, 'step_size': 15, 'gamma': 0.7893884176312379}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:11:40,764][0m Trial 48 finished with value: 0.06141729448148955 and parameters: {'observation_period_num': 75, 'train_rates': 0.877358964461855, 'learning_rate': 0.0005423648121961392, 'batch_size': 122, 'step_size': 9, 'gamma': 0.7622714253420946}. Best is trial 31 with value: 0.025027773890727505.[0m
[32m[I 2025-02-05 00:12:41,926][0m Trial 49 finished with value: 0.05507667327397748 and parameters: {'observation_period_num': 32, 'train_rates': 0.9349616451489414, 'learning_rate': 0.00038663648382896684, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8374782696280335}. Best is trial 31 with value: 0.025027773890727505.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-05 00:12:41,936][0m A new study created in memory with name: no-name-d251d103-3fd0-4a42-b1ea-24bd4a5ebd43[0m
[32m[I 2025-02-05 00:13:21,134][0m Trial 0 finished with value: 0.6353224548664722 and parameters: {'observation_period_num': 202, 'train_rates': 0.9340494173665895, 'learning_rate': 2.2350463769586743e-06, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9519615877828838}. Best is trial 0 with value: 0.6353224548664722.[0m
[32m[I 2025-02-05 00:14:47,359][0m Trial 1 finished with value: 0.173135132073559 and parameters: {'observation_period_num': 8, 'train_rates': 0.7716611403274174, 'learning_rate': 0.0006055953897602771, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9636861491267577}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:15:11,382][0m Trial 2 finished with value: 0.24380088333160646 and parameters: {'observation_period_num': 92, 'train_rates': 0.7834401429438743, 'learning_rate': 8.202875415733193e-05, 'batch_size': 238, 'step_size': 7, 'gamma': 0.926104745495941}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:15:58,699][0m Trial 3 finished with value: 0.21610350440616258 and parameters: {'observation_period_num': 170, 'train_rates': 0.8828664140769874, 'learning_rate': 2.640771753665612e-05, 'batch_size': 122, 'step_size': 8, 'gamma': 0.9071565715624326}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:16:22,207][0m Trial 4 finished with value: 0.3178654293837822 and parameters: {'observation_period_num': 218, 'train_rates': 0.6065012832081506, 'learning_rate': 0.000660314704885317, 'batch_size': 193, 'step_size': 3, 'gamma': 0.8505277968888196}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:16:51,609][0m Trial 5 finished with value: 0.19799239933490753 and parameters: {'observation_period_num': 121, 'train_rates': 0.9299252907861801, 'learning_rate': 0.00016829051824403672, 'batch_size': 210, 'step_size': 3, 'gamma': 0.8168635565548487}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:17:34,591][0m Trial 6 finished with value: 2.416007084912622 and parameters: {'observation_period_num': 124, 'train_rates': 0.7707521221192719, 'learning_rate': 1.063184111958081e-06, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8662677293785636}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:17:57,251][0m Trial 7 finished with value: 0.24575706392860802 and parameters: {'observation_period_num': 146, 'train_rates': 0.6503166311806012, 'learning_rate': 0.0001317838541521598, 'batch_size': 208, 'step_size': 12, 'gamma': 0.8632202981137821}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:18:29,679][0m Trial 8 finished with value: 0.6092599396570832 and parameters: {'observation_period_num': 175, 'train_rates': 0.7963934273323374, 'learning_rate': 5.401306818184097e-06, 'batch_size': 160, 'step_size': 6, 'gamma': 0.7675495585990166}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:18:58,592][0m Trial 9 finished with value: 0.22235801348159479 and parameters: {'observation_period_num': 58, 'train_rates': 0.7921819846195726, 'learning_rate': 7.884297343045987e-05, 'batch_size': 203, 'step_size': 10, 'gamma': 0.983969913292516}. Best is trial 1 with value: 0.173135132073559.[0m
[32m[I 2025-02-05 00:22:33,339][0m Trial 10 finished with value: 0.163286153884495 and parameters: {'observation_period_num': 22, 'train_rates': 0.7098135829816477, 'learning_rate': 0.0009748484593326499, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9896334490667807}. Best is trial 10 with value: 0.163286153884495.[0m
[32m[I 2025-02-05 00:25:19,648][0m Trial 11 finished with value: 0.1469587906030938 and parameters: {'observation_period_num': 16, 'train_rates': 0.6949147412626984, 'learning_rate': 0.0009934942304400186, 'batch_size': 28, 'step_size': 15, 'gamma': 0.9870395990464249}. Best is trial 11 with value: 0.1469587906030938.[0m
[32m[I 2025-02-05 00:28:33,928][0m Trial 12 finished with value: 0.14018569521524094 and parameters: {'observation_period_num': 5, 'train_rates': 0.6911144719146379, 'learning_rate': 0.0009004933772944667, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9871069949770082}. Best is trial 12 with value: 0.14018569521524094.[0m
[32m[I 2025-02-05 00:33:19,962][0m Trial 13 finished with value: 0.2814943704219505 and parameters: {'observation_period_num': 57, 'train_rates': 0.6966373418172358, 'learning_rate': 0.000300066067938264, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9155144372153499}. Best is trial 12 with value: 0.14018569521524094.[0m
[32m[I 2025-02-05 00:34:30,414][0m Trial 14 finished with value: 0.24286695573634157 and parameters: {'observation_period_num': 48, 'train_rates': 0.7024313152545746, 'learning_rate': 2.3775549982620195e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.942673257007115}. Best is trial 12 with value: 0.14018569521524094.[0m
[32m[I 2025-02-05 00:35:35,147][0m Trial 15 finished with value: 0.17388855658049313 and parameters: {'observation_period_num': 85, 'train_rates': 0.6369542929068648, 'learning_rate': 0.00033645652074269273, 'batch_size': 70, 'step_size': 11, 'gamma': 0.9095630350286469}. Best is trial 12 with value: 0.14018569521524094.[0m
[32m[I 2025-02-05 00:37:11,192][0m Trial 16 finished with value: 0.18267674575845647 and parameters: {'observation_period_num': 7, 'train_rates': 0.7255916783163562, 'learning_rate': 9.416527271238218e-06, 'batch_size': 51, 'step_size': 15, 'gamma': 0.9674697247011904}. Best is trial 12 with value: 0.14018569521524094.[0m
[32m[I 2025-02-05 00:38:13,460][0m Trial 17 finished with value: 0.04818388266933833 and parameters: {'observation_period_num': 30, 'train_rates': 0.8310163944400604, 'learning_rate': 0.00029274762873602293, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8207729512690776}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:39:09,540][0m Trial 18 finished with value: 0.04931948377805598 and parameters: {'observation_period_num': 37, 'train_rates': 0.8542106520458891, 'learning_rate': 0.0003301963079663028, 'batch_size': 100, 'step_size': 13, 'gamma': 0.800345771529625}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:40:09,211][0m Trial 19 finished with value: 0.11782574526965618 and parameters: {'observation_period_num': 96, 'train_rates': 0.8599120498609473, 'learning_rate': 6.300318111860463e-05, 'batch_size': 94, 'step_size': 5, 'gamma': 0.7931420014312154}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:41:02,873][0m Trial 20 finished with value: 0.05154225245184664 and parameters: {'observation_period_num': 45, 'train_rates': 0.846073294758024, 'learning_rate': 0.00021762626059102676, 'batch_size': 103, 'step_size': 13, 'gamma': 0.820362087985547}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:41:58,110][0m Trial 21 finished with value: 0.04947729468044946 and parameters: {'observation_period_num': 37, 'train_rates': 0.8470349883325481, 'learning_rate': 0.00026935397629987255, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8286053286942637}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:42:56,578][0m Trial 22 finished with value: 0.049981613928566176 and parameters: {'observation_period_num': 37, 'train_rates': 0.8321712341827475, 'learning_rate': 0.0003956497893561924, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8277025362805369}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:43:48,739][0m Trial 23 finished with value: 0.21709036144343288 and parameters: {'observation_period_num': 251, 'train_rates': 0.8983050442332223, 'learning_rate': 5.197203008159721e-05, 'batch_size': 103, 'step_size': 12, 'gamma': 0.7889028012743824}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:44:30,284][0m Trial 24 finished with value: 0.06166539179242175 and parameters: {'observation_period_num': 71, 'train_rates': 0.8244548764643805, 'learning_rate': 0.00013731605966027104, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8409884602266353}. Best is trial 17 with value: 0.04818388266933833.[0m
[32m[I 2025-02-05 00:45:09,029][0m Trial 25 finished with value: 0.046744126826524734 and parameters: {'observation_period_num': 36, 'train_rates': 0.9834938237975381, 'learning_rate': 0.00044337134205373117, 'batch_size': 164, 'step_size': 13, 'gamma': 0.8886704946296804}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:45:45,779][0m Trial 26 finished with value: 0.07175688445568085 and parameters: {'observation_period_num': 75, 'train_rates': 0.9489365677237214, 'learning_rate': 0.0004126286217899884, 'batch_size': 168, 'step_size': 14, 'gamma': 0.7954159055781638}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:46:23,539][0m Trial 27 finished with value: 0.054102781337577024 and parameters: {'observation_period_num': 29, 'train_rates': 0.8947398148602794, 'learning_rate': 0.00011940075864523005, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8783110358313849}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:47:17,885][0m Trial 28 finished with value: 0.26947101950645447 and parameters: {'observation_period_num': 107, 'train_rates': 0.9795697753153784, 'learning_rate': 1.1565131297863251e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.8865656415939318}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:47:53,448][0m Trial 29 finished with value: 0.1642829328775406 and parameters: {'observation_period_num': 67, 'train_rates': 0.985345983203481, 'learning_rate': 4.3555728800622816e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.7579416039483314}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:49:03,593][0m Trial 30 finished with value: 0.13567411675028604 and parameters: {'observation_period_num': 145, 'train_rates': 0.9481570842522467, 'learning_rate': 0.0005139625121541087, 'batch_size': 81, 'step_size': 14, 'gamma': 0.8039162153628748}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:49:46,879][0m Trial 31 finished with value: 0.04703918928701674 and parameters: {'observation_period_num': 30, 'train_rates': 0.863900373013316, 'learning_rate': 0.00022523095254052317, 'batch_size': 135, 'step_size': 13, 'gamma': 0.7763851789316678}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:50:25,037][0m Trial 32 finished with value: 0.16953144813457646 and parameters: {'observation_period_num': 27, 'train_rates': 0.7446988625151719, 'learning_rate': 0.00021962443491683198, 'batch_size': 145, 'step_size': 13, 'gamma': 0.7840376345042022}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:51:05,479][0m Trial 33 finished with value: 0.05571339101824042 and parameters: {'observation_period_num': 54, 'train_rates': 0.8742567780883217, 'learning_rate': 0.0005784290441134468, 'batch_size': 144, 'step_size': 14, 'gamma': 0.775219760909836}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:51:30,166][0m Trial 34 finished with value: 0.09182184934616089 and parameters: {'observation_period_num': 35, 'train_rates': 0.9225807955152996, 'learning_rate': 0.0001000535237218365, 'batch_size': 255, 'step_size': 10, 'gamma': 0.7519111144715613}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:53:20,864][0m Trial 35 finished with value: 0.047872096841985526 and parameters: {'observation_period_num': 17, 'train_rates': 0.8128798234916487, 'learning_rate': 0.00017311898127177192, 'batch_size': 48, 'step_size': 12, 'gamma': 0.8935869481697886}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:54:25,411][0m Trial 36 finished with value: 0.16949341061286177 and parameters: {'observation_period_num': 18, 'train_rates': 0.7635288609847117, 'learning_rate': 0.00018662094471018738, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8851656701255052}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:56:30,115][0m Trial 37 finished with value: 0.13411357473006238 and parameters: {'observation_period_num': 79, 'train_rates': 0.8132937091681676, 'learning_rate': 0.0006148081417468419, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9336507574598779}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:58:30,670][0m Trial 38 finished with value: 0.0999089364113623 and parameters: {'observation_period_num': 102, 'train_rates': 0.809822790405073, 'learning_rate': 3.66654607715145e-05, 'batch_size': 42, 'step_size': 8, 'gamma': 0.8503293702738431}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:59:05,868][0m Trial 39 finished with value: 0.05363213804718178 and parameters: {'observation_period_num': 19, 'train_rates': 0.9112873960754428, 'learning_rate': 0.00016052661016511415, 'batch_size': 178, 'step_size': 9, 'gamma': 0.899659592298632}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 00:59:55,243][0m Trial 40 finished with value: 0.17111466233344638 and parameters: {'observation_period_num': 195, 'train_rates': 0.8768037472704832, 'learning_rate': 8.21551780348537e-05, 'batch_size': 115, 'step_size': 14, 'gamma': 0.856831709936222}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 01:00:37,977][0m Trial 41 finished with value: 0.048666743844111945 and parameters: {'observation_period_num': 43, 'train_rates': 0.8549331570590589, 'learning_rate': 0.0002725297263804788, 'batch_size': 131, 'step_size': 13, 'gamma': 0.8051197398229635}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 01:01:19,821][0m Trial 42 finished with value: 0.19138356231705322 and parameters: {'observation_period_num': 61, 'train_rates': 0.7714317612787237, 'learning_rate': 0.00023880732679880355, 'batch_size': 124, 'step_size': 12, 'gamma': 0.8121769717347592}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 01:02:01,723][0m Trial 43 finished with value: 0.05629892274737358 and parameters: {'observation_period_num': 49, 'train_rates': 0.803943887445699, 'learning_rate': 0.0005086598168952461, 'batch_size': 135, 'step_size': 14, 'gamma': 0.7713652003849935}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 01:02:43,396][0m Trial 44 finished with value: 0.04722358658909798 and parameters: {'observation_period_num': 5, 'train_rates': 0.9653109120218133, 'learning_rate': 0.0007482513000400314, 'batch_size': 153, 'step_size': 12, 'gamma': 0.8374018576038113}. Best is trial 25 with value: 0.046744126826524734.[0m
[32m[I 2025-02-05 01:03:16,445][0m Trial 45 finished with value: 0.04486493393778801 and parameters: {'observation_period_num': 7, 'train_rates': 0.9645613337122512, 'learning_rate': 0.0007259864803608803, 'batch_size': 195, 'step_size': 7, 'gamma': 0.842671736967826}. Best is trial 45 with value: 0.04486493393778801.[0m
[32m[I 2025-02-05 01:03:45,840][0m Trial 46 finished with value: 0.04192909225821495 and parameters: {'observation_period_num': 15, 'train_rates': 0.9638757921127494, 'learning_rate': 0.0007522031243509124, 'batch_size': 221, 'step_size': 6, 'gamma': 0.8731957484409746}. Best is trial 46 with value: 0.04192909225821495.[0m
[32m[I 2025-02-05 01:04:15,025][0m Trial 47 finished with value: 0.047437261790037155 and parameters: {'observation_period_num': 5, 'train_rates': 0.9664267820650645, 'learning_rate': 0.0007832724780855372, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8698611414723074}. Best is trial 46 with value: 0.04192909225821495.[0m
[32m[I 2025-02-05 01:04:44,844][0m Trial 48 finished with value: 0.04590559005737305 and parameters: {'observation_period_num': 13, 'train_rates': 0.9597385064572886, 'learning_rate': 0.0008229087123672829, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8421434424210409}. Best is trial 46 with value: 0.04192909225821495.[0m
[32m[I 2025-02-05 01:05:14,174][0m Trial 49 finished with value: 0.05695420503616333 and parameters: {'observation_period_num': 16, 'train_rates': 0.9404540462577088, 'learning_rate': 0.00043837638431775685, 'batch_size': 224, 'step_size': 4, 'gamma': 0.8589123823153999}. Best is trial 46 with value: 0.04192909225821495.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-05 01:05:14,185][0m A new study created in memory with name: no-name-e28c5147-7879-4291-b4bf-4639fdf6b677[0m
[32m[I 2025-02-05 01:05:42,089][0m Trial 0 finished with value: 0.17516204989106088 and parameters: {'observation_period_num': 144, 'train_rates': 0.9149178006884813, 'learning_rate': 4.7662861696687846e-05, 'batch_size': 224, 'step_size': 5, 'gamma': 0.9522406433956379}. Best is trial 0 with value: 0.17516204989106088.[0m
[32m[I 2025-02-05 01:06:15,608][0m Trial 1 finished with value: 0.28213516139147576 and parameters: {'observation_period_num': 175, 'train_rates': 0.8974330207558292, 'learning_rate': 2.5819827979936767e-05, 'batch_size': 170, 'step_size': 9, 'gamma': 0.8212557818243171}. Best is trial 0 with value: 0.17516204989106088.[0m
[32m[I 2025-02-05 01:06:59,630][0m Trial 2 finished with value: 0.30175691777533226 and parameters: {'observation_period_num': 244, 'train_rates': 0.6975694084998325, 'learning_rate': 0.00048173955178953433, 'batch_size': 108, 'step_size': 14, 'gamma': 0.871159047533647}. Best is trial 0 with value: 0.17516204989106088.[0m
[32m[I 2025-02-05 01:08:02,863][0m Trial 3 finished with value: 0.08915910945974127 and parameters: {'observation_period_num': 52, 'train_rates': 0.905716731291342, 'learning_rate': 2.12483601680815e-05, 'batch_size': 93, 'step_size': 14, 'gamma': 0.8072402066200163}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:09:10,375][0m Trial 4 finished with value: 0.36211154273484697 and parameters: {'observation_period_num': 123, 'train_rates': 0.613469640465443, 'learning_rate': 1.0960650804300724e-05, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9139386974209618}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:10:41,193][0m Trial 5 finished with value: 0.22144915944306084 and parameters: {'observation_period_num': 107, 'train_rates': 0.8020459934535042, 'learning_rate': 9.778566027665612e-06, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9164738461803195}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:13:19,902][0m Trial 6 finished with value: 0.2221735369149453 and parameters: {'observation_period_num': 112, 'train_rates': 0.6779588221438896, 'learning_rate': 9.549200881650087e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9370197497518681}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:13:53,366][0m Trial 7 finished with value: 0.12246809006896928 and parameters: {'observation_period_num': 146, 'train_rates': 0.9086192131056623, 'learning_rate': 0.0005031386479308796, 'batch_size': 176, 'step_size': 10, 'gamma': 0.8976107681593608}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:14:26,742][0m Trial 8 finished with value: 0.38498044880645826 and parameters: {'observation_period_num': 242, 'train_rates': 0.6427580016168202, 'learning_rate': 0.000264233993071122, 'batch_size': 134, 'step_size': 15, 'gamma': 0.865650843380673}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:14:59,431][0m Trial 9 finished with value: 0.4843207337306096 and parameters: {'observation_period_num': 189, 'train_rates': 0.7648656069428246, 'learning_rate': 7.93704149215455e-06, 'batch_size': 157, 'step_size': 5, 'gamma': 0.9346726743432778}. Best is trial 3 with value: 0.08915910945974127.[0m
Early stopping at epoch 37
[32m[I 2025-02-05 01:15:25,298][0m Trial 10 finished with value: 2.0135371685028076 and parameters: {'observation_period_num': 21, 'train_rates': 0.9786713110150078, 'learning_rate': 1.2814613875297549e-06, 'batch_size': 95, 'step_size': 1, 'gamma': 0.7656721628223654}. Best is trial 3 with value: 0.08915910945974127.[0m
[32m[I 2025-02-05 01:15:54,965][0m Trial 11 finished with value: 0.05227226105260078 and parameters: {'observation_period_num': 46, 'train_rates': 0.8724351859226245, 'learning_rate': 0.0009241767334830139, 'batch_size': 211, 'step_size': 12, 'gamma': 0.7891765263887426}. Best is trial 11 with value: 0.05227226105260078.[0m
[32m[I 2025-02-05 01:16:20,672][0m Trial 12 finished with value: 0.058274240169635755 and parameters: {'observation_period_num': 36, 'train_rates': 0.8338511403921359, 'learning_rate': 0.00013161696464076119, 'batch_size': 223, 'step_size': 12, 'gamma': 0.7754483028959381}. Best is trial 11 with value: 0.05227226105260078.[0m
[32m[I 2025-02-05 01:16:44,842][0m Trial 13 finished with value: 0.07055167351883342 and parameters: {'observation_period_num': 65, 'train_rates': 0.8194987334529882, 'learning_rate': 0.0008602589182349396, 'batch_size': 254, 'step_size': 12, 'gamma': 0.750827392192915}. Best is trial 11 with value: 0.05227226105260078.[0m
[32m[I 2025-02-05 01:17:12,633][0m Trial 14 finished with value: 0.048536207444137995 and parameters: {'observation_period_num': 6, 'train_rates': 0.8472929551654024, 'learning_rate': 0.00014729507723957114, 'batch_size': 212, 'step_size': 12, 'gamma': 0.7970658257763487}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:17:43,131][0m Trial 15 finished with value: 0.052899939016761065 and parameters: {'observation_period_num': 12, 'train_rates': 0.8546140748985327, 'learning_rate': 0.00021745440372156072, 'batch_size': 203, 'step_size': 7, 'gamma': 0.8132833193506529}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:18:05,053][0m Trial 16 finished with value: 0.21085728206083007 and parameters: {'observation_period_num': 73, 'train_rates': 0.7584985835230409, 'learning_rate': 0.0007450682428530373, 'batch_size': 251, 'step_size': 11, 'gamma': 0.9884639412246855}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:18:38,300][0m Trial 17 finished with value: 0.06381364911794662 and parameters: {'observation_period_num': 6, 'train_rates': 0.9828824229724027, 'learning_rate': 7.119767997291084e-05, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8437401078585806}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:19:18,350][0m Trial 18 finished with value: 0.06689033988667162 and parameters: {'observation_period_num': 87, 'train_rates': 0.8675205435747576, 'learning_rate': 0.0003003152615596484, 'batch_size': 138, 'step_size': 13, 'gamma': 0.7881136981066683}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:19:42,009][0m Trial 19 finished with value: 0.19332128353802105 and parameters: {'observation_period_num': 39, 'train_rates': 0.7675236188732174, 'learning_rate': 0.0001375728087059239, 'batch_size': 226, 'step_size': 15, 'gamma': 0.8403371696534541}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:20:12,900][0m Trial 20 finished with value: 1.1657830476760864 and parameters: {'observation_period_num': 84, 'train_rates': 0.9511232585477108, 'learning_rate': 1.361815164096683e-06, 'batch_size': 198, 'step_size': 10, 'gamma': 0.7903423584895324}. Best is trial 14 with value: 0.048536207444137995.[0m
[32m[I 2025-02-05 01:20:42,971][0m Trial 21 finished with value: 0.04391330227982707 and parameters: {'observation_period_num': 7, 'train_rates': 0.8609590620946834, 'learning_rate': 0.0002387080768878746, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8108397645277955}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:21:15,290][0m Trial 22 finished with value: 0.046854085435992794 and parameters: {'observation_period_num': 32, 'train_rates': 0.8701353532388842, 'learning_rate': 0.0003295427447980636, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8298461712071659}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:21:47,339][0m Trial 23 finished with value: 0.0494637550658347 and parameters: {'observation_period_num': 26, 'train_rates': 0.8356060330608513, 'learning_rate': 0.00019665247700041387, 'batch_size': 179, 'step_size': 7, 'gamma': 0.8447792508724852}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:22:29,795][0m Trial 24 finished with value: 0.08690729398619045 and parameters: {'observation_period_num': 9, 'train_rates': 0.947858247601586, 'learning_rate': 5.639381467468703e-05, 'batch_size': 150, 'step_size': 4, 'gamma': 0.826006864346978}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:22:52,376][0m Trial 25 finished with value: 0.1693698331168512 and parameters: {'observation_period_num': 58, 'train_rates': 0.7166408094865349, 'learning_rate': 0.00048480451740393537, 'batch_size': 237, 'step_size': 8, 'gamma': 0.8645985501215826}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:23:22,915][0m Trial 26 finished with value: 0.18387234364151955 and parameters: {'observation_period_num': 29, 'train_rates': 0.7862829883080428, 'learning_rate': 0.0003549695809641742, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8027635285214533}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:24:11,069][0m Trial 27 finished with value: 0.06878681764006615 and parameters: {'observation_period_num': 5, 'train_rates': 0.8812684684185424, 'learning_rate': 0.0001529616382411794, 'batch_size': 126, 'step_size': 2, 'gamma': 0.8291760896087772}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:24:49,233][0m Trial 28 finished with value: 0.1113664494787759 and parameters: {'observation_period_num': 92, 'train_rates': 0.9345532231270823, 'learning_rate': 8.994058174677399e-05, 'batch_size': 161, 'step_size': 8, 'gamma': 0.880238283758764}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:25:16,362][0m Trial 29 finished with value: 0.12385789601615181 and parameters: {'observation_period_num': 30, 'train_rates': 0.8524194865357932, 'learning_rate': 6.189431853172388e-05, 'batch_size': 226, 'step_size': 5, 'gamma': 0.7670262283112422}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:25:41,046][0m Trial 30 finished with value: 0.4065412580194869 and parameters: {'observation_period_num': 50, 'train_rates': 0.7339661876849549, 'learning_rate': 3.582446246080446e-05, 'batch_size': 214, 'step_size': 3, 'gamma': 0.8530100795825913}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:26:12,769][0m Trial 31 finished with value: 0.046446700769711316 and parameters: {'observation_period_num': 26, 'train_rates': 0.8319069612623554, 'learning_rate': 0.00020042146345965983, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8382857671880036}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:26:42,827][0m Trial 32 finished with value: 0.05140222657491537 and parameters: {'observation_period_num': 21, 'train_rates': 0.8059872602678195, 'learning_rate': 0.000355800292390717, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8270177405712169}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:27:16,606][0m Trial 33 finished with value: 0.11770050674676895 and parameters: {'observation_period_num': 146, 'train_rates': 0.8878399762240002, 'learning_rate': 0.00018297659903490713, 'batch_size': 171, 'step_size': 9, 'gamma': 0.8025154667846076}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:27:40,641][0m Trial 34 finished with value: 0.06524928156719652 and parameters: {'observation_period_num': 71, 'train_rates': 0.8359822185428484, 'learning_rate': 0.0005524949613335012, 'batch_size': 242, 'step_size': 6, 'gamma': 0.8158592223218634}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:28:14,539][0m Trial 35 finished with value: 0.13983734975301146 and parameters: {'observation_period_num': 41, 'train_rates': 0.9200439106133262, 'learning_rate': 2.1778763171894845e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8826427384874782}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:28:42,751][0m Trial 36 finished with value: 0.13597402032338032 and parameters: {'observation_period_num': 171, 'train_rates': 0.7886265842130126, 'learning_rate': 0.00011438879327068764, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8560387485745736}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:29:11,179][0m Trial 37 finished with value: 0.10309626203444269 and parameters: {'observation_period_num': 20, 'train_rates': 0.8157773638948462, 'learning_rate': 3.158679342738836e-05, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8337869201135198}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:29:47,070][0m Trial 38 finished with value: 0.07088633923698537 and parameters: {'observation_period_num': 59, 'train_rates': 0.8993947993519165, 'learning_rate': 0.00026918311651213733, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7999911928500038}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:30:32,443][0m Trial 39 finished with value: 0.6768814552538466 and parameters: {'observation_period_num': 230, 'train_rates': 0.8517612818256678, 'learning_rate': 4.023441652181978e-06, 'batch_size': 116, 'step_size': 5, 'gamma': 0.8154175638973075}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:31:14,750][0m Trial 40 finished with value: 0.340201037232153 and parameters: {'observation_period_num': 103, 'train_rates': 0.9224180126288417, 'learning_rate': 4.382819245253808e-05, 'batch_size': 141, 'step_size': 4, 'gamma': 0.7783510923876346}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:31:47,159][0m Trial 41 finished with value: 0.047147617144529044 and parameters: {'observation_period_num': 24, 'train_rates': 0.8317776030668408, 'learning_rate': 0.000209094315634601, 'batch_size': 178, 'step_size': 7, 'gamma': 0.8497479350868667}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:32:25,364][0m Trial 42 finished with value: 0.05159926319277612 and parameters: {'observation_period_num': 17, 'train_rates': 0.8628326516161647, 'learning_rate': 8.680083387693939e-05, 'batch_size': 157, 'step_size': 9, 'gamma': 0.8949173253500654}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:32:55,267][0m Trial 43 finished with value: 0.046887157098868 and parameters: {'observation_period_num': 36, 'train_rates': 0.8295476970445875, 'learning_rate': 0.0005664715266056088, 'batch_size': 187, 'step_size': 6, 'gamma': 0.8557512878254503}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:33:33,392][0m Trial 44 finished with value: 0.05187010453904376 and parameters: {'observation_period_num': 48, 'train_rates': 0.8246469138703922, 'learning_rate': 0.0006698405467928866, 'batch_size': 149, 'step_size': 6, 'gamma': 0.8589450781046608}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:34:06,357][0m Trial 45 finished with value: 0.04534581750631332 and parameters: {'observation_period_num': 32, 'train_rates': 0.8031601580524226, 'learning_rate': 0.0004033396234029621, 'batch_size': 172, 'step_size': 5, 'gamma': 0.8715036575246785}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:34:36,752][0m Trial 46 finished with value: 0.0479953304034858 and parameters: {'observation_period_num': 36, 'train_rates': 0.7991193153862878, 'learning_rate': 0.00045128754749998163, 'batch_size': 186, 'step_size': 5, 'gamma': 0.8740151515643233}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:35:09,329][0m Trial 47 finished with value: 0.23053562065845803 and parameters: {'observation_period_num': 125, 'train_rates': 0.7763898896419242, 'learning_rate': 0.0004012814018816733, 'batch_size': 168, 'step_size': 4, 'gamma': 0.8966329902923177}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:39:37,943][0m Trial 48 finished with value: 0.09636264599881693 and parameters: {'observation_period_num': 72, 'train_rates': 0.8824171170734532, 'learning_rate': 0.0009919879553628563, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9085850126308448}. Best is trial 21 with value: 0.04391330227982707.[0m
[32m[I 2025-02-05 01:41:12,238][0m Trial 49 finished with value: 0.18959167073784478 and parameters: {'observation_period_num': 57, 'train_rates': 0.7458566159317439, 'learning_rate': 0.0006417314647938603, 'batch_size': 53, 'step_size': 6, 'gamma': 0.8385124952745696}. Best is trial 21 with value: 0.04391330227982707.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-05 01:41:12,249][0m A new study created in memory with name: no-name-09817b7f-bbac-48bf-8956-c4e023aaceef[0m
[32m[I 2025-02-05 01:41:47,695][0m Trial 0 finished with value: 0.34400477256622164 and parameters: {'observation_period_num': 154, 'train_rates': 0.6430110819726835, 'learning_rate': 1.1927129066538967e-05, 'batch_size': 136, 'step_size': 8, 'gamma': 0.944274184260952}. Best is trial 0 with value: 0.34400477256622164.[0m
[32m[I 2025-02-05 01:42:22,812][0m Trial 1 finished with value: 0.03165838122367859 and parameters: {'observation_period_num': 7, 'train_rates': 0.98145117244669, 'learning_rate': 0.0006894929422308206, 'batch_size': 191, 'step_size': 10, 'gamma': 0.820162543612845}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:43:23,243][0m Trial 2 finished with value: 0.05583225480384297 and parameters: {'observation_period_num': 15, 'train_rates': 0.9082039199229717, 'learning_rate': 6.928970077993943e-05, 'batch_size': 100, 'step_size': 6, 'gamma': 0.8280339559704222}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:44:00,104][0m Trial 3 finished with value: 0.2888870688370123 and parameters: {'observation_period_num': 251, 'train_rates': 0.707784258468884, 'learning_rate': 9.622836997200125e-05, 'batch_size': 131, 'step_size': 14, 'gamma': 0.8176299272345631}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:45:30,662][0m Trial 4 finished with value: 0.04874368478037995 and parameters: {'observation_period_num': 12, 'train_rates': 0.8511099928147803, 'learning_rate': 0.00021797658454244867, 'batch_size': 62, 'step_size': 15, 'gamma': 0.937518760558511}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:45:57,053][0m Trial 5 finished with value: 0.2639917846188184 and parameters: {'observation_period_num': 200, 'train_rates': 0.7461536416725816, 'learning_rate': 0.0009670348665931508, 'batch_size': 188, 'step_size': 4, 'gamma': 0.7642264256119116}. Best is trial 1 with value: 0.03165838122367859.[0m
Early stopping at epoch 95
[32m[I 2025-02-05 01:47:13,002][0m Trial 6 finished with value: 0.06864260524511337 and parameters: {'observation_period_num': 56, 'train_rates': 0.9310619768554602, 'learning_rate': 0.0008953909135764704, 'batch_size': 73, 'step_size': 1, 'gamma': 0.8528592244751934}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:49:05,948][0m Trial 7 finished with value: 0.2118440750849287 and parameters: {'observation_period_num': 32, 'train_rates': 0.8977799909376796, 'learning_rate': 3.367004445182977e-06, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8695245925142288}. Best is trial 1 with value: 0.03165838122367859.[0m
Early stopping at epoch 90
[32m[I 2025-02-05 01:49:40,360][0m Trial 8 finished with value: 0.43131023399413576 and parameters: {'observation_period_num': 192, 'train_rates': 0.6576347431939035, 'learning_rate': 0.00012570576743981273, 'batch_size': 128, 'step_size': 1, 'gamma': 0.8722501190192549}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:50:04,164][0m Trial 9 finished with value: 0.24672011130690152 and parameters: {'observation_period_num': 222, 'train_rates': 0.8964783132415213, 'learning_rate': 4.810936887140226e-05, 'batch_size': 252, 'step_size': 4, 'gamma': 0.9392322208577161}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:50:36,979][0m Trial 10 finished with value: 0.8875170946121216 and parameters: {'observation_period_num': 81, 'train_rates': 0.9657443326833753, 'learning_rate': 1.5436690386001523e-06, 'batch_size': 194, 'step_size': 11, 'gamma': 0.77782122223185}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:55:23,892][0m Trial 11 finished with value: 0.10060881687274117 and parameters: {'observation_period_num': 90, 'train_rates': 0.8315292749100327, 'learning_rate': 0.0003056085907616245, 'batch_size': 18, 'step_size': 15, 'gamma': 0.982349704638299}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:55:50,081][0m Trial 12 finished with value: 0.04003852331105557 and parameters: {'observation_period_num': 9, 'train_rates': 0.8178066438850746, 'learning_rate': 0.00030775538369644023, 'batch_size': 221, 'step_size': 11, 'gamma': 0.9206578841806595}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:56:16,933][0m Trial 13 finished with value: 0.34923303590057797 and parameters: {'observation_period_num': 112, 'train_rates': 0.7733595452294981, 'learning_rate': 1.58964362432719e-05, 'batch_size': 204, 'step_size': 11, 'gamma': 0.9089892590720879}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:56:44,147][0m Trial 14 finished with value: 0.06638500094413757 and parameters: {'observation_period_num': 52, 'train_rates': 0.9883596922101329, 'learning_rate': 0.00037817115922586547, 'batch_size': 243, 'step_size': 11, 'gamma': 0.8023606104998212}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:57:16,087][0m Trial 15 finished with value: 0.13002372813035196 and parameters: {'observation_period_num': 140, 'train_rates': 0.8213068538017742, 'learning_rate': 0.0004898245631083693, 'batch_size': 172, 'step_size': 10, 'gamma': 0.8985601596014072}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:57:37,480][0m Trial 16 finished with value: 0.22104200896891682 and parameters: {'observation_period_num': 53, 'train_rates': 0.6014645197727555, 'learning_rate': 0.000174092503519053, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8437791453521519}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:58:01,479][0m Trial 17 finished with value: 0.2887818698944458 and parameters: {'observation_period_num': 87, 'train_rates': 0.7221511837193658, 'learning_rate': 2.3170415543145117e-05, 'batch_size': 227, 'step_size': 9, 'gamma': 0.9760975515162018}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:58:35,368][0m Trial 18 finished with value: 0.05275609030535346 and parameters: {'observation_period_num': 32, 'train_rates': 0.863317793779982, 'learning_rate': 0.0005282903769994189, 'batch_size': 168, 'step_size': 13, 'gamma': 0.9011114269556523}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:59:09,544][0m Trial 19 finished with value: 0.21462235486868655 and parameters: {'observation_period_num': 6, 'train_rates': 0.7986099431825875, 'learning_rate': 7.406714507660384e-06, 'batch_size': 164, 'step_size': 12, 'gamma': 0.8047755649738886}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 01:59:39,087][0m Trial 20 finished with value: 0.20475129783153534 and parameters: {'observation_period_num': 117, 'train_rates': 0.9497154206798681, 'learning_rate': 4.2474371343954054e-05, 'batch_size': 219, 'step_size': 9, 'gamma': 0.8755302808851003}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:00:36,227][0m Trial 21 finished with value: 0.0390967724925023 and parameters: {'observation_period_num': 7, 'train_rates': 0.8581148683797056, 'learning_rate': 0.00022638486222419603, 'batch_size': 98, 'step_size': 15, 'gamma': 0.9387275542388334}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:01:30,499][0m Trial 22 finished with value: 0.094569738865373 and parameters: {'observation_period_num': 33, 'train_rates': 0.8710940357416588, 'learning_rate': 0.00022351856732702178, 'batch_size': 108, 'step_size': 13, 'gamma': 0.9609103667687832}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:02:05,063][0m Trial 23 finished with value: 0.20246898251001916 and parameters: {'observation_period_num': 61, 'train_rates': 0.7831996651136643, 'learning_rate': 0.000536886628955697, 'batch_size': 157, 'step_size': 10, 'gamma': 0.9276412416597656}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:03:04,533][0m Trial 24 finished with value: 0.05295461530814756 and parameters: {'observation_period_num': 37, 'train_rates': 0.8181806024077886, 'learning_rate': 0.0001177878357129728, 'batch_size': 91, 'step_size': 6, 'gamma': 0.919274882024775}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:03:46,362][0m Trial 25 finished with value: 0.03664766038356699 and parameters: {'observation_period_num': 8, 'train_rates': 0.933263515991125, 'learning_rate': 0.0008044066381204881, 'batch_size': 147, 'step_size': 12, 'gamma': 0.9611827243276364}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:04:31,575][0m Trial 26 finished with value: 0.12411967664957047 and parameters: {'observation_period_num': 73, 'train_rates': 0.9889132958354301, 'learning_rate': 0.0008861652329131284, 'batch_size': 148, 'step_size': 14, 'gamma': 0.9516442979061392}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:05:23,420][0m Trial 27 finished with value: 0.047495316847776754 and parameters: {'observation_period_num': 26, 'train_rates': 0.9335524078619245, 'learning_rate': 0.0006112024122847732, 'batch_size': 118, 'step_size': 14, 'gamma': 0.9580447436761764}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:06:36,990][0m Trial 28 finished with value: 0.05648755427935849 and parameters: {'observation_period_num': 46, 'train_rates': 0.9606063457698208, 'learning_rate': 0.00030372655641210615, 'batch_size': 80, 'step_size': 12, 'gamma': 0.9851796242080442}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:07:18,403][0m Trial 29 finished with value: 0.10523449124312707 and parameters: {'observation_period_num': 164, 'train_rates': 0.9166197457656177, 'learning_rate': 0.00017720477468316092, 'batch_size': 141, 'step_size': 8, 'gamma': 0.9678852371660176}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:07:49,789][0m Trial 30 finished with value: 0.12085538807215991 and parameters: {'observation_period_num': 107, 'train_rates': 0.8829420185325296, 'learning_rate': 7.151026025759838e-05, 'batch_size': 186, 'step_size': 15, 'gamma': 0.8850981155851539}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:08:20,454][0m Trial 31 finished with value: 0.038104236125946045 and parameters: {'observation_period_num': 6, 'train_rates': 0.9377495347329841, 'learning_rate': 0.00032507478639533466, 'batch_size': 206, 'step_size': 12, 'gamma': 0.920760907494047}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:08:51,293][0m Trial 32 finished with value: 0.04671433940529823 and parameters: {'observation_period_num': 23, 'train_rates': 0.9472633599078929, 'learning_rate': 0.000680369518959159, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9340004346916322}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:09:26,800][0m Trial 33 finished with value: 0.03683806476412249 and parameters: {'observation_period_num': 7, 'train_rates': 0.9172299075180953, 'learning_rate': 0.000345704827669832, 'batch_size': 178, 'step_size': 9, 'gamma': 0.94908811600044}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:10:01,730][0m Trial 34 finished with value: 0.03793361985452099 and parameters: {'observation_period_num': 20, 'train_rates': 0.9254988916210576, 'learning_rate': 0.0003962805569613204, 'batch_size': 179, 'step_size': 9, 'gamma': 0.8386326749219458}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:10:35,350][0m Trial 35 finished with value: 0.03575519306629192 and parameters: {'observation_period_num': 20, 'train_rates': 0.9135003019977557, 'learning_rate': 0.0009920670652747462, 'batch_size': 179, 'step_size': 9, 'gamma': 0.8309366067866286}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:11:23,069][0m Trial 36 finished with value: 0.04996323585510254 and parameters: {'observation_period_num': 41, 'train_rates': 0.9725393078174798, 'learning_rate': 0.0008001293850865628, 'batch_size': 132, 'step_size': 7, 'gamma': 0.81728333232437}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:12:02,769][0m Trial 37 finished with value: 0.07712460881980712 and parameters: {'observation_period_num': 65, 'train_rates': 0.9028781500113185, 'learning_rate': 0.0006551884982695373, 'batch_size': 153, 'step_size': 10, 'gamma': 0.8286132432791029}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:12:38,169][0m Trial 38 finished with value: 0.03635534459329027 and parameters: {'observation_period_num': 23, 'train_rates': 0.9133566525665016, 'learning_rate': 0.0008314934640189465, 'batch_size': 173, 'step_size': 7, 'gamma': 0.7866495714059911}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:13:22,435][0m Trial 39 finished with value: 0.04314583167433739 and parameters: {'observation_period_num': 24, 'train_rates': 0.9681959487143526, 'learning_rate': 0.0009982230994176455, 'batch_size': 142, 'step_size': 5, 'gamma': 0.7793341896852002}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:14:09,112][0m Trial 40 finished with value: 0.14052003320004489 and parameters: {'observation_period_num': 249, 'train_rates': 0.8906114558332497, 'learning_rate': 0.00048130894057678855, 'batch_size': 120, 'step_size': 7, 'gamma': 0.7530324996118358}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:14:42,309][0m Trial 41 finished with value: 0.0347258597612381 and parameters: {'observation_period_num': 19, 'train_rates': 0.9158966286877896, 'learning_rate': 0.0006740771141442951, 'batch_size': 179, 'step_size': 8, 'gamma': 0.7968203394781864}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:15:14,204][0m Trial 42 finished with value: 0.037801914934544836 and parameters: {'observation_period_num': 20, 'train_rates': 0.8453330467452249, 'learning_rate': 0.0009769418042856475, 'batch_size': 187, 'step_size': 6, 'gamma': 0.7890145288465135}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:15:50,734][0m Trial 43 finished with value: 0.052315981749077915 and parameters: {'observation_period_num': 40, 'train_rates': 0.8782416533840984, 'learning_rate': 0.0007669430906747397, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8033802474794651}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:16:21,023][0m Trial 44 finished with value: 0.48678824535668125 and parameters: {'observation_period_num': 17, 'train_rates': 0.9117586494907656, 'learning_rate': 5.26165900449918e-06, 'batch_size': 207, 'step_size': 8, 'gamma': 0.7903182132173547}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:16:56,072][0m Trial 45 finished with value: 0.09344160556793213 and parameters: {'observation_period_num': 44, 'train_rates': 0.9500099796944731, 'learning_rate': 0.00045864837831863817, 'batch_size': 181, 'step_size': 2, 'gamma': 0.8148970411604204}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:17:25,362][0m Trial 46 finished with value: 0.10303238779306412 and parameters: {'observation_period_num': 71, 'train_rates': 0.9742322441501954, 'learning_rate': 0.0006666395379472707, 'batch_size': 233, 'step_size': 10, 'gamma': 0.8538291010495658}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:17:54,435][0m Trial 47 finished with value: 0.16693194184748772 and parameters: {'observation_period_num': 160, 'train_rates': 0.8964981145891389, 'learning_rate': 0.0002408784683218201, 'batch_size': 194, 'step_size': 5, 'gamma': 0.7671437861479532}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:18:30,529][0m Trial 48 finished with value: 0.8868399592909483 and parameters: {'observation_period_num': 52, 'train_rates': 0.9304622049967213, 'learning_rate': 2.0190404945727647e-06, 'batch_size': 172, 'step_size': 8, 'gamma': 0.8266210604198074}. Best is trial 1 with value: 0.03165838122367859.[0m
[32m[I 2025-02-05 02:19:08,593][0m Trial 49 finished with value: 0.039846981676776756 and parameters: {'observation_period_num': 16, 'train_rates': 0.8485311383746932, 'learning_rate': 0.000153073237185603, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8565954495901679}. Best is trial 1 with value: 0.03165838122367859.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-05 02:19:08,603][0m A new study created in memory with name: no-name-62577074-381f-4618-a5c6-ce969189e98a[0m
[32m[I 2025-02-05 02:23:35,550][0m Trial 0 finished with value: 0.6598211137739115 and parameters: {'observation_period_num': 66, 'train_rates': 0.623765225213111, 'learning_rate': 8.650056293578892e-06, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9001294668489539}. Best is trial 0 with value: 0.6598211137739115.[0m
[32m[I 2025-02-05 02:24:47,988][0m Trial 1 finished with value: 0.09605870458735041 and parameters: {'observation_period_num': 139, 'train_rates': 0.8230816321123055, 'learning_rate': 0.00016137476620278635, 'batch_size': 72, 'step_size': 6, 'gamma': 0.9164581773100555}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:25:12,918][0m Trial 2 finished with value: 0.161117821931839 and parameters: {'observation_period_num': 104, 'train_rates': 0.9251557661407268, 'learning_rate': 8.641643446944563e-05, 'batch_size': 250, 'step_size': 7, 'gamma': 0.8084254424245444}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:26:37,188][0m Trial 3 finished with value: 0.7014979019125686 and parameters: {'observation_period_num': 230, 'train_rates': 0.7331550918823826, 'learning_rate': 4.975947840076853e-06, 'batch_size': 54, 'step_size': 5, 'gamma': 0.7586922461286805}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:27:24,030][0m Trial 4 finished with value: 0.8641425769082431 and parameters: {'observation_period_num': 83, 'train_rates': 0.7475282445755821, 'learning_rate': 3.4510813104586332e-06, 'batch_size': 111, 'step_size': 6, 'gamma': 0.8331791089573412}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:29:52,172][0m Trial 5 finished with value: 0.11991412258566085 and parameters: {'observation_period_num': 175, 'train_rates': 0.8049743746879441, 'learning_rate': 8.105702800088403e-05, 'batch_size': 33, 'step_size': 6, 'gamma': 0.8671439504737293}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:32:29,024][0m Trial 6 finished with value: 0.22270961261967637 and parameters: {'observation_period_num': 107, 'train_rates': 0.7285457522096688, 'learning_rate': 0.00010726241139050827, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8524541189008042}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:33:04,604][0m Trial 7 finished with value: 0.3488210453556123 and parameters: {'observation_period_num': 45, 'train_rates': 0.7156714914134, 'learning_rate': 1.4893109093540843e-05, 'batch_size': 139, 'step_size': 7, 'gamma': 0.8412929438464317}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:33:49,365][0m Trial 8 finished with value: 0.6505161041268412 and parameters: {'observation_period_num': 47, 'train_rates': 0.6014574274390163, 'learning_rate': 1.6499978422846237e-06, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9524170819306799}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:34:15,976][0m Trial 9 finished with value: 0.19742348790168762 and parameters: {'observation_period_num': 77, 'train_rates': 0.9452742441502535, 'learning_rate': 6.639514804398825e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.8445123195980282}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:34:50,156][0m Trial 10 finished with value: 0.2161539595048091 and parameters: {'observation_period_num': 157, 'train_rates': 0.8759031750857812, 'learning_rate': 0.000932390411118513, 'batch_size': 170, 'step_size': 10, 'gamma': 0.9850982701965055}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:36:03,277][0m Trial 11 finished with value: 0.1197508820371718 and parameters: {'observation_period_num': 168, 'train_rates': 0.8291695691333609, 'learning_rate': 0.0004009423443901903, 'batch_size': 71, 'step_size': 2, 'gamma': 0.915603647568401}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:37:06,971][0m Trial 12 finished with value: 0.12653703073449168 and parameters: {'observation_period_num': 197, 'train_rates': 0.8465589200637258, 'learning_rate': 0.00043188127746020337, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9150449162603964}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:38:20,946][0m Trial 13 finished with value: 0.10160720133804946 and parameters: {'observation_period_num': 142, 'train_rates': 0.8197429642746779, 'learning_rate': 0.00026203152063303335, 'batch_size': 71, 'step_size': 3, 'gamma': 0.9172037223021099}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:39:01,762][0m Trial 14 finished with value: 0.11222808239742732 and parameters: {'observation_period_num': 138, 'train_rates': 0.8891287415897202, 'learning_rate': 0.00024994975977173545, 'batch_size': 146, 'step_size': 4, 'gamma': 0.950256639216492}. Best is trial 1 with value: 0.09605870458735041.[0m
[32m[I 2025-02-05 02:39:39,065][0m Trial 15 finished with value: 0.06912381201982498 and parameters: {'observation_period_num': 15, 'train_rates': 0.987587952873499, 'learning_rate': 3.202210832926904e-05, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8926053556625757}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:40:14,006][0m Trial 16 finished with value: 0.0868455022573471 and parameters: {'observation_period_num': 16, 'train_rates': 0.988883345931275, 'learning_rate': 2.0226284189270904e-05, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8878251583619007}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:40:49,344][0m Trial 17 finished with value: 0.07369759678840637 and parameters: {'observation_period_num': 12, 'train_rates': 0.9897210887154474, 'learning_rate': 2.544525176605098e-05, 'batch_size': 189, 'step_size': 10, 'gamma': 0.8847798788546408}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:41:22,233][0m Trial 18 finished with value: 0.07072602212429047 and parameters: {'observation_period_num': 5, 'train_rates': 0.9896236751464716, 'learning_rate': 4.2584088240040326e-05, 'batch_size': 202, 'step_size': 11, 'gamma': 0.8034583056341827}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:41:54,008][0m Trial 19 finished with value: 0.08668655157089233 and parameters: {'observation_period_num': 6, 'train_rates': 0.9353415612847428, 'learning_rate': 4.209038009111273e-05, 'batch_size': 211, 'step_size': 13, 'gamma': 0.779925175732468}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:42:30,012][0m Trial 20 finished with value: 0.11592715978622437 and parameters: {'observation_period_num': 37, 'train_rates': 0.9621942949901876, 'learning_rate': 4.3625004331596845e-05, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8094833529719718}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:43:02,944][0m Trial 21 finished with value: 0.08437172323465347 and parameters: {'observation_period_num': 20, 'train_rates': 0.9786807034066924, 'learning_rate': 2.4211963688479977e-05, 'batch_size': 198, 'step_size': 9, 'gamma': 0.8700145064378249}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:43:40,258][0m Trial 22 finished with value: 0.2284485774162488 and parameters: {'observation_period_num': 33, 'train_rates': 0.9065861531440791, 'learning_rate': 1.0710487032087312e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.8141030780175768}. Best is trial 15 with value: 0.06912381201982498.[0m
[32m[I 2025-02-05 02:44:09,165][0m Trial 23 finished with value: 0.06123803183436394 and parameters: {'observation_period_num': 5, 'train_rates': 0.984823759767796, 'learning_rate': 3.3063650101254234e-05, 'batch_size': 224, 'step_size': 11, 'gamma': 0.8764158854594332}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:44:38,230][0m Trial 24 finished with value: 0.1581425815820694 and parameters: {'observation_period_num': 61, 'train_rates': 0.9507943106952182, 'learning_rate': 4.673980745927861e-05, 'batch_size': 228, 'step_size': 12, 'gamma': 0.7886817481098775}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:45:04,060][0m Trial 25 finished with value: 0.1343709381744843 and parameters: {'observation_period_num': 6, 'train_rates': 0.8686219284184036, 'learning_rate': 6.89226149365282e-06, 'batch_size': 253, 'step_size': 11, 'gamma': 0.9418850272276741}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:45:33,265][0m Trial 26 finished with value: 0.153920691702273 and parameters: {'observation_period_num': 31, 'train_rates': 0.9208939879171617, 'learning_rate': 1.4480904545305593e-05, 'batch_size': 216, 'step_size': 9, 'gamma': 0.8634487771323871}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:46:09,386][0m Trial 27 finished with value: 0.2685775048675991 and parameters: {'observation_period_num': 105, 'train_rates': 0.7789251781350337, 'learning_rate': 3.370113309659021e-05, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8289870330162777}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:46:50,511][0m Trial 28 finished with value: 0.1805140588528894 and parameters: {'observation_period_num': 58, 'train_rates': 0.6792745134513457, 'learning_rate': 0.0001226337618669378, 'batch_size': 122, 'step_size': 8, 'gamma': 0.7546727166773399}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:47:17,471][0m Trial 29 finished with value: 0.4140057861804962 and parameters: {'observation_period_num': 72, 'train_rates': 0.9621071966057051, 'learning_rate': 8.917799707348929e-06, 'batch_size': 237, 'step_size': 12, 'gamma': 0.8875133827485628}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:47:44,835][0m Trial 30 finished with value: 0.24307133835643085 and parameters: {'observation_period_num': 251, 'train_rates': 0.9007757044539421, 'learning_rate': 5.429900759361348e-05, 'batch_size': 211, 'step_size': 11, 'gamma': 0.8973423191235018}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:48:19,782][0m Trial 31 finished with value: 0.0881168469786644 and parameters: {'observation_period_num': 22, 'train_rates': 0.9805032408761014, 'learning_rate': 2.5277964364957575e-05, 'batch_size': 189, 'step_size': 10, 'gamma': 0.8787593615401732}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:48:53,462][0m Trial 32 finished with value: 0.08611705154180527 and parameters: {'observation_period_num': 10, 'train_rates': 0.9844563040565106, 'learning_rate': 1.6989366141759784e-05, 'batch_size': 193, 'step_size': 10, 'gamma': 0.9019748638638889}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:49:28,653][0m Trial 33 finished with value: 0.07832656055688858 and parameters: {'observation_period_num': 27, 'train_rates': 0.9593609614123859, 'learning_rate': 3.04544460441417e-05, 'batch_size': 177, 'step_size': 8, 'gamma': 0.9354196139566074}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:50:06,834][0m Trial 34 finished with value: 0.07406434829151907 and parameters: {'observation_period_num': 52, 'train_rates': 0.9261575396075539, 'learning_rate': 6.135446444913092e-05, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8577417924404989}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:50:34,172][0m Trial 35 finished with value: 0.42355653643608093 and parameters: {'observation_period_num': 39, 'train_rates': 0.9379310098154012, 'learning_rate': 4.612105691808977e-06, 'batch_size': 240, 'step_size': 11, 'gamma': 0.879529641851466}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:51:05,690][0m Trial 36 finished with value: 0.13620968163013458 and parameters: {'observation_period_num': 90, 'train_rates': 0.9896402829416644, 'learning_rate': 0.0001480836559027927, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8998176705452124}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:51:55,776][0m Trial 37 finished with value: 0.0637887567281723 and parameters: {'observation_period_num': 18, 'train_rates': 0.9670432116434632, 'learning_rate': 8.158780472268531e-05, 'batch_size': 127, 'step_size': 12, 'gamma': 0.7931388539064101}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:52:43,696][0m Trial 38 finished with value: 0.09464477977404992 and parameters: {'observation_period_num': 91, 'train_rates': 0.9161637575343639, 'learning_rate': 6.619850746100423e-05, 'batch_size': 122, 'step_size': 14, 'gamma': 0.7803709099874877}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:53:32,677][0m Trial 39 finished with value: 0.14099112153053284 and parameters: {'observation_period_num': 117, 'train_rates': 0.9643053719532277, 'learning_rate': 0.00021257609424492647, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8217565021679624}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:54:23,042][0m Trial 40 finished with value: 0.1776799877042785 and parameters: {'observation_period_num': 24, 'train_rates': 0.7775410442974552, 'learning_rate': 0.00011819088988588829, 'batch_size': 108, 'step_size': 14, 'gamma': 0.7961791312626506}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:54:56,590][0m Trial 41 finished with value: 0.0805140808224678 and parameters: {'observation_period_num': 8, 'train_rates': 0.9697306326270683, 'learning_rate': 8.708184553170548e-05, 'batch_size': 198, 'step_size': 9, 'gamma': 0.7673615067887003}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:55:36,496][0m Trial 42 finished with value: 0.0815662051240603 and parameters: {'observation_period_num': 42, 'train_rates': 0.9433290401382586, 'learning_rate': 3.1362206848144376e-05, 'batch_size': 154, 'step_size': 12, 'gamma': 0.8526741960004685}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:56:11,703][0m Trial 43 finished with value: 0.17097994685173035 and parameters: {'observation_period_num': 18, 'train_rates': 0.9524888944585075, 'learning_rate': 1.1278892236364181e-05, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8021840783699747}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:56:46,869][0m Trial 44 finished with value: 0.8599104615358206 and parameters: {'observation_period_num': 49, 'train_rates': 0.6419208805343513, 'learning_rate': 2.2334101487772277e-06, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8354294896283244}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:57:16,376][0m Trial 45 finished with value: 0.19780980050563812 and parameters: {'observation_period_num': 199, 'train_rates': 0.9732470013418056, 'learning_rate': 8.554282837623619e-05, 'batch_size': 222, 'step_size': 10, 'gamma': 0.8455271031021483}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:57:46,069][0m Trial 46 finished with value: 0.13627420365810394 and parameters: {'observation_period_num': 16, 'train_rates': 0.989765831009482, 'learning_rate': 2.098561533809802e-05, 'batch_size': 240, 'step_size': 7, 'gamma': 0.8725621078044632}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:58:18,420][0m Trial 47 finished with value: 0.06999117916167094 and parameters: {'observation_period_num': 30, 'train_rates': 0.8694202575198093, 'learning_rate': 3.473865672552626e-05, 'batch_size': 184, 'step_size': 13, 'gamma': 0.9239647438303411}. Best is trial 23 with value: 0.06123803183436394.[0m
[32m[I 2025-02-05 02:59:18,170][0m Trial 48 finished with value: 0.05713221862127906 and parameters: {'observation_period_num': 68, 'train_rates': 0.8550319420655299, 'learning_rate': 4.297475997116145e-05, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9272863896840712}. Best is trial 48 with value: 0.05713221862127906.[0m
[32m[I 2025-02-05 03:01:12,750][0m Trial 49 finished with value: 0.10325129842160692 and parameters: {'observation_period_num': 72, 'train_rates': 0.8565218525422735, 'learning_rate': 7.806578637278519e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.978095770756573}. Best is trial 48 with value: 0.05713221862127906.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-05 03:01:12,760][0m A new study created in memory with name: no-name-f7879248-b1b9-484f-9a8b-227a8a106f13[0m
[32m[I 2025-02-05 03:01:39,828][0m Trial 0 finished with value: 0.2395492047071457 and parameters: {'observation_period_num': 154, 'train_rates': 0.9760782301640625, 'learning_rate': 0.00011278001610592038, 'batch_size': 246, 'step_size': 14, 'gamma': 0.8264572745732484}. Best is trial 0 with value: 0.2395492047071457.[0m
[32m[I 2025-02-05 03:02:08,470][0m Trial 1 finished with value: 0.22073317896199707 and parameters: {'observation_period_num': 103, 'train_rates': 0.7384429163500907, 'learning_rate': 0.0004948734035166335, 'batch_size': 184, 'step_size': 6, 'gamma': 0.7980807601688705}. Best is trial 1 with value: 0.22073317896199707.[0m
[32m[I 2025-02-05 03:04:20,008][0m Trial 2 finished with value: 0.08897251609957567 and parameters: {'observation_period_num': 48, 'train_rates': 0.9056643697517771, 'learning_rate': 1.4363837094597675e-05, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8085353460564246}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:05:19,619][0m Trial 3 finished with value: 0.4365460239652032 and parameters: {'observation_period_num': 99, 'train_rates': 0.8702459477466563, 'learning_rate': 3.484883384884238e-06, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9110049855001323}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:05:48,520][0m Trial 4 finished with value: 0.1863439679145813 and parameters: {'observation_period_num': 113, 'train_rates': 0.980331943475966, 'learning_rate': 0.0002524964601465372, 'batch_size': 221, 'step_size': 14, 'gamma': 0.9016100815313592}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:08:01,082][0m Trial 5 finished with value: 0.10739814815763941 and parameters: {'observation_period_num': 76, 'train_rates': 0.9420886036515908, 'learning_rate': 1.741332076640938e-05, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9513131460412564}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:08:44,567][0m Trial 6 finished with value: 1.118976354598999 and parameters: {'observation_period_num': 80, 'train_rates': 0.9656604733702573, 'learning_rate': 2.5801143562076748e-06, 'batch_size': 142, 'step_size': 9, 'gamma': 0.8417557972114673}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:09:11,396][0m Trial 7 finished with value: 0.368906170129776 and parameters: {'observation_period_num': 242, 'train_rates': 0.919911895155568, 'learning_rate': 0.0003038513435911882, 'batch_size': 242, 'step_size': 1, 'gamma': 0.8862243877732453}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:09:38,644][0m Trial 8 finished with value: 0.8573116660118103 and parameters: {'observation_period_num': 169, 'train_rates': 0.9368880428009339, 'learning_rate': 1.735513065930949e-06, 'batch_size': 233, 'step_size': 2, 'gamma': 0.9864970205463454}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:09:59,551][0m Trial 9 finished with value: 0.4305812327398194 and parameters: {'observation_period_num': 218, 'train_rates': 0.6511722239776753, 'learning_rate': 0.0001416763696078276, 'batch_size': 241, 'step_size': 2, 'gamma': 0.8221149151925953}. Best is trial 2 with value: 0.08897251609957567.[0m
[32m[I 2025-02-05 03:12:04,655][0m Trial 10 finished with value: 0.06370778237139897 and parameters: {'observation_period_num': 6, 'train_rates': 0.8129695840170659, 'learning_rate': 1.7174917734621487e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.7662723298851712}. Best is trial 10 with value: 0.06370778237139897.[0m
[32m[I 2025-02-05 03:16:25,289][0m Trial 11 finished with value: 0.04601255250318024 and parameters: {'observation_period_num': 10, 'train_rates': 0.8157473504185414, 'learning_rate': 1.6370120882291116e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7503533547202111}. Best is trial 11 with value: 0.04601255250318024.[0m
[32m[I 2025-02-05 03:21:47,884][0m Trial 12 finished with value: 0.0352304901380639 and parameters: {'observation_period_num': 7, 'train_rates': 0.8058462956656766, 'learning_rate': 3.6010324688498536e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7560436034379239}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:26:27,879][0m Trial 13 finished with value: 0.15837293783585057 and parameters: {'observation_period_num': 12, 'train_rates': 0.7665247216306889, 'learning_rate': 6.285955721294568e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7536731221136581}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:27:38,263][0m Trial 14 finished with value: 0.20580007331293138 and parameters: {'observation_period_num': 42, 'train_rates': 0.8324361161392031, 'learning_rate': 6.610778387308845e-06, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7784686822684296}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:28:27,516][0m Trial 15 finished with value: 0.18280026059850907 and parameters: {'observation_period_num': 41, 'train_rates': 0.7168799233791058, 'learning_rate': 4.411661297039121e-05, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8564043621408288}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:32:21,481][0m Trial 16 finished with value: 0.17821853729964648 and parameters: {'observation_period_num': 26, 'train_rates': 0.6977488933389676, 'learning_rate': 7.213480892941667e-06, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7509749823667847}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:33:04,284][0m Trial 17 finished with value: 0.15617292327806354 and parameters: {'observation_period_num': 66, 'train_rates': 0.8503118875237279, 'learning_rate': 2.9240839012458333e-05, 'batch_size': 135, 'step_size': 7, 'gamma': 0.7889533652526886}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:34:14,350][0m Trial 18 finished with value: 0.25305381204400745 and parameters: {'observation_period_num': 143, 'train_rates': 0.7832703633141063, 'learning_rate': 0.0008628119343968318, 'batch_size': 72, 'step_size': 10, 'gamma': 0.780994657293225}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:34:51,893][0m Trial 19 finished with value: 0.6402313380090525 and parameters: {'observation_period_num': 184, 'train_rates': 0.6232275606650008, 'learning_rate': 8.181135567252262e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8652530711409614}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:35:26,177][0m Trial 20 finished with value: 0.3208728088065982 and parameters: {'observation_period_num': 60, 'train_rates': 0.8784437147177285, 'learning_rate': 6.816846731061129e-06, 'batch_size': 170, 'step_size': 5, 'gamma': 0.927808016355124}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:37:13,742][0m Trial 21 finished with value: 0.06139114976870969 and parameters: {'observation_period_num': 5, 'train_rates': 0.8174306365204616, 'learning_rate': 2.5478836191057896e-05, 'batch_size': 50, 'step_size': 11, 'gamma': 0.7676821269820888}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:38:56,968][0m Trial 22 finished with value: 0.05427295996187191 and parameters: {'observation_period_num': 5, 'train_rates': 0.811256422751868, 'learning_rate': 2.9339297016136856e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.7748268761272944}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:44:10,487][0m Trial 23 finished with value: 0.17867622004281838 and parameters: {'observation_period_num': 33, 'train_rates': 0.7683374005245637, 'learning_rate': 4.974725621117759e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8001429302332461}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:45:25,492][0m Trial 24 finished with value: 0.2679501757839231 and parameters: {'observation_period_num': 23, 'train_rates': 0.7436894330983033, 'learning_rate': 1.1377227958614973e-05, 'batch_size': 66, 'step_size': 10, 'gamma': 0.7592813252767978}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:46:57,807][0m Trial 25 finished with value: 0.06740397019230801 and parameters: {'observation_period_num': 55, 'train_rates': 0.8014666827632535, 'learning_rate': 3.187039611730784e-05, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8151580146394514}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:50:22,916][0m Trial 26 finished with value: 0.23211995819435033 and parameters: {'observation_period_num': 24, 'train_rates': 0.8501991843486805, 'learning_rate': 1.1380707237741347e-06, 'batch_size': 26, 'step_size': 12, 'gamma': 0.7802949653840658}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:53:08,958][0m Trial 27 finished with value: 0.0939913540727125 and parameters: {'observation_period_num': 88, 'train_rates': 0.8891370597031855, 'learning_rate': 0.0001341052930973305, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8474200961379206}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:54:01,155][0m Trial 28 finished with value: 0.5876264780932079 and parameters: {'observation_period_num': 124, 'train_rates': 0.6943509946501639, 'learning_rate': 4.139471334693102e-06, 'batch_size': 94, 'step_size': 13, 'gamma': 0.8333948003454911}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:55:33,137][0m Trial 29 finished with value: 0.03852180680932571 and parameters: {'observation_period_num': 18, 'train_rates': 0.8443850516495482, 'learning_rate': 9.936270331118426e-05, 'batch_size': 60, 'step_size': 14, 'gamma': 0.7665782470456104}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:56:43,735][0m Trial 30 finished with value: 0.04442451865504228 and parameters: {'observation_period_num': 26, 'train_rates': 0.8408254434663543, 'learning_rate': 8.464385486939017e-05, 'batch_size': 77, 'step_size': 14, 'gamma': 0.7979471964313217}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:57:53,858][0m Trial 31 finished with value: 0.04366416275568525 and parameters: {'observation_period_num': 25, 'train_rates': 0.8488449869052825, 'learning_rate': 9.421789638204298e-05, 'batch_size': 79, 'step_size': 14, 'gamma': 0.7520569237693209}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:59:02,803][0m Trial 32 finished with value: 0.04537276754400513 and parameters: {'observation_period_num': 32, 'train_rates': 0.8462273530341372, 'learning_rate': 9.0822474652378e-05, 'batch_size': 80, 'step_size': 14, 'gamma': 0.7955057733564179}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 03:59:52,185][0m Trial 33 finished with value: 0.05041822160810533 and parameters: {'observation_period_num': 49, 'train_rates': 0.8637723870199574, 'learning_rate': 0.0001958456867785119, 'batch_size': 115, 'step_size': 15, 'gamma': 0.8096078965505711}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:00:55,762][0m Trial 34 finished with value: 0.08060808804811247 and parameters: {'observation_period_num': 68, 'train_rates': 0.8969568620563814, 'learning_rate': 0.0004499742666263763, 'batch_size': 90, 'step_size': 14, 'gamma': 0.7677499832168979}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:02:26,163][0m Trial 35 finished with value: 0.1856715304831278 and parameters: {'observation_period_num': 21, 'train_rates': 0.7890883321981284, 'learning_rate': 6.779471710600513e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.7874547981230327}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:03:02,028][0m Trial 36 finished with value: 0.09833199205578014 and parameters: {'observation_period_num': 97, 'train_rates': 0.8373794304767495, 'learning_rate': 0.00011746926876650955, 'batch_size': 163, 'step_size': 14, 'gamma': 0.802140713079497}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:05:36,460][0m Trial 37 finished with value: 0.060037820232854346 and parameters: {'observation_period_num': 43, 'train_rates': 0.9174493852203083, 'learning_rate': 0.0002088940314096611, 'batch_size': 37, 'step_size': 15, 'gamma': 0.7661383723500813}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:06:06,342][0m Trial 38 finished with value: 0.1432233087503378 and parameters: {'observation_period_num': 77, 'train_rates': 0.8675052575327342, 'learning_rate': 4.096075571175178e-05, 'batch_size': 200, 'step_size': 14, 'gamma': 0.8286734939032289}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:06:56,823][0m Trial 39 finished with value: 0.18848913899576503 and parameters: {'observation_period_num': 36, 'train_rates': 0.7728304147595395, 'learning_rate': 0.00037159936661645146, 'batch_size': 105, 'step_size': 13, 'gamma': 0.8857506823089273}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:08:16,355][0m Trial 40 finished with value: 0.18059885871264483 and parameters: {'observation_period_num': 53, 'train_rates': 0.753361380696172, 'learning_rate': 0.00017116250860480542, 'batch_size': 63, 'step_size': 10, 'gamma': 0.7870444605011554}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:09:25,959][0m Trial 41 finished with value: 0.047941053550132375 and parameters: {'observation_period_num': 24, 'train_rates': 0.8313783753914444, 'learning_rate': 7.765258974809046e-05, 'batch_size': 81, 'step_size': 14, 'gamma': 0.7931133329453155}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:10:38,094][0m Trial 42 finished with value: 0.04554650289091197 and parameters: {'observation_period_num': 18, 'train_rates': 0.8538832848156571, 'learning_rate': 8.929219707411138e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.7717907601186063}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:11:23,870][0m Trial 43 finished with value: 0.054193317628743354 and parameters: {'observation_period_num': 34, 'train_rates': 0.8828470879206332, 'learning_rate': 0.00010506880552784128, 'batch_size': 131, 'step_size': 15, 'gamma': 0.8178736847793342}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:12:31,259][0m Trial 44 finished with value: 0.05766852405564538 and parameters: {'observation_period_num': 31, 'train_rates': 0.9407051078186597, 'learning_rate': 6.0461769914979484e-05, 'batch_size': 88, 'step_size': 14, 'gamma': 0.7600633025950383}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:13:27,877][0m Trial 45 finished with value: 0.07347566940223993 and parameters: {'observation_period_num': 64, 'train_rates': 0.9073948251137662, 'learning_rate': 0.00031785390274303055, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8031900426330196}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:15:34,081][0m Trial 46 finished with value: 0.10055611843374863 and parameters: {'observation_period_num': 112, 'train_rates': 0.8356430520872827, 'learning_rate': 0.0001605887610987591, 'batch_size': 41, 'step_size': 14, 'gamma': 0.7591798839496535}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:16:49,528][0m Trial 47 finished with value: 0.174709656006418 and parameters: {'observation_period_num': 16, 'train_rates': 0.7940144824213813, 'learning_rate': 0.0002496677497165705, 'batch_size': 69, 'step_size': 15, 'gamma': 0.9783823352804281}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:17:34,578][0m Trial 48 finished with value: 0.3063057940683247 and parameters: {'observation_period_num': 184, 'train_rates': 0.8246268669455217, 'learning_rate': 2.1447609675900785e-05, 'batch_size': 115, 'step_size': 8, 'gamma': 0.7947122880366022}. Best is trial 12 with value: 0.0352304901380639.[0m
[32m[I 2025-02-05 04:20:42,668][0m Trial 49 finished with value: 0.06509712550789118 and parameters: {'observation_period_num': 44, 'train_rates': 0.9658123539762942, 'learning_rate': 4.150056503631948e-05, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7511109704304502}. Best is trial 12 with value: 0.0352304901380639.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.8634069975137922, 'learning_rate': 0.0009757295617322969, 'batch_size': 92, 'step_size': 15, 'gamma': 0.753672879705485}
Epoch 1/300, trend Loss: 0.4028 | 0.1735
Epoch 2/300, trend Loss: 0.1338 | 0.0933
Epoch 3/300, trend Loss: 0.1192 | 0.1048
Epoch 4/300, trend Loss: 0.1157 | 0.1273
Epoch 5/300, trend Loss: 0.1247 | 0.0955
Epoch 6/300, trend Loss: 0.1132 | 0.0638
Epoch 7/300, trend Loss: 0.1019 | 0.0648
Epoch 8/300, trend Loss: 0.1020 | 0.0657
Epoch 9/300, trend Loss: 0.0967 | 0.0605
Epoch 10/300, trend Loss: 0.1006 | 0.0607
Epoch 11/300, trend Loss: 0.0964 | 0.0593
Epoch 12/300, trend Loss: 0.0940 | 0.0644
Epoch 13/300, trend Loss: 0.0990 | 0.0806
Epoch 14/300, trend Loss: 0.0934 | 0.0915
Epoch 15/300, trend Loss: 0.0935 | 0.1029
Epoch 16/300, trend Loss: 0.0856 | 0.0557
Epoch 17/300, trend Loss: 0.0933 | 0.1339
Epoch 18/300, trend Loss: 0.1000 | 0.0681
Epoch 19/300, trend Loss: 0.0901 | 0.0481
Epoch 20/300, trend Loss: 0.0837 | 0.0420
Epoch 21/300, trend Loss: 0.0829 | 0.0409
Epoch 22/300, trend Loss: 0.0826 | 0.0410
Epoch 23/300, trend Loss: 0.0883 | 0.0480
Epoch 24/300, trend Loss: 0.0864 | 0.0385
Epoch 25/300, trend Loss: 0.0772 | 0.0388
Epoch 26/300, trend Loss: 0.0774 | 0.0396
Epoch 27/300, trend Loss: 0.0764 | 0.0397
Epoch 28/300, trend Loss: 0.0756 | 0.0392
Epoch 29/300, trend Loss: 0.0754 | 0.0383
Epoch 30/300, trend Loss: 0.0759 | 0.0392
Epoch 31/300, trend Loss: 0.0761 | 0.0384
Epoch 32/300, trend Loss: 0.0760 | 0.0403
Epoch 33/300, trend Loss: 0.0730 | 0.0365
Epoch 34/300, trend Loss: 0.0719 | 0.0347
Epoch 35/300, trend Loss: 0.0719 | 0.0351
Epoch 36/300, trend Loss: 0.0721 | 0.0352
Epoch 37/300, trend Loss: 0.0714 | 0.0347
Epoch 38/300, trend Loss: 0.0706 | 0.0344
Epoch 39/300, trend Loss: 0.0705 | 0.0346
Epoch 40/300, trend Loss: 0.0708 | 0.0344
Epoch 41/300, trend Loss: 0.0710 | 0.0346
Epoch 42/300, trend Loss: 0.0717 | 0.0351
Epoch 43/300, trend Loss: 0.0716 | 0.0350
Epoch 44/300, trend Loss: 0.0708 | 0.0349
Epoch 45/300, trend Loss: 0.0696 | 0.0346
Epoch 46/300, trend Loss: 0.0688 | 0.0344
Epoch 47/300, trend Loss: 0.0695 | 0.0388
Epoch 48/300, trend Loss: 0.0707 | 0.0434
Epoch 49/300, trend Loss: 0.0701 | 0.0428
Epoch 50/300, trend Loss: 0.0685 | 0.0380
Epoch 51/300, trend Loss: 0.0669 | 0.0349
Epoch 52/300, trend Loss: 0.0661 | 0.0337
Epoch 53/300, trend Loss: 0.0656 | 0.0333
Epoch 54/300, trend Loss: 0.0656 | 0.0332
Epoch 55/300, trend Loss: 0.0658 | 0.0335
Epoch 56/300, trend Loss: 0.0659 | 0.0333
Epoch 57/300, trend Loss: 0.0656 | 0.0338
Epoch 58/300, trend Loss: 0.0655 | 0.0342
Epoch 59/300, trend Loss: 0.0655 | 0.0340
Epoch 60/300, trend Loss: 0.0657 | 0.0339
Epoch 61/300, trend Loss: 0.0656 | 0.0344
Epoch 62/300, trend Loss: 0.0663 | 0.0341
Epoch 63/300, trend Loss: 0.0660 | 0.0329
Epoch 64/300, trend Loss: 0.0653 | 0.0324
Epoch 65/300, trend Loss: 0.0649 | 0.0325
Epoch 66/300, trend Loss: 0.0646 | 0.0322
Epoch 67/300, trend Loss: 0.0641 | 0.0318
Epoch 68/300, trend Loss: 0.0635 | 0.0313
Epoch 69/300, trend Loss: 0.0631 | 0.0307
Epoch 70/300, trend Loss: 0.0630 | 0.0308
Epoch 71/300, trend Loss: 0.0629 | 0.0309
Epoch 72/300, trend Loss: 0.0628 | 0.0307
Epoch 73/300, trend Loss: 0.0627 | 0.0306
Epoch 74/300, trend Loss: 0.0626 | 0.0305
Epoch 75/300, trend Loss: 0.0626 | 0.0305
Epoch 76/300, trend Loss: 0.0629 | 0.0317
Epoch 77/300, trend Loss: 0.0631 | 0.0315
Epoch 78/300, trend Loss: 0.0624 | 0.0311
Epoch 79/300, trend Loss: 0.0621 | 0.0308
Epoch 80/300, trend Loss: 0.0623 | 0.0310
Epoch 81/300, trend Loss: 0.0623 | 0.0309
Epoch 82/300, trend Loss: 0.0622 | 0.0308
Epoch 83/300, trend Loss: 0.0622 | 0.0306
Epoch 84/300, trend Loss: 0.0622 | 0.0302
Epoch 85/300, trend Loss: 0.0624 | 0.0300
Epoch 86/300, trend Loss: 0.0637 | 0.0310
Epoch 87/300, trend Loss: 0.0621 | 0.0295
Epoch 88/300, trend Loss: 0.0615 | 0.0295
Epoch 89/300, trend Loss: 0.0612 | 0.0295
Epoch 90/300, trend Loss: 0.0609 | 0.0294
Epoch 91/300, trend Loss: 0.0607 | 0.0295
Epoch 92/300, trend Loss: 0.0605 | 0.0294
Epoch 93/300, trend Loss: 0.0605 | 0.0293
Epoch 94/300, trend Loss: 0.0604 | 0.0293
Epoch 95/300, trend Loss: 0.0605 | 0.0294
Epoch 96/300, trend Loss: 0.0605 | 0.0296
Epoch 97/300, trend Loss: 0.0605 | 0.0296
Epoch 98/300, trend Loss: 0.0604 | 0.0295
Epoch 99/300, trend Loss: 0.0602 | 0.0293
Epoch 100/300, trend Loss: 0.0601 | 0.0292
Epoch 101/300, trend Loss: 0.0601 | 0.0292
Epoch 102/300, trend Loss: 0.0603 | 0.0291
Epoch 103/300, trend Loss: 0.0603 | 0.0290
Epoch 104/300, trend Loss: 0.0601 | 0.0289
Epoch 105/300, trend Loss: 0.0599 | 0.0289
Epoch 106/300, trend Loss: 0.0597 | 0.0290
Epoch 107/300, trend Loss: 0.0596 | 0.0289
Epoch 108/300, trend Loss: 0.0596 | 0.0289
Epoch 109/300, trend Loss: 0.0596 | 0.0289
Epoch 110/300, trend Loss: 0.0595 | 0.0289
Epoch 111/300, trend Loss: 0.0595 | 0.0288
Epoch 112/300, trend Loss: 0.0594 | 0.0288
Epoch 113/300, trend Loss: 0.0594 | 0.0287
Epoch 114/300, trend Loss: 0.0594 | 0.0287
Epoch 115/300, trend Loss: 0.0593 | 0.0287
Epoch 116/300, trend Loss: 0.0593 | 0.0287
Epoch 117/300, trend Loss: 0.0593 | 0.0286
Epoch 118/300, trend Loss: 0.0592 | 0.0286
Epoch 119/300, trend Loss: 0.0592 | 0.0286
Epoch 120/300, trend Loss: 0.0592 | 0.0286
Epoch 121/300, trend Loss: 0.0592 | 0.0286
Epoch 122/300, trend Loss: 0.0591 | 0.0286
Epoch 123/300, trend Loss: 0.0591 | 0.0286
Epoch 124/300, trend Loss: 0.0591 | 0.0285
Epoch 125/300, trend Loss: 0.0591 | 0.0285
Epoch 126/300, trend Loss: 0.0591 | 0.0285
Epoch 127/300, trend Loss: 0.0590 | 0.0285
Epoch 128/300, trend Loss: 0.0590 | 0.0285
Epoch 129/300, trend Loss: 0.0590 | 0.0285
Epoch 130/300, trend Loss: 0.0590 | 0.0285
Epoch 131/300, trend Loss: 0.0590 | 0.0285
Epoch 132/300, trend Loss: 0.0590 | 0.0284
Epoch 133/300, trend Loss: 0.0589 | 0.0284
Epoch 134/300, trend Loss: 0.0589 | 0.0284
Epoch 135/300, trend Loss: 0.0589 | 0.0284
Epoch 136/300, trend Loss: 0.0589 | 0.0284
Epoch 137/300, trend Loss: 0.0589 | 0.0284
Epoch 138/300, trend Loss: 0.0589 | 0.0284
Epoch 139/300, trend Loss: 0.0589 | 0.0284
Epoch 140/300, trend Loss: 0.0588 | 0.0284
Epoch 141/300, trend Loss: 0.0588 | 0.0284
Epoch 142/300, trend Loss: 0.0588 | 0.0284
Epoch 143/300, trend Loss: 0.0588 | 0.0284
Epoch 144/300, trend Loss: 0.0588 | 0.0283
Epoch 145/300, trend Loss: 0.0588 | 0.0283
Epoch 146/300, trend Loss: 0.0588 | 0.0283
Epoch 147/300, trend Loss: 0.0588 | 0.0283
Epoch 148/300, trend Loss: 0.0588 | 0.0283
Epoch 149/300, trend Loss: 0.0587 | 0.0283
Epoch 150/300, trend Loss: 0.0587 | 0.0283
Epoch 151/300, trend Loss: 0.0587 | 0.0283
Epoch 152/300, trend Loss: 0.0587 | 0.0283
Epoch 153/300, trend Loss: 0.0587 | 0.0283
Epoch 154/300, trend Loss: 0.0587 | 0.0283
Epoch 155/300, trend Loss: 0.0587 | 0.0283
Epoch 156/300, trend Loss: 0.0587 | 0.0283
Epoch 157/300, trend Loss: 0.0587 | 0.0283
Epoch 158/300, trend Loss: 0.0587 | 0.0283
Epoch 159/300, trend Loss: 0.0587 | 0.0283
Epoch 160/300, trend Loss: 0.0587 | 0.0283
Epoch 161/300, trend Loss: 0.0587 | 0.0283
Epoch 162/300, trend Loss: 0.0587 | 0.0283
Epoch 163/300, trend Loss: 0.0586 | 0.0283
Epoch 164/300, trend Loss: 0.0586 | 0.0283
Epoch 165/300, trend Loss: 0.0586 | 0.0282
Epoch 166/300, trend Loss: 0.0586 | 0.0282
Epoch 167/300, trend Loss: 0.0586 | 0.0282
Epoch 168/300, trend Loss: 0.0586 | 0.0282
Epoch 169/300, trend Loss: 0.0586 | 0.0282
Epoch 170/300, trend Loss: 0.0586 | 0.0282
Epoch 171/300, trend Loss: 0.0586 | 0.0282
Epoch 172/300, trend Loss: 0.0586 | 0.0282
Epoch 173/300, trend Loss: 0.0586 | 0.0282
Epoch 174/300, trend Loss: 0.0586 | 0.0282
Epoch 175/300, trend Loss: 0.0586 | 0.0282
Epoch 176/300, trend Loss: 0.0586 | 0.0282
Epoch 177/300, trend Loss: 0.0586 | 0.0282
Epoch 178/300, trend Loss: 0.0586 | 0.0282
Epoch 179/300, trend Loss: 0.0586 | 0.0282
Epoch 180/300, trend Loss: 0.0586 | 0.0282
Epoch 181/300, trend Loss: 0.0586 | 0.0282
Epoch 182/300, trend Loss: 0.0586 | 0.0282
Epoch 183/300, trend Loss: 0.0586 | 0.0282
Epoch 184/300, trend Loss: 0.0586 | 0.0282
Epoch 185/300, trend Loss: 0.0586 | 0.0282
Epoch 186/300, trend Loss: 0.0586 | 0.0282
Epoch 187/300, trend Loss: 0.0586 | 0.0282
Epoch 188/300, trend Loss: 0.0586 | 0.0282
Epoch 189/300, trend Loss: 0.0585 | 0.0282
Epoch 190/300, trend Loss: 0.0585 | 0.0282
Epoch 191/300, trend Loss: 0.0585 | 0.0282
Epoch 192/300, trend Loss: 0.0585 | 0.0282
Epoch 193/300, trend Loss: 0.0585 | 0.0282
Epoch 194/300, trend Loss: 0.0585 | 0.0282
Epoch 195/300, trend Loss: 0.0585 | 0.0282
Epoch 196/300, trend Loss: 0.0585 | 0.0282
Epoch 197/300, trend Loss: 0.0585 | 0.0282
Epoch 198/300, trend Loss: 0.0585 | 0.0282
Epoch 199/300, trend Loss: 0.0585 | 0.0282
Epoch 200/300, trend Loss: 0.0585 | 0.0282
Epoch 201/300, trend Loss: 0.0585 | 0.0282
Epoch 202/300, trend Loss: 0.0585 | 0.0282
Epoch 203/300, trend Loss: 0.0585 | 0.0282
Epoch 204/300, trend Loss: 0.0585 | 0.0282
Epoch 205/300, trend Loss: 0.0585 | 0.0282
Epoch 206/300, trend Loss: 0.0585 | 0.0282
Epoch 207/300, trend Loss: 0.0585 | 0.0282
Epoch 208/300, trend Loss: 0.0585 | 0.0282
Epoch 209/300, trend Loss: 0.0585 | 0.0282
Epoch 210/300, trend Loss: 0.0585 | 0.0282
Epoch 211/300, trend Loss: 0.0585 | 0.0282
Epoch 212/300, trend Loss: 0.0585 | 0.0282
Epoch 213/300, trend Loss: 0.0585 | 0.0282
Epoch 214/300, trend Loss: 0.0585 | 0.0282
Epoch 215/300, trend Loss: 0.0585 | 0.0282
Epoch 216/300, trend Loss: 0.0585 | 0.0282
Epoch 217/300, trend Loss: 0.0585 | 0.0282
Epoch 218/300, trend Loss: 0.0585 | 0.0282
Epoch 219/300, trend Loss: 0.0585 | 0.0282
Epoch 220/300, trend Loss: 0.0585 | 0.0282
Epoch 221/300, trend Loss: 0.0585 | 0.0282
Epoch 222/300, trend Loss: 0.0585 | 0.0282
Epoch 223/300, trend Loss: 0.0585 | 0.0282
Epoch 224/300, trend Loss: 0.0585 | 0.0282
Epoch 225/300, trend Loss: 0.0585 | 0.0282
Epoch 226/300, trend Loss: 0.0585 | 0.0282
Epoch 227/300, trend Loss: 0.0585 | 0.0282
Epoch 228/300, trend Loss: 0.0585 | 0.0282
Epoch 229/300, trend Loss: 0.0585 | 0.0282
Epoch 230/300, trend Loss: 0.0585 | 0.0282
Epoch 231/300, trend Loss: 0.0585 | 0.0282
Epoch 232/300, trend Loss: 0.0585 | 0.0282
Epoch 233/300, trend Loss: 0.0585 | 0.0282
Epoch 234/300, trend Loss: 0.0585 | 0.0282
Epoch 235/300, trend Loss: 0.0585 | 0.0282
Epoch 236/300, trend Loss: 0.0585 | 0.0282
Epoch 237/300, trend Loss: 0.0585 | 0.0282
Epoch 238/300, trend Loss: 0.0585 | 0.0282
Epoch 239/300, trend Loss: 0.0585 | 0.0282
Epoch 240/300, trend Loss: 0.0585 | 0.0282
Epoch 241/300, trend Loss: 0.0585 | 0.0282
Epoch 242/300, trend Loss: 0.0585 | 0.0282
Epoch 243/300, trend Loss: 0.0585 | 0.0282
Epoch 244/300, trend Loss: 0.0585 | 0.0282
Epoch 245/300, trend Loss: 0.0585 | 0.0282
Epoch 246/300, trend Loss: 0.0585 | 0.0282
Epoch 247/300, trend Loss: 0.0585 | 0.0282
Epoch 248/300, trend Loss: 0.0585 | 0.0282
Epoch 249/300, trend Loss: 0.0585 | 0.0282
Epoch 250/300, trend Loss: 0.0585 | 0.0282
Epoch 251/300, trend Loss: 0.0585 | 0.0282
Epoch 252/300, trend Loss: 0.0585 | 0.0282
Epoch 253/300, trend Loss: 0.0585 | 0.0282
Epoch 254/300, trend Loss: 0.0585 | 0.0282
Epoch 255/300, trend Loss: 0.0585 | 0.0282
Epoch 256/300, trend Loss: 0.0585 | 0.0282
Epoch 257/300, trend Loss: 0.0585 | 0.0282
Epoch 258/300, trend Loss: 0.0585 | 0.0282
Epoch 259/300, trend Loss: 0.0585 | 0.0282
Epoch 260/300, trend Loss: 0.0585 | 0.0282
Epoch 261/300, trend Loss: 0.0585 | 0.0282
Epoch 262/300, trend Loss: 0.0585 | 0.0282
Epoch 263/300, trend Loss: 0.0585 | 0.0282
Epoch 264/300, trend Loss: 0.0585 | 0.0282
Epoch 265/300, trend Loss: 0.0585 | 0.0282
Epoch 266/300, trend Loss: 0.0585 | 0.0282
Epoch 267/300, trend Loss: 0.0585 | 0.0282
Epoch 268/300, trend Loss: 0.0585 | 0.0282
Epoch 269/300, trend Loss: 0.0585 | 0.0282
Epoch 270/300, trend Loss: 0.0585 | 0.0282
Epoch 271/300, trend Loss: 0.0585 | 0.0282
Epoch 272/300, trend Loss: 0.0585 | 0.0282
Epoch 273/300, trend Loss: 0.0585 | 0.0282
Epoch 274/300, trend Loss: 0.0585 | 0.0282
Epoch 275/300, trend Loss: 0.0585 | 0.0282
Epoch 276/300, trend Loss: 0.0585 | 0.0282
Epoch 277/300, trend Loss: 0.0585 | 0.0282
Epoch 278/300, trend Loss: 0.0585 | 0.0282
Epoch 279/300, trend Loss: 0.0585 | 0.0282
Epoch 280/300, trend Loss: 0.0585 | 0.0282
Epoch 281/300, trend Loss: 0.0585 | 0.0282
Epoch 282/300, trend Loss: 0.0585 | 0.0282
Epoch 283/300, trend Loss: 0.0585 | 0.0282
Epoch 284/300, trend Loss: 0.0585 | 0.0282
Epoch 285/300, trend Loss: 0.0585 | 0.0282
Epoch 286/300, trend Loss: 0.0585 | 0.0282
Epoch 287/300, trend Loss: 0.0585 | 0.0282
Epoch 288/300, trend Loss: 0.0585 | 0.0282
Epoch 289/300, trend Loss: 0.0585 | 0.0282
Epoch 290/300, trend Loss: 0.0585 | 0.0282
Epoch 291/300, trend Loss: 0.0585 | 0.0282
Epoch 292/300, trend Loss: 0.0585 | 0.0282
Epoch 293/300, trend Loss: 0.0585 | 0.0282
Epoch 294/300, trend Loss: 0.0585 | 0.0282
Epoch 295/300, trend Loss: 0.0585 | 0.0282
Epoch 296/300, trend Loss: 0.0585 | 0.0282
Epoch 297/300, trend Loss: 0.0585 | 0.0282
Epoch 298/300, trend Loss: 0.0585 | 0.0281
Epoch 299/300, trend Loss: 0.0585 | 0.0281
Epoch 300/300, trend Loss: 0.0585 | 0.0281
Training seasonal_0 component with params: {'observation_period_num': 15, 'train_rates': 0.9638757921127494, 'learning_rate': 0.0007522031243509124, 'batch_size': 221, 'step_size': 6, 'gamma': 0.8731957484409746}
Epoch 1/300, seasonal_0 Loss: 0.7473 | 0.3892
Epoch 2/300, seasonal_0 Loss: 0.2518 | 0.2569
Epoch 3/300, seasonal_0 Loss: 0.2199 | 0.3287
Epoch 4/300, seasonal_0 Loss: 0.2530 | 0.5492
Epoch 5/300, seasonal_0 Loss: 0.2029 | 0.2566
Epoch 6/300, seasonal_0 Loss: 0.2059 | 0.3239
Epoch 7/300, seasonal_0 Loss: 0.1683 | 0.2238
Epoch 8/300, seasonal_0 Loss: 0.1626 | 0.1781
Epoch 9/300, seasonal_0 Loss: 0.1668 | 0.1779
Epoch 10/300, seasonal_0 Loss: 0.1265 | 0.1391
Epoch 11/300, seasonal_0 Loss: 0.1448 | 0.1744
Epoch 12/300, seasonal_0 Loss: 0.1245 | 0.1131
Epoch 13/300, seasonal_0 Loss: 0.1411 | 0.1139
Epoch 14/300, seasonal_0 Loss: 0.1200 | 0.1203
Epoch 15/300, seasonal_0 Loss: 0.1376 | 0.2419
Epoch 16/300, seasonal_0 Loss: 0.1266 | 0.1088
Epoch 17/300, seasonal_0 Loss: 0.1277 | 0.1473
Epoch 18/300, seasonal_0 Loss: 0.1258 | 0.1147
Epoch 19/300, seasonal_0 Loss: 0.1364 | 0.1867
Epoch 20/300, seasonal_0 Loss: 0.1681 | 0.2150
Epoch 21/300, seasonal_0 Loss: 0.1857 | 0.1520
Epoch 22/300, seasonal_0 Loss: 0.1778 | 0.2169
Epoch 23/300, seasonal_0 Loss: 0.1679 | 0.1201
Epoch 24/300, seasonal_0 Loss: 0.1680 | 0.3037
Epoch 25/300, seasonal_0 Loss: 0.1483 | 0.1221
Epoch 26/300, seasonal_0 Loss: 0.1304 | 0.1997
Epoch 27/300, seasonal_0 Loss: 0.1163 | 0.1031
Epoch 28/300, seasonal_0 Loss: 0.1114 | 0.1387
Epoch 29/300, seasonal_0 Loss: 0.1050 | 0.0933
Epoch 30/300, seasonal_0 Loss: 0.1037 | 0.1146
Epoch 31/300, seasonal_0 Loss: 0.1000 | 0.0866
Epoch 32/300, seasonal_0 Loss: 0.1008 | 0.1086
Epoch 33/300, seasonal_0 Loss: 0.0989 | 0.0825
Epoch 34/300, seasonal_0 Loss: 0.1004 | 0.1129
Epoch 35/300, seasonal_0 Loss: 0.0972 | 0.0796
Epoch 36/300, seasonal_0 Loss: 0.0975 | 0.1026
Epoch 37/300, seasonal_0 Loss: 0.0941 | 0.0766
Epoch 38/300, seasonal_0 Loss: 0.0932 | 0.0897
Epoch 39/300, seasonal_0 Loss: 0.0911 | 0.0734
Epoch 40/300, seasonal_0 Loss: 0.0904 | 0.0820
Epoch 41/300, seasonal_0 Loss: 0.0888 | 0.0712
Epoch 42/300, seasonal_0 Loss: 0.0883 | 0.0762
Epoch 43/300, seasonal_0 Loss: 0.0871 | 0.0694
Epoch 44/300, seasonal_0 Loss: 0.0866 | 0.0729
Epoch 45/300, seasonal_0 Loss: 0.0859 | 0.0680
Epoch 46/300, seasonal_0 Loss: 0.0854 | 0.0707
Epoch 47/300, seasonal_0 Loss: 0.0848 | 0.0668
Epoch 48/300, seasonal_0 Loss: 0.0845 | 0.0684
Epoch 49/300, seasonal_0 Loss: 0.0840 | 0.0658
Epoch 50/300, seasonal_0 Loss: 0.0838 | 0.0667
Epoch 51/300, seasonal_0 Loss: 0.0834 | 0.0649
Epoch 52/300, seasonal_0 Loss: 0.0832 | 0.0653
Epoch 53/300, seasonal_0 Loss: 0.0829 | 0.0642
Epoch 54/300, seasonal_0 Loss: 0.0827 | 0.0641
Epoch 55/300, seasonal_0 Loss: 0.0824 | 0.0636
Epoch 56/300, seasonal_0 Loss: 0.0822 | 0.0633
Epoch 57/300, seasonal_0 Loss: 0.0820 | 0.0629
Epoch 58/300, seasonal_0 Loss: 0.0818 | 0.0626
Epoch 59/300, seasonal_0 Loss: 0.0816 | 0.0623
Epoch 60/300, seasonal_0 Loss: 0.0815 | 0.0620
Epoch 61/300, seasonal_0 Loss: 0.0813 | 0.0617
Epoch 62/300, seasonal_0 Loss: 0.0811 | 0.0614
Epoch 63/300, seasonal_0 Loss: 0.0810 | 0.0612
Epoch 64/300, seasonal_0 Loss: 0.0808 | 0.0609
Epoch 65/300, seasonal_0 Loss: 0.0807 | 0.0607
Epoch 66/300, seasonal_0 Loss: 0.0806 | 0.0605
Epoch 67/300, seasonal_0 Loss: 0.0804 | 0.0603
Epoch 68/300, seasonal_0 Loss: 0.0803 | 0.0600
Epoch 69/300, seasonal_0 Loss: 0.0802 | 0.0598
Epoch 70/300, seasonal_0 Loss: 0.0801 | 0.0597
Epoch 71/300, seasonal_0 Loss: 0.0800 | 0.0595
Epoch 72/300, seasonal_0 Loss: 0.0799 | 0.0593
Epoch 73/300, seasonal_0 Loss: 0.0797 | 0.0591
Epoch 74/300, seasonal_0 Loss: 0.0797 | 0.0590
Epoch 75/300, seasonal_0 Loss: 0.0796 | 0.0588
Epoch 76/300, seasonal_0 Loss: 0.0795 | 0.0587
Epoch 77/300, seasonal_0 Loss: 0.0794 | 0.0585
Epoch 78/300, seasonal_0 Loss: 0.0793 | 0.0584
Epoch 79/300, seasonal_0 Loss: 0.0792 | 0.0582
Epoch 80/300, seasonal_0 Loss: 0.0791 | 0.0581
Epoch 81/300, seasonal_0 Loss: 0.0791 | 0.0580
Epoch 82/300, seasonal_0 Loss: 0.0790 | 0.0579
Epoch 83/300, seasonal_0 Loss: 0.0789 | 0.0578
Epoch 84/300, seasonal_0 Loss: 0.0789 | 0.0576
Epoch 85/300, seasonal_0 Loss: 0.0788 | 0.0576
Epoch 86/300, seasonal_0 Loss: 0.0787 | 0.0575
Epoch 87/300, seasonal_0 Loss: 0.0787 | 0.0574
Epoch 88/300, seasonal_0 Loss: 0.0786 | 0.0573
Epoch 89/300, seasonal_0 Loss: 0.0786 | 0.0572
Epoch 90/300, seasonal_0 Loss: 0.0785 | 0.0571
Epoch 91/300, seasonal_0 Loss: 0.0785 | 0.0570
Epoch 92/300, seasonal_0 Loss: 0.0784 | 0.0569
Epoch 93/300, seasonal_0 Loss: 0.0784 | 0.0569
Epoch 94/300, seasonal_0 Loss: 0.0783 | 0.0568
Epoch 95/300, seasonal_0 Loss: 0.0783 | 0.0567
Epoch 96/300, seasonal_0 Loss: 0.0782 | 0.0567
Epoch 97/300, seasonal_0 Loss: 0.0782 | 0.0566
Epoch 98/300, seasonal_0 Loss: 0.0782 | 0.0565
Epoch 99/300, seasonal_0 Loss: 0.0781 | 0.0565
Epoch 100/300, seasonal_0 Loss: 0.0781 | 0.0564
Epoch 101/300, seasonal_0 Loss: 0.0781 | 0.0564
Epoch 102/300, seasonal_0 Loss: 0.0780 | 0.0563
Epoch 103/300, seasonal_0 Loss: 0.0780 | 0.0563
Epoch 104/300, seasonal_0 Loss: 0.0780 | 0.0562
Epoch 105/300, seasonal_0 Loss: 0.0779 | 0.0562
Epoch 106/300, seasonal_0 Loss: 0.0779 | 0.0562
Epoch 107/300, seasonal_0 Loss: 0.0779 | 0.0561
Epoch 108/300, seasonal_0 Loss: 0.0779 | 0.0561
Epoch 109/300, seasonal_0 Loss: 0.0778 | 0.0560
Epoch 110/300, seasonal_0 Loss: 0.0778 | 0.0560
Epoch 111/300, seasonal_0 Loss: 0.0778 | 0.0560
Epoch 112/300, seasonal_0 Loss: 0.0778 | 0.0559
Epoch 113/300, seasonal_0 Loss: 0.0778 | 0.0559
Epoch 114/300, seasonal_0 Loss: 0.0777 | 0.0559
Epoch 115/300, seasonal_0 Loss: 0.0777 | 0.0558
Epoch 116/300, seasonal_0 Loss: 0.0777 | 0.0558
Epoch 117/300, seasonal_0 Loss: 0.0777 | 0.0558
Epoch 118/300, seasonal_0 Loss: 0.0777 | 0.0558
Epoch 119/300, seasonal_0 Loss: 0.0776 | 0.0557
Epoch 120/300, seasonal_0 Loss: 0.0776 | 0.0557
Epoch 121/300, seasonal_0 Loss: 0.0776 | 0.0557
Epoch 122/300, seasonal_0 Loss: 0.0776 | 0.0557
Epoch 123/300, seasonal_0 Loss: 0.0776 | 0.0556
Epoch 124/300, seasonal_0 Loss: 0.0776 | 0.0556
Epoch 125/300, seasonal_0 Loss: 0.0776 | 0.0556
Epoch 126/300, seasonal_0 Loss: 0.0776 | 0.0556
Epoch 127/300, seasonal_0 Loss: 0.0775 | 0.0556
Epoch 128/300, seasonal_0 Loss: 0.0775 | 0.0556
Epoch 129/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 130/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 131/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 132/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 133/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 134/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 135/300, seasonal_0 Loss: 0.0775 | 0.0555
Epoch 136/300, seasonal_0 Loss: 0.0775 | 0.0554
Epoch 137/300, seasonal_0 Loss: 0.0775 | 0.0554
Epoch 138/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 139/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 140/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 141/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 142/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 143/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 144/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 145/300, seasonal_0 Loss: 0.0774 | 0.0554
Epoch 146/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 147/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 148/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 149/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 150/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 151/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 152/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 153/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 154/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 155/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 156/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 157/300, seasonal_0 Loss: 0.0774 | 0.0553
Epoch 158/300, seasonal_0 Loss: 0.0773 | 0.0553
Epoch 159/300, seasonal_0 Loss: 0.0773 | 0.0553
Epoch 160/300, seasonal_0 Loss: 0.0773 | 0.0553
Epoch 161/300, seasonal_0 Loss: 0.0773 | 0.0553
Epoch 162/300, seasonal_0 Loss: 0.0773 | 0.0553
Epoch 163/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 164/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 165/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 166/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 167/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 168/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 169/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 170/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 171/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 172/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 173/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 174/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 175/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 176/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 177/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 178/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 179/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 180/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 181/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 182/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 183/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 184/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 185/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 186/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 187/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 188/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 189/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 190/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 191/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 192/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 193/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 194/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 195/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 196/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 197/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 198/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 199/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 200/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 201/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 202/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 203/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 204/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 205/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 206/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 207/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 208/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 209/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 210/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 211/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 212/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 213/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 214/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 215/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 216/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 217/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 218/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 219/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 220/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 221/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 222/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 223/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 224/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 225/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 226/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 227/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 228/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 229/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 230/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 231/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 232/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 233/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 234/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 235/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 236/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 237/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 238/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 239/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 240/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 241/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 242/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 243/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 244/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 245/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 246/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 247/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 248/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 249/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 250/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 251/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 252/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 253/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 254/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 255/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 256/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 257/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 258/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 259/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 260/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 261/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 262/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 263/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 264/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 265/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 266/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 267/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 268/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 269/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 270/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 271/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 272/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 273/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 274/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 275/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 276/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 277/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 278/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 279/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 280/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 281/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 282/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 283/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 284/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 285/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 286/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 287/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 288/300, seasonal_0 Loss: 0.0773 | 0.0552
Epoch 289/300, seasonal_0 Loss: 0.0773 | 0.0552
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.8609590620946834, 'learning_rate': 0.0002387080768878746, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8108397645277955}
Epoch 1/300, seasonal_1 Loss: 0.4983 | 0.2606
Epoch 2/300, seasonal_1 Loss: 0.2134 | 0.2564
Epoch 3/300, seasonal_1 Loss: 0.2788 | 0.2377
Epoch 4/300, seasonal_1 Loss: 0.2013 | 0.2100
Epoch 5/300, seasonal_1 Loss: 0.1814 | 0.1479
Epoch 6/300, seasonal_1 Loss: 0.1688 | 0.1299
Epoch 7/300, seasonal_1 Loss: 0.1467 | 0.1222
Epoch 8/300, seasonal_1 Loss: 0.1508 | 0.1345
Epoch 9/300, seasonal_1 Loss: 0.1396 | 0.1182
Epoch 10/300, seasonal_1 Loss: 0.1294 | 0.1059
Epoch 11/300, seasonal_1 Loss: 0.1238 | 0.1011
Epoch 12/300, seasonal_1 Loss: 0.1216 | 0.0966
Epoch 13/300, seasonal_1 Loss: 0.1186 | 0.0911
Epoch 14/300, seasonal_1 Loss: 0.1180 | 0.0885
Epoch 15/300, seasonal_1 Loss: 0.1154 | 0.0841
Epoch 16/300, seasonal_1 Loss: 0.1148 | 0.0828
Epoch 17/300, seasonal_1 Loss: 0.1128 | 0.0790
Epoch 18/300, seasonal_1 Loss: 0.1128 | 0.0777
Epoch 19/300, seasonal_1 Loss: 0.1110 | 0.0750
Epoch 20/300, seasonal_1 Loss: 0.1114 | 0.0758
Epoch 21/300, seasonal_1 Loss: 0.1105 | 0.0731
Epoch 22/300, seasonal_1 Loss: 0.1113 | 0.0767
Epoch 23/300, seasonal_1 Loss: 0.1102 | 0.0727
Epoch 24/300, seasonal_1 Loss: 0.1119 | 0.0771
Epoch 25/300, seasonal_1 Loss: 0.1108 | 0.0726
Epoch 26/300, seasonal_1 Loss: 0.1112 | 0.0794
Epoch 27/300, seasonal_1 Loss: 0.1090 | 0.0732
Epoch 28/300, seasonal_1 Loss: 0.1092 | 0.0765
Epoch 29/300, seasonal_1 Loss: 0.1064 | 0.0713
Epoch 30/300, seasonal_1 Loss: 0.1058 | 0.0711
Epoch 31/300, seasonal_1 Loss: 0.1042 | 0.0686
Epoch 32/300, seasonal_1 Loss: 0.1038 | 0.0681
Epoch 33/300, seasonal_1 Loss: 0.1030 | 0.0674
Epoch 34/300, seasonal_1 Loss: 0.1027 | 0.0671
Epoch 35/300, seasonal_1 Loss: 0.1024 | 0.0668
Epoch 36/300, seasonal_1 Loss: 0.1021 | 0.0666
Epoch 37/300, seasonal_1 Loss: 0.1019 | 0.0664
Epoch 38/300, seasonal_1 Loss: 0.1017 | 0.0662
Epoch 39/300, seasonal_1 Loss: 0.1016 | 0.0661
Epoch 40/300, seasonal_1 Loss: 0.1014 | 0.0659
Epoch 41/300, seasonal_1 Loss: 0.1012 | 0.0658
Epoch 42/300, seasonal_1 Loss: 0.1011 | 0.0656
Epoch 43/300, seasonal_1 Loss: 0.1010 | 0.0655
Epoch 44/300, seasonal_1 Loss: 0.1008 | 0.0654
Epoch 45/300, seasonal_1 Loss: 0.1007 | 0.0653
Epoch 46/300, seasonal_1 Loss: 0.1006 | 0.0652
Epoch 47/300, seasonal_1 Loss: 0.1005 | 0.0651
Epoch 48/300, seasonal_1 Loss: 0.1004 | 0.0650
Epoch 49/300, seasonal_1 Loss: 0.1003 | 0.0649
Epoch 50/300, seasonal_1 Loss: 0.1002 | 0.0649
Epoch 51/300, seasonal_1 Loss: 0.1001 | 0.0648
Epoch 52/300, seasonal_1 Loss: 0.1000 | 0.0647
Epoch 53/300, seasonal_1 Loss: 0.1000 | 0.0646
Epoch 54/300, seasonal_1 Loss: 0.0999 | 0.0646
Epoch 55/300, seasonal_1 Loss: 0.0998 | 0.0645
Epoch 56/300, seasonal_1 Loss: 0.0998 | 0.0645
Epoch 57/300, seasonal_1 Loss: 0.0997 | 0.0644
Epoch 58/300, seasonal_1 Loss: 0.0996 | 0.0644
Epoch 59/300, seasonal_1 Loss: 0.0996 | 0.0643
Epoch 60/300, seasonal_1 Loss: 0.0995 | 0.0643
Epoch 61/300, seasonal_1 Loss: 0.0995 | 0.0642
Epoch 62/300, seasonal_1 Loss: 0.0994 | 0.0642
Epoch 63/300, seasonal_1 Loss: 0.0994 | 0.0642
Epoch 64/300, seasonal_1 Loss: 0.0994 | 0.0641
Epoch 65/300, seasonal_1 Loss: 0.0993 | 0.0641
Epoch 66/300, seasonal_1 Loss: 0.0993 | 0.0641
Epoch 67/300, seasonal_1 Loss: 0.0992 | 0.0640
Epoch 68/300, seasonal_1 Loss: 0.0992 | 0.0640
Epoch 69/300, seasonal_1 Loss: 0.0992 | 0.0640
Epoch 70/300, seasonal_1 Loss: 0.0992 | 0.0640
Epoch 71/300, seasonal_1 Loss: 0.0991 | 0.0639
Epoch 72/300, seasonal_1 Loss: 0.0991 | 0.0639
Epoch 73/300, seasonal_1 Loss: 0.0991 | 0.0639
Epoch 74/300, seasonal_1 Loss: 0.0991 | 0.0639
Epoch 75/300, seasonal_1 Loss: 0.0990 | 0.0638
Epoch 76/300, seasonal_1 Loss: 0.0990 | 0.0638
Epoch 77/300, seasonal_1 Loss: 0.0990 | 0.0638
Epoch 78/300, seasonal_1 Loss: 0.0990 | 0.0638
Epoch 79/300, seasonal_1 Loss: 0.0990 | 0.0638
Epoch 80/300, seasonal_1 Loss: 0.0989 | 0.0638
Epoch 81/300, seasonal_1 Loss: 0.0989 | 0.0637
Epoch 82/300, seasonal_1 Loss: 0.0989 | 0.0637
Epoch 83/300, seasonal_1 Loss: 0.0989 | 0.0637
Epoch 84/300, seasonal_1 Loss: 0.0989 | 0.0637
Epoch 85/300, seasonal_1 Loss: 0.0989 | 0.0637
Epoch 86/300, seasonal_1 Loss: 0.0988 | 0.0637
Epoch 87/300, seasonal_1 Loss: 0.0988 | 0.0637
Epoch 88/300, seasonal_1 Loss: 0.0988 | 0.0637
Epoch 89/300, seasonal_1 Loss: 0.0988 | 0.0637
Epoch 90/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 91/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 92/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 93/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 94/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 95/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 96/300, seasonal_1 Loss: 0.0988 | 0.0636
Epoch 97/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 98/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 99/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 100/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 101/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 102/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 103/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 104/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 105/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 106/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 107/300, seasonal_1 Loss: 0.0987 | 0.0636
Epoch 108/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 109/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 110/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 111/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 112/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 113/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 114/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 115/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 116/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 117/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 118/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 119/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 120/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 121/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 122/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 123/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 124/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 125/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 126/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 127/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 128/300, seasonal_1 Loss: 0.0987 | 0.0635
Epoch 129/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 130/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 131/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 132/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 133/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 134/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 135/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 136/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 137/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 138/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 139/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 140/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 141/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 142/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 143/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 144/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 145/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 146/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 147/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 148/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 149/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 150/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 151/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 152/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 153/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 154/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 155/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 156/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 157/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 158/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 159/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 160/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 161/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 162/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 163/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 164/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 165/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 166/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 167/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 168/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 169/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 170/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 171/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 172/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 173/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 174/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 175/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 176/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 177/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 178/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 179/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 180/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 181/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 182/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 183/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 184/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 185/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 186/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 187/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 188/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 189/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 190/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 191/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 192/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 193/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 194/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 195/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 196/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 197/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 198/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 199/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 200/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 201/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 202/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 203/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 204/300, seasonal_1 Loss: 0.0986 | 0.0635
Epoch 205/300, seasonal_1 Loss: 0.0986 | 0.0635
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 7, 'train_rates': 0.98145117244669, 'learning_rate': 0.0006894929422308206, 'batch_size': 191, 'step_size': 10, 'gamma': 0.820162543612845}
Epoch 1/300, seasonal_2 Loss: 0.4895 | 0.2534
Epoch 2/300, seasonal_2 Loss: 0.1978 | 0.1412
Epoch 3/300, seasonal_2 Loss: 0.1523 | 0.1676
Epoch 4/300, seasonal_2 Loss: 0.1712 | 0.2573
Epoch 5/300, seasonal_2 Loss: 0.2196 | 0.1617
Epoch 6/300, seasonal_2 Loss: 0.3306 | 0.5880
Epoch 7/300, seasonal_2 Loss: 0.1794 | 0.1890
Epoch 8/300, seasonal_2 Loss: 0.1968 | 0.1296
Epoch 9/300, seasonal_2 Loss: 0.2173 | 0.1423
Epoch 10/300, seasonal_2 Loss: 0.1480 | 0.1355
Epoch 11/300, seasonal_2 Loss: 0.1330 | 0.0907
Epoch 12/300, seasonal_2 Loss: 0.1133 | 0.0879
Epoch 13/300, seasonal_2 Loss: 0.1180 | 0.0882
Epoch 14/300, seasonal_2 Loss: 0.1082 | 0.0788
Epoch 15/300, seasonal_2 Loss: 0.1095 | 0.0756
Epoch 16/300, seasonal_2 Loss: 0.1006 | 0.0757
Epoch 17/300, seasonal_2 Loss: 0.1010 | 0.0727
Epoch 18/300, seasonal_2 Loss: 0.0975 | 0.0675
Epoch 19/300, seasonal_2 Loss: 0.0939 | 0.0700
Epoch 20/300, seasonal_2 Loss: 0.0911 | 0.0658
Epoch 21/300, seasonal_2 Loss: 0.0912 | 0.0662
Epoch 22/300, seasonal_2 Loss: 0.0894 | 0.0631
Epoch 23/300, seasonal_2 Loss: 0.0869 | 0.0629
Epoch 24/300, seasonal_2 Loss: 0.0846 | 0.0590
Epoch 25/300, seasonal_2 Loss: 0.0837 | 0.0585
Epoch 26/300, seasonal_2 Loss: 0.0825 | 0.0561
Epoch 27/300, seasonal_2 Loss: 0.0814 | 0.0576
Epoch 28/300, seasonal_2 Loss: 0.0803 | 0.0545
Epoch 29/300, seasonal_2 Loss: 0.0796 | 0.0521
Epoch 30/300, seasonal_2 Loss: 0.0789 | 0.0510
Epoch 31/300, seasonal_2 Loss: 0.0785 | 0.0511
Epoch 32/300, seasonal_2 Loss: 0.0782 | 0.0521
Epoch 33/300, seasonal_2 Loss: 0.0782 | 0.0503
Epoch 34/300, seasonal_2 Loss: 0.0782 | 0.0510
Epoch 35/300, seasonal_2 Loss: 0.0789 | 0.0522
Epoch 36/300, seasonal_2 Loss: 0.0798 | 0.0516
Epoch 37/300, seasonal_2 Loss: 0.0789 | 0.0482
Epoch 38/300, seasonal_2 Loss: 0.0773 | 0.0475
Epoch 39/300, seasonal_2 Loss: 0.0778 | 0.0477
Epoch 40/300, seasonal_2 Loss: 0.0791 | 0.0464
Epoch 41/300, seasonal_2 Loss: 0.0780 | 0.0461
Epoch 42/300, seasonal_2 Loss: 0.0757 | 0.0466
Epoch 43/300, seasonal_2 Loss: 0.0753 | 0.0463
Epoch 44/300, seasonal_2 Loss: 0.0748 | 0.0453
Epoch 45/300, seasonal_2 Loss: 0.0746 | 0.0447
Epoch 46/300, seasonal_2 Loss: 0.0745 | 0.0444
Epoch 47/300, seasonal_2 Loss: 0.0741 | 0.0442
Epoch 48/300, seasonal_2 Loss: 0.0737 | 0.0439
Epoch 49/300, seasonal_2 Loss: 0.0735 | 0.0437
Epoch 50/300, seasonal_2 Loss: 0.0733 | 0.0433
Epoch 51/300, seasonal_2 Loss: 0.0731 | 0.0430
Epoch 52/300, seasonal_2 Loss: 0.0730 | 0.0427
Epoch 53/300, seasonal_2 Loss: 0.0728 | 0.0425
Epoch 54/300, seasonal_2 Loss: 0.0726 | 0.0423
Epoch 55/300, seasonal_2 Loss: 0.0724 | 0.0422
Epoch 56/300, seasonal_2 Loss: 0.0723 | 0.0420
Epoch 57/300, seasonal_2 Loss: 0.0721 | 0.0418
Epoch 58/300, seasonal_2 Loss: 0.0720 | 0.0417
Epoch 59/300, seasonal_2 Loss: 0.0719 | 0.0415
Epoch 60/300, seasonal_2 Loss: 0.0717 | 0.0414
Epoch 61/300, seasonal_2 Loss: 0.0716 | 0.0412
Epoch 62/300, seasonal_2 Loss: 0.0715 | 0.0411
Epoch 63/300, seasonal_2 Loss: 0.0714 | 0.0410
Epoch 64/300, seasonal_2 Loss: 0.0713 | 0.0409
Epoch 65/300, seasonal_2 Loss: 0.0712 | 0.0409
Epoch 66/300, seasonal_2 Loss: 0.0711 | 0.0408
Epoch 67/300, seasonal_2 Loss: 0.0710 | 0.0407
Epoch 68/300, seasonal_2 Loss: 0.0709 | 0.0406
Epoch 69/300, seasonal_2 Loss: 0.0709 | 0.0406
Epoch 70/300, seasonal_2 Loss: 0.0708 | 0.0405
Epoch 71/300, seasonal_2 Loss: 0.0707 | 0.0404
Epoch 72/300, seasonal_2 Loss: 0.0706 | 0.0404
Epoch 73/300, seasonal_2 Loss: 0.0706 | 0.0403
Epoch 74/300, seasonal_2 Loss: 0.0705 | 0.0403
Epoch 75/300, seasonal_2 Loss: 0.0704 | 0.0402
Epoch 76/300, seasonal_2 Loss: 0.0704 | 0.0402
Epoch 77/300, seasonal_2 Loss: 0.0703 | 0.0401
Epoch 78/300, seasonal_2 Loss: 0.0703 | 0.0401
Epoch 79/300, seasonal_2 Loss: 0.0702 | 0.0401
Epoch 80/300, seasonal_2 Loss: 0.0702 | 0.0400
Epoch 81/300, seasonal_2 Loss: 0.0701 | 0.0400
Epoch 82/300, seasonal_2 Loss: 0.0701 | 0.0400
Epoch 83/300, seasonal_2 Loss: 0.0700 | 0.0399
Epoch 84/300, seasonal_2 Loss: 0.0700 | 0.0399
Epoch 85/300, seasonal_2 Loss: 0.0699 | 0.0399
Epoch 86/300, seasonal_2 Loss: 0.0699 | 0.0399
Epoch 87/300, seasonal_2 Loss: 0.0698 | 0.0398
Epoch 88/300, seasonal_2 Loss: 0.0698 | 0.0398
Epoch 89/300, seasonal_2 Loss: 0.0698 | 0.0398
Epoch 90/300, seasonal_2 Loss: 0.0697 | 0.0398
Epoch 91/300, seasonal_2 Loss: 0.0697 | 0.0397
Epoch 92/300, seasonal_2 Loss: 0.0696 | 0.0397
Epoch 93/300, seasonal_2 Loss: 0.0696 | 0.0397
Epoch 94/300, seasonal_2 Loss: 0.0696 | 0.0397
Epoch 95/300, seasonal_2 Loss: 0.0695 | 0.0397
Epoch 96/300, seasonal_2 Loss: 0.0695 | 0.0397
Epoch 97/300, seasonal_2 Loss: 0.0695 | 0.0396
Epoch 98/300, seasonal_2 Loss: 0.0695 | 0.0396
Epoch 99/300, seasonal_2 Loss: 0.0694 | 0.0396
Epoch 100/300, seasonal_2 Loss: 0.0694 | 0.0396
Epoch 101/300, seasonal_2 Loss: 0.0694 | 0.0396
Epoch 102/300, seasonal_2 Loss: 0.0693 | 0.0396
Epoch 103/300, seasonal_2 Loss: 0.0693 | 0.0396
Epoch 104/300, seasonal_2 Loss: 0.0693 | 0.0395
Epoch 105/300, seasonal_2 Loss: 0.0693 | 0.0395
Epoch 106/300, seasonal_2 Loss: 0.0692 | 0.0395
Epoch 107/300, seasonal_2 Loss: 0.0692 | 0.0395
Epoch 108/300, seasonal_2 Loss: 0.0692 | 0.0395
Epoch 109/300, seasonal_2 Loss: 0.0692 | 0.0395
Epoch 110/300, seasonal_2 Loss: 0.0692 | 0.0395
Epoch 111/300, seasonal_2 Loss: 0.0691 | 0.0395
Epoch 112/300, seasonal_2 Loss: 0.0691 | 0.0395
Epoch 113/300, seasonal_2 Loss: 0.0691 | 0.0394
Epoch 114/300, seasonal_2 Loss: 0.0691 | 0.0394
Epoch 115/300, seasonal_2 Loss: 0.0691 | 0.0394
Epoch 116/300, seasonal_2 Loss: 0.0691 | 0.0394
Epoch 117/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 118/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 119/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 120/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 121/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 122/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 123/300, seasonal_2 Loss: 0.0690 | 0.0394
Epoch 124/300, seasonal_2 Loss: 0.0689 | 0.0394
Epoch 125/300, seasonal_2 Loss: 0.0689 | 0.0394
Epoch 126/300, seasonal_2 Loss: 0.0689 | 0.0394
Epoch 127/300, seasonal_2 Loss: 0.0689 | 0.0394
Epoch 128/300, seasonal_2 Loss: 0.0689 | 0.0394
Epoch 129/300, seasonal_2 Loss: 0.0689 | 0.0393
Epoch 130/300, seasonal_2 Loss: 0.0689 | 0.0393
Epoch 131/300, seasonal_2 Loss: 0.0689 | 0.0393
Epoch 132/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 133/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 134/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 135/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 136/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 137/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 138/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 139/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 140/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 141/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 142/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 143/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 144/300, seasonal_2 Loss: 0.0688 | 0.0393
Epoch 145/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 146/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 147/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 148/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 149/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 150/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 151/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 152/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 153/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 154/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 155/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 156/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 157/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 158/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 159/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 160/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 161/300, seasonal_2 Loss: 0.0687 | 0.0393
Epoch 162/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 163/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 164/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 165/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 166/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 167/300, seasonal_2 Loss: 0.0687 | 0.0392
Epoch 168/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 169/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 170/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 171/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 172/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 173/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 174/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 175/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 176/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 177/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 178/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 179/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 180/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 181/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 182/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 183/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 184/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 185/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 186/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 187/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 188/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 189/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 190/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 191/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 192/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 193/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 194/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 195/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 196/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 197/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 198/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 199/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 200/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 201/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 202/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 203/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 204/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 205/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 206/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 207/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 208/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 209/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 210/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 211/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 212/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 213/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 214/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 215/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 216/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 217/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 218/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 219/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 220/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 221/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 222/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 223/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 224/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 225/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 226/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 227/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 228/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 229/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 230/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 231/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 232/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 233/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 234/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 235/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 236/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 237/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 238/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 239/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 240/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 241/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 242/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 243/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 244/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 245/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 246/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 247/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 248/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 249/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 250/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 251/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 252/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 253/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 254/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 255/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 256/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 257/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 258/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 259/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 260/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 261/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 262/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 263/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 264/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 265/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 266/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 267/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 268/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 269/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 270/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 271/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 272/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 273/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 274/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 275/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 276/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 277/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 278/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 279/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 280/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 281/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 282/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 283/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 284/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 285/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 286/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 287/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 288/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 289/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 290/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 291/300, seasonal_2 Loss: 0.0686 | 0.0392
Epoch 292/300, seasonal_2 Loss: 0.0686 | 0.0392
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 68, 'train_rates': 0.8550319420655299, 'learning_rate': 4.297475997116145e-05, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9272863896840712}
Epoch 1/300, seasonal_3 Loss: 0.4994 | 0.6377
Epoch 2/300, seasonal_3 Loss: 0.3695 | 0.5091
Epoch 3/300, seasonal_3 Loss: 0.3065 | 0.3949
Epoch 4/300, seasonal_3 Loss: 0.2989 | 0.3762
Epoch 5/300, seasonal_3 Loss: 0.2776 | 0.3299
Epoch 6/300, seasonal_3 Loss: 0.2505 | 0.3034
Epoch 7/300, seasonal_3 Loss: 0.2265 | 0.2974
Epoch 8/300, seasonal_3 Loss: 0.2104 | 0.2793
Epoch 9/300, seasonal_3 Loss: 0.1995 | 0.2593
Epoch 10/300, seasonal_3 Loss: 0.1909 | 0.2476
Epoch 11/300, seasonal_3 Loss: 0.1828 | 0.2357
Epoch 12/300, seasonal_3 Loss: 0.1755 | 0.2213
Epoch 13/300, seasonal_3 Loss: 0.1688 | 0.2074
Epoch 14/300, seasonal_3 Loss: 0.1628 | 0.1949
Epoch 15/300, seasonal_3 Loss: 0.1573 | 0.1834
Epoch 16/300, seasonal_3 Loss: 0.1524 | 0.1717
Epoch 17/300, seasonal_3 Loss: 0.1486 | 0.1632
Epoch 18/300, seasonal_3 Loss: 0.1449 | 0.1555
Epoch 19/300, seasonal_3 Loss: 0.1413 | 0.1488
Epoch 20/300, seasonal_3 Loss: 0.1381 | 0.1429
Epoch 21/300, seasonal_3 Loss: 0.1350 | 0.1377
Epoch 22/300, seasonal_3 Loss: 0.1323 | 0.1331
Epoch 23/300, seasonal_3 Loss: 0.1299 | 0.1289
Epoch 24/300, seasonal_3 Loss: 0.1276 | 0.1245
Epoch 25/300, seasonal_3 Loss: 0.1260 | 0.1214
Epoch 26/300, seasonal_3 Loss: 0.1242 | 0.1185
Epoch 27/300, seasonal_3 Loss: 0.1224 | 0.1159
Epoch 28/300, seasonal_3 Loss: 0.1206 | 0.1133
Epoch 29/300, seasonal_3 Loss: 0.1190 | 0.1107
Epoch 30/300, seasonal_3 Loss: 0.1174 | 0.1081
Epoch 31/300, seasonal_3 Loss: 0.1160 | 0.1055
Epoch 32/300, seasonal_3 Loss: 0.1149 | 0.1037
Epoch 33/300, seasonal_3 Loss: 0.1135 | 0.1019
Epoch 34/300, seasonal_3 Loss: 0.1119 | 0.1000
Epoch 35/300, seasonal_3 Loss: 0.1104 | 0.0983
Epoch 36/300, seasonal_3 Loss: 0.1090 | 0.0973
Epoch 37/300, seasonal_3 Loss: 0.1078 | 0.0973
Epoch 38/300, seasonal_3 Loss: 0.1065 | 0.0981
Epoch 39/300, seasonal_3 Loss: 0.1051 | 0.0992
Epoch 40/300, seasonal_3 Loss: 0.1037 | 0.0978
Epoch 41/300, seasonal_3 Loss: 0.1026 | 0.0940
Epoch 42/300, seasonal_3 Loss: 0.1027 | 0.0903
Epoch 43/300, seasonal_3 Loss: 0.1047 | 0.0971
Epoch 44/300, seasonal_3 Loss: 0.1062 | 0.1155
Epoch 45/300, seasonal_3 Loss: 0.1033 | 0.0981
Epoch 46/300, seasonal_3 Loss: 0.1021 | 0.0862
Epoch 47/300, seasonal_3 Loss: 0.1059 | 0.1195
Epoch 48/300, seasonal_3 Loss: 0.1054 | 0.1443
Epoch 49/300, seasonal_3 Loss: 0.0997 | 0.0979
Epoch 50/300, seasonal_3 Loss: 0.0986 | 0.0902
Epoch 51/300, seasonal_3 Loss: 0.1006 | 0.0946
Epoch 52/300, seasonal_3 Loss: 0.0965 | 0.0808
Epoch 53/300, seasonal_3 Loss: 0.0952 | 0.0896
Epoch 54/300, seasonal_3 Loss: 0.0945 | 0.0899
Epoch 55/300, seasonal_3 Loss: 0.0925 | 0.0812
Epoch 56/300, seasonal_3 Loss: 0.0920 | 0.0800
Epoch 57/300, seasonal_3 Loss: 0.0916 | 0.0790
Epoch 58/300, seasonal_3 Loss: 0.0907 | 0.0767
Epoch 59/300, seasonal_3 Loss: 0.0902 | 0.0769
Epoch 60/300, seasonal_3 Loss: 0.0897 | 0.0772
Epoch 61/300, seasonal_3 Loss: 0.0890 | 0.0766
Epoch 62/300, seasonal_3 Loss: 0.0884 | 0.0750
Epoch 63/300, seasonal_3 Loss: 0.0880 | 0.0743
Epoch 64/300, seasonal_3 Loss: 0.0876 | 0.0738
Epoch 65/300, seasonal_3 Loss: 0.0871 | 0.0731
Epoch 66/300, seasonal_3 Loss: 0.0867 | 0.0723
Epoch 67/300, seasonal_3 Loss: 0.0864 | 0.0720
Epoch 68/300, seasonal_3 Loss: 0.0860 | 0.0718
Epoch 69/300, seasonal_3 Loss: 0.0856 | 0.0718
Epoch 70/300, seasonal_3 Loss: 0.0852 | 0.0711
Epoch 71/300, seasonal_3 Loss: 0.0848 | 0.0703
Epoch 72/300, seasonal_3 Loss: 0.0844 | 0.0700
Epoch 73/300, seasonal_3 Loss: 0.0840 | 0.0698
Epoch 74/300, seasonal_3 Loss: 0.0837 | 0.0695
Epoch 75/300, seasonal_3 Loss: 0.0834 | 0.0688
Epoch 76/300, seasonal_3 Loss: 0.0832 | 0.0682
Epoch 77/300, seasonal_3 Loss: 0.0830 | 0.0680
Epoch 78/300, seasonal_3 Loss: 0.0828 | 0.0681
Epoch 79/300, seasonal_3 Loss: 0.0825 | 0.0681
Epoch 80/300, seasonal_3 Loss: 0.0821 | 0.0676
Epoch 81/300, seasonal_3 Loss: 0.0817 | 0.0669
Epoch 82/300, seasonal_3 Loss: 0.0814 | 0.0666
Epoch 83/300, seasonal_3 Loss: 0.0812 | 0.0668
Epoch 84/300, seasonal_3 Loss: 0.0809 | 0.0674
Epoch 85/300, seasonal_3 Loss: 0.0806 | 0.0669
Epoch 86/300, seasonal_3 Loss: 0.0804 | 0.0659
Epoch 87/300, seasonal_3 Loss: 0.0804 | 0.0649
Epoch 88/300, seasonal_3 Loss: 0.0805 | 0.0648
Epoch 89/300, seasonal_3 Loss: 0.0805 | 0.0658
Epoch 90/300, seasonal_3 Loss: 0.0802 | 0.0668
Epoch 91/300, seasonal_3 Loss: 0.0798 | 0.0672
Epoch 92/300, seasonal_3 Loss: 0.0794 | 0.0649
Epoch 93/300, seasonal_3 Loss: 0.0792 | 0.0642
Epoch 94/300, seasonal_3 Loss: 0.0792 | 0.0658
Epoch 95/300, seasonal_3 Loss: 0.0790 | 0.0676
Epoch 96/300, seasonal_3 Loss: 0.0784 | 0.0672
Epoch 97/300, seasonal_3 Loss: 0.0781 | 0.0651
Epoch 98/300, seasonal_3 Loss: 0.0783 | 0.0629
Epoch 99/300, seasonal_3 Loss: 0.0787 | 0.0637
Epoch 100/300, seasonal_3 Loss: 0.0790 | 0.0682
Epoch 101/300, seasonal_3 Loss: 0.0786 | 0.0696
Epoch 102/300, seasonal_3 Loss: 0.0778 | 0.0650
Epoch 103/300, seasonal_3 Loss: 0.0775 | 0.0626
Epoch 104/300, seasonal_3 Loss: 0.0778 | 0.0654
Epoch 105/300, seasonal_3 Loss: 0.0776 | 0.0680
Epoch 106/300, seasonal_3 Loss: 0.0768 | 0.0659
Epoch 107/300, seasonal_3 Loss: 0.0765 | 0.0617
Epoch 108/300, seasonal_3 Loss: 0.0769 | 0.0619
Epoch 109/300, seasonal_3 Loss: 0.0771 | 0.0654
Epoch 110/300, seasonal_3 Loss: 0.0766 | 0.0654
Epoch 111/300, seasonal_3 Loss: 0.0759 | 0.0620
Epoch 112/300, seasonal_3 Loss: 0.0757 | 0.0616
Epoch 113/300, seasonal_3 Loss: 0.0758 | 0.0634
Epoch 114/300, seasonal_3 Loss: 0.0755 | 0.0639
Epoch 115/300, seasonal_3 Loss: 0.0750 | 0.0612
Epoch 116/300, seasonal_3 Loss: 0.0750 | 0.0602
Epoch 117/300, seasonal_3 Loss: 0.0751 | 0.0610
Epoch 118/300, seasonal_3 Loss: 0.0749 | 0.0615
Epoch 119/300, seasonal_3 Loss: 0.0746 | 0.0607
Epoch 120/300, seasonal_3 Loss: 0.0743 | 0.0601
Epoch 121/300, seasonal_3 Loss: 0.0742 | 0.0606
Epoch 122/300, seasonal_3 Loss: 0.0741 | 0.0607
Epoch 123/300, seasonal_3 Loss: 0.0738 | 0.0600
Epoch 124/300, seasonal_3 Loss: 0.0737 | 0.0594
Epoch 125/300, seasonal_3 Loss: 0.0737 | 0.0594
Epoch 126/300, seasonal_3 Loss: 0.0736 | 0.0595
Epoch 127/300, seasonal_3 Loss: 0.0735 | 0.0594
Epoch 128/300, seasonal_3 Loss: 0.0733 | 0.0592
Epoch 129/300, seasonal_3 Loss: 0.0731 | 0.0591
Epoch 130/300, seasonal_3 Loss: 0.0730 | 0.0592
Epoch 131/300, seasonal_3 Loss: 0.0729 | 0.0592
Epoch 132/300, seasonal_3 Loss: 0.0727 | 0.0589
Epoch 133/300, seasonal_3 Loss: 0.0726 | 0.0587
Epoch 134/300, seasonal_3 Loss: 0.0726 | 0.0585
Epoch 135/300, seasonal_3 Loss: 0.0725 | 0.0585
Epoch 136/300, seasonal_3 Loss: 0.0723 | 0.0585
Epoch 137/300, seasonal_3 Loss: 0.0722 | 0.0584
Epoch 138/300, seasonal_3 Loss: 0.0721 | 0.0584
Epoch 139/300, seasonal_3 Loss: 0.0720 | 0.0584
Epoch 140/300, seasonal_3 Loss: 0.0718 | 0.0583
Epoch 141/300, seasonal_3 Loss: 0.0717 | 0.0582
Epoch 142/300, seasonal_3 Loss: 0.0716 | 0.0580
Epoch 143/300, seasonal_3 Loss: 0.0715 | 0.0579
Epoch 144/300, seasonal_3 Loss: 0.0715 | 0.0579
Epoch 145/300, seasonal_3 Loss: 0.0714 | 0.0578
Epoch 146/300, seasonal_3 Loss: 0.0712 | 0.0578
Epoch 147/300, seasonal_3 Loss: 0.0711 | 0.0577
Epoch 148/300, seasonal_3 Loss: 0.0710 | 0.0577
Epoch 149/300, seasonal_3 Loss: 0.0709 | 0.0577
Epoch 150/300, seasonal_3 Loss: 0.0708 | 0.0576
Epoch 151/300, seasonal_3 Loss: 0.0707 | 0.0575
Epoch 152/300, seasonal_3 Loss: 0.0707 | 0.0574
Epoch 153/300, seasonal_3 Loss: 0.0706 | 0.0573
Epoch 154/300, seasonal_3 Loss: 0.0705 | 0.0573
Epoch 155/300, seasonal_3 Loss: 0.0704 | 0.0572
Epoch 156/300, seasonal_3 Loss: 0.0703 | 0.0572
Epoch 157/300, seasonal_3 Loss: 0.0702 | 0.0572
Epoch 158/300, seasonal_3 Loss: 0.0701 | 0.0571
Epoch 159/300, seasonal_3 Loss: 0.0700 | 0.0571
Epoch 160/300, seasonal_3 Loss: 0.0699 | 0.0570
Epoch 161/300, seasonal_3 Loss: 0.0698 | 0.0569
Epoch 162/300, seasonal_3 Loss: 0.0698 | 0.0569
Epoch 163/300, seasonal_3 Loss: 0.0697 | 0.0568
Epoch 164/300, seasonal_3 Loss: 0.0696 | 0.0568
Epoch 165/300, seasonal_3 Loss: 0.0695 | 0.0567
Epoch 166/300, seasonal_3 Loss: 0.0694 | 0.0567
Epoch 167/300, seasonal_3 Loss: 0.0694 | 0.0567
Epoch 168/300, seasonal_3 Loss: 0.0693 | 0.0566
Epoch 169/300, seasonal_3 Loss: 0.0692 | 0.0566
Epoch 170/300, seasonal_3 Loss: 0.0691 | 0.0566
Epoch 171/300, seasonal_3 Loss: 0.0690 | 0.0565
Epoch 172/300, seasonal_3 Loss: 0.0690 | 0.0564
Epoch 173/300, seasonal_3 Loss: 0.0689 | 0.0564
Epoch 174/300, seasonal_3 Loss: 0.0688 | 0.0563
Epoch 175/300, seasonal_3 Loss: 0.0687 | 0.0563
Epoch 176/300, seasonal_3 Loss: 0.0687 | 0.0563
Epoch 177/300, seasonal_3 Loss: 0.0686 | 0.0563
Epoch 178/300, seasonal_3 Loss: 0.0685 | 0.0562
Epoch 179/300, seasonal_3 Loss: 0.0684 | 0.0562
Epoch 180/300, seasonal_3 Loss: 0.0684 | 0.0562
Epoch 181/300, seasonal_3 Loss: 0.0683 | 0.0561
Epoch 182/300, seasonal_3 Loss: 0.0682 | 0.0561
Epoch 183/300, seasonal_3 Loss: 0.0682 | 0.0560
Epoch 184/300, seasonal_3 Loss: 0.0681 | 0.0560
Epoch 185/300, seasonal_3 Loss: 0.0680 | 0.0560
Epoch 186/300, seasonal_3 Loss: 0.0680 | 0.0559
Epoch 187/300, seasonal_3 Loss: 0.0679 | 0.0559
Epoch 188/300, seasonal_3 Loss: 0.0678 | 0.0559
Epoch 189/300, seasonal_3 Loss: 0.0677 | 0.0558
Epoch 190/300, seasonal_3 Loss: 0.0677 | 0.0558
Epoch 191/300, seasonal_3 Loss: 0.0676 | 0.0558
Epoch 192/300, seasonal_3 Loss: 0.0676 | 0.0557
Epoch 193/300, seasonal_3 Loss: 0.0675 | 0.0557
Epoch 194/300, seasonal_3 Loss: 0.0674 | 0.0557
Epoch 195/300, seasonal_3 Loss: 0.0674 | 0.0556
Epoch 196/300, seasonal_3 Loss: 0.0673 | 0.0556
Epoch 197/300, seasonal_3 Loss: 0.0672 | 0.0556
Epoch 198/300, seasonal_3 Loss: 0.0672 | 0.0556
Epoch 199/300, seasonal_3 Loss: 0.0671 | 0.0556
Epoch 200/300, seasonal_3 Loss: 0.0671 | 0.0555
Epoch 201/300, seasonal_3 Loss: 0.0670 | 0.0555
Epoch 202/300, seasonal_3 Loss: 0.0669 | 0.0555
Epoch 203/300, seasonal_3 Loss: 0.0669 | 0.0554
Epoch 204/300, seasonal_3 Loss: 0.0668 | 0.0554
Epoch 205/300, seasonal_3 Loss: 0.0668 | 0.0554
Epoch 206/300, seasonal_3 Loss: 0.0667 | 0.0554
Epoch 207/300, seasonal_3 Loss: 0.0667 | 0.0553
Epoch 208/300, seasonal_3 Loss: 0.0666 | 0.0553
Epoch 209/300, seasonal_3 Loss: 0.0666 | 0.0553
Epoch 210/300, seasonal_3 Loss: 0.0665 | 0.0553
Epoch 211/300, seasonal_3 Loss: 0.0664 | 0.0552
Epoch 212/300, seasonal_3 Loss: 0.0664 | 0.0552
Epoch 213/300, seasonal_3 Loss: 0.0663 | 0.0552
Epoch 214/300, seasonal_3 Loss: 0.0663 | 0.0552
Epoch 215/300, seasonal_3 Loss: 0.0662 | 0.0551
Epoch 216/300, seasonal_3 Loss: 0.0662 | 0.0551
Epoch 217/300, seasonal_3 Loss: 0.0661 | 0.0551
Epoch 218/300, seasonal_3 Loss: 0.0661 | 0.0551
Epoch 219/300, seasonal_3 Loss: 0.0660 | 0.0551
Epoch 220/300, seasonal_3 Loss: 0.0660 | 0.0550
Epoch 221/300, seasonal_3 Loss: 0.0659 | 0.0550
Epoch 222/300, seasonal_3 Loss: 0.0659 | 0.0550
Epoch 223/300, seasonal_3 Loss: 0.0658 | 0.0550
Epoch 224/300, seasonal_3 Loss: 0.0658 | 0.0550
Epoch 225/300, seasonal_3 Loss: 0.0657 | 0.0549
Epoch 226/300, seasonal_3 Loss: 0.0657 | 0.0549
Epoch 227/300, seasonal_3 Loss: 0.0656 | 0.0549
Epoch 228/300, seasonal_3 Loss: 0.0656 | 0.0549
Epoch 229/300, seasonal_3 Loss: 0.0655 | 0.0549
Epoch 230/300, seasonal_3 Loss: 0.0655 | 0.0549
Epoch 231/300, seasonal_3 Loss: 0.0655 | 0.0548
Epoch 232/300, seasonal_3 Loss: 0.0654 | 0.0548
Epoch 233/300, seasonal_3 Loss: 0.0654 | 0.0548
Epoch 234/300, seasonal_3 Loss: 0.0653 | 0.0548
Epoch 235/300, seasonal_3 Loss: 0.0653 | 0.0548
Epoch 236/300, seasonal_3 Loss: 0.0652 | 0.0548
Epoch 237/300, seasonal_3 Loss: 0.0652 | 0.0547
Epoch 238/300, seasonal_3 Loss: 0.0651 | 0.0547
Epoch 239/300, seasonal_3 Loss: 0.0651 | 0.0547
Epoch 240/300, seasonal_3 Loss: 0.0651 | 0.0547
Epoch 241/300, seasonal_3 Loss: 0.0650 | 0.0547
Epoch 242/300, seasonal_3 Loss: 0.0650 | 0.0547
Epoch 243/300, seasonal_3 Loss: 0.0649 | 0.0546
Epoch 244/300, seasonal_3 Loss: 0.0649 | 0.0546
Epoch 245/300, seasonal_3 Loss: 0.0649 | 0.0546
Epoch 246/300, seasonal_3 Loss: 0.0648 | 0.0546
Epoch 247/300, seasonal_3 Loss: 0.0648 | 0.0546
Epoch 248/300, seasonal_3 Loss: 0.0647 | 0.0546
Epoch 249/300, seasonal_3 Loss: 0.0647 | 0.0546
Epoch 250/300, seasonal_3 Loss: 0.0647 | 0.0545
Epoch 251/300, seasonal_3 Loss: 0.0646 | 0.0545
Epoch 252/300, seasonal_3 Loss: 0.0646 | 0.0545
Epoch 253/300, seasonal_3 Loss: 0.0645 | 0.0545
Epoch 254/300, seasonal_3 Loss: 0.0645 | 0.0545
Epoch 255/300, seasonal_3 Loss: 0.0645 | 0.0545
Epoch 256/300, seasonal_3 Loss: 0.0644 | 0.0545
Epoch 257/300, seasonal_3 Loss: 0.0644 | 0.0545
Epoch 258/300, seasonal_3 Loss: 0.0644 | 0.0544
Epoch 259/300, seasonal_3 Loss: 0.0643 | 0.0544
Epoch 260/300, seasonal_3 Loss: 0.0643 | 0.0544
Epoch 261/300, seasonal_3 Loss: 0.0643 | 0.0544
Epoch 262/300, seasonal_3 Loss: 0.0642 | 0.0544
Epoch 263/300, seasonal_3 Loss: 0.0642 | 0.0544
Epoch 264/300, seasonal_3 Loss: 0.0641 | 0.0544
Epoch 265/300, seasonal_3 Loss: 0.0641 | 0.0544
Epoch 266/300, seasonal_3 Loss: 0.0641 | 0.0544
Epoch 267/300, seasonal_3 Loss: 0.0640 | 0.0543
Epoch 268/300, seasonal_3 Loss: 0.0640 | 0.0543
Epoch 269/300, seasonal_3 Loss: 0.0640 | 0.0543
Epoch 270/300, seasonal_3 Loss: 0.0639 | 0.0543
Epoch 271/300, seasonal_3 Loss: 0.0639 | 0.0543
Epoch 272/300, seasonal_3 Loss: 0.0639 | 0.0543
Epoch 273/300, seasonal_3 Loss: 0.0639 | 0.0543
Epoch 274/300, seasonal_3 Loss: 0.0638 | 0.0543
Epoch 275/300, seasonal_3 Loss: 0.0638 | 0.0543
Epoch 276/300, seasonal_3 Loss: 0.0638 | 0.0542
Epoch 277/300, seasonal_3 Loss: 0.0637 | 0.0542
Epoch 278/300, seasonal_3 Loss: 0.0637 | 0.0542
Epoch 279/300, seasonal_3 Loss: 0.0637 | 0.0542
Epoch 280/300, seasonal_3 Loss: 0.0636 | 0.0542
Epoch 281/300, seasonal_3 Loss: 0.0636 | 0.0542
Epoch 282/300, seasonal_3 Loss: 0.0636 | 0.0542
Epoch 283/300, seasonal_3 Loss: 0.0636 | 0.0542
Epoch 284/300, seasonal_3 Loss: 0.0635 | 0.0542
Epoch 285/300, seasonal_3 Loss: 0.0635 | 0.0542
Epoch 286/300, seasonal_3 Loss: 0.0635 | 0.0541
Epoch 287/300, seasonal_3 Loss: 0.0634 | 0.0541
Epoch 288/300, seasonal_3 Loss: 0.0634 | 0.0541
Epoch 289/300, seasonal_3 Loss: 0.0634 | 0.0541
Epoch 290/300, seasonal_3 Loss: 0.0634 | 0.0541
Epoch 291/300, seasonal_3 Loss: 0.0633 | 0.0541
Epoch 292/300, seasonal_3 Loss: 0.0633 | 0.0541
Epoch 293/300, seasonal_3 Loss: 0.0633 | 0.0541
Epoch 294/300, seasonal_3 Loss: 0.0633 | 0.0541
Epoch 295/300, seasonal_3 Loss: 0.0632 | 0.0541
Epoch 296/300, seasonal_3 Loss: 0.0632 | 0.0541
Epoch 297/300, seasonal_3 Loss: 0.0632 | 0.0541
Epoch 298/300, seasonal_3 Loss: 0.0632 | 0.0541
Epoch 299/300, seasonal_3 Loss: 0.0631 | 0.0540
Epoch 300/300, seasonal_3 Loss: 0.0631 | 0.0540
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.8058462956656766, 'learning_rate': 3.6010324688498536e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7560436034379239}
Epoch 1/300, resid Loss: 0.3673 | 0.1806
Epoch 2/300, resid Loss: 0.1501 | 0.1418
Epoch 3/300, resid Loss: 0.1341 | 0.1118
Epoch 4/300, resid Loss: 0.1256 | 0.0937
Epoch 5/300, resid Loss: 0.1199 | 0.0852
Epoch 6/300, resid Loss: 0.1160 | 0.0817
Epoch 7/300, resid Loss: 0.1127 | 0.0799
Epoch 8/300, resid Loss: 0.1115 | 0.0781
Epoch 9/300, resid Loss: 0.1097 | 0.0767
Epoch 10/300, resid Loss: 0.1080 | 0.0753
Epoch 11/300, resid Loss: 0.1063 | 0.0738
Epoch 12/300, resid Loss: 0.1046 | 0.0710
Epoch 13/300, resid Loss: 0.1036 | 0.0698
Epoch 14/300, resid Loss: 0.1024 | 0.0686
Epoch 15/300, resid Loss: 0.1014 | 0.0674
Epoch 16/300, resid Loss: 0.1004 | 0.0662
Epoch 17/300, resid Loss: 0.0995 | 0.0650
Epoch 18/300, resid Loss: 0.0984 | 0.0634
Epoch 19/300, resid Loss: 0.0976 | 0.0625
Epoch 20/300, resid Loss: 0.0968 | 0.0617
Epoch 21/300, resid Loss: 0.0961 | 0.0608
Epoch 22/300, resid Loss: 0.0953 | 0.0600
Epoch 23/300, resid Loss: 0.0946 | 0.0589
Epoch 24/300, resid Loss: 0.0940 | 0.0583
Epoch 25/300, resid Loss: 0.0934 | 0.0578
Epoch 26/300, resid Loss: 0.0928 | 0.0572
Epoch 27/300, resid Loss: 0.0923 | 0.0567
Epoch 28/300, resid Loss: 0.0918 | 0.0562
Epoch 29/300, resid Loss: 0.0913 | 0.0560
Epoch 30/300, resid Loss: 0.0909 | 0.0556
Epoch 31/300, resid Loss: 0.0905 | 0.0552
Epoch 32/300, resid Loss: 0.0901 | 0.0548
Epoch 33/300, resid Loss: 0.0898 | 0.0545
Epoch 34/300, resid Loss: 0.0894 | 0.0545
Epoch 35/300, resid Loss: 0.0892 | 0.0541
Epoch 36/300, resid Loss: 0.0889 | 0.0538
Epoch 37/300, resid Loss: 0.0886 | 0.0534
Epoch 38/300, resid Loss: 0.0883 | 0.0530
Epoch 39/300, resid Loss: 0.0880 | 0.0527
Epoch 40/300, resid Loss: 0.0878 | 0.0527
Epoch 41/300, resid Loss: 0.0876 | 0.0524
Epoch 42/300, resid Loss: 0.0873 | 0.0521
Epoch 43/300, resid Loss: 0.0871 | 0.0518
Epoch 44/300, resid Loss: 0.0869 | 0.0515
Epoch 45/300, resid Loss: 0.0867 | 0.0515
Epoch 46/300, resid Loss: 0.0866 | 0.0513
Epoch 47/300, resid Loss: 0.0864 | 0.0510
Epoch 48/300, resid Loss: 0.0862 | 0.0508
Epoch 49/300, resid Loss: 0.0861 | 0.0505
Epoch 50/300, resid Loss: 0.0859 | 0.0503
Epoch 51/300, resid Loss: 0.0857 | 0.0503
Epoch 52/300, resid Loss: 0.0856 | 0.0501
Epoch 53/300, resid Loss: 0.0855 | 0.0499
Epoch 54/300, resid Loss: 0.0854 | 0.0498
Epoch 55/300, resid Loss: 0.0853 | 0.0496
Epoch 56/300, resid Loss: 0.0851 | 0.0496
Epoch 57/300, resid Loss: 0.0851 | 0.0495
Epoch 58/300, resid Loss: 0.0850 | 0.0493
Epoch 59/300, resid Loss: 0.0849 | 0.0492
Epoch 60/300, resid Loss: 0.0848 | 0.0490
Epoch 61/300, resid Loss: 0.0847 | 0.0489
Epoch 62/300, resid Loss: 0.0846 | 0.0488
Epoch 63/300, resid Loss: 0.0845 | 0.0487
Epoch 64/300, resid Loss: 0.0844 | 0.0486
Epoch 65/300, resid Loss: 0.0844 | 0.0485
Epoch 66/300, resid Loss: 0.0843 | 0.0484
Epoch 67/300, resid Loss: 0.0842 | 0.0483
Epoch 68/300, resid Loss: 0.0842 | 0.0482
Epoch 69/300, resid Loss: 0.0841 | 0.0481
Epoch 70/300, resid Loss: 0.0841 | 0.0480
Epoch 71/300, resid Loss: 0.0840 | 0.0480
Epoch 72/300, resid Loss: 0.0840 | 0.0479
Epoch 73/300, resid Loss: 0.0839 | 0.0478
Epoch 74/300, resid Loss: 0.0838 | 0.0477
Epoch 75/300, resid Loss: 0.0838 | 0.0477
Epoch 76/300, resid Loss: 0.0838 | 0.0476
Epoch 77/300, resid Loss: 0.0837 | 0.0476
Epoch 78/300, resid Loss: 0.0837 | 0.0475
Epoch 79/300, resid Loss: 0.0836 | 0.0474
Epoch 80/300, resid Loss: 0.0836 | 0.0474
Epoch 81/300, resid Loss: 0.0836 | 0.0474
Epoch 82/300, resid Loss: 0.0836 | 0.0473
Epoch 83/300, resid Loss: 0.0835 | 0.0473
Epoch 84/300, resid Loss: 0.0835 | 0.0472
Epoch 85/300, resid Loss: 0.0835 | 0.0472
Epoch 86/300, resid Loss: 0.0834 | 0.0472
Epoch 87/300, resid Loss: 0.0834 | 0.0471
Epoch 88/300, resid Loss: 0.0834 | 0.0471
Epoch 89/300, resid Loss: 0.0834 | 0.0471
Epoch 90/300, resid Loss: 0.0834 | 0.0470
Epoch 91/300, resid Loss: 0.0833 | 0.0470
Epoch 92/300, resid Loss: 0.0833 | 0.0470
Epoch 93/300, resid Loss: 0.0833 | 0.0469
Epoch 94/300, resid Loss: 0.0833 | 0.0469
Epoch 95/300, resid Loss: 0.0833 | 0.0469
Epoch 96/300, resid Loss: 0.0833 | 0.0469
Epoch 97/300, resid Loss: 0.0832 | 0.0468
Epoch 98/300, resid Loss: 0.0832 | 0.0468
Epoch 99/300, resid Loss: 0.0832 | 0.0468
Epoch 100/300, resid Loss: 0.0832 | 0.0468
Epoch 101/300, resid Loss: 0.0832 | 0.0468
Epoch 102/300, resid Loss: 0.0832 | 0.0467
Epoch 103/300, resid Loss: 0.0832 | 0.0467
Epoch 104/300, resid Loss: 0.0832 | 0.0467
Epoch 105/300, resid Loss: 0.0831 | 0.0467
Epoch 106/300, resid Loss: 0.0831 | 0.0467
Epoch 107/300, resid Loss: 0.0831 | 0.0467
Epoch 108/300, resid Loss: 0.0831 | 0.0467
Epoch 109/300, resid Loss: 0.0831 | 0.0466
Epoch 110/300, resid Loss: 0.0831 | 0.0466
Epoch 111/300, resid Loss: 0.0831 | 0.0466
Epoch 112/300, resid Loss: 0.0831 | 0.0466
Epoch 113/300, resid Loss: 0.0831 | 0.0466
Epoch 114/300, resid Loss: 0.0831 | 0.0466
Epoch 115/300, resid Loss: 0.0831 | 0.0466
Epoch 116/300, resid Loss: 0.0831 | 0.0466
Epoch 117/300, resid Loss: 0.0831 | 0.0466
Epoch 118/300, resid Loss: 0.0830 | 0.0466
Epoch 119/300, resid Loss: 0.0830 | 0.0466
Epoch 120/300, resid Loss: 0.0830 | 0.0466
Epoch 121/300, resid Loss: 0.0830 | 0.0465
Epoch 122/300, resid Loss: 0.0830 | 0.0465
Epoch 123/300, resid Loss: 0.0830 | 0.0465
Epoch 124/300, resid Loss: 0.0830 | 0.0465
Epoch 125/300, resid Loss: 0.0830 | 0.0465
Epoch 126/300, resid Loss: 0.0830 | 0.0465
Epoch 127/300, resid Loss: 0.0830 | 0.0465
Epoch 128/300, resid Loss: 0.0830 | 0.0465
Epoch 129/300, resid Loss: 0.0830 | 0.0465
Epoch 130/300, resid Loss: 0.0830 | 0.0465
Epoch 131/300, resid Loss: 0.0830 | 0.0465
Epoch 132/300, resid Loss: 0.0830 | 0.0465
Epoch 133/300, resid Loss: 0.0830 | 0.0465
Epoch 134/300, resid Loss: 0.0830 | 0.0465
Epoch 135/300, resid Loss: 0.0830 | 0.0465
Epoch 136/300, resid Loss: 0.0830 | 0.0465
Epoch 137/300, resid Loss: 0.0830 | 0.0465
Epoch 138/300, resid Loss: 0.0830 | 0.0465
Epoch 139/300, resid Loss: 0.0830 | 0.0465
Epoch 140/300, resid Loss: 0.0830 | 0.0465
Epoch 141/300, resid Loss: 0.0830 | 0.0465
Epoch 142/300, resid Loss: 0.0830 | 0.0465
Epoch 143/300, resid Loss: 0.0830 | 0.0465
Epoch 144/300, resid Loss: 0.0830 | 0.0465
Epoch 145/300, resid Loss: 0.0830 | 0.0465
Epoch 146/300, resid Loss: 0.0830 | 0.0465
Epoch 147/300, resid Loss: 0.0830 | 0.0465
Epoch 148/300, resid Loss: 0.0830 | 0.0465
Epoch 149/300, resid Loss: 0.0830 | 0.0465
Epoch 150/300, resid Loss: 0.0830 | 0.0465
Epoch 151/300, resid Loss: 0.0830 | 0.0465
Epoch 152/300, resid Loss: 0.0830 | 0.0465
Epoch 153/300, resid Loss: 0.0830 | 0.0465
Epoch 154/300, resid Loss: 0.0830 | 0.0465
Epoch 155/300, resid Loss: 0.0830 | 0.0465
Epoch 156/300, resid Loss: 0.0830 | 0.0465
Epoch 157/300, resid Loss: 0.0830 | 0.0465
Epoch 158/300, resid Loss: 0.0830 | 0.0465
Epoch 159/300, resid Loss: 0.0830 | 0.0465
Epoch 160/300, resid Loss: 0.0830 | 0.0465
Epoch 161/300, resid Loss: 0.0830 | 0.0465
Epoch 162/300, resid Loss: 0.0830 | 0.0465
Epoch 163/300, resid Loss: 0.0830 | 0.0465
Epoch 164/300, resid Loss: 0.0830 | 0.0465
Epoch 165/300, resid Loss: 0.0830 | 0.0465
Epoch 166/300, resid Loss: 0.0830 | 0.0465
Epoch 167/300, resid Loss: 0.0829 | 0.0465
Epoch 168/300, resid Loss: 0.0829 | 0.0465
Epoch 169/300, resid Loss: 0.0829 | 0.0464
Epoch 170/300, resid Loss: 0.0829 | 0.0464
Epoch 171/300, resid Loss: 0.0829 | 0.0464
Epoch 172/300, resid Loss: 0.0829 | 0.0464
Epoch 173/300, resid Loss: 0.0829 | 0.0464
Epoch 174/300, resid Loss: 0.0829 | 0.0464
Epoch 175/300, resid Loss: 0.0829 | 0.0464
Epoch 176/300, resid Loss: 0.0829 | 0.0464
Epoch 177/300, resid Loss: 0.0829 | 0.0464
Epoch 178/300, resid Loss: 0.0829 | 0.0464
Epoch 179/300, resid Loss: 0.0829 | 0.0464
Epoch 180/300, resid Loss: 0.0829 | 0.0464
Epoch 181/300, resid Loss: 0.0829 | 0.0464
Epoch 182/300, resid Loss: 0.0829 | 0.0464
Epoch 183/300, resid Loss: 0.0829 | 0.0464
Epoch 184/300, resid Loss: 0.0829 | 0.0464
Epoch 185/300, resid Loss: 0.0829 | 0.0464
Epoch 186/300, resid Loss: 0.0829 | 0.0464
Epoch 187/300, resid Loss: 0.0829 | 0.0464
Epoch 188/300, resid Loss: 0.0829 | 0.0464
Epoch 189/300, resid Loss: 0.0829 | 0.0464
Epoch 190/300, resid Loss: 0.0829 | 0.0464
Epoch 191/300, resid Loss: 0.0829 | 0.0464
Epoch 192/300, resid Loss: 0.0829 | 0.0464
Epoch 193/300, resid Loss: 0.0829 | 0.0464
Epoch 194/300, resid Loss: 0.0829 | 0.0464
Epoch 195/300, resid Loss: 0.0829 | 0.0464
Epoch 196/300, resid Loss: 0.0829 | 0.0464
Epoch 197/300, resid Loss: 0.0829 | 0.0464
Epoch 198/300, resid Loss: 0.0829 | 0.0464
Epoch 199/300, resid Loss: 0.0829 | 0.0464
Epoch 200/300, resid Loss: 0.0829 | 0.0464
Epoch 201/300, resid Loss: 0.0829 | 0.0464
Epoch 202/300, resid Loss: 0.0829 | 0.0464
Epoch 203/300, resid Loss: 0.0829 | 0.0464
Epoch 204/300, resid Loss: 0.0829 | 0.0464
Epoch 205/300, resid Loss: 0.0829 | 0.0464
Epoch 206/300, resid Loss: 0.0829 | 0.0464
Epoch 207/300, resid Loss: 0.0829 | 0.0464
Epoch 208/300, resid Loss: 0.0829 | 0.0464
Epoch 209/300, resid Loss: 0.0829 | 0.0464
Epoch 210/300, resid Loss: 0.0829 | 0.0464
Epoch 211/300, resid Loss: 0.0829 | 0.0464
Epoch 212/300, resid Loss: 0.0829 | 0.0464
Epoch 213/300, resid Loss: 0.0829 | 0.0464
Epoch 214/300, resid Loss: 0.0829 | 0.0464
Epoch 215/300, resid Loss: 0.0829 | 0.0464
Epoch 216/300, resid Loss: 0.0829 | 0.0464
Epoch 217/300, resid Loss: 0.0829 | 0.0464
Epoch 218/300, resid Loss: 0.0829 | 0.0464
Epoch 219/300, resid Loss: 0.0829 | 0.0464
Epoch 220/300, resid Loss: 0.0829 | 0.0464
Epoch 221/300, resid Loss: 0.0829 | 0.0464
Epoch 222/300, resid Loss: 0.0829 | 0.0464
Epoch 223/300, resid Loss: 0.0829 | 0.0464
Epoch 224/300, resid Loss: 0.0829 | 0.0464
Epoch 225/300, resid Loss: 0.0829 | 0.0464
Epoch 226/300, resid Loss: 0.0829 | 0.0464
Epoch 227/300, resid Loss: 0.0829 | 0.0464
Epoch 228/300, resid Loss: 0.0829 | 0.0464
Epoch 229/300, resid Loss: 0.0829 | 0.0464
Epoch 230/300, resid Loss: 0.0829 | 0.0464
Epoch 231/300, resid Loss: 0.0829 | 0.0464
Epoch 232/300, resid Loss: 0.0829 | 0.0464
Epoch 233/300, resid Loss: 0.0829 | 0.0464
Epoch 234/300, resid Loss: 0.0829 | 0.0464
Epoch 235/300, resid Loss: 0.0829 | 0.0464
Epoch 236/300, resid Loss: 0.0829 | 0.0464
Epoch 237/300, resid Loss: 0.0829 | 0.0464
Epoch 238/300, resid Loss: 0.0829 | 0.0464
Epoch 239/300, resid Loss: 0.0829 | 0.0464
Epoch 240/300, resid Loss: 0.0829 | 0.0464
Epoch 241/300, resid Loss: 0.0829 | 0.0464
Epoch 242/300, resid Loss: 0.0829 | 0.0464
Epoch 243/300, resid Loss: 0.0829 | 0.0464
Epoch 244/300, resid Loss: 0.0829 | 0.0464
Epoch 245/300, resid Loss: 0.0829 | 0.0464
Epoch 246/300, resid Loss: 0.0829 | 0.0464
Epoch 247/300, resid Loss: 0.0829 | 0.0464
Epoch 248/300, resid Loss: 0.0829 | 0.0464
Epoch 249/300, resid Loss: 0.0829 | 0.0464
Epoch 250/300, resid Loss: 0.0829 | 0.0464
Epoch 251/300, resid Loss: 0.0829 | 0.0464
Epoch 252/300, resid Loss: 0.0829 | 0.0464
Epoch 253/300, resid Loss: 0.0829 | 0.0464
Epoch 254/300, resid Loss: 0.0829 | 0.0464
Epoch 255/300, resid Loss: 0.0829 | 0.0464
Epoch 256/300, resid Loss: 0.0829 | 0.0464
Epoch 257/300, resid Loss: 0.0829 | 0.0464
Epoch 258/300, resid Loss: 0.0829 | 0.0464
Epoch 259/300, resid Loss: 0.0829 | 0.0464
Epoch 260/300, resid Loss: 0.0829 | 0.0464
Epoch 261/300, resid Loss: 0.0829 | 0.0464
Epoch 262/300, resid Loss: 0.0829 | 0.0464
Epoch 263/300, resid Loss: 0.0829 | 0.0464
Epoch 264/300, resid Loss: 0.0829 | 0.0464
Epoch 265/300, resid Loss: 0.0829 | 0.0464
Epoch 266/300, resid Loss: 0.0829 | 0.0464
Epoch 267/300, resid Loss: 0.0829 | 0.0464
Epoch 268/300, resid Loss: 0.0829 | 0.0464
Epoch 269/300, resid Loss: 0.0829 | 0.0464
Epoch 270/300, resid Loss: 0.0829 | 0.0464
Epoch 271/300, resid Loss: 0.0829 | 0.0464
Epoch 272/300, resid Loss: 0.0829 | 0.0464
Epoch 273/300, resid Loss: 0.0829 | 0.0464
Epoch 274/300, resid Loss: 0.0829 | 0.0464
Epoch 275/300, resid Loss: 0.0829 | 0.0464
Epoch 276/300, resid Loss: 0.0829 | 0.0464
Epoch 277/300, resid Loss: 0.0829 | 0.0464
Epoch 278/300, resid Loss: 0.0829 | 0.0464
Epoch 279/300, resid Loss: 0.0829 | 0.0464
Epoch 280/300, resid Loss: 0.0829 | 0.0464
Epoch 281/300, resid Loss: 0.0829 | 0.0464
Epoch 282/300, resid Loss: 0.0829 | 0.0464
Epoch 283/300, resid Loss: 0.0829 | 0.0464
Epoch 284/300, resid Loss: 0.0829 | 0.0464
Epoch 285/300, resid Loss: 0.0829 | 0.0464
Epoch 286/300, resid Loss: 0.0829 | 0.0464
Epoch 287/300, resid Loss: 0.0829 | 0.0464
Epoch 288/300, resid Loss: 0.0829 | 0.0464
Epoch 289/300, resid Loss: 0.0829 | 0.0464
Epoch 290/300, resid Loss: 0.0829 | 0.0464
Epoch 291/300, resid Loss: 0.0829 | 0.0464
Epoch 292/300, resid Loss: 0.0829 | 0.0464
Epoch 293/300, resid Loss: 0.0829 | 0.0464
Epoch 294/300, resid Loss: 0.0829 | 0.0464
Epoch 295/300, resid Loss: 0.0829 | 0.0464
Epoch 296/300, resid Loss: 0.0829 | 0.0464
Epoch 297/300, resid Loss: 0.0829 | 0.0464
Epoch 298/300, resid Loss: 0.0829 | 0.0464
Epoch 299/300, resid Loss: 0.0829 | 0.0464
Epoch 300/300, resid Loss: 0.0829 | 0.0464
Runtime (seconds): 1567.7570116519928
0.0009757295617322969
[150.80226]
[0.6074385]
[-4.031305]
[11.4681]
[3.9957697]
[9.654425]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 10.619010012829676
RMSE: 3.2586822509765625
MAE: 3.2586822509765625
R-squared: nan
[172.49667]
