ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 03:12:01,250][0m A new study created in memory with name: no-name-efa177fe-7152-4f34-987d-8afab7cb7c16[0m
[32m[I 2025-01-06 03:12:47,946][0m Trial 0 finished with value: 1.0258071280761378 and parameters: {'observation_period_num': 126, 'train_rates': 0.6995991431111915, 'learning_rate': 1.1573891583457269e-06, 'batch_size': 232, 'step_size': 5, 'gamma': 0.8574927808828021}. Best is trial 0 with value: 1.0258071280761378.[0m
[32m[I 2025-01-06 03:14:20,601][0m Trial 1 finished with value: 0.2421606746652434 and parameters: {'observation_period_num': 215, 'train_rates': 0.7282481803078871, 'learning_rate': 1.1637672183054893e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9690303435749004}. Best is trial 1 with value: 0.2421606746652434.[0m
[32m[I 2025-01-06 03:15:37,744][0m Trial 2 finished with value: 0.41776157302014966 and parameters: {'observation_period_num': 126, 'train_rates': 0.8078220508836292, 'learning_rate': 4.2203654057767055e-06, 'batch_size': 164, 'step_size': 13, 'gamma': 0.9041697707881557}. Best is trial 1 with value: 0.2421606746652434.[0m
[32m[I 2025-01-06 03:16:36,928][0m Trial 3 finished with value: 0.2059872642110391 and parameters: {'observation_period_num': 221, 'train_rates': 0.6665732885501745, 'learning_rate': 0.00014014245067431468, 'batch_size': 128, 'step_size': 7, 'gamma': 0.8104014199948996}. Best is trial 3 with value: 0.2059872642110391.[0m
[32m[I 2025-01-06 03:17:33,254][0m Trial 4 finished with value: 0.3844035863876343 and parameters: {'observation_period_num': 116, 'train_rates': 0.954608684287109, 'learning_rate': 4.383294916682865e-05, 'batch_size': 182, 'step_size': 13, 'gamma': 0.9433696487263186}. Best is trial 3 with value: 0.2059872642110391.[0m
[32m[I 2025-01-06 03:18:53,963][0m Trial 5 finished with value: 0.11091299610028804 and parameters: {'observation_period_num': 140, 'train_rates': 0.7478354364903037, 'learning_rate': 0.00034273818290589365, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9268426043283245}. Best is trial 5 with value: 0.11091299610028804.[0m
[32m[I 2025-01-06 03:20:09,054][0m Trial 6 finished with value: 0.1374196646359598 and parameters: {'observation_period_num': 152, 'train_rates': 0.8193197611931957, 'learning_rate': 0.00017970463831838338, 'batch_size': 150, 'step_size': 10, 'gamma': 0.867240677362693}. Best is trial 5 with value: 0.11091299610028804.[0m
[32m[I 2025-01-06 03:21:44,996][0m Trial 7 finished with value: 0.20237524169790852 and parameters: {'observation_period_num': 229, 'train_rates': 0.6333360252876157, 'learning_rate': 0.00018412512135146478, 'batch_size': 59, 'step_size': 6, 'gamma': 0.9740836523215247}. Best is trial 5 with value: 0.11091299610028804.[0m
[32m[I 2025-01-06 03:23:03,163][0m Trial 8 finished with value: 0.12289865873754025 and parameters: {'observation_period_num': 116, 'train_rates': 0.6982910686726376, 'learning_rate': 0.0005141549396489169, 'batch_size': 130, 'step_size': 14, 'gamma': 0.9148981135369523}. Best is trial 5 with value: 0.11091299610028804.[0m
[32m[I 2025-01-06 03:26:05,434][0m Trial 9 finished with value: 0.0856209453837625 and parameters: {'observation_period_num': 84, 'train_rates': 0.9597829529397552, 'learning_rate': 5.467404009082757e-05, 'batch_size': 36, 'step_size': 5, 'gamma': 0.7916101126181073}. Best is trial 9 with value: 0.0856209453837625.[0m
Early stopping at epoch 90
[32m[I 2025-01-06 03:29:24,132][0m Trial 10 finished with value: 0.05819651884276692 and parameters: {'observation_period_num': 9, 'train_rates': 0.9861221218389284, 'learning_rate': 3.382361784089473e-05, 'batch_size': 31, 'step_size': 2, 'gamma': 0.7535338207681571}. Best is trial 10 with value: 0.05819651884276692.[0m
Early stopping at epoch 55
[32m[I 2025-01-06 03:33:18,052][0m Trial 11 finished with value: 0.06208260643940706 and parameters: {'observation_period_num': 5, 'train_rates': 0.9825511149138713, 'learning_rate': 3.409137058188738e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.7626121919131222}. Best is trial 10 with value: 0.05819651884276692.[0m
Early stopping at epoch 51
[32m[I 2025-01-06 03:35:44,075][0m Trial 12 finished with value: 0.23106939646683328 and parameters: {'observation_period_num': 13, 'train_rates': 0.9007528274901049, 'learning_rate': 1.2458375018983995e-05, 'batch_size': 26, 'step_size': 1, 'gamma': 0.7524224656287573}. Best is trial 10 with value: 0.05819651884276692.[0m
Early stopping at epoch 42
[32m[I 2025-01-06 03:36:44,775][0m Trial 13 finished with value: 0.8772873200342327 and parameters: {'observation_period_num': 9, 'train_rates': 0.8867427399736488, 'learning_rate': 1.6283461840681668e-05, 'batch_size': 96, 'step_size': 1, 'gamma': 0.7503709424833939}. Best is trial 10 with value: 0.05819651884276692.[0m
[32m[I 2025-01-06 03:38:25,762][0m Trial 14 finished with value: 0.14195550978183746 and parameters: {'observation_period_num': 59, 'train_rates': 0.9892496065327226, 'learning_rate': 6.611042989270855e-05, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8016818025925462}. Best is trial 10 with value: 0.05819651884276692.[0m
[32m[I 2025-01-06 03:43:27,905][0m Trial 15 finished with value: 0.1939710959886151 and parameters: {'observation_period_num': 45, 'train_rates': 0.8912005393512531, 'learning_rate': 4.8568719447765e-06, 'batch_size': 21, 'step_size': 3, 'gamma': 0.8350383523884054}. Best is trial 10 with value: 0.05819651884276692.[0m
[32m[I 2025-01-06 03:44:42,431][0m Trial 16 finished with value: 0.30728033967147583 and parameters: {'observation_period_num': 48, 'train_rates': 0.9242342796278138, 'learning_rate': 2.224543565928142e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.7781687382787176}. Best is trial 10 with value: 0.05819651884276692.[0m
Early stopping at epoch 72
[32m[I 2025-01-06 03:46:24,139][0m Trial 17 finished with value: 0.15661141212191879 and parameters: {'observation_period_num': 179, 'train_rates': 0.8494982420910264, 'learning_rate': 0.0009652376604365616, 'batch_size': 56, 'step_size': 1, 'gamma': 0.7715652926681187}. Best is trial 10 with value: 0.05819651884276692.[0m
[32m[I 2025-01-06 03:53:09,641][0m Trial 18 finished with value: 0.21045047640800477 and parameters: {'observation_period_num': 79, 'train_rates': 0.988001948426019, 'learning_rate': 6.635698108536865e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8282089544522067}. Best is trial 10 with value: 0.05819651884276692.[0m
[32m[I 2025-01-06 03:54:18,175][0m Trial 19 finished with value: 0.05004096093419215 and parameters: {'observation_period_num': 26, 'train_rates': 0.8507271807217399, 'learning_rate': 8.86910970868032e-05, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8889266628266626}. Best is trial 19 with value: 0.05004096093419215.[0m
[32m[I 2025-01-06 03:55:13,447][0m Trial 20 finished with value: 0.053912226704255946 and parameters: {'observation_period_num': 29, 'train_rates': 0.7746581700533701, 'learning_rate': 9.119117501600844e-05, 'batch_size': 238, 'step_size': 9, 'gamma': 0.8874408459018662}. Best is trial 19 with value: 0.05004096093419215.[0m
[32m[I 2025-01-06 03:56:28,213][0m Trial 21 finished with value: 0.05517124427108882 and parameters: {'observation_period_num': 30, 'train_rates': 0.76306608554664, 'learning_rate': 0.00010000483096766211, 'batch_size': 251, 'step_size': 9, 'gamma': 0.8956299083755176}. Best is trial 19 with value: 0.05004096093419215.[0m
[32m[I 2025-01-06 03:57:25,085][0m Trial 22 finished with value: 0.06194414901335159 and parameters: {'observation_period_num': 34, 'train_rates': 0.766657588133078, 'learning_rate': 0.00010067534337834026, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8934650361360661}. Best is trial 19 with value: 0.05004096093419215.[0m
[32m[I 2025-01-06 03:58:40,817][0m Trial 23 finished with value: 0.08609191507778384 and parameters: {'observation_period_num': 79, 'train_rates': 0.8468608477986205, 'learning_rate': 9.08855034006818e-05, 'batch_size': 221, 'step_size': 8, 'gamma': 0.8849850602203814}. Best is trial 19 with value: 0.05004096093419215.[0m
[32m[I 2025-01-06 03:59:55,479][0m Trial 24 finished with value: 0.04094446405211342 and parameters: {'observation_period_num': 31, 'train_rates': 0.7803501103130773, 'learning_rate': 0.00035387896627647877, 'batch_size': 254, 'step_size': 11, 'gamma': 0.8538778669509042}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:00:34,408][0m Trial 25 finished with value: 0.1365306720240897 and parameters: {'observation_period_num': 252, 'train_rates': 0.7864001270724285, 'learning_rate': 0.0003066621467043234, 'batch_size': 217, 'step_size': 11, 'gamma': 0.8454710136807909}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:01:52,274][0m Trial 26 finished with value: 0.07496436750148952 and parameters: {'observation_period_num': 64, 'train_rates': 0.8508415835292754, 'learning_rate': 0.0008767356353756565, 'batch_size': 239, 'step_size': 7, 'gamma': 0.8793014886582219}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:02:52,042][0m Trial 27 finished with value: 0.07628597520320242 and parameters: {'observation_period_num': 100, 'train_rates': 0.8165686887786332, 'learning_rate': 0.00039503987000303253, 'batch_size': 203, 'step_size': 15, 'gamma': 0.8624496699679194}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:03:42,605][0m Trial 28 finished with value: 0.0417386507999138 and parameters: {'observation_period_num': 27, 'train_rates': 0.7261544360278492, 'learning_rate': 0.00022358704359782067, 'batch_size': 192, 'step_size': 11, 'gamma': 0.9393387663605828}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:04:37,603][0m Trial 29 finished with value: 0.0820373606925108 and parameters: {'observation_period_num': 62, 'train_rates': 0.7120808632020451, 'learning_rate': 0.00021031264632894183, 'batch_size': 184, 'step_size': 12, 'gamma': 0.9424722908304132}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:05:42,584][0m Trial 30 finished with value: 0.0628655806623422 and parameters: {'observation_period_num': 30, 'train_rates': 0.6639257754965492, 'learning_rate': 0.0004681602919526426, 'batch_size': 213, 'step_size': 11, 'gamma': 0.9537005455708693}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:07:01,592][0m Trial 31 finished with value: 0.0449364882896674 and parameters: {'observation_period_num': 25, 'train_rates': 0.7741651201201254, 'learning_rate': 0.000270424996897321, 'batch_size': 236, 'step_size': 9, 'gamma': 0.8545721047528172}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:07:42,194][0m Trial 32 finished with value: 1.306384336261849 and parameters: {'observation_period_num': 45, 'train_rates': 0.73740146255195, 'learning_rate': 1.0575061370288541e-06, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8436850483505758}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:08:54,267][0m Trial 33 finished with value: 0.05746254827394042 and parameters: {'observation_period_num': 21, 'train_rates': 0.7948266279860767, 'learning_rate': 0.000601240176252009, 'batch_size': 228, 'step_size': 8, 'gamma': 0.9881129171021859}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:09:38,581][0m Trial 34 finished with value: 0.08208686757361752 and parameters: {'observation_period_num': 44, 'train_rates': 0.700877682237691, 'learning_rate': 0.0002602942816607517, 'batch_size': 238, 'step_size': 11, 'gamma': 0.8198284458469325}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:10:38,397][0m Trial 35 finished with value: 0.07809609031206684 and parameters: {'observation_period_num': 98, 'train_rates': 0.8336258299040157, 'learning_rate': 0.0001304638640989138, 'batch_size': 181, 'step_size': 7, 'gamma': 0.9125535786771306}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:11:40,514][0m Trial 36 finished with value: 0.8024529312562472 and parameters: {'observation_period_num': 168, 'train_rates': 0.6730187156362825, 'learning_rate': 2.277005422257e-06, 'batch_size': 191, 'step_size': 12, 'gamma': 0.8559164284811203}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:12:44,613][0m Trial 37 finished with value: 0.056017003111759486 and parameters: {'observation_period_num': 60, 'train_rates': 0.7522423555075137, 'learning_rate': 0.00014845654842360537, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8741071256169013}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:13:37,126][0m Trial 38 finished with value: 0.041217150563587907 and parameters: {'observation_period_num': 23, 'train_rates': 0.8751364759759244, 'learning_rate': 0.00025636556883282323, 'batch_size': 207, 'step_size': 10, 'gamma': 0.9269818085697392}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:14:49,506][0m Trial 39 finished with value: 0.05141267988203776 and parameters: {'observation_period_num': 19, 'train_rates': 0.7246038140431907, 'learning_rate': 0.0007049501296190661, 'batch_size': 168, 'step_size': 10, 'gamma': 0.9301982303985414}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:15:46,308][0m Trial 40 finished with value: 0.0711329083319183 and parameters: {'observation_period_num': 95, 'train_rates': 0.8059204608614478, 'learning_rate': 0.0002582286619773069, 'batch_size': 205, 'step_size': 13, 'gamma': 0.9607056174691583}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:16:55,011][0m Trial 41 finished with value: 0.05668224182224622 and parameters: {'observation_period_num': 22, 'train_rates': 0.859788693587981, 'learning_rate': 0.00037196884718904524, 'batch_size': 230, 'step_size': 9, 'gamma': 0.9281239708104652}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:17:53,379][0m Trial 42 finished with value: 0.07270331341842441 and parameters: {'observation_period_num': 37, 'train_rates': 0.8709890780293293, 'learning_rate': 0.00019801346118864243, 'batch_size': 245, 'step_size': 11, 'gamma': 0.9133090311049082}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:18:59,390][0m Trial 43 finished with value: 0.10092361271381378 and parameters: {'observation_period_num': 55, 'train_rates': 0.9256639195069223, 'learning_rate': 5.65399013116207e-05, 'batch_size': 224, 'step_size': 10, 'gamma': 0.9373790137226268}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:19:53,014][0m Trial 44 finished with value: 0.08383231444878782 and parameters: {'observation_period_num': 5, 'train_rates': 0.6013496857049232, 'learning_rate': 0.00013914386502340332, 'batch_size': 213, 'step_size': 6, 'gamma': 0.9024476565767352}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:20:38,939][0m Trial 45 finished with value: 0.05359568001744085 and parameters: {'observation_period_num': 68, 'train_rates': 0.7831849276167855, 'learning_rate': 0.0002514944228626649, 'batch_size': 143, 'step_size': 8, 'gamma': 0.8519549264113808}. Best is trial 24 with value: 0.04094446405211342.[0m
[32m[I 2025-01-06 04:21:27,279][0m Trial 46 finished with value: 0.03490870201721034 and parameters: {'observation_period_num': 20, 'train_rates': 0.82498963871814, 'learning_rate': 0.0005517853734017093, 'batch_size': 198, 'step_size': 14, 'gamma': 0.875121809239049}. Best is trial 46 with value: 0.03490870201721034.[0m
[32m[I 2025-01-06 04:22:09,276][0m Trial 47 finished with value: 0.04457113129752023 and parameters: {'observation_period_num': 15, 'train_rates': 0.8215283597276034, 'learning_rate': 0.0006793696638109107, 'batch_size': 174, 'step_size': 14, 'gamma': 0.8706046861732322}. Best is trial 46 with value: 0.03490870201721034.[0m
[32m[I 2025-01-06 04:22:52,760][0m Trial 48 finished with value: 0.027319840021400785 and parameters: {'observation_period_num': 14, 'train_rates': 0.8347021708328278, 'learning_rate': 0.0005705938137745586, 'batch_size': 173, 'step_size': 15, 'gamma': 0.86739050385293}. Best is trial 48 with value: 0.027319840021400785.[0m
[32m[I 2025-01-06 04:23:38,983][0m Trial 49 finished with value: 0.05099642658305745 and parameters: {'observation_period_num': 37, 'train_rates': 0.9151929889513328, 'learning_rate': 0.0005733281845535764, 'batch_size': 157, 'step_size': 15, 'gamma': 0.9691501961915934}. Best is trial 48 with value: 0.027319840021400785.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 04:23:38,995][0m A new study created in memory with name: no-name-e7823e20-6a22-4a91-b51f-49a716d46d5e[0m
[32m[I 2025-01-06 04:24:17,560][0m Trial 0 finished with value: 0.7120726626722058 and parameters: {'observation_period_num': 149, 'train_rates': 0.7160377287037936, 'learning_rate': 4.3460368221199334e-06, 'batch_size': 156, 'step_size': 9, 'gamma': 0.7504664170392493}. Best is trial 0 with value: 0.7120726626722058.[0m
[32m[I 2025-01-06 04:25:46,662][0m Trial 1 finished with value: 0.12311407693526755 and parameters: {'observation_period_num': 122, 'train_rates': 0.7587054295464699, 'learning_rate': 0.000182050570812692, 'batch_size': 58, 'step_size': 6, 'gamma': 0.9159505877470873}. Best is trial 1 with value: 0.12311407693526755.[0m
[32m[I 2025-01-06 04:27:19,190][0m Trial 2 finished with value: 0.20371212319629947 and parameters: {'observation_period_num': 188, 'train_rates': 0.7330331488824091, 'learning_rate': 0.00041488297367805186, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9556258098667356}. Best is trial 1 with value: 0.12311407693526755.[0m
[32m[I 2025-01-06 04:28:11,151][0m Trial 3 finished with value: 0.27540537260938436 and parameters: {'observation_period_num': 208, 'train_rates': 0.9068865904067334, 'learning_rate': 3.132602237678897e-05, 'batch_size': 195, 'step_size': 6, 'gamma': 0.8444835486383561}. Best is trial 1 with value: 0.12311407693526755.[0m
[32m[I 2025-01-06 04:28:57,604][0m Trial 4 finished with value: 0.09537438321661192 and parameters: {'observation_period_num': 88, 'train_rates': 0.9012412760932982, 'learning_rate': 0.0003246151563409097, 'batch_size': 225, 'step_size': 8, 'gamma': 0.8896028475207156}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:30:05,687][0m Trial 5 finished with value: 0.10105612047386071 and parameters: {'observation_period_num': 26, 'train_rates': 0.6693328079227897, 'learning_rate': 0.00032407417520259266, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9664363935562511}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:31:05,816][0m Trial 6 finished with value: 0.38386170186611435 and parameters: {'observation_period_num': 105, 'train_rates': 0.8870418256519332, 'learning_rate': 6.993208119239273e-06, 'batch_size': 164, 'step_size': 10, 'gamma': 0.9363247349400903}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:32:25,792][0m Trial 7 finished with value: 0.32256962165236475 and parameters: {'observation_period_num': 22, 'train_rates': 0.8978587438061252, 'learning_rate': 1.7157784654299745e-06, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9018787344021099}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:34:24,643][0m Trial 8 finished with value: 0.2183516754008628 and parameters: {'observation_period_num': 188, 'train_rates': 0.6653406704128214, 'learning_rate': 0.0002551701314111356, 'batch_size': 42, 'step_size': 13, 'gamma': 0.8096388281100002}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:35:21,536][0m Trial 9 finished with value: 1.3358914139917342 and parameters: {'observation_period_num': 177, 'train_rates': 0.8302601005181762, 'learning_rate': 2.761094005477966e-06, 'batch_size': 226, 'step_size': 1, 'gamma': 0.9383535551720149}. Best is trial 4 with value: 0.09537438321661192.[0m
Early stopping at epoch 76
[32m[I 2025-01-06 04:36:15,809][0m Trial 10 finished with value: 0.4227575957775116 and parameters: {'observation_period_num': 82, 'train_rates': 0.9812736274106175, 'learning_rate': 5.829429613809976e-05, 'batch_size': 244, 'step_size': 1, 'gamma': 0.8596346744287896}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:37:19,266][0m Trial 11 finished with value: 0.12281116026063119 and parameters: {'observation_period_num': 27, 'train_rates': 0.6079136910117059, 'learning_rate': 0.0008037746268966529, 'batch_size': 111, 'step_size': 15, 'gamma': 0.9804222485239222}. Best is trial 4 with value: 0.09537438321661192.[0m
[32m[I 2025-01-06 04:38:41,670][0m Trial 12 finished with value: 0.06572594004313624 and parameters: {'observation_period_num': 65, 'train_rates': 0.8256423392950026, 'learning_rate': 0.0001239280431154594, 'batch_size': 110, 'step_size': 5, 'gamma': 0.8888102175659353}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:39:48,969][0m Trial 13 finished with value: 0.1218804485268063 and parameters: {'observation_period_num': 73, 'train_rates': 0.8251322199431527, 'learning_rate': 8.084617416462244e-05, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8991389990472965}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:41:11,278][0m Trial 14 finished with value: 0.3148598074913025 and parameters: {'observation_period_num': 65, 'train_rates': 0.9833760642518634, 'learning_rate': 1.8213252604509904e-05, 'batch_size': 138, 'step_size': 4, 'gamma': 0.8148146888621175}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:47:04,721][0m Trial 15 finished with value: 0.125243035358784 and parameters: {'observation_period_num': 55, 'train_rates': 0.8408579611841712, 'learning_rate': 0.00010466346724329429, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8796515736193102}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:48:01,023][0m Trial 16 finished with value: 0.14762185513973236 and parameters: {'observation_period_num': 240, 'train_rates': 0.9330867258881159, 'learning_rate': 0.00058098284626283, 'batch_size': 195, 'step_size': 4, 'gamma': 0.8300684604219687}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:49:23,283][0m Trial 17 finished with value: 0.19233509360290155 and parameters: {'observation_period_num': 101, 'train_rates': 0.7845788262560703, 'learning_rate': 1.3003030542353242e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.8876404671129593}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:50:29,342][0m Trial 18 finished with value: 0.13361004213956032 and parameters: {'observation_period_num': 144, 'train_rates': 0.8706019715487202, 'learning_rate': 0.00013677277328270188, 'batch_size': 135, 'step_size': 5, 'gamma': 0.7952873297692548}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:51:56,720][0m Trial 19 finished with value: 0.39256441593170166 and parameters: {'observation_period_num': 48, 'train_rates': 0.9407280792443966, 'learning_rate': 4.1063590655491934e-05, 'batch_size': 256, 'step_size': 2, 'gamma': 0.8607407880331698}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:52:48,708][0m Trial 20 finished with value: 0.10189903319692936 and parameters: {'observation_period_num': 94, 'train_rates': 0.7945391454712913, 'learning_rate': 0.0009907513254759134, 'batch_size': 220, 'step_size': 8, 'gamma': 0.9363140830515467}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:53:50,228][0m Trial 21 finished with value: 0.078022198770811 and parameters: {'observation_period_num': 10, 'train_rates': 0.6711550580708231, 'learning_rate': 0.0003135143876737476, 'batch_size': 109, 'step_size': 15, 'gamma': 0.9819852258375573}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:54:40,187][0m Trial 22 finished with value: 0.09705500119616525 and parameters: {'observation_period_num': 10, 'train_rates': 0.625334487024386, 'learning_rate': 0.0002020174354990211, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9174801258392983}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:55:55,562][0m Trial 23 finished with value: 0.0810187544532438 and parameters: {'observation_period_num': 40, 'train_rates': 0.8553710396473129, 'learning_rate': 0.0005239763107101618, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9825326208584234}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:57:27,910][0m Trial 24 finished with value: 0.06776497625783262 and parameters: {'observation_period_num': 41, 'train_rates': 0.8549924816644355, 'learning_rate': 0.00011761027776151283, 'batch_size': 91, 'step_size': 15, 'gamma': 0.9839445672145433}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 04:58:56,808][0m Trial 25 finished with value: 0.08732068854570389 and parameters: {'observation_period_num': 13, 'train_rates': 0.7023095642523259, 'learning_rate': 7.583514089650502e-05, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9657096208403206}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 05:00:13,424][0m Trial 26 finished with value: 0.08534008226496108 and parameters: {'observation_period_num': 41, 'train_rates': 0.7578998797552547, 'learning_rate': 0.00013373814115696483, 'batch_size': 119, 'step_size': 13, 'gamma': 0.9513186606229055}. Best is trial 12 with value: 0.06572594004313624.[0m
[32m[I 2025-01-06 05:01:27,399][0m Trial 27 finished with value: 0.06166518027709279 and parameters: {'observation_period_num': 6, 'train_rates': 0.81197576514761, 'learning_rate': 6.188860763018078e-05, 'batch_size': 160, 'step_size': 14, 'gamma': 0.98424867857328}. Best is trial 27 with value: 0.06166518027709279.[0m
[32m[I 2025-01-06 05:02:54,574][0m Trial 28 finished with value: 0.14609041698061662 and parameters: {'observation_period_num': 65, 'train_rates': 0.8203784986717287, 'learning_rate': 2.786917035968046e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.7820927104901346}. Best is trial 27 with value: 0.06166518027709279.[0m
[32m[I 2025-01-06 05:04:06,829][0m Trial 29 finished with value: 0.3671039975078814 and parameters: {'observation_period_num': 120, 'train_rates': 0.7730428613944221, 'learning_rate': 1.1077818988030863e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.7706331908335684}. Best is trial 27 with value: 0.06166518027709279.[0m
[32m[I 2025-01-06 05:05:35,841][0m Trial 30 finished with value: 0.19953583540060582 and parameters: {'observation_period_num': 36, 'train_rates': 0.8663285169311516, 'learning_rate': 4.906497918199917e-05, 'batch_size': 176, 'step_size': 3, 'gamma': 0.7519927380228493}. Best is trial 27 with value: 0.06166518027709279.[0m
[32m[I 2025-01-06 05:07:19,688][0m Trial 31 finished with value: 0.058118897020450165 and parameters: {'observation_period_num': 5, 'train_rates': 0.8071335368542981, 'learning_rate': 0.00010984676061907822, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9846677449802616}. Best is trial 31 with value: 0.058118897020450165.[0m
[32m[I 2025-01-06 05:09:02,814][0m Trial 32 finished with value: 0.057876848325997536 and parameters: {'observation_period_num': 11, 'train_rates': 0.8065516480638705, 'learning_rate': 0.00011731030340549436, 'batch_size': 64, 'step_size': 14, 'gamma': 0.9890832299145604}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:10:42,943][0m Trial 33 finished with value: 0.05922600839260991 and parameters: {'observation_period_num': 7, 'train_rates': 0.8060311958067666, 'learning_rate': 6.228399075278798e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9649552586029405}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:12:19,253][0m Trial 34 finished with value: 0.07466522582225352 and parameters: {'observation_period_num': 6, 'train_rates': 0.7473641597380385, 'learning_rate': 2.2288251354123524e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9641299755449092}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:16:03,876][0m Trial 35 finished with value: 0.09356600039985759 and parameters: {'observation_period_num': 24, 'train_rates': 0.7115918347522704, 'learning_rate': 6.674778484131436e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.9454659290895234}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:17:51,959][0m Trial 36 finished with value: 0.0654534235505796 and parameters: {'observation_period_num': 24, 'train_rates': 0.8040407667755917, 'learning_rate': 0.00018886362214408261, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9899615447665439}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:20:07,049][0m Trial 37 finished with value: 0.11270470300390636 and parameters: {'observation_period_num': 53, 'train_rates': 0.7355679821612922, 'learning_rate': 4.1805906879850455e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9261111135271595}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:22:03,139][0m Trial 38 finished with value: 0.07175236577923233 and parameters: {'observation_period_num': 19, 'train_rates': 0.7729735453493883, 'learning_rate': 3.27181853527572e-05, 'batch_size': 52, 'step_size': 11, 'gamma': 0.9696257722247817}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:24:42,946][0m Trial 39 finished with value: 0.061256572656009506 and parameters: {'observation_period_num': 6, 'train_rates': 0.8050880881854852, 'learning_rate': 7.751941666663375e-05, 'batch_size': 33, 'step_size': 13, 'gamma': 0.9571447250951151}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:27:18,139][0m Trial 40 finished with value: 0.08087983525123078 and parameters: {'observation_period_num': 28, 'train_rates': 0.7861751751623293, 'learning_rate': 8.813959551092546e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9615499238490665}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:28:50,718][0m Trial 41 finished with value: 0.061321699397361026 and parameters: {'observation_period_num': 33, 'train_rates': 0.8071088395131615, 'learning_rate': 6.011519127789659e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9697347601056804}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:30:13,973][0m Trial 42 finished with value: 0.06936850149789427 and parameters: {'observation_period_num': 5, 'train_rates': 0.8017765619581875, 'learning_rate': 0.0001914103536085856, 'batch_size': 73, 'step_size': 13, 'gamma': 0.9531103567845934}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:32:21,599][0m Trial 43 finished with value: 0.0639273952774802 and parameters: {'observation_period_num': 34, 'train_rates': 0.84467835712339, 'learning_rate': 3.542519987692877e-05, 'batch_size': 51, 'step_size': 12, 'gamma': 0.9731553026083682}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:34:07,341][0m Trial 44 finished with value: 0.07696477768581304 and parameters: {'observation_period_num': 21, 'train_rates': 0.7716562398678366, 'learning_rate': 5.366619983198332e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9445591853328026}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:37:06,622][0m Trial 45 finished with value: 0.18501180819258456 and parameters: {'observation_period_num': 170, 'train_rates': 0.8822727627580046, 'learning_rate': 9.455119048699114e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9280165550453235}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:38:19,127][0m Trial 46 finished with value: 0.09758754635287314 and parameters: {'observation_period_num': 47, 'train_rates': 0.7329086618184619, 'learning_rate': 0.00024108216859981815, 'batch_size': 95, 'step_size': 10, 'gamma': 0.9731961679525449}. Best is trial 32 with value: 0.057876848325997536.[0m
[32m[I 2025-01-06 05:40:01,369][0m Trial 47 finished with value: 0.0571024326980114 and parameters: {'observation_period_num': 16, 'train_rates': 0.8384098880342319, 'learning_rate': 0.00015438870002164033, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9591877465940127}. Best is trial 47 with value: 0.0571024326980114.[0m
[32m[I 2025-01-06 05:42:20,855][0m Trial 48 finished with value: 0.0915453425094263 and parameters: {'observation_period_num': 80, 'train_rates': 0.8367664190785802, 'learning_rate': 0.00043372485423149123, 'batch_size': 46, 'step_size': 15, 'gamma': 0.9100239665389624}. Best is trial 47 with value: 0.0571024326980114.[0m
[32m[I 2025-01-06 05:45:08,724][0m Trial 49 finished with value: 0.1954252506459589 and parameters: {'observation_period_num': 223, 'train_rates': 0.9000141042456968, 'learning_rate': 0.00014169761785515274, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9560535287103337}. Best is trial 47 with value: 0.0571024326980114.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 05:45:08,734][0m A new study created in memory with name: no-name-41124bce-277a-4c2a-bc16-974096113368[0m
[32m[I 2025-01-06 05:45:56,831][0m Trial 0 finished with value: 0.13334879571343444 and parameters: {'observation_period_num': 60, 'train_rates': 0.8142960088799849, 'learning_rate': 3.457585445860715e-05, 'batch_size': 231, 'step_size': 13, 'gamma': 0.7731708950158306}. Best is trial 0 with value: 0.13334879571343444.[0m
[32m[I 2025-01-06 05:46:58,518][0m Trial 1 finished with value: 0.8064312480216802 and parameters: {'observation_period_num': 220, 'train_rates': 0.8429016038280646, 'learning_rate': 1.2936713882405338e-06, 'batch_size': 242, 'step_size': 9, 'gamma': 0.9134070376116958}. Best is trial 0 with value: 0.13334879571343444.[0m
[32m[I 2025-01-06 05:50:07,176][0m Trial 2 finished with value: 0.05122111365199089 and parameters: {'observation_period_num': 51, 'train_rates': 0.9837403033276491, 'learning_rate': 6.304830075850973e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.7668082979667789}. Best is trial 2 with value: 0.05122111365199089.[0m
[32m[I 2025-01-06 05:52:24,416][0m Trial 3 finished with value: 0.21721261752932305 and parameters: {'observation_period_num': 188, 'train_rates': 0.6734785559347323, 'learning_rate': 1.6654778306049797e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.903318722101553}. Best is trial 2 with value: 0.05122111365199089.[0m
[32m[I 2025-01-06 05:53:39,668][0m Trial 4 finished with value: 0.29114683900239335 and parameters: {'observation_period_num': 213, 'train_rates': 0.6714093594225875, 'learning_rate': 1.2265451493337132e-05, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9306547639073169}. Best is trial 2 with value: 0.05122111365199089.[0m
[32m[I 2025-01-06 05:54:58,984][0m Trial 5 finished with value: 0.16953824056878183 and parameters: {'observation_period_num': 35, 'train_rates': 0.8070468504373589, 'learning_rate': 1.4576056062527412e-05, 'batch_size': 123, 'step_size': 4, 'gamma': 0.9082503245280935}. Best is trial 2 with value: 0.05122111365199089.[0m
[32m[I 2025-01-06 05:55:59,304][0m Trial 6 finished with value: 0.05387198179960251 and parameters: {'observation_period_num': 22, 'train_rates': 0.9679744344555601, 'learning_rate': 0.0003811875431070675, 'batch_size': 224, 'step_size': 6, 'gamma': 0.7849974722262288}. Best is trial 2 with value: 0.05122111365199089.[0m
[32m[I 2025-01-06 06:00:37,437][0m Trial 7 finished with value: 0.020188789814710617 and parameters: {'observation_period_num': 16, 'train_rates': 0.9893353972234566, 'learning_rate': 4.562446513834744e-05, 'batch_size': 24, 'step_size': 15, 'gamma': 0.98302074298137}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:01:54,613][0m Trial 8 finished with value: 0.1342301837554792 and parameters: {'observation_period_num': 124, 'train_rates': 0.9275693308071566, 'learning_rate': 0.00011467366129655456, 'batch_size': 91, 'step_size': 3, 'gamma': 0.7977888150772479}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:05:48,366][0m Trial 9 finished with value: 0.11322800384627449 and parameters: {'observation_period_num': 107, 'train_rates': 0.9685310697059908, 'learning_rate': 8.60921822119826e-05, 'batch_size': 28, 'step_size': 1, 'gamma': 0.874180340492586}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:06:56,052][0m Trial 10 finished with value: 0.09232260334542909 and parameters: {'observation_period_num': 89, 'train_rates': 0.7355981729717479, 'learning_rate': 0.0009842252136824314, 'batch_size': 163, 'step_size': 15, 'gamma': 0.9855516668525576}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:12:43,468][0m Trial 11 finished with value: 0.057797749564991764 and parameters: {'observation_period_num': 7, 'train_rates': 0.8925322276852167, 'learning_rate': 4.282371713329001e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.829440168040783}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:14:33,282][0m Trial 12 finished with value: 0.0752163827419281 and parameters: {'observation_period_num': 68, 'train_rates': 0.9857210871260115, 'learning_rate': 9.340757719633991e-05, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8393625332559034}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:15:29,585][0m Trial 13 finished with value: 0.12919383952303118 and parameters: {'observation_period_num': 163, 'train_rates': 0.8902679677832489, 'learning_rate': 0.00025854971791963835, 'batch_size': 141, 'step_size': 12, 'gamma': 0.9832218871989512}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:16:58,276][0m Trial 14 finished with value: 0.2072994854478609 and parameters: {'observation_period_num': 59, 'train_rates': 0.6082564147641075, 'learning_rate': 4.2350370630271936e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.7513489414238836}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:17:59,578][0m Trial 15 finished with value: 0.26496751600620794 and parameters: {'observation_period_num': 151, 'train_rates': 0.9092040731863691, 'learning_rate': 5.141364070444986e-06, 'batch_size': 104, 'step_size': 14, 'gamma': 0.9505149674351977}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:18:50,468][0m Trial 16 finished with value: 0.040654050240792876 and parameters: {'observation_period_num': 36, 'train_rates': 0.8595411999888664, 'learning_rate': 0.00021936089005850532, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8679382977468176}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:19:32,581][0m Trial 17 finished with value: 0.18323263742512078 and parameters: {'observation_period_num': 247, 'train_rates': 0.7433823409267432, 'learning_rate': 0.00023476382147047484, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8660439014574222}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:20:16,795][0m Trial 18 finished with value: 0.03151647369702195 and parameters: {'observation_period_num': 7, 'train_rates': 0.8542298982541132, 'learning_rate': 0.0005525397543292202, 'batch_size': 195, 'step_size': 7, 'gamma': 0.8682895671083495}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:20:59,331][0m Trial 19 finished with value: 0.0764586572845777 and parameters: {'observation_period_num': 86, 'train_rates': 0.7647887343117673, 'learning_rate': 0.0005630471509042445, 'batch_size': 201, 'step_size': 6, 'gamma': 0.9568021966624041}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:21:59,567][0m Trial 20 finished with value: 0.037630144506692886 and parameters: {'observation_period_num': 5, 'train_rates': 0.9427544856737983, 'learning_rate': 0.0008228725895321579, 'batch_size': 194, 'step_size': 7, 'gamma': 0.8236968455288288}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:22:46,043][0m Trial 21 finished with value: 0.0401051864027977 and parameters: {'observation_period_num': 10, 'train_rates': 0.9370987877454636, 'learning_rate': 0.0006495678604322916, 'batch_size': 195, 'step_size': 5, 'gamma': 0.8111364511495132}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:23:45,148][0m Trial 22 finished with value: 0.03545958548784256 and parameters: {'observation_period_num': 6, 'train_rates': 0.9404669937953923, 'learning_rate': 0.0007692074758832655, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8344388440670919}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:24:41,579][0m Trial 23 finished with value: 0.0401806950587826 and parameters: {'observation_period_num': 30, 'train_rates': 0.8634887530564372, 'learning_rate': 0.00016122752584782132, 'batch_size': 139, 'step_size': 8, 'gamma': 0.85720514382402}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:26:01,606][0m Trial 24 finished with value: 0.04687344502996315 and parameters: {'observation_period_num': 42, 'train_rates': 0.9397695319702699, 'learning_rate': 0.0004303449053730158, 'batch_size': 116, 'step_size': 9, 'gamma': 0.88853598726278}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:27:12,438][0m Trial 25 finished with value: 0.05467383341183738 and parameters: {'observation_period_num': 83, 'train_rates': 0.8903321240035053, 'learning_rate': 0.00039522458737729606, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8416404757416716}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:28:03,371][0m Trial 26 finished with value: 1.2482268965762595 and parameters: {'observation_period_num': 21, 'train_rates': 0.8354987951770247, 'learning_rate': 1.1149189243427846e-06, 'batch_size': 255, 'step_size': 3, 'gamma': 0.8532964833808843}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:28:50,738][0m Trial 27 finished with value: 0.5701861672623213 and parameters: {'observation_period_num': 75, 'train_rates': 0.9105348981737709, 'learning_rate': 5.232511602350894e-06, 'batch_size': 214, 'step_size': 9, 'gamma': 0.887372544564465}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:29:53,268][0m Trial 28 finished with value: 0.10247203707695007 and parameters: {'observation_period_num': 106, 'train_rates': 0.96092719678494, 'learning_rate': 0.00015643387037992014, 'batch_size': 167, 'step_size': 7, 'gamma': 0.808320632307016}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:31:06,587][0m Trial 29 finished with value: 0.06903653772813934 and parameters: {'observation_period_num': 56, 'train_rates': 0.7827165460995333, 'learning_rate': 2.7812159526665167e-05, 'batch_size': 122, 'step_size': 11, 'gamma': 0.926761700673911}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:32:18,370][0m Trial 30 finished with value: 0.13452774551652727 and parameters: {'observation_period_num': 25, 'train_rates': 0.8207428020363448, 'learning_rate': 4.7660581109772235e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.888257399463551}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:33:02,806][0m Trial 31 finished with value: 0.043015025556087494 and parameters: {'observation_period_num': 9, 'train_rates': 0.9519078006267488, 'learning_rate': 0.0008288620344089533, 'batch_size': 194, 'step_size': 7, 'gamma': 0.829959529601716}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:33:41,415][0m Trial 32 finished with value: 0.03743351995944977 and parameters: {'observation_period_num': 6, 'train_rates': 0.9239686120619008, 'learning_rate': 0.0006556474181227206, 'batch_size': 232, 'step_size': 8, 'gamma': 0.8218310437094409}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:34:21,815][0m Trial 33 finished with value: 0.05387137686620601 and parameters: {'observation_period_num': 47, 'train_rates': 0.8619555095051294, 'learning_rate': 0.0003214827179747003, 'batch_size': 235, 'step_size': 10, 'gamma': 0.8483719923958347}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:35:05,315][0m Trial 34 finished with value: 0.04380638152360916 and parameters: {'observation_period_num': 21, 'train_rates': 0.9856662336928268, 'learning_rate': 0.0006019228659193302, 'batch_size': 253, 'step_size': 5, 'gamma': 0.8156508624092997}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:35:45,362][0m Trial 35 finished with value: 0.7842999645432488 and parameters: {'observation_period_num': 43, 'train_rates': 0.9164084653037274, 'learning_rate': 2.6052704614885175e-06, 'batch_size': 211, 'step_size': 9, 'gamma': 0.7908055014206519}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:36:27,967][0m Trial 36 finished with value: 0.03663008000169481 and parameters: {'observation_period_num': 18, 'train_rates': 0.8807405731934076, 'learning_rate': 0.0004976158084008659, 'batch_size': 230, 'step_size': 8, 'gamma': 0.774540018610133}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:37:11,573][0m Trial 37 finished with value: 0.27457305373919705 and parameters: {'observation_period_num': 62, 'train_rates': 0.8760806337114653, 'learning_rate': 2.504772569232389e-05, 'batch_size': 178, 'step_size': 5, 'gamma': 0.7679725431363059}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:37:50,571][0m Trial 38 finished with value: 0.34366320495150593 and parameters: {'observation_period_num': 21, 'train_rates': 0.8445766767262937, 'learning_rate': 8.92959577364233e-06, 'batch_size': 223, 'step_size': 8, 'gamma': 0.7789543739130816}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:38:44,121][0m Trial 39 finished with value: 0.12360144610847196 and parameters: {'observation_period_num': 195, 'train_rates': 0.8203799938644802, 'learning_rate': 6.505408332493922e-05, 'batch_size': 103, 'step_size': 13, 'gamma': 0.7508746958107667}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:39:33,302][0m Trial 40 finished with value: 0.052962712943553925 and parameters: {'observation_period_num': 49, 'train_rates': 0.9663668569721666, 'learning_rate': 0.00048583265248989345, 'batch_size': 155, 'step_size': 6, 'gamma': 0.9159677771891982}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:40:15,032][0m Trial 41 finished with value: 0.04192196788887183 and parameters: {'observation_period_num': 5, 'train_rates': 0.9188350452944402, 'learning_rate': 0.0003090452915793558, 'batch_size': 232, 'step_size': 8, 'gamma': 0.7992239716642284}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:40:52,670][0m Trial 42 finished with value: 0.0461803810518296 and parameters: {'observation_period_num': 32, 'train_rates': 0.8957255900729644, 'learning_rate': 0.0009520849102844979, 'batch_size': 243, 'step_size': 8, 'gamma': 0.8998932500854808}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:41:47,979][0m Trial 43 finished with value: 0.05367635563015938 and parameters: {'observation_period_num': 18, 'train_rates': 0.9300192987499363, 'learning_rate': 0.00016271504868039074, 'batch_size': 212, 'step_size': 9, 'gamma': 0.800128494921851}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:42:45,768][0m Trial 44 finished with value: 0.0519588440656662 and parameters: {'observation_period_num': 32, 'train_rates': 0.9565701539800378, 'learning_rate': 0.0006945459705197909, 'batch_size': 223, 'step_size': 7, 'gamma': 0.8340931177511804}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:43:46,230][0m Trial 45 finished with value: 0.04927652799769452 and parameters: {'observation_period_num': 15, 'train_rates': 0.8771201184869903, 'learning_rate': 0.0004776720972197899, 'batch_size': 242, 'step_size': 11, 'gamma': 0.8774003103649659}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:46:08,456][0m Trial 46 finished with value: 0.04786310427718692 and parameters: {'observation_period_num': 38, 'train_rates': 0.9786159470361446, 'learning_rate': 0.00013340401904842085, 'batch_size': 47, 'step_size': 4, 'gamma': 0.8238514859616087}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:46:58,886][0m Trial 47 finished with value: 0.6633871047616863 and parameters: {'observation_period_num': 140, 'train_rates': 0.9013419437833651, 'learning_rate': 1.6935849293151448e-06, 'batch_size': 183, 'step_size': 8, 'gamma': 0.9709133676740739}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:47:56,053][0m Trial 48 finished with value: 0.05440937838441617 and parameters: {'observation_period_num': 103, 'train_rates': 0.8442627817313992, 'learning_rate': 0.000287476627058362, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8622779116268501}. Best is trial 7 with value: 0.020188789814710617.[0m
[32m[I 2025-01-06 06:49:30,891][0m Trial 49 finished with value: 0.14189399133867292 and parameters: {'observation_period_num': 172, 'train_rates': 0.8795970194613949, 'learning_rate': 1.8566466455329076e-05, 'batch_size': 65, 'step_size': 14, 'gamma': 0.7610433681442572}. Best is trial 7 with value: 0.020188789814710617.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 06:49:30,901][0m A new study created in memory with name: no-name-84a3f2dd-6905-4488-ac60-c3a606ec63c2[0m
[32m[I 2025-01-06 06:50:31,893][0m Trial 0 finished with value: 0.7076473236083984 and parameters: {'observation_period_num': 226, 'train_rates': 0.9742356469964907, 'learning_rate': 4.001645420109449e-06, 'batch_size': 212, 'step_size': 1, 'gamma': 0.9382031864439782}. Best is trial 0 with value: 0.7076473236083984.[0m
[32m[I 2025-01-06 06:51:38,932][0m Trial 1 finished with value: 0.8467420744446089 and parameters: {'observation_period_num': 126, 'train_rates': 0.7938024423131311, 'learning_rate': 2.9505170701179276e-06, 'batch_size': 154, 'step_size': 9, 'gamma': 0.8802738709937237}. Best is trial 0 with value: 0.7076473236083984.[0m
[32m[I 2025-01-06 06:53:34,605][0m Trial 2 finished with value: 0.21432164110327986 and parameters: {'observation_period_num': 240, 'train_rates': 0.7407002116180558, 'learning_rate': 2.3753731482211443e-05, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9078455287310494}. Best is trial 2 with value: 0.21432164110327986.[0m
[32m[I 2025-01-06 06:56:13,401][0m Trial 3 finished with value: 0.10503866287809265 and parameters: {'observation_period_num': 220, 'train_rates': 0.7919696891176262, 'learning_rate': 0.00023496235820540183, 'batch_size': 33, 'step_size': 3, 'gamma': 0.8879481247375195}. Best is trial 3 with value: 0.10503866287809265.[0m
[32m[I 2025-01-06 06:57:46,066][0m Trial 4 finished with value: 0.08801551949433925 and parameters: {'observation_period_num': 94, 'train_rates': 0.7278266323438167, 'learning_rate': 4.680818175651892e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.9790702451737449}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 06:58:53,104][0m Trial 5 finished with value: 0.11438092589378357 and parameters: {'observation_period_num': 145, 'train_rates': 0.9386233086465003, 'learning_rate': 0.00019072253665692676, 'batch_size': 203, 'step_size': 9, 'gamma': 0.906232340273212}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:03:34,107][0m Trial 6 finished with value: 0.1930568058038899 and parameters: {'observation_period_num': 195, 'train_rates': 0.7197581582200825, 'learning_rate': 0.0003242366653514081, 'batch_size': 18, 'step_size': 9, 'gamma': 0.799706192678673}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:04:45,517][0m Trial 7 finished with value: 0.11180120706558228 and parameters: {'observation_period_num': 210, 'train_rates': 0.9568651472705602, 'learning_rate': 0.0005164393917225851, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8328873345911445}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:05:39,668][0m Trial 8 finished with value: 1.5556469993360007 and parameters: {'observation_period_num': 231, 'train_rates': 0.8032255050991897, 'learning_rate': 1.1506927082615905e-06, 'batch_size': 167, 'step_size': 7, 'gamma': 0.78351521984408}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:06:17,360][0m Trial 9 finished with value: 0.6422765763431733 and parameters: {'observation_period_num': 189, 'train_rates': 0.6085899779072523, 'learning_rate': 9.175377541131344e-06, 'batch_size': 226, 'step_size': 11, 'gamma': 0.7729751208981293}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:07:23,107][0m Trial 10 finished with value: 0.09792023188299649 and parameters: {'observation_period_num': 31, 'train_rates': 0.6054773657187883, 'learning_rate': 7.626002753085165e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9866361569964183}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:08:14,388][0m Trial 11 finished with value: 0.10193573100411374 and parameters: {'observation_period_num': 31, 'train_rates': 0.6065159034758556, 'learning_rate': 6.598639852842738e-05, 'batch_size': 100, 'step_size': 15, 'gamma': 0.9883712143711263}. Best is trial 4 with value: 0.08801551949433925.[0m
[32m[I 2025-01-06 07:09:06,968][0m Trial 12 finished with value: 0.05973757518294417 and parameters: {'observation_period_num': 35, 'train_rates': 0.6670997543565925, 'learning_rate': 5.327065048802376e-05, 'batch_size': 96, 'step_size': 15, 'gamma': 0.986089632255599}. Best is trial 12 with value: 0.05973757518294417.[0m
[32m[I 2025-01-06 07:10:03,511][0m Trial 13 finished with value: 0.09730141136001368 and parameters: {'observation_period_num': 82, 'train_rates': 0.6815038388461376, 'learning_rate': 2.490886155277412e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9493490687083516}. Best is trial 12 with value: 0.05973757518294417.[0m
[32m[I 2025-01-06 07:10:47,328][0m Trial 14 finished with value: 0.07841697784178316 and parameters: {'observation_period_num': 75, 'train_rates': 0.6730114065742451, 'learning_rate': 7.987074265299626e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9512689912918291}. Best is trial 12 with value: 0.05973757518294417.[0m
[32m[I 2025-01-06 07:11:29,230][0m Trial 15 finished with value: 0.083551365618873 and parameters: {'observation_period_num': 62, 'train_rates': 0.6722833213099573, 'learning_rate': 0.00012270033916096914, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9423899583572585}. Best is trial 12 with value: 0.05973757518294417.[0m
[32m[I 2025-01-06 07:12:43,826][0m Trial 16 finished with value: 0.035965499489029426 and parameters: {'observation_period_num': 14, 'train_rates': 0.8484292040220792, 'learning_rate': 0.0007950790543032997, 'batch_size': 124, 'step_size': 12, 'gamma': 0.957408944176482}. Best is trial 16 with value: 0.035965499489029426.[0m
[32m[I 2025-01-06 07:14:00,202][0m Trial 17 finished with value: 0.0279076109808054 and parameters: {'observation_period_num': 13, 'train_rates': 0.8906244168396145, 'learning_rate': 0.0007447966074326961, 'batch_size': 176, 'step_size': 13, 'gamma': 0.8414491101707311}. Best is trial 17 with value: 0.0279076109808054.[0m
[32m[I 2025-01-06 07:15:16,783][0m Trial 18 finished with value: 0.029064630509688043 and parameters: {'observation_period_num': 20, 'train_rates': 0.8916303484388975, 'learning_rate': 0.0009976419117421597, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8438534526032319}. Best is trial 17 with value: 0.0279076109808054.[0m
[32m[I 2025-01-06 07:16:38,304][0m Trial 19 finished with value: 0.0739684403200463 and parameters: {'observation_period_num': 110, 'train_rates': 0.8984546668432332, 'learning_rate': 0.0006750282861881106, 'batch_size': 179, 'step_size': 13, 'gamma': 0.8410179421370275}. Best is trial 17 with value: 0.0279076109808054.[0m
[32m[I 2025-01-06 07:17:54,506][0m Trial 20 finished with value: 0.03753674030303955 and parameters: {'observation_period_num': 10, 'train_rates': 0.9142281197198006, 'learning_rate': 0.0003836112081566809, 'batch_size': 253, 'step_size': 13, 'gamma': 0.8446496124834741}. Best is trial 17 with value: 0.0279076109808054.[0m
[32m[I 2025-01-06 07:19:13,665][0m Trial 21 finished with value: 0.024019276300146258 and parameters: {'observation_period_num': 12, 'train_rates': 0.8625308577317593, 'learning_rate': 0.0009560811759465162, 'batch_size': 184, 'step_size': 11, 'gamma': 0.8196661258033011}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:20:39,234][0m Trial 22 finished with value: 0.03476759514121144 and parameters: {'observation_period_num': 39, 'train_rates': 0.8660480893773871, 'learning_rate': 0.0009257846546651828, 'batch_size': 186, 'step_size': 11, 'gamma': 0.8172745307872179}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:21:50,591][0m Trial 23 finished with value: 0.04158195298320303 and parameters: {'observation_period_num': 54, 'train_rates': 0.8675960608726092, 'learning_rate': 0.0009228961698815876, 'batch_size': 154, 'step_size': 10, 'gamma': 0.750220425018259}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:23:08,664][0m Trial 24 finished with value: 0.03604873423100499 and parameters: {'observation_period_num': 6, 'train_rates': 0.8350407937618984, 'learning_rate': 0.00019427852747471464, 'batch_size': 190, 'step_size': 13, 'gamma': 0.8641249407908128}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:24:29,424][0m Trial 25 finished with value: 0.04417661431020704 and parameters: {'observation_period_num': 57, 'train_rates': 0.8999886536621569, 'learning_rate': 0.0004140837088616086, 'batch_size': 152, 'step_size': 14, 'gamma': 0.8143178213258696}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:25:50,491][0m Trial 26 finished with value: 0.08466275027079612 and parameters: {'observation_period_num': 166, 'train_rates': 0.8268744810787901, 'learning_rate': 0.0005462476438617831, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8604461808155897}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:26:56,308][0m Trial 27 finished with value: 0.04994557536754411 and parameters: {'observation_period_num': 22, 'train_rates': 0.9339688712288189, 'learning_rate': 0.00012659475784967437, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8143639772674044}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:27:50,827][0m Trial 28 finished with value: 0.04248284769512839 and parameters: {'observation_period_num': 44, 'train_rates': 0.8782755113844571, 'learning_rate': 0.00025918600597306575, 'batch_size': 197, 'step_size': 10, 'gamma': 0.8510962420790046}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:28:53,432][0m Trial 29 finished with value: 0.057012174278497696 and parameters: {'observation_period_num': 72, 'train_rates': 0.9771617934431887, 'learning_rate': 0.0004912724905769586, 'batch_size': 142, 'step_size': 14, 'gamma': 0.7881826508759463}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:29:48,693][0m Trial 30 finished with value: 0.06102617457509041 and parameters: {'observation_period_num': 100, 'train_rates': 0.9365927607075081, 'learning_rate': 0.0009830143958199074, 'batch_size': 209, 'step_size': 8, 'gamma': 0.8286190312397521}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:30:39,240][0m Trial 31 finished with value: 0.03918876718304834 and parameters: {'observation_period_num': 46, 'train_rates': 0.8705367968544896, 'learning_rate': 0.0008789874240006863, 'batch_size': 184, 'step_size': 11, 'gamma': 0.8136310185199018}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:31:28,843][0m Trial 32 finished with value: 0.13976908146261094 and parameters: {'observation_period_num': 21, 'train_rates': 0.8109227825773935, 'learning_rate': 1.3043377918165432e-05, 'batch_size': 170, 'step_size': 10, 'gamma': 0.8750430560335968}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:32:24,442][0m Trial 33 finished with value: 0.031554810783010354 and parameters: {'observation_period_num': 6, 'train_rates': 0.8581908642366808, 'learning_rate': 0.0006271995925150388, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8260451570980979}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:33:17,510][0m Trial 34 finished with value: 0.03518795402385946 and parameters: {'observation_period_num': 6, 'train_rates': 0.7865040743625752, 'learning_rate': 0.00032280813288953244, 'batch_size': 157, 'step_size': 14, 'gamma': 0.8870260384542471}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:34:15,899][0m Trial 35 finished with value: 0.034651483222842214 and parameters: {'observation_period_num': 23, 'train_rates': 0.894496434235061, 'learning_rate': 0.0005961321533969815, 'batch_size': 136, 'step_size': 13, 'gamma': 0.8294412497836217}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:35:01,175][0m Trial 36 finished with value: 0.0913406900548312 and parameters: {'observation_period_num': 145, 'train_rates': 0.8501841096191621, 'learning_rate': 0.00014251617266712095, 'batch_size': 203, 'step_size': 12, 'gamma': 0.9013446422710827}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:35:55,063][0m Trial 37 finished with value: 0.06420423877745987 and parameters: {'observation_period_num': 24, 'train_rates': 0.915853265337518, 'learning_rate': 0.0002984946654354381, 'batch_size': 162, 'step_size': 1, 'gamma': 0.9213003892563018}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:37:10,008][0m Trial 38 finished with value: 0.6578841209411621 and parameters: {'observation_period_num': 48, 'train_rates': 0.9611385789826277, 'learning_rate': 4.137482130858411e-06, 'batch_size': 219, 'step_size': 8, 'gamma': 0.8528599797860512}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:38:28,754][0m Trial 39 finished with value: 0.0559152716950472 and parameters: {'observation_period_num': 69, 'train_rates': 0.7737919688782016, 'learning_rate': 0.0006745552580721915, 'batch_size': 147, 'step_size': 9, 'gamma': 0.8026632996671837}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:39:16,097][0m Trial 40 finished with value: 0.04129023056154061 and parameters: {'observation_period_num': 6, 'train_rates': 0.7616401997942164, 'learning_rate': 0.00019357021522635532, 'batch_size': 238, 'step_size': 10, 'gamma': 0.7991711796048785}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:40:28,039][0m Trial 41 finished with value: 0.03260919133968213 and parameters: {'observation_period_num': 21, 'train_rates': 0.8898077011053881, 'learning_rate': 0.0005416754655283034, 'batch_size': 135, 'step_size': 13, 'gamma': 0.8304156940498232}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:41:18,962][0m Trial 42 finished with value: 0.03521966072231473 and parameters: {'observation_period_num': 34, 'train_rates': 0.8243910800978138, 'learning_rate': 0.0004353386130393434, 'batch_size': 175, 'step_size': 14, 'gamma': 0.8342794257741671}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:42:20,987][0m Trial 43 finished with value: 0.03544390342145596 and parameters: {'observation_period_num': 18, 'train_rates': 0.9098189041958015, 'learning_rate': 0.0006553172295111786, 'batch_size': 195, 'step_size': 12, 'gamma': 0.8736039161559529}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:43:54,241][0m Trial 44 finished with value: 0.04488815441278808 and parameters: {'observation_period_num': 30, 'train_rates': 0.8866349279264959, 'learning_rate': 0.00044115859069493343, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8254011835356092}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:45:11,659][0m Trial 45 finished with value: 0.03801908369096139 and parameters: {'observation_period_num': 42, 'train_rates': 0.8521704670982534, 'learning_rate': 0.00025281892345383286, 'batch_size': 134, 'step_size': 11, 'gamma': 0.8410936653051959}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:46:14,135][0m Trial 46 finished with value: 0.923467002773737 and parameters: {'observation_period_num': 87, 'train_rates': 0.9264709087030081, 'learning_rate': 1.0478073152561056e-06, 'batch_size': 166, 'step_size': 15, 'gamma': 0.8578992218003533}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:47:50,356][0m Trial 47 finished with value: 0.14691741570709196 and parameters: {'observation_period_num': 252, 'train_rates': 0.9545101885738951, 'learning_rate': 0.0006791759519481715, 'batch_size': 74, 'step_size': 14, 'gamma': 0.7727416327700878}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:48:54,413][0m Trial 48 finished with value: 0.02931829550212321 and parameters: {'observation_period_num': 13, 'train_rates': 0.8854517863505549, 'learning_rate': 0.0003567121685403805, 'batch_size': 141, 'step_size': 12, 'gamma': 0.8041654358763944}. Best is trial 21 with value: 0.024019276300146258.[0m
[32m[I 2025-01-06 07:49:51,966][0m Trial 49 finished with value: 0.047169060336885976 and parameters: {'observation_period_num': 59, 'train_rates': 0.8560108736484326, 'learning_rate': 0.0003593179043079012, 'batch_size': 208, 'step_size': 9, 'gamma': 0.7930429778370865}. Best is trial 21 with value: 0.024019276300146258.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-06 07:49:51,977][0m A new study created in memory with name: no-name-3bb1b70e-59ee-48d2-950c-e234f42c4512[0m
[32m[I 2025-01-06 07:51:03,120][0m Trial 0 finished with value: 0.16988158164539746 and parameters: {'observation_period_num': 177, 'train_rates': 0.7819333568357969, 'learning_rate': 2.1332210843549426e-05, 'batch_size': 77, 'step_size': 8, 'gamma': 0.857816646669243}. Best is trial 0 with value: 0.16988158164539746.[0m
[32m[I 2025-01-06 07:52:06,467][0m Trial 1 finished with value: 0.07324490547180176 and parameters: {'observation_period_num': 77, 'train_rates': 0.7078295911049977, 'learning_rate': 9.645009605137712e-05, 'batch_size': 112, 'step_size': 6, 'gamma': 0.9022554586793095}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:53:04,733][0m Trial 2 finished with value: 0.08399140696073401 and parameters: {'observation_period_num': 10, 'train_rates': 0.70458698943656, 'learning_rate': 1.319720418133703e-05, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8762869171062833}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:53:41,599][0m Trial 3 finished with value: 0.1781469463504434 and parameters: {'observation_period_num': 93, 'train_rates': 0.856579929330956, 'learning_rate': 5.7509905034623614e-05, 'batch_size': 247, 'step_size': 6, 'gamma': 0.7986304945435435}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:54:41,084][0m Trial 4 finished with value: 0.19167034036541586 and parameters: {'observation_period_num': 222, 'train_rates': 0.651928461542361, 'learning_rate': 0.00047514965661662757, 'batch_size': 78, 'step_size': 9, 'gamma': 0.9556774650616129}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:55:20,597][0m Trial 5 finished with value: 0.14091093475754196 and parameters: {'observation_period_num': 201, 'train_rates': 0.8120212275954748, 'learning_rate': 0.000369595758693388, 'batch_size': 196, 'step_size': 15, 'gamma': 0.9279418063962639}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:56:40,729][0m Trial 6 finished with value: 0.6451238317064719 and parameters: {'observation_period_num': 139, 'train_rates': 0.6409970405699881, 'learning_rate': 6.7718154428023825e-06, 'batch_size': 57, 'step_size': 2, 'gamma': 0.8293751674655014}. Best is trial 1 with value: 0.07324490547180176.[0m
[32m[I 2025-01-06 07:57:55,828][0m Trial 7 finished with value: 0.03634513795168142 and parameters: {'observation_period_num': 30, 'train_rates': 0.9019115125339161, 'learning_rate': 0.0008579789570434466, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7536126800313734}. Best is trial 7 with value: 0.03634513795168142.[0m
[32m[I 2025-01-06 07:59:28,853][0m Trial 8 finished with value: 0.8080712219662791 and parameters: {'observation_period_num': 106, 'train_rates': 0.7726975711163838, 'learning_rate': 2.903357825677583e-06, 'batch_size': 74, 'step_size': 3, 'gamma': 0.795580828178935}. Best is trial 7 with value: 0.03634513795168142.[0m
[32m[I 2025-01-06 08:00:31,473][0m Trial 9 finished with value: 0.23002821498590967 and parameters: {'observation_period_num': 161, 'train_rates': 0.7366019442269686, 'learning_rate': 8.122627860019857e-05, 'batch_size': 208, 'step_size': 6, 'gamma': 0.7653433456986363}. Best is trial 7 with value: 0.03634513795168142.[0m
[32m[I 2025-01-06 08:06:57,492][0m Trial 10 finished with value: 0.01598093130936225 and parameters: {'observation_period_num': 5, 'train_rates': 0.9849397688047757, 'learning_rate': 0.0009829581034448537, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7546047724778451}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:13:46,787][0m Trial 11 finished with value: 0.016592397480397612 and parameters: {'observation_period_num': 6, 'train_rates': 0.9875187296975099, 'learning_rate': 0.0007359625337220642, 'batch_size': 16, 'step_size': 15, 'gamma': 0.751603655650488}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:19:03,111][0m Trial 12 finished with value: 0.07612753276889389 and parameters: {'observation_period_num': 48, 'train_rates': 0.9874419670141148, 'learning_rate': 0.0001974387560908962, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7959387886086137}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:23:57,463][0m Trial 13 finished with value: 0.0669014832991011 and parameters: {'observation_period_num': 56, 'train_rates': 0.9824814816607625, 'learning_rate': 0.0009924815084262936, 'batch_size': 23, 'step_size': 13, 'gamma': 0.7524488780060543}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:26:51,570][0m Trial 14 finished with value: 0.029134415480282467 and parameters: {'observation_period_num': 17, 'train_rates': 0.913841689695502, 'learning_rate': 0.00023141768597342656, 'batch_size': 42, 'step_size': 11, 'gamma': 0.8218255362109337}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:28:13,714][0m Trial 15 finished with value: 0.7786933546039191 and parameters: {'observation_period_num': 247, 'train_rates': 0.9350151480295381, 'learning_rate': 1.0704777743719055e-06, 'batch_size': 173, 'step_size': 15, 'gamma': 0.7871005974477204}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:29:51,927][0m Trial 16 finished with value: 0.042863213791719916 and parameters: {'observation_period_num': 59, 'train_rates': 0.8579064012474752, 'learning_rate': 0.0001574310987422597, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8399757968774757}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:32:41,880][0m Trial 17 finished with value: 0.03392865398982123 and parameters: {'observation_period_num': 6, 'train_rates': 0.954766098714344, 'learning_rate': 0.00046656114735543286, 'batch_size': 43, 'step_size': 12, 'gamma': 0.9757841420464493}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:37:05,662][0m Trial 18 finished with value: 0.034658913001202156 and parameters: {'observation_period_num': 35, 'train_rates': 0.858375485754215, 'learning_rate': 0.0009188291366659761, 'batch_size': 24, 'step_size': 15, 'gamma': 0.7727710256165039}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:38:26,232][0m Trial 19 finished with value: 0.12132544362661886 and parameters: {'observation_period_num': 119, 'train_rates': 0.8923155980529357, 'learning_rate': 5.4608779554645035e-05, 'batch_size': 173, 'step_size': 8, 'gamma': 0.8747555957054237}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:39:46,466][0m Trial 20 finished with value: 0.06505408883094788 and parameters: {'observation_period_num': 75, 'train_rates': 0.9596453578649274, 'learning_rate': 0.00033367916502817274, 'batch_size': 155, 'step_size': 13, 'gamma': 0.8117326023187684}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:42:15,426][0m Trial 21 finished with value: 0.025828757945463243 and parameters: {'observation_period_num': 5, 'train_rates': 0.9162237252304363, 'learning_rate': 0.00019745807263548914, 'batch_size': 48, 'step_size': 11, 'gamma': 0.8311451899588068}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:44:54,549][0m Trial 22 finished with value: 0.025481414049863815 and parameters: {'observation_period_num': 31, 'train_rates': 0.9880217464363203, 'learning_rate': 0.0005807331785074333, 'batch_size': 46, 'step_size': 11, 'gamma': 0.7763580805157272}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:46:52,614][0m Trial 23 finished with value: 0.047471625812813244 and parameters: {'observation_period_num': 37, 'train_rates': 0.9615114396817894, 'learning_rate': 0.0006312077641120429, 'batch_size': 63, 'step_size': 14, 'gamma': 0.7742636292315229}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:53:18,115][0m Trial 24 finished with value: 0.1320719491276476 and parameters: {'observation_period_num': 75, 'train_rates': 0.9875903355379833, 'learning_rate': 0.00033526608665844065, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7524256838270413}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:54:48,366][0m Trial 25 finished with value: 0.043094150239930434 and parameters: {'observation_period_num': 25, 'train_rates': 0.9421076747461604, 'learning_rate': 0.0001236979650861758, 'batch_size': 97, 'step_size': 14, 'gamma': 0.780958900147136}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 08:57:46,526][0m Trial 26 finished with value: 0.07250925732009551 and parameters: {'observation_period_num': 48, 'train_rates': 0.8830425546876552, 'learning_rate': 0.0005064578261492627, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8101754454399097}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:01:09,367][0m Trial 27 finished with value: 0.040307143259616124 and parameters: {'observation_period_num': 26, 'train_rates': 0.9357266858714197, 'learning_rate': 0.0006563184450043018, 'batch_size': 36, 'step_size': 15, 'gamma': 0.7659642438471944}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:03:00,281][0m Trial 28 finished with value: 0.09954737756933485 and parameters: {'observation_period_num': 67, 'train_rates': 0.8182170123989009, 'learning_rate': 3.29244210487538e-05, 'batch_size': 57, 'step_size': 4, 'gamma': 0.8505047082891914}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:04:42,742][0m Trial 29 finished with value: 0.07992647588253021 and parameters: {'observation_period_num': 161, 'train_rates': 0.97427121302815, 'learning_rate': 0.00027804863895736795, 'batch_size': 83, 'step_size': 9, 'gamma': 0.8990473423071157}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:06:16,804][0m Trial 30 finished with value: 0.15827016601175362 and parameters: {'observation_period_num': 43, 'train_rates': 0.6125850403541127, 'learning_rate': 1.9884578077646093e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.781305921550512}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:09:00,146][0m Trial 31 finished with value: 0.026243148189855785 and parameters: {'observation_period_num': 5, 'train_rates': 0.9165620115969527, 'learning_rate': 0.0006521091145847311, 'batch_size': 43, 'step_size': 11, 'gamma': 0.8148053755463773}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:11:25,961][0m Trial 32 finished with value: 0.035046071293098585 and parameters: {'observation_period_num': 18, 'train_rates': 0.9286792575665836, 'learning_rate': 0.00019663159671678558, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8617598576984414}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:18:04,862][0m Trial 33 finished with value: 0.0330517769198526 and parameters: {'observation_period_num': 19, 'train_rates': 0.9628055449219853, 'learning_rate': 0.0009181514003898996, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7613979754015959}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:19:45,809][0m Trial 34 finished with value: 0.03139030932843342 and parameters: {'observation_period_num': 8, 'train_rates': 0.8822412587174133, 'learning_rate': 0.00011327902358464429, 'batch_size': 93, 'step_size': 10, 'gamma': 0.8377251529789511}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:23:05,903][0m Trial 35 finished with value: 0.11785567551851273 and parameters: {'observation_period_num': 87, 'train_rates': 0.9888698704400444, 'learning_rate': 0.0004629397889854745, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8906616799550425}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:24:46,275][0m Trial 36 finished with value: 0.04396293131525025 and parameters: {'observation_period_num': 31, 'train_rates': 0.9536659823900884, 'learning_rate': 0.0002593968803918065, 'batch_size': 118, 'step_size': 14, 'gamma': 0.8018640489795588}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:27:21,525][0m Trial 37 finished with value: 0.033725251854097436 and parameters: {'observation_period_num': 5, 'train_rates': 0.9199418777270918, 'learning_rate': 6.920093776346739e-05, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9198634729666919}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:31:28,073][0m Trial 38 finished with value: 0.07323132645576558 and parameters: {'observation_period_num': 94, 'train_rates': 0.9672598444072865, 'learning_rate': 0.0006198661674715147, 'batch_size': 29, 'step_size': 7, 'gamma': 0.7823029933381928}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:32:52,172][0m Trial 39 finished with value: 0.039476405521315976 and parameters: {'observation_period_num': 58, 'train_rates': 0.8327855854185975, 'learning_rate': 0.00033116134999165605, 'batch_size': 83, 'step_size': 12, 'gamma': 0.7647157149744168}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:34:10,312][0m Trial 40 finished with value: 0.4164798477452672 and parameters: {'observation_period_num': 188, 'train_rates': 0.6980964573292472, 'learning_rate': 6.507551016732027e-06, 'batch_size': 108, 'step_size': 15, 'gamma': 0.7504688348475007}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:36:32,866][0m Trial 41 finished with value: 0.031033587336105152 and parameters: {'observation_period_num': 15, 'train_rates': 0.9068002780822392, 'learning_rate': 0.0006886072779912495, 'batch_size': 47, 'step_size': 11, 'gamma': 0.8243489032141497}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:38:07,419][0m Trial 42 finished with value: 0.02629375062104756 and parameters: {'observation_period_num': 8, 'train_rates': 0.7935257224464699, 'learning_rate': 0.00046775947061756607, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8077152369010725}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:38:47,451][0m Trial 43 finished with value: 0.04889321327209473 and parameters: {'observation_period_num': 38, 'train_rates': 0.9441828772889866, 'learning_rate': 0.000700824319600831, 'batch_size': 243, 'step_size': 9, 'gamma': 0.7925569049734401}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:41:57,407][0m Trial 44 finished with value: 0.03789067205567009 and parameters: {'observation_period_num': 25, 'train_rates': 0.8670262197461801, 'learning_rate': 0.00015853198060442866, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8156830715957567}. Best is trial 10 with value: 0.01598093130936225.[0m
Early stopping at epoch 84
[32m[I 2025-01-06 09:42:59,850][0m Trial 45 finished with value: 0.07575008368216256 and parameters: {'observation_period_num': 18, 'train_rates': 0.757084797239856, 'learning_rate': 0.0004806323200438302, 'batch_size': 74, 'step_size': 1, 'gamma': 0.8385780764843819}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:45:01,130][0m Trial 46 finished with value: 0.06761749800246672 and parameters: {'observation_period_num': 50, 'train_rates': 0.8397478411345148, 'learning_rate': 0.0003600305522255196, 'batch_size': 46, 'step_size': 14, 'gamma': 0.852607908284549}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:48:15,320][0m Trial 47 finished with value: 0.02716706992154981 and parameters: {'observation_period_num': 6, 'train_rates': 0.9211219736469456, 'learning_rate': 0.0008008978717885172, 'batch_size': 30, 'step_size': 7, 'gamma': 0.7964533028158225}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:54:48,042][0m Trial 48 finished with value: 0.04207872700544051 and parameters: {'observation_period_num': 30, 'train_rates': 0.972416808699085, 'learning_rate': 0.00018086435430088283, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7697324830399738}. Best is trial 10 with value: 0.01598093130936225.[0m
[32m[I 2025-01-06 09:56:11,924][0m Trial 49 finished with value: 0.08164159735043844 and parameters: {'observation_period_num': 131, 'train_rates': 0.8937984792557655, 'learning_rate': 0.0009744352067127243, 'batch_size': 136, 'step_size': 12, 'gamma': 0.7615066737858027}. Best is trial 10 with value: 0.01598093130936225.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 09:56:11,935][0m A new study created in memory with name: no-name-f692c8a7-ce06-4621-84d8-430e5299489a[0m
[32m[I 2025-01-06 09:57:29,298][0m Trial 0 finished with value: 0.22165276419441654 and parameters: {'observation_period_num': 235, 'train_rates': 0.7271555058525598, 'learning_rate': 2.1765899822006493e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.9122104577733425}. Best is trial 0 with value: 0.22165276419441654.[0m
[32m[I 2025-01-06 09:58:40,061][0m Trial 1 finished with value: 0.39464515820145607 and parameters: {'observation_period_num': 215, 'train_rates': 0.7018997913470882, 'learning_rate': 2.171460838479136e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8306685638955031}. Best is trial 0 with value: 0.22165276419441654.[0m
[32m[I 2025-01-06 10:00:04,596][0m Trial 2 finished with value: 0.17971877270585584 and parameters: {'observation_period_num': 152, 'train_rates': 0.6730423214786301, 'learning_rate': 0.00017204847983650842, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9847664339107807}. Best is trial 2 with value: 0.17971877270585584.[0m
[32m[I 2025-01-06 10:02:26,048][0m Trial 3 finished with value: 0.05706890328819382 and parameters: {'observation_period_num': 45, 'train_rates': 0.8406540523937562, 'learning_rate': 4.444054272345405e-05, 'batch_size': 49, 'step_size': 11, 'gamma': 0.9064881639026598}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:04:01,388][0m Trial 4 finished with value: 1.1927667093344043 and parameters: {'observation_period_num': 249, 'train_rates': 0.7371278501241907, 'learning_rate': 1.1668765486284824e-06, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8111977257776135}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:05:39,760][0m Trial 5 finished with value: 0.11149722750997171 and parameters: {'observation_period_num': 126, 'train_rates': 0.9549133985410388, 'learning_rate': 9.358490462769453e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9865410962415865}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:07:40,641][0m Trial 6 finished with value: 0.13054411113262177 and parameters: {'observation_period_num': 215, 'train_rates': 0.9886321158676722, 'learning_rate': 0.0003524892545369286, 'batch_size': 67, 'step_size': 6, 'gamma': 0.9581684814584919}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:09:06,178][0m Trial 7 finished with value: 0.707562119153429 and parameters: {'observation_period_num': 127, 'train_rates': 0.6438287136129057, 'learning_rate': 7.765702229704156e-06, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9129742268568941}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:10:36,759][0m Trial 8 finished with value: 0.40121924271454684 and parameters: {'observation_period_num': 150, 'train_rates': 0.6701134096692836, 'learning_rate': 2.3681869699171626e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.7553274120549556}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:12:01,711][0m Trial 9 finished with value: 0.1654749603549135 and parameters: {'observation_period_num': 42, 'train_rates': 0.6484098095509367, 'learning_rate': 7.707378372657765e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8029140077671139}. Best is trial 3 with value: 0.05706890328819382.[0m
[32m[I 2025-01-06 10:17:13,416][0m Trial 10 finished with value: 0.05666680015427501 and parameters: {'observation_period_num': 6, 'train_rates': 0.871295235138965, 'learning_rate': 3.6382899410333e-06, 'batch_size': 21, 'step_size': 14, 'gamma': 0.8796834664975804}. Best is trial 10 with value: 0.05666680015427501.[0m
[32m[I 2025-01-06 10:21:32,698][0m Trial 11 finished with value: 0.06028550589965913 and parameters: {'observation_period_num': 11, 'train_rates': 0.8495197222005556, 'learning_rate': 3.8016649859481656e-06, 'batch_size': 24, 'step_size': 15, 'gamma': 0.8817417265862744}. Best is trial 10 with value: 0.05666680015427501.[0m
[32m[I 2025-01-06 10:27:04,527][0m Trial 12 finished with value: 0.07745612332289167 and parameters: {'observation_period_num': 60, 'train_rates': 0.8561296926970311, 'learning_rate': 4.92219929256914e-06, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8619372869471308}. Best is trial 10 with value: 0.05666680015427501.[0m
[32m[I 2025-01-06 10:27:57,119][0m Trial 13 finished with value: 0.03925908788021665 and parameters: {'observation_period_num': 7, 'train_rates': 0.8856795430912607, 'learning_rate': 0.0009760031162541833, 'batch_size': 171, 'step_size': 12, 'gamma': 0.9164149404309}. Best is trial 13 with value: 0.03925908788021665.[0m
[32m[I 2025-01-06 10:28:46,601][0m Trial 14 finished with value: 0.03516212133655834 and parameters: {'observation_period_num': 8, 'train_rates': 0.9096114121809799, 'learning_rate': 0.0009064824265006803, 'batch_size': 181, 'step_size': 13, 'gamma': 0.9408193703455434}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:29:37,644][0m Trial 15 finished with value: 0.09316158046325047 and parameters: {'observation_period_num': 83, 'train_rates': 0.9175876205594825, 'learning_rate': 0.000891585309540662, 'batch_size': 181, 'step_size': 12, 'gamma': 0.947212746030886}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:30:24,149][0m Trial 16 finished with value: 0.0960874841627428 and parameters: {'observation_period_num': 87, 'train_rates': 0.7941167283854913, 'learning_rate': 0.000969655462873677, 'batch_size': 175, 'step_size': 7, 'gamma': 0.9414524719560993}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:31:20,403][0m Trial 17 finished with value: 0.04302834514696319 and parameters: {'observation_period_num': 29, 'train_rates': 0.9242240211727657, 'learning_rate': 0.0003584647988033597, 'batch_size': 169, 'step_size': 13, 'gamma': 0.9302792985098478}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:32:12,619][0m Trial 18 finished with value: 0.08205905696377158 and parameters: {'observation_period_num': 84, 'train_rates': 0.7770182328142059, 'learning_rate': 0.00037825011904765195, 'batch_size': 220, 'step_size': 15, 'gamma': 0.9587226097533026}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:33:17,929][0m Trial 19 finished with value: 0.049770002696534685 and parameters: {'observation_period_num': 62, 'train_rates': 0.8997830092799478, 'learning_rate': 0.00018268930268125632, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8526884048055505}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:34:11,212][0m Trial 20 finished with value: 0.04708268865942955 and parameters: {'observation_period_num': 24, 'train_rates': 0.9708062136371596, 'learning_rate': 0.0006498396560530638, 'batch_size': 200, 'step_size': 12, 'gamma': 0.9081541295795943}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:35:05,840][0m Trial 21 finished with value: 0.041586760092865334 and parameters: {'observation_period_num': 31, 'train_rates': 0.9323659024339497, 'learning_rate': 0.000465589364618768, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9316051012671334}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:36:13,009][0m Trial 22 finished with value: 0.06418453143876895 and parameters: {'observation_period_num': 23, 'train_rates': 0.9417334353702075, 'learning_rate': 0.0005306645195372934, 'batch_size': 141, 'step_size': 13, 'gamma': 0.9298910172089995}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:37:19,868][0m Trial 23 finished with value: 0.037492083842461964 and parameters: {'observation_period_num': 6, 'train_rates': 0.8923069653370664, 'learning_rate': 0.0002220789678890882, 'batch_size': 156, 'step_size': 11, 'gamma': 0.9680511077513039}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:38:16,388][0m Trial 24 finished with value: 0.04189086488427774 and parameters: {'observation_period_num': 5, 'train_rates': 0.89118846244785, 'learning_rate': 0.00019882716238285784, 'batch_size': 217, 'step_size': 11, 'gamma': 0.9682750985266165}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:39:17,488][0m Trial 25 finished with value: 0.05093196390172698 and parameters: {'observation_period_num': 63, 'train_rates': 0.8388644379375386, 'learning_rate': 0.00023141675153842575, 'batch_size': 159, 'step_size': 10, 'gamma': 0.967781293034854}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:40:13,714][0m Trial 26 finished with value: 0.05141612277789549 and parameters: {'observation_period_num': 47, 'train_rates': 0.8109289938172431, 'learning_rate': 0.0009950841796490977, 'batch_size': 194, 'step_size': 9, 'gamma': 0.9866084650384004}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:41:08,562][0m Trial 27 finished with value: 0.06803428195416927 and parameters: {'observation_period_num': 114, 'train_rates': 0.8819350200297121, 'learning_rate': 0.00010217112344932447, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8927545756192605}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:41:45,042][0m Trial 28 finished with value: 0.12425269948814827 and parameters: {'observation_period_num': 102, 'train_rates': 0.8183694050959749, 'learning_rate': 0.0005939486967277735, 'batch_size': 230, 'step_size': 14, 'gamma': 0.9433544932802858}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:42:38,429][0m Trial 29 finished with value: 0.15270795859396458 and parameters: {'observation_period_num': 175, 'train_rates': 0.7698726002238233, 'learning_rate': 0.00026720688577411836, 'batch_size': 160, 'step_size': 11, 'gamma': 0.9163238467865555}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:43:36,509][0m Trial 30 finished with value: 0.2281825390077175 and parameters: {'observation_period_num': 19, 'train_rates': 0.9094633731737903, 'learning_rate': 3.738814510648687e-05, 'batch_size': 127, 'step_size': 1, 'gamma': 0.8958691282995577}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:44:40,389][0m Trial 31 finished with value: 0.04444434372651065 and parameters: {'observation_period_num': 36, 'train_rates': 0.9442042674206796, 'learning_rate': 0.0005276121187374546, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9250209695150038}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:45:33,415][0m Trial 32 finished with value: 0.09136844671370119 and parameters: {'observation_period_num': 7, 'train_rates': 0.6004649892641938, 'learning_rate': 0.0006221896310339391, 'batch_size': 200, 'step_size': 13, 'gamma': 0.9668415354325732}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:46:46,933][0m Trial 33 finished with value: 0.05929474756121635 and parameters: {'observation_period_num': 32, 'train_rates': 0.9315928062082983, 'learning_rate': 0.00012402139302308712, 'batch_size': 120, 'step_size': 15, 'gamma': 0.94247127125374}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:47:45,653][0m Trial 34 finished with value: 0.10199478230085866 and parameters: {'observation_period_num': 57, 'train_rates': 0.8799185147158168, 'learning_rate': 0.0003177396363614024, 'batch_size': 186, 'step_size': 9, 'gamma': 0.9259780612983048}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:48:57,051][0m Trial 35 finished with value: 0.06748047471046448 and parameters: {'observation_period_num': 20, 'train_rates': 0.9569960193591269, 'learning_rate': 5.678886462728255e-05, 'batch_size': 155, 'step_size': 14, 'gamma': 0.9526248557442338}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:49:58,117][0m Trial 36 finished with value: 0.16244353353977203 and parameters: {'observation_period_num': 73, 'train_rates': 0.9865191156731096, 'learning_rate': 0.0001373223333998889, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9761555908455588}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:50:40,223][0m Trial 37 finished with value: 0.03812924119867856 and parameters: {'observation_period_num': 43, 'train_rates': 0.8643744459726956, 'learning_rate': 0.0004492228094439095, 'batch_size': 171, 'step_size': 10, 'gamma': 0.896616081328607}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:51:27,360][0m Trial 38 finished with value: 0.04329324812420363 and parameters: {'observation_period_num': 49, 'train_rates': 0.8229157188450833, 'learning_rate': 0.000768582567168349, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8496570188007887}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:52:13,528][0m Trial 39 finished with value: 0.2815716861118197 and parameters: {'observation_period_num': 170, 'train_rates': 0.8630181925961952, 'learning_rate': 1.2146772702354852e-05, 'batch_size': 188, 'step_size': 10, 'gamma': 0.8982538076818849}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:52:57,672][0m Trial 40 finished with value: 0.18931202167478695 and parameters: {'observation_period_num': 223, 'train_rates': 0.7457874525932866, 'learning_rate': 0.00041969659393187455, 'batch_size': 207, 'step_size': 8, 'gamma': 0.884497088103219}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:53:50,822][0m Trial 41 finished with value: 0.03715393592247227 and parameters: {'observation_period_num': 36, 'train_rates': 0.8978288477343046, 'learning_rate': 0.0005291348185420632, 'batch_size': 166, 'step_size': 12, 'gamma': 0.9361782075745314}. Best is trial 14 with value: 0.03516212133655834.[0m
[32m[I 2025-01-06 10:54:36,095][0m Trial 42 finished with value: 0.032089789187510126 and parameters: {'observation_period_num': 17, 'train_rates': 0.900703787250127, 'learning_rate': 0.00030259516427507636, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9106995074474016}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 10:55:32,451][0m Trial 43 finished with value: 0.052209502421051476 and parameters: {'observation_period_num': 41, 'train_rates': 0.9045578079488749, 'learning_rate': 0.00015117375261995597, 'batch_size': 130, 'step_size': 10, 'gamma': 0.9056006322503098}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 10:56:22,063][0m Trial 44 finished with value: 0.032338971655953216 and parameters: {'observation_period_num': 17, 'train_rates': 0.8429350974873857, 'learning_rate': 0.00026200361703935035, 'batch_size': 162, 'step_size': 11, 'gamma': 0.8676888022623721}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 10:57:14,056][0m Trial 45 finished with value: 0.4808980967849493 and parameters: {'observation_period_num': 17, 'train_rates': 0.8364967767060371, 'learning_rate': 1.7603171124084759e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8726425286840783}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 10:58:05,753][0m Trial 46 finished with value: 0.06148408786501995 and parameters: {'observation_period_num': 18, 'train_rates': 0.9123920214840069, 'learning_rate': 6.728061586335771e-05, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8323314026599318}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 10:59:03,200][0m Trial 47 finished with value: 0.04386508349027183 and parameters: {'observation_period_num': 53, 'train_rates': 0.8469154976980539, 'learning_rate': 0.0002513354363933654, 'batch_size': 110, 'step_size': 11, 'gamma': 0.7845386067716531}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 11:00:37,404][0m Trial 48 finished with value: 0.050100577948223646 and parameters: {'observation_period_num': 34, 'train_rates': 0.9595150645064394, 'learning_rate': 0.00028492539461266037, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8623448382251157}. Best is trial 42 with value: 0.032089789187510126.[0m
[32m[I 2025-01-06 11:01:28,219][0m Trial 49 finished with value: 0.2250946359023736 and parameters: {'observation_period_num': 72, 'train_rates': 0.8726606595653413, 'learning_rate': 2.1054366101352645e-05, 'batch_size': 184, 'step_size': 14, 'gamma': 0.9777178499875309}. Best is trial 42 with value: 0.032089789187510126.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 14, 'train_rates': 0.8347021708328278, 'learning_rate': 0.0005705938137745586, 'batch_size': 173, 'step_size': 15, 'gamma': 0.86739050385293}
Epoch 1/300, trend Loss: 0.3646 | 0.2781
Epoch 2/300, trend Loss: 0.1779 | 0.3021
Epoch 3/300, trend Loss: 0.1792 | 0.1763
Epoch 4/300, trend Loss: 0.1601 | 0.1300
Epoch 5/300, trend Loss: 0.1468 | 0.1266
Epoch 6/300, trend Loss: 0.1203 | 0.1453
Epoch 7/300, trend Loss: 0.1255 | 0.0994
Epoch 8/300, trend Loss: 0.1168 | 0.0806
Epoch 9/300, trend Loss: 0.1060 | 0.1200
Epoch 10/300, trend Loss: 0.1082 | 0.1249
Epoch 11/300, trend Loss: 0.1319 | 0.1162
Epoch 12/300, trend Loss: 0.1374 | 0.0688
Epoch 13/300, trend Loss: 0.1080 | 0.0602
Epoch 14/300, trend Loss: 0.1261 | 0.1241
Epoch 15/300, trend Loss: 0.1230 | 0.1009
Epoch 16/300, trend Loss: 0.1268 | 0.1529
Epoch 17/300, trend Loss: 0.1170 | 0.0837
Epoch 18/300, trend Loss: 0.1103 | 0.0792
Epoch 19/300, trend Loss: 0.1145 | 0.0701
Epoch 20/300, trend Loss: 0.0966 | 0.0507
Epoch 21/300, trend Loss: 0.0950 | 0.1104
Epoch 22/300, trend Loss: 0.1037 | 0.0703
Epoch 23/300, trend Loss: 0.1037 | 0.0570
Epoch 24/300, trend Loss: 0.0973 | 0.0547
Epoch 25/300, trend Loss: 0.0889 | 0.0466
Epoch 26/300, trend Loss: 0.0876 | 0.0601
Epoch 27/300, trend Loss: 0.0894 | 0.0435
Epoch 28/300, trend Loss: 0.0853 | 0.0485
Epoch 29/300, trend Loss: 0.0868 | 0.0426
Epoch 30/300, trend Loss: 0.0826 | 0.0672
Epoch 31/300, trend Loss: 0.0890 | 0.0778
Epoch 32/300, trend Loss: 0.0952 | 0.0470
Epoch 33/300, trend Loss: 0.0835 | 0.0443
Epoch 34/300, trend Loss: 0.0824 | 0.0518
Epoch 35/300, trend Loss: 0.0841 | 0.0390
Epoch 36/300, trend Loss: 0.0795 | 0.0382
Epoch 37/300, trend Loss: 0.0797 | 0.0378
Epoch 38/300, trend Loss: 0.0758 | 0.0527
Epoch 39/300, trend Loss: 0.0768 | 0.0375
Epoch 40/300, trend Loss: 0.0756 | 0.0352
Epoch 41/300, trend Loss: 0.0743 | 0.0426
Epoch 42/300, trend Loss: 0.0733 | 0.0411
Epoch 43/300, trend Loss: 0.0732 | 0.0335
Epoch 44/300, trend Loss: 0.0723 | 0.0330
Epoch 45/300, trend Loss: 0.0727 | 0.0434
Epoch 46/300, trend Loss: 0.0722 | 0.0338
Epoch 47/300, trend Loss: 0.0710 | 0.0325
Epoch 48/300, trend Loss: 0.0724 | 0.0356
Epoch 49/300, trend Loss: 0.0728 | 0.0383
Epoch 50/300, trend Loss: 0.0731 | 0.0327
Epoch 51/300, trend Loss: 0.0727 | 0.0326
Epoch 52/300, trend Loss: 0.0718 | 0.0376
Epoch 53/300, trend Loss: 0.0706 | 0.0419
Epoch 54/300, trend Loss: 0.0729 | 0.0328
Epoch 55/300, trend Loss: 0.0713 | 0.0322
Epoch 56/300, trend Loss: 0.0697 | 0.0391
Epoch 57/300, trend Loss: 0.0698 | 0.0311
Epoch 58/300, trend Loss: 0.0700 | 0.0330
Epoch 59/300, trend Loss: 0.0712 | 0.0342
Epoch 60/300, trend Loss: 0.0684 | 0.0427
Epoch 61/300, trend Loss: 0.0695 | 0.0308
Epoch 62/300, trend Loss: 0.0688 | 0.0308
Epoch 63/300, trend Loss: 0.0683 | 0.0407
Epoch 64/300, trend Loss: 0.0686 | 0.0302
Epoch 65/300, trend Loss: 0.0680 | 0.0291
Epoch 66/300, trend Loss: 0.0687 | 0.0391
Epoch 67/300, trend Loss: 0.0685 | 0.0292
Epoch 68/300, trend Loss: 0.0682 | 0.0297
Epoch 69/300, trend Loss: 0.0677 | 0.0357
Epoch 70/300, trend Loss: 0.0679 | 0.0286
Epoch 71/300, trend Loss: 0.0672 | 0.0291
Epoch 72/300, trend Loss: 0.0654 | 0.0330
Epoch 73/300, trend Loss: 0.0650 | 0.0285
Epoch 74/300, trend Loss: 0.0644 | 0.0282
Epoch 75/300, trend Loss: 0.0640 | 0.0327
Epoch 76/300, trend Loss: 0.0636 | 0.0269
Epoch 77/300, trend Loss: 0.0630 | 0.0300
Epoch 78/300, trend Loss: 0.0627 | 0.0281
Epoch 79/300, trend Loss: 0.0630 | 0.0260
Epoch 80/300, trend Loss: 0.0634 | 0.0307
Epoch 81/300, trend Loss: 0.0636 | 0.0288
Epoch 82/300, trend Loss: 0.0627 | 0.0256
Epoch 83/300, trend Loss: 0.0621 | 0.0275
Epoch 84/300, trend Loss: 0.0614 | 0.0284
Epoch 85/300, trend Loss: 0.0617 | 0.0253
Epoch 86/300, trend Loss: 0.0619 | 0.0286
Epoch 87/300, trend Loss: 0.0615 | 0.0264
Epoch 88/300, trend Loss: 0.0611 | 0.0251
Epoch 89/300, trend Loss: 0.0605 | 0.0278
Epoch 90/300, trend Loss: 0.0598 | 0.0254
Epoch 91/300, trend Loss: 0.0595 | 0.0244
Epoch 92/300, trend Loss: 0.0596 | 0.0258
Epoch 93/300, trend Loss: 0.0602 | 0.0237
Epoch 94/300, trend Loss: 0.0604 | 0.0250
Epoch 95/300, trend Loss: 0.0625 | 0.0274
Epoch 96/300, trend Loss: 0.0609 | 0.0247
Epoch 97/300, trend Loss: 0.0622 | 0.0259
Epoch 98/300, trend Loss: 0.0610 | 0.0271
Epoch 99/300, trend Loss: 0.0597 | 0.0252
Epoch 100/300, trend Loss: 0.0599 | 0.0244
Epoch 101/300, trend Loss: 0.0591 | 0.0263
Epoch 102/300, trend Loss: 0.0590 | 0.0235
Epoch 103/300, trend Loss: 0.0588 | 0.0262
Epoch 104/300, trend Loss: 0.0584 | 0.0230
Epoch 105/300, trend Loss: 0.0583 | 0.0231
Epoch 106/300, trend Loss: 0.0584 | 0.0241
Epoch 107/300, trend Loss: 0.0600 | 0.0249
Epoch 108/300, trend Loss: 0.0599 | 0.0243
Epoch 109/300, trend Loss: 0.0586 | 0.0238
Epoch 110/300, trend Loss: 0.0598 | 0.0248
Epoch 111/300, trend Loss: 0.0599 | 0.0262
Epoch 112/300, trend Loss: 0.0592 | 0.0237
Epoch 113/300, trend Loss: 0.0592 | 0.0257
Epoch 114/300, trend Loss: 0.0578 | 0.0242
Epoch 115/300, trend Loss: 0.0574 | 0.0233
Epoch 116/300, trend Loss: 0.0575 | 0.0250
Epoch 117/300, trend Loss: 0.0572 | 0.0230
Epoch 118/300, trend Loss: 0.0572 | 0.0245
Epoch 119/300, trend Loss: 0.0573 | 0.0230
Epoch 120/300, trend Loss: 0.0572 | 0.0245
Epoch 121/300, trend Loss: 0.0573 | 0.0226
Epoch 122/300, trend Loss: 0.0569 | 0.0237
Epoch 123/300, trend Loss: 0.0569 | 0.0224
Epoch 124/300, trend Loss: 0.0571 | 0.0244
Epoch 125/300, trend Loss: 0.0583 | 0.0228
Epoch 126/300, trend Loss: 0.0590 | 0.0260
Epoch 127/300, trend Loss: 0.0624 | 0.0246
Epoch 128/300, trend Loss: 0.0631 | 0.0275
Epoch 129/300, trend Loss: 0.0658 | 0.0327
Epoch 130/300, trend Loss: 0.0670 | 0.0256
Epoch 131/300, trend Loss: 0.0722 | 0.0539
Epoch 132/300, trend Loss: 0.0717 | 0.0297
Epoch 133/300, trend Loss: 0.0714 | 0.0631
Epoch 134/300, trend Loss: 0.0663 | 0.0294
Epoch 135/300, trend Loss: 0.0637 | 0.0473
Epoch 136/300, trend Loss: 0.0595 | 0.0259
Epoch 137/300, trend Loss: 0.0577 | 0.0322
Epoch 138/300, trend Loss: 0.0566 | 0.0244
Epoch 139/300, trend Loss: 0.0561 | 0.0260
Epoch 140/300, trend Loss: 0.0558 | 0.0244
Epoch 141/300, trend Loss: 0.0556 | 0.0245
Epoch 142/300, trend Loss: 0.0555 | 0.0241
Epoch 143/300, trend Loss: 0.0554 | 0.0241
Epoch 144/300, trend Loss: 0.0553 | 0.0239
Epoch 145/300, trend Loss: 0.0553 | 0.0238
Epoch 146/300, trend Loss: 0.0552 | 0.0236
Epoch 147/300, trend Loss: 0.0551 | 0.0236
Epoch 148/300, trend Loss: 0.0551 | 0.0235
Epoch 149/300, trend Loss: 0.0551 | 0.0234
Epoch 150/300, trend Loss: 0.0551 | 0.0234
Epoch 151/300, trend Loss: 0.0550 | 0.0234
Epoch 152/300, trend Loss: 0.0549 | 0.0234
Epoch 153/300, trend Loss: 0.0549 | 0.0233
Epoch 154/300, trend Loss: 0.0549 | 0.0232
Epoch 155/300, trend Loss: 0.0548 | 0.0232
Epoch 156/300, trend Loss: 0.0548 | 0.0231
Epoch 157/300, trend Loss: 0.0547 | 0.0231
Epoch 158/300, trend Loss: 0.0547 | 0.0230
Epoch 159/300, trend Loss: 0.0547 | 0.0230
Epoch 160/300, trend Loss: 0.0546 | 0.0230
Epoch 161/300, trend Loss: 0.0546 | 0.0230
Epoch 162/300, trend Loss: 0.0546 | 0.0229
Epoch 163/300, trend Loss: 0.0545 | 0.0229
Epoch 164/300, trend Loss: 0.0545 | 0.0229
Epoch 165/300, trend Loss: 0.0545 | 0.0228
Epoch 166/300, trend Loss: 0.0544 | 0.0228
Epoch 167/300, trend Loss: 0.0544 | 0.0228
Epoch 168/300, trend Loss: 0.0544 | 0.0227
Epoch 169/300, trend Loss: 0.0543 | 0.0227
Epoch 170/300, trend Loss: 0.0543 | 0.0227
Epoch 171/300, trend Loss: 0.0543 | 0.0227
Epoch 172/300, trend Loss: 0.0543 | 0.0226
Epoch 173/300, trend Loss: 0.0542 | 0.0226
Epoch 174/300, trend Loss: 0.0542 | 0.0226
Epoch 175/300, trend Loss: 0.0542 | 0.0226
Epoch 176/300, trend Loss: 0.0541 | 0.0226
Epoch 177/300, trend Loss: 0.0541 | 0.0225
Epoch 178/300, trend Loss: 0.0541 | 0.0225
Epoch 179/300, trend Loss: 0.0541 | 0.0225
Epoch 180/300, trend Loss: 0.0540 | 0.0225
Epoch 181/300, trend Loss: 0.0540 | 0.0225
Epoch 182/300, trend Loss: 0.0540 | 0.0224
Epoch 183/300, trend Loss: 0.0540 | 0.0224
Epoch 184/300, trend Loss: 0.0540 | 0.0224
Epoch 185/300, trend Loss: 0.0539 | 0.0224
Epoch 186/300, trend Loss: 0.0539 | 0.0224
Epoch 187/300, trend Loss: 0.0539 | 0.0223
Epoch 188/300, trend Loss: 0.0539 | 0.0223
Epoch 189/300, trend Loss: 0.0538 | 0.0223
Epoch 190/300, trend Loss: 0.0538 | 0.0223
Epoch 191/300, trend Loss: 0.0538 | 0.0223
Epoch 192/300, trend Loss: 0.0538 | 0.0223
Epoch 193/300, trend Loss: 0.0538 | 0.0222
Epoch 194/300, trend Loss: 0.0537 | 0.0222
Epoch 195/300, trend Loss: 0.0537 | 0.0222
Epoch 196/300, trend Loss: 0.0537 | 0.0222
Epoch 197/300, trend Loss: 0.0537 | 0.0222
Epoch 198/300, trend Loss: 0.0537 | 0.0222
Epoch 199/300, trend Loss: 0.0537 | 0.0222
Epoch 200/300, trend Loss: 0.0536 | 0.0221
Epoch 201/300, trend Loss: 0.0536 | 0.0221
Epoch 202/300, trend Loss: 0.0536 | 0.0221
Epoch 203/300, trend Loss: 0.0536 | 0.0221
Epoch 204/300, trend Loss: 0.0536 | 0.0221
Epoch 205/300, trend Loss: 0.0536 | 0.0221
Epoch 206/300, trend Loss: 0.0535 | 0.0221
Epoch 207/300, trend Loss: 0.0535 | 0.0221
Epoch 208/300, trend Loss: 0.0535 | 0.0220
Epoch 209/300, trend Loss: 0.0535 | 0.0220
Epoch 210/300, trend Loss: 0.0535 | 0.0220
Epoch 211/300, trend Loss: 0.0535 | 0.0220
Epoch 212/300, trend Loss: 0.0534 | 0.0220
Epoch 213/300, trend Loss: 0.0534 | 0.0220
Epoch 214/300, trend Loss: 0.0534 | 0.0220
Epoch 215/300, trend Loss: 0.0534 | 0.0220
Epoch 216/300, trend Loss: 0.0534 | 0.0220
Epoch 217/300, trend Loss: 0.0534 | 0.0220
Epoch 218/300, trend Loss: 0.0534 | 0.0219
Epoch 219/300, trend Loss: 0.0534 | 0.0219
Epoch 220/300, trend Loss: 0.0533 | 0.0219
Epoch 221/300, trend Loss: 0.0533 | 0.0219
Epoch 222/300, trend Loss: 0.0533 | 0.0219
Epoch 223/300, trend Loss: 0.0533 | 0.0219
Epoch 224/300, trend Loss: 0.0533 | 0.0219
Epoch 225/300, trend Loss: 0.0533 | 0.0219
Epoch 226/300, trend Loss: 0.0533 | 0.0219
Epoch 227/300, trend Loss: 0.0533 | 0.0219
Epoch 228/300, trend Loss: 0.0532 | 0.0219
Epoch 229/300, trend Loss: 0.0532 | 0.0219
Epoch 230/300, trend Loss: 0.0532 | 0.0218
Epoch 231/300, trend Loss: 0.0532 | 0.0218
Epoch 232/300, trend Loss: 0.0532 | 0.0218
Epoch 233/300, trend Loss: 0.0532 | 0.0218
Epoch 234/300, trend Loss: 0.0532 | 0.0218
Epoch 235/300, trend Loss: 0.0532 | 0.0218
Epoch 236/300, trend Loss: 0.0532 | 0.0218
Epoch 237/300, trend Loss: 0.0532 | 0.0218
Epoch 238/300, trend Loss: 0.0531 | 0.0218
Epoch 239/300, trend Loss: 0.0531 | 0.0218
Epoch 240/300, trend Loss: 0.0531 | 0.0218
Epoch 241/300, trend Loss: 0.0531 | 0.0218
Epoch 242/300, trend Loss: 0.0531 | 0.0218
Epoch 243/300, trend Loss: 0.0531 | 0.0218
Epoch 244/300, trend Loss: 0.0531 | 0.0218
Epoch 245/300, trend Loss: 0.0531 | 0.0218
Epoch 246/300, trend Loss: 0.0531 | 0.0217
Epoch 247/300, trend Loss: 0.0531 | 0.0217
Epoch 248/300, trend Loss: 0.0531 | 0.0217
Epoch 249/300, trend Loss: 0.0531 | 0.0217
Epoch 250/300, trend Loss: 0.0530 | 0.0217
Epoch 251/300, trend Loss: 0.0530 | 0.0217
Epoch 252/300, trend Loss: 0.0530 | 0.0217
Epoch 253/300, trend Loss: 0.0530 | 0.0217
Epoch 254/300, trend Loss: 0.0530 | 0.0217
Epoch 255/300, trend Loss: 0.0530 | 0.0217
Epoch 256/300, trend Loss: 0.0530 | 0.0217
Epoch 257/300, trend Loss: 0.0530 | 0.0217
Epoch 258/300, trend Loss: 0.0530 | 0.0217
Epoch 259/300, trend Loss: 0.0530 | 0.0217
Epoch 260/300, trend Loss: 0.0530 | 0.0217
Epoch 261/300, trend Loss: 0.0530 | 0.0217
Epoch 262/300, trend Loss: 0.0530 | 0.0217
Epoch 263/300, trend Loss: 0.0530 | 0.0217
Epoch 264/300, trend Loss: 0.0529 | 0.0217
Epoch 265/300, trend Loss: 0.0529 | 0.0217
Epoch 266/300, trend Loss: 0.0529 | 0.0217
Epoch 267/300, trend Loss: 0.0529 | 0.0217
Epoch 268/300, trend Loss: 0.0529 | 0.0217
Epoch 269/300, trend Loss: 0.0529 | 0.0217
Epoch 270/300, trend Loss: 0.0529 | 0.0216
Epoch 271/300, trend Loss: 0.0529 | 0.0216
Epoch 272/300, trend Loss: 0.0529 | 0.0216
Epoch 273/300, trend Loss: 0.0529 | 0.0216
Epoch 274/300, trend Loss: 0.0529 | 0.0216
Epoch 275/300, trend Loss: 0.0529 | 0.0216
Epoch 276/300, trend Loss: 0.0529 | 0.0216
Epoch 277/300, trend Loss: 0.0529 | 0.0216
Epoch 278/300, trend Loss: 0.0529 | 0.0216
Epoch 279/300, trend Loss: 0.0529 | 0.0216
Epoch 280/300, trend Loss: 0.0529 | 0.0216
Epoch 281/300, trend Loss: 0.0529 | 0.0216
Epoch 282/300, trend Loss: 0.0529 | 0.0216
Epoch 283/300, trend Loss: 0.0528 | 0.0216
Epoch 284/300, trend Loss: 0.0528 | 0.0216
Epoch 285/300, trend Loss: 0.0528 | 0.0216
Epoch 286/300, trend Loss: 0.0528 | 0.0216
Epoch 287/300, trend Loss: 0.0528 | 0.0216
Epoch 288/300, trend Loss: 0.0528 | 0.0216
Epoch 289/300, trend Loss: 0.0528 | 0.0216
Epoch 290/300, trend Loss: 0.0528 | 0.0216
Epoch 291/300, trend Loss: 0.0528 | 0.0216
Epoch 292/300, trend Loss: 0.0528 | 0.0216
Epoch 293/300, trend Loss: 0.0528 | 0.0216
Epoch 294/300, trend Loss: 0.0528 | 0.0216
Epoch 295/300, trend Loss: 0.0528 | 0.0216
Epoch 296/300, trend Loss: 0.0528 | 0.0216
Epoch 297/300, trend Loss: 0.0528 | 0.0216
Epoch 298/300, trend Loss: 0.0528 | 0.0216
Epoch 299/300, trend Loss: 0.0528 | 0.0216
Epoch 300/300, trend Loss: 0.0528 | 0.0216
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.8384098880342319, 'learning_rate': 0.00015438870002164033, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9591877465940127}
Epoch 1/300, seasonal_0 Loss: 0.2657 | 0.1946
Epoch 2/300, seasonal_0 Loss: 0.1599 | 0.1419
Epoch 3/300, seasonal_0 Loss: 0.1408 | 0.1083
Epoch 4/300, seasonal_0 Loss: 0.1316 | 0.0984
Epoch 5/300, seasonal_0 Loss: 0.1318 | 0.1190
Epoch 6/300, seasonal_0 Loss: 0.1317 | 0.1699
Epoch 7/300, seasonal_0 Loss: 0.1282 | 0.1789
Epoch 8/300, seasonal_0 Loss: 0.1218 | 0.1778
Epoch 9/300, seasonal_0 Loss: 0.1253 | 0.1214
Epoch 10/300, seasonal_0 Loss: 0.1375 | 0.1054
Epoch 11/300, seasonal_0 Loss: 0.1295 | 0.0922
Epoch 12/300, seasonal_0 Loss: 0.1167 | 0.0875
Epoch 13/300, seasonal_0 Loss: 0.1193 | 0.0911
Epoch 14/300, seasonal_0 Loss: 0.1223 | 0.0972
Epoch 15/300, seasonal_0 Loss: 0.1197 | 0.0879
Epoch 16/300, seasonal_0 Loss: 0.1081 | 0.0792
Epoch 17/300, seasonal_0 Loss: 0.1034 | 0.0786
Epoch 18/300, seasonal_0 Loss: 0.1024 | 0.0787
Epoch 19/300, seasonal_0 Loss: 0.1012 | 0.0775
Epoch 20/300, seasonal_0 Loss: 0.1000 | 0.0762
Epoch 21/300, seasonal_0 Loss: 0.0993 | 0.0746
Epoch 22/300, seasonal_0 Loss: 0.0993 | 0.0736
Epoch 23/300, seasonal_0 Loss: 0.0997 | 0.0740
Epoch 24/300, seasonal_0 Loss: 0.0989 | 0.0724
Epoch 25/300, seasonal_0 Loss: 0.0959 | 0.0681
Epoch 26/300, seasonal_0 Loss: 0.0924 | 0.0660
Epoch 27/300, seasonal_0 Loss: 0.0907 | 0.0653
Epoch 28/300, seasonal_0 Loss: 0.0911 | 0.0700
Epoch 29/300, seasonal_0 Loss: 0.0920 | 0.0702
Epoch 30/300, seasonal_0 Loss: 0.0927 | 0.0711
Epoch 31/300, seasonal_0 Loss: 0.0925 | 0.0716
Epoch 32/300, seasonal_0 Loss: 0.0911 | 0.0681
Epoch 33/300, seasonal_0 Loss: 0.0880 | 0.0647
Epoch 34/300, seasonal_0 Loss: 0.0858 | 0.0632
Epoch 35/300, seasonal_0 Loss: 0.0901 | 0.0641
Epoch 36/300, seasonal_0 Loss: 0.0992 | 0.0725
Epoch 37/300, seasonal_0 Loss: 0.0979 | 0.0703
Epoch 38/300, seasonal_0 Loss: 0.0874 | 0.0736
Epoch 39/300, seasonal_0 Loss: 0.0844 | 0.0650
Epoch 40/300, seasonal_0 Loss: 0.0842 | 0.0618
Epoch 41/300, seasonal_0 Loss: 0.0859 | 0.0597
Epoch 42/300, seasonal_0 Loss: 0.0855 | 0.0611
Epoch 43/300, seasonal_0 Loss: 0.0872 | 0.0633
Epoch 44/300, seasonal_0 Loss: 0.0872 | 0.0657
Epoch 45/300, seasonal_0 Loss: 0.0824 | 0.0625
Epoch 46/300, seasonal_0 Loss: 0.0777 | 0.0621
Epoch 47/300, seasonal_0 Loss: 0.0783 | 0.0686
Epoch 48/300, seasonal_0 Loss: 0.0784 | 0.0595
Epoch 49/300, seasonal_0 Loss: 0.0775 | 0.0629
Epoch 50/300, seasonal_0 Loss: 0.0775 | 0.0575
Epoch 51/300, seasonal_0 Loss: 0.0760 | 0.0622
Epoch 52/300, seasonal_0 Loss: 0.0753 | 0.0572
Epoch 53/300, seasonal_0 Loss: 0.0745 | 0.0625
Epoch 54/300, seasonal_0 Loss: 0.0747 | 0.0588
Epoch 55/300, seasonal_0 Loss: 0.0777 | 0.0683
Epoch 56/300, seasonal_0 Loss: 0.0795 | 0.0646
Epoch 57/300, seasonal_0 Loss: 0.0757 | 0.0655
Epoch 58/300, seasonal_0 Loss: 0.0729 | 0.0618
Epoch 59/300, seasonal_0 Loss: 0.0724 | 0.0626
Epoch 60/300, seasonal_0 Loss: 0.0742 | 0.0607
Epoch 61/300, seasonal_0 Loss: 0.0760 | 0.0589
Epoch 62/300, seasonal_0 Loss: 0.0786 | 0.0600
Epoch 63/300, seasonal_0 Loss: 0.0777 | 0.0634
Epoch 64/300, seasonal_0 Loss: 0.0747 | 0.0632
Epoch 65/300, seasonal_0 Loss: 0.0710 | 0.0594
Epoch 66/300, seasonal_0 Loss: 0.0698 | 0.0634
Epoch 67/300, seasonal_0 Loss: 0.0690 | 0.0581
Epoch 68/300, seasonal_0 Loss: 0.0686 | 0.0581
Epoch 69/300, seasonal_0 Loss: 0.0685 | 0.0568
Epoch 70/300, seasonal_0 Loss: 0.0685 | 0.0573
Epoch 71/300, seasonal_0 Loss: 0.0682 | 0.0575
Epoch 72/300, seasonal_0 Loss: 0.0674 | 0.0582
Epoch 73/300, seasonal_0 Loss: 0.0667 | 0.0582
Epoch 74/300, seasonal_0 Loss: 0.0663 | 0.0576
Epoch 75/300, seasonal_0 Loss: 0.0662 | 0.0576
Epoch 76/300, seasonal_0 Loss: 0.0665 | 0.0577
Epoch 77/300, seasonal_0 Loss: 0.0674 | 0.0588
Epoch 78/300, seasonal_0 Loss: 0.0699 | 0.0631
Epoch 79/300, seasonal_0 Loss: 0.0720 | 0.0627
Epoch 80/300, seasonal_0 Loss: 0.0677 | 0.0631
Epoch 81/300, seasonal_0 Loss: 0.0660 | 0.0621
Epoch 82/300, seasonal_0 Loss: 0.0687 | 0.0612
Epoch 83/300, seasonal_0 Loss: 0.0748 | 0.0615
Epoch 84/300, seasonal_0 Loss: 0.0763 | 0.0617
Epoch 85/300, seasonal_0 Loss: 0.0718 | 0.0631
Epoch 86/300, seasonal_0 Loss: 0.0647 | 0.0658
Epoch 87/300, seasonal_0 Loss: 0.0646 | 0.0635
Epoch 88/300, seasonal_0 Loss: 0.0633 | 0.0611
Epoch 89/300, seasonal_0 Loss: 0.0643 | 0.0623
Epoch 90/300, seasonal_0 Loss: 0.0711 | 0.0619
Epoch 91/300, seasonal_0 Loss: 0.0690 | 0.0623
Epoch 92/300, seasonal_0 Loss: 0.0678 | 0.0669
Epoch 93/300, seasonal_0 Loss: 0.0701 | 0.0642
Epoch 94/300, seasonal_0 Loss: 0.0686 | 0.0647
Epoch 95/300, seasonal_0 Loss: 0.0657 | 0.0597
Epoch 96/300, seasonal_0 Loss: 0.0650 | 0.0618
Epoch 97/300, seasonal_0 Loss: 0.0675 | 0.0595
Epoch 98/300, seasonal_0 Loss: 0.0653 | 0.0626
Epoch 99/300, seasonal_0 Loss: 0.0628 | 0.0587
Epoch 100/300, seasonal_0 Loss: 0.0620 | 0.0606
Epoch 101/300, seasonal_0 Loss: 0.0641 | 0.0607
Epoch 102/300, seasonal_0 Loss: 0.0652 | 0.0669
Epoch 103/300, seasonal_0 Loss: 0.0639 | 0.0616
Epoch 104/300, seasonal_0 Loss: 0.0617 | 0.0578
Epoch 105/300, seasonal_0 Loss: 0.0612 | 0.0589
Epoch 106/300, seasonal_0 Loss: 0.0625 | 0.0602
Epoch 107/300, seasonal_0 Loss: 0.0628 | 0.0603
Epoch 108/300, seasonal_0 Loss: 0.0629 | 0.0621
Epoch 109/300, seasonal_0 Loss: 0.0636 | 0.0613
Epoch 110/300, seasonal_0 Loss: 0.0637 | 0.0611
Epoch 111/300, seasonal_0 Loss: 0.0621 | 0.0613
Epoch 112/300, seasonal_0 Loss: 0.0614 | 0.0625
Epoch 113/300, seasonal_0 Loss: 0.0617 | 0.0638
Epoch 114/300, seasonal_0 Loss: 0.0634 | 0.0633
Epoch 115/300, seasonal_0 Loss: 0.0676 | 0.0651
Epoch 116/300, seasonal_0 Loss: 0.0658 | 0.0661
Epoch 117/300, seasonal_0 Loss: 0.0645 | 0.0715
Epoch 118/300, seasonal_0 Loss: 0.0638 | 0.0724
Epoch 119/300, seasonal_0 Loss: 0.0619 | 0.0728
Epoch 120/300, seasonal_0 Loss: 0.0609 | 0.0699
Epoch 121/300, seasonal_0 Loss: 0.0612 | 0.0681
Epoch 122/300, seasonal_0 Loss: 0.0624 | 0.0648
Epoch 123/300, seasonal_0 Loss: 0.0643 | 0.0634
Epoch 124/300, seasonal_0 Loss: 0.0619 | 0.0634
Epoch 125/300, seasonal_0 Loss: 0.0601 | 0.0615
Epoch 126/300, seasonal_0 Loss: 0.0600 | 0.0656
Epoch 127/300, seasonal_0 Loss: 0.0614 | 0.0658
Epoch 128/300, seasonal_0 Loss: 0.0602 | 0.0647
Epoch 129/300, seasonal_0 Loss: 0.0599 | 0.0641
Epoch 130/300, seasonal_0 Loss: 0.0576 | 0.0621
Epoch 131/300, seasonal_0 Loss: 0.0568 | 0.0621
Epoch 132/300, seasonal_0 Loss: 0.0577 | 0.0650
Epoch 133/300, seasonal_0 Loss: 0.0597 | 0.0649
Epoch 134/300, seasonal_0 Loss: 0.0597 | 0.0657
Epoch 135/300, seasonal_0 Loss: 0.0572 | 0.0632
Epoch 136/300, seasonal_0 Loss: 0.0563 | 0.0638
Epoch 137/300, seasonal_0 Loss: 0.0559 | 0.0640
Epoch 138/300, seasonal_0 Loss: 0.0561 | 0.0647
Epoch 139/300, seasonal_0 Loss: 0.0572 | 0.0659
Epoch 140/300, seasonal_0 Loss: 0.0575 | 0.0651
Epoch 141/300, seasonal_0 Loss: 0.0572 | 0.0637
Epoch 142/300, seasonal_0 Loss: 0.0556 | 0.0620
Epoch 143/300, seasonal_0 Loss: 0.0549 | 0.0622
Epoch 144/300, seasonal_0 Loss: 0.0557 | 0.0641
Epoch 145/300, seasonal_0 Loss: 0.0566 | 0.0666
Epoch 146/300, seasonal_0 Loss: 0.0567 | 0.0667
Epoch 147/300, seasonal_0 Loss: 0.0560 | 0.0657
Epoch 148/300, seasonal_0 Loss: 0.0550 | 0.0650
Epoch 149/300, seasonal_0 Loss: 0.0546 | 0.0644
Epoch 150/300, seasonal_0 Loss: 0.0547 | 0.0639
Epoch 151/300, seasonal_0 Loss: 0.0549 | 0.0651
Epoch 152/300, seasonal_0 Loss: 0.0556 | 0.0657
Epoch 153/300, seasonal_0 Loss: 0.0550 | 0.0642
Epoch 154/300, seasonal_0 Loss: 0.0545 | 0.0628
Epoch 155/300, seasonal_0 Loss: 0.0545 | 0.0626
Epoch 156/300, seasonal_0 Loss: 0.0551 | 0.0650
Epoch 157/300, seasonal_0 Loss: 0.0569 | 0.0670
Epoch 158/300, seasonal_0 Loss: 0.0570 | 0.0652
Epoch 159/300, seasonal_0 Loss: 0.0546 | 0.0643
Epoch 160/300, seasonal_0 Loss: 0.0536 | 0.0635
Epoch 161/300, seasonal_0 Loss: 0.0533 | 0.0650
Epoch 162/300, seasonal_0 Loss: 0.0544 | 0.0663
Epoch 163/300, seasonal_0 Loss: 0.0542 | 0.0674
Epoch 164/300, seasonal_0 Loss: 0.0549 | 0.0676
Epoch 165/300, seasonal_0 Loss: 0.0538 | 0.0670
Epoch 166/300, seasonal_0 Loss: 0.0535 | 0.0657
Epoch 167/300, seasonal_0 Loss: 0.0545 | 0.0654
Epoch 168/300, seasonal_0 Loss: 0.0554 | 0.0652
Epoch 169/300, seasonal_0 Loss: 0.0557 | 0.0658
Epoch 170/300, seasonal_0 Loss: 0.0527 | 0.0657
Epoch 171/300, seasonal_0 Loss: 0.0520 | 0.0655
Epoch 172/300, seasonal_0 Loss: 0.0519 | 0.0666
Epoch 173/300, seasonal_0 Loss: 0.0521 | 0.0666
Epoch 174/300, seasonal_0 Loss: 0.0519 | 0.0661
Epoch 175/300, seasonal_0 Loss: 0.0516 | 0.0657
Epoch 176/300, seasonal_0 Loss: 0.0512 | 0.0652
Epoch 177/300, seasonal_0 Loss: 0.0511 | 0.0651
Epoch 178/300, seasonal_0 Loss: 0.0515 | 0.0662
Epoch 179/300, seasonal_0 Loss: 0.0521 | 0.0664
Epoch 180/300, seasonal_0 Loss: 0.0524 | 0.0675
Epoch 181/300, seasonal_0 Loss: 0.0514 | 0.0667
Epoch 182/300, seasonal_0 Loss: 0.0509 | 0.0668
Epoch 183/300, seasonal_0 Loss: 0.0517 | 0.0682
Epoch 184/300, seasonal_0 Loss: 0.0521 | 0.0695
Epoch 185/300, seasonal_0 Loss: 0.0528 | 0.0704
Epoch 186/300, seasonal_0 Loss: 0.0522 | 0.0709
Epoch 187/300, seasonal_0 Loss: 0.0519 | 0.0721
Epoch 188/300, seasonal_0 Loss: 0.0516 | 0.0707
Epoch 189/300, seasonal_0 Loss: 0.0534 | 0.0699
Epoch 190/300, seasonal_0 Loss: 0.0545 | 0.0680
Epoch 191/300, seasonal_0 Loss: 0.0529 | 0.0658
Epoch 192/300, seasonal_0 Loss: 0.0521 | 0.0676
Epoch 193/300, seasonal_0 Loss: 0.0511 | 0.0672
Epoch 194/300, seasonal_0 Loss: 0.0515 | 0.0681
Epoch 195/300, seasonal_0 Loss: 0.0501 | 0.0671
Epoch 196/300, seasonal_0 Loss: 0.0499 | 0.0666
Epoch 197/300, seasonal_0 Loss: 0.0493 | 0.0677
Epoch 198/300, seasonal_0 Loss: 0.0490 | 0.0699
Epoch 199/300, seasonal_0 Loss: 0.0488 | 0.0691
Epoch 200/300, seasonal_0 Loss: 0.0484 | 0.0690
Epoch 201/300, seasonal_0 Loss: 0.0483 | 0.0689
Epoch 202/300, seasonal_0 Loss: 0.0483 | 0.0691
Epoch 203/300, seasonal_0 Loss: 0.0481 | 0.0684
Epoch 204/300, seasonal_0 Loss: 0.0474 | 0.0673
Epoch 205/300, seasonal_0 Loss: 0.0472 | 0.0678
Epoch 206/300, seasonal_0 Loss: 0.0468 | 0.0674
Epoch 207/300, seasonal_0 Loss: 0.0463 | 0.0671
Epoch 208/300, seasonal_0 Loss: 0.0464 | 0.0657
Epoch 209/300, seasonal_0 Loss: 0.0478 | 0.0673
Epoch 210/300, seasonal_0 Loss: 0.0486 | 0.0699
Epoch 211/300, seasonal_0 Loss: 0.0486 | 0.0702
Epoch 212/300, seasonal_0 Loss: 0.0478 | 0.0687
Epoch 213/300, seasonal_0 Loss: 0.0462 | 0.0686
Epoch 214/300, seasonal_0 Loss: 0.0466 | 0.0678
Epoch 215/300, seasonal_0 Loss: 0.0467 | 0.0674
Epoch 216/300, seasonal_0 Loss: 0.0458 | 0.0665
Epoch 217/300, seasonal_0 Loss: 0.0448 | 0.0658
Epoch 218/300, seasonal_0 Loss: 0.0454 | 0.0657
Epoch 219/300, seasonal_0 Loss: 0.0443 | 0.0671
Epoch 220/300, seasonal_0 Loss: 0.0445 | 0.0692
Epoch 221/300, seasonal_0 Loss: 0.0448 | 0.0707
Epoch 222/300, seasonal_0 Loss: 0.0445 | 0.0682
Epoch 223/300, seasonal_0 Loss: 0.0439 | 0.0670
Epoch 224/300, seasonal_0 Loss: 0.0480 | 0.0710
Epoch 225/300, seasonal_0 Loss: 0.0452 | 0.0665
Epoch 226/300, seasonal_0 Loss: 0.0446 | 0.0656
Epoch 227/300, seasonal_0 Loss: 0.0440 | 0.0650
Epoch 228/300, seasonal_0 Loss: 0.0432 | 0.0662
Epoch 229/300, seasonal_0 Loss: 0.0438 | 0.0680
Epoch 230/300, seasonal_0 Loss: 0.0433 | 0.0676
Epoch 231/300, seasonal_0 Loss: 0.0422 | 0.0669
Epoch 232/300, seasonal_0 Loss: 0.0426 | 0.0661
Epoch 233/300, seasonal_0 Loss: 0.0414 | 0.0649
Epoch 234/300, seasonal_0 Loss: 0.0409 | 0.0650
Epoch 235/300, seasonal_0 Loss: 0.0412 | 0.0657
Epoch 236/300, seasonal_0 Loss: 0.0409 | 0.0677
Epoch 237/300, seasonal_0 Loss: 0.0406 | 0.0683
Epoch 238/300, seasonal_0 Loss: 0.0424 | 0.0696
Epoch 239/300, seasonal_0 Loss: 0.0433 | 0.0681
Epoch 240/300, seasonal_0 Loss: 0.0411 | 0.0660
Epoch 241/300, seasonal_0 Loss: 0.0398 | 0.0653
Epoch 242/300, seasonal_0 Loss: 0.0401 | 0.0664
Epoch 243/300, seasonal_0 Loss: 0.0404 | 0.0663
Epoch 244/300, seasonal_0 Loss: 0.0393 | 0.0671
Epoch 245/300, seasonal_0 Loss: 0.0391 | 0.0668
Epoch 246/300, seasonal_0 Loss: 0.0397 | 0.0670
Epoch 247/300, seasonal_0 Loss: 0.0387 | 0.0666
Epoch 248/300, seasonal_0 Loss: 0.0410 | 0.0684
Epoch 249/300, seasonal_0 Loss: 0.0413 | 0.0663
Epoch 250/300, seasonal_0 Loss: 0.0403 | 0.0659
Epoch 251/300, seasonal_0 Loss: 0.0399 | 0.0658
Epoch 252/300, seasonal_0 Loss: 0.0395 | 0.0660
Epoch 253/300, seasonal_0 Loss: 0.0392 | 0.0668
Epoch 254/300, seasonal_0 Loss: 0.0390 | 0.0673
Epoch 255/300, seasonal_0 Loss: 0.0392 | 0.0672
Epoch 256/300, seasonal_0 Loss: 0.0391 | 0.0667
Epoch 257/300, seasonal_0 Loss: 0.0381 | 0.0670
Epoch 258/300, seasonal_0 Loss: 0.0378 | 0.0666
Epoch 259/300, seasonal_0 Loss: 0.0377 | 0.0672
Epoch 260/300, seasonal_0 Loss: 0.0379 | 0.0668
Epoch 261/300, seasonal_0 Loss: 0.0381 | 0.0664
Epoch 262/300, seasonal_0 Loss: 0.0378 | 0.0673
Epoch 263/300, seasonal_0 Loss: 0.0378 | 0.0680
Epoch 264/300, seasonal_0 Loss: 0.0376 | 0.0684
Epoch 265/300, seasonal_0 Loss: 0.0372 | 0.0682
Epoch 266/300, seasonal_0 Loss: 0.0376 | 0.0678
Epoch 267/300, seasonal_0 Loss: 0.0376 | 0.0673
Epoch 268/300, seasonal_0 Loss: 0.0368 | 0.0678
Epoch 269/300, seasonal_0 Loss: 0.0368 | 0.0676
Epoch 270/300, seasonal_0 Loss: 0.0395 | 0.0682
Epoch 271/300, seasonal_0 Loss: 0.0399 | 0.0678
Epoch 272/300, seasonal_0 Loss: 0.0384 | 0.0677
Epoch 273/300, seasonal_0 Loss: 0.0378 | 0.0682
Epoch 274/300, seasonal_0 Loss: 0.0375 | 0.0691
Epoch 275/300, seasonal_0 Loss: 0.0372 | 0.0694
Epoch 276/300, seasonal_0 Loss: 0.0377 | 0.0684
Epoch 277/300, seasonal_0 Loss: 0.0373 | 0.0682
Epoch 278/300, seasonal_0 Loss: 0.0367 | 0.0688
Epoch 279/300, seasonal_0 Loss: 0.0363 | 0.0676
Epoch 280/300, seasonal_0 Loss: 0.0361 | 0.0671
Epoch 281/300, seasonal_0 Loss: 0.0360 | 0.0669
Epoch 282/300, seasonal_0 Loss: 0.0359 | 0.0675
Epoch 283/300, seasonal_0 Loss: 0.0357 | 0.0694
Epoch 284/300, seasonal_0 Loss: 0.0358 | 0.0697
Epoch 285/300, seasonal_0 Loss: 0.0356 | 0.0692
Epoch 286/300, seasonal_0 Loss: 0.0355 | 0.0694
Epoch 287/300, seasonal_0 Loss: 0.0359 | 0.0678
Epoch 288/300, seasonal_0 Loss: 0.0370 | 0.0675
Epoch 289/300, seasonal_0 Loss: 0.0356 | 0.0672
Epoch 290/300, seasonal_0 Loss: 0.0405 | 0.0681
Epoch 291/300, seasonal_0 Loss: 0.0396 | 0.0681
Epoch 292/300, seasonal_0 Loss: 0.0382 | 0.0695
Epoch 293/300, seasonal_0 Loss: 0.0377 | 0.0695
Epoch 294/300, seasonal_0 Loss: 0.0375 | 0.0692
Epoch 295/300, seasonal_0 Loss: 0.0365 | 0.0682
Epoch 296/300, seasonal_0 Loss: 0.0361 | 0.0680
Epoch 297/300, seasonal_0 Loss: 0.0360 | 0.0678
Epoch 298/300, seasonal_0 Loss: 0.0359 | 0.0676
Epoch 299/300, seasonal_0 Loss: 0.0356 | 0.0676
Epoch 300/300, seasonal_0 Loss: 0.0352 | 0.0679
Training seasonal_1 component with params: {'observation_period_num': 16, 'train_rates': 0.9893353972234566, 'learning_rate': 4.562446513834744e-05, 'batch_size': 24, 'step_size': 15, 'gamma': 0.98302074298137}
Epoch 1/300, seasonal_1 Loss: 0.2948 | 0.1771
Epoch 2/300, seasonal_1 Loss: 0.1462 | 0.1240
Epoch 3/300, seasonal_1 Loss: 0.1208 | 0.0938
Epoch 4/300, seasonal_1 Loss: 0.1096 | 0.0768
Epoch 5/300, seasonal_1 Loss: 0.1040 | 0.0680
Epoch 6/300, seasonal_1 Loss: 0.1000 | 0.0634
Epoch 7/300, seasonal_1 Loss: 0.0966 | 0.0608
Epoch 8/300, seasonal_1 Loss: 0.0938 | 0.0592
Epoch 9/300, seasonal_1 Loss: 0.0913 | 0.0580
Epoch 10/300, seasonal_1 Loss: 0.0892 | 0.0568
Epoch 11/300, seasonal_1 Loss: 0.0874 | 0.0555
Epoch 12/300, seasonal_1 Loss: 0.0858 | 0.0541
Epoch 13/300, seasonal_1 Loss: 0.0844 | 0.0529
Epoch 14/300, seasonal_1 Loss: 0.0834 | 0.0520
Epoch 15/300, seasonal_1 Loss: 0.0829 | 0.0510
Epoch 16/300, seasonal_1 Loss: 0.0829 | 0.0497
Epoch 17/300, seasonal_1 Loss: 0.0833 | 0.0489
Epoch 18/300, seasonal_1 Loss: 0.0831 | 0.0492
Epoch 19/300, seasonal_1 Loss: 0.0819 | 0.0488
Epoch 20/300, seasonal_1 Loss: 0.0798 | 0.0472
Epoch 21/300, seasonal_1 Loss: 0.0778 | 0.0453
Epoch 22/300, seasonal_1 Loss: 0.0762 | 0.0435
Epoch 23/300, seasonal_1 Loss: 0.0748 | 0.0420
Epoch 24/300, seasonal_1 Loss: 0.0737 | 0.0409
Epoch 25/300, seasonal_1 Loss: 0.0727 | 0.0401
Epoch 26/300, seasonal_1 Loss: 0.0717 | 0.0396
Epoch 27/300, seasonal_1 Loss: 0.0708 | 0.0393
Epoch 28/300, seasonal_1 Loss: 0.0699 | 0.0393
Epoch 29/300, seasonal_1 Loss: 0.0690 | 0.0394
Epoch 30/300, seasonal_1 Loss: 0.0681 | 0.0395
Epoch 31/300, seasonal_1 Loss: 0.0672 | 0.0395
Epoch 32/300, seasonal_1 Loss: 0.0664 | 0.0397
Epoch 33/300, seasonal_1 Loss: 0.0656 | 0.0398
Epoch 34/300, seasonal_1 Loss: 0.0649 | 0.0396
Epoch 35/300, seasonal_1 Loss: 0.0643 | 0.0392
Epoch 36/300, seasonal_1 Loss: 0.0636 | 0.0385
Epoch 37/300, seasonal_1 Loss: 0.0630 | 0.0379
Epoch 38/300, seasonal_1 Loss: 0.0626 | 0.0373
Epoch 39/300, seasonal_1 Loss: 0.0621 | 0.0367
Epoch 40/300, seasonal_1 Loss: 0.0617 | 0.0362
Epoch 41/300, seasonal_1 Loss: 0.0614 | 0.0358
Epoch 42/300, seasonal_1 Loss: 0.0610 | 0.0354
Epoch 43/300, seasonal_1 Loss: 0.0607 | 0.0349
Epoch 44/300, seasonal_1 Loss: 0.0604 | 0.0345
Epoch 45/300, seasonal_1 Loss: 0.0602 | 0.0342
Epoch 46/300, seasonal_1 Loss: 0.0599 | 0.0338
Epoch 47/300, seasonal_1 Loss: 0.0596 | 0.0335
Epoch 48/300, seasonal_1 Loss: 0.0594 | 0.0331
Epoch 49/300, seasonal_1 Loss: 0.0592 | 0.0328
Epoch 50/300, seasonal_1 Loss: 0.0590 | 0.0326
Epoch 51/300, seasonal_1 Loss: 0.0588 | 0.0323
Epoch 52/300, seasonal_1 Loss: 0.0586 | 0.0321
Epoch 53/300, seasonal_1 Loss: 0.0584 | 0.0319
Epoch 54/300, seasonal_1 Loss: 0.0582 | 0.0315
Epoch 55/300, seasonal_1 Loss: 0.0580 | 0.0312
Epoch 56/300, seasonal_1 Loss: 0.0578 | 0.0310
Epoch 57/300, seasonal_1 Loss: 0.0576 | 0.0307
Epoch 58/300, seasonal_1 Loss: 0.0574 | 0.0304
Epoch 59/300, seasonal_1 Loss: 0.0572 | 0.0301
Epoch 60/300, seasonal_1 Loss: 0.0570 | 0.0298
Epoch 61/300, seasonal_1 Loss: 0.0569 | 0.0293
Epoch 62/300, seasonal_1 Loss: 0.0567 | 0.0289
Epoch 63/300, seasonal_1 Loss: 0.0565 | 0.0285
Epoch 64/300, seasonal_1 Loss: 0.0563 | 0.0281
Epoch 65/300, seasonal_1 Loss: 0.0561 | 0.0277
Epoch 66/300, seasonal_1 Loss: 0.0559 | 0.0273
Epoch 67/300, seasonal_1 Loss: 0.0558 | 0.0270
Epoch 68/300, seasonal_1 Loss: 0.0556 | 0.0269
Epoch 69/300, seasonal_1 Loss: 0.0553 | 0.0271
Epoch 70/300, seasonal_1 Loss: 0.0551 | 0.0273
Epoch 71/300, seasonal_1 Loss: 0.0548 | 0.0274
Epoch 72/300, seasonal_1 Loss: 0.0545 | 0.0274
Epoch 73/300, seasonal_1 Loss: 0.0542 | 0.0273
Epoch 74/300, seasonal_1 Loss: 0.0538 | 0.0271
Epoch 75/300, seasonal_1 Loss: 0.0535 | 0.0268
Epoch 76/300, seasonal_1 Loss: 0.0533 | 0.0265
Epoch 77/300, seasonal_1 Loss: 0.0531 | 0.0260
Epoch 78/300, seasonal_1 Loss: 0.0529 | 0.0254
Epoch 79/300, seasonal_1 Loss: 0.0528 | 0.0248
Epoch 80/300, seasonal_1 Loss: 0.0527 | 0.0242
Epoch 81/300, seasonal_1 Loss: 0.0525 | 0.0237
Epoch 82/300, seasonal_1 Loss: 0.0523 | 0.0233
Epoch 83/300, seasonal_1 Loss: 0.0521 | 0.0230
Epoch 84/300, seasonal_1 Loss: 0.0519 | 0.0226
Epoch 85/300, seasonal_1 Loss: 0.0517 | 0.0224
Epoch 86/300, seasonal_1 Loss: 0.0515 | 0.0223
Epoch 87/300, seasonal_1 Loss: 0.0513 | 0.0222
Epoch 88/300, seasonal_1 Loss: 0.0511 | 0.0222
Epoch 89/300, seasonal_1 Loss: 0.0509 | 0.0222
Epoch 90/300, seasonal_1 Loss: 0.0507 | 0.0222
Epoch 91/300, seasonal_1 Loss: 0.0505 | 0.0220
Epoch 92/300, seasonal_1 Loss: 0.0504 | 0.0220
Epoch 93/300, seasonal_1 Loss: 0.0502 | 0.0220
Epoch 94/300, seasonal_1 Loss: 0.0501 | 0.0219
Epoch 95/300, seasonal_1 Loss: 0.0500 | 0.0218
Epoch 96/300, seasonal_1 Loss: 0.0499 | 0.0217
Epoch 97/300, seasonal_1 Loss: 0.0498 | 0.0215
Epoch 98/300, seasonal_1 Loss: 0.0497 | 0.0213
Epoch 99/300, seasonal_1 Loss: 0.0497 | 0.0207
Epoch 100/300, seasonal_1 Loss: 0.0496 | 0.0204
Epoch 101/300, seasonal_1 Loss: 0.0495 | 0.0200
Epoch 102/300, seasonal_1 Loss: 0.0495 | 0.0197
Epoch 103/300, seasonal_1 Loss: 0.0494 | 0.0195
Epoch 104/300, seasonal_1 Loss: 0.0494 | 0.0193
Epoch 105/300, seasonal_1 Loss: 0.0493 | 0.0192
Epoch 106/300, seasonal_1 Loss: 0.0492 | 0.0190
Epoch 107/300, seasonal_1 Loss: 0.0492 | 0.0190
Epoch 108/300, seasonal_1 Loss: 0.0491 | 0.0190
Epoch 109/300, seasonal_1 Loss: 0.0490 | 0.0191
Epoch 110/300, seasonal_1 Loss: 0.0489 | 0.0191
Epoch 111/300, seasonal_1 Loss: 0.0488 | 0.0190
Epoch 112/300, seasonal_1 Loss: 0.0487 | 0.0190
Epoch 113/300, seasonal_1 Loss: 0.0486 | 0.0189
Epoch 114/300, seasonal_1 Loss: 0.0485 | 0.0188
Epoch 115/300, seasonal_1 Loss: 0.0484 | 0.0188
Epoch 116/300, seasonal_1 Loss: 0.0484 | 0.0187
Epoch 117/300, seasonal_1 Loss: 0.0483 | 0.0186
Epoch 118/300, seasonal_1 Loss: 0.0481 | 0.0186
Epoch 119/300, seasonal_1 Loss: 0.0481 | 0.0186
Epoch 120/300, seasonal_1 Loss: 0.0480 | 0.0187
Epoch 121/300, seasonal_1 Loss: 0.0479 | 0.0190
Epoch 122/300, seasonal_1 Loss: 0.0479 | 0.0191
Epoch 123/300, seasonal_1 Loss: 0.0479 | 0.0193
Epoch 124/300, seasonal_1 Loss: 0.0478 | 0.0195
Epoch 125/300, seasonal_1 Loss: 0.0478 | 0.0196
Epoch 126/300, seasonal_1 Loss: 0.0477 | 0.0197
Epoch 127/300, seasonal_1 Loss: 0.0476 | 0.0197
Epoch 128/300, seasonal_1 Loss: 0.0475 | 0.0194
Epoch 129/300, seasonal_1 Loss: 0.0473 | 0.0191
Epoch 130/300, seasonal_1 Loss: 0.0471 | 0.0187
Epoch 131/300, seasonal_1 Loss: 0.0470 | 0.0183
Epoch 132/300, seasonal_1 Loss: 0.0468 | 0.0180
Epoch 133/300, seasonal_1 Loss: 0.0467 | 0.0178
Epoch 134/300, seasonal_1 Loss: 0.0465 | 0.0176
Epoch 135/300, seasonal_1 Loss: 0.0464 | 0.0175
Epoch 136/300, seasonal_1 Loss: 0.0463 | 0.0175
Epoch 137/300, seasonal_1 Loss: 0.0461 | 0.0175
Epoch 138/300, seasonal_1 Loss: 0.0460 | 0.0176
Epoch 139/300, seasonal_1 Loss: 0.0459 | 0.0177
Epoch 140/300, seasonal_1 Loss: 0.0458 | 0.0178
Epoch 141/300, seasonal_1 Loss: 0.0457 | 0.0179
Epoch 142/300, seasonal_1 Loss: 0.0457 | 0.0179
Epoch 143/300, seasonal_1 Loss: 0.0456 | 0.0180
Epoch 144/300, seasonal_1 Loss: 0.0455 | 0.0178
Epoch 145/300, seasonal_1 Loss: 0.0455 | 0.0178
Epoch 146/300, seasonal_1 Loss: 0.0454 | 0.0176
Epoch 147/300, seasonal_1 Loss: 0.0454 | 0.0174
Epoch 148/300, seasonal_1 Loss: 0.0453 | 0.0171
Epoch 149/300, seasonal_1 Loss: 0.0453 | 0.0170
Epoch 150/300, seasonal_1 Loss: 0.0453 | 0.0173
Epoch 151/300, seasonal_1 Loss: 0.0452 | 0.0188
Epoch 152/300, seasonal_1 Loss: 0.0451 | 0.0205
Epoch 153/300, seasonal_1 Loss: 0.0450 | 0.0211
Epoch 154/300, seasonal_1 Loss: 0.0450 | 0.0198
Epoch 155/300, seasonal_1 Loss: 0.0451 | 0.0183
Epoch 156/300, seasonal_1 Loss: 0.0450 | 0.0175
Epoch 157/300, seasonal_1 Loss: 0.0447 | 0.0171
Epoch 158/300, seasonal_1 Loss: 0.0444 | 0.0169
Epoch 159/300, seasonal_1 Loss: 0.0441 | 0.0168
Epoch 160/300, seasonal_1 Loss: 0.0438 | 0.0167
Epoch 161/300, seasonal_1 Loss: 0.0436 | 0.0167
Epoch 162/300, seasonal_1 Loss: 0.0434 | 0.0167
Epoch 163/300, seasonal_1 Loss: 0.0432 | 0.0167
Epoch 164/300, seasonal_1 Loss: 0.0431 | 0.0168
Epoch 165/300, seasonal_1 Loss: 0.0429 | 0.0168
Epoch 166/300, seasonal_1 Loss: 0.0428 | 0.0167
Epoch 167/300, seasonal_1 Loss: 0.0426 | 0.0168
Epoch 168/300, seasonal_1 Loss: 0.0425 | 0.0168
Epoch 169/300, seasonal_1 Loss: 0.0424 | 0.0168
Epoch 170/300, seasonal_1 Loss: 0.0422 | 0.0168
Epoch 171/300, seasonal_1 Loss: 0.0421 | 0.0168
Epoch 172/300, seasonal_1 Loss: 0.0420 | 0.0167
Epoch 173/300, seasonal_1 Loss: 0.0419 | 0.0167
Epoch 174/300, seasonal_1 Loss: 0.0417 | 0.0166
Epoch 175/300, seasonal_1 Loss: 0.0416 | 0.0166
Epoch 176/300, seasonal_1 Loss: 0.0415 | 0.0174
Epoch 177/300, seasonal_1 Loss: 0.0413 | 0.0174
Epoch 178/300, seasonal_1 Loss: 0.0408 | 0.0166
Epoch 179/300, seasonal_1 Loss: 0.0398 | 0.0155
Epoch 180/300, seasonal_1 Loss: 0.0395 | 0.0158
Epoch 181/300, seasonal_1 Loss: 0.0388 | 0.0158
Epoch 182/300, seasonal_1 Loss: 0.0389 | 0.0158
Epoch 183/300, seasonal_1 Loss: 0.0387 | 0.0151
Epoch 184/300, seasonal_1 Loss: 0.0376 | 0.0155
Epoch 185/300, seasonal_1 Loss: 0.0370 | 0.0156
Epoch 186/300, seasonal_1 Loss: 0.0368 | 0.0154
Epoch 187/300, seasonal_1 Loss: 0.0368 | 0.0158
Epoch 188/300, seasonal_1 Loss: 0.0362 | 0.0161
Epoch 189/300, seasonal_1 Loss: 0.0375 | 0.0168
Epoch 190/300, seasonal_1 Loss: 0.0378 | 0.0166
Epoch 191/300, seasonal_1 Loss: 0.0369 | 0.0169
Epoch 192/300, seasonal_1 Loss: 0.0362 | 0.0174
Epoch 193/300, seasonal_1 Loss: 0.0357 | 0.0178
Epoch 194/300, seasonal_1 Loss: 0.0358 | 0.0178
Epoch 195/300, seasonal_1 Loss: 0.0363 | 0.0175
Epoch 196/300, seasonal_1 Loss: 0.0405 | 0.0180
Epoch 197/300, seasonal_1 Loss: 0.0361 | 0.0170
Epoch 198/300, seasonal_1 Loss: 0.0355 | 0.0161
Epoch 199/300, seasonal_1 Loss: 0.0355 | 0.0164
Epoch 200/300, seasonal_1 Loss: 0.0350 | 0.0161
Epoch 201/300, seasonal_1 Loss: 0.0388 | 0.0162
Epoch 202/300, seasonal_1 Loss: 0.0365 | 0.0158
Epoch 203/300, seasonal_1 Loss: 0.0363 | 0.0157
Epoch 204/300, seasonal_1 Loss: 0.0352 | 0.0157
Epoch 205/300, seasonal_1 Loss: 0.0347 | 0.0156
Epoch 206/300, seasonal_1 Loss: 0.0343 | 0.0157
Epoch 207/300, seasonal_1 Loss: 0.0343 | 0.0161
Epoch 208/300, seasonal_1 Loss: 0.0360 | 0.0158
Epoch 209/300, seasonal_1 Loss: 0.0354 | 0.0159
Epoch 210/300, seasonal_1 Loss: 0.0347 | 0.0162
Epoch 211/300, seasonal_1 Loss: 0.0343 | 0.0159
Epoch 212/300, seasonal_1 Loss: 0.0339 | 0.0160
Epoch 213/300, seasonal_1 Loss: 0.0343 | 0.0158
Epoch 214/300, seasonal_1 Loss: 0.0350 | 0.0158
Epoch 215/300, seasonal_1 Loss: 0.0341 | 0.0161
Epoch 216/300, seasonal_1 Loss: 0.0330 | 0.0164
Epoch 217/300, seasonal_1 Loss: 0.0329 | 0.0170
Epoch 218/300, seasonal_1 Loss: 0.0329 | 0.0163
Epoch 219/300, seasonal_1 Loss: 0.0348 | 0.0158
Epoch 220/300, seasonal_1 Loss: 0.0337 | 0.0160
Epoch 221/300, seasonal_1 Loss: 0.0335 | 0.0174
Epoch 222/300, seasonal_1 Loss: 0.0332 | 0.0173
Epoch 223/300, seasonal_1 Loss: 0.0323 | 0.0159
Epoch 224/300, seasonal_1 Loss: 0.0331 | 0.0170
Epoch 225/300, seasonal_1 Loss: 0.0328 | 0.0162
Epoch 226/300, seasonal_1 Loss: 0.0322 | 0.0164
Epoch 227/300, seasonal_1 Loss: 0.0313 | 0.0172
Epoch 228/300, seasonal_1 Loss: 0.0311 | 0.0165
Epoch 229/300, seasonal_1 Loss: 0.0332 | 0.0165
Epoch 230/300, seasonal_1 Loss: 0.0323 | 0.0173
Epoch 231/300, seasonal_1 Loss: 0.0326 | 0.0180
Epoch 232/300, seasonal_1 Loss: 0.0316 | 0.0170
Epoch 233/300, seasonal_1 Loss: 0.0340 | 0.0169
Epoch 234/300, seasonal_1 Loss: 0.0336 | 0.0169
Epoch 235/300, seasonal_1 Loss: 0.0318 | 0.0170
Epoch 236/300, seasonal_1 Loss: 0.0306 | 0.0173
Epoch 237/300, seasonal_1 Loss: 0.0321 | 0.0171
Epoch 238/300, seasonal_1 Loss: 0.0321 | 0.0190
Epoch 239/300, seasonal_1 Loss: 0.0323 | 0.0178
Epoch 240/300, seasonal_1 Loss: 0.0315 | 0.0180
Epoch 241/300, seasonal_1 Loss: 0.0304 | 0.0183
Epoch 242/300, seasonal_1 Loss: 0.0299 | 0.0188
Epoch 243/300, seasonal_1 Loss: 0.0313 | 0.0198
Epoch 244/300, seasonal_1 Loss: 0.0337 | 0.0181
Epoch 245/300, seasonal_1 Loss: 0.0308 | 0.0179
Epoch 246/300, seasonal_1 Loss: 0.0348 | 0.0187
Epoch 247/300, seasonal_1 Loss: 0.0332 | 0.0176
Epoch 248/300, seasonal_1 Loss: 0.0318 | 0.0175
Epoch 249/300, seasonal_1 Loss: 0.0300 | 0.0175
Epoch 250/300, seasonal_1 Loss: 0.0307 | 0.0185
Epoch 251/300, seasonal_1 Loss: 0.0322 | 0.0178
Epoch 252/300, seasonal_1 Loss: 0.0305 | 0.0174
Epoch 253/300, seasonal_1 Loss: 0.0298 | 0.0175
Epoch 254/300, seasonal_1 Loss: 0.0293 | 0.0177
Epoch 255/300, seasonal_1 Loss: 0.0292 | 0.0178
Epoch 256/300, seasonal_1 Loss: 0.0288 | 0.0181
Epoch 257/300, seasonal_1 Loss: 0.0286 | 0.0185
Epoch 258/300, seasonal_1 Loss: 0.0284 | 0.0182
Epoch 259/300, seasonal_1 Loss: 0.0283 | 0.0218
Epoch 260/300, seasonal_1 Loss: 0.0385 | 0.0189
Epoch 261/300, seasonal_1 Loss: 0.0318 | 0.0175
Epoch 262/300, seasonal_1 Loss: 0.0299 | 0.0181
Epoch 263/300, seasonal_1 Loss: 0.0290 | 0.0179
Epoch 264/300, seasonal_1 Loss: 0.0307 | 0.0196
Epoch 265/300, seasonal_1 Loss: 0.0315 | 0.0183
Epoch 266/300, seasonal_1 Loss: 0.0307 | 0.0182
Epoch 267/300, seasonal_1 Loss: 0.0292 | 0.0188
Epoch 268/300, seasonal_1 Loss: 0.0295 | 0.0183
Epoch 269/300, seasonal_1 Loss: 0.0280 | 0.0189
Epoch 270/300, seasonal_1 Loss: 0.0285 | 0.0188
Epoch 271/300, seasonal_1 Loss: 0.0279 | 0.0195
Epoch 272/300, seasonal_1 Loss: 0.0323 | 0.0205
Epoch 273/300, seasonal_1 Loss: 0.0310 | 0.0194
Epoch 274/300, seasonal_1 Loss: 0.0302 | 0.0188
Epoch 275/300, seasonal_1 Loss: 0.0288 | 0.0193
Epoch 276/300, seasonal_1 Loss: 0.0290 | 0.0188
Epoch 277/300, seasonal_1 Loss: 0.0286 | 0.0199
Epoch 278/300, seasonal_1 Loss: 0.0309 | 0.0201
Epoch 279/300, seasonal_1 Loss: 0.0305 | 0.0191
Epoch 280/300, seasonal_1 Loss: 0.0283 | 0.0188
Epoch 281/300, seasonal_1 Loss: 0.0274 | 0.0190
Epoch 282/300, seasonal_1 Loss: 0.0272 | 0.0190
Epoch 283/300, seasonal_1 Loss: 0.0272 | 0.0193
Epoch 284/300, seasonal_1 Loss: 0.0283 | 0.0197
Epoch 285/300, seasonal_1 Loss: 0.0269 | 0.0194
Epoch 286/300, seasonal_1 Loss: 0.0283 | 0.0208
Epoch 287/300, seasonal_1 Loss: 0.0301 | 0.0219
Epoch 288/300, seasonal_1 Loss: 0.0303 | 0.0205
Epoch 289/300, seasonal_1 Loss: 0.0294 | 0.0216
Epoch 290/300, seasonal_1 Loss: 0.0288 | 0.0200
Epoch 291/300, seasonal_1 Loss: 0.0274 | 0.0199
Epoch 292/300, seasonal_1 Loss: 0.0270 | 0.0197
Epoch 293/300, seasonal_1 Loss: 0.0268 | 0.0198
Epoch 294/300, seasonal_1 Loss: 0.0267 | 0.0196
Epoch 295/300, seasonal_1 Loss: 0.0275 | 0.0201
Epoch 296/300, seasonal_1 Loss: 0.0271 | 0.0208
Epoch 297/300, seasonal_1 Loss: 0.0300 | 0.0204
Epoch 298/300, seasonal_1 Loss: 0.0296 | 0.0201
Epoch 299/300, seasonal_1 Loss: 0.0267 | 0.0196
Epoch 300/300, seasonal_1 Loss: 0.0277 | 0.0217
Training seasonal_2 component with params: {'observation_period_num': 12, 'train_rates': 0.8625308577317593, 'learning_rate': 0.0009560811759465162, 'batch_size': 184, 'step_size': 11, 'gamma': 0.8196661258033011}
Epoch 1/300, seasonal_2 Loss: 0.5937 | 0.2826
Epoch 2/300, seasonal_2 Loss: 0.1929 | 0.1669
Epoch 3/300, seasonal_2 Loss: 0.1795 | 0.1601
Epoch 4/300, seasonal_2 Loss: 0.1700 | 0.1923
Epoch 5/300, seasonal_2 Loss: 0.1904 | 0.4037
Epoch 6/300, seasonal_2 Loss: 0.1819 | 0.1757
Epoch 7/300, seasonal_2 Loss: 0.1612 | 0.1104
Epoch 8/300, seasonal_2 Loss: 0.1543 | 0.0985
Epoch 9/300, seasonal_2 Loss: 0.1329 | 0.1302
Epoch 10/300, seasonal_2 Loss: 0.1323 | 0.0820
Epoch 11/300, seasonal_2 Loss: 0.1301 | 0.0914
Epoch 12/300, seasonal_2 Loss: 0.1215 | 0.0657
Epoch 13/300, seasonal_2 Loss: 0.1040 | 0.0810
Epoch 14/300, seasonal_2 Loss: 0.1163 | 0.0886
Epoch 15/300, seasonal_2 Loss: 0.1090 | 0.0572
Epoch 16/300, seasonal_2 Loss: 0.0975 | 0.0674
Epoch 17/300, seasonal_2 Loss: 0.1121 | 0.0658
Epoch 18/300, seasonal_2 Loss: 0.0980 | 0.0646
Epoch 19/300, seasonal_2 Loss: 0.0964 | 0.0775
Epoch 20/300, seasonal_2 Loss: 0.0941 | 0.0574
Epoch 21/300, seasonal_2 Loss: 0.0871 | 0.0508
Epoch 22/300, seasonal_2 Loss: 0.0861 | 0.0521
Epoch 23/300, seasonal_2 Loss: 0.0836 | 0.0584
Epoch 24/300, seasonal_2 Loss: 0.0837 | 0.0502
Epoch 25/300, seasonal_2 Loss: 0.0822 | 0.0454
Epoch 26/300, seasonal_2 Loss: 0.0804 | 0.0484
Epoch 27/300, seasonal_2 Loss: 0.0795 | 0.0520
Epoch 28/300, seasonal_2 Loss: 0.0787 | 0.0476
Epoch 29/300, seasonal_2 Loss: 0.0777 | 0.0408
Epoch 30/300, seasonal_2 Loss: 0.0769 | 0.0401
Epoch 31/300, seasonal_2 Loss: 0.0765 | 0.0462
Epoch 32/300, seasonal_2 Loss: 0.0761 | 0.0472
Epoch 33/300, seasonal_2 Loss: 0.0759 | 0.0387
Epoch 34/300, seasonal_2 Loss: 0.0750 | 0.0377
Epoch 35/300, seasonal_2 Loss: 0.0750 | 0.0439
Epoch 36/300, seasonal_2 Loss: 0.0744 | 0.0443
Epoch 37/300, seasonal_2 Loss: 0.0743 | 0.0364
Epoch 38/300, seasonal_2 Loss: 0.0736 | 0.0362
Epoch 39/300, seasonal_2 Loss: 0.0736 | 0.0422
Epoch 40/300, seasonal_2 Loss: 0.0728 | 0.0421
Epoch 41/300, seasonal_2 Loss: 0.0730 | 0.0358
Epoch 42/300, seasonal_2 Loss: 0.0723 | 0.0365
Epoch 43/300, seasonal_2 Loss: 0.0726 | 0.0481
Epoch 44/300, seasonal_2 Loss: 0.0730 | 0.0353
Epoch 45/300, seasonal_2 Loss: 0.0724 | 0.0352
Epoch 46/300, seasonal_2 Loss: 0.0729 | 0.0477
Epoch 47/300, seasonal_2 Loss: 0.0726 | 0.0348
Epoch 48/300, seasonal_2 Loss: 0.0718 | 0.0352
Epoch 49/300, seasonal_2 Loss: 0.0721 | 0.0437
Epoch 50/300, seasonal_2 Loss: 0.0718 | 0.0344
Epoch 51/300, seasonal_2 Loss: 0.0715 | 0.0357
Epoch 52/300, seasonal_2 Loss: 0.0725 | 0.0407
Epoch 53/300, seasonal_2 Loss: 0.0717 | 0.0345
Epoch 54/300, seasonal_2 Loss: 0.0707 | 0.0403
Epoch 55/300, seasonal_2 Loss: 0.0704 | 0.0351
Epoch 56/300, seasonal_2 Loss: 0.0698 | 0.0345
Epoch 57/300, seasonal_2 Loss: 0.0701 | 0.0385
Epoch 58/300, seasonal_2 Loss: 0.0704 | 0.0341
Epoch 59/300, seasonal_2 Loss: 0.0709 | 0.0368
Epoch 60/300, seasonal_2 Loss: 0.0703 | 0.0353
Epoch 61/300, seasonal_2 Loss: 0.0702 | 0.0352
Epoch 62/300, seasonal_2 Loss: 0.0700 | 0.0347
Epoch 63/300, seasonal_2 Loss: 0.0682 | 0.0347
Epoch 64/300, seasonal_2 Loss: 0.0676 | 0.0342
Epoch 65/300, seasonal_2 Loss: 0.0674 | 0.0338
Epoch 66/300, seasonal_2 Loss: 0.0671 | 0.0337
Epoch 67/300, seasonal_2 Loss: 0.0669 | 0.0333
Epoch 68/300, seasonal_2 Loss: 0.0668 | 0.0333
Epoch 69/300, seasonal_2 Loss: 0.0666 | 0.0331
Epoch 70/300, seasonal_2 Loss: 0.0665 | 0.0331
Epoch 71/300, seasonal_2 Loss: 0.0663 | 0.0329
Epoch 72/300, seasonal_2 Loss: 0.0662 | 0.0330
Epoch 73/300, seasonal_2 Loss: 0.0660 | 0.0327
Epoch 74/300, seasonal_2 Loss: 0.0659 | 0.0327
Epoch 75/300, seasonal_2 Loss: 0.0658 | 0.0325
Epoch 76/300, seasonal_2 Loss: 0.0657 | 0.0325
Epoch 77/300, seasonal_2 Loss: 0.0656 | 0.0323
Epoch 78/300, seasonal_2 Loss: 0.0655 | 0.0323
Epoch 79/300, seasonal_2 Loss: 0.0654 | 0.0322
Epoch 80/300, seasonal_2 Loss: 0.0653 | 0.0321
Epoch 81/300, seasonal_2 Loss: 0.0652 | 0.0320
Epoch 82/300, seasonal_2 Loss: 0.0652 | 0.0320
Epoch 83/300, seasonal_2 Loss: 0.0651 | 0.0319
Epoch 84/300, seasonal_2 Loss: 0.0650 | 0.0317
Epoch 85/300, seasonal_2 Loss: 0.0649 | 0.0318
Epoch 86/300, seasonal_2 Loss: 0.0648 | 0.0315
Epoch 87/300, seasonal_2 Loss: 0.0647 | 0.0317
Epoch 88/300, seasonal_2 Loss: 0.0647 | 0.0314
Epoch 89/300, seasonal_2 Loss: 0.0646 | 0.0317
Epoch 90/300, seasonal_2 Loss: 0.0645 | 0.0311
Epoch 91/300, seasonal_2 Loss: 0.0645 | 0.0316
Epoch 92/300, seasonal_2 Loss: 0.0644 | 0.0310
Epoch 93/300, seasonal_2 Loss: 0.0643 | 0.0314
Epoch 94/300, seasonal_2 Loss: 0.0643 | 0.0309
Epoch 95/300, seasonal_2 Loss: 0.0642 | 0.0314
Epoch 96/300, seasonal_2 Loss: 0.0641 | 0.0308
Epoch 97/300, seasonal_2 Loss: 0.0641 | 0.0312
Epoch 98/300, seasonal_2 Loss: 0.0640 | 0.0307
Epoch 99/300, seasonal_2 Loss: 0.0640 | 0.0310
Epoch 100/300, seasonal_2 Loss: 0.0639 | 0.0306
Epoch 101/300, seasonal_2 Loss: 0.0639 | 0.0308
Epoch 102/300, seasonal_2 Loss: 0.0638 | 0.0306
Epoch 103/300, seasonal_2 Loss: 0.0638 | 0.0307
Epoch 104/300, seasonal_2 Loss: 0.0637 | 0.0306
Epoch 105/300, seasonal_2 Loss: 0.0637 | 0.0306
Epoch 106/300, seasonal_2 Loss: 0.0636 | 0.0305
Epoch 107/300, seasonal_2 Loss: 0.0636 | 0.0305
Epoch 108/300, seasonal_2 Loss: 0.0636 | 0.0304
Epoch 109/300, seasonal_2 Loss: 0.0635 | 0.0304
Epoch 110/300, seasonal_2 Loss: 0.0635 | 0.0303
Epoch 111/300, seasonal_2 Loss: 0.0634 | 0.0303
Epoch 112/300, seasonal_2 Loss: 0.0634 | 0.0303
Epoch 113/300, seasonal_2 Loss: 0.0634 | 0.0302
Epoch 114/300, seasonal_2 Loss: 0.0634 | 0.0302
Epoch 115/300, seasonal_2 Loss: 0.0633 | 0.0302
Epoch 116/300, seasonal_2 Loss: 0.0633 | 0.0301
Epoch 117/300, seasonal_2 Loss: 0.0633 | 0.0301
Epoch 118/300, seasonal_2 Loss: 0.0632 | 0.0301
Epoch 119/300, seasonal_2 Loss: 0.0632 | 0.0300
Epoch 120/300, seasonal_2 Loss: 0.0632 | 0.0300
Epoch 121/300, seasonal_2 Loss: 0.0632 | 0.0300
Epoch 122/300, seasonal_2 Loss: 0.0631 | 0.0300
Epoch 123/300, seasonal_2 Loss: 0.0631 | 0.0299
Epoch 124/300, seasonal_2 Loss: 0.0631 | 0.0299
Epoch 125/300, seasonal_2 Loss: 0.0631 | 0.0299
Epoch 126/300, seasonal_2 Loss: 0.0630 | 0.0299
Epoch 127/300, seasonal_2 Loss: 0.0630 | 0.0299
Epoch 128/300, seasonal_2 Loss: 0.0630 | 0.0298
Epoch 129/300, seasonal_2 Loss: 0.0630 | 0.0298
Epoch 130/300, seasonal_2 Loss: 0.0630 | 0.0298
Epoch 131/300, seasonal_2 Loss: 0.0630 | 0.0298
Epoch 132/300, seasonal_2 Loss: 0.0629 | 0.0298
Epoch 133/300, seasonal_2 Loss: 0.0629 | 0.0297
Epoch 134/300, seasonal_2 Loss: 0.0629 | 0.0297
Epoch 135/300, seasonal_2 Loss: 0.0629 | 0.0297
Epoch 136/300, seasonal_2 Loss: 0.0629 | 0.0297
Epoch 137/300, seasonal_2 Loss: 0.0629 | 0.0297
Epoch 138/300, seasonal_2 Loss: 0.0628 | 0.0297
Epoch 139/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 140/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 141/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 142/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 143/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 144/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 145/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 146/300, seasonal_2 Loss: 0.0627 | 0.0296
Epoch 147/300, seasonal_2 Loss: 0.0627 | 0.0296
Epoch 148/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 149/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 150/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 151/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 152/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 153/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 154/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 155/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 156/300, seasonal_2 Loss: 0.0627 | 0.0295
Epoch 157/300, seasonal_2 Loss: 0.0626 | 0.0295
Epoch 158/300, seasonal_2 Loss: 0.0626 | 0.0295
Epoch 159/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 160/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 161/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 162/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 163/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 164/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 165/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 166/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 167/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 168/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 169/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 170/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 171/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 172/300, seasonal_2 Loss: 0.0626 | 0.0294
Epoch 173/300, seasonal_2 Loss: 0.0625 | 0.0294
Epoch 174/300, seasonal_2 Loss: 0.0625 | 0.0294
Epoch 175/300, seasonal_2 Loss: 0.0625 | 0.0294
Epoch 176/300, seasonal_2 Loss: 0.0625 | 0.0294
Epoch 177/300, seasonal_2 Loss: 0.0625 | 0.0294
Epoch 178/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 179/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 180/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 181/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 182/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 183/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 184/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 185/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 186/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 187/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 188/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 189/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 190/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 191/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 192/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 193/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 194/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 195/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 196/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 197/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 198/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 199/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 200/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 201/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 202/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 203/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 204/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 205/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 206/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 207/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 208/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 209/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 210/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 211/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 212/300, seasonal_2 Loss: 0.0625 | 0.0293
Epoch 213/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 214/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 215/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 216/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 217/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 218/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 219/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 220/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 221/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 222/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 223/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 224/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 225/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 226/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 227/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 228/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 229/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 230/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 231/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 232/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 233/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 234/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 235/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 236/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 237/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 238/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 239/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 240/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 241/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 242/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 243/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 244/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 245/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 246/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 247/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 248/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 249/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 250/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 251/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 252/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 253/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 254/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 255/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 256/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 257/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 258/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 259/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 260/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 261/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 262/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 263/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 264/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 265/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 266/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 267/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 268/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 269/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 270/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 271/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 272/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 273/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 274/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 275/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 276/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 277/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 278/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 279/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 280/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 281/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 282/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 283/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 284/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 285/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 286/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 287/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 288/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 289/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 290/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 291/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 292/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 293/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 294/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 295/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 296/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 297/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 298/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 299/300, seasonal_2 Loss: 0.0624 | 0.0292
Epoch 300/300, seasonal_2 Loss: 0.0624 | 0.0292
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.9849397688047757, 'learning_rate': 0.0009829581034448537, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7546047724778451}
Epoch 1/300, seasonal_3 Loss: 0.1500 | 0.0693
Epoch 2/300, seasonal_3 Loss: 0.0910 | 0.0688
Epoch 3/300, seasonal_3 Loss: 0.0871 | 0.0528
Epoch 4/300, seasonal_3 Loss: 0.0824 | 0.0769
Epoch 5/300, seasonal_3 Loss: 0.0791 | 0.0408
Epoch 6/300, seasonal_3 Loss: 0.0768 | 0.0551
Epoch 7/300, seasonal_3 Loss: 0.0792 | 0.0620
Epoch 8/300, seasonal_3 Loss: 0.0737 | 0.0512
Epoch 9/300, seasonal_3 Loss: 0.0687 | 0.0318
Epoch 10/300, seasonal_3 Loss: 0.0686 | 0.0398
Epoch 11/300, seasonal_3 Loss: 0.0722 | 0.0348
Epoch 12/300, seasonal_3 Loss: 0.0689 | 0.0361
Epoch 13/300, seasonal_3 Loss: 0.0654 | 0.0339
Epoch 14/300, seasonal_3 Loss: 0.0649 | 0.0331
Epoch 15/300, seasonal_3 Loss: 0.0628 | 0.0299
Epoch 16/300, seasonal_3 Loss: 0.0608 | 0.0290
Epoch 17/300, seasonal_3 Loss: 0.0610 | 0.0261
Epoch 18/300, seasonal_3 Loss: 0.0600 | 0.0279
Epoch 19/300, seasonal_3 Loss: 0.0588 | 0.0305
Epoch 20/300, seasonal_3 Loss: 0.0602 | 0.0348
Epoch 21/300, seasonal_3 Loss: 0.0598 | 0.0221
Epoch 22/300, seasonal_3 Loss: 0.0570 | 0.0276
Epoch 23/300, seasonal_3 Loss: 0.0621 | 0.0334
Epoch 24/300, seasonal_3 Loss: 0.0592 | 0.0216
Epoch 25/300, seasonal_3 Loss: 0.0565 | 0.0205
Epoch 26/300, seasonal_3 Loss: 0.0558 | 0.0170
Epoch 27/300, seasonal_3 Loss: 0.0533 | 0.0187
Epoch 28/300, seasonal_3 Loss: 0.0535 | 0.0187
Epoch 29/300, seasonal_3 Loss: 0.0524 | 0.0265
Epoch 30/300, seasonal_3 Loss: 0.0548 | 0.0300
Epoch 31/300, seasonal_3 Loss: 0.0546 | 0.0180
Epoch 32/300, seasonal_3 Loss: 0.0516 | 0.0164
Epoch 33/300, seasonal_3 Loss: 0.0502 | 0.0181
Epoch 34/300, seasonal_3 Loss: 0.0494 | 0.0167
Epoch 35/300, seasonal_3 Loss: 0.0494 | 0.0191
Epoch 36/300, seasonal_3 Loss: 0.0488 | 0.0174
Epoch 37/300, seasonal_3 Loss: 0.0498 | 0.0184
Epoch 38/300, seasonal_3 Loss: 0.0498 | 0.0197
Epoch 39/300, seasonal_3 Loss: 0.0486 | 0.0160
Epoch 40/300, seasonal_3 Loss: 0.0482 | 0.0147
Epoch 41/300, seasonal_3 Loss: 0.0474 | 0.0125
Epoch 42/300, seasonal_3 Loss: 0.0471 | 0.0127
Epoch 43/300, seasonal_3 Loss: 0.0470 | 0.0130
Epoch 44/300, seasonal_3 Loss: 0.0469 | 0.0123
Epoch 45/300, seasonal_3 Loss: 0.0462 | 0.0130
Epoch 46/300, seasonal_3 Loss: 0.0453 | 0.0133
Epoch 47/300, seasonal_3 Loss: 0.0447 | 0.0131
Epoch 48/300, seasonal_3 Loss: 0.0444 | 0.0131
Epoch 49/300, seasonal_3 Loss: 0.0445 | 0.0139
Epoch 50/300, seasonal_3 Loss: 0.0444 | 0.0132
Epoch 51/300, seasonal_3 Loss: 0.0442 | 0.0128
Epoch 52/300, seasonal_3 Loss: 0.0437 | 0.0134
Epoch 53/300, seasonal_3 Loss: 0.0435 | 0.0140
Epoch 54/300, seasonal_3 Loss: 0.0429 | 0.0131
Epoch 55/300, seasonal_3 Loss: 0.0424 | 0.0132
Epoch 56/300, seasonal_3 Loss: 0.0421 | 0.0134
Epoch 57/300, seasonal_3 Loss: 0.0417 | 0.0140
Epoch 58/300, seasonal_3 Loss: 0.0416 | 0.0138
Epoch 59/300, seasonal_3 Loss: 0.0414 | 0.0140
Epoch 60/300, seasonal_3 Loss: 0.0414 | 0.0132
Epoch 61/300, seasonal_3 Loss: 0.0410 | 0.0140
Epoch 62/300, seasonal_3 Loss: 0.0406 | 0.0140
Epoch 63/300, seasonal_3 Loss: 0.0402 | 0.0147
Epoch 64/300, seasonal_3 Loss: 0.0400 | 0.0141
Epoch 65/300, seasonal_3 Loss: 0.0399 | 0.0143
Epoch 66/300, seasonal_3 Loss: 0.0403 | 0.0142
Epoch 67/300, seasonal_3 Loss: 0.0413 | 0.0141
Epoch 68/300, seasonal_3 Loss: 0.0412 | 0.0149
Epoch 69/300, seasonal_3 Loss: 0.0388 | 0.0138
Epoch 70/300, seasonal_3 Loss: 0.0411 | 0.0142
Epoch 71/300, seasonal_3 Loss: 0.0410 | 0.0140
Epoch 72/300, seasonal_3 Loss: 0.0395 | 0.0140
Epoch 73/300, seasonal_3 Loss: 0.0392 | 0.0137
Epoch 74/300, seasonal_3 Loss: 0.0391 | 0.0140
Epoch 75/300, seasonal_3 Loss: 0.0369 | 0.0142
Epoch 76/300, seasonal_3 Loss: 0.0360 | 0.0148
Epoch 77/300, seasonal_3 Loss: 0.0387 | 0.0149
Epoch 78/300, seasonal_3 Loss: 0.0386 | 0.0148
Epoch 79/300, seasonal_3 Loss: 0.0382 | 0.0148
Epoch 80/300, seasonal_3 Loss: 0.0381 | 0.0150
Epoch 81/300, seasonal_3 Loss: 0.0378 | 0.0148
Epoch 82/300, seasonal_3 Loss: 0.0383 | 0.0150
Epoch 83/300, seasonal_3 Loss: 0.0378 | 0.0153
Epoch 84/300, seasonal_3 Loss: 0.0367 | 0.0158
Epoch 85/300, seasonal_3 Loss: 0.0349 | 0.0158
Epoch 86/300, seasonal_3 Loss: 0.0338 | 0.0156
Epoch 87/300, seasonal_3 Loss: 0.0340 | 0.0158
Epoch 88/300, seasonal_3 Loss: 0.0325 | 0.0155
Epoch 89/300, seasonal_3 Loss: 0.0321 | 0.0158
Epoch 90/300, seasonal_3 Loss: 0.0327 | 0.0166
Epoch 91/300, seasonal_3 Loss: 0.0363 | 0.0157
Epoch 92/300, seasonal_3 Loss: 0.0382 | 0.0163
Epoch 93/300, seasonal_3 Loss: 0.0358 | 0.0158
Epoch 94/300, seasonal_3 Loss: 0.0349 | 0.0158
Epoch 95/300, seasonal_3 Loss: 0.0342 | 0.0158
Epoch 96/300, seasonal_3 Loss: 0.0327 | 0.0158
Epoch 97/300, seasonal_3 Loss: 0.0317 | 0.0154
Epoch 98/300, seasonal_3 Loss: 0.0316 | 0.0155
Epoch 99/300, seasonal_3 Loss: 0.0321 | 0.0155
Epoch 100/300, seasonal_3 Loss: 0.0316 | 0.0154
Epoch 101/300, seasonal_3 Loss: 0.0312 | 0.0156
Epoch 102/300, seasonal_3 Loss: 0.0310 | 0.0154
Epoch 103/300, seasonal_3 Loss: 0.0317 | 0.0161
Epoch 104/300, seasonal_3 Loss: 0.0317 | 0.0159
Epoch 105/300, seasonal_3 Loss: 0.0315 | 0.0155
Epoch 106/300, seasonal_3 Loss: 0.0308 | 0.0158
Epoch 107/300, seasonal_3 Loss: 0.0308 | 0.0157
Epoch 108/300, seasonal_3 Loss: 0.0302 | 0.0159
Epoch 109/300, seasonal_3 Loss: 0.0302 | 0.0157
Epoch 110/300, seasonal_3 Loss: 0.0309 | 0.0159
Epoch 111/300, seasonal_3 Loss: 0.0314 | 0.0157
Epoch 112/300, seasonal_3 Loss: 0.0301 | 0.0157
Epoch 113/300, seasonal_3 Loss: 0.0345 | 0.0160
Epoch 114/300, seasonal_3 Loss: 0.0311 | 0.0159
Epoch 115/300, seasonal_3 Loss: 0.0301 | 0.0159
Epoch 116/300, seasonal_3 Loss: 0.0297 | 0.0158
Epoch 117/300, seasonal_3 Loss: 0.0295 | 0.0158
Epoch 118/300, seasonal_3 Loss: 0.0292 | 0.0160
Epoch 119/300, seasonal_3 Loss: 0.0301 | 0.0159
Epoch 120/300, seasonal_3 Loss: 0.0293 | 0.0158
Epoch 121/300, seasonal_3 Loss: 0.0306 | 0.0162
Epoch 122/300, seasonal_3 Loss: 0.0297 | 0.0160
Epoch 123/300, seasonal_3 Loss: 0.0292 | 0.0159
Epoch 124/300, seasonal_3 Loss: 0.0287 | 0.0158
Epoch 125/300, seasonal_3 Loss: 0.0284 | 0.0159
Epoch 126/300, seasonal_3 Loss: 0.0286 | 0.0158
Epoch 127/300, seasonal_3 Loss: 0.0285 | 0.0159
Epoch 128/300, seasonal_3 Loss: 0.0284 | 0.0158
Epoch 129/300, seasonal_3 Loss: 0.0283 | 0.0160
Epoch 130/300, seasonal_3 Loss: 0.0285 | 0.0158
Epoch 131/300, seasonal_3 Loss: 0.0275 | 0.0158
Epoch 132/300, seasonal_3 Loss: 0.0274 | 0.0159
Epoch 133/300, seasonal_3 Loss: 0.0273 | 0.0159
Epoch 134/300, seasonal_3 Loss: 0.0273 | 0.0160
Epoch 135/300, seasonal_3 Loss: 0.0274 | 0.0159
Epoch 136/300, seasonal_3 Loss: 0.0272 | 0.0159
Epoch 137/300, seasonal_3 Loss: 0.0273 | 0.0160
Epoch 138/300, seasonal_3 Loss: 0.0272 | 0.0159
Epoch 139/300, seasonal_3 Loss: 0.0271 | 0.0161
Epoch 140/300, seasonal_3 Loss: 0.0274 | 0.0159
Epoch 141/300, seasonal_3 Loss: 0.0281 | 0.0162
Epoch 142/300, seasonal_3 Loss: 0.0280 | 0.0161
Epoch 143/300, seasonal_3 Loss: 0.0272 | 0.0160
Epoch 144/300, seasonal_3 Loss: 0.0270 | 0.0160
Epoch 145/300, seasonal_3 Loss: 0.0269 | 0.0160
Epoch 146/300, seasonal_3 Loss: 0.0268 | 0.0160
Epoch 147/300, seasonal_3 Loss: 0.0268 | 0.0160
Epoch 148/300, seasonal_3 Loss: 0.0269 | 0.0160
Epoch 149/300, seasonal_3 Loss: 0.0268 | 0.0160
Epoch 150/300, seasonal_3 Loss: 0.0270 | 0.0160
Epoch 151/300, seasonal_3 Loss: 0.0269 | 0.0160
Epoch 152/300, seasonal_3 Loss: 0.0269 | 0.0160
Epoch 153/300, seasonal_3 Loss: 0.0268 | 0.0160
Epoch 154/300, seasonal_3 Loss: 0.0268 | 0.0160
Epoch 155/300, seasonal_3 Loss: 0.0267 | 0.0160
Epoch 156/300, seasonal_3 Loss: 0.0267 | 0.0160
Epoch 157/300, seasonal_3 Loss: 0.0266 | 0.0160
Epoch 158/300, seasonal_3 Loss: 0.0267 | 0.0161
Epoch 159/300, seasonal_3 Loss: 0.0265 | 0.0161
Epoch 160/300, seasonal_3 Loss: 0.0266 | 0.0160
Epoch 161/300, seasonal_3 Loss: 0.0268 | 0.0161
Epoch 162/300, seasonal_3 Loss: 0.0267 | 0.0160
Epoch 163/300, seasonal_3 Loss: 0.0267 | 0.0161
Epoch 164/300, seasonal_3 Loss: 0.0265 | 0.0161
Epoch 165/300, seasonal_3 Loss: 0.0266 | 0.0161
Epoch 166/300, seasonal_3 Loss: 0.0267 | 0.0161
Epoch 167/300, seasonal_3 Loss: 0.0266 | 0.0161
Epoch 168/300, seasonal_3 Loss: 0.0266 | 0.0161
Epoch 169/300, seasonal_3 Loss: 0.0265 | 0.0161
Epoch 170/300, seasonal_3 Loss: 0.0264 | 0.0161
Epoch 171/300, seasonal_3 Loss: 0.0264 | 0.0161
Epoch 172/300, seasonal_3 Loss: 0.0265 | 0.0161
Epoch 173/300, seasonal_3 Loss: 0.0266 | 0.0161
Epoch 174/300, seasonal_3 Loss: 0.0264 | 0.0161
Epoch 175/300, seasonal_3 Loss: 0.0263 | 0.0161
Epoch 176/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 177/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 178/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 179/300, seasonal_3 Loss: 0.0264 | 0.0161
Epoch 180/300, seasonal_3 Loss: 0.0263 | 0.0161
Epoch 181/300, seasonal_3 Loss: 0.0264 | 0.0161
Epoch 182/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 183/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 184/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 185/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 186/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 187/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 188/300, seasonal_3 Loss: 0.0262 | 0.0161
Epoch 189/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 190/300, seasonal_3 Loss: 0.0261 | 0.0161
Epoch 191/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 192/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 193/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 194/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 195/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 196/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 197/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 198/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 199/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 200/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 201/300, seasonal_3 Loss: 0.0260 | 0.0161
Epoch 202/300, seasonal_3 Loss: 0.0260 | 0.0162
Epoch 203/300, seasonal_3 Loss: 0.0260 | 0.0162
Epoch 204/300, seasonal_3 Loss: 0.0260 | 0.0162
Epoch 205/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 206/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 207/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 208/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 209/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 210/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 211/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 212/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 213/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 214/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 215/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 216/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 217/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 218/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 219/300, seasonal_3 Loss: 0.0259 | 0.0162
Epoch 220/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 221/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 222/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 223/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 224/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 225/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 226/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 227/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 228/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 229/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 230/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 231/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 232/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 233/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 234/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 235/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 236/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 237/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 238/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 239/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 240/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 241/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 242/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 243/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 244/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 245/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 246/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 247/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 248/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 249/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 250/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 251/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 252/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 253/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 254/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 255/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 256/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 257/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 258/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 259/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 260/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 261/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 262/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 263/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 264/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 265/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 266/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 267/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 268/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 269/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 270/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 271/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 272/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 273/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 274/300, seasonal_3 Loss: 0.0258 | 0.0162
Epoch 275/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 276/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 277/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 278/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 279/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 280/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 281/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 282/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 283/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 284/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 285/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 286/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 287/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 288/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 289/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 290/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 291/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 292/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 293/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 294/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 295/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 296/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 297/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 298/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 299/300, seasonal_3 Loss: 0.0257 | 0.0162
Epoch 300/300, seasonal_3 Loss: 0.0257 | 0.0162
Training resid component with params: {'observation_period_num': 17, 'train_rates': 0.900703787250127, 'learning_rate': 0.00030259516427507636, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9106995074474016}
Epoch 1/300, resid Loss: 0.2904 | 0.2259
Epoch 2/300, resid Loss: 0.1935 | 0.1984
Epoch 3/300, resid Loss: 0.1774 | 0.3212
Epoch 4/300, resid Loss: 0.1594 | 0.2474
Epoch 5/300, resid Loss: 0.1525 | 0.1324
Epoch 6/300, resid Loss: 0.1343 | 0.1286
Epoch 7/300, resid Loss: 0.1384 | 0.1627
Epoch 8/300, resid Loss: 0.1335 | 0.1123
Epoch 9/300, resid Loss: 0.1364 | 0.1285
Epoch 10/300, resid Loss: 0.1438 | 0.1211
Epoch 11/300, resid Loss: 0.1160 | 0.1229
Epoch 12/300, resid Loss: 0.1174 | 0.0918
Epoch 13/300, resid Loss: 0.1139 | 0.0941
Epoch 14/300, resid Loss: 0.1416 | 0.0980
Epoch 15/300, resid Loss: 0.1459 | 0.1036
Epoch 16/300, resid Loss: 0.1557 | 0.1027
Epoch 17/300, resid Loss: 0.1140 | 0.0790
Epoch 18/300, resid Loss: 0.1170 | 0.1051
Epoch 19/300, resid Loss: 0.1058 | 0.0732
Epoch 20/300, resid Loss: 0.1040 | 0.1381
Epoch 21/300, resid Loss: 0.1005 | 0.0758
Epoch 22/300, resid Loss: 0.0950 | 0.0667
Epoch 23/300, resid Loss: 0.0918 | 0.0676
Epoch 24/300, resid Loss: 0.0931 | 0.0608
Epoch 25/300, resid Loss: 0.0880 | 0.0599
Epoch 26/300, resid Loss: 0.0885 | 0.0610
Epoch 27/300, resid Loss: 0.0881 | 0.0582
Epoch 28/300, resid Loss: 0.0851 | 0.0551
Epoch 29/300, resid Loss: 0.0837 | 0.0551
Epoch 30/300, resid Loss: 0.0825 | 0.0552
Epoch 31/300, resid Loss: 0.0818 | 0.0535
Epoch 32/300, resid Loss: 0.0811 | 0.0518
Epoch 33/300, resid Loss: 0.0804 | 0.0506
Epoch 34/300, resid Loss: 0.0800 | 0.0499
Epoch 35/300, resid Loss: 0.0804 | 0.0492
Epoch 36/300, resid Loss: 0.0814 | 0.0493
Epoch 37/300, resid Loss: 0.0796 | 0.0525
Epoch 38/300, resid Loss: 0.0784 | 0.0544
Epoch 39/300, resid Loss: 0.0780 | 0.0536
Epoch 40/300, resid Loss: 0.0772 | 0.0489
Epoch 41/300, resid Loss: 0.0767 | 0.0462
Epoch 42/300, resid Loss: 0.0776 | 0.0534
Epoch 43/300, resid Loss: 0.0793 | 0.0614
Epoch 44/300, resid Loss: 0.0806 | 0.0488
Epoch 45/300, resid Loss: 0.0844 | 0.0581
Epoch 46/300, resid Loss: 0.0840 | 0.0629
Epoch 47/300, resid Loss: 0.0820 | 0.0497
Epoch 48/300, resid Loss: 0.0846 | 0.0496
Epoch 49/300, resid Loss: 0.0802 | 0.0511
Epoch 50/300, resid Loss: 0.0842 | 0.0792
Epoch 51/300, resid Loss: 0.0823 | 0.0526
Epoch 52/300, resid Loss: 0.0783 | 0.0525
Epoch 53/300, resid Loss: 0.0809 | 0.0509
Epoch 54/300, resid Loss: 0.0810 | 0.0459
Epoch 55/300, resid Loss: 0.0750 | 0.0504
Epoch 56/300, resid Loss: 0.0757 | 0.0440
Epoch 57/300, resid Loss: 0.0731 | 0.0421
Epoch 58/300, resid Loss: 0.0721 | 0.0423
Epoch 59/300, resid Loss: 0.0714 | 0.0404
Epoch 60/300, resid Loss: 0.0710 | 0.0399
Epoch 61/300, resid Loss: 0.0706 | 0.0403
Epoch 62/300, resid Loss: 0.0703 | 0.0416
Epoch 63/300, resid Loss: 0.0701 | 0.0399
Epoch 64/300, resid Loss: 0.0697 | 0.0388
Epoch 65/300, resid Loss: 0.0694 | 0.0386
Epoch 66/300, resid Loss: 0.0692 | 0.0388
Epoch 67/300, resid Loss: 0.0690 | 0.0385
Epoch 68/300, resid Loss: 0.0688 | 0.0381
Epoch 69/300, resid Loss: 0.0685 | 0.0380
Epoch 70/300, resid Loss: 0.0684 | 0.0381
Epoch 71/300, resid Loss: 0.0683 | 0.0382
Epoch 72/300, resid Loss: 0.0686 | 0.0385
Epoch 73/300, resid Loss: 0.0694 | 0.0392
Epoch 74/300, resid Loss: 0.0710 | 0.0392
Epoch 75/300, resid Loss: 0.0717 | 0.0379
Epoch 76/300, resid Loss: 0.0699 | 0.0385
Epoch 77/300, resid Loss: 0.0700 | 0.0417
Epoch 78/300, resid Loss: 0.0715 | 0.0431
Epoch 79/300, resid Loss: 0.0700 | 0.0404
Epoch 80/300, resid Loss: 0.0676 | 0.0371
Epoch 81/300, resid Loss: 0.0675 | 0.0371
Epoch 82/300, resid Loss: 0.0675 | 0.0368
Epoch 83/300, resid Loss: 0.0666 | 0.0382
Epoch 84/300, resid Loss: 0.0667 | 0.0376
Epoch 85/300, resid Loss: 0.0665 | 0.0366
Epoch 86/300, resid Loss: 0.0660 | 0.0364
Epoch 87/300, resid Loss: 0.0658 | 0.0362
Epoch 88/300, resid Loss: 0.0656 | 0.0359
Epoch 89/300, resid Loss: 0.0655 | 0.0358
Epoch 90/300, resid Loss: 0.0653 | 0.0358
Epoch 91/300, resid Loss: 0.0651 | 0.0357
Epoch 92/300, resid Loss: 0.0649 | 0.0355
Epoch 93/300, resid Loss: 0.0648 | 0.0355
Epoch 94/300, resid Loss: 0.0647 | 0.0354
Epoch 95/300, resid Loss: 0.0646 | 0.0353
Epoch 96/300, resid Loss: 0.0644 | 0.0352
Epoch 97/300, resid Loss: 0.0643 | 0.0351
Epoch 98/300, resid Loss: 0.0641 | 0.0349
Epoch 99/300, resid Loss: 0.0640 | 0.0348
Epoch 100/300, resid Loss: 0.0639 | 0.0347
Epoch 101/300, resid Loss: 0.0638 | 0.0347
Epoch 102/300, resid Loss: 0.0637 | 0.0347
Epoch 103/300, resid Loss: 0.0636 | 0.0346
Epoch 104/300, resid Loss: 0.0635 | 0.0345
Epoch 105/300, resid Loss: 0.0634 | 0.0345
Epoch 106/300, resid Loss: 0.0632 | 0.0343
Epoch 107/300, resid Loss: 0.0631 | 0.0343
Epoch 108/300, resid Loss: 0.0631 | 0.0342
Epoch 109/300, resid Loss: 0.0630 | 0.0342
Epoch 110/300, resid Loss: 0.0630 | 0.0341
Epoch 111/300, resid Loss: 0.0629 | 0.0340
Epoch 112/300, resid Loss: 0.0628 | 0.0339
Epoch 113/300, resid Loss: 0.0626 | 0.0337
Epoch 114/300, resid Loss: 0.0624 | 0.0337
Epoch 115/300, resid Loss: 0.0624 | 0.0337
Epoch 116/300, resid Loss: 0.0625 | 0.0339
Epoch 117/300, resid Loss: 0.0625 | 0.0338
Epoch 118/300, resid Loss: 0.0624 | 0.0336
Epoch 119/300, resid Loss: 0.0621 | 0.0334
Epoch 120/300, resid Loss: 0.0619 | 0.0334
Epoch 121/300, resid Loss: 0.0619 | 0.0334
Epoch 122/300, resid Loss: 0.0619 | 0.0333
Epoch 123/300, resid Loss: 0.0618 | 0.0332
Epoch 124/300, resid Loss: 0.0616 | 0.0330
Epoch 125/300, resid Loss: 0.0615 | 0.0329
Epoch 126/300, resid Loss: 0.0614 | 0.0329
Epoch 127/300, resid Loss: 0.0613 | 0.0329
Epoch 128/300, resid Loss: 0.0613 | 0.0328
Epoch 129/300, resid Loss: 0.0612 | 0.0327
Epoch 130/300, resid Loss: 0.0611 | 0.0326
Epoch 131/300, resid Loss: 0.0610 | 0.0326
Epoch 132/300, resid Loss: 0.0609 | 0.0325
Epoch 133/300, resid Loss: 0.0609 | 0.0324
Epoch 134/300, resid Loss: 0.0608 | 0.0324
Epoch 135/300, resid Loss: 0.0607 | 0.0323
Epoch 136/300, resid Loss: 0.0607 | 0.0323
Epoch 137/300, resid Loss: 0.0606 | 0.0322
Epoch 138/300, resid Loss: 0.0605 | 0.0322
Epoch 139/300, resid Loss: 0.0605 | 0.0321
Epoch 140/300, resid Loss: 0.0604 | 0.0321
Epoch 141/300, resid Loss: 0.0604 | 0.0320
Epoch 142/300, resid Loss: 0.0603 | 0.0320
Epoch 143/300, resid Loss: 0.0603 | 0.0319
Epoch 144/300, resid Loss: 0.0602 | 0.0319
Epoch 145/300, resid Loss: 0.0602 | 0.0319
Epoch 146/300, resid Loss: 0.0601 | 0.0318
Epoch 147/300, resid Loss: 0.0601 | 0.0318
Epoch 148/300, resid Loss: 0.0600 | 0.0317
Epoch 149/300, resid Loss: 0.0600 | 0.0317
Epoch 150/300, resid Loss: 0.0599 | 0.0317
Epoch 151/300, resid Loss: 0.0599 | 0.0316
Epoch 152/300, resid Loss: 0.0598 | 0.0316
Epoch 153/300, resid Loss: 0.0598 | 0.0315
Epoch 154/300, resid Loss: 0.0597 | 0.0315
Epoch 155/300, resid Loss: 0.0597 | 0.0315
Epoch 156/300, resid Loss: 0.0596 | 0.0314
Epoch 157/300, resid Loss: 0.0596 | 0.0314
Epoch 158/300, resid Loss: 0.0596 | 0.0314
Epoch 159/300, resid Loss: 0.0595 | 0.0313
Epoch 160/300, resid Loss: 0.0595 | 0.0313
Epoch 161/300, resid Loss: 0.0594 | 0.0313
Epoch 162/300, resid Loss: 0.0594 | 0.0312
Epoch 163/300, resid Loss: 0.0594 | 0.0312
Epoch 164/300, resid Loss: 0.0593 | 0.0312
Epoch 165/300, resid Loss: 0.0593 | 0.0311
Epoch 166/300, resid Loss: 0.0592 | 0.0311
Epoch 167/300, resid Loss: 0.0592 | 0.0311
Epoch 168/300, resid Loss: 0.0592 | 0.0311
Epoch 169/300, resid Loss: 0.0591 | 0.0310
Epoch 170/300, resid Loss: 0.0591 | 0.0310
Epoch 171/300, resid Loss: 0.0591 | 0.0310
Epoch 172/300, resid Loss: 0.0590 | 0.0309
Epoch 173/300, resid Loss: 0.0590 | 0.0309
Epoch 174/300, resid Loss: 0.0590 | 0.0309
Epoch 175/300, resid Loss: 0.0589 | 0.0308
Epoch 176/300, resid Loss: 0.0589 | 0.0308
Epoch 177/300, resid Loss: 0.0589 | 0.0308
Epoch 178/300, resid Loss: 0.0588 | 0.0308
Epoch 179/300, resid Loss: 0.0588 | 0.0307
Epoch 180/300, resid Loss: 0.0588 | 0.0307
Epoch 181/300, resid Loss: 0.0587 | 0.0307
Epoch 182/300, resid Loss: 0.0587 | 0.0307
Epoch 183/300, resid Loss: 0.0587 | 0.0306
Epoch 184/300, resid Loss: 0.0587 | 0.0306
Epoch 185/300, resid Loss: 0.0586 | 0.0306
Epoch 186/300, resid Loss: 0.0586 | 0.0306
Epoch 187/300, resid Loss: 0.0586 | 0.0305
Epoch 188/300, resid Loss: 0.0585 | 0.0305
Epoch 189/300, resid Loss: 0.0585 | 0.0305
Epoch 190/300, resid Loss: 0.0585 | 0.0305
Epoch 191/300, resid Loss: 0.0585 | 0.0304
Epoch 192/300, resid Loss: 0.0584 | 0.0304
Epoch 193/300, resid Loss: 0.0584 | 0.0304
Epoch 194/300, resid Loss: 0.0584 | 0.0304
Epoch 195/300, resid Loss: 0.0584 | 0.0304
Epoch 196/300, resid Loss: 0.0583 | 0.0303
Epoch 197/300, resid Loss: 0.0583 | 0.0303
Epoch 198/300, resid Loss: 0.0583 | 0.0303
Epoch 199/300, resid Loss: 0.0583 | 0.0303
Epoch 200/300, resid Loss: 0.0582 | 0.0302
Epoch 201/300, resid Loss: 0.0582 | 0.0302
Epoch 202/300, resid Loss: 0.0582 | 0.0302
Epoch 203/300, resid Loss: 0.0582 | 0.0302
Epoch 204/300, resid Loss: 0.0582 | 0.0302
Epoch 205/300, resid Loss: 0.0581 | 0.0302
Epoch 206/300, resid Loss: 0.0581 | 0.0301
Epoch 207/300, resid Loss: 0.0581 | 0.0301
Epoch 208/300, resid Loss: 0.0582 | 0.0301
Epoch 209/300, resid Loss: 0.0582 | 0.0301
Epoch 210/300, resid Loss: 0.0583 | 0.0302
Epoch 211/300, resid Loss: 0.0583 | 0.0300
Epoch 212/300, resid Loss: 0.0584 | 0.0306
Epoch 213/300, resid Loss: 0.0585 | 0.0300
Epoch 214/300, resid Loss: 0.0586 | 0.0310
Epoch 215/300, resid Loss: 0.0585 | 0.0300
Epoch 216/300, resid Loss: 0.0584 | 0.0313
Epoch 217/300, resid Loss: 0.0582 | 0.0300
Epoch 218/300, resid Loss: 0.0581 | 0.0308
Epoch 219/300, resid Loss: 0.0580 | 0.0299
Epoch 220/300, resid Loss: 0.0579 | 0.0304
Epoch 221/300, resid Loss: 0.0579 | 0.0298
Epoch 222/300, resid Loss: 0.0578 | 0.0301
Epoch 223/300, resid Loss: 0.0578 | 0.0298
Epoch 224/300, resid Loss: 0.0578 | 0.0300
Epoch 225/300, resid Loss: 0.0578 | 0.0299
Epoch 226/300, resid Loss: 0.0577 | 0.0299
Epoch 227/300, resid Loss: 0.0577 | 0.0298
Epoch 228/300, resid Loss: 0.0577 | 0.0299
Epoch 229/300, resid Loss: 0.0577 | 0.0298
Epoch 230/300, resid Loss: 0.0577 | 0.0298
Epoch 231/300, resid Loss: 0.0577 | 0.0298
Epoch 232/300, resid Loss: 0.0577 | 0.0298
Epoch 233/300, resid Loss: 0.0576 | 0.0298
Epoch 234/300, resid Loss: 0.0576 | 0.0298
Epoch 235/300, resid Loss: 0.0576 | 0.0298
Epoch 236/300, resid Loss: 0.0576 | 0.0298
Epoch 237/300, resid Loss: 0.0576 | 0.0297
Epoch 238/300, resid Loss: 0.0576 | 0.0297
Epoch 239/300, resid Loss: 0.0576 | 0.0297
Epoch 240/300, resid Loss: 0.0575 | 0.0297
Epoch 241/300, resid Loss: 0.0575 | 0.0297
Epoch 242/300, resid Loss: 0.0575 | 0.0297
Epoch 243/300, resid Loss: 0.0575 | 0.0297
Epoch 244/300, resid Loss: 0.0575 | 0.0297
Epoch 245/300, resid Loss: 0.0575 | 0.0297
Epoch 246/300, resid Loss: 0.0575 | 0.0297
Epoch 247/300, resid Loss: 0.0575 | 0.0297
Epoch 248/300, resid Loss: 0.0575 | 0.0296
Epoch 249/300, resid Loss: 0.0574 | 0.0296
Epoch 250/300, resid Loss: 0.0574 | 0.0296
Epoch 251/300, resid Loss: 0.0574 | 0.0296
Epoch 252/300, resid Loss: 0.0574 | 0.0296
Epoch 253/300, resid Loss: 0.0574 | 0.0296
Epoch 254/300, resid Loss: 0.0574 | 0.0296
Epoch 255/300, resid Loss: 0.0574 | 0.0296
Epoch 256/300, resid Loss: 0.0574 | 0.0296
Epoch 257/300, resid Loss: 0.0574 | 0.0296
Epoch 258/300, resid Loss: 0.0574 | 0.0296
Epoch 259/300, resid Loss: 0.0573 | 0.0296
Epoch 260/300, resid Loss: 0.0573 | 0.0296
Epoch 261/300, resid Loss: 0.0573 | 0.0295
Epoch 262/300, resid Loss: 0.0573 | 0.0295
Epoch 263/300, resid Loss: 0.0573 | 0.0295
Epoch 264/300, resid Loss: 0.0573 | 0.0295
Epoch 265/300, resid Loss: 0.0573 | 0.0295
Epoch 266/300, resid Loss: 0.0573 | 0.0295
Epoch 267/300, resid Loss: 0.0573 | 0.0295
Epoch 268/300, resid Loss: 0.0573 | 0.0295
Epoch 269/300, resid Loss: 0.0573 | 0.0295
Epoch 270/300, resid Loss: 0.0573 | 0.0295
Epoch 271/300, resid Loss: 0.0572 | 0.0295
Epoch 272/300, resid Loss: 0.0572 | 0.0295
Epoch 273/300, resid Loss: 0.0572 | 0.0295
Epoch 274/300, resid Loss: 0.0572 | 0.0295
Epoch 275/300, resid Loss: 0.0572 | 0.0295
Epoch 276/300, resid Loss: 0.0572 | 0.0295
Epoch 277/300, resid Loss: 0.0572 | 0.0295
Epoch 278/300, resid Loss: 0.0572 | 0.0295
Epoch 279/300, resid Loss: 0.0572 | 0.0295
Epoch 280/300, resid Loss: 0.0572 | 0.0295
Epoch 281/300, resid Loss: 0.0572 | 0.0294
Epoch 282/300, resid Loss: 0.0572 | 0.0294
Epoch 283/300, resid Loss: 0.0572 | 0.0294
Epoch 284/300, resid Loss: 0.0572 | 0.0294
Epoch 285/300, resid Loss: 0.0571 | 0.0294
Epoch 286/300, resid Loss: 0.0571 | 0.0294
Epoch 287/300, resid Loss: 0.0571 | 0.0294
Epoch 288/300, resid Loss: 0.0571 | 0.0294
Epoch 289/300, resid Loss: 0.0571 | 0.0294
Epoch 290/300, resid Loss: 0.0571 | 0.0294
Epoch 291/300, resid Loss: 0.0571 | 0.0294
Epoch 292/300, resid Loss: 0.0571 | 0.0294
Epoch 293/300, resid Loss: 0.0571 | 0.0294
Epoch 294/300, resid Loss: 0.0571 | 0.0294
Epoch 295/300, resid Loss: 0.0571 | 0.0294
Epoch 296/300, resid Loss: 0.0571 | 0.0294
Epoch 297/300, resid Loss: 0.0571 | 0.0294
Epoch 298/300, resid Loss: 0.0571 | 0.0294
Epoch 299/300, resid Loss: 0.0571 | 0.0294
Epoch 300/300, resid Loss: 0.0571 | 0.0294
Runtime (seconds): 2497.9748606681824
0.0005705938137745586
[214.7854]
[0.26086536]
[-3.9415054]
[3.5150142]
[0.6685558]
[9.429444]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 106.96159977093339
RMSE: 10.34222412109375
MAE: 10.34222412109375
R-squared: nan
[224.71777]
