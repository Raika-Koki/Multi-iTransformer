[32m[I 2025-02-03 17:19:52,459][0m A new study created in memory with name: no-name-27506e48-a3a3-44d2-8e73-21acb25a23ba[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
Early stopping at epoch 54
[32m[I 2025-02-03 17:20:25,965][0m Trial 0 finished with value: 1.2140927668030757 and parameters: {'observation_period_num': 152, 'train_rates': 0.7359264252139945, 'learning_rate': 1.9689574759666362e-06, 'batch_size': 84, 'step_size': 1, 'gamma': 0.7948068719975417}. Best is trial 0 with value: 1.2140927668030757.[0m
[32m[I 2025-02-03 17:22:06,216][0m Trial 1 finished with value: 0.2187409954192596 and parameters: {'observation_period_num': 195, 'train_rates': 0.6122354315150432, 'learning_rate': 9.361612184929913e-05, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9786309945102857}. Best is trial 1 with value: 0.2187409954192596.[0m
[32m[I 2025-02-03 17:23:01,433][0m Trial 2 finished with value: 0.16270922303287036 and parameters: {'observation_period_num': 160, 'train_rates': 0.7553665651781305, 'learning_rate': 6.47076191098271e-05, 'batch_size': 91, 'step_size': 15, 'gamma': 0.8508161788206952}. Best is trial 2 with value: 0.16270922303287036.[0m
[32m[I 2025-02-03 17:23:28,177][0m Trial 3 finished with value: 0.8519153961193598 and parameters: {'observation_period_num': 98, 'train_rates': 0.776305029696003, 'learning_rate': 1.4102670593911487e-06, 'batch_size': 202, 'step_size': 8, 'gamma': 0.936987431274287}. Best is trial 2 with value: 0.16270922303287036.[0m
[32m[I 2025-02-03 17:24:01,360][0m Trial 4 finished with value: 0.25092906402366605 and parameters: {'observation_period_num': 104, 'train_rates': 0.6579596462255904, 'learning_rate': 5.1126526793693714e-05, 'batch_size': 147, 'step_size': 3, 'gamma': 0.8458616808265828}. Best is trial 2 with value: 0.16270922303287036.[0m
[32m[I 2025-02-03 17:24:49,619][0m Trial 5 finished with value: 0.12139799131585455 and parameters: {'observation_period_num': 63, 'train_rates': 0.6921509840692928, 'learning_rate': 0.0001912902131838973, 'batch_size': 102, 'step_size': 4, 'gamma': 0.7999820526703104}. Best is trial 5 with value: 0.12139799131585455.[0m
[32m[I 2025-02-03 17:25:46,648][0m Trial 6 finished with value: 0.21445792811772046 and parameters: {'observation_period_num': 72, 'train_rates': 0.6436589863520905, 'learning_rate': 2.5041880619855052e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8117057598467007}. Best is trial 5 with value: 0.12139799131585455.[0m
[32m[I 2025-02-03 17:26:07,033][0m Trial 7 finished with value: 0.1663871482965794 and parameters: {'observation_period_num': 146, 'train_rates': 0.6649811155483863, 'learning_rate': 0.00012244232996688745, 'batch_size': 242, 'step_size': 7, 'gamma': 0.9730663907022666}. Best is trial 5 with value: 0.12139799131585455.[0m
[32m[I 2025-02-03 17:26:40,281][0m Trial 8 finished with value: 0.18553213775157928 and parameters: {'observation_period_num': 97, 'train_rates': 0.963253615380411, 'learning_rate': 4.793196771617543e-05, 'batch_size': 189, 'step_size': 13, 'gamma': 0.9666114201465784}. Best is trial 5 with value: 0.12139799131585455.[0m
[32m[I 2025-02-03 17:27:02,955][0m Trial 9 finished with value: 0.052032014236171195 and parameters: {'observation_period_num': 14, 'train_rates': 0.7118743240729425, 'learning_rate': 0.0003914857406286756, 'batch_size': 242, 'step_size': 12, 'gamma': 0.9165167402132011}. Best is trial 9 with value: 0.052032014236171195.[0m
[32m[I 2025-02-03 17:27:29,177][0m Trial 10 finished with value: 0.04095399644354294 and parameters: {'observation_period_num': 6, 'train_rates': 0.8524722420981447, 'learning_rate': 0.0009514332856830394, 'batch_size': 249, 'step_size': 11, 'gamma': 0.9050774739382764}. Best is trial 10 with value: 0.04095399644354294.[0m
[32m[I 2025-02-03 17:27:55,065][0m Trial 11 finished with value: 0.05157932737486416 and parameters: {'observation_period_num': 13, 'train_rates': 0.8790288155943492, 'learning_rate': 0.0007972201977517431, 'batch_size': 238, 'step_size': 11, 'gamma': 0.9062305511760786}. Best is trial 10 with value: 0.04095399644354294.[0m
[32m[I 2025-02-03 17:28:25,005][0m Trial 12 finished with value: 0.03714982056340506 and parameters: {'observation_period_num': 11, 'train_rates': 0.8685860282318415, 'learning_rate': 0.0008546514195398231, 'batch_size': 198, 'step_size': 10, 'gamma': 0.8948288148462173}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:28:55,182][0m Trial 13 finished with value: 0.05354208141090115 and parameters: {'observation_period_num': 37, 'train_rates': 0.8495776266380428, 'learning_rate': 0.0007964133384228216, 'batch_size': 195, 'step_size': 10, 'gamma': 0.8882237020026685}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:29:26,277][0m Trial 14 finished with value: 0.30496766333803166 and parameters: {'observation_period_num': 250, 'train_rates': 0.8498273144124701, 'learning_rate': 1.2941035704646009e-05, 'batch_size': 159, 'step_size': 15, 'gamma': 0.7598767230223797}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:29:50,760][0m Trial 15 finished with value: 0.10281223803758621 and parameters: {'observation_period_num': 49, 'train_rates': 0.9456233819674348, 'learning_rate': 0.0003811232587385302, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8655174638831554}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:30:25,554][0m Trial 16 finished with value: 0.20776309630424855 and parameters: {'observation_period_num': 9, 'train_rates': 0.9058122742744197, 'learning_rate': 6.577637724299197e-06, 'batch_size': 171, 'step_size': 5, 'gamma': 0.9342430290990826}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:31:11,768][0m Trial 17 finished with value: 0.04922444621721903 and parameters: {'observation_period_num': 35, 'train_rates': 0.8148182565344004, 'learning_rate': 0.00028501412597714006, 'batch_size': 120, 'step_size': 10, 'gamma': 0.8914558719730671}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:31:35,778][0m Trial 18 finished with value: 0.1079418150062897 and parameters: {'observation_period_num': 81, 'train_rates': 0.82202376764613, 'learning_rate': 0.000500891715544777, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9389211498519439}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:32:03,383][0m Trial 19 finished with value: 0.09748785024774927 and parameters: {'observation_period_num': 195, 'train_rates': 0.9188979266575543, 'learning_rate': 0.000932580371066212, 'batch_size': 220, 'step_size': 13, 'gamma': 0.8349703775362622}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:32:35,294][0m Trial 20 finished with value: 0.11497346082023371 and parameters: {'observation_period_num': 125, 'train_rates': 0.8863868434476567, 'learning_rate': 0.0001772081702387755, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8826349683538182}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:33:21,103][0m Trial 21 finished with value: 0.054277602287511974 and parameters: {'observation_period_num': 36, 'train_rates': 0.8056891625418695, 'learning_rate': 0.00025485038835426925, 'batch_size': 123, 'step_size': 11, 'gamma': 0.9000596299550764}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:34:06,156][0m Trial 22 finished with value: 0.046361035735310405 and parameters: {'observation_period_num': 33, 'train_rates': 0.8405274997920008, 'learning_rate': 0.0004695006725845616, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8742221081936918}. Best is trial 12 with value: 0.03714982056340506.[0m
[32m[I 2025-02-03 17:35:45,725][0m Trial 23 finished with value: 0.03466960328860559 and parameters: {'observation_period_num': 7, 'train_rates': 0.8595873995105556, 'learning_rate': 0.0005558832589791693, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8717446688820923}. Best is trial 23 with value: 0.03466960328860559.[0m
[32m[I 2025-02-03 17:40:33,693][0m Trial 24 finished with value: 0.03121056923855893 and parameters: {'observation_period_num': 6, 'train_rates': 0.9319207490185476, 'learning_rate': 0.0009814938077774109, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9214074510694684}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:46:16,180][0m Trial 25 finished with value: 0.050241418182849884 and parameters: {'observation_period_num': 50, 'train_rates': 0.9785489784310244, 'learning_rate': 0.00012570785288247547, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9542247942875325}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:47:58,233][0m Trial 26 finished with value: 0.053153791979831805 and parameters: {'observation_period_num': 21, 'train_rates': 0.9421308563309531, 'learning_rate': 0.0007136212440369032, 'batch_size': 58, 'step_size': 12, 'gamma': 0.9278178776331422}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:52:06,535][0m Trial 27 finished with value: 0.08335908430310535 and parameters: {'observation_period_num': 54, 'train_rates': 0.8857096305280899, 'learning_rate': 0.0005346012769986629, 'batch_size': 22, 'step_size': 8, 'gamma': 0.8647882335856204}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:53:58,544][0m Trial 28 finished with value: 0.2651263592618235 and parameters: {'observation_period_num': 82, 'train_rates': 0.9069866609593613, 'learning_rate': 3.561876406442485e-06, 'batch_size': 50, 'step_size': 12, 'gamma': 0.825596490319564}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:55:12,895][0m Trial 29 finished with value: 0.08923921456780677 and parameters: {'observation_period_num': 29, 'train_rates': 0.7858603550286511, 'learning_rate': 2.9382792454266674e-05, 'batch_size': 70, 'step_size': 9, 'gamma': 0.7626565465172274}. Best is trial 24 with value: 0.03121056923855893.[0m
[32m[I 2025-02-03 17:58:28,246][0m Trial 30 finished with value: 0.030719790562807797 and parameters: {'observation_period_num': 5, 'train_rates': 0.9439469500552293, 'learning_rate': 0.0002586122586310003, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9176291943212712}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:01:36,879][0m Trial 31 finished with value: 0.03405149732120512 and parameters: {'observation_period_num': 8, 'train_rates': 0.9285413191390349, 'learning_rate': 0.00028551399879483594, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9149341676517126}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:04:23,972][0m Trial 32 finished with value: 0.03118392322581298 and parameters: {'observation_period_num': 5, 'train_rates': 0.9317244548744469, 'learning_rate': 0.00025648530048263507, 'batch_size': 35, 'step_size': 1, 'gamma': 0.9522071621057979}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:07:44,241][0m Trial 33 finished with value: 0.048472024500370026 and parameters: {'observation_period_num': 24, 'train_rates': 0.984626700885962, 'learning_rate': 8.786393777352982e-05, 'batch_size': 30, 'step_size': 1, 'gamma': 0.95486990903955}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:10:17,925][0m Trial 34 finished with value: 0.048987619329870925 and parameters: {'observation_period_num': 47, 'train_rates': 0.9305729584357022, 'learning_rate': 0.0002590145703963542, 'batch_size': 37, 'step_size': 1, 'gamma': 0.9187165852654952}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:12:33,786][0m Trial 35 finished with value: 0.1702805600803474 and parameters: {'observation_period_num': 207, 'train_rates': 0.9577510027239892, 'learning_rate': 0.00014947651349886678, 'batch_size': 41, 'step_size': 2, 'gamma': 0.9873935341929931}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:13:58,440][0m Trial 36 finished with value: 0.040716231527577446 and parameters: {'observation_period_num': 24, 'train_rates': 0.9151196706978194, 'learning_rate': 0.0003170895570120911, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9581628183306463}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:19:56,200][0m Trial 37 finished with value: 0.05488055148253254 and parameters: {'observation_period_num': 65, 'train_rates': 0.9647011218067433, 'learning_rate': 8.058682682591546e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9446572998420293}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:20:57,736][0m Trial 38 finished with value: 0.7419027654748214 and parameters: {'observation_period_num': 166, 'train_rates': 0.9387776125148547, 'learning_rate': 1.0113316207055915e-06, 'batch_size': 93, 'step_size': 4, 'gamma': 0.9183499578886964}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:23:41,139][0m Trial 39 finished with value: 0.08821800611913204 and parameters: {'observation_period_num': 117, 'train_rates': 0.8943586117306397, 'learning_rate': 0.00020931473365670602, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9465406922941483}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:25:20,830][0m Trial 40 finished with value: 0.11808182896766688 and parameters: {'observation_period_num': 61, 'train_rates': 0.7395088768191138, 'learning_rate': 1.8666279678839133e-05, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9816357032703739}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:27:05,680][0m Trial 41 finished with value: 0.033249370753765106 and parameters: {'observation_period_num': 7, 'train_rates': 0.988931481766186, 'learning_rate': 0.0005710729289522479, 'batch_size': 59, 'step_size': 1, 'gamma': 0.9265137730614039}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:28:33,242][0m Trial 42 finished with value: 0.06316572427749634 and parameters: {'observation_period_num': 5, 'train_rates': 0.983457709954453, 'learning_rate': 5.4538538865618724e-05, 'batch_size': 69, 'step_size': 1, 'gamma': 0.932393657398052}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:31:49,619][0m Trial 43 finished with value: 0.04001546892990817 and parameters: {'observation_period_num': 20, 'train_rates': 0.9545289250598771, 'learning_rate': 0.0003750829608181144, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9146335952748764}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:33:54,751][0m Trial 44 finished with value: 0.0827283609481085 and parameters: {'observation_period_num': 40, 'train_rates': 0.9278420390560016, 'learning_rate': 0.0005874338716892559, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9636786789678385}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:35:09,303][0m Trial 45 finished with value: 0.07317402264040508 and parameters: {'observation_period_num': 18, 'train_rates': 0.9697917540182922, 'learning_rate': 0.00012699105707347102, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9226053905398262}. Best is trial 30 with value: 0.030719790562807797.[0m
[32m[I 2025-02-03 18:38:50,546][0m Trial 46 finished with value: 0.030155414496274555 and parameters: {'observation_period_num': 5, 'train_rates': 0.988471372030556, 'learning_rate': 0.00019036134555618905, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9729472098446305}. Best is trial 46 with value: 0.030155414496274555.[0m
[32m[I 2025-02-03 18:39:52,138][0m Trial 47 finished with value: 0.03953985124826431 and parameters: {'observation_period_num': 25, 'train_rates': 0.9825165781424264, 'learning_rate': 0.00019888007372414383, 'batch_size': 100, 'step_size': 5, 'gamma': 0.9758858131966748}. Best is trial 46 with value: 0.030155414496274555.[0m
[32m[I 2025-02-03 18:43:32,021][0m Trial 48 finished with value: 0.12691742013539037 and parameters: {'observation_period_num': 144, 'train_rates': 0.9889811011082934, 'learning_rate': 0.0006418556820002488, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9665425313764261}. Best is trial 46 with value: 0.030155414496274555.[0m
[32m[I 2025-02-03 18:45:07,437][0m Trial 49 finished with value: 0.046825046677674564 and parameters: {'observation_period_num': 42, 'train_rates': 0.9519691568407435, 'learning_rate': 0.0004018902482956407, 'batch_size': 62, 'step_size': 2, 'gamma': 0.9464346692370348}. Best is trial 46 with value: 0.030155414496274555.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2044 | 0.1037
Epoch 2/300, Loss: 0.0895 | 0.0692
Epoch 3/300, Loss: 0.0743 | 0.0659
Epoch 4/300, Loss: 0.0677 | 0.0658
Epoch 5/300, Loss: 0.0635 | 0.0630
Epoch 6/300, Loss: 0.0593 | 0.0601
Epoch 7/300, Loss: 0.0553 | 0.0567
Epoch 8/300, Loss: 0.0521 | 0.0551
Epoch 9/300, Loss: 0.0498 | 0.0534
Epoch 10/300, Loss: 0.0480 | 0.0519
Epoch 11/300, Loss: 0.0463 | 0.0510
Epoch 12/300, Loss: 0.0443 | 0.0499
Epoch 13/300, Loss: 0.0423 | 0.0490
Epoch 14/300, Loss: 0.0402 | 0.0479
Epoch 15/300, Loss: 0.0385 | 0.0463
Epoch 16/300, Loss: 0.0373 | 0.0450
Epoch 17/300, Loss: 0.0367 | 0.0451
Epoch 18/300, Loss: 0.0361 | 0.0450
Epoch 19/300, Loss: 0.0357 | 0.0436
Epoch 20/300, Loss: 0.0355 | 0.0424
Epoch 21/300, Loss: 0.0350 | 0.0400
Epoch 22/300, Loss: 0.0344 | 0.0386
Epoch 23/300, Loss: 0.0340 | 0.0382
Epoch 24/300, Loss: 0.0334 | 0.0380
Epoch 25/300, Loss: 0.0330 | 0.0384
Epoch 26/300, Loss: 0.0328 | 0.0384
Epoch 27/300, Loss: 0.0324 | 0.0378
Epoch 28/300, Loss: 0.0321 | 0.0362
Epoch 29/300, Loss: 0.0318 | 0.0358
Epoch 30/300, Loss: 0.0316 | 0.0346
Epoch 31/300, Loss: 0.0314 | 0.0333
Epoch 32/300, Loss: 0.0312 | 0.0337
Epoch 33/300, Loss: 0.0307 | 0.0336
Epoch 34/300, Loss: 0.0303 | 0.0329
Epoch 35/300, Loss: 0.0300 | 0.0332
Epoch 36/300, Loss: 0.0296 | 0.0336
Epoch 37/300, Loss: 0.0292 | 0.0322
Epoch 38/300, Loss: 0.0289 | 0.0315
Epoch 39/300, Loss: 0.0287 | 0.0322
Epoch 40/300, Loss: 0.0285 | 0.0299
Epoch 41/300, Loss: 0.0283 | 0.0290
Epoch 42/300, Loss: 0.0282 | 0.0279
Epoch 43/300, Loss: 0.0280 | 0.0265
Epoch 44/300, Loss: 0.0277 | 0.0262
Epoch 45/300, Loss: 0.0274 | 0.0253
Epoch 46/300, Loss: 0.0268 | 0.0236
Epoch 47/300, Loss: 0.0262 | 0.0223
Epoch 48/300, Loss: 0.0257 | 0.0215
Epoch 49/300, Loss: 0.0253 | 0.0212
Epoch 50/300, Loss: 0.0251 | 0.0212
Epoch 51/300, Loss: 0.0248 | 0.0213
Epoch 52/300, Loss: 0.0246 | 0.0214
Epoch 53/300, Loss: 0.0243 | 0.0211
Epoch 54/300, Loss: 0.0241 | 0.0209
Epoch 55/300, Loss: 0.0239 | 0.0209
Epoch 56/300, Loss: 0.0239 | 0.0210
Epoch 57/300, Loss: 0.0239 | 0.0221
Epoch 58/300, Loss: 0.0235 | 0.0222
Epoch 59/300, Loss: 0.0234 | 0.0225
Epoch 60/300, Loss: 0.0231 | 0.0225
Epoch 61/300, Loss: 0.0231 | 0.0226
Epoch 62/300, Loss: 0.0231 | 0.0220
Epoch 63/300, Loss: 0.0229 | 0.0212
Epoch 64/300, Loss: 0.0228 | 0.0205
Epoch 65/300, Loss: 0.0227 | 0.0203
Epoch 66/300, Loss: 0.0227 | 0.0197
Epoch 67/300, Loss: 0.0225 | 0.0196
Epoch 68/300, Loss: 0.0224 | 0.0198
Epoch 69/300, Loss: 0.0221 | 0.0196
Epoch 70/300, Loss: 0.0220 | 0.0201
Epoch 71/300, Loss: 0.0219 | 0.0200
Epoch 72/300, Loss: 0.0218 | 0.0198
Epoch 73/300, Loss: 0.0217 | 0.0198
Epoch 74/300, Loss: 0.0216 | 0.0196
Epoch 75/300, Loss: 0.0215 | 0.0193
Epoch 76/300, Loss: 0.0215 | 0.0191
Epoch 77/300, Loss: 0.0214 | 0.0194
Epoch 78/300, Loss: 0.0212 | 0.0192
Epoch 79/300, Loss: 0.0211 | 0.0191
Epoch 80/300, Loss: 0.0210 | 0.0192
Epoch 81/300, Loss: 0.0208 | 0.0192
Epoch 82/300, Loss: 0.0206 | 0.0196
Epoch 83/300, Loss: 0.0206 | 0.0195
Epoch 84/300, Loss: 0.0205 | 0.0192
Epoch 85/300, Loss: 0.0205 | 0.0192
Epoch 86/300, Loss: 0.0203 | 0.0193
Epoch 87/300, Loss: 0.0202 | 0.0195
Epoch 88/300, Loss: 0.0201 | 0.0193
Epoch 89/300, Loss: 0.0201 | 0.0193
Epoch 90/300, Loss: 0.0200 | 0.0193
Epoch 91/300, Loss: 0.0200 | 0.0193
Epoch 92/300, Loss: 0.0198 | 0.0193
Epoch 93/300, Loss: 0.0197 | 0.0195
Epoch 94/300, Loss: 0.0196 | 0.0195
Epoch 95/300, Loss: 0.0195 | 0.0196
Epoch 96/300, Loss: 0.0194 | 0.0198
Epoch 97/300, Loss: 0.0194 | 0.0198
Epoch 98/300, Loss: 0.0193 | 0.0200
Epoch 99/300, Loss: 0.0192 | 0.0200
Epoch 100/300, Loss: 0.0191 | 0.0202
Epoch 101/300, Loss: 0.0191 | 0.0202
Epoch 102/300, Loss: 0.0190 | 0.0204
Epoch 103/300, Loss: 0.0190 | 0.0206
Epoch 104/300, Loss: 0.0189 | 0.0205
Epoch 105/300, Loss: 0.0189 | 0.0209
Epoch 106/300, Loss: 0.0189 | 0.0207
Epoch 107/300, Loss: 0.0189 | 0.0212
Epoch 108/300, Loss: 0.0191 | 0.0207
Epoch 109/300, Loss: 0.0195 | 0.0217
Epoch 110/300, Loss: 0.0207 | 0.0215
Epoch 111/300, Loss: 0.0194 | 0.0208
Epoch 112/300, Loss: 0.0196 | 0.0222
Epoch 113/300, Loss: 0.0193 | 0.0217
Epoch 114/300, Loss: 0.0194 | 0.0229
Epoch 115/300, Loss: 0.0191 | 0.0224
Epoch 116/300, Loss: 0.0190 | 0.0229
Epoch 117/300, Loss: 0.0189 | 0.0224
Epoch 118/300, Loss: 0.0188 | 0.0224
Epoch 119/300, Loss: 0.0187 | 0.0219
Epoch 120/300, Loss: 0.0186 | 0.0219
Epoch 121/300, Loss: 0.0186 | 0.0214
Epoch 122/300, Loss: 0.0184 | 0.0215
Epoch 123/300, Loss: 0.0184 | 0.0212
Epoch 124/300, Loss: 0.0183 | 0.0211
Epoch 125/300, Loss: 0.0182 | 0.0213
Epoch 126/300, Loss: 0.0181 | 0.0211
Epoch 127/300, Loss: 0.0181 | 0.0213
Epoch 128/300, Loss: 0.0180 | 0.0212
Epoch 129/300, Loss: 0.0180 | 0.0214
Epoch 130/300, Loss: 0.0178 | 0.0212
Epoch 131/300, Loss: 0.0179 | 0.0215
Epoch 132/300, Loss: 0.0177 | 0.0213
Epoch 133/300, Loss: 0.0177 | 0.0216
Epoch 134/300, Loss: 0.0176 | 0.0214
Epoch 135/300, Loss: 0.0177 | 0.0217
Epoch 136/300, Loss: 0.0175 | 0.0215
Epoch 137/300, Loss: 0.0175 | 0.0218
Epoch 138/300, Loss: 0.0174 | 0.0216
Epoch 139/300, Loss: 0.0175 | 0.0218
Epoch 140/300, Loss: 0.0173 | 0.0217
Epoch 141/300, Loss: 0.0174 | 0.0219
Epoch 142/300, Loss: 0.0172 | 0.0218
Epoch 143/300, Loss: 0.0173 | 0.0220
Epoch 144/300, Loss: 0.0171 | 0.0219
Epoch 145/300, Loss: 0.0172 | 0.0222
Epoch 146/300, Loss: 0.0171 | 0.0220
Epoch 147/300, Loss: 0.0171 | 0.0223
Epoch 148/300, Loss: 0.0170 | 0.0221
Epoch 149/300, Loss: 0.0171 | 0.0224
Epoch 150/300, Loss: 0.0169 | 0.0222
Epoch 151/300, Loss: 0.0170 | 0.0225
Epoch 152/300, Loss: 0.0169 | 0.0223
Epoch 153/300, Loss: 0.0169 | 0.0226
Epoch 154/300, Loss: 0.0168 | 0.0224
Epoch 155/300, Loss: 0.0169 | 0.0227
Epoch 156/300, Loss: 0.0168 | 0.0225
Epoch 157/300, Loss: 0.0168 | 0.0227
Epoch 158/300, Loss: 0.0167 | 0.0226
Epoch 159/300, Loss: 0.0167 | 0.0228
Epoch 160/300, Loss: 0.0166 | 0.0226
Epoch 161/300, Loss: 0.0167 | 0.0228
Epoch 162/300, Loss: 0.0166 | 0.0227
Epoch 163/300, Loss: 0.0166 | 0.0229
Epoch 164/300, Loss: 0.0165 | 0.0227
Epoch 165/300, Loss: 0.0166 | 0.0229
Epoch 166/300, Loss: 0.0165 | 0.0228
Epoch 167/300, Loss: 0.0165 | 0.0229
Epoch 168/300, Loss: 0.0164 | 0.0228
Epoch 169/300, Loss: 0.0165 | 0.0230
Epoch 170/300, Loss: 0.0164 | 0.0229
Epoch 171/300, Loss: 0.0164 | 0.0231
Epoch 172/300, Loss: 0.0163 | 0.0230
Epoch 173/300, Loss: 0.0164 | 0.0231
Epoch 174/300, Loss: 0.0163 | 0.0231
Epoch 175/300, Loss: 0.0163 | 0.0233
Epoch 176/300, Loss: 0.0163 | 0.0231
Epoch 177/300, Loss: 0.0163 | 0.0233
Epoch 178/300, Loss: 0.0162 | 0.0233
Epoch 179/300, Loss: 0.0162 | 0.0234
Epoch 180/300, Loss: 0.0162 | 0.0234
Epoch 181/300, Loss: 0.0162 | 0.0235
Epoch 182/300, Loss: 0.0162 | 0.0234
Epoch 183/300, Loss: 0.0162 | 0.0235
Epoch 184/300, Loss: 0.0162 | 0.0234
Epoch 185/300, Loss: 0.0162 | 0.0234
Epoch 186/300, Loss: 0.0162 | 0.0234
Epoch 187/300, Loss: 0.0161 | 0.0234
Epoch 188/300, Loss: 0.0161 | 0.0233
Epoch 189/300, Loss: 0.0161 | 0.0233
Epoch 190/300, Loss: 0.0161 | 0.0232
Epoch 191/300, Loss: 0.0161 | 0.0232
Epoch 192/300, Loss: 0.0161 | 0.0231
Epoch 193/300, Loss: 0.0161 | 0.0230
Epoch 194/300, Loss: 0.0160 | 0.0230
Epoch 195/300, Loss: 0.0160 | 0.0229
Epoch 196/300, Loss: 0.0160 | 0.0228
Epoch 197/300, Loss: 0.0160 | 0.0228
Epoch 198/300, Loss: 0.0160 | 0.0227
Epoch 199/300, Loss: 0.0159 | 0.0227
Epoch 200/300, Loss: 0.0159 | 0.0227
Epoch 201/300, Loss: 0.0159 | 0.0226
Epoch 202/300, Loss: 0.0159 | 0.0226
Epoch 203/300, Loss: 0.0159 | 0.0226
Epoch 204/300, Loss: 0.0159 | 0.0226
Epoch 205/300, Loss: 0.0159 | 0.0226
Epoch 206/300, Loss: 0.0158 | 0.0226
Epoch 207/300, Loss: 0.0158 | 0.0226
Epoch 208/300, Loss: 0.0158 | 0.0226
Epoch 209/300, Loss: 0.0158 | 0.0226
Epoch 210/300, Loss: 0.0158 | 0.0227
Epoch 211/300, Loss: 0.0158 | 0.0227
Epoch 212/300, Loss: 0.0158 | 0.0227
Epoch 213/300, Loss: 0.0158 | 0.0227
Epoch 214/300, Loss: 0.0158 | 0.0227
Epoch 215/300, Loss: 0.0158 | 0.0228
Epoch 216/300, Loss: 0.0158 | 0.0228
Epoch 217/300, Loss: 0.0157 | 0.0228
Epoch 218/300, Loss: 0.0157 | 0.0229
Epoch 219/300, Loss: 0.0157 | 0.0229
Epoch 220/300, Loss: 0.0157 | 0.0229
Epoch 221/300, Loss: 0.0157 | 0.0230
Epoch 222/300, Loss: 0.0156 | 0.0230
Epoch 223/300, Loss: 0.0156 | 0.0230
Epoch 224/300, Loss: 0.0156 | 0.0231
Epoch 225/300, Loss: 0.0156 | 0.0231
Epoch 226/300, Loss: 0.0156 | 0.0231
Epoch 227/300, Loss: 0.0155 | 0.0231
Epoch 228/300, Loss: 0.0155 | 0.0232
Epoch 229/300, Loss: 0.0155 | 0.0232
Epoch 230/300, Loss: 0.0155 | 0.0232
Epoch 231/300, Loss: 0.0155 | 0.0232
Epoch 232/300, Loss: 0.0155 | 0.0233
Epoch 233/300, Loss: 0.0155 | 0.0233
Epoch 234/300, Loss: 0.0154 | 0.0233
Epoch 235/300, Loss: 0.0154 | 0.0233
Epoch 236/300, Loss: 0.0154 | 0.0233
Epoch 237/300, Loss: 0.0154 | 0.0234
Epoch 238/300, Loss: 0.0154 | 0.0234
Epoch 239/300, Loss: 0.0154 | 0.0234
Epoch 240/300, Loss: 0.0154 | 0.0234
Epoch 241/300, Loss: 0.0154 | 0.0235
Epoch 242/300, Loss: 0.0154 | 0.0235
Epoch 243/300, Loss: 0.0153 | 0.0235
Epoch 244/300, Loss: 0.0153 | 0.0235
Epoch 245/300, Loss: 0.0153 | 0.0235
Epoch 246/300, Loss: 0.0153 | 0.0236
Epoch 247/300, Loss: 0.0153 | 0.0236
Epoch 248/300, Loss: 0.0153 | 0.0236
Epoch 249/300, Loss: 0.0153 | 0.0236
Epoch 250/300, Loss: 0.0153 | 0.0236
Epoch 251/300, Loss: 0.0153 | 0.0237
Epoch 252/300, Loss: 0.0153 | 0.0237
Epoch 253/300, Loss: 0.0153 | 0.0237
Epoch 254/300, Loss: 0.0152 | 0.0237
Epoch 255/300, Loss: 0.0152 | 0.0237
Epoch 256/300, Loss: 0.0152 | 0.0237
Epoch 257/300, Loss: 0.0152 | 0.0238
Epoch 258/300, Loss: 0.0152 | 0.0238
Epoch 259/300, Loss: 0.0152 | 0.0238
Epoch 260/300, Loss: 0.0152 | 0.0238
Epoch 261/300, Loss: 0.0152 | 0.0238
Epoch 262/300, Loss: 0.0152 | 0.0239
Epoch 263/300, Loss: 0.0152 | 0.0239
Epoch 264/300, Loss: 0.0152 | 0.0239
Epoch 265/300, Loss: 0.0152 | 0.0239
Epoch 266/300, Loss: 0.0152 | 0.0239
Epoch 267/300, Loss: 0.0152 | 0.0239
Epoch 268/300, Loss: 0.0152 | 0.0239
Epoch 269/300, Loss: 0.0151 | 0.0240
Epoch 270/300, Loss: 0.0151 | 0.0240
Epoch 271/300, Loss: 0.0151 | 0.0240
Epoch 272/300, Loss: 0.0151 | 0.0240
Epoch 273/300, Loss: 0.0151 | 0.0240
Epoch 274/300, Loss: 0.0151 | 0.0240
Epoch 275/300, Loss: 0.0151 | 0.0241
Epoch 276/300, Loss: 0.0151 | 0.0241
Epoch 277/300, Loss: 0.0151 | 0.0241
Epoch 278/300, Loss: 0.0151 | 0.0241
Epoch 279/300, Loss: 0.0151 | 0.0241
Epoch 280/300, Loss: 0.0151 | 0.0241
Epoch 281/300, Loss: 0.0151 | 0.0241
Epoch 282/300, Loss: 0.0151 | 0.0241
Epoch 283/300, Loss: 0.0151 | 0.0242
Epoch 284/300, Loss: 0.0151 | 0.0242
Epoch 285/300, Loss: 0.0151 | 0.0242
Epoch 286/300, Loss: 0.0151 | 0.0242
Epoch 287/300, Loss: 0.0151 | 0.0242
Epoch 288/300, Loss: 0.0151 | 0.0242
Epoch 289/300, Loss: 0.0150 | 0.0242
Epoch 290/300, Loss: 0.0150 | 0.0242
Epoch 291/300, Loss: 0.0150 | 0.0242
Epoch 292/300, Loss: 0.0150 | 0.0243
Epoch 293/300, Loss: 0.0150 | 0.0243
Epoch 294/300, Loss: 0.0150 | 0.0243
Epoch 295/300, Loss: 0.0150 | 0.0243
Epoch 296/300, Loss: 0.0150 | 0.0243
Epoch 297/300, Loss: 0.0150 | 0.0243
Epoch 298/300, Loss: 0.0150 | 0.0243
Epoch 299/300, Loss: 0.0150 | 0.0243
Epoch 300/300, Loss: 0.0150 | 0.0243
Runtime (seconds): 667.6392703056335
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 6.860577033890877
RMSE: 2.6192703247070312
MAE: 2.6192703247070312
R-squared: nan
[119.17073]
