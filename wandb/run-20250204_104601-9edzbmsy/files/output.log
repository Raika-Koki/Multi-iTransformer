[32m[I 2025-02-04 10:46:04,296][0m A new study created in memory with name: no-name-48d5d8f9-696a-4e30-b179-f6bea78983ba[0m
[32m[I 2025-02-04 10:47:00,224][0m Trial 0 finished with value: 0.08436488750663218 and parameters: {'observation_period_num': 42, 'train_rates': 0.893673605637821, 'learning_rate': 2.464235047025816e-05, 'batch_size': 102, 'step_size': 12, 'gamma': 0.7822920299633914}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:47:54,482][0m Trial 1 finished with value: 0.12672846558246206 and parameters: {'observation_period_num': 167, 'train_rates': 0.7483767527807834, 'learning_rate': 0.00010264457198174235, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8200505112927063}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:48:31,225][0m Trial 2 finished with value: 0.565963864326477 and parameters: {'observation_period_num': 245, 'train_rates': 0.9683243434191596, 'learning_rate': 1.7597206329884484e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.7557318283911391}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:49:02,386][0m Trial 3 finished with value: 0.10887989461421967 and parameters: {'observation_period_num': 79, 'train_rates': 0.7836270112942582, 'learning_rate': 4.905597800899977e-05, 'batch_size': 175, 'step_size': 15, 'gamma': 0.7987814378052862}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:49:59,773][0m Trial 4 finished with value: 0.6452150828097446 and parameters: {'observation_period_num': 102, 'train_rates': 0.8247724246467419, 'learning_rate': 1.541382018896742e-06, 'batch_size': 93, 'step_size': 12, 'gamma': 0.7678626381890853}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:51:00,789][0m Trial 5 finished with value: 0.1286858733939497 and parameters: {'observation_period_num': 43, 'train_rates': 0.7920120040091341, 'learning_rate': 1.742737092843796e-05, 'batch_size': 88, 'step_size': 8, 'gamma': 0.8076880679504332}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:52:56,658][0m Trial 6 finished with value: 0.16820146076930603 and parameters: {'observation_period_num': 70, 'train_rates': 0.7330301802958208, 'learning_rate': 2.4090736544813385e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.8326379895725923}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:53:57,301][0m Trial 7 finished with value: 0.45078434569891107 and parameters: {'observation_period_num': 169, 'train_rates': 0.9385500592681907, 'learning_rate': 5.6965396697365614e-06, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8262612976407422}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:55:20,746][0m Trial 8 finished with value: 0.09949326027056267 and parameters: {'observation_period_num': 104, 'train_rates': 0.8887048740809661, 'learning_rate': 0.0005824373840108132, 'batch_size': 66, 'step_size': 4, 'gamma': 0.9385379005172172}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:56:05,209][0m Trial 9 finished with value: 0.2032398160946488 and parameters: {'observation_period_num': 61, 'train_rates': 0.7239201096069326, 'learning_rate': 2.677841670877082e-05, 'batch_size': 115, 'step_size': 3, 'gamma': 0.9245686691982857}. Best is trial 0 with value: 0.08436488750663218.[0m
[32m[I 2025-02-04 10:56:25,957][0m Trial 10 finished with value: 0.05847686923060172 and parameters: {'observation_period_num': 9, 'train_rates': 0.6580591808240325, 'learning_rate': 0.0004095607435600913, 'batch_size': 251, 'step_size': 8, 'gamma': 0.8832213347644247}. Best is trial 10 with value: 0.05847686923060172.[0m
[32m[I 2025-02-04 10:56:46,093][0m Trial 11 finished with value: 0.060947929503293724 and parameters: {'observation_period_num': 7, 'train_rates': 0.6683390054572375, 'learning_rate': 0.0003891054638639877, 'batch_size': 250, 'step_size': 8, 'gamma': 0.8780698701059314}. Best is trial 10 with value: 0.05847686923060172.[0m
[32m[I 2025-02-04 10:57:06,887][0m Trial 12 finished with value: 0.05117292015118548 and parameters: {'observation_period_num': 8, 'train_rates': 0.6220543762649183, 'learning_rate': 0.000894673312871983, 'batch_size': 254, 'step_size': 7, 'gamma': 0.8821774152082723}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:57:27,575][0m Trial 13 finished with value: 0.1792811365457068 and parameters: {'observation_period_num': 13, 'train_rates': 0.6154511467757631, 'learning_rate': 0.000203288469466931, 'batch_size': 252, 'step_size': 1, 'gamma': 0.8854177695670111}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:57:50,355][0m Trial 14 finished with value: 0.2272390552739582 and parameters: {'observation_period_num': 152, 'train_rates': 0.6215545416903016, 'learning_rate': 0.0008119270704014943, 'batch_size': 209, 'step_size': 7, 'gamma': 0.9883974548322354}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:58:14,905][0m Trial 15 finished with value: 0.06936881815393765 and parameters: {'observation_period_num': 8, 'train_rates': 0.6693279897867036, 'learning_rate': 0.0002182759530536696, 'batch_size': 211, 'step_size': 6, 'gamma': 0.9075757633637681}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:58:37,377][0m Trial 16 finished with value: 0.24722938794966798 and parameters: {'observation_period_num': 211, 'train_rates': 0.6621788366596136, 'learning_rate': 0.0009820952605523042, 'batch_size': 212, 'step_size': 10, 'gamma': 0.8646551894778076}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:59:01,251][0m Trial 17 finished with value: 0.10927191476653748 and parameters: {'observation_period_num': 37, 'train_rates': 0.7022962026617909, 'learning_rate': 9.217372504203157e-05, 'batch_size': 226, 'step_size': 9, 'gamma': 0.9507987555811181}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:59:27,228][0m Trial 18 finished with value: 0.2709422469536463 and parameters: {'observation_period_num': 133, 'train_rates': 0.6029399959502727, 'learning_rate': 0.00026320852042803776, 'batch_size': 177, 'step_size': 2, 'gamma': 0.8529864887708264}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 10:59:59,555][0m Trial 19 finished with value: 0.31062079592795666 and parameters: {'observation_period_num': 105, 'train_rates': 0.6497067796748914, 'learning_rate': 0.00012477986357525264, 'batch_size': 146, 'step_size': 6, 'gamma': 0.9020342691551022}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 11:00:25,205][0m Trial 20 finished with value: 0.0523916712599033 and parameters: {'observation_period_num': 31, 'train_rates': 0.8393974849814789, 'learning_rate': 0.00042042021828699207, 'batch_size': 234, 'step_size': 9, 'gamma': 0.8535902799365139}. Best is trial 12 with value: 0.05117292015118548.[0m
[32m[I 2025-02-04 11:00:50,701][0m Trial 21 finished with value: 0.04864202652781926 and parameters: {'observation_period_num': 33, 'train_rates': 0.8528216003314191, 'learning_rate': 0.0004560467471444617, 'batch_size': 241, 'step_size': 9, 'gamma': 0.8499795548001473}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:01:16,504][0m Trial 22 finished with value: 0.05126951597952124 and parameters: {'observation_period_num': 30, 'train_rates': 0.8419705624496024, 'learning_rate': 0.000998878115226405, 'batch_size': 233, 'step_size': 10, 'gamma': 0.8439356517457308}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:01:48,454][0m Trial 23 finished with value: 0.08801789317083986 and parameters: {'observation_period_num': 64, 'train_rates': 0.8561556465296787, 'learning_rate': 0.0008713450236900384, 'batch_size': 185, 'step_size': 10, 'gamma': 0.8448230631002938}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:02:15,650][0m Trial 24 finished with value: 0.059991084355509504 and parameters: {'observation_period_num': 31, 'train_rates': 0.8917816983104219, 'learning_rate': 0.0004697912651365838, 'batch_size': 231, 'step_size': 13, 'gamma': 0.8653559476877497}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:02:43,456][0m Trial 25 finished with value: 0.6441999209159497 and parameters: {'observation_period_num': 89, 'train_rates': 0.7682651587040453, 'learning_rate': 7.3124996022536405e-06, 'batch_size': 192, 'step_size': 6, 'gamma': 0.9132385434149131}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:03:09,373][0m Trial 26 finished with value: 0.06874539208199297 and parameters: {'observation_period_num': 52, 'train_rates': 0.8078611863168943, 'learning_rate': 0.00018422436799624494, 'batch_size': 233, 'step_size': 10, 'gamma': 0.8403023178919359}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:03:38,147][0m Trial 27 finished with value: 0.06951141357421875 and parameters: {'observation_period_num': 25, 'train_rates': 0.865429728657175, 'learning_rate': 5.2020691093048294e-05, 'batch_size': 204, 'step_size': 7, 'gamma': 0.8949014777607439}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:04:02,735][0m Trial 28 finished with value: 0.11705644428730011 and parameters: {'observation_period_num': 126, 'train_rates': 0.942568483130346, 'learning_rate': 0.0006545812609670364, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8057847186096192}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:04:49,938][0m Trial 29 finished with value: 0.05778758967871623 and parameters: {'observation_period_num': 51, 'train_rates': 0.9234950968179296, 'learning_rate': 0.0009766239956965136, 'batch_size': 125, 'step_size': 11, 'gamma': 0.7819765373768497}. Best is trial 21 with value: 0.04864202652781926.[0m
[32m[I 2025-02-04 11:05:25,310][0m Trial 30 finished with value: 0.04290595244948227 and parameters: {'observation_period_num': 18, 'train_rates': 0.8288035671745032, 'learning_rate': 0.0002802193566731037, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8641166278517134}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:05:51,298][0m Trial 31 finished with value: 0.04989444541444225 and parameters: {'observation_period_num': 23, 'train_rates': 0.8274489149330085, 'learning_rate': 0.00031602446707374197, 'batch_size': 225, 'step_size': 9, 'gamma': 0.8634398753892485}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:06:25,848][0m Trial 32 finished with value: 0.0520313526916353 and parameters: {'observation_period_num': 20, 'train_rates': 0.8122918944174805, 'learning_rate': 0.00030498831918408776, 'batch_size': 161, 'step_size': 9, 'gamma': 0.8745273169751854}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:06:54,808][0m Trial 33 finished with value: 0.07478713071052458 and parameters: {'observation_period_num': 41, 'train_rates': 0.7573654934341078, 'learning_rate': 0.00013386729683566092, 'batch_size': 197, 'step_size': 8, 'gamma': 0.8597038569656562}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:07:32,412][0m Trial 34 finished with value: 0.09569779158133879 and parameters: {'observation_period_num': 80, 'train_rates': 0.8750095013358887, 'learning_rate': 7.641243714374321e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8917801038892603}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:07:59,373][0m Trial 35 finished with value: 0.08217652650358521 and parameters: {'observation_period_num': 54, 'train_rates': 0.7797254300898699, 'learning_rate': 0.0001606360523193027, 'batch_size': 220, 'step_size': 11, 'gamma': 0.8241363417488011}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:08:26,625][0m Trial 36 finished with value: 0.054667431893739314 and parameters: {'observation_period_num': 22, 'train_rates': 0.9175330380037028, 'learning_rate': 0.000327714037627465, 'batch_size': 242, 'step_size': 5, 'gamma': 0.8144243337978645}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:09:07,461][0m Trial 37 finished with value: 0.18122634291648865 and parameters: {'observation_period_num': 244, 'train_rates': 0.9843242101442623, 'learning_rate': 0.0005387391777268054, 'batch_size': 142, 'step_size': 9, 'gamma': 0.9244839837805234}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:09:39,702][0m Trial 38 finished with value: 0.08263162759963624 and parameters: {'observation_period_num': 77, 'train_rates': 0.8267773989312914, 'learning_rate': 4.850475119909453e-05, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8703931486566746}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:10:06,098][0m Trial 39 finished with value: 0.3623804287476973 and parameters: {'observation_period_num': 40, 'train_rates': 0.7934455596985435, 'learning_rate': 1.1529730610570703e-05, 'batch_size': 220, 'step_size': 11, 'gamma': 0.8338848360127382}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:11:12,449][0m Trial 40 finished with value: 0.6523823442794819 and parameters: {'observation_period_num': 5, 'train_rates': 0.7410888478558082, 'learning_rate': 2.477979826608474e-06, 'batch_size': 76, 'step_size': 7, 'gamma': 0.7959354467523065}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:11:37,946][0m Trial 41 finished with value: 0.04856606415630161 and parameters: {'observation_period_num': 22, 'train_rates': 0.8433138378026903, 'learning_rate': 0.0006667399631318095, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8460822318950625}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:12:03,144][0m Trial 42 finished with value: 0.04492985154340779 and parameters: {'observation_period_num': 19, 'train_rates': 0.8531155648697806, 'learning_rate': 0.0006152098413636901, 'batch_size': 241, 'step_size': 8, 'gamma': 0.8513798030035549}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:12:27,935][0m Trial 43 finished with value: 0.04530607776035296 and parameters: {'observation_period_num': 21, 'train_rates': 0.8458294805629664, 'learning_rate': 0.0006132633816095858, 'batch_size': 242, 'step_size': 8, 'gamma': 0.8542663324464034}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:15:09,159][0m Trial 44 finished with value: 0.05814267360213874 and parameters: {'observation_period_num': 45, 'train_rates': 0.9055599947745034, 'learning_rate': 0.0005955485848807249, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8201522056339589}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:15:34,925][0m Trial 45 finished with value: 0.04489652305841446 and parameters: {'observation_period_num': 19, 'train_rates': 0.8559054997663439, 'learning_rate': 0.0006558619739554352, 'batch_size': 238, 'step_size': 10, 'gamma': 0.8336065005607052}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:16:00,155][0m Trial 46 finished with value: 0.06268179862872585 and parameters: {'observation_period_num': 61, 'train_rates': 0.8749725020863541, 'learning_rate': 0.0006495304101766723, 'batch_size': 242, 'step_size': 13, 'gamma': 0.8269567365882717}. Best is trial 30 with value: 0.04290595244948227.[0m
[32m[I 2025-02-04 11:16:49,460][0m Trial 47 finished with value: 0.04257786891970711 and parameters: {'observation_period_num': 16, 'train_rates': 0.809205067797003, 'learning_rate': 0.00023651263870381594, 'batch_size': 113, 'step_size': 10, 'gamma': 0.78655343167034}. Best is trial 47 with value: 0.04257786891970711.[0m
[32m[I 2025-02-04 11:17:34,775][0m Trial 48 finished with value: 0.04505419984510049 and parameters: {'observation_period_num': 15, 'train_rates': 0.7987801162844006, 'learning_rate': 0.00025827870238792904, 'batch_size': 118, 'step_size': 11, 'gamma': 0.7553892309830086}. Best is trial 47 with value: 0.04257786891970711.[0m
[32m[I 2025-02-04 11:18:24,110][0m Trial 49 finished with value: 0.10635446456830892 and parameters: {'observation_period_num': 185, 'train_rates': 0.8075204179094156, 'learning_rate': 0.00022424650016182002, 'batch_size': 104, 'step_size': 12, 'gamma': 0.7573504394873356}. Best is trial 47 with value: 0.04257786891970711.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3425 | 0.2699
Epoch 2/300, Loss: 0.1481 | 0.3234
Epoch 3/300, Loss: 0.1370 | 0.1414
Epoch 4/300, Loss: 0.1241 | 0.1969
Epoch 5/300, Loss: 0.0965 | 0.1098
Epoch 6/300, Loss: 0.1026 | 0.1087
Epoch 7/300, Loss: 0.0968 | 0.0995
Epoch 8/300, Loss: 0.1048 | 0.1017
Epoch 9/300, Loss: 0.1089 | 0.0994
Epoch 10/300, Loss: 0.1070 | 0.0989
Epoch 11/300, Loss: 0.1051 | 0.0837
Epoch 12/300, Loss: 0.0809 | 0.0811
Epoch 13/300, Loss: 0.0722 | 0.0784
Epoch 14/300, Loss: 0.0714 | 0.0784
Epoch 15/300, Loss: 0.0715 | 0.0789
Epoch 16/300, Loss: 0.0705 | 0.0755
Epoch 17/300, Loss: 0.0675 | 0.0714
Epoch 18/300, Loss: 0.0629 | 0.0711
Epoch 19/300, Loss: 0.0623 | 0.0704
Epoch 20/300, Loss: 0.0614 | 0.0709
Epoch 21/300, Loss: 0.0601 | 0.0692
Epoch 22/300, Loss: 0.0589 | 0.0680
Epoch 23/300, Loss: 0.0574 | 0.0677
Epoch 24/300, Loss: 0.0568 | 0.0673
Epoch 25/300, Loss: 0.0562 | 0.0673
Epoch 26/300, Loss: 0.0553 | 0.0660
Epoch 27/300, Loss: 0.0546 | 0.0655
Epoch 28/300, Loss: 0.0536 | 0.0652
Epoch 29/300, Loss: 0.0530 | 0.0647
Epoch 30/300, Loss: 0.0526 | 0.0642
Epoch 31/300, Loss: 0.0521 | 0.0638
Epoch 32/300, Loss: 0.0516 | 0.0638
Epoch 33/300, Loss: 0.0511 | 0.0633
Epoch 34/300, Loss: 0.0508 | 0.0629
Epoch 35/300, Loss: 0.0506 | 0.0625
Epoch 36/300, Loss: 0.0506 | 0.0621
Epoch 37/300, Loss: 0.0508 | 0.0619
Epoch 38/300, Loss: 0.0508 | 0.0615
Epoch 39/300, Loss: 0.0512 | 0.0611
Epoch 40/300, Loss: 0.0517 | 0.0609
Epoch 41/300, Loss: 0.0532 | 0.0603
Epoch 42/300, Loss: 0.0534 | 0.0597
Epoch 43/300, Loss: 0.0516 | 0.0597
Epoch 44/300, Loss: 0.0497 | 0.0597
Epoch 45/300, Loss: 0.0484 | 0.0599
Epoch 46/300, Loss: 0.0475 | 0.0606
Epoch 47/300, Loss: 0.0472 | 0.0607
Epoch 48/300, Loss: 0.0469 | 0.0606
Epoch 49/300, Loss: 0.0465 | 0.0606
Epoch 50/300, Loss: 0.0460 | 0.0603
Epoch 51/300, Loss: 0.0456 | 0.0601
Epoch 52/300, Loss: 0.0452 | 0.0599
Epoch 53/300, Loss: 0.0450 | 0.0597
Epoch 54/300, Loss: 0.0447 | 0.0595
Epoch 55/300, Loss: 0.0445 | 0.0593
Epoch 56/300, Loss: 0.0444 | 0.0591
Epoch 57/300, Loss: 0.0442 | 0.0590
Epoch 58/300, Loss: 0.0440 | 0.0589
Epoch 59/300, Loss: 0.0439 | 0.0587
Epoch 60/300, Loss: 0.0437 | 0.0585
Epoch 61/300, Loss: 0.0436 | 0.0583
Epoch 62/300, Loss: 0.0434 | 0.0582
Epoch 63/300, Loss: 0.0433 | 0.0581
Epoch 64/300, Loss: 0.0432 | 0.0580
Epoch 65/300, Loss: 0.0431 | 0.0579
Epoch 66/300, Loss: 0.0429 | 0.0577
Epoch 67/300, Loss: 0.0428 | 0.0576
Epoch 68/300, Loss: 0.0427 | 0.0575
Epoch 69/300, Loss: 0.0426 | 0.0574
Epoch 70/300, Loss: 0.0425 | 0.0573
Epoch 71/300, Loss: 0.0424 | 0.0572
Epoch 72/300, Loss: 0.0424 | 0.0571
Epoch 73/300, Loss: 0.0423 | 0.0570
Epoch 74/300, Loss: 0.0422 | 0.0569
Epoch 75/300, Loss: 0.0421 | 0.0568
Epoch 76/300, Loss: 0.0421 | 0.0567
Epoch 77/300, Loss: 0.0420 | 0.0567
Epoch 78/300, Loss: 0.0419 | 0.0566
Epoch 79/300, Loss: 0.0419 | 0.0565
Epoch 80/300, Loss: 0.0418 | 0.0565
Epoch 81/300, Loss: 0.0417 | 0.0564
Epoch 82/300, Loss: 0.0417 | 0.0564
Epoch 83/300, Loss: 0.0416 | 0.0563
Epoch 84/300, Loss: 0.0416 | 0.0562
Epoch 85/300, Loss: 0.0416 | 0.0562
Epoch 86/300, Loss: 0.0415 | 0.0561
Epoch 87/300, Loss: 0.0415 | 0.0561
Epoch 88/300, Loss: 0.0414 | 0.0561
Epoch 89/300, Loss: 0.0414 | 0.0560
Epoch 90/300, Loss: 0.0414 | 0.0560
Epoch 91/300, Loss: 0.0413 | 0.0559
Epoch 92/300, Loss: 0.0413 | 0.0559
Epoch 93/300, Loss: 0.0413 | 0.0559
Epoch 94/300, Loss: 0.0412 | 0.0558
Epoch 95/300, Loss: 0.0412 | 0.0558
Epoch 96/300, Loss: 0.0412 | 0.0558
Epoch 97/300, Loss: 0.0411 | 0.0557
Epoch 98/300, Loss: 0.0411 | 0.0557
Epoch 99/300, Loss: 0.0411 | 0.0557
Epoch 100/300, Loss: 0.0411 | 0.0556
Epoch 101/300, Loss: 0.0410 | 0.0556
Epoch 102/300, Loss: 0.0410 | 0.0556
Epoch 103/300, Loss: 0.0410 | 0.0556
Epoch 104/300, Loss: 0.0410 | 0.0556
Epoch 105/300, Loss: 0.0410 | 0.0555
Epoch 106/300, Loss: 0.0409 | 0.0555
Epoch 107/300, Loss: 0.0409 | 0.0555
Epoch 108/300, Loss: 0.0409 | 0.0555
Epoch 109/300, Loss: 0.0409 | 0.0555
Epoch 110/300, Loss: 0.0409 | 0.0554
Epoch 111/300, Loss: 0.0409 | 0.0554
Epoch 112/300, Loss: 0.0409 | 0.0554
Epoch 113/300, Loss: 0.0408 | 0.0554
Epoch 114/300, Loss: 0.0408 | 0.0554
Epoch 115/300, Loss: 0.0408 | 0.0554
Epoch 116/300, Loss: 0.0408 | 0.0554
Epoch 117/300, Loss: 0.0408 | 0.0553
Epoch 118/300, Loss: 0.0408 | 0.0553
Epoch 119/300, Loss: 0.0408 | 0.0553
Epoch 120/300, Loss: 0.0408 | 0.0553
Epoch 121/300, Loss: 0.0408 | 0.0553
Epoch 122/300, Loss: 0.0407 | 0.0553
Epoch 123/300, Loss: 0.0407 | 0.0553
Epoch 124/300, Loss: 0.0407 | 0.0553
Epoch 125/300, Loss: 0.0407 | 0.0553
Epoch 126/300, Loss: 0.0407 | 0.0553
Epoch 127/300, Loss: 0.0407 | 0.0552
Epoch 128/300, Loss: 0.0407 | 0.0552
Epoch 129/300, Loss: 0.0407 | 0.0552
Epoch 130/300, Loss: 0.0407 | 0.0552
Epoch 131/300, Loss: 0.0407 | 0.0552
Epoch 132/300, Loss: 0.0407 | 0.0552
Epoch 133/300, Loss: 0.0407 | 0.0552
Epoch 134/300, Loss: 0.0407 | 0.0552
Epoch 135/300, Loss: 0.0407 | 0.0552
Epoch 136/300, Loss: 0.0407 | 0.0552
Epoch 137/300, Loss: 0.0407 | 0.0552
Epoch 138/300, Loss: 0.0407 | 0.0552
Epoch 139/300, Loss: 0.0406 | 0.0552
Epoch 140/300, Loss: 0.0406 | 0.0552
Epoch 141/300, Loss: 0.0406 | 0.0552
Epoch 142/300, Loss: 0.0406 | 0.0552
Epoch 143/300, Loss: 0.0406 | 0.0552
Epoch 144/300, Loss: 0.0406 | 0.0552
Epoch 145/300, Loss: 0.0406 | 0.0551
Epoch 146/300, Loss: 0.0406 | 0.0551
Epoch 147/300, Loss: 0.0406 | 0.0551
Epoch 148/300, Loss: 0.0406 | 0.0551
Epoch 149/300, Loss: 0.0406 | 0.0551
Epoch 150/300, Loss: 0.0406 | 0.0551
Epoch 151/300, Loss: 0.0406 | 0.0551
Epoch 152/300, Loss: 0.0406 | 0.0551
Epoch 153/300, Loss: 0.0406 | 0.0551
Epoch 154/300, Loss: 0.0406 | 0.0551
Epoch 155/300, Loss: 0.0406 | 0.0551
Epoch 156/300, Loss: 0.0406 | 0.0551
Epoch 157/300, Loss: 0.0406 | 0.0551
Epoch 158/300, Loss: 0.0406 | 0.0551
Epoch 159/300, Loss: 0.0406 | 0.0551
Epoch 160/300, Loss: 0.0406 | 0.0551
Epoch 161/300, Loss: 0.0406 | 0.0551
Epoch 162/300, Loss: 0.0406 | 0.0551
Epoch 163/300, Loss: 0.0406 | 0.0551
Epoch 164/300, Loss: 0.0406 | 0.0551
Epoch 165/300, Loss: 0.0406 | 0.0551
Epoch 166/300, Loss: 0.0406 | 0.0551
Epoch 167/300, Loss: 0.0406 | 0.0551
Epoch 168/300, Loss: 0.0406 | 0.0551
Epoch 169/300, Loss: 0.0406 | 0.0551
Epoch 170/300, Loss: 0.0406 | 0.0551
Epoch 171/300, Loss: 0.0406 | 0.0551
Epoch 172/300, Loss: 0.0406 | 0.0551
Epoch 173/300, Loss: 0.0406 | 0.0551
Epoch 174/300, Loss: 0.0406 | 0.0551
Epoch 175/300, Loss: 0.0406 | 0.0551
Epoch 176/300, Loss: 0.0406 | 0.0551
Epoch 177/300, Loss: 0.0406 | 0.0551
Epoch 178/300, Loss: 0.0406 | 0.0551
Epoch 179/300, Loss: 0.0406 | 0.0551
Epoch 180/300, Loss: 0.0406 | 0.0551
Epoch 181/300, Loss: 0.0406 | 0.0551
Epoch 182/300, Loss: 0.0406 | 0.0551
Epoch 183/300, Loss: 0.0406 | 0.0551
Epoch 184/300, Loss: 0.0406 | 0.0551
Epoch 185/300, Loss: 0.0406 | 0.0551
Epoch 186/300, Loss: 0.0406 | 0.0551
Epoch 187/300, Loss: 0.0406 | 0.0551
Epoch 188/300, Loss: 0.0406 | 0.0551
Epoch 189/300, Loss: 0.0406 | 0.0551
Epoch 190/300, Loss: 0.0406 | 0.0551
Epoch 191/300, Loss: 0.0406 | 0.0551
Epoch 192/300, Loss: 0.0406 | 0.0551
Epoch 193/300, Loss: 0.0406 | 0.0551
Epoch 194/300, Loss: 0.0406 | 0.0551
Epoch 195/300, Loss: 0.0406 | 0.0551
Epoch 196/300, Loss: 0.0406 | 0.0551
Epoch 197/300, Loss: 0.0406 | 0.0551
Epoch 198/300, Loss: 0.0406 | 0.0551
Epoch 199/300, Loss: 0.0406 | 0.0551
Epoch 200/300, Loss: 0.0406 | 0.0551
Epoch 201/300, Loss: 0.0406 | 0.0551
Epoch 202/300, Loss: 0.0406 | 0.0551
Epoch 203/300, Loss: 0.0406 | 0.0551
Epoch 204/300, Loss: 0.0406 | 0.0551
Epoch 205/300, Loss: 0.0406 | 0.0551
Epoch 206/300, Loss: 0.0406 | 0.0551
Epoch 207/300, Loss: 0.0406 | 0.0551
Epoch 208/300, Loss: 0.0406 | 0.0551
Epoch 209/300, Loss: 0.0406 | 0.0551
Epoch 210/300, Loss: 0.0406 | 0.0551
Epoch 211/300, Loss: 0.0406 | 0.0551
Epoch 212/300, Loss: 0.0406 | 0.0551
Epoch 213/300, Loss: 0.0406 | 0.0551
Epoch 214/300, Loss: 0.0406 | 0.0551
Epoch 215/300, Loss: 0.0406 | 0.0551
Epoch 216/300, Loss: 0.0406 | 0.0551
Epoch 217/300, Loss: 0.0406 | 0.0551
Epoch 218/300, Loss: 0.0406 | 0.0551
Epoch 219/300, Loss: 0.0406 | 0.0551
Epoch 220/300, Loss: 0.0406 | 0.0551
Epoch 221/300, Loss: 0.0406 | 0.0551
Epoch 222/300, Loss: 0.0406 | 0.0551
Epoch 223/300, Loss: 0.0406 | 0.0551
Epoch 224/300, Loss: 0.0406 | 0.0551
Epoch 225/300, Loss: 0.0406 | 0.0551
Epoch 226/300, Loss: 0.0406 | 0.0551
Epoch 227/300, Loss: 0.0406 | 0.0551
Epoch 228/300, Loss: 0.0406 | 0.0551
Epoch 229/300, Loss: 0.0406 | 0.0551
Epoch 230/300, Loss: 0.0406 | 0.0551
Epoch 231/300, Loss: 0.0406 | 0.0551
Epoch 232/300, Loss: 0.0406 | 0.0551
Epoch 233/300, Loss: 0.0406 | 0.0551
Epoch 234/300, Loss: 0.0406 | 0.0551
Epoch 235/300, Loss: 0.0406 | 0.0551
Epoch 236/300, Loss: 0.0406 | 0.0551
Epoch 237/300, Loss: 0.0406 | 0.0551
Epoch 238/300, Loss: 0.0406 | 0.0551
Epoch 239/300, Loss: 0.0406 | 0.0551
Epoch 240/300, Loss: 0.0406 | 0.0551
Epoch 241/300, Loss: 0.0406 | 0.0551
Epoch 242/300, Loss: 0.0406 | 0.0551
Epoch 243/300, Loss: 0.0406 | 0.0551
Epoch 244/300, Loss: 0.0406 | 0.0551
Epoch 245/300, Loss: 0.0406 | 0.0551
Epoch 246/300, Loss: 0.0406 | 0.0551
Epoch 247/300, Loss: 0.0406 | 0.0551
Epoch 248/300, Loss: 0.0406 | 0.0551
Epoch 249/300, Loss: 0.0406 | 0.0551
Epoch 250/300, Loss: 0.0406 | 0.0551
Epoch 251/300, Loss: 0.0406 | 0.0551
Epoch 252/300, Loss: 0.0406 | 0.0551
Epoch 253/300, Loss: 0.0406 | 0.0551
Epoch 254/300, Loss: 0.0406 | 0.0551
Epoch 255/300, Loss: 0.0406 | 0.0551
Epoch 256/300, Loss: 0.0406 | 0.0551
Epoch 257/300, Loss: 0.0406 | 0.0551
Epoch 258/300, Loss: 0.0406 | 0.0551
Epoch 259/300, Loss: 0.0406 | 0.0551
Epoch 260/300, Loss: 0.0406 | 0.0551
Epoch 261/300, Loss: 0.0406 | 0.0551
Epoch 262/300, Loss: 0.0406 | 0.0551
Epoch 263/300, Loss: 0.0406 | 0.0551
Epoch 264/300, Loss: 0.0406 | 0.0551
Epoch 265/300, Loss: 0.0406 | 0.0551
Epoch 266/300, Loss: 0.0406 | 0.0551
Epoch 267/300, Loss: 0.0406 | 0.0551
Epoch 268/300, Loss: 0.0406 | 0.0551
Epoch 269/300, Loss: 0.0406 | 0.0551
Epoch 270/300, Loss: 0.0406 | 0.0551
Epoch 271/300, Loss: 0.0406 | 0.0551
Epoch 272/300, Loss: 0.0406 | 0.0551
Epoch 273/300, Loss: 0.0406 | 0.0551
Epoch 274/300, Loss: 0.0406 | 0.0551
Epoch 275/300, Loss: 0.0406 | 0.0551
Epoch 276/300, Loss: 0.0406 | 0.0551
Epoch 277/300, Loss: 0.0406 | 0.0551
Epoch 278/300, Loss: 0.0406 | 0.0551
Epoch 279/300, Loss: 0.0406 | 0.0551
Epoch 280/300, Loss: 0.0406 | 0.0551
Epoch 281/300, Loss: 0.0406 | 0.0551
Early stopping
Runtime (seconds): 137.17861557006836
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 20.714540476968978
RMSE: 4.551322937011719
MAE: 4.551322937011719
R-squared: nan
[113.021324]
