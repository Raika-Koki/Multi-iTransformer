[32m[I 2025-02-08 21:22:44,065][0m A new study created in memory with name: no-name-ae75cee6-163b-441b-b526-6476beada956[0m
[32m[I 2025-02-08 21:23:31,442][0m Trial 0 finished with value: 0.4658837101157996 and parameters: {'observation_period_num': 87, 'train_rates': 0.6772226007299038, 'learning_rate': 4.873912961035396e-06, 'batch_size': 109, 'step_size': 13, 'gamma': 0.9220015570124434}. Best is trial 0 with value: 0.4658837101157996.[0m
[32m[I 2025-02-08 21:24:35,504][0m Trial 1 finished with value: 0.281644599330849 and parameters: {'observation_period_num': 79, 'train_rates': 0.778629334268063, 'learning_rate': 0.00012871358742585633, 'batch_size': 84, 'step_size': 2, 'gamma': 0.768054670742166}. Best is trial 1 with value: 0.281644599330849.[0m
[32m[I 2025-02-08 21:25:06,034][0m Trial 2 finished with value: 0.3226869646355057 and parameters: {'observation_period_num': 225, 'train_rates': 0.679782387454243, 'learning_rate': 0.00021686480172423368, 'batch_size': 166, 'step_size': 14, 'gamma': 0.8491263313130307}. Best is trial 1 with value: 0.281644599330849.[0m
[32m[I 2025-02-08 21:25:54,974][0m Trial 3 finished with value: 0.6009462605327812 and parameters: {'observation_period_num': 118, 'train_rates': 0.6281779171595068, 'learning_rate': 4.747385956898466e-06, 'batch_size': 91, 'step_size': 6, 'gamma': 0.8989065400922669}. Best is trial 1 with value: 0.281644599330849.[0m
[32m[I 2025-02-08 21:30:56,048][0m Trial 4 finished with value: 0.18784471152783583 and parameters: {'observation_period_num': 23, 'train_rates': 0.7201707276979779, 'learning_rate': 2.6202742022014934e-06, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9596183230611307}. Best is trial 4 with value: 0.18784471152783583.[0m
[32m[I 2025-02-08 21:31:24,560][0m Trial 5 finished with value: 0.1949696420126096 and parameters: {'observation_period_num': 113, 'train_rates': 0.7796462906101838, 'learning_rate': 0.0001735513777077618, 'batch_size': 200, 'step_size': 12, 'gamma': 0.925052037520875}. Best is trial 4 with value: 0.18784471152783583.[0m
[32m[I 2025-02-08 21:31:54,369][0m Trial 6 finished with value: 0.10163849923345777 and parameters: {'observation_period_num': 95, 'train_rates': 0.8637861555897912, 'learning_rate': 0.00031614141709118844, 'batch_size': 208, 'step_size': 6, 'gamma': 0.7926512190253595}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:32:19,557][0m Trial 7 finished with value: 0.2695783034872078 and parameters: {'observation_period_num': 74, 'train_rates': 0.6850114303429342, 'learning_rate': 3.793214152164509e-05, 'batch_size': 218, 'step_size': 11, 'gamma': 0.9348118999528717}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:32:57,099][0m Trial 8 finished with value: 0.19757144153118134 and parameters: {'observation_period_num': 214, 'train_rates': 0.9500744290990111, 'learning_rate': 0.0004970455870422009, 'batch_size': 160, 'step_size': 9, 'gamma': 0.8230344295813121}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:33:20,043][0m Trial 9 finished with value: 0.7261702875785639 and parameters: {'observation_period_num': 222, 'train_rates': 0.6347508040804092, 'learning_rate': 2.0676516317660173e-06, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8794304357242698}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:33:45,825][0m Trial 10 finished with value: 0.17560705542564392 and parameters: {'observation_period_num': 158, 'train_rates': 0.9227065714161768, 'learning_rate': 0.000796347856914056, 'batch_size': 245, 'step_size': 5, 'gamma': 0.7611710678048076}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:34:13,489][0m Trial 11 finished with value: 0.1717066390841615 and parameters: {'observation_period_num': 147, 'train_rates': 0.91086283530974, 'learning_rate': 0.0009652339849825825, 'batch_size': 253, 'step_size': 5, 'gamma': 0.752273236650993}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:34:39,425][0m Trial 12 finished with value: 0.1605822305943145 and parameters: {'observation_period_num': 168, 'train_rates': 0.8710880080840889, 'learning_rate': 0.0009559978119425199, 'batch_size': 246, 'step_size': 6, 'gamma': 0.8032337174692503}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:35:11,776][0m Trial 13 finished with value: 0.5645794365904606 and parameters: {'observation_period_num': 189, 'train_rates': 0.867442846414654, 'learning_rate': 4.2033121180179635e-05, 'batch_size': 179, 'step_size': 2, 'gamma': 0.8076113706402999}. Best is trial 6 with value: 0.10163849923345777.[0m
[32m[I 2025-02-08 21:35:38,054][0m Trial 14 finished with value: 0.049069371581430266 and parameters: {'observation_period_num': 20, 'train_rates': 0.8300501413234105, 'learning_rate': 0.00036337085270927766, 'batch_size': 231, 'step_size': 7, 'gamma': 0.8064750728584927}. Best is trial 14 with value: 0.049069371581430266.[0m
[32m[I 2025-02-08 21:36:23,138][0m Trial 15 finished with value: 0.05903085756340418 and parameters: {'observation_period_num': 21, 'train_rates': 0.8442856880216173, 'learning_rate': 8.180470103967323e-05, 'batch_size': 129, 'step_size': 8, 'gamma': 0.843418217503864}. Best is trial 14 with value: 0.049069371581430266.[0m
[32m[I 2025-02-08 21:37:07,906][0m Trial 16 finished with value: 0.04827473189174776 and parameters: {'observation_period_num': 10, 'train_rates': 0.830764993827936, 'learning_rate': 8.091950059914904e-05, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8474680134411515}. Best is trial 16 with value: 0.04827473189174776.[0m
[32m[I 2025-02-08 21:38:45,678][0m Trial 17 finished with value: 0.10447001774896855 and parameters: {'observation_period_num': 48, 'train_rates': 0.8231642395395926, 'learning_rate': 1.2017199285799882e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.8604004309693969}. Best is trial 16 with value: 0.04827473189174776.[0m
[32m[I 2025-02-08 21:39:23,388][0m Trial 18 finished with value: 0.19756204148656445 and parameters: {'observation_period_num': 12, 'train_rates': 0.7459096700940588, 'learning_rate': 1.7375055287222475e-05, 'batch_size': 144, 'step_size': 15, 'gamma': 0.834732031611012}. Best is trial 16 with value: 0.04827473189174776.[0m
[32m[I 2025-02-08 21:40:07,216][0m Trial 19 finished with value: 0.0888161785570921 and parameters: {'observation_period_num': 49, 'train_rates': 0.8086906537134392, 'learning_rate': 7.341703986224203e-05, 'batch_size': 127, 'step_size': 4, 'gamma': 0.8759703476486791}. Best is trial 16 with value: 0.04827473189174776.[0m
[32m[I 2025-02-08 21:41:32,761][0m Trial 20 finished with value: 0.07704054564237595 and parameters: {'observation_period_num': 46, 'train_rates': 0.985725356926182, 'learning_rate': 0.0003893544374895419, 'batch_size': 73, 'step_size': 9, 'gamma': 0.7839339221483679}. Best is trial 16 with value: 0.04827473189174776.[0m
[32m[I 2025-02-08 21:42:18,723][0m Trial 21 finished with value: 0.046055310023571716 and parameters: {'observation_period_num': 5, 'train_rates': 0.8268940252847409, 'learning_rate': 8.27341150572082e-05, 'batch_size': 124, 'step_size': 8, 'gamma': 0.8419497023815223}. Best is trial 21 with value: 0.046055310023571716.[0m
[32m[I 2025-02-08 21:43:14,194][0m Trial 22 finished with value: 0.048932310438058416 and parameters: {'observation_period_num': 9, 'train_rates': 0.8982862764117714, 'learning_rate': 0.00010065163539725662, 'batch_size': 110, 'step_size': 7, 'gamma': 0.8226433016642346}. Best is trial 21 with value: 0.046055310023571716.[0m
[32m[I 2025-02-08 21:44:11,562][0m Trial 23 finished with value: 0.0428013415243782 and parameters: {'observation_period_num': 8, 'train_rates': 0.898288510250112, 'learning_rate': 7.760446403865652e-05, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8241803939625697}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:45:53,512][0m Trial 24 finished with value: 0.08804830099995199 and parameters: {'observation_period_num': 42, 'train_rates': 0.9487960030985105, 'learning_rate': 2.1408018811244306e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8949672291797145}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:46:44,274][0m Trial 25 finished with value: 0.2084210159168866 and parameters: {'observation_period_num': 65, 'train_rates': 0.7625486449083377, 'learning_rate': 5.324375975245129e-05, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8617328109262405}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:47:24,180][0m Trial 26 finished with value: 0.06461287511496985 and parameters: {'observation_period_num': 5, 'train_rates': 0.8016491650353014, 'learning_rate': 2.8726017390974785e-05, 'batch_size': 148, 'step_size': 11, 'gamma': 0.8304179845164986}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:47:59,212][0m Trial 27 finished with value: 0.40498434397947547 and parameters: {'observation_period_num': 32, 'train_rates': 0.8831755079950476, 'learning_rate': 1.1373368391169472e-05, 'batch_size': 182, 'step_size': 4, 'gamma': 0.8938275937481087}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:50:24,521][0m Trial 28 finished with value: 0.7329426212977338 and parameters: {'observation_period_num': 58, 'train_rates': 0.8420368189077185, 'learning_rate': 1.0465811393448484e-06, 'batch_size': 37, 'step_size': 7, 'gamma': 0.7797773220894448}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:51:18,773][0m Trial 29 finished with value: 0.0685334098866073 and parameters: {'observation_period_num': 31, 'train_rates': 0.9449079287996677, 'learning_rate': 5.9171171705015095e-05, 'batch_size': 113, 'step_size': 12, 'gamma': 0.8502095857805224}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:52:08,615][0m Trial 30 finished with value: 0.46353791770525277 and parameters: {'observation_period_num': 251, 'train_rates': 0.7440744423978071, 'learning_rate': 0.00014089096237119263, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9873782721706499}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:52:58,979][0m Trial 31 finished with value: 0.049889034006125765 and parameters: {'observation_period_num': 12, 'train_rates': 0.8972677891049285, 'learning_rate': 9.9605750433917e-05, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8193527984965568}. Best is trial 23 with value: 0.0428013415243782.[0m
[32m[I 2025-02-08 21:54:16,252][0m Trial 32 finished with value: 0.041170657176821855 and parameters: {'observation_period_num': 5, 'train_rates': 0.9123240435762383, 'learning_rate': 0.00010977776941552497, 'batch_size': 78, 'step_size': 8, 'gamma': 0.8177502520245776}. Best is trial 32 with value: 0.041170657176821855.[0m
[32m[I 2025-02-08 21:55:36,572][0m Trial 33 finished with value: 0.15949106216430664 and parameters: {'observation_period_num': 88, 'train_rates': 0.9862597790425589, 'learning_rate': 0.00023554514755035213, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8377965331938895}. Best is trial 32 with value: 0.041170657176821855.[0m
[32m[I 2025-02-08 21:56:41,484][0m Trial 34 finished with value: 0.04968315133410042 and parameters: {'observation_period_num': 32, 'train_rates': 0.9289960655953771, 'learning_rate': 0.0001474673669105319, 'batch_size': 95, 'step_size': 8, 'gamma': 0.8542471656083306}. Best is trial 32 with value: 0.041170657176821855.[0m
[32m[I 2025-02-08 21:57:23,088][0m Trial 35 finished with value: 0.03782931904782329 and parameters: {'observation_period_num': 7, 'train_rates': 0.7907321135515284, 'learning_rate': 0.00022273904345671477, 'batch_size': 139, 'step_size': 10, 'gamma': 0.8700446467699646}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 21:58:03,689][0m Trial 36 finished with value: 0.06447247554248266 and parameters: {'observation_period_num': 34, 'train_rates': 0.7833222526609664, 'learning_rate': 0.00021081381133905607, 'batch_size': 145, 'step_size': 10, 'gamma': 0.8717975005132826}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 21:59:26,649][0m Trial 37 finished with value: 0.13642547434637312 and parameters: {'observation_period_num': 5, 'train_rates': 0.7112112202688721, 'learning_rate': 0.00011108540517825841, 'batch_size': 60, 'step_size': 12, 'gamma': 0.8140728044544262}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:00:32,426][0m Trial 38 finished with value: 0.1380485207197212 and parameters: {'observation_period_num': 60, 'train_rates': 0.8572518426192666, 'learning_rate': 0.000238959331451377, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9107127664438545}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:01:07,121][0m Trial 39 finished with value: 0.1994354562434767 and parameters: {'observation_period_num': 110, 'train_rates': 0.8062449535686071, 'learning_rate': 2.9345714376618784e-05, 'batch_size': 157, 'step_size': 13, 'gamma': 0.7934717156606447}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:01:44,224][0m Trial 40 finished with value: 0.11373674124479294 and parameters: {'observation_period_num': 74, 'train_rates': 0.9630063586579563, 'learning_rate': 0.0005971311950606578, 'batch_size': 175, 'step_size': 11, 'gamma': 0.8846616588494146}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:02:29,046][0m Trial 41 finished with value: 0.05711297578894334 and parameters: {'observation_period_num': 24, 'train_rates': 0.8839799697049737, 'learning_rate': 6.081816267876311e-05, 'batch_size': 134, 'step_size': 9, 'gamma': 0.8431446286674024}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:03:16,406][0m Trial 42 finished with value: 0.046918915475116056 and parameters: {'observation_period_num': 20, 'train_rates': 0.828905337286274, 'learning_rate': 0.000146780114619144, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8644424253179803}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:04:21,960][0m Trial 43 finished with value: 0.1680036320277814 and parameters: {'observation_period_num': 20, 'train_rates': 0.7611161900642589, 'learning_rate': 0.0001765389099105864, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8651503297626197}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:05:17,690][0m Trial 44 finished with value: 0.046648309170342864 and parameters: {'observation_period_num': 34, 'train_rates': 0.7891083917653693, 'learning_rate': 0.00028251805147453286, 'batch_size': 101, 'step_size': 6, 'gamma': 0.825877320638259}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:06:02,371][0m Trial 45 finished with value: 0.1506236741375258 and parameters: {'observation_period_num': 40, 'train_rates': 0.6065643893309922, 'learning_rate': 0.00029742495529533764, 'batch_size': 103, 'step_size': 6, 'gamma': 0.830920658094478}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:08:34,356][0m Trial 46 finished with value: 0.08620511379522326 and parameters: {'observation_period_num': 54, 'train_rates': 0.7821625041558216, 'learning_rate': 0.0005366448693962287, 'batch_size': 34, 'step_size': 5, 'gamma': 0.7922938931678627}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:09:44,391][0m Trial 47 finished with value: 0.28148403769262886 and parameters: {'observation_period_num': 101, 'train_rates': 0.7193802898264425, 'learning_rate': 4.5249098452918046e-05, 'batch_size': 70, 'step_size': 6, 'gamma': 0.7738103178097222}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:10:50,342][0m Trial 48 finished with value: 0.05233799188777252 and parameters: {'observation_period_num': 25, 'train_rates': 0.9095452966832899, 'learning_rate': 0.0002986486632483044, 'batch_size': 91, 'step_size': 3, 'gamma': 0.8025135915772414}. Best is trial 35 with value: 0.03782931904782329.[0m
[32m[I 2025-02-08 22:11:41,638][0m Trial 49 finished with value: 0.06160588500840726 and parameters: {'observation_period_num': 69, 'train_rates': 0.8558746395936354, 'learning_rate': 0.0001776103207227911, 'batch_size': 116, 'step_size': 7, 'gamma': 0.8253843008013334}. Best is trial 35 with value: 0.03782931904782329.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2825 | 0.2343
Epoch 2/300, Loss: 0.1712 | 0.1615
Epoch 3/300, Loss: 0.1579 | 0.1495
Epoch 4/300, Loss: 0.1775 | 0.1252
Epoch 5/300, Loss: 0.1426 | 0.1377
Epoch 6/300, Loss: 0.1718 | 0.1528
Epoch 7/300, Loss: 0.1415 | 0.0981
Epoch 8/300, Loss: 0.1320 | 0.0871
Epoch 9/300, Loss: 0.1233 | 0.0832
Epoch 10/300, Loss: 0.1181 | 0.0770
Epoch 11/300, Loss: 0.1147 | 0.0742
Epoch 12/300, Loss: 0.1112 | 0.0704
Epoch 13/300, Loss: 0.1104 | 0.0694
Epoch 14/300, Loss: 0.1079 | 0.0705
Epoch 15/300, Loss: 0.1073 | 0.0722
Epoch 16/300, Loss: 0.1066 | 0.0692
Epoch 17/300, Loss: 0.1061 | 0.0660
Epoch 18/300, Loss: 0.1051 | 0.0645
Epoch 19/300, Loss: 0.1037 | 0.0635
Epoch 20/300, Loss: 0.1024 | 0.0627
Epoch 21/300, Loss: 0.1019 | 0.0626
Epoch 22/300, Loss: 0.1029 | 0.0622
Epoch 23/300, Loss: 0.1051 | 0.0623
Epoch 24/300, Loss: 0.1054 | 0.0619
Epoch 25/300, Loss: 0.1048 | 0.0610
Epoch 26/300, Loss: 0.1057 | 0.0635
Epoch 27/300, Loss: 0.1087 | 0.0630
Epoch 28/300, Loss: 0.1088 | 0.0600
Epoch 29/300, Loss: 0.1116 | 0.0638
Epoch 30/300, Loss: 0.1157 | 0.0694
Epoch 31/300, Loss: 0.1236 | 0.0811
Epoch 32/300, Loss: 0.1084 | 0.0701
Epoch 33/300, Loss: 0.1017 | 0.0658
Epoch 34/300, Loss: 0.1053 | 0.0637
Epoch 35/300, Loss: 0.0985 | 0.0548
Epoch 36/300, Loss: 0.0975 | 0.0607
Epoch 37/300, Loss: 0.0936 | 0.0547
Epoch 38/300, Loss: 0.0905 | 0.0535
Epoch 39/300, Loss: 0.0911 | 0.0517
Epoch 40/300, Loss: 0.0894 | 0.0513
Epoch 41/300, Loss: 0.0895 | 0.0516
Epoch 42/300, Loss: 0.0880 | 0.0508
Epoch 43/300, Loss: 0.0877 | 0.0506
Epoch 44/300, Loss: 0.0878 | 0.0498
Epoch 45/300, Loss: 0.0870 | 0.0492
Epoch 46/300, Loss: 0.0867 | 0.0489
Epoch 47/300, Loss: 0.0861 | 0.0490
Epoch 48/300, Loss: 0.0859 | 0.0491
Epoch 49/300, Loss: 0.0860 | 0.0486
Epoch 50/300, Loss: 0.0855 | 0.0480
Epoch 51/300, Loss: 0.0851 | 0.0474
Epoch 52/300, Loss: 0.0847 | 0.0473
Epoch 53/300, Loss: 0.0844 | 0.0473
Epoch 54/300, Loss: 0.0841 | 0.0472
Epoch 55/300, Loss: 0.0839 | 0.0470
Epoch 56/300, Loss: 0.0836 | 0.0466
Epoch 57/300, Loss: 0.0834 | 0.0463
Epoch 58/300, Loss: 0.0831 | 0.0460
Epoch 59/300, Loss: 0.0829 | 0.0459
Epoch 60/300, Loss: 0.0827 | 0.0459
Epoch 61/300, Loss: 0.0824 | 0.0459
Epoch 62/300, Loss: 0.0823 | 0.0459
Epoch 63/300, Loss: 0.0822 | 0.0457
Epoch 64/300, Loss: 0.0820 | 0.0456
Epoch 65/300, Loss: 0.0818 | 0.0454
Epoch 66/300, Loss: 0.0816 | 0.0452
Epoch 67/300, Loss: 0.0814 | 0.0451
Epoch 68/300, Loss: 0.0813 | 0.0450
Epoch 69/300, Loss: 0.0811 | 0.0450
Epoch 70/300, Loss: 0.0810 | 0.0449
Epoch 71/300, Loss: 0.0808 | 0.0449
Epoch 72/300, Loss: 0.0807 | 0.0449
Epoch 73/300, Loss: 0.0805 | 0.0448
Epoch 74/300, Loss: 0.0804 | 0.0447
Epoch 75/300, Loss: 0.0803 | 0.0445
Epoch 76/300, Loss: 0.0801 | 0.0444
Epoch 77/300, Loss: 0.0800 | 0.0443
Epoch 78/300, Loss: 0.0798 | 0.0441
Epoch 79/300, Loss: 0.0797 | 0.0440
Epoch 80/300, Loss: 0.0795 | 0.0438
Epoch 81/300, Loss: 0.0794 | 0.0436
Epoch 82/300, Loss: 0.0793 | 0.0435
Epoch 83/300, Loss: 0.0792 | 0.0433
Epoch 84/300, Loss: 0.0790 | 0.0432
Epoch 85/300, Loss: 0.0789 | 0.0430
Epoch 86/300, Loss: 0.0787 | 0.0429
Epoch 87/300, Loss: 0.0786 | 0.0428
Epoch 88/300, Loss: 0.0785 | 0.0426
Epoch 89/300, Loss: 0.0784 | 0.0425
Epoch 90/300, Loss: 0.0783 | 0.0423
Epoch 91/300, Loss: 0.0781 | 0.0421
Epoch 92/300, Loss: 0.0780 | 0.0419
Epoch 93/300, Loss: 0.0779 | 0.0417
Epoch 94/300, Loss: 0.0778 | 0.0415
Epoch 95/300, Loss: 0.0778 | 0.0414
Epoch 96/300, Loss: 0.0777 | 0.0413
Epoch 97/300, Loss: 0.0776 | 0.0413
Epoch 98/300, Loss: 0.0774 | 0.0413
Epoch 99/300, Loss: 0.0774 | 0.0412
Epoch 100/300, Loss: 0.0773 | 0.0411
Epoch 101/300, Loss: 0.0772 | 0.0408
Epoch 102/300, Loss: 0.0771 | 0.0406
Epoch 103/300, Loss: 0.0770 | 0.0404
Epoch 104/300, Loss: 0.0770 | 0.0403
Epoch 105/300, Loss: 0.0769 | 0.0403
Epoch 106/300, Loss: 0.0767 | 0.0405
Epoch 107/300, Loss: 0.0767 | 0.0404
Epoch 108/300, Loss: 0.0766 | 0.0402
Epoch 109/300, Loss: 0.0765 | 0.0400
Epoch 110/300, Loss: 0.0765 | 0.0399
Epoch 111/300, Loss: 0.0764 | 0.0399
Epoch 112/300, Loss: 0.0763 | 0.0398
Epoch 113/300, Loss: 0.0762 | 0.0398
Epoch 114/300, Loss: 0.0762 | 0.0397
Epoch 115/300, Loss: 0.0761 | 0.0396
Epoch 116/300, Loss: 0.0761 | 0.0395
Epoch 117/300, Loss: 0.0760 | 0.0395
Epoch 118/300, Loss: 0.0760 | 0.0395
Epoch 119/300, Loss: 0.0759 | 0.0394
Epoch 120/300, Loss: 0.0759 | 0.0393
Epoch 121/300, Loss: 0.0758 | 0.0393
Epoch 122/300, Loss: 0.0758 | 0.0392
Epoch 123/300, Loss: 0.0757 | 0.0392
Epoch 124/300, Loss: 0.0757 | 0.0391
Epoch 125/300, Loss: 0.0757 | 0.0390
Epoch 126/300, Loss: 0.0756 | 0.0390
Epoch 127/300, Loss: 0.0756 | 0.0390
Epoch 128/300, Loss: 0.0755 | 0.0389
Epoch 129/300, Loss: 0.0755 | 0.0389
Epoch 130/300, Loss: 0.0755 | 0.0388
Epoch 131/300, Loss: 0.0754 | 0.0388
Epoch 132/300, Loss: 0.0754 | 0.0387
Epoch 133/300, Loss: 0.0754 | 0.0387
Epoch 134/300, Loss: 0.0753 | 0.0386
Epoch 135/300, Loss: 0.0753 | 0.0386
Epoch 136/300, Loss: 0.0753 | 0.0386
Epoch 137/300, Loss: 0.0752 | 0.0385
Epoch 138/300, Loss: 0.0752 | 0.0385
Epoch 139/300, Loss: 0.0752 | 0.0385
Epoch 140/300, Loss: 0.0752 | 0.0384
Epoch 141/300, Loss: 0.0751 | 0.0384
Epoch 142/300, Loss: 0.0751 | 0.0384
Epoch 143/300, Loss: 0.0751 | 0.0383
Epoch 144/300, Loss: 0.0751 | 0.0383
Epoch 145/300, Loss: 0.0750 | 0.0383
Epoch 146/300, Loss: 0.0750 | 0.0382
Epoch 147/300, Loss: 0.0750 | 0.0382
Epoch 148/300, Loss: 0.0750 | 0.0382
Epoch 149/300, Loss: 0.0749 | 0.0381
Epoch 150/300, Loss: 0.0749 | 0.0381
Epoch 151/300, Loss: 0.0749 | 0.0381
Epoch 152/300, Loss: 0.0749 | 0.0381
Epoch 153/300, Loss: 0.0749 | 0.0380
Epoch 154/300, Loss: 0.0749 | 0.0380
Epoch 155/300, Loss: 0.0748 | 0.0380
Epoch 156/300, Loss: 0.0748 | 0.0380
Epoch 157/300, Loss: 0.0748 | 0.0380
Epoch 158/300, Loss: 0.0748 | 0.0379
Epoch 159/300, Loss: 0.0748 | 0.0379
Epoch 160/300, Loss: 0.0748 | 0.0379
Epoch 161/300, Loss: 0.0747 | 0.0379
Epoch 162/300, Loss: 0.0747 | 0.0378
Epoch 163/300, Loss: 0.0747 | 0.0378
Epoch 164/300, Loss: 0.0747 | 0.0378
Epoch 165/300, Loss: 0.0747 | 0.0378
Epoch 166/300, Loss: 0.0747 | 0.0378
Epoch 167/300, Loss: 0.0747 | 0.0378
Epoch 168/300, Loss: 0.0746 | 0.0377
Epoch 169/300, Loss: 0.0746 | 0.0377
Epoch 170/300, Loss: 0.0746 | 0.0377
Epoch 171/300, Loss: 0.0746 | 0.0377
Epoch 172/300, Loss: 0.0746 | 0.0377
Epoch 173/300, Loss: 0.0746 | 0.0377
Epoch 174/300, Loss: 0.0746 | 0.0377
Epoch 175/300, Loss: 0.0746 | 0.0376
Epoch 176/300, Loss: 0.0746 | 0.0376
Epoch 177/300, Loss: 0.0745 | 0.0376
Epoch 178/300, Loss: 0.0745 | 0.0376
Epoch 179/300, Loss: 0.0745 | 0.0376
Epoch 180/300, Loss: 0.0745 | 0.0376
Epoch 181/300, Loss: 0.0745 | 0.0376
Epoch 182/300, Loss: 0.0745 | 0.0376
Epoch 183/300, Loss: 0.0745 | 0.0375
Epoch 184/300, Loss: 0.0745 | 0.0375
Epoch 185/300, Loss: 0.0745 | 0.0375
Epoch 186/300, Loss: 0.0745 | 0.0375
Epoch 187/300, Loss: 0.0745 | 0.0375
Epoch 188/300, Loss: 0.0744 | 0.0375
Epoch 189/300, Loss: 0.0744 | 0.0375
Epoch 190/300, Loss: 0.0744 | 0.0375
Epoch 191/300, Loss: 0.0744 | 0.0375
Epoch 192/300, Loss: 0.0744 | 0.0375
Epoch 193/300, Loss: 0.0744 | 0.0375
Epoch 194/300, Loss: 0.0744 | 0.0374
Epoch 195/300, Loss: 0.0744 | 0.0374
Epoch 196/300, Loss: 0.0744 | 0.0374
Epoch 197/300, Loss: 0.0744 | 0.0374
Epoch 198/300, Loss: 0.0744 | 0.0374
Epoch 199/300, Loss: 0.0744 | 0.0374
Epoch 200/300, Loss: 0.0744 | 0.0374
Epoch 201/300, Loss: 0.0744 | 0.0374
Epoch 202/300, Loss: 0.0744 | 0.0374
Epoch 203/300, Loss: 0.0744 | 0.0374
Epoch 204/300, Loss: 0.0744 | 0.0374
Epoch 205/300, Loss: 0.0743 | 0.0374
Epoch 206/300, Loss: 0.0743 | 0.0374
Epoch 207/300, Loss: 0.0743 | 0.0374
Epoch 208/300, Loss: 0.0743 | 0.0374
Epoch 209/300, Loss: 0.0743 | 0.0373
Epoch 210/300, Loss: 0.0743 | 0.0373
Epoch 211/300, Loss: 0.0743 | 0.0373
Epoch 212/300, Loss: 0.0743 | 0.0373
Epoch 213/300, Loss: 0.0743 | 0.0373
Epoch 214/300, Loss: 0.0743 | 0.0373
Epoch 215/300, Loss: 0.0743 | 0.0373
Epoch 216/300, Loss: 0.0743 | 0.0373
Epoch 217/300, Loss: 0.0743 | 0.0373
Epoch 218/300, Loss: 0.0743 | 0.0373
Epoch 219/300, Loss: 0.0743 | 0.0373
Epoch 220/300, Loss: 0.0743 | 0.0373
Epoch 221/300, Loss: 0.0743 | 0.0373
Epoch 222/300, Loss: 0.0743 | 0.0373
Epoch 223/300, Loss: 0.0743 | 0.0373
Epoch 224/300, Loss: 0.0743 | 0.0373
Epoch 225/300, Loss: 0.0743 | 0.0373
Epoch 226/300, Loss: 0.0743 | 0.0373
Epoch 227/300, Loss: 0.0743 | 0.0373
Epoch 228/300, Loss: 0.0743 | 0.0373
Epoch 229/300, Loss: 0.0743 | 0.0373
Epoch 230/300, Loss: 0.0743 | 0.0373
Epoch 231/300, Loss: 0.0743 | 0.0373
Epoch 232/300, Loss: 0.0743 | 0.0373
Epoch 233/300, Loss: 0.0743 | 0.0372
Epoch 234/300, Loss: 0.0743 | 0.0372
Epoch 235/300, Loss: 0.0743 | 0.0372
Epoch 236/300, Loss: 0.0742 | 0.0372
Epoch 237/300, Loss: 0.0742 | 0.0372
Epoch 238/300, Loss: 0.0742 | 0.0372
Epoch 239/300, Loss: 0.0742 | 0.0372
Epoch 240/300, Loss: 0.0742 | 0.0372
Epoch 241/300, Loss: 0.0742 | 0.0372
Epoch 242/300, Loss: 0.0742 | 0.0372
Epoch 243/300, Loss: 0.0742 | 0.0372
Epoch 244/300, Loss: 0.0742 | 0.0372
Epoch 245/300, Loss: 0.0742 | 0.0372
Epoch 246/300, Loss: 0.0742 | 0.0372
Epoch 247/300, Loss: 0.0742 | 0.0372
Epoch 248/300, Loss: 0.0742 | 0.0372
Epoch 249/300, Loss: 0.0742 | 0.0372
Epoch 250/300, Loss: 0.0742 | 0.0372
Epoch 251/300, Loss: 0.0742 | 0.0372
Epoch 252/300, Loss: 0.0742 | 0.0372
Epoch 253/300, Loss: 0.0742 | 0.0372
Epoch 254/300, Loss: 0.0742 | 0.0372
Epoch 255/300, Loss: 0.0742 | 0.0372
Epoch 256/300, Loss: 0.0742 | 0.0372
Epoch 257/300, Loss: 0.0742 | 0.0372
Epoch 258/300, Loss: 0.0742 | 0.0372
Epoch 259/300, Loss: 0.0742 | 0.0372
Epoch 260/300, Loss: 0.0742 | 0.0372
Epoch 261/300, Loss: 0.0742 | 0.0372
Epoch 262/300, Loss: 0.0742 | 0.0372
Epoch 263/300, Loss: 0.0742 | 0.0372
Epoch 264/300, Loss: 0.0742 | 0.0372
Epoch 265/300, Loss: 0.0742 | 0.0372
Epoch 266/300, Loss: 0.0742 | 0.0372
Epoch 267/300, Loss: 0.0742 | 0.0372
Epoch 268/300, Loss: 0.0742 | 0.0372
Epoch 269/300, Loss: 0.0742 | 0.0372
Epoch 270/300, Loss: 0.0742 | 0.0372
Epoch 271/300, Loss: 0.0742 | 0.0372
Epoch 272/300, Loss: 0.0742 | 0.0372
Epoch 273/300, Loss: 0.0742 | 0.0372
Epoch 274/300, Loss: 0.0742 | 0.0372
Epoch 275/300, Loss: 0.0742 | 0.0372
Epoch 276/300, Loss: 0.0742 | 0.0372
Epoch 277/300, Loss: 0.0742 | 0.0372
Epoch 278/300, Loss: 0.0742 | 0.0372
Epoch 279/300, Loss: 0.0742 | 0.0372
Epoch 280/300, Loss: 0.0742 | 0.0372
Epoch 281/300, Loss: 0.0742 | 0.0372
Epoch 282/300, Loss: 0.0742 | 0.0372
Epoch 283/300, Loss: 0.0742 | 0.0372
Epoch 284/300, Loss: 0.0742 | 0.0372
Epoch 285/300, Loss: 0.0742 | 0.0372
Epoch 286/300, Loss: 0.0742 | 0.0372
Epoch 287/300, Loss: 0.0742 | 0.0372
Epoch 288/300, Loss: 0.0742 | 0.0372
Epoch 289/300, Loss: 0.0742 | 0.0372
Epoch 290/300, Loss: 0.0742 | 0.0372
Epoch 291/300, Loss: 0.0742 | 0.0372
Epoch 292/300, Loss: 0.0742 | 0.0372
Epoch 293/300, Loss: 0.0742 | 0.0372
Epoch 294/300, Loss: 0.0742 | 0.0372
Epoch 295/300, Loss: 0.0742 | 0.0372
Epoch 296/300, Loss: 0.0742 | 0.0372
Epoch 297/300, Loss: 0.0742 | 0.0372
Epoch 298/300, Loss: 0.0742 | 0.0372
Epoch 299/300, Loss: 0.0742 | 0.0372
Epoch 300/300, Loss: 0.0742 | 0.0372
Runtime (seconds): 122.73648118972778
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 369.68358909734525
RMSE: 19.227157592773438
MAE: 19.227157592773438
R-squared: nan
[206.36716]
