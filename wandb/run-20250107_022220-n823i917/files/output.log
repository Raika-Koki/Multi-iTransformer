ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 02:22:21,579][0m A new study created in memory with name: no-name-6878357b-e1b9-4d03-9fb7-42ffa3dd0780[0m
[32m[I 2025-01-07 02:23:17,479][0m Trial 0 finished with value: 0.09464868810457018 and parameters: {'observation_period_num': 200, 'train_rates': 0.9214580005446066, 'learning_rate': 0.00017962612604131481, 'batch_size': 99, 'step_size': 14, 'gamma': 0.8294634582895039}. Best is trial 0 with value: 0.09464868810457018.[0m
[32m[I 2025-01-07 02:23:49,509][0m Trial 1 finished with value: 0.0968385860323906 and parameters: {'observation_period_num': 11, 'train_rates': 0.9308236500946505, 'learning_rate': 2.276440540451489e-05, 'batch_size': 253, 'step_size': 2, 'gamma': 0.9865708193267049}. Best is trial 0 with value: 0.09464868810457018.[0m
[32m[I 2025-01-07 02:25:12,828][0m Trial 2 finished with value: 0.4121900428282587 and parameters: {'observation_period_num': 51, 'train_rates': 0.6858116679802165, 'learning_rate': 2.2343495229318093e-06, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9662099282850634}. Best is trial 0 with value: 0.09464868810457018.[0m
[32m[I 2025-01-07 02:26:19,573][0m Trial 3 finished with value: 0.16939008156613014 and parameters: {'observation_period_num': 224, 'train_rates': 0.6935979837824158, 'learning_rate': 0.0003810506571048544, 'batch_size': 67, 'step_size': 14, 'gamma': 0.7763436640149373}. Best is trial 0 with value: 0.09464868810457018.[0m
[32m[I 2025-01-07 02:28:04,425][0m Trial 4 finished with value: 0.0579867910242936 and parameters: {'observation_period_num': 50, 'train_rates': 0.6927790591300709, 'learning_rate': 8.682300568444555e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8065357832802611}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:28:32,222][0m Trial 5 finished with value: 0.10207988260619676 and parameters: {'observation_period_num': 77, 'train_rates': 0.9068667292999666, 'learning_rate': 0.00010214570559324379, 'batch_size': 237, 'step_size': 10, 'gamma': 0.8046618087143175}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:29:07,967][0m Trial 6 finished with value: 0.1816784847399284 and parameters: {'observation_period_num': 193, 'train_rates': 0.9159859698955701, 'learning_rate': 5.029872888691241e-05, 'batch_size': 160, 'step_size': 3, 'gamma': 0.948401083127306}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:30:09,956][0m Trial 7 finished with value: 0.0719922137471427 and parameters: {'observation_period_num': 73, 'train_rates': 0.9609044281267739, 'learning_rate': 0.00023205173218891477, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9838138050436578}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:30:45,768][0m Trial 8 finished with value: 0.11566064504943058 and parameters: {'observation_period_num': 139, 'train_rates': 0.7101983767967865, 'learning_rate': 8.767227496488105e-05, 'batch_size': 137, 'step_size': 4, 'gamma': 0.9292485619589057}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:31:08,569][0m Trial 9 finished with value: 0.9689529214586531 and parameters: {'observation_period_num': 134, 'train_rates': 0.6648953024893381, 'learning_rate': 8.965035999788093e-06, 'batch_size': 243, 'step_size': 4, 'gamma': 0.7503987885732999}. Best is trial 4 with value: 0.0579867910242936.[0m
[32m[I 2025-01-07 02:35:34,936][0m Trial 10 finished with value: 0.04711621253076812 and parameters: {'observation_period_num': 13, 'train_rates': 0.8027044690104486, 'learning_rate': 7.271207574073294e-06, 'batch_size': 19, 'step_size': 8, 'gamma': 0.8759961591206638}. Best is trial 10 with value: 0.04711621253076812.[0m
[32m[I 2025-01-07 02:40:36,094][0m Trial 11 finished with value: 0.04925654948197512 and parameters: {'observation_period_num': 7, 'train_rates': 0.8046391993684603, 'learning_rate': 6.527964736031375e-06, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8828946811156454}. Best is trial 10 with value: 0.04711621253076812.[0m
[32m[I 2025-01-07 02:45:35,898][0m Trial 12 finished with value: 0.0677931686919747 and parameters: {'observation_period_num': 11, 'train_rates': 0.809472518954947, 'learning_rate': 2.6846500378700344e-06, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8840257428942463}. Best is trial 10 with value: 0.04711621253076812.[0m
[32m[I 2025-01-07 02:49:54,106][0m Trial 13 finished with value: 0.04487388260557802 and parameters: {'observation_period_num': 12, 'train_rates': 0.8066899787335622, 'learning_rate': 7.618462404404391e-06, 'batch_size': 20, 'step_size': 11, 'gamma': 0.8854867847811825}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:50:32,119][0m Trial 14 finished with value: 0.10744260532364348 and parameters: {'observation_period_num': 101, 'train_rates': 0.8420694207695548, 'learning_rate': 0.0009509249957070349, 'batch_size': 168, 'step_size': 11, 'gamma': 0.916598558245387}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:51:36,353][0m Trial 15 finished with value: 0.12412719130101556 and parameters: {'observation_period_num': 41, 'train_rates': 0.7531060923047385, 'learning_rate': 1.4181173750863427e-05, 'batch_size': 79, 'step_size': 6, 'gamma': 0.856821256671188}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:53:55,705][0m Trial 16 finished with value: 0.5113600593346815 and parameters: {'observation_period_num': 111, 'train_rates': 0.8630657236472938, 'learning_rate': 1.1112419696805746e-06, 'batch_size': 38, 'step_size': 11, 'gamma': 0.852874941885333}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:54:22,059][0m Trial 17 finished with value: 0.6397190342170064 and parameters: {'observation_period_num': 34, 'train_rates': 0.614973007096581, 'learning_rate': 4.419604716925684e-06, 'batch_size': 200, 'step_size': 12, 'gamma': 0.902663012066015}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:55:08,915][0m Trial 18 finished with value: 0.26837555046695394 and parameters: {'observation_period_num': 149, 'train_rates': 0.7493791581613419, 'learning_rate': 2.8089922798973108e-05, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8356510796602118}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:55:51,349][0m Trial 19 finished with value: 0.20011839598160366 and parameters: {'observation_period_num': 80, 'train_rates': 0.7577431391301647, 'learning_rate': 1.3190845950252544e-05, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8981743757284825}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 02:58:14,927][0m Trial 20 finished with value: 0.5326058268547058 and parameters: {'observation_period_num': 251, 'train_rates': 0.857694860674069, 'learning_rate': 1.137312245042277e-06, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8679124018878678}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:01:21,013][0m Trial 21 finished with value: 0.061395268507911285 and parameters: {'observation_period_num': 19, 'train_rates': 0.7960728985307596, 'learning_rate': 7.05072845311088e-06, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8813564972049356}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:05:26,885][0m Trial 22 finished with value: 0.05918563828871396 and parameters: {'observation_period_num': 6, 'train_rates': 0.822958444707349, 'learning_rate': 4.41799805496291e-06, 'batch_size': 22, 'step_size': 6, 'gamma': 0.9257726772521798}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:07:07,153][0m Trial 23 finished with value: 0.11616181680723164 and parameters: {'observation_period_num': 29, 'train_rates': 0.7750636842749918, 'learning_rate': 7.187365585604942e-06, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8922451948232893}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:12:30,047][0m Trial 24 finished with value: 0.12491139781752822 and parameters: {'observation_period_num': 54, 'train_rates': 0.8631476627022907, 'learning_rate': 2.780867959682099e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8416981867985167}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:14:22,210][0m Trial 25 finished with value: 0.08680732690011429 and parameters: {'observation_period_num': 64, 'train_rates': 0.7827581429409707, 'learning_rate': 1.7980406327175866e-05, 'batch_size': 45, 'step_size': 7, 'gamma': 0.8694679658556281}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:15:33,858][0m Trial 26 finished with value: 0.07964256722306444 and parameters: {'observation_period_num': 97, 'train_rates': 0.7370900065185296, 'learning_rate': 4.5807739194216346e-05, 'batch_size': 82, 'step_size': 10, 'gamma': 0.9046161414741173}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:17:22,417][0m Trial 27 finished with value: 0.1220679973632517 and parameters: {'observation_period_num': 26, 'train_rates': 0.830318784703678, 'learning_rate': 4.3966890562500415e-06, 'batch_size': 52, 'step_size': 5, 'gamma': 0.9481734285258144}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:18:38,499][0m Trial 28 finished with value: 0.07308921034385032 and parameters: {'observation_period_num': 6, 'train_rates': 0.8842868762265846, 'learning_rate': 1.051474273026065e-05, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8176599680842032}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:19:30,769][0m Trial 29 finished with value: 0.5319999999704614 and parameters: {'observation_period_num': 170, 'train_rates': 0.7971376830171109, 'learning_rate': 1.9284328540500635e-06, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8765888238081859}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:21:59,498][0m Trial 30 finished with value: 0.09558172586063544 and parameters: {'observation_period_num': 35, 'train_rates': 0.817252672283428, 'learning_rate': 5.7609511986624126e-06, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8540982466797592}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:23:36,794][0m Trial 31 finished with value: 0.15391059478474084 and parameters: {'observation_period_num': 50, 'train_rates': 0.6089664007057984, 'learning_rate': 5.0909377709567667e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.797527098097257}. Best is trial 13 with value: 0.04487388260557802.[0m
[32m[I 2025-01-07 03:26:05,653][0m Trial 32 finished with value: 0.041697711424802886 and parameters: {'observation_period_num': 22, 'train_rates': 0.6541062738812278, 'learning_rate': 9.590466489539106e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8276491548595934}. Best is trial 32 with value: 0.041697711424802886.[0m
[32m[I 2025-01-07 03:29:00,219][0m Trial 33 finished with value: 0.1476431671980908 and parameters: {'observation_period_num': 23, 'train_rates': 0.6447158917358492, 'learning_rate': 3.077446221342993e-05, 'batch_size': 27, 'step_size': 7, 'gamma': 0.8225658833225007}. Best is trial 32 with value: 0.041697711424802886.[0m
[32m[I 2025-01-07 03:30:32,465][0m Trial 34 finished with value: 0.10196448592912583 and parameters: {'observation_period_num': 5, 'train_rates': 0.7150959095227468, 'learning_rate': 1.9072735908938405e-05, 'batch_size': 61, 'step_size': 2, 'gamma': 0.8490463629900902}. Best is trial 32 with value: 0.041697711424802886.[0m
[32m[I 2025-01-07 03:35:41,144][0m Trial 35 finished with value: 0.05401678805832159 and parameters: {'observation_period_num': 39, 'train_rates': 0.7737533819890501, 'learning_rate': 0.00016633194992250484, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8633057282149278}. Best is trial 32 with value: 0.041697711424802886.[0m
[32m[I 2025-01-07 03:37:31,927][0m Trial 36 finished with value: 0.3124264724736596 and parameters: {'observation_period_num': 61, 'train_rates': 0.8880797649740939, 'learning_rate': 1.6789063404482001e-06, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9179207711180549}. Best is trial 32 with value: 0.041697711424802886.[0m
[32m[I 2025-01-07 03:39:03,508][0m Trial 37 finished with value: 0.03952469103804344 and parameters: {'observation_period_num': 24, 'train_rates': 0.7211009647785065, 'learning_rate': 0.0004086915508039303, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8287212634225006}. Best is trial 37 with value: 0.03952469103804344.[0m
Early stopping at epoch 59
[32m[I 2025-01-07 03:39:49,574][0m Trial 38 finished with value: 0.08531656997936958 and parameters: {'observation_period_num': 21, 'train_rates': 0.6635753183489576, 'learning_rate': 0.0003989560612404823, 'batch_size': 71, 'step_size': 1, 'gamma': 0.7951925300704198}. Best is trial 37 with value: 0.03952469103804344.[0m
[32m[I 2025-01-07 03:40:59,795][0m Trial 39 finished with value: 0.06293506278077615 and parameters: {'observation_period_num': 82, 'train_rates': 0.7148162479707137, 'learning_rate': 0.0005057915170017124, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8282324825307269}. Best is trial 37 with value: 0.03952469103804344.[0m
[32m[I 2025-01-07 03:41:58,143][0m Trial 40 finished with value: 0.10187006576601508 and parameters: {'observation_period_num': 64, 'train_rates': 0.6486440828471823, 'learning_rate': 0.0001372958965412821, 'batch_size': 120, 'step_size': 4, 'gamma': 0.8160505739149985}. Best is trial 37 with value: 0.03952469103804344.[0m
[32m[I 2025-01-07 03:44:37,184][0m Trial 41 finished with value: 0.03091887133159758 and parameters: {'observation_period_num': 17, 'train_rates': 0.7374219831283599, 'learning_rate': 0.000259005902421501, 'batch_size': 34, 'step_size': 5, 'gamma': 0.7822294511431461}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:46:30,286][0m Trial 42 finished with value: 0.036625856647831724 and parameters: {'observation_period_num': 19, 'train_rates': 0.733199381461975, 'learning_rate': 0.0002669328244344728, 'batch_size': 51, 'step_size': 5, 'gamma': 0.7749050736153198}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:48:16,441][0m Trial 43 finished with value: 0.05350747683218547 and parameters: {'observation_period_num': 45, 'train_rates': 0.687049888955326, 'learning_rate': 0.00026456884141009564, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7770344973218305}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:49:51,131][0m Trial 44 finished with value: 0.06435125187113326 and parameters: {'observation_period_num': 20, 'train_rates': 0.7254128274542919, 'learning_rate': 0.000587311529730907, 'batch_size': 63, 'step_size': 3, 'gamma': 0.755588089408094}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:52:18,043][0m Trial 45 finished with value: 0.09030054909275871 and parameters: {'observation_period_num': 43, 'train_rates': 0.669441926319327, 'learning_rate': 0.0001120286951525874, 'batch_size': 32, 'step_size': 4, 'gamma': 0.779703962537378}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:54:35,968][0m Trial 46 finished with value: 0.0895455018854526 and parameters: {'observation_period_num': 30, 'train_rates': 0.6979948721276318, 'learning_rate': 0.0003135270282539353, 'batch_size': 41, 'step_size': 3, 'gamma': 0.7693203542686634}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:55:28,531][0m Trial 47 finished with value: 0.05653467131154453 and parameters: {'observation_period_num': 16, 'train_rates': 0.7410183509331023, 'learning_rate': 0.0001917034604786988, 'batch_size': 161, 'step_size': 6, 'gamma': 0.7915419679285698}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:57:24,486][0m Trial 48 finished with value: 0.08224622644961645 and parameters: {'observation_period_num': 56, 'train_rates': 0.7308128960626282, 'learning_rate': 6.748963823479076e-05, 'batch_size': 49, 'step_size': 5, 'gamma': 0.7629651785848554}. Best is trial 41 with value: 0.03091887133159758.[0m
[32m[I 2025-01-07 03:58:13,520][0m Trial 49 finished with value: 0.06679561641414775 and parameters: {'observation_period_num': 32, 'train_rates': 0.6328831965919474, 'learning_rate': 0.0007767222931758043, 'batch_size': 215, 'step_size': 7, 'gamma': 0.7844969159318647}. Best is trial 41 with value: 0.03091887133159758.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 03:58:13,530][0m A new study created in memory with name: no-name-36d1f5bf-180a-4e3a-91ce-0699d82640f3[0m
[32m[I 2025-01-07 03:59:24,282][0m Trial 0 finished with value: 0.2849633652981513 and parameters: {'observation_period_num': 17, 'train_rates': 0.9377578377465764, 'learning_rate': 4.131892864064948e-06, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9537538848671114}. Best is trial 0 with value: 0.2849633652981513.[0m
[32m[I 2025-01-07 04:01:08,381][0m Trial 1 finished with value: 0.6874019241114275 and parameters: {'observation_period_num': 205, 'train_rates': 0.8414087747169903, 'learning_rate': 3.1703408367517628e-06, 'batch_size': 61, 'step_size': 3, 'gamma': 0.8471220198318004}. Best is trial 0 with value: 0.2849633652981513.[0m
[32m[I 2025-01-07 04:01:54,201][0m Trial 2 finished with value: 0.28733002682684233 and parameters: {'observation_period_num': 250, 'train_rates': 0.673066409386171, 'learning_rate': 6.305569525078194e-05, 'batch_size': 219, 'step_size': 12, 'gamma': 0.9225549134595773}. Best is trial 0 with value: 0.2849633652981513.[0m
[32m[I 2025-01-07 04:03:21,613][0m Trial 3 finished with value: 0.18301783665651228 and parameters: {'observation_period_num': 198, 'train_rates': 0.8363568538640647, 'learning_rate': 0.000798256631849624, 'batch_size': 77, 'step_size': 3, 'gamma': 0.9737686575128826}. Best is trial 3 with value: 0.18301783665651228.[0m
[32m[I 2025-01-07 04:04:11,522][0m Trial 4 finished with value: 0.11141415342562871 and parameters: {'observation_period_num': 28, 'train_rates': 0.6353019837295162, 'learning_rate': 3.8497359659769e-05, 'batch_size': 181, 'step_size': 7, 'gamma': 0.9260632110858339}. Best is trial 4 with value: 0.11141415342562871.[0m
Early stopping at epoch 68
[32m[I 2025-01-07 04:05:06,744][0m Trial 5 finished with value: 0.39976083282111347 and parameters: {'observation_period_num': 124, 'train_rates': 0.8050308803190207, 'learning_rate': 4.962737866557026e-05, 'batch_size': 97, 'step_size': 1, 'gamma': 0.8254386089355582}. Best is trial 4 with value: 0.11141415342562871.[0m
[32m[I 2025-01-07 04:06:23,713][0m Trial 6 finished with value: 0.8550778180360794 and parameters: {'observation_period_num': 153, 'train_rates': 0.8357390884178854, 'learning_rate': 1.1069547667206105e-06, 'batch_size': 115, 'step_size': 12, 'gamma': 0.8130424318879118}. Best is trial 4 with value: 0.11141415342562871.[0m
Early stopping at epoch 85
[32m[I 2025-01-07 04:07:14,402][0m Trial 7 finished with value: 1.4376619747706822 and parameters: {'observation_period_num': 184, 'train_rates': 0.7472778915869144, 'learning_rate': 1.9711835861167133e-06, 'batch_size': 162, 'step_size': 2, 'gamma': 0.7534814218025832}. Best is trial 4 with value: 0.11141415342562871.[0m
[32m[I 2025-01-07 04:08:22,172][0m Trial 8 finished with value: 0.23269478548532244 and parameters: {'observation_period_num': 235, 'train_rates': 0.9040340845744701, 'learning_rate': 2.5291090389513127e-05, 'batch_size': 156, 'step_size': 10, 'gamma': 0.9289181500570701}. Best is trial 4 with value: 0.11141415342562871.[0m
[32m[I 2025-01-07 04:09:15,708][0m Trial 9 finished with value: 1.1670635627424586 and parameters: {'observation_period_num': 182, 'train_rates': 0.6089904678830661, 'learning_rate': 2.1128242904576715e-06, 'batch_size': 109, 'step_size': 5, 'gamma': 0.8238434509517145}. Best is trial 4 with value: 0.11141415342562871.[0m
[32m[I 2025-01-07 04:13:20,950][0m Trial 10 finished with value: 0.0896178975060422 and parameters: {'observation_period_num': 15, 'train_rates': 0.6834531392011672, 'learning_rate': 0.00033142510391116726, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8877867930642969}. Best is trial 10 with value: 0.0896178975060422.[0m
[32m[I 2025-01-07 04:14:00,489][0m Trial 11 finished with value: 0.07953465831663448 and parameters: {'observation_period_num': 9, 'train_rates': 0.6941540964824896, 'learning_rate': 0.0004705580187344992, 'batch_size': 205, 'step_size': 15, 'gamma': 0.8828051010040149}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:14:39,674][0m Trial 12 finished with value: 0.09801858270359393 and parameters: {'observation_period_num': 67, 'train_rates': 0.7189245183199209, 'learning_rate': 0.0009939884165141746, 'batch_size': 240, 'step_size': 15, 'gamma': 0.8832656027794875}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:18:45,236][0m Trial 13 finished with value: 0.23747298731765848 and parameters: {'observation_period_num': 68, 'train_rates': 0.6903699012269449, 'learning_rate': 0.00024603210034748627, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8760347477513022}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:19:28,214][0m Trial 14 finished with value: 0.09712074474467859 and parameters: {'observation_period_num': 58, 'train_rates': 0.7606521184703352, 'learning_rate': 0.00023282988108671145, 'batch_size': 201, 'step_size': 13, 'gamma': 0.8947093159134195}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:23:41,181][0m Trial 15 finished with value: 0.16346632391214372 and parameters: {'observation_period_num': 103, 'train_rates': 0.6580899759721301, 'learning_rate': 0.00024123833169309008, 'batch_size': 18, 'step_size': 9, 'gamma': 0.8557844994306322}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:25:31,539][0m Trial 16 finished with value: 0.08162171783217465 and parameters: {'observation_period_num': 11, 'train_rates': 0.7178385158780984, 'learning_rate': 0.0004039188389767062, 'batch_size': 49, 'step_size': 14, 'gamma': 0.7707700081877645}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:26:59,406][0m Trial 17 finished with value: 0.08184905923957372 and parameters: {'observation_period_num': 43, 'train_rates': 0.7646505012687332, 'learning_rate': 9.680724675009906e-05, 'batch_size': 65, 'step_size': 13, 'gamma': 0.7608873144318424}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:27:37,801][0m Trial 18 finished with value: 0.39083094793287193 and parameters: {'observation_period_num': 100, 'train_rates': 0.7223574499617448, 'learning_rate': 1.7045414783510117e-05, 'batch_size': 249, 'step_size': 10, 'gamma': 0.7880575681262881}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:28:37,329][0m Trial 19 finished with value: 0.2510800623163885 and parameters: {'observation_period_num': 6, 'train_rates': 0.6011397896373852, 'learning_rate': 9.18946539059447e-06, 'batch_size': 192, 'step_size': 13, 'gamma': 0.7948104661703432}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:30:48,107][0m Trial 20 finished with value: 0.10207033045711268 and parameters: {'observation_period_num': 87, 'train_rates': 0.7857975091868247, 'learning_rate': 0.0005035046515334215, 'batch_size': 45, 'step_size': 11, 'gamma': 0.846505484156857}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:31:59,382][0m Trial 21 finished with value: 0.09176262750388672 and parameters: {'observation_period_num': 46, 'train_rates': 0.7230492978400368, 'learning_rate': 0.00012213358876566937, 'batch_size': 76, 'step_size': 14, 'gamma': 0.751234253252531}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:33:56,762][0m Trial 22 finished with value: 0.0815272660349388 and parameters: {'observation_period_num': 37, 'train_rates': 0.7597927525064364, 'learning_rate': 0.00010086888564709998, 'batch_size': 46, 'step_size': 13, 'gamma': 0.7807194001083761}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:36:00,326][0m Trial 23 finished with value: 0.09643077926936551 and parameters: {'observation_period_num': 31, 'train_rates': 0.7070783210236246, 'learning_rate': 0.00013836742005531472, 'batch_size': 44, 'step_size': 14, 'gamma': 0.7841188274026493}. Best is trial 11 with value: 0.07953465831663448.[0m
[32m[I 2025-01-07 04:37:00,727][0m Trial 24 finished with value: 0.0712643771742781 and parameters: {'observation_period_num': 6, 'train_rates': 0.6457600246118657, 'learning_rate': 0.0005304571833000312, 'batch_size': 127, 'step_size': 7, 'gamma': 0.7714799318359248}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:37:50,719][0m Trial 25 finished with value: 0.0933125766457581 and parameters: {'observation_period_num': 39, 'train_rates': 0.6483653545228046, 'learning_rate': 0.0005385015249274407, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8039244324466959}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:38:46,813][0m Trial 26 finished with value: 0.16602999391739792 and parameters: {'observation_period_num': 84, 'train_rates': 0.6320398889707641, 'learning_rate': 0.00013426183193770183, 'batch_size': 119, 'step_size': 8, 'gamma': 0.9079268684816187}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:39:40,283][0m Trial 27 finished with value: 0.09786494821310043 and parameters: {'observation_period_num': 140, 'train_rates': 0.9821825909848515, 'learning_rate': 0.000646172927036833, 'batch_size': 170, 'step_size': 6, 'gamma': 0.7763313851066977}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:40:39,160][0m Trial 28 finished with value: 0.08893536592945069 and parameters: {'observation_period_num': 55, 'train_rates': 0.8009771705707298, 'learning_rate': 7.019883840205477e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.826139037238804}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:41:50,858][0m Trial 29 finished with value: 0.07170735098539836 and parameters: {'observation_period_num': 21, 'train_rates': 0.8925751237055957, 'learning_rate': 0.0001827862398203183, 'batch_size': 134, 'step_size': 5, 'gamma': 0.8600678823463962}. Best is trial 24 with value: 0.0712643771742781.[0m
[32m[I 2025-01-07 04:42:44,117][0m Trial 30 finished with value: 0.058067480102181435 and parameters: {'observation_period_num': 5, 'train_rates': 0.9107300004498359, 'learning_rate': 0.0001915654768943117, 'batch_size': 138, 'step_size': 4, 'gamma': 0.9588977063578098}. Best is trial 30 with value: 0.058067480102181435.[0m
[32m[I 2025-01-07 04:43:39,681][0m Trial 31 finished with value: 0.05849546307393353 and parameters: {'observation_period_num': 6, 'train_rates': 0.9109733541673185, 'learning_rate': 0.0002193455058843175, 'batch_size': 134, 'step_size': 4, 'gamma': 0.956697333819481}. Best is trial 30 with value: 0.058067480102181435.[0m
[32m[I 2025-01-07 04:44:33,596][0m Trial 32 finished with value: 0.06541740025083224 and parameters: {'observation_period_num': 26, 'train_rates': 0.9028981018696942, 'learning_rate': 0.00018803124912449161, 'batch_size': 139, 'step_size': 4, 'gamma': 0.9539967362757281}. Best is trial 30 with value: 0.058067480102181435.[0m
[32m[I 2025-01-07 04:45:27,878][0m Trial 33 finished with value: 0.057999976413342556 and parameters: {'observation_period_num': 5, 'train_rates': 0.9301637750501695, 'learning_rate': 0.00028867369951536504, 'batch_size': 148, 'step_size': 4, 'gamma': 0.9529669210430264}. Best is trial 33 with value: 0.057999976413342556.[0m
[32m[I 2025-01-07 04:46:21,618][0m Trial 34 finished with value: 0.07960425499117091 and parameters: {'observation_period_num': 25, 'train_rates': 0.9426912365091286, 'learning_rate': 7.654128329720927e-05, 'batch_size': 149, 'step_size': 3, 'gamma': 0.9883844869021859}. Best is trial 33 with value: 0.057999976413342556.[0m
[32m[I 2025-01-07 04:47:31,229][0m Trial 35 finished with value: 0.059144379889092796 and parameters: {'observation_period_num': 25, 'train_rates': 0.8880152326584781, 'learning_rate': 0.00029484523919807754, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9522546430548867}. Best is trial 33 with value: 0.057999976413342556.[0m
[32m[I 2025-01-07 04:48:43,100][0m Trial 36 finished with value: 0.09322579080501964 and parameters: {'observation_period_num': 53, 'train_rates': 0.8804839890158093, 'learning_rate': 0.00032026770611261673, 'batch_size': 88, 'step_size': 4, 'gamma': 0.9503363863532731}. Best is trial 33 with value: 0.057999976413342556.[0m
[32m[I 2025-01-07 04:49:54,247][0m Trial 37 finished with value: 0.05195588298891423 and parameters: {'observation_period_num': 23, 'train_rates': 0.9425265451425714, 'learning_rate': 0.0007913262091711074, 'batch_size': 98, 'step_size': 1, 'gamma': 0.9522344623588731}. Best is trial 37 with value: 0.05195588298891423.[0m
[32m[I 2025-01-07 04:50:44,642][0m Trial 38 finished with value: 0.06891753524541855 and parameters: {'observation_period_num': 73, 'train_rates': 0.9468084787151799, 'learning_rate': 0.0008242571330838731, 'batch_size': 172, 'step_size': 1, 'gamma': 0.9713543517064211}. Best is trial 37 with value: 0.05195588298891423.[0m
[32m[I 2025-01-07 04:51:51,339][0m Trial 39 finished with value: 0.02509966865181923 and parameters: {'observation_period_num': 18, 'train_rates': 0.9893415657115091, 'learning_rate': 0.000716628072975474, 'batch_size': 108, 'step_size': 2, 'gamma': 0.9395745594912682}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:53:01,525][0m Trial 40 finished with value: 0.03625493496656418 and parameters: {'observation_period_num': 38, 'train_rates': 0.9845954754500247, 'learning_rate': 0.0007469075151696243, 'batch_size': 102, 'step_size': 2, 'gamma': 0.9360723427638175}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:54:13,137][0m Trial 41 finished with value: 0.033915359526872635 and parameters: {'observation_period_num': 37, 'train_rates': 0.9881939574687728, 'learning_rate': 0.0007238195540521036, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9313068884202074}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:55:20,269][0m Trial 42 finished with value: 0.04306021332740784 and parameters: {'observation_period_num': 36, 'train_rates': 0.9899317632240404, 'learning_rate': 0.0007033918033518879, 'batch_size': 107, 'step_size': 2, 'gamma': 0.9378817835908513}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:56:26,554][0m Trial 43 finished with value: 0.0482015460729599 and parameters: {'observation_period_num': 43, 'train_rates': 0.983932518321996, 'learning_rate': 0.0007271299290199281, 'batch_size': 107, 'step_size': 2, 'gamma': 0.9371227020285832}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:57:34,079][0m Trial 44 finished with value: 0.03961459919810295 and parameters: {'observation_period_num': 47, 'train_rates': 0.9865899983803832, 'learning_rate': 0.0009716293803610097, 'batch_size': 106, 'step_size': 2, 'gamma': 0.937153902831686}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 04:58:51,672][0m Trial 45 finished with value: 0.06885790700713794 and parameters: {'observation_period_num': 82, 'train_rates': 0.9645173868868223, 'learning_rate': 0.000978060105664277, 'batch_size': 85, 'step_size': 2, 'gamma': 0.9164976099954019}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 05:00:06,082][0m Trial 46 finished with value: 0.05933840572834015 and parameters: {'observation_period_num': 61, 'train_rates': 0.9675904644954205, 'learning_rate': 0.0006359854498840973, 'batch_size': 111, 'step_size': 3, 'gamma': 0.9323220239910633}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 05:01:44,185][0m Trial 47 finished with value: 0.08272435047008374 and parameters: {'observation_period_num': 114, 'train_rates': 0.9621901063501541, 'learning_rate': 0.00040070924142715064, 'batch_size': 68, 'step_size': 2, 'gamma': 0.9059910837865944}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 05:02:52,569][0m Trial 48 finished with value: 0.09238255769014359 and parameters: {'observation_period_num': 157, 'train_rates': 0.9839604833192345, 'learning_rate': 0.0009059750178194481, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9399547834960784}. Best is trial 39 with value: 0.02509966865181923.[0m
[32m[I 2025-01-07 05:04:00,022][0m Trial 49 finished with value: 0.14973414977894553 and parameters: {'observation_period_num': 226, 'train_rates': 0.8551545795778532, 'learning_rate': 0.00038462856069612135, 'batch_size': 102, 'step_size': 1, 'gamma': 0.919893309424451}. Best is trial 39 with value: 0.02509966865181923.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 05:04:00,033][0m A new study created in memory with name: no-name-708b785a-7cd8-40a8-9193-96f05a6a1042[0m
[32m[I 2025-01-07 05:04:45,259][0m Trial 0 finished with value: 0.29822475896325223 and parameters: {'observation_period_num': 206, 'train_rates': 0.6713730543469747, 'learning_rate': 1.6162838883705186e-05, 'batch_size': 168, 'step_size': 14, 'gamma': 0.8834890579492187}. Best is trial 0 with value: 0.29822475896325223.[0m
[32m[I 2025-01-07 05:05:38,121][0m Trial 1 finished with value: 0.17081566421670208 and parameters: {'observation_period_num': 191, 'train_rates': 0.9072391660324881, 'learning_rate': 0.0007234612378779866, 'batch_size': 227, 'step_size': 9, 'gamma': 0.9722640377402405}. Best is trial 1 with value: 0.17081566421670208.[0m
[32m[I 2025-01-07 05:07:16,250][0m Trial 2 finished with value: 0.22138208059081133 and parameters: {'observation_period_num': 119, 'train_rates': 0.9029004404559289, 'learning_rate': 5.393670444934169e-06, 'batch_size': 67, 'step_size': 14, 'gamma': 0.937006876072034}. Best is trial 1 with value: 0.17081566421670208.[0m
[32m[I 2025-01-07 05:11:51,515][0m Trial 3 finished with value: 0.19927282702460908 and parameters: {'observation_period_num': 249, 'train_rates': 0.7518231858388821, 'learning_rate': 0.0002029444046329361, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8174602998080119}. Best is trial 1 with value: 0.17081566421670208.[0m
[32m[I 2025-01-07 05:12:54,988][0m Trial 4 finished with value: 0.1314597595798767 and parameters: {'observation_period_num': 112, 'train_rates': 0.6678825798194041, 'learning_rate': 2.812016804492265e-05, 'batch_size': 104, 'step_size': 15, 'gamma': 0.9564769553007364}. Best is trial 4 with value: 0.1314597595798767.[0m
[32m[I 2025-01-07 05:15:24,394][0m Trial 5 finished with value: 0.1402578385422964 and parameters: {'observation_period_num': 112, 'train_rates': 0.7406829548546277, 'learning_rate': 4.807701505983728e-05, 'batch_size': 35, 'step_size': 3, 'gamma': 0.834156901109152}. Best is trial 4 with value: 0.1314597595798767.[0m
[32m[I 2025-01-07 05:17:13,450][0m Trial 6 finished with value: 0.41730182959837614 and parameters: {'observation_period_num': 48, 'train_rates': 0.9289444095024206, 'learning_rate': 2.1959694306855585e-06, 'batch_size': 58, 'step_size': 11, 'gamma': 0.8285465236050615}. Best is trial 4 with value: 0.1314597595798767.[0m
[32m[I 2025-01-07 05:17:53,645][0m Trial 7 finished with value: 0.111465549237634 and parameters: {'observation_period_num': 76, 'train_rates': 0.6698140943575424, 'learning_rate': 0.000537431807272944, 'batch_size': 249, 'step_size': 7, 'gamma': 0.8553293960458784}. Best is trial 7 with value: 0.111465549237634.[0m
[32m[I 2025-01-07 05:18:35,921][0m Trial 8 finished with value: 0.09306125005536009 and parameters: {'observation_period_num': 133, 'train_rates': 0.7338292199447907, 'learning_rate': 0.0008348328095583125, 'batch_size': 193, 'step_size': 10, 'gamma': 0.7781039875956526}. Best is trial 8 with value: 0.09306125005536009.[0m
[32m[I 2025-01-07 05:19:44,898][0m Trial 9 finished with value: 0.24383580295935922 and parameters: {'observation_period_num': 243, 'train_rates': 0.6096585333216314, 'learning_rate': 0.000461111029964176, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8694577135170426}. Best is trial 8 with value: 0.09306125005536009.[0m
[32m[I 2025-01-07 05:20:28,598][0m Trial 10 finished with value: 0.18508259933896182 and parameters: {'observation_period_num': 168, 'train_rates': 0.8234021560928313, 'learning_rate': 0.00015546418590410425, 'batch_size': 178, 'step_size': 4, 'gamma': 0.7533005290686591}. Best is trial 8 with value: 0.09306125005536009.[0m
[32m[I 2025-01-07 05:20:52,349][0m Trial 11 finished with value: 0.05520294066218656 and parameters: {'observation_period_num': 54, 'train_rates': 0.8073760977220945, 'learning_rate': 0.0009688273209120654, 'batch_size': 255, 'step_size': 6, 'gamma': 0.7625106972933725}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:21:21,854][0m Trial 12 finished with value: 0.0571230479235788 and parameters: {'observation_period_num': 6, 'train_rates': 0.8254789538308407, 'learning_rate': 0.00013625008668163047, 'batch_size': 202, 'step_size': 6, 'gamma': 0.7537098616243194}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:21:50,748][0m Trial 13 finished with value: 0.0666584468939725 and parameters: {'observation_period_num': 14, 'train_rates': 0.8380827759248927, 'learning_rate': 9.016308822968633e-05, 'batch_size': 224, 'step_size': 6, 'gamma': 0.7837602033254909}. Best is trial 11 with value: 0.05520294066218656.[0m
Early stopping at epoch 55
[32m[I 2025-01-07 05:22:16,135][0m Trial 14 finished with value: 0.10711930818417494 and parameters: {'observation_period_num': 8, 'train_rates': 0.8558326820205021, 'learning_rate': 0.00023803326208427127, 'batch_size': 139, 'step_size': 1, 'gamma': 0.7563977087388436}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:22:49,436][0m Trial 15 finished with value: 0.14003552496433258 and parameters: {'observation_period_num': 53, 'train_rates': 0.9887652972424732, 'learning_rate': 8.101733371538254e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.796330810474779}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:23:20,435][0m Trial 16 finished with value: 0.31042641863986187 and parameters: {'observation_period_num': 44, 'train_rates': 0.7842235054583334, 'learning_rate': 1.4073702169249281e-05, 'batch_size': 205, 'step_size': 8, 'gamma': 0.7511013698448222}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:23:59,053][0m Trial 17 finished with value: 0.10734464033790257 and parameters: {'observation_period_num': 82, 'train_rates': 0.7997790605473172, 'learning_rate': 0.0003094946852981752, 'batch_size': 150, 'step_size': 1, 'gamma': 0.9198552318003166}. Best is trial 11 with value: 0.05520294066218656.[0m
[32m[I 2025-01-07 05:24:26,975][0m Trial 18 finished with value: 0.048597783951142844 and parameters: {'observation_period_num': 5, 'train_rates': 0.871656786312178, 'learning_rate': 0.0009487171967033857, 'batch_size': 224, 'step_size': 3, 'gamma': 0.8018827649236908}. Best is trial 18 with value: 0.048597783951142844.[0m
[32m[I 2025-01-07 05:25:19,343][0m Trial 19 finished with value: 0.038720072039312575 and parameters: {'observation_period_num': 29, 'train_rates': 0.8699790530445575, 'learning_rate': 0.0009238002866587449, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8005893903796508}. Best is trial 19 with value: 0.038720072039312575.[0m
Early stopping at epoch 85
[32m[I 2025-01-07 05:26:09,086][0m Trial 20 finished with value: 1.387129783630371 and parameters: {'observation_period_num': 32, 'train_rates': 0.9684096473813535, 'learning_rate': 1.1067149308884908e-06, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8058115722688891}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:26:59,754][0m Trial 21 finished with value: 0.05497434839975735 and parameters: {'observation_period_num': 74, 'train_rates': 0.8739781688559328, 'learning_rate': 0.00046426996720050204, 'batch_size': 111, 'step_size': 4, 'gamma': 0.7758539261809875}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:27:50,502][0m Trial 22 finished with value: 0.05293449084903743 and parameters: {'observation_period_num': 78, 'train_rates': 0.8715473406959733, 'learning_rate': 0.00038257890315260295, 'batch_size': 110, 'step_size': 3, 'gamma': 0.8446152467821029}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:28:51,463][0m Trial 23 finished with value: 0.04119231924414635 and parameters: {'observation_period_num': 24, 'train_rates': 0.8807730697563096, 'learning_rate': 0.00035290399037839166, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8406101372823379}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:30:00,038][0m Trial 24 finished with value: 0.041672086577124644 and parameters: {'observation_period_num': 27, 'train_rates': 0.9294024378594881, 'learning_rate': 0.0008547057570923619, 'batch_size': 86, 'step_size': 3, 'gamma': 0.8936015112940442}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:31:08,307][0m Trial 25 finished with value: 0.0437170902424189 and parameters: {'observation_period_num': 24, 'train_rates': 0.9433324253464389, 'learning_rate': 0.0003177568952406635, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8969102643481447}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:31:55,446][0m Trial 26 finished with value: 0.039817472994327544 and parameters: {'observation_period_num': 33, 'train_rates': 0.897382552221895, 'learning_rate': 0.0005404827563833377, 'batch_size': 128, 'step_size': 2, 'gamma': 0.9024077171640478}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:32:42,076][0m Trial 27 finished with value: 0.1464178942270198 and parameters: {'observation_period_num': 64, 'train_rates': 0.8978754030415298, 'learning_rate': 8.887217003214089e-05, 'batch_size': 127, 'step_size': 1, 'gamma': 0.9151839857836787}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:33:24,006][0m Trial 28 finished with value: 0.2762312591075897 and parameters: {'observation_period_num': 35, 'train_rates': 0.9601811489059353, 'learning_rate': 4.63539577520434e-05, 'batch_size': 151, 'step_size': 2, 'gamma': 0.8624862101693057}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:34:06,972][0m Trial 29 finished with value: 0.05573816644027829 and parameters: {'observation_period_num': 91, 'train_rates': 0.8518896211328385, 'learning_rate': 0.0001494607930007596, 'batch_size': 129, 'step_size': 5, 'gamma': 0.8818663629741363}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:34:43,150][0m Trial 30 finished with value: 0.7207380714087651 and parameters: {'observation_period_num': 144, 'train_rates': 0.8968391831147559, 'learning_rate': 1.0091032253910175e-05, 'batch_size': 171, 'step_size': 2, 'gamma': 0.820575807676408}. Best is trial 19 with value: 0.038720072039312575.[0m
[32m[I 2025-01-07 05:35:53,735][0m Trial 31 finished with value: 0.037765400571243786 and parameters: {'observation_period_num': 29, 'train_rates': 0.9269418558934589, 'learning_rate': 0.0006023559669411848, 'batch_size': 84, 'step_size': 3, 'gamma': 0.9190949877584685}. Best is trial 31 with value: 0.037765400571243786.[0m
[32m[I 2025-01-07 05:37:02,114][0m Trial 32 finished with value: 0.050061848081715474 and parameters: {'observation_period_num': 37, 'train_rates': 0.9240085530870277, 'learning_rate': 0.0006152637818603202, 'batch_size': 86, 'step_size': 5, 'gamma': 0.9127235347843784}. Best is trial 31 with value: 0.037765400571243786.[0m
[32m[I 2025-01-07 05:38:39,969][0m Trial 33 finished with value: 0.07319471629980569 and parameters: {'observation_period_num': 93, 'train_rates': 0.8879435757385917, 'learning_rate': 0.00023900746417335978, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9422751813940853}. Best is trial 31 with value: 0.037765400571243786.[0m
[32m[I 2025-01-07 05:39:43,554][0m Trial 34 finished with value: 0.03241016858926982 and parameters: {'observation_period_num': 20, 'train_rates': 0.9177111313892508, 'learning_rate': 0.0006473094053747668, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8858469081159696}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:40:59,977][0m Trial 35 finished with value: 0.06937226160351499 and parameters: {'observation_period_num': 61, 'train_rates': 0.9134108486862299, 'learning_rate': 0.0006031852349378283, 'batch_size': 74, 'step_size': 4, 'gamma': 0.9738325013287671}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:43:18,068][0m Trial 36 finished with value: 0.08661574267960609 and parameters: {'observation_period_num': 22, 'train_rates': 0.9521330592056968, 'learning_rate': 0.0006231933982100287, 'batch_size': 42, 'step_size': 8, 'gamma': 0.933886870361901}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:44:02,028][0m Trial 37 finished with value: 0.10786479779678147 and parameters: {'observation_period_num': 41, 'train_rates': 0.7747633535624598, 'learning_rate': 0.00021726530836757747, 'batch_size': 124, 'step_size': 1, 'gamma': 0.883511682937424}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:44:43,841][0m Trial 38 finished with value: 0.40791890025138855 and parameters: {'observation_period_num': 217, 'train_rates': 0.986714538795337, 'learning_rate': 6.791079804770725e-06, 'batch_size': 147, 'step_size': 5, 'gamma': 0.9503372033473931}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:48:50,594][0m Trial 39 finished with value: 0.05957373970650076 and parameters: {'observation_period_num': 102, 'train_rates': 0.9137181733302444, 'learning_rate': 2.618945909499e-05, 'batch_size': 22, 'step_size': 13, 'gamma': 0.9019413785552841}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:49:39,511][0m Trial 40 finished with value: 0.07935011774166524 and parameters: {'observation_period_num': 64, 'train_rates': 0.6944901083318553, 'learning_rate': 0.000463515352099076, 'batch_size': 99, 'step_size': 7, 'gamma': 0.8816482270992395}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:50:41,065][0m Trial 41 finished with value: 0.05301620379763808 and parameters: {'observation_period_num': 20, 'train_rates': 0.939766893629351, 'learning_rate': 0.00034035853738155497, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8509756536948269}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:51:55,729][0m Trial 42 finished with value: 0.03564325214737852 and parameters: {'observation_period_num': 18, 'train_rates': 0.887436118182847, 'learning_rate': 0.0007660733513228346, 'batch_size': 76, 'step_size': 2, 'gamma': 0.8345128963644022}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:53:05,225][0m Trial 43 finished with value: 0.038274118985708164 and parameters: {'observation_period_num': 47, 'train_rates': 0.8589787181402342, 'learning_rate': 0.0006718154241522996, 'batch_size': 78, 'step_size': 2, 'gamma': 0.9242280327072677}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:54:31,480][0m Trial 44 finished with value: 0.0819209041272072 and parameters: {'observation_period_num': 47, 'train_rates': 0.8544635093315233, 'learning_rate': 0.0007198749765744698, 'batch_size': 63, 'step_size': 4, 'gamma': 0.9278818198915082}. Best is trial 34 with value: 0.03241016858926982.[0m
[32m[I 2025-01-07 05:55:46,124][0m Trial 45 finished with value: 0.031076290872036002 and parameters: {'observation_period_num': 15, 'train_rates': 0.9137639894106382, 'learning_rate': 0.0009193782982711474, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9584505156140349}. Best is trial 45 with value: 0.031076290872036002.[0m
[32m[I 2025-01-07 05:57:03,064][0m Trial 46 finished with value: 0.03309127433129041 and parameters: {'observation_period_num': 13, 'train_rates': 0.9147213128789677, 'learning_rate': 0.0007575218974023146, 'batch_size': 76, 'step_size': 1, 'gamma': 0.9851709537536028}. Best is trial 45 with value: 0.031076290872036002.[0m
[32m[I 2025-01-07 05:59:13,638][0m Trial 47 finished with value: 0.17258358374238014 and parameters: {'observation_period_num': 14, 'train_rates': 0.9632772363797578, 'learning_rate': 3.277038522943599e-06, 'batch_size': 45, 'step_size': 1, 'gamma': 0.9832741736214285}. Best is trial 45 with value: 0.031076290872036002.[0m
[32m[I 2025-01-07 06:00:57,296][0m Trial 48 finished with value: 0.11811007665736335 and parameters: {'observation_period_num': 181, 'train_rates': 0.9193430762067709, 'learning_rate': 0.00043000452009721616, 'batch_size': 52, 'step_size': 1, 'gamma': 0.9684031411396267}. Best is trial 45 with value: 0.031076290872036002.[0m
[32m[I 2025-01-07 06:02:15,687][0m Trial 49 finished with value: 0.05577336216068014 and parameters: {'observation_period_num': 14, 'train_rates': 0.9360861499754694, 'learning_rate': 0.0001135465981913446, 'batch_size': 75, 'step_size': 10, 'gamma': 0.9634022172703919}. Best is trial 45 with value: 0.031076290872036002.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 06:02:15,697][0m A new study created in memory with name: no-name-5128d8dc-549b-476d-a0f5-0a59805a48b8[0m
[32m[I 2025-01-07 06:02:53,482][0m Trial 0 finished with value: 0.42228400869802996 and parameters: {'observation_period_num': 183, 'train_rates': 0.9404472700675273, 'learning_rate': 6.933257506258052e-06, 'batch_size': 159, 'step_size': 14, 'gamma': 0.8276331522884981}. Best is trial 0 with value: 0.42228400869802996.[0m
[32m[I 2025-01-07 06:03:25,646][0m Trial 1 finished with value: 0.7282493114471436 and parameters: {'observation_period_num': 210, 'train_rates': 0.9627853558035514, 'learning_rate': 1.3746125897633918e-06, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9086313816010684}. Best is trial 0 with value: 0.42228400869802996.[0m
[32m[I 2025-01-07 06:04:13,371][0m Trial 2 finished with value: 0.07912206586281006 and parameters: {'observation_period_num': 69, 'train_rates': 0.664469899544173, 'learning_rate': 0.00021765133241698505, 'batch_size': 99, 'step_size': 4, 'gamma': 0.852535918240463}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:04:47,730][0m Trial 3 finished with value: 0.20908598396025838 and parameters: {'observation_period_num': 223, 'train_rates': 0.6366457226084136, 'learning_rate': 0.0003457562016668351, 'batch_size': 129, 'step_size': 14, 'gamma': 0.8566850661986736}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:06:27,231][0m Trial 4 finished with value: 0.22982651394667084 and parameters: {'observation_period_num': 70, 'train_rates': 0.6973006773465871, 'learning_rate': 1.6734253469365007e-05, 'batch_size': 46, 'step_size': 2, 'gamma': 0.9283438793387317}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:07:02,787][0m Trial 5 finished with value: 0.09536366385515048 and parameters: {'observation_period_num': 52, 'train_rates': 0.6079990917122452, 'learning_rate': 0.0002234468322801281, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9311080278315113}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:07:36,527][0m Trial 6 finished with value: 0.20085982572434197 and parameters: {'observation_period_num': 89, 'train_rates': 0.6104365500055347, 'learning_rate': 8.062962576643973e-05, 'batch_size': 136, 'step_size': 9, 'gamma': 0.8891727617428191}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:10:42,019][0m Trial 7 finished with value: 0.1640629906545986 and parameters: {'observation_period_num': 12, 'train_rates': 0.9813618413042249, 'learning_rate': 1.042355209258853e-06, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9604845587042024}. Best is trial 2 with value: 0.07912206586281006.[0m
Early stopping at epoch 35
[32m[I 2025-01-07 06:10:55,759][0m Trial 8 finished with value: 3.295703705436632 and parameters: {'observation_period_num': 231, 'train_rates': 0.8872706350925382, 'learning_rate': 1.070362271367877e-06, 'batch_size': 161, 'step_size': 1, 'gamma': 0.7501349708492842}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:11:35,194][0m Trial 9 finished with value: 0.36837005615234375 and parameters: {'observation_period_num': 199, 'train_rates': 0.9766822351674116, 'learning_rate': 4.40759451045603e-05, 'batch_size': 155, 'step_size': 3, 'gamma': 0.7850644325400177}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:12:40,223][0m Trial 10 finished with value: 0.08851374226910516 and parameters: {'observation_period_num': 139, 'train_rates': 0.7717546175379115, 'learning_rate': 0.0007356978867705278, 'batch_size': 76, 'step_size': 6, 'gamma': 0.8218171893535057}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:13:41,383][0m Trial 11 finished with value: 0.09382611503069466 and parameters: {'observation_period_num': 138, 'train_rates': 0.7634497182769532, 'learning_rate': 0.0008127458270581312, 'batch_size': 81, 'step_size': 6, 'gamma': 0.8226703277325518}. Best is trial 2 with value: 0.07912206586281006.[0m
[32m[I 2025-01-07 06:14:43,652][0m Trial 12 finished with value: 0.07252976298332214 and parameters: {'observation_period_num': 131, 'train_rates': 0.8166568492891269, 'learning_rate': 0.000713283051123517, 'batch_size': 84, 'step_size': 5, 'gamma': 0.8423022433305903}. Best is trial 12 with value: 0.07252976298332214.[0m
[32m[I 2025-01-07 06:15:09,110][0m Trial 13 finished with value: 0.1121783358129588 and parameters: {'observation_period_num': 111, 'train_rates': 0.8528958245402951, 'learning_rate': 0.00016782774253216038, 'batch_size': 247, 'step_size': 4, 'gamma': 0.8624977112919198}. Best is trial 12 with value: 0.07252976298332214.[0m
[32m[I 2025-01-07 06:16:03,889][0m Trial 14 finished with value: 0.11206604434672357 and parameters: {'observation_period_num': 164, 'train_rates': 0.6990805666710542, 'learning_rate': 9.686388166672446e-05, 'batch_size': 87, 'step_size': 9, 'gamma': 0.7935987302411144}. Best is trial 12 with value: 0.07252976298332214.[0m
[32m[I 2025-01-07 06:16:57,408][0m Trial 15 finished with value: 0.037901564497839324 and parameters: {'observation_period_num': 36, 'train_rates': 0.8116914072478963, 'learning_rate': 0.00034345076727243653, 'batch_size': 103, 'step_size': 4, 'gamma': 0.985306629414304}. Best is trial 15 with value: 0.037901564497839324.[0m
[32m[I 2025-01-07 06:22:21,486][0m Trial 16 finished with value: 0.02504583232012796 and parameters: {'observation_period_num': 7, 'train_rates': 0.8351250925913493, 'learning_rate': 0.0009363199146312423, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9867509888919517}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:26:58,291][0m Trial 17 finished with value: 0.026213599795070083 and parameters: {'observation_period_num': 11, 'train_rates': 0.9079596416388567, 'learning_rate': 0.00040475001334104366, 'batch_size': 20, 'step_size': 8, 'gamma': 0.9894491665964753}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:29:01,133][0m Trial 18 finished with value: 0.04156935422548226 and parameters: {'observation_period_num': 5, 'train_rates': 0.881495483160165, 'learning_rate': 1.622534072548127e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.9894577833214808}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:32:27,554][0m Trial 19 finished with value: 0.04197283387323147 and parameters: {'observation_period_num': 34, 'train_rates': 0.9084327644480064, 'learning_rate': 0.00044396698480903235, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9568805539255312}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:37:41,785][0m Trial 20 finished with value: 0.07520850134980199 and parameters: {'observation_period_num': 99, 'train_rates': 0.8433078567611213, 'learning_rate': 8.237457556746785e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.955097704317081}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:39:27,939][0m Trial 21 finished with value: 0.044245688435572195 and parameters: {'observation_period_num': 32, 'train_rates': 0.8064856164134786, 'learning_rate': 0.0003414400866356411, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9862456593406708}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:41:03,122][0m Trial 22 finished with value: 0.05049179990064297 and parameters: {'observation_period_num': 30, 'train_rates': 0.750832500172994, 'learning_rate': 0.0009764292613369826, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9787041253864245}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:47:03,952][0m Trial 23 finished with value: 0.049491001328373974 and parameters: {'observation_period_num': 54, 'train_rates': 0.9274320200977523, 'learning_rate': 0.00045546766226801347, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9365796217529172}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:48:12,511][0m Trial 24 finished with value: 0.03519082585270506 and parameters: {'observation_period_num': 19, 'train_rates': 0.8478261548771986, 'learning_rate': 0.00014088144609625223, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9684957851848927}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:49:05,832][0m Trial 25 finished with value: 0.1735311609077576 and parameters: {'observation_period_num': 252, 'train_rates': 0.8561164699413996, 'learning_rate': 0.000141270758992913, 'batch_size': 199, 'step_size': 7, 'gamma': 0.9663607744995846}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:51:06,421][0m Trial 26 finished with value: 0.0322577984497687 and parameters: {'observation_period_num': 12, 'train_rates': 0.8898576439886958, 'learning_rate': 4.681553520264771e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9056125826368837}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:53:36,700][0m Trial 27 finished with value: 0.09491488520727782 and parameters: {'observation_period_num': 5, 'train_rates': 0.8968302443637105, 'learning_rate': 2.5462847382797098e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.8897987061901297}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 06:55:25,233][0m Trial 28 finished with value: 0.06597722755380409 and parameters: {'observation_period_num': 60, 'train_rates': 0.9348898418244258, 'learning_rate': 3.356626257237697e-05, 'batch_size': 61, 'step_size': 10, 'gamma': 0.9025281296567583}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:01:39,991][0m Trial 29 finished with value: 0.08411658710281321 and parameters: {'observation_period_num': 87, 'train_rates': 0.9483214908449836, 'learning_rate': 5.594365465582223e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9380380860198038}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:03:10,766][0m Trial 30 finished with value: 0.06326697431817777 and parameters: {'observation_period_num': 38, 'train_rates': 0.9184332991354454, 'learning_rate': 1.8010411683315024e-05, 'batch_size': 66, 'step_size': 15, 'gamma': 0.9162835087057662}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:05:39,386][0m Trial 31 finished with value: 0.03200817573815584 and parameters: {'observation_period_num': 22, 'train_rates': 0.8739293080710115, 'learning_rate': 5.399797381982448e-05, 'batch_size': 37, 'step_size': 7, 'gamma': 0.9685749890907385}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:08:19,111][0m Trial 32 finished with value: 0.033423163961353045 and parameters: {'observation_period_num': 22, 'train_rates': 0.8662846061443339, 'learning_rate': 5.598456935789619e-05, 'batch_size': 35, 'step_size': 8, 'gamma': 0.944771988031745}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:10:33,242][0m Trial 33 finished with value: 0.061300847850739956 and parameters: {'observation_period_num': 46, 'train_rates': 0.828074966643522, 'learning_rate': 1.0127338404128763e-05, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9749971411626925}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:12:17,107][0m Trial 34 finished with value: 0.04080491498708725 and parameters: {'observation_period_num': 19, 'train_rates': 0.7869645820675775, 'learning_rate': 2.9695732114554208e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9521215901901179}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:15:19,594][0m Trial 35 finished with value: 0.12557234366734824 and parameters: {'observation_period_num': 71, 'train_rates': 0.9533009611423389, 'learning_rate': 5.069002273424946e-06, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9717819723049792}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:16:39,549][0m Trial 36 finished with value: 0.03549485074320279 and parameters: {'observation_period_num': 19, 'train_rates': 0.8762855837728561, 'learning_rate': 0.00021476190452409023, 'batch_size': 71, 'step_size': 11, 'gamma': 0.9191536218819365}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:20:09,215][0m Trial 37 finished with value: 0.05080282946802535 and parameters: {'observation_period_num': 65, 'train_rates': 0.908336003963943, 'learning_rate': 2.5685676663617793e-05, 'batch_size': 26, 'step_size': 13, 'gamma': 0.8883009523100974}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:21:49,888][0m Trial 38 finished with value: 0.06821406503043485 and parameters: {'observation_period_num': 46, 'train_rates': 0.7409976619316796, 'learning_rate': 5.853305941117136e-05, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8758611765353159}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:22:42,280][0m Trial 39 finished with value: 0.025684086302254595 and parameters: {'observation_period_num': 6, 'train_rates': 0.8294784946761755, 'learning_rate': 0.0005447597279088165, 'batch_size': 121, 'step_size': 9, 'gamma': 0.9467445179682965}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:23:23,169][0m Trial 40 finished with value: 0.09695519126190212 and parameters: {'observation_period_num': 73, 'train_rates': 0.8268717468653558, 'learning_rate': 0.0005428769752574724, 'batch_size': 190, 'step_size': 6, 'gamma': 0.950865201038566}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:24:15,227][0m Trial 41 finished with value: 0.0314023304306637 and parameters: {'observation_period_num': 9, 'train_rates': 0.7912569849681029, 'learning_rate': 0.0005632529917932399, 'batch_size': 116, 'step_size': 9, 'gamma': 0.925977575638}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:25:01,437][0m Trial 42 finished with value: 0.03049543536948636 and parameters: {'observation_period_num': 10, 'train_rates': 0.7984101081830068, 'learning_rate': 0.00025848398958079545, 'batch_size': 142, 'step_size': 8, 'gamma': 0.9251996865540154}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:26:04,474][0m Trial 43 finished with value: 0.03257338822357298 and parameters: {'observation_period_num': 9, 'train_rates': 0.7845578783537848, 'learning_rate': 0.0002613959147856235, 'batch_size': 118, 'step_size': 10, 'gamma': 0.928258451831667}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:26:47,973][0m Trial 44 finished with value: 0.053760224224432655 and parameters: {'observation_period_num': 42, 'train_rates': 0.7233869771798567, 'learning_rate': 0.0005947708556362713, 'batch_size': 145, 'step_size': 9, 'gamma': 0.917885300103667}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:27:31,081][0m Trial 45 finished with value: 0.04266263934480677 and parameters: {'observation_period_num': 26, 'train_rates': 0.7730913286182006, 'learning_rate': 0.0009778099098686137, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9263052268634976}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:28:23,173][0m Trial 46 finished with value: 0.029177867026642115 and parameters: {'observation_period_num': 6, 'train_rates': 0.7984989312395797, 'learning_rate': 0.0003031668408516262, 'batch_size': 124, 'step_size': 8, 'gamma': 0.945427859108325}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:29:12,955][0m Trial 47 finished with value: 0.04651298580898179 and parameters: {'observation_period_num': 57, 'train_rates': 0.8259222129393997, 'learning_rate': 0.0002805512154736778, 'batch_size': 128, 'step_size': 8, 'gamma': 0.946491639499635}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:29:49,852][0m Trial 48 finished with value: 0.11947786069919016 and parameters: {'observation_period_num': 161, 'train_rates': 0.7317369523904118, 'learning_rate': 0.0001932692005353974, 'batch_size': 238, 'step_size': 8, 'gamma': 0.9812982678851327}. Best is trial 16 with value: 0.02504583232012796.[0m
[32m[I 2025-01-07 07:30:53,031][0m Trial 49 finished with value: 0.03141226771650678 and parameters: {'observation_period_num': 5, 'train_rates': 0.8000350963699326, 'learning_rate': 0.00040211962717003855, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9448305208318573}. Best is trial 16 with value: 0.02504583232012796.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 07:30:53,042][0m A new study created in memory with name: no-name-a8d2ec6a-bff3-4f8c-a4e2-6c57bc26eb0f[0m
[32m[I 2025-01-07 07:31:31,138][0m Trial 0 finished with value: 0.2051726359034508 and parameters: {'observation_period_num': 88, 'train_rates': 0.6036912762769971, 'learning_rate': 8.073939929313216e-05, 'batch_size': 250, 'step_size': 7, 'gamma': 0.9370150721696532}. Best is trial 0 with value: 0.2051726359034508.[0m
[32m[I 2025-01-07 07:32:11,901][0m Trial 1 finished with value: 0.4979340434074402 and parameters: {'observation_period_num': 244, 'train_rates': 0.9603311117486063, 'learning_rate': 1.274724505123466e-05, 'batch_size': 201, 'step_size': 9, 'gamma': 0.7776052415003942}. Best is trial 0 with value: 0.2051726359034508.[0m
[32m[I 2025-01-07 07:33:00,061][0m Trial 2 finished with value: 0.18770318333277855 and parameters: {'observation_period_num': 186, 'train_rates': 0.8087574999221468, 'learning_rate': 1.5920751939193535e-05, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9745354871325307}. Best is trial 2 with value: 0.18770318333277855.[0m
[32m[I 2025-01-07 07:33:49,842][0m Trial 3 finished with value: 0.47825224629651447 and parameters: {'observation_period_num': 209, 'train_rates': 0.7602352311988955, 'learning_rate': 5.099328260496486e-06, 'batch_size': 110, 'step_size': 11, 'gamma': 0.888791594663755}. Best is trial 2 with value: 0.18770318333277855.[0m
[32m[I 2025-01-07 07:34:28,895][0m Trial 4 finished with value: 0.629839151689451 and parameters: {'observation_period_num': 244, 'train_rates': 0.7034836111352685, 'learning_rate': 3.5957018310408534e-06, 'batch_size': 187, 'step_size': 13, 'gamma': 0.8798110421992713}. Best is trial 2 with value: 0.18770318333277855.[0m
[32m[I 2025-01-07 07:35:10,938][0m Trial 5 finished with value: 0.03890417927503586 and parameters: {'observation_period_num': 8, 'train_rates': 0.830485985101252, 'learning_rate': 0.00014771818958720149, 'batch_size': 244, 'step_size': 10, 'gamma': 0.9567310491355969}. Best is trial 5 with value: 0.03890417927503586.[0m
[32m[I 2025-01-07 07:35:50,357][0m Trial 6 finished with value: 0.4037659168243408 and parameters: {'observation_period_num': 121, 'train_rates': 0.953983623074361, 'learning_rate': 1.1443059997213783e-05, 'batch_size': 241, 'step_size': 9, 'gamma': 0.7740039955005424}. Best is trial 5 with value: 0.03890417927503586.[0m
[32m[I 2025-01-07 07:38:40,363][0m Trial 7 finished with value: 0.13353422157487518 and parameters: {'observation_period_num': 233, 'train_rates': 0.7406987898188852, 'learning_rate': 5.616593390089049e-05, 'batch_size': 26, 'step_size': 2, 'gamma': 0.98100108045585}. Best is trial 5 with value: 0.03890417927503586.[0m
[32m[I 2025-01-07 07:39:08,095][0m Trial 8 finished with value: 0.12381086992117689 and parameters: {'observation_period_num': 126, 'train_rates': 0.6300795182567124, 'learning_rate': 7.431967556709261e-05, 'batch_size': 241, 'step_size': 13, 'gamma': 0.9847180411161301}. Best is trial 5 with value: 0.03890417927503586.[0m
[32m[I 2025-01-07 07:39:48,097][0m Trial 9 finished with value: 0.14861111260529525 and parameters: {'observation_period_num': 218, 'train_rates': 0.8511980543835733, 'learning_rate': 9.094496812462608e-05, 'batch_size': 161, 'step_size': 8, 'gamma': 0.8798823572920871}. Best is trial 5 with value: 0.03890417927503586.[0m
[32m[I 2025-01-07 07:40:55,430][0m Trial 10 finished with value: 0.02341206560787909 and parameters: {'observation_period_num': 9, 'train_rates': 0.8846627773097425, 'learning_rate': 0.0009020752840000765, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9286077608627766}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:42:03,318][0m Trial 11 finished with value: 0.026096466336303248 and parameters: {'observation_period_num': 12, 'train_rates': 0.8814201512437196, 'learning_rate': 0.0006493893112659455, 'batch_size': 97, 'step_size': 4, 'gamma': 0.938639477326679}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:43:17,050][0m Trial 12 finished with value: 0.02421890871005346 and parameters: {'observation_period_num': 8, 'train_rates': 0.8905702763237021, 'learning_rate': 0.0008945155336254121, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9211615737454382}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:44:53,946][0m Trial 13 finished with value: 0.06729024839039673 and parameters: {'observation_period_num': 58, 'train_rates': 0.9060704930759518, 'learning_rate': 0.0009058521861508131, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9172890593088057}. Best is trial 10 with value: 0.02341206560787909.[0m
Early stopping at epoch 74
[32m[I 2025-01-07 07:45:44,159][0m Trial 14 finished with value: 0.12008180526807961 and parameters: {'observation_period_num': 49, 'train_rates': 0.911056288014574, 'learning_rate': 0.0003209740869838265, 'batch_size': 89, 'step_size': 1, 'gamma': 0.8333048207998979}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:47:40,736][0m Trial 15 finished with value: 0.03893081843852997 and parameters: {'observation_period_num': 45, 'train_rates': 0.9896228656232993, 'learning_rate': 0.0002724836523965453, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8339722047168835}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:48:30,503][0m Trial 16 finished with value: 0.7125910999268059 and parameters: {'observation_period_num': 165, 'train_rates': 0.8636236536368874, 'learning_rate': 1.5107109526721896e-06, 'batch_size': 125, 'step_size': 3, 'gamma': 0.9122937416938172}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:49:45,862][0m Trial 17 finished with value: 0.07021957487115844 and parameters: {'observation_period_num': 87, 'train_rates': 0.7903914575061446, 'learning_rate': 0.0003608666102821014, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9094719733834647}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:50:36,679][0m Trial 18 finished with value: 0.03811207413673401 and parameters: {'observation_period_num': 31, 'train_rates': 0.9213098767708798, 'learning_rate': 0.0009733350755237334, 'batch_size': 135, 'step_size': 3, 'gamma': 0.8510470009233781}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:54:50,608][0m Trial 19 finished with value: 0.07277578443966129 and parameters: {'observation_period_num': 81, 'train_rates': 0.885175374597506, 'learning_rate': 3.445858884685424e-05, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9476121239888847}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:56:33,852][0m Trial 20 finished with value: 0.04663381982590718 and parameters: {'observation_period_num': 6, 'train_rates': 0.684751293983712, 'learning_rate': 0.0005473548023262331, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8525511245279718}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:57:33,128][0m Trial 21 finished with value: 0.032895423989910424 and parameters: {'observation_period_num': 25, 'train_rates': 0.8785656595014469, 'learning_rate': 0.00018768804344056165, 'batch_size': 97, 'step_size': 4, 'gamma': 0.9316104145994007}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:58:37,026][0m Trial 22 finished with value: 0.06995598816884165 and parameters: {'observation_period_num': 67, 'train_rates': 0.8352413865865969, 'learning_rate': 0.0005545949758014299, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9611683558607709}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 07:59:31,156][0m Trial 23 finished with value: 0.0385383142508226 and parameters: {'observation_period_num': 25, 'train_rates': 0.9328624528769074, 'learning_rate': 0.0005742300807661042, 'batch_size': 113, 'step_size': 3, 'gamma': 0.8987207953739051}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 08:00:08,575][0m Trial 24 finished with value: 0.033107020538942565 and parameters: {'observation_period_num': 5, 'train_rates': 0.7936180967020068, 'learning_rate': 0.0009492887531890196, 'batch_size': 150, 'step_size': 5, 'gamma': 0.9307418117833545}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 08:01:19,701][0m Trial 25 finished with value: 0.06810973722885989 and parameters: {'observation_period_num': 104, 'train_rates': 0.8884148841826991, 'learning_rate': 0.00016484835029021683, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9243998807538657}. Best is trial 10 with value: 0.02341206560787909.[0m
[32m[I 2025-01-07 08:02:15,050][0m Trial 26 finished with value: 0.022351626306772232 and parameters: {'observation_period_num': 33, 'train_rates': 0.9888740408441951, 'learning_rate': 0.0004707699420385272, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9461332238832573}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:03:08,862][0m Trial 27 finished with value: 0.03854124993085861 and parameters: {'observation_period_num': 31, 'train_rates': 0.9736155740444663, 'learning_rate': 0.0003603211050897823, 'batch_size': 118, 'step_size': 4, 'gamma': 0.9614526831822877}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:05:10,814][0m Trial 28 finished with value: 0.09978594271827945 and parameters: {'observation_period_num': 164, 'train_rates': 0.950436566914079, 'learning_rate': 0.0002269665536576907, 'batch_size': 45, 'step_size': 2, 'gamma': 0.9009587616252736}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:05:56,585][0m Trial 29 finished with value: 0.06414911334176321 and parameters: {'observation_period_num': 72, 'train_rates': 0.9359511728536342, 'learning_rate': 0.00011793024378418582, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9418672292058932}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:06:35,224][0m Trial 30 finished with value: 0.060132621257912876 and parameters: {'observation_period_num': 101, 'train_rates': 0.8212009708531315, 'learning_rate': 0.00040054812480440897, 'batch_size': 146, 'step_size': 6, 'gamma': 0.7984716928859372}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:07:31,659][0m Trial 31 finished with value: 0.05259752227943771 and parameters: {'observation_period_num': 41, 'train_rates': 0.8993030490872143, 'learning_rate': 0.0006658841289870235, 'batch_size': 104, 'step_size': 4, 'gamma': 0.943264383408525}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:08:29,549][0m Trial 32 finished with value: 0.027504986455793575 and parameters: {'observation_period_num': 19, 'train_rates': 0.8578531298550214, 'learning_rate': 0.0006177060515545869, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9585686907196924}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:09:51,298][0m Trial 33 finished with value: 0.05995388329029083 and parameters: {'observation_period_num': 56, 'train_rates': 0.9898557268900932, 'learning_rate': 0.0009982620421018608, 'batch_size': 75, 'step_size': 4, 'gamma': 0.9265741004802555}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:10:22,625][0m Trial 34 finished with value: 0.04455397062095031 and parameters: {'observation_period_num': 18, 'train_rates': 0.7780285626544159, 'learning_rate': 0.0004787076307585312, 'batch_size': 186, 'step_size': 5, 'gamma': 0.9708115769730801}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:11:09,446][0m Trial 35 finished with value: 0.032895168649589586 and parameters: {'observation_period_num': 33, 'train_rates': 0.8674100476761247, 'learning_rate': 0.0006886672908662718, 'batch_size': 121, 'step_size': 3, 'gamma': 0.8930239166578012}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:12:07,323][0m Trial 36 finished with value: 0.03182765245846995 and parameters: {'observation_period_num': 14, 'train_rates': 0.9381928390734533, 'learning_rate': 0.0002755602085382645, 'batch_size': 103, 'step_size': 8, 'gamma': 0.9453990424842279}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:13:11,632][0m Trial 37 finished with value: 0.17831327878098632 and parameters: {'observation_period_num': 45, 'train_rates': 0.8400044055314326, 'learning_rate': 8.446314758039382e-06, 'batch_size': 86, 'step_size': 6, 'gamma': 0.867648989336982}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:13:50,819][0m Trial 38 finished with value: 0.07068789005279541 and parameters: {'observation_period_num': 5, 'train_rates': 0.9662679156779078, 'learning_rate': 2.1507820842871534e-05, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9720737991259412}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:15:10,387][0m Trial 39 finished with value: 0.2565008958755122 and parameters: {'observation_period_num': 63, 'train_rates': 0.7366153727979408, 'learning_rate': 4.575673937167044e-05, 'batch_size': 62, 'step_size': 2, 'gamma': 0.7567879255181669}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:15:47,902][0m Trial 40 finished with value: 0.055646919395013525 and parameters: {'observation_period_num': 38, 'train_rates': 0.8088131280419966, 'learning_rate': 0.00013779800015078283, 'batch_size': 150, 'step_size': 4, 'gamma': 0.9873913467774084}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:16:47,996][0m Trial 41 finished with value: 0.02848268341395497 and parameters: {'observation_period_num': 21, 'train_rates': 0.8549215380881952, 'learning_rate': 0.0006880013604240159, 'batch_size': 95, 'step_size': 2, 'gamma': 0.9550771887088849}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:17:45,220][0m Trial 42 finished with value: 0.05106995523217236 and parameters: {'observation_period_num': 19, 'train_rates': 0.915907203960498, 'learning_rate': 0.00042857174555035603, 'batch_size': 112, 'step_size': 1, 'gamma': 0.920682537919263}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:18:54,265][0m Trial 43 finished with value: 0.025576328647469077 and parameters: {'observation_period_num': 13, 'train_rates': 0.8202074769837282, 'learning_rate': 0.0007594337519837458, 'batch_size': 82, 'step_size': 3, 'gamma': 0.9680460482219124}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:20:00,963][0m Trial 44 finished with value: 0.05214549962963377 and parameters: {'observation_period_num': 54, 'train_rates': 0.758542644598109, 'learning_rate': 0.00021713916434879486, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9364598384363615}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:21:25,524][0m Trial 45 finished with value: 0.13704512506210553 and parameters: {'observation_period_num': 155, 'train_rates': 0.813119942580907, 'learning_rate': 0.0007230291110295694, 'batch_size': 62, 'step_size': 5, 'gamma': 0.9705565368876746}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:22:15,654][0m Trial 46 finished with value: 0.02903109429123467 and parameters: {'observation_period_num': 14, 'train_rates': 0.9008023752936448, 'learning_rate': 0.0004380171079738045, 'batch_size': 125, 'step_size': 4, 'gamma': 0.9065068915014196}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:23:08,695][0m Trial 47 finished with value: 0.1491898839876532 and parameters: {'observation_period_num': 196, 'train_rates': 0.8412672146083768, 'learning_rate': 0.0002818872190709411, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9519166789343725}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:23:44,906][0m Trial 48 finished with value: 0.9239415285710074 and parameters: {'observation_period_num': 34, 'train_rates': 0.8735540504440668, 'learning_rate': 1.4124283944658058e-06, 'batch_size': 256, 'step_size': 5, 'gamma': 0.8825657764701981}. Best is trial 26 with value: 0.022351626306772232.[0m
[32m[I 2025-01-07 08:25:08,094][0m Trial 49 finished with value: 0.1397580276889774 and parameters: {'observation_period_num': 146, 'train_rates': 0.6226757550298957, 'learning_rate': 9.646115743036903e-05, 'batch_size': 52, 'step_size': 3, 'gamma': 0.9807781881975224}. Best is trial 26 with value: 0.022351626306772232.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 08:25:08,104][0m A new study created in memory with name: no-name-8f99df84-6d72-46f7-beb0-be3e25d4f9e8[0m
[32m[I 2025-01-07 08:26:05,558][0m Trial 0 finished with value: 0.5031214691343762 and parameters: {'observation_period_num': 117, 'train_rates': 0.9261192278819375, 'learning_rate': 3.499012578615786e-06, 'batch_size': 134, 'step_size': 9, 'gamma': 0.9340516902475445}. Best is trial 0 with value: 0.5031214691343762.[0m
[32m[I 2025-01-07 08:27:14,927][0m Trial 1 finished with value: 0.23595311746154865 and parameters: {'observation_period_num': 44, 'train_rates': 0.8666713902872374, 'learning_rate': 3.91622947646765e-06, 'batch_size': 91, 'step_size': 14, 'gamma': 0.9369293186661172}. Best is trial 1 with value: 0.23595311746154865.[0m
[32m[I 2025-01-07 08:28:07,761][0m Trial 2 finished with value: 0.9000840607133608 and parameters: {'observation_period_num': 197, 'train_rates': 0.7934808356434028, 'learning_rate': 1.0357549538759505e-06, 'batch_size': 207, 'step_size': 3, 'gamma': 0.8084007858968402}. Best is trial 1 with value: 0.23595311746154865.[0m
[32m[I 2025-01-07 08:28:53,307][0m Trial 3 finished with value: 0.2086416855640245 and parameters: {'observation_period_num': 166, 'train_rates': 0.8563070241767871, 'learning_rate': 3.2751983472149156e-05, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9607181535020421}. Best is trial 3 with value: 0.2086416855640245.[0m
[32m[I 2025-01-07 08:31:44,753][0m Trial 4 finished with value: 0.1586789165242271 and parameters: {'observation_period_num': 107, 'train_rates': 0.7288475410589573, 'learning_rate': 0.0003792940370369366, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9104730046683956}. Best is trial 4 with value: 0.1586789165242271.[0m
[32m[I 2025-01-07 08:32:38,794][0m Trial 5 finished with value: 0.14938057871425853 and parameters: {'observation_period_num': 197, 'train_rates': 0.7594150108400495, 'learning_rate': 0.0003212846687329948, 'batch_size': 155, 'step_size': 2, 'gamma': 0.9684223490580104}. Best is trial 5 with value: 0.14938057871425853.[0m
[32m[I 2025-01-07 08:33:47,989][0m Trial 6 finished with value: 0.08882825374603272 and parameters: {'observation_period_num': 83, 'train_rates': 0.9427138761664948, 'learning_rate': 7.472268546685424e-05, 'batch_size': 108, 'step_size': 9, 'gamma': 0.9164723206879253}. Best is trial 6 with value: 0.08882825374603272.[0m
[32m[I 2025-01-07 08:34:55,660][0m Trial 7 finished with value: 0.7413564316391745 and parameters: {'observation_period_num': 152, 'train_rates': 0.7870507417110344, 'learning_rate': 1.8512565880515352e-06, 'batch_size': 95, 'step_size': 8, 'gamma': 0.8640807069016782}. Best is trial 6 with value: 0.08882825374603272.[0m
[32m[I 2025-01-07 08:36:08,989][0m Trial 8 finished with value: 0.15929318943493803 and parameters: {'observation_period_num': 216, 'train_rates': 0.7927616015749741, 'learning_rate': 0.00010689372983677068, 'batch_size': 80, 'step_size': 2, 'gamma': 0.9313190228661808}. Best is trial 6 with value: 0.08882825374603272.[0m
[32m[I 2025-01-07 08:36:50,776][0m Trial 9 finished with value: 0.24185488248980322 and parameters: {'observation_period_num': 11, 'train_rates': 0.6345558730155133, 'learning_rate': 2.9174197208730536e-06, 'batch_size': 186, 'step_size': 7, 'gamma': 0.940296012577145}. Best is trial 6 with value: 0.08882825374603272.[0m
[32m[I 2025-01-07 08:41:13,221][0m Trial 10 finished with value: 0.06223631458124146 and parameters: {'observation_period_num': 75, 'train_rates': 0.9780722397714522, 'learning_rate': 1.878530335073334e-05, 'batch_size': 23, 'step_size': 11, 'gamma': 0.8584309491129758}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:43:59,275][0m Trial 11 finished with value: 0.07955176555193387 and parameters: {'observation_period_num': 75, 'train_rates': 0.9732034977188161, 'learning_rate': 2.3861022950884913e-05, 'batch_size': 40, 'step_size': 11, 'gamma': 0.8607937819099438}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:48:01,777][0m Trial 12 finished with value: 0.09661534509143314 and parameters: {'observation_period_num': 65, 'train_rates': 0.9872897592448008, 'learning_rate': 1.1669887403187355e-05, 'batch_size': 25, 'step_size': 12, 'gamma': 0.8402580452988191}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:50:10,788][0m Trial 13 finished with value: 0.0780973955988884 and parameters: {'observation_period_num': 22, 'train_rates': 0.985694424779476, 'learning_rate': 1.2094139311082184e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.768206932781145}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:51:00,857][0m Trial 14 finished with value: 0.18065116423017838 and parameters: {'observation_period_num': 6, 'train_rates': 0.8847831257327261, 'learning_rate': 1.1047074782593208e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.7531175755279953}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:52:40,522][0m Trial 15 finished with value: 0.17431915582728988 and parameters: {'observation_period_num': 39, 'train_rates': 0.9183245297150923, 'learning_rate': 8.587183014535073e-06, 'batch_size': 61, 'step_size': 11, 'gamma': 0.7528401931839536}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:54:10,805][0m Trial 16 finished with value: 0.1746100101140211 and parameters: {'observation_period_num': 56, 'train_rates': 0.6006916591614722, 'learning_rate': 0.0009452552713843451, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8002996202497916}. Best is trial 10 with value: 0.06223631458124146.[0m
[32m[I 2025-01-07 08:58:09,262][0m Trial 17 finished with value: 0.05990934843554142 and parameters: {'observation_period_num': 35, 'train_rates': 0.6778454824933109, 'learning_rate': 3.648738249270453e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8067442791079622}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:02:51,097][0m Trial 18 finished with value: 0.1396801222337259 and parameters: {'observation_period_num': 97, 'train_rates': 0.6762806823791809, 'learning_rate': 6.496952698482226e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8232414368524272}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:03:32,048][0m Trial 19 finished with value: 0.35508208596633256 and parameters: {'observation_period_num': 252, 'train_rates': 0.6983069840695784, 'learning_rate': 3.278976061942629e-05, 'batch_size': 123, 'step_size': 4, 'gamma': 0.8968770352601604}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:04:14,282][0m Trial 20 finished with value: 0.09030772697086595 and parameters: {'observation_period_num': 135, 'train_rates': 0.8312584710147971, 'learning_rate': 0.0001591700482580452, 'batch_size': 159, 'step_size': 10, 'gamma': 0.7871109653797281}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:05:58,581][0m Trial 21 finished with value: 0.08737944228242533 and parameters: {'observation_period_num': 12, 'train_rates': 0.9562109958047766, 'learning_rate': 1.9202873960696297e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.7795643189008931}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:08:03,792][0m Trial 22 finished with value: 0.12709409735813273 and parameters: {'observation_period_num': 31, 'train_rates': 0.9012701743008861, 'learning_rate': 6.546888402689993e-06, 'batch_size': 46, 'step_size': 10, 'gamma': 0.8371135181251106}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:09:09,671][0m Trial 23 finished with value: 0.0683806430882264 and parameters: {'observation_period_num': 31, 'train_rates': 0.6639474346272981, 'learning_rate': 5.021765639501799e-05, 'batch_size': 73, 'step_size': 13, 'gamma': 0.774531656777224}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:10:14,829][0m Trial 24 finished with value: 0.09457033903648455 and parameters: {'observation_period_num': 55, 'train_rates': 0.668729022809093, 'learning_rate': 4.5882297035488266e-05, 'batch_size': 80, 'step_size': 15, 'gamma': 0.8844724129054602}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:15:07,854][0m Trial 25 finished with value: 0.11366607469679722 and parameters: {'observation_period_num': 81, 'train_rates': 0.7228500997961135, 'learning_rate': 0.0001457729989861767, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8147908874792779}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:16:16,029][0m Trial 26 finished with value: 0.18757301059354192 and parameters: {'observation_period_num': 39, 'train_rates': 0.6308683854738368, 'learning_rate': 1.9107888509904355e-05, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8412315335057904}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:18:35,866][0m Trial 27 finished with value: 0.09012825491613355 and parameters: {'observation_period_num': 93, 'train_rates': 0.7467249437104589, 'learning_rate': 5.07606595861774e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7936547709604651}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:20:43,064][0m Trial 28 finished with value: 0.09689801852087898 and parameters: {'observation_period_num': 64, 'train_rates': 0.655337805181364, 'learning_rate': 0.0002780097544417016, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7707084238611729}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:21:27,392][0m Trial 29 finished with value: 0.6635576906768678 and parameters: {'observation_period_num': 127, 'train_rates': 0.699803178852075, 'learning_rate': 4.9091604214246e-06, 'batch_size': 123, 'step_size': 9, 'gamma': 0.8294405308387007}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:22:45,962][0m Trial 30 finished with value: 0.061059125781764645 and parameters: {'observation_period_num': 25, 'train_rates': 0.8271191892619058, 'learning_rate': 1.9073065049341112e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8543584260874627}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:23:42,230][0m Trial 31 finished with value: 0.06682839643123538 and parameters: {'observation_period_num': 28, 'train_rates': 0.8385442049631853, 'learning_rate': 1.7775567345051085e-05, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8560121676054625}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:24:36,752][0m Trial 32 finished with value: 0.11505921743427704 and parameters: {'observation_period_num': 50, 'train_rates': 0.8322339750642201, 'learning_rate': 1.6485940494025777e-05, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8522699309240213}. Best is trial 17 with value: 0.05990934843554142.[0m
[32m[I 2025-01-07 09:25:53,857][0m Trial 33 finished with value: 0.05655473806325672 and parameters: {'observation_period_num': 32, 'train_rates': 0.8237942493981932, 'learning_rate': 2.8719361926722716e-05, 'batch_size': 93, 'step_size': 9, 'gamma': 0.8856078408079224}. Best is trial 33 with value: 0.05655473806325672.[0m
[32m[I 2025-01-07 09:26:49,828][0m Trial 34 finished with value: 0.07248684057044238 and parameters: {'observation_period_num': 18, 'train_rates': 0.7708596059260031, 'learning_rate': 3.2065800308177705e-05, 'batch_size': 143, 'step_size': 7, 'gamma': 0.8876164945835123}. Best is trial 33 with value: 0.05655473806325672.[0m
[32m[I 2025-01-07 09:28:02,943][0m Trial 35 finished with value: 0.06731486038079583 and parameters: {'observation_period_num': 43, 'train_rates': 0.8623416260565006, 'learning_rate': 2.5372062203069983e-05, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8771215503775579}. Best is trial 33 with value: 0.05655473806325672.[0m
[32m[I 2025-01-07 09:31:07,173][0m Trial 36 finished with value: 0.14059810062230862 and parameters: {'observation_period_num': 107, 'train_rates': 0.8127776036982594, 'learning_rate': 6.508433827642837e-06, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8971127921240925}. Best is trial 33 with value: 0.05655473806325672.[0m
[32m[I 2025-01-07 09:31:58,905][0m Trial 37 finished with value: 0.07238757450665746 and parameters: {'observation_period_num': 67, 'train_rates': 0.9032738441655965, 'learning_rate': 4.065835422439927e-05, 'batch_size': 122, 'step_size': 11, 'gamma': 0.8711156402609773}. Best is trial 33 with value: 0.05655473806325672.[0m
[32m[I 2025-01-07 09:34:05,703][0m Trial 38 finished with value: 0.042293359783931554 and parameters: {'observation_period_num': 5, 'train_rates': 0.9355177948190141, 'learning_rate': 6.777185818001509e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.809790326166248}. Best is trial 38 with value: 0.042293359783931554.[0m
[32m[I 2025-01-07 09:35:29,817][0m Trial 39 finished with value: 0.041179849705013045 and parameters: {'observation_period_num': 5, 'train_rates': 0.8141860907803719, 'learning_rate': 9.53063259521516e-05, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8103590223275221}. Best is trial 39 with value: 0.041179849705013045.[0m
[32m[I 2025-01-07 09:37:22,159][0m Trial 40 finished with value: 0.04377026329402789 and parameters: {'observation_period_num': 5, 'train_rates': 0.8080397750364405, 'learning_rate': 0.00010467425675322592, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8146289086081041}. Best is trial 39 with value: 0.041179849705013045.[0m
[32m[I 2025-01-07 09:39:12,447][0m Trial 41 finished with value: 0.046484970998806385 and parameters: {'observation_period_num': 5, 'train_rates': 0.8090284922645109, 'learning_rate': 7.486450076489424e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.8091842311008804}. Best is trial 39 with value: 0.041179849705013045.[0m
[32m[I 2025-01-07 09:41:04,679][0m Trial 42 finished with value: 0.02537795030412839 and parameters: {'observation_period_num': 5, 'train_rates': 0.8087906059086023, 'learning_rate': 0.00011183111229191689, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9862243716668841}. Best is trial 42 with value: 0.02537795030412839.[0m
Early stopping at epoch 68
[32m[I 2025-01-07 09:42:26,928][0m Trial 43 finished with value: 0.06427887782609443 and parameters: {'observation_period_num': 8, 'train_rates': 0.8041661508720169, 'learning_rate': 0.00013528762085745236, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8172437705349315}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:43:48,929][0m Trial 44 finished with value: 0.05826661593102394 and parameters: {'observation_period_num': 14, 'train_rates': 0.7762048230184574, 'learning_rate': 8.921116156638637e-05, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8037539065803356}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:45:43,112][0m Trial 45 finished with value: 0.026852376759052277 and parameters: {'observation_period_num': 6, 'train_rates': 0.8504173999131537, 'learning_rate': 0.00025138612856563533, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9705559613815693}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:48:52,503][0m Trial 46 finished with value: 0.06375076877562681 and parameters: {'observation_period_num': 21, 'train_rates': 0.8520918092515661, 'learning_rate': 0.0002308588705006103, 'batch_size': 30, 'step_size': 3, 'gamma': 0.9771474027923314}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:51:11,194][0m Trial 47 finished with value: 0.1389364273553448 and parameters: {'observation_period_num': 164, 'train_rates': 0.8796258178687283, 'learning_rate': 0.0005746592714219672, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9451752943786133}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:52:23,959][0m Trial 48 finished with value: 0.04883313916148024 and parameters: {'observation_period_num': 18, 'train_rates': 0.936145288937652, 'learning_rate': 0.000103854503465199, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9672464608927589}. Best is trial 42 with value: 0.02537795030412839.[0m
[32m[I 2025-01-07 09:53:53,613][0m Trial 49 finished with value: 0.12147625655813894 and parameters: {'observation_period_num': 188, 'train_rates': 0.7914808204735153, 'learning_rate': 0.00020893192210290793, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9860335142218304}. Best is trial 42 with value: 0.02537795030412839.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.7374219831283599, 'learning_rate': 0.000259005902421501, 'batch_size': 34, 'step_size': 5, 'gamma': 0.7822294511431461}
Epoch 1/300, trend Loss: 0.2792 | 0.3139
Epoch 2/300, trend Loss: 0.1404 | 0.1388
Epoch 3/300, trend Loss: 0.1264 | 0.1029
Epoch 4/300, trend Loss: 0.1216 | 0.0991
Epoch 5/300, trend Loss: 0.1118 | 0.0867
Epoch 6/300, trend Loss: 0.1050 | 0.0897
Epoch 7/300, trend Loss: 0.1017 | 0.0865
Epoch 8/300, trend Loss: 0.1001 | 0.0874
Epoch 9/300, trend Loss: 0.0981 | 0.0813
Epoch 10/300, trend Loss: 0.0968 | 0.0812
Epoch 11/300, trend Loss: 0.0988 | 0.0790
Epoch 12/300, trend Loss: 0.0956 | 0.0842
Epoch 13/300, trend Loss: 0.0915 | 0.1001
Epoch 14/300, trend Loss: 0.0881 | 0.1346
Epoch 15/300, trend Loss: 0.0877 | 0.1459
Epoch 16/300, trend Loss: 0.0880 | 0.1213
Epoch 17/300, trend Loss: 0.0863 | 0.1090
Epoch 18/300, trend Loss: 0.0844 | 0.0955
Epoch 19/300, trend Loss: 0.0828 | 0.0937
Epoch 20/300, trend Loss: 0.0818 | 0.0824
Epoch 21/300, trend Loss: 0.0805 | 0.0810
Epoch 22/300, trend Loss: 0.0797 | 0.0738
Epoch 23/300, trend Loss: 0.0789 | 0.0715
Epoch 24/300, trend Loss: 0.0783 | 0.0694
Epoch 25/300, trend Loss: 0.0779 | 0.0660
Epoch 26/300, trend Loss: 0.0774 | 0.0647
Epoch 27/300, trend Loss: 0.0771 | 0.0626
Epoch 28/300, trend Loss: 0.0768 | 0.0615
Epoch 29/300, trend Loss: 0.0765 | 0.0604
Epoch 30/300, trend Loss: 0.0763 | 0.0592
Epoch 31/300, trend Loss: 0.0760 | 0.0581
Epoch 32/300, trend Loss: 0.0758 | 0.0574
Epoch 33/300, trend Loss: 0.0756 | 0.0570
Epoch 34/300, trend Loss: 0.0755 | 0.0559
Epoch 35/300, trend Loss: 0.0753 | 0.0555
Epoch 36/300, trend Loss: 0.0752 | 0.0548
Epoch 37/300, trend Loss: 0.0751 | 0.0545
Epoch 38/300, trend Loss: 0.0749 | 0.0542
Epoch 39/300, trend Loss: 0.0748 | 0.0537
Epoch 40/300, trend Loss: 0.0747 | 0.0535
Epoch 41/300, trend Loss: 0.0746 | 0.0531
Epoch 42/300, trend Loss: 0.0745 | 0.0530
Epoch 43/300, trend Loss: 0.0744 | 0.0528
Epoch 44/300, trend Loss: 0.0743 | 0.0526
Epoch 45/300, trend Loss: 0.0742 | 0.0524
Epoch 46/300, trend Loss: 0.0741 | 0.0523
Epoch 47/300, trend Loss: 0.0741 | 0.0522
Epoch 48/300, trend Loss: 0.0740 | 0.0521
Epoch 49/300, trend Loss: 0.0739 | 0.0520
Epoch 50/300, trend Loss: 0.0739 | 0.0519
Epoch 51/300, trend Loss: 0.0738 | 0.0518
Epoch 52/300, trend Loss: 0.0738 | 0.0517
Epoch 53/300, trend Loss: 0.0737 | 0.0517
Epoch 54/300, trend Loss: 0.0737 | 0.0516
Epoch 55/300, trend Loss: 0.0737 | 0.0515
Epoch 56/300, trend Loss: 0.0736 | 0.0515
Epoch 57/300, trend Loss: 0.0736 | 0.0514
Epoch 58/300, trend Loss: 0.0736 | 0.0514
Epoch 59/300, trend Loss: 0.0736 | 0.0513
Epoch 60/300, trend Loss: 0.0736 | 0.0513
Epoch 61/300, trend Loss: 0.0735 | 0.0512
Epoch 62/300, trend Loss: 0.0735 | 0.0512
Epoch 63/300, trend Loss: 0.0735 | 0.0512
Epoch 64/300, trend Loss: 0.0735 | 0.0511
Epoch 65/300, trend Loss: 0.0735 | 0.0511
Epoch 66/300, trend Loss: 0.0735 | 0.0511
Epoch 67/300, trend Loss: 0.0735 | 0.0510
Epoch 68/300, trend Loss: 0.0735 | 0.0510
Epoch 69/300, trend Loss: 0.0734 | 0.0510
Epoch 70/300, trend Loss: 0.0734 | 0.0510
Epoch 71/300, trend Loss: 0.0734 | 0.0510
Epoch 72/300, trend Loss: 0.0734 | 0.0509
Epoch 73/300, trend Loss: 0.0734 | 0.0509
Epoch 74/300, trend Loss: 0.0734 | 0.0509
Epoch 75/300, trend Loss: 0.0734 | 0.0509
Epoch 76/300, trend Loss: 0.0734 | 0.0509
Epoch 77/300, trend Loss: 0.0734 | 0.0509
Epoch 78/300, trend Loss: 0.0734 | 0.0509
Epoch 79/300, trend Loss: 0.0734 | 0.0509
Epoch 80/300, trend Loss: 0.0734 | 0.0508
Epoch 81/300, trend Loss: 0.0734 | 0.0508
Epoch 82/300, trend Loss: 0.0734 | 0.0508
Epoch 83/300, trend Loss: 0.0734 | 0.0508
Epoch 84/300, trend Loss: 0.0734 | 0.0508
Epoch 85/300, trend Loss: 0.0734 | 0.0508
Epoch 86/300, trend Loss: 0.0734 | 0.0508
Epoch 87/300, trend Loss: 0.0734 | 0.0508
Epoch 88/300, trend Loss: 0.0734 | 0.0508
Epoch 89/300, trend Loss: 0.0734 | 0.0508
Epoch 90/300, trend Loss: 0.0734 | 0.0508
Epoch 91/300, trend Loss: 0.0734 | 0.0508
Epoch 92/300, trend Loss: 0.0734 | 0.0508
Epoch 93/300, trend Loss: 0.0734 | 0.0508
Epoch 94/300, trend Loss: 0.0734 | 0.0508
Epoch 95/300, trend Loss: 0.0734 | 0.0508
Epoch 96/300, trend Loss: 0.0734 | 0.0508
Epoch 97/300, trend Loss: 0.0734 | 0.0508
Epoch 98/300, trend Loss: 0.0734 | 0.0508
Epoch 99/300, trend Loss: 0.0734 | 0.0508
Epoch 100/300, trend Loss: 0.0734 | 0.0508
Epoch 101/300, trend Loss: 0.0734 | 0.0508
Epoch 102/300, trend Loss: 0.0734 | 0.0508
Epoch 103/300, trend Loss: 0.0734 | 0.0508
Epoch 104/300, trend Loss: 0.0734 | 0.0508
Epoch 105/300, trend Loss: 0.0734 | 0.0508
Epoch 106/300, trend Loss: 0.0734 | 0.0508
Epoch 107/300, trend Loss: 0.0734 | 0.0508
Epoch 108/300, trend Loss: 0.0734 | 0.0508
Epoch 109/300, trend Loss: 0.0734 | 0.0508
Epoch 110/300, trend Loss: 0.0734 | 0.0508
Epoch 111/300, trend Loss: 0.0734 | 0.0508
Epoch 112/300, trend Loss: 0.0734 | 0.0507
Epoch 113/300, trend Loss: 0.0734 | 0.0507
Epoch 114/300, trend Loss: 0.0734 | 0.0507
Epoch 115/300, trend Loss: 0.0734 | 0.0507
Epoch 116/300, trend Loss: 0.0734 | 0.0507
Epoch 117/300, trend Loss: 0.0734 | 0.0507
Epoch 118/300, trend Loss: 0.0734 | 0.0507
Epoch 119/300, trend Loss: 0.0734 | 0.0507
Epoch 120/300, trend Loss: 0.0734 | 0.0507
Epoch 121/300, trend Loss: 0.0734 | 0.0507
Epoch 122/300, trend Loss: 0.0734 | 0.0507
Epoch 123/300, trend Loss: 0.0734 | 0.0507
Epoch 124/300, trend Loss: 0.0734 | 0.0507
Epoch 125/300, trend Loss: 0.0734 | 0.0507
Epoch 126/300, trend Loss: 0.0734 | 0.0507
Epoch 127/300, trend Loss: 0.0734 | 0.0507
Epoch 128/300, trend Loss: 0.0734 | 0.0507
Epoch 129/300, trend Loss: 0.0734 | 0.0507
Epoch 130/300, trend Loss: 0.0734 | 0.0507
Epoch 131/300, trend Loss: 0.0734 | 0.0507
Epoch 132/300, trend Loss: 0.0734 | 0.0507
Epoch 133/300, trend Loss: 0.0734 | 0.0507
Epoch 134/300, trend Loss: 0.0734 | 0.0507
Epoch 135/300, trend Loss: 0.0734 | 0.0507
Epoch 136/300, trend Loss: 0.0734 | 0.0507
Epoch 137/300, trend Loss: 0.0734 | 0.0507
Epoch 138/300, trend Loss: 0.0734 | 0.0507
Epoch 139/300, trend Loss: 0.0734 | 0.0507
Epoch 140/300, trend Loss: 0.0734 | 0.0507
Epoch 141/300, trend Loss: 0.0734 | 0.0507
Epoch 142/300, trend Loss: 0.0734 | 0.0507
Epoch 143/300, trend Loss: 0.0734 | 0.0507
Epoch 144/300, trend Loss: 0.0734 | 0.0507
Epoch 145/300, trend Loss: 0.0734 | 0.0507
Epoch 146/300, trend Loss: 0.0734 | 0.0507
Epoch 147/300, trend Loss: 0.0734 | 0.0507
Epoch 148/300, trend Loss: 0.0734 | 0.0507
Epoch 149/300, trend Loss: 0.0734 | 0.0507
Epoch 150/300, trend Loss: 0.0734 | 0.0507
Epoch 151/300, trend Loss: 0.0734 | 0.0507
Epoch 152/300, trend Loss: 0.0734 | 0.0507
Epoch 153/300, trend Loss: 0.0734 | 0.0507
Epoch 154/300, trend Loss: 0.0734 | 0.0507
Epoch 155/300, trend Loss: 0.0734 | 0.0507
Epoch 156/300, trend Loss: 0.0734 | 0.0507
Epoch 157/300, trend Loss: 0.0734 | 0.0507
Epoch 158/300, trend Loss: 0.0734 | 0.0507
Epoch 159/300, trend Loss: 0.0734 | 0.0507
Epoch 160/300, trend Loss: 0.0734 | 0.0507
Epoch 161/300, trend Loss: 0.0734 | 0.0507
Epoch 162/300, trend Loss: 0.0734 | 0.0507
Epoch 163/300, trend Loss: 0.0734 | 0.0507
Epoch 164/300, trend Loss: 0.0734 | 0.0507
Epoch 165/300, trend Loss: 0.0734 | 0.0507
Epoch 166/300, trend Loss: 0.0734 | 0.0507
Epoch 167/300, trend Loss: 0.0734 | 0.0507
Epoch 168/300, trend Loss: 0.0734 | 0.0507
Epoch 169/300, trend Loss: 0.0734 | 0.0507
Epoch 170/300, trend Loss: 0.0734 | 0.0507
Epoch 171/300, trend Loss: 0.0734 | 0.0507
Epoch 172/300, trend Loss: 0.0734 | 0.0507
Epoch 173/300, trend Loss: 0.0734 | 0.0507
Epoch 174/300, trend Loss: 0.0734 | 0.0507
Epoch 175/300, trend Loss: 0.0734 | 0.0507
Epoch 176/300, trend Loss: 0.0734 | 0.0507
Epoch 177/300, trend Loss: 0.0734 | 0.0507
Epoch 178/300, trend Loss: 0.0734 | 0.0507
Epoch 179/300, trend Loss: 0.0734 | 0.0507
Epoch 180/300, trend Loss: 0.0734 | 0.0507
Epoch 181/300, trend Loss: 0.0734 | 0.0507
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 18, 'train_rates': 0.9893415657115091, 'learning_rate': 0.000716628072975474, 'batch_size': 108, 'step_size': 2, 'gamma': 0.9395745594912682}
Epoch 1/300, seasonal_0 Loss: 0.4153 | 0.2609
Epoch 2/300, seasonal_0 Loss: 0.1715 | 0.1787
Epoch 3/300, seasonal_0 Loss: 0.1534 | 0.1316
Epoch 4/300, seasonal_0 Loss: 0.1551 | 0.1273
Epoch 5/300, seasonal_0 Loss: 0.1495 | 0.1309
Epoch 6/300, seasonal_0 Loss: 0.1423 | 0.1113
Epoch 7/300, seasonal_0 Loss: 0.1191 | 0.0984
Epoch 8/300, seasonal_0 Loss: 0.1219 | 0.0919
Epoch 9/300, seasonal_0 Loss: 0.1131 | 0.0944
Epoch 10/300, seasonal_0 Loss: 0.1132 | 0.1030
Epoch 11/300, seasonal_0 Loss: 0.1119 | 0.0934
Epoch 12/300, seasonal_0 Loss: 0.1095 | 0.0863
Epoch 13/300, seasonal_0 Loss: 0.1081 | 0.0841
Epoch 14/300, seasonal_0 Loss: 0.1034 | 0.0768
Epoch 15/300, seasonal_0 Loss: 0.0949 | 0.0720
Epoch 16/300, seasonal_0 Loss: 0.0913 | 0.0718
Epoch 17/300, seasonal_0 Loss: 0.0899 | 0.0717
Epoch 18/300, seasonal_0 Loss: 0.0874 | 0.0702
Epoch 19/300, seasonal_0 Loss: 0.0851 | 0.0687
Epoch 20/300, seasonal_0 Loss: 0.0849 | 0.0683
Epoch 21/300, seasonal_0 Loss: 0.0867 | 0.0693
Epoch 22/300, seasonal_0 Loss: 0.0895 | 0.0779
Epoch 23/300, seasonal_0 Loss: 0.0909 | 0.0836
Epoch 24/300, seasonal_0 Loss: 0.0932 | 0.0859
Epoch 25/300, seasonal_0 Loss: 0.1025 | 0.1073
Epoch 26/300, seasonal_0 Loss: 0.0986 | 0.0771
Epoch 27/300, seasonal_0 Loss: 0.1012 | 0.0876
Epoch 28/300, seasonal_0 Loss: 0.1125 | 0.1085
Epoch 29/300, seasonal_0 Loss: 0.1134 | 0.1937
Epoch 30/300, seasonal_0 Loss: 0.1102 | 0.1540
Epoch 31/300, seasonal_0 Loss: 0.1008 | 0.0773
Epoch 32/300, seasonal_0 Loss: 0.0897 | 0.0718
Epoch 33/300, seasonal_0 Loss: 0.0818 | 0.0650
Epoch 34/300, seasonal_0 Loss: 0.0810 | 0.0662
Epoch 35/300, seasonal_0 Loss: 0.0796 | 0.0629
Epoch 36/300, seasonal_0 Loss: 0.0787 | 0.0612
Epoch 37/300, seasonal_0 Loss: 0.0781 | 0.0608
Epoch 38/300, seasonal_0 Loss: 0.0771 | 0.0608
Epoch 39/300, seasonal_0 Loss: 0.0764 | 0.0601
Epoch 40/300, seasonal_0 Loss: 0.0760 | 0.0591
Epoch 41/300, seasonal_0 Loss: 0.0756 | 0.0584
Epoch 42/300, seasonal_0 Loss: 0.0753 | 0.0578
Epoch 43/300, seasonal_0 Loss: 0.0750 | 0.0572
Epoch 44/300, seasonal_0 Loss: 0.0748 | 0.0567
Epoch 45/300, seasonal_0 Loss: 0.0745 | 0.0562
Epoch 46/300, seasonal_0 Loss: 0.0743 | 0.0558
Epoch 47/300, seasonal_0 Loss: 0.0741 | 0.0553
Epoch 48/300, seasonal_0 Loss: 0.0739 | 0.0549
Epoch 49/300, seasonal_0 Loss: 0.0737 | 0.0545
Epoch 50/300, seasonal_0 Loss: 0.0735 | 0.0541
Epoch 51/300, seasonal_0 Loss: 0.0733 | 0.0538
Epoch 52/300, seasonal_0 Loss: 0.0732 | 0.0534
Epoch 53/300, seasonal_0 Loss: 0.0730 | 0.0531
Epoch 54/300, seasonal_0 Loss: 0.0729 | 0.0528
Epoch 55/300, seasonal_0 Loss: 0.0727 | 0.0525
Epoch 56/300, seasonal_0 Loss: 0.0726 | 0.0522
Epoch 57/300, seasonal_0 Loss: 0.0724 | 0.0519
Epoch 58/300, seasonal_0 Loss: 0.0723 | 0.0516
Epoch 59/300, seasonal_0 Loss: 0.0722 | 0.0514
Epoch 60/300, seasonal_0 Loss: 0.0721 | 0.0512
Epoch 61/300, seasonal_0 Loss: 0.0720 | 0.0509
Epoch 62/300, seasonal_0 Loss: 0.0719 | 0.0507
Epoch 63/300, seasonal_0 Loss: 0.0718 | 0.0505
Epoch 64/300, seasonal_0 Loss: 0.0717 | 0.0503
Epoch 65/300, seasonal_0 Loss: 0.0716 | 0.0502
Epoch 66/300, seasonal_0 Loss: 0.0715 | 0.0500
Epoch 67/300, seasonal_0 Loss: 0.0714 | 0.0498
Epoch 68/300, seasonal_0 Loss: 0.0713 | 0.0497
Epoch 69/300, seasonal_0 Loss: 0.0713 | 0.0495
Epoch 70/300, seasonal_0 Loss: 0.0712 | 0.0494
Epoch 71/300, seasonal_0 Loss: 0.0711 | 0.0493
Epoch 72/300, seasonal_0 Loss: 0.0711 | 0.0492
Epoch 73/300, seasonal_0 Loss: 0.0710 | 0.0490
Epoch 74/300, seasonal_0 Loss: 0.0710 | 0.0489
Epoch 75/300, seasonal_0 Loss: 0.0709 | 0.0488
Epoch 76/300, seasonal_0 Loss: 0.0709 | 0.0488
Epoch 77/300, seasonal_0 Loss: 0.0708 | 0.0487
Epoch 78/300, seasonal_0 Loss: 0.0708 | 0.0486
Epoch 79/300, seasonal_0 Loss: 0.0707 | 0.0485
Epoch 80/300, seasonal_0 Loss: 0.0707 | 0.0484
Epoch 81/300, seasonal_0 Loss: 0.0707 | 0.0484
Epoch 82/300, seasonal_0 Loss: 0.0706 | 0.0483
Epoch 83/300, seasonal_0 Loss: 0.0706 | 0.0482
Epoch 84/300, seasonal_0 Loss: 0.0706 | 0.0482
Epoch 85/300, seasonal_0 Loss: 0.0705 | 0.0481
Epoch 86/300, seasonal_0 Loss: 0.0705 | 0.0481
Epoch 87/300, seasonal_0 Loss: 0.0705 | 0.0480
Epoch 88/300, seasonal_0 Loss: 0.0705 | 0.0480
Epoch 89/300, seasonal_0 Loss: 0.0704 | 0.0479
Epoch 90/300, seasonal_0 Loss: 0.0704 | 0.0479
Epoch 91/300, seasonal_0 Loss: 0.0704 | 0.0479
Epoch 92/300, seasonal_0 Loss: 0.0704 | 0.0478
Epoch 93/300, seasonal_0 Loss: 0.0704 | 0.0478
Epoch 94/300, seasonal_0 Loss: 0.0703 | 0.0478
Epoch 95/300, seasonal_0 Loss: 0.0703 | 0.0477
Epoch 96/300, seasonal_0 Loss: 0.0703 | 0.0477
Epoch 97/300, seasonal_0 Loss: 0.0703 | 0.0477
Epoch 98/300, seasonal_0 Loss: 0.0703 | 0.0477
Epoch 99/300, seasonal_0 Loss: 0.0703 | 0.0476
Epoch 100/300, seasonal_0 Loss: 0.0703 | 0.0476
Epoch 101/300, seasonal_0 Loss: 0.0702 | 0.0476
Epoch 102/300, seasonal_0 Loss: 0.0702 | 0.0476
Epoch 103/300, seasonal_0 Loss: 0.0702 | 0.0476
Epoch 104/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 105/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 106/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 107/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 108/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 109/300, seasonal_0 Loss: 0.0702 | 0.0475
Epoch 110/300, seasonal_0 Loss: 0.0702 | 0.0474
Epoch 111/300, seasonal_0 Loss: 0.0702 | 0.0474
Epoch 112/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 113/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 114/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 115/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 116/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 117/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 118/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 119/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 120/300, seasonal_0 Loss: 0.0701 | 0.0474
Epoch 121/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 122/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 123/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 124/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 125/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 126/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 127/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 128/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 129/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 130/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 131/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 132/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 133/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 134/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 135/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 136/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 137/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 138/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 139/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 140/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 141/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 142/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 143/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 144/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 145/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 146/300, seasonal_0 Loss: 0.0701 | 0.0473
Epoch 147/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 148/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 149/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 150/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 151/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 152/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 153/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 154/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 155/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 156/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 157/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 158/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 159/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 160/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 161/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 162/300, seasonal_0 Loss: 0.0700 | 0.0473
Epoch 163/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 164/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 165/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 166/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 167/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 168/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 169/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 170/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 171/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 172/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 173/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 174/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 175/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 176/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 177/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 178/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 179/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 180/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 181/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 182/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 183/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 184/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 185/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 186/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 187/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 188/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 189/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 190/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 191/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 192/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 193/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 194/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 195/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 196/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 197/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 198/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 199/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 200/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 201/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 202/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 203/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 204/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 205/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 206/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 207/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 208/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 209/300, seasonal_0 Loss: 0.0700 | 0.0472
Epoch 210/300, seasonal_0 Loss: 0.0700 | 0.0472
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 15, 'train_rates': 0.9137639894106382, 'learning_rate': 0.0009193782982711474, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9584505156140349}
Epoch 1/300, seasonal_1 Loss: 0.2683 | 0.2231
Epoch 2/300, seasonal_1 Loss: 0.1353 | 0.1316
Epoch 3/300, seasonal_1 Loss: 0.1192 | 0.1044
Epoch 4/300, seasonal_1 Loss: 0.1108 | 0.1076
Epoch 5/300, seasonal_1 Loss: 0.1045 | 0.0952
Epoch 6/300, seasonal_1 Loss: 0.0991 | 0.0879
Epoch 7/300, seasonal_1 Loss: 0.0972 | 0.0794
Epoch 8/300, seasonal_1 Loss: 0.0947 | 0.0816
Epoch 9/300, seasonal_1 Loss: 0.0995 | 0.0714
Epoch 10/300, seasonal_1 Loss: 0.0928 | 0.0745
Epoch 11/300, seasonal_1 Loss: 0.0911 | 0.0622
Epoch 12/300, seasonal_1 Loss: 0.1050 | 0.0739
Epoch 13/300, seasonal_1 Loss: 0.0871 | 0.0548
Epoch 14/300, seasonal_1 Loss: 0.0819 | 0.0502
Epoch 15/300, seasonal_1 Loss: 0.0783 | 0.0481
Epoch 16/300, seasonal_1 Loss: 0.0826 | 0.0455
Epoch 17/300, seasonal_1 Loss: 0.0906 | 0.0676
Epoch 18/300, seasonal_1 Loss: 0.0814 | 0.0472
Epoch 19/300, seasonal_1 Loss: 0.0736 | 0.0418
Epoch 20/300, seasonal_1 Loss: 0.0698 | 0.0385
Epoch 21/300, seasonal_1 Loss: 0.0682 | 0.0385
Epoch 22/300, seasonal_1 Loss: 0.0675 | 0.0463
Epoch 23/300, seasonal_1 Loss: 0.0670 | 0.0421
Epoch 24/300, seasonal_1 Loss: 0.0663 | 0.0470
Epoch 25/300, seasonal_1 Loss: 0.0656 | 0.0400
Epoch 26/300, seasonal_1 Loss: 0.0654 | 0.0460
Epoch 27/300, seasonal_1 Loss: 0.0661 | 0.0432
Epoch 28/300, seasonal_1 Loss: 0.0680 | 0.0493
Epoch 29/300, seasonal_1 Loss: 0.0658 | 0.0444
Epoch 30/300, seasonal_1 Loss: 0.0646 | 0.0524
Epoch 31/300, seasonal_1 Loss: 0.0642 | 0.0436
Epoch 32/300, seasonal_1 Loss: 0.0628 | 0.0462
Epoch 33/300, seasonal_1 Loss: 0.0614 | 0.0398
Epoch 34/300, seasonal_1 Loss: 0.0616 | 0.0407
Epoch 35/300, seasonal_1 Loss: 0.0626 | 0.0374
Epoch 36/300, seasonal_1 Loss: 0.0624 | 0.0385
Epoch 37/300, seasonal_1 Loss: 0.0617 | 0.0370
Epoch 38/300, seasonal_1 Loss: 0.0609 | 0.0338
Epoch 39/300, seasonal_1 Loss: 0.0599 | 0.0336
Epoch 40/300, seasonal_1 Loss: 0.0591 | 0.0349
Epoch 41/300, seasonal_1 Loss: 0.0587 | 0.0362
Epoch 42/300, seasonal_1 Loss: 0.0588 | 0.0364
Epoch 43/300, seasonal_1 Loss: 0.0584 | 0.0329
Epoch 44/300, seasonal_1 Loss: 0.0579 | 0.0327
Epoch 45/300, seasonal_1 Loss: 0.0572 | 0.0357
Epoch 46/300, seasonal_1 Loss: 0.0568 | 0.0362
Epoch 47/300, seasonal_1 Loss: 0.0565 | 0.0349
Epoch 48/300, seasonal_1 Loss: 0.0564 | 0.0346
Epoch 49/300, seasonal_1 Loss: 0.0572 | 0.0355
Epoch 50/300, seasonal_1 Loss: 0.0585 | 0.0367
Epoch 51/300, seasonal_1 Loss: 0.0602 | 0.0365
Epoch 52/300, seasonal_1 Loss: 0.0625 | 0.0428
Epoch 53/300, seasonal_1 Loss: 0.0668 | 0.0808
Epoch 54/300, seasonal_1 Loss: 0.0712 | 0.1536
Epoch 55/300, seasonal_1 Loss: 0.0687 | 0.0726
Epoch 56/300, seasonal_1 Loss: 0.0704 | 0.0366
Epoch 57/300, seasonal_1 Loss: 0.0600 | 0.0439
Epoch 58/300, seasonal_1 Loss: 0.0572 | 0.0453
Epoch 59/300, seasonal_1 Loss: 0.0566 | 0.0394
Epoch 60/300, seasonal_1 Loss: 0.0567 | 0.0385
Epoch 61/300, seasonal_1 Loss: 0.0567 | 0.0382
Epoch 62/300, seasonal_1 Loss: 0.0563 | 0.0372
Epoch 63/300, seasonal_1 Loss: 0.0555 | 0.0372
Epoch 64/300, seasonal_1 Loss: 0.0546 | 0.0368
Epoch 65/300, seasonal_1 Loss: 0.0541 | 0.0361
Epoch 66/300, seasonal_1 Loss: 0.0536 | 0.0357
Epoch 67/300, seasonal_1 Loss: 0.0533 | 0.0352
Epoch 68/300, seasonal_1 Loss: 0.0531 | 0.0348
Epoch 69/300, seasonal_1 Loss: 0.0530 | 0.0344
Epoch 70/300, seasonal_1 Loss: 0.0530 | 0.0341
Epoch 71/300, seasonal_1 Loss: 0.0530 | 0.0339
Epoch 72/300, seasonal_1 Loss: 0.0529 | 0.0337
Epoch 73/300, seasonal_1 Loss: 0.0529 | 0.0336
Epoch 74/300, seasonal_1 Loss: 0.0528 | 0.0336
Epoch 75/300, seasonal_1 Loss: 0.0529 | 0.0335
Epoch 76/300, seasonal_1 Loss: 0.0528 | 0.0332
Epoch 77/300, seasonal_1 Loss: 0.0523 | 0.0328
Epoch 78/300, seasonal_1 Loss: 0.0517 | 0.0323
Epoch 79/300, seasonal_1 Loss: 0.0513 | 0.0320
Epoch 80/300, seasonal_1 Loss: 0.0511 | 0.0317
Epoch 81/300, seasonal_1 Loss: 0.0511 | 0.0315
Epoch 82/300, seasonal_1 Loss: 0.0511 | 0.0315
Epoch 83/300, seasonal_1 Loss: 0.0512 | 0.0315
Epoch 84/300, seasonal_1 Loss: 0.0511 | 0.0314
Epoch 85/300, seasonal_1 Loss: 0.0510 | 0.0313
Epoch 86/300, seasonal_1 Loss: 0.0508 | 0.0311
Epoch 87/300, seasonal_1 Loss: 0.0507 | 0.0310
Epoch 88/300, seasonal_1 Loss: 0.0506 | 0.0309
Epoch 89/300, seasonal_1 Loss: 0.0505 | 0.0308
Epoch 90/300, seasonal_1 Loss: 0.0504 | 0.0307
Epoch 91/300, seasonal_1 Loss: 0.0504 | 0.0306
Epoch 92/300, seasonal_1 Loss: 0.0503 | 0.0305
Epoch 93/300, seasonal_1 Loss: 0.0503 | 0.0305
Epoch 94/300, seasonal_1 Loss: 0.0502 | 0.0304
Epoch 95/300, seasonal_1 Loss: 0.0502 | 0.0303
Epoch 96/300, seasonal_1 Loss: 0.0502 | 0.0303
Epoch 97/300, seasonal_1 Loss: 0.0501 | 0.0302
Epoch 98/300, seasonal_1 Loss: 0.0501 | 0.0302
Epoch 99/300, seasonal_1 Loss: 0.0501 | 0.0302
Epoch 100/300, seasonal_1 Loss: 0.0500 | 0.0301
Epoch 101/300, seasonal_1 Loss: 0.0500 | 0.0301
Epoch 102/300, seasonal_1 Loss: 0.0500 | 0.0300
Epoch 103/300, seasonal_1 Loss: 0.0499 | 0.0300
Epoch 104/300, seasonal_1 Loss: 0.0499 | 0.0300
Epoch 105/300, seasonal_1 Loss: 0.0499 | 0.0299
Epoch 106/300, seasonal_1 Loss: 0.0499 | 0.0299
Epoch 107/300, seasonal_1 Loss: 0.0498 | 0.0299
Epoch 108/300, seasonal_1 Loss: 0.0498 | 0.0298
Epoch 109/300, seasonal_1 Loss: 0.0498 | 0.0298
Epoch 110/300, seasonal_1 Loss: 0.0498 | 0.0298
Epoch 111/300, seasonal_1 Loss: 0.0498 | 0.0298
Epoch 112/300, seasonal_1 Loss: 0.0497 | 0.0297
Epoch 113/300, seasonal_1 Loss: 0.0497 | 0.0297
Epoch 114/300, seasonal_1 Loss: 0.0497 | 0.0297
Epoch 115/300, seasonal_1 Loss: 0.0497 | 0.0297
Epoch 116/300, seasonal_1 Loss: 0.0497 | 0.0296
Epoch 117/300, seasonal_1 Loss: 0.0496 | 0.0296
Epoch 118/300, seasonal_1 Loss: 0.0496 | 0.0296
Epoch 119/300, seasonal_1 Loss: 0.0496 | 0.0296
Epoch 120/300, seasonal_1 Loss: 0.0496 | 0.0296
Epoch 121/300, seasonal_1 Loss: 0.0496 | 0.0296
Epoch 122/300, seasonal_1 Loss: 0.0496 | 0.0295
Epoch 123/300, seasonal_1 Loss: 0.0496 | 0.0295
Epoch 124/300, seasonal_1 Loss: 0.0495 | 0.0295
Epoch 125/300, seasonal_1 Loss: 0.0495 | 0.0295
Epoch 126/300, seasonal_1 Loss: 0.0495 | 0.0295
Epoch 127/300, seasonal_1 Loss: 0.0495 | 0.0295
Epoch 128/300, seasonal_1 Loss: 0.0495 | 0.0295
Epoch 129/300, seasonal_1 Loss: 0.0495 | 0.0294
Epoch 130/300, seasonal_1 Loss: 0.0495 | 0.0294
Epoch 131/300, seasonal_1 Loss: 0.0495 | 0.0294
Epoch 132/300, seasonal_1 Loss: 0.0495 | 0.0294
Epoch 133/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 134/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 135/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 136/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 137/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 138/300, seasonal_1 Loss: 0.0494 | 0.0294
Epoch 139/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 140/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 141/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 142/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 143/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 144/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 145/300, seasonal_1 Loss: 0.0494 | 0.0293
Epoch 146/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 147/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 148/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 149/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 150/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 151/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 152/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 153/300, seasonal_1 Loss: 0.0493 | 0.0293
Epoch 154/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 155/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 156/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 157/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 158/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 159/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 160/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 161/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 162/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 163/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 164/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 165/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 166/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 167/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 168/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 169/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 170/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 171/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 172/300, seasonal_1 Loss: 0.0493 | 0.0292
Epoch 173/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 174/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 175/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 176/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 177/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 178/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 179/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 180/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 181/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 182/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 183/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 184/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 185/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 186/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 187/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 188/300, seasonal_1 Loss: 0.0492 | 0.0292
Epoch 189/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 190/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 191/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 192/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 193/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 194/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 195/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 196/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 197/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 198/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 199/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 200/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 201/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 202/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 203/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 204/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 205/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 206/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 207/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 208/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 209/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 210/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 211/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 212/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 213/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 214/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 215/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 216/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 217/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 218/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 219/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 220/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 221/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 222/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 223/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 224/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 225/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 226/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 227/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 228/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 229/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 230/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 231/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 232/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 233/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 234/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 235/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 236/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 237/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 238/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 239/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 240/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 241/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 242/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 243/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 244/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 245/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 246/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 247/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 248/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 249/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 250/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 251/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 252/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 253/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 254/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 255/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 256/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 257/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 258/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 259/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 260/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 261/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 262/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 263/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 264/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 265/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 266/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 267/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 268/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 269/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 270/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 271/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 272/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 273/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 274/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 275/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 276/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 277/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 278/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 279/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 280/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 281/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 282/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 283/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 284/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 285/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 286/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 287/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 288/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 289/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 290/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 291/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 292/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 293/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 294/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 295/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 296/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 297/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 298/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 299/300, seasonal_1 Loss: 0.0492 | 0.0291
Epoch 300/300, seasonal_1 Loss: 0.0492 | 0.0291
Training seasonal_2 component with params: {'observation_period_num': 7, 'train_rates': 0.8351250925913493, 'learning_rate': 0.0009363199146312423, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9867509888919517}
Epoch 1/300, seasonal_2 Loss: 0.1655 | 0.0657
Epoch 2/300, seasonal_2 Loss: 0.0988 | 0.0533
Epoch 3/300, seasonal_2 Loss: 0.0904 | 0.0573
Epoch 4/300, seasonal_2 Loss: 0.0878 | 0.0548
Epoch 5/300, seasonal_2 Loss: 0.0853 | 0.0554
Epoch 6/300, seasonal_2 Loss: 0.0885 | 0.0421
Epoch 7/300, seasonal_2 Loss: 0.0785 | 0.0346
Epoch 8/300, seasonal_2 Loss: 0.0767 | 0.0346
Epoch 9/300, seasonal_2 Loss: 0.0759 | 0.0357
Epoch 10/300, seasonal_2 Loss: 0.0777 | 0.0488
Epoch 11/300, seasonal_2 Loss: 0.0785 | 0.0474
Epoch 12/300, seasonal_2 Loss: 0.0770 | 0.0383
Epoch 13/300, seasonal_2 Loss: 0.0713 | 0.0388
Epoch 14/300, seasonal_2 Loss: 0.0728 | 0.0348
Epoch 15/300, seasonal_2 Loss: 0.0738 | 0.0447
Epoch 16/300, seasonal_2 Loss: 0.0709 | 0.0365
Epoch 17/300, seasonal_2 Loss: 0.0710 | 0.0362
Epoch 18/300, seasonal_2 Loss: 0.0685 | 0.0310
Epoch 19/300, seasonal_2 Loss: 0.0676 | 0.0330
Epoch 20/300, seasonal_2 Loss: 0.0676 | 0.0350
Epoch 21/300, seasonal_2 Loss: 0.0692 | 0.0329
Epoch 22/300, seasonal_2 Loss: 0.0684 | 0.0464
Epoch 23/300, seasonal_2 Loss: 0.0682 | 0.0352
Epoch 24/300, seasonal_2 Loss: 0.0706 | 0.0407
Epoch 25/300, seasonal_2 Loss: 0.0701 | 0.0304
Epoch 26/300, seasonal_2 Loss: 0.0694 | 0.0391
Epoch 27/300, seasonal_2 Loss: 0.0666 | 0.0321
Epoch 28/300, seasonal_2 Loss: 0.0654 | 0.0311
Epoch 29/300, seasonal_2 Loss: 0.0662 | 0.0373
Epoch 30/300, seasonal_2 Loss: 0.0646 | 0.0293
Epoch 31/300, seasonal_2 Loss: 0.0644 | 0.0298
Epoch 32/300, seasonal_2 Loss: 0.0644 | 0.0300
Epoch 33/300, seasonal_2 Loss: 0.0624 | 0.0312
Epoch 34/300, seasonal_2 Loss: 0.0628 | 0.0278
Epoch 35/300, seasonal_2 Loss: 0.0638 | 0.0288
Epoch 36/300, seasonal_2 Loss: 0.0656 | 0.0303
Epoch 37/300, seasonal_2 Loss: 0.0676 | 0.0342
Epoch 38/300, seasonal_2 Loss: 0.0628 | 0.0296
Epoch 39/300, seasonal_2 Loss: 0.0606 | 0.0284
Epoch 40/300, seasonal_2 Loss: 0.0615 | 0.0282
Epoch 41/300, seasonal_2 Loss: 0.0589 | 0.0275
Epoch 42/300, seasonal_2 Loss: 0.0591 | 0.0275
Epoch 43/300, seasonal_2 Loss: 0.0596 | 0.0300
Epoch 44/300, seasonal_2 Loss: 0.0606 | 0.0265
Epoch 45/300, seasonal_2 Loss: 0.0602 | 0.0325
Epoch 46/300, seasonal_2 Loss: 0.0652 | 0.0283
Epoch 47/300, seasonal_2 Loss: 0.0609 | 0.0269
Epoch 48/300, seasonal_2 Loss: 0.0619 | 0.0317
Epoch 49/300, seasonal_2 Loss: 0.0595 | 0.0254
Epoch 50/300, seasonal_2 Loss: 0.0587 | 0.0266
Epoch 51/300, seasonal_2 Loss: 0.0582 | 0.0265
Epoch 52/300, seasonal_2 Loss: 0.0577 | 0.0281
Epoch 53/300, seasonal_2 Loss: 0.0571 | 0.0242
Epoch 54/300, seasonal_2 Loss: 0.0573 | 0.0256
Epoch 55/300, seasonal_2 Loss: 0.0561 | 0.0258
Epoch 56/300, seasonal_2 Loss: 0.0552 | 0.0255
Epoch 57/300, seasonal_2 Loss: 0.0558 | 0.0277
Epoch 58/300, seasonal_2 Loss: 0.0570 | 0.0278
Epoch 59/300, seasonal_2 Loss: 0.0592 | 0.0264
Epoch 60/300, seasonal_2 Loss: 0.0588 | 0.0277
Epoch 61/300, seasonal_2 Loss: 0.0581 | 0.0266
Epoch 62/300, seasonal_2 Loss: 0.0547 | 0.0259
Epoch 63/300, seasonal_2 Loss: 0.0532 | 0.0247
Epoch 64/300, seasonal_2 Loss: 0.0539 | 0.0279
Epoch 65/300, seasonal_2 Loss: 0.0542 | 0.0252
Epoch 66/300, seasonal_2 Loss: 0.0566 | 0.0301
Epoch 67/300, seasonal_2 Loss: 0.0542 | 0.0250
Epoch 68/300, seasonal_2 Loss: 0.0554 | 0.0268
Epoch 69/300, seasonal_2 Loss: 0.0592 | 0.0281
Epoch 70/300, seasonal_2 Loss: 0.0660 | 0.0259
Epoch 71/300, seasonal_2 Loss: 0.0571 | 0.0260
Epoch 72/300, seasonal_2 Loss: 0.0550 | 0.0252
Epoch 73/300, seasonal_2 Loss: 0.0568 | 0.0284
Epoch 74/300, seasonal_2 Loss: 0.0553 | 0.0267
Epoch 75/300, seasonal_2 Loss: 0.0558 | 0.0240
Epoch 76/300, seasonal_2 Loss: 0.0536 | 0.0262
Epoch 77/300, seasonal_2 Loss: 0.0524 | 0.0238
Epoch 78/300, seasonal_2 Loss: 0.0529 | 0.0269
Epoch 79/300, seasonal_2 Loss: 0.0533 | 0.0255
Epoch 80/300, seasonal_2 Loss: 0.0518 | 0.0259
Epoch 81/300, seasonal_2 Loss: 0.0527 | 0.0281
Epoch 82/300, seasonal_2 Loss: 0.0526 | 0.0252
Epoch 83/300, seasonal_2 Loss: 0.0519 | 0.0255
Epoch 84/300, seasonal_2 Loss: 0.0515 | 0.0253
Epoch 85/300, seasonal_2 Loss: 0.0502 | 0.0256
Epoch 86/300, seasonal_2 Loss: 0.0540 | 0.0268
Epoch 87/300, seasonal_2 Loss: 0.0525 | 0.0250
Epoch 88/300, seasonal_2 Loss: 0.0588 | 0.0256
Epoch 89/300, seasonal_2 Loss: 0.0575 | 0.0263
Epoch 90/300, seasonal_2 Loss: 0.0600 | 0.0290
Epoch 91/300, seasonal_2 Loss: 0.0541 | 0.0248
Epoch 92/300, seasonal_2 Loss: 0.0510 | 0.0246
Epoch 93/300, seasonal_2 Loss: 0.0499 | 0.0233
Epoch 94/300, seasonal_2 Loss: 0.0500 | 0.0282
Epoch 95/300, seasonal_2 Loss: 0.0594 | 0.0307
Epoch 96/300, seasonal_2 Loss: 0.0625 | 0.0279
Epoch 97/300, seasonal_2 Loss: 0.0569 | 0.0310
Epoch 98/300, seasonal_2 Loss: 0.0547 | 0.0268
Epoch 99/300, seasonal_2 Loss: 0.0524 | 0.0263
Epoch 100/300, seasonal_2 Loss: 0.0502 | 0.0246
Epoch 101/300, seasonal_2 Loss: 0.0537 | 0.0402
Epoch 102/300, seasonal_2 Loss: 0.0531 | 0.0272
Epoch 103/300, seasonal_2 Loss: 0.0555 | 0.0253
Epoch 104/300, seasonal_2 Loss: 0.0531 | 0.0261
Epoch 105/300, seasonal_2 Loss: 0.0505 | 0.0245
Epoch 106/300, seasonal_2 Loss: 0.0494 | 0.0257
Epoch 107/300, seasonal_2 Loss: 0.0487 | 0.0247
Epoch 108/300, seasonal_2 Loss: 0.0491 | 0.0243
Epoch 109/300, seasonal_2 Loss: 0.0478 | 0.0253
Epoch 110/300, seasonal_2 Loss: 0.0477 | 0.0241
Epoch 111/300, seasonal_2 Loss: 0.0480 | 0.0249
Epoch 112/300, seasonal_2 Loss: 0.0492 | 0.0253
Epoch 113/300, seasonal_2 Loss: 0.0503 | 0.0234
Epoch 114/300, seasonal_2 Loss: 0.0500 | 0.0259
Epoch 115/300, seasonal_2 Loss: 0.0517 | 0.0252
Epoch 116/300, seasonal_2 Loss: 0.0472 | 0.0250
Epoch 117/300, seasonal_2 Loss: 0.0461 | 0.0249
Epoch 118/300, seasonal_2 Loss: 0.0475 | 0.0260
Epoch 119/300, seasonal_2 Loss: 0.0472 | 0.0261
Epoch 120/300, seasonal_2 Loss: 0.0425 | 0.0259
Epoch 121/300, seasonal_2 Loss: 0.0576 | 0.0335
Epoch 122/300, seasonal_2 Loss: 0.0633 | 0.0311
Epoch 123/300, seasonal_2 Loss: 0.0600 | 0.0297
Epoch 124/300, seasonal_2 Loss: 0.0565 | 0.0275
Epoch 125/300, seasonal_2 Loss: 0.0518 | 0.0255
Epoch 126/300, seasonal_2 Loss: 0.0497 | 0.0256
Epoch 127/300, seasonal_2 Loss: 0.0479 | 0.0247
Epoch 128/300, seasonal_2 Loss: 0.0471 | 0.0246
Epoch 129/300, seasonal_2 Loss: 0.0465 | 0.0256
Epoch 130/300, seasonal_2 Loss: 0.0466 | 0.0250
Epoch 131/300, seasonal_2 Loss: 0.0460 | 0.0265
Epoch 132/300, seasonal_2 Loss: 0.0474 | 0.0299
Epoch 133/300, seasonal_2 Loss: 0.0475 | 0.0262
Epoch 134/300, seasonal_2 Loss: 0.0477 | 0.0306
Epoch 135/300, seasonal_2 Loss: 0.0486 | 0.0405
Epoch 136/300, seasonal_2 Loss: 0.0492 | 0.0281
Epoch 137/300, seasonal_2 Loss: 0.0581 | 0.0258
Epoch 138/300, seasonal_2 Loss: 0.0546 | 0.0272
Epoch 139/300, seasonal_2 Loss: 0.0525 | 0.0237
Epoch 140/300, seasonal_2 Loss: 0.0487 | 0.0262
Epoch 141/300, seasonal_2 Loss: 0.0480 | 0.0250
Epoch 142/300, seasonal_2 Loss: 0.0621 | 0.0285
Epoch 143/300, seasonal_2 Loss: 0.0543 | 0.0244
Epoch 144/300, seasonal_2 Loss: 0.0521 | 0.0245
Epoch 145/300, seasonal_2 Loss: 0.0525 | 0.0254
Epoch 146/300, seasonal_2 Loss: 0.0503 | 0.0246
Epoch 147/300, seasonal_2 Loss: 0.0484 | 0.0236
Epoch 148/300, seasonal_2 Loss: 0.0469 | 0.0233
Epoch 149/300, seasonal_2 Loss: 0.0461 | 0.0229
Epoch 150/300, seasonal_2 Loss: 0.0455 | 0.0234
Epoch 151/300, seasonal_2 Loss: 0.0451 | 0.0233
Epoch 152/300, seasonal_2 Loss: 0.0502 | 0.0246
Epoch 153/300, seasonal_2 Loss: 0.0484 | 0.0231
Epoch 154/300, seasonal_2 Loss: 0.0461 | 0.0230
Epoch 155/300, seasonal_2 Loss: 0.0452 | 0.0234
Epoch 156/300, seasonal_2 Loss: 0.0448 | 0.0237
Epoch 157/300, seasonal_2 Loss: 0.0446 | 0.0234
Epoch 158/300, seasonal_2 Loss: 0.0441 | 0.0241
Epoch 159/300, seasonal_2 Loss: 0.0440 | 0.0230
Epoch 160/300, seasonal_2 Loss: 0.0438 | 0.0252
Epoch 161/300, seasonal_2 Loss: 0.0455 | 0.0244
Epoch 162/300, seasonal_2 Loss: 0.0450 | 0.0278
Epoch 163/300, seasonal_2 Loss: 0.0453 | 0.0255
Epoch 164/300, seasonal_2 Loss: 0.0504 | 0.0274
Epoch 165/300, seasonal_2 Loss: 0.0463 | 0.0256
Epoch 166/300, seasonal_2 Loss: 0.0497 | 0.0260
Epoch 167/300, seasonal_2 Loss: 0.0494 | 0.0281
Epoch 168/300, seasonal_2 Loss: 0.0471 | 0.0248
Epoch 169/300, seasonal_2 Loss: 0.0437 | 0.0263
Epoch 170/300, seasonal_2 Loss: 0.0450 | 0.0245
Epoch 171/300, seasonal_2 Loss: 0.0449 | 0.0296
Epoch 172/300, seasonal_2 Loss: 0.0442 | 0.0253
Epoch 173/300, seasonal_2 Loss: 0.0452 | 0.0257
Epoch 174/300, seasonal_2 Loss: 0.0537 | 0.0278
Epoch 175/300, seasonal_2 Loss: 0.0528 | 0.0264
Epoch 176/300, seasonal_2 Loss: 0.0507 | 0.0263
Epoch 177/300, seasonal_2 Loss: 0.0489 | 0.0257
Epoch 178/300, seasonal_2 Loss: 0.0467 | 0.0240
Epoch 179/300, seasonal_2 Loss: 0.0448 | 0.0248
Epoch 180/300, seasonal_2 Loss: 0.0424 | 0.0242
Epoch 181/300, seasonal_2 Loss: 0.0440 | 0.0263
Epoch 182/300, seasonal_2 Loss: 0.0467 | 0.0249
Epoch 183/300, seasonal_2 Loss: 0.0450 | 0.0251
Epoch 184/300, seasonal_2 Loss: 0.0436 | 0.0252
Epoch 185/300, seasonal_2 Loss: 0.0431 | 0.0253
Epoch 186/300, seasonal_2 Loss: 0.0501 | 0.0271
Epoch 187/300, seasonal_2 Loss: 0.0470 | 0.0237
Epoch 188/300, seasonal_2 Loss: 0.0445 | 0.0245
Epoch 189/300, seasonal_2 Loss: 0.0435 | 0.0248
Epoch 190/300, seasonal_2 Loss: 0.0426 | 0.0246
Epoch 191/300, seasonal_2 Loss: 0.0454 | 0.0280
Epoch 192/300, seasonal_2 Loss: 0.0500 | 0.0249
Epoch 193/300, seasonal_2 Loss: 0.0477 | 0.0250
Epoch 194/300, seasonal_2 Loss: 0.0448 | 0.0256
Epoch 195/300, seasonal_2 Loss: 0.0472 | 0.0245
Epoch 196/300, seasonal_2 Loss: 0.0465 | 0.0265
Epoch 197/300, seasonal_2 Loss: 0.0485 | 0.0243
Epoch 198/300, seasonal_2 Loss: 0.0479 | 0.0235
Epoch 199/300, seasonal_2 Loss: 0.0428 | 0.0230
Epoch 200/300, seasonal_2 Loss: 0.0399 | 0.0235
Epoch 201/300, seasonal_2 Loss: 0.0382 | 0.0246
Epoch 202/300, seasonal_2 Loss: 0.0366 | 0.0328
Epoch 203/300, seasonal_2 Loss: 0.0399 | 0.0246
Epoch 204/300, seasonal_2 Loss: 0.0458 | 0.0257
Epoch 205/300, seasonal_2 Loss: 0.0442 | 0.0245
Epoch 206/300, seasonal_2 Loss: 0.0377 | 0.0256
Epoch 207/300, seasonal_2 Loss: 0.0393 | 0.0242
Epoch 208/300, seasonal_2 Loss: 0.0443 | 0.0414
Epoch 209/300, seasonal_2 Loss: 0.0626 | 0.0373
Epoch 210/300, seasonal_2 Loss: 0.0518 | 0.0268
Epoch 211/300, seasonal_2 Loss: 0.0457 | 0.0244
Epoch 212/300, seasonal_2 Loss: 0.0434 | 0.0250
Epoch 213/300, seasonal_2 Loss: 0.0448 | 0.0247
Epoch 214/300, seasonal_2 Loss: 0.0411 | 0.0255
Epoch 215/300, seasonal_2 Loss: 0.0485 | 0.0244
Epoch 216/300, seasonal_2 Loss: 0.0457 | 0.0236
Epoch 217/300, seasonal_2 Loss: 0.0442 | 0.0242
Epoch 218/300, seasonal_2 Loss: 0.0433 | 0.0282
Epoch 219/300, seasonal_2 Loss: 0.0478 | 0.0243
Epoch 220/300, seasonal_2 Loss: 0.0391 | 0.0249
Epoch 221/300, seasonal_2 Loss: 0.0401 | 0.0232
Epoch 222/300, seasonal_2 Loss: 0.0440 | 0.0234
Epoch 223/300, seasonal_2 Loss: 0.0430 | 0.0237
Epoch 224/300, seasonal_2 Loss: 0.0425 | 0.0239
Epoch 225/300, seasonal_2 Loss: 0.0422 | 0.0239
Epoch 226/300, seasonal_2 Loss: 0.0422 | 0.0238
Epoch 227/300, seasonal_2 Loss: 0.0416 | 0.0240
Epoch 228/300, seasonal_2 Loss: 0.0422 | 0.0239
Epoch 229/300, seasonal_2 Loss: 0.0381 | 0.0239
Epoch 230/300, seasonal_2 Loss: 0.0425 | 0.0233
Epoch 231/300, seasonal_2 Loss: 0.0474 | 0.0336
Epoch 232/300, seasonal_2 Loss: 0.0547 | 0.0282
Epoch 233/300, seasonal_2 Loss: 0.0486 | 0.0244
Epoch 234/300, seasonal_2 Loss: 0.0452 | 0.0248
Epoch 235/300, seasonal_2 Loss: 0.0436 | 0.0245
Epoch 236/300, seasonal_2 Loss: 0.0479 | 0.0254
Epoch 237/300, seasonal_2 Loss: 0.0399 | 0.0242
Epoch 238/300, seasonal_2 Loss: 0.0410 | 0.0246
Epoch 239/300, seasonal_2 Loss: 0.0430 | 0.0239
Epoch 240/300, seasonal_2 Loss: 0.0421 | 0.0238
Epoch 241/300, seasonal_2 Loss: 0.0417 | 0.0243
Epoch 242/300, seasonal_2 Loss: 0.0500 | 0.0245
Epoch 243/300, seasonal_2 Loss: 0.0448 | 0.0241
Epoch 244/300, seasonal_2 Loss: 0.0429 | 0.0246
Epoch 245/300, seasonal_2 Loss: 0.0403 | 0.0257
Epoch 246/300, seasonal_2 Loss: 0.0424 | 0.0246
Epoch 247/300, seasonal_2 Loss: 0.0491 | 0.0336
Epoch 248/300, seasonal_2 Loss: 0.0495 | 0.0241
Epoch 249/300, seasonal_2 Loss: 0.0442 | 0.0248
Epoch 250/300, seasonal_2 Loss: 0.0431 | 0.0237
Epoch 251/300, seasonal_2 Loss: 0.0423 | 0.0239
Epoch 252/300, seasonal_2 Loss: 0.0416 | 0.0241
Epoch 253/300, seasonal_2 Loss: 0.0412 | 0.0242
Epoch 254/300, seasonal_2 Loss: 0.0505 | 0.0249
Epoch 255/300, seasonal_2 Loss: 0.0453 | 0.0244
Epoch 256/300, seasonal_2 Loss: 0.0449 | 0.0307
Epoch 257/300, seasonal_2 Loss: 0.0446 | 0.0302
Epoch 258/300, seasonal_2 Loss: 0.0382 | 0.0255
Epoch 259/300, seasonal_2 Loss: 0.0451 | 0.0282
Epoch 260/300, seasonal_2 Loss: 0.0481 | 0.0272
Epoch 261/300, seasonal_2 Loss: 0.0441 | 0.0248
Epoch 262/300, seasonal_2 Loss: 0.0417 | 0.0243
Epoch 263/300, seasonal_2 Loss: 0.0414 | 0.0252
Epoch 264/300, seasonal_2 Loss: 0.0431 | 0.0259
Epoch 265/300, seasonal_2 Loss: 0.0442 | 0.0241
Epoch 266/300, seasonal_2 Loss: 0.0441 | 0.0241
Epoch 267/300, seasonal_2 Loss: 0.0419 | 0.0256
Epoch 268/300, seasonal_2 Loss: 0.0588 | 0.0286
Epoch 269/300, seasonal_2 Loss: 0.0531 | 0.0253
Epoch 270/300, seasonal_2 Loss: 0.0471 | 0.0241
Epoch 271/300, seasonal_2 Loss: 0.0446 | 0.0243
Epoch 272/300, seasonal_2 Loss: 0.0446 | 0.0265
Epoch 273/300, seasonal_2 Loss: 0.0455 | 0.0248
Epoch 274/300, seasonal_2 Loss: 0.0438 | 0.0337
Epoch 275/300, seasonal_2 Loss: 0.0466 | 0.0259
Epoch 276/300, seasonal_2 Loss: 0.0424 | 0.0259
Epoch 277/300, seasonal_2 Loss: 0.0365 | 0.0254
Epoch 278/300, seasonal_2 Loss: 0.0422 | 0.0262
Epoch 279/300, seasonal_2 Loss: 0.0372 | 0.0242
Epoch 280/300, seasonal_2 Loss: 0.0477 | 0.0253
Epoch 281/300, seasonal_2 Loss: 0.0486 | 0.0277
Epoch 282/300, seasonal_2 Loss: 0.0559 | 0.0376
Epoch 283/300, seasonal_2 Loss: 0.0562 | 0.0245
Epoch 284/300, seasonal_2 Loss: 0.0488 | 0.0237
Epoch 285/300, seasonal_2 Loss: 0.0461 | 0.0235
Epoch 286/300, seasonal_2 Loss: 0.0446 | 0.0238
Epoch 287/300, seasonal_2 Loss: 0.0450 | 0.0242
Epoch 288/300, seasonal_2 Loss: 0.0389 | 0.0235
Epoch 289/300, seasonal_2 Loss: 0.0366 | 0.0238
Epoch 290/300, seasonal_2 Loss: 0.0364 | 0.0236
Epoch 291/300, seasonal_2 Loss: 0.0384 | 0.0244
Epoch 292/300, seasonal_2 Loss: 0.0482 | 0.0280
Epoch 293/300, seasonal_2 Loss: 0.0442 | 0.0235
Epoch 294/300, seasonal_2 Loss: 0.0398 | 0.0238
Epoch 295/300, seasonal_2 Loss: 0.0363 | 0.0244
Epoch 296/300, seasonal_2 Loss: 0.0371 | 0.0236
Epoch 297/300, seasonal_2 Loss: 0.0400 | 0.0241
Epoch 298/300, seasonal_2 Loss: 0.0335 | 0.0239
Epoch 299/300, seasonal_2 Loss: 0.0353 | 0.0242
Epoch 300/300, seasonal_2 Loss: 0.0366 | 0.0242
Training seasonal_3 component with params: {'observation_period_num': 33, 'train_rates': 0.9888740408441951, 'learning_rate': 0.0004707699420385272, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9461332238832573}
Epoch 1/300, seasonal_3 Loss: 0.2623 | 0.2042
Epoch 2/300, seasonal_3 Loss: 0.1644 | 0.1418
Epoch 3/300, seasonal_3 Loss: 0.1367 | 0.1293
Epoch 4/300, seasonal_3 Loss: 0.1290 | 0.1133
Epoch 5/300, seasonal_3 Loss: 0.1201 | 0.1061
Epoch 6/300, seasonal_3 Loss: 0.1075 | 0.0979
Epoch 7/300, seasonal_3 Loss: 0.1031 | 0.0840
Epoch 8/300, seasonal_3 Loss: 0.1142 | 0.0940
Epoch 9/300, seasonal_3 Loss: 0.1025 | 0.1003
Epoch 10/300, seasonal_3 Loss: 0.1002 | 0.0910
Epoch 11/300, seasonal_3 Loss: 0.0954 | 0.0863
Epoch 12/300, seasonal_3 Loss: 0.0999 | 0.0755
Epoch 13/300, seasonal_3 Loss: 0.0966 | 0.0878
Epoch 14/300, seasonal_3 Loss: 0.0962 | 0.0937
Epoch 15/300, seasonal_3 Loss: 0.0900 | 0.0813
Epoch 16/300, seasonal_3 Loss: 0.1021 | 0.1083
Epoch 17/300, seasonal_3 Loss: 0.1096 | 0.0958
Epoch 18/300, seasonal_3 Loss: 0.1180 | 0.1252
Epoch 19/300, seasonal_3 Loss: 0.1407 | 0.1325
Epoch 20/300, seasonal_3 Loss: 0.1661 | 0.2192
Epoch 21/300, seasonal_3 Loss: 0.1709 | 0.2541
Epoch 22/300, seasonal_3 Loss: 0.1442 | 0.3617
Epoch 23/300, seasonal_3 Loss: 0.1153 | 0.1349
Epoch 24/300, seasonal_3 Loss: 0.1041 | 0.1145
Epoch 25/300, seasonal_3 Loss: 0.0852 | 0.0754
Epoch 26/300, seasonal_3 Loss: 0.0748 | 0.0571
Epoch 27/300, seasonal_3 Loss: 0.0719 | 0.0512
Epoch 28/300, seasonal_3 Loss: 0.0709 | 0.0485
Epoch 29/300, seasonal_3 Loss: 0.0705 | 0.0491
Epoch 30/300, seasonal_3 Loss: 0.0703 | 0.0486
Epoch 31/300, seasonal_3 Loss: 0.0711 | 0.0485
Epoch 32/300, seasonal_3 Loss: 0.0719 | 0.0490
Epoch 33/300, seasonal_3 Loss: 0.0711 | 0.0505
Epoch 34/300, seasonal_3 Loss: 0.0695 | 0.0478
Epoch 35/300, seasonal_3 Loss: 0.0678 | 0.0451
Epoch 36/300, seasonal_3 Loss: 0.0666 | 0.0441
Epoch 37/300, seasonal_3 Loss: 0.0663 | 0.0465
Epoch 38/300, seasonal_3 Loss: 0.0674 | 0.0548
Epoch 39/300, seasonal_3 Loss: 0.0693 | 0.0681
Epoch 40/300, seasonal_3 Loss: 0.0703 | 0.0678
Epoch 41/300, seasonal_3 Loss: 0.0687 | 0.0554
Epoch 42/300, seasonal_3 Loss: 0.0685 | 0.0447
Epoch 43/300, seasonal_3 Loss: 0.0704 | 0.0499
Epoch 44/300, seasonal_3 Loss: 0.0680 | 0.0411
Epoch 45/300, seasonal_3 Loss: 0.0637 | 0.0418
Epoch 46/300, seasonal_3 Loss: 0.0635 | 0.0428
Epoch 47/300, seasonal_3 Loss: 0.0631 | 0.0430
Epoch 48/300, seasonal_3 Loss: 0.0619 | 0.0417
Epoch 49/300, seasonal_3 Loss: 0.0610 | 0.0399
Epoch 50/300, seasonal_3 Loss: 0.0607 | 0.0396
Epoch 51/300, seasonal_3 Loss: 0.0604 | 0.0407
Epoch 52/300, seasonal_3 Loss: 0.0600 | 0.0404
Epoch 53/300, seasonal_3 Loss: 0.0597 | 0.0388
Epoch 54/300, seasonal_3 Loss: 0.0600 | 0.0386
Epoch 55/300, seasonal_3 Loss: 0.0604 | 0.0401
Epoch 56/300, seasonal_3 Loss: 0.0601 | 0.0384
Epoch 57/300, seasonal_3 Loss: 0.0595 | 0.0380
Epoch 58/300, seasonal_3 Loss: 0.0593 | 0.0382
Epoch 59/300, seasonal_3 Loss: 0.0592 | 0.0390
Epoch 60/300, seasonal_3 Loss: 0.0596 | 0.0403
Epoch 61/300, seasonal_3 Loss: 0.0603 | 0.0417
Epoch 62/300, seasonal_3 Loss: 0.0600 | 0.0399
Epoch 63/300, seasonal_3 Loss: 0.0589 | 0.0377
Epoch 64/300, seasonal_3 Loss: 0.0587 | 0.0387
Epoch 65/300, seasonal_3 Loss: 0.0585 | 0.0379
Epoch 66/300, seasonal_3 Loss: 0.0582 | 0.0365
Epoch 67/300, seasonal_3 Loss: 0.0579 | 0.0366
Epoch 68/300, seasonal_3 Loss: 0.0573 | 0.0367
Epoch 69/300, seasonal_3 Loss: 0.0570 | 0.0373
Epoch 70/300, seasonal_3 Loss: 0.0574 | 0.0378
Epoch 71/300, seasonal_3 Loss: 0.0576 | 0.0376
Epoch 72/300, seasonal_3 Loss: 0.0571 | 0.0369
Epoch 73/300, seasonal_3 Loss: 0.0566 | 0.0366
Epoch 74/300, seasonal_3 Loss: 0.0561 | 0.0363
Epoch 75/300, seasonal_3 Loss: 0.0558 | 0.0359
Epoch 76/300, seasonal_3 Loss: 0.0557 | 0.0358
Epoch 77/300, seasonal_3 Loss: 0.0556 | 0.0358
Epoch 78/300, seasonal_3 Loss: 0.0554 | 0.0357
Epoch 79/300, seasonal_3 Loss: 0.0553 | 0.0357
Epoch 80/300, seasonal_3 Loss: 0.0551 | 0.0356
Epoch 81/300, seasonal_3 Loss: 0.0550 | 0.0355
Epoch 82/300, seasonal_3 Loss: 0.0549 | 0.0355
Epoch 83/300, seasonal_3 Loss: 0.0548 | 0.0355
Epoch 84/300, seasonal_3 Loss: 0.0547 | 0.0354
Epoch 85/300, seasonal_3 Loss: 0.0546 | 0.0354
Epoch 86/300, seasonal_3 Loss: 0.0544 | 0.0353
Epoch 87/300, seasonal_3 Loss: 0.0543 | 0.0353
Epoch 88/300, seasonal_3 Loss: 0.0542 | 0.0352
Epoch 89/300, seasonal_3 Loss: 0.0541 | 0.0352
Epoch 90/300, seasonal_3 Loss: 0.0540 | 0.0352
Epoch 91/300, seasonal_3 Loss: 0.0539 | 0.0351
Epoch 92/300, seasonal_3 Loss: 0.0538 | 0.0351
Epoch 93/300, seasonal_3 Loss: 0.0538 | 0.0350
Epoch 94/300, seasonal_3 Loss: 0.0537 | 0.0350
Epoch 95/300, seasonal_3 Loss: 0.0536 | 0.0350
Epoch 96/300, seasonal_3 Loss: 0.0535 | 0.0349
Epoch 97/300, seasonal_3 Loss: 0.0534 | 0.0349
Epoch 98/300, seasonal_3 Loss: 0.0533 | 0.0348
Epoch 99/300, seasonal_3 Loss: 0.0533 | 0.0348
Epoch 100/300, seasonal_3 Loss: 0.0532 | 0.0348
Epoch 101/300, seasonal_3 Loss: 0.0531 | 0.0347
Epoch 102/300, seasonal_3 Loss: 0.0530 | 0.0347
Epoch 103/300, seasonal_3 Loss: 0.0530 | 0.0346
Epoch 104/300, seasonal_3 Loss: 0.0529 | 0.0346
Epoch 105/300, seasonal_3 Loss: 0.0528 | 0.0346
Epoch 106/300, seasonal_3 Loss: 0.0527 | 0.0345
Epoch 107/300, seasonal_3 Loss: 0.0527 | 0.0345
Epoch 108/300, seasonal_3 Loss: 0.0526 | 0.0344
Epoch 109/300, seasonal_3 Loss: 0.0525 | 0.0344
Epoch 110/300, seasonal_3 Loss: 0.0525 | 0.0344
Epoch 111/300, seasonal_3 Loss: 0.0524 | 0.0343
Epoch 112/300, seasonal_3 Loss: 0.0524 | 0.0343
Epoch 113/300, seasonal_3 Loss: 0.0523 | 0.0342
Epoch 114/300, seasonal_3 Loss: 0.0522 | 0.0342
Epoch 115/300, seasonal_3 Loss: 0.0522 | 0.0342
Epoch 116/300, seasonal_3 Loss: 0.0521 | 0.0341
Epoch 117/300, seasonal_3 Loss: 0.0521 | 0.0341
Epoch 118/300, seasonal_3 Loss: 0.0520 | 0.0341
Epoch 119/300, seasonal_3 Loss: 0.0520 | 0.0340
Epoch 120/300, seasonal_3 Loss: 0.0519 | 0.0340
Epoch 121/300, seasonal_3 Loss: 0.0519 | 0.0340
Epoch 122/300, seasonal_3 Loss: 0.0518 | 0.0339
Epoch 123/300, seasonal_3 Loss: 0.0518 | 0.0339
Epoch 124/300, seasonal_3 Loss: 0.0517 | 0.0338
Epoch 125/300, seasonal_3 Loss: 0.0517 | 0.0338
Epoch 126/300, seasonal_3 Loss: 0.0516 | 0.0338
Epoch 127/300, seasonal_3 Loss: 0.0516 | 0.0337
Epoch 128/300, seasonal_3 Loss: 0.0515 | 0.0337
Epoch 129/300, seasonal_3 Loss: 0.0515 | 0.0337
Epoch 130/300, seasonal_3 Loss: 0.0515 | 0.0336
Epoch 131/300, seasonal_3 Loss: 0.0514 | 0.0336
Epoch 132/300, seasonal_3 Loss: 0.0514 | 0.0336
Epoch 133/300, seasonal_3 Loss: 0.0513 | 0.0335
Epoch 134/300, seasonal_3 Loss: 0.0513 | 0.0335
Epoch 135/300, seasonal_3 Loss: 0.0513 | 0.0335
Epoch 136/300, seasonal_3 Loss: 0.0512 | 0.0334
Epoch 137/300, seasonal_3 Loss: 0.0512 | 0.0334
Epoch 138/300, seasonal_3 Loss: 0.0512 | 0.0334
Epoch 139/300, seasonal_3 Loss: 0.0511 | 0.0334
Epoch 140/300, seasonal_3 Loss: 0.0511 | 0.0333
Epoch 141/300, seasonal_3 Loss: 0.0511 | 0.0333
Epoch 142/300, seasonal_3 Loss: 0.0510 | 0.0333
Epoch 143/300, seasonal_3 Loss: 0.0510 | 0.0332
Epoch 144/300, seasonal_3 Loss: 0.0510 | 0.0332
Epoch 145/300, seasonal_3 Loss: 0.0509 | 0.0332
Epoch 146/300, seasonal_3 Loss: 0.0509 | 0.0331
Epoch 147/300, seasonal_3 Loss: 0.0509 | 0.0331
Epoch 148/300, seasonal_3 Loss: 0.0509 | 0.0331
Epoch 149/300, seasonal_3 Loss: 0.0508 | 0.0331
Epoch 150/300, seasonal_3 Loss: 0.0508 | 0.0330
Epoch 151/300, seasonal_3 Loss: 0.0508 | 0.0330
Epoch 152/300, seasonal_3 Loss: 0.0508 | 0.0330
Epoch 153/300, seasonal_3 Loss: 0.0507 | 0.0330
Epoch 154/300, seasonal_3 Loss: 0.0507 | 0.0329
Epoch 155/300, seasonal_3 Loss: 0.0507 | 0.0329
Epoch 156/300, seasonal_3 Loss: 0.0507 | 0.0329
Epoch 157/300, seasonal_3 Loss: 0.0506 | 0.0329
Epoch 158/300, seasonal_3 Loss: 0.0506 | 0.0328
Epoch 159/300, seasonal_3 Loss: 0.0506 | 0.0328
Epoch 160/300, seasonal_3 Loss: 0.0506 | 0.0328
Epoch 161/300, seasonal_3 Loss: 0.0506 | 0.0328
Epoch 162/300, seasonal_3 Loss: 0.0505 | 0.0327
Epoch 163/300, seasonal_3 Loss: 0.0505 | 0.0327
Epoch 164/300, seasonal_3 Loss: 0.0505 | 0.0327
Epoch 165/300, seasonal_3 Loss: 0.0505 | 0.0327
Epoch 166/300, seasonal_3 Loss: 0.0505 | 0.0327
Epoch 167/300, seasonal_3 Loss: 0.0504 | 0.0326
Epoch 168/300, seasonal_3 Loss: 0.0504 | 0.0326
Epoch 169/300, seasonal_3 Loss: 0.0504 | 0.0326
Epoch 170/300, seasonal_3 Loss: 0.0504 | 0.0326
Epoch 171/300, seasonal_3 Loss: 0.0504 | 0.0326
Epoch 172/300, seasonal_3 Loss: 0.0504 | 0.0325
Epoch 173/300, seasonal_3 Loss: 0.0503 | 0.0325
Epoch 174/300, seasonal_3 Loss: 0.0503 | 0.0325
Epoch 175/300, seasonal_3 Loss: 0.0503 | 0.0325
Epoch 176/300, seasonal_3 Loss: 0.0503 | 0.0325
Epoch 177/300, seasonal_3 Loss: 0.0503 | 0.0324
Epoch 178/300, seasonal_3 Loss: 0.0503 | 0.0324
Epoch 179/300, seasonal_3 Loss: 0.0503 | 0.0324
Epoch 180/300, seasonal_3 Loss: 0.0503 | 0.0324
Epoch 181/300, seasonal_3 Loss: 0.0502 | 0.0324
Epoch 182/300, seasonal_3 Loss: 0.0502 | 0.0324
Epoch 183/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 184/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 185/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 186/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 187/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 188/300, seasonal_3 Loss: 0.0502 | 0.0323
Epoch 189/300, seasonal_3 Loss: 0.0501 | 0.0323
Epoch 190/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 191/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 192/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 193/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 194/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 195/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 196/300, seasonal_3 Loss: 0.0501 | 0.0322
Epoch 197/300, seasonal_3 Loss: 0.0501 | 0.0321
Epoch 198/300, seasonal_3 Loss: 0.0501 | 0.0321
Epoch 199/300, seasonal_3 Loss: 0.0501 | 0.0321
Epoch 200/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 201/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 202/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 203/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 204/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 205/300, seasonal_3 Loss: 0.0500 | 0.0321
Epoch 206/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 207/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 208/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 209/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 210/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 211/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 212/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 213/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 214/300, seasonal_3 Loss: 0.0500 | 0.0320
Epoch 215/300, seasonal_3 Loss: 0.0499 | 0.0320
Epoch 216/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 217/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 218/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 219/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 220/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 221/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 222/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 223/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 224/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 225/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 226/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 227/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 228/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 229/300, seasonal_3 Loss: 0.0499 | 0.0319
Epoch 230/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 231/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 232/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 233/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 234/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 235/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 236/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 237/300, seasonal_3 Loss: 0.0499 | 0.0318
Epoch 238/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 239/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 240/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 241/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 242/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 243/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 244/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 245/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 246/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 247/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 248/300, seasonal_3 Loss: 0.0498 | 0.0318
Epoch 249/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 250/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 251/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 252/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 253/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 254/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 255/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 256/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 257/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 258/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 259/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 260/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 261/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 262/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 263/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 264/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 265/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 266/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 267/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 268/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 269/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 270/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 271/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 272/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 273/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 274/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 275/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 276/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 277/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 278/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 279/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 280/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 281/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 282/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 283/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 284/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 285/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 286/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 287/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 288/300, seasonal_3 Loss: 0.0498 | 0.0317
Epoch 289/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 290/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 291/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 292/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 293/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 294/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 295/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 296/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 297/300, seasonal_3 Loss: 0.0498 | 0.0316
Epoch 298/300, seasonal_3 Loss: 0.0497 | 0.0316
Epoch 299/300, seasonal_3 Loss: 0.0497 | 0.0316
Epoch 300/300, seasonal_3 Loss: 0.0497 | 0.0316
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8087906059086023, 'learning_rate': 0.00011183111229191689, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9862243716668841}
Epoch 1/300, resid Loss: 0.2941 | 0.1752
Epoch 2/300, resid Loss: 0.1447 | 0.1049
Epoch 3/300, resid Loss: 0.1237 | 0.0858
Epoch 4/300, resid Loss: 0.1144 | 0.0758
Epoch 5/300, resid Loss: 0.1093 | 0.0703
Epoch 6/300, resid Loss: 0.1061 | 0.0674
Epoch 7/300, resid Loss: 0.1046 | 0.0670
Epoch 8/300, resid Loss: 0.1040 | 0.0684
Epoch 9/300, resid Loss: 0.1033 | 0.0684
Epoch 10/300, resid Loss: 0.1025 | 0.0659
Epoch 11/300, resid Loss: 0.1024 | 0.0625
Epoch 12/300, resid Loss: 0.1021 | 0.0591
Epoch 13/300, resid Loss: 0.1007 | 0.0563
Epoch 14/300, resid Loss: 0.0983 | 0.0536
Epoch 15/300, resid Loss: 0.0954 | 0.0511
Epoch 16/300, resid Loss: 0.0924 | 0.0488
Epoch 17/300, resid Loss: 0.0896 | 0.0469
Epoch 18/300, resid Loss: 0.0872 | 0.0452
Epoch 19/300, resid Loss: 0.0855 | 0.0445
Epoch 20/300, resid Loss: 0.0847 | 0.0443
Epoch 21/300, resid Loss: 0.0840 | 0.0438
Epoch 22/300, resid Loss: 0.0834 | 0.0429
Epoch 23/300, resid Loss: 0.0830 | 0.0421
Epoch 24/300, resid Loss: 0.0826 | 0.0415
Epoch 25/300, resid Loss: 0.0823 | 0.0409
Epoch 26/300, resid Loss: 0.0820 | 0.0403
Epoch 27/300, resid Loss: 0.0816 | 0.0399
Epoch 28/300, resid Loss: 0.0812 | 0.0398
Epoch 29/300, resid Loss: 0.0804 | 0.0394
Epoch 30/300, resid Loss: 0.0795 | 0.0390
Epoch 31/300, resid Loss: 0.0786 | 0.0385
Epoch 32/300, resid Loss: 0.0779 | 0.0381
Epoch 33/300, resid Loss: 0.0772 | 0.0376
Epoch 34/300, resid Loss: 0.0766 | 0.0372
Epoch 35/300, resid Loss: 0.0760 | 0.0367
Epoch 36/300, resid Loss: 0.0754 | 0.0363
Epoch 37/300, resid Loss: 0.0748 | 0.0358
Epoch 38/300, resid Loss: 0.0741 | 0.0353
Epoch 39/300, resid Loss: 0.0735 | 0.0348
Epoch 40/300, resid Loss: 0.0728 | 0.0344
Epoch 41/300, resid Loss: 0.0721 | 0.0339
Epoch 42/300, resid Loss: 0.0715 | 0.0335
Epoch 43/300, resid Loss: 0.0709 | 0.0332
Epoch 44/300, resid Loss: 0.0704 | 0.0328
Epoch 45/300, resid Loss: 0.0700 | 0.0326
Epoch 46/300, resid Loss: 0.0697 | 0.0324
Epoch 47/300, resid Loss: 0.0695 | 0.0323
Epoch 48/300, resid Loss: 0.0692 | 0.0322
Epoch 49/300, resid Loss: 0.0690 | 0.0322
Epoch 50/300, resid Loss: 0.0687 | 0.0321
Epoch 51/300, resid Loss: 0.0685 | 0.0320
Epoch 52/300, resid Loss: 0.0684 | 0.0320
Epoch 53/300, resid Loss: 0.0682 | 0.0319
Epoch 54/300, resid Loss: 0.0680 | 0.0319
Epoch 55/300, resid Loss: 0.0679 | 0.0318
Epoch 56/300, resid Loss: 0.0678 | 0.0318
Epoch 57/300, resid Loss: 0.0678 | 0.0318
Epoch 58/300, resid Loss: 0.0677 | 0.0318
Epoch 59/300, resid Loss: 0.0677 | 0.0318
Epoch 60/300, resid Loss: 0.0677 | 0.0320
Epoch 61/300, resid Loss: 0.0676 | 0.0323
Epoch 62/300, resid Loss: 0.0674 | 0.0326
Epoch 63/300, resid Loss: 0.0672 | 0.0331
Epoch 64/300, resid Loss: 0.0670 | 0.0332
Epoch 65/300, resid Loss: 0.0668 | 0.0330
Epoch 66/300, resid Loss: 0.0665 | 0.0329
Epoch 67/300, resid Loss: 0.0662 | 0.0327
Epoch 68/300, resid Loss: 0.0660 | 0.0325
Epoch 69/300, resid Loss: 0.0658 | 0.0323
Epoch 70/300, resid Loss: 0.0655 | 0.0321
Epoch 71/300, resid Loss: 0.0653 | 0.0318
Epoch 72/300, resid Loss: 0.0650 | 0.0315
Epoch 73/300, resid Loss: 0.0648 | 0.0312
Epoch 74/300, resid Loss: 0.0646 | 0.0308
Epoch 75/300, resid Loss: 0.0643 | 0.0306
Epoch 76/300, resid Loss: 0.0641 | 0.0303
Epoch 77/300, resid Loss: 0.0638 | 0.0300
Epoch 78/300, resid Loss: 0.0636 | 0.0298
Epoch 79/300, resid Loss: 0.0634 | 0.0295
Epoch 80/300, resid Loss: 0.0632 | 0.0292
Epoch 81/300, resid Loss: 0.0630 | 0.0290
Epoch 82/300, resid Loss: 0.0628 | 0.0287
Epoch 83/300, resid Loss: 0.0627 | 0.0285
Epoch 84/300, resid Loss: 0.0626 | 0.0283
Epoch 85/300, resid Loss: 0.0625 | 0.0282
Epoch 86/300, resid Loss: 0.0625 | 0.0281
Epoch 87/300, resid Loss: 0.0624 | 0.0282
Epoch 88/300, resid Loss: 0.0625 | 0.0287
Epoch 89/300, resid Loss: 0.0627 | 0.0300
Epoch 90/300, resid Loss: 0.0632 | 0.0335
Epoch 91/300, resid Loss: 0.0634 | 0.0333
Epoch 92/300, resid Loss: 0.0631 | 0.0321
Epoch 93/300, resid Loss: 0.0629 | 0.0313
Epoch 94/300, resid Loss: 0.0628 | 0.0307
Epoch 95/300, resid Loss: 0.0626 | 0.0301
Epoch 96/300, resid Loss: 0.0623 | 0.0295
Epoch 97/300, resid Loss: 0.0619 | 0.0290
Epoch 98/300, resid Loss: 0.0615 | 0.0286
Epoch 99/300, resid Loss: 0.0612 | 0.0282
Epoch 100/300, resid Loss: 0.0608 | 0.0280
Epoch 101/300, resid Loss: 0.0606 | 0.0278
Epoch 102/300, resid Loss: 0.0604 | 0.0277
Epoch 103/300, resid Loss: 0.0603 | 0.0278
Epoch 104/300, resid Loss: 0.0604 | 0.0281
Epoch 105/300, resid Loss: 0.0606 | 0.0286
Epoch 106/300, resid Loss: 0.0608 | 0.0283
Epoch 107/300, resid Loss: 0.0608 | 0.0277
Epoch 108/300, resid Loss: 0.0607 | 0.0274
Epoch 109/300, resid Loss: 0.0610 | 0.0273
Epoch 110/300, resid Loss: 0.0610 | 0.0268
Epoch 111/300, resid Loss: 0.0606 | 0.0264
Epoch 112/300, resid Loss: 0.0603 | 0.0262
Epoch 113/300, resid Loss: 0.0601 | 0.0261
Epoch 114/300, resid Loss: 0.0597 | 0.0261
Epoch 115/300, resid Loss: 0.0593 | 0.0261
Epoch 116/300, resid Loss: 0.0589 | 0.0260
Epoch 117/300, resid Loss: 0.0587 | 0.0260
Epoch 118/300, resid Loss: 0.0585 | 0.0259
Epoch 119/300, resid Loss: 0.0584 | 0.0259
Epoch 120/300, resid Loss: 0.0582 | 0.0259
Epoch 121/300, resid Loss: 0.0582 | 0.0259
Epoch 122/300, resid Loss: 0.0581 | 0.0258
Epoch 123/300, resid Loss: 0.0580 | 0.0257
Epoch 124/300, resid Loss: 0.0579 | 0.0257
Epoch 125/300, resid Loss: 0.0578 | 0.0257
Epoch 126/300, resid Loss: 0.0577 | 0.0257
Epoch 127/300, resid Loss: 0.0576 | 0.0257
Epoch 128/300, resid Loss: 0.0576 | 0.0257
Epoch 129/300, resid Loss: 0.0575 | 0.0257
Epoch 130/300, resid Loss: 0.0575 | 0.0258
Epoch 131/300, resid Loss: 0.0574 | 0.0258
Epoch 132/300, resid Loss: 0.0573 | 0.0258
Epoch 133/300, resid Loss: 0.0573 | 0.0258
Epoch 134/300, resid Loss: 0.0572 | 0.0258
Epoch 135/300, resid Loss: 0.0571 | 0.0259
Epoch 136/300, resid Loss: 0.0570 | 0.0259
Epoch 137/300, resid Loss: 0.0570 | 0.0260
Epoch 138/300, resid Loss: 0.0569 | 0.0260
Epoch 139/300, resid Loss: 0.0568 | 0.0263
Epoch 140/300, resid Loss: 0.0568 | 0.0264
Epoch 141/300, resid Loss: 0.0567 | 0.0264
Epoch 142/300, resid Loss: 0.0566 | 0.0262
Epoch 143/300, resid Loss: 0.0565 | 0.0261
Epoch 144/300, resid Loss: 0.0563 | 0.0260
Epoch 145/300, resid Loss: 0.0563 | 0.0260
Epoch 146/300, resid Loss: 0.0562 | 0.0260
Epoch 147/300, resid Loss: 0.0561 | 0.0262
Epoch 148/300, resid Loss: 0.0561 | 0.0266
Epoch 149/300, resid Loss: 0.0561 | 0.0270
Epoch 150/300, resid Loss: 0.0560 | 0.0270
Epoch 151/300, resid Loss: 0.0560 | 0.0268
Epoch 152/300, resid Loss: 0.0559 | 0.0267
Epoch 153/300, resid Loss: 0.0558 | 0.0266
Epoch 154/300, resid Loss: 0.0557 | 0.0266
Epoch 155/300, resid Loss: 0.0556 | 0.0267
Epoch 156/300, resid Loss: 0.0556 | 0.0270
Epoch 157/300, resid Loss: 0.0555 | 0.0272
Epoch 158/300, resid Loss: 0.0555 | 0.0273
Epoch 159/300, resid Loss: 0.0554 | 0.0273
Epoch 160/300, resid Loss: 0.0552 | 0.0272
Epoch 161/300, resid Loss: 0.0550 | 0.0272
Epoch 162/300, resid Loss: 0.0547 | 0.0273
Epoch 163/300, resid Loss: 0.0542 | 0.0274
Epoch 164/300, resid Loss: 0.0536 | 0.0277
Epoch 165/300, resid Loss: 0.0531 | 0.0283
Epoch 166/300, resid Loss: 0.0534 | 0.0280
Epoch 167/300, resid Loss: 0.0531 | 0.0276
Epoch 168/300, resid Loss: 0.0534 | 0.0277
Epoch 169/300, resid Loss: 0.0542 | 0.0290
Epoch 170/300, resid Loss: 0.0550 | 0.0281
Epoch 171/300, resid Loss: 0.0539 | 0.0276
Epoch 172/300, resid Loss: 0.0528 | 0.0261
Epoch 173/300, resid Loss: 0.0517 | 0.0262
Epoch 174/300, resid Loss: 0.0515 | 0.0261
Epoch 175/300, resid Loss: 0.0513 | 0.0273
Epoch 176/300, resid Loss: 0.0516 | 0.0254
Epoch 177/300, resid Loss: 0.0531 | 0.0279
Epoch 178/300, resid Loss: 0.0514 | 0.0253
Epoch 179/300, resid Loss: 0.0503 | 0.0254
Epoch 180/300, resid Loss: 0.0500 | 0.0253
Epoch 181/300, resid Loss: 0.0515 | 0.0292
Epoch 182/300, resid Loss: 0.0519 | 0.0256
Epoch 183/300, resid Loss: 0.0505 | 0.0257
Epoch 184/300, resid Loss: 0.0501 | 0.0253
Epoch 185/300, resid Loss: 0.0496 | 0.0254
Epoch 186/300, resid Loss: 0.0493 | 0.0253
Epoch 187/300, resid Loss: 0.0508 | 0.0302
Epoch 188/300, resid Loss: 0.0516 | 0.0256
Epoch 189/300, resid Loss: 0.0504 | 0.0264
Epoch 190/300, resid Loss: 0.0509 | 0.0254
Epoch 191/300, resid Loss: 0.0493 | 0.0253
Epoch 192/300, resid Loss: 0.0494 | 0.0263
Epoch 193/300, resid Loss: 0.0499 | 0.0257
Epoch 194/300, resid Loss: 0.0496 | 0.0254
Epoch 195/300, resid Loss: 0.0491 | 0.0253
Epoch 196/300, resid Loss: 0.0488 | 0.0256
Epoch 197/300, resid Loss: 0.0490 | 0.0251
Epoch 198/300, resid Loss: 0.0499 | 0.0300
Epoch 199/300, resid Loss: 0.0506 | 0.0255
Epoch 200/300, resid Loss: 0.0493 | 0.0255
Epoch 201/300, resid Loss: 0.0491 | 0.0255
Epoch 202/300, resid Loss: 0.0489 | 0.0253
Epoch 203/300, resid Loss: 0.0487 | 0.0252
Epoch 204/300, resid Loss: 0.0484 | 0.0251
Epoch 205/300, resid Loss: 0.0482 | 0.0252
Epoch 206/300, resid Loss: 0.0482 | 0.0249
Epoch 207/300, resid Loss: 0.0521 | 0.0289
Epoch 208/300, resid Loss: 0.0507 | 0.0259
Epoch 209/300, resid Loss: 0.0493 | 0.0261
Epoch 210/300, resid Loss: 0.0490 | 0.0256
Epoch 211/300, resid Loss: 0.0488 | 0.0257
Epoch 212/300, resid Loss: 0.0487 | 0.0256
Epoch 213/300, resid Loss: 0.0485 | 0.0255
Epoch 214/300, resid Loss: 0.0484 | 0.0254
Epoch 215/300, resid Loss: 0.0482 | 0.0253
Epoch 216/300, resid Loss: 0.0481 | 0.0252
Epoch 217/300, resid Loss: 0.0479 | 0.0251
Epoch 218/300, resid Loss: 0.0478 | 0.0250
Epoch 219/300, resid Loss: 0.0477 | 0.0251
Epoch 220/300, resid Loss: 0.0479 | 0.0249
Epoch 221/300, resid Loss: 0.0486 | 0.0280
Epoch 222/300, resid Loss: 0.0492 | 0.0260
Epoch 223/300, resid Loss: 0.0487 | 0.0255
Epoch 224/300, resid Loss: 0.0484 | 0.0255
Epoch 225/300, resid Loss: 0.0483 | 0.0255
Epoch 226/300, resid Loss: 0.0480 | 0.0251
Epoch 227/300, resid Loss: 0.0478 | 0.0254
Epoch 228/300, resid Loss: 0.0476 | 0.0249
Epoch 229/300, resid Loss: 0.0476 | 0.0258
Epoch 230/300, resid Loss: 0.0482 | 0.0250
Epoch 231/300, resid Loss: 0.0474 | 0.0249
Epoch 232/300, resid Loss: 0.0478 | 0.0276
Epoch 233/300, resid Loss: 0.0486 | 0.0258
Epoch 234/300, resid Loss: 0.0481 | 0.0253
Epoch 235/300, resid Loss: 0.0476 | 0.0251
Epoch 236/300, resid Loss: 0.0473 | 0.0252
Epoch 237/300, resid Loss: 0.0470 | 0.0249
Epoch 238/300, resid Loss: 0.0470 | 0.0254
Epoch 239/300, resid Loss: 0.0477 | 0.0249
Epoch 240/300, resid Loss: 0.0468 | 0.0251
Epoch 241/300, resid Loss: 0.0470 | 0.0249
Epoch 242/300, resid Loss: 0.0497 | 0.0300
Epoch 243/300, resid Loss: 0.0488 | 0.0265
Epoch 244/300, resid Loss: 0.0483 | 0.0266
Epoch 245/300, resid Loss: 0.0481 | 0.0267
Epoch 246/300, resid Loss: 0.0480 | 0.0267
Epoch 247/300, resid Loss: 0.0479 | 0.0268
Epoch 248/300, resid Loss: 0.0478 | 0.0268
Epoch 249/300, resid Loss: 0.0477 | 0.0269
Epoch 250/300, resid Loss: 0.0476 | 0.0267
Epoch 251/300, resid Loss: 0.0475 | 0.0268
Epoch 252/300, resid Loss: 0.0474 | 0.0264
Epoch 253/300, resid Loss: 0.0474 | 0.0268
Epoch 254/300, resid Loss: 0.0473 | 0.0259
Epoch 255/300, resid Loss: 0.0472 | 0.0264
Epoch 256/300, resid Loss: 0.0470 | 0.0257
Epoch 257/300, resid Loss: 0.0469 | 0.0260
Epoch 258/300, resid Loss: 0.0468 | 0.0256
Epoch 259/300, resid Loss: 0.0467 | 0.0257
Epoch 260/300, resid Loss: 0.0467 | 0.0254
Epoch 261/300, resid Loss: 0.0465 | 0.0257
Epoch 262/300, resid Loss: 0.0466 | 0.0252
Epoch 263/300, resid Loss: 0.0466 | 0.0275
Epoch 264/300, resid Loss: 0.0473 | 0.0259
Epoch 265/300, resid Loss: 0.0466 | 0.0254
Epoch 266/300, resid Loss: 0.0465 | 0.0274
Epoch 267/300, resid Loss: 0.0470 | 0.0257
Epoch 268/300, resid Loss: 0.0463 | 0.0255
Epoch 269/300, resid Loss: 0.0462 | 0.0267
Epoch 270/300, resid Loss: 0.0467 | 0.0254
Epoch 271/300, resid Loss: 0.0463 | 0.0294
Epoch 272/300, resid Loss: 0.0471 | 0.0270
Epoch 273/300, resid Loss: 0.0466 | 0.0260
Epoch 274/300, resid Loss: 0.0461 | 0.0261
Epoch 275/300, resid Loss: 0.0460 | 0.0259
Epoch 276/300, resid Loss: 0.0458 | 0.0268
Epoch 277/300, resid Loss: 0.0461 | 0.0256
Epoch 278/300, resid Loss: 0.0464 | 0.0324
Epoch 279/300, resid Loss: 0.0472 | 0.0283
Epoch 280/300, resid Loss: 0.0467 | 0.0275
Epoch 281/300, resid Loss: 0.0463 | 0.0266
Epoch 282/300, resid Loss: 0.0459 | 0.0267
Epoch 283/300, resid Loss: 0.0457 | 0.0269
Epoch 284/300, resid Loss: 0.0456 | 0.0269
Epoch 285/300, resid Loss: 0.0455 | 0.0276
Epoch 286/300, resid Loss: 0.0456 | 0.0270
Epoch 287/300, resid Loss: 0.0453 | 0.0302
Epoch 288/300, resid Loss: 0.0459 | 0.0271
Epoch 289/300, resid Loss: 0.0453 | 0.0322
Epoch 290/300, resid Loss: 0.0461 | 0.0279
Epoch 291/300, resid Loss: 0.0452 | 0.0292
Epoch 292/300, resid Loss: 0.0452 | 0.0289
Epoch 293/300, resid Loss: 0.0450 | 0.0314
Epoch 294/300, resid Loss: 0.0453 | 0.0286
Epoch 295/300, resid Loss: 0.0453 | 0.0330
Epoch 296/300, resid Loss: 0.0459 | 0.0292
Epoch 297/300, resid Loss: 0.0449 | 0.0300
Epoch 298/300, resid Loss: 0.0447 | 0.0313
Epoch 299/300, resid Loss: 0.0447 | 0.0307
Epoch 300/300, resid Loss: 0.0445 | 0.0321
Runtime (seconds): 2106.5230207443237
0.000259005902421501
[198.33118]
[-0.1373783]
[-4.3797984]
[4.009307]
[0.6534624]
[9.299464]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 744.4031472839415
RMSE: 27.28375244140625
MAE: 27.28375244140625
R-squared: nan
[207.77625]
