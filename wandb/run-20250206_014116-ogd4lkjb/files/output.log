[32m[I 2025-02-06 01:41:20,231][0m A new study created in memory with name: no-name-7f7356a3-4919-41f7-b544-d0208931014e[0m
[32m[I 2025-02-06 01:42:06,531][0m Trial 0 finished with value: 1.018982281124657 and parameters: {'observation_period_num': 130, 'train_rates': 0.9360315402301991, 'learning_rate': 2.0878574109783987e-06, 'batch_size': 133, 'step_size': 3, 'gamma': 0.7726555355403283}. Best is trial 0 with value: 1.018982281124657.[0m
[32m[I 2025-02-06 01:42:34,345][0m Trial 1 finished with value: 0.23567353765610682 and parameters: {'observation_period_num': 94, 'train_rates': 0.7502439008845079, 'learning_rate': 0.0003032434810468694, 'batch_size': 190, 'step_size': 6, 'gamma': 0.9319841884852473}. Best is trial 1 with value: 0.23567353765610682.[0m
[32m[I 2025-02-06 01:43:00,565][0m Trial 2 finished with value: 1.2011284437444476 and parameters: {'observation_period_num': 87, 'train_rates': 0.8134788019223036, 'learning_rate': 2.1542671489606066e-06, 'batch_size': 217, 'step_size': 7, 'gamma': 0.8856987575982265}. Best is trial 1 with value: 0.23567353765610682.[0m
[32m[I 2025-02-06 01:44:14,843][0m Trial 3 finished with value: 0.06280663234685532 and parameters: {'observation_period_num': 13, 'train_rates': 0.8357593069725695, 'learning_rate': 9.671743429252011e-05, 'batch_size': 75, 'step_size': 4, 'gamma': 0.7742465717677285}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:44:43,131][0m Trial 4 finished with value: 0.6098694735858988 and parameters: {'observation_period_num': 9, 'train_rates': 0.7641479582616845, 'learning_rate': 3.5933919383037485e-06, 'batch_size': 198, 'step_size': 3, 'gamma': 0.9531441498048404}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:45:48,960][0m Trial 5 finished with value: 0.1421944777945947 and parameters: {'observation_period_num': 25, 'train_rates': 0.6759191631091563, 'learning_rate': 0.00013791249258196384, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9291136492983771}. Best is trial 3 with value: 0.06280663234685532.[0m
Early stopping at epoch 83
[32m[I 2025-02-06 01:46:20,534][0m Trial 6 finished with value: 0.34591255417789324 and parameters: {'observation_period_num': 156, 'train_rates': 0.9118280089229566, 'learning_rate': 0.0005625019724879279, 'batch_size': 156, 'step_size': 1, 'gamma': 0.8551873100243496}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:47:01,177][0m Trial 7 finished with value: 0.5027255713275918 and parameters: {'observation_period_num': 213, 'train_rates': 0.8379445044954382, 'learning_rate': 1.3731947228595343e-05, 'batch_size': 129, 'step_size': 2, 'gamma': 0.9027510903745798}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:48:05,907][0m Trial 8 finished with value: 0.10939652385223554 and parameters: {'observation_period_num': 167, 'train_rates': 0.7821465275294605, 'learning_rate': 0.0001226755649898509, 'batch_size': 77, 'step_size': 11, 'gamma': 0.8551466270752429}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:49:11,007][0m Trial 9 finished with value: 0.07674757350030256 and parameters: {'observation_period_num': 14, 'train_rates': 0.7986865737894352, 'learning_rate': 4.095663627343208e-05, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8658019236125349}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:52:22,788][0m Trial 10 finished with value: 0.2295181026061376 and parameters: {'observation_period_num': 66, 'train_rates': 0.6036208616771767, 'learning_rate': 1.5240317188772023e-05, 'batch_size': 22, 'step_size': 10, 'gamma': 0.7554096687079775}. Best is trial 3 with value: 0.06280663234685532.[0m
[32m[I 2025-02-06 01:53:37,893][0m Trial 11 finished with value: 0.057116098151096424 and parameters: {'observation_period_num': 45, 'train_rates': 0.8641659825954136, 'learning_rate': 4.927333997943685e-05, 'batch_size': 73, 'step_size': 15, 'gamma': 0.8075600460585888}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 01:56:20,508][0m Trial 12 finished with value: 0.062016040338596826 and parameters: {'observation_period_num': 58, 'train_rates': 0.8787974851726352, 'learning_rate': 5.76389912705904e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8071244980157698}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:00:31,799][0m Trial 13 finished with value: 0.059081141286221085 and parameters: {'observation_period_num': 55, 'train_rates': 0.8898222417740195, 'learning_rate': 3.5840918229067925e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8085662394599401}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:02:38,347][0m Trial 14 finished with value: 0.12080982732772827 and parameters: {'observation_period_num': 52, 'train_rates': 0.9574021371423478, 'learning_rate': 1.4160332276991437e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8173523506929332}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:03:32,158][0m Trial 15 finished with value: 0.5708942413330078 and parameters: {'observation_period_num': 97, 'train_rates': 0.9841509181132178, 'learning_rate': 8.117075598711158e-06, 'batch_size': 115, 'step_size': 13, 'gamma': 0.8196763647381207}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:05:20,091][0m Trial 16 finished with value: 0.23751080813716133 and parameters: {'observation_period_num': 252, 'train_rates': 0.8824524349561822, 'learning_rate': 2.8673985437163048e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9837809272358669}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:05:45,493][0m Trial 17 finished with value: 0.6808873254319896 and parameters: {'observation_period_num': 122, 'train_rates': 0.8792556492920566, 'learning_rate': 5.809247960086212e-06, 'batch_size': 249, 'step_size': 9, 'gamma': 0.7965282921261955}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:06:34,418][0m Trial 18 finished with value: 0.16312440537917344 and parameters: {'observation_period_num': 51, 'train_rates': 0.7129209203206901, 'learning_rate': 0.0002850347497739241, 'batch_size': 104, 'step_size': 13, 'gamma': 0.8407406011414708}. Best is trial 11 with value: 0.057116098151096424.[0m
[32m[I 2025-02-06 02:11:17,407][0m Trial 19 finished with value: 0.052149504896194215 and parameters: {'observation_period_num': 38, 'train_rates': 0.9145759039803693, 'learning_rate': 0.0009514347107393815, 'batch_size': 20, 'step_size': 14, 'gamma': 0.8272531586444335}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:13:00,928][0m Trial 20 finished with value: 0.09280259583841313 and parameters: {'observation_period_num': 34, 'train_rates': 0.9384949774687203, 'learning_rate': 0.0009440882749803856, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8338392351740372}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:17:51,515][0m Trial 21 finished with value: 0.06916131698272444 and parameters: {'observation_period_num': 77, 'train_rates': 0.9054946625027703, 'learning_rate': 2.9497832287992664e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7890363982306471}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:20:13,080][0m Trial 22 finished with value: 0.3238648451762657 and parameters: {'observation_period_num': 38, 'train_rates': 0.8513952298844312, 'learning_rate': 1.076503526326593e-06, 'batch_size': 39, 'step_size': 14, 'gamma': 0.8364178601910454}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:26:18,723][0m Trial 23 finished with value: 0.163650856776671 and parameters: {'observation_period_num': 110, 'train_rates': 0.9847092625532131, 'learning_rate': 0.0002434425900379673, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7858565544579516}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:27:15,452][0m Trial 24 finished with value: 0.15167227301789427 and parameters: {'observation_period_num': 74, 'train_rates': 0.9104373478996807, 'learning_rate': 0.0009755337256049074, 'batch_size': 102, 'step_size': 14, 'gamma': 0.8837872972924314}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:28:43,584][0m Trial 25 finished with value: 0.05243528780554587 and parameters: {'observation_period_num': 36, 'train_rates': 0.8719489272461453, 'learning_rate': 6.610014460929216e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.8186962371220811}. Best is trial 19 with value: 0.052149504896194215.[0m
[32m[I 2025-02-06 02:30:16,507][0m Trial 26 finished with value: 0.04953435858534205 and parameters: {'observation_period_num': 38, 'train_rates': 0.8574527770586952, 'learning_rate': 8.164520937485249e-05, 'batch_size': 59, 'step_size': 12, 'gamma': 0.7516621654178448}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:31:48,065][0m Trial 27 finished with value: 0.05297102827770526 and parameters: {'observation_period_num': 28, 'train_rates': 0.815815122135437, 'learning_rate': 8.245861862074895e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8299656485110822}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:32:28,643][0m Trial 28 finished with value: 0.14303545617027047 and parameters: {'observation_period_num': 151, 'train_rates': 0.9428474788539442, 'learning_rate': 0.00020112404480448012, 'batch_size': 152, 'step_size': 11, 'gamma': 0.7567208142401158}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:33:35,946][0m Trial 29 finished with value: 0.17496123426316076 and parameters: {'observation_period_num': 127, 'train_rates': 0.9602520404251345, 'learning_rate': 0.0005532381925806845, 'batch_size': 91, 'step_size': 12, 'gamma': 0.7669803268488646}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:35:06,083][0m Trial 30 finished with value: 0.16492206802323073 and parameters: {'observation_period_num': 201, 'train_rates': 0.8479184378587673, 'learning_rate': 0.00047339811527242944, 'batch_size': 58, 'step_size': 8, 'gamma': 0.7754219410903116}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:36:34,875][0m Trial 31 finished with value: 0.05276576599673084 and parameters: {'observation_period_num': 27, 'train_rates': 0.8133304844513887, 'learning_rate': 7.347450738140385e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.830121265110297}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:38:47,963][0m Trial 32 finished with value: 0.15906747813851826 and parameters: {'observation_period_num': 24, 'train_rates': 0.7364180183647314, 'learning_rate': 6.626940625117605e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.8490246248712205}. Best is trial 26 with value: 0.04953435858534205.[0m
[32m[I 2025-02-06 02:40:14,348][0m Trial 33 finished with value: 0.03387989705571762 and parameters: {'observation_period_num': 6, 'train_rates': 0.8164474942734045, 'learning_rate': 0.0001638295350312845, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8919030603358202}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:41:19,476][0m Trial 34 finished with value: 0.038549576517270534 and parameters: {'observation_period_num': 8, 'train_rates': 0.9178054415613954, 'learning_rate': 0.00014963104165077905, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9103617161789621}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:42:07,222][0m Trial 35 finished with value: 0.043128166410882594 and parameters: {'observation_period_num': 5, 'train_rates': 0.9209593567790577, 'learning_rate': 0.00016998365543747886, 'batch_size': 127, 'step_size': 6, 'gamma': 0.9121567264931507}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:42:49,075][0m Trial 36 finished with value: 0.1702295273542404 and parameters: {'observation_period_num': 5, 'train_rates': 0.7730921177749669, 'learning_rate': 0.00019826036539714425, 'batch_size': 129, 'step_size': 6, 'gamma': 0.9110782456372626}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:43:27,372][0m Trial 37 finished with value: 0.04517851398115693 and parameters: {'observation_period_num': 15, 'train_rates': 0.8256566117549141, 'learning_rate': 0.00013864419826386807, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9430163824985861}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:43:58,907][0m Trial 38 finished with value: 0.17669626042423048 and parameters: {'observation_period_num': 15, 'train_rates': 0.7434407294593263, 'learning_rate': 0.00014427429699020978, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9497954394545473}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:44:37,292][0m Trial 39 finished with value: 0.03642353985239478 and parameters: {'observation_period_num': 7, 'train_rates': 0.8229187404764132, 'learning_rate': 0.00039997317343771913, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9135080964868533}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:45:06,157][0m Trial 40 finished with value: 0.1515407500753811 and parameters: {'observation_period_num': 19, 'train_rates': 0.6922853414079976, 'learning_rate': 0.0003651980401423961, 'batch_size': 183, 'step_size': 7, 'gamma': 0.912371990615069}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:45:44,981][0m Trial 41 finished with value: 0.03422464720706523 and parameters: {'observation_period_num': 6, 'train_rates': 0.8231563123944199, 'learning_rate': 0.0003718526445505922, 'batch_size': 145, 'step_size': 5, 'gamma': 0.9326080221048658}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:46:26,577][0m Trial 42 finished with value: 0.06330948282704979 and parameters: {'observation_period_num': 5, 'train_rates': 0.7949761963423643, 'learning_rate': 0.0006246383762650918, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8904690194749622}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:47:07,856][0m Trial 43 finished with value: 0.0352719069790032 and parameters: {'observation_period_num': 7, 'train_rates': 0.8017091385396041, 'learning_rate': 0.00038923098928375914, 'batch_size': 139, 'step_size': 4, 'gamma': 0.9297340914036066}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:47:41,116][0m Trial 44 finished with value: 0.17618501730836356 and parameters: {'observation_period_num': 23, 'train_rates': 0.7627367927700166, 'learning_rate': 0.00038809215166474607, 'batch_size': 171, 'step_size': 4, 'gamma': 0.9279978696102499}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:48:29,040][0m Trial 45 finished with value: 0.036608664859785145 and parameters: {'observation_period_num': 17, 'train_rates': 0.7931321750356009, 'learning_rate': 0.0002857348760898136, 'batch_size': 117, 'step_size': 5, 'gamma': 0.9661343334803685}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:48:57,490][0m Trial 46 finished with value: 0.20637245818278954 and parameters: {'observation_period_num': 45, 'train_rates': 0.7876979080909147, 'learning_rate': 0.00034797179871660813, 'batch_size': 198, 'step_size': 4, 'gamma': 0.9723478916211741}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:49:37,990][0m Trial 47 finished with value: 0.08022906135552185 and parameters: {'observation_period_num': 64, 'train_rates': 0.834191715233503, 'learning_rate': 0.0006441570125109209, 'batch_size': 141, 'step_size': 2, 'gamma': 0.9682424257035932}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:50:25,545][0m Trial 48 finished with value: 0.03934420715451865 and parameters: {'observation_period_num': 18, 'train_rates': 0.8066380259381613, 'learning_rate': 0.00024706262619396375, 'batch_size': 118, 'step_size': 5, 'gamma': 0.9303746267107926}. Best is trial 33 with value: 0.03387989705571762.[0m
[32m[I 2025-02-06 02:50:56,735][0m Trial 49 finished with value: 0.24157652877411753 and parameters: {'observation_period_num': 93, 'train_rates': 0.7613053079429383, 'learning_rate': 0.000698149426613281, 'batch_size': 169, 'step_size': 7, 'gamma': 0.9416768968637395}. Best is trial 33 with value: 0.03387989705571762.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2397 | 0.1575
Epoch 2/300, Loss: 0.1359 | 0.1086
Epoch 3/300, Loss: 0.1265 | 0.0942
Epoch 4/300, Loss: 0.1193 | 0.0886
Epoch 5/300, Loss: 0.1204 | 0.0885
Epoch 6/300, Loss: 0.1222 | 0.0950
Epoch 7/300, Loss: 0.1263 | 0.0767
Epoch 8/300, Loss: 0.1328 | 0.1227
Epoch 9/300, Loss: 0.1305 | 0.1250
Epoch 10/300, Loss: 0.1108 | 0.0858
Epoch 11/300, Loss: 0.1050 | 0.0681
Epoch 12/300, Loss: 0.1197 | 0.0766
Epoch 13/300, Loss: 0.1105 | 0.0646
Epoch 14/300, Loss: 0.1035 | 0.0662
Epoch 15/300, Loss: 0.1023 | 0.0677
Epoch 16/300, Loss: 0.1003 | 0.0652
Epoch 17/300, Loss: 0.0973 | 0.0620
Epoch 18/300, Loss: 0.0945 | 0.0598
Epoch 19/300, Loss: 0.0926 | 0.0576
Epoch 20/300, Loss: 0.0911 | 0.0559
Epoch 21/300, Loss: 0.0900 | 0.0553
Epoch 22/300, Loss: 0.0894 | 0.0559
Epoch 23/300, Loss: 0.0889 | 0.0550
Epoch 24/300, Loss: 0.0881 | 0.0532
Epoch 25/300, Loss: 0.0870 | 0.0515
Epoch 26/300, Loss: 0.0861 | 0.0501
Epoch 27/300, Loss: 0.0852 | 0.0490
Epoch 28/300, Loss: 0.0845 | 0.0482
Epoch 29/300, Loss: 0.0839 | 0.0475
Epoch 30/300, Loss: 0.0834 | 0.0468
Epoch 31/300, Loss: 0.0829 | 0.0462
Epoch 32/300, Loss: 0.0825 | 0.0456
Epoch 33/300, Loss: 0.0820 | 0.0451
Epoch 34/300, Loss: 0.0816 | 0.0447
Epoch 35/300, Loss: 0.0813 | 0.0442
Epoch 36/300, Loss: 0.0809 | 0.0438
Epoch 37/300, Loss: 0.0806 | 0.0436
Epoch 38/300, Loss: 0.0804 | 0.0433
Epoch 39/300, Loss: 0.0801 | 0.0430
Epoch 40/300, Loss: 0.0799 | 0.0429
Epoch 41/300, Loss: 0.0798 | 0.0427
Epoch 42/300, Loss: 0.0795 | 0.0425
Epoch 43/300, Loss: 0.0793 | 0.0425
Epoch 44/300, Loss: 0.0790 | 0.0422
Epoch 45/300, Loss: 0.0786 | 0.0419
Epoch 46/300, Loss: 0.0783 | 0.0417
Epoch 47/300, Loss: 0.0783 | 0.0416
Epoch 48/300, Loss: 0.0782 | 0.0415
Epoch 49/300, Loss: 0.0782 | 0.0414
Epoch 50/300, Loss: 0.0780 | 0.0411
Epoch 51/300, Loss: 0.0777 | 0.0410
Epoch 52/300, Loss: 0.0775 | 0.0408
Epoch 53/300, Loss: 0.0773 | 0.0406
Epoch 54/300, Loss: 0.0772 | 0.0405
Epoch 55/300, Loss: 0.0771 | 0.0403
Epoch 56/300, Loss: 0.0770 | 0.0402
Epoch 57/300, Loss: 0.0769 | 0.0401
Epoch 58/300, Loss: 0.0769 | 0.0401
Epoch 59/300, Loss: 0.0768 | 0.0400
Epoch 60/300, Loss: 0.0768 | 0.0399
Epoch 61/300, Loss: 0.0767 | 0.0400
Epoch 62/300, Loss: 0.0766 | 0.0400
Epoch 63/300, Loss: 0.0764 | 0.0398
Epoch 64/300, Loss: 0.0762 | 0.0398
Epoch 65/300, Loss: 0.0760 | 0.0396
Epoch 66/300, Loss: 0.0758 | 0.0395
Epoch 67/300, Loss: 0.0757 | 0.0394
Epoch 68/300, Loss: 0.0757 | 0.0392
Epoch 69/300, Loss: 0.0757 | 0.0392
Epoch 70/300, Loss: 0.0757 | 0.0391
Epoch 71/300, Loss: 0.0756 | 0.0390
Epoch 72/300, Loss: 0.0755 | 0.0389
Epoch 73/300, Loss: 0.0754 | 0.0389
Epoch 74/300, Loss: 0.0752 | 0.0389
Epoch 75/300, Loss: 0.0750 | 0.0388
Epoch 76/300, Loss: 0.0750 | 0.0388
Epoch 77/300, Loss: 0.0749 | 0.0387
Epoch 78/300, Loss: 0.0749 | 0.0387
Epoch 79/300, Loss: 0.0749 | 0.0386
Epoch 80/300, Loss: 0.0749 | 0.0386
Epoch 81/300, Loss: 0.0748 | 0.0385
Epoch 82/300, Loss: 0.0749 | 0.0385
Epoch 83/300, Loss: 0.0749 | 0.0384
Epoch 84/300, Loss: 0.0748 | 0.0384
Epoch 85/300, Loss: 0.0748 | 0.0383
Epoch 86/300, Loss: 0.0748 | 0.0383
Epoch 87/300, Loss: 0.0748 | 0.0383
Epoch 88/300, Loss: 0.0747 | 0.0383
Epoch 89/300, Loss: 0.0747 | 0.0383
Epoch 90/300, Loss: 0.0746 | 0.0383
Epoch 91/300, Loss: 0.0746 | 0.0384
Epoch 92/300, Loss: 0.0745 | 0.0383
Epoch 93/300, Loss: 0.0744 | 0.0382
Epoch 94/300, Loss: 0.0743 | 0.0382
Epoch 95/300, Loss: 0.0742 | 0.0381
Epoch 96/300, Loss: 0.0740 | 0.0380
Epoch 97/300, Loss: 0.0739 | 0.0379
Epoch 98/300, Loss: 0.0738 | 0.0378
Epoch 99/300, Loss: 0.0737 | 0.0378
Epoch 100/300, Loss: 0.0736 | 0.0377
Epoch 101/300, Loss: 0.0735 | 0.0377
Epoch 102/300, Loss: 0.0735 | 0.0376
Epoch 103/300, Loss: 0.0735 | 0.0376
Epoch 104/300, Loss: 0.0734 | 0.0376
Epoch 105/300, Loss: 0.0734 | 0.0375
Epoch 106/300, Loss: 0.0734 | 0.0375
Epoch 107/300, Loss: 0.0733 | 0.0375
Epoch 108/300, Loss: 0.0733 | 0.0374
Epoch 109/300, Loss: 0.0733 | 0.0374
Epoch 110/300, Loss: 0.0733 | 0.0374
Epoch 111/300, Loss: 0.0733 | 0.0374
Epoch 112/300, Loss: 0.0732 | 0.0374
Epoch 113/300, Loss: 0.0732 | 0.0373
Epoch 114/300, Loss: 0.0732 | 0.0373
Epoch 115/300, Loss: 0.0732 | 0.0373
Epoch 116/300, Loss: 0.0732 | 0.0373
Epoch 117/300, Loss: 0.0731 | 0.0373
Epoch 118/300, Loss: 0.0731 | 0.0373
Epoch 119/300, Loss: 0.0731 | 0.0372
Epoch 120/300, Loss: 0.0731 | 0.0372
Epoch 121/300, Loss: 0.0731 | 0.0372
Epoch 122/300, Loss: 0.0731 | 0.0372
Epoch 123/300, Loss: 0.0730 | 0.0372
Epoch 124/300, Loss: 0.0730 | 0.0372
Epoch 125/300, Loss: 0.0730 | 0.0372
Epoch 126/300, Loss: 0.0730 | 0.0371
Epoch 127/300, Loss: 0.0730 | 0.0371
Epoch 128/300, Loss: 0.0730 | 0.0371
Epoch 129/300, Loss: 0.0730 | 0.0371
Epoch 130/300, Loss: 0.0730 | 0.0371
Epoch 131/300, Loss: 0.0729 | 0.0371
Epoch 132/300, Loss: 0.0729 | 0.0371
Epoch 133/300, Loss: 0.0729 | 0.0371
Epoch 134/300, Loss: 0.0729 | 0.0371
Epoch 135/300, Loss: 0.0729 | 0.0371
Epoch 136/300, Loss: 0.0729 | 0.0371
Epoch 137/300, Loss: 0.0729 | 0.0370
Epoch 138/300, Loss: 0.0729 | 0.0370
Epoch 139/300, Loss: 0.0729 | 0.0370
Epoch 140/300, Loss: 0.0729 | 0.0370
Epoch 141/300, Loss: 0.0728 | 0.0370
Epoch 142/300, Loss: 0.0728 | 0.0370
Epoch 143/300, Loss: 0.0728 | 0.0370
Epoch 144/300, Loss: 0.0728 | 0.0370
Epoch 145/300, Loss: 0.0728 | 0.0370
Epoch 146/300, Loss: 0.0728 | 0.0370
Epoch 147/300, Loss: 0.0728 | 0.0370
Epoch 148/300, Loss: 0.0728 | 0.0370
Epoch 149/300, Loss: 0.0728 | 0.0370
Epoch 150/300, Loss: 0.0728 | 0.0370
Epoch 151/300, Loss: 0.0728 | 0.0370
Epoch 152/300, Loss: 0.0728 | 0.0370
Epoch 153/300, Loss: 0.0728 | 0.0370
Epoch 154/300, Loss: 0.0728 | 0.0369
Epoch 155/300, Loss: 0.0728 | 0.0369
Epoch 156/300, Loss: 0.0728 | 0.0369
Epoch 157/300, Loss: 0.0727 | 0.0369
Epoch 158/300, Loss: 0.0727 | 0.0369
Epoch 159/300, Loss: 0.0727 | 0.0369
Epoch 160/300, Loss: 0.0727 | 0.0369
Epoch 161/300, Loss: 0.0727 | 0.0369
Epoch 162/300, Loss: 0.0727 | 0.0369
Epoch 163/300, Loss: 0.0727 | 0.0369
Epoch 164/300, Loss: 0.0727 | 0.0369
Epoch 165/300, Loss: 0.0727 | 0.0369
Epoch 166/300, Loss: 0.0727 | 0.0369
Epoch 167/300, Loss: 0.0727 | 0.0369
Epoch 168/300, Loss: 0.0727 | 0.0369
Epoch 169/300, Loss: 0.0727 | 0.0369
Epoch 170/300, Loss: 0.0727 | 0.0369
Epoch 171/300, Loss: 0.0727 | 0.0369
Epoch 172/300, Loss: 0.0727 | 0.0369
Epoch 173/300, Loss: 0.0727 | 0.0369
Epoch 174/300, Loss: 0.0727 | 0.0369
Epoch 175/300, Loss: 0.0727 | 0.0369
Epoch 176/300, Loss: 0.0727 | 0.0369
Epoch 177/300, Loss: 0.0727 | 0.0369
Epoch 178/300, Loss: 0.0727 | 0.0369
Epoch 179/300, Loss: 0.0727 | 0.0369
Epoch 180/300, Loss: 0.0727 | 0.0369
Epoch 181/300, Loss: 0.0727 | 0.0369
Epoch 182/300, Loss: 0.0727 | 0.0369
Epoch 183/300, Loss: 0.0727 | 0.0369
Epoch 184/300, Loss: 0.0727 | 0.0369
Epoch 185/300, Loss: 0.0727 | 0.0369
Epoch 186/300, Loss: 0.0727 | 0.0369
Epoch 187/300, Loss: 0.0727 | 0.0369
Epoch 188/300, Loss: 0.0727 | 0.0369
Epoch 189/300, Loss: 0.0727 | 0.0369
Epoch 190/300, Loss: 0.0727 | 0.0369
Epoch 191/300, Loss: 0.0727 | 0.0369
Epoch 192/300, Loss: 0.0727 | 0.0369
Epoch 193/300, Loss: 0.0727 | 0.0369
Epoch 194/300, Loss: 0.0727 | 0.0369
Epoch 195/300, Loss: 0.0727 | 0.0369
Epoch 196/300, Loss: 0.0727 | 0.0369
Epoch 197/300, Loss: 0.0727 | 0.0369
Epoch 198/300, Loss: 0.0727 | 0.0369
Epoch 199/300, Loss: 0.0727 | 0.0369
Epoch 200/300, Loss: 0.0727 | 0.0369
Epoch 201/300, Loss: 0.0727 | 0.0369
Epoch 202/300, Loss: 0.0726 | 0.0369
Epoch 203/300, Loss: 0.0726 | 0.0369
Epoch 204/300, Loss: 0.0726 | 0.0369
Epoch 205/300, Loss: 0.0726 | 0.0368
Epoch 206/300, Loss: 0.0726 | 0.0368
Epoch 207/300, Loss: 0.0726 | 0.0368
Epoch 208/300, Loss: 0.0726 | 0.0368
Epoch 209/300, Loss: 0.0726 | 0.0368
Epoch 210/300, Loss: 0.0726 | 0.0368
Epoch 211/300, Loss: 0.0726 | 0.0368
Epoch 212/300, Loss: 0.0726 | 0.0368
Epoch 213/300, Loss: 0.0726 | 0.0368
Epoch 214/300, Loss: 0.0726 | 0.0368
Epoch 215/300, Loss: 0.0726 | 0.0368
Epoch 216/300, Loss: 0.0726 | 0.0368
Epoch 217/300, Loss: 0.0726 | 0.0368
Epoch 218/300, Loss: 0.0726 | 0.0368
Epoch 219/300, Loss: 0.0726 | 0.0368
Epoch 220/300, Loss: 0.0726 | 0.0368
Epoch 221/300, Loss: 0.0726 | 0.0368
Epoch 222/300, Loss: 0.0726 | 0.0368
Epoch 223/300, Loss: 0.0726 | 0.0368
Epoch 224/300, Loss: 0.0726 | 0.0368
Epoch 225/300, Loss: 0.0726 | 0.0368
Epoch 226/300, Loss: 0.0726 | 0.0368
Epoch 227/300, Loss: 0.0726 | 0.0368
Epoch 228/300, Loss: 0.0726 | 0.0368
Epoch 229/300, Loss: 0.0726 | 0.0368
Epoch 230/300, Loss: 0.0726 | 0.0368
Epoch 231/300, Loss: 0.0726 | 0.0368
Epoch 232/300, Loss: 0.0726 | 0.0368
Epoch 233/300, Loss: 0.0726 | 0.0368
Epoch 234/300, Loss: 0.0726 | 0.0368
Epoch 235/300, Loss: 0.0726 | 0.0368
Epoch 236/300, Loss: 0.0726 | 0.0368
Epoch 237/300, Loss: 0.0726 | 0.0368
Epoch 238/300, Loss: 0.0726 | 0.0368
Epoch 239/300, Loss: 0.0726 | 0.0368
Epoch 240/300, Loss: 0.0726 | 0.0368
Epoch 241/300, Loss: 0.0726 | 0.0368
Epoch 242/300, Loss: 0.0726 | 0.0368
Epoch 243/300, Loss: 0.0726 | 0.0368
Epoch 244/300, Loss: 0.0726 | 0.0368
Epoch 245/300, Loss: 0.0726 | 0.0368
Epoch 246/300, Loss: 0.0726 | 0.0368
Epoch 247/300, Loss: 0.0726 | 0.0368
Epoch 248/300, Loss: 0.0726 | 0.0368
Epoch 249/300, Loss: 0.0726 | 0.0368
Epoch 250/300, Loss: 0.0726 | 0.0368
Epoch 251/300, Loss: 0.0726 | 0.0368
Epoch 252/300, Loss: 0.0726 | 0.0368
Epoch 253/300, Loss: 0.0726 | 0.0368
Epoch 254/300, Loss: 0.0726 | 0.0368
Epoch 255/300, Loss: 0.0726 | 0.0368
Epoch 256/300, Loss: 0.0726 | 0.0368
Epoch 257/300, Loss: 0.0726 | 0.0368
Epoch 258/300, Loss: 0.0726 | 0.0368
Epoch 259/300, Loss: 0.0726 | 0.0368
Epoch 260/300, Loss: 0.0726 | 0.0368
Epoch 261/300, Loss: 0.0726 | 0.0368
Epoch 262/300, Loss: 0.0726 | 0.0368
Epoch 263/300, Loss: 0.0726 | 0.0368
Epoch 264/300, Loss: 0.0726 | 0.0368
Epoch 265/300, Loss: 0.0726 | 0.0368
Epoch 266/300, Loss: 0.0726 | 0.0368
Epoch 267/300, Loss: 0.0726 | 0.0368
Epoch 268/300, Loss: 0.0726 | 0.0368
Epoch 269/300, Loss: 0.0726 | 0.0368
Epoch 270/300, Loss: 0.0726 | 0.0368
Epoch 271/300, Loss: 0.0726 | 0.0368
Epoch 272/300, Loss: 0.0726 | 0.0368
Epoch 273/300, Loss: 0.0726 | 0.0368
Epoch 274/300, Loss: 0.0726 | 0.0368
Epoch 275/300, Loss: 0.0726 | 0.0368
Epoch 276/300, Loss: 0.0726 | 0.0368
Epoch 277/300, Loss: 0.0726 | 0.0368
Epoch 278/300, Loss: 0.0726 | 0.0368
Epoch 279/300, Loss: 0.0726 | 0.0368
Epoch 280/300, Loss: 0.0726 | 0.0368
Epoch 281/300, Loss: 0.0726 | 0.0368
Epoch 282/300, Loss: 0.0726 | 0.0368
Epoch 283/300, Loss: 0.0726 | 0.0368
Epoch 284/300, Loss: 0.0726 | 0.0368
Epoch 285/300, Loss: 0.0726 | 0.0368
Epoch 286/300, Loss: 0.0726 | 0.0368
Epoch 287/300, Loss: 0.0726 | 0.0368
Epoch 288/300, Loss: 0.0726 | 0.0368
Epoch 289/300, Loss: 0.0726 | 0.0368
Epoch 290/300, Loss: 0.0726 | 0.0368
Epoch 291/300, Loss: 0.0726 | 0.0368
Epoch 292/300, Loss: 0.0726 | 0.0368
Epoch 293/300, Loss: 0.0726 | 0.0368
Epoch 294/300, Loss: 0.0726 | 0.0368
Epoch 295/300, Loss: 0.0726 | 0.0368
Epoch 296/300, Loss: 0.0726 | 0.0368
Epoch 297/300, Loss: 0.0726 | 0.0368
Epoch 298/300, Loss: 0.0726 | 0.0368
Epoch 299/300, Loss: 0.0726 | 0.0368
Epoch 300/300, Loss: 0.0726 | 0.0368
Runtime (seconds): 250.68626284599304
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 112.73017866187729
RMSE: 10.617446899414062
MAE: 10.617446899414062
R-squared: nan
[186.09256]
