[32m[I 2025-02-03 18:56:47,155][0m A new study created in memory with name: no-name-2deba163-6510-48ac-b9f8-2cb494656e6b[0m
[32m[I 2025-02-03 19:01:46,044][0m Trial 0 finished with value: 0.1138195488601923 and parameters: {'observation_period_num': 201, 'train_rates': 0.9419928784629668, 'learning_rate': 5.5277296444777356e-05, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8681719292535512}. Best is trial 0 with value: 0.1138195488601923.[0m
[32m[I 2025-02-03 19:02:13,310][0m Trial 1 finished with value: 0.8236571550369263 and parameters: {'observation_period_num': 252, 'train_rates': 0.9793155203914488, 'learning_rate': 6.229378520809037e-06, 'batch_size': 234, 'step_size': 3, 'gamma': 0.7813169144728221}. Best is trial 0 with value: 0.1138195488601923.[0m
[32m[I 2025-02-03 19:03:22,354][0m Trial 2 finished with value: 0.30264228591724623 and parameters: {'observation_period_num': 83, 'train_rates': 0.6663269255726327, 'learning_rate': 2.7337038917552076e-05, 'batch_size': 67, 'step_size': 3, 'gamma': 0.7971607739538772}. Best is trial 0 with value: 0.1138195488601923.[0m
[32m[I 2025-02-03 19:03:43,253][0m Trial 3 finished with value: 0.09351163318139825 and parameters: {'observation_period_num': 45, 'train_rates': 0.6222100290101004, 'learning_rate': 0.0005879307508799713, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9793340520334927}. Best is trial 3 with value: 0.09351163318139825.[0m
[32m[I 2025-02-03 19:05:33,848][0m Trial 4 finished with value: 0.18083416490911322 and parameters: {'observation_period_num': 149, 'train_rates': 0.6182742982329117, 'learning_rate': 0.0001585467476069976, 'batch_size': 38, 'step_size': 8, 'gamma': 0.9598599635131527}. Best is trial 3 with value: 0.09351163318139825.[0m
Early stopping at epoch 92
[32m[I 2025-02-03 19:06:07,010][0m Trial 5 finished with value: 0.4698886275291443 and parameters: {'observation_period_num': 54, 'train_rates': 0.9425621464729664, 'learning_rate': 1.667073340737792e-05, 'batch_size': 181, 'step_size': 2, 'gamma': 0.7902498572507358}. Best is trial 3 with value: 0.09351163318139825.[0m
[32m[I 2025-02-03 19:06:33,976][0m Trial 6 finished with value: 0.1467456195001187 and parameters: {'observation_period_num': 201, 'train_rates': 0.7413439548733236, 'learning_rate': 0.000616107715512445, 'batch_size': 189, 'step_size': 3, 'gamma': 0.8716376887646339}. Best is trial 3 with value: 0.09351163318139825.[0m
[32m[I 2025-02-03 19:06:59,016][0m Trial 7 finished with value: 1.0265556046070943 and parameters: {'observation_period_num': 153, 'train_rates': 0.7194273078268435, 'learning_rate': 1.8271391690226795e-06, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8329731304299454}. Best is trial 3 with value: 0.09351163318139825.[0m
[32m[I 2025-02-03 19:07:54,201][0m Trial 8 finished with value: 0.09308585990220308 and parameters: {'observation_period_num': 42, 'train_rates': 0.7968201938759968, 'learning_rate': 1.6776989511365275e-05, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9795056680140104}. Best is trial 8 with value: 0.09308585990220308.[0m
[32m[I 2025-02-03 19:08:48,601][0m Trial 9 finished with value: 0.5634406789353019 and parameters: {'observation_period_num': 155, 'train_rates': 0.7895288736932622, 'learning_rate': 3.271877266764929e-06, 'batch_size': 93, 'step_size': 9, 'gamma': 0.8796194990991608}. Best is trial 8 with value: 0.09308585990220308.[0m
[32m[I 2025-02-03 19:09:39,503][0m Trial 10 finished with value: 0.04923762934109599 and parameters: {'observation_period_num': 18, 'train_rates': 0.867949960906705, 'learning_rate': 9.092483893697077e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.93557573372691}. Best is trial 10 with value: 0.04923762934109599.[0m
[32m[I 2025-02-03 19:10:29,306][0m Trial 11 finished with value: 0.04235197183017324 and parameters: {'observation_period_num': 10, 'train_rates': 0.860656198487588, 'learning_rate': 0.00010427360482184935, 'batch_size': 117, 'step_size': 15, 'gamma': 0.9355568262842746}. Best is trial 11 with value: 0.04235197183017324.[0m
[32m[I 2025-02-03 19:11:12,997][0m Trial 12 finished with value: 0.04345511074562173 and parameters: {'observation_period_num': 6, 'train_rates': 0.8545966951680997, 'learning_rate': 0.00014985640145900208, 'batch_size': 135, 'step_size': 15, 'gamma': 0.9251721368933407}. Best is trial 11 with value: 0.04235197183017324.[0m
[32m[I 2025-02-03 19:11:52,893][0m Trial 13 finished with value: 0.034828947797576164 and parameters: {'observation_period_num': 6, 'train_rates': 0.8723999347094247, 'learning_rate': 0.00021072827690207953, 'batch_size': 151, 'step_size': 15, 'gamma': 0.9294301181329061}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:12:29,851][0m Trial 14 finished with value: 0.13729201210662723 and parameters: {'observation_period_num': 97, 'train_rates': 0.8768518568204623, 'learning_rate': 0.00031016232545461915, 'batch_size': 165, 'step_size': 12, 'gamma': 0.9174333257100622}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:13:08,808][0m Trial 15 finished with value: 0.08141361853617156 and parameters: {'observation_period_num': 93, 'train_rates': 0.8247657265007327, 'learning_rate': 0.0003022726202396041, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9037665124181062}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:13:55,840][0m Trial 16 finished with value: 0.04777778884576213 and parameters: {'observation_period_num': 10, 'train_rates': 0.8949097556254367, 'learning_rate': 5.895679017477936e-05, 'batch_size': 126, 'step_size': 13, 'gamma': 0.9481883002142525}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:15:24,671][0m Trial 17 finished with value: 0.08896621725251598 and parameters: {'observation_period_num': 73, 'train_rates': 0.9139197495433171, 'learning_rate': 0.0008855265641114754, 'batch_size': 64, 'step_size': 10, 'gamma': 0.8323159110364217}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:15:58,387][0m Trial 18 finished with value: 0.10056001520493268 and parameters: {'observation_period_num': 115, 'train_rates': 0.7564418576568149, 'learning_rate': 0.00015752785043323438, 'batch_size': 158, 'step_size': 15, 'gamma': 0.7517325488560456}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:16:28,767][0m Trial 19 finished with value: 0.11163558479474515 and parameters: {'observation_period_num': 43, 'train_rates': 0.8316951317460385, 'learning_rate': 0.0002967537871399828, 'batch_size': 199, 'step_size': 11, 'gamma': 0.8950034153265598}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:17:48,697][0m Trial 20 finished with value: 0.05126188322901726 and parameters: {'observation_period_num': 26, 'train_rates': 0.982800671298766, 'learning_rate': 3.895432788551737e-05, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9568048693615963}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:18:32,399][0m Trial 21 finished with value: 0.03954939162763565 and parameters: {'observation_period_num': 6, 'train_rates': 0.8439219025663441, 'learning_rate': 0.00011960060145900967, 'batch_size': 131, 'step_size': 15, 'gamma': 0.9252975390716636}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:19:18,308][0m Trial 22 finished with value: 0.054204307280797295 and parameters: {'observation_period_num': 30, 'train_rates': 0.8229144115990098, 'learning_rate': 8.815366581909338e-05, 'batch_size': 122, 'step_size': 13, 'gamma': 0.9089519297897514}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:19:56,162][0m Trial 23 finished with value: 0.05851313420571387 and parameters: {'observation_period_num': 66, 'train_rates': 0.9170520332078334, 'learning_rate': 9.97133856412799e-05, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9383829519816794}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:20:46,113][0m Trial 24 finished with value: 0.040793768584115216 and parameters: {'observation_period_num': 6, 'train_rates': 0.7855922667051106, 'learning_rate': 0.00023718541420250517, 'batch_size': 107, 'step_size': 11, 'gamma': 0.8900124573263251}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:21:41,647][0m Trial 25 finished with value: 0.0555994448348366 and parameters: {'observation_period_num': 30, 'train_rates': 0.7815900661300637, 'learning_rate': 0.0002761982544169819, 'batch_size': 99, 'step_size': 11, 'gamma': 0.8476670108999917}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:22:18,298][0m Trial 26 finished with value: 0.10232617362682098 and parameters: {'observation_period_num': 58, 'train_rates': 0.6998353967190732, 'learning_rate': 0.0005906052967107585, 'batch_size': 139, 'step_size': 10, 'gamma': 0.8881350135018651}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:22:49,038][0m Trial 27 finished with value: 0.10475926803281674 and parameters: {'observation_period_num': 118, 'train_rates': 0.7615911586120002, 'learning_rate': 0.00020574982184925632, 'batch_size': 175, 'step_size': 6, 'gamma': 0.8571085067855544}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:23:19,197][0m Trial 28 finished with value: 0.06585755145975522 and parameters: {'observation_period_num': 29, 'train_rates': 0.8277117652403101, 'learning_rate': 0.00043080748265830613, 'batch_size': 201, 'step_size': 14, 'gamma': 0.9169575650790373}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:24:28,361][0m Trial 29 finished with value: 0.10657925417439805 and parameters: {'observation_period_num': 198, 'train_rates': 0.9479574640260795, 'learning_rate': 5.358869072318086e-05, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9674256443768294}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:26:57,460][0m Trial 30 finished with value: 0.042073089719023495 and parameters: {'observation_period_num': 7, 'train_rates': 0.8023212859134277, 'learning_rate': 0.000997611796107566, 'batch_size': 35, 'step_size': 12, 'gamma': 0.8958958482072382}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:30:28,170][0m Trial 31 finished with value: 0.04337637060567656 and parameters: {'observation_period_num': 9, 'train_rates': 0.8110007634242726, 'learning_rate': 0.0005030494411157717, 'batch_size': 25, 'step_size': 12, 'gamma': 0.8936413407940841}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:32:20,153][0m Trial 32 finished with value: 0.0418093060029716 and parameters: {'observation_period_num': 23, 'train_rates': 0.8882749334353067, 'learning_rate': 0.0007802026202146894, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8635733720031383}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:34:12,373][0m Trial 33 finished with value: 0.04745025812542137 and parameters: {'observation_period_num': 39, 'train_rates': 0.8916052711504913, 'learning_rate': 0.0002018043156293531, 'batch_size': 50, 'step_size': 7, 'gamma': 0.860109411193196}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:39:21,565][0m Trial 34 finished with value: 0.18576051052567993 and parameters: {'observation_period_num': 235, 'train_rates': 0.8506929788627106, 'learning_rate': 0.0004273639152281939, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8274800772925466}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:41:05,902][0m Trial 35 finished with value: 0.0540608895489894 and parameters: {'observation_period_num': 22, 'train_rates': 0.9145585590196594, 'learning_rate': 2.1717129946261158e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8749557639694961}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:42:04,495][0m Trial 36 finished with value: 0.17153485309570393 and parameters: {'observation_period_num': 49, 'train_rates': 0.9515195161017829, 'learning_rate': 6.760663052239866e-06, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9894553912839839}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:42:58,516][0m Trial 37 finished with value: 0.09615345078122894 and parameters: {'observation_period_num': 70, 'train_rates': 0.643953269551017, 'learning_rate': 0.0007243331085040143, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8202329840809504}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:43:39,676][0m Trial 38 finished with value: 0.17886039443005064 and parameters: {'observation_period_num': 168, 'train_rates': 0.8444016291418804, 'learning_rate': 6.016550342144382e-05, 'batch_size': 134, 'step_size': 5, 'gamma': 0.8479626371550549}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:44:17,742][0m Trial 39 finished with value: 0.12356917560100555 and parameters: {'observation_period_num': 86, 'train_rates': 0.8940551688212964, 'learning_rate': 0.00021093491787056061, 'batch_size': 152, 'step_size': 1, 'gamma': 0.9218399421520987}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:45:30,423][0m Trial 40 finished with value: 0.06533535661762707 and parameters: {'observation_period_num': 37, 'train_rates': 0.7745963187244552, 'learning_rate': 0.0001282956278267316, 'batch_size': 72, 'step_size': 13, 'gamma': 0.8833177698745744}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:48:00,638][0m Trial 41 finished with value: 0.04343814716835297 and parameters: {'observation_period_num': 6, 'train_rates': 0.8057596460760718, 'learning_rate': 0.0009308898336049128, 'batch_size': 35, 'step_size': 12, 'gamma': 0.899536763889462}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:49:48,152][0m Trial 42 finished with value: 0.08443812479575476 and parameters: {'observation_period_num': 23, 'train_rates': 0.7286999760913859, 'learning_rate': 0.00041986418567824247, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9062510320384796}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:52:37,773][0m Trial 43 finished with value: 0.036668978944546037 and parameters: {'observation_period_num': 17, 'train_rates': 0.8784480349079916, 'learning_rate': 0.0009502013623816985, 'batch_size': 33, 'step_size': 11, 'gamma': 0.8627461719148584}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:54:08,742][0m Trial 44 finished with value: 0.06316689248739576 and parameters: {'observation_period_num': 54, 'train_rates': 0.8876820065508964, 'learning_rate': 0.00039234791117488004, 'batch_size': 62, 'step_size': 8, 'gamma': 0.8615104342359919}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:57:50,340][0m Trial 45 finished with value: 0.045608670227754285 and parameters: {'observation_period_num': 18, 'train_rates': 0.8790957858203472, 'learning_rate': 0.0005683138446200373, 'batch_size': 25, 'step_size': 11, 'gamma': 0.8457342496315294}. Best is trial 13 with value: 0.034828947797576164.[0m
[32m[I 2025-02-03 19:58:26,261][0m Trial 46 finished with value: 0.032480221387802384 and parameters: {'observation_period_num': 19, 'train_rates': 0.930499741651007, 'learning_rate': 0.000734674908029234, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8705543521621941}. Best is trial 46 with value: 0.032480221387802384.[0m
[32m[I 2025-02-03 19:58:55,473][0m Trial 47 finished with value: 0.21560566127300262 and parameters: {'observation_period_num': 36, 'train_rates': 0.9411060554464763, 'learning_rate': 9.776310914829787e-06, 'batch_size': 222, 'step_size': 15, 'gamma': 0.816577614469592}. Best is trial 46 with value: 0.032480221387802384.[0m
[32m[I 2025-02-03 19:59:29,459][0m Trial 48 finished with value: 0.036788652892465946 and parameters: {'observation_period_num': 18, 'train_rates': 0.926621728063461, 'learning_rate': 0.0002351341140985444, 'batch_size': 184, 'step_size': 14, 'gamma': 0.9308574308277262}. Best is trial 46 with value: 0.032480221387802384.[0m
[32m[I 2025-02-03 19:59:56,834][0m Trial 49 finished with value: 0.05051247030496597 and parameters: {'observation_period_num': 17, 'train_rates': 0.9658732681708697, 'learning_rate': 0.00012500535833686257, 'batch_size': 251, 'step_size': 14, 'gamma': 0.9465329155041394}. Best is trial 46 with value: 0.032480221387802384.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6127 | 0.3694
Epoch 2/300, Loss: 0.2744 | 0.1788
Epoch 3/300, Loss: 0.2042 | 0.1511
Epoch 4/300, Loss: 0.1741 | 0.3009
Epoch 5/300, Loss: 0.1646 | 0.5231
Epoch 6/300, Loss: 0.1441 | 0.1788
Epoch 7/300, Loss: 0.1423 | 0.2817
Epoch 8/300, Loss: 0.1588 | 0.1339
Epoch 9/300, Loss: 0.1042 | 0.0991
Epoch 10/300, Loss: 0.0907 | 0.0966
Epoch 11/300, Loss: 0.0915 | 0.1027
Epoch 12/300, Loss: 0.0940 | 0.0859
Epoch 13/300, Loss: 0.0818 | 0.1943
Epoch 14/300, Loss: 0.0878 | 0.1650
Epoch 15/300, Loss: 0.0960 | 0.0865
Epoch 16/300, Loss: 0.0888 | 0.0687
Epoch 17/300, Loss: 0.0678 | 0.0939
Epoch 18/300, Loss: 0.0644 | 0.1140
Epoch 19/300, Loss: 0.0634 | 0.0637
Epoch 20/300, Loss: 0.0717 | 0.0732
Epoch 21/300, Loss: 0.0699 | 0.0794
Epoch 22/300, Loss: 0.0623 | 0.1092
Epoch 23/300, Loss: 0.0636 | 0.0646
Epoch 24/300, Loss: 0.0569 | 0.0544
Epoch 25/300, Loss: 0.0531 | 0.0576
Epoch 26/300, Loss: 0.0505 | 0.0741
Epoch 27/300, Loss: 0.0513 | 0.0753
Epoch 28/300, Loss: 0.0537 | 0.0537
Epoch 29/300, Loss: 0.0656 | 0.0592
Epoch 30/300, Loss: 0.0745 | 0.1031
Epoch 31/300, Loss: 0.0775 | 0.0819
Epoch 32/300, Loss: 0.0822 | 0.0661
Epoch 33/300, Loss: 0.0812 | 0.1020
Epoch 34/300, Loss: 0.0620 | 0.0843
Epoch 35/300, Loss: 0.0596 | 0.0597
Epoch 36/300, Loss: 0.0490 | 0.0601
Epoch 37/300, Loss: 0.0487 | 0.0479
Epoch 38/300, Loss: 0.0451 | 0.0606
Epoch 39/300, Loss: 0.0427 | 0.0476
Epoch 40/300, Loss: 0.0423 | 0.0452
Epoch 41/300, Loss: 0.0438 | 0.0520
Epoch 42/300, Loss: 0.0450 | 0.0554
Epoch 43/300, Loss: 0.0416 | 0.0429
Epoch 44/300, Loss: 0.0380 | 0.0422
Epoch 45/300, Loss: 0.0368 | 0.0433
Epoch 46/300, Loss: 0.0359 | 0.0437
Epoch 47/300, Loss: 0.0353 | 0.0422
Epoch 48/300, Loss: 0.0347 | 0.0402
Epoch 49/300, Loss: 0.0344 | 0.0406
Epoch 50/300, Loss: 0.0344 | 0.0449
Epoch 51/300, Loss: 0.0351 | 0.0411
Epoch 52/300, Loss: 0.0360 | 0.0390
Epoch 53/300, Loss: 0.0372 | 0.0392
Epoch 54/300, Loss: 0.0401 | 0.0451
Epoch 55/300, Loss: 0.0424 | 0.0420
Epoch 56/300, Loss: 0.0416 | 0.0456
Epoch 57/300, Loss: 0.0419 | 0.0431
Epoch 58/300, Loss: 0.0440 | 0.0706
Epoch 59/300, Loss: 0.0430 | 0.0426
Epoch 60/300, Loss: 0.0365 | 0.0551
Epoch 61/300, Loss: 0.0379 | 0.0401
Epoch 62/300, Loss: 0.0342 | 0.0398
Epoch 63/300, Loss: 0.0326 | 0.0445
Epoch 64/300, Loss: 0.0314 | 0.0372
Epoch 65/300, Loss: 0.0301 | 0.0369
Epoch 66/300, Loss: 0.0298 | 0.0395
Epoch 67/300, Loss: 0.0295 | 0.0368
Epoch 68/300, Loss: 0.0291 | 0.0362
Epoch 69/300, Loss: 0.0291 | 0.0397
Epoch 70/300, Loss: 0.0288 | 0.0383
Epoch 71/300, Loss: 0.0288 | 0.0350
Epoch 72/300, Loss: 0.0289 | 0.0399
Epoch 73/300, Loss: 0.0285 | 0.0386
Epoch 74/300, Loss: 0.0290 | 0.0342
Epoch 75/300, Loss: 0.0291 | 0.0399
Epoch 76/300, Loss: 0.0289 | 0.0420
Epoch 77/300, Loss: 0.0311 | 0.0354
Epoch 78/300, Loss: 0.0307 | 0.0451
Epoch 79/300, Loss: 0.0297 | 0.0374
Epoch 80/300, Loss: 0.0298 | 0.0339
Epoch 81/300, Loss: 0.0294 | 0.0445
Epoch 82/300, Loss: 0.0283 | 0.0345
Epoch 83/300, Loss: 0.0270 | 0.0333
Epoch 84/300, Loss: 0.0265 | 0.0343
Epoch 85/300, Loss: 0.0264 | 0.0339
Epoch 86/300, Loss: 0.0264 | 0.0341
Epoch 87/300, Loss: 0.0266 | 0.0343
Epoch 88/300, Loss: 0.0272 | 0.0351
Epoch 89/300, Loss: 0.0282 | 0.0364
Epoch 90/300, Loss: 0.0294 | 0.0369
Epoch 91/300, Loss: 0.0292 | 0.0356
Epoch 92/300, Loss: 0.0265 | 0.0345
Epoch 93/300, Loss: 0.0260 | 0.0335
Epoch 94/300, Loss: 0.0264 | 0.0338
Epoch 95/300, Loss: 0.0263 | 0.0335
Epoch 96/300, Loss: 0.0258 | 0.0334
Epoch 97/300, Loss: 0.0253 | 0.0334
Epoch 98/300, Loss: 0.0249 | 0.0332
Epoch 99/300, Loss: 0.0247 | 0.0331
Epoch 100/300, Loss: 0.0246 | 0.0330
Epoch 101/300, Loss: 0.0244 | 0.0329
Epoch 102/300, Loss: 0.0243 | 0.0329
Epoch 103/300, Loss: 0.0242 | 0.0328
Epoch 104/300, Loss: 0.0241 | 0.0327
Epoch 105/300, Loss: 0.0241 | 0.0327
Epoch 106/300, Loss: 0.0240 | 0.0326
Epoch 107/300, Loss: 0.0239 | 0.0326
Epoch 108/300, Loss: 0.0238 | 0.0326
Epoch 109/300, Loss: 0.0237 | 0.0325
Epoch 110/300, Loss: 0.0237 | 0.0325
Epoch 111/300, Loss: 0.0236 | 0.0325
Epoch 112/300, Loss: 0.0235 | 0.0324
Epoch 113/300, Loss: 0.0234 | 0.0324
Epoch 114/300, Loss: 0.0233 | 0.0323
Epoch 115/300, Loss: 0.0232 | 0.0323
Epoch 116/300, Loss: 0.0232 | 0.0322
Epoch 117/300, Loss: 0.0231 | 0.0322
Epoch 118/300, Loss: 0.0231 | 0.0322
Epoch 119/300, Loss: 0.0230 | 0.0322
Epoch 120/300, Loss: 0.0231 | 0.0322
Epoch 121/300, Loss: 0.0231 | 0.0323
Epoch 122/300, Loss: 0.0232 | 0.0323
Epoch 123/300, Loss: 0.0232 | 0.0323
Epoch 124/300, Loss: 0.0230 | 0.0322
Epoch 125/300, Loss: 0.0228 | 0.0320
Epoch 126/300, Loss: 0.0226 | 0.0319
Epoch 127/300, Loss: 0.0225 | 0.0319
Epoch 128/300, Loss: 0.0224 | 0.0319
Epoch 129/300, Loss: 0.0224 | 0.0319
Epoch 130/300, Loss: 0.0224 | 0.0320
Epoch 131/300, Loss: 0.0224 | 0.0320
Epoch 132/300, Loss: 0.0224 | 0.0320
Epoch 133/300, Loss: 0.0223 | 0.0320
Epoch 134/300, Loss: 0.0222 | 0.0320
Epoch 135/300, Loss: 0.0221 | 0.0319
Epoch 136/300, Loss: 0.0220 | 0.0319
Epoch 137/300, Loss: 0.0220 | 0.0318
Epoch 138/300, Loss: 0.0220 | 0.0318
Epoch 139/300, Loss: 0.0220 | 0.0318
Epoch 140/300, Loss: 0.0219 | 0.0318
Epoch 141/300, Loss: 0.0219 | 0.0317
Epoch 142/300, Loss: 0.0218 | 0.0317
Epoch 143/300, Loss: 0.0217 | 0.0316
Epoch 144/300, Loss: 0.0217 | 0.0317
Epoch 145/300, Loss: 0.0216 | 0.0317
Epoch 146/300, Loss: 0.0216 | 0.0317
Epoch 147/300, Loss: 0.0216 | 0.0317
Epoch 148/300, Loss: 0.0215 | 0.0317
Epoch 149/300, Loss: 0.0214 | 0.0317
Epoch 150/300, Loss: 0.0214 | 0.0317
Epoch 151/300, Loss: 0.0214 | 0.0316
Epoch 152/300, Loss: 0.0213 | 0.0317
Epoch 153/300, Loss: 0.0213 | 0.0315
Epoch 154/300, Loss: 0.0213 | 0.0317
Epoch 155/300, Loss: 0.0212 | 0.0315
Epoch 156/300, Loss: 0.0212 | 0.0317
Epoch 157/300, Loss: 0.0212 | 0.0315
Epoch 158/300, Loss: 0.0211 | 0.0318
Epoch 159/300, Loss: 0.0211 | 0.0314
Epoch 160/300, Loss: 0.0211 | 0.0320
Epoch 161/300, Loss: 0.0212 | 0.0312
Epoch 162/300, Loss: 0.0212 | 0.0320
Epoch 163/300, Loss: 0.0214 | 0.0314
Epoch 164/300, Loss: 0.0220 | 0.0324
Epoch 165/300, Loss: 0.0231 | 0.0327
Epoch 166/300, Loss: 0.0252 | 0.0321
Epoch 167/300, Loss: 0.0298 | 0.0468
Epoch 168/300, Loss: 0.0321 | 0.0342
Epoch 169/300, Loss: 0.0325 | 0.0720
Epoch 170/300, Loss: 0.0270 | 0.0345
Epoch 171/300, Loss: 0.0238 | 0.0424
Epoch 172/300, Loss: 0.0220 | 0.0316
Epoch 173/300, Loss: 0.0213 | 0.0347
Epoch 174/300, Loss: 0.0210 | 0.0318
Epoch 175/300, Loss: 0.0209 | 0.0332
Epoch 176/300, Loss: 0.0208 | 0.0319
Epoch 177/300, Loss: 0.0208 | 0.0326
Epoch 178/300, Loss: 0.0207 | 0.0320
Epoch 179/300, Loss: 0.0207 | 0.0323
Epoch 180/300, Loss: 0.0207 | 0.0320
Epoch 181/300, Loss: 0.0207 | 0.0321
Epoch 182/300, Loss: 0.0206 | 0.0320
Epoch 183/300, Loss: 0.0206 | 0.0320
Epoch 184/300, Loss: 0.0206 | 0.0320
Epoch 185/300, Loss: 0.0206 | 0.0319
Epoch 186/300, Loss: 0.0205 | 0.0319
Epoch 187/300, Loss: 0.0205 | 0.0319
Epoch 188/300, Loss: 0.0205 | 0.0319
Epoch 189/300, Loss: 0.0205 | 0.0319
Epoch 190/300, Loss: 0.0205 | 0.0319
Epoch 191/300, Loss: 0.0204 | 0.0318
Epoch 192/300, Loss: 0.0204 | 0.0318
Epoch 193/300, Loss: 0.0204 | 0.0318
Epoch 194/300, Loss: 0.0204 | 0.0318
Epoch 195/300, Loss: 0.0204 | 0.0318
Epoch 196/300, Loss: 0.0204 | 0.0318
Epoch 197/300, Loss: 0.0203 | 0.0318
Epoch 198/300, Loss: 0.0203 | 0.0318
Epoch 199/300, Loss: 0.0203 | 0.0318
Epoch 200/300, Loss: 0.0203 | 0.0318
Epoch 201/300, Loss: 0.0203 | 0.0318
Epoch 202/300, Loss: 0.0203 | 0.0318
Epoch 203/300, Loss: 0.0202 | 0.0317
Epoch 204/300, Loss: 0.0202 | 0.0317
Epoch 205/300, Loss: 0.0202 | 0.0317
Epoch 206/300, Loss: 0.0202 | 0.0317
Epoch 207/300, Loss: 0.0202 | 0.0317
Epoch 208/300, Loss: 0.0202 | 0.0317
Epoch 209/300, Loss: 0.0202 | 0.0317
Epoch 210/300, Loss: 0.0201 | 0.0317
Epoch 211/300, Loss: 0.0201 | 0.0317
Epoch 212/300, Loss: 0.0201 | 0.0317
Epoch 213/300, Loss: 0.0201 | 0.0317
Epoch 214/300, Loss: 0.0201 | 0.0317
Epoch 215/300, Loss: 0.0201 | 0.0317
Epoch 216/300, Loss: 0.0201 | 0.0317
Epoch 217/300, Loss: 0.0201 | 0.0317
Epoch 218/300, Loss: 0.0200 | 0.0317
Epoch 219/300, Loss: 0.0200 | 0.0317
Epoch 220/300, Loss: 0.0200 | 0.0317
Epoch 221/300, Loss: 0.0200 | 0.0317
Epoch 222/300, Loss: 0.0200 | 0.0317
Epoch 223/300, Loss: 0.0200 | 0.0317
Epoch 224/300, Loss: 0.0200 | 0.0317
Epoch 225/300, Loss: 0.0200 | 0.0317
Epoch 226/300, Loss: 0.0200 | 0.0317
Epoch 227/300, Loss: 0.0199 | 0.0317
Epoch 228/300, Loss: 0.0199 | 0.0317
Epoch 229/300, Loss: 0.0199 | 0.0317
Epoch 230/300, Loss: 0.0199 | 0.0317
Epoch 231/300, Loss: 0.0199 | 0.0317
Epoch 232/300, Loss: 0.0199 | 0.0317
Epoch 233/300, Loss: 0.0199 | 0.0317
Epoch 234/300, Loss: 0.0199 | 0.0317
Epoch 235/300, Loss: 0.0199 | 0.0316
Epoch 236/300, Loss: 0.0199 | 0.0316
Epoch 237/300, Loss: 0.0198 | 0.0316
Epoch 238/300, Loss: 0.0198 | 0.0316
Epoch 239/300, Loss: 0.0198 | 0.0316
Epoch 240/300, Loss: 0.0198 | 0.0316
Epoch 241/300, Loss: 0.0198 | 0.0316
Epoch 242/300, Loss: 0.0198 | 0.0316
Epoch 243/300, Loss: 0.0198 | 0.0316
Epoch 244/300, Loss: 0.0198 | 0.0316
Epoch 245/300, Loss: 0.0198 | 0.0316
Epoch 246/300, Loss: 0.0198 | 0.0316
Epoch 247/300, Loss: 0.0198 | 0.0316
Epoch 248/300, Loss: 0.0198 | 0.0316
Epoch 249/300, Loss: 0.0198 | 0.0316
Epoch 250/300, Loss: 0.0197 | 0.0316
Epoch 251/300, Loss: 0.0197 | 0.0316
Epoch 252/300, Loss: 0.0197 | 0.0316
Epoch 253/300, Loss: 0.0197 | 0.0316
Epoch 254/300, Loss: 0.0197 | 0.0316
Epoch 255/300, Loss: 0.0197 | 0.0316
Epoch 256/300, Loss: 0.0197 | 0.0316
Epoch 257/300, Loss: 0.0197 | 0.0316
Epoch 258/300, Loss: 0.0197 | 0.0316
Epoch 259/300, Loss: 0.0197 | 0.0316
Epoch 260/300, Loss: 0.0197 | 0.0316
Epoch 261/300, Loss: 0.0197 | 0.0316
Epoch 262/300, Loss: 0.0197 | 0.0316
Epoch 263/300, Loss: 0.0197 | 0.0316
Epoch 264/300, Loss: 0.0197 | 0.0316
Epoch 265/300, Loss: 0.0196 | 0.0316
Epoch 266/300, Loss: 0.0196 | 0.0316
Epoch 267/300, Loss: 0.0196 | 0.0316
Epoch 268/300, Loss: 0.0196 | 0.0316
Epoch 269/300, Loss: 0.0196 | 0.0316
Epoch 270/300, Loss: 0.0196 | 0.0316
Epoch 271/300, Loss: 0.0196 | 0.0316
Epoch 272/300, Loss: 0.0196 | 0.0316
Epoch 273/300, Loss: 0.0196 | 0.0316
Epoch 274/300, Loss: 0.0196 | 0.0316
Epoch 275/300, Loss: 0.0196 | 0.0316
Epoch 276/300, Loss: 0.0196 | 0.0316
Epoch 277/300, Loss: 0.0196 | 0.0316
Epoch 278/300, Loss: 0.0196 | 0.0316
Epoch 279/300, Loss: 0.0196 | 0.0316
Epoch 280/300, Loss: 0.0196 | 0.0316
Epoch 281/300, Loss: 0.0196 | 0.0316
Epoch 282/300, Loss: 0.0196 | 0.0316
Epoch 283/300, Loss: 0.0196 | 0.0316
Epoch 284/300, Loss: 0.0196 | 0.0316
Epoch 285/300, Loss: 0.0196 | 0.0316
Epoch 286/300, Loss: 0.0195 | 0.0316
Epoch 287/300, Loss: 0.0195 | 0.0316
Epoch 288/300, Loss: 0.0195 | 0.0316
Epoch 289/300, Loss: 0.0195 | 0.0316
Epoch 290/300, Loss: 0.0195 | 0.0316
Epoch 291/300, Loss: 0.0195 | 0.0316
Epoch 292/300, Loss: 0.0195 | 0.0316
Epoch 293/300, Loss: 0.0195 | 0.0316
Epoch 294/300, Loss: 0.0195 | 0.0316
Epoch 295/300, Loss: 0.0195 | 0.0316
Epoch 296/300, Loss: 0.0195 | 0.0316
Epoch 297/300, Loss: 0.0195 | 0.0316
Epoch 298/300, Loss: 0.0195 | 0.0316
Epoch 299/300, Loss: 0.0195 | 0.0316
Epoch 300/300, Loss: 0.0195 | 0.0316
Runtime (seconds): 107.2641716003418
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 13.426416298898403
RMSE: 3.6642074584960938
MAE: 3.6642074584960938
R-squared: nan
[116.30579]
