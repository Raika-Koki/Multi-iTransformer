[32m[I 2025-02-02 16:39:29,824][0m A new study created in memory with name: no-name-7cad68ba-56f6-40fc-9009-f031e5a9580f[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-02 16:40:04,834][0m Trial 0 finished with value: 0.12045327983601554 and parameters: {'observation_period_num': 221, 'train_rates': 0.8602503535373549, 'learning_rate': 0.00022643477419596525, 'batch_size': 168, 'step_size': 1, 'gamma': 0.9660438230947053}. Best is trial 0 with value: 0.12045327983601554.[0m
[32m[I 2025-02-02 16:40:36,298][0m Trial 1 finished with value: 0.89343781974253 and parameters: {'observation_period_num': 170, 'train_rates': 0.7673848276921877, 'learning_rate': 2.2824835023343974e-06, 'batch_size': 169, 'step_size': 12, 'gamma': 0.7821731987492444}. Best is trial 0 with value: 0.12045327983601554.[0m
[32m[I 2025-02-02 16:41:44,762][0m Trial 2 finished with value: 0.8619339789683054 and parameters: {'observation_period_num': 160, 'train_rates': 0.6951710135838648, 'learning_rate': 6.720923612739766e-06, 'batch_size': 71, 'step_size': 1, 'gamma': 0.8994618070196068}. Best is trial 0 with value: 0.12045327983601554.[0m
[32m[I 2025-02-02 16:42:15,648][0m Trial 3 finished with value: 0.06594416499137878 and parameters: {'observation_period_num': 44, 'train_rates': 0.9771718468837467, 'learning_rate': 0.00012264621651896185, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8608749821266408}. Best is trial 3 with value: 0.06594416499137878.[0m
[32m[I 2025-02-02 16:42:58,027][0m Trial 4 finished with value: 0.07607437235033401 and parameters: {'observation_period_num': 130, 'train_rates': 0.8863035435047725, 'learning_rate': 0.0007407104483927692, 'batch_size': 135, 'step_size': 3, 'gamma': 0.974139078155198}. Best is trial 3 with value: 0.06594416499137878.[0m
[32m[I 2025-02-02 16:43:56,403][0m Trial 5 finished with value: 0.3821639742078555 and parameters: {'observation_period_num': 94, 'train_rates': 0.6907459846980727, 'learning_rate': 3.464815911663722e-05, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8025475225022085}. Best is trial 3 with value: 0.06594416499137878.[0m
[32m[I 2025-02-02 16:45:20,878][0m Trial 6 finished with value: 0.05968019613197872 and parameters: {'observation_period_num': 73, 'train_rates': 0.9391784944144855, 'learning_rate': 0.0003823161132333952, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9549109370135292}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:47:48,018][0m Trial 7 finished with value: 0.2384448413821784 and parameters: {'observation_period_num': 232, 'train_rates': 0.8219895856094652, 'learning_rate': 3.93863440188247e-06, 'batch_size': 33, 'step_size': 10, 'gamma': 0.8180692512298618}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:48:12,475][0m Trial 8 finished with value: 0.331542264928322 and parameters: {'observation_period_num': 39, 'train_rates': 0.7226348205374861, 'learning_rate': 6.5590234394810565e-06, 'batch_size': 220, 'step_size': 8, 'gamma': 0.9769599786580655}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:48:58,979][0m Trial 9 finished with value: 0.13405075647471415 and parameters: {'observation_period_num': 198, 'train_rates': 0.7521676034273801, 'learning_rate': 0.00028464724596778966, 'batch_size': 106, 'step_size': 12, 'gamma': 0.82552750240859}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:52:10,488][0m Trial 10 finished with value: 0.21780263372410832 and parameters: {'observation_period_num': 83, 'train_rates': 0.6175303110062658, 'learning_rate': 3.81256086643972e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9206514952830017}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:52:37,874][0m Trial 11 finished with value: 0.07746582478284836 and parameters: {'observation_period_num': 14, 'train_rates': 0.986863102160811, 'learning_rate': 0.00013739767922038974, 'batch_size': 246, 'step_size': 7, 'gamma': 0.8719361706091813}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:53:11,302][0m Trial 12 finished with value: 0.07491402328014374 and parameters: {'observation_period_num': 64, 'train_rates': 0.9869347198159272, 'learning_rate': 0.000978668162630124, 'batch_size': 199, 'step_size': 6, 'gamma': 0.8568514608039346}. Best is trial 6 with value: 0.05968019613197872.[0m
[32m[I 2025-02-02 16:53:36,658][0m Trial 13 finished with value: 0.05825795233249664 and parameters: {'observation_period_num': 7, 'train_rates': 0.925288686930455, 'learning_rate': 9.091290897896409e-05, 'batch_size': 256, 'step_size': 10, 'gamma': 0.9306375068882016}. Best is trial 13 with value: 0.05825795233249664.[0m
[32m[I 2025-02-02 16:54:27,765][0m Trial 14 finished with value: 0.059037069169183574 and parameters: {'observation_period_num': 8, 'train_rates': 0.9103895710292478, 'learning_rate': 6.65304258977447e-05, 'batch_size': 121, 'step_size': 10, 'gamma': 0.9317738425389387}. Best is trial 13 with value: 0.05825795233249664.[0m
[32m[I 2025-02-02 16:55:14,954][0m Trial 15 finished with value: 0.07999921072149808 and parameters: {'observation_period_num': 8, 'train_rates': 0.9088101748383628, 'learning_rate': 1.7420573260766093e-05, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9293689845906489}. Best is trial 13 with value: 0.05825795233249664.[0m
[32m[I 2025-02-02 16:55:38,010][0m Trial 16 finished with value: 0.10006070480183447 and parameters: {'observation_period_num': 119, 'train_rates': 0.8298330621592814, 'learning_rate': 7.580839696095323e-05, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9349965220310014}. Best is trial 13 with value: 0.05825795233249664.[0m
[32m[I 2025-02-02 16:56:13,750][0m Trial 17 finished with value: 0.09759119362161871 and parameters: {'observation_period_num': 35, 'train_rates': 0.9221095154670875, 'learning_rate': 1.3872965703405226e-05, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8959444804782898}. Best is trial 13 with value: 0.05825795233249664.[0m
[32m[I 2025-02-02 16:57:06,411][0m Trial 18 finished with value: 0.05493283340880148 and parameters: {'observation_period_num': 9, 'train_rates': 0.8643735920087869, 'learning_rate': 5.963629300260609e-05, 'batch_size': 107, 'step_size': 13, 'gamma': 0.8988541867389539}. Best is trial 18 with value: 0.05493283340880148.[0m
[32m[I 2025-02-02 16:58:03,223][0m Trial 19 finished with value: 0.08870442635323628 and parameters: {'observation_period_num': 109, 'train_rates': 0.8541645324740655, 'learning_rate': 2.4840445358353944e-05, 'batch_size': 97, 'step_size': 13, 'gamma': 0.894451661709996}. Best is trial 18 with value: 0.05493283340880148.[0m
[32m[I 2025-02-02 16:58:37,411][0m Trial 20 finished with value: 0.07549981851998257 and parameters: {'observation_period_num': 52, 'train_rates': 0.7987742862295825, 'learning_rate': 5.766959333313396e-05, 'batch_size': 158, 'step_size': 14, 'gamma': 0.7511933635204328}. Best is trial 18 with value: 0.05493283340880148.[0m
[32m[I 2025-02-02 16:59:32,072][0m Trial 21 finished with value: 0.04707813829092994 and parameters: {'observation_period_num': 6, 'train_rates': 0.8883193027003995, 'learning_rate': 8.6665412606672e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9456645548835164}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:01:31,660][0m Trial 22 finished with value: 0.05787456232774225 and parameters: {'observation_period_num': 29, 'train_rates': 0.8773815926836693, 'learning_rate': 0.0001282078768942719, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9489569941280058}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:03:23,399][0m Trial 23 finished with value: 0.06318603342983842 and parameters: {'observation_period_num': 33, 'train_rates': 0.8700822752200117, 'learning_rate': 0.00017352201078156353, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9538474010359234}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:04:18,974][0m Trial 24 finished with value: 0.05271033236854955 and parameters: {'observation_period_num': 26, 'train_rates': 0.8377227904533817, 'learning_rate': 0.0004716351870579752, 'batch_size': 100, 'step_size': 13, 'gamma': 0.9886486251175451}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:05:12,772][0m Trial 25 finished with value: 0.08193824162446746 and parameters: {'observation_period_num': 60, 'train_rates': 0.8191159796915843, 'learning_rate': 0.0003827051391224741, 'batch_size': 102, 'step_size': 13, 'gamma': 0.9788849426260124}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:06:16,205][0m Trial 26 finished with value: 0.04864702466162204 and parameters: {'observation_period_num': 22, 'train_rates': 0.777241230368752, 'learning_rate': 0.0005629778122545542, 'batch_size': 82, 'step_size': 13, 'gamma': 0.9104171683969613}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:07:16,799][0m Trial 27 finished with value: 0.2143498699520236 and parameters: {'observation_period_num': 97, 'train_rates': 0.7750518615631983, 'learning_rate': 0.0005965936708606343, 'batch_size': 83, 'step_size': 11, 'gamma': 0.9866744823016397}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:07:50,538][0m Trial 28 finished with value: 0.22681379139423372 and parameters: {'observation_period_num': 144, 'train_rates': 0.7330530128552674, 'learning_rate': 0.00047934672417704083, 'batch_size': 147, 'step_size': 15, 'gamma': 0.9189739628436787}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:09:30,824][0m Trial 29 finished with value: 0.05043397405290905 and parameters: {'observation_period_num': 26, 'train_rates': 0.8380892250853725, 'learning_rate': 0.0002206710676914928, 'batch_size': 54, 'step_size': 14, 'gamma': 0.959866879636322}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:10:59,332][0m Trial 30 finished with value: 0.27214035015243915 and parameters: {'observation_period_num': 247, 'train_rates': 0.7953636684945772, 'learning_rate': 0.00022161925082897732, 'batch_size': 54, 'step_size': 14, 'gamma': 0.9557575264234287}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:12:01,253][0m Trial 31 finished with value: 0.05422255049785997 and parameters: {'observation_period_num': 25, 'train_rates': 0.8401707914461202, 'learning_rate': 0.000262548487406022, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9667266021730248}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:12:47,400][0m Trial 32 finished with value: 0.05127315059733048 and parameters: {'observation_period_num': 52, 'train_rates': 0.8083707213813051, 'learning_rate': 0.0009740350827329815, 'batch_size': 120, 'step_size': 12, 'gamma': 0.9886368509066079}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:13:32,389][0m Trial 33 finished with value: 0.06762828265529978 and parameters: {'observation_period_num': 45, 'train_rates': 0.7716508938445094, 'learning_rate': 0.0009398897285566743, 'batch_size': 119, 'step_size': 12, 'gamma': 0.942006431979284}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:14:56,041][0m Trial 34 finished with value: 0.06053859146063078 and parameters: {'observation_period_num': 73, 'train_rates': 0.8076422730131269, 'learning_rate': 0.0003045316524462481, 'batch_size': 62, 'step_size': 11, 'gamma': 0.911642961118496}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:15:46,560][0m Trial 35 finished with value: 0.06603034929587291 and parameters: {'observation_period_num': 50, 'train_rates': 0.9464106454139766, 'learning_rate': 0.0005932428240896292, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9645295538739552}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:18:33,686][0m Trial 36 finished with value: 0.13312247005686303 and parameters: {'observation_period_num': 23, 'train_rates': 0.8897863175003128, 'learning_rate': 1.1071125376579602e-06, 'batch_size': 33, 'step_size': 12, 'gamma': 0.8743101090969595}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:19:30,903][0m Trial 37 finished with value: 0.3598168070658117 and parameters: {'observation_period_num': 196, 'train_rates': 0.6664803354230866, 'learning_rate': 0.00019406431886055285, 'batch_size': 79, 'step_size': 14, 'gamma': 0.9679016124530765}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:20:43,754][0m Trial 38 finished with value: 0.08041250703205303 and parameters: {'observation_period_num': 66, 'train_rates': 0.7432580505016402, 'learning_rate': 0.0007816817045742181, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8842266771069731}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:21:21,138][0m Trial 39 finished with value: 0.05485546519780684 and parameters: {'observation_period_num': 19, 'train_rates': 0.7831469841500405, 'learning_rate': 0.0003974934407464332, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8490811412352531}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:21:49,503][0m Trial 40 finished with value: 0.07596522307821682 and parameters: {'observation_period_num': 50, 'train_rates': 0.7588773510455629, 'learning_rate': 0.00010240153070777064, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9119698261198708}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:22:53,253][0m Trial 41 finished with value: 0.05305266730598549 and parameters: {'observation_period_num': 26, 'train_rates': 0.8397638047392364, 'learning_rate': 0.0006205968561224911, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9827364997689946}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:23:44,682][0m Trial 42 finished with value: 0.058714901647348515 and parameters: {'observation_period_num': 38, 'train_rates': 0.8507741817389216, 'learning_rate': 0.00042121758526728965, 'batch_size': 112, 'step_size': 11, 'gamma': 0.9863716417600814}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:24:33,786][0m Trial 43 finished with value: 0.14310764426114025 and parameters: {'observation_period_num': 86, 'train_rates': 0.7079960922524227, 'learning_rate': 0.00016411434246281603, 'batch_size': 98, 'step_size': 13, 'gamma': 0.9482145910179276}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:25:46,381][0m Trial 44 finished with value: 0.05176317149072612 and parameters: {'observation_period_num': 22, 'train_rates': 0.8141644839893395, 'learning_rate': 0.0003065587614755122, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9699903585906434}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:27:03,815][0m Trial 45 finished with value: 0.05959217766250827 and parameters: {'observation_period_num': 17, 'train_rates': 0.8965170001308852, 'learning_rate': 0.0002927839572465914, 'batch_size': 75, 'step_size': 9, 'gamma': 0.972351400514349}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:32:23,356][0m Trial 46 finished with value: 0.06994780646951011 and parameters: {'observation_period_num': 40, 'train_rates': 0.8165653360553548, 'learning_rate': 4.0635304935441406e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9623449320585072}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:34:38,612][0m Trial 47 finished with value: 0.09436057010025745 and parameters: {'observation_period_num': 57, 'train_rates': 0.7959397518141101, 'learning_rate': 0.0009835770698559548, 'batch_size': 38, 'step_size': 11, 'gamma': 0.940629002420812}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:36:10,712][0m Trial 48 finished with value: 0.22731347927952758 and parameters: {'observation_period_num': 167, 'train_rates': 0.8186095247180458, 'learning_rate': 0.00023587122128622295, 'batch_size': 55, 'step_size': 14, 'gamma': 0.9582173641925527}. Best is trial 21 with value: 0.04707813829092994.[0m
[32m[I 2025-02-02 17:36:57,279][0m Trial 49 finished with value: 0.0411121345873625 and parameters: {'observation_period_num': 6, 'train_rates': 0.9448607813410415, 'learning_rate': 0.0007095409531714306, 'batch_size': 135, 'step_size': 12, 'gamma': 0.9212205237021571}. Best is trial 49 with value: 0.0411121345873625.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_PFE_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3815 | 0.1332
Epoch 2/300, Loss: 0.1506 | 0.1048
Epoch 3/300, Loss: 0.1351 | 0.1063
Epoch 4/300, Loss: 0.1284 | 0.0864
Epoch 5/300, Loss: 0.1195 | 0.0817
Epoch 6/300, Loss: 0.1258 | 0.0865
Epoch 7/300, Loss: 0.1242 | 0.0986
Epoch 8/300, Loss: 0.1190 | 0.1141
Epoch 9/300, Loss: 0.1536 | 0.1084
Epoch 10/300, Loss: 0.1287 | 0.0708
Epoch 11/300, Loss: 0.1657 | 0.1538
Epoch 12/300, Loss: 0.1186 | 0.0691
Epoch 13/300, Loss: 0.0988 | 0.0640
Epoch 14/300, Loss: 0.1285 | 0.1148
Epoch 15/300, Loss: 0.1124 | 0.0960
Epoch 16/300, Loss: 0.1179 | 0.0786
Epoch 17/300, Loss: 0.1163 | 0.0932
Epoch 18/300, Loss: 0.1070 | 0.0866
Epoch 19/300, Loss: 0.1158 | 0.0656
Epoch 20/300, Loss: 0.0928 | 0.0605
Epoch 21/300, Loss: 0.0956 | 0.0677
Epoch 22/300, Loss: 0.0981 | 0.0964
Epoch 23/300, Loss: 0.0923 | 0.0708
Epoch 24/300, Loss: 0.0860 | 0.0589
Epoch 25/300, Loss: 0.0823 | 0.0531
Epoch 26/300, Loss: 0.0793 | 0.0530
Epoch 27/300, Loss: 0.0803 | 0.0525
Epoch 28/300, Loss: 0.0820 | 0.0516
Epoch 29/300, Loss: 0.0796 | 0.0513
Epoch 30/300, Loss: 0.0762 | 0.0501
Epoch 31/300, Loss: 0.0740 | 0.0492
Epoch 32/300, Loss: 0.0741 | 0.0496
Epoch 33/300, Loss: 0.0740 | 0.0505
Epoch 34/300, Loss: 0.0728 | 0.0492
Epoch 35/300, Loss: 0.0712 | 0.0476
Epoch 36/300, Loss: 0.0710 | 0.0471
Epoch 37/300, Loss: 0.0703 | 0.0470
Epoch 38/300, Loss: 0.0704 | 0.0473
Epoch 39/300, Loss: 0.0708 | 0.0498
Epoch 40/300, Loss: 0.0721 | 0.0542
Epoch 41/300, Loss: 0.0750 | 0.0646
Epoch 42/300, Loss: 0.0778 | 0.0790
Epoch 43/300, Loss: 0.0778 | 0.0661
Epoch 44/300, Loss: 0.0820 | 0.0565
Epoch 45/300, Loss: 0.0874 | 0.0631
Epoch 46/300, Loss: 0.0803 | 0.0494
Epoch 47/300, Loss: 0.0726 | 0.0495
Epoch 48/300, Loss: 0.0722 | 0.0472
Epoch 49/300, Loss: 0.0728 | 0.0504
Epoch 50/300, Loss: 0.0706 | 0.0496
Epoch 51/300, Loss: 0.0695 | 0.0466
Epoch 52/300, Loss: 0.0690 | 0.0461
Epoch 53/300, Loss: 0.0679 | 0.0453
Epoch 54/300, Loss: 0.0677 | 0.0454
Epoch 55/300, Loss: 0.0676 | 0.0458
Epoch 56/300, Loss: 0.0693 | 0.0440
Epoch 57/300, Loss: 0.0670 | 0.0409
Epoch 58/300, Loss: 0.0682 | 0.0442
Epoch 59/300, Loss: 0.0685 | 0.0437
Epoch 60/300, Loss: 0.0690 | 0.0440
Epoch 61/300, Loss: 0.0665 | 0.0427
Epoch 62/300, Loss: 0.0657 | 0.0440
Epoch 63/300, Loss: 0.0657 | 0.0431
Epoch 64/300, Loss: 0.0645 | 0.0408
Epoch 65/300, Loss: 0.0634 | 0.0398
Epoch 66/300, Loss: 0.0631 | 0.0415
Epoch 67/300, Loss: 0.0656 | 0.0458
Epoch 68/300, Loss: 0.0649 | 0.0443
Epoch 69/300, Loss: 0.0652 | 0.0440
Epoch 70/300, Loss: 0.0634 | 0.0464
Epoch 71/300, Loss: 0.0629 | 0.0448
Epoch 72/300, Loss: 0.0615 | 0.0428
Epoch 73/300, Loss: 0.0613 | 0.0401
Epoch 74/300, Loss: 0.0618 | 0.0424
Epoch 75/300, Loss: 0.0653 | 0.0476
Epoch 76/300, Loss: 0.0650 | 0.0467
Epoch 77/300, Loss: 0.0633 | 0.0491
Epoch 78/300, Loss: 0.0629 | 0.0457
Epoch 79/300, Loss: 0.0625 | 0.0428
Epoch 80/300, Loss: 0.0623 | 0.0425
Epoch 81/300, Loss: 0.0615 | 0.0401
Epoch 82/300, Loss: 0.0608 | 0.0396
Epoch 83/300, Loss: 0.0602 | 0.0403
Epoch 84/300, Loss: 0.0596 | 0.0404
Epoch 85/300, Loss: 0.0599 | 0.0421
Epoch 86/300, Loss: 0.0608 | 0.0432
Epoch 87/300, Loss: 0.0620 | 0.0445
Epoch 88/300, Loss: 0.0616 | 0.0419
Epoch 89/300, Loss: 0.0598 | 0.0394
Epoch 90/300, Loss: 0.0607 | 0.0402
Epoch 91/300, Loss: 0.0594 | 0.0464
Epoch 92/300, Loss: 0.0581 | 0.0414
Epoch 93/300, Loss: 0.0579 | 0.0389
Epoch 94/300, Loss: 0.0579 | 0.0393
Epoch 95/300, Loss: 0.0575 | 0.0387
Epoch 96/300, Loss: 0.0574 | 0.0379
Epoch 97/300, Loss: 0.0577 | 0.0403
Epoch 98/300, Loss: 0.0583 | 0.0408
Epoch 99/300, Loss: 0.0594 | 0.0415
Epoch 100/300, Loss: 0.0599 | 0.0439
Epoch 101/300, Loss: 0.0620 | 0.0448
Epoch 102/300, Loss: 0.0616 | 0.0511
Epoch 103/300, Loss: 0.0614 | 0.0446
Epoch 104/300, Loss: 0.0621 | 0.0500
Epoch 105/300, Loss: 0.0648 | 0.0612
Epoch 106/300, Loss: 0.0661 | 0.0615
Epoch 107/300, Loss: 0.0635 | 0.0529
Epoch 108/300, Loss: 0.0612 | 0.0430
Epoch 109/300, Loss: 0.0677 | 0.0522
Epoch 110/300, Loss: 0.0675 | 0.0464
Epoch 111/300, Loss: 0.0756 | 0.0502
Epoch 112/300, Loss: 0.0802 | 0.0516
Epoch 113/300, Loss: 0.0678 | 0.0476
Epoch 114/300, Loss: 0.0658 | 0.0475
Epoch 115/300, Loss: 0.0640 | 0.0471
Epoch 116/300, Loss: 0.0639 | 0.0532
Epoch 117/300, Loss: 0.0645 | 0.0510
Epoch 118/300, Loss: 0.0617 | 0.0406
Epoch 119/300, Loss: 0.0568 | 0.0431
Epoch 120/300, Loss: 0.0574 | 0.0415
Epoch 121/300, Loss: 0.0562 | 0.0394
Epoch 122/300, Loss: 0.0553 | 0.0396
Epoch 123/300, Loss: 0.0544 | 0.0396
Epoch 124/300, Loss: 0.0547 | 0.0396
Epoch 125/300, Loss: 0.0545 | 0.0412
Epoch 126/300, Loss: 0.0546 | 0.0403
Epoch 127/300, Loss: 0.0544 | 0.0402
Epoch 128/300, Loss: 0.0537 | 0.0401
Epoch 129/300, Loss: 0.0528 | 0.0398
Epoch 130/300, Loss: 0.0527 | 0.0403
Epoch 131/300, Loss: 0.0527 | 0.0413
Epoch 132/300, Loss: 0.0531 | 0.0409
Epoch 133/300, Loss: 0.0526 | 0.0405
Epoch 134/300, Loss: 0.0517 | 0.0397
Epoch 135/300, Loss: 0.0540 | 0.0390
Epoch 136/300, Loss: 0.0538 | 0.0394
Epoch 137/300, Loss: 0.0537 | 0.0394
Epoch 138/300, Loss: 0.0535 | 0.0396
Epoch 139/300, Loss: 0.0516 | 0.0399
Epoch 140/300, Loss: 0.0507 | 0.0395
Epoch 141/300, Loss: 0.0535 | 0.0397
Epoch 142/300, Loss: 0.0526 | 0.0397
Epoch 143/300, Loss: 0.0502 | 0.0399
Epoch 144/300, Loss: 0.0498 | 0.0405
Epoch 145/300, Loss: 0.0510 | 0.0411
Epoch 146/300, Loss: 0.0495 | 0.0398
Epoch 147/300, Loss: 0.0525 | 0.0414
Epoch 148/300, Loss: 0.0510 | 0.0408
Epoch 149/300, Loss: 0.0505 | 0.0412
Epoch 150/300, Loss: 0.0511 | 0.0400
Epoch 151/300, Loss: 0.0490 | 0.0398
Epoch 152/300, Loss: 0.0513 | 0.0414
Epoch 153/300, Loss: 0.0505 | 0.0400
Epoch 154/300, Loss: 0.0488 | 0.0396
Epoch 155/300, Loss: 0.0484 | 0.0400
Epoch 156/300, Loss: 0.0482 | 0.0395
Epoch 157/300, Loss: 0.0491 | 0.0403
Epoch 158/300, Loss: 0.0493 | 0.0400
Epoch 159/300, Loss: 0.0475 | 0.0393
Epoch 160/300, Loss: 0.0469 | 0.0394
Epoch 161/300, Loss: 0.0467 | 0.0396
Epoch 162/300, Loss: 0.0465 | 0.0398
Epoch 163/300, Loss: 0.0465 | 0.0402
Epoch 164/300, Loss: 0.0465 | 0.0401
Epoch 165/300, Loss: 0.0464 | 0.0401
Epoch 166/300, Loss: 0.0462 | 0.0399
Epoch 167/300, Loss: 0.0460 | 0.0397
Epoch 168/300, Loss: 0.0460 | 0.0396
Epoch 169/300, Loss: 0.0459 | 0.0397
Epoch 170/300, Loss: 0.0460 | 0.0400
Epoch 171/300, Loss: 0.0460 | 0.0401
Epoch 172/300, Loss: 0.0459 | 0.0402
Epoch 173/300, Loss: 0.0457 | 0.0400
Epoch 174/300, Loss: 0.0455 | 0.0400
Epoch 175/300, Loss: 0.0454 | 0.0400
Epoch 176/300, Loss: 0.0453 | 0.0402
Epoch 177/300, Loss: 0.0454 | 0.0407
Epoch 178/300, Loss: 0.0457 | 0.0411
Epoch 179/300, Loss: 0.0456 | 0.0412
Epoch 180/300, Loss: 0.0455 | 0.0409
Epoch 181/300, Loss: 0.0451 | 0.0406
Epoch 182/300, Loss: 0.0450 | 0.0404
Epoch 183/300, Loss: 0.0451 | 0.0408
Epoch 184/300, Loss: 0.0452 | 0.0410
Epoch 185/300, Loss: 0.0452 | 0.0411
Epoch 186/300, Loss: 0.0450 | 0.0410
Epoch 187/300, Loss: 0.0446 | 0.0408
Epoch 188/300, Loss: 0.0444 | 0.0408
Epoch 189/300, Loss: 0.0444 | 0.0412
Epoch 190/300, Loss: 0.0445 | 0.0415
Epoch 191/300, Loss: 0.0444 | 0.0417
Epoch 192/300, Loss: 0.0443 | 0.0415
Epoch 193/300, Loss: 0.0441 | 0.0414
Epoch 194/300, Loss: 0.0441 | 0.0415
Epoch 195/300, Loss: 0.0440 | 0.0416
Epoch 196/300, Loss: 0.0440 | 0.0417
Epoch 197/300, Loss: 0.0438 | 0.0417
Epoch 198/300, Loss: 0.0437 | 0.0417
Epoch 199/300, Loss: 0.0435 | 0.0416
Epoch 200/300, Loss: 0.0434 | 0.0418
Epoch 201/300, Loss: 0.0435 | 0.0419
Epoch 202/300, Loss: 0.0439 | 0.0431
Epoch 203/300, Loss: 0.0469 | 0.0431
Epoch 204/300, Loss: 0.0452 | 0.0420
Epoch 205/300, Loss: 0.0441 | 0.0420
Epoch 206/300, Loss: 0.0437 | 0.0425
Epoch 207/300, Loss: 0.0435 | 0.0417
Epoch 208/300, Loss: 0.0433 | 0.0424
Epoch 209/300, Loss: 0.0432 | 0.0426
Epoch 210/300, Loss: 0.0433 | 0.0419
Epoch 211/300, Loss: 0.0436 | 0.0426
Epoch 212/300, Loss: 0.0433 | 0.0421
Epoch 213/300, Loss: 0.0430 | 0.0421
Epoch 214/300, Loss: 0.0429 | 0.0427
Epoch 215/300, Loss: 0.0429 | 0.0424
Epoch 216/300, Loss: 0.0428 | 0.0429
Epoch 217/300, Loss: 0.0428 | 0.0433
Epoch 218/300, Loss: 0.0427 | 0.0426
Epoch 219/300, Loss: 0.0426 | 0.0431
Epoch 220/300, Loss: 0.0425 | 0.0429
Epoch 221/300, Loss: 0.0424 | 0.0426
Epoch 222/300, Loss: 0.0423 | 0.0432
Epoch 223/300, Loss: 0.0426 | 0.0429
Epoch 224/300, Loss: 0.0431 | 0.0429
Epoch 225/300, Loss: 0.0428 | 0.0431
Epoch 226/300, Loss: 0.0426 | 0.0427
Epoch 227/300, Loss: 0.0425 | 0.0432
Epoch 228/300, Loss: 0.0424 | 0.0433
Epoch 229/300, Loss: 0.0424 | 0.0433
Epoch 230/300, Loss: 0.0423 | 0.0436
Epoch 231/300, Loss: 0.0422 | 0.0434
Epoch 232/300, Loss: 0.0421 | 0.0434
Epoch 233/300, Loss: 0.0420 | 0.0437
Epoch 234/300, Loss: 0.0419 | 0.0436
Epoch 235/300, Loss: 0.0418 | 0.0437
Epoch 236/300, Loss: 0.0417 | 0.0438
Epoch 237/300, Loss: 0.0416 | 0.0437
Epoch 238/300, Loss: 0.0420 | 0.0439
Epoch 239/300, Loss: 0.0442 | 0.0444
Epoch 240/300, Loss: 0.0441 | 0.0448
Epoch 241/300, Loss: 0.0436 | 0.0438
Epoch 242/300, Loss: 0.0428 | 0.0441
Epoch 243/300, Loss: 0.0418 | 0.0439
Epoch 244/300, Loss: 0.0417 | 0.0445
Epoch 245/300, Loss: 0.0421 | 0.0443
Epoch 246/300, Loss: 0.0414 | 0.0442
Epoch 247/300, Loss: 0.0412 | 0.0445
Epoch 248/300, Loss: 0.0411 | 0.0441
Epoch 249/300, Loss: 0.0414 | 0.0446
Epoch 250/300, Loss: 0.0421 | 0.0444
Epoch 251/300, Loss: 0.0426 | 0.0440
Epoch 252/300, Loss: 0.0423 | 0.0440
Epoch 253/300, Loss: 0.0421 | 0.0443
Epoch 254/300, Loss: 0.0420 | 0.0443
Epoch 255/300, Loss: 0.0418 | 0.0446
Epoch 256/300, Loss: 0.0416 | 0.0445
Epoch 257/300, Loss: 0.0413 | 0.0447
Epoch 258/300, Loss: 0.0410 | 0.0448
Epoch 259/300, Loss: 0.0409 | 0.0448
Epoch 260/300, Loss: 0.0408 | 0.0448
Epoch 261/300, Loss: 0.0407 | 0.0448
Epoch 262/300, Loss: 0.0406 | 0.0448
Epoch 263/300, Loss: 0.0405 | 0.0449
Epoch 264/300, Loss: 0.0405 | 0.0449
Epoch 265/300, Loss: 0.0404 | 0.0450
Epoch 266/300, Loss: 0.0404 | 0.0450
Epoch 267/300, Loss: 0.0404 | 0.0450
Epoch 268/300, Loss: 0.0405 | 0.0451
Epoch 269/300, Loss: 0.0406 | 0.0451
Epoch 270/300, Loss: 0.0414 | 0.0452
Epoch 271/300, Loss: 0.0406 | 0.0451
Epoch 272/300, Loss: 0.0403 | 0.0454
Epoch 273/300, Loss: 0.0402 | 0.0451
Epoch 274/300, Loss: 0.0402 | 0.0453
Epoch 275/300, Loss: 0.0402 | 0.0452
Epoch 276/300, Loss: 0.0404 | 0.0454
Epoch 277/300, Loss: 0.0401 | 0.0453
Epoch 278/300, Loss: 0.0401 | 0.0455
Epoch 279/300, Loss: 0.0400 | 0.0453
Epoch 280/300, Loss: 0.0402 | 0.0455
Epoch 281/300, Loss: 0.0400 | 0.0454
Epoch 282/300, Loss: 0.0400 | 0.0456
Epoch 283/300, Loss: 0.0399 | 0.0455
Epoch 284/300, Loss: 0.0401 | 0.0456
Epoch 285/300, Loss: 0.0398 | 0.0456
Epoch 286/300, Loss: 0.0398 | 0.0458
Epoch 287/300, Loss: 0.0398 | 0.0456
Epoch 288/300, Loss: 0.0400 | 0.0458
Epoch 289/300, Loss: 0.0397 | 0.0457
Epoch 290/300, Loss: 0.0397 | 0.0459
Epoch 291/300, Loss: 0.0397 | 0.0457
Epoch 292/300, Loss: 0.0398 | 0.0459
Epoch 293/300, Loss: 0.0396 | 0.0458
Epoch 294/300, Loss: 0.0397 | 0.0460
Epoch 295/300, Loss: 0.0396 | 0.0458
Epoch 296/300, Loss: 0.0396 | 0.0460
Epoch 297/300, Loss: 0.0395 | 0.0459
Epoch 298/300, Loss: 0.0395 | 0.0461
Epoch 299/300, Loss: 0.0395 | 0.0459
Epoch 300/300, Loss: 0.0395 | 0.0462
Runtime (seconds): 137.50853848457336
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.16866339546686504
RMSE: 0.4106864929199219
MAE: 0.4106864929199219
R-squared: nan
[24.82216]
