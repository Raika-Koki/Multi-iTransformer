[32m[I 2025-02-05 17:27:47,536][0m A new study created in memory with name: no-name-32d99af6-c174-41f9-9c37-c3a56e203a30[0m
[32m[I 2025-02-05 17:28:32,531][0m Trial 0 finished with value: 0.14702118528382743 and parameters: {'observation_period_num': 5, 'train_rates': 0.7396201915703007, 'learning_rate': 0.0002567876394540638, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9193949030864496}. Best is trial 0 with value: 0.14702118528382743.[0m
[32m[I 2025-02-05 17:29:24,797][0m Trial 1 finished with value: 0.28672196415630546 and parameters: {'observation_period_num': 232, 'train_rates': 0.6624588650569597, 'learning_rate': 7.806388488210394e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.7995583404462766}. Best is trial 0 with value: 0.14702118528382743.[0m
[32m[I 2025-02-05 17:30:42,341][0m Trial 2 finished with value: 0.0382686323179448 and parameters: {'observation_period_num': 16, 'train_rates': 0.8842626223788139, 'learning_rate': 0.00013619923058612736, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9521258193781177}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:31:06,186][0m Trial 3 finished with value: 0.6096173642228727 and parameters: {'observation_period_num': 141, 'train_rates': 0.7937638348651207, 'learning_rate': 3.316665926473164e-06, 'batch_size': 237, 'step_size': 9, 'gamma': 0.8016928685406365}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:31:39,594][0m Trial 4 finished with value: 0.14587929600509264 and parameters: {'observation_period_num': 87, 'train_rates': 0.8710217883747167, 'learning_rate': 0.00010671490766634757, 'batch_size': 170, 'step_size': 4, 'gamma': 0.8475266707479244}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:32:04,406][0m Trial 5 finished with value: 0.1880935291203183 and parameters: {'observation_period_num': 40, 'train_rates': 0.6694807664595992, 'learning_rate': 0.0005259830086852891, 'batch_size': 200, 'step_size': 2, 'gamma': 0.9820471358336604}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:32:51,127][0m Trial 6 finished with value: 0.471660756304083 and parameters: {'observation_period_num': 227, 'train_rates': 0.601801195407831, 'learning_rate': 1.9934732634231032e-05, 'batch_size': 89, 'step_size': 2, 'gamma': 0.9553847469368535}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:33:16,134][0m Trial 7 finished with value: 0.30908172620260754 and parameters: {'observation_period_num': 114, 'train_rates': 0.6637596891232341, 'learning_rate': 2.4652021982545816e-05, 'batch_size': 198, 'step_size': 12, 'gamma': 0.9693486010215848}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:33:40,103][0m Trial 8 finished with value: 0.16785665075791065 and parameters: {'observation_period_num': 157, 'train_rates': 0.7901011379619945, 'learning_rate': 0.00014529964099695348, 'batch_size': 236, 'step_size': 2, 'gamma': 0.9481695982969632}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:34:15,930][0m Trial 9 finished with value: 0.6656073411926627 and parameters: {'observation_period_num': 111, 'train_rates': 0.7204539851629796, 'learning_rate': 8.067350918434225e-06, 'batch_size': 145, 'step_size': 9, 'gamma': 0.8543023562945449}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:36:44,586][0m Trial 10 finished with value: 0.10010610024134318 and parameters: {'observation_period_num': 58, 'train_rates': 0.9796290061288526, 'learning_rate': 0.00099189066805335, 'batch_size': 40, 'step_size': 6, 'gamma': 0.9068866173084594}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:42:13,670][0m Trial 11 finished with value: 0.10664176475256681 and parameters: {'observation_period_num': 56, 'train_rates': 0.9864563929319355, 'learning_rate': 0.0007381227988281503, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9016155574981712}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:47:20,283][0m Trial 12 finished with value: 0.04112632225837503 and parameters: {'observation_period_num': 12, 'train_rates': 0.9548774862845966, 'learning_rate': 0.0006674193018060304, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9010973586750072}. Best is trial 2 with value: 0.0382686323179448.[0m
[32m[I 2025-02-05 17:49:00,601][0m Trial 13 finished with value: 0.028815664895722327 and parameters: {'observation_period_num': 11, 'train_rates': 0.8927734567904503, 'learning_rate': 0.00028285263226168484, 'batch_size': 57, 'step_size': 8, 'gamma': 0.8801381828390938}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:50:29,899][0m Trial 14 finished with value: 0.05481513403356075 and parameters: {'observation_period_num': 35, 'train_rates': 0.893876448313712, 'learning_rate': 5.12286243846916e-05, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8258580005012045}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:51:21,553][0m Trial 15 finished with value: 0.1291653932920143 and parameters: {'observation_period_num': 172, 'train_rates': 0.8803980771320071, 'learning_rate': 0.00024327858743432241, 'batch_size': 108, 'step_size': 10, 'gamma': 0.7511042160996297}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:53:09,799][0m Trial 16 finished with value: 0.07755388811702642 and parameters: {'observation_period_num': 80, 'train_rates': 0.9236176468795793, 'learning_rate': 0.00020855383730984455, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8778519305570611}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:54:19,423][0m Trial 17 finished with value: 0.6378775238990784 and parameters: {'observation_period_num': 197, 'train_rates': 0.83861927788513, 'learning_rate': 1.3290997252526276e-06, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9302009085311023}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:54:59,582][0m Trial 18 finished with value: 0.06324517657556156 and parameters: {'observation_period_num': 25, 'train_rates': 0.828708236203298, 'learning_rate': 4.9731734491270995e-05, 'batch_size': 138, 'step_size': 11, 'gamma': 0.8799868939771862}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:57:11,772][0m Trial 19 finished with value: 0.1035590740011155 and parameters: {'observation_period_num': 75, 'train_rates': 0.9290498783448176, 'learning_rate': 0.0003868819927834448, 'batch_size': 43, 'step_size': 1, 'gamma': 0.9422097146458828}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 17:58:02,557][0m Trial 20 finished with value: 0.14247954065172835 and parameters: {'observation_period_num': 102, 'train_rates': 0.8385986463774433, 'learning_rate': 1.5106730491836294e-05, 'batch_size': 106, 'step_size': 15, 'gamma': 0.9855631326267212}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:00:18,353][0m Trial 21 finished with value: 0.038544505681185164 and parameters: {'observation_period_num': 6, 'train_rates': 0.9428542991696574, 'learning_rate': 0.00040901108146946597, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9017061746849813}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:02:46,629][0m Trial 22 finished with value: 0.033214900457111014 and parameters: {'observation_period_num': 6, 'train_rates': 0.9154301524074285, 'learning_rate': 0.0003363944871222571, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8844943128749194}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:04:25,877][0m Trial 23 finished with value: 0.07246455120645007 and parameters: {'observation_period_num': 54, 'train_rates': 0.8995342545930105, 'learning_rate': 0.00015496939599582088, 'batch_size': 57, 'step_size': 4, 'gamma': 0.8586987235944349}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:07:33,371][0m Trial 24 finished with value: 0.041413738527967606 and parameters: {'observation_period_num': 23, 'train_rates': 0.8629573203396357, 'learning_rate': 8.463941229327844e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8284414082341881}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:08:56,688][0m Trial 25 finished with value: 0.06333663197987863 and parameters: {'observation_period_num': 43, 'train_rates': 0.910814031737112, 'learning_rate': 0.0003318350292143984, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8831135350435289}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:09:56,208][0m Trial 26 finished with value: 0.062532145632894 and parameters: {'observation_period_num': 25, 'train_rates': 0.8158934823904086, 'learning_rate': 4.583417571627167e-05, 'batch_size': 91, 'step_size': 5, 'gamma': 0.9264497067133974}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:10:49,768][0m Trial 27 finished with value: 0.09696352469921111 and parameters: {'observation_period_num': 66, 'train_rates': 0.957201996919198, 'learning_rate': 0.0001391532612322413, 'batch_size': 113, 'step_size': 8, 'gamma': 0.8326117205727078}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:13:30,165][0m Trial 28 finished with value: 0.07116970661621694 and parameters: {'observation_period_num': 38, 'train_rates': 0.8593447619143114, 'learning_rate': 6.69310057242788e-05, 'batch_size': 34, 'step_size': 3, 'gamma': 0.7843681388590389}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:14:11,893][0m Trial 29 finished with value: 0.15870500630628834 and parameters: {'observation_period_num': 6, 'train_rates': 0.754189843990169, 'learning_rate': 0.00024393334842401972, 'batch_size': 125, 'step_size': 5, 'gamma': 0.9181340772268505}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:14:44,267][0m Trial 30 finished with value: 0.22978246670502883 and parameters: {'observation_period_num': 94, 'train_rates': 0.7606767463361008, 'learning_rate': 0.00019270544102582132, 'batch_size': 159, 'step_size': 10, 'gamma': 0.8900769704033396}. Best is trial 13 with value: 0.028815664895722327.[0m
[32m[I 2025-02-05 18:16:44,883][0m Trial 31 finished with value: 0.02750247986342922 and parameters: {'observation_period_num': 12, 'train_rates': 0.9465985822649413, 'learning_rate': 0.00040571914931564375, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8716227364107967}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:18:28,519][0m Trial 32 finished with value: 0.03704172970416645 and parameters: {'observation_period_num': 18, 'train_rates': 0.9107455888520295, 'learning_rate': 0.00034768476013885454, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8609370223175158}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:20:13,993][0m Trial 33 finished with value: 0.04844966496003198 and parameters: {'observation_period_num': 25, 'train_rates': 0.9692303811209712, 'learning_rate': 0.000941785802071811, 'batch_size': 57, 'step_size': 7, 'gamma': 0.860685733236624}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:21:25,701][0m Trial 34 finished with value: 0.061914691844811805 and parameters: {'observation_period_num': 6, 'train_rates': 0.9300306497749699, 'learning_rate': 0.000531965811110131, 'batch_size': 84, 'step_size': 9, 'gamma': 0.874065837418143}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:23:19,652][0m Trial 35 finished with value: 0.07398736472599782 and parameters: {'observation_period_num': 44, 'train_rates': 0.9112180020899346, 'learning_rate': 0.0003210560891359962, 'batch_size': 51, 'step_size': 7, 'gamma': 0.8412904705591291}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:26:39,141][0m Trial 36 finished with value: 0.04270783687631289 and parameters: {'observation_period_num': 21, 'train_rates': 0.9411427092101322, 'learning_rate': 0.0005334772432742438, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8129199229834179}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:27:37,243][0m Trial 37 finished with value: 0.10855446070249826 and parameters: {'observation_period_num': 133, 'train_rates': 0.8851466814331407, 'learning_rate': 0.00010060929467691107, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8672080993560423}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:29:00,974][0m Trial 38 finished with value: 0.09273395131934772 and parameters: {'observation_period_num': 68, 'train_rates': 0.9623233417785058, 'learning_rate': 0.0003137712428791487, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8446246907438196}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:33:24,366][0m Trial 39 finished with value: 0.2895804755775731 and parameters: {'observation_period_num': 247, 'train_rates': 0.8585460201928856, 'learning_rate': 0.00042599715319450136, 'batch_size': 19, 'step_size': 10, 'gamma': 0.8880853548599001}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:35:30,446][0m Trial 40 finished with value: 0.13754569830845542 and parameters: {'observation_period_num': 33, 'train_rates': 0.9087849909820986, 'learning_rate': 8.680330335755955e-06, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8113245995756628}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:36:57,989][0m Trial 41 finished with value: 0.037776601757552174 and parameters: {'observation_period_num': 17, 'train_rates': 0.8748124548783418, 'learning_rate': 0.00013744827633115937, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9129692750650413}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:38:28,437][0m Trial 42 finished with value: 0.04026530484148001 and parameters: {'observation_period_num': 16, 'train_rates': 0.8805188835737082, 'learning_rate': 0.00011111551432639984, 'batch_size': 63, 'step_size': 3, 'gamma': 0.9181283776866732}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:39:32,693][0m Trial 43 finished with value: 0.05644678598320162 and parameters: {'observation_period_num': 49, 'train_rates': 0.8100935944998037, 'learning_rate': 0.0001829288902716003, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8673785849126731}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:42:34,618][0m Trial 44 finished with value: 0.08092904336312239 and parameters: {'observation_period_num': 36, 'train_rates': 0.942287499676847, 'learning_rate': 0.0006889228534202287, 'batch_size': 32, 'step_size': 6, 'gamma': 0.910078556212263}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:43:07,764][0m Trial 45 finished with value: 0.04724663740196148 and parameters: {'observation_period_num': 17, 'train_rates': 0.9195344449556965, 'learning_rate': 0.0002701450986967546, 'batch_size': 193, 'step_size': 5, 'gamma': 0.8960584330977521}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:44:08,548][0m Trial 46 finished with value: 0.12451300545698359 and parameters: {'observation_period_num': 14, 'train_rates': 0.6049955720379661, 'learning_rate': 0.0004386904638023597, 'batch_size': 74, 'step_size': 3, 'gamma': 0.850905820490623}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:44:35,749][0m Trial 47 finished with value: 0.06482034921646118 and parameters: {'observation_period_num': 29, 'train_rates': 0.979431218180558, 'learning_rate': 0.0006073346421554489, 'batch_size': 249, 'step_size': 9, 'gamma': 0.9623362789533423}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:45:27,198][0m Trial 48 finished with value: 0.1342554317606491 and parameters: {'observation_period_num': 5, 'train_rates': 0.7015967517474473, 'learning_rate': 0.0008098193957313904, 'batch_size': 98, 'step_size': 4, 'gamma': 0.8912331007870843}. Best is trial 31 with value: 0.02750247986342922.[0m
[32m[I 2025-02-05 18:46:52,532][0m Trial 49 finished with value: 0.15049994700941546 and parameters: {'observation_period_num': 205, 'train_rates': 0.8952042291148579, 'learning_rate': 2.8884401645283364e-05, 'batch_size': 62, 'step_size': 12, 'gamma': 0.8708834160311849}. Best is trial 31 with value: 0.02750247986342922.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.2613 | 0.1549
Epoch 2/300, Loss: 0.1269 | 0.1229
Epoch 3/300, Loss: 0.1123 | 0.1158
Epoch 4/300, Loss: 0.1040 | 0.0994
Epoch 5/300, Loss: 0.0988 | 0.0897
Epoch 6/300, Loss: 0.0966 | 0.0904
Epoch 7/300, Loss: 0.0971 | 0.0939
Epoch 8/300, Loss: 0.0951 | 0.0687
Epoch 9/300, Loss: 0.0887 | 0.0750
Epoch 10/300, Loss: 0.0858 | 0.0692
Epoch 11/300, Loss: 0.0824 | 0.0627
Epoch 12/300, Loss: 0.0776 | 0.0584
Epoch 13/300, Loss: 0.0746 | 0.0528
Epoch 14/300, Loss: 0.0740 | 0.0508
Epoch 15/300, Loss: 0.0723 | 0.0540
Epoch 16/300, Loss: 0.0710 | 0.0510
Epoch 17/300, Loss: 0.0714 | 0.0490
Epoch 18/300, Loss: 0.0721 | 0.0499
Epoch 19/300, Loss: 0.0708 | 0.0508
Epoch 20/300, Loss: 0.0714 | 0.0505
Epoch 21/300, Loss: 0.0725 | 0.0503
Epoch 22/300, Loss: 0.0724 | 0.0450
Epoch 23/300, Loss: 0.0708 | 0.0415
Epoch 24/300, Loss: 0.0696 | 0.0387
Epoch 25/300, Loss: 0.0683 | 0.0386
Epoch 26/300, Loss: 0.0670 | 0.0385
Epoch 27/300, Loss: 0.0667 | 0.0385
Epoch 28/300, Loss: 0.0659 | 0.0378
Epoch 29/300, Loss: 0.0653 | 0.0370
Epoch 30/300, Loss: 0.0651 | 0.0368
Epoch 31/300, Loss: 0.0641 | 0.0354
Epoch 32/300, Loss: 0.0633 | 0.0346
Epoch 33/300, Loss: 0.0626 | 0.0348
Epoch 34/300, Loss: 0.0621 | 0.0339
Epoch 35/300, Loss: 0.0616 | 0.0337
Epoch 36/300, Loss: 0.0608 | 0.0343
Epoch 37/300, Loss: 0.0603 | 0.0338
Epoch 38/300, Loss: 0.0598 | 0.0335
Epoch 39/300, Loss: 0.0592 | 0.0329
Epoch 40/300, Loss: 0.0588 | 0.0330
Epoch 41/300, Loss: 0.0583 | 0.0324
Epoch 42/300, Loss: 0.0580 | 0.0319
Epoch 43/300, Loss: 0.0578 | 0.0308
Epoch 44/300, Loss: 0.0580 | 0.0311
Epoch 45/300, Loss: 0.0583 | 0.0322
Epoch 46/300, Loss: 0.0584 | 0.0327
Epoch 47/300, Loss: 0.0578 | 0.0328
Epoch 48/300, Loss: 0.0580 | 0.0318
Epoch 49/300, Loss: 0.0590 | 0.0335
Epoch 50/300, Loss: 0.0581 | 0.0325
Epoch 51/300, Loss: 0.0563 | 0.0308
Epoch 52/300, Loss: 0.0554 | 0.0307
Epoch 53/300, Loss: 0.0549 | 0.0305
Epoch 54/300, Loss: 0.0546 | 0.0302
Epoch 55/300, Loss: 0.0543 | 0.0300
Epoch 56/300, Loss: 0.0541 | 0.0299
Epoch 57/300, Loss: 0.0540 | 0.0295
Epoch 58/300, Loss: 0.0540 | 0.0294
Epoch 59/300, Loss: 0.0538 | 0.0294
Epoch 60/300, Loss: 0.0536 | 0.0293
Epoch 61/300, Loss: 0.0535 | 0.0293
Epoch 62/300, Loss: 0.0534 | 0.0293
Epoch 63/300, Loss: 0.0531 | 0.0292
Epoch 64/300, Loss: 0.0530 | 0.0295
Epoch 65/300, Loss: 0.0529 | 0.0293
Epoch 66/300, Loss: 0.0527 | 0.0291
Epoch 67/300, Loss: 0.0527 | 0.0289
Epoch 68/300, Loss: 0.0527 | 0.0288
Epoch 69/300, Loss: 0.0527 | 0.0287
Epoch 70/300, Loss: 0.0526 | 0.0286
Epoch 71/300, Loss: 0.0528 | 0.0286
Epoch 72/300, Loss: 0.0528 | 0.0286
Epoch 73/300, Loss: 0.0528 | 0.0287
Epoch 74/300, Loss: 0.0527 | 0.0289
Epoch 75/300, Loss: 0.0527 | 0.0296
Epoch 76/300, Loss: 0.0526 | 0.0296
Epoch 77/300, Loss: 0.0526 | 0.0296
Epoch 78/300, Loss: 0.0528 | 0.0290
Epoch 79/300, Loss: 0.0530 | 0.0287
Epoch 80/300, Loss: 0.0534 | 0.0288
Epoch 81/300, Loss: 0.0538 | 0.0298
Epoch 82/300, Loss: 0.0548 | 0.0319
Epoch 83/300, Loss: 0.0550 | 0.0316
Epoch 84/300, Loss: 0.0541 | 0.0308
Epoch 85/300, Loss: 0.0542 | 0.0297
Epoch 86/300, Loss: 0.0547 | 0.0294
Epoch 87/300, Loss: 0.0544 | 0.0293
Epoch 88/300, Loss: 0.0538 | 0.0293
Epoch 89/300, Loss: 0.0536 | 0.0295
Epoch 90/300, Loss: 0.0535 | 0.0295
Epoch 91/300, Loss: 0.0529 | 0.0295
Epoch 92/300, Loss: 0.0525 | 0.0295
Epoch 93/300, Loss: 0.0520 | 0.0294
Epoch 94/300, Loss: 0.0516 | 0.0294
Epoch 95/300, Loss: 0.0515 | 0.0293
Epoch 96/300, Loss: 0.0516 | 0.0293
Epoch 97/300, Loss: 0.0516 | 0.0293
Epoch 98/300, Loss: 0.0515 | 0.0293
Epoch 99/300, Loss: 0.0514 | 0.0293
Epoch 100/300, Loss: 0.0512 | 0.0292
Epoch 101/300, Loss: 0.0510 | 0.0290
Epoch 102/300, Loss: 0.0508 | 0.0289
Epoch 103/300, Loss: 0.0507 | 0.0289
Epoch 104/300, Loss: 0.0506 | 0.0288
Epoch 105/300, Loss: 0.0506 | 0.0287
Epoch 106/300, Loss: 0.0506 | 0.0287
Epoch 107/300, Loss: 0.0505 | 0.0286
Epoch 108/300, Loss: 0.0505 | 0.0286
Epoch 109/300, Loss: 0.0504 | 0.0285
Epoch 110/300, Loss: 0.0504 | 0.0285
Epoch 111/300, Loss: 0.0503 | 0.0284
Epoch 112/300, Loss: 0.0503 | 0.0284
Epoch 113/300, Loss: 0.0502 | 0.0283
Epoch 114/300, Loss: 0.0502 | 0.0283
Epoch 115/300, Loss: 0.0501 | 0.0283
Epoch 116/300, Loss: 0.0501 | 0.0283
Epoch 117/300, Loss: 0.0501 | 0.0282
Epoch 118/300, Loss: 0.0500 | 0.0282
Epoch 119/300, Loss: 0.0500 | 0.0282
Epoch 120/300, Loss: 0.0500 | 0.0282
Epoch 121/300, Loss: 0.0499 | 0.0282
Epoch 122/300, Loss: 0.0499 | 0.0282
Epoch 123/300, Loss: 0.0499 | 0.0282
Epoch 124/300, Loss: 0.0499 | 0.0281
Epoch 125/300, Loss: 0.0499 | 0.0281
Epoch 126/300, Loss: 0.0498 | 0.0281
Epoch 127/300, Loss: 0.0498 | 0.0281
Epoch 128/300, Loss: 0.0498 | 0.0281
Epoch 129/300, Loss: 0.0498 | 0.0281
Epoch 130/300, Loss: 0.0498 | 0.0281
Epoch 131/300, Loss: 0.0498 | 0.0281
Epoch 132/300, Loss: 0.0497 | 0.0281
Epoch 133/300, Loss: 0.0497 | 0.0281
Epoch 134/300, Loss: 0.0497 | 0.0281
Epoch 135/300, Loss: 0.0497 | 0.0281
Epoch 136/300, Loss: 0.0497 | 0.0281
Epoch 137/300, Loss: 0.0497 | 0.0281
Epoch 138/300, Loss: 0.0497 | 0.0281
Epoch 139/300, Loss: 0.0497 | 0.0281
Epoch 140/300, Loss: 0.0496 | 0.0281
Epoch 141/300, Loss: 0.0496 | 0.0281
Epoch 142/300, Loss: 0.0496 | 0.0281
Epoch 143/300, Loss: 0.0496 | 0.0281
Epoch 144/300, Loss: 0.0496 | 0.0281
Epoch 145/300, Loss: 0.0496 | 0.0281
Epoch 146/300, Loss: 0.0496 | 0.0281
Epoch 147/300, Loss: 0.0496 | 0.0281
Epoch 148/300, Loss: 0.0496 | 0.0281
Epoch 149/300, Loss: 0.0496 | 0.0281
Epoch 150/300, Loss: 0.0496 | 0.0281
Epoch 151/300, Loss: 0.0495 | 0.0281
Epoch 152/300, Loss: 0.0495 | 0.0281
Epoch 153/300, Loss: 0.0495 | 0.0280
Epoch 154/300, Loss: 0.0495 | 0.0280
Epoch 155/300, Loss: 0.0495 | 0.0280
Epoch 156/300, Loss: 0.0495 | 0.0280
Epoch 157/300, Loss: 0.0495 | 0.0280
Epoch 158/300, Loss: 0.0495 | 0.0280
Epoch 159/300, Loss: 0.0495 | 0.0280
Epoch 160/300, Loss: 0.0495 | 0.0280
Epoch 161/300, Loss: 0.0495 | 0.0280
Epoch 162/300, Loss: 0.0495 | 0.0280
Epoch 163/300, Loss: 0.0495 | 0.0280
Epoch 164/300, Loss: 0.0495 | 0.0280
Epoch 165/300, Loss: 0.0495 | 0.0280
Epoch 166/300, Loss: 0.0495 | 0.0280
Epoch 167/300, Loss: 0.0495 | 0.0280
Epoch 168/300, Loss: 0.0495 | 0.0280
Epoch 169/300, Loss: 0.0495 | 0.0280
Epoch 170/300, Loss: 0.0495 | 0.0280
Epoch 171/300, Loss: 0.0494 | 0.0280
Epoch 172/300, Loss: 0.0494 | 0.0280
Epoch 173/300, Loss: 0.0494 | 0.0280
Epoch 174/300, Loss: 0.0494 | 0.0280
Epoch 175/300, Loss: 0.0494 | 0.0280
Epoch 176/300, Loss: 0.0494 | 0.0280
Epoch 177/300, Loss: 0.0494 | 0.0280
Epoch 178/300, Loss: 0.0494 | 0.0280
Epoch 179/300, Loss: 0.0494 | 0.0280
Epoch 180/300, Loss: 0.0494 | 0.0280
Epoch 181/300, Loss: 0.0494 | 0.0280
Epoch 182/300, Loss: 0.0494 | 0.0280
Epoch 183/300, Loss: 0.0494 | 0.0280
Epoch 184/300, Loss: 0.0494 | 0.0280
Epoch 185/300, Loss: 0.0494 | 0.0280
Epoch 186/300, Loss: 0.0494 | 0.0280
Epoch 187/300, Loss: 0.0494 | 0.0280
Epoch 188/300, Loss: 0.0494 | 0.0280
Epoch 189/300, Loss: 0.0494 | 0.0280
Epoch 190/300, Loss: 0.0494 | 0.0280
Epoch 191/300, Loss: 0.0494 | 0.0280
Epoch 192/300, Loss: 0.0494 | 0.0280
Epoch 193/300, Loss: 0.0494 | 0.0280
Epoch 194/300, Loss: 0.0494 | 0.0280
Epoch 195/300, Loss: 0.0494 | 0.0280
Epoch 196/300, Loss: 0.0494 | 0.0280
Epoch 197/300, Loss: 0.0494 | 0.0280
Epoch 198/300, Loss: 0.0494 | 0.0280
Epoch 199/300, Loss: 0.0494 | 0.0280
Epoch 200/300, Loss: 0.0494 | 0.0280
Epoch 201/300, Loss: 0.0494 | 0.0280
Epoch 202/300, Loss: 0.0494 | 0.0280
Epoch 203/300, Loss: 0.0494 | 0.0280
Epoch 204/300, Loss: 0.0494 | 0.0280
Epoch 205/300, Loss: 0.0494 | 0.0280
Epoch 206/300, Loss: 0.0494 | 0.0280
Epoch 207/300, Loss: 0.0494 | 0.0280
Epoch 208/300, Loss: 0.0494 | 0.0280
Epoch 209/300, Loss: 0.0494 | 0.0280
Epoch 210/300, Loss: 0.0494 | 0.0280
Epoch 211/300, Loss: 0.0494 | 0.0280
Epoch 212/300, Loss: 0.0494 | 0.0280
Epoch 213/300, Loss: 0.0494 | 0.0280
Epoch 214/300, Loss: 0.0494 | 0.0280
Epoch 215/300, Loss: 0.0494 | 0.0280
Epoch 216/300, Loss: 0.0494 | 0.0280
Epoch 217/300, Loss: 0.0494 | 0.0280
Epoch 218/300, Loss: 0.0494 | 0.0280
Epoch 219/300, Loss: 0.0494 | 0.0280
Epoch 220/300, Loss: 0.0494 | 0.0280
Epoch 221/300, Loss: 0.0494 | 0.0280
Epoch 222/300, Loss: 0.0494 | 0.0280
Epoch 223/300, Loss: 0.0494 | 0.0280
Epoch 224/300, Loss: 0.0494 | 0.0280
Epoch 225/300, Loss: 0.0494 | 0.0280
Epoch 226/300, Loss: 0.0494 | 0.0280
Epoch 227/300, Loss: 0.0494 | 0.0280
Epoch 228/300, Loss: 0.0494 | 0.0280
Epoch 229/300, Loss: 0.0494 | 0.0280
Epoch 230/300, Loss: 0.0494 | 0.0280
Epoch 231/300, Loss: 0.0494 | 0.0280
Epoch 232/300, Loss: 0.0494 | 0.0280
Epoch 233/300, Loss: 0.0494 | 0.0280
Epoch 234/300, Loss: 0.0494 | 0.0280
Epoch 235/300, Loss: 0.0494 | 0.0280
Epoch 236/300, Loss: 0.0494 | 0.0280
Epoch 237/300, Loss: 0.0494 | 0.0280
Epoch 238/300, Loss: 0.0494 | 0.0280
Epoch 239/300, Loss: 0.0494 | 0.0280
Epoch 240/300, Loss: 0.0494 | 0.0280
Epoch 241/300, Loss: 0.0494 | 0.0280
Epoch 242/300, Loss: 0.0494 | 0.0280
Epoch 243/300, Loss: 0.0494 | 0.0280
Epoch 244/300, Loss: 0.0494 | 0.0280
Epoch 245/300, Loss: 0.0494 | 0.0280
Epoch 246/300, Loss: 0.0494 | 0.0280
Epoch 247/300, Loss: 0.0494 | 0.0280
Epoch 248/300, Loss: 0.0494 | 0.0280
Epoch 249/300, Loss: 0.0494 | 0.0280
Epoch 250/300, Loss: 0.0494 | 0.0280
Epoch 251/300, Loss: 0.0494 | 0.0280
Epoch 252/300, Loss: 0.0494 | 0.0280
Epoch 253/300, Loss: 0.0494 | 0.0280
Epoch 254/300, Loss: 0.0494 | 0.0280
Epoch 255/300, Loss: 0.0494 | 0.0280
Epoch 256/300, Loss: 0.0494 | 0.0280
Epoch 257/300, Loss: 0.0494 | 0.0280
Epoch 258/300, Loss: 0.0494 | 0.0280
Epoch 259/300, Loss: 0.0494 | 0.0280
Epoch 260/300, Loss: 0.0494 | 0.0280
Epoch 261/300, Loss: 0.0494 | 0.0280
Epoch 262/300, Loss: 0.0494 | 0.0280
Epoch 263/300, Loss: 0.0494 | 0.0280
Epoch 264/300, Loss: 0.0494 | 0.0280
Epoch 265/300, Loss: 0.0494 | 0.0280
Epoch 266/300, Loss: 0.0494 | 0.0280
Epoch 267/300, Loss: 0.0494 | 0.0280
Epoch 268/300, Loss: 0.0494 | 0.0280
Epoch 269/300, Loss: 0.0494 | 0.0280
Epoch 270/300, Loss: 0.0494 | 0.0280
Epoch 271/300, Loss: 0.0494 | 0.0280
Epoch 272/300, Loss: 0.0494 | 0.0280
Epoch 273/300, Loss: 0.0494 | 0.0280
Epoch 274/300, Loss: 0.0494 | 0.0280
Epoch 275/300, Loss: 0.0494 | 0.0280
Epoch 276/300, Loss: 0.0494 | 0.0280
Epoch 277/300, Loss: 0.0494 | 0.0280
Epoch 278/300, Loss: 0.0494 | 0.0280
Epoch 279/300, Loss: 0.0494 | 0.0280
Epoch 280/300, Loss: 0.0494 | 0.0280
Epoch 281/300, Loss: 0.0494 | 0.0280
Epoch 282/300, Loss: 0.0494 | 0.0280
Epoch 283/300, Loss: 0.0494 | 0.0280
Epoch 284/300, Loss: 0.0494 | 0.0280
Epoch 285/300, Loss: 0.0494 | 0.0280
Epoch 286/300, Loss: 0.0494 | 0.0280
Epoch 287/300, Loss: 0.0494 | 0.0280
Epoch 288/300, Loss: 0.0494 | 0.0280
Epoch 289/300, Loss: 0.0494 | 0.0280
Epoch 290/300, Loss: 0.0494 | 0.0280
Epoch 291/300, Loss: 0.0494 | 0.0280
Epoch 292/300, Loss: 0.0494 | 0.0280
Epoch 293/300, Loss: 0.0494 | 0.0280
Epoch 294/300, Loss: 0.0494 | 0.0280
Epoch 295/300, Loss: 0.0494 | 0.0280
Epoch 296/300, Loss: 0.0494 | 0.0280
Epoch 297/300, Loss: 0.0494 | 0.0280
Epoch 298/300, Loss: 0.0494 | 0.0280
Epoch 299/300, Loss: 0.0494 | 0.0280
Epoch 300/300, Loss: 0.0494 | 0.0280
Runtime (seconds): 365.0359489917755
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 49.060260214842856
RMSE: 7.004302978515625
MAE: 7.004302978515625
R-squared: nan
[190.5657]
