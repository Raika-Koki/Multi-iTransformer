[32m[I 2025-02-07 20:32:45,008][0m A new study created in memory with name: no-name-e5114d87-d287-489b-bff7-b615779447e7[0m
[32m[I 2025-02-07 20:33:32,069][0m Trial 0 finished with value: 0.3322286605834961 and parameters: {'observation_period_num': 69, 'train_rates': 0.9781089817001116, 'learning_rate': 1.0235419057182395e-05, 'batch_size': 136, 'step_size': 6, 'gamma': 0.951985018882203}. Best is trial 0 with value: 0.3322286605834961.[0m
[32m[I 2025-02-07 20:35:38,816][0m Trial 1 finished with value: 0.43247342748608697 and parameters: {'observation_period_num': 236, 'train_rates': 0.6239814833343509, 'learning_rate': 0.0001876117245516258, 'batch_size': 33, 'step_size': 2, 'gamma': 0.8091464364408166}. Best is trial 0 with value: 0.3322286605834961.[0m
[32m[I 2025-02-07 20:36:39,651][0m Trial 2 finished with value: 0.18843662296049893 and parameters: {'observation_period_num': 110, 'train_rates': 0.783318064035402, 'learning_rate': 2.09317880422556e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8037120638307174}. Best is trial 2 with value: 0.18843662296049893.[0m
[32m[I 2025-02-07 20:37:14,669][0m Trial 3 finished with value: 0.16296813719932188 and parameters: {'observation_period_num': 136, 'train_rates': 0.7773940431548005, 'learning_rate': 0.00010793398039830491, 'batch_size': 153, 'step_size': 5, 'gamma': 0.8628729652802425}. Best is trial 3 with value: 0.16296813719932188.[0m
[32m[I 2025-02-07 20:37:36,483][0m Trial 4 finished with value: 0.3586548887008478 and parameters: {'observation_period_num': 189, 'train_rates': 0.6988861823060835, 'learning_rate': 0.00012386122791845992, 'batch_size': 245, 'step_size': 6, 'gamma': 0.8378173064180091}. Best is trial 3 with value: 0.16296813719932188.[0m
[32m[I 2025-02-07 20:38:46,451][0m Trial 5 finished with value: 0.41998171210289004 and parameters: {'observation_period_num': 251, 'train_rates': 0.9511350292779784, 'learning_rate': 1.1927113951573518e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8639030258711826}. Best is trial 3 with value: 0.16296813719932188.[0m
[32m[I 2025-02-07 20:39:25,113][0m Trial 6 finished with value: 0.11310491671400924 and parameters: {'observation_period_num': 132, 'train_rates': 0.8094494638924212, 'learning_rate': 0.00024392952291535825, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9892677333379202}. Best is trial 6 with value: 0.11310491671400924.[0m
[32m[I 2025-02-07 20:40:00,165][0m Trial 7 finished with value: 0.24418571591377258 and parameters: {'observation_period_num': 162, 'train_rates': 0.9662031461223763, 'learning_rate': 0.0002572284035475169, 'batch_size': 179, 'step_size': 7, 'gamma': 0.8691782032068288}. Best is trial 6 with value: 0.11310491671400924.[0m
[32m[I 2025-02-07 20:40:58,909][0m Trial 8 finished with value: 0.3006278815691014 and parameters: {'observation_period_num': 164, 'train_rates': 0.6928085124721062, 'learning_rate': 0.0005570620421745671, 'batch_size': 80, 'step_size': 10, 'gamma': 0.8472389681477103}. Best is trial 6 with value: 0.11310491671400924.[0m
[32m[I 2025-02-07 20:41:24,088][0m Trial 9 finished with value: 0.9514007251419707 and parameters: {'observation_period_num': 160, 'train_rates': 0.7062141013960109, 'learning_rate': 7.6046266558264685e-06, 'batch_size': 204, 'step_size': 2, 'gamma': 0.8663852421215682}. Best is trial 6 with value: 0.11310491671400924.[0m
[32m[I 2025-02-07 20:42:12,352][0m Trial 10 finished with value: 0.325748913760843 and parameters: {'observation_period_num': 19, 'train_rates': 0.8543454486298169, 'learning_rate': 1.4527609479523752e-06, 'batch_size': 123, 'step_size': 12, 'gamma': 0.9874104842666638}. Best is trial 6 with value: 0.11310491671400924.[0m
[32m[I 2025-02-07 20:42:45,582][0m Trial 11 finished with value: 0.11116319182009474 and parameters: {'observation_period_num': 102, 'train_rates': 0.8089095750343098, 'learning_rate': 0.0008665856906758979, 'batch_size': 174, 'step_size': 4, 'gamma': 0.7527360203560355}. Best is trial 11 with value: 0.11116319182009474.[0m
[32m[I 2025-02-07 20:43:16,464][0m Trial 12 finished with value: 0.09808463112746094 and parameters: {'observation_period_num': 70, 'train_rates': 0.8631736066656337, 'learning_rate': 0.0006984533271372388, 'batch_size': 207, 'step_size': 4, 'gamma': 0.7554712931336599}. Best is trial 12 with value: 0.09808463112746094.[0m
[32m[I 2025-02-07 20:43:44,241][0m Trial 13 finished with value: 0.11516656631531984 and parameters: {'observation_period_num': 70, 'train_rates': 0.8851918998645417, 'learning_rate': 0.000815783422455948, 'batch_size': 222, 'step_size': 15, 'gamma': 0.760798052315672}. Best is trial 12 with value: 0.09808463112746094.[0m
Early stopping at epoch 43
[32m[I 2025-02-07 20:44:00,292][0m Trial 14 finished with value: 0.6394906603043375 and parameters: {'observation_period_num': 75, 'train_rates': 0.8956050094939734, 'learning_rate': 5.650167590819976e-05, 'batch_size': 185, 'step_size': 1, 'gamma': 0.7526521780486858}. Best is trial 12 with value: 0.09808463112746094.[0m
[32m[I 2025-02-07 20:44:24,800][0m Trial 15 finished with value: 0.057482712071489646 and parameters: {'observation_period_num': 21, 'train_rates': 0.8387741411176989, 'learning_rate': 0.0009690742432437667, 'batch_size': 251, 'step_size': 4, 'gamma': 0.7882120709267191}. Best is trial 15 with value: 0.057482712071489646.[0m
[32m[I 2025-02-07 20:44:50,211][0m Trial 16 finished with value: 0.06478725797073408 and parameters: {'observation_period_num': 5, 'train_rates': 0.9119952525521917, 'learning_rate': 0.00046775183963364514, 'batch_size': 252, 'step_size': 3, 'gamma': 0.792959266706181}. Best is trial 15 with value: 0.057482712071489646.[0m
[32m[I 2025-02-07 20:45:15,590][0m Trial 17 finished with value: 0.09270396741339937 and parameters: {'observation_period_num': 5, 'train_rates': 0.9145678976386068, 'learning_rate': 5.011938315443684e-05, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9098818516112005}. Best is trial 15 with value: 0.057482712071489646.[0m
[32m[I 2025-02-07 20:45:42,505][0m Trial 18 finished with value: 0.06087878718972206 and parameters: {'observation_period_num': 34, 'train_rates': 0.9242634198391413, 'learning_rate': 0.0003510044718940371, 'batch_size': 239, 'step_size': 8, 'gamma': 0.795815540344776}. Best is trial 15 with value: 0.057482712071489646.[0m
[32m[I 2025-02-07 20:46:08,236][0m Trial 19 finished with value: 0.05065626367926598 and parameters: {'observation_period_num': 38, 'train_rates': 0.8312497584510086, 'learning_rate': 0.0003599691842789168, 'batch_size': 226, 'step_size': 12, 'gamma': 0.7834857672321092}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:46:34,858][0m Trial 20 finished with value: 0.8179707076612359 and parameters: {'observation_period_num': 37, 'train_rates': 0.756817258281395, 'learning_rate': 2.397014563533492e-06, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8274953804558314}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:47:00,735][0m Trial 21 finished with value: 0.05150512288452663 and parameters: {'observation_period_num': 30, 'train_rates': 0.8360822961321673, 'learning_rate': 0.00031587428429123834, 'batch_size': 232, 'step_size': 8, 'gamma': 0.7838907972508209}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:47:28,931][0m Trial 22 finished with value: 0.08273009976890303 and parameters: {'observation_period_num': 44, 'train_rates': 0.8202122673660333, 'learning_rate': 8.317047339808463e-05, 'batch_size': 219, 'step_size': 12, 'gamma': 0.7766476142900182}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:47:55,000][0m Trial 23 finished with value: 0.07422274800005907 and parameters: {'observation_period_num': 47, 'train_rates': 0.8365727276481979, 'learning_rate': 0.00018720019461683626, 'batch_size': 234, 'step_size': 14, 'gamma': 0.8178589638837986}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:48:23,526][0m Trial 24 finished with value: 0.16069029740177943 and parameters: {'observation_period_num': 25, 'train_rates': 0.7314408318288226, 'learning_rate': 0.0004332348232287894, 'batch_size': 193, 'step_size': 11, 'gamma': 0.7824249165739452}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:48:59,293][0m Trial 25 finished with value: 0.06774920646665114 and parameters: {'observation_period_num': 53, 'train_rates': 0.8633477437178888, 'learning_rate': 0.000861334864628095, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8970142217486251}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:49:25,162][0m Trial 26 finished with value: 0.0976927287356798 and parameters: {'observation_period_num': 99, 'train_rates': 0.8370815293599336, 'learning_rate': 0.0003454602487414217, 'batch_size': 231, 'step_size': 7, 'gamma': 0.7801420386116601}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:49:53,366][0m Trial 27 finished with value: 0.23829713642177447 and parameters: {'observation_period_num': 89, 'train_rates': 0.7563164978260888, 'learning_rate': 0.00014794418790117339, 'batch_size': 198, 'step_size': 9, 'gamma': 0.7737582033644181}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:50:14,018][0m Trial 28 finished with value: 0.22263251912067075 and parameters: {'observation_period_num': 21, 'train_rates': 0.6507441507596566, 'learning_rate': 3.896379076693568e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8996461337266838}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:51:06,440][0m Trial 29 finished with value: 0.08248986229300499 and parameters: {'observation_period_num': 54, 'train_rates': 0.8777925197839069, 'learning_rate': 7.385785777262694e-05, 'batch_size': 108, 'step_size': 6, 'gamma': 0.8205161657947174}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:51:32,466][0m Trial 30 finished with value: 0.167984441572707 and parameters: {'observation_period_num': 83, 'train_rates': 0.7879695730648633, 'learning_rate': 2.510443527491327e-05, 'batch_size': 212, 'step_size': 15, 'gamma': 0.9261012457655954}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:51:59,552][0m Trial 31 finished with value: 0.055907242000103 and parameters: {'observation_period_num': 32, 'train_rates': 0.9338572726051724, 'learning_rate': 0.00033839073723754227, 'batch_size': 238, 'step_size': 8, 'gamma': 0.7975817219430725}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:57:37,816][0m Trial 32 finished with value: 0.13422901483325214 and parameters: {'observation_period_num': 62, 'train_rates': 0.9477025709189577, 'learning_rate': 0.0002861662302458385, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8049766849203286}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:58:06,519][0m Trial 33 finished with value: 0.05131715536117554 and parameters: {'observation_period_num': 20, 'train_rates': 0.9834822144924383, 'learning_rate': 0.0009726891396729522, 'batch_size': 234, 'step_size': 7, 'gamma': 0.7888694058222814}. Best is trial 19 with value: 0.05065626367926598.[0m
[32m[I 2025-02-07 20:58:35,719][0m Trial 34 finished with value: 0.04070170968770981 and parameters: {'observation_period_num': 31, 'train_rates': 0.989276814647439, 'learning_rate': 0.0005581498207565264, 'batch_size': 233, 'step_size': 7, 'gamma': 0.7702546836226295}. Best is trial 34 with value: 0.04070170968770981.[0m
[32m[I 2025-02-07 20:59:05,129][0m Trial 35 finished with value: 0.26968368887901306 and parameters: {'observation_period_num': 214, 'train_rates': 0.9879800368329071, 'learning_rate': 0.0005466954341561098, 'batch_size': 229, 'step_size': 6, 'gamma': 0.7671676002355253}. Best is trial 34 with value: 0.04070170968770981.[0m
[32m[I 2025-02-07 21:00:51,626][0m Trial 36 finished with value: 0.14415051837762197 and parameters: {'observation_period_num': 120, 'train_rates': 0.9739766051803977, 'learning_rate': 0.0001897669004339614, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8434902206215033}. Best is trial 34 with value: 0.04070170968770981.[0m
[32m[I 2025-02-07 21:01:33,540][0m Trial 37 finished with value: 0.03462865203619003 and parameters: {'observation_period_num': 7, 'train_rates': 0.9892915809020656, 'learning_rate': 0.00010677938351632241, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8120853031554471}. Best is trial 37 with value: 0.03462865203619003.[0m
[32m[I 2025-02-07 21:02:12,659][0m Trial 38 finished with value: 0.05290716513991356 and parameters: {'observation_period_num': 7, 'train_rates': 0.9590464698292487, 'learning_rate': 0.00010808701883801255, 'batch_size': 161, 'step_size': 10, 'gamma': 0.8123025089785785}. Best is trial 37 with value: 0.03462865203619003.[0m
[32m[I 2025-02-07 21:02:59,604][0m Trial 39 finished with value: 0.053435761481523514 and parameters: {'observation_period_num': 57, 'train_rates': 0.9893758747619387, 'learning_rate': 0.00018187221600005863, 'batch_size': 142, 'step_size': 11, 'gamma': 0.8346942995539459}. Best is trial 37 with value: 0.03462865203619003.[0m
[32m[I 2025-02-07 21:03:59,323][0m Trial 40 finished with value: 0.039308056369962464 and parameters: {'observation_period_num': 13, 'train_rates': 0.9444793096424104, 'learning_rate': 0.0005063738355056589, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8508825775226363}. Best is trial 37 with value: 0.03462865203619003.[0m
[32m[I 2025-02-07 21:04:57,026][0m Trial 41 finished with value: 0.029975574443024167 and parameters: {'observation_period_num': 10, 'train_rates': 0.9397450102324132, 'learning_rate': 0.0005905356317070175, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8502288176079276}. Best is trial 41 with value: 0.029975574443024167.[0m
[32m[I 2025-02-07 21:06:03,783][0m Trial 42 finished with value: 0.03061979440972209 and parameters: {'observation_period_num': 11, 'train_rates': 0.9399636471392098, 'learning_rate': 0.0005338725013604547, 'batch_size': 93, 'step_size': 11, 'gamma': 0.8521555907213072}. Best is trial 41 with value: 0.029975574443024167.[0m
[32m[I 2025-02-07 21:07:05,572][0m Trial 43 finished with value: 0.03402358548704944 and parameters: {'observation_period_num': 13, 'train_rates': 0.9423776563465289, 'learning_rate': 0.0005989033524582135, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8539532856590424}. Best is trial 41 with value: 0.029975574443024167.[0m
[32m[I 2025-02-07 21:08:07,411][0m Trial 44 finished with value: 0.036222956622439316 and parameters: {'observation_period_num': 13, 'train_rates': 0.9422925288456495, 'learning_rate': 0.0006763561155523697, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8559373699399971}. Best is trial 41 with value: 0.029975574443024167.[0m
[32m[I 2025-02-07 21:09:33,682][0m Trial 45 finished with value: 0.027900849185560062 and parameters: {'observation_period_num': 15, 'train_rates': 0.9614778426653978, 'learning_rate': 0.0002271195019082663, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8806208244570233}. Best is trial 45 with value: 0.027900849185560062.[0m
[32m[I 2025-02-07 21:10:54,786][0m Trial 46 finished with value: 0.1870872215791182 and parameters: {'observation_period_num': 141, 'train_rates': 0.911711347828693, 'learning_rate': 0.00025084892111169367, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8919863786417725}. Best is trial 45 with value: 0.027900849185560062.[0m
[32m[I 2025-02-07 21:11:44,436][0m Trial 47 finished with value: 0.21219126880168915 and parameters: {'observation_period_num': 44, 'train_rates': 0.9591431260269057, 'learning_rate': 1.3243536005151434e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8750963258369505}. Best is trial 45 with value: 0.027900849185560062.[0m
[32m[I 2025-02-07 21:13:32,962][0m Trial 48 finished with value: 0.039441600666197646 and parameters: {'observation_period_num': 14, 'train_rates': 0.9001137598228937, 'learning_rate': 0.00014859422028131594, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8828540039454733}. Best is trial 45 with value: 0.027900849185560062.[0m
[32m[I 2025-02-07 21:14:39,770][0m Trial 49 finished with value: 0.24893810130331828 and parameters: {'observation_period_num': 211, 'train_rates': 0.9672230894979836, 'learning_rate': 0.00023524010138379026, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8611174307023989}. Best is trial 45 with value: 0.027900849185560062.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5227 | 0.2667
Epoch 2/300, Loss: 0.1815 | 0.1692
Epoch 3/300, Loss: 0.1512 | 0.1429
Epoch 4/300, Loss: 0.1376 | 0.1270
Epoch 5/300, Loss: 0.1320 | 0.1190
Epoch 6/300, Loss: 0.1307 | 0.1131
Epoch 7/300, Loss: 0.1368 | 0.1009
Epoch 8/300, Loss: 0.1624 | 0.1305
Epoch 9/300, Loss: 0.1838 | 0.2200
Epoch 10/300, Loss: 0.1808 | 0.2957
Epoch 11/300, Loss: 0.1447 | 0.2877
Epoch 12/300, Loss: 0.1329 | 0.1949
Epoch 13/300, Loss: 0.1181 | 0.1144
Epoch 14/300, Loss: 0.1094 | 0.0939
Epoch 15/300, Loss: 0.1053 | 0.0844
Epoch 16/300, Loss: 0.1060 | 0.0789
Epoch 17/300, Loss: 0.1076 | 0.0714
Epoch 18/300, Loss: 0.1005 | 0.0683
Epoch 19/300, Loss: 0.0932 | 0.0638
Epoch 20/300, Loss: 0.0891 | 0.0605
Epoch 21/300, Loss: 0.0883 | 0.0578
Epoch 22/300, Loss: 0.0889 | 0.0568
Epoch 23/300, Loss: 0.0872 | 0.0571
Epoch 24/300, Loss: 0.0836 | 0.0542
Epoch 25/300, Loss: 0.0795 | 0.0523
Epoch 26/300, Loss: 0.0777 | 0.0505
Epoch 27/300, Loss: 0.0772 | 0.0484
Epoch 28/300, Loss: 0.0782 | 0.0476
Epoch 29/300, Loss: 0.0804 | 0.0523
Epoch 30/300, Loss: 0.0815 | 0.0551
Epoch 31/300, Loss: 0.0807 | 0.0552
Epoch 32/300, Loss: 0.0785 | 0.0522
Epoch 33/300, Loss: 0.0760 | 0.0482
Epoch 34/300, Loss: 0.0740 | 0.0465
Epoch 35/300, Loss: 0.0742 | 0.0460
Epoch 36/300, Loss: 0.0783 | 0.0485
Epoch 37/300, Loss: 0.0834 | 0.0474
Epoch 38/300, Loss: 0.0763 | 0.0447
Epoch 39/300, Loss: 0.0728 | 0.0437
Epoch 40/300, Loss: 0.0721 | 0.0427
Epoch 41/300, Loss: 0.0718 | 0.0418
Epoch 42/300, Loss: 0.0715 | 0.0415
Epoch 43/300, Loss: 0.0710 | 0.0414
Epoch 44/300, Loss: 0.0702 | 0.0411
Epoch 45/300, Loss: 0.0695 | 0.0412
Epoch 46/300, Loss: 0.0687 | 0.0413
Epoch 47/300, Loss: 0.0681 | 0.0418
Epoch 48/300, Loss: 0.0678 | 0.0423
Epoch 49/300, Loss: 0.0678 | 0.0426
Epoch 50/300, Loss: 0.0681 | 0.0427
Epoch 51/300, Loss: 0.0685 | 0.0439
Epoch 52/300, Loss: 0.0690 | 0.0451
Epoch 53/300, Loss: 0.0688 | 0.0452
Epoch 54/300, Loss: 0.0680 | 0.0443
Epoch 55/300, Loss: 0.0669 | 0.0433
Epoch 56/300, Loss: 0.0660 | 0.0420
Epoch 57/300, Loss: 0.0656 | 0.0422
Epoch 58/300, Loss: 0.0657 | 0.0425
Epoch 59/300, Loss: 0.0666 | 0.0428
Epoch 60/300, Loss: 0.0678 | 0.0423
Epoch 61/300, Loss: 0.0694 | 0.0410
Epoch 62/300, Loss: 0.0716 | 0.0443
Epoch 63/300, Loss: 0.0722 | 0.0465
Epoch 64/300, Loss: 0.0699 | 0.0450
Epoch 65/300, Loss: 0.0665 | 0.0421
Epoch 66/300, Loss: 0.0648 | 0.0402
Epoch 67/300, Loss: 0.0657 | 0.0375
Epoch 68/300, Loss: 0.0687 | 0.0368
Epoch 69/300, Loss: 0.0693 | 0.0363
Epoch 70/300, Loss: 0.0677 | 0.0357
Epoch 71/300, Loss: 0.0664 | 0.0352
Epoch 72/300, Loss: 0.0658 | 0.0349
Epoch 73/300, Loss: 0.0659 | 0.0345
Epoch 74/300, Loss: 0.0656 | 0.0349
Epoch 75/300, Loss: 0.0650 | 0.0354
Epoch 76/300, Loss: 0.0645 | 0.0353
Epoch 77/300, Loss: 0.0639 | 0.0347
Epoch 78/300, Loss: 0.0635 | 0.0346
Epoch 79/300, Loss: 0.0629 | 0.0352
Epoch 80/300, Loss: 0.0623 | 0.0356
Epoch 81/300, Loss: 0.0620 | 0.0359
Epoch 82/300, Loss: 0.0619 | 0.0360
Epoch 83/300, Loss: 0.0617 | 0.0362
Epoch 84/300, Loss: 0.0614 | 0.0363
Epoch 85/300, Loss: 0.0614 | 0.0364
Epoch 86/300, Loss: 0.0615 | 0.0365
Epoch 87/300, Loss: 0.0617 | 0.0364
Epoch 88/300, Loss: 0.0619 | 0.0362
Epoch 89/300, Loss: 0.0619 | 0.0356
Epoch 90/300, Loss: 0.0616 | 0.0357
Epoch 91/300, Loss: 0.0612 | 0.0360
Epoch 92/300, Loss: 0.0609 | 0.0362
Epoch 93/300, Loss: 0.0606 | 0.0363
Epoch 94/300, Loss: 0.0604 | 0.0364
Epoch 95/300, Loss: 0.0602 | 0.0361
Epoch 96/300, Loss: 0.0601 | 0.0363
Epoch 97/300, Loss: 0.0601 | 0.0364
Epoch 98/300, Loss: 0.0601 | 0.0365
Epoch 99/300, Loss: 0.0602 | 0.0365
Epoch 100/300, Loss: 0.0602 | 0.0360
Epoch 101/300, Loss: 0.0601 | 0.0362
Epoch 102/300, Loss: 0.0599 | 0.0364
Epoch 103/300, Loss: 0.0598 | 0.0367
Epoch 104/300, Loss: 0.0597 | 0.0370
Epoch 105/300, Loss: 0.0596 | 0.0373
Epoch 106/300, Loss: 0.0595 | 0.0373
Epoch 107/300, Loss: 0.0594 | 0.0378
Epoch 108/300, Loss: 0.0592 | 0.0383
Epoch 109/300, Loss: 0.0591 | 0.0387
Epoch 110/300, Loss: 0.0591 | 0.0390
Epoch 111/300, Loss: 0.0591 | 0.0392
Epoch 112/300, Loss: 0.0591 | 0.0396
Epoch 113/300, Loss: 0.0591 | 0.0400
Epoch 114/300, Loss: 0.0590 | 0.0403
Epoch 115/300, Loss: 0.0590 | 0.0406
Epoch 116/300, Loss: 0.0589 | 0.0408
Epoch 117/300, Loss: 0.0589 | 0.0409
Epoch 118/300, Loss: 0.0589 | 0.0412
Epoch 119/300, Loss: 0.0587 | 0.0413
Epoch 120/300, Loss: 0.0586 | 0.0412
Epoch 121/300, Loss: 0.0585 | 0.0409
Epoch 122/300, Loss: 0.0585 | 0.0393
Epoch 123/300, Loss: 0.0585 | 0.0388
Epoch 124/300, Loss: 0.0585 | 0.0378
Epoch 125/300, Loss: 0.0587 | 0.0368
Epoch 126/300, Loss: 0.0589 | 0.0358
Epoch 127/300, Loss: 0.0591 | 0.0350
Epoch 128/300, Loss: 0.0595 | 0.0339
Epoch 129/300, Loss: 0.0595 | 0.0339
Epoch 130/300, Loss: 0.0591 | 0.0342
Epoch 131/300, Loss: 0.0586 | 0.0347
Epoch 132/300, Loss: 0.0582 | 0.0352
Epoch 133/300, Loss: 0.0580 | 0.0360
Epoch 134/300, Loss: 0.0579 | 0.0364
Epoch 135/300, Loss: 0.0578 | 0.0366
Epoch 136/300, Loss: 0.0577 | 0.0367
Epoch 137/300, Loss: 0.0575 | 0.0368
Epoch 138/300, Loss: 0.0574 | 0.0369
Epoch 139/300, Loss: 0.0572 | 0.0367
Epoch 140/300, Loss: 0.0571 | 0.0369
Epoch 141/300, Loss: 0.0570 | 0.0370
Epoch 142/300, Loss: 0.0569 | 0.0371
Epoch 143/300, Loss: 0.0569 | 0.0371
Epoch 144/300, Loss: 0.0568 | 0.0367
Epoch 145/300, Loss: 0.0568 | 0.0366
Epoch 146/300, Loss: 0.0567 | 0.0364
Epoch 147/300, Loss: 0.0567 | 0.0362
Epoch 148/300, Loss: 0.0567 | 0.0360
Epoch 149/300, Loss: 0.0566 | 0.0358
Epoch 150/300, Loss: 0.0566 | 0.0355
Epoch 151/300, Loss: 0.0566 | 0.0354
Epoch 152/300, Loss: 0.0565 | 0.0354
Epoch 153/300, Loss: 0.0565 | 0.0354
Epoch 154/300, Loss: 0.0565 | 0.0355
Epoch 155/300, Loss: 0.0564 | 0.0355
Epoch 156/300, Loss: 0.0564 | 0.0356
Epoch 157/300, Loss: 0.0564 | 0.0356
Epoch 158/300, Loss: 0.0563 | 0.0357
Epoch 159/300, Loss: 0.0563 | 0.0357
Epoch 160/300, Loss: 0.0563 | 0.0357
Epoch 161/300, Loss: 0.0562 | 0.0356
Epoch 162/300, Loss: 0.0562 | 0.0356
Epoch 163/300, Loss: 0.0561 | 0.0356
Epoch 164/300, Loss: 0.0561 | 0.0355
Epoch 165/300, Loss: 0.0561 | 0.0355
Epoch 166/300, Loss: 0.0560 | 0.0353
Epoch 167/300, Loss: 0.0560 | 0.0353
Epoch 168/300, Loss: 0.0560 | 0.0352
Epoch 169/300, Loss: 0.0560 | 0.0352
Epoch 170/300, Loss: 0.0559 | 0.0352
Epoch 171/300, Loss: 0.0559 | 0.0351
Epoch 172/300, Loss: 0.0559 | 0.0351
Epoch 173/300, Loss: 0.0559 | 0.0351
Epoch 174/300, Loss: 0.0558 | 0.0351
Epoch 175/300, Loss: 0.0558 | 0.0351
Epoch 176/300, Loss: 0.0558 | 0.0350
Epoch 177/300, Loss: 0.0558 | 0.0350
Epoch 178/300, Loss: 0.0557 | 0.0349
Epoch 179/300, Loss: 0.0557 | 0.0349
Epoch 180/300, Loss: 0.0557 | 0.0349
Epoch 181/300, Loss: 0.0557 | 0.0349
Epoch 182/300, Loss: 0.0557 | 0.0348
Epoch 183/300, Loss: 0.0556 | 0.0348
Epoch 184/300, Loss: 0.0556 | 0.0348
Epoch 185/300, Loss: 0.0556 | 0.0347
Epoch 186/300, Loss: 0.0556 | 0.0347
Epoch 187/300, Loss: 0.0556 | 0.0347
Epoch 188/300, Loss: 0.0555 | 0.0347
Epoch 189/300, Loss: 0.0555 | 0.0346
Epoch 190/300, Loss: 0.0555 | 0.0346
Epoch 191/300, Loss: 0.0555 | 0.0346
Epoch 192/300, Loss: 0.0555 | 0.0346
Epoch 193/300, Loss: 0.0555 | 0.0346
Epoch 194/300, Loss: 0.0554 | 0.0345
Epoch 195/300, Loss: 0.0554 | 0.0345
Epoch 196/300, Loss: 0.0554 | 0.0345
Epoch 197/300, Loss: 0.0554 | 0.0345
Epoch 198/300, Loss: 0.0554 | 0.0344
Epoch 199/300, Loss: 0.0554 | 0.0344
Epoch 200/300, Loss: 0.0553 | 0.0344
Epoch 201/300, Loss: 0.0553 | 0.0344
Epoch 202/300, Loss: 0.0553 | 0.0344
Epoch 203/300, Loss: 0.0553 | 0.0344
Epoch 204/300, Loss: 0.0553 | 0.0343
Epoch 205/300, Loss: 0.0553 | 0.0343
Epoch 206/300, Loss: 0.0553 | 0.0343
Epoch 207/300, Loss: 0.0552 | 0.0343
Epoch 208/300, Loss: 0.0552 | 0.0343
Epoch 209/300, Loss: 0.0552 | 0.0343
Epoch 210/300, Loss: 0.0552 | 0.0342
Epoch 211/300, Loss: 0.0552 | 0.0342
Epoch 212/300, Loss: 0.0552 | 0.0342
Epoch 213/300, Loss: 0.0552 | 0.0342
Epoch 214/300, Loss: 0.0552 | 0.0342
Epoch 215/300, Loss: 0.0552 | 0.0342
Epoch 216/300, Loss: 0.0551 | 0.0341
Epoch 217/300, Loss: 0.0551 | 0.0341
Epoch 218/300, Loss: 0.0551 | 0.0341
Epoch 219/300, Loss: 0.0551 | 0.0341
Epoch 220/300, Loss: 0.0551 | 0.0341
Epoch 221/300, Loss: 0.0551 | 0.0341
Epoch 222/300, Loss: 0.0551 | 0.0341
Epoch 223/300, Loss: 0.0551 | 0.0341
Epoch 224/300, Loss: 0.0551 | 0.0340
Epoch 225/300, Loss: 0.0551 | 0.0340
Epoch 226/300, Loss: 0.0550 | 0.0340
Epoch 227/300, Loss: 0.0550 | 0.0340
Epoch 228/300, Loss: 0.0550 | 0.0340
Epoch 229/300, Loss: 0.0550 | 0.0340
Epoch 230/300, Loss: 0.0550 | 0.0340
Epoch 231/300, Loss: 0.0550 | 0.0340
Epoch 232/300, Loss: 0.0550 | 0.0340
Epoch 233/300, Loss: 0.0550 | 0.0340
Epoch 234/300, Loss: 0.0550 | 0.0339
Epoch 235/300, Loss: 0.0550 | 0.0339
Epoch 236/300, Loss: 0.0550 | 0.0339
Epoch 237/300, Loss: 0.0550 | 0.0339
Epoch 238/300, Loss: 0.0550 | 0.0339
Epoch 239/300, Loss: 0.0550 | 0.0339
Epoch 240/300, Loss: 0.0549 | 0.0339
Epoch 241/300, Loss: 0.0549 | 0.0339
Epoch 242/300, Loss: 0.0549 | 0.0339
Epoch 243/300, Loss: 0.0549 | 0.0339
Epoch 244/300, Loss: 0.0549 | 0.0339
Epoch 245/300, Loss: 0.0549 | 0.0339
Epoch 246/300, Loss: 0.0549 | 0.0338
Epoch 247/300, Loss: 0.0549 | 0.0338
Epoch 248/300, Loss: 0.0549 | 0.0338
Epoch 249/300, Loss: 0.0549 | 0.0338
Epoch 250/300, Loss: 0.0549 | 0.0338
Epoch 251/300, Loss: 0.0549 | 0.0338
Epoch 252/300, Loss: 0.0549 | 0.0338
Epoch 253/300, Loss: 0.0549 | 0.0338
Epoch 254/300, Loss: 0.0549 | 0.0338
Epoch 255/300, Loss: 0.0549 | 0.0338
Epoch 256/300, Loss: 0.0549 | 0.0338
Epoch 257/300, Loss: 0.0549 | 0.0338
Epoch 258/300, Loss: 0.0549 | 0.0338
Epoch 259/300, Loss: 0.0548 | 0.0338
Epoch 260/300, Loss: 0.0548 | 0.0338
Epoch 261/300, Loss: 0.0548 | 0.0338
Epoch 262/300, Loss: 0.0548 | 0.0338
Epoch 263/300, Loss: 0.0548 | 0.0337
Epoch 264/300, Loss: 0.0548 | 0.0337
Epoch 265/300, Loss: 0.0548 | 0.0337
Epoch 266/300, Loss: 0.0548 | 0.0337
Epoch 267/300, Loss: 0.0548 | 0.0337
Epoch 268/300, Loss: 0.0548 | 0.0337
Epoch 269/300, Loss: 0.0548 | 0.0337
Epoch 270/300, Loss: 0.0548 | 0.0337
Epoch 271/300, Loss: 0.0548 | 0.0337
Epoch 272/300, Loss: 0.0548 | 0.0337
Epoch 273/300, Loss: 0.0548 | 0.0337
Epoch 274/300, Loss: 0.0548 | 0.0337
Epoch 275/300, Loss: 0.0548 | 0.0337
Epoch 276/300, Loss: 0.0548 | 0.0337
Epoch 277/300, Loss: 0.0548 | 0.0337
Epoch 278/300, Loss: 0.0548 | 0.0337
Epoch 279/300, Loss: 0.0548 | 0.0337
Epoch 280/300, Loss: 0.0548 | 0.0337
Epoch 281/300, Loss: 0.0548 | 0.0337
Epoch 282/300, Loss: 0.0548 | 0.0337
Epoch 283/300, Loss: 0.0548 | 0.0337
Epoch 284/300, Loss: 0.0548 | 0.0337
Epoch 285/300, Loss: 0.0548 | 0.0337
Epoch 286/300, Loss: 0.0548 | 0.0337
Epoch 287/300, Loss: 0.0548 | 0.0337
Epoch 288/300, Loss: 0.0548 | 0.0337
Epoch 289/300, Loss: 0.0548 | 0.0337
Epoch 290/300, Loss: 0.0548 | 0.0337
Epoch 291/300, Loss: 0.0548 | 0.0337
Epoch 292/300, Loss: 0.0548 | 0.0336
Epoch 293/300, Loss: 0.0548 | 0.0336
Epoch 294/300, Loss: 0.0548 | 0.0336
Epoch 295/300, Loss: 0.0547 | 0.0336
Epoch 296/300, Loss: 0.0547 | 0.0336
Epoch 297/300, Loss: 0.0547 | 0.0336
Epoch 298/300, Loss: 0.0547 | 0.0336
Epoch 299/300, Loss: 0.0547 | 0.0336
Epoch 300/300, Loss: 0.0547 | 0.0336
Runtime (seconds): 261.36948442459106
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 62.66161234327592
RMSE: 7.9159088134765625
MAE: 7.9159088134765625
R-squared: nan
[201.2259]
