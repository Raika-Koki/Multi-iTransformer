[32m[I 2025-02-08 12:52:54,936][0m A new study created in memory with name: no-name-db99c96b-dfdd-495b-a83c-6b8885a89539[0m
[32m[I 2025-02-08 12:53:32,137][0m Trial 0 finished with value: 0.8618039212276026 and parameters: {'observation_period_num': 100, 'train_rates': 0.7661487798265876, 'learning_rate': 1.6698466855375745e-06, 'batch_size': 140, 'step_size': 12, 'gamma': 0.7992453658230679}. Best is trial 0 with value: 0.8618039212276026.[0m
[32m[I 2025-02-08 12:53:55,902][0m Trial 1 finished with value: 0.6104041042740159 and parameters: {'observation_period_num': 77, 'train_rates': 0.6174689935167567, 'learning_rate': 6.579741978489263e-05, 'batch_size': 201, 'step_size': 6, 'gamma': 0.9682310354152596}. Best is trial 1 with value: 0.6104041042740159.[0m
[32m[I 2025-02-08 12:55:07,817][0m Trial 2 finished with value: 0.7972244109426226 and parameters: {'observation_period_num': 204, 'train_rates': 0.9700606573646433, 'learning_rate': 8.939214344079629e-06, 'batch_size': 80, 'step_size': 2, 'gamma': 0.8339174233359805}. Best is trial 1 with value: 0.6104041042740159.[0m
[32m[I 2025-02-08 12:55:31,431][0m Trial 3 finished with value: 0.49225132741154204 and parameters: {'observation_period_num': 122, 'train_rates': 0.6554655960733565, 'learning_rate': 7.005069238944245e-06, 'batch_size': 229, 'step_size': 14, 'gamma': 0.959991145239165}. Best is trial 3 with value: 0.49225132741154204.[0m
[32m[I 2025-02-08 12:56:04,249][0m Trial 4 finished with value: 0.5226724875642128 and parameters: {'observation_period_num': 219, 'train_rates': 0.8912555845872812, 'learning_rate': 4.892819800962172e-06, 'batch_size': 178, 'step_size': 14, 'gamma': 0.9696656772197785}. Best is trial 3 with value: 0.49225132741154204.[0m
[32m[I 2025-02-08 12:59:44,964][0m Trial 5 finished with value: 0.4157507680864959 and parameters: {'observation_period_num': 151, 'train_rates': 0.614587342441357, 'learning_rate': 4.061558703997488e-06, 'batch_size': 19, 'step_size': 4, 'gamma': 0.8624163539607328}. Best is trial 5 with value: 0.4157507680864959.[0m
[32m[I 2025-02-08 13:00:15,743][0m Trial 6 finished with value: 0.32699634890605916 and parameters: {'observation_period_num': 186, 'train_rates': 0.6072948589283044, 'learning_rate': 3.05327850011825e-05, 'batch_size': 155, 'step_size': 11, 'gamma': 0.9301158698629187}. Best is trial 6 with value: 0.32699634890605916.[0m
[32m[I 2025-02-08 13:01:28,530][0m Trial 7 finished with value: 0.1129932930925861 and parameters: {'observation_period_num': 80, 'train_rates': 0.8248687016938618, 'learning_rate': 4.16706328683252e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.7852263139949415}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:02:16,164][0m Trial 8 finished with value: 0.6738254899978637 and parameters: {'observation_period_num': 216, 'train_rates': 0.8205018076270614, 'learning_rate': 2.703095805091844e-06, 'batch_size': 112, 'step_size': 6, 'gamma': 0.8600671803529327}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:02:46,067][0m Trial 9 finished with value: 0.37199312686920166 and parameters: {'observation_period_num': 30, 'train_rates': 0.9161012311423383, 'learning_rate': 1.0547126941333766e-05, 'batch_size': 215, 'step_size': 9, 'gamma': 0.7596712987716092}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:04:42,052][0m Trial 10 finished with value: 0.14395068461850563 and parameters: {'observation_period_num': 5, 'train_rates': 0.7312527726890875, 'learning_rate': 0.0008501831401574198, 'batch_size': 43, 'step_size': 8, 'gamma': 0.7548758825478833}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:06:51,205][0m Trial 11 finished with value: 0.1481385173645752 and parameters: {'observation_period_num': 11, 'train_rates': 0.7371902348976486, 'learning_rate': 0.0006628855023163619, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7595722880486847}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:08:01,471][0m Trial 12 finished with value: 0.17430094465420298 and parameters: {'observation_period_num': 53, 'train_rates': 0.7120171842005442, 'learning_rate': 0.0009978659305281184, 'batch_size': 70, 'step_size': 8, 'gamma': 0.8012531642049089}. Best is trial 7 with value: 0.1129932930925861.[0m
[32m[I 2025-02-08 13:09:13,007][0m Trial 13 finished with value: 0.06870699271364382 and parameters: {'observation_period_num': 61, 'train_rates': 0.8183833288447893, 'learning_rate': 0.00022873724346178403, 'batch_size': 77, 'step_size': 5, 'gamma': 0.7970217765689432}. Best is trial 13 with value: 0.06870699271364382.[0m
Early stopping at epoch 63
[32m[I 2025-02-08 13:09:49,129][0m Trial 14 finished with value: 0.22265774983554082 and parameters: {'observation_period_num': 75, 'train_rates': 0.82600960721477, 'learning_rate': 0.00016809261101292128, 'batch_size': 100, 'step_size': 1, 'gamma': 0.8102875489962472}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:10:41,775][0m Trial 15 finished with value: 0.11560677678524693 and parameters: {'observation_period_num': 150, 'train_rates': 0.8616991469986943, 'learning_rate': 0.00017330930299940366, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9190348282488364}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:12:03,053][0m Trial 16 finished with value: 0.1467895743993446 and parameters: {'observation_period_num': 65, 'train_rates': 0.7938010515477754, 'learning_rate': 4.222862700869842e-05, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8280244442823421}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:12:28,491][0m Trial 17 finished with value: 0.1677081137895584 and parameters: {'observation_period_num': 104, 'train_rates': 0.9388011325200171, 'learning_rate': 0.00018306826185423913, 'batch_size': 251, 'step_size': 11, 'gamma': 0.7802968266110915}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:13:31,449][0m Trial 18 finished with value: 0.16686914524724406 and parameters: {'observation_period_num': 97, 'train_rates': 0.8718840792471313, 'learning_rate': 2.3732804128748268e-05, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8968005668469031}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:14:12,139][0m Trial 19 finished with value: 0.2026463674286804 and parameters: {'observation_period_num': 42, 'train_rates': 0.6938643976186175, 'learning_rate': 7.706230910170105e-05, 'batch_size': 125, 'step_size': 4, 'gamma': 0.8294850399021203}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:15:51,268][0m Trial 20 finished with value: 0.2501163123441594 and parameters: {'observation_period_num': 248, 'train_rates': 0.8273560518259576, 'learning_rate': 0.00036272438992634087, 'batch_size': 51, 'step_size': 10, 'gamma': 0.7826095185971507}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:16:40,991][0m Trial 21 finished with value: 0.14307705583897504 and parameters: {'observation_period_num': 155, 'train_rates': 0.8648404338963391, 'learning_rate': 0.00016336050326935095, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9006268767722733}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:17:18,968][0m Trial 22 finished with value: 0.15223881504221864 and parameters: {'observation_period_num': 141, 'train_rates': 0.7824016359402588, 'learning_rate': 0.0003578785905132171, 'batch_size': 149, 'step_size': 4, 'gamma': 0.9309856034127882}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:18:17,329][0m Trial 23 finished with value: 0.12325859386792684 and parameters: {'observation_period_num': 174, 'train_rates': 0.8553701477742032, 'learning_rate': 0.00010129374437834445, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8941095187212784}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:19:37,941][0m Trial 24 finished with value: 0.17431922553995854 and parameters: {'observation_period_num': 125, 'train_rates': 0.8406147424117407, 'learning_rate': 0.000347613121989142, 'batch_size': 66, 'step_size': 7, 'gamma': 0.931612808332382}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:23:20,962][0m Trial 25 finished with value: 0.2052520844390837 and parameters: {'observation_period_num': 86, 'train_rates': 0.8990122698088423, 'learning_rate': 2.0642554438323032e-05, 'batch_size': 25, 'step_size': 2, 'gamma': 0.8425572495832374}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:23:54,037][0m Trial 26 finished with value: 0.46247364202884733 and parameters: {'observation_period_num': 116, 'train_rates': 0.7524931205269609, 'learning_rate': 4.732369028881497e-05, 'batch_size': 167, 'step_size': 5, 'gamma': 0.7805738102604424}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:24:44,212][0m Trial 27 finished with value: 0.06889041695954665 and parameters: {'observation_period_num': 31, 'train_rates': 0.803766625242609, 'learning_rate': 0.00011166737333426801, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9497570577382176}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:25:28,584][0m Trial 28 finished with value: 0.10004211802127932 and parameters: {'observation_period_num': 30, 'train_rates': 0.7966087151150881, 'learning_rate': 1.7307694039449686e-05, 'batch_size': 130, 'step_size': 9, 'gamma': 0.9861517041750587}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:26:11,611][0m Trial 29 finished with value: 0.584393757710354 and parameters: {'observation_period_num': 22, 'train_rates': 0.7667615174622692, 'learning_rate': 1.314532380303223e-06, 'batch_size': 134, 'step_size': 12, 'gamma': 0.9507199497051272}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:26:53,438][0m Trial 30 finished with value: 0.12367949736504831 and parameters: {'observation_period_num': 52, 'train_rates': 0.8014828387800544, 'learning_rate': 1.4396185933429407e-05, 'batch_size': 134, 'step_size': 9, 'gamma': 0.9885804677524854}. Best is trial 13 with value: 0.06870699271364382.[0m
[32m[I 2025-02-08 13:27:39,539][0m Trial 31 finished with value: 0.055609872278112635 and parameters: {'observation_period_num': 36, 'train_rates': 0.7895492276587628, 'learning_rate': 0.00010836290930054841, 'batch_size': 120, 'step_size': 7, 'gamma': 0.9842089195173924}. Best is trial 31 with value: 0.055609872278112635.[0m
[32m[I 2025-02-08 13:28:23,674][0m Trial 32 finished with value: 0.18238208103548528 and parameters: {'observation_period_num': 36, 'train_rates': 0.7766385667700876, 'learning_rate': 9.580369203848499e-05, 'batch_size': 122, 'step_size': 7, 'gamma': 0.9861318700912354}. Best is trial 31 with value: 0.055609872278112635.[0m
[32m[I 2025-02-08 13:28:58,815][0m Trial 33 finished with value: 0.050395047078974804 and parameters: {'observation_period_num': 20, 'train_rates': 0.7977259145529352, 'learning_rate': 0.00025622403397751383, 'batch_size': 168, 'step_size': 9, 'gamma': 0.9744581450125874}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:29:28,996][0m Trial 34 finished with value: 0.2462119162979432 and parameters: {'observation_period_num': 57, 'train_rates': 0.6614430897202662, 'learning_rate': 0.0005104316689941154, 'batch_size': 172, 'step_size': 10, 'gamma': 0.9521328588296605}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:29:57,465][0m Trial 35 finished with value: 0.17570678615309857 and parameters: {'observation_period_num': 17, 'train_rates': 0.7543201595170188, 'learning_rate': 0.0002790532413393589, 'batch_size': 200, 'step_size': 15, 'gamma': 0.9739251898483158}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:30:36,459][0m Trial 36 finished with value: 0.06766203989153323 and parameters: {'observation_period_num': 41, 'train_rates': 0.8058427979928848, 'learning_rate': 0.00010637093430965726, 'batch_size': 148, 'step_size': 7, 'gamma': 0.9479313743757141}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:31:03,636][0m Trial 37 finished with value: 0.20669488642512068 and parameters: {'observation_period_num': 45, 'train_rates': 0.7005527492450662, 'learning_rate': 0.0002492914419333519, 'batch_size': 190, 'step_size': 10, 'gamma': 0.964261099739073}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:31:43,008][0m Trial 38 finished with value: 0.11625474892936631 and parameters: {'observation_period_num': 66, 'train_rates': 0.846388988333481, 'learning_rate': 6.222877417009224e-05, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8779866529455308}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:32:22,581][0m Trial 39 finished with value: 0.05902508646249771 and parameters: {'observation_period_num': 18, 'train_rates': 0.9797740440509688, 'learning_rate': 0.00011351595528646176, 'batch_size': 164, 'step_size': 3, 'gamma': 0.973174095588685}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:33:03,305][0m Trial 40 finished with value: 0.06376189738512039 and parameters: {'observation_period_num': 24, 'train_rates': 0.9839520128475362, 'learning_rate': 0.0001226388353416706, 'batch_size': 164, 'step_size': 13, 'gamma': 0.971327344842437}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:33:44,421][0m Trial 41 finished with value: 0.05780293047428131 and parameters: {'observation_period_num': 18, 'train_rates': 0.9874230240688101, 'learning_rate': 0.00012320845399277616, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9740535828217708}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:34:19,999][0m Trial 42 finished with value: 0.08834017068147659 and parameters: {'observation_period_num': 16, 'train_rates': 0.9889574199372859, 'learning_rate': 5.8698114343041945e-05, 'batch_size': 186, 'step_size': 13, 'gamma': 0.9736968303666687}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:34:59,211][0m Trial 43 finished with value: 0.06514418870210648 and parameters: {'observation_period_num': 23, 'train_rates': 0.9526707364201132, 'learning_rate': 0.00014490124242940596, 'batch_size': 161, 'step_size': 13, 'gamma': 0.9746184117178225}. Best is trial 33 with value: 0.050395047078974804.[0m
[32m[I 2025-02-08 13:35:37,648][0m Trial 44 finished with value: 0.04196508601307869 and parameters: {'observation_period_num': 6, 'train_rates': 0.989953752164563, 'learning_rate': 3.1760542161515166e-05, 'batch_size': 181, 'step_size': 12, 'gamma': 0.9607140268680169}. Best is trial 44 with value: 0.04196508601307869.[0m
[32m[I 2025-02-08 13:36:09,760][0m Trial 45 finished with value: 0.0698254257440567 and parameters: {'observation_period_num': 6, 'train_rates': 0.9610939418800819, 'learning_rate': 3.368475234951546e-05, 'batch_size': 220, 'step_size': 12, 'gamma': 0.9394217569862017}. Best is trial 44 with value: 0.04196508601307869.[0m
[32m[I 2025-02-08 13:36:46,054][0m Trial 46 finished with value: 0.05736720612493612 and parameters: {'observation_period_num': 9, 'train_rates': 0.9213428351732997, 'learning_rate': 7.06666744335928e-05, 'batch_size': 180, 'step_size': 11, 'gamma': 0.9591902248492137}. Best is trial 44 with value: 0.04196508601307869.[0m
[32m[I 2025-02-08 13:37:22,326][0m Trial 47 finished with value: 0.05691425787381163 and parameters: {'observation_period_num': 10, 'train_rates': 0.9289026460312446, 'learning_rate': 7.585789724221937e-05, 'batch_size': 181, 'step_size': 11, 'gamma': 0.9117115481346469}. Best is trial 44 with value: 0.04196508601307869.[0m
[32m[I 2025-02-08 13:37:53,498][0m Trial 48 finished with value: 0.06806541828994166 and parameters: {'observation_period_num': 5, 'train_rates': 0.9239510919816272, 'learning_rate': 3.0353196694717953e-05, 'batch_size': 200, 'step_size': 11, 'gamma': 0.9152290229719106}. Best is trial 44 with value: 0.04196508601307869.[0m
[32m[I 2025-02-08 13:38:27,033][0m Trial 49 finished with value: 0.08000864638227982 and parameters: {'observation_period_num': 45, 'train_rates': 0.896244031132394, 'learning_rate': 8.163077464295012e-05, 'batch_size': 183, 'step_size': 10, 'gamma': 0.9576982834225334}. Best is trial 44 with value: 0.04196508601307869.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3365 | 0.6916
Epoch 2/300, Loss: 0.2675 | 0.4490
Epoch 3/300, Loss: 0.2119 | 0.4010
Epoch 4/300, Loss: 0.1788 | 0.3289
Epoch 5/300, Loss: 0.1637 | 0.2842
Epoch 6/300, Loss: 0.1524 | 0.2571
Epoch 7/300, Loss: 0.1470 | 0.2393
Epoch 8/300, Loss: 0.1429 | 0.2271
Epoch 9/300, Loss: 0.1394 | 0.2141
Epoch 10/300, Loss: 0.1357 | 0.2043
Epoch 11/300, Loss: 0.1338 | 0.1936
Epoch 12/300, Loss: 0.1316 | 0.1837
Epoch 13/300, Loss: 0.1292 | 0.1751
Epoch 14/300, Loss: 0.1269 | 0.1651
Epoch 15/300, Loss: 0.1253 | 0.1579
Epoch 16/300, Loss: 0.1235 | 0.1494
Epoch 17/300, Loss: 0.1217 | 0.1434
Epoch 18/300, Loss: 0.1197 | 0.1342
Epoch 19/300, Loss: 0.1182 | 0.1280
Epoch 20/300, Loss: 0.1166 | 0.1207
Epoch 21/300, Loss: 0.1153 | 0.1158
Epoch 22/300, Loss: 0.1138 | 0.1088
Epoch 23/300, Loss: 0.1124 | 0.1034
Epoch 24/300, Loss: 0.1112 | 0.0972
Epoch 25/300, Loss: 0.1101 | 0.0924
Epoch 26/300, Loss: 0.1091 | 0.0877
Epoch 27/300, Loss: 0.1081 | 0.0832
Epoch 28/300, Loss: 0.1072 | 0.0792
Epoch 29/300, Loss: 0.1064 | 0.0754
Epoch 30/300, Loss: 0.1057 | 0.0723
Epoch 31/300, Loss: 0.1051 | 0.0694
Epoch 32/300, Loss: 0.1045 | 0.0668
Epoch 33/300, Loss: 0.1040 | 0.0640
Epoch 34/300, Loss: 0.1037 | 0.0616
Epoch 35/300, Loss: 0.1035 | 0.0595
Epoch 36/300, Loss: 0.1029 | 0.0579
Epoch 37/300, Loss: 0.1022 | 0.0572
Epoch 38/300, Loss: 0.1019 | 0.0575
Epoch 39/300, Loss: 0.1027 | 0.0581
Epoch 40/300, Loss: 0.1031 | 0.0568
Epoch 41/300, Loss: 0.1020 | 0.0532
Epoch 42/300, Loss: 0.1030 | 0.0518
Epoch 43/300, Loss: 0.1094 | 0.0525
Epoch 44/300, Loss: 0.1085 | 0.0495
Epoch 45/300, Loss: 0.1020 | 0.0552
Epoch 46/300, Loss: 0.1130 | 0.0576
Epoch 47/300, Loss: 0.1037 | 0.0507
Epoch 48/300, Loss: 0.1092 | 0.0512
Epoch 49/300, Loss: 0.1038 | 0.0483
Epoch 50/300, Loss: 0.1044 | 0.0509
Epoch 51/300, Loss: 0.1017 | 0.0468
Epoch 52/300, Loss: 0.1027 | 0.0504
Epoch 53/300, Loss: 0.0997 | 0.0475
Epoch 54/300, Loss: 0.1006 | 0.0471
Epoch 55/300, Loss: 0.0980 | 0.0455
Epoch 56/300, Loss: 0.0987 | 0.0460
Epoch 57/300, Loss: 0.0968 | 0.0455
Epoch 58/300, Loss: 0.0973 | 0.0449
Epoch 59/300, Loss: 0.0964 | 0.0443
Epoch 60/300, Loss: 0.0964 | 0.0443
Epoch 61/300, Loss: 0.0957 | 0.0443
Epoch 62/300, Loss: 0.0958 | 0.0436
Epoch 63/300, Loss: 0.0954 | 0.0433
Epoch 64/300, Loss: 0.0952 | 0.0432
Epoch 65/300, Loss: 0.0949 | 0.0432
Epoch 66/300, Loss: 0.0948 | 0.0428
Epoch 67/300, Loss: 0.0945 | 0.0424
Epoch 68/300, Loss: 0.0943 | 0.0423
Epoch 69/300, Loss: 0.0941 | 0.0423
Epoch 70/300, Loss: 0.0939 | 0.0420
Epoch 71/300, Loss: 0.0937 | 0.0417
Epoch 72/300, Loss: 0.0935 | 0.0416
Epoch 73/300, Loss: 0.0933 | 0.0416
Epoch 74/300, Loss: 0.0931 | 0.0413
Epoch 75/300, Loss: 0.0929 | 0.0411
Epoch 76/300, Loss: 0.0927 | 0.0410
Epoch 77/300, Loss: 0.0926 | 0.0410
Epoch 78/300, Loss: 0.0924 | 0.0408
Epoch 79/300, Loss: 0.0922 | 0.0405
Epoch 80/300, Loss: 0.0920 | 0.0404
Epoch 81/300, Loss: 0.0918 | 0.0404
Epoch 82/300, Loss: 0.0917 | 0.0403
Epoch 83/300, Loss: 0.0915 | 0.0400
Epoch 84/300, Loss: 0.0913 | 0.0399
Epoch 85/300, Loss: 0.0911 | 0.0398
Epoch 86/300, Loss: 0.0910 | 0.0397
Epoch 87/300, Loss: 0.0908 | 0.0396
Epoch 88/300, Loss: 0.0906 | 0.0394
Epoch 89/300, Loss: 0.0904 | 0.0393
Epoch 90/300, Loss: 0.0903 | 0.0393
Epoch 91/300, Loss: 0.0901 | 0.0391
Epoch 92/300, Loss: 0.0899 | 0.0389
Epoch 93/300, Loss: 0.0898 | 0.0388
Epoch 94/300, Loss: 0.0896 | 0.0388
Epoch 95/300, Loss: 0.0894 | 0.0387
Epoch 96/300, Loss: 0.0893 | 0.0385
Epoch 97/300, Loss: 0.0891 | 0.0383
Epoch 98/300, Loss: 0.0889 | 0.0383
Epoch 99/300, Loss: 0.0887 | 0.0382
Epoch 100/300, Loss: 0.0886 | 0.0381
Epoch 101/300, Loss: 0.0884 | 0.0379
Epoch 102/300, Loss: 0.0882 | 0.0378
Epoch 103/300, Loss: 0.0880 | 0.0377
Epoch 104/300, Loss: 0.0879 | 0.0377
Epoch 105/300, Loss: 0.0877 | 0.0376
Epoch 106/300, Loss: 0.0876 | 0.0374
Epoch 107/300, Loss: 0.0874 | 0.0372
Epoch 108/300, Loss: 0.0872 | 0.0372
Epoch 109/300, Loss: 0.0870 | 0.0371
Epoch 110/300, Loss: 0.0869 | 0.0370
Epoch 111/300, Loss: 0.0867 | 0.0368
Epoch 112/300, Loss: 0.0865 | 0.0367
Epoch 113/300, Loss: 0.0863 | 0.0367
Epoch 114/300, Loss: 0.0862 | 0.0366
Epoch 115/300, Loss: 0.0860 | 0.0365
Epoch 116/300, Loss: 0.0858 | 0.0363
Epoch 117/300, Loss: 0.0856 | 0.0361
Epoch 118/300, Loss: 0.0854 | 0.0361
Epoch 119/300, Loss: 0.0853 | 0.0361
Epoch 120/300, Loss: 0.0851 | 0.0360
Epoch 121/300, Loss: 0.0849 | 0.0358
Epoch 122/300, Loss: 0.0848 | 0.0356
Epoch 123/300, Loss: 0.0846 | 0.0355
Epoch 124/300, Loss: 0.0844 | 0.0355
Epoch 125/300, Loss: 0.0842 | 0.0355
Epoch 126/300, Loss: 0.0841 | 0.0353
Epoch 127/300, Loss: 0.0839 | 0.0351
Epoch 128/300, Loss: 0.0837 | 0.0349
Epoch 129/300, Loss: 0.0836 | 0.0348
Epoch 130/300, Loss: 0.0834 | 0.0349
Epoch 131/300, Loss: 0.0832 | 0.0349
Epoch 132/300, Loss: 0.0831 | 0.0348
Epoch 133/300, Loss: 0.0829 | 0.0346
Epoch 134/300, Loss: 0.0828 | 0.0343
Epoch 135/300, Loss: 0.0826 | 0.0341
Epoch 136/300, Loss: 0.0825 | 0.0342
Epoch 137/300, Loss: 0.0823 | 0.0344
Epoch 138/300, Loss: 0.0822 | 0.0345
Epoch 139/300, Loss: 0.0820 | 0.0344
Epoch 140/300, Loss: 0.0819 | 0.0340
Epoch 141/300, Loss: 0.0818 | 0.0336
Epoch 142/300, Loss: 0.0817 | 0.0334
Epoch 143/300, Loss: 0.0815 | 0.0335
Epoch 144/300, Loss: 0.0814 | 0.0339
Epoch 145/300, Loss: 0.0813 | 0.0345
Epoch 146/300, Loss: 0.0813 | 0.0346
Epoch 147/300, Loss: 0.0811 | 0.0339
Epoch 148/300, Loss: 0.0809 | 0.0330
Epoch 149/300, Loss: 0.0811 | 0.0325
Epoch 150/300, Loss: 0.0812 | 0.0325
Epoch 151/300, Loss: 0.0808 | 0.0331
Epoch 152/300, Loss: 0.0807 | 0.0350
Epoch 153/300, Loss: 0.0816 | 0.0370
Epoch 154/300, Loss: 0.0818 | 0.0356
Epoch 155/300, Loss: 0.0805 | 0.0328
Epoch 156/300, Loss: 0.0817 | 0.0324
Epoch 157/300, Loss: 0.0835 | 0.0327
Epoch 158/300, Loss: 0.0809 | 0.0339
Epoch 159/300, Loss: 0.0822 | 0.0381
Epoch 160/300, Loss: 0.0830 | 0.0353
Epoch 161/300, Loss: 0.0803 | 0.0332
Epoch 162/300, Loss: 0.0829 | 0.0331
Epoch 163/300, Loss: 0.0802 | 0.0337
Epoch 164/300, Loss: 0.0813 | 0.0353
Epoch 165/300, Loss: 0.0804 | 0.0330
Epoch 166/300, Loss: 0.0803 | 0.0334
Epoch 167/300, Loss: 0.0802 | 0.0332
Epoch 168/300, Loss: 0.0795 | 0.0342
Epoch 169/300, Loss: 0.0799 | 0.0331
Epoch 170/300, Loss: 0.0791 | 0.0331
Epoch 171/300, Loss: 0.0795 | 0.0332
Epoch 172/300, Loss: 0.0787 | 0.0335
Epoch 173/300, Loss: 0.0791 | 0.0332
Epoch 174/300, Loss: 0.0785 | 0.0328
Epoch 175/300, Loss: 0.0788 | 0.0331
Epoch 176/300, Loss: 0.0784 | 0.0332
Epoch 177/300, Loss: 0.0784 | 0.0332
Epoch 178/300, Loss: 0.0782 | 0.0329
Epoch 179/300, Loss: 0.0782 | 0.0329
Epoch 180/300, Loss: 0.0780 | 0.0331
Epoch 181/300, Loss: 0.0779 | 0.0332
Epoch 182/300, Loss: 0.0778 | 0.0330
Epoch 183/300, Loss: 0.0777 | 0.0329
Epoch 184/300, Loss: 0.0777 | 0.0330
Epoch 185/300, Loss: 0.0775 | 0.0332
Epoch 186/300, Loss: 0.0775 | 0.0332
Epoch 187/300, Loss: 0.0774 | 0.0330
Epoch 188/300, Loss: 0.0773 | 0.0330
Epoch 189/300, Loss: 0.0772 | 0.0331
Epoch 190/300, Loss: 0.0771 | 0.0332
Epoch 191/300, Loss: 0.0771 | 0.0331
Epoch 192/300, Loss: 0.0770 | 0.0330
Epoch 193/300, Loss: 0.0769 | 0.0331
Epoch 194/300, Loss: 0.0768 | 0.0332
Epoch 195/300, Loss: 0.0767 | 0.0333
Epoch 196/300, Loss: 0.0767 | 0.0332
Epoch 197/300, Loss: 0.0766 | 0.0331
Epoch 198/300, Loss: 0.0765 | 0.0331
Epoch 199/300, Loss: 0.0764 | 0.0333
Epoch 200/300, Loss: 0.0764 | 0.0334
Epoch 201/300, Loss: 0.0763 | 0.0333
Epoch 202/300, Loss: 0.0762 | 0.0332
Epoch 203/300, Loss: 0.0761 | 0.0333
Epoch 204/300, Loss: 0.0761 | 0.0334
Epoch 205/300, Loss: 0.0760 | 0.0335
Epoch 206/300, Loss: 0.0759 | 0.0334
Epoch 207/300, Loss: 0.0758 | 0.0334
Epoch 208/300, Loss: 0.0758 | 0.0334
Epoch 209/300, Loss: 0.0757 | 0.0335
Epoch 210/300, Loss: 0.0756 | 0.0337
Epoch 211/300, Loss: 0.0756 | 0.0336
Epoch 212/300, Loss: 0.0755 | 0.0336
Epoch 213/300, Loss: 0.0754 | 0.0335
Epoch 214/300, Loss: 0.0754 | 0.0336
Epoch 215/300, Loss: 0.0753 | 0.0338
Epoch 216/300, Loss: 0.0752 | 0.0339
Epoch 217/300, Loss: 0.0752 | 0.0338
Epoch 218/300, Loss: 0.0751 | 0.0337
Epoch 219/300, Loss: 0.0751 | 0.0337
Epoch 220/300, Loss: 0.0750 | 0.0338
Epoch 221/300, Loss: 0.0749 | 0.0340
Epoch 222/300, Loss: 0.0749 | 0.0341
Epoch 223/300, Loss: 0.0748 | 0.0341
Epoch 224/300, Loss: 0.0747 | 0.0340
Epoch 225/300, Loss: 0.0747 | 0.0339
Epoch 226/300, Loss: 0.0746 | 0.0340
Epoch 227/300, Loss: 0.0746 | 0.0341
Epoch 228/300, Loss: 0.0745 | 0.0343
Epoch 229/300, Loss: 0.0745 | 0.0344
Epoch 230/300, Loss: 0.0744 | 0.0343
Epoch 231/300, Loss: 0.0744 | 0.0342
Epoch 232/300, Loss: 0.0743 | 0.0341
Epoch 233/300, Loss: 0.0743 | 0.0342
Epoch 234/300, Loss: 0.0742 | 0.0344
Epoch 235/300, Loss: 0.0742 | 0.0347
Epoch 236/300, Loss: 0.0741 | 0.0348
Epoch 237/300, Loss: 0.0741 | 0.0347
Epoch 238/300, Loss: 0.0740 | 0.0344
Epoch 239/300, Loss: 0.0740 | 0.0342
Epoch 240/300, Loss: 0.0740 | 0.0342
Epoch 241/300, Loss: 0.0739 | 0.0345
Epoch 242/300, Loss: 0.0739 | 0.0350
Epoch 243/300, Loss: 0.0739 | 0.0354
Epoch 244/300, Loss: 0.0739 | 0.0354
Epoch 245/300, Loss: 0.0738 | 0.0349
Epoch 246/300, Loss: 0.0737 | 0.0343
Epoch 247/300, Loss: 0.0738 | 0.0341
Epoch 248/300, Loss: 0.0739 | 0.0342
Epoch 249/300, Loss: 0.0737 | 0.0350
Epoch 250/300, Loss: 0.0737 | 0.0362
Epoch 251/300, Loss: 0.0740 | 0.0366
Epoch 252/300, Loss: 0.0738 | 0.0355
Epoch 253/300, Loss: 0.0735 | 0.0343
Epoch 254/300, Loss: 0.0740 | 0.0341
Epoch 255/300, Loss: 0.0741 | 0.0345
Epoch 256/300, Loss: 0.0735 | 0.0358
Epoch 257/300, Loss: 0.0739 | 0.0368
Epoch 258/300, Loss: 0.0739 | 0.0356
Epoch 259/300, Loss: 0.0733 | 0.0345
Epoch 260/300, Loss: 0.0738 | 0.0346
Epoch 261/300, Loss: 0.0735 | 0.0352
Epoch 262/300, Loss: 0.0733 | 0.0360
Epoch 263/300, Loss: 0.0735 | 0.0355
Epoch 264/300, Loss: 0.0731 | 0.0348
Epoch 265/300, Loss: 0.0733 | 0.0349
Epoch 266/300, Loss: 0.0732 | 0.0354
Epoch 267/300, Loss: 0.0731 | 0.0358
Epoch 268/300, Loss: 0.0732 | 0.0353
Epoch 269/300, Loss: 0.0729 | 0.0349
Epoch 270/300, Loss: 0.0731 | 0.0351
Epoch 271/300, Loss: 0.0729 | 0.0355
Epoch 272/300, Loss: 0.0729 | 0.0357
Epoch 273/300, Loss: 0.0729 | 0.0353
Epoch 274/300, Loss: 0.0728 | 0.0351
Epoch 275/300, Loss: 0.0728 | 0.0352
Epoch 276/300, Loss: 0.0727 | 0.0355
Epoch 277/300, Loss: 0.0727 | 0.0356
Epoch 278/300, Loss: 0.0727 | 0.0353
Epoch 279/300, Loss: 0.0726 | 0.0352
Epoch 280/300, Loss: 0.0726 | 0.0353
Epoch 281/300, Loss: 0.0726 | 0.0355
Epoch 282/300, Loss: 0.0726 | 0.0356
Epoch 283/300, Loss: 0.0725 | 0.0353
Epoch 284/300, Loss: 0.0725 | 0.0352
Epoch 285/300, Loss: 0.0725 | 0.0353
Epoch 286/300, Loss: 0.0724 | 0.0355
Epoch 287/300, Loss: 0.0724 | 0.0355
Epoch 288/300, Loss: 0.0724 | 0.0354
Epoch 289/300, Loss: 0.0724 | 0.0353
Epoch 290/300, Loss: 0.0724 | 0.0354
Epoch 291/300, Loss: 0.0723 | 0.0355
Epoch 292/300, Loss: 0.0723 | 0.0355
Epoch 293/300, Loss: 0.0723 | 0.0354
Epoch 294/300, Loss: 0.0722 | 0.0353
Epoch 295/300, Loss: 0.0722 | 0.0354
Epoch 296/300, Loss: 0.0722 | 0.0355
Epoch 297/300, Loss: 0.0722 | 0.0355
Epoch 298/300, Loss: 0.0722 | 0.0355
Epoch 299/300, Loss: 0.0721 | 0.0354
Epoch 300/300, Loss: 0.0721 | 0.0354
Runtime (seconds): 110.57859921455383
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 299.09584997803904
RMSE: 17.294387817382812
MAE: 17.294387817382812
R-squared: nan
[204.43439]
