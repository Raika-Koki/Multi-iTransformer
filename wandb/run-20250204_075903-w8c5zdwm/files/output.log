[32m[I 2025-02-04 07:59:07,087][0m A new study created in memory with name: no-name-a1d0c69f-7256-4393-a1fd-111131e41513[0m
[32m[I 2025-02-04 07:59:32,844][0m Trial 0 finished with value: 1.0335334684699773 and parameters: {'observation_period_num': 237, 'train_rates': 0.6481316682692562, 'learning_rate': 1.9926264550484318e-06, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8270040058834992}. Best is trial 0 with value: 1.0335334684699773.[0m
[32m[I 2025-02-04 07:59:58,561][0m Trial 1 finished with value: 0.134118410046705 and parameters: {'observation_period_num': 136, 'train_rates': 0.8230490412669516, 'learning_rate': 0.0005522965981519788, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9582544376960063}. Best is trial 1 with value: 0.134118410046705.[0m
[32m[I 2025-02-04 08:03:41,084][0m Trial 2 finished with value: 0.159883262653803 and parameters: {'observation_period_num': 153, 'train_rates': 0.7524283773218088, 'learning_rate': 0.000324789219581328, 'batch_size': 21, 'step_size': 2, 'gamma': 0.887409028487528}. Best is trial 1 with value: 0.134118410046705.[0m
[32m[I 2025-02-04 08:04:27,784][0m Trial 3 finished with value: 0.541311542902674 and parameters: {'observation_period_num': 227, 'train_rates': 0.8775369179998103, 'learning_rate': 3.848670109878481e-06, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8669674845273007}. Best is trial 1 with value: 0.134118410046705.[0m
[32m[I 2025-02-04 08:04:58,105][0m Trial 4 finished with value: 0.07182331503715195 and parameters: {'observation_period_num': 7, 'train_rates': 0.6016722797729417, 'learning_rate': 0.0001723561613754149, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9280021889254815}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:05:21,473][0m Trial 5 finished with value: 1.3065524319989965 and parameters: {'observation_period_num': 216, 'train_rates': 0.8413398339830065, 'learning_rate': 1.3593937749165734e-06, 'batch_size': 233, 'step_size': 4, 'gamma': 0.8714225179550532}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:06:17,202][0m Trial 6 finished with value: 0.4079856354691808 and parameters: {'observation_period_num': 30, 'train_rates': 0.827311645945764, 'learning_rate': 2.4593495559782e-06, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9571802196531795}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:07:01,555][0m Trial 7 finished with value: 0.2614914165752234 and parameters: {'observation_period_num': 174, 'train_rates': 0.8490542813128358, 'learning_rate': 1.8480476093561512e-05, 'batch_size': 119, 'step_size': 8, 'gamma': 0.8257748071418374}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:07:53,554][0m Trial 8 finished with value: 0.3962558155710047 and parameters: {'observation_period_num': 137, 'train_rates': 0.6655258153533499, 'learning_rate': 1.1372354380987357e-05, 'batch_size': 86, 'step_size': 3, 'gamma': 0.8606860076514828}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:08:24,186][0m Trial 9 finished with value: 0.7342619101866535 and parameters: {'observation_period_num': 184, 'train_rates': 0.7834280802966538, 'learning_rate': 3.6059731367034293e-06, 'batch_size': 177, 'step_size': 7, 'gamma': 0.9376090440592949}. Best is trial 4 with value: 0.07182331503715195.[0m
[32m[I 2025-02-04 08:10:30,715][0m Trial 10 finished with value: 0.026299252174794674 and parameters: {'observation_period_num': 7, 'train_rates': 0.9683967487847965, 'learning_rate': 0.00010896495854977635, 'batch_size': 47, 'step_size': 14, 'gamma': 0.7539667111454588}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:13:06,511][0m Trial 11 finished with value: 0.028598591133400245 and parameters: {'observation_period_num': 12, 'train_rates': 0.9635214378386102, 'learning_rate': 0.00011573425257151812, 'batch_size': 38, 'step_size': 13, 'gamma': 0.7541461860562386}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:17:45,533][0m Trial 12 finished with value: 0.11193532609429799 and parameters: {'observation_period_num': 66, 'train_rates': 0.973900992862703, 'learning_rate': 7.22607064277623e-05, 'batch_size': 21, 'step_size': 14, 'gamma': 0.7502534994464465}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:19:24,717][0m Trial 13 finished with value: 0.09071466326713562 and parameters: {'observation_period_num': 83, 'train_rates': 0.977942615560044, 'learning_rate': 9.214069299395041e-05, 'batch_size': 61, 'step_size': 15, 'gamma': 0.7518363335074546}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:21:04,072][0m Trial 14 finished with value: 0.06683654329180717 and parameters: {'observation_period_num': 79, 'train_rates': 0.9134224061737064, 'learning_rate': 0.0009505976610754902, 'batch_size': 58, 'step_size': 12, 'gamma': 0.7912475077624306}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:22:44,282][0m Trial 15 finished with value: 0.05271370718255639 and parameters: {'observation_period_num': 37, 'train_rates': 0.9453565536661651, 'learning_rate': 6.544180180618579e-05, 'batch_size': 58, 'step_size': 11, 'gamma': 0.7870229860849003}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:25:07,924][0m Trial 16 finished with value: 0.04724357678013156 and parameters: {'observation_period_num': 5, 'train_rates': 0.9113242511467605, 'learning_rate': 3.2219625876595536e-05, 'batch_size': 40, 'step_size': 12, 'gamma': 0.7913558121059763}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:26:18,427][0m Trial 17 finished with value: 0.06787215343270546 and parameters: {'observation_period_num': 102, 'train_rates': 0.9319920158035833, 'learning_rate': 0.00018836568272077476, 'batch_size': 83, 'step_size': 14, 'gamma': 0.7726976452862225}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:27:00,461][0m Trial 18 finished with value: 0.09841085919811401 and parameters: {'observation_period_num': 44, 'train_rates': 0.8892499471633493, 'learning_rate': 3.4098149349743094e-05, 'batch_size': 143, 'step_size': 10, 'gamma': 0.825264803890509}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:28:20,242][0m Trial 19 finished with value: 0.18819691240787506 and parameters: {'observation_period_num': 54, 'train_rates': 0.9874875099308914, 'learning_rate': 9.127305908879512e-06, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8154757191904778}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:30:47,727][0m Trial 20 finished with value: 0.31025881552822376 and parameters: {'observation_period_num': 105, 'train_rates': 0.7356624093283475, 'learning_rate': 0.00018237403714758217, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9848221152791687}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:32:55,157][0m Trial 21 finished with value: 0.045598669155013 and parameters: {'observation_period_num': 6, 'train_rates': 0.9433819346886524, 'learning_rate': 4.0911027512459855e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.7848253240604032}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:35:05,861][0m Trial 22 finished with value: 0.046598196431825105 and parameters: {'observation_period_num': 19, 'train_rates': 0.940367526095967, 'learning_rate': 5.3481369086848187e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.7702761352829228}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:41:04,989][0m Trial 23 finished with value: 0.051225718987338686 and parameters: {'observation_period_num': 27, 'train_rates': 0.9539923302694366, 'learning_rate': 0.00012875466478697237, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7706161480761964}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:42:30,332][0m Trial 24 finished with value: 0.09438940126403987 and parameters: {'observation_period_num': 54, 'train_rates': 0.8859099529067407, 'learning_rate': 1.7716948417266398e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.7958577896898422}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:44:44,877][0m Trial 25 finished with value: 0.028409359471820224 and parameters: {'observation_period_num': 5, 'train_rates': 0.9638768220405558, 'learning_rate': 0.0003367258641626614, 'batch_size': 44, 'step_size': 13, 'gamma': 0.752542731319484}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:45:47,955][0m Trial 26 finished with value: 0.04979831352829933 and parameters: {'observation_period_num': 25, 'train_rates': 0.9886703164999272, 'learning_rate': 0.00033609828443170503, 'batch_size': 97, 'step_size': 14, 'gamma': 0.7586459185195715}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:48:25,911][0m Trial 27 finished with value: 0.11927230591555277 and parameters: {'observation_period_num': 102, 'train_rates': 0.9134281206449248, 'learning_rate': 0.00034723831367198925, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8060052322356462}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:49:16,869][0m Trial 28 finished with value: 0.10431734449574219 and parameters: {'observation_period_num': 64, 'train_rates': 0.8601299306306496, 'learning_rate': 0.0009392811324671496, 'batch_size': 113, 'step_size': 13, 'gamma': 0.8420072646459362}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:49:45,654][0m Trial 29 finished with value: 0.07696929491086774 and parameters: {'observation_period_num': 40, 'train_rates': 0.8004902342070512, 'learning_rate': 0.00012106138650842475, 'batch_size': 195, 'step_size': 9, 'gamma': 0.7706483928236711}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:50:16,680][0m Trial 30 finished with value: 0.05358568854797422 and parameters: {'observation_period_num': 20, 'train_rates': 0.697848409307277, 'learning_rate': 0.0005968613468244638, 'batch_size': 169, 'step_size': 11, 'gamma': 0.9008579120303495}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:52:16,133][0m Trial 31 finished with value: 0.047884588716206725 and parameters: {'observation_period_num': 7, 'train_rates': 0.9545101752313871, 'learning_rate': 4.6956212432635877e-05, 'batch_size': 50, 'step_size': 12, 'gamma': 0.7768324292858937}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:53:38,409][0m Trial 32 finished with value: 0.03079581788123244 and parameters: {'observation_period_num': 6, 'train_rates': 0.9660740742023998, 'learning_rate': 0.0002220707009587957, 'batch_size': 74, 'step_size': 13, 'gamma': 0.7502707373149343}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:54:56,587][0m Trial 33 finished with value: 0.1255015359849346 and parameters: {'observation_period_num': 251, 'train_rates': 0.9640076074212226, 'learning_rate': 0.00026134411282987174, 'batch_size': 71, 'step_size': 14, 'gamma': 0.7608101033232991}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:57:44,268][0m Trial 34 finished with value: 0.06985492713567687 and parameters: {'observation_period_num': 45, 'train_rates': 0.9159274937949994, 'learning_rate': 0.0005269401202587948, 'batch_size': 34, 'step_size': 13, 'gamma': 0.7534742836248575}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 08:58:44,866][0m Trial 35 finished with value: 0.04632934040966488 and parameters: {'observation_period_num': 19, 'train_rates': 0.8930505526280381, 'learning_rate': 9.944338130635371e-05, 'batch_size': 95, 'step_size': 10, 'gamma': 0.8063561930887657}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:02:23,927][0m Trial 36 finished with value: 0.047147625881545946 and parameters: {'observation_period_num': 18, 'train_rates': 0.9640159165092905, 'learning_rate': 0.0002436120259424537, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8476382469678034}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:03:45,360][0m Trial 37 finished with value: 0.04151323155144562 and parameters: {'observation_period_num': 34, 'train_rates': 0.9294149481957312, 'learning_rate': 0.0004077210706998501, 'batch_size': 74, 'step_size': 9, 'gamma': 0.7636026718160447}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:04:32,136][0m Trial 38 finished with value: 0.045855973347944695 and parameters: {'observation_period_num': 6, 'train_rates': 0.8690550272509613, 'learning_rate': 0.00014230650258249708, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8998506652510616}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:04:57,515][0m Trial 39 finished with value: 0.08313952443096465 and parameters: {'observation_period_num': 80, 'train_rates': 0.8998419167651889, 'learning_rate': 0.000572941704039135, 'batch_size': 244, 'step_size': 14, 'gamma': 0.8020252919945728}. Best is trial 10 with value: 0.026299252174794674.[0m
Early stopping at epoch 55
[32m[I 2025-02-04 09:06:02,825][0m Trial 40 finished with value: 0.14259637548373297 and parameters: {'observation_period_num': 57, 'train_rates': 0.9687175412105327, 'learning_rate': 0.0002278840578107438, 'batch_size': 51, 'step_size': 1, 'gamma': 0.7787115478736303}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:07:15,092][0m Trial 41 finished with value: 0.04232643869253668 and parameters: {'observation_period_num': 33, 'train_rates': 0.931367284714118, 'learning_rate': 0.00043011943052546746, 'batch_size': 82, 'step_size': 6, 'gamma': 0.7629734657323561}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:08:37,359][0m Trial 42 finished with value: 0.04019108179368471 and parameters: {'observation_period_num': 31, 'train_rates': 0.9290370520096645, 'learning_rate': 0.0004103941580136429, 'batch_size': 71, 'step_size': 8, 'gamma': 0.7631355878981568}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:10:13,072][0m Trial 43 finished with value: 0.052401184733655 and parameters: {'observation_period_num': 17, 'train_rates': 0.9751796685605023, 'learning_rate': 8.149099348174199e-05, 'batch_size': 65, 'step_size': 8, 'gamma': 0.7518234884155826}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:11:06,827][0m Trial 44 finished with value: 0.13261529803276062 and parameters: {'observation_period_num': 192, 'train_rates': 0.9877610979508531, 'learning_rate': 0.0007464971480265632, 'batch_size': 110, 'step_size': 7, 'gamma': 0.785017283202359}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:15:32,068][0m Trial 45 finished with value: 0.05775692204024582 and parameters: {'observation_period_num': 14, 'train_rates': 0.6024746877854729, 'learning_rate': 0.00029220694661182336, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7642861092495196}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:16:35,090][0m Trial 46 finished with value: 0.08322888538241387 and parameters: {'observation_period_num': 152, 'train_rates': 0.9575192981251099, 'learning_rate': 0.00015139764074137324, 'batch_size': 92, 'step_size': 11, 'gamma': 0.7780159599463886}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:18:20,810][0m Trial 47 finished with value: 0.04926187577380982 and parameters: {'observation_period_num': 32, 'train_rates': 0.925282540092462, 'learning_rate': 0.00019648737981923294, 'batch_size': 55, 'step_size': 4, 'gamma': 0.7511092470240516}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:21:35,684][0m Trial 48 finished with value: 0.06009764267662211 and parameters: {'observation_period_num': 48, 'train_rates': 0.8351151587652579, 'learning_rate': 6.671517176888671e-05, 'batch_size': 27, 'step_size': 5, 'gamma': 0.7964835574606852}. Best is trial 10 with value: 0.026299252174794674.[0m
[32m[I 2025-02-04 09:23:49,816][0m Trial 49 finished with value: 0.04722289493145652 and parameters: {'observation_period_num': 30, 'train_rates': 0.8157915979899637, 'learning_rate': 0.0001064270947955015, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8188151073017584}. Best is trial 10 with value: 0.026299252174794674.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2472 | 0.1572
Epoch 2/300, Loss: 0.1126 | 0.1057
Epoch 3/300, Loss: 0.0932 | 0.0831
Epoch 4/300, Loss: 0.0823 | 0.0741
Epoch 5/300, Loss: 0.0752 | 0.0710
Epoch 6/300, Loss: 0.0718 | 0.0708
Epoch 7/300, Loss: 0.0710 | 0.0690
Epoch 8/300, Loss: 0.0718 | 0.0795
Epoch 9/300, Loss: 0.0740 | 0.0637
Epoch 10/300, Loss: 0.0665 | 0.0622
Epoch 11/300, Loss: 0.0623 | 0.0618
Epoch 12/300, Loss: 0.0603 | 0.0614
Epoch 13/300, Loss: 0.0588 | 0.0607
Epoch 14/300, Loss: 0.0577 | 0.0602
Epoch 15/300, Loss: 0.0565 | 0.0592
Epoch 16/300, Loss: 0.0559 | 0.0587
Epoch 17/300, Loss: 0.0550 | 0.0581
Epoch 18/300, Loss: 0.0542 | 0.0575
Epoch 19/300, Loss: 0.0532 | 0.0567
Epoch 20/300, Loss: 0.0523 | 0.0560
Epoch 21/300, Loss: 0.0517 | 0.0553
Epoch 22/300, Loss: 0.0506 | 0.0565
Epoch 23/300, Loss: 0.0501 | 0.0556
Epoch 24/300, Loss: 0.0499 | 0.0552
Epoch 25/300, Loss: 0.0492 | 0.0550
Epoch 26/300, Loss: 0.0494 | 0.0546
Epoch 27/300, Loss: 0.0496 | 0.0544
Epoch 28/300, Loss: 0.0499 | 0.0542
Epoch 29/300, Loss: 0.0492 | 0.0523
Epoch 30/300, Loss: 0.0485 | 0.0577
Epoch 31/300, Loss: 0.0447 | 0.0508
Epoch 32/300, Loss: 0.0437 | 0.0504
Epoch 33/300, Loss: 0.0426 | 0.0486
Epoch 34/300, Loss: 0.0421 | 0.0481
Epoch 35/300, Loss: 0.0417 | 0.0474
Epoch 36/300, Loss: 0.0414 | 0.0467
Epoch 37/300, Loss: 0.0415 | 0.0466
Epoch 38/300, Loss: 0.0413 | 0.0463
Epoch 39/300, Loss: 0.0412 | 0.0461
Epoch 40/300, Loss: 0.0410 | 0.0458
Epoch 41/300, Loss: 0.0410 | 0.0456
Epoch 42/300, Loss: 0.0409 | 0.0453
Epoch 43/300, Loss: 0.0410 | 0.0448
Epoch 44/300, Loss: 0.0414 | 0.0449
Epoch 45/300, Loss: 0.0413 | 0.0448
Epoch 46/300, Loss: 0.0414 | 0.0446
Epoch 47/300, Loss: 0.0413 | 0.0445
Epoch 48/300, Loss: 0.0413 | 0.0443
Epoch 49/300, Loss: 0.0412 | 0.0442
Epoch 50/300, Loss: 0.0414 | 0.0457
Epoch 51/300, Loss: 0.0411 | 0.0459
Epoch 52/300, Loss: 0.0405 | 0.0457
Epoch 53/300, Loss: 0.0400 | 0.0454
Epoch 54/300, Loss: 0.0396 | 0.0450
Epoch 55/300, Loss: 0.0391 | 0.0445
Epoch 56/300, Loss: 0.0387 | 0.0441
Epoch 57/300, Loss: 0.0386 | 0.0433
Epoch 58/300, Loss: 0.0384 | 0.0426
Epoch 59/300, Loss: 0.0382 | 0.0425
Epoch 60/300, Loss: 0.0382 | 0.0425
Epoch 61/300, Loss: 0.0382 | 0.0427
Epoch 62/300, Loss: 0.0382 | 0.0428
Epoch 63/300, Loss: 0.0383 | 0.0430
Epoch 64/300, Loss: 0.0384 | 0.0443
Epoch 65/300, Loss: 0.0384 | 0.0437
Epoch 66/300, Loss: 0.0380 | 0.0432
Epoch 67/300, Loss: 0.0377 | 0.0428
Epoch 68/300, Loss: 0.0375 | 0.0426
Epoch 69/300, Loss: 0.0373 | 0.0424
Epoch 70/300, Loss: 0.0372 | 0.0422
Epoch 71/300, Loss: 0.0372 | 0.0423
Epoch 72/300, Loss: 0.0371 | 0.0419
Epoch 73/300, Loss: 0.0368 | 0.0417
Epoch 74/300, Loss: 0.0366 | 0.0415
Epoch 75/300, Loss: 0.0365 | 0.0413
Epoch 76/300, Loss: 0.0363 | 0.0412
Epoch 77/300, Loss: 0.0362 | 0.0410
Epoch 78/300, Loss: 0.0362 | 0.0410
Epoch 79/300, Loss: 0.0361 | 0.0408
Epoch 80/300, Loss: 0.0359 | 0.0406
Epoch 81/300, Loss: 0.0358 | 0.0405
Epoch 82/300, Loss: 0.0358 | 0.0404
Epoch 83/300, Loss: 0.0357 | 0.0403
Epoch 84/300, Loss: 0.0356 | 0.0402
Epoch 85/300, Loss: 0.0356 | 0.0401
Epoch 86/300, Loss: 0.0355 | 0.0400
Epoch 87/300, Loss: 0.0354 | 0.0399
Epoch 88/300, Loss: 0.0354 | 0.0399
Epoch 89/300, Loss: 0.0353 | 0.0398
Epoch 90/300, Loss: 0.0353 | 0.0398
Epoch 91/300, Loss: 0.0352 | 0.0397
Epoch 92/300, Loss: 0.0352 | 0.0396
Epoch 93/300, Loss: 0.0351 | 0.0395
Epoch 94/300, Loss: 0.0351 | 0.0395
Epoch 95/300, Loss: 0.0350 | 0.0394
Epoch 96/300, Loss: 0.0350 | 0.0394
Epoch 97/300, Loss: 0.0350 | 0.0393
Epoch 98/300, Loss: 0.0349 | 0.0393
Epoch 99/300, Loss: 0.0349 | 0.0392
Epoch 100/300, Loss: 0.0348 | 0.0391
Epoch 101/300, Loss: 0.0348 | 0.0391
Epoch 102/300, Loss: 0.0348 | 0.0391
Epoch 103/300, Loss: 0.0347 | 0.0390
Epoch 104/300, Loss: 0.0347 | 0.0390
Epoch 105/300, Loss: 0.0347 | 0.0390
Epoch 106/300, Loss: 0.0346 | 0.0389
Epoch 107/300, Loss: 0.0346 | 0.0389
Epoch 108/300, Loss: 0.0346 | 0.0388
Epoch 109/300, Loss: 0.0346 | 0.0388
Epoch 110/300, Loss: 0.0346 | 0.0388
Epoch 111/300, Loss: 0.0345 | 0.0388
Epoch 112/300, Loss: 0.0345 | 0.0387
Epoch 113/300, Loss: 0.0345 | 0.0387
Epoch 114/300, Loss: 0.0345 | 0.0387
Epoch 115/300, Loss: 0.0344 | 0.0386
Epoch 116/300, Loss: 0.0344 | 0.0386
Epoch 117/300, Loss: 0.0344 | 0.0386
Epoch 118/300, Loss: 0.0344 | 0.0386
Epoch 119/300, Loss: 0.0344 | 0.0385
Epoch 120/300, Loss: 0.0343 | 0.0385
Epoch 121/300, Loss: 0.0343 | 0.0385
Epoch 122/300, Loss: 0.0343 | 0.0385
Epoch 123/300, Loss: 0.0343 | 0.0385
Epoch 124/300, Loss: 0.0343 | 0.0384
Epoch 125/300, Loss: 0.0343 | 0.0384
Epoch 126/300, Loss: 0.0343 | 0.0384
Epoch 127/300, Loss: 0.0343 | 0.0384
Epoch 128/300, Loss: 0.0342 | 0.0384
Epoch 129/300, Loss: 0.0342 | 0.0384
Epoch 130/300, Loss: 0.0342 | 0.0383
Epoch 131/300, Loss: 0.0342 | 0.0383
Epoch 132/300, Loss: 0.0342 | 0.0383
Epoch 133/300, Loss: 0.0342 | 0.0383
Epoch 134/300, Loss: 0.0342 | 0.0383
Epoch 135/300, Loss: 0.0342 | 0.0383
Epoch 136/300, Loss: 0.0342 | 0.0383
Epoch 137/300, Loss: 0.0341 | 0.0383
Epoch 138/300, Loss: 0.0341 | 0.0382
Epoch 139/300, Loss: 0.0341 | 0.0382
Epoch 140/300, Loss: 0.0341 | 0.0382
Epoch 141/300, Loss: 0.0341 | 0.0382
Epoch 142/300, Loss: 0.0341 | 0.0382
Epoch 143/300, Loss: 0.0341 | 0.0382
Epoch 144/300, Loss: 0.0341 | 0.0382
Epoch 145/300, Loss: 0.0341 | 0.0382
Epoch 146/300, Loss: 0.0341 | 0.0382
Epoch 147/300, Loss: 0.0341 | 0.0382
Epoch 148/300, Loss: 0.0341 | 0.0382
Epoch 149/300, Loss: 0.0341 | 0.0381
Epoch 150/300, Loss: 0.0341 | 0.0381
Epoch 151/300, Loss: 0.0341 | 0.0381
Epoch 152/300, Loss: 0.0340 | 0.0381
Epoch 153/300, Loss: 0.0340 | 0.0381
Epoch 154/300, Loss: 0.0340 | 0.0381
Epoch 155/300, Loss: 0.0340 | 0.0381
Epoch 156/300, Loss: 0.0340 | 0.0381
Epoch 157/300, Loss: 0.0340 | 0.0381
Epoch 158/300, Loss: 0.0340 | 0.0381
Epoch 159/300, Loss: 0.0340 | 0.0381
Epoch 160/300, Loss: 0.0340 | 0.0381
Epoch 161/300, Loss: 0.0340 | 0.0381
Epoch 162/300, Loss: 0.0340 | 0.0381
Epoch 163/300, Loss: 0.0340 | 0.0381
Epoch 164/300, Loss: 0.0340 | 0.0381
Epoch 165/300, Loss: 0.0340 | 0.0381
Epoch 166/300, Loss: 0.0340 | 0.0381
Epoch 167/300, Loss: 0.0340 | 0.0381
Epoch 168/300, Loss: 0.0340 | 0.0381
Epoch 169/300, Loss: 0.0340 | 0.0381
Epoch 170/300, Loss: 0.0340 | 0.0380
Epoch 171/300, Loss: 0.0340 | 0.0380
Epoch 172/300, Loss: 0.0340 | 0.0380
Epoch 173/300, Loss: 0.0340 | 0.0380
Epoch 174/300, Loss: 0.0340 | 0.0380
Epoch 175/300, Loss: 0.0340 | 0.0380
Epoch 176/300, Loss: 0.0340 | 0.0380
Epoch 177/300, Loss: 0.0340 | 0.0380
Epoch 178/300, Loss: 0.0340 | 0.0380
Epoch 179/300, Loss: 0.0340 | 0.0380
Epoch 180/300, Loss: 0.0340 | 0.0380
Epoch 181/300, Loss: 0.0340 | 0.0380
Epoch 182/300, Loss: 0.0340 | 0.0380
Epoch 183/300, Loss: 0.0340 | 0.0380
Epoch 184/300, Loss: 0.0340 | 0.0380
Epoch 185/300, Loss: 0.0340 | 0.0380
Epoch 186/300, Loss: 0.0340 | 0.0380
Epoch 187/300, Loss: 0.0340 | 0.0380
Epoch 188/300, Loss: 0.0340 | 0.0380
Epoch 189/300, Loss: 0.0340 | 0.0380
Epoch 190/300, Loss: 0.0339 | 0.0380
Epoch 191/300, Loss: 0.0339 | 0.0380
Epoch 192/300, Loss: 0.0339 | 0.0380
Epoch 193/300, Loss: 0.0339 | 0.0380
Epoch 194/300, Loss: 0.0339 | 0.0380
Epoch 195/300, Loss: 0.0339 | 0.0380
Epoch 196/300, Loss: 0.0339 | 0.0380
Epoch 197/300, Loss: 0.0339 | 0.0380
Epoch 198/300, Loss: 0.0339 | 0.0380
Epoch 199/300, Loss: 0.0339 | 0.0380
Epoch 200/300, Loss: 0.0339 | 0.0380
Epoch 201/300, Loss: 0.0339 | 0.0380
Epoch 202/300, Loss: 0.0339 | 0.0380
Epoch 203/300, Loss: 0.0339 | 0.0380
Epoch 204/300, Loss: 0.0339 | 0.0380
Epoch 205/300, Loss: 0.0339 | 0.0380
Epoch 206/300, Loss: 0.0339 | 0.0380
Epoch 207/300, Loss: 0.0339 | 0.0380
Epoch 208/300, Loss: 0.0339 | 0.0380
Epoch 209/300, Loss: 0.0339 | 0.0380
Epoch 210/300, Loss: 0.0339 | 0.0380
Epoch 211/300, Loss: 0.0339 | 0.0380
Epoch 212/300, Loss: 0.0339 | 0.0380
Epoch 213/300, Loss: 0.0339 | 0.0380
Epoch 214/300, Loss: 0.0339 | 0.0380
Epoch 215/300, Loss: 0.0339 | 0.0380
Epoch 216/300, Loss: 0.0339 | 0.0380
Epoch 217/300, Loss: 0.0339 | 0.0380
Epoch 218/300, Loss: 0.0339 | 0.0380
Epoch 219/300, Loss: 0.0339 | 0.0380
Epoch 220/300, Loss: 0.0339 | 0.0380
Epoch 221/300, Loss: 0.0339 | 0.0380
Epoch 222/300, Loss: 0.0339 | 0.0380
Epoch 223/300, Loss: 0.0339 | 0.0380
Epoch 224/300, Loss: 0.0339 | 0.0380
Epoch 225/300, Loss: 0.0339 | 0.0380
Epoch 226/300, Loss: 0.0339 | 0.0380
Epoch 227/300, Loss: 0.0339 | 0.0380
Epoch 228/300, Loss: 0.0339 | 0.0380
Epoch 229/300, Loss: 0.0339 | 0.0380
Epoch 230/300, Loss: 0.0339 | 0.0380
Epoch 231/300, Loss: 0.0339 | 0.0380
Epoch 232/300, Loss: 0.0339 | 0.0380
Epoch 233/300, Loss: 0.0339 | 0.0380
Epoch 234/300, Loss: 0.0339 | 0.0380
Epoch 235/300, Loss: 0.0339 | 0.0380
Epoch 236/300, Loss: 0.0339 | 0.0380
Epoch 237/300, Loss: 0.0339 | 0.0380
Epoch 238/300, Loss: 0.0339 | 0.0380
Epoch 239/300, Loss: 0.0339 | 0.0380
Epoch 240/300, Loss: 0.0339 | 0.0380
Epoch 241/300, Loss: 0.0339 | 0.0380
Epoch 242/300, Loss: 0.0339 | 0.0380
Epoch 243/300, Loss: 0.0339 | 0.0380
Epoch 244/300, Loss: 0.0339 | 0.0380
Epoch 245/300, Loss: 0.0339 | 0.0380
Epoch 246/300, Loss: 0.0339 | 0.0380
Epoch 247/300, Loss: 0.0339 | 0.0380
Epoch 248/300, Loss: 0.0339 | 0.0380
Epoch 249/300, Loss: 0.0339 | 0.0380
Epoch 250/300, Loss: 0.0339 | 0.0380
Epoch 251/300, Loss: 0.0339 | 0.0380
Epoch 252/300, Loss: 0.0339 | 0.0380
Epoch 253/300, Loss: 0.0339 | 0.0380
Epoch 254/300, Loss: 0.0339 | 0.0380
Epoch 255/300, Loss: 0.0339 | 0.0380
Epoch 256/300, Loss: 0.0339 | 0.0380
Early stopping
Runtime (seconds): 324.7610273361206
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 2.7644592943252064
RMSE: 1.6626663208007812
MAE: 1.6626663208007812
R-squared: nan
[113.482666]
