[32m[I 2025-02-06 00:51:36,396][0m A new study created in memory with name: no-name-3749f4dd-11b9-45d7-8e9a-f695fa982654[0m
[32m[I 2025-02-06 00:52:00,911][0m Trial 0 finished with value: 0.18260825244084555 and parameters: {'observation_period_num': 179, 'train_rates': 0.7989758661303605, 'learning_rate': 0.0003106971266687144, 'batch_size': 239, 'step_size': 7, 'gamma': 0.8139692198827788}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:52:26,134][0m Trial 1 finished with value: 0.5535795156351864 and parameters: {'observation_period_num': 116, 'train_rates': 0.6801912611078256, 'learning_rate': 1.415245164418027e-05, 'batch_size': 212, 'step_size': 5, 'gamma': 0.9275913702531502}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:52:49,750][0m Trial 2 finished with value: 0.20128690705926885 and parameters: {'observation_period_num': 40, 'train_rates': 0.768029660102686, 'learning_rate': 0.00020661858227769497, 'batch_size': 235, 'step_size': 14, 'gamma': 0.8165862399305944}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:53:28,961][0m Trial 3 finished with value: 0.26026253161184926 and parameters: {'observation_period_num': 148, 'train_rates': 0.7719204144058233, 'learning_rate': 0.00019698969191684276, 'batch_size': 131, 'step_size': 3, 'gamma': 0.9060630087030305}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:54:37,980][0m Trial 4 finished with value: 0.20241924835799133 and parameters: {'observation_period_num': 142, 'train_rates': 0.6628371519357417, 'learning_rate': 0.00033402507694656745, 'batch_size': 67, 'step_size': 8, 'gamma': 0.8471807942343558}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:55:55,699][0m Trial 5 finished with value: 0.3199853524196856 and parameters: {'observation_period_num': 242, 'train_rates': 0.852457367612537, 'learning_rate': 5.367828198897154e-05, 'batch_size': 67, 'step_size': 4, 'gamma': 0.752448075955209}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:56:40,166][0m Trial 6 finished with value: 0.25073437770297624 and parameters: {'observation_period_num': 98, 'train_rates': 0.6165283924177289, 'learning_rate': 0.0002676142607661151, 'batch_size': 100, 'step_size': 12, 'gamma': 0.881989280347212}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:58:08,330][0m Trial 7 finished with value: 0.199078446475126 and parameters: {'observation_period_num': 50, 'train_rates': 0.6977961206646397, 'learning_rate': 7.94105456384954e-05, 'batch_size': 54, 'step_size': 10, 'gamma': 0.7787075915302436}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 00:58:31,363][0m Trial 8 finished with value: 0.2682409015195123 and parameters: {'observation_period_num': 247, 'train_rates': 0.6283829640026551, 'learning_rate': 0.00018760061988152944, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9259480344872575}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 01:02:46,899][0m Trial 9 finished with value: 0.5880230932101982 and parameters: {'observation_period_num': 212, 'train_rates': 0.6263065365889895, 'learning_rate': 2.0020337573962007e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9286299568137055}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 01:03:23,805][0m Trial 10 finished with value: 0.6550365686416626 and parameters: {'observation_period_num': 189, 'train_rates': 0.9804436163569243, 'learning_rate': 5.544222867668781e-06, 'batch_size': 168, 'step_size': 1, 'gamma': 0.9851354881435336}. Best is trial 0 with value: 0.18260825244084555.[0m
[32m[I 2025-02-06 01:07:41,391][0m Trial 11 finished with value: 0.04394057724857703 and parameters: {'observation_period_num': 23, 'train_rates': 0.8485634851691757, 'learning_rate': 0.000980734161466091, 'batch_size': 21, 'step_size': 9, 'gamma': 0.7824220535315762}. Best is trial 11 with value: 0.04394057724857703.[0m
[32m[I 2025-02-06 01:08:20,135][0m Trial 12 finished with value: 0.031546093788243794 and parameters: {'observation_period_num': 12, 'train_rates': 0.8673682539748044, 'learning_rate': 0.0009964093519883263, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8144962892494206}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:09:02,848][0m Trial 13 finished with value: 0.03215715711490369 and parameters: {'observation_period_num': 10, 'train_rates': 0.9043806770243065, 'learning_rate': 0.0008380073527721943, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8057848097120681}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:09:46,095][0m Trial 14 finished with value: 0.62813401222229 and parameters: {'observation_period_num': 5, 'train_rates': 0.9519371033032681, 'learning_rate': 1.3133243954964154e-06, 'batch_size': 146, 'step_size': 11, 'gamma': 0.8426589861017454}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:10:21,437][0m Trial 15 finished with value: 0.08165477112282155 and parameters: {'observation_period_num': 79, 'train_rates': 0.9110449243917846, 'learning_rate': 0.0009426194569428619, 'batch_size': 171, 'step_size': 6, 'gamma': 0.8080248442580749}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:11:11,865][0m Trial 16 finished with value: 0.1059915475218266 and parameters: {'observation_period_num': 64, 'train_rates': 0.8914122470209744, 'learning_rate': 0.0008675864546309965, 'batch_size': 113, 'step_size': 9, 'gamma': 0.8531623201117298}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:11:47,830][0m Trial 17 finished with value: 0.07079122430086136 and parameters: {'observation_period_num': 5, 'train_rates': 0.9159807309329187, 'learning_rate': 6.579060809286598e-05, 'batch_size': 172, 'step_size': 7, 'gamma': 0.7548709295226219}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:12:43,452][0m Trial 18 finished with value: 0.120176411924556 and parameters: {'observation_period_num': 89, 'train_rates': 0.8447355099364104, 'learning_rate': 0.0005182196305122702, 'batch_size': 101, 'step_size': 11, 'gamma': 0.7954212763193682}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:13:21,098][0m Trial 19 finished with value: 0.12385764682133282 and parameters: {'observation_period_num': 37, 'train_rates': 0.8097385846055024, 'learning_rate': 9.572374998484652e-05, 'batch_size': 149, 'step_size': 2, 'gamma': 0.8765239609970052}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:13:47,929][0m Trial 20 finished with value: 0.5424704142913964 and parameters: {'observation_period_num': 67, 'train_rates': 0.7311086175321722, 'learning_rate': 5.196286834077398e-06, 'batch_size': 204, 'step_size': 15, 'gamma': 0.8314379664505456}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:14:34,979][0m Trial 21 finished with value: 0.03350468852767979 and parameters: {'observation_period_num': 17, 'train_rates': 0.8613565441986921, 'learning_rate': 0.0005688711967604764, 'batch_size': 125, 'step_size': 8, 'gamma': 0.7811038117097446}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:15:23,012][0m Trial 22 finished with value: 0.03867710986836797 and parameters: {'observation_period_num': 21, 'train_rates': 0.8832750761265454, 'learning_rate': 0.0006186531657535715, 'batch_size': 124, 'step_size': 8, 'gamma': 0.7771800566611684}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:16:02,476][0m Trial 23 finished with value: 0.05395035018377444 and parameters: {'observation_period_num': 28, 'train_rates': 0.9425723763504342, 'learning_rate': 0.0005034235878681753, 'batch_size': 159, 'step_size': 6, 'gamma': 0.7986655534059284}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:16:55,199][0m Trial 24 finished with value: 0.062059199594353374 and parameters: {'observation_period_num': 59, 'train_rates': 0.8699886907996258, 'learning_rate': 0.00011651129509889165, 'batch_size': 107, 'step_size': 10, 'gamma': 0.8270422472785542}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:17:27,233][0m Trial 25 finished with value: 0.04040835484078056 and parameters: {'observation_period_num': 13, 'train_rates': 0.820789683969303, 'learning_rate': 0.0005767121031742326, 'batch_size': 186, 'step_size': 8, 'gamma': 0.7692565297212379}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:18:10,528][0m Trial 26 finished with value: 0.0624730254833897 and parameters: {'observation_period_num': 45, 'train_rates': 0.9182439577676502, 'learning_rate': 0.0003578757935194848, 'batch_size': 140, 'step_size': 5, 'gamma': 0.79459927026407}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:19:21,856][0m Trial 27 finished with value: 0.17199880862964018 and parameters: {'observation_period_num': 111, 'train_rates': 0.9543360969899919, 'learning_rate': 3.244769707806172e-05, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8681477288030623}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:20:06,733][0m Trial 28 finished with value: 0.055201256872939516 and parameters: {'observation_period_num': 28, 'train_rates': 0.823430860153798, 'learning_rate': 0.0001424893195328305, 'batch_size': 123, 'step_size': 7, 'gamma': 0.765531659552976}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:20:35,388][0m Trial 29 finished with value: 0.1332283467054367 and parameters: {'observation_period_num': 71, 'train_rates': 0.9852745188607054, 'learning_rate': 0.0003474478822002083, 'batch_size': 237, 'step_size': 6, 'gamma': 0.8199958515853404}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:21:06,064][0m Trial 30 finished with value: 0.19628993106051917 and parameters: {'observation_period_num': 49, 'train_rates': 0.7760681972280296, 'learning_rate': 0.0006135703560558042, 'batch_size': 188, 'step_size': 9, 'gamma': 0.8012754616901122}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:21:50,501][0m Trial 31 finished with value: 0.036308410623278775 and parameters: {'observation_period_num': 19, 'train_rates': 0.8763387158630224, 'learning_rate': 0.0006105483297756554, 'batch_size': 134, 'step_size': 8, 'gamma': 0.7828921322369841}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:22:28,792][0m Trial 32 finished with value: 0.040096677181756364 and parameters: {'observation_period_num': 21, 'train_rates': 0.8755910640306491, 'learning_rate': 0.0009747240641468783, 'batch_size': 155, 'step_size': 8, 'gamma': 0.7868032338852223}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:23:13,240][0m Trial 33 finished with value: 0.04744422343575348 and parameters: {'observation_period_num': 35, 'train_rates': 0.8998836588253729, 'learning_rate': 0.0004266549252565267, 'batch_size': 137, 'step_size': 5, 'gamma': 0.8097739219465175}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:24:14,202][0m Trial 34 finished with value: 0.0675912494710707 and parameters: {'observation_period_num': 5, 'train_rates': 0.7930674345492186, 'learning_rate': 0.00023590443959762043, 'batch_size': 90, 'step_size': 7, 'gamma': 0.835555793789125}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:24:43,629][0m Trial 35 finished with value: 0.0689690113067627 and parameters: {'observation_period_num': 42, 'train_rates': 0.9365404799819282, 'learning_rate': 0.0006951202153781978, 'batch_size': 222, 'step_size': 11, 'gamma': 0.8161458571778833}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:25:07,246][0m Trial 36 finished with value: 0.1645495871964254 and parameters: {'observation_period_num': 161, 'train_rates': 0.8586723490404744, 'learning_rate': 0.00016263037375887688, 'batch_size': 251, 'step_size': 9, 'gamma': 0.7639412279341995}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:25:53,893][0m Trial 37 finished with value: 0.10202783180974803 and parameters: {'observation_period_num': 129, 'train_rates': 0.8420079264649799, 'learning_rate': 0.0002838457067928209, 'batch_size': 119, 'step_size': 4, 'gamma': 0.8543048602527388}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:26:37,265][0m Trial 38 finished with value: 0.05509898028008744 and parameters: {'observation_period_num': 55, 'train_rates': 0.828214584993382, 'learning_rate': 0.0004544346935345661, 'batch_size': 130, 'step_size': 12, 'gamma': 0.782620146132366}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:27:05,779][0m Trial 39 finished with value: 0.2619964430920589 and parameters: {'observation_period_num': 83, 'train_rates': 0.7308283015309379, 'learning_rate': 0.00023782812568516512, 'batch_size': 181, 'step_size': 4, 'gamma': 0.754018693889133}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:27:45,643][0m Trial 40 finished with value: 0.06980531802619568 and parameters: {'observation_period_num': 14, 'train_rates': 0.9284017573404638, 'learning_rate': 4.702075127498627e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8943640132640625}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:28:32,401][0m Trial 41 finished with value: 0.033211278640306915 and parameters: {'observation_period_num': 17, 'train_rates': 0.8903867745929163, 'learning_rate': 0.0006840854607375592, 'batch_size': 126, 'step_size': 8, 'gamma': 0.7799504328466873}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:29:16,448][0m Trial 42 finished with value: 0.04763750490304586 and parameters: {'observation_period_num': 34, 'train_rates': 0.8994715121544348, 'learning_rate': 0.0007229096114668438, 'batch_size': 136, 'step_size': 8, 'gamma': 0.7699578550136298}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:30:19,897][0m Trial 43 finished with value: 0.03242966378836528 and parameters: {'observation_period_num': 14, 'train_rates': 0.8606417037857318, 'learning_rate': 0.00038055899011987007, 'batch_size': 90, 'step_size': 10, 'gamma': 0.791490946853997}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:31:44,118][0m Trial 44 finished with value: 0.19480042602356706 and parameters: {'observation_period_num': 228, 'train_rates': 0.8594013853632366, 'learning_rate': 0.00035492536953792155, 'batch_size': 62, 'step_size': 10, 'gamma': 0.8200312232417261}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:32:44,640][0m Trial 45 finished with value: 0.03603782958549554 and parameters: {'observation_period_num': 14, 'train_rates': 0.7942879465774749, 'learning_rate': 0.0007429316870413618, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8053323157962474}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:33:52,729][0m Trial 46 finished with value: 0.07502448913596925 and parameters: {'observation_period_num': 31, 'train_rates': 0.8361767978181758, 'learning_rate': 1.0509249882582934e-05, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9793137089396204}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:36:41,741][0m Trial 47 finished with value: 0.13999184401868617 and parameters: {'observation_period_num': 50, 'train_rates': 0.8945846802268638, 'learning_rate': 0.00040826131302907693, 'batch_size': 33, 'step_size': 9, 'gamma': 0.7931099945227861}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:37:33,318][0m Trial 48 finished with value: 0.8503750565997715 and parameters: {'observation_period_num': 192, 'train_rates': 0.8715494038376446, 'learning_rate': 1.5069166846624826e-06, 'batch_size': 107, 'step_size': 13, 'gamma': 0.7725904446353289}. Best is trial 12 with value: 0.031546093788243794.[0m
[32m[I 2025-02-06 01:39:23,432][0m Trial 49 finished with value: 0.12663169018924236 and parameters: {'observation_period_num': 105, 'train_rates': 0.9639897366535469, 'learning_rate': 0.0001996005552967187, 'batch_size': 53, 'step_size': 6, 'gamma': 0.7603971513096855}. Best is trial 12 with value: 0.031546093788243794.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4891 | 0.2616
Epoch 2/300, Loss: 0.1815 | 0.2390
Epoch 3/300, Loss: 0.1520 | 0.1896
Epoch 4/300, Loss: 0.1257 | 0.1344
Epoch 5/300, Loss: 0.1424 | 0.1161
Epoch 6/300, Loss: 0.1208 | 0.0922
Epoch 7/300, Loss: 0.1327 | 0.1028
Epoch 8/300, Loss: 0.1252 | 0.1225
Epoch 9/300, Loss: 0.1450 | 0.2378
Epoch 10/300, Loss: 0.1591 | 0.3677
Epoch 11/300, Loss: 0.1278 | 0.0769
Epoch 12/300, Loss: 0.1160 | 0.0760
Epoch 13/300, Loss: 0.1127 | 0.1706
Epoch 14/300, Loss: 0.1072 | 0.0701
Epoch 15/300, Loss: 0.1024 | 0.1189
Epoch 16/300, Loss: 0.0978 | 0.0640
Epoch 17/300, Loss: 0.0944 | 0.0926
Epoch 18/300, Loss: 0.0917 | 0.0629
Epoch 19/300, Loss: 0.0918 | 0.0651
Epoch 20/300, Loss: 0.0911 | 0.0787
Epoch 21/300, Loss: 0.0912 | 0.0648
Epoch 22/300, Loss: 0.0959 | 0.0670
Epoch 23/300, Loss: 0.0978 | 0.0765
Epoch 24/300, Loss: 0.0953 | 0.0722
Epoch 25/300, Loss: 0.0884 | 0.0623
Epoch 26/300, Loss: 0.0864 | 0.0666
Epoch 27/300, Loss: 0.0862 | 0.0577
Epoch 28/300, Loss: 0.0827 | 0.0523
Epoch 29/300, Loss: 0.0817 | 0.0628
Epoch 30/300, Loss: 0.0802 | 0.0563
Epoch 31/300, Loss: 0.0798 | 0.0525
Epoch 32/300, Loss: 0.0797 | 0.0606
Epoch 33/300, Loss: 0.0790 | 0.0510
Epoch 34/300, Loss: 0.0785 | 0.0522
Epoch 35/300, Loss: 0.0784 | 0.0555
Epoch 36/300, Loss: 0.0778 | 0.0494
Epoch 37/300, Loss: 0.0775 | 0.0530
Epoch 38/300, Loss: 0.0770 | 0.0498
Epoch 39/300, Loss: 0.0767 | 0.0502
Epoch 40/300, Loss: 0.0764 | 0.0500
Epoch 41/300, Loss: 0.0761 | 0.0489
Epoch 42/300, Loss: 0.0759 | 0.0493
Epoch 43/300, Loss: 0.0757 | 0.0483
Epoch 44/300, Loss: 0.0755 | 0.0484
Epoch 45/300, Loss: 0.0753 | 0.0478
Epoch 46/300, Loss: 0.0751 | 0.0479
Epoch 47/300, Loss: 0.0749 | 0.0473
Epoch 48/300, Loss: 0.0748 | 0.0473
Epoch 49/300, Loss: 0.0747 | 0.0469
Epoch 50/300, Loss: 0.0745 | 0.0468
Epoch 51/300, Loss: 0.0744 | 0.0466
Epoch 52/300, Loss: 0.0743 | 0.0464
Epoch 53/300, Loss: 0.0742 | 0.0462
Epoch 54/300, Loss: 0.0741 | 0.0459
Epoch 55/300, Loss: 0.0740 | 0.0460
Epoch 56/300, Loss: 0.0739 | 0.0456
Epoch 57/300, Loss: 0.0738 | 0.0458
Epoch 58/300, Loss: 0.0737 | 0.0453
Epoch 59/300, Loss: 0.0736 | 0.0455
Epoch 60/300, Loss: 0.0735 | 0.0451
Epoch 61/300, Loss: 0.0734 | 0.0452
Epoch 62/300, Loss: 0.0734 | 0.0450
Epoch 63/300, Loss: 0.0733 | 0.0450
Epoch 64/300, Loss: 0.0732 | 0.0448
Epoch 65/300, Loss: 0.0732 | 0.0448
Epoch 66/300, Loss: 0.0731 | 0.0447
Epoch 67/300, Loss: 0.0731 | 0.0446
Epoch 68/300, Loss: 0.0730 | 0.0445
Epoch 69/300, Loss: 0.0730 | 0.0445
Epoch 70/300, Loss: 0.0729 | 0.0444
Epoch 71/300, Loss: 0.0729 | 0.0443
Epoch 72/300, Loss: 0.0728 | 0.0443
Epoch 73/300, Loss: 0.0728 | 0.0442
Epoch 74/300, Loss: 0.0728 | 0.0442
Epoch 75/300, Loss: 0.0727 | 0.0441
Epoch 76/300, Loss: 0.0727 | 0.0441
Epoch 77/300, Loss: 0.0727 | 0.0440
Epoch 78/300, Loss: 0.0726 | 0.0440
Epoch 79/300, Loss: 0.0726 | 0.0439
Epoch 80/300, Loss: 0.0726 | 0.0439
Epoch 81/300, Loss: 0.0725 | 0.0439
Epoch 82/300, Loss: 0.0725 | 0.0438
Epoch 83/300, Loss: 0.0725 | 0.0438
Epoch 84/300, Loss: 0.0725 | 0.0438
Epoch 85/300, Loss: 0.0724 | 0.0437
Epoch 86/300, Loss: 0.0724 | 0.0437
Epoch 87/300, Loss: 0.0724 | 0.0437
Epoch 88/300, Loss: 0.0724 | 0.0436
Epoch 89/300, Loss: 0.0724 | 0.0436
Epoch 90/300, Loss: 0.0723 | 0.0436
Epoch 91/300, Loss: 0.0723 | 0.0436
Epoch 92/300, Loss: 0.0723 | 0.0435
Epoch 93/300, Loss: 0.0723 | 0.0435
Epoch 94/300, Loss: 0.0723 | 0.0435
Epoch 95/300, Loss: 0.0723 | 0.0435
Epoch 96/300, Loss: 0.0722 | 0.0435
Epoch 97/300, Loss: 0.0722 | 0.0435
Epoch 98/300, Loss: 0.0722 | 0.0434
Epoch 99/300, Loss: 0.0722 | 0.0434
Epoch 100/300, Loss: 0.0722 | 0.0434
Epoch 101/300, Loss: 0.0722 | 0.0434
Epoch 102/300, Loss: 0.0722 | 0.0434
Epoch 103/300, Loss: 0.0722 | 0.0434
Epoch 104/300, Loss: 0.0722 | 0.0434
Epoch 105/300, Loss: 0.0722 | 0.0433
Epoch 106/300, Loss: 0.0721 | 0.0433
Epoch 107/300, Loss: 0.0721 | 0.0433
Epoch 108/300, Loss: 0.0721 | 0.0433
Epoch 109/300, Loss: 0.0721 | 0.0433
Epoch 110/300, Loss: 0.0721 | 0.0433
Epoch 111/300, Loss: 0.0721 | 0.0433
Epoch 112/300, Loss: 0.0721 | 0.0433
Epoch 113/300, Loss: 0.0721 | 0.0433
Epoch 114/300, Loss: 0.0721 | 0.0433
Epoch 115/300, Loss: 0.0721 | 0.0433
Epoch 116/300, Loss: 0.0721 | 0.0433
Epoch 117/300, Loss: 0.0721 | 0.0433
Epoch 118/300, Loss: 0.0721 | 0.0432
Epoch 119/300, Loss: 0.0721 | 0.0432
Epoch 120/300, Loss: 0.0721 | 0.0432
Epoch 121/300, Loss: 0.0721 | 0.0432
Epoch 122/300, Loss: 0.0721 | 0.0432
Epoch 123/300, Loss: 0.0721 | 0.0432
Epoch 124/300, Loss: 0.0721 | 0.0432
Epoch 125/300, Loss: 0.0721 | 0.0432
Epoch 126/300, Loss: 0.0721 | 0.0432
Epoch 127/300, Loss: 0.0721 | 0.0432
Epoch 128/300, Loss: 0.0721 | 0.0432
Epoch 129/300, Loss: 0.0720 | 0.0432
Epoch 130/300, Loss: 0.0720 | 0.0432
Epoch 131/300, Loss: 0.0720 | 0.0432
Epoch 132/300, Loss: 0.0720 | 0.0432
Epoch 133/300, Loss: 0.0720 | 0.0432
Epoch 134/300, Loss: 0.0720 | 0.0432
Epoch 135/300, Loss: 0.0720 | 0.0432
Epoch 136/300, Loss: 0.0720 | 0.0432
Epoch 137/300, Loss: 0.0720 | 0.0432
Epoch 138/300, Loss: 0.0720 | 0.0432
Epoch 139/300, Loss: 0.0720 | 0.0432
Epoch 140/300, Loss: 0.0720 | 0.0432
Epoch 141/300, Loss: 0.0720 | 0.0432
Epoch 142/300, Loss: 0.0720 | 0.0432
Epoch 143/300, Loss: 0.0720 | 0.0432
Epoch 144/300, Loss: 0.0720 | 0.0432
Epoch 145/300, Loss: 0.0720 | 0.0432
Epoch 146/300, Loss: 0.0720 | 0.0432
Epoch 147/300, Loss: 0.0720 | 0.0432
Epoch 148/300, Loss: 0.0720 | 0.0432
Epoch 149/300, Loss: 0.0720 | 0.0432
Epoch 150/300, Loss: 0.0720 | 0.0432
Epoch 151/300, Loss: 0.0720 | 0.0432
Epoch 152/300, Loss: 0.0720 | 0.0432
Epoch 153/300, Loss: 0.0720 | 0.0432
Epoch 154/300, Loss: 0.0720 | 0.0432
Epoch 155/300, Loss: 0.0720 | 0.0432
Epoch 156/300, Loss: 0.0720 | 0.0432
Epoch 157/300, Loss: 0.0720 | 0.0432
Epoch 158/300, Loss: 0.0720 | 0.0432
Epoch 159/300, Loss: 0.0720 | 0.0432
Epoch 160/300, Loss: 0.0720 | 0.0432
Epoch 161/300, Loss: 0.0720 | 0.0432
Epoch 162/300, Loss: 0.0720 | 0.0432
Epoch 163/300, Loss: 0.0720 | 0.0432
Epoch 164/300, Loss: 0.0720 | 0.0432
Epoch 165/300, Loss: 0.0720 | 0.0432
Epoch 166/300, Loss: 0.0720 | 0.0432
Epoch 167/300, Loss: 0.0720 | 0.0432
Epoch 168/300, Loss: 0.0720 | 0.0432
Epoch 169/300, Loss: 0.0720 | 0.0432
Epoch 170/300, Loss: 0.0720 | 0.0432
Epoch 171/300, Loss: 0.0720 | 0.0432
Epoch 172/300, Loss: 0.0720 | 0.0431
Epoch 173/300, Loss: 0.0720 | 0.0431
Epoch 174/300, Loss: 0.0720 | 0.0431
Epoch 175/300, Loss: 0.0720 | 0.0431
Epoch 176/300, Loss: 0.0720 | 0.0431
Epoch 177/300, Loss: 0.0720 | 0.0431
Epoch 178/300, Loss: 0.0720 | 0.0431
Epoch 179/300, Loss: 0.0720 | 0.0431
Epoch 180/300, Loss: 0.0720 | 0.0431
Epoch 181/300, Loss: 0.0720 | 0.0431
Epoch 182/300, Loss: 0.0720 | 0.0431
Epoch 183/300, Loss: 0.0720 | 0.0431
Epoch 184/300, Loss: 0.0720 | 0.0431
Epoch 185/300, Loss: 0.0720 | 0.0431
Epoch 186/300, Loss: 0.0720 | 0.0431
Epoch 187/300, Loss: 0.0720 | 0.0431
Epoch 188/300, Loss: 0.0720 | 0.0431
Epoch 189/300, Loss: 0.0720 | 0.0431
Epoch 190/300, Loss: 0.0720 | 0.0431
Epoch 191/300, Loss: 0.0720 | 0.0431
Epoch 192/300, Loss: 0.0720 | 0.0431
Epoch 193/300, Loss: 0.0720 | 0.0431
Epoch 194/300, Loss: 0.0720 | 0.0431
Epoch 195/300, Loss: 0.0720 | 0.0431
Epoch 196/300, Loss: 0.0720 | 0.0431
Epoch 197/300, Loss: 0.0720 | 0.0431
Epoch 198/300, Loss: 0.0720 | 0.0431
Epoch 199/300, Loss: 0.0720 | 0.0431
Epoch 200/300, Loss: 0.0720 | 0.0431
Epoch 201/300, Loss: 0.0720 | 0.0431
Epoch 202/300, Loss: 0.0720 | 0.0431
Epoch 203/300, Loss: 0.0720 | 0.0431
Epoch 204/300, Loss: 0.0720 | 0.0431
Epoch 205/300, Loss: 0.0720 | 0.0431
Epoch 206/300, Loss: 0.0720 | 0.0431
Epoch 207/300, Loss: 0.0720 | 0.0431
Epoch 208/300, Loss: 0.0720 | 0.0431
Epoch 209/300, Loss: 0.0720 | 0.0431
Epoch 210/300, Loss: 0.0720 | 0.0431
Epoch 211/300, Loss: 0.0720 | 0.0431
Epoch 212/300, Loss: 0.0720 | 0.0431
Epoch 213/300, Loss: 0.0720 | 0.0431
Epoch 214/300, Loss: 0.0720 | 0.0431
Epoch 215/300, Loss: 0.0720 | 0.0431
Epoch 216/300, Loss: 0.0720 | 0.0431
Epoch 217/300, Loss: 0.0720 | 0.0431
Epoch 218/300, Loss: 0.0720 | 0.0431
Epoch 219/300, Loss: 0.0720 | 0.0431
Epoch 220/300, Loss: 0.0720 | 0.0431
Epoch 221/300, Loss: 0.0720 | 0.0431
Epoch 222/300, Loss: 0.0720 | 0.0431
Epoch 223/300, Loss: 0.0720 | 0.0431
Epoch 224/300, Loss: 0.0720 | 0.0431
Epoch 225/300, Loss: 0.0720 | 0.0431
Epoch 226/300, Loss: 0.0720 | 0.0431
Epoch 227/300, Loss: 0.0720 | 0.0431
Epoch 228/300, Loss: 0.0720 | 0.0431
Epoch 229/300, Loss: 0.0720 | 0.0431
Epoch 230/300, Loss: 0.0720 | 0.0431
Epoch 231/300, Loss: 0.0720 | 0.0431
Epoch 232/300, Loss: 0.0720 | 0.0431
Epoch 233/300, Loss: 0.0720 | 0.0431
Epoch 234/300, Loss: 0.0720 | 0.0431
Epoch 235/300, Loss: 0.0720 | 0.0431
Epoch 236/300, Loss: 0.0720 | 0.0431
Epoch 237/300, Loss: 0.0720 | 0.0431
Early stopping
Runtime (seconds): 89.41261434555054
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 307.4276589064393
RMSE: 17.533615112304688
MAE: 17.533615112304688
R-squared: nan
[180.42639]
