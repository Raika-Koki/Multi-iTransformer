ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 01:59:42,027][0m A new study created in memory with name: no-name-c12a3af4-5482-4847-99c3-de86720f90ab[0m
[32m[I 2025-01-03 02:02:33,273][0m Trial 0 finished with value: 0.6451038109495285 and parameters: {'observation_period_num': 138, 'train_rates': 0.7159406470391486, 'learning_rate': 5.697798946727327e-05, 'batch_size': 153, 'step_size': 12, 'gamma': 0.7572589906240361}. Best is trial 0 with value: 0.6451038109495285.[0m
[32m[I 2025-01-03 02:06:29,708][0m Trial 1 finished with value: 0.34263225964137484 and parameters: {'observation_period_num': 126, 'train_rates': 0.9628955296017647, 'learning_rate': 2.9862500121467637e-06, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9259943783569049}. Best is trial 1 with value: 0.34263225964137484.[0m
[32m[I 2025-01-03 02:09:25,574][0m Trial 2 finished with value: 0.9620435595057393 and parameters: {'observation_period_num': 147, 'train_rates': 0.6261921611908107, 'learning_rate': 4.227618100436321e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.7609872284858111}. Best is trial 1 with value: 0.34263225964137484.[0m
[32m[I 2025-01-03 02:11:00,733][0m Trial 3 finished with value: 0.2582214566559758 and parameters: {'observation_period_num': 34, 'train_rates': 0.8059262418465661, 'learning_rate': 8.066052152414874e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8237574453511831}. Best is trial 3 with value: 0.2582214566559758.[0m
[32m[I 2025-01-03 02:12:30,416][0m Trial 4 finished with value: 0.37788758699486896 and parameters: {'observation_period_num': 65, 'train_rates': 0.9008661104589015, 'learning_rate': 9.97065584237468e-06, 'batch_size': 138, 'step_size': 8, 'gamma': 0.91557176713265}. Best is trial 3 with value: 0.2582214566559758.[0m
[32m[I 2025-01-03 02:17:32,221][0m Trial 5 finished with value: 1.2220200573811766 and parameters: {'observation_period_num': 209, 'train_rates': 0.8443189224126253, 'learning_rate': 1.418576779450279e-06, 'batch_size': 98, 'step_size': 7, 'gamma': 0.9084249734322053}. Best is trial 3 with value: 0.2582214566559758.[0m
[32m[I 2025-01-03 02:20:58,051][0m Trial 6 finished with value: 0.17398409656073788 and parameters: {'observation_period_num': 140, 'train_rates': 0.9282076699195185, 'learning_rate': 0.00015680291899743625, 'batch_size': 105, 'step_size': 12, 'gamma': 0.7505802045071769}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:21:28,604][0m Trial 7 finished with value: 1.351589400260175 and parameters: {'observation_period_num': 21, 'train_rates': 0.8751978068431671, 'learning_rate': 1.1574185765983589e-06, 'batch_size': 248, 'step_size': 10, 'gamma': 0.9107397574911811}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:23:52,719][0m Trial 8 finished with value: 0.7826744358508139 and parameters: {'observation_period_num': 121, 'train_rates': 0.6919488271413817, 'learning_rate': 0.00041964108175333566, 'batch_size': 66, 'step_size': 8, 'gamma': 0.755748860306305}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:27:54,812][0m Trial 9 finished with value: 0.9215084604918957 and parameters: {'observation_period_num': 170, 'train_rates': 0.8851749744348082, 'learning_rate': 3.5888186274241164e-06, 'batch_size': 251, 'step_size': 15, 'gamma': 0.8432911096019495}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:34:34,685][0m Trial 10 finished with value: 0.4167756140232086 and parameters: {'observation_period_num': 243, 'train_rates': 0.978628316525824, 'learning_rate': 0.0008318957331220385, 'batch_size': 182, 'step_size': 3, 'gamma': 0.9808604125542781}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:36:12,144][0m Trial 11 finished with value: 0.3654548104123963 and parameters: {'observation_period_num': 77, 'train_rates': 0.7811704708353674, 'learning_rate': 0.00016043522032827231, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8143965136345674}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:38:05,807][0m Trial 12 finished with value: 0.2776447788413988 and parameters: {'observation_period_num': 12, 'train_rates': 0.8004911868639221, 'learning_rate': 0.0001412708897459796, 'batch_size': 32, 'step_size': 5, 'gamma': 0.80489519113485}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:39:28,753][0m Trial 13 finished with value: 0.40041298820422244 and parameters: {'observation_period_num': 67, 'train_rates': 0.761272950792802, 'learning_rate': 0.0001302398462155338, 'batch_size': 106, 'step_size': 13, 'gamma': 0.799032453254378}. Best is trial 6 with value: 0.17398409656073788.[0m
Early stopping at epoch 73
[32m[I 2025-01-03 02:41:05,649][0m Trial 14 finished with value: 1.2372460141777992 and parameters: {'observation_period_num': 96, 'train_rates': 0.9273243326731977, 'learning_rate': 1.5434704387586346e-05, 'batch_size': 193, 'step_size': 1, 'gamma': 0.849780282287012}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:45:28,123][0m Trial 15 finished with value: 0.3312697568505081 and parameters: {'observation_period_num': 188, 'train_rates': 0.824601294892536, 'learning_rate': 0.0003348831710500526, 'batch_size': 58, 'step_size': 10, 'gamma': 0.7928704577828709}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:46:28,020][0m Trial 16 finished with value: 0.4927279111234893 and parameters: {'observation_period_num': 51, 'train_rates': 0.7290400605964265, 'learning_rate': 6.24810647238337e-05, 'batch_size': 118, 'step_size': 6, 'gamma': 0.8737760624972468}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:48:23,522][0m Trial 17 finished with value: 0.8343424239983925 and parameters: {'observation_period_num': 101, 'train_rates': 0.6352707600582694, 'learning_rate': 1.9384950588327985e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8331458107376276}. Best is trial 6 with value: 0.17398409656073788.[0m
[32m[I 2025-01-03 02:50:12,346][0m Trial 18 finished with value: 0.15236363781022502 and parameters: {'observation_period_num': 44, 'train_rates': 0.9374508900253753, 'learning_rate': 0.0002995740318867941, 'batch_size': 37, 'step_size': 9, 'gamma': 0.7840452899363046}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 02:54:19,734][0m Trial 19 finished with value: 0.820661451384338 and parameters: {'observation_period_num': 166, 'train_rates': 0.9388762924177534, 'learning_rate': 0.0008877748244226257, 'batch_size': 167, 'step_size': 12, 'gamma': 0.7792515463538735}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 02:56:28,527][0m Trial 20 finished with value: 0.1571594476699829 and parameters: {'observation_period_num': 96, 'train_rates': 0.9280958185293835, 'learning_rate': 0.0003059430807064933, 'batch_size': 215, 'step_size': 9, 'gamma': 0.8691124805327604}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 02:58:39,456][0m Trial 21 finished with value: 0.15301155758664964 and parameters: {'observation_period_num': 97, 'train_rates': 0.9178959875139167, 'learning_rate': 0.0003302403731035134, 'batch_size': 210, 'step_size': 9, 'gamma': 0.8761075645444348}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 03:00:52,178][0m Trial 22 finished with value: 0.27483377410586635 and parameters: {'observation_period_num': 103, 'train_rates': 0.8593527434747572, 'learning_rate': 0.00039593802742562745, 'batch_size': 224, 'step_size': 9, 'gamma': 0.8796498418115343}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 03:01:51,651][0m Trial 23 finished with value: 0.1879543513059616 and parameters: {'observation_period_num': 43, 'train_rates': 0.9559228760314219, 'learning_rate': 0.0002722582253399852, 'batch_size': 220, 'step_size': 6, 'gamma': 0.9511948904464306}. Best is trial 18 with value: 0.15236363781022502.[0m
[32m[I 2025-01-03 03:03:45,451][0m Trial 24 finished with value: 0.11904220283031464 and parameters: {'observation_period_num': 80, 'train_rates': 0.9854086572152597, 'learning_rate': 0.0006083615364939145, 'batch_size': 209, 'step_size': 9, 'gamma': 0.8567407696251732}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:05:47,520][0m Trial 25 finished with value: 0.19548159837722778 and parameters: {'observation_period_num': 86, 'train_rates': 0.9891636157510343, 'learning_rate': 0.0006817135430572059, 'batch_size': 195, 'step_size': 7, 'gamma': 0.8893559805854604}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:07:00,180][0m Trial 26 finished with value: 0.2229600316243814 and parameters: {'observation_period_num': 56, 'train_rates': 0.9028202639672872, 'learning_rate': 0.0006338517926922095, 'batch_size': 166, 'step_size': 9, 'gamma': 0.8477805636472553}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:09:45,818][0m Trial 27 finished with value: 0.19462646543979645 and parameters: {'observation_period_num': 117, 'train_rates': 0.9605657428414045, 'learning_rate': 0.00021186714416111512, 'batch_size': 235, 'step_size': 11, 'gamma': 0.9484345285209426}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:10:31,809][0m Trial 28 finished with value: 0.17908047499892477 and parameters: {'observation_period_num': 33, 'train_rates': 0.9067507848415409, 'learning_rate': 0.0005513990271942658, 'batch_size': 201, 'step_size': 7, 'gamma': 0.8593269363001539}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:12:22,605][0m Trial 29 finished with value: 0.13182732462882996 and parameters: {'observation_period_num': 77, 'train_rates': 0.98784694946804, 'learning_rate': 7.827344955702853e-05, 'batch_size': 145, 'step_size': 11, 'gamma': 0.8943358007948332}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:14:04,459][0m Trial 30 finished with value: 0.15807223320007324 and parameters: {'observation_period_num': 70, 'train_rates': 0.9850348903076726, 'learning_rate': 3.7376801954345646e-05, 'batch_size': 149, 'step_size': 13, 'gamma': 0.7727004798018131}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:16:01,234][0m Trial 31 finished with value: 0.19576405592866847 and parameters: {'observation_period_num': 83, 'train_rates': 0.9484658606505713, 'learning_rate': 7.121538191582004e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8877383119354361}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:17:07,233][0m Trial 32 finished with value: 0.19684287905693054 and parameters: {'observation_period_num': 48, 'train_rates': 0.9662172749824617, 'learning_rate': 8.981784546799665e-05, 'batch_size': 176, 'step_size': 10, 'gamma': 0.8971580477593009}. Best is trial 24 with value: 0.11904220283031464.[0m
[32m[I 2025-01-03 03:20:02,018][0m Trial 33 finished with value: 0.10880113393068314 and parameters: {'observation_period_num': 118, 'train_rates': 0.9851274780248752, 'learning_rate': 0.00020778443095009712, 'batch_size': 83, 'step_size': 9, 'gamma': 0.9296365303263788}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:23:23,642][0m Trial 34 finished with value: 0.19030082864420755 and parameters: {'observation_period_num': 133, 'train_rates': 0.9652731844764889, 'learning_rate': 0.00019815661592865541, 'batch_size': 84, 'step_size': 13, 'gamma': 0.938354767444821}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:26:11,255][0m Trial 35 finished with value: 0.19629213281653143 and parameters: {'observation_period_num': 111, 'train_rates': 0.9457779778125225, 'learning_rate': 0.00011025257934892439, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9862798957570797}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:29:17,175][0m Trial 36 finished with value: 2.404292148702285 and parameters: {'observation_period_num': 31, 'train_rates': 0.982630643983736, 'learning_rate': 0.0005268034194182267, 'batch_size': 22, 'step_size': 8, 'gamma': 0.9714025533559646}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:32:59,901][0m Trial 37 finished with value: 0.25462119588985554 and parameters: {'observation_period_num': 159, 'train_rates': 0.8785564173481135, 'learning_rate': 4.2834460546473486e-05, 'batch_size': 133, 'step_size': 6, 'gamma': 0.9324022434140035}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:34:23,989][0m Trial 38 finished with value: 0.14607108152896456 and parameters: {'observation_period_num': 7, 'train_rates': 0.9653300189011952, 'learning_rate': 2.6980359066424183e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.9041055094700963}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:35:15,598][0m Trial 39 finished with value: 0.16540427424095488 and parameters: {'observation_period_num': 5, 'train_rates': 0.9691463049918605, 'learning_rate': 2.252348568478596e-05, 'batch_size': 83, 'step_size': 15, 'gamma': 0.9168449380426363}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:38:57,140][0m Trial 40 finished with value: 0.22466012107771496 and parameters: {'observation_period_num': 149, 'train_rates': 0.8928295734160299, 'learning_rate': 9.673569586910175e-06, 'batch_size': 49, 'step_size': 14, 'gamma': 0.9037683769210162}. Best is trial 33 with value: 0.10880113393068314.[0m
[32m[I 2025-01-03 03:41:38,031][0m Trial 41 finished with value: 0.06650143726305528 and parameters: {'observation_period_num': 21, 'train_rates': 0.9890533163909936, 'learning_rate': 5.139405561267344e-05, 'batch_size': 25, 'step_size': 12, 'gamma': 0.9228430787551086}. Best is trial 41 with value: 0.06650143726305528.[0m
[32m[I 2025-01-03 03:44:18,050][0m Trial 42 finished with value: 0.07152020807067554 and parameters: {'observation_period_num': 22, 'train_rates': 0.9897860099538847, 'learning_rate': 3.1822756416502224e-05, 'batch_size': 25, 'step_size': 14, 'gamma': 0.9236220326043393}. Best is trial 41 with value: 0.06650143726305528.[0m
[32m[I 2025-01-03 03:46:51,516][0m Trial 43 finished with value: 0.06401321418741916 and parameters: {'observation_period_num': 19, 'train_rates': 0.984090652064828, 'learning_rate': 4.57887602022304e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.923385042422348}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 03:49:09,614][0m Trial 44 finished with value: 0.17233134757459553 and parameters: {'observation_period_num': 24, 'train_rates': 0.9479216230617353, 'learning_rate': 1.1839658851307062e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.922080588247911}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 03:52:17,312][0m Trial 45 finished with value: 0.11476074886741117 and parameters: {'observation_period_num': 16, 'train_rates': 0.9128138138175139, 'learning_rate': 4.812505797072969e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9585104138748469}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 03:55:45,115][0m Trial 46 finished with value: 0.23322106922777103 and parameters: {'observation_period_num': 20, 'train_rates': 0.8473993669191866, 'learning_rate': 5.2860443978038496e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9640843563811218}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 03:56:47,558][0m Trial 47 finished with value: 0.3170999512076378 and parameters: {'observation_period_num': 17, 'train_rates': 0.9202444405011736, 'learning_rate': 5.343676767925337e-06, 'batch_size': 63, 'step_size': 14, 'gamma': 0.9426933129124282}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 04:03:44,755][0m Trial 48 finished with value: 0.15832949474326566 and parameters: {'observation_period_num': 252, 'train_rates': 0.913515692896615, 'learning_rate': 3.1564501059987295e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9650046909817341}. Best is trial 43 with value: 0.06401321418741916.[0m
[32m[I 2025-01-03 04:08:37,541][0m Trial 49 finished with value: 0.27825084215671314 and parameters: {'observation_period_num': 202, 'train_rates': 0.8224441870030765, 'learning_rate': 4.525326676028777e-05, 'batch_size': 50, 'step_size': 12, 'gamma': 0.9294043775218612}. Best is trial 43 with value: 0.06401321418741916.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 04:08:37,548][0m A new study created in memory with name: no-name-688d0ffa-613b-4a06-9515-090056fa6055[0m
[32m[I 2025-01-03 04:12:50,444][0m Trial 0 finished with value: 0.3913089118119496 and parameters: {'observation_period_num': 187, 'train_rates': 0.8060192699908892, 'learning_rate': 0.00020170711469159866, 'batch_size': 92, 'step_size': 9, 'gamma': 0.9401842273902414}. Best is trial 0 with value: 0.3913089118119496.[0m
[32m[I 2025-01-03 04:13:54,579][0m Trial 1 finished with value: 1.1459060961785523 and parameters: {'observation_period_num': 55, 'train_rates': 0.6028313599358346, 'learning_rate': 6.240467756315996e-06, 'batch_size': 55, 'step_size': 9, 'gamma': 0.9299877926652916}. Best is trial 0 with value: 0.3913089118119496.[0m
[32m[I 2025-01-03 04:19:12,283][0m Trial 2 finished with value: 0.19232095777988434 and parameters: {'observation_period_num': 208, 'train_rates': 0.9255043668918747, 'learning_rate': 0.00023040408407803942, 'batch_size': 217, 'step_size': 11, 'gamma': 0.797443242317403}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:20:11,857][0m Trial 3 finished with value: 0.663012573389503 and parameters: {'observation_period_num': 50, 'train_rates': 0.7455922878554464, 'learning_rate': 0.0003157339705657156, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9897703131979521}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:22:54,875][0m Trial 4 finished with value: 0.3593592941761017 and parameters: {'observation_period_num': 113, 'train_rates': 0.9846257808530332, 'learning_rate': 7.154742296416699e-06, 'batch_size': 113, 'step_size': 8, 'gamma': 0.9420608830210756}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:26:29,743][0m Trial 5 finished with value: 1.2622991972475208 and parameters: {'observation_period_num': 162, 'train_rates': 0.8041108995157459, 'learning_rate': 0.0009902740210861595, 'batch_size': 154, 'step_size': 5, 'gamma': 0.9694906637614874}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:27:28,342][0m Trial 6 finished with value: 0.21603796598183583 and parameters: {'observation_period_num': 8, 'train_rates': 0.9154745281130928, 'learning_rate': 0.00015329756798851356, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9831227763390493}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:32:26,220][0m Trial 7 finished with value: 0.2021200139850578 and parameters: {'observation_period_num': 193, 'train_rates': 0.946262673901967, 'learning_rate': 0.00034819127555115516, 'batch_size': 130, 'step_size': 12, 'gamma': 0.8054969696614779}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:35:20,577][0m Trial 8 finished with value: 0.3705671813850309 and parameters: {'observation_period_num': 130, 'train_rates': 0.8193789081358573, 'learning_rate': 0.0001229198125211736, 'batch_size': 137, 'step_size': 3, 'gamma': 0.8518141457495919}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:39:17,543][0m Trial 9 finished with value: 0.276637900796571 and parameters: {'observation_period_num': 161, 'train_rates': 0.9167260931077985, 'learning_rate': 1.952811006650622e-05, 'batch_size': 80, 'step_size': 10, 'gamma': 0.821842297527676}. Best is trial 2 with value: 0.19232095777988434.[0m
Early stopping at epoch 32
[32m[I 2025-01-03 04:41:00,465][0m Trial 10 finished with value: 2.0129807395710526 and parameters: {'observation_period_num': 237, 'train_rates': 0.7028283278078089, 'learning_rate': 1.212512735004652e-06, 'batch_size': 245, 'step_size': 1, 'gamma': 0.758495394462202}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:47:29,895][0m Trial 11 finished with value: 0.22639351319044065 and parameters: {'observation_period_num': 249, 'train_rates': 0.9136561812186619, 'learning_rate': 0.0008328907527503576, 'batch_size': 197, 'step_size': 12, 'gamma': 0.782692309913391}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:52:56,676][0m Trial 12 finished with value: 0.2215474545955658 and parameters: {'observation_period_num': 208, 'train_rates': 0.9653518732636167, 'learning_rate': 4.9675029842249434e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.8126080917846263}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 04:58:29,521][0m Trial 13 finished with value: 0.20656252960706578 and parameters: {'observation_period_num': 206, 'train_rates': 0.8734853727207363, 'learning_rate': 6.0539662682940265e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8802589221862634}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 05:02:22,494][0m Trial 14 finished with value: 0.22624616755730909 and parameters: {'observation_period_num': 166, 'train_rates': 0.8645958789942257, 'learning_rate': 0.000429280175645016, 'batch_size': 185, 'step_size': 12, 'gamma': 0.8092562533166935}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 05:05:14,722][0m Trial 15 finished with value: 0.21731191873550415 and parameters: {'observation_period_num': 123, 'train_rates': 0.9512548550175418, 'learning_rate': 8.85402228811398e-05, 'batch_size': 250, 'step_size': 15, 'gamma': 0.8608979939516521}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 05:10:36,903][0m Trial 16 finished with value: 0.4836658494196076 and parameters: {'observation_period_num': 221, 'train_rates': 0.8484041688245644, 'learning_rate': 2.247962371142843e-05, 'batch_size': 174, 'step_size': 11, 'gamma': 0.7582763243837057}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 05:14:36,945][0m Trial 17 finished with value: 0.480405655374779 and parameters: {'observation_period_num': 190, 'train_rates': 0.7395607875822098, 'learning_rate': 0.00044318148656156123, 'batch_size': 224, 'step_size': 7, 'gamma': 0.8978443672882697}. Best is trial 2 with value: 0.19232095777988434.[0m
[32m[I 2025-01-03 05:21:19,269][0m Trial 18 finished with value: 0.17264465987682343 and parameters: {'observation_period_num': 252, 'train_rates': 0.9319852374603154, 'learning_rate': 0.00027607871810912427, 'batch_size': 222, 'step_size': 13, 'gamma': 0.8374076445214669}. Best is trial 18 with value: 0.17264465987682343.[0m
[32m[I 2025-01-03 05:27:42,120][0m Trial 19 finished with value: 0.44635073468089104 and parameters: {'observation_period_num': 248, 'train_rates': 0.8935970925609656, 'learning_rate': 1.2064163901638679e-05, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8307103800721024}. Best is trial 18 with value: 0.17264465987682343.[0m
[32m[I 2025-01-03 05:32:11,858][0m Trial 20 finished with value: 1.0339822751425563 and parameters: {'observation_period_num': 225, 'train_rates': 0.630135761998089, 'learning_rate': 4.260179453502508e-05, 'batch_size': 218, 'step_size': 15, 'gamma': 0.7867740893073777}. Best is trial 18 with value: 0.17264465987682343.[0m
[32m[I 2025-01-03 05:37:01,434][0m Trial 21 finished with value: 0.19394931197166443 and parameters: {'observation_period_num': 190, 'train_rates': 0.944991264715372, 'learning_rate': 0.0002480625512691973, 'batch_size': 165, 'step_size': 11, 'gamma': 0.7911070526250619}. Best is trial 18 with value: 0.17264465987682343.[0m
[32m[I 2025-01-03 05:43:05,978][0m Trial 22 finished with value: 0.0999804362654686 and parameters: {'observation_period_num': 221, 'train_rates': 0.9887294419846075, 'learning_rate': 0.0001889104841907421, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8419575709029429}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 05:49:12,723][0m Trial 23 finished with value: 0.13834261894226074 and parameters: {'observation_period_num': 225, 'train_rates': 0.9839455672096219, 'learning_rate': 8.715635117071628e-05, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8382239954731076}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 05:55:28,988][0m Trial 24 finished with value: 0.12470939010381699 and parameters: {'observation_period_num': 231, 'train_rates': 0.9839809214177853, 'learning_rate': 0.00011604919411517745, 'batch_size': 193, 'step_size': 9, 'gamma': 0.8416711998556357}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:01:33,423][0m Trial 25 finished with value: 0.13589705526828766 and parameters: {'observation_period_num': 225, 'train_rates': 0.9742760745348983, 'learning_rate': 8.094850203434576e-05, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8867354003892655}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:05:07,722][0m Trial 26 finished with value: 0.12829259037971497 and parameters: {'observation_period_num': 140, 'train_rates': 0.9828154546931678, 'learning_rate': 9.480474568510162e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8854596385462844}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:07:07,658][0m Trial 27 finished with value: 0.27187309684647115 and parameters: {'observation_period_num': 90, 'train_rates': 0.8902366026938242, 'learning_rate': 3.303958364494768e-05, 'batch_size': 158, 'step_size': 5, 'gamma': 0.9152173977485236}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:10:48,318][0m Trial 28 finished with value: 0.12107554078102112 and parameters: {'observation_period_num': 145, 'train_rates': 0.9868964908972335, 'learning_rate': 0.00013836824851178403, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8631956835041623}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:14:46,329][0m Trial 29 finished with value: 0.3232797138097528 and parameters: {'observation_period_num': 173, 'train_rates': 0.8367940774154696, 'learning_rate': 0.00015654611373053487, 'batch_size': 182, 'step_size': 9, 'gamma': 0.8543219617162001}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:16:39,710][0m Trial 30 finished with value: 0.5818111174432191 and parameters: {'observation_period_num': 92, 'train_rates': 0.7618158001100035, 'learning_rate': 0.0006071662777845054, 'batch_size': 118, 'step_size': 4, 'gamma': 0.8676691047113656}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:20:15,639][0m Trial 31 finished with value: 0.11736075580120087 and parameters: {'observation_period_num': 143, 'train_rates': 0.9854605313932039, 'learning_rate': 0.0001143565584580511, 'batch_size': 148, 'step_size': 7, 'gamma': 0.902813661183321}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:23:44,905][0m Trial 32 finished with value: 0.20209774374961853 and parameters: {'observation_period_num': 141, 'train_rates': 0.9585571509493378, 'learning_rate': 0.00013616656729451525, 'batch_size': 150, 'step_size': 7, 'gamma': 0.9114567723658485}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:25:48,434][0m Trial 33 finished with value: 0.1868969351053238 and parameters: {'observation_period_num': 87, 'train_rates': 0.961918741340251, 'learning_rate': 0.00017599830543572783, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9023932152549919}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:30:08,906][0m Trial 34 finished with value: 0.1928759838282729 and parameters: {'observation_period_num': 176, 'train_rates': 0.9355787875465421, 'learning_rate': 6.108527361284496e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9458544512039446}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:33:56,690][0m Trial 35 finished with value: 0.11461696028709412 and parameters: {'observation_period_num': 150, 'train_rates': 0.9859093064386506, 'learning_rate': 0.00021130055769463783, 'batch_size': 187, 'step_size': 10, 'gamma': 0.845367780282116}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:36:22,264][0m Trial 36 finished with value: 0.18320812743840334 and parameters: {'observation_period_num': 108, 'train_rates': 0.8984540235716498, 'learning_rate': 0.0002052001234583387, 'batch_size': 147, 'step_size': 10, 'gamma': 0.9263032497098868}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:40:08,301][0m Trial 37 finished with value: 1.0359951257705688 and parameters: {'observation_period_num': 149, 'train_rates': 0.9613718077277015, 'learning_rate': 2.264428585627464e-06, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8650037157675814}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:40:51,354][0m Trial 38 finished with value: 0.8136559374947324 and parameters: {'observation_period_num': 37, 'train_rates': 0.6853042230625217, 'learning_rate': 0.0006589715952427294, 'batch_size': 169, 'step_size': 6, 'gamma': 0.8771543472304126}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:42:25,827][0m Trial 39 finished with value: 0.22670187268938338 and parameters: {'observation_period_num': 67, 'train_rates': 0.9272835707313323, 'learning_rate': 0.00038910718163067, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9568624158057097}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:44:39,191][0m Trial 40 finished with value: 0.40380321971594996 and parameters: {'observation_period_num': 107, 'train_rates': 0.7859201500766164, 'learning_rate': 0.00026189981366827523, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8478650580548105}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:48:25,802][0m Trial 41 finished with value: 0.12574522197246552 and parameters: {'observation_period_num': 147, 'train_rates': 0.9889103163498617, 'learning_rate': 0.00011901258712453248, 'batch_size': 187, 'step_size': 9, 'gamma': 0.82771789652107}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:51:22,250][0m Trial 42 finished with value: 0.1218210756778717 and parameters: {'observation_period_num': 123, 'train_rates': 0.9862533884020207, 'learning_rate': 0.00019109654463303756, 'batch_size': 207, 'step_size': 9, 'gamma': 0.8465712767427668}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:54:07,930][0m Trial 43 finished with value: 0.19570893049240112 and parameters: {'observation_period_num': 118, 'train_rates': 0.945463111608467, 'learning_rate': 0.00021483050305157303, 'batch_size': 206, 'step_size': 8, 'gamma': 0.8564690007116327}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 06:57:23,024][0m Trial 44 finished with value: 0.2065858542919159 and parameters: {'observation_period_num': 129, 'train_rates': 0.9688718090288599, 'learning_rate': 0.0005617717581658333, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8949583685243094}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 07:01:25,462][0m Trial 45 finished with value: 0.11854158341884613 and parameters: {'observation_period_num': 158, 'train_rates': 0.9895337236931657, 'learning_rate': 0.0003122829176814721, 'batch_size': 147, 'step_size': 7, 'gamma': 0.8207305059947776}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 07:05:12,892][0m Trial 46 finished with value: 0.19700110503813115 and parameters: {'observation_period_num': 157, 'train_rates': 0.9121996256349376, 'learning_rate': 0.00025959233772369846, 'batch_size': 144, 'step_size': 5, 'gamma': 0.8185876308265915}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 07:09:40,483][0m Trial 47 finished with value: 0.1905546337366104 and parameters: {'observation_period_num': 179, 'train_rates': 0.9445517387534496, 'learning_rate': 0.0003876764668878607, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8706156017342157}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 07:14:58,873][0m Trial 48 finished with value: 0.24748165905475616 and parameters: {'observation_period_num': 201, 'train_rates': 0.969368561318112, 'learning_rate': 6.114492401576976e-05, 'batch_size': 139, 'step_size': 4, 'gamma': 0.8257679325822155}. Best is trial 22 with value: 0.0999804362654686.[0m
[32m[I 2025-01-03 07:18:42,428][0m Trial 49 finished with value: 0.26362626898288727 and parameters: {'observation_period_num': 155, 'train_rates': 0.9108528675480014, 'learning_rate': 0.0003279070320560153, 'batch_size': 126, 'step_size': 2, 'gamma': 0.8077172246082976}. Best is trial 22 with value: 0.0999804362654686.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 07:18:42,435][0m A new study created in memory with name: no-name-75980d2f-8499-4bcf-a95b-84bc5edde60f[0m
[32m[I 2025-01-03 07:20:10,527][0m Trial 0 finished with value: 0.4409429115466337 and parameters: {'observation_period_num': 72, 'train_rates': 0.7928822944597224, 'learning_rate': 0.000877213806788325, 'batch_size': 239, 'step_size': 7, 'gamma': 0.8529092128436521}. Best is trial 0 with value: 0.4409429115466337.[0m
[32m[I 2025-01-03 07:26:35,400][0m Trial 1 finished with value: 0.622265732010051 and parameters: {'observation_period_num': 248, 'train_rates': 0.9051312817502604, 'learning_rate': 8.181614653014648e-06, 'batch_size': 186, 'step_size': 8, 'gamma': 0.8044026775258167}. Best is trial 0 with value: 0.4409429115466337.[0m
[32m[I 2025-01-03 07:27:13,341][0m Trial 2 finished with value: 0.9412124645774779 and parameters: {'observation_period_num': 24, 'train_rates': 0.685611203801286, 'learning_rate': 4.687407602675781e-06, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9891047981348045}. Best is trial 0 with value: 0.4409429115466337.[0m
[32m[I 2025-01-03 07:27:59,727][0m Trial 3 finished with value: 1.0257360477138449 and parameters: {'observation_period_num': 33, 'train_rates': 0.8891372677756966, 'learning_rate': 1.806693548158806e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.858942040912561}. Best is trial 0 with value: 0.4409429115466337.[0m
[32m[I 2025-01-03 07:30:17,779][0m Trial 4 finished with value: 0.6393954801012058 and parameters: {'observation_period_num': 118, 'train_rates': 0.723239559018864, 'learning_rate': 0.00035274463325881485, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8856577171731868}. Best is trial 0 with value: 0.4409429115466337.[0m
[32m[I 2025-01-03 07:31:58,657][0m Trial 5 finished with value: 0.11825244128704071 and parameters: {'observation_period_num': 71, 'train_rates': 0.9872213423900731, 'learning_rate': 6.495326848203435e-05, 'batch_size': 200, 'step_size': 13, 'gamma': 0.8665279562739301}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:33:26,058][0m Trial 6 finished with value: 0.23954855864956265 and parameters: {'observation_period_num': 38, 'train_rates': 0.8197457615265378, 'learning_rate': 6.174333180305567e-05, 'batch_size': 41, 'step_size': 5, 'gamma': 0.8666033637945891}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:38:57,676][0m Trial 7 finished with value: 0.46908031558990476 and parameters: {'observation_period_num': 228, 'train_rates': 0.8163745090332277, 'learning_rate': 8.018044758426058e-06, 'batch_size': 48, 'step_size': 15, 'gamma': 0.9005339222110942}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:41:04,235][0m Trial 8 finished with value: 2.399282716326977 and parameters: {'observation_period_num': 38, 'train_rates': 0.8634665232737131, 'learning_rate': 0.000956938812154767, 'batch_size': 29, 'step_size': 5, 'gamma': 0.9134758926693561}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:41:24,367][0m Trial 9 finished with value: 0.20156133438392385 and parameters: {'observation_period_num': 11, 'train_rates': 0.8885937829674031, 'learning_rate': 3.639220295662627e-05, 'batch_size': 218, 'step_size': 14, 'gamma': 0.9441037890423165}. Best is trial 5 with value: 0.11825244128704071.[0m
Early stopping at epoch 86
[32m[I 2025-01-03 07:45:32,801][0m Trial 10 finished with value: 0.2870711088180542 and parameters: {'observation_period_num': 185, 'train_rates': 0.9851258932447671, 'learning_rate': 0.00012406952860056304, 'batch_size': 188, 'step_size': 2, 'gamma': 0.7512587535604212}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:47:38,645][0m Trial 11 finished with value: 0.16934289038181305 and parameters: {'observation_period_num': 88, 'train_rates': 0.9861125517351016, 'learning_rate': 2.929970687255583e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9603484382588501}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:49:59,156][0m Trial 12 finished with value: 0.2067340612411499 and parameters: {'observation_period_num': 100, 'train_rates': 0.9759329211862237, 'learning_rate': 1.9931385541277817e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.9734538562778631}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:52:50,944][0m Trial 13 finished with value: 0.9248066469551546 and parameters: {'observation_period_num': 154, 'train_rates': 0.6078228973304743, 'learning_rate': 0.00010927018267059419, 'batch_size': 214, 'step_size': 15, 'gamma': 0.8089872441313009}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:54:49,917][0m Trial 14 finished with value: 0.2319489801955071 and parameters: {'observation_period_num': 86, 'train_rates': 0.945189713214124, 'learning_rate': 1.90189541089337e-05, 'batch_size': 148, 'step_size': 10, 'gamma': 0.938027438347381}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 07:58:31,755][0m Trial 15 finished with value: 0.18663954734802246 and parameters: {'observation_period_num': 151, 'train_rates': 0.9378120827639741, 'learning_rate': 0.00014796086895293032, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8263399659607322}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:00:22,688][0m Trial 16 finished with value: 0.1365959644317627 and parameters: {'observation_period_num': 78, 'train_rates': 0.9863711543662202, 'learning_rate': 4.1216345135668966e-05, 'batch_size': 181, 'step_size': 15, 'gamma': 0.9314542638894604}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:01:47,838][0m Trial 17 finished with value: 0.162590123950801 and parameters: {'observation_period_num': 62, 'train_rates': 0.9349567114624515, 'learning_rate': 0.00022872136901288604, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9223174269935321}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:04:20,691][0m Trial 18 finished with value: 0.39359724306085575 and parameters: {'observation_period_num': 126, 'train_rates': 0.7749499039981274, 'learning_rate': 5.9573962448949856e-05, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8370186514358278}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:05:34,774][0m Trial 19 finished with value: 1.4126415535775145 and parameters: {'observation_period_num': 59, 'train_rates': 0.8412939100843054, 'learning_rate': 1.194684294834586e-06, 'batch_size': 197, 'step_size': 13, 'gamma': 0.8960024765390165}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:09:10,907][0m Trial 20 finished with value: 0.565883654768254 and parameters: {'observation_period_num': 170, 'train_rates': 0.7505421367641503, 'learning_rate': 0.00039560008803773457, 'batch_size': 125, 'step_size': 2, 'gamma': 0.7823274090894741}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:10:36,159][0m Trial 21 finished with value: 0.1790172072748343 and parameters: {'observation_period_num': 61, 'train_rates': 0.9421526264917984, 'learning_rate': 0.0002554055606055804, 'batch_size': 98, 'step_size': 12, 'gamma': 0.9309404057928053}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:13:04,514][0m Trial 22 finished with value: 0.1621618642343729 and parameters: {'observation_period_num': 103, 'train_rates': 0.9261224514837927, 'learning_rate': 7.260168488872676e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.910873560651137}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:15:30,507][0m Trial 23 finished with value: 0.1663884618319571 and parameters: {'observation_period_num': 104, 'train_rates': 0.9157654219382036, 'learning_rate': 6.131348519436705e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.878260970822915}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:18:58,893][0m Trial 24 finished with value: 0.3276764154434204 and parameters: {'observation_period_num': 139, 'train_rates': 0.9657143319333521, 'learning_rate': 1.263973905717631e-05, 'batch_size': 142, 'step_size': 11, 'gamma': 0.9072285119931925}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:20:50,238][0m Trial 25 finished with value: 0.2478429362978167 and parameters: {'observation_period_num': 80, 'train_rates': 0.8638950279915646, 'learning_rate': 3.977325747068683e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9574228758746478}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:23:36,475][0m Trial 26 finished with value: 0.17993566638133565 and parameters: {'observation_period_num': 113, 'train_rates': 0.9572564711708462, 'learning_rate': 7.814136167474344e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.88350975260681}. Best is trial 5 with value: 0.11825244128704071.[0m
[32m[I 2025-01-03 08:27:44,113][0m Trial 27 finished with value: 0.0851566381752491 and parameters: {'observation_period_num': 51, 'train_rates': 0.9898873235701335, 'learning_rate': 2.5120900751798626e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8407730014682415}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:28:52,786][0m Trial 28 finished with value: 0.6587902307510376 and parameters: {'observation_period_num': 48, 'train_rates': 0.9887549787256872, 'learning_rate': 3.4111840259725887e-06, 'batch_size': 167, 'step_size': 7, 'gamma': 0.8442716011618823}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:29:07,590][0m Trial 29 finished with value: 1.254328642510409 and parameters: {'observation_period_num': 5, 'train_rates': 0.6288218333396957, 'learning_rate': 2.0054054365941095e-05, 'batch_size': 230, 'step_size': 5, 'gamma': 0.827213109323343}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:32:54,192][0m Trial 30 finished with value: 0.19010890284916024 and parameters: {'observation_period_num': 72, 'train_rates': 0.8931823545418582, 'learning_rate': 1.2070105833457459e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8513309675967335}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:35:04,411][0m Trial 31 finished with value: 0.26373645663261414 and parameters: {'observation_period_num': 94, 'train_rates': 0.9606944801142572, 'learning_rate': 3.636248871745475e-05, 'batch_size': 199, 'step_size': 8, 'gamma': 0.8701483750661982}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:36:56,848][0m Trial 32 finished with value: 0.14382654726505278 and parameters: {'observation_period_num': 74, 'train_rates': 0.921947366882838, 'learning_rate': 7.893935208049423e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9154924970586188}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:39:40,558][0m Trial 33 finished with value: 0.1279944050404812 and parameters: {'observation_period_num': 51, 'train_rates': 0.9115078019238196, 'learning_rate': 0.00015258812617338435, 'batch_size': 23, 'step_size': 8, 'gamma': 0.795458750970855}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:43:15,374][0m Trial 34 finished with value: 1.1050218376246366 and parameters: {'observation_period_num': 19, 'train_rates': 0.9627111448519685, 'learning_rate': 0.0005415108336137568, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8099328457167335}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:44:49,073][0m Trial 35 finished with value: 0.12530515198746034 and parameters: {'observation_period_num': 26, 'train_rates': 0.9048721834270773, 'learning_rate': 0.00020317024503173584, 'batch_size': 41, 'step_size': 8, 'gamma': 0.7780426982243182}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:46:35,354][0m Trial 36 finished with value: 0.21998692764022246 and parameters: {'observation_period_num': 29, 'train_rates': 0.8677233725608531, 'learning_rate': 0.00018400258779998412, 'batch_size': 34, 'step_size': 8, 'gamma': 0.7807931387707825}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:48:00,913][0m Trial 37 finished with value: 0.2743341049298327 and parameters: {'observation_period_num': 43, 'train_rates': 0.9032016317995195, 'learning_rate': 0.000469613837296256, 'batch_size': 45, 'step_size': 6, 'gamma': 0.7760373374728498}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:49:18,164][0m Trial 38 finished with value: 0.29891003697932833 and parameters: {'observation_period_num': 53, 'train_rates': 0.8264397912606611, 'learning_rate': 0.0002814502205324543, 'batch_size': 51, 'step_size': 3, 'gamma': 0.7970259332389861}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:51:06,535][0m Trial 39 finished with value: 0.812099531913797 and parameters: {'observation_period_num': 19, 'train_rates': 0.78717180327445, 'learning_rate': 0.0006569323991318207, 'batch_size': 31, 'step_size': 9, 'gamma': 0.7680414672298361}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:53:43,324][0m Trial 40 finished with value: 0.18058367671681122 and parameters: {'observation_period_num': 32, 'train_rates': 0.8784291873442682, 'learning_rate': 0.00011000513095525325, 'batch_size': 25, 'step_size': 4, 'gamma': 0.8195717170696117}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:55:13,695][0m Trial 41 finished with value: 0.2429778042766783 and parameters: {'observation_period_num': 66, 'train_rates': 0.9063916532550552, 'learning_rate': 4.401599668350572e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.7960266705157474}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 08:56:14,128][0m Trial 42 finished with value: 0.23354366421699524 and parameters: {'observation_period_num': 43, 'train_rates': 0.9539275839581212, 'learning_rate': 0.00018336753000916143, 'batch_size': 157, 'step_size': 6, 'gamma': 0.7578407451059824}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:01:38,933][0m Trial 43 finished with value: 0.26321428174904254 and parameters: {'observation_period_num': 214, 'train_rates': 0.8469376820008622, 'learning_rate': 4.9596200126599514e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8642685992174262}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:02:51,295][0m Trial 44 finished with value: 0.18557319045066833 and parameters: {'observation_period_num': 51, 'train_rates': 0.9739880040205603, 'learning_rate': 2.744717101245274e-05, 'batch_size': 188, 'step_size': 15, 'gamma': 0.8537967619504768}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:03:54,631][0m Trial 45 finished with value: 0.6778966945508397 and parameters: {'observation_period_num': 21, 'train_rates': 0.6699232368731198, 'learning_rate': 9.787018321701316e-05, 'batch_size': 53, 'step_size': 14, 'gamma': 0.7934718057432781}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:05:53,086][0m Trial 46 finished with value: 0.2205761820077896 and parameters: {'observation_period_num': 83, 'train_rates': 0.9857638750296132, 'learning_rate': 2.4049081711020602e-05, 'batch_size': 239, 'step_size': 9, 'gamma': 0.8925669134898984}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:06:48,018][0m Trial 47 finished with value: 0.611025557324693 and parameters: {'observation_period_num': 38, 'train_rates': 0.9239738906800434, 'learning_rate': 5.698478792788743e-06, 'batch_size': 132, 'step_size': 7, 'gamma': 0.8348145030693095}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:08:31,601][0m Trial 48 finished with value: 0.3487255573272705 and parameters: {'observation_period_num': 73, 'train_rates': 0.945122765873226, 'learning_rate': 1.3314448290991043e-05, 'batch_size': 209, 'step_size': 13, 'gamma': 0.9855050132816676}. Best is trial 27 with value: 0.0851566381752491.[0m
[32m[I 2025-01-03 09:11:22,118][0m Trial 49 finished with value: 0.10194093801758507 and parameters: {'observation_period_num': 5, 'train_rates': 0.9703738559600751, 'learning_rate': 0.00019252760042874064, 'batch_size': 24, 'step_size': 10, 'gamma': 0.8169626144816757}. Best is trial 27 with value: 0.0851566381752491.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 09:11:22,125][0m A new study created in memory with name: no-name-f687a3bf-3184-4a2e-8e3d-53e1092eb514[0m
[32m[I 2025-01-03 09:11:50,049][0m Trial 0 finished with value: 0.40660477895748437 and parameters: {'observation_period_num': 19, 'train_rates': 0.792321169323795, 'learning_rate': 0.00013700353961596052, 'batch_size': 229, 'step_size': 5, 'gamma': 0.8270320338272444}. Best is trial 0 with value: 0.40660477895748437.[0m
[32m[I 2025-01-03 09:12:55,467][0m Trial 1 finished with value: 1.1262547301168138 and parameters: {'observation_period_num': 55, 'train_rates': 0.6755306807730005, 'learning_rate': 2.8527181930566094e-06, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9056868689709775}. Best is trial 0 with value: 0.40660477895748437.[0m
[32m[I 2025-01-03 09:17:06,685][0m Trial 2 finished with value: 1.5397799449442484 and parameters: {'observation_period_num': 214, 'train_rates': 0.6081595413457594, 'learning_rate': 5.136614661140483e-06, 'batch_size': 193, 'step_size': 8, 'gamma': 0.8567549212516543}. Best is trial 0 with value: 0.40660477895748437.[0m
[32m[I 2025-01-03 09:19:07,354][0m Trial 3 finished with value: 1.4316552446789719 and parameters: {'observation_period_num': 93, 'train_rates': 0.8539246401008295, 'learning_rate': 1.0123258243617312e-06, 'batch_size': 137, 'step_size': 14, 'gamma': 0.7936827065922057}. Best is trial 0 with value: 0.40660477895748437.[0m
[32m[I 2025-01-03 09:24:06,748][0m Trial 4 finished with value: 0.5699094838170863 and parameters: {'observation_period_num': 220, 'train_rates': 0.7547503234636119, 'learning_rate': 0.00021480218592726614, 'batch_size': 204, 'step_size': 3, 'gamma': 0.8898506937058835}. Best is trial 0 with value: 0.40660477895748437.[0m
[32m[I 2025-01-03 09:25:22,837][0m Trial 5 finished with value: 0.31124233348029 and parameters: {'observation_period_num': 18, 'train_rates': 0.7853469352556046, 'learning_rate': 6.761132513362197e-05, 'batch_size': 45, 'step_size': 12, 'gamma': 0.7538731438634438}. Best is trial 5 with value: 0.31124233348029.[0m
[32m[I 2025-01-03 09:26:29,434][0m Trial 6 finished with value: 0.5237407146301944 and parameters: {'observation_period_num': 50, 'train_rates': 0.9222418661987004, 'learning_rate': 7.271129749673383e-06, 'batch_size': 207, 'step_size': 8, 'gamma': 0.9371011399180712}. Best is trial 5 with value: 0.31124233348029.[0m
[32m[I 2025-01-03 09:27:40,564][0m Trial 7 finished with value: 0.6282426758196341 and parameters: {'observation_period_num': 64, 'train_rates': 0.7354121155888753, 'learning_rate': 0.0007644607950418457, 'batch_size': 167, 'step_size': 9, 'gamma': 0.7590651638274771}. Best is trial 5 with value: 0.31124233348029.[0m
[32m[I 2025-01-03 09:31:33,460][0m Trial 8 finished with value: 0.3469044081866741 and parameters: {'observation_period_num': 168, 'train_rates': 0.8476034311618421, 'learning_rate': 0.00022976346795483497, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8840418234579042}. Best is trial 5 with value: 0.31124233348029.[0m
Early stopping at epoch 59
[32m[I 2025-01-03 09:33:12,449][0m Trial 9 finished with value: 0.5019364674401477 and parameters: {'observation_period_num': 115, 'train_rates': 0.9566155631030543, 'learning_rate': 0.0001916900642654946, 'batch_size': 88, 'step_size': 1, 'gamma': 0.7952876346804926}. Best is trial 5 with value: 0.31124233348029.[0m
[32m[I 2025-01-03 09:37:20,129][0m Trial 10 finished with value: 0.22938048622521365 and parameters: {'observation_period_num': 157, 'train_rates': 0.8723767814498175, 'learning_rate': 3.0772204695915894e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9813480112971262}. Best is trial 10 with value: 0.22938048622521365.[0m
[32m[I 2025-01-03 09:41:53,672][0m Trial 11 finished with value: 0.3328777268796624 and parameters: {'observation_period_num': 163, 'train_rates': 0.880505887118853, 'learning_rate': 3.2775400386512455e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9694652333222977}. Best is trial 10 with value: 0.22938048622521365.[0m
[32m[I 2025-01-03 09:46:25,862][0m Trial 12 finished with value: 0.09758699791772026 and parameters: {'observation_period_num': 155, 'train_rates': 0.9877637410505005, 'learning_rate': 2.948840452398173e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9791911152998297}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 09:51:08,882][0m Trial 13 finished with value: 0.1024380478148277 and parameters: {'observation_period_num': 160, 'train_rates': 0.9863709342968604, 'learning_rate': 1.8247939883101387e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9872085462880114}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 09:56:08,984][0m Trial 14 finished with value: 0.12285388696652193 and parameters: {'observation_period_num': 190, 'train_rates': 0.9767391629313856, 'learning_rate': 1.542889576836132e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9451842052128993}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 09:59:41,309][0m Trial 15 finished with value: 0.1649211198091507 and parameters: {'observation_period_num': 138, 'train_rates': 0.9896688580612522, 'learning_rate': 1.662666246661288e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9321546113008702}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:06:13,418][0m Trial 16 finished with value: 0.1513023976116411 and parameters: {'observation_period_num': 245, 'train_rates': 0.9084114083791713, 'learning_rate': 5.982975510274782e-05, 'batch_size': 42, 'step_size': 12, 'gamma': 0.9786875002245834}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:08:35,476][0m Trial 17 finished with value: 0.31819743632573133 and parameters: {'observation_period_num': 101, 'train_rates': 0.9403136846026068, 'learning_rate': 1.0393897823836234e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.9522144132988106}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:12:00,507][0m Trial 18 finished with value: 0.6948179257666971 and parameters: {'observation_period_num': 141, 'train_rates': 0.9096587494825517, 'learning_rate': 2.5378942366727226e-06, 'batch_size': 107, 'step_size': 12, 'gamma': 0.9193164483015449}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:17:01,963][0m Trial 19 finished with value: 0.10620134323835373 and parameters: {'observation_period_num': 189, 'train_rates': 0.9812840123996572, 'learning_rate': 3.594261764004152e-05, 'batch_size': 62, 'step_size': 10, 'gamma': 0.9894636122017673}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:21:24,301][0m Trial 20 finished with value: 0.30561084653041803 and parameters: {'observation_period_num': 190, 'train_rates': 0.8225627927683187, 'learning_rate': 0.0006608681873086732, 'batch_size': 253, 'step_size': 6, 'gamma': 0.8492398030566413}. Best is trial 12 with value: 0.09758699791772026.[0m
[32m[I 2025-01-03 10:27:14,933][0m Trial 21 finished with value: 0.09181208574566348 and parameters: {'observation_period_num': 194, 'train_rates': 0.9894826081507747, 'learning_rate': 3.074878363177102e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9846438940431447}. Best is trial 21 with value: 0.09181208574566348.[0m
[32m[I 2025-01-03 10:33:22,258][0m Trial 22 finished with value: 0.1658244088187918 and parameters: {'observation_period_num': 213, 'train_rates': 0.9478764018652875, 'learning_rate': 1.6269616068988355e-05, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9593117818854494}. Best is trial 21 with value: 0.09181208574566348.[0m
[32m[I 2025-01-03 10:40:11,988][0m Trial 23 finished with value: 0.188316548899026 and parameters: {'observation_period_num': 246, 'train_rates': 0.9475824748547845, 'learning_rate': 5.8018264676824687e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9899740761865587}. Best is trial 21 with value: 0.09181208574566348.[0m
[32m[I 2025-01-03 10:44:28,915][0m Trial 24 finished with value: 0.18485729011797136 and parameters: {'observation_period_num': 174, 'train_rates': 0.8998021972364514, 'learning_rate': 2.726930428149399e-05, 'batch_size': 63, 'step_size': 11, 'gamma': 0.9597625492986978}. Best is trial 21 with value: 0.09181208574566348.[0m
[32m[I 2025-01-03 10:47:45,290][0m Trial 25 finished with value: 0.07550511509180069 and parameters: {'observation_period_num': 123, 'train_rates': 0.9889949330638821, 'learning_rate': 9.907810625451581e-05, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9174181152044592}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 10:50:27,324][0m Trial 26 finished with value: 0.1777371418442619 and parameters: {'observation_period_num': 116, 'train_rates': 0.9375423709215849, 'learning_rate': 0.00010883428998727879, 'batch_size': 103, 'step_size': 13, 'gamma': 0.9164242499450442}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 10:53:48,962][0m Trial 27 finished with value: 0.6213291256698137 and parameters: {'observation_period_num': 128, 'train_rates': 0.9658226433537203, 'learning_rate': 0.00044731461438186145, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8958726546271203}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 10:55:39,463][0m Trial 28 finished with value: 0.5982403715655986 and parameters: {'observation_period_num': 89, 'train_rates': 0.6860122623090241, 'learning_rate': 8.513643715318551e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.9301728178178624}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 10:59:12,352][0m Trial 29 finished with value: 0.1828726451128524 and parameters: {'observation_period_num': 146, 'train_rates': 0.8925704541529068, 'learning_rate': 0.00011660712509919923, 'batch_size': 69, 'step_size': 13, 'gamma': 0.8755526504164984}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:04:38,898][0m Trial 30 finished with value: 0.6141551255437606 and parameters: {'observation_period_num': 228, 'train_rates': 0.8085734130601874, 'learning_rate': 0.0003680970076259026, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9619503127030478}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:09:56,791][0m Trial 31 finished with value: 0.09167188576289585 and parameters: {'observation_period_num': 182, 'train_rates': 0.9874089854550557, 'learning_rate': 4.515955117091274e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.9729625671667371}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:15:07,196][0m Trial 32 finished with value: 0.14570458059750716 and parameters: {'observation_period_num': 195, 'train_rates': 0.9254841888446952, 'learning_rate': 4.520473759713264e-05, 'batch_size': 37, 'step_size': 9, 'gamma': 0.9663154863911001}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:20:17,367][0m Trial 33 finished with value: 0.1346390192369813 and parameters: {'observation_period_num': 176, 'train_rates': 0.9631128399318455, 'learning_rate': 2.417223503319022e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9080971499451264}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:24:21,149][0m Trial 34 finished with value: 0.9807629335610119 and parameters: {'observation_period_num': 201, 'train_rates': 0.626023370801512, 'learning_rate': 1.0331375432902135e-05, 'batch_size': 50, 'step_size': 11, 'gamma': 0.974228177533876}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:27:38,443][0m Trial 35 finished with value: 0.16891780728047048 and parameters: {'observation_period_num': 126, 'train_rates': 0.9615645290806897, 'learning_rate': 4.5832204912202824e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.9452331573924604}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:31:19,782][0m Trial 36 finished with value: 0.15184530057013035 and parameters: {'observation_period_num': 149, 'train_rates': 0.928642064859185, 'learning_rate': 0.00014878594054822298, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8537879253479522}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:33:21,523][0m Trial 37 finished with value: 0.0905865877866745 and parameters: {'observation_period_num': 75, 'train_rates': 0.9862884240440666, 'learning_rate': 8.720557786990422e-05, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9242635996632755}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:34:45,066][0m Trial 38 finished with value: 0.5439552723557926 and parameters: {'observation_period_num': 71, 'train_rates': 0.716907075593053, 'learning_rate': 0.0001466610195074959, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9221802683763404}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:35:57,693][0m Trial 39 finished with value: 0.2332287129635612 and parameters: {'observation_period_num': 26, 'train_rates': 0.8524047203446348, 'learning_rate': 8.047618172356982e-05, 'batch_size': 50, 'step_size': 8, 'gamma': 0.8991187548122612}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:37:47,722][0m Trial 40 finished with value: 0.3673283547983257 and parameters: {'observation_period_num': 90, 'train_rates': 0.7715750166071776, 'learning_rate': 0.00029801026388140536, 'batch_size': 123, 'step_size': 10, 'gamma': 0.818336593229463}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:39:57,405][0m Trial 41 finished with value: 0.10289384705516008 and parameters: {'observation_period_num': 46, 'train_rates': 0.9778850151977114, 'learning_rate': 4.688381837614018e-05, 'batch_size': 31, 'step_size': 9, 'gamma': 0.9458512231485132}. Best is trial 25 with value: 0.07550511509180069.[0m
[32m[I 2025-01-03 11:41:29,893][0m Trial 42 finished with value: 0.05205507576465607 and parameters: {'observation_period_num': 6, 'train_rates': 0.9889632755230612, 'learning_rate': 8.771818578909478e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9751930038944867}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:43:34,759][0m Trial 43 finished with value: 0.1877913652756892 and parameters: {'observation_period_num': 76, 'train_rates': 0.9621119055562022, 'learning_rate': 9.116927019520885e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.9549765609598491}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:44:27,607][0m Trial 44 finished with value: 0.13805980919986158 and parameters: {'observation_period_num': 19, 'train_rates': 0.9243086844206213, 'learning_rate': 0.00019925242415610322, 'batch_size': 90, 'step_size': 8, 'gamma': 0.9354816591817624}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:45:34,811][0m Trial 45 finished with value: 0.1273407471298382 and parameters: {'observation_period_num': 6, 'train_rates': 0.9489017896868088, 'learning_rate': 7.204297841822257e-05, 'batch_size': 70, 'step_size': 12, 'gamma': 0.8709179951965439}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:46:28,480][0m Trial 46 finished with value: 0.20595113933086395 and parameters: {'observation_period_num': 38, 'train_rates': 0.9682717302192497, 'learning_rate': 0.0001578474089062446, 'batch_size': 194, 'step_size': 13, 'gamma': 0.9696452784474412}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:49:24,849][0m Trial 47 finished with value: 0.07449797665079434 and parameters: {'observation_period_num': 105, 'train_rates': 0.988490931975437, 'learning_rate': 0.00010714275407691662, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8384377642286431}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:52:18,175][0m Trial 48 finished with value: 0.44013381004333496 and parameters: {'observation_period_num': 110, 'train_rates': 0.8866253061339845, 'learning_rate': 0.00027023856731220834, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8364063420099388}. Best is trial 42 with value: 0.05205507576465607.[0m
[32m[I 2025-01-03 11:53:47,276][0m Trial 49 finished with value: 0.26933206989413305 and parameters: {'observation_period_num': 62, 'train_rates': 0.8641555290415764, 'learning_rate': 0.00010466825990675638, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8173710324722729}. Best is trial 42 with value: 0.05205507576465607.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 11:53:47,284][0m A new study created in memory with name: no-name-4f691e1a-300b-480a-bbda-87003554066e[0m
[32m[I 2025-01-03 11:59:25,426][0m Trial 0 finished with value: 0.10842393338680267 and parameters: {'observation_period_num': 210, 'train_rates': 0.9838998744360512, 'learning_rate': 8.544591689516725e-05, 'batch_size': 189, 'step_size': 12, 'gamma': 0.9211499250709538}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:00:05,670][0m Trial 1 finished with value: 0.4601943441768632 and parameters: {'observation_period_num': 29, 'train_rates': 0.9063038319163002, 'learning_rate': 9.613078943082805e-06, 'batch_size': 223, 'step_size': 11, 'gamma': 0.8758609228426044}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:03:41,152][0m Trial 2 finished with value: 0.41529133915901184 and parameters: {'observation_period_num': 145, 'train_rates': 0.9542741610824481, 'learning_rate': 1.335025829332329e-05, 'batch_size': 212, 'step_size': 12, 'gamma': 0.8723966235251182}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:06:09,415][0m Trial 3 finished with value: 0.9679853229370332 and parameters: {'observation_period_num': 121, 'train_rates': 0.7179747467628513, 'learning_rate': 0.00041530022201028387, 'batch_size': 51, 'step_size': 12, 'gamma': 0.98359733813323}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:08:32,038][0m Trial 4 finished with value: 0.6837704129143182 and parameters: {'observation_period_num': 123, 'train_rates': 0.711040577794211, 'learning_rate': 2.669134018087041e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9053806869608894}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:10:47,527][0m Trial 5 finished with value: 0.5220576236551663 and parameters: {'observation_period_num': 103, 'train_rates': 0.851301576452846, 'learning_rate': 0.0005791979258704806, 'batch_size': 95, 'step_size': 11, 'gamma': 0.7843770887657383}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:16:47,443][0m Trial 6 finished with value: 0.33356578426158173 and parameters: {'observation_period_num': 231, 'train_rates': 0.9138884075468137, 'learning_rate': 8.051013208345408e-05, 'batch_size': 161, 'step_size': 3, 'gamma': 0.7502438806990718}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:19:38,616][0m Trial 7 finished with value: 0.5472796411369723 and parameters: {'observation_period_num': 130, 'train_rates': 0.7983377672444505, 'learning_rate': 0.0001375322927636158, 'batch_size': 145, 'step_size': 6, 'gamma': 0.7850215726947256}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:21:34,357][0m Trial 8 finished with value: 0.1526452898979187 and parameters: {'observation_period_num': 80, 'train_rates': 0.9696162708342699, 'learning_rate': 7.752379259106519e-05, 'batch_size': 98, 'step_size': 15, 'gamma': 0.9604020900060072}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:26:56,425][0m Trial 9 finished with value: 0.33864630170918386 and parameters: {'observation_period_num': 218, 'train_rates': 0.8404329530494404, 'learning_rate': 0.0003956581512722846, 'batch_size': 67, 'step_size': 3, 'gamma': 0.8716247728933837}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:30:28,049][0m Trial 10 finished with value: 1.6289354965567044 and parameters: {'observation_period_num': 188, 'train_rates': 0.6038192950724426, 'learning_rate': 1.3866853648305356e-06, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9530592032080144}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:31:48,639][0m Trial 11 finished with value: 0.13496750593185425 and parameters: {'observation_period_num': 58, 'train_rates': 0.9791825239285297, 'learning_rate': 7.982851553589373e-05, 'batch_size': 180, 'step_size': 14, 'gamma': 0.9355065743239274}. Best is trial 0 with value: 0.10842393338680267.[0m
[32m[I 2025-01-03 12:32:15,475][0m Trial 12 finished with value: 0.09357591718435287 and parameters: {'observation_period_num': 10, 'train_rates': 0.9857613932140004, 'learning_rate': 0.00010809072436970056, 'batch_size': 182, 'step_size': 9, 'gamma': 0.9226328828840273}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:32:39,874][0m Trial 13 finished with value: 0.13575521066196058 and parameters: {'observation_period_num': 8, 'train_rates': 0.88837298121025, 'learning_rate': 0.00017451329556731815, 'batch_size': 195, 'step_size': 8, 'gamma': 0.9131633190437404}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:36:47,942][0m Trial 14 finished with value: 0.2812868654727936 and parameters: {'observation_period_num': 164, 'train_rates': 0.9719849951203902, 'learning_rate': 2.8124006958666174e-05, 'batch_size': 249, 'step_size': 8, 'gamma': 0.8197276647177908}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:42:56,191][0m Trial 15 finished with value: 0.7328206777572632 and parameters: {'observation_period_num': 249, 'train_rates': 0.7393406484427495, 'learning_rate': 4.030596959599994e-06, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9113165756734584}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:47:31,753][0m Trial 16 finished with value: 0.15346634740179235 and parameters: {'observation_period_num': 187, 'train_rates': 0.9205638265309394, 'learning_rate': 0.00025594363471062386, 'batch_size': 172, 'step_size': 9, 'gamma': 0.9898789115836327}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:48:19,595][0m Trial 17 finished with value: 1.2807214177958828 and parameters: {'observation_period_num': 43, 'train_rates': 0.6008610234143154, 'learning_rate': 6.0915563692488515e-05, 'batch_size': 124, 'step_size': 6, 'gamma': 0.8360077379159001}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:49:52,969][0m Trial 18 finished with value: 1.0442985696227927 and parameters: {'observation_period_num': 72, 'train_rates': 0.8418746807873656, 'learning_rate': 1.2842261335575298e-05, 'batch_size': 213, 'step_size': 1, 'gamma': 0.9305432324046518}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:54:26,118][0m Trial 19 finished with value: 0.9671958010832729 and parameters: {'observation_period_num': 201, 'train_rates': 0.7779161948883854, 'learning_rate': 0.0009470254614530373, 'batch_size': 189, 'step_size': 13, 'gamma': 0.8396564123213045}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:57:37,412][0m Trial 20 finished with value: 0.805023187097975 and parameters: {'observation_period_num': 162, 'train_rates': 0.6696838314438986, 'learning_rate': 0.00014623126627076903, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8954490804064562}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:58:01,509][0m Trial 21 finished with value: 0.1743265986442566 and parameters: {'observation_period_num': 6, 'train_rates': 0.9517459712478756, 'learning_rate': 5.014471625178952e-05, 'batch_size': 179, 'step_size': 14, 'gamma': 0.9383691943959405}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 12:59:07,341][0m Trial 22 finished with value: 0.15231797099113464 and parameters: {'observation_period_num': 48, 'train_rates': 0.988419241094847, 'learning_rate': 4.149083744200716e-05, 'batch_size': 227, 'step_size': 13, 'gamma': 0.9326772974499875}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:00:43,216][0m Trial 23 finished with value: 0.171239972114563 and parameters: {'observation_period_num': 70, 'train_rates': 0.9365880413394653, 'learning_rate': 0.00012462555204079907, 'batch_size': 196, 'step_size': 13, 'gamma': 0.9633433284573374}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:01:25,333][0m Trial 24 finished with value: 0.1567008376121521 and parameters: {'observation_period_num': 30, 'train_rates': 0.8853152612047852, 'learning_rate': 0.0002479108325249932, 'batch_size': 164, 'step_size': 14, 'gamma': 0.8954136694662994}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:03:42,467][0m Trial 25 finished with value: 0.22957853972911835 and parameters: {'observation_period_num': 97, 'train_rates': 0.9776066660462768, 'learning_rate': 2.107324773509396e-05, 'batch_size': 201, 'step_size': 9, 'gamma': 0.9222649742358027}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:04:59,560][0m Trial 26 finished with value: 0.1332060694694519 and parameters: {'observation_period_num': 53, 'train_rates': 0.9883685677849499, 'learning_rate': 9.87128296951355e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.9478217027569719}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:05:39,536][0m Trial 27 finished with value: 0.23068085534775512 and parameters: {'observation_period_num': 26, 'train_rates': 0.8716113432742051, 'learning_rate': 0.0002501349675990136, 'batch_size': 116, 'step_size': 7, 'gamma': 0.9490090655243898}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:07:56,982][0m Trial 28 finished with value: 0.17070887681473507 and parameters: {'observation_period_num': 98, 'train_rates': 0.937666052335527, 'learning_rate': 0.00010120312233849375, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9726829398451841}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:08:28,549][0m Trial 29 finished with value: 0.6551101782659846 and parameters: {'observation_period_num': 21, 'train_rates': 0.9134834945754685, 'learning_rate': 5.944517428790756e-06, 'batch_size': 231, 'step_size': 10, 'gamma': 0.8548852346764554}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:09:16,481][0m Trial 30 finished with value: 0.37275057858053073 and parameters: {'observation_period_num': 37, 'train_rates': 0.8022145510764751, 'learning_rate': 4.87035985255368e-05, 'batch_size': 153, 'step_size': 12, 'gamma': 0.8872507065703461}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:10:36,961][0m Trial 31 finished with value: 0.19115103781223297 and parameters: {'observation_period_num': 59, 'train_rates': 0.9517101157017155, 'learning_rate': 8.293776821143314e-05, 'batch_size': 177, 'step_size': 14, 'gamma': 0.9439977473699153}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:11:57,113][0m Trial 32 finished with value: 0.13542985916137695 and parameters: {'observation_period_num': 55, 'train_rates': 0.978966188054152, 'learning_rate': 3.433213759610364e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9237615744124537}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:14:05,329][0m Trial 33 finished with value: 0.1319279670715332 and parameters: {'observation_period_num': 85, 'train_rates': 0.9892926603069849, 'learning_rate': 1.792450504738416e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9700478766872157}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:16:43,225][0m Trial 34 finished with value: 0.2049165316015841 and parameters: {'observation_period_num': 109, 'train_rates': 0.9419101862160573, 'learning_rate': 1.7821123262364628e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.9755820559276148}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:20:21,963][0m Trial 35 finished with value: 0.2614293320636664 and parameters: {'observation_period_num': 143, 'train_rates': 0.9602496991976771, 'learning_rate': 1.0013751809876617e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.967783149506365}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:22:16,342][0m Trial 36 finished with value: 0.38082121340213 and parameters: {'observation_period_num': 83, 'train_rates': 0.9248840001998623, 'learning_rate': 6.271046308938614e-06, 'batch_size': 133, 'step_size': 11, 'gamma': 0.9529944073085456}. Best is trial 12 with value: 0.09357591718435287.[0m
[32m[I 2025-01-03 13:24:04,428][0m Trial 37 finished with value: 0.08369731158018112 and parameters: {'observation_period_num': 16, 'train_rates': 0.9871987692010593, 'learning_rate': 1.8270241037436968e-05, 'batch_size': 40, 'step_size': 12, 'gamma': 0.983234441915445}. Best is trial 37 with value: 0.08369731158018112.[0m
[32m[I 2025-01-03 13:26:13,713][0m Trial 38 finished with value: 0.3022998917773859 and parameters: {'observation_period_num': 11, 'train_rates': 0.8974826376990457, 'learning_rate': 1.9571489264444277e-06, 'batch_size': 30, 'step_size': 12, 'gamma': 0.9847584295790515}. Best is trial 37 with value: 0.08369731158018112.[0m
[32m[I 2025-01-03 13:30:11,523][0m Trial 39 finished with value: 0.25501861156673605 and parameters: {'observation_period_num': 153, 'train_rates': 0.9610648914040283, 'learning_rate': 1.5869848883555825e-05, 'batch_size': 46, 'step_size': 5, 'gamma': 0.8832348959553274}. Best is trial 37 with value: 0.08369731158018112.[0m
[32m[I 2025-01-03 13:33:16,919][0m Trial 40 finished with value: 0.24347925201558943 and parameters: {'observation_period_num': 132, 'train_rates': 0.8627944588631589, 'learning_rate': 3.000687930640266e-05, 'batch_size': 80, 'step_size': 13, 'gamma': 0.9779693391681189}. Best is trial 37 with value: 0.08369731158018112.[0m
[32m[I 2025-01-03 13:34:44,789][0m Trial 41 finished with value: 0.07842573523521423 and parameters: {'observation_period_num': 19, 'train_rates': 0.9879728176997175, 'learning_rate': 0.00019584153967016805, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9562493057902612}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:36:16,884][0m Trial 42 finished with value: 0.20104919373989105 and parameters: {'observation_period_num': 21, 'train_rates': 0.9897890321471938, 'learning_rate': 0.0003789975690681456, 'batch_size': 44, 'step_size': 10, 'gamma': 0.9640694984280013}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:37:27,048][0m Trial 43 finished with value: 0.17858085030411172 and parameters: {'observation_period_num': 37, 'train_rates': 0.956484580143291, 'learning_rate': 2.152776859749971e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.9612642363069659}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:39:31,065][0m Trial 44 finished with value: 0.12606990762344264 and parameters: {'observation_period_num': 22, 'train_rates': 0.9297671340723622, 'learning_rate': 0.00018907606462381052, 'batch_size': 33, 'step_size': 12, 'gamma': 0.9179657037917027}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:41:38,134][0m Trial 45 finished with value: 2.395350305401549 and parameters: {'observation_period_num': 18, 'train_rates': 0.9332033896920183, 'learning_rate': 0.0006151822931297875, 'batch_size': 32, 'step_size': 7, 'gamma': 0.9166249081853127}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:44:28,903][0m Trial 46 finished with value: 0.13392546158475974 and parameters: {'observation_period_num': 36, 'train_rates': 0.9669570176298117, 'learning_rate': 0.0002081762803307397, 'batch_size': 23, 'step_size': 10, 'gamma': 0.9028992498161429}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:50:22,051][0m Trial 47 finished with value: 0.1965812984467542 and parameters: {'observation_period_num': 221, 'train_rates': 0.9119023766302067, 'learning_rate': 0.0003386459763650714, 'batch_size': 38, 'step_size': 11, 'gamma': 0.9277444786934531}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 13:53:56,335][0m Trial 48 finished with value: 0.20926108650206177 and parameters: {'observation_period_num': 14, 'train_rates': 0.8187312489384528, 'learning_rate': 6.403755412282046e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8665647776939285}. Best is trial 41 with value: 0.07842573523521423.[0m
[32m[I 2025-01-03 14:00:27,647][0m Trial 49 finished with value: 0.1825397350644105 and parameters: {'observation_period_num': 242, 'train_rates': 0.9451280664025123, 'learning_rate': 0.00016190463948462068, 'batch_size': 60, 'step_size': 15, 'gamma': 0.9052987944795902}. Best is trial 41 with value: 0.07842573523521423.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-03 14:00:27,654][0m A new study created in memory with name: no-name-0b92c22a-40d1-4f12-b6c6-68e3cd8e4774[0m
[32m[I 2025-01-03 14:02:23,019][0m Trial 0 finished with value: 0.32415970210922496 and parameters: {'observation_period_num': 84, 'train_rates': 0.9375834027851722, 'learning_rate': 1.0659295901239974e-05, 'batch_size': 144, 'step_size': 12, 'gamma': 0.9851454861637249}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:04:27,338][0m Trial 1 finished with value: 1.3890841147809658 and parameters: {'observation_period_num': 114, 'train_rates': 0.6637834014783601, 'learning_rate': 2.2042061081193356e-06, 'batch_size': 247, 'step_size': 10, 'gamma': 0.9659981788528333}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:07:14,860][0m Trial 2 finished with value: 0.9869289055419045 and parameters: {'observation_period_num': 142, 'train_rates': 0.665863741183583, 'learning_rate': 2.2467346761363632e-05, 'batch_size': 91, 'step_size': 4, 'gamma': 0.7696776235768439}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:12:38,029][0m Trial 3 finished with value: 0.7475809928147868 and parameters: {'observation_period_num': 234, 'train_rates': 0.7446036765081383, 'learning_rate': 1.085422186348005e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.8876335978224842}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:14:50,581][0m Trial 4 finished with value: 0.5510513348249712 and parameters: {'observation_period_num': 107, 'train_rates': 0.7255409325444451, 'learning_rate': 8.476168068304352e-05, 'batch_size': 52, 'step_size': 11, 'gamma': 0.9738404116369168}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:15:13,726][0m Trial 5 finished with value: 1.9782854802537673 and parameters: {'observation_period_num': 16, 'train_rates': 0.6904134243855158, 'learning_rate': 1.3175447291791687e-06, 'batch_size': 162, 'step_size': 6, 'gamma': 0.8250562275576061}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:19:29,529][0m Trial 6 finished with value: 0.44945029660742336 and parameters: {'observation_period_num': 175, 'train_rates': 0.9323457480435473, 'learning_rate': 3.1157327523941745e-05, 'batch_size': 115, 'step_size': 1, 'gamma': 0.9396684963715782}. Best is trial 0 with value: 0.32415970210922496.[0m
[32m[I 2025-01-03 14:24:00,026][0m Trial 7 finished with value: 0.23833097982105245 and parameters: {'observation_period_num': 188, 'train_rates': 0.8714657027840862, 'learning_rate': 4.937234049476185e-05, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8577750001429432}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:28:18,489][0m Trial 8 finished with value: 1.1010415095108501 and parameters: {'observation_period_num': 205, 'train_rates': 0.6308547314784432, 'learning_rate': 7.70338792390161e-06, 'batch_size': 29, 'step_size': 3, 'gamma': 0.8719679135534742}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:33:28,494][0m Trial 9 finished with value: 2.23493814384532 and parameters: {'observation_period_num': 230, 'train_rates': 0.6864595178825981, 'learning_rate': 0.0008204689262146562, 'batch_size': 30, 'step_size': 8, 'gamma': 0.9798718435039547}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:34:28,241][0m Trial 10 finished with value: 0.23935575490435138 and parameters: {'observation_period_num': 48, 'train_rates': 0.8424676634020029, 'learning_rate': 0.00014396248323643484, 'batch_size': 190, 'step_size': 15, 'gamma': 0.8142565745405246}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:35:24,463][0m Trial 11 finished with value: 0.24109322521193274 and parameters: {'observation_period_num': 44, 'train_rates': 0.8505489281876687, 'learning_rate': 0.0001867980302336774, 'batch_size': 207, 'step_size': 15, 'gamma': 0.8113898623908781}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:39:37,611][0m Trial 12 finished with value: 0.3077245921776767 and parameters: {'observation_period_num': 185, 'train_rates': 0.828863979365492, 'learning_rate': 0.0001391383611969295, 'batch_size': 182, 'step_size': 15, 'gamma': 0.8286786550946125}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:40:52,696][0m Trial 13 finished with value: 0.25395246148109435 and parameters: {'observation_period_num': 56, 'train_rates': 0.8793348599873172, 'learning_rate': 0.0004588452483372611, 'batch_size': 105, 'step_size': 13, 'gamma': 0.7515835236325433}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:44:05,462][0m Trial 14 finished with value: 0.5561814951557779 and parameters: {'observation_period_num': 148, 'train_rates': 0.7867667036724476, 'learning_rate': 6.890061317504247e-05, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9043588490176276}. Best is trial 7 with value: 0.23833097982105245.[0m
[32m[I 2025-01-03 14:44:59,611][0m Trial 15 finished with value: 0.13046510041924952 and parameters: {'observation_period_num': 10, 'train_rates': 0.8957858535225417, 'learning_rate': 0.0003056756484174742, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8403224970602615}. Best is trial 15 with value: 0.13046510041924952.[0m
[32m[I 2025-01-03 14:46:00,160][0m Trial 16 finished with value: 0.15061697363853455 and parameters: {'observation_period_num': 13, 'train_rates': 0.9770139578772644, 'learning_rate': 0.0004214653782790058, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8467522346881105}. Best is trial 15 with value: 0.13046510041924952.[0m
[32m[I 2025-01-03 14:46:57,370][0m Trial 17 finished with value: 0.07296715676784515 and parameters: {'observation_period_num': 5, 'train_rates': 0.9839643355486101, 'learning_rate': 0.0003422112368170394, 'batch_size': 78, 'step_size': 9, 'gamma': 0.8481630314364652}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:48:51,200][0m Trial 18 finished with value: 0.5611521601676941 and parameters: {'observation_period_num': 80, 'train_rates': 0.9896262311201195, 'learning_rate': 0.0008704610375947194, 'batch_size': 128, 'step_size': 7, 'gamma': 0.7865552259035826}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:49:36,029][0m Trial 19 finished with value: 0.12240925918758651 and parameters: {'observation_period_num': 6, 'train_rates': 0.9176296769347955, 'learning_rate': 0.00024854211362082724, 'batch_size': 89, 'step_size': 9, 'gamma': 0.916521325017632}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:50:24,048][0m Trial 20 finished with value: 0.14698335840258486 and parameters: {'observation_period_num': 34, 'train_rates': 0.9116061984630146, 'learning_rate': 0.00027689928767386535, 'batch_size': 148, 'step_size': 5, 'gamma': 0.9212498861645729}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:51:12,871][0m Trial 21 finished with value: 0.17056514754377564 and parameters: {'observation_period_num': 7, 'train_rates': 0.950758520158942, 'learning_rate': 0.000369300268292365, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8793209939440995}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:52:56,778][0m Trial 22 finished with value: 0.1485904903740299 and parameters: {'observation_period_num': 70, 'train_rates': 0.9150665535216946, 'learning_rate': 0.00022688060676908126, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8488949118695107}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:53:43,622][0m Trial 23 finished with value: 0.6831592170498039 and parameters: {'observation_period_num': 28, 'train_rates': 0.8919772296728503, 'learning_rate': 0.0006159579715014837, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9337404840461834}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:54:20,610][0m Trial 24 finished with value: 0.17954331636428833 and parameters: {'observation_period_num': 7, 'train_rates': 0.9610305446991423, 'learning_rate': 0.00012375995922817996, 'batch_size': 121, 'step_size': 10, 'gamma': 0.911109038748199}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:55:08,918][0m Trial 25 finished with value: 0.3181126743606541 and parameters: {'observation_period_num': 31, 'train_rates': 0.8019453657990171, 'learning_rate': 0.0002744799047923578, 'batch_size': 86, 'step_size': 8, 'gamma': 0.7967925622186466}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 14:59:20,218][0m Trial 26 finished with value: 0.16661069814174895 and parameters: {'observation_period_num': 65, 'train_rates': 0.9129924086742157, 'learning_rate': 8.270718204184642e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8952987114605264}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:00:43,792][0m Trial 27 finished with value: 2.5345419027187206 and parameters: {'observation_period_num': 28, 'train_rates': 0.96327492496343, 'learning_rate': 0.0009587899319201381, 'batch_size': 53, 'step_size': 7, 'gamma': 0.9523242839291408}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:01:45,852][0m Trial 28 finished with value: 0.5488804255795275 and parameters: {'observation_period_num': 46, 'train_rates': 0.7986396022192643, 'learning_rate': 0.00046975221722638443, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8605127120013459}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:03:45,901][0m Trial 29 finished with value: 0.22624501207640751 and parameters: {'observation_period_num': 87, 'train_rates': 0.9327538591231385, 'learning_rate': 4.627759634222538e-05, 'batch_size': 148, 'step_size': 12, 'gamma': 0.8404505855714322}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:05:55,290][0m Trial 30 finished with value: 0.36765162339434027 and parameters: {'observation_period_num': 96, 'train_rates': 0.887943965590963, 'learning_rate': 1.3957354858302294e-05, 'batch_size': 109, 'step_size': 7, 'gamma': 0.8669918515157978}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:06:46,284][0m Trial 31 finished with value: 0.15066452809408598 and parameters: {'observation_period_num': 35, 'train_rates': 0.9126887489981703, 'learning_rate': 0.00021427749540529193, 'batch_size': 146, 'step_size': 4, 'gamma': 0.9167165404614825}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:07:19,714][0m Trial 32 finished with value: 0.16505821306597102 and parameters: {'observation_period_num': 19, 'train_rates': 0.943975958093155, 'learning_rate': 0.00029405663776870537, 'batch_size': 164, 'step_size': 5, 'gamma': 0.930605572150298}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:07:59,858][0m Trial 33 finished with value: 0.29746298582723296 and parameters: {'observation_period_num': 27, 'train_rates': 0.865248947171192, 'learning_rate': 0.0005629671624358545, 'batch_size': 134, 'step_size': 2, 'gamma': 0.9489603638838865}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:10:50,238][0m Trial 34 finished with value: 0.5348791710506141 and parameters: {'observation_period_num': 125, 'train_rates': 0.9051107107712116, 'learning_rate': 5.893741657292224e-06, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8839592968836372}. Best is trial 17 with value: 0.07296715676784515.[0m
[32m[I 2025-01-03 15:12:35,769][0m Trial 35 finished with value: 0.0541505517065525 and parameters: {'observation_period_num': 6, 'train_rates': 0.9830714172914075, 'learning_rate': 0.00011718000342070418, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9221493220438013}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:14:33,563][0m Trial 36 finished with value: 0.07679754481292687 and parameters: {'observation_period_num': 61, 'train_rates': 0.9821533206383226, 'learning_rate': 0.00010006589944206381, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8959644252115251}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:16:21,997][0m Trial 37 finished with value: 0.08348548039793968 and parameters: {'observation_period_num': 64, 'train_rates': 0.9859331005441407, 'learning_rate': 0.00010223230787313284, 'batch_size': 39, 'step_size': 11, 'gamma': 0.8937457598184607}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:19:12,470][0m Trial 38 finished with value: 0.08918670564889908 and parameters: {'observation_period_num': 106, 'train_rates': 0.9889281392893778, 'learning_rate': 5.083762431397057e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9012358411324757}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:22:59,768][0m Trial 39 finished with value: 0.17595211995972526 and parameters: {'observation_period_num': 57, 'train_rates': 0.9595866945325883, 'learning_rate': 2.841577522289613e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8842050456503946}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:24:58,896][0m Trial 40 finished with value: 0.1033344566822052 and parameters: {'observation_period_num': 75, 'train_rates': 0.981244222209464, 'learning_rate': 0.00010042958385276733, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9629369016602204}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:27:57,942][0m Trial 41 finished with value: 0.10681884735822678 and parameters: {'observation_period_num': 113, 'train_rates': 0.987831995796222, 'learning_rate': 5.753394798256533e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8967689773763303}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:30:35,753][0m Trial 42 finished with value: 0.1377676674660216 and parameters: {'observation_period_num': 100, 'train_rates': 0.9671300816945003, 'learning_rate': 3.87345059647054e-05, 'batch_size': 41, 'step_size': 10, 'gamma': 0.900918154285349}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:34:43,443][0m Trial 43 finished with value: 0.20268626792250938 and parameters: {'observation_period_num': 155, 'train_rates': 0.9434685353501882, 'learning_rate': 1.8212620358818414e-05, 'batch_size': 32, 'step_size': 12, 'gamma': 0.8739725600892396}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:37:41,340][0m Trial 44 finished with value: 0.17133408897025612 and parameters: {'observation_period_num': 121, 'train_rates': 0.9344403783268753, 'learning_rate': 0.00016282335071203343, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8913361971622128}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:41:13,067][0m Trial 45 finished with value: 0.16967275738716125 and parameters: {'observation_period_num': 133, 'train_rates': 0.9701689534620177, 'learning_rate': 0.0001162501626694112, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9309911725680787}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:43:59,298][0m Trial 46 finished with value: 0.07520492120899937 and parameters: {'observation_period_num': 92, 'train_rates': 0.9887561662539042, 'learning_rate': 7.479806465309727e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.9070447550101606}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:46:14,590][0m Trial 47 finished with value: 1.0588905092742709 and parameters: {'observation_period_num': 85, 'train_rates': 0.6076310999058419, 'learning_rate': 8.64251482152177e-05, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8609632676791497}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:47:32,116][0m Trial 48 finished with value: 0.5493362007185447 and parameters: {'observation_period_num': 58, 'train_rates': 0.7491284082486144, 'learning_rate': 6.864095647698593e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9463601646958595}. Best is trial 35 with value: 0.0541505517065525.[0m
[32m[I 2025-01-03 15:54:43,192][0m Trial 49 finished with value: 0.20018207178666042 and parameters: {'observation_period_num': 251, 'train_rates': 0.9519118391425424, 'learning_rate': 0.00017744268496546555, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9104590902365796}. Best is trial 35 with value: 0.0541505517065525.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 19, 'train_rates': 0.984090652064828, 'learning_rate': 4.57887602022304e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.923385042422348}
Epoch 1/300, trend Loss: 0.6352 | 0.7610
Epoch 2/300, trend Loss: 0.4279 | 0.5815
Epoch 3/300, trend Loss: 0.3273 | 0.5067
Epoch 4/300, trend Loss: 0.2654 | 0.4242
Epoch 5/300, trend Loss: 0.2357 | 0.3661
Epoch 6/300, trend Loss: 0.2159 | 0.3518
Epoch 7/300, trend Loss: 0.2055 | 0.3214
Epoch 8/300, trend Loss: 0.1970 | 0.3176
Epoch 9/300, trend Loss: 0.1888 | 0.2772
Epoch 10/300, trend Loss: 0.1875 | 0.2451
Epoch 11/300, trend Loss: 0.1876 | 0.2362
Epoch 12/300, trend Loss: 0.1848 | 0.2379
Epoch 13/300, trend Loss: 0.1843 | 0.2650
Epoch 14/300, trend Loss: 0.1797 | 0.2019
Epoch 15/300, trend Loss: 0.1788 | 0.2047
Epoch 16/300, trend Loss: 0.1730 | 0.1987
Epoch 17/300, trend Loss: 0.1612 | 0.1997
Epoch 18/300, trend Loss: 0.1599 | 0.1781
Epoch 19/300, trend Loss: 0.1609 | 0.1691
Epoch 20/300, trend Loss: 0.1527 | 0.1678
Epoch 21/300, trend Loss: 0.1488 | 0.1824
Epoch 22/300, trend Loss: 0.1450 | 0.1610
Epoch 23/300, trend Loss: 0.1429 | 0.1505
Epoch 24/300, trend Loss: 0.1425 | 0.1525
Epoch 25/300, trend Loss: 0.1411 | 0.1528
Epoch 26/300, trend Loss: 0.1382 | 0.1469
Epoch 27/300, trend Loss: 0.1346 | 0.1358
Epoch 28/300, trend Loss: 0.1375 | 0.1366
Epoch 29/300, trend Loss: 0.1325 | 0.1339
Epoch 30/300, trend Loss: 0.1297 | 0.1382
Epoch 31/300, trend Loss: 0.1307 | 0.1323
Epoch 32/300, trend Loss: 0.1316 | 0.1300
Epoch 33/300, trend Loss: 0.1312 | 0.1238
Epoch 34/300, trend Loss: 0.1294 | 0.1206
Epoch 35/300, trend Loss: 0.1272 | 0.1237
Epoch 36/300, trend Loss: 0.1288 | 0.1173
Epoch 37/300, trend Loss: 0.1316 | 0.1173
Epoch 38/300, trend Loss: 0.1278 | 0.1167
Epoch 39/300, trend Loss: 0.1220 | 0.1300
Epoch 40/300, trend Loss: 0.1186 | 0.1128
Epoch 41/300, trend Loss: 0.1197 | 0.1094
Epoch 42/300, trend Loss: 0.1213 | 0.1126
Epoch 43/300, trend Loss: 0.1173 | 0.1138
Epoch 44/300, trend Loss: 0.1151 | 0.1133
Epoch 45/300, trend Loss: 0.1122 | 0.0985
Epoch 46/300, trend Loss: 0.1103 | 0.0952
Epoch 47/300, trend Loss: 0.1096 | 0.0952
Epoch 48/300, trend Loss: 0.1051 | 0.1014
Epoch 49/300, trend Loss: 0.1046 | 0.1023
Epoch 50/300, trend Loss: 0.1034 | 0.0943
Epoch 51/300, trend Loss: 0.1042 | 0.0926
Epoch 52/300, trend Loss: 0.1035 | 0.0905
Epoch 53/300, trend Loss: 0.1025 | 0.0869
Epoch 54/300, trend Loss: 0.1021 | 0.0875
Epoch 55/300, trend Loss: 0.1006 | 0.0880
Epoch 56/300, trend Loss: 0.1018 | 0.0915
Epoch 57/300, trend Loss: 0.1012 | 0.0904
Epoch 58/300, trend Loss: 0.1005 | 0.0926
Epoch 59/300, trend Loss: 0.1017 | 0.0924
Epoch 60/300, trend Loss: 0.1007 | 0.0840
Epoch 61/300, trend Loss: 0.1040 | 0.0942
Epoch 62/300, trend Loss: 0.1006 | 0.0847
Epoch 63/300, trend Loss: 0.0975 | 0.0890
Epoch 64/300, trend Loss: 0.0956 | 0.0860
Epoch 65/300, trend Loss: 0.0944 | 0.0810
Epoch 66/300, trend Loss: 0.0926 | 0.0805
Epoch 67/300, trend Loss: 0.0922 | 0.0842
Epoch 68/300, trend Loss: 0.0913 | 0.0825
Epoch 69/300, trend Loss: 0.0895 | 0.0776
Epoch 70/300, trend Loss: 0.0897 | 0.0770
Epoch 71/300, trend Loss: 0.0893 | 0.0774
Epoch 72/300, trend Loss: 0.0880 | 0.0800
Epoch 73/300, trend Loss: 0.0872 | 0.0795
Epoch 74/300, trend Loss: 0.0862 | 0.0765
Epoch 75/300, trend Loss: 0.0862 | 0.0757
Epoch 76/300, trend Loss: 0.0842 | 0.0757
Epoch 77/300, trend Loss: 0.0845 | 0.0770
Epoch 78/300, trend Loss: 0.0838 | 0.0773
Epoch 79/300, trend Loss: 0.0836 | 0.0741
Epoch 80/300, trend Loss: 0.0831 | 0.0727
Epoch 81/300, trend Loss: 0.0828 | 0.0738
Epoch 82/300, trend Loss: 0.0829 | 0.0756
Epoch 83/300, trend Loss: 0.0826 | 0.0756
Epoch 84/300, trend Loss: 0.0820 | 0.0736
Epoch 85/300, trend Loss: 0.0821 | 0.0757
Epoch 86/300, trend Loss: 0.0813 | 0.0747
Epoch 87/300, trend Loss: 0.0811 | 0.0754
Epoch 88/300, trend Loss: 0.0809 | 0.0720
Epoch 89/300, trend Loss: 0.0800 | 0.0713
Epoch 90/300, trend Loss: 0.0797 | 0.0714
Epoch 91/300, trend Loss: 0.0794 | 0.0724
Epoch 92/300, trend Loss: 0.0787 | 0.0754
Epoch 93/300, trend Loss: 0.0784 | 0.0701
Epoch 94/300, trend Loss: 0.0780 | 0.0697
Epoch 95/300, trend Loss: 0.0778 | 0.0695
Epoch 96/300, trend Loss: 0.0766 | 0.0707
Epoch 97/300, trend Loss: 0.0768 | 0.0702
Epoch 98/300, trend Loss: 0.0754 | 0.0696
Epoch 99/300, trend Loss: 0.0757 | 0.0728
Epoch 100/300, trend Loss: 0.0751 | 0.0715
Epoch 101/300, trend Loss: 0.0749 | 0.0705
Epoch 102/300, trend Loss: 0.0746 | 0.0708
Epoch 103/300, trend Loss: 0.0743 | 0.0671
Epoch 104/300, trend Loss: 0.0739 | 0.0699
Epoch 105/300, trend Loss: 0.0738 | 0.0699
Epoch 106/300, trend Loss: 0.0733 | 0.0729
Epoch 107/300, trend Loss: 0.0733 | 0.0715
Epoch 108/300, trend Loss: 0.0740 | 0.0689
Epoch 109/300, trend Loss: 0.0728 | 0.0666
Epoch 110/300, trend Loss: 0.0719 | 0.0688
Epoch 111/300, trend Loss: 0.0723 | 0.0698
Epoch 112/300, trend Loss: 0.0717 | 0.0683
Epoch 113/300, trend Loss: 0.0708 | 0.0672
Epoch 114/300, trend Loss: 0.0712 | 0.0679
Epoch 115/300, trend Loss: 0.0707 | 0.0687
Epoch 116/300, trend Loss: 0.0704 | 0.0674
Epoch 117/300, trend Loss: 0.0704 | 0.0676
Epoch 118/300, trend Loss: 0.0706 | 0.0685
Epoch 119/300, trend Loss: 0.0701 | 0.0681
Epoch 120/300, trend Loss: 0.0696 | 0.0683
Epoch 121/300, trend Loss: 0.0702 | 0.0696
Epoch 122/300, trend Loss: 0.0694 | 0.0683
Epoch 123/300, trend Loss: 0.0685 | 0.0675
Epoch 124/300, trend Loss: 0.0686 | 0.0677
Epoch 125/300, trend Loss: 0.0682 | 0.0681
Epoch 126/300, trend Loss: 0.0683 | 0.0676
Epoch 127/300, trend Loss: 0.0678 | 0.0672
Epoch 128/300, trend Loss: 0.0678 | 0.0686
Epoch 129/300, trend Loss: 0.0682 | 0.0676
Epoch 130/300, trend Loss: 0.0673 | 0.0691
Epoch 131/300, trend Loss: 0.0677 | 0.0678
Epoch 132/300, trend Loss: 0.0677 | 0.0665
Epoch 133/300, trend Loss: 0.0673 | 0.0679
Epoch 134/300, trend Loss: 0.0668 | 0.0696
Epoch 135/300, trend Loss: 0.0665 | 0.0683
Epoch 136/300, trend Loss: 0.0665 | 0.0668
Epoch 137/300, trend Loss: 0.0667 | 0.0677
Epoch 138/300, trend Loss: 0.0658 | 0.0665
Epoch 139/300, trend Loss: 0.0659 | 0.0683
Epoch 140/300, trend Loss: 0.0654 | 0.0699
Epoch 141/300, trend Loss: 0.0660 | 0.0668
Epoch 142/300, trend Loss: 0.0659 | 0.0667
Epoch 143/300, trend Loss: 0.0651 | 0.0679
Epoch 144/300, trend Loss: 0.0652 | 0.0681
Epoch 145/300, trend Loss: 0.0655 | 0.0694
Epoch 146/300, trend Loss: 0.0649 | 0.0680
Epoch 147/300, trend Loss: 0.0648 | 0.0684
Epoch 148/300, trend Loss: 0.0645 | 0.0684
Epoch 149/300, trend Loss: 0.0643 | 0.0686
Epoch 150/300, trend Loss: 0.0655 | 0.0679
Epoch 151/300, trend Loss: 0.0641 | 0.0667
Epoch 152/300, trend Loss: 0.0637 | 0.0673
Epoch 153/300, trend Loss: 0.0635 | 0.0671
Epoch 154/300, trend Loss: 0.0637 | 0.0676
Epoch 155/300, trend Loss: 0.0643 | 0.0681
Epoch 156/300, trend Loss: 0.0633 | 0.0660
Epoch 157/300, trend Loss: 0.0650 | 0.0677
Epoch 158/300, trend Loss: 0.0640 | 0.0676
Epoch 159/300, trend Loss: 0.0624 | 0.0667
Epoch 160/300, trend Loss: 0.0630 | 0.0658
Epoch 161/300, trend Loss: 0.0630 | 0.0666
Epoch 162/300, trend Loss: 0.0631 | 0.0664
Epoch 163/300, trend Loss: 0.0629 | 0.0672
Epoch 164/300, trend Loss: 0.0628 | 0.0653
Epoch 165/300, trend Loss: 0.0626 | 0.0655
Epoch 166/300, trend Loss: 0.0626 | 0.0657
Epoch 167/300, trend Loss: 0.0626 | 0.0666
Epoch 168/300, trend Loss: 0.0624 | 0.0659
Epoch 169/300, trend Loss: 0.0625 | 0.0661
Epoch 170/300, trend Loss: 0.0625 | 0.0657
Epoch 171/300, trend Loss: 0.0623 | 0.0669
Epoch 172/300, trend Loss: 0.0622 | 0.0662
Epoch 173/300, trend Loss: 0.0617 | 0.0660
Epoch 174/300, trend Loss: 0.0622 | 0.0663
Epoch 175/300, trend Loss: 0.0613 | 0.0672
Epoch 176/300, trend Loss: 0.0617 | 0.0669
Epoch 177/300, trend Loss: 0.0617 | 0.0662
Epoch 178/300, trend Loss: 0.0609 | 0.0652
Epoch 179/300, trend Loss: 0.0615 | 0.0672
Epoch 180/300, trend Loss: 0.0619 | 0.0660
Epoch 181/300, trend Loss: 0.0616 | 0.0670
Epoch 182/300, trend Loss: 0.0615 | 0.0676
Epoch 183/300, trend Loss: 0.0607 | 0.0653
Epoch 184/300, trend Loss: 0.0607 | 0.0664
Epoch 185/300, trend Loss: 0.0610 | 0.0670
Epoch 186/300, trend Loss: 0.0611 | 0.0651
Epoch 187/300, trend Loss: 0.0608 | 0.0655
Epoch 188/300, trend Loss: 0.0611 | 0.0668
Epoch 189/300, trend Loss: 0.0607 | 0.0662
Epoch 190/300, trend Loss: 0.0609 | 0.0668
Epoch 191/300, trend Loss: 0.0605 | 0.0655
Epoch 192/300, trend Loss: 0.0603 | 0.0669
Epoch 193/300, trend Loss: 0.0608 | 0.0665
Epoch 194/300, trend Loss: 0.0596 | 0.0659
Epoch 195/300, trend Loss: 0.0598 | 0.0682
Epoch 196/300, trend Loss: 0.0600 | 0.0667
Epoch 197/300, trend Loss: 0.0601 | 0.0648
Epoch 198/300, trend Loss: 0.0602 | 0.0669
Epoch 199/300, trend Loss: 0.0593 | 0.0673
Epoch 200/300, trend Loss: 0.0603 | 0.0661
Epoch 201/300, trend Loss: 0.0599 | 0.0660
Epoch 202/300, trend Loss: 0.0602 | 0.0659
Epoch 203/300, trend Loss: 0.0593 | 0.0666
Epoch 204/300, trend Loss: 0.0602 | 0.0675
Epoch 205/300, trend Loss: 0.0597 | 0.0662
Epoch 206/300, trend Loss: 0.0593 | 0.0655
Epoch 207/300, trend Loss: 0.0595 | 0.0649
Epoch 208/300, trend Loss: 0.0610 | 0.0654
Epoch 209/300, trend Loss: 0.0601 | 0.0651
Epoch 210/300, trend Loss: 0.0596 | 0.0648
Epoch 211/300, trend Loss: 0.0588 | 0.0646
Epoch 212/300, trend Loss: 0.0598 | 0.0656
Epoch 213/300, trend Loss: 0.0590 | 0.0657
Epoch 214/300, trend Loss: 0.0597 | 0.0647
Epoch 215/300, trend Loss: 0.0589 | 0.0667
Epoch 216/300, trend Loss: 0.0591 | 0.0665
Epoch 217/300, trend Loss: 0.0587 | 0.0658
Epoch 218/300, trend Loss: 0.0596 | 0.0655
Epoch 219/300, trend Loss: 0.0588 | 0.0663
Epoch 220/300, trend Loss: 0.0596 | 0.0666
Epoch 221/300, trend Loss: 0.0588 | 0.0654
Epoch 222/300, trend Loss: 0.0587 | 0.0657
Epoch 223/300, trend Loss: 0.0590 | 0.0656
Epoch 224/300, trend Loss: 0.0590 | 0.0666
Epoch 225/300, trend Loss: 0.0586 | 0.0660
Epoch 226/300, trend Loss: 0.0585 | 0.0657
Epoch 227/300, trend Loss: 0.0584 | 0.0668
Epoch 228/300, trend Loss: 0.0591 | 0.0649
Epoch 229/300, trend Loss: 0.0590 | 0.0645
Epoch 230/300, trend Loss: 0.0586 | 0.0656
Epoch 231/300, trend Loss: 0.0581 | 0.0662
Epoch 232/300, trend Loss: 0.0582 | 0.0661
Epoch 233/300, trend Loss: 0.0581 | 0.0657
Epoch 234/300, trend Loss: 0.0583 | 0.0671
Epoch 235/300, trend Loss: 0.0585 | 0.0653
Epoch 236/300, trend Loss: 0.0584 | 0.0654
Epoch 237/300, trend Loss: 0.0583 | 0.0654
Epoch 238/300, trend Loss: 0.0582 | 0.0661
Epoch 239/300, trend Loss: 0.0580 | 0.0654
Epoch 240/300, trend Loss: 0.0581 | 0.0661
Epoch 241/300, trend Loss: 0.0582 | 0.0667
Epoch 242/300, trend Loss: 0.0583 | 0.0663
Epoch 243/300, trend Loss: 0.0580 | 0.0660
Epoch 244/300, trend Loss: 0.0576 | 0.0657
Epoch 245/300, trend Loss: 0.0580 | 0.0663
Epoch 246/300, trend Loss: 0.0580 | 0.0667
Epoch 247/300, trend Loss: 0.0582 | 0.0648
Epoch 248/300, trend Loss: 0.0581 | 0.0644
Epoch 249/300, trend Loss: 0.0579 | 0.0656
Epoch 250/300, trend Loss: 0.0575 | 0.0661
Epoch 251/300, trend Loss: 0.0579 | 0.0649
Epoch 252/300, trend Loss: 0.0577 | 0.0656
Epoch 253/300, trend Loss: 0.0579 | 0.0652
Epoch 254/300, trend Loss: 0.0581 | 0.0648
Epoch 255/300, trend Loss: 0.0582 | 0.0654
Epoch 256/300, trend Loss: 0.0579 | 0.0656
Epoch 257/300, trend Loss: 0.0580 | 0.0655
Epoch 258/300, trend Loss: 0.0577 | 0.0659
Epoch 259/300, trend Loss: 0.0576 | 0.0652
Epoch 260/300, trend Loss: 0.0577 | 0.0648
Epoch 261/300, trend Loss: 0.0573 | 0.0653
Epoch 262/300, trend Loss: 0.0571 | 0.0656
Epoch 263/300, trend Loss: 0.0576 | 0.0652
Epoch 264/300, trend Loss: 0.0574 | 0.0662
Epoch 265/300, trend Loss: 0.0573 | 0.0654
Epoch 266/300, trend Loss: 0.0575 | 0.0657
Epoch 267/300, trend Loss: 0.0574 | 0.0667
Epoch 268/300, trend Loss: 0.0573 | 0.0657
Epoch 269/300, trend Loss: 0.0580 | 0.0659
Epoch 270/300, trend Loss: 0.0569 | 0.0667
Epoch 271/300, trend Loss: 0.0572 | 0.0658
Epoch 272/300, trend Loss: 0.0572 | 0.0657
Epoch 273/300, trend Loss: 0.0576 | 0.0656
Epoch 274/300, trend Loss: 0.0575 | 0.0660
Epoch 275/300, trend Loss: 0.0569 | 0.0650
Epoch 276/300, trend Loss: 0.0572 | 0.0652
Epoch 277/300, trend Loss: 0.0569 | 0.0664
Epoch 278/300, trend Loss: 0.0570 | 0.0660
Epoch 279/300, trend Loss: 0.0572 | 0.0661
Epoch 280/300, trend Loss: 0.0569 | 0.0658
Epoch 281/300, trend Loss: 0.0570 | 0.0658
Epoch 282/300, trend Loss: 0.0576 | 0.0658
Epoch 283/300, trend Loss: 0.0567 | 0.0661
Epoch 284/300, trend Loss: 0.0569 | 0.0654
Epoch 285/300, trend Loss: 0.0572 | 0.0661
Epoch 286/300, trend Loss: 0.0568 | 0.0657
Epoch 287/300, trend Loss: 0.0569 | 0.0657
Epoch 288/300, trend Loss: 0.0568 | 0.0653
Epoch 289/300, trend Loss: 0.0572 | 0.0659
Epoch 290/300, trend Loss: 0.0567 | 0.0656
Epoch 291/300, trend Loss: 0.0565 | 0.0661
Epoch 292/300, trend Loss: 0.0570 | 0.0658
Epoch 293/300, trend Loss: 0.0576 | 0.0658
Epoch 294/300, trend Loss: 0.0572 | 0.0660
Epoch 295/300, trend Loss: 0.0574 | 0.0661
Epoch 296/300, trend Loss: 0.0572 | 0.0667
Epoch 297/300, trend Loss: 0.0562 | 0.0661
Epoch 298/300, trend Loss: 0.0572 | 0.0660
Epoch 299/300, trend Loss: 0.0564 | 0.0658
Epoch 300/300, trend Loss: 0.0573 | 0.0653
Training seasonal_0 component with params: {'observation_period_num': 221, 'train_rates': 0.9887294419846075, 'learning_rate': 0.0001889104841907421, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8419575709029429}
Epoch 1/300, seasonal_0 Loss: 1.1631 | 1.3068
Epoch 2/300, seasonal_0 Loss: 0.7171 | 0.5853
Epoch 3/300, seasonal_0 Loss: 0.5731 | 0.5070
Epoch 4/300, seasonal_0 Loss: 0.4444 | 0.4250
Epoch 5/300, seasonal_0 Loss: 0.4363 | 0.3763
Epoch 6/300, seasonal_0 Loss: 0.4515 | 0.3143
Epoch 7/300, seasonal_0 Loss: 0.5089 | 0.3991
Epoch 8/300, seasonal_0 Loss: 0.3824 | 0.3335
Epoch 9/300, seasonal_0 Loss: 0.3385 | 0.3018
Epoch 10/300, seasonal_0 Loss: 0.3183 | 0.2733
Epoch 11/300, seasonal_0 Loss: 0.3353 | 0.2522
Epoch 12/300, seasonal_0 Loss: 0.2761 | 0.2406
Epoch 13/300, seasonal_0 Loss: 0.2854 | 0.1998
Epoch 14/300, seasonal_0 Loss: 0.2942 | 0.3570
Epoch 15/300, seasonal_0 Loss: 0.2952 | 0.2195
Epoch 16/300, seasonal_0 Loss: 0.2514 | 0.1830
Epoch 17/300, seasonal_0 Loss: 0.2241 | 0.1841
Epoch 18/300, seasonal_0 Loss: 0.2146 | 0.1792
Epoch 19/300, seasonal_0 Loss: 0.2060 | 0.1736
Epoch 20/300, seasonal_0 Loss: 0.2049 | 0.1705
Epoch 21/300, seasonal_0 Loss: 0.1987 | 0.1612
Epoch 22/300, seasonal_0 Loss: 0.1924 | 0.1621
Epoch 23/300, seasonal_0 Loss: 0.1848 | 0.1545
Epoch 24/300, seasonal_0 Loss: 0.1817 | 0.1538
Epoch 25/300, seasonal_0 Loss: 0.1843 | 0.1493
Epoch 26/300, seasonal_0 Loss: 0.1892 | 0.1524
Epoch 27/300, seasonal_0 Loss: 0.1942 | 0.1474
Epoch 28/300, seasonal_0 Loss: 0.1829 | 0.1494
Epoch 29/300, seasonal_0 Loss: 0.1782 | 0.1432
Epoch 30/300, seasonal_0 Loss: 0.1704 | 0.1440
Epoch 31/300, seasonal_0 Loss: 0.1675 | 0.1396
Epoch 32/300, seasonal_0 Loss: 0.1641 | 0.1410
Epoch 33/300, seasonal_0 Loss: 0.1627 | 0.1374
Epoch 34/300, seasonal_0 Loss: 0.1615 | 0.1368
Epoch 35/300, seasonal_0 Loss: 0.1602 | 0.1343
Epoch 36/300, seasonal_0 Loss: 0.1592 | 0.1343
Epoch 37/300, seasonal_0 Loss: 0.1582 | 0.1327
Epoch 38/300, seasonal_0 Loss: 0.1571 | 0.1318
Epoch 39/300, seasonal_0 Loss: 0.1572 | 0.1305
Epoch 40/300, seasonal_0 Loss: 0.1558 | 0.1298
Epoch 41/300, seasonal_0 Loss: 0.1558 | 0.1289
Epoch 42/300, seasonal_0 Loss: 0.1541 | 0.1280
Epoch 43/300, seasonal_0 Loss: 0.1536 | 0.1272
Epoch 44/300, seasonal_0 Loss: 0.1530 | 0.1272
Epoch 45/300, seasonal_0 Loss: 0.1526 | 0.1255
Epoch 46/300, seasonal_0 Loss: 0.1520 | 0.1257
Epoch 47/300, seasonal_0 Loss: 0.1519 | 0.1252
Epoch 48/300, seasonal_0 Loss: 0.1512 | 0.1240
Epoch 49/300, seasonal_0 Loss: 0.1501 | 0.1239
Epoch 50/300, seasonal_0 Loss: 0.1500 | 0.1230
Epoch 51/300, seasonal_0 Loss: 0.1495 | 0.1225
Epoch 52/300, seasonal_0 Loss: 0.1491 | 0.1217
Epoch 53/300, seasonal_0 Loss: 0.1491 | 0.1216
Epoch 54/300, seasonal_0 Loss: 0.1483 | 0.1213
Epoch 55/300, seasonal_0 Loss: 0.1478 | 0.1203
Epoch 56/300, seasonal_0 Loss: 0.1471 | 0.1198
Epoch 57/300, seasonal_0 Loss: 0.1469 | 0.1194
Epoch 58/300, seasonal_0 Loss: 0.1466 | 0.1193
Epoch 59/300, seasonal_0 Loss: 0.1464 | 0.1192
Epoch 60/300, seasonal_0 Loss: 0.1459 | 0.1189
Epoch 61/300, seasonal_0 Loss: 0.1456 | 0.1183
Epoch 62/300, seasonal_0 Loss: 0.1446 | 0.1181
Epoch 63/300, seasonal_0 Loss: 0.1454 | 0.1180
Epoch 64/300, seasonal_0 Loss: 0.1453 | 0.1172
Epoch 65/300, seasonal_0 Loss: 0.1443 | 0.1168
Epoch 66/300, seasonal_0 Loss: 0.1439 | 0.1169
Epoch 67/300, seasonal_0 Loss: 0.1438 | 0.1164
Epoch 68/300, seasonal_0 Loss: 0.1435 | 0.1161
Epoch 69/300, seasonal_0 Loss: 0.1431 | 0.1160
Epoch 70/300, seasonal_0 Loss: 0.1425 | 0.1157
Epoch 71/300, seasonal_0 Loss: 0.1426 | 0.1154
Epoch 72/300, seasonal_0 Loss: 0.1427 | 0.1153
Epoch 73/300, seasonal_0 Loss: 0.1420 | 0.1155
Epoch 74/300, seasonal_0 Loss: 0.1423 | 0.1152
Epoch 75/300, seasonal_0 Loss: 0.1421 | 0.1147
Epoch 76/300, seasonal_0 Loss: 0.1418 | 0.1144
Epoch 77/300, seasonal_0 Loss: 0.1416 | 0.1143
Epoch 78/300, seasonal_0 Loss: 0.1412 | 0.1140
Epoch 79/300, seasonal_0 Loss: 0.1411 | 0.1138
Epoch 80/300, seasonal_0 Loss: 0.1407 | 0.1133
Epoch 81/300, seasonal_0 Loss: 0.1410 | 0.1132
Epoch 82/300, seasonal_0 Loss: 0.1399 | 0.1131
Epoch 83/300, seasonal_0 Loss: 0.1403 | 0.1131
Epoch 84/300, seasonal_0 Loss: 0.1400 | 0.1128
Epoch 85/300, seasonal_0 Loss: 0.1395 | 0.1126
Epoch 86/300, seasonal_0 Loss: 0.1398 | 0.1127
Epoch 87/300, seasonal_0 Loss: 0.1399 | 0.1123
Epoch 88/300, seasonal_0 Loss: 0.1395 | 0.1120
Epoch 89/300, seasonal_0 Loss: 0.1391 | 0.1116
Epoch 90/300, seasonal_0 Loss: 0.1392 | 0.1116
Epoch 91/300, seasonal_0 Loss: 0.1387 | 0.1115
Epoch 92/300, seasonal_0 Loss: 0.1393 | 0.1113
Epoch 93/300, seasonal_0 Loss: 0.1390 | 0.1113
Epoch 94/300, seasonal_0 Loss: 0.1384 | 0.1114
Epoch 95/300, seasonal_0 Loss: 0.1385 | 0.1112
Epoch 96/300, seasonal_0 Loss: 0.1381 | 0.1109
Epoch 97/300, seasonal_0 Loss: 0.1377 | 0.1109
Epoch 98/300, seasonal_0 Loss: 0.1383 | 0.1109
Epoch 99/300, seasonal_0 Loss: 0.1382 | 0.1108
Epoch 100/300, seasonal_0 Loss: 0.1377 | 0.1106
Epoch 101/300, seasonal_0 Loss: 0.1383 | 0.1107
Epoch 102/300, seasonal_0 Loss: 0.1380 | 0.1104
Epoch 103/300, seasonal_0 Loss: 0.1374 | 0.1102
Epoch 104/300, seasonal_0 Loss: 0.1373 | 0.1102
Epoch 105/300, seasonal_0 Loss: 0.1378 | 0.1101
Epoch 106/300, seasonal_0 Loss: 0.1379 | 0.1101
Epoch 107/300, seasonal_0 Loss: 0.1370 | 0.1099
Epoch 108/300, seasonal_0 Loss: 0.1372 | 0.1098
Epoch 109/300, seasonal_0 Loss: 0.1373 | 0.1098
Epoch 110/300, seasonal_0 Loss: 0.1372 | 0.1098
Epoch 111/300, seasonal_0 Loss: 0.1364 | 0.1098
Epoch 112/300, seasonal_0 Loss: 0.1368 | 0.1095
Epoch 113/300, seasonal_0 Loss: 0.1371 | 0.1096
Epoch 114/300, seasonal_0 Loss: 0.1369 | 0.1095
Epoch 115/300, seasonal_0 Loss: 0.1370 | 0.1094
Epoch 116/300, seasonal_0 Loss: 0.1369 | 0.1094
Epoch 117/300, seasonal_0 Loss: 0.1365 | 0.1094
Epoch 118/300, seasonal_0 Loss: 0.1369 | 0.1093
Epoch 119/300, seasonal_0 Loss: 0.1365 | 0.1093
Epoch 120/300, seasonal_0 Loss: 0.1367 | 0.1091
Epoch 121/300, seasonal_0 Loss: 0.1358 | 0.1089
Epoch 122/300, seasonal_0 Loss: 0.1365 | 0.1088
Epoch 123/300, seasonal_0 Loss: 0.1365 | 0.1087
Epoch 124/300, seasonal_0 Loss: 0.1362 | 0.1087
Epoch 125/300, seasonal_0 Loss: 0.1351 | 0.1086
Epoch 126/300, seasonal_0 Loss: 0.1360 | 0.1086
Epoch 127/300, seasonal_0 Loss: 0.1360 | 0.1085
Epoch 128/300, seasonal_0 Loss: 0.1358 | 0.1085
Epoch 129/300, seasonal_0 Loss: 0.1355 | 0.1084
Epoch 130/300, seasonal_0 Loss: 0.1357 | 0.1084
Epoch 131/300, seasonal_0 Loss: 0.1357 | 0.1084
Epoch 132/300, seasonal_0 Loss: 0.1357 | 0.1084
Epoch 133/300, seasonal_0 Loss: 0.1352 | 0.1083
Epoch 134/300, seasonal_0 Loss: 0.1356 | 0.1083
Epoch 135/300, seasonal_0 Loss: 0.1355 | 0.1083
Epoch 136/300, seasonal_0 Loss: 0.1349 | 0.1082
Epoch 137/300, seasonal_0 Loss: 0.1355 | 0.1082
Epoch 138/300, seasonal_0 Loss: 0.1353 | 0.1082
Epoch 139/300, seasonal_0 Loss: 0.1358 | 0.1083
Epoch 140/300, seasonal_0 Loss: 0.1355 | 0.1083
Epoch 141/300, seasonal_0 Loss: 0.1351 | 0.1083
Epoch 142/300, seasonal_0 Loss: 0.1356 | 0.1082
Epoch 143/300, seasonal_0 Loss: 0.1351 | 0.1082
Epoch 144/300, seasonal_0 Loss: 0.1348 | 0.1082
Epoch 145/300, seasonal_0 Loss: 0.1352 | 0.1081
Epoch 146/300, seasonal_0 Loss: 0.1351 | 0.1081
Epoch 147/300, seasonal_0 Loss: 0.1350 | 0.1080
Epoch 148/300, seasonal_0 Loss: 0.1355 | 0.1080
Epoch 149/300, seasonal_0 Loss: 0.1350 | 0.1080
Epoch 150/300, seasonal_0 Loss: 0.1351 | 0.1079
Epoch 151/300, seasonal_0 Loss: 0.1349 | 0.1079
Epoch 152/300, seasonal_0 Loss: 0.1347 | 0.1079
Epoch 153/300, seasonal_0 Loss: 0.1354 | 0.1079
Epoch 154/300, seasonal_0 Loss: 0.1353 | 0.1079
Epoch 155/300, seasonal_0 Loss: 0.1353 | 0.1079
Epoch 156/300, seasonal_0 Loss: 0.1349 | 0.1079
Epoch 157/300, seasonal_0 Loss: 0.1347 | 0.1079
Epoch 158/300, seasonal_0 Loss: 0.1351 | 0.1080
Epoch 159/300, seasonal_0 Loss: 0.1357 | 0.1080
Epoch 160/300, seasonal_0 Loss: 0.1357 | 0.1079
Epoch 161/300, seasonal_0 Loss: 0.1346 | 0.1079
Epoch 162/300, seasonal_0 Loss: 0.1346 | 0.1078
Epoch 163/300, seasonal_0 Loss: 0.1345 | 0.1078
Epoch 164/300, seasonal_0 Loss: 0.1346 | 0.1077
Epoch 165/300, seasonal_0 Loss: 0.1344 | 0.1077
Epoch 166/300, seasonal_0 Loss: 0.1343 | 0.1077
Epoch 167/300, seasonal_0 Loss: 0.1353 | 0.1077
Epoch 168/300, seasonal_0 Loss: 0.1347 | 0.1077
Epoch 169/300, seasonal_0 Loss: 0.1343 | 0.1076
Epoch 170/300, seasonal_0 Loss: 0.1341 | 0.1076
Epoch 171/300, seasonal_0 Loss: 0.1350 | 0.1076
Epoch 172/300, seasonal_0 Loss: 0.1346 | 0.1076
Epoch 173/300, seasonal_0 Loss: 0.1345 | 0.1076
Epoch 174/300, seasonal_0 Loss: 0.1348 | 0.1076
Epoch 175/300, seasonal_0 Loss: 0.1349 | 0.1076
Epoch 176/300, seasonal_0 Loss: 0.1342 | 0.1076
Epoch 177/300, seasonal_0 Loss: 0.1347 | 0.1076
Epoch 178/300, seasonal_0 Loss: 0.1352 | 0.1076
Epoch 179/300, seasonal_0 Loss: 0.1343 | 0.1076
Epoch 180/300, seasonal_0 Loss: 0.1349 | 0.1076
Epoch 181/300, seasonal_0 Loss: 0.1343 | 0.1075
Epoch 182/300, seasonal_0 Loss: 0.1345 | 0.1075
Epoch 183/300, seasonal_0 Loss: 0.1353 | 0.1074
Epoch 184/300, seasonal_0 Loss: 0.1350 | 0.1074
Epoch 185/300, seasonal_0 Loss: 0.1342 | 0.1074
Epoch 186/300, seasonal_0 Loss: 0.1345 | 0.1074
Epoch 187/300, seasonal_0 Loss: 0.1345 | 0.1074
Epoch 188/300, seasonal_0 Loss: 0.1341 | 0.1074
Epoch 189/300, seasonal_0 Loss: 0.1354 | 0.1074
Epoch 190/300, seasonal_0 Loss: 0.1345 | 0.1074
Epoch 191/300, seasonal_0 Loss: 0.1347 | 0.1074
Epoch 192/300, seasonal_0 Loss: 0.1340 | 0.1074
Epoch 193/300, seasonal_0 Loss: 0.1342 | 0.1074
Epoch 194/300, seasonal_0 Loss: 0.1344 | 0.1074
Epoch 195/300, seasonal_0 Loss: 0.1353 | 0.1074
Epoch 196/300, seasonal_0 Loss: 0.1347 | 0.1074
Epoch 197/300, seasonal_0 Loss: 0.1344 | 0.1074
Epoch 198/300, seasonal_0 Loss: 0.1346 | 0.1074
Epoch 199/300, seasonal_0 Loss: 0.1343 | 0.1074
Epoch 200/300, seasonal_0 Loss: 0.1343 | 0.1074
Epoch 201/300, seasonal_0 Loss: 0.1341 | 0.1074
Epoch 202/300, seasonal_0 Loss: 0.1340 | 0.1074
Epoch 203/300, seasonal_0 Loss: 0.1348 | 0.1074
Epoch 204/300, seasonal_0 Loss: 0.1343 | 0.1074
Epoch 205/300, seasonal_0 Loss: 0.1342 | 0.1074
Epoch 206/300, seasonal_0 Loss: 0.1340 | 0.1074
Epoch 207/300, seasonal_0 Loss: 0.1341 | 0.1074
Epoch 208/300, seasonal_0 Loss: 0.1345 | 0.1074
Epoch 209/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 210/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 211/300, seasonal_0 Loss: 0.1349 | 0.1074
Epoch 212/300, seasonal_0 Loss: 0.1342 | 0.1074
Epoch 213/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 214/300, seasonal_0 Loss: 0.1339 | 0.1074
Epoch 215/300, seasonal_0 Loss: 0.1346 | 0.1073
Epoch 216/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 217/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 218/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 219/300, seasonal_0 Loss: 0.1346 | 0.1073
Epoch 220/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 221/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 222/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 223/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 224/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 225/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 226/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 227/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 228/300, seasonal_0 Loss: 0.1336 | 0.1073
Epoch 229/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 230/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 231/300, seasonal_0 Loss: 0.1338 | 0.1073
Epoch 232/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 233/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 234/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 235/300, seasonal_0 Loss: 0.1337 | 0.1073
Epoch 236/300, seasonal_0 Loss: 0.1346 | 0.1073
Epoch 237/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 238/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 239/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 240/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 241/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 242/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 243/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 244/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 245/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 246/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 247/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 248/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 249/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 250/300, seasonal_0 Loss: 0.1348 | 0.1073
Epoch 251/300, seasonal_0 Loss: 0.1346 | 0.1073
Epoch 252/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 253/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 254/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 255/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 256/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 257/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 258/300, seasonal_0 Loss: 0.1337 | 0.1073
Epoch 259/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 260/300, seasonal_0 Loss: 0.1337 | 0.1073
Epoch 261/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 262/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 263/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 264/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 265/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 266/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 267/300, seasonal_0 Loss: 0.1336 | 0.1073
Epoch 268/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 269/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 270/300, seasonal_0 Loss: 0.1337 | 0.1073
Epoch 271/300, seasonal_0 Loss: 0.1344 | 0.1073
Epoch 272/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 273/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 274/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 275/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 276/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 277/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 278/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 279/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 280/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 281/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 282/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 283/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 284/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 285/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 286/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 287/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 288/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 289/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 290/300, seasonal_0 Loss: 0.1340 | 0.1073
Epoch 291/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 292/300, seasonal_0 Loss: 0.1339 | 0.1073
Epoch 293/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 294/300, seasonal_0 Loss: 0.1341 | 0.1073
Epoch 295/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 296/300, seasonal_0 Loss: 0.1345 | 0.1073
Epoch 297/300, seasonal_0 Loss: 0.1347 | 0.1073
Epoch 298/300, seasonal_0 Loss: 0.1343 | 0.1073
Epoch 299/300, seasonal_0 Loss: 0.1342 | 0.1073
Epoch 300/300, seasonal_0 Loss: 0.1341 | 0.1073
Training seasonal_1 component with params: {'observation_period_num': 51, 'train_rates': 0.9898873235701335, 'learning_rate': 2.5120900751798626e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8407730014682415}
Epoch 1/300, seasonal_1 Loss: 0.6596 | 0.5615
Epoch 2/300, seasonal_1 Loss: 0.4246 | 0.4107
Epoch 3/300, seasonal_1 Loss: 0.3248 | 0.3415
Epoch 4/300, seasonal_1 Loss: 0.2603 | 0.2958
Epoch 5/300, seasonal_1 Loss: 0.2306 | 0.2641
Epoch 6/300, seasonal_1 Loss: 0.2119 | 0.2432
Epoch 7/300, seasonal_1 Loss: 0.1991 | 0.2300
Epoch 8/300, seasonal_1 Loss: 0.1903 | 0.2212
Epoch 9/300, seasonal_1 Loss: 0.1848 | 0.2093
Epoch 10/300, seasonal_1 Loss: 0.1796 | 0.1940
Epoch 11/300, seasonal_1 Loss: 0.1743 | 0.1871
Epoch 12/300, seasonal_1 Loss: 0.1700 | 0.1792
Epoch 13/300, seasonal_1 Loss: 0.1665 | 0.1791
Epoch 14/300, seasonal_1 Loss: 0.1618 | 0.1731
Epoch 15/300, seasonal_1 Loss: 0.1593 | 0.1650
Epoch 16/300, seasonal_1 Loss: 0.1569 | 0.1581
Epoch 17/300, seasonal_1 Loss: 0.1554 | 0.1538
Epoch 18/300, seasonal_1 Loss: 0.1534 | 0.1470
Epoch 19/300, seasonal_1 Loss: 0.1513 | 0.1452
Epoch 20/300, seasonal_1 Loss: 0.1497 | 0.1432
Epoch 21/300, seasonal_1 Loss: 0.1474 | 0.1420
Epoch 22/300, seasonal_1 Loss: 0.1461 | 0.1383
Epoch 23/300, seasonal_1 Loss: 0.1444 | 0.1363
Epoch 24/300, seasonal_1 Loss: 0.1428 | 0.1360
Epoch 25/300, seasonal_1 Loss: 0.1417 | 0.1349
Epoch 26/300, seasonal_1 Loss: 0.1401 | 0.1345
Epoch 27/300, seasonal_1 Loss: 0.1384 | 0.1324
Epoch 28/300, seasonal_1 Loss: 0.1370 | 0.1250
Epoch 29/300, seasonal_1 Loss: 0.1355 | 0.1241
Epoch 30/300, seasonal_1 Loss: 0.1339 | 0.1220
Epoch 31/300, seasonal_1 Loss: 0.1332 | 0.1202
Epoch 32/300, seasonal_1 Loss: 0.1323 | 0.1216
Epoch 33/300, seasonal_1 Loss: 0.1301 | 0.1188
Epoch 34/300, seasonal_1 Loss: 0.1298 | 0.1194
Epoch 35/300, seasonal_1 Loss: 0.1286 | 0.1200
Epoch 36/300, seasonal_1 Loss: 0.1277 | 0.1182
Epoch 37/300, seasonal_1 Loss: 0.1268 | 0.1166
Epoch 38/300, seasonal_1 Loss: 0.1262 | 0.1166
Epoch 39/300, seasonal_1 Loss: 0.1250 | 0.1147
Epoch 40/300, seasonal_1 Loss: 0.1245 | 0.1132
Epoch 41/300, seasonal_1 Loss: 0.1235 | 0.1130
Epoch 42/300, seasonal_1 Loss: 0.1228 | 0.1110
Epoch 43/300, seasonal_1 Loss: 0.1221 | 0.1107
Epoch 44/300, seasonal_1 Loss: 0.1219 | 0.1100
Epoch 45/300, seasonal_1 Loss: 0.1212 | 0.1097
Epoch 46/300, seasonal_1 Loss: 0.1202 | 0.1114
Epoch 47/300, seasonal_1 Loss: 0.1200 | 0.1126
Epoch 48/300, seasonal_1 Loss: 0.1195 | 0.1129
Epoch 49/300, seasonal_1 Loss: 0.1194 | 0.1125
Epoch 50/300, seasonal_1 Loss: 0.1186 | 0.1111
Epoch 51/300, seasonal_1 Loss: 0.1178 | 0.1099
Epoch 52/300, seasonal_1 Loss: 0.1176 | 0.1097
Epoch 53/300, seasonal_1 Loss: 0.1174 | 0.1075
Epoch 54/300, seasonal_1 Loss: 0.1167 | 0.1079
Epoch 55/300, seasonal_1 Loss: 0.1160 | 0.1065
Epoch 56/300, seasonal_1 Loss: 0.1160 | 0.1066
Epoch 57/300, seasonal_1 Loss: 0.1151 | 0.1065
Epoch 58/300, seasonal_1 Loss: 0.1152 | 0.1067
Epoch 59/300, seasonal_1 Loss: 0.1146 | 0.1069
Epoch 60/300, seasonal_1 Loss: 0.1145 | 0.1067
Epoch 61/300, seasonal_1 Loss: 0.1139 | 0.1070
Epoch 62/300, seasonal_1 Loss: 0.1141 | 0.1064
Epoch 63/300, seasonal_1 Loss: 0.1133 | 0.1054
Epoch 64/300, seasonal_1 Loss: 0.1134 | 0.1051
Epoch 65/300, seasonal_1 Loss: 0.1132 | 0.1056
Epoch 66/300, seasonal_1 Loss: 0.1130 | 0.1054
Epoch 67/300, seasonal_1 Loss: 0.1127 | 0.1049
Epoch 68/300, seasonal_1 Loss: 0.1117 | 0.1046
Epoch 69/300, seasonal_1 Loss: 0.1124 | 0.1044
Epoch 70/300, seasonal_1 Loss: 0.1114 | 0.1043
Epoch 71/300, seasonal_1 Loss: 0.1115 | 0.1050
Epoch 72/300, seasonal_1 Loss: 0.1118 | 0.1044
Epoch 73/300, seasonal_1 Loss: 0.1110 | 0.1032
Epoch 74/300, seasonal_1 Loss: 0.1109 | 0.1037
Epoch 75/300, seasonal_1 Loss: 0.1110 | 0.1037
Epoch 76/300, seasonal_1 Loss: 0.1110 | 0.1032
Epoch 77/300, seasonal_1 Loss: 0.1107 | 0.1040
Epoch 78/300, seasonal_1 Loss: 0.1104 | 0.1037
Epoch 79/300, seasonal_1 Loss: 0.1106 | 0.1037
Epoch 80/300, seasonal_1 Loss: 0.1101 | 0.1037
Epoch 81/300, seasonal_1 Loss: 0.1100 | 0.1033
Epoch 82/300, seasonal_1 Loss: 0.1103 | 0.1032
Epoch 83/300, seasonal_1 Loss: 0.1095 | 0.1028
Epoch 84/300, seasonal_1 Loss: 0.1096 | 0.1029
Epoch 85/300, seasonal_1 Loss: 0.1099 | 0.1034
Epoch 86/300, seasonal_1 Loss: 0.1095 | 0.1036
Epoch 87/300, seasonal_1 Loss: 0.1090 | 0.1032
Epoch 88/300, seasonal_1 Loss: 0.1097 | 0.1029
Epoch 89/300, seasonal_1 Loss: 0.1097 | 0.1033
Epoch 90/300, seasonal_1 Loss: 0.1093 | 0.1032
Epoch 91/300, seasonal_1 Loss: 0.1087 | 0.1029
Epoch 92/300, seasonal_1 Loss: 0.1093 | 0.1024
Epoch 93/300, seasonal_1 Loss: 0.1092 | 0.1026
Epoch 94/300, seasonal_1 Loss: 0.1083 | 0.1029
Epoch 95/300, seasonal_1 Loss: 0.1092 | 0.1023
Epoch 96/300, seasonal_1 Loss: 0.1082 | 0.1026
Epoch 97/300, seasonal_1 Loss: 0.1083 | 0.1026
Epoch 98/300, seasonal_1 Loss: 0.1092 | 0.1027
Epoch 99/300, seasonal_1 Loss: 0.1089 | 0.1025
Epoch 100/300, seasonal_1 Loss: 0.1088 | 0.1023
Epoch 101/300, seasonal_1 Loss: 0.1085 | 0.1023
Epoch 102/300, seasonal_1 Loss: 0.1086 | 0.1023
Epoch 103/300, seasonal_1 Loss: 0.1081 | 0.1027
Epoch 104/300, seasonal_1 Loss: 0.1081 | 0.1026
Epoch 105/300, seasonal_1 Loss: 0.1084 | 0.1026
Epoch 106/300, seasonal_1 Loss: 0.1082 | 0.1026
Epoch 107/300, seasonal_1 Loss: 0.1083 | 0.1023
Epoch 108/300, seasonal_1 Loss: 0.1081 | 0.1024
Epoch 109/300, seasonal_1 Loss: 0.1082 | 0.1022
Epoch 110/300, seasonal_1 Loss: 0.1081 | 0.1022
Epoch 111/300, seasonal_1 Loss: 0.1079 | 0.1023
Epoch 112/300, seasonal_1 Loss: 0.1078 | 0.1027
Epoch 113/300, seasonal_1 Loss: 0.1077 | 0.1024
Epoch 114/300, seasonal_1 Loss: 0.1075 | 0.1022
Epoch 115/300, seasonal_1 Loss: 0.1081 | 0.1021
Epoch 116/300, seasonal_1 Loss: 0.1075 | 0.1022
Epoch 117/300, seasonal_1 Loss: 0.1078 | 0.1019
Epoch 118/300, seasonal_1 Loss: 0.1077 | 0.1019
Epoch 119/300, seasonal_1 Loss: 0.1078 | 0.1020
Epoch 120/300, seasonal_1 Loss: 0.1077 | 0.1020
Epoch 121/300, seasonal_1 Loss: 0.1073 | 0.1020
Epoch 122/300, seasonal_1 Loss: 0.1080 | 0.1020
Epoch 123/300, seasonal_1 Loss: 0.1076 | 0.1019
Epoch 124/300, seasonal_1 Loss: 0.1078 | 0.1020
Epoch 125/300, seasonal_1 Loss: 0.1077 | 0.1022
Epoch 126/300, seasonal_1 Loss: 0.1076 | 0.1022
Epoch 127/300, seasonal_1 Loss: 0.1077 | 0.1022
Epoch 128/300, seasonal_1 Loss: 0.1077 | 0.1019
Epoch 129/300, seasonal_1 Loss: 0.1078 | 0.1019
Epoch 130/300, seasonal_1 Loss: 0.1074 | 0.1018
Epoch 131/300, seasonal_1 Loss: 0.1080 | 0.1020
Epoch 132/300, seasonal_1 Loss: 0.1076 | 0.1021
Epoch 133/300, seasonal_1 Loss: 0.1075 | 0.1022
Epoch 134/300, seasonal_1 Loss: 0.1071 | 0.1019
Epoch 135/300, seasonal_1 Loss: 0.1074 | 0.1019
Epoch 136/300, seasonal_1 Loss: 0.1076 | 0.1018
Epoch 137/300, seasonal_1 Loss: 0.1074 | 0.1018
Epoch 138/300, seasonal_1 Loss: 0.1069 | 0.1018
Epoch 139/300, seasonal_1 Loss: 0.1075 | 0.1017
Epoch 140/300, seasonal_1 Loss: 0.1070 | 0.1018
Epoch 141/300, seasonal_1 Loss: 0.1072 | 0.1017
Epoch 142/300, seasonal_1 Loss: 0.1073 | 0.1018
Epoch 143/300, seasonal_1 Loss: 0.1074 | 0.1018
Epoch 144/300, seasonal_1 Loss: 0.1075 | 0.1017
Epoch 145/300, seasonal_1 Loss: 0.1073 | 0.1017
Epoch 146/300, seasonal_1 Loss: 0.1075 | 0.1017
Epoch 147/300, seasonal_1 Loss: 0.1070 | 0.1016
Epoch 148/300, seasonal_1 Loss: 0.1080 | 0.1015
Epoch 149/300, seasonal_1 Loss: 0.1074 | 0.1015
Epoch 150/300, seasonal_1 Loss: 0.1076 | 0.1016
Epoch 151/300, seasonal_1 Loss: 0.1068 | 0.1017
Epoch 152/300, seasonal_1 Loss: 0.1074 | 0.1017
Epoch 153/300, seasonal_1 Loss: 0.1076 | 0.1017
Epoch 154/300, seasonal_1 Loss: 0.1071 | 0.1016
Epoch 155/300, seasonal_1 Loss: 0.1068 | 0.1016
Epoch 156/300, seasonal_1 Loss: 0.1075 | 0.1017
Epoch 157/300, seasonal_1 Loss: 0.1072 | 0.1017
Epoch 158/300, seasonal_1 Loss: 0.1073 | 0.1017
Epoch 159/300, seasonal_1 Loss: 0.1073 | 0.1017
Epoch 160/300, seasonal_1 Loss: 0.1076 | 0.1017
Epoch 161/300, seasonal_1 Loss: 0.1070 | 0.1017
Epoch 162/300, seasonal_1 Loss: 0.1074 | 0.1017
Epoch 163/300, seasonal_1 Loss: 0.1073 | 0.1016
Epoch 164/300, seasonal_1 Loss: 0.1074 | 0.1016
Epoch 165/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 166/300, seasonal_1 Loss: 0.1075 | 0.1016
Epoch 167/300, seasonal_1 Loss: 0.1078 | 0.1016
Epoch 168/300, seasonal_1 Loss: 0.1074 | 0.1016
Epoch 169/300, seasonal_1 Loss: 0.1072 | 0.1016
Epoch 170/300, seasonal_1 Loss: 0.1067 | 0.1016
Epoch 171/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 172/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 173/300, seasonal_1 Loss: 0.1066 | 0.1015
Epoch 174/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 175/300, seasonal_1 Loss: 0.1077 | 0.1014
Epoch 176/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 177/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 178/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 179/300, seasonal_1 Loss: 0.1078 | 0.1015
Epoch 180/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 181/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 182/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 183/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 184/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 185/300, seasonal_1 Loss: 0.1076 | 0.1015
Epoch 186/300, seasonal_1 Loss: 0.1074 | 0.1015
Epoch 187/300, seasonal_1 Loss: 0.1068 | 0.1014
Epoch 188/300, seasonal_1 Loss: 0.1071 | 0.1014
Epoch 189/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 190/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 191/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 192/300, seasonal_1 Loss: 0.1076 | 0.1015
Epoch 193/300, seasonal_1 Loss: 0.1074 | 0.1015
Epoch 194/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 195/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 196/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 197/300, seasonal_1 Loss: 0.1079 | 0.1015
Epoch 198/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 199/300, seasonal_1 Loss: 0.1078 | 0.1015
Epoch 200/300, seasonal_1 Loss: 0.1066 | 0.1015
Epoch 201/300, seasonal_1 Loss: 0.1066 | 0.1015
Epoch 202/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 203/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 204/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 205/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 206/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 207/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 208/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 209/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 210/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 211/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 212/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 213/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 214/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 215/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 216/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 217/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 218/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 219/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 220/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 221/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 222/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 223/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 224/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 225/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 226/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 227/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 228/300, seasonal_1 Loss: 0.1076 | 0.1015
Epoch 229/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 230/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 231/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 232/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 233/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 234/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 235/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 236/300, seasonal_1 Loss: 0.1076 | 0.1015
Epoch 237/300, seasonal_1 Loss: 0.1062 | 0.1015
Epoch 238/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 239/300, seasonal_1 Loss: 0.1065 | 0.1015
Epoch 240/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 241/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 242/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 243/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 244/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 245/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 246/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 247/300, seasonal_1 Loss: 0.1062 | 0.1015
Epoch 248/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 249/300, seasonal_1 Loss: 0.1075 | 0.1015
Epoch 250/300, seasonal_1 Loss: 0.1074 | 0.1015
Epoch 251/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 252/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 253/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 254/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 255/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 256/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 257/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 258/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 259/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 260/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 261/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 262/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 263/300, seasonal_1 Loss: 0.1074 | 0.1015
Epoch 264/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 265/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 266/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 267/300, seasonal_1 Loss: 0.1065 | 0.1015
Epoch 268/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 269/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 270/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 271/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 272/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 273/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 274/300, seasonal_1 Loss: 0.1070 | 0.1015
Epoch 275/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 276/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 277/300, seasonal_1 Loss: 0.1072 | 0.1015
Epoch 278/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 279/300, seasonal_1 Loss: 0.1069 | 0.1015
Epoch 280/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 281/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 282/300, seasonal_1 Loss: 0.1063 | 0.1015
Epoch 283/300, seasonal_1 Loss: 0.1067 | 0.1015
Epoch 284/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 285/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 286/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 287/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 288/300, seasonal_1 Loss: 0.1078 | 0.1015
Epoch 289/300, seasonal_1 Loss: 0.1066 | 0.1015
Epoch 290/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 291/300, seasonal_1 Loss: 0.1063 | 0.1015
Epoch 292/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 293/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 294/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 295/300, seasonal_1 Loss: 0.1073 | 0.1015
Epoch 296/300, seasonal_1 Loss: 0.1068 | 0.1015
Epoch 297/300, seasonal_1 Loss: 0.1071 | 0.1015
Epoch 298/300, seasonal_1 Loss: 0.1062 | 0.1015
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9889632755230612, 'learning_rate': 8.771818578909478e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9751930038944867}
Epoch 1/300, seasonal_2 Loss: 0.7522 | 0.7124
Epoch 2/300, seasonal_2 Loss: 0.5978 | 0.5317
Epoch 3/300, seasonal_2 Loss: 0.4216 | 0.4622
Epoch 4/300, seasonal_2 Loss: 0.3123 | 0.3637
Epoch 5/300, seasonal_2 Loss: 0.2685 | 0.3123
Epoch 6/300, seasonal_2 Loss: 0.2425 | 0.2744
Epoch 7/300, seasonal_2 Loss: 0.2528 | 0.2587
Epoch 8/300, seasonal_2 Loss: 0.2409 | 0.2314
Epoch 9/300, seasonal_2 Loss: 0.2399 | 0.2263
Epoch 10/300, seasonal_2 Loss: 0.2341 | 0.2070
Epoch 11/300, seasonal_2 Loss: 0.2336 | 0.2560
Epoch 12/300, seasonal_2 Loss: 0.2331 | 0.2184
Epoch 13/300, seasonal_2 Loss: 0.2032 | 0.1942
Epoch 14/300, seasonal_2 Loss: 0.2095 | 0.1728
Epoch 15/300, seasonal_2 Loss: 0.2141 | 0.1791
Epoch 16/300, seasonal_2 Loss: 0.2113 | 0.1823
Epoch 17/300, seasonal_2 Loss: 0.2015 | 0.1870
Epoch 18/300, seasonal_2 Loss: 0.2206 | 0.1819
Epoch 19/300, seasonal_2 Loss: 0.1967 | 0.2016
Epoch 20/300, seasonal_2 Loss: 0.1997 | 0.1685
Epoch 21/300, seasonal_2 Loss: 0.1977 | 0.1598
Epoch 22/300, seasonal_2 Loss: 0.1905 | 0.1621
Epoch 23/300, seasonal_2 Loss: 0.1836 | 0.1541
Epoch 24/300, seasonal_2 Loss: 0.1839 | 0.1864
Epoch 25/300, seasonal_2 Loss: 0.1697 | 0.1420
Epoch 26/300, seasonal_2 Loss: 0.1743 | 0.1760
Epoch 27/300, seasonal_2 Loss: 0.1581 | 0.1459
Epoch 28/300, seasonal_2 Loss: 0.1688 | 0.1358
Epoch 29/300, seasonal_2 Loss: 0.1850 | 0.1907
Epoch 30/300, seasonal_2 Loss: 0.1535 | 0.1405
Epoch 31/300, seasonal_2 Loss: 0.1620 | 0.1482
Epoch 32/300, seasonal_2 Loss: 0.1468 | 0.1346
Epoch 33/300, seasonal_2 Loss: 0.1419 | 0.1223
Epoch 34/300, seasonal_2 Loss: 0.1339 | 0.1260
Epoch 35/300, seasonal_2 Loss: 0.1323 | 0.1101
Epoch 36/300, seasonal_2 Loss: 0.1365 | 0.1132
Epoch 37/300, seasonal_2 Loss: 0.1286 | 0.1158
Epoch 38/300, seasonal_2 Loss: 0.1180 | 0.0990
Epoch 39/300, seasonal_2 Loss: 0.1231 | 0.1000
Epoch 40/300, seasonal_2 Loss: 0.1130 | 0.1000
Epoch 41/300, seasonal_2 Loss: 0.1112 | 0.0900
Epoch 42/300, seasonal_2 Loss: 0.1153 | 0.0857
Epoch 43/300, seasonal_2 Loss: 0.1172 | 0.0959
Epoch 44/300, seasonal_2 Loss: 0.1309 | 0.0994
Epoch 45/300, seasonal_2 Loss: 0.1332 | 0.1028
Epoch 46/300, seasonal_2 Loss: 0.1428 | 0.0920
Epoch 47/300, seasonal_2 Loss: 0.1558 | 0.1211
Epoch 48/300, seasonal_2 Loss: 0.1449 | 0.1138
Epoch 49/300, seasonal_2 Loss: 0.1438 | 0.1035
Epoch 50/300, seasonal_2 Loss: 0.1273 | 0.0952
Epoch 51/300, seasonal_2 Loss: 0.1246 | 0.0933
Epoch 52/300, seasonal_2 Loss: 0.1080 | 0.0909
Epoch 53/300, seasonal_2 Loss: 0.1075 | 0.0861
Epoch 54/300, seasonal_2 Loss: 0.1074 | 0.0824
Epoch 55/300, seasonal_2 Loss: 0.1099 | 0.0827
Epoch 56/300, seasonal_2 Loss: 0.1042 | 0.0809
Epoch 57/300, seasonal_2 Loss: 0.0982 | 0.0752
Epoch 58/300, seasonal_2 Loss: 0.1016 | 0.0677
Epoch 59/300, seasonal_2 Loss: 0.0988 | 0.0794
Epoch 60/300, seasonal_2 Loss: 0.0998 | 0.0790
Epoch 61/300, seasonal_2 Loss: 0.1156 | 0.0682
Epoch 62/300, seasonal_2 Loss: 0.1150 | 0.0783
Epoch 63/300, seasonal_2 Loss: 0.1261 | 0.0841
Epoch 64/300, seasonal_2 Loss: 0.1252 | 0.0896
Epoch 65/300, seasonal_2 Loss: 0.1262 | 0.0805
Epoch 66/300, seasonal_2 Loss: 0.1236 | 0.0960
Epoch 67/300, seasonal_2 Loss: 0.1088 | 0.0820
Epoch 68/300, seasonal_2 Loss: 0.1090 | 0.0827
Epoch 69/300, seasonal_2 Loss: 0.1054 | 0.0719
Epoch 70/300, seasonal_2 Loss: 0.1019 | 0.0750
Epoch 71/300, seasonal_2 Loss: 0.1005 | 0.0731
Epoch 72/300, seasonal_2 Loss: 0.0968 | 0.0673
Epoch 73/300, seasonal_2 Loss: 0.1043 | 0.0820
Epoch 74/300, seasonal_2 Loss: 0.1046 | 0.0865
Epoch 75/300, seasonal_2 Loss: 0.1086 | 0.0732
Epoch 76/300, seasonal_2 Loss: 0.0996 | 0.0664
Epoch 77/300, seasonal_2 Loss: 0.0971 | 0.0725
Epoch 78/300, seasonal_2 Loss: 0.0983 | 0.0677
Epoch 79/300, seasonal_2 Loss: 0.0909 | 0.0676
Epoch 80/300, seasonal_2 Loss: 0.0907 | 0.0677
Epoch 81/300, seasonal_2 Loss: 0.0856 | 0.0699
Epoch 82/300, seasonal_2 Loss: 0.0825 | 0.0608
Epoch 83/300, seasonal_2 Loss: 0.0848 | 0.0604
Epoch 84/300, seasonal_2 Loss: 0.0797 | 0.0609
Epoch 85/300, seasonal_2 Loss: 0.0785 | 0.0587
Epoch 86/300, seasonal_2 Loss: 0.0826 | 0.0592
Epoch 87/300, seasonal_2 Loss: 0.0749 | 0.0546
Epoch 88/300, seasonal_2 Loss: 0.0746 | 0.0542
Epoch 89/300, seasonal_2 Loss: 0.0726 | 0.0521
Epoch 90/300, seasonal_2 Loss: 0.0721 | 0.0526
Epoch 91/300, seasonal_2 Loss: 0.0723 | 0.0535
Epoch 92/300, seasonal_2 Loss: 0.0734 | 0.0590
Epoch 93/300, seasonal_2 Loss: 0.0765 | 0.0577
Epoch 94/300, seasonal_2 Loss: 0.0841 | 0.0607
Epoch 95/300, seasonal_2 Loss: 0.0823 | 0.0727
Epoch 96/300, seasonal_2 Loss: 0.0874 | 0.0601
Epoch 97/300, seasonal_2 Loss: 0.0853 | 0.0551
Epoch 98/300, seasonal_2 Loss: 0.0831 | 0.0628
Epoch 99/300, seasonal_2 Loss: 0.0790 | 0.0568
Epoch 100/300, seasonal_2 Loss: 0.0785 | 0.0550
Epoch 101/300, seasonal_2 Loss: 0.0865 | 0.0577
Epoch 102/300, seasonal_2 Loss: 0.0731 | 0.0590
Epoch 103/300, seasonal_2 Loss: 0.0721 | 0.0525
Epoch 104/300, seasonal_2 Loss: 0.0795 | 0.0638
Epoch 105/300, seasonal_2 Loss: 0.0746 | 0.0590
Epoch 106/300, seasonal_2 Loss: 0.0749 | 0.0555
Epoch 107/300, seasonal_2 Loss: 0.0790 | 0.0578
Epoch 108/300, seasonal_2 Loss: 0.0756 | 0.0593
Epoch 109/300, seasonal_2 Loss: 0.0747 | 0.0508
Epoch 110/300, seasonal_2 Loss: 0.0729 | 0.0535
Epoch 111/300, seasonal_2 Loss: 0.0690 | 0.0508
Epoch 112/300, seasonal_2 Loss: 0.0691 | 0.0507
Epoch 113/300, seasonal_2 Loss: 0.0716 | 0.0546
Epoch 114/300, seasonal_2 Loss: 0.0721 | 0.0468
Epoch 115/300, seasonal_2 Loss: 0.0686 | 0.0493
Epoch 116/300, seasonal_2 Loss: 0.0679 | 0.0536
Epoch 117/300, seasonal_2 Loss: 0.0723 | 0.0498
Epoch 118/300, seasonal_2 Loss: 0.0725 | 0.0551
Epoch 119/300, seasonal_2 Loss: 0.0757 | 0.0513
Epoch 120/300, seasonal_2 Loss: 0.0733 | 0.0558
Epoch 121/300, seasonal_2 Loss: 0.0750 | 0.0556
Epoch 122/300, seasonal_2 Loss: 0.0777 | 0.0501
Epoch 123/300, seasonal_2 Loss: 0.0672 | 0.0499
Epoch 124/300, seasonal_2 Loss: 0.0642 | 0.0497
Epoch 125/300, seasonal_2 Loss: 0.0702 | 0.0480
Epoch 126/300, seasonal_2 Loss: 0.0637 | 0.0513
Epoch 127/300, seasonal_2 Loss: 0.0622 | 0.0482
Epoch 128/300, seasonal_2 Loss: 0.0662 | 0.0482
Epoch 129/300, seasonal_2 Loss: 0.0607 | 0.0519
Epoch 130/300, seasonal_2 Loss: 0.0594 | 0.0460
Epoch 131/300, seasonal_2 Loss: 0.0614 | 0.0469
Epoch 132/300, seasonal_2 Loss: 0.0624 | 0.0549
Epoch 133/300, seasonal_2 Loss: 0.0597 | 0.0457
Epoch 134/300, seasonal_2 Loss: 0.0609 | 0.0470
Epoch 135/300, seasonal_2 Loss: 0.0585 | 0.0482
Epoch 136/300, seasonal_2 Loss: 0.0576 | 0.0453
Epoch 137/300, seasonal_2 Loss: 0.0603 | 0.0472
Epoch 138/300, seasonal_2 Loss: 0.0597 | 0.0468
Epoch 139/300, seasonal_2 Loss: 0.0628 | 0.0468
Epoch 140/300, seasonal_2 Loss: 0.0628 | 0.0515
Epoch 141/300, seasonal_2 Loss: 0.0625 | 0.0477
Epoch 142/300, seasonal_2 Loss: 0.0694 | 0.0523
Epoch 143/300, seasonal_2 Loss: 0.0705 | 0.0584
Epoch 144/300, seasonal_2 Loss: 0.0661 | 0.0474
Epoch 145/300, seasonal_2 Loss: 0.0705 | 0.0576
Epoch 146/300, seasonal_2 Loss: 0.0629 | 0.0515
Epoch 147/300, seasonal_2 Loss: 0.0601 | 0.0484
Epoch 148/300, seasonal_2 Loss: 0.0605 | 0.0494
Epoch 149/300, seasonal_2 Loss: 0.0564 | 0.0456
Epoch 150/300, seasonal_2 Loss: 0.0550 | 0.0449
Epoch 151/300, seasonal_2 Loss: 0.0549 | 0.0484
Epoch 152/300, seasonal_2 Loss: 0.0542 | 0.0442
Epoch 153/300, seasonal_2 Loss: 0.0532 | 0.0468
Epoch 154/300, seasonal_2 Loss: 0.0516 | 0.0465
Epoch 155/300, seasonal_2 Loss: 0.0528 | 0.0449
Epoch 156/300, seasonal_2 Loss: 0.0532 | 0.0454
Epoch 157/300, seasonal_2 Loss: 0.0509 | 0.0457
Epoch 158/300, seasonal_2 Loss: 0.0529 | 0.0444
Epoch 159/300, seasonal_2 Loss: 0.0522 | 0.0450
Epoch 160/300, seasonal_2 Loss: 0.0509 | 0.0441
Epoch 161/300, seasonal_2 Loss: 0.0508 | 0.0454
Epoch 162/300, seasonal_2 Loss: 0.0517 | 0.0446
Epoch 163/300, seasonal_2 Loss: 0.0514 | 0.0460
Epoch 164/300, seasonal_2 Loss: 0.0524 | 0.0484
Epoch 165/300, seasonal_2 Loss: 0.0551 | 0.0457
Epoch 166/300, seasonal_2 Loss: 0.0571 | 0.0496
Epoch 167/300, seasonal_2 Loss: 0.0570 | 0.0479
Epoch 168/300, seasonal_2 Loss: 0.0581 | 0.0469
Epoch 169/300, seasonal_2 Loss: 0.0564 | 0.0462
Epoch 170/300, seasonal_2 Loss: 0.0550 | 0.0519
Epoch 171/300, seasonal_2 Loss: 0.0533 | 0.0447
Epoch 172/300, seasonal_2 Loss: 0.0505 | 0.0479
Epoch 173/300, seasonal_2 Loss: 0.0513 | 0.0449
Epoch 174/300, seasonal_2 Loss: 0.0492 | 0.0434
Epoch 175/300, seasonal_2 Loss: 0.0485 | 0.0450
Epoch 176/300, seasonal_2 Loss: 0.0478 | 0.0454
Epoch 177/300, seasonal_2 Loss: 0.0471 | 0.0424
Epoch 178/300, seasonal_2 Loss: 0.0478 | 0.0441
Epoch 179/300, seasonal_2 Loss: 0.0565 | 0.0465
Epoch 180/300, seasonal_2 Loss: 0.0539 | 0.0424
Epoch 181/300, seasonal_2 Loss: 0.0492 | 0.0436
Epoch 182/300, seasonal_2 Loss: 0.0470 | 0.0443
Epoch 183/300, seasonal_2 Loss: 0.0463 | 0.0433
Epoch 184/300, seasonal_2 Loss: 0.0476 | 0.0430
Epoch 185/300, seasonal_2 Loss: 0.0478 | 0.0469
Epoch 186/300, seasonal_2 Loss: 0.0465 | 0.0441
Epoch 187/300, seasonal_2 Loss: 0.0464 | 0.0445
Epoch 188/300, seasonal_2 Loss: 0.0483 | 0.0467
Epoch 189/300, seasonal_2 Loss: 0.0478 | 0.0474
Epoch 190/300, seasonal_2 Loss: 0.0470 | 0.0415
Epoch 191/300, seasonal_2 Loss: 0.0511 | 0.0471
Epoch 192/300, seasonal_2 Loss: 0.0485 | 0.0449
Epoch 193/300, seasonal_2 Loss: 0.0475 | 0.0440
Epoch 194/300, seasonal_2 Loss: 0.0521 | 0.0477
Epoch 195/300, seasonal_2 Loss: 0.0495 | 0.0448
Epoch 196/300, seasonal_2 Loss: 0.0453 | 0.0428
Epoch 197/300, seasonal_2 Loss: 0.0494 | 0.0471
Epoch 198/300, seasonal_2 Loss: 0.0468 | 0.0448
Epoch 199/300, seasonal_2 Loss: 0.0446 | 0.0431
Epoch 200/300, seasonal_2 Loss: 0.0478 | 0.0440
Epoch 201/300, seasonal_2 Loss: 0.0464 | 0.0448
Epoch 202/300, seasonal_2 Loss: 0.0441 | 0.0436
Epoch 203/300, seasonal_2 Loss: 0.0438 | 0.0437
Epoch 204/300, seasonal_2 Loss: 0.0426 | 0.0458
Epoch 205/300, seasonal_2 Loss: 0.0422 | 0.0432
Epoch 206/300, seasonal_2 Loss: 0.0424 | 0.0430
Epoch 207/300, seasonal_2 Loss: 0.0417 | 0.0439
Epoch 208/300, seasonal_2 Loss: 0.0415 | 0.0445
Epoch 209/300, seasonal_2 Loss: 0.0420 | 0.0426
Epoch 210/300, seasonal_2 Loss: 0.0433 | 0.0446
Epoch 211/300, seasonal_2 Loss: 0.0438 | 0.0442
Epoch 212/300, seasonal_2 Loss: 0.0440 | 0.0437
Epoch 213/300, seasonal_2 Loss: 0.0440 | 0.0437
Epoch 214/300, seasonal_2 Loss: 0.0427 | 0.0456
Epoch 215/300, seasonal_2 Loss: 0.0416 | 0.0435
Epoch 216/300, seasonal_2 Loss: 0.0421 | 0.0432
Epoch 217/300, seasonal_2 Loss: 0.0426 | 0.0465
Epoch 218/300, seasonal_2 Loss: 0.0420 | 0.0439
Epoch 219/300, seasonal_2 Loss: 0.0421 | 0.0437
Epoch 220/300, seasonal_2 Loss: 0.0435 | 0.0459
Epoch 221/300, seasonal_2 Loss: 0.0435 | 0.0439
Epoch 222/300, seasonal_2 Loss: 0.0424 | 0.0460
Epoch 223/300, seasonal_2 Loss: 0.0477 | 0.0461
Epoch 224/300, seasonal_2 Loss: 0.0448 | 0.0440
Epoch 225/300, seasonal_2 Loss: 0.0424 | 0.0451
Epoch 226/300, seasonal_2 Loss: 0.0449 | 0.0441
Epoch 227/300, seasonal_2 Loss: 0.0424 | 0.0433
Epoch 228/300, seasonal_2 Loss: 0.0392 | 0.0444
Epoch 229/300, seasonal_2 Loss: 0.0393 | 0.0444
Epoch 230/300, seasonal_2 Loss: 0.0382 | 0.0424
Epoch 231/300, seasonal_2 Loss: 0.0377 | 0.0438
Epoch 232/300, seasonal_2 Loss: 0.0373 | 0.0435
Epoch 233/300, seasonal_2 Loss: 0.0386 | 0.0427
Epoch 234/300, seasonal_2 Loss: 0.0389 | 0.0425
Epoch 235/300, seasonal_2 Loss: 0.0462 | 0.0459
Epoch 236/300, seasonal_2 Loss: 0.0448 | 0.0439
Epoch 237/300, seasonal_2 Loss: 0.0416 | 0.0430
Epoch 238/300, seasonal_2 Loss: 0.0398 | 0.0426
Epoch 239/300, seasonal_2 Loss: 0.0387 | 0.0424
Epoch 240/300, seasonal_2 Loss: 0.0366 | 0.0430
Epoch 241/300, seasonal_2 Loss: 0.0359 | 0.0421
Epoch 242/300, seasonal_2 Loss: 0.0363 | 0.0421
Epoch 243/300, seasonal_2 Loss: 0.0361 | 0.0436
Epoch 244/300, seasonal_2 Loss: 0.0359 | 0.0449
Epoch 245/300, seasonal_2 Loss: 0.0365 | 0.0419
Epoch 246/300, seasonal_2 Loss: 0.0370 | 0.0436
Epoch 247/300, seasonal_2 Loss: 0.0376 | 0.0429
Epoch 248/300, seasonal_2 Loss: 0.0372 | 0.0436
Epoch 249/300, seasonal_2 Loss: 0.0371 | 0.0441
Epoch 250/300, seasonal_2 Loss: 0.0376 | 0.0457
Epoch 251/300, seasonal_2 Loss: 0.0363 | 0.0430
Epoch 252/300, seasonal_2 Loss: 0.0371 | 0.0454
Epoch 253/300, seasonal_2 Loss: 0.0365 | 0.0457
Epoch 254/300, seasonal_2 Loss: 0.0353 | 0.0429
Epoch 255/300, seasonal_2 Loss: 0.0375 | 0.0446
Epoch 256/300, seasonal_2 Loss: 0.0360 | 0.0435
Epoch 257/300, seasonal_2 Loss: 0.0350 | 0.0438
Epoch 258/300, seasonal_2 Loss: 0.0363 | 0.0442
Epoch 259/300, seasonal_2 Loss: 0.0356 | 0.0452
Epoch 260/300, seasonal_2 Loss: 0.0357 | 0.0432
Epoch 261/300, seasonal_2 Loss: 0.0365 | 0.0454
Epoch 262/300, seasonal_2 Loss: 0.0369 | 0.0459
Epoch 263/300, seasonal_2 Loss: 0.0363 | 0.0438
Epoch 264/300, seasonal_2 Loss: 0.0369 | 0.0431
Epoch 265/300, seasonal_2 Loss: 0.0359 | 0.0438
Epoch 266/300, seasonal_2 Loss: 0.0368 | 0.0419
Epoch 267/300, seasonal_2 Loss: 0.0361 | 0.0431
Epoch 268/300, seasonal_2 Loss: 0.0352 | 0.0459
Epoch 269/300, seasonal_2 Loss: 0.0341 | 0.0433
Epoch 270/300, seasonal_2 Loss: 0.0342 | 0.0432
Epoch 271/300, seasonal_2 Loss: 0.0339 | 0.0440
Epoch 272/300, seasonal_2 Loss: 0.0347 | 0.0435
Epoch 273/300, seasonal_2 Loss: 0.0339 | 0.0433
Epoch 274/300, seasonal_2 Loss: 0.0340 | 0.0447
Epoch 275/300, seasonal_2 Loss: 0.0338 | 0.0449
Epoch 276/300, seasonal_2 Loss: 0.0342 | 0.0437
Epoch 277/300, seasonal_2 Loss: 0.0354 | 0.0432
Epoch 278/300, seasonal_2 Loss: 0.0352 | 0.0451
Epoch 279/300, seasonal_2 Loss: 0.0346 | 0.0433
Epoch 280/300, seasonal_2 Loss: 0.0353 | 0.0451
Epoch 281/300, seasonal_2 Loss: 0.0349 | 0.0432
Epoch 282/300, seasonal_2 Loss: 0.0347 | 0.0441
Epoch 283/300, seasonal_2 Loss: 0.0353 | 0.0454
Epoch 284/300, seasonal_2 Loss: 0.0337 | 0.0440
Epoch 285/300, seasonal_2 Loss: 0.0330 | 0.0436
Epoch 286/300, seasonal_2 Loss: 0.0332 | 0.0458
Epoch 287/300, seasonal_2 Loss: 0.0325 | 0.0444
Epoch 288/300, seasonal_2 Loss: 0.0327 | 0.0433
Epoch 289/300, seasonal_2 Loss: 0.0330 | 0.0450
Epoch 290/300, seasonal_2 Loss: 0.0330 | 0.0444
Epoch 291/300, seasonal_2 Loss: 0.0332 | 0.0438
Epoch 292/300, seasonal_2 Loss: 0.0322 | 0.0441
Epoch 293/300, seasonal_2 Loss: 0.0334 | 0.0445
Epoch 294/300, seasonal_2 Loss: 0.0328 | 0.0434
Epoch 295/300, seasonal_2 Loss: 0.0327 | 0.0448
Epoch 296/300, seasonal_2 Loss: 0.0326 | 0.0458
Epoch 297/300, seasonal_2 Loss: 0.0323 | 0.0432
Epoch 298/300, seasonal_2 Loss: 0.0318 | 0.0455
Epoch 299/300, seasonal_2 Loss: 0.0324 | 0.0442
Epoch 300/300, seasonal_2 Loss: 0.0338 | 0.0433
Training seasonal_3 component with params: {'observation_period_num': 19, 'train_rates': 0.9879728176997175, 'learning_rate': 0.00019584153967016805, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9562493057902612}
Epoch 1/300, seasonal_3 Loss: 0.6581 | 0.6829
Epoch 2/300, seasonal_3 Loss: 0.6948 | 0.6866
Epoch 3/300, seasonal_3 Loss: 0.6020 | 0.5742
Epoch 4/300, seasonal_3 Loss: 0.5962 | 0.5979
Epoch 5/300, seasonal_3 Loss: 0.5452 | 0.5637
Epoch 6/300, seasonal_3 Loss: 0.5240 | 0.4021
Epoch 7/300, seasonal_3 Loss: 0.4959 | 0.5068
Epoch 8/300, seasonal_3 Loss: 0.5609 | 0.4815
Epoch 9/300, seasonal_3 Loss: 0.4942 | 0.5389
Epoch 10/300, seasonal_3 Loss: 0.3686 | 0.3527
Epoch 11/300, seasonal_3 Loss: 0.3169 | 0.3298
Epoch 12/300, seasonal_3 Loss: 0.3311 | 0.3187
Epoch 13/300, seasonal_3 Loss: 0.2909 | 0.2755
Epoch 14/300, seasonal_3 Loss: 0.2712 | 0.2468
Epoch 15/300, seasonal_3 Loss: 0.2566 | 0.2350
Epoch 16/300, seasonal_3 Loss: 0.2502 | 0.2939
Epoch 17/300, seasonal_3 Loss: 0.2471 | 0.2248
Epoch 18/300, seasonal_3 Loss: 0.2453 | 0.2258
Epoch 19/300, seasonal_3 Loss: 0.2664 | 0.2060
Epoch 20/300, seasonal_3 Loss: 0.2355 | 0.2279
Epoch 21/300, seasonal_3 Loss: 0.2206 | 0.2032
Epoch 22/300, seasonal_3 Loss: 0.2089 | 0.2425
Epoch 23/300, seasonal_3 Loss: 0.2183 | 0.2288
Epoch 24/300, seasonal_3 Loss: 0.2048 | 0.1860
Epoch 25/300, seasonal_3 Loss: 0.2259 | 0.2406
Epoch 26/300, seasonal_3 Loss: 0.2229 | 0.1975
Epoch 27/300, seasonal_3 Loss: 0.2201 | 0.1998
Epoch 28/300, seasonal_3 Loss: 0.2192 | 0.1679
Epoch 29/300, seasonal_3 Loss: 0.2251 | 0.1666
Epoch 30/300, seasonal_3 Loss: 0.2006 | 0.1884
Epoch 31/300, seasonal_3 Loss: 0.1977 | 0.1541
Epoch 32/300, seasonal_3 Loss: 0.1945 | 0.1803
Epoch 33/300, seasonal_3 Loss: 0.1753 | 0.1740
Epoch 34/300, seasonal_3 Loss: 0.1581 | 0.1460
Epoch 35/300, seasonal_3 Loss: 0.1623 | 0.1439
Epoch 36/300, seasonal_3 Loss: 0.1664 | 0.1940
Epoch 37/300, seasonal_3 Loss: 0.1559 | 0.1412
Epoch 38/300, seasonal_3 Loss: 0.1628 | 0.1482
Epoch 39/300, seasonal_3 Loss: 0.1729 | 0.1480
Epoch 40/300, seasonal_3 Loss: 0.1644 | 0.1612
Epoch 41/300, seasonal_3 Loss: 0.1526 | 0.1351
Epoch 42/300, seasonal_3 Loss: 0.1596 | 0.1560
Epoch 43/300, seasonal_3 Loss: 0.1493 | 0.1430
Epoch 44/300, seasonal_3 Loss: 0.1665 | 0.1549
Epoch 45/300, seasonal_3 Loss: 0.1601 | 0.1268
Epoch 46/300, seasonal_3 Loss: 0.1597 | 0.1532
Epoch 47/300, seasonal_3 Loss: 0.1554 | 0.1217
Epoch 48/300, seasonal_3 Loss: 0.1528 | 0.1646
Epoch 49/300, seasonal_3 Loss: 0.1493 | 0.1355
Epoch 50/300, seasonal_3 Loss: 0.1482 | 0.1103
Epoch 51/300, seasonal_3 Loss: 0.1419 | 0.1206
Epoch 52/300, seasonal_3 Loss: 0.1365 | 0.1166
Epoch 53/300, seasonal_3 Loss: 0.1260 | 0.1035
Epoch 54/300, seasonal_3 Loss: 0.1257 | 0.1129
Epoch 55/300, seasonal_3 Loss: 0.1295 | 0.0972
Epoch 56/300, seasonal_3 Loss: 0.1123 | 0.1028
Epoch 57/300, seasonal_3 Loss: 0.1117 | 0.0861
Epoch 58/300, seasonal_3 Loss: 0.1113 | 0.1052
Epoch 59/300, seasonal_3 Loss: 0.1109 | 0.0937
Epoch 60/300, seasonal_3 Loss: 0.1097 | 0.0855
Epoch 61/300, seasonal_3 Loss: 0.1142 | 0.0906
Epoch 62/300, seasonal_3 Loss: 0.1190 | 0.0939
Epoch 63/300, seasonal_3 Loss: 0.1245 | 0.1057
Epoch 64/300, seasonal_3 Loss: 0.1174 | 0.0977
Epoch 65/300, seasonal_3 Loss: 0.1263 | 0.0969
Epoch 66/300, seasonal_3 Loss: 0.1266 | 0.0958
Epoch 67/300, seasonal_3 Loss: 0.1157 | 0.0900
Epoch 68/300, seasonal_3 Loss: 0.1220 | 0.1132
Epoch 69/300, seasonal_3 Loss: 0.1027 | 0.0865
Epoch 70/300, seasonal_3 Loss: 0.1014 | 0.0873
Epoch 71/300, seasonal_3 Loss: 0.1022 | 0.0902
Epoch 72/300, seasonal_3 Loss: 0.1015 | 0.0851
Epoch 73/300, seasonal_3 Loss: 0.1068 | 0.0864
Epoch 74/300, seasonal_3 Loss: 0.1079 | 0.0920
Epoch 75/300, seasonal_3 Loss: 0.1124 | 0.0894
Epoch 76/300, seasonal_3 Loss: 0.1077 | 0.0888
Epoch 77/300, seasonal_3 Loss: 0.1055 | 0.1083
Epoch 78/300, seasonal_3 Loss: 0.1037 | 0.0828
Epoch 79/300, seasonal_3 Loss: 0.0999 | 0.0964
Epoch 80/300, seasonal_3 Loss: 0.1058 | 0.1105
Epoch 81/300, seasonal_3 Loss: 0.1015 | 0.0844
Epoch 82/300, seasonal_3 Loss: 0.0922 | 0.0972
Epoch 83/300, seasonal_3 Loss: 0.0865 | 0.0877
Epoch 84/300, seasonal_3 Loss: 0.0888 | 0.0743
Epoch 85/300, seasonal_3 Loss: 0.0899 | 0.0919
Epoch 86/300, seasonal_3 Loss: 0.0876 | 0.0846
Epoch 87/300, seasonal_3 Loss: 0.0939 | 0.0737
Epoch 88/300, seasonal_3 Loss: 0.0904 | 0.0811
Epoch 89/300, seasonal_3 Loss: 0.0908 | 0.0903
Epoch 90/300, seasonal_3 Loss: 0.0900 | 0.0806
Epoch 91/300, seasonal_3 Loss: 0.0893 | 0.0715
Epoch 92/300, seasonal_3 Loss: 0.0863 | 0.0829
Epoch 93/300, seasonal_3 Loss: 0.0812 | 0.0756
Epoch 94/300, seasonal_3 Loss: 0.0783 | 0.0806
Epoch 95/300, seasonal_3 Loss: 0.0884 | 0.0701
Epoch 96/300, seasonal_3 Loss: 0.0834 | 0.0845
Epoch 97/300, seasonal_3 Loss: 0.0834 | 0.0657
Epoch 98/300, seasonal_3 Loss: 0.0867 | 0.1101
Epoch 99/300, seasonal_3 Loss: 0.0850 | 0.1047
Epoch 100/300, seasonal_3 Loss: 0.0871 | 0.0710
Epoch 101/300, seasonal_3 Loss: 0.0841 | 0.0810
Epoch 102/300, seasonal_3 Loss: 0.0827 | 0.0824
Epoch 103/300, seasonal_3 Loss: 0.0862 | 0.0721
Epoch 104/300, seasonal_3 Loss: 0.0864 | 0.0954
Epoch 105/300, seasonal_3 Loss: 0.0807 | 0.0817
Epoch 106/300, seasonal_3 Loss: 0.0742 | 0.0821
Epoch 107/300, seasonal_3 Loss: 0.0724 | 0.0837
Epoch 108/300, seasonal_3 Loss: 0.0702 | 0.0714
Epoch 109/300, seasonal_3 Loss: 0.0688 | 0.0728
Epoch 110/300, seasonal_3 Loss: 0.0692 | 0.0765
Epoch 111/300, seasonal_3 Loss: 0.0678 | 0.0720
Epoch 112/300, seasonal_3 Loss: 0.0662 | 0.0778
Epoch 113/300, seasonal_3 Loss: 0.0641 | 0.0686
Epoch 114/300, seasonal_3 Loss: 0.0663 | 0.0710
Epoch 115/300, seasonal_3 Loss: 0.0671 | 0.0755
Epoch 116/300, seasonal_3 Loss: 0.0673 | 0.0729
Epoch 117/300, seasonal_3 Loss: 0.0680 | 0.0733
Epoch 118/300, seasonal_3 Loss: 0.0670 | 0.0759
Epoch 119/300, seasonal_3 Loss: 0.0691 | 0.0741
Epoch 120/300, seasonal_3 Loss: 0.0646 | 0.0674
Epoch 121/300, seasonal_3 Loss: 0.0623 | 0.0722
Epoch 122/300, seasonal_3 Loss: 0.0595 | 0.0716
Epoch 123/300, seasonal_3 Loss: 0.0611 | 0.0651
Epoch 124/300, seasonal_3 Loss: 0.0634 | 0.0858
Epoch 125/300, seasonal_3 Loss: 0.0657 | 0.0717
Epoch 126/300, seasonal_3 Loss: 0.0659 | 0.0702
Epoch 127/300, seasonal_3 Loss: 0.0656 | 0.0765
Epoch 128/300, seasonal_3 Loss: 0.0620 | 0.0648
Epoch 129/300, seasonal_3 Loss: 0.0634 | 0.0724
Epoch 130/300, seasonal_3 Loss: 0.0680 | 0.0816
Epoch 131/300, seasonal_3 Loss: 0.0669 | 0.0693
Epoch 132/300, seasonal_3 Loss: 0.0629 | 0.0834
Epoch 133/300, seasonal_3 Loss: 0.0680 | 0.0806
Epoch 134/300, seasonal_3 Loss: 0.0670 | 0.0708
Epoch 135/300, seasonal_3 Loss: 0.0620 | 0.0703
Epoch 136/300, seasonal_3 Loss: 0.0595 | 0.0712
Epoch 137/300, seasonal_3 Loss: 0.0575 | 0.0666
Epoch 138/300, seasonal_3 Loss: 0.0571 | 0.0733
Epoch 139/300, seasonal_3 Loss: 0.0570 | 0.0710
Epoch 140/300, seasonal_3 Loss: 0.0572 | 0.0668
Epoch 141/300, seasonal_3 Loss: 0.0559 | 0.0730
Epoch 142/300, seasonal_3 Loss: 0.0539 | 0.0631
Epoch 143/300, seasonal_3 Loss: 0.0534 | 0.0656
Epoch 144/300, seasonal_3 Loss: 0.0522 | 0.0689
Epoch 145/300, seasonal_3 Loss: 0.0527 | 0.0630
Epoch 146/300, seasonal_3 Loss: 0.0528 | 0.0761
Epoch 147/300, seasonal_3 Loss: 0.0530 | 0.0674
Epoch 148/300, seasonal_3 Loss: 0.0547 | 0.0767
Epoch 149/300, seasonal_3 Loss: 0.0533 | 0.0707
Epoch 150/300, seasonal_3 Loss: 0.0543 | 0.0717
Epoch 151/300, seasonal_3 Loss: 0.0535 | 0.0722
Epoch 152/300, seasonal_3 Loss: 0.0532 | 0.0724
Epoch 153/300, seasonal_3 Loss: 0.0513 | 0.0725
Epoch 154/300, seasonal_3 Loss: 0.0497 | 0.0723
Epoch 155/300, seasonal_3 Loss: 0.0508 | 0.0693
Epoch 156/300, seasonal_3 Loss: 0.0507 | 0.0680
Epoch 157/300, seasonal_3 Loss: 0.0535 | 0.0745
Epoch 158/300, seasonal_3 Loss: 0.0524 | 0.0665
Epoch 159/300, seasonal_3 Loss: 0.0504 | 0.0730
Epoch 160/300, seasonal_3 Loss: 0.0527 | 0.0807
Epoch 161/300, seasonal_3 Loss: 0.0523 | 0.0668
Epoch 162/300, seasonal_3 Loss: 0.0509 | 0.0748
Epoch 163/300, seasonal_3 Loss: 0.0505 | 0.0712
Epoch 164/300, seasonal_3 Loss: 0.0502 | 0.0635
Epoch 165/300, seasonal_3 Loss: 0.0476 | 0.0712
Epoch 166/300, seasonal_3 Loss: 0.0462 | 0.0643
Epoch 167/300, seasonal_3 Loss: 0.0477 | 0.0694
Epoch 168/300, seasonal_3 Loss: 0.0482 | 0.0718
Epoch 169/300, seasonal_3 Loss: 0.0479 | 0.0651
Epoch 170/300, seasonal_3 Loss: 0.0476 | 0.0703
Epoch 171/300, seasonal_3 Loss: 0.0490 | 0.0684
Epoch 172/300, seasonal_3 Loss: 0.0475 | 0.0615
Epoch 173/300, seasonal_3 Loss: 0.0464 | 0.0716
Epoch 174/300, seasonal_3 Loss: 0.0461 | 0.0649
Epoch 175/300, seasonal_3 Loss: 0.0453 | 0.0659
Epoch 176/300, seasonal_3 Loss: 0.0449 | 0.0707
Epoch 177/300, seasonal_3 Loss: 0.0449 | 0.0624
Epoch 178/300, seasonal_3 Loss: 0.0453 | 0.0694
Epoch 179/300, seasonal_3 Loss: 0.0453 | 0.0650
Epoch 180/300, seasonal_3 Loss: 0.0452 | 0.0640
Epoch 181/300, seasonal_3 Loss: 0.0447 | 0.0664
Epoch 182/300, seasonal_3 Loss: 0.0432 | 0.0624
Epoch 183/300, seasonal_3 Loss: 0.0419 | 0.0671
Epoch 184/300, seasonal_3 Loss: 0.0414 | 0.0659
Epoch 185/300, seasonal_3 Loss: 0.0416 | 0.0667
Epoch 186/300, seasonal_3 Loss: 0.0412 | 0.0661
Epoch 187/300, seasonal_3 Loss: 0.0413 | 0.0660
Epoch 188/300, seasonal_3 Loss: 0.0413 | 0.0636
Epoch 189/300, seasonal_3 Loss: 0.0419 | 0.0684
Epoch 190/300, seasonal_3 Loss: 0.0510 | 0.0635
Epoch 191/300, seasonal_3 Loss: 0.0433 | 0.0647
Epoch 192/300, seasonal_3 Loss: 0.0498 | 0.0684
Epoch 193/300, seasonal_3 Loss: 0.0434 | 0.0660
Epoch 194/300, seasonal_3 Loss: 0.0427 | 0.0645
Epoch 195/300, seasonal_3 Loss: 0.0434 | 0.0685
Epoch 196/300, seasonal_3 Loss: 0.0435 | 0.0631
Epoch 197/300, seasonal_3 Loss: 0.0417 | 0.0687
Epoch 198/300, seasonal_3 Loss: 0.0405 | 0.0647
Epoch 199/300, seasonal_3 Loss: 0.0412 | 0.0646
Epoch 200/300, seasonal_3 Loss: 0.0406 | 0.0680
Epoch 201/300, seasonal_3 Loss: 0.0400 | 0.0661
Epoch 202/300, seasonal_3 Loss: 0.0393 | 0.0670
Epoch 203/300, seasonal_3 Loss: 0.0391 | 0.0654
Epoch 204/300, seasonal_3 Loss: 0.0386 | 0.0632
Epoch 205/300, seasonal_3 Loss: 0.0388 | 0.0666
Epoch 206/300, seasonal_3 Loss: 0.0377 | 0.0639
Epoch 207/300, seasonal_3 Loss: 0.0382 | 0.0633
Epoch 208/300, seasonal_3 Loss: 0.0377 | 0.0672
Epoch 209/300, seasonal_3 Loss: 0.0379 | 0.0653
Epoch 210/300, seasonal_3 Loss: 0.0382 | 0.0648
Epoch 211/300, seasonal_3 Loss: 0.0381 | 0.0687
Epoch 212/300, seasonal_3 Loss: 0.0370 | 0.0640
Epoch 213/300, seasonal_3 Loss: 0.0368 | 0.0685
Epoch 214/300, seasonal_3 Loss: 0.0368 | 0.0668
Epoch 215/300, seasonal_3 Loss: 0.0374 | 0.0663
Epoch 216/300, seasonal_3 Loss: 0.0373 | 0.0700
Epoch 217/300, seasonal_3 Loss: 0.0370 | 0.0622
Epoch 218/300, seasonal_3 Loss: 0.0367 | 0.0675
Epoch 219/300, seasonal_3 Loss: 0.0364 | 0.0652
Epoch 220/300, seasonal_3 Loss: 0.0361 | 0.0652
Epoch 221/300, seasonal_3 Loss: 0.0362 | 0.0686
Epoch 222/300, seasonal_3 Loss: 0.0360 | 0.0651
Epoch 223/300, seasonal_3 Loss: 0.0361 | 0.0658
Epoch 224/300, seasonal_3 Loss: 0.0362 | 0.0669
Epoch 225/300, seasonal_3 Loss: 0.0359 | 0.0628
Epoch 226/300, seasonal_3 Loss: 0.0366 | 0.0685
Epoch 227/300, seasonal_3 Loss: 0.0369 | 0.0677
Epoch 228/300, seasonal_3 Loss: 0.0363 | 0.0657
Epoch 229/300, seasonal_3 Loss: 0.0352 | 0.0685
Epoch 230/300, seasonal_3 Loss: 0.0361 | 0.0664
Epoch 231/300, seasonal_3 Loss: 0.0360 | 0.0681
Epoch 232/300, seasonal_3 Loss: 0.0361 | 0.0701
Epoch 233/300, seasonal_3 Loss: 0.0361 | 0.0679
Epoch 234/300, seasonal_3 Loss: 0.0348 | 0.0668
Epoch 235/300, seasonal_3 Loss: 0.0356 | 0.0674
Epoch 236/300, seasonal_3 Loss: 0.0350 | 0.0679
Epoch 237/300, seasonal_3 Loss: 0.0344 | 0.0667
Epoch 238/300, seasonal_3 Loss: 0.0344 | 0.0660
Epoch 239/300, seasonal_3 Loss: 0.0342 | 0.0668
Epoch 240/300, seasonal_3 Loss: 0.0342 | 0.0691
Epoch 241/300, seasonal_3 Loss: 0.0339 | 0.0665
Epoch 242/300, seasonal_3 Loss: 0.0335 | 0.0683
Epoch 243/300, seasonal_3 Loss: 0.0337 | 0.0654
Epoch 244/300, seasonal_3 Loss: 0.0338 | 0.0646
Epoch 245/300, seasonal_3 Loss: 0.0334 | 0.0681
Epoch 246/300, seasonal_3 Loss: 0.0329 | 0.0651
Epoch 247/300, seasonal_3 Loss: 0.0334 | 0.0669
Epoch 248/300, seasonal_3 Loss: 0.0331 | 0.0667
Epoch 249/300, seasonal_3 Loss: 0.0328 | 0.0665
Epoch 250/300, seasonal_3 Loss: 0.0331 | 0.0682
Epoch 251/300, seasonal_3 Loss: 0.0331 | 0.0668
Epoch 252/300, seasonal_3 Loss: 0.0329 | 0.0665
Epoch 253/300, seasonal_3 Loss: 0.0327 | 0.0663
Epoch 254/300, seasonal_3 Loss: 0.0323 | 0.0621
Epoch 255/300, seasonal_3 Loss: 0.0327 | 0.0687
Epoch 256/300, seasonal_3 Loss: 0.0323 | 0.0644
Epoch 257/300, seasonal_3 Loss: 0.0328 | 0.0666
Epoch 258/300, seasonal_3 Loss: 0.0324 | 0.0675
Epoch 259/300, seasonal_3 Loss: 0.0323 | 0.0637
Epoch 260/300, seasonal_3 Loss: 0.0318 | 0.0658
Epoch 261/300, seasonal_3 Loss: 0.0326 | 0.0627
Epoch 262/300, seasonal_3 Loss: 0.0324 | 0.0649
Epoch 263/300, seasonal_3 Loss: 0.0317 | 0.0669
Epoch 264/300, seasonal_3 Loss: 0.0317 | 0.0657
Epoch 265/300, seasonal_3 Loss: 0.0314 | 0.0666
Epoch 266/300, seasonal_3 Loss: 0.0314 | 0.0636
Epoch 267/300, seasonal_3 Loss: 0.0314 | 0.0648
Epoch 268/300, seasonal_3 Loss: 0.0315 | 0.0668
Epoch 269/300, seasonal_3 Loss: 0.0319 | 0.0681
Epoch 270/300, seasonal_3 Loss: 0.0312 | 0.0678
Epoch 271/300, seasonal_3 Loss: 0.0313 | 0.0648
Epoch 272/300, seasonal_3 Loss: 0.0311 | 0.0648
Epoch 273/300, seasonal_3 Loss: 0.0313 | 0.0651
Epoch 274/300, seasonal_3 Loss: 0.0311 | 0.0633
Epoch 275/300, seasonal_3 Loss: 0.0313 | 0.0653
Epoch 276/300, seasonal_3 Loss: 0.0310 | 0.0641
Epoch 277/300, seasonal_3 Loss: 0.0307 | 0.0652
Epoch 278/300, seasonal_3 Loss: 0.0301 | 0.0630
Epoch 279/300, seasonal_3 Loss: 0.0303 | 0.0655
Epoch 280/300, seasonal_3 Loss: 0.0301 | 0.0654
Epoch 281/300, seasonal_3 Loss: 0.0303 | 0.0653
Epoch 282/300, seasonal_3 Loss: 0.0300 | 0.0658
Epoch 283/300, seasonal_3 Loss: 0.0304 | 0.0658
Epoch 284/300, seasonal_3 Loss: 0.0302 | 0.0641
Epoch 285/300, seasonal_3 Loss: 0.0299 | 0.0687
Epoch 286/300, seasonal_3 Loss: 0.0301 | 0.0640
Epoch 287/300, seasonal_3 Loss: 0.0296 | 0.0650
Epoch 288/300, seasonal_3 Loss: 0.0302 | 0.0638
Epoch 289/300, seasonal_3 Loss: 0.0298 | 0.0659
Epoch 290/300, seasonal_3 Loss: 0.0297 | 0.0667
Epoch 291/300, seasonal_3 Loss: 0.0298 | 0.0657
Epoch 292/300, seasonal_3 Loss: 0.0296 | 0.0666
Epoch 293/300, seasonal_3 Loss: 0.0297 | 0.0649
Epoch 294/300, seasonal_3 Loss: 0.0298 | 0.0645
Epoch 295/300, seasonal_3 Loss: 0.0296 | 0.0658
Epoch 296/300, seasonal_3 Loss: 0.0295 | 0.0637
Epoch 297/300, seasonal_3 Loss: 0.0296 | 0.0643
Epoch 298/300, seasonal_3 Loss: 0.0292 | 0.0641
Epoch 299/300, seasonal_3 Loss: 0.0294 | 0.0649
Epoch 300/300, seasonal_3 Loss: 0.0294 | 0.0666
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9830714172914075, 'learning_rate': 0.00011718000342070418, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9221493220438013}
Epoch 1/300, resid Loss: 0.6391 | 0.7429
Epoch 2/300, resid Loss: 0.5686 | 0.5881
Epoch 3/300, resid Loss: 0.4080 | 0.5307
Epoch 4/300, resid Loss: 0.3521 | 0.4614
Epoch 5/300, resid Loss: 0.3132 | 0.3773
Epoch 6/300, resid Loss: 0.2820 | 0.3342
Epoch 7/300, resid Loss: 0.2388 | 0.3156
Epoch 8/300, resid Loss: 0.2567 | 0.2976
Epoch 9/300, resid Loss: 0.2345 | 0.2489
Epoch 10/300, resid Loss: 0.2187 | 0.2130
Epoch 11/300, resid Loss: 0.2265 | 0.2760
Epoch 12/300, resid Loss: 0.2030 | 0.2376
Epoch 13/300, resid Loss: 0.1939 | 0.2055
Epoch 14/300, resid Loss: 0.1969 | 0.1784
Epoch 15/300, resid Loss: 0.2067 | 0.2248
Epoch 16/300, resid Loss: 0.2217 | 0.2036
Epoch 17/300, resid Loss: 0.2162 | 0.1965
Epoch 18/300, resid Loss: 0.1965 | 0.1779
Epoch 19/300, resid Loss: 0.1834 | 0.2597
Epoch 20/300, resid Loss: 0.1711 | 0.1672
Epoch 21/300, resid Loss: 0.1632 | 0.1541
Epoch 22/300, resid Loss: 0.1571 | 0.1532
Epoch 23/300, resid Loss: 0.1402 | 0.1473
Epoch 24/300, resid Loss: 0.1358 | 0.1355
Epoch 25/300, resid Loss: 0.1329 | 0.1330
Epoch 26/300, resid Loss: 0.1336 | 0.1234
Epoch 27/300, resid Loss: 0.1330 | 0.1313
Epoch 28/300, resid Loss: 0.1267 | 0.1160
Epoch 29/300, resid Loss: 0.1312 | 0.1085
Epoch 30/300, resid Loss: 0.1297 | 0.1508
Epoch 31/300, resid Loss: 0.1260 | 0.1127
Epoch 32/300, resid Loss: 0.1304 | 0.1142
Epoch 33/300, resid Loss: 0.1361 | 0.1097
Epoch 34/300, resid Loss: 0.1313 | 0.1473
Epoch 35/300, resid Loss: 0.1307 | 0.1244
Epoch 36/300, resid Loss: 0.1331 | 0.1077
Epoch 37/300, resid Loss: 0.1210 | 0.1105
Epoch 38/300, resid Loss: 0.1104 | 0.0960
Epoch 39/300, resid Loss: 0.1098 | 0.0832
Epoch 40/300, resid Loss: 0.1151 | 0.0861
Epoch 41/300, resid Loss: 0.1073 | 0.0919
Epoch 42/300, resid Loss: 0.1017 | 0.0839
Epoch 43/300, resid Loss: 0.1022 | 0.0794
Epoch 44/300, resid Loss: 0.1060 | 0.0880
Epoch 45/300, resid Loss: 0.1020 | 0.1002
Epoch 46/300, resid Loss: 0.1076 | 0.0820
Epoch 47/300, resid Loss: 0.1045 | 0.0820
Epoch 48/300, resid Loss: 0.1059 | 0.1038
Epoch 49/300, resid Loss: 0.1040 | 0.0848
Epoch 50/300, resid Loss: 0.1055 | 0.0720
Epoch 51/300, resid Loss: 0.1086 | 0.0826
Epoch 52/300, resid Loss: 0.1002 | 0.0933
Epoch 53/300, resid Loss: 0.0952 | 0.0716
Epoch 54/300, resid Loss: 0.0949 | 0.0690
Epoch 55/300, resid Loss: 0.0910 | 0.0857
Epoch 56/300, resid Loss: 0.0871 | 0.0638
Epoch 57/300, resid Loss: 0.0835 | 0.0683
Epoch 58/300, resid Loss: 0.0831 | 0.0723
Epoch 59/300, resid Loss: 0.0811 | 0.0624
Epoch 60/300, resid Loss: 0.0798 | 0.0626
Epoch 61/300, resid Loss: 0.0798 | 0.0677
Epoch 62/300, resid Loss: 0.0810 | 0.0746
Epoch 63/300, resid Loss: 0.0802 | 0.0601
Epoch 64/300, resid Loss: 0.0813 | 0.0608
Epoch 65/300, resid Loss: 0.0840 | 0.0789
Epoch 66/300, resid Loss: 0.0843 | 0.0678
Epoch 67/300, resid Loss: 0.0813 | 0.0621
Epoch 68/300, resid Loss: 0.0835 | 0.0680
Epoch 69/300, resid Loss: 0.0817 | 0.0712
Epoch 70/300, resid Loss: 0.0817 | 0.0696
Epoch 71/300, resid Loss: 0.0817 | 0.0596
Epoch 72/300, resid Loss: 0.0842 | 0.0708
Epoch 73/300, resid Loss: 0.0791 | 0.0643
Epoch 74/300, resid Loss: 0.0773 | 0.0580
Epoch 75/300, resid Loss: 0.0782 | 0.0582
Epoch 76/300, resid Loss: 0.0746 | 0.0593
Epoch 77/300, resid Loss: 0.0723 | 0.0572
Epoch 78/300, resid Loss: 0.0711 | 0.0551
Epoch 79/300, resid Loss: 0.0730 | 0.0600
Epoch 80/300, resid Loss: 0.0707 | 0.0553
Epoch 81/300, resid Loss: 0.0699 | 0.0550
Epoch 82/300, resid Loss: 0.0698 | 0.0573
Epoch 83/300, resid Loss: 0.0677 | 0.0566
Epoch 84/300, resid Loss: 0.0679 | 0.0532
Epoch 85/300, resid Loss: 0.0670 | 0.0548
Epoch 86/300, resid Loss: 0.0665 | 0.0569
Epoch 87/300, resid Loss: 0.0655 | 0.0538
Epoch 88/300, resid Loss: 0.0653 | 0.0518
Epoch 89/300, resid Loss: 0.0656 | 0.0561
Epoch 90/300, resid Loss: 0.0648 | 0.0539
Epoch 91/300, resid Loss: 0.0644 | 0.0531
Epoch 92/300, resid Loss: 0.0654 | 0.0525
Epoch 93/300, resid Loss: 0.0642 | 0.0534
Epoch 94/300, resid Loss: 0.0646 | 0.0511
Epoch 95/300, resid Loss: 0.0631 | 0.0518
Epoch 96/300, resid Loss: 0.0638 | 0.0565
Epoch 97/300, resid Loss: 0.0622 | 0.0514
Epoch 98/300, resid Loss: 0.0628 | 0.0493
Epoch 99/300, resid Loss: 0.0628 | 0.0551
Epoch 100/300, resid Loss: 0.0616 | 0.0522
Epoch 101/300, resid Loss: 0.0629 | 0.0510
Epoch 102/300, resid Loss: 0.0617 | 0.0493
Epoch 103/300, resid Loss: 0.0628 | 0.0547
Epoch 104/300, resid Loss: 0.0605 | 0.0489
Epoch 105/300, resid Loss: 0.0603 | 0.0489
Epoch 106/300, resid Loss: 0.0609 | 0.0511
Epoch 107/300, resid Loss: 0.0613 | 0.0504
Epoch 108/300, resid Loss: 0.0604 | 0.0499
Epoch 109/300, resid Loss: 0.0600 | 0.0492
Epoch 110/300, resid Loss: 0.0602 | 0.0508
Epoch 111/300, resid Loss: 0.0591 | 0.0482
Epoch 112/300, resid Loss: 0.0590 | 0.0508
Epoch 113/300, resid Loss: 0.0584 | 0.0484
Epoch 114/300, resid Loss: 0.0588 | 0.0489
Epoch 115/300, resid Loss: 0.0584 | 0.0491
Epoch 116/300, resid Loss: 0.0574 | 0.0498
Epoch 117/300, resid Loss: 0.0567 | 0.0486
Epoch 118/300, resid Loss: 0.0573 | 0.0482
Epoch 119/300, resid Loss: 0.0569 | 0.0491
Epoch 120/300, resid Loss: 0.0558 | 0.0477
Epoch 121/300, resid Loss: 0.0562 | 0.0476
Epoch 122/300, resid Loss: 0.0553 | 0.0481
Epoch 123/300, resid Loss: 0.0553 | 0.0488
Epoch 124/300, resid Loss: 0.0553 | 0.0475
Epoch 125/300, resid Loss: 0.0548 | 0.0474
Epoch 126/300, resid Loss: 0.0556 | 0.0479
Epoch 127/300, resid Loss: 0.0553 | 0.0473
Epoch 128/300, resid Loss: 0.0548 | 0.0465
Epoch 129/300, resid Loss: 0.0542 | 0.0489
Epoch 130/300, resid Loss: 0.0542 | 0.0461
Epoch 131/300, resid Loss: 0.0539 | 0.0473
Epoch 132/300, resid Loss: 0.0532 | 0.0466
Epoch 133/300, resid Loss: 0.0525 | 0.0466
Epoch 134/300, resid Loss: 0.0528 | 0.0465
Epoch 135/300, resid Loss: 0.0529 | 0.0463
Epoch 136/300, resid Loss: 0.0536 | 0.0466
Epoch 137/300, resid Loss: 0.0528 | 0.0460
Epoch 138/300, resid Loss: 0.0525 | 0.0469
Epoch 139/300, resid Loss: 0.0522 | 0.0460
Epoch 140/300, resid Loss: 0.0523 | 0.0461
Epoch 141/300, resid Loss: 0.0524 | 0.0455
Epoch 142/300, resid Loss: 0.0521 | 0.0457
Epoch 143/300, resid Loss: 0.0515 | 0.0461
Epoch 144/300, resid Loss: 0.0519 | 0.0453
Epoch 145/300, resid Loss: 0.0516 | 0.0464
Epoch 146/300, resid Loss: 0.0508 | 0.0459
Epoch 147/300, resid Loss: 0.0515 | 0.0461
Epoch 148/300, resid Loss: 0.0519 | 0.0459
Epoch 149/300, resid Loss: 0.0509 | 0.0451
Epoch 150/300, resid Loss: 0.0510 | 0.0455
Epoch 151/300, resid Loss: 0.0508 | 0.0463
Epoch 152/300, resid Loss: 0.0505 | 0.0455
Epoch 153/300, resid Loss: 0.0507 | 0.0450
Epoch 154/300, resid Loss: 0.0504 | 0.0456
Epoch 155/300, resid Loss: 0.0505 | 0.0461
Epoch 156/300, resid Loss: 0.0504 | 0.0451
Epoch 157/300, resid Loss: 0.0499 | 0.0457
Epoch 158/300, resid Loss: 0.0498 | 0.0453
Epoch 159/300, resid Loss: 0.0502 | 0.0463
Epoch 160/300, resid Loss: 0.0500 | 0.0454
Epoch 161/300, resid Loss: 0.0498 | 0.0453
Epoch 162/300, resid Loss: 0.0495 | 0.0451
Epoch 163/300, resid Loss: 0.0493 | 0.0452
Epoch 164/300, resid Loss: 0.0497 | 0.0453
Epoch 165/300, resid Loss: 0.0495 | 0.0452
Epoch 166/300, resid Loss: 0.0494 | 0.0444
Epoch 167/300, resid Loss: 0.0487 | 0.0450
Epoch 168/300, resid Loss: 0.0496 | 0.0448
Epoch 169/300, resid Loss: 0.0496 | 0.0447
Epoch 170/300, resid Loss: 0.0494 | 0.0447
Epoch 171/300, resid Loss: 0.0492 | 0.0442
Epoch 172/300, resid Loss: 0.0487 | 0.0448
Epoch 173/300, resid Loss: 0.0488 | 0.0446
Epoch 174/300, resid Loss: 0.0488 | 0.0447
Epoch 175/300, resid Loss: 0.0483 | 0.0448
Epoch 176/300, resid Loss: 0.0480 | 0.0445
Epoch 177/300, resid Loss: 0.0483 | 0.0444
Epoch 178/300, resid Loss: 0.0484 | 0.0445
Epoch 179/300, resid Loss: 0.0481 | 0.0449
Epoch 180/300, resid Loss: 0.0479 | 0.0445
Epoch 181/300, resid Loss: 0.0483 | 0.0446
Epoch 182/300, resid Loss: 0.0481 | 0.0447
Epoch 183/300, resid Loss: 0.0484 | 0.0448
Epoch 184/300, resid Loss: 0.0477 | 0.0448
Epoch 185/300, resid Loss: 0.0480 | 0.0451
Epoch 186/300, resid Loss: 0.0479 | 0.0449
Epoch 187/300, resid Loss: 0.0480 | 0.0442
Epoch 188/300, resid Loss: 0.0475 | 0.0450
Epoch 189/300, resid Loss: 0.0480 | 0.0444
Epoch 190/300, resid Loss: 0.0478 | 0.0444
Epoch 191/300, resid Loss: 0.0480 | 0.0449
Epoch 192/300, resid Loss: 0.0475 | 0.0448
Epoch 193/300, resid Loss: 0.0477 | 0.0448
Epoch 194/300, resid Loss: 0.0475 | 0.0443
Epoch 195/300, resid Loss: 0.0473 | 0.0442
Epoch 196/300, resid Loss: 0.0476 | 0.0443
Epoch 197/300, resid Loss: 0.0479 | 0.0445
Epoch 198/300, resid Loss: 0.0474 | 0.0440
Epoch 199/300, resid Loss: 0.0476 | 0.0442
Epoch 200/300, resid Loss: 0.0475 | 0.0441
Epoch 201/300, resid Loss: 0.0466 | 0.0445
Epoch 202/300, resid Loss: 0.0474 | 0.0446
Epoch 203/300, resid Loss: 0.0470 | 0.0442
Epoch 204/300, resid Loss: 0.0471 | 0.0441
Epoch 205/300, resid Loss: 0.0467 | 0.0441
Epoch 206/300, resid Loss: 0.0469 | 0.0442
Epoch 207/300, resid Loss: 0.0479 | 0.0441
Epoch 208/300, resid Loss: 0.0472 | 0.0439
Epoch 209/300, resid Loss: 0.0468 | 0.0440
Epoch 210/300, resid Loss: 0.0466 | 0.0440
Epoch 211/300, resid Loss: 0.0464 | 0.0435
Epoch 212/300, resid Loss: 0.0471 | 0.0440
Epoch 213/300, resid Loss: 0.0468 | 0.0437
Epoch 214/300, resid Loss: 0.0470 | 0.0435
Epoch 215/300, resid Loss: 0.0467 | 0.0439
Epoch 216/300, resid Loss: 0.0464 | 0.0439
Epoch 217/300, resid Loss: 0.0465 | 0.0440
Epoch 218/300, resid Loss: 0.0468 | 0.0441
Epoch 219/300, resid Loss: 0.0468 | 0.0442
Epoch 220/300, resid Loss: 0.0466 | 0.0442
Epoch 221/300, resid Loss: 0.0465 | 0.0442
Epoch 222/300, resid Loss: 0.0465 | 0.0440
Epoch 223/300, resid Loss: 0.0470 | 0.0440
Epoch 224/300, resid Loss: 0.0466 | 0.0442
Epoch 225/300, resid Loss: 0.0463 | 0.0443
Epoch 226/300, resid Loss: 0.0466 | 0.0444
Epoch 227/300, resid Loss: 0.0461 | 0.0442
Epoch 228/300, resid Loss: 0.0462 | 0.0441
Epoch 229/300, resid Loss: 0.0464 | 0.0440
Epoch 230/300, resid Loss: 0.0460 | 0.0437
Epoch 231/300, resid Loss: 0.0460 | 0.0439
Epoch 232/300, resid Loss: 0.0462 | 0.0441
Epoch 233/300, resid Loss: 0.0462 | 0.0439
Epoch 234/300, resid Loss: 0.0459 | 0.0440
Epoch 235/300, resid Loss: 0.0460 | 0.0439
Epoch 236/300, resid Loss: 0.0457 | 0.0438
Epoch 237/300, resid Loss: 0.0463 | 0.0439
Epoch 238/300, resid Loss: 0.0459 | 0.0441
Epoch 239/300, resid Loss: 0.0456 | 0.0437
Epoch 240/300, resid Loss: 0.0459 | 0.0438
Epoch 241/300, resid Loss: 0.0458 | 0.0437
Epoch 242/300, resid Loss: 0.0462 | 0.0436
Epoch 243/300, resid Loss: 0.0458 | 0.0438
Epoch 244/300, resid Loss: 0.0456 | 0.0438
Epoch 245/300, resid Loss: 0.0458 | 0.0439
Epoch 246/300, resid Loss: 0.0456 | 0.0439
Epoch 247/300, resid Loss: 0.0458 | 0.0439
Epoch 248/300, resid Loss: 0.0456 | 0.0440
Epoch 249/300, resid Loss: 0.0457 | 0.0441
Epoch 250/300, resid Loss: 0.0452 | 0.0437
Epoch 251/300, resid Loss: 0.0454 | 0.0437
Epoch 252/300, resid Loss: 0.0458 | 0.0436
Epoch 253/300, resid Loss: 0.0456 | 0.0436
Epoch 254/300, resid Loss: 0.0456 | 0.0434
Epoch 255/300, resid Loss: 0.0456 | 0.0436
Epoch 256/300, resid Loss: 0.0459 | 0.0436
Epoch 257/300, resid Loss: 0.0456 | 0.0436
Epoch 258/300, resid Loss: 0.0456 | 0.0438
Epoch 259/300, resid Loss: 0.0458 | 0.0438
Epoch 260/300, resid Loss: 0.0453 | 0.0438
Epoch 261/300, resid Loss: 0.0455 | 0.0438
Epoch 262/300, resid Loss: 0.0454 | 0.0438
Epoch 263/300, resid Loss: 0.0457 | 0.0437
Epoch 264/300, resid Loss: 0.0453 | 0.0437
Epoch 265/300, resid Loss: 0.0454 | 0.0435
Epoch 266/300, resid Loss: 0.0454 | 0.0434
Epoch 267/300, resid Loss: 0.0453 | 0.0435
Epoch 268/300, resid Loss: 0.0457 | 0.0436
Epoch 269/300, resid Loss: 0.0452 | 0.0437
Epoch 270/300, resid Loss: 0.0452 | 0.0437
Epoch 271/300, resid Loss: 0.0451 | 0.0437
Epoch 272/300, resid Loss: 0.0452 | 0.0438
Epoch 273/300, resid Loss: 0.0455 | 0.0438
Epoch 274/300, resid Loss: 0.0452 | 0.0438
Epoch 275/300, resid Loss: 0.0456 | 0.0440
Epoch 276/300, resid Loss: 0.0453 | 0.0439
Epoch 277/300, resid Loss: 0.0452 | 0.0436
Epoch 278/300, resid Loss: 0.0448 | 0.0436
Epoch 279/300, resid Loss: 0.0452 | 0.0434
Epoch 280/300, resid Loss: 0.0448 | 0.0434
Epoch 281/300, resid Loss: 0.0449 | 0.0436
Epoch 282/300, resid Loss: 0.0451 | 0.0437
Epoch 283/300, resid Loss: 0.0455 | 0.0436
Epoch 284/300, resid Loss: 0.0452 | 0.0435
Epoch 285/300, resid Loss: 0.0451 | 0.0435
Epoch 286/300, resid Loss: 0.0449 | 0.0437
Epoch 287/300, resid Loss: 0.0451 | 0.0438
Epoch 288/300, resid Loss: 0.0454 | 0.0439
Epoch 289/300, resid Loss: 0.0450 | 0.0438
Epoch 290/300, resid Loss: 0.0448 | 0.0437
Epoch 291/300, resid Loss: 0.0448 | 0.0438
Epoch 292/300, resid Loss: 0.0452 | 0.0437
Epoch 293/300, resid Loss: 0.0451 | 0.0438
Epoch 294/300, resid Loss: 0.0453 | 0.0438
Epoch 295/300, resid Loss: 0.0449 | 0.0438
Epoch 296/300, resid Loss: 0.0452 | 0.0437
Epoch 297/300, resid Loss: 0.0453 | 0.0438
Epoch 298/300, resid Loss: 0.0453 | 0.0437
Epoch 299/300, resid Loss: 0.0449 | 0.0437
Epoch 300/300, resid Loss: 0.0450 | 0.0437
Runtime (seconds): 3332.7127685546875
4.57887602022304e-05
[215.0829]
[0.8863215]
[1.0692114]
[5.195834]
[-0.38383123]
[6.3288507]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 2.858481184579432
RMSE: 1.690704345703125
MAE: 1.690704345703125
R-squared: nan
[228.17929]
File aapl_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
