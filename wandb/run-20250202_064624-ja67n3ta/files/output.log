[32m[I 2025-02-02 06:46:25,580][0m A new study created in memory with name: no-name-65e35481-325a-4d4f-a56a-6d738da63c28[0m
[32m[I 2025-02-02 06:47:50,398][0m Trial 0 finished with value: 0.1949088659988759 and parameters: {'observation_period_num': 171, 'train_rates': 0.8286437504213929, 'learning_rate': 3.4262333032902225e-05, 'batch_size': 62, 'step_size': 1, 'gamma': 0.9680395088280795}. Best is trial 0 with value: 0.1949088659988759.[0m
Early stopping at epoch 90
[32m[I 2025-02-02 06:48:13,770][0m Trial 1 finished with value: 0.30983322148859815 and parameters: {'observation_period_num': 161, 'train_rates': 0.7949861443076288, 'learning_rate': 0.0001271701140727751, 'batch_size': 231, 'step_size': 2, 'gamma': 0.752210871587794}. Best is trial 0 with value: 0.1949088659988759.[0m
[32m[I 2025-02-02 06:48:50,412][0m Trial 2 finished with value: 0.13004446935842517 and parameters: {'observation_period_num': 67, 'train_rates': 0.8745370475950294, 'learning_rate': 0.00017616577004765448, 'batch_size': 167, 'step_size': 13, 'gamma': 0.7988805549635529}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:51:13,945][0m Trial 3 finished with value: 0.26792246958305094 and parameters: {'observation_period_num': 117, 'train_rates': 0.7445036813084985, 'learning_rate': 2.661093876104841e-06, 'batch_size': 33, 'step_size': 11, 'gamma': 0.9187534133400992}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:51:43,950][0m Trial 4 finished with value: 0.8886817693710327 and parameters: {'observation_period_num': 240, 'train_rates': 0.9767226649048393, 'learning_rate': 1.2204402353564039e-06, 'batch_size': 207, 'step_size': 15, 'gamma': 0.9107952740183517}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:52:22,561][0m Trial 5 finished with value: 0.22621126843165168 and parameters: {'observation_period_num': 80, 'train_rates': 0.8881433955259538, 'learning_rate': 1.9363665819612095e-05, 'batch_size': 158, 'step_size': 12, 'gamma': 0.9271877733856058}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:53:04,870][0m Trial 6 finished with value: 0.7229952812194824 and parameters: {'observation_period_num': 248, 'train_rates': 0.9662867521594202, 'learning_rate': 4.035201604548067e-06, 'batch_size': 139, 'step_size': 10, 'gamma': 0.9345683623389264}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:53:38,823][0m Trial 7 finished with value: 0.2104630417162829 and parameters: {'observation_period_num': 17, 'train_rates': 0.931433048318781, 'learning_rate': 5.8615638441886943e-05, 'batch_size': 190, 'step_size': 7, 'gamma': 0.8508889271101104}. Best is trial 2 with value: 0.13004446935842517.[0m
[32m[I 2025-02-02 06:54:57,224][0m Trial 8 finished with value: 0.10194679857832006 and parameters: {'observation_period_num': 41, 'train_rates': 0.8030690495381232, 'learning_rate': 7.160176514406557e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.7776241805561102}. Best is trial 8 with value: 0.10194679857832006.[0m
[32m[I 2025-02-02 06:55:21,562][0m Trial 9 finished with value: 0.10835767397701009 and parameters: {'observation_period_num': 69, 'train_rates': 0.6329134180412195, 'learning_rate': 0.00025944506440361495, 'batch_size': 209, 'step_size': 7, 'gamma': 0.9759477665659446}. Best is trial 8 with value: 0.10194679857832006.[0m
[32m[I 2025-02-02 06:56:16,101][0m Trial 10 finished with value: 0.06540378843797065 and parameters: {'observation_period_num': 7, 'train_rates': 0.7109065415626578, 'learning_rate': 0.0009937955278547935, 'batch_size': 95, 'step_size': 5, 'gamma': 0.8341873520064531}. Best is trial 10 with value: 0.06540378843797065.[0m
[32m[I 2025-02-02 06:57:10,610][0m Trial 11 finished with value: 0.06664914171622452 and parameters: {'observation_period_num': 6, 'train_rates': 0.6966178531217071, 'learning_rate': 0.0009426233259048231, 'batch_size': 90, 'step_size': 4, 'gamma': 0.8238449884093579}. Best is trial 10 with value: 0.06540378843797065.[0m
[32m[I 2025-02-02 06:58:01,606][0m Trial 12 finished with value: 0.06435020295980375 and parameters: {'observation_period_num': 5, 'train_rates': 0.6731735381020946, 'learning_rate': 0.000977800692743784, 'batch_size': 95, 'step_size': 4, 'gamma': 0.8433596458384233}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 06:58:44,390][0m Trial 13 finished with value: 0.09823028556037443 and parameters: {'observation_period_num': 34, 'train_rates': 0.6035323602506064, 'learning_rate': 0.0009698396746432898, 'batch_size': 107, 'step_size': 4, 'gamma': 0.8694749621495074}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 06:59:29,288][0m Trial 14 finished with value: 0.11450873570782798 and parameters: {'observation_period_num': 105, 'train_rates': 0.6807137911986869, 'learning_rate': 0.00039673151069588873, 'batch_size': 104, 'step_size': 5, 'gamma': 0.8377781257848228}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:02:09,112][0m Trial 15 finished with value: 0.09140934201207761 and parameters: {'observation_period_num': 45, 'train_rates': 0.7213268517204785, 'learning_rate': 0.0004682756456462084, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8044324510010055}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:02:46,746][0m Trial 16 finished with value: 0.3811163218971539 and parameters: {'observation_period_num': 157, 'train_rates': 0.6617290058319254, 'learning_rate': 1.0448672806228574e-05, 'batch_size': 131, 'step_size': 9, 'gamma': 0.8837604851663537}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:03:57,203][0m Trial 17 finished with value: 0.07444054931402207 and parameters: {'observation_period_num': 6, 'train_rates': 0.755901324683755, 'learning_rate': 0.0005524757160979324, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8806136834881138}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:04:34,522][0m Trial 18 finished with value: 0.13766660385952872 and parameters: {'observation_period_num': 88, 'train_rates': 0.6469485714420278, 'learning_rate': 0.00011590073149500523, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8145548851663427}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:08:29,265][0m Trial 19 finished with value: 0.17921294848688624 and parameters: {'observation_period_num': 203, 'train_rates': 0.6003631148467357, 'learning_rate': 0.00025003057872609915, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8541497169245246}. Best is trial 12 with value: 0.06435020295980375.[0m
Early stopping at epoch 56
[32m[I 2025-02-02 07:09:23,542][0m Trial 20 finished with value: 0.12389165226495914 and parameters: {'observation_period_num': 47, 'train_rates': 0.76584218201756, 'learning_rate': 0.0006609514395710492, 'batch_size': 55, 'step_size': 1, 'gamma': 0.7662101140020985}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:10:16,004][0m Trial 21 finished with value: 0.08046691192996717 and parameters: {'observation_period_num': 11, 'train_rates': 0.6995681763647725, 'learning_rate': 0.0009825276141974698, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8237577701565968}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:11:12,996][0m Trial 22 finished with value: 0.08258130550647175 and parameters: {'observation_period_num': 25, 'train_rates': 0.7096304947663045, 'learning_rate': 0.00033789455310591636, 'batch_size': 89, 'step_size': 4, 'gamma': 0.836983099862451}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:11:57,174][0m Trial 23 finished with value: 0.09431883617185072 and parameters: {'observation_period_num': 57, 'train_rates': 0.6818110098846564, 'learning_rate': 0.0009651249784690026, 'batch_size': 115, 'step_size': 6, 'gamma': 0.7889650043449549}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:13:00,372][0m Trial 24 finished with value: 0.08743673209199514 and parameters: {'observation_period_num': 27, 'train_rates': 0.7417321721458177, 'learning_rate': 0.00043011929929866134, 'batch_size': 83, 'step_size': 3, 'gamma': 0.8262002695730825}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:13:33,458][0m Trial 25 finished with value: 0.0674163498816102 and parameters: {'observation_period_num': 10, 'train_rates': 0.6350626803232218, 'learning_rate': 0.0005948095386650561, 'batch_size': 151, 'step_size': 5, 'gamma': 0.8674988766315874}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:14:18,393][0m Trial 26 finished with value: 0.08497076462476681 and parameters: {'observation_period_num': 30, 'train_rates': 0.7201495595384488, 'learning_rate': 0.00020159381800998014, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9004174503384333}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:15:59,517][0m Trial 27 finished with value: 0.1528003467972596 and parameters: {'observation_period_num': 97, 'train_rates': 0.6767185855372534, 'learning_rate': 0.00010800999796939813, 'batch_size': 45, 'step_size': 2, 'gamma': 0.8479417172185819}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:17:03,479][0m Trial 28 finished with value: 0.3031173797590392 and parameters: {'observation_period_num': 128, 'train_rates': 0.776996711119316, 'learning_rate': 9.887369752230595e-06, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8095305126099742}. Best is trial 12 with value: 0.06435020295980375.[0m
Early stopping at epoch 96
[32m[I 2025-02-02 07:18:27,487][0m Trial 29 finished with value: 0.2944881054018634 and parameters: {'observation_period_num': 71, 'train_rates': 0.8340535469768188, 'learning_rate': 2.636876455692634e-05, 'batch_size': 63, 'step_size': 2, 'gamma': 0.7898260658146329}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:18:56,745][0m Trial 30 finished with value: 0.4050298527004273 and parameters: {'observation_period_num': 183, 'train_rates': 0.7338998233921452, 'learning_rate': 5.0608850930081884e-05, 'batch_size': 176, 'step_size': 1, 'gamma': 0.892154638255316}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:19:29,186][0m Trial 31 finished with value: 0.06542755409921568 and parameters: {'observation_period_num': 8, 'train_rates': 0.6379410280405962, 'learning_rate': 0.000661000086821868, 'batch_size': 154, 'step_size': 5, 'gamma': 0.8664998130165555}. Best is trial 12 with value: 0.06435020295980375.[0m
[32m[I 2025-02-02 07:20:02,892][0m Trial 32 finished with value: 0.06290869419822466 and parameters: {'observation_period_num': 7, 'train_rates': 0.6220519565651503, 'learning_rate': 0.0007455616197794973, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9524784207507826}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:20:36,471][0m Trial 33 finished with value: 0.08603758635386743 and parameters: {'observation_period_num': 54, 'train_rates': 0.6186514489913509, 'learning_rate': 0.0006423082320343905, 'batch_size': 149, 'step_size': 6, 'gamma': 0.9395122884469942}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:21:17,187][0m Trial 34 finished with value: 0.0831557424622822 and parameters: {'observation_period_num': 29, 'train_rates': 0.6601088163217256, 'learning_rate': 0.00016953233092996005, 'batch_size': 123, 'step_size': 8, 'gamma': 0.955267515769493}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:21:45,464][0m Trial 35 finished with value: 0.08116621241362951 and parameters: {'observation_period_num': 19, 'train_rates': 0.6479570152964119, 'learning_rate': 0.00031298939758457877, 'batch_size': 181, 'step_size': 5, 'gamma': 0.984540148330418}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:22:07,333][0m Trial 36 finished with value: 0.1181980078007378 and parameters: {'observation_period_num': 58, 'train_rates': 0.6189856251742426, 'learning_rate': 0.000683718632982111, 'batch_size': 249, 'step_size': 6, 'gamma': 0.8620603821480803}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:22:41,981][0m Trial 37 finished with value: 0.09094265848398209 and parameters: {'observation_period_num': 41, 'train_rates': 0.6637008248849727, 'learning_rate': 0.00016904915309285044, 'batch_size': 140, 'step_size': 9, 'gamma': 0.8791265253246973}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:23:10,455][0m Trial 38 finished with value: 0.15882033453537867 and parameters: {'observation_period_num': 150, 'train_rates': 0.6246398564369593, 'learning_rate': 0.00037923266679794565, 'batch_size': 162, 'step_size': 3, 'gamma': 0.9119255660718206}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:24:06,282][0m Trial 39 finished with value: 0.08917906431511777 and parameters: {'observation_period_num': 5, 'train_rates': 0.8345376039086448, 'learning_rate': 0.0006541097882060973, 'batch_size': 100, 'step_size': 8, 'gamma': 0.957192115178092}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:24:33,601][0m Trial 40 finished with value: 0.596761597527398 and parameters: {'observation_period_num': 20, 'train_rates': 0.6902432165237017, 'learning_rate': 3.2635591235436605e-06, 'batch_size': 202, 'step_size': 15, 'gamma': 0.83936234107052}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:25:18,090][0m Trial 41 finished with value: 0.07293993862183303 and parameters: {'observation_period_num': 7, 'train_rates': 0.6965377322696171, 'learning_rate': 0.0008617501339842974, 'batch_size': 120, 'step_size': 4, 'gamma': 0.8232463381568669}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:25:57,946][0m Trial 42 finished with value: 0.09378681207911807 and parameters: {'observation_period_num': 36, 'train_rates': 0.8022833662310381, 'learning_rate': 0.0004984810675086125, 'batch_size': 140, 'step_size': 5, 'gamma': 0.8525346773747984}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:26:56,718][0m Trial 43 finished with value: 0.07027435297008665 and parameters: {'observation_period_num': 20, 'train_rates': 0.6451386517777743, 'learning_rate': 0.0007558732110538594, 'batch_size': 79, 'step_size': 4, 'gamma': 0.831076674793367}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:27:43,947][0m Trial 44 finished with value: 0.7548868930644798 and parameters: {'observation_period_num': 219, 'train_rates': 0.6713908762253318, 'learning_rate': 1.4920134253882216e-06, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8586776447332383}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:28:15,058][0m Trial 45 finished with value: 0.08669316721585311 and parameters: {'observation_period_num': 17, 'train_rates': 0.7116273243199361, 'learning_rate': 0.0002599252996146304, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7968756937984363}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:29:58,943][0m Trial 46 finished with value: 0.11631783153424063 and parameters: {'observation_period_num': 37, 'train_rates': 0.8687195243182313, 'learning_rate': 0.0009980103108015587, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8451646050576396}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:30:30,932][0m Trial 47 finished with value: 0.11475985160059127 and parameters: {'observation_period_num': 78, 'train_rates': 0.6117925075149456, 'learning_rate': 0.0004736905240183391, 'batch_size': 151, 'step_size': 3, 'gamma': 0.8154928609803516}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:31:06,839][0m Trial 48 finished with value: 0.09259058006348149 and parameters: {'observation_period_num': 46, 'train_rates': 0.637624163395868, 'learning_rate': 0.00027122473356419076, 'batch_size': 136, 'step_size': 4, 'gamma': 0.8966810432028737}. Best is trial 32 with value: 0.06290869419822466.[0m
[32m[I 2025-02-02 07:32:14,392][0m Trial 49 finished with value: 0.11427449704453407 and parameters: {'observation_period_num': 14, 'train_rates': 0.6559554520310854, 'learning_rate': 7.996421306742362e-05, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8750672289045743}. Best is trial 32 with value: 0.06290869419822466.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_SBUX_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5191 | 0.2502
Epoch 2/300, Loss: 0.2567 | 0.1916
Epoch 3/300, Loss: 0.1894 | 0.1393
Epoch 4/300, Loss: 0.1595 | 0.1579
Epoch 5/300, Loss: 0.1463 | 0.1297
Epoch 6/300, Loss: 0.1367 | 0.1369
Epoch 7/300, Loss: 0.1262 | 0.1286
Epoch 8/300, Loss: 0.1295 | 0.1143
Epoch 9/300, Loss: 0.1199 | 0.1063
Epoch 10/300, Loss: 0.1158 | 0.0949
Epoch 11/300, Loss: 0.1160 | 0.0942
Epoch 12/300, Loss: 0.1167 | 0.0998
Epoch 13/300, Loss: 0.1271 | 0.1177
Epoch 14/300, Loss: 0.1183 | 0.1268
Epoch 15/300, Loss: 0.1194 | 0.0940
Epoch 16/300, Loss: 0.1195 | 0.0879
Epoch 17/300, Loss: 0.1317 | 0.1716
Epoch 18/300, Loss: 0.1244 | 0.1020
Epoch 19/300, Loss: 0.1149 | 0.0959
Epoch 20/300, Loss: 0.1121 | 0.0967
Epoch 21/300, Loss: 0.1106 | 0.1088
Epoch 22/300, Loss: 0.1048 | 0.0868
Epoch 23/300, Loss: 0.1015 | 0.0951
Epoch 24/300, Loss: 0.0992 | 0.0847
Epoch 25/300, Loss: 0.0974 | 0.0812
Epoch 26/300, Loss: 0.0960 | 0.0759
Epoch 27/300, Loss: 0.0954 | 0.0755
Epoch 28/300, Loss: 0.0962 | 0.0735
Epoch 29/300, Loss: 0.0956 | 0.0745
Epoch 30/300, Loss: 0.0988 | 0.0736
Epoch 31/300, Loss: 0.0998 | 0.0823
Epoch 32/300, Loss: 0.1077 | 0.0980
Epoch 33/300, Loss: 0.1043 | 0.1129
Epoch 34/300, Loss: 0.0980 | 0.0746
Epoch 35/300, Loss: 0.0966 | 0.0902
Epoch 36/300, Loss: 0.0969 | 0.0804
Epoch 37/300, Loss: 0.0947 | 0.0907
Epoch 38/300, Loss: 0.0947 | 0.0793
Epoch 39/300, Loss: 0.0933 | 0.0876
Epoch 40/300, Loss: 0.0926 | 0.0759
Epoch 41/300, Loss: 0.0917 | 0.0807
Epoch 42/300, Loss: 0.0924 | 0.0751
Epoch 43/300, Loss: 0.0937 | 0.0795
Epoch 44/300, Loss: 0.0957 | 0.0781
Epoch 45/300, Loss: 0.0995 | 0.0834
Epoch 46/300, Loss: 0.1119 | 0.0968
Epoch 47/300, Loss: 0.1140 | 0.0841
Epoch 48/300, Loss: 0.1205 | 0.0924
Epoch 49/300, Loss: 0.1077 | 0.0781
Epoch 50/300, Loss: 0.1083 | 0.0972
Epoch 51/300, Loss: 0.1146 | 0.0793
Epoch 52/300, Loss: 0.1000 | 0.0800
Epoch 53/300, Loss: 0.0955 | 0.0751
Epoch 54/300, Loss: 0.0905 | 0.0690
Epoch 55/300, Loss: 0.0874 | 0.0703
Epoch 56/300, Loss: 0.0877 | 0.0731
Epoch 57/300, Loss: 0.0875 | 0.0696
Epoch 58/300, Loss: 0.0867 | 0.0732
Epoch 59/300, Loss: 0.0886 | 0.0708
Epoch 60/300, Loss: 0.0872 | 0.0747
Epoch 61/300, Loss: 0.0871 | 0.0717
Epoch 62/300, Loss: 0.0868 | 0.0747
Epoch 63/300, Loss: 0.0866 | 0.0711
Epoch 64/300, Loss: 0.0861 | 0.0732
Epoch 65/300, Loss: 0.0856 | 0.0688
Epoch 66/300, Loss: 0.0848 | 0.0710
Epoch 67/300, Loss: 0.0841 | 0.0675
Epoch 68/300, Loss: 0.0835 | 0.0692
Epoch 69/300, Loss: 0.0827 | 0.0668
Epoch 70/300, Loss: 0.0824 | 0.0681
Epoch 71/300, Loss: 0.0819 | 0.0663
Epoch 72/300, Loss: 0.0817 | 0.0671
Epoch 73/300, Loss: 0.0813 | 0.0658
Epoch 74/300, Loss: 0.0812 | 0.0664
Epoch 75/300, Loss: 0.0811 | 0.0655
Epoch 76/300, Loss: 0.0811 | 0.0659
Epoch 77/300, Loss: 0.0811 | 0.0650
Epoch 78/300, Loss: 0.0812 | 0.0653
Epoch 79/300, Loss: 0.0814 | 0.0644
Epoch 80/300, Loss: 0.0813 | 0.0648
Epoch 81/300, Loss: 0.0813 | 0.0644
Epoch 82/300, Loss: 0.0808 | 0.0651
Epoch 83/300, Loss: 0.0811 | 0.0653
Epoch 84/300, Loss: 0.0815 | 0.0657
Epoch 85/300, Loss: 0.0815 | 0.0651
Epoch 86/300, Loss: 0.0810 | 0.0654
Epoch 87/300, Loss: 0.0794 | 0.0640
Epoch 88/300, Loss: 0.0793 | 0.0656
Epoch 89/300, Loss: 0.0794 | 0.0647
Epoch 90/300, Loss: 0.0790 | 0.0652
Epoch 91/300, Loss: 0.0785 | 0.0640
Epoch 92/300, Loss: 0.0780 | 0.0642
Epoch 93/300, Loss: 0.0780 | 0.0635
Epoch 94/300, Loss: 0.0779 | 0.0638
Epoch 95/300, Loss: 0.0778 | 0.0634
Epoch 96/300, Loss: 0.0777 | 0.0641
Epoch 97/300, Loss: 0.0773 | 0.0632
Epoch 98/300, Loss: 0.0774 | 0.0642
Epoch 99/300, Loss: 0.0772 | 0.0634
Epoch 100/300, Loss: 0.0775 | 0.0649
Epoch 101/300, Loss: 0.0775 | 0.0631
Epoch 102/300, Loss: 0.0776 | 0.0651
Epoch 103/300, Loss: 0.0775 | 0.0628
Epoch 104/300, Loss: 0.0768 | 0.0637
Epoch 105/300, Loss: 0.0762 | 0.0627
Epoch 106/300, Loss: 0.0761 | 0.0630
Epoch 107/300, Loss: 0.0760 | 0.0621
Epoch 108/300, Loss: 0.0760 | 0.0624
Epoch 109/300, Loss: 0.0759 | 0.0627
Epoch 110/300, Loss: 0.0758 | 0.0633
Epoch 111/300, Loss: 0.0756 | 0.0629
Epoch 112/300, Loss: 0.0753 | 0.0631
Epoch 113/300, Loss: 0.0754 | 0.0624
Epoch 114/300, Loss: 0.0755 | 0.0634
Epoch 115/300, Loss: 0.0759 | 0.0629
Epoch 116/300, Loss: 0.0756 | 0.0631
Epoch 117/300, Loss: 0.0746 | 0.0618
Epoch 118/300, Loss: 0.0743 | 0.0625
Epoch 119/300, Loss: 0.0744 | 0.0620
Epoch 120/300, Loss: 0.0746 | 0.0619
Epoch 121/300, Loss: 0.0742 | 0.0617
Epoch 122/300, Loss: 0.0737 | 0.0621
Epoch 123/300, Loss: 0.0738 | 0.0622
Epoch 124/300, Loss: 0.0744 | 0.0629
Epoch 125/300, Loss: 0.0741 | 0.0623
Epoch 126/300, Loss: 0.0732 | 0.0617
Epoch 127/300, Loss: 0.0728 | 0.0617
Epoch 128/300, Loss: 0.0728 | 0.0616
Epoch 129/300, Loss: 0.0730 | 0.0616
Epoch 130/300, Loss: 0.0729 | 0.0616
Epoch 131/300, Loss: 0.0727 | 0.0618
Epoch 132/300, Loss: 0.0726 | 0.0620
Epoch 133/300, Loss: 0.0730 | 0.0623
Epoch 134/300, Loss: 0.0731 | 0.0625
Epoch 135/300, Loss: 0.0725 | 0.0618
Epoch 136/300, Loss: 0.0720 | 0.0616
Epoch 137/300, Loss: 0.0719 | 0.0617
Epoch 138/300, Loss: 0.0720 | 0.0617
Epoch 139/300, Loss: 0.0721 | 0.0617
Epoch 140/300, Loss: 0.0720 | 0.0618
Epoch 141/300, Loss: 0.0717 | 0.0619
Epoch 142/300, Loss: 0.0719 | 0.0622
Epoch 143/300, Loss: 0.0720 | 0.0625
Epoch 144/300, Loss: 0.0718 | 0.0622
Epoch 145/300, Loss: 0.0714 | 0.0619
Epoch 146/300, Loss: 0.0712 | 0.0618
Epoch 147/300, Loss: 0.0712 | 0.0618
Epoch 148/300, Loss: 0.0713 | 0.0619
Epoch 149/300, Loss: 0.0715 | 0.0619
Epoch 150/300, Loss: 0.0711 | 0.0620
Epoch 151/300, Loss: 0.0710 | 0.0621
Epoch 152/300, Loss: 0.0711 | 0.0623
Epoch 153/300, Loss: 0.0710 | 0.0624
Epoch 154/300, Loss: 0.0708 | 0.0623
Epoch 155/300, Loss: 0.0706 | 0.0620
Epoch 156/300, Loss: 0.0705 | 0.0620
Epoch 157/300, Loss: 0.0706 | 0.0620
Epoch 158/300, Loss: 0.0707 | 0.0620
Epoch 159/300, Loss: 0.0706 | 0.0620
Epoch 160/300, Loss: 0.0704 | 0.0621
Epoch 161/300, Loss: 0.0704 | 0.0623
Epoch 162/300, Loss: 0.0704 | 0.0624
Epoch 163/300, Loss: 0.0703 | 0.0624
Epoch 164/300, Loss: 0.0701 | 0.0622
Epoch 165/300, Loss: 0.0700 | 0.0621
Epoch 166/300, Loss: 0.0700 | 0.0621
Epoch 167/300, Loss: 0.0701 | 0.0621
Epoch 168/300, Loss: 0.0700 | 0.0621
Epoch 169/300, Loss: 0.0699 | 0.0621
Epoch 170/300, Loss: 0.0699 | 0.0622
Epoch 171/300, Loss: 0.0699 | 0.0624
Epoch 172/300, Loss: 0.0698 | 0.0624
Epoch 173/300, Loss: 0.0697 | 0.0622
Epoch 174/300, Loss: 0.0696 | 0.0622
Epoch 175/300, Loss: 0.0696 | 0.0621
Epoch 176/300, Loss: 0.0696 | 0.0621
Epoch 177/300, Loss: 0.0695 | 0.0621
Epoch 178/300, Loss: 0.0694 | 0.0621
Epoch 179/300, Loss: 0.0694 | 0.0622
Epoch 180/300, Loss: 0.0694 | 0.0623
Epoch 181/300, Loss: 0.0694 | 0.0623
Epoch 182/300, Loss: 0.0693 | 0.0622
Epoch 183/300, Loss: 0.0692 | 0.0622
Epoch 184/300, Loss: 0.0692 | 0.0621
Epoch 185/300, Loss: 0.0692 | 0.0621
Epoch 186/300, Loss: 0.0691 | 0.0621
Epoch 187/300, Loss: 0.0691 | 0.0622
Epoch 188/300, Loss: 0.0691 | 0.0622
Epoch 189/300, Loss: 0.0690 | 0.0623
Epoch 190/300, Loss: 0.0690 | 0.0623
Epoch 191/300, Loss: 0.0689 | 0.0622
Epoch 192/300, Loss: 0.0689 | 0.0622
Epoch 193/300, Loss: 0.0689 | 0.0621
Epoch 194/300, Loss: 0.0688 | 0.0621
Epoch 195/300, Loss: 0.0688 | 0.0621
Epoch 196/300, Loss: 0.0688 | 0.0622
Epoch 197/300, Loss: 0.0688 | 0.0622
Epoch 198/300, Loss: 0.0687 | 0.0622
Epoch 199/300, Loss: 0.0687 | 0.0622
Epoch 200/300, Loss: 0.0686 | 0.0622
Epoch 201/300, Loss: 0.0686 | 0.0621
Epoch 202/300, Loss: 0.0686 | 0.0621
Epoch 203/300, Loss: 0.0686 | 0.0622
Epoch 204/300, Loss: 0.0685 | 0.0622
Epoch 205/300, Loss: 0.0685 | 0.0622
Epoch 206/300, Loss: 0.0685 | 0.0622
Epoch 207/300, Loss: 0.0685 | 0.0622
Epoch 208/300, Loss: 0.0684 | 0.0622
Epoch 209/300, Loss: 0.0684 | 0.0621
Epoch 210/300, Loss: 0.0684 | 0.0622
Epoch 211/300, Loss: 0.0684 | 0.0622
Epoch 212/300, Loss: 0.0683 | 0.0622
Epoch 213/300, Loss: 0.0683 | 0.0622
Epoch 214/300, Loss: 0.0683 | 0.0622
Epoch 215/300, Loss: 0.0683 | 0.0622
Epoch 216/300, Loss: 0.0682 | 0.0622
Epoch 217/300, Loss: 0.0682 | 0.0622
Epoch 218/300, Loss: 0.0682 | 0.0622
Epoch 219/300, Loss: 0.0682 | 0.0622
Epoch 220/300, Loss: 0.0682 | 0.0622
Epoch 221/300, Loss: 0.0681 | 0.0622
Epoch 222/300, Loss: 0.0681 | 0.0622
Epoch 223/300, Loss: 0.0681 | 0.0621
Epoch 224/300, Loss: 0.0681 | 0.0622
Epoch 225/300, Loss: 0.0681 | 0.0622
Epoch 226/300, Loss: 0.0680 | 0.0621
Epoch 227/300, Loss: 0.0680 | 0.0621
Epoch 228/300, Loss: 0.0680 | 0.0621
Epoch 229/300, Loss: 0.0680 | 0.0621
Epoch 230/300, Loss: 0.0680 | 0.0621
Epoch 231/300, Loss: 0.0679 | 0.0621
Epoch 232/300, Loss: 0.0679 | 0.0621
Epoch 233/300, Loss: 0.0679 | 0.0621
Epoch 234/300, Loss: 0.0679 | 0.0621
Epoch 235/300, Loss: 0.0679 | 0.0621
Epoch 236/300, Loss: 0.0679 | 0.0621
Epoch 237/300, Loss: 0.0678 | 0.0621
Epoch 238/300, Loss: 0.0678 | 0.0621
Epoch 239/300, Loss: 0.0678 | 0.0621
Epoch 240/300, Loss: 0.0678 | 0.0621
Epoch 241/300, Loss: 0.0678 | 0.0621
Epoch 242/300, Loss: 0.0678 | 0.0621
Epoch 243/300, Loss: 0.0677 | 0.0621
Epoch 244/300, Loss: 0.0677 | 0.0621
Epoch 245/300, Loss: 0.0677 | 0.0621
Epoch 246/300, Loss: 0.0677 | 0.0621
Epoch 247/300, Loss: 0.0677 | 0.0621
Epoch 248/300, Loss: 0.0677 | 0.0621
Epoch 249/300, Loss: 0.0677 | 0.0621
Epoch 250/300, Loss: 0.0676 | 0.0621
Epoch 251/300, Loss: 0.0676 | 0.0621
Epoch 252/300, Loss: 0.0676 | 0.0621
Epoch 253/300, Loss: 0.0676 | 0.0621
Epoch 254/300, Loss: 0.0676 | 0.0621
Epoch 255/300, Loss: 0.0676 | 0.0621
Epoch 256/300, Loss: 0.0676 | 0.0621
Epoch 257/300, Loss: 0.0676 | 0.0621
Epoch 258/300, Loss: 0.0675 | 0.0621
Epoch 259/300, Loss: 0.0675 | 0.0621
Epoch 260/300, Loss: 0.0675 | 0.0621
Epoch 261/300, Loss: 0.0675 | 0.0621
Epoch 262/300, Loss: 0.0675 | 0.0621
Epoch 263/300, Loss: 0.0675 | 0.0621
Epoch 264/300, Loss: 0.0675 | 0.0621
Epoch 265/300, Loss: 0.0675 | 0.0621
Epoch 266/300, Loss: 0.0675 | 0.0621
Epoch 267/300, Loss: 0.0674 | 0.0621
Epoch 268/300, Loss: 0.0674 | 0.0621
Epoch 269/300, Loss: 0.0674 | 0.0621
Epoch 270/300, Loss: 0.0674 | 0.0621
Epoch 271/300, Loss: 0.0674 | 0.0621
Epoch 272/300, Loss: 0.0674 | 0.0621
Epoch 273/300, Loss: 0.0674 | 0.0621
Epoch 274/300, Loss: 0.0674 | 0.0621
Epoch 275/300, Loss: 0.0674 | 0.0621
Epoch 276/300, Loss: 0.0674 | 0.0621
Epoch 277/300, Loss: 0.0673 | 0.0621
Epoch 278/300, Loss: 0.0673 | 0.0621
Epoch 279/300, Loss: 0.0673 | 0.0621
Epoch 280/300, Loss: 0.0673 | 0.0621
Epoch 281/300, Loss: 0.0673 | 0.0621
Epoch 282/300, Loss: 0.0673 | 0.0621
Epoch 283/300, Loss: 0.0673 | 0.0621
Epoch 284/300, Loss: 0.0673 | 0.0621
Epoch 285/300, Loss: 0.0673 | 0.0621
Epoch 286/300, Loss: 0.0673 | 0.0621
Epoch 287/300, Loss: 0.0673 | 0.0621
Epoch 288/300, Loss: 0.0672 | 0.0621
Epoch 289/300, Loss: 0.0672 | 0.0621
Epoch 290/300, Loss: 0.0672 | 0.0621
Epoch 291/300, Loss: 0.0672 | 0.0621
Epoch 292/300, Loss: 0.0672 | 0.0621
Epoch 293/300, Loss: 0.0672 | 0.0621
Epoch 294/300, Loss: 0.0672 | 0.0621
Epoch 295/300, Loss: 0.0672 | 0.0621
Epoch 296/300, Loss: 0.0672 | 0.0621
Epoch 297/300, Loss: 0.0672 | 0.0621
Epoch 298/300, Loss: 0.0672 | 0.0621
Epoch 299/300, Loss: 0.0672 | 0.0621
Epoch 300/300, Loss: 0.0672 | 0.0621
Runtime (seconds): 98.54504799842834
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.13523597741732374
RMSE: 0.36774444580078125
MAE: 0.36774444580078125
R-squared: nan
[101.14226]
