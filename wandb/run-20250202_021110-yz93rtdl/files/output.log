ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-02 02:11:14,258][0m A new study created in memory with name: no-name-99e1a17c-ebc2-48c4-8909-2820e7231fd6[0m
[32m[I 2025-02-02 02:13:57,805][0m Trial 0 finished with value: 0.28049143579685026 and parameters: {'observation_period_num': 94, 'train_rates': 0.9244575654541607, 'learning_rate': 0.00017125473700768784, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9252382812592379}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:15:34,811][0m Trial 1 finished with value: 1.0816411257230047 and parameters: {'observation_period_num': 186, 'train_rates': 0.6081571964702347, 'learning_rate': 2.4224728163626493e-05, 'batch_size': 244, 'step_size': 13, 'gamma': 0.8877092710563593}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:17:13,379][0m Trial 2 finished with value: 0.5264909212758555 and parameters: {'observation_period_num': 158, 'train_rates': 0.8187125101271928, 'learning_rate': 3.3361661763431937e-05, 'batch_size': 219, 'step_size': 15, 'gamma': 0.7823095605787382}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:19:24,977][0m Trial 3 finished with value: 1.067857133148627 and parameters: {'observation_period_num': 215, 'train_rates': 0.7044658012668512, 'learning_rate': 0.0009538601241210497, 'batch_size': 172, 'step_size': 2, 'gamma': 0.7987415116475354}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:21:47,001][0m Trial 4 finished with value: 0.841041648030281 and parameters: {'observation_period_num': 179, 'train_rates': 0.8197876111978304, 'learning_rate': 3.946554878610162e-06, 'batch_size': 39, 'step_size': 10, 'gamma': 0.7522444886946168}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:23:04,067][0m Trial 5 finished with value: 0.6767215522435995 and parameters: {'observation_period_num': 125, 'train_rates': 0.7701462522287559, 'learning_rate': 8.650967554564462e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9464729395290734}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:24:48,976][0m Trial 6 finished with value: 1.088758341048012 and parameters: {'observation_period_num': 159, 'train_rates': 0.8522773675300235, 'learning_rate': 3.6673524028161355e-06, 'batch_size': 107, 'step_size': 6, 'gamma': 0.7943150426043555}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:26:16,107][0m Trial 7 finished with value: 0.3503585250129527 and parameters: {'observation_period_num': 130, 'train_rates': 0.92164537559398, 'learning_rate': 0.0006286671778769192, 'batch_size': 178, 'step_size': 15, 'gamma': 0.8357875795160365}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:26:40,687][0m Trial 8 finished with value: 0.48231431162496174 and parameters: {'observation_period_num': 23, 'train_rates': 0.7949501030069462, 'learning_rate': 0.0003202324437995319, 'batch_size': 240, 'step_size': 10, 'gamma': 0.9772239320349729}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:27:52,413][0m Trial 9 finished with value: 1.3173499239815607 and parameters: {'observation_period_num': 115, 'train_rates': 0.8984788053159282, 'learning_rate': 4.105912445267327e-06, 'batch_size': 176, 'step_size': 13, 'gamma': 0.7582340031573993}. Best is trial 0 with value: 0.28049143579685026.[0m
[32m[I 2025-02-02 02:33:01,380][0m Trial 10 finished with value: 0.1464497933262273 and parameters: {'observation_period_num': 54, 'train_rates': 0.9805868554781789, 'learning_rate': 0.00013723893946968678, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8984746733621746}. Best is trial 10 with value: 0.1464497933262273.[0m
[32m[I 2025-02-02 02:36:53,883][0m Trial 11 finished with value: 0.4556523483246565 and parameters: {'observation_period_num': 53, 'train_rates': 0.9727250986988366, 'learning_rate': 0.000127763446547395, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9017247238215197}. Best is trial 10 with value: 0.1464497933262273.[0m
[32m[I 2025-02-02 02:38:31,447][0m Trial 12 finished with value: 0.4931352060465586 and parameters: {'observation_period_num': 74, 'train_rates': 0.9709679361329016, 'learning_rate': 0.00013149439392401595, 'batch_size': 64, 'step_size': 3, 'gamma': 0.9250227347418873}. Best is trial 10 with value: 0.1464497933262273.[0m
[32m[I 2025-02-02 02:43:04,108][0m Trial 13 finished with value: 0.12227907218039036 and parameters: {'observation_period_num': 83, 'train_rates': 0.9889193106795873, 'learning_rate': 2.581064910647769e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8570831490927148}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:48:54,540][0m Trial 14 finished with value: 0.16743363499168365 and parameters: {'observation_period_num': 10, 'train_rates': 0.9787834002734451, 'learning_rate': 1.7856519675648295e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8538145260840735}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:50:20,247][0m Trial 15 finished with value: 1.7324784845615477 and parameters: {'observation_period_num': 43, 'train_rates': 0.8979736872857885, 'learning_rate': 1.1637551905378158e-06, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8477135493115087}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:51:09,060][0m Trial 16 finished with value: 0.6387502890378426 and parameters: {'observation_period_num': 79, 'train_rates': 0.7260770618887135, 'learning_rate': 5.018171479490157e-05, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8216329527097359}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:52:46,895][0m Trial 17 finished with value: 0.5045124724291373 and parameters: {'observation_period_num': 49, 'train_rates': 0.8575036863070923, 'learning_rate': 1.2955174069533792e-05, 'batch_size': 60, 'step_size': 5, 'gamma': 0.8777808788107807}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:56:03,193][0m Trial 18 finished with value: 1.2959173917770386 and parameters: {'observation_period_num': 252, 'train_rates': 0.9436706576913558, 'learning_rate': 1.0570688853473876e-05, 'batch_size': 155, 'step_size': 1, 'gamma': 0.9133592981023033}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:58:17,361][0m Trial 19 finished with value: 0.13034479320049286 and parameters: {'observation_period_num': 97, 'train_rates': 0.9898954571058461, 'learning_rate': 0.0003080223132654546, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9603018565153691}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 02:59:10,514][0m Trial 20 finished with value: 0.9592556067995066 and parameters: {'observation_period_num': 95, 'train_rates': 0.6012797295908122, 'learning_rate': 0.000363937932118558, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9863485697221395}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 03:01:16,126][0m Trial 21 finished with value: 0.37119169533252716 and parameters: {'observation_period_num': 75, 'train_rates': 0.9490307464038084, 'learning_rate': 6.034710667450627e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9530028247891335}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 03:07:00,665][0m Trial 22 finished with value: 0.3073308238616356 and parameters: {'observation_period_num': 109, 'train_rates': 0.9863829389335225, 'learning_rate': 0.00026546159920149803, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9565704379851429}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 03:08:19,760][0m Trial 23 finished with value: 0.32946869565380943 and parameters: {'observation_period_num': 34, 'train_rates': 0.8798437887003514, 'learning_rate': 0.0005255376828092012, 'batch_size': 78, 'step_size': 7, 'gamma': 0.863511510946318}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 03:10:28,054][0m Trial 24 finished with value: 0.36539167388417254 and parameters: {'observation_period_num': 70, 'train_rates': 0.9395995733309628, 'learning_rate': 4.6417994247241476e-05, 'batch_size': 49, 'step_size': 5, 'gamma': 0.8797878314335255}. Best is trial 13 with value: 0.12227907218039036.[0m
[32m[I 2025-02-02 03:16:54,528][0m Trial 25 finished with value: 0.10421674734070188 and parameters: {'observation_period_num': 98, 'train_rates': 0.9854045640514372, 'learning_rate': 9.472860256449676e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.827835140034455}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:18:59,595][0m Trial 26 finished with value: 0.5348446990555598 and parameters: {'observation_period_num': 143, 'train_rates': 0.91221164142098, 'learning_rate': 8.066148615736342e-06, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8242038611619117}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:20:06,386][0m Trial 27 finished with value: 0.460055410861969 and parameters: {'observation_period_num': 101, 'train_rates': 0.9524543503915768, 'learning_rate': 7.745795951224875e-05, 'batch_size': 138, 'step_size': 9, 'gamma': 0.8101991538721304}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:22:07,867][0m Trial 28 finished with value: 0.7342512208695818 and parameters: {'observation_period_num': 136, 'train_rates': 0.6506324057172296, 'learning_rate': 0.00020328376763888116, 'batch_size': 37, 'step_size': 8, 'gamma': 0.837527767216163}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:24:42,302][0m Trial 29 finished with value: 0.2829448997974396 and parameters: {'observation_period_num': 90, 'train_rates': 0.8706540691471399, 'learning_rate': 3.203894885002962e-05, 'batch_size': 37, 'step_size': 11, 'gamma': 0.8654490729880662}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:26:01,481][0m Trial 30 finished with value: 0.31215079371153454 and parameters: {'observation_period_num': 113, 'train_rates': 0.9264558670444729, 'learning_rate': 9.146339165247034e-05, 'batch_size': 101, 'step_size': 9, 'gamma': 0.9357076300854161}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:32:14,640][0m Trial 31 finished with value: 0.1469550565816462 and parameters: {'observation_period_num': 64, 'train_rates': 0.989141173575133, 'learning_rate': 0.00016711791651090305, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8948561040493933}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:35:26,745][0m Trial 32 finished with value: 0.40103294054667155 and parameters: {'observation_period_num': 89, 'train_rates': 0.9581332287855878, 'learning_rate': 0.0002477338990447418, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9101841115381322}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:37:30,407][0m Trial 33 finished with value: 0.4130970388650894 and parameters: {'observation_period_num': 54, 'train_rates': 0.9592304779461225, 'learning_rate': 2.2756933913494756e-05, 'batch_size': 51, 'step_size': 5, 'gamma': 0.9628416012522435}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:38:55,445][0m Trial 34 finished with value: 0.14957454800605774 and parameters: {'observation_period_num': 31, 'train_rates': 0.9890795961262189, 'learning_rate': 0.00011281824443867091, 'batch_size': 78, 'step_size': 7, 'gamma': 0.7723878010953505}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:42:11,338][0m Trial 35 finished with value: 0.4231689943740116 and parameters: {'observation_period_num': 87, 'train_rates': 0.9305714329659279, 'learning_rate': 0.00045461374421500655, 'batch_size': 31, 'step_size': 11, 'gamma': 0.8871562619874505}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:43:53,599][0m Trial 36 finished with value: 0.3350885624557623 and parameters: {'observation_period_num': 153, 'train_rates': 0.9038459778040684, 'learning_rate': 0.0007326886821942176, 'batch_size': 204, 'step_size': 9, 'gamma': 0.9212962244563028}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:49:29,848][0m Trial 37 finished with value: 0.2959170330892547 and parameters: {'observation_period_num': 60, 'train_rates': 0.8346572093988405, 'learning_rate': 4.182155327420108e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8509012910768978}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:51:14,772][0m Trial 38 finished with value: 0.4347450403456992 and parameters: {'observation_period_num': 124, 'train_rates': 0.9669716885970525, 'learning_rate': 0.00018514770854547816, 'batch_size': 60, 'step_size': 8, 'gamma': 0.9360817460626389}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:53:39,469][0m Trial 39 finished with value: 0.5029642941863515 and parameters: {'observation_period_num': 202, 'train_rates': 0.7591454083360157, 'learning_rate': 7.14995266779349e-05, 'batch_size': 41, 'step_size': 12, 'gamma': 0.7995873323401467}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 03:56:28,500][0m Trial 40 finished with value: 0.6570550028396689 and parameters: {'observation_period_num': 5, 'train_rates': 0.6563945809833621, 'learning_rate': 2.3640192594783702e-05, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8351648614436614}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:02:53,161][0m Trial 41 finished with value: 0.1359428083806327 and parameters: {'observation_period_num': 60, 'train_rates': 0.9888943838541382, 'learning_rate': 0.0001520700157258967, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8988723946940945}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:06:27,457][0m Trial 42 finished with value: 0.4212179596846302 and parameters: {'observation_period_num': 97, 'train_rates': 0.9666431722049343, 'learning_rate': 0.0001575003443829999, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9004367771020916}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:08:57,360][0m Trial 43 finished with value: 2.0877365411856235 and parameters: {'observation_period_num': 39, 'train_rates': 0.9331213279793273, 'learning_rate': 0.000899176265126515, 'batch_size': 41, 'step_size': 6, 'gamma': 0.8812773271921076}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:13:04,303][0m Trial 44 finished with value: 0.43708321877888273 and parameters: {'observation_period_num': 23, 'train_rates': 0.9713250021534315, 'learning_rate': 0.00010209146090666007, 'batch_size': 25, 'step_size': 8, 'gamma': 0.8667867639222515}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:14:56,031][0m Trial 45 finished with value: 0.1970089077949524 and parameters: {'observation_period_num': 63, 'train_rates': 0.9886732192772085, 'learning_rate': 0.0003830784217378847, 'batch_size': 59, 'step_size': 6, 'gamma': 0.9716200774268233}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:16:18,022][0m Trial 46 finished with value: 0.2756407207876537 and parameters: {'observation_period_num': 81, 'train_rates': 0.8890153950519958, 'learning_rate': 0.000266731363398421, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9327330909815432}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:22:22,070][0m Trial 47 finished with value: 0.2595460623260436 and parameters: {'observation_period_num': 104, 'train_rates': 0.913641982737196, 'learning_rate': 0.00012262127033923855, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8174704846134753}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:23:41,515][0m Trial 48 finished with value: 0.5886402726173401 and parameters: {'observation_period_num': 122, 'train_rates': 0.9413183673578487, 'learning_rate': 3.372148795294681e-05, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8386610090586073}. Best is trial 25 with value: 0.10421674734070188.[0m
[32m[I 2025-02-02 04:25:59,539][0m Trial 49 finished with value: 0.44399491662070867 and parameters: {'observation_period_num': 20, 'train_rates': 0.957182923893114, 'learning_rate': 1.7656488685347327e-05, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9077462715009846}. Best is trial 25 with value: 0.10421674734070188.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-02 04:25:59,551][0m A new study created in memory with name: no-name-f18db62e-a96e-4b2f-8377-53d7ca80a4b3[0m
[32m[I 2025-02-02 04:26:48,201][0m Trial 0 finished with value: 0.8878186764103351 and parameters: {'observation_period_num': 87, 'train_rates': 0.7883578541899611, 'learning_rate': 1.4895045580673344e-05, 'batch_size': 246, 'step_size': 8, 'gamma': 0.9231743969401389}. Best is trial 0 with value: 0.8878186764103351.[0m
[32m[I 2025-02-02 04:27:55,837][0m Trial 1 finished with value: 0.5143639216366557 and parameters: {'observation_period_num': 116, 'train_rates': 0.7916793733749155, 'learning_rate': 7.784938799511093e-05, 'batch_size': 190, 'step_size': 12, 'gamma': 0.960798659527695}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:29:51,738][0m Trial 2 finished with value: 1.3031237774778572 and parameters: {'observation_period_num': 185, 'train_rates': 0.7698908430923567, 'learning_rate': 4.4237389949609406e-06, 'batch_size': 183, 'step_size': 2, 'gamma': 0.9458482465852552}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:30:21,541][0m Trial 3 finished with value: 1.6765336965469329 and parameters: {'observation_period_num': 23, 'train_rates': 0.8364863738822725, 'learning_rate': 1.667597750633942e-06, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8688040495366804}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:31:11,557][0m Trial 4 finished with value: 1.7489809067956954 and parameters: {'observation_period_num': 83, 'train_rates': 0.6498789613498824, 'learning_rate': 3.118289425609e-06, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8776045857879953}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:32:48,535][0m Trial 5 finished with value: 0.8309664375615756 and parameters: {'observation_period_num': 142, 'train_rates': 0.7600959554197884, 'learning_rate': 9.463009298438886e-06, 'batch_size': 59, 'step_size': 14, 'gamma': 0.8293477186541724}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:33:50,391][0m Trial 6 finished with value: 0.7123087912687138 and parameters: {'observation_period_num': 107, 'train_rates': 0.8109516050778967, 'learning_rate': 1.6793476087521274e-05, 'batch_size': 250, 'step_size': 14, 'gamma': 0.8636037302930287}. Best is trial 1 with value: 0.5143639216366557.[0m
[32m[I 2025-02-02 04:35:21,036][0m Trial 7 finished with value: 0.3267831266449209 and parameters: {'observation_period_num': 139, 'train_rates': 0.8786464705474788, 'learning_rate': 0.00030848261650767716, 'batch_size': 174, 'step_size': 7, 'gamma': 0.8043340308914922}. Best is trial 7 with value: 0.3267831266449209.[0m
[32m[I 2025-02-02 04:36:50,885][0m Trial 8 finished with value: 0.6287119276683126 and parameters: {'observation_period_num': 144, 'train_rates': 0.7127360134588494, 'learning_rate': 0.00025148883952169247, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9058222489905143}. Best is trial 7 with value: 0.3267831266449209.[0m
[32m[I 2025-02-02 04:39:35,510][0m Trial 9 finished with value: 0.29799161314964295 and parameters: {'observation_period_num': 223, 'train_rates': 0.8810774078746346, 'learning_rate': 0.00012716584965641824, 'batch_size': 78, 'step_size': 13, 'gamma': 0.8074764740344226}. Best is trial 9 with value: 0.29799161314964295.[0m
[32m[I 2025-02-02 04:44:06,057][0m Trial 10 finished with value: 2.093007204044296 and parameters: {'observation_period_num': 219, 'train_rates': 0.9698524993997079, 'learning_rate': 0.000964037278957738, 'batch_size': 22, 'step_size': 11, 'gamma': 0.7616274819274804}. Best is trial 9 with value: 0.29799161314964295.[0m
[32m[I 2025-02-02 04:47:18,545][0m Trial 11 finished with value: 0.4204774816830953 and parameters: {'observation_period_num': 252, 'train_rates': 0.9000659470993517, 'learning_rate': 0.00011232827424204913, 'batch_size': 133, 'step_size': 6, 'gamma': 0.7833408131466854}. Best is trial 9 with value: 0.29799161314964295.[0m
[32m[I 2025-02-02 04:49:28,408][0m Trial 12 finished with value: 0.29768202625013684 and parameters: {'observation_period_num': 186, 'train_rates': 0.8931896898170033, 'learning_rate': 0.0005057581431238181, 'batch_size': 110, 'step_size': 10, 'gamma': 0.8043152622806261}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 04:51:56,774][0m Trial 13 finished with value: 0.5655600428581238 and parameters: {'observation_period_num': 194, 'train_rates': 0.9760175729344359, 'learning_rate': 0.0008614694266976328, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8274582859552424}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 04:55:03,833][0m Trial 14 finished with value: 0.351468867207148 and parameters: {'observation_period_num': 242, 'train_rates': 0.9082458090956128, 'learning_rate': 6.464837407073479e-05, 'batch_size': 71, 'step_size': 10, 'gamma': 0.7543698534372222}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 04:57:10,101][0m Trial 15 finished with value: 0.3106639128168317 and parameters: {'observation_period_num': 186, 'train_rates': 0.8658566795791209, 'learning_rate': 0.00025033447691787954, 'batch_size': 141, 'step_size': 12, 'gamma': 0.8194017025717072}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:03:08,236][0m Trial 16 finished with value: 0.356666193207105 and parameters: {'observation_period_num': 212, 'train_rates': 0.9454984635823845, 'learning_rate': 4.3158195284008076e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7953675000987979}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:05:06,585][0m Trial 17 finished with value: 0.3723106160759926 and parameters: {'observation_period_num': 167, 'train_rates': 0.9267333967182846, 'learning_rate': 0.0004870645369022281, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8462963826021177}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:07:44,697][0m Trial 18 finished with value: 0.33436978459358213 and parameters: {'observation_period_num': 225, 'train_rates': 0.84587432788541, 'learning_rate': 0.00013696395134009753, 'batch_size': 147, 'step_size': 12, 'gamma': 0.7919083746755995}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:09:28,319][0m Trial 19 finished with value: 1.0481456024614395 and parameters: {'observation_period_num': 168, 'train_rates': 0.6029341155001593, 'learning_rate': 2.8559068071745193e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.7727199485339449}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:10:26,092][0m Trial 20 finished with value: 0.7861495518715391 and parameters: {'observation_period_num': 5, 'train_rates': 0.7391299186465923, 'learning_rate': 0.00044251160870496147, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8948144361019883}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:12:40,310][0m Trial 21 finished with value: 0.3055263341915223 and parameters: {'observation_period_num': 194, 'train_rates': 0.8652578800718229, 'learning_rate': 0.00018584449635687992, 'batch_size': 134, 'step_size': 12, 'gamma': 0.8224713692460734}. Best is trial 12 with value: 0.29768202625013684.[0m
[32m[I 2025-02-02 05:15:07,600][0m Trial 22 finished with value: 0.286865080879452 and parameters: {'observation_period_num': 207, 'train_rates': 0.8803007298801657, 'learning_rate': 0.0001543309750587898, 'batch_size': 123, 'step_size': 13, 'gamma': 0.8447733514866346}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:18:08,098][0m Trial 23 finished with value: 0.3933710644837298 and parameters: {'observation_period_num': 231, 'train_rates': 0.9438999034831163, 'learning_rate': 0.0005145487917841464, 'batch_size': 86, 'step_size': 13, 'gamma': 0.8424910961309218}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:20:28,283][0m Trial 24 finished with value: 0.37437523568957304 and parameters: {'observation_period_num': 208, 'train_rates': 0.8238192555101341, 'learning_rate': 8.514367901767959e-05, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8071550522078366}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:22:56,103][0m Trial 25 finished with value: 0.31110678750500337 and parameters: {'observation_period_num': 165, 'train_rates': 0.8949739875456361, 'learning_rate': 4.758386843499211e-05, 'batch_size': 39, 'step_size': 10, 'gamma': 0.8485797833319455}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:25:55,786][0m Trial 26 finished with value: 0.3683400862458823 and parameters: {'observation_period_num': 236, 'train_rates': 0.9239534079482866, 'learning_rate': 0.0001519600361916583, 'batch_size': 159, 'step_size': 13, 'gamma': 0.7819122799699499}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:28:22,123][0m Trial 27 finished with value: 0.482002569194931 and parameters: {'observation_period_num': 205, 'train_rates': 0.8586295049382464, 'learning_rate': 0.0005556738869375019, 'batch_size': 82, 'step_size': 11, 'gamma': 0.981286675700989}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:30:22,002][0m Trial 28 finished with value: 0.2891299457448285 and parameters: {'observation_period_num': 174, 'train_rates': 0.8820992623564564, 'learning_rate': 0.000350591264272468, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8095906089195344}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:31:05,705][0m Trial 29 finished with value: 0.38478797674179077 and parameters: {'observation_period_num': 65, 'train_rates': 0.9461642885625012, 'learning_rate': 0.0003040977417699578, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8825242532426792}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:32:49,755][0m Trial 30 finished with value: 0.5259323877916017 and parameters: {'observation_period_num': 165, 'train_rates': 0.8070855030135446, 'learning_rate': 0.0007666482355855497, 'batch_size': 120, 'step_size': 11, 'gamma': 0.8560734636082085}. Best is trial 22 with value: 0.286865080879452.[0m
[32m[I 2025-02-02 05:34:54,752][0m Trial 31 finished with value: 0.2853776786557611 and parameters: {'observation_period_num': 181, 'train_rates': 0.884207684746761, 'learning_rate': 0.0001972289459885028, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8120558317314526}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:37:00,217][0m Trial 32 finished with value: 0.3005176946397655 and parameters: {'observation_period_num': 179, 'train_rates': 0.9129809004780872, 'learning_rate': 0.0002041818372892004, 'batch_size': 118, 'step_size': 15, 'gamma': 0.835340345899367}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:38:38,369][0m Trial 33 finished with value: 0.2943757606798566 and parameters: {'observation_period_num': 152, 'train_rates': 0.8495846073047524, 'learning_rate': 0.00037047653029216566, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8195675535614969}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:39:49,344][0m Trial 34 finished with value: 0.5639552369590634 and parameters: {'observation_period_num': 122, 'train_rates': 0.7863477604862413, 'learning_rate': 8.300948727496185e-05, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8183312024243062}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:40:53,111][0m Trial 35 finished with value: 0.31367818510139384 and parameters: {'observation_period_num': 106, 'train_rates': 0.8402147192529795, 'learning_rate': 0.0002602871390905421, 'batch_size': 152, 'step_size': 14, 'gamma': 0.9166284478861563}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:42:28,177][0m Trial 36 finished with value: 0.4487613298391041 and parameters: {'observation_period_num': 153, 'train_rates': 0.8236264856353434, 'learning_rate': 0.00032015112396571266, 'batch_size': 195, 'step_size': 1, 'gamma': 0.9421017142066167}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:44:00,308][0m Trial 37 finished with value: 0.8451741520951433 and parameters: {'observation_period_num': 153, 'train_rates': 0.7802090094865182, 'learning_rate': 2.0082980451926934e-05, 'batch_size': 228, 'step_size': 14, 'gamma': 0.8690369083378381}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:45:19,896][0m Trial 38 finished with value: 0.793386611425988 and parameters: {'observation_period_num': 128, 'train_rates': 0.8486506853453547, 'learning_rate': 9.60946678990265e-06, 'batch_size': 131, 'step_size': 12, 'gamma': 0.8341495237963172}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:45:53,790][0m Trial 39 finished with value: 0.5545729241358108 and parameters: {'observation_period_num': 50, 'train_rates': 0.7482521959030237, 'learning_rate': 0.0003999778326250197, 'batch_size': 180, 'step_size': 13, 'gamma': 0.7697965578906918}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:47:06,390][0m Trial 40 finished with value: 1.3186590820550919 and parameters: {'observation_period_num': 106, 'train_rates': 0.8765066725951525, 'learning_rate': 1.145892606003562e-06, 'batch_size': 99, 'step_size': 15, 'gamma': 0.8602859384881157}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:49:09,225][0m Trial 41 finished with value: 0.36778716556727886 and parameters: {'observation_period_num': 178, 'train_rates': 0.8963472624105328, 'learning_rate': 0.0006544861732911324, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8112636990020916}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:51:34,642][0m Trial 42 finished with value: 0.3142577155519979 and parameters: {'observation_period_num': 202, 'train_rates': 0.886168026164868, 'learning_rate': 0.00018088501002512639, 'batch_size': 158, 'step_size': 14, 'gamma': 0.7995801106223225}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:53:10,026][0m Trial 43 finished with value: 0.3491054519368393 and parameters: {'observation_period_num': 147, 'train_rates': 0.826259118411074, 'learning_rate': 0.00033407489665804855, 'batch_size': 94, 'step_size': 9, 'gamma': 0.7889671901447085}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:55:26,100][0m Trial 44 finished with value: 0.3732913692792257 and parameters: {'observation_period_num': 190, 'train_rates': 0.9242667027206748, 'learning_rate': 0.0006220003789038996, 'batch_size': 131, 'step_size': 12, 'gamma': 0.8140282555400787}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:57:18,743][0m Trial 45 finished with value: 1.024778491920895 and parameters: {'observation_period_num': 176, 'train_rates': 0.8055590122881514, 'learning_rate': 3.4429874754497977e-06, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8341611171845257}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 05:58:52,870][0m Trial 46 finished with value: 0.636023998260498 and parameters: {'observation_period_num': 135, 'train_rates': 0.9611082899250601, 'learning_rate': 0.00010014104685526598, 'batch_size': 143, 'step_size': 4, 'gamma': 0.8006031973494828}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 06:00:41,670][0m Trial 47 finished with value: 0.3212437756619481 and parameters: {'observation_period_num': 157, 'train_rates': 0.8768823174128709, 'learning_rate': 0.0002124553605225922, 'batch_size': 111, 'step_size': 11, 'gamma': 0.7778384269153177}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 06:03:04,079][0m Trial 48 finished with value: 0.357259178146493 and parameters: {'observation_period_num': 196, 'train_rates': 0.8571073891468798, 'learning_rate': 5.528352735521545e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8256235487596495}. Best is trial 31 with value: 0.2853776786557611.[0m
[32m[I 2025-02-02 06:05:12,199][0m Trial 49 finished with value: 0.8531199291805133 and parameters: {'observation_period_num': 212, 'train_rates': 0.6783676667546736, 'learning_rate': 0.0007673366769181379, 'batch_size': 168, 'step_size': 7, 'gamma': 0.876669509508418}. Best is trial 31 with value: 0.2853776786557611.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-02 06:05:12,207][0m A new study created in memory with name: no-name-2a491780-5d82-47e2-b65e-9ba215ec85ef[0m
[32m[I 2025-02-02 06:07:09,843][0m Trial 0 finished with value: 2.0649491178578345 and parameters: {'observation_period_num': 193, 'train_rates': 0.7057031660940485, 'learning_rate': 1.2512257057523295e-06, 'batch_size': 128, 'step_size': 15, 'gamma': 0.7766709584286684}. Best is trial 0 with value: 2.0649491178578345.[0m
[32m[I 2025-02-02 06:08:50,189][0m Trial 1 finished with value: 1.337486247981296 and parameters: {'observation_period_num': 152, 'train_rates': 0.757010131373168, 'learning_rate': 0.0005272470631071465, 'batch_size': 62, 'step_size': 3, 'gamma': 0.9850306062606283}. Best is trial 1 with value: 1.337486247981296.[0m
[32m[I 2025-02-02 06:10:30,697][0m Trial 2 finished with value: 0.5246905858356755 and parameters: {'observation_period_num': 163, 'train_rates': 0.7700533411569815, 'learning_rate': 8.529085851255065e-05, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8446930764572731}. Best is trial 2 with value: 0.5246905858356755.[0m
[32m[I 2025-02-02 06:11:53,483][0m Trial 3 finished with value: 0.7170424461364746 and parameters: {'observation_period_num': 125, 'train_rates': 0.9736651820019273, 'learning_rate': 4.2951128738393836e-05, 'batch_size': 223, 'step_size': 7, 'gamma': 0.9159500738838181}. Best is trial 2 with value: 0.5246905858356755.[0m
[32m[I 2025-02-02 06:13:07,491][0m Trial 4 finished with value: 0.7821053331925548 and parameters: {'observation_period_num': 131, 'train_rates': 0.6026272448790507, 'learning_rate': 2.3188896515384146e-05, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8450466479287992}. Best is trial 2 with value: 0.5246905858356755.[0m
[32m[I 2025-02-02 06:13:31,885][0m Trial 5 finished with value: 1.0569826006307834 and parameters: {'observation_period_num': 26, 'train_rates': 0.649770634923592, 'learning_rate': 2.097644663761169e-05, 'batch_size': 211, 'step_size': 7, 'gamma': 0.7954109597923551}. Best is trial 2 with value: 0.5246905858356755.[0m
[32m[I 2025-02-02 06:14:25,083][0m Trial 6 finished with value: 1.5467840223245217 and parameters: {'observation_period_num': 91, 'train_rates': 0.7518211041769614, 'learning_rate': 1.3951120852374647e-06, 'batch_size': 195, 'step_size': 12, 'gamma': 0.9198336651080717}. Best is trial 2 with value: 0.5246905858356755.[0m
[32m[I 2025-02-02 06:15:14,676][0m Trial 7 finished with value: 0.31925056743146346 and parameters: {'observation_period_num': 78, 'train_rates': 0.8779930764050035, 'learning_rate': 0.00023893167550175678, 'batch_size': 227, 'step_size': 1, 'gamma': 0.9779830773808906}. Best is trial 7 with value: 0.31925056743146346.[0m
[32m[I 2025-02-02 06:17:08,337][0m Trial 8 finished with value: 0.5851479887962341 and parameters: {'observation_period_num': 193, 'train_rates': 0.7062525251647503, 'learning_rate': 0.0007455957378030963, 'batch_size': 252, 'step_size': 9, 'gamma': 0.7970136727643334}. Best is trial 7 with value: 0.31925056743146346.[0m
[32m[I 2025-02-02 06:19:09,841][0m Trial 9 finished with value: 1.1540205493259312 and parameters: {'observation_period_num': 201, 'train_rates': 0.7029269597524246, 'learning_rate': 6.743143095506109e-06, 'batch_size': 140, 'step_size': 1, 'gamma': 0.9871655476762369}. Best is trial 7 with value: 0.31925056743146346.[0m
[32m[I 2025-02-02 06:19:47,421][0m Trial 10 finished with value: 0.31961886420797764 and parameters: {'observation_period_num': 34, 'train_rates': 0.8941639159027736, 'learning_rate': 0.00018288253942211707, 'batch_size': 169, 'step_size': 4, 'gamma': 0.9236595834645362}. Best is trial 7 with value: 0.31925056743146346.[0m
[32m[I 2025-02-02 06:20:24,751][0m Trial 11 finished with value: 0.289839117215314 and parameters: {'observation_period_num': 31, 'train_rates': 0.888204017373834, 'learning_rate': 0.00020814832232305427, 'batch_size': 171, 'step_size': 4, 'gamma': 0.9378932570707562}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:21:11,121][0m Trial 12 finished with value: 0.3899593804186831 and parameters: {'observation_period_num': 73, 'train_rates': 0.8667638702092032, 'learning_rate': 0.00022352557466924955, 'batch_size': 173, 'step_size': 1, 'gamma': 0.9527480767153231}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:21:37,311][0m Trial 13 finished with value: 0.3427805575525731 and parameters: {'observation_period_num': 6, 'train_rates': 0.8652978921873069, 'learning_rate': 0.00023181951705860117, 'batch_size': 254, 'step_size': 4, 'gamma': 0.8873150975330913}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:22:23,366][0m Trial 14 finished with value: 0.4171105921268463 and parameters: {'observation_period_num': 69, 'train_rates': 0.953704950915892, 'learning_rate': 8.556323084330441e-05, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9539255227132541}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:26:06,738][0m Trial 15 finished with value: 2.076370895610136 and parameters: {'observation_period_num': 53, 'train_rates': 0.8239897614082818, 'learning_rate': 0.0008827888044969383, 'batch_size': 24, 'step_size': 2, 'gamma': 0.9548379489823967}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:27:08,273][0m Trial 16 finished with value: 0.31746148254348444 and parameters: {'observation_period_num': 99, 'train_rates': 0.9058811899367317, 'learning_rate': 0.0003517108951035288, 'batch_size': 221, 'step_size': 6, 'gamma': 0.8782246502541166}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:30:11,533][0m Trial 17 finished with value: 0.8161020214651146 and parameters: {'observation_period_num': 241, 'train_rates': 0.924838012701909, 'learning_rate': 7.502888072826348e-06, 'batch_size': 149, 'step_size': 8, 'gamma': 0.8842348007134687}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:31:21,973][0m Trial 18 finished with value: 0.3861664627680559 and parameters: {'observation_period_num': 111, 'train_rates': 0.8243769924988985, 'learning_rate': 8.439925177957336e-05, 'batch_size': 105, 'step_size': 6, 'gamma': 0.8526160945762618}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:31:52,779][0m Trial 19 finished with value: 0.35093305375286726 and parameters: {'observation_period_num': 39, 'train_rates': 0.9179721874738712, 'learning_rate': 0.00038444098198818093, 'batch_size': 207, 'step_size': 10, 'gamma': 0.8182851105113438}. Best is trial 11 with value: 0.289839117215314.[0m
[32m[I 2025-02-02 06:33:00,074][0m Trial 20 finished with value: 0.26262933015823364 and parameters: {'observation_period_num': 100, 'train_rates': 0.9880004824620005, 'learning_rate': 4.469821098221268e-05, 'batch_size': 160, 'step_size': 5, 'gamma': 0.894409638412743}. Best is trial 20 with value: 0.26262933015823364.[0m
[32m[I 2025-02-02 06:34:04,376][0m Trial 21 finished with value: 0.6869743466377258 and parameters: {'observation_period_num': 98, 'train_rates': 0.9686845554222515, 'learning_rate': 4.283407707209776e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.8993092928188821}. Best is trial 20 with value: 0.26262933015823364.[0m
[32m[I 2025-02-02 06:34:59,912][0m Trial 22 finished with value: 0.13090169429779053 and parameters: {'observation_period_num': 7, 'train_rates': 0.986857989740456, 'learning_rate': 0.00012131491893654789, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8697989196823271}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:35:53,371][0m Trial 23 finished with value: 0.2010117769241333 and parameters: {'observation_period_num': 5, 'train_rates': 0.9850157468902637, 'learning_rate': 0.00013042351527482954, 'batch_size': 122, 'step_size': 3, 'gamma': 0.8590425342471258}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:36:49,384][0m Trial 24 finished with value: 0.6231775283813477 and parameters: {'observation_period_num': 11, 'train_rates': 0.9898492047870086, 'learning_rate': 1.3405345559760666e-05, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8653829666542807}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:38:04,492][0m Trial 25 finished with value: 0.4328140642331994 and parameters: {'observation_period_num': 56, 'train_rates': 0.944372191109134, 'learning_rate': 0.00010827648426517901, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8202870376204903}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:38:52,313][0m Trial 26 finished with value: 0.33144456148147583 and parameters: {'observation_period_num': 8, 'train_rates': 0.9892507245834293, 'learning_rate': 6.124443166876283e-05, 'batch_size': 145, 'step_size': 3, 'gamma': 0.8286751648081301}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:40:08,750][0m Trial 27 finished with value: 0.5585866772211515 and parameters: {'observation_period_num': 52, 'train_rates': 0.9330435121821812, 'learning_rate': 1.1840179821998871e-05, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9020867295373057}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:41:08,214][0m Trial 28 finished with value: 0.3874708441672502 and parameters: {'observation_period_num': 19, 'train_rates': 0.9541119053941016, 'learning_rate': 0.00013584739776957704, 'batch_size': 110, 'step_size': 5, 'gamma': 0.8639816374880681}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:44:10,010][0m Trial 29 finished with value: 0.3951415394393491 and parameters: {'observation_period_num': 250, 'train_rates': 0.8425729045435009, 'learning_rate': 4.4413613184682985e-05, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8975185431327655}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:46:04,372][0m Trial 30 finished with value: 1.573455963659724 and parameters: {'observation_period_num': 159, 'train_rates': 0.9612289661046186, 'learning_rate': 2.8590860055733446e-06, 'batch_size': 90, 'step_size': 2, 'gamma': 0.8636122717850551}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:46:48,389][0m Trial 31 finished with value: 0.15234695374965668 and parameters: {'observation_period_num': 35, 'train_rates': 0.988492900571821, 'learning_rate': 0.00014334046076017214, 'batch_size': 155, 'step_size': 4, 'gamma': 0.9405966083779876}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:47:38,962][0m Trial 32 finished with value: 0.16025224328041077 and parameters: {'observation_period_num': 48, 'train_rates': 0.9820976990940722, 'learning_rate': 0.0001415162185265401, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9313033472586553}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:48:29,393][0m Trial 33 finished with value: 0.3698337353066186 and parameters: {'observation_period_num': 45, 'train_rates': 0.9378131264070753, 'learning_rate': 0.00013686267822174864, 'batch_size': 129, 'step_size': 3, 'gamma': 0.9691876830460986}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:50:38,101][0m Trial 34 finished with value: 0.5251298695802689 and parameters: {'observation_period_num': 24, 'train_rates': 0.9673121603821909, 'learning_rate': 0.00044502510219608763, 'batch_size': 48, 'step_size': 2, 'gamma': 0.9313562898251992}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:51:27,680][0m Trial 35 finished with value: 0.42234924987510397 and parameters: {'observation_period_num': 63, 'train_rates': 0.9159133257605321, 'learning_rate': 6.590914836480781e-05, 'batch_size': 132, 'step_size': 4, 'gamma': 0.9095992526559569}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:52:39,225][0m Trial 36 finished with value: 0.5261314511299133 and parameters: {'observation_period_num': 20, 'train_rates': 0.9722580477837065, 'learning_rate': 0.00015110305360635463, 'batch_size': 94, 'step_size': 7, 'gamma': 0.7587904616568607}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:53:19,135][0m Trial 37 finished with value: 0.6473394284489027 and parameters: {'observation_period_num': 42, 'train_rates': 0.7820429174525446, 'learning_rate': 2.6249433874324484e-05, 'batch_size': 147, 'step_size': 15, 'gamma': 0.8381465649115097}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:54:53,858][0m Trial 38 finished with value: 0.4231864904997333 and parameters: {'observation_period_num': 137, 'train_rates': 0.9470641366017409, 'learning_rate': 0.0005537399700870193, 'batch_size': 116, 'step_size': 6, 'gamma': 0.9384520007241361}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:56:33,399][0m Trial 39 finished with value: 0.26495527142464226 and parameters: {'observation_period_num': 6, 'train_rates': 0.9774886014887312, 'learning_rate': 0.0002831909714985915, 'batch_size': 62, 'step_size': 2, 'gamma': 0.8507310527672582}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:57:03,033][0m Trial 40 finished with value: 0.5703956288961886 and parameters: {'observation_period_num': 26, 'train_rates': 0.7379225895928014, 'learning_rate': 6.751098576750386e-05, 'batch_size': 185, 'step_size': 7, 'gamma': 0.8758586763871978}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 06:58:23,054][0m Trial 41 finished with value: 0.25309109687805176 and parameters: {'observation_period_num': 116, 'train_rates': 0.9839581201587683, 'learning_rate': 3.8877900080880455e-05, 'batch_size': 160, 'step_size': 5, 'gamma': 0.9120149941612953}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 07:00:38,722][0m Trial 42 finished with value: 0.42157655358314516 and parameters: {'observation_period_num': 192, 'train_rates': 0.9350297815818375, 'learning_rate': 0.00010270049539401096, 'batch_size': 152, 'step_size': 4, 'gamma': 0.9144983626662057}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 07:01:43,001][0m Trial 43 finished with value: 0.493032342330976 and parameters: {'observation_period_num': 80, 'train_rates': 0.9617994855723048, 'learning_rate': 3.526409047018388e-05, 'batch_size': 105, 'step_size': 5, 'gamma': 0.968966657822938}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 07:03:48,570][0m Trial 44 finished with value: 0.7522384306013365 and parameters: {'observation_period_num': 177, 'train_rates': 0.9018860064817913, 'learning_rate': 1.5911120010566308e-05, 'batch_size': 131, 'step_size': 3, 'gamma': 0.9260505156176009}. Best is trial 22 with value: 0.13090169429779053.[0m
[32m[I 2025-02-02 07:05:31,852][0m Trial 45 finished with value: 0.1291191726922989 and parameters: {'observation_period_num': 145, 'train_rates': 0.9896491265874469, 'learning_rate': 0.00015031019279272868, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9423980468281211}. Best is trial 45 with value: 0.1291191726922989.[0m
[32m[I 2025-02-02 07:06:56,222][0m Trial 46 finished with value: 0.6669069727372486 and parameters: {'observation_period_num': 149, 'train_rates': 0.679617390464321, 'learning_rate': 0.00016392220823880686, 'batch_size': 135, 'step_size': 4, 'gamma': 0.9450313325724402}. Best is trial 45 with value: 0.1291191726922989.[0m
[32m[I 2025-02-02 07:08:08,322][0m Trial 47 finished with value: 0.7287044444867166 and parameters: {'observation_period_num': 142, 'train_rates': 0.6141628894059448, 'learning_rate': 0.0002613487944716847, 'batch_size': 180, 'step_size': 1, 'gamma': 0.9693144224784568}. Best is trial 45 with value: 0.1291191726922989.[0m
[32m[I 2025-02-02 07:09:14,752][0m Trial 48 finished with value: 0.378852519597716 and parameters: {'observation_period_num': 34, 'train_rates': 0.9551667650320337, 'learning_rate': 0.0001246785503770653, 'batch_size': 97, 'step_size': 3, 'gamma': 0.9452011102680964}. Best is trial 45 with value: 0.1291191726922989.[0m
[32m[I 2025-02-02 07:11:19,650][0m Trial 49 finished with value: 0.5795860886573792 and parameters: {'observation_period_num': 171, 'train_rates': 0.9701514853865465, 'learning_rate': 0.0005766435652526499, 'batch_size': 119, 'step_size': 7, 'gamma': 0.7900228576902643}. Best is trial 45 with value: 0.1291191726922989.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-02 07:11:19,661][0m A new study created in memory with name: no-name-d92ca412-cd38-490f-9e28-e0e7705697fd[0m
[32m[I 2025-02-02 07:13:08,764][0m Trial 0 finished with value: 0.5296744984836631 and parameters: {'observation_period_num': 161, 'train_rates': 0.8714196086468549, 'learning_rate': 3.310607982220475e-05, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9172343972315795}. Best is trial 0 with value: 0.5296744984836631.[0m
[32m[I 2025-02-02 07:15:33,435][0m Trial 1 finished with value: 0.8167853299817891 and parameters: {'observation_period_num': 237, 'train_rates': 0.6443599680366596, 'learning_rate': 0.0004255976867942924, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8529013836446461}. Best is trial 0 with value: 0.5296744984836631.[0m
[32m[I 2025-02-02 07:16:20,404][0m Trial 2 finished with value: 1.2685636574995585 and parameters: {'observation_period_num': 47, 'train_rates': 0.6919484635713467, 'learning_rate': 3.3224050891095572e-06, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8122098844113911}. Best is trial 0 with value: 0.5296744984836631.[0m
[32m[I 2025-02-02 07:19:46,330][0m Trial 3 finished with value: 0.4940830114133218 and parameters: {'observation_period_num': 252, 'train_rates': 0.9685069978976106, 'learning_rate': 0.00011935540283962418, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8030766468262895}. Best is trial 3 with value: 0.4940830114133218.[0m
[32m[I 2025-02-02 07:20:44,687][0m Trial 4 finished with value: 1.1579066654469103 and parameters: {'observation_period_num': 99, 'train_rates': 0.8683400343637306, 'learning_rate': 3.560978431156989e-06, 'batch_size': 252, 'step_size': 6, 'gamma': 0.9497015317319827}. Best is trial 3 with value: 0.4940830114133218.[0m
[32m[I 2025-02-02 07:21:59,267][0m Trial 5 finished with value: 1.0042460688295212 and parameters: {'observation_period_num': 35, 'train_rates': 0.8717131652866128, 'learning_rate': 1.7092382303354956e-06, 'batch_size': 80, 'step_size': 14, 'gamma': 0.7773030937799171}. Best is trial 3 with value: 0.4940830114133218.[0m
[32m[I 2025-02-02 07:24:15,909][0m Trial 6 finished with value: 0.6527275411959956 and parameters: {'observation_period_num': 205, 'train_rates': 0.75259191157812, 'learning_rate': 0.0002756402241193863, 'batch_size': 71, 'step_size': 2, 'gamma': 0.7876157662032504}. Best is trial 3 with value: 0.4940830114133218.[0m
[32m[I 2025-02-02 07:26:53,307][0m Trial 7 finished with value: 0.29211788132505595 and parameters: {'observation_period_num': 50, 'train_rates': 0.8358102033519468, 'learning_rate': 0.00026630367318448213, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9006874215466729}. Best is trial 7 with value: 0.29211788132505595.[0m
[32m[I 2025-02-02 07:28:49,446][0m Trial 8 finished with value: 0.31213777577588925 and parameters: {'observation_period_num': 173, 'train_rates': 0.8811134032172987, 'learning_rate': 0.00010386584949952753, 'batch_size': 161, 'step_size': 9, 'gamma': 0.9779983595444326}. Best is trial 7 with value: 0.29211788132505595.[0m
[32m[I 2025-02-02 07:29:48,800][0m Trial 9 finished with value: 1.6817230233606302 and parameters: {'observation_period_num': 109, 'train_rates': 0.7575778071085862, 'learning_rate': 4.482353876720363e-06, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8814025380989543}. Best is trial 7 with value: 0.29211788132505595.[0m
[32m[I 2025-02-02 07:34:43,946][0m Trial 10 finished with value: 0.13401194289326668 and parameters: {'observation_period_num': 68, 'train_rates': 0.9890883537677629, 'learning_rate': 1.915941240448754e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.875191720048181}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:39:53,888][0m Trial 11 finished with value: 0.3985168687482872 and parameters: {'observation_period_num': 10, 'train_rates': 0.9658335816127414, 'learning_rate': 2.0880592709708187e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8788718743317371}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:44:32,876][0m Trial 12 finished with value: 0.138957180082798 and parameters: {'observation_period_num': 73, 'train_rates': 0.9857045352949132, 'learning_rate': 2.2788287589491998e-05, 'batch_size': 21, 'step_size': 11, 'gamma': 0.914875708285816}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:46:53,389][0m Trial 13 finished with value: 0.4177289719765003 and parameters: {'observation_period_num': 93, 'train_rates': 0.9365553139402275, 'learning_rate': 1.7283963962716002e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.8490632694527935}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:47:46,919][0m Trial 14 finished with value: 0.3762102425098419 and parameters: {'observation_period_num': 75, 'train_rates': 0.9863038017217354, 'learning_rate': 1.3549816844705134e-05, 'batch_size': 148, 'step_size': 11, 'gamma': 0.9227319577516288}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:53:12,273][0m Trial 15 finished with value: 0.27672422251066264 and parameters: {'observation_period_num': 123, 'train_rates': 0.919067601722623, 'learning_rate': 6.929674239386769e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8350024721276095}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:54:51,412][0m Trial 16 finished with value: 0.5446796243347138 and parameters: {'observation_period_num': 65, 'train_rates': 0.7966492330477621, 'learning_rate': 9.07636379827909e-06, 'batch_size': 53, 'step_size': 12, 'gamma': 0.9453938725951443}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:56:37,662][0m Trial 17 finished with value: 0.39238227347723037 and parameters: {'observation_period_num': 149, 'train_rates': 0.925597357447567, 'learning_rate': 4.381174542328406e-05, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8936211909744584}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:57:11,647][0m Trial 18 finished with value: 0.2869180738925934 and parameters: {'observation_period_num': 6, 'train_rates': 0.9849800334936315, 'learning_rate': 0.00092596009997568, 'batch_size': 197, 'step_size': 8, 'gamma': 0.7514228300643389}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:58:30,841][0m Trial 19 finished with value: 1.0952411453754032 and parameters: {'observation_period_num': 80, 'train_rates': 0.6140580087594314, 'learning_rate': 6.321280431614233e-06, 'batch_size': 55, 'step_size': 4, 'gamma': 0.987062095646567}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 07:59:21,629][0m Trial 20 finished with value: 0.36509785387251115 and parameters: {'observation_period_num': 31, 'train_rates': 0.8244901431086041, 'learning_rate': 4.721551925416006e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.945343450787005}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:04:21,065][0m Trial 21 finished with value: 0.2778794058525617 and parameters: {'observation_period_num': 127, 'train_rates': 0.9234507448310142, 'learning_rate': 8.334171667457524e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8361840028236952}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:10:13,457][0m Trial 22 finished with value: 0.32666189298033715 and parameters: {'observation_period_num': 124, 'train_rates': 0.9435473901242128, 'learning_rate': 6.096507163925295e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8318176482476525}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:12:26,537][0m Trial 23 finished with value: 0.33450137889560533 and parameters: {'observation_period_num': 140, 'train_rates': 0.9035476696992201, 'learning_rate': 2.460556465528298e-05, 'batch_size': 42, 'step_size': 13, 'gamma': 0.8669020447854527}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:14:08,598][0m Trial 24 finished with value: 0.5711169440815919 and parameters: {'observation_period_num': 103, 'train_rates': 0.9519204998426437, 'learning_rate': 9.79084913425423e-06, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9125696395695468}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:16:36,233][0m Trial 25 finished with value: 0.30806689195392667 and parameters: {'observation_period_num': 181, 'train_rates': 0.9068950953323719, 'learning_rate': 0.00020192370820379605, 'batch_size': 40, 'step_size': 12, 'gamma': 0.8601048798515762}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:19:53,953][0m Trial 26 finished with value: 0.43354879407321706 and parameters: {'observation_period_num': 65, 'train_rates': 0.9649701433186658, 'learning_rate': 2.8179079749580924e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8264160499328531}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:21:31,352][0m Trial 27 finished with value: 0.8230709433555603 and parameters: {'observation_period_num': 120, 'train_rates': 0.9848366829831398, 'learning_rate': 1.1795988298924248e-06, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8923436347876826}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:22:34,443][0m Trial 28 finished with value: 0.292175539994498 and parameters: {'observation_period_num': 82, 'train_rates': 0.9036279554780408, 'learning_rate': 0.00015115995120554052, 'batch_size': 98, 'step_size': 5, 'gamma': 0.9279285348525159}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:25:35,729][0m Trial 29 finished with value: 0.49826702015082724 and parameters: {'observation_period_num': 155, 'train_rates': 0.8333117953245972, 'learning_rate': 6.642242085780776e-05, 'batch_size': 29, 'step_size': 1, 'gamma': 0.8758103556817017}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:26:39,071][0m Trial 30 finished with value: 0.5792751988849124 and parameters: {'observation_period_num': 54, 'train_rates': 0.9489203736266243, 'learning_rate': 1.1046309922218838e-05, 'batch_size': 97, 'step_size': 12, 'gamma': 0.9674913939922368}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:30:55,825][0m Trial 31 finished with value: 0.32185308901327 and parameters: {'observation_period_num': 128, 'train_rates': 0.9207369572446307, 'learning_rate': 8.279290143297967e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8391467813676969}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:33:33,719][0m Trial 32 finished with value: 0.29777679353570324 and parameters: {'observation_period_num': 202, 'train_rates': 0.8866133144005028, 'learning_rate': 4.3872939176915236e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8185920186858248}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:37:58,586][0m Trial 33 finished with value: 0.2975263391140003 and parameters: {'observation_period_num': 139, 'train_rates': 0.8528392275845466, 'learning_rate': 3.414963033315409e-05, 'batch_size': 20, 'step_size': 14, 'gamma': 0.848375363474666}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:39:08,083][0m Trial 34 finished with value: 0.9426696427077669 and parameters: {'observation_period_num': 32, 'train_rates': 0.6838604887288348, 'learning_rate': 0.0005061242770378362, 'batch_size': 68, 'step_size': 14, 'gamma': 0.8029027908017885}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:40:17,062][0m Trial 35 finished with value: 0.31580960750579834 and parameters: {'observation_period_num': 111, 'train_rates': 0.9259793895293448, 'learning_rate': 0.00013571310270274063, 'batch_size': 254, 'step_size': 13, 'gamma': 0.9083876220789247}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:43:37,021][0m Trial 36 finished with value: 0.16113515198230743 and parameters: {'observation_period_num': 90, 'train_rates': 0.9896225069659486, 'learning_rate': 1.572133687873022e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8613467116988066}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:44:55,341][0m Trial 37 finished with value: 0.42470407485961914 and parameters: {'observation_period_num': 85, 'train_rates': 0.9881467419849449, 'learning_rate': 5.377739766505112e-06, 'batch_size': 79, 'step_size': 8, 'gamma': 0.9330236098376573}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:47:58,627][0m Trial 38 finished with value: 0.8591980895783642 and parameters: {'observation_period_num': 70, 'train_rates': 0.9649485562115826, 'learning_rate': 2.383314759019159e-06, 'batch_size': 33, 'step_size': 10, 'gamma': 0.861501871574116}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:49:54,711][0m Trial 39 finished with value: 0.5228784350844903 and parameters: {'observation_period_num': 96, 'train_rates': 0.957217298086307, 'learning_rate': 1.6401964641695506e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8074834032129983}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:50:22,742][0m Trial 40 finished with value: 1.2548536072473158 and parameters: {'observation_period_num': 47, 'train_rates': 0.7595421695600084, 'learning_rate': 8.035092526922483e-06, 'batch_size': 222, 'step_size': 11, 'gamma': 0.8876811974553929}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:53:40,001][0m Trial 41 finished with value: 0.3417962572368031 and parameters: {'observation_period_num': 114, 'train_rates': 0.9382171286166017, 'learning_rate': 3.362237920024349e-05, 'batch_size': 29, 'step_size': 12, 'gamma': 0.8435308867985838}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 08:59:23,050][0m Trial 42 finished with value: 0.4488161979596826 and parameters: {'observation_period_num': 91, 'train_rates': 0.9724646516870402, 'learning_rate': 9.06964505988015e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.795766954463811}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:02:04,938][0m Trial 43 finished with value: 0.3400070497111694 and parameters: {'observation_period_num': 133, 'train_rates': 0.894928921604828, 'learning_rate': 2.1480998569622848e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8269334185188232}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:04:05,844][0m Trial 44 finished with value: 0.386731492340722 and parameters: {'observation_period_num': 172, 'train_rates': 0.8636658012034645, 'learning_rate': 6.079774626107883e-05, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8574167619982254}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:06:12,092][0m Trial 45 finished with value: 0.48291537236599696 and parameters: {'observation_period_num': 58, 'train_rates': 0.920192690517325, 'learning_rate': 1.4831675030331364e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.7772134500308546}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:07:29,188][0m Trial 46 finished with value: 0.433982252604083 and parameters: {'observation_period_num': 105, 'train_rates': 0.9668407728466452, 'learning_rate': 0.0001826568152380021, 'batch_size': 89, 'step_size': 15, 'gamma': 0.8697216300431589}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:08:12,755][0m Trial 47 finished with value: 0.4649797137621995 and parameters: {'observation_period_num': 43, 'train_rates': 0.9375247808038261, 'learning_rate': 2.6644971929600656e-05, 'batch_size': 137, 'step_size': 14, 'gamma': 0.902469244497396}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:11:25,156][0m Trial 48 finished with value: 0.5440558414396082 and parameters: {'observation_period_num': 25, 'train_rates': 0.7354234730465745, 'learning_rate': 3.911714201665521e-05, 'batch_size': 25, 'step_size': 8, 'gamma': 0.8355275280812504}. Best is trial 10 with value: 0.13401194289326668.[0m
[32m[I 2025-02-02 09:12:08,085][0m Trial 49 finished with value: 1.2281912631573884 and parameters: {'observation_period_num': 73, 'train_rates': 0.8003559528793105, 'learning_rate': 3.5619570304200908e-06, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8142227028322626}. Best is trial 10 with value: 0.13401194289326668.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-02 09:12:08,095][0m A new study created in memory with name: no-name-f5811e4c-c847-4cc8-9a09-4b7991089efc[0m
[32m[I 2025-02-02 09:15:56,503][0m Trial 0 finished with value: 0.6734988338141867 and parameters: {'observation_period_num': 169, 'train_rates': 0.7063351807332215, 'learning_rate': 2.77007635671957e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.9377592646580567}. Best is trial 0 with value: 0.6734988338141867.[0m
[32m[I 2025-02-02 09:19:39,962][0m Trial 1 finished with value: 1.0112677598514983 and parameters: {'observation_period_num': 29, 'train_rates': 0.7547112366120071, 'learning_rate': 0.0001545734203684932, 'batch_size': 22, 'step_size': 12, 'gamma': 0.9542085570175189}. Best is trial 0 with value: 0.6734988338141867.[0m
[32m[I 2025-02-02 09:23:42,788][0m Trial 2 finished with value: 0.13685504164622755 and parameters: {'observation_period_num': 155, 'train_rates': 0.9827852057408223, 'learning_rate': 3.0038439602364655e-05, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8447853061858805}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:25:53,363][0m Trial 3 finished with value: 1.768486615261733 and parameters: {'observation_period_num': 215, 'train_rates': 0.6848576655028624, 'learning_rate': 1.5545762249756766e-06, 'batch_size': 161, 'step_size': 1, 'gamma': 0.926897323603334}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:28:38,666][0m Trial 4 finished with value: 1.411632776260376 and parameters: {'observation_period_num': 214, 'train_rates': 0.9651472951900264, 'learning_rate': 1.7471261874181662e-06, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8495068097233038}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:29:09,461][0m Trial 5 finished with value: 0.4991971864354136 and parameters: {'observation_period_num': 36, 'train_rates': 0.8535326986581646, 'learning_rate': 2.11542612112473e-05, 'batch_size': 194, 'step_size': 14, 'gamma': 0.9480305102667081}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:31:59,563][0m Trial 6 finished with value: 0.6979953646659851 and parameters: {'observation_period_num': 218, 'train_rates': 0.9702963798188615, 'learning_rate': 1.4028739614177737e-05, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9347186190203367}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:34:03,860][0m Trial 7 finished with value: 0.3990606665611267 and parameters: {'observation_period_num': 176, 'train_rates': 0.9516252667372895, 'learning_rate': 0.0003924641107504518, 'batch_size': 136, 'step_size': 6, 'gamma': 0.7763104689686862}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:35:28,137][0m Trial 8 finished with value: 0.3256662556873494 and parameters: {'observation_period_num': 118, 'train_rates': 0.9300671479632314, 'learning_rate': 7.42096049717225e-05, 'batch_size': 84, 'step_size': 5, 'gamma': 0.9451956973013701}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:37:18,085][0m Trial 9 finished with value: 0.9208798369864818 and parameters: {'observation_period_num': 170, 'train_rates': 0.6341604530627675, 'learning_rate': 3.8472896016382726e-05, 'batch_size': 41, 'step_size': 3, 'gamma': 0.9256748636490332}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:38:13,973][0m Trial 10 finished with value: 0.9776349886139827 and parameters: {'observation_period_num': 91, 'train_rates': 0.8595756459464107, 'learning_rate': 5.178099914080164e-06, 'batch_size': 244, 'step_size': 9, 'gamma': 0.8406750566115446}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:39:30,527][0m Trial 11 finished with value: 0.3108163224357479 and parameters: {'observation_period_num': 104, 'train_rates': 0.8885703670208447, 'learning_rate': 8.998254098905814e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.8774021884226144}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:40:52,494][0m Trial 12 finished with value: 0.7962655667373885 and parameters: {'observation_period_num': 75, 'train_rates': 0.8775563277533682, 'learning_rate': 0.0009079479598000984, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8828324963037467}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:42:30,312][0m Trial 13 finished with value: 0.37122408165866605 and parameters: {'observation_period_num': 135, 'train_rates': 0.8960429523620792, 'learning_rate': 0.00013102837983979896, 'batch_size': 69, 'step_size': 4, 'gamma': 0.8034713207730932}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:43:58,199][0m Trial 14 finished with value: 0.8286355511040099 and parameters: {'observation_period_num': 135, 'train_rates': 0.8074042301844434, 'learning_rate': 8.168255524751349e-06, 'batch_size': 86, 'step_size': 9, 'gamma': 0.8898908561929706}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:45:39,503][0m Trial 15 finished with value: 0.3256525695323944 and parameters: {'observation_period_num': 73, 'train_rates': 0.9871632951989124, 'learning_rate': 6.41264388459926e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8190841946618064}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:46:41,477][0m Trial 16 finished with value: 0.5029153147259274 and parameters: {'observation_period_num': 98, 'train_rates': 0.8056141451861195, 'learning_rate': 0.0002572690583777607, 'batch_size': 101, 'step_size': 8, 'gamma': 0.9875657377053999}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:48:43,725][0m Trial 17 finished with value: 0.7020556666250304 and parameters: {'observation_period_num': 7, 'train_rates': 0.91392013318864, 'learning_rate': 7.3047195355202095e-06, 'batch_size': 47, 'step_size': 6, 'gamma': 0.7610278169516858}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:51:39,925][0m Trial 18 finished with value: 0.45983318162019476 and parameters: {'observation_period_num': 246, 'train_rates': 0.8399686034592314, 'learning_rate': 6.299384923807672e-05, 'batch_size': 159, 'step_size': 4, 'gamma': 0.900019466926272}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:57:26,686][0m Trial 19 finished with value: 0.5312676697242551 and parameters: {'observation_period_num': 154, 'train_rates': 0.9266971304072542, 'learning_rate': 3.4854563029749222e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8607836613073916}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 09:59:37,713][0m Trial 20 finished with value: 0.43169296757160175 and parameters: {'observation_period_num': 54, 'train_rates': 0.9004882330941563, 'learning_rate': 0.0005306624033396903, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8107421734147522}. Best is trial 2 with value: 0.13685504164622755.[0m
Early stopping at epoch 60
[32m[I 2025-02-02 10:00:41,425][0m Trial 21 finished with value: 0.7074100123324865 and parameters: {'observation_period_num': 71, 'train_rates': 0.9753858185529694, 'learning_rate': 6.019675369991043e-05, 'batch_size': 58, 'step_size': 1, 'gamma': 0.8170737225166733}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:02:01,238][0m Trial 22 finished with value: 0.19221539795398712 and parameters: {'observation_period_num': 107, 'train_rates': 0.9889223056262315, 'learning_rate': 0.00011132876342746449, 'batch_size': 90, 'step_size': 3, 'gamma': 0.8372260960391156}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:03:19,622][0m Trial 23 finished with value: 0.44788248596652863 and parameters: {'observation_period_num': 115, 'train_rates': 0.9345744821580731, 'learning_rate': 0.00016007124019426443, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8377787754257334}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:05:03,836][0m Trial 24 finished with value: 0.7044362912052556 and parameters: {'observation_period_num': 148, 'train_rates': 0.9528029888072459, 'learning_rate': 1.8397600365343282e-05, 'batch_size': 112, 'step_size': 5, 'gamma': 0.8623344308061259}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:06:03,215][0m Trial 25 finished with value: 0.6515013874488504 and parameters: {'observation_period_num': 104, 'train_rates': 0.7702810109200231, 'learning_rate': 0.00010885917381136849, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9039312785067313}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:08:57,131][0m Trial 26 finished with value: 0.16650734945785167 and parameters: {'observation_period_num': 195, 'train_rates': 0.9846522768064724, 'learning_rate': 4.35336222115238e-05, 'batch_size': 36, 'step_size': 5, 'gamma': 0.7905532206365637}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:11:53,009][0m Trial 27 finished with value: 0.2208720605288233 and parameters: {'observation_period_num': 194, 'train_rates': 0.9848671706803349, 'learning_rate': 3.585788974622005e-05, 'batch_size': 36, 'step_size': 4, 'gamma': 0.7887251226806816}. Best is trial 2 with value: 0.13685504164622755.[0m
Early stopping at epoch 96
[32m[I 2025-02-02 10:14:37,924][0m Trial 28 finished with value: 1.004925483151486 and parameters: {'observation_period_num': 188, 'train_rates': 0.9449897810832398, 'learning_rate': 1.1964012520270108e-05, 'batch_size': 34, 'step_size': 2, 'gamma': 0.7507314294618549}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:18:36,501][0m Trial 29 finished with value: 0.6183293519233113 and parameters: {'observation_period_num': 156, 'train_rates': 0.7129363575081902, 'learning_rate': 3.138990773909365e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.7916821247345618}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:22:01,736][0m Trial 30 finished with value: 0.1397319734096527 and parameters: {'observation_period_num': 247, 'train_rates': 0.9840287881170484, 'learning_rate': 0.00021974937596288763, 'batch_size': 62, 'step_size': 7, 'gamma': 0.8318145100506732}. Best is trial 2 with value: 0.13685504164622755.[0m
[32m[I 2025-02-02 10:25:30,250][0m Trial 31 finished with value: 0.1231386736035347 and parameters: {'observation_period_num': 247, 'train_rates': 0.9897894713224675, 'learning_rate': 0.00025351991977514245, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8244239613542147}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:28:58,555][0m Trial 32 finished with value: 0.3775907004954385 and parameters: {'observation_period_num': 249, 'train_rates': 0.9549713873425341, 'learning_rate': 0.0002450024234440051, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8252435296457271}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:32:20,404][0m Trial 33 finished with value: 0.30638841302557424 and parameters: {'observation_period_num': 232, 'train_rates': 0.9191455924151128, 'learning_rate': 0.00022389221996851164, 'batch_size': 30, 'step_size': 7, 'gamma': 0.7933274775393577}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:34:18,723][0m Trial 34 finished with value: 0.9366004180107543 and parameters: {'observation_period_num': 200, 'train_rates': 0.6102886270399333, 'learning_rate': 0.0005028961052778484, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8299804826233405}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:37:46,568][0m Trial 35 finished with value: 1.9754005894064903 and parameters: {'observation_period_num': 234, 'train_rates': 0.9649011518778121, 'learning_rate': 0.0008644426136230841, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8491089139978681}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:40:19,627][0m Trial 36 finished with value: 0.28183919191360474 and parameters: {'observation_period_num': 200, 'train_rates': 0.9887185141025177, 'learning_rate': 4.825093465895155e-05, 'batch_size': 188, 'step_size': 8, 'gamma': 0.776897618585053}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:42:41,364][0m Trial 37 finished with value: 0.8895925988149906 and parameters: {'observation_period_num': 216, 'train_rates': 0.7353678205607734, 'learning_rate': 2.324336596583667e-05, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8026542188626147}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:46:32,650][0m Trial 38 finished with value: 0.8646077215671539 and parameters: {'observation_period_num': 184, 'train_rates': 0.9587314209210693, 'learning_rate': 1.1028234807029477e-06, 'batch_size': 25, 'step_size': 15, 'gamma': 0.8579994717037598}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:49:06,519][0m Trial 39 finished with value: 0.3400036785472384 and parameters: {'observation_period_num': 223, 'train_rates': 0.8289207169961512, 'learning_rate': 0.0003636937772071059, 'batch_size': 133, 'step_size': 12, 'gamma': 0.8078499899921899}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:51:29,125][0m Trial 40 finished with value: 0.7646482651172676 and parameters: {'observation_period_num': 237, 'train_rates': 0.6688746759246382, 'learning_rate': 0.00015007667341108624, 'batch_size': 256, 'step_size': 5, 'gamma': 0.8467783442351299}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:54:08,969][0m Trial 41 finished with value: 0.20032908022403717 and parameters: {'observation_period_num': 204, 'train_rates': 0.9896341390503649, 'learning_rate': 9.71005150599476e-05, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8335447920747515}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:57:41,250][0m Trial 42 finished with value: 0.6221109096493039 and parameters: {'observation_period_num': 252, 'train_rates': 0.9689110791806406, 'learning_rate': 0.00018480066778095715, 'batch_size': 46, 'step_size': 2, 'gamma': 0.7776112568599333}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 10:59:39,897][0m Trial 43 finished with value: 0.46672807110855913 and parameters: {'observation_period_num': 161, 'train_rates': 0.9459110140199638, 'learning_rate': 4.46875547000287e-05, 'batch_size': 76, 'step_size': 6, 'gamma': 0.8694358122341473}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:01:51,764][0m Trial 44 finished with value: 0.3509678787897142 and parameters: {'observation_period_num': 181, 'train_rates': 0.9360658995103459, 'learning_rate': 0.00032011662595604623, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8195528958575827}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:03:36,204][0m Trial 45 finished with value: 0.39775094725922044 and parameters: {'observation_period_num': 124, 'train_rates': 0.9640836827964295, 'learning_rate': 0.00011764012743871993, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8395833771444379}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:05:35,697][0m Trial 46 finished with value: 0.4249185753355694 and parameters: {'observation_period_num': 167, 'train_rates': 0.9080081144372746, 'learning_rate': 8.078847907635002e-05, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8545241084566682}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:08:23,968][0m Trial 47 finished with value: 0.5001832934526297 and parameters: {'observation_period_num': 212, 'train_rates': 0.8765914099763203, 'learning_rate': 1.7079503567779083e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8276034656038254}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:11:24,195][0m Trial 48 finished with value: 0.561168807082706 and parameters: {'observation_period_num': 225, 'train_rates': 0.9672190360898754, 'learning_rate': 2.6843486708241513e-05, 'batch_size': 67, 'step_size': 7, 'gamma': 0.8723499198687218}. Best is trial 31 with value: 0.1231386736035347.[0m
[32m[I 2025-02-02 11:12:44,028][0m Trial 49 finished with value: 0.8002026677131653 and parameters: {'observation_period_num': 84, 'train_rates': 0.9745000272575154, 'learning_rate': 0.000602914637285848, 'batch_size': 79, 'step_size': 4, 'gamma': 0.7675687994418535}. Best is trial 31 with value: 0.1231386736035347.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-02 11:12:44,038][0m A new study created in memory with name: no-name-8ffca275-1c14-4158-849c-5b7d88beecf3[0m
[32m[I 2025-02-02 11:15:46,312][0m Trial 0 finished with value: 2.0801688823699953 and parameters: {'observation_period_num': 243, 'train_rates': 0.9079117865427946, 'learning_rate': 1.4519668659866384e-06, 'batch_size': 214, 'step_size': 5, 'gamma': 0.8670637785908196}. Best is trial 0 with value: 2.0801688823699953.[0m
[32m[I 2025-02-02 11:17:02,855][0m Trial 1 finished with value: 0.8989123524063163 and parameters: {'observation_period_num': 137, 'train_rates': 0.7187794047234372, 'learning_rate': 0.000862572629304742, 'batch_size': 253, 'step_size': 10, 'gamma': 0.9630224934897558}. Best is trial 1 with value: 0.8989123524063163.[0m
[32m[I 2025-02-02 11:18:45,075][0m Trial 2 finished with value: 0.5767437743418145 and parameters: {'observation_period_num': 136, 'train_rates': 0.9416776133105089, 'learning_rate': 1.464156224985608e-05, 'batch_size': 65, 'step_size': 14, 'gamma': 0.9793041973368524}. Best is trial 2 with value: 0.5767437743418145.[0m
[32m[I 2025-02-02 11:21:03,088][0m Trial 3 finished with value: 0.8813140208684551 and parameters: {'observation_period_num': 232, 'train_rates': 0.6167099757831671, 'learning_rate': 0.00032380406291205975, 'batch_size': 60, 'step_size': 3, 'gamma': 0.76936921979815}. Best is trial 2 with value: 0.5767437743418145.[0m
[32m[I 2025-02-02 11:22:34,222][0m Trial 4 finished with value: 0.5348115563392639 and parameters: {'observation_period_num': 131, 'train_rates': 0.9667284812460106, 'learning_rate': 7.341724555337146e-05, 'batch_size': 140, 'step_size': 15, 'gamma': 0.8454483306780264}. Best is trial 4 with value: 0.5348115563392639.[0m
[32m[I 2025-02-02 11:24:39,419][0m Trial 5 finished with value: 1.0687480796080546 and parameters: {'observation_period_num': 109, 'train_rates': 0.6722665946179336, 'learning_rate': 6.348225099254504e-06, 'batch_size': 37, 'step_size': 15, 'gamma': 0.8778377044400755}. Best is trial 4 with value: 0.5348115563392639.[0m
[32m[I 2025-02-02 11:26:38,877][0m Trial 6 finished with value: 1.8479702472686768 and parameters: {'observation_period_num': 167, 'train_rates': 0.9615848710366532, 'learning_rate': 1.446586388618946e-06, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8549517905632992}. Best is trial 4 with value: 0.5348115563392639.[0m
[32m[I 2025-02-02 11:27:21,079][0m Trial 7 finished with value: 0.6953687622297279 and parameters: {'observation_period_num': 21, 'train_rates': 0.6091795791034277, 'learning_rate': 0.00022513607913877517, 'batch_size': 110, 'step_size': 5, 'gamma': 0.8361514813533971}. Best is trial 4 with value: 0.5348115563392639.[0m
Early stopping at epoch 42
[32m[I 2025-02-02 11:28:10,902][0m Trial 8 finished with value: 1.7348137393732475 and parameters: {'observation_period_num': 201, 'train_rates': 0.6482599695503911, 'learning_rate': 1.4264996554772727e-05, 'batch_size': 203, 'step_size': 1, 'gamma': 0.7584784750754644}. Best is trial 4 with value: 0.5348115563392639.[0m
[32m[I 2025-02-02 11:29:47,223][0m Trial 9 finished with value: 1.735475782575554 and parameters: {'observation_period_num': 181, 'train_rates': 0.6123401607233644, 'learning_rate': 1.132528105226878e-06, 'batch_size': 125, 'step_size': 12, 'gamma': 0.8168084356459071}. Best is trial 4 with value: 0.5348115563392639.[0m
[32m[I 2025-02-02 11:30:32,288][0m Trial 10 finished with value: 0.40769757063628287 and parameters: {'observation_period_num': 71, 'train_rates': 0.8412219640955053, 'learning_rate': 6.678970650534469e-05, 'batch_size': 170, 'step_size': 8, 'gamma': 0.9173852093262939}. Best is trial 10 with value: 0.40769757063628287.[0m
[32m[I 2025-02-02 11:31:11,856][0m Trial 11 finished with value: 0.4219666048884392 and parameters: {'observation_period_num': 62, 'train_rates': 0.8339170760601036, 'learning_rate': 9.018452300273808e-05, 'batch_size': 168, 'step_size': 8, 'gamma': 0.9176926953058321}. Best is trial 10 with value: 0.40769757063628287.[0m
[32m[I 2025-02-02 11:31:46,746][0m Trial 12 finished with value: 0.4246061266377601 and parameters: {'observation_period_num': 54, 'train_rates': 0.8416866426126334, 'learning_rate': 6.913831112665043e-05, 'batch_size': 179, 'step_size': 8, 'gamma': 0.9212894508390507}. Best is trial 10 with value: 0.40769757063628287.[0m
[32m[I 2025-02-02 11:32:29,148][0m Trial 13 finished with value: 0.5714148201113162 and parameters: {'observation_period_num': 72, 'train_rates': 0.8003407154622517, 'learning_rate': 6.752075887377544e-05, 'batch_size': 178, 'step_size': 8, 'gamma': 0.9207258950627986}. Best is trial 10 with value: 0.40769757063628287.[0m
[32m[I 2025-02-02 11:33:25,123][0m Trial 14 finished with value: 0.3018604736684299 and parameters: {'observation_period_num': 5, 'train_rates': 0.8609440938510519, 'learning_rate': 0.00028474222505684907, 'batch_size': 103, 'step_size': 10, 'gamma': 0.917879075928163}. Best is trial 14 with value: 0.3018604736684299.[0m
[32m[I 2025-02-02 11:34:20,002][0m Trial 15 finished with value: 0.29844094649121 and parameters: {'observation_period_num': 14, 'train_rates': 0.8761711765239961, 'learning_rate': 0.0002712587809849574, 'batch_size': 108, 'step_size': 10, 'gamma': 0.898823238030758}. Best is trial 15 with value: 0.29844094649121.[0m
[32m[I 2025-02-02 11:35:27,772][0m Trial 16 finished with value: 1.1016036009943835 and parameters: {'observation_period_num': 23, 'train_rates': 0.8952032828683115, 'learning_rate': 0.0009820759403438395, 'batch_size': 89, 'step_size': 11, 'gamma': 0.8907562395086224}. Best is trial 15 with value: 0.29844094649121.[0m
[32m[I 2025-02-02 11:36:22,404][0m Trial 17 finished with value: 0.8129716236505482 and parameters: {'observation_period_num': 10, 'train_rates': 0.7545580403317598, 'learning_rate': 0.00033924120123910616, 'batch_size': 93, 'step_size': 10, 'gamma': 0.9486582191852926}. Best is trial 15 with value: 0.29844094649121.[0m
[32m[I 2025-02-02 11:41:31,629][0m Trial 18 finished with value: 0.2807806475256344 and parameters: {'observation_period_num': 37, 'train_rates': 0.8825897100272316, 'learning_rate': 0.00021279844175507107, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8958352071955805}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:45:23,474][0m Trial 19 finished with value: 0.34344933648033443 and parameters: {'observation_period_num': 100, 'train_rates': 0.9119483032564517, 'learning_rate': 0.00012688307717886884, 'batch_size': 25, 'step_size': 6, 'gamma': 0.7993873859730403}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:46:58,114][0m Trial 20 finished with value: 0.787377090634484 and parameters: {'observation_period_num': 40, 'train_rates': 0.7913945087450556, 'learning_rate': 0.000553299477136069, 'batch_size': 56, 'step_size': 6, 'gamma': 0.8948258930120221}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:47:58,481][0m Trial 21 finished with value: 0.32245650533249826 and parameters: {'observation_period_num': 5, 'train_rates': 0.8688906123276349, 'learning_rate': 0.00017426460793013357, 'batch_size': 92, 'step_size': 10, 'gamma': 0.9493897769568558}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:48:48,527][0m Trial 22 finished with value: 0.42817265303929647 and parameters: {'observation_period_num': 38, 'train_rates': 0.8715159080042281, 'learning_rate': 2.7596713083041394e-05, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8924390906799705}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:50:01,436][0m Trial 23 finished with value: 0.7370380599756499 and parameters: {'observation_period_num': 93, 'train_rates': 0.8060457372649994, 'learning_rate': 0.0004837990146808279, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9408482166175245}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:55:19,340][0m Trial 24 finished with value: 0.33429847392051115 and parameters: {'observation_period_num': 37, 'train_rates': 0.921148247246223, 'learning_rate': 0.0002164596550153994, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8995371787026227}. Best is trial 18 with value: 0.2807806475256344.[0m
[32m[I 2025-02-02 11:57:40,073][0m Trial 25 finished with value: 0.13955552875995636 and parameters: {'observation_period_num': 23, 'train_rates': 0.9850317004950162, 'learning_rate': 0.00014043203375313146, 'batch_size': 44, 'step_size': 3, 'gamma': 0.8717419545969135}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:00:08,701][0m Trial 26 finished with value: 0.33088886737823486 and parameters: {'observation_period_num': 83, 'train_rates': 0.987855782764984, 'learning_rate': 3.2057105816920854e-05, 'batch_size': 41, 'step_size': 2, 'gamma': 0.8691573172999099}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:02:30,282][0m Trial 27 finished with value: 0.45375116687753925 and parameters: {'observation_period_num': 48, 'train_rates': 0.944863393962092, 'learning_rate': 0.00011459596477267718, 'batch_size': 42, 'step_size': 4, 'gamma': 0.821427534987554}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:05:07,805][0m Trial 28 finished with value: 0.5942794827356079 and parameters: {'observation_period_num': 25, 'train_rates': 0.8871393710898038, 'learning_rate': 3.701812240164825e-05, 'batch_size': 36, 'step_size': 1, 'gamma': 0.873925462462689}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:06:18,845][0m Trial 29 finished with value: 0.7745789331302309 and parameters: {'observation_period_num': 57, 'train_rates': 0.7638371286138713, 'learning_rate': 0.0005343659019914531, 'batch_size': 75, 'step_size': 4, 'gamma': 0.8498566725255802}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:10:38,830][0m Trial 30 finished with value: 0.7284518611687486 and parameters: {'observation_period_num': 112, 'train_rates': 0.9346184088827537, 'learning_rate': 4.039971652618018e-06, 'batch_size': 22, 'step_size': 7, 'gamma': 0.860199775687012}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:11:29,091][0m Trial 31 finished with value: 0.3254031633788889 and parameters: {'observation_period_num': 5, 'train_rates': 0.862117915255797, 'learning_rate': 0.0003065246109935946, 'batch_size': 111, 'step_size': 9, 'gamma': 0.9346136432636666}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:11:53,798][0m Trial 32 finished with value: 0.3707738716255203 and parameters: {'observation_period_num': 24, 'train_rates': 0.8216398834716652, 'learning_rate': 0.00015094713231391697, 'batch_size': 247, 'step_size': 11, 'gamma': 0.902072959401259}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:13:34,834][0m Trial 33 finished with value: 0.877282158021004 and parameters: {'observation_period_num': 29, 'train_rates': 0.8940846758525189, 'learning_rate': 0.0007072049756314088, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9777920210781755}. Best is trial 25 with value: 0.13955552875995636.[0m
[32m[I 2025-02-02 12:14:51,618][0m Trial 34 finished with value: 0.11595320701599121 and parameters: {'observation_period_num': 16, 'train_rates': 0.9854601338434934, 'learning_rate': 0.00032830707345255205, 'batch_size': 81, 'step_size': 9, 'gamma': 0.879491415483277}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:16:13,268][0m Trial 35 finished with value: 0.1695500761270523 and parameters: {'observation_period_num': 44, 'train_rates': 0.9860981037625043, 'learning_rate': 0.00043599670984043547, 'batch_size': 77, 'step_size': 7, 'gamma': 0.8806073636341147}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:17:34,687][0m Trial 36 finished with value: 0.14981086552143097 and parameters: {'observation_period_num': 76, 'train_rates': 0.9895191930586104, 'learning_rate': 0.00036431182089996034, 'batch_size': 76, 'step_size': 7, 'gamma': 0.8786239196143597}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:18:57,476][0m Trial 37 finished with value: 0.17833928763866425 and parameters: {'observation_period_num': 77, 'train_rates': 0.9846913841488776, 'learning_rate': 0.00042092672995337413, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8379771821574942}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:20:29,331][0m Trial 38 finished with value: 1.2208762656558643 and parameters: {'observation_period_num': 117, 'train_rates': 0.9611822616110013, 'learning_rate': 0.0007404496006620933, 'batch_size': 65, 'step_size': 5, 'gamma': 0.8815235312517222}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:22:38,409][0m Trial 39 finished with value: 0.5953072951390193 and parameters: {'observation_period_num': 153, 'train_rates': 0.9676122000643567, 'learning_rate': 0.0004122540108240149, 'batch_size': 49, 'step_size': 7, 'gamma': 0.859499999255344}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:23:26,878][0m Trial 40 finished with value: 0.6619062308939347 and parameters: {'observation_period_num': 49, 'train_rates': 0.9447049068019763, 'learning_rate': 0.0007098875985452764, 'batch_size': 129, 'step_size': 4, 'gamma': 0.8255592694199566}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:24:46,028][0m Trial 41 finished with value: 0.23904861509799957 and parameters: {'observation_period_num': 81, 'train_rates': 0.9785436091610236, 'learning_rate': 0.00040663472097342144, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8411363739922512}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:26:18,165][0m Trial 42 finished with value: 0.13275852799415588 and parameters: {'observation_period_num': 64, 'train_rates': 0.9831529193158135, 'learning_rate': 0.0001812768134557827, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8071202412149296}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:27:49,539][0m Trial 43 finished with value: 0.4674038163951186 and parameters: {'observation_period_num': 65, 'train_rates': 0.9579010732008715, 'learning_rate': 0.00010007104445365003, 'batch_size': 66, 'step_size': 9, 'gamma': 0.7955481911601558}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:28:51,662][0m Trial 44 finished with value: 0.39802918809032006 and parameters: {'observation_period_num': 97, 'train_rates': 0.9226680487887922, 'learning_rate': 0.00016963943682402383, 'batch_size': 147, 'step_size': 9, 'gamma': 0.8798113820466112}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:32:14,099][0m Trial 45 finished with value: 0.4128474295139313 and parameters: {'observation_period_num': 246, 'train_rates': 0.9859276060480485, 'learning_rate': 4.505363604120529e-05, 'batch_size': 87, 'step_size': 2, 'gamma': 0.8601365026459898}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:34:07,709][0m Trial 46 finished with value: 0.427402208479035 and parameters: {'observation_period_num': 50, 'train_rates': 0.9482190354558826, 'learning_rate': 0.00023500634295365019, 'batch_size': 52, 'step_size': 11, 'gamma': 0.7822403034253178}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:34:57,174][0m Trial 47 finished with value: 0.7667390838128711 and parameters: {'observation_period_num': 63, 'train_rates': 0.6981731771537328, 'learning_rate': 1.4136605691905973e-05, 'batch_size': 98, 'step_size': 13, 'gamma': 0.8812791439201016}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:37:34,079][0m Trial 48 finished with value: 0.5450598175012613 and parameters: {'observation_period_num': 198, 'train_rates': 0.9715651880695065, 'learning_rate': 5.5257655786276895e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.9066840485145539}. Best is trial 34 with value: 0.11595320701599121.[0m
[32m[I 2025-02-02 12:40:30,780][0m Trial 49 finished with value: 0.5466187260488966 and parameters: {'observation_period_num': 18, 'train_rates': 0.9317534027062443, 'learning_rate': 2.277074394508647e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.7566462317753243}. Best is trial 34 with value: 0.11595320701599121.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 98, 'train_rates': 0.9854045640514372, 'learning_rate': 9.472860256449676e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.827835140034455}
Epoch 1/300, trend Loss: 0.5658 | 0.5684
Epoch 2/300, trend Loss: 0.4658 | 0.4754
Epoch 3/300, trend Loss: 0.4059 | 0.3978
Epoch 4/300, trend Loss: 0.3447 | 0.3911
Epoch 5/300, trend Loss: 0.3076 | 0.3057
Epoch 6/300, trend Loss: 0.2891 | 0.2491
Epoch 7/300, trend Loss: 0.2663 | 0.2391
Epoch 8/300, trend Loss: 0.2629 | 0.2296
Epoch 9/300, trend Loss: 0.2571 | 0.2724
Epoch 10/300, trend Loss: 0.2443 | 0.2154
Epoch 11/300, trend Loss: 0.2477 | 0.2083
Epoch 12/300, trend Loss: 0.2249 | 0.2324
Epoch 13/300, trend Loss: 0.2137 | 0.2220
Epoch 14/300, trend Loss: 0.2081 | 0.2478
Epoch 15/300, trend Loss: 0.2070 | 0.2252
Epoch 16/300, trend Loss: 0.2062 | 0.2045
Epoch 17/300, trend Loss: 0.2043 | 0.1980
Epoch 18/300, trend Loss: 0.1961 | 0.1860
Epoch 19/300, trend Loss: 0.1974 | 0.2208
Epoch 20/300, trend Loss: 0.1974 | 0.1987
Epoch 21/300, trend Loss: 0.1880 | 0.1754
Epoch 22/300, trend Loss: 0.1922 | 0.1752
Epoch 23/300, trend Loss: 0.1823 | 0.1805
Epoch 24/300, trend Loss: 0.1837 | 0.2000
Epoch 25/300, trend Loss: 0.1826 | 0.1872
Epoch 26/300, trend Loss: 0.1769 | 0.1689
Epoch 27/300, trend Loss: 0.1765 | 0.1622
Epoch 28/300, trend Loss: 0.1706 | 0.1867
Epoch 29/300, trend Loss: 0.1671 | 0.1668
Epoch 30/300, trend Loss: 0.1679 | 0.1626
Epoch 31/300, trend Loss: 0.1641 | 0.1546
Epoch 32/300, trend Loss: 0.1600 | 0.1505
Epoch 33/300, trend Loss: 0.1597 | 0.1729
Epoch 34/300, trend Loss: 0.1567 | 0.1624
Epoch 35/300, trend Loss: 0.1553 | 0.1527
Epoch 36/300, trend Loss: 0.1543 | 0.1474
Epoch 37/300, trend Loss: 0.1519 | 0.1440
Epoch 38/300, trend Loss: 0.1509 | 0.1485
Epoch 39/300, trend Loss: 0.1496 | 0.1487
Epoch 40/300, trend Loss: 0.1474 | 0.1462
Epoch 41/300, trend Loss: 0.1475 | 0.1446
Epoch 42/300, trend Loss: 0.1467 | 0.1385
Epoch 43/300, trend Loss: 0.1446 | 0.1395
Epoch 44/300, trend Loss: 0.1443 | 0.1401
Epoch 45/300, trend Loss: 0.1437 | 0.1387
Epoch 46/300, trend Loss: 0.1419 | 0.1390
Epoch 47/300, trend Loss: 0.1413 | 0.1354
Epoch 48/300, trend Loss: 0.1405 | 0.1345
Epoch 49/300, trend Loss: 0.1396 | 0.1342
Epoch 50/300, trend Loss: 0.1396 | 0.1342
Epoch 51/300, trend Loss: 0.1386 | 0.1355
Epoch 52/300, trend Loss: 0.1371 | 0.1359
Epoch 53/300, trend Loss: 0.1372 | 0.1340
Epoch 54/300, trend Loss: 0.1353 | 0.1309
Epoch 55/300, trend Loss: 0.1354 | 0.1324
Epoch 56/300, trend Loss: 0.1352 | 0.1314
Epoch 57/300, trend Loss: 0.1354 | 0.1329
Epoch 58/300, trend Loss: 0.1337 | 0.1328
Epoch 59/300, trend Loss: 0.1335 | 0.1314
Epoch 60/300, trend Loss: 0.1330 | 0.1309
Epoch 61/300, trend Loss: 0.1322 | 0.1310
Epoch 62/300, trend Loss: 0.1321 | 0.1298
Epoch 63/300, trend Loss: 0.1318 | 0.1294
Epoch 64/300, trend Loss: 0.1309 | 0.1297
Epoch 65/300, trend Loss: 0.1315 | 0.1289
Epoch 66/300, trend Loss: 0.1300 | 0.1289
Epoch 67/300, trend Loss: 0.1302 | 0.1282
Epoch 68/300, trend Loss: 0.1295 | 0.1282
Epoch 69/300, trend Loss: 0.1301 | 0.1287
Epoch 70/300, trend Loss: 0.1300 | 0.1277
Epoch 71/300, trend Loss: 0.1289 | 0.1281
Epoch 72/300, trend Loss: 0.1283 | 0.1273
Epoch 73/300, trend Loss: 0.1284 | 0.1274
Epoch 74/300, trend Loss: 0.1288 | 0.1268
Epoch 75/300, trend Loss: 0.1282 | 0.1271
Epoch 76/300, trend Loss: 0.1276 | 0.1265
Epoch 77/300, trend Loss: 0.1275 | 0.1273
Epoch 78/300, trend Loss: 0.1270 | 0.1266
Epoch 79/300, trend Loss: 0.1272 | 0.1267
Epoch 80/300, trend Loss: 0.1269 | 0.1267
Epoch 81/300, trend Loss: 0.1264 | 0.1265
Epoch 82/300, trend Loss: 0.1275 | 0.1265
Epoch 83/300, trend Loss: 0.1269 | 0.1253
Epoch 84/300, trend Loss: 0.1262 | 0.1259
Epoch 85/300, trend Loss: 0.1264 | 0.1262
Epoch 86/300, trend Loss: 0.1263 | 0.1261
Epoch 87/300, trend Loss: 0.1259 | 0.1263
Epoch 88/300, trend Loss: 0.1262 | 0.1261
Epoch 89/300, trend Loss: 0.1260 | 0.1255
Epoch 90/300, trend Loss: 0.1260 | 0.1258
Epoch 91/300, trend Loss: 0.1247 | 0.1258
Epoch 92/300, trend Loss: 0.1252 | 0.1253
Epoch 93/300, trend Loss: 0.1246 | 0.1256
Epoch 94/300, trend Loss: 0.1251 | 0.1252
Epoch 95/300, trend Loss: 0.1256 | 0.1252
Epoch 96/300, trend Loss: 0.1245 | 0.1253
Epoch 97/300, trend Loss: 0.1250 | 0.1248
Epoch 98/300, trend Loss: 0.1247 | 0.1248
Epoch 99/300, trend Loss: 0.1249 | 0.1248
Epoch 100/300, trend Loss: 0.1244 | 0.1248
Epoch 101/300, trend Loss: 0.1238 | 0.1243
Epoch 102/300, trend Loss: 0.1250 | 0.1246
Epoch 103/300, trend Loss: 0.1240 | 0.1244
Epoch 104/300, trend Loss: 0.1245 | 0.1244
Epoch 105/300, trend Loss: 0.1241 | 0.1244
Epoch 106/300, trend Loss: 0.1243 | 0.1239
Epoch 107/300, trend Loss: 0.1246 | 0.1241
Epoch 108/300, trend Loss: 0.1240 | 0.1241
Epoch 109/300, trend Loss: 0.1243 | 0.1238
Epoch 110/300, trend Loss: 0.1247 | 0.1240
Epoch 111/300, trend Loss: 0.1242 | 0.1242
Epoch 112/300, trend Loss: 0.1244 | 0.1242
Epoch 113/300, trend Loss: 0.1241 | 0.1240
Epoch 114/300, trend Loss: 0.1236 | 0.1238
Epoch 115/300, trend Loss: 0.1241 | 0.1239
Epoch 116/300, trend Loss: 0.1238 | 0.1237
Epoch 117/300, trend Loss: 0.1241 | 0.1239
Epoch 118/300, trend Loss: 0.1241 | 0.1240
Epoch 119/300, trend Loss: 0.1238 | 0.1240
Epoch 120/300, trend Loss: 0.1239 | 0.1238
Epoch 121/300, trend Loss: 0.1235 | 0.1237
Epoch 122/300, trend Loss: 0.1240 | 0.1237
Epoch 123/300, trend Loss: 0.1234 | 0.1238
Epoch 124/300, trend Loss: 0.1229 | 0.1238
Epoch 125/300, trend Loss: 0.1237 | 0.1238
Epoch 126/300, trend Loss: 0.1237 | 0.1238
Epoch 127/300, trend Loss: 0.1240 | 0.1236
Epoch 128/300, trend Loss: 0.1232 | 0.1237
Epoch 129/300, trend Loss: 0.1236 | 0.1236
Epoch 130/300, trend Loss: 0.1230 | 0.1237
Epoch 131/300, trend Loss: 0.1229 | 0.1238
Epoch 132/300, trend Loss: 0.1237 | 0.1238
Epoch 133/300, trend Loss: 0.1233 | 0.1237
Epoch 134/300, trend Loss: 0.1238 | 0.1239
Epoch 135/300, trend Loss: 0.1235 | 0.1237
Epoch 136/300, trend Loss: 0.1237 | 0.1237
Epoch 137/300, trend Loss: 0.1235 | 0.1237
Epoch 138/300, trend Loss: 0.1229 | 0.1237
Epoch 139/300, trend Loss: 0.1237 | 0.1238
Epoch 140/300, trend Loss: 0.1231 | 0.1237
Epoch 141/300, trend Loss: 0.1237 | 0.1237
Epoch 142/300, trend Loss: 0.1228 | 0.1237
Epoch 143/300, trend Loss: 0.1235 | 0.1238
Epoch 144/300, trend Loss: 0.1238 | 0.1238
Epoch 145/300, trend Loss: 0.1230 | 0.1237
Epoch 146/300, trend Loss: 0.1233 | 0.1237
Epoch 147/300, trend Loss: 0.1227 | 0.1237
Epoch 148/300, trend Loss: 0.1236 | 0.1237
Epoch 149/300, trend Loss: 0.1231 | 0.1237
Epoch 150/300, trend Loss: 0.1225 | 0.1236
Epoch 151/300, trend Loss: 0.1233 | 0.1236
Epoch 152/300, trend Loss: 0.1230 | 0.1235
Epoch 153/300, trend Loss: 0.1228 | 0.1235
Epoch 154/300, trend Loss: 0.1238 | 0.1236
Epoch 155/300, trend Loss: 0.1231 | 0.1236
Epoch 156/300, trend Loss: 0.1231 | 0.1235
Epoch 157/300, trend Loss: 0.1226 | 0.1235
Epoch 158/300, trend Loss: 0.1230 | 0.1235
Epoch 159/300, trend Loss: 0.1231 | 0.1235
Epoch 160/300, trend Loss: 0.1237 | 0.1235
Epoch 161/300, trend Loss: 0.1232 | 0.1235
Epoch 162/300, trend Loss: 0.1230 | 0.1234
Epoch 163/300, trend Loss: 0.1223 | 0.1234
Epoch 164/300, trend Loss: 0.1229 | 0.1234
Epoch 165/300, trend Loss: 0.1230 | 0.1234
Epoch 166/300, trend Loss: 0.1232 | 0.1234
Epoch 167/300, trend Loss: 0.1231 | 0.1234
Epoch 168/300, trend Loss: 0.1232 | 0.1234
Epoch 169/300, trend Loss: 0.1237 | 0.1234
Epoch 170/300, trend Loss: 0.1231 | 0.1234
Epoch 171/300, trend Loss: 0.1237 | 0.1234
Epoch 172/300, trend Loss: 0.1241 | 0.1234
Epoch 173/300, trend Loss: 0.1237 | 0.1234
Epoch 174/300, trend Loss: 0.1219 | 0.1234
Epoch 175/300, trend Loss: 0.1232 | 0.1234
Epoch 176/300, trend Loss: 0.1225 | 0.1234
Epoch 177/300, trend Loss: 0.1232 | 0.1234
Epoch 178/300, trend Loss: 0.1237 | 0.1234
Epoch 179/300, trend Loss: 0.1234 | 0.1235
Epoch 180/300, trend Loss: 0.1230 | 0.1235
Epoch 181/300, trend Loss: 0.1224 | 0.1235
Epoch 182/300, trend Loss: 0.1229 | 0.1235
Epoch 183/300, trend Loss: 0.1228 | 0.1235
Epoch 184/300, trend Loss: 0.1229 | 0.1235
Epoch 185/300, trend Loss: 0.1231 | 0.1234
Epoch 186/300, trend Loss: 0.1240 | 0.1234
Epoch 187/300, trend Loss: 0.1236 | 0.1234
Epoch 188/300, trend Loss: 0.1231 | 0.1234
Epoch 189/300, trend Loss: 0.1225 | 0.1234
Epoch 190/300, trend Loss: 0.1227 | 0.1234
Epoch 191/300, trend Loss: 0.1224 | 0.1234
Epoch 192/300, trend Loss: 0.1228 | 0.1234
Epoch 193/300, trend Loss: 0.1231 | 0.1234
Epoch 194/300, trend Loss: 0.1228 | 0.1234
Epoch 195/300, trend Loss: 0.1227 | 0.1234
Epoch 196/300, trend Loss: 0.1229 | 0.1234
Epoch 197/300, trend Loss: 0.1230 | 0.1234
Epoch 198/300, trend Loss: 0.1232 | 0.1234
Epoch 199/300, trend Loss: 0.1229 | 0.1234
Epoch 200/300, trend Loss: 0.1225 | 0.1234
Epoch 201/300, trend Loss: 0.1229 | 0.1234
Epoch 202/300, trend Loss: 0.1226 | 0.1234
Epoch 203/300, trend Loss: 0.1224 | 0.1234
Epoch 204/300, trend Loss: 0.1229 | 0.1234
Epoch 205/300, trend Loss: 0.1226 | 0.1234
Epoch 206/300, trend Loss: 0.1229 | 0.1234
Epoch 207/300, trend Loss: 0.1226 | 0.1234
Epoch 208/300, trend Loss: 0.1228 | 0.1234
Epoch 209/300, trend Loss: 0.1234 | 0.1234
Epoch 210/300, trend Loss: 0.1227 | 0.1234
Epoch 211/300, trend Loss: 0.1245 | 0.1234
Epoch 212/300, trend Loss: 0.1222 | 0.1234
Epoch 213/300, trend Loss: 0.1231 | 0.1234
Epoch 214/300, trend Loss: 0.1222 | 0.1234
Epoch 215/300, trend Loss: 0.1229 | 0.1234
Epoch 216/300, trend Loss: 0.1231 | 0.1234
Epoch 217/300, trend Loss: 0.1238 | 0.1234
Epoch 218/300, trend Loss: 0.1228 | 0.1234
Epoch 219/300, trend Loss: 0.1225 | 0.1234
Epoch 220/300, trend Loss: 0.1226 | 0.1234
Epoch 221/300, trend Loss: 0.1233 | 0.1234
Epoch 222/300, trend Loss: 0.1232 | 0.1234
Epoch 223/300, trend Loss: 0.1233 | 0.1234
Epoch 224/300, trend Loss: 0.1229 | 0.1234
Epoch 225/300, trend Loss: 0.1225 | 0.1234
Epoch 226/300, trend Loss: 0.1235 | 0.1234
Epoch 227/300, trend Loss: 0.1231 | 0.1234
Epoch 228/300, trend Loss: 0.1230 | 0.1234
Epoch 229/300, trend Loss: 0.1224 | 0.1234
Epoch 230/300, trend Loss: 0.1231 | 0.1234
Epoch 231/300, trend Loss: 0.1229 | 0.1234
Epoch 232/300, trend Loss: 0.1227 | 0.1234
Epoch 233/300, trend Loss: 0.1228 | 0.1234
Epoch 234/300, trend Loss: 0.1224 | 0.1234
Epoch 235/300, trend Loss: 0.1222 | 0.1234
Epoch 236/300, trend Loss: 0.1226 | 0.1234
Epoch 237/300, trend Loss: 0.1231 | 0.1234
Epoch 238/300, trend Loss: 0.1230 | 0.1234
Epoch 239/300, trend Loss: 0.1224 | 0.1234
Epoch 240/300, trend Loss: 0.1237 | 0.1234
Epoch 241/300, trend Loss: 0.1235 | 0.1234
Epoch 242/300, trend Loss: 0.1222 | 0.1234
Epoch 243/300, trend Loss: 0.1231 | 0.1234
Epoch 244/300, trend Loss: 0.1233 | 0.1234
Epoch 245/300, trend Loss: 0.1225 | 0.1234
Epoch 246/300, trend Loss: 0.1227 | 0.1234
Epoch 247/300, trend Loss: 0.1236 | 0.1234
Epoch 248/300, trend Loss: 0.1230 | 0.1234
Epoch 249/300, trend Loss: 0.1234 | 0.1234
Epoch 250/300, trend Loss: 0.1232 | 0.1234
Epoch 251/300, trend Loss: 0.1225 | 0.1234
Epoch 252/300, trend Loss: 0.1240 | 0.1234
Epoch 253/300, trend Loss: 0.1229 | 0.1234
Epoch 254/300, trend Loss: 0.1222 | 0.1234
Epoch 255/300, trend Loss: 0.1225 | 0.1234
Epoch 256/300, trend Loss: 0.1229 | 0.1234
Epoch 257/300, trend Loss: 0.1220 | 0.1234
Epoch 258/300, trend Loss: 0.1230 | 0.1234
Epoch 259/300, trend Loss: 0.1230 | 0.1234
Epoch 260/300, trend Loss: 0.1225 | 0.1234
Epoch 261/300, trend Loss: 0.1230 | 0.1234
Epoch 262/300, trend Loss: 0.1225 | 0.1234
Epoch 263/300, trend Loss: 0.1229 | 0.1234
Epoch 264/300, trend Loss: 0.1233 | 0.1234
Epoch 265/300, trend Loss: 0.1231 | 0.1234
Epoch 266/300, trend Loss: 0.1227 | 0.1234
Epoch 267/300, trend Loss: 0.1230 | 0.1234
Epoch 268/300, trend Loss: 0.1225 | 0.1234
Epoch 269/300, trend Loss: 0.1234 | 0.1234
Epoch 270/300, trend Loss: 0.1234 | 0.1234
Epoch 271/300, trend Loss: 0.1231 | 0.1234
Epoch 272/300, trend Loss: 0.1229 | 0.1234
Epoch 273/300, trend Loss: 0.1222 | 0.1234
Epoch 274/300, trend Loss: 0.1230 | 0.1234
Epoch 275/300, trend Loss: 0.1230 | 0.1234
Epoch 276/300, trend Loss: 0.1230 | 0.1234
Epoch 277/300, trend Loss: 0.1233 | 0.1234
Epoch 278/300, trend Loss: 0.1230 | 0.1234
Epoch 279/300, trend Loss: 0.1232 | 0.1234
Epoch 280/300, trend Loss: 0.1232 | 0.1234
Epoch 281/300, trend Loss: 0.1226 | 0.1234
Epoch 282/300, trend Loss: 0.1226 | 0.1234
Epoch 283/300, trend Loss: 0.1225 | 0.1234
Epoch 284/300, trend Loss: 0.1233 | 0.1234
Epoch 285/300, trend Loss: 0.1229 | 0.1234
Epoch 286/300, trend Loss: 0.1226 | 0.1234
Epoch 287/300, trend Loss: 0.1231 | 0.1234
Epoch 288/300, trend Loss: 0.1222 | 0.1234
Epoch 289/300, trend Loss: 0.1235 | 0.1234
Epoch 290/300, trend Loss: 0.1232 | 0.1234
Epoch 291/300, trend Loss: 0.1224 | 0.1234
Epoch 292/300, trend Loss: 0.1230 | 0.1234
Epoch 293/300, trend Loss: 0.1224 | 0.1234
Epoch 294/300, trend Loss: 0.1227 | 0.1234
Epoch 295/300, trend Loss: 0.1232 | 0.1234
Epoch 296/300, trend Loss: 0.1237 | 0.1234
Epoch 297/300, trend Loss: 0.1229 | 0.1234
Epoch 298/300, trend Loss: 0.1224 | 0.1234
Epoch 299/300, trend Loss: 0.1227 | 0.1234
Epoch 300/300, trend Loss: 0.1235 | 0.1234
Training seasonal_0 component with params: {'observation_period_num': 181, 'train_rates': 0.884207684746761, 'learning_rate': 0.0001972289459885028, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8120558317314526}
Epoch 1/300, seasonal_0 Loss: 0.9352 | 1.3275
Epoch 2/300, seasonal_0 Loss: 0.7039 | 0.9519
Epoch 3/300, seasonal_0 Loss: 0.6220 | 0.7783
Epoch 4/300, seasonal_0 Loss: 0.5480 | 0.7431
Epoch 5/300, seasonal_0 Loss: 0.5037 | 0.6741
Epoch 6/300, seasonal_0 Loss: 0.4690 | 0.5883
Epoch 7/300, seasonal_0 Loss: 0.5360 | 0.7479
Epoch 8/300, seasonal_0 Loss: 0.4554 | 0.6279
Epoch 9/300, seasonal_0 Loss: 0.6853 | 0.5650
Epoch 10/300, seasonal_0 Loss: 0.4671 | 0.6138
Epoch 11/300, seasonal_0 Loss: 0.3923 | 0.5541
Epoch 12/300, seasonal_0 Loss: 0.3616 | 0.5378
Epoch 13/300, seasonal_0 Loss: 0.3862 | 0.6061
Epoch 14/300, seasonal_0 Loss: 0.3614 | 0.5074
Epoch 15/300, seasonal_0 Loss: 0.3029 | 0.4858
Epoch 16/300, seasonal_0 Loss: 0.2886 | 0.4714
Epoch 17/300, seasonal_0 Loss: 0.2727 | 0.4634
Epoch 18/300, seasonal_0 Loss: 0.2669 | 0.4541
Epoch 19/300, seasonal_0 Loss: 0.2579 | 0.4393
Epoch 20/300, seasonal_0 Loss: 0.2551 | 0.4445
Epoch 21/300, seasonal_0 Loss: 0.2565 | 0.4269
Epoch 22/300, seasonal_0 Loss: 0.2456 | 0.4249
Epoch 23/300, seasonal_0 Loss: 0.2413 | 0.4185
Epoch 24/300, seasonal_0 Loss: 0.2325 | 0.4071
Epoch 25/300, seasonal_0 Loss: 0.2291 | 0.4038
Epoch 26/300, seasonal_0 Loss: 0.2288 | 0.3991
Epoch 27/300, seasonal_0 Loss: 0.2260 | 0.3946
Epoch 28/300, seasonal_0 Loss: 0.2219 | 0.3970
Epoch 29/300, seasonal_0 Loss: 0.2177 | 0.3853
Epoch 30/300, seasonal_0 Loss: 0.2186 | 0.3906
Epoch 31/300, seasonal_0 Loss: 0.2242 | 0.3818
Epoch 32/300, seasonal_0 Loss: 0.2176 | 0.3813
Epoch 33/300, seasonal_0 Loss: 0.2144 | 0.3814
Epoch 34/300, seasonal_0 Loss: 0.2124 | 0.3720
Epoch 35/300, seasonal_0 Loss: 0.2108 | 0.3737
Epoch 36/300, seasonal_0 Loss: 0.2106 | 0.3698
Epoch 37/300, seasonal_0 Loss: 0.2040 | 0.3700
Epoch 38/300, seasonal_0 Loss: 0.2016 | 0.3670
Epoch 39/300, seasonal_0 Loss: 0.1993 | 0.3647
Epoch 40/300, seasonal_0 Loss: 0.1977 | 0.3635
Epoch 41/300, seasonal_0 Loss: 0.1973 | 0.3596
Epoch 42/300, seasonal_0 Loss: 0.1949 | 0.3591
Epoch 43/300, seasonal_0 Loss: 0.1937 | 0.3569
Epoch 44/300, seasonal_0 Loss: 0.1924 | 0.3550
Epoch 45/300, seasonal_0 Loss: 0.1918 | 0.3540
Epoch 46/300, seasonal_0 Loss: 0.1905 | 0.3518
Epoch 47/300, seasonal_0 Loss: 0.1894 | 0.3517
Epoch 48/300, seasonal_0 Loss: 0.1879 | 0.3504
Epoch 49/300, seasonal_0 Loss: 0.1875 | 0.3499
Epoch 50/300, seasonal_0 Loss: 0.1866 | 0.3482
Epoch 51/300, seasonal_0 Loss: 0.1852 | 0.3463
Epoch 52/300, seasonal_0 Loss: 0.1853 | 0.3451
Epoch 53/300, seasonal_0 Loss: 0.1836 | 0.3441
Epoch 54/300, seasonal_0 Loss: 0.1832 | 0.3427
Epoch 55/300, seasonal_0 Loss: 0.1831 | 0.3436
Epoch 56/300, seasonal_0 Loss: 0.1827 | 0.3414
Epoch 57/300, seasonal_0 Loss: 0.1820 | 0.3403
Epoch 58/300, seasonal_0 Loss: 0.1813 | 0.3402
Epoch 59/300, seasonal_0 Loss: 0.1807 | 0.3390
Epoch 60/300, seasonal_0 Loss: 0.1803 | 0.3380
Epoch 61/300, seasonal_0 Loss: 0.1785 | 0.3375
Epoch 62/300, seasonal_0 Loss: 0.1789 | 0.3384
Epoch 63/300, seasonal_0 Loss: 0.1782 | 0.3357
Epoch 64/300, seasonal_0 Loss: 0.1775 | 0.3364
Epoch 65/300, seasonal_0 Loss: 0.1775 | 0.3359
Epoch 66/300, seasonal_0 Loss: 0.1762 | 0.3350
Epoch 67/300, seasonal_0 Loss: 0.1758 | 0.3342
Epoch 68/300, seasonal_0 Loss: 0.1762 | 0.3331
Epoch 69/300, seasonal_0 Loss: 0.1760 | 0.3335
Epoch 70/300, seasonal_0 Loss: 0.1749 | 0.3336
Epoch 71/300, seasonal_0 Loss: 0.1747 | 0.3321
Epoch 72/300, seasonal_0 Loss: 0.1740 | 0.3314
Epoch 73/300, seasonal_0 Loss: 0.1735 | 0.3309
Epoch 74/300, seasonal_0 Loss: 0.1738 | 0.3304
Epoch 75/300, seasonal_0 Loss: 0.1729 | 0.3310
Epoch 76/300, seasonal_0 Loss: 0.1730 | 0.3299
Epoch 77/300, seasonal_0 Loss: 0.1717 | 0.3298
Epoch 78/300, seasonal_0 Loss: 0.1719 | 0.3296
Epoch 79/300, seasonal_0 Loss: 0.1718 | 0.3287
Epoch 80/300, seasonal_0 Loss: 0.1714 | 0.3285
Epoch 81/300, seasonal_0 Loss: 0.1709 | 0.3279
Epoch 82/300, seasonal_0 Loss: 0.1701 | 0.3287
Epoch 83/300, seasonal_0 Loss: 0.1708 | 0.3269
Epoch 84/300, seasonal_0 Loss: 0.1715 | 0.3274
Epoch 85/300, seasonal_0 Loss: 0.1698 | 0.3263
Epoch 86/300, seasonal_0 Loss: 0.1702 | 0.3260
Epoch 87/300, seasonal_0 Loss: 0.1695 | 0.3255
Epoch 88/300, seasonal_0 Loss: 0.1692 | 0.3245
Epoch 89/300, seasonal_0 Loss: 0.1691 | 0.3240
Epoch 90/300, seasonal_0 Loss: 0.1698 | 0.3248
Epoch 91/300, seasonal_0 Loss: 0.1678 | 0.3246
Epoch 92/300, seasonal_0 Loss: 0.1684 | 0.3253
Epoch 93/300, seasonal_0 Loss: 0.1679 | 0.3254
Epoch 94/300, seasonal_0 Loss: 0.1675 | 0.3238
Epoch 95/300, seasonal_0 Loss: 0.1681 | 0.3245
Epoch 96/300, seasonal_0 Loss: 0.1674 | 0.3235
Epoch 97/300, seasonal_0 Loss: 0.1681 | 0.3243
Epoch 98/300, seasonal_0 Loss: 0.1681 | 0.3224
Epoch 99/300, seasonal_0 Loss: 0.1667 | 0.3226
Epoch 100/300, seasonal_0 Loss: 0.1671 | 0.3237
Epoch 101/300, seasonal_0 Loss: 0.1671 | 0.3229
Epoch 102/300, seasonal_0 Loss: 0.1664 | 0.3220
Epoch 103/300, seasonal_0 Loss: 0.1664 | 0.3220
Epoch 104/300, seasonal_0 Loss: 0.1665 | 0.3220
Epoch 105/300, seasonal_0 Loss: 0.1657 | 0.3219
Epoch 106/300, seasonal_0 Loss: 0.1660 | 0.3216
Epoch 107/300, seasonal_0 Loss: 0.1658 | 0.3210
Epoch 108/300, seasonal_0 Loss: 0.1660 | 0.3212
Epoch 109/300, seasonal_0 Loss: 0.1650 | 0.3209
Epoch 110/300, seasonal_0 Loss: 0.1648 | 0.3209
Epoch 111/300, seasonal_0 Loss: 0.1652 | 0.3204
Epoch 112/300, seasonal_0 Loss: 0.1652 | 0.3207
Epoch 113/300, seasonal_0 Loss: 0.1651 | 0.3204
Epoch 114/300, seasonal_0 Loss: 0.1649 | 0.3201
Epoch 115/300, seasonal_0 Loss: 0.1646 | 0.3198
Epoch 116/300, seasonal_0 Loss: 0.1650 | 0.3195
Epoch 117/300, seasonal_0 Loss: 0.1644 | 0.3202
Epoch 118/300, seasonal_0 Loss: 0.1645 | 0.3198
Epoch 119/300, seasonal_0 Loss: 0.1641 | 0.3200
Epoch 120/300, seasonal_0 Loss: 0.1639 | 0.3204
Epoch 121/300, seasonal_0 Loss: 0.1634 | 0.3205
Epoch 122/300, seasonal_0 Loss: 0.1643 | 0.3203
Epoch 123/300, seasonal_0 Loss: 0.1634 | 0.3199
Epoch 124/300, seasonal_0 Loss: 0.1641 | 0.3194
Epoch 125/300, seasonal_0 Loss: 0.1644 | 0.3191
Epoch 126/300, seasonal_0 Loss: 0.1638 | 0.3186
Epoch 127/300, seasonal_0 Loss: 0.1635 | 0.3193
Epoch 128/300, seasonal_0 Loss: 0.1638 | 0.3195
Epoch 129/300, seasonal_0 Loss: 0.1634 | 0.3190
Epoch 130/300, seasonal_0 Loss: 0.1634 | 0.3193
Epoch 131/300, seasonal_0 Loss: 0.1637 | 0.3189
Epoch 132/300, seasonal_0 Loss: 0.1638 | 0.3188
Epoch 133/300, seasonal_0 Loss: 0.1628 | 0.3187
Epoch 134/300, seasonal_0 Loss: 0.1629 | 0.3188
Epoch 135/300, seasonal_0 Loss: 0.1628 | 0.3190
Epoch 136/300, seasonal_0 Loss: 0.1631 | 0.3196
Epoch 137/300, seasonal_0 Loss: 0.1629 | 0.3194
Epoch 138/300, seasonal_0 Loss: 0.1627 | 0.3191
Epoch 139/300, seasonal_0 Loss: 0.1636 | 0.3190
Epoch 140/300, seasonal_0 Loss: 0.1625 | 0.3189
Epoch 141/300, seasonal_0 Loss: 0.1631 | 0.3191
Epoch 142/300, seasonal_0 Loss: 0.1621 | 0.3184
Epoch 143/300, seasonal_0 Loss: 0.1626 | 0.3180
Epoch 144/300, seasonal_0 Loss: 0.1625 | 0.3178
Epoch 145/300, seasonal_0 Loss: 0.1621 | 0.3179
Epoch 146/300, seasonal_0 Loss: 0.1631 | 0.3177
Epoch 147/300, seasonal_0 Loss: 0.1624 | 0.3180
Epoch 148/300, seasonal_0 Loss: 0.1628 | 0.3183
Epoch 149/300, seasonal_0 Loss: 0.1624 | 0.3182
Epoch 150/300, seasonal_0 Loss: 0.1621 | 0.3179
Epoch 151/300, seasonal_0 Loss: 0.1618 | 0.3179
Epoch 152/300, seasonal_0 Loss: 0.1617 | 0.3180
Epoch 153/300, seasonal_0 Loss: 0.1618 | 0.3183
Epoch 154/300, seasonal_0 Loss: 0.1616 | 0.3182
Epoch 155/300, seasonal_0 Loss: 0.1619 | 0.3178
Epoch 156/300, seasonal_0 Loss: 0.1611 | 0.3179
Epoch 157/300, seasonal_0 Loss: 0.1609 | 0.3180
Epoch 158/300, seasonal_0 Loss: 0.1617 | 0.3178
Epoch 159/300, seasonal_0 Loss: 0.1620 | 0.3176
Epoch 160/300, seasonal_0 Loss: 0.1623 | 0.3177
Epoch 161/300, seasonal_0 Loss: 0.1614 | 0.3178
Epoch 162/300, seasonal_0 Loss: 0.1619 | 0.3182
Epoch 163/300, seasonal_0 Loss: 0.1613 | 0.3178
Epoch 164/300, seasonal_0 Loss: 0.1617 | 0.3176
Epoch 165/300, seasonal_0 Loss: 0.1627 | 0.3176
Epoch 166/300, seasonal_0 Loss: 0.1608 | 0.3178
Epoch 167/300, seasonal_0 Loss: 0.1611 | 0.3179
Epoch 168/300, seasonal_0 Loss: 0.1614 | 0.3179
Epoch 169/300, seasonal_0 Loss: 0.1624 | 0.3179
Epoch 170/300, seasonal_0 Loss: 0.1614 | 0.3178
Epoch 171/300, seasonal_0 Loss: 0.1614 | 0.3178
Epoch 172/300, seasonal_0 Loss: 0.1617 | 0.3178
Epoch 173/300, seasonal_0 Loss: 0.1615 | 0.3177
Epoch 174/300, seasonal_0 Loss: 0.1613 | 0.3176
Epoch 175/300, seasonal_0 Loss: 0.1616 | 0.3176
Epoch 176/300, seasonal_0 Loss: 0.1614 | 0.3175
Epoch 177/300, seasonal_0 Loss: 0.1614 | 0.3175
Epoch 178/300, seasonal_0 Loss: 0.1606 | 0.3174
Epoch 179/300, seasonal_0 Loss: 0.1620 | 0.3175
Epoch 180/300, seasonal_0 Loss: 0.1618 | 0.3174
Epoch 181/300, seasonal_0 Loss: 0.1613 | 0.3173
Epoch 182/300, seasonal_0 Loss: 0.1605 | 0.3173
Epoch 183/300, seasonal_0 Loss: 0.1605 | 0.3173
Epoch 184/300, seasonal_0 Loss: 0.1611 | 0.3173
Epoch 185/300, seasonal_0 Loss: 0.1617 | 0.3173
Epoch 186/300, seasonal_0 Loss: 0.1615 | 0.3174
Epoch 187/300, seasonal_0 Loss: 0.1619 | 0.3174
Epoch 188/300, seasonal_0 Loss: 0.1608 | 0.3174
Epoch 189/300, seasonal_0 Loss: 0.1610 | 0.3174
Epoch 190/300, seasonal_0 Loss: 0.1615 | 0.3174
Epoch 191/300, seasonal_0 Loss: 0.1620 | 0.3174
Epoch 192/300, seasonal_0 Loss: 0.1613 | 0.3173
Epoch 193/300, seasonal_0 Loss: 0.1609 | 0.3172
Epoch 194/300, seasonal_0 Loss: 0.1611 | 0.3172
Epoch 195/300, seasonal_0 Loss: 0.1606 | 0.3172
Epoch 196/300, seasonal_0 Loss: 0.1613 | 0.3171
Epoch 197/300, seasonal_0 Loss: 0.1610 | 0.3171
Epoch 198/300, seasonal_0 Loss: 0.1606 | 0.3171
Epoch 199/300, seasonal_0 Loss: 0.1602 | 0.3171
Epoch 200/300, seasonal_0 Loss: 0.1608 | 0.3171
Epoch 201/300, seasonal_0 Loss: 0.1610 | 0.3171
Epoch 202/300, seasonal_0 Loss: 0.1613 | 0.3170
Epoch 203/300, seasonal_0 Loss: 0.1614 | 0.3169
Epoch 204/300, seasonal_0 Loss: 0.1606 | 0.3169
Epoch 205/300, seasonal_0 Loss: 0.1611 | 0.3169
Epoch 206/300, seasonal_0 Loss: 0.1611 | 0.3169
Epoch 207/300, seasonal_0 Loss: 0.1610 | 0.3169
Epoch 208/300, seasonal_0 Loss: 0.1606 | 0.3169
Epoch 209/300, seasonal_0 Loss: 0.1616 | 0.3169
Epoch 210/300, seasonal_0 Loss: 0.1611 | 0.3169
Epoch 211/300, seasonal_0 Loss: 0.1605 | 0.3169
Epoch 212/300, seasonal_0 Loss: 0.1609 | 0.3169
Epoch 213/300, seasonal_0 Loss: 0.1608 | 0.3169
Epoch 214/300, seasonal_0 Loss: 0.1611 | 0.3169
Epoch 215/300, seasonal_0 Loss: 0.1613 | 0.3168
Epoch 216/300, seasonal_0 Loss: 0.1607 | 0.3168
Epoch 217/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 218/300, seasonal_0 Loss: 0.1615 | 0.3168
Epoch 219/300, seasonal_0 Loss: 0.1614 | 0.3168
Epoch 220/300, seasonal_0 Loss: 0.1603 | 0.3168
Epoch 221/300, seasonal_0 Loss: 0.1603 | 0.3168
Epoch 222/300, seasonal_0 Loss: 0.1611 | 0.3168
Epoch 223/300, seasonal_0 Loss: 0.1612 | 0.3168
Epoch 224/300, seasonal_0 Loss: 0.1611 | 0.3168
Epoch 225/300, seasonal_0 Loss: 0.1607 | 0.3168
Epoch 226/300, seasonal_0 Loss: 0.1610 | 0.3168
Epoch 227/300, seasonal_0 Loss: 0.1610 | 0.3169
Epoch 228/300, seasonal_0 Loss: 0.1609 | 0.3169
Epoch 229/300, seasonal_0 Loss: 0.1603 | 0.3169
Epoch 230/300, seasonal_0 Loss: 0.1607 | 0.3169
Epoch 231/300, seasonal_0 Loss: 0.1606 | 0.3168
Epoch 232/300, seasonal_0 Loss: 0.1605 | 0.3168
Epoch 233/300, seasonal_0 Loss: 0.1612 | 0.3168
Epoch 234/300, seasonal_0 Loss: 0.1601 | 0.3168
Epoch 235/300, seasonal_0 Loss: 0.1611 | 0.3168
Epoch 236/300, seasonal_0 Loss: 0.1604 | 0.3168
Epoch 237/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 238/300, seasonal_0 Loss: 0.1608 | 0.3168
Epoch 239/300, seasonal_0 Loss: 0.1612 | 0.3168
Epoch 240/300, seasonal_0 Loss: 0.1612 | 0.3168
Epoch 241/300, seasonal_0 Loss: 0.1600 | 0.3168
Epoch 242/300, seasonal_0 Loss: 0.1612 | 0.3168
Epoch 243/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 244/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 245/300, seasonal_0 Loss: 0.1615 | 0.3168
Epoch 246/300, seasonal_0 Loss: 0.1607 | 0.3168
Epoch 247/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 248/300, seasonal_0 Loss: 0.1604 | 0.3168
Epoch 249/300, seasonal_0 Loss: 0.1605 | 0.3168
Epoch 250/300, seasonal_0 Loss: 0.1604 | 0.3168
Epoch 251/300, seasonal_0 Loss: 0.1608 | 0.3168
Epoch 252/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 253/300, seasonal_0 Loss: 0.1607 | 0.3168
Epoch 254/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 255/300, seasonal_0 Loss: 0.1609 | 0.3168
Epoch 256/300, seasonal_0 Loss: 0.1604 | 0.3168
Epoch 257/300, seasonal_0 Loss: 0.1606 | 0.3168
Epoch 258/300, seasonal_0 Loss: 0.1612 | 0.3167
Epoch 259/300, seasonal_0 Loss: 0.1611 | 0.3167
Epoch 260/300, seasonal_0 Loss: 0.1607 | 0.3167
Epoch 261/300, seasonal_0 Loss: 0.1603 | 0.3167
Epoch 262/300, seasonal_0 Loss: 0.1611 | 0.3167
Epoch 263/300, seasonal_0 Loss: 0.1611 | 0.3167
Epoch 264/300, seasonal_0 Loss: 0.1606 | 0.3167
Epoch 265/300, seasonal_0 Loss: 0.1601 | 0.3167
Epoch 266/300, seasonal_0 Loss: 0.1606 | 0.3167
Epoch 267/300, seasonal_0 Loss: 0.1608 | 0.3167
Epoch 268/300, seasonal_0 Loss: 0.1604 | 0.3167
Epoch 269/300, seasonal_0 Loss: 0.1621 | 0.3167
Epoch 270/300, seasonal_0 Loss: 0.1609 | 0.3167
Epoch 271/300, seasonal_0 Loss: 0.1607 | 0.3167
Epoch 272/300, seasonal_0 Loss: 0.1604 | 0.3166
Epoch 273/300, seasonal_0 Loss: 0.1607 | 0.3166
Epoch 274/300, seasonal_0 Loss: 0.1608 | 0.3166
Epoch 275/300, seasonal_0 Loss: 0.1607 | 0.3166
Epoch 276/300, seasonal_0 Loss: 0.1608 | 0.3167
Epoch 277/300, seasonal_0 Loss: 0.1601 | 0.3167
Epoch 278/300, seasonal_0 Loss: 0.1613 | 0.3167
Epoch 279/300, seasonal_0 Loss: 0.1614 | 0.3167
Epoch 280/300, seasonal_0 Loss: 0.1601 | 0.3167
Epoch 281/300, seasonal_0 Loss: 0.1604 | 0.3167
Epoch 282/300, seasonal_0 Loss: 0.1608 | 0.3167
Epoch 283/300, seasonal_0 Loss: 0.1607 | 0.3167
Epoch 284/300, seasonal_0 Loss: 0.1609 | 0.3167
Epoch 285/300, seasonal_0 Loss: 0.1604 | 0.3167
Epoch 286/300, seasonal_0 Loss: 0.1608 | 0.3167
Epoch 287/300, seasonal_0 Loss: 0.1603 | 0.3167
Epoch 288/300, seasonal_0 Loss: 0.1604 | 0.3167
Epoch 289/300, seasonal_0 Loss: 0.1615 | 0.3167
Epoch 290/300, seasonal_0 Loss: 0.1608 | 0.3167
Epoch 291/300, seasonal_0 Loss: 0.1600 | 0.3167
Epoch 292/300, seasonal_0 Loss: 0.1610 | 0.3167
Epoch 293/300, seasonal_0 Loss: 0.1617 | 0.3167
Epoch 294/300, seasonal_0 Loss: 0.1609 | 0.3167
Epoch 295/300, seasonal_0 Loss: 0.1609 | 0.3167
Epoch 296/300, seasonal_0 Loss: 0.1600 | 0.3167
Epoch 297/300, seasonal_0 Loss: 0.1603 | 0.3167
Epoch 298/300, seasonal_0 Loss: 0.1607 | 0.3167
Epoch 299/300, seasonal_0 Loss: 0.1605 | 0.3167
Epoch 300/300, seasonal_0 Loss: 0.1607 | 0.3167
Training seasonal_1 component with params: {'observation_period_num': 145, 'train_rates': 0.9896491265874469, 'learning_rate': 0.00015031019279272868, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9423980468281211}
Epoch 1/300, seasonal_1 Loss: 1.2456 | 1.3953
Epoch 2/300, seasonal_1 Loss: 0.8178 | 0.8419
Epoch 3/300, seasonal_1 Loss: 0.6832 | 0.7146
Epoch 4/300, seasonal_1 Loss: 0.5986 | 0.6243
Epoch 5/300, seasonal_1 Loss: 0.5566 | 0.5802
Epoch 6/300, seasonal_1 Loss: 0.4978 | 0.5305
Epoch 7/300, seasonal_1 Loss: 0.5556 | 0.5268
Epoch 8/300, seasonal_1 Loss: 0.4694 | 0.4906
Epoch 9/300, seasonal_1 Loss: 0.5471 | 0.4776
Epoch 10/300, seasonal_1 Loss: 0.4522 | 0.4635
Epoch 11/300, seasonal_1 Loss: 0.4096 | 0.4273
Epoch 12/300, seasonal_1 Loss: 0.3713 | 0.4107
Epoch 13/300, seasonal_1 Loss: 0.3446 | 0.3846
Epoch 14/300, seasonal_1 Loss: 0.3460 | 0.3678
Epoch 15/300, seasonal_1 Loss: 0.3269 | 0.3564
Epoch 16/300, seasonal_1 Loss: 0.3279 | 0.3410
Epoch 17/300, seasonal_1 Loss: 0.3033 | 0.3331
Epoch 18/300, seasonal_1 Loss: 0.2942 | 0.3194
Epoch 19/300, seasonal_1 Loss: 0.3024 | 0.3090
Epoch 20/300, seasonal_1 Loss: 0.2885 | 0.3020
Epoch 21/300, seasonal_1 Loss: 0.2967 | 0.2912
Epoch 22/300, seasonal_1 Loss: 0.2792 | 0.2902
Epoch 23/300, seasonal_1 Loss: 0.2807 | 0.2809
Epoch 24/300, seasonal_1 Loss: 0.2871 | 0.2781
Epoch 25/300, seasonal_1 Loss: 0.2713 | 0.2701
Epoch 26/300, seasonal_1 Loss: 0.2685 | 0.2625
Epoch 27/300, seasonal_1 Loss: 0.2576 | 0.2588
Epoch 28/300, seasonal_1 Loss: 0.2534 | 0.2532
Epoch 29/300, seasonal_1 Loss: 0.2571 | 0.2465
Epoch 30/300, seasonal_1 Loss: 0.2443 | 0.2475
Epoch 31/300, seasonal_1 Loss: 0.2406 | 0.2377
Epoch 32/300, seasonal_1 Loss: 0.2364 | 0.2384
Epoch 33/300, seasonal_1 Loss: 0.2342 | 0.2336
Epoch 34/300, seasonal_1 Loss: 0.2331 | 0.2296
Epoch 35/300, seasonal_1 Loss: 0.2289 | 0.2283
Epoch 36/300, seasonal_1 Loss: 0.2266 | 0.2233
Epoch 37/300, seasonal_1 Loss: 0.2238 | 0.2221
Epoch 38/300, seasonal_1 Loss: 0.2210 | 0.2181
Epoch 39/300, seasonal_1 Loss: 0.2198 | 0.2159
Epoch 40/300, seasonal_1 Loss: 0.2185 | 0.2140
Epoch 41/300, seasonal_1 Loss: 0.2163 | 0.2117
Epoch 42/300, seasonal_1 Loss: 0.2158 | 0.2099
Epoch 43/300, seasonal_1 Loss: 0.2132 | 0.2080
Epoch 44/300, seasonal_1 Loss: 0.2126 | 0.2045
Epoch 45/300, seasonal_1 Loss: 0.2104 | 0.2032
Epoch 46/300, seasonal_1 Loss: 0.2096 | 0.2015
Epoch 47/300, seasonal_1 Loss: 0.2084 | 0.1997
Epoch 48/300, seasonal_1 Loss: 0.2076 | 0.1982
Epoch 49/300, seasonal_1 Loss: 0.2070 | 0.1966
Epoch 50/300, seasonal_1 Loss: 0.2055 | 0.1960
Epoch 51/300, seasonal_1 Loss: 0.2045 | 0.1935
Epoch 52/300, seasonal_1 Loss: 0.2029 | 0.1930
Epoch 53/300, seasonal_1 Loss: 0.2029 | 0.1912
Epoch 54/300, seasonal_1 Loss: 0.2017 | 0.1893
Epoch 55/300, seasonal_1 Loss: 0.2010 | 0.1884
Epoch 56/300, seasonal_1 Loss: 0.2001 | 0.1873
Epoch 57/300, seasonal_1 Loss: 0.1997 | 0.1865
Epoch 58/300, seasonal_1 Loss: 0.1992 | 0.1846
Epoch 59/300, seasonal_1 Loss: 0.1977 | 0.1840
Epoch 60/300, seasonal_1 Loss: 0.1983 | 0.1834
Epoch 61/300, seasonal_1 Loss: 0.1970 | 0.1825
Epoch 62/300, seasonal_1 Loss: 0.1962 | 0.1811
Epoch 63/300, seasonal_1 Loss: 0.1964 | 0.1810
Epoch 64/300, seasonal_1 Loss: 0.1952 | 0.1802
Epoch 65/300, seasonal_1 Loss: 0.1951 | 0.1794
Epoch 66/300, seasonal_1 Loss: 0.1938 | 0.1774
Epoch 67/300, seasonal_1 Loss: 0.1940 | 0.1768
Epoch 68/300, seasonal_1 Loss: 0.1932 | 0.1767
Epoch 69/300, seasonal_1 Loss: 0.1925 | 0.1754
Epoch 70/300, seasonal_1 Loss: 0.1919 | 0.1752
Epoch 71/300, seasonal_1 Loss: 0.1910 | 0.1738
Epoch 72/300, seasonal_1 Loss: 0.1909 | 0.1736
Epoch 73/300, seasonal_1 Loss: 0.1911 | 0.1733
Epoch 74/300, seasonal_1 Loss: 0.1906 | 0.1722
Epoch 75/300, seasonal_1 Loss: 0.1900 | 0.1717
Epoch 76/300, seasonal_1 Loss: 0.1895 | 0.1717
Epoch 77/300, seasonal_1 Loss: 0.1894 | 0.1719
Epoch 78/300, seasonal_1 Loss: 0.1895 | 0.1709
Epoch 79/300, seasonal_1 Loss: 0.1891 | 0.1702
Epoch 80/300, seasonal_1 Loss: 0.1885 | 0.1697
Epoch 81/300, seasonal_1 Loss: 0.1879 | 0.1697
Epoch 82/300, seasonal_1 Loss: 0.1879 | 0.1685
Epoch 83/300, seasonal_1 Loss: 0.1882 | 0.1683
Epoch 84/300, seasonal_1 Loss: 0.1875 | 0.1677
Epoch 85/300, seasonal_1 Loss: 0.1866 | 0.1667
Epoch 86/300, seasonal_1 Loss: 0.1869 | 0.1671
Epoch 87/300, seasonal_1 Loss: 0.1868 | 0.1669
Epoch 88/300, seasonal_1 Loss: 0.1858 | 0.1663
Epoch 89/300, seasonal_1 Loss: 0.1856 | 0.1658
Epoch 90/300, seasonal_1 Loss: 0.1857 | 0.1661
Epoch 91/300, seasonal_1 Loss: 0.1853 | 0.1655
Epoch 92/300, seasonal_1 Loss: 0.1850 | 0.1647
Epoch 93/300, seasonal_1 Loss: 0.1852 | 0.1643
Epoch 94/300, seasonal_1 Loss: 0.1843 | 0.1637
Epoch 95/300, seasonal_1 Loss: 0.1839 | 0.1636
Epoch 96/300, seasonal_1 Loss: 0.1837 | 0.1635
Epoch 97/300, seasonal_1 Loss: 0.1841 | 0.1630
Epoch 98/300, seasonal_1 Loss: 0.1832 | 0.1628
Epoch 99/300, seasonal_1 Loss: 0.1837 | 0.1628
Epoch 100/300, seasonal_1 Loss: 0.1836 | 0.1624
Epoch 101/300, seasonal_1 Loss: 0.1835 | 0.1622
Epoch 102/300, seasonal_1 Loss: 0.1829 | 0.1625
Epoch 103/300, seasonal_1 Loss: 0.1835 | 0.1619
Epoch 104/300, seasonal_1 Loss: 0.1829 | 0.1611
Epoch 105/300, seasonal_1 Loss: 0.1827 | 0.1613
Epoch 106/300, seasonal_1 Loss: 0.1837 | 0.1612
Epoch 107/300, seasonal_1 Loss: 0.1822 | 0.1604
Epoch 108/300, seasonal_1 Loss: 0.1821 | 0.1602
Epoch 109/300, seasonal_1 Loss: 0.1817 | 0.1600
Epoch 110/300, seasonal_1 Loss: 0.1815 | 0.1600
Epoch 111/300, seasonal_1 Loss: 0.1816 | 0.1597
Epoch 112/300, seasonal_1 Loss: 0.1811 | 0.1598
Epoch 113/300, seasonal_1 Loss: 0.1811 | 0.1594
Epoch 114/300, seasonal_1 Loss: 0.1813 | 0.1594
Epoch 115/300, seasonal_1 Loss: 0.1801 | 0.1591
Epoch 116/300, seasonal_1 Loss: 0.1811 | 0.1590
Epoch 117/300, seasonal_1 Loss: 0.1808 | 0.1591
Epoch 118/300, seasonal_1 Loss: 0.1807 | 0.1586
Epoch 119/300, seasonal_1 Loss: 0.1801 | 0.1581
Epoch 120/300, seasonal_1 Loss: 0.1803 | 0.1580
Epoch 121/300, seasonal_1 Loss: 0.1799 | 0.1583
Epoch 122/300, seasonal_1 Loss: 0.1802 | 0.1582
Epoch 123/300, seasonal_1 Loss: 0.1803 | 0.1579
Epoch 124/300, seasonal_1 Loss: 0.1799 | 0.1577
Epoch 125/300, seasonal_1 Loss: 0.1800 | 0.1580
Epoch 126/300, seasonal_1 Loss: 0.1794 | 0.1574
Epoch 127/300, seasonal_1 Loss: 0.1792 | 0.1573
Epoch 128/300, seasonal_1 Loss: 0.1795 | 0.1576
Epoch 129/300, seasonal_1 Loss: 0.1792 | 0.1578
Epoch 130/300, seasonal_1 Loss: 0.1792 | 0.1575
Epoch 131/300, seasonal_1 Loss: 0.1792 | 0.1571
Epoch 132/300, seasonal_1 Loss: 0.1790 | 0.1570
Epoch 133/300, seasonal_1 Loss: 0.1789 | 0.1570
Epoch 134/300, seasonal_1 Loss: 0.1790 | 0.1571
Epoch 135/300, seasonal_1 Loss: 0.1797 | 0.1571
Epoch 136/300, seasonal_1 Loss: 0.1792 | 0.1568
Epoch 137/300, seasonal_1 Loss: 0.1793 | 0.1566
Epoch 138/300, seasonal_1 Loss: 0.1795 | 0.1562
Epoch 139/300, seasonal_1 Loss: 0.1787 | 0.1562
Epoch 140/300, seasonal_1 Loss: 0.1783 | 0.1564
Epoch 141/300, seasonal_1 Loss: 0.1782 | 0.1564
Epoch 142/300, seasonal_1 Loss: 0.1788 | 0.1563
Epoch 143/300, seasonal_1 Loss: 0.1789 | 0.1561
Epoch 144/300, seasonal_1 Loss: 0.1781 | 0.1562
Epoch 145/300, seasonal_1 Loss: 0.1783 | 0.1562
Epoch 146/300, seasonal_1 Loss: 0.1786 | 0.1560
Epoch 147/300, seasonal_1 Loss: 0.1781 | 0.1558
Epoch 148/300, seasonal_1 Loss: 0.1785 | 0.1556
Epoch 149/300, seasonal_1 Loss: 0.1781 | 0.1557
Epoch 150/300, seasonal_1 Loss: 0.1780 | 0.1554
Epoch 151/300, seasonal_1 Loss: 0.1785 | 0.1553
Epoch 152/300, seasonal_1 Loss: 0.1782 | 0.1554
Epoch 153/300, seasonal_1 Loss: 0.1778 | 0.1553
Epoch 154/300, seasonal_1 Loss: 0.1784 | 0.1551
Epoch 155/300, seasonal_1 Loss: 0.1773 | 0.1549
Epoch 156/300, seasonal_1 Loss: 0.1780 | 0.1547
Epoch 157/300, seasonal_1 Loss: 0.1777 | 0.1550
Epoch 158/300, seasonal_1 Loss: 0.1779 | 0.1549
Epoch 159/300, seasonal_1 Loss: 0.1773 | 0.1548
Epoch 160/300, seasonal_1 Loss: 0.1771 | 0.1548
Epoch 161/300, seasonal_1 Loss: 0.1777 | 0.1547
Epoch 162/300, seasonal_1 Loss: 0.1777 | 0.1548
Epoch 163/300, seasonal_1 Loss: 0.1773 | 0.1549
Epoch 164/300, seasonal_1 Loss: 0.1774 | 0.1549
Epoch 165/300, seasonal_1 Loss: 0.1778 | 0.1550
Epoch 166/300, seasonal_1 Loss: 0.1779 | 0.1550
Epoch 167/300, seasonal_1 Loss: 0.1770 | 0.1548
Epoch 168/300, seasonal_1 Loss: 0.1776 | 0.1547
Epoch 169/300, seasonal_1 Loss: 0.1778 | 0.1547
Epoch 170/300, seasonal_1 Loss: 0.1771 | 0.1546
Epoch 171/300, seasonal_1 Loss: 0.1773 | 0.1547
Epoch 172/300, seasonal_1 Loss: 0.1771 | 0.1546
Epoch 173/300, seasonal_1 Loss: 0.1775 | 0.1545
Epoch 174/300, seasonal_1 Loss: 0.1769 | 0.1545
Epoch 175/300, seasonal_1 Loss: 0.1766 | 0.1545
Epoch 176/300, seasonal_1 Loss: 0.1767 | 0.1544
Epoch 177/300, seasonal_1 Loss: 0.1773 | 0.1545
Epoch 178/300, seasonal_1 Loss: 0.1777 | 0.1543
Epoch 179/300, seasonal_1 Loss: 0.1774 | 0.1542
Epoch 180/300, seasonal_1 Loss: 0.1777 | 0.1541
Epoch 181/300, seasonal_1 Loss: 0.1774 | 0.1540
Epoch 182/300, seasonal_1 Loss: 0.1775 | 0.1541
Epoch 183/300, seasonal_1 Loss: 0.1768 | 0.1542
Epoch 184/300, seasonal_1 Loss: 0.1771 | 0.1542
Epoch 185/300, seasonal_1 Loss: 0.1767 | 0.1541
Epoch 186/300, seasonal_1 Loss: 0.1773 | 0.1540
Epoch 187/300, seasonal_1 Loss: 0.1768 | 0.1540
Epoch 188/300, seasonal_1 Loss: 0.1763 | 0.1541
Epoch 189/300, seasonal_1 Loss: 0.1768 | 0.1541
Epoch 190/300, seasonal_1 Loss: 0.1775 | 0.1541
Epoch 191/300, seasonal_1 Loss: 0.1759 | 0.1540
Epoch 192/300, seasonal_1 Loss: 0.1769 | 0.1541
Epoch 193/300, seasonal_1 Loss: 0.1767 | 0.1540
Epoch 194/300, seasonal_1 Loss: 0.1766 | 0.1540
Epoch 195/300, seasonal_1 Loss: 0.1773 | 0.1539
Epoch 196/300, seasonal_1 Loss: 0.1766 | 0.1540
Epoch 197/300, seasonal_1 Loss: 0.1764 | 0.1540
Epoch 198/300, seasonal_1 Loss: 0.1766 | 0.1539
Epoch 199/300, seasonal_1 Loss: 0.1767 | 0.1539
Epoch 200/300, seasonal_1 Loss: 0.1770 | 0.1538
Epoch 201/300, seasonal_1 Loss: 0.1765 | 0.1538
Epoch 202/300, seasonal_1 Loss: 0.1768 | 0.1537
Epoch 203/300, seasonal_1 Loss: 0.1769 | 0.1537
Epoch 204/300, seasonal_1 Loss: 0.1767 | 0.1537
Epoch 205/300, seasonal_1 Loss: 0.1767 | 0.1538
Epoch 206/300, seasonal_1 Loss: 0.1762 | 0.1538
Epoch 207/300, seasonal_1 Loss: 0.1765 | 0.1538
Epoch 208/300, seasonal_1 Loss: 0.1771 | 0.1538
Epoch 209/300, seasonal_1 Loss: 0.1770 | 0.1538
Epoch 210/300, seasonal_1 Loss: 0.1765 | 0.1538
Epoch 211/300, seasonal_1 Loss: 0.1764 | 0.1538
Epoch 212/300, seasonal_1 Loss: 0.1773 | 0.1538
Epoch 213/300, seasonal_1 Loss: 0.1762 | 0.1538
Epoch 214/300, seasonal_1 Loss: 0.1774 | 0.1537
Epoch 215/300, seasonal_1 Loss: 0.1769 | 0.1537
Epoch 216/300, seasonal_1 Loss: 0.1771 | 0.1536
Epoch 217/300, seasonal_1 Loss: 0.1765 | 0.1536
Epoch 218/300, seasonal_1 Loss: 0.1763 | 0.1536
Epoch 219/300, seasonal_1 Loss: 0.1760 | 0.1536
Epoch 220/300, seasonal_1 Loss: 0.1766 | 0.1536
Epoch 221/300, seasonal_1 Loss: 0.1762 | 0.1536
Epoch 222/300, seasonal_1 Loss: 0.1762 | 0.1536
Epoch 223/300, seasonal_1 Loss: 0.1767 | 0.1536
Epoch 224/300, seasonal_1 Loss: 0.1768 | 0.1537
Epoch 225/300, seasonal_1 Loss: 0.1774 | 0.1536
Epoch 226/300, seasonal_1 Loss: 0.1773 | 0.1536
Epoch 227/300, seasonal_1 Loss: 0.1758 | 0.1536
Epoch 228/300, seasonal_1 Loss: 0.1758 | 0.1536
Epoch 229/300, seasonal_1 Loss: 0.1764 | 0.1536
Epoch 230/300, seasonal_1 Loss: 0.1768 | 0.1536
Epoch 231/300, seasonal_1 Loss: 0.1770 | 0.1536
Epoch 232/300, seasonal_1 Loss: 0.1768 | 0.1536
Epoch 233/300, seasonal_1 Loss: 0.1761 | 0.1535
Epoch 234/300, seasonal_1 Loss: 0.1763 | 0.1535
Epoch 235/300, seasonal_1 Loss: 0.1769 | 0.1535
Epoch 236/300, seasonal_1 Loss: 0.1768 | 0.1535
Epoch 237/300, seasonal_1 Loss: 0.1763 | 0.1535
Epoch 238/300, seasonal_1 Loss: 0.1763 | 0.1535
Epoch 239/300, seasonal_1 Loss: 0.1761 | 0.1535
Epoch 240/300, seasonal_1 Loss: 0.1760 | 0.1535
Epoch 241/300, seasonal_1 Loss: 0.1762 | 0.1535
Epoch 242/300, seasonal_1 Loss: 0.1764 | 0.1535
Epoch 243/300, seasonal_1 Loss: 0.1763 | 0.1535
Epoch 244/300, seasonal_1 Loss: 0.1764 | 0.1535
Epoch 245/300, seasonal_1 Loss: 0.1767 | 0.1535
Epoch 246/300, seasonal_1 Loss: 0.1764 | 0.1535
Epoch 247/300, seasonal_1 Loss: 0.1757 | 0.1535
Epoch 248/300, seasonal_1 Loss: 0.1758 | 0.1535
Epoch 249/300, seasonal_1 Loss: 0.1766 | 0.1535
Epoch 250/300, seasonal_1 Loss: 0.1755 | 0.1535
Epoch 251/300, seasonal_1 Loss: 0.1763 | 0.1535
Epoch 252/300, seasonal_1 Loss: 0.1761 | 0.1535
Epoch 253/300, seasonal_1 Loss: 0.1765 | 0.1535
Epoch 254/300, seasonal_1 Loss: 0.1762 | 0.1535
Epoch 255/300, seasonal_1 Loss: 0.1762 | 0.1535
Epoch 256/300, seasonal_1 Loss: 0.1760 | 0.1535
Epoch 257/300, seasonal_1 Loss: 0.1762 | 0.1534
Epoch 258/300, seasonal_1 Loss: 0.1761 | 0.1534
Epoch 259/300, seasonal_1 Loss: 0.1762 | 0.1534
Epoch 260/300, seasonal_1 Loss: 0.1767 | 0.1534
Epoch 261/300, seasonal_1 Loss: 0.1761 | 0.1534
Epoch 262/300, seasonal_1 Loss: 0.1763 | 0.1534
Epoch 263/300, seasonal_1 Loss: 0.1764 | 0.1534
Epoch 264/300, seasonal_1 Loss: 0.1759 | 0.1534
Epoch 265/300, seasonal_1 Loss: 0.1759 | 0.1534
Epoch 266/300, seasonal_1 Loss: 0.1759 | 0.1534
Epoch 267/300, seasonal_1 Loss: 0.1757 | 0.1534
Epoch 268/300, seasonal_1 Loss: 0.1767 | 0.1534
Epoch 269/300, seasonal_1 Loss: 0.1757 | 0.1534
Epoch 270/300, seasonal_1 Loss: 0.1756 | 0.1534
Epoch 271/300, seasonal_1 Loss: 0.1761 | 0.1534
Epoch 272/300, seasonal_1 Loss: 0.1759 | 0.1534
Epoch 273/300, seasonal_1 Loss: 0.1762 | 0.1534
Epoch 274/300, seasonal_1 Loss: 0.1760 | 0.1534
Epoch 275/300, seasonal_1 Loss: 0.1763 | 0.1534
Epoch 276/300, seasonal_1 Loss: 0.1762 | 0.1534
Epoch 277/300, seasonal_1 Loss: 0.1768 | 0.1534
Epoch 278/300, seasonal_1 Loss: 0.1763 | 0.1534
Epoch 279/300, seasonal_1 Loss: 0.1766 | 0.1534
Epoch 280/300, seasonal_1 Loss: 0.1760 | 0.1534
Epoch 281/300, seasonal_1 Loss: 0.1755 | 0.1534
Epoch 282/300, seasonal_1 Loss: 0.1760 | 0.1534
Epoch 283/300, seasonal_1 Loss: 0.1758 | 0.1534
Epoch 284/300, seasonal_1 Loss: 0.1760 | 0.1534
Epoch 285/300, seasonal_1 Loss: 0.1757 | 0.1534
Epoch 286/300, seasonal_1 Loss: 0.1759 | 0.1534
Epoch 287/300, seasonal_1 Loss: 0.1762 | 0.1534
Epoch 288/300, seasonal_1 Loss: 0.1764 | 0.1534
Epoch 289/300, seasonal_1 Loss: 0.1760 | 0.1533
Epoch 290/300, seasonal_1 Loss: 0.1759 | 0.1533
Epoch 291/300, seasonal_1 Loss: 0.1756 | 0.1533
Epoch 292/300, seasonal_1 Loss: 0.1765 | 0.1533
Epoch 293/300, seasonal_1 Loss: 0.1770 | 0.1533
Epoch 294/300, seasonal_1 Loss: 0.1761 | 0.1533
Epoch 295/300, seasonal_1 Loss: 0.1757 | 0.1533
Epoch 296/300, seasonal_1 Loss: 0.1765 | 0.1533
Epoch 297/300, seasonal_1 Loss: 0.1763 | 0.1533
Epoch 298/300, seasonal_1 Loss: 0.1771 | 0.1533
Epoch 299/300, seasonal_1 Loss: 0.1755 | 0.1533
Epoch 300/300, seasonal_1 Loss: 0.1762 | 0.1533
Training seasonal_2 component with params: {'observation_period_num': 68, 'train_rates': 0.9890883537677629, 'learning_rate': 1.915941240448754e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.875191720048181}
Epoch 1/300, seasonal_2 Loss: 1.0050 | 1.0731
Epoch 2/300, seasonal_2 Loss: 0.7184 | 0.7490
Epoch 3/300, seasonal_2 Loss: 0.5733 | 0.6297
Epoch 4/300, seasonal_2 Loss: 0.4971 | 0.5624
Epoch 5/300, seasonal_2 Loss: 0.4456 | 0.5144
Epoch 6/300, seasonal_2 Loss: 0.4068 | 0.4809
Epoch 7/300, seasonal_2 Loss: 0.3804 | 0.4529
Epoch 8/300, seasonal_2 Loss: 0.3574 | 0.4252
Epoch 9/300, seasonal_2 Loss: 0.3397 | 0.4041
Epoch 10/300, seasonal_2 Loss: 0.3238 | 0.3899
Epoch 11/300, seasonal_2 Loss: 0.3115 | 0.3730
Epoch 12/300, seasonal_2 Loss: 0.3018 | 0.3596
Epoch 13/300, seasonal_2 Loss: 0.2927 | 0.3440
Epoch 14/300, seasonal_2 Loss: 0.2842 | 0.3315
Epoch 15/300, seasonal_2 Loss: 0.2765 | 0.3219
Epoch 16/300, seasonal_2 Loss: 0.2691 | 0.3119
Epoch 17/300, seasonal_2 Loss: 0.2650 | 0.3035
Epoch 18/300, seasonal_2 Loss: 0.2592 | 0.2955
Epoch 19/300, seasonal_2 Loss: 0.2543 | 0.2876
Epoch 20/300, seasonal_2 Loss: 0.2503 | 0.2811
Epoch 21/300, seasonal_2 Loss: 0.2461 | 0.2784
Epoch 22/300, seasonal_2 Loss: 0.2426 | 0.2675
Epoch 23/300, seasonal_2 Loss: 0.2394 | 0.2610
Epoch 24/300, seasonal_2 Loss: 0.2366 | 0.2533
Epoch 25/300, seasonal_2 Loss: 0.2334 | 0.2480
Epoch 26/300, seasonal_2 Loss: 0.2300 | 0.2492
Epoch 27/300, seasonal_2 Loss: 0.2272 | 0.2439
Epoch 28/300, seasonal_2 Loss: 0.2258 | 0.2389
Epoch 29/300, seasonal_2 Loss: 0.2236 | 0.2322
Epoch 30/300, seasonal_2 Loss: 0.2219 | 0.2254
Epoch 31/300, seasonal_2 Loss: 0.2203 | 0.2293
Epoch 32/300, seasonal_2 Loss: 0.2185 | 0.2357
Epoch 33/300, seasonal_2 Loss: 0.2157 | 0.2257
Epoch 34/300, seasonal_2 Loss: 0.2146 | 0.2175
Epoch 35/300, seasonal_2 Loss: 0.2131 | 0.2108
Epoch 36/300, seasonal_2 Loss: 0.2107 | 0.2081
Epoch 37/300, seasonal_2 Loss: 0.2096 | 0.2090
Epoch 38/300, seasonal_2 Loss: 0.2074 | 0.2108
Epoch 39/300, seasonal_2 Loss: 0.2067 | 0.2066
Epoch 40/300, seasonal_2 Loss: 0.2052 | 0.2034
Epoch 41/300, seasonal_2 Loss: 0.2040 | 0.1988
Epoch 42/300, seasonal_2 Loss: 0.2023 | 0.2013
Epoch 43/300, seasonal_2 Loss: 0.2014 | 0.2016
Epoch 44/300, seasonal_2 Loss: 0.2002 | 0.1980
Epoch 45/300, seasonal_2 Loss: 0.1996 | 0.1926
Epoch 46/300, seasonal_2 Loss: 0.1987 | 0.1898
Epoch 47/300, seasonal_2 Loss: 0.1971 | 0.1912
Epoch 48/300, seasonal_2 Loss: 0.1968 | 0.1906
Epoch 49/300, seasonal_2 Loss: 0.1961 | 0.1869
Epoch 50/300, seasonal_2 Loss: 0.1955 | 0.1853
Epoch 51/300, seasonal_2 Loss: 0.1945 | 0.1829
Epoch 52/300, seasonal_2 Loss: 0.1932 | 0.1831
Epoch 53/300, seasonal_2 Loss: 0.1925 | 0.1835
Epoch 54/300, seasonal_2 Loss: 0.1924 | 0.1816
Epoch 55/300, seasonal_2 Loss: 0.1914 | 0.1791
Epoch 56/300, seasonal_2 Loss: 0.1905 | 0.1786
Epoch 57/300, seasonal_2 Loss: 0.1896 | 0.1787
Epoch 58/300, seasonal_2 Loss: 0.1901 | 0.1788
Epoch 59/300, seasonal_2 Loss: 0.1892 | 0.1772
Epoch 60/300, seasonal_2 Loss: 0.1883 | 0.1768
Epoch 61/300, seasonal_2 Loss: 0.1881 | 0.1742
Epoch 62/300, seasonal_2 Loss: 0.1872 | 0.1740
Epoch 63/300, seasonal_2 Loss: 0.1871 | 0.1741
Epoch 64/300, seasonal_2 Loss: 0.1871 | 0.1728
Epoch 65/300, seasonal_2 Loss: 0.1852 | 0.1715
Epoch 66/300, seasonal_2 Loss: 0.1850 | 0.1720
Epoch 67/300, seasonal_2 Loss: 0.1846 | 0.1700
Epoch 68/300, seasonal_2 Loss: 0.1849 | 0.1706
Epoch 69/300, seasonal_2 Loss: 0.1843 | 0.1702
Epoch 70/300, seasonal_2 Loss: 0.1842 | 0.1697
Epoch 71/300, seasonal_2 Loss: 0.1835 | 0.1700
Epoch 72/300, seasonal_2 Loss: 0.1832 | 0.1678
Epoch 73/300, seasonal_2 Loss: 0.1828 | 0.1673
Epoch 74/300, seasonal_2 Loss: 0.1828 | 0.1674
Epoch 75/300, seasonal_2 Loss: 0.1822 | 0.1668
Epoch 76/300, seasonal_2 Loss: 0.1818 | 0.1661
Epoch 77/300, seasonal_2 Loss: 0.1821 | 0.1660
Epoch 78/300, seasonal_2 Loss: 0.1816 | 0.1653
Epoch 79/300, seasonal_2 Loss: 0.1805 | 0.1647
Epoch 80/300, seasonal_2 Loss: 0.1808 | 0.1645
Epoch 81/300, seasonal_2 Loss: 0.1802 | 0.1634
Epoch 82/300, seasonal_2 Loss: 0.1806 | 0.1631
Epoch 83/300, seasonal_2 Loss: 0.1800 | 0.1635
Epoch 84/300, seasonal_2 Loss: 0.1797 | 0.1634
Epoch 85/300, seasonal_2 Loss: 0.1788 | 0.1635
Epoch 86/300, seasonal_2 Loss: 0.1791 | 0.1632
Epoch 87/300, seasonal_2 Loss: 0.1788 | 0.1622
Epoch 88/300, seasonal_2 Loss: 0.1789 | 0.1622
Epoch 89/300, seasonal_2 Loss: 0.1780 | 0.1618
Epoch 90/300, seasonal_2 Loss: 0.1780 | 0.1612
Epoch 91/300, seasonal_2 Loss: 0.1773 | 0.1616
Epoch 92/300, seasonal_2 Loss: 0.1779 | 0.1610
Epoch 93/300, seasonal_2 Loss: 0.1787 | 0.1606
Epoch 94/300, seasonal_2 Loss: 0.1772 | 0.1605
Epoch 95/300, seasonal_2 Loss: 0.1773 | 0.1601
Epoch 96/300, seasonal_2 Loss: 0.1776 | 0.1597
Epoch 97/300, seasonal_2 Loss: 0.1778 | 0.1598
Epoch 98/300, seasonal_2 Loss: 0.1762 | 0.1595
Epoch 99/300, seasonal_2 Loss: 0.1767 | 0.1592
Epoch 100/300, seasonal_2 Loss: 0.1755 | 0.1582
Epoch 101/300, seasonal_2 Loss: 0.1761 | 0.1582
Epoch 102/300, seasonal_2 Loss: 0.1767 | 0.1581
Epoch 103/300, seasonal_2 Loss: 0.1762 | 0.1582
Epoch 104/300, seasonal_2 Loss: 0.1756 | 0.1585
Epoch 105/300, seasonal_2 Loss: 0.1757 | 0.1586
Epoch 106/300, seasonal_2 Loss: 0.1755 | 0.1577
Epoch 107/300, seasonal_2 Loss: 0.1756 | 0.1575
Epoch 108/300, seasonal_2 Loss: 0.1755 | 0.1576
Epoch 109/300, seasonal_2 Loss: 0.1751 | 0.1573
Epoch 110/300, seasonal_2 Loss: 0.1752 | 0.1569
Epoch 111/300, seasonal_2 Loss: 0.1749 | 0.1571
Epoch 112/300, seasonal_2 Loss: 0.1750 | 0.1569
Epoch 113/300, seasonal_2 Loss: 0.1743 | 0.1571
Epoch 114/300, seasonal_2 Loss: 0.1743 | 0.1570
Epoch 115/300, seasonal_2 Loss: 0.1743 | 0.1562
Epoch 116/300, seasonal_2 Loss: 0.1746 | 0.1559
Epoch 117/300, seasonal_2 Loss: 0.1747 | 0.1559
Epoch 118/300, seasonal_2 Loss: 0.1747 | 0.1559
Epoch 119/300, seasonal_2 Loss: 0.1739 | 0.1559
Epoch 120/300, seasonal_2 Loss: 0.1737 | 0.1562
Epoch 121/300, seasonal_2 Loss: 0.1741 | 0.1561
Epoch 122/300, seasonal_2 Loss: 0.1743 | 0.1561
Epoch 123/300, seasonal_2 Loss: 0.1737 | 0.1561
Epoch 124/300, seasonal_2 Loss: 0.1739 | 0.1561
Epoch 125/300, seasonal_2 Loss: 0.1737 | 0.1559
Epoch 126/300, seasonal_2 Loss: 0.1733 | 0.1556
Epoch 127/300, seasonal_2 Loss: 0.1737 | 0.1555
Epoch 128/300, seasonal_2 Loss: 0.1734 | 0.1554
Epoch 129/300, seasonal_2 Loss: 0.1739 | 0.1552
Epoch 130/300, seasonal_2 Loss: 0.1738 | 0.1554
Epoch 131/300, seasonal_2 Loss: 0.1737 | 0.1553
Epoch 132/300, seasonal_2 Loss: 0.1733 | 0.1552
Epoch 133/300, seasonal_2 Loss: 0.1738 | 0.1555
Epoch 134/300, seasonal_2 Loss: 0.1735 | 0.1553
Epoch 135/300, seasonal_2 Loss: 0.1729 | 0.1551
Epoch 136/300, seasonal_2 Loss: 0.1726 | 0.1551
Epoch 137/300, seasonal_2 Loss: 0.1730 | 0.1547
Epoch 138/300, seasonal_2 Loss: 0.1736 | 0.1546
Epoch 139/300, seasonal_2 Loss: 0.1728 | 0.1546
Epoch 140/300, seasonal_2 Loss: 0.1725 | 0.1545
Epoch 141/300, seasonal_2 Loss: 0.1730 | 0.1546
Epoch 142/300, seasonal_2 Loss: 0.1730 | 0.1546
Epoch 143/300, seasonal_2 Loss: 0.1731 | 0.1544
Epoch 144/300, seasonal_2 Loss: 0.1724 | 0.1544
Epoch 145/300, seasonal_2 Loss: 0.1726 | 0.1541
Epoch 146/300, seasonal_2 Loss: 0.1727 | 0.1541
Epoch 147/300, seasonal_2 Loss: 0.1724 | 0.1542
Epoch 148/300, seasonal_2 Loss: 0.1725 | 0.1541
Epoch 149/300, seasonal_2 Loss: 0.1724 | 0.1539
Epoch 150/300, seasonal_2 Loss: 0.1722 | 0.1538
Epoch 151/300, seasonal_2 Loss: 0.1724 | 0.1540
Epoch 152/300, seasonal_2 Loss: 0.1732 | 0.1540
Epoch 153/300, seasonal_2 Loss: 0.1726 | 0.1540
Epoch 154/300, seasonal_2 Loss: 0.1729 | 0.1539
Epoch 155/300, seasonal_2 Loss: 0.1722 | 0.1537
Epoch 156/300, seasonal_2 Loss: 0.1721 | 0.1536
Epoch 157/300, seasonal_2 Loss: 0.1723 | 0.1534
Epoch 158/300, seasonal_2 Loss: 0.1728 | 0.1536
Epoch 159/300, seasonal_2 Loss: 0.1728 | 0.1534
Epoch 160/300, seasonal_2 Loss: 0.1727 | 0.1535
Epoch 161/300, seasonal_2 Loss: 0.1726 | 0.1534
Epoch 162/300, seasonal_2 Loss: 0.1722 | 0.1533
Epoch 163/300, seasonal_2 Loss: 0.1720 | 0.1532
Epoch 164/300, seasonal_2 Loss: 0.1721 | 0.1532
Epoch 165/300, seasonal_2 Loss: 0.1712 | 0.1532
Epoch 166/300, seasonal_2 Loss: 0.1721 | 0.1532
Epoch 167/300, seasonal_2 Loss: 0.1717 | 0.1533
Epoch 168/300, seasonal_2 Loss: 0.1721 | 0.1534
Epoch 169/300, seasonal_2 Loss: 0.1722 | 0.1534
Epoch 170/300, seasonal_2 Loss: 0.1721 | 0.1534
Epoch 171/300, seasonal_2 Loss: 0.1720 | 0.1535
Epoch 172/300, seasonal_2 Loss: 0.1722 | 0.1534
Epoch 173/300, seasonal_2 Loss: 0.1714 | 0.1533
Epoch 174/300, seasonal_2 Loss: 0.1720 | 0.1532
Epoch 175/300, seasonal_2 Loss: 0.1707 | 0.1531
Epoch 176/300, seasonal_2 Loss: 0.1711 | 0.1531
Epoch 177/300, seasonal_2 Loss: 0.1723 | 0.1531
Epoch 178/300, seasonal_2 Loss: 0.1719 | 0.1530
Epoch 179/300, seasonal_2 Loss: 0.1721 | 0.1529
Epoch 180/300, seasonal_2 Loss: 0.1720 | 0.1527
Epoch 181/300, seasonal_2 Loss: 0.1715 | 0.1527
Epoch 182/300, seasonal_2 Loss: 0.1720 | 0.1528
Epoch 183/300, seasonal_2 Loss: 0.1712 | 0.1527
Epoch 184/300, seasonal_2 Loss: 0.1716 | 0.1527
Epoch 185/300, seasonal_2 Loss: 0.1718 | 0.1527
Epoch 186/300, seasonal_2 Loss: 0.1719 | 0.1526
Epoch 187/300, seasonal_2 Loss: 0.1716 | 0.1526
Epoch 188/300, seasonal_2 Loss: 0.1715 | 0.1527
Epoch 189/300, seasonal_2 Loss: 0.1717 | 0.1527
Epoch 190/300, seasonal_2 Loss: 0.1713 | 0.1528
Epoch 191/300, seasonal_2 Loss: 0.1717 | 0.1527
Epoch 192/300, seasonal_2 Loss: 0.1715 | 0.1527
Epoch 193/300, seasonal_2 Loss: 0.1709 | 0.1527
Epoch 194/300, seasonal_2 Loss: 0.1706 | 0.1526
Epoch 195/300, seasonal_2 Loss: 0.1714 | 0.1527
Epoch 196/300, seasonal_2 Loss: 0.1717 | 0.1528
Epoch 197/300, seasonal_2 Loss: 0.1714 | 0.1527
Epoch 198/300, seasonal_2 Loss: 0.1718 | 0.1527
Epoch 199/300, seasonal_2 Loss: 0.1708 | 0.1526
Epoch 200/300, seasonal_2 Loss: 0.1720 | 0.1527
Epoch 201/300, seasonal_2 Loss: 0.1713 | 0.1526
Epoch 202/300, seasonal_2 Loss: 0.1713 | 0.1526
Epoch 203/300, seasonal_2 Loss: 0.1719 | 0.1526
Epoch 204/300, seasonal_2 Loss: 0.1715 | 0.1526
Epoch 205/300, seasonal_2 Loss: 0.1715 | 0.1526
Epoch 206/300, seasonal_2 Loss: 0.1723 | 0.1526
Epoch 207/300, seasonal_2 Loss: 0.1712 | 0.1526
Epoch 208/300, seasonal_2 Loss: 0.1715 | 0.1525
Epoch 209/300, seasonal_2 Loss: 0.1713 | 0.1525
Epoch 210/300, seasonal_2 Loss: 0.1716 | 0.1525
Epoch 211/300, seasonal_2 Loss: 0.1713 | 0.1525
Epoch 212/300, seasonal_2 Loss: 0.1708 | 0.1524
Epoch 213/300, seasonal_2 Loss: 0.1718 | 0.1525
Epoch 214/300, seasonal_2 Loss: 0.1712 | 0.1524
Epoch 215/300, seasonal_2 Loss: 0.1717 | 0.1524
Epoch 216/300, seasonal_2 Loss: 0.1716 | 0.1524
Epoch 217/300, seasonal_2 Loss: 0.1713 | 0.1525
Epoch 218/300, seasonal_2 Loss: 0.1720 | 0.1524
Epoch 219/300, seasonal_2 Loss: 0.1715 | 0.1525
Epoch 220/300, seasonal_2 Loss: 0.1720 | 0.1525
Epoch 221/300, seasonal_2 Loss: 0.1712 | 0.1525
Epoch 222/300, seasonal_2 Loss: 0.1712 | 0.1525
Epoch 223/300, seasonal_2 Loss: 0.1721 | 0.1525
Epoch 224/300, seasonal_2 Loss: 0.1712 | 0.1525
Epoch 225/300, seasonal_2 Loss: 0.1714 | 0.1525
Epoch 226/300, seasonal_2 Loss: 0.1718 | 0.1525
Epoch 227/300, seasonal_2 Loss: 0.1720 | 0.1525
Epoch 228/300, seasonal_2 Loss: 0.1712 | 0.1525
Epoch 229/300, seasonal_2 Loss: 0.1712 | 0.1524
Epoch 230/300, seasonal_2 Loss: 0.1714 | 0.1524
Epoch 231/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 232/300, seasonal_2 Loss: 0.1707 | 0.1524
Epoch 233/300, seasonal_2 Loss: 0.1719 | 0.1524
Epoch 234/300, seasonal_2 Loss: 0.1712 | 0.1524
Epoch 235/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 236/300, seasonal_2 Loss: 0.1709 | 0.1524
Epoch 237/300, seasonal_2 Loss: 0.1717 | 0.1524
Epoch 238/300, seasonal_2 Loss: 0.1716 | 0.1524
Epoch 239/300, seasonal_2 Loss: 0.1719 | 0.1524
Epoch 240/300, seasonal_2 Loss: 0.1709 | 0.1524
Epoch 241/300, seasonal_2 Loss: 0.1711 | 0.1524
Epoch 242/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 243/300, seasonal_2 Loss: 0.1717 | 0.1524
Epoch 244/300, seasonal_2 Loss: 0.1718 | 0.1524
Epoch 245/300, seasonal_2 Loss: 0.1700 | 0.1524
Epoch 246/300, seasonal_2 Loss: 0.1710 | 0.1524
Epoch 247/300, seasonal_2 Loss: 0.1710 | 0.1524
Epoch 248/300, seasonal_2 Loss: 0.1710 | 0.1523
Epoch 249/300, seasonal_2 Loss: 0.1716 | 0.1524
Epoch 250/300, seasonal_2 Loss: 0.1716 | 0.1524
Epoch 251/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 252/300, seasonal_2 Loss: 0.1710 | 0.1524
Epoch 253/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 254/300, seasonal_2 Loss: 0.1711 | 0.1524
Epoch 255/300, seasonal_2 Loss: 0.1716 | 0.1524
Epoch 256/300, seasonal_2 Loss: 0.1710 | 0.1524
Epoch 257/300, seasonal_2 Loss: 0.1714 | 0.1524
Epoch 258/300, seasonal_2 Loss: 0.1704 | 0.1524
Epoch 259/300, seasonal_2 Loss: 0.1712 | 0.1524
Epoch 260/300, seasonal_2 Loss: 0.1710 | 0.1524
Epoch 261/300, seasonal_2 Loss: 0.1708 | 0.1524
Epoch 262/300, seasonal_2 Loss: 0.1707 | 0.1524
Epoch 263/300, seasonal_2 Loss: 0.1713 | 0.1524
Epoch 264/300, seasonal_2 Loss: 0.1717 | 0.1524
Epoch 265/300, seasonal_2 Loss: 0.1715 | 0.1524
Epoch 266/300, seasonal_2 Loss: 0.1711 | 0.1524
Epoch 267/300, seasonal_2 Loss: 0.1705 | 0.1524
Epoch 268/300, seasonal_2 Loss: 0.1708 | 0.1524
Epoch 269/300, seasonal_2 Loss: 0.1702 | 0.1523
Epoch 270/300, seasonal_2 Loss: 0.1704 | 0.1523
Epoch 271/300, seasonal_2 Loss: 0.1720 | 0.1523
Epoch 272/300, seasonal_2 Loss: 0.1711 | 0.1523
Epoch 273/300, seasonal_2 Loss: 0.1714 | 0.1523
Epoch 274/300, seasonal_2 Loss: 0.1716 | 0.1523
Epoch 275/300, seasonal_2 Loss: 0.1709 | 0.1523
Epoch 276/300, seasonal_2 Loss: 0.1712 | 0.1523
Epoch 277/300, seasonal_2 Loss: 0.1707 | 0.1523
Epoch 278/300, seasonal_2 Loss: 0.1708 | 0.1523
Epoch 279/300, seasonal_2 Loss: 0.1708 | 0.1523
Epoch 280/300, seasonal_2 Loss: 0.1712 | 0.1523
Epoch 281/300, seasonal_2 Loss: 0.1715 | 0.1523
Epoch 282/300, seasonal_2 Loss: 0.1707 | 0.1523
Epoch 283/300, seasonal_2 Loss: 0.1705 | 0.1523
Epoch 284/300, seasonal_2 Loss: 0.1714 | 0.1523
Epoch 285/300, seasonal_2 Loss: 0.1713 | 0.1523
Epoch 286/300, seasonal_2 Loss: 0.1716 | 0.1523
Epoch 287/300, seasonal_2 Loss: 0.1708 | 0.1523
Epoch 288/300, seasonal_2 Loss: 0.1707 | 0.1523
Epoch 289/300, seasonal_2 Loss: 0.1711 | 0.1523
Epoch 290/300, seasonal_2 Loss: 0.1715 | 0.1523
Epoch 291/300, seasonal_2 Loss: 0.1710 | 0.1523
Epoch 292/300, seasonal_2 Loss: 0.1710 | 0.1523
Epoch 293/300, seasonal_2 Loss: 0.1718 | 0.1523
Epoch 294/300, seasonal_2 Loss: 0.1710 | 0.1523
Epoch 295/300, seasonal_2 Loss: 0.1717 | 0.1523
Epoch 296/300, seasonal_2 Loss: 0.1717 | 0.1523
Epoch 297/300, seasonal_2 Loss: 0.1708 | 0.1523
Epoch 298/300, seasonal_2 Loss: 0.1708 | 0.1523
Epoch 299/300, seasonal_2 Loss: 0.1711 | 0.1523
Epoch 300/300, seasonal_2 Loss: 0.1707 | 0.1523
Training seasonal_3 component with params: {'observation_period_num': 247, 'train_rates': 0.9897894713224675, 'learning_rate': 0.00025351991977514245, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8244239613542147}
Epoch 1/300, seasonal_3 Loss: 1.0247 | 0.9702
Epoch 2/300, seasonal_3 Loss: 0.8523 | 0.7140
Epoch 3/300, seasonal_3 Loss: 0.7101 | 0.6278
Epoch 4/300, seasonal_3 Loss: 0.5741 | 0.4817
Epoch 5/300, seasonal_3 Loss: 0.5239 | 0.4144
Epoch 6/300, seasonal_3 Loss: 0.5652 | 0.4246
Epoch 7/300, seasonal_3 Loss: 0.4776 | 0.4415
Epoch 8/300, seasonal_3 Loss: 0.4571 | 0.3629
Epoch 9/300, seasonal_3 Loss: 0.4313 | 0.3148
Epoch 10/300, seasonal_3 Loss: 0.3971 | 0.2980
Epoch 11/300, seasonal_3 Loss: 0.4023 | 0.2946
Epoch 12/300, seasonal_3 Loss: 0.4289 | 0.2825
Epoch 13/300, seasonal_3 Loss: 0.3498 | 0.2933
Epoch 14/300, seasonal_3 Loss: 0.3869 | 0.2856
Epoch 15/300, seasonal_3 Loss: 0.3358 | 0.2530
Epoch 16/300, seasonal_3 Loss: 0.2935 | 0.2467
Epoch 17/300, seasonal_3 Loss: 0.2861 | 0.2350
Epoch 18/300, seasonal_3 Loss: 0.2790 | 0.2263
Epoch 19/300, seasonal_3 Loss: 0.2715 | 0.2179
Epoch 20/300, seasonal_3 Loss: 0.2636 | 0.2184
Epoch 21/300, seasonal_3 Loss: 0.2540 | 0.2182
Epoch 22/300, seasonal_3 Loss: 0.2478 | 0.2018
Epoch 23/300, seasonal_3 Loss: 0.2476 | 0.1988
Epoch 24/300, seasonal_3 Loss: 0.2413 | 0.2058
Epoch 25/300, seasonal_3 Loss: 0.2348 | 0.1949
Epoch 26/300, seasonal_3 Loss: 0.2362 | 0.1882
Epoch 27/300, seasonal_3 Loss: 0.2289 | 0.1841
Epoch 28/300, seasonal_3 Loss: 0.2263 | 0.1872
Epoch 29/300, seasonal_3 Loss: 0.2220 | 0.1825
Epoch 30/300, seasonal_3 Loss: 0.2207 | 0.1792
Epoch 31/300, seasonal_3 Loss: 0.2169 | 0.1784
Epoch 32/300, seasonal_3 Loss: 0.2167 | 0.1743
Epoch 33/300, seasonal_3 Loss: 0.2124 | 0.1741
Epoch 34/300, seasonal_3 Loss: 0.2096 | 0.1713
Epoch 35/300, seasonal_3 Loss: 0.2077 | 0.1683
Epoch 36/300, seasonal_3 Loss: 0.2069 | 0.1683
Epoch 37/300, seasonal_3 Loss: 0.2055 | 0.1674
Epoch 38/300, seasonal_3 Loss: 0.2036 | 0.1656
Epoch 39/300, seasonal_3 Loss: 0.2028 | 0.1627
Epoch 40/300, seasonal_3 Loss: 0.2013 | 0.1630
Epoch 41/300, seasonal_3 Loss: 0.2004 | 0.1606
Epoch 42/300, seasonal_3 Loss: 0.1992 | 0.1595
Epoch 43/300, seasonal_3 Loss: 0.1981 | 0.1586
Epoch 44/300, seasonal_3 Loss: 0.1976 | 0.1579
Epoch 45/300, seasonal_3 Loss: 0.1961 | 0.1574
Epoch 46/300, seasonal_3 Loss: 0.1949 | 0.1555
Epoch 47/300, seasonal_3 Loss: 0.1945 | 0.1555
Epoch 48/300, seasonal_3 Loss: 0.1942 | 0.1548
Epoch 49/300, seasonal_3 Loss: 0.1924 | 0.1529
Epoch 50/300, seasonal_3 Loss: 0.1916 | 0.1524
Epoch 51/300, seasonal_3 Loss: 0.1919 | 0.1518
Epoch 52/300, seasonal_3 Loss: 0.1915 | 0.1507
Epoch 53/300, seasonal_3 Loss: 0.1898 | 0.1499
Epoch 54/300, seasonal_3 Loss: 0.1901 | 0.1496
Epoch 55/300, seasonal_3 Loss: 0.1891 | 0.1494
Epoch 56/300, seasonal_3 Loss: 0.1884 | 0.1496
Epoch 57/300, seasonal_3 Loss: 0.1885 | 0.1483
Epoch 58/300, seasonal_3 Loss: 0.1875 | 0.1480
Epoch 59/300, seasonal_3 Loss: 0.1875 | 0.1477
Epoch 60/300, seasonal_3 Loss: 0.1869 | 0.1475
Epoch 61/300, seasonal_3 Loss: 0.1868 | 0.1469
Epoch 62/300, seasonal_3 Loss: 0.1865 | 0.1467
Epoch 63/300, seasonal_3 Loss: 0.1854 | 0.1463
Epoch 64/300, seasonal_3 Loss: 0.1849 | 0.1454
Epoch 65/300, seasonal_3 Loss: 0.1847 | 0.1459
Epoch 66/300, seasonal_3 Loss: 0.1841 | 0.1454
Epoch 67/300, seasonal_3 Loss: 0.1846 | 0.1447
Epoch 68/300, seasonal_3 Loss: 0.1836 | 0.1443
Epoch 69/300, seasonal_3 Loss: 0.1841 | 0.1442
Epoch 70/300, seasonal_3 Loss: 0.1833 | 0.1443
Epoch 71/300, seasonal_3 Loss: 0.1829 | 0.1435
Epoch 72/300, seasonal_3 Loss: 0.1831 | 0.1432
Epoch 73/300, seasonal_3 Loss: 0.1825 | 0.1434
Epoch 74/300, seasonal_3 Loss: 0.1816 | 0.1436
Epoch 75/300, seasonal_3 Loss: 0.1823 | 0.1435
Epoch 76/300, seasonal_3 Loss: 0.1818 | 0.1432
Epoch 77/300, seasonal_3 Loss: 0.1821 | 0.1432
Epoch 78/300, seasonal_3 Loss: 0.1819 | 0.1431
Epoch 79/300, seasonal_3 Loss: 0.1826 | 0.1427
Epoch 80/300, seasonal_3 Loss: 0.1821 | 0.1424
Epoch 81/300, seasonal_3 Loss: 0.1816 | 0.1425
Epoch 82/300, seasonal_3 Loss: 0.1819 | 0.1424
Epoch 83/300, seasonal_3 Loss: 0.1816 | 0.1421
Epoch 84/300, seasonal_3 Loss: 0.1804 | 0.1419
Epoch 85/300, seasonal_3 Loss: 0.1819 | 0.1417
Epoch 86/300, seasonal_3 Loss: 0.1809 | 0.1417
Epoch 87/300, seasonal_3 Loss: 0.1806 | 0.1414
Epoch 88/300, seasonal_3 Loss: 0.1816 | 0.1414
Epoch 89/300, seasonal_3 Loss: 0.1800 | 0.1414
Epoch 90/300, seasonal_3 Loss: 0.1799 | 0.1411
Epoch 91/300, seasonal_3 Loss: 0.1797 | 0.1411
Epoch 92/300, seasonal_3 Loss: 0.1807 | 0.1409
Epoch 93/300, seasonal_3 Loss: 0.1814 | 0.1409
Epoch 94/300, seasonal_3 Loss: 0.1797 | 0.1409
Epoch 95/300, seasonal_3 Loss: 0.1807 | 0.1408
Epoch 96/300, seasonal_3 Loss: 0.1794 | 0.1408
Epoch 97/300, seasonal_3 Loss: 0.1800 | 0.1407
Epoch 98/300, seasonal_3 Loss: 0.1806 | 0.1405
Epoch 99/300, seasonal_3 Loss: 0.1797 | 0.1404
Epoch 100/300, seasonal_3 Loss: 0.1792 | 0.1404
Epoch 101/300, seasonal_3 Loss: 0.1804 | 0.1404
Epoch 102/300, seasonal_3 Loss: 0.1798 | 0.1402
Epoch 103/300, seasonal_3 Loss: 0.1791 | 0.1401
Epoch 104/300, seasonal_3 Loss: 0.1803 | 0.1401
Epoch 105/300, seasonal_3 Loss: 0.1797 | 0.1400
Epoch 106/300, seasonal_3 Loss: 0.1792 | 0.1401
Epoch 107/300, seasonal_3 Loss: 0.1790 | 0.1401
Epoch 108/300, seasonal_3 Loss: 0.1799 | 0.1400
Epoch 109/300, seasonal_3 Loss: 0.1788 | 0.1401
Epoch 110/300, seasonal_3 Loss: 0.1789 | 0.1401
Epoch 111/300, seasonal_3 Loss: 0.1787 | 0.1400
Epoch 112/300, seasonal_3 Loss: 0.1793 | 0.1400
Epoch 113/300, seasonal_3 Loss: 0.1791 | 0.1400
Epoch 114/300, seasonal_3 Loss: 0.1792 | 0.1400
Epoch 115/300, seasonal_3 Loss: 0.1791 | 0.1400
Epoch 116/300, seasonal_3 Loss: 0.1804 | 0.1400
Epoch 117/300, seasonal_3 Loss: 0.1789 | 0.1399
Epoch 118/300, seasonal_3 Loss: 0.1791 | 0.1400
Epoch 119/300, seasonal_3 Loss: 0.1787 | 0.1399
Epoch 120/300, seasonal_3 Loss: 0.1789 | 0.1399
Epoch 121/300, seasonal_3 Loss: 0.1790 | 0.1399
Epoch 122/300, seasonal_3 Loss: 0.1791 | 0.1399
Epoch 123/300, seasonal_3 Loss: 0.1792 | 0.1399
Epoch 124/300, seasonal_3 Loss: 0.1784 | 0.1399
Epoch 125/300, seasonal_3 Loss: 0.1790 | 0.1399
Epoch 126/300, seasonal_3 Loss: 0.1792 | 0.1398
Epoch 127/300, seasonal_3 Loss: 0.1791 | 0.1398
Epoch 128/300, seasonal_3 Loss: 0.1787 | 0.1398
Epoch 129/300, seasonal_3 Loss: 0.1781 | 0.1398
Epoch 130/300, seasonal_3 Loss: 0.1787 | 0.1398
Epoch 131/300, seasonal_3 Loss: 0.1789 | 0.1397
Epoch 132/300, seasonal_3 Loss: 0.1787 | 0.1397
Epoch 133/300, seasonal_3 Loss: 0.1787 | 0.1397
Epoch 134/300, seasonal_3 Loss: 0.1791 | 0.1397
Epoch 135/300, seasonal_3 Loss: 0.1790 | 0.1397
Epoch 136/300, seasonal_3 Loss: 0.1793 | 0.1397
Epoch 137/300, seasonal_3 Loss: 0.1787 | 0.1396
Epoch 138/300, seasonal_3 Loss: 0.1791 | 0.1397
Epoch 139/300, seasonal_3 Loss: 0.1791 | 0.1396
Epoch 140/300, seasonal_3 Loss: 0.1784 | 0.1396
Epoch 141/300, seasonal_3 Loss: 0.1792 | 0.1396
Epoch 142/300, seasonal_3 Loss: 0.1785 | 0.1396
Epoch 143/300, seasonal_3 Loss: 0.1785 | 0.1396
Epoch 144/300, seasonal_3 Loss: 0.1789 | 0.1396
Epoch 145/300, seasonal_3 Loss: 0.1790 | 0.1396
Epoch 146/300, seasonal_3 Loss: 0.1789 | 0.1396
Epoch 147/300, seasonal_3 Loss: 0.1790 | 0.1396
Epoch 148/300, seasonal_3 Loss: 0.1791 | 0.1396
Epoch 149/300, seasonal_3 Loss: 0.1788 | 0.1396
Epoch 150/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 151/300, seasonal_3 Loss: 0.1791 | 0.1395
Epoch 152/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 153/300, seasonal_3 Loss: 0.1794 | 0.1395
Epoch 154/300, seasonal_3 Loss: 0.1783 | 0.1395
Epoch 155/300, seasonal_3 Loss: 0.1783 | 0.1395
Epoch 156/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 157/300, seasonal_3 Loss: 0.1786 | 0.1395
Epoch 158/300, seasonal_3 Loss: 0.1780 | 0.1395
Epoch 159/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 160/300, seasonal_3 Loss: 0.1784 | 0.1395
Epoch 161/300, seasonal_3 Loss: 0.1785 | 0.1395
Epoch 162/300, seasonal_3 Loss: 0.1790 | 0.1395
Epoch 163/300, seasonal_3 Loss: 0.1786 | 0.1395
Epoch 164/300, seasonal_3 Loss: 0.1791 | 0.1395
Epoch 165/300, seasonal_3 Loss: 0.1784 | 0.1395
Epoch 166/300, seasonal_3 Loss: 0.1786 | 0.1395
Epoch 167/300, seasonal_3 Loss: 0.1792 | 0.1395
Epoch 168/300, seasonal_3 Loss: 0.1792 | 0.1395
Epoch 169/300, seasonal_3 Loss: 0.1785 | 0.1395
Epoch 170/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 171/300, seasonal_3 Loss: 0.1790 | 0.1395
Epoch 172/300, seasonal_3 Loss: 0.1783 | 0.1395
Epoch 173/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 174/300, seasonal_3 Loss: 0.1792 | 0.1395
Epoch 175/300, seasonal_3 Loss: 0.1785 | 0.1395
Epoch 176/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 177/300, seasonal_3 Loss: 0.1778 | 0.1395
Epoch 178/300, seasonal_3 Loss: 0.1791 | 0.1395
Epoch 179/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 180/300, seasonal_3 Loss: 0.1782 | 0.1395
Epoch 181/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 182/300, seasonal_3 Loss: 0.1782 | 0.1395
Epoch 183/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 184/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 185/300, seasonal_3 Loss: 0.1793 | 0.1395
Epoch 186/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 187/300, seasonal_3 Loss: 0.1790 | 0.1395
Epoch 188/300, seasonal_3 Loss: 0.1790 | 0.1395
Epoch 189/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 190/300, seasonal_3 Loss: 0.1786 | 0.1395
Epoch 191/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 192/300, seasonal_3 Loss: 0.1783 | 0.1395
Epoch 193/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 194/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 195/300, seasonal_3 Loss: 0.1786 | 0.1395
Epoch 196/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 197/300, seasonal_3 Loss: 0.1792 | 0.1395
Epoch 198/300, seasonal_3 Loss: 0.1783 | 0.1395
Epoch 199/300, seasonal_3 Loss: 0.1784 | 0.1395
Epoch 200/300, seasonal_3 Loss: 0.1785 | 0.1395
Epoch 201/300, seasonal_3 Loss: 0.1787 | 0.1395
Epoch 202/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 203/300, seasonal_3 Loss: 0.1790 | 0.1395
Epoch 204/300, seasonal_3 Loss: 0.1781 | 0.1395
Epoch 205/300, seasonal_3 Loss: 0.1791 | 0.1395
Epoch 206/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 207/300, seasonal_3 Loss: 0.1789 | 0.1395
Epoch 208/300, seasonal_3 Loss: 0.1779 | 0.1395
Epoch 209/300, seasonal_3 Loss: 0.1793 | 0.1395
Epoch 210/300, seasonal_3 Loss: 0.1785 | 0.1395
Epoch 211/300, seasonal_3 Loss: 0.1788 | 0.1395
Epoch 212/300, seasonal_3 Loss: 0.1792 | 0.1395
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 16, 'train_rates': 0.9854601338434934, 'learning_rate': 0.00032830707345255205, 'batch_size': 81, 'step_size': 9, 'gamma': 0.879491415483277}
Epoch 1/300, resid Loss: 0.9585 | 0.8047
Epoch 2/300, resid Loss: 0.9321 | 0.6914
Epoch 3/300, resid Loss: 0.7326 | 0.7153
Epoch 4/300, resid Loss: 0.6493 | 0.6476
Epoch 5/300, resid Loss: 0.5983 | 0.5696
Epoch 6/300, resid Loss: 0.6171 | 0.4904
Epoch 7/300, resid Loss: 0.5412 | 0.4791
Epoch 8/300, resid Loss: 0.5068 | 0.4352
Epoch 9/300, resid Loss: 0.5363 | 0.4455
Epoch 10/300, resid Loss: 0.4883 | 0.4523
Epoch 11/300, resid Loss: 0.4796 | 0.3785
Epoch 12/300, resid Loss: 0.4695 | 0.4904
Epoch 13/300, resid Loss: 0.4647 | 0.3573
Epoch 14/300, resid Loss: 0.4689 | 0.3997
Epoch 15/300, resid Loss: 0.5081 | 0.3386
Epoch 16/300, resid Loss: 0.4211 | 0.3857
Epoch 17/300, resid Loss: 0.3801 | 0.3045
Epoch 18/300, resid Loss: 0.3459 | 0.2777
Epoch 19/300, resid Loss: 0.3859 | 0.3030
Epoch 20/300, resid Loss: 0.3367 | 0.2773
Epoch 21/300, resid Loss: 0.3388 | 0.2680
Epoch 22/300, resid Loss: 0.3668 | 0.2613
Epoch 23/300, resid Loss: 0.3314 | 0.2500
Epoch 24/300, resid Loss: 0.3106 | 0.2203
Epoch 25/300, resid Loss: 0.3234 | 0.2222
Epoch 26/300, resid Loss: 0.3122 | 0.2054
Epoch 27/300, resid Loss: 0.2932 | 0.2512
Epoch 28/300, resid Loss: 0.2793 | 0.1895
Epoch 29/300, resid Loss: 0.2528 | 0.2017
Epoch 30/300, resid Loss: 0.2598 | 0.1814
Epoch 31/300, resid Loss: 0.2822 | 0.2745
Epoch 32/300, resid Loss: 0.2519 | 0.1929
Epoch 33/300, resid Loss: 0.2587 | 0.1813
Epoch 34/300, resid Loss: 0.2585 | 0.1868
Epoch 35/300, resid Loss: 0.2479 | 0.1697
Epoch 36/300, resid Loss: 0.2587 | 0.1847
Epoch 37/300, resid Loss: 0.2367 | 0.1589
Epoch 38/300, resid Loss: 0.2271 | 0.1695
Epoch 39/300, resid Loss: 0.2199 | 0.1557
Epoch 40/300, resid Loss: 0.2154 | 0.1609
Epoch 41/300, resid Loss: 0.2116 | 0.1518
Epoch 42/300, resid Loss: 0.2145 | 0.1623
Epoch 43/300, resid Loss: 0.2043 | 0.1476
Epoch 44/300, resid Loss: 0.2015 | 0.1541
Epoch 45/300, resid Loss: 0.1972 | 0.1474
Epoch 46/300, resid Loss: 0.1975 | 0.1480
Epoch 47/300, resid Loss: 0.1974 | 0.1444
Epoch 48/300, resid Loss: 0.1900 | 0.1480
Epoch 49/300, resid Loss: 0.1894 | 0.1402
Epoch 50/300, resid Loss: 0.1920 | 0.1510
Epoch 51/300, resid Loss: 0.1906 | 0.1420
Epoch 52/300, resid Loss: 0.1884 | 0.1402
Epoch 53/300, resid Loss: 0.1858 | 0.1426
Epoch 54/300, resid Loss: 0.1878 | 0.1419
Epoch 55/300, resid Loss: 0.1890 | 0.1387
Epoch 56/300, resid Loss: 0.1794 | 0.1408
Epoch 57/300, resid Loss: 0.1793 | 0.1342
Epoch 58/300, resid Loss: 0.1845 | 0.1458
Epoch 59/300, resid Loss: 0.1811 | 0.1355
Epoch 60/300, resid Loss: 0.1771 | 0.1338
Epoch 61/300, resid Loss: 0.1738 | 0.1368
Epoch 62/300, resid Loss: 0.1733 | 0.1352
Epoch 63/300, resid Loss: 0.1726 | 0.1334
Epoch 64/300, resid Loss: 0.1680 | 0.1357
Epoch 65/300, resid Loss: 0.1687 | 0.1307
Epoch 66/300, resid Loss: 0.1681 | 0.1349
Epoch 67/300, resid Loss: 0.1671 | 0.1318
Epoch 68/300, resid Loss: 0.1653 | 0.1323
Epoch 69/300, resid Loss: 0.1634 | 0.1334
Epoch 70/300, resid Loss: 0.1616 | 0.1309
Epoch 71/300, resid Loss: 0.1620 | 0.1317
Epoch 72/300, resid Loss: 0.1602 | 0.1319
Epoch 73/300, resid Loss: 0.1606 | 0.1284
Epoch 74/300, resid Loss: 0.1587 | 0.1306
Epoch 75/300, resid Loss: 0.1583 | 0.1283
Epoch 76/300, resid Loss: 0.1567 | 0.1287
Epoch 77/300, resid Loss: 0.1560 | 0.1293
Epoch 78/300, resid Loss: 0.1549 | 0.1278
Epoch 79/300, resid Loss: 0.1549 | 0.1287
Epoch 80/300, resid Loss: 0.1538 | 0.1278
Epoch 81/300, resid Loss: 0.1536 | 0.1276
Epoch 82/300, resid Loss: 0.1526 | 0.1278
Epoch 83/300, resid Loss: 0.1515 | 0.1255
Epoch 84/300, resid Loss: 0.1512 | 0.1264
Epoch 85/300, resid Loss: 0.1508 | 0.1258
Epoch 86/300, resid Loss: 0.1497 | 0.1255
Epoch 87/300, resid Loss: 0.1491 | 0.1239
Epoch 88/300, resid Loss: 0.1485 | 0.1245
Epoch 89/300, resid Loss: 0.1482 | 0.1260
Epoch 90/300, resid Loss: 0.1472 | 0.1243
Epoch 91/300, resid Loss: 0.1479 | 0.1249
Epoch 92/300, resid Loss: 0.1467 | 0.1240
Epoch 93/300, resid Loss: 0.1456 | 0.1237
Epoch 94/300, resid Loss: 0.1450 | 0.1235
Epoch 95/300, resid Loss: 0.1450 | 0.1229
Epoch 96/300, resid Loss: 0.1447 | 0.1218
Epoch 97/300, resid Loss: 0.1442 | 0.1218
Epoch 98/300, resid Loss: 0.1433 | 0.1216
Epoch 99/300, resid Loss: 0.1433 | 0.1219
Epoch 100/300, resid Loss: 0.1421 | 0.1221
Epoch 101/300, resid Loss: 0.1418 | 0.1226
Epoch 102/300, resid Loss: 0.1421 | 0.1215
Epoch 103/300, resid Loss: 0.1410 | 0.1202
Epoch 104/300, resid Loss: 0.1412 | 0.1213
Epoch 105/300, resid Loss: 0.1414 | 0.1205
Epoch 106/300, resid Loss: 0.1407 | 0.1198
Epoch 107/300, resid Loss: 0.1400 | 0.1199
Epoch 108/300, resid Loss: 0.1398 | 0.1192
Epoch 109/300, resid Loss: 0.1388 | 0.1200
Epoch 110/300, resid Loss: 0.1388 | 0.1199
Epoch 111/300, resid Loss: 0.1392 | 0.1193
Epoch 112/300, resid Loss: 0.1381 | 0.1195
Epoch 113/300, resid Loss: 0.1378 | 0.1194
Epoch 114/300, resid Loss: 0.1374 | 0.1193
Epoch 115/300, resid Loss: 0.1377 | 0.1186
Epoch 116/300, resid Loss: 0.1369 | 0.1185
Epoch 117/300, resid Loss: 0.1363 | 0.1181
Epoch 118/300, resid Loss: 0.1363 | 0.1184
Epoch 119/300, resid Loss: 0.1360 | 0.1190
Epoch 120/300, resid Loss: 0.1363 | 0.1183
Epoch 121/300, resid Loss: 0.1363 | 0.1176
Epoch 122/300, resid Loss: 0.1358 | 0.1183
Epoch 123/300, resid Loss: 0.1354 | 0.1174
Epoch 124/300, resid Loss: 0.1355 | 0.1171
Epoch 125/300, resid Loss: 0.1346 | 0.1173
Epoch 126/300, resid Loss: 0.1355 | 0.1174
Epoch 127/300, resid Loss: 0.1349 | 0.1160
Epoch 128/300, resid Loss: 0.1338 | 0.1158
Epoch 129/300, resid Loss: 0.1340 | 0.1158
Epoch 130/300, resid Loss: 0.1344 | 0.1164
Epoch 131/300, resid Loss: 0.1337 | 0.1156
Epoch 132/300, resid Loss: 0.1335 | 0.1158
Epoch 133/300, resid Loss: 0.1338 | 0.1163
Epoch 134/300, resid Loss: 0.1324 | 0.1162
Epoch 135/300, resid Loss: 0.1332 | 0.1154
Epoch 136/300, resid Loss: 0.1325 | 0.1151
Epoch 137/300, resid Loss: 0.1335 | 0.1153
Epoch 138/300, resid Loss: 0.1319 | 0.1149
Epoch 139/300, resid Loss: 0.1320 | 0.1150
Epoch 140/300, resid Loss: 0.1327 | 0.1145
Epoch 141/300, resid Loss: 0.1322 | 0.1148
Epoch 142/300, resid Loss: 0.1319 | 0.1150
Epoch 143/300, resid Loss: 0.1318 | 0.1150
Epoch 144/300, resid Loss: 0.1320 | 0.1144
Epoch 145/300, resid Loss: 0.1314 | 0.1141
Epoch 146/300, resid Loss: 0.1318 | 0.1144
Epoch 147/300, resid Loss: 0.1313 | 0.1140
Epoch 148/300, resid Loss: 0.1318 | 0.1138
Epoch 149/300, resid Loss: 0.1316 | 0.1144
Epoch 150/300, resid Loss: 0.1305 | 0.1143
Epoch 151/300, resid Loss: 0.1308 | 0.1145
Epoch 152/300, resid Loss: 0.1307 | 0.1137
Epoch 153/300, resid Loss: 0.1294 | 0.1134
Epoch 154/300, resid Loss: 0.1304 | 0.1135
Epoch 155/300, resid Loss: 0.1304 | 0.1135
Epoch 156/300, resid Loss: 0.1306 | 0.1138
Epoch 157/300, resid Loss: 0.1311 | 0.1137
Epoch 158/300, resid Loss: 0.1297 | 0.1137
Epoch 159/300, resid Loss: 0.1301 | 0.1134
Epoch 160/300, resid Loss: 0.1292 | 0.1130
Epoch 161/300, resid Loss: 0.1298 | 0.1131
Epoch 162/300, resid Loss: 0.1300 | 0.1129
Epoch 163/300, resid Loss: 0.1306 | 0.1129
Epoch 164/300, resid Loss: 0.1297 | 0.1134
Epoch 165/300, resid Loss: 0.1300 | 0.1136
Epoch 166/300, resid Loss: 0.1293 | 0.1135
Epoch 167/300, resid Loss: 0.1287 | 0.1132
Epoch 168/300, resid Loss: 0.1295 | 0.1132
Epoch 169/300, resid Loss: 0.1292 | 0.1130
Epoch 170/300, resid Loss: 0.1290 | 0.1130
Epoch 171/300, resid Loss: 0.1291 | 0.1130
Epoch 172/300, resid Loss: 0.1292 | 0.1129
Epoch 173/300, resid Loss: 0.1287 | 0.1128
Epoch 174/300, resid Loss: 0.1291 | 0.1126
Epoch 175/300, resid Loss: 0.1281 | 0.1128
Epoch 176/300, resid Loss: 0.1285 | 0.1127
Epoch 177/300, resid Loss: 0.1284 | 0.1129
Epoch 178/300, resid Loss: 0.1283 | 0.1129
Epoch 179/300, resid Loss: 0.1283 | 0.1132
Epoch 180/300, resid Loss: 0.1288 | 0.1131
Epoch 181/300, resid Loss: 0.1288 | 0.1125
Epoch 182/300, resid Loss: 0.1275 | 0.1127
Epoch 183/300, resid Loss: 0.1279 | 0.1127
Epoch 184/300, resid Loss: 0.1289 | 0.1128
Epoch 185/300, resid Loss: 0.1279 | 0.1129
Epoch 186/300, resid Loss: 0.1286 | 0.1129
Epoch 187/300, resid Loss: 0.1285 | 0.1129
Epoch 188/300, resid Loss: 0.1290 | 0.1129
Epoch 189/300, resid Loss: 0.1279 | 0.1127
Epoch 190/300, resid Loss: 0.1284 | 0.1127
Epoch 191/300, resid Loss: 0.1273 | 0.1127
Epoch 192/300, resid Loss: 0.1276 | 0.1125
Epoch 193/300, resid Loss: 0.1275 | 0.1123
Epoch 194/300, resid Loss: 0.1274 | 0.1123
Epoch 195/300, resid Loss: 0.1277 | 0.1122
Epoch 196/300, resid Loss: 0.1278 | 0.1122
Epoch 197/300, resid Loss: 0.1279 | 0.1123
Epoch 198/300, resid Loss: 0.1283 | 0.1121
Epoch 199/300, resid Loss: 0.1280 | 0.1123
Epoch 200/300, resid Loss: 0.1271 | 0.1124
Epoch 201/300, resid Loss: 0.1276 | 0.1123
Epoch 202/300, resid Loss: 0.1279 | 0.1121
Epoch 203/300, resid Loss: 0.1273 | 0.1120
Epoch 204/300, resid Loss: 0.1275 | 0.1120
Epoch 205/300, resid Loss: 0.1281 | 0.1120
Epoch 206/300, resid Loss: 0.1273 | 0.1119
Epoch 207/300, resid Loss: 0.1276 | 0.1119
Epoch 208/300, resid Loss: 0.1274 | 0.1118
Epoch 209/300, resid Loss: 0.1278 | 0.1119
Epoch 210/300, resid Loss: 0.1276 | 0.1120
Epoch 211/300, resid Loss: 0.1274 | 0.1117
Epoch 212/300, resid Loss: 0.1274 | 0.1117
Epoch 213/300, resid Loss: 0.1277 | 0.1116
Epoch 214/300, resid Loss: 0.1274 | 0.1117
Epoch 215/300, resid Loss: 0.1278 | 0.1116
Epoch 216/300, resid Loss: 0.1274 | 0.1115
Epoch 217/300, resid Loss: 0.1270 | 0.1116
Epoch 218/300, resid Loss: 0.1273 | 0.1116
Epoch 219/300, resid Loss: 0.1279 | 0.1117
Epoch 220/300, resid Loss: 0.1270 | 0.1117
Epoch 221/300, resid Loss: 0.1269 | 0.1116
Epoch 222/300, resid Loss: 0.1272 | 0.1117
Epoch 223/300, resid Loss: 0.1277 | 0.1116
Epoch 224/300, resid Loss: 0.1270 | 0.1117
Epoch 225/300, resid Loss: 0.1267 | 0.1117
Epoch 226/300, resid Loss: 0.1270 | 0.1115
Epoch 227/300, resid Loss: 0.1275 | 0.1115
Epoch 228/300, resid Loss: 0.1265 | 0.1115
Epoch 229/300, resid Loss: 0.1272 | 0.1115
Epoch 230/300, resid Loss: 0.1273 | 0.1116
Epoch 231/300, resid Loss: 0.1279 | 0.1116
Epoch 232/300, resid Loss: 0.1268 | 0.1115
Epoch 233/300, resid Loss: 0.1277 | 0.1115
Epoch 234/300, resid Loss: 0.1268 | 0.1116
Epoch 235/300, resid Loss: 0.1274 | 0.1115
Epoch 236/300, resid Loss: 0.1270 | 0.1114
Epoch 237/300, resid Loss: 0.1277 | 0.1114
Epoch 238/300, resid Loss: 0.1276 | 0.1114
Epoch 239/300, resid Loss: 0.1267 | 0.1114
Epoch 240/300, resid Loss: 0.1269 | 0.1114
Epoch 241/300, resid Loss: 0.1269 | 0.1114
Epoch 242/300, resid Loss: 0.1274 | 0.1114
Epoch 243/300, resid Loss: 0.1268 | 0.1114
Epoch 244/300, resid Loss: 0.1265 | 0.1114
Epoch 245/300, resid Loss: 0.1275 | 0.1114
Epoch 246/300, resid Loss: 0.1273 | 0.1115
Epoch 247/300, resid Loss: 0.1265 | 0.1115
Epoch 248/300, resid Loss: 0.1266 | 0.1115
Epoch 249/300, resid Loss: 0.1274 | 0.1115
Epoch 250/300, resid Loss: 0.1267 | 0.1115
Epoch 251/300, resid Loss: 0.1272 | 0.1114
Epoch 252/300, resid Loss: 0.1270 | 0.1113
Epoch 253/300, resid Loss: 0.1275 | 0.1113
Epoch 254/300, resid Loss: 0.1267 | 0.1113
Epoch 255/300, resid Loss: 0.1278 | 0.1113
Epoch 256/300, resid Loss: 0.1268 | 0.1113
Epoch 257/300, resid Loss: 0.1274 | 0.1113
Epoch 258/300, resid Loss: 0.1269 | 0.1112
Epoch 259/300, resid Loss: 0.1266 | 0.1112
Epoch 260/300, resid Loss: 0.1270 | 0.1112
Epoch 261/300, resid Loss: 0.1270 | 0.1112
Epoch 262/300, resid Loss: 0.1271 | 0.1112
Epoch 263/300, resid Loss: 0.1265 | 0.1112
Epoch 264/300, resid Loss: 0.1280 | 0.1112
Epoch 265/300, resid Loss: 0.1269 | 0.1112
Epoch 266/300, resid Loss: 0.1260 | 0.1112
Epoch 267/300, resid Loss: 0.1265 | 0.1112
Epoch 268/300, resid Loss: 0.1268 | 0.1113
Epoch 269/300, resid Loss: 0.1273 | 0.1112
Epoch 270/300, resid Loss: 0.1278 | 0.1112
Epoch 271/300, resid Loss: 0.1256 | 0.1112
Epoch 272/300, resid Loss: 0.1273 | 0.1112
Epoch 273/300, resid Loss: 0.1273 | 0.1112
Epoch 274/300, resid Loss: 0.1269 | 0.1112
Epoch 275/300, resid Loss: 0.1273 | 0.1112
Epoch 276/300, resid Loss: 0.1269 | 0.1113
Epoch 277/300, resid Loss: 0.1266 | 0.1113
Epoch 278/300, resid Loss: 0.1266 | 0.1113
Epoch 279/300, resid Loss: 0.1276 | 0.1113
Epoch 280/300, resid Loss: 0.1273 | 0.1113
Epoch 281/300, resid Loss: 0.1276 | 0.1113
Epoch 282/300, resid Loss: 0.1269 | 0.1113
Epoch 283/300, resid Loss: 0.1263 | 0.1113
Epoch 284/300, resid Loss: 0.1267 | 0.1113
Epoch 285/300, resid Loss: 0.1267 | 0.1113
Epoch 286/300, resid Loss: 0.1264 | 0.1112
Epoch 287/300, resid Loss: 0.1269 | 0.1112
Epoch 288/300, resid Loss: 0.1268 | 0.1113
Epoch 289/300, resid Loss: 0.1273 | 0.1112
Epoch 290/300, resid Loss: 0.1269 | 0.1112
Epoch 291/300, resid Loss: 0.1275 | 0.1112
Epoch 292/300, resid Loss: 0.1270 | 0.1113
Epoch 293/300, resid Loss: 0.1271 | 0.1113
Epoch 294/300, resid Loss: 0.1261 | 0.1113
Epoch 295/300, resid Loss: 0.1267 | 0.1113
Epoch 296/300, resid Loss: 0.1269 | 0.1112
Epoch 297/300, resid Loss: 0.1257 | 0.1112
Epoch 298/300, resid Loss: 0.1270 | 0.1112
Epoch 299/300, resid Loss: 0.1274 | 0.1112
Epoch 300/300, resid Loss: 0.1269 | 0.1112
Runtime (seconds): 3328.806239128113
9.472860256449676e-05
[101.71832]
[3.6491811]
[-1.804104]
[0.6071721]
[-2.4889026]
[-3.1186886]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 15.500181421870366
RMSE: 3.9370269775390625
MAE: 3.9370269775390625
R-squared: nan
[98.56297]
