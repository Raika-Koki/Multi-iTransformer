ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-11 02:37:23,532][0m A new study created in memory with name: no-name-3a09222f-c811-47a4-9728-e343d4edf934[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-11 02:37:58,763][0m Trial 0 finished with value: 0.17263759405755286 and parameters: {'observation_period_num': 223, 'train_rates': 0.7792064616552331, 'learning_rate': 0.00023727585164424323, 'batch_size': 148, 'step_size': 7, 'gamma': 0.7806257695931682}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:38:43,651][0m Trial 1 finished with value: 0.3153619996315457 and parameters: {'observation_period_num': 99, 'train_rates': 0.86476828118485, 'learning_rate': 1.1241663242987283e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8907650829011672}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:39:16,770][0m Trial 2 finished with value: 0.4416793182001009 and parameters: {'observation_period_num': 121, 'train_rates': 0.8073308195979498, 'learning_rate': 8.549627895244039e-06, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8600979247604448}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:39:43,093][0m Trial 3 finished with value: 0.596897242961933 and parameters: {'observation_period_num': 53, 'train_rates': 0.6275671647498663, 'learning_rate': 1.7751393689927612e-05, 'batch_size': 174, 'step_size': 15, 'gamma': 0.7540469894297581}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:40:12,297][0m Trial 4 finished with value: 0.26925230921096316 and parameters: {'observation_period_num': 66, 'train_rates': 0.603605838179955, 'learning_rate': 6.200326481082795e-05, 'batch_size': 153, 'step_size': 15, 'gamma': 0.9566511698520882}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:40:45,078][0m Trial 5 finished with value: 0.30119139353434243 and parameters: {'observation_period_num': 225, 'train_rates': 0.7910685364732781, 'learning_rate': 7.670384601976992e-05, 'batch_size': 152, 'step_size': 2, 'gamma': 0.9349883701663368}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:41:17,273][0m Trial 6 finished with value: 1.03425071627966 and parameters: {'observation_period_num': 36, 'train_rates': 0.6926283213421811, 'learning_rate': 3.638084133305908e-06, 'batch_size': 148, 'step_size': 3, 'gamma': 0.8598579470634389}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:41:40,405][0m Trial 7 finished with value: 1.7617700099945068 and parameters: {'observation_period_num': 140, 'train_rates': 0.9360524431375263, 'learning_rate': 1.0715510850973323e-06, 'batch_size': 245, 'step_size': 7, 'gamma': 0.9373512074050533}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:42:24,354][0m Trial 8 finished with value: 0.25776960892690515 and parameters: {'observation_period_num': 91, 'train_rates': 0.8738768343016836, 'learning_rate': 1.978855522800233e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8031619919611658}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:43:28,227][0m Trial 9 finished with value: 0.9924551065558586 and parameters: {'observation_period_num': 164, 'train_rates': 0.6161853567526198, 'learning_rate': 5.01386935332291e-06, 'batch_size': 63, 'step_size': 2, 'gamma': 0.8212947122669165}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:46:23,081][0m Trial 10 finished with value: 0.2597460814374056 and parameters: {'observation_period_num': 244, 'train_rates': 0.7165585330422399, 'learning_rate': 0.0007013092546528292, 'batch_size': 24, 'step_size': 7, 'gamma': 0.7608676572266007}. Best is trial 0 with value: 0.17263759405755286.[0m
[32m[I 2025-01-11 02:47:18,382][0m Trial 11 finished with value: 0.08455023914575577 and parameters: {'observation_period_num': 205, 'train_rates': 0.979866711640696, 'learning_rate': 0.0003186210322497385, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8020299530819638}. Best is trial 11 with value: 0.08455023914575577.[0m
[32m[I 2025-01-11 02:48:25,332][0m Trial 12 finished with value: 0.08174709975719452 and parameters: {'observation_period_num': 190, 'train_rates': 0.9882218997922841, 'learning_rate': 0.0004247372023682946, 'batch_size': 85, 'step_size': 10, 'gamma': 0.7999220157340036}. Best is trial 12 with value: 0.08174709975719452.[0m
[32m[I 2025-01-11 02:49:34,405][0m Trial 13 finished with value: 0.0740017220377922 and parameters: {'observation_period_num': 185, 'train_rates': 0.9887363821887286, 'learning_rate': 0.0009603898999174158, 'batch_size': 81, 'step_size': 10, 'gamma': 0.820948353276809}. Best is trial 13 with value: 0.0740017220377922.[0m
[32m[I 2025-01-11 02:50:47,322][0m Trial 14 finished with value: 0.0755135789513588 and parameters: {'observation_period_num': 183, 'train_rates': 0.9751243542320162, 'learning_rate': 0.0009628389832703422, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8286195784131236}. Best is trial 13 with value: 0.0740017220377922.[0m
[32m[I 2025-01-11 02:52:47,233][0m Trial 15 finished with value: 0.07123072198096742 and parameters: {'observation_period_num': 171, 'train_rates': 0.9157377786697232, 'learning_rate': 0.000881215785291472, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8399334941219591}. Best is trial 15 with value: 0.07123072198096742.[0m
[32m[I 2025-01-11 02:57:52,140][0m Trial 16 finished with value: 0.06347057420330551 and parameters: {'observation_period_num': 171, 'train_rates': 0.9254997956335471, 'learning_rate': 5.9002861777899796e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.8896011662433632}. Best is trial 16 with value: 0.06347057420330551.[0m
[32m[I 2025-01-11 03:01:59,490][0m Trial 17 finished with value: 0.06872876194354735 and parameters: {'observation_period_num': 147, 'train_rates': 0.9053304359564458, 'learning_rate': 0.00012757346216204458, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9111281888188217}. Best is trial 16 with value: 0.06347057420330551.[0m
[32m[I 2025-01-11 03:04:48,355][0m Trial 18 finished with value: 0.07040471387247146 and parameters: {'observation_period_num': 140, 'train_rates': 0.8556298480641769, 'learning_rate': 0.0001000661682094504, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9000580906153574}. Best is trial 16 with value: 0.06347057420330551.[0m
[32m[I 2025-01-11 03:10:15,255][0m Trial 19 finished with value: 0.05978489213107482 and parameters: {'observation_period_num': 116, 'train_rates': 0.9177926884760065, 'learning_rate': 0.0001583497591694822, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9038908607766539}. Best is trial 19 with value: 0.05978489213107482.[0m
[32m[I 2025-01-11 03:10:43,577][0m Trial 20 finished with value: 0.16479337764885238 and parameters: {'observation_period_num': 10, 'train_rates': 0.8305558150469972, 'learning_rate': 4.201255013431036e-05, 'batch_size': 195, 'step_size': 1, 'gamma': 0.9698783827436498}. Best is trial 19 with value: 0.05978489213107482.[0m
[32m[I 2025-01-11 03:15:49,753][0m Trial 21 finished with value: 0.054045180473550274 and parameters: {'observation_period_num': 120, 'train_rates': 0.9106112615180986, 'learning_rate': 0.00015277058231323077, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9063953617932297}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:17:39,696][0m Trial 22 finished with value: 0.07847800140349281 and parameters: {'observation_period_num': 113, 'train_rates': 0.9469703804483209, 'learning_rate': 0.00018437503508967422, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8853322797369696}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:19:37,233][0m Trial 23 finished with value: 0.08027265566067958 and parameters: {'observation_period_num': 84, 'train_rates': 0.8862233967125279, 'learning_rate': 4.1239262025820694e-05, 'batch_size': 45, 'step_size': 6, 'gamma': 0.9232539020981027}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:23:11,939][0m Trial 24 finished with value: 0.08295522412148917 and parameters: {'observation_period_num': 153, 'train_rates': 0.9414139621365611, 'learning_rate': 0.00014515036006677664, 'batch_size': 25, 'step_size': 5, 'gamma': 0.8736985167592202}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:24:42,953][0m Trial 25 finished with value: 0.0940824111780295 and parameters: {'observation_period_num': 119, 'train_rates': 0.9081818642750203, 'learning_rate': 0.0003931659214869298, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9105028357353389}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:29:33,893][0m Trial 26 finished with value: 0.06172150528679291 and parameters: {'observation_period_num': 102, 'train_rates': 0.8315921612648001, 'learning_rate': 6.290443625640948e-05, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9468770447552508}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:31:43,271][0m Trial 27 finished with value: 0.27412140467676566 and parameters: {'observation_period_num': 68, 'train_rates': 0.7607985027722728, 'learning_rate': 2.271300436722453e-05, 'batch_size': 37, 'step_size': 1, 'gamma': 0.9895923283044511}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:32:34,827][0m Trial 28 finished with value: 0.08204517334827263 and parameters: {'observation_period_num': 80, 'train_rates': 0.8374810504791778, 'learning_rate': 0.00023785228096722367, 'batch_size': 104, 'step_size': 3, 'gamma': 0.9556907399365601}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:33:46,936][0m Trial 29 finished with value: 0.2361598009486324 and parameters: {'observation_period_num': 106, 'train_rates': 0.7601145949564061, 'learning_rate': 0.00024817920829892, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9312242688515713}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:34:09,679][0m Trial 30 finished with value: 0.17637658225638525 and parameters: {'observation_period_num': 128, 'train_rates': 0.8215226670388547, 'learning_rate': 0.00011195643957526749, 'batch_size': 242, 'step_size': 8, 'gamma': 0.9534684248056344}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:38:27,382][0m Trial 31 finished with value: 0.06922978566442445 and parameters: {'observation_period_num': 134, 'train_rates': 0.8939039961509287, 'learning_rate': 6.18678820898648e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.915500627177423}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:43:24,437][0m Trial 32 finished with value: 0.07563875618296256 and parameters: {'observation_period_num': 103, 'train_rates': 0.8549061757092166, 'learning_rate': 5.4446160142046346e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.8910840129601854}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:45:26,164][0m Trial 33 finished with value: 0.13839798299379127 and parameters: {'observation_period_num': 98, 'train_rates': 0.9551108164146983, 'learning_rate': 2.6870775681143688e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.8761495667292574}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:47:54,070][0m Trial 34 finished with value: 0.12900079909814607 and parameters: {'observation_period_num': 157, 'train_rates': 0.9221195069174647, 'learning_rate': 8.766766778766299e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8990946564333765}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:53:10,458][0m Trial 35 finished with value: 0.06540193381797836 and parameters: {'observation_period_num': 121, 'train_rates': 0.8828394033495591, 'learning_rate': 0.0001761054912202966, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9410559412720425}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:54:39,541][0m Trial 36 finished with value: 0.25956626639899794 and parameters: {'observation_period_num': 44, 'train_rates': 0.8095463695032002, 'learning_rate': 1.3804018825058683e-05, 'batch_size': 56, 'step_size': 5, 'gamma': 0.8449117300653674}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:55:10,952][0m Trial 37 finished with value: 0.4677262306213379 and parameters: {'observation_period_num': 69, 'train_rates': 0.9568995997131368, 'learning_rate': 3.589527186754647e-05, 'batch_size': 184, 'step_size': 3, 'gamma': 0.8655537992155686}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:56:06,380][0m Trial 38 finished with value: 0.49798852572074304 and parameters: {'observation_period_num': 168, 'train_rates': 0.860123630404932, 'learning_rate': 8.232366542050106e-06, 'batch_size': 93, 'step_size': 6, 'gamma': 0.901741768707967}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:56:34,022][0m Trial 39 finished with value: 0.09233556687831879 and parameters: {'observation_period_num': 132, 'train_rates': 0.9245045240428056, 'learning_rate': 0.0005186727110161521, 'batch_size': 216, 'step_size': 2, 'gamma': 0.9847606917524844}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:57:11,239][0m Trial 40 finished with value: 0.25383369602286476 and parameters: {'observation_period_num': 56, 'train_rates': 0.7812555104554247, 'learning_rate': 7.80106966984017e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9244395377590853}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 03:59:43,621][0m Trial 41 finished with value: 0.056196576425662406 and parameters: {'observation_period_num': 119, 'train_rates': 0.8853837732494426, 'learning_rate': 0.0001800688441424115, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9344108607172197}. Best is trial 21 with value: 0.054045180473550274.[0m
[32m[I 2025-01-11 04:02:17,458][0m Trial 42 finished with value: 0.05169213652417883 and parameters: {'observation_period_num': 108, 'train_rates': 0.8913535313684456, 'learning_rate': 0.00018766207829767458, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9450488208571559}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:04:43,501][0m Trial 43 finished with value: 0.1110265989217066 and parameters: {'observation_period_num': 91, 'train_rates': 0.8701591535857451, 'learning_rate': 0.00029591954407565764, 'batch_size': 36, 'step_size': 3, 'gamma': 0.9681186080557543}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:07:36,452][0m Trial 44 finished with value: 0.07076438523075554 and parameters: {'observation_period_num': 112, 'train_rates': 0.8432461298557, 'learning_rate': 0.00017615762245402807, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9397506618584327}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:08:51,244][0m Trial 45 finished with value: 0.11953110930193153 and parameters: {'observation_period_num': 96, 'train_rates': 0.896278070660253, 'learning_rate': 0.00013483893388172258, 'batch_size': 73, 'step_size': 1, 'gamma': 0.9479121207247565}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:10:52,750][0m Trial 46 finished with value: 0.7914066129488501 and parameters: {'observation_period_num': 81, 'train_rates': 0.6557450941916829, 'learning_rate': 1.2028566032625535e-06, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9672782291521772}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:12:19,326][0m Trial 47 finished with value: 0.10353370569646358 and parameters: {'observation_period_num': 127, 'train_rates': 0.8016779303965763, 'learning_rate': 0.0005473197700463713, 'batch_size': 56, 'step_size': 5, 'gamma': 0.9219720612680894}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:13:37,399][0m Trial 48 finished with value: 0.07051256819510124 and parameters: {'observation_period_num': 113, 'train_rates': 0.8750905600181453, 'learning_rate': 0.0003364281355404877, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9305484493391557}. Best is trial 42 with value: 0.05169213652417883.[0m
[32m[I 2025-01-11 04:15:24,780][0m Trial 49 finished with value: 0.06605320932035606 and parameters: {'observation_period_num': 86, 'train_rates': 0.8226410103101642, 'learning_rate': 0.00020474171279926396, 'batch_size': 47, 'step_size': 9, 'gamma': 0.975819582189156}. Best is trial 42 with value: 0.05169213652417883.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-11 04:15:24,790][0m A new study created in memory with name: no-name-369ddbff-8369-4639-bf35-84f793c8545e[0m
[32m[I 2025-01-11 04:15:50,841][0m Trial 0 finished with value: 1.5369316843963376 and parameters: {'observation_period_num': 15, 'train_rates': 0.8737096253504434, 'learning_rate': 7.335820183426507e-06, 'batch_size': 232, 'step_size': 7, 'gamma': 0.836664667360783}. Best is trial 0 with value: 1.5369316843963376.[0m
[32m[I 2025-01-11 04:16:25,125][0m Trial 1 finished with value: 0.4672649162156241 and parameters: {'observation_period_num': 138, 'train_rates': 0.8459789211713125, 'learning_rate': 2.0097748509651986e-05, 'batch_size': 152, 'step_size': 3, 'gamma': 0.8933482107204561}. Best is trial 1 with value: 0.4672649162156241.[0m
[32m[I 2025-01-11 04:19:38,236][0m Trial 2 finished with value: 0.5092075558684678 and parameters: {'observation_period_num': 110, 'train_rates': 0.6759464816233575, 'learning_rate': 2.390760643561724e-06, 'batch_size': 22, 'step_size': 14, 'gamma': 0.9798947417235964}. Best is trial 1 with value: 0.4672649162156241.[0m
[32m[I 2025-01-11 04:20:10,759][0m Trial 3 finished with value: 0.06827523032098118 and parameters: {'observation_period_num': 41, 'train_rates': 0.7932827968313432, 'learning_rate': 0.0005317128789989791, 'batch_size': 162, 'step_size': 3, 'gamma': 0.9504133108818273}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:20:41,030][0m Trial 4 finished with value: 0.25598898853944696 and parameters: {'observation_period_num': 171, 'train_rates': 0.9007210892053074, 'learning_rate': 4.030207995572227e-05, 'batch_size': 186, 'step_size': 10, 'gamma': 0.8496770414280694}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:22:11,980][0m Trial 5 finished with value: 0.34320897001571254 and parameters: {'observation_period_num': 37, 'train_rates': 0.7212874167799787, 'learning_rate': 1.2392084597126943e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8010104535871974}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:22:51,552][0m Trial 6 finished with value: 0.46482247862815856 and parameters: {'observation_period_num': 132, 'train_rates': 0.7783867554307238, 'learning_rate': 5.2222163822520845e-05, 'batch_size': 126, 'step_size': 2, 'gamma': 0.9362929391383913}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:25:51,024][0m Trial 7 finished with value: 0.16096737518906593 and parameters: {'observation_period_num': 184, 'train_rates': 0.8373615591616418, 'learning_rate': 1.8940957457657322e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8076612085916189}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:26:38,796][0m Trial 8 finished with value: 0.0746373057467831 and parameters: {'observation_period_num': 45, 'train_rates': 0.7999072274387413, 'learning_rate': 0.00014694706083115758, 'batch_size': 107, 'step_size': 9, 'gamma': 0.7999657285541144}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:27:05,209][0m Trial 9 finished with value: 0.4109321926985312 and parameters: {'observation_period_num': 133, 'train_rates': 0.6131522259098734, 'learning_rate': 2.6997772099117313e-05, 'batch_size': 165, 'step_size': 6, 'gamma': 0.9886575153886016}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:27:30,901][0m Trial 10 finished with value: 0.10209714621305466 and parameters: {'observation_period_num': 250, 'train_rates': 0.9591991021326071, 'learning_rate': 0.0009430780841210979, 'batch_size': 228, 'step_size': 4, 'gamma': 0.9067035299966477}. Best is trial 3 with value: 0.06827523032098118.[0m
Early stopping at epoch 48
[32m[I 2025-01-11 04:27:56,290][0m Trial 11 finished with value: 0.44486189696338013 and parameters: {'observation_period_num': 60, 'train_rates': 0.765754179676405, 'learning_rate': 0.00038126573076151796, 'batch_size': 99, 'step_size': 1, 'gamma': 0.7567750585729124}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:28:43,314][0m Trial 12 finished with value: 0.23192673395661748 and parameters: {'observation_period_num': 71, 'train_rates': 0.7286739177135746, 'learning_rate': 0.00017932642482764116, 'batch_size': 101, 'step_size': 9, 'gamma': 0.7624964462899406}. Best is trial 3 with value: 0.06827523032098118.[0m
[32m[I 2025-01-11 04:29:12,142][0m Trial 13 finished with value: 0.06280522758966868 and parameters: {'observation_period_num': 6, 'train_rates': 0.802545336560699, 'learning_rate': 0.00016489390380022503, 'batch_size': 195, 'step_size': 5, 'gamma': 0.9408537007841876}. Best is trial 13 with value: 0.06280522758966868.[0m
[32m[I 2025-01-11 04:29:42,256][0m Trial 14 finished with value: 0.04060303548570065 and parameters: {'observation_period_num': 10, 'train_rates': 0.9307862553509456, 'learning_rate': 0.0007503049398560546, 'batch_size': 201, 'step_size': 5, 'gamma': 0.9493272251459248}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:30:13,542][0m Trial 15 finished with value: 0.07004223018884659 and parameters: {'observation_period_num': 6, 'train_rates': 0.971801164397833, 'learning_rate': 0.00014026062186230318, 'batch_size': 203, 'step_size': 5, 'gamma': 0.9352276772373689}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:30:37,955][0m Trial 16 finished with value: 0.085668499140363 and parameters: {'observation_period_num': 87, 'train_rates': 0.9074009616870057, 'learning_rate': 0.00034499975134257477, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8986945629754781}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:31:06,215][0m Trial 17 finished with value: 0.0935171311844288 and parameters: {'observation_period_num': 94, 'train_rates': 0.9205445700796842, 'learning_rate': 0.0009842453735825223, 'batch_size': 209, 'step_size': 5, 'gamma': 0.966070144101645}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:31:39,941][0m Trial 18 finished with value: 0.26139897108078003 and parameters: {'observation_period_num': 18, 'train_rates': 0.9895947313424515, 'learning_rate': 8.277361214474532e-05, 'batch_size': 184, 'step_size': 1, 'gamma': 0.9182115238938757}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:31:59,336][0m Trial 19 finished with value: 0.31459916982089625 and parameters: {'observation_period_num': 237, 'train_rates': 0.6540066041002485, 'learning_rate': 0.0002490383417477322, 'batch_size': 245, 'step_size': 6, 'gamma': 0.8783836242404061}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:32:28,861][0m Trial 20 finished with value: 0.06534085422754288 and parameters: {'observation_period_num': 5, 'train_rates': 0.9385253524783611, 'learning_rate': 7.801438697927313e-05, 'batch_size': 211, 'step_size': 15, 'gamma': 0.9541853466410009}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:32:58,601][0m Trial 21 finished with value: 0.06505721062421799 and parameters: {'observation_period_num': 5, 'train_rates': 0.9397921338251376, 'learning_rate': 5.6375907908556334e-05, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9614371149548381}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:33:28,666][0m Trial 22 finished with value: 0.05140593041976293 and parameters: {'observation_period_num': 30, 'train_rates': 0.8718441209897058, 'learning_rate': 0.0005223390560651773, 'batch_size': 190, 'step_size': 12, 'gamma': 0.9242892555089463}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:33:59,340][0m Trial 23 finished with value: 0.0654275869630804 and parameters: {'observation_period_num': 35, 'train_rates': 0.8641719236936327, 'learning_rate': 0.0004315505967302338, 'batch_size': 182, 'step_size': 12, 'gamma': 0.9264934635610503}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:34:38,404][0m Trial 24 finished with value: 0.06392116316660183 and parameters: {'observation_period_num': 64, 'train_rates': 0.8241610274511547, 'learning_rate': 0.0006701767792119749, 'batch_size': 138, 'step_size': 8, 'gamma': 0.8614631441455295}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:35:04,737][0m Trial 25 finished with value: 0.06152758610936312 and parameters: {'observation_period_num': 31, 'train_rates': 0.8888656825374041, 'learning_rate': 0.00022561199493591015, 'batch_size': 229, 'step_size': 12, 'gamma': 0.8827654428832672}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:35:31,035][0m Trial 26 finished with value: 0.0661973098873115 and parameters: {'observation_period_num': 28, 'train_rates': 0.8749015295749959, 'learning_rate': 0.00029463375016594324, 'batch_size': 225, 'step_size': 12, 'gamma': 0.8874142085034069}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:35:55,364][0m Trial 27 finished with value: 0.07765449196943697 and parameters: {'observation_period_num': 51, 'train_rates': 0.8904982183386418, 'learning_rate': 0.0005856945203236404, 'batch_size': 245, 'step_size': 13, 'gamma': 0.8648128739029307}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:36:21,159][0m Trial 28 finished with value: 0.07414758950471878 and parameters: {'observation_period_num': 78, 'train_rates': 0.9239774758224439, 'learning_rate': 0.00023918911342420653, 'batch_size': 222, 'step_size': 10, 'gamma': 0.9111784482871979}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:36:53,122][0m Trial 29 finished with value: 1.0114066946379683 and parameters: {'observation_period_num': 104, 'train_rates': 0.8680587214250692, 'learning_rate': 2.9884461475832803e-06, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8313114748094966}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:37:18,800][0m Trial 30 finished with value: 0.0813559889793396 and parameters: {'observation_period_num': 21, 'train_rates': 0.9533118997579593, 'learning_rate': 9.349561560363183e-05, 'batch_size': 239, 'step_size': 13, 'gamma': 0.8348826981960815}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:37:47,439][0m Trial 31 finished with value: 0.05929644547402859 and parameters: {'observation_period_num': 24, 'train_rates': 0.8292528033607185, 'learning_rate': 0.0007234511725423173, 'batch_size': 195, 'step_size': 7, 'gamma': 0.9415331581645413}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:38:13,988][0m Trial 32 finished with value: 1.330344377202469 and parameters: {'observation_period_num': 25, 'train_rates': 0.8367389663392081, 'learning_rate': 1.1802377447596143e-06, 'batch_size': 219, 'step_size': 7, 'gamma': 0.9740562448699625}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:38:52,378][0m Trial 33 finished with value: 0.09518541892369588 and parameters: {'observation_period_num': 53, 'train_rates': 0.8852402625000247, 'learning_rate': 0.0006974650793899187, 'batch_size': 143, 'step_size': 8, 'gamma': 0.9208247716916912}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:39:24,848][0m Trial 34 finished with value: 0.048587307580537545 and parameters: {'observation_period_num': 32, 'train_rates': 0.8552710528880304, 'learning_rate': 0.0009718238964802372, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8882132801854229}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:39:55,558][0m Trial 35 finished with value: 0.09976322422253675 and parameters: {'observation_period_num': 115, 'train_rates': 0.8568053369776698, 'learning_rate': 0.0005004242814628289, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9451580514437905}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:40:28,854][0m Trial 36 finished with value: 0.09902461446248569 and parameters: {'observation_period_num': 161, 'train_rates': 0.8182417559036308, 'learning_rate': 0.0009879608600372423, 'batch_size': 156, 'step_size': 6, 'gamma': 0.9016596293705114}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:40:56,071][0m Trial 37 finished with value: 0.21429231152457492 and parameters: {'observation_period_num': 44, 'train_rates': 0.7629847500869923, 'learning_rate': 0.0006705774485780813, 'batch_size': 191, 'step_size': 3, 'gamma': 0.9288797033682392}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:41:39,412][0m Trial 38 finished with value: 0.3962163052088778 and parameters: {'observation_period_num': 77, 'train_rates': 0.8519835337488122, 'learning_rate': 8.483349965644476e-06, 'batch_size': 124, 'step_size': 9, 'gamma': 0.9552828826652771}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:42:13,624][0m Trial 39 finished with value: 0.06561417197592037 and parameters: {'observation_period_num': 22, 'train_rates': 0.9119508357759748, 'learning_rate': 0.0004246465924424707, 'batch_size': 173, 'step_size': 4, 'gamma': 0.9817467572506002}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:42:38,492][0m Trial 40 finished with value: 0.2678205271291992 and parameters: {'observation_period_num': 218, 'train_rates': 0.7308338299940553, 'learning_rate': 0.0007304376139269036, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8934908821517322}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:43:04,612][0m Trial 41 finished with value: 0.06767950077122505 and parameters: {'observation_period_num': 34, 'train_rates': 0.8879210620906679, 'learning_rate': 0.00025974229635227414, 'batch_size': 216, 'step_size': 11, 'gamma': 0.8788870389394973}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:43:39,681][0m Trial 42 finished with value: 0.07576482447276589 and parameters: {'observation_period_num': 39, 'train_rates': 0.8203113212425485, 'learning_rate': 0.0004788841347081799, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8528131349046998}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:44:04,013][0m Trial 43 finished with value: 0.0727325440032804 and parameters: {'observation_period_num': 59, 'train_rates': 0.8491496298696464, 'learning_rate': 0.0007705491012363809, 'batch_size': 233, 'step_size': 11, 'gamma': 0.915182605718419}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:44:36,342][0m Trial 44 finished with value: 0.052830934781452704 and parameters: {'observation_period_num': 16, 'train_rates': 0.9012761066795265, 'learning_rate': 0.0003302789463092911, 'batch_size': 182, 'step_size': 10, 'gamma': 0.8809902229699181}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:45:08,610][0m Trial 45 finished with value: 0.05366328698365799 and parameters: {'observation_period_num': 18, 'train_rates': 0.9342189313308853, 'learning_rate': 0.0003580737708904609, 'batch_size': 184, 'step_size': 9, 'gamma': 0.9446202905974512}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:45:54,401][0m Trial 46 finished with value: 0.09562215106664124 and parameters: {'observation_period_num': 44, 'train_rates': 0.937473774590692, 'learning_rate': 0.0003530670453468844, 'batch_size': 127, 'step_size': 10, 'gamma': 0.9677964288755829}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:47:06,590][0m Trial 47 finished with value: 0.04708467452031261 and parameters: {'observation_period_num': 16, 'train_rates': 0.9641598468692486, 'learning_rate': 0.00016784659659787208, 'batch_size': 81, 'step_size': 10, 'gamma': 0.9319009998362227}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:48:32,431][0m Trial 48 finished with value: 0.05337439777003121 and parameters: {'observation_period_num': 16, 'train_rates': 0.9670356579314039, 'learning_rate': 0.00012836178763166997, 'batch_size': 68, 'step_size': 10, 'gamma': 0.908021332661538}. Best is trial 14 with value: 0.04060303548570065.[0m
[32m[I 2025-01-11 04:50:07,535][0m Trial 49 finished with value: 0.10276728868484497 and parameters: {'observation_period_num': 52, 'train_rates': 0.9867456871210458, 'learning_rate': 2.967447258475103e-05, 'batch_size': 61, 'step_size': 13, 'gamma': 0.8457797187302446}. Best is trial 14 with value: 0.04060303548570065.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-11 04:50:07,545][0m A new study created in memory with name: no-name-92e5912f-2e7b-4864-8ba4-382770b5f745[0m
[32m[I 2025-01-11 04:50:59,144][0m Trial 0 finished with value: 0.2850746095599157 and parameters: {'observation_period_num': 49, 'train_rates': 0.6077156555950368, 'learning_rate': 8.971461621621121e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.8036888270641614}. Best is trial 0 with value: 0.2850746095599157.[0m
[32m[I 2025-01-11 04:52:28,007][0m Trial 1 finished with value: 0.7515587940615179 and parameters: {'observation_period_num': 160, 'train_rates': 0.9186702548620991, 'learning_rate': 1.4635909267782418e-06, 'batch_size': 61, 'step_size': 12, 'gamma': 0.9019497597751043}. Best is trial 0 with value: 0.2850746095599157.[0m
[32m[I 2025-01-11 04:52:56,515][0m Trial 2 finished with value: 0.29672503654124605 and parameters: {'observation_period_num': 189, 'train_rates': 0.8465827362597624, 'learning_rate': 4.5011108528558055e-05, 'batch_size': 193, 'step_size': 8, 'gamma': 0.7574446884747147}. Best is trial 0 with value: 0.2850746095599157.[0m
[32m[I 2025-01-11 04:54:01,934][0m Trial 3 finished with value: 0.1741831749677658 and parameters: {'observation_period_num': 148, 'train_rates': 0.9775871867893822, 'learning_rate': 3.5683485785570804e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.8941369063277165}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:54:28,278][0m Trial 4 finished with value: 0.4601329308356682 and parameters: {'observation_period_num': 143, 'train_rates': 0.6583897063892448, 'learning_rate': 3.9636745939168464e-05, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8340412342676019}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:55:34,544][0m Trial 5 finished with value: 0.17717566443117042 and parameters: {'observation_period_num': 167, 'train_rates': 0.9320038143449851, 'learning_rate': 4.3498789047331396e-05, 'batch_size': 83, 'step_size': 10, 'gamma': 0.7693042731352958}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:56:00,766][0m Trial 6 finished with value: 0.7120795039602801 and parameters: {'observation_period_num': 62, 'train_rates': 0.6342169911833869, 'learning_rate': 2.9020969052084514e-05, 'batch_size': 178, 'step_size': 3, 'gamma': 0.775533774501336}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:57:20,878][0m Trial 7 finished with value: 0.4347111409416004 and parameters: {'observation_period_num': 45, 'train_rates': 0.9662695733323916, 'learning_rate': 3.9046058480305354e-05, 'batch_size': 73, 'step_size': 2, 'gamma': 0.8023189708755927}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:57:43,372][0m Trial 8 finished with value: 0.1971028709092114 and parameters: {'observation_period_num': 126, 'train_rates': 0.8086095110805227, 'learning_rate': 6.232332462124347e-05, 'batch_size': 251, 'step_size': 13, 'gamma': 0.9213550019887128}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 04:58:42,547][0m Trial 9 finished with value: 0.34706770373603046 and parameters: {'observation_period_num': 229, 'train_rates': 0.8918953023157523, 'learning_rate': 1.02426268636106e-05, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7883218241204804}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 05:02:31,230][0m Trial 10 finished with value: 0.2303472135506344 and parameters: {'observation_period_num': 89, 'train_rates': 0.7356345500544084, 'learning_rate': 0.0009199423358316222, 'batch_size': 20, 'step_size': 5, 'gamma': 0.9767476510758478}. Best is trial 3 with value: 0.1741831749677658.[0m
[32m[I 2025-01-11 05:03:16,471][0m Trial 11 finished with value: 0.10190845280885696 and parameters: {'observation_period_num': 201, 'train_rates': 0.9871877861106584, 'learning_rate': 0.00025155501110812406, 'batch_size': 126, 'step_size': 11, 'gamma': 0.8693420882870716}. Best is trial 11 with value: 0.10190845280885696.[0m
[32m[I 2025-01-11 05:03:58,052][0m Trial 12 finished with value: 0.08372404426336288 and parameters: {'observation_period_num': 246, 'train_rates': 0.9872424331808064, 'learning_rate': 0.000394017824921841, 'batch_size': 135, 'step_size': 11, 'gamma': 0.8637908028973948}. Best is trial 12 with value: 0.08372404426336288.[0m
[32m[I 2025-01-11 05:04:36,690][0m Trial 13 finished with value: 0.10806202020232133 and parameters: {'observation_period_num': 243, 'train_rates': 0.8594380519700525, 'learning_rate': 0.0007147362816017073, 'batch_size': 135, 'step_size': 12, 'gamma': 0.8549311321248627}. Best is trial 12 with value: 0.08372404426336288.[0m
[32m[I 2025-01-11 05:05:12,533][0m Trial 14 finished with value: 0.291626960529712 and parameters: {'observation_period_num': 219, 'train_rates': 0.7332667275263896, 'learning_rate': 0.00028691028199759604, 'batch_size': 131, 'step_size': 15, 'gamma': 0.953821106239217}. Best is trial 12 with value: 0.08372404426336288.[0m
[32m[I 2025-01-11 05:05:50,171][0m Trial 15 finished with value: 0.08891695737838745 and parameters: {'observation_period_num': 202, 'train_rates': 0.985224782920982, 'learning_rate': 0.0002983518877357687, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8597725905573202}. Best is trial 12 with value: 0.08372404426336288.[0m
[32m[I 2025-01-11 05:06:17,930][0m Trial 16 finished with value: 0.06561478972434998 and parameters: {'observation_period_num': 6, 'train_rates': 0.9305061388468465, 'learning_rate': 0.00022122800998858067, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8403958740276471}. Best is trial 16 with value: 0.06561478972434998.[0m
[32m[I 2025-01-11 05:06:43,570][0m Trial 17 finished with value: 0.07271978259086609 and parameters: {'observation_period_num': 13, 'train_rates': 0.92338036132838, 'learning_rate': 0.00013927565857973665, 'batch_size': 243, 'step_size': 6, 'gamma': 0.8330773301150702}. Best is trial 16 with value: 0.06561478972434998.[0m
[32m[I 2025-01-11 05:07:06,100][0m Trial 18 finished with value: 0.2448176576479061 and parameters: {'observation_period_num': 9, 'train_rates': 0.7738035611317038, 'learning_rate': 0.0001277694218042047, 'batch_size': 252, 'step_size': 5, 'gamma': 0.8257315013543242}. Best is trial 16 with value: 0.06561478972434998.[0m
[32m[I 2025-01-11 05:07:32,475][0m Trial 19 finished with value: 0.5168064211384725 and parameters: {'observation_period_num': 26, 'train_rates': 0.8789903651233971, 'learning_rate': 1.167424755673232e-05, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8318022599102444}. Best is trial 16 with value: 0.06561478972434998.[0m
[32m[I 2025-01-11 05:07:58,291][0m Trial 20 finished with value: 0.14045128226280212 and parameters: {'observation_period_num': 95, 'train_rates': 0.9267806203509193, 'learning_rate': 0.00013170066763367044, 'batch_size': 221, 'step_size': 4, 'gamma': 0.8947955772236512}. Best is trial 16 with value: 0.06561478972434998.[0m
[32m[I 2025-01-11 05:08:26,240][0m Trial 21 finished with value: 0.0445798821747303 and parameters: {'observation_period_num': 8, 'train_rates': 0.9385509131755957, 'learning_rate': 0.0005680149952013268, 'batch_size': 220, 'step_size': 7, 'gamma': 0.84391861925428}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:08:53,921][0m Trial 22 finished with value: 0.052609991282224655 and parameters: {'observation_period_num': 7, 'train_rates': 0.9391535133803469, 'learning_rate': 0.0005057254032675128, 'batch_size': 224, 'step_size': 7, 'gamma': 0.8197087849131638}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:09:20,488][0m Trial 23 finished with value: 0.05921424667917607 and parameters: {'observation_period_num': 31, 'train_rates': 0.8218113314891238, 'learning_rate': 0.000529520144912649, 'batch_size': 212, 'step_size': 8, 'gamma': 0.8104539513815675}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:09:47,169][0m Trial 24 finished with value: 0.058286095375035915 and parameters: {'observation_period_num': 31, 'train_rates': 0.828007422349726, 'learning_rate': 0.0005326054749804102, 'batch_size': 203, 'step_size': 8, 'gamma': 0.8100336723153653}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:10:12,463][0m Trial 25 finished with value: 0.20640327083602758 and parameters: {'observation_period_num': 73, 'train_rates': 0.7532186278847603, 'learning_rate': 0.0009656556453663356, 'batch_size': 199, 'step_size': 7, 'gamma': 0.8124091476691353}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:10:40,622][0m Trial 26 finished with value: 0.21437815224867215 and parameters: {'observation_period_num': 115, 'train_rates': 0.6814406819467203, 'learning_rate': 0.0005026181618780929, 'batch_size': 165, 'step_size': 9, 'gamma': 0.7939849528008783}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:11:06,954][0m Trial 27 finished with value: 1.0918130976168514 and parameters: {'observation_period_num': 36, 'train_rates': 0.8915458829087288, 'learning_rate': 2.8063899397096586e-06, 'batch_size': 235, 'step_size': 7, 'gamma': 0.8485484398442039}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:11:35,267][0m Trial 28 finished with value: 0.08656634713866208 and parameters: {'observation_period_num': 67, 'train_rates': 0.8418915985348533, 'learning_rate': 0.0001905464634261632, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8809748767415363}. Best is trial 21 with value: 0.0445798821747303.[0m
Early stopping at epoch 57
[32m[I 2025-01-11 05:11:50,880][0m Trial 29 finished with value: 0.5333027243614197 and parameters: {'observation_period_num': 54, 'train_rates': 0.9546193585331036, 'learning_rate': 7.973001599969346e-05, 'batch_size': 233, 'step_size': 1, 'gamma': 0.8166668163383032}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:12:26,277][0m Trial 30 finished with value: 0.05888408867317156 and parameters: {'observation_period_num': 23, 'train_rates': 0.7906291007548008, 'learning_rate': 0.0005266838230401726, 'batch_size': 153, 'step_size': 4, 'gamma': 0.9202320820811543}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:12:59,413][0m Trial 31 finished with value: 0.0581134171836605 and parameters: {'observation_period_num': 23, 'train_rates': 0.7971535226030959, 'learning_rate': 0.0005103690042706557, 'batch_size': 157, 'step_size': 3, 'gamma': 0.9322531049254074}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:13:42,280][0m Trial 32 finished with value: 0.17643423770906097 and parameters: {'observation_period_num': 45, 'train_rates': 0.7103807154889982, 'learning_rate': 0.0006710612997920412, 'batch_size': 111, 'step_size': 2, 'gamma': 0.9488061452872133}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:14:14,647][0m Trial 33 finished with value: 0.05250907003214587 and parameters: {'observation_period_num': 23, 'train_rates': 0.8316485300755249, 'learning_rate': 0.000352371246778913, 'batch_size': 173, 'step_size': 8, 'gamma': 0.8777517945515958}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:14:46,998][0m Trial 34 finished with value: 0.051147888193732685 and parameters: {'observation_period_num': 5, 'train_rates': 0.8684160825434661, 'learning_rate': 0.0003398211878151151, 'batch_size': 182, 'step_size': 5, 'gamma': 0.9220316438096415}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:15:19,336][0m Trial 35 finished with value: 0.055840439978055656 and parameters: {'observation_period_num': 8, 'train_rates': 0.86956810573483, 'learning_rate': 0.0003497756957390225, 'batch_size': 182, 'step_size': 5, 'gamma': 0.8846265206800809}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:15:51,742][0m Trial 36 finished with value: 0.06910353947879998 and parameters: {'observation_period_num': 45, 'train_rates': 0.9029595678680389, 'learning_rate': 0.00017064734068331476, 'batch_size': 185, 'step_size': 7, 'gamma': 0.9096575633788285}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:16:27,407][0m Trial 37 finished with value: 0.09662409126758575 and parameters: {'observation_period_num': 88, 'train_rates': 0.9505600187726698, 'learning_rate': 8.962633152143402e-05, 'batch_size': 167, 'step_size': 8, 'gamma': 0.878228024404078}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:16:57,743][0m Trial 38 finished with value: 0.7453407947631443 and parameters: {'observation_period_num': 21, 'train_rates': 0.9072482794649059, 'learning_rate': 1.3100065652866096e-06, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8461307942747002}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:17:21,803][0m Trial 39 finished with value: 0.423153217992739 and parameters: {'observation_period_num': 54, 'train_rates': 0.8494423856948422, 'learning_rate': 1.824038839017926e-05, 'batch_size': 233, 'step_size': 6, 'gamma': 0.8993166966480917}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:17:51,750][0m Trial 40 finished with value: 0.04843633621931076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9578873535982562, 'learning_rate': 0.0003348691741160113, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9844653477581535}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:18:27,357][0m Trial 41 finished with value: 0.06590073555707932 and parameters: {'observation_period_num': 17, 'train_rates': 0.9504266203713476, 'learning_rate': 0.0003709056924980143, 'batch_size': 172, 'step_size': 9, 'gamma': 0.9722111784491261}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:18:57,097][0m Trial 42 finished with value: 0.04476504400372505 and parameters: {'observation_period_num': 5, 'train_rates': 0.9635535421286632, 'learning_rate': 0.0007733703662154423, 'batch_size': 209, 'step_size': 8, 'gamma': 0.9842129278323828}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:19:27,027][0m Trial 43 finished with value: 0.065114326775074 and parameters: {'observation_period_num': 43, 'train_rates': 0.9646163343972624, 'learning_rate': 0.0008084690321378258, 'batch_size': 211, 'step_size': 10, 'gamma': 0.9654744318931467}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:20:07,363][0m Trial 44 finished with value: 0.05464163957206347 and parameters: {'observation_period_num': 35, 'train_rates': 0.9104330173182429, 'learning_rate': 0.0009843426143544673, 'batch_size': 147, 'step_size': 8, 'gamma': 0.9840279318245784}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:20:37,911][0m Trial 45 finished with value: 0.045038935739458316 and parameters: {'observation_period_num': 6, 'train_rates': 0.8858870970812524, 'learning_rate': 0.00022163895522002753, 'batch_size': 199, 'step_size': 8, 'gamma': 0.9392076868835685}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:21:07,465][0m Trial 46 finished with value: 0.6899705529212952 and parameters: {'observation_period_num': 174, 'train_rates': 0.966243449477721, 'learning_rate': 3.3848074987921173e-06, 'batch_size': 201, 'step_size': 9, 'gamma': 0.9895674793886103}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:21:37,811][0m Trial 47 finished with value: 0.07307281432782903 and parameters: {'observation_period_num': 16, 'train_rates': 0.8844013097690071, 'learning_rate': 6.24793872808978e-05, 'batch_size': 190, 'step_size': 13, 'gamma': 0.9383350543773552}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:21:55,375][0m Trial 48 finished with value: 0.23689313427583866 and parameters: {'observation_period_num': 146, 'train_rates': 0.6055701326719025, 'learning_rate': 0.00023443082393412637, 'batch_size': 243, 'step_size': 10, 'gamma': 0.9608537509749085}. Best is trial 21 with value: 0.0445798821747303.[0m
[32m[I 2025-01-11 05:22:23,235][0m Trial 49 finished with value: 0.09935900568962097 and parameters: {'observation_period_num': 77, 'train_rates': 0.9421083581234945, 'learning_rate': 0.00017491065295411002, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9774393985646272}. Best is trial 21 with value: 0.0445798821747303.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-11 05:22:23,245][0m A new study created in memory with name: no-name-2cb33e5e-da3a-45ae-bca3-85bc9c108cd7[0m
[32m[I 2025-01-11 05:22:54,525][0m Trial 0 finished with value: 0.05377475917339325 and parameters: {'observation_period_num': 41, 'train_rates': 0.9627306547083602, 'learning_rate': 0.000642739965001238, 'batch_size': 190, 'step_size': 7, 'gamma': 0.8756955008722285}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:24:11,815][0m Trial 1 finished with value: 0.3213846290138425 and parameters: {'observation_period_num': 193, 'train_rates': 0.7186272760983963, 'learning_rate': 0.00011098357667078818, 'batch_size': 57, 'step_size': 6, 'gamma': 0.7629894047102554}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:25:00,007][0m Trial 2 finished with value: 0.07964413469639894 and parameters: {'observation_period_num': 102, 'train_rates': 0.8123005614483234, 'learning_rate': 0.00030781556294233285, 'batch_size': 106, 'step_size': 9, 'gamma': 0.7907073907865868}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:25:37,408][0m Trial 3 finished with value: 0.9595008423102314 and parameters: {'observation_period_num': 48, 'train_rates': 0.8402605498164784, 'learning_rate': 2.8422906108217294e-06, 'batch_size': 150, 'step_size': 10, 'gamma': 0.8823060915419395}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:26:03,739][0m Trial 4 finished with value: 1.0457430811461337 and parameters: {'observation_period_num': 82, 'train_rates': 0.6506902906560106, 'learning_rate': 2.0666407108637033e-06, 'batch_size': 172, 'step_size': 2, 'gamma': 0.8353181558725051}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:26:43,474][0m Trial 5 finished with value: 0.6635422706604004 and parameters: {'observation_period_num': 154, 'train_rates': 0.9593560656483735, 'learning_rate': 3.945391567927526e-06, 'batch_size': 148, 'step_size': 11, 'gamma': 0.933146415511981}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:27:42,034][0m Trial 6 finished with value: 2.872781095966216 and parameters: {'observation_period_num': 90, 'train_rates': 0.913345836937093, 'learning_rate': 2.4520732578732694e-06, 'batch_size': 94, 'step_size': 4, 'gamma': 0.8073238533597701}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:28:18,492][0m Trial 7 finished with value: 0.2989825200374663 and parameters: {'observation_period_num': 11, 'train_rates': 0.7172900744688552, 'learning_rate': 1.718756624754163e-05, 'batch_size': 134, 'step_size': 13, 'gamma': 0.834321912400926}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:28:44,888][0m Trial 8 finished with value: 1.477712679769575 and parameters: {'observation_period_num': 225, 'train_rates': 0.8578675230096676, 'learning_rate': 1.223173917527646e-06, 'batch_size': 206, 'step_size': 9, 'gamma': 0.8766720109769429}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:29:10,197][0m Trial 9 finished with value: 1.5794728681745722 and parameters: {'observation_period_num': 86, 'train_rates': 0.757695434482714, 'learning_rate': 2.1498349427351438e-06, 'batch_size': 199, 'step_size': 7, 'gamma': 0.8749954056773441}. Best is trial 0 with value: 0.05377475917339325.[0m
[32m[I 2025-01-11 05:29:37,863][0m Trial 10 finished with value: 0.04541259631514549 and parameters: {'observation_period_num': 7, 'train_rates': 0.9815158496234694, 'learning_rate': 0.0006996363610337626, 'batch_size': 238, 'step_size': 14, 'gamma': 0.982526726916759}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:30:03,603][0m Trial 11 finished with value: 0.05512053146958351 and parameters: {'observation_period_num': 7, 'train_rates': 0.9899949674607258, 'learning_rate': 0.0009225267807870056, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9843260940217565}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:30:27,463][0m Trial 12 finished with value: 0.0691596195101738 and parameters: {'observation_period_num': 46, 'train_rates': 0.9214270978820903, 'learning_rate': 0.000422505445161223, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9885128618258745}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:30:55,738][0m Trial 13 finished with value: 0.13219697028398514 and parameters: {'observation_period_num': 42, 'train_rates': 0.8942955414041276, 'learning_rate': 5.929033501240191e-05, 'batch_size': 210, 'step_size': 5, 'gamma': 0.9265007964571207}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:31:23,482][0m Trial 14 finished with value: 0.1067039892077446 and parameters: {'observation_period_num': 144, 'train_rates': 0.9793173856851844, 'learning_rate': 0.0009179915252620958, 'batch_size': 224, 'step_size': 12, 'gamma': 0.9325796425974658}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:31:48,201][0m Trial 15 finished with value: 0.15334828125236377 and parameters: {'observation_period_num': 5, 'train_rates': 0.6035684117499696, 'learning_rate': 0.0001821873116662506, 'batch_size': 178, 'step_size': 3, 'gamma': 0.9576139704888282}. Best is trial 10 with value: 0.04541259631514549.[0m
Early stopping at epoch 64
[32m[I 2025-01-11 05:32:04,203][0m Trial 16 finished with value: 3.1596012115478516 and parameters: {'observation_period_num': 54, 'train_rates': 0.9312159356092503, 'learning_rate': 1.5263025233010555e-05, 'batch_size': 256, 'step_size': 1, 'gamma': 0.8440510750764187}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:32:36,220][0m Trial 17 finished with value: 0.07122583504162687 and parameters: {'observation_period_num': 113, 'train_rates': 0.8738186870976506, 'learning_rate': 0.0003639664476503676, 'batch_size': 176, 'step_size': 8, 'gamma': 0.9042897279787214}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:33:03,703][0m Trial 18 finished with value: 0.1722899079322815 and parameters: {'observation_period_num': 66, 'train_rates': 0.9459504993439385, 'learning_rate': 5.532136643423862e-05, 'batch_size': 212, 'step_size': 13, 'gamma': 0.7505500214666565}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:36:06,368][0m Trial 19 finished with value: 0.047490035426681454 and parameters: {'observation_period_num': 29, 'train_rates': 0.7953535306239059, 'learning_rate': 0.0001497576287607169, 'batch_size': 27, 'step_size': 7, 'gamma': 0.9584478813763931}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:39:28,864][0m Trial 20 finished with value: 0.14807335493187473 and parameters: {'observation_period_num': 167, 'train_rates': 0.7849072702629271, 'learning_rate': 0.0001585283073532745, 'batch_size': 23, 'step_size': 11, 'gamma': 0.9632808173551872}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:41:56,702][0m Trial 21 finished with value: 0.0517609881921837 and parameters: {'observation_period_num': 27, 'train_rates': 0.8270004625519296, 'learning_rate': 0.0006075896580064798, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9583452439504248}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:44:50,511][0m Trial 22 finished with value: 0.09841360596806875 and parameters: {'observation_period_num': 26, 'train_rates': 0.8087622970742666, 'learning_rate': 0.00026152331196299894, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9626388527314821}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:46:19,768][0m Trial 23 finished with value: 0.17571461659440019 and parameters: {'observation_period_num': 27, 'train_rates': 0.7428957874691384, 'learning_rate': 0.00047334184806291715, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9085978251956868}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:47:34,046][0m Trial 24 finished with value: 0.20477902062319137 and parameters: {'observation_period_num': 64, 'train_rates': 0.6848912081039189, 'learning_rate': 9.226545274300953e-05, 'batch_size': 60, 'step_size': 5, 'gamma': 0.9530639745014202}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:48:34,040][0m Trial 25 finished with value: 0.04968401574744628 and parameters: {'observation_period_num': 25, 'train_rates': 0.8400125298353053, 'learning_rate': 0.0002236547344202646, 'batch_size': 90, 'step_size': 9, 'gamma': 0.9892000883097548}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:49:35,181][0m Trial 26 finished with value: 0.09697559228270117 and parameters: {'observation_period_num': 123, 'train_rates': 0.8786761982575801, 'learning_rate': 3.5475581984707715e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.9807797621804967}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:50:17,080][0m Trial 27 finished with value: 0.1931697596795857 and parameters: {'observation_period_num': 26, 'train_rates': 0.7676968835012336, 'learning_rate': 0.00017342395850985983, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9456610397493915}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:51:21,128][0m Trial 28 finished with value: 0.10118257547590402 and parameters: {'observation_period_num': 249, 'train_rates': 0.8505044552142687, 'learning_rate': 0.00010451341329016466, 'batch_size': 79, 'step_size': 12, 'gamma': 0.974590714950826}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:52:32,253][0m Trial 29 finished with value: 0.06453665284644132 and parameters: {'observation_period_num': 72, 'train_rates': 0.7908807757180416, 'learning_rate': 0.00023905119919161126, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9095178375524671}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:54:19,342][0m Trial 30 finished with value: 0.14696926619985456 and parameters: {'observation_period_num': 20, 'train_rates': 0.6820903317486949, 'learning_rate': 0.0006502432317483298, 'batch_size': 42, 'step_size': 6, 'gamma': 0.9719517450431243}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 05:59:31,996][0m Trial 31 finished with value: 0.05815468314941432 and parameters: {'observation_period_num': 34, 'train_rates': 0.8228996302939567, 'learning_rate': 0.0005687321678733812, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9438163855707711}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:01:23,927][0m Trial 32 finished with value: 0.09443040099752144 and parameters: {'observation_period_num': 54, 'train_rates': 0.8302219877331156, 'learning_rate': 0.0006972043869169119, 'batch_size': 46, 'step_size': 7, 'gamma': 0.9898384819237361}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:03:24,110][0m Trial 33 finished with value: 0.17472030783819545 and parameters: {'observation_period_num': 37, 'train_rates': 0.730660548140621, 'learning_rate': 0.00029382498144260577, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9690272497476987}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:04:08,326][0m Trial 34 finished with value: 0.16730555173143477 and parameters: {'observation_period_num': 6, 'train_rates': 0.7733646161449033, 'learning_rate': 0.00042385469422275376, 'batch_size': 114, 'step_size': 10, 'gamma': 0.926924015899788}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:05:22,974][0m Trial 35 finished with value: 0.057569383177906275 and parameters: {'observation_period_num': 20, 'train_rates': 0.8117726887891632, 'learning_rate': 0.00011878671668580785, 'batch_size': 69, 'step_size': 8, 'gamma': 0.9437382510718516}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:06:18,086][0m Trial 36 finished with value: 0.0801529987227349 and parameters: {'observation_period_num': 101, 'train_rates': 0.8822300770754355, 'learning_rate': 0.00020408354164348693, 'batch_size': 100, 'step_size': 4, 'gamma': 0.9172926517573143}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:06:56,277][0m Trial 37 finished with value: 0.06347455640378244 and parameters: {'observation_period_num': 58, 'train_rates': 0.906055060166313, 'learning_rate': 0.0009118743504563847, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8929726070189963}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:07:39,035][0m Trial 38 finished with value: 0.3272902909011975 and parameters: {'observation_period_num': 38, 'train_rates': 0.853560234568498, 'learning_rate': 6.504611974558052e-06, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9526066436326236}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:09:38,272][0m Trial 39 finished with value: 0.24239101419158654 and parameters: {'observation_period_num': 180, 'train_rates': 0.6975416733628362, 'learning_rate': 7.567951788180224e-05, 'batch_size': 36, 'step_size': 11, 'gamma': 0.7825533776526196}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:10:40,328][0m Trial 40 finished with value: 0.07870888345461478 and parameters: {'observation_period_num': 78, 'train_rates': 0.8048578914638193, 'learning_rate': 0.00013833664622154332, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8516791917062094}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:11:07,850][0m Trial 41 finished with value: 0.060067567974328995 and parameters: {'observation_period_num': 46, 'train_rates': 0.9499006383882109, 'learning_rate': 0.0006132949934843037, 'batch_size': 227, 'step_size': 7, 'gamma': 0.8617308865163122}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:11:39,520][0m Trial 42 finished with value: 0.06886810809373856 and parameters: {'observation_period_num': 15, 'train_rates': 0.9694816105741387, 'learning_rate': 0.0003157470428045815, 'batch_size': 192, 'step_size': 7, 'gamma': 0.8203111681693934}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:12:19,787][0m Trial 43 finished with value: 0.05369484521406519 and parameters: {'observation_period_num': 31, 'train_rates': 0.939419311334674, 'learning_rate': 0.0004829559492904449, 'batch_size': 148, 'step_size': 4, 'gamma': 0.9711739692177348}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:12:54,953][0m Trial 44 finished with value: 0.050633362569707505 and parameters: {'observation_period_num': 29, 'train_rates': 0.8394077838047498, 'learning_rate': 0.00048402065129638915, 'batch_size': 154, 'step_size': 4, 'gamma': 0.9807477337810379}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:13:29,499][0m Trial 45 finished with value: 0.06282931638260682 and parameters: {'observation_period_num': 94, 'train_rates': 0.8425723404955, 'learning_rate': 0.0007214812412280367, 'batch_size': 159, 'step_size': 4, 'gamma': 0.9793780836592205}. Best is trial 10 with value: 0.04541259631514549.[0m
[32m[I 2025-01-11 06:14:20,699][0m Trial 46 finished with value: 0.035738424945566526 and parameters: {'observation_period_num': 13, 'train_rates': 0.8646647291462399, 'learning_rate': 0.0003461364670810924, 'batch_size': 110, 'step_size': 1, 'gamma': 0.9875158085457361}. Best is trial 46 with value: 0.035738424945566526.[0m
[32m[I 2025-01-11 06:15:07,843][0m Trial 47 finished with value: 0.10069838315248489 and parameters: {'observation_period_num': 206, 'train_rates': 0.8690503053302886, 'learning_rate': 0.00034370783754372774, 'batch_size': 111, 'step_size': 1, 'gamma': 0.9899695409418671}. Best is trial 46 with value: 0.035738424945566526.[0m
[32m[I 2025-01-11 06:15:54,849][0m Trial 48 finished with value: 0.06875833451844342 and parameters: {'observation_period_num': 14, 'train_rates': 0.8950587977190186, 'learning_rate': 3.712286151673272e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9754004387326007}. Best is trial 46 with value: 0.035738424945566526.[0m
[32m[I 2025-01-11 06:16:50,356][0m Trial 49 finished with value: 0.13265730901667389 and parameters: {'observation_period_num': 50, 'train_rates': 0.8608589137282149, 'learning_rate': 2.1435972562256848e-05, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9814906862530125}. Best is trial 46 with value: 0.035738424945566526.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-11 06:16:50,366][0m A new study created in memory with name: no-name-a48713f8-bc88-4eff-8385-767add348a89[0m
[32m[I 2025-01-11 06:17:21,491][0m Trial 0 finished with value: 0.4025793292568725 and parameters: {'observation_period_num': 118, 'train_rates': 0.6363648774036563, 'learning_rate': 2.9586008014522217e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8836305416121026}. Best is trial 0 with value: 0.4025793292568725.[0m
[32m[I 2025-01-11 06:17:42,338][0m Trial 1 finished with value: 0.7106522814727124 and parameters: {'observation_period_num': 63, 'train_rates': 0.74753177614532, 'learning_rate': 7.001643583150888e-06, 'batch_size': 253, 'step_size': 3, 'gamma': 0.9855565965229852}. Best is trial 0 with value: 0.4025793292568725.[0m
[32m[I 2025-01-11 06:19:58,144][0m Trial 2 finished with value: 0.13558947294100737 and parameters: {'observation_period_num': 9, 'train_rates': 0.6275778106948173, 'learning_rate': 0.0002805503962080708, 'batch_size': 31, 'step_size': 4, 'gamma': 0.7924022324627789}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:20:38,566][0m Trial 3 finished with value: 0.14494584811230501 and parameters: {'observation_period_num': 82, 'train_rates': 0.8996305716977988, 'learning_rate': 3.2470610166302506e-05, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9184007251446452}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:21:09,886][0m Trial 4 finished with value: 0.2733872869202705 and parameters: {'observation_period_num': 140, 'train_rates': 0.7428717427207666, 'learning_rate': 0.0003709167929361245, 'batch_size': 160, 'step_size': 2, 'gamma': 0.9284373114372699}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:21:52,998][0m Trial 5 finished with value: 0.41821909881083763 and parameters: {'observation_period_num': 242, 'train_rates': 0.6050896373222902, 'learning_rate': 2.398527927295978e-05, 'batch_size': 94, 'step_size': 15, 'gamma': 0.9522307731487745}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:22:19,384][0m Trial 6 finished with value: 1.078833887106771 and parameters: {'observation_period_num': 234, 'train_rates': 0.6248634990337826, 'learning_rate': 4.734848463485327e-06, 'batch_size': 162, 'step_size': 14, 'gamma': 0.8464020910130824}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:24:42,688][0m Trial 7 finished with value: 0.21285682111978532 and parameters: {'observation_period_num': 239, 'train_rates': 0.9263348662749035, 'learning_rate': 7.349906990017894e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8099137685668496}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:25:59,551][0m Trial 8 finished with value: 0.21521647185732742 and parameters: {'observation_period_num': 69, 'train_rates': 0.8079183591503079, 'learning_rate': 2.3094316471423194e-05, 'batch_size': 65, 'step_size': 2, 'gamma': 0.9424276889613364}. Best is trial 2 with value: 0.13558947294100737.[0m
[32m[I 2025-01-11 06:26:32,439][0m Trial 9 finished with value: 0.06423768517121954 and parameters: {'observation_period_num': 51, 'train_rates': 0.8172337168158367, 'learning_rate': 0.000252207521375325, 'batch_size': 161, 'step_size': 12, 'gamma': 0.9396988112700793}. Best is trial 9 with value: 0.06423768517121954.[0m
[32m[I 2025-01-11 06:26:59,986][0m Trial 10 finished with value: 0.04526575282216072 and parameters: {'observation_period_num': 19, 'train_rates': 0.9866665715155103, 'learning_rate': 0.0009516263825637384, 'batch_size': 223, 'step_size': 10, 'gamma': 0.7604850403096222}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:27:29,546][0m Trial 11 finished with value: 0.051704294979572296 and parameters: {'observation_period_num': 5, 'train_rates': 0.9890922996393938, 'learning_rate': 0.0007995973496015906, 'batch_size': 222, 'step_size': 10, 'gamma': 0.7694224925180926}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:27:57,352][0m Trial 12 finished with value: 0.05026743933558464 and parameters: {'observation_period_num': 6, 'train_rates': 0.9674092714292997, 'learning_rate': 0.000876721818086531, 'batch_size': 227, 'step_size': 9, 'gamma': 0.750886705811571}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:28:27,296][0m Trial 13 finished with value: 1.1426024436950684 and parameters: {'observation_period_num': 33, 'train_rates': 0.9704262788439613, 'learning_rate': 1.1189836647457981e-06, 'batch_size': 208, 'step_size': 7, 'gamma': 0.7527710721014698}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:28:53,230][0m Trial 14 finished with value: 0.0842920610750163 and parameters: {'observation_period_num': 119, 'train_rates': 0.8763239870661751, 'learning_rate': 0.0008940101988579908, 'batch_size': 208, 'step_size': 8, 'gamma': 0.8401403957337866}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:29:18,804][0m Trial 15 finished with value: 0.2236529439687729 and parameters: {'observation_period_num': 202, 'train_rates': 0.9451993575628777, 'learning_rate': 0.00013267218079074928, 'batch_size': 236, 'step_size': 6, 'gamma': 0.793704175694777}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:29:47,242][0m Trial 16 finished with value: 0.08396903585533881 and parameters: {'observation_period_num': 167, 'train_rates': 0.8648903162353638, 'learning_rate': 0.0006302257796336722, 'batch_size': 188, 'step_size': 10, 'gamma': 0.7519848475932436}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:30:43,279][0m Trial 17 finished with value: 0.0544907312128651 and parameters: {'observation_period_num': 28, 'train_rates': 0.9392034808213972, 'learning_rate': 0.00016768684842777008, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8224385789280376}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:31:14,093][0m Trial 18 finished with value: 0.13921813090780266 and parameters: {'observation_period_num': 84, 'train_rates': 0.8493098631951672, 'learning_rate': 7.468960244006615e-05, 'batch_size': 187, 'step_size': 12, 'gamma': 0.8821666588545126}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:31:35,489][0m Trial 19 finished with value: 0.23150998521437766 and parameters: {'observation_period_num': 38, 'train_rates': 0.7174645136752169, 'learning_rate': 0.000467987119674336, 'batch_size': 256, 'step_size': 5, 'gamma': 0.7789819605428894}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:32:05,679][0m Trial 20 finished with value: 0.1489804051952921 and parameters: {'observation_period_num': 99, 'train_rates': 0.9045067878557403, 'learning_rate': 8.765755816603485e-05, 'batch_size': 188, 'step_size': 8, 'gamma': 0.8625724281330586}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:32:33,294][0m Trial 21 finished with value: 0.046174027025699615 and parameters: {'observation_period_num': 7, 'train_rates': 0.9836014448353215, 'learning_rate': 0.0009643008650666671, 'batch_size': 228, 'step_size': 10, 'gamma': 0.7713318128371945}. Best is trial 10 with value: 0.04526575282216072.[0m
[32m[I 2025-01-11 06:33:01,264][0m Trial 22 finished with value: 0.04342333599925041 and parameters: {'observation_period_num': 15, 'train_rates': 0.9847265248704172, 'learning_rate': 0.0007753533139251889, 'batch_size': 237, 'step_size': 11, 'gamma': 0.7721765623839526}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:33:28,763][0m Trial 23 finished with value: 0.056715477257966995 and parameters: {'observation_period_num': 29, 'train_rates': 0.9839799138025491, 'learning_rate': 0.0004488484812144616, 'batch_size': 239, 'step_size': 12, 'gamma': 0.8086625799354303}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:33:58,640][0m Trial 24 finished with value: 0.052043311297893524 and parameters: {'observation_period_num': 48, 'train_rates': 0.945229158844356, 'learning_rate': 0.000996518822011374, 'batch_size': 206, 'step_size': 11, 'gamma': 0.7738063838417901}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:34:27,019][0m Trial 25 finished with value: 0.054085037826827026 and parameters: {'observation_period_num': 23, 'train_rates': 0.9084057646272824, 'learning_rate': 0.00021749677995272414, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7904235633554304}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:34:52,803][0m Trial 26 finished with value: 0.07199078798294067 and parameters: {'observation_period_num': 63, 'train_rates': 0.9568879010863598, 'learning_rate': 0.0005139433622053682, 'batch_size': 242, 'step_size': 9, 'gamma': 0.824835518714853}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:35:23,434][0m Trial 27 finished with value: 0.08642717084009403 and parameters: {'observation_period_num': 164, 'train_rates': 0.8355191013599165, 'learning_rate': 0.00034657591611113217, 'batch_size': 178, 'step_size': 11, 'gamma': 0.7700892766789617}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:36:12,003][0m Trial 28 finished with value: 0.05610530405347027 and parameters: {'observation_period_num': 16, 'train_rates': 0.8859240287567598, 'learning_rate': 0.0001428838894946817, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8009288101437145}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:36:42,057][0m Trial 29 finished with value: 0.2820469362473269 and parameters: {'observation_period_num': 102, 'train_rates': 0.9236948524023573, 'learning_rate': 4.655190791962657e-05, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8371296212419959}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:37:03,517][0m Trial 30 finished with value: 1.0328449419566563 and parameters: {'observation_period_num': 45, 'train_rates': 0.68680320600944, 'learning_rate': 7.63071680141584e-06, 'batch_size': 232, 'step_size': 11, 'gamma': 0.7781493955190185}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:37:31,263][0m Trial 31 finished with value: 0.0620792955160141 and parameters: {'observation_period_num': 6, 'train_rates': 0.9892557822602733, 'learning_rate': 0.0006053908670959281, 'batch_size': 225, 'step_size': 9, 'gamma': 0.7568719223611273}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:37:56,833][0m Trial 32 finished with value: 0.04958325996994972 and parameters: {'observation_period_num': 17, 'train_rates': 0.9638736111567929, 'learning_rate': 0.0009665796219136995, 'batch_size': 252, 'step_size': 10, 'gamma': 0.7626202070726712}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:38:22,242][0m Trial 33 finished with value: 0.06817185878753662 and parameters: {'observation_period_num': 63, 'train_rates': 0.9614141420139645, 'learning_rate': 0.0006096106635166322, 'batch_size': 247, 'step_size': 13, 'gamma': 0.7858239708387367}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:38:45,883][0m Trial 34 finished with value: 0.0659724622964859 and parameters: {'observation_period_num': 25, 'train_rates': 0.9296530828850817, 'learning_rate': 0.00030258926907139887, 'batch_size': 256, 'step_size': 10, 'gamma': 0.7665241822089085}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:39:11,285][0m Trial 35 finished with value: 0.2204606747802566 and parameters: {'observation_period_num': 51, 'train_rates': 0.7716381460847261, 'learning_rate': 0.00037201232326287314, 'batch_size': 217, 'step_size': 7, 'gamma': 0.8962030731447284}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:39:36,877][0m Trial 36 finished with value: 0.05328268185257912 and parameters: {'observation_period_num': 38, 'train_rates': 0.967493882225333, 'learning_rate': 0.0009616781179620872, 'batch_size': 243, 'step_size': 11, 'gamma': 0.9812031374726536}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:40:23,156][0m Trial 37 finished with value: 0.06400730085287075 and parameters: {'observation_period_num': 78, 'train_rates': 0.915447854374008, 'learning_rate': 0.00020811690667775517, 'batch_size': 125, 'step_size': 13, 'gamma': 0.8079868294939219}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:41:01,672][0m Trial 38 finished with value: 0.2912313911262192 and parameters: {'observation_period_num': 16, 'train_rates': 0.892311513082567, 'learning_rate': 1.1863620835409322e-05, 'batch_size': 147, 'step_size': 15, 'gamma': 0.7655189819153929}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:41:33,241][0m Trial 39 finished with value: 0.050336435437202454 and parameters: {'observation_period_num': 23, 'train_rates': 0.9503735203937252, 'learning_rate': 0.0006489956210880029, 'batch_size': 195, 'step_size': 8, 'gamma': 0.7847811935699488}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:42:01,586][0m Trial 40 finished with value: 1.030586293368789 and parameters: {'observation_period_num': 95, 'train_rates': 0.6770102005383581, 'learning_rate': 1.586613004437997e-06, 'batch_size': 175, 'step_size': 14, 'gamma': 0.7620207267382458}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:42:29,191][0m Trial 41 finished with value: 0.04907975345849991 and parameters: {'observation_period_num': 8, 'train_rates': 0.9755867470768411, 'learning_rate': 0.0009885519001765022, 'batch_size': 225, 'step_size': 9, 'gamma': 0.7508823136143802}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:42:56,921][0m Trial 42 finished with value: 0.06570935994386673 and parameters: {'observation_period_num': 17, 'train_rates': 0.9897813825546766, 'learning_rate': 0.0003940703750507628, 'batch_size': 232, 'step_size': 10, 'gamma': 0.7932702265764803}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:43:26,481][0m Trial 43 finished with value: 0.06604361534118652 and parameters: {'observation_period_num': 40, 'train_rates': 0.9734237198843645, 'learning_rate': 0.0007506954248195506, 'batch_size': 214, 'step_size': 9, 'gamma': 0.7772147208705544}. Best is trial 22 with value: 0.04342333599925041.[0m
[32m[I 2025-01-11 06:43:52,001][0m Trial 44 finished with value: 0.04176926240324974 and parameters: {'observation_period_num': 6, 'train_rates': 0.9345754270698067, 'learning_rate': 0.0009910408868499443, 'batch_size': 249, 'step_size': 12, 'gamma': 0.8200718314815263}. Best is trial 44 with value: 0.04176926240324974.[0m
[32m[I 2025-01-11 06:45:21,233][0m Trial 45 finished with value: 0.031746342833259424 and parameters: {'observation_period_num': 10, 'train_rates': 0.9311902731119093, 'learning_rate': 0.0002882251271567547, 'batch_size': 64, 'step_size': 12, 'gamma': 0.8207335566474638}. Best is trial 45 with value: 0.031746342833259424.[0m
[32m[I 2025-01-11 06:46:40,869][0m Trial 46 finished with value: 0.06287136633435021 and parameters: {'observation_period_num': 55, 'train_rates': 0.9366674022725232, 'learning_rate': 0.0002733165288004701, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8564322571700095}. Best is trial 45 with value: 0.031746342833259424.[0m
[32m[I 2025-01-11 06:51:06,575][0m Trial 47 finished with value: 0.0965381935889544 and parameters: {'observation_period_num': 136, 'train_rates': 0.918761034000054, 'learning_rate': 0.000517904108881579, 'batch_size': 20, 'step_size': 13, 'gamma': 0.8203214610546401}. Best is trial 45 with value: 0.031746342833259424.[0m
Early stopping at epoch 84
[32m[I 2025-01-11 06:52:07,348][0m Trial 48 finished with value: 0.22054740895400754 and parameters: {'observation_period_num': 33, 'train_rates': 0.7806213847517658, 'learning_rate': 0.00071036264337159, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8350658206563052}. Best is trial 45 with value: 0.031746342833259424.[0m
[32m[I 2025-01-11 06:53:07,493][0m Trial 49 finished with value: 0.03632455038457689 and parameters: {'observation_period_num': 12, 'train_rates': 0.951865732817595, 'learning_rate': 0.0003170255794868499, 'batch_size': 97, 'step_size': 14, 'gamma': 0.9101960444359706}. Best is trial 45 with value: 0.031746342833259424.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-11 06:53:07,502][0m A new study created in memory with name: no-name-4cf496a1-5c64-49ff-aa27-33b31034508f[0m
[32m[I 2025-01-11 06:53:31,942][0m Trial 0 finished with value: 0.5959325950189757 and parameters: {'observation_period_num': 213, 'train_rates': 0.86154111711839, 'learning_rate': 1.4433448437972281e-05, 'batch_size': 224, 'step_size': 6, 'gamma': 0.8230775838536764}. Best is trial 0 with value: 0.5959325950189757.[0m
[32m[I 2025-01-11 06:55:00,851][0m Trial 1 finished with value: 0.7478500572338062 and parameters: {'observation_period_num': 30, 'train_rates': 0.6969109354329052, 'learning_rate': 1.4585041588801044e-06, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8734748279696116}. Best is trial 0 with value: 0.5959325950189757.[0m
[32m[I 2025-01-11 06:55:32,287][0m Trial 2 finished with value: 0.6476985812187195 and parameters: {'observation_period_num': 92, 'train_rates': 0.9599297558891049, 'learning_rate': 6.008113014444354e-06, 'batch_size': 189, 'step_size': 8, 'gamma': 0.989400189741072}. Best is trial 0 with value: 0.5959325950189757.[0m
[32m[I 2025-01-11 06:58:03,124][0m Trial 3 finished with value: 0.5250993120173614 and parameters: {'observation_period_num': 242, 'train_rates': 0.8229119693936336, 'learning_rate': 1.496518787780601e-06, 'batch_size': 31, 'step_size': 11, 'gamma': 0.8260653869047583}. Best is trial 3 with value: 0.5250993120173614.[0m
[32m[I 2025-01-11 06:58:22,380][0m Trial 4 finished with value: 0.698482181051726 and parameters: {'observation_period_num': 216, 'train_rates': 0.6598957261691819, 'learning_rate': 8.341365275740524e-05, 'batch_size': 233, 'step_size': 3, 'gamma': 0.7710984389360587}. Best is trial 3 with value: 0.5250993120173614.[0m
[32m[I 2025-01-11 07:01:12,538][0m Trial 5 finished with value: 0.2255603482595599 and parameters: {'observation_period_num': 83, 'train_rates': 0.6597740324659618, 'learning_rate': 0.0008449466427702864, 'batch_size': 25, 'step_size': 9, 'gamma': 0.9419887689765867}. Best is trial 5 with value: 0.2255603482595599.[0m
[32m[I 2025-01-11 07:01:55,338][0m Trial 6 finished with value: 0.584749086269511 and parameters: {'observation_period_num': 31, 'train_rates': 0.7236010744249242, 'learning_rate': 1.827801072380874e-05, 'batch_size': 112, 'step_size': 7, 'gamma': 0.7897820750838932}. Best is trial 5 with value: 0.2255603482595599.[0m
[32m[I 2025-01-11 07:02:16,598][0m Trial 7 finished with value: 0.6603097376888091 and parameters: {'observation_period_num': 150, 'train_rates': 0.697239441080677, 'learning_rate': 8.314369054127789e-06, 'batch_size': 239, 'step_size': 14, 'gamma': 0.9442792847662865}. Best is trial 5 with value: 0.2255603482595599.[0m
[32m[I 2025-01-11 07:06:31,658][0m Trial 8 finished with value: 0.5181167233102727 and parameters: {'observation_period_num': 112, 'train_rates': 0.8134674953593478, 'learning_rate': 2.075244909907845e-06, 'batch_size': 19, 'step_size': 5, 'gamma': 0.7907461724397794}. Best is trial 5 with value: 0.2255603482595599.[0m
[32m[I 2025-01-11 07:07:19,697][0m Trial 9 finished with value: 0.2077245314915975 and parameters: {'observation_period_num': 157, 'train_rates': 0.9377921244484122, 'learning_rate': 2.0637819096292453e-05, 'batch_size': 116, 'step_size': 15, 'gamma': 0.9501224858256645}. Best is trial 9 with value: 0.2077245314915975.[0m
[32m[I 2025-01-11 07:08:09,413][0m Trial 10 finished with value: 0.14722801744937897 and parameters: {'observation_period_num': 169, 'train_rates': 0.9808263302666426, 'learning_rate': 6.172956075014858e-05, 'batch_size': 116, 'step_size': 15, 'gamma': 0.9097145218840484}. Best is trial 10 with value: 0.14722801744937897.[0m
[32m[I 2025-01-11 07:08:56,962][0m Trial 11 finished with value: 0.11688347160816193 and parameters: {'observation_period_num': 173, 'train_rates': 0.9798177736557436, 'learning_rate': 9.573412356519681e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9176200066730759}. Best is trial 11 with value: 0.11688347160816193.[0m
[32m[I 2025-01-11 07:09:31,347][0m Trial 12 finished with value: 0.11847462138761318 and parameters: {'observation_period_num': 178, 'train_rates': 0.8917553185400198, 'learning_rate': 0.00011027580777170214, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8911459891877211}. Best is trial 11 with value: 0.11688347160816193.[0m
[32m[I 2025-01-11 07:10:05,727][0m Trial 13 finished with value: 0.08171742959824173 and parameters: {'observation_period_num': 188, 'train_rates': 0.8962153166654192, 'learning_rate': 0.0002521626108828074, 'batch_size': 160, 'step_size': 12, 'gamma': 0.8860119631775621}. Best is trial 13 with value: 0.08171742959824173.[0m
[32m[I 2025-01-11 07:11:12,505][0m Trial 14 finished with value: 0.0814028512686491 and parameters: {'observation_period_num': 205, 'train_rates': 0.9056423371937176, 'learning_rate': 0.00035149822754546896, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8420356607232025}. Best is trial 14 with value: 0.0814028512686491.[0m
[32m[I 2025-01-11 07:12:21,875][0m Trial 15 finished with value: 0.08000519138934849 and parameters: {'observation_period_num': 252, 'train_rates': 0.8988238870999279, 'learning_rate': 0.00044234646305019463, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8393796584834184}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:13:21,208][0m Trial 16 finished with value: 0.2811199761645336 and parameters: {'observation_period_num': 243, 'train_rates': 0.7558252065734845, 'learning_rate': 0.0008487889591554157, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8331056116092882}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:14:31,470][0m Trial 17 finished with value: 0.09077827505222182 and parameters: {'observation_period_num': 252, 'train_rates': 0.9241810018516379, 'learning_rate': 0.00030029158992633446, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8515083329608356}. Best is trial 15 with value: 0.08000519138934849.[0m
Early stopping at epoch 79
[32m[I 2025-01-11 07:15:20,719][0m Trial 18 finished with value: 0.25382995181844714 and parameters: {'observation_period_num': 212, 'train_rates': 0.853557673916313, 'learning_rate': 0.0003005015177608408, 'batch_size': 82, 'step_size': 1, 'gamma': 0.8465879099594809}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:16:49,354][0m Trial 19 finished with value: 0.24041664595449022 and parameters: {'observation_period_num': 130, 'train_rates': 0.7586776153657083, 'learning_rate': 0.000392512701339366, 'batch_size': 53, 'step_size': 12, 'gamma': 0.7574789196616879}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:17:37,964][0m Trial 20 finished with value: 0.3866172687268593 and parameters: {'observation_period_num': 201, 'train_rates': 0.6125981724226955, 'learning_rate': 3.7845563786669206e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.7999818529285138}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:18:11,853][0m Trial 21 finished with value: 0.0860621860536797 and parameters: {'observation_period_num': 202, 'train_rates': 0.9125940514865156, 'learning_rate': 0.00017121587879549162, 'batch_size': 157, 'step_size': 12, 'gamma': 0.8749836483189195}. Best is trial 15 with value: 0.08000519138934849.[0m
[32m[I 2025-01-11 07:18:48,318][0m Trial 22 finished with value: 0.07403736674915189 and parameters: {'observation_period_num': 227, 'train_rates': 0.8817707402005753, 'learning_rate': 0.00048341492914725365, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8528370807709718}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:19:14,624][0m Trial 23 finished with value: 0.12609092957879367 and parameters: {'observation_period_num': 227, 'train_rates': 0.8604929722830924, 'learning_rate': 0.0005595667050102141, 'batch_size': 198, 'step_size': 13, 'gamma': 0.8475470966595525}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:20:07,815][0m Trial 24 finished with value: 0.0904583465926957 and parameters: {'observation_period_num': 231, 'train_rates': 0.8782926290667684, 'learning_rate': 0.0001666696712440573, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8596627319122034}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:20:42,902][0m Trial 25 finished with value: 0.13059631584809336 and parameters: {'observation_period_num': 249, 'train_rates': 0.8257392687615125, 'learning_rate': 0.0006607786686214878, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8097068031916963}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:22:22,081][0m Trial 26 finished with value: 0.08669857680797577 and parameters: {'observation_period_num': 193, 'train_rates': 0.9367848665841181, 'learning_rate': 0.0004343936401593663, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8178715869009932}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:22:59,281][0m Trial 27 finished with value: 0.10309046654836505 and parameters: {'observation_period_num': 225, 'train_rates': 0.8450609472506501, 'learning_rate': 0.0002127319058754784, 'batch_size': 138, 'step_size': 8, 'gamma': 0.8355704236822887}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:23:28,452][0m Trial 28 finished with value: 0.14881942104548215 and parameters: {'observation_period_num': 56, 'train_rates': 0.7928982277499933, 'learning_rate': 3.629051328345235e-05, 'batch_size': 183, 'step_size': 14, 'gamma': 0.9066082251010931}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:24:21,721][0m Trial 29 finished with value: 0.07670649271929943 and parameters: {'observation_period_num': 214, 'train_rates': 0.8845661419805995, 'learning_rate': 0.0009676692078758312, 'batch_size': 99, 'step_size': 5, 'gamma': 0.856862958300187}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:25:11,994][0m Trial 30 finished with value: 0.09237433728555536 and parameters: {'observation_period_num': 136, 'train_rates': 0.7901085851707401, 'learning_rate': 0.0009295665432621724, 'batch_size': 97, 'step_size': 5, 'gamma': 0.8917237903806732}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:26:04,506][0m Trial 31 finished with value: 0.08019017041833312 and parameters: {'observation_period_num': 211, 'train_rates': 0.9017873892891791, 'learning_rate': 0.0004927346974260604, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8629751769079927}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:26:45,986][0m Trial 32 finished with value: 0.10127037018537521 and parameters: {'observation_period_num': 232, 'train_rates': 0.952814399900196, 'learning_rate': 0.00048316500403490666, 'batch_size': 131, 'step_size': 3, 'gamma': 0.8651172021920581}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:27:39,080][0m Trial 33 finished with value: 0.08841999999380537 and parameters: {'observation_period_num': 221, 'train_rates': 0.8768786098409161, 'learning_rate': 0.0005706495683355051, 'batch_size': 99, 'step_size': 3, 'gamma': 0.8757161238786224}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:28:57,127][0m Trial 34 finished with value: 0.0978925730813952 and parameters: {'observation_period_num': 191, 'train_rates': 0.87684118987075, 'learning_rate': 0.00017264797657007213, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8197030638206706}. Best is trial 22 with value: 0.07403736674915189.[0m
Early stopping at epoch 94
[32m[I 2025-01-11 07:29:43,934][0m Trial 35 finished with value: 0.18295324994967535 and parameters: {'observation_period_num': 236, 'train_rates': 0.8419309685430465, 'learning_rate': 0.0007174426393069539, 'batch_size': 103, 'step_size': 1, 'gamma': 0.8617709899895775}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:31:51,217][0m Trial 36 finished with value: 0.21269862696598393 and parameters: {'observation_period_num': 214, 'train_rates': 0.9224134176699569, 'learning_rate': 5.5628701426679695e-05, 'batch_size': 41, 'step_size': 2, 'gamma': 0.8832846505002387}. Best is trial 22 with value: 0.07403736674915189.[0m
[32m[I 2025-01-11 07:32:32,625][0m Trial 37 finished with value: 0.0654243677854538 and parameters: {'observation_period_num': 10, 'train_rates': 0.9581669644156457, 'learning_rate': 0.00014567379159899002, 'batch_size': 145, 'step_size': 6, 'gamma': 0.826492619075985}. Best is trial 37 with value: 0.0654243677854538.[0m
[32m[I 2025-01-11 07:33:02,275][0m Trial 38 finished with value: 0.08344938606023788 and parameters: {'observation_period_num': 8, 'train_rates': 0.9666972004571466, 'learning_rate': 0.0001386721893067859, 'batch_size': 210, 'step_size': 7, 'gamma': 0.7733113629685853}. Best is trial 37 with value: 0.0654243677854538.[0m
[32m[I 2025-01-11 07:33:35,735][0m Trial 39 finished with value: 0.06771288812160492 and parameters: {'observation_period_num': 85, 'train_rates': 0.9430253816695114, 'learning_rate': 0.0009924070918670795, 'batch_size': 172, 'step_size': 6, 'gamma': 0.8067366535484081}. Best is trial 37 with value: 0.0654243677854538.[0m
[32m[I 2025-01-11 07:34:09,085][0m Trial 40 finished with value: 0.06383740156888962 and parameters: {'observation_period_num': 65, 'train_rates': 0.9543723092654912, 'learning_rate': 0.0009991522731356817, 'batch_size': 175, 'step_size': 6, 'gamma': 0.8060396367064849}. Best is trial 40 with value: 0.06383740156888962.[0m
[32m[I 2025-01-11 07:34:42,634][0m Trial 41 finished with value: 0.06308860331773758 and parameters: {'observation_period_num': 73, 'train_rates': 0.9492421100140491, 'learning_rate': 0.0009726964758205565, 'batch_size': 173, 'step_size': 6, 'gamma': 0.803367554464176}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:35:16,214][0m Trial 42 finished with value: 0.07310395687818527 and parameters: {'observation_period_num': 91, 'train_rates': 0.9490856406209447, 'learning_rate': 0.0006966523099069795, 'batch_size': 175, 'step_size': 6, 'gamma': 0.7864636330144089}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:35:49,988][0m Trial 43 finished with value: 0.07500480860471725 and parameters: {'observation_period_num': 81, 'train_rates': 0.9536432614925421, 'learning_rate': 0.0006893412353682278, 'batch_size': 177, 'step_size': 6, 'gamma': 0.7811418913764916}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:36:25,544][0m Trial 44 finished with value: 0.0683627799153328 and parameters: {'observation_period_num': 60, 'train_rates': 0.9887505442620959, 'learning_rate': 0.0009372922565835318, 'batch_size': 172, 'step_size': 6, 'gamma': 0.8009492812711656}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:36:55,774][0m Trial 45 finished with value: 1.018792748451233 and parameters: {'observation_period_num': 67, 'train_rates': 0.9836952420552777, 'learning_rate': 3.5814842721644574e-06, 'batch_size': 217, 'step_size': 7, 'gamma': 0.803131560251214}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:37:31,444][0m Trial 46 finished with value: 0.7171280384063721 and parameters: {'observation_period_num': 43, 'train_rates': 0.9647744461898827, 'learning_rate': 8.481434439759018e-06, 'batch_size': 166, 'step_size': 4, 'gamma': 0.7618326682597523}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:37:55,183][0m Trial 47 finished with value: 0.10078514367341995 and parameters: {'observation_period_num': 109, 'train_rates': 0.9365627757680728, 'learning_rate': 0.0009473419953523866, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8083146093295258}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:38:26,717][0m Trial 48 finished with value: 0.0709732174873352 and parameters: {'observation_period_num': 7, 'train_rates': 0.9837495485655888, 'learning_rate': 0.00022841824163699482, 'batch_size': 195, 'step_size': 4, 'gamma': 0.7957719401847091}. Best is trial 41 with value: 0.06308860331773758.[0m
[32m[I 2025-01-11 07:39:04,339][0m Trial 49 finished with value: 0.060116957873106 and parameters: {'observation_period_num': 30, 'train_rates': 0.9898221984737691, 'learning_rate': 0.00034058433302648037, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8271664401688998}. Best is trial 49 with value: 0.060116957873106.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 108, 'train_rates': 0.8913535313684456, 'learning_rate': 0.00018766207829767458, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9450488208571559}
Epoch 1/300, trend Loss: 0.5113 | 0.4208
Epoch 2/300, trend Loss: 0.3629 | 0.3372
Epoch 3/300, trend Loss: 0.3090 | 0.2922
Epoch 4/300, trend Loss: 0.2673 | 0.2269
Epoch 5/300, trend Loss: 0.2349 | 0.1878
Epoch 6/300, trend Loss: 0.2083 | 0.1634
Epoch 7/300, trend Loss: 0.1908 | 0.1444
Epoch 8/300, trend Loss: 0.1766 | 0.1579
Epoch 9/300, trend Loss: 0.1674 | 0.1948
Epoch 10/300, trend Loss: 0.1596 | 0.1273
Epoch 11/300, trend Loss: 0.1508 | 0.1127
Epoch 12/300, trend Loss: 0.1438 | 0.1027
Epoch 13/300, trend Loss: 0.1389 | 0.0952
Epoch 14/300, trend Loss: 0.1355 | 0.0929
Epoch 15/300, trend Loss: 0.1335 | 0.1037
Epoch 16/300, trend Loss: 0.1320 | 0.1350
Epoch 17/300, trend Loss: 0.1300 | 0.1459
Epoch 18/300, trend Loss: 0.1272 | 0.1507
Epoch 19/300, trend Loss: 0.1241 | 0.1470
Epoch 20/300, trend Loss: 0.1212 | 0.1395
Epoch 21/300, trend Loss: 0.1186 | 0.1363
Epoch 22/300, trend Loss: 0.1166 | 0.1310
Epoch 23/300, trend Loss: 0.1151 | 0.1257
Epoch 24/300, trend Loss: 0.1139 | 0.1181
Epoch 25/300, trend Loss: 0.1132 | 0.1048
Epoch 26/300, trend Loss: 0.1131 | 0.0938
Epoch 27/300, trend Loss: 0.1133 | 0.0810
Epoch 28/300, trend Loss: 0.1131 | 0.0768
Epoch 29/300, trend Loss: 0.1124 | 0.0772
Epoch 30/300, trend Loss: 0.1115 | 0.0763
Epoch 31/300, trend Loss: 0.1110 | 0.0760
Epoch 32/300, trend Loss: 0.1103 | 0.0764
Epoch 33/300, trend Loss: 0.1096 | 0.0763
Epoch 34/300, trend Loss: 0.1089 | 0.0767
Epoch 35/300, trend Loss: 0.1083 | 0.0771
Epoch 36/300, trend Loss: 0.1077 | 0.0771
Epoch 37/300, trend Loss: 0.1072 | 0.0773
Epoch 38/300, trend Loss: 0.1067 | 0.0775
Epoch 39/300, trend Loss: 0.1062 | 0.0774
Epoch 40/300, trend Loss: 0.1058 | 0.0773
Epoch 41/300, trend Loss: 0.1054 | 0.0776
Epoch 42/300, trend Loss: 0.1050 | 0.0775
Epoch 43/300, trend Loss: 0.1047 | 0.0773
Epoch 44/300, trend Loss: 0.1044 | 0.0777
Epoch 45/300, trend Loss: 0.1041 | 0.0776
Epoch 46/300, trend Loss: 0.1038 | 0.0773
Epoch 47/300, trend Loss: 0.1035 | 0.0778
Epoch 48/300, trend Loss: 0.1033 | 0.0774
Epoch 49/300, trend Loss: 0.1031 | 0.0769
Epoch 50/300, trend Loss: 0.1029 | 0.0773
Epoch 51/300, trend Loss: 0.1027 | 0.0766
Epoch 52/300, trend Loss: 0.1025 | 0.0756
Epoch 53/300, trend Loss: 0.1024 | 0.0756
Epoch 54/300, trend Loss: 0.1022 | 0.0743
Epoch 55/300, trend Loss: 0.1021 | 0.0726
Epoch 56/300, trend Loss: 0.1019 | 0.0722
Epoch 57/300, trend Loss: 0.1017 | 0.0706
Epoch 58/300, trend Loss: 0.1015 | 0.0690
Epoch 59/300, trend Loss: 0.1011 | 0.0685
Epoch 60/300, trend Loss: 0.1008 | 0.0674
Epoch 61/300, trend Loss: 0.1004 | 0.0667
Epoch 62/300, trend Loss: 0.1000 | 0.0663
Epoch 63/300, trend Loss: 0.0996 | 0.0661
Epoch 64/300, trend Loss: 0.0992 | 0.0663
Epoch 65/300, trend Loss: 0.0989 | 0.0661
Epoch 66/300, trend Loss: 0.0987 | 0.0670
Epoch 67/300, trend Loss: 0.0985 | 0.0685
Epoch 68/300, trend Loss: 0.0985 | 0.0689
Epoch 69/300, trend Loss: 0.0984 | 0.0719
Epoch 70/300, trend Loss: 0.0984 | 0.0757
Epoch 71/300, trend Loss: 0.0983 | 0.0768
Epoch 72/300, trend Loss: 0.0982 | 0.0805
Epoch 73/300, trend Loss: 0.0980 | 0.0831
Epoch 74/300, trend Loss: 0.0977 | 0.0823
Epoch 75/300, trend Loss: 0.0974 | 0.0832
Epoch 76/300, trend Loss: 0.0971 | 0.0830
Epoch 77/300, trend Loss: 0.0967 | 0.0813
Epoch 78/300, trend Loss: 0.0965 | 0.0809
Epoch 79/300, trend Loss: 0.0962 | 0.0802
Epoch 80/300, trend Loss: 0.0959 | 0.0788
Epoch 81/300, trend Loss: 0.0957 | 0.0784
Epoch 82/300, trend Loss: 0.0955 | 0.0776
Epoch 83/300, trend Loss: 0.0954 | 0.0767
Epoch 84/300, trend Loss: 0.0952 | 0.0763
Epoch 85/300, trend Loss: 0.0950 | 0.0757
Epoch 86/300, trend Loss: 0.0949 | 0.0750
Epoch 87/300, trend Loss: 0.0948 | 0.0747
Epoch 88/300, trend Loss: 0.0946 | 0.0742
Epoch 89/300, trend Loss: 0.0945 | 0.0738
Epoch 90/300, trend Loss: 0.0944 | 0.0735
Epoch 91/300, trend Loss: 0.0943 | 0.0731
Epoch 92/300, trend Loss: 0.0942 | 0.0727
Epoch 93/300, trend Loss: 0.0942 | 0.0725
Epoch 94/300, trend Loss: 0.0941 | 0.0722
Epoch 95/300, trend Loss: 0.0940 | 0.0719
Epoch 96/300, trend Loss: 0.0939 | 0.0717
Epoch 97/300, trend Loss: 0.0938 | 0.0714
Epoch 98/300, trend Loss: 0.0938 | 0.0712
Epoch 99/300, trend Loss: 0.0937 | 0.0710
Epoch 100/300, trend Loss: 0.0936 | 0.0707
Epoch 101/300, trend Loss: 0.0936 | 0.0706
Epoch 102/300, trend Loss: 0.0935 | 0.0704
Epoch 103/300, trend Loss: 0.0935 | 0.0702
Epoch 104/300, trend Loss: 0.0934 | 0.0701
Epoch 105/300, trend Loss: 0.0934 | 0.0699
Epoch 106/300, trend Loss: 0.0933 | 0.0697
Epoch 107/300, trend Loss: 0.0933 | 0.0696
Epoch 108/300, trend Loss: 0.0932 | 0.0695
Epoch 109/300, trend Loss: 0.0932 | 0.0693
Epoch 110/300, trend Loss: 0.0931 | 0.0692
Epoch 111/300, trend Loss: 0.0931 | 0.0691
Epoch 112/300, trend Loss: 0.0930 | 0.0689
Epoch 113/300, trend Loss: 0.0930 | 0.0689
Epoch 114/300, trend Loss: 0.0930 | 0.0687
Epoch 115/300, trend Loss: 0.0929 | 0.0686
Epoch 116/300, trend Loss: 0.0929 | 0.0686
Epoch 117/300, trend Loss: 0.0928 | 0.0684
Epoch 118/300, trend Loss: 0.0928 | 0.0683
Epoch 119/300, trend Loss: 0.0928 | 0.0683
Epoch 120/300, trend Loss: 0.0928 | 0.0682
Epoch 121/300, trend Loss: 0.0927 | 0.0680
Epoch 122/300, trend Loss: 0.0927 | 0.0680
Epoch 123/300, trend Loss: 0.0927 | 0.0679
Epoch 124/300, trend Loss: 0.0926 | 0.0678
Epoch 125/300, trend Loss: 0.0926 | 0.0678
Epoch 126/300, trend Loss: 0.0926 | 0.0677
Epoch 127/300, trend Loss: 0.0926 | 0.0676
Epoch 128/300, trend Loss: 0.0925 | 0.0676
Epoch 129/300, trend Loss: 0.0925 | 0.0675
Epoch 130/300, trend Loss: 0.0925 | 0.0674
Epoch 131/300, trend Loss: 0.0925 | 0.0674
Epoch 132/300, trend Loss: 0.0925 | 0.0674
Epoch 133/300, trend Loss: 0.0924 | 0.0673
Epoch 134/300, trend Loss: 0.0924 | 0.0673
Epoch 135/300, trend Loss: 0.0924 | 0.0672
Epoch 136/300, trend Loss: 0.0924 | 0.0671
Epoch 137/300, trend Loss: 0.0924 | 0.0671
Epoch 138/300, trend Loss: 0.0923 | 0.0671
Epoch 139/300, trend Loss: 0.0923 | 0.0670
Epoch 140/300, trend Loss: 0.0923 | 0.0670
Epoch 141/300, trend Loss: 0.0923 | 0.0670
Epoch 142/300, trend Loss: 0.0923 | 0.0669
Epoch 143/300, trend Loss: 0.0923 | 0.0669
Epoch 144/300, trend Loss: 0.0923 | 0.0668
Epoch 145/300, trend Loss: 0.0922 | 0.0668
Epoch 146/300, trend Loss: 0.0922 | 0.0668
Epoch 147/300, trend Loss: 0.0922 | 0.0668
Epoch 148/300, trend Loss: 0.0922 | 0.0667
Epoch 149/300, trend Loss: 0.0922 | 0.0667
Epoch 150/300, trend Loss: 0.0922 | 0.0667
Epoch 151/300, trend Loss: 0.0922 | 0.0666
Epoch 152/300, trend Loss: 0.0922 | 0.0666
Epoch 153/300, trend Loss: 0.0922 | 0.0666
Epoch 154/300, trend Loss: 0.0922 | 0.0666
Epoch 155/300, trend Loss: 0.0921 | 0.0666
Epoch 156/300, trend Loss: 0.0921 | 0.0665
Epoch 157/300, trend Loss: 0.0921 | 0.0665
Epoch 158/300, trend Loss: 0.0921 | 0.0665
Epoch 159/300, trend Loss: 0.0921 | 0.0665
Epoch 160/300, trend Loss: 0.0921 | 0.0664
Epoch 161/300, trend Loss: 0.0921 | 0.0664
Epoch 162/300, trend Loss: 0.0921 | 0.0664
Epoch 163/300, trend Loss: 0.0921 | 0.0664
Epoch 164/300, trend Loss: 0.0921 | 0.0664
Epoch 165/300, trend Loss: 0.0921 | 0.0664
Epoch 166/300, trend Loss: 0.0921 | 0.0664
Epoch 167/300, trend Loss: 0.0921 | 0.0663
Epoch 168/300, trend Loss: 0.0921 | 0.0663
Epoch 169/300, trend Loss: 0.0921 | 0.0663
Epoch 170/300, trend Loss: 0.0920 | 0.0663
Epoch 171/300, trend Loss: 0.0920 | 0.0663
Epoch 172/300, trend Loss: 0.0920 | 0.0663
Epoch 173/300, trend Loss: 0.0920 | 0.0663
Epoch 174/300, trend Loss: 0.0920 | 0.0663
Epoch 175/300, trend Loss: 0.0920 | 0.0662
Epoch 176/300, trend Loss: 0.0920 | 0.0662
Epoch 177/300, trend Loss: 0.0920 | 0.0662
Epoch 178/300, trend Loss: 0.0920 | 0.0662
Epoch 179/300, trend Loss: 0.0920 | 0.0662
Epoch 180/300, trend Loss: 0.0920 | 0.0662
Epoch 181/300, trend Loss: 0.0920 | 0.0662
Epoch 182/300, trend Loss: 0.0920 | 0.0662
Epoch 183/300, trend Loss: 0.0920 | 0.0662
Epoch 184/300, trend Loss: 0.0920 | 0.0662
Epoch 185/300, trend Loss: 0.0920 | 0.0662
Epoch 186/300, trend Loss: 0.0920 | 0.0662
Epoch 187/300, trend Loss: 0.0920 | 0.0662
Epoch 188/300, trend Loss: 0.0920 | 0.0661
Epoch 189/300, trend Loss: 0.0920 | 0.0661
Epoch 190/300, trend Loss: 0.0920 | 0.0661
Epoch 191/300, trend Loss: 0.0920 | 0.0661
Epoch 192/300, trend Loss: 0.0920 | 0.0661
Epoch 193/300, trend Loss: 0.0920 | 0.0661
Epoch 194/300, trend Loss: 0.0920 | 0.0661
Epoch 195/300, trend Loss: 0.0920 | 0.0661
Epoch 196/300, trend Loss: 0.0920 | 0.0661
Epoch 197/300, trend Loss: 0.0920 | 0.0661
Epoch 198/300, trend Loss: 0.0920 | 0.0661
Epoch 199/300, trend Loss: 0.0920 | 0.0661
Epoch 200/300, trend Loss: 0.0920 | 0.0661
Epoch 201/300, trend Loss: 0.0920 | 0.0661
Epoch 202/300, trend Loss: 0.0920 | 0.0661
Epoch 203/300, trend Loss: 0.0920 | 0.0661
Epoch 204/300, trend Loss: 0.0920 | 0.0661
Epoch 205/300, trend Loss: 0.0920 | 0.0661
Epoch 206/300, trend Loss: 0.0920 | 0.0661
Epoch 207/300, trend Loss: 0.0920 | 0.0661
Epoch 208/300, trend Loss: 0.0920 | 0.0661
Epoch 209/300, trend Loss: 0.0919 | 0.0661
Epoch 210/300, trend Loss: 0.0919 | 0.0661
Epoch 211/300, trend Loss: 0.0919 | 0.0661
Epoch 212/300, trend Loss: 0.0919 | 0.0660
Epoch 213/300, trend Loss: 0.0919 | 0.0660
Epoch 214/300, trend Loss: 0.0919 | 0.0660
Epoch 215/300, trend Loss: 0.0919 | 0.0660
Epoch 216/300, trend Loss: 0.0919 | 0.0660
Epoch 217/300, trend Loss: 0.0919 | 0.0660
Epoch 218/300, trend Loss: 0.0919 | 0.0660
Epoch 219/300, trend Loss: 0.0919 | 0.0660
Epoch 220/300, trend Loss: 0.0919 | 0.0660
Epoch 221/300, trend Loss: 0.0919 | 0.0660
Epoch 222/300, trend Loss: 0.0919 | 0.0660
Epoch 223/300, trend Loss: 0.0919 | 0.0660
Epoch 224/300, trend Loss: 0.0919 | 0.0660
Epoch 225/300, trend Loss: 0.0919 | 0.0660
Epoch 226/300, trend Loss: 0.0919 | 0.0660
Epoch 227/300, trend Loss: 0.0919 | 0.0660
Epoch 228/300, trend Loss: 0.0919 | 0.0660
Epoch 229/300, trend Loss: 0.0919 | 0.0660
Epoch 230/300, trend Loss: 0.0919 | 0.0660
Epoch 231/300, trend Loss: 0.0919 | 0.0660
Epoch 232/300, trend Loss: 0.0919 | 0.0660
Epoch 233/300, trend Loss: 0.0919 | 0.0660
Epoch 234/300, trend Loss: 0.0919 | 0.0660
Epoch 235/300, trend Loss: 0.0919 | 0.0660
Epoch 236/300, trend Loss: 0.0919 | 0.0660
Epoch 237/300, trend Loss: 0.0919 | 0.0660
Epoch 238/300, trend Loss: 0.0919 | 0.0660
Epoch 239/300, trend Loss: 0.0919 | 0.0660
Epoch 240/300, trend Loss: 0.0919 | 0.0660
Epoch 241/300, trend Loss: 0.0919 | 0.0660
Epoch 242/300, trend Loss: 0.0919 | 0.0660
Epoch 243/300, trend Loss: 0.0919 | 0.0660
Epoch 244/300, trend Loss: 0.0919 | 0.0660
Epoch 245/300, trend Loss: 0.0919 | 0.0660
Epoch 246/300, trend Loss: 0.0919 | 0.0660
Epoch 247/300, trend Loss: 0.0919 | 0.0660
Epoch 248/300, trend Loss: 0.0919 | 0.0660
Epoch 249/300, trend Loss: 0.0919 | 0.0660
Epoch 250/300, trend Loss: 0.0919 | 0.0660
Epoch 251/300, trend Loss: 0.0919 | 0.0660
Epoch 252/300, trend Loss: 0.0919 | 0.0660
Epoch 253/300, trend Loss: 0.0919 | 0.0660
Epoch 254/300, trend Loss: 0.0919 | 0.0660
Epoch 255/300, trend Loss: 0.0919 | 0.0660
Epoch 256/300, trend Loss: 0.0919 | 0.0660
Epoch 257/300, trend Loss: 0.0919 | 0.0660
Epoch 258/300, trend Loss: 0.0919 | 0.0660
Epoch 259/300, trend Loss: 0.0919 | 0.0660
Epoch 260/300, trend Loss: 0.0919 | 0.0660
Epoch 261/300, trend Loss: 0.0919 | 0.0660
Epoch 262/300, trend Loss: 0.0919 | 0.0660
Epoch 263/300, trend Loss: 0.0919 | 0.0660
Epoch 264/300, trend Loss: 0.0919 | 0.0660
Epoch 265/300, trend Loss: 0.0919 | 0.0660
Epoch 266/300, trend Loss: 0.0919 | 0.0660
Epoch 267/300, trend Loss: 0.0919 | 0.0660
Epoch 268/300, trend Loss: 0.0919 | 0.0660
Epoch 269/300, trend Loss: 0.0919 | 0.0660
Epoch 270/300, trend Loss: 0.0919 | 0.0660
Epoch 271/300, trend Loss: 0.0919 | 0.0660
Epoch 272/300, trend Loss: 0.0919 | 0.0660
Epoch 273/300, trend Loss: 0.0919 | 0.0660
Epoch 274/300, trend Loss: 0.0919 | 0.0660
Epoch 275/300, trend Loss: 0.0919 | 0.0660
Epoch 276/300, trend Loss: 0.0919 | 0.0660
Epoch 277/300, trend Loss: 0.0919 | 0.0660
Epoch 278/300, trend Loss: 0.0919 | 0.0660
Epoch 279/300, trend Loss: 0.0919 | 0.0660
Epoch 280/300, trend Loss: 0.0919 | 0.0660
Epoch 281/300, trend Loss: 0.0919 | 0.0660
Epoch 282/300, trend Loss: 0.0919 | 0.0660
Epoch 283/300, trend Loss: 0.0919 | 0.0660
Epoch 284/300, trend Loss: 0.0919 | 0.0660
Epoch 285/300, trend Loss: 0.0919 | 0.0660
Epoch 286/300, trend Loss: 0.0919 | 0.0661
Epoch 287/300, trend Loss: 0.0919 | 0.0661
Epoch 288/300, trend Loss: 0.0919 | 0.0661
Epoch 289/300, trend Loss: 0.0919 | 0.0661
Epoch 290/300, trend Loss: 0.0919 | 0.0661
Epoch 291/300, trend Loss: 0.0919 | 0.0661
Epoch 292/300, trend Loss: 0.0919 | 0.0661
Epoch 293/300, trend Loss: 0.0919 | 0.0661
Epoch 294/300, trend Loss: 0.0919 | 0.0661
Epoch 295/300, trend Loss: 0.0919 | 0.0661
Epoch 296/300, trend Loss: 0.0919 | 0.0661
Epoch 297/300, trend Loss: 0.0919 | 0.0661
Epoch 298/300, trend Loss: 0.0919 | 0.0661
Epoch 299/300, trend Loss: 0.0919 | 0.0661
Epoch 300/300, trend Loss: 0.0919 | 0.0661
Training seasonal_0 component with params: {'observation_period_num': 10, 'train_rates': 0.9307862553509456, 'learning_rate': 0.0007503049398560546, 'batch_size': 201, 'step_size': 5, 'gamma': 0.9493272251459248}
Epoch 1/300, seasonal_0 Loss: 3.9527 | 1.4895
Epoch 2/300, seasonal_0 Loss: 1.2515 | 1.4574
Epoch 3/300, seasonal_0 Loss: 1.1857 | 1.5034
Epoch 4/300, seasonal_0 Loss: 1.3857 | 1.4632
Epoch 5/300, seasonal_0 Loss: 1.5231 | 1.6924
Epoch 6/300, seasonal_0 Loss: 1.2007 | 2.0817
Epoch 7/300, seasonal_0 Loss: 0.9571 | 1.7947
Epoch 8/300, seasonal_0 Loss: 1.0556 | 1.9298
Epoch 9/300, seasonal_0 Loss: 0.9998 | 1.8853
Epoch 10/300, seasonal_0 Loss: 1.0120 | 1.8983
Epoch 11/300, seasonal_0 Loss: 1.0046 | 1.9082
Epoch 12/300, seasonal_0 Loss: 0.9994 | 1.8992
Epoch 13/300, seasonal_0 Loss: 1.0013 | 1.9010
Epoch 14/300, seasonal_0 Loss: 0.9927 | 1.8907
Epoch 15/300, seasonal_0 Loss: 0.9093 | 1.9060
Epoch 16/300, seasonal_0 Loss: 0.7639 | 0.4886
Epoch 17/300, seasonal_0 Loss: 0.8381 | 1.4915
Epoch 18/300, seasonal_0 Loss: 1.2395 | 2.0951
Epoch 19/300, seasonal_0 Loss: 0.9493 | 1.8526
Epoch 20/300, seasonal_0 Loss: 1.0099 | 1.9762
Epoch 21/300, seasonal_0 Loss: 0.9682 | 1.9164
Epoch 22/300, seasonal_0 Loss: 0.9848 | 1.9507
Epoch 23/300, seasonal_0 Loss: 0.9744 | 1.9331
Epoch 24/300, seasonal_0 Loss: 0.9767 | 1.9487
Epoch 25/300, seasonal_0 Loss: 0.9724 | 1.9432
Epoch 26/300, seasonal_0 Loss: 0.9713 | 1.9504
Epoch 27/300, seasonal_0 Loss: 0.9696 | 1.9497
Epoch 28/300, seasonal_0 Loss: 0.9695 | 1.9491
Epoch 29/300, seasonal_0 Loss: 0.9676 | 1.9549
Epoch 30/300, seasonal_0 Loss: 0.9662 | 1.9540
Epoch 31/300, seasonal_0 Loss: 0.9643 | 1.9577
Epoch 32/300, seasonal_0 Loss: 0.9636 | 1.9585
Epoch 33/300, seasonal_0 Loss: 0.9632 | 1.9571
Epoch 34/300, seasonal_0 Loss: 0.9617 | 1.9618
Epoch 35/300, seasonal_0 Loss: 0.9608 | 1.9618
Epoch 36/300, seasonal_0 Loss: 0.9589 | 1.9640
Epoch 37/300, seasonal_0 Loss: 0.9587 | 1.9654
Epoch 38/300, seasonal_0 Loss: 0.9582 | 1.9639
Epoch 39/300, seasonal_0 Loss: 0.9570 | 1.9675
Epoch 40/300, seasonal_0 Loss: 0.9564 | 1.9681
Epoch 41/300, seasonal_0 Loss: 0.9546 | 1.9694
Epoch 42/300, seasonal_0 Loss: 0.9546 | 1.9710
Epoch 43/300, seasonal_0 Loss: 0.9541 | 1.9697
Epoch 44/300, seasonal_0 Loss: 0.9530 | 1.9723
Epoch 45/300, seasonal_0 Loss: 0.9527 | 1.9734
Epoch 46/300, seasonal_0 Loss: 0.9510 | 1.9741
Epoch 47/300, seasonal_0 Loss: 0.9511 | 1.9756
Epoch 48/300, seasonal_0 Loss: 0.9507 | 1.9746
Epoch 49/300, seasonal_0 Loss: 0.9496 | 1.9765
Epoch 50/300, seasonal_0 Loss: 0.9495 | 1.9777
Epoch 51/300, seasonal_0 Loss: 0.9480 | 1.9782
Epoch 52/300, seasonal_0 Loss: 0.9481 | 1.9795
Epoch 53/300, seasonal_0 Loss: 0.9478 | 1.9788
Epoch 54/300, seasonal_0 Loss: 0.9468 | 1.9801
Epoch 55/300, seasonal_0 Loss: 0.9468 | 1.9814
Epoch 56/300, seasonal_0 Loss: 0.9454 | 1.9818
Epoch 57/300, seasonal_0 Loss: 0.9455 | 1.9829
Epoch 58/300, seasonal_0 Loss: 0.9453 | 1.9824
Epoch 59/300, seasonal_0 Loss: 0.9444 | 1.9834
Epoch 60/300, seasonal_0 Loss: 0.9444 | 1.9846
Epoch 61/300, seasonal_0 Loss: 0.9432 | 1.9850
Epoch 62/300, seasonal_0 Loss: 0.9433 | 1.9859
Epoch 63/300, seasonal_0 Loss: 0.9431 | 1.9855
Epoch 64/300, seasonal_0 Loss: 0.9423 | 1.9862
Epoch 65/300, seasonal_0 Loss: 0.9423 | 1.9874
Epoch 66/300, seasonal_0 Loss: 0.9413 | 1.9877
Epoch 67/300, seasonal_0 Loss: 0.9413 | 1.9885
Epoch 68/300, seasonal_0 Loss: 0.9412 | 1.9882
Epoch 69/300, seasonal_0 Loss: 0.9405 | 1.9888
Epoch 70/300, seasonal_0 Loss: 0.9405 | 1.9898
Epoch 71/300, seasonal_0 Loss: 0.9396 | 1.9901
Epoch 72/300, seasonal_0 Loss: 0.9396 | 1.9908
Epoch 73/300, seasonal_0 Loss: 0.9396 | 1.9906
Epoch 74/300, seasonal_0 Loss: 0.9388 | 1.9910
Epoch 75/300, seasonal_0 Loss: 0.9389 | 1.9919
Epoch 76/300, seasonal_0 Loss: 0.9381 | 1.9923
Epoch 77/300, seasonal_0 Loss: 0.9381 | 1.9929
Epoch 78/300, seasonal_0 Loss: 0.9381 | 1.9927
Epoch 79/300, seasonal_0 Loss: 0.9374 | 1.9930
Epoch 80/300, seasonal_0 Loss: 0.9375 | 1.9938
Epoch 81/300, seasonal_0 Loss: 0.9368 | 1.9942
Epoch 82/300, seasonal_0 Loss: 0.9368 | 1.9947
Epoch 83/300, seasonal_0 Loss: 0.9368 | 1.9946
Epoch 84/300, seasonal_0 Loss: 0.9362 | 1.9948
Epoch 85/300, seasonal_0 Loss: 0.9362 | 1.9955
Epoch 86/300, seasonal_0 Loss: 0.9356 | 1.9959
Epoch 87/300, seasonal_0 Loss: 0.9356 | 1.9963
Epoch 88/300, seasonal_0 Loss: 0.9356 | 1.9963
Epoch 89/300, seasonal_0 Loss: 0.9351 | 1.9965
Epoch 90/300, seasonal_0 Loss: 0.9351 | 1.9970
Epoch 91/300, seasonal_0 Loss: 0.9346 | 1.9974
Epoch 92/300, seasonal_0 Loss: 0.9346 | 1.9978
Epoch 93/300, seasonal_0 Loss: 0.9345 | 1.9978
Epoch 94/300, seasonal_0 Loss: 0.9341 | 1.9979
Epoch 95/300, seasonal_0 Loss: 0.9341 | 1.9984
Epoch 96/300, seasonal_0 Loss: 0.9336 | 1.9987
Epoch 97/300, seasonal_0 Loss: 0.9336 | 1.9991
Epoch 98/300, seasonal_0 Loss: 0.9336 | 1.9991
Epoch 99/300, seasonal_0 Loss: 0.9332 | 1.9992
Epoch 100/300, seasonal_0 Loss: 0.9332 | 1.9996
Epoch 101/300, seasonal_0 Loss: 0.9328 | 1.9999
Epoch 102/300, seasonal_0 Loss: 0.9328 | 2.0003
Epoch 103/300, seasonal_0 Loss: 0.9328 | 2.0003
Epoch 104/300, seasonal_0 Loss: 0.9324 | 2.0004
Epoch 105/300, seasonal_0 Loss: 0.9324 | 2.0008
Epoch 106/300, seasonal_0 Loss: 0.9320 | 2.0010
Epoch 107/300, seasonal_0 Loss: 0.9320 | 2.0013
Epoch 108/300, seasonal_0 Loss: 0.9320 | 2.0014
Epoch 109/300, seasonal_0 Loss: 0.9316 | 2.0015
Epoch 110/300, seasonal_0 Loss: 0.9317 | 2.0018
Epoch 111/300, seasonal_0 Loss: 0.9313 | 2.0020
Epoch 112/300, seasonal_0 Loss: 0.9313 | 2.0023
Epoch 113/300, seasonal_0 Loss: 0.9313 | 2.0024
Epoch 114/300, seasonal_0 Loss: 0.9310 | 2.0025
Epoch 115/300, seasonal_0 Loss: 0.9310 | 2.0027
Epoch 116/300, seasonal_0 Loss: 0.9307 | 2.0029
Epoch 117/300, seasonal_0 Loss: 0.9307 | 2.0032
Epoch 118/300, seasonal_0 Loss: 0.9307 | 2.0033
Epoch 119/300, seasonal_0 Loss: 0.9304 | 2.0033
Epoch 120/300, seasonal_0 Loss: 0.9304 | 2.0036
Epoch 121/300, seasonal_0 Loss: 0.9301 | 2.0037
Epoch 122/300, seasonal_0 Loss: 0.9301 | 2.0040
Epoch 123/300, seasonal_0 Loss: 0.9301 | 2.0041
Epoch 124/300, seasonal_0 Loss: 0.9299 | 2.0041
Epoch 125/300, seasonal_0 Loss: 0.9299 | 2.0043
Epoch 126/300, seasonal_0 Loss: 0.9296 | 2.0045
Epoch 127/300, seasonal_0 Loss: 0.9296 | 2.0047
Epoch 128/300, seasonal_0 Loss: 0.9296 | 2.0048
Epoch 129/300, seasonal_0 Loss: 0.9294 | 2.0048
Epoch 130/300, seasonal_0 Loss: 0.9294 | 2.0050
Epoch 131/300, seasonal_0 Loss: 0.9292 | 2.0051
Epoch 132/300, seasonal_0 Loss: 0.9292 | 2.0053
Epoch 133/300, seasonal_0 Loss: 0.9292 | 2.0054
Epoch 134/300, seasonal_0 Loss: 0.9289 | 2.0055
Epoch 135/300, seasonal_0 Loss: 0.9290 | 2.0056
Epoch 136/300, seasonal_0 Loss: 0.9287 | 2.0058
Epoch 137/300, seasonal_0 Loss: 0.9288 | 2.0059
Epoch 138/300, seasonal_0 Loss: 0.9288 | 2.0060
Epoch 139/300, seasonal_0 Loss: 0.9285 | 2.0061
Epoch 140/300, seasonal_0 Loss: 0.9286 | 2.0062
Epoch 141/300, seasonal_0 Loss: 0.9284 | 2.0063
Epoch 142/300, seasonal_0 Loss: 0.9284 | 2.0065
Epoch 143/300, seasonal_0 Loss: 0.9284 | 2.0065
Epoch 144/300, seasonal_0 Loss: 0.9282 | 2.0066
Epoch 145/300, seasonal_0 Loss: 0.9282 | 2.0067
Epoch 146/300, seasonal_0 Loss: 0.9280 | 2.0068
Epoch 147/300, seasonal_0 Loss: 0.9280 | 2.0069
Epoch 148/300, seasonal_0 Loss: 0.9280 | 2.0070
Epoch 149/300, seasonal_0 Loss: 0.9279 | 2.0071
Epoch 150/300, seasonal_0 Loss: 0.9279 | 2.0072
Epoch 151/300, seasonal_0 Loss: 0.9277 | 2.0073
Epoch 152/300, seasonal_0 Loss: 0.9277 | 2.0074
Epoch 153/300, seasonal_0 Loss: 0.9277 | 2.0075
Epoch 154/300, seasonal_0 Loss: 0.9276 | 2.0075
Epoch 155/300, seasonal_0 Loss: 0.9276 | 2.0076
Epoch 156/300, seasonal_0 Loss: 0.9274 | 2.0077
Epoch 157/300, seasonal_0 Loss: 0.9274 | 2.0078
Epoch 158/300, seasonal_0 Loss: 0.9274 | 2.0079
Epoch 159/300, seasonal_0 Loss: 0.9273 | 2.0079
Epoch 160/300, seasonal_0 Loss: 0.9273 | 2.0080
Epoch 161/300, seasonal_0 Loss: 0.9272 | 2.0081
Epoch 162/300, seasonal_0 Loss: 0.9272 | 2.0082
Epoch 163/300, seasonal_0 Loss: 0.9272 | 2.0082
Epoch 164/300, seasonal_0 Loss: 0.9270 | 2.0083
Epoch 165/300, seasonal_0 Loss: 0.9271 | 2.0084
Epoch 166/300, seasonal_0 Loss: 0.9269 | 2.0084
Epoch 167/300, seasonal_0 Loss: 0.9269 | 2.0085
Epoch 168/300, seasonal_0 Loss: 0.9269 | 2.0086
Epoch 169/300, seasonal_0 Loss: 0.9268 | 2.0086
Epoch 170/300, seasonal_0 Loss: 0.9268 | 2.0087
Epoch 171/300, seasonal_0 Loss: 0.9267 | 2.0087
Epoch 172/300, seasonal_0 Loss: 0.9267 | 2.0088
Epoch 173/300, seasonal_0 Loss: 0.9267 | 2.0089
Epoch 174/300, seasonal_0 Loss: 0.9266 | 2.0089
Epoch 175/300, seasonal_0 Loss: 0.9266 | 2.0090
Epoch 176/300, seasonal_0 Loss: 0.9265 | 2.0090
Epoch 177/300, seasonal_0 Loss: 0.9265 | 2.0091
Epoch 178/300, seasonal_0 Loss: 0.9265 | 2.0091
Epoch 179/300, seasonal_0 Loss: 0.9264 | 2.0092
Epoch 180/300, seasonal_0 Loss: 0.9264 | 2.0092
Epoch 181/300, seasonal_0 Loss: 0.9264 | 2.0093
Epoch 182/300, seasonal_0 Loss: 0.9264 | 2.0094
Epoch 183/300, seasonal_0 Loss: 0.9264 | 2.0094
Epoch 184/300, seasonal_0 Loss: 0.9263 | 2.0094
Epoch 185/300, seasonal_0 Loss: 0.9263 | 2.0095
Epoch 186/300, seasonal_0 Loss: 0.9262 | 2.0095
Epoch 187/300, seasonal_0 Loss: 0.9262 | 2.0096
Epoch 188/300, seasonal_0 Loss: 0.9262 | 2.0096
Epoch 189/300, seasonal_0 Loss: 0.9261 | 2.0097
Epoch 190/300, seasonal_0 Loss: 0.9261 | 2.0097
Epoch 191/300, seasonal_0 Loss: 0.9260 | 2.0098
Epoch 192/300, seasonal_0 Loss: 0.9260 | 2.0098
Epoch 193/300, seasonal_0 Loss: 0.9260 | 2.0098
Epoch 194/300, seasonal_0 Loss: 0.9260 | 2.0099
Epoch 195/300, seasonal_0 Loss: 0.9260 | 2.0099
Epoch 196/300, seasonal_0 Loss: 0.9259 | 2.0099
Epoch 197/300, seasonal_0 Loss: 0.9259 | 2.0100
Epoch 198/300, seasonal_0 Loss: 0.9259 | 2.0100
Epoch 199/300, seasonal_0 Loss: 0.9259 | 2.0101
Epoch 200/300, seasonal_0 Loss: 0.9259 | 2.0101
Epoch 201/300, seasonal_0 Loss: 0.9258 | 2.0101
Epoch 202/300, seasonal_0 Loss: 0.9258 | 2.0102
Epoch 203/300, seasonal_0 Loss: 0.9258 | 2.0102
Epoch 204/300, seasonal_0 Loss: 0.9257 | 2.0102
Epoch 205/300, seasonal_0 Loss: 0.9257 | 2.0103
Epoch 206/300, seasonal_0 Loss: 0.9257 | 2.0103
Epoch 207/300, seasonal_0 Loss: 0.9257 | 2.0103
Epoch 208/300, seasonal_0 Loss: 0.9257 | 2.0104
Epoch 209/300, seasonal_0 Loss: 0.9256 | 2.0104
Epoch 210/300, seasonal_0 Loss: 0.9256 | 2.0104
Epoch 211/300, seasonal_0 Loss: 0.9256 | 2.0104
Epoch 212/300, seasonal_0 Loss: 0.9256 | 2.0105
Epoch 213/300, seasonal_0 Loss: 0.9256 | 2.0105
Epoch 214/300, seasonal_0 Loss: 0.9255 | 2.0105
Epoch 215/300, seasonal_0 Loss: 0.9255 | 2.0105
Epoch 216/300, seasonal_0 Loss: 0.9255 | 2.0106
Epoch 217/300, seasonal_0 Loss: 0.9255 | 2.0106
Epoch 218/300, seasonal_0 Loss: 0.9255 | 2.0106
Epoch 219/300, seasonal_0 Loss: 0.9254 | 2.0107
Epoch 220/300, seasonal_0 Loss: 0.9255 | 2.0107
Epoch 221/300, seasonal_0 Loss: 0.9254 | 2.0107
Epoch 222/300, seasonal_0 Loss: 0.9254 | 2.0107
Epoch 223/300, seasonal_0 Loss: 0.9254 | 2.0107
Epoch 224/300, seasonal_0 Loss: 0.9254 | 2.0108
Epoch 225/300, seasonal_0 Loss: 0.9254 | 2.0108
Epoch 226/300, seasonal_0 Loss: 0.9253 | 2.0108
Epoch 227/300, seasonal_0 Loss: 0.9253 | 2.0108
Epoch 228/300, seasonal_0 Loss: 0.9253 | 2.0109
Epoch 229/300, seasonal_0 Loss: 0.9253 | 2.0109
Epoch 230/300, seasonal_0 Loss: 0.9253 | 2.0109
Epoch 231/300, seasonal_0 Loss: 0.9253 | 2.0109
Epoch 232/300, seasonal_0 Loss: 0.9253 | 2.0109
Epoch 233/300, seasonal_0 Loss: 0.9253 | 2.0110
Epoch 234/300, seasonal_0 Loss: 0.9252 | 2.0110
Epoch 235/300, seasonal_0 Loss: 0.9252 | 2.0110
Epoch 236/300, seasonal_0 Loss: 0.9252 | 2.0110
Epoch 237/300, seasonal_0 Loss: 0.9252 | 2.0110
Epoch 238/300, seasonal_0 Loss: 0.9252 | 2.0110
Epoch 239/300, seasonal_0 Loss: 0.9252 | 2.0111
Epoch 240/300, seasonal_0 Loss: 0.9252 | 2.0111
Epoch 241/300, seasonal_0 Loss: 0.9251 | 2.0111
Epoch 242/300, seasonal_0 Loss: 0.9251 | 2.0111
Epoch 243/300, seasonal_0 Loss: 0.9251 | 2.0111
Epoch 244/300, seasonal_0 Loss: 0.9251 | 2.0111
Epoch 245/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 246/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 247/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 248/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 249/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 250/300, seasonal_0 Loss: 0.9251 | 2.0112
Epoch 251/300, seasonal_0 Loss: 0.9250 | 2.0112
Epoch 252/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 253/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 254/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 255/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 256/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 257/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 258/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 259/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 260/300, seasonal_0 Loss: 0.9250 | 2.0113
Epoch 261/300, seasonal_0 Loss: 0.9250 | 2.0114
Epoch 262/300, seasonal_0 Loss: 0.9250 | 2.0114
Epoch 263/300, seasonal_0 Loss: 0.9250 | 2.0114
Epoch 264/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 265/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 266/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 267/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 268/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 269/300, seasonal_0 Loss: 0.9249 | 2.0114
Epoch 270/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 271/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 272/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 273/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 274/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 275/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 276/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 277/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 278/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 279/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 280/300, seasonal_0 Loss: 0.9249 | 2.0115
Epoch 281/300, seasonal_0 Loss: 0.9248 | 2.0115
Epoch 282/300, seasonal_0 Loss: 0.9248 | 2.0115
Epoch 283/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 284/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 285/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 286/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 287/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 288/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 289/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 290/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 291/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 292/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 293/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 294/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 295/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 296/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 297/300, seasonal_0 Loss: 0.9248 | 2.0116
Epoch 298/300, seasonal_0 Loss: 0.9248 | 2.0117
Epoch 299/300, seasonal_0 Loss: 0.9248 | 2.0117
Epoch 300/300, seasonal_0 Loss: 0.9248 | 2.0117
Training seasonal_1 component with params: {'observation_period_num': 8, 'train_rates': 0.9385509131755957, 'learning_rate': 0.0005680149952013268, 'batch_size': 220, 'step_size': 7, 'gamma': 0.84391861925428}
Epoch 1/300, seasonal_1 Loss: 2.8157 | 0.8768
Epoch 2/300, seasonal_1 Loss: 0.8531 | 0.6896
Epoch 3/300, seasonal_1 Loss: 0.5432 | 0.2074
Epoch 4/300, seasonal_1 Loss: 0.2741 | 0.1765
Epoch 5/300, seasonal_1 Loss: 0.2444 | 0.4176
Epoch 6/300, seasonal_1 Loss: 0.3730 | 0.2242
Epoch 7/300, seasonal_1 Loss: 0.4118 | 0.3940
Epoch 8/300, seasonal_1 Loss: 0.4653 | 0.2331
Epoch 9/300, seasonal_1 Loss: 0.3801 | 0.2810
Epoch 10/300, seasonal_1 Loss: 0.3969 | 0.2725
Epoch 11/300, seasonal_1 Loss: 0.2862 | 0.1437
Epoch 12/300, seasonal_1 Loss: 0.1706 | 0.0962
Epoch 13/300, seasonal_1 Loss: 0.1526 | 0.0843
Epoch 14/300, seasonal_1 Loss: 0.1461 | 0.0859
Epoch 15/300, seasonal_1 Loss: 0.1623 | 0.0896
Epoch 16/300, seasonal_1 Loss: 0.1887 | 0.2570
Epoch 17/300, seasonal_1 Loss: 0.2203 | 0.2009
Epoch 18/300, seasonal_1 Loss: 0.1729 | 0.1083
Epoch 19/300, seasonal_1 Loss: 0.2164 | 0.1831
Epoch 20/300, seasonal_1 Loss: 0.1618 | 0.1138
Epoch 21/300, seasonal_1 Loss: 0.1448 | 0.0835
Epoch 22/300, seasonal_1 Loss: 0.1383 | 0.0669
Epoch 23/300, seasonal_1 Loss: 0.1264 | 0.0651
Epoch 24/300, seasonal_1 Loss: 0.1510 | 0.0610
Epoch 25/300, seasonal_1 Loss: 0.1340 | 0.0670
Epoch 26/300, seasonal_1 Loss: 0.1273 | 0.0712
Epoch 27/300, seasonal_1 Loss: 0.1352 | 0.1230
Epoch 28/300, seasonal_1 Loss: 0.1270 | 0.1304
Epoch 29/300, seasonal_1 Loss: 0.1203 | 0.0596
Epoch 30/300, seasonal_1 Loss: 0.1169 | 0.0934
Epoch 31/300, seasonal_1 Loss: 0.1168 | 0.0597
Epoch 32/300, seasonal_1 Loss: 0.1086 | 0.0794
Epoch 33/300, seasonal_1 Loss: 0.1112 | 0.0598
Epoch 34/300, seasonal_1 Loss: 0.1139 | 0.0768
Epoch 35/300, seasonal_1 Loss: 0.1196 | 0.0792
Epoch 36/300, seasonal_1 Loss: 0.1087 | 0.0588
Epoch 37/300, seasonal_1 Loss: 0.1012 | 0.0694
Epoch 38/300, seasonal_1 Loss: 0.1050 | 0.0671
Epoch 39/300, seasonal_1 Loss: 0.1007 | 0.0722
Epoch 40/300, seasonal_1 Loss: 0.0955 | 0.0546
Epoch 41/300, seasonal_1 Loss: 0.0876 | 0.0558
Epoch 42/300, seasonal_1 Loss: 0.0872 | 0.0529
Epoch 43/300, seasonal_1 Loss: 0.0851 | 0.0537
Epoch 44/300, seasonal_1 Loss: 0.0844 | 0.0490
Epoch 45/300, seasonal_1 Loss: 0.0821 | 0.0507
Epoch 46/300, seasonal_1 Loss: 0.0818 | 0.0481
Epoch 47/300, seasonal_1 Loss: 0.0802 | 0.0477
Epoch 48/300, seasonal_1 Loss: 0.0798 | 0.0459
Epoch 49/300, seasonal_1 Loss: 0.0787 | 0.0470
Epoch 50/300, seasonal_1 Loss: 0.0782 | 0.0466
Epoch 51/300, seasonal_1 Loss: 0.0776 | 0.0458
Epoch 52/300, seasonal_1 Loss: 0.0772 | 0.0450
Epoch 53/300, seasonal_1 Loss: 0.0768 | 0.0444
Epoch 54/300, seasonal_1 Loss: 0.0767 | 0.0439
Epoch 55/300, seasonal_1 Loss: 0.0762 | 0.0438
Epoch 56/300, seasonal_1 Loss: 0.0759 | 0.0448
Epoch 57/300, seasonal_1 Loss: 0.0757 | 0.0466
Epoch 58/300, seasonal_1 Loss: 0.0756 | 0.0458
Epoch 59/300, seasonal_1 Loss: 0.0751 | 0.0428
Epoch 60/300, seasonal_1 Loss: 0.0755 | 0.0414
Epoch 61/300, seasonal_1 Loss: 0.0766 | 0.0433
Epoch 62/300, seasonal_1 Loss: 0.0751 | 0.0439
Epoch 63/300, seasonal_1 Loss: 0.0754 | 0.0449
Epoch 64/300, seasonal_1 Loss: 0.0756 | 0.0412
Epoch 65/300, seasonal_1 Loss: 0.0748 | 0.0429
Epoch 66/300, seasonal_1 Loss: 0.0741 | 0.0422
Epoch 67/300, seasonal_1 Loss: 0.0740 | 0.0407
Epoch 68/300, seasonal_1 Loss: 0.0736 | 0.0409
Epoch 69/300, seasonal_1 Loss: 0.0734 | 0.0408
Epoch 70/300, seasonal_1 Loss: 0.0732 | 0.0420
Epoch 71/300, seasonal_1 Loss: 0.0732 | 0.0409
Epoch 72/300, seasonal_1 Loss: 0.0729 | 0.0400
Epoch 73/300, seasonal_1 Loss: 0.0731 | 0.0397
Epoch 74/300, seasonal_1 Loss: 0.0728 | 0.0402
Epoch 75/300, seasonal_1 Loss: 0.0728 | 0.0421
Epoch 76/300, seasonal_1 Loss: 0.0732 | 0.0391
Epoch 77/300, seasonal_1 Loss: 0.0726 | 0.0402
Epoch 78/300, seasonal_1 Loss: 0.0722 | 0.0402
Epoch 79/300, seasonal_1 Loss: 0.0722 | 0.0394
Epoch 80/300, seasonal_1 Loss: 0.0720 | 0.0394
Epoch 81/300, seasonal_1 Loss: 0.0720 | 0.0393
Epoch 82/300, seasonal_1 Loss: 0.0718 | 0.0396
Epoch 83/300, seasonal_1 Loss: 0.0718 | 0.0395
Epoch 84/300, seasonal_1 Loss: 0.0717 | 0.0389
Epoch 85/300, seasonal_1 Loss: 0.0717 | 0.0392
Epoch 86/300, seasonal_1 Loss: 0.0715 | 0.0391
Epoch 87/300, seasonal_1 Loss: 0.0715 | 0.0390
Epoch 88/300, seasonal_1 Loss: 0.0714 | 0.0386
Epoch 89/300, seasonal_1 Loss: 0.0714 | 0.0389
Epoch 90/300, seasonal_1 Loss: 0.0713 | 0.0388
Epoch 91/300, seasonal_1 Loss: 0.0712 | 0.0386
Epoch 92/300, seasonal_1 Loss: 0.0711 | 0.0385
Epoch 93/300, seasonal_1 Loss: 0.0711 | 0.0386
Epoch 94/300, seasonal_1 Loss: 0.0710 | 0.0384
Epoch 95/300, seasonal_1 Loss: 0.0710 | 0.0383
Epoch 96/300, seasonal_1 Loss: 0.0709 | 0.0383
Epoch 97/300, seasonal_1 Loss: 0.0709 | 0.0383
Epoch 98/300, seasonal_1 Loss: 0.0709 | 0.0382
Epoch 99/300, seasonal_1 Loss: 0.0708 | 0.0382
Epoch 100/300, seasonal_1 Loss: 0.0708 | 0.0382
Epoch 101/300, seasonal_1 Loss: 0.0708 | 0.0381
Epoch 102/300, seasonal_1 Loss: 0.0707 | 0.0381
Epoch 103/300, seasonal_1 Loss: 0.0707 | 0.0381
Epoch 104/300, seasonal_1 Loss: 0.0706 | 0.0380
Epoch 105/300, seasonal_1 Loss: 0.0706 | 0.0380
Epoch 106/300, seasonal_1 Loss: 0.0706 | 0.0380
Epoch 107/300, seasonal_1 Loss: 0.0706 | 0.0379
Epoch 108/300, seasonal_1 Loss: 0.0705 | 0.0379
Epoch 109/300, seasonal_1 Loss: 0.0705 | 0.0379
Epoch 110/300, seasonal_1 Loss: 0.0705 | 0.0379
Epoch 111/300, seasonal_1 Loss: 0.0705 | 0.0378
Epoch 112/300, seasonal_1 Loss: 0.0704 | 0.0378
Epoch 113/300, seasonal_1 Loss: 0.0704 | 0.0378
Epoch 114/300, seasonal_1 Loss: 0.0704 | 0.0378
Epoch 115/300, seasonal_1 Loss: 0.0704 | 0.0378
Epoch 116/300, seasonal_1 Loss: 0.0704 | 0.0377
Epoch 117/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 118/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 119/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 120/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 121/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 122/300, seasonal_1 Loss: 0.0703 | 0.0377
Epoch 123/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 124/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 125/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 126/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 127/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 128/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 129/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 130/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 131/300, seasonal_1 Loss: 0.0701 | 0.0376
Epoch 132/300, seasonal_1 Loss: 0.0701 | 0.0376
Epoch 133/300, seasonal_1 Loss: 0.0701 | 0.0376
Epoch 134/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 135/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 136/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 137/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 138/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 139/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 140/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 141/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 142/300, seasonal_1 Loss: 0.0701 | 0.0375
Epoch 143/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 144/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 145/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 146/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 147/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 148/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 149/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 150/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 151/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 152/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 153/300, seasonal_1 Loss: 0.0700 | 0.0375
Epoch 154/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 155/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 156/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 157/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 158/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 159/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 160/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 161/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 162/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 163/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 164/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 165/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 166/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 167/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 168/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 169/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 170/300, seasonal_1 Loss: 0.0700 | 0.0374
Epoch 171/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 172/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 173/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 174/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 175/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 176/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 177/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 178/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 179/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 180/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 181/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 182/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 183/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 184/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 185/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 186/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 187/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 188/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 189/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 190/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 191/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 192/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 193/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 194/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 195/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 196/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 197/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 198/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 199/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 200/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 201/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 202/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 203/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 204/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 205/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 206/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 207/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 208/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 209/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 210/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 211/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 212/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 213/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 214/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 215/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 216/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 217/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 218/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 219/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 220/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 221/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 222/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 223/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 224/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 225/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 226/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 227/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 228/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 229/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 230/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 231/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 232/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 233/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 234/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 235/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 236/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 237/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 238/300, seasonal_1 Loss: 0.0699 | 0.0374
Epoch 239/300, seasonal_1 Loss: 0.0699 | 0.0374
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 13, 'train_rates': 0.8646647291462399, 'learning_rate': 0.0003461364670810924, 'batch_size': 110, 'step_size': 1, 'gamma': 0.9875158085457361}
Epoch 1/300, seasonal_2 Loss: 1.1891 | 0.1813
Epoch 2/300, seasonal_2 Loss: 0.2028 | 0.2621
Epoch 3/300, seasonal_2 Loss: 0.1758 | 0.1126
Epoch 4/300, seasonal_2 Loss: 0.1863 | 0.1074
Epoch 5/300, seasonal_2 Loss: 0.1505 | 0.0949
Epoch 6/300, seasonal_2 Loss: 0.1285 | 0.0756
Epoch 7/300, seasonal_2 Loss: 0.1477 | 0.1167
Epoch 8/300, seasonal_2 Loss: 0.1252 | 0.0973
Epoch 9/300, seasonal_2 Loss: 0.1176 | 0.0629
Epoch 10/300, seasonal_2 Loss: 0.1434 | 0.0851
Epoch 11/300, seasonal_2 Loss: 0.1197 | 0.0705
Epoch 12/300, seasonal_2 Loss: 0.1338 | 0.0803
Epoch 13/300, seasonal_2 Loss: 0.1307 | 0.1321
Epoch 14/300, seasonal_2 Loss: 0.1532 | 0.1500
Epoch 15/300, seasonal_2 Loss: 0.1334 | 0.1195
Epoch 16/300, seasonal_2 Loss: 0.1408 | 0.0852
Epoch 17/300, seasonal_2 Loss: 0.1605 | 0.0900
Epoch 18/300, seasonal_2 Loss: 0.1751 | 0.1716
Epoch 19/300, seasonal_2 Loss: 0.1661 | 0.1206
Epoch 20/300, seasonal_2 Loss: 0.1138 | 0.0537
Epoch 21/300, seasonal_2 Loss: 0.1495 | 0.1206
Epoch 22/300, seasonal_2 Loss: 0.1386 | 0.2061
Epoch 23/300, seasonal_2 Loss: 0.1365 | 0.0618
Epoch 24/300, seasonal_2 Loss: 0.1501 | 0.1180
Epoch 25/300, seasonal_2 Loss: 0.1385 | 0.1431
Epoch 26/300, seasonal_2 Loss: 0.1454 | 0.1418
Epoch 27/300, seasonal_2 Loss: 0.1843 | 0.1092
Epoch 28/300, seasonal_2 Loss: 0.1740 | 0.0677
Epoch 29/300, seasonal_2 Loss: 0.1636 | 0.0521
Epoch 30/300, seasonal_2 Loss: 0.1414 | 0.0760
Epoch 31/300, seasonal_2 Loss: 0.1038 | 0.0592
Epoch 32/300, seasonal_2 Loss: 0.1072 | 0.0763
Epoch 33/300, seasonal_2 Loss: 0.0993 | 0.0841
Epoch 34/300, seasonal_2 Loss: 0.1122 | 0.0747
Epoch 35/300, seasonal_2 Loss: 0.1119 | 0.0543
Epoch 36/300, seasonal_2 Loss: 0.1299 | 0.0602
Epoch 37/300, seasonal_2 Loss: 0.1105 | 0.0520
Epoch 38/300, seasonal_2 Loss: 0.1019 | 0.0524
Epoch 39/300, seasonal_2 Loss: 0.0970 | 0.0652
Epoch 40/300, seasonal_2 Loss: 0.1027 | 0.0816
Epoch 41/300, seasonal_2 Loss: 0.1054 | 0.0995
Epoch 42/300, seasonal_2 Loss: 0.1117 | 0.0742
Epoch 43/300, seasonal_2 Loss: 0.1054 | 0.0929
Epoch 44/300, seasonal_2 Loss: 0.1529 | 0.0915
Epoch 45/300, seasonal_2 Loss: 0.1372 | 0.0745
Epoch 46/300, seasonal_2 Loss: 0.1139 | 0.0596
Epoch 47/300, seasonal_2 Loss: 0.0968 | 0.0582
Epoch 48/300, seasonal_2 Loss: 0.0856 | 0.0568
Epoch 49/300, seasonal_2 Loss: 0.0840 | 0.0572
Epoch 50/300, seasonal_2 Loss: 0.0858 | 0.0675
Epoch 51/300, seasonal_2 Loss: 0.0940 | 0.0750
Epoch 52/300, seasonal_2 Loss: 0.0980 | 0.0946
Epoch 53/300, seasonal_2 Loss: 0.1121 | 0.0779
Epoch 54/300, seasonal_2 Loss: 0.0964 | 0.0540
Epoch 55/300, seasonal_2 Loss: 0.0868 | 0.0547
Epoch 56/300, seasonal_2 Loss: 0.0865 | 0.0866
Epoch 57/300, seasonal_2 Loss: 0.0800 | 0.0817
Epoch 58/300, seasonal_2 Loss: 0.0778 | 0.0582
Epoch 59/300, seasonal_2 Loss: 0.0947 | 0.0580
Epoch 60/300, seasonal_2 Loss: 0.0882 | 0.0545
Epoch 61/300, seasonal_2 Loss: 0.0849 | 0.0641
Epoch 62/300, seasonal_2 Loss: 0.0840 | 0.0577
Epoch 63/300, seasonal_2 Loss: 0.0892 | 0.0543
Epoch 64/300, seasonal_2 Loss: 0.0776 | 0.0632
Epoch 65/300, seasonal_2 Loss: 0.0755 | 0.0737
Epoch 66/300, seasonal_2 Loss: 0.0840 | 0.0453
Epoch 67/300, seasonal_2 Loss: 0.0890 | 0.0474
Epoch 68/300, seasonal_2 Loss: 0.0891 | 0.0496
Epoch 69/300, seasonal_2 Loss: 0.0735 | 0.0546
Epoch 70/300, seasonal_2 Loss: 0.1009 | 0.0562
Epoch 71/300, seasonal_2 Loss: 0.1033 | 0.0587
Epoch 72/300, seasonal_2 Loss: 0.0825 | 0.0962
Epoch 73/300, seasonal_2 Loss: 0.0823 | 0.0553
Epoch 74/300, seasonal_2 Loss: 0.0877 | 0.0415
Epoch 75/300, seasonal_2 Loss: 0.0769 | 0.0506
Epoch 76/300, seasonal_2 Loss: 0.0674 | 0.0590
Epoch 77/300, seasonal_2 Loss: 0.0739 | 0.0534
Epoch 78/300, seasonal_2 Loss: 0.0702 | 0.0559
Epoch 79/300, seasonal_2 Loss: 0.0644 | 0.0471
Epoch 80/300, seasonal_2 Loss: 0.0677 | 0.0420
Epoch 81/300, seasonal_2 Loss: 0.0642 | 0.0451
Epoch 82/300, seasonal_2 Loss: 0.0613 | 0.0461
Epoch 83/300, seasonal_2 Loss: 0.0613 | 0.0527
Epoch 84/300, seasonal_2 Loss: 0.0593 | 0.0466
Epoch 85/300, seasonal_2 Loss: 0.0584 | 0.0428
Epoch 86/300, seasonal_2 Loss: 0.0582 | 0.0436
Epoch 87/300, seasonal_2 Loss: 0.0574 | 0.0458
Epoch 88/300, seasonal_2 Loss: 0.0573 | 0.0467
Epoch 89/300, seasonal_2 Loss: 0.0567 | 0.0454
Epoch 90/300, seasonal_2 Loss: 0.0565 | 0.0439
Epoch 91/300, seasonal_2 Loss: 0.0563 | 0.0433
Epoch 92/300, seasonal_2 Loss: 0.0561 | 0.0447
Epoch 93/300, seasonal_2 Loss: 0.0560 | 0.0446
Epoch 94/300, seasonal_2 Loss: 0.0558 | 0.0441
Epoch 95/300, seasonal_2 Loss: 0.0558 | 0.0437
Epoch 96/300, seasonal_2 Loss: 0.0559 | 0.0447
Epoch 97/300, seasonal_2 Loss: 0.0567 | 0.0452
Epoch 98/300, seasonal_2 Loss: 0.0578 | 0.0417
Epoch 99/300, seasonal_2 Loss: 0.0578 | 0.0443
Epoch 100/300, seasonal_2 Loss: 0.0610 | 0.0452
Epoch 101/300, seasonal_2 Loss: 0.0581 | 0.0431
Epoch 102/300, seasonal_2 Loss: 0.0562 | 0.0414
Epoch 103/300, seasonal_2 Loss: 0.0552 | 0.0420
Epoch 104/300, seasonal_2 Loss: 0.0549 | 0.0410
Epoch 105/300, seasonal_2 Loss: 0.0558 | 0.0437
Epoch 106/300, seasonal_2 Loss: 0.0563 | 0.0433
Epoch 107/300, seasonal_2 Loss: 0.0550 | 0.0421
Epoch 108/300, seasonal_2 Loss: 0.0544 | 0.0414
Epoch 109/300, seasonal_2 Loss: 0.0545 | 0.0412
Epoch 110/300, seasonal_2 Loss: 0.0546 | 0.0418
Epoch 111/300, seasonal_2 Loss: 0.0541 | 0.0413
Epoch 112/300, seasonal_2 Loss: 0.0543 | 0.0438
Epoch 113/300, seasonal_2 Loss: 0.0553 | 0.0427
Epoch 114/300, seasonal_2 Loss: 0.0540 | 0.0422
Epoch 115/300, seasonal_2 Loss: 0.0535 | 0.0413
Epoch 116/300, seasonal_2 Loss: 0.0538 | 0.0415
Epoch 117/300, seasonal_2 Loss: 0.0537 | 0.0412
Epoch 118/300, seasonal_2 Loss: 0.0532 | 0.0426
Epoch 119/300, seasonal_2 Loss: 0.0537 | 0.0427
Epoch 120/300, seasonal_2 Loss: 0.0534 | 0.0423
Epoch 121/300, seasonal_2 Loss: 0.0529 | 0.0417
Epoch 122/300, seasonal_2 Loss: 0.0533 | 0.0417
Epoch 123/300, seasonal_2 Loss: 0.0535 | 0.0410
Epoch 124/300, seasonal_2 Loss: 0.0531 | 0.0421
Epoch 125/300, seasonal_2 Loss: 0.0532 | 0.0427
Epoch 126/300, seasonal_2 Loss: 0.0528 | 0.0421
Epoch 127/300, seasonal_2 Loss: 0.0524 | 0.0414
Epoch 128/300, seasonal_2 Loss: 0.0526 | 0.0410
Epoch 129/300, seasonal_2 Loss: 0.0525 | 0.0416
Epoch 130/300, seasonal_2 Loss: 0.0521 | 0.0429
Epoch 131/300, seasonal_2 Loss: 0.0521 | 0.0417
Epoch 132/300, seasonal_2 Loss: 0.0516 | 0.0415
Epoch 133/300, seasonal_2 Loss: 0.0518 | 0.0414
Epoch 134/300, seasonal_2 Loss: 0.0518 | 0.0419
Epoch 135/300, seasonal_2 Loss: 0.0514 | 0.0425
Epoch 136/300, seasonal_2 Loss: 0.0515 | 0.0421
Epoch 137/300, seasonal_2 Loss: 0.0511 | 0.0418
Epoch 138/300, seasonal_2 Loss: 0.0512 | 0.0417
Epoch 139/300, seasonal_2 Loss: 0.0513 | 0.0420
Epoch 140/300, seasonal_2 Loss: 0.0509 | 0.0428
Epoch 141/300, seasonal_2 Loss: 0.0510 | 0.0423
Epoch 142/300, seasonal_2 Loss: 0.0507 | 0.0420
Epoch 143/300, seasonal_2 Loss: 0.0508 | 0.0420
Epoch 144/300, seasonal_2 Loss: 0.0508 | 0.0421
Epoch 145/300, seasonal_2 Loss: 0.0504 | 0.0429
Epoch 146/300, seasonal_2 Loss: 0.0506 | 0.0427
Epoch 147/300, seasonal_2 Loss: 0.0503 | 0.0422
Epoch 148/300, seasonal_2 Loss: 0.0503 | 0.0422
Epoch 149/300, seasonal_2 Loss: 0.0504 | 0.0424
Epoch 150/300, seasonal_2 Loss: 0.0500 | 0.0432
Epoch 151/300, seasonal_2 Loss: 0.0501 | 0.0429
Epoch 152/300, seasonal_2 Loss: 0.0499 | 0.0425
Epoch 153/300, seasonal_2 Loss: 0.0498 | 0.0426
Epoch 154/300, seasonal_2 Loss: 0.0499 | 0.0430
Epoch 155/300, seasonal_2 Loss: 0.0496 | 0.0437
Epoch 156/300, seasonal_2 Loss: 0.0497 | 0.0433
Epoch 157/300, seasonal_2 Loss: 0.0494 | 0.0430
Epoch 158/300, seasonal_2 Loss: 0.0494 | 0.0433
Epoch 159/300, seasonal_2 Loss: 0.0494 | 0.0440
Epoch 160/300, seasonal_2 Loss: 0.0491 | 0.0444
Epoch 161/300, seasonal_2 Loss: 0.0491 | 0.0441
Epoch 162/300, seasonal_2 Loss: 0.0489 | 0.0443
Epoch 163/300, seasonal_2 Loss: 0.0489 | 0.0450
Epoch 164/300, seasonal_2 Loss: 0.0487 | 0.0458
Epoch 165/300, seasonal_2 Loss: 0.0485 | 0.0462
Epoch 166/300, seasonal_2 Loss: 0.0484 | 0.0465
Epoch 167/300, seasonal_2 Loss: 0.0482 | 0.0474
Epoch 168/300, seasonal_2 Loss: 0.0480 | 0.0486
Epoch 169/300, seasonal_2 Loss: 0.0477 | 0.0495
Epoch 170/300, seasonal_2 Loss: 0.0475 | 0.0502
Epoch 171/300, seasonal_2 Loss: 0.0471 | 0.0510
Epoch 172/300, seasonal_2 Loss: 0.0468 | 0.0519
Epoch 173/300, seasonal_2 Loss: 0.0464 | 0.0527
Epoch 174/300, seasonal_2 Loss: 0.0459 | 0.0535
Epoch 175/300, seasonal_2 Loss: 0.0454 | 0.0537
Epoch 176/300, seasonal_2 Loss: 0.0449 | 0.0539
Epoch 177/300, seasonal_2 Loss: 0.0445 | 0.0535
Epoch 178/300, seasonal_2 Loss: 0.0440 | 0.0539
Epoch 179/300, seasonal_2 Loss: 0.0435 | 0.0527
Epoch 180/300, seasonal_2 Loss: 0.0431 | 0.0535
Epoch 181/300, seasonal_2 Loss: 0.0429 | 0.0495
Epoch 182/300, seasonal_2 Loss: 0.0431 | 0.0519
Epoch 183/300, seasonal_2 Loss: 0.0439 | 0.0445
Epoch 184/300, seasonal_2 Loss: 0.0488 | 0.0388
Epoch 185/300, seasonal_2 Loss: 0.0440 | 0.0454
Epoch 186/300, seasonal_2 Loss: 0.0430 | 0.0522
Epoch 187/300, seasonal_2 Loss: 0.0428 | 0.0446
Epoch 188/300, seasonal_2 Loss: 0.0417 | 0.0526
Epoch 189/300, seasonal_2 Loss: 0.0413 | 0.0469
Epoch 190/300, seasonal_2 Loss: 0.0411 | 0.0494
Epoch 191/300, seasonal_2 Loss: 0.0409 | 0.0485
Epoch 192/300, seasonal_2 Loss: 0.0408 | 0.0485
Epoch 193/300, seasonal_2 Loss: 0.0407 | 0.0485
Epoch 194/300, seasonal_2 Loss: 0.0405 | 0.0482
Epoch 195/300, seasonal_2 Loss: 0.0404 | 0.0484
Epoch 196/300, seasonal_2 Loss: 0.0403 | 0.0481
Epoch 197/300, seasonal_2 Loss: 0.0403 | 0.0482
Epoch 198/300, seasonal_2 Loss: 0.0402 | 0.0479
Epoch 199/300, seasonal_2 Loss: 0.0401 | 0.0480
Epoch 200/300, seasonal_2 Loss: 0.0400 | 0.0478
Epoch 201/300, seasonal_2 Loss: 0.0399 | 0.0479
Epoch 202/300, seasonal_2 Loss: 0.0399 | 0.0477
Epoch 203/300, seasonal_2 Loss: 0.0398 | 0.0477
Epoch 204/300, seasonal_2 Loss: 0.0398 | 0.0476
Epoch 205/300, seasonal_2 Loss: 0.0397 | 0.0476
Epoch 206/300, seasonal_2 Loss: 0.0396 | 0.0475
Epoch 207/300, seasonal_2 Loss: 0.0396 | 0.0475
Epoch 208/300, seasonal_2 Loss: 0.0395 | 0.0474
Epoch 209/300, seasonal_2 Loss: 0.0395 | 0.0474
Epoch 210/300, seasonal_2 Loss: 0.0394 | 0.0473
Epoch 211/300, seasonal_2 Loss: 0.0394 | 0.0473
Epoch 212/300, seasonal_2 Loss: 0.0394 | 0.0472
Epoch 213/300, seasonal_2 Loss: 0.0393 | 0.0472
Epoch 214/300, seasonal_2 Loss: 0.0393 | 0.0471
Epoch 215/300, seasonal_2 Loss: 0.0392 | 0.0471
Epoch 216/300, seasonal_2 Loss: 0.0392 | 0.0470
Epoch 217/300, seasonal_2 Loss: 0.0392 | 0.0470
Epoch 218/300, seasonal_2 Loss: 0.0391 | 0.0470
Epoch 219/300, seasonal_2 Loss: 0.0391 | 0.0469
Epoch 220/300, seasonal_2 Loss: 0.0391 | 0.0469
Epoch 221/300, seasonal_2 Loss: 0.0390 | 0.0469
Epoch 222/300, seasonal_2 Loss: 0.0390 | 0.0468
Epoch 223/300, seasonal_2 Loss: 0.0390 | 0.0468
Epoch 224/300, seasonal_2 Loss: 0.0389 | 0.0468
Epoch 225/300, seasonal_2 Loss: 0.0389 | 0.0467
Epoch 226/300, seasonal_2 Loss: 0.0389 | 0.0467
Epoch 227/300, seasonal_2 Loss: 0.0388 | 0.0467
Epoch 228/300, seasonal_2 Loss: 0.0388 | 0.0466
Epoch 229/300, seasonal_2 Loss: 0.0388 | 0.0466
Epoch 230/300, seasonal_2 Loss: 0.0388 | 0.0466
Epoch 231/300, seasonal_2 Loss: 0.0387 | 0.0466
Epoch 232/300, seasonal_2 Loss: 0.0387 | 0.0465
Epoch 233/300, seasonal_2 Loss: 0.0387 | 0.0465
Epoch 234/300, seasonal_2 Loss: 0.0387 | 0.0465
Epoch 235/300, seasonal_2 Loss: 0.0387 | 0.0465
Epoch 236/300, seasonal_2 Loss: 0.0386 | 0.0464
Epoch 237/300, seasonal_2 Loss: 0.0386 | 0.0464
Epoch 238/300, seasonal_2 Loss: 0.0386 | 0.0464
Epoch 239/300, seasonal_2 Loss: 0.0386 | 0.0464
Epoch 240/300, seasonal_2 Loss: 0.0386 | 0.0463
Epoch 241/300, seasonal_2 Loss: 0.0385 | 0.0463
Epoch 242/300, seasonal_2 Loss: 0.0385 | 0.0463
Epoch 243/300, seasonal_2 Loss: 0.0385 | 0.0463
Epoch 244/300, seasonal_2 Loss: 0.0385 | 0.0462
Epoch 245/300, seasonal_2 Loss: 0.0385 | 0.0462
Epoch 246/300, seasonal_2 Loss: 0.0384 | 0.0462
Epoch 247/300, seasonal_2 Loss: 0.0384 | 0.0462
Epoch 248/300, seasonal_2 Loss: 0.0384 | 0.0462
Epoch 249/300, seasonal_2 Loss: 0.0384 | 0.0461
Epoch 250/300, seasonal_2 Loss: 0.0384 | 0.0461
Epoch 251/300, seasonal_2 Loss: 0.0384 | 0.0461
Epoch 252/300, seasonal_2 Loss: 0.0384 | 0.0461
Epoch 253/300, seasonal_2 Loss: 0.0383 | 0.0461
Epoch 254/300, seasonal_2 Loss: 0.0383 | 0.0461
Epoch 255/300, seasonal_2 Loss: 0.0383 | 0.0460
Epoch 256/300, seasonal_2 Loss: 0.0383 | 0.0460
Epoch 257/300, seasonal_2 Loss: 0.0383 | 0.0460
Epoch 258/300, seasonal_2 Loss: 0.0383 | 0.0460
Epoch 259/300, seasonal_2 Loss: 0.0383 | 0.0460
Epoch 260/300, seasonal_2 Loss: 0.0382 | 0.0460
Epoch 261/300, seasonal_2 Loss: 0.0382 | 0.0460
Epoch 262/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 263/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 264/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 265/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 266/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 267/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 268/300, seasonal_2 Loss: 0.0382 | 0.0459
Epoch 269/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 270/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 271/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 272/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 273/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 274/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 275/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 276/300, seasonal_2 Loss: 0.0381 | 0.0458
Epoch 277/300, seasonal_2 Loss: 0.0381 | 0.0457
Epoch 278/300, seasonal_2 Loss: 0.0381 | 0.0457
Epoch 279/300, seasonal_2 Loss: 0.0381 | 0.0457
Epoch 280/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 281/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 282/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 283/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 284/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 285/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 286/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 287/300, seasonal_2 Loss: 0.0380 | 0.0457
Epoch 288/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 289/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 290/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 291/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 292/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 293/300, seasonal_2 Loss: 0.0380 | 0.0456
Epoch 294/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 295/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 296/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 297/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 298/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 299/300, seasonal_2 Loss: 0.0379 | 0.0456
Epoch 300/300, seasonal_2 Loss: 0.0379 | 0.0455
Training seasonal_3 component with params: {'observation_period_num': 10, 'train_rates': 0.9311902731119093, 'learning_rate': 0.0002882251271567547, 'batch_size': 64, 'step_size': 12, 'gamma': 0.8207335566474638}
Epoch 1/300, seasonal_3 Loss: 0.6918 | 0.1199
Epoch 2/300, seasonal_3 Loss: 0.1708 | 0.0855
Epoch 3/300, seasonal_3 Loss: 0.1199 | 0.0582
Epoch 4/300, seasonal_3 Loss: 0.1629 | 0.0746
Epoch 5/300, seasonal_3 Loss: 0.1757 | 0.0763
Epoch 6/300, seasonal_3 Loss: 0.1400 | 0.0573
Epoch 7/300, seasonal_3 Loss: 0.1073 | 0.0486
Epoch 8/300, seasonal_3 Loss: 0.0992 | 0.0481
Epoch 9/300, seasonal_3 Loss: 0.0986 | 0.0472
Epoch 10/300, seasonal_3 Loss: 0.1060 | 0.0678
Epoch 11/300, seasonal_3 Loss: 0.1024 | 0.0560
Epoch 12/300, seasonal_3 Loss: 0.0914 | 0.0486
Epoch 13/300, seasonal_3 Loss: 0.0944 | 0.0503
Epoch 14/300, seasonal_3 Loss: 0.1021 | 0.0623
Epoch 15/300, seasonal_3 Loss: 0.1015 | 0.0601
Epoch 16/300, seasonal_3 Loss: 0.0884 | 0.0697
Epoch 17/300, seasonal_3 Loss: 0.0957 | 0.0820
Epoch 18/300, seasonal_3 Loss: 0.0975 | 0.0548
Epoch 19/300, seasonal_3 Loss: 0.1213 | 0.0661
Epoch 20/300, seasonal_3 Loss: 0.1174 | 0.0740
Epoch 21/300, seasonal_3 Loss: 0.0926 | 0.0482
Epoch 22/300, seasonal_3 Loss: 0.0863 | 0.0503
Epoch 23/300, seasonal_3 Loss: 0.0840 | 0.0515
Epoch 24/300, seasonal_3 Loss: 0.0783 | 0.0438
Epoch 25/300, seasonal_3 Loss: 0.0705 | 0.0371
Epoch 26/300, seasonal_3 Loss: 0.0768 | 0.0465
Epoch 27/300, seasonal_3 Loss: 0.0802 | 0.0544
Epoch 28/300, seasonal_3 Loss: 0.0683 | 0.0415
Epoch 29/300, seasonal_3 Loss: 0.0662 | 0.0381
Epoch 30/300, seasonal_3 Loss: 0.0706 | 0.0351
Epoch 31/300, seasonal_3 Loss: 0.0676 | 0.0433
Epoch 32/300, seasonal_3 Loss: 0.0669 | 0.0314
Epoch 33/300, seasonal_3 Loss: 0.0731 | 0.0412
Epoch 34/300, seasonal_3 Loss: 0.0694 | 0.0327
Epoch 35/300, seasonal_3 Loss: 0.0779 | 0.0348
Epoch 36/300, seasonal_3 Loss: 0.0788 | 0.0348
Epoch 37/300, seasonal_3 Loss: 0.0888 | 0.0473
Epoch 38/300, seasonal_3 Loss: 0.0953 | 0.0692
Epoch 39/300, seasonal_3 Loss: 0.0782 | 0.0388
Epoch 40/300, seasonal_3 Loss: 0.0832 | 0.0514
Epoch 41/300, seasonal_3 Loss: 0.0747 | 0.0406
Epoch 42/300, seasonal_3 Loss: 0.0753 | 0.0511
Epoch 43/300, seasonal_3 Loss: 0.0750 | 0.0500
Epoch 44/300, seasonal_3 Loss: 0.0710 | 0.0407
Epoch 45/300, seasonal_3 Loss: 0.0732 | 0.0365
Epoch 46/300, seasonal_3 Loss: 0.0684 | 0.0322
Epoch 47/300, seasonal_3 Loss: 0.0669 | 0.0378
Epoch 48/300, seasonal_3 Loss: 0.0641 | 0.0412
Epoch 49/300, seasonal_3 Loss: 0.0636 | 0.0456
Epoch 50/300, seasonal_3 Loss: 0.0639 | 0.0506
Epoch 51/300, seasonal_3 Loss: 0.0623 | 0.0372
Epoch 52/300, seasonal_3 Loss: 0.0637 | 0.0391
Epoch 53/300, seasonal_3 Loss: 0.0620 | 0.0332
Epoch 54/300, seasonal_3 Loss: 0.0625 | 0.0379
Epoch 55/300, seasonal_3 Loss: 0.0618 | 0.0365
Epoch 56/300, seasonal_3 Loss: 0.0614 | 0.0341
Epoch 57/300, seasonal_3 Loss: 0.0570 | 0.0306
Epoch 58/300, seasonal_3 Loss: 0.0552 | 0.0332
Epoch 59/300, seasonal_3 Loss: 0.0579 | 0.0336
Epoch 60/300, seasonal_3 Loss: 0.0574 | 0.0400
Epoch 61/300, seasonal_3 Loss: 0.0561 | 0.0475
Epoch 62/300, seasonal_3 Loss: 0.0544 | 0.0335
Epoch 63/300, seasonal_3 Loss: 0.0513 | 0.0409
Epoch 64/300, seasonal_3 Loss: 0.0483 | 0.0311
Epoch 65/300, seasonal_3 Loss: 0.0474 | 0.0436
Epoch 66/300, seasonal_3 Loss: 0.0469 | 0.0308
Epoch 67/300, seasonal_3 Loss: 0.0477 | 0.0335
Epoch 68/300, seasonal_3 Loss: 0.0494 | 0.0330
Epoch 69/300, seasonal_3 Loss: 0.0516 | 0.0393
Epoch 70/300, seasonal_3 Loss: 0.0563 | 0.0268
Epoch 71/300, seasonal_3 Loss: 0.0550 | 0.0336
Epoch 72/300, seasonal_3 Loss: 0.0586 | 0.0310
Epoch 73/300, seasonal_3 Loss: 0.0578 | 0.0476
Epoch 74/300, seasonal_3 Loss: 0.0536 | 0.0584
Epoch 75/300, seasonal_3 Loss: 0.0501 | 0.0539
Epoch 76/300, seasonal_3 Loss: 0.0483 | 0.0548
Epoch 77/300, seasonal_3 Loss: 0.0451 | 0.0559
Epoch 78/300, seasonal_3 Loss: 0.0435 | 0.0522
Epoch 79/300, seasonal_3 Loss: 0.0426 | 0.0392
Epoch 80/300, seasonal_3 Loss: 0.0415 | 0.0411
Epoch 81/300, seasonal_3 Loss: 0.0405 | 0.0381
Epoch 82/300, seasonal_3 Loss: 0.0397 | 0.0382
Epoch 83/300, seasonal_3 Loss: 0.0393 | 0.0371
Epoch 84/300, seasonal_3 Loss: 0.0390 | 0.0358
Epoch 85/300, seasonal_3 Loss: 0.0393 | 0.0373
Epoch 86/300, seasonal_3 Loss: 0.0396 | 0.0383
Epoch 87/300, seasonal_3 Loss: 0.0399 | 0.0395
Epoch 88/300, seasonal_3 Loss: 0.0398 | 0.0409
Epoch 89/300, seasonal_3 Loss: 0.0394 | 0.0415
Epoch 90/300, seasonal_3 Loss: 0.0390 | 0.0416
Epoch 91/300, seasonal_3 Loss: 0.0391 | 0.0385
Epoch 92/300, seasonal_3 Loss: 0.0396 | 0.0376
Epoch 93/300, seasonal_3 Loss: 0.0403 | 0.0367
Epoch 94/300, seasonal_3 Loss: 0.0402 | 0.0366
Epoch 95/300, seasonal_3 Loss: 0.0398 | 0.0368
Epoch 96/300, seasonal_3 Loss: 0.0393 | 0.0372
Epoch 97/300, seasonal_3 Loss: 0.0393 | 0.0374
Epoch 98/300, seasonal_3 Loss: 0.0394 | 0.0368
Epoch 99/300, seasonal_3 Loss: 0.0393 | 0.0362
Epoch 100/300, seasonal_3 Loss: 0.0392 | 0.0352
Epoch 101/300, seasonal_3 Loss: 0.0389 | 0.0342
Epoch 102/300, seasonal_3 Loss: 0.0388 | 0.0335
Epoch 103/300, seasonal_3 Loss: 0.0391 | 0.0341
Epoch 104/300, seasonal_3 Loss: 0.0393 | 0.0358
Epoch 105/300, seasonal_3 Loss: 0.0391 | 0.0373
Epoch 106/300, seasonal_3 Loss: 0.0389 | 0.0386
Epoch 107/300, seasonal_3 Loss: 0.0389 | 0.0389
Epoch 108/300, seasonal_3 Loss: 0.0382 | 0.0388
Epoch 109/300, seasonal_3 Loss: 0.0376 | 0.0360
Epoch 110/300, seasonal_3 Loss: 0.0380 | 0.0339
Epoch 111/300, seasonal_3 Loss: 0.0387 | 0.0327
Epoch 112/300, seasonal_3 Loss: 0.0383 | 0.0322
Epoch 113/300, seasonal_3 Loss: 0.0371 | 0.0330
Epoch 114/300, seasonal_3 Loss: 0.0366 | 0.0344
Epoch 115/300, seasonal_3 Loss: 0.0372 | 0.0374
Epoch 116/300, seasonal_3 Loss: 0.0375 | 0.0389
Epoch 117/300, seasonal_3 Loss: 0.0368 | 0.0386
Epoch 118/300, seasonal_3 Loss: 0.0361 | 0.0370
Epoch 119/300, seasonal_3 Loss: 0.0358 | 0.0351
Epoch 120/300, seasonal_3 Loss: 0.0356 | 0.0341
Epoch 121/300, seasonal_3 Loss: 0.0353 | 0.0333
Epoch 122/300, seasonal_3 Loss: 0.0351 | 0.0339
Epoch 123/300, seasonal_3 Loss: 0.0348 | 0.0347
Epoch 124/300, seasonal_3 Loss: 0.0346 | 0.0352
Epoch 125/300, seasonal_3 Loss: 0.0345 | 0.0355
Epoch 126/300, seasonal_3 Loss: 0.0344 | 0.0355
Epoch 127/300, seasonal_3 Loss: 0.0343 | 0.0350
Epoch 128/300, seasonal_3 Loss: 0.0342 | 0.0346
Epoch 129/300, seasonal_3 Loss: 0.0341 | 0.0343
Epoch 130/300, seasonal_3 Loss: 0.0341 | 0.0343
Epoch 131/300, seasonal_3 Loss: 0.0340 | 0.0344
Epoch 132/300, seasonal_3 Loss: 0.0340 | 0.0345
Epoch 133/300, seasonal_3 Loss: 0.0339 | 0.0345
Epoch 134/300, seasonal_3 Loss: 0.0339 | 0.0344
Epoch 135/300, seasonal_3 Loss: 0.0338 | 0.0343
Epoch 136/300, seasonal_3 Loss: 0.0337 | 0.0342
Epoch 137/300, seasonal_3 Loss: 0.0337 | 0.0342
Epoch 138/300, seasonal_3 Loss: 0.0336 | 0.0343
Epoch 139/300, seasonal_3 Loss: 0.0336 | 0.0344
Epoch 140/300, seasonal_3 Loss: 0.0335 | 0.0344
Epoch 141/300, seasonal_3 Loss: 0.0335 | 0.0343
Epoch 142/300, seasonal_3 Loss: 0.0334 | 0.0342
Epoch 143/300, seasonal_3 Loss: 0.0334 | 0.0341
Epoch 144/300, seasonal_3 Loss: 0.0333 | 0.0341
Epoch 145/300, seasonal_3 Loss: 0.0333 | 0.0340
Epoch 146/300, seasonal_3 Loss: 0.0332 | 0.0340
Epoch 147/300, seasonal_3 Loss: 0.0332 | 0.0339
Epoch 148/300, seasonal_3 Loss: 0.0331 | 0.0339
Epoch 149/300, seasonal_3 Loss: 0.0331 | 0.0339
Epoch 150/300, seasonal_3 Loss: 0.0330 | 0.0340
Epoch 151/300, seasonal_3 Loss: 0.0330 | 0.0340
Epoch 152/300, seasonal_3 Loss: 0.0329 | 0.0340
Epoch 153/300, seasonal_3 Loss: 0.0329 | 0.0339
Epoch 154/300, seasonal_3 Loss: 0.0329 | 0.0339
Epoch 155/300, seasonal_3 Loss: 0.0328 | 0.0339
Epoch 156/300, seasonal_3 Loss: 0.0328 | 0.0339
Epoch 157/300, seasonal_3 Loss: 0.0327 | 0.0338
Epoch 158/300, seasonal_3 Loss: 0.0327 | 0.0338
Epoch 159/300, seasonal_3 Loss: 0.0327 | 0.0338
Epoch 160/300, seasonal_3 Loss: 0.0326 | 0.0338
Epoch 161/300, seasonal_3 Loss: 0.0326 | 0.0338
Epoch 162/300, seasonal_3 Loss: 0.0325 | 0.0339
Epoch 163/300, seasonal_3 Loss: 0.0325 | 0.0338
Epoch 164/300, seasonal_3 Loss: 0.0325 | 0.0338
Epoch 165/300, seasonal_3 Loss: 0.0325 | 0.0338
Epoch 166/300, seasonal_3 Loss: 0.0324 | 0.0338
Epoch 167/300, seasonal_3 Loss: 0.0324 | 0.0338
Epoch 168/300, seasonal_3 Loss: 0.0324 | 0.0338
Epoch 169/300, seasonal_3 Loss: 0.0323 | 0.0338
Epoch 170/300, seasonal_3 Loss: 0.0323 | 0.0338
Epoch 171/300, seasonal_3 Loss: 0.0323 | 0.0338
Epoch 172/300, seasonal_3 Loss: 0.0322 | 0.0338
Epoch 173/300, seasonal_3 Loss: 0.0322 | 0.0338
Epoch 174/300, seasonal_3 Loss: 0.0322 | 0.0338
Epoch 175/300, seasonal_3 Loss: 0.0322 | 0.0338
Epoch 176/300, seasonal_3 Loss: 0.0321 | 0.0338
Epoch 177/300, seasonal_3 Loss: 0.0321 | 0.0338
Epoch 178/300, seasonal_3 Loss: 0.0321 | 0.0338
Epoch 179/300, seasonal_3 Loss: 0.0321 | 0.0338
Epoch 180/300, seasonal_3 Loss: 0.0320 | 0.0338
Epoch 181/300, seasonal_3 Loss: 0.0320 | 0.0338
Epoch 182/300, seasonal_3 Loss: 0.0320 | 0.0338
Epoch 183/300, seasonal_3 Loss: 0.0320 | 0.0338
Epoch 184/300, seasonal_3 Loss: 0.0320 | 0.0338
Epoch 185/300, seasonal_3 Loss: 0.0319 | 0.0338
Epoch 186/300, seasonal_3 Loss: 0.0319 | 0.0338
Epoch 187/300, seasonal_3 Loss: 0.0319 | 0.0338
Epoch 188/300, seasonal_3 Loss: 0.0319 | 0.0338
Epoch 189/300, seasonal_3 Loss: 0.0319 | 0.0338
Epoch 190/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 191/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 192/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 193/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 194/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 195/300, seasonal_3 Loss: 0.0318 | 0.0338
Epoch 196/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 197/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 198/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 199/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 200/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 201/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 202/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 203/300, seasonal_3 Loss: 0.0317 | 0.0338
Epoch 204/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 205/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 206/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 207/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 208/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 209/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 210/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 211/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 212/300, seasonal_3 Loss: 0.0316 | 0.0338
Epoch 213/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 214/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 215/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 216/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 217/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 218/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 219/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 220/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 221/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 222/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 223/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 224/300, seasonal_3 Loss: 0.0315 | 0.0338
Epoch 225/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 226/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 227/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 228/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 229/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 230/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 231/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 232/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 233/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 234/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 235/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 236/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 237/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 238/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 239/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 240/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 241/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 242/300, seasonal_3 Loss: 0.0314 | 0.0338
Epoch 243/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 244/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 245/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 246/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 247/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 248/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 249/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 250/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 251/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 252/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 253/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 254/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 255/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 256/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 257/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 258/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 259/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 260/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 261/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 262/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 263/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 264/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 265/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 266/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 267/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 268/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 269/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 270/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 271/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 272/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 273/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 274/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 275/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 276/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 277/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 278/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 279/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 280/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 281/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 282/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 283/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 284/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 285/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 286/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 287/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 288/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 289/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 290/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 291/300, seasonal_3 Loss: 0.0313 | 0.0338
Epoch 292/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 293/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 294/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 295/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 296/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 297/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 298/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 299/300, seasonal_3 Loss: 0.0312 | 0.0338
Epoch 300/300, seasonal_3 Loss: 0.0312 | 0.0338
Training resid component with params: {'observation_period_num': 30, 'train_rates': 0.9898221984737691, 'learning_rate': 0.00034058433302648037, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8271664401688998}
Epoch 1/300, resid Loss: 1.2832 | 0.2507
Epoch 2/300, resid Loss: 0.3474 | 0.2283
Epoch 3/300, resid Loss: 0.2271 | 0.1720
Epoch 4/300, resid Loss: 0.2174 | 0.1253
Epoch 5/300, resid Loss: 0.2035 | 0.1162
Epoch 6/300, resid Loss: 0.2557 | 0.1301
Epoch 7/300, resid Loss: 0.2636 | 0.1495
Epoch 8/300, resid Loss: 0.2236 | 0.1443
Epoch 9/300, resid Loss: 0.2929 | 0.2063
Epoch 10/300, resid Loss: 0.3572 | 0.4360
Epoch 11/300, resid Loss: 0.3607 | 0.2404
Epoch 12/300, resid Loss: 0.3756 | 0.2086
Epoch 13/300, resid Loss: 0.3929 | 0.2313
Epoch 14/300, resid Loss: 0.3361 | 0.1206
Epoch 15/300, resid Loss: 0.3823 | 0.1434
Epoch 16/300, resid Loss: 0.1898 | 0.0999
Epoch 17/300, resid Loss: 0.1923 | 0.1465
Epoch 18/300, resid Loss: 0.2130 | 0.0970
Epoch 19/300, resid Loss: 0.1920 | 0.0728
Epoch 20/300, resid Loss: 0.1736 | 0.0717
Epoch 21/300, resid Loss: 0.1475 | 0.0937
Epoch 22/300, resid Loss: 0.1579 | 0.1462
Epoch 23/300, resid Loss: 0.1533 | 0.2822
Epoch 24/300, resid Loss: 0.1240 | 0.1051
Epoch 25/300, resid Loss: 0.1188 | 0.1051
Epoch 26/300, resid Loss: 0.1031 | 0.0666
Epoch 27/300, resid Loss: 0.0937 | 0.0672
Epoch 28/300, resid Loss: 0.0908 | 0.0621
Epoch 29/300, resid Loss: 0.0853 | 0.0707
Epoch 30/300, resid Loss: 0.0822 | 0.0681
Epoch 31/300, resid Loss: 0.0812 | 0.0607
Epoch 32/300, resid Loss: 0.0804 | 0.0586
Epoch 33/300, resid Loss: 0.0771 | 0.0683
Epoch 34/300, resid Loss: 0.0769 | 0.0680
Epoch 35/300, resid Loss: 0.0754 | 0.0595
Epoch 36/300, resid Loss: 0.0763 | 0.0597
Epoch 37/300, resid Loss: 0.0740 | 0.0639
Epoch 38/300, resid Loss: 0.0744 | 0.0621
Epoch 39/300, resid Loss: 0.0737 | 0.0564
Epoch 40/300, resid Loss: 0.0728 | 0.0631
Epoch 41/300, resid Loss: 0.0713 | 0.0611
Epoch 42/300, resid Loss: 0.0702 | 0.0586
Epoch 43/300, resid Loss: 0.0698 | 0.0587
Epoch 44/300, resid Loss: 0.0689 | 0.0614
Epoch 45/300, resid Loss: 0.0683 | 0.0590
Epoch 46/300, resid Loss: 0.0677 | 0.0563
Epoch 47/300, resid Loss: 0.0670 | 0.0591
Epoch 48/300, resid Loss: 0.0671 | 0.0583
Epoch 49/300, resid Loss: 0.0663 | 0.0574
Epoch 50/300, resid Loss: 0.0661 | 0.0568
Epoch 51/300, resid Loss: 0.0655 | 0.0573
Epoch 52/300, resid Loss: 0.0653 | 0.0571
Epoch 53/300, resid Loss: 0.0649 | 0.0554
Epoch 54/300, resid Loss: 0.0646 | 0.0568
Epoch 55/300, resid Loss: 0.0645 | 0.0561
Epoch 56/300, resid Loss: 0.0640 | 0.0553
Epoch 57/300, resid Loss: 0.0638 | 0.0555
Epoch 58/300, resid Loss: 0.0635 | 0.0555
Epoch 59/300, resid Loss: 0.0634 | 0.0547
Epoch 60/300, resid Loss: 0.0631 | 0.0546
Epoch 61/300, resid Loss: 0.0629 | 0.0547
Epoch 62/300, resid Loss: 0.0628 | 0.0543
Epoch 63/300, resid Loss: 0.0626 | 0.0540
Epoch 64/300, resid Loss: 0.0624 | 0.0540
Epoch 65/300, resid Loss: 0.0623 | 0.0537
Epoch 66/300, resid Loss: 0.0622 | 0.0535
Epoch 67/300, resid Loss: 0.0620 | 0.0534
Epoch 68/300, resid Loss: 0.0619 | 0.0532
Epoch 69/300, resid Loss: 0.0618 | 0.0531
Epoch 70/300, resid Loss: 0.0617 | 0.0529
Epoch 71/300, resid Loss: 0.0616 | 0.0528
Epoch 72/300, resid Loss: 0.0615 | 0.0527
Epoch 73/300, resid Loss: 0.0614 | 0.0526
Epoch 74/300, resid Loss: 0.0613 | 0.0525
Epoch 75/300, resid Loss: 0.0612 | 0.0523
Epoch 76/300, resid Loss: 0.0611 | 0.0522
Epoch 77/300, resid Loss: 0.0611 | 0.0521
Epoch 78/300, resid Loss: 0.0610 | 0.0520
Epoch 79/300, resid Loss: 0.0609 | 0.0520
Epoch 80/300, resid Loss: 0.0609 | 0.0519
Epoch 81/300, resid Loss: 0.0608 | 0.0518
Epoch 82/300, resid Loss: 0.0607 | 0.0517
Epoch 83/300, resid Loss: 0.0607 | 0.0517
Epoch 84/300, resid Loss: 0.0606 | 0.0516
Epoch 85/300, resid Loss: 0.0606 | 0.0515
Epoch 86/300, resid Loss: 0.0605 | 0.0515
Epoch 87/300, resid Loss: 0.0605 | 0.0514
Epoch 88/300, resid Loss: 0.0604 | 0.0513
Epoch 89/300, resid Loss: 0.0604 | 0.0513
Epoch 90/300, resid Loss: 0.0604 | 0.0512
Epoch 91/300, resid Loss: 0.0603 | 0.0512
Epoch 92/300, resid Loss: 0.0603 | 0.0511
Epoch 93/300, resid Loss: 0.0602 | 0.0511
Epoch 94/300, resid Loss: 0.0602 | 0.0511
Epoch 95/300, resid Loss: 0.0602 | 0.0510
Epoch 96/300, resid Loss: 0.0601 | 0.0510
Epoch 97/300, resid Loss: 0.0601 | 0.0509
Epoch 98/300, resid Loss: 0.0601 | 0.0509
Epoch 99/300, resid Loss: 0.0601 | 0.0509
Epoch 100/300, resid Loss: 0.0600 | 0.0508
Epoch 101/300, resid Loss: 0.0600 | 0.0508
Epoch 102/300, resid Loss: 0.0600 | 0.0508
Epoch 103/300, resid Loss: 0.0600 | 0.0508
Epoch 104/300, resid Loss: 0.0600 | 0.0507
Epoch 105/300, resid Loss: 0.0599 | 0.0507
Epoch 106/300, resid Loss: 0.0599 | 0.0507
Epoch 107/300, resid Loss: 0.0599 | 0.0507
Epoch 108/300, resid Loss: 0.0599 | 0.0506
Epoch 109/300, resid Loss: 0.0599 | 0.0506
Epoch 110/300, resid Loss: 0.0599 | 0.0506
Epoch 111/300, resid Loss: 0.0598 | 0.0506
Epoch 112/300, resid Loss: 0.0598 | 0.0506
Epoch 113/300, resid Loss: 0.0598 | 0.0505
Epoch 114/300, resid Loss: 0.0598 | 0.0505
Epoch 115/300, resid Loss: 0.0598 | 0.0505
Epoch 116/300, resid Loss: 0.0598 | 0.0505
Epoch 117/300, resid Loss: 0.0598 | 0.0505
Epoch 118/300, resid Loss: 0.0598 | 0.0505
Epoch 119/300, resid Loss: 0.0597 | 0.0504
Epoch 120/300, resid Loss: 0.0597 | 0.0504
Epoch 121/300, resid Loss: 0.0597 | 0.0504
Epoch 122/300, resid Loss: 0.0597 | 0.0504
Epoch 123/300, resid Loss: 0.0597 | 0.0504
Epoch 124/300, resid Loss: 0.0597 | 0.0504
Epoch 125/300, resid Loss: 0.0597 | 0.0504
Epoch 126/300, resid Loss: 0.0597 | 0.0504
Epoch 127/300, resid Loss: 0.0597 | 0.0504
Epoch 128/300, resid Loss: 0.0597 | 0.0503
Epoch 129/300, resid Loss: 0.0597 | 0.0503
Epoch 130/300, resid Loss: 0.0597 | 0.0503
Epoch 131/300, resid Loss: 0.0596 | 0.0503
Epoch 132/300, resid Loss: 0.0596 | 0.0503
Epoch 133/300, resid Loss: 0.0596 | 0.0503
Epoch 134/300, resid Loss: 0.0596 | 0.0503
Epoch 135/300, resid Loss: 0.0596 | 0.0503
Epoch 136/300, resid Loss: 0.0596 | 0.0503
Epoch 137/300, resid Loss: 0.0596 | 0.0503
Epoch 138/300, resid Loss: 0.0596 | 0.0503
Epoch 139/300, resid Loss: 0.0596 | 0.0503
Epoch 140/300, resid Loss: 0.0596 | 0.0503
Epoch 141/300, resid Loss: 0.0596 | 0.0503
Epoch 142/300, resid Loss: 0.0596 | 0.0503
Epoch 143/300, resid Loss: 0.0596 | 0.0503
Epoch 144/300, resid Loss: 0.0596 | 0.0502
Epoch 145/300, resid Loss: 0.0596 | 0.0502
Epoch 146/300, resid Loss: 0.0596 | 0.0502
Epoch 147/300, resid Loss: 0.0596 | 0.0502
Epoch 148/300, resid Loss: 0.0596 | 0.0502
Epoch 149/300, resid Loss: 0.0596 | 0.0502
Epoch 150/300, resid Loss: 0.0596 | 0.0502
Epoch 151/300, resid Loss: 0.0596 | 0.0502
Epoch 152/300, resid Loss: 0.0596 | 0.0502
Epoch 153/300, resid Loss: 0.0596 | 0.0502
Epoch 154/300, resid Loss: 0.0596 | 0.0502
Epoch 155/300, resid Loss: 0.0596 | 0.0502
Epoch 156/300, resid Loss: 0.0596 | 0.0502
Epoch 157/300, resid Loss: 0.0596 | 0.0502
Epoch 158/300, resid Loss: 0.0596 | 0.0502
Epoch 159/300, resid Loss: 0.0596 | 0.0502
Epoch 160/300, resid Loss: 0.0596 | 0.0502
Epoch 161/300, resid Loss: 0.0596 | 0.0502
Epoch 162/300, resid Loss: 0.0596 | 0.0502
Epoch 163/300, resid Loss: 0.0596 | 0.0502
Epoch 164/300, resid Loss: 0.0596 | 0.0502
Epoch 165/300, resid Loss: 0.0596 | 0.0502
Epoch 166/300, resid Loss: 0.0595 | 0.0502
Epoch 167/300, resid Loss: 0.0595 | 0.0502
Epoch 168/300, resid Loss: 0.0595 | 0.0502
Epoch 169/300, resid Loss: 0.0595 | 0.0502
Epoch 170/300, resid Loss: 0.0595 | 0.0502
Epoch 171/300, resid Loss: 0.0595 | 0.0502
Epoch 172/300, resid Loss: 0.0595 | 0.0502
Epoch 173/300, resid Loss: 0.0595 | 0.0502
Epoch 174/300, resid Loss: 0.0595 | 0.0502
Epoch 175/300, resid Loss: 0.0595 | 0.0502
Epoch 176/300, resid Loss: 0.0595 | 0.0502
Epoch 177/300, resid Loss: 0.0595 | 0.0502
Epoch 178/300, resid Loss: 0.0595 | 0.0502
Epoch 179/300, resid Loss: 0.0595 | 0.0502
Epoch 180/300, resid Loss: 0.0595 | 0.0502
Epoch 181/300, resid Loss: 0.0595 | 0.0502
Epoch 182/300, resid Loss: 0.0595 | 0.0502
Epoch 183/300, resid Loss: 0.0595 | 0.0502
Epoch 184/300, resid Loss: 0.0595 | 0.0502
Epoch 185/300, resid Loss: 0.0595 | 0.0502
Epoch 186/300, resid Loss: 0.0595 | 0.0502
Epoch 187/300, resid Loss: 0.0595 | 0.0502
Epoch 188/300, resid Loss: 0.0595 | 0.0502
Epoch 189/300, resid Loss: 0.0595 | 0.0502
Epoch 190/300, resid Loss: 0.0595 | 0.0502
Epoch 191/300, resid Loss: 0.0595 | 0.0502
Epoch 192/300, resid Loss: 0.0595 | 0.0502
Epoch 193/300, resid Loss: 0.0595 | 0.0502
Epoch 194/300, resid Loss: 0.0595 | 0.0502
Epoch 195/300, resid Loss: 0.0595 | 0.0502
Epoch 196/300, resid Loss: 0.0595 | 0.0502
Epoch 197/300, resid Loss: 0.0595 | 0.0502
Epoch 198/300, resid Loss: 0.0595 | 0.0502
Epoch 199/300, resid Loss: 0.0595 | 0.0502
Epoch 200/300, resid Loss: 0.0595 | 0.0502
Epoch 201/300, resid Loss: 0.0595 | 0.0502
Epoch 202/300, resid Loss: 0.0595 | 0.0502
Epoch 203/300, resid Loss: 0.0595 | 0.0502
Epoch 204/300, resid Loss: 0.0595 | 0.0502
Epoch 205/300, resid Loss: 0.0595 | 0.0502
Epoch 206/300, resid Loss: 0.0595 | 0.0502
Epoch 207/300, resid Loss: 0.0595 | 0.0502
Epoch 208/300, resid Loss: 0.0595 | 0.0502
Epoch 209/300, resid Loss: 0.0595 | 0.0502
Epoch 210/300, resid Loss: 0.0595 | 0.0502
Epoch 211/300, resid Loss: 0.0595 | 0.0502
Epoch 212/300, resid Loss: 0.0595 | 0.0502
Epoch 213/300, resid Loss: 0.0595 | 0.0502
Epoch 214/300, resid Loss: 0.0595 | 0.0502
Epoch 215/300, resid Loss: 0.0595 | 0.0502
Epoch 216/300, resid Loss: 0.0595 | 0.0502
Epoch 217/300, resid Loss: 0.0595 | 0.0502
Epoch 218/300, resid Loss: 0.0595 | 0.0502
Epoch 219/300, resid Loss: 0.0595 | 0.0502
Epoch 220/300, resid Loss: 0.0595 | 0.0502
Epoch 221/300, resid Loss: 0.0595 | 0.0502
Epoch 222/300, resid Loss: 0.0595 | 0.0502
Epoch 223/300, resid Loss: 0.0595 | 0.0502
Epoch 224/300, resid Loss: 0.0595 | 0.0502
Epoch 225/300, resid Loss: 0.0595 | 0.0502
Epoch 226/300, resid Loss: 0.0595 | 0.0502
Epoch 227/300, resid Loss: 0.0595 | 0.0502
Epoch 228/300, resid Loss: 0.0595 | 0.0502
Epoch 229/300, resid Loss: 0.0595 | 0.0502
Epoch 230/300, resid Loss: 0.0595 | 0.0502
Epoch 231/300, resid Loss: 0.0595 | 0.0502
Early stopping for resid
Runtime (seconds): 4596.653164148331
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[152.68352]
[-0.05860343]
[3.391205]
[13.593843]
[2.45087]
[21.166327]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 15.154302791692317
RMSE: 3.892852783203125
MAE: 3.892852783203125
R-squared: nan
[193.22714]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
