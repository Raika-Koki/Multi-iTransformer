ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-31 14:26:48,460][0m A new study created in memory with name: no-name-0d130b2d-c3f7-4dfe-bb8d-eb122fc99f87[0m
[32m[I 2025-01-31 14:27:15,776][0m Trial 0 finished with value: 0.13600516812055966 and parameters: {'observation_period_num': 125, 'train_rates': 0.8433741758623235, 'learning_rate': 0.0009540858077157769, 'batch_size': 200, 'step_size': 2, 'gamma': 0.9224439849755982}. Best is trial 0 with value: 0.13600516812055966.[0m
[32m[I 2025-01-31 14:28:25,832][0m Trial 1 finished with value: 0.15510051068997535 and parameters: {'observation_period_num': 160, 'train_rates': 0.8306481264074413, 'learning_rate': 0.00014848302452205443, 'batch_size': 72, 'step_size': 12, 'gamma': 0.9384796736396945}. Best is trial 0 with value: 0.13600516812055966.[0m
[32m[I 2025-01-31 14:28:48,669][0m Trial 2 finished with value: 0.4674096148276964 and parameters: {'observation_period_num': 111, 'train_rates': 0.8151856496939996, 'learning_rate': 8.788344457156481e-06, 'batch_size': 232, 'step_size': 2, 'gamma': 0.942080528127245}. Best is trial 0 with value: 0.13600516812055966.[0m
[32m[I 2025-01-31 14:29:15,210][0m Trial 3 finished with value: 0.2641265423858867 and parameters: {'observation_period_num': 233, 'train_rates': 0.9064640085082086, 'learning_rate': 0.00013876570074842807, 'batch_size': 223, 'step_size': 2, 'gamma': 0.9545153318505684}. Best is trial 0 with value: 0.13600516812055966.[0m
[32m[I 2025-01-31 14:30:31,225][0m Trial 4 finished with value: 0.10608324746691808 and parameters: {'observation_period_num': 59, 'train_rates': 0.790497390723232, 'learning_rate': 0.00018506349654177505, 'batch_size': 65, 'step_size': 13, 'gamma': 0.8803097115391989}. Best is trial 4 with value: 0.10608324746691808.[0m
[32m[I 2025-01-31 14:30:57,355][0m Trial 5 finished with value: 0.9286674857139587 and parameters: {'observation_period_num': 19, 'train_rates': 0.960117954106948, 'learning_rate': 1.4354076611078705e-06, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8602636307161824}. Best is trial 4 with value: 0.10608324746691808.[0m
[32m[I 2025-01-31 14:31:31,654][0m Trial 6 finished with value: 0.23576897640951275 and parameters: {'observation_period_num': 182, 'train_rates': 0.7003579969729071, 'learning_rate': 0.0002763556445473819, 'batch_size': 137, 'step_size': 1, 'gamma': 0.895043108833937}. Best is trial 4 with value: 0.10608324746691808.[0m
[32m[I 2025-01-31 14:31:53,440][0m Trial 7 finished with value: 0.6428877723246887 and parameters: {'observation_period_num': 163, 'train_rates': 0.857218236659922, 'learning_rate': 7.3152310654597155e-06, 'batch_size': 248, 'step_size': 5, 'gamma': 0.7965980848006604}. Best is trial 4 with value: 0.10608324746691808.[0m
[32m[I 2025-01-31 14:32:20,761][0m Trial 8 finished with value: 0.13943477690654948 and parameters: {'observation_period_num': 146, 'train_rates': 0.7562997136192388, 'learning_rate': 0.0002362824692059361, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9329521747155718}. Best is trial 4 with value: 0.10608324746691808.[0m
[32m[I 2025-01-31 14:33:12,117][0m Trial 9 finished with value: 0.10002948144476809 and parameters: {'observation_period_num': 12, 'train_rates': 0.8297998284939267, 'learning_rate': 4.492858203434392e-05, 'batch_size': 104, 'step_size': 14, 'gamma': 0.9678117773464354}. Best is trial 9 with value: 0.10002948144476809.[0m
[32m[I 2025-01-31 14:37:19,544][0m Trial 10 finished with value: 0.06109797462010128 and parameters: {'observation_period_num': 5, 'train_rates': 0.6275483421132769, 'learning_rate': 3.8196610118856964e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9863085417950891}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:40:34,854][0m Trial 11 finished with value: 0.07693375658589541 and parameters: {'observation_period_num': 10, 'train_rates': 0.6047729977049499, 'learning_rate': 2.9535524122668922e-05, 'batch_size': 21, 'step_size': 7, 'gamma': 0.9841139100051413}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:44:08,075][0m Trial 12 finished with value: 0.1335351920997103 and parameters: {'observation_period_num': 70, 'train_rates': 0.604907480601024, 'learning_rate': 2.3722306686714223e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7503970000074971}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:47:32,257][0m Trial 13 finished with value: 0.13271515437016315 and parameters: {'observation_period_num': 59, 'train_rates': 0.6086932317959248, 'learning_rate': 4.0955911468556125e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.9794071512271}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:48:52,822][0m Trial 14 finished with value: 0.11596473076737913 and parameters: {'observation_period_num': 6, 'train_rates': 0.6843725477622048, 'learning_rate': 8.319016971466654e-06, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9897122553177229}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:49:25,810][0m Trial 15 finished with value: 0.22351665670673052 and parameters: {'observation_period_num': 88, 'train_rates': 0.6650094391306572, 'learning_rate': 2.446233675213144e-05, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9109570035017548}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:51:03,841][0m Trial 16 finished with value: 0.5504168198962156 and parameters: {'observation_period_num': 39, 'train_rates': 0.6473541984392112, 'learning_rate': 1.2725842957131092e-06, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8370348129018452}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:51:53,195][0m Trial 17 finished with value: 0.09021465148709387 and parameters: {'observation_period_num': 35, 'train_rates': 0.734926081740056, 'learning_rate': 6.676019408371572e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9882512998230186}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:52:35,510][0m Trial 18 finished with value: 0.5962078299967696 and parameters: {'observation_period_num': 93, 'train_rates': 0.6405790586926206, 'learning_rate': 3.438447519409507e-06, 'batch_size': 104, 'step_size': 4, 'gamma': 0.8181763096470086}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:54:49,712][0m Trial 19 finished with value: 0.19714692914281753 and parameters: {'observation_period_num': 251, 'train_rates': 0.719130092990117, 'learning_rate': 1.6684779766575604e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.960739044613852}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:55:36,352][0m Trial 20 finished with value: 0.10767053062541408 and parameters: {'observation_period_num': 38, 'train_rates': 0.6025850784727568, 'learning_rate': 8.5161028502935e-05, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9025763809790509}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:56:08,456][0m Trial 21 finished with value: 0.10572371598039673 and parameters: {'observation_period_num': 32, 'train_rates': 0.7438953014289446, 'learning_rate': 7.040252899446411e-05, 'batch_size': 158, 'step_size': 10, 'gamma': 0.9893598450388528}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:57:01,845][0m Trial 22 finished with value: 0.06485391008788817 and parameters: {'observation_period_num': 6, 'train_rates': 0.6428420701658599, 'learning_rate': 0.0004779163747551636, 'batch_size': 83, 'step_size': 8, 'gamma': 0.9669374785098446}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:58:49,133][0m Trial 23 finished with value: 0.07045527018299201 and parameters: {'observation_period_num': 5, 'train_rates': 0.646857862655914, 'learning_rate': 0.0007291427789994275, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9604769313584541}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 14:59:44,759][0m Trial 24 finished with value: 0.1008660520066568 and parameters: {'observation_period_num': 56, 'train_rates': 0.6567966413663063, 'learning_rate': 0.0009745970622915999, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9553844012070458}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:01:23,435][0m Trial 25 finished with value: 0.06830826779964465 and parameters: {'observation_period_num': 5, 'train_rates': 0.691086849877521, 'learning_rate': 0.0004926758508590082, 'batch_size': 47, 'step_size': 5, 'gamma': 0.9665436086093311}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:02:47,847][0m Trial 26 finished with value: 0.13148397648479468 and parameters: {'observation_period_num': 83, 'train_rates': 0.6920103554097129, 'learning_rate': 0.0005141532050793292, 'batch_size': 53, 'step_size': 5, 'gamma': 0.925832403454874}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:03:30,322][0m Trial 27 finished with value: 0.09446915394316117 and parameters: {'observation_period_num': 28, 'train_rates': 0.7663312376862519, 'learning_rate': 0.00029685654784391066, 'batch_size': 123, 'step_size': 6, 'gamma': 0.9682009937996072}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:04:30,278][0m Trial 28 finished with value: 0.0950419308288394 and parameters: {'observation_period_num': 45, 'train_rates': 0.6324081741315959, 'learning_rate': 0.0005364248246170518, 'batch_size': 72, 'step_size': 4, 'gamma': 0.8734464906938434}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:06:02,087][0m Trial 29 finished with value: 0.12748950175617052 and parameters: {'observation_period_num': 118, 'train_rates': 0.7161671432494682, 'learning_rate': 0.00038507933665865465, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9151375425711341}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:07:55,679][0m Trial 30 finished with value: 0.1838830627025442 and parameters: {'observation_period_num': 200, 'train_rates': 0.6757634904606469, 'learning_rate': 0.00012429401878795136, 'batch_size': 37, 'step_size': 6, 'gamma': 0.943709791890238}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:10:05,327][0m Trial 31 finished with value: 0.1209964899478648 and parameters: {'observation_period_num': 20, 'train_rates': 0.6403459348875657, 'learning_rate': 0.0006077918283049812, 'batch_size': 33, 'step_size': 8, 'gamma': 0.9701432228164141}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:11:15,039][0m Trial 32 finished with value: 0.06700749883940528 and parameters: {'observation_period_num': 5, 'train_rates': 0.6276916256929177, 'learning_rate': 0.0007001547751258686, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9480783098917512}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:12:21,273][0m Trial 33 finished with value: 0.09266584522520323 and parameters: {'observation_period_num': 24, 'train_rates': 0.6707866322934892, 'learning_rate': 0.0009365591555914215, 'batch_size': 68, 'step_size': 11, 'gamma': 0.9446195106200032}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:13:12,848][0m Trial 34 finished with value: 0.13833550324781538 and parameters: {'observation_period_num': 50, 'train_rates': 0.6251218973506362, 'learning_rate': 0.0005076624491636336, 'batch_size': 85, 'step_size': 9, 'gamma': 0.9294327079592156}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:14:27,976][0m Trial 35 finished with value: 0.1230378637297286 and parameters: {'observation_period_num': 74, 'train_rates': 0.700063924498299, 'learning_rate': 0.00034365363536457516, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9485461782533119}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:15:10,325][0m Trial 36 finished with value: 0.13655243813991547 and parameters: {'observation_period_num': 103, 'train_rates': 0.7852755852524642, 'learning_rate': 0.0001848860004234694, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9729993031250006}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:16:17,564][0m Trial 37 finished with value: 0.144815968684153 and parameters: {'observation_period_num': 134, 'train_rates': 0.8678421200385635, 'learning_rate': 0.00011076075051820577, 'batch_size': 78, 'step_size': 11, 'gamma': 0.9336561882323379}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:18:32,904][0m Trial 38 finished with value: 0.13822941493845353 and parameters: {'observation_period_num': 21, 'train_rates': 0.6225591946805258, 'learning_rate': 4.105039243433415e-06, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9541269121681126}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:19:10,813][0m Trial 39 finished with value: 0.08082401007413864 and parameters: {'observation_period_num': 69, 'train_rates': 0.9753367782889049, 'learning_rate': 0.0001957908235307307, 'batch_size': 158, 'step_size': 12, 'gamma': 0.8918014251527175}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:20:26,729][0m Trial 40 finished with value: 0.0674388177711018 and parameters: {'observation_period_num': 5, 'train_rates': 0.7212129582131839, 'learning_rate': 0.00042769648957567317, 'batch_size': 64, 'step_size': 6, 'gamma': 0.9176418268313922}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:21:43,399][0m Trial 41 finished with value: 0.06797013809619425 and parameters: {'observation_period_num': 5, 'train_rates': 0.7140560344750344, 'learning_rate': 0.00035889347869924934, 'batch_size': 63, 'step_size': 6, 'gamma': 0.9147407680534425}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:22:59,689][0m Trial 42 finished with value: 0.07868259367218779 and parameters: {'observation_period_num': 20, 'train_rates': 0.7221313906821292, 'learning_rate': 0.0003825736742726499, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8615612338776544}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:23:39,664][0m Trial 43 finished with value: 0.07445101704328291 and parameters: {'observation_period_num': 16, 'train_rates': 0.6623192490638858, 'learning_rate': 0.0002441884471583969, 'batch_size': 116, 'step_size': 15, 'gamma': 0.9182640703630496}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:24:36,316][0m Trial 44 finished with value: 0.116279660027977 and parameters: {'observation_period_num': 48, 'train_rates': 0.7628465092087026, 'learning_rate': 0.0007513835706894682, 'batch_size': 87, 'step_size': 7, 'gamma': 0.8950745111107956}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:25:30,643][0m Trial 45 finished with value: 0.11984662959535777 and parameters: {'observation_period_num': 29, 'train_rates': 0.8058064402867179, 'learning_rate': 1.5876738808292202e-05, 'batch_size': 97, 'step_size': 9, 'gamma': 0.9390455102189748}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:25:52,917][0m Trial 46 finished with value: 0.1603452269668861 and parameters: {'observation_period_num': 14, 'train_rates': 0.6203378710214942, 'learning_rate': 0.000155045566695569, 'batch_size': 203, 'step_size': 1, 'gamma': 0.8832654824691889}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:26:57,485][0m Trial 47 finished with value: 0.0667533329709571 and parameters: {'observation_period_num': 5, 'train_rates': 0.7071155618636887, 'learning_rate': 0.0003478310181637735, 'batch_size': 73, 'step_size': 6, 'gamma': 0.9056279971961513}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:27:54,515][0m Trial 48 finished with value: 0.2736843640498121 and parameters: {'observation_period_num': 204, 'train_rates': 0.6807973802632263, 'learning_rate': 0.0007271355885164944, 'batch_size': 75, 'step_size': 8, 'gamma': 0.9769455011849414}. Best is trial 10 with value: 0.06109797462010128.[0m
[32m[I 2025-01-31 15:31:29,799][0m Trial 49 finished with value: 0.4162004117150696 and parameters: {'observation_period_num': 37, 'train_rates': 0.9330156199551969, 'learning_rate': 1.9001057212369404e-06, 'batch_size': 26, 'step_size': 7, 'gamma': 0.7590986270921977}. Best is trial 10 with value: 0.06109797462010128.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-31 15:31:29,809][0m A new study created in memory with name: no-name-b3a625ce-7b0c-4877-95af-0a8e354a0184[0m
[32m[I 2025-01-31 15:31:53,127][0m Trial 0 finished with value: 0.1382898880650822 and parameters: {'observation_period_num': 85, 'train_rates': 0.7817456873763997, 'learning_rate': 0.0002527399272172391, 'batch_size': 244, 'step_size': 4, 'gamma': 0.8211246864569163}. Best is trial 0 with value: 0.1382898880650822.[0m
[32m[I 2025-01-31 15:32:28,494][0m Trial 1 finished with value: 0.2480754191046878 and parameters: {'observation_period_num': 79, 'train_rates': 0.6408931651765549, 'learning_rate': 6.865674120605353e-05, 'batch_size': 131, 'step_size': 5, 'gamma': 0.87756703054124}. Best is trial 0 with value: 0.1382898880650822.[0m
[32m[I 2025-01-31 15:32:56,752][0m Trial 2 finished with value: 0.2598041987915834 and parameters: {'observation_period_num': 185, 'train_rates': 0.8919080069720937, 'learning_rate': 3.8627637906286524e-05, 'batch_size': 199, 'step_size': 15, 'gamma': 0.7884093958606639}. Best is trial 0 with value: 0.1382898880650822.[0m
[32m[I 2025-01-31 15:33:27,202][0m Trial 3 finished with value: 0.20537009247658006 and parameters: {'observation_period_num': 185, 'train_rates': 0.885798087226636, 'learning_rate': 0.00033368919736516823, 'batch_size': 186, 'step_size': 11, 'gamma': 0.9280082190561627}. Best is trial 0 with value: 0.1382898880650822.[0m
[32m[I 2025-01-31 15:33:54,338][0m Trial 4 finished with value: 0.10294805552574472 and parameters: {'observation_period_num': 55, 'train_rates': 0.6407333722774862, 'learning_rate': 0.0001752755527754661, 'batch_size': 185, 'step_size': 14, 'gamma': 0.8827966146079942}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:34:33,458][0m Trial 5 finished with value: 0.1642910780814978 and parameters: {'observation_period_num': 86, 'train_rates': 0.8641098289426206, 'learning_rate': 2.668415486498104e-05, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9432699338284395}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:35:15,576][0m Trial 6 finished with value: 0.317716121673584 and parameters: {'observation_period_num': 170, 'train_rates': 0.9576614988371461, 'learning_rate': 0.00012453856416189235, 'batch_size': 139, 'step_size': 9, 'gamma': 0.9306719184603088}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:36:17,906][0m Trial 7 finished with value: 0.32976749118169146 and parameters: {'observation_period_num': 58, 'train_rates': 0.8705360701053521, 'learning_rate': 3.2550581662141948e-06, 'batch_size': 89, 'step_size': 13, 'gamma': 0.7736906894247572}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:36:53,837][0m Trial 8 finished with value: 0.10714972764253616 and parameters: {'observation_period_num': 98, 'train_rates': 0.9832873256035204, 'learning_rate': 0.0001224191750354352, 'batch_size': 173, 'step_size': 7, 'gamma': 0.7774697480927824}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:37:18,586][0m Trial 9 finished with value: 0.7998584508895874 and parameters: {'observation_period_num': 196, 'train_rates': 0.9262522240300504, 'learning_rate': 2.0916915336744825e-06, 'batch_size': 240, 'step_size': 9, 'gamma': 0.8892151859677548}. Best is trial 4 with value: 0.10294805552574472.[0m
[32m[I 2025-01-31 15:40:37,879][0m Trial 10 finished with value: 0.07377395694111956 and parameters: {'observation_period_num': 5, 'train_rates': 0.6066454393668468, 'learning_rate': 0.000759055513196187, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9781769571421395}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:43:58,221][0m Trial 11 finished with value: 0.0797099107996746 and parameters: {'observation_period_num': 5, 'train_rates': 0.6064607648835677, 'learning_rate': 0.0009626580593657142, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9824019878528153}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:47:36,310][0m Trial 12 finished with value: 0.07993214445248727 and parameters: {'observation_period_num': 6, 'train_rates': 0.6003097930389286, 'learning_rate': 0.0006911387077112604, 'batch_size': 19, 'step_size': 12, 'gamma': 0.9833612018161821}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:51:53,372][0m Trial 13 finished with value: 0.08762107700692802 and parameters: {'observation_period_num': 7, 'train_rates': 0.7139499863134685, 'learning_rate': 0.0009466015713365427, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9887194751820026}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:53:03,446][0m Trial 14 finished with value: 0.5201063380217004 and parameters: {'observation_period_num': 238, 'train_rates': 0.7120612323960499, 'learning_rate': 7.479151447855507e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.955282080312001}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:54:19,524][0m Trial 15 finished with value: 0.09109682510284858 and parameters: {'observation_period_num': 31, 'train_rates': 0.7083204090391647, 'learning_rate': 0.00043123283387722154, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8411313252719943}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:56:02,171][0m Trial 16 finished with value: 0.15353207589599363 and parameters: {'observation_period_num': 128, 'train_rates': 0.7745443767941561, 'learning_rate': 0.0009402802134314762, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9635820399757699}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:56:48,362][0m Trial 17 finished with value: 0.10339543746595972 and parameters: {'observation_period_num': 32, 'train_rates': 0.6040023473248952, 'learning_rate': 0.0004274780265164784, 'batch_size': 93, 'step_size': 15, 'gamma': 0.9068875630213165}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:58:30,699][0m Trial 18 finished with value: 0.19201396541730553 and parameters: {'observation_period_num': 119, 'train_rates': 0.6639209343099299, 'learning_rate': 1.2627433530795528e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.8517871806023045}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 15:59:30,464][0m Trial 19 finished with value: 0.5189420905610507 and parameters: {'observation_period_num': 36, 'train_rates': 0.824398215399523, 'learning_rate': 1.1024677295091074e-06, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9097300909615181}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:00:12,722][0m Trial 20 finished with value: 0.10295049876323777 and parameters: {'observation_period_num': 60, 'train_rates': 0.6756135247565994, 'learning_rate': 6.744817945891811e-05, 'batch_size': 110, 'step_size': 9, 'gamma': 0.9714507064810662}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:04:15,928][0m Trial 21 finished with value: 0.0978972938822446 and parameters: {'observation_period_num': 9, 'train_rates': 0.6014388198858726, 'learning_rate': 0.0007276074463442734, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9859026570019682}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:06:06,561][0m Trial 22 finished with value: 0.08116779957627017 and parameters: {'observation_period_num': 6, 'train_rates': 0.6000471345930569, 'learning_rate': 0.00044954299066805626, 'batch_size': 38, 'step_size': 14, 'gamma': 0.9691816726973013}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:09:41,063][0m Trial 23 finished with value: 0.1513700253828238 and parameters: {'observation_period_num': 31, 'train_rates': 0.6412824313506508, 'learning_rate': 0.0006080576142278939, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9432851933272537}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:11:00,553][0m Trial 24 finished with value: 0.09239998890251076 and parameters: {'observation_period_num': 24, 'train_rates': 0.7367518489187992, 'learning_rate': 0.00021928814104926614, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9883467044362475}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:13:02,526][0m Trial 25 finished with value: 0.10187280068762737 and parameters: {'observation_period_num': 46, 'train_rates': 0.681319640948953, 'learning_rate': 0.0009246815184953952, 'batch_size': 37, 'step_size': 15, 'gamma': 0.951148001547561}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:14:01,551][0m Trial 26 finished with value: 0.13090834928822986 and parameters: {'observation_period_num': 67, 'train_rates': 0.6300380638010098, 'learning_rate': 0.0005351224815760814, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9170653872186804}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:16:35,398][0m Trial 27 finished with value: 0.19399618558640028 and parameters: {'observation_period_num': 148, 'train_rates': 0.7559190152793878, 'learning_rate': 0.00026153834338708506, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9759441380961953}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:17:22,662][0m Trial 28 finished with value: 0.10199770368417801 and parameters: {'observation_period_num': 19, 'train_rates': 0.8253058739733603, 'learning_rate': 0.00013559131011748677, 'batch_size': 111, 'step_size': 10, 'gamma': 0.9321466654240891}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:18:53,551][0m Trial 29 finished with value: 0.15756259985260299 and parameters: {'observation_period_num': 102, 'train_rates': 0.6660564714193495, 'learning_rate': 0.0003036270271883538, 'batch_size': 48, 'step_size': 2, 'gamma': 0.8203789601019624}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:19:50,134][0m Trial 30 finished with value: 0.19722144742925724 and parameters: {'observation_period_num': 237, 'train_rates': 0.6311749370952742, 'learning_rate': 6.576680505524802e-05, 'batch_size': 73, 'step_size': 5, 'gamma': 0.9583051205038732}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:21:56,898][0m Trial 31 finished with value: 0.08825155150364428 and parameters: {'observation_period_num': 15, 'train_rates': 0.6010662444979861, 'learning_rate': 0.0005677794598563058, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9672604884144256}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:24:13,386][0m Trial 32 finished with value: 0.11932720390318498 and parameters: {'observation_period_num': 47, 'train_rates': 0.6287950356971033, 'learning_rate': 0.0004362147883518994, 'batch_size': 31, 'step_size': 13, 'gamma': 0.9748995510789947}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:25:35,942][0m Trial 33 finished with value: 0.08077386499534421 and parameters: {'observation_period_num': 9, 'train_rates': 0.6542731167125365, 'learning_rate': 0.000998723473846456, 'batch_size': 54, 'step_size': 15, 'gamma': 0.9770026744460902}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:26:58,583][0m Trial 34 finished with value: 0.12972793416064454 and parameters: {'observation_period_num': 77, 'train_rates': 0.650904077831976, 'learning_rate': 0.0006878127116322513, 'batch_size': 53, 'step_size': 15, 'gamma': 0.989575620429375}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:30:28,587][0m Trial 35 finished with value: 0.13509423486470745 and parameters: {'observation_period_num': 43, 'train_rates': 0.6200815909811108, 'learning_rate': 0.00019831070657726533, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9437496490113696}. Best is trial 10 with value: 0.07377395694111956.[0m
[32m[I 2025-01-31 16:31:34,214][0m Trial 36 finished with value: 0.07351303542938491 and parameters: {'observation_period_num': 5, 'train_rates': 0.6923391728978608, 'learning_rate': 0.00029827732900110084, 'batch_size': 72, 'step_size': 12, 'gamma': 0.75792516772863}. Best is trial 36 with value: 0.07351303542938491.[0m
[32m[I 2025-01-31 16:32:04,529][0m Trial 37 finished with value: 0.08855502179409439 and parameters: {'observation_period_num': 23, 'train_rates': 0.6902125760274361, 'learning_rate': 0.0003443187174796589, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7501904207358472}. Best is trial 36 with value: 0.07351303542938491.[0m
[32m[I 2025-01-31 16:33:03,220][0m Trial 38 finished with value: 0.22084432711554725 and parameters: {'observation_period_num': 77, 'train_rates': 0.6258922287832107, 'learning_rate': 9.514351729226426e-05, 'batch_size': 74, 'step_size': 7, 'gamma': 0.809621128822715}. Best is trial 36 with value: 0.07351303542938491.[0m
[32m[I 2025-01-31 16:35:41,157][0m Trial 39 finished with value: 0.09126303405728813 and parameters: {'observation_period_num': 21, 'train_rates': 0.7284373128401438, 'learning_rate': 3.1096377278786576e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8909808459689252}. Best is trial 36 with value: 0.07351303542938491.[0m
[32m[I 2025-01-31 16:36:25,489][0m Trial 40 finished with value: 0.10521626217933737 and parameters: {'observation_period_num': 50, 'train_rates': 0.80663195541677, 'learning_rate': 0.0002744320649873038, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8581568338327026}. Best is trial 36 with value: 0.07351303542938491.[0m
[32m[I 2025-01-31 16:37:48,066][0m Trial 41 finished with value: 0.07176952523319688 and parameters: {'observation_period_num': 5, 'train_rates': 0.6535166838932545, 'learning_rate': 0.0009865596345313245, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8003077589058271}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:38:09,465][0m Trial 42 finished with value: 0.07679392560436116 and parameters: {'observation_period_num': 6, 'train_rates': 0.6900028181443913, 'learning_rate': 0.0006445605469034764, 'batch_size': 230, 'step_size': 14, 'gamma': 0.7532743256401055}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:38:33,679][0m Trial 43 finished with value: 0.08799932691046095 and parameters: {'observation_period_num': 38, 'train_rates': 0.6899047130734345, 'learning_rate': 0.0003529171303931838, 'batch_size': 223, 'step_size': 13, 'gamma': 0.7565193602615701}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:38:57,800][0m Trial 44 finished with value: 0.07782694187032001 and parameters: {'observation_period_num': 20, 'train_rates': 0.696066850997534, 'learning_rate': 0.0006603696172595573, 'batch_size': 205, 'step_size': 14, 'gamma': 0.7989197400395722}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:39:23,765][0m Trial 45 finished with value: 0.11242691959798988 and parameters: {'observation_period_num': 18, 'train_rates': 0.7507582217284142, 'learning_rate': 4.657081879033246e-05, 'batch_size': 205, 'step_size': 14, 'gamma': 0.7898005238759549}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:39:47,592][0m Trial 46 finished with value: 0.11679839554327257 and parameters: {'observation_period_num': 63, 'train_rates': 0.7063416732224685, 'learning_rate': 0.00015333025287240265, 'batch_size': 214, 'step_size': 14, 'gamma': 0.7676134853890358}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:40:09,251][0m Trial 47 finished with value: 0.20994361354162577 and parameters: {'observation_period_num': 27, 'train_rates': 0.6982820366216346, 'learning_rate': 1.537007225018127e-05, 'batch_size': 254, 'step_size': 13, 'gamma': 0.7933128343146294}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:40:30,672][0m Trial 48 finished with value: 0.16853608369114864 and parameters: {'observation_period_num': 204, 'train_rates': 0.7262962425363364, 'learning_rate': 0.0007189642254369102, 'batch_size': 234, 'step_size': 13, 'gamma': 0.8042213367250414}. Best is trial 41 with value: 0.07176952523319688.[0m
[32m[I 2025-01-31 16:40:57,132][0m Trial 49 finished with value: 0.1392011349025112 and parameters: {'observation_period_num': 155, 'train_rates': 0.6638913402300082, 'learning_rate': 0.0005225517832115421, 'batch_size': 185, 'step_size': 14, 'gamma': 0.7666957311786937}. Best is trial 41 with value: 0.07176952523319688.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-31 16:40:57,142][0m A new study created in memory with name: no-name-3830e6ac-6ed4-4635-bed2-923e191c7b40[0m
[32m[I 2025-01-31 16:44:03,693][0m Trial 0 finished with value: 0.1804967669814415 and parameters: {'observation_period_num': 232, 'train_rates': 0.7064690029521828, 'learning_rate': 0.0005306992652444819, 'batch_size': 23, 'step_size': 15, 'gamma': 0.7709619312022687}. Best is trial 0 with value: 0.1804967669814415.[0m
[32m[I 2025-01-31 16:44:37,841][0m Trial 1 finished with value: 0.16815877814641159 and parameters: {'observation_period_num': 16, 'train_rates': 0.9091764031450894, 'learning_rate': 4.5884498776006116e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.851006740628178}. Best is trial 1 with value: 0.16815877814641159.[0m
[32m[I 2025-01-31 16:47:57,100][0m Trial 2 finished with value: 0.1748714218779308 and parameters: {'observation_period_num': 122, 'train_rates': 0.7830785716042533, 'learning_rate': 4.600187286589293e-06, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9275171730607692}. Best is trial 1 with value: 0.16815877814641159.[0m
[32m[I 2025-01-31 16:48:26,457][0m Trial 3 finished with value: 0.2505065680044815 and parameters: {'observation_period_num': 164, 'train_rates': 0.8273768528540845, 'learning_rate': 3.0729343705184224e-05, 'batch_size': 180, 'step_size': 5, 'gamma': 0.8594442706553329}. Best is trial 1 with value: 0.16815877814641159.[0m
[32m[I 2025-01-31 16:48:52,779][0m Trial 4 finished with value: 0.38791889411410646 and parameters: {'observation_period_num': 23, 'train_rates': 0.6434827789290732, 'learning_rate': 1.8756286633765678e-05, 'batch_size': 179, 'step_size': 1, 'gamma': 0.8891768462913864}. Best is trial 1 with value: 0.16815877814641159.[0m
[32m[I 2025-01-31 16:49:14,242][0m Trial 5 finished with value: 0.09870180567105612 and parameters: {'observation_period_num': 10, 'train_rates': 0.7197112651405273, 'learning_rate': 0.00016687765168930775, 'batch_size': 244, 'step_size': 3, 'gamma': 0.8844599038809604}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:52:34,957][0m Trial 6 finished with value: 0.1586200795429533 and parameters: {'observation_period_num': 206, 'train_rates': 0.6474311242465393, 'learning_rate': 0.00010622488131866921, 'batch_size': 20, 'step_size': 5, 'gamma': 0.8177032718639797}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:53:01,248][0m Trial 7 finished with value: 0.11356727902552691 and parameters: {'observation_period_num': 19, 'train_rates': 0.6837520942481172, 'learning_rate': 3.71154062810926e-05, 'batch_size': 185, 'step_size': 11, 'gamma': 0.9807874983299084}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:53:37,527][0m Trial 8 finished with value: 0.2550399801053038 and parameters: {'observation_period_num': 5, 'train_rates': 0.9261223908671083, 'learning_rate': 4.775049320689392e-06, 'batch_size': 165, 'step_size': 14, 'gamma': 0.9698752228925948}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:54:18,171][0m Trial 9 finished with value: 0.31303215545709945 and parameters: {'observation_period_num': 72, 'train_rates': 0.6344916452098435, 'learning_rate': 1.44745818615358e-05, 'batch_size': 111, 'step_size': 3, 'gamma': 0.8819016753147674}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:54:39,534][0m Trial 10 finished with value: 0.11934016564605283 and parameters: {'observation_period_num': 94, 'train_rates': 0.7682048110633474, 'learning_rate': 0.0009130244758551407, 'batch_size': 253, 'step_size': 7, 'gamma': 0.7630914814857273}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:55:01,000][0m Trial 11 finished with value: 0.10687618650206664 and parameters: {'observation_period_num': 53, 'train_rates': 0.7161732207197239, 'learning_rate': 0.00017531000221154082, 'batch_size': 249, 'step_size': 11, 'gamma': 0.9849586721765328}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:55:22,443][0m Trial 12 finished with value: 0.10823021846938419 and parameters: {'observation_period_num': 57, 'train_rates': 0.7401098826765768, 'learning_rate': 0.0002136149808633261, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9472202794896294}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:55:47,553][0m Trial 13 finished with value: 0.11813751073910835 and parameters: {'observation_period_num': 58, 'train_rates': 0.8379933741597809, 'learning_rate': 0.0001964303854436084, 'batch_size': 229, 'step_size': 9, 'gamma': 0.9157990159496778}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:56:11,180][0m Trial 14 finished with value: 1.956653276505453 and parameters: {'observation_period_num': 107, 'train_rates': 0.7080178581703735, 'learning_rate': 1.3040089948799691e-06, 'batch_size': 219, 'step_size': 7, 'gamma': 0.8192354816858385}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:57:00,617][0m Trial 15 finished with value: 0.16088174457879775 and parameters: {'observation_period_num': 151, 'train_rates': 0.8661651362170696, 'learning_rate': 0.00010811876958837057, 'batch_size': 110, 'step_size': 13, 'gamma': 0.9881979086916405}. Best is trial 5 with value: 0.09870180567105612.[0m
[32m[I 2025-01-31 16:57:30,693][0m Trial 16 finished with value: 0.06308462470769882 and parameters: {'observation_period_num': 60, 'train_rates': 0.9814694788835652, 'learning_rate': 0.00044654053644803486, 'batch_size': 215, 'step_size': 2, 'gamma': 0.9153775691426143}. Best is trial 16 with value: 0.06308462470769882.[0m
[32m[I 2025-01-31 16:58:01,072][0m Trial 17 finished with value: 0.11092865467071533 and parameters: {'observation_period_num': 85, 'train_rates': 0.9784940115621207, 'learning_rate': 0.0004930450882614881, 'batch_size': 213, 'step_size': 1, 'gamma': 0.9078057450344279}. Best is trial 16 with value: 0.06308462470769882.[0m
[32m[I 2025-01-31 16:58:47,398][0m Trial 18 finished with value: 0.055254582315683365 and parameters: {'observation_period_num': 41, 'train_rates': 0.975556199535795, 'learning_rate': 0.0004226098956036641, 'batch_size': 134, 'step_size': 3, 'gamma': 0.8345578615900618}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:00:03,557][0m Trial 19 finished with value: 0.24971359043285765 and parameters: {'observation_period_num': 37, 'train_rates': 0.9602650924089017, 'learning_rate': 0.0009977841336172581, 'batch_size': 76, 'step_size': 3, 'gamma': 0.8067519415402377}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:00:44,941][0m Trial 20 finished with value: 0.1609316865603129 and parameters: {'observation_period_num': 151, 'train_rates': 0.8876010208968239, 'learning_rate': 0.000435167622609137, 'batch_size': 138, 'step_size': 5, 'gamma': 0.8501928396061312}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:01:15,112][0m Trial 21 finished with value: 0.24501287937164307 and parameters: {'observation_period_num': 47, 'train_rates': 0.9480248604964854, 'learning_rate': 0.000317160118577821, 'batch_size': 206, 'step_size': 3, 'gamma': 0.8911806175773331}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:01:57,482][0m Trial 22 finished with value: 0.06589724123477936 and parameters: {'observation_period_num': 37, 'train_rates': 0.9863373953800137, 'learning_rate': 7.366535866500806e-05, 'batch_size': 145, 'step_size': 2, 'gamma': 0.937662706300118}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:02:39,981][0m Trial 23 finished with value: 0.07857798039913177 and parameters: {'observation_period_num': 34, 'train_rates': 0.9897243777400612, 'learning_rate': 7.98261902507397e-05, 'batch_size': 146, 'step_size': 1, 'gamma': 0.949458099973322}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:03:33,876][0m Trial 24 finished with value: 0.23853633631000204 and parameters: {'observation_period_num': 76, 'train_rates': 0.9370881919781964, 'learning_rate': 6.431220281913885e-05, 'batch_size': 107, 'step_size': 2, 'gamma': 0.9450858894166794}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:04:41,632][0m Trial 25 finished with value: 0.16810943666220146 and parameters: {'observation_period_num': 103, 'train_rates': 0.9040969738421136, 'learning_rate': 0.0003077497479287677, 'batch_size': 83, 'step_size': 4, 'gamma': 0.8345545745328431}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:05:22,139][0m Trial 26 finished with value: 0.11370553821325302 and parameters: {'observation_period_num': 76, 'train_rates': 0.9899503706411797, 'learning_rate': 0.0006601815528025779, 'batch_size': 154, 'step_size': 6, 'gamma': 0.9247662227365778}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:06:10,679][0m Trial 27 finished with value: 0.40489625930786133 and parameters: {'observation_period_num': 37, 'train_rates': 0.9611537187449197, 'learning_rate': 1.0010072771677553e-05, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9580493333137001}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:07:15,295][0m Trial 28 finished with value: 0.1324322789526769 and parameters: {'observation_period_num': 120, 'train_rates': 0.8551477430307457, 'learning_rate': 0.00011423207422707341, 'batch_size': 81, 'step_size': 4, 'gamma': 0.9039372679947337}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:07:43,655][0m Trial 29 finished with value: 0.3145645817296695 and parameters: {'observation_period_num': 185, 'train_rates': 0.9185958880638798, 'learning_rate': 0.0003275838024914778, 'batch_size': 197, 'step_size': 2, 'gamma': 0.7920517417672694}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:09:24,708][0m Trial 30 finished with value: 0.1402146550964498 and parameters: {'observation_period_num': 62, 'train_rates': 0.8831412976259314, 'learning_rate': 0.0005983321655145932, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8664119595357153}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:10:09,008][0m Trial 31 finished with value: 0.08435188978910446 and parameters: {'observation_period_num': 34, 'train_rates': 0.9817995809648044, 'learning_rate': 6.651961937155835e-05, 'batch_size': 143, 'step_size': 1, 'gamma': 0.9340200691592282}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:10:46,879][0m Trial 32 finished with value: 0.21174722909927368 and parameters: {'observation_period_num': 251, 'train_rates': 0.9890332822563025, 'learning_rate': 6.554640985606502e-05, 'batch_size': 153, 'step_size': 1, 'gamma': 0.9369749338789778}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:11:36,952][0m Trial 33 finished with value: 0.2991447518579662 and parameters: {'observation_period_num': 29, 'train_rates': 0.956285360334121, 'learning_rate': 2.2255964578893297e-05, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9633806251709344}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:12:13,254][0m Trial 34 finished with value: 0.31917653105994487 and parameters: {'observation_period_num': 42, 'train_rates': 0.9356890816845926, 'learning_rate': 4.514997025137275e-05, 'batch_size': 162, 'step_size': 2, 'gamma': 0.8407062414163603}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:13:01,195][0m Trial 35 finished with value: 0.172325438710493 and parameters: {'observation_period_num': 20, 'train_rates': 0.6022270763243734, 'learning_rate': 9.877285493594011e-05, 'batch_size': 92, 'step_size': 1, 'gamma': 0.9225443833068083}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:14:42,982][0m Trial 36 finished with value: 0.2629855024069548 and parameters: {'observation_period_num': 5, 'train_rates': 0.9661041633048525, 'learning_rate': 0.00027316777943456745, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9497991450162591}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:15:13,559][0m Trial 37 finished with value: 0.1601225564843171 and parameters: {'observation_period_num': 72, 'train_rates': 0.8989180998548323, 'learning_rate': 0.00014075963447674289, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8995718562957141}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:15:55,877][0m Trial 38 finished with value: 0.13755415385238734 and parameters: {'observation_period_num': 24, 'train_rates': 0.806013684092332, 'learning_rate': 3.288271399676069e-05, 'batch_size': 130, 'step_size': 3, 'gamma': 0.8795891365190198}. Best is trial 18 with value: 0.055254582315683365.[0m
Early stopping at epoch 45
[32m[I 2025-01-31 17:16:12,403][0m Trial 39 finished with value: 1.0265143581649714 and parameters: {'observation_period_num': 91, 'train_rates': 0.9204434870274902, 'learning_rate': 8.439542012210298e-06, 'batch_size': 167, 'step_size': 1, 'gamma': 0.7895354570226182}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:16:38,867][0m Trial 40 finished with value: 1.0933102369308472 and parameters: {'observation_period_num': 50, 'train_rates': 0.9433980360953822, 'learning_rate': 1.1301980642789794e-06, 'batch_size': 233, 'step_size': 5, 'gamma': 0.9688523030879866}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:17:23,039][0m Trial 41 finished with value: 0.5141558051109314 and parameters: {'observation_period_num': 33, 'train_rates': 0.9740704878530825, 'learning_rate': 5.4440059496190276e-05, 'batch_size': 137, 'step_size': 1, 'gamma': 0.9121557836664672}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:18:05,047][0m Trial 42 finished with value: 0.06322159618139267 and parameters: {'observation_period_num': 16, 'train_rates': 0.9787849069841699, 'learning_rate': 8.053410399792578e-05, 'batch_size': 147, 'step_size': 2, 'gamma': 0.9391885181028781}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:18:45,346][0m Trial 43 finished with value: 0.38272204995155334 and parameters: {'observation_period_num': 14, 'train_rates': 0.9650518999129432, 'learning_rate': 2.1650890307554704e-05, 'batch_size': 150, 'step_size': 2, 'gamma': 0.9336668150238947}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:19:19,738][0m Trial 44 finished with value: 0.25767573714256287 and parameters: {'observation_period_num': 64, 'train_rates': 0.9460245171079182, 'learning_rate': 8.032598470125316e-05, 'batch_size': 172, 'step_size': 3, 'gamma': 0.9455296728684954}. Best is trial 18 with value: 0.055254582315683365.[0m
[32m[I 2025-01-31 17:20:14,066][0m Trial 45 finished with value: 0.046387407928705215 and parameters: {'observation_period_num': 18, 'train_rates': 0.9899559636877091, 'learning_rate': 0.0001484242418827821, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9751444067087647}. Best is trial 45 with value: 0.046387407928705215.[0m
[32m[I 2025-01-31 17:21:12,807][0m Trial 46 finished with value: 0.16435815828541914 and parameters: {'observation_period_num': 19, 'train_rates': 0.9274032147228761, 'learning_rate': 0.00015829907078200993, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9724005734993841}. Best is trial 45 with value: 0.046387407928705215.[0m
[32m[I 2025-01-31 17:22:03,166][0m Trial 47 finished with value: 0.295726478099823 and parameters: {'observation_period_num': 9, 'train_rates': 0.9711679379953954, 'learning_rate': 0.00021803431543687888, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9746096926601463}. Best is trial 45 with value: 0.046387407928705215.[0m
[32m[I 2025-01-31 17:22:29,661][0m Trial 48 finished with value: 0.08748466054651331 and parameters: {'observation_period_num': 47, 'train_rates': 0.6767919394831061, 'learning_rate': 0.0003894643991257514, 'batch_size': 181, 'step_size': 8, 'gamma': 0.8750590572520078}. Best is trial 45 with value: 0.046387407928705215.[0m
[32m[I 2025-01-31 17:23:27,799][0m Trial 49 finished with value: 0.20834115126749853 and parameters: {'observation_period_num': 25, 'train_rates': 0.9514376393039857, 'learning_rate': 0.0007396645592246909, 'batch_size': 101, 'step_size': 2, 'gamma': 0.8942010162325182}. Best is trial 45 with value: 0.046387407928705215.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-31 17:23:27,810][0m A new study created in memory with name: no-name-0fc6f19a-3076-4ff9-9493-b1e219b7790a[0m
[32m[I 2025-01-31 17:26:30,267][0m Trial 0 finished with value: 0.3456259784953935 and parameters: {'observation_period_num': 165, 'train_rates': 0.9625124102769863, 'learning_rate': 5.2010004873872666e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.8176748591043252}. Best is trial 0 with value: 0.3456259784953935.[0m
[32m[I 2025-01-31 17:27:12,514][0m Trial 1 finished with value: 0.7199093103408813 and parameters: {'observation_period_num': 130, 'train_rates': 0.9577315281264105, 'learning_rate': 3.059127462198449e-06, 'batch_size': 142, 'step_size': 8, 'gamma': 0.7970779000680122}. Best is trial 0 with value: 0.3456259784953935.[0m
[32m[I 2025-01-31 17:28:04,938][0m Trial 2 finished with value: 0.17767546446537333 and parameters: {'observation_period_num': 219, 'train_rates': 0.60493013315092, 'learning_rate': 0.0006183365175893641, 'batch_size': 78, 'step_size': 12, 'gamma': 0.7589096936843744}. Best is trial 2 with value: 0.17767546446537333.[0m
[32m[I 2025-01-31 17:28:27,180][0m Trial 3 finished with value: 2.8316725444267776 and parameters: {'observation_period_num': 81, 'train_rates': 0.6212718921848551, 'learning_rate': 1.0221944735024573e-06, 'batch_size': 212, 'step_size': 14, 'gamma': 0.7620944951694543}. Best is trial 2 with value: 0.17767546446537333.[0m
Early stopping at epoch 59
[32m[I 2025-01-31 17:29:29,382][0m Trial 4 finished with value: 0.2662369620227656 and parameters: {'observation_period_num': 75, 'train_rates': 0.947870870774216, 'learning_rate': 0.0009905595884946523, 'batch_size': 55, 'step_size': 1, 'gamma': 0.7727821051535775}. Best is trial 2 with value: 0.17767546446537333.[0m
[32m[I 2025-01-31 17:30:07,593][0m Trial 5 finished with value: 0.42413246631622314 and parameters: {'observation_period_num': 141, 'train_rates': 0.9714279341414114, 'learning_rate': 0.00010032807859006304, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8606580032878632}. Best is trial 2 with value: 0.17767546446537333.[0m
[32m[I 2025-01-31 17:31:23,190][0m Trial 6 finished with value: 0.1324725357385782 and parameters: {'observation_period_num': 100, 'train_rates': 0.7177894103774455, 'learning_rate': 2.851359639588916e-05, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9174510972805686}. Best is trial 6 with value: 0.1324725357385782.[0m
[32m[I 2025-01-31 17:32:28,373][0m Trial 7 finished with value: 0.379914028324732 and parameters: {'observation_period_num': 183, 'train_rates': 0.9261436020957563, 'learning_rate': 7.070889369786875e-06, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9587515522410068}. Best is trial 6 with value: 0.1324725357385782.[0m
[32m[I 2025-01-31 17:32:49,808][0m Trial 8 finished with value: 0.2182839042034702 and parameters: {'observation_period_num': 198, 'train_rates': 0.699590465781514, 'learning_rate': 0.0006869030283420966, 'batch_size': 232, 'step_size': 2, 'gamma': 0.8127547358404631}. Best is trial 6 with value: 0.1324725357385782.[0m
[32m[I 2025-01-31 17:33:16,268][0m Trial 9 finished with value: 0.402891170782603 and parameters: {'observation_period_num': 75, 'train_rates': 0.6725733251336966, 'learning_rate': 3.481156078328221e-06, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9650022485356489}. Best is trial 6 with value: 0.1324725357385782.[0m
[32m[I 2025-01-31 17:34:06,499][0m Trial 10 finished with value: 0.1222299031656364 and parameters: {'observation_period_num': 12, 'train_rates': 0.8227913805696805, 'learning_rate': 1.76057313003281e-05, 'batch_size': 108, 'step_size': 15, 'gamma': 0.919582562770694}. Best is trial 10 with value: 0.1222299031656364.[0m
[32m[I 2025-01-31 17:34:55,345][0m Trial 11 finished with value: 0.12023550103295524 and parameters: {'observation_period_num': 13, 'train_rates': 0.8018201941292219, 'learning_rate': 1.7369074401827536e-05, 'batch_size': 111, 'step_size': 14, 'gamma': 0.9139196522682866}. Best is trial 11 with value: 0.12023550103295524.[0m
[32m[I 2025-01-31 17:35:43,542][0m Trial 12 finished with value: 0.13137587808169326 and parameters: {'observation_period_num': 7, 'train_rates': 0.8432393594419049, 'learning_rate': 1.7820862887990348e-05, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9020265646878542}. Best is trial 11 with value: 0.12023550103295524.[0m
[32m[I 2025-01-31 17:36:31,997][0m Trial 13 finished with value: 0.08550488187723608 and parameters: {'observation_period_num': 8, 'train_rates': 0.8068973858161852, 'learning_rate': 0.00012331606105339573, 'batch_size': 110, 'step_size': 13, 'gamma': 0.9271989559485357}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:37:03,961][0m Trial 14 finished with value: 0.09701375539104144 and parameters: {'observation_period_num': 39, 'train_rates': 0.7777065156748207, 'learning_rate': 0.0001522036067161269, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8705622384550191}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:37:34,169][0m Trial 15 finished with value: 0.10133983859721618 and parameters: {'observation_period_num': 45, 'train_rates': 0.7555643827003806, 'learning_rate': 0.00018500721640885148, 'batch_size': 171, 'step_size': 12, 'gamma': 0.8619616712359284}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:38:03,258][0m Trial 16 finished with value: 0.12689938920797134 and parameters: {'observation_period_num': 46, 'train_rates': 0.865236755205287, 'learning_rate': 0.00024224944395183684, 'batch_size': 194, 'step_size': 12, 'gamma': 0.9865930759111285}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:38:27,674][0m Trial 17 finished with value: 0.1358725912752924 and parameters: {'observation_period_num': 42, 'train_rates': 0.8898617621679074, 'learning_rate': 0.00023482964464828355, 'batch_size': 247, 'step_size': 11, 'gamma': 0.8761338864014766}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:38:58,917][0m Trial 18 finished with value: 0.15697657411915183 and parameters: {'observation_period_num': 107, 'train_rates': 0.7714267318193505, 'learning_rate': 7.1832756861432e-05, 'batch_size': 166, 'step_size': 7, 'gamma': 0.8350321524469368}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:39:39,282][0m Trial 19 finished with value: 0.1092617850240371 and parameters: {'observation_period_num': 38, 'train_rates': 0.7496229737025776, 'learning_rate': 0.00013154004293744028, 'batch_size': 127, 'step_size': 13, 'gamma': 0.941322392561651}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:40:08,314][0m Trial 20 finished with value: 0.14755915150195859 and parameters: {'observation_period_num': 61, 'train_rates': 0.8969502560648089, 'learning_rate': 0.0003851373566271906, 'batch_size': 212, 'step_size': 10, 'gamma': 0.8885073659989913}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:40:40,442][0m Trial 21 finished with value: 0.09277166907956873 and parameters: {'observation_period_num': 29, 'train_rates': 0.7670315623456718, 'learning_rate': 0.0001550823189139439, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8582146946743576}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:41:11,973][0m Trial 22 finished with value: 0.20761449426912063 and parameters: {'observation_period_num': 248, 'train_rates': 0.8016601522056251, 'learning_rate': 4.912451037348533e-05, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8466557772991544}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:41:50,183][0m Trial 23 finished with value: 0.09797765655556546 and parameters: {'observation_period_num': 27, 'train_rates': 0.7701618087839369, 'learning_rate': 0.0003900665650967617, 'batch_size': 136, 'step_size': 11, 'gamma': 0.8876498705523198}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:42:18,428][0m Trial 24 finished with value: 0.09033505009958542 and parameters: {'observation_period_num': 21, 'train_rates': 0.7172336826234638, 'learning_rate': 0.00012559052286620733, 'batch_size': 179, 'step_size': 14, 'gamma': 0.9417543659104755}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:42:44,886][0m Trial 25 finished with value: 0.11028623708155486 and parameters: {'observation_period_num': 25, 'train_rates': 0.6522320834395147, 'learning_rate': 9.975102668000268e-05, 'batch_size': 188, 'step_size': 14, 'gamma': 0.9350021445046027}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:43:08,464][0m Trial 26 finished with value: 0.14644667716312657 and parameters: {'observation_period_num': 102, 'train_rates': 0.7316594446998661, 'learning_rate': 3.9841979150590496e-05, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9813269330846448}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:43:58,982][0m Trial 27 finished with value: 0.11084420513361692 and parameters: {'observation_period_num': 57, 'train_rates': 0.6947880535585201, 'learning_rate': 0.0003575219070977161, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9556998903785611}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:44:40,554][0m Trial 28 finished with value: 0.10779875076629898 and parameters: {'observation_period_num': 29, 'train_rates': 0.8345854553714271, 'learning_rate': 7.583248014050186e-05, 'batch_size': 132, 'step_size': 11, 'gamma': 0.9364540589526}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:49:03,224][0m Trial 29 finished with value: 0.13645390315785325 and parameters: {'observation_period_num': 155, 'train_rates': 0.7335594080275971, 'learning_rate': 3.0942533644656116e-05, 'batch_size': 17, 'step_size': 9, 'gamma': 0.898689952644891}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:50:44,573][0m Trial 30 finished with value: 0.09131672286749941 and parameters: {'observation_period_num': 8, 'train_rates': 0.8082006865272653, 'learning_rate': 7.012025311381998e-05, 'batch_size': 51, 'step_size': 6, 'gamma': 0.8343813622284788}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:53:24,431][0m Trial 31 finished with value: 0.09221977038042886 and parameters: {'observation_period_num': 5, 'train_rates': 0.8100589339562734, 'learning_rate': 6.0880676743622424e-05, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8299926620527933}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:55:23,436][0m Trial 32 finished with value: 0.101215613672466 and parameters: {'observation_period_num': 9, 'train_rates': 0.8122624714209549, 'learning_rate': 6.332271655395592e-05, 'batch_size': 43, 'step_size': 4, 'gamma': 0.8252093163168805}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 17:57:41,894][0m Trial 33 finished with value: 0.12549645086764394 and parameters: {'observation_period_num': 7, 'train_rates': 0.8647363020554193, 'learning_rate': 4.50593757072221e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.7960835950173275}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:02:47,108][0m Trial 34 finished with value: 0.10214734895796072 and parameters: {'observation_period_num': 61, 'train_rates': 0.7893416377666675, 'learning_rate': 0.00010244263227381038, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7885870621361668}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:04:07,954][0m Trial 35 finished with value: 0.15789653891784566 and parameters: {'observation_period_num': 20, 'train_rates': 0.8541455263413721, 'learning_rate': 2.4312895706289738e-05, 'batch_size': 67, 'step_size': 4, 'gamma': 0.8425413933480854}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:05:50,521][0m Trial 36 finished with value: 0.21038431338092778 and parameters: {'observation_period_num': 91, 'train_rates': 0.8211859056665121, 'learning_rate': 1.2155500513299189e-05, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8071314329091578}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:09:03,416][0m Trial 37 finished with value: 0.16847386002402187 and parameters: {'observation_period_num': 121, 'train_rates': 0.8861368704074388, 'learning_rate': 0.00024501861956434173, 'batch_size': 27, 'step_size': 8, 'gamma': 0.8304909091563066}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:10:02,044][0m Trial 38 finished with value: 1.07520037821231 and parameters: {'observation_period_num': 55, 'train_rates': 0.6384869197394004, 'learning_rate': 1.1523716536231307e-06, 'batch_size': 74, 'step_size': 3, 'gamma': 0.7810498371527894}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:10:59,260][0m Trial 39 finished with value: 0.10032208569073224 and parameters: {'observation_period_num': 20, 'train_rates': 0.7134202821868519, 'learning_rate': 6.272991007764228e-05, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8506603132418252}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:11:52,341][0m Trial 40 finished with value: 0.11710591771100697 and parameters: {'observation_period_num': 71, 'train_rates': 0.7890877310095471, 'learning_rate': 0.0001013136643139943, 'batch_size': 96, 'step_size': 1, 'gamma': 0.9699496727688492}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:12:28,398][0m Trial 41 finished with value: 0.0979307560250163 and parameters: {'observation_period_num': 31, 'train_rates': 0.7607042915212837, 'learning_rate': 0.0001511759340714357, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8528352564967873}. Best is trial 13 with value: 0.08550488187723608.[0m
[32m[I 2025-01-31 18:13:48,520][0m Trial 42 finished with value: 0.08530208532614471 and parameters: {'observation_period_num': 20, 'train_rates': 0.7469466120030503, 'learning_rate': 8.266859920398167e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8061833029289074}. Best is trial 42 with value: 0.08530208532614471.[0m
[32m[I 2025-01-31 18:15:04,004][0m Trial 43 finished with value: 0.09406261232416362 and parameters: {'observation_period_num': 17, 'train_rates': 0.7431386457378837, 'learning_rate': 3.9736038891628443e-05, 'batch_size': 65, 'step_size': 14, 'gamma': 0.8178658023090755}. Best is trial 42 with value: 0.08530208532614471.[0m
[32m[I 2025-01-31 18:17:32,680][0m Trial 44 finished with value: 0.07498839588288984 and parameters: {'observation_period_num': 5, 'train_rates': 0.6987359743281827, 'learning_rate': 8.411452895580493e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.7503102330102386}. Best is trial 44 with value: 0.07498839588288984.[0m
[32m[I 2025-01-31 18:18:53,787][0m Trial 45 finished with value: 0.1021469725916783 and parameters: {'observation_period_num': 49, 'train_rates': 0.6860323386465464, 'learning_rate': 8.256731746436304e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7514703483610753}. Best is trial 44 with value: 0.07498839588288984.[0m
[32m[I 2025-01-31 18:20:23,109][0m Trial 46 finished with value: 0.08867631814261298 and parameters: {'observation_period_num': 34, 'train_rates': 0.6674200191000413, 'learning_rate': 0.0006086359241871711, 'batch_size': 50, 'step_size': 8, 'gamma': 0.767228621574242}. Best is trial 44 with value: 0.07498839588288984.[0m
[32m[I 2025-01-31 18:22:55,737][0m Trial 47 finished with value: 0.0926610163493039 and parameters: {'observation_period_num': 34, 'train_rates': 0.6732929956544433, 'learning_rate': 0.0006306941466999599, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7651709279802187}. Best is trial 44 with value: 0.07498839588288984.[0m
[32m[I 2025-01-31 18:23:31,709][0m Trial 48 finished with value: 0.1004410738421696 and parameters: {'observation_period_num': 70, 'train_rates': 0.6205741683037883, 'learning_rate': 0.0008373209154041346, 'batch_size': 123, 'step_size': 8, 'gamma': 0.7645996339636766}. Best is trial 44 with value: 0.07498839588288984.[0m
[32m[I 2025-01-31 18:24:19,287][0m Trial 49 finished with value: 0.08078487453287875 and parameters: {'observation_period_num': 20, 'train_rates': 0.7120327477114254, 'learning_rate': 0.00046698172691461237, 'batch_size': 100, 'step_size': 10, 'gamma': 0.7510307821383309}. Best is trial 44 with value: 0.07498839588288984.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-31 18:24:19,297][0m A new study created in memory with name: no-name-6ded8c4f-6146-4810-a9e6-4130a2a4c6e7[0m
[32m[I 2025-01-31 18:26:50,455][0m Trial 0 finished with value: 0.21242985184703556 and parameters: {'observation_period_num': 121, 'train_rates': 0.7531031771533092, 'learning_rate': 8.354005100363198e-06, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8429146697819356}. Best is trial 0 with value: 0.21242985184703556.[0m
[32m[I 2025-01-31 18:27:15,152][0m Trial 1 finished with value: 0.4248129655233989 and parameters: {'observation_period_num': 97, 'train_rates': 0.6288326302495482, 'learning_rate': 9.95569048621983e-06, 'batch_size': 198, 'step_size': 5, 'gamma': 0.8085507565960529}. Best is trial 0 with value: 0.21242985184703556.[0m
[32m[I 2025-01-31 18:27:55,475][0m Trial 2 finished with value: 0.25127965211868286 and parameters: {'observation_period_num': 62, 'train_rates': 0.9518130208012585, 'learning_rate': 9.15794471098001e-05, 'batch_size': 148, 'step_size': 10, 'gamma': 0.985354021233665}. Best is trial 0 with value: 0.21242985184703556.[0m
[32m[I 2025-01-31 18:28:34,705][0m Trial 3 finished with value: 0.23799854361492653 and parameters: {'observation_period_num': 104, 'train_rates': 0.838666521149896, 'learning_rate': 1.7709490038260332e-05, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8728141356073015}. Best is trial 0 with value: 0.21242985184703556.[0m
Early stopping at epoch 85
[32m[I 2025-01-31 18:29:19,095][0m Trial 4 finished with value: 0.65735803906982 and parameters: {'observation_period_num': 23, 'train_rates': 0.9119136692400047, 'learning_rate': 2.927396955805802e-06, 'batch_size': 116, 'step_size': 2, 'gamma': 0.7870263806242901}. Best is trial 0 with value: 0.21242985184703556.[0m
[32m[I 2025-01-31 18:32:48,133][0m Trial 5 finished with value: 0.17859020413317042 and parameters: {'observation_period_num': 74, 'train_rates': 0.919450070381942, 'learning_rate': 0.0002948419336023983, 'batch_size': 26, 'step_size': 1, 'gamma': 0.8817082950201085}. Best is trial 5 with value: 0.17859020413317042.[0m
[32m[I 2025-01-31 18:33:12,290][0m Trial 6 finished with value: 0.4157419406172835 and parameters: {'observation_period_num': 91, 'train_rates': 0.8501976234867143, 'learning_rate': 5.463373289106479e-06, 'batch_size': 229, 'step_size': 6, 'gamma': 0.8669180280061419}. Best is trial 5 with value: 0.17859020413317042.[0m
[32m[I 2025-01-31 18:33:43,885][0m Trial 7 finished with value: 0.30744919664875575 and parameters: {'observation_period_num': 233, 'train_rates': 0.6119842820973694, 'learning_rate': 0.0008448895696130474, 'batch_size': 137, 'step_size': 5, 'gamma': 0.8367352236287049}. Best is trial 5 with value: 0.17859020413317042.[0m
[32m[I 2025-01-31 18:34:09,405][0m Trial 8 finished with value: 0.19259438367218387 and parameters: {'observation_period_num': 64, 'train_rates': 0.7964832089842275, 'learning_rate': 2.695059359828222e-05, 'batch_size': 227, 'step_size': 1, 'gamma': 0.977532828494347}. Best is trial 5 with value: 0.17859020413317042.[0m
[32m[I 2025-01-31 18:35:19,739][0m Trial 9 finished with value: 0.15995953662729845 and parameters: {'observation_period_num': 30, 'train_rates': 0.8599336250026046, 'learning_rate': 1.1184766144357406e-05, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9427825611752094}. Best is trial 9 with value: 0.15995953662729845.[0m
[32m[I 2025-01-31 18:36:17,556][0m Trial 10 finished with value: 0.5538900030213733 and parameters: {'observation_period_num': 187, 'train_rates': 0.7048649282001888, 'learning_rate': 1.4760876875160475e-06, 'batch_size': 77, 'step_size': 15, 'gamma': 0.9327917228508872}. Best is trial 9 with value: 0.15995953662729845.[0m
[32m[I 2025-01-31 18:42:17,781][0m Trial 11 finished with value: 0.3207868099212646 and parameters: {'observation_period_num': 18, 'train_rates': 0.973037736157933, 'learning_rate': 0.00013769313377563687, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9232254553770313}. Best is trial 9 with value: 0.15995953662729845.[0m
[32m[I 2025-01-31 18:43:41,489][0m Trial 12 finished with value: 0.208438009372523 and parameters: {'observation_period_num': 152, 'train_rates': 0.8935069313853125, 'learning_rate': 0.0006556956975536573, 'batch_size': 64, 'step_size': 3, 'gamma': 0.9173098260457724}. Best is trial 9 with value: 0.15995953662729845.[0m
[32m[I 2025-01-31 18:44:57,059][0m Trial 13 finished with value: 0.14197830423232047 and parameters: {'observation_period_num': 56, 'train_rates': 0.8930810511034811, 'learning_rate': 9.386265585149225e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.7514002126072613}. Best is trial 13 with value: 0.14197830423232047.[0m
[32m[I 2025-01-31 18:46:07,396][0m Trial 14 finished with value: 0.1169606580369519 and parameters: {'observation_period_num': 5, 'train_rates': 0.8607953599905068, 'learning_rate': 5.920854623787111e-05, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7809656721854361}. Best is trial 14 with value: 0.1169606580369519.[0m
[32m[I 2025-01-31 18:46:57,565][0m Trial 15 finished with value: 0.09685204107714787 and parameters: {'observation_period_num': 5, 'train_rates': 0.7919476336346337, 'learning_rate': 6.185780704037295e-05, 'batch_size': 104, 'step_size': 10, 'gamma': 0.754349827403422}. Best is trial 15 with value: 0.09685204107714787.[0m
[32m[I 2025-01-31 18:47:27,085][0m Trial 16 finished with value: 0.10735208288289953 and parameters: {'observation_period_num': 6, 'train_rates': 0.7667850544302354, 'learning_rate': 4.1871689455283635e-05, 'batch_size': 175, 'step_size': 13, 'gamma': 0.7584864589483201}. Best is trial 15 with value: 0.09685204107714787.[0m
[32m[I 2025-01-31 18:47:54,910][0m Trial 17 finished with value: 0.08918601750450976 and parameters: {'observation_period_num': 41, 'train_rates': 0.7082676265717258, 'learning_rate': 0.00023374552622811037, 'batch_size': 179, 'step_size': 13, 'gamma': 0.7598984743317435}. Best is trial 17 with value: 0.08918601750450976.[0m
[32m[I 2025-01-31 18:48:21,487][0m Trial 18 finished with value: 0.08685076431500924 and parameters: {'observation_period_num': 41, 'train_rates': 0.6770744635421437, 'learning_rate': 0.00025686597735492697, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8052895921159144}. Best is trial 18 with value: 0.08685076431500924.[0m
[32m[I 2025-01-31 18:48:45,730][0m Trial 19 finished with value: 0.14537061308510602 and parameters: {'observation_period_num': 171, 'train_rates': 0.689647084222362, 'learning_rate': 0.00025894965018256145, 'batch_size': 195, 'step_size': 13, 'gamma': 0.8107135467943808}. Best is trial 18 with value: 0.08685076431500924.[0m
[32m[I 2025-01-31 18:49:14,065][0m Trial 20 finished with value: 0.08418422428885412 and parameters: {'observation_period_num': 44, 'train_rates': 0.6601535973859171, 'learning_rate': 0.0003471943468040959, 'batch_size': 170, 'step_size': 13, 'gamma': 0.7833866698555435}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:49:42,391][0m Trial 21 finished with value: 0.08521092579813058 and parameters: {'observation_period_num': 43, 'train_rates': 0.6716810132846278, 'learning_rate': 0.00034577001168052115, 'batch_size': 167, 'step_size': 13, 'gamma': 0.7857015409239331}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:50:11,370][0m Trial 22 finished with value: 0.0843928806707263 and parameters: {'observation_period_num': 45, 'train_rates': 0.6563947279180045, 'learning_rate': 0.0004068136277706901, 'batch_size': 162, 'step_size': 15, 'gamma': 0.7976804601363394}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:50:40,480][0m Trial 23 finished with value: 0.12992331442924646 and parameters: {'observation_period_num': 79, 'train_rates': 0.6429140518369478, 'learning_rate': 0.0005844825023300715, 'batch_size': 158, 'step_size': 15, 'gamma': 0.7841205058030272}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:50:59,892][0m Trial 24 finished with value: 0.12702130883451915 and parameters: {'observation_period_num': 128, 'train_rates': 0.6486237164799624, 'learning_rate': 0.00043072188225131526, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8359118023034183}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:51:32,449][0m Trial 25 finished with value: 0.09162627661735126 and parameters: {'observation_period_num': 49, 'train_rates': 0.7329354719738966, 'learning_rate': 0.0009872623961142415, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8164746773136556}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:51:56,818][0m Trial 26 finished with value: 0.1194249245880255 and parameters: {'observation_period_num': 111, 'train_rates': 0.6702084809808702, 'learning_rate': 0.0001602480824355696, 'batch_size': 210, 'step_size': 12, 'gamma': 0.7758082430064601}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:52:33,159][0m Trial 27 finished with value: 0.12288789270282949 and parameters: {'observation_period_num': 81, 'train_rates': 0.6055605342439194, 'learning_rate': 0.0004543780747528841, 'batch_size': 121, 'step_size': 15, 'gamma': 0.7976714419252536}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:53:01,533][0m Trial 28 finished with value: 0.16072686379495607 and parameters: {'observation_period_num': 219, 'train_rates': 0.6592794455829826, 'learning_rate': 0.00014358660992787846, 'batch_size': 158, 'step_size': 14, 'gamma': 0.8284200849629607}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:53:25,103][0m Trial 29 finished with value: 0.13161654691218064 and parameters: {'observation_period_num': 142, 'train_rates': 0.7322703149435904, 'learning_rate': 0.0003949417660363576, 'batch_size': 214, 'step_size': 11, 'gamma': 0.773633068251548}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:53:55,179][0m Trial 30 finished with value: 0.09121727819887468 and parameters: {'observation_period_num': 30, 'train_rates': 0.7360372030265318, 'learning_rate': 0.00017899175094810628, 'batch_size': 171, 'step_size': 8, 'gamma': 0.8455625437799303}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:54:21,482][0m Trial 31 finished with value: 0.08940520174254661 and parameters: {'observation_period_num': 42, 'train_rates': 0.6725922477440518, 'learning_rate': 0.0002973010952342525, 'batch_size': 188, 'step_size': 12, 'gamma': 0.7928988947590591}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:54:55,092][0m Trial 32 finished with value: 0.09678329380465225 and parameters: {'observation_period_num': 44, 'train_rates': 0.633904439630928, 'learning_rate': 0.0006647392132582621, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8528900609690327}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:55:21,038][0m Trial 33 finished with value: 0.11730494689927952 and parameters: {'observation_period_num': 65, 'train_rates': 0.6933881266902151, 'learning_rate': 8.952205016630453e-05, 'batch_size': 189, 'step_size': 11, 'gamma': 0.8066123793202223}. Best is trial 20 with value: 0.08418422428885412.[0m
[32m[I 2025-01-31 18:55:43,481][0m Trial 34 finished with value: 0.0836209702261147 and parameters: {'observation_period_num': 34, 'train_rates': 0.6262202037820367, 'learning_rate': 0.00036366288110758745, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8224026291994908}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:56:06,912][0m Trial 35 finished with value: 0.11216220761725249 and parameters: {'observation_period_num': 90, 'train_rates': 0.627580678784701, 'learning_rate': 0.0004578762419886204, 'batch_size': 212, 'step_size': 14, 'gamma': 0.8205772145338299}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:56:38,112][0m Trial 36 finished with value: 0.08819767164590096 and parameters: {'observation_period_num': 24, 'train_rates': 0.6163658996012831, 'learning_rate': 0.0001150598833398466, 'batch_size': 150, 'step_size': 15, 'gamma': 0.7669147656757288}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:56:58,157][0m Trial 37 finished with value: 0.12588227206939145 and parameters: {'observation_period_num': 111, 'train_rates': 0.6525190837333924, 'learning_rate': 0.00034568635532094926, 'batch_size': 234, 'step_size': 13, 'gamma': 0.8900898712846903}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:57:20,700][0m Trial 38 finished with value: 0.11621079417486761 and parameters: {'observation_period_num': 59, 'train_rates': 0.6017975481598601, 'learning_rate': 0.00020474468887638945, 'batch_size': 200, 'step_size': 11, 'gamma': 0.8593595088689894}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:57:59,585][0m Trial 39 finished with value: 0.09728940160679614 and parameters: {'observation_period_num': 72, 'train_rates': 0.7153418499530542, 'learning_rate': 0.0007339418046534324, 'batch_size': 128, 'step_size': 8, 'gamma': 0.795056863967547}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:58:46,354][0m Trial 40 finished with value: 0.1620088578075976 and parameters: {'observation_period_num': 100, 'train_rates': 0.7668786293297163, 'learning_rate': 2.3445571887965078e-05, 'batch_size': 105, 'step_size': 14, 'gamma': 0.826104839464111}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:59:15,009][0m Trial 41 finished with value: 0.08661600120855997 and parameters: {'observation_period_num': 34, 'train_rates': 0.677509619958855, 'learning_rate': 0.0002515247411309978, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8017611483171256}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 18:59:43,996][0m Trial 42 finished with value: 0.08610699675854643 and parameters: {'observation_period_num': 20, 'train_rates': 0.6271845704091981, 'learning_rate': 0.0004998400833752059, 'batch_size': 166, 'step_size': 13, 'gamma': 0.8014415403794656}. Best is trial 34 with value: 0.0836209702261147.[0m
[32m[I 2025-01-31 19:00:15,400][0m Trial 43 finished with value: 0.06714374387131254 and parameters: {'observation_period_num': 17, 'train_rates': 0.6263361589618415, 'learning_rate': 0.0005369785453693631, 'batch_size': 151, 'step_size': 13, 'gamma': 0.789885516485259}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:00:46,443][0m Trial 44 finished with value: 0.0915618247748492 and parameters: {'observation_period_num': 51, 'train_rates': 0.6549948943945663, 'learning_rate': 0.0009295024630740217, 'batch_size': 149, 'step_size': 15, 'gamma': 0.7715930832110561}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:01:20,353][0m Trial 45 finished with value: 0.23411234459550442 and parameters: {'observation_period_num': 15, 'train_rates': 0.6199101019984352, 'learning_rate': 4.551036766492375e-06, 'batch_size': 137, 'step_size': 14, 'gamma': 0.7848903929559808}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:01:44,630][0m Trial 46 finished with value: 0.08769085157949191 and parameters: {'observation_period_num': 33, 'train_rates': 0.634931539686953, 'learning_rate': 0.0003355756335719066, 'batch_size': 205, 'step_size': 9, 'gamma': 0.790978334180449}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:02:06,349][0m Trial 47 finished with value: 0.09815269183897272 and parameters: {'observation_period_num': 67, 'train_rates': 0.6933096980621589, 'learning_rate': 0.0006401316317009327, 'batch_size': 224, 'step_size': 6, 'gamma': 0.8136001128329481}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:02:39,416][0m Trial 48 finished with value: 0.07726576374648335 and parameters: {'observation_period_num': 15, 'train_rates': 0.6594986484728798, 'learning_rate': 0.00019866389166867557, 'batch_size': 147, 'step_size': 11, 'gamma': 0.7648040787717684}. Best is trial 43 with value: 0.06714374387131254.[0m
[32m[I 2025-01-31 19:03:10,680][0m Trial 49 finished with value: 0.15042602551300716 and parameters: {'observation_period_num': 11, 'train_rates': 0.6425134260810093, 'learning_rate': 1.361461824697964e-05, 'batch_size': 148, 'step_size': 11, 'gamma': 0.765331382448839}. Best is trial 43 with value: 0.06714374387131254.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-31 19:03:10,690][0m A new study created in memory with name: no-name-002d3356-c141-4e65-9d37-b47334a01f53[0m
[32m[I 2025-01-31 19:04:31,200][0m Trial 0 finished with value: 0.16390772234039624 and parameters: {'observation_period_num': 135, 'train_rates': 0.8719145224729956, 'learning_rate': 0.0003761396488803437, 'batch_size': 66, 'step_size': 1, 'gamma': 0.8960872142217153}. Best is trial 0 with value: 0.16390772234039624.[0m
[32m[I 2025-01-31 19:06:02,236][0m Trial 1 finished with value: 0.2038563780924853 and parameters: {'observation_period_num': 247, 'train_rates': 0.7173119796074324, 'learning_rate': 1.579194948527678e-05, 'batch_size': 48, 'step_size': 7, 'gamma': 0.9668959689137293}. Best is trial 0 with value: 0.16390772234039624.[0m
[32m[I 2025-01-31 19:06:27,255][0m Trial 2 finished with value: 0.10819998311036322 and parameters: {'observation_period_num': 57, 'train_rates': 0.7836568698944925, 'learning_rate': 0.00048420502246114403, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9584913222775373}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:07:01,748][0m Trial 3 finished with value: 0.2299623702653606 and parameters: {'observation_period_num': 221, 'train_rates': 0.6008485206046151, 'learning_rate': 0.0008619612080079553, 'batch_size': 119, 'step_size': 12, 'gamma': 0.938501365103347}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:07:32,354][0m Trial 4 finished with value: 0.2175230085849762 and parameters: {'observation_period_num': 55, 'train_rates': 0.9386202054742332, 'learning_rate': 0.0004978996370334666, 'batch_size': 201, 'step_size': 8, 'gamma': 0.8105168781232321}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:08:31,463][0m Trial 5 finished with value: 0.5958251189715756 and parameters: {'observation_period_num': 28, 'train_rates': 0.9308753250042396, 'learning_rate': 1.8239760898275817e-06, 'batch_size': 99, 'step_size': 5, 'gamma': 0.8640463935769253}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:09:05,774][0m Trial 6 finished with value: 0.27197393774986267 and parameters: {'observation_period_num': 91, 'train_rates': 0.9452661750369791, 'learning_rate': 0.00017243402638668026, 'batch_size': 173, 'step_size': 3, 'gamma': 0.942848166170584}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:09:30,167][0m Trial 7 finished with value: 0.5444277698717147 and parameters: {'observation_period_num': 169, 'train_rates': 0.8854437323059925, 'learning_rate': 7.3302890692776455e-06, 'batch_size': 228, 'step_size': 15, 'gamma': 0.7754487471076095}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:10:19,444][0m Trial 8 finished with value: 0.2823676219757865 and parameters: {'observation_period_num': 107, 'train_rates': 0.7313127146848174, 'learning_rate': 1.1644048957613548e-05, 'batch_size': 99, 'step_size': 11, 'gamma': 0.7553197535580198}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:11:52,972][0m Trial 9 finished with value: 0.20192508410206803 and parameters: {'observation_period_num': 165, 'train_rates': 0.9004158195644416, 'learning_rate': 3.455900548090197e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9822553071784307}. Best is trial 2 with value: 0.10819998311036322.[0m
[32m[I 2025-01-31 19:12:24,286][0m Trial 10 finished with value: 0.10205685302412773 and parameters: {'observation_period_num': 8, 'train_rates': 0.8000528716275609, 'learning_rate': 0.00011374917919936052, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8846869534492511}. Best is trial 10 with value: 0.10205685302412773.[0m
[32m[I 2025-01-31 19:12:47,422][0m Trial 11 finished with value: 0.1149130158446016 and parameters: {'observation_period_num': 16, 'train_rates': 0.7937323402214643, 'learning_rate': 9.989006075325638e-05, 'batch_size': 253, 'step_size': 5, 'gamma': 0.879459403692589}. Best is trial 10 with value: 0.10205685302412773.[0m
[32m[I 2025-01-31 19:13:20,838][0m Trial 12 finished with value: 0.13834557715242943 and parameters: {'observation_period_num': 61, 'train_rates': 0.8049275498295924, 'learning_rate': 8.558225878211106e-05, 'batch_size': 165, 'step_size': 5, 'gamma': 0.8368072578251735}. Best is trial 10 with value: 0.10205685302412773.[0m
[32m[I 2025-01-31 19:13:54,437][0m Trial 13 finished with value: 0.08722385725498598 and parameters: {'observation_period_num': 9, 'train_rates': 0.7969956180834965, 'learning_rate': 0.0002207922524728594, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9134748311714049}. Best is trial 13 with value: 0.08722385725498598.[0m
[32m[I 2025-01-31 19:14:26,722][0m Trial 14 finished with value: 0.0869741892265208 and parameters: {'observation_period_num': 6, 'train_rates': 0.6965576998066438, 'learning_rate': 5.8199304396196125e-05, 'batch_size': 155, 'step_size': 10, 'gamma': 0.9109602790665935}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:15:00,216][0m Trial 15 finished with value: 0.15058688993279246 and parameters: {'observation_period_num': 43, 'train_rates': 0.6437588856010626, 'learning_rate': 4.016550088376277e-05, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9170125976216857}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:15:35,390][0m Trial 16 finished with value: 0.11297037800594123 and parameters: {'observation_period_num': 88, 'train_rates': 0.6857617728742554, 'learning_rate': 0.0002189171573705019, 'batch_size': 132, 'step_size': 13, 'gamma': 0.913158520079991}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:16:02,754][0m Trial 17 finished with value: 0.11972144344505274 and parameters: {'observation_period_num': 30, 'train_rates': 0.7464375573527098, 'learning_rate': 5.4721566823451355e-05, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8597280232786516}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:21:30,454][0m Trial 18 finished with value: 0.13333423085761875 and parameters: {'observation_period_num': 5, 'train_rates': 0.8492158049301006, 'learning_rate': 2.493053381540916e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9125149904715977}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:22:01,790][0m Trial 19 finished with value: 0.29209906875261993 and parameters: {'observation_period_num': 136, 'train_rates': 0.6668116897357594, 'learning_rate': 2.100788272966324e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.8191749919592279}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:22:56,079][0m Trial 20 finished with value: 0.2524036467075348 and parameters: {'observation_period_num': 101, 'train_rates': 0.9892205162972225, 'learning_rate': 4.705930418256035e-06, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9371411447570445}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:23:27,288][0m Trial 21 finished with value: 0.11588295772969127 and parameters: {'observation_period_num': 9, 'train_rates': 0.8299985238008865, 'learning_rate': 0.00014500072419177445, 'batch_size': 181, 'step_size': 3, 'gamma': 0.8796228812417359}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:24:01,649][0m Trial 22 finished with value: 0.11999386495228895 and parameters: {'observation_period_num': 74, 'train_rates': 0.7642445473004129, 'learning_rate': 6.842936569392044e-05, 'batch_size': 154, 'step_size': 7, 'gamma': 0.8998564806787701}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:24:29,479][0m Trial 23 finished with value: 0.10497662420734505 and parameters: {'observation_period_num': 33, 'train_rates': 0.8223060919117283, 'learning_rate': 0.00023872588191669672, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8488067153773503}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:24:54,145][0m Trial 24 finished with value: 0.10456612433242325 and parameters: {'observation_period_num': 8, 'train_rates': 0.691352760677317, 'learning_rate': 0.00011288482073270929, 'batch_size': 225, 'step_size': 3, 'gamma': 0.8908526175512371}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:25:26,589][0m Trial 25 finished with value: 0.12736612531093344 and parameters: {'observation_period_num': 43, 'train_rates': 0.7654375158262721, 'learning_rate': 2.358172385765326e-05, 'batch_size': 165, 'step_size': 11, 'gamma': 0.9255955297124661}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:26:00,919][0m Trial 26 finished with value: 0.10003198919947758 and parameters: {'observation_period_num': 70, 'train_rates': 0.6274587550577266, 'learning_rate': 0.00027961790675876504, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8317960103208961}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:26:53,303][0m Trial 27 finished with value: 0.10276509108287947 and parameters: {'observation_period_num': 72, 'train_rates': 0.6236258964092967, 'learning_rate': 0.0002801232123995537, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8004385932556837}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:27:29,006][0m Trial 28 finished with value: 0.14353164744389377 and parameters: {'observation_period_num': 122, 'train_rates': 0.6559572279727746, 'learning_rate': 0.0007504737196016323, 'batch_size': 132, 'step_size': 10, 'gamma': 0.834403851947805}. Best is trial 14 with value: 0.0869741892265208.[0m
Early stopping at epoch 66
[32m[I 2025-01-31 19:27:56,230][0m Trial 29 finished with value: 0.16102123288156814 and parameters: {'observation_period_num': 27, 'train_rates': 0.7055404225677349, 'learning_rate': 0.0003377951793691998, 'batch_size': 123, 'step_size': 1, 'gamma': 0.7889953121548052}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:28:51,085][0m Trial 30 finished with value: 0.2029077788145924 and parameters: {'observation_period_num': 76, 'train_rates': 0.6052789971379285, 'learning_rate': 0.00046369782056967975, 'batch_size': 77, 'step_size': 13, 'gamma': 0.9007724479856318}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:29:30,164][0m Trial 31 finished with value: 0.13150320770066293 and parameters: {'observation_period_num': 19, 'train_rates': 0.8547007262734226, 'learning_rate': 5.111572103241193e-05, 'batch_size': 147, 'step_size': 7, 'gamma': 0.881738353515265}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:29:58,656][0m Trial 32 finished with value: 0.10809640466020658 and parameters: {'observation_period_num': 42, 'train_rates': 0.7320606428054528, 'learning_rate': 0.00014246862607536915, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8433498058633452}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:30:32,837][0m Trial 33 finished with value: 0.11408255885341274 and parameters: {'observation_period_num': 52, 'train_rates': 0.7692118995005311, 'learning_rate': 7.973388618970822e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8588231254065763}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:31:06,131][0m Trial 34 finished with value: 0.09812532201088885 and parameters: {'observation_period_num': 23, 'train_rates': 0.8098233647546978, 'learning_rate': 0.00019729186025622914, 'batch_size': 168, 'step_size': 6, 'gamma': 0.9642360583347257}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:31:44,293][0m Trial 35 finished with value: 0.21306352268598006 and parameters: {'observation_period_num': 248, 'train_rates': 0.6270647907651057, 'learning_rate': 0.0007301238841548552, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9672848206018361}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:32:10,957][0m Trial 36 finished with value: 0.1033568629700036 and parameters: {'observation_period_num': 62, 'train_rates': 0.6813125801144005, 'learning_rate': 0.0003818526406933862, 'batch_size': 194, 'step_size': 10, 'gamma': 0.9544502477524508}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:32:48,217][0m Trial 37 finished with value: 0.10135026122872712 and parameters: {'observation_period_num': 26, 'train_rates': 0.7109882658255826, 'learning_rate': 0.00018870382993987577, 'batch_size': 132, 'step_size': 7, 'gamma': 0.9753860121270521}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:33:14,841][0m Trial 38 finished with value: 0.18848845564449826 and parameters: {'observation_period_num': 200, 'train_rates': 0.8607720760138861, 'learning_rate': 0.000987356455813348, 'batch_size': 212, 'step_size': 8, 'gamma': 0.9260948437822536}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:33:45,946][0m Trial 39 finished with value: 0.1111157949098766 and parameters: {'observation_period_num': 39, 'train_rates': 0.8228131073972279, 'learning_rate': 0.0005512374148122043, 'batch_size': 178, 'step_size': 4, 'gamma': 0.9882264045649464}. Best is trial 14 with value: 0.0869741892265208.[0m
[32m[I 2025-01-31 19:34:35,479][0m Trial 40 finished with value: 0.08082206302913286 and parameters: {'observation_period_num': 20, 'train_rates': 0.7465566988942867, 'learning_rate': 0.00030558680295740494, 'batch_size': 104, 'step_size': 6, 'gamma': 0.954790714329748}. Best is trial 40 with value: 0.08082206302913286.[0m
[32m[I 2025-01-31 19:35:24,770][0m Trial 41 finished with value: 0.08081911012211586 and parameters: {'observation_period_num': 20, 'train_rates': 0.7452771918435558, 'learning_rate': 0.00029097551737687166, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9525584335960607}. Best is trial 41 with value: 0.08081911012211586.[0m
[32m[I 2025-01-31 19:36:15,970][0m Trial 42 finished with value: 0.08258158598019152 and parameters: {'observation_period_num': 22, 'train_rates': 0.7492769355898455, 'learning_rate': 0.0001783876114544579, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9526534621251442}. Best is trial 41 with value: 0.08081911012211586.[0m
[32m[I 2025-01-31 19:37:07,420][0m Trial 43 finished with value: 0.08140365896778845 and parameters: {'observation_period_num': 18, 'train_rates': 0.7448856696078023, 'learning_rate': 0.0005724373279913121, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9375639255058564}. Best is trial 41 with value: 0.08081911012211586.[0m
[32m[I 2025-01-31 19:37:58,776][0m Trial 44 finished with value: 0.10380462617487521 and parameters: {'observation_period_num': 51, 'train_rates': 0.7453372910326003, 'learning_rate': 0.0005796922475423653, 'batch_size': 95, 'step_size': 4, 'gamma': 0.9489671257935252}. Best is trial 41 with value: 0.08081911012211586.[0m
[32m[I 2025-01-31 19:39:10,329][0m Trial 45 finished with value: 0.07881805748500005 and parameters: {'observation_period_num': 19, 'train_rates': 0.7302727276373985, 'learning_rate': 0.0003840879854958616, 'batch_size': 68, 'step_size': 2, 'gamma': 0.9403281739989863}. Best is trial 45 with value: 0.07881805748500005.[0m
[32m[I 2025-01-31 19:41:00,860][0m Trial 46 finished with value: 0.073633298703935 and parameters: {'observation_period_num': 16, 'train_rates': 0.727922262462855, 'learning_rate': 0.00042986678296899443, 'batch_size': 43, 'step_size': 1, 'gamma': 0.9398746161729022}. Best is trial 46 with value: 0.073633298703935.[0m
[32m[I 2025-01-31 19:43:00,208][0m Trial 47 finished with value: 0.15519180710398556 and parameters: {'observation_period_num': 226, 'train_rates': 0.7284851685753049, 'learning_rate': 0.00037561315437541887, 'batch_size': 37, 'step_size': 1, 'gamma': 0.9359954987368069}. Best is trial 46 with value: 0.073633298703935.[0m
[32m[I 2025-01-31 19:44:24,941][0m Trial 48 finished with value: 0.08517976505725597 and parameters: {'observation_period_num': 18, 'train_rates': 0.782346934974989, 'learning_rate': 0.0005759504622430041, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9776725163165039}. Best is trial 46 with value: 0.073633298703935.[0m
[32m[I 2025-01-31 19:46:06,699][0m Trial 49 finished with value: 0.08908050060272217 and parameters: {'observation_period_num': 35, 'train_rates': 0.7241269261406218, 'learning_rate': 0.0007057061645487404, 'batch_size': 46, 'step_size': 2, 'gamma': 0.9268755268910946}. Best is trial 46 with value: 0.073633298703935.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.6275483421132769, 'learning_rate': 3.8196610118856964e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9863085417950891}
Epoch 1/300, trend Loss: 0.2658 | 0.1945
Epoch 2/300, trend Loss: 0.1599 | 0.1568
Epoch 3/300, trend Loss: 0.1463 | 0.1822
Epoch 4/300, trend Loss: 0.1377 | 0.2321
Epoch 5/300, trend Loss: 0.1308 | 0.2924
Epoch 6/300, trend Loss: 0.1254 | 0.3408
Epoch 7/300, trend Loss: 0.1216 | 0.3552
Epoch 8/300, trend Loss: 0.1190 | 0.3425
Epoch 9/300, trend Loss: 0.1170 | 0.3187
Epoch 10/300, trend Loss: 0.1153 | 0.2863
Epoch 11/300, trend Loss: 0.1137 | 0.2494
Epoch 12/300, trend Loss: 0.1122 | 0.2127
Epoch 13/300, trend Loss: 0.1107 | 0.1807
Epoch 14/300, trend Loss: 0.1093 | 0.1551
Epoch 15/300, trend Loss: 0.1078 | 0.1364
Epoch 16/300, trend Loss: 0.1064 | 0.1228
Epoch 17/300, trend Loss: 0.1050 | 0.1131
Epoch 18/300, trend Loss: 0.1038 | 0.1059
Epoch 19/300, trend Loss: 0.1027 | 0.1004
Epoch 20/300, trend Loss: 0.1018 | 0.0962
Epoch 21/300, trend Loss: 0.1009 | 0.0931
Epoch 22/300, trend Loss: 0.1001 | 0.0903
Epoch 23/300, trend Loss: 0.0992 | 0.0879
Epoch 24/300, trend Loss: 0.0984 | 0.0857
Epoch 25/300, trend Loss: 0.0976 | 0.0836
Epoch 26/300, trend Loss: 0.0968 | 0.0816
Epoch 27/300, trend Loss: 0.0960 | 0.0799
Epoch 28/300, trend Loss: 0.0952 | 0.0785
Epoch 29/300, trend Loss: 0.0946 | 0.0773
Epoch 30/300, trend Loss: 0.0939 | 0.0764
Epoch 31/300, trend Loss: 0.0933 | 0.0756
Epoch 32/300, trend Loss: 0.0927 | 0.0750
Epoch 33/300, trend Loss: 0.0921 | 0.0743
Epoch 34/300, trend Loss: 0.0916 | 0.0738
Epoch 35/300, trend Loss: 0.0910 | 0.0732
Epoch 36/300, trend Loss: 0.0905 | 0.0726
Epoch 37/300, trend Loss: 0.0899 | 0.0720
Epoch 38/300, trend Loss: 0.0894 | 0.0715
Epoch 39/300, trend Loss: 0.0888 | 0.0709
Epoch 40/300, trend Loss: 0.0882 | 0.0703
Epoch 41/300, trend Loss: 0.0877 | 0.0698
Epoch 42/300, trend Loss: 0.0871 | 0.0692
Epoch 43/300, trend Loss: 0.0865 | 0.0687
Epoch 44/300, trend Loss: 0.0860 | 0.0683
Epoch 45/300, trend Loss: 0.0855 | 0.0679
Epoch 46/300, trend Loss: 0.0850 | 0.0677
Epoch 47/300, trend Loss: 0.0846 | 0.0675
Epoch 48/300, trend Loss: 0.0842 | 0.0675
Epoch 49/300, trend Loss: 0.0839 | 0.0675
Epoch 50/300, trend Loss: 0.0836 | 0.0675
Epoch 51/300, trend Loss: 0.0834 | 0.0677
Epoch 52/300, trend Loss: 0.0832 | 0.0678
Epoch 53/300, trend Loss: 0.0830 | 0.0679
Epoch 54/300, trend Loss: 0.0828 | 0.0679
Epoch 55/300, trend Loss: 0.0825 | 0.0679
Epoch 56/300, trend Loss: 0.0823 | 0.0678
Epoch 57/300, trend Loss: 0.0820 | 0.0676
Epoch 58/300, trend Loss: 0.0817 | 0.0674
Epoch 59/300, trend Loss: 0.0815 | 0.0671
Epoch 60/300, trend Loss: 0.0812 | 0.0668
Epoch 61/300, trend Loss: 0.0810 | 0.0665
Epoch 62/300, trend Loss: 0.0807 | 0.0662
Epoch 63/300, trend Loss: 0.0805 | 0.0659
Epoch 64/300, trend Loss: 0.0803 | 0.0656
Epoch 65/300, trend Loss: 0.0800 | 0.0653
Epoch 66/300, trend Loss: 0.0798 | 0.0650
Epoch 67/300, trend Loss: 0.0796 | 0.0648
Epoch 68/300, trend Loss: 0.0795 | 0.0646
Epoch 69/300, trend Loss: 0.0793 | 0.0644
Epoch 70/300, trend Loss: 0.0791 | 0.0643
Epoch 71/300, trend Loss: 0.0789 | 0.0642
Epoch 72/300, trend Loss: 0.0788 | 0.0641
Epoch 73/300, trend Loss: 0.0786 | 0.0639
Epoch 74/300, trend Loss: 0.0783 | 0.0637
Epoch 75/300, trend Loss: 0.0781 | 0.0636
Epoch 76/300, trend Loss: 0.0779 | 0.0634
Epoch 77/300, trend Loss: 0.0777 | 0.0631
Epoch 78/300, trend Loss: 0.0775 | 0.0629
Epoch 79/300, trend Loss: 0.0774 | 0.0627
Epoch 80/300, trend Loss: 0.0772 | 0.0624
Epoch 81/300, trend Loss: 0.0770 | 0.0622
Epoch 82/300, trend Loss: 0.0768 | 0.0620
Epoch 83/300, trend Loss: 0.0767 | 0.0619
Epoch 84/300, trend Loss: 0.0765 | 0.0617
Epoch 85/300, trend Loss: 0.0764 | 0.0616
Epoch 86/300, trend Loss: 0.0762 | 0.0614
Epoch 87/300, trend Loss: 0.0761 | 0.0613
Epoch 88/300, trend Loss: 0.0759 | 0.0612
Epoch 89/300, trend Loss: 0.0757 | 0.0611
Epoch 90/300, trend Loss: 0.0756 | 0.0610
Epoch 91/300, trend Loss: 0.0754 | 0.0609
Epoch 92/300, trend Loss: 0.0753 | 0.0608
Epoch 93/300, trend Loss: 0.0751 | 0.0608
Epoch 94/300, trend Loss: 0.0749 | 0.0607
Epoch 95/300, trend Loss: 0.0748 | 0.0606
Epoch 96/300, trend Loss: 0.0746 | 0.0606
Epoch 97/300, trend Loss: 0.0745 | 0.0606
Epoch 98/300, trend Loss: 0.0743 | 0.0605
Epoch 99/300, trend Loss: 0.0742 | 0.0605
Epoch 100/300, trend Loss: 0.0740 | 0.0605
Epoch 101/300, trend Loss: 0.0739 | 0.0605
Epoch 102/300, trend Loss: 0.0737 | 0.0605
Epoch 103/300, trend Loss: 0.0736 | 0.0606
Epoch 104/300, trend Loss: 0.0734 | 0.0606
Epoch 105/300, trend Loss: 0.0733 | 0.0606
Epoch 106/300, trend Loss: 0.0732 | 0.0607
Epoch 107/300, trend Loss: 0.0730 | 0.0607
Epoch 108/300, trend Loss: 0.0729 | 0.0608
Epoch 109/300, trend Loss: 0.0728 | 0.0608
Epoch 110/300, trend Loss: 0.0726 | 0.0609
Epoch 111/300, trend Loss: 0.0725 | 0.0609
Epoch 112/300, trend Loss: 0.0724 | 0.0609
Epoch 113/300, trend Loss: 0.0722 | 0.0609
Epoch 114/300, trend Loss: 0.0721 | 0.0609
Epoch 115/300, trend Loss: 0.0720 | 0.0610
Epoch 116/300, trend Loss: 0.0718 | 0.0610
Epoch 117/300, trend Loss: 0.0717 | 0.0609
Epoch 118/300, trend Loss: 0.0716 | 0.0609
Epoch 119/300, trend Loss: 0.0714 | 0.0609
Epoch 120/300, trend Loss: 0.0713 | 0.0608
Epoch 121/300, trend Loss: 0.0712 | 0.0608
Epoch 122/300, trend Loss: 0.0711 | 0.0608
Epoch 123/300, trend Loss: 0.0710 | 0.0608
Epoch 124/300, trend Loss: 0.0709 | 0.0608
Epoch 125/300, trend Loss: 0.0708 | 0.0609
Epoch 126/300, trend Loss: 0.0707 | 0.0612
Epoch 127/300, trend Loss: 0.0707 | 0.0618
Epoch 128/300, trend Loss: 0.0707 | 0.0627
Epoch 129/300, trend Loss: 0.0707 | 0.0640
Epoch 130/300, trend Loss: 0.0707 | 0.0651
Epoch 131/300, trend Loss: 0.0706 | 0.0660
Epoch 132/300, trend Loss: 0.0704 | 0.0661
Epoch 133/300, trend Loss: 0.0702 | 0.0661
Epoch 134/300, trend Loss: 0.0701 | 0.0658
Epoch 135/300, trend Loss: 0.0699 | 0.0652
Epoch 136/300, trend Loss: 0.0698 | 0.0646
Epoch 137/300, trend Loss: 0.0697 | 0.0642
Epoch 138/300, trend Loss: 0.0696 | 0.0642
Epoch 139/300, trend Loss: 0.0695 | 0.0642
Epoch 140/300, trend Loss: 0.0694 | 0.0639
Epoch 141/300, trend Loss: 0.0693 | 0.0633
Epoch 142/300, trend Loss: 0.0692 | 0.0629
Epoch 143/300, trend Loss: 0.0691 | 0.0631
Epoch 144/300, trend Loss: 0.0689 | 0.0634
Epoch 145/300, trend Loss: 0.0688 | 0.0632
Epoch 146/300, trend Loss: 0.0686 | 0.0625
Epoch 147/300, trend Loss: 0.0685 | 0.0619
Epoch 148/300, trend Loss: 0.0683 | 0.0623
Epoch 149/300, trend Loss: 0.0681 | 0.0627
Epoch 150/300, trend Loss: 0.0679 | 0.0623
Epoch 151/300, trend Loss: 0.0676 | 0.0615
Epoch 152/300, trend Loss: 0.0674 | 0.0613
Epoch 153/300, trend Loss: 0.0672 | 0.0618
Epoch 154/300, trend Loss: 0.0668 | 0.0616
Epoch 155/300, trend Loss: 0.0665 | 0.0610
Epoch 156/300, trend Loss: 0.0662 | 0.0608
Epoch 157/300, trend Loss: 0.0659 | 0.0610
Epoch 158/300, trend Loss: 0.0655 | 0.0608
Epoch 159/300, trend Loss: 0.0650 | 0.0604
Epoch 160/300, trend Loss: 0.0645 | 0.0604
Epoch 161/300, trend Loss: 0.0639 | 0.0604
Epoch 162/300, trend Loss: 0.0632 | 0.0602
Epoch 163/300, trend Loss: 0.0622 | 0.0601
Epoch 164/300, trend Loss: 0.0614 | 0.0601
Epoch 165/300, trend Loss: 0.0603 | 0.0599
Epoch 166/300, trend Loss: 0.0592 | 0.0596
Epoch 167/300, trend Loss: 0.0588 | 0.0595
Epoch 168/300, trend Loss: 0.0585 | 0.0596
Epoch 169/300, trend Loss: 0.0581 | 0.0596
Epoch 170/300, trend Loss: 0.0626 | 0.0600
Epoch 171/300, trend Loss: 0.0584 | 0.0601
Epoch 172/300, trend Loss: 0.0578 | 0.0603
Epoch 173/300, trend Loss: 0.0576 | 0.0598
Epoch 174/300, trend Loss: 0.0595 | 0.0609
Epoch 175/300, trend Loss: 0.0576 | 0.0598
Epoch 176/300, trend Loss: 0.0571 | 0.0608
Epoch 177/300, trend Loss: 0.0571 | 0.0599
Epoch 178/300, trend Loss: 0.0570 | 0.0610
Epoch 179/300, trend Loss: 0.0567 | 0.0599
Epoch 180/300, trend Loss: 0.0566 | 0.0609
Epoch 181/300, trend Loss: 0.0566 | 0.0600
Epoch 182/300, trend Loss: 0.0563 | 0.0610
Epoch 183/300, trend Loss: 0.0564 | 0.0600
Epoch 184/300, trend Loss: 0.0559 | 0.0609
Epoch 185/300, trend Loss: 0.0563 | 0.0601
Epoch 186/300, trend Loss: 0.0557 | 0.0608
Epoch 187/300, trend Loss: 0.0559 | 0.0602
Epoch 188/300, trend Loss: 0.0554 | 0.0607
Epoch 189/300, trend Loss: 0.0556 | 0.0604
Epoch 190/300, trend Loss: 0.0553 | 0.0606
Epoch 191/300, trend Loss: 0.0555 | 0.0605
Epoch 192/300, trend Loss: 0.0551 | 0.0605
Epoch 193/300, trend Loss: 0.0553 | 0.0606
Epoch 194/300, trend Loss: 0.0549 | 0.0605
Epoch 195/300, trend Loss: 0.0551 | 0.0607
Epoch 196/300, trend Loss: 0.0547 | 0.0605
Epoch 197/300, trend Loss: 0.0549 | 0.0607
Epoch 198/300, trend Loss: 0.0546 | 0.0605
Epoch 199/300, trend Loss: 0.0547 | 0.0608
Epoch 200/300, trend Loss: 0.0544 | 0.0605
Epoch 201/300, trend Loss: 0.0545 | 0.0609
Epoch 202/300, trend Loss: 0.0542 | 0.0606
Epoch 203/300, trend Loss: 0.0543 | 0.0609
Epoch 204/300, trend Loss: 0.0541 | 0.0606
Epoch 205/300, trend Loss: 0.0542 | 0.0610
Epoch 206/300, trend Loss: 0.0539 | 0.0607
Epoch 207/300, trend Loss: 0.0540 | 0.0610
Epoch 208/300, trend Loss: 0.0537 | 0.0607
Epoch 209/300, trend Loss: 0.0538 | 0.0611
Epoch 210/300, trend Loss: 0.0536 | 0.0608
Epoch 211/300, trend Loss: 0.0537 | 0.0612
Epoch 212/300, trend Loss: 0.0534 | 0.0609
Epoch 213/300, trend Loss: 0.0535 | 0.0612
Epoch 214/300, trend Loss: 0.0533 | 0.0609
Epoch 215/300, trend Loss: 0.0533 | 0.0613
Epoch 216/300, trend Loss: 0.0531 | 0.0610
Epoch 217/300, trend Loss: 0.0532 | 0.0613
Epoch 218/300, trend Loss: 0.0529 | 0.0610
Epoch 219/300, trend Loss: 0.0530 | 0.0612
Epoch 220/300, trend Loss: 0.0528 | 0.0611
Epoch 221/300, trend Loss: 0.0528 | 0.0616
Epoch 222/300, trend Loss: 0.0526 | 0.0614
Epoch 223/300, trend Loss: 0.0527 | 0.0612
Epoch 224/300, trend Loss: 0.0525 | 0.0606
Epoch 225/300, trend Loss: 0.0526 | 0.0615
Epoch 226/300, trend Loss: 0.0523 | 0.0624
Epoch 227/300, trend Loss: 0.0524 | 0.0619
Epoch 228/300, trend Loss: 0.0522 | 0.0606
Epoch 229/300, trend Loss: 0.0526 | 0.0619
Epoch 230/300, trend Loss: 0.0523 | 0.0619
Epoch 231/300, trend Loss: 0.0521 | 0.0609
Epoch 232/300, trend Loss: 0.0520 | 0.0618
Epoch 233/300, trend Loss: 0.0520 | 0.0618
Epoch 234/300, trend Loss: 0.0517 | 0.0613
Epoch 235/300, trend Loss: 0.0518 | 0.0617
Epoch 236/300, trend Loss: 0.0516 | 0.0618
Epoch 237/300, trend Loss: 0.0516 | 0.0617
Epoch 238/300, trend Loss: 0.0514 | 0.0614
Epoch 239/300, trend Loss: 0.0515 | 0.0619
Epoch 240/300, trend Loss: 0.0513 | 0.0620
Epoch 241/300, trend Loss: 0.0513 | 0.0619
Epoch 242/300, trend Loss: 0.0512 | 0.0614
Epoch 243/300, trend Loss: 0.0513 | 0.0621
Epoch 244/300, trend Loss: 0.0511 | 0.0626
Epoch 245/300, trend Loss: 0.0511 | 0.0622
Epoch 246/300, trend Loss: 0.0509 | 0.0613
Epoch 247/300, trend Loss: 0.0511 | 0.0622
Epoch 248/300, trend Loss: 0.0509 | 0.0633
Epoch 249/300, trend Loss: 0.0509 | 0.0620
Epoch 250/300, trend Loss: 0.0508 | 0.0617
Epoch 251/300, trend Loss: 0.0508 | 0.0632
Epoch 252/300, trend Loss: 0.0506 | 0.0623
Epoch 253/300, trend Loss: 0.0505 | 0.0619
Epoch 254/300, trend Loss: 0.0506 | 0.0628
Epoch 255/300, trend Loss: 0.0505 | 0.0626
Epoch 256/300, trend Loss: 0.0503 | 0.0621
Epoch 257/300, trend Loss: 0.0504 | 0.0629
Epoch 258/300, trend Loss: 0.0502 | 0.0630
Epoch 259/300, trend Loss: 0.0501 | 0.0624
Epoch 260/300, trend Loss: 0.0501 | 0.0627
Epoch 261/300, trend Loss: 0.0501 | 0.0634
Epoch 262/300, trend Loss: 0.0499 | 0.0627
Epoch 263/300, trend Loss: 0.0499 | 0.0627
Epoch 264/300, trend Loss: 0.0498 | 0.0634
Epoch 265/300, trend Loss: 0.0498 | 0.0635
Epoch 266/300, trend Loss: 0.0497 | 0.0626
Epoch 267/300, trend Loss: 0.0498 | 0.0631
Epoch 268/300, trend Loss: 0.0496 | 0.0642
Epoch 269/300, trend Loss: 0.0496 | 0.0632
Epoch 270/300, trend Loss: 0.0495 | 0.0626
Epoch 271/300, trend Loss: 0.0495 | 0.0642
Epoch 272/300, trend Loss: 0.0494 | 0.0639
Epoch 273/300, trend Loss: 0.0493 | 0.0629
Epoch 274/300, trend Loss: 0.0494 | 0.0638
Epoch 275/300, trend Loss: 0.0494 | 0.0639
Epoch 276/300, trend Loss: 0.0491 | 0.0632
Epoch 277/300, trend Loss: 0.0492 | 0.0641
Epoch 278/300, trend Loss: 0.0490 | 0.0641
Epoch 279/300, trend Loss: 0.0489 | 0.0635
Epoch 280/300, trend Loss: 0.0489 | 0.0641
Epoch 281/300, trend Loss: 0.0489 | 0.0644
Epoch 282/300, trend Loss: 0.0487 | 0.0637
Epoch 283/300, trend Loss: 0.0488 | 0.0643
Epoch 284/300, trend Loss: 0.0487 | 0.0649
Epoch 285/300, trend Loss: 0.0486 | 0.0640
Epoch 286/300, trend Loss: 0.0486 | 0.0641
Epoch 287/300, trend Loss: 0.0486 | 0.0653
Epoch 288/300, trend Loss: 0.0484 | 0.0644
Epoch 289/300, trend Loss: 0.0484 | 0.0640
Epoch 290/300, trend Loss: 0.0484 | 0.0653
Epoch 291/300, trend Loss: 0.0483 | 0.0652
Epoch 292/300, trend Loss: 0.0482 | 0.0641
Epoch 293/300, trend Loss: 0.0483 | 0.0649
Epoch 294/300, trend Loss: 0.0482 | 0.0658
Epoch 295/300, trend Loss: 0.0480 | 0.0644
Epoch 296/300, trend Loss: 0.0481 | 0.0650
Epoch 297/300, trend Loss: 0.0481 | 0.0654
Epoch 298/300, trend Loss: 0.0478 | 0.0647
Epoch 299/300, trend Loss: 0.0479 | 0.0655
Epoch 300/300, trend Loss: 0.0478 | 0.0655
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.6535166838932545, 'learning_rate': 0.0009865596345313245, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8003077589058271}
Epoch 1/300, seasonal_0 Loss: 0.3558 | 0.1997
Epoch 2/300, seasonal_0 Loss: 0.1495 | 0.2195
Epoch 3/300, seasonal_0 Loss: 0.1327 | 0.1408
Epoch 4/300, seasonal_0 Loss: 0.1327 | 0.2098
Epoch 5/300, seasonal_0 Loss: 0.1326 | 0.2080
Epoch 6/300, seasonal_0 Loss: 0.1313 | 0.2064
Epoch 7/300, seasonal_0 Loss: 0.1290 | 0.2341
Epoch 8/300, seasonal_0 Loss: 0.1118 | 0.1110
Epoch 9/300, seasonal_0 Loss: 0.1035 | 0.1197
Epoch 10/300, seasonal_0 Loss: 0.1073 | 0.1456
Epoch 11/300, seasonal_0 Loss: 0.1041 | 0.1004
Epoch 12/300, seasonal_0 Loss: 0.1043 | 0.1374
Epoch 13/300, seasonal_0 Loss: 0.1007 | 0.0849
Epoch 14/300, seasonal_0 Loss: 0.0994 | 0.1147
Epoch 15/300, seasonal_0 Loss: 0.0973 | 0.0929
Epoch 16/300, seasonal_0 Loss: 0.0969 | 0.1151
Epoch 17/300, seasonal_0 Loss: 0.0941 | 0.0938
Epoch 18/300, seasonal_0 Loss: 0.0951 | 0.1097
Epoch 19/300, seasonal_0 Loss: 0.0919 | 0.0904
Epoch 20/300, seasonal_0 Loss: 0.0933 | 0.1086
Epoch 21/300, seasonal_0 Loss: 0.0909 | 0.0939
Epoch 22/300, seasonal_0 Loss: 0.0909 | 0.0948
Epoch 23/300, seasonal_0 Loss: 0.0893 | 0.0934
Epoch 24/300, seasonal_0 Loss: 0.0920 | 0.0874
Epoch 25/300, seasonal_0 Loss: 0.0879 | 0.0896
Epoch 26/300, seasonal_0 Loss: 0.0891 | 0.0984
Epoch 27/300, seasonal_0 Loss: 0.0874 | 0.0825
Epoch 28/300, seasonal_0 Loss: 0.0866 | 0.0872
Epoch 29/300, seasonal_0 Loss: 0.0850 | 0.0890
Epoch 30/300, seasonal_0 Loss: 0.0844 | 0.0793
Epoch 31/300, seasonal_0 Loss: 0.0836 | 0.0850
Epoch 32/300, seasonal_0 Loss: 0.0835 | 0.0817
Epoch 33/300, seasonal_0 Loss: 0.0828 | 0.0842
Epoch 34/300, seasonal_0 Loss: 0.0835 | 0.0866
Epoch 35/300, seasonal_0 Loss: 0.0825 | 0.0850
Epoch 36/300, seasonal_0 Loss: 0.0816 | 0.0737
Epoch 37/300, seasonal_0 Loss: 0.0803 | 0.0783
Epoch 38/300, seasonal_0 Loss: 0.0799 | 0.0732
Epoch 39/300, seasonal_0 Loss: 0.0795 | 0.0828
Epoch 40/300, seasonal_0 Loss: 0.0798 | 0.0752
Epoch 41/300, seasonal_0 Loss: 0.0794 | 0.0876
Epoch 42/300, seasonal_0 Loss: 0.0801 | 0.0771
Epoch 43/300, seasonal_0 Loss: 0.0779 | 0.0776
Epoch 44/300, seasonal_0 Loss: 0.0765 | 0.0710
Epoch 45/300, seasonal_0 Loss: 0.0759 | 0.0763
Epoch 46/300, seasonal_0 Loss: 0.0756 | 0.0825
Epoch 47/300, seasonal_0 Loss: 0.0756 | 0.0764
Epoch 48/300, seasonal_0 Loss: 0.0754 | 0.0737
Epoch 49/300, seasonal_0 Loss: 0.0753 | 0.0853
Epoch 50/300, seasonal_0 Loss: 0.0741 | 0.0702
Epoch 51/300, seasonal_0 Loss: 0.0733 | 0.0728
Epoch 52/300, seasonal_0 Loss: 0.0728 | 0.0712
Epoch 53/300, seasonal_0 Loss: 0.0720 | 0.0730
Epoch 54/300, seasonal_0 Loss: 0.0713 | 0.0709
Epoch 55/300, seasonal_0 Loss: 0.0709 | 0.0766
Epoch 56/300, seasonal_0 Loss: 0.0708 | 0.0725
Epoch 57/300, seasonal_0 Loss: 0.0709 | 0.0767
Epoch 58/300, seasonal_0 Loss: 0.0706 | 0.0722
Epoch 59/300, seasonal_0 Loss: 0.0697 | 0.0795
Epoch 60/300, seasonal_0 Loss: 0.0706 | 0.0729
Epoch 61/300, seasonal_0 Loss: 0.0708 | 0.0787
Epoch 62/300, seasonal_0 Loss: 0.0700 | 0.0711
Epoch 63/300, seasonal_0 Loss: 0.0664 | 0.0798
Epoch 64/300, seasonal_0 Loss: 0.0718 | 0.0686
Epoch 65/300, seasonal_0 Loss: 0.0705 | 0.0691
Epoch 66/300, seasonal_0 Loss: 0.0657 | 0.0678
Epoch 67/300, seasonal_0 Loss: 0.0702 | 0.0711
Epoch 68/300, seasonal_0 Loss: 0.0658 | 0.0697
Epoch 69/300, seasonal_0 Loss: 0.0690 | 0.0730
Epoch 70/300, seasonal_0 Loss: 0.0679 | 0.0730
Epoch 71/300, seasonal_0 Loss: 0.0699 | 0.0723
Epoch 72/300, seasonal_0 Loss: 0.0639 | 0.0698
Epoch 73/300, seasonal_0 Loss: 0.0648 | 0.0715
Epoch 74/300, seasonal_0 Loss: 0.0638 | 0.0709
Epoch 75/300, seasonal_0 Loss: 0.0622 | 0.0707
Epoch 76/300, seasonal_0 Loss: 0.0630 | 0.0702
Epoch 77/300, seasonal_0 Loss: 0.0605 | 0.0699
Epoch 78/300, seasonal_0 Loss: 0.0606 | 0.0692
Epoch 79/300, seasonal_0 Loss: 0.0623 | 0.0735
Epoch 80/300, seasonal_0 Loss: 0.0632 | 0.0699
Epoch 81/300, seasonal_0 Loss: 0.0610 | 0.0716
Epoch 82/300, seasonal_0 Loss: 0.0617 | 0.0694
Epoch 83/300, seasonal_0 Loss: 0.0594 | 0.0693
Epoch 84/300, seasonal_0 Loss: 0.0621 | 0.0701
Epoch 85/300, seasonal_0 Loss: 0.0605 | 0.0698
Epoch 86/300, seasonal_0 Loss: 0.0593 | 0.0685
Epoch 87/300, seasonal_0 Loss: 0.0585 | 0.0683
Epoch 88/300, seasonal_0 Loss: 0.0579 | 0.0679
Epoch 89/300, seasonal_0 Loss: 0.0575 | 0.0677
Epoch 90/300, seasonal_0 Loss: 0.0572 | 0.0675
Epoch 91/300, seasonal_0 Loss: 0.0570 | 0.0675
Epoch 92/300, seasonal_0 Loss: 0.0568 | 0.0677
Epoch 93/300, seasonal_0 Loss: 0.0567 | 0.0677
Epoch 94/300, seasonal_0 Loss: 0.0565 | 0.0677
Epoch 95/300, seasonal_0 Loss: 0.0563 | 0.0678
Epoch 96/300, seasonal_0 Loss: 0.0562 | 0.0678
Epoch 97/300, seasonal_0 Loss: 0.0560 | 0.0679
Epoch 98/300, seasonal_0 Loss: 0.0559 | 0.0680
Epoch 99/300, seasonal_0 Loss: 0.0558 | 0.0682
Epoch 100/300, seasonal_0 Loss: 0.0558 | 0.0682
Epoch 101/300, seasonal_0 Loss: 0.0557 | 0.0682
Epoch 102/300, seasonal_0 Loss: 0.0556 | 0.0683
Epoch 103/300, seasonal_0 Loss: 0.0555 | 0.0684
Epoch 104/300, seasonal_0 Loss: 0.0554 | 0.0685
Epoch 105/300, seasonal_0 Loss: 0.0553 | 0.0685
Epoch 106/300, seasonal_0 Loss: 0.0553 | 0.0687
Epoch 107/300, seasonal_0 Loss: 0.0553 | 0.0688
Epoch 108/300, seasonal_0 Loss: 0.0552 | 0.0689
Epoch 109/300, seasonal_0 Loss: 0.0551 | 0.0689
Epoch 110/300, seasonal_0 Loss: 0.0550 | 0.0690
Epoch 111/300, seasonal_0 Loss: 0.0549 | 0.0690
Epoch 112/300, seasonal_0 Loss: 0.0549 | 0.0691
Epoch 113/300, seasonal_0 Loss: 0.0547 | 0.0692
Epoch 114/300, seasonal_0 Loss: 0.0547 | 0.0694
Epoch 115/300, seasonal_0 Loss: 0.0546 | 0.0692
Epoch 116/300, seasonal_0 Loss: 0.0545 | 0.0694
Epoch 117/300, seasonal_0 Loss: 0.0544 | 0.0692
Epoch 118/300, seasonal_0 Loss: 0.0542 | 0.0693
Epoch 119/300, seasonal_0 Loss: 0.0541 | 0.0691
Epoch 120/300, seasonal_0 Loss: 0.0540 | 0.0692
Epoch 121/300, seasonal_0 Loss: 0.0539 | 0.0692
Epoch 122/300, seasonal_0 Loss: 0.0538 | 0.0693
Epoch 123/300, seasonal_0 Loss: 0.0537 | 0.0693
Epoch 124/300, seasonal_0 Loss: 0.0536 | 0.0694
Epoch 125/300, seasonal_0 Loss: 0.0535 | 0.0694
Epoch 126/300, seasonal_0 Loss: 0.0535 | 0.0695
Epoch 127/300, seasonal_0 Loss: 0.0534 | 0.0696
Epoch 128/300, seasonal_0 Loss: 0.0533 | 0.0697
Epoch 129/300, seasonal_0 Loss: 0.0532 | 0.0697
Epoch 130/300, seasonal_0 Loss: 0.0532 | 0.0698
Epoch 131/300, seasonal_0 Loss: 0.0531 | 0.0698
Epoch 132/300, seasonal_0 Loss: 0.0531 | 0.0698
Epoch 133/300, seasonal_0 Loss: 0.0530 | 0.0698
Epoch 134/300, seasonal_0 Loss: 0.0529 | 0.0699
Epoch 135/300, seasonal_0 Loss: 0.0529 | 0.0700
Epoch 136/300, seasonal_0 Loss: 0.0528 | 0.0700
Epoch 137/300, seasonal_0 Loss: 0.0528 | 0.0700
Epoch 138/300, seasonal_0 Loss: 0.0527 | 0.0700
Epoch 139/300, seasonal_0 Loss: 0.0527 | 0.0700
Epoch 140/300, seasonal_0 Loss: 0.0526 | 0.0701
Epoch 141/300, seasonal_0 Loss: 0.0526 | 0.0701
Epoch 142/300, seasonal_0 Loss: 0.0525 | 0.0701
Epoch 143/300, seasonal_0 Loss: 0.0525 | 0.0702
Epoch 144/300, seasonal_0 Loss: 0.0525 | 0.0702
Epoch 145/300, seasonal_0 Loss: 0.0524 | 0.0702
Epoch 146/300, seasonal_0 Loss: 0.0524 | 0.0703
Epoch 147/300, seasonal_0 Loss: 0.0523 | 0.0703
Epoch 148/300, seasonal_0 Loss: 0.0523 | 0.0703
Epoch 149/300, seasonal_0 Loss: 0.0523 | 0.0704
Epoch 150/300, seasonal_0 Loss: 0.0522 | 0.0704
Epoch 151/300, seasonal_0 Loss: 0.0522 | 0.0705
Epoch 152/300, seasonal_0 Loss: 0.0522 | 0.0705
Epoch 153/300, seasonal_0 Loss: 0.0521 | 0.0706
Epoch 154/300, seasonal_0 Loss: 0.0521 | 0.0706
Epoch 155/300, seasonal_0 Loss: 0.0521 | 0.0706
Epoch 156/300, seasonal_0 Loss: 0.0520 | 0.0707
Epoch 157/300, seasonal_0 Loss: 0.0520 | 0.0708
Epoch 158/300, seasonal_0 Loss: 0.0520 | 0.0708
Epoch 159/300, seasonal_0 Loss: 0.0519 | 0.0709
Epoch 160/300, seasonal_0 Loss: 0.0519 | 0.0709
Epoch 161/300, seasonal_0 Loss: 0.0519 | 0.0710
Epoch 162/300, seasonal_0 Loss: 0.0518 | 0.0709
Epoch 163/300, seasonal_0 Loss: 0.0518 | 0.0709
Epoch 164/300, seasonal_0 Loss: 0.0518 | 0.0710
Epoch 165/300, seasonal_0 Loss: 0.0518 | 0.0710
Epoch 166/300, seasonal_0 Loss: 0.0518 | 0.0710
Epoch 167/300, seasonal_0 Loss: 0.0517 | 0.0710
Epoch 168/300, seasonal_0 Loss: 0.0517 | 0.0710
Epoch 169/300, seasonal_0 Loss: 0.0517 | 0.0711
Epoch 170/300, seasonal_0 Loss: 0.0517 | 0.0710
Epoch 171/300, seasonal_0 Loss: 0.0516 | 0.0710
Epoch 172/300, seasonal_0 Loss: 0.0516 | 0.0710
Epoch 173/300, seasonal_0 Loss: 0.0516 | 0.0710
Epoch 174/300, seasonal_0 Loss: 0.0516 | 0.0710
Epoch 175/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 176/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 177/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 178/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 179/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 180/300, seasonal_0 Loss: 0.0515 | 0.0710
Epoch 181/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 182/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 183/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 184/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 185/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 186/300, seasonal_0 Loss: 0.0514 | 0.0710
Epoch 187/300, seasonal_0 Loss: 0.0513 | 0.0710
Epoch 188/300, seasonal_0 Loss: 0.0513 | 0.0710
Epoch 189/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 190/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 191/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 192/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 193/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 194/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 195/300, seasonal_0 Loss: 0.0513 | 0.0711
Epoch 196/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 197/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 198/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 199/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 200/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 201/300, seasonal_0 Loss: 0.0512 | 0.0711
Epoch 202/300, seasonal_0 Loss: 0.0512 | 0.0712
Epoch 203/300, seasonal_0 Loss: 0.0512 | 0.0712
Epoch 204/300, seasonal_0 Loss: 0.0512 | 0.0712
Epoch 205/300, seasonal_0 Loss: 0.0512 | 0.0712
Epoch 206/300, seasonal_0 Loss: 0.0512 | 0.0712
Epoch 207/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 208/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 209/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 210/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 211/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 212/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 213/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 214/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 215/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 216/300, seasonal_0 Loss: 0.0511 | 0.0712
Epoch 217/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 218/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 219/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 220/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 221/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 222/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 223/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 224/300, seasonal_0 Loss: 0.0511 | 0.0713
Epoch 225/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 226/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 227/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 228/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 229/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 230/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 231/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 232/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 233/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 234/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 235/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 236/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 237/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 238/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 239/300, seasonal_0 Loss: 0.0510 | 0.0713
Epoch 240/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 241/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 242/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 243/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 244/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 245/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 246/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 247/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 248/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 249/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 250/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 251/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 252/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 253/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 254/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 255/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 256/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 257/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 258/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 259/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 260/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 261/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 262/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 263/300, seasonal_0 Loss: 0.0510 | 0.0714
Epoch 264/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 265/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 266/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 267/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 268/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 269/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 270/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 271/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 272/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 273/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 274/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 275/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 276/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 277/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 278/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 279/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 280/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 281/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 282/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 283/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 284/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 285/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 286/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 287/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 288/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 289/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 290/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 291/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 292/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 293/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 294/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 295/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 296/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 297/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 298/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 299/300, seasonal_0 Loss: 0.0509 | 0.0714
Epoch 300/300, seasonal_0 Loss: 0.0509 | 0.0714
Training seasonal_1 component with params: {'observation_period_num': 18, 'train_rates': 0.9899559636877091, 'learning_rate': 0.0001484242418827821, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9751444067087647}
Epoch 1/300, seasonal_1 Loss: 0.3988 | 0.2022
Epoch 2/300, seasonal_1 Loss: 0.2127 | 0.1597
Epoch 3/300, seasonal_1 Loss: 0.1787 | 0.1445
Epoch 4/300, seasonal_1 Loss: 0.1810 | 0.1407
Epoch 5/300, seasonal_1 Loss: 0.1945 | 0.1320
Epoch 6/300, seasonal_1 Loss: 0.2163 | 0.1419
Epoch 7/300, seasonal_1 Loss: 0.2289 | 0.3883
Epoch 8/300, seasonal_1 Loss: 0.1769 | 0.1501
Epoch 9/300, seasonal_1 Loss: 0.2050 | 0.1182
Epoch 10/300, seasonal_1 Loss: 0.2326 | 0.1131
Epoch 11/300, seasonal_1 Loss: 0.1582 | 0.1033
Epoch 12/300, seasonal_1 Loss: 0.1285 | 0.0966
Epoch 13/300, seasonal_1 Loss: 0.1217 | 0.0904
Epoch 14/300, seasonal_1 Loss: 0.1212 | 0.0850
Epoch 15/300, seasonal_1 Loss: 0.1189 | 0.0817
Epoch 16/300, seasonal_1 Loss: 0.1157 | 0.0794
Epoch 17/300, seasonal_1 Loss: 0.1132 | 0.0776
Epoch 18/300, seasonal_1 Loss: 0.1116 | 0.0762
Epoch 19/300, seasonal_1 Loss: 0.1104 | 0.0750
Epoch 20/300, seasonal_1 Loss: 0.1091 | 0.0741
Epoch 21/300, seasonal_1 Loss: 0.1078 | 0.0732
Epoch 22/300, seasonal_1 Loss: 0.1063 | 0.0724
Epoch 23/300, seasonal_1 Loss: 0.1050 | 0.0716
Epoch 24/300, seasonal_1 Loss: 0.1039 | 0.0710
Epoch 25/300, seasonal_1 Loss: 0.1029 | 0.0706
Epoch 26/300, seasonal_1 Loss: 0.1021 | 0.0702
Epoch 27/300, seasonal_1 Loss: 0.1013 | 0.0700
Epoch 28/300, seasonal_1 Loss: 0.1007 | 0.0699
Epoch 29/300, seasonal_1 Loss: 0.1001 | 0.0698
Epoch 30/300, seasonal_1 Loss: 0.0995 | 0.0696
Epoch 31/300, seasonal_1 Loss: 0.0989 | 0.0694
Epoch 32/300, seasonal_1 Loss: 0.0983 | 0.0690
Epoch 33/300, seasonal_1 Loss: 0.0977 | 0.0687
Epoch 34/300, seasonal_1 Loss: 0.0975 | 0.0684
Epoch 35/300, seasonal_1 Loss: 0.0979 | 0.0687
Epoch 36/300, seasonal_1 Loss: 0.0989 | 0.0716
Epoch 37/300, seasonal_1 Loss: 0.0996 | 0.0782
Epoch 38/300, seasonal_1 Loss: 0.0985 | 0.0806
Epoch 39/300, seasonal_1 Loss: 0.0961 | 0.0742
Epoch 40/300, seasonal_1 Loss: 0.0948 | 0.0678
Epoch 41/300, seasonal_1 Loss: 0.0960 | 0.0666
Epoch 42/300, seasonal_1 Loss: 0.0968 | 0.0686
Epoch 43/300, seasonal_1 Loss: 0.0947 | 0.0665
Epoch 44/300, seasonal_1 Loss: 0.0925 | 0.0653
Epoch 45/300, seasonal_1 Loss: 0.0917 | 0.0651
Epoch 46/300, seasonal_1 Loss: 0.0915 | 0.0651
Epoch 47/300, seasonal_1 Loss: 0.0913 | 0.0649
Epoch 48/300, seasonal_1 Loss: 0.0911 | 0.0647
Epoch 49/300, seasonal_1 Loss: 0.0906 | 0.0644
Epoch 50/300, seasonal_1 Loss: 0.0900 | 0.0641
Epoch 51/300, seasonal_1 Loss: 0.0895 | 0.0640
Epoch 52/300, seasonal_1 Loss: 0.0897 | 0.0646
Epoch 53/300, seasonal_1 Loss: 0.0906 | 0.0660
Epoch 54/300, seasonal_1 Loss: 0.0908 | 0.0664
Epoch 55/300, seasonal_1 Loss: 0.0897 | 0.0650
Epoch 56/300, seasonal_1 Loss: 0.0882 | 0.0632
Epoch 57/300, seasonal_1 Loss: 0.0874 | 0.0623
Epoch 58/300, seasonal_1 Loss: 0.0874 | 0.0621
Epoch 59/300, seasonal_1 Loss: 0.0875 | 0.0621
Epoch 60/300, seasonal_1 Loss: 0.0872 | 0.0619
Epoch 61/300, seasonal_1 Loss: 0.0867 | 0.0615
Epoch 62/300, seasonal_1 Loss: 0.0861 | 0.0612
Epoch 63/300, seasonal_1 Loss: 0.0858 | 0.0612
Epoch 64/300, seasonal_1 Loss: 0.0856 | 0.0613
Epoch 65/300, seasonal_1 Loss: 0.0854 | 0.0612
Epoch 66/300, seasonal_1 Loss: 0.0851 | 0.0607
Epoch 67/300, seasonal_1 Loss: 0.0847 | 0.0601
Epoch 68/300, seasonal_1 Loss: 0.0843 | 0.0596
Epoch 69/300, seasonal_1 Loss: 0.0841 | 0.0592
Epoch 70/300, seasonal_1 Loss: 0.0839 | 0.0590
Epoch 71/300, seasonal_1 Loss: 0.0837 | 0.0587
Epoch 72/300, seasonal_1 Loss: 0.0834 | 0.0585
Epoch 73/300, seasonal_1 Loss: 0.0831 | 0.0583
Epoch 74/300, seasonal_1 Loss: 0.0829 | 0.0581
Epoch 75/300, seasonal_1 Loss: 0.0826 | 0.0578
Epoch 76/300, seasonal_1 Loss: 0.0823 | 0.0576
Epoch 77/300, seasonal_1 Loss: 0.0820 | 0.0573
Epoch 78/300, seasonal_1 Loss: 0.0818 | 0.0569
Epoch 79/300, seasonal_1 Loss: 0.0815 | 0.0566
Epoch 80/300, seasonal_1 Loss: 0.0813 | 0.0563
Epoch 81/300, seasonal_1 Loss: 0.0811 | 0.0560
Epoch 82/300, seasonal_1 Loss: 0.0809 | 0.0557
Epoch 83/300, seasonal_1 Loss: 0.0807 | 0.0554
Epoch 84/300, seasonal_1 Loss: 0.0805 | 0.0551
Epoch 85/300, seasonal_1 Loss: 0.0803 | 0.0549
Epoch 86/300, seasonal_1 Loss: 0.0801 | 0.0546
Epoch 87/300, seasonal_1 Loss: 0.0799 | 0.0543
Epoch 88/300, seasonal_1 Loss: 0.0797 | 0.0540
Epoch 89/300, seasonal_1 Loss: 0.0796 | 0.0537
Epoch 90/300, seasonal_1 Loss: 0.0794 | 0.0534
Epoch 91/300, seasonal_1 Loss: 0.0793 | 0.0532
Epoch 92/300, seasonal_1 Loss: 0.0791 | 0.0529
Epoch 93/300, seasonal_1 Loss: 0.0790 | 0.0527
Epoch 94/300, seasonal_1 Loss: 0.0789 | 0.0525
Epoch 95/300, seasonal_1 Loss: 0.0787 | 0.0523
Epoch 96/300, seasonal_1 Loss: 0.0786 | 0.0521
Epoch 97/300, seasonal_1 Loss: 0.0785 | 0.0519
Epoch 98/300, seasonal_1 Loss: 0.0784 | 0.0517
Epoch 99/300, seasonal_1 Loss: 0.0783 | 0.0516
Epoch 100/300, seasonal_1 Loss: 0.0782 | 0.0514
Epoch 101/300, seasonal_1 Loss: 0.0780 | 0.0513
Epoch 102/300, seasonal_1 Loss: 0.0779 | 0.0511
Epoch 103/300, seasonal_1 Loss: 0.0778 | 0.0510
Epoch 104/300, seasonal_1 Loss: 0.0777 | 0.0508
Epoch 105/300, seasonal_1 Loss: 0.0776 | 0.0507
Epoch 106/300, seasonal_1 Loss: 0.0775 | 0.0505
Epoch 107/300, seasonal_1 Loss: 0.0774 | 0.0504
Epoch 108/300, seasonal_1 Loss: 0.0774 | 0.0502
Epoch 109/300, seasonal_1 Loss: 0.0773 | 0.0501
Epoch 110/300, seasonal_1 Loss: 0.0772 | 0.0500
Epoch 111/300, seasonal_1 Loss: 0.0771 | 0.0498
Epoch 112/300, seasonal_1 Loss: 0.0770 | 0.0497
Epoch 113/300, seasonal_1 Loss: 0.0769 | 0.0496
Epoch 114/300, seasonal_1 Loss: 0.0769 | 0.0495
Epoch 115/300, seasonal_1 Loss: 0.0768 | 0.0494
Epoch 116/300, seasonal_1 Loss: 0.0767 | 0.0492
Epoch 117/300, seasonal_1 Loss: 0.0766 | 0.0491
Epoch 118/300, seasonal_1 Loss: 0.0766 | 0.0490
Epoch 119/300, seasonal_1 Loss: 0.0765 | 0.0489
Epoch 120/300, seasonal_1 Loss: 0.0764 | 0.0488
Epoch 121/300, seasonal_1 Loss: 0.0764 | 0.0487
Epoch 122/300, seasonal_1 Loss: 0.0763 | 0.0486
Epoch 123/300, seasonal_1 Loss: 0.0762 | 0.0485
Epoch 124/300, seasonal_1 Loss: 0.0762 | 0.0484
Epoch 125/300, seasonal_1 Loss: 0.0761 | 0.0483
Epoch 126/300, seasonal_1 Loss: 0.0760 | 0.0482
Epoch 127/300, seasonal_1 Loss: 0.0760 | 0.0481
Epoch 128/300, seasonal_1 Loss: 0.0759 | 0.0481
Epoch 129/300, seasonal_1 Loss: 0.0759 | 0.0480
Epoch 130/300, seasonal_1 Loss: 0.0758 | 0.0479
Epoch 131/300, seasonal_1 Loss: 0.0758 | 0.0478
Epoch 132/300, seasonal_1 Loss: 0.0757 | 0.0477
Epoch 133/300, seasonal_1 Loss: 0.0757 | 0.0476
Epoch 134/300, seasonal_1 Loss: 0.0756 | 0.0476
Epoch 135/300, seasonal_1 Loss: 0.0756 | 0.0475
Epoch 136/300, seasonal_1 Loss: 0.0755 | 0.0474
Epoch 137/300, seasonal_1 Loss: 0.0755 | 0.0473
Epoch 138/300, seasonal_1 Loss: 0.0754 | 0.0473
Epoch 139/300, seasonal_1 Loss: 0.0754 | 0.0472
Epoch 140/300, seasonal_1 Loss: 0.0754 | 0.0471
Epoch 141/300, seasonal_1 Loss: 0.0753 | 0.0471
Epoch 142/300, seasonal_1 Loss: 0.0753 | 0.0470
Epoch 143/300, seasonal_1 Loss: 0.0752 | 0.0470
Epoch 144/300, seasonal_1 Loss: 0.0752 | 0.0469
Epoch 145/300, seasonal_1 Loss: 0.0752 | 0.0468
Epoch 146/300, seasonal_1 Loss: 0.0751 | 0.0468
Epoch 147/300, seasonal_1 Loss: 0.0751 | 0.0467
Epoch 148/300, seasonal_1 Loss: 0.0750 | 0.0467
Epoch 149/300, seasonal_1 Loss: 0.0750 | 0.0466
Epoch 150/300, seasonal_1 Loss: 0.0750 | 0.0466
Epoch 151/300, seasonal_1 Loss: 0.0749 | 0.0465
Epoch 152/300, seasonal_1 Loss: 0.0749 | 0.0465
Epoch 153/300, seasonal_1 Loss: 0.0749 | 0.0464
Epoch 154/300, seasonal_1 Loss: 0.0748 | 0.0464
Epoch 155/300, seasonal_1 Loss: 0.0748 | 0.0463
Epoch 156/300, seasonal_1 Loss: 0.0748 | 0.0463
Epoch 157/300, seasonal_1 Loss: 0.0748 | 0.0462
Epoch 158/300, seasonal_1 Loss: 0.0747 | 0.0462
Epoch 159/300, seasonal_1 Loss: 0.0747 | 0.0461
Epoch 160/300, seasonal_1 Loss: 0.0747 | 0.0461
Epoch 161/300, seasonal_1 Loss: 0.0747 | 0.0461
Epoch 162/300, seasonal_1 Loss: 0.0746 | 0.0460
Epoch 163/300, seasonal_1 Loss: 0.0746 | 0.0460
Epoch 164/300, seasonal_1 Loss: 0.0746 | 0.0459
Epoch 165/300, seasonal_1 Loss: 0.0746 | 0.0459
Epoch 166/300, seasonal_1 Loss: 0.0745 | 0.0459
Epoch 167/300, seasonal_1 Loss: 0.0745 | 0.0458
Epoch 168/300, seasonal_1 Loss: 0.0745 | 0.0458
Epoch 169/300, seasonal_1 Loss: 0.0745 | 0.0458
Epoch 170/300, seasonal_1 Loss: 0.0744 | 0.0457
Epoch 171/300, seasonal_1 Loss: 0.0744 | 0.0457
Epoch 172/300, seasonal_1 Loss: 0.0744 | 0.0457
Epoch 173/300, seasonal_1 Loss: 0.0744 | 0.0456
Epoch 174/300, seasonal_1 Loss: 0.0744 | 0.0456
Epoch 175/300, seasonal_1 Loss: 0.0743 | 0.0456
Epoch 176/300, seasonal_1 Loss: 0.0743 | 0.0456
Epoch 177/300, seasonal_1 Loss: 0.0743 | 0.0455
Epoch 178/300, seasonal_1 Loss: 0.0743 | 0.0455
Epoch 179/300, seasonal_1 Loss: 0.0743 | 0.0455
Epoch 180/300, seasonal_1 Loss: 0.0742 | 0.0455
Epoch 181/300, seasonal_1 Loss: 0.0742 | 0.0454
Epoch 182/300, seasonal_1 Loss: 0.0742 | 0.0454
Epoch 183/300, seasonal_1 Loss: 0.0742 | 0.0454
Epoch 184/300, seasonal_1 Loss: 0.0742 | 0.0454
Epoch 185/300, seasonal_1 Loss: 0.0742 | 0.0453
Epoch 186/300, seasonal_1 Loss: 0.0742 | 0.0453
Epoch 187/300, seasonal_1 Loss: 0.0741 | 0.0453
Epoch 188/300, seasonal_1 Loss: 0.0741 | 0.0453
Epoch 189/300, seasonal_1 Loss: 0.0741 | 0.0453
Epoch 190/300, seasonal_1 Loss: 0.0741 | 0.0452
Epoch 191/300, seasonal_1 Loss: 0.0741 | 0.0452
Epoch 192/300, seasonal_1 Loss: 0.0741 | 0.0452
Epoch 193/300, seasonal_1 Loss: 0.0741 | 0.0452
Epoch 194/300, seasonal_1 Loss: 0.0740 | 0.0452
Epoch 195/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 196/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 197/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 198/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 199/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 200/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 201/300, seasonal_1 Loss: 0.0740 | 0.0451
Epoch 202/300, seasonal_1 Loss: 0.0740 | 0.0450
Epoch 203/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 204/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 205/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 206/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 207/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 208/300, seasonal_1 Loss: 0.0739 | 0.0450
Epoch 209/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 210/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 211/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 212/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 213/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 214/300, seasonal_1 Loss: 0.0738 | 0.0449
Epoch 215/300, seasonal_1 Loss: 0.0738 | 0.0449
Epoch 216/300, seasonal_1 Loss: 0.0738 | 0.0449
Epoch 217/300, seasonal_1 Loss: 0.0738 | 0.0449
Epoch 218/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 219/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 220/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 221/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 222/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 223/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 224/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 225/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 226/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 227/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 228/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 229/300, seasonal_1 Loss: 0.0738 | 0.0448
Epoch 230/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 231/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 232/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 233/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 234/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 235/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 236/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 237/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 238/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 239/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 240/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 241/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 242/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 243/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 244/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 245/300, seasonal_1 Loss: 0.0737 | 0.0447
Epoch 246/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 247/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 248/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 249/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 250/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 251/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 252/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 253/300, seasonal_1 Loss: 0.0737 | 0.0446
Epoch 254/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 255/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 256/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 257/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 258/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 259/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 260/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 261/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 262/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 263/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 264/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 265/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 266/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 267/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 268/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 269/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 270/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 271/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 272/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 273/300, seasonal_1 Loss: 0.0736 | 0.0446
Epoch 274/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 275/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 276/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 277/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 278/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 279/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 280/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 281/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 282/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 283/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 284/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 285/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 286/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 287/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 288/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 289/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 290/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 291/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 292/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 293/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 294/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 295/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 296/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 297/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 298/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 299/300, seasonal_1 Loss: 0.0736 | 0.0445
Epoch 300/300, seasonal_1 Loss: 0.0736 | 0.0445
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.6987359743281827, 'learning_rate': 8.411452895580493e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.7503102330102386}
Epoch 1/300, seasonal_2 Loss: 0.3785 | 0.2213
Epoch 2/300, seasonal_2 Loss: 0.1628 | 0.1481
Epoch 3/300, seasonal_2 Loss: 0.1476 | 0.1344
Epoch 4/300, seasonal_2 Loss: 0.1399 | 0.1262
Epoch 5/300, seasonal_2 Loss: 0.1326 | 0.1170
Epoch 6/300, seasonal_2 Loss: 0.1288 | 0.1147
Epoch 7/300, seasonal_2 Loss: 0.1256 | 0.1142
Epoch 8/300, seasonal_2 Loss: 0.1229 | 0.1153
Epoch 9/300, seasonal_2 Loss: 0.1219 | 0.1148
Epoch 10/300, seasonal_2 Loss: 0.1202 | 0.1145
Epoch 11/300, seasonal_2 Loss: 0.1185 | 0.1139
Epoch 12/300, seasonal_2 Loss: 0.1173 | 0.1141
Epoch 13/300, seasonal_2 Loss: 0.1162 | 0.1128
Epoch 14/300, seasonal_2 Loss: 0.1142 | 0.1105
Epoch 15/300, seasonal_2 Loss: 0.1127 | 0.1070
Epoch 16/300, seasonal_2 Loss: 0.1115 | 0.1052
Epoch 17/300, seasonal_2 Loss: 0.1102 | 0.1031
Epoch 18/300, seasonal_2 Loss: 0.1093 | 0.1013
Epoch 19/300, seasonal_2 Loss: 0.1085 | 0.0999
Epoch 20/300, seasonal_2 Loss: 0.1079 | 0.0989
Epoch 21/300, seasonal_2 Loss: 0.1073 | 0.0980
Epoch 22/300, seasonal_2 Loss: 0.1068 | 0.0974
Epoch 23/300, seasonal_2 Loss: 0.1063 | 0.0968
Epoch 24/300, seasonal_2 Loss: 0.1059 | 0.0961
Epoch 25/300, seasonal_2 Loss: 0.1055 | 0.0955
Epoch 26/300, seasonal_2 Loss: 0.1050 | 0.0951
Epoch 27/300, seasonal_2 Loss: 0.1047 | 0.0946
Epoch 28/300, seasonal_2 Loss: 0.1043 | 0.0941
Epoch 29/300, seasonal_2 Loss: 0.1040 | 0.0938
Epoch 30/300, seasonal_2 Loss: 0.1037 | 0.0934
Epoch 31/300, seasonal_2 Loss: 0.1034 | 0.0930
Epoch 32/300, seasonal_2 Loss: 0.1031 | 0.0926
Epoch 33/300, seasonal_2 Loss: 0.1029 | 0.0925
Epoch 34/300, seasonal_2 Loss: 0.1026 | 0.0922
Epoch 35/300, seasonal_2 Loss: 0.1024 | 0.0919
Epoch 36/300, seasonal_2 Loss: 0.1022 | 0.0918
Epoch 37/300, seasonal_2 Loss: 0.1020 | 0.0915
Epoch 38/300, seasonal_2 Loss: 0.1019 | 0.0913
Epoch 39/300, seasonal_2 Loss: 0.1017 | 0.0910
Epoch 40/300, seasonal_2 Loss: 0.1015 | 0.0909
Epoch 41/300, seasonal_2 Loss: 0.1014 | 0.0907
Epoch 42/300, seasonal_2 Loss: 0.1012 | 0.0905
Epoch 43/300, seasonal_2 Loss: 0.1011 | 0.0904
Epoch 44/300, seasonal_2 Loss: 0.1010 | 0.0902
Epoch 45/300, seasonal_2 Loss: 0.1009 | 0.0901
Epoch 46/300, seasonal_2 Loss: 0.1008 | 0.0899
Epoch 47/300, seasonal_2 Loss: 0.1007 | 0.0898
Epoch 48/300, seasonal_2 Loss: 0.1006 | 0.0897
Epoch 49/300, seasonal_2 Loss: 0.1005 | 0.0896
Epoch 50/300, seasonal_2 Loss: 0.1004 | 0.0894
Epoch 51/300, seasonal_2 Loss: 0.1004 | 0.0893
Epoch 52/300, seasonal_2 Loss: 0.1003 | 0.0893
Epoch 53/300, seasonal_2 Loss: 0.1003 | 0.0892
Epoch 54/300, seasonal_2 Loss: 0.1002 | 0.0891
Epoch 55/300, seasonal_2 Loss: 0.1002 | 0.0890
Epoch 56/300, seasonal_2 Loss: 0.1001 | 0.0890
Epoch 57/300, seasonal_2 Loss: 0.1001 | 0.0889
Epoch 58/300, seasonal_2 Loss: 0.1000 | 0.0889
Epoch 59/300, seasonal_2 Loss: 0.1000 | 0.0888
Epoch 60/300, seasonal_2 Loss: 0.1000 | 0.0888
Epoch 61/300, seasonal_2 Loss: 0.0999 | 0.0887
Epoch 62/300, seasonal_2 Loss: 0.0999 | 0.0887
Epoch 63/300, seasonal_2 Loss: 0.0999 | 0.0887
Epoch 64/300, seasonal_2 Loss: 0.0998 | 0.0887
Epoch 65/300, seasonal_2 Loss: 0.0998 | 0.0886
Epoch 66/300, seasonal_2 Loss: 0.0998 | 0.0886
Epoch 67/300, seasonal_2 Loss: 0.0998 | 0.0886
Epoch 68/300, seasonal_2 Loss: 0.0997 | 0.0886
Epoch 69/300, seasonal_2 Loss: 0.0997 | 0.0885
Epoch 70/300, seasonal_2 Loss: 0.0997 | 0.0885
Epoch 71/300, seasonal_2 Loss: 0.0997 | 0.0885
Epoch 72/300, seasonal_2 Loss: 0.0997 | 0.0885
Epoch 73/300, seasonal_2 Loss: 0.0997 | 0.0885
Epoch 74/300, seasonal_2 Loss: 0.0996 | 0.0885
Epoch 75/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 76/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 77/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 78/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 79/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 80/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 81/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 82/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 83/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 84/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 85/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 86/300, seasonal_2 Loss: 0.0996 | 0.0884
Epoch 87/300, seasonal_2 Loss: 0.0996 | 0.0883
Epoch 88/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 89/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 90/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 91/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 92/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 93/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 94/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 95/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 96/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 97/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 98/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 99/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 100/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 101/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 102/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 103/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 104/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 105/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 106/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 107/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 108/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 109/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 110/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 111/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 112/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 113/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 114/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 115/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 116/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 117/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 118/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 119/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 120/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 121/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 122/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 123/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 124/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 125/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 126/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 127/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 128/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 129/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 130/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 131/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 132/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 133/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 134/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 135/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 136/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 137/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 138/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 139/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 140/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 141/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 142/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 143/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 144/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 145/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 146/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 147/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 148/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 149/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 150/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 151/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 152/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 153/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 154/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 155/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 156/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 157/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 158/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 159/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 160/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 161/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 162/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 163/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 164/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 165/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 166/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 167/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 168/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 169/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 170/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 171/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 172/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 173/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 174/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 175/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 176/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 177/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 178/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 179/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 180/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 181/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 182/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 183/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 184/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 185/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 186/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 187/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 188/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 189/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 190/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 191/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 192/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 193/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 194/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 195/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 196/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 197/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 198/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 199/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 200/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 201/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 202/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 203/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 204/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 205/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 206/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 207/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 208/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 209/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 210/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 211/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 212/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 213/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 214/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 215/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 216/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 217/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 218/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 219/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 220/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 221/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 222/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 223/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 224/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 225/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 226/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 227/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 228/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 229/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 230/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 231/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 232/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 233/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 234/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 235/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 236/300, seasonal_2 Loss: 0.0995 | 0.0883
Epoch 237/300, seasonal_2 Loss: 0.0995 | 0.0883
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.6263361589618415, 'learning_rate': 0.0005369785453693631, 'batch_size': 151, 'step_size': 13, 'gamma': 0.789885516485259}
Epoch 1/300, seasonal_3 Loss: 0.7256 | 0.3907
Epoch 2/300, seasonal_3 Loss: 0.2963 | 0.2505
Epoch 3/300, seasonal_3 Loss: 0.2304 | 0.1876
Epoch 4/300, seasonal_3 Loss: 0.2332 | 0.1614
Epoch 5/300, seasonal_3 Loss: 0.1920 | 0.1389
Epoch 6/300, seasonal_3 Loss: 0.1524 | 0.1195
Epoch 7/300, seasonal_3 Loss: 0.1327 | 0.1143
Epoch 8/300, seasonal_3 Loss: 0.1287 | 0.1174
Epoch 9/300, seasonal_3 Loss: 0.1267 | 0.1103
Epoch 10/300, seasonal_3 Loss: 0.1229 | 0.1043
Epoch 11/300, seasonal_3 Loss: 0.1191 | 0.1001
Epoch 12/300, seasonal_3 Loss: 0.1167 | 0.0968
Epoch 13/300, seasonal_3 Loss: 0.1153 | 0.0949
Epoch 14/300, seasonal_3 Loss: 0.1142 | 0.0971
Epoch 15/300, seasonal_3 Loss: 0.1144 | 0.0965
Epoch 16/300, seasonal_3 Loss: 0.1175 | 0.0957
Epoch 17/300, seasonal_3 Loss: 0.1379 | 0.1654
Epoch 18/300, seasonal_3 Loss: 0.1284 | 0.1259
Epoch 19/300, seasonal_3 Loss: 0.1204 | 0.1194
Epoch 20/300, seasonal_3 Loss: 0.1207 | 0.1068
Epoch 21/300, seasonal_3 Loss: 0.1111 | 0.1064
Epoch 22/300, seasonal_3 Loss: 0.1085 | 0.0921
Epoch 23/300, seasonal_3 Loss: 0.1073 | 0.0939
Epoch 24/300, seasonal_3 Loss: 0.1067 | 0.0894
Epoch 25/300, seasonal_3 Loss: 0.1043 | 0.0890
Epoch 26/300, seasonal_3 Loss: 0.1048 | 0.0883
Epoch 27/300, seasonal_3 Loss: 0.1023 | 0.0892
Epoch 28/300, seasonal_3 Loss: 0.1023 | 0.0869
Epoch 29/300, seasonal_3 Loss: 0.1014 | 0.0899
Epoch 30/300, seasonal_3 Loss: 0.1007 | 0.0852
Epoch 31/300, seasonal_3 Loss: 0.1000 | 0.0899
Epoch 32/300, seasonal_3 Loss: 0.0998 | 0.0852
Epoch 33/300, seasonal_3 Loss: 0.0988 | 0.0885
Epoch 34/300, seasonal_3 Loss: 0.0987 | 0.0844
Epoch 35/300, seasonal_3 Loss: 0.0980 | 0.0897
Epoch 36/300, seasonal_3 Loss: 0.0977 | 0.0833
Epoch 37/300, seasonal_3 Loss: 0.0971 | 0.0893
Epoch 38/300, seasonal_3 Loss: 0.0970 | 0.0828
Epoch 39/300, seasonal_3 Loss: 0.0963 | 0.0886
Epoch 40/300, seasonal_3 Loss: 0.0960 | 0.0827
Epoch 41/300, seasonal_3 Loss: 0.0954 | 0.0881
Epoch 42/300, seasonal_3 Loss: 0.0954 | 0.0812
Epoch 43/300, seasonal_3 Loss: 0.0949 | 0.0888
Epoch 44/300, seasonal_3 Loss: 0.0946 | 0.0808
Epoch 45/300, seasonal_3 Loss: 0.0940 | 0.0874
Epoch 46/300, seasonal_3 Loss: 0.0940 | 0.0798
Epoch 47/300, seasonal_3 Loss: 0.0934 | 0.0868
Epoch 48/300, seasonal_3 Loss: 0.0936 | 0.0798
Epoch 49/300, seasonal_3 Loss: 0.0932 | 0.0868
Epoch 50/300, seasonal_3 Loss: 0.0934 | 0.0786
Epoch 51/300, seasonal_3 Loss: 0.0929 | 0.0861
Epoch 52/300, seasonal_3 Loss: 0.0934 | 0.0781
Epoch 53/300, seasonal_3 Loss: 0.0925 | 0.0818
Epoch 54/300, seasonal_3 Loss: 0.0938 | 0.0819
Epoch 55/300, seasonal_3 Loss: 0.0925 | 0.0796
Epoch 56/300, seasonal_3 Loss: 0.0945 | 0.0842
Epoch 57/300, seasonal_3 Loss: 0.0925 | 0.0787
Epoch 58/300, seasonal_3 Loss: 0.0942 | 0.0845
Epoch 59/300, seasonal_3 Loss: 0.0924 | 0.0788
Epoch 60/300, seasonal_3 Loss: 0.0929 | 0.0856
Epoch 61/300, seasonal_3 Loss: 0.0913 | 0.0782
Epoch 62/300, seasonal_3 Loss: 0.0912 | 0.0822
Epoch 63/300, seasonal_3 Loss: 0.0899 | 0.0768
Epoch 64/300, seasonal_3 Loss: 0.0900 | 0.0806
Epoch 65/300, seasonal_3 Loss: 0.0894 | 0.0764
Epoch 66/300, seasonal_3 Loss: 0.0893 | 0.0805
Epoch 67/300, seasonal_3 Loss: 0.0889 | 0.0760
Epoch 68/300, seasonal_3 Loss: 0.0888 | 0.0789
Epoch 69/300, seasonal_3 Loss: 0.0886 | 0.0765
Epoch 70/300, seasonal_3 Loss: 0.0885 | 0.0782
Epoch 71/300, seasonal_3 Loss: 0.0884 | 0.0769
Epoch 72/300, seasonal_3 Loss: 0.0884 | 0.0781
Epoch 73/300, seasonal_3 Loss: 0.0883 | 0.0769
Epoch 74/300, seasonal_3 Loss: 0.0884 | 0.0777
Epoch 75/300, seasonal_3 Loss: 0.0883 | 0.0772
Epoch 76/300, seasonal_3 Loss: 0.0880 | 0.0778
Epoch 77/300, seasonal_3 Loss: 0.0877 | 0.0777
Epoch 78/300, seasonal_3 Loss: 0.0875 | 0.0779
Epoch 79/300, seasonal_3 Loss: 0.0873 | 0.0777
Epoch 80/300, seasonal_3 Loss: 0.0872 | 0.0778
Epoch 81/300, seasonal_3 Loss: 0.0871 | 0.0777
Epoch 82/300, seasonal_3 Loss: 0.0870 | 0.0777
Epoch 83/300, seasonal_3 Loss: 0.0870 | 0.0777
Epoch 84/300, seasonal_3 Loss: 0.0869 | 0.0776
Epoch 85/300, seasonal_3 Loss: 0.0868 | 0.0776
Epoch 86/300, seasonal_3 Loss: 0.0867 | 0.0776
Epoch 87/300, seasonal_3 Loss: 0.0866 | 0.0776
Epoch 88/300, seasonal_3 Loss: 0.0866 | 0.0775
Epoch 89/300, seasonal_3 Loss: 0.0865 | 0.0775
Epoch 90/300, seasonal_3 Loss: 0.0864 | 0.0775
Epoch 91/300, seasonal_3 Loss: 0.0864 | 0.0775
Epoch 92/300, seasonal_3 Loss: 0.0863 | 0.0775
Epoch 93/300, seasonal_3 Loss: 0.0862 | 0.0775
Epoch 94/300, seasonal_3 Loss: 0.0862 | 0.0775
Epoch 95/300, seasonal_3 Loss: 0.0861 | 0.0774
Epoch 96/300, seasonal_3 Loss: 0.0861 | 0.0774
Epoch 97/300, seasonal_3 Loss: 0.0860 | 0.0774
Epoch 98/300, seasonal_3 Loss: 0.0860 | 0.0774
Epoch 99/300, seasonal_3 Loss: 0.0859 | 0.0774
Epoch 100/300, seasonal_3 Loss: 0.0859 | 0.0774
Epoch 101/300, seasonal_3 Loss: 0.0858 | 0.0774
Epoch 102/300, seasonal_3 Loss: 0.0858 | 0.0774
Epoch 103/300, seasonal_3 Loss: 0.0857 | 0.0773
Epoch 104/300, seasonal_3 Loss: 0.0857 | 0.0773
Epoch 105/300, seasonal_3 Loss: 0.0856 | 0.0773
Epoch 106/300, seasonal_3 Loss: 0.0856 | 0.0773
Epoch 107/300, seasonal_3 Loss: 0.0856 | 0.0773
Epoch 108/300, seasonal_3 Loss: 0.0855 | 0.0773
Epoch 109/300, seasonal_3 Loss: 0.0855 | 0.0773
Epoch 110/300, seasonal_3 Loss: 0.0855 | 0.0773
Epoch 111/300, seasonal_3 Loss: 0.0854 | 0.0773
Epoch 112/300, seasonal_3 Loss: 0.0854 | 0.0773
Epoch 113/300, seasonal_3 Loss: 0.0853 | 0.0773
Epoch 114/300, seasonal_3 Loss: 0.0853 | 0.0772
Epoch 115/300, seasonal_3 Loss: 0.0853 | 0.0772
Epoch 116/300, seasonal_3 Loss: 0.0853 | 0.0772
Epoch 117/300, seasonal_3 Loss: 0.0852 | 0.0772
Epoch 118/300, seasonal_3 Loss: 0.0852 | 0.0772
Epoch 119/300, seasonal_3 Loss: 0.0852 | 0.0772
Epoch 120/300, seasonal_3 Loss: 0.0851 | 0.0772
Epoch 121/300, seasonal_3 Loss: 0.0851 | 0.0772
Epoch 122/300, seasonal_3 Loss: 0.0851 | 0.0772
Epoch 123/300, seasonal_3 Loss: 0.0851 | 0.0772
Epoch 124/300, seasonal_3 Loss: 0.0850 | 0.0772
Epoch 125/300, seasonal_3 Loss: 0.0850 | 0.0772
Epoch 126/300, seasonal_3 Loss: 0.0850 | 0.0772
Epoch 127/300, seasonal_3 Loss: 0.0850 | 0.0772
Epoch 128/300, seasonal_3 Loss: 0.0850 | 0.0772
Epoch 129/300, seasonal_3 Loss: 0.0849 | 0.0772
Epoch 130/300, seasonal_3 Loss: 0.0849 | 0.0772
Epoch 131/300, seasonal_3 Loss: 0.0849 | 0.0771
Epoch 132/300, seasonal_3 Loss: 0.0849 | 0.0771
Epoch 133/300, seasonal_3 Loss: 0.0849 | 0.0771
Epoch 134/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 135/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 136/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 137/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 138/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 139/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 140/300, seasonal_3 Loss: 0.0848 | 0.0771
Epoch 141/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 142/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 143/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 144/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 145/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 146/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 147/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 148/300, seasonal_3 Loss: 0.0847 | 0.0771
Epoch 149/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 150/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 151/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 152/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 153/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 154/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 155/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 156/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 157/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 158/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 159/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 160/300, seasonal_3 Loss: 0.0846 | 0.0771
Epoch 161/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 162/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 163/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 164/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 165/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 166/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 167/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 168/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 169/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 170/300, seasonal_3 Loss: 0.0845 | 0.0771
Epoch 171/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 172/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 173/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 174/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 175/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 176/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 177/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 178/300, seasonal_3 Loss: 0.0845 | 0.0770
Epoch 179/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 180/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 181/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 182/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 183/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 184/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 185/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 186/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 187/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 188/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 189/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 190/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 191/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 192/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 193/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 194/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 195/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 196/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 197/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 198/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 199/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 200/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 201/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 202/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 203/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 204/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 205/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 206/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 207/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 208/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 209/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 210/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 211/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 212/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 213/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 214/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 215/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 216/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 217/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 218/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 219/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 220/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 221/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 222/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 223/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 224/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 225/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 226/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 227/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 228/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 229/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 230/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 231/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 232/300, seasonal_3 Loss: 0.0844 | 0.0770
Epoch 233/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 234/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 235/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 236/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 237/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 238/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 239/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 240/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 241/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 242/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 243/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 244/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 245/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 246/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 247/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 248/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 249/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 250/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 251/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 252/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 253/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 254/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 255/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 256/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 257/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 258/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 259/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 260/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 261/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 262/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 263/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 264/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 265/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 266/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 267/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 268/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 269/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 270/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 271/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 272/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 273/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 274/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 275/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 276/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 277/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 278/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 279/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 280/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 281/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 282/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 283/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 284/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 285/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 286/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 287/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 288/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 289/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 290/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 291/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 292/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 293/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 294/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 295/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 296/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 297/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 298/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 299/300, seasonal_3 Loss: 0.0843 | 0.0770
Epoch 300/300, seasonal_3 Loss: 0.0843 | 0.0770
Training resid component with params: {'observation_period_num': 16, 'train_rates': 0.727922262462855, 'learning_rate': 0.00042986678296899443, 'batch_size': 43, 'step_size': 1, 'gamma': 0.9398746161729022}
Epoch 1/300, resid Loss: 0.2625 | 0.1687
Epoch 2/300, resid Loss: 0.1438 | 0.1347
Epoch 3/300, resid Loss: 0.1331 | 0.1537
Epoch 4/300, resid Loss: 0.1238 | 0.1484
Epoch 5/300, resid Loss: 0.1163 | 0.1401
Epoch 6/300, resid Loss: 0.1116 | 0.1351
Epoch 7/300, resid Loss: 0.1072 | 0.1265
Epoch 8/300, resid Loss: 0.1053 | 0.1243
Epoch 9/300, resid Loss: 0.1074 | 0.1122
Epoch 10/300, resid Loss: 0.1138 | 0.1035
Epoch 11/300, resid Loss: 0.1065 | 0.1010
Epoch 12/300, resid Loss: 0.0977 | 0.0975
Epoch 13/300, resid Loss: 0.0938 | 0.0979
Epoch 14/300, resid Loss: 0.0915 | 0.0960
Epoch 15/300, resid Loss: 0.0899 | 0.0917
Epoch 16/300, resid Loss: 0.0887 | 0.0904
Epoch 17/300, resid Loss: 0.0876 | 0.0885
Epoch 18/300, resid Loss: 0.0869 | 0.0879
Epoch 19/300, resid Loss: 0.0861 | 0.0869
Epoch 20/300, resid Loss: 0.0854 | 0.0863
Epoch 21/300, resid Loss: 0.0847 | 0.0857
Epoch 22/300, resid Loss: 0.0842 | 0.0853
Epoch 23/300, resid Loss: 0.0837 | 0.0849
Epoch 24/300, resid Loss: 0.0833 | 0.0846
Epoch 25/300, resid Loss: 0.0830 | 0.0844
Epoch 26/300, resid Loss: 0.0828 | 0.0842
Epoch 27/300, resid Loss: 0.0826 | 0.0840
Epoch 28/300, resid Loss: 0.0824 | 0.0840
Epoch 29/300, resid Loss: 0.0822 | 0.0840
Epoch 30/300, resid Loss: 0.0820 | 0.0840
Epoch 31/300, resid Loss: 0.0819 | 0.0840
Epoch 32/300, resid Loss: 0.0818 | 0.0840
Epoch 33/300, resid Loss: 0.0817 | 0.0840
Epoch 34/300, resid Loss: 0.0816 | 0.0839
Epoch 35/300, resid Loss: 0.0816 | 0.0838
Epoch 36/300, resid Loss: 0.0816 | 0.0838
Epoch 37/300, resid Loss: 0.0815 | 0.0837
Epoch 38/300, resid Loss: 0.0814 | 0.0836
Epoch 39/300, resid Loss: 0.0813 | 0.0836
Epoch 40/300, resid Loss: 0.0812 | 0.0836
Epoch 41/300, resid Loss: 0.0811 | 0.0835
Epoch 42/300, resid Loss: 0.0810 | 0.0835
Epoch 43/300, resid Loss: 0.0809 | 0.0834
Epoch 44/300, resid Loss: 0.0809 | 0.0833
Epoch 45/300, resid Loss: 0.0808 | 0.0833
Epoch 46/300, resid Loss: 0.0808 | 0.0832
Epoch 47/300, resid Loss: 0.0808 | 0.0832
Epoch 48/300, resid Loss: 0.0807 | 0.0832
Epoch 49/300, resid Loss: 0.0807 | 0.0831
Epoch 50/300, resid Loss: 0.0807 | 0.0831
Epoch 51/300, resid Loss: 0.0807 | 0.0831
Epoch 52/300, resid Loss: 0.0806 | 0.0830
Epoch 53/300, resid Loss: 0.0806 | 0.0830
Epoch 54/300, resid Loss: 0.0806 | 0.0830
Epoch 55/300, resid Loss: 0.0806 | 0.0830
Epoch 56/300, resid Loss: 0.0806 | 0.0830
Epoch 57/300, resid Loss: 0.0806 | 0.0830
Epoch 58/300, resid Loss: 0.0806 | 0.0829
Epoch 59/300, resid Loss: 0.0806 | 0.0829
Epoch 60/300, resid Loss: 0.0806 | 0.0829
Epoch 61/300, resid Loss: 0.0806 | 0.0829
Epoch 62/300, resid Loss: 0.0806 | 0.0829
Epoch 63/300, resid Loss: 0.0805 | 0.0829
Epoch 64/300, resid Loss: 0.0805 | 0.0829
Epoch 65/300, resid Loss: 0.0805 | 0.0829
Epoch 66/300, resid Loss: 0.0805 | 0.0829
Epoch 67/300, resid Loss: 0.0805 | 0.0829
Epoch 68/300, resid Loss: 0.0805 | 0.0829
Epoch 69/300, resid Loss: 0.0805 | 0.0829
Epoch 70/300, resid Loss: 0.0805 | 0.0829
Epoch 71/300, resid Loss: 0.0805 | 0.0829
Epoch 72/300, resid Loss: 0.0805 | 0.0829
Epoch 73/300, resid Loss: 0.0805 | 0.0829
Epoch 74/300, resid Loss: 0.0805 | 0.0829
Epoch 75/300, resid Loss: 0.0805 | 0.0829
Epoch 76/300, resid Loss: 0.0805 | 0.0829
Epoch 77/300, resid Loss: 0.0805 | 0.0829
Epoch 78/300, resid Loss: 0.0805 | 0.0829
Epoch 79/300, resid Loss: 0.0805 | 0.0829
Epoch 80/300, resid Loss: 0.0805 | 0.0829
Epoch 81/300, resid Loss: 0.0805 | 0.0829
Epoch 82/300, resid Loss: 0.0805 | 0.0829
Epoch 83/300, resid Loss: 0.0805 | 0.0829
Epoch 84/300, resid Loss: 0.0805 | 0.0829
Epoch 85/300, resid Loss: 0.0805 | 0.0829
Epoch 86/300, resid Loss: 0.0805 | 0.0829
Epoch 87/300, resid Loss: 0.0805 | 0.0829
Epoch 88/300, resid Loss: 0.0805 | 0.0829
Epoch 89/300, resid Loss: 0.0805 | 0.0829
Epoch 90/300, resid Loss: 0.0805 | 0.0829
Epoch 91/300, resid Loss: 0.0805 | 0.0829
Epoch 92/300, resid Loss: 0.0805 | 0.0829
Epoch 93/300, resid Loss: 0.0805 | 0.0829
Epoch 94/300, resid Loss: 0.0805 | 0.0829
Epoch 95/300, resid Loss: 0.0805 | 0.0829
Epoch 96/300, resid Loss: 0.0805 | 0.0829
Epoch 97/300, resid Loss: 0.0805 | 0.0829
Epoch 98/300, resid Loss: 0.0805 | 0.0829
Epoch 99/300, resid Loss: 0.0805 | 0.0829
Epoch 100/300, resid Loss: 0.0805 | 0.0829
Epoch 101/300, resid Loss: 0.0805 | 0.0829
Epoch 102/300, resid Loss: 0.0805 | 0.0829
Epoch 103/300, resid Loss: 0.0805 | 0.0829
Epoch 104/300, resid Loss: 0.0805 | 0.0829
Epoch 105/300, resid Loss: 0.0805 | 0.0829
Epoch 106/300, resid Loss: 0.0805 | 0.0829
Epoch 107/300, resid Loss: 0.0805 | 0.0829
Epoch 108/300, resid Loss: 0.0805 | 0.0829
Epoch 109/300, resid Loss: 0.0805 | 0.0829
Epoch 110/300, resid Loss: 0.0805 | 0.0829
Epoch 111/300, resid Loss: 0.0805 | 0.0829
Epoch 112/300, resid Loss: 0.0805 | 0.0829
Epoch 113/300, resid Loss: 0.0805 | 0.0829
Epoch 114/300, resid Loss: 0.0805 | 0.0829
Epoch 115/300, resid Loss: 0.0805 | 0.0829
Epoch 116/300, resid Loss: 0.0805 | 0.0829
Epoch 117/300, resid Loss: 0.0805 | 0.0829
Epoch 118/300, resid Loss: 0.0805 | 0.0829
Epoch 119/300, resid Loss: 0.0805 | 0.0829
Epoch 120/300, resid Loss: 0.0805 | 0.0829
Epoch 121/300, resid Loss: 0.0805 | 0.0829
Epoch 122/300, resid Loss: 0.0805 | 0.0829
Epoch 123/300, resid Loss: 0.0805 | 0.0829
Epoch 124/300, resid Loss: 0.0805 | 0.0829
Epoch 125/300, resid Loss: 0.0805 | 0.0829
Epoch 126/300, resid Loss: 0.0805 | 0.0829
Epoch 127/300, resid Loss: 0.0805 | 0.0829
Epoch 128/300, resid Loss: 0.0805 | 0.0829
Epoch 129/300, resid Loss: 0.0805 | 0.0829
Epoch 130/300, resid Loss: 0.0805 | 0.0829
Epoch 131/300, resid Loss: 0.0805 | 0.0829
Early stopping for resid
Runtime (seconds): 1738.9544591903687
3.8196610118856964e-05
[98.58053]
[5.112959]
[-0.3004798]
[0.89056414]
[-3.1163025]
[0.05231646]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.08433965413132682
RMSE: 0.29041290283203125
MAE: 0.29041290283203125
R-squared: nan
[101.21959]
