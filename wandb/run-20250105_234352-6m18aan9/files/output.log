[32m[I 2025-01-05 23:43:54,263][0m A new study created in memory with name: no-name-3dcaf9d2-7ec0-4261-b632-e5b8baf1d999[0m
[32m[I 2025-01-05 23:48:32,480][0m Trial 0 finished with value: 1.148329373889794 and parameters: {'observation_period_num': 235, 'train_rates': 0.6362195863700756, 'learning_rate': 1.993671658504207e-05, 'batch_size': 221, 'step_size': 13, 'gamma': 0.9772906079222371}. Best is trial 0 with value: 1.148329373889794.[0m
[32m[I 2025-01-05 23:50:36,812][0m Trial 1 finished with value: 1.4670006452986877 and parameters: {'observation_period_num': 107, 'train_rates': 0.7707288477273753, 'learning_rate': 1.2549251347880671e-06, 'batch_size': 124, 'step_size': 11, 'gamma': 0.9203016662190961}. Best is trial 0 with value: 1.148329373889794.[0m
[32m[I 2025-01-05 23:52:30,106][0m Trial 2 finished with value: 0.4141434263946401 and parameters: {'observation_period_num': 91, 'train_rates': 0.8591598941749878, 'learning_rate': 1.3905306625476262e-05, 'batch_size': 176, 'step_size': 13, 'gamma': 0.900111012838494}. Best is trial 2 with value: 0.4141434263946401.[0m
[32m[I 2025-01-05 23:54:57,924][0m Trial 3 finished with value: 0.9545696167672266 and parameters: {'observation_period_num': 134, 'train_rates': 0.6536928563002714, 'learning_rate': 2.2924359302500122e-05, 'batch_size': 243, 'step_size': 7, 'gamma': 0.8393739618849878}. Best is trial 2 with value: 0.4141434263946401.[0m
[32m[I 2025-01-05 23:59:12,079][0m Trial 4 finished with value: 0.2311424825396111 and parameters: {'observation_period_num': 182, 'train_rates': 0.9034287456964325, 'learning_rate': 7.803818933987414e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8787421595148904}. Best is trial 4 with value: 0.2311424825396111.[0m
[32m[I 2025-01-06 00:00:07,724][0m Trial 5 finished with value: 0.5443231136415878 and parameters: {'observation_period_num': 25, 'train_rates': 0.8490229105390741, 'learning_rate': 4.5643605970240625e-06, 'batch_size': 71, 'step_size': 6, 'gamma': 0.916680340339797}. Best is trial 4 with value: 0.2311424825396111.[0m
[32m[I 2025-01-06 00:03:28,813][0m Trial 6 finished with value: 0.23217491805553436 and parameters: {'observation_period_num': 140, 'train_rates': 0.9800018745537439, 'learning_rate': 0.00029248547903686604, 'batch_size': 186, 'step_size': 2, 'gamma': 0.8113402226911574}. Best is trial 4 with value: 0.2311424825396111.[0m
[32m[I 2025-01-06 00:04:42,822][0m Trial 7 finished with value: 0.21340275475236237 and parameters: {'observation_period_num': 43, 'train_rates': 0.8952622634770306, 'learning_rate': 5.0638812412950856e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.853281889760601}. Best is trial 7 with value: 0.21340275475236237.[0m
[32m[I 2025-01-06 00:07:58,765][0m Trial 8 finished with value: 1.3353801618203025 and parameters: {'observation_period_num': 184, 'train_rates': 0.608569433915305, 'learning_rate': 3.4373409831098494e-06, 'batch_size': 230, 'step_size': 12, 'gamma': 0.9375022258001368}. Best is trial 7 with value: 0.21340275475236237.[0m
[32m[I 2025-01-06 00:11:20,692][0m Trial 9 finished with value: 0.8119230983499995 and parameters: {'observation_period_num': 169, 'train_rates': 0.6999846339070944, 'learning_rate': 3.09266881944825e-05, 'batch_size': 83, 'step_size': 5, 'gamma': 0.8822169920491878}. Best is trial 7 with value: 0.21340275475236237.[0m
Early stopping at epoch 56
[32m[I 2025-01-06 00:12:12,223][0m Trial 10 finished with value: 0.5236508038556464 and parameters: {'observation_period_num': 9, 'train_rates': 0.9639577384368403, 'learning_rate': 0.0004897449882851663, 'batch_size': 47, 'step_size': 1, 'gamma': 0.7624441382623761}. Best is trial 7 with value: 0.21340275475236237.[0m
[32m[I 2025-01-06 00:14:49,553][0m Trial 11 finished with value: 0.15683057936855438 and parameters: {'observation_period_num': 60, 'train_rates': 0.8974684398751847, 'learning_rate': 0.0001482845946876886, 'batch_size': 25, 'step_size': 9, 'gamma': 0.8389939936309614}. Best is trial 11 with value: 0.15683057936855438.[0m
[32m[I 2025-01-06 00:18:24,115][0m Trial 12 finished with value: 0.14503153802996332 and parameters: {'observation_period_num': 54, 'train_rates': 0.908913952644476, 'learning_rate': 0.00019762270669626284, 'batch_size': 18, 'step_size': 10, 'gamma': 0.832911166642478}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:22:02,874][0m Trial 13 finished with value: 0.4448803775012493 and parameters: {'observation_period_num': 57, 'train_rates': 0.8011133755310429, 'learning_rate': 0.0001666693267415593, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7970980298092816}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:26:19,477][0m Trial 14 finished with value: 2.145824319911453 and parameters: {'observation_period_num': 69, 'train_rates': 0.9265270789711184, 'learning_rate': 0.0008670920256176083, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8211298712129653}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:28:05,908][0m Trial 15 finished with value: 0.34538982650421185 and parameters: {'observation_period_num': 88, 'train_rates': 0.8127495477961512, 'learning_rate': 0.00012957702124160553, 'batch_size': 110, 'step_size': 9, 'gamma': 0.779176788675113}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:28:52,531][0m Trial 16 finished with value: 0.7900389256213494 and parameters: {'observation_period_num': 37, 'train_rates': 0.7397519525577185, 'learning_rate': 0.0002719406139581753, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8434168457146755}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:30:43,405][0m Trial 17 finished with value: 0.16377654055069232 and parameters: {'observation_period_num': 68, 'train_rates': 0.9398513232600207, 'learning_rate': 9.329630849682671e-05, 'batch_size': 40, 'step_size': 4, 'gamma': 0.7925312980744227}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:32:48,114][0m Trial 18 finished with value: 0.34768107207491994 and parameters: {'observation_period_num': 99, 'train_rates': 0.8656364547090157, 'learning_rate': 0.0002903651172276523, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8602347566399634}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:33:52,254][0m Trial 19 finished with value: 1.2422135227006188 and parameters: {'observation_period_num': 22, 'train_rates': 0.8283517226143675, 'learning_rate': 0.0009422328118091168, 'batch_size': 61, 'step_size': 7, 'gamma': 0.8244803714099863}. Best is trial 12 with value: 0.14503153802996332.[0m
[32m[I 2025-01-06 00:36:21,029][0m Trial 20 finished with value: 0.12574146277849982 and parameters: {'observation_period_num': 5, 'train_rates': 0.8924661103606577, 'learning_rate': 5.7474372089143045e-05, 'batch_size': 27, 'step_size': 15, 'gamma': 0.7757899955961514}. Best is trial 20 with value: 0.12574146277849982.[0m
[32m[I 2025-01-06 00:38:23,079][0m Trial 21 finished with value: 0.10402506870813057 and parameters: {'observation_period_num': 7, 'train_rates': 0.8965031409065943, 'learning_rate': 5.49582349833455e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.7529861863750305}. Best is trial 21 with value: 0.10402506870813057.[0m
[32m[I 2025-01-06 00:40:11,174][0m Trial 22 finished with value: 0.08292438454669097 and parameters: {'observation_period_num': 8, 'train_rates': 0.9509150542315178, 'learning_rate': 4.3828818520571296e-05, 'batch_size': 39, 'step_size': 15, 'gamma': 0.7611632355347908}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:41:01,052][0m Trial 23 finished with value: 0.2555642293180738 and parameters: {'observation_period_num': 8, 'train_rates': 0.9406362999658199, 'learning_rate': 1.1106672215075535e-05, 'batch_size': 87, 'step_size': 15, 'gamma': 0.752731963170374}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:42:50,389][0m Trial 24 finished with value: 0.09277354579332263 and parameters: {'observation_period_num': 29, 'train_rates': 0.9854291860348988, 'learning_rate': 4.554028719737141e-05, 'batch_size': 39, 'step_size': 14, 'gamma': 0.7689411010942794}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:43:59,563][0m Trial 25 finished with value: 0.12198133766651154 and parameters: {'observation_period_num': 39, 'train_rates': 0.983275834344919, 'learning_rate': 4.2543950686035275e-05, 'batch_size': 66, 'step_size': 14, 'gamma': 0.7515952477051766}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:45:34,946][0m Trial 26 finished with value: 0.2394572886136862 and parameters: {'observation_period_num': 28, 'train_rates': 0.9601638268055858, 'learning_rate': 7.003820025375074e-06, 'batch_size': 46, 'step_size': 13, 'gamma': 0.7778960646464481}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:52:14,201][0m Trial 27 finished with value: 0.17054656147956848 and parameters: {'observation_period_num': 250, 'train_rates': 0.9886748289647261, 'learning_rate': 3.1078040587693684e-05, 'batch_size': 147, 'step_size': 14, 'gamma': 0.8012042366052365}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:54:02,872][0m Trial 28 finished with value: 0.14252963547523206 and parameters: {'observation_period_num': 77, 'train_rates': 0.941545820603613, 'learning_rate': 8.862994414011543e-05, 'batch_size': 104, 'step_size': 14, 'gamma': 0.7685418937911916}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:56:44,910][0m Trial 29 finished with value: 0.274882388849781 and parameters: {'observation_period_num': 117, 'train_rates': 0.8713086230144815, 'learning_rate': 2.0183274860135706e-05, 'batch_size': 40, 'step_size': 12, 'gamma': 0.7845986710348828}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:57:30,516][0m Trial 30 finished with value: 0.7383773309696648 and parameters: {'observation_period_num': 21, 'train_rates': 0.7634013308659666, 'learning_rate': 1.1717502951729045e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.7501732335451322}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 00:58:41,283][0m Trial 31 finished with value: 0.1250096596777439 and parameters: {'observation_period_num': 46, 'train_rates': 0.9699731769688498, 'learning_rate': 4.6423813152394634e-05, 'batch_size': 66, 'step_size': 14, 'gamma': 0.7641062522863797}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:00:52,187][0m Trial 32 finished with value: 0.1277550309896469 and parameters: {'observation_period_num': 35, 'train_rates': 0.989801215557656, 'learning_rate': 3.7641440228216146e-05, 'batch_size': 33, 'step_size': 13, 'gamma': 0.9670269012968359}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:02:00,382][0m Trial 33 finished with value: 0.14253111178469327 and parameters: {'observation_period_num': 5, 'train_rates': 0.9513014051440521, 'learning_rate': 1.7463343340146433e-05, 'batch_size': 59, 'step_size': 15, 'gamma': 0.7537130752213133}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:03:51,634][0m Trial 34 finished with value: 0.11775183039501247 and parameters: {'observation_period_num': 42, 'train_rates': 0.9188209359608145, 'learning_rate': 6.258552998078976e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.809632400994879}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:09:04,343][0m Trial 35 finished with value: 0.1659457842014494 and parameters: {'observation_period_num': 210, 'train_rates': 0.9194429572163498, 'learning_rate': 6.998871672304433e-05, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8092275908847308}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:10:36,566][0m Trial 36 finished with value: 0.28403221871425854 and parameters: {'observation_period_num': 23, 'train_rates': 0.8399852920023366, 'learning_rate': 2.6623391750685357e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.7854388439617518}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:13:57,246][0m Trial 37 finished with value: 1.5462287470091098 and parameters: {'observation_period_num': 149, 'train_rates': 0.8863787883811022, 'learning_rate': 1.0526238023269446e-06, 'batch_size': 195, 'step_size': 13, 'gamma': 0.7681965877238298}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:16:22,285][0m Trial 38 finished with value: 0.11717409891517538 and parameters: {'observation_period_num': 78, 'train_rates': 0.9339997050838328, 'learning_rate': 0.00012100105026805606, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8047822176906071}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:18:55,409][0m Trial 39 finished with value: 0.2760238013121003 and parameters: {'observation_period_num': 114, 'train_rates': 0.8738832092962052, 'learning_rate': 0.00010702275652075917, 'batch_size': 56, 'step_size': 15, 'gamma': 0.7927654644957285}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:20:50,498][0m Trial 40 finished with value: 0.24277592450380325 and parameters: {'observation_period_num': 83, 'train_rates': 0.9304086897136755, 'learning_rate': 7.92886560671439e-06, 'batch_size': 76, 'step_size': 12, 'gamma': 0.892557336946238}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:23:09,045][0m Trial 41 finished with value: 0.1335295670833744 and parameters: {'observation_period_num': 50, 'train_rates': 0.9159962470856646, 'learning_rate': 6.090420673833784e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8105343546143827}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:24:31,357][0m Trial 42 finished with value: 0.09441356276246635 and parameters: {'observation_period_num': 18, 'train_rates': 0.9550691561522755, 'learning_rate': 0.00011116390607132475, 'batch_size': 51, 'step_size': 14, 'gamma': 0.7715577899553892}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:25:57,084][0m Trial 43 finished with value: 0.11254834501366866 and parameters: {'observation_period_num': 18, 'train_rates': 0.9611494001992251, 'learning_rate': 0.00020281605722551274, 'batch_size': 51, 'step_size': 15, 'gamma': 0.7668383106844989}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:26:49,154][0m Trial 44 finished with value: 0.48495148164089596 and parameters: {'observation_period_num': 14, 'train_rates': 0.9638250659569078, 'learning_rate': 0.000573229669370718, 'batch_size': 87, 'step_size': 14, 'gamma': 0.7647783707994168}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:28:18,377][0m Trial 45 finished with value: 0.09845128483909497 and parameters: {'observation_period_num': 17, 'train_rates': 0.9526907325187872, 'learning_rate': 0.00021534184140479905, 'batch_size': 49, 'step_size': 15, 'gamma': 0.7743536930550892}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:29:46,444][0m Trial 46 finished with value: 0.11517516740515263 and parameters: {'observation_period_num': 27, 'train_rates': 0.9731532581501543, 'learning_rate': 0.0002204631831694908, 'batch_size': 51, 'step_size': 13, 'gamma': 0.7887852301940944}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:32:30,926][0m Trial 47 finished with value: 1.1397488479126883 and parameters: {'observation_period_num': 32, 'train_rates': 0.690350348543336, 'learning_rate': 0.00035257209310115986, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9892248984484271}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:33:39,062][0m Trial 48 finished with value: 0.6131154261786362 and parameters: {'observation_period_num': 16, 'train_rates': 0.9508419362644405, 'learning_rate': 1.8344040269911746e-06, 'batch_size': 73, 'step_size': 12, 'gamma': 0.7754843033473873}. Best is trial 22 with value: 0.08292438454669097.[0m
[32m[I 2025-01-06 01:34:55,618][0m Trial 49 finished with value: 0.18677057474851608 and parameters: {'observation_period_num': 63, 'train_rates': 0.8997048068221284, 'learning_rate': 8.139944464726002e-05, 'batch_size': 203, 'step_size': 15, 'gamma': 0.9145671390369245}. Best is trial 22 with value: 0.08292438454669097.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.8287 | 1.0279
Epoch 2/300, Loss: 0.5614 | 0.7319
Epoch 3/300, Loss: 0.4121 | 0.6073
Epoch 4/300, Loss: 0.3338 | 0.4726
Epoch 5/300, Loss: 0.2962 | 0.4289
Epoch 6/300, Loss: 0.2674 | 0.4054
Epoch 7/300, Loss: 0.2420 | 0.3646
Epoch 8/300, Loss: 0.2279 | 0.3325
Epoch 9/300, Loss: 0.2188 | 0.3368
Epoch 10/300, Loss: 0.2146 | 0.3184
Epoch 11/300, Loss: 0.2089 | 0.2851
Epoch 12/300, Loss: 0.2084 | 0.2902
Epoch 13/300, Loss: 0.2063 | 0.2955
Epoch 14/300, Loss: 0.2012 | 0.2661
Epoch 15/300, Loss: 0.1913 | 0.2544
Epoch 16/300, Loss: 0.1834 | 0.2420
Epoch 17/300, Loss: 0.1783 | 0.2366
Epoch 18/300, Loss: 0.1699 | 0.2232
Epoch 19/300, Loss: 0.1661 | 0.2283
Epoch 20/300, Loss: 0.1671 | 0.2204
Epoch 21/300, Loss: 0.1682 | 0.2176
Epoch 22/300, Loss: 0.1663 | 0.2197
Epoch 23/300, Loss: 0.1667 | 0.2084
Epoch 24/300, Loss: 0.1632 | 0.1994
Epoch 25/300, Loss: 0.1548 | 0.1909
Epoch 26/300, Loss: 0.1492 | 0.1892
Epoch 27/300, Loss: 0.1454 | 0.1864
Epoch 28/300, Loss: 0.1420 | 0.1787
Epoch 29/300, Loss: 0.1393 | 0.1762
Epoch 30/300, Loss: 0.1383 | 0.1769
Epoch 31/300, Loss: 0.1364 | 0.1710
Epoch 32/300, Loss: 0.1343 | 0.1680
Epoch 33/300, Loss: 0.1330 | 0.1669
Epoch 34/300, Loss: 0.1319 | 0.1637
Epoch 35/300, Loss: 0.1301 | 0.1607
Epoch 36/300, Loss: 0.1296 | 0.1590
Epoch 37/300, Loss: 0.1285 | 0.1564
Epoch 38/300, Loss: 0.1273 | 0.1541
Epoch 39/300, Loss: 0.1267 | 0.1530
Epoch 40/300, Loss: 0.1258 | 0.1505
Epoch 41/300, Loss: 0.1241 | 0.1488
Epoch 42/300, Loss: 0.1241 | 0.1469
Epoch 43/300, Loss: 0.1224 | 0.1451
Epoch 44/300, Loss: 0.1225 | 0.1435
Epoch 45/300, Loss: 0.1215 | 0.1418
Epoch 46/300, Loss: 0.1211 | 0.1397
Epoch 47/300, Loss: 0.1202 | 0.1395
Epoch 48/300, Loss: 0.1195 | 0.1381
Epoch 49/300, Loss: 0.1187 | 0.1362
Epoch 50/300, Loss: 0.1182 | 0.1358
Epoch 51/300, Loss: 0.1182 | 0.1342
Epoch 52/300, Loss: 0.1166 | 0.1326
Epoch 53/300, Loss: 0.1163 | 0.1318
Epoch 54/300, Loss: 0.1157 | 0.1300
Epoch 55/300, Loss: 0.1151 | 0.1291
Epoch 56/300, Loss: 0.1148 | 0.1280
Epoch 57/300, Loss: 0.1145 | 0.1279
Epoch 58/300, Loss: 0.1138 | 0.1262
Epoch 59/300, Loss: 0.1131 | 0.1260
Epoch 60/300, Loss: 0.1128 | 0.1243
Epoch 61/300, Loss: 0.1122 | 0.1234
Epoch 62/300, Loss: 0.1115 | 0.1228
Epoch 63/300, Loss: 0.1121 | 0.1223
Epoch 64/300, Loss: 0.1113 | 0.1224
Epoch 65/300, Loss: 0.1105 | 0.1204
Epoch 66/300, Loss: 0.1106 | 0.1204
Epoch 67/300, Loss: 0.1103 | 0.1194
Epoch 68/300, Loss: 0.1096 | 0.1190
Epoch 69/300, Loss: 0.1093 | 0.1180
Epoch 70/300, Loss: 0.1096 | 0.1171
Epoch 71/300, Loss: 0.1084 | 0.1172
Epoch 72/300, Loss: 0.1086 | 0.1173
Epoch 73/300, Loss: 0.1082 | 0.1165
Epoch 74/300, Loss: 0.1078 | 0.1161
Epoch 75/300, Loss: 0.1075 | 0.1150
Epoch 76/300, Loss: 0.1080 | 0.1143
Epoch 77/300, Loss: 0.1069 | 0.1139
Epoch 78/300, Loss: 0.1062 | 0.1139
Epoch 79/300, Loss: 0.1066 | 0.1134
Epoch 80/300, Loss: 0.1062 | 0.1128
Epoch 81/300, Loss: 0.1062 | 0.1125
Epoch 82/300, Loss: 0.1062 | 0.1122
Epoch 83/300, Loss: 0.1063 | 0.1115
Epoch 84/300, Loss: 0.1055 | 0.1110
Epoch 85/300, Loss: 0.1054 | 0.1109
Epoch 86/300, Loss: 0.1052 | 0.1105
Epoch 87/300, Loss: 0.1051 | 0.1103
Epoch 88/300, Loss: 0.1057 | 0.1101
Epoch 89/300, Loss: 0.1048 | 0.1097
Epoch 90/300, Loss: 0.1041 | 0.1096
Epoch 91/300, Loss: 0.1040 | 0.1095
Epoch 92/300, Loss: 0.1042 | 0.1091
Epoch 93/300, Loss: 0.1038 | 0.1088
Epoch 94/300, Loss: 0.1039 | 0.1085
Epoch 95/300, Loss: 0.1036 | 0.1082
Epoch 96/300, Loss: 0.1038 | 0.1080
Epoch 97/300, Loss: 0.1030 | 0.1076
Epoch 98/300, Loss: 0.1033 | 0.1075
Epoch 99/300, Loss: 0.1037 | 0.1076
Epoch 100/300, Loss: 0.1038 | 0.1075
Epoch 101/300, Loss: 0.1029 | 0.1071
Epoch 102/300, Loss: 0.1034 | 0.1069
Epoch 103/300, Loss: 0.1032 | 0.1071
Epoch 104/300, Loss: 0.1033 | 0.1070
Epoch 105/300, Loss: 0.1031 | 0.1068
Epoch 106/300, Loss: 0.1028 | 0.1065
Epoch 107/300, Loss: 0.1028 | 0.1064
Epoch 108/300, Loss: 0.1028 | 0.1061
Epoch 109/300, Loss: 0.1025 | 0.1062
Epoch 110/300, Loss: 0.1021 | 0.1060
Epoch 111/300, Loss: 0.1022 | 0.1056
Epoch 112/300, Loss: 0.1021 | 0.1054
Epoch 113/300, Loss: 0.1026 | 0.1054
Epoch 114/300, Loss: 0.1018 | 0.1052
Epoch 115/300, Loss: 0.1021 | 0.1052
Epoch 116/300, Loss: 0.1020 | 0.1051
Epoch 117/300, Loss: 0.1023 | 0.1050
Epoch 118/300, Loss: 0.1020 | 0.1050
Epoch 119/300, Loss: 0.1018 | 0.1051
Epoch 120/300, Loss: 0.1016 | 0.1048
Epoch 121/300, Loss: 0.1026 | 0.1046
Epoch 122/300, Loss: 0.1022 | 0.1047
Epoch 123/300, Loss: 0.1018 | 0.1046
Epoch 124/300, Loss: 0.1017 | 0.1046
Epoch 125/300, Loss: 0.1013 | 0.1044
Epoch 126/300, Loss: 0.1015 | 0.1043
Epoch 127/300, Loss: 0.1014 | 0.1043
Epoch 128/300, Loss: 0.1019 | 0.1042
Epoch 129/300, Loss: 0.1011 | 0.1041
Epoch 130/300, Loss: 0.1013 | 0.1041
Epoch 131/300, Loss: 0.1012 | 0.1041
Epoch 132/300, Loss: 0.1011 | 0.1041
Epoch 133/300, Loss: 0.1013 | 0.1040
Epoch 134/300, Loss: 0.1015 | 0.1039
Epoch 135/300, Loss: 0.1014 | 0.1038
Epoch 136/300, Loss: 0.1013 | 0.1037
Epoch 137/300, Loss: 0.1014 | 0.1037
Epoch 138/300, Loss: 0.1016 | 0.1035
Epoch 139/300, Loss: 0.1018 | 0.1034
Epoch 140/300, Loss: 0.1015 | 0.1034
Epoch 141/300, Loss: 0.1013 | 0.1034
Epoch 142/300, Loss: 0.1011 | 0.1034
Epoch 143/300, Loss: 0.1011 | 0.1034
Epoch 144/300, Loss: 0.1016 | 0.1034
Epoch 145/300, Loss: 0.1015 | 0.1034
Epoch 146/300, Loss: 0.1009 | 0.1033
Epoch 147/300, Loss: 0.1012 | 0.1033
Epoch 148/300, Loss: 0.1013 | 0.1033
Epoch 149/300, Loss: 0.1007 | 0.1032
Epoch 150/300, Loss: 0.1006 | 0.1031
Epoch 151/300, Loss: 0.1008 | 0.1031
Epoch 152/300, Loss: 0.1011 | 0.1031
Epoch 153/300, Loss: 0.1005 | 0.1031
Epoch 154/300, Loss: 0.1009 | 0.1031
Epoch 155/300, Loss: 0.1013 | 0.1031
Epoch 156/300, Loss: 0.1014 | 0.1031
Epoch 157/300, Loss: 0.1014 | 0.1030
Epoch 158/300, Loss: 0.1009 | 0.1030
Epoch 159/300, Loss: 0.1008 | 0.1030
Epoch 160/300, Loss: 0.1006 | 0.1030
Epoch 161/300, Loss: 0.1015 | 0.1030
Epoch 162/300, Loss: 0.1007 | 0.1030
Epoch 163/300, Loss: 0.1007 | 0.1030
Epoch 164/300, Loss: 0.1011 | 0.1030
Epoch 165/300, Loss: 0.1015 | 0.1030
Epoch 166/300, Loss: 0.1008 | 0.1030
Epoch 167/300, Loss: 0.1008 | 0.1029
Epoch 168/300, Loss: 0.1013 | 0.1029
Epoch 169/300, Loss: 0.1013 | 0.1029
Epoch 170/300, Loss: 0.1009 | 0.1029
Epoch 171/300, Loss: 0.1011 | 0.1029
Epoch 172/300, Loss: 0.1005 | 0.1028
Epoch 173/300, Loss: 0.1006 | 0.1028
Epoch 174/300, Loss: 0.1013 | 0.1028
Epoch 175/300, Loss: 0.1007 | 0.1028
Epoch 176/300, Loss: 0.1008 | 0.1027
Epoch 177/300, Loss: 0.1006 | 0.1027
Epoch 178/300, Loss: 0.1002 | 0.1027
Epoch 179/300, Loss: 0.1003 | 0.1028
Epoch 180/300, Loss: 0.1004 | 0.1028
Epoch 181/300, Loss: 0.1012 | 0.1027
Epoch 182/300, Loss: 0.1006 | 0.1027
Epoch 183/300, Loss: 0.1011 | 0.1027
Epoch 184/300, Loss: 0.1008 | 0.1027
Epoch 185/300, Loss: 0.1009 | 0.1027
Epoch 186/300, Loss: 0.1007 | 0.1027
Epoch 187/300, Loss: 0.1005 | 0.1026
Epoch 188/300, Loss: 0.1013 | 0.1026
Epoch 189/300, Loss: 0.1014 | 0.1026
Epoch 190/300, Loss: 0.1006 | 0.1026
Epoch 191/300, Loss: 0.1009 | 0.1026
Epoch 192/300, Loss: 0.1001 | 0.1026
Epoch 193/300, Loss: 0.1007 | 0.1026
Epoch 194/300, Loss: 0.1006 | 0.1026
Epoch 195/300, Loss: 0.1012 | 0.1026
Epoch 196/300, Loss: 0.1016 | 0.1026
Epoch 197/300, Loss: 0.1007 | 0.1026
Epoch 198/300, Loss: 0.1006 | 0.1026
Epoch 199/300, Loss: 0.1009 | 0.1026
Epoch 200/300, Loss: 0.1007 | 0.1026
Epoch 201/300, Loss: 0.1007 | 0.1026
Epoch 202/300, Loss: 0.1011 | 0.1026
Epoch 203/300, Loss: 0.1000 | 0.1026
Epoch 204/300, Loss: 0.1007 | 0.1026
Epoch 205/300, Loss: 0.1005 | 0.1026
Epoch 206/300, Loss: 0.1001 | 0.1026
Epoch 207/300, Loss: 0.1004 | 0.1026
Epoch 208/300, Loss: 0.1006 | 0.1025
Epoch 209/300, Loss: 0.1003 | 0.1025
Epoch 210/300, Loss: 0.1006 | 0.1025
Epoch 211/300, Loss: 0.1003 | 0.1025
Epoch 212/300, Loss: 0.1012 | 0.1025
Epoch 213/300, Loss: 0.1005 | 0.1026
Epoch 214/300, Loss: 0.1012 | 0.1025
Epoch 215/300, Loss: 0.1004 | 0.1025
Epoch 216/300, Loss: 0.1002 | 0.1025
Epoch 217/300, Loss: 0.0998 | 0.1025
Epoch 218/300, Loss: 0.1010 | 0.1025
Epoch 219/300, Loss: 0.1005 | 0.1025
Epoch 220/300, Loss: 0.1010 | 0.1025
Epoch 221/300, Loss: 0.1004 | 0.1025
Epoch 222/300, Loss: 0.1004 | 0.1025
Epoch 223/300, Loss: 0.1005 | 0.1025
Epoch 224/300, Loss: 0.1004 | 0.1025
Epoch 225/300, Loss: 0.1003 | 0.1025
Epoch 226/300, Loss: 0.1009 | 0.1025
Epoch 227/300, Loss: 0.1004 | 0.1025
Epoch 228/300, Loss: 0.1011 | 0.1025
Epoch 229/300, Loss: 0.1003 | 0.1025
Epoch 230/300, Loss: 0.1006 | 0.1025
Epoch 231/300, Loss: 0.1000 | 0.1025
Epoch 232/300, Loss: 0.1008 | 0.1025
Epoch 233/300, Loss: 0.1006 | 0.1025
Epoch 234/300, Loss: 0.1008 | 0.1025
Epoch 235/300, Loss: 0.1009 | 0.1025
Epoch 236/300, Loss: 0.1009 | 0.1025
Epoch 237/300, Loss: 0.1003 | 0.1025
Epoch 238/300, Loss: 0.1009 | 0.1025
Epoch 239/300, Loss: 0.1010 | 0.1025
Epoch 240/300, Loss: 0.1008 | 0.1025
Epoch 241/300, Loss: 0.1004 | 0.1025
Epoch 242/300, Loss: 0.1002 | 0.1025
Epoch 243/300, Loss: 0.1013 | 0.1025
Epoch 244/300, Loss: 0.1005 | 0.1025
Epoch 245/300, Loss: 0.1006 | 0.1025
Epoch 246/300, Loss: 0.1009 | 0.1025
Epoch 247/300, Loss: 0.1008 | 0.1025
Epoch 248/300, Loss: 0.1011 | 0.1025
Epoch 249/300, Loss: 0.1006 | 0.1025
Epoch 250/300, Loss: 0.1007 | 0.1025
Epoch 251/300, Loss: 0.1007 | 0.1025
Epoch 252/300, Loss: 0.1006 | 0.1025
Epoch 253/300, Loss: 0.1005 | 0.1025
Epoch 254/300, Loss: 0.1012 | 0.1025
Epoch 255/300, Loss: 0.1002 | 0.1025
Epoch 256/300, Loss: 0.1003 | 0.1025
Epoch 257/300, Loss: 0.1008 | 0.1025
Epoch 258/300, Loss: 0.1008 | 0.1025
Epoch 259/300, Loss: 0.1005 | 0.1025
Epoch 260/300, Loss: 0.1006 | 0.1025
Epoch 261/300, Loss: 0.1012 | 0.1025
Epoch 262/300, Loss: 0.1004 | 0.1025
Epoch 263/300, Loss: 0.1009 | 0.1025
Epoch 264/300, Loss: 0.1004 | 0.1025
Epoch 265/300, Loss: 0.1002 | 0.1025
Epoch 266/300, Loss: 0.1010 | 0.1025
Epoch 267/300, Loss: 0.0999 | 0.1025
Epoch 268/300, Loss: 0.1007 | 0.1025
Epoch 269/300, Loss: 0.1011 | 0.1025
Epoch 270/300, Loss: 0.1009 | 0.1025
Epoch 271/300, Loss: 0.1006 | 0.1025
Epoch 272/300, Loss: 0.1001 | 0.1025
Epoch 273/300, Loss: 0.1002 | 0.1025
Epoch 274/300, Loss: 0.1011 | 0.1025
Epoch 275/300, Loss: 0.1010 | 0.1025
Epoch 276/300, Loss: 0.1005 | 0.1025
Epoch 277/300, Loss: 0.1007 | 0.1025
Epoch 278/300, Loss: 0.1005 | 0.1025
Epoch 279/300, Loss: 0.1000 | 0.1025
Epoch 280/300, Loss: 0.1003 | 0.1025
Epoch 281/300, Loss: 0.1009 | 0.1025
Epoch 282/300, Loss: 0.1005 | 0.1025
Epoch 283/300, Loss: 0.1013 | 0.1025
Epoch 284/300, Loss: 0.1004 | 0.1025
Epoch 285/300, Loss: 0.1010 | 0.1025
Epoch 286/300, Loss: 0.1010 | 0.1025
Epoch 287/300, Loss: 0.1007 | 0.1025
Epoch 288/300, Loss: 0.0999 | 0.1025
Epoch 289/300, Loss: 0.1009 | 0.1025
Epoch 290/300, Loss: 0.1008 | 0.1025
Epoch 291/300, Loss: 0.1010 | 0.1025
Epoch 292/300, Loss: 0.1004 | 0.1025
Epoch 293/300, Loss: 0.1007 | 0.1025
Epoch 294/300, Loss: 0.1008 | 0.1025
Epoch 295/300, Loss: 0.1005 | 0.1025
Epoch 296/300, Loss: 0.1002 | 0.1025
Epoch 297/300, Loss: 0.1007 | 0.1025
Epoch 298/300, Loss: 0.1005 | 0.1025
Epoch 299/300, Loss: 0.0995 | 0.1025
Epoch 300/300, Loss: 0.1004 | 0.1025
Runtime (seconds): 346.87431621551514
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 788.6570418486372
RMSE: 28.083038330078125
MAE: 28.083038330078125
R-squared: nan
[179.77696]
