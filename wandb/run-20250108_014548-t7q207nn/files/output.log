[32m[I 2025-01-08 01:45:49,399][0m A new study created in memory with name: no-name-3fb34c4a-60ba-449d-a47c-01d1d606bc8d[0m
[32m[I 2025-01-08 01:47:07,314][0m Trial 0 finished with value: 0.20149828641549916 and parameters: {'observation_period_num': 34, 'train_rates': 0.9520924686992593, 'learning_rate': 2.117130536888317e-05, 'batch_size': 61, 'step_size': 4, 'gamma': 0.8820419640780522}. Best is trial 0 with value: 0.20149828641549916.[0m
Early stopping at epoch 42
[32m[I 2025-01-08 01:49:34,679][0m Trial 1 finished with value: 1.676959857113003 and parameters: {'observation_period_num': 235, 'train_rates': 0.8589099517337407, 'learning_rate': 2.0703780633416485e-06, 'batch_size': 93, 'step_size': 1, 'gamma': 0.7738762196466236}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 01:52:45,391][0m Trial 2 finished with value: 0.23462551968698284 and parameters: {'observation_period_num': 128, 'train_rates': 0.861931221891006, 'learning_rate': 6.272465508125743e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.778037863613165}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 01:56:43,447][0m Trial 3 finished with value: 1.315143833932221 and parameters: {'observation_period_num': 169, 'train_rates': 0.8390080152477758, 'learning_rate': 0.0006439293884700898, 'batch_size': 45, 'step_size': 12, 'gamma': 0.9003869605797187}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:02:30,632][0m Trial 4 finished with value: 0.432903273368445 and parameters: {'observation_period_num': 217, 'train_rates': 0.9539928370064843, 'learning_rate': 0.0005467266322072921, 'batch_size': 46, 'step_size': 3, 'gamma': 0.877497832628675}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:04:11,274][0m Trial 5 finished with value: 0.6621574435679558 and parameters: {'observation_period_num': 80, 'train_rates': 0.8002496057164271, 'learning_rate': 3.695532321056464e-06, 'batch_size': 91, 'step_size': 14, 'gamma': 0.832342749752164}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:06:59,658][0m Trial 6 finished with value: 0.8658950068056583 and parameters: {'observation_period_num': 129, 'train_rates': 0.8312953064961479, 'learning_rate': 5.77309243158828e-06, 'batch_size': 235, 'step_size': 9, 'gamma': 0.9132709391742155}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:08:56,122][0m Trial 7 finished with value: 1.0315418944639319 and parameters: {'observation_period_num': 110, 'train_rates': 0.6496138850339568, 'learning_rate': 3.6565876561183345e-05, 'batch_size': 236, 'step_size': 13, 'gamma': 0.8810638931116604}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:10:18,148][0m Trial 8 finished with value: 0.6845168078199346 and parameters: {'observation_period_num': 68, 'train_rates': 0.7573278890808339, 'learning_rate': 0.00026224428409986887, 'batch_size': 96, 'step_size': 7, 'gamma': 0.777772032603714}. Best is trial 0 with value: 0.20149828641549916.[0m
[32m[I 2025-01-08 02:14:25,162][0m Trial 9 finished with value: 0.15050718188285828 and parameters: {'observation_period_num': 167, 'train_rates': 0.9754380124194901, 'learning_rate': 0.0001131852761637054, 'batch_size': 221, 'step_size': 9, 'gamma': 0.9337310513881456}. Best is trial 9 with value: 0.15050718188285828.[0m
[32m[I 2025-01-08 02:19:08,987][0m Trial 10 finished with value: 0.15560147166252136 and parameters: {'observation_period_num': 188, 'train_rates': 0.987726188080463, 'learning_rate': 0.0001238037219519824, 'batch_size': 177, 'step_size': 9, 'gamma': 0.9780502889710061}. Best is trial 9 with value: 0.15050718188285828.[0m
[32m[I 2025-01-08 02:23:54,181][0m Trial 11 finished with value: 0.14082907140254974 and parameters: {'observation_period_num': 189, 'train_rates': 0.9670171370212176, 'learning_rate': 0.00012828968658462912, 'batch_size': 174, 'step_size': 9, 'gamma': 0.9896503801038306}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:27:47,408][0m Trial 12 finished with value: 0.17775490688674644 and parameters: {'observation_period_num': 168, 'train_rates': 0.9084876596297095, 'learning_rate': 0.00013400918814934534, 'batch_size': 177, 'step_size': 7, 'gamma': 0.9821742671086574}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:32:04,378][0m Trial 13 finished with value: 1.0141124058710902 and parameters: {'observation_period_num': 204, 'train_rates': 0.7252911597722689, 'learning_rate': 1.4601143902374947e-05, 'batch_size': 186, 'step_size': 11, 'gamma': 0.9441651775896929}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:35:46,022][0m Trial 14 finished with value: 0.18642037183292642 and parameters: {'observation_period_num': 157, 'train_rates': 0.913991854965142, 'learning_rate': 9.073299353816024e-05, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9448005188749272}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:42:05,641][0m Trial 15 finished with value: 0.1726338319049394 and parameters: {'observation_period_num': 249, 'train_rates': 0.9112500154532839, 'learning_rate': 0.000220809270751797, 'batch_size': 210, 'step_size': 5, 'gamma': 0.9417860594458208}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:45:42,397][0m Trial 16 finished with value: 0.24160544574260712 and parameters: {'observation_period_num': 145, 'train_rates': 0.9890398737671084, 'learning_rate': 4.271923613304234e-05, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9877882435315132}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:49:22,095][0m Trial 17 finished with value: 1.1274431897089408 and parameters: {'observation_period_num': 194, 'train_rates': 0.6194982102337029, 'learning_rate': 1.2574334425838596e-05, 'batch_size': 149, 'step_size': 8, 'gamma': 0.8362585062928862}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:51:06,350][0m Trial 18 finished with value: 1.0663927859573994 and parameters: {'observation_period_num': 95, 'train_rates': 0.7052962972572607, 'learning_rate': 0.0009510907658357574, 'batch_size': 205, 'step_size': 11, 'gamma': 0.9211549697829697}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:56:43,958][0m Trial 19 finished with value: 0.204531889016691 and parameters: {'observation_period_num': 221, 'train_rates': 0.9448276039394139, 'learning_rate': 0.000323635335449161, 'batch_size': 125, 'step_size': 15, 'gamma': 0.968720313831362}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 02:57:12,046][0m Trial 20 finished with value: 0.20230459860680294 and parameters: {'observation_period_num': 18, 'train_rates': 0.8897112890760065, 'learning_rate': 6.529549669806194e-05, 'batch_size': 209, 'step_size': 9, 'gamma': 0.8441526251421128}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:01:55,874][0m Trial 21 finished with value: 0.16912415623664856 and parameters: {'observation_period_num': 186, 'train_rates': 0.9862423519432831, 'learning_rate': 0.0001513956116614824, 'batch_size': 169, 'step_size': 9, 'gamma': 0.9660923734905922}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:06:23,070][0m Trial 22 finished with value: 0.14562956988811493 and parameters: {'observation_period_num': 182, 'train_rates': 0.9563003099668892, 'learning_rate': 0.00012750441471283457, 'batch_size': 164, 'step_size': 11, 'gamma': 0.9612410172468279}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:09:47,165][0m Trial 23 finished with value: 0.18053185976133113 and parameters: {'observation_period_num': 144, 'train_rates': 0.9422362806313881, 'learning_rate': 0.00038731473194667426, 'batch_size': 117, 'step_size': 11, 'gamma': 0.9553751921238305}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:13:47,104][0m Trial 24 finished with value: 0.21675012883900097 and parameters: {'observation_period_num': 175, 'train_rates': 0.886274372317337, 'learning_rate': 0.00018831535607888097, 'batch_size': 160, 'step_size': 7, 'gamma': 0.928793840651691}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:18:55,118][0m Trial 25 finished with value: 0.1600649505853653 and parameters: {'observation_period_num': 202, 'train_rates': 0.9601430499569459, 'learning_rate': 7.602189210292069e-05, 'batch_size': 190, 'step_size': 12, 'gamma': 0.9593903793522746}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:24:29,126][0m Trial 26 finished with value: 0.24024993181228638 and parameters: {'observation_period_num': 219, 'train_rates': 0.9273734423050245, 'learning_rate': 2.6947207727596793e-05, 'batch_size': 225, 'step_size': 10, 'gamma': 0.9050095717547252}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:27:52,050][0m Trial 27 finished with value: 0.3803639309341783 and parameters: {'observation_period_num': 158, 'train_rates': 0.8095690347750387, 'learning_rate': 4.4374508066118574e-05, 'batch_size': 155, 'step_size': 8, 'gamma': 0.9894177449874364}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:30:48,535][0m Trial 28 finished with value: 0.21478396902481714 and parameters: {'observation_period_num': 130, 'train_rates': 0.8817089063927779, 'learning_rate': 0.0001107891409401172, 'batch_size': 197, 'step_size': 12, 'gamma': 0.9410564201271695}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:37:00,449][0m Trial 29 finished with value: 0.6300417184829712 and parameters: {'observation_period_num': 236, 'train_rates': 0.9644221565480605, 'learning_rate': 1.6866243247663464e-05, 'batch_size': 221, 'step_size': 4, 'gamma': 0.860671740776206}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:40:43,981][0m Trial 30 finished with value: 0.5397178446742851 and parameters: {'observation_period_num': 178, 'train_rates': 0.7711819571044208, 'learning_rate': 0.00040584284182698286, 'batch_size': 126, 'step_size': 10, 'gamma': 0.7507430196486429}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:45:30,168][0m Trial 31 finished with value: 0.1812334954738617 and parameters: {'observation_period_num': 192, 'train_rates': 0.9853884232728092, 'learning_rate': 0.00016170773137549944, 'batch_size': 175, 'step_size': 9, 'gamma': 0.9696271970009714}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:49:07,833][0m Trial 32 finished with value: 0.9189743399620056 and parameters: {'observation_period_num': 149, 'train_rates': 0.9668809814301625, 'learning_rate': 1.0693361324355188e-06, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9764136196057334}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:54:19,036][0m Trial 33 finished with value: 0.1528286188840866 and parameters: {'observation_period_num': 208, 'train_rates': 0.9312325829993278, 'learning_rate': 0.00011389604319179878, 'batch_size': 249, 'step_size': 6, 'gamma': 0.9559222980265834}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 03:59:34,469][0m Trial 34 finished with value: 0.23059014976024628 and parameters: {'observation_period_num': 210, 'train_rates': 0.9344031641382792, 'learning_rate': 5.7895301476298705e-05, 'batch_size': 240, 'step_size': 6, 'gamma': 0.8965061244558353}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:05:13,726][0m Trial 35 finished with value: 0.5449040683222489 and parameters: {'observation_period_num': 231, 'train_rates': 0.8704984532110105, 'learning_rate': 2.579880971233402e-05, 'batch_size': 255, 'step_size': 2, 'gamma': 0.9336137902327093}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:09:12,381][0m Trial 36 finished with value: 0.17655915021896362 and parameters: {'observation_period_num': 166, 'train_rates': 0.9261759468550934, 'learning_rate': 8.70469862642077e-05, 'batch_size': 221, 'step_size': 5, 'gamma': 0.950864792269875}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:15:41,251][0m Trial 37 finished with value: 0.18657319247722626 and parameters: {'observation_period_num': 246, 'train_rates': 0.9597422852760461, 'learning_rate': 0.0006412230144744046, 'batch_size': 243, 'step_size': 13, 'gamma': 0.9213914180009941}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:18:05,973][0m Trial 38 finished with value: 0.7165291315101715 and parameters: {'observation_period_num': 115, 'train_rates': 0.8547402141252654, 'learning_rate': 8.106612621237075e-06, 'batch_size': 201, 'step_size': 6, 'gamma': 0.8088299994087113}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:19:27,769][0m Trial 39 finished with value: 0.22682703546669386 and parameters: {'observation_period_num': 58, 'train_rates': 0.8997385622281696, 'learning_rate': 0.000267076440592796, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9588750656439172}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:23:25,346][0m Trial 40 finished with value: 0.6251379512558298 and parameters: {'observation_period_num': 180, 'train_rates': 0.8268802156223447, 'learning_rate': 5.7080819385278834e-05, 'batch_size': 226, 'step_size': 1, 'gamma': 0.8942214359170906}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:28:20,648][0m Trial 41 finished with value: 0.151622012257576 and parameters: {'observation_period_num': 194, 'train_rates': 0.9748850068333509, 'learning_rate': 0.00011478089088272243, 'batch_size': 184, 'step_size': 10, 'gamma': 0.9733487428143748}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:33:29,961][0m Trial 42 finished with value: 0.15497680008411407 and parameters: {'observation_period_num': 201, 'train_rates': 0.9535246265061159, 'learning_rate': 0.00011707094800089687, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9725072493098966}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:39:45,421][0m Trial 43 finished with value: 0.17305991380679897 and parameters: {'observation_period_num': 219, 'train_rates': 0.9704865030087962, 'learning_rate': 0.00023234413189875377, 'batch_size': 20, 'step_size': 8, 'gamma': 0.9587841559398681}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:43:37,648][0m Trial 44 finished with value: 0.15600637951134388 and parameters: {'observation_period_num': 162, 'train_rates': 0.9322590766433424, 'learning_rate': 9.354064235686619e-05, 'batch_size': 137, 'step_size': 12, 'gamma': 0.980559650183555}. Best is trial 11 with value: 0.14082907140254974.[0m
[32m[I 2025-01-08 04:48:59,108][0m Trial 45 finished with value: 0.13754025101661682 and parameters: {'observation_period_num': 208, 'train_rates': 0.9719157455520252, 'learning_rate': 0.00017915685321046612, 'batch_size': 216, 'step_size': 7, 'gamma': 0.9357016620540933}. Best is trial 45 with value: 0.13754025101661682.[0m
[32m[I 2025-01-08 04:53:56,418][0m Trial 46 finished with value: 0.16853854060173035 and parameters: {'observation_period_num': 194, 'train_rates': 0.975820148020033, 'learning_rate': 0.0004784365272353731, 'batch_size': 186, 'step_size': 11, 'gamma': 0.9280916890663027}. Best is trial 45 with value: 0.13754025101661682.[0m
[32m[I 2025-01-08 04:59:59,277][0m Trial 47 finished with value: 0.13559025526046753 and parameters: {'observation_period_num': 231, 'train_rates': 0.9492792651933809, 'learning_rate': 0.0003138123643778718, 'batch_size': 216, 'step_size': 7, 'gamma': 0.911211354706431}. Best is trial 47 with value: 0.13559025526046753.[0m
[32m[I 2025-01-08 05:05:41,144][0m Trial 48 finished with value: 0.16251686025173107 and parameters: {'observation_period_num': 228, 'train_rates': 0.9145446823900373, 'learning_rate': 0.00032724747059367093, 'batch_size': 210, 'step_size': 7, 'gamma': 0.9051145206581006}. Best is trial 47 with value: 0.13559025526046753.[0m
[32m[I 2025-01-08 05:12:31,854][0m Trial 49 finished with value: 2.0905282084147134 and parameters: {'observation_period_num': 252, 'train_rates': 0.9449036544478762, 'learning_rate': 0.0009942386026825455, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9139567889294262}. Best is trial 47 with value: 0.13559025526046753.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 1.0198 | 0.9467
Epoch 2/300, Loss: 0.8035 | 0.7031
Epoch 3/300, Loss: 0.7138 | 0.6461
Epoch 4/300, Loss: 0.5746 | 0.5688
Epoch 5/300, Loss: 0.4831 | 0.5525
Epoch 6/300, Loss: 0.4288 | 0.4890
Epoch 7/300, Loss: 0.4232 | 0.4628
Epoch 8/300, Loss: 0.3536 | 0.4100
Epoch 9/300, Loss: 0.3272 | 0.3959
Epoch 10/300, Loss: 0.3003 | 0.3429
Epoch 11/300, Loss: 0.4005 | 0.3423
Epoch 12/300, Loss: 0.3594 | 0.3178
Epoch 13/300, Loss: 0.2973 | 0.3426
Epoch 14/300, Loss: 0.2680 | 0.3313
Epoch 15/300, Loss: 0.2487 | 0.2750
Epoch 16/300, Loss: 0.2723 | 0.2720
Epoch 17/300, Loss: 0.2916 | 0.2738
Epoch 18/300, Loss: 0.2793 | 0.3116
Epoch 19/300, Loss: 0.2312 | 0.2509
Epoch 20/300, Loss: 0.2117 | 0.2477
Epoch 21/300, Loss: 0.2000 | 0.2314
Epoch 22/300, Loss: 0.2025 | 0.2356
Epoch 23/300, Loss: 0.1982 | 0.2193
Epoch 24/300, Loss: 0.1963 | 0.2256
Epoch 25/300, Loss: 0.1993 | 0.2145
Epoch 26/300, Loss: 0.1921 | 0.2198
Epoch 27/300, Loss: 0.1858 | 0.2075
Epoch 28/300, Loss: 0.1766 | 0.2078
Epoch 29/300, Loss: 0.1715 | 0.2018
Epoch 30/300, Loss: 0.1692 | 0.1995
Epoch 31/300, Loss: 0.1689 | 0.1986
Epoch 32/300, Loss: 0.1688 | 0.1939
Epoch 33/300, Loss: 0.1696 | 0.1946
Epoch 34/300, Loss: 0.1742 | 0.1917
Epoch 35/300, Loss: 0.1802 | 0.1979
Epoch 36/300, Loss: 0.1799 | 0.1942
Epoch 37/300, Loss: 0.1754 | 0.1903
Epoch 38/300, Loss: 0.1669 | 0.1916
Epoch 39/300, Loss: 0.1650 | 0.1877
Epoch 40/300, Loss: 0.1623 | 0.1863
Epoch 41/300, Loss: 0.1598 | 0.1848
Epoch 42/300, Loss: 0.1575 | 0.1839
Epoch 43/300, Loss: 0.1571 | 0.1818
Epoch 44/300, Loss: 0.1550 | 0.1792
Epoch 45/300, Loss: 0.1537 | 0.1784
Epoch 46/300, Loss: 0.1528 | 0.1766
Epoch 47/300, Loss: 0.1514 | 0.1774
Epoch 48/300, Loss: 0.1507 | 0.1752
Epoch 49/300, Loss: 0.1500 | 0.1773
Epoch 50/300, Loss: 0.1490 | 0.1734
Epoch 51/300, Loss: 0.1486 | 0.1739
Epoch 52/300, Loss: 0.1476 | 0.1717
Epoch 53/300, Loss: 0.1463 | 0.1720
Epoch 54/300, Loss: 0.1456 | 0.1698
Epoch 55/300, Loss: 0.1446 | 0.1696
Epoch 56/300, Loss: 0.1446 | 0.1681
Epoch 57/300, Loss: 0.1430 | 0.1684
Epoch 58/300, Loss: 0.1433 | 0.1674
Epoch 59/300, Loss: 0.1430 | 0.1665
Epoch 60/300, Loss: 0.1421 | 0.1651
Epoch 61/300, Loss: 0.1409 | 0.1656
Epoch 62/300, Loss: 0.1411 | 0.1632
Epoch 63/300, Loss: 0.1403 | 0.1627
Epoch 64/300, Loss: 0.1398 | 0.1605
Epoch 65/300, Loss: 0.1395 | 0.1623
Epoch 66/300, Loss: 0.1384 | 0.1620
Epoch 67/300, Loss: 0.1385 | 0.1604
Epoch 68/300, Loss: 0.1384 | 0.1594
Epoch 69/300, Loss: 0.1374 | 0.1595
Epoch 70/300, Loss: 0.1371 | 0.1589
Epoch 71/300, Loss: 0.1370 | 0.1574
Epoch 72/300, Loss: 0.1357 | 0.1580
Epoch 73/300, Loss: 0.1355 | 0.1568
Epoch 74/300, Loss: 0.1356 | 0.1565
Epoch 75/300, Loss: 0.1346 | 0.1564
Epoch 76/300, Loss: 0.1348 | 0.1551
Epoch 77/300, Loss: 0.1344 | 0.1546
Epoch 78/300, Loss: 0.1342 | 0.1540
Epoch 79/300, Loss: 0.1338 | 0.1538
Epoch 80/300, Loss: 0.1329 | 0.1537
Epoch 81/300, Loss: 0.1345 | 0.1531
Epoch 82/300, Loss: 0.1330 | 0.1537
Epoch 83/300, Loss: 0.1324 | 0.1524
Epoch 84/300, Loss: 0.1317 | 0.1533
Epoch 85/300, Loss: 0.1322 | 0.1517
Epoch 86/300, Loss: 0.1322 | 0.1512
Epoch 87/300, Loss: 0.1314 | 0.1510
Epoch 88/300, Loss: 0.1318 | 0.1502
Epoch 89/300, Loss: 0.1310 | 0.1502
Epoch 90/300, Loss: 0.1308 | 0.1500
Epoch 91/300, Loss: 0.1304 | 0.1505
Epoch 92/300, Loss: 0.1307 | 0.1493
Epoch 93/300, Loss: 0.1301 | 0.1491
Epoch 94/300, Loss: 0.1297 | 0.1493
Epoch 95/300, Loss: 0.1296 | 0.1490
Epoch 96/300, Loss: 0.1298 | 0.1493
Epoch 97/300, Loss: 0.1291 | 0.1483
Epoch 98/300, Loss: 0.1287 | 0.1482
Epoch 99/300, Loss: 0.1287 | 0.1481
Epoch 100/300, Loss: 0.1290 | 0.1475
Epoch 101/300, Loss: 0.1283 | 0.1470
Epoch 102/300, Loss: 0.1288 | 0.1469
Epoch 103/300, Loss: 0.1283 | 0.1475
Epoch 104/300, Loss: 0.1280 | 0.1470
Epoch 105/300, Loss: 0.1284 | 0.1465
Epoch 106/300, Loss: 0.1280 | 0.1463
Epoch 107/300, Loss: 0.1283 | 0.1465
Epoch 108/300, Loss: 0.1278 | 0.1466
Epoch 109/300, Loss: 0.1275 | 0.1455
Epoch 110/300, Loss: 0.1269 | 0.1453
Epoch 111/300, Loss: 0.1268 | 0.1458
Epoch 112/300, Loss: 0.1267 | 0.1461
Epoch 113/300, Loss: 0.1272 | 0.1453
Epoch 114/300, Loss: 0.1265 | 0.1448
Epoch 115/300, Loss: 0.1267 | 0.1455
Epoch 116/300, Loss: 0.1266 | 0.1455
Epoch 117/300, Loss: 0.1266 | 0.1453
Epoch 118/300, Loss: 0.1261 | 0.1454
Epoch 119/300, Loss: 0.1257 | 0.1450
Epoch 120/300, Loss: 0.1253 | 0.1446
Epoch 121/300, Loss: 0.1261 | 0.1441
Epoch 122/300, Loss: 0.1262 | 0.1441
Epoch 123/300, Loss: 0.1259 | 0.1440
Epoch 124/300, Loss: 0.1257 | 0.1440
Epoch 125/300, Loss: 0.1249 | 0.1438
Epoch 126/300, Loss: 0.1251 | 0.1439
Epoch 127/300, Loss: 0.1247 | 0.1437
Epoch 128/300, Loss: 0.1252 | 0.1435
Epoch 129/300, Loss: 0.1253 | 0.1432
Epoch 130/300, Loss: 0.1249 | 0.1435
Epoch 131/300, Loss: 0.1248 | 0.1434
Epoch 132/300, Loss: 0.1255 | 0.1430
Epoch 133/300, Loss: 0.1247 | 0.1434
Epoch 134/300, Loss: 0.1247 | 0.1433
Epoch 135/300, Loss: 0.1245 | 0.1430
Epoch 136/300, Loss: 0.1241 | 0.1429
Epoch 137/300, Loss: 0.1245 | 0.1430
Epoch 138/300, Loss: 0.1244 | 0.1432
Epoch 139/300, Loss: 0.1248 | 0.1433
Epoch 140/300, Loss: 0.1239 | 0.1430
Epoch 141/300, Loss: 0.1245 | 0.1427
Epoch 142/300, Loss: 0.1243 | 0.1425
Epoch 143/300, Loss: 0.1240 | 0.1423
Epoch 144/300, Loss: 0.1243 | 0.1423
Epoch 145/300, Loss: 0.1236 | 0.1424
Epoch 146/300, Loss: 0.1236 | 0.1427
Epoch 147/300, Loss: 0.1244 | 0.1426
Epoch 148/300, Loss: 0.1234 | 0.1424
Epoch 149/300, Loss: 0.1240 | 0.1423
Epoch 150/300, Loss: 0.1232 | 0.1421
Epoch 151/300, Loss: 0.1234 | 0.1421
Epoch 152/300, Loss: 0.1237 | 0.1419
Epoch 153/300, Loss: 0.1230 | 0.1420
Epoch 154/300, Loss: 0.1243 | 0.1422
Epoch 155/300, Loss: 0.1234 | 0.1423
Epoch 156/300, Loss: 0.1231 | 0.1423
Epoch 157/300, Loss: 0.1233 | 0.1424
Epoch 158/300, Loss: 0.1238 | 0.1421
Epoch 159/300, Loss: 0.1232 | 0.1420
Epoch 160/300, Loss: 0.1235 | 0.1418
Epoch 161/300, Loss: 0.1233 | 0.1416
Epoch 162/300, Loss: 0.1235 | 0.1414
Epoch 163/300, Loss: 0.1231 | 0.1414
Epoch 164/300, Loss: 0.1236 | 0.1414
Epoch 165/300, Loss: 0.1229 | 0.1412
Epoch 166/300, Loss: 0.1236 | 0.1413
Epoch 167/300, Loss: 0.1228 | 0.1410
Epoch 168/300, Loss: 0.1233 | 0.1407
Epoch 169/300, Loss: 0.1226 | 0.1406
Epoch 170/300, Loss: 0.1234 | 0.1408
Epoch 171/300, Loss: 0.1223 | 0.1408
Epoch 172/300, Loss: 0.1233 | 0.1406
Epoch 173/300, Loss: 0.1226 | 0.1406
Epoch 174/300, Loss: 0.1231 | 0.1406
Epoch 175/300, Loss: 0.1222 | 0.1407
Epoch 176/300, Loss: 0.1228 | 0.1407
Epoch 177/300, Loss: 0.1228 | 0.1408
Epoch 178/300, Loss: 0.1218 | 0.1409
Epoch 179/300, Loss: 0.1224 | 0.1408
Epoch 180/300, Loss: 0.1221 | 0.1408
Epoch 181/300, Loss: 0.1226 | 0.1409
Epoch 182/300, Loss: 0.1228 | 0.1410
Epoch 183/300, Loss: 0.1220 | 0.1409
Epoch 184/300, Loss: 0.1225 | 0.1408
Epoch 185/300, Loss: 0.1221 | 0.1407
Epoch 186/300, Loss: 0.1221 | 0.1406
Epoch 187/300, Loss: 0.1224 | 0.1406
Epoch 188/300, Loss: 0.1223 | 0.1406
Epoch 189/300, Loss: 0.1227 | 0.1404
Epoch 190/300, Loss: 0.1223 | 0.1404
Epoch 191/300, Loss: 0.1226 | 0.1405
Epoch 192/300, Loss: 0.1229 | 0.1405
Epoch 193/300, Loss: 0.1223 | 0.1404
Epoch 194/300, Loss: 0.1226 | 0.1402
Epoch 195/300, Loss: 0.1225 | 0.1401
Epoch 196/300, Loss: 0.1226 | 0.1401
Epoch 197/300, Loss: 0.1221 | 0.1401
Epoch 198/300, Loss: 0.1227 | 0.1401
Epoch 199/300, Loss: 0.1222 | 0.1400
Epoch 200/300, Loss: 0.1219 | 0.1399
Epoch 201/300, Loss: 0.1228 | 0.1400
Epoch 202/300, Loss: 0.1224 | 0.1400
Epoch 203/300, Loss: 0.1218 | 0.1400
Epoch 204/300, Loss: 0.1217 | 0.1400
Epoch 205/300, Loss: 0.1222 | 0.1400
Epoch 206/300, Loss: 0.1226 | 0.1400
Epoch 207/300, Loss: 0.1217 | 0.1400
Epoch 208/300, Loss: 0.1219 | 0.1400
Epoch 209/300, Loss: 0.1219 | 0.1400
Epoch 210/300, Loss: 0.1218 | 0.1400
Epoch 211/300, Loss: 0.1220 | 0.1400
Epoch 212/300, Loss: 0.1221 | 0.1400
Epoch 213/300, Loss: 0.1219 | 0.1399
Epoch 214/300, Loss: 0.1221 | 0.1399
Epoch 215/300, Loss: 0.1222 | 0.1399
Epoch 216/300, Loss: 0.1223 | 0.1400
Epoch 217/300, Loss: 0.1222 | 0.1400
Epoch 218/300, Loss: 0.1222 | 0.1400
Epoch 219/300, Loss: 0.1221 | 0.1400
Epoch 220/300, Loss: 0.1226 | 0.1400
Epoch 221/300, Loss: 0.1220 | 0.1400
Epoch 222/300, Loss: 0.1218 | 0.1400
Epoch 223/300, Loss: 0.1216 | 0.1399
Epoch 224/300, Loss: 0.1226 | 0.1399
Epoch 225/300, Loss: 0.1217 | 0.1399
Epoch 226/300, Loss: 0.1222 | 0.1399
Epoch 227/300, Loss: 0.1219 | 0.1399
Epoch 228/300, Loss: 0.1220 | 0.1399
Epoch 229/300, Loss: 0.1218 | 0.1399
Epoch 230/300, Loss: 0.1216 | 0.1399
Epoch 231/300, Loss: 0.1218 | 0.1398
Epoch 232/300, Loss: 0.1221 | 0.1398
Epoch 233/300, Loss: 0.1218 | 0.1399
Epoch 234/300, Loss: 0.1215 | 0.1399
Epoch 235/300, Loss: 0.1217 | 0.1399
Epoch 236/300, Loss: 0.1216 | 0.1399
Epoch 237/300, Loss: 0.1220 | 0.1399
Epoch 238/300, Loss: 0.1217 | 0.1398
Epoch 239/300, Loss: 0.1218 | 0.1398
Epoch 240/300, Loss: 0.1218 | 0.1398
Epoch 241/300, Loss: 0.1219 | 0.1398
Epoch 242/300, Loss: 0.1216 | 0.1398
Epoch 243/300, Loss: 0.1222 | 0.1398
Epoch 244/300, Loss: 0.1214 | 0.1398
Epoch 245/300, Loss: 0.1218 | 0.1398
Epoch 246/300, Loss: 0.1214 | 0.1398
Epoch 247/300, Loss: 0.1220 | 0.1398
Epoch 248/300, Loss: 0.1216 | 0.1398
Epoch 249/300, Loss: 0.1214 | 0.1398
Epoch 250/300, Loss: 0.1221 | 0.1398
Epoch 251/300, Loss: 0.1212 | 0.1398
Epoch 252/300, Loss: 0.1221 | 0.1398
Epoch 253/300, Loss: 0.1219 | 0.1398
Epoch 254/300, Loss: 0.1215 | 0.1397
Epoch 255/300, Loss: 0.1221 | 0.1397
Epoch 256/300, Loss: 0.1214 | 0.1397
Epoch 257/300, Loss: 0.1218 | 0.1397
Epoch 258/300, Loss: 0.1215 | 0.1397
Epoch 259/300, Loss: 0.1223 | 0.1397
Epoch 260/300, Loss: 0.1211 | 0.1397
Epoch 261/300, Loss: 0.1215 | 0.1397
Epoch 262/300, Loss: 0.1216 | 0.1397
Epoch 263/300, Loss: 0.1215 | 0.1397
Epoch 264/300, Loss: 0.1221 | 0.1396
Epoch 265/300, Loss: 0.1214 | 0.1396
Epoch 266/300, Loss: 0.1214 | 0.1396
Epoch 267/300, Loss: 0.1216 | 0.1396
Epoch 268/300, Loss: 0.1221 | 0.1397
Epoch 269/300, Loss: 0.1219 | 0.1397
Epoch 270/300, Loss: 0.1217 | 0.1397
Epoch 271/300, Loss: 0.1218 | 0.1397
Epoch 272/300, Loss: 0.1215 | 0.1397
Epoch 273/300, Loss: 0.1215 | 0.1397
Epoch 274/300, Loss: 0.1218 | 0.1397
Epoch 275/300, Loss: 0.1215 | 0.1397
Epoch 276/300, Loss: 0.1221 | 0.1396
Epoch 277/300, Loss: 0.1213 | 0.1396
Epoch 278/300, Loss: 0.1220 | 0.1396
Epoch 279/300, Loss: 0.1214 | 0.1396
Epoch 280/300, Loss: 0.1210 | 0.1396
Epoch 281/300, Loss: 0.1211 | 0.1396
Epoch 282/300, Loss: 0.1214 | 0.1396
Epoch 283/300, Loss: 0.1216 | 0.1396
Epoch 284/300, Loss: 0.1217 | 0.1396
Epoch 285/300, Loss: 0.1215 | 0.1396
Epoch 286/300, Loss: 0.1218 | 0.1396
Epoch 287/300, Loss: 0.1218 | 0.1396
Epoch 288/300, Loss: 0.1215 | 0.1396
Epoch 289/300, Loss: 0.1219 | 0.1396
Epoch 290/300, Loss: 0.1215 | 0.1396
Epoch 291/300, Loss: 0.1216 | 0.1396
Epoch 292/300, Loss: 0.1215 | 0.1396
Epoch 293/300, Loss: 0.1217 | 0.1396
Epoch 294/300, Loss: 0.1216 | 0.1396
Epoch 295/300, Loss: 0.1212 | 0.1396
Epoch 296/300, Loss: 0.1214 | 0.1396
Epoch 297/300, Loss: 0.1218 | 0.1396
Epoch 298/300, Loss: 0.1212 | 0.1396
Epoch 299/300, Loss: 0.1220 | 0.1396
Epoch 300/300, Loss: 0.1213 | 0.1396
Runtime (seconds): 1079.753634929657
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1803.3744059139863
RMSE: 42.466156005859375
MAE: 42.466156005859375
R-squared: nan
[182.59384]
