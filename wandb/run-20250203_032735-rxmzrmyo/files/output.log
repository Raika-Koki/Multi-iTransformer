[32m[I 2025-02-03 03:27:39,375][0m A new study created in memory with name: no-name-c85f8bea-66ea-42bd-b7f6-aeceb8dd0e0a[0m
[32m[I 2025-02-03 03:28:33,512][0m Trial 0 finished with value: 0.08352813124656677 and parameters: {'observation_period_num': 189, 'train_rates': 0.9627032994183171, 'learning_rate': 7.919231940644544e-05, 'batch_size': 110, 'step_size': 6, 'gamma': 0.972846780176329}. Best is trial 0 with value: 0.08352813124656677.[0m
[32m[I 2025-02-03 03:29:11,343][0m Trial 1 finished with value: 0.2493221939465646 and parameters: {'observation_period_num': 68, 'train_rates': 0.6378384398360102, 'learning_rate': 5.097597713661768e-06, 'batch_size': 130, 'step_size': 3, 'gamma': 0.973554599067795}. Best is trial 0 with value: 0.08352813124656677.[0m
[32m[I 2025-02-03 03:29:32,311][0m Trial 2 finished with value: 0.04539128531270503 and parameters: {'observation_period_num': 10, 'train_rates': 0.6632304762219983, 'learning_rate': 0.0002675289669017138, 'batch_size': 249, 'step_size': 5, 'gamma': 0.8866225504685048}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:30:09,141][0m Trial 3 finished with value: 0.06426642035533275 and parameters: {'observation_period_num': 6, 'train_rates': 0.6199942834764804, 'learning_rate': 3.7775047265258185e-05, 'batch_size': 137, 'step_size': 2, 'gamma': 0.9666669643800655}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:30:43,946][0m Trial 4 finished with value: 0.1426475706665996 and parameters: {'observation_period_num': 243, 'train_rates': 0.891876123498724, 'learning_rate': 0.00016729267494456588, 'batch_size': 169, 'step_size': 2, 'gamma': 0.823164168059843}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:31:20,613][0m Trial 5 finished with value: 0.0679407382183137 and parameters: {'observation_period_num': 93, 'train_rates': 0.7177552717331649, 'learning_rate': 7.778931797960938e-05, 'batch_size': 138, 'step_size': 6, 'gamma': 0.9478214305248727}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:31:50,641][0m Trial 6 finished with value: 0.04627228270455628 and parameters: {'observation_period_num': 72, 'train_rates': 0.8997282267788014, 'learning_rate': 0.00020207392121219096, 'batch_size': 205, 'step_size': 11, 'gamma': 0.8508744818249806}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:32:27,807][0m Trial 7 finished with value: 0.07163911873937735 and parameters: {'observation_period_num': 119, 'train_rates': 0.6889697079913704, 'learning_rate': 0.00020767359408530652, 'batch_size': 135, 'step_size': 7, 'gamma': 0.8238890642147848}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:33:04,322][0m Trial 8 finished with value: 0.7210515647627121 and parameters: {'observation_period_num': 201, 'train_rates': 0.8700280118264441, 'learning_rate': 1.0273590295423233e-06, 'batch_size': 157, 'step_size': 5, 'gamma': 0.826867328986243}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:33:27,806][0m Trial 9 finished with value: 0.07852145290935587 and parameters: {'observation_period_num': 150, 'train_rates': 0.6740399849830786, 'learning_rate': 0.0004956285926040402, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8053183908361757}. Best is trial 2 with value: 0.04539128531270503.[0m
[32m[I 2025-02-03 03:35:52,072][0m Trial 10 finished with value: 0.02507299233683094 and parameters: {'observation_period_num': 15, 'train_rates': 0.7744547878783756, 'learning_rate': 0.0007976592971336622, 'batch_size': 36, 'step_size': 15, 'gamma': 0.7561167230705764}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:38:04,549][0m Trial 11 finished with value: 0.02579203680514881 and parameters: {'observation_period_num': 5, 'train_rates': 0.777180781941161, 'learning_rate': 0.0008067371849169, 'batch_size': 39, 'step_size': 15, 'gamma': 0.7522136898443855}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:41:13,934][0m Trial 12 finished with value: 0.029563066491753047 and parameters: {'observation_period_num': 40, 'train_rates': 0.7789442205371877, 'learning_rate': 0.000957292402160356, 'batch_size': 27, 'step_size': 15, 'gamma': 0.7507667643757567}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:45:48,036][0m Trial 13 finished with value: 0.03398022661606471 and parameters: {'observation_period_num': 47, 'train_rates': 0.7676598651046904, 'learning_rate': 0.0009357830955740342, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7545184277061758}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:47:14,423][0m Trial 14 finished with value: 0.06142272885476447 and parameters: {'observation_period_num': 30, 'train_rates': 0.8178606234913173, 'learning_rate': 1.1414719025093825e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.7714559464941577}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:48:25,024][0m Trial 15 finished with value: 0.04184006385762116 and parameters: {'observation_period_num': 110, 'train_rates': 0.7552332252520199, 'learning_rate': 0.0004617331652217104, 'batch_size': 71, 'step_size': 10, 'gamma': 0.7876679228164906}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:49:50,105][0m Trial 16 finished with value: 0.10051789144867833 and parameters: {'observation_period_num': 151, 'train_rates': 0.8161194225699803, 'learning_rate': 1.0793984880398807e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9106826204550627}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:51:44,741][0m Trial 17 finished with value: 0.04401990508119906 and parameters: {'observation_period_num': 68, 'train_rates': 0.7312973562993396, 'learning_rate': 9.038345157930912e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.7879050584043471}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:52:48,151][0m Trial 18 finished with value: 0.04410550684920397 and parameters: {'observation_period_num': 26, 'train_rates': 0.8322140906631793, 'learning_rate': 0.00045076333752515926, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8632812682040845}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:54:53,967][0m Trial 19 finished with value: 0.06225558412926538 and parameters: {'observation_period_num': 89, 'train_rates': 0.9636497365570904, 'learning_rate': 2.6486836128880478e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.7782526532618332}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 03:55:50,336][0m Trial 20 finished with value: 0.26479711875148204 and parameters: {'observation_period_num': 50, 'train_rates': 0.8502626933485368, 'learning_rate': 2.07622339715257e-06, 'batch_size': 101, 'step_size': 9, 'gamma': 0.9157260755980932}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 04:00:30,533][0m Trial 21 finished with value: 0.030852497760522162 and parameters: {'observation_period_num': 38, 'train_rates': 0.7775089064778142, 'learning_rate': 0.000997221170695031, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7510017700416762}. Best is trial 10 with value: 0.02507299233683094.[0m
[32m[I 2025-02-03 04:02:46,226][0m Trial 22 finished with value: 0.022857451658254785 and parameters: {'observation_period_num': 5, 'train_rates': 0.7938011147100427, 'learning_rate': 0.0007416334001401709, 'batch_size': 39, 'step_size': 15, 'gamma': 0.7506949042804572}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:03:50,839][0m Trial 23 finished with value: 0.02874548547669257 and parameters: {'observation_period_num': 13, 'train_rates': 0.7281887554660313, 'learning_rate': 0.0005042338619624862, 'batch_size': 82, 'step_size': 13, 'gamma': 0.801827463235735}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:06:03,501][0m Trial 24 finished with value: 0.02311946208820815 and parameters: {'observation_period_num': 9, 'train_rates': 0.8011296804420711, 'learning_rate': 0.00034167867118097694, 'batch_size': 40, 'step_size': 14, 'gamma': 0.7631117236529369}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:07:48,276][0m Trial 25 finished with value: 0.03924795089981943 and parameters: {'observation_period_num': 53, 'train_rates': 0.8024999289371526, 'learning_rate': 0.00028051070576229765, 'batch_size': 50, 'step_size': 14, 'gamma': 0.7715863883178978}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:10:33,962][0m Trial 26 finished with value: 0.03175532008256703 and parameters: {'observation_period_num': 22, 'train_rates': 0.8522214005542951, 'learning_rate': 0.0001080744784427776, 'batch_size': 33, 'step_size': 11, 'gamma': 0.807310347475848}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:11:48,282][0m Trial 27 finished with value: 0.030515240759747785 and parameters: {'observation_period_num': 28, 'train_rates': 0.930057272272692, 'learning_rate': 0.0003692727508517539, 'batch_size': 81, 'step_size': 14, 'gamma': 0.769864468907925}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:12:40,318][0m Trial 28 finished with value: 0.03426311883543219 and parameters: {'observation_period_num': 60, 'train_rates': 0.7434339999540039, 'learning_rate': 0.0006236059062878115, 'batch_size': 101, 'step_size': 12, 'gamma': 0.7903205623021766}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:14:02,609][0m Trial 29 finished with value: 0.061714964646756444 and parameters: {'observation_period_num': 89, 'train_rates': 0.6951808000542957, 'learning_rate': 4.9929784497549444e-05, 'batch_size': 57, 'step_size': 14, 'gamma': 0.8454747593781701}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:14:47,250][0m Trial 30 finished with value: 0.06169619601457677 and parameters: {'observation_period_num': 147, 'train_rates': 0.798981765070209, 'learning_rate': 0.0001321455068707684, 'batch_size': 120, 'step_size': 11, 'gamma': 0.7623716751319196}. Best is trial 22 with value: 0.022857451658254785.[0m
[32m[I 2025-02-03 04:17:06,928][0m Trial 31 finished with value: 0.022555096200108527 and parameters: {'observation_period_num': 6, 'train_rates': 0.7879826091534547, 'learning_rate': 0.0006875080765550336, 'batch_size': 38, 'step_size': 15, 'gamma': 0.7656648014590053}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:22:40,143][0m Trial 32 finished with value: 0.03139912633983331 and parameters: {'observation_period_num': 22, 'train_rates': 0.8331628555333329, 'learning_rate': 0.0003254809533089437, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7823655570503089}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:25:29,034][0m Trial 33 finished with value: 0.022731174352170022 and parameters: {'observation_period_num': 5, 'train_rates': 0.7946013602744773, 'learning_rate': 0.0006187585548671844, 'batch_size': 31, 'step_size': 13, 'gamma': 0.7663360365237004}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:26:43,074][0m Trial 34 finished with value: 0.04194598990242656 and parameters: {'observation_period_num': 38, 'train_rates': 0.8029846255606553, 'learning_rate': 0.0002627782139485436, 'batch_size': 72, 'step_size': 13, 'gamma': 0.7951549431402883}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:28:35,272][0m Trial 35 finished with value: 0.025090137274506 and parameters: {'observation_period_num': 5, 'train_rates': 0.8676383532643984, 'learning_rate': 0.0006169789927180573, 'batch_size': 51, 'step_size': 14, 'gamma': 0.817613272402953}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:31:18,364][0m Trial 36 finished with value: 0.07743908047654269 and parameters: {'observation_period_num': 194, 'train_rates': 0.7523831454237991, 'learning_rate': 5.55567463389536e-05, 'batch_size': 29, 'step_size': 12, 'gamma': 0.7706997552442701}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:32:31,349][0m Trial 37 finished with value: 0.09224410197280337 and parameters: {'observation_period_num': 247, 'train_rates': 0.7120909553625777, 'learning_rate': 0.00018990871275182474, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8908845905438572}. Best is trial 31 with value: 0.022555096200108527.[0m
Early stopping at epoch 68
[32m[I 2025-02-03 04:33:19,005][0m Trial 38 finished with value: 0.2235065719688126 and parameters: {'observation_period_num': 78, 'train_rates': 0.9336030451435939, 'learning_rate': 2.1131399000061467e-05, 'batch_size': 88, 'step_size': 1, 'gamma': 0.8359330177702625}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:33:51,706][0m Trial 39 finished with value: 0.08021182081647063 and parameters: {'observation_period_num': 220, 'train_rates': 0.8415358971447437, 'learning_rate': 0.00014409375499158214, 'batch_size': 173, 'step_size': 10, 'gamma': 0.7647803300776508}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:36:40,019][0m Trial 40 finished with value: 0.02839841364260806 and parameters: {'observation_period_num': 19, 'train_rates': 0.8233789787252686, 'learning_rate': 0.00031641505799115274, 'batch_size': 32, 'step_size': 4, 'gamma': 0.8109483324888829}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:38:57,683][0m Trial 41 finished with value: 0.026680230455482064 and parameters: {'observation_period_num': 5, 'train_rates': 0.7893651657420215, 'learning_rate': 0.0006692551413015689, 'batch_size': 38, 'step_size': 15, 'gamma': 0.9825491750247043}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:39:18,615][0m Trial 42 finished with value: 0.04031598582892245 and parameters: {'observation_period_num': 16, 'train_rates': 0.643174019478147, 'learning_rate': 0.000665906791491754, 'batch_size': 254, 'step_size': 14, 'gamma': 0.7625283544702951}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:42:41,525][0m Trial 43 finished with value: 0.03609491181970936 and parameters: {'observation_period_num': 33, 'train_rates': 0.7632531800578436, 'learning_rate': 0.0003637287582832599, 'batch_size': 25, 'step_size': 15, 'gamma': 0.7776033059978328}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:44:22,511][0m Trial 44 finished with value: 0.028304569435452943 and parameters: {'observation_period_num': 18, 'train_rates': 0.7927052354438751, 'learning_rate': 0.0002269530907871108, 'batch_size': 52, 'step_size': 14, 'gamma': 0.7596374989484587}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:44:50,877][0m Trial 45 finished with value: 0.04765626609729356 and parameters: {'observation_period_num': 57, 'train_rates': 0.885464916040678, 'learning_rate': 0.0007456010996077835, 'batch_size': 229, 'step_size': 15, 'gamma': 0.7939004897896712}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:47:06,645][0m Trial 46 finished with value: 0.034748095084006426 and parameters: {'observation_period_num': 44, 'train_rates': 0.74028497022531, 'learning_rate': 0.00044889616523114876, 'batch_size': 37, 'step_size': 7, 'gamma': 0.7505251251735298}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:48:10,977][0m Trial 47 finished with value: 0.06312094671648548 and parameters: {'observation_period_num': 5, 'train_rates': 0.6015181614262303, 'learning_rate': 0.000950659813112625, 'batch_size': 70, 'step_size': 13, 'gamma': 0.9425539461145556}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:48:47,413][0m Trial 48 finished with value: 0.03176222491790267 and parameters: {'observation_period_num': 13, 'train_rates': 0.7107328854508772, 'learning_rate': 0.0005393608998008565, 'batch_size': 148, 'step_size': 12, 'gamma': 0.7800144873354674}. Best is trial 31 with value: 0.022555096200108527.[0m
[32m[I 2025-02-03 04:49:18,654][0m Trial 49 finished with value: 0.05231495772089277 and parameters: {'observation_period_num': 167, 'train_rates': 0.8117427575034902, 'learning_rate': 0.00039322638410987226, 'batch_size': 186, 'step_size': 15, 'gamma': 0.79800348132552}. Best is trial 31 with value: 0.022555096200108527.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2789 | 0.1151
Epoch 2/300, Loss: 0.1009 | 0.0677
Epoch 3/300, Loss: 0.0830 | 0.0582
Epoch 4/300, Loss: 0.0698 | 0.0539
Epoch 5/300, Loss: 0.0630 | 0.0596
Epoch 6/300, Loss: 0.0598 | 0.0568
Epoch 7/300, Loss: 0.0638 | 0.0648
Epoch 8/300, Loss: 0.0749 | 0.0528
Epoch 9/300, Loss: 0.0625 | 0.0518
Epoch 10/300, Loss: 0.0499 | 0.0429
Epoch 11/300, Loss: 0.0510 | 0.0448
Epoch 12/300, Loss: 0.0452 | 0.0398
Epoch 13/300, Loss: 0.0398 | 0.0390
Epoch 14/300, Loss: 0.0388 | 0.0389
Epoch 15/300, Loss: 0.0390 | 0.0412
Epoch 16/300, Loss: 0.0394 | 0.0395
Epoch 17/300, Loss: 0.0393 | 0.0380
Epoch 18/300, Loss: 0.0373 | 0.0403
Epoch 19/300, Loss: 0.0368 | 0.0350
Epoch 20/300, Loss: 0.0359 | 0.0442
Epoch 21/300, Loss: 0.0383 | 0.0382
Epoch 22/300, Loss: 0.0356 | 0.0428
Epoch 23/300, Loss: 0.0366 | 0.0333
Epoch 24/300, Loss: 0.0354 | 0.0380
Epoch 25/300, Loss: 0.0363 | 0.0381
Epoch 26/300, Loss: 0.0356 | 0.0355
Epoch 27/300, Loss: 0.0395 | 0.0317
Epoch 28/300, Loss: 0.0318 | 0.0376
Epoch 29/300, Loss: 0.0299 | 0.0340
Epoch 30/300, Loss: 0.0307 | 0.0355
Epoch 31/300, Loss: 0.0300 | 0.0316
Epoch 32/300, Loss: 0.0304 | 0.0450
Epoch 33/300, Loss: 0.0373 | 0.0327
Epoch 34/300, Loss: 0.0276 | 0.0365
Epoch 35/300, Loss: 0.0269 | 0.0432
Epoch 36/300, Loss: 0.0268 | 0.0388
Epoch 37/300, Loss: 0.0272 | 0.0438
Epoch 38/300, Loss: 0.0282 | 0.0310
Epoch 39/300, Loss: 0.0275 | 0.0425
Epoch 40/300, Loss: 0.0338 | 0.0341
Epoch 41/300, Loss: 0.0268 | 0.0340
Epoch 42/300, Loss: 0.0259 | 0.0332
Epoch 43/300, Loss: 0.0256 | 0.0323
Epoch 44/300, Loss: 0.0287 | 0.0344
Epoch 45/300, Loss: 0.0267 | 0.0458
Epoch 46/300, Loss: 0.0316 | 0.0387
Epoch 47/300, Loss: 0.0275 | 0.0278
Epoch 48/300, Loss: 0.0244 | 0.0346
Epoch 49/300, Loss: 0.0242 | 0.0353
Epoch 50/300, Loss: 0.0245 | 0.0350
Epoch 51/300, Loss: 0.0244 | 0.0360
Epoch 52/300, Loss: 0.0248 | 0.0371
Epoch 53/300, Loss: 0.0246 | 0.0341
Epoch 54/300, Loss: 0.0253 | 0.0393
Epoch 55/300, Loss: 0.0252 | 0.0325
Epoch 56/300, Loss: 0.0267 | 0.0413
Epoch 57/300, Loss: 0.0245 | 0.0318
Epoch 58/300, Loss: 0.0261 | 0.0422
Epoch 59/300, Loss: 0.0241 | 0.0305
Epoch 60/300, Loss: 0.0277 | 0.0334
Epoch 61/300, Loss: 0.0249 | 0.0326
Epoch 62/300, Loss: 0.0251 | 0.0336
Epoch 63/300, Loss: 0.0238 | 0.0310
Epoch 64/300, Loss: 0.0236 | 0.0305
Epoch 65/300, Loss: 0.0229 | 0.0301
Epoch 66/300, Loss: 0.0227 | 0.0302
Epoch 67/300, Loss: 0.0224 | 0.0297
Epoch 68/300, Loss: 0.0222 | 0.0308
Epoch 69/300, Loss: 0.0222 | 0.0305
Epoch 70/300, Loss: 0.0227 | 0.0285
Epoch 71/300, Loss: 0.0226 | 0.0286
Epoch 72/300, Loss: 0.0221 | 0.0282
Epoch 73/300, Loss: 0.0218 | 0.0286
Epoch 74/300, Loss: 0.0217 | 0.0286
Epoch 75/300, Loss: 0.0219 | 0.0301
Epoch 76/300, Loss: 0.0213 | 0.0286
Epoch 77/300, Loss: 0.0218 | 0.0275
Epoch 78/300, Loss: 0.0207 | 0.0278
Epoch 79/300, Loss: 0.0215 | 0.0270
Epoch 80/300, Loss: 0.0204 | 0.0273
Epoch 81/300, Loss: 0.0214 | 0.0267
Epoch 82/300, Loss: 0.0203 | 0.0270
Epoch 83/300, Loss: 0.0209 | 0.0267
Epoch 84/300, Loss: 0.0203 | 0.0268
Epoch 85/300, Loss: 0.0205 | 0.0260
Epoch 86/300, Loss: 0.0199 | 0.0261
Epoch 87/300, Loss: 0.0197 | 0.0258
Epoch 88/300, Loss: 0.0194 | 0.0258
Epoch 89/300, Loss: 0.0192 | 0.0257
Epoch 90/300, Loss: 0.0190 | 0.0256
Epoch 91/300, Loss: 0.0189 | 0.0253
Epoch 92/300, Loss: 0.0187 | 0.0252
Epoch 93/300, Loss: 0.0184 | 0.0251
Epoch 94/300, Loss: 0.0183 | 0.0251
Epoch 95/300, Loss: 0.0181 | 0.0250
Epoch 96/300, Loss: 0.0180 | 0.0249
Epoch 97/300, Loss: 0.0179 | 0.0248
Epoch 98/300, Loss: 0.0178 | 0.0247
Epoch 99/300, Loss: 0.0176 | 0.0246
Epoch 100/300, Loss: 0.0176 | 0.0246
Epoch 101/300, Loss: 0.0175 | 0.0245
Epoch 102/300, Loss: 0.0174 | 0.0245
Epoch 103/300, Loss: 0.0173 | 0.0244
Epoch 104/300, Loss: 0.0172 | 0.0244
Epoch 105/300, Loss: 0.0171 | 0.0243
Epoch 106/300, Loss: 0.0170 | 0.0244
Epoch 107/300, Loss: 0.0169 | 0.0243
Epoch 108/300, Loss: 0.0168 | 0.0243
Epoch 109/300, Loss: 0.0167 | 0.0242
Epoch 110/300, Loss: 0.0167 | 0.0242
Epoch 111/300, Loss: 0.0166 | 0.0241
Epoch 112/300, Loss: 0.0165 | 0.0241
Epoch 113/300, Loss: 0.0165 | 0.0240
Epoch 114/300, Loss: 0.0164 | 0.0240
Epoch 115/300, Loss: 0.0163 | 0.0240
Epoch 116/300, Loss: 0.0163 | 0.0239
Epoch 117/300, Loss: 0.0162 | 0.0239
Epoch 118/300, Loss: 0.0162 | 0.0239
Epoch 119/300, Loss: 0.0161 | 0.0238
Epoch 120/300, Loss: 0.0161 | 0.0238
Epoch 121/300, Loss: 0.0160 | 0.0238
Epoch 122/300, Loss: 0.0160 | 0.0237
Epoch 123/300, Loss: 0.0160 | 0.0237
Epoch 124/300, Loss: 0.0159 | 0.0237
Epoch 125/300, Loss: 0.0159 | 0.0237
Epoch 126/300, Loss: 0.0158 | 0.0236
Epoch 127/300, Loss: 0.0158 | 0.0236
Epoch 128/300, Loss: 0.0158 | 0.0236
Epoch 129/300, Loss: 0.0157 | 0.0235
Epoch 130/300, Loss: 0.0157 | 0.0235
Epoch 131/300, Loss: 0.0157 | 0.0235
Epoch 132/300, Loss: 0.0156 | 0.0235
Epoch 133/300, Loss: 0.0156 | 0.0234
Epoch 134/300, Loss: 0.0156 | 0.0234
Epoch 135/300, Loss: 0.0156 | 0.0234
Epoch 136/300, Loss: 0.0155 | 0.0233
Epoch 137/300, Loss: 0.0155 | 0.0233
Epoch 138/300, Loss: 0.0155 | 0.0233
Epoch 139/300, Loss: 0.0155 | 0.0233
Epoch 140/300, Loss: 0.0154 | 0.0233
Epoch 141/300, Loss: 0.0154 | 0.0233
Epoch 142/300, Loss: 0.0154 | 0.0233
Epoch 143/300, Loss: 0.0154 | 0.0233
Epoch 144/300, Loss: 0.0153 | 0.0232
Epoch 145/300, Loss: 0.0153 | 0.0232
Epoch 146/300, Loss: 0.0153 | 0.0232
Epoch 147/300, Loss: 0.0153 | 0.0232
Epoch 148/300, Loss: 0.0153 | 0.0232
Epoch 149/300, Loss: 0.0153 | 0.0232
Epoch 150/300, Loss: 0.0153 | 0.0232
Epoch 151/300, Loss: 0.0152 | 0.0231
Epoch 152/300, Loss: 0.0152 | 0.0231
Epoch 153/300, Loss: 0.0152 | 0.0231
Epoch 154/300, Loss: 0.0152 | 0.0231
Epoch 155/300, Loss: 0.0152 | 0.0231
Epoch 156/300, Loss: 0.0152 | 0.0231
Epoch 157/300, Loss: 0.0152 | 0.0231
Epoch 158/300, Loss: 0.0151 | 0.0231
Epoch 159/300, Loss: 0.0151 | 0.0231
Epoch 160/300, Loss: 0.0151 | 0.0231
Epoch 161/300, Loss: 0.0151 | 0.0231
Epoch 162/300, Loss: 0.0151 | 0.0231
Epoch 163/300, Loss: 0.0151 | 0.0231
Epoch 164/300, Loss: 0.0151 | 0.0231
Epoch 165/300, Loss: 0.0151 | 0.0231
Epoch 166/300, Loss: 0.0151 | 0.0231
Epoch 167/300, Loss: 0.0151 | 0.0231
Epoch 168/300, Loss: 0.0150 | 0.0231
Epoch 169/300, Loss: 0.0150 | 0.0231
Epoch 170/300, Loss: 0.0150 | 0.0231
Epoch 171/300, Loss: 0.0150 | 0.0231
Epoch 172/300, Loss: 0.0150 | 0.0231
Epoch 173/300, Loss: 0.0150 | 0.0231
Epoch 174/300, Loss: 0.0150 | 0.0230
Epoch 175/300, Loss: 0.0150 | 0.0230
Epoch 176/300, Loss: 0.0150 | 0.0230
Epoch 177/300, Loss: 0.0150 | 0.0230
Epoch 178/300, Loss: 0.0150 | 0.0230
Epoch 179/300, Loss: 0.0150 | 0.0230
Epoch 180/300, Loss: 0.0150 | 0.0230
Epoch 181/300, Loss: 0.0150 | 0.0230
Epoch 182/300, Loss: 0.0150 | 0.0230
Epoch 183/300, Loss: 0.0150 | 0.0230
Epoch 184/300, Loss: 0.0150 | 0.0230
Epoch 185/300, Loss: 0.0150 | 0.0230
Epoch 186/300, Loss: 0.0149 | 0.0230
Epoch 187/300, Loss: 0.0149 | 0.0230
Epoch 188/300, Loss: 0.0149 | 0.0230
Epoch 189/300, Loss: 0.0149 | 0.0230
Epoch 190/300, Loss: 0.0149 | 0.0230
Epoch 191/300, Loss: 0.0149 | 0.0230
Epoch 192/300, Loss: 0.0149 | 0.0230
Epoch 193/300, Loss: 0.0149 | 0.0230
Epoch 194/300, Loss: 0.0149 | 0.0230
Epoch 195/300, Loss: 0.0149 | 0.0230
Epoch 196/300, Loss: 0.0149 | 0.0230
Epoch 197/300, Loss: 0.0149 | 0.0230
Epoch 198/300, Loss: 0.0149 | 0.0230
Epoch 199/300, Loss: 0.0149 | 0.0230
Epoch 200/300, Loss: 0.0149 | 0.0230
Epoch 201/300, Loss: 0.0149 | 0.0230
Epoch 202/300, Loss: 0.0149 | 0.0230
Epoch 203/300, Loss: 0.0149 | 0.0230
Epoch 204/300, Loss: 0.0149 | 0.0230
Epoch 205/300, Loss: 0.0149 | 0.0230
Epoch 206/300, Loss: 0.0149 | 0.0230
Epoch 207/300, Loss: 0.0149 | 0.0230
Epoch 208/300, Loss: 0.0149 | 0.0230
Epoch 209/300, Loss: 0.0149 | 0.0230
Epoch 210/300, Loss: 0.0149 | 0.0230
Epoch 211/300, Loss: 0.0149 | 0.0230
Epoch 212/300, Loss: 0.0149 | 0.0230
Epoch 213/300, Loss: 0.0149 | 0.0230
Epoch 214/300, Loss: 0.0149 | 0.0230
Epoch 215/300, Loss: 0.0149 | 0.0230
Epoch 216/300, Loss: 0.0149 | 0.0230
Epoch 217/300, Loss: 0.0149 | 0.0230
Epoch 218/300, Loss: 0.0149 | 0.0230
Epoch 219/300, Loss: 0.0149 | 0.0230
Epoch 220/300, Loss: 0.0149 | 0.0230
Epoch 221/300, Loss: 0.0149 | 0.0230
Epoch 222/300, Loss: 0.0149 | 0.0230
Epoch 223/300, Loss: 0.0149 | 0.0230
Epoch 224/300, Loss: 0.0149 | 0.0230
Epoch 225/300, Loss: 0.0149 | 0.0230
Epoch 226/300, Loss: 0.0149 | 0.0230
Epoch 227/300, Loss: 0.0149 | 0.0230
Epoch 228/300, Loss: 0.0149 | 0.0230
Epoch 229/300, Loss: 0.0149 | 0.0230
Epoch 230/300, Loss: 0.0149 | 0.0230
Epoch 231/300, Loss: 0.0149 | 0.0230
Epoch 232/300, Loss: 0.0149 | 0.0230
Epoch 233/300, Loss: 0.0149 | 0.0230
Epoch 234/300, Loss: 0.0149 | 0.0230
Epoch 235/300, Loss: 0.0149 | 0.0230
Epoch 236/300, Loss: 0.0148 | 0.0230
Epoch 237/300, Loss: 0.0148 | 0.0230
Epoch 238/300, Loss: 0.0148 | 0.0230
Epoch 239/300, Loss: 0.0148 | 0.0230
Epoch 240/300, Loss: 0.0148 | 0.0230
Epoch 241/300, Loss: 0.0148 | 0.0230
Epoch 242/300, Loss: 0.0148 | 0.0230
Epoch 243/300, Loss: 0.0148 | 0.0230
Epoch 244/300, Loss: 0.0148 | 0.0230
Epoch 245/300, Loss: 0.0148 | 0.0230
Epoch 246/300, Loss: 0.0148 | 0.0230
Epoch 247/300, Loss: 0.0148 | 0.0230
Epoch 248/300, Loss: 0.0148 | 0.0230
Epoch 249/300, Loss: 0.0148 | 0.0230
Epoch 250/300, Loss: 0.0148 | 0.0230
Epoch 251/300, Loss: 0.0148 | 0.0230
Epoch 252/300, Loss: 0.0148 | 0.0230
Epoch 253/300, Loss: 0.0148 | 0.0230
Epoch 254/300, Loss: 0.0148 | 0.0230
Epoch 255/300, Loss: 0.0148 | 0.0230
Epoch 256/300, Loss: 0.0148 | 0.0230
Epoch 257/300, Loss: 0.0148 | 0.0230
Epoch 258/300, Loss: 0.0148 | 0.0230
Epoch 259/300, Loss: 0.0148 | 0.0230
Epoch 260/300, Loss: 0.0148 | 0.0230
Epoch 261/300, Loss: 0.0148 | 0.0230
Epoch 262/300, Loss: 0.0148 | 0.0230
Epoch 263/300, Loss: 0.0148 | 0.0230
Epoch 264/300, Loss: 0.0148 | 0.0230
Epoch 265/300, Loss: 0.0148 | 0.0230
Epoch 266/300, Loss: 0.0148 | 0.0230
Epoch 267/300, Loss: 0.0148 | 0.0230
Epoch 268/300, Loss: 0.0148 | 0.0230
Epoch 269/300, Loss: 0.0148 | 0.0230
Epoch 270/300, Loss: 0.0148 | 0.0230
Epoch 271/300, Loss: 0.0148 | 0.0230
Epoch 272/300, Loss: 0.0148 | 0.0230
Epoch 273/300, Loss: 0.0148 | 0.0230
Epoch 274/300, Loss: 0.0148 | 0.0230
Epoch 275/300, Loss: 0.0148 | 0.0230
Epoch 276/300, Loss: 0.0148 | 0.0230
Epoch 277/300, Loss: 0.0148 | 0.0230
Epoch 278/300, Loss: 0.0148 | 0.0230
Epoch 279/300, Loss: 0.0148 | 0.0230
Epoch 280/300, Loss: 0.0148 | 0.0230
Epoch 281/300, Loss: 0.0148 | 0.0230
Epoch 282/300, Loss: 0.0148 | 0.0230
Epoch 283/300, Loss: 0.0148 | 0.0230
Epoch 284/300, Loss: 0.0148 | 0.0230
Epoch 285/300, Loss: 0.0148 | 0.0230
Epoch 286/300, Loss: 0.0148 | 0.0230
Epoch 287/300, Loss: 0.0148 | 0.0230
Epoch 288/300, Loss: 0.0148 | 0.0230
Epoch 289/300, Loss: 0.0148 | 0.0230
Epoch 290/300, Loss: 0.0148 | 0.0230
Epoch 291/300, Loss: 0.0148 | 0.0230
Epoch 292/300, Loss: 0.0148 | 0.0230
Epoch 293/300, Loss: 0.0148 | 0.0230
Epoch 294/300, Loss: 0.0148 | 0.0230
Epoch 295/300, Loss: 0.0148 | 0.0230
Epoch 296/300, Loss: 0.0148 | 0.0230
Epoch 297/300, Loss: 0.0148 | 0.0230
Epoch 298/300, Loss: 0.0148 | 0.0230
Epoch 299/300, Loss: 0.0148 | 0.0230
Epoch 300/300, Loss: 0.0148 | 0.0230
Runtime (seconds): 419.97375893592834
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 20.263460488058627
RMSE: 4.501495361328125
MAE: 4.501495361328125
R-squared: nan
[148.59851]
