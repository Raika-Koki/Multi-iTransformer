[32m[I 2025-01-04 19:34:25,092][0m A new study created in memory with name: no-name-77bfd3e3-d29f-49fe-b97d-0b52bf8a8932[0m
Early stopping at epoch 79
[32m[I 2025-01-04 19:34:58,676][0m Trial 0 finished with value: 0.3262484669685364 and parameters: {'observation_period_num': 165, 'train_rates': 0.9453897556744019, 'learning_rate': 0.0003839727324164365, 'batch_size': 187, 'step_size': 1, 'gamma': 0.8575901226925544}. Best is trial 0 with value: 0.3262484669685364.[0m
[32m[I 2025-01-04 19:35:44,625][0m Trial 1 finished with value: 0.10959820702062016 and parameters: {'observation_period_num': 163, 'train_rates': 0.9390044425082689, 'learning_rate': 0.00011182069783197693, 'batch_size': 115, 'step_size': 6, 'gamma': 0.8265218435989323}. Best is trial 1 with value: 0.10959820702062016.[0m
[32m[I 2025-01-04 19:36:17,069][0m Trial 2 finished with value: 0.7132864738332814 and parameters: {'observation_period_num': 104, 'train_rates': 0.6440128298107689, 'learning_rate': 1.1591259205411726e-06, 'batch_size': 210, 'step_size': 13, 'gamma': 0.9365282816037974}. Best is trial 1 with value: 0.10959820702062016.[0m
[32m[I 2025-01-04 19:36:49,187][0m Trial 3 finished with value: 0.32792455420982086 and parameters: {'observation_period_num': 243, 'train_rates': 0.7010212869640938, 'learning_rate': 0.0004836310725086194, 'batch_size': 192, 'step_size': 1, 'gamma': 0.8993263940737577}. Best is trial 1 with value: 0.10959820702062016.[0m
[32m[I 2025-01-04 19:37:49,349][0m Trial 4 finished with value: 0.050595194936312475 and parameters: {'observation_period_num': 30, 'train_rates': 0.7223269368201319, 'learning_rate': 3.384265724353684e-05, 'batch_size': 63, 'step_size': 14, 'gamma': 0.9165223304987816}. Best is trial 4 with value: 0.050595194936312475.[0m
[32m[I 2025-01-04 19:38:29,925][0m Trial 5 finished with value: 0.7551442908518242 and parameters: {'observation_period_num': 67, 'train_rates': 0.8973839775977603, 'learning_rate': 3.3125630241767177e-06, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8401497363406532}. Best is trial 4 with value: 0.050595194936312475.[0m
[32m[I 2025-01-04 19:39:07,394][0m Trial 6 finished with value: 0.2007544395977977 and parameters: {'observation_period_num': 241, 'train_rates': 0.8871592793014929, 'learning_rate': 7.403553583098638e-05, 'batch_size': 208, 'step_size': 2, 'gamma': 0.9645914054116139}. Best is trial 4 with value: 0.050595194936312475.[0m
[32m[I 2025-01-04 19:39:43,077][0m Trial 7 finished with value: 0.061974252890353354 and parameters: {'observation_period_num': 43, 'train_rates': 0.6671775899061569, 'learning_rate': 9.236334834149283e-05, 'batch_size': 131, 'step_size': 11, 'gamma': 0.9224435495312364}. Best is trial 4 with value: 0.050595194936312475.[0m
[32m[I 2025-01-04 19:40:24,890][0m Trial 8 finished with value: 0.03419977170977616 and parameters: {'observation_period_num': 10, 'train_rates': 0.929939201848879, 'learning_rate': 0.0003849124567821311, 'batch_size': 188, 'step_size': 9, 'gamma': 0.9022983019604165}. Best is trial 8 with value: 0.03419977170977616.[0m
[32m[I 2025-01-04 19:41:00,241][0m Trial 9 finished with value: 0.2764301056352754 and parameters: {'observation_period_num': 7, 'train_rates': 0.7396377681202347, 'learning_rate': 1.317855433500678e-05, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8339681822830879}. Best is trial 8 with value: 0.03419977170977616.[0m
[32m[I 2025-01-04 19:41:36,712][0m Trial 10 finished with value: 0.05505194564565959 and parameters: {'observation_period_num': 85, 'train_rates': 0.8166994991212962, 'learning_rate': 0.0009993955532894981, 'batch_size': 250, 'step_size': 10, 'gamma': 0.7528012328185525}. Best is trial 8 with value: 0.03419977170977616.[0m
[32m[I 2025-01-04 19:43:25,666][0m Trial 11 finished with value: 0.03049325833910078 and parameters: {'observation_period_num': 5, 'train_rates': 0.8160073348898162, 'learning_rate': 1.78228953962348e-05, 'batch_size': 38, 'step_size': 15, 'gamma': 0.9801053078559331}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:47:05,246][0m Trial 12 finished with value: 0.03796104091549628 and parameters: {'observation_period_num': 5, 'train_rates': 0.8174577173670655, 'learning_rate': 6.484144412975223e-06, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9852873873353096}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:48:06,324][0m Trial 13 finished with value: 0.12563876807689667 and parameters: {'observation_period_num': 131, 'train_rates': 0.9796328541742303, 'learning_rate': 2.1596305866664686e-05, 'batch_size': 86, 'step_size': 15, 'gamma': 0.9585517696402005}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:48:49,509][0m Trial 14 finished with value: 0.0441983479674886 and parameters: {'observation_period_num': 53, 'train_rates': 0.8682925688921243, 'learning_rate': 0.0002097011312085983, 'batch_size': 152, 'step_size': 8, 'gamma': 0.8830320507847947}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:51:03,031][0m Trial 15 finished with value: 0.03959204111015424 and parameters: {'observation_period_num': 32, 'train_rates': 0.7810721098386966, 'learning_rate': 5.0398697257797684e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.7791252463366881}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:51:44,687][0m Trial 16 finished with value: 0.21351410204594115 and parameters: {'observation_period_num': 107, 'train_rates': 0.841400033339393, 'learning_rate': 1.6814088881338916e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.9531144753284103}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:52:35,080][0m Trial 17 finished with value: 0.2830766326515004 and parameters: {'observation_period_num': 191, 'train_rates': 0.7685195119395521, 'learning_rate': 6.778337291442751e-06, 'batch_size': 95, 'step_size': 15, 'gamma': 0.9839517766813412}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:53:07,025][0m Trial 18 finished with value: 0.19546994992282704 and parameters: {'observation_period_num': 73, 'train_rates': 0.6133624820618827, 'learning_rate': 0.0001963144685840133, 'batch_size': 252, 'step_size': 5, 'gamma': 0.8025934314700369}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:54:26,984][0m Trial 19 finished with value: 0.24489095357543264 and parameters: {'observation_period_num': 21, 'train_rates': 0.9389868298288735, 'learning_rate': 1.8830141228905016e-06, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8990759171041757}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:55:05,954][0m Trial 20 finished with value: 0.3594650096527421 and parameters: {'observation_period_num': 128, 'train_rates': 0.8571312428314511, 'learning_rate': 8.064728814800114e-06, 'batch_size': 167, 'step_size': 13, 'gamma': 0.867518365559152}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:58:18,199][0m Trial 21 finished with value: 0.0450978534232171 and parameters: {'observation_period_num': 5, 'train_rates': 0.8225293031922944, 'learning_rate': 4.347046867876995e-06, 'batch_size': 21, 'step_size': 12, 'gamma': 0.986376847788887}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 19:59:31,025][0m Trial 22 finished with value: 0.07659392281868387 and parameters: {'observation_period_num': 45, 'train_rates': 0.7946422958031826, 'learning_rate': 9.187012529145246e-06, 'batch_size': 54, 'step_size': 12, 'gamma': 0.988239652577258}. Best is trial 11 with value: 0.03049325833910078.[0m
[32m[I 2025-01-04 20:01:28,044][0m Trial 23 finished with value: 0.026763504663359944 and parameters: {'observation_period_num': 9, 'train_rates': 0.9072666222515678, 'learning_rate': 3.155989208062347e-05, 'batch_size': 38, 'step_size': 14, 'gamma': 0.9547672983411452}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:03:03,408][0m Trial 24 finished with value: 0.04549646278756494 and parameters: {'observation_period_num': 54, 'train_rates': 0.9038139332000218, 'learning_rate': 4.253863245900065e-05, 'batch_size': 46, 'step_size': 14, 'gamma': 0.9408022315291577}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:04:21,303][0m Trial 25 finished with value: 0.054527141283953556 and parameters: {'observation_period_num': 19, 'train_rates': 0.9219923317697103, 'learning_rate': 2.3281891444417686e-05, 'batch_size': 83, 'step_size': 15, 'gamma': 0.9217701568849285}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:05:20,220][0m Trial 26 finished with value: 0.10460640490055084 and parameters: {'observation_period_num': 81, 'train_rates': 0.9829513058825112, 'learning_rate': 5.890219724375686e-05, 'batch_size': 104, 'step_size': 13, 'gamma': 0.966131465015263}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:07:18,480][0m Trial 27 finished with value: 0.03716687030088501 and parameters: {'observation_period_num': 31, 'train_rates': 0.9593797212001547, 'learning_rate': 0.0001272398152260332, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9420896709262795}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:08:03,587][0m Trial 28 finished with value: 0.06764170283881518 and parameters: {'observation_period_num': 61, 'train_rates': 0.8743313489724085, 'learning_rate': 3.4098114977438626e-05, 'batch_size': 133, 'step_size': 11, 'gamma': 0.9018223249850703}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:08:45,225][0m Trial 29 finished with value: 0.1683636535616482 and parameters: {'observation_period_num': 217, 'train_rates': 0.9133119780022677, 'learning_rate': 0.0003712512965381452, 'batch_size': 228, 'step_size': 3, 'gamma': 0.8782643910067558}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:09:28,195][0m Trial 30 finished with value: 0.03266361142675429 and parameters: {'observation_period_num': 21, 'train_rates': 0.84410685897502, 'learning_rate': 0.0006805587583015272, 'batch_size': 173, 'step_size': 14, 'gamma': 0.9662822602158018}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:10:07,478][0m Trial 31 finished with value: 0.0456312853655144 and parameters: {'observation_period_num': 22, 'train_rates': 0.8377153837685937, 'learning_rate': 0.0004613997782162845, 'batch_size': 172, 'step_size': 14, 'gamma': 0.9675567594525948}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:10:50,899][0m Trial 32 finished with value: 0.08832195401191711 and parameters: {'observation_period_num': 42, 'train_rates': 0.9549184674667922, 'learning_rate': 0.0009555940267091188, 'batch_size': 179, 'step_size': 15, 'gamma': 0.9428183128662512}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:11:59,020][0m Trial 33 finished with value: 0.06349319638684392 and parameters: {'observation_period_num': 19, 'train_rates': 0.926524677149489, 'learning_rate': 0.00027729118555013557, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9699087014177599}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:12:49,455][0m Trial 34 finished with value: 0.027020336584972205 and parameters: {'observation_period_num': 5, 'train_rates': 0.8500090640535738, 'learning_rate': 0.0006997725109937411, 'batch_size': 121, 'step_size': 14, 'gamma': 0.9264781236232321}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:13:47,246][0m Trial 35 finished with value: 0.15660040733600644 and parameters: {'observation_period_num': 165, 'train_rates': 0.8520188583979537, 'learning_rate': 0.0005901157502605035, 'batch_size': 123, 'step_size': 14, 'gamma': 0.9304401111312773}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:14:35,842][0m Trial 36 finished with value: 0.04434408223436725 and parameters: {'observation_period_num': 36, 'train_rates': 0.7665124328442109, 'learning_rate': 0.0006794962975184292, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9508499330297853}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:15:29,095][0m Trial 37 finished with value: 0.09916124891455776 and parameters: {'observation_period_num': 22, 'train_rates': 0.8879551660577885, 'learning_rate': 1.3734399465501719e-05, 'batch_size': 149, 'step_size': 12, 'gamma': 0.9722107710143929}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:16:37,365][0m Trial 38 finished with value: 0.057001829458861404 and parameters: {'observation_period_num': 91, 'train_rates': 0.7996913591531297, 'learning_rate': 0.00012680610347683033, 'batch_size': 78, 'step_size': 13, 'gamma': 0.8512853316667547}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:17:31,786][0m Trial 39 finished with value: 0.05924402701557508 and parameters: {'observation_period_num': 54, 'train_rates': 0.875812576310827, 'learning_rate': 7.939176800175244e-05, 'batch_size': 98, 'step_size': 11, 'gamma': 0.9136343202479441}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:18:27,990][0m Trial 40 finished with value: 0.14654875684519555 and parameters: {'observation_period_num': 185, 'train_rates': 0.7308347978633433, 'learning_rate': 2.6917151800893195e-05, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9759358241252434}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:19:06,146][0m Trial 41 finished with value: 0.03103843393937214 and parameters: {'observation_period_num': 12, 'train_rates': 0.8351192864425869, 'learning_rate': 0.0007399958433377996, 'batch_size': 191, 'step_size': 14, 'gamma': 0.9065521896867715}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:19:45,481][0m Trial 42 finished with value: 0.028011513112976057 and parameters: {'observation_period_num': 13, 'train_rates': 0.8329632425689031, 'learning_rate': 0.0006500815907952172, 'batch_size': 221, 'step_size': 14, 'gamma': 0.9333678948848834}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:20:23,094][0m Trial 43 finished with value: 0.03292022710198601 and parameters: {'observation_period_num': 12, 'train_rates': 0.8241799240233557, 'learning_rate': 0.0002621429640222375, 'batch_size': 226, 'step_size': 13, 'gamma': 0.9283219962245202}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:21:00,225][0m Trial 44 finished with value: 0.050883894476850156 and parameters: {'observation_period_num': 34, 'train_rates': 0.7979376013503201, 'learning_rate': 0.0007650718552946482, 'batch_size': 228, 'step_size': 15, 'gamma': 0.9086537072479243}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:21:37,037][0m Trial 45 finished with value: 0.04063613484060014 and parameters: {'observation_period_num': 8, 'train_rates': 0.7573347750600481, 'learning_rate': 0.00042107153772133303, 'batch_size': 199, 'step_size': 14, 'gamma': 0.9533606724635505}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:22:16,464][0m Trial 46 finished with value: 0.06377159918011878 and parameters: {'observation_period_num': 44, 'train_rates': 0.8630124376221088, 'learning_rate': 0.00018238057526351114, 'batch_size': 218, 'step_size': 12, 'gamma': 0.8871836020888695}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:22:55,663][0m Trial 47 finished with value: 0.11903617265495924 and parameters: {'observation_period_num': 69, 'train_rates': 0.8949997498419711, 'learning_rate': 0.0003157157557658733, 'batch_size': 243, 'step_size': 13, 'gamma': 0.9321769643573529}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:24:46,963][0m Trial 48 finished with value: 0.10116385603136292 and parameters: {'observation_period_num': 144, 'train_rates': 0.8055515722308015, 'learning_rate': 1.7842483529881786e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8952386040368139}. Best is trial 23 with value: 0.026763504663359944.[0m
[32m[I 2025-01-04 20:25:27,789][0m Trial 49 finished with value: 0.026547143967286777 and parameters: {'observation_period_num': 14, 'train_rates': 0.8319832523387294, 'learning_rate': 0.000523817298821932, 'batch_size': 142, 'step_size': 11, 'gamma': 0.9177867632227987}. Best is trial 49 with value: 0.026547143967286777.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AAPL_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2972 | 0.1747
Epoch 2/300, Loss: 0.1549 | 0.2338
Epoch 3/300, Loss: 0.2459 | 0.2536
Epoch 4/300, Loss: 0.1673 | 0.2053
Epoch 5/300, Loss: 0.1772 | 0.1403
Epoch 6/300, Loss: 0.1466 | 0.1095
Epoch 7/300, Loss: 0.1125 | 0.1076
Epoch 8/300, Loss: 0.1104 | 0.0704
Epoch 9/300, Loss: 0.1033 | 0.0645
Epoch 10/300, Loss: 0.1063 | 0.0673
Epoch 11/300, Loss: 0.1051 | 0.0592
Epoch 12/300, Loss: 0.1000 | 0.0554
Epoch 13/300, Loss: 0.1070 | 0.0637
Epoch 14/300, Loss: 0.1107 | 0.0847
Epoch 15/300, Loss: 0.1119 | 0.0884
Epoch 16/300, Loss: 0.1093 | 0.0579
Epoch 17/300, Loss: 0.1310 | 0.2132
Epoch 18/300, Loss: 0.1356 | 0.1092
Epoch 19/300, Loss: 0.1183 | 0.0989
Epoch 20/300, Loss: 0.1277 | 0.0546
Epoch 21/300, Loss: 0.1032 | 0.0852
Epoch 22/300, Loss: 0.1174 | 0.0517
Epoch 23/300, Loss: 0.0963 | 0.0824
Epoch 24/300, Loss: 0.1080 | 0.0629
Epoch 25/300, Loss: 0.0994 | 0.0684
Epoch 26/300, Loss: 0.1039 | 0.0503
Epoch 27/300, Loss: 0.0985 | 0.0726
Epoch 28/300, Loss: 0.0985 | 0.0478
Epoch 29/300, Loss: 0.0852 | 0.0471
Epoch 30/300, Loss: 0.0853 | 0.0397
Epoch 31/300, Loss: 0.0795 | 0.0456
Epoch 32/300, Loss: 0.0792 | 0.0359
Epoch 33/300, Loss: 0.0764 | 0.0378
Epoch 34/300, Loss: 0.0752 | 0.0375
Epoch 35/300, Loss: 0.0750 | 0.0347
Epoch 36/300, Loss: 0.0747 | 0.0379
Epoch 37/300, Loss: 0.0742 | 0.0377
Epoch 38/300, Loss: 0.0742 | 0.0345
Epoch 39/300, Loss: 0.0735 | 0.0364
Epoch 40/300, Loss: 0.0738 | 0.0381
Epoch 41/300, Loss: 0.0762 | 0.0360
Epoch 42/300, Loss: 0.0780 | 0.0355
Epoch 43/300, Loss: 0.0765 | 0.0345
Epoch 44/300, Loss: 0.0785 | 0.0345
Epoch 45/300, Loss: 0.0847 | 0.0376
Epoch 46/300, Loss: 0.0893 | 0.0376
Epoch 47/300, Loss: 0.0912 | 0.0422
Epoch 48/300, Loss: 0.0872 | 0.0415
Epoch 49/300, Loss: 0.0875 | 0.0408
Epoch 50/300, Loss: 0.0866 | 0.0449
Epoch 51/300, Loss: 0.0798 | 0.0514
Epoch 52/300, Loss: 0.0799 | 0.0457
Epoch 53/300, Loss: 0.0774 | 0.0344
Epoch 54/300, Loss: 0.0725 | 0.0339
Epoch 55/300, Loss: 0.0746 | 0.0353
Epoch 56/300, Loss: 0.0797 | 0.0357
Epoch 57/300, Loss: 0.0803 | 0.0368
Epoch 58/300, Loss: 0.0792 | 0.0462
Epoch 59/300, Loss: 0.0800 | 0.0433
Epoch 60/300, Loss: 0.0803 | 0.0364
Epoch 61/300, Loss: 0.0699 | 0.0387
Epoch 62/300, Loss: 0.0718 | 0.0332
Epoch 63/300, Loss: 0.0741 | 0.0344
Epoch 64/300, Loss: 0.0732 | 0.0342
Epoch 65/300, Loss: 0.0693 | 0.0455
Epoch 66/300, Loss: 0.0709 | 0.0325
Epoch 67/300, Loss: 0.0683 | 0.0314
Epoch 68/300, Loss: 0.0666 | 0.0361
Epoch 69/300, Loss: 0.0665 | 0.0302
Epoch 70/300, Loss: 0.0654 | 0.0293
Epoch 71/300, Loss: 0.0653 | 0.0373
Epoch 72/300, Loss: 0.0643 | 0.0295
Epoch 73/300, Loss: 0.0632 | 0.0293
Epoch 74/300, Loss: 0.0629 | 0.0317
Epoch 75/300, Loss: 0.0626 | 0.0277
Epoch 76/300, Loss: 0.0621 | 0.0277
Epoch 77/300, Loss: 0.0619 | 0.0313
Epoch 78/300, Loss: 0.0610 | 0.0271
Epoch 79/300, Loss: 0.0607 | 0.0268
Epoch 80/300, Loss: 0.0605 | 0.0295
Epoch 81/300, Loss: 0.0603 | 0.0266
Epoch 82/300, Loss: 0.0606 | 0.0251
Epoch 83/300, Loss: 0.0604 | 0.0294
Epoch 84/300, Loss: 0.0598 | 0.0259
Epoch 85/300, Loss: 0.0599 | 0.0253
Epoch 86/300, Loss: 0.0603 | 0.0292
Epoch 87/300, Loss: 0.0598 | 0.0261
Epoch 88/300, Loss: 0.0596 | 0.0240
Epoch 89/300, Loss: 0.0590 | 0.0262
Epoch 90/300, Loss: 0.0585 | 0.0253
Epoch 91/300, Loss: 0.0583 | 0.0237
Epoch 92/300, Loss: 0.0581 | 0.0253
Epoch 93/300, Loss: 0.0579 | 0.0248
Epoch 94/300, Loss: 0.0579 | 0.0234
Epoch 95/300, Loss: 0.0584 | 0.0242
Epoch 96/300, Loss: 0.0594 | 0.0273
Epoch 97/300, Loss: 0.0603 | 0.0252
Epoch 98/300, Loss: 0.0587 | 0.0245
Epoch 99/300, Loss: 0.0597 | 0.0246
Epoch 100/300, Loss: 0.0581 | 0.0238
Epoch 101/300, Loss: 0.0580 | 0.0239
Epoch 102/300, Loss: 0.0570 | 0.0255
Epoch 103/300, Loss: 0.0566 | 0.0237
Epoch 104/300, Loss: 0.0567 | 0.0240
Epoch 105/300, Loss: 0.0579 | 0.0243
Epoch 106/300, Loss: 0.0573 | 0.0244
Epoch 107/300, Loss: 0.0563 | 0.0236
Epoch 108/300, Loss: 0.0563 | 0.0246
Epoch 109/300, Loss: 0.0558 | 0.0241
Epoch 110/300, Loss: 0.0559 | 0.0235
Epoch 111/300, Loss: 0.0561 | 0.0242
Epoch 112/300, Loss: 0.0571 | 0.0232
Epoch 113/300, Loss: 0.0589 | 0.0265
Epoch 114/300, Loss: 0.0569 | 0.0233
Epoch 115/300, Loss: 0.0563 | 0.0240
Epoch 116/300, Loss: 0.0555 | 0.0241
Epoch 117/300, Loss: 0.0556 | 0.0243
Epoch 118/300, Loss: 0.0558 | 0.0230
Epoch 119/300, Loss: 0.0557 | 0.0238
Epoch 120/300, Loss: 0.0548 | 0.0247
Epoch 121/300, Loss: 0.0545 | 0.0232
Epoch 122/300, Loss: 0.0546 | 0.0232
Epoch 123/300, Loss: 0.0549 | 0.0250
Epoch 124/300, Loss: 0.0558 | 0.0238
Epoch 125/300, Loss: 0.0570 | 0.0222
Epoch 126/300, Loss: 0.0547 | 0.0241
Epoch 127/300, Loss: 0.0554 | 0.0227
Epoch 128/300, Loss: 0.0542 | 0.0228
Epoch 129/300, Loss: 0.0540 | 0.0233
Epoch 130/300, Loss: 0.0542 | 0.0228
Epoch 131/300, Loss: 0.0544 | 0.0230
Epoch 132/300, Loss: 0.0539 | 0.0236
Epoch 133/300, Loss: 0.0534 | 0.0230
Epoch 134/300, Loss: 0.0532 | 0.0228
Epoch 135/300, Loss: 0.0534 | 0.0233
Epoch 136/300, Loss: 0.0537 | 0.0233
Epoch 137/300, Loss: 0.0546 | 0.0237
Epoch 138/300, Loss: 0.0550 | 0.0232
Epoch 139/300, Loss: 0.0535 | 0.0226
Epoch 140/300, Loss: 0.0532 | 0.0221
Epoch 141/300, Loss: 0.0531 | 0.0227
Epoch 142/300, Loss: 0.0530 | 0.0231
Epoch 143/300, Loss: 0.0529 | 0.0224
Epoch 144/300, Loss: 0.0526 | 0.0228
Epoch 145/300, Loss: 0.0525 | 0.0227
Epoch 146/300, Loss: 0.0523 | 0.0223
Epoch 147/300, Loss: 0.0523 | 0.0226
Epoch 148/300, Loss: 0.0525 | 0.0227
Epoch 149/300, Loss: 0.0532 | 0.0234
Epoch 150/300, Loss: 0.0540 | 0.0220
Epoch 151/300, Loss: 0.0534 | 0.0230
Epoch 152/300, Loss: 0.0536 | 0.0221
Epoch 153/300, Loss: 0.0526 | 0.0224
Epoch 154/300, Loss: 0.0521 | 0.0225
Epoch 155/300, Loss: 0.0520 | 0.0223
Epoch 156/300, Loss: 0.0519 | 0.0224
Epoch 157/300, Loss: 0.0519 | 0.0220
Epoch 158/300, Loss: 0.0518 | 0.0219
Epoch 159/300, Loss: 0.0518 | 0.0223
Epoch 160/300, Loss: 0.0519 | 0.0224
Epoch 161/300, Loss: 0.0520 | 0.0224
Epoch 162/300, Loss: 0.0519 | 0.0226
Epoch 163/300, Loss: 0.0516 | 0.0223
Epoch 164/300, Loss: 0.0515 | 0.0223
Epoch 165/300, Loss: 0.0517 | 0.0226
Epoch 166/300, Loss: 0.0523 | 0.0229
Epoch 167/300, Loss: 0.0530 | 0.0220
Epoch 168/300, Loss: 0.0519 | 0.0223
Epoch 169/300, Loss: 0.0519 | 0.0218
Epoch 170/300, Loss: 0.0514 | 0.0222
Epoch 171/300, Loss: 0.0512 | 0.0223
Epoch 172/300, Loss: 0.0512 | 0.0221
Epoch 173/300, Loss: 0.0513 | 0.0223
Epoch 174/300, Loss: 0.0513 | 0.0218
Epoch 175/300, Loss: 0.0512 | 0.0218
Epoch 176/300, Loss: 0.0512 | 0.0221
Epoch 177/300, Loss: 0.0512 | 0.0220
Epoch 178/300, Loss: 0.0510 | 0.0222
Epoch 179/300, Loss: 0.0509 | 0.0223
Epoch 180/300, Loss: 0.0509 | 0.0222
Epoch 181/300, Loss: 0.0512 | 0.0221
Epoch 182/300, Loss: 0.0512 | 0.0217
Epoch 183/300, Loss: 0.0509 | 0.0218
Epoch 184/300, Loss: 0.0511 | 0.0220
Epoch 185/300, Loss: 0.0509 | 0.0220
Epoch 186/300, Loss: 0.0506 | 0.0221
Epoch 187/300, Loss: 0.0506 | 0.0221
Epoch 188/300, Loss: 0.0508 | 0.0219
Epoch 189/300, Loss: 0.0508 | 0.0217
Epoch 190/300, Loss: 0.0506 | 0.0218
Epoch 191/300, Loss: 0.0506 | 0.0219
Epoch 192/300, Loss: 0.0505 | 0.0220
Epoch 193/300, Loss: 0.0504 | 0.0220
Epoch 194/300, Loss: 0.0504 | 0.0220
Epoch 195/300, Loss: 0.0505 | 0.0218
Epoch 196/300, Loss: 0.0504 | 0.0217
Epoch 197/300, Loss: 0.0503 | 0.0218
Epoch 198/300, Loss: 0.0503 | 0.0219
Epoch 199/300, Loss: 0.0502 | 0.0220
Epoch 200/300, Loss: 0.0502 | 0.0220
Epoch 201/300, Loss: 0.0502 | 0.0218
Epoch 202/300, Loss: 0.0502 | 0.0217
Epoch 203/300, Loss: 0.0501 | 0.0217
Epoch 204/300, Loss: 0.0501 | 0.0218
Epoch 205/300, Loss: 0.0501 | 0.0219
Epoch 206/300, Loss: 0.0500 | 0.0219
Epoch 207/300, Loss: 0.0500 | 0.0218
Epoch 208/300, Loss: 0.0500 | 0.0217
Epoch 209/300, Loss: 0.0500 | 0.0217
Epoch 210/300, Loss: 0.0499 | 0.0218
Epoch 211/300, Loss: 0.0499 | 0.0218
Epoch 212/300, Loss: 0.0499 | 0.0219
Epoch 213/300, Loss: 0.0499 | 0.0218
Epoch 214/300, Loss: 0.0498 | 0.0218
Epoch 215/300, Loss: 0.0498 | 0.0217
Epoch 216/300, Loss: 0.0498 | 0.0218
Epoch 217/300, Loss: 0.0498 | 0.0218
Epoch 218/300, Loss: 0.0497 | 0.0218
Epoch 219/300, Loss: 0.0497 | 0.0218
Epoch 220/300, Loss: 0.0497 | 0.0218
Epoch 221/300, Loss: 0.0497 | 0.0218
Epoch 222/300, Loss: 0.0497 | 0.0218
Epoch 223/300, Loss: 0.0496 | 0.0218
Epoch 224/300, Loss: 0.0496 | 0.0218
Epoch 225/300, Loss: 0.0496 | 0.0218
Epoch 226/300, Loss: 0.0496 | 0.0218
Epoch 227/300, Loss: 0.0496 | 0.0218
Epoch 228/300, Loss: 0.0495 | 0.0218
Epoch 229/300, Loss: 0.0495 | 0.0218
Epoch 230/300, Loss: 0.0495 | 0.0218
Epoch 231/300, Loss: 0.0495 | 0.0218
Epoch 232/300, Loss: 0.0495 | 0.0217
Epoch 233/300, Loss: 0.0495 | 0.0217
Epoch 234/300, Loss: 0.0494 | 0.0218
Epoch 235/300, Loss: 0.0494 | 0.0218
Epoch 236/300, Loss: 0.0494 | 0.0218
Epoch 237/300, Loss: 0.0494 | 0.0217
Epoch 238/300, Loss: 0.0494 | 0.0217
Epoch 239/300, Loss: 0.0493 | 0.0217
Epoch 240/300, Loss: 0.0493 | 0.0217
Epoch 241/300, Loss: 0.0493 | 0.0217
Epoch 242/300, Loss: 0.0493 | 0.0217
Epoch 243/300, Loss: 0.0493 | 0.0217
Epoch 244/300, Loss: 0.0493 | 0.0217
Epoch 245/300, Loss: 0.0493 | 0.0217
Epoch 246/300, Loss: 0.0492 | 0.0217
Epoch 247/300, Loss: 0.0492 | 0.0217
Epoch 248/300, Loss: 0.0492 | 0.0217
Epoch 249/300, Loss: 0.0492 | 0.0217
Epoch 250/300, Loss: 0.0492 | 0.0217
Epoch 251/300, Loss: 0.0492 | 0.0217
Epoch 252/300, Loss: 0.0491 | 0.0217
Epoch 253/300, Loss: 0.0491 | 0.0217
Epoch 254/300, Loss: 0.0491 | 0.0217
Epoch 255/300, Loss: 0.0491 | 0.0217
Epoch 256/300, Loss: 0.0491 | 0.0217
Epoch 257/300, Loss: 0.0491 | 0.0217
Epoch 258/300, Loss: 0.0491 | 0.0217
Epoch 259/300, Loss: 0.0491 | 0.0217
Epoch 260/300, Loss: 0.0490 | 0.0217
Epoch 261/300, Loss: 0.0490 | 0.0217
Epoch 262/300, Loss: 0.0490 | 0.0217
Epoch 263/300, Loss: 0.0490 | 0.0217
Epoch 264/300, Loss: 0.0490 | 0.0217
Epoch 265/300, Loss: 0.0490 | 0.0217
Epoch 266/300, Loss: 0.0490 | 0.0217
Epoch 267/300, Loss: 0.0489 | 0.0217
Epoch 268/300, Loss: 0.0489 | 0.0217
Epoch 269/300, Loss: 0.0489 | 0.0217
Epoch 270/300, Loss: 0.0489 | 0.0217
Epoch 271/300, Loss: 0.0489 | 0.0217
Epoch 272/300, Loss: 0.0489 | 0.0217
Epoch 273/300, Loss: 0.0489 | 0.0217
Epoch 274/300, Loss: 0.0489 | 0.0217
Epoch 275/300, Loss: 0.0489 | 0.0217
Epoch 276/300, Loss: 0.0489 | 0.0217
Epoch 277/300, Loss: 0.0488 | 0.0217
Epoch 278/300, Loss: 0.0488 | 0.0217
Epoch 279/300, Loss: 0.0488 | 0.0217
Epoch 280/300, Loss: 0.0488 | 0.0217
Epoch 281/300, Loss: 0.0488 | 0.0218
Epoch 282/300, Loss: 0.0488 | 0.0217
Epoch 283/300, Loss: 0.0489 | 0.0218
Epoch 284/300, Loss: 0.0489 | 0.0217
Epoch 285/300, Loss: 0.0489 | 0.0219
Epoch 286/300, Loss: 0.0489 | 0.0217
Epoch 287/300, Loss: 0.0489 | 0.0221
Epoch 288/300, Loss: 0.0489 | 0.0217
Epoch 289/300, Loss: 0.0488 | 0.0221
Epoch 290/300, Loss: 0.0488 | 0.0217
Epoch 291/300, Loss: 0.0488 | 0.0220
Epoch 292/300, Loss: 0.0488 | 0.0217
Epoch 293/300, Loss: 0.0487 | 0.0220
Epoch 294/300, Loss: 0.0487 | 0.0217
Epoch 295/300, Loss: 0.0487 | 0.0218
Epoch 296/300, Loss: 0.0487 | 0.0217
Epoch 297/300, Loss: 0.0487 | 0.0218
Epoch 298/300, Loss: 0.0486 | 0.0217
Epoch 299/300, Loss: 0.0486 | 0.0217
Epoch 300/300, Loss: 0.0486 | 0.0217
Runtime (seconds): 118.63957095146179
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 5.398305092006922
RMSE: 2.32342529296875
MAE: 2.32342529296875
R-squared: nan
[232.60657]
