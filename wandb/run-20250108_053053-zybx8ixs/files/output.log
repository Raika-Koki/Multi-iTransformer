[32m[I 2025-01-08 05:30:54,938][0m A new study created in memory with name: no-name-d5a9763b-7616-49a3-878e-0751fa0f467c[0m
[32m[I 2025-01-08 05:33:09,518][0m Trial 0 finished with value: 0.8758889345022348 and parameters: {'observation_period_num': 126, 'train_rates': 0.6803956058526256, 'learning_rate': 2.8350765047068968e-05, 'batch_size': 224, 'step_size': 12, 'gamma': 0.9739208065775932}. Best is trial 0 with value: 0.8758889345022348.[0m
[32m[I 2025-01-08 05:36:49,308][0m Trial 1 finished with value: 0.4334845855864564 and parameters: {'observation_period_num': 163, 'train_rates': 0.8365841853178887, 'learning_rate': 4.806815491774066e-05, 'batch_size': 212, 'step_size': 4, 'gamma': 0.883204141980044}. Best is trial 1 with value: 0.4334845855864564.[0m
[32m[I 2025-01-08 05:37:27,967][0m Trial 2 finished with value: 1.2699636312537506 and parameters: {'observation_period_num': 34, 'train_rates': 0.6678974148760656, 'learning_rate': 3.132546674413606e-06, 'batch_size': 194, 'step_size': 15, 'gamma': 0.8213938745083824}. Best is trial 1 with value: 0.4334845855864564.[0m
[32m[I 2025-01-08 05:39:33,423][0m Trial 3 finished with value: 0.3361293526778694 and parameters: {'observation_period_num': 100, 'train_rates': 0.8633534020848744, 'learning_rate': 0.000481679219992923, 'batch_size': 187, 'step_size': 7, 'gamma': 0.8261829547848106}. Best is trial 3 with value: 0.3361293526778694.[0m
[32m[I 2025-01-08 05:43:40,291][0m Trial 4 finished with value: 0.19587915108121676 and parameters: {'observation_period_num': 168, 'train_rates': 0.9174351696109464, 'learning_rate': 0.00022366708558205594, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9897885315908966}. Best is trial 4 with value: 0.19587915108121676.[0m
[32m[I 2025-01-08 05:47:22,521][0m Trial 5 finished with value: 0.31301605240220115 and parameters: {'observation_period_num': 155, 'train_rates': 0.8586044052332567, 'learning_rate': 0.00012360126035153407, 'batch_size': 41, 'step_size': 12, 'gamma': 0.7855817473332374}. Best is trial 4 with value: 0.19587915108121676.[0m
Early stopping at epoch 79
[32m[I 2025-01-08 05:51:31,152][0m Trial 6 finished with value: 0.545526893557729 and parameters: {'observation_period_num': 190, 'train_rates': 0.9869824244640484, 'learning_rate': 1.4089305436086862e-05, 'batch_size': 27, 'step_size': 1, 'gamma': 0.860853992449756}. Best is trial 4 with value: 0.19587915108121676.[0m
[32m[I 2025-01-08 05:56:11,903][0m Trial 7 finished with value: 0.16129269398222065 and parameters: {'observation_period_num': 187, 'train_rates': 0.9645672686200362, 'learning_rate': 0.000159812857435169, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9129279516626783}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 05:58:11,543][0m Trial 8 finished with value: 0.3044528536340024 and parameters: {'observation_period_num': 93, 'train_rates': 0.8857988333573397, 'learning_rate': 0.0006769623562984916, 'batch_size': 175, 'step_size': 9, 'gamma': 0.9113859989009279}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:02:44,674][0m Trial 9 finished with value: 1.0543325193612452 and parameters: {'observation_period_num': 236, 'train_rates': 0.6191278961612025, 'learning_rate': 0.00022029696784491771, 'batch_size': 111, 'step_size': 3, 'gamma': 0.7801776151835763}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:08:24,750][0m Trial 10 finished with value: 0.7380787308623151 and parameters: {'observation_period_num': 250, 'train_rates': 0.7741676643936002, 'learning_rate': 4.480758802383573e-06, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9266466701534254}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:13:53,340][0m Trial 11 finished with value: 0.18286742269992828 and parameters: {'observation_period_num': 206, 'train_rates': 0.9851991318172324, 'learning_rate': 0.00013060937580536785, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9829488046444412}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:19:41,549][0m Trial 12 finished with value: 0.17947028577327728 and parameters: {'observation_period_num': 218, 'train_rates': 0.9861956590052203, 'learning_rate': 7.383341848545223e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.9454307627423577}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:24:59,497][0m Trial 13 finished with value: 0.201982460958136 and parameters: {'observation_period_num': 212, 'train_rates': 0.9362085523300792, 'learning_rate': 3.8518102697246784e-05, 'batch_size': 139, 'step_size': 5, 'gamma': 0.9365947080627289}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:29:49,819][0m Trial 14 finished with value: 1.3699859661054394 and parameters: {'observation_period_num': 219, 'train_rates': 0.7613471844110711, 'learning_rate': 1.1300116364558736e-06, 'batch_size': 83, 'step_size': 11, 'gamma': 0.9521904328561089}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:34:15,488][0m Trial 15 finished with value: 0.16212352454662324 and parameters: {'observation_period_num': 183, 'train_rates': 0.9462526705975327, 'learning_rate': 6.967570519330222e-05, 'batch_size': 139, 'step_size': 8, 'gamma': 0.9035122446272525}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:37:17,566][0m Trial 16 finished with value: 0.2761840393783849 and parameters: {'observation_period_num': 130, 'train_rates': 0.9235241580406125, 'learning_rate': 1.5414080626727227e-05, 'batch_size': 147, 'step_size': 9, 'gamma': 0.887840626481488}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:37:43,618][0m Trial 17 finished with value: 0.25135881661254666 and parameters: {'observation_period_num': 16, 'train_rates': 0.8192713073852051, 'learning_rate': 0.0002954204724156019, 'batch_size': 156, 'step_size': 11, 'gamma': 0.8464447345530612}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:42:07,627][0m Trial 18 finished with value: 0.2597717344760895 and parameters: {'observation_period_num': 182, 'train_rates': 0.9402942559702073, 'learning_rate': 1.5420286046790338e-05, 'batch_size': 255, 'step_size': 15, 'gamma': 0.903809848023696}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:44:52,043][0m Trial 19 finished with value: 1.5030344534932145 and parameters: {'observation_period_num': 136, 'train_rates': 0.7343485226729618, 'learning_rate': 0.0009245497035950392, 'batch_size': 114, 'step_size': 8, 'gamma': 0.7542624708617136}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:46:42,289][0m Trial 20 finished with value: 0.1964772114703475 and parameters: {'observation_period_num': 78, 'train_rates': 0.8813670892989933, 'learning_rate': 9.405440117215334e-05, 'batch_size': 55, 'step_size': 13, 'gamma': 0.9079724065391964}. Best is trial 7 with value: 0.16129269398222065.[0m
[32m[I 2025-01-08 06:51:43,203][0m Trial 21 finished with value: 0.15177832543849945 and parameters: {'observation_period_num': 195, 'train_rates': 0.9714940136528822, 'learning_rate': 6.95291346538177e-05, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9525920192212101}. Best is trial 21 with value: 0.15177832543849945.[0m
[32m[I 2025-01-08 06:56:23,363][0m Trial 22 finished with value: 0.15066274338298374 and parameters: {'observation_period_num': 192, 'train_rates': 0.9517436230615862, 'learning_rate': 5.8931383890410815e-05, 'batch_size': 125, 'step_size': 8, 'gamma': 0.960213589853571}. Best is trial 22 with value: 0.15066274338298374.[0m
[32m[I 2025-01-08 07:01:06,894][0m Trial 23 finished with value: 0.16418950162201285 and parameters: {'observation_period_num': 192, 'train_rates': 0.9614988785837487, 'learning_rate': 2.6968793268196716e-05, 'batch_size': 95, 'step_size': 10, 'gamma': 0.9547887692558121}. Best is trial 22 with value: 0.15066274338298374.[0m
[32m[I 2025-01-08 07:04:33,389][0m Trial 24 finished with value: 0.20699173469337628 and parameters: {'observation_period_num': 149, 'train_rates': 0.9017572393675348, 'learning_rate': 0.00018418406630705032, 'batch_size': 116, 'step_size': 7, 'gamma': 0.9577756495027759}. Best is trial 22 with value: 0.15066274338298374.[0m
[32m[I 2025-01-08 07:10:56,705][0m Trial 25 finished with value: 0.1474492141178676 and parameters: {'observation_period_num': 239, 'train_rates': 0.9616767858198777, 'learning_rate': 5.03779530328416e-05, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9238810464139319}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:17:00,430][0m Trial 26 finished with value: 0.3999649659668889 and parameters: {'observation_period_num': 239, 'train_rates': 0.9060658607190668, 'learning_rate': 9.165475568983739e-06, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9668398709406077}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:22:51,732][0m Trial 27 finished with value: 0.37388619100888265 and parameters: {'observation_period_num': 231, 'train_rates': 0.808954552398669, 'learning_rate': 4.573311339931185e-05, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9383393194000319}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:29:36,021][0m Trial 28 finished with value: 0.15576652714184352 and parameters: {'observation_period_num': 251, 'train_rates': 0.9614027552095386, 'learning_rate': 0.00034418205318526403, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9257411759355937}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:34:23,228][0m Trial 29 finished with value: 0.3213978395320042 and parameters: {'observation_period_num': 202, 'train_rates': 0.8698195278593398, 'learning_rate': 2.4407893035103063e-05, 'batch_size': 165, 'step_size': 7, 'gamma': 0.974741359680415}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:39:13,179][0m Trial 30 finished with value: 0.7013707511759332 and parameters: {'observation_period_num': 226, 'train_rates': 0.7041552256996305, 'learning_rate': 5.931058011525736e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.965449985085877}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:45:55,841][0m Trial 31 finished with value: 0.16967460854073702 and parameters: {'observation_period_num': 251, 'train_rates': 0.9568742393148527, 'learning_rate': 0.00039065668894807553, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9286083768527527}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:52:40,446][0m Trial 32 finished with value: 0.15439561009407043 and parameters: {'observation_period_num': 252, 'train_rates': 0.9667134294078853, 'learning_rate': 0.00010487378994809339, 'batch_size': 98, 'step_size': 5, 'gamma': 0.8900272807889539}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 07:56:40,846][0m Trial 33 finished with value: 0.1933188305158447 and parameters: {'observation_period_num': 169, 'train_rates': 0.9190298306402526, 'learning_rate': 9.769377849776537e-05, 'batch_size': 103, 'step_size': 5, 'gamma': 0.8807282866065621}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 08:02:00,577][0m Trial 34 finished with value: 0.3430594503879547 and parameters: {'observation_period_num': 203, 'train_rates': 0.9896791790356483, 'learning_rate': 3.689159715881599e-05, 'batch_size': 126, 'step_size': 3, 'gamma': 0.8879631037871036}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 08:04:27,674][0m Trial 35 finished with value: 0.34676298378936704 and parameters: {'observation_period_num': 114, 'train_rates': 0.8481520793245745, 'learning_rate': 2.2938366889797117e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8625416654924485}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 08:10:26,545][0m Trial 36 finished with value: 0.20921440816826836 and parameters: {'observation_period_num': 238, 'train_rates': 0.8938194009891111, 'learning_rate': 5.20922660985532e-05, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9205120755086426}. Best is trial 25 with value: 0.1474492141178676.[0m
[32m[I 2025-01-08 08:14:45,278][0m Trial 37 finished with value: 0.13320019419248008 and parameters: {'observation_period_num': 171, 'train_rates': 0.9301162210820958, 'learning_rate': 9.450622163502901e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.8383918696315286}. Best is trial 37 with value: 0.13320019419248008.[0m
[32m[I 2025-01-08 08:19:11,154][0m Trial 38 finished with value: 0.22433197423441326 and parameters: {'observation_period_num': 173, 'train_rates': 0.9291387874251756, 'learning_rate': 7.546793096488124e-06, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8288887830777659}. Best is trial 37 with value: 0.13320019419248008.[0m
[32m[I 2025-01-08 08:23:29,027][0m Trial 39 finished with value: 0.40447965594048196 and parameters: {'observation_period_num': 149, 'train_rates': 0.8336050015263001, 'learning_rate': 7.390500582961856e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.840833887501364}. Best is trial 37 with value: 0.13320019419248008.[0m
[32m[I 2025-01-08 08:27:16,028][0m Trial 40 finished with value: 0.16481427818890146 and parameters: {'observation_period_num': 154, 'train_rates': 0.910384926158855, 'learning_rate': 0.00014529138480643344, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8096867594793107}. Best is trial 37 with value: 0.13320019419248008.[0m
[32m[I 2025-01-08 08:32:21,098][0m Trial 41 finished with value: 0.1294067155483157 and parameters: {'observation_period_num': 194, 'train_rates': 0.9691735588082226, 'learning_rate': 0.00010066456267324813, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8739388612595483}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:37:23,455][0m Trial 42 finished with value: 0.18689923665740274 and parameters: {'observation_period_num': 194, 'train_rates': 0.9724436997230232, 'learning_rate': 3.6725982359898015e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8150037137751306}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:41:51,549][0m Trial 43 finished with value: 0.1388457855814463 and parameters: {'observation_period_num': 174, 'train_rates': 0.9451870137120595, 'learning_rate': 0.0002481107697973804, 'batch_size': 47, 'step_size': 8, 'gamma': 0.8029903164718647}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:46:10,580][0m Trial 44 finished with value: 0.13412335515022278 and parameters: {'observation_period_num': 179, 'train_rates': 0.9406539929300811, 'learning_rate': 0.00027324215053874514, 'batch_size': 216, 'step_size': 8, 'gamma': 0.7976584781412932}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:50:09,780][0m Trial 45 finished with value: 0.1429535299539566 and parameters: {'observation_period_num': 165, 'train_rates': 0.9360322450907738, 'learning_rate': 0.00019341279337465556, 'batch_size': 217, 'step_size': 11, 'gamma': 0.7976516478225295}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:54:03,868][0m Trial 46 finished with value: 0.14791874587535858 and parameters: {'observation_period_num': 163, 'train_rates': 0.933345461035641, 'learning_rate': 0.000507835864103001, 'batch_size': 211, 'step_size': 11, 'gamma': 0.7996477660996024}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 08:57:08,251][0m Trial 47 finished with value: 0.2636964880858762 and parameters: {'observation_period_num': 137, 'train_rates': 0.8727109798782626, 'learning_rate': 0.00023268228806554874, 'batch_size': 243, 'step_size': 10, 'gamma': 0.7817937502110424}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 09:00:27,578][0m Trial 48 finished with value: 0.9315243931002699 and parameters: {'observation_period_num': 178, 'train_rates': 0.6285747249241469, 'learning_rate': 0.0005857629334844143, 'batch_size': 201, 'step_size': 12, 'gamma': 0.7932921402704023}. Best is trial 41 with value: 0.1294067155483157.[0m
[32m[I 2025-01-08 09:03:04,391][0m Trial 49 finished with value: 0.22090057170466532 and parameters: {'observation_period_num': 122, 'train_rates': 0.8892285506597253, 'learning_rate': 0.0002712621322758805, 'batch_size': 230, 'step_size': 13, 'gamma': 0.7654489130100985}. Best is trial 41 with value: 0.1294067155483157.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.8244 | 0.9800
Epoch 2/300, Loss: 0.7022 | 0.6986
Epoch 3/300, Loss: 0.5655 | 0.7199
Epoch 4/300, Loss: 0.5034 | 0.5318
Epoch 5/300, Loss: 0.4130 | 0.4831
Epoch 6/300, Loss: 0.3946 | 0.4636
Epoch 7/300, Loss: 0.3473 | 0.4350
Epoch 8/300, Loss: 0.3208 | 0.3553
Epoch 9/300, Loss: 0.3398 | 0.3752
Epoch 10/300, Loss: 0.3154 | 0.3522
Epoch 11/300, Loss: 0.3127 | 0.3320
Epoch 12/300, Loss: 0.3595 | 0.3576
Epoch 13/300, Loss: 0.2915 | 0.3267
Epoch 14/300, Loss: 0.2416 | 0.2926
Epoch 15/300, Loss: 0.2415 | 0.3085
Epoch 16/300, Loss: 0.2229 | 0.2795
Epoch 17/300, Loss: 0.2266 | 0.2844
Epoch 18/300, Loss: 0.2405 | 0.2646
Epoch 19/300, Loss: 0.2137 | 0.2786
Epoch 20/300, Loss: 0.2030 | 0.2565
Epoch 21/300, Loss: 0.1979 | 0.2490
Epoch 22/300, Loss: 0.1918 | 0.2414
Epoch 23/300, Loss: 0.1934 | 0.2399
Epoch 24/300, Loss: 0.1938 | 0.2420
Epoch 25/300, Loss: 0.1814 | 0.2299
Epoch 26/300, Loss: 0.1781 | 0.2297
Epoch 27/300, Loss: 0.1769 | 0.2280
Epoch 28/300, Loss: 0.1754 | 0.2182
Epoch 29/300, Loss: 0.1747 | 0.2203
Epoch 30/300, Loss: 0.1674 | 0.2137
Epoch 31/300, Loss: 0.1659 | 0.2108
Epoch 32/300, Loss: 0.1653 | 0.2093
Epoch 33/300, Loss: 0.1641 | 0.2053
Epoch 34/300, Loss: 0.1621 | 0.2037
Epoch 35/300, Loss: 0.1602 | 0.2020
Epoch 36/300, Loss: 0.1586 | 0.1989
Epoch 37/300, Loss: 0.1584 | 0.1976
Epoch 38/300, Loss: 0.1573 | 0.1957
Epoch 39/300, Loss: 0.1565 | 0.1941
Epoch 40/300, Loss: 0.1554 | 0.1918
Epoch 41/300, Loss: 0.1543 | 0.1890
Epoch 42/300, Loss: 0.1532 | 0.1887
Epoch 43/300, Loss: 0.1529 | 0.1866
Epoch 44/300, Loss: 0.1526 | 0.1855
Epoch 45/300, Loss: 0.1509 | 0.1844
Epoch 46/300, Loss: 0.1506 | 0.1824
Epoch 47/300, Loss: 0.1501 | 0.1817
Epoch 48/300, Loss: 0.1492 | 0.1810
Epoch 49/300, Loss: 0.1483 | 0.1792
Epoch 50/300, Loss: 0.1481 | 0.1776
Epoch 51/300, Loss: 0.1472 | 0.1767
Epoch 52/300, Loss: 0.1470 | 0.1753
Epoch 53/300, Loss: 0.1461 | 0.1749
Epoch 54/300, Loss: 0.1459 | 0.1738
Epoch 55/300, Loss: 0.1448 | 0.1728
Epoch 56/300, Loss: 0.1450 | 0.1715
Epoch 57/300, Loss: 0.1447 | 0.1706
Epoch 58/300, Loss: 0.1438 | 0.1703
Epoch 59/300, Loss: 0.1432 | 0.1694
Epoch 60/300, Loss: 0.1426 | 0.1684
Epoch 61/300, Loss: 0.1426 | 0.1674
Epoch 62/300, Loss: 0.1423 | 0.1673
Epoch 63/300, Loss: 0.1420 | 0.1668
Epoch 64/300, Loss: 0.1415 | 0.1658
Epoch 65/300, Loss: 0.1401 | 0.1650
Epoch 66/300, Loss: 0.1400 | 0.1640
Epoch 67/300, Loss: 0.1400 | 0.1634
Epoch 68/300, Loss: 0.1396 | 0.1629
Epoch 69/300, Loss: 0.1394 | 0.1623
Epoch 70/300, Loss: 0.1394 | 0.1618
Epoch 71/300, Loss: 0.1385 | 0.1610
Epoch 72/300, Loss: 0.1383 | 0.1609
Epoch 73/300, Loss: 0.1374 | 0.1604
Epoch 74/300, Loss: 0.1377 | 0.1596
Epoch 75/300, Loss: 0.1370 | 0.1590
Epoch 76/300, Loss: 0.1371 | 0.1583
Epoch 77/300, Loss: 0.1365 | 0.1582
Epoch 78/300, Loss: 0.1362 | 0.1581
Epoch 79/300, Loss: 0.1363 | 0.1577
Epoch 80/300, Loss: 0.1358 | 0.1571
Epoch 81/300, Loss: 0.1366 | 0.1569
Epoch 82/300, Loss: 0.1356 | 0.1568
Epoch 83/300, Loss: 0.1349 | 0.1563
Epoch 84/300, Loss: 0.1353 | 0.1560
Epoch 85/300, Loss: 0.1346 | 0.1555
Epoch 86/300, Loss: 0.1344 | 0.1549
Epoch 87/300, Loss: 0.1342 | 0.1546
Epoch 88/300, Loss: 0.1340 | 0.1540
Epoch 89/300, Loss: 0.1337 | 0.1539
Epoch 90/300, Loss: 0.1339 | 0.1537
Epoch 91/300, Loss: 0.1335 | 0.1534
Epoch 92/300, Loss: 0.1338 | 0.1532
Epoch 93/300, Loss: 0.1330 | 0.1528
Epoch 94/300, Loss: 0.1328 | 0.1525
Epoch 95/300, Loss: 0.1325 | 0.1522
Epoch 96/300, Loss: 0.1328 | 0.1521
Epoch 97/300, Loss: 0.1321 | 0.1517
Epoch 98/300, Loss: 0.1314 | 0.1514
Epoch 99/300, Loss: 0.1324 | 0.1513
Epoch 100/300, Loss: 0.1322 | 0.1511
Epoch 101/300, Loss: 0.1318 | 0.1509
Epoch 102/300, Loss: 0.1313 | 0.1507
Epoch 103/300, Loss: 0.1313 | 0.1504
Epoch 104/300, Loss: 0.1310 | 0.1503
Epoch 105/300, Loss: 0.1311 | 0.1502
Epoch 106/300, Loss: 0.1307 | 0.1501
Epoch 107/300, Loss: 0.1310 | 0.1499
Epoch 108/300, Loss: 0.1310 | 0.1500
Epoch 109/300, Loss: 0.1301 | 0.1498
Epoch 110/300, Loss: 0.1303 | 0.1495
Epoch 111/300, Loss: 0.1303 | 0.1492
Epoch 112/300, Loss: 0.1301 | 0.1490
Epoch 113/300, Loss: 0.1299 | 0.1488
Epoch 114/300, Loss: 0.1299 | 0.1487
Epoch 115/300, Loss: 0.1295 | 0.1487
Epoch 116/300, Loss: 0.1296 | 0.1484
Epoch 117/300, Loss: 0.1297 | 0.1483
Epoch 118/300, Loss: 0.1298 | 0.1482
Epoch 119/300, Loss: 0.1300 | 0.1480
Epoch 120/300, Loss: 0.1295 | 0.1479
Epoch 121/300, Loss: 0.1291 | 0.1478
Epoch 122/300, Loss: 0.1303 | 0.1478
Epoch 123/300, Loss: 0.1296 | 0.1479
Epoch 124/300, Loss: 0.1289 | 0.1477
Epoch 125/300, Loss: 0.1291 | 0.1476
Epoch 126/300, Loss: 0.1298 | 0.1475
Epoch 127/300, Loss: 0.1289 | 0.1472
Epoch 128/300, Loss: 0.1293 | 0.1471
Epoch 129/300, Loss: 0.1289 | 0.1470
Epoch 130/300, Loss: 0.1286 | 0.1469
Epoch 131/300, Loss: 0.1301 | 0.1469
Epoch 132/300, Loss: 0.1287 | 0.1467
Epoch 133/300, Loss: 0.1289 | 0.1466
Epoch 134/300, Loss: 0.1288 | 0.1466
Epoch 135/300, Loss: 0.1282 | 0.1463
Epoch 136/300, Loss: 0.1285 | 0.1464
Epoch 137/300, Loss: 0.1291 | 0.1464
Epoch 138/300, Loss: 0.1283 | 0.1463
Epoch 139/300, Loss: 0.1286 | 0.1462
Epoch 140/300, Loss: 0.1280 | 0.1461
Epoch 141/300, Loss: 0.1285 | 0.1463
Epoch 142/300, Loss: 0.1285 | 0.1462
Epoch 143/300, Loss: 0.1288 | 0.1461
Epoch 144/300, Loss: 0.1282 | 0.1461
Epoch 145/300, Loss: 0.1280 | 0.1461
Epoch 146/300, Loss: 0.1280 | 0.1460
Epoch 147/300, Loss: 0.1277 | 0.1459
Epoch 148/300, Loss: 0.1284 | 0.1458
Epoch 149/300, Loss: 0.1280 | 0.1458
Epoch 150/300, Loss: 0.1279 | 0.1457
Epoch 151/300, Loss: 0.1278 | 0.1457
Epoch 152/300, Loss: 0.1276 | 0.1456
Epoch 153/300, Loss: 0.1288 | 0.1455
Epoch 154/300, Loss: 0.1284 | 0.1455
Epoch 155/300, Loss: 0.1277 | 0.1455
Epoch 156/300, Loss: 0.1278 | 0.1454
Epoch 157/300, Loss: 0.1291 | 0.1454
Epoch 158/300, Loss: 0.1279 | 0.1453
Epoch 159/300, Loss: 0.1273 | 0.1453
Epoch 160/300, Loss: 0.1279 | 0.1453
Epoch 161/300, Loss: 0.1279 | 0.1452
Epoch 162/300, Loss: 0.1281 | 0.1452
Epoch 163/300, Loss: 0.1277 | 0.1452
Epoch 164/300, Loss: 0.1277 | 0.1451
Epoch 165/300, Loss: 0.1283 | 0.1450
Epoch 166/300, Loss: 0.1276 | 0.1450
Epoch 167/300, Loss: 0.1271 | 0.1450
Epoch 168/300, Loss: 0.1284 | 0.1450
Epoch 169/300, Loss: 0.1278 | 0.1450
Epoch 170/300, Loss: 0.1275 | 0.1450
Epoch 171/300, Loss: 0.1275 | 0.1450
Epoch 172/300, Loss: 0.1276 | 0.1450
Epoch 173/300, Loss: 0.1274 | 0.1449
Epoch 174/300, Loss: 0.1280 | 0.1449
Epoch 175/300, Loss: 0.1275 | 0.1449
Epoch 176/300, Loss: 0.1276 | 0.1448
Epoch 177/300, Loss: 0.1279 | 0.1448
Epoch 178/300, Loss: 0.1272 | 0.1448
Epoch 179/300, Loss: 0.1270 | 0.1449
Epoch 180/300, Loss: 0.1271 | 0.1449
Epoch 181/300, Loss: 0.1275 | 0.1448
Epoch 182/300, Loss: 0.1270 | 0.1448
Epoch 183/300, Loss: 0.1270 | 0.1448
Epoch 184/300, Loss: 0.1267 | 0.1448
Epoch 185/300, Loss: 0.1278 | 0.1448
Epoch 186/300, Loss: 0.1277 | 0.1448
Epoch 187/300, Loss: 0.1272 | 0.1447
Epoch 188/300, Loss: 0.1274 | 0.1448
Epoch 189/300, Loss: 0.1275 | 0.1448
Epoch 190/300, Loss: 0.1276 | 0.1447
Epoch 191/300, Loss: 0.1278 | 0.1447
Epoch 192/300, Loss: 0.1276 | 0.1447
Epoch 193/300, Loss: 0.1274 | 0.1447
Epoch 194/300, Loss: 0.1274 | 0.1447
Epoch 195/300, Loss: 0.1275 | 0.1446
Epoch 196/300, Loss: 0.1275 | 0.1446
Epoch 197/300, Loss: 0.1270 | 0.1446
Epoch 198/300, Loss: 0.1266 | 0.1446
Epoch 199/300, Loss: 0.1269 | 0.1446
Epoch 200/300, Loss: 0.1269 | 0.1446
Epoch 201/300, Loss: 0.1271 | 0.1445
Epoch 202/300, Loss: 0.1275 | 0.1446
Epoch 203/300, Loss: 0.1280 | 0.1445
Epoch 204/300, Loss: 0.1271 | 0.1445
Epoch 205/300, Loss: 0.1275 | 0.1445
Epoch 206/300, Loss: 0.1275 | 0.1445
Epoch 207/300, Loss: 0.1270 | 0.1445
Epoch 208/300, Loss: 0.1272 | 0.1445
Epoch 209/300, Loss: 0.1278 | 0.1445
Epoch 210/300, Loss: 0.1270 | 0.1445
Epoch 211/300, Loss: 0.1270 | 0.1445
Epoch 212/300, Loss: 0.1274 | 0.1445
Epoch 213/300, Loss: 0.1274 | 0.1445
Epoch 214/300, Loss: 0.1273 | 0.1445
Epoch 215/300, Loss: 0.1271 | 0.1445
Epoch 216/300, Loss: 0.1275 | 0.1445
Epoch 217/300, Loss: 0.1274 | 0.1444
Epoch 218/300, Loss: 0.1271 | 0.1444
Epoch 219/300, Loss: 0.1276 | 0.1444
Epoch 220/300, Loss: 0.1276 | 0.1444
Epoch 221/300, Loss: 0.1266 | 0.1444
Epoch 222/300, Loss: 0.1270 | 0.1444
Epoch 223/300, Loss: 0.1268 | 0.1444
Epoch 224/300, Loss: 0.1269 | 0.1444
Epoch 225/300, Loss: 0.1276 | 0.1444
Epoch 226/300, Loss: 0.1276 | 0.1444
Epoch 227/300, Loss: 0.1269 | 0.1444
Epoch 228/300, Loss: 0.1270 | 0.1444
Epoch 229/300, Loss: 0.1272 | 0.1444
Epoch 230/300, Loss: 0.1267 | 0.1444
Epoch 231/300, Loss: 0.1271 | 0.1444
Epoch 232/300, Loss: 0.1266 | 0.1444
Epoch 233/300, Loss: 0.1275 | 0.1444
Epoch 234/300, Loss: 0.1270 | 0.1444
Epoch 235/300, Loss: 0.1267 | 0.1444
Epoch 236/300, Loss: 0.1270 | 0.1444
Epoch 237/300, Loss: 0.1268 | 0.1444
Epoch 238/300, Loss: 0.1269 | 0.1444
Epoch 239/300, Loss: 0.1279 | 0.1444
Epoch 240/300, Loss: 0.1275 | 0.1444
Epoch 241/300, Loss: 0.1274 | 0.1444
Epoch 242/300, Loss: 0.1268 | 0.1444
Epoch 243/300, Loss: 0.1271 | 0.1444
Epoch 244/300, Loss: 0.1275 | 0.1444
Epoch 245/300, Loss: 0.1271 | 0.1444
Epoch 246/300, Loss: 0.1269 | 0.1444
Epoch 247/300, Loss: 0.1278 | 0.1444
Epoch 248/300, Loss: 0.1269 | 0.1444
Epoch 249/300, Loss: 0.1272 | 0.1444
Epoch 250/300, Loss: 0.1269 | 0.1444
Epoch 251/300, Loss: 0.1268 | 0.1444
Epoch 252/300, Loss: 0.1275 | 0.1444
Epoch 253/300, Loss: 0.1271 | 0.1444
Epoch 254/300, Loss: 0.1272 | 0.1444
Epoch 255/300, Loss: 0.1276 | 0.1444
Epoch 256/300, Loss: 0.1271 | 0.1444
Epoch 257/300, Loss: 0.1270 | 0.1444
Epoch 258/300, Loss: 0.1273 | 0.1444
Epoch 259/300, Loss: 0.1275 | 0.1444
Epoch 260/300, Loss: 0.1272 | 0.1444
Epoch 261/300, Loss: 0.1272 | 0.1444
Epoch 262/300, Loss: 0.1265 | 0.1444
Epoch 263/300, Loss: 0.1270 | 0.1444
Epoch 264/300, Loss: 0.1274 | 0.1444
Epoch 265/300, Loss: 0.1272 | 0.1444
Epoch 266/300, Loss: 0.1274 | 0.1444
Epoch 267/300, Loss: 0.1273 | 0.1444
Epoch 268/300, Loss: 0.1275 | 0.1444
Epoch 269/300, Loss: 0.1275 | 0.1444
Epoch 270/300, Loss: 0.1267 | 0.1444
Epoch 271/300, Loss: 0.1275 | 0.1444
Epoch 272/300, Loss: 0.1268 | 0.1444
Epoch 273/300, Loss: 0.1274 | 0.1444
Epoch 274/300, Loss: 0.1278 | 0.1444
Epoch 275/300, Loss: 0.1274 | 0.1444
Epoch 276/300, Loss: 0.1272 | 0.1444
Epoch 277/300, Loss: 0.1273 | 0.1444
Epoch 278/300, Loss: 0.1271 | 0.1444
Epoch 279/300, Loss: 0.1274 | 0.1444
Epoch 280/300, Loss: 0.1270 | 0.1444
Epoch 281/300, Loss: 0.1274 | 0.1444
Epoch 282/300, Loss: 0.1272 | 0.1444
Epoch 283/300, Loss: 0.1269 | 0.1444
Epoch 284/300, Loss: 0.1275 | 0.1444
Epoch 285/300, Loss: 0.1269 | 0.1444
Epoch 286/300, Loss: 0.1276 | 0.1444
Epoch 287/300, Loss: 0.1271 | 0.1444
Epoch 288/300, Loss: 0.1270 | 0.1444
Epoch 289/300, Loss: 0.1272 | 0.1444
Epoch 290/300, Loss: 0.1274 | 0.1444
Epoch 291/300, Loss: 0.1272 | 0.1444
Epoch 292/300, Loss: 0.1272 | 0.1444
Epoch 293/300, Loss: 0.1265 | 0.1444
Epoch 294/300, Loss: 0.1274 | 0.1444
Epoch 295/300, Loss: 0.1268 | 0.1444
Epoch 296/300, Loss: 0.1265 | 0.1444
Epoch 297/300, Loss: 0.1273 | 0.1444
Epoch 298/300, Loss: 0.1275 | 0.1444
Epoch 299/300, Loss: 0.1268 | 0.1444
Epoch 300/300, Loss: 0.1280 | 0.1444
Runtime (seconds): 901.374279499054
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1153.1162416210864
RMSE: 33.95756530761719
MAE: 33.95756530761719
R-squared: nan
[195.09244]
