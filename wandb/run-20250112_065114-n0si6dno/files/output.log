ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-12 06:51:15,540][0m A new study created in memory with name: no-name-a6ad5edf-a385-47c0-a140-0fe89e9ca4c4[0m
[32m[I 2025-01-12 06:51:54,143][0m Trial 0 finished with value: 0.7080346422104896 and parameters: {'observation_period_num': 185, 'train_rates': 0.7147679961314017, 'learning_rate': 5.257120905084747e-06, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8691717501880076}. Best is trial 0 with value: 0.7080346422104896.[0m
[32m[I 2025-01-12 06:52:15,613][0m Trial 1 finished with value: 0.6141608502250688 and parameters: {'observation_period_num': 145, 'train_rates': 0.6837469875025459, 'learning_rate': 1.6165239682816853e-05, 'batch_size': 222, 'step_size': 13, 'gamma': 0.9297565518575214}. Best is trial 1 with value: 0.6141608502250688.[0m
[32m[I 2025-01-12 06:55:07,897][0m Trial 2 finished with value: 0.08925000673685318 and parameters: {'observation_period_num': 187, 'train_rates': 0.9862120402359449, 'learning_rate': 0.0005145611608781162, 'batch_size': 32, 'step_size': 10, 'gamma': 0.8529054144198869}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 06:55:46,586][0m Trial 3 finished with value: 0.6687757102381281 and parameters: {'observation_period_num': 245, 'train_rates': 0.6924318086780523, 'learning_rate': 1.0878882141156119e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.7764275436467059}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 06:56:10,164][0m Trial 4 finished with value: 0.7373115273238061 and parameters: {'observation_period_num': 143, 'train_rates': 0.69460314951114, 'learning_rate': 9.952306663092104e-06, 'batch_size': 202, 'step_size': 4, 'gamma': 0.930958127300084}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 06:59:36,516][0m Trial 5 finished with value: 0.5859902852111393 and parameters: {'observation_period_num': 194, 'train_rates': 0.9675130734547681, 'learning_rate': 1.3547219674682928e-06, 'batch_size': 26, 'step_size': 10, 'gamma': 0.9519267599782563}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:00:33,331][0m Trial 6 finished with value: 0.2037361705549347 and parameters: {'observation_period_num': 58, 'train_rates': 0.6487259548889285, 'learning_rate': 9.780817929513403e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.9255359950619252}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:01:04,217][0m Trial 7 finished with value: 1.514569042020776 and parameters: {'observation_period_num': 41, 'train_rates': 0.6381156519115588, 'learning_rate': 2.871096213686131e-06, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9848862408583572}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:01:31,802][0m Trial 8 finished with value: 0.776680376008153 and parameters: {'observation_period_num': 59, 'train_rates': 0.7239000980113063, 'learning_rate': 7.094454565120594e-06, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8480664105983561}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:01:56,028][0m Trial 9 finished with value: 0.3470121101268287 and parameters: {'observation_period_num': 13, 'train_rates': 0.894946404568117, 'learning_rate': 9.352224248990827e-05, 'batch_size': 244, 'step_size': 3, 'gamma': 0.7694218913107485}. Best is trial 2 with value: 0.08925000673685318.[0m
Early stopping at epoch 99
[32m[I 2025-01-12 07:04:15,088][0m Trial 10 finished with value: 0.10456861873467763 and parameters: {'observation_period_num': 250, 'train_rates': 0.8337237227506128, 'learning_rate': 0.0009996804750848044, 'batch_size': 34, 'step_size': 1, 'gamma': 0.8238968447961735}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:06:45,619][0m Trial 11 finished with value: 0.12092072195866528 and parameters: {'observation_period_num': 250, 'train_rates': 0.8428653549866446, 'learning_rate': 0.0007966996370315072, 'batch_size': 32, 'step_size': 1, 'gamma': 0.8240364802844582}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:08:10,959][0m Trial 12 finished with value: 0.10452834516763687 and parameters: {'observation_period_num': 205, 'train_rates': 0.9889867215369228, 'learning_rate': 0.0007883841453782349, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8160396781953614}. Best is trial 2 with value: 0.08925000673685318.[0m
[32m[I 2025-01-12 07:09:29,969][0m Trial 13 finished with value: 0.08575619757175446 and parameters: {'observation_period_num': 199, 'train_rates': 0.9823311768054496, 'learning_rate': 0.00021356361332938004, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8082494352259895}. Best is trial 13 with value: 0.08575619757175446.[0m
[32m[I 2025-01-12 07:10:43,818][0m Trial 14 finished with value: 0.07200541995363692 and parameters: {'observation_period_num': 104, 'train_rates': 0.9233163224343512, 'learning_rate': 0.00022400434357063896, 'batch_size': 75, 'step_size': 9, 'gamma': 0.8903186049956051}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:11:47,758][0m Trial 15 finished with value: 0.07591370287260929 and parameters: {'observation_period_num': 102, 'train_rates': 0.9171231863769146, 'learning_rate': 0.00022011443114730236, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8992173768161886}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:12:24,133][0m Trial 16 finished with value: 0.17858743622141368 and parameters: {'observation_period_num': 100, 'train_rates': 0.8966977521860092, 'learning_rate': 5.086230103806377e-05, 'batch_size': 153, 'step_size': 7, 'gamma': 0.8976500368035608}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:13:22,298][0m Trial 17 finished with value: 0.08439668562067182 and parameters: {'observation_period_num': 103, 'train_rates': 0.9200636094566172, 'learning_rate': 0.0002321451342920956, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8959121007164629}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:14:10,726][0m Trial 18 finished with value: 0.22739658224610224 and parameters: {'observation_period_num': 110, 'train_rates': 0.7719316740311268, 'learning_rate': 0.00022487310147316215, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8861101731206453}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:14:47,240][0m Trial 19 finished with value: 0.1237530381101923 and parameters: {'observation_period_num': 77, 'train_rates': 0.932553527159686, 'learning_rate': 6.933145243191751e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.9734336102975489}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:16:09,316][0m Trial 20 finished with value: 0.23183682928153124 and parameters: {'observation_period_num': 159, 'train_rates': 0.8490563025183041, 'learning_rate': 3.1831930180907946e-05, 'batch_size': 61, 'step_size': 3, 'gamma': 0.9071589805859611}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:17:10,292][0m Trial 21 finished with value: 0.08105980034982949 and parameters: {'observation_period_num': 108, 'train_rates': 0.9001683440667445, 'learning_rate': 0.0002276138810021595, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8748140818050976}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:18:11,465][0m Trial 22 finished with value: 0.08155885610092922 and parameters: {'observation_period_num': 120, 'train_rates': 0.8740026052466323, 'learning_rate': 0.0003520610710201377, 'batch_size': 88, 'step_size': 5, 'gamma': 0.8771481085496106}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:18:57,642][0m Trial 23 finished with value: 0.075629721183588 and parameters: {'observation_period_num': 89, 'train_rates': 0.929588209372196, 'learning_rate': 0.00015833000833182068, 'batch_size': 125, 'step_size': 8, 'gamma': 0.852481902911805}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:19:43,681][0m Trial 24 finished with value: 0.07993918357462418 and parameters: {'observation_period_num': 85, 'train_rates': 0.9428983567885387, 'learning_rate': 0.0001263197212264091, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8542706264966335}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:21:09,326][0m Trial 25 finished with value: 0.23771827070809462 and parameters: {'observation_period_num': 33, 'train_rates': 0.7672986485373, 'learning_rate': 3.7670049377367894e-05, 'batch_size': 57, 'step_size': 11, 'gamma': 0.9153652782800131}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:21:43,134][0m Trial 26 finished with value: 0.10224688053131104 and parameters: {'observation_period_num': 81, 'train_rates': 0.952320569138086, 'learning_rate': 0.0003411845468921288, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8402696490345529}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:22:28,664][0m Trial 27 finished with value: 0.09414994621530492 and parameters: {'observation_period_num': 142, 'train_rates': 0.8161053767674923, 'learning_rate': 0.00014545191243605797, 'batch_size': 114, 'step_size': 9, 'gamma': 0.9518863323073423}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:24:14,876][0m Trial 28 finished with value: 0.17285148675929116 and parameters: {'observation_period_num': 125, 'train_rates': 0.8720825852466373, 'learning_rate': 1.9626412304915234e-05, 'batch_size': 49, 'step_size': 9, 'gamma': 0.7978187554482175}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:24:57,244][0m Trial 29 finished with value: 0.07360593765974045 and parameters: {'observation_period_num': 167, 'train_rates': 0.9195673152216541, 'learning_rate': 0.000535735207906375, 'batch_size': 132, 'step_size': 6, 'gamma': 0.8653738223229248}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:25:35,566][0m Trial 30 finished with value: 0.09058753796297296 and parameters: {'observation_period_num': 222, 'train_rates': 0.8620021289845196, 'learning_rate': 0.00046310889684781015, 'batch_size': 136, 'step_size': 6, 'gamma': 0.8608186233132389}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:26:23,929][0m Trial 31 finished with value: 0.07661262612305414 and parameters: {'observation_period_num': 166, 'train_rates': 0.920169323034166, 'learning_rate': 0.0006079343903377868, 'batch_size': 113, 'step_size': 6, 'gamma': 0.8695975234808472}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:27:08,128][0m Trial 32 finished with value: 0.11250926965653006 and parameters: {'observation_period_num': 158, 'train_rates': 0.9520699584804072, 'learning_rate': 0.0001500549737297981, 'batch_size': 127, 'step_size': 7, 'gamma': 0.8363669709308785}. Best is trial 14 with value: 0.07200541995363692.[0m
[32m[I 2025-01-12 07:28:02,679][0m Trial 33 finished with value: 0.061910937268902934 and parameters: {'observation_period_num': 91, 'train_rates': 0.9114999947559888, 'learning_rate': 0.0003644759979733309, 'batch_size': 104, 'step_size': 8, 'gamma': 0.889647975733808}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:28:53,043][0m Trial 34 finished with value: 0.12827000826502 and parameters: {'observation_period_num': 174, 'train_rates': 0.887520261385521, 'learning_rate': 0.0003939077850403213, 'batch_size': 106, 'step_size': 11, 'gamma': 0.8835194483923057}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:29:34,764][0m Trial 35 finished with value: 0.0735955610871315 and parameters: {'observation_period_num': 137, 'train_rates': 0.9525526991804089, 'learning_rate': 0.0006584142209220223, 'batch_size': 136, 'step_size': 8, 'gamma': 0.8630893584501702}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:30:04,913][0m Trial 36 finished with value: 0.08675876259803772 and parameters: {'observation_period_num': 133, 'train_rates': 0.9657005303095486, 'learning_rate': 0.0005609396726510723, 'batch_size': 197, 'step_size': 9, 'gamma': 0.8613929164234955}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:30:40,327][0m Trial 37 finished with value: 0.1606327134392439 and parameters: {'observation_period_num': 149, 'train_rates': 0.8036926139761243, 'learning_rate': 0.00035419225273605726, 'batch_size': 144, 'step_size': 10, 'gamma': 0.9197079003825933}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:31:14,523][0m Trial 38 finished with value: 0.08527475595474243 and parameters: {'observation_period_num': 181, 'train_rates': 0.9630783312055488, 'learning_rate': 0.0009203734247821285, 'batch_size': 168, 'step_size': 4, 'gamma': 0.8919013341107689}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:32:25,696][0m Trial 39 finished with value: 0.11737608819637659 and parameters: {'observation_period_num': 69, 'train_rates': 0.9083145490363376, 'learning_rate': 0.0006131951951677458, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9450499041656572}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:33:06,284][0m Trial 40 finished with value: 0.08932442080251055 and parameters: {'observation_period_num': 120, 'train_rates': 0.9391533622395262, 'learning_rate': 0.00030241327182076107, 'batch_size': 142, 'step_size': 6, 'gamma': 0.7871750143559906}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:33:52,218][0m Trial 41 finished with value: 0.06263964250683784 and parameters: {'observation_period_num': 90, 'train_rates': 0.9373131192986957, 'learning_rate': 0.0004972197947087334, 'batch_size': 124, 'step_size': 8, 'gamma': 0.8409528823009531}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:34:41,952][0m Trial 42 finished with value: 0.09290650486946106 and parameters: {'observation_period_num': 132, 'train_rates': 0.9741002742976488, 'learning_rate': 0.000544205237555935, 'batch_size': 118, 'step_size': 8, 'gamma': 0.8395310338874042}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:35:35,128][0m Trial 43 finished with value: 0.06599884146922513 and parameters: {'observation_period_num': 60, 'train_rates': 0.8818599177891912, 'learning_rate': 0.0006939341403355642, 'batch_size': 105, 'step_size': 11, 'gamma': 0.8674895615344745}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:36:30,245][0m Trial 44 finished with value: 0.6663363554584446 and parameters: {'observation_period_num': 45, 'train_rates': 0.8848188390416364, 'learning_rate': 3.341599095418555e-06, 'batch_size': 102, 'step_size': 13, 'gamma': 0.9340925829786665}. Best is trial 33 with value: 0.061910937268902934.[0m
[32m[I 2025-01-12 07:41:54,049][0m Trial 45 finished with value: 0.05898545982677545 and parameters: {'observation_period_num': 68, 'train_rates': 0.9535935971255709, 'learning_rate': 0.0006764158865808568, 'batch_size': 17, 'step_size': 11, 'gamma': 0.908353186745297}. Best is trial 45 with value: 0.05898545982677545.[0m
[32m[I 2025-01-12 07:47:01,041][0m Trial 46 finished with value: 0.12186803215998782 and parameters: {'observation_period_num': 60, 'train_rates': 0.8556010728460031, 'learning_rate': 0.0009637883072379008, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9104335488675063}. Best is trial 45 with value: 0.05898545982677545.[0m
[32m[I 2025-01-12 07:48:31,581][0m Trial 47 finished with value: 0.18018505897727127 and parameters: {'observation_period_num': 24, 'train_rates': 0.6122197694192388, 'learning_rate': 8.759486211973949e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8813704328093372}. Best is trial 45 with value: 0.05898545982677545.[0m
[32m[I 2025-01-12 07:49:42,124][0m Trial 48 finished with value: 0.9849587823477227 and parameters: {'observation_period_num': 48, 'train_rates': 0.8268591425629281, 'learning_rate': 1.071844806038209e-06, 'batch_size': 74, 'step_size': 12, 'gamma': 0.936880824625759}. Best is trial 45 with value: 0.05898545982677545.[0m
[32m[I 2025-01-12 07:54:25,818][0m Trial 49 finished with value: 0.10117435952027638 and parameters: {'observation_period_num': 72, 'train_rates': 0.9895927764031949, 'learning_rate': 0.0002683766693637256, 'batch_size': 20, 'step_size': 10, 'gamma': 0.9045015075714314}. Best is trial 45 with value: 0.05898545982677545.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-12 07:54:25,828][0m A new study created in memory with name: no-name-4889fe9e-3ac7-4f5b-8e2c-bcfee1e4d821[0m
[32m[I 2025-01-12 07:55:19,308][0m Trial 0 finished with value: 0.06954900431446731 and parameters: {'observation_period_num': 49, 'train_rates': 0.8678467036562914, 'learning_rate': 0.0005756617943143312, 'batch_size': 104, 'step_size': 14, 'gamma': 0.8952360834738295}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:56:50,313][0m Trial 1 finished with value: 0.4763317317689905 and parameters: {'observation_period_num': 73, 'train_rates': 0.7301763904971355, 'learning_rate': 1.6235710545427146e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.8930507745752965}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:57:28,409][0m Trial 2 finished with value: 0.3454341204944721 and parameters: {'observation_period_num': 68, 'train_rates': 0.748369971788285, 'learning_rate': 0.0002972770601526925, 'batch_size': 128, 'step_size': 2, 'gamma': 0.8092420276969888}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:58:24,616][0m Trial 3 finished with value: 0.3169819312226184 and parameters: {'observation_period_num': 20, 'train_rates': 0.7167341674764525, 'learning_rate': 1.0705964877340229e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.9122525094840223}. Best is trial 0 with value: 0.06954900431446731.[0m
Early stopping at epoch 72
[32m[I 2025-01-12 07:58:43,814][0m Trial 4 finished with value: 1.788156396150589 and parameters: {'observation_period_num': 122, 'train_rates': 0.7175778368057603, 'learning_rate': 4.205375532806379e-06, 'batch_size': 190, 'step_size': 1, 'gamma': 0.8618731814043717}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:59:09,343][0m Trial 5 finished with value: 0.22861275041645224 and parameters: {'observation_period_num': 85, 'train_rates': 0.7126391100800833, 'learning_rate': 0.0008194078526602918, 'batch_size': 191, 'step_size': 4, 'gamma': 0.8235772978680374}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:59:27,143][0m Trial 6 finished with value: 0.8163071451945229 and parameters: {'observation_period_num': 230, 'train_rates': 0.6099388688654663, 'learning_rate': 1.4005635210018845e-05, 'batch_size': 248, 'step_size': 2, 'gamma': 0.9373063338093438}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 07:59:59,054][0m Trial 7 finished with value: 0.07011377811431885 and parameters: {'observation_period_num': 32, 'train_rates': 0.9885878422687794, 'learning_rate': 0.00026275004007207296, 'batch_size': 202, 'step_size': 14, 'gamma': 0.8380043891799419}. Best is trial 0 with value: 0.06954900431446731.[0m
Early stopping at epoch 82
[32m[I 2025-01-12 08:02:19,058][0m Trial 8 finished with value: 0.2718967652839163 and parameters: {'observation_period_num': 223, 'train_rates': 0.983430566566931, 'learning_rate': 7.109706262841693e-05, 'batch_size': 32, 'step_size': 1, 'gamma': 0.8588403419559055}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 08:05:00,062][0m Trial 9 finished with value: 0.3179623926808038 and parameters: {'observation_period_num': 246, 'train_rates': 0.6686583263240784, 'learning_rate': 0.0006435183069585333, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8148031547296701}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 08:05:44,891][0m Trial 10 finished with value: 0.162032612848803 and parameters: {'observation_period_num': 174, 'train_rates': 0.8684065609273226, 'learning_rate': 7.25875098882051e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.762446848469343}. Best is trial 0 with value: 0.06954900431446731.[0m
[32m[I 2025-01-12 08:06:18,982][0m Trial 11 finished with value: 0.05733885243535042 and parameters: {'observation_period_num': 11, 'train_rates': 0.9784999632181376, 'learning_rate': 0.00017081631872580317, 'batch_size': 184, 'step_size': 15, 'gamma': 0.9877752250836713}. Best is trial 11 with value: 0.05733885243535042.[0m
[32m[I 2025-01-12 08:06:55,752][0m Trial 12 finished with value: 0.06715474911704468 and parameters: {'observation_period_num': 18, 'train_rates': 0.8676563102688417, 'learning_rate': 0.0001386025699825297, 'batch_size': 154, 'step_size': 15, 'gamma': 0.9839275343408581}. Best is trial 11 with value: 0.05733885243535042.[0m
[32m[I 2025-01-12 08:07:29,275][0m Trial 13 finished with value: 0.05532963751327424 and parameters: {'observation_period_num': 8, 'train_rates': 0.9076164994905741, 'learning_rate': 0.00010909351044835463, 'batch_size': 172, 'step_size': 11, 'gamma': 0.9878420164751064}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:08:05,376][0m Trial 14 finished with value: 0.13739548630335113 and parameters: {'observation_period_num': 103, 'train_rates': 0.922938942469412, 'learning_rate': 3.8183751712764074e-05, 'batch_size': 163, 'step_size': 11, 'gamma': 0.9857108674239163}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:08:31,315][0m Trial 15 finished with value: 0.479825884103775 and parameters: {'observation_period_num': 5, 'train_rates': 0.9217494929005647, 'learning_rate': 1.4261624159397823e-06, 'batch_size': 232, 'step_size': 12, 'gamma': 0.9544288707585316}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:08:56,131][0m Trial 16 finished with value: 0.13848908190476036 and parameters: {'observation_period_num': 160, 'train_rates': 0.8065625968188219, 'learning_rate': 0.00014643466835203095, 'batch_size': 215, 'step_size': 8, 'gamma': 0.9529224258718599}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:09:32,366][0m Trial 17 finished with value: 0.10530628356122479 and parameters: {'observation_period_num': 47, 'train_rates': 0.9334110973646722, 'learning_rate': 3.9971829819463206e-05, 'batch_size': 166, 'step_size': 8, 'gamma': 0.9889432624585572}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:10:07,635][0m Trial 18 finished with value: 0.0918746822722553 and parameters: {'observation_period_num': 150, 'train_rates': 0.8301776551845346, 'learning_rate': 0.0002327019108884041, 'batch_size': 148, 'step_size': 12, 'gamma': 0.9297377661725428}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:11:11,575][0m Trial 19 finished with value: 0.09337094039828689 and parameters: {'observation_period_num': 194, 'train_rates': 0.9512962625485957, 'learning_rate': 9.229105815778043e-05, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9602671634079873}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:11:43,805][0m Trial 20 finished with value: 0.2809229379762774 and parameters: {'observation_period_num': 49, 'train_rates': 0.8891278910663725, 'learning_rate': 2.8580043221621107e-05, 'batch_size': 177, 'step_size': 12, 'gamma': 0.7732375635839488}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:12:10,531][0m Trial 21 finished with value: 0.06169570337480573 and parameters: {'observation_period_num': 5, 'train_rates': 0.840983904387673, 'learning_rate': 0.00014543405634068445, 'batch_size': 217, 'step_size': 15, 'gamma': 0.9755126652189349}. Best is trial 13 with value: 0.05532963751327424.[0m
[32m[I 2025-01-12 08:12:35,360][0m Trial 22 finished with value: 0.05299540276633801 and parameters: {'observation_period_num': 8, 'train_rates': 0.8232588019731839, 'learning_rate': 0.00037601327197752575, 'batch_size': 221, 'step_size': 13, 'gamma': 0.9671623477663189}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:12:59,380][0m Trial 23 finished with value: 0.08083908259868622 and parameters: {'observation_period_num': 38, 'train_rates': 0.9553286512494624, 'learning_rate': 0.0004007510755622276, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9657155799137559}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:13:24,279][0m Trial 24 finished with value: 0.23554963165456733 and parameters: {'observation_period_num': 64, 'train_rates': 0.7774667100256636, 'learning_rate': 0.0004398059161829676, 'batch_size': 223, 'step_size': 10, 'gamma': 0.9358276325855819}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:13:54,575][0m Trial 25 finished with value: 0.08844843361435867 and parameters: {'observation_period_num': 101, 'train_rates': 0.8995399810388552, 'learning_rate': 0.0009839075220253323, 'batch_size': 184, 'step_size': 13, 'gamma': 0.9128477719775379}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:14:24,806][0m Trial 26 finished with value: 0.06697366386651993 and parameters: {'observation_period_num': 24, 'train_rates': 0.960252393633768, 'learning_rate': 0.00018020608325155008, 'batch_size': 206, 'step_size': 9, 'gamma': 0.9691116293096615}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:14:49,479][0m Trial 27 finished with value: 0.06596944432512583 and parameters: {'observation_period_num': 8, 'train_rates': 0.8232563359455499, 'learning_rate': 9.203955992309939e-05, 'batch_size': 240, 'step_size': 13, 'gamma': 0.9466415539523988}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:15:26,499][0m Trial 28 finished with value: 0.19420609620952964 and parameters: {'observation_period_num': 32, 'train_rates': 0.7724352461512747, 'learning_rate': 0.00035477412614727234, 'batch_size': 136, 'step_size': 11, 'gamma': 0.9184690905591812}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:16:17,361][0m Trial 29 finished with value: 0.0817056610215482 and parameters: {'observation_period_num': 56, 'train_rates': 0.8561385326034109, 'learning_rate': 0.0005162269685771851, 'batch_size': 105, 'step_size': 15, 'gamma': 0.8925552848663211}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:16:49,693][0m Trial 30 finished with value: 0.10468018011135213 and parameters: {'observation_period_num': 90, 'train_rates': 0.8933139332200187, 'learning_rate': 6.39201043953663e-05, 'batch_size': 174, 'step_size': 14, 'gamma': 0.9745583677259919}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:17:16,399][0m Trial 31 finished with value: 0.061398034798661266 and parameters: {'observation_period_num': 8, 'train_rates': 0.842959284076236, 'learning_rate': 0.00012953903450037526, 'batch_size': 221, 'step_size': 15, 'gamma': 0.9779032333911258}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:17:42,754][0m Trial 32 finished with value: 0.08145947432061797 and parameters: {'observation_period_num': 45, 'train_rates': 0.7933888829310178, 'learning_rate': 0.00019085195157425127, 'batch_size': 201, 'step_size': 15, 'gamma': 0.9670959489029842}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:18:08,374][0m Trial 33 finished with value: 0.06558855887382262 and parameters: {'observation_period_num': 20, 'train_rates': 0.9052076691772517, 'learning_rate': 0.00011551106575443853, 'batch_size': 223, 'step_size': 14, 'gamma': 0.9856702024814789}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:18:32,513][0m Trial 34 finished with value: 0.23122068899325454 and parameters: {'observation_period_num': 72, 'train_rates': 0.8559263546704681, 'learning_rate': 2.3622093932013923e-05, 'batch_size': 239, 'step_size': 13, 'gamma': 0.94541582266491}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:19:01,549][0m Trial 35 finished with value: 0.11220826618993128 and parameters: {'observation_period_num': 29, 'train_rates': 0.8152383302490209, 'learning_rate': 5.522511609780707e-05, 'batch_size': 192, 'step_size': 11, 'gamma': 0.8929726351083966}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:19:26,503][0m Trial 36 finished with value: 0.18890298714693787 and parameters: {'observation_period_num': 16, 'train_rates': 0.7503651372616431, 'learning_rate': 0.00026734241979110965, 'batch_size': 209, 'step_size': 15, 'gamma': 0.9246071283074089}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:20:05,198][0m Trial 37 finished with value: 0.30058912391012366 and parameters: {'observation_period_num': 59, 'train_rates': 0.8786411642421509, 'learning_rate': 8.787720638114519e-06, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9896413992400946}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:20:29,523][0m Trial 38 finished with value: 0.12114404125122337 and parameters: {'observation_period_num': 129, 'train_rates': 0.8428005002578554, 'learning_rate': 0.0006437597072602861, 'batch_size': 229, 'step_size': 7, 'gamma': 0.8789993101511386}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:21:01,618][0m Trial 39 finished with value: 0.07356909662485123 and parameters: {'observation_period_num': 38, 'train_rates': 0.9706360388204474, 'learning_rate': 0.000307123711082939, 'batch_size': 191, 'step_size': 14, 'gamma': 0.786060033127808}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:21:38,031][0m Trial 40 finished with value: 0.13006931776441008 and parameters: {'observation_period_num': 81, 'train_rates': 0.9376845498710918, 'learning_rate': 4.787187813336112e-05, 'batch_size': 166, 'step_size': 12, 'gamma': 0.9447041353168597}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:22:04,498][0m Trial 41 finished with value: 0.06456848663293709 and parameters: {'observation_period_num': 6, 'train_rates': 0.84008564841764, 'learning_rate': 0.00011150857522430436, 'batch_size': 216, 'step_size': 15, 'gamma': 0.9747826326075005}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:22:31,732][0m Trial 42 finished with value: 0.05713520606456719 and parameters: {'observation_period_num': 5, 'train_rates': 0.7936089906075824, 'learning_rate': 0.00018411249728490813, 'batch_size': 198, 'step_size': 14, 'gamma': 0.975252728002154}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:23:00,984][0m Trial 43 finished with value: 0.058073118549262365 and parameters: {'observation_period_num': 16, 'train_rates': 0.7929132286424446, 'learning_rate': 0.00020215520983171353, 'batch_size': 180, 'step_size': 14, 'gamma': 0.9609472551590454}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:23:30,586][0m Trial 44 finished with value: 0.2018428701127459 and parameters: {'observation_period_num': 23, 'train_rates': 0.7376464129750667, 'learning_rate': 0.00020141207610655922, 'batch_size': 176, 'step_size': 14, 'gamma': 0.9577974598162494}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:23:56,015][0m Trial 45 finished with value: 0.17861413397504303 and parameters: {'observation_period_num': 35, 'train_rates': 0.6860311343013501, 'learning_rate': 0.0006878975092432989, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9602458953745411}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:24:29,207][0m Trial 46 finished with value: 0.18804239168587075 and parameters: {'observation_period_num': 16, 'train_rates': 0.7698193915149618, 'learning_rate': 0.00033004852553646964, 'batch_size': 155, 'step_size': 14, 'gamma': 0.8447155320654396}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:24:58,441][0m Trial 47 finished with value: 0.08207148855712078 and parameters: {'observation_period_num': 27, 'train_rates': 0.7894292482754414, 'learning_rate': 7.962648289000148e-05, 'batch_size': 183, 'step_size': 12, 'gamma': 0.9376722547927977}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:25:34,755][0m Trial 48 finished with value: 0.18663092935617848 and parameters: {'observation_period_num': 45, 'train_rates': 0.7000428400014567, 'learning_rate': 0.00022095311960237574, 'batch_size': 129, 'step_size': 11, 'gamma': 0.978237846287967}. Best is trial 22 with value: 0.05299540276633801.[0m
[32m[I 2025-01-12 08:26:06,186][0m Trial 49 finished with value: 2.1961931739439513 and parameters: {'observation_period_num': 18, 'train_rates': 0.7561685164221398, 'learning_rate': 1.3278902324162362e-06, 'batch_size': 163, 'step_size': 4, 'gamma': 0.906254618887987}. Best is trial 22 with value: 0.05299540276633801.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-12 08:26:06,196][0m A new study created in memory with name: no-name-e7336fcd-42b6-4d4b-a7b0-a1a8520ab3e0[0m
[32m[I 2025-01-12 08:28:45,720][0m Trial 0 finished with value: 0.34250173939669387 and parameters: {'observation_period_num': 59, 'train_rates': 0.7402154851156219, 'learning_rate': 1.3664968743374854e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7847717298034455}. Best is trial 0 with value: 0.34250173939669387.[0m
[32m[I 2025-01-12 08:29:51,629][0m Trial 1 finished with value: 0.3714831101743481 and parameters: {'observation_period_num': 78, 'train_rates': 0.7527119599859677, 'learning_rate': 2.313460839996513e-05, 'batch_size': 73, 'step_size': 12, 'gamma': 0.8134261205436372}. Best is trial 0 with value: 0.34250173939669387.[0m
[32m[I 2025-01-12 08:30:25,814][0m Trial 2 finished with value: 2.493932752048268 and parameters: {'observation_period_num': 42, 'train_rates': 0.8542270429217678, 'learning_rate': 1.295651080998354e-06, 'batch_size': 165, 'step_size': 6, 'gamma': 0.8189314475085104}. Best is trial 0 with value: 0.34250173939669387.[0m
[32m[I 2025-01-12 08:30:45,130][0m Trial 3 finished with value: 0.8940206149115635 and parameters: {'observation_period_num': 47, 'train_rates': 0.6578683014371355, 'learning_rate': 6.617533426923552e-06, 'batch_size': 255, 'step_size': 11, 'gamma': 0.9429065126843035}. Best is trial 0 with value: 0.34250173939669387.[0m
[32m[I 2025-01-12 08:31:11,893][0m Trial 4 finished with value: 0.1998458968623182 and parameters: {'observation_period_num': 41, 'train_rates': 0.6234415457013245, 'learning_rate': 0.000698638581607103, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8245647615322723}. Best is trial 4 with value: 0.1998458968623182.[0m
[32m[I 2025-01-12 08:31:35,508][0m Trial 5 finished with value: 0.4299185308444239 and parameters: {'observation_period_num': 88, 'train_rates': 0.7610441211916279, 'learning_rate': 3.375689128505096e-05, 'batch_size': 223, 'step_size': 8, 'gamma': 0.9691705174244287}. Best is trial 4 with value: 0.1998458968623182.[0m
[32m[I 2025-01-12 08:32:11,939][0m Trial 6 finished with value: 0.3965724854210653 and parameters: {'observation_period_num': 43, 'train_rates': 0.7529551277635852, 'learning_rate': 3.197558897452534e-05, 'batch_size': 139, 'step_size': 4, 'gamma': 0.9459814450153857}. Best is trial 4 with value: 0.1998458968623182.[0m
Early stopping at epoch 62
[32m[I 2025-01-12 08:32:37,163][0m Trial 7 finished with value: 1.2485062911730855 and parameters: {'observation_period_num': 174, 'train_rates': 0.7859325261738794, 'learning_rate': 1.3639289069503866e-05, 'batch_size': 124, 'step_size': 1, 'gamma': 0.8112529078788178}. Best is trial 4 with value: 0.1998458968623182.[0m
[32m[I 2025-01-12 08:33:13,323][0m Trial 8 finished with value: 0.09190549701452255 and parameters: {'observation_period_num': 140, 'train_rates': 0.9724025061890202, 'learning_rate': 0.0009040959136609464, 'batch_size': 161, 'step_size': 11, 'gamma': 0.7656660284469667}. Best is trial 8 with value: 0.09190549701452255.[0m
Early stopping at epoch 74
[32m[I 2025-01-12 08:35:45,865][0m Trial 9 finished with value: 1.3749642847559829 and parameters: {'observation_period_num': 192, 'train_rates': 0.7548350647731784, 'learning_rate': 1.7242595918283542e-06, 'batch_size': 22, 'step_size': 1, 'gamma': 0.8594996408742546}. Best is trial 8 with value: 0.09190549701452255.[0m
[32m[I 2025-01-12 08:36:15,633][0m Trial 10 finished with value: 0.11691238731145859 and parameters: {'observation_period_num': 230, 'train_rates': 0.9899038167942119, 'learning_rate': 0.0009175912185950824, 'batch_size': 203, 'step_size': 15, 'gamma': 0.7555609810218246}. Best is trial 8 with value: 0.09190549701452255.[0m
[32m[I 2025-01-12 08:36:45,732][0m Trial 11 finished with value: 0.08619684725999832 and parameters: {'observation_period_num': 226, 'train_rates': 0.9879264173799681, 'learning_rate': 0.0009685027128043512, 'batch_size': 199, 'step_size': 15, 'gamma': 0.756341932796182}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:37:15,432][0m Trial 12 finished with value: 0.14694379270076752 and parameters: {'observation_period_num': 128, 'train_rates': 0.9898264503174237, 'learning_rate': 0.0002191000405069388, 'batch_size': 200, 'step_size': 15, 'gamma': 0.7588698921913896}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:38:07,862][0m Trial 13 finished with value: 0.09011674748854039 and parameters: {'observation_period_num': 231, 'train_rates': 0.9034901816976374, 'learning_rate': 0.00012747146348002762, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8775974681065879}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:39:00,851][0m Trial 14 finished with value: 0.08954832905809239 and parameters: {'observation_period_num': 250, 'train_rates': 0.9051270460943501, 'learning_rate': 0.00013975689132270485, 'batch_size': 99, 'step_size': 14, 'gamma': 0.889344058971432}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:40:08,104][0m Trial 15 finished with value: 0.09759573153998136 and parameters: {'observation_period_num': 248, 'train_rates': 0.9101727850370679, 'learning_rate': 0.0002316423009198997, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8886597297030051}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:41:36,995][0m Trial 16 finished with value: 0.09966023539289318 and parameters: {'observation_period_num': 201, 'train_rates': 0.9160041884702752, 'learning_rate': 9.29970739979539e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.910669100246143}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:42:21,745][0m Trial 17 finished with value: 0.09669798006114083 and parameters: {'observation_period_num': 159, 'train_rates': 0.8366252578411901, 'learning_rate': 0.00031428457450971797, 'batch_size': 116, 'step_size': 15, 'gamma': 0.8620792886149906}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:42:45,931][0m Trial 18 finished with value: 0.1830219328403473 and parameters: {'observation_period_num': 210, 'train_rates': 0.9380458286217115, 'learning_rate': 7.969215486347702e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.9172768173086687}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:43:14,256][0m Trial 19 finished with value: 0.1064070500433445 and parameters: {'observation_period_num': 249, 'train_rates': 0.865677123741324, 'learning_rate': 0.0004032040378776965, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8452655032922894}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:43:54,385][0m Trial 20 finished with value: 0.18002764617695527 and parameters: {'observation_period_num': 116, 'train_rates': 0.946354106268264, 'learning_rate': 5.937480512368239e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.786907571001277}. Best is trial 11 with value: 0.08619684725999832.[0m
[32m[I 2025-01-12 08:44:49,856][0m Trial 21 finished with value: 0.054390732300861264 and parameters: {'observation_period_num': 7, 'train_rates': 0.8885791910733839, 'learning_rate': 0.00013596666204018445, 'batch_size': 102, 'step_size': 13, 'gamma': 0.8896147074163283}. Best is trial 21 with value: 0.054390732300861264.[0m
[32m[I 2025-01-12 08:45:44,899][0m Trial 22 finished with value: 0.05184940731598307 and parameters: {'observation_period_num': 13, 'train_rates': 0.8750124913141173, 'learning_rate': 0.0001343222291121172, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9026556285975819}. Best is trial 22 with value: 0.05184940731598307.[0m
[32m[I 2025-01-12 08:47:18,490][0m Trial 23 finished with value: 0.031674780313954744 and parameters: {'observation_period_num': 5, 'train_rates': 0.8259420758579452, 'learning_rate': 0.0004920066681186387, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9060231932367988}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 08:49:11,534][0m Trial 24 finished with value: 0.03392398101219002 and parameters: {'observation_period_num': 8, 'train_rates': 0.865534292284923, 'learning_rate': 0.000432836782033141, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9133340439259442}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 08:51:21,192][0m Trial 25 finished with value: 0.03585281931458319 and parameters: {'observation_period_num': 10, 'train_rates': 0.8083608805471629, 'learning_rate': 0.0004086265045161208, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9148002894006967}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 08:53:26,026][0m Trial 26 finished with value: 0.040850727595667935 and parameters: {'observation_period_num': 22, 'train_rates': 0.8179339742263705, 'learning_rate': 0.0005258616091470169, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9357433718763212}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 08:55:08,477][0m Trial 27 finished with value: 0.052695138056555554 and parameters: {'observation_period_num': 25, 'train_rates': 0.8035720347932208, 'learning_rate': 0.0004734051138785549, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9728349771937224}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 08:56:20,724][0m Trial 28 finished with value: 0.06907239373562471 and parameters: {'observation_period_num': 92, 'train_rates': 0.8336032102514211, 'learning_rate': 0.00025486234000629283, 'batch_size': 72, 'step_size': 12, 'gamma': 0.9308555241291805}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:00:42,823][0m Trial 29 finished with value: 0.2114812257689434 and parameters: {'observation_period_num': 69, 'train_rates': 0.7160451535005304, 'learning_rate': 0.0005490614339598285, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9225307618700131}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:02:30,126][0m Trial 30 finished with value: 0.21700320688665428 and parameters: {'observation_period_num': 60, 'train_rates': 0.7197629879554085, 'learning_rate': 5.722637196880749e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9888674439437287}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:05:02,229][0m Trial 31 finished with value: 0.05776021081735106 and parameters: {'observation_period_num': 25, 'train_rates': 0.8085034531356616, 'learning_rate': 0.00037573065624691684, 'batch_size': 33, 'step_size': 11, 'gamma': 0.9441246370814605}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:06:25,556][0m Trial 32 finished with value: 0.0455687463239102 and parameters: {'observation_period_num': 24, 'train_rates': 0.8284866917506459, 'learning_rate': 0.0006233705919613823, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9012880334417109}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:08:59,400][0m Trial 33 finished with value: 0.17090302880442068 and parameters: {'observation_period_num': 10, 'train_rates': 0.7818189700943875, 'learning_rate': 0.00019962487894324346, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9289832002646611}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:10:03,300][0m Trial 34 finished with value: 0.053960097722182084 and parameters: {'observation_period_num': 31, 'train_rates': 0.8213541344574692, 'learning_rate': 0.00035615202570501406, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9597574618488256}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:11:53,721][0m Trial 35 finished with value: 0.08903031331197969 and parameters: {'observation_period_num': 56, 'train_rates': 0.8500211775197453, 'learning_rate': 0.0005814419507780017, 'batch_size': 47, 'step_size': 11, 'gamma': 0.9332297690740492}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:13:19,766][0m Trial 36 finished with value: 0.3997872959285027 and parameters: {'observation_period_num': 6, 'train_rates': 0.7226328604155573, 'learning_rate': 4.0661579683698935e-06, 'batch_size': 56, 'step_size': 6, 'gamma': 0.9044052643794265}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:15:39,777][0m Trial 37 finished with value: 0.21879442296981622 and parameters: {'observation_period_num': 40, 'train_rates': 0.7849605013971069, 'learning_rate': 1.98349027550496e-05, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9558558718923424}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:16:57,641][0m Trial 38 finished with value: 0.06268060597806023 and parameters: {'observation_period_num': 77, 'train_rates': 0.855568792427487, 'learning_rate': 0.00018874326793668925, 'batch_size': 68, 'step_size': 11, 'gamma': 0.8740437981065428}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:17:51,876][0m Trial 39 finished with value: 0.1701929739676416 and parameters: {'observation_period_num': 51, 'train_rates': 0.696870673577371, 'learning_rate': 0.0006155025723194077, 'batch_size': 86, 'step_size': 7, 'gamma': 0.9155174254924611}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:21:25,119][0m Trial 40 finished with value: 0.35530589352838876 and parameters: {'observation_period_num': 100, 'train_rates': 0.7703387654254131, 'learning_rate': 7.4360847851415765e-06, 'batch_size': 22, 'step_size': 10, 'gamma': 0.9403664311783284}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:22:44,306][0m Trial 41 finished with value: 0.051633145941315954 and parameters: {'observation_period_num': 24, 'train_rates': 0.8192936024153272, 'learning_rate': 0.0007120507002611466, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8993072291031682}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:24:41,864][0m Trial 42 finished with value: 0.04205021865311123 and parameters: {'observation_period_num': 34, 'train_rates': 0.8420304788583575, 'learning_rate': 0.0003312327630789056, 'batch_size': 44, 'step_size': 13, 'gamma': 0.897804279836498}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:26:49,454][0m Trial 43 finished with value: 0.04269228245451059 and parameters: {'observation_period_num': 38, 'train_rates': 0.8811744736508824, 'learning_rate': 0.00030956503136436134, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8837611785369537}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:28:32,445][0m Trial 44 finished with value: 0.03448045885144735 and parameters: {'observation_period_num': 18, 'train_rates': 0.847303198317642, 'learning_rate': 0.0007798956710310253, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8464114593410338}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:30:06,651][0m Trial 45 finished with value: 0.04303596670971389 and parameters: {'observation_period_num': 18, 'train_rates': 0.800376683740258, 'learning_rate': 0.0009875178551827888, 'batch_size': 54, 'step_size': 3, 'gamma': 0.8243441067297278}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:33:03,950][0m Trial 46 finished with value: 0.03234814561850636 and parameters: {'observation_period_num': 5, 'train_rates': 0.8567509193590245, 'learning_rate': 0.0004618842059451607, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8496099349441122}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:38:21,490][0m Trial 47 finished with value: 0.13238161743130933 and parameters: {'observation_period_num': 46, 'train_rates': 0.8613781882592435, 'learning_rate': 0.00043281185119264015, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8439924833106803}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:39:10,873][0m Trial 48 finished with value: 0.12626024337599068 and parameters: {'observation_period_num': 5, 'train_rates': 0.6166551195876901, 'learning_rate': 0.0007643516099450344, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8404617070578877}. Best is trial 23 with value: 0.031674780313954744.[0m
[32m[I 2025-01-12 09:41:50,944][0m Trial 49 finished with value: 0.13400098447365544 and parameters: {'observation_period_num': 16, 'train_rates': 0.6459483507008232, 'learning_rate': 0.0001738398073991452, 'batch_size': 27, 'step_size': 13, 'gamma': 0.8078025128491847}. Best is trial 23 with value: 0.031674780313954744.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-12 09:41:50,955][0m A new study created in memory with name: no-name-d9205ab6-f2af-4a47-9000-d54e3d798be3[0m
[32m[I 2025-01-12 09:42:46,990][0m Trial 0 finished with value: 0.09400450019165874 and parameters: {'observation_period_num': 212, 'train_rates': 0.8248950277211554, 'learning_rate': 0.00025092515387400017, 'batch_size': 90, 'step_size': 7, 'gamma': 0.83163484764071}. Best is trial 0 with value: 0.09400450019165874.[0m
[32m[I 2025-01-12 09:43:29,599][0m Trial 1 finished with value: 0.9032213925995711 and parameters: {'observation_period_num': 231, 'train_rates': 0.7260388086336594, 'learning_rate': 2.2865946842186997e-06, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8313837992681034}. Best is trial 0 with value: 0.09400450019165874.[0m
[32m[I 2025-01-12 09:44:31,771][0m Trial 2 finished with value: 0.08115900293721454 and parameters: {'observation_period_num': 161, 'train_rates': 0.9341978674748758, 'learning_rate': 0.0005188569918966602, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8393465125478872}. Best is trial 2 with value: 0.08115900293721454.[0m
[32m[I 2025-01-12 09:45:05,105][0m Trial 3 finished with value: 0.6055607594941792 and parameters: {'observation_period_num': 15, 'train_rates': 0.6445688916753243, 'learning_rate': 4.3945610508992095e-06, 'batch_size': 138, 'step_size': 6, 'gamma': 0.9328029399197534}. Best is trial 2 with value: 0.08115900293721454.[0m
[32m[I 2025-01-12 09:48:44,818][0m Trial 4 finished with value: 0.4086090692264192 and parameters: {'observation_period_num': 129, 'train_rates': 0.7113524290409792, 'learning_rate': 3.282872959271025e-06, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9720260402094572}. Best is trial 2 with value: 0.08115900293721454.[0m
[32m[I 2025-01-12 09:49:19,144][0m Trial 5 finished with value: 1.9001698517485668 and parameters: {'observation_period_num': 47, 'train_rates': 0.738761585465221, 'learning_rate': 3.7776294748662128e-06, 'batch_size': 145, 'step_size': 12, 'gamma': 0.7710710089897304}. Best is trial 2 with value: 0.08115900293721454.[0m
Early stopping at epoch 74
[32m[I 2025-01-12 09:49:42,340][0m Trial 6 finished with value: 1.4363389206784112 and parameters: {'observation_period_num': 222, 'train_rates': 0.8567010305406519, 'learning_rate': 1.1217869437418886e-06, 'batch_size': 177, 'step_size': 2, 'gamma': 0.7808699918696469}. Best is trial 2 with value: 0.08115900293721454.[0m
[32m[I 2025-01-12 09:50:31,724][0m Trial 7 finished with value: 0.20533996200728638 and parameters: {'observation_period_num': 69, 'train_rates': 0.851597666284065, 'learning_rate': 1.964954338590949e-05, 'batch_size': 110, 'step_size': 5, 'gamma': 0.9210549506400368}. Best is trial 2 with value: 0.08115900293721454.[0m
[32m[I 2025-01-12 09:52:22,808][0m Trial 8 finished with value: 0.07698310739983062 and parameters: {'observation_period_num': 76, 'train_rates': 0.9246641964779448, 'learning_rate': 0.0002578718028529467, 'batch_size': 50, 'step_size': 7, 'gamma': 0.924734222757117}. Best is trial 8 with value: 0.07698310739983062.[0m
[32m[I 2025-01-12 09:53:13,680][0m Trial 9 finished with value: 0.25235111681750405 and parameters: {'observation_period_num': 247, 'train_rates': 0.6532395730676812, 'learning_rate': 0.0002200751517087087, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8721701768839636}. Best is trial 8 with value: 0.07698310739983062.[0m
[32m[I 2025-01-12 09:58:51,120][0m Trial 10 finished with value: 0.06742881372815272 and parameters: {'observation_period_num': 107, 'train_rates': 0.9357150064376399, 'learning_rate': 5.998052236528685e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9727749441606559}. Best is trial 10 with value: 0.06742881372815272.[0m
[32m[I 2025-01-12 10:03:32,374][0m Trial 11 finished with value: 0.0695562539405601 and parameters: {'observation_period_num': 93, 'train_rates': 0.9850618958682934, 'learning_rate': 5.836179264977112e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.9878382897799967}. Best is trial 10 with value: 0.06742881372815272.[0m
[32m[I 2025-01-12 10:03:58,375][0m Trial 12 finished with value: 0.2253796011209488 and parameters: {'observation_period_num': 114, 'train_rates': 0.9881784556315542, 'learning_rate': 4.933143534331292e-05, 'batch_size': 244, 'step_size': 9, 'gamma': 0.9829855078767262}. Best is trial 10 with value: 0.06742881372815272.[0m
[32m[I 2025-01-12 10:09:46,968][0m Trial 13 finished with value: 0.06469659701637599 and parameters: {'observation_period_num': 114, 'train_rates': 0.9759027409275365, 'learning_rate': 4.0798919089493265e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9899709251427357}. Best is trial 13 with value: 0.06469659701637599.[0m
[32m[I 2025-01-12 10:11:32,070][0m Trial 14 finished with value: 0.1402029830755459 and parameters: {'observation_period_num': 163, 'train_rates': 0.9227118215020496, 'learning_rate': 1.6188468540021216e-05, 'batch_size': 51, 'step_size': 15, 'gamma': 0.9541163638002528}. Best is trial 13 with value: 0.06469659701637599.[0m
[32m[I 2025-01-12 10:13:13,660][0m Trial 15 finished with value: 0.08407728327438235 and parameters: {'observation_period_num': 145, 'train_rates': 0.8832891329438537, 'learning_rate': 8.832944580047671e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8905480949242908}. Best is trial 13 with value: 0.06469659701637599.[0m
[32m[I 2025-01-12 10:13:39,017][0m Trial 16 finished with value: 0.6114150695381413 and parameters: {'observation_period_num': 185, 'train_rates': 0.7783238964199135, 'learning_rate': 1.0623209035932452e-05, 'batch_size': 200, 'step_size': 3, 'gamma': 0.9497230649825532}. Best is trial 13 with value: 0.06469659701637599.[0m
[32m[I 2025-01-12 10:19:20,982][0m Trial 17 finished with value: 0.058472863453276014 and parameters: {'observation_period_num': 107, 'train_rates': 0.9524788944071946, 'learning_rate': 0.00010824007985395416, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8976883529821741}. Best is trial 17 with value: 0.058472863453276014.[0m
[32m[I 2025-01-12 10:20:48,762][0m Trial 18 finished with value: 0.0671442844948479 and parameters: {'observation_period_num': 45, 'train_rates': 0.8898622421227821, 'learning_rate': 0.00014422940400680507, 'batch_size': 63, 'step_size': 15, 'gamma': 0.8865894906015394}. Best is trial 17 with value: 0.058472863453276014.[0m
[32m[I 2025-01-12 10:23:11,248][0m Trial 19 finished with value: 0.09930062535991016 and parameters: {'observation_period_num': 189, 'train_rates': 0.9577016278502751, 'learning_rate': 0.0009053649850311851, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7987207531318354}. Best is trial 17 with value: 0.058472863453276014.[0m
[32m[I 2025-01-12 10:23:45,075][0m Trial 20 finished with value: 0.2984403780359306 and parameters: {'observation_period_num': 127, 'train_rates': 0.8934025586112242, 'learning_rate': 2.9366416278398758e-05, 'batch_size': 166, 'step_size': 4, 'gamma': 0.9114186157117471}. Best is trial 17 with value: 0.058472863453276014.[0m
[32m[I 2025-01-12 10:25:08,510][0m Trial 21 finished with value: 0.05359069698879236 and parameters: {'observation_period_num': 32, 'train_rates': 0.8919704467913276, 'learning_rate': 0.0001375623783555957, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8888945031857641}. Best is trial 21 with value: 0.05359069698879236.[0m
[32m[I 2025-01-12 10:26:32,804][0m Trial 22 finished with value: 0.06367573954842308 and parameters: {'observation_period_num': 42, 'train_rates': 0.9662306048759797, 'learning_rate': 0.00011700900067153663, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8549728117217433}. Best is trial 21 with value: 0.05359069698879236.[0m
[32m[I 2025-01-12 10:27:41,808][0m Trial 23 finished with value: 0.044539868762262264 and parameters: {'observation_period_num': 9, 'train_rates': 0.8179271429593731, 'learning_rate': 0.00013329026097884495, 'batch_size': 76, 'step_size': 14, 'gamma': 0.8522911415963321}. Best is trial 23 with value: 0.044539868762262264.[0m
[32m[I 2025-01-12 10:28:28,082][0m Trial 24 finished with value: 0.04639552900558322 and parameters: {'observation_period_num': 11, 'train_rates': 0.7964906652159106, 'learning_rate': 0.0003706089871438148, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8102340460133851}. Best is trial 23 with value: 0.044539868762262264.[0m
[32m[I 2025-01-12 10:29:13,862][0m Trial 25 finished with value: 0.03831734351935934 and parameters: {'observation_period_num': 16, 'train_rates': 0.7924189324878312, 'learning_rate': 0.0006114784127005914, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8077155630877532}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:30:00,313][0m Trial 26 finished with value: 0.0615582797502358 and parameters: {'observation_period_num': 6, 'train_rates': 0.789467988456424, 'learning_rate': 0.00047267512442004337, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8044520852289301}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:30:31,987][0m Trial 27 finished with value: 0.18211386572612925 and parameters: {'observation_period_num': 26, 'train_rates': 0.7650000385528025, 'learning_rate': 0.0008306754923887898, 'batch_size': 161, 'step_size': 14, 'gamma': 0.8121983776324395}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:31:15,206][0m Trial 28 finished with value: 0.06129502724198734 and parameters: {'observation_period_num': 65, 'train_rates': 0.817660432719324, 'learning_rate': 0.0004988364553258966, 'batch_size': 124, 'step_size': 14, 'gamma': 0.7613858128766056}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:32:07,332][0m Trial 29 finished with value: 0.045634825247429164 and parameters: {'observation_period_num': 19, 'train_rates': 0.8115230303551404, 'learning_rate': 0.00026882602832152335, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8295009299977867}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:33:03,290][0m Trial 30 finished with value: 0.054437194625476755 and parameters: {'observation_period_num': 59, 'train_rates': 0.8281290765033829, 'learning_rate': 0.0002562413255932753, 'batch_size': 93, 'step_size': 13, 'gamma': 0.8337360706241139}. Best is trial 25 with value: 0.03831734351935934.[0m
[32m[I 2025-01-12 10:33:55,221][0m Trial 31 finished with value: 0.03712024160491928 and parameters: {'observation_period_num': 6, 'train_rates': 0.8112117427058345, 'learning_rate': 0.00034953422211778963, 'batch_size': 100, 'step_size': 14, 'gamma': 0.8544002100199674}. Best is trial 31 with value: 0.03712024160491928.[0m
[32m[I 2025-01-12 10:34:52,284][0m Trial 32 finished with value: 0.18907270803436882 and parameters: {'observation_period_num': 26, 'train_rates': 0.7499874705429613, 'learning_rate': 0.00019786468753466962, 'batch_size': 86, 'step_size': 11, 'gamma': 0.8545558510464422}. Best is trial 31 with value: 0.03712024160491928.[0m
[32m[I 2025-01-12 10:35:39,784][0m Trial 33 finished with value: 0.15380377198259035 and parameters: {'observation_period_num': 21, 'train_rates': 0.6963442288526265, 'learning_rate': 0.00035977253920688575, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8521756838887509}. Best is trial 31 with value: 0.03712024160491928.[0m
[32m[I 2025-01-12 10:36:21,104][0m Trial 34 finished with value: 0.053744317854152006 and parameters: {'observation_period_num': 36, 'train_rates': 0.8136924757113838, 'learning_rate': 0.000685502462503207, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8232678539627162}. Best is trial 31 with value: 0.03712024160491928.[0m
[32m[I 2025-01-12 10:37:29,020][0m Trial 35 finished with value: 0.03381244261022452 and parameters: {'observation_period_num': 8, 'train_rates': 0.8456229418099526, 'learning_rate': 0.00039113318646696903, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7929133370411884}. Best is trial 35 with value: 0.03381244261022452.[0m
[32m[I 2025-01-12 10:38:38,890][0m Trial 36 finished with value: 0.03300830301020172 and parameters: {'observation_period_num': 5, 'train_rates': 0.8457659737328437, 'learning_rate': 0.0006379327979549752, 'batch_size': 76, 'step_size': 15, 'gamma': 0.7850137987084901}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:39:18,068][0m Trial 37 finished with value: 0.05363850078519878 and parameters: {'observation_period_num': 54, 'train_rates': 0.8496401608621882, 'learning_rate': 0.0005892773867958977, 'batch_size': 141, 'step_size': 15, 'gamma': 0.7894670305757299}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:40:19,264][0m Trial 38 finished with value: 0.07779171069463094 and parameters: {'observation_period_num': 87, 'train_rates': 0.8620253819798087, 'learning_rate': 0.00036664045769816816, 'batch_size': 88, 'step_size': 15, 'gamma': 0.757579974300199}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:40:56,278][0m Trial 39 finished with value: 0.03538023474452607 and parameters: {'observation_period_num': 5, 'train_rates': 0.8401593054666475, 'learning_rate': 0.000977061158384094, 'batch_size': 154, 'step_size': 12, 'gamma': 0.785918213242899}. Best is trial 36 with value: 0.03300830301020172.[0m
Early stopping at epoch 66
[32m[I 2025-01-12 10:41:21,878][0m Trial 40 finished with value: 0.11679452424464018 and parameters: {'observation_period_num': 35, 'train_rates': 0.8424286894139804, 'learning_rate': 0.000924168951052828, 'batch_size': 152, 'step_size': 1, 'gamma': 0.7745682313977773}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:41:54,505][0m Trial 41 finished with value: 0.03980066811647078 and parameters: {'observation_period_num': 6, 'train_rates': 0.8659131612728818, 'learning_rate': 0.0006367155869878231, 'batch_size': 179, 'step_size': 12, 'gamma': 0.7857952753832558}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:42:21,617][0m Trial 42 finished with value: 0.174788942605215 and parameters: {'observation_period_num': 19, 'train_rates': 0.7585824288434487, 'learning_rate': 0.0009740089896813119, 'batch_size': 198, 'step_size': 14, 'gamma': 0.7945866197829885}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:43:00,123][0m Trial 43 finished with value: 0.1506584659804384 and parameters: {'observation_period_num': 5, 'train_rates': 0.7197411329239498, 'learning_rate': 0.0004244719708025972, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8188233537095916}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:43:51,471][0m Trial 44 finished with value: 0.05823298583190555 and parameters: {'observation_period_num': 48, 'train_rates': 0.8347623526843033, 'learning_rate': 0.0006390920831900305, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7757575967522211}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:44:35,967][0m Trial 45 finished with value: 0.06215472338796669 and parameters: {'observation_period_num': 28, 'train_rates': 0.7976988865075247, 'learning_rate': 0.00020242687268696364, 'batch_size': 117, 'step_size': 11, 'gamma': 0.7501229040701558}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:45:30,220][0m Trial 46 finished with value: 0.1476620033652436 and parameters: {'observation_period_num': 17, 'train_rates': 0.6039587345427126, 'learning_rate': 0.0003151251751499626, 'batch_size': 78, 'step_size': 12, 'gamma': 0.7651817388991403}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:48:08,593][0m Trial 47 finished with value: 0.06003131391304844 and parameters: {'observation_period_num': 72, 'train_rates': 0.9083108950999197, 'learning_rate': 0.00048553314915706687, 'batch_size': 34, 'step_size': 7, 'gamma': 0.84134889459456}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:48:44,992][0m Trial 48 finished with value: 0.4213205071238728 and parameters: {'observation_period_num': 39, 'train_rates': 0.8680746104084571, 'learning_rate': 8.957667352805078e-06, 'batch_size': 149, 'step_size': 14, 'gamma': 0.7819286389301232}. Best is trial 36 with value: 0.03300830301020172.[0m
[32m[I 2025-01-12 10:49:06,031][0m Trial 49 finished with value: 0.9916742991351482 and parameters: {'observation_period_num': 85, 'train_rates': 0.7783100696256053, 'learning_rate': 1.6466778224940566e-06, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8735738119402877}. Best is trial 36 with value: 0.03300830301020172.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-12 10:49:06,041][0m A new study created in memory with name: no-name-b6924c3a-27ca-4319-a6f0-38a5b930f820[0m
[32m[I 2025-01-12 10:49:37,690][0m Trial 0 finished with value: 0.25150826942191523 and parameters: {'observation_period_num': 174, 'train_rates': 0.7471998749778116, 'learning_rate': 0.000556543990851804, 'batch_size': 149, 'step_size': 11, 'gamma': 0.8911124851503527}. Best is trial 0 with value: 0.25150826942191523.[0m
[32m[I 2025-01-12 10:50:00,946][0m Trial 1 finished with value: 0.6114042582858926 and parameters: {'observation_period_num': 125, 'train_rates': 0.7378335246965609, 'learning_rate': 1.41820422138711e-05, 'batch_size': 228, 'step_size': 15, 'gamma': 0.7887310009157582}. Best is trial 0 with value: 0.25150826942191523.[0m
[32m[I 2025-01-12 10:50:38,148][0m Trial 2 finished with value: 0.8206824047242222 and parameters: {'observation_period_num': 99, 'train_rates': 0.6563715629783102, 'learning_rate': 6.52792160047598e-06, 'batch_size': 118, 'step_size': 3, 'gamma': 0.946032129244944}. Best is trial 0 with value: 0.25150826942191523.[0m
[32m[I 2025-01-12 10:51:59,684][0m Trial 3 finished with value: 0.3113076633308083 and parameters: {'observation_period_num': 252, 'train_rates': 0.7870366347261041, 'learning_rate': 8.58504407065854e-06, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9702077244162628}. Best is trial 0 with value: 0.25150826942191523.[0m
[32m[I 2025-01-12 10:52:24,311][0m Trial 4 finished with value: 0.8185999496061294 and parameters: {'observation_period_num': 204, 'train_rates': 0.8226439847937264, 'learning_rate': 1.613833490788863e-06, 'batch_size': 213, 'step_size': 12, 'gamma': 0.9507901267680254}. Best is trial 0 with value: 0.25150826942191523.[0m
[32m[I 2025-01-12 10:52:54,849][0m Trial 5 finished with value: 0.16652336156868655 and parameters: {'observation_period_num': 26, 'train_rates': 0.678676988270991, 'learning_rate': 0.00040075651189989427, 'batch_size': 153, 'step_size': 9, 'gamma': 0.7844300467007582}. Best is trial 5 with value: 0.16652336156868655.[0m
[32m[I 2025-01-12 10:53:21,021][0m Trial 6 finished with value: 0.31695976853370667 and parameters: {'observation_period_num': 167, 'train_rates': 0.9842664673330206, 'learning_rate': 4.0234267498158174e-05, 'batch_size': 232, 'step_size': 7, 'gamma': 0.8862994802975412}. Best is trial 5 with value: 0.16652336156868655.[0m
[32m[I 2025-01-12 10:54:13,759][0m Trial 7 finished with value: 0.5506714413332384 and parameters: {'observation_period_num': 130, 'train_rates': 0.7716347734962655, 'learning_rate': 1.0807430298872116e-05, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9265248811835356}. Best is trial 5 with value: 0.16652336156868655.[0m
[32m[I 2025-01-12 10:55:26,992][0m Trial 8 finished with value: 0.506362668426853 and parameters: {'observation_period_num': 68, 'train_rates': 0.8716882003312778, 'learning_rate': 5.710050449985406e-06, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8910647497901797}. Best is trial 5 with value: 0.16652336156868655.[0m
[32m[I 2025-01-12 10:55:48,060][0m Trial 9 finished with value: 1.068269115512014 and parameters: {'observation_period_num': 100, 'train_rates': 0.7965737257867577, 'learning_rate': 1.5924247413138582e-06, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8185552715838511}. Best is trial 5 with value: 0.16652336156868655.[0m
[32m[I 2025-01-12 11:00:03,986][0m Trial 10 finished with value: 0.12720887338461942 and parameters: {'observation_period_num': 5, 'train_rates': 0.6288739667717113, 'learning_rate': 0.0009317484215334095, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7534160957179311}. Best is trial 10 with value: 0.12720887338461942.[0m
Early stopping at epoch 70
[32m[I 2025-01-12 11:02:57,053][0m Trial 11 finished with value: 0.1437847688794136 and parameters: {'observation_period_num': 7, 'train_rates': 0.6131605979972939, 'learning_rate': 0.0008382869312850455, 'batch_size': 17, 'step_size': 1, 'gamma': 0.757735905642322}. Best is trial 10 with value: 0.12720887338461942.[0m
Early stopping at epoch 44
[32m[I 2025-01-12 11:04:29,151][0m Trial 12 finished with value: 0.22638618999248658 and parameters: {'observation_period_num': 5, 'train_rates': 0.6007877323503317, 'learning_rate': 0.00014847412266829924, 'batch_size': 20, 'step_size': 1, 'gamma': 0.751789833688759}. Best is trial 10 with value: 0.12720887338461942.[0m
[32m[I 2025-01-12 11:08:36,132][0m Trial 13 finished with value: 0.1744803153176926 and parameters: {'observation_period_num': 46, 'train_rates': 0.6127387150104492, 'learning_rate': 0.0009364701154241974, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8327633098503212}. Best is trial 10 with value: 0.12720887338461942.[0m
Early stopping at epoch 49
[32m[I 2025-01-12 11:09:22,568][0m Trial 14 finished with value: 0.4458399216334025 and parameters: {'observation_period_num': 54, 'train_rates': 0.6598116004254156, 'learning_rate': 0.00013203421182009256, 'batch_size': 47, 'step_size': 1, 'gamma': 0.7536219576613998}. Best is trial 10 with value: 0.12720887338461942.[0m
[32m[I 2025-01-12 11:10:09,934][0m Trial 15 finished with value: 0.17486216160468757 and parameters: {'observation_period_num': 7, 'train_rates': 0.7013736602403182, 'learning_rate': 0.0001834051414296031, 'batch_size': 99, 'step_size': 4, 'gamma': 0.8373104579927035}. Best is trial 10 with value: 0.12720887338461942.[0m
[32m[I 2025-01-12 11:10:36,363][0m Trial 16 finished with value: 0.45318501649400156 and parameters: {'observation_period_num': 75, 'train_rates': 0.6269494415010052, 'learning_rate': 5.0033829895997646e-05, 'batch_size': 172, 'step_size': 5, 'gamma': 0.7892056494016856}. Best is trial 10 with value: 0.12720887338461942.[0m
Early stopping at epoch 89
[32m[I 2025-01-12 11:12:40,035][0m Trial 17 finished with value: 0.06341901621011299 and parameters: {'observation_period_num': 34, 'train_rates': 0.8695976392590662, 'learning_rate': 0.0009811177352063898, 'batch_size': 39, 'step_size': 1, 'gamma': 0.8087495189124226}. Best is trial 17 with value: 0.06341901621011299.[0m
[32m[I 2025-01-12 11:14:17,182][0m Trial 18 finished with value: 0.05367244482040405 and parameters: {'observation_period_num': 40, 'train_rates': 0.9022554698965072, 'learning_rate': 0.00031455285767682204, 'batch_size': 57, 'step_size': 3, 'gamma': 0.8572020612343896}. Best is trial 18 with value: 0.05367244482040405.[0m
[32m[I 2025-01-12 11:16:05,342][0m Trial 19 finished with value: 0.049690308454244034 and parameters: {'observation_period_num': 37, 'train_rates': 0.921412142383066, 'learning_rate': 0.0003672749153807405, 'batch_size': 51, 'step_size': 4, 'gamma': 0.8644896120725987}. Best is trial 19 with value: 0.049690308454244034.[0m
[32m[I 2025-01-12 11:17:11,064][0m Trial 20 finished with value: 0.07431541383266449 and parameters: {'observation_period_num': 89, 'train_rates': 0.977784623796458, 'learning_rate': 0.0002723524382999008, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8597946879249811}. Best is trial 19 with value: 0.049690308454244034.[0m
[32m[I 2025-01-12 11:18:39,470][0m Trial 21 finished with value: 0.0875952878864262 and parameters: {'observation_period_num': 39, 'train_rates': 0.9129403938713214, 'learning_rate': 0.00010322795316221801, 'batch_size': 63, 'step_size': 3, 'gamma': 0.8596078269943638}. Best is trial 19 with value: 0.049690308454244034.[0m
[32m[I 2025-01-12 11:20:49,531][0m Trial 22 finished with value: 0.04704171055663935 and parameters: {'observation_period_num': 34, 'train_rates': 0.915699904654193, 'learning_rate': 0.0003598038178767985, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8126856885317351}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:22:01,611][0m Trial 23 finished with value: 0.06084092458089193 and parameters: {'observation_period_num': 65, 'train_rates': 0.9316731732831965, 'learning_rate': 0.00031793312232082004, 'batch_size': 77, 'step_size': 5, 'gamma': 0.846410365434121}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:24:25,484][0m Trial 24 finished with value: 0.05390145214819492 and parameters: {'observation_period_num': 28, 'train_rates': 0.9220034670603039, 'learning_rate': 7.408695712405668e-05, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9101610537971205}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:25:12,644][0m Trial 25 finished with value: 0.07404956775997078 and parameters: {'observation_period_num': 123, 'train_rates': 0.86483581594956, 'learning_rate': 0.0002246674330532875, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8220921355958737}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:27:27,793][0m Trial 26 finished with value: 0.19816262360130038 and parameters: {'observation_period_num': 84, 'train_rates': 0.9515005279744054, 'learning_rate': 2.247476521554126e-05, 'batch_size': 41, 'step_size': 4, 'gamma': 0.8745119660849131}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:28:36,664][0m Trial 27 finished with value: 0.0832980240579524 and parameters: {'observation_period_num': 51, 'train_rates': 0.8947615365235567, 'learning_rate': 7.856347788879677e-05, 'batch_size': 79, 'step_size': 6, 'gamma': 0.8518523906688787}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:29:25,628][0m Trial 28 finished with value: 0.08186875616911776 and parameters: {'observation_period_num': 26, 'train_rates': 0.835199979422818, 'learning_rate': 0.00048413414597019513, 'batch_size': 111, 'step_size': 2, 'gamma': 0.8043120385132929}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:30:07,309][0m Trial 29 finished with value: 0.08847461640834808 and parameters: {'observation_period_num': 163, 'train_rates': 0.9525822921334224, 'learning_rate': 0.00048153425446182535, 'batch_size': 139, 'step_size': 11, 'gamma': 0.9098460370729818}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:31:36,595][0m Trial 30 finished with value: 0.06554414733723782 and parameters: {'observation_period_num': 108, 'train_rates': 0.8393570928268136, 'learning_rate': 0.0002898030708751671, 'batch_size': 57, 'step_size': 6, 'gamma': 0.8765963795829166}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:34:09,945][0m Trial 31 finished with value: 0.06783392444654346 and parameters: {'observation_period_num': 61, 'train_rates': 0.8974248373943264, 'learning_rate': 6.317930573256403e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8988437497767775}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:36:59,334][0m Trial 32 finished with value: 0.049416981801828914 and parameters: {'observation_period_num': 29, 'train_rates': 0.9302781408473535, 'learning_rate': 0.00010241567772881018, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9111098160712032}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:38:37,416][0m Trial 33 finished with value: 0.049887815135575476 and parameters: {'observation_period_num': 18, 'train_rates': 0.9572861045809204, 'learning_rate': 0.0005828620585148281, 'batch_size': 59, 'step_size': 2, 'gamma': 0.8670681785163172}. Best is trial 22 with value: 0.04704171055663935.[0m
[32m[I 2025-01-12 11:41:53,144][0m Trial 34 finished with value: 0.045109647128618126 and parameters: {'observation_period_num': 26, 'train_rates': 0.9549361392925797, 'learning_rate': 0.0005040876078115706, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9328857135709495}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:44:28,828][0m Trial 35 finished with value: 0.11005413319383349 and parameters: {'observation_period_num': 196, 'train_rates': 0.9368300266750063, 'learning_rate': 0.0006062734972991895, 'batch_size': 34, 'step_size': 2, 'gamma': 0.9812677497820339}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:47:51,487][0m Trial 36 finished with value: 0.0817575769033283 and parameters: {'observation_period_num': 18, 'train_rates': 0.9892671177193678, 'learning_rate': 2.4058822542714644e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9447753168427139}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:49:45,389][0m Trial 37 finished with value: 0.07384073816239833 and parameters: {'observation_period_num': 80, 'train_rates': 0.965471502156853, 'learning_rate': 0.00017925108694263808, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9363409198884235}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:50:21,922][0m Trial 38 finished with value: 0.07344787117619456 and parameters: {'observation_period_num': 51, 'train_rates': 0.8878961522742336, 'learning_rate': 0.00011326703854408692, 'batch_size': 157, 'step_size': 5, 'gamma': 0.9635226869351662}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:50:52,134][0m Trial 39 finished with value: 0.1148761510848999 and parameters: {'observation_period_num': 242, 'train_rates': 0.9350952319379683, 'learning_rate': 0.0003587670539137225, 'batch_size': 187, 'step_size': 3, 'gamma': 0.9226778559267201}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:52:09,675][0m Trial 40 finished with value: 0.44720491588889777 and parameters: {'observation_period_num': 143, 'train_rates': 0.8631104994874467, 'learning_rate': 4.520019441663253e-06, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9046749877311578}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:53:59,892][0m Trial 41 finished with value: 0.051239080388437615 and parameters: {'observation_period_num': 27, 'train_rates': 0.9588532836133261, 'learning_rate': 0.0006036108292221055, 'batch_size': 52, 'step_size': 2, 'gamma': 0.8827519330681045}. Best is trial 34 with value: 0.045109647128618126.[0m
[32m[I 2025-01-12 11:57:18,020][0m Trial 42 finished with value: 0.038113122668705486 and parameters: {'observation_period_num': 18, 'train_rates': 0.9679298794992738, 'learning_rate': 0.0005819322505787369, 'batch_size': 29, 'step_size': 2, 'gamma': 0.924313280988352}. Best is trial 42 with value: 0.038113122668705486.[0m
[32m[I 2025-01-12 12:00:32,740][0m Trial 43 finished with value: 0.03237835579221287 and parameters: {'observation_period_num': 17, 'train_rates': 0.9156974077105391, 'learning_rate': 0.0001973599645845138, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9599849094922285}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:03:27,985][0m Trial 44 finished with value: 0.16440615820534088 and parameters: {'observation_period_num': 19, 'train_rates': 0.7393135146662062, 'learning_rate': 0.00022799562575386345, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9577627749741405}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:07:27,292][0m Trial 45 finished with value: 0.04286897457812144 and parameters: {'observation_period_num': 18, 'train_rates': 0.96886965110255, 'learning_rate': 0.0006823747202016455, 'batch_size': 24, 'step_size': 1, 'gamma': 0.9272204523747827}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:11:17,278][0m Trial 46 finished with value: 0.03532121463545731 and parameters: {'observation_period_num': 21, 'train_rates': 0.97403413680131, 'learning_rate': 0.0006455721995765763, 'batch_size': 25, 'step_size': 1, 'gamma': 0.9273092987948849}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:15:09,215][0m Trial 47 finished with value: 0.034980900264505684 and parameters: {'observation_period_num': 17, 'train_rates': 0.9805339005434122, 'learning_rate': 0.0006462899884296325, 'batch_size': 25, 'step_size': 1, 'gamma': 0.9285960343035972}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:19:30,847][0m Trial 48 finished with value: 0.04135672557048309 and parameters: {'observation_period_num': 14, 'train_rates': 0.9737563364284078, 'learning_rate': 0.0007380732458445833, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9814629435367036}. Best is trial 43 with value: 0.03237835579221287.[0m
[32m[I 2025-01-12 12:23:45,237][0m Trial 49 finished with value: 0.05071293719505009 and parameters: {'observation_period_num': 12, 'train_rates': 0.9871380211656294, 'learning_rate': 0.0007955226991719941, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9856665116436943}. Best is trial 43 with value: 0.03237835579221287.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 12:23:45,247][0m A new study created in memory with name: no-name-093b81bb-276c-4b30-9394-e363e4d6811f[0m
[32m[I 2025-01-12 12:24:10,226][0m Trial 0 finished with value: 0.12717139093499435 and parameters: {'observation_period_num': 106, 'train_rates': 0.8065930078599689, 'learning_rate': 9.420320457966617e-05, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9854520136117988}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:25:47,319][0m Trial 1 finished with value: 0.24580361448615962 and parameters: {'observation_period_num': 40, 'train_rates': 0.6437005067569299, 'learning_rate': 2.959151059616154e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.7622969672899512}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:26:26,244][0m Trial 2 finished with value: 1.4741730605656247 and parameters: {'observation_period_num': 155, 'train_rates': 0.8484969801701037, 'learning_rate': 1.0829538621515687e-06, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8035563235368367}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:26:56,407][0m Trial 3 finished with value: 0.6380093071241021 and parameters: {'observation_period_num': 85, 'train_rates': 0.8978679947917563, 'learning_rate': 9.502190729102096e-06, 'batch_size': 189, 'step_size': 8, 'gamma': 0.8792730056588557}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:27:26,850][0m Trial 4 finished with value: 0.25115623130974635 and parameters: {'observation_period_num': 86, 'train_rates': 0.703091309147386, 'learning_rate': 0.00011604454519609331, 'batch_size': 165, 'step_size': 11, 'gamma': 0.9737342741368638}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:29:19,798][0m Trial 5 finished with value: 0.546312782614904 and parameters: {'observation_period_num': 109, 'train_rates': 0.899370847599832, 'learning_rate': 2.603566198435245e-06, 'batch_size': 47, 'step_size': 11, 'gamma': 0.7925164718721996}. Best is trial 0 with value: 0.12717139093499435.[0m
Early stopping at epoch 53
[32m[I 2025-01-12 12:30:22,150][0m Trial 6 finished with value: 0.47798420054706064 and parameters: {'observation_period_num': 184, 'train_rates': 0.7709354178991499, 'learning_rate': 0.00011866469853811587, 'batch_size': 40, 'step_size': 1, 'gamma': 0.7825118515972529}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:32:44,499][0m Trial 7 finished with value: 0.21395354323319188 and parameters: {'observation_period_num': 51, 'train_rates': 0.7098833811412327, 'learning_rate': 3.460826521716686e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.8797717690950968}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:33:35,711][0m Trial 8 finished with value: 0.2000863851169284 and parameters: {'observation_period_num': 156, 'train_rates': 0.6457195617506681, 'learning_rate': 0.00023962347027414838, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9449087953309345}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:35:41,824][0m Trial 9 finished with value: 0.2423788168589058 and parameters: {'observation_period_num': 124, 'train_rates': 0.8699197881928982, 'learning_rate': 1.7242475886731143e-05, 'batch_size': 41, 'step_size': 6, 'gamma': 0.75773312193269}. Best is trial 0 with value: 0.12717139093499435.[0m
[32m[I 2025-01-12 12:36:05,928][0m Trial 10 finished with value: 0.10602342337369919 and parameters: {'observation_period_num': 251, 'train_rates': 0.9824764690390413, 'learning_rate': 0.0007064641726885613, 'batch_size': 249, 'step_size': 2, 'gamma': 0.9311124008078814}. Best is trial 10 with value: 0.10602342337369919.[0m
[32m[I 2025-01-12 12:36:30,109][0m Trial 11 finished with value: 0.17869234085083008 and parameters: {'observation_period_num': 233, 'train_rates': 0.985072461429942, 'learning_rate': 0.0006418054689284969, 'batch_size': 251, 'step_size': 1, 'gamma': 0.9330910588397223}. Best is trial 10 with value: 0.10602342337369919.[0m
[32m[I 2025-01-12 12:36:53,808][0m Trial 12 finished with value: 0.1315755397081375 and parameters: {'observation_period_num': 233, 'train_rates': 0.9795237222270214, 'learning_rate': 0.0009520116388670987, 'batch_size': 245, 'step_size': 3, 'gamma': 0.9876035952358435}. Best is trial 10 with value: 0.10602342337369919.[0m
[32m[I 2025-01-12 12:37:19,994][0m Trial 13 finished with value: 0.12015166014029394 and parameters: {'observation_period_num': 195, 'train_rates': 0.812881199978659, 'learning_rate': 0.0002938956294846597, 'batch_size': 203, 'step_size': 15, 'gamma': 0.9281083677767343}. Best is trial 10 with value: 0.10602342337369919.[0m
[32m[I 2025-01-12 12:37:47,758][0m Trial 14 finished with value: 0.10054504871368408 and parameters: {'observation_period_num': 251, 'train_rates': 0.9352811468681019, 'learning_rate': 0.0003036464693268433, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9121644739995429}. Best is trial 14 with value: 0.10054504871368408.[0m
[32m[I 2025-01-12 12:38:16,138][0m Trial 15 finished with value: 0.0905829668045044 and parameters: {'observation_period_num': 248, 'train_rates': 0.9540418363009208, 'learning_rate': 0.00037616850133891425, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8437506717121473}. Best is trial 15 with value: 0.0905829668045044.[0m
[32m[I 2025-01-12 12:39:02,282][0m Trial 16 finished with value: 0.07672411331493931 and parameters: {'observation_period_num': 203, 'train_rates': 0.9198013193343588, 'learning_rate': 0.00035624082391479876, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8402631841803602}. Best is trial 16 with value: 0.07672411331493931.[0m
[32m[I 2025-01-12 12:39:56,516][0m Trial 17 finished with value: 0.11120292249153245 and parameters: {'observation_period_num': 197, 'train_rates': 0.9233189514697284, 'learning_rate': 6.384703360974617e-05, 'batch_size': 98, 'step_size': 13, 'gamma': 0.8412939470666694}. Best is trial 16 with value: 0.07672411331493931.[0m
[32m[I 2025-01-12 12:40:44,079][0m Trial 18 finished with value: 0.04292929511746215 and parameters: {'observation_period_num': 6, 'train_rates': 0.939465333604022, 'learning_rate': 0.0003979053614182512, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8340897266974815}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:41:37,132][0m Trial 19 finished with value: 0.44533709230010443 and parameters: {'observation_period_num': 10, 'train_rates': 0.8666042582184714, 'learning_rate': 5.220996794545765e-06, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8229231535474689}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:42:12,886][0m Trial 20 finished with value: 0.26429488290758696 and parameters: {'observation_period_num': 152, 'train_rates': 0.7574278506477011, 'learning_rate': 0.0001734695074636764, 'batch_size': 137, 'step_size': 13, 'gamma': 0.8584328851385314}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:42:47,096][0m Trial 21 finished with value: 0.09126775711774826 and parameters: {'observation_period_num': 217, 'train_rates': 0.9401494718030001, 'learning_rate': 0.00044575044442314116, 'batch_size': 165, 'step_size': 9, 'gamma': 0.834127257227764}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:43:35,501][0m Trial 22 finished with value: 0.07891082146057957 and parameters: {'observation_period_num': 176, 'train_rates': 0.9453770997907246, 'learning_rate': 0.000457576196375443, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8558742070947429}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:44:43,271][0m Trial 23 finished with value: 0.09971912894046532 and parameters: {'observation_period_num': 175, 'train_rates': 0.9048623185393447, 'learning_rate': 0.0005549282560610263, 'batch_size': 78, 'step_size': 9, 'gamma': 0.9021031215472456}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:45:28,170][0m Trial 24 finished with value: 0.202781034977112 and parameters: {'observation_period_num': 210, 'train_rates': 0.849977545634971, 'learning_rate': 6.416412072782783e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8186392893516551}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:46:14,195][0m Trial 25 finished with value: 0.10302731394767761 and parameters: {'observation_period_num': 172, 'train_rates': 0.9568716189786719, 'learning_rate': 0.0001996510087973271, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8608878086998999}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:47:33,524][0m Trial 26 finished with value: 0.10411970528272482 and parameters: {'observation_period_num': 140, 'train_rates': 0.8848098642257756, 'learning_rate': 0.0008904689940573054, 'batch_size': 66, 'step_size': 14, 'gamma': 0.8977716710113686}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:48:08,060][0m Trial 27 finished with value: 0.06942052908919075 and parameters: {'observation_period_num': 18, 'train_rates': 0.8314087555447167, 'learning_rate': 0.0001724975973393612, 'batch_size': 153, 'step_size': 5, 'gamma': 0.8129699851913451}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:48:40,986][0m Trial 28 finished with value: 0.06891600333002149 and parameters: {'observation_period_num': 7, 'train_rates': 0.8336806757480537, 'learning_rate': 0.00016384367410123235, 'batch_size': 166, 'step_size': 4, 'gamma': 0.8063203589854278}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:49:16,299][0m Trial 29 finished with value: 0.10519376927665595 and parameters: {'observation_period_num': 7, 'train_rates': 0.8034165815327843, 'learning_rate': 6.070824529566778e-05, 'batch_size': 157, 'step_size': 4, 'gamma': 0.7757800872865773}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:49:53,298][0m Trial 30 finished with value: 0.12481552835113259 and parameters: {'observation_period_num': 29, 'train_rates': 0.8410535739157637, 'learning_rate': 0.00015889711135164253, 'batch_size': 151, 'step_size': 4, 'gamma': 0.8078215023279218}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:50:21,754][0m Trial 31 finished with value: 0.15097295269671723 and parameters: {'observation_period_num': 63, 'train_rates': 0.8293455094386936, 'learning_rate': 0.0001089697488990458, 'batch_size': 185, 'step_size': 5, 'gamma': 0.8252533864188972}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:50:50,969][0m Trial 32 finished with value: 0.31077999623890895 and parameters: {'observation_period_num': 26, 'train_rates': 0.775111962601987, 'learning_rate': 0.00024078975006922238, 'batch_size': 177, 'step_size': 3, 'gamma': 0.8030414685661469}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:51:29,288][0m Trial 33 finished with value: 0.3793358670210275 and parameters: {'observation_period_num': 28, 'train_rates': 0.7397254922308147, 'learning_rate': 3.598632373034365e-05, 'batch_size': 129, 'step_size': 7, 'gamma': 0.7734495964620691}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:52:09,348][0m Trial 34 finished with value: 0.06737848300918489 and parameters: {'observation_period_num': 55, 'train_rates': 0.9198676019061118, 'learning_rate': 0.00036561108090808373, 'batch_size': 146, 'step_size': 5, 'gamma': 0.8111947975589084}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:52:47,171][0m Trial 35 finished with value: 0.15678389723684222 and parameters: {'observation_period_num': 62, 'train_rates': 0.8731993039420991, 'learning_rate': 7.919640758681326e-05, 'batch_size': 145, 'step_size': 5, 'gamma': 0.7928249349752019}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:53:13,572][0m Trial 36 finished with value: 0.3345412740505065 and parameters: {'observation_period_num': 43, 'train_rates': 0.6074972496510747, 'learning_rate': 0.00015608314398838898, 'batch_size': 170, 'step_size': 7, 'gamma': 0.8134489590343735}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:53:52,758][0m Trial 37 finished with value: 0.3254156995968646 and parameters: {'observation_period_num': 18, 'train_rates': 0.8304334347869549, 'learning_rate': 2.037922934233627e-05, 'batch_size': 138, 'step_size': 4, 'gamma': 0.793112531164449}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:54:17,122][0m Trial 38 finished with value: 0.18323722361943345 and parameters: {'observation_period_num': 90, 'train_rates': 0.7827890547282802, 'learning_rate': 0.00010263449890321511, 'batch_size': 224, 'step_size': 6, 'gamma': 0.875094610467924}. Best is trial 18 with value: 0.04292929511746215.[0m
Early stopping at epoch 73
[32m[I 2025-01-12 12:55:01,078][0m Trial 39 finished with value: 1.701546789900574 and parameters: {'observation_period_num': 67, 'train_rates': 0.9021578914939539, 'learning_rate': 1.3176334506079682e-06, 'batch_size': 93, 'step_size': 2, 'gamma': 0.7514261243779162}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:55:36,409][0m Trial 40 finished with value: 0.1947921642235347 and parameters: {'observation_period_num': 42, 'train_rates': 0.8127615958046907, 'learning_rate': 4.0315664824498116e-05, 'batch_size': 156, 'step_size': 8, 'gamma': 0.8026828278809861}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:56:22,215][0m Trial 41 finished with value: 0.05564147453846001 and parameters: {'observation_period_num': 7, 'train_rates': 0.9304272855084763, 'learning_rate': 0.0002903425666369287, 'batch_size': 127, 'step_size': 5, 'gamma': 0.826108012426219}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:57:08,274][0m Trial 42 finished with value: 0.0686725303530693 and parameters: {'observation_period_num': 18, 'train_rates': 0.9638779525467399, 'learning_rate': 0.0002453083645152623, 'batch_size': 132, 'step_size': 5, 'gamma': 0.8285237030240438}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:57:54,063][0m Trial 43 finished with value: 0.05948270857334137 and parameters: {'observation_period_num': 9, 'train_rates': 0.9666742897117855, 'learning_rate': 0.0005597321797396227, 'batch_size': 131, 'step_size': 4, 'gamma': 0.8327224161003808}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:58:49,467][0m Trial 44 finished with value: 0.09401335567235947 and parameters: {'observation_period_num': 51, 'train_rates': 0.9735984420160428, 'learning_rate': 0.0006461767606431559, 'batch_size': 106, 'step_size': 2, 'gamma': 0.8306219427737469}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 12:59:35,125][0m Trial 45 finished with value: 0.06093137711286545 and parameters: {'observation_period_num': 32, 'train_rates': 0.9662578178392623, 'learning_rate': 0.0004897693267011254, 'batch_size': 130, 'step_size': 8, 'gamma': 0.8669138960721133}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 13:00:49,421][0m Trial 46 finished with value: 0.0536582225088506 and parameters: {'observation_period_num': 34, 'train_rates': 0.9222521968106665, 'learning_rate': 0.0004867160359294484, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8498309878810894}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 13:02:28,121][0m Trial 47 finished with value: 0.08725300431251526 and parameters: {'observation_period_num': 78, 'train_rates': 0.9887701181827109, 'learning_rate': 0.0008636416660970049, 'batch_size': 58, 'step_size': 8, 'gamma': 0.8825421685888764}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 13:03:42,750][0m Trial 48 finished with value: 0.048564940013668755 and parameters: {'observation_period_num': 35, 'train_rates': 0.9286477586298799, 'learning_rate': 0.0004877890059039085, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8493306749999127}. Best is trial 18 with value: 0.04292929511746215.[0m
[32m[I 2025-01-12 13:04:55,099][0m Trial 49 finished with value: 0.08222128038222973 and parameters: {'observation_period_num': 116, 'train_rates': 0.9315151157777318, 'learning_rate': 0.0006573076358276903, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8497236205049581}. Best is trial 18 with value: 0.04292929511746215.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 68, 'train_rates': 0.9535935971255709, 'learning_rate': 0.0006764158865808568, 'batch_size': 17, 'step_size': 11, 'gamma': 0.908353186745297}
Epoch 1/300, trend Loss: 0.2928 | 0.1308
Epoch 2/300, trend Loss: 0.1606 | 0.1032
Epoch 3/300, trend Loss: 0.1349 | 0.0958
Epoch 4/300, trend Loss: 0.1257 | 0.0860
Epoch 5/300, trend Loss: 0.1174 | 0.0795
Epoch 6/300, trend Loss: 0.1113 | 0.0766
Epoch 7/300, trend Loss: 0.1062 | 0.0817
Epoch 8/300, trend Loss: 0.1030 | 0.0856
Epoch 9/300, trend Loss: 0.0964 | 0.0810
Epoch 10/300, trend Loss: 0.0928 | 0.0799
Epoch 11/300, trend Loss: 0.0893 | 0.0797
Epoch 12/300, trend Loss: 0.0844 | 0.0758
Epoch 13/300, trend Loss: 0.0824 | 0.0747
Epoch 14/300, trend Loss: 0.0795 | 0.0698
Epoch 15/300, trend Loss: 0.0782 | 0.0775
Epoch 16/300, trend Loss: 0.0762 | 0.0700
Epoch 17/300, trend Loss: 0.0739 | 0.0732
Epoch 18/300, trend Loss: 0.0716 | 0.0659
Epoch 19/300, trend Loss: 0.0707 | 0.0729
Epoch 20/300, trend Loss: 0.0677 | 0.0641
Epoch 21/300, trend Loss: 0.0667 | 0.0635
Epoch 22/300, trend Loss: 0.0665 | 0.0691
Epoch 23/300, trend Loss: 0.0646 | 0.0676
Epoch 24/300, trend Loss: 0.0639 | 0.0615
Epoch 25/300, trend Loss: 0.0614 | 0.0616
Epoch 26/300, trend Loss: 0.0611 | 0.0592
Epoch 27/300, trend Loss: 0.0595 | 0.0665
Epoch 28/300, trend Loss: 0.0622 | 0.0734
Epoch 29/300, trend Loss: 0.0588 | 0.0668
Epoch 30/300, trend Loss: 0.0587 | 0.0687
Epoch 31/300, trend Loss: 0.0594 | 0.0687
Epoch 32/300, trend Loss: 0.0582 | 0.0805
Epoch 33/300, trend Loss: 0.0581 | 0.0663
Epoch 34/300, trend Loss: 0.0623 | 0.0733
Epoch 35/300, trend Loss: 0.0563 | 0.0700
Epoch 36/300, trend Loss: 0.0544 | 0.0867
Epoch 37/300, trend Loss: 0.0577 | 0.0704
Epoch 38/300, trend Loss: 0.0524 | 0.0715
Epoch 39/300, trend Loss: 0.0527 | 0.0736
Epoch 40/300, trend Loss: 0.0543 | 0.0752
Epoch 41/300, trend Loss: 0.0526 | 0.0739
Epoch 42/300, trend Loss: 0.0539 | 0.0763
Epoch 43/300, trend Loss: 0.0526 | 0.0752
Epoch 44/300, trend Loss: 0.0480 | 0.0661
Epoch 45/300, trend Loss: 0.0489 | 0.0690
Epoch 46/300, trend Loss: 0.0488 | 0.0627
Epoch 47/300, trend Loss: 0.0473 | 0.0609
Epoch 48/300, trend Loss: 0.0459 | 0.0596
Epoch 49/300, trend Loss: 0.0448 | 0.0617
Epoch 50/300, trend Loss: 0.0454 | 0.0611
Epoch 51/300, trend Loss: 0.0503 | 0.0671
Epoch 52/300, trend Loss: 0.0456 | 0.0576
Epoch 53/300, trend Loss: 0.0431 | 0.0609
Epoch 54/300, trend Loss: 0.0423 | 0.0575
Epoch 55/300, trend Loss: 0.0422 | 0.0589
Epoch 56/300, trend Loss: 0.0419 | 0.0568
Epoch 57/300, trend Loss: 0.0417 | 0.0591
Epoch 58/300, trend Loss: 0.0417 | 0.0596
Epoch 59/300, trend Loss: 0.0409 | 0.0601
Epoch 60/300, trend Loss: 0.0407 | 0.0621
Epoch 61/300, trend Loss: 0.0410 | 0.0631
Epoch 62/300, trend Loss: 0.0414 | 0.0655
Epoch 63/300, trend Loss: 0.0413 | 0.0663
Epoch 64/300, trend Loss: 0.0402 | 0.0610
Epoch 65/300, trend Loss: 0.0390 | 0.0603
Epoch 66/300, trend Loss: 0.0393 | 0.0625
Epoch 67/300, trend Loss: 0.0395 | 0.0634
Epoch 68/300, trend Loss: 0.0393 | 0.0648
Epoch 69/300, trend Loss: 0.0410 | 0.0681
Epoch 70/300, trend Loss: 0.0424 | 0.0637
Epoch 71/300, trend Loss: 0.0381 | 0.0639
Epoch 72/300, trend Loss: 0.0373 | 0.0643
Epoch 73/300, trend Loss: 0.0367 | 0.0626
Epoch 74/300, trend Loss: 0.0371 | 0.0593
Epoch 75/300, trend Loss: 0.0375 | 0.0595
Epoch 76/300, trend Loss: 0.0371 | 0.0610
Epoch 77/300, trend Loss: 0.0375 | 0.0605
Epoch 78/300, trend Loss: 0.0368 | 0.0614
Epoch 79/300, trend Loss: 0.0374 | 0.0601
Epoch 80/300, trend Loss: 0.0360 | 0.0627
Epoch 81/300, trend Loss: 0.0368 | 0.0610
Epoch 82/300, trend Loss: 0.0352 | 0.0653
Epoch 83/300, trend Loss: 0.0355 | 0.0636
Epoch 84/300, trend Loss: 0.0351 | 0.0651
Epoch 85/300, trend Loss: 0.0351 | 0.0621
Epoch 86/300, trend Loss: 0.0360 | 0.0657
Epoch 87/300, trend Loss: 0.0352 | 0.0624
Epoch 88/300, trend Loss: 0.0363 | 0.0654
Epoch 89/300, trend Loss: 0.0371 | 0.0645
Epoch 90/300, trend Loss: 0.0340 | 0.0625
Epoch 91/300, trend Loss: 0.0336 | 0.0657
Epoch 92/300, trend Loss: 0.0338 | 0.0656
Epoch 93/300, trend Loss: 0.0344 | 0.0686
Epoch 94/300, trend Loss: 0.0343 | 0.0656
Epoch 95/300, trend Loss: 0.0341 | 0.0689
Epoch 96/300, trend Loss: 0.0340 | 0.0639
Epoch 97/300, trend Loss: 0.0332 | 0.0667
Epoch 98/300, trend Loss: 0.0326 | 0.0620
Epoch 99/300, trend Loss: 0.0325 | 0.0660
Epoch 100/300, trend Loss: 0.0320 | 0.0645
Epoch 101/300, trend Loss: 0.0319 | 0.0665
Epoch 102/300, trend Loss: 0.0318 | 0.0648
Epoch 103/300, trend Loss: 0.0320 | 0.0670
Epoch 104/300, trend Loss: 0.0317 | 0.0655
Epoch 105/300, trend Loss: 0.0316 | 0.0674
Epoch 106/300, trend Loss: 0.0322 | 0.0637
Epoch 107/300, trend Loss: 0.0314 | 0.0684
Epoch 108/300, trend Loss: 0.0318 | 0.0631
Epoch 109/300, trend Loss: 0.0325 | 0.0668
Epoch 110/300, trend Loss: 0.0332 | 0.0628
Epoch 111/300, trend Loss: 0.0333 | 0.0683
Epoch 112/300, trend Loss: 0.0319 | 0.0651
Epoch 113/300, trend Loss: 0.0321 | 0.0672
Epoch 114/300, trend Loss: 0.0307 | 0.0664
Epoch 115/300, trend Loss: 0.0312 | 0.0673
Epoch 116/300, trend Loss: 0.0305 | 0.0662
Epoch 117/300, trend Loss: 0.0311 | 0.0679
Epoch 118/300, trend Loss: 0.0308 | 0.0677
Epoch 119/300, trend Loss: 0.0344 | 0.0681
Epoch 120/300, trend Loss: 0.0336 | 0.0704
Epoch 121/300, trend Loss: 0.0307 | 0.0652
Epoch 122/300, trend Loss: 0.0305 | 0.0672
Epoch 123/300, trend Loss: 0.0300 | 0.0674
Epoch 124/300, trend Loss: 0.0298 | 0.0685
Epoch 125/300, trend Loss: 0.0298 | 0.0689
Epoch 126/300, trend Loss: 0.0299 | 0.0690
Epoch 127/300, trend Loss: 0.0297 | 0.0691
Epoch 128/300, trend Loss: 0.0296 | 0.0688
Epoch 129/300, trend Loss: 0.0293 | 0.0683
Epoch 130/300, trend Loss: 0.0291 | 0.0685
Epoch 131/300, trend Loss: 0.0289 | 0.0682
Epoch 132/300, trend Loss: 0.0288 | 0.0689
Epoch 133/300, trend Loss: 0.0286 | 0.0687
Epoch 134/300, trend Loss: 0.0286 | 0.0696
Epoch 135/300, trend Loss: 0.0285 | 0.0686
Epoch 136/300, trend Loss: 0.0285 | 0.0703
Epoch 137/300, trend Loss: 0.0285 | 0.0681
Epoch 138/300, trend Loss: 0.0283 | 0.0703
Epoch 139/300, trend Loss: 0.0283 | 0.0670
Epoch 140/300, trend Loss: 0.0280 | 0.0687
Epoch 141/300, trend Loss: 0.0279 | 0.0664
Epoch 142/300, trend Loss: 0.0278 | 0.0682
Epoch 143/300, trend Loss: 0.0277 | 0.0668
Epoch 144/300, trend Loss: 0.0277 | 0.0686
Epoch 145/300, trend Loss: 0.0276 | 0.0678
Epoch 146/300, trend Loss: 0.0275 | 0.0696
Epoch 147/300, trend Loss: 0.0274 | 0.0690
Epoch 148/300, trend Loss: 0.0274 | 0.0709
Epoch 149/300, trend Loss: 0.0273 | 0.0697
Epoch 150/300, trend Loss: 0.0273 | 0.0714
Epoch 151/300, trend Loss: 0.0273 | 0.0686
Epoch 152/300, trend Loss: 0.0272 | 0.0694
Epoch 153/300, trend Loss: 0.0272 | 0.0666
Epoch 154/300, trend Loss: 0.0271 | 0.0683
Epoch 155/300, trend Loss: 0.0271 | 0.0664
Epoch 156/300, trend Loss: 0.0270 | 0.0686
Epoch 157/300, trend Loss: 0.0269 | 0.0681
Epoch 158/300, trend Loss: 0.0268 | 0.0701
Epoch 159/300, trend Loss: 0.0268 | 0.0707
Epoch 160/300, trend Loss: 0.0267 | 0.0726
Epoch 161/300, trend Loss: 0.0267 | 0.0729
Epoch 162/300, trend Loss: 0.0266 | 0.0720
Epoch 163/300, trend Loss: 0.0266 | 0.0698
Epoch 164/300, trend Loss: 0.0265 | 0.0693
Epoch 165/300, trend Loss: 0.0265 | 0.0680
Epoch 166/300, trend Loss: 0.0265 | 0.0692
Epoch 167/300, trend Loss: 0.0264 | 0.0688
Epoch 168/300, trend Loss: 0.0264 | 0.0706
Epoch 169/300, trend Loss: 0.0263 | 0.0707
Epoch 170/300, trend Loss: 0.0263 | 0.0722
Epoch 171/300, trend Loss: 0.0262 | 0.0721
Epoch 172/300, trend Loss: 0.0262 | 0.0727
Epoch 173/300, trend Loss: 0.0261 | 0.0713
Epoch 174/300, trend Loss: 0.0261 | 0.0711
Epoch 175/300, trend Loss: 0.0261 | 0.0702
Epoch 176/300, trend Loss: 0.0260 | 0.0709
Epoch 177/300, trend Loss: 0.0260 | 0.0706
Epoch 178/300, trend Loss: 0.0260 | 0.0718
Epoch 179/300, trend Loss: 0.0259 | 0.0718
Epoch 180/300, trend Loss: 0.0259 | 0.0726
Epoch 181/300, trend Loss: 0.0258 | 0.0723
Epoch 182/300, trend Loss: 0.0258 | 0.0726
Epoch 183/300, trend Loss: 0.0258 | 0.0721
Epoch 184/300, trend Loss: 0.0257 | 0.0723
Epoch 185/300, trend Loss: 0.0257 | 0.0720
Epoch 186/300, trend Loss: 0.0257 | 0.0726
Epoch 187/300, trend Loss: 0.0257 | 0.0724
Epoch 188/300, trend Loss: 0.0256 | 0.0732
Epoch 189/300, trend Loss: 0.0256 | 0.0730
Epoch 190/300, trend Loss: 0.0256 | 0.0734
Epoch 191/300, trend Loss: 0.0255 | 0.0732
Epoch 192/300, trend Loss: 0.0255 | 0.0735
Epoch 193/300, trend Loss: 0.0255 | 0.0734
Epoch 194/300, trend Loss: 0.0255 | 0.0739
Epoch 195/300, trend Loss: 0.0254 | 0.0738
Epoch 196/300, trend Loss: 0.0254 | 0.0741
Epoch 197/300, trend Loss: 0.0254 | 0.0740
Epoch 198/300, trend Loss: 0.0254 | 0.0743
Epoch 199/300, trend Loss: 0.0254 | 0.0744
Epoch 200/300, trend Loss: 0.0253 | 0.0747
Epoch 201/300, trend Loss: 0.0253 | 0.0746
Epoch 202/300, trend Loss: 0.0253 | 0.0749
Epoch 203/300, trend Loss: 0.0252 | 0.0747
Epoch 204/300, trend Loss: 0.0252 | 0.0750
Epoch 205/300, trend Loss: 0.0252 | 0.0749
Epoch 206/300, trend Loss: 0.0252 | 0.0752
Epoch 207/300, trend Loss: 0.0251 | 0.0750
Epoch 208/300, trend Loss: 0.0251 | 0.0752
Epoch 209/300, trend Loss: 0.0251 | 0.0750
Epoch 210/300, trend Loss: 0.0251 | 0.0752
Epoch 211/300, trend Loss: 0.0250 | 0.0749
Epoch 212/300, trend Loss: 0.0250 | 0.0752
Epoch 213/300, trend Loss: 0.0250 | 0.0749
Epoch 214/300, trend Loss: 0.0250 | 0.0751
Epoch 215/300, trend Loss: 0.0249 | 0.0748
Epoch 216/300, trend Loss: 0.0249 | 0.0749
Epoch 217/300, trend Loss: 0.0249 | 0.0746
Epoch 218/300, trend Loss: 0.0249 | 0.0748
Epoch 219/300, trend Loss: 0.0248 | 0.0745
Epoch 220/300, trend Loss: 0.0248 | 0.0747
Epoch 221/300, trend Loss: 0.0248 | 0.0743
Epoch 222/300, trend Loss: 0.0248 | 0.0745
Epoch 223/300, trend Loss: 0.0248 | 0.0742
Epoch 224/300, trend Loss: 0.0247 | 0.0744
Epoch 225/300, trend Loss: 0.0247 | 0.0741
Epoch 226/300, trend Loss: 0.0247 | 0.0742
Epoch 227/300, trend Loss: 0.0247 | 0.0739
Epoch 228/300, trend Loss: 0.0247 | 0.0740
Epoch 229/300, trend Loss: 0.0247 | 0.0738
Epoch 230/300, trend Loss: 0.0246 | 0.0738
Epoch 231/300, trend Loss: 0.0246 | 0.0736
Epoch 232/300, trend Loss: 0.0246 | 0.0736
Epoch 233/300, trend Loss: 0.0246 | 0.0734
Epoch 234/300, trend Loss: 0.0246 | 0.0734
Epoch 235/300, trend Loss: 0.0246 | 0.0733
Epoch 236/300, trend Loss: 0.0245 | 0.0733
Epoch 237/300, trend Loss: 0.0245 | 0.0732
Epoch 238/300, trend Loss: 0.0245 | 0.0731
Epoch 239/300, trend Loss: 0.0245 | 0.0730
Epoch 240/300, trend Loss: 0.0245 | 0.0730
Epoch 241/300, trend Loss: 0.0245 | 0.0729
Epoch 242/300, trend Loss: 0.0244 | 0.0729
Epoch 243/300, trend Loss: 0.0244 | 0.0728
Epoch 244/300, trend Loss: 0.0244 | 0.0728
Epoch 245/300, trend Loss: 0.0244 | 0.0727
Epoch 246/300, trend Loss: 0.0244 | 0.0727
Epoch 247/300, trend Loss: 0.0244 | 0.0727
Epoch 248/300, trend Loss: 0.0244 | 0.0727
Epoch 249/300, trend Loss: 0.0243 | 0.0726
Epoch 250/300, trend Loss: 0.0243 | 0.0726
Epoch 251/300, trend Loss: 0.0243 | 0.0726
Epoch 252/300, trend Loss: 0.0243 | 0.0726
Epoch 253/300, trend Loss: 0.0243 | 0.0725
Epoch 254/300, trend Loss: 0.0243 | 0.0725
Epoch 255/300, trend Loss: 0.0243 | 0.0725
Epoch 256/300, trend Loss: 0.0243 | 0.0725
Epoch 257/300, trend Loss: 0.0242 | 0.0725
Epoch 258/300, trend Loss: 0.0242 | 0.0725
Epoch 259/300, trend Loss: 0.0242 | 0.0725
Epoch 260/300, trend Loss: 0.0242 | 0.0724
Epoch 261/300, trend Loss: 0.0242 | 0.0724
Epoch 262/300, trend Loss: 0.0242 | 0.0724
Epoch 263/300, trend Loss: 0.0242 | 0.0724
Epoch 264/300, trend Loss: 0.0242 | 0.0724
Epoch 265/300, trend Loss: 0.0242 | 0.0724
Epoch 266/300, trend Loss: 0.0241 | 0.0724
Epoch 267/300, trend Loss: 0.0241 | 0.0724
Epoch 268/300, trend Loss: 0.0241 | 0.0724
Epoch 269/300, trend Loss: 0.0241 | 0.0724
Epoch 270/300, trend Loss: 0.0241 | 0.0724
Epoch 271/300, trend Loss: 0.0241 | 0.0724
Epoch 272/300, trend Loss: 0.0241 | 0.0724
Epoch 273/300, trend Loss: 0.0241 | 0.0724
Epoch 274/300, trend Loss: 0.0241 | 0.0724
Epoch 275/300, trend Loss: 0.0241 | 0.0724
Epoch 276/300, trend Loss: 0.0241 | 0.0724
Epoch 277/300, trend Loss: 0.0240 | 0.0724
Epoch 278/300, trend Loss: 0.0240 | 0.0724
Epoch 279/300, trend Loss: 0.0240 | 0.0724
Epoch 280/300, trend Loss: 0.0240 | 0.0724
Epoch 281/300, trend Loss: 0.0240 | 0.0725
Epoch 282/300, trend Loss: 0.0240 | 0.0724
Epoch 283/300, trend Loss: 0.0240 | 0.0725
Epoch 284/300, trend Loss: 0.0240 | 0.0725
Epoch 285/300, trend Loss: 0.0240 | 0.0725
Epoch 286/300, trend Loss: 0.0240 | 0.0725
Epoch 287/300, trend Loss: 0.0240 | 0.0725
Epoch 288/300, trend Loss: 0.0240 | 0.0725
Epoch 289/300, trend Loss: 0.0240 | 0.0725
Epoch 290/300, trend Loss: 0.0240 | 0.0725
Epoch 291/300, trend Loss: 0.0240 | 0.0725
Epoch 292/300, trend Loss: 0.0240 | 0.0725
Epoch 293/300, trend Loss: 0.0239 | 0.0725
Epoch 294/300, trend Loss: 0.0239 | 0.0725
Epoch 295/300, trend Loss: 0.0239 | 0.0725
Epoch 296/300, trend Loss: 0.0239 | 0.0725
Epoch 297/300, trend Loss: 0.0239 | 0.0725
Epoch 298/300, trend Loss: 0.0239 | 0.0725
Epoch 299/300, trend Loss: 0.0239 | 0.0725
Epoch 300/300, trend Loss: 0.0239 | 0.0725
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.8232588019731839, 'learning_rate': 0.00037601327197752575, 'batch_size': 221, 'step_size': 13, 'gamma': 0.9671623477663189}
Epoch 1/300, seasonal_0 Loss: 2.3992 | 0.3685
Epoch 2/300, seasonal_0 Loss: 0.3558 | 0.8669
Epoch 3/300, seasonal_0 Loss: 0.2637 | 0.1649
Epoch 4/300, seasonal_0 Loss: 0.2880 | 0.1530
Epoch 5/300, seasonal_0 Loss: 0.2338 | 0.1726
Epoch 6/300, seasonal_0 Loss: 0.4180 | 0.3432
Epoch 7/300, seasonal_0 Loss: 0.6173 | 0.3644
Epoch 8/300, seasonal_0 Loss: 0.5370 | 0.2392
Epoch 9/300, seasonal_0 Loss: 0.4086 | 0.2699
Epoch 10/300, seasonal_0 Loss: 0.2181 | 0.1840
Epoch 11/300, seasonal_0 Loss: 0.2230 | 0.1525
Epoch 12/300, seasonal_0 Loss: 0.2920 | 0.2634
Epoch 13/300, seasonal_0 Loss: 0.3250 | 0.1080
Epoch 14/300, seasonal_0 Loss: 0.2396 | 0.2175
Epoch 15/300, seasonal_0 Loss: 0.1595 | 0.0995
Epoch 16/300, seasonal_0 Loss: 0.1478 | 0.1863
Epoch 17/300, seasonal_0 Loss: 0.2426 | 0.1273
Epoch 18/300, seasonal_0 Loss: 0.1663 | 0.0978
Epoch 19/300, seasonal_0 Loss: 0.1500 | 0.0832
Epoch 20/300, seasonal_0 Loss: 0.1632 | 0.1446
Epoch 21/300, seasonal_0 Loss: 0.1697 | 0.0895
Epoch 22/300, seasonal_0 Loss: 0.1927 | 0.1413
Epoch 23/300, seasonal_0 Loss: 0.2537 | 0.1747
Epoch 24/300, seasonal_0 Loss: 0.2729 | 0.1190
Epoch 25/300, seasonal_0 Loss: 0.2107 | 0.2001
Epoch 26/300, seasonal_0 Loss: 0.1847 | 0.1045
Epoch 27/300, seasonal_0 Loss: 0.1617 | 0.1555
Epoch 28/300, seasonal_0 Loss: 0.1948 | 0.1517
Epoch 29/300, seasonal_0 Loss: 0.1960 | 0.0916
Epoch 30/300, seasonal_0 Loss: 0.2254 | 0.1520
Epoch 31/300, seasonal_0 Loss: 0.2173 | 0.1538
Epoch 32/300, seasonal_0 Loss: 0.2056 | 0.1343
Epoch 33/300, seasonal_0 Loss: 0.1624 | 0.0973
Epoch 34/300, seasonal_0 Loss: 0.1321 | 0.0836
Epoch 35/300, seasonal_0 Loss: 0.1161 | 0.0639
Epoch 36/300, seasonal_0 Loss: 0.1048 | 0.0589
Epoch 37/300, seasonal_0 Loss: 0.1001 | 0.0557
Epoch 38/300, seasonal_0 Loss: 0.0975 | 0.0528
Epoch 39/300, seasonal_0 Loss: 0.0948 | 0.0602
Epoch 40/300, seasonal_0 Loss: 0.1021 | 0.0777
Epoch 41/300, seasonal_0 Loss: 0.1017 | 0.0547
Epoch 42/300, seasonal_0 Loss: 0.0970 | 0.0578
Epoch 43/300, seasonal_0 Loss: 0.1059 | 0.0578
Epoch 44/300, seasonal_0 Loss: 0.1233 | 0.0903
Epoch 45/300, seasonal_0 Loss: 0.1474 | 0.0617
Epoch 46/300, seasonal_0 Loss: 0.1412 | 0.1042
Epoch 47/300, seasonal_0 Loss: 0.1468 | 0.0711
Epoch 48/300, seasonal_0 Loss: 0.1264 | 0.1276
Epoch 49/300, seasonal_0 Loss: 0.1233 | 0.0641
Epoch 50/300, seasonal_0 Loss: 0.1164 | 0.0836
Epoch 51/300, seasonal_0 Loss: 0.1092 | 0.0545
Epoch 52/300, seasonal_0 Loss: 0.1074 | 0.0679
Epoch 53/300, seasonal_0 Loss: 0.0999 | 0.0551
Epoch 54/300, seasonal_0 Loss: 0.1019 | 0.0486
Epoch 55/300, seasonal_0 Loss: 0.0962 | 0.0654
Epoch 56/300, seasonal_0 Loss: 0.1033 | 0.0617
Epoch 57/300, seasonal_0 Loss: 0.1004 | 0.0636
Epoch 58/300, seasonal_0 Loss: 0.1061 | 0.0520
Epoch 59/300, seasonal_0 Loss: 0.0952 | 0.0497
Epoch 60/300, seasonal_0 Loss: 0.1028 | 0.0663
Epoch 61/300, seasonal_0 Loss: 0.1021 | 0.0841
Epoch 62/300, seasonal_0 Loss: 0.1030 | 0.0510
Epoch 63/300, seasonal_0 Loss: 0.0915 | 0.0497
Epoch 64/300, seasonal_0 Loss: 0.1010 | 0.0502
Epoch 65/300, seasonal_0 Loss: 0.1047 | 0.0577
Epoch 66/300, seasonal_0 Loss: 0.0950 | 0.0481
Epoch 67/300, seasonal_0 Loss: 0.0921 | 0.0666
Epoch 68/300, seasonal_0 Loss: 0.1015 | 0.0477
Epoch 69/300, seasonal_0 Loss: 0.1138 | 0.1060
Epoch 70/300, seasonal_0 Loss: 0.1263 | 0.0774
Epoch 71/300, seasonal_0 Loss: 0.1294 | 0.0484
Epoch 72/300, seasonal_0 Loss: 0.1247 | 0.0543
Epoch 73/300, seasonal_0 Loss: 0.1076 | 0.0606
Epoch 74/300, seasonal_0 Loss: 0.1124 | 0.0565
Epoch 75/300, seasonal_0 Loss: 0.1021 | 0.0855
Epoch 76/300, seasonal_0 Loss: 0.1451 | 0.1146
Epoch 77/300, seasonal_0 Loss: 0.1411 | 0.0624
Epoch 78/300, seasonal_0 Loss: 0.1018 | 0.0595
Epoch 79/300, seasonal_0 Loss: 0.1120 | 0.0701
Epoch 80/300, seasonal_0 Loss: 0.1259 | 0.0632
Epoch 81/300, seasonal_0 Loss: 0.1087 | 0.0653
Epoch 82/300, seasonal_0 Loss: 0.1288 | 0.1156
Epoch 83/300, seasonal_0 Loss: 0.1322 | 0.0533
Epoch 84/300, seasonal_0 Loss: 0.1038 | 0.0686
Epoch 85/300, seasonal_0 Loss: 0.0916 | 0.0527
Epoch 86/300, seasonal_0 Loss: 0.1006 | 0.0760
Epoch 87/300, seasonal_0 Loss: 0.1048 | 0.0618
Epoch 88/300, seasonal_0 Loss: 0.0919 | 0.0416
Epoch 89/300, seasonal_0 Loss: 0.0861 | 0.0608
Epoch 90/300, seasonal_0 Loss: 0.0938 | 0.0496
Epoch 91/300, seasonal_0 Loss: 0.0890 | 0.0502
Epoch 92/300, seasonal_0 Loss: 0.0910 | 0.0673
Epoch 93/300, seasonal_0 Loss: 0.0865 | 0.0515
Epoch 94/300, seasonal_0 Loss: 0.0947 | 0.0605
Epoch 95/300, seasonal_0 Loss: 0.0881 | 0.0414
Epoch 96/300, seasonal_0 Loss: 0.0823 | 0.0526
Epoch 97/300, seasonal_0 Loss: 0.0829 | 0.0527
Epoch 98/300, seasonal_0 Loss: 0.0855 | 0.0540
Epoch 99/300, seasonal_0 Loss: 0.0814 | 0.0442
Epoch 100/300, seasonal_0 Loss: 0.0925 | 0.0699
Epoch 101/300, seasonal_0 Loss: 0.0892 | 0.0404
Epoch 102/300, seasonal_0 Loss: 0.0878 | 0.0639
Epoch 103/300, seasonal_0 Loss: 0.0988 | 0.0531
Epoch 104/300, seasonal_0 Loss: 0.0935 | 0.0763
Epoch 105/300, seasonal_0 Loss: 0.0750 | 0.0408
Epoch 106/300, seasonal_0 Loss: 0.0757 | 0.0557
Epoch 107/300, seasonal_0 Loss: 0.0774 | 0.0444
Epoch 108/300, seasonal_0 Loss: 0.0800 | 0.0471
Epoch 109/300, seasonal_0 Loss: 0.0779 | 0.0552
Epoch 110/300, seasonal_0 Loss: 0.0804 | 0.0510
Epoch 111/300, seasonal_0 Loss: 0.0742 | 0.0456
Epoch 112/300, seasonal_0 Loss: 0.0764 | 0.0497
Epoch 113/300, seasonal_0 Loss: 0.0759 | 0.0391
Epoch 114/300, seasonal_0 Loss: 0.0717 | 0.0433
Epoch 115/300, seasonal_0 Loss: 0.0709 | 0.0417
Epoch 116/300, seasonal_0 Loss: 0.0768 | 0.0536
Epoch 117/300, seasonal_0 Loss: 0.0725 | 0.0371
Epoch 118/300, seasonal_0 Loss: 0.0705 | 0.0414
Epoch 119/300, seasonal_0 Loss: 0.0722 | 0.0446
Epoch 120/300, seasonal_0 Loss: 0.0737 | 0.0499
Epoch 121/300, seasonal_0 Loss: 0.0706 | 0.0371
Epoch 122/300, seasonal_0 Loss: 0.0758 | 0.0541
Epoch 123/300, seasonal_0 Loss: 0.0729 | 0.0341
Epoch 124/300, seasonal_0 Loss: 0.0677 | 0.0440
Epoch 125/300, seasonal_0 Loss: 0.0714 | 0.0478
Epoch 126/300, seasonal_0 Loss: 0.0757 | 0.0544
Epoch 127/300, seasonal_0 Loss: 0.0696 | 0.0381
Epoch 128/300, seasonal_0 Loss: 0.0698 | 0.0444
Epoch 129/300, seasonal_0 Loss: 0.0703 | 0.0351
Epoch 130/300, seasonal_0 Loss: 0.0668 | 0.0424
Epoch 131/300, seasonal_0 Loss: 0.0646 | 0.0360
Epoch 132/300, seasonal_0 Loss: 0.0662 | 0.0445
Epoch 133/300, seasonal_0 Loss: 0.0629 | 0.0360
Epoch 134/300, seasonal_0 Loss: 0.0648 | 0.0395
Epoch 135/300, seasonal_0 Loss: 0.0676 | 0.0347
Epoch 136/300, seasonal_0 Loss: 0.0689 | 0.0485
Epoch 137/300, seasonal_0 Loss: 0.0669 | 0.0412
Epoch 138/300, seasonal_0 Loss: 0.0679 | 0.0463
Epoch 139/300, seasonal_0 Loss: 0.0664 | 0.0417
Epoch 140/300, seasonal_0 Loss: 0.0649 | 0.0420
Epoch 141/300, seasonal_0 Loss: 0.0638 | 0.0359
Epoch 142/300, seasonal_0 Loss: 0.0615 | 0.0380
Epoch 143/300, seasonal_0 Loss: 0.0632 | 0.0404
Epoch 144/300, seasonal_0 Loss: 0.0624 | 0.0384
Epoch 145/300, seasonal_0 Loss: 0.0624 | 0.0367
Epoch 146/300, seasonal_0 Loss: 0.0621 | 0.0408
Epoch 147/300, seasonal_0 Loss: 0.0623 | 0.0383
Epoch 148/300, seasonal_0 Loss: 0.0604 | 0.0360
Epoch 149/300, seasonal_0 Loss: 0.0609 | 0.0347
Epoch 150/300, seasonal_0 Loss: 0.0629 | 0.0393
Epoch 151/300, seasonal_0 Loss: 0.0647 | 0.0406
Epoch 152/300, seasonal_0 Loss: 0.0629 | 0.0370
Epoch 153/300, seasonal_0 Loss: 0.0649 | 0.0459
Epoch 154/300, seasonal_0 Loss: 0.0673 | 0.0397
Epoch 155/300, seasonal_0 Loss: 0.0728 | 0.0513
Epoch 156/300, seasonal_0 Loss: 0.0753 | 0.0380
Epoch 157/300, seasonal_0 Loss: 0.0668 | 0.0495
Epoch 158/300, seasonal_0 Loss: 0.0686 | 0.0412
Epoch 159/300, seasonal_0 Loss: 0.0610 | 0.0497
Epoch 160/300, seasonal_0 Loss: 0.0596 | 0.0371
Epoch 161/300, seasonal_0 Loss: 0.0606 | 0.0498
Epoch 162/300, seasonal_0 Loss: 0.0676 | 0.0431
Epoch 163/300, seasonal_0 Loss: 0.0692 | 0.0573
Epoch 164/300, seasonal_0 Loss: 0.0735 | 0.0619
Epoch 165/300, seasonal_0 Loss: 0.0769 | 0.0550
Epoch 166/300, seasonal_0 Loss: 0.0748 | 0.0399
Epoch 167/300, seasonal_0 Loss: 0.0679 | 0.0540
Epoch 168/300, seasonal_0 Loss: 0.0630 | 0.0401
Epoch 169/300, seasonal_0 Loss: 0.0626 | 0.0361
Epoch 170/300, seasonal_0 Loss: 0.0634 | 0.0449
Epoch 171/300, seasonal_0 Loss: 0.0684 | 0.0480
Epoch 172/300, seasonal_0 Loss: 0.0671 | 0.0425
Epoch 173/300, seasonal_0 Loss: 0.0687 | 0.0547
Epoch 174/300, seasonal_0 Loss: 0.0643 | 0.0354
Epoch 175/300, seasonal_0 Loss: 0.0605 | 0.0409
Epoch 176/300, seasonal_0 Loss: 0.0593 | 0.0393
Epoch 177/300, seasonal_0 Loss: 0.0581 | 0.0360
Epoch 178/300, seasonal_0 Loss: 0.0597 | 0.0474
Epoch 179/300, seasonal_0 Loss: 0.0635 | 0.0440
Epoch 180/300, seasonal_0 Loss: 0.0569 | 0.0400
Epoch 181/300, seasonal_0 Loss: 0.0628 | 0.0481
Epoch 182/300, seasonal_0 Loss: 0.0589 | 0.0415
Epoch 183/300, seasonal_0 Loss: 0.0528 | 0.0491
Epoch 184/300, seasonal_0 Loss: 0.0566 | 0.0391
Epoch 185/300, seasonal_0 Loss: 0.0525 | 0.0481
Epoch 186/300, seasonal_0 Loss: 0.0519 | 0.0376
Epoch 187/300, seasonal_0 Loss: 0.0509 | 0.0385
Epoch 188/300, seasonal_0 Loss: 0.0491 | 0.0434
Epoch 189/300, seasonal_0 Loss: 0.0528 | 0.0448
Epoch 190/300, seasonal_0 Loss: 0.0483 | 0.0391
Epoch 191/300, seasonal_0 Loss: 0.0512 | 0.0421
Epoch 192/300, seasonal_0 Loss: 0.0526 | 0.0403
Epoch 193/300, seasonal_0 Loss: 0.0495 | 0.0470
Epoch 194/300, seasonal_0 Loss: 0.0495 | 0.0361
Epoch 195/300, seasonal_0 Loss: 0.0484 | 0.0443
Epoch 196/300, seasonal_0 Loss: 0.0498 | 0.0380
Epoch 197/300, seasonal_0 Loss: 0.0470 | 0.0393
Epoch 198/300, seasonal_0 Loss: 0.0468 | 0.0452
Epoch 199/300, seasonal_0 Loss: 0.0490 | 0.0428
Epoch 200/300, seasonal_0 Loss: 0.0478 | 0.0372
Epoch 201/300, seasonal_0 Loss: 0.0455 | 0.0420
Epoch 202/300, seasonal_0 Loss: 0.0467 | 0.0351
Epoch 203/300, seasonal_0 Loss: 0.0473 | 0.0485
Epoch 204/300, seasonal_0 Loss: 0.0468 | 0.0381
Epoch 205/300, seasonal_0 Loss: 0.0464 | 0.0381
Epoch 206/300, seasonal_0 Loss: 0.0435 | 0.0422
Epoch 207/300, seasonal_0 Loss: 0.0454 | 0.0354
Epoch 208/300, seasonal_0 Loss: 0.0453 | 0.0435
Epoch 209/300, seasonal_0 Loss: 0.0473 | 0.0417
Epoch 210/300, seasonal_0 Loss: 0.0480 | 0.0386
Epoch 211/300, seasonal_0 Loss: 0.0447 | 0.0452
Epoch 212/300, seasonal_0 Loss: 0.0451 | 0.0343
Epoch 213/300, seasonal_0 Loss: 0.0452 | 0.0433
Epoch 214/300, seasonal_0 Loss: 0.0444 | 0.0397
Epoch 215/300, seasonal_0 Loss: 0.0447 | 0.0363
Epoch 216/300, seasonal_0 Loss: 0.0436 | 0.0435
Epoch 217/300, seasonal_0 Loss: 0.0439 | 0.0370
Epoch 218/300, seasonal_0 Loss: 0.0442 | 0.0402
Epoch 219/300, seasonal_0 Loss: 0.0441 | 0.0413
Epoch 220/300, seasonal_0 Loss: 0.0444 | 0.0355
Epoch 221/300, seasonal_0 Loss: 0.0437 | 0.0451
Epoch 222/300, seasonal_0 Loss: 0.0418 | 0.0361
Epoch 223/300, seasonal_0 Loss: 0.0416 | 0.0378
Epoch 224/300, seasonal_0 Loss: 0.0415 | 0.0392
Epoch 225/300, seasonal_0 Loss: 0.0425 | 0.0381
Epoch 226/300, seasonal_0 Loss: 0.0416 | 0.0366
Epoch 227/300, seasonal_0 Loss: 0.0415 | 0.0401
Epoch 228/300, seasonal_0 Loss: 0.0411 | 0.0356
Epoch 229/300, seasonal_0 Loss: 0.0408 | 0.0398
Epoch 230/300, seasonal_0 Loss: 0.0403 | 0.0390
Epoch 231/300, seasonal_0 Loss: 0.0411 | 0.0394
Epoch 232/300, seasonal_0 Loss: 0.0410 | 0.0377
Epoch 233/300, seasonal_0 Loss: 0.0414 | 0.0395
Epoch 234/300, seasonal_0 Loss: 0.0413 | 0.0363
Epoch 235/300, seasonal_0 Loss: 0.0407 | 0.0421
Epoch 236/300, seasonal_0 Loss: 0.0401 | 0.0371
Epoch 237/300, seasonal_0 Loss: 0.0405 | 0.0402
Epoch 238/300, seasonal_0 Loss: 0.0408 | 0.0375
Epoch 239/300, seasonal_0 Loss: 0.0406 | 0.0405
Epoch 240/300, seasonal_0 Loss: 0.0413 | 0.0369
Epoch 241/300, seasonal_0 Loss: 0.0420 | 0.0446
Epoch 242/300, seasonal_0 Loss: 0.0400 | 0.0366
Epoch 243/300, seasonal_0 Loss: 0.0405 | 0.0411
Epoch 244/300, seasonal_0 Loss: 0.0419 | 0.0389
Epoch 245/300, seasonal_0 Loss: 0.0411 | 0.0428
Epoch 246/300, seasonal_0 Loss: 0.0419 | 0.0367
Epoch 247/300, seasonal_0 Loss: 0.0430 | 0.0470
Epoch 248/300, seasonal_0 Loss: 0.0416 | 0.0372
Epoch 249/300, seasonal_0 Loss: 0.0407 | 0.0406
Epoch 250/300, seasonal_0 Loss: 0.0405 | 0.0411
Epoch 251/300, seasonal_0 Loss: 0.0433 | 0.0427
Epoch 252/300, seasonal_0 Loss: 0.0410 | 0.0385
Epoch 253/300, seasonal_0 Loss: 0.0440 | 0.0425
Epoch 254/300, seasonal_0 Loss: 0.0450 | 0.0377
Epoch 255/300, seasonal_0 Loss: 0.0406 | 0.0458
Epoch 256/300, seasonal_0 Loss: 0.0420 | 0.0391
Epoch 257/300, seasonal_0 Loss: 0.0417 | 0.0446
Epoch 258/300, seasonal_0 Loss: 0.0441 | 0.0390
Epoch 259/300, seasonal_0 Loss: 0.0402 | 0.0423
Epoch 260/300, seasonal_0 Loss: 0.0411 | 0.0411
Epoch 261/300, seasonal_0 Loss: 0.0414 | 0.0436
Epoch 262/300, seasonal_0 Loss: 0.0422 | 0.0393
Epoch 263/300, seasonal_0 Loss: 0.0414 | 0.0428
Epoch 264/300, seasonal_0 Loss: 0.0395 | 0.0395
Epoch 265/300, seasonal_0 Loss: 0.0388 | 0.0393
Epoch 266/300, seasonal_0 Loss: 0.0396 | 0.0405
Epoch 267/300, seasonal_0 Loss: 0.0396 | 0.0389
Epoch 268/300, seasonal_0 Loss: 0.0385 | 0.0407
Epoch 269/300, seasonal_0 Loss: 0.0389 | 0.0418
Epoch 270/300, seasonal_0 Loss: 0.0386 | 0.0388
Epoch 271/300, seasonal_0 Loss: 0.0406 | 0.0396
Epoch 272/300, seasonal_0 Loss: 0.0378 | 0.0405
Epoch 273/300, seasonal_0 Loss: 0.0385 | 0.0374
Epoch 274/300, seasonal_0 Loss: 0.0389 | 0.0424
Epoch 275/300, seasonal_0 Loss: 0.0385 | 0.0385
Epoch 276/300, seasonal_0 Loss: 0.0377 | 0.0397
Epoch 277/300, seasonal_0 Loss: 0.0380 | 0.0430
Epoch 278/300, seasonal_0 Loss: 0.0389 | 0.0396
Epoch 279/300, seasonal_0 Loss: 0.0399 | 0.0398
Epoch 280/300, seasonal_0 Loss: 0.0372 | 0.0414
Epoch 281/300, seasonal_0 Loss: 0.0379 | 0.0369
Epoch 282/300, seasonal_0 Loss: 0.0383 | 0.0447
Epoch 283/300, seasonal_0 Loss: 0.0376 | 0.0382
Epoch 284/300, seasonal_0 Loss: 0.0377 | 0.0408
Epoch 285/300, seasonal_0 Loss: 0.0366 | 0.0422
Epoch 286/300, seasonal_0 Loss: 0.0365 | 0.0376
Epoch 287/300, seasonal_0 Loss: 0.0373 | 0.0402
Epoch 288/300, seasonal_0 Loss: 0.0363 | 0.0404
Epoch 289/300, seasonal_0 Loss: 0.0372 | 0.0388
Epoch 290/300, seasonal_0 Loss: 0.0375 | 0.0442
Epoch 291/300, seasonal_0 Loss: 0.0359 | 0.0366
Epoch 292/300, seasonal_0 Loss: 0.0356 | 0.0408
Epoch 293/300, seasonal_0 Loss: 0.0349 | 0.0394
Epoch 294/300, seasonal_0 Loss: 0.0345 | 0.0384
Epoch 295/300, seasonal_0 Loss: 0.0351 | 0.0401
Epoch 296/300, seasonal_0 Loss: 0.0351 | 0.0396
Epoch 297/300, seasonal_0 Loss: 0.0352 | 0.0397
Epoch 298/300, seasonal_0 Loss: 0.0358 | 0.0428
Epoch 299/300, seasonal_0 Loss: 0.0350 | 0.0369
Epoch 300/300, seasonal_0 Loss: 0.0350 | 0.0397
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.8259420758579452, 'learning_rate': 0.0004920066681186387, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9060231932367988}
Epoch 1/300, seasonal_1 Loss: 1.1120 | 1.2965
Epoch 2/300, seasonal_1 Loss: 0.8671 | 1.2471
Epoch 3/300, seasonal_1 Loss: 0.9150 | 1.2472
Epoch 4/300, seasonal_1 Loss: 0.8952 | 1.2461
Epoch 5/300, seasonal_1 Loss: 0.8951 | 1.2546
Epoch 6/300, seasonal_1 Loss: 1.0022 | 1.2995
Epoch 7/300, seasonal_1 Loss: 0.9309 | 1.2468
Epoch 8/300, seasonal_1 Loss: 0.8958 | 1.2468
Epoch 9/300, seasonal_1 Loss: 0.9106 | 1.2618
Epoch 10/300, seasonal_1 Loss: 0.9029 | 1.2798
Epoch 11/300, seasonal_1 Loss: 0.8953 | 1.2669
Epoch 12/300, seasonal_1 Loss: 0.9075 | 1.2630
Epoch 13/300, seasonal_1 Loss: 0.9239 | 1.2600
Epoch 14/300, seasonal_1 Loss: 0.9536 | 1.2643
Epoch 15/300, seasonal_1 Loss: 0.9732 | 1.2838
Epoch 16/300, seasonal_1 Loss: 0.9726 | 1.2470
Epoch 17/300, seasonal_1 Loss: 0.9894 | 1.3469
Epoch 18/300, seasonal_1 Loss: 0.9736 | 1.4172
Epoch 19/300, seasonal_1 Loss: 0.9547 | 1.4535
Epoch 20/300, seasonal_1 Loss: 0.9435 | 1.4747
Epoch 21/300, seasonal_1 Loss: 0.9382 | 1.5238
Epoch 22/300, seasonal_1 Loss: 0.9239 | 1.5291
Epoch 23/300, seasonal_1 Loss: 0.9220 | 1.5372
Epoch 24/300, seasonal_1 Loss: 0.9198 | 1.5446
Epoch 25/300, seasonal_1 Loss: 0.9177 | 1.5515
Epoch 26/300, seasonal_1 Loss: 0.9157 | 1.5579
Epoch 27/300, seasonal_1 Loss: 0.9130 | 1.5851
Epoch 28/300, seasonal_1 Loss: 0.9061 | 1.5857
Epoch 29/300, seasonal_1 Loss: 0.9057 | 1.5891
Epoch 30/300, seasonal_1 Loss: 0.9047 | 1.5922
Epoch 31/300, seasonal_1 Loss: 0.9038 | 1.5953
Epoch 32/300, seasonal_1 Loss: 0.9028 | 1.5984
Epoch 33/300, seasonal_1 Loss: 0.9019 | 1.6013
Epoch 34/300, seasonal_1 Loss: 0.8997 | 1.6180
Epoch 35/300, seasonal_1 Loss: 0.8959 | 1.6188
Epoch 36/300, seasonal_1 Loss: 0.8957 | 1.6203
Epoch 37/300, seasonal_1 Loss: 0.8952 | 1.6219
Epoch 38/300, seasonal_1 Loss: 0.8946 | 1.6236
Epoch 39/300, seasonal_1 Loss: 0.8941 | 1.6252
Epoch 40/300, seasonal_1 Loss: 0.8923 | 1.6365
Epoch 41/300, seasonal_1 Loss: 0.8900 | 1.6377
Epoch 42/300, seasonal_1 Loss: 0.8897 | 1.6384
Epoch 43/300, seasonal_1 Loss: 0.8894 | 1.6393
Epoch 44/300, seasonal_1 Loss: 0.8891 | 1.6403
Epoch 45/300, seasonal_1 Loss: 0.8888 | 1.6412
Epoch 46/300, seasonal_1 Loss: 0.8885 | 1.6422
Epoch 47/300, seasonal_1 Loss: 0.8868 | 1.6500
Epoch 48/300, seasonal_1 Loss: 0.8854 | 1.6515
Epoch 49/300, seasonal_1 Loss: 0.8851 | 1.6519
Epoch 50/300, seasonal_1 Loss: 0.8849 | 1.6524
Epoch 51/300, seasonal_1 Loss: 0.8847 | 1.6530
Epoch 52/300, seasonal_1 Loss: 0.8845 | 1.6536
Epoch 53/300, seasonal_1 Loss: 0.8831 | 1.6593
Epoch 54/300, seasonal_1 Loss: 0.8822 | 1.6609
Epoch 55/300, seasonal_1 Loss: 0.8819 | 1.6611
Epoch 56/300, seasonal_1 Loss: 0.8818 | 1.6614
Epoch 57/300, seasonal_1 Loss: 0.8817 | 1.6618
Epoch 58/300, seasonal_1 Loss: 0.8815 | 1.6622
Epoch 59/300, seasonal_1 Loss: 0.8814 | 1.6626
Epoch 60/300, seasonal_1 Loss: 0.8801 | 1.6668
Epoch 61/300, seasonal_1 Loss: 0.8795 | 1.6683
Epoch 62/300, seasonal_1 Loss: 0.8793 | 1.6685
Epoch 63/300, seasonal_1 Loss: 0.8792 | 1.6687
Epoch 64/300, seasonal_1 Loss: 0.8791 | 1.6690
Epoch 65/300, seasonal_1 Loss: 0.8790 | 1.6692
Epoch 66/300, seasonal_1 Loss: 0.8779 | 1.6724
Epoch 67/300, seasonal_1 Loss: 0.8775 | 1.6738
Epoch 68/300, seasonal_1 Loss: 0.8773 | 1.6740
Epoch 69/300, seasonal_1 Loss: 0.8772 | 1.6742
Epoch 70/300, seasonal_1 Loss: 0.8772 | 1.6743
Epoch 71/300, seasonal_1 Loss: 0.8771 | 1.6745
Epoch 72/300, seasonal_1 Loss: 0.8771 | 1.6746
Epoch 73/300, seasonal_1 Loss: 0.8761 | 1.6770
Epoch 74/300, seasonal_1 Loss: 0.8758 | 1.6783
Epoch 75/300, seasonal_1 Loss: 0.8756 | 1.6786
Epoch 76/300, seasonal_1 Loss: 0.8756 | 1.6787
Epoch 77/300, seasonal_1 Loss: 0.8755 | 1.6788
Epoch 78/300, seasonal_1 Loss: 0.8755 | 1.6789
Epoch 79/300, seasonal_1 Loss: 0.8746 | 1.6808
Epoch 80/300, seasonal_1 Loss: 0.8744 | 1.6819
Epoch 81/300, seasonal_1 Loss: 0.8743 | 1.6822
Epoch 82/300, seasonal_1 Loss: 0.8742 | 1.6823
Epoch 83/300, seasonal_1 Loss: 0.8742 | 1.6824
Epoch 84/300, seasonal_1 Loss: 0.8742 | 1.6824
Epoch 85/300, seasonal_1 Loss: 0.8742 | 1.6825
Epoch 86/300, seasonal_1 Loss: 0.8734 | 1.6840
Epoch 87/300, seasonal_1 Loss: 0.8732 | 1.6850
Epoch 88/300, seasonal_1 Loss: 0.8731 | 1.6853
Epoch 89/300, seasonal_1 Loss: 0.8731 | 1.6854
Epoch 90/300, seasonal_1 Loss: 0.8731 | 1.6854
Epoch 91/300, seasonal_1 Loss: 0.8730 | 1.6855
Epoch 92/300, seasonal_1 Loss: 0.8723 | 1.6867
Epoch 93/300, seasonal_1 Loss: 0.8722 | 1.6875
Epoch 94/300, seasonal_1 Loss: 0.8722 | 1.6878
Epoch 95/300, seasonal_1 Loss: 0.8721 | 1.6879
Epoch 96/300, seasonal_1 Loss: 0.8721 | 1.6880
Epoch 97/300, seasonal_1 Loss: 0.8721 | 1.6880
Epoch 98/300, seasonal_1 Loss: 0.8721 | 1.6880
Epoch 99/300, seasonal_1 Loss: 0.8714 | 1.6890
Epoch 100/300, seasonal_1 Loss: 0.8714 | 1.6897
Epoch 101/300, seasonal_1 Loss: 0.8713 | 1.6900
Epoch 102/300, seasonal_1 Loss: 0.8713 | 1.6902
Epoch 103/300, seasonal_1 Loss: 0.8713 | 1.6902
Epoch 104/300, seasonal_1 Loss: 0.8713 | 1.6902
Epoch 105/300, seasonal_1 Loss: 0.8707 | 1.6910
Epoch 106/300, seasonal_1 Loss: 0.8706 | 1.6916
Epoch 107/300, seasonal_1 Loss: 0.8706 | 1.6919
Epoch 108/300, seasonal_1 Loss: 0.8706 | 1.6921
Epoch 109/300, seasonal_1 Loss: 0.8706 | 1.6921
Epoch 110/300, seasonal_1 Loss: 0.8705 | 1.6921
Epoch 111/300, seasonal_1 Loss: 0.8705 | 1.6921
Epoch 112/300, seasonal_1 Loss: 0.8700 | 1.6928
Epoch 113/300, seasonal_1 Loss: 0.8700 | 1.6933
Epoch 114/300, seasonal_1 Loss: 0.8699 | 1.6936
Epoch 115/300, seasonal_1 Loss: 0.8699 | 1.6937
Epoch 116/300, seasonal_1 Loss: 0.8699 | 1.6938
Epoch 117/300, seasonal_1 Loss: 0.8699 | 1.6938
Epoch 118/300, seasonal_1 Loss: 0.8694 | 1.6943
Epoch 119/300, seasonal_1 Loss: 0.8694 | 1.6948
Epoch 120/300, seasonal_1 Loss: 0.8694 | 1.6951
Epoch 121/300, seasonal_1 Loss: 0.8694 | 1.6952
Epoch 122/300, seasonal_1 Loss: 0.8694 | 1.6952
Epoch 123/300, seasonal_1 Loss: 0.8694 | 1.6953
Epoch 124/300, seasonal_1 Loss: 0.8694 | 1.6953
Epoch 125/300, seasonal_1 Loss: 0.8689 | 1.6957
Epoch 126/300, seasonal_1 Loss: 0.8689 | 1.6961
Epoch 127/300, seasonal_1 Loss: 0.8689 | 1.6964
Epoch 128/300, seasonal_1 Loss: 0.8689 | 1.6965
Epoch 129/300, seasonal_1 Loss: 0.8689 | 1.6966
Epoch 130/300, seasonal_1 Loss: 0.8689 | 1.6966
Epoch 131/300, seasonal_1 Loss: 0.8685 | 1.6969
Epoch 132/300, seasonal_1 Loss: 0.8684 | 1.6973
Epoch 133/300, seasonal_1 Loss: 0.8684 | 1.6975
Epoch 134/300, seasonal_1 Loss: 0.8684 | 1.6976
Epoch 135/300, seasonal_1 Loss: 0.8684 | 1.6977
Epoch 136/300, seasonal_1 Loss: 0.8684 | 1.6978
Epoch 137/300, seasonal_1 Loss: 0.8684 | 1.6978
Epoch 138/300, seasonal_1 Loss: 0.8680 | 1.6981
Epoch 139/300, seasonal_1 Loss: 0.8680 | 1.6984
Epoch 140/300, seasonal_1 Loss: 0.8680 | 1.6986
Epoch 141/300, seasonal_1 Loss: 0.8680 | 1.6987
Epoch 142/300, seasonal_1 Loss: 0.8680 | 1.6988
Epoch 143/300, seasonal_1 Loss: 0.8680 | 1.6988
Epoch 144/300, seasonal_1 Loss: 0.8677 | 1.6991
Epoch 145/300, seasonal_1 Loss: 0.8677 | 1.6993
Epoch 146/300, seasonal_1 Loss: 0.8677 | 1.6995
Epoch 147/300, seasonal_1 Loss: 0.8677 | 1.6996
Epoch 148/300, seasonal_1 Loss: 0.8677 | 1.6997
Epoch 149/300, seasonal_1 Loss: 0.8677 | 1.6997
Epoch 150/300, seasonal_1 Loss: 0.8676 | 1.6998
Epoch 151/300, seasonal_1 Loss: 0.8674 | 1.7000
Epoch 152/300, seasonal_1 Loss: 0.8673 | 1.7002
Epoch 153/300, seasonal_1 Loss: 0.8673 | 1.7004
Epoch 154/300, seasonal_1 Loss: 0.8673 | 1.7005
Epoch 155/300, seasonal_1 Loss: 0.8673 | 1.7005
Epoch 156/300, seasonal_1 Loss: 0.8673 | 1.7006
Epoch 157/300, seasonal_1 Loss: 0.8671 | 1.7008
Epoch 158/300, seasonal_1 Loss: 0.8671 | 1.7010
Epoch 159/300, seasonal_1 Loss: 0.8670 | 1.7011
Epoch 160/300, seasonal_1 Loss: 0.8670 | 1.7012
Epoch 161/300, seasonal_1 Loss: 0.8670 | 1.7013
Epoch 162/300, seasonal_1 Loss: 0.8670 | 1.7014
Epoch 163/300, seasonal_1 Loss: 0.8670 | 1.7014
Epoch 164/300, seasonal_1 Loss: 0.8668 | 1.7015
Epoch 165/300, seasonal_1 Loss: 0.8668 | 1.7017
Epoch 166/300, seasonal_1 Loss: 0.8668 | 1.7018
Epoch 167/300, seasonal_1 Loss: 0.8668 | 1.7019
Epoch 168/300, seasonal_1 Loss: 0.8668 | 1.7020
Epoch 169/300, seasonal_1 Loss: 0.8668 | 1.7020
Epoch 170/300, seasonal_1 Loss: 0.8665 | 1.7022
Epoch 171/300, seasonal_1 Loss: 0.8665 | 1.7023
Epoch 172/300, seasonal_1 Loss: 0.8665 | 1.7025
Epoch 173/300, seasonal_1 Loss: 0.8665 | 1.7026
Epoch 174/300, seasonal_1 Loss: 0.8665 | 1.7026
Epoch 175/300, seasonal_1 Loss: 0.8665 | 1.7027
Epoch 176/300, seasonal_1 Loss: 0.8665 | 1.7027
Epoch 177/300, seasonal_1 Loss: 0.8663 | 1.7028
Epoch 178/300, seasonal_1 Loss: 0.8663 | 1.7030
Epoch 179/300, seasonal_1 Loss: 0.8663 | 1.7031
Epoch 180/300, seasonal_1 Loss: 0.8663 | 1.7031
Epoch 181/300, seasonal_1 Loss: 0.8663 | 1.7032
Epoch 182/300, seasonal_1 Loss: 0.8663 | 1.7032
Epoch 183/300, seasonal_1 Loss: 0.8661 | 1.7034
Epoch 184/300, seasonal_1 Loss: 0.8661 | 1.7035
Epoch 185/300, seasonal_1 Loss: 0.8661 | 1.7036
Epoch 186/300, seasonal_1 Loss: 0.8661 | 1.7036
Epoch 187/300, seasonal_1 Loss: 0.8661 | 1.7037
Epoch 188/300, seasonal_1 Loss: 0.8661 | 1.7038
Epoch 189/300, seasonal_1 Loss: 0.8661 | 1.7038
Epoch 190/300, seasonal_1 Loss: 0.8660 | 1.7039
Epoch 191/300, seasonal_1 Loss: 0.8660 | 1.7040
Epoch 192/300, seasonal_1 Loss: 0.8660 | 1.7041
Epoch 193/300, seasonal_1 Loss: 0.8660 | 1.7041
Epoch 194/300, seasonal_1 Loss: 0.8659 | 1.7042
Epoch 195/300, seasonal_1 Loss: 0.8659 | 1.7042
Epoch 196/300, seasonal_1 Loss: 0.8658 | 1.7043
Epoch 197/300, seasonal_1 Loss: 0.8658 | 1.7044
Epoch 198/300, seasonal_1 Loss: 0.8658 | 1.7045
Epoch 199/300, seasonal_1 Loss: 0.8658 | 1.7045
Epoch 200/300, seasonal_1 Loss: 0.8658 | 1.7046
Epoch 201/300, seasonal_1 Loss: 0.8658 | 1.7046
Epoch 202/300, seasonal_1 Loss: 0.8658 | 1.7047
Epoch 203/300, seasonal_1 Loss: 0.8656 | 1.7048
Epoch 204/300, seasonal_1 Loss: 0.8656 | 1.7048
Epoch 205/300, seasonal_1 Loss: 0.8656 | 1.7049
Epoch 206/300, seasonal_1 Loss: 0.8656 | 1.7050
Epoch 207/300, seasonal_1 Loss: 0.8656 | 1.7050
Epoch 208/300, seasonal_1 Loss: 0.8656 | 1.7050
Epoch 209/300, seasonal_1 Loss: 0.8655 | 1.7051
Epoch 210/300, seasonal_1 Loss: 0.8655 | 1.7052
Epoch 211/300, seasonal_1 Loss: 0.8655 | 1.7052
Epoch 212/300, seasonal_1 Loss: 0.8655 | 1.7053
Epoch 213/300, seasonal_1 Loss: 0.8655 | 1.7053
Epoch 214/300, seasonal_1 Loss: 0.8655 | 1.7054
Epoch 215/300, seasonal_1 Loss: 0.8655 | 1.7054
Epoch 216/300, seasonal_1 Loss: 0.8654 | 1.7055
Epoch 217/300, seasonal_1 Loss: 0.8654 | 1.7055
Epoch 218/300, seasonal_1 Loss: 0.8654 | 1.7056
Epoch 219/300, seasonal_1 Loss: 0.8654 | 1.7056
Epoch 220/300, seasonal_1 Loss: 0.8654 | 1.7057
Epoch 221/300, seasonal_1 Loss: 0.8654 | 1.7057
Epoch 222/300, seasonal_1 Loss: 0.8653 | 1.7058
Epoch 223/300, seasonal_1 Loss: 0.8653 | 1.7058
Epoch 224/300, seasonal_1 Loss: 0.8653 | 1.7059
Epoch 225/300, seasonal_1 Loss: 0.8653 | 1.7059
Epoch 226/300, seasonal_1 Loss: 0.8653 | 1.7060
Epoch 227/300, seasonal_1 Loss: 0.8653 | 1.7060
Epoch 228/300, seasonal_1 Loss: 0.8653 | 1.7060
Epoch 229/300, seasonal_1 Loss: 0.8652 | 1.7061
Epoch 230/300, seasonal_1 Loss: 0.8652 | 1.7061
Epoch 231/300, seasonal_1 Loss: 0.8652 | 1.7062
Epoch 232/300, seasonal_1 Loss: 0.8652 | 1.7062
Epoch 233/300, seasonal_1 Loss: 0.8652 | 1.7062
Epoch 234/300, seasonal_1 Loss: 0.8652 | 1.7063
Epoch 235/300, seasonal_1 Loss: 0.8651 | 1.7063
Epoch 236/300, seasonal_1 Loss: 0.8651 | 1.7064
Epoch 237/300, seasonal_1 Loss: 0.8651 | 1.7064
Epoch 238/300, seasonal_1 Loss: 0.8651 | 1.7064
Epoch 239/300, seasonal_1 Loss: 0.8651 | 1.7065
Epoch 240/300, seasonal_1 Loss: 0.8651 | 1.7065
Epoch 241/300, seasonal_1 Loss: 0.8651 | 1.7065
Epoch 242/300, seasonal_1 Loss: 0.8650 | 1.7066
Epoch 243/300, seasonal_1 Loss: 0.8650 | 1.7066
Epoch 244/300, seasonal_1 Loss: 0.8650 | 1.7066
Epoch 245/300, seasonal_1 Loss: 0.8650 | 1.7067
Epoch 246/300, seasonal_1 Loss: 0.8650 | 1.7067
Epoch 247/300, seasonal_1 Loss: 0.8650 | 1.7067
Epoch 248/300, seasonal_1 Loss: 0.8649 | 1.7068
Epoch 249/300, seasonal_1 Loss: 0.8649 | 1.7068
Epoch 250/300, seasonal_1 Loss: 0.8649 | 1.7068
Epoch 251/300, seasonal_1 Loss: 0.8649 | 1.7069
Epoch 252/300, seasonal_1 Loss: 0.8649 | 1.7069
Epoch 253/300, seasonal_1 Loss: 0.8649 | 1.7069
Epoch 254/300, seasonal_1 Loss: 0.8649 | 1.7070
Epoch 255/300, seasonal_1 Loss: 0.8649 | 1.7070
Epoch 256/300, seasonal_1 Loss: 0.8649 | 1.7070
Epoch 257/300, seasonal_1 Loss: 0.8649 | 1.7070
Epoch 258/300, seasonal_1 Loss: 0.8649 | 1.7071
Epoch 259/300, seasonal_1 Loss: 0.8649 | 1.7071
Epoch 260/300, seasonal_1 Loss: 0.8649 | 1.7071
Epoch 261/300, seasonal_1 Loss: 0.8648 | 1.7072
Epoch 262/300, seasonal_1 Loss: 0.8648 | 1.7072
Epoch 263/300, seasonal_1 Loss: 0.8648 | 1.7072
Epoch 264/300, seasonal_1 Loss: 0.8648 | 1.7072
Epoch 265/300, seasonal_1 Loss: 0.8648 | 1.7073
Epoch 266/300, seasonal_1 Loss: 0.8648 | 1.7073
Epoch 267/300, seasonal_1 Loss: 0.8648 | 1.7073
Epoch 268/300, seasonal_1 Loss: 0.8648 | 1.7073
Epoch 269/300, seasonal_1 Loss: 0.8648 | 1.7074
Epoch 270/300, seasonal_1 Loss: 0.8648 | 1.7074
Epoch 271/300, seasonal_1 Loss: 0.8648 | 1.7074
Epoch 272/300, seasonal_1 Loss: 0.8648 | 1.7074
Epoch 273/300, seasonal_1 Loss: 0.8648 | 1.7074
Epoch 274/300, seasonal_1 Loss: 0.8647 | 1.7075
Epoch 275/300, seasonal_1 Loss: 0.8647 | 1.7075
Epoch 276/300, seasonal_1 Loss: 0.8647 | 1.7075
Epoch 277/300, seasonal_1 Loss: 0.8647 | 1.7075
Epoch 278/300, seasonal_1 Loss: 0.8647 | 1.7075
Epoch 279/300, seasonal_1 Loss: 0.8647 | 1.7076
Epoch 280/300, seasonal_1 Loss: 0.8647 | 1.7076
Epoch 281/300, seasonal_1 Loss: 0.8647 | 1.7076
Epoch 282/300, seasonal_1 Loss: 0.8647 | 1.7076
Epoch 283/300, seasonal_1 Loss: 0.8647 | 1.7076
Epoch 284/300, seasonal_1 Loss: 0.8647 | 1.7077
Epoch 285/300, seasonal_1 Loss: 0.8647 | 1.7077
Epoch 286/300, seasonal_1 Loss: 0.8647 | 1.7077
Epoch 287/300, seasonal_1 Loss: 0.8646 | 1.7077
Epoch 288/300, seasonal_1 Loss: 0.8646 | 1.7077
Epoch 289/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 290/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 291/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 292/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 293/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 294/300, seasonal_1 Loss: 0.8646 | 1.7078
Epoch 295/300, seasonal_1 Loss: 0.8646 | 1.7079
Epoch 296/300, seasonal_1 Loss: 0.8646 | 1.7079
Epoch 297/300, seasonal_1 Loss: 0.8646 | 1.7079
Epoch 298/300, seasonal_1 Loss: 0.8646 | 1.7079
Epoch 299/300, seasonal_1 Loss: 0.8646 | 1.7079
Epoch 300/300, seasonal_1 Loss: 0.8645 | 1.7079
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8457659737328437, 'learning_rate': 0.0006379327979549752, 'batch_size': 76, 'step_size': 15, 'gamma': 0.7850137987084901}
Epoch 1/300, seasonal_2 Loss: 1.3779 | 0.3512
Epoch 2/300, seasonal_2 Loss: 0.4336 | 0.4508
Epoch 3/300, seasonal_2 Loss: 0.3752 | 0.4019
Epoch 4/300, seasonal_2 Loss: 0.2724 | 0.1192
Epoch 5/300, seasonal_2 Loss: 0.1749 | 0.0811
Epoch 6/300, seasonal_2 Loss: 0.1585 | 0.0974
Epoch 7/300, seasonal_2 Loss: 0.1426 | 0.0799
Epoch 8/300, seasonal_2 Loss: 0.1446 | 0.0682
Epoch 9/300, seasonal_2 Loss: 0.1620 | 0.1916
Epoch 10/300, seasonal_2 Loss: 0.1483 | 0.1454
Epoch 11/300, seasonal_2 Loss: 0.1566 | 0.0837
Epoch 12/300, seasonal_2 Loss: 0.1481 | 0.0699
Epoch 13/300, seasonal_2 Loss: 0.1416 | 0.0743
Epoch 14/300, seasonal_2 Loss: 0.1227 | 0.0663
Epoch 15/300, seasonal_2 Loss: 0.1342 | 0.1218
Epoch 16/300, seasonal_2 Loss: 0.1376 | 0.0735
Epoch 17/300, seasonal_2 Loss: 0.1640 | 0.1158
Epoch 18/300, seasonal_2 Loss: 0.1589 | 0.0797
Epoch 19/300, seasonal_2 Loss: 0.1205 | 0.0949
Epoch 20/300, seasonal_2 Loss: 0.1188 | 0.0708
Epoch 21/300, seasonal_2 Loss: 0.1245 | 0.1099
Epoch 22/300, seasonal_2 Loss: 0.1292 | 0.0678
Epoch 23/300, seasonal_2 Loss: 0.1274 | 0.0480
Epoch 24/300, seasonal_2 Loss: 0.1170 | 0.0553
Epoch 25/300, seasonal_2 Loss: 0.1016 | 0.0440
Epoch 26/300, seasonal_2 Loss: 0.1062 | 0.0548
Epoch 27/300, seasonal_2 Loss: 0.1143 | 0.0716
Epoch 28/300, seasonal_2 Loss: 0.1217 | 0.0986
Epoch 29/300, seasonal_2 Loss: 0.1265 | 0.0642
Epoch 30/300, seasonal_2 Loss: 0.1254 | 0.0577
Epoch 31/300, seasonal_2 Loss: 0.1243 | 0.0426
Epoch 32/300, seasonal_2 Loss: 0.1176 | 0.0402
Epoch 33/300, seasonal_2 Loss: 0.1026 | 0.0402
Epoch 34/300, seasonal_2 Loss: 0.1046 | 0.0371
Epoch 35/300, seasonal_2 Loss: 0.1005 | 0.0448
Epoch 36/300, seasonal_2 Loss: 0.1107 | 0.0971
Epoch 37/300, seasonal_2 Loss: 0.1249 | 0.1230
Epoch 38/300, seasonal_2 Loss: 0.1146 | 0.0695
Epoch 39/300, seasonal_2 Loss: 0.1174 | 0.0472
Epoch 40/300, seasonal_2 Loss: 0.1089 | 0.0435
Epoch 41/300, seasonal_2 Loss: 0.1188 | 0.0726
Epoch 42/300, seasonal_2 Loss: 0.1083 | 0.0516
Epoch 43/300, seasonal_2 Loss: 0.0970 | 0.0694
Epoch 44/300, seasonal_2 Loss: 0.0920 | 0.0515
Epoch 45/300, seasonal_2 Loss: 0.0960 | 0.0549
Epoch 46/300, seasonal_2 Loss: 0.0935 | 0.0507
Epoch 47/300, seasonal_2 Loss: 0.0963 | 0.0655
Epoch 48/300, seasonal_2 Loss: 0.0925 | 0.0591
Epoch 49/300, seasonal_2 Loss: 0.0943 | 0.0584
Epoch 50/300, seasonal_2 Loss: 0.0878 | 0.0521
Epoch 51/300, seasonal_2 Loss: 0.0874 | 0.0453
Epoch 52/300, seasonal_2 Loss: 0.0888 | 0.0509
Epoch 53/300, seasonal_2 Loss: 0.0862 | 0.0561
Epoch 54/300, seasonal_2 Loss: 0.0855 | 0.0417
Epoch 55/300, seasonal_2 Loss: 0.0860 | 0.0374
Epoch 56/300, seasonal_2 Loss: 0.0770 | 0.0362
Epoch 57/300, seasonal_2 Loss: 0.0777 | 0.0456
Epoch 58/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 59/300, seasonal_2 Loss: 0.0784 | 0.0383
Epoch 60/300, seasonal_2 Loss: 0.0837 | 0.0397
Epoch 61/300, seasonal_2 Loss: 0.0800 | 0.0446
Epoch 62/300, seasonal_2 Loss: 0.0805 | 0.0552
Epoch 63/300, seasonal_2 Loss: 0.0766 | 0.0851
Epoch 64/300, seasonal_2 Loss: 0.0763 | 0.0567
Epoch 65/300, seasonal_2 Loss: 0.0744 | 0.0588
Epoch 66/300, seasonal_2 Loss: 0.0714 | 0.0464
Epoch 67/300, seasonal_2 Loss: 0.0701 | 0.0459
Epoch 68/300, seasonal_2 Loss: 0.0665 | 0.0418
Epoch 69/300, seasonal_2 Loss: 0.0661 | 0.0477
Epoch 70/300, seasonal_2 Loss: 0.0647 | 0.0484
Epoch 71/300, seasonal_2 Loss: 0.0622 | 0.0411
Epoch 72/300, seasonal_2 Loss: 0.0628 | 0.0414
Epoch 73/300, seasonal_2 Loss: 0.0660 | 0.0440
Epoch 74/300, seasonal_2 Loss: 0.0617 | 0.0580
Epoch 75/300, seasonal_2 Loss: 0.0656 | 0.0345
Epoch 76/300, seasonal_2 Loss: 0.0624 | 0.0356
Epoch 77/300, seasonal_2 Loss: 0.0608 | 0.0352
Epoch 78/300, seasonal_2 Loss: 0.0599 | 0.0351
Epoch 79/300, seasonal_2 Loss: 0.0597 | 0.0332
Epoch 80/300, seasonal_2 Loss: 0.0594 | 0.0331
Epoch 81/300, seasonal_2 Loss: 0.0595 | 0.0322
Epoch 82/300, seasonal_2 Loss: 0.0593 | 0.0326
Epoch 83/300, seasonal_2 Loss: 0.0599 | 0.0326
Epoch 84/300, seasonal_2 Loss: 0.0590 | 0.0370
Epoch 85/300, seasonal_2 Loss: 0.0597 | 0.0385
Epoch 86/300, seasonal_2 Loss: 0.0589 | 0.0393
Epoch 87/300, seasonal_2 Loss: 0.0588 | 0.0362
Epoch 88/300, seasonal_2 Loss: 0.0589 | 0.0337
Epoch 89/300, seasonal_2 Loss: 0.0592 | 0.0328
Epoch 90/300, seasonal_2 Loss: 0.0590 | 0.0324
Epoch 91/300, seasonal_2 Loss: 0.0584 | 0.0340
Epoch 92/300, seasonal_2 Loss: 0.0579 | 0.0349
Epoch 93/300, seasonal_2 Loss: 0.0574 | 0.0358
Epoch 94/300, seasonal_2 Loss: 0.0574 | 0.0362
Epoch 95/300, seasonal_2 Loss: 0.0568 | 0.0358
Epoch 96/300, seasonal_2 Loss: 0.0564 | 0.0352
Epoch 97/300, seasonal_2 Loss: 0.0563 | 0.0340
Epoch 98/300, seasonal_2 Loss: 0.0565 | 0.0339
Epoch 99/300, seasonal_2 Loss: 0.0583 | 0.0392
Epoch 100/300, seasonal_2 Loss: 0.0599 | 0.0378
Epoch 101/300, seasonal_2 Loss: 0.0577 | 0.0383
Epoch 102/300, seasonal_2 Loss: 0.0542 | 0.0368
Epoch 103/300, seasonal_2 Loss: 0.0511 | 0.0357
Epoch 104/300, seasonal_2 Loss: 0.0492 | 0.0346
Epoch 105/300, seasonal_2 Loss: 0.0483 | 0.0349
Epoch 106/300, seasonal_2 Loss: 0.0478 | 0.0352
Epoch 107/300, seasonal_2 Loss: 0.0472 | 0.0349
Epoch 108/300, seasonal_2 Loss: 0.0467 | 0.0343
Epoch 109/300, seasonal_2 Loss: 0.0464 | 0.0346
Epoch 110/300, seasonal_2 Loss: 0.0462 | 0.0346
Epoch 111/300, seasonal_2 Loss: 0.0461 | 0.0347
Epoch 112/300, seasonal_2 Loss: 0.0459 | 0.0344
Epoch 113/300, seasonal_2 Loss: 0.0458 | 0.0344
Epoch 114/300, seasonal_2 Loss: 0.0456 | 0.0342
Epoch 115/300, seasonal_2 Loss: 0.0455 | 0.0346
Epoch 116/300, seasonal_2 Loss: 0.0456 | 0.0346
Epoch 117/300, seasonal_2 Loss: 0.0454 | 0.0343
Epoch 118/300, seasonal_2 Loss: 0.0454 | 0.0338
Epoch 119/300, seasonal_2 Loss: 0.0453 | 0.0339
Epoch 120/300, seasonal_2 Loss: 0.0452 | 0.0343
Epoch 121/300, seasonal_2 Loss: 0.0454 | 0.0348
Epoch 122/300, seasonal_2 Loss: 0.0452 | 0.0339
Epoch 123/300, seasonal_2 Loss: 0.0452 | 0.0336
Epoch 124/300, seasonal_2 Loss: 0.0450 | 0.0339
Epoch 125/300, seasonal_2 Loss: 0.0448 | 0.0343
Epoch 126/300, seasonal_2 Loss: 0.0447 | 0.0342
Epoch 127/300, seasonal_2 Loss: 0.0446 | 0.0338
Epoch 128/300, seasonal_2 Loss: 0.0446 | 0.0336
Epoch 129/300, seasonal_2 Loss: 0.0444 | 0.0343
Epoch 130/300, seasonal_2 Loss: 0.0444 | 0.0343
Epoch 131/300, seasonal_2 Loss: 0.0444 | 0.0337
Epoch 132/300, seasonal_2 Loss: 0.0443 | 0.0336
Epoch 133/300, seasonal_2 Loss: 0.0443 | 0.0337
Epoch 134/300, seasonal_2 Loss: 0.0440 | 0.0341
Epoch 135/300, seasonal_2 Loss: 0.0440 | 0.0342
Epoch 136/300, seasonal_2 Loss: 0.0440 | 0.0334
Epoch 137/300, seasonal_2 Loss: 0.0438 | 0.0334
Epoch 138/300, seasonal_2 Loss: 0.0436 | 0.0339
Epoch 139/300, seasonal_2 Loss: 0.0436 | 0.0340
Epoch 140/300, seasonal_2 Loss: 0.0436 | 0.0335
Epoch 141/300, seasonal_2 Loss: 0.0435 | 0.0334
Epoch 142/300, seasonal_2 Loss: 0.0434 | 0.0338
Epoch 143/300, seasonal_2 Loss: 0.0433 | 0.0341
Epoch 144/300, seasonal_2 Loss: 0.0434 | 0.0338
Epoch 145/300, seasonal_2 Loss: 0.0432 | 0.0333
Epoch 146/300, seasonal_2 Loss: 0.0431 | 0.0337
Epoch 147/300, seasonal_2 Loss: 0.0431 | 0.0341
Epoch 148/300, seasonal_2 Loss: 0.0431 | 0.0337
Epoch 149/300, seasonal_2 Loss: 0.0429 | 0.0334
Epoch 150/300, seasonal_2 Loss: 0.0429 | 0.0337
Epoch 151/300, seasonal_2 Loss: 0.0429 | 0.0342
Epoch 152/300, seasonal_2 Loss: 0.0428 | 0.0339
Epoch 153/300, seasonal_2 Loss: 0.0427 | 0.0336
Epoch 154/300, seasonal_2 Loss: 0.0427 | 0.0338
Epoch 155/300, seasonal_2 Loss: 0.0426 | 0.0339
Epoch 156/300, seasonal_2 Loss: 0.0426 | 0.0338
Epoch 157/300, seasonal_2 Loss: 0.0425 | 0.0337
Epoch 158/300, seasonal_2 Loss: 0.0425 | 0.0339
Epoch 159/300, seasonal_2 Loss: 0.0424 | 0.0342
Epoch 160/300, seasonal_2 Loss: 0.0424 | 0.0340
Epoch 161/300, seasonal_2 Loss: 0.0423 | 0.0339
Epoch 162/300, seasonal_2 Loss: 0.0423 | 0.0339
Epoch 163/300, seasonal_2 Loss: 0.0423 | 0.0340
Epoch 164/300, seasonal_2 Loss: 0.0422 | 0.0340
Epoch 165/300, seasonal_2 Loss: 0.0421 | 0.0340
Epoch 166/300, seasonal_2 Loss: 0.0421 | 0.0341
Epoch 167/300, seasonal_2 Loss: 0.0421 | 0.0342
Epoch 168/300, seasonal_2 Loss: 0.0420 | 0.0341
Epoch 169/300, seasonal_2 Loss: 0.0420 | 0.0341
Epoch 170/300, seasonal_2 Loss: 0.0420 | 0.0341
Epoch 171/300, seasonal_2 Loss: 0.0419 | 0.0341
Epoch 172/300, seasonal_2 Loss: 0.0419 | 0.0342
Epoch 173/300, seasonal_2 Loss: 0.0418 | 0.0342
Epoch 174/300, seasonal_2 Loss: 0.0418 | 0.0343
Epoch 175/300, seasonal_2 Loss: 0.0418 | 0.0343
Epoch 176/300, seasonal_2 Loss: 0.0417 | 0.0343
Epoch 177/300, seasonal_2 Loss: 0.0417 | 0.0343
Epoch 178/300, seasonal_2 Loss: 0.0417 | 0.0343
Epoch 179/300, seasonal_2 Loss: 0.0416 | 0.0343
Epoch 180/300, seasonal_2 Loss: 0.0416 | 0.0344
Epoch 181/300, seasonal_2 Loss: 0.0416 | 0.0344
Epoch 182/300, seasonal_2 Loss: 0.0415 | 0.0344
Epoch 183/300, seasonal_2 Loss: 0.0415 | 0.0344
Epoch 184/300, seasonal_2 Loss: 0.0415 | 0.0344
Epoch 185/300, seasonal_2 Loss: 0.0415 | 0.0344
Epoch 186/300, seasonal_2 Loss: 0.0414 | 0.0345
Epoch 187/300, seasonal_2 Loss: 0.0414 | 0.0345
Epoch 188/300, seasonal_2 Loss: 0.0414 | 0.0345
Epoch 189/300, seasonal_2 Loss: 0.0413 | 0.0345
Epoch 190/300, seasonal_2 Loss: 0.0413 | 0.0345
Epoch 191/300, seasonal_2 Loss: 0.0413 | 0.0346
Epoch 192/300, seasonal_2 Loss: 0.0413 | 0.0346
Epoch 193/300, seasonal_2 Loss: 0.0413 | 0.0346
Epoch 194/300, seasonal_2 Loss: 0.0412 | 0.0346
Epoch 195/300, seasonal_2 Loss: 0.0412 | 0.0346
Epoch 196/300, seasonal_2 Loss: 0.0412 | 0.0346
Epoch 197/300, seasonal_2 Loss: 0.0412 | 0.0347
Epoch 198/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 199/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 200/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 201/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 202/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 203/300, seasonal_2 Loss: 0.0411 | 0.0347
Epoch 204/300, seasonal_2 Loss: 0.0410 | 0.0348
Epoch 205/300, seasonal_2 Loss: 0.0410 | 0.0348
Epoch 206/300, seasonal_2 Loss: 0.0410 | 0.0348
Epoch 207/300, seasonal_2 Loss: 0.0410 | 0.0348
Epoch 208/300, seasonal_2 Loss: 0.0410 | 0.0348
Epoch 209/300, seasonal_2 Loss: 0.0409 | 0.0348
Epoch 210/300, seasonal_2 Loss: 0.0409 | 0.0348
Epoch 211/300, seasonal_2 Loss: 0.0409 | 0.0349
Epoch 212/300, seasonal_2 Loss: 0.0409 | 0.0349
Epoch 213/300, seasonal_2 Loss: 0.0409 | 0.0349
Epoch 214/300, seasonal_2 Loss: 0.0409 | 0.0349
Epoch 215/300, seasonal_2 Loss: 0.0409 | 0.0349
Epoch 216/300, seasonal_2 Loss: 0.0408 | 0.0349
Epoch 217/300, seasonal_2 Loss: 0.0408 | 0.0349
Epoch 218/300, seasonal_2 Loss: 0.0408 | 0.0349
Epoch 219/300, seasonal_2 Loss: 0.0408 | 0.0349
Epoch 220/300, seasonal_2 Loss: 0.0408 | 0.0350
Epoch 221/300, seasonal_2 Loss: 0.0408 | 0.0350
Epoch 222/300, seasonal_2 Loss: 0.0408 | 0.0350
Epoch 223/300, seasonal_2 Loss: 0.0408 | 0.0350
Epoch 224/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 225/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 226/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 227/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 228/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 229/300, seasonal_2 Loss: 0.0407 | 0.0350
Epoch 230/300, seasonal_2 Loss: 0.0407 | 0.0351
Epoch 231/300, seasonal_2 Loss: 0.0407 | 0.0351
Epoch 232/300, seasonal_2 Loss: 0.0407 | 0.0351
Epoch 233/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 234/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 235/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 236/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 237/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 238/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 239/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 240/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 241/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 242/300, seasonal_2 Loss: 0.0406 | 0.0351
Epoch 243/300, seasonal_2 Loss: 0.0406 | 0.0352
Epoch 244/300, seasonal_2 Loss: 0.0406 | 0.0352
Epoch 245/300, seasonal_2 Loss: 0.0406 | 0.0352
Epoch 246/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 247/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 248/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 249/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 250/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 251/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 252/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 253/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 254/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 255/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 256/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 257/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 258/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 259/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 260/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 261/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 262/300, seasonal_2 Loss: 0.0405 | 0.0352
Epoch 263/300, seasonal_2 Loss: 0.0405 | 0.0353
Epoch 264/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 265/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 266/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 267/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 268/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 269/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 270/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 271/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 272/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 273/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 274/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 275/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 276/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 277/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 278/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 279/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 280/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 281/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 282/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 283/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 284/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 285/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 286/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 287/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 288/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 289/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 290/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 291/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 292/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 293/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 294/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 295/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 296/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 297/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 298/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 299/300, seasonal_2 Loss: 0.0404 | 0.0353
Epoch 300/300, seasonal_2 Loss: 0.0404 | 0.0353
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.9156974077105391, 'learning_rate': 0.0001973599645845138, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9599849094922285}
Epoch 1/300, seasonal_3 Loss: 0.4362 | 0.0795
Epoch 2/300, seasonal_3 Loss: 0.1451 | 0.0747
Epoch 3/300, seasonal_3 Loss: 0.1402 | 0.0731
Epoch 4/300, seasonal_3 Loss: 0.1314 | 0.0582
Epoch 5/300, seasonal_3 Loss: 0.1226 | 0.0773
Epoch 6/300, seasonal_3 Loss: 0.1238 | 0.0752
Epoch 7/300, seasonal_3 Loss: 0.1199 | 0.0756
Epoch 8/300, seasonal_3 Loss: 0.1301 | 0.0873
Epoch 9/300, seasonal_3 Loss: 0.1211 | 0.0569
Epoch 10/300, seasonal_3 Loss: 0.1097 | 0.0882
Epoch 11/300, seasonal_3 Loss: 0.1048 | 0.0641
Epoch 12/300, seasonal_3 Loss: 0.0965 | 0.0760
Epoch 13/300, seasonal_3 Loss: 0.0868 | 0.0616
Epoch 14/300, seasonal_3 Loss: 0.0807 | 0.0452
Epoch 15/300, seasonal_3 Loss: 0.0782 | 0.0601
Epoch 16/300, seasonal_3 Loss: 0.0774 | 0.0569
Epoch 17/300, seasonal_3 Loss: 0.0733 | 0.0461
Epoch 18/300, seasonal_3 Loss: 0.0733 | 0.0541
Epoch 19/300, seasonal_3 Loss: 0.0684 | 0.0659
Epoch 20/300, seasonal_3 Loss: 0.0665 | 0.0455
Epoch 21/300, seasonal_3 Loss: 0.0619 | 0.0510
Epoch 22/300, seasonal_3 Loss: 0.0574 | 0.0422
Epoch 23/300, seasonal_3 Loss: 0.0567 | 0.0362
Epoch 24/300, seasonal_3 Loss: 0.0567 | 0.0361
Epoch 25/300, seasonal_3 Loss: 0.0565 | 0.0341
Epoch 26/300, seasonal_3 Loss: 0.0545 | 0.0394
Epoch 27/300, seasonal_3 Loss: 0.0532 | 0.0392
Epoch 28/300, seasonal_3 Loss: 0.0525 | 0.0388
Epoch 29/300, seasonal_3 Loss: 0.0507 | 0.0378
Epoch 30/300, seasonal_3 Loss: 0.0479 | 0.0417
Epoch 31/300, seasonal_3 Loss: 0.0520 | 0.0416
Epoch 32/300, seasonal_3 Loss: 0.0515 | 0.0341
Epoch 33/300, seasonal_3 Loss: 0.0485 | 0.0395
Epoch 34/300, seasonal_3 Loss: 0.0478 | 0.0390
Epoch 35/300, seasonal_3 Loss: 0.0462 | 0.0497
Epoch 36/300, seasonal_3 Loss: 0.0530 | 0.0367
Epoch 37/300, seasonal_3 Loss: 0.0516 | 0.0403
Epoch 38/300, seasonal_3 Loss: 0.0482 | 0.0359
Epoch 39/300, seasonal_3 Loss: 0.0466 | 0.0352
Epoch 40/300, seasonal_3 Loss: 0.0449 | 0.0342
Epoch 41/300, seasonal_3 Loss: 0.0461 | 0.0347
Epoch 42/300, seasonal_3 Loss: 0.0422 | 0.0407
Epoch 43/300, seasonal_3 Loss: 0.0454 | 0.0470
Epoch 44/300, seasonal_3 Loss: 0.0417 | 0.0490
Epoch 45/300, seasonal_3 Loss: 0.0380 | 0.0455
Epoch 46/300, seasonal_3 Loss: 0.0368 | 0.0480
Epoch 47/300, seasonal_3 Loss: 0.0355 | 0.0446
Epoch 48/300, seasonal_3 Loss: 0.0370 | 0.0472
Epoch 49/300, seasonal_3 Loss: 0.0620 | 0.0984
Epoch 50/300, seasonal_3 Loss: 0.0805 | 0.0440
Epoch 51/300, seasonal_3 Loss: 0.0562 | 0.0383
Epoch 52/300, seasonal_3 Loss: 0.0468 | 0.0362
Epoch 53/300, seasonal_3 Loss: 0.0451 | 0.0356
Epoch 54/300, seasonal_3 Loss: 0.0436 | 0.0368
Epoch 55/300, seasonal_3 Loss: 0.0385 | 0.0347
Epoch 56/300, seasonal_3 Loss: 0.0330 | 0.0302
Epoch 57/300, seasonal_3 Loss: 0.0323 | 0.0317
Epoch 58/300, seasonal_3 Loss: 0.0303 | 0.0310
Epoch 59/300, seasonal_3 Loss: 0.0301 | 0.0336
Epoch 60/300, seasonal_3 Loss: 0.0302 | 0.0338
Epoch 61/300, seasonal_3 Loss: 0.0303 | 0.0388
Epoch 62/300, seasonal_3 Loss: 0.0298 | 0.0410
Epoch 63/300, seasonal_3 Loss: 0.0294 | 0.0434
Epoch 64/300, seasonal_3 Loss: 0.0302 | 0.0458
Epoch 65/300, seasonal_3 Loss: 0.0393 | 0.0404
Epoch 66/300, seasonal_3 Loss: 0.0381 | 0.0439
Epoch 67/300, seasonal_3 Loss: 0.0316 | 0.0385
Epoch 68/300, seasonal_3 Loss: 0.0307 | 0.0373
Epoch 69/300, seasonal_3 Loss: 0.0314 | 0.0359
Epoch 70/300, seasonal_3 Loss: 0.0326 | 0.0355
Epoch 71/300, seasonal_3 Loss: 0.0343 | 0.0368
Epoch 72/300, seasonal_3 Loss: 0.0346 | 0.0392
Epoch 73/300, seasonal_3 Loss: 0.0335 | 0.0386
Epoch 74/300, seasonal_3 Loss: 0.0315 | 0.0434
Epoch 75/300, seasonal_3 Loss: 0.0306 | 0.0330
Epoch 76/300, seasonal_3 Loss: 0.0290 | 0.0319
Epoch 77/300, seasonal_3 Loss: 0.0291 | 0.0318
Epoch 78/300, seasonal_3 Loss: 0.0276 | 0.0309
Epoch 79/300, seasonal_3 Loss: 0.0271 | 0.0316
Epoch 80/300, seasonal_3 Loss: 0.0268 | 0.0317
Epoch 81/300, seasonal_3 Loss: 0.0267 | 0.0328
Epoch 82/300, seasonal_3 Loss: 0.0265 | 0.0327
Epoch 83/300, seasonal_3 Loss: 0.0266 | 0.0333
Epoch 84/300, seasonal_3 Loss: 0.0270 | 0.0344
Epoch 85/300, seasonal_3 Loss: 0.0271 | 0.0334
Epoch 86/300, seasonal_3 Loss: 0.0273 | 0.0316
Epoch 87/300, seasonal_3 Loss: 0.0282 | 0.0373
Epoch 88/300, seasonal_3 Loss: 0.0302 | 0.0371
Epoch 89/300, seasonal_3 Loss: 0.0292 | 0.0395
Epoch 90/300, seasonal_3 Loss: 0.0291 | 0.0409
Epoch 91/300, seasonal_3 Loss: 0.0307 | 0.0436
Epoch 92/300, seasonal_3 Loss: 0.0299 | 0.0460
Epoch 93/300, seasonal_3 Loss: 0.0312 | 0.0401
Epoch 94/300, seasonal_3 Loss: 0.0287 | 0.0354
Epoch 95/300, seasonal_3 Loss: 0.0284 | 0.0333
Epoch 96/300, seasonal_3 Loss: 0.0288 | 0.0345
Epoch 97/300, seasonal_3 Loss: 0.0286 | 0.0359
Epoch 98/300, seasonal_3 Loss: 0.0289 | 0.0350
Epoch 99/300, seasonal_3 Loss: 0.0291 | 0.0359
Epoch 100/300, seasonal_3 Loss: 0.0292 | 0.0336
Epoch 101/300, seasonal_3 Loss: 0.0290 | 0.0357
Epoch 102/300, seasonal_3 Loss: 0.0285 | 0.0355
Epoch 103/300, seasonal_3 Loss: 0.0276 | 0.0357
Epoch 104/300, seasonal_3 Loss: 0.0274 | 0.0351
Epoch 105/300, seasonal_3 Loss: 0.0271 | 0.0341
Epoch 106/300, seasonal_3 Loss: 0.0268 | 0.0340
Epoch 107/300, seasonal_3 Loss: 0.0263 | 0.0328
Epoch 108/300, seasonal_3 Loss: 0.0256 | 0.0338
Epoch 109/300, seasonal_3 Loss: 0.0254 | 0.0343
Epoch 110/300, seasonal_3 Loss: 0.0252 | 0.0341
Epoch 111/300, seasonal_3 Loss: 0.0249 | 0.0349
Epoch 112/300, seasonal_3 Loss: 0.0249 | 0.0344
Epoch 113/300, seasonal_3 Loss: 0.0252 | 0.0340
Epoch 114/300, seasonal_3 Loss: 0.0249 | 0.0347
Epoch 115/300, seasonal_3 Loss: 0.0252 | 0.0326
Epoch 116/300, seasonal_3 Loss: 0.0249 | 0.0335
Epoch 117/300, seasonal_3 Loss: 0.0249 | 0.0354
Epoch 118/300, seasonal_3 Loss: 0.0250 | 0.0357
Epoch 119/300, seasonal_3 Loss: 0.0259 | 0.0381
Epoch 120/300, seasonal_3 Loss: 0.0264 | 0.0367
Epoch 121/300, seasonal_3 Loss: 0.0265 | 0.0409
Epoch 122/300, seasonal_3 Loss: 0.0265 | 0.0389
Epoch 123/300, seasonal_3 Loss: 0.0252 | 0.0316
Epoch 124/300, seasonal_3 Loss: 0.0248 | 0.0320
Epoch 125/300, seasonal_3 Loss: 0.0243 | 0.0329
Epoch 126/300, seasonal_3 Loss: 0.0239 | 0.0323
Epoch 127/300, seasonal_3 Loss: 0.0239 | 0.0323
Epoch 128/300, seasonal_3 Loss: 0.0238 | 0.0337
Epoch 129/300, seasonal_3 Loss: 0.0237 | 0.0339
Epoch 130/300, seasonal_3 Loss: 0.0235 | 0.0345
Epoch 131/300, seasonal_3 Loss: 0.0234 | 0.0343
Epoch 132/300, seasonal_3 Loss: 0.0234 | 0.0339
Epoch 133/300, seasonal_3 Loss: 0.0234 | 0.0332
Epoch 134/300, seasonal_3 Loss: 0.0235 | 0.0332
Epoch 135/300, seasonal_3 Loss: 0.0236 | 0.0336
Epoch 136/300, seasonal_3 Loss: 0.0235 | 0.0340
Epoch 137/300, seasonal_3 Loss: 0.0235 | 0.0340
Epoch 138/300, seasonal_3 Loss: 0.0234 | 0.0331
Epoch 139/300, seasonal_3 Loss: 0.0234 | 0.0330
Epoch 140/300, seasonal_3 Loss: 0.0233 | 0.0338
Epoch 141/300, seasonal_3 Loss: 0.0239 | 0.0351
Epoch 142/300, seasonal_3 Loss: 0.0240 | 0.0366
Epoch 143/300, seasonal_3 Loss: 0.0240 | 0.0388
Epoch 144/300, seasonal_3 Loss: 0.0241 | 0.0402
Epoch 145/300, seasonal_3 Loss: 0.0241 | 0.0405
Epoch 146/300, seasonal_3 Loss: 0.0241 | 0.0412
Epoch 147/300, seasonal_3 Loss: 0.0243 | 0.0493
Epoch 148/300, seasonal_3 Loss: 0.0240 | 0.0489
Epoch 149/300, seasonal_3 Loss: 0.0241 | 0.0461
Epoch 150/300, seasonal_3 Loss: 0.0243 | 0.0461
Epoch 151/300, seasonal_3 Loss: 0.0241 | 0.0456
Epoch 152/300, seasonal_3 Loss: 0.0237 | 0.0431
Epoch 153/300, seasonal_3 Loss: 0.0230 | 0.0384
Epoch 154/300, seasonal_3 Loss: 0.0230 | 0.0395
Epoch 155/300, seasonal_3 Loss: 0.0230 | 0.0374
Epoch 156/300, seasonal_3 Loss: 0.0228 | 0.0367
Epoch 157/300, seasonal_3 Loss: 0.0226 | 0.0362
Epoch 158/300, seasonal_3 Loss: 0.0225 | 0.0372
Epoch 159/300, seasonal_3 Loss: 0.0223 | 0.0381
Epoch 160/300, seasonal_3 Loss: 0.0222 | 0.0385
Epoch 161/300, seasonal_3 Loss: 0.0222 | 0.0383
Epoch 162/300, seasonal_3 Loss: 0.0224 | 0.0380
Epoch 163/300, seasonal_3 Loss: 0.0224 | 0.0377
Epoch 164/300, seasonal_3 Loss: 0.0222 | 0.0375
Epoch 165/300, seasonal_3 Loss: 0.0221 | 0.0372
Epoch 166/300, seasonal_3 Loss: 0.0220 | 0.0368
Epoch 167/300, seasonal_3 Loss: 0.0220 | 0.0364
Epoch 168/300, seasonal_3 Loss: 0.0221 | 0.0366
Epoch 169/300, seasonal_3 Loss: 0.0220 | 0.0368
Epoch 170/300, seasonal_3 Loss: 0.0219 | 0.0373
Epoch 171/300, seasonal_3 Loss: 0.0219 | 0.0380
Epoch 172/300, seasonal_3 Loss: 0.0219 | 0.0385
Epoch 173/300, seasonal_3 Loss: 0.0220 | 0.0375
Epoch 174/300, seasonal_3 Loss: 0.0220 | 0.0359
Epoch 175/300, seasonal_3 Loss: 0.0219 | 0.0356
Epoch 176/300, seasonal_3 Loss: 0.0219 | 0.0362
Epoch 177/300, seasonal_3 Loss: 0.0219 | 0.0380
Epoch 178/300, seasonal_3 Loss: 0.0218 | 0.0391
Epoch 179/300, seasonal_3 Loss: 0.0218 | 0.0406
Epoch 180/300, seasonal_3 Loss: 0.0218 | 0.0417
Epoch 181/300, seasonal_3 Loss: 0.0218 | 0.0430
Epoch 182/300, seasonal_3 Loss: 0.0217 | 0.0417
Epoch 183/300, seasonal_3 Loss: 0.0217 | 0.0396
Epoch 184/300, seasonal_3 Loss: 0.0217 | 0.0391
Epoch 185/300, seasonal_3 Loss: 0.0216 | 0.0394
Epoch 186/300, seasonal_3 Loss: 0.0217 | 0.0389
Epoch 187/300, seasonal_3 Loss: 0.0217 | 0.0394
Epoch 188/300, seasonal_3 Loss: 0.0216 | 0.0392
Epoch 189/300, seasonal_3 Loss: 0.0216 | 0.0391
Epoch 190/300, seasonal_3 Loss: 0.0216 | 0.0401
Epoch 191/300, seasonal_3 Loss: 0.0217 | 0.0411
Epoch 192/300, seasonal_3 Loss: 0.0217 | 0.0412
Epoch 193/300, seasonal_3 Loss: 0.0216 | 0.0402
Epoch 194/300, seasonal_3 Loss: 0.0215 | 0.0386
Epoch 195/300, seasonal_3 Loss: 0.0214 | 0.0377
Epoch 196/300, seasonal_3 Loss: 0.0214 | 0.0382
Epoch 197/300, seasonal_3 Loss: 0.0214 | 0.0386
Epoch 198/300, seasonal_3 Loss: 0.0213 | 0.0388
Epoch 199/300, seasonal_3 Loss: 0.0213 | 0.0389
Epoch 200/300, seasonal_3 Loss: 0.0213 | 0.0388
Epoch 201/300, seasonal_3 Loss: 0.0213 | 0.0385
Epoch 202/300, seasonal_3 Loss: 0.0213 | 0.0379
Epoch 203/300, seasonal_3 Loss: 0.0212 | 0.0371
Epoch 204/300, seasonal_3 Loss: 0.0212 | 0.0377
Epoch 205/300, seasonal_3 Loss: 0.0212 | 0.0381
Epoch 206/300, seasonal_3 Loss: 0.0212 | 0.0388
Epoch 207/300, seasonal_3 Loss: 0.0212 | 0.0397
Epoch 208/300, seasonal_3 Loss: 0.0212 | 0.0399
Epoch 209/300, seasonal_3 Loss: 0.0212 | 0.0394
Epoch 210/300, seasonal_3 Loss: 0.0212 | 0.0387
Epoch 211/300, seasonal_3 Loss: 0.0212 | 0.0377
Epoch 212/300, seasonal_3 Loss: 0.0212 | 0.0380
Epoch 213/300, seasonal_3 Loss: 0.0211 | 0.0384
Epoch 214/300, seasonal_3 Loss: 0.0211 | 0.0388
Epoch 215/300, seasonal_3 Loss: 0.0211 | 0.0392
Epoch 216/300, seasonal_3 Loss: 0.0211 | 0.0393
Epoch 217/300, seasonal_3 Loss: 0.0211 | 0.0391
Epoch 218/300, seasonal_3 Loss: 0.0210 | 0.0386
Epoch 219/300, seasonal_3 Loss: 0.0210 | 0.0382
Epoch 220/300, seasonal_3 Loss: 0.0210 | 0.0383
Epoch 221/300, seasonal_3 Loss: 0.0210 | 0.0386
Epoch 222/300, seasonal_3 Loss: 0.0209 | 0.0389
Epoch 223/300, seasonal_3 Loss: 0.0210 | 0.0391
Epoch 224/300, seasonal_3 Loss: 0.0210 | 0.0390
Epoch 225/300, seasonal_3 Loss: 0.0209 | 0.0386
Epoch 226/300, seasonal_3 Loss: 0.0209 | 0.0382
Epoch 227/300, seasonal_3 Loss: 0.0209 | 0.0381
Epoch 228/300, seasonal_3 Loss: 0.0209 | 0.0381
Epoch 229/300, seasonal_3 Loss: 0.0209 | 0.0384
Epoch 230/300, seasonal_3 Loss: 0.0209 | 0.0386
Epoch 231/300, seasonal_3 Loss: 0.0209 | 0.0387
Epoch 232/300, seasonal_3 Loss: 0.0209 | 0.0386
Epoch 233/300, seasonal_3 Loss: 0.0208 | 0.0382
Epoch 234/300, seasonal_3 Loss: 0.0208 | 0.0380
Epoch 235/300, seasonal_3 Loss: 0.0208 | 0.0380
Epoch 236/300, seasonal_3 Loss: 0.0208 | 0.0381
Epoch 237/300, seasonal_3 Loss: 0.0208 | 0.0384
Epoch 238/300, seasonal_3 Loss: 0.0208 | 0.0386
Epoch 239/300, seasonal_3 Loss: 0.0208 | 0.0386
Epoch 240/300, seasonal_3 Loss: 0.0208 | 0.0384
Epoch 241/300, seasonal_3 Loss: 0.0208 | 0.0382
Epoch 242/300, seasonal_3 Loss: 0.0208 | 0.0380
Epoch 243/300, seasonal_3 Loss: 0.0208 | 0.0381
Epoch 244/300, seasonal_3 Loss: 0.0208 | 0.0381
Epoch 245/300, seasonal_3 Loss: 0.0208 | 0.0384
Epoch 246/300, seasonal_3 Loss: 0.0208 | 0.0385
Epoch 247/300, seasonal_3 Loss: 0.0208 | 0.0386
Epoch 248/300, seasonal_3 Loss: 0.0208 | 0.0385
Epoch 249/300, seasonal_3 Loss: 0.0207 | 0.0382
Epoch 250/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 251/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 252/300, seasonal_3 Loss: 0.0207 | 0.0381
Epoch 253/300, seasonal_3 Loss: 0.0207 | 0.0383
Epoch 254/300, seasonal_3 Loss: 0.0207 | 0.0384
Epoch 255/300, seasonal_3 Loss: 0.0207 | 0.0384
Epoch 256/300, seasonal_3 Loss: 0.0207 | 0.0382
Epoch 257/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 258/300, seasonal_3 Loss: 0.0207 | 0.0378
Epoch 259/300, seasonal_3 Loss: 0.0207 | 0.0378
Epoch 260/300, seasonal_3 Loss: 0.0207 | 0.0379
Epoch 261/300, seasonal_3 Loss: 0.0207 | 0.0381
Epoch 262/300, seasonal_3 Loss: 0.0207 | 0.0383
Epoch 263/300, seasonal_3 Loss: 0.0207 | 0.0383
Epoch 264/300, seasonal_3 Loss: 0.0207 | 0.0382
Epoch 265/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 266/300, seasonal_3 Loss: 0.0207 | 0.0379
Epoch 267/300, seasonal_3 Loss: 0.0207 | 0.0379
Epoch 268/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 269/300, seasonal_3 Loss: 0.0207 | 0.0382
Epoch 270/300, seasonal_3 Loss: 0.0207 | 0.0383
Epoch 271/300, seasonal_3 Loss: 0.0207 | 0.0384
Epoch 272/300, seasonal_3 Loss: 0.0207 | 0.0383
Epoch 273/300, seasonal_3 Loss: 0.0207 | 0.0382
Epoch 274/300, seasonal_3 Loss: 0.0207 | 0.0380
Epoch 275/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 276/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 277/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 278/300, seasonal_3 Loss: 0.0206 | 0.0382
Epoch 279/300, seasonal_3 Loss: 0.0206 | 0.0382
Epoch 280/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 281/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 282/300, seasonal_3 Loss: 0.0206 | 0.0379
Epoch 283/300, seasonal_3 Loss: 0.0206 | 0.0379
Epoch 284/300, seasonal_3 Loss: 0.0206 | 0.0379
Epoch 285/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 286/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 287/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 288/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 289/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 290/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 291/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 292/300, seasonal_3 Loss: 0.0206 | 0.0380
Epoch 293/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 294/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 295/300, seasonal_3 Loss: 0.0206 | 0.0382
Epoch 296/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 297/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 298/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 299/300, seasonal_3 Loss: 0.0206 | 0.0381
Epoch 300/300, seasonal_3 Loss: 0.0206 | 0.0381
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.939465333604022, 'learning_rate': 0.0003979053614182512, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8340897266974815}
Epoch 1/300, resid Loss: 1.4662 | 0.6133
Epoch 2/300, resid Loss: 0.2797 | 0.1998
Epoch 3/300, resid Loss: 0.2054 | 0.0998
Epoch 4/300, resid Loss: 0.1982 | 0.1002
Epoch 5/300, resid Loss: 0.1564 | 0.0702
Epoch 6/300, resid Loss: 0.1404 | 0.0862
Epoch 7/300, resid Loss: 0.1383 | 0.0851
Epoch 8/300, resid Loss: 0.1195 | 0.0702
Epoch 9/300, resid Loss: 0.1240 | 0.0644
Epoch 10/300, resid Loss: 0.1256 | 0.0816
Epoch 11/300, resid Loss: 0.1410 | 0.1204
Epoch 12/300, resid Loss: 0.1468 | 0.0988
Epoch 13/300, resid Loss: 0.1368 | 0.0766
Epoch 14/300, resid Loss: 0.1113 | 0.0727
Epoch 15/300, resid Loss: 0.0978 | 0.0644
Epoch 16/300, resid Loss: 0.1215 | 0.0708
Epoch 17/300, resid Loss: 0.1356 | 0.0755
Epoch 18/300, resid Loss: 0.1629 | 0.1096
Epoch 19/300, resid Loss: 0.1671 | 0.0656
Epoch 20/300, resid Loss: 0.1832 | 0.1054
Epoch 21/300, resid Loss: 0.1706 | 0.0855
Epoch 22/300, resid Loss: 0.2369 | 0.1271
Epoch 23/300, resid Loss: 0.2423 | 0.0871
Epoch 24/300, resid Loss: 0.2225 | 0.1654
Epoch 25/300, resid Loss: 0.2390 | 0.0865
Epoch 26/300, resid Loss: 0.2581 | 0.1710
Epoch 27/300, resid Loss: 0.2526 | 0.1948
Epoch 28/300, resid Loss: 0.2508 | 0.2291
Epoch 29/300, resid Loss: 0.2367 | 0.0897
Epoch 30/300, resid Loss: 0.2098 | 0.1313
Epoch 31/300, resid Loss: 0.1772 | 0.0963
Epoch 32/300, resid Loss: 0.1532 | 0.0765
Epoch 33/300, resid Loss: 0.1668 | 0.1543
Epoch 34/300, resid Loss: 0.1419 | 0.0867
Epoch 35/300, resid Loss: 0.1623 | 0.1432
Epoch 36/300, resid Loss: 0.1580 | 0.2201
Epoch 37/300, resid Loss: 0.1386 | 0.0573
Epoch 38/300, resid Loss: 0.1382 | 0.0588
Epoch 39/300, resid Loss: 0.1159 | 0.0910
Epoch 40/300, resid Loss: 0.0990 | 0.0876
Epoch 41/300, resid Loss: 0.0889 | 0.0541
Epoch 42/300, resid Loss: 0.0979 | 0.0541
Epoch 43/300, resid Loss: 0.0918 | 0.0630
Epoch 44/300, resid Loss: 0.0933 | 0.0532
Epoch 45/300, resid Loss: 0.0864 | 0.0446
Epoch 46/300, resid Loss: 0.0793 | 0.0492
Epoch 47/300, resid Loss: 0.0766 | 0.0511
Epoch 48/300, resid Loss: 0.0750 | 0.0468
Epoch 49/300, resid Loss: 0.0745 | 0.0421
Epoch 50/300, resid Loss: 0.0742 | 0.0421
Epoch 51/300, resid Loss: 0.0725 | 0.0409
Epoch 52/300, resid Loss: 0.0719 | 0.0426
Epoch 53/300, resid Loss: 0.0715 | 0.0410
Epoch 54/300, resid Loss: 0.0717 | 0.0425
Epoch 55/300, resid Loss: 0.0714 | 0.0394
Epoch 56/300, resid Loss: 0.0717 | 0.0407
Epoch 57/300, resid Loss: 0.0717 | 0.0393
Epoch 58/300, resid Loss: 0.0722 | 0.0418
Epoch 59/300, resid Loss: 0.0720 | 0.0414
Epoch 60/300, resid Loss: 0.0727 | 0.0392
Epoch 61/300, resid Loss: 0.0710 | 0.0376
Epoch 62/300, resid Loss: 0.0709 | 0.0382
Epoch 63/300, resid Loss: 0.0691 | 0.0396
Epoch 64/300, resid Loss: 0.0686 | 0.0377
Epoch 65/300, resid Loss: 0.0680 | 0.0388
Epoch 66/300, resid Loss: 0.0686 | 0.0362
Epoch 67/300, resid Loss: 0.0680 | 0.0377
Epoch 68/300, resid Loss: 0.0675 | 0.0378
Epoch 69/300, resid Loss: 0.0696 | 0.0407
Epoch 70/300, resid Loss: 0.0719 | 0.0384
Epoch 71/300, resid Loss: 0.0688 | 0.0374
Epoch 72/300, resid Loss: 0.0680 | 0.0384
Epoch 73/300, resid Loss: 0.0676 | 0.0370
Epoch 74/300, resid Loss: 0.0674 | 0.0380
Epoch 75/300, resid Loss: 0.0672 | 0.0361
Epoch 76/300, resid Loss: 0.0668 | 0.0381
Epoch 77/300, resid Loss: 0.0664 | 0.0354
Epoch 78/300, resid Loss: 0.0655 | 0.0360
Epoch 79/300, resid Loss: 0.0650 | 0.0351
Epoch 80/300, resid Loss: 0.0644 | 0.0352
Epoch 81/300, resid Loss: 0.0640 | 0.0350
Epoch 82/300, resid Loss: 0.0638 | 0.0341
Epoch 83/300, resid Loss: 0.0638 | 0.0343
Epoch 84/300, resid Loss: 0.0635 | 0.0340
Epoch 85/300, resid Loss: 0.0633 | 0.0346
Epoch 86/300, resid Loss: 0.0631 | 0.0331
Epoch 87/300, resid Loss: 0.0631 | 0.0336
Epoch 88/300, resid Loss: 0.0628 | 0.0332
Epoch 89/300, resid Loss: 0.0628 | 0.0328
Epoch 90/300, resid Loss: 0.0627 | 0.0320
Epoch 91/300, resid Loss: 0.0628 | 0.0327
Epoch 92/300, resid Loss: 0.0625 | 0.0332
Epoch 93/300, resid Loss: 0.0627 | 0.0340
Epoch 94/300, resid Loss: 0.0631 | 0.0325
Epoch 95/300, resid Loss: 0.0623 | 0.0326
Epoch 96/300, resid Loss: 0.0622 | 0.0324
Epoch 97/300, resid Loss: 0.0619 | 0.0322
Epoch 98/300, resid Loss: 0.0617 | 0.0321
Epoch 99/300, resid Loss: 0.0617 | 0.0320
Epoch 100/300, resid Loss: 0.0614 | 0.0323
Epoch 101/300, resid Loss: 0.0614 | 0.0320
Epoch 102/300, resid Loss: 0.0612 | 0.0318
Epoch 103/300, resid Loss: 0.0611 | 0.0317
Epoch 104/300, resid Loss: 0.0611 | 0.0317
Epoch 105/300, resid Loss: 0.0610 | 0.0319
Epoch 106/300, resid Loss: 0.0609 | 0.0317
Epoch 107/300, resid Loss: 0.0608 | 0.0316
Epoch 108/300, resid Loss: 0.0608 | 0.0316
Epoch 109/300, resid Loss: 0.0607 | 0.0316
Epoch 110/300, resid Loss: 0.0607 | 0.0316
Epoch 111/300, resid Loss: 0.0606 | 0.0315
Epoch 112/300, resid Loss: 0.0606 | 0.0314
Epoch 113/300, resid Loss: 0.0605 | 0.0315
Epoch 114/300, resid Loss: 0.0605 | 0.0314
Epoch 115/300, resid Loss: 0.0604 | 0.0314
Epoch 116/300, resid Loss: 0.0604 | 0.0313
Epoch 117/300, resid Loss: 0.0604 | 0.0313
Epoch 118/300, resid Loss: 0.0603 | 0.0313
Epoch 119/300, resid Loss: 0.0603 | 0.0313
Epoch 120/300, resid Loss: 0.0603 | 0.0313
Epoch 121/300, resid Loss: 0.0602 | 0.0312
Epoch 122/300, resid Loss: 0.0602 | 0.0312
Epoch 123/300, resid Loss: 0.0602 | 0.0312
Epoch 124/300, resid Loss: 0.0601 | 0.0312
Epoch 125/300, resid Loss: 0.0601 | 0.0312
Epoch 126/300, resid Loss: 0.0601 | 0.0311
Epoch 127/300, resid Loss: 0.0600 | 0.0311
Epoch 128/300, resid Loss: 0.0600 | 0.0311
Epoch 129/300, resid Loss: 0.0600 | 0.0311
Epoch 130/300, resid Loss: 0.0600 | 0.0311
Epoch 131/300, resid Loss: 0.0599 | 0.0311
Epoch 132/300, resid Loss: 0.0599 | 0.0310
Epoch 133/300, resid Loss: 0.0599 | 0.0310
Epoch 134/300, resid Loss: 0.0599 | 0.0310
Epoch 135/300, resid Loss: 0.0599 | 0.0310
Epoch 136/300, resid Loss: 0.0598 | 0.0310
Epoch 137/300, resid Loss: 0.0598 | 0.0310
Epoch 138/300, resid Loss: 0.0598 | 0.0310
Epoch 139/300, resid Loss: 0.0598 | 0.0309
Epoch 140/300, resid Loss: 0.0598 | 0.0309
Epoch 141/300, resid Loss: 0.0598 | 0.0309
Epoch 142/300, resid Loss: 0.0597 | 0.0309
Epoch 143/300, resid Loss: 0.0597 | 0.0309
Epoch 144/300, resid Loss: 0.0597 | 0.0309
Epoch 145/300, resid Loss: 0.0597 | 0.0309
Epoch 146/300, resid Loss: 0.0597 | 0.0309
Epoch 147/300, resid Loss: 0.0597 | 0.0309
Epoch 148/300, resid Loss: 0.0597 | 0.0309
Epoch 149/300, resid Loss: 0.0596 | 0.0308
Epoch 150/300, resid Loss: 0.0596 | 0.0308
Epoch 151/300, resid Loss: 0.0596 | 0.0308
Epoch 152/300, resid Loss: 0.0596 | 0.0308
Epoch 153/300, resid Loss: 0.0596 | 0.0308
Epoch 154/300, resid Loss: 0.0596 | 0.0308
Epoch 155/300, resid Loss: 0.0596 | 0.0308
Epoch 156/300, resid Loss: 0.0596 | 0.0308
Epoch 157/300, resid Loss: 0.0596 | 0.0308
Epoch 158/300, resid Loss: 0.0595 | 0.0308
Epoch 159/300, resid Loss: 0.0595 | 0.0308
Epoch 160/300, resid Loss: 0.0595 | 0.0308
Epoch 161/300, resid Loss: 0.0595 | 0.0308
Epoch 162/300, resid Loss: 0.0595 | 0.0308
Epoch 163/300, resid Loss: 0.0595 | 0.0308
Epoch 164/300, resid Loss: 0.0595 | 0.0307
Epoch 165/300, resid Loss: 0.0595 | 0.0307
Epoch 166/300, resid Loss: 0.0595 | 0.0307
Epoch 167/300, resid Loss: 0.0595 | 0.0307
Epoch 168/300, resid Loss: 0.0595 | 0.0307
Epoch 169/300, resid Loss: 0.0595 | 0.0307
Epoch 170/300, resid Loss: 0.0595 | 0.0307
Epoch 171/300, resid Loss: 0.0594 | 0.0307
Epoch 172/300, resid Loss: 0.0594 | 0.0307
Epoch 173/300, resid Loss: 0.0594 | 0.0307
Epoch 174/300, resid Loss: 0.0594 | 0.0307
Epoch 175/300, resid Loss: 0.0594 | 0.0307
Epoch 176/300, resid Loss: 0.0594 | 0.0307
Epoch 177/300, resid Loss: 0.0594 | 0.0307
Epoch 178/300, resid Loss: 0.0594 | 0.0307
Epoch 179/300, resid Loss: 0.0594 | 0.0307
Epoch 180/300, resid Loss: 0.0594 | 0.0307
Epoch 181/300, resid Loss: 0.0594 | 0.0307
Epoch 182/300, resid Loss: 0.0594 | 0.0307
Epoch 183/300, resid Loss: 0.0594 | 0.0307
Epoch 184/300, resid Loss: 0.0594 | 0.0307
Epoch 185/300, resid Loss: 0.0594 | 0.0307
Epoch 186/300, resid Loss: 0.0594 | 0.0307
Epoch 187/300, resid Loss: 0.0594 | 0.0307
Epoch 188/300, resid Loss: 0.0594 | 0.0307
Epoch 189/300, resid Loss: 0.0594 | 0.0307
Epoch 190/300, resid Loss: 0.0594 | 0.0307
Epoch 191/300, resid Loss: 0.0594 | 0.0307
Epoch 192/300, resid Loss: 0.0594 | 0.0307
Epoch 193/300, resid Loss: 0.0594 | 0.0307
Epoch 194/300, resid Loss: 0.0594 | 0.0306
Epoch 195/300, resid Loss: 0.0593 | 0.0306
Epoch 196/300, resid Loss: 0.0593 | 0.0306
Epoch 197/300, resid Loss: 0.0593 | 0.0306
Epoch 198/300, resid Loss: 0.0593 | 0.0306
Epoch 199/300, resid Loss: 0.0593 | 0.0306
Epoch 200/300, resid Loss: 0.0593 | 0.0306
Epoch 201/300, resid Loss: 0.0593 | 0.0306
Epoch 202/300, resid Loss: 0.0593 | 0.0306
Epoch 203/300, resid Loss: 0.0593 | 0.0306
Epoch 204/300, resid Loss: 0.0593 | 0.0306
Epoch 205/300, resid Loss: 0.0593 | 0.0306
Epoch 206/300, resid Loss: 0.0593 | 0.0306
Epoch 207/300, resid Loss: 0.0593 | 0.0306
Epoch 208/300, resid Loss: 0.0593 | 0.0306
Epoch 209/300, resid Loss: 0.0593 | 0.0306
Epoch 210/300, resid Loss: 0.0593 | 0.0306
Epoch 211/300, resid Loss: 0.0593 | 0.0306
Epoch 212/300, resid Loss: 0.0593 | 0.0306
Epoch 213/300, resid Loss: 0.0593 | 0.0306
Epoch 214/300, resid Loss: 0.0593 | 0.0306
Epoch 215/300, resid Loss: 0.0593 | 0.0306
Epoch 216/300, resid Loss: 0.0593 | 0.0306
Epoch 217/300, resid Loss: 0.0593 | 0.0306
Epoch 218/300, resid Loss: 0.0593 | 0.0306
Epoch 219/300, resid Loss: 0.0593 | 0.0306
Epoch 220/300, resid Loss: 0.0593 | 0.0306
Epoch 221/300, resid Loss: 0.0593 | 0.0306
Epoch 222/300, resid Loss: 0.0593 | 0.0306
Epoch 223/300, resid Loss: 0.0593 | 0.0306
Epoch 224/300, resid Loss: 0.0593 | 0.0306
Epoch 225/300, resid Loss: 0.0593 | 0.0306
Epoch 226/300, resid Loss: 0.0593 | 0.0306
Epoch 227/300, resid Loss: 0.0593 | 0.0306
Epoch 228/300, resid Loss: 0.0593 | 0.0306
Epoch 229/300, resid Loss: 0.0593 | 0.0306
Epoch 230/300, resid Loss: 0.0593 | 0.0306
Epoch 231/300, resid Loss: 0.0593 | 0.0306
Epoch 232/300, resid Loss: 0.0593 | 0.0306
Epoch 233/300, resid Loss: 0.0593 | 0.0306
Epoch 234/300, resid Loss: 0.0593 | 0.0306
Epoch 235/300, resid Loss: 0.0593 | 0.0306
Epoch 236/300, resid Loss: 0.0593 | 0.0306
Epoch 237/300, resid Loss: 0.0593 | 0.0306
Epoch 238/300, resid Loss: 0.0593 | 0.0306
Epoch 239/300, resid Loss: 0.0593 | 0.0306
Epoch 240/300, resid Loss: 0.0593 | 0.0306
Epoch 241/300, resid Loss: 0.0593 | 0.0306
Epoch 242/300, resid Loss: 0.0593 | 0.0306
Epoch 243/300, resid Loss: 0.0593 | 0.0306
Epoch 244/300, resid Loss: 0.0593 | 0.0306
Epoch 245/300, resid Loss: 0.0593 | 0.0306
Epoch 246/300, resid Loss: 0.0593 | 0.0306
Epoch 247/300, resid Loss: 0.0593 | 0.0306
Epoch 248/300, resid Loss: 0.0593 | 0.0306
Epoch 249/300, resid Loss: 0.0593 | 0.0306
Epoch 250/300, resid Loss: 0.0593 | 0.0306
Epoch 251/300, resid Loss: 0.0593 | 0.0306
Epoch 252/300, resid Loss: 0.0593 | 0.0306
Epoch 253/300, resid Loss: 0.0593 | 0.0306
Epoch 254/300, resid Loss: 0.0593 | 0.0306
Epoch 255/300, resid Loss: 0.0593 | 0.0306
Epoch 256/300, resid Loss: 0.0593 | 0.0306
Epoch 257/300, resid Loss: 0.0593 | 0.0306
Epoch 258/300, resid Loss: 0.0593 | 0.0306
Epoch 259/300, resid Loss: 0.0593 | 0.0306
Epoch 260/300, resid Loss: 0.0593 | 0.0306
Epoch 261/300, resid Loss: 0.0593 | 0.0306
Epoch 262/300, resid Loss: 0.0593 | 0.0306
Epoch 263/300, resid Loss: 0.0593 | 0.0306
Epoch 264/300, resid Loss: 0.0593 | 0.0306
Epoch 265/300, resid Loss: 0.0593 | 0.0306
Epoch 266/300, resid Loss: 0.0593 | 0.0306
Epoch 267/300, resid Loss: 0.0593 | 0.0306
Epoch 268/300, resid Loss: 0.0593 | 0.0306
Epoch 269/300, resid Loss: 0.0593 | 0.0306
Epoch 270/300, resid Loss: 0.0593 | 0.0306
Epoch 271/300, resid Loss: 0.0593 | 0.0306
Epoch 272/300, resid Loss: 0.0593 | 0.0306
Epoch 273/300, resid Loss: 0.0593 | 0.0306
Epoch 274/300, resid Loss: 0.0593 | 0.0306
Epoch 275/300, resid Loss: 0.0593 | 0.0306
Epoch 276/300, resid Loss: 0.0593 | 0.0306
Epoch 277/300, resid Loss: 0.0593 | 0.0306
Epoch 278/300, resid Loss: 0.0593 | 0.0306
Epoch 279/300, resid Loss: 0.0593 | 0.0306
Epoch 280/300, resid Loss: 0.0593 | 0.0306
Epoch 281/300, resid Loss: 0.0593 | 0.0306
Epoch 282/300, resid Loss: 0.0593 | 0.0306
Epoch 283/300, resid Loss: 0.0593 | 0.0306
Epoch 284/300, resid Loss: 0.0593 | 0.0306
Epoch 285/300, resid Loss: 0.0593 | 0.0306
Epoch 286/300, resid Loss: 0.0593 | 0.0306
Epoch 287/300, resid Loss: 0.0593 | 0.0306
Epoch 288/300, resid Loss: 0.0593 | 0.0306
Epoch 289/300, resid Loss: 0.0593 | 0.0306
Epoch 290/300, resid Loss: 0.0593 | 0.0306
Epoch 291/300, resid Loss: 0.0593 | 0.0306
Epoch 292/300, resid Loss: 0.0593 | 0.0306
Epoch 293/300, resid Loss: 0.0593 | 0.0306
Epoch 294/300, resid Loss: 0.0593 | 0.0306
Epoch 295/300, resid Loss: 0.0593 | 0.0306
Epoch 296/300, resid Loss: 0.0593 | 0.0306
Epoch 297/300, resid Loss: 0.0593 | 0.0306
Epoch 298/300, resid Loss: 0.0593 | 0.0306
Epoch 299/300, resid Loss: 0.0593 | 0.0306
Epoch 300/300, resid Loss: 0.0593 | 0.0306
Runtime (seconds): 8110.943500995636
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[154.62796]
[-4.7249937]
[-0.30207044]
[13.863556]
[-1.5010364]
[19.848896]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 572.5342816731427
RMSE: 23.927688598632812
MAE: 23.927688598632812
R-squared: nan
[181.81232]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
