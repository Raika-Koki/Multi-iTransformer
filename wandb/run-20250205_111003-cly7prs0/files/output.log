[32m[I 2025-02-05 11:10:08,783][0m A new study created in memory with name: no-name-4fbf1045-5cae-4a26-a9d6-3b3ab92673b5[0m
[32m[I 2025-02-05 11:10:34,601][0m Trial 0 finished with value: 0.18726001679897308 and parameters: {'observation_period_num': 212, 'train_rates': 0.9522066402229131, 'learning_rate': 0.0006279456674317133, 'batch_size': 245, 'step_size': 8, 'gamma': 0.8230086442253894}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:11:49,648][0m Trial 1 finished with value: 0.285758857560821 and parameters: {'observation_period_num': 201, 'train_rates': 0.7468710778890423, 'learning_rate': 0.0001917189567533482, 'batch_size': 63, 'step_size': 8, 'gamma': 0.9178966978481384}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:13:33,533][0m Trial 2 finished with value: 0.27658689916133883 and parameters: {'observation_period_num': 195, 'train_rates': 0.7225561339366994, 'learning_rate': 4.700593739971164e-05, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9855340120199025}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:14:17,422][0m Trial 3 finished with value: 1.3205648665924186 and parameters: {'observation_period_num': 162, 'train_rates': 0.6370390561878004, 'learning_rate': 5.3529618340222106e-06, 'batch_size': 101, 'step_size': 3, 'gamma': 0.8013085807697485}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:14:48,199][0m Trial 4 finished with value: 0.3204645812511444 and parameters: {'observation_period_num': 156, 'train_rates': 0.9513961917695173, 'learning_rate': 2.8456978528236504e-05, 'batch_size': 202, 'step_size': 6, 'gamma': 0.915701382207953}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:15:22,359][0m Trial 5 finished with value: 0.5780905852961474 and parameters: {'observation_period_num': 7, 'train_rates': 0.8774693946060395, 'learning_rate': 2.0445340735634064e-06, 'batch_size': 179, 'step_size': 14, 'gamma': 0.8273580168237409}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:16:13,672][0m Trial 6 finished with value: 0.2172880326974414 and parameters: {'observation_period_num': 48, 'train_rates': 0.8149599588087658, 'learning_rate': 9.717893624075927e-06, 'batch_size': 108, 'step_size': 13, 'gamma': 0.823534935328955}. Best is trial 0 with value: 0.18726001679897308.[0m
[32m[I 2025-02-05 11:17:30,876][0m Trial 7 finished with value: 0.13629597425460815 and parameters: {'observation_period_num': 98, 'train_rates': 0.9768220733155791, 'learning_rate': 9.480034861911452e-05, 'batch_size': 77, 'step_size': 9, 'gamma': 0.9177263350882114}. Best is trial 7 with value: 0.13629597425460815.[0m
[32m[I 2025-02-05 11:17:52,786][0m Trial 8 finished with value: 0.20250936373489414 and parameters: {'observation_period_num': 72, 'train_rates': 0.6890149551842579, 'learning_rate': 0.00010974803898107665, 'batch_size': 236, 'step_size': 13, 'gamma': 0.9557763570750643}. Best is trial 7 with value: 0.13629597425460815.[0m
[32m[I 2025-02-05 11:18:34,511][0m Trial 9 finished with value: 0.5693443371699407 and parameters: {'observation_period_num': 230, 'train_rates': 0.6678762532324636, 'learning_rate': 4.0244246254682185e-06, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9896942378043301}. Best is trial 7 with value: 0.13629597425460815.[0m
[32m[I 2025-02-05 11:24:16,026][0m Trial 10 finished with value: 0.1368160034633345 and parameters: {'observation_period_num': 97, 'train_rates': 0.9875398773692011, 'learning_rate': 0.000856718360619403, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7554160009435397}. Best is trial 7 with value: 0.13629597425460815.[0m
[32m[I 2025-02-05 11:27:50,454][0m Trial 11 finished with value: 0.12588545402039342 and parameters: {'observation_period_num': 98, 'train_rates': 0.9768564489984028, 'learning_rate': 0.0008168445851901277, 'batch_size': 27, 'step_size': 11, 'gamma': 0.7583691428622431}. Best is trial 11 with value: 0.12588545402039342.[0m
[32m[I 2025-02-05 11:29:14,450][0m Trial 12 finished with value: 0.15026241692482228 and parameters: {'observation_period_num': 115, 'train_rates': 0.8777049816884791, 'learning_rate': 0.0001912926127518376, 'batch_size': 65, 'step_size': 11, 'gamma': 0.8820734170089903}. Best is trial 11 with value: 0.12588545402039342.[0m
[32m[I 2025-02-05 11:33:34,503][0m Trial 13 finished with value: 0.1344213785187712 and parameters: {'observation_period_num': 62, 'train_rates': 0.8938993860225142, 'learning_rate': 0.0004055601678061641, 'batch_size': 21, 'step_size': 10, 'gamma': 0.8769785253811223}. Best is trial 11 with value: 0.12588545402039342.[0m
[32m[I 2025-02-05 11:37:54,804][0m Trial 14 finished with value: 0.07319098363655645 and parameters: {'observation_period_num': 48, 'train_rates': 0.8860161519471259, 'learning_rate': 0.0004366861938825828, 'batch_size': 21, 'step_size': 6, 'gamma': 0.7533133000908763}. Best is trial 14 with value: 0.07319098363655645.[0m
[32m[I 2025-02-05 11:38:32,467][0m Trial 15 finished with value: 0.062021889859796876 and parameters: {'observation_period_num': 17, 'train_rates': 0.8145582297139813, 'learning_rate': 0.00036362715397286055, 'batch_size': 149, 'step_size': 5, 'gamma': 0.7568710772536947}. Best is trial 15 with value: 0.062021889859796876.[0m
[32m[I 2025-02-05 11:39:09,148][0m Trial 16 finished with value: 0.05016613464118538 and parameters: {'observation_period_num': 6, 'train_rates': 0.8108016548437926, 'learning_rate': 0.0002676035037437301, 'batch_size': 155, 'step_size': 5, 'gamma': 0.7838603448905478}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:39:45,039][0m Trial 17 finished with value: 0.08937565741873292 and parameters: {'observation_period_num': 5, 'train_rates': 0.8088131763377235, 'learning_rate': 4.31030529646255e-05, 'batch_size': 157, 'step_size': 4, 'gamma': 0.7840820495397386}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:40:23,791][0m Trial 18 finished with value: 0.19084179972127724 and parameters: {'observation_period_num': 21, 'train_rates': 0.7724140826824285, 'learning_rate': 0.00021710509157849526, 'batch_size': 142, 'step_size': 5, 'gamma': 0.788903122580783}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:40:53,718][0m Trial 19 finished with value: 0.12825325378775596 and parameters: {'observation_period_num': 31, 'train_rates': 0.8299347463645376, 'learning_rate': 0.00010001794824767958, 'batch_size': 193, 'step_size': 2, 'gamma': 0.8469069713473119}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:41:30,211][0m Trial 20 finished with value: 0.42197662279654746 and parameters: {'observation_period_num': 139, 'train_rates': 0.8450921171403307, 'learning_rate': 1.822615150764212e-05, 'batch_size': 152, 'step_size': 6, 'gamma': 0.7774573256524574}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:42:19,362][0m Trial 21 finished with value: 0.05635414453166904 and parameters: {'observation_period_num': 41, 'train_rates': 0.9100319984065897, 'learning_rate': 0.00037541614233287666, 'batch_size': 122, 'step_size': 6, 'gamma': 0.7520122849295}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:43:02,817][0m Trial 22 finished with value: 0.19065618991851807 and parameters: {'observation_period_num': 33, 'train_rates': 0.7699767710526435, 'learning_rate': 0.0003014252029306805, 'batch_size': 124, 'step_size': 4, 'gamma': 0.8021645348793541}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:43:40,864][0m Trial 23 finished with value: 0.14014736843218498 and parameters: {'observation_period_num': 69, 'train_rates': 0.9250033553673648, 'learning_rate': 7.725911396465087e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.7738502489476202}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:44:06,910][0m Trial 24 finished with value: 0.20042474191221926 and parameters: {'observation_period_num': 18, 'train_rates': 0.7866147666412869, 'learning_rate': 0.00039655980688955446, 'batch_size': 217, 'step_size': 4, 'gamma': 0.8010455514801642}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:44:51,336][0m Trial 25 finished with value: 0.06941935689886501 and parameters: {'observation_period_num': 45, 'train_rates': 0.8497613380606843, 'learning_rate': 0.00016322475891910687, 'batch_size': 127, 'step_size': 5, 'gamma': 0.7726975553138917}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:45:22,012][0m Trial 26 finished with value: 0.16157124095582334 and parameters: {'observation_period_num': 5, 'train_rates': 0.7312896287959023, 'learning_rate': 0.0005752841263859615, 'batch_size': 173, 'step_size': 3, 'gamma': 0.8509863511150921}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:46:28,750][0m Trial 27 finished with value: 0.10428982554003596 and parameters: {'observation_period_num': 86, 'train_rates': 0.913945731783292, 'learning_rate': 0.0009288697893108667, 'batch_size': 85, 'step_size': 7, 'gamma': 0.7528170383251251}. Best is trial 16 with value: 0.05016613464118538.[0m
Early stopping at epoch 48
[32m[I 2025-02-05 11:46:49,000][0m Trial 28 finished with value: 0.5588446010896309 and parameters: {'observation_period_num': 41, 'train_rates': 0.864225283354205, 'learning_rate': 6.13498983780758e-05, 'batch_size': 144, 'step_size': 1, 'gamma': 0.7692570451223908}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:47:22,336][0m Trial 29 finished with value: 0.07637415778691366 and parameters: {'observation_period_num': 60, 'train_rates': 0.9247637123248821, 'learning_rate': 0.0002787231776485805, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8079954931762465}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:48:07,025][0m Trial 30 finished with value: 0.061772056975058005 and parameters: {'observation_period_num': 23, 'train_rates': 0.8363406069778655, 'learning_rate': 0.0001372065009869187, 'batch_size': 124, 'step_size': 5, 'gamma': 0.8390295203086566}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:48:52,692][0m Trial 31 finished with value: 0.05633704031710504 and parameters: {'observation_period_num': 23, 'train_rates': 0.8257519571853762, 'learning_rate': 0.00013201074529716906, 'batch_size': 127, 'step_size': 5, 'gamma': 0.8371111590786808}. Best is trial 16 with value: 0.05016613464118538.[0m
[32m[I 2025-02-05 11:49:37,014][0m Trial 32 finished with value: 0.0498356460213917 and parameters: {'observation_period_num': 28, 'train_rates': 0.8416629325072345, 'learning_rate': 0.0001483587701851126, 'batch_size': 129, 'step_size': 7, 'gamma': 0.8444242377576245}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:50:34,435][0m Trial 33 finished with value: 0.17106529222943415 and parameters: {'observation_period_num': 34, 'train_rates': 0.7542579704488332, 'learning_rate': 0.00022356793409299553, 'batch_size': 92, 'step_size': 7, 'gamma': 0.8614621533275497}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:51:15,528][0m Trial 34 finished with value: 0.11932993800890863 and parameters: {'observation_period_num': 54, 'train_rates': 0.7946805473391649, 'learning_rate': 2.5653854674444224e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.8938278624228808}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:52:14,084][0m Trial 35 finished with value: 0.24519610427208802 and parameters: {'observation_period_num': 80, 'train_rates': 0.907397830282783, 'learning_rate': 4.808072185461923e-05, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8141023566544145}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:53:05,934][0m Trial 36 finished with value: 0.15242555437708746 and parameters: {'observation_period_num': 165, 'train_rates': 0.9397874567561656, 'learning_rate': 0.0005825429794028547, 'batch_size': 113, 'step_size': 6, 'gamma': 0.8971329334707884}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:53:39,663][0m Trial 37 finished with value: 0.053424960663241726 and parameters: {'observation_period_num': 25, 'train_rates': 0.8526401268705718, 'learning_rate': 0.00013760547369427324, 'batch_size': 168, 'step_size': 9, 'gamma': 0.8395298265374808}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:54:06,760][0m Trial 38 finished with value: 0.16874329286673673 and parameters: {'observation_period_num': 189, 'train_rates': 0.8624715222418096, 'learning_rate': 0.00013552463572470027, 'batch_size': 209, 'step_size': 9, 'gamma': 0.833435264474838}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:54:42,713][0m Trial 39 finished with value: 0.198729940831318 and parameters: {'observation_period_num': 25, 'train_rates': 0.822948240871184, 'learning_rate': 1.3953576263561124e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.8621167166405049}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:55:11,479][0m Trial 40 finished with value: 0.2695241245454636 and parameters: {'observation_period_num': 119, 'train_rates': 0.7095628699791129, 'learning_rate': 6.914624030621139e-05, 'batch_size': 174, 'step_size': 8, 'gamma': 0.8167163901604907}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:55:46,350][0m Trial 41 finished with value: 0.1459541541699366 and parameters: {'observation_period_num': 17, 'train_rates': 0.614275976327981, 'learning_rate': 0.0002459706911654262, 'batch_size': 137, 'step_size': 6, 'gamma': 0.846727664706105}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:56:35,508][0m Trial 42 finished with value: 0.05108201860760649 and parameters: {'observation_period_num': 36, 'train_rates': 0.8608975607955041, 'learning_rate': 0.00015408179900880666, 'batch_size': 119, 'step_size': 9, 'gamma': 0.7979159620711873}. Best is trial 32 with value: 0.0498356460213917.[0m
[32m[I 2025-02-05 11:57:30,748][0m Trial 43 finished with value: 0.042393184117621406 and parameters: {'observation_period_num': 14, 'train_rates': 0.8564477574819118, 'learning_rate': 0.00013473777446748733, 'batch_size': 103, 'step_size': 9, 'gamma': 0.825885308385574}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 11:58:50,786][0m Trial 44 finished with value: 0.6163597241859531 and parameters: {'observation_period_num': 12, 'train_rates': 0.8640420615852753, 'learning_rate': 1.198182136960287e-06, 'batch_size': 70, 'step_size': 10, 'gamma': 0.7925244309467088}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 11:59:44,016][0m Trial 45 finished with value: 0.054342031269595865 and parameters: {'observation_period_num': 36, 'train_rates': 0.8484960065687189, 'learning_rate': 8.744514821087703e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.8204942381772105}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 12:01:27,561][0m Trial 46 finished with value: 0.0628505391545936 and parameters: {'observation_period_num': 51, 'train_rates': 0.8059463985584732, 'learning_rate': 3.9202867571685935e-05, 'batch_size': 51, 'step_size': 12, 'gamma': 0.8274811685848674}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 12:02:24,410][0m Trial 47 finished with value: 0.20726174611297454 and parameters: {'observation_period_num': 248, 'train_rates': 0.8818533182843835, 'learning_rate': 0.00019496579941701047, 'batch_size': 95, 'step_size': 11, 'gamma': 0.797111867807013}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 12:03:34,659][0m Trial 48 finished with value: 0.07364957334282689 and parameters: {'observation_period_num': 62, 'train_rates': 0.8929124253170471, 'learning_rate': 0.00011025170198841476, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8603273319729867}. Best is trial 43 with value: 0.042393184117621406.[0m
[32m[I 2025-02-05 12:04:22,323][0m Trial 49 finished with value: 0.1774697397628431 and parameters: {'observation_period_num': 5, 'train_rates': 0.7830113015264162, 'learning_rate': 5.84107098008642e-05, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9591621932708776}. Best is trial 43 with value: 0.042393184117621406.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.7308 | 0.5299
Epoch 2/300, Loss: 0.2511 | 0.3305
Epoch 3/300, Loss: 0.1832 | 0.3333
Epoch 4/300, Loss: 0.1602 | 0.2269
Epoch 5/300, Loss: 0.1492 | 0.1908
Epoch 6/300, Loss: 0.1483 | 0.1527
Epoch 7/300, Loss: 0.1501 | 0.1473
Epoch 8/300, Loss: 0.1418 | 0.1292
Epoch 9/300, Loss: 0.1291 | 0.1169
Epoch 10/300, Loss: 0.1313 | 0.1326
Epoch 11/300, Loss: 0.1407 | 0.1829
Epoch 12/300, Loss: 0.1348 | 0.1541
Epoch 13/300, Loss: 0.1195 | 0.0985
Epoch 14/300, Loss: 0.1212 | 0.1008
Epoch 15/300, Loss: 0.1244 | 0.0941
Epoch 16/300, Loss: 0.1211 | 0.0882
Epoch 17/300, Loss: 0.1182 | 0.0839
Epoch 18/300, Loss: 0.1147 | 0.0817
Epoch 19/300, Loss: 0.1106 | 0.0801
Epoch 20/300, Loss: 0.1074 | 0.0777
Epoch 21/300, Loss: 0.1051 | 0.0758
Epoch 22/300, Loss: 0.1045 | 0.0746
Epoch 23/300, Loss: 0.1048 | 0.0739
Epoch 24/300, Loss: 0.1048 | 0.0737
Epoch 25/300, Loss: 0.1038 | 0.0730
Epoch 26/300, Loss: 0.1016 | 0.0721
Epoch 27/300, Loss: 0.1001 | 0.0715
Epoch 28/300, Loss: 0.0994 | 0.0712
Epoch 29/300, Loss: 0.0991 | 0.0706
Epoch 30/300, Loss: 0.0986 | 0.0699
Epoch 31/300, Loss: 0.0981 | 0.0693
Epoch 32/300, Loss: 0.0977 | 0.0688
Epoch 33/300, Loss: 0.0972 | 0.0683
Epoch 34/300, Loss: 0.0969 | 0.0680
Epoch 35/300, Loss: 0.0965 | 0.0677
Epoch 36/300, Loss: 0.0961 | 0.0675
Epoch 37/300, Loss: 0.0957 | 0.0672
Epoch 38/300, Loss: 0.0954 | 0.0668
Epoch 39/300, Loss: 0.0952 | 0.0666
Epoch 40/300, Loss: 0.0949 | 0.0664
Epoch 41/300, Loss: 0.0946 | 0.0662
Epoch 42/300, Loss: 0.0943 | 0.0658
Epoch 43/300, Loss: 0.0941 | 0.0656
Epoch 44/300, Loss: 0.0939 | 0.0655
Epoch 45/300, Loss: 0.0936 | 0.0653
Epoch 46/300, Loss: 0.0934 | 0.0650
Epoch 47/300, Loss: 0.0932 | 0.0649
Epoch 48/300, Loss: 0.0930 | 0.0647
Epoch 49/300, Loss: 0.0928 | 0.0646
Epoch 50/300, Loss: 0.0926 | 0.0645
Epoch 51/300, Loss: 0.0924 | 0.0642
Epoch 52/300, Loss: 0.0923 | 0.0641
Epoch 53/300, Loss: 0.0921 | 0.0640
Epoch 54/300, Loss: 0.0920 | 0.0639
Epoch 55/300, Loss: 0.0918 | 0.0637
Epoch 56/300, Loss: 0.0917 | 0.0636
Epoch 57/300, Loss: 0.0915 | 0.0635
Epoch 58/300, Loss: 0.0914 | 0.0634
Epoch 59/300, Loss: 0.0913 | 0.0633
Epoch 60/300, Loss: 0.0911 | 0.0631
Epoch 61/300, Loss: 0.0910 | 0.0630
Epoch 62/300, Loss: 0.0909 | 0.0629
Epoch 63/300, Loss: 0.0908 | 0.0628
Epoch 64/300, Loss: 0.0907 | 0.0627
Epoch 65/300, Loss: 0.0906 | 0.0626
Epoch 66/300, Loss: 0.0905 | 0.0625
Epoch 67/300, Loss: 0.0904 | 0.0625
Epoch 68/300, Loss: 0.0903 | 0.0624
Epoch 69/300, Loss: 0.0902 | 0.0623
Epoch 70/300, Loss: 0.0901 | 0.0622
Epoch 71/300, Loss: 0.0900 | 0.0621
Epoch 72/300, Loss: 0.0899 | 0.0621
Epoch 73/300, Loss: 0.0898 | 0.0620
Epoch 74/300, Loss: 0.0898 | 0.0619
Epoch 75/300, Loss: 0.0897 | 0.0619
Epoch 76/300, Loss: 0.0896 | 0.0618
Epoch 77/300, Loss: 0.0896 | 0.0618
Epoch 78/300, Loss: 0.0895 | 0.0617
Epoch 79/300, Loss: 0.0894 | 0.0616
Epoch 80/300, Loss: 0.0894 | 0.0616
Epoch 81/300, Loss: 0.0893 | 0.0615
Epoch 82/300, Loss: 0.0892 | 0.0615
Epoch 83/300, Loss: 0.0892 | 0.0614
Epoch 84/300, Loss: 0.0891 | 0.0614
Epoch 85/300, Loss: 0.0891 | 0.0613
Epoch 86/300, Loss: 0.0890 | 0.0613
Epoch 87/300, Loss: 0.0890 | 0.0612
Epoch 88/300, Loss: 0.0889 | 0.0612
Epoch 89/300, Loss: 0.0889 | 0.0612
Epoch 90/300, Loss: 0.0888 | 0.0611
Epoch 91/300, Loss: 0.0888 | 0.0611
Epoch 92/300, Loss: 0.0888 | 0.0610
Epoch 93/300, Loss: 0.0887 | 0.0610
Epoch 94/300, Loss: 0.0887 | 0.0610
Epoch 95/300, Loss: 0.0886 | 0.0610
Epoch 96/300, Loss: 0.0886 | 0.0609
Epoch 97/300, Loss: 0.0886 | 0.0609
Epoch 98/300, Loss: 0.0885 | 0.0609
Epoch 99/300, Loss: 0.0885 | 0.0608
Epoch 100/300, Loss: 0.0885 | 0.0608
Epoch 101/300, Loss: 0.0885 | 0.0608
Epoch 102/300, Loss: 0.0884 | 0.0608
Epoch 103/300, Loss: 0.0884 | 0.0607
Epoch 104/300, Loss: 0.0884 | 0.0607
Epoch 105/300, Loss: 0.0883 | 0.0607
Epoch 106/300, Loss: 0.0883 | 0.0607
Epoch 107/300, Loss: 0.0883 | 0.0606
Epoch 108/300, Loss: 0.0883 | 0.0606
Epoch 109/300, Loss: 0.0882 | 0.0606
Epoch 110/300, Loss: 0.0882 | 0.0606
Epoch 111/300, Loss: 0.0882 | 0.0606
Epoch 112/300, Loss: 0.0882 | 0.0605
Epoch 113/300, Loss: 0.0882 | 0.0605
Epoch 114/300, Loss: 0.0881 | 0.0605
Epoch 115/300, Loss: 0.0881 | 0.0605
Epoch 116/300, Loss: 0.0881 | 0.0605
Epoch 117/300, Loss: 0.0881 | 0.0605
Epoch 118/300, Loss: 0.0881 | 0.0604
Epoch 119/300, Loss: 0.0881 | 0.0604
Epoch 120/300, Loss: 0.0881 | 0.0604
Epoch 121/300, Loss: 0.0880 | 0.0604
Epoch 122/300, Loss: 0.0880 | 0.0604
Epoch 123/300, Loss: 0.0880 | 0.0604
Epoch 124/300, Loss: 0.0880 | 0.0604
Epoch 125/300, Loss: 0.0880 | 0.0603
Epoch 126/300, Loss: 0.0880 | 0.0603
Epoch 127/300, Loss: 0.0880 | 0.0603
Epoch 128/300, Loss: 0.0880 | 0.0603
Epoch 129/300, Loss: 0.0879 | 0.0603
Epoch 130/300, Loss: 0.0879 | 0.0603
Epoch 131/300, Loss: 0.0879 | 0.0603
Epoch 132/300, Loss: 0.0879 | 0.0603
Epoch 133/300, Loss: 0.0879 | 0.0603
Epoch 134/300, Loss: 0.0879 | 0.0603
Epoch 135/300, Loss: 0.0879 | 0.0603
Epoch 136/300, Loss: 0.0879 | 0.0602
Epoch 137/300, Loss: 0.0879 | 0.0602
Epoch 138/300, Loss: 0.0879 | 0.0602
Epoch 139/300, Loss: 0.0879 | 0.0602
Epoch 140/300, Loss: 0.0878 | 0.0602
Epoch 141/300, Loss: 0.0878 | 0.0602
Epoch 142/300, Loss: 0.0878 | 0.0602
Epoch 143/300, Loss: 0.0878 | 0.0602
Epoch 144/300, Loss: 0.0878 | 0.0602
Epoch 145/300, Loss: 0.0878 | 0.0602
Epoch 146/300, Loss: 0.0878 | 0.0602
Epoch 147/300, Loss: 0.0878 | 0.0602
Epoch 148/300, Loss: 0.0878 | 0.0602
Epoch 149/300, Loss: 0.0878 | 0.0602
Epoch 150/300, Loss: 0.0878 | 0.0602
Epoch 151/300, Loss: 0.0878 | 0.0602
Epoch 152/300, Loss: 0.0878 | 0.0602
Epoch 153/300, Loss: 0.0878 | 0.0602
Epoch 154/300, Loss: 0.0878 | 0.0601
Epoch 155/300, Loss: 0.0878 | 0.0601
Epoch 156/300, Loss: 0.0878 | 0.0601
Epoch 157/300, Loss: 0.0878 | 0.0601
Epoch 158/300, Loss: 0.0878 | 0.0601
Epoch 159/300, Loss: 0.0878 | 0.0601
Epoch 160/300, Loss: 0.0877 | 0.0601
Epoch 161/300, Loss: 0.0877 | 0.0601
Epoch 162/300, Loss: 0.0877 | 0.0601
Epoch 163/300, Loss: 0.0877 | 0.0601
Epoch 164/300, Loss: 0.0877 | 0.0601
Epoch 165/300, Loss: 0.0877 | 0.0601
Epoch 166/300, Loss: 0.0877 | 0.0601
Epoch 167/300, Loss: 0.0877 | 0.0601
Epoch 168/300, Loss: 0.0877 | 0.0601
Epoch 169/300, Loss: 0.0877 | 0.0601
Epoch 170/300, Loss: 0.0877 | 0.0601
Epoch 171/300, Loss: 0.0877 | 0.0601
Epoch 172/300, Loss: 0.0877 | 0.0601
Epoch 173/300, Loss: 0.0877 | 0.0601
Epoch 174/300, Loss: 0.0877 | 0.0601
Epoch 175/300, Loss: 0.0877 | 0.0601
Epoch 176/300, Loss: 0.0877 | 0.0601
Epoch 177/300, Loss: 0.0877 | 0.0601
Epoch 178/300, Loss: 0.0877 | 0.0601
Epoch 179/300, Loss: 0.0877 | 0.0601
Epoch 180/300, Loss: 0.0877 | 0.0601
Epoch 181/300, Loss: 0.0877 | 0.0601
Epoch 182/300, Loss: 0.0877 | 0.0601
Epoch 183/300, Loss: 0.0877 | 0.0601
Epoch 184/300, Loss: 0.0877 | 0.0601
Epoch 185/300, Loss: 0.0877 | 0.0601
Epoch 186/300, Loss: 0.0877 | 0.0601
Epoch 187/300, Loss: 0.0877 | 0.0601
Epoch 188/300, Loss: 0.0877 | 0.0601
Epoch 189/300, Loss: 0.0877 | 0.0601
Epoch 190/300, Loss: 0.0877 | 0.0601
Epoch 191/300, Loss: 0.0877 | 0.0601
Epoch 192/300, Loss: 0.0877 | 0.0601
Epoch 193/300, Loss: 0.0877 | 0.0601
Epoch 194/300, Loss: 0.0877 | 0.0601
Epoch 195/300, Loss: 0.0877 | 0.0601
Epoch 196/300, Loss: 0.0877 | 0.0601
Epoch 197/300, Loss: 0.0877 | 0.0601
Epoch 198/300, Loss: 0.0877 | 0.0601
Epoch 199/300, Loss: 0.0877 | 0.0601
Epoch 200/300, Loss: 0.0877 | 0.0601
Epoch 201/300, Loss: 0.0877 | 0.0601
Epoch 202/300, Loss: 0.0877 | 0.0601
Epoch 203/300, Loss: 0.0877 | 0.0601
Epoch 204/300, Loss: 0.0877 | 0.0601
Epoch 205/300, Loss: 0.0877 | 0.0601
Epoch 206/300, Loss: 0.0877 | 0.0601
Epoch 207/300, Loss: 0.0877 | 0.0601
Epoch 208/300, Loss: 0.0877 | 0.0601
Epoch 209/300, Loss: 0.0877 | 0.0601
Epoch 210/300, Loss: 0.0877 | 0.0601
Epoch 211/300, Loss: 0.0877 | 0.0601
Epoch 212/300, Loss: 0.0877 | 0.0601
Epoch 213/300, Loss: 0.0877 | 0.0601
Epoch 214/300, Loss: 0.0877 | 0.0601
Epoch 215/300, Loss: 0.0877 | 0.0601
Epoch 216/300, Loss: 0.0877 | 0.0601
Epoch 217/300, Loss: 0.0877 | 0.0601
Epoch 218/300, Loss: 0.0877 | 0.0601
Epoch 219/300, Loss: 0.0877 | 0.0601
Epoch 220/300, Loss: 0.0877 | 0.0601
Epoch 221/300, Loss: 0.0877 | 0.0601
Epoch 222/300, Loss: 0.0877 | 0.0601
Epoch 223/300, Loss: 0.0877 | 0.0601
Epoch 224/300, Loss: 0.0877 | 0.0601
Epoch 225/300, Loss: 0.0877 | 0.0601
Epoch 226/300, Loss: 0.0877 | 0.0601
Epoch 227/300, Loss: 0.0877 | 0.0601
Epoch 228/300, Loss: 0.0877 | 0.0601
Epoch 229/300, Loss: 0.0877 | 0.0601
Epoch 230/300, Loss: 0.0877 | 0.0601
Epoch 231/300, Loss: 0.0877 | 0.0601
Epoch 232/300, Loss: 0.0877 | 0.0601
Epoch 233/300, Loss: 0.0877 | 0.0601
Epoch 234/300, Loss: 0.0877 | 0.0601
Epoch 235/300, Loss: 0.0877 | 0.0601
Epoch 236/300, Loss: 0.0877 | 0.0601
Epoch 237/300, Loss: 0.0877 | 0.0601
Epoch 238/300, Loss: 0.0877 | 0.0601
Epoch 239/300, Loss: 0.0877 | 0.0601
Epoch 240/300, Loss: 0.0877 | 0.0601
Epoch 241/300, Loss: 0.0877 | 0.0601
Epoch 242/300, Loss: 0.0877 | 0.0601
Epoch 243/300, Loss: 0.0877 | 0.0601
Epoch 244/300, Loss: 0.0877 | 0.0601
Epoch 245/300, Loss: 0.0877 | 0.0601
Epoch 246/300, Loss: 0.0877 | 0.0601
Epoch 247/300, Loss: 0.0877 | 0.0601
Epoch 248/300, Loss: 0.0877 | 0.0601
Epoch 249/300, Loss: 0.0877 | 0.0601
Epoch 250/300, Loss: 0.0877 | 0.0601
Epoch 251/300, Loss: 0.0877 | 0.0601
Epoch 252/300, Loss: 0.0877 | 0.0601
Epoch 253/300, Loss: 0.0877 | 0.0601
Epoch 254/300, Loss: 0.0877 | 0.0601
Epoch 255/300, Loss: 0.0877 | 0.0601
Epoch 256/300, Loss: 0.0877 | 0.0601
Epoch 257/300, Loss: 0.0877 | 0.0601
Epoch 258/300, Loss: 0.0877 | 0.0601
Epoch 259/300, Loss: 0.0877 | 0.0601
Epoch 260/300, Loss: 0.0877 | 0.0601
Epoch 261/300, Loss: 0.0877 | 0.0601
Epoch 262/300, Loss: 0.0877 | 0.0601
Epoch 263/300, Loss: 0.0877 | 0.0601
Epoch 264/300, Loss: 0.0877 | 0.0601
Epoch 265/300, Loss: 0.0877 | 0.0601
Epoch 266/300, Loss: 0.0877 | 0.0601
Epoch 267/300, Loss: 0.0877 | 0.0601
Epoch 268/300, Loss: 0.0877 | 0.0601
Epoch 269/300, Loss: 0.0877 | 0.0601
Epoch 270/300, Loss: 0.0877 | 0.0601
Epoch 271/300, Loss: 0.0877 | 0.0601
Epoch 272/300, Loss: 0.0877 | 0.0601
Epoch 273/300, Loss: 0.0877 | 0.0601
Epoch 274/300, Loss: 0.0877 | 0.0601
Epoch 275/300, Loss: 0.0877 | 0.0601
Epoch 276/300, Loss: 0.0877 | 0.0601
Epoch 277/300, Loss: 0.0877 | 0.0601
Epoch 278/300, Loss: 0.0877 | 0.0601
Epoch 279/300, Loss: 0.0877 | 0.0601
Epoch 280/300, Loss: 0.0877 | 0.0601
Epoch 281/300, Loss: 0.0877 | 0.0601
Epoch 282/300, Loss: 0.0877 | 0.0601
Epoch 283/300, Loss: 0.0877 | 0.0601
Epoch 284/300, Loss: 0.0877 | 0.0601
Epoch 285/300, Loss: 0.0877 | 0.0601
Early stopping
Runtime (seconds): 157.39427828788757
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 59.986131163546816
RMSE: 7.7450714111328125
MAE: 7.7450714111328125
R-squared: nan
[197.89507]
