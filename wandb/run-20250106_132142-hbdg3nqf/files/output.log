ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 13:21:43,264][0m A new study created in memory with name: no-name-f03264d0-86f7-458a-8429-79ee0041bff3[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
Early stopping at epoch 61
[32m[I 2025-01-06 13:22:23,003][0m Trial 0 finished with value: 0.9114193709360229 and parameters: {'observation_period_num': 16, 'train_rates': 0.8530069149959357, 'learning_rate': 8.395425806349773e-06, 'batch_size': 102, 'step_size': 1, 'gamma': 0.8224477357849092}. Best is trial 0 with value: 0.9114193709360229.[0m
[32m[I 2025-01-06 13:23:01,937][0m Trial 1 finished with value: 0.3301775888872466 and parameters: {'observation_period_num': 148, 'train_rates': 0.6804889214690003, 'learning_rate': 1.689932451322812e-05, 'batch_size': 115, 'step_size': 3, 'gamma': 0.9633869311761413}. Best is trial 1 with value: 0.3301775888872466.[0m
[32m[I 2025-01-06 13:24:01,656][0m Trial 2 finished with value: 0.3358642763246304 and parameters: {'observation_period_num': 92, 'train_rates': 0.8018354541693057, 'learning_rate': 2.2656159505325715e-06, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8844625569163638}. Best is trial 1 with value: 0.3301775888872466.[0m
[32m[I 2025-01-06 13:24:41,542][0m Trial 3 finished with value: 0.3890956938266754 and parameters: {'observation_period_num': 102, 'train_rates': 0.9362844785282656, 'learning_rate': 7.88485002804529e-06, 'batch_size': 214, 'step_size': 7, 'gamma': 0.8003849470618613}. Best is trial 1 with value: 0.3301775888872466.[0m
[32m[I 2025-01-06 13:26:11,847][0m Trial 4 finished with value: 0.25065611223883344 and parameters: {'observation_period_num': 81, 'train_rates': 0.6975225588062052, 'learning_rate': 3.0204970102924113e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.889770444665989}. Best is trial 4 with value: 0.25065611223883344.[0m
Early stopping at epoch 87
[32m[I 2025-01-06 13:26:53,895][0m Trial 5 finished with value: 0.6699325201533189 and parameters: {'observation_period_num': 139, 'train_rates': 0.766865594658612, 'learning_rate': 1.12424308422135e-05, 'batch_size': 147, 'step_size': 1, 'gamma': 0.8848767710766485}. Best is trial 4 with value: 0.25065611223883344.[0m
[32m[I 2025-01-06 13:27:36,927][0m Trial 6 finished with value: 0.3740463027996676 and parameters: {'observation_period_num': 204, 'train_rates': 0.7352509136401675, 'learning_rate': 9.15636861163151e-05, 'batch_size': 130, 'step_size': 3, 'gamma': 0.8204645894604635}. Best is trial 4 with value: 0.25065611223883344.[0m
[32m[I 2025-01-06 13:28:09,984][0m Trial 7 finished with value: 0.22348786733652415 and parameters: {'observation_period_num': 117, 'train_rates': 0.6784043069344743, 'learning_rate': 0.00015563479088976558, 'batch_size': 234, 'step_size': 7, 'gamma': 0.883661868349644}. Best is trial 7 with value: 0.22348786733652415.[0m
[32m[I 2025-01-06 13:29:15,331][0m Trial 8 finished with value: 0.07939009970877574 and parameters: {'observation_period_num': 142, 'train_rates': 0.8800060222607263, 'learning_rate': 4.2569518221300214e-05, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9034248054088718}. Best is trial 8 with value: 0.07939009970877574.[0m
[32m[I 2025-01-06 13:29:41,019][0m Trial 9 finished with value: 0.6880927780006505 and parameters: {'observation_period_num': 228, 'train_rates': 0.7388106536696333, 'learning_rate': 2.0880288321923594e-05, 'batch_size': 239, 'step_size': 1, 'gamma': 0.9102776385145946}. Best is trial 8 with value: 0.07939009970877574.[0m
[32m[I 2025-01-06 13:33:37,143][0m Trial 10 finished with value: 0.12153707825237041 and parameters: {'observation_period_num': 175, 'train_rates': 0.9499895489538459, 'learning_rate': 0.0006190168399497606, 'batch_size': 23, 'step_size': 14, 'gamma': 0.7501189444944426}. Best is trial 8 with value: 0.07939009970877574.[0m
[32m[I 2025-01-06 13:39:23,321][0m Trial 11 finished with value: 0.07022218720780479 and parameters: {'observation_period_num': 181, 'train_rates': 0.9841010871186241, 'learning_rate': 0.000983739350668013, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7708839868381174}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:44:33,206][0m Trial 12 finished with value: 0.11211839086281325 and parameters: {'observation_period_num': 250, 'train_rates': 0.876499815411901, 'learning_rate': 0.0009028752204226853, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9492097501577033}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:46:00,411][0m Trial 13 finished with value: 0.09706775844097137 and parameters: {'observation_period_num': 177, 'train_rates': 0.9831009752872154, 'learning_rate': 0.000179257155415503, 'batch_size': 65, 'step_size': 11, 'gamma': 0.7719287143808071}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:46:43,986][0m Trial 14 finished with value: 0.6366508340632374 and parameters: {'observation_period_num': 49, 'train_rates': 0.8789683855719478, 'learning_rate': 1.28067438725766e-06, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8400659681756049}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:48:03,675][0m Trial 15 finished with value: 0.21875320279553337 and parameters: {'observation_period_num': 185, 'train_rates': 0.616225172768604, 'learning_rate': 0.0003611618481356152, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9325603663560674}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:48:39,450][0m Trial 16 finished with value: 0.10426457971334457 and parameters: {'observation_period_num': 148, 'train_rates': 0.9217201988129333, 'learning_rate': 4.8236045354986196e-05, 'batch_size': 174, 'step_size': 9, 'gamma': 0.9898149453670944}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:49:42,237][0m Trial 17 finished with value: 0.29772056446427925 and parameters: {'observation_period_num': 212, 'train_rates': 0.8224639799225008, 'learning_rate': 4.310801201741638e-06, 'batch_size': 82, 'step_size': 13, 'gamma': 0.85196255905599}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:52:15,937][0m Trial 18 finished with value: 0.08202384412288666 and parameters: {'observation_period_num': 66, 'train_rates': 0.9895464104807743, 'learning_rate': 0.0003256001067086928, 'batch_size': 38, 'step_size': 9, 'gamma': 0.7923491597438589}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:53:25,134][0m Trial 19 finished with value: 0.09591310423570171 and parameters: {'observation_period_num': 161, 'train_rates': 0.899292107682739, 'learning_rate': 9.102510125582368e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.9160185490029903}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:55:56,890][0m Trial 20 finished with value: 0.07469424433563943 and parameters: {'observation_period_num': 120, 'train_rates': 0.8454965424268359, 'learning_rate': 4.937559566020986e-05, 'batch_size': 37, 'step_size': 15, 'gamma': 0.8523776683604477}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 13:59:26,386][0m Trial 21 finished with value: 0.07393951866912533 and parameters: {'observation_period_num': 124, 'train_rates': 0.8363673502711478, 'learning_rate': 6.051905942136244e-05, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8562838907930214}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:02:28,656][0m Trial 22 finished with value: 0.07735427351144608 and parameters: {'observation_period_num': 119, 'train_rates': 0.8192852746244155, 'learning_rate': 6.127751630376706e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8588459840074066}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:04:44,595][0m Trial 23 finished with value: 0.08270913328115757 and parameters: {'observation_period_num': 109, 'train_rates': 0.840018310622954, 'learning_rate': 0.00015540797437155215, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8285930721578176}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:10:47,312][0m Trial 24 finished with value: 0.10886654104815413 and parameters: {'observation_period_num': 65, 'train_rates': 0.9477297719672929, 'learning_rate': 0.0003530643092723048, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7948643084824863}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:12:10,446][0m Trial 25 finished with value: 0.22991771562086358 and parameters: {'observation_period_num': 125, 'train_rates': 0.7747256200132571, 'learning_rate': 9.454836669779294e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8626180538154132}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:14:22,131][0m Trial 26 finished with value: 0.07750462478057282 and parameters: {'observation_period_num': 164, 'train_rates': 0.9126305299121323, 'learning_rate': 2.8046145658114343e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.7597661772207808}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:15:43,456][0m Trial 27 finished with value: 0.27401952704816473 and parameters: {'observation_period_num': 88, 'train_rates': 0.8575545887514118, 'learning_rate': 3.870476831633129e-06, 'batch_size': 65, 'step_size': 15, 'gamma': 0.7791598825154525}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:16:25,361][0m Trial 28 finished with value: 0.24258739786727407 and parameters: {'observation_period_num': 209, 'train_rates': 0.6097288030812104, 'learning_rate': 0.0009513664702823097, 'batch_size': 104, 'step_size': 12, 'gamma': 0.8104708520349492}. Best is trial 11 with value: 0.07022218720780479.[0m
[32m[I 2025-01-06 14:19:06,372][0m Trial 29 finished with value: 0.058890288579277694 and parameters: {'observation_period_num': 25, 'train_rates': 0.8468573000995744, 'learning_rate': 1.1808422493721218e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8410458669033719}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:20:39,145][0m Trial 30 finished with value: 0.13560776023079663 and parameters: {'observation_period_num': 23, 'train_rates': 0.7891958107089656, 'learning_rate': 9.275452538835573e-06, 'batch_size': 55, 'step_size': 5, 'gamma': 0.8356586281318757}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:23:29,473][0m Trial 31 finished with value: 0.059643619486855134 and parameters: {'observation_period_num': 8, 'train_rates': 0.8409852867396674, 'learning_rate': 5.360546075151468e-06, 'batch_size': 31, 'step_size': 9, 'gamma': 0.8470006720863839}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:26:46,900][0m Trial 32 finished with value: 0.06254542330175303 and parameters: {'observation_period_num': 16, 'train_rates': 0.8180617787551763, 'learning_rate': 6.202642738415662e-06, 'batch_size': 26, 'step_size': 9, 'gamma': 0.8107147604617722}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:29:31,879][0m Trial 33 finished with value: 0.06702967525400516 and parameters: {'observation_period_num': 6, 'train_rates': 0.802731457779608, 'learning_rate': 5.277879902952543e-06, 'batch_size': 30, 'step_size': 9, 'gamma': 0.8082047246068366}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:30:25,693][0m Trial 34 finished with value: 0.14819996267348737 and parameters: {'observation_period_num': 13, 'train_rates': 0.8054623549791549, 'learning_rate': 5.1563213495726564e-06, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8135090863262976}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:32:07,268][0m Trial 35 finished with value: 0.38391112871015604 and parameters: {'observation_period_num': 33, 'train_rates': 0.7460037658782601, 'learning_rate': 2.3055351083033658e-06, 'batch_size': 48, 'step_size': 8, 'gamma': 0.8381484833360956}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:33:18,752][0m Trial 36 finished with value: 0.09087616866952596 and parameters: {'observation_period_num': 7, 'train_rates': 0.7905049827520267, 'learning_rate': 1.3416835800724657e-05, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8051529154141818}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:34:18,656][0m Trial 37 finished with value: 0.10716779030931806 and parameters: {'observation_period_num': 32, 'train_rates': 0.8589733503766054, 'learning_rate': 7.1155965778675e-06, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8721565970224395}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:35:02,226][0m Trial 38 finished with value: 0.4464944061620275 and parameters: {'observation_period_num': 46, 'train_rates': 0.815107594129523, 'learning_rate': 2.6258260432254404e-06, 'batch_size': 132, 'step_size': 6, 'gamma': 0.8257956510115224}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:37:25,508][0m Trial 39 finished with value: 0.26454214162281264 and parameters: {'observation_period_num': 5, 'train_rates': 0.709142626286621, 'learning_rate': 1.5711515183652442e-06, 'batch_size': 33, 'step_size': 8, 'gamma': 0.7856676841632833}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:38:09,099][0m Trial 40 finished with value: 0.24193987286672358 and parameters: {'observation_period_num': 26, 'train_rates': 0.7619232732540363, 'learning_rate': 1.594924350901083e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.8710853059000574}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:43:10,244][0m Trial 41 finished with value: 0.2515682886075822 and parameters: {'observation_period_num': 40, 'train_rates': 0.7845077366525698, 'learning_rate': 5.9721452654913055e-06, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7689420998570408}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:44:33,127][0m Trial 42 finished with value: 0.35310098935654954 and parameters: {'observation_period_num': 57, 'train_rates': 0.6470604882649624, 'learning_rate': 3.1642188916807715e-06, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8182214174300847}. Best is trial 29 with value: 0.058890288579277694.[0m
[32m[I 2025-01-06 14:48:19,053][0m Trial 43 finished with value: 0.05472938804743216 and parameters: {'observation_period_num': 18, 'train_rates': 0.9010430249850242, 'learning_rate': 1.0168882140003834e-05, 'batch_size': 24, 'step_size': 8, 'gamma': 0.8053418411805509}. Best is trial 43 with value: 0.05472938804743216.[0m
[32m[I 2025-01-06 14:48:54,485][0m Trial 44 finished with value: 0.27937471209203496 and parameters: {'observation_period_num': 20, 'train_rates': 0.8955951619908129, 'learning_rate': 9.449835005575074e-06, 'batch_size': 207, 'step_size': 7, 'gamma': 0.802532312484948}. Best is trial 43 with value: 0.05472938804743216.[0m
[32m[I 2025-01-06 14:51:57,779][0m Trial 45 finished with value: 0.04454021825076048 and parameters: {'observation_period_num': 16, 'train_rates': 0.8643625236512765, 'learning_rate': 2.037700551585825e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8432835553122393}. Best is trial 45 with value: 0.04454021825076048.[0m
[32m[I 2025-01-06 14:52:22,187][0m Trial 46 finished with value: 0.0972655819076572 and parameters: {'observation_period_num': 18, 'train_rates': 0.865905299076444, 'learning_rate': 2.122444202288961e-05, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8469642193921121}. Best is trial 45 with value: 0.04454021825076048.[0m
[32m[I 2025-01-06 14:53:57,427][0m Trial 47 finished with value: 0.09467173121030757 and parameters: {'observation_period_num': 32, 'train_rates': 0.8968238389827417, 'learning_rate': 1.306237025339619e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.8234744193916171}. Best is trial 45 with value: 0.04454021825076048.[0m
[32m[I 2025-01-06 14:55:48,509][0m Trial 48 finished with value: 0.0706612365456997 and parameters: {'observation_period_num': 67, 'train_rates': 0.8345775241404648, 'learning_rate': 2.0199234889475343e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.8802426102583186}. Best is trial 45 with value: 0.04454021825076048.[0m
[32m[I 2025-01-06 14:56:27,883][0m Trial 49 finished with value: 0.17586547771901315 and parameters: {'observation_period_num': 44, 'train_rates': 0.9274585436198031, 'learning_rate': 7.54212202335598e-06, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8938387462804305}. Best is trial 45 with value: 0.04454021825076048.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 14:56:27,891][0m A new study created in memory with name: no-name-2037e2dd-1b74-42aa-abcd-9685e0788e4e[0m
[32m[I 2025-01-06 14:56:59,968][0m Trial 0 finished with value: 0.4161576287287502 and parameters: {'observation_period_num': 132, 'train_rates': 0.924688767240514, 'learning_rate': 2.4014532081339653e-06, 'batch_size': 196, 'step_size': 13, 'gamma': 0.9452446951974984}. Best is trial 0 with value: 0.4161576287287502.[0m
[32m[I 2025-01-06 14:57:23,046][0m Trial 1 finished with value: 1.03334023466388 and parameters: {'observation_period_num': 174, 'train_rates': 0.6291195405606863, 'learning_rate': 1.5987832118612636e-06, 'batch_size': 219, 'step_size': 12, 'gamma': 0.8004529658433078}. Best is trial 0 with value: 0.4161576287287502.[0m
[32m[I 2025-01-06 14:57:58,462][0m Trial 2 finished with value: 0.5321480651406476 and parameters: {'observation_period_num': 190, 'train_rates': 0.7520714011388114, 'learning_rate': 1.4930746045527338e-05, 'batch_size': 145, 'step_size': 9, 'gamma': 0.7516460902426676}. Best is trial 0 with value: 0.4161576287287502.[0m
[32m[I 2025-01-06 14:58:51,889][0m Trial 3 finished with value: 0.10673447698354721 and parameters: {'observation_period_num': 129, 'train_rates': 0.9690801769389394, 'learning_rate': 9.8933434713387e-05, 'batch_size': 111, 'step_size': 6, 'gamma': 0.9346748037338614}. Best is trial 3 with value: 0.10673447698354721.[0m
Early stopping at epoch 59
[32m[I 2025-01-06 14:59:08,156][0m Trial 4 finished with value: 0.44695457989552484 and parameters: {'observation_period_num': 73, 'train_rates': 0.7730465635619364, 'learning_rate': 0.00018863528069222037, 'batch_size': 231, 'step_size': 1, 'gamma': 0.7987594007470666}. Best is trial 3 with value: 0.10673447698354721.[0m
[32m[I 2025-01-06 15:00:21,415][0m Trial 5 finished with value: 0.11907053988356635 and parameters: {'observation_period_num': 93, 'train_rates': 0.8516152504282161, 'learning_rate': 0.0007244850872723154, 'batch_size': 73, 'step_size': 12, 'gamma': 0.760032181284351}. Best is trial 3 with value: 0.10673447698354721.[0m
[32m[I 2025-01-06 15:00:54,041][0m Trial 6 finished with value: 0.10086722563394168 and parameters: {'observation_period_num': 92, 'train_rates': 0.9137610317927107, 'learning_rate': 0.0004502879377863457, 'batch_size': 204, 'step_size': 14, 'gamma': 0.7898757786093686}. Best is trial 6 with value: 0.10086722563394168.[0m
[32m[I 2025-01-06 15:01:19,517][0m Trial 7 finished with value: 0.5671120596240973 and parameters: {'observation_period_num': 151, 'train_rates': 0.6694955962878789, 'learning_rate': 8.342909439103786e-06, 'batch_size': 240, 'step_size': 11, 'gamma': 0.8599206166295339}. Best is trial 6 with value: 0.10086722563394168.[0m
[32m[I 2025-01-06 15:01:52,437][0m Trial 8 finished with value: 0.2156172473263997 and parameters: {'observation_period_num': 122, 'train_rates': 0.9016564249801338, 'learning_rate': 1.1570685248139305e-05, 'batch_size': 249, 'step_size': 15, 'gamma': 0.916495645289825}. Best is trial 6 with value: 0.10086722563394168.[0m
[32m[I 2025-01-06 15:02:27,124][0m Trial 9 finished with value: 0.6671087499392234 and parameters: {'observation_period_num': 114, 'train_rates': 0.8130702514444705, 'learning_rate': 1.0750684255456891e-06, 'batch_size': 247, 'step_size': 7, 'gamma': 0.8517479203154699}. Best is trial 6 with value: 0.10086722563394168.[0m
[32m[I 2025-01-06 15:08:02,562][0m Trial 10 finished with value: 0.04906362839924392 and parameters: {'observation_period_num': 6, 'train_rates': 0.9685241801084397, 'learning_rate': 0.0007682179951311409, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9830545358355485}. Best is trial 10 with value: 0.04906362839924392.[0m
[32m[I 2025-01-06 15:11:19,768][0m Trial 11 finished with value: 0.05522449206440679 and parameters: {'observation_period_num': 12, 'train_rates': 0.9896086516532281, 'learning_rate': 0.0008921326456729626, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9810841181753077}. Best is trial 10 with value: 0.04906362839924392.[0m
[32m[I 2025-01-06 15:16:15,563][0m Trial 12 finished with value: 0.06271186143886752 and parameters: {'observation_period_num': 5, 'train_rates': 0.9862747523464802, 'learning_rate': 0.0009952438862156105, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9843664687871568}. Best is trial 10 with value: 0.04906362839924392.[0m
[32m[I 2025-01-06 15:21:48,498][0m Trial 13 finished with value: 0.03903630037481586 and parameters: {'observation_period_num': 10, 'train_rates': 0.9797436397446038, 'learning_rate': 8.450499018631672e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9829350533290653}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:23:10,481][0m Trial 14 finished with value: 0.1373311429633759 and parameters: {'observation_period_num': 245, 'train_rates': 0.8583542547865617, 'learning_rate': 4.1364101093533114e-05, 'batch_size': 59, 'step_size': 4, 'gamma': 0.898081316033425}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:24:30,227][0m Trial 15 finished with value: 0.06621821611015885 and parameters: {'observation_period_num': 42, 'train_rates': 0.9257878222081171, 'learning_rate': 0.0002282235867048464, 'batch_size': 71, 'step_size': 1, 'gamma': 0.9615785235867927}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:25:20,531][0m Trial 16 finished with value: 0.21438795011245793 and parameters: {'observation_period_num': 26, 'train_rates': 0.7139082172341518, 'learning_rate': 5.4000260479038015e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.8935953333384216}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:27:20,321][0m Trial 17 finished with value: 0.0854745331284118 and parameters: {'observation_period_num': 50, 'train_rates': 0.8742646987969198, 'learning_rate': 0.0002972333242708463, 'batch_size': 44, 'step_size': 9, 'gamma': 0.9889927415766562}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:27:59,063][0m Trial 18 finished with value: 0.08277329057455063 and parameters: {'observation_period_num': 57, 'train_rates': 0.9489419679188609, 'learning_rate': 0.00012535414687496024, 'batch_size': 162, 'step_size': 3, 'gamma': 0.9545723151950505}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:28:55,260][0m Trial 19 finished with value: 0.29816703692726465 and parameters: {'observation_period_num': 28, 'train_rates': 0.8111405581313751, 'learning_rate': 4.700532410474367e-06, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8363616462161031}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:31:01,354][0m Trial 20 finished with value: 0.16236944011215007 and parameters: {'observation_period_num': 248, 'train_rates': 0.951449405177455, 'learning_rate': 2.498626894175362e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9224170095710617}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:35:57,602][0m Trial 21 finished with value: 0.05351445684209466 and parameters: {'observation_period_num': 11, 'train_rates': 0.9894459372626863, 'learning_rate': 0.0006164609204089769, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9708638172194186}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:41:12,398][0m Trial 22 finished with value: 0.042400767365770954 and parameters: {'observation_period_num': 8, 'train_rates': 0.947515484340159, 'learning_rate': 0.0004337716982779767, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9660455560944389}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:43:07,723][0m Trial 23 finished with value: 0.1540323902920979 and parameters: {'observation_period_num': 65, 'train_rates': 0.9007527142146584, 'learning_rate': 0.0003839866998894938, 'batch_size': 48, 'step_size': 8, 'gamma': 0.9606497161136314}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:44:22,115][0m Trial 24 finished with value: 0.06297596499619894 and parameters: {'observation_period_num': 32, 'train_rates': 0.9483955960989428, 'learning_rate': 8.56989839775477e-05, 'batch_size': 81, 'step_size': 6, 'gamma': 0.9316164857697202}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:49:34,316][0m Trial 25 finished with value: 0.14849634446814797 and parameters: {'observation_period_num': 81, 'train_rates': 0.8897122781996317, 'learning_rate': 0.0001734843609946431, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8990720505079297}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:50:22,924][0m Trial 26 finished with value: 0.10637787480083147 and parameters: {'observation_period_num': 41, 'train_rates': 0.8400081084679557, 'learning_rate': 0.00047189273433849747, 'batch_size': 123, 'step_size': 8, 'gamma': 0.9679853577182356}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:53:14,571][0m Trial 27 finished with value: 0.053316406271559126 and parameters: {'observation_period_num': 22, 'train_rates': 0.9520645990246792, 'learning_rate': 0.00026426869860738755, 'batch_size': 37, 'step_size': 5, 'gamma': 0.9526915946889892}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:54:53,291][0m Trial 28 finished with value: 0.1485496000298914 and parameters: {'observation_period_num': 216, 'train_rates': 0.9420518867848536, 'learning_rate': 6.882974928843773e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8820598707009419}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:55:29,367][0m Trial 29 finished with value: 0.08169254835180657 and parameters: {'observation_period_num': 5, 'train_rates': 0.9188588017895469, 'learning_rate': 2.7698434613979105e-05, 'batch_size': 170, 'step_size': 7, 'gamma': 0.9448669395315337}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:56:42,113][0m Trial 30 finished with value: 0.07112258359372059 and parameters: {'observation_period_num': 54, 'train_rates': 0.9263567190291302, 'learning_rate': 0.00014097691387444407, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9402558802150736}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 15:59:31,622][0m Trial 31 finished with value: 0.05292411148548126 and parameters: {'observation_period_num': 24, 'train_rates': 0.9642630627856238, 'learning_rate': 0.0002812743822231015, 'batch_size': 35, 'step_size': 5, 'gamma': 0.9758516469108116}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:02:43,139][0m Trial 32 finished with value: 0.05347689402834425 and parameters: {'observation_period_num': 29, 'train_rates': 0.9639000586365809, 'learning_rate': 0.00036178281223330026, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9734055549131965}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:04:09,270][0m Trial 33 finished with value: 0.2173281212975398 and parameters: {'observation_period_num': 40, 'train_rates': 0.6093603349294618, 'learning_rate': 0.0005812621981863446, 'batch_size': 55, 'step_size': 7, 'gamma': 0.914784583562813}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:07:40,790][0m Trial 34 finished with value: 0.041419048901907236 and parameters: {'observation_period_num': 17, 'train_rates': 0.9709390972781025, 'learning_rate': 0.0002613155304813488, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9866433909933556}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:13:19,025][0m Trial 35 finished with value: 0.1020816894437092 and parameters: {'observation_period_num': 155, 'train_rates': 0.9753553236849175, 'learning_rate': 0.0001137148361681546, 'batch_size': 17, 'step_size': 10, 'gamma': 0.9895019689195045}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:14:49,999][0m Trial 36 finished with value: 0.05096382246858577 and parameters: {'observation_period_num': 16, 'train_rates': 0.9331834537509645, 'learning_rate': 0.00018788974625036505, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9541966910972666}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:17:56,082][0m Trial 37 finished with value: 0.13861510361380436 and parameters: {'observation_period_num': 68, 'train_rates': 0.8822282432147874, 'learning_rate': 0.0005725822300869388, 'batch_size': 29, 'step_size': 12, 'gamma': 0.931608838604034}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:19:32,703][0m Trial 38 finished with value: 0.23422755007918977 and parameters: {'observation_period_num': 101, 'train_rates': 0.7038135028438497, 'learning_rate': 7.274346185930473e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.9705949457645536}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:20:39,188][0m Trial 39 finished with value: 0.22254309348241713 and parameters: {'observation_period_num': 43, 'train_rates': 0.7687742020932212, 'learning_rate': 0.00042511079604992696, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8316342200392707}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:21:31,725][0m Trial 40 finished with value: 0.13120350834255512 and parameters: {'observation_period_num': 190, 'train_rates': 0.9064839435120619, 'learning_rate': 0.0008510598967339506, 'batch_size': 134, 'step_size': 8, 'gamma': 0.94513100269456}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:25:01,639][0m Trial 41 finished with value: 0.0485716859231124 and parameters: {'observation_period_num': 19, 'train_rates': 0.9369398399134727, 'learning_rate': 0.00018810382859632, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9561950834641594}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:28:33,341][0m Trial 42 finished with value: 0.06943389028310776 and parameters: {'observation_period_num': 19, 'train_rates': 0.9716210496634043, 'learning_rate': 0.0001617683673229881, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9884772891707565}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:31:04,712][0m Trial 43 finished with value: 0.06548553680101138 and parameters: {'observation_period_num': 6, 'train_rates': 0.9699756931197708, 'learning_rate': 0.00022926061390500039, 'batch_size': 40, 'step_size': 9, 'gamma': 0.965376005410051}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:37:01,607][0m Trial 44 finished with value: 0.06547099484785183 and parameters: {'observation_period_num': 80, 'train_rates': 0.9355657922739389, 'learning_rate': 4.242688622291397e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9786725367215839}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:40:23,856][0m Trial 45 finished with value: 0.09115630741182126 and parameters: {'observation_period_num': 36, 'train_rates': 0.8633038570892799, 'learning_rate': 9.421224228519095e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9527057387655262}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:41:30,881][0m Trial 46 finished with value: 0.05158608278814163 and parameters: {'observation_period_num': 13, 'train_rates': 0.9111084664690609, 'learning_rate': 0.0006963187837555372, 'batch_size': 222, 'step_size': 8, 'gamma': 0.9796116866817793}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:42:32,599][0m Trial 47 finished with value: 0.156773641705513 and parameters: {'observation_period_num': 53, 'train_rates': 0.9817960979955387, 'learning_rate': 1.8997160229140677e-05, 'batch_size': 196, 'step_size': 13, 'gamma': 0.7807866366834022}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:43:56,085][0m Trial 48 finished with value: 0.1429609462186014 and parameters: {'observation_period_num': 147, 'train_rates': 0.8281964395461566, 'learning_rate': 0.0003413424895090486, 'batch_size': 69, 'step_size': 7, 'gamma': 0.9240720285880999}. Best is trial 13 with value: 0.03903630037481586.[0m
[32m[I 2025-01-06 16:45:41,635][0m Trial 49 finished with value: 0.043989618703470394 and parameters: {'observation_period_num': 16, 'train_rates': 0.9598564002971349, 'learning_rate': 0.00020790655172833123, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9596970404863155}. Best is trial 13 with value: 0.03903630037481586.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 16:45:41,645][0m A new study created in memory with name: no-name-68fca396-e169-4305-badc-eb07f46744c9[0m
[32m[I 2025-01-06 16:46:23,971][0m Trial 0 finished with value: 0.2772409124166984 and parameters: {'observation_period_num': 92, 'train_rates': 0.6812603544131678, 'learning_rate': 0.00019876932804815787, 'batch_size': 115, 'step_size': 10, 'gamma': 0.9765199540848937}. Best is trial 0 with value: 0.2772409124166984.[0m
[32m[I 2025-01-06 16:46:59,649][0m Trial 1 finished with value: 0.18828160639907143 and parameters: {'observation_period_num': 26, 'train_rates': 0.7445744283008874, 'learning_rate': 0.0009586028929866687, 'batch_size': 217, 'step_size': 15, 'gamma': 0.9615951722288072}. Best is trial 1 with value: 0.18828160639907143.[0m
[32m[I 2025-01-06 16:47:39,014][0m Trial 2 finished with value: 1.2138829745658457 and parameters: {'observation_period_num': 107, 'train_rates': 0.7435406276767152, 'learning_rate': 1.0489592910387074e-06, 'batch_size': 154, 'step_size': 5, 'gamma': 0.8790596426291779}. Best is trial 1 with value: 0.18828160639907143.[0m
[32m[I 2025-01-06 16:48:25,435][0m Trial 3 finished with value: 0.13165342807769775 and parameters: {'observation_period_num': 189, 'train_rates': 0.9729084643026562, 'learning_rate': 6.43532936272898e-05, 'batch_size': 172, 'step_size': 13, 'gamma': 0.7540566153803052}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:49:44,195][0m Trial 4 finished with value: 0.4625542568951526 and parameters: {'observation_period_num': 217, 'train_rates': 0.624289181604975, 'learning_rate': 5.10264894118816e-06, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8769193550081948}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:50:36,266][0m Trial 5 finished with value: 0.7822970715514694 and parameters: {'observation_period_num': 158, 'train_rates': 0.7041097520542394, 'learning_rate': 1.1554492366018985e-06, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8577095358799736}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:52:30,716][0m Trial 6 finished with value: 0.25017476131936084 and parameters: {'observation_period_num': 197, 'train_rates': 0.9568917672620372, 'learning_rate': 5.837503932187002e-06, 'batch_size': 48, 'step_size': 11, 'gamma': 0.7942900366796853}. Best is trial 3 with value: 0.13165342807769775.[0m
Early stopping at epoch 98
[32m[I 2025-01-06 16:55:18,898][0m Trial 7 finished with value: 0.27386512949329905 and parameters: {'observation_period_num': 148, 'train_rates': 0.9566334316649949, 'learning_rate': 1.7257855042478966e-05, 'batch_size': 33, 'step_size': 2, 'gamma': 0.7853073476146845}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:56:01,117][0m Trial 8 finished with value: 0.2636967152434631 and parameters: {'observation_period_num': 119, 'train_rates': 0.6607633925379439, 'learning_rate': 0.0009179069775054545, 'batch_size': 111, 'step_size': 13, 'gamma': 0.984008781371916}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:56:35,211][0m Trial 9 finished with value: 0.43411184758110183 and parameters: {'observation_period_num': 32, 'train_rates': 0.8113859856157051, 'learning_rate': 4.361285193505442e-06, 'batch_size': 177, 'step_size': 6, 'gamma': 0.8302777886845999}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:57:04,546][0m Trial 10 finished with value: 0.1375087884128393 and parameters: {'observation_period_num': 252, 'train_rates': 0.8528306438619667, 'learning_rate': 9.83860738259728e-05, 'batch_size': 251, 'step_size': 15, 'gamma': 0.7549395676032511}. Best is trial 3 with value: 0.13165342807769775.[0m
[32m[I 2025-01-06 16:57:34,030][0m Trial 11 finished with value: 0.12802000925254398 and parameters: {'observation_period_num': 249, 'train_rates': 0.8754032115237237, 'learning_rate': 9.298300785370247e-05, 'batch_size': 253, 'step_size': 15, 'gamma': 0.756188746208811}. Best is trial 11 with value: 0.12802000925254398.[0m
[32m[I 2025-01-06 16:58:04,344][0m Trial 12 finished with value: 0.17027130441945643 and parameters: {'observation_period_num': 250, 'train_rates': 0.8961628247656119, 'learning_rate': 6.149927072057793e-05, 'batch_size': 203, 'step_size': 9, 'gamma': 0.7548002626854932}. Best is trial 11 with value: 0.12802000925254398.[0m
[32m[I 2025-01-06 16:58:37,241][0m Trial 13 finished with value: 0.11530251277459634 and parameters: {'observation_period_num': 197, 'train_rates': 0.9061699363331178, 'learning_rate': 0.00020207161869169037, 'batch_size': 249, 'step_size': 15, 'gamma': 0.8116820972528359}. Best is trial 13 with value: 0.11530251277459634.[0m
[32m[I 2025-01-06 16:59:16,063][0m Trial 14 finished with value: 0.11739006091522265 and parameters: {'observation_period_num': 221, 'train_rates': 0.8843911326045715, 'learning_rate': 0.0003119436726881423, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8085262470216904}. Best is trial 13 with value: 0.11530251277459634.[0m
[32m[I 2025-01-06 16:59:56,751][0m Trial 15 finished with value: 0.09091989201307296 and parameters: {'observation_period_num': 179, 'train_rates': 0.9099876639241964, 'learning_rate': 0.00037288117668068347, 'batch_size': 219, 'step_size': 8, 'gamma': 0.8259486904180663}. Best is trial 15 with value: 0.09091989201307296.[0m
[32m[I 2025-01-06 17:00:38,017][0m Trial 16 finished with value: 0.10815205604865633 and parameters: {'observation_period_num': 160, 'train_rates': 0.9169683657996621, 'learning_rate': 0.00030030562604481273, 'batch_size': 216, 'step_size': 7, 'gamma': 0.9122934831092684}. Best is trial 15 with value: 0.09091989201307296.[0m
[32m[I 2025-01-06 17:01:05,111][0m Trial 17 finished with value: 0.09098224906141267 and parameters: {'observation_period_num': 67, 'train_rates': 0.8115407736510944, 'learning_rate': 0.0004214479338743802, 'batch_size': 214, 'step_size': 7, 'gamma': 0.9226992023772775}. Best is trial 15 with value: 0.09091989201307296.[0m
[32m[I 2025-01-06 17:01:35,894][0m Trial 18 finished with value: 0.05605707397418363 and parameters: {'observation_period_num': 71, 'train_rates': 0.8395623262885912, 'learning_rate': 0.0005294483653979334, 'batch_size': 187, 'step_size': 4, 'gamma': 0.9233937452769749}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:02:09,025][0m Trial 19 finished with value: 0.13711542871437574 and parameters: {'observation_period_num': 66, 'train_rates': 0.8422093172070166, 'learning_rate': 2.0945226627221994e-05, 'batch_size': 186, 'step_size': 3, 'gamma': 0.9282536808161429}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:02:45,483][0m Trial 20 finished with value: 0.21528268639647632 and parameters: {'observation_period_num': 79, 'train_rates': 0.7722705091461752, 'learning_rate': 0.0005506958064005506, 'batch_size': 149, 'step_size': 4, 'gamma': 0.8477893743710504}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:03:14,773][0m Trial 21 finished with value: 0.07724594573877501 and parameters: {'observation_period_num': 57, 'train_rates': 0.8156709419499566, 'learning_rate': 0.0004643701496035694, 'batch_size': 219, 'step_size': 8, 'gamma': 0.9157519448470333}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:03:44,237][0m Trial 22 finished with value: 0.09737708317606072 and parameters: {'observation_period_num': 9, 'train_rates': 0.8385500143163503, 'learning_rate': 0.0001641444589239082, 'batch_size': 230, 'step_size': 1, 'gamma': 0.8937306612709479}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:04:14,427][0m Trial 23 finished with value: 0.19979742456624353 and parameters: {'observation_period_num': 42, 'train_rates': 0.7837708390038015, 'learning_rate': 0.0005613502010043467, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9558398666842588}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:04:45,424][0m Trial 24 finished with value: 0.09982714802026749 and parameters: {'observation_period_num': 139, 'train_rates': 0.9378170085785089, 'learning_rate': 0.0007510503808640773, 'batch_size': 228, 'step_size': 5, 'gamma': 0.9418158863999357}. Best is trial 18 with value: 0.05605707397418363.[0m
[32m[I 2025-01-06 17:05:22,371][0m Trial 25 finished with value: 0.04534265932352016 and parameters: {'observation_period_num': 48, 'train_rates': 0.8614675303694281, 'learning_rate': 0.00034930417837490153, 'batch_size': 163, 'step_size': 9, 'gamma': 0.897551614724832}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:06:01,361][0m Trial 26 finished with value: 0.0620868775551602 and parameters: {'observation_period_num': 55, 'train_rates': 0.8573436378190161, 'learning_rate': 0.00012229191089314378, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8978128188550001}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:06:46,405][0m Trial 27 finished with value: 0.08476339523397868 and parameters: {'observation_period_num': 93, 'train_rates': 0.859591314085523, 'learning_rate': 3.4873564473055974e-05, 'batch_size': 127, 'step_size': 10, 'gamma': 0.8932553133803253}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:07:23,667][0m Trial 28 finished with value: 0.05620293257394767 and parameters: {'observation_period_num': 46, 'train_rates': 0.8340272686180396, 'learning_rate': 0.00012059498052527782, 'batch_size': 156, 'step_size': 9, 'gamma': 0.9008259224276688}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:08:14,424][0m Trial 29 finished with value: 0.1601053904814479 and parameters: {'observation_period_num': 11, 'train_rates': 0.7378752491626294, 'learning_rate': 0.00019706631398663137, 'batch_size': 102, 'step_size': 9, 'gamma': 0.9389907261216277}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:08:54,050][0m Trial 30 finished with value: 0.2475125501789042 and parameters: {'observation_period_num': 88, 'train_rates': 0.764656408128334, 'learning_rate': 5.184536600182249e-05, 'batch_size': 136, 'step_size': 6, 'gamma': 0.9695408543663553}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:09:31,366][0m Trial 31 finished with value: 0.05775458199611629 and parameters: {'observation_period_num': 51, 'train_rates': 0.8285439405587206, 'learning_rate': 0.00012983283438246638, 'batch_size': 158, 'step_size': 10, 'gamma': 0.9013206928040105}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:10:06,969][0m Trial 32 finished with value: 0.05599444755490262 and parameters: {'observation_period_num': 45, 'train_rates': 0.8250083566122142, 'learning_rate': 0.0002015826231921938, 'batch_size': 169, 'step_size': 9, 'gamma': 0.9006748245136829}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:10:42,456][0m Trial 33 finished with value: 0.050813361412227746 and parameters: {'observation_period_num': 24, 'train_rates': 0.7929792061007543, 'learning_rate': 0.00023902249396959643, 'batch_size': 165, 'step_size': 9, 'gamma': 0.8605417782374452}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:11:17,290][0m Trial 34 finished with value: 0.05449726298784267 and parameters: {'observation_period_num': 25, 'train_rates': 0.8011157239321162, 'learning_rate': 0.0002579869408618615, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8605176843130042}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:11:50,920][0m Trial 35 finished with value: 0.1685044056612928 and parameters: {'observation_period_num': 26, 'train_rates': 0.7102887290874, 'learning_rate': 0.0002681768703972413, 'batch_size': 171, 'step_size': 7, 'gamma': 0.8586747700003621}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:12:37,554][0m Trial 36 finished with value: 0.05429199045301459 and parameters: {'observation_period_num': 19, 'train_rates': 0.7918258491537858, 'learning_rate': 0.00021580870975060546, 'batch_size': 128, 'step_size': 11, 'gamma': 0.8741471341539926}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:13:24,159][0m Trial 37 finished with value: 0.18221257130304971 and parameters: {'observation_period_num': 17, 'train_rates': 0.7546936526995378, 'learning_rate': 8.212021632639298e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8775512708195001}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:14:26,918][0m Trial 38 finished with value: 0.0575407192791779 and parameters: {'observation_period_num': 27, 'train_rates': 0.7979202235403523, 'learning_rate': 4.234240994797555e-05, 'batch_size': 87, 'step_size': 13, 'gamma': 0.8651927704023608}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:15:06,837][0m Trial 39 finished with value: 0.16935771312564613 and parameters: {'observation_period_num': 36, 'train_rates': 0.7257772525207075, 'learning_rate': 0.0007260367965891795, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8437269289517632}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:16:21,039][0m Trial 40 finished with value: 0.21670181553757362 and parameters: {'observation_period_num': 6, 'train_rates': 0.7870688427928729, 'learning_rate': 9.780629911713205e-06, 'batch_size': 71, 'step_size': 11, 'gamma': 0.8738501792938319}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:16:59,563][0m Trial 41 finished with value: 0.04819497186535607 and parameters: {'observation_period_num': 21, 'train_rates': 0.795036549036614, 'learning_rate': 0.00025341037860779786, 'batch_size': 168, 'step_size': 9, 'gamma': 0.8852540351220398}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:17:45,236][0m Trial 42 finished with value: 0.14966408693310967 and parameters: {'observation_period_num': 20, 'train_rates': 0.6756474309680333, 'learning_rate': 0.0002460924549660976, 'batch_size': 115, 'step_size': 11, 'gamma': 0.8845991972900135}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:18:26,941][0m Trial 43 finished with value: 0.054977940958886105 and parameters: {'observation_period_num': 33, 'train_rates': 0.7926434442177647, 'learning_rate': 0.00034607218203302294, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8446579247524106}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:19:09,708][0m Trial 44 finished with value: 0.23915695781891164 and parameters: {'observation_period_num': 105, 'train_rates': 0.7717186597483345, 'learning_rate': 0.0009737188619973498, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8670154690199444}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:19:48,985][0m Trial 45 finished with value: 0.17810810170565655 and parameters: {'observation_period_num': 19, 'train_rates': 0.7469410669324877, 'learning_rate': 0.00016680883951507487, 'batch_size': 176, 'step_size': 13, 'gamma': 0.8842950250645397}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:20:28,400][0m Trial 46 finished with value: 0.06203521491678071 and parameters: {'observation_period_num': 6, 'train_rates': 0.87199845759852, 'learning_rate': 6.660642142988261e-05, 'batch_size': 200, 'step_size': 12, 'gamma': 0.8339408501829768}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:21:17,276][0m Trial 47 finished with value: 0.04886804686652289 and parameters: {'observation_period_num': 36, 'train_rates': 0.8087548589832858, 'learning_rate': 0.00023498408314035187, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8615930392181615}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:22:04,368][0m Trial 48 finished with value: 0.05117842634589646 and parameters: {'observation_period_num': 35, 'train_rates': 0.8075449699024605, 'learning_rate': 0.00015139436524047863, 'batch_size': 121, 'step_size': 10, 'gamma': 0.8552008915006246}. Best is trial 25 with value: 0.04534265932352016.[0m
[32m[I 2025-01-06 17:23:00,436][0m Trial 49 finished with value: 0.3718883991241455 and parameters: {'observation_period_num': 39, 'train_rates': 0.9847839354406915, 'learning_rate': 1.8593259973887918e-06, 'batch_size': 115, 'step_size': 10, 'gamma': 0.8519247555483171}. Best is trial 25 with value: 0.04534265932352016.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 17:23:00,447][0m A new study created in memory with name: no-name-232c2047-2345-445a-ae5a-888e4a2aa523[0m
[32m[I 2025-01-06 17:23:29,821][0m Trial 0 finished with value: 0.46677405636571545 and parameters: {'observation_period_num': 233, 'train_rates': 0.6538945383802289, 'learning_rate': 2.5234232118445452e-05, 'batch_size': 212, 'step_size': 10, 'gamma': 0.7876190218226709}. Best is trial 0 with value: 0.46677405636571545.[0m
[32m[I 2025-01-06 17:24:06,280][0m Trial 1 finished with value: 0.32471088691037675 and parameters: {'observation_period_num': 239, 'train_rates': 0.8179627877002116, 'learning_rate': 8.118019338229517e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8337488511027177}. Best is trial 1 with value: 0.32471088691037675.[0m
[32m[I 2025-01-06 17:24:48,637][0m Trial 2 finished with value: 0.19662239059654918 and parameters: {'observation_period_num': 36, 'train_rates': 0.7629758111024961, 'learning_rate': 0.00019256332778119664, 'batch_size': 244, 'step_size': 10, 'gamma': 0.7736702930727689}. Best is trial 2 with value: 0.19662239059654918.[0m
[32m[I 2025-01-06 17:25:28,870][0m Trial 3 finished with value: 0.23107389118029414 and parameters: {'observation_period_num': 22, 'train_rates': 0.7035157041691281, 'learning_rate': 1.1261198992920032e-05, 'batch_size': 196, 'step_size': 15, 'gamma': 0.9248525120152324}. Best is trial 2 with value: 0.19662239059654918.[0m
[32m[I 2025-01-06 17:25:59,082][0m Trial 4 finished with value: 0.37452536821365356 and parameters: {'observation_period_num': 249, 'train_rates': 0.9364135124807234, 'learning_rate': 6.074546631929469e-06, 'batch_size': 226, 'step_size': 10, 'gamma': 0.8324336692901734}. Best is trial 2 with value: 0.19662239059654918.[0m
[32m[I 2025-01-06 17:27:43,044][0m Trial 5 finished with value: 0.10582178289240057 and parameters: {'observation_period_num': 234, 'train_rates': 0.9475040546843665, 'learning_rate': 3.713569053607331e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8207506144466982}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:28:22,342][0m Trial 6 finished with value: 0.15376773476600647 and parameters: {'observation_period_num': 119, 'train_rates': 0.9897757977276282, 'learning_rate': 2.5050069713307862e-05, 'batch_size': 209, 'step_size': 14, 'gamma': 0.8312513738489035}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:28:57,464][0m Trial 7 finished with value: 0.4931463363299046 and parameters: {'observation_period_num': 245, 'train_rates': 0.8423346431410631, 'learning_rate': 2.15238344164589e-06, 'batch_size': 208, 'step_size': 13, 'gamma': 0.8962275076940547}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:29:43,166][0m Trial 8 finished with value: 1.078393389527281 and parameters: {'observation_period_num': 176, 'train_rates': 0.7324337750494632, 'learning_rate': 1.8594092189900427e-06, 'batch_size': 110, 'step_size': 8, 'gamma': 0.7700847165852702}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:30:42,775][0m Trial 9 finished with value: 0.37722232591274174 and parameters: {'observation_period_num': 177, 'train_rates': 0.9072772762325165, 'learning_rate': 4.082973859072423e-06, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8672374031926957}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:36:13,906][0m Trial 10 finished with value: 0.12658806668745504 and parameters: {'observation_period_num': 90, 'train_rates': 0.8837583649481591, 'learning_rate': 0.00023456076830001988, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9856376601327851}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:40:39,251][0m Trial 11 finished with value: 0.12243454050586884 and parameters: {'observation_period_num': 82, 'train_rates': 0.8843834507209829, 'learning_rate': 0.0002660702281612399, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9891895240232691}. Best is trial 5 with value: 0.10582178289240057.[0m
[32m[I 2025-01-06 17:45:34,884][0m Trial 12 finished with value: 0.09381004219705408 and parameters: {'observation_period_num': 88, 'train_rates': 0.9885807557094941, 'learning_rate': 0.0007395815562730035, 'batch_size': 20, 'step_size': 2, 'gamma': 0.9887976479418472}. Best is trial 12 with value: 0.09381004219705408.[0m
[32m[I 2025-01-06 17:47:14,727][0m Trial 13 finished with value: 0.08070987462997437 and parameters: {'observation_period_num': 178, 'train_rates': 0.9874902228440466, 'learning_rate': 0.0009887994210763167, 'batch_size': 64, 'step_size': 2, 'gamma': 0.9482597834975276}. Best is trial 13 with value: 0.08070987462997437.[0m
[32m[I 2025-01-06 17:48:45,036][0m Trial 14 finished with value: 0.08307972550392151 and parameters: {'observation_period_num': 170, 'train_rates': 0.9863869184228627, 'learning_rate': 0.0009821187351666451, 'batch_size': 64, 'step_size': 1, 'gamma': 0.9506350111194263}. Best is trial 13 with value: 0.08070987462997437.[0m
[32m[I 2025-01-06 17:50:01,136][0m Trial 15 finished with value: 0.07842921167612076 and parameters: {'observation_period_num': 169, 'train_rates': 0.9444994052833496, 'learning_rate': 0.0008181452418564195, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9341322759232138}. Best is trial 15 with value: 0.07842921167612076.[0m
[32m[I 2025-01-06 17:51:05,462][0m Trial 16 finished with value: 0.10349316760827015 and parameters: {'observation_period_num': 198, 'train_rates': 0.8467311867099895, 'learning_rate': 0.00010495496805213483, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9359837429938953}. Best is trial 15 with value: 0.07842921167612076.[0m
[32m[I 2025-01-06 17:51:57,523][0m Trial 17 finished with value: 0.07209235926469167 and parameters: {'observation_period_num': 141, 'train_rates': 0.9360935217336255, 'learning_rate': 0.0004777991094576787, 'batch_size': 137, 'step_size': 2, 'gamma': 0.9028306556311239}. Best is trial 17 with value: 0.07209235926469167.[0m
[32m[I 2025-01-06 17:52:43,381][0m Trial 18 finished with value: 0.06677273045008505 and parameters: {'observation_period_num': 132, 'train_rates': 0.9295201635220592, 'learning_rate': 0.0003789173662386537, 'batch_size': 143, 'step_size': 6, 'gamma': 0.8937064034803079}. Best is trial 18 with value: 0.06677273045008505.[0m
[32m[I 2025-01-06 17:53:19,825][0m Trial 19 finished with value: 0.30081793722499156 and parameters: {'observation_period_num': 133, 'train_rates': 0.6216897762647245, 'learning_rate': 9.25247693748047e-05, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8869225545206554}. Best is trial 18 with value: 0.06677273045008505.[0m
[32m[I 2025-01-06 17:53:58,568][0m Trial 20 finished with value: 0.09575509455723641 and parameters: {'observation_period_num': 129, 'train_rates': 0.792825966551592, 'learning_rate': 0.0003815931345900589, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8703813795021297}. Best is trial 18 with value: 0.06677273045008505.[0m
[32m[I 2025-01-06 17:54:49,343][0m Trial 21 finished with value: 0.08840664218556374 and parameters: {'observation_period_num': 149, 'train_rates': 0.933193660925789, 'learning_rate': 0.0005223402982658858, 'batch_size': 125, 'step_size': 1, 'gamma': 0.9064506554345384}. Best is trial 18 with value: 0.06677273045008505.[0m
[32m[I 2025-01-06 17:55:37,884][0m Trial 22 finished with value: 0.06657634897539948 and parameters: {'observation_period_num': 110, 'train_rates': 0.8939346463010672, 'learning_rate': 0.00011145937169936345, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9179125239528997}. Best is trial 22 with value: 0.06657634897539948.[0m
[32m[I 2025-01-06 17:56:42,631][0m Trial 23 finished with value: 0.06793003604035813 and parameters: {'observation_period_num': 107, 'train_rates': 0.8810768557080206, 'learning_rate': 8.837241927530771e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.8615769117287574}. Best is trial 22 with value: 0.06657634897539948.[0m
[32m[I 2025-01-06 17:57:35,042][0m Trial 24 finished with value: 0.08023316043583888 and parameters: {'observation_period_num': 109, 'train_rates': 0.866699391324926, 'learning_rate': 8.303715952811794e-05, 'batch_size': 172, 'step_size': 8, 'gamma': 0.862083584645364}. Best is trial 22 with value: 0.06657634897539948.[0m
[32m[I 2025-01-06 17:58:36,528][0m Trial 25 finished with value: 0.06940843963197299 and parameters: {'observation_period_num': 64, 'train_rates': 0.9128467609410954, 'learning_rate': 5.189772495875589e-05, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8508824248849245}. Best is trial 22 with value: 0.06657634897539948.[0m
[32m[I 2025-01-06 17:59:32,332][0m Trial 26 finished with value: 0.053772814243155366 and parameters: {'observation_period_num': 54, 'train_rates': 0.81248666677265, 'learning_rate': 0.00014120017894633864, 'batch_size': 98, 'step_size': 8, 'gamma': 0.8081613287486809}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:00:23,754][0m Trial 27 finished with value: 0.0544686909247972 and parameters: {'observation_period_num': 51, 'train_rates': 0.8064620623526036, 'learning_rate': 0.0001792212969336524, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8004614594417728}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:01:18,967][0m Trial 28 finished with value: 0.05397035805777297 and parameters: {'observation_period_num': 50, 'train_rates': 0.796923839229775, 'learning_rate': 0.000153150736269737, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8018109085449491}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:02:19,846][0m Trial 29 finished with value: 0.11101765349609047 and parameters: {'observation_period_num': 52, 'train_rates': 0.7980705073627636, 'learning_rate': 2.113758499705675e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8007911582874245}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:04:39,406][0m Trial 30 finished with value: 0.14058873716149412 and parameters: {'observation_period_num': 13, 'train_rates': 0.6811835212342913, 'learning_rate': 0.00015777102661631721, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8044985113336613}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:05:44,394][0m Trial 31 finished with value: 0.2032331402540732 and parameters: {'observation_period_num': 59, 'train_rates': 0.7645778165730944, 'learning_rate': 0.00013392187625582786, 'batch_size': 92, 'step_size': 9, 'gamma': 0.7548674414089386}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:06:50,422][0m Trial 32 finished with value: 0.058614623905071675 and parameters: {'observation_period_num': 36, 'train_rates': 0.8181489795580439, 'learning_rate': 5.8339632521853076e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.7935729249625616}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:08:02,360][0m Trial 33 finished with value: 0.05876336435102067 and parameters: {'observation_period_num': 33, 'train_rates': 0.8184486818421132, 'learning_rate': 5.4387123757648883e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.8013506466739155}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:09:16,028][0m Trial 34 finished with value: 0.2115018993162957 and parameters: {'observation_period_num': 51, 'train_rates': 0.7564240150377992, 'learning_rate': 5.332279131347577e-05, 'batch_size': 89, 'step_size': 9, 'gamma': 0.7901117429858496}. Best is trial 26 with value: 0.053772814243155366.[0m
[32m[I 2025-01-06 18:09:55,614][0m Trial 35 finished with value: 0.04015365206093217 and parameters: {'observation_period_num': 7, 'train_rates': 0.8044711110803403, 'learning_rate': 0.00021224206852945554, 'batch_size': 165, 'step_size': 10, 'gamma': 0.7805582436202745}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:10:34,892][0m Trial 36 finished with value: 0.17941706888495185 and parameters: {'observation_period_num': 8, 'train_rates': 0.7848201225793978, 'learning_rate': 0.00020498344585506635, 'batch_size': 162, 'step_size': 10, 'gamma': 0.7757491051822802}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:11:08,423][0m Trial 37 finished with value: 0.1627345620163122 and parameters: {'observation_period_num': 25, 'train_rates': 0.7278634744237261, 'learning_rate': 0.0003330306453468115, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8212013759386434}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:11:53,265][0m Trial 38 finished with value: 0.19072666221804785 and parameters: {'observation_period_num': 73, 'train_rates': 0.8416357666822081, 'learning_rate': 1.683395239459693e-05, 'batch_size': 154, 'step_size': 7, 'gamma': 0.7573560670924031}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:12:26,653][0m Trial 39 finished with value: 0.23894296264175527 and parameters: {'observation_period_num': 43, 'train_rates': 0.7705504946567658, 'learning_rate': 3.43870294326016e-05, 'batch_size': 191, 'step_size': 12, 'gamma': 0.8146283165351311}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:13:13,584][0m Trial 40 finished with value: 0.8463590384926647 and parameters: {'observation_period_num': 22, 'train_rates': 0.7380349077819972, 'learning_rate': 1.0442947300040825e-06, 'batch_size': 187, 'step_size': 10, 'gamma': 0.7787788945590011}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:14:16,740][0m Trial 41 finished with value: 0.04615235897602328 and parameters: {'observation_period_num': 35, 'train_rates': 0.817392506890848, 'learning_rate': 0.00016694275752784436, 'batch_size': 108, 'step_size': 11, 'gamma': 0.7881536160371294}. Best is trial 35 with value: 0.04015365206093217.[0m
[32m[I 2025-01-06 18:15:02,761][0m Trial 42 finished with value: 0.037332724958499416 and parameters: {'observation_period_num': 7, 'train_rates': 0.8088436970100088, 'learning_rate': 0.0001662107846125909, 'batch_size': 121, 'step_size': 11, 'gamma': 0.8408959791848059}. Best is trial 42 with value: 0.037332724958499416.[0m
[32m[I 2025-01-06 18:15:51,792][0m Trial 43 finished with value: 0.0332045073861867 and parameters: {'observation_period_num': 9, 'train_rates': 0.8333729945903622, 'learning_rate': 0.0002406975539692526, 'batch_size': 120, 'step_size': 14, 'gamma': 0.8398722373928426}. Best is trial 43 with value: 0.0332045073861867.[0m
[32m[I 2025-01-06 18:16:43,312][0m Trial 44 finished with value: 0.03493810308909759 and parameters: {'observation_period_num': 5, 'train_rates': 0.8347836214211957, 'learning_rate': 0.00025958880073332524, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8442472592992513}. Best is trial 43 with value: 0.0332045073861867.[0m
[32m[I 2025-01-06 18:17:34,338][0m Trial 45 finished with value: 0.03435415180671186 and parameters: {'observation_period_num': 18, 'train_rates': 0.8572475943711291, 'learning_rate': 0.0002803046652966376, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8437988805327108}. Best is trial 43 with value: 0.0332045073861867.[0m
[32m[I 2025-01-06 18:18:28,418][0m Trial 46 finished with value: 0.03287018682756182 and parameters: {'observation_period_num': 8, 'train_rates': 0.8526313085320683, 'learning_rate': 0.00026846189681850623, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8390983336972552}. Best is trial 46 with value: 0.03287018682756182.[0m
[32m[I 2025-01-06 18:19:18,844][0m Trial 47 finished with value: 0.03850949605424535 and parameters: {'observation_period_num': 21, 'train_rates': 0.8571959207796717, 'learning_rate': 0.0006203347394828148, 'batch_size': 123, 'step_size': 15, 'gamma': 0.8427358664772222}. Best is trial 46 with value: 0.03287018682756182.[0m
[32m[I 2025-01-06 18:20:04,396][0m Trial 48 finished with value: 0.03261456232673178 and parameters: {'observation_period_num': 16, 'train_rates': 0.8366222479405765, 'learning_rate': 0.0002755448625132184, 'batch_size': 137, 'step_size': 14, 'gamma': 0.8385649722264282}. Best is trial 48 with value: 0.03261456232673178.[0m
[32m[I 2025-01-06 18:20:49,606][0m Trial 49 finished with value: 0.03515483879443235 and parameters: {'observation_period_num': 23, 'train_rates': 0.8372418340043624, 'learning_rate': 0.000328641176453887, 'batch_size': 138, 'step_size': 14, 'gamma': 0.8302724383102327}. Best is trial 48 with value: 0.03261456232673178.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-06 18:20:49,615][0m A new study created in memory with name: no-name-3c115bea-45b2-4e7a-8a62-49acb9555f27[0m
[32m[I 2025-01-06 18:21:35,748][0m Trial 0 finished with value: 0.18156513571739197 and parameters: {'observation_period_num': 203, 'train_rates': 0.9414386900166802, 'learning_rate': 2.4990004102456933e-05, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8334134016817083}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:23:21,823][0m Trial 1 finished with value: 0.38442591466216197 and parameters: {'observation_period_num': 146, 'train_rates': 0.6143196616496979, 'learning_rate': 3.5977734853582263e-06, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9412022381068506}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:23:56,242][0m Trial 2 finished with value: 0.21585550316856275 and parameters: {'observation_period_num': 84, 'train_rates': 0.6868242829150137, 'learning_rate': 0.0008434642240274181, 'batch_size': 211, 'step_size': 2, 'gamma': 0.8108124508181985}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:24:46,853][0m Trial 3 finished with value: 0.30404138716857987 and parameters: {'observation_period_num': 230, 'train_rates': 0.7835775295920467, 'learning_rate': 1.759276047926948e-05, 'batch_size': 219, 'step_size': 12, 'gamma': 0.7733260191492793}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:29:22,785][0m Trial 4 finished with value: 0.34986094203664037 and parameters: {'observation_period_num': 209, 'train_rates': 0.6411125539860946, 'learning_rate': 2.7505417431530916e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9278936692169144}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:29:57,298][0m Trial 5 finished with value: 0.3627110724546471 and parameters: {'observation_period_num': 178, 'train_rates': 0.9117405989567243, 'learning_rate': 1.5759822801312085e-05, 'batch_size': 229, 'step_size': 2, 'gamma': 0.8948514821936502}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:31:47,448][0m Trial 6 finished with value: 0.43919978892966494 and parameters: {'observation_period_num': 138, 'train_rates': 0.9743781390305479, 'learning_rate': 1.1614964773179614e-06, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8498232447286529}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:32:32,390][0m Trial 7 finished with value: 0.20646334806586797 and parameters: {'observation_period_num': 73, 'train_rates': 0.6204995300359467, 'learning_rate': 9.283061466513077e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.802632153866379}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:33:16,984][0m Trial 8 finished with value: 0.3052930136064391 and parameters: {'observation_period_num': 191, 'train_rates': 0.6852301100457714, 'learning_rate': 5.696181364174157e-05, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8457105309269803}. Best is trial 0 with value: 0.18156513571739197.[0m
[32m[I 2025-01-06 18:33:45,767][0m Trial 9 finished with value: 0.10264912992715836 and parameters: {'observation_period_num': 155, 'train_rates': 0.9258617794056421, 'learning_rate': 0.0007364017536195308, 'batch_size': 244, 'step_size': 6, 'gamma': 0.9686645439252667}. Best is trial 9 with value: 0.10264912992715836.[0m
[32m[I 2025-01-06 18:34:32,699][0m Trial 10 finished with value: 0.03724970590358354 and parameters: {'observation_period_num': 21, 'train_rates': 0.8552712441435135, 'learning_rate': 0.0004127878828508606, 'batch_size': 161, 'step_size': 6, 'gamma': 0.9787459211160705}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:35:18,149][0m Trial 11 finished with value: 0.041516551263995616 and parameters: {'observation_period_num': 18, 'train_rates': 0.8608887660213117, 'learning_rate': 0.0009988063686658965, 'batch_size': 167, 'step_size': 7, 'gamma': 0.9892732714257781}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:36:00,587][0m Trial 12 finished with value: 0.03997451454809267 and parameters: {'observation_period_num': 6, 'train_rates': 0.8513163051417972, 'learning_rate': 0.00022996520527015065, 'batch_size': 168, 'step_size': 8, 'gamma': 0.9859497296378911}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:36:47,631][0m Trial 13 finished with value: 0.04033669243093397 and parameters: {'observation_period_num': 17, 'train_rates': 0.8130144470534211, 'learning_rate': 0.00022995856518241134, 'batch_size': 166, 'step_size': 9, 'gamma': 0.9026825501024706}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:37:34,766][0m Trial 14 finished with value: 0.07490732360119913 and parameters: {'observation_period_num': 60, 'train_rates': 0.7885414029090642, 'learning_rate': 0.0002352406337994257, 'batch_size': 160, 'step_size': 4, 'gamma': 0.9598749090087186}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:38:38,603][0m Trial 15 finished with value: 0.057961078421321026 and parameters: {'observation_period_num': 47, 'train_rates': 0.8519531485528125, 'learning_rate': 0.00023318240123279432, 'batch_size': 131, 'step_size': 7, 'gamma': 0.9888856310144066}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:39:14,788][0m Trial 16 finished with value: 0.23743865709648773 and parameters: {'observation_period_num': 111, 'train_rates': 0.7329708706095596, 'learning_rate': 0.00013571819170200296, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9140134914083159}. Best is trial 10 with value: 0.03724970590358354.[0m
[32m[I 2025-01-06 18:40:37,070][0m Trial 17 finished with value: 0.03295286587672308 and parameters: {'observation_period_num': 11, 'train_rates': 0.8803492520714177, 'learning_rate': 0.0003361009930398204, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8815291736295032}. Best is trial 17 with value: 0.03295286587672308.[0m
[32m[I 2025-01-06 18:41:42,459][0m Trial 18 finished with value: 0.07538555987576333 and parameters: {'observation_period_num': 103, 'train_rates': 0.8881093424483383, 'learning_rate': 0.00042588121111636297, 'batch_size': 85, 'step_size': 4, 'gamma': 0.8774843075127401}. Best is trial 17 with value: 0.03295286587672308.[0m
Early stopping at epoch 43
[32m[I 2025-01-06 18:42:17,333][0m Trial 19 finished with value: 0.7923517227172852 and parameters: {'observation_period_num': 43, 'train_rates': 0.9761292316936331, 'learning_rate': 7.52357930381213e-06, 'batch_size': 82, 'step_size': 1, 'gamma': 0.759752325017006}. Best is trial 17 with value: 0.03295286587672308.[0m
[32m[I 2025-01-06 18:43:02,342][0m Trial 20 finished with value: 0.07037811023966442 and parameters: {'observation_period_num': 31, 'train_rates': 0.8147219930913506, 'learning_rate': 6.0123320793958956e-05, 'batch_size': 138, 'step_size': 4, 'gamma': 0.865861038487953}. Best is trial 17 with value: 0.03295286587672308.[0m
[32m[I 2025-01-06 18:43:47,898][0m Trial 21 finished with value: 0.029859499865108066 and parameters: {'observation_period_num': 5, 'train_rates': 0.8473465458141944, 'learning_rate': 0.00041912675790117164, 'batch_size': 143, 'step_size': 6, 'gamma': 0.9553175758568694}. Best is trial 21 with value: 0.029859499865108066.[0m
[32m[I 2025-01-06 18:44:37,536][0m Trial 22 finished with value: 0.04788086901150518 and parameters: {'observation_period_num': 33, 'train_rates': 0.878753748209233, 'learning_rate': 0.0004269449056522619, 'batch_size': 131, 'step_size': 5, 'gamma': 0.9485044694620993}. Best is trial 21 with value: 0.029859499865108066.[0m
[32m[I 2025-01-06 18:45:54,749][0m Trial 23 finished with value: 0.02934138737695418 and parameters: {'observation_period_num': 5, 'train_rates': 0.8347835452869053, 'learning_rate': 0.000442678255953135, 'batch_size': 73, 'step_size': 3, 'gamma': 0.9267838925302443}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:47:16,171][0m Trial 24 finished with value: 0.16443445415217897 and parameters: {'observation_period_num': 9, 'train_rates': 0.7558746086831283, 'learning_rate': 0.00011568417256799646, 'batch_size': 64, 'step_size': 3, 'gamma': 0.9254608944386418}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:48:18,228][0m Trial 25 finished with value: 0.06636644616013482 and parameters: {'observation_period_num': 59, 'train_rates': 0.8256806655329632, 'learning_rate': 0.00048336629554438437, 'batch_size': 93, 'step_size': 1, 'gamma': 0.8896019671908949}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:49:38,911][0m Trial 26 finished with value: 0.06123906020520083 and parameters: {'observation_period_num': 89, 'train_rates': 0.8977074294847306, 'learning_rate': 5.525726333798243e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.9341036653052414}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:50:35,446][0m Trial 27 finished with value: 0.05888702534139156 and parameters: {'observation_period_num': 48, 'train_rates': 0.947119941437492, 'learning_rate': 0.00016170979153399547, 'batch_size': 113, 'step_size': 5, 'gamma': 0.9096923641314791}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:53:09,996][0m Trial 28 finished with value: 0.22769631768672285 and parameters: {'observation_period_num': 67, 'train_rates': 0.7667947519635927, 'learning_rate': 0.0006090009452343433, 'batch_size': 36, 'step_size': 3, 'gamma': 0.9519081832647952}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:54:10,620][0m Trial 29 finished with value: 0.10621523574241959 and parameters: {'observation_period_num': 251, 'train_rates': 0.8214288347919853, 'learning_rate': 0.00036897008858500553, 'batch_size': 113, 'step_size': 6, 'gamma': 0.8755975813443585}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:55:12,854][0m Trial 30 finished with value: 0.09087326307917198 and parameters: {'observation_period_num': 31, 'train_rates': 0.9414470176290679, 'learning_rate': 8.616338221943786e-05, 'batch_size': 145, 'step_size': 2, 'gamma': 0.8254962874447994}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:56:14,598][0m Trial 31 finished with value: 0.03826291214462389 and parameters: {'observation_period_num': 26, 'train_rates': 0.8681431627835445, 'learning_rate': 0.0003145318623834593, 'batch_size': 186, 'step_size': 6, 'gamma': 0.9666078585187752}. Best is trial 23 with value: 0.02934138737695418.[0m
[32m[I 2025-01-06 18:57:23,064][0m Trial 32 finished with value: 0.029181752459979816 and parameters: {'observation_period_num': 7, 'train_rates': 0.8508633419045722, 'learning_rate': 0.0006639530582666196, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9416458963466766}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 18:58:21,920][0m Trial 33 finished with value: 0.03136319406933871 and parameters: {'observation_period_num': 5, 'train_rates': 0.8323798770950855, 'learning_rate': 0.000627945218718123, 'batch_size': 193, 'step_size': 15, 'gamma': 0.9185011327371606}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 18:59:04,947][0m Trial 34 finished with value: 0.058271225245931244 and parameters: {'observation_period_num': 42, 'train_rates': 0.8313732544852221, 'learning_rate': 0.0006654477446914846, 'batch_size': 192, 'step_size': 9, 'gamma': 0.9435511010184355}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:00:05,014][0m Trial 35 finished with value: 0.0332906005193585 and parameters: {'observation_period_num': 5, 'train_rates': 0.8026431970055733, 'learning_rate': 0.0009948841497563797, 'batch_size': 201, 'step_size': 15, 'gamma': 0.9191800402326662}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:01:04,458][0m Trial 36 finished with value: 0.15522295120023524 and parameters: {'observation_period_num': 5, 'train_rates': 0.7341799035405121, 'learning_rate': 0.0006186419938757308, 'batch_size': 151, 'step_size': 12, 'gamma': 0.9366893754569833}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:02:05,535][0m Trial 37 finished with value: 0.08809766779429483 and parameters: {'observation_period_num': 79, 'train_rates': 0.8394772829802164, 'learning_rate': 0.0001685989161733301, 'batch_size': 178, 'step_size': 14, 'gamma': 0.9556798366715383}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:03:05,149][0m Trial 38 finished with value: 0.13552721444354116 and parameters: {'observation_period_num': 56, 'train_rates': 0.9135262059612086, 'learning_rate': 1.4204544212754102e-05, 'batch_size': 215, 'step_size': 10, 'gamma': 0.9002353607061129}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:03:51,605][0m Trial 39 finished with value: 1.347154589869627 and parameters: {'observation_period_num': 161, 'train_rates': 0.765076536329532, 'learning_rate': 1.3195973590100493e-06, 'batch_size': 229, 'step_size': 7, 'gamma': 0.9241958440162649}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:07:14,773][0m Trial 40 finished with value: 0.21806279178275617 and parameters: {'observation_period_num': 35, 'train_rates': 0.7788469565766212, 'learning_rate': 0.000632082886788669, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9428289050055423}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:09:11,582][0m Trial 41 finished with value: 0.029295270430025947 and parameters: {'observation_period_num': 18, 'train_rates': 0.8940033716371123, 'learning_rate': 0.00031452438948805774, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8905671912074514}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:10:57,267][0m Trial 42 finished with value: 0.04455953147262335 and parameters: {'observation_period_num': 24, 'train_rates': 0.8360029804395114, 'learning_rate': 0.0002927617120302009, 'batch_size': 54, 'step_size': 5, 'gamma': 0.8930487060106955}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:17:06,362][0m Trial 43 finished with value: 0.04099270041230359 and parameters: {'observation_period_num': 20, 'train_rates': 0.9192432424731912, 'learning_rate': 0.0005377638421647366, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9110210164569541}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:19:06,112][0m Trial 44 finished with value: 0.03537530283548523 and parameters: {'observation_period_num': 18, 'train_rates': 0.9003222879655668, 'learning_rate': 0.0008245887949987071, 'batch_size': 51, 'step_size': 6, 'gamma': 0.96894715780047}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:20:04,181][0m Trial 45 finished with value: 0.05974951023936913 and parameters: {'observation_period_num': 38, 'train_rates': 0.8725906594593771, 'learning_rate': 3.984082475446655e-05, 'batch_size': 121, 'step_size': 9, 'gamma': 0.86345113637903}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:21:07,254][0m Trial 46 finished with value: 0.09742472990769749 and parameters: {'observation_period_num': 128, 'train_rates': 0.9543821758273945, 'learning_rate': 0.00019741758817851793, 'batch_size': 103, 'step_size': 5, 'gamma': 0.9322994628818843}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:21:50,789][0m Trial 47 finished with value: 0.03521577185736253 and parameters: {'observation_period_num': 16, 'train_rates': 0.8009084776634837, 'learning_rate': 0.0002668355876795166, 'batch_size': 151, 'step_size': 8, 'gamma': 0.9752239016718902}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:22:33,561][0m Trial 48 finished with value: 0.03226900966410042 and parameters: {'observation_period_num': 5, 'train_rates': 0.8449800509100732, 'learning_rate': 0.0008462477376876618, 'batch_size': 205, 'step_size': 15, 'gamma': 0.8490507099070187}. Best is trial 32 with value: 0.029181752459979816.[0m
[32m[I 2025-01-06 19:23:24,158][0m Trial 49 finished with value: 0.1972190426985423 and parameters: {'observation_period_num': 206, 'train_rates': 0.8636327113327548, 'learning_rate': 0.00010278249859517947, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8006471843213165}. Best is trial 32 with value: 0.029181752459979816.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 19:23:24,167][0m A new study created in memory with name: no-name-2bb7f3f7-5483-49fc-ae7e-f1a30655c626[0m
[32m[I 2025-01-06 19:24:10,471][0m Trial 0 finished with value: 0.2551143292280344 and parameters: {'observation_period_num': 138, 'train_rates': 0.7690301517381436, 'learning_rate': 0.0006468661990437621, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8574438057221206}. Best is trial 0 with value: 0.2551143292280344.[0m
[32m[I 2025-01-06 19:25:08,275][0m Trial 1 finished with value: 0.3381022102677017 and parameters: {'observation_period_num': 248, 'train_rates': 0.6661146589770234, 'learning_rate': 3.118482568565465e-05, 'batch_size': 80, 'step_size': 15, 'gamma': 0.8323899597695901}. Best is trial 0 with value: 0.2551143292280344.[0m
Early stopping at epoch 71
[32m[I 2025-01-06 19:25:31,511][0m Trial 2 finished with value: 0.9363996258997591 and parameters: {'observation_period_num': 123, 'train_rates': 0.7670514656122, 'learning_rate': 2.867018999932959e-06, 'batch_size': 212, 'step_size': 2, 'gamma': 0.7741301962942255}. Best is trial 0 with value: 0.2551143292280344.[0m
[32m[I 2025-01-06 19:26:21,862][0m Trial 3 finished with value: 0.09519969094668003 and parameters: {'observation_period_num': 98, 'train_rates': 0.9472774672664088, 'learning_rate': 2.4679497350526152e-05, 'batch_size': 118, 'step_size': 11, 'gamma': 0.8982857023532573}. Best is trial 3 with value: 0.09519969094668003.[0m
[32m[I 2025-01-06 19:27:04,544][0m Trial 4 finished with value: 0.5015590039809422 and parameters: {'observation_period_num': 148, 'train_rates': 0.7684232841495672, 'learning_rate': 4.639441976592256e-06, 'batch_size': 125, 'step_size': 13, 'gamma': 0.9259083734069058}. Best is trial 3 with value: 0.09519969094668003.[0m
[32m[I 2025-01-06 19:27:56,922][0m Trial 5 finished with value: 0.06452242116774282 and parameters: {'observation_period_num': 97, 'train_rates': 0.9239970283522896, 'learning_rate': 0.0009569404776235913, 'batch_size': 114, 'step_size': 3, 'gamma': 0.8250701482660331}. Best is trial 5 with value: 0.06452242116774282.[0m
[32m[I 2025-01-06 19:29:10,681][0m Trial 6 finished with value: 0.1859461570588442 and parameters: {'observation_period_num': 36, 'train_rates': 0.7102914665907094, 'learning_rate': 3.670804847901508e-05, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8643795322690985}. Best is trial 5 with value: 0.06452242116774282.[0m
[32m[I 2025-01-06 19:32:44,245][0m Trial 7 finished with value: 0.04411526617001404 and parameters: {'observation_period_num': 24, 'train_rates': 0.8198601546303796, 'learning_rate': 0.000301189305746826, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8464915894107863}. Best is trial 7 with value: 0.04411526617001404.[0m
[32m[I 2025-01-06 19:33:28,832][0m Trial 8 finished with value: 0.17063853473347776 and parameters: {'observation_period_num': 244, 'train_rates': 0.8244366234588345, 'learning_rate': 0.00047844213795088556, 'batch_size': 150, 'step_size': 7, 'gamma': 0.9642472106836752}. Best is trial 7 with value: 0.04411526617001404.[0m
[32m[I 2025-01-06 19:34:20,496][0m Trial 9 finished with value: 0.16944448786091326 and parameters: {'observation_period_num': 143, 'train_rates': 0.805483939536263, 'learning_rate': 2.544967315619555e-05, 'batch_size': 136, 'step_size': 3, 'gamma': 0.9713524882964896}. Best is trial 7 with value: 0.04411526617001404.[0m
[32m[I 2025-01-06 19:37:12,189][0m Trial 10 finished with value: 0.0347258592822722 and parameters: {'observation_period_num': 6, 'train_rates': 0.8812984889932698, 'learning_rate': 0.00015605652161351075, 'batch_size': 34, 'step_size': 5, 'gamma': 0.7665159425502898}. Best is trial 10 with value: 0.0347258592822722.[0m
[32m[I 2025-01-06 19:42:29,983][0m Trial 11 finished with value: 0.032664858954756154 and parameters: {'observation_period_num': 19, 'train_rates': 0.8823294646275693, 'learning_rate': 0.00014657149880752403, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7532551440831766}. Best is trial 11 with value: 0.032664858954756154.[0m
[32m[I 2025-01-06 19:47:32,462][0m Trial 12 finished with value: 0.03145599544235093 and parameters: {'observation_period_num': 6, 'train_rates': 0.8783716380087113, 'learning_rate': 0.00011847500458471152, 'batch_size': 18, 'step_size': 5, 'gamma': 0.7541003702869485}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:48:56,677][0m Trial 13 finished with value: 0.05133971863821761 and parameters: {'observation_period_num': 56, 'train_rates': 0.8740902096393575, 'learning_rate': 0.0001405075613872828, 'batch_size': 66, 'step_size': 10, 'gamma': 0.7982064432520183}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:49:35,553][0m Trial 14 finished with value: 0.11602502316236496 and parameters: {'observation_period_num': 60, 'train_rates': 0.9834218227252918, 'learning_rate': 9.119054847250973e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.7526462273306249}. Best is trial 12 with value: 0.03145599544235093.[0m
Early stopping at epoch 56
[32m[I 2025-01-06 19:52:14,079][0m Trial 15 finished with value: 0.30759397756494034 and parameters: {'observation_period_num': 189, 'train_rates': 0.8765113633026616, 'learning_rate': 1.1120047003993722e-05, 'batch_size': 19, 'step_size': 1, 'gamma': 0.7997734796206198}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:53:59,993][0m Trial 16 finished with value: 0.03500760704961442 and parameters: {'observation_period_num': 5, 'train_rates': 0.9112494482419586, 'learning_rate': 8.421995287918055e-05, 'batch_size': 55, 'step_size': 9, 'gamma': 0.7952192751183103}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:55:08,140][0m Trial 17 finished with value: 1.683540940284729 and parameters: {'observation_period_num': 69, 'train_rates': 0.9858032699253285, 'learning_rate': 1.2366566247975098e-06, 'batch_size': 94, 'step_size': 4, 'gamma': 0.7510938419124885}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:55:45,295][0m Trial 18 finished with value: 0.20040451517977445 and parameters: {'observation_period_num': 36, 'train_rates': 0.619016764925921, 'learning_rate': 0.0003051167253273827, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8993932120811106}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:57:42,200][0m Trial 19 finished with value: 0.09001765258387844 and parameters: {'observation_period_num': 187, 'train_rates': 0.8546429375921903, 'learning_rate': 5.861615572833772e-05, 'batch_size': 45, 'step_size': 8, 'gamma': 0.781597467933659}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 19:58:48,927][0m Trial 20 finished with value: 0.06904122673061698 and parameters: {'observation_period_num': 79, 'train_rates': 0.9402795116581846, 'learning_rate': 0.00019707148520656316, 'batch_size': 97, 'step_size': 5, 'gamma': 0.8130612914181263}. Best is trial 12 with value: 0.03145599544235093.[0m
[32m[I 2025-01-06 20:01:21,639][0m Trial 21 finished with value: 0.030534792787605715 and parameters: {'observation_period_num': 5, 'train_rates': 0.8948219621134168, 'learning_rate': 0.0001751408728097162, 'batch_size': 37, 'step_size': 5, 'gamma': 0.7683624516725148}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:06:31,482][0m Trial 22 finished with value: 0.05030599152054086 and parameters: {'observation_period_num': 27, 'train_rates': 0.8498184809230449, 'learning_rate': 0.0002867390842629003, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7721081441454216}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:08:36,333][0m Trial 23 finished with value: 0.052769953151416166 and parameters: {'observation_period_num': 6, 'train_rates': 0.8943902397025147, 'learning_rate': 6.652609840531636e-05, 'batch_size': 45, 'step_size': 4, 'gamma': 0.7516938406539015}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:11:01,308][0m Trial 24 finished with value: 0.05038258759304881 and parameters: {'observation_period_num': 42, 'train_rates': 0.9560089742776077, 'learning_rate': 0.00016381891962503276, 'batch_size': 40, 'step_size': 7, 'gamma': 0.7849963470664705}. Best is trial 21 with value: 0.030534792787605715.[0m
Early stopping at epoch 63
[32m[I 2025-01-06 20:11:48,542][0m Trial 25 finished with value: 0.27115487011239 and parameters: {'observation_period_num': 21, 'train_rates': 0.8439279710498018, 'learning_rate': 1.4264479684645784e-05, 'batch_size': 76, 'step_size': 1, 'gamma': 0.8116679464026616}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:16:54,576][0m Trial 26 finished with value: 0.0738266024102525 and parameters: {'observation_period_num': 50, 'train_rates': 0.9151850260753653, 'learning_rate': 0.00010930673447261184, 'batch_size': 18, 'step_size': 3, 'gamma': 0.7673152962198355}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:18:35,985][0m Trial 27 finished with value: 0.2266100811667164 and parameters: {'observation_period_num': 96, 'train_rates': 0.7418068406396698, 'learning_rate': 4.6395285594428524e-05, 'batch_size': 48, 'step_size': 6, 'gamma': 0.8347749402070567}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:19:13,953][0m Trial 28 finished with value: 0.03261906020343304 and parameters: {'observation_period_num': 19, 'train_rates': 0.8960400719729609, 'learning_rate': 0.0004492976325885665, 'batch_size': 183, 'step_size': 9, 'gamma': 0.8851430152889624}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:19:50,344][0m Trial 29 finished with value: 0.08288678533719458 and parameters: {'observation_period_num': 78, 'train_rates': 0.7917664897092952, 'learning_rate': 0.000655293150973535, 'batch_size': 171, 'step_size': 9, 'gamma': 0.8785848692546945}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:20:23,182][0m Trial 30 finished with value: 0.07199175655841827 and parameters: {'observation_period_num': 46, 'train_rates': 0.9584659569019596, 'learning_rate': 0.0005173750538052691, 'batch_size': 206, 'step_size': 10, 'gamma': 0.9427040664075416}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:20:53,790][0m Trial 31 finished with value: 0.03604026000997792 and parameters: {'observation_period_num': 20, 'train_rates': 0.9005179819263068, 'learning_rate': 0.00027790915397807894, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8844951256862448}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:21:22,488][0m Trial 32 finished with value: 0.045707035383478326 and parameters: {'observation_period_num': 22, 'train_rates': 0.8628224012056774, 'learning_rate': 0.0009404150670719253, 'batch_size': 242, 'step_size': 8, 'gamma': 0.9163004805132198}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:21:55,223][0m Trial 33 finished with value: 0.04361955184006292 and parameters: {'observation_period_num': 13, 'train_rates': 0.8373480651348275, 'learning_rate': 0.0004049818094416302, 'batch_size': 181, 'step_size': 4, 'gamma': 0.843703054453342}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:22:35,226][0m Trial 34 finished with value: 0.058052364620983295 and parameters: {'observation_period_num': 32, 'train_rates': 0.9339896471683939, 'learning_rate': 0.00022005850390286816, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9885031542677735}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:25:08,178][0m Trial 35 finished with value: 0.08292385589950248 and parameters: {'observation_period_num': 118, 'train_rates': 0.8918524002241568, 'learning_rate': 0.00010773950435204582, 'batch_size': 35, 'step_size': 6, 'gamma': 0.7609431354255192}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:26:04,722][0m Trial 36 finished with value: 0.1112281379620891 and parameters: {'observation_period_num': 171, 'train_rates': 0.7823840875072032, 'learning_rate': 0.000743409539177985, 'batch_size': 90, 'step_size': 7, 'gamma': 0.7848235092507815}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:26:35,071][0m Trial 37 finished with value: 0.23601431277574308 and parameters: {'observation_period_num': 65, 'train_rates': 0.7420676246524045, 'learning_rate': 0.00040628510525803584, 'batch_size': 224, 'step_size': 2, 'gamma': 0.8590508035800597}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:28:07,974][0m Trial 38 finished with value: 0.051227409297075026 and parameters: {'observation_period_num': 40, 'train_rates': 0.9198657746549662, 'learning_rate': 6.475941951078659e-05, 'batch_size': 61, 'step_size': 12, 'gamma': 0.8167877317403011}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:28:42,037][0m Trial 39 finished with value: 0.1353075407954105 and parameters: {'observation_period_num': 230, 'train_rates': 0.8170392937279876, 'learning_rate': 0.00013994049346511642, 'batch_size': 188, 'step_size': 5, 'gamma': 0.9107986473682456}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:29:35,279][0m Trial 40 finished with value: 0.06363324075937271 and parameters: {'observation_period_num': 17, 'train_rates': 0.9656243907673122, 'learning_rate': 3.8042316422210346e-05, 'batch_size': 114, 'step_size': 8, 'gamma': 0.8886086214485036}. Best is trial 21 with value: 0.030534792787605715.[0m
[32m[I 2025-01-06 20:32:07,442][0m Trial 41 finished with value: 0.029870987446470695 and parameters: {'observation_period_num': 6, 'train_rates': 0.880570471003369, 'learning_rate': 0.00017389891457605066, 'batch_size': 36, 'step_size': 6, 'gamma': 0.7660361017657058}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:35:05,226][0m Trial 42 finished with value: 0.04202898176243672 and parameters: {'observation_period_num': 31, 'train_rates': 0.8665204456933869, 'learning_rate': 0.0002455786100100154, 'batch_size': 30, 'step_size': 6, 'gamma': 0.7759546383750349}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:36:20,091][0m Trial 43 finished with value: 0.03737142546727583 and parameters: {'observation_period_num': 5, 'train_rates': 0.9020352606901295, 'learning_rate': 0.0003815822484229552, 'batch_size': 78, 'step_size': 4, 'gamma': 0.7605625436653743}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:39:03,693][0m Trial 44 finished with value: 0.029933410296478426 and parameters: {'observation_period_num': 14, 'train_rates': 0.8306657941732133, 'learning_rate': 0.00019215830789855935, 'batch_size': 32, 'step_size': 7, 'gamma': 0.7929503085544669}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:41:54,692][0m Trial 45 finished with value: 0.04269901115633203 and parameters: {'observation_period_num': 14, 'train_rates': 0.8157533543774176, 'learning_rate': 0.0005560263240842175, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7932173585391824}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:43:27,616][0m Trial 46 finished with value: 0.05291847872415852 and parameters: {'observation_period_num': 51, 'train_rates': 0.8323286880421134, 'learning_rate': 0.00020929189322050403, 'batch_size': 56, 'step_size': 7, 'gamma': 0.8730175297667151}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:44:49,969][0m Trial 47 finished with value: 0.04984835124410251 and parameters: {'observation_period_num': 29, 'train_rates': 0.9302659461825361, 'learning_rate': 0.00011840910739125401, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9374151479810517}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:46:04,033][0m Trial 48 finished with value: 0.05829089052725438 and parameters: {'observation_period_num': 12, 'train_rates': 0.8782424030189082, 'learning_rate': 8.427886463139125e-05, 'batch_size': 105, 'step_size': 3, 'gamma': 0.850975537037278}. Best is trial 41 with value: 0.029870987446470695.[0m
[32m[I 2025-01-06 20:49:25,403][0m Trial 49 finished with value: 0.04798575218867611 and parameters: {'observation_period_num': 41, 'train_rates': 0.8603647926881198, 'learning_rate': 2.0465555516969947e-05, 'batch_size': 28, 'step_size': 11, 'gamma': 0.829473867851139}. Best is trial 41 with value: 0.029870987446470695.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 16, 'train_rates': 0.8643625236512765, 'learning_rate': 2.037700551585825e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8432835553122393}
Epoch 1/300, trend Loss: 0.6933 | 0.4196
Epoch 2/300, trend Loss: 0.2915 | 0.2501
Epoch 3/300, trend Loss: 0.2113 | 0.1995
Epoch 4/300, trend Loss: 0.1859 | 0.1646
Epoch 5/300, trend Loss: 0.1720 | 0.1474
Epoch 6/300, trend Loss: 0.1619 | 0.1292
Epoch 7/300, trend Loss: 0.1536 | 0.1137
Epoch 8/300, trend Loss: 0.1466 | 0.1009
Epoch 9/300, trend Loss: 0.1412 | 0.0943
Epoch 10/300, trend Loss: 0.1368 | 0.0867
Epoch 11/300, trend Loss: 0.1330 | 0.0808
Epoch 12/300, trend Loss: 0.1297 | 0.0762
Epoch 13/300, trend Loss: 0.1271 | 0.0737
Epoch 14/300, trend Loss: 0.1250 | 0.0711
Epoch 15/300, trend Loss: 0.1232 | 0.0690
Epoch 16/300, trend Loss: 0.1216 | 0.0674
Epoch 17/300, trend Loss: 0.1203 | 0.0666
Epoch 18/300, trend Loss: 0.1192 | 0.0655
Epoch 19/300, trend Loss: 0.1181 | 0.0645
Epoch 20/300, trend Loss: 0.1172 | 0.0637
Epoch 21/300, trend Loss: 0.1164 | 0.0635
Epoch 22/300, trend Loss: 0.1156 | 0.0628
Epoch 23/300, trend Loss: 0.1149 | 0.0622
Epoch 24/300, trend Loss: 0.1142 | 0.0616
Epoch 25/300, trend Loss: 0.1136 | 0.0616
Epoch 26/300, trend Loss: 0.1130 | 0.0611
Epoch 27/300, trend Loss: 0.1124 | 0.0606
Epoch 28/300, trend Loss: 0.1118 | 0.0601
Epoch 29/300, trend Loss: 0.1113 | 0.0601
Epoch 30/300, trend Loss: 0.1108 | 0.0596
Epoch 31/300, trend Loss: 0.1103 | 0.0592
Epoch 32/300, trend Loss: 0.1099 | 0.0588
Epoch 33/300, trend Loss: 0.1094 | 0.0586
Epoch 34/300, trend Loss: 0.1090 | 0.0582
Epoch 35/300, trend Loss: 0.1086 | 0.0579
Epoch 36/300, trend Loss: 0.1082 | 0.0576
Epoch 37/300, trend Loss: 0.1078 | 0.0574
Epoch 38/300, trend Loss: 0.1074 | 0.0571
Epoch 39/300, trend Loss: 0.1071 | 0.0569
Epoch 40/300, trend Loss: 0.1067 | 0.0567
Epoch 41/300, trend Loss: 0.1064 | 0.0566
Epoch 42/300, trend Loss: 0.1061 | 0.0564
Epoch 43/300, trend Loss: 0.1058 | 0.0563
Epoch 44/300, trend Loss: 0.1055 | 0.0561
Epoch 45/300, trend Loss: 0.1051 | 0.0561
Epoch 46/300, trend Loss: 0.1049 | 0.0560
Epoch 47/300, trend Loss: 0.1046 | 0.0559
Epoch 48/300, trend Loss: 0.1044 | 0.0558
Epoch 49/300, trend Loss: 0.1041 | 0.0557
Epoch 50/300, trend Loss: 0.1039 | 0.0556
Epoch 51/300, trend Loss: 0.1037 | 0.0556
Epoch 52/300, trend Loss: 0.1035 | 0.0555
Epoch 53/300, trend Loss: 0.1033 | 0.0555
Epoch 54/300, trend Loss: 0.1031 | 0.0554
Epoch 55/300, trend Loss: 0.1030 | 0.0553
Epoch 56/300, trend Loss: 0.1028 | 0.0552
Epoch 57/300, trend Loss: 0.1026 | 0.0552
Epoch 58/300, trend Loss: 0.1025 | 0.0551
Epoch 59/300, trend Loss: 0.1023 | 0.0551
Epoch 60/300, trend Loss: 0.1022 | 0.0550
Epoch 61/300, trend Loss: 0.1021 | 0.0550
Epoch 62/300, trend Loss: 0.1020 | 0.0549
Epoch 63/300, trend Loss: 0.1018 | 0.0549
Epoch 64/300, trend Loss: 0.1017 | 0.0548
Epoch 65/300, trend Loss: 0.1016 | 0.0548
Epoch 66/300, trend Loss: 0.1015 | 0.0547
Epoch 67/300, trend Loss: 0.1014 | 0.0547
Epoch 68/300, trend Loss: 0.1013 | 0.0546
Epoch 69/300, trend Loss: 0.1012 | 0.0546
Epoch 70/300, trend Loss: 0.1012 | 0.0546
Epoch 71/300, trend Loss: 0.1011 | 0.0545
Epoch 72/300, trend Loss: 0.1010 | 0.0545
Epoch 73/300, trend Loss: 0.1009 | 0.0544
Epoch 74/300, trend Loss: 0.1009 | 0.0544
Epoch 75/300, trend Loss: 0.1008 | 0.0544
Epoch 76/300, trend Loss: 0.1007 | 0.0544
Epoch 77/300, trend Loss: 0.1006 | 0.0544
Epoch 78/300, trend Loss: 0.1006 | 0.0543
Epoch 79/300, trend Loss: 0.1005 | 0.0543
Epoch 80/300, trend Loss: 0.1005 | 0.0543
Epoch 81/300, trend Loss: 0.1004 | 0.0543
Epoch 82/300, trend Loss: 0.1004 | 0.0543
Epoch 83/300, trend Loss: 0.1003 | 0.0543
Epoch 84/300, trend Loss: 0.1003 | 0.0543
Epoch 85/300, trend Loss: 0.1002 | 0.0543
Epoch 86/300, trend Loss: 0.1002 | 0.0543
Epoch 87/300, trend Loss: 0.1001 | 0.0543
Epoch 88/300, trend Loss: 0.1001 | 0.0542
Epoch 89/300, trend Loss: 0.1000 | 0.0542
Epoch 90/300, trend Loss: 0.1000 | 0.0542
Epoch 91/300, trend Loss: 0.1000 | 0.0542
Epoch 92/300, trend Loss: 0.0999 | 0.0542
Epoch 93/300, trend Loss: 0.0999 | 0.0542
Epoch 94/300, trend Loss: 0.0999 | 0.0542
Epoch 95/300, trend Loss: 0.0998 | 0.0542
Epoch 96/300, trend Loss: 0.0998 | 0.0542
Epoch 97/300, trend Loss: 0.0998 | 0.0542
Epoch 98/300, trend Loss: 0.0998 | 0.0542
Epoch 99/300, trend Loss: 0.0997 | 0.0542
Epoch 100/300, trend Loss: 0.0997 | 0.0542
Epoch 101/300, trend Loss: 0.0997 | 0.0541
Epoch 102/300, trend Loss: 0.0997 | 0.0541
Epoch 103/300, trend Loss: 0.0996 | 0.0541
Epoch 104/300, trend Loss: 0.0996 | 0.0541
Epoch 105/300, trend Loss: 0.0996 | 0.0541
Epoch 106/300, trend Loss: 0.0996 | 0.0541
Epoch 107/300, trend Loss: 0.0996 | 0.0541
Epoch 108/300, trend Loss: 0.0995 | 0.0541
Epoch 109/300, trend Loss: 0.0995 | 0.0541
Epoch 110/300, trend Loss: 0.0995 | 0.0541
Epoch 111/300, trend Loss: 0.0995 | 0.0541
Epoch 112/300, trend Loss: 0.0995 | 0.0541
Epoch 113/300, trend Loss: 0.0995 | 0.0541
Epoch 114/300, trend Loss: 0.0994 | 0.0541
Epoch 115/300, trend Loss: 0.0994 | 0.0541
Epoch 116/300, trend Loss: 0.0994 | 0.0541
Epoch 117/300, trend Loss: 0.0994 | 0.0541
Epoch 118/300, trend Loss: 0.0994 | 0.0541
Epoch 119/300, trend Loss: 0.0994 | 0.0541
Epoch 120/300, trend Loss: 0.0994 | 0.0541
Epoch 121/300, trend Loss: 0.0994 | 0.0541
Epoch 122/300, trend Loss: 0.0994 | 0.0540
Epoch 123/300, trend Loss: 0.0994 | 0.0540
Epoch 124/300, trend Loss: 0.0993 | 0.0540
Epoch 125/300, trend Loss: 0.0993 | 0.0540
Epoch 126/300, trend Loss: 0.0993 | 0.0540
Epoch 127/300, trend Loss: 0.0993 | 0.0540
Epoch 128/300, trend Loss: 0.0993 | 0.0540
Epoch 129/300, trend Loss: 0.0993 | 0.0540
Epoch 130/300, trend Loss: 0.0993 | 0.0540
Epoch 131/300, trend Loss: 0.0993 | 0.0540
Epoch 132/300, trend Loss: 0.0993 | 0.0540
Epoch 133/300, trend Loss: 0.0993 | 0.0540
Epoch 134/300, trend Loss: 0.0993 | 0.0540
Epoch 135/300, trend Loss: 0.0993 | 0.0540
Epoch 136/300, trend Loss: 0.0993 | 0.0540
Epoch 137/300, trend Loss: 0.0993 | 0.0540
Epoch 138/300, trend Loss: 0.0993 | 0.0540
Epoch 139/300, trend Loss: 0.0992 | 0.0540
Epoch 140/300, trend Loss: 0.0992 | 0.0540
Epoch 141/300, trend Loss: 0.0992 | 0.0540
Epoch 142/300, trend Loss: 0.0992 | 0.0540
Epoch 143/300, trend Loss: 0.0992 | 0.0540
Epoch 144/300, trend Loss: 0.0992 | 0.0540
Epoch 145/300, trend Loss: 0.0992 | 0.0540
Epoch 146/300, trend Loss: 0.0992 | 0.0540
Epoch 147/300, trend Loss: 0.0992 | 0.0540
Epoch 148/300, trend Loss: 0.0992 | 0.0540
Epoch 149/300, trend Loss: 0.0992 | 0.0540
Epoch 150/300, trend Loss: 0.0992 | 0.0540
Epoch 151/300, trend Loss: 0.0992 | 0.0540
Epoch 152/300, trend Loss: 0.0992 | 0.0540
Epoch 153/300, trend Loss: 0.0992 | 0.0540
Epoch 154/300, trend Loss: 0.0992 | 0.0540
Epoch 155/300, trend Loss: 0.0992 | 0.0540
Epoch 156/300, trend Loss: 0.0992 | 0.0540
Epoch 157/300, trend Loss: 0.0992 | 0.0540
Epoch 158/300, trend Loss: 0.0992 | 0.0540
Epoch 159/300, trend Loss: 0.0992 | 0.0540
Epoch 160/300, trend Loss: 0.0992 | 0.0540
Epoch 161/300, trend Loss: 0.0992 | 0.0540
Epoch 162/300, trend Loss: 0.0992 | 0.0540
Epoch 163/300, trend Loss: 0.0992 | 0.0540
Epoch 164/300, trend Loss: 0.0992 | 0.0540
Epoch 165/300, trend Loss: 0.0992 | 0.0540
Epoch 166/300, trend Loss: 0.0992 | 0.0540
Epoch 167/300, trend Loss: 0.0992 | 0.0540
Epoch 168/300, trend Loss: 0.0992 | 0.0540
Epoch 169/300, trend Loss: 0.0992 | 0.0540
Epoch 170/300, trend Loss: 0.0992 | 0.0540
Epoch 171/300, trend Loss: 0.0992 | 0.0540
Epoch 172/300, trend Loss: 0.0992 | 0.0540
Epoch 173/300, trend Loss: 0.0992 | 0.0540
Epoch 174/300, trend Loss: 0.0992 | 0.0540
Epoch 175/300, trend Loss: 0.0992 | 0.0540
Epoch 176/300, trend Loss: 0.0992 | 0.0540
Epoch 177/300, trend Loss: 0.0992 | 0.0540
Epoch 178/300, trend Loss: 0.0992 | 0.0540
Epoch 179/300, trend Loss: 0.0992 | 0.0540
Epoch 180/300, trend Loss: 0.0992 | 0.0540
Epoch 181/300, trend Loss: 0.0992 | 0.0540
Epoch 182/300, trend Loss: 0.0992 | 0.0540
Epoch 183/300, trend Loss: 0.0992 | 0.0540
Epoch 184/300, trend Loss: 0.0992 | 0.0540
Epoch 185/300, trend Loss: 0.0992 | 0.0540
Epoch 186/300, trend Loss: 0.0992 | 0.0540
Epoch 187/300, trend Loss: 0.0992 | 0.0540
Epoch 188/300, trend Loss: 0.0992 | 0.0540
Epoch 189/300, trend Loss: 0.0992 | 0.0540
Epoch 190/300, trend Loss: 0.0992 | 0.0540
Epoch 191/300, trend Loss: 0.0992 | 0.0540
Epoch 192/300, trend Loss: 0.0992 | 0.0540
Epoch 193/300, trend Loss: 0.0992 | 0.0540
Epoch 194/300, trend Loss: 0.0992 | 0.0540
Epoch 195/300, trend Loss: 0.0992 | 0.0540
Epoch 196/300, trend Loss: 0.0992 | 0.0540
Epoch 197/300, trend Loss: 0.0992 | 0.0540
Epoch 198/300, trend Loss: 0.0992 | 0.0540
Epoch 199/300, trend Loss: 0.0992 | 0.0540
Epoch 200/300, trend Loss: 0.0992 | 0.0540
Epoch 201/300, trend Loss: 0.0992 | 0.0540
Epoch 202/300, trend Loss: 0.0992 | 0.0540
Epoch 203/300, trend Loss: 0.0992 | 0.0540
Epoch 204/300, trend Loss: 0.0992 | 0.0540
Epoch 205/300, trend Loss: 0.0992 | 0.0540
Epoch 206/300, trend Loss: 0.0992 | 0.0540
Epoch 207/300, trend Loss: 0.0992 | 0.0540
Epoch 208/300, trend Loss: 0.0992 | 0.0540
Epoch 209/300, trend Loss: 0.0992 | 0.0540
Epoch 210/300, trend Loss: 0.0991 | 0.0541
Epoch 211/300, trend Loss: 0.0991 | 0.0541
Epoch 212/300, trend Loss: 0.0991 | 0.0541
Epoch 213/300, trend Loss: 0.0991 | 0.0541
Epoch 214/300, trend Loss: 0.0991 | 0.0541
Epoch 215/300, trend Loss: 0.0991 | 0.0541
Epoch 216/300, trend Loss: 0.0991 | 0.0541
Epoch 217/300, trend Loss: 0.0991 | 0.0541
Epoch 218/300, trend Loss: 0.0991 | 0.0541
Epoch 219/300, trend Loss: 0.0991 | 0.0541
Epoch 220/300, trend Loss: 0.0991 | 0.0541
Epoch 221/300, trend Loss: 0.0991 | 0.0541
Epoch 222/300, trend Loss: 0.0991 | 0.0541
Epoch 223/300, trend Loss: 0.0991 | 0.0541
Epoch 224/300, trend Loss: 0.0991 | 0.0541
Epoch 225/300, trend Loss: 0.0991 | 0.0541
Epoch 226/300, trend Loss: 0.0991 | 0.0541
Epoch 227/300, trend Loss: 0.0991 | 0.0541
Epoch 228/300, trend Loss: 0.0991 | 0.0541
Epoch 229/300, trend Loss: 0.0991 | 0.0541
Epoch 230/300, trend Loss: 0.0991 | 0.0541
Epoch 231/300, trend Loss: 0.0991 | 0.0541
Epoch 232/300, trend Loss: 0.0991 | 0.0541
Epoch 233/300, trend Loss: 0.0991 | 0.0541
Epoch 234/300, trend Loss: 0.0991 | 0.0541
Epoch 235/300, trend Loss: 0.0991 | 0.0541
Epoch 236/300, trend Loss: 0.0991 | 0.0541
Epoch 237/300, trend Loss: 0.0991 | 0.0541
Epoch 238/300, trend Loss: 0.0991 | 0.0541
Epoch 239/300, trend Loss: 0.0991 | 0.0541
Epoch 240/300, trend Loss: 0.0991 | 0.0541
Epoch 241/300, trend Loss: 0.0991 | 0.0541
Epoch 242/300, trend Loss: 0.0991 | 0.0541
Epoch 243/300, trend Loss: 0.0991 | 0.0541
Epoch 244/300, trend Loss: 0.0991 | 0.0541
Epoch 245/300, trend Loss: 0.0991 | 0.0541
Epoch 246/300, trend Loss: 0.0991 | 0.0541
Epoch 247/300, trend Loss: 0.0991 | 0.0541
Epoch 248/300, trend Loss: 0.0991 | 0.0541
Epoch 249/300, trend Loss: 0.0991 | 0.0541
Epoch 250/300, trend Loss: 0.0991 | 0.0541
Epoch 251/300, trend Loss: 0.0991 | 0.0541
Epoch 252/300, trend Loss: 0.0991 | 0.0541
Epoch 253/300, trend Loss: 0.0991 | 0.0541
Epoch 254/300, trend Loss: 0.0991 | 0.0541
Epoch 255/300, trend Loss: 0.0991 | 0.0541
Epoch 256/300, trend Loss: 0.0991 | 0.0541
Epoch 257/300, trend Loss: 0.0991 | 0.0541
Epoch 258/300, trend Loss: 0.0991 | 0.0541
Epoch 259/300, trend Loss: 0.0991 | 0.0541
Epoch 260/300, trend Loss: 0.0991 | 0.0541
Epoch 261/300, trend Loss: 0.0991 | 0.0541
Epoch 262/300, trend Loss: 0.0991 | 0.0541
Epoch 263/300, trend Loss: 0.0991 | 0.0541
Epoch 264/300, trend Loss: 0.0991 | 0.0541
Epoch 265/300, trend Loss: 0.0991 | 0.0541
Epoch 266/300, trend Loss: 0.0991 | 0.0541
Epoch 267/300, trend Loss: 0.0991 | 0.0541
Epoch 268/300, trend Loss: 0.0991 | 0.0541
Epoch 269/300, trend Loss: 0.0991 | 0.0541
Epoch 270/300, trend Loss: 0.0991 | 0.0541
Epoch 271/300, trend Loss: 0.0991 | 0.0541
Epoch 272/300, trend Loss: 0.0991 | 0.0541
Epoch 273/300, trend Loss: 0.0991 | 0.0541
Epoch 274/300, trend Loss: 0.0991 | 0.0541
Epoch 275/300, trend Loss: 0.0991 | 0.0541
Epoch 276/300, trend Loss: 0.0991 | 0.0541
Epoch 277/300, trend Loss: 0.0991 | 0.0541
Epoch 278/300, trend Loss: 0.0991 | 0.0541
Epoch 279/300, trend Loss: 0.0991 | 0.0541
Epoch 280/300, trend Loss: 0.0991 | 0.0541
Epoch 281/300, trend Loss: 0.0991 | 0.0541
Epoch 282/300, trend Loss: 0.0991 | 0.0541
Epoch 283/300, trend Loss: 0.0991 | 0.0541
Epoch 284/300, trend Loss: 0.0991 | 0.0541
Epoch 285/300, trend Loss: 0.0991 | 0.0541
Epoch 286/300, trend Loss: 0.0991 | 0.0541
Epoch 287/300, trend Loss: 0.0991 | 0.0541
Epoch 288/300, trend Loss: 0.0991 | 0.0541
Epoch 289/300, trend Loss: 0.0991 | 0.0541
Epoch 290/300, trend Loss: 0.0991 | 0.0541
Epoch 291/300, trend Loss: 0.0991 | 0.0541
Epoch 292/300, trend Loss: 0.0991 | 0.0541
Epoch 293/300, trend Loss: 0.0991 | 0.0541
Epoch 294/300, trend Loss: 0.0991 | 0.0541
Epoch 295/300, trend Loss: 0.0991 | 0.0541
Epoch 296/300, trend Loss: 0.0991 | 0.0541
Epoch 297/300, trend Loss: 0.0991 | 0.0541
Epoch 298/300, trend Loss: 0.0991 | 0.0541
Epoch 299/300, trend Loss: 0.0991 | 0.0541
Epoch 300/300, trend Loss: 0.0991 | 0.0541
Training seasonal_0 component with params: {'observation_period_num': 10, 'train_rates': 0.9797436397446038, 'learning_rate': 8.450499018631672e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9829350533290653}
Epoch 1/300, seasonal_0 Loss: 0.1983 | 0.1056
Epoch 2/300, seasonal_0 Loss: 0.1364 | 0.0893
Epoch 3/300, seasonal_0 Loss: 0.1270 | 0.0839
Epoch 4/300, seasonal_0 Loss: 0.1219 | 0.0808
Epoch 5/300, seasonal_0 Loss: 0.1179 | 0.0790
Epoch 6/300, seasonal_0 Loss: 0.1144 | 0.0776
Epoch 7/300, seasonal_0 Loss: 0.1105 | 0.0768
Epoch 8/300, seasonal_0 Loss: 0.1068 | 0.0757
Epoch 9/300, seasonal_0 Loss: 0.1033 | 0.0747
Epoch 10/300, seasonal_0 Loss: 0.1006 | 0.0743
Epoch 11/300, seasonal_0 Loss: 0.0984 | 0.0745
Epoch 12/300, seasonal_0 Loss: 0.0965 | 0.0739
Epoch 13/300, seasonal_0 Loss: 0.0945 | 0.0731
Epoch 14/300, seasonal_0 Loss: 0.0926 | 0.0719
Epoch 15/300, seasonal_0 Loss: 0.0908 | 0.0706
Epoch 16/300, seasonal_0 Loss: 0.0891 | 0.0689
Epoch 17/300, seasonal_0 Loss: 0.0876 | 0.0669
Epoch 18/300, seasonal_0 Loss: 0.0862 | 0.0644
Epoch 19/300, seasonal_0 Loss: 0.0849 | 0.0618
Epoch 20/300, seasonal_0 Loss: 0.0837 | 0.0594
Epoch 21/300, seasonal_0 Loss: 0.0825 | 0.0577
Epoch 22/300, seasonal_0 Loss: 0.0814 | 0.0562
Epoch 23/300, seasonal_0 Loss: 0.0804 | 0.0552
Epoch 24/300, seasonal_0 Loss: 0.0795 | 0.0544
Epoch 25/300, seasonal_0 Loss: 0.0787 | 0.0538
Epoch 26/300, seasonal_0 Loss: 0.0780 | 0.0535
Epoch 27/300, seasonal_0 Loss: 0.0772 | 0.0534
Epoch 28/300, seasonal_0 Loss: 0.0765 | 0.0531
Epoch 29/300, seasonal_0 Loss: 0.0758 | 0.0527
Epoch 30/300, seasonal_0 Loss: 0.0750 | 0.0520
Epoch 31/300, seasonal_0 Loss: 0.0742 | 0.0510
Epoch 32/300, seasonal_0 Loss: 0.0734 | 0.0499
Epoch 33/300, seasonal_0 Loss: 0.0727 | 0.0489
Epoch 34/300, seasonal_0 Loss: 0.0720 | 0.0478
Epoch 35/300, seasonal_0 Loss: 0.0712 | 0.0469
Epoch 36/300, seasonal_0 Loss: 0.0705 | 0.0460
Epoch 37/300, seasonal_0 Loss: 0.0697 | 0.0452
Epoch 38/300, seasonal_0 Loss: 0.0691 | 0.0445
Epoch 39/300, seasonal_0 Loss: 0.0684 | 0.0439
Epoch 40/300, seasonal_0 Loss: 0.0677 | 0.0434
Epoch 41/300, seasonal_0 Loss: 0.0669 | 0.0429
Epoch 42/300, seasonal_0 Loss: 0.0662 | 0.0425
Epoch 43/300, seasonal_0 Loss: 0.0655 | 0.0423
Epoch 44/300, seasonal_0 Loss: 0.0648 | 0.0422
Epoch 45/300, seasonal_0 Loss: 0.0642 | 0.0423
Epoch 46/300, seasonal_0 Loss: 0.0635 | 0.0424
Epoch 47/300, seasonal_0 Loss: 0.0629 | 0.0430
Epoch 48/300, seasonal_0 Loss: 0.0623 | 0.0435
Epoch 49/300, seasonal_0 Loss: 0.0617 | 0.0447
Epoch 50/300, seasonal_0 Loss: 0.0612 | 0.0453
Epoch 51/300, seasonal_0 Loss: 0.0608 | 0.0451
Epoch 52/300, seasonal_0 Loss: 0.0603 | 0.0439
Epoch 53/300, seasonal_0 Loss: 0.0601 | 0.0430
Epoch 54/300, seasonal_0 Loss: 0.0599 | 0.0421
Epoch 55/300, seasonal_0 Loss: 0.0592 | 0.0418
Epoch 56/300, seasonal_0 Loss: 0.0587 | 0.0415
Epoch 57/300, seasonal_0 Loss: 0.0581 | 0.0417
Epoch 58/300, seasonal_0 Loss: 0.0575 | 0.0418
Epoch 59/300, seasonal_0 Loss: 0.0571 | 0.0423
Epoch 60/300, seasonal_0 Loss: 0.0565 | 0.0423
Epoch 61/300, seasonal_0 Loss: 0.0562 | 0.0425
Epoch 62/300, seasonal_0 Loss: 0.0557 | 0.0419
Epoch 63/300, seasonal_0 Loss: 0.0552 | 0.0417
Epoch 64/300, seasonal_0 Loss: 0.0549 | 0.0414
Epoch 65/300, seasonal_0 Loss: 0.0547 | 0.0418
Epoch 66/300, seasonal_0 Loss: 0.0543 | 0.0423
Epoch 67/300, seasonal_0 Loss: 0.0542 | 0.0426
Epoch 68/300, seasonal_0 Loss: 0.0539 | 0.0418
Epoch 69/300, seasonal_0 Loss: 0.0543 | 0.0410
Epoch 70/300, seasonal_0 Loss: 0.0533 | 0.0403
Epoch 71/300, seasonal_0 Loss: 0.0528 | 0.0405
Epoch 72/300, seasonal_0 Loss: 0.0523 | 0.0405
Epoch 73/300, seasonal_0 Loss: 0.0521 | 0.0407
Epoch 74/300, seasonal_0 Loss: 0.0515 | 0.0410
Epoch 75/300, seasonal_0 Loss: 0.0510 | 0.0413
Epoch 76/300, seasonal_0 Loss: 0.0498 | 0.0415
Epoch 77/300, seasonal_0 Loss: 0.0471 | 0.0427
Epoch 78/300, seasonal_0 Loss: 0.0456 | 0.0421
Epoch 79/300, seasonal_0 Loss: 0.0452 | 0.0434
Epoch 80/300, seasonal_0 Loss: 0.0503 | 0.0422
Epoch 81/300, seasonal_0 Loss: 0.0447 | 0.0415
Epoch 82/300, seasonal_0 Loss: 0.0436 | 0.0414
Epoch 83/300, seasonal_0 Loss: 0.0468 | 0.0439
Epoch 84/300, seasonal_0 Loss: 0.0450 | 0.0418
Epoch 85/300, seasonal_0 Loss: 0.0435 | 0.0519
Epoch 86/300, seasonal_0 Loss: 0.0438 | 0.0418
Epoch 87/300, seasonal_0 Loss: 0.0423 | 0.0406
Epoch 88/300, seasonal_0 Loss: 0.0417 | 0.0414
Epoch 89/300, seasonal_0 Loss: 0.0414 | 0.0411
Epoch 90/300, seasonal_0 Loss: 0.0415 | 0.0422
Epoch 91/300, seasonal_0 Loss: 0.0409 | 0.0415
Epoch 92/300, seasonal_0 Loss: 0.0408 | 0.0425
Epoch 93/300, seasonal_0 Loss: 0.0403 | 0.0421
Epoch 94/300, seasonal_0 Loss: 0.0403 | 0.0427
Epoch 95/300, seasonal_0 Loss: 0.0397 | 0.0423
Epoch 96/300, seasonal_0 Loss: 0.0396 | 0.0427
Epoch 97/300, seasonal_0 Loss: 0.0393 | 0.0426
Epoch 98/300, seasonal_0 Loss: 0.0393 | 0.0429
Epoch 99/300, seasonal_0 Loss: 0.0388 | 0.0425
Epoch 100/300, seasonal_0 Loss: 0.0387 | 0.0429
Epoch 101/300, seasonal_0 Loss: 0.0383 | 0.0427
Epoch 102/300, seasonal_0 Loss: 0.0384 | 0.0430
Epoch 103/300, seasonal_0 Loss: 0.0379 | 0.0428
Epoch 104/300, seasonal_0 Loss: 0.0378 | 0.0430
Epoch 105/300, seasonal_0 Loss: 0.0374 | 0.0428
Epoch 106/300, seasonal_0 Loss: 0.0374 | 0.0430
Epoch 107/300, seasonal_0 Loss: 0.0370 | 0.0430
Epoch 108/300, seasonal_0 Loss: 0.0369 | 0.0431
Epoch 109/300, seasonal_0 Loss: 0.0366 | 0.0429
Epoch 110/300, seasonal_0 Loss: 0.0365 | 0.0432
Epoch 111/300, seasonal_0 Loss: 0.0362 | 0.0430
Epoch 112/300, seasonal_0 Loss: 0.0361 | 0.0431
Epoch 113/300, seasonal_0 Loss: 0.0358 | 0.0429
Epoch 114/300, seasonal_0 Loss: 0.0357 | 0.0432
Epoch 115/300, seasonal_0 Loss: 0.0354 | 0.0430
Epoch 116/300, seasonal_0 Loss: 0.0353 | 0.0432
Epoch 117/300, seasonal_0 Loss: 0.0351 | 0.0430
Epoch 118/300, seasonal_0 Loss: 0.0349 | 0.0431
Epoch 119/300, seasonal_0 Loss: 0.0347 | 0.0433
Epoch 120/300, seasonal_0 Loss: 0.0345 | 0.0433
Epoch 121/300, seasonal_0 Loss: 0.0343 | 0.0435
Epoch 122/300, seasonal_0 Loss: 0.0343 | 0.0440
Epoch 123/300, seasonal_0 Loss: 0.0340 | 0.0434
Epoch 124/300, seasonal_0 Loss: 0.0339 | 0.0440
Epoch 125/300, seasonal_0 Loss: 0.0339 | 0.0443
Epoch 126/300, seasonal_0 Loss: 0.0337 | 0.0445
Epoch 127/300, seasonal_0 Loss: 0.0336 | 0.0442
Epoch 128/300, seasonal_0 Loss: 0.0332 | 0.0438
Epoch 129/300, seasonal_0 Loss: 0.0333 | 0.0434
Epoch 130/300, seasonal_0 Loss: 0.0330 | 0.0438
Epoch 131/300, seasonal_0 Loss: 0.0327 | 0.0436
Epoch 132/300, seasonal_0 Loss: 0.0325 | 0.0438
Epoch 133/300, seasonal_0 Loss: 0.0323 | 0.0440
Epoch 134/300, seasonal_0 Loss: 0.0322 | 0.0442
Epoch 135/300, seasonal_0 Loss: 0.0321 | 0.0445
Epoch 136/300, seasonal_0 Loss: 0.0319 | 0.0444
Epoch 137/300, seasonal_0 Loss: 0.0317 | 0.0447
Epoch 138/300, seasonal_0 Loss: 0.0315 | 0.0446
Epoch 139/300, seasonal_0 Loss: 0.0314 | 0.0446
Epoch 140/300, seasonal_0 Loss: 0.0313 | 0.0447
Epoch 141/300, seasonal_0 Loss: 0.0311 | 0.0447
Epoch 142/300, seasonal_0 Loss: 0.0310 | 0.0450
Epoch 143/300, seasonal_0 Loss: 0.0309 | 0.0449
Epoch 144/300, seasonal_0 Loss: 0.0308 | 0.0450
Epoch 145/300, seasonal_0 Loss: 0.0306 | 0.0450
Epoch 146/300, seasonal_0 Loss: 0.0305 | 0.0453
Epoch 147/300, seasonal_0 Loss: 0.0304 | 0.0455
Epoch 148/300, seasonal_0 Loss: 0.0303 | 0.0459
Epoch 149/300, seasonal_0 Loss: 0.0303 | 0.0455
Epoch 150/300, seasonal_0 Loss: 0.0301 | 0.0454
Epoch 151/300, seasonal_0 Loss: 0.0301 | 0.0450
Epoch 152/300, seasonal_0 Loss: 0.0302 | 0.0443
Epoch 153/300, seasonal_0 Loss: 0.0301 | 0.0437
Epoch 154/300, seasonal_0 Loss: 0.0299 | 0.0437
Epoch 155/300, seasonal_0 Loss: 0.0299 | 0.0433
Epoch 156/300, seasonal_0 Loss: 0.0297 | 0.0434
Epoch 157/300, seasonal_0 Loss: 0.0297 | 0.0437
Epoch 158/300, seasonal_0 Loss: 0.0296 | 0.0439
Epoch 159/300, seasonal_0 Loss: 0.0296 | 0.0442
Epoch 160/300, seasonal_0 Loss: 0.0294 | 0.0444
Epoch 161/300, seasonal_0 Loss: 0.0294 | 0.0446
Epoch 162/300, seasonal_0 Loss: 0.0293 | 0.0449
Epoch 163/300, seasonal_0 Loss: 0.0292 | 0.0449
Epoch 164/300, seasonal_0 Loss: 0.0290 | 0.0450
Epoch 165/300, seasonal_0 Loss: 0.0291 | 0.0454
Epoch 166/300, seasonal_0 Loss: 0.0292 | 0.0453
Epoch 167/300, seasonal_0 Loss: 0.0290 | 0.0452
Epoch 168/300, seasonal_0 Loss: 0.0287 | 0.0454
Epoch 169/300, seasonal_0 Loss: 0.0287 | 0.0453
Epoch 170/300, seasonal_0 Loss: 0.0285 | 0.0452
Epoch 171/300, seasonal_0 Loss: 0.0284 | 0.0457
Epoch 172/300, seasonal_0 Loss: 0.0281 | 0.0450
Epoch 173/300, seasonal_0 Loss: 0.0280 | 0.0452
Epoch 174/300, seasonal_0 Loss: 0.0279 | 0.0451
Epoch 175/300, seasonal_0 Loss: 0.0278 | 0.0448
Epoch 176/300, seasonal_0 Loss: 0.0277 | 0.0447
Epoch 177/300, seasonal_0 Loss: 0.0276 | 0.0447
Epoch 178/300, seasonal_0 Loss: 0.0275 | 0.0446
Epoch 179/300, seasonal_0 Loss: 0.0274 | 0.0444
Epoch 180/300, seasonal_0 Loss: 0.0273 | 0.0447
Epoch 181/300, seasonal_0 Loss: 0.0272 | 0.0443
Epoch 182/300, seasonal_0 Loss: 0.0272 | 0.0446
Epoch 183/300, seasonal_0 Loss: 0.0271 | 0.0441
Epoch 184/300, seasonal_0 Loss: 0.0271 | 0.0448
Epoch 185/300, seasonal_0 Loss: 0.0270 | 0.0443
Epoch 186/300, seasonal_0 Loss: 0.0269 | 0.0444
Epoch 187/300, seasonal_0 Loss: 0.0269 | 0.0446
Epoch 188/300, seasonal_0 Loss: 0.0268 | 0.0450
Epoch 189/300, seasonal_0 Loss: 0.0267 | 0.0445
Epoch 190/300, seasonal_0 Loss: 0.0266 | 0.0449
Epoch 191/300, seasonal_0 Loss: 0.0266 | 0.0452
Epoch 192/300, seasonal_0 Loss: 0.0265 | 0.0450
Epoch 193/300, seasonal_0 Loss: 0.0265 | 0.0448
Epoch 194/300, seasonal_0 Loss: 0.0264 | 0.0453
Epoch 195/300, seasonal_0 Loss: 0.0264 | 0.0446
Epoch 196/300, seasonal_0 Loss: 0.0264 | 0.0453
Epoch 197/300, seasonal_0 Loss: 0.0263 | 0.0451
Epoch 198/300, seasonal_0 Loss: 0.0261 | 0.0450
Epoch 199/300, seasonal_0 Loss: 0.0260 | 0.0449
Epoch 200/300, seasonal_0 Loss: 0.0260 | 0.0450
Epoch 201/300, seasonal_0 Loss: 0.0258 | 0.0448
Epoch 202/300, seasonal_0 Loss: 0.0257 | 0.0453
Epoch 203/300, seasonal_0 Loss: 0.0256 | 0.0450
Epoch 204/300, seasonal_0 Loss: 0.0256 | 0.0452
Epoch 205/300, seasonal_0 Loss: 0.0255 | 0.0451
Epoch 206/300, seasonal_0 Loss: 0.0255 | 0.0453
Epoch 207/300, seasonal_0 Loss: 0.0254 | 0.0454
Epoch 208/300, seasonal_0 Loss: 0.0254 | 0.0452
Epoch 209/300, seasonal_0 Loss: 0.0253 | 0.0459
Epoch 210/300, seasonal_0 Loss: 0.0252 | 0.0456
Epoch 211/300, seasonal_0 Loss: 0.0251 | 0.0461
Epoch 212/300, seasonal_0 Loss: 0.0251 | 0.0457
Epoch 213/300, seasonal_0 Loss: 0.0250 | 0.0465
Epoch 214/300, seasonal_0 Loss: 0.0249 | 0.0459
Epoch 215/300, seasonal_0 Loss: 0.0249 | 0.0466
Epoch 216/300, seasonal_0 Loss: 0.0248 | 0.0463
Epoch 217/300, seasonal_0 Loss: 0.0247 | 0.0468
Epoch 218/300, seasonal_0 Loss: 0.0247 | 0.0466
Epoch 219/300, seasonal_0 Loss: 0.0246 | 0.0470
Epoch 220/300, seasonal_0 Loss: 0.0246 | 0.0468
Epoch 221/300, seasonal_0 Loss: 0.0245 | 0.0471
Epoch 222/300, seasonal_0 Loss: 0.0245 | 0.0472
Epoch 223/300, seasonal_0 Loss: 0.0244 | 0.0472
Epoch 224/300, seasonal_0 Loss: 0.0244 | 0.0474
Epoch 225/300, seasonal_0 Loss: 0.0243 | 0.0472
Epoch 226/300, seasonal_0 Loss: 0.0243 | 0.0477
Epoch 227/300, seasonal_0 Loss: 0.0242 | 0.0474
Epoch 228/300, seasonal_0 Loss: 0.0241 | 0.0478
Epoch 229/300, seasonal_0 Loss: 0.0241 | 0.0477
Epoch 230/300, seasonal_0 Loss: 0.0240 | 0.0479
Epoch 231/300, seasonal_0 Loss: 0.0240 | 0.0479
Epoch 232/300, seasonal_0 Loss: 0.0239 | 0.0480
Epoch 233/300, seasonal_0 Loss: 0.0240 | 0.0485
Epoch 234/300, seasonal_0 Loss: 0.0239 | 0.0481
Epoch 235/300, seasonal_0 Loss: 0.0240 | 0.0488
Epoch 236/300, seasonal_0 Loss: 0.0238 | 0.0483
Epoch 237/300, seasonal_0 Loss: 0.0238 | 0.0489
Epoch 238/300, seasonal_0 Loss: 0.0237 | 0.0487
Epoch 239/300, seasonal_0 Loss: 0.0237 | 0.0489
Epoch 240/300, seasonal_0 Loss: 0.0236 | 0.0489
Epoch 241/300, seasonal_0 Loss: 0.0235 | 0.0489
Epoch 242/300, seasonal_0 Loss: 0.0234 | 0.0489
Epoch 243/300, seasonal_0 Loss: 0.0234 | 0.0490
Epoch 244/300, seasonal_0 Loss: 0.0234 | 0.0490
Epoch 245/300, seasonal_0 Loss: 0.0233 | 0.0490
Epoch 246/300, seasonal_0 Loss: 0.0233 | 0.0490
Epoch 247/300, seasonal_0 Loss: 0.0232 | 0.0490
Epoch 248/300, seasonal_0 Loss: 0.0232 | 0.0491
Epoch 249/300, seasonal_0 Loss: 0.0232 | 0.0490
Epoch 250/300, seasonal_0 Loss: 0.0231 | 0.0490
Epoch 251/300, seasonal_0 Loss: 0.0231 | 0.0490
Epoch 252/300, seasonal_0 Loss: 0.0230 | 0.0490
Epoch 253/300, seasonal_0 Loss: 0.0230 | 0.0490
Epoch 254/300, seasonal_0 Loss: 0.0230 | 0.0491
Epoch 255/300, seasonal_0 Loss: 0.0230 | 0.0490
Epoch 256/300, seasonal_0 Loss: 0.0229 | 0.0490
Epoch 257/300, seasonal_0 Loss: 0.0229 | 0.0490
Epoch 258/300, seasonal_0 Loss: 0.0228 | 0.0490
Epoch 259/300, seasonal_0 Loss: 0.0228 | 0.0489
Epoch 260/300, seasonal_0 Loss: 0.0227 | 0.0490
Epoch 261/300, seasonal_0 Loss: 0.0227 | 0.0490
Epoch 262/300, seasonal_0 Loss: 0.0227 | 0.0490
Epoch 263/300, seasonal_0 Loss: 0.0227 | 0.0490
Epoch 264/300, seasonal_0 Loss: 0.0226 | 0.0491
Epoch 265/300, seasonal_0 Loss: 0.0226 | 0.0491
Epoch 266/300, seasonal_0 Loss: 0.0225 | 0.0491
Epoch 267/300, seasonal_0 Loss: 0.0225 | 0.0491
Epoch 268/300, seasonal_0 Loss: 0.0225 | 0.0491
Epoch 269/300, seasonal_0 Loss: 0.0224 | 0.0490
Epoch 270/300, seasonal_0 Loss: 0.0224 | 0.0491
Epoch 271/300, seasonal_0 Loss: 0.0224 | 0.0491
Epoch 272/300, seasonal_0 Loss: 0.0224 | 0.0491
Epoch 273/300, seasonal_0 Loss: 0.0223 | 0.0490
Epoch 274/300, seasonal_0 Loss: 0.0223 | 0.0491
Epoch 275/300, seasonal_0 Loss: 0.0223 | 0.0491
Epoch 276/300, seasonal_0 Loss: 0.0222 | 0.0490
Epoch 277/300, seasonal_0 Loss: 0.0222 | 0.0491
Epoch 278/300, seasonal_0 Loss: 0.0222 | 0.0490
Epoch 279/300, seasonal_0 Loss: 0.0221 | 0.0491
Epoch 280/300, seasonal_0 Loss: 0.0221 | 0.0489
Epoch 281/300, seasonal_0 Loss: 0.0221 | 0.0491
Epoch 282/300, seasonal_0 Loss: 0.0221 | 0.0489
Epoch 283/300, seasonal_0 Loss: 0.0220 | 0.0490
Epoch 284/300, seasonal_0 Loss: 0.0220 | 0.0489
Epoch 285/300, seasonal_0 Loss: 0.0220 | 0.0491
Epoch 286/300, seasonal_0 Loss: 0.0220 | 0.0488
Epoch 287/300, seasonal_0 Loss: 0.0219 | 0.0491
Epoch 288/300, seasonal_0 Loss: 0.0219 | 0.0488
Epoch 289/300, seasonal_0 Loss: 0.0218 | 0.0490
Epoch 290/300, seasonal_0 Loss: 0.0218 | 0.0488
Epoch 291/300, seasonal_0 Loss: 0.0218 | 0.0490
Epoch 292/300, seasonal_0 Loss: 0.0218 | 0.0488
Epoch 293/300, seasonal_0 Loss: 0.0217 | 0.0489
Epoch 294/300, seasonal_0 Loss: 0.0217 | 0.0488
Epoch 295/300, seasonal_0 Loss: 0.0217 | 0.0488
Epoch 296/300, seasonal_0 Loss: 0.0216 | 0.0487
Epoch 297/300, seasonal_0 Loss: 0.0216 | 0.0488
Epoch 298/300, seasonal_0 Loss: 0.0216 | 0.0486
Epoch 299/300, seasonal_0 Loss: 0.0216 | 0.0487
Epoch 300/300, seasonal_0 Loss: 0.0216 | 0.0487
Training seasonal_1 component with params: {'observation_period_num': 48, 'train_rates': 0.8614675303694281, 'learning_rate': 0.00034930417837490153, 'batch_size': 163, 'step_size': 9, 'gamma': 0.897551614724832}
Epoch 1/300, seasonal_1 Loss: 0.4511 | 0.2823
Epoch 2/300, seasonal_1 Loss: 0.3091 | 0.2396
Epoch 3/300, seasonal_1 Loss: 0.2814 | 0.1931
Epoch 4/300, seasonal_1 Loss: 0.2579 | 0.3395
Epoch 5/300, seasonal_1 Loss: 0.2597 | 0.1732
Epoch 6/300, seasonal_1 Loss: 0.2168 | 0.1272
Epoch 7/300, seasonal_1 Loss: 0.2936 | 0.1549
Epoch 8/300, seasonal_1 Loss: 0.1818 | 0.1313
Epoch 9/300, seasonal_1 Loss: 0.1473 | 0.1168
Epoch 10/300, seasonal_1 Loss: 0.1415 | 0.0970
Epoch 11/300, seasonal_1 Loss: 0.1332 | 0.0930
Epoch 12/300, seasonal_1 Loss: 0.1243 | 0.0988
Epoch 13/300, seasonal_1 Loss: 0.1212 | 0.0926
Epoch 14/300, seasonal_1 Loss: 0.1173 | 0.0858
Epoch 15/300, seasonal_1 Loss: 0.1138 | 0.0826
Epoch 16/300, seasonal_1 Loss: 0.1121 | 0.0756
Epoch 17/300, seasonal_1 Loss: 0.1160 | 0.1091
Epoch 18/300, seasonal_1 Loss: 0.1207 | 0.1039
Epoch 19/300, seasonal_1 Loss: 0.1185 | 0.1207
Epoch 20/300, seasonal_1 Loss: 0.1113 | 0.1374
Epoch 21/300, seasonal_1 Loss: 0.1145 | 0.0867
Epoch 22/300, seasonal_1 Loss: 0.1073 | 0.0754
Epoch 23/300, seasonal_1 Loss: 0.1036 | 0.0802
Epoch 24/300, seasonal_1 Loss: 0.1068 | 0.0651
Epoch 25/300, seasonal_1 Loss: 0.1056 | 0.0895
Epoch 26/300, seasonal_1 Loss: 0.1006 | 0.0685
Epoch 27/300, seasonal_1 Loss: 0.0966 | 0.0799
Epoch 28/300, seasonal_1 Loss: 0.1059 | 0.1367
Epoch 29/300, seasonal_1 Loss: 0.1063 | 0.0748
Epoch 30/300, seasonal_1 Loss: 0.1011 | 0.0633
Epoch 31/300, seasonal_1 Loss: 0.0981 | 0.0634
Epoch 32/300, seasonal_1 Loss: 0.0937 | 0.0686
Epoch 33/300, seasonal_1 Loss: 0.0950 | 0.0660
Epoch 34/300, seasonal_1 Loss: 0.0910 | 0.0775
Epoch 35/300, seasonal_1 Loss: 0.0930 | 0.0832
Epoch 36/300, seasonal_1 Loss: 0.0905 | 0.0602
Epoch 37/300, seasonal_1 Loss: 0.0887 | 0.0621
Epoch 38/300, seasonal_1 Loss: 0.0859 | 0.0568
Epoch 39/300, seasonal_1 Loss: 0.0876 | 0.0641
Epoch 40/300, seasonal_1 Loss: 0.0876 | 0.0552
Epoch 41/300, seasonal_1 Loss: 0.0877 | 0.0629
Epoch 42/300, seasonal_1 Loss: 0.0869 | 0.0718
Epoch 43/300, seasonal_1 Loss: 0.0918 | 0.0585
Epoch 44/300, seasonal_1 Loss: 0.0858 | 0.0649
Epoch 45/300, seasonal_1 Loss: 0.0823 | 0.0574
Epoch 46/300, seasonal_1 Loss: 0.0813 | 0.0585
Epoch 47/300, seasonal_1 Loss: 0.0812 | 0.0588
Epoch 48/300, seasonal_1 Loss: 0.0803 | 0.0570
Epoch 49/300, seasonal_1 Loss: 0.0786 | 0.0576
Epoch 50/300, seasonal_1 Loss: 0.0780 | 0.0591
Epoch 51/300, seasonal_1 Loss: 0.0782 | 0.0549
Epoch 52/300, seasonal_1 Loss: 0.0769 | 0.0541
Epoch 53/300, seasonal_1 Loss: 0.0771 | 0.0532
Epoch 54/300, seasonal_1 Loss: 0.0760 | 0.0536
Epoch 55/300, seasonal_1 Loss: 0.0759 | 0.0532
Epoch 56/300, seasonal_1 Loss: 0.0752 | 0.0560
Epoch 57/300, seasonal_1 Loss: 0.0749 | 0.0529
Epoch 58/300, seasonal_1 Loss: 0.0747 | 0.0522
Epoch 59/300, seasonal_1 Loss: 0.0738 | 0.0520
Epoch 60/300, seasonal_1 Loss: 0.0736 | 0.0515
Epoch 61/300, seasonal_1 Loss: 0.0734 | 0.0516
Epoch 62/300, seasonal_1 Loss: 0.0735 | 0.0525
Epoch 63/300, seasonal_1 Loss: 0.0726 | 0.0522
Epoch 64/300, seasonal_1 Loss: 0.0722 | 0.0506
Epoch 65/300, seasonal_1 Loss: 0.0721 | 0.0502
Epoch 66/300, seasonal_1 Loss: 0.0715 | 0.0500
Epoch 67/300, seasonal_1 Loss: 0.0715 | 0.0494
Epoch 68/300, seasonal_1 Loss: 0.0714 | 0.0497
Epoch 69/300, seasonal_1 Loss: 0.0710 | 0.0502
Epoch 70/300, seasonal_1 Loss: 0.0707 | 0.0495
Epoch 71/300, seasonal_1 Loss: 0.0705 | 0.0489
Epoch 72/300, seasonal_1 Loss: 0.0702 | 0.0500
Epoch 73/300, seasonal_1 Loss: 0.0697 | 0.0500
Epoch 74/300, seasonal_1 Loss: 0.0698 | 0.0493
Epoch 75/300, seasonal_1 Loss: 0.0698 | 0.0494
Epoch 76/300, seasonal_1 Loss: 0.0694 | 0.0490
Epoch 77/300, seasonal_1 Loss: 0.0689 | 0.0479
Epoch 78/300, seasonal_1 Loss: 0.0686 | 0.0480
Epoch 79/300, seasonal_1 Loss: 0.0685 | 0.0479
Epoch 80/300, seasonal_1 Loss: 0.0685 | 0.0476
Epoch 81/300, seasonal_1 Loss: 0.0681 | 0.0477
Epoch 82/300, seasonal_1 Loss: 0.0677 | 0.0484
Epoch 83/300, seasonal_1 Loss: 0.0676 | 0.0476
Epoch 84/300, seasonal_1 Loss: 0.0675 | 0.0477
Epoch 85/300, seasonal_1 Loss: 0.0672 | 0.0475
Epoch 86/300, seasonal_1 Loss: 0.0669 | 0.0470
Epoch 87/300, seasonal_1 Loss: 0.0668 | 0.0469
Epoch 88/300, seasonal_1 Loss: 0.0667 | 0.0471
Epoch 89/300, seasonal_1 Loss: 0.0665 | 0.0469
Epoch 90/300, seasonal_1 Loss: 0.0663 | 0.0470
Epoch 91/300, seasonal_1 Loss: 0.0662 | 0.0469
Epoch 92/300, seasonal_1 Loss: 0.0661 | 0.0466
Epoch 93/300, seasonal_1 Loss: 0.0659 | 0.0466
Epoch 94/300, seasonal_1 Loss: 0.0658 | 0.0466
Epoch 95/300, seasonal_1 Loss: 0.0657 | 0.0465
Epoch 96/300, seasonal_1 Loss: 0.0655 | 0.0465
Epoch 97/300, seasonal_1 Loss: 0.0654 | 0.0464
Epoch 98/300, seasonal_1 Loss: 0.0653 | 0.0464
Epoch 99/300, seasonal_1 Loss: 0.0652 | 0.0463
Epoch 100/300, seasonal_1 Loss: 0.0651 | 0.0462
Epoch 101/300, seasonal_1 Loss: 0.0650 | 0.0462
Epoch 102/300, seasonal_1 Loss: 0.0649 | 0.0462
Epoch 103/300, seasonal_1 Loss: 0.0648 | 0.0461
Epoch 104/300, seasonal_1 Loss: 0.0647 | 0.0461
Epoch 105/300, seasonal_1 Loss: 0.0646 | 0.0460
Epoch 106/300, seasonal_1 Loss: 0.0645 | 0.0460
Epoch 107/300, seasonal_1 Loss: 0.0644 | 0.0459
Epoch 108/300, seasonal_1 Loss: 0.0643 | 0.0459
Epoch 109/300, seasonal_1 Loss: 0.0642 | 0.0458
Epoch 110/300, seasonal_1 Loss: 0.0641 | 0.0458
Epoch 111/300, seasonal_1 Loss: 0.0640 | 0.0457
Epoch 112/300, seasonal_1 Loss: 0.0639 | 0.0458
Epoch 113/300, seasonal_1 Loss: 0.0639 | 0.0457
Epoch 114/300, seasonal_1 Loss: 0.0638 | 0.0457
Epoch 115/300, seasonal_1 Loss: 0.0637 | 0.0456
Epoch 116/300, seasonal_1 Loss: 0.0636 | 0.0457
Epoch 117/300, seasonal_1 Loss: 0.0636 | 0.0455
Epoch 118/300, seasonal_1 Loss: 0.0635 | 0.0457
Epoch 119/300, seasonal_1 Loss: 0.0634 | 0.0454
Epoch 120/300, seasonal_1 Loss: 0.0634 | 0.0457
Epoch 121/300, seasonal_1 Loss: 0.0633 | 0.0453
Epoch 122/300, seasonal_1 Loss: 0.0633 | 0.0459
Epoch 123/300, seasonal_1 Loss: 0.0632 | 0.0452
Epoch 124/300, seasonal_1 Loss: 0.0632 | 0.0461
Epoch 125/300, seasonal_1 Loss: 0.0632 | 0.0451
Epoch 126/300, seasonal_1 Loss: 0.0634 | 0.0471
Epoch 127/300, seasonal_1 Loss: 0.0636 | 0.0450
Epoch 128/300, seasonal_1 Loss: 0.0641 | 0.0482
Epoch 129/300, seasonal_1 Loss: 0.0654 | 0.0449
Epoch 130/300, seasonal_1 Loss: 0.0673 | 0.0510
Epoch 131/300, seasonal_1 Loss: 0.0721 | 0.0549
Epoch 132/300, seasonal_1 Loss: 0.0711 | 0.0478
Epoch 133/300, seasonal_1 Loss: 0.0720 | 0.0695
Epoch 134/300, seasonal_1 Loss: 0.0674 | 0.0479
Epoch 135/300, seasonal_1 Loss: 0.0653 | 0.0517
Epoch 136/300, seasonal_1 Loss: 0.0638 | 0.0463
Epoch 137/300, seasonal_1 Loss: 0.0631 | 0.0463
Epoch 138/300, seasonal_1 Loss: 0.0627 | 0.0458
Epoch 139/300, seasonal_1 Loss: 0.0626 | 0.0454
Epoch 140/300, seasonal_1 Loss: 0.0624 | 0.0457
Epoch 141/300, seasonal_1 Loss: 0.0624 | 0.0452
Epoch 142/300, seasonal_1 Loss: 0.0623 | 0.0457
Epoch 143/300, seasonal_1 Loss: 0.0623 | 0.0452
Epoch 144/300, seasonal_1 Loss: 0.0622 | 0.0456
Epoch 145/300, seasonal_1 Loss: 0.0622 | 0.0452
Epoch 146/300, seasonal_1 Loss: 0.0621 | 0.0455
Epoch 147/300, seasonal_1 Loss: 0.0621 | 0.0452
Epoch 148/300, seasonal_1 Loss: 0.0621 | 0.0453
Epoch 149/300, seasonal_1 Loss: 0.0620 | 0.0452
Epoch 150/300, seasonal_1 Loss: 0.0620 | 0.0453
Epoch 151/300, seasonal_1 Loss: 0.0619 | 0.0452
Epoch 152/300, seasonal_1 Loss: 0.0619 | 0.0452
Epoch 153/300, seasonal_1 Loss: 0.0619 | 0.0451
Epoch 154/300, seasonal_1 Loss: 0.0619 | 0.0452
Epoch 155/300, seasonal_1 Loss: 0.0618 | 0.0451
Epoch 156/300, seasonal_1 Loss: 0.0618 | 0.0451
Epoch 157/300, seasonal_1 Loss: 0.0618 | 0.0451
Epoch 158/300, seasonal_1 Loss: 0.0617 | 0.0451
Epoch 159/300, seasonal_1 Loss: 0.0617 | 0.0451
Epoch 160/300, seasonal_1 Loss: 0.0617 | 0.0451
Epoch 161/300, seasonal_1 Loss: 0.0617 | 0.0451
Epoch 162/300, seasonal_1 Loss: 0.0616 | 0.0451
Epoch 163/300, seasonal_1 Loss: 0.0616 | 0.0450
Epoch 164/300, seasonal_1 Loss: 0.0616 | 0.0450
Epoch 165/300, seasonal_1 Loss: 0.0616 | 0.0450
Epoch 166/300, seasonal_1 Loss: 0.0615 | 0.0450
Epoch 167/300, seasonal_1 Loss: 0.0615 | 0.0450
Epoch 168/300, seasonal_1 Loss: 0.0615 | 0.0450
Epoch 169/300, seasonal_1 Loss: 0.0615 | 0.0450
Epoch 170/300, seasonal_1 Loss: 0.0614 | 0.0450
Epoch 171/300, seasonal_1 Loss: 0.0614 | 0.0450
Epoch 172/300, seasonal_1 Loss: 0.0614 | 0.0450
Epoch 173/300, seasonal_1 Loss: 0.0614 | 0.0450
Epoch 174/300, seasonal_1 Loss: 0.0614 | 0.0449
Epoch 175/300, seasonal_1 Loss: 0.0613 | 0.0449
Epoch 176/300, seasonal_1 Loss: 0.0613 | 0.0449
Epoch 177/300, seasonal_1 Loss: 0.0613 | 0.0449
Epoch 178/300, seasonal_1 Loss: 0.0613 | 0.0449
Epoch 179/300, seasonal_1 Loss: 0.0613 | 0.0449
Epoch 180/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 181/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 182/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 183/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 184/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 185/300, seasonal_1 Loss: 0.0612 | 0.0449
Epoch 186/300, seasonal_1 Loss: 0.0611 | 0.0449
Epoch 187/300, seasonal_1 Loss: 0.0611 | 0.0449
Epoch 188/300, seasonal_1 Loss: 0.0611 | 0.0449
Epoch 189/300, seasonal_1 Loss: 0.0611 | 0.0448
Epoch 190/300, seasonal_1 Loss: 0.0611 | 0.0448
Epoch 191/300, seasonal_1 Loss: 0.0611 | 0.0448
Epoch 192/300, seasonal_1 Loss: 0.0611 | 0.0448
Epoch 193/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 194/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 195/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 196/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 197/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 198/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 199/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 200/300, seasonal_1 Loss: 0.0610 | 0.0448
Epoch 201/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 202/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 203/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 204/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 205/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 206/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 207/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 208/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 209/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 210/300, seasonal_1 Loss: 0.0609 | 0.0448
Epoch 211/300, seasonal_1 Loss: 0.0608 | 0.0448
Epoch 212/300, seasonal_1 Loss: 0.0608 | 0.0448
Epoch 213/300, seasonal_1 Loss: 0.0608 | 0.0448
Epoch 214/300, seasonal_1 Loss: 0.0608 | 0.0448
Epoch 215/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 216/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 217/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 218/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 219/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 220/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 221/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 222/300, seasonal_1 Loss: 0.0608 | 0.0447
Epoch 223/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 224/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 225/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 226/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 227/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 228/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 229/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 230/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 231/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 232/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 233/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 234/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 235/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 236/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 237/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 238/300, seasonal_1 Loss: 0.0607 | 0.0447
Epoch 239/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 240/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 241/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 242/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 243/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 244/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 245/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 246/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 247/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 248/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 249/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 250/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 251/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 252/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 253/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 254/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 255/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 256/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 257/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 258/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 259/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 260/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 261/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 262/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 263/300, seasonal_1 Loss: 0.0606 | 0.0447
Epoch 264/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 265/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 266/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 267/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 268/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 269/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 270/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 271/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 272/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 273/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 274/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 275/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 276/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 277/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 278/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 279/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 280/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 281/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 282/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 283/300, seasonal_1 Loss: 0.0605 | 0.0447
Epoch 284/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 285/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 286/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 287/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 288/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 289/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 290/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 291/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 292/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 293/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 294/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 295/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 296/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 297/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 298/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 299/300, seasonal_1 Loss: 0.0605 | 0.0446
Epoch 300/300, seasonal_1 Loss: 0.0605 | 0.0446
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.8366222479405765, 'learning_rate': 0.0002755448625132184, 'batch_size': 137, 'step_size': 14, 'gamma': 0.8385649722264282}
Epoch 1/300, seasonal_2 Loss: 0.4227 | 0.1975
Epoch 2/300, seasonal_2 Loss: 0.2256 | 0.1528
Epoch 3/300, seasonal_2 Loss: 0.2126 | 0.1248
Epoch 4/300, seasonal_2 Loss: 0.1679 | 0.1063
Epoch 5/300, seasonal_2 Loss: 0.1620 | 0.1316
Epoch 6/300, seasonal_2 Loss: 0.1645 | 0.1560
Epoch 7/300, seasonal_2 Loss: 0.1571 | 0.1132
Epoch 8/300, seasonal_2 Loss: 0.1382 | 0.0788
Epoch 9/300, seasonal_2 Loss: 0.1465 | 0.1059
Epoch 10/300, seasonal_2 Loss: 0.1548 | 0.0821
Epoch 11/300, seasonal_2 Loss: 0.1441 | 0.0798
Epoch 12/300, seasonal_2 Loss: 0.1402 | 0.0870
Epoch 13/300, seasonal_2 Loss: 0.1280 | 0.0723
Epoch 14/300, seasonal_2 Loss: 0.1133 | 0.0629
Epoch 15/300, seasonal_2 Loss: 0.1204 | 0.0791
Epoch 16/300, seasonal_2 Loss: 0.1235 | 0.0682
Epoch 17/300, seasonal_2 Loss: 0.1211 | 0.0645
Epoch 18/300, seasonal_2 Loss: 0.1152 | 0.0670
Epoch 19/300, seasonal_2 Loss: 0.1081 | 0.0626
Epoch 20/300, seasonal_2 Loss: 0.1073 | 0.0601
Epoch 21/300, seasonal_2 Loss: 0.1119 | 0.0637
Epoch 22/300, seasonal_2 Loss: 0.1134 | 0.0822
Epoch 23/300, seasonal_2 Loss: 0.1098 | 0.0665
Epoch 24/300, seasonal_2 Loss: 0.1068 | 0.0735
Epoch 25/300, seasonal_2 Loss: 0.1081 | 0.0932
Epoch 26/300, seasonal_2 Loss: 0.1068 | 0.0625
Epoch 27/300, seasonal_2 Loss: 0.1005 | 0.0555
Epoch 28/300, seasonal_2 Loss: 0.1005 | 0.0626
Epoch 29/300, seasonal_2 Loss: 0.1066 | 0.0668
Epoch 30/300, seasonal_2 Loss: 0.1118 | 0.0578
Epoch 31/300, seasonal_2 Loss: 0.1020 | 0.0594
Epoch 32/300, seasonal_2 Loss: 0.0968 | 0.0582
Epoch 33/300, seasonal_2 Loss: 0.1013 | 0.0579
Epoch 34/300, seasonal_2 Loss: 0.0975 | 0.0563
Epoch 35/300, seasonal_2 Loss: 0.0925 | 0.0558
Epoch 36/300, seasonal_2 Loss: 0.0915 | 0.0541
Epoch 37/300, seasonal_2 Loss: 0.0909 | 0.0498
Epoch 38/300, seasonal_2 Loss: 0.0903 | 0.0490
Epoch 39/300, seasonal_2 Loss: 0.0897 | 0.0492
Epoch 40/300, seasonal_2 Loss: 0.0890 | 0.0500
Epoch 41/300, seasonal_2 Loss: 0.0883 | 0.0519
Epoch 42/300, seasonal_2 Loss: 0.0874 | 0.0526
Epoch 43/300, seasonal_2 Loss: 0.0864 | 0.0512
Epoch 44/300, seasonal_2 Loss: 0.0857 | 0.0500
Epoch 45/300, seasonal_2 Loss: 0.0853 | 0.0501
Epoch 46/300, seasonal_2 Loss: 0.0853 | 0.0497
Epoch 47/300, seasonal_2 Loss: 0.0854 | 0.0492
Epoch 48/300, seasonal_2 Loss: 0.0856 | 0.0487
Epoch 49/300, seasonal_2 Loss: 0.0857 | 0.0481
Epoch 50/300, seasonal_2 Loss: 0.0855 | 0.0464
Epoch 51/300, seasonal_2 Loss: 0.0846 | 0.0457
Epoch 52/300, seasonal_2 Loss: 0.0839 | 0.0470
Epoch 53/300, seasonal_2 Loss: 0.0840 | 0.0482
Epoch 54/300, seasonal_2 Loss: 0.0840 | 0.0481
Epoch 55/300, seasonal_2 Loss: 0.0837 | 0.0477
Epoch 56/300, seasonal_2 Loss: 0.0829 | 0.0463
Epoch 57/300, seasonal_2 Loss: 0.0819 | 0.0442
Epoch 58/300, seasonal_2 Loss: 0.0815 | 0.0432
Epoch 59/300, seasonal_2 Loss: 0.0818 | 0.0431
Epoch 60/300, seasonal_2 Loss: 0.0820 | 0.0432
Epoch 61/300, seasonal_2 Loss: 0.0814 | 0.0433
Epoch 62/300, seasonal_2 Loss: 0.0805 | 0.0437
Epoch 63/300, seasonal_2 Loss: 0.0802 | 0.0441
Epoch 64/300, seasonal_2 Loss: 0.0800 | 0.0438
Epoch 65/300, seasonal_2 Loss: 0.0797 | 0.0431
Epoch 66/300, seasonal_2 Loss: 0.0793 | 0.0427
Epoch 67/300, seasonal_2 Loss: 0.0789 | 0.0420
Epoch 68/300, seasonal_2 Loss: 0.0786 | 0.0416
Epoch 69/300, seasonal_2 Loss: 0.0785 | 0.0414
Epoch 70/300, seasonal_2 Loss: 0.0783 | 0.0413
Epoch 71/300, seasonal_2 Loss: 0.0781 | 0.0411
Epoch 72/300, seasonal_2 Loss: 0.0779 | 0.0410
Epoch 73/300, seasonal_2 Loss: 0.0777 | 0.0410
Epoch 74/300, seasonal_2 Loss: 0.0775 | 0.0408
Epoch 75/300, seasonal_2 Loss: 0.0774 | 0.0407
Epoch 76/300, seasonal_2 Loss: 0.0772 | 0.0405
Epoch 77/300, seasonal_2 Loss: 0.0770 | 0.0403
Epoch 78/300, seasonal_2 Loss: 0.0769 | 0.0402
Epoch 79/300, seasonal_2 Loss: 0.0767 | 0.0400
Epoch 80/300, seasonal_2 Loss: 0.0766 | 0.0399
Epoch 81/300, seasonal_2 Loss: 0.0765 | 0.0398
Epoch 82/300, seasonal_2 Loss: 0.0763 | 0.0397
Epoch 83/300, seasonal_2 Loss: 0.0762 | 0.0396
Epoch 84/300, seasonal_2 Loss: 0.0761 | 0.0394
Epoch 85/300, seasonal_2 Loss: 0.0759 | 0.0393
Epoch 86/300, seasonal_2 Loss: 0.0758 | 0.0392
Epoch 87/300, seasonal_2 Loss: 0.0757 | 0.0392
Epoch 88/300, seasonal_2 Loss: 0.0756 | 0.0390
Epoch 89/300, seasonal_2 Loss: 0.0755 | 0.0390
Epoch 90/300, seasonal_2 Loss: 0.0754 | 0.0389
Epoch 91/300, seasonal_2 Loss: 0.0752 | 0.0388
Epoch 92/300, seasonal_2 Loss: 0.0751 | 0.0387
Epoch 93/300, seasonal_2 Loss: 0.0750 | 0.0387
Epoch 94/300, seasonal_2 Loss: 0.0749 | 0.0386
Epoch 95/300, seasonal_2 Loss: 0.0748 | 0.0385
Epoch 96/300, seasonal_2 Loss: 0.0747 | 0.0385
Epoch 97/300, seasonal_2 Loss: 0.0746 | 0.0384
Epoch 98/300, seasonal_2 Loss: 0.0746 | 0.0383
Epoch 99/300, seasonal_2 Loss: 0.0744 | 0.0383
Epoch 100/300, seasonal_2 Loss: 0.0744 | 0.0382
Epoch 101/300, seasonal_2 Loss: 0.0743 | 0.0382
Epoch 102/300, seasonal_2 Loss: 0.0742 | 0.0381
Epoch 103/300, seasonal_2 Loss: 0.0741 | 0.0380
Epoch 104/300, seasonal_2 Loss: 0.0740 | 0.0380
Epoch 105/300, seasonal_2 Loss: 0.0740 | 0.0379
Epoch 106/300, seasonal_2 Loss: 0.0739 | 0.0379
Epoch 107/300, seasonal_2 Loss: 0.0738 | 0.0378
Epoch 108/300, seasonal_2 Loss: 0.0737 | 0.0378
Epoch 109/300, seasonal_2 Loss: 0.0736 | 0.0378
Epoch 110/300, seasonal_2 Loss: 0.0736 | 0.0377
Epoch 111/300, seasonal_2 Loss: 0.0735 | 0.0377
Epoch 112/300, seasonal_2 Loss: 0.0734 | 0.0376
Epoch 113/300, seasonal_2 Loss: 0.0734 | 0.0376
Epoch 114/300, seasonal_2 Loss: 0.0733 | 0.0376
Epoch 115/300, seasonal_2 Loss: 0.0732 | 0.0375
Epoch 116/300, seasonal_2 Loss: 0.0732 | 0.0375
Epoch 117/300, seasonal_2 Loss: 0.0731 | 0.0374
Epoch 118/300, seasonal_2 Loss: 0.0730 | 0.0374
Epoch 119/300, seasonal_2 Loss: 0.0730 | 0.0374
Epoch 120/300, seasonal_2 Loss: 0.0729 | 0.0373
Epoch 121/300, seasonal_2 Loss: 0.0728 | 0.0373
Epoch 122/300, seasonal_2 Loss: 0.0728 | 0.0373
Epoch 123/300, seasonal_2 Loss: 0.0727 | 0.0372
Epoch 124/300, seasonal_2 Loss: 0.0727 | 0.0372
Epoch 125/300, seasonal_2 Loss: 0.0726 | 0.0372
Epoch 126/300, seasonal_2 Loss: 0.0726 | 0.0371
Epoch 127/300, seasonal_2 Loss: 0.0725 | 0.0371
Epoch 128/300, seasonal_2 Loss: 0.0724 | 0.0371
Epoch 129/300, seasonal_2 Loss: 0.0724 | 0.0371
Epoch 130/300, seasonal_2 Loss: 0.0723 | 0.0370
Epoch 131/300, seasonal_2 Loss: 0.0723 | 0.0370
Epoch 132/300, seasonal_2 Loss: 0.0722 | 0.0370
Epoch 133/300, seasonal_2 Loss: 0.0722 | 0.0369
Epoch 134/300, seasonal_2 Loss: 0.0721 | 0.0369
Epoch 135/300, seasonal_2 Loss: 0.0721 | 0.0369
Epoch 136/300, seasonal_2 Loss: 0.0720 | 0.0369
Epoch 137/300, seasonal_2 Loss: 0.0720 | 0.0368
Epoch 138/300, seasonal_2 Loss: 0.0719 | 0.0368
Epoch 139/300, seasonal_2 Loss: 0.0719 | 0.0368
Epoch 140/300, seasonal_2 Loss: 0.0719 | 0.0368
Epoch 141/300, seasonal_2 Loss: 0.0718 | 0.0367
Epoch 142/300, seasonal_2 Loss: 0.0718 | 0.0367
Epoch 143/300, seasonal_2 Loss: 0.0717 | 0.0367
Epoch 144/300, seasonal_2 Loss: 0.0717 | 0.0367
Epoch 145/300, seasonal_2 Loss: 0.0716 | 0.0366
Epoch 146/300, seasonal_2 Loss: 0.0716 | 0.0366
Epoch 147/300, seasonal_2 Loss: 0.0716 | 0.0366
Epoch 148/300, seasonal_2 Loss: 0.0715 | 0.0366
Epoch 149/300, seasonal_2 Loss: 0.0715 | 0.0366
Epoch 150/300, seasonal_2 Loss: 0.0714 | 0.0365
Epoch 151/300, seasonal_2 Loss: 0.0714 | 0.0365
Epoch 152/300, seasonal_2 Loss: 0.0714 | 0.0365
Epoch 153/300, seasonal_2 Loss: 0.0713 | 0.0365
Epoch 154/300, seasonal_2 Loss: 0.0713 | 0.0365
Epoch 155/300, seasonal_2 Loss: 0.0713 | 0.0364
Epoch 156/300, seasonal_2 Loss: 0.0712 | 0.0364
Epoch 157/300, seasonal_2 Loss: 0.0712 | 0.0364
Epoch 158/300, seasonal_2 Loss: 0.0712 | 0.0364
Epoch 159/300, seasonal_2 Loss: 0.0711 | 0.0364
Epoch 160/300, seasonal_2 Loss: 0.0711 | 0.0363
Epoch 161/300, seasonal_2 Loss: 0.0711 | 0.0363
Epoch 162/300, seasonal_2 Loss: 0.0710 | 0.0363
Epoch 163/300, seasonal_2 Loss: 0.0710 | 0.0363
Epoch 164/300, seasonal_2 Loss: 0.0710 | 0.0363
Epoch 165/300, seasonal_2 Loss: 0.0710 | 0.0363
Epoch 166/300, seasonal_2 Loss: 0.0709 | 0.0362
Epoch 167/300, seasonal_2 Loss: 0.0709 | 0.0362
Epoch 168/300, seasonal_2 Loss: 0.0709 | 0.0362
Epoch 169/300, seasonal_2 Loss: 0.0709 | 0.0362
Epoch 170/300, seasonal_2 Loss: 0.0708 | 0.0362
Epoch 171/300, seasonal_2 Loss: 0.0708 | 0.0362
Epoch 172/300, seasonal_2 Loss: 0.0708 | 0.0362
Epoch 173/300, seasonal_2 Loss: 0.0708 | 0.0361
Epoch 174/300, seasonal_2 Loss: 0.0707 | 0.0361
Epoch 175/300, seasonal_2 Loss: 0.0707 | 0.0361
Epoch 176/300, seasonal_2 Loss: 0.0707 | 0.0361
Epoch 177/300, seasonal_2 Loss: 0.0707 | 0.0361
Epoch 178/300, seasonal_2 Loss: 0.0707 | 0.0361
Epoch 179/300, seasonal_2 Loss: 0.0706 | 0.0361
Epoch 180/300, seasonal_2 Loss: 0.0706 | 0.0361
Epoch 181/300, seasonal_2 Loss: 0.0706 | 0.0360
Epoch 182/300, seasonal_2 Loss: 0.0706 | 0.0360
Epoch 183/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 184/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 185/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 186/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 187/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 188/300, seasonal_2 Loss: 0.0705 | 0.0360
Epoch 189/300, seasonal_2 Loss: 0.0704 | 0.0360
Epoch 190/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 191/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 192/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 193/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 194/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 195/300, seasonal_2 Loss: 0.0704 | 0.0359
Epoch 196/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 197/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 198/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 199/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 200/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 201/300, seasonal_2 Loss: 0.0703 | 0.0359
Epoch 202/300, seasonal_2 Loss: 0.0703 | 0.0358
Epoch 203/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 204/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 205/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 206/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 207/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 208/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 209/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 210/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 211/300, seasonal_2 Loss: 0.0702 | 0.0358
Epoch 212/300, seasonal_2 Loss: 0.0701 | 0.0358
Epoch 213/300, seasonal_2 Loss: 0.0701 | 0.0358
Epoch 214/300, seasonal_2 Loss: 0.0701 | 0.0358
Epoch 215/300, seasonal_2 Loss: 0.0701 | 0.0358
Epoch 216/300, seasonal_2 Loss: 0.0701 | 0.0358
Epoch 217/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 218/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 219/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 220/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 221/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 222/300, seasonal_2 Loss: 0.0701 | 0.0357
Epoch 223/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 224/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 225/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 226/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 227/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 228/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 229/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 230/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 231/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 232/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 233/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 234/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 235/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 236/300, seasonal_2 Loss: 0.0700 | 0.0357
Epoch 237/300, seasonal_2 Loss: 0.0699 | 0.0357
Epoch 238/300, seasonal_2 Loss: 0.0699 | 0.0357
Epoch 239/300, seasonal_2 Loss: 0.0699 | 0.0357
Epoch 240/300, seasonal_2 Loss: 0.0699 | 0.0357
Epoch 241/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 242/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 243/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 244/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 245/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 246/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 247/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 248/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 249/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 250/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 251/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 252/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 253/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 254/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 255/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 256/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 257/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 258/300, seasonal_2 Loss: 0.0699 | 0.0356
Epoch 259/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 260/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 261/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 262/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 263/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 264/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 265/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 266/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 267/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 268/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 269/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 270/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 271/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 272/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 273/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 274/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 275/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 276/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 277/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 278/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 279/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 280/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 281/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 282/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 283/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 284/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 285/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 286/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 287/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 288/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 289/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 290/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 291/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 292/300, seasonal_2 Loss: 0.0698 | 0.0356
Epoch 293/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 294/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 295/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 296/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 297/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 298/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 299/300, seasonal_2 Loss: 0.0698 | 0.0355
Epoch 300/300, seasonal_2 Loss: 0.0698 | 0.0355
Training seasonal_3 component with params: {'observation_period_num': 7, 'train_rates': 0.8508633419045722, 'learning_rate': 0.0006639530582666196, 'batch_size': 151, 'step_size': 7, 'gamma': 0.9416458963466766}
Epoch 1/300, seasonal_3 Loss: 0.5684 | 0.1625
Epoch 2/300, seasonal_3 Loss: 0.2122 | 0.1568
Epoch 3/300, seasonal_3 Loss: 0.1861 | 0.1114
Epoch 4/300, seasonal_3 Loss: 0.1537 | 0.0907
Epoch 5/300, seasonal_3 Loss: 0.1362 | 0.0711
Epoch 6/300, seasonal_3 Loss: 0.1279 | 0.0625
Epoch 7/300, seasonal_3 Loss: 0.1183 | 0.0615
Epoch 8/300, seasonal_3 Loss: 0.1183 | 0.0622
Epoch 9/300, seasonal_3 Loss: 0.1262 | 0.0695
Epoch 10/300, seasonal_3 Loss: 0.1263 | 0.0674
Epoch 11/300, seasonal_3 Loss: 0.1147 | 0.0649
Epoch 12/300, seasonal_3 Loss: 0.1153 | 0.0660
Epoch 13/300, seasonal_3 Loss: 0.1144 | 0.0615
Epoch 14/300, seasonal_3 Loss: 0.1343 | 0.1172
Epoch 15/300, seasonal_3 Loss: 0.1261 | 0.0793
Epoch 16/300, seasonal_3 Loss: 0.1100 | 0.0531
Epoch 17/300, seasonal_3 Loss: 0.1072 | 0.0636
Epoch 18/300, seasonal_3 Loss: 0.1109 | 0.0681
Epoch 19/300, seasonal_3 Loss: 0.1140 | 0.0596
Epoch 20/300, seasonal_3 Loss: 0.1035 | 0.0621
Epoch 21/300, seasonal_3 Loss: 0.1046 | 0.0630
Epoch 22/300, seasonal_3 Loss: 0.0972 | 0.0463
Epoch 23/300, seasonal_3 Loss: 0.0952 | 0.0485
Epoch 24/300, seasonal_3 Loss: 0.0932 | 0.0454
Epoch 25/300, seasonal_3 Loss: 0.0907 | 0.0439
Epoch 26/300, seasonal_3 Loss: 0.0915 | 0.0462
Epoch 27/300, seasonal_3 Loss: 0.0909 | 0.0451
Epoch 28/300, seasonal_3 Loss: 0.0889 | 0.0441
Epoch 29/300, seasonal_3 Loss: 0.0867 | 0.0463
Epoch 30/300, seasonal_3 Loss: 0.0857 | 0.0427
Epoch 31/300, seasonal_3 Loss: 0.0844 | 0.0435
Epoch 32/300, seasonal_3 Loss: 0.0837 | 0.0412
Epoch 33/300, seasonal_3 Loss: 0.0829 | 0.0391
Epoch 34/300, seasonal_3 Loss: 0.0839 | 0.0395
Epoch 35/300, seasonal_3 Loss: 0.0830 | 0.0378
Epoch 36/300, seasonal_3 Loss: 0.0818 | 0.0375
Epoch 37/300, seasonal_3 Loss: 0.0806 | 0.0376
Epoch 38/300, seasonal_3 Loss: 0.0796 | 0.0372
Epoch 39/300, seasonal_3 Loss: 0.0786 | 0.0374
Epoch 40/300, seasonal_3 Loss: 0.0780 | 0.0376
Epoch 41/300, seasonal_3 Loss: 0.0779 | 0.0377
Epoch 42/300, seasonal_3 Loss: 0.0779 | 0.0376
Epoch 43/300, seasonal_3 Loss: 0.0776 | 0.0378
Epoch 44/300, seasonal_3 Loss: 0.0772 | 0.0383
Epoch 45/300, seasonal_3 Loss: 0.0768 | 0.0378
Epoch 46/300, seasonal_3 Loss: 0.0761 | 0.0366
Epoch 47/300, seasonal_3 Loss: 0.0754 | 0.0351
Epoch 48/300, seasonal_3 Loss: 0.0753 | 0.0345
Epoch 49/300, seasonal_3 Loss: 0.0756 | 0.0344
Epoch 50/300, seasonal_3 Loss: 0.0763 | 0.0344
Epoch 51/300, seasonal_3 Loss: 0.0774 | 0.0350
Epoch 52/300, seasonal_3 Loss: 0.0769 | 0.0354
Epoch 53/300, seasonal_3 Loss: 0.0750 | 0.0356
Epoch 54/300, seasonal_3 Loss: 0.0750 | 0.0360
Epoch 55/300, seasonal_3 Loss: 0.0746 | 0.0352
Epoch 56/300, seasonal_3 Loss: 0.0742 | 0.0338
Epoch 57/300, seasonal_3 Loss: 0.0735 | 0.0342
Epoch 58/300, seasonal_3 Loss: 0.0744 | 0.0352
Epoch 59/300, seasonal_3 Loss: 0.0756 | 0.0374
Epoch 60/300, seasonal_3 Loss: 0.0766 | 0.0361
Epoch 61/300, seasonal_3 Loss: 0.0757 | 0.0384
Epoch 62/300, seasonal_3 Loss: 0.0764 | 0.0346
Epoch 63/300, seasonal_3 Loss: 0.0755 | 0.0368
Epoch 64/300, seasonal_3 Loss: 0.0794 | 0.0394
Epoch 65/300, seasonal_3 Loss: 0.0831 | 0.0368
Epoch 66/300, seasonal_3 Loss: 0.0773 | 0.0359
Epoch 67/300, seasonal_3 Loss: 0.0764 | 0.0361
Epoch 68/300, seasonal_3 Loss: 0.0783 | 0.0351
Epoch 69/300, seasonal_3 Loss: 0.0783 | 0.0348
Epoch 70/300, seasonal_3 Loss: 0.0743 | 0.0365
Epoch 71/300, seasonal_3 Loss: 0.0751 | 0.0346
Epoch 72/300, seasonal_3 Loss: 0.0753 | 0.0340
Epoch 73/300, seasonal_3 Loss: 0.0742 | 0.0341
Epoch 74/300, seasonal_3 Loss: 0.0744 | 0.0339
Epoch 75/300, seasonal_3 Loss: 0.0730 | 0.0330
Epoch 76/300, seasonal_3 Loss: 0.0737 | 0.0329
Epoch 77/300, seasonal_3 Loss: 0.0770 | 0.0361
Epoch 78/300, seasonal_3 Loss: 0.0774 | 0.0359
Epoch 79/300, seasonal_3 Loss: 0.0745 | 0.0327
Epoch 80/300, seasonal_3 Loss: 0.0743 | 0.0351
Epoch 81/300, seasonal_3 Loss: 0.0782 | 0.0363
Epoch 82/300, seasonal_3 Loss: 0.0801 | 0.0363
Epoch 83/300, seasonal_3 Loss: 0.0817 | 0.0394
Epoch 84/300, seasonal_3 Loss: 0.0787 | 0.0362
Epoch 85/300, seasonal_3 Loss: 0.0755 | 0.0343
Epoch 86/300, seasonal_3 Loss: 0.0778 | 0.0347
Epoch 87/300, seasonal_3 Loss: 0.0821 | 0.0395
Epoch 88/300, seasonal_3 Loss: 0.0850 | 0.0422
Epoch 89/300, seasonal_3 Loss: 0.0867 | 0.0521
Epoch 90/300, seasonal_3 Loss: 0.0810 | 0.0386
Epoch 91/300, seasonal_3 Loss: 0.0726 | 0.0333
Epoch 92/300, seasonal_3 Loss: 0.0736 | 0.0330
Epoch 93/300, seasonal_3 Loss: 0.0735 | 0.0318
Epoch 94/300, seasonal_3 Loss: 0.0700 | 0.0324
Epoch 95/300, seasonal_3 Loss: 0.0705 | 0.0323
Epoch 96/300, seasonal_3 Loss: 0.0694 | 0.0322
Epoch 97/300, seasonal_3 Loss: 0.0684 | 0.0321
Epoch 98/300, seasonal_3 Loss: 0.0682 | 0.0313
Epoch 99/300, seasonal_3 Loss: 0.0681 | 0.0317
Epoch 100/300, seasonal_3 Loss: 0.0680 | 0.0314
Epoch 101/300, seasonal_3 Loss: 0.0683 | 0.0315
Epoch 102/300, seasonal_3 Loss: 0.0680 | 0.0315
Epoch 103/300, seasonal_3 Loss: 0.0676 | 0.0310
Epoch 104/300, seasonal_3 Loss: 0.0673 | 0.0314
Epoch 105/300, seasonal_3 Loss: 0.0669 | 0.0308
Epoch 106/300, seasonal_3 Loss: 0.0666 | 0.0309
Epoch 107/300, seasonal_3 Loss: 0.0665 | 0.0308
Epoch 108/300, seasonal_3 Loss: 0.0663 | 0.0305
Epoch 109/300, seasonal_3 Loss: 0.0662 | 0.0309
Epoch 110/300, seasonal_3 Loss: 0.0660 | 0.0304
Epoch 111/300, seasonal_3 Loss: 0.0658 | 0.0305
Epoch 112/300, seasonal_3 Loss: 0.0656 | 0.0304
Epoch 113/300, seasonal_3 Loss: 0.0654 | 0.0301
Epoch 114/300, seasonal_3 Loss: 0.0653 | 0.0304
Epoch 115/300, seasonal_3 Loss: 0.0652 | 0.0300
Epoch 116/300, seasonal_3 Loss: 0.0651 | 0.0302
Epoch 117/300, seasonal_3 Loss: 0.0649 | 0.0300
Epoch 118/300, seasonal_3 Loss: 0.0648 | 0.0299
Epoch 119/300, seasonal_3 Loss: 0.0648 | 0.0300
Epoch 120/300, seasonal_3 Loss: 0.0646 | 0.0298
Epoch 121/300, seasonal_3 Loss: 0.0646 | 0.0298
Epoch 122/300, seasonal_3 Loss: 0.0644 | 0.0298
Epoch 123/300, seasonal_3 Loss: 0.0643 | 0.0297
Epoch 124/300, seasonal_3 Loss: 0.0642 | 0.0298
Epoch 125/300, seasonal_3 Loss: 0.0641 | 0.0296
Epoch 126/300, seasonal_3 Loss: 0.0640 | 0.0297
Epoch 127/300, seasonal_3 Loss: 0.0639 | 0.0296
Epoch 128/300, seasonal_3 Loss: 0.0638 | 0.0296
Epoch 129/300, seasonal_3 Loss: 0.0638 | 0.0296
Epoch 130/300, seasonal_3 Loss: 0.0637 | 0.0295
Epoch 131/300, seasonal_3 Loss: 0.0636 | 0.0295
Epoch 132/300, seasonal_3 Loss: 0.0635 | 0.0295
Epoch 133/300, seasonal_3 Loss: 0.0634 | 0.0295
Epoch 134/300, seasonal_3 Loss: 0.0633 | 0.0295
Epoch 135/300, seasonal_3 Loss: 0.0633 | 0.0295
Epoch 136/300, seasonal_3 Loss: 0.0632 | 0.0295
Epoch 137/300, seasonal_3 Loss: 0.0631 | 0.0295
Epoch 138/300, seasonal_3 Loss: 0.0631 | 0.0295
Epoch 139/300, seasonal_3 Loss: 0.0630 | 0.0295
Epoch 140/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 141/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 142/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 143/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 144/300, seasonal_3 Loss: 0.0627 | 0.0295
Epoch 145/300, seasonal_3 Loss: 0.0627 | 0.0295
Epoch 146/300, seasonal_3 Loss: 0.0626 | 0.0296
Epoch 147/300, seasonal_3 Loss: 0.0626 | 0.0296
Epoch 148/300, seasonal_3 Loss: 0.0625 | 0.0297
Epoch 149/300, seasonal_3 Loss: 0.0625 | 0.0297
Epoch 150/300, seasonal_3 Loss: 0.0625 | 0.0298
Epoch 151/300, seasonal_3 Loss: 0.0624 | 0.0298
Epoch 152/300, seasonal_3 Loss: 0.0624 | 0.0301
Epoch 153/300, seasonal_3 Loss: 0.0624 | 0.0302
Epoch 154/300, seasonal_3 Loss: 0.0624 | 0.0304
Epoch 155/300, seasonal_3 Loss: 0.0625 | 0.0307
Epoch 156/300, seasonal_3 Loss: 0.0626 | 0.0310
Epoch 157/300, seasonal_3 Loss: 0.0629 | 0.0309
Epoch 158/300, seasonal_3 Loss: 0.0634 | 0.0300
Epoch 159/300, seasonal_3 Loss: 0.0647 | 0.0315
Epoch 160/300, seasonal_3 Loss: 0.0653 | 0.0316
Epoch 161/300, seasonal_3 Loss: 0.0664 | 0.0309
Epoch 162/300, seasonal_3 Loss: 0.0645 | 0.0299
Epoch 163/300, seasonal_3 Loss: 0.0634 | 0.0300
Epoch 164/300, seasonal_3 Loss: 0.0632 | 0.0301
Epoch 165/300, seasonal_3 Loss: 0.0635 | 0.0304
Epoch 166/300, seasonal_3 Loss: 0.0634 | 0.0300
Epoch 167/300, seasonal_3 Loss: 0.0634 | 0.0299
Epoch 168/300, seasonal_3 Loss: 0.0628 | 0.0293
Epoch 169/300, seasonal_3 Loss: 0.0621 | 0.0294
Epoch 170/300, seasonal_3 Loss: 0.0619 | 0.0293
Epoch 171/300, seasonal_3 Loss: 0.0619 | 0.0294
Epoch 172/300, seasonal_3 Loss: 0.0620 | 0.0295
Epoch 173/300, seasonal_3 Loss: 0.0622 | 0.0297
Epoch 174/300, seasonal_3 Loss: 0.0623 | 0.0297
Epoch 175/300, seasonal_3 Loss: 0.0624 | 0.0295
Epoch 176/300, seasonal_3 Loss: 0.0620 | 0.0292
Epoch 177/300, seasonal_3 Loss: 0.0616 | 0.0292
Epoch 178/300, seasonal_3 Loss: 0.0615 | 0.0293
Epoch 179/300, seasonal_3 Loss: 0.0615 | 0.0293
Epoch 180/300, seasonal_3 Loss: 0.0614 | 0.0293
Epoch 181/300, seasonal_3 Loss: 0.0614 | 0.0293
Epoch 182/300, seasonal_3 Loss: 0.0615 | 0.0294
Epoch 183/300, seasonal_3 Loss: 0.0616 | 0.0295
Epoch 184/300, seasonal_3 Loss: 0.0616 | 0.0295
Epoch 185/300, seasonal_3 Loss: 0.0616 | 0.0294
Epoch 186/300, seasonal_3 Loss: 0.0614 | 0.0293
Epoch 187/300, seasonal_3 Loss: 0.0612 | 0.0292
Epoch 188/300, seasonal_3 Loss: 0.0611 | 0.0292
Epoch 189/300, seasonal_3 Loss: 0.0611 | 0.0293
Epoch 190/300, seasonal_3 Loss: 0.0611 | 0.0293
Epoch 191/300, seasonal_3 Loss: 0.0610 | 0.0293
Epoch 192/300, seasonal_3 Loss: 0.0610 | 0.0293
Epoch 193/300, seasonal_3 Loss: 0.0611 | 0.0294
Epoch 194/300, seasonal_3 Loss: 0.0611 | 0.0294
Epoch 195/300, seasonal_3 Loss: 0.0611 | 0.0294
Epoch 196/300, seasonal_3 Loss: 0.0610 | 0.0293
Epoch 197/300, seasonal_3 Loss: 0.0609 | 0.0292
Epoch 198/300, seasonal_3 Loss: 0.0608 | 0.0292
Epoch 199/300, seasonal_3 Loss: 0.0608 | 0.0292
Epoch 200/300, seasonal_3 Loss: 0.0608 | 0.0292
Epoch 201/300, seasonal_3 Loss: 0.0608 | 0.0293
Epoch 202/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 203/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 204/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 205/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 206/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 207/300, seasonal_3 Loss: 0.0607 | 0.0293
Epoch 208/300, seasonal_3 Loss: 0.0606 | 0.0292
Epoch 209/300, seasonal_3 Loss: 0.0606 | 0.0292
Epoch 210/300, seasonal_3 Loss: 0.0606 | 0.0292
Epoch 211/300, seasonal_3 Loss: 0.0605 | 0.0292
Epoch 212/300, seasonal_3 Loss: 0.0605 | 0.0292
Epoch 213/300, seasonal_3 Loss: 0.0605 | 0.0292
Epoch 214/300, seasonal_3 Loss: 0.0605 | 0.0292
Epoch 215/300, seasonal_3 Loss: 0.0605 | 0.0293
Epoch 216/300, seasonal_3 Loss: 0.0605 | 0.0292
Epoch 217/300, seasonal_3 Loss: 0.0604 | 0.0292
Epoch 218/300, seasonal_3 Loss: 0.0604 | 0.0292
Epoch 219/300, seasonal_3 Loss: 0.0604 | 0.0292
Epoch 220/300, seasonal_3 Loss: 0.0604 | 0.0292
Epoch 221/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 222/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 223/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 224/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 225/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 226/300, seasonal_3 Loss: 0.0603 | 0.0292
Epoch 227/300, seasonal_3 Loss: 0.0602 | 0.0292
Epoch 228/300, seasonal_3 Loss: 0.0602 | 0.0292
Epoch 229/300, seasonal_3 Loss: 0.0602 | 0.0292
Epoch 230/300, seasonal_3 Loss: 0.0602 | 0.0292
Epoch 231/300, seasonal_3 Loss: 0.0602 | 0.0292
Epoch 232/300, seasonal_3 Loss: 0.0602 | 0.0291
Epoch 233/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 234/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 235/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 236/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 237/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 238/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 239/300, seasonal_3 Loss: 0.0601 | 0.0291
Epoch 240/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 241/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 242/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 243/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 244/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 245/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 246/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 247/300, seasonal_3 Loss: 0.0600 | 0.0291
Epoch 248/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 249/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 250/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 251/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 252/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 253/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 254/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 255/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 256/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 257/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 258/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 259/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 260/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 261/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 262/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 263/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 264/300, seasonal_3 Loss: 0.0598 | 0.0290
Epoch 265/300, seasonal_3 Loss: 0.0597 | 0.0290
Epoch 266/300, seasonal_3 Loss: 0.0597 | 0.0290
Epoch 267/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 268/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 269/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 270/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 271/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 272/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 273/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 274/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 275/300, seasonal_3 Loss: 0.0597 | 0.0289
Epoch 276/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 277/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 278/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 279/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 280/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 281/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 282/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 283/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 284/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 285/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 286/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 287/300, seasonal_3 Loss: 0.0596 | 0.0289
Epoch 288/300, seasonal_3 Loss: 0.0595 | 0.0289
Epoch 289/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 290/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 291/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 292/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 293/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 294/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 295/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 296/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 297/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 298/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 299/300, seasonal_3 Loss: 0.0595 | 0.0288
Epoch 300/300, seasonal_3 Loss: 0.0595 | 0.0288
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.880570471003369, 'learning_rate': 0.00017389891457605066, 'batch_size': 36, 'step_size': 6, 'gamma': 0.7660361017657058}
Epoch 1/300, resid Loss: 0.3452 | 0.1268
Epoch 2/300, resid Loss: 0.1423 | 0.0842
Epoch 3/300, resid Loss: 0.1252 | 0.0712
Epoch 4/300, resid Loss: 0.1151 | 0.0682
Epoch 5/300, resid Loss: 0.1109 | 0.0649
Epoch 6/300, resid Loss: 0.1078 | 0.0632
Epoch 7/300, resid Loss: 0.1054 | 0.0697
Epoch 8/300, resid Loss: 0.1052 | 0.0703
Epoch 9/300, resid Loss: 0.1036 | 0.0705
Epoch 10/300, resid Loss: 0.1013 | 0.0656
Epoch 11/300, resid Loss: 0.1004 | 0.0659
Epoch 12/300, resid Loss: 0.0993 | 0.0652
Epoch 13/300, resid Loss: 0.0980 | 0.0550
Epoch 14/300, resid Loss: 0.0985 | 0.0542
Epoch 15/300, resid Loss: 0.0966 | 0.0533
Epoch 16/300, resid Loss: 0.0938 | 0.0522
Epoch 17/300, resid Loss: 0.0916 | 0.0504
Epoch 18/300, resid Loss: 0.0895 | 0.0487
Epoch 19/300, resid Loss: 0.0877 | 0.0470
Epoch 20/300, resid Loss: 0.0865 | 0.0456
Epoch 21/300, resid Loss: 0.0855 | 0.0444
Epoch 22/300, resid Loss: 0.0846 | 0.0437
Epoch 23/300, resid Loss: 0.0840 | 0.0429
Epoch 24/300, resid Loss: 0.0834 | 0.0424
Epoch 25/300, resid Loss: 0.0829 | 0.0421
Epoch 26/300, resid Loss: 0.0825 | 0.0417
Epoch 27/300, resid Loss: 0.0822 | 0.0414
Epoch 28/300, resid Loss: 0.0819 | 0.0412
Epoch 29/300, resid Loss: 0.0817 | 0.0410
Epoch 30/300, resid Loss: 0.0814 | 0.0408
Epoch 31/300, resid Loss: 0.0812 | 0.0407
Epoch 32/300, resid Loss: 0.0811 | 0.0405
Epoch 33/300, resid Loss: 0.0809 | 0.0404
Epoch 34/300, resid Loss: 0.0807 | 0.0404
Epoch 35/300, resid Loss: 0.0806 | 0.0403
Epoch 36/300, resid Loss: 0.0805 | 0.0402
Epoch 37/300, resid Loss: 0.0804 | 0.0402
Epoch 38/300, resid Loss: 0.0803 | 0.0401
Epoch 39/300, resid Loss: 0.0802 | 0.0400
Epoch 40/300, resid Loss: 0.0801 | 0.0401
Epoch 41/300, resid Loss: 0.0800 | 0.0400
Epoch 42/300, resid Loss: 0.0799 | 0.0400
Epoch 43/300, resid Loss: 0.0798 | 0.0400
Epoch 44/300, resid Loss: 0.0798 | 0.0400
Epoch 45/300, resid Loss: 0.0797 | 0.0400
Epoch 46/300, resid Loss: 0.0796 | 0.0399
Epoch 47/300, resid Loss: 0.0796 | 0.0399
Epoch 48/300, resid Loss: 0.0795 | 0.0399
Epoch 49/300, resid Loss: 0.0795 | 0.0398
Epoch 50/300, resid Loss: 0.0794 | 0.0398
Epoch 51/300, resid Loss: 0.0793 | 0.0398
Epoch 52/300, resid Loss: 0.0793 | 0.0399
Epoch 53/300, resid Loss: 0.0792 | 0.0399
Epoch 54/300, resid Loss: 0.0792 | 0.0398
Epoch 55/300, resid Loss: 0.0792 | 0.0399
Epoch 56/300, resid Loss: 0.0791 | 0.0399
Epoch 57/300, resid Loss: 0.0791 | 0.0399
Epoch 58/300, resid Loss: 0.0791 | 0.0399
Epoch 59/300, resid Loss: 0.0791 | 0.0399
Epoch 60/300, resid Loss: 0.0790 | 0.0399
Epoch 61/300, resid Loss: 0.0790 | 0.0399
Epoch 62/300, resid Loss: 0.0790 | 0.0399
Epoch 63/300, resid Loss: 0.0790 | 0.0399
Epoch 64/300, resid Loss: 0.0790 | 0.0399
Epoch 65/300, resid Loss: 0.0790 | 0.0399
Epoch 66/300, resid Loss: 0.0789 | 0.0399
Epoch 67/300, resid Loss: 0.0789 | 0.0399
Epoch 68/300, resid Loss: 0.0789 | 0.0399
Epoch 69/300, resid Loss: 0.0789 | 0.0399
Epoch 70/300, resid Loss: 0.0789 | 0.0399
Epoch 71/300, resid Loss: 0.0789 | 0.0399
Epoch 72/300, resid Loss: 0.0789 | 0.0399
Epoch 73/300, resid Loss: 0.0789 | 0.0399
Epoch 74/300, resid Loss: 0.0789 | 0.0399
Epoch 75/300, resid Loss: 0.0789 | 0.0399
Epoch 76/300, resid Loss: 0.0788 | 0.0399
Epoch 77/300, resid Loss: 0.0788 | 0.0399
Epoch 78/300, resid Loss: 0.0788 | 0.0399
Epoch 79/300, resid Loss: 0.0788 | 0.0399
Epoch 80/300, resid Loss: 0.0788 | 0.0399
Epoch 81/300, resid Loss: 0.0788 | 0.0399
Epoch 82/300, resid Loss: 0.0788 | 0.0399
Epoch 83/300, resid Loss: 0.0788 | 0.0399
Epoch 84/300, resid Loss: 0.0788 | 0.0399
Epoch 85/300, resid Loss: 0.0788 | 0.0399
Epoch 86/300, resid Loss: 0.0788 | 0.0399
Epoch 87/300, resid Loss: 0.0788 | 0.0399
Epoch 88/300, resid Loss: 0.0788 | 0.0399
Epoch 89/300, resid Loss: 0.0788 | 0.0399
Epoch 90/300, resid Loss: 0.0788 | 0.0399
Epoch 91/300, resid Loss: 0.0788 | 0.0399
Epoch 92/300, resid Loss: 0.0788 | 0.0399
Epoch 93/300, resid Loss: 0.0788 | 0.0399
Epoch 94/300, resid Loss: 0.0788 | 0.0399
Epoch 95/300, resid Loss: 0.0788 | 0.0399
Epoch 96/300, resid Loss: 0.0788 | 0.0399
Epoch 97/300, resid Loss: 0.0788 | 0.0399
Epoch 98/300, resid Loss: 0.0788 | 0.0399
Epoch 99/300, resid Loss: 0.0788 | 0.0399
Epoch 100/300, resid Loss: 0.0788 | 0.0399
Epoch 101/300, resid Loss: 0.0788 | 0.0399
Epoch 102/300, resid Loss: 0.0788 | 0.0399
Epoch 103/300, resid Loss: 0.0788 | 0.0399
Epoch 104/300, resid Loss: 0.0788 | 0.0399
Epoch 105/300, resid Loss: 0.0788 | 0.0399
Epoch 106/300, resid Loss: 0.0788 | 0.0399
Epoch 107/300, resid Loss: 0.0788 | 0.0399
Epoch 108/300, resid Loss: 0.0788 | 0.0399
Epoch 109/300, resid Loss: 0.0788 | 0.0399
Epoch 110/300, resid Loss: 0.0788 | 0.0399
Epoch 111/300, resid Loss: 0.0788 | 0.0399
Epoch 112/300, resid Loss: 0.0788 | 0.0399
Epoch 113/300, resid Loss: 0.0788 | 0.0399
Epoch 114/300, resid Loss: 0.0788 | 0.0399
Epoch 115/300, resid Loss: 0.0788 | 0.0399
Epoch 116/300, resid Loss: 0.0788 | 0.0399
Epoch 117/300, resid Loss: 0.0788 | 0.0399
Epoch 118/300, resid Loss: 0.0788 | 0.0399
Epoch 119/300, resid Loss: 0.0788 | 0.0399
Epoch 120/300, resid Loss: 0.0788 | 0.0399
Epoch 121/300, resid Loss: 0.0788 | 0.0399
Epoch 122/300, resid Loss: 0.0788 | 0.0399
Epoch 123/300, resid Loss: 0.0788 | 0.0399
Epoch 124/300, resid Loss: 0.0788 | 0.0399
Epoch 125/300, resid Loss: 0.0788 | 0.0399
Epoch 126/300, resid Loss: 0.0788 | 0.0399
Epoch 127/300, resid Loss: 0.0788 | 0.0399
Epoch 128/300, resid Loss: 0.0788 | 0.0399
Epoch 129/300, resid Loss: 0.0788 | 0.0399
Epoch 130/300, resid Loss: 0.0788 | 0.0399
Epoch 131/300, resid Loss: 0.0788 | 0.0399
Epoch 132/300, resid Loss: 0.0788 | 0.0399
Epoch 133/300, resid Loss: 0.0788 | 0.0399
Epoch 134/300, resid Loss: 0.0788 | 0.0399
Epoch 135/300, resid Loss: 0.0788 | 0.0399
Epoch 136/300, resid Loss: 0.0788 | 0.0399
Epoch 137/300, resid Loss: 0.0788 | 0.0399
Epoch 138/300, resid Loss: 0.0788 | 0.0399
Epoch 139/300, resid Loss: 0.0788 | 0.0399
Epoch 140/300, resid Loss: 0.0788 | 0.0399
Epoch 141/300, resid Loss: 0.0788 | 0.0399
Epoch 142/300, resid Loss: 0.0788 | 0.0399
Epoch 143/300, resid Loss: 0.0788 | 0.0399
Epoch 144/300, resid Loss: 0.0788 | 0.0399
Epoch 145/300, resid Loss: 0.0788 | 0.0399
Epoch 146/300, resid Loss: 0.0788 | 0.0399
Epoch 147/300, resid Loss: 0.0788 | 0.0399
Epoch 148/300, resid Loss: 0.0788 | 0.0399
Epoch 149/300, resid Loss: 0.0788 | 0.0399
Epoch 150/300, resid Loss: 0.0788 | 0.0399
Epoch 151/300, resid Loss: 0.0788 | 0.0399
Epoch 152/300, resid Loss: 0.0788 | 0.0399
Epoch 153/300, resid Loss: 0.0788 | 0.0399
Epoch 154/300, resid Loss: 0.0788 | 0.0399
Epoch 155/300, resid Loss: 0.0788 | 0.0399
Epoch 156/300, resid Loss: 0.0788 | 0.0399
Epoch 157/300, resid Loss: 0.0788 | 0.0399
Epoch 158/300, resid Loss: 0.0788 | 0.0399
Epoch 159/300, resid Loss: 0.0788 | 0.0399
Epoch 160/300, resid Loss: 0.0788 | 0.0399
Epoch 161/300, resid Loss: 0.0788 | 0.0399
Epoch 162/300, resid Loss: 0.0788 | 0.0399
Epoch 163/300, resid Loss: 0.0788 | 0.0399
Epoch 164/300, resid Loss: 0.0788 | 0.0399
Epoch 165/300, resid Loss: 0.0788 | 0.0399
Epoch 166/300, resid Loss: 0.0788 | 0.0399
Epoch 167/300, resid Loss: 0.0788 | 0.0399
Epoch 168/300, resid Loss: 0.0788 | 0.0399
Epoch 169/300, resid Loss: 0.0788 | 0.0399
Epoch 170/300, resid Loss: 0.0788 | 0.0399
Epoch 171/300, resid Loss: 0.0788 | 0.0399
Early stopping for resid
Runtime (seconds): 2138.307481765747
2.037700551585825e-05
[193.41002]
[-0.15281023]
[-2.2390122]
[1.4588252]
[0.5467707]
[12.9285965]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 78.0112449079752
RMSE: 8.8323974609375
MAE: 8.8323974609375
R-squared: nan
[205.9524]
