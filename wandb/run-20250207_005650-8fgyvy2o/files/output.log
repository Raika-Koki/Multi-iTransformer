[32m[I 2025-02-07 00:56:55,688][0m A new study created in memory with name: no-name-d60887f6-8037-47f4-a946-2197cf298b16[0m
[32m[I 2025-02-07 00:57:26,221][0m Trial 0 finished with value: 0.4036973340198734 and parameters: {'observation_period_num': 128, 'train_rates': 0.6293258211923016, 'learning_rate': 0.0001162320848135679, 'batch_size': 153, 'step_size': 2, 'gamma': 0.9406607022707566}. Best is trial 0 with value: 0.4036973340198734.[0m
[32m[I 2025-02-07 00:58:07,618][0m Trial 1 finished with value: 0.8942195773124695 and parameters: {'observation_period_num': 148, 'train_rates': 0.9873436544524046, 'learning_rate': 2.5922114794201438e-06, 'batch_size': 150, 'step_size': 7, 'gamma': 0.910959275730689}. Best is trial 0 with value: 0.4036973340198734.[0m
[32m[I 2025-02-07 01:00:26,592][0m Trial 2 finished with value: 0.17717441593512837 and parameters: {'observation_period_num': 229, 'train_rates': 0.8103738171015913, 'learning_rate': 0.00010606486727610996, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8215248218161562}. Best is trial 2 with value: 0.17717441593512837.[0m
[32m[I 2025-02-07 01:01:08,308][0m Trial 3 finished with value: 0.8828098880257577 and parameters: {'observation_period_num': 98, 'train_rates': 0.7836544634595219, 'learning_rate': 1.2237931734968616e-06, 'batch_size': 131, 'step_size': 13, 'gamma': 0.7734564467589039}. Best is trial 2 with value: 0.17717441593512837.[0m
[32m[I 2025-02-07 01:02:06,196][0m Trial 4 finished with value: 0.7773416671624853 and parameters: {'observation_period_num': 158, 'train_rates': 0.8299285328796676, 'learning_rate': 1.7209510444690206e-06, 'batch_size': 91, 'step_size': 1, 'gamma': 0.9554548498461783}. Best is trial 2 with value: 0.17717441593512837.[0m
[32m[I 2025-02-07 01:02:27,099][0m Trial 5 finished with value: 2.221957631455693 and parameters: {'observation_period_num': 12, 'train_rates': 0.6247162249766884, 'learning_rate': 1.7705502480355902e-06, 'batch_size': 249, 'step_size': 10, 'gamma': 0.7970398016314171}. Best is trial 2 with value: 0.17717441593512837.[0m
[32m[I 2025-02-07 01:02:53,492][0m Trial 6 finished with value: 0.5489982262273817 and parameters: {'observation_period_num': 213, 'train_rates': 0.7333323190606946, 'learning_rate': 1.0522449996353531e-05, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8026907338956981}. Best is trial 2 with value: 0.17717441593512837.[0m
[32m[I 2025-02-07 01:06:22,088][0m Trial 7 finished with value: 0.07972224190061383 and parameters: {'observation_period_num': 65, 'train_rates': 0.8153515548018224, 'learning_rate': 1.5289004678994943e-05, 'batch_size': 25, 'step_size': 8, 'gamma': 0.8073105110415518}. Best is trial 7 with value: 0.07972224190061383.[0m
[32m[I 2025-02-07 01:06:59,383][0m Trial 8 finished with value: 0.6402506822986263 and parameters: {'observation_period_num': 47, 'train_rates': 0.7533307387925801, 'learning_rate': 4.37280454554001e-06, 'batch_size': 139, 'step_size': 7, 'gamma': 0.8961329635813506}. Best is trial 7 with value: 0.07972224190061383.[0m
[32m[I 2025-02-07 01:07:28,740][0m Trial 9 finished with value: 0.1206986700375396 and parameters: {'observation_period_num': 93, 'train_rates': 0.8859012754577869, 'learning_rate': 0.00015747436091102858, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8075062209366766}. Best is trial 7 with value: 0.07972224190061383.[0m
[32m[I 2025-02-07 01:10:43,518][0m Trial 10 finished with value: 0.06703154176850862 and parameters: {'observation_period_num': 48, 'train_rates': 0.9466540832766671, 'learning_rate': 0.0007386998017231882, 'batch_size': 30, 'step_size': 4, 'gamma': 0.8569415954682227}. Best is trial 10 with value: 0.06703154176850862.[0m
[32m[I 2025-02-07 01:16:29,507][0m Trial 11 finished with value: 0.07313832308611144 and parameters: {'observation_period_num': 48, 'train_rates': 0.9688868726300722, 'learning_rate': 0.0006732181639716589, 'batch_size': 17, 'step_size': 4, 'gamma': 0.8565038105708527}. Best is trial 10 with value: 0.06703154176850862.[0m
[32m[I 2025-02-07 01:17:54,079][0m Trial 12 finished with value: 0.029888253060647244 and parameters: {'observation_period_num': 8, 'train_rates': 0.935574431605743, 'learning_rate': 0.0009003700483904098, 'batch_size': 70, 'step_size': 4, 'gamma': 0.8530354433406545}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:19:17,074][0m Trial 13 finished with value: 0.031089876729035466 and parameters: {'observation_period_num': 6, 'train_rates': 0.9119442860831778, 'learning_rate': 0.0009672644621624352, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8581100129340433}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:20:31,256][0m Trial 14 finished with value: 0.03565358432630698 and parameters: {'observation_period_num': 10, 'train_rates': 0.8996211883335458, 'learning_rate': 0.00030652362500920856, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9852549883402686}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:21:52,452][0m Trial 15 finished with value: 0.0605571742259687 and parameters: {'observation_period_num': 8, 'train_rates': 0.8965396658627631, 'learning_rate': 4.767934562117753e-05, 'batch_size': 71, 'step_size': 3, 'gamma': 0.8826904920989608}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:22:46,700][0m Trial 16 finished with value: 0.1895089045362181 and parameters: {'observation_period_num': 195, 'train_rates': 0.918634130580267, 'learning_rate': 0.0009625636701921788, 'batch_size': 104, 'step_size': 5, 'gamma': 0.8381254469742347}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:24:15,736][0m Trial 17 finished with value: 0.10523126899597034 and parameters: {'observation_period_num': 79, 'train_rates': 0.8636982081972363, 'learning_rate': 0.00035256790482752186, 'batch_size': 62, 'step_size': 1, 'gamma': 0.9182291935955874}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:25:10,783][0m Trial 18 finished with value: 0.11296322828892505 and parameters: {'observation_period_num': 31, 'train_rates': 0.933289926008422, 'learning_rate': 4.763073226707626e-05, 'batch_size': 112, 'step_size': 6, 'gamma': 0.7544225051598329}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:26:31,024][0m Trial 19 finished with value: 0.2411346207142425 and parameters: {'observation_period_num': 121, 'train_rates': 0.6980666349351313, 'learning_rate': 0.0003757342759116319, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8658995698186562}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:28:10,639][0m Trial 20 finished with value: 0.13498897262117973 and parameters: {'observation_period_num': 177, 'train_rates': 0.8551384417631764, 'learning_rate': 0.00015233551417565434, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8442595809967447}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:29:24,164][0m Trial 21 finished with value: 0.12009970173739747 and parameters: {'observation_period_num': 7, 'train_rates': 0.9005325434245008, 'learning_rate': 0.0003439716967180463, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9786250699080566}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:30:20,402][0m Trial 22 finished with value: 0.04895530426158355 and parameters: {'observation_period_num': 33, 'train_rates': 0.9520458300143544, 'learning_rate': 0.00043130718171567015, 'batch_size': 112, 'step_size': 5, 'gamma': 0.8839274436392653}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:31:22,239][0m Trial 23 finished with value: 0.059639837211225094 and parameters: {'observation_period_num': 29, 'train_rates': 0.8619209885415559, 'learning_rate': 0.0009884985272144615, 'batch_size': 92, 'step_size': 3, 'gamma': 0.9245705222072739}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:33:44,957][0m Trial 24 finished with value: 0.08920611441135406 and parameters: {'observation_period_num': 68, 'train_rates': 0.9877518997489889, 'learning_rate': 0.00022514275500178167, 'batch_size': 42, 'step_size': 5, 'gamma': 0.8299464510485913}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:35:01,238][0m Trial 25 finished with value: 0.040344134616444864 and parameters: {'observation_period_num': 15, 'train_rates': 0.916788680750082, 'learning_rate': 0.0005176512485688667, 'batch_size': 79, 'step_size': 2, 'gamma': 0.9893309104691024}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:36:59,622][0m Trial 26 finished with value: 0.06278551681867421 and parameters: {'observation_period_num': 29, 'train_rates': 0.8490269677820156, 'learning_rate': 0.00018143625932365984, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8819464656845531}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:37:48,207][0m Trial 27 finished with value: 0.08663415063672991 and parameters: {'observation_period_num': 65, 'train_rates': 0.8858072461041436, 'learning_rate': 7.346448185640212e-05, 'batch_size': 123, 'step_size': 6, 'gamma': 0.9599668640289173}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:38:25,774][0m Trial 28 finished with value: 0.19882921874523163 and parameters: {'observation_period_num': 95, 'train_rates': 0.9497834604276483, 'learning_rate': 0.00029038816174184364, 'batch_size': 170, 'step_size': 2, 'gamma': 0.8998756086990176}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:38:54,106][0m Trial 29 finished with value: 0.2935506163980775 and parameters: {'observation_period_num': 249, 'train_rates': 0.6686620421293171, 'learning_rate': 0.0005922131772452721, 'batch_size': 172, 'step_size': 6, 'gamma': 0.9337593614818008}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:39:48,785][0m Trial 30 finished with value: 0.14420097132367413 and parameters: {'observation_period_num': 117, 'train_rates': 0.779712535112668, 'learning_rate': 9.606608158805294e-05, 'batch_size': 96, 'step_size': 1, 'gamma': 0.9501105877906706}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:41:05,340][0m Trial 31 finished with value: 0.048768975896628826 and parameters: {'observation_period_num': 17, 'train_rates': 0.9150188164622165, 'learning_rate': 0.000527707262509624, 'batch_size': 76, 'step_size': 2, 'gamma': 0.9751467845833578}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:42:25,564][0m Trial 32 finished with value: 0.05654807221414386 and parameters: {'observation_period_num': 6, 'train_rates': 0.9310362437190025, 'learning_rate': 0.0005749148841457756, 'batch_size': 75, 'step_size': 3, 'gamma': 0.9784206656919863}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:43:36,826][0m Trial 33 finished with value: 0.12852266430854797 and parameters: {'observation_period_num': 41, 'train_rates': 0.971386889145349, 'learning_rate': 0.00024032215518520578, 'batch_size': 85, 'step_size': 2, 'gamma': 0.983713370768831}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:45:05,840][0m Trial 34 finished with value: 0.07565323865421898 and parameters: {'observation_period_num': 22, 'train_rates': 0.8784692955886336, 'learning_rate': 0.0009750740277502006, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9892458691694651}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:47:11,323][0m Trial 35 finished with value: 0.11449538662287467 and parameters: {'observation_period_num': 60, 'train_rates': 0.911241483073976, 'learning_rate': 0.0004701858549192771, 'batch_size': 46, 'step_size': 5, 'gamma': 0.965143362280965}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:47:58,473][0m Trial 36 finished with value: 0.07788569771388465 and parameters: {'observation_period_num': 20, 'train_rates': 0.838521924302036, 'learning_rate': 2.016122636964122e-05, 'batch_size': 119, 'step_size': 8, 'gamma': 0.9434716752917307}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:48:43,951][0m Trial 37 finished with value: 0.06961691379547119 and parameters: {'observation_period_num': 41, 'train_rates': 0.966142339223543, 'learning_rate': 0.0007464652704937374, 'batch_size': 136, 'step_size': 2, 'gamma': 0.8483411595186323}. Best is trial 12 with value: 0.029888253060647244.[0m
Early stopping at epoch 68
[32m[I 2025-02-07 01:49:21,269][0m Trial 38 finished with value: 0.38445290841147567 and parameters: {'observation_period_num': 140, 'train_rates': 0.8220882579435095, 'learning_rate': 0.00011345640730364145, 'batch_size': 99, 'step_size': 1, 'gamma': 0.8241196926057753}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:51:32,646][0m Trial 39 finished with value: 0.20715585748889837 and parameters: {'observation_period_num': 78, 'train_rates': 0.603079693977655, 'learning_rate': 0.00024823935436918697, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8710934867448665}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:52:35,554][0m Trial 40 finished with value: 0.041084056308587524 and parameters: {'observation_period_num': 5, 'train_rates': 0.7959175191412273, 'learning_rate': 0.0004516514634015838, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9058857792858588}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:53:38,730][0m Trial 41 finished with value: 0.0415119721705245 and parameters: {'observation_period_num': 18, 'train_rates': 0.801513718500098, 'learning_rate': 0.0005006008657488425, 'batch_size': 86, 'step_size': 3, 'gamma': 0.9666688880841411}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:55:11,084][0m Trial 42 finished with value: 0.25256806163425416 and parameters: {'observation_period_num': 5, 'train_rates': 0.7760372831957154, 'learning_rate': 5.409484412992493e-06, 'batch_size': 57, 'step_size': 4, 'gamma': 0.9038621017006135}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:55:47,457][0m Trial 43 finished with value: 0.1546396708451443 and parameters: {'observation_period_num': 25, 'train_rates': 0.7293741259218505, 'learning_rate': 0.0007473982546742021, 'batch_size': 150, 'step_size': 3, 'gamma': 0.9336459237060286}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:57:10,048][0m Trial 44 finished with value: 0.05937008275786071 and parameters: {'observation_period_num': 51, 'train_rates': 0.9301250051004218, 'learning_rate': 0.00030639712845168584, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8168140768882872}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 01:58:04,616][0m Trial 45 finished with value: 0.0926715171757317 and parameters: {'observation_period_num': 38, 'train_rates': 0.870932318219284, 'learning_rate': 0.0007325896323866725, 'batch_size': 106, 'step_size': 11, 'gamma': 0.7827759123128714}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 02:03:56,637][0m Trial 46 finished with value: 0.03439207796920214 and parameters: {'observation_period_num': 16, 'train_rates': 0.8989776310401724, 'learning_rate': 0.00014546298052994414, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8649700178891169}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 02:08:26,138][0m Trial 47 finished with value: 0.04458852973529848 and parameters: {'observation_period_num': 17, 'train_rates': 0.901138162094753, 'learning_rate': 0.00016061980850327862, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8633357606601544}. Best is trial 12 with value: 0.029888253060647244.[0m
[32m[I 2025-02-07 02:10:58,761][0m Trial 48 finished with value: 0.06675895013742977 and parameters: {'observation_period_num': 53, 'train_rates': 0.9422489002252679, 'learning_rate': 8.438998465697515e-05, 'batch_size': 38, 'step_size': 4, 'gamma': 0.8743355818603836}. Best is trial 12 with value: 0.029888253060647244.[0m
Early stopping at epoch 83
[32m[I 2025-02-07 02:13:53,222][0m Trial 49 finished with value: 0.300538070499897 and parameters: {'observation_period_num': 108, 'train_rates': 0.9613947202627059, 'learning_rate': 6.660629638483011e-05, 'batch_size': 28, 'step_size': 1, 'gamma': 0.8511394273987466}. Best is trial 12 with value: 0.029888253060647244.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3303 | 0.1646
Epoch 2/300, Loss: 0.1453 | 0.1985
Epoch 3/300, Loss: 0.1209 | 0.1325
Epoch 4/300, Loss: 0.1119 | 0.1449
Epoch 5/300, Loss: 0.1093 | 0.0911
Epoch 6/300, Loss: 0.1274 | 0.1281
Epoch 7/300, Loss: 0.1021 | 0.0734
Epoch 8/300, Loss: 0.0956 | 0.0720
Epoch 9/300, Loss: 0.0890 | 0.0672
Epoch 10/300, Loss: 0.0857 | 0.0591
Epoch 11/300, Loss: 0.0931 | 0.0524
Epoch 12/300, Loss: 0.0935 | 0.0517
Epoch 13/300, Loss: 0.0866 | 0.0484
Epoch 14/300, Loss: 0.0872 | 0.0472
Epoch 15/300, Loss: 0.0798 | 0.0488
Epoch 16/300, Loss: 0.0796 | 0.0477
Epoch 17/300, Loss: 0.0795 | 0.0527
Epoch 18/300, Loss: 0.0803 | 0.0500
Epoch 19/300, Loss: 0.0802 | 0.0502
Epoch 20/300, Loss: 0.0794 | 0.0505
Epoch 21/300, Loss: 0.0766 | 0.0484
Epoch 22/300, Loss: 0.0745 | 0.0459
Epoch 23/300, Loss: 0.0734 | 0.0461
Epoch 24/300, Loss: 0.0727 | 0.0459
Epoch 25/300, Loss: 0.0722 | 0.0458
Epoch 26/300, Loss: 0.0718 | 0.0458
Epoch 27/300, Loss: 0.0716 | 0.0455
Epoch 28/300, Loss: 0.0716 | 0.0457
Epoch 29/300, Loss: 0.0717 | 0.0450
Epoch 30/300, Loss: 0.0720 | 0.0446
Epoch 31/300, Loss: 0.0724 | 0.0421
Epoch 32/300, Loss: 0.0730 | 0.0410
Epoch 33/300, Loss: 0.0736 | 0.0405
Epoch 34/300, Loss: 0.0736 | 0.0410
Epoch 35/300, Loss: 0.0729 | 0.0420
Epoch 36/300, Loss: 0.0715 | 0.0430
Epoch 37/300, Loss: 0.0707 | 0.0433
Epoch 38/300, Loss: 0.0712 | 0.0443
Epoch 39/300, Loss: 0.0724 | 0.0467
Epoch 40/300, Loss: 0.0735 | 0.0482
Epoch 41/300, Loss: 0.0732 | 0.0498
Epoch 42/300, Loss: 0.0720 | 0.0470
Epoch 43/300, Loss: 0.0706 | 0.0446
Epoch 44/300, Loss: 0.0692 | 0.0420
Epoch 45/300, Loss: 0.0685 | 0.0412
Epoch 46/300, Loss: 0.0681 | 0.0406
Epoch 47/300, Loss: 0.0679 | 0.0407
Epoch 48/300, Loss: 0.0677 | 0.0404
Epoch 49/300, Loss: 0.0675 | 0.0404
Epoch 50/300, Loss: 0.0672 | 0.0402
Epoch 51/300, Loss: 0.0669 | 0.0402
Epoch 52/300, Loss: 0.0666 | 0.0400
Epoch 53/300, Loss: 0.0664 | 0.0398
Epoch 54/300, Loss: 0.0661 | 0.0397
Epoch 55/300, Loss: 0.0659 | 0.0395
Epoch 56/300, Loss: 0.0658 | 0.0393
Epoch 57/300, Loss: 0.0656 | 0.0391
Epoch 58/300, Loss: 0.0655 | 0.0391
Epoch 59/300, Loss: 0.0654 | 0.0389
Epoch 60/300, Loss: 0.0652 | 0.0388
Epoch 61/300, Loss: 0.0652 | 0.0387
Epoch 62/300, Loss: 0.0651 | 0.0386
Epoch 63/300, Loss: 0.0650 | 0.0385
Epoch 64/300, Loss: 0.0650 | 0.0384
Epoch 65/300, Loss: 0.0649 | 0.0383
Epoch 66/300, Loss: 0.0649 | 0.0382
Epoch 67/300, Loss: 0.0648 | 0.0381
Epoch 68/300, Loss: 0.0648 | 0.0381
Epoch 69/300, Loss: 0.0647 | 0.0380
Epoch 70/300, Loss: 0.0647 | 0.0380
Epoch 71/300, Loss: 0.0647 | 0.0379
Epoch 72/300, Loss: 0.0647 | 0.0379
Epoch 73/300, Loss: 0.0646 | 0.0378
Epoch 74/300, Loss: 0.0646 | 0.0378
Epoch 75/300, Loss: 0.0646 | 0.0378
Epoch 76/300, Loss: 0.0646 | 0.0377
Epoch 77/300, Loss: 0.0645 | 0.0377
Epoch 78/300, Loss: 0.0645 | 0.0377
Epoch 79/300, Loss: 0.0645 | 0.0376
Epoch 80/300, Loss: 0.0645 | 0.0376
Epoch 81/300, Loss: 0.0645 | 0.0376
Epoch 82/300, Loss: 0.0644 | 0.0376
Epoch 83/300, Loss: 0.0644 | 0.0375
Epoch 84/300, Loss: 0.0644 | 0.0375
Epoch 85/300, Loss: 0.0644 | 0.0375
Epoch 86/300, Loss: 0.0644 | 0.0375
Epoch 87/300, Loss: 0.0644 | 0.0375
Epoch 88/300, Loss: 0.0644 | 0.0374
Epoch 89/300, Loss: 0.0644 | 0.0374
Epoch 90/300, Loss: 0.0644 | 0.0374
Epoch 91/300, Loss: 0.0643 | 0.0374
Epoch 92/300, Loss: 0.0643 | 0.0374
Epoch 93/300, Loss: 0.0643 | 0.0374
Epoch 94/300, Loss: 0.0643 | 0.0374
Epoch 95/300, Loss: 0.0643 | 0.0374
Epoch 96/300, Loss: 0.0643 | 0.0374
Epoch 97/300, Loss: 0.0643 | 0.0374
Epoch 98/300, Loss: 0.0643 | 0.0373
Epoch 99/300, Loss: 0.0643 | 0.0373
Epoch 100/300, Loss: 0.0643 | 0.0373
Epoch 101/300, Loss: 0.0643 | 0.0373
Epoch 102/300, Loss: 0.0643 | 0.0373
Epoch 103/300, Loss: 0.0643 | 0.0373
Epoch 104/300, Loss: 0.0643 | 0.0373
Epoch 105/300, Loss: 0.0643 | 0.0373
Epoch 106/300, Loss: 0.0643 | 0.0373
Epoch 107/300, Loss: 0.0643 | 0.0373
Epoch 108/300, Loss: 0.0643 | 0.0373
Epoch 109/300, Loss: 0.0643 | 0.0373
Epoch 110/300, Loss: 0.0643 | 0.0373
Epoch 111/300, Loss: 0.0643 | 0.0373
Epoch 112/300, Loss: 0.0643 | 0.0373
Epoch 113/300, Loss: 0.0643 | 0.0373
Epoch 114/300, Loss: 0.0643 | 0.0373
Epoch 115/300, Loss: 0.0643 | 0.0373
Epoch 116/300, Loss: 0.0643 | 0.0373
Epoch 117/300, Loss: 0.0643 | 0.0373
Epoch 118/300, Loss: 0.0643 | 0.0373
Epoch 119/300, Loss: 0.0643 | 0.0373
Epoch 120/300, Loss: 0.0643 | 0.0373
Epoch 121/300, Loss: 0.0643 | 0.0373
Epoch 122/300, Loss: 0.0643 | 0.0373
Epoch 123/300, Loss: 0.0643 | 0.0373
Epoch 124/300, Loss: 0.0643 | 0.0373
Epoch 125/300, Loss: 0.0643 | 0.0373
Epoch 126/300, Loss: 0.0643 | 0.0373
Epoch 127/300, Loss: 0.0643 | 0.0373
Epoch 128/300, Loss: 0.0643 | 0.0373
Epoch 129/300, Loss: 0.0643 | 0.0373
Epoch 130/300, Loss: 0.0643 | 0.0373
Epoch 131/300, Loss: 0.0643 | 0.0373
Epoch 132/300, Loss: 0.0643 | 0.0372
Epoch 133/300, Loss: 0.0643 | 0.0372
Epoch 134/300, Loss: 0.0643 | 0.0372
Epoch 135/300, Loss: 0.0643 | 0.0372
Epoch 136/300, Loss: 0.0643 | 0.0372
Epoch 137/300, Loss: 0.0643 | 0.0372
Epoch 138/300, Loss: 0.0643 | 0.0372
Epoch 139/300, Loss: 0.0643 | 0.0372
Epoch 140/300, Loss: 0.0643 | 0.0372
Epoch 141/300, Loss: 0.0642 | 0.0372
Epoch 142/300, Loss: 0.0642 | 0.0372
Epoch 143/300, Loss: 0.0642 | 0.0372
Epoch 144/300, Loss: 0.0642 | 0.0372
Epoch 145/300, Loss: 0.0642 | 0.0372
Epoch 146/300, Loss: 0.0642 | 0.0372
Epoch 147/300, Loss: 0.0642 | 0.0372
Epoch 148/300, Loss: 0.0642 | 0.0372
Epoch 149/300, Loss: 0.0642 | 0.0372
Epoch 150/300, Loss: 0.0642 | 0.0372
Epoch 151/300, Loss: 0.0642 | 0.0372
Epoch 152/300, Loss: 0.0642 | 0.0372
Epoch 153/300, Loss: 0.0642 | 0.0372
Epoch 154/300, Loss: 0.0642 | 0.0372
Epoch 155/300, Loss: 0.0642 | 0.0372
Epoch 156/300, Loss: 0.0642 | 0.0372
Epoch 157/300, Loss: 0.0642 | 0.0372
Epoch 158/300, Loss: 0.0642 | 0.0372
Epoch 159/300, Loss: 0.0642 | 0.0372
Epoch 160/300, Loss: 0.0642 | 0.0372
Epoch 161/300, Loss: 0.0642 | 0.0372
Epoch 162/300, Loss: 0.0642 | 0.0372
Epoch 163/300, Loss: 0.0642 | 0.0372
Epoch 164/300, Loss: 0.0642 | 0.0372
Epoch 165/300, Loss: 0.0642 | 0.0372
Epoch 166/300, Loss: 0.0642 | 0.0372
Epoch 167/300, Loss: 0.0642 | 0.0372
Epoch 168/300, Loss: 0.0642 | 0.0372
Epoch 169/300, Loss: 0.0642 | 0.0372
Epoch 170/300, Loss: 0.0642 | 0.0372
Epoch 171/300, Loss: 0.0642 | 0.0372
Epoch 172/300, Loss: 0.0642 | 0.0372
Epoch 173/300, Loss: 0.0642 | 0.0372
Epoch 174/300, Loss: 0.0642 | 0.0372
Epoch 175/300, Loss: 0.0642 | 0.0372
Epoch 176/300, Loss: 0.0642 | 0.0372
Epoch 177/300, Loss: 0.0642 | 0.0372
Epoch 178/300, Loss: 0.0642 | 0.0372
Epoch 179/300, Loss: 0.0642 | 0.0372
Epoch 180/300, Loss: 0.0642 | 0.0372
Epoch 181/300, Loss: 0.0642 | 0.0372
Epoch 182/300, Loss: 0.0642 | 0.0372
Epoch 183/300, Loss: 0.0642 | 0.0372
Epoch 184/300, Loss: 0.0642 | 0.0372
Epoch 185/300, Loss: 0.0642 | 0.0372
Epoch 186/300, Loss: 0.0642 | 0.0372
Early stopping
Runtime (seconds): 158.8158438205719
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.37187738926149905
RMSE: 0.6098175048828125
MAE: 0.6098175048828125
R-squared: nan
[193.90982]
