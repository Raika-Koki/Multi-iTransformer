ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-08 06:32:51,743][0m A new study created in memory with name: no-name-970719f9-d792-4fd3-b4f4-0f1c59140e28[0m
[32m[I 2025-02-08 06:34:29,415][0m Trial 0 finished with value: 0.13860949242039572 and parameters: {'observation_period_num': 176, 'train_rates': 0.9433776591198223, 'learning_rate': 0.0008479534456711403, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8442978847514673}. Best is trial 0 with value: 0.13860949242039572.[0m
[32m[I 2025-02-08 06:35:47,657][0m Trial 1 finished with value: 0.11174151357219705 and parameters: {'observation_period_num': 171, 'train_rates': 0.8471826141766587, 'learning_rate': 0.0001664064942542694, 'batch_size': 66, 'step_size': 8, 'gamma': 0.838976679238377}. Best is trial 1 with value: 0.11174151357219705.[0m
[32m[I 2025-02-08 06:37:13,387][0m Trial 2 finished with value: 0.21402854241173844 and parameters: {'observation_period_num': 220, 'train_rates': 0.852202674127144, 'learning_rate': 1.7478269548780934e-05, 'batch_size': 60, 'step_size': 10, 'gamma': 0.8716349952096569}. Best is trial 1 with value: 0.11174151357219705.[0m
[32m[I 2025-02-08 06:38:53,216][0m Trial 3 finished with value: 0.05305263423688815 and parameters: {'observation_period_num': 33, 'train_rates': 0.9032568712142826, 'learning_rate': 0.00012267724565271986, 'batch_size': 57, 'step_size': 4, 'gamma': 0.8358408090853681}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:39:40,071][0m Trial 4 finished with value: 0.3313852681226028 and parameters: {'observation_period_num': 24, 'train_rates': 0.6067837010678836, 'learning_rate': 4.28920975138708e-06, 'batch_size': 94, 'step_size': 14, 'gamma': 0.9357569190293952}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:40:04,113][0m Trial 5 finished with value: 0.19153068224861197 and parameters: {'observation_period_num': 237, 'train_rates': 0.8124763148761304, 'learning_rate': 0.0005106847344258582, 'batch_size': 235, 'step_size': 9, 'gamma': 0.7690671233044681}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:40:35,623][0m Trial 6 finished with value: 0.39507875486314875 and parameters: {'observation_period_num': 58, 'train_rates': 0.8220542816971226, 'learning_rate': 3.937739167202233e-06, 'batch_size': 178, 'step_size': 11, 'gamma': 0.9170365270094677}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:41:20,119][0m Trial 7 finished with value: 1.2558982931357576 and parameters: {'observation_period_num': 112, 'train_rates': 0.6266436075108773, 'learning_rate': 1.8723502139791049e-06, 'batch_size': 102, 'step_size': 15, 'gamma': 0.7930515140366593}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:42:22,002][0m Trial 8 finished with value: 0.22900418372668885 and parameters: {'observation_period_num': 176, 'train_rates': 0.837143330749754, 'learning_rate': 1.4230892967546177e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8784363396370031}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:43:39,824][0m Trial 9 finished with value: 0.4690128637586426 and parameters: {'observation_period_num': 216, 'train_rates': 0.644252700359319, 'learning_rate': 5.584609740262077e-06, 'batch_size': 55, 'step_size': 4, 'gamma': 0.9349060302424339}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:48:15,113][0m Trial 10 finished with value: 0.11217217594385147 and parameters: {'observation_period_num': 82, 'train_rates': 0.9637562849093488, 'learning_rate': 8.859657476879689e-05, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9816579011018379}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:48:50,611][0m Trial 11 finished with value: 0.17423100703292424 and parameters: {'observation_period_num': 10, 'train_rates': 0.7258270671335765, 'learning_rate': 9.442730832725207e-05, 'batch_size': 143, 'step_size': 6, 'gamma': 0.8165161555439366}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 06:52:08,227][0m Trial 12 finished with value: 0.10820144638419152 and parameters: {'observation_period_num': 154, 'train_rates': 0.9040992968338065, 'learning_rate': 0.000176268189564055, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8281963389041979}. Best is trial 3 with value: 0.05305263423688815.[0m
Early stopping at epoch 73
[32m[I 2025-02-08 06:56:22,414][0m Trial 13 finished with value: 0.10197497364485039 and parameters: {'observation_period_num': 129, 'train_rates': 0.906603633041147, 'learning_rate': 0.0003449781572844658, 'batch_size': 16, 'step_size': 1, 'gamma': 0.7522576144630604}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:01:26,154][0m Trial 14 finished with value: 0.1016371744962139 and parameters: {'observation_period_num': 123, 'train_rates': 0.9078497977697866, 'learning_rate': 0.00032007536390382327, 'batch_size': 18, 'step_size': 3, 'gamma': 0.7701901044104172}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:02:06,796][0m Trial 15 finished with value: 0.35592790835715354 and parameters: {'observation_period_num': 61, 'train_rates': 0.740882249213063, 'learning_rate': 5.075150222272369e-05, 'batch_size': 131, 'step_size': 3, 'gamma': 0.7931218303613303}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:02:32,022][0m Trial 16 finished with value: 0.1244455502430598 and parameters: {'observation_period_num': 100, 'train_rates': 0.8952040234343683, 'learning_rate': 0.0003031944273683637, 'batch_size': 256, 'step_size': 7, 'gamma': 0.7887400160828434}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:03:21,687][0m Trial 17 finished with value: 0.10937776416540146 and parameters: {'observation_period_num': 43, 'train_rates': 0.9880214928203992, 'learning_rate': 3.137752327174621e-05, 'batch_size': 126, 'step_size': 5, 'gamma': 0.8924338746592307}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:03:55,273][0m Trial 18 finished with value: 0.1317380099552464 and parameters: {'observation_period_num': 138, 'train_rates': 0.9325797090121465, 'learning_rate': 0.0009409626311180719, 'batch_size': 183, 'step_size': 3, 'gamma': 0.8120333511908688}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:06:17,671][0m Trial 19 finished with value: 0.19938940535101926 and parameters: {'observation_period_num': 81, 'train_rates': 0.7710559344747542, 'learning_rate': 0.00015018689754602252, 'batch_size': 35, 'step_size': 7, 'gamma': 0.7506897370691775}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:07:25,739][0m Trial 20 finished with value: 0.07264522389798868 and parameters: {'observation_period_num': 34, 'train_rates': 0.8749999409642699, 'learning_rate': 5.297191694725284e-05, 'batch_size': 84, 'step_size': 4, 'gamma': 0.8524929837494244}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:09:28,018][0m Trial 21 finished with value: 0.058159802721517107 and parameters: {'observation_period_num': 5, 'train_rates': 0.8746841433706423, 'learning_rate': 4.287481770500087e-05, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8489800145902435}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:10:39,889][0m Trial 22 finished with value: 0.06147214382351762 and parameters: {'observation_period_num': 6, 'train_rates': 0.8777138619040611, 'learning_rate': 4.55963508938159e-05, 'batch_size': 79, 'step_size': 5, 'gamma': 0.861186265063591}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:11:28,122][0m Trial 23 finished with value: 0.20163787961240828 and parameters: {'observation_period_num': 5, 'train_rates': 0.785436143767975, 'learning_rate': 2.056864757971049e-05, 'batch_size': 111, 'step_size': 6, 'gamma': 0.8950940214162875}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:13:34,297][0m Trial 24 finished with value: 0.09511731730130228 and parameters: {'observation_period_num': 20, 'train_rates': 0.8712840655862882, 'learning_rate': 9.954657489326812e-06, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8593323747276865}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:14:46,624][0m Trial 25 finished with value: 0.13437662767358574 and parameters: {'observation_period_num': 50, 'train_rates': 0.9431150547184756, 'learning_rate': 3.8425821946409975e-05, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8255631551142593}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:16:06,406][0m Trial 26 finished with value: 0.07917404712797119 and parameters: {'observation_period_num': 71, 'train_rates': 0.8637962611137007, 'learning_rate': 0.00010366095168638923, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8930368493305831}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:18:04,960][0m Trial 27 finished with value: 0.17308319410745135 and parameters: {'observation_period_num': 33, 'train_rates': 0.7592740181680769, 'learning_rate': 6.631025727326719e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8640139902876536}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:18:42,661][0m Trial 28 finished with value: 0.07412468857466135 and parameters: {'observation_period_num': 22, 'train_rates': 0.8048577569963329, 'learning_rate': 3.6513225244407227e-05, 'batch_size': 156, 'step_size': 8, 'gamma': 0.9110389166034335}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:19:28,904][0m Trial 29 finished with value: 0.23290360220410722 and parameters: {'observation_period_num': 6, 'train_rates': 0.6819550582936721, 'learning_rate': 8.67549326773649e-06, 'batch_size': 111, 'step_size': 5, 'gamma': 0.8450943920985042}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:21:23,787][0m Trial 30 finished with value: 0.08731369288211846 and parameters: {'observation_period_num': 36, 'train_rates': 0.971932675809351, 'learning_rate': 2.5952282805333796e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8061135942171853}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:22:38,232][0m Trial 31 finished with value: 0.06840820591309692 and parameters: {'observation_period_num': 40, 'train_rates': 0.8807825413405519, 'learning_rate': 5.804061726773689e-05, 'batch_size': 76, 'step_size': 4, 'gamma': 0.851728087782204}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:24:09,186][0m Trial 32 finished with value: 0.08418220695522097 and parameters: {'observation_period_num': 44, 'train_rates': 0.9261774871624514, 'learning_rate': 0.0001775237981698323, 'batch_size': 64, 'step_size': 2, 'gamma': 0.8323124991453973}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:25:29,016][0m Trial 33 finished with value: 0.05835840960822092 and parameters: {'observation_period_num': 18, 'train_rates': 0.8802158655297405, 'learning_rate': 6.825032566211204e-05, 'batch_size': 73, 'step_size': 4, 'gamma': 0.8465888555126408}. Best is trial 3 with value: 0.05305263423688815.[0m
[32m[I 2025-02-08 07:26:58,469][0m Trial 34 finished with value: 0.052165280572061455 and parameters: {'observation_period_num': 19, 'train_rates': 0.8445377418392842, 'learning_rate': 0.0001125182123987034, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8764236408911983}. Best is trial 34 with value: 0.052165280572061455.[0m
[32m[I 2025-02-08 07:28:34,257][0m Trial 35 finished with value: 0.049939233188827835 and parameters: {'observation_period_num': 23, 'train_rates': 0.8368450130259951, 'learning_rate': 0.00012873410744001135, 'batch_size': 56, 'step_size': 3, 'gamma': 0.8782539533660724}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:31:00,952][0m Trial 36 finished with value: 0.11597000606931172 and parameters: {'observation_period_num': 61, 'train_rates': 0.8388581167695729, 'learning_rate': 0.0005656683444915567, 'batch_size': 36, 'step_size': 1, 'gamma': 0.8819571361929117}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:32:43,798][0m Trial 37 finished with value: 0.0884592771898992 and parameters: {'observation_period_num': 85, 'train_rates': 0.8246079717585316, 'learning_rate': 0.00020523478043055112, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9147237265122852}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:33:41,580][0m Trial 38 finished with value: 0.061399558572208184 and parameters: {'observation_period_num': 24, 'train_rates': 0.8553584026828969, 'learning_rate': 0.00012361953145714424, 'batch_size': 97, 'step_size': 2, 'gamma': 0.8822600634022546}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:35:00,367][0m Trial 39 finished with value: 0.16730944599424089 and parameters: {'observation_period_num': 196, 'train_rates': 0.7875321718365167, 'learning_rate': 0.00024074982919705616, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9312625840834013}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:35:30,668][0m Trial 40 finished with value: 0.06603120037579584 and parameters: {'observation_period_num': 57, 'train_rates': 0.8248860101217026, 'learning_rate': 0.0004914503337239592, 'batch_size': 191, 'step_size': 3, 'gamma': 0.8717472799425243}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:36:55,793][0m Trial 41 finished with value: 0.058517626366101506 and parameters: {'observation_period_num': 19, 'train_rates': 0.84005607034713, 'learning_rate': 7.789664482295686e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.8426963589122449}. Best is trial 35 with value: 0.049939233188827835.[0m
[32m[I 2025-02-08 07:39:54,965][0m Trial 42 finished with value: 0.039368275360300624 and parameters: {'observation_period_num': 27, 'train_rates': 0.9210574205498879, 'learning_rate': 0.00010281860071970859, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8407477740156711}. Best is trial 42 with value: 0.039368275360300624.[0m
[32m[I 2025-02-08 07:43:20,846][0m Trial 43 finished with value: 0.05443075315746586 and parameters: {'observation_period_num': 28, 'train_rates': 0.9617745866289991, 'learning_rate': 0.00012889583483002024, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9038871536202068}. Best is trial 42 with value: 0.039368275360300624.[0m
[32m[I 2025-02-08 07:46:12,266][0m Trial 44 finished with value: 0.5023153515781943 and parameters: {'observation_period_num': 250, 'train_rates': 0.9534881413419093, 'learning_rate': 1.0542389447355984e-06, 'batch_size': 31, 'step_size': 13, 'gamma': 0.9636586321919124}. Best is trial 42 with value: 0.039368275360300624.[0m
[32m[I 2025-02-08 07:49:25,700][0m Trial 45 finished with value: 0.1027579767363412 and parameters: {'observation_period_num': 73, 'train_rates': 0.9153342726315601, 'learning_rate': 0.00012571372932305215, 'batch_size': 28, 'step_size': 11, 'gamma': 0.9072159151496259}. Best is trial 42 with value: 0.039368275360300624.[0m
[32m[I 2025-02-08 07:51:06,488][0m Trial 46 finished with value: 0.038427061313272115 and parameters: {'observation_period_num': 26, 'train_rates': 0.9316245193494359, 'learning_rate': 0.000116284026956772, 'batch_size': 58, 'step_size': 15, 'gamma': 0.9243323443769295}. Best is trial 46 with value: 0.038427061313272115.[0m
[32m[I 2025-02-08 07:52:11,442][0m Trial 47 finished with value: 0.11153973764348253 and parameters: {'observation_period_num': 53, 'train_rates': 0.9266642639469177, 'learning_rate': 0.00026114215881705485, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9316317687693837}. Best is trial 46 with value: 0.038427061313272115.[0m
[32m[I 2025-02-08 07:53:50,616][0m Trial 48 finished with value: 0.15279201181675386 and parameters: {'observation_period_num': 97, 'train_rates': 0.8979375516085825, 'learning_rate': 0.00043283615140904196, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9512802893661457}. Best is trial 46 with value: 0.038427061313272115.[0m
[32m[I 2025-02-08 07:55:35,245][0m Trial 49 finished with value: 0.05373176696800416 and parameters: {'observation_period_num': 46, 'train_rates': 0.9470947492101784, 'learning_rate': 9.086172977627367e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.833174397556611}. Best is trial 46 with value: 0.038427061313272115.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 07:55:35,256][0m A new study created in memory with name: no-name-a588e605-b44c-4edd-ac4c-34b1985dd1e1[0m
[32m[I 2025-02-08 07:56:23,280][0m Trial 0 finished with value: 0.2615262902639749 and parameters: {'observation_period_num': 89, 'train_rates': 0.7748759178179031, 'learning_rate': 3.173584578711312e-05, 'batch_size': 108, 'step_size': 8, 'gamma': 0.8713046666474702}. Best is trial 0 with value: 0.2615262902639749.[0m
[32m[I 2025-02-08 07:56:51,266][0m Trial 1 finished with value: 0.156080961227417 and parameters: {'observation_period_num': 146, 'train_rates': 0.9356213340628616, 'learning_rate': 0.000835644204773466, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8601104812414286}. Best is trial 1 with value: 0.156080961227417.[0m
[32m[I 2025-02-08 07:57:15,482][0m Trial 2 finished with value: 0.3299901735709151 and parameters: {'observation_period_num': 88, 'train_rates': 0.7742905779499347, 'learning_rate': 5.390739357619164e-05, 'batch_size': 233, 'step_size': 7, 'gamma': 0.7683974228370144}. Best is trial 1 with value: 0.156080961227417.[0m
[32m[I 2025-02-08 07:58:04,852][0m Trial 3 finished with value: 0.6103446846589063 and parameters: {'observation_period_num': 228, 'train_rates': 0.8290607656559071, 'learning_rate': 1.8475941501148895e-06, 'batch_size': 106, 'step_size': 8, 'gamma': 0.9373637592301206}. Best is trial 1 with value: 0.156080961227417.[0m
[32m[I 2025-02-08 07:58:34,253][0m Trial 4 finished with value: 0.41756758005892647 and parameters: {'observation_period_num': 207, 'train_rates': 0.7786698951981295, 'learning_rate': 2.8082298009133622e-05, 'batch_size': 192, 'step_size': 13, 'gamma': 0.8119570361780724}. Best is trial 1 with value: 0.156080961227417.[0m
[32m[I 2025-02-08 07:59:22,144][0m Trial 5 finished with value: 0.924940839759986 and parameters: {'observation_period_num': 110, 'train_rates': 0.8658525554411136, 'learning_rate': 2.8615089908085406e-06, 'batch_size': 118, 'step_size': 6, 'gamma': 0.7543720187919338}. Best is trial 1 with value: 0.156080961227417.[0m
[32m[I 2025-02-08 08:00:50,298][0m Trial 6 finished with value: 0.1494164707916322 and parameters: {'observation_period_num': 71, 'train_rates': 0.8517292118495214, 'learning_rate': 5.225302964127384e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.7508002366286225}. Best is trial 6 with value: 0.1494164707916322.[0m
[32m[I 2025-02-08 08:02:37,376][0m Trial 7 finished with value: 0.04799087697866073 and parameters: {'observation_period_num': 17, 'train_rates': 0.8378999804420225, 'learning_rate': 6.0198156215423405e-05, 'batch_size': 52, 'step_size': 11, 'gamma': 0.9486900328999195}. Best is trial 7 with value: 0.04799087697866073.[0m
[32m[I 2025-02-08 08:04:54,342][0m Trial 8 finished with value: 0.1966795083705564 and parameters: {'observation_period_num': 188, 'train_rates': 0.858921958739141, 'learning_rate': 0.0003012533899848131, 'batch_size': 39, 'step_size': 10, 'gamma': 0.8682326591626219}. Best is trial 7 with value: 0.04799087697866073.[0m
[32m[I 2025-02-08 08:07:02,052][0m Trial 9 finished with value: 0.14314465126477358 and parameters: {'observation_period_num': 83, 'train_rates': 0.9119126060551933, 'learning_rate': 0.00014863160417255352, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8902290396863464}. Best is trial 7 with value: 0.04799087697866073.[0m
[32m[I 2025-02-08 08:07:31,781][0m Trial 10 finished with value: 0.19691921887241814 and parameters: {'observation_period_num': 8, 'train_rates': 0.6586144636455228, 'learning_rate': 7.656698724835573e-06, 'batch_size': 162, 'step_size': 15, 'gamma': 0.9867059337059021}. Best is trial 7 with value: 0.04799087697866073.[0m
[32m[I 2025-02-08 08:11:58,333][0m Trial 11 finished with value: 0.043274264639386766 and parameters: {'observation_period_num': 16, 'train_rates': 0.9561668121893425, 'learning_rate': 0.00018421527641651202, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9348461730931992}. Best is trial 11 with value: 0.043274264639386766.[0m
[32m[I 2025-02-08 08:16:08,056][0m Trial 12 finished with value: 0.033096372794646486 and parameters: {'observation_period_num': 8, 'train_rates': 0.9826434517870474, 'learning_rate': 0.00016457774355828589, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9572259920018774}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:21:39,780][0m Trial 13 finished with value: 0.03689705999568105 and parameters: {'observation_period_num': 39, 'train_rates': 0.9892255450789162, 'learning_rate': 0.0003461292269427239, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9276619547654807}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:22:58,382][0m Trial 14 finished with value: 0.051945194602012634 and parameters: {'observation_period_num': 41, 'train_rates': 0.9848300536498069, 'learning_rate': 0.0008024829524656556, 'batch_size': 80, 'step_size': 15, 'gamma': 0.983692756262011}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:27:47,823][0m Trial 15 finished with value: 0.17351947564697057 and parameters: {'observation_period_num': 48, 'train_rates': 0.6871264590247446, 'learning_rate': 0.00032227615400035265, 'batch_size': 16, 'step_size': 14, 'gamma': 0.90828128307781}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:29:05,598][0m Trial 16 finished with value: 0.12474379688501358 and parameters: {'observation_period_num': 146, 'train_rates': 0.9829286433255259, 'learning_rate': 0.00016031937408535392, 'batch_size': 81, 'step_size': 3, 'gamma': 0.967217075502244}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:29:43,968][0m Trial 17 finished with value: 0.144895379812692 and parameters: {'observation_period_num': 47, 'train_rates': 0.9036716829772776, 'learning_rate': 1.2558851279642502e-05, 'batch_size': 160, 'step_size': 13, 'gamma': 0.9229329193156685}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:30:38,110][0m Trial 18 finished with value: 0.20178390697638193 and parameters: {'observation_period_num': 39, 'train_rates': 0.6151771575358571, 'learning_rate': 0.0004409241869513001, 'batch_size': 82, 'step_size': 5, 'gamma': 0.960310203509322}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:33:52,626][0m Trial 19 finished with value: 0.23628801751540476 and parameters: {'observation_period_num': 121, 'train_rates': 0.7350970756234987, 'learning_rate': 0.00010214579980753747, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9110264647481514}. Best is trial 12 with value: 0.033096372794646486.[0m
Early stopping at epoch 75
[32m[I 2025-02-08 08:34:25,386][0m Trial 20 finished with value: 0.15546772676052842 and parameters: {'observation_period_num': 65, 'train_rates': 0.8963035011920151, 'learning_rate': 0.00042132046281829463, 'batch_size': 142, 'step_size': 1, 'gamma': 0.8334096151357171}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:39:48,875][0m Trial 21 finished with value: 0.04027002865899794 and parameters: {'observation_period_num': 9, 'train_rates': 0.9457560561818413, 'learning_rate': 0.00018361668189277372, 'batch_size': 18, 'step_size': 10, 'gamma': 0.934002507941837}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:41:28,919][0m Trial 22 finished with value: 0.03671405689250788 and parameters: {'observation_period_num': 10, 'train_rates': 0.9428305428915239, 'learning_rate': 9.937645072710311e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.8968245360372675}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:43:01,141][0m Trial 23 finished with value: 0.04621212337823475 and parameters: {'observation_period_num': 29, 'train_rates': 0.9770693758796564, 'learning_rate': 9.135325992635467e-05, 'batch_size': 67, 'step_size': 13, 'gamma': 0.8937060760706254}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:45:29,421][0m Trial 24 finished with value: 0.07507265510037542 and parameters: {'observation_period_num': 60, 'train_rates': 0.9312281881138803, 'learning_rate': 2.4298491562147452e-05, 'batch_size': 39, 'step_size': 9, 'gamma': 0.8942553914952248}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:48:07,165][0m Trial 25 finished with value: 0.03671959415078163 and parameters: {'observation_period_num': 5, 'train_rates': 0.9898594115227742, 'learning_rate': 0.0005927411819671625, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9544301528827329}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:49:09,671][0m Trial 26 finished with value: 0.0673366706063853 and parameters: {'observation_period_num': 26, 'train_rates': 0.892626475626134, 'learning_rate': 0.0006552785580425754, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9642304044046309}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:50:41,872][0m Trial 27 finished with value: 0.18255145847797394 and parameters: {'observation_period_num': 252, 'train_rates': 0.9546948781104234, 'learning_rate': 8.482484820511462e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8412782875298425}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:51:30,341][0m Trial 28 finished with value: 0.041016406832485024 and parameters: {'observation_period_num': 7, 'train_rates': 0.9263817343708749, 'learning_rate': 0.0002383309474456809, 'batch_size': 128, 'step_size': 12, 'gamma': 0.9523853755936352}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:53:49,009][0m Trial 29 finished with value: 0.12404895514603942 and parameters: {'observation_period_num': 99, 'train_rates': 0.8092103246066983, 'learning_rate': 0.0005474041673506083, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9747172659301669}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:54:50,078][0m Trial 30 finished with value: 0.32002715991353087 and parameters: {'observation_period_num': 169, 'train_rates': 0.962226599669175, 'learning_rate': 1.9923160449368122e-05, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9107133855535335}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:57:58,947][0m Trial 31 finished with value: 0.05181196540594101 and parameters: {'observation_period_num': 29, 'train_rates': 0.9829910770290728, 'learning_rate': 0.0009728386695941459, 'batch_size': 32, 'step_size': 14, 'gamma': 0.9244115920006225}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 08:59:49,075][0m Trial 32 finished with value: 0.03574127480387688 and parameters: {'observation_period_num': 5, 'train_rates': 0.9595679856218985, 'learning_rate': 0.000317494226005335, 'batch_size': 54, 'step_size': 15, 'gamma': 0.9475196911518293}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:01:29,003][0m Trial 33 finished with value: 0.12186952391741696 and parameters: {'observation_period_num': 23, 'train_rates': 0.8761233218936774, 'learning_rate': 0.00014384318012530345, 'batch_size': 57, 'step_size': 12, 'gamma': 0.9464248533563986}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:02:51,812][0m Trial 34 finished with value: 0.03435143185730692 and parameters: {'observation_period_num': 6, 'train_rates': 0.9354411682436367, 'learning_rate': 0.00025483519283974143, 'batch_size': 71, 'step_size': 14, 'gamma': 0.8802786797030313}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:04:12,640][0m Trial 35 finished with value: 0.07310671331577523 and parameters: {'observation_period_num': 62, 'train_rates': 0.9260294653671862, 'learning_rate': 4.6017766368668385e-05, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8491565518659034}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:04:37,826][0m Trial 36 finished with value: 0.15360862016677856 and parameters: {'observation_period_num': 79, 'train_rates': 0.9429754141117959, 'learning_rate': 0.00010893940976855212, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8186510784327088}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:05:38,455][0m Trial 37 finished with value: 0.10066778212785721 and parameters: {'observation_period_num': 50, 'train_rates': 0.9631098318886363, 'learning_rate': 3.933075970083848e-05, 'batch_size': 102, 'step_size': 12, 'gamma': 0.8809327454844476}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:07:13,149][0m Trial 38 finished with value: 0.18152668157761748 and parameters: {'observation_period_num': 30, 'train_rates': 0.7524353985299794, 'learning_rate': 6.838453274042656e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8578195577547442}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:07:42,620][0m Trial 39 finished with value: 0.8034821503898577 and parameters: {'observation_period_num': 99, 'train_rates': 0.8768579329645322, 'learning_rate': 1.0467630875491506e-06, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8760969221843443}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:08:32,966][0m Trial 40 finished with value: 0.04292399270838722 and parameters: {'observation_period_num': 19, 'train_rates': 0.9183841800386912, 'learning_rate': 0.00026678917228622356, 'batch_size': 118, 'step_size': 9, 'gamma': 0.8065956463613912}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:10:29,396][0m Trial 41 finished with value: 0.03942287615923719 and parameters: {'observation_period_num': 7, 'train_rates': 0.9702129725431952, 'learning_rate': 0.0006067425206969333, 'batch_size': 53, 'step_size': 14, 'gamma': 0.9471587265782245}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:12:33,804][0m Trial 42 finished with value: 0.05645920696561454 and parameters: {'observation_period_num': 5, 'train_rates': 0.9381355365529798, 'learning_rate': 0.00021793231973844628, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9016975877937561}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:13:58,997][0m Trial 43 finished with value: 0.05454661857848074 and parameters: {'observation_period_num': 21, 'train_rates': 0.9480962885550658, 'learning_rate': 0.00046990223126801316, 'batch_size': 71, 'step_size': 12, 'gamma': 0.979687809649176}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:17:25,322][0m Trial 44 finished with value: 0.07283963310053047 and parameters: {'observation_period_num': 32, 'train_rates': 0.964948686068774, 'learning_rate': 0.00013473754523352115, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9573093077336634}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:19:30,245][0m Trial 45 finished with value: 0.043147295622748244 and parameters: {'observation_period_num': 15, 'train_rates': 0.8327808624546843, 'learning_rate': 0.0003539927567337454, 'batch_size': 44, 'step_size': 15, 'gamma': 0.7825181291986676}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:20:34,516][0m Trial 46 finished with value: 0.10113319925404349 and parameters: {'observation_period_num': 50, 'train_rates': 0.9133043691425303, 'learning_rate': 0.0007307398322588001, 'batch_size': 90, 'step_size': 11, 'gamma': 0.9377934609311682}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:23:29,779][0m Trial 47 finished with value: 0.035795416748314574 and parameters: {'observation_period_num': 5, 'train_rates': 0.8845400521210923, 'learning_rate': 0.0002567955879649744, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9709576795283521}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:26:34,629][0m Trial 48 finished with value: 0.06094203274865885 and parameters: {'observation_period_num': 37, 'train_rates': 0.8870814822851382, 'learning_rate': 0.0002395749943890646, 'batch_size': 30, 'step_size': 15, 'gamma': 0.9738985093500603}. Best is trial 12 with value: 0.033096372794646486.[0m
[32m[I 2025-02-08 09:27:49,211][0m Trial 49 finished with value: 0.0396812687141036 and parameters: {'observation_period_num': 19, 'train_rates': 0.8661073029374909, 'learning_rate': 0.00012910588578482262, 'batch_size': 75, 'step_size': 13, 'gamma': 0.8668665446554601}. Best is trial 12 with value: 0.033096372794646486.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-08 09:27:49,221][0m A new study created in memory with name: no-name-cdbd970f-de92-41b9-b9be-b67f284ffced[0m
[32m[I 2025-02-08 09:28:37,227][0m Trial 0 finished with value: 0.2059346296886603 and parameters: {'observation_period_num': 166, 'train_rates': 0.6779992099069279, 'learning_rate': 0.00012328618995562557, 'batch_size': 97, 'step_size': 10, 'gamma': 0.7585929086437411}. Best is trial 0 with value: 0.2059346296886603.[0m
[32m[I 2025-02-08 09:30:31,430][0m Trial 1 finished with value: 0.49323124420504966 and parameters: {'observation_period_num': 61, 'train_rates': 0.6722882552611742, 'learning_rate': 2.5688340410409117e-06, 'batch_size': 40, 'step_size': 4, 'gamma': 0.8646448573506976}. Best is trial 0 with value: 0.2059346296886603.[0m
Early stopping at epoch 94
[32m[I 2025-02-08 09:31:57,729][0m Trial 2 finished with value: 1.0740079217779952 and parameters: {'observation_period_num': 49, 'train_rates': 0.845080608973809, 'learning_rate': 3.7710739987975333e-06, 'batch_size': 62, 'step_size': 2, 'gamma': 0.7549425420003814}. Best is trial 0 with value: 0.2059346296886603.[0m
[32m[I 2025-02-08 09:33:07,694][0m Trial 3 finished with value: 0.18711622360889324 and parameters: {'observation_period_num': 48, 'train_rates': 0.7750664100749299, 'learning_rate': 5.550688601989109e-05, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9891390094751209}. Best is trial 3 with value: 0.18711622360889324.[0m
[32m[I 2025-02-08 09:34:25,910][0m Trial 4 finished with value: 0.3238683704637055 and parameters: {'observation_period_num': 238, 'train_rates': 0.7423713372726377, 'learning_rate': 1.4567417025714608e-05, 'batch_size': 60, 'step_size': 7, 'gamma': 0.9509253298063942}. Best is trial 3 with value: 0.18711622360889324.[0m
[32m[I 2025-02-08 09:35:02,951][0m Trial 5 finished with value: 0.07762712985277176 and parameters: {'observation_period_num': 27, 'train_rates': 0.9567044844762189, 'learning_rate': 7.197103104464753e-05, 'batch_size': 175, 'step_size': 7, 'gamma': 0.885421713632064}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:35:34,574][0m Trial 6 finished with value: 0.6844023742474789 and parameters: {'observation_period_num': 153, 'train_rates': 0.8775991652303278, 'learning_rate': 1.6392301889268685e-05, 'batch_size': 187, 'step_size': 1, 'gamma': 0.9000001674131171}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:36:04,691][0m Trial 7 finished with value: 0.24512790493558698 and parameters: {'observation_period_num': 214, 'train_rates': 0.82510168619276, 'learning_rate': 0.0002717873230825425, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8821465280424364}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:38:31,583][0m Trial 8 finished with value: 0.08672933238267433 and parameters: {'observation_period_num': 49, 'train_rates': 0.9121586497711262, 'learning_rate': 1.4691177227011414e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.7775658386297627}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:39:12,219][0m Trial 9 finished with value: 0.6060699745694559 and parameters: {'observation_period_num': 12, 'train_rates': 0.6029716021386959, 'learning_rate': 1.1809076408529787e-06, 'batch_size': 118, 'step_size': 14, 'gamma': 0.9076057399326627}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:39:37,570][0m Trial 10 finished with value: 0.09767115861177444 and parameters: {'observation_period_num': 98, 'train_rates': 0.9780137977809013, 'learning_rate': 0.0009750698625547329, 'batch_size': 256, 'step_size': 6, 'gamma': 0.8298240346435248}. Best is trial 5 with value: 0.07762712985277176.[0m
[32m[I 2025-02-08 09:40:17,290][0m Trial 11 finished with value: 0.07148565351963043 and parameters: {'observation_period_num': 6, 'train_rates': 0.9751643740375753, 'learning_rate': 2.8583743203533682e-05, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8086799918069664}. Best is trial 11 with value: 0.07148565351963043.[0m
[32m[I 2025-02-08 09:40:56,670][0m Trial 12 finished with value: 0.06642437726259232 and parameters: {'observation_period_num': 6, 'train_rates': 0.9764870811376086, 'learning_rate': 5.5024916267522135e-05, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8277571543122897}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:41:36,665][0m Trial 13 finished with value: 0.08101818841853331 and parameters: {'observation_period_num': 106, 'train_rates': 0.9206422024932052, 'learning_rate': 0.0002876197492743224, 'batch_size': 153, 'step_size': 10, 'gamma': 0.8167755959324284}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:42:08,058][0m Trial 14 finished with value: 0.08914229273796082 and parameters: {'observation_period_num': 8, 'train_rates': 0.9885433576632949, 'learning_rate': 2.553880864779358e-05, 'batch_size': 221, 'step_size': 12, 'gamma': 0.8095927163281105}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:42:51,643][0m Trial 15 finished with value: 0.4531454786597958 and parameters: {'observation_period_num': 84, 'train_rates': 0.9143217245185357, 'learning_rate': 5.693456084658381e-06, 'batch_size': 136, 'step_size': 8, 'gamma': 0.8453433660865751}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:43:20,682][0m Trial 16 finished with value: 0.13026843965053558 and parameters: {'observation_period_num': 128, 'train_rates': 0.9418673756803029, 'learning_rate': 0.00019849371833681573, 'batch_size': 216, 'step_size': 12, 'gamma': 0.795204461521948}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:43:59,099][0m Trial 17 finished with value: 0.09810190421861929 and parameters: {'observation_period_num': 72, 'train_rates': 0.8849387929993293, 'learning_rate': 4.531931160947288e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.8499990276342493}. Best is trial 12 with value: 0.06642437726259232.[0m
[32m[I 2025-02-08 09:44:50,392][0m Trial 18 finished with value: 0.03558632306806691 and parameters: {'observation_period_num': 6, 'train_rates': 0.854823290490429, 'learning_rate': 0.0007473014067854961, 'batch_size': 111, 'step_size': 5, 'gamma': 0.7860494546694116}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:45:41,699][0m Trial 19 finished with value: 0.04782991111278534 and parameters: {'observation_period_num': 33, 'train_rates': 0.8173438507755855, 'learning_rate': 0.0008175933428308653, 'batch_size': 107, 'step_size': 4, 'gamma': 0.7776830900467645}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:46:32,243][0m Trial 20 finished with value: 0.18232879313988426 and parameters: {'observation_period_num': 35, 'train_rates': 0.7863132803138577, 'learning_rate': 0.0008823030198383932, 'batch_size': 108, 'step_size': 4, 'gamma': 0.7819004592336732}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:47:17,653][0m Trial 21 finished with value: 0.046315894178722214 and parameters: {'observation_period_num': 28, 'train_rates': 0.8354573375249678, 'learning_rate': 0.0004713476091243016, 'batch_size': 126, 'step_size': 4, 'gamma': 0.7735230282520686}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:48:02,966][0m Trial 22 finished with value: 0.048253257258618176 and parameters: {'observation_period_num': 31, 'train_rates': 0.8350045973523074, 'learning_rate': 0.0005083968599621416, 'batch_size': 124, 'step_size': 4, 'gamma': 0.7762187652947075}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:49:01,057][0m Trial 23 finished with value: 0.18242393465368612 and parameters: {'observation_period_num': 71, 'train_rates': 0.728641252064115, 'learning_rate': 0.0006111242711979914, 'batch_size': 87, 'step_size': 5, 'gamma': 0.7593341455946481}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:49:43,617][0m Trial 24 finished with value: 0.07909535189187336 and parameters: {'observation_period_num': 26, 'train_rates': 0.8083442040214474, 'learning_rate': 0.00044768127307627986, 'batch_size': 132, 'step_size': 2, 'gamma': 0.7916406813847093}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:50:39,114][0m Trial 25 finished with value: 0.12991091416975886 and parameters: {'observation_period_num': 115, 'train_rates': 0.8636926698545401, 'learning_rate': 0.00015032785858232588, 'batch_size': 100, 'step_size': 3, 'gamma': 0.7995426436594213}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:54:39,151][0m Trial 26 finished with value: 0.2557735432899833 and parameters: {'observation_period_num': 163, 'train_rates': 0.7602318410028661, 'learning_rate': 0.0003528050390519577, 'batch_size': 20, 'step_size': 6, 'gamma': 0.7515396642316301}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:55:44,015][0m Trial 27 finished with value: 0.08975252905188882 and parameters: {'observation_period_num': 81, 'train_rates': 0.8133882081500594, 'learning_rate': 0.00011366940645769825, 'batch_size': 81, 'step_size': 5, 'gamma': 0.7753946099942606}. Best is trial 18 with value: 0.03558632306806691.[0m
Early stopping at epoch 87
[32m[I 2025-02-08 09:56:28,449][0m Trial 28 finished with value: 0.0794799510216297 and parameters: {'observation_period_num': 42, 'train_rates': 0.852718118311304, 'learning_rate': 0.0006507875620791202, 'batch_size': 115, 'step_size': 1, 'gamma': 0.8348166656152238}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:57:16,136][0m Trial 29 finished with value: 0.4132751086477526 and parameters: {'observation_period_num': 204, 'train_rates': 0.701962205022186, 'learning_rate': 0.00010209075993526257, 'batch_size': 98, 'step_size': 3, 'gamma': 0.7634487878444872}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:58:00,504][0m Trial 30 finished with value: 0.039956425036330824 and parameters: {'observation_period_num': 20, 'train_rates': 0.8950865392554281, 'learning_rate': 0.00020103927328322396, 'batch_size': 137, 'step_size': 5, 'gamma': 0.9297755184197513}. Best is trial 18 with value: 0.03558632306806691.[0m
[32m[I 2025-02-08 09:58:43,856][0m Trial 31 finished with value: 0.03490629569860175 and parameters: {'observation_period_num': 18, 'train_rates': 0.891390838428539, 'learning_rate': 0.00022238016286435283, 'batch_size': 141, 'step_size': 5, 'gamma': 0.9615407837241159}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 09:59:26,775][0m Trial 32 finished with value: 0.04098041027784347 and parameters: {'observation_period_num': 19, 'train_rates': 0.8982220755982314, 'learning_rate': 0.00019858923285923888, 'batch_size': 141, 'step_size': 6, 'gamma': 0.9361422425256747}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:00:06,803][0m Trial 33 finished with value: 0.09663754769346931 and parameters: {'observation_period_num': 62, 'train_rates': 0.8900679608930254, 'learning_rate': 0.0002703349229416197, 'batch_size': 145, 'step_size': 6, 'gamma': 0.9388510638346843}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:00:38,113][0m Trial 34 finished with value: 0.04989594593644142 and parameters: {'observation_period_num': 18, 'train_rates': 0.9357977001137273, 'learning_rate': 0.00017548208237919247, 'batch_size': 199, 'step_size': 8, 'gamma': 0.9586647274980657}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:01:20,713][0m Trial 35 finished with value: 0.08638204521920582 and parameters: {'observation_period_num': 58, 'train_rates': 0.8994896085194236, 'learning_rate': 0.0002261623133827331, 'batch_size': 141, 'step_size': 5, 'gamma': 0.9868606168933927}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:02:49,209][0m Trial 36 finished with value: 0.049628358197415595 and parameters: {'observation_period_num': 47, 'train_rates': 0.8695300572719717, 'learning_rate': 0.00010077747601490654, 'batch_size': 64, 'step_size': 7, 'gamma': 0.925124426308915}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:03:26,898][0m Trial 37 finished with value: 0.05576428771018982 and parameters: {'observation_period_num': 20, 'train_rates': 0.9483840507292196, 'learning_rate': 7.563470182597066e-05, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9720527766989582}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:04:31,603][0m Trial 38 finished with value: 0.05390523759914296 and parameters: {'observation_period_num': 56, 'train_rates': 0.8557319870624759, 'learning_rate': 0.0003801093205010629, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9309047254310803}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:05:03,886][0m Trial 39 finished with value: 0.11673133966234542 and parameters: {'observation_period_num': 142, 'train_rates': 0.8969973163439496, 'learning_rate': 0.00014503922421371676, 'batch_size': 192, 'step_size': 6, 'gamma': 0.9185547121003915}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:05:43,396][0m Trial 40 finished with value: 0.4396085965124917 and parameters: {'observation_period_num': 186, 'train_rates': 0.9239667954384329, 'learning_rate': 6.566672551639942e-06, 'batch_size': 149, 'step_size': 8, 'gamma': 0.9510047849617165}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:06:26,466][0m Trial 41 finished with value: 0.045417305074156596 and parameters: {'observation_period_num': 23, 'train_rates': 0.8403401496618161, 'learning_rate': 0.00042071253683933753, 'batch_size': 131, 'step_size': 5, 'gamma': 0.9730663103724595}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:07:12,958][0m Trial 42 finished with value: 0.037178139972001506 and parameters: {'observation_period_num': 18, 'train_rates': 0.84348147368034, 'learning_rate': 0.000323132783618836, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9691394427462721}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:08:04,247][0m Trial 43 finished with value: 0.04438668930744008 and parameters: {'observation_period_num': 41, 'train_rates': 0.8758315444857836, 'learning_rate': 0.00021655004651406667, 'batch_size': 113, 'step_size': 5, 'gamma': 0.9436655148860407}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:08:38,088][0m Trial 44 finished with value: 0.1701975046186754 and parameters: {'observation_period_num': 13, 'train_rates': 0.7946697399342958, 'learning_rate': 0.0006325408276385975, 'batch_size': 164, 'step_size': 6, 'gamma': 0.965546339116552}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:09:41,293][0m Trial 45 finished with value: 0.12268950404630992 and parameters: {'observation_period_num': 5, 'train_rates': 0.6224598151043753, 'learning_rate': 0.00030086948370368946, 'batch_size': 72, 'step_size': 2, 'gamma': 0.9065593056536502}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:10:26,450][0m Trial 46 finished with value: 0.06129857095388266 and parameters: {'observation_period_num': 18, 'train_rates': 0.8985356436214958, 'learning_rate': 6.96159141805975e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.8814783741800437}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:11:12,470][0m Trial 47 finished with value: 0.2523644520499842 and parameters: {'observation_period_num': 247, 'train_rates': 0.8643072069797518, 'learning_rate': 3.9740836927067263e-05, 'batch_size': 121, 'step_size': 4, 'gamma': 0.8972395121322441}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:11:49,919][0m Trial 48 finished with value: 0.069428451359272 and parameters: {'observation_period_num': 40, 'train_rates': 0.9624000190341009, 'learning_rate': 0.0001361748063106942, 'batch_size': 177, 'step_size': 3, 'gamma': 0.984143116540835}. Best is trial 31 with value: 0.03490629569860175.[0m
[32m[I 2025-02-08 10:12:28,755][0m Trial 49 finished with value: 0.20363929699844038 and parameters: {'observation_period_num': 53, 'train_rates': 0.9270095636945802, 'learning_rate': 2.0851875072637803e-05, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8710873465117354}. Best is trial 31 with value: 0.03490629569860175.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-08 10:12:28,765][0m A new study created in memory with name: no-name-2b42eaec-ef55-44fa-b430-bb0ecd6eadd3[0m
[32m[I 2025-02-08 10:13:21,628][0m Trial 0 finished with value: 1.0478553901974557 and parameters: {'observation_period_num': 177, 'train_rates': 0.6510768116897633, 'learning_rate': 1.040267956905423e-06, 'batch_size': 86, 'step_size': 3, 'gamma': 0.9834273094252846}. Best is trial 0 with value: 1.0478553901974557.[0m
[32m[I 2025-02-08 10:13:53,303][0m Trial 1 finished with value: 0.7515519666671753 and parameters: {'observation_period_num': 123, 'train_rates': 0.8943237650554543, 'learning_rate': 1.6048978263761993e-06, 'batch_size': 183, 'step_size': 7, 'gamma': 0.959690801890479}. Best is trial 1 with value: 0.7515519666671753.[0m
[32m[I 2025-02-08 10:14:47,013][0m Trial 2 finished with value: 0.3437461881609312 and parameters: {'observation_period_num': 157, 'train_rates': 0.6693419266752786, 'learning_rate': 0.00010308509925584049, 'batch_size': 86, 'step_size': 5, 'gamma': 0.9304352479144136}. Best is trial 2 with value: 0.3437461881609312.[0m
[32m[I 2025-02-08 10:17:41,029][0m Trial 3 finished with value: 0.14347951897509295 and parameters: {'observation_period_num': 167, 'train_rates': 0.7910738138867662, 'learning_rate': 0.00044142684814205504, 'batch_size': 28, 'step_size': 14, 'gamma': 0.928620776808312}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:18:36,436][0m Trial 4 finished with value: 0.1967281997203827 and parameters: {'observation_period_num': 223, 'train_rates': 0.9721677266115933, 'learning_rate': 3.550419891704409e-05, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9373478081903688}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:20:02,822][0m Trial 5 finished with value: 0.2887503178175329 and parameters: {'observation_period_num': 239, 'train_rates': 0.7720707052711587, 'learning_rate': 3.100142811490977e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.9373134108286603}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:23:39,317][0m Trial 6 finished with value: 0.24071485894957398 and parameters: {'observation_period_num': 84, 'train_rates': 0.840673125014981, 'learning_rate': 2.7378818356105337e-06, 'batch_size': 24, 'step_size': 13, 'gamma': 0.7708378375144744}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:24:04,781][0m Trial 7 finished with value: 0.16482450984356475 and parameters: {'observation_period_num': 39, 'train_rates': 0.8767245718604586, 'learning_rate': 2.1958615984286665e-05, 'batch_size': 246, 'step_size': 9, 'gamma': 0.8276478190855101}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:25:27,248][0m Trial 8 finished with value: 0.366729136959121 and parameters: {'observation_period_num': 213, 'train_rates': 0.7307887634533148, 'learning_rate': 2.2842132602887115e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.7618899771100518}. Best is trial 3 with value: 0.14347951897509295.[0m
[32m[I 2025-02-08 10:25:57,611][0m Trial 9 finished with value: 0.137763027232106 and parameters: {'observation_period_num': 186, 'train_rates': 0.8046531908763132, 'learning_rate': 0.00017960045537785107, 'batch_size': 175, 'step_size': 12, 'gamma': 0.7764342068128807}. Best is trial 9 with value: 0.137763027232106.[0m
[32m[I 2025-02-08 10:26:36,031][0m Trial 10 finished with value: 0.12588895857334137 and parameters: {'observation_period_num': 105, 'train_rates': 0.9887709312599695, 'learning_rate': 0.0009929229720027544, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8438945082878617}. Best is trial 10 with value: 0.12588895857334137.[0m
[32m[I 2025-02-08 10:27:16,810][0m Trial 11 finished with value: 0.08425697684288025 and parameters: {'observation_period_num': 101, 'train_rates': 0.9887102730664907, 'learning_rate': 0.0008654257304566808, 'batch_size': 168, 'step_size': 11, 'gamma': 0.8398753826969312}. Best is trial 11 with value: 0.08425697684288025.[0m
[32m[I 2025-02-08 10:27:54,454][0m Trial 12 finished with value: 0.08907228708267212 and parameters: {'observation_period_num': 88, 'train_rates': 0.9803789162402, 'learning_rate': 0.0009284569250678253, 'batch_size': 171, 'step_size': 10, 'gamma': 0.8603705029186762}. Best is trial 11 with value: 0.08425697684288025.[0m
[32m[I 2025-02-08 10:28:22,766][0m Trial 13 finished with value: 0.07005538046360016 and parameters: {'observation_period_num': 46, 'train_rates': 0.9247334262854636, 'learning_rate': 0.0009411865519655156, 'batch_size': 226, 'step_size': 10, 'gamma': 0.881698889140255}. Best is trial 13 with value: 0.07005538046360016.[0m
[32m[I 2025-02-08 10:28:50,115][0m Trial 14 finished with value: 0.0396849624812603 and parameters: {'observation_period_num': 7, 'train_rates': 0.9243038368507053, 'learning_rate': 0.0002698279412851041, 'batch_size': 238, 'step_size': 15, 'gamma': 0.886977149647331}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:29:17,610][0m Trial 15 finished with value: 0.04088854417204857 and parameters: {'observation_period_num': 6, 'train_rates': 0.9194283788223298, 'learning_rate': 0.0002299372405031657, 'batch_size': 246, 'step_size': 15, 'gamma': 0.8923804283351404}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:29:47,190][0m Trial 16 finished with value: 0.0440491430927068 and parameters: {'observation_period_num': 5, 'train_rates': 0.9190794431845979, 'learning_rate': 0.0001549474072697987, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8914655292438074}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:30:10,396][0m Trial 17 finished with value: 0.060747713752578016 and parameters: {'observation_period_num': 33, 'train_rates': 0.8592689483855553, 'learning_rate': 7.042685449885025e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9029087259844117}. Best is trial 14 with value: 0.0396849624812603.[0m
Early stopping at epoch 60
[32m[I 2025-02-08 10:30:29,949][0m Trial 18 finished with value: 0.11013266346722006 and parameters: {'observation_period_num': 7, 'train_rates': 0.9278169798675568, 'learning_rate': 0.00033456475646364366, 'batch_size': 210, 'step_size': 1, 'gamma': 0.8023874915288604}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:31:10,949][0m Trial 19 finished with value: 0.21844803951756842 and parameters: {'observation_period_num': 60, 'train_rates': 0.8316895388737953, 'learning_rate': 7.831953031660655e-06, 'batch_size': 134, 'step_size': 15, 'gamma': 0.9080645406040395}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:31:42,775][0m Trial 20 finished with value: 0.06628239154815674 and parameters: {'observation_period_num': 66, 'train_rates': 0.9440567828380194, 'learning_rate': 0.00029484484496248377, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8645088652393843}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:32:12,876][0m Trial 21 finished with value: 0.05135348352581956 and parameters: {'observation_period_num': 17, 'train_rates': 0.897956955445679, 'learning_rate': 0.0001237340481205356, 'batch_size': 231, 'step_size': 15, 'gamma': 0.8877056565457037}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:32:36,205][0m Trial 22 finished with value: 0.152170480491271 and parameters: {'observation_period_num': 7, 'train_rates': 0.6072850957127904, 'learning_rate': 5.733035481588653e-05, 'batch_size': 211, 'step_size': 14, 'gamma': 0.9011484712846287}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:33:01,364][0m Trial 23 finished with value: 0.055216167122125626 and parameters: {'observation_period_num': 29, 'train_rates': 0.9180590765614058, 'learning_rate': 0.0001806914110874172, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8757597769091631}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:33:28,754][0m Trial 24 finished with value: 0.08638249337673187 and parameters: {'observation_period_num': 57, 'train_rates': 0.9568389157835535, 'learning_rate': 0.00047109879509486704, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9105680299460576}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:34:01,068][0m Trial 25 finished with value: 0.04449862582569427 and parameters: {'observation_period_num': 20, 'train_rates': 0.8881390561769872, 'learning_rate': 0.00021454748022382454, 'batch_size': 189, 'step_size': 14, 'gamma': 0.8165422145063245}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:34:45,287][0m Trial 26 finished with value: 0.06314924530985802 and parameters: {'observation_period_num': 73, 'train_rates': 0.8545436256756527, 'learning_rate': 9.089592917578418e-05, 'batch_size': 137, 'step_size': 12, 'gamma': 0.8525038738957991}. Best is trial 14 with value: 0.0396849624812603.[0m
[32m[I 2025-02-08 10:35:27,187][0m Trial 27 finished with value: 0.03029877022795734 and parameters: {'observation_period_num': 5, 'train_rates': 0.9432591849909532, 'learning_rate': 0.0004789819860554699, 'batch_size': 149, 'step_size': 14, 'gamma': 0.8894572963823834}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:36:12,830][0m Trial 28 finished with value: 0.06892389804124832 and parameters: {'observation_period_num': 45, 'train_rates': 0.9541587414346582, 'learning_rate': 0.0005341356453851753, 'batch_size': 139, 'step_size': 13, 'gamma': 0.920605774287537}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:36:57,482][0m Trial 29 finished with value: 0.24465030755857575 and parameters: {'observation_period_num': 139, 'train_rates': 0.7328273915129613, 'learning_rate': 0.0002779281006372114, 'batch_size': 112, 'step_size': 3, 'gamma': 0.9899948944741453}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:37:34,317][0m Trial 30 finished with value: 0.14965131859392372 and parameters: {'observation_period_num': 25, 'train_rates': 0.8177702294013552, 'learning_rate': 9.994888653891653e-06, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9676719099625701}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:38:03,435][0m Trial 31 finished with value: 0.03346555314812006 and parameters: {'observation_period_num': 6, 'train_rates': 0.9139069192287161, 'learning_rate': 0.0005177403486112097, 'batch_size': 220, 'step_size': 15, 'gamma': 0.8900292466336966}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:38:35,006][0m Trial 32 finished with value: 0.041232322388496556 and parameters: {'observation_period_num': 21, 'train_rates': 0.8962665300209648, 'learning_rate': 0.0005898753524862567, 'batch_size': 196, 'step_size': 14, 'gamma': 0.8737656049555237}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:39:01,818][0m Trial 33 finished with value: 0.1073891893029213 and parameters: {'observation_period_num': 39, 'train_rates': 0.9452438764683782, 'learning_rate': 0.0003361178072826407, 'batch_size': 242, 'step_size': 15, 'gamma': 0.9534824555392492}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:39:29,417][0m Trial 34 finished with value: 0.03773966544996137 and parameters: {'observation_period_num': 5, 'train_rates': 0.868047080540438, 'learning_rate': 0.0006010959568686342, 'batch_size': 218, 'step_size': 4, 'gamma': 0.8896897707990412}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:39:56,519][0m Trial 35 finished with value: 0.06681199668030534 and parameters: {'observation_period_num': 52, 'train_rates': 0.8725536246630026, 'learning_rate': 0.0006125438281633863, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9185509935588347}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:40:28,835][0m Trial 36 finished with value: 0.09572294966540353 and parameters: {'observation_period_num': 130, 'train_rates': 0.8952291501558566, 'learning_rate': 0.0004179148331930966, 'batch_size': 188, 'step_size': 5, 'gamma': 0.8610091629633778}. Best is trial 27 with value: 0.03029877022795734.[0m
[32m[I 2025-02-08 10:41:28,721][0m Trial 37 finished with value: 0.030110707506537437 and parameters: {'observation_period_num': 21, 'train_rates': 0.9636580450164239, 'learning_rate': 0.000648628096492995, 'batch_size': 108, 'step_size': 3, 'gamma': 0.9474621713921687}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:42:33,054][0m Trial 38 finished with value: 0.04685260512326893 and parameters: {'observation_period_num': 33, 'train_rates': 0.9611811480187369, 'learning_rate': 0.000636926125419638, 'batch_size': 94, 'step_size': 3, 'gamma': 0.9496631954007716}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:43:16,985][0m Trial 39 finished with value: 0.8628057458183982 and parameters: {'observation_period_num': 72, 'train_rates': 0.7756668964888755, 'learning_rate': 1.7351840029713775e-06, 'batch_size': 118, 'step_size': 1, 'gamma': 0.9645818103172885}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:44:27,314][0m Trial 40 finished with value: 0.13309089529715226 and parameters: {'observation_period_num': 189, 'train_rates': 0.968052276278326, 'learning_rate': 0.00040863153977270533, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9392450309466133}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:45:53,590][0m Trial 41 finished with value: 0.03649361937976063 and parameters: {'observation_period_num': 19, 'train_rates': 0.9323627082133613, 'learning_rate': 0.000676530432357535, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9257990465204492}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:47:17,632][0m Trial 42 finished with value: 0.04511617626185003 and parameters: {'observation_period_num': 19, 'train_rates': 0.945517788279014, 'learning_rate': 0.0007209821771800233, 'batch_size': 70, 'step_size': 2, 'gamma': 0.9748334011640533}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:49:32,230][0m Trial 43 finished with value: 0.03713847531212701 and parameters: {'observation_period_num': 17, 'train_rates': 0.874935122140736, 'learning_rate': 0.0004026153399464766, 'batch_size': 41, 'step_size': 4, 'gamma': 0.9265919635772247}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:51:56,083][0m Trial 44 finished with value: 0.044563769579301 and parameters: {'observation_period_num': 40, 'train_rates': 0.9028981631174567, 'learning_rate': 0.0004268448971072784, 'batch_size': 39, 'step_size': 2, 'gamma': 0.927183938818909}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:57:29,815][0m Trial 45 finished with value: 0.04350783403150206 and parameters: {'observation_period_num': 20, 'train_rates': 0.8425851259489651, 'learning_rate': 0.00013184613642728779, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9422667959817116}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 10:59:33,128][0m Trial 46 finished with value: 0.13558731470977403 and parameters: {'observation_period_num': 84, 'train_rates': 0.937280675159124, 'learning_rate': 0.0007998752990790661, 'batch_size': 46, 'step_size': 7, 'gamma': 0.9297638497879586}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 11:00:52,552][0m Trial 47 finished with value: 0.04216342711565541 and parameters: {'observation_period_num': 31, 'train_rates': 0.9655233749156971, 'learning_rate': 0.00036315523322886145, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9501656475356853}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 11:02:22,673][0m Trial 48 finished with value: 0.07376528065302845 and parameters: {'observation_period_num': 50, 'train_rates': 0.9083447528288492, 'learning_rate': 0.00022734641433594583, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9159057894203908}. Best is trial 37 with value: 0.030110707506537437.[0m
[32m[I 2025-02-08 11:04:37,073][0m Trial 49 finished with value: 0.18322002781918767 and parameters: {'observation_period_num': 250, 'train_rates': 0.8755597378960878, 'learning_rate': 0.0008395840868172152, 'batch_size': 38, 'step_size': 8, 'gamma': 0.978415582846937}. Best is trial 37 with value: 0.030110707506537437.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-08 11:04:37,084][0m A new study created in memory with name: no-name-99e30c5c-89fb-4328-a42c-0d5f81929f68[0m
[32m[I 2025-02-08 11:05:16,259][0m Trial 0 finished with value: 0.37859826467253943 and parameters: {'observation_period_num': 15, 'train_rates': 0.6490432169135262, 'learning_rate': 1.2750211125197913e-06, 'batch_size': 120, 'step_size': 12, 'gamma': 0.9725488988080949}. Best is trial 0 with value: 0.37859826467253943.[0m
[32m[I 2025-02-08 11:05:49,310][0m Trial 1 finished with value: 0.29166433712789186 and parameters: {'observation_period_num': 6, 'train_rates': 0.6435827828422888, 'learning_rate': 6.1976024623444e-06, 'batch_size': 158, 'step_size': 3, 'gamma': 0.9111720024375733}. Best is trial 1 with value: 0.29166433712789186.[0m
[32m[I 2025-02-08 11:06:19,345][0m Trial 2 finished with value: 0.6091020204347467 and parameters: {'observation_period_num': 244, 'train_rates': 0.8385301203067546, 'learning_rate': 1.917957669133634e-06, 'batch_size': 187, 'step_size': 12, 'gamma': 0.9662293097979084}. Best is trial 1 with value: 0.29166433712789186.[0m
[32m[I 2025-02-08 11:07:08,105][0m Trial 3 finished with value: 0.33411588391103303 and parameters: {'observation_period_num': 68, 'train_rates': 0.7161448563048318, 'learning_rate': 2.1357804697758384e-05, 'batch_size': 103, 'step_size': 5, 'gamma': 0.8619135409967192}. Best is trial 1 with value: 0.29166433712789186.[0m
[32m[I 2025-02-08 11:08:24,549][0m Trial 4 finished with value: 0.33370246798244874 and parameters: {'observation_period_num': 21, 'train_rates': 0.7723038756728535, 'learning_rate': 1.7499908935741122e-06, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9321068240876442}. Best is trial 1 with value: 0.29166433712789186.[0m
[32m[I 2025-02-08 11:09:13,536][0m Trial 5 finished with value: 0.13158701435267497 and parameters: {'observation_period_num': 174, 'train_rates': 0.9503789625291124, 'learning_rate': 0.0006054314707894564, 'batch_size': 126, 'step_size': 11, 'gamma': 0.7522865314612455}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:09:41,009][0m Trial 6 finished with value: 0.9609100222587585 and parameters: {'observation_period_num': 49, 'train_rates': 0.9641178460435589, 'learning_rate': 1.8267762658933496e-06, 'batch_size': 254, 'step_size': 12, 'gamma': 0.7504663514599602}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:10:06,715][0m Trial 7 finished with value: 0.9748271388286578 and parameters: {'observation_period_num': 37, 'train_rates': 0.9005815402948569, 'learning_rate': 1.6085186689709493e-06, 'batch_size': 254, 'step_size': 6, 'gamma': 0.8320075368374674}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:11:21,254][0m Trial 8 finished with value: 0.4000594044076035 and parameters: {'observation_period_num': 17, 'train_rates': 0.753126540608284, 'learning_rate': 2.565921358447214e-06, 'batch_size': 69, 'step_size': 14, 'gamma': 0.7518167461238201}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:11:56,072][0m Trial 9 finished with value: 0.22359150493522886 and parameters: {'observation_period_num': 107, 'train_rates': 0.7569397269929109, 'learning_rate': 0.00012903673295518086, 'batch_size': 144, 'step_size': 11, 'gamma': 0.9306804518825635}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:15:58,769][0m Trial 10 finished with value: 0.1381220183024804 and parameters: {'observation_period_num': 200, 'train_rates': 0.9783460018348861, 'learning_rate': 0.0007767527068605975, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8069503881740353}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:19:12,095][0m Trial 11 finished with value: 0.1695878457678251 and parameters: {'observation_period_num': 201, 'train_rates': 0.9564153732775029, 'learning_rate': 0.0007396334801405992, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8003271813244198}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:23:20,322][0m Trial 12 finished with value: 0.1574365705849448 and parameters: {'observation_period_num': 171, 'train_rates': 0.874974644908873, 'learning_rate': 0.0009296291869522058, 'batch_size': 21, 'step_size': 8, 'gamma': 0.8002830514101437}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:23:53,346][0m Trial 13 finished with value: 0.1605214625597 and parameters: {'observation_period_num': 163, 'train_rates': 0.9847545792871255, 'learning_rate': 0.0002378662345431373, 'batch_size': 185, 'step_size': 15, 'gamma': 0.7913419985966559}. Best is trial 5 with value: 0.13158701435267497.[0m
Early stopping at epoch 75
[32m[I 2025-02-08 11:24:50,155][0m Trial 14 finished with value: 0.27678318379761335 and parameters: {'observation_period_num': 229, 'train_rates': 0.9157094923681798, 'learning_rate': 0.000218637879054639, 'batch_size': 72, 'step_size': 1, 'gamma': 0.8405256491253195}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:25:18,096][0m Trial 15 finished with value: 0.16070641448291448 and parameters: {'observation_period_num': 133, 'train_rates': 0.8345688187031454, 'learning_rate': 5.8528868824204536e-05, 'batch_size': 207, 'step_size': 9, 'gamma': 0.7769750513405803}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:26:15,121][0m Trial 16 finished with value: 0.13906754880416683 and parameters: {'observation_period_num': 193, 'train_rates': 0.9407963668530471, 'learning_rate': 0.0004733665223640656, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8251274169435744}. Best is trial 5 with value: 0.13158701435267497.[0m
[32m[I 2025-02-08 11:28:45,453][0m Trial 17 finished with value: 0.11925002187490463 and parameters: {'observation_period_num': 129, 'train_rates': 0.9898983135749793, 'learning_rate': 3.933424366546965e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8918935800525787}. Best is trial 17 with value: 0.11925002187490463.[0m
[32m[I 2025-02-08 11:30:31,558][0m Trial 18 finished with value: 0.14581134460866452 and parameters: {'observation_period_num': 106, 'train_rates': 0.8601248645958657, 'learning_rate': 1.841609649165624e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.9003579511977858}. Best is trial 17 with value: 0.11925002187490463.[0m
[32m[I 2025-02-08 11:31:25,574][0m Trial 19 finished with value: 0.10925120763156725 and parameters: {'observation_period_num': 142, 'train_rates': 0.9186602094455542, 'learning_rate': 5.8925945863962447e-05, 'batch_size': 105, 'step_size': 10, 'gamma': 0.8833053656577504}. Best is trial 19 with value: 0.10925120763156725.[0m
[32m[I 2025-02-08 11:32:23,569][0m Trial 20 finished with value: 0.10897972696909199 and parameters: {'observation_period_num': 141, 'train_rates': 0.9044725458396706, 'learning_rate': 6.505577320726274e-05, 'batch_size': 95, 'step_size': 6, 'gamma': 0.8852022497085532}. Best is trial 20 with value: 0.10897972696909199.[0m
[32m[I 2025-02-08 11:33:26,085][0m Trial 21 finished with value: 0.11597420830352634 and parameters: {'observation_period_num': 125, 'train_rates': 0.9101512592029191, 'learning_rate': 4.434962303789525e-05, 'batch_size': 91, 'step_size': 6, 'gamma': 0.8831990333140164}. Best is trial 20 with value: 0.10897972696909199.[0m
[32m[I 2025-02-08 11:34:22,893][0m Trial 22 finished with value: 0.1076136397176914 and parameters: {'observation_period_num': 89, 'train_rates': 0.8140827402805724, 'learning_rate': 7.393860423798018e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8639011360396047}. Best is trial 22 with value: 0.1076136397176914.[0m
[32m[I 2025-02-08 11:35:13,120][0m Trial 23 finished with value: 0.13432332008115708 and parameters: {'observation_period_num': 79, 'train_rates': 0.8066909254917726, 'learning_rate': 8.902504926662886e-05, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8567893759912723}. Best is trial 22 with value: 0.1076136397176914.[0m
[32m[I 2025-02-08 11:35:52,624][0m Trial 24 finished with value: 0.3262617068693918 and parameters: {'observation_period_num': 88, 'train_rates': 0.8819192889915287, 'learning_rate': 1.3789583224861614e-05, 'batch_size': 149, 'step_size': 4, 'gamma': 0.8685632674287238}. Best is trial 22 with value: 0.1076136397176914.[0m
[32m[I 2025-02-08 11:36:54,207][0m Trial 25 finished with value: 0.33165717427923336 and parameters: {'observation_period_num': 148, 'train_rates': 0.7013489433212883, 'learning_rate': 9.295262274408205e-05, 'batch_size': 82, 'step_size': 1, 'gamma': 0.9282629366045758}. Best is trial 22 with value: 0.1076136397176914.[0m
[32m[I 2025-02-08 11:38:28,226][0m Trial 26 finished with value: 0.36002333844461404 and parameters: {'observation_period_num': 145, 'train_rates': 0.8092987903037452, 'learning_rate': 8.281409174189725e-06, 'batch_size': 55, 'step_size': 4, 'gamma': 0.8489824856310669}. Best is trial 22 with value: 0.1076136397176914.[0m
[32m[I 2025-02-08 11:39:16,737][0m Trial 27 finished with value: 0.07505792591878226 and parameters: {'observation_period_num': 100, 'train_rates': 0.8514136335003379, 'learning_rate': 0.0001935947942700114, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9103354333862564}. Best is trial 27 with value: 0.07505792591878226.[0m
[32m[I 2025-02-08 11:40:03,217][0m Trial 28 finished with value: 0.08555221308645168 and parameters: {'observation_period_num': 104, 'train_rates': 0.8596487394308208, 'learning_rate': 0.00027659848053295774, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9069169426103116}. Best is trial 27 with value: 0.07505792591878226.[0m
[32m[I 2025-02-08 11:40:41,119][0m Trial 29 finished with value: 0.17768671105842332 and parameters: {'observation_period_num': 105, 'train_rates': 0.6114132344966667, 'learning_rate': 0.0003205242349013938, 'batch_size': 122, 'step_size': 2, 'gamma': 0.9611595379212824}. Best is trial 27 with value: 0.07505792591878226.[0m
[32m[I 2025-02-08 11:41:14,903][0m Trial 30 finished with value: 0.05866745798339864 and parameters: {'observation_period_num': 55, 'train_rates': 0.8365368017838348, 'learning_rate': 0.00014605147497873726, 'batch_size': 171, 'step_size': 3, 'gamma': 0.9826338300987871}. Best is trial 30 with value: 0.05866745798339864.[0m
[32m[I 2025-02-08 11:41:49,493][0m Trial 31 finished with value: 0.057206411231187436 and parameters: {'observation_period_num': 63, 'train_rates': 0.8416600150126278, 'learning_rate': 0.00014998079446795954, 'batch_size': 167, 'step_size': 3, 'gamma': 0.9888796747325604}. Best is trial 31 with value: 0.057206411231187436.[0m
[32m[I 2025-02-08 11:42:23,643][0m Trial 32 finished with value: 0.06223864644765854 and parameters: {'observation_period_num': 60, 'train_rates': 0.8537644288031957, 'learning_rate': 0.0001474360166496097, 'batch_size': 167, 'step_size': 2, 'gamma': 0.9492850077112777}. Best is trial 31 with value: 0.057206411231187436.[0m
[32m[I 2025-02-08 11:42:58,175][0m Trial 33 finished with value: 0.06876992893300046 and parameters: {'observation_period_num': 59, 'train_rates': 0.8354822936807059, 'learning_rate': 0.00015142575562694636, 'batch_size': 167, 'step_size': 3, 'gamma': 0.9867704588680218}. Best is trial 31 with value: 0.057206411231187436.[0m
[32m[I 2025-02-08 11:43:30,966][0m Trial 34 finished with value: 0.20694938286057007 and parameters: {'observation_period_num': 60, 'train_rates': 0.7913247612736287, 'learning_rate': 0.0001549486213503051, 'batch_size': 169, 'step_size': 1, 'gamma': 0.9820612746367255}. Best is trial 31 with value: 0.057206411231187436.[0m
[32m[I 2025-02-08 11:43:59,612][0m Trial 35 finished with value: 0.05129350130746682 and parameters: {'observation_period_num': 36, 'train_rates': 0.8307065178320369, 'learning_rate': 0.0004323578856726652, 'batch_size': 211, 'step_size': 5, 'gamma': 0.9542677836790041}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:44:25,513][0m Trial 36 finished with value: 0.179441547212162 and parameters: {'observation_period_num': 31, 'train_rates': 0.7803613862792156, 'learning_rate': 0.0004085289606735318, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9608201715334861}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:44:52,157][0m Trial 37 finished with value: 0.147739652941178 and parameters: {'observation_period_num': 8, 'train_rates': 0.7365155440701989, 'learning_rate': 0.0004165197580413549, 'batch_size': 213, 'step_size': 5, 'gamma': 0.9490092434389317}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:45:22,670][0m Trial 38 finished with value: 0.07678645883776969 and parameters: {'observation_period_num': 42, 'train_rates': 0.8302021560263654, 'learning_rate': 0.00010784588565770984, 'batch_size': 189, 'step_size': 2, 'gamma': 0.9452679027226025}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:45:50,653][0m Trial 39 finished with value: 0.08844397270412588 and parameters: {'observation_period_num': 26, 'train_rates': 0.8858791875296429, 'learning_rate': 2.4046684690666445e-05, 'batch_size': 233, 'step_size': 4, 'gamma': 0.9758454098690087}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:46:19,236][0m Trial 40 finished with value: 0.1655009800568223 and parameters: {'observation_period_num': 72, 'train_rates': 0.6749148325142326, 'learning_rate': 0.000521300113025584, 'batch_size': 181, 'step_size': 3, 'gamma': 0.9710637109262985}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:46:54,417][0m Trial 41 finished with value: 0.08344665130578591 and parameters: {'observation_period_num': 56, 'train_rates': 0.8233022851323577, 'learning_rate': 0.00016125903554947623, 'batch_size': 162, 'step_size': 3, 'gamma': 0.9880140928658593}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:47:25,371][0m Trial 42 finished with value: 0.061522576957941055 and parameters: {'observation_period_num': 49, 'train_rates': 0.8415890436783432, 'learning_rate': 0.00013287223188664313, 'batch_size': 198, 'step_size': 3, 'gamma': 0.9512629833671399}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:47:55,767][0m Trial 43 finished with value: 0.0650066191780156 and parameters: {'observation_period_num': 45, 'train_rates': 0.8508507318227755, 'learning_rate': 0.0003020700033030718, 'batch_size': 202, 'step_size': 5, 'gamma': 0.9443210816504614}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:48:22,310][0m Trial 44 finished with value: 0.09866677637861557 and parameters: {'observation_period_num': 72, 'train_rates': 0.7941560894144437, 'learning_rate': 0.00011689140556048235, 'batch_size': 225, 'step_size': 4, 'gamma': 0.9190822614460936}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:49:01,833][0m Trial 45 finished with value: 0.18175934477055328 and parameters: {'observation_period_num': 35, 'train_rates': 0.7753685544185746, 'learning_rate': 0.00020260386516377638, 'batch_size': 140, 'step_size': 2, 'gamma': 0.9568103062835848}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:49:33,366][0m Trial 46 finished with value: 0.2412972528811382 and parameters: {'observation_period_num': 6, 'train_rates': 0.875792524133808, 'learning_rate': 3.576083014998603e-06, 'batch_size': 194, 'step_size': 1, 'gamma': 0.9709446068573545}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:49:58,817][0m Trial 47 finished with value: 0.16695489195629584 and parameters: {'observation_period_num': 20, 'train_rates': 0.75336963511128, 'learning_rate': 0.0006666210396706967, 'batch_size': 240, 'step_size': 7, 'gamma': 0.9366400756469698}. Best is trial 35 with value: 0.05129350130746682.[0m
[32m[I 2025-02-08 11:50:33,089][0m Trial 48 finished with value: 0.05075932243330912 and parameters: {'observation_period_num': 53, 'train_rates': 0.8641443636034551, 'learning_rate': 0.0003579179295962485, 'batch_size': 179, 'step_size': 3, 'gamma': 0.9543366816257685}. Best is trial 48 with value: 0.05075932243330912.[0m
[32m[I 2025-02-08 11:51:08,666][0m Trial 49 finished with value: 0.10191180691430249 and parameters: {'observation_period_num': 52, 'train_rates': 0.933618225040079, 'learning_rate': 0.0009357690443929878, 'batch_size': 179, 'step_size': 5, 'gamma': 0.9789568261896808}. Best is trial 48 with value: 0.05075932243330912.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-08 11:51:08,678][0m A new study created in memory with name: no-name-82bf4fb4-bec0-4f0f-ace8-f14bd57a3111[0m
[32m[I 2025-02-08 11:51:37,618][0m Trial 0 finished with value: 0.21871985495090485 and parameters: {'observation_period_num': 214, 'train_rates': 0.9397892506259069, 'learning_rate': 0.00011639721861676105, 'batch_size': 213, 'step_size': 5, 'gamma': 0.959464049881136}. Best is trial 0 with value: 0.21871985495090485.[0m
Early stopping at epoch 54
[32m[I 2025-02-08 11:51:57,533][0m Trial 1 finished with value: 0.565325460792367 and parameters: {'observation_period_num': 166, 'train_rates': 0.8908307405533593, 'learning_rate': 4.418442566403987e-05, 'batch_size': 160, 'step_size': 1, 'gamma': 0.798649420523835}. Best is trial 0 with value: 0.21871985495090485.[0m
[32m[I 2025-02-08 11:52:21,516][0m Trial 2 finished with value: 0.1444201753155826 and parameters: {'observation_period_num': 137, 'train_rates': 0.8450003077385645, 'learning_rate': 0.0002017446187465341, 'batch_size': 252, 'step_size': 6, 'gamma': 0.9817337130652555}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:52:51,292][0m Trial 3 finished with value: 0.6653666118780772 and parameters: {'observation_period_num': 173, 'train_rates': 0.9142122190986702, 'learning_rate': 2.086670846119533e-06, 'batch_size': 212, 'step_size': 11, 'gamma': 0.9002218753467998}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:53:17,748][0m Trial 4 finished with value: 0.5415890611393351 and parameters: {'observation_period_num': 135, 'train_rates': 0.7992622343385417, 'learning_rate': 4.49994042297969e-06, 'batch_size': 212, 'step_size': 14, 'gamma': 0.893067501199037}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:53:40,588][0m Trial 5 finished with value: 0.2761293828956109 and parameters: {'observation_period_num': 138, 'train_rates': 0.7297382838596829, 'learning_rate': 6.554559533479219e-05, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8872485092507556}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:54:39,620][0m Trial 6 finished with value: 0.28501592245366836 and parameters: {'observation_period_num': 227, 'train_rates': 0.7302115115033632, 'learning_rate': 0.00017443403131916227, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8975118676941536}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:55:04,556][0m Trial 7 finished with value: 0.318418258273852 and parameters: {'observation_period_num': 97, 'train_rates': 0.7177804751478819, 'learning_rate': 0.0009915352432299446, 'batch_size': 216, 'step_size': 15, 'gamma': 0.8325130091678966}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:56:04,491][0m Trial 8 finished with value: 0.16399464989528384 and parameters: {'observation_period_num': 41, 'train_rates': 0.6080868280783506, 'learning_rate': 0.0005994913249841616, 'batch_size': 76, 'step_size': 4, 'gamma': 0.8672951450889886}. Best is trial 2 with value: 0.1444201753155826.[0m
[32m[I 2025-02-08 11:57:22,203][0m Trial 9 finished with value: 0.13932697456910123 and parameters: {'observation_period_num': 52, 'train_rates': 0.9009003334879404, 'learning_rate': 5.2397324481807024e-05, 'batch_size': 73, 'step_size': 13, 'gamma': 0.9879547187205413}. Best is trial 9 with value: 0.13932697456910123.[0m
[32m[I 2025-02-08 12:01:53,062][0m Trial 10 finished with value: 0.0721737147398191 and parameters: {'observation_period_num': 18, 'train_rates': 0.9754449118034995, 'learning_rate': 1.0444195354288358e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.7510964963718405}. Best is trial 10 with value: 0.0721737147398191.[0m
[32m[I 2025-02-08 12:06:53,737][0m Trial 11 finished with value: 0.059047943088172496 and parameters: {'observation_period_num': 10, 'train_rates': 0.970688787412044, 'learning_rate': 1.3283186561282085e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.7647624485609907}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:13:09,819][0m Trial 12 finished with value: 0.07120774686336517 and parameters: {'observation_period_num': 6, 'train_rates': 0.9891839761262262, 'learning_rate': 1.1284681742722353e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7516287863305577}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:17:07,095][0m Trial 13 finished with value: 0.14248506562865298 and parameters: {'observation_period_num': 77, 'train_rates': 0.9843383322464877, 'learning_rate': 1.1896451021403053e-05, 'batch_size': 25, 'step_size': 9, 'gamma': 0.7516232544126308}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:18:54,432][0m Trial 14 finished with value: 0.10703654805210983 and parameters: {'observation_period_num': 25, 'train_rates': 0.8499047404725272, 'learning_rate': 1.4823207662013404e-05, 'batch_size': 51, 'step_size': 7, 'gamma': 0.7848971593052532}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:19:45,686][0m Trial 15 finished with value: 0.945880936724799 and parameters: {'observation_period_num': 9, 'train_rates': 0.9529058355951068, 'learning_rate': 1.0831041232006914e-06, 'batch_size': 125, 'step_size': 10, 'gamma': 0.7944563029059736}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:20:33,217][0m Trial 16 finished with value: 0.48158547149937087 and parameters: {'observation_period_num': 78, 'train_rates': 0.8674658683642934, 'learning_rate': 4.126371616844203e-06, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8325299788014544}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:22:27,988][0m Trial 17 finished with value: 0.22559629215930516 and parameters: {'observation_period_num': 50, 'train_rates': 0.7859202590761296, 'learning_rate': 1.7374826147446108e-05, 'batch_size': 45, 'step_size': 11, 'gamma': 0.7736269891719219}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:22:56,175][0m Trial 18 finished with value: 0.6719874347235908 and parameters: {'observation_period_num': 90, 'train_rates': 0.6230823989490566, 'learning_rate': 5.370490548410145e-06, 'batch_size': 164, 'step_size': 7, 'gamma': 0.8233260274523782}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:23:56,646][0m Trial 19 finished with value: 0.0788281298381217 and parameters: {'observation_period_num': 5, 'train_rates': 0.9365590615560012, 'learning_rate': 2.7852396554786296e-05, 'batch_size': 102, 'step_size': 2, 'gamma': 0.9345181290635841}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:29:57,713][0m Trial 20 finished with value: 0.13827552852478434 and parameters: {'observation_period_num': 107, 'train_rates': 0.983795449655397, 'learning_rate': 8.530831625878433e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8100832258714659}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:32:29,396][0m Trial 21 finished with value: 0.07186482101678848 and parameters: {'observation_period_num': 31, 'train_rates': 0.9875560892546266, 'learning_rate': 2.5570281596567083e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.7547970503053495}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:34:43,763][0m Trial 22 finished with value: 0.08646703092036424 and parameters: {'observation_period_num': 38, 'train_rates': 0.9539207163264788, 'learning_rate': 2.4258975505500157e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7714519390831878}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:36:25,727][0m Trial 23 finished with value: 0.28728844685589566 and parameters: {'observation_period_num': 57, 'train_rates': 0.9299842798400497, 'learning_rate': 7.330848735924839e-06, 'batch_size': 57, 'step_size': 10, 'gamma': 0.7638340359437028}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:39:38,498][0m Trial 24 finished with value: 0.18772444888376272 and parameters: {'observation_period_num': 32, 'train_rates': 0.9644990723266518, 'learning_rate': 2.9855404055206622e-06, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8602252395774395}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:40:35,365][0m Trial 25 finished with value: 0.18174188594408644 and parameters: {'observation_period_num': 65, 'train_rates': 0.8755581202468821, 'learning_rate': 2.4404866208489775e-05, 'batch_size': 98, 'step_size': 7, 'gamma': 0.7835631954360746}. Best is trial 11 with value: 0.059047943088172496.[0m
[32m[I 2025-02-08 12:41:59,775][0m Trial 26 finished with value: 0.03862974620474282 and parameters: {'observation_period_num': 6, 'train_rates': 0.8113718403943919, 'learning_rate': 0.00010501553756976802, 'batch_size': 64, 'step_size': 9, 'gamma': 0.8144081648107044}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:43:15,892][0m Trial 27 finished with value: 0.13643566669668525 and parameters: {'observation_period_num': 9, 'train_rates': 0.6758566743718123, 'learning_rate': 0.00012426794136691441, 'batch_size': 63, 'step_size': 6, 'gamma': 0.848284598562808}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:44:16,903][0m Trial 28 finished with value: 0.1799220891893401 and parameters: {'observation_period_num': 24, 'train_rates': 0.7934912588085645, 'learning_rate': 0.00034564405780873706, 'batch_size': 88, 'step_size': 11, 'gamma': 0.7962390539027692}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:48:50,409][0m Trial 29 finished with value: 0.11964972099225842 and parameters: {'observation_period_num': 203, 'train_rates': 0.8192748325990876, 'learning_rate': 8.397306586784341e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8140943635982798}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:49:25,390][0m Trial 30 finished with value: 0.2797609231845793 and parameters: {'observation_period_num': 71, 'train_rates': 0.7524234336422759, 'learning_rate': 4.154733574498571e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.7753531257674411}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:51:53,087][0m Trial 31 finished with value: 0.07015384632078084 and parameters: {'observation_period_num': 28, 'train_rates': 0.921356663116439, 'learning_rate': 1.5860118392997534e-05, 'batch_size': 39, 'step_size': 9, 'gamma': 0.7619944139133068}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:54:33,971][0m Trial 32 finished with value: 0.06357724129432632 and parameters: {'observation_period_num': 7, 'train_rates': 0.9163535089392224, 'learning_rate': 1.681842281616882e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8053721967340771}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:55:59,665][0m Trial 33 finished with value: 0.07147105964454445 and parameters: {'observation_period_num': 43, 'train_rates': 0.9115668377947674, 'learning_rate': 3.909627140335465e-05, 'batch_size': 67, 'step_size': 8, 'gamma': 0.801275247220737}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 12:58:39,202][0m Trial 34 finished with value: 0.08477448395222699 and parameters: {'observation_period_num': 23, 'train_rates': 0.8890295934798573, 'learning_rate': 1.6658078059284817e-05, 'batch_size': 35, 'step_size': 6, 'gamma': 0.7857030665478033}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 13:00:14,049][0m Trial 35 finished with value: 0.102355192896429 and parameters: {'observation_period_num': 171, 'train_rates': 0.8369480802347509, 'learning_rate': 9.967564909522227e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8461716553773374}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 13:00:48,841][0m Trial 36 finished with value: 0.18291260309196544 and parameters: {'observation_period_num': 117, 'train_rates': 0.9267162544249683, 'learning_rate': 5.9078720312926905e-05, 'batch_size': 188, 'step_size': 12, 'gamma': 0.8115366846935389}. Best is trial 26 with value: 0.03862974620474282.[0m
[32m[I 2025-02-08 13:03:26,970][0m Trial 37 finished with value: 0.035596714580371495 and parameters: {'observation_period_num': 17, 'train_rates': 0.8738639666170294, 'learning_rate': 0.00021000923474693892, 'batch_size': 35, 'step_size': 10, 'gamma': 0.7658087048420249}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:04:20,931][0m Trial 38 finished with value: 0.13761379596583334 and parameters: {'observation_period_num': 150, 'train_rates': 0.8619565751874496, 'learning_rate': 0.0002705543375018266, 'batch_size': 105, 'step_size': 13, 'gamma': 0.9196905835425744}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:05:20,282][0m Trial 39 finished with value: 0.1984435281133264 and parameters: {'observation_period_num': 252, 'train_rates': 0.8185051264275881, 'learning_rate': 0.0001735115683648651, 'batch_size': 88, 'step_size': 10, 'gamma': 0.9622087422259951}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:08:01,895][0m Trial 40 finished with value: 0.1930372113628047 and parameters: {'observation_period_num': 189, 'train_rates': 0.8890591275571083, 'learning_rate': 0.0005359750495330751, 'batch_size': 33, 'step_size': 11, 'gamma': 0.8282060011679729}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:09:58,417][0m Trial 41 finished with value: 0.03815970117492335 and parameters: {'observation_period_num': 16, 'train_rates': 0.9123126110745785, 'learning_rate': 0.000141928293379764, 'batch_size': 50, 'step_size': 8, 'gamma': 0.7660928900801428}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:11:26,082][0m Trial 42 finished with value: 0.04282462146754066 and parameters: {'observation_period_num': 14, 'train_rates': 0.9025513563752681, 'learning_rate': 0.00013895850093907107, 'batch_size': 66, 'step_size': 8, 'gamma': 0.7697706939639571}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:12:43,464][0m Trial 43 finished with value: 0.17979324266382138 and parameters: {'observation_period_num': 17, 'train_rates': 0.775603269224195, 'learning_rate': 0.00014191022717509235, 'batch_size': 68, 'step_size': 5, 'gamma': 0.7678925380950312}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:14:22,850][0m Trial 44 finished with value: 0.060532720826835515 and parameters: {'observation_period_num': 44, 'train_rates': 0.8295942520921143, 'learning_rate': 0.00026014202324584973, 'batch_size': 54, 'step_size': 8, 'gamma': 0.7814442932574907}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:15:50,561][0m Trial 45 finished with value: 0.06119129190995931 and parameters: {'observation_period_num': 60, 'train_rates': 0.899855476472681, 'learning_rate': 8.184413361607325e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.7892176201526512}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:17:05,688][0m Trial 46 finished with value: 0.04577686470126946 and parameters: {'observation_period_num': 18, 'train_rates': 0.9496740423348196, 'learning_rate': 0.0004340034052775176, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8813668887891706}. Best is trial 37 with value: 0.035596714580371495.[0m
[32m[I 2025-02-08 13:18:20,717][0m Trial 47 finished with value: 0.03508329744904469 and parameters: {'observation_period_num': 17, 'train_rates': 0.9470646080402849, 'learning_rate': 0.00045457222652129585, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8829388798391502}. Best is trial 47 with value: 0.03508329744904469.[0m
[32m[I 2025-02-08 13:19:23,523][0m Trial 48 finished with value: 0.07097544402365871 and parameters: {'observation_period_num': 39, 'train_rates': 0.8521335267935005, 'learning_rate': 0.0009292628204877547, 'batch_size': 90, 'step_size': 4, 'gamma': 0.909247517237531}. Best is trial 47 with value: 0.03508329744904469.[0m
[32m[I 2025-02-08 13:20:18,147][0m Trial 49 finished with value: 0.055046185688183535 and parameters: {'observation_period_num': 52, 'train_rates': 0.8822668646761307, 'learning_rate': 0.00022272970228031752, 'batch_size': 106, 'step_size': 6, 'gamma': 0.8879434264972569}. Best is trial 47 with value: 0.03508329744904469.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 26, 'train_rates': 0.9316245193494359, 'learning_rate': 0.000116284026956772, 'batch_size': 58, 'step_size': 15, 'gamma': 0.9243323443769295}
Epoch 1/300, trend Loss: 0.2545 | 0.2491
Epoch 2/300, trend Loss: 0.1669 | 0.1921
Epoch 3/300, trend Loss: 0.1386 | 0.1607
Epoch 4/300, trend Loss: 0.1252 | 0.1368
Epoch 5/300, trend Loss: 0.1172 | 0.1294
Epoch 6/300, trend Loss: 0.1124 | 0.1362
Epoch 7/300, trend Loss: 0.1084 | 0.1498
Epoch 8/300, trend Loss: 0.1072 | 0.1317
Epoch 9/300, trend Loss: 0.1104 | 0.0881
Epoch 10/300, trend Loss: 0.1096 | 0.0831
Epoch 11/300, trend Loss: 0.0979 | 0.0802
Epoch 12/300, trend Loss: 0.0942 | 0.0771
Epoch 13/300, trend Loss: 0.0914 | 0.0725
Epoch 14/300, trend Loss: 0.0888 | 0.0684
Epoch 15/300, trend Loss: 0.0867 | 0.0652
Epoch 16/300, trend Loss: 0.0849 | 0.0637
Epoch 17/300, trend Loss: 0.0834 | 0.0607
Epoch 18/300, trend Loss: 0.0826 | 0.0583
Epoch 19/300, trend Loss: 0.0817 | 0.0562
Epoch 20/300, trend Loss: 0.0806 | 0.0549
Epoch 21/300, trend Loss: 0.0794 | 0.0542
Epoch 22/300, trend Loss: 0.0783 | 0.0538
Epoch 23/300, trend Loss: 0.0776 | 0.0533
Epoch 24/300, trend Loss: 0.0783 | 0.0512
Epoch 25/300, trend Loss: 0.0794 | 0.0509
Epoch 26/300, trend Loss: 0.0787 | 0.0503
Epoch 27/300, trend Loss: 0.0782 | 0.0499
Epoch 28/300, trend Loss: 0.0781 | 0.0499
Epoch 29/300, trend Loss: 0.0782 | 0.0502
Epoch 30/300, trend Loss: 0.0783 | 0.0508
Epoch 31/300, trend Loss: 0.0782 | 0.0547
Epoch 32/300, trend Loss: 0.0782 | 0.0592
Epoch 33/300, trend Loss: 0.0774 | 0.0650
Epoch 34/300, trend Loss: 0.0769 | 0.0681
Epoch 35/300, trend Loss: 0.0772 | 0.0637
Epoch 36/300, trend Loss: 0.0791 | 0.0532
Epoch 37/300, trend Loss: 0.0835 | 0.0506
Epoch 38/300, trend Loss: 0.0879 | 0.0509
Epoch 39/300, trend Loss: 0.0747 | 0.0473
Epoch 40/300, trend Loss: 0.0739 | 0.0479
Epoch 41/300, trend Loss: 0.0753 | 0.0519
Epoch 42/300, trend Loss: 0.0768 | 0.0534
Epoch 43/300, trend Loss: 0.0763 | 0.0506
Epoch 44/300, trend Loss: 0.0742 | 0.0451
Epoch 45/300, trend Loss: 0.0740 | 0.0427
Epoch 46/300, trend Loss: 0.0766 | 0.0539
Epoch 47/300, trend Loss: 0.0809 | 0.0481
Epoch 48/300, trend Loss: 0.0776 | 0.0433
Epoch 49/300, trend Loss: 0.0723 | 0.0429
Epoch 50/300, trend Loss: 0.0711 | 0.0477
Epoch 51/300, trend Loss: 0.0700 | 0.0467
Epoch 52/300, trend Loss: 0.0685 | 0.0443
Epoch 53/300, trend Loss: 0.0674 | 0.0429
Epoch 54/300, trend Loss: 0.0665 | 0.0421
Epoch 55/300, trend Loss: 0.0660 | 0.0412
Epoch 56/300, trend Loss: 0.0657 | 0.0406
Epoch 57/300, trend Loss: 0.0656 | 0.0403
Epoch 58/300, trend Loss: 0.0657 | 0.0402
Epoch 59/300, trend Loss: 0.0658 | 0.0404
Epoch 60/300, trend Loss: 0.0658 | 0.0407
Epoch 61/300, trend Loss: 0.0657 | 0.0414
Epoch 62/300, trend Loss: 0.0653 | 0.0415
Epoch 63/300, trend Loss: 0.0645 | 0.0411
Epoch 64/300, trend Loss: 0.0638 | 0.0402
Epoch 65/300, trend Loss: 0.0634 | 0.0392
Epoch 66/300, trend Loss: 0.0633 | 0.0385
Epoch 67/300, trend Loss: 0.0636 | 0.0380
Epoch 68/300, trend Loss: 0.0641 | 0.0380
Epoch 69/300, trend Loss: 0.0649 | 0.0390
Epoch 70/300, trend Loss: 0.0655 | 0.0395
Epoch 71/300, trend Loss: 0.0648 | 0.0394
Epoch 72/300, trend Loss: 0.0639 | 0.0392
Epoch 73/300, trend Loss: 0.0633 | 0.0402
Epoch 74/300, trend Loss: 0.0630 | 0.0422
Epoch 75/300, trend Loss: 0.0629 | 0.0435
Epoch 76/300, trend Loss: 0.0626 | 0.0439
Epoch 77/300, trend Loss: 0.0628 | 0.0415
Epoch 78/300, trend Loss: 0.0637 | 0.0396
Epoch 79/300, trend Loss: 0.0660 | 0.0419
Epoch 80/300, trend Loss: 0.0675 | 0.0400
Epoch 81/300, trend Loss: 0.0621 | 0.0374
Epoch 82/300, trend Loss: 0.0616 | 0.0387
Epoch 83/300, trend Loss: 0.0624 | 0.0430
Epoch 84/300, trend Loss: 0.0618 | 0.0418
Epoch 85/300, trend Loss: 0.0604 | 0.0397
Epoch 86/300, trend Loss: 0.0592 | 0.0387
Epoch 87/300, trend Loss: 0.0587 | 0.0381
Epoch 88/300, trend Loss: 0.0584 | 0.0379
Epoch 89/300, trend Loss: 0.0581 | 0.0379
Epoch 90/300, trend Loss: 0.0579 | 0.0382
Epoch 91/300, trend Loss: 0.0578 | 0.0392
Epoch 92/300, trend Loss: 0.0579 | 0.0395
Epoch 93/300, trend Loss: 0.0580 | 0.0394
Epoch 94/300, trend Loss: 0.0581 | 0.0392
Epoch 95/300, trend Loss: 0.0581 | 0.0389
Epoch 96/300, trend Loss: 0.0579 | 0.0387
Epoch 97/300, trend Loss: 0.0576 | 0.0387
Epoch 98/300, trend Loss: 0.0573 | 0.0389
Epoch 99/300, trend Loss: 0.0569 | 0.0396
Epoch 100/300, trend Loss: 0.0567 | 0.0396
Epoch 101/300, trend Loss: 0.0564 | 0.0392
Epoch 102/300, trend Loss: 0.0564 | 0.0385
Epoch 103/300, trend Loss: 0.0565 | 0.0379
Epoch 104/300, trend Loss: 0.0568 | 0.0376
Epoch 105/300, trend Loss: 0.0574 | 0.0377
Epoch 106/300, trend Loss: 0.0582 | 0.0400
Epoch 107/300, trend Loss: 0.0592 | 0.0405
Epoch 108/300, trend Loss: 0.0581 | 0.0386
Epoch 109/300, trend Loss: 0.0582 | 0.0424
Epoch 110/300, trend Loss: 0.0575 | 0.0440
Epoch 111/300, trend Loss: 0.0559 | 0.0403
Epoch 112/300, trend Loss: 0.0551 | 0.0392
Epoch 113/300, trend Loss: 0.0549 | 0.0390
Epoch 114/300, trend Loss: 0.0551 | 0.0391
Epoch 115/300, trend Loss: 0.0553 | 0.0392
Epoch 116/300, trend Loss: 0.0548 | 0.0390
Epoch 117/300, trend Loss: 0.0541 | 0.0387
Epoch 118/300, trend Loss: 0.0535 | 0.0383
Epoch 119/300, trend Loss: 0.0531 | 0.0381
Epoch 120/300, trend Loss: 0.0529 | 0.0379
Epoch 121/300, trend Loss: 0.0528 | 0.0382
Epoch 122/300, trend Loss: 0.0527 | 0.0380
Epoch 123/300, trend Loss: 0.0527 | 0.0377
Epoch 124/300, trend Loss: 0.0526 | 0.0376
Epoch 125/300, trend Loss: 0.0525 | 0.0375
Epoch 126/300, trend Loss: 0.0523 | 0.0375
Epoch 127/300, trend Loss: 0.0521 | 0.0374
Epoch 128/300, trend Loss: 0.0518 | 0.0373
Epoch 129/300, trend Loss: 0.0516 | 0.0373
Epoch 130/300, trend Loss: 0.0514 | 0.0369
Epoch 131/300, trend Loss: 0.0512 | 0.0366
Epoch 132/300, trend Loss: 0.0511 | 0.0364
Epoch 133/300, trend Loss: 0.0510 | 0.0363
Epoch 134/300, trend Loss: 0.0510 | 0.0362
Epoch 135/300, trend Loss: 0.0509 | 0.0363
Epoch 136/300, trend Loss: 0.0508 | 0.0368
Epoch 137/300, trend Loss: 0.0507 | 0.0370
Epoch 138/300, trend Loss: 0.0506 | 0.0371
Epoch 139/300, trend Loss: 0.0504 | 0.0369
Epoch 140/300, trend Loss: 0.0502 | 0.0367
Epoch 141/300, trend Loss: 0.0501 | 0.0364
Epoch 142/300, trend Loss: 0.0500 | 0.0362
Epoch 143/300, trend Loss: 0.0499 | 0.0360
Epoch 144/300, trend Loss: 0.0498 | 0.0358
Epoch 145/300, trend Loss: 0.0498 | 0.0357
Epoch 146/300, trend Loss: 0.0500 | 0.0357
Epoch 147/300, trend Loss: 0.0501 | 0.0360
Epoch 148/300, trend Loss: 0.0502 | 0.0362
Epoch 149/300, trend Loss: 0.0502 | 0.0361
Epoch 150/300, trend Loss: 0.0499 | 0.0356
Epoch 151/300, trend Loss: 0.0499 | 0.0349
Epoch 152/300, trend Loss: 0.0503 | 0.0355
Epoch 153/300, trend Loss: 0.0511 | 0.0371
Epoch 154/300, trend Loss: 0.0507 | 0.0360
Epoch 155/300, trend Loss: 0.0514 | 0.0430
Epoch 156/300, trend Loss: 0.0505 | 0.0386
Epoch 157/300, trend Loss: 0.0495 | 0.0369
Epoch 158/300, trend Loss: 0.0492 | 0.0370
Epoch 159/300, trend Loss: 0.0492 | 0.0371
Epoch 160/300, trend Loss: 0.0493 | 0.0369
Epoch 161/300, trend Loss: 0.0494 | 0.0368
Epoch 162/300, trend Loss: 0.0496 | 0.0367
Epoch 163/300, trend Loss: 0.0496 | 0.0366
Epoch 164/300, trend Loss: 0.0495 | 0.0365
Epoch 165/300, trend Loss: 0.0493 | 0.0365
Epoch 166/300, trend Loss: 0.0492 | 0.0364
Epoch 167/300, trend Loss: 0.0490 | 0.0365
Epoch 168/300, trend Loss: 0.0488 | 0.0368
Epoch 169/300, trend Loss: 0.0486 | 0.0369
Epoch 170/300, trend Loss: 0.0483 | 0.0369
Epoch 171/300, trend Loss: 0.0480 | 0.0368
Epoch 172/300, trend Loss: 0.0477 | 0.0367
Epoch 173/300, trend Loss: 0.0476 | 0.0366
Epoch 174/300, trend Loss: 0.0475 | 0.0365
Epoch 175/300, trend Loss: 0.0473 | 0.0364
Epoch 176/300, trend Loss: 0.0472 | 0.0362
Epoch 177/300, trend Loss: 0.0470 | 0.0362
Epoch 178/300, trend Loss: 0.0468 | 0.0361
Epoch 179/300, trend Loss: 0.0466 | 0.0360
Epoch 180/300, trend Loss: 0.0465 | 0.0358
Epoch 181/300, trend Loss: 0.0464 | 0.0354
Epoch 182/300, trend Loss: 0.0463 | 0.0353
Epoch 183/300, trend Loss: 0.0461 | 0.0353
Epoch 184/300, trend Loss: 0.0458 | 0.0353
Epoch 185/300, trend Loss: 0.0456 | 0.0353
Epoch 186/300, trend Loss: 0.0453 | 0.0354
Epoch 187/300, trend Loss: 0.0451 | 0.0355
Epoch 188/300, trend Loss: 0.0450 | 0.0357
Epoch 189/300, trend Loss: 0.0449 | 0.0358
Epoch 190/300, trend Loss: 0.0448 | 0.0361
Epoch 191/300, trend Loss: 0.0446 | 0.0364
Epoch 192/300, trend Loss: 0.0444 | 0.0367
Epoch 193/300, trend Loss: 0.0441 | 0.0371
Epoch 194/300, trend Loss: 0.0437 | 0.0374
Epoch 195/300, trend Loss: 0.0432 | 0.0375
Epoch 196/300, trend Loss: 0.0428 | 0.0374
Epoch 197/300, trend Loss: 0.0425 | 0.0371
Epoch 198/300, trend Loss: 0.0424 | 0.0367
Epoch 199/300, trend Loss: 0.0425 | 0.0363
Epoch 200/300, trend Loss: 0.0426 | 0.0359
Epoch 201/300, trend Loss: 0.0426 | 0.0357
Epoch 202/300, trend Loss: 0.0422 | 0.0356
Epoch 203/300, trend Loss: 0.0415 | 0.0355
Epoch 204/300, trend Loss: 0.0408 | 0.0353
Epoch 205/300, trend Loss: 0.0404 | 0.0353
Epoch 206/300, trend Loss: 0.0405 | 0.0354
Epoch 207/300, trend Loss: 0.0410 | 0.0354
Epoch 208/300, trend Loss: 0.0419 | 0.0360
Epoch 209/300, trend Loss: 0.0433 | 0.0395
Epoch 210/300, trend Loss: 0.0444 | 0.0474
Epoch 211/300, trend Loss: 0.0436 | 0.0499
Epoch 212/300, trend Loss: 0.0414 | 0.0379
Epoch 213/300, trend Loss: 0.0410 | 0.0353
Epoch 214/300, trend Loss: 0.0430 | 0.0376
Epoch 215/300, trend Loss: 0.0440 | 0.0363
Epoch 216/300, trend Loss: 0.0425 | 0.0354
Epoch 217/300, trend Loss: 0.0399 | 0.0358
Epoch 218/300, trend Loss: 0.0383 | 0.0358
Epoch 219/300, trend Loss: 0.0380 | 0.0356
Epoch 220/300, trend Loss: 0.0381 | 0.0356
Epoch 221/300, trend Loss: 0.0382 | 0.0355
Epoch 222/300, trend Loss: 0.0380 | 0.0353
Epoch 223/300, trend Loss: 0.0377 | 0.0351
Epoch 224/300, trend Loss: 0.0375 | 0.0350
Epoch 225/300, trend Loss: 0.0373 | 0.0350
Epoch 226/300, trend Loss: 0.0372 | 0.0350
Epoch 227/300, trend Loss: 0.0371 | 0.0351
Epoch 228/300, trend Loss: 0.0370 | 0.0351
Epoch 229/300, trend Loss: 0.0370 | 0.0352
Epoch 230/300, trend Loss: 0.0369 | 0.0352
Epoch 231/300, trend Loss: 0.0368 | 0.0353
Epoch 232/300, trend Loss: 0.0368 | 0.0353
Epoch 233/300, trend Loss: 0.0367 | 0.0353
Epoch 234/300, trend Loss: 0.0366 | 0.0353
Epoch 235/300, trend Loss: 0.0366 | 0.0353
Epoch 236/300, trend Loss: 0.0365 | 0.0353
Epoch 237/300, trend Loss: 0.0365 | 0.0353
Epoch 238/300, trend Loss: 0.0364 | 0.0353
Epoch 239/300, trend Loss: 0.0364 | 0.0353
Epoch 240/300, trend Loss: 0.0363 | 0.0352
Epoch 241/300, trend Loss: 0.0363 | 0.0352
Epoch 242/300, trend Loss: 0.0362 | 0.0352
Epoch 243/300, trend Loss: 0.0362 | 0.0351
Epoch 244/300, trend Loss: 0.0361 | 0.0351
Epoch 245/300, trend Loss: 0.0361 | 0.0351
Epoch 246/300, trend Loss: 0.0360 | 0.0351
Epoch 247/300, trend Loss: 0.0360 | 0.0351
Epoch 248/300, trend Loss: 0.0360 | 0.0351
Epoch 249/300, trend Loss: 0.0359 | 0.0352
Epoch 250/300, trend Loss: 0.0359 | 0.0351
Epoch 251/300, trend Loss: 0.0359 | 0.0353
Epoch 252/300, trend Loss: 0.0359 | 0.0352
Epoch 253/300, trend Loss: 0.0358 | 0.0353
Epoch 254/300, trend Loss: 0.0358 | 0.0351
Epoch 255/300, trend Loss: 0.0357 | 0.0352
Epoch 256/300, trend Loss: 0.0357 | 0.0351
Epoch 257/300, trend Loss: 0.0356 | 0.0352
Epoch 258/300, trend Loss: 0.0356 | 0.0351
Epoch 259/300, trend Loss: 0.0355 | 0.0351
Epoch 260/300, trend Loss: 0.0355 | 0.0351
Epoch 261/300, trend Loss: 0.0355 | 0.0351
Epoch 262/300, trend Loss: 0.0354 | 0.0351
Epoch 263/300, trend Loss: 0.0354 | 0.0351
Epoch 264/300, trend Loss: 0.0354 | 0.0351
Epoch 265/300, trend Loss: 0.0353 | 0.0351
Epoch 266/300, trend Loss: 0.0353 | 0.0351
Epoch 267/300, trend Loss: 0.0353 | 0.0351
Epoch 268/300, trend Loss: 0.0352 | 0.0351
Epoch 269/300, trend Loss: 0.0352 | 0.0351
Epoch 270/300, trend Loss: 0.0352 | 0.0351
Epoch 271/300, trend Loss: 0.0351 | 0.0351
Epoch 272/300, trend Loss: 0.0351 | 0.0351
Epoch 273/300, trend Loss: 0.0351 | 0.0351
Epoch 274/300, trend Loss: 0.0350 | 0.0351
Epoch 275/300, trend Loss: 0.0350 | 0.0351
Epoch 276/300, trend Loss: 0.0350 | 0.0351
Epoch 277/300, trend Loss: 0.0350 | 0.0351
Epoch 278/300, trend Loss: 0.0349 | 0.0351
Epoch 279/300, trend Loss: 0.0349 | 0.0351
Epoch 280/300, trend Loss: 0.0349 | 0.0351
Epoch 281/300, trend Loss: 0.0348 | 0.0351
Epoch 282/300, trend Loss: 0.0348 | 0.0350
Epoch 283/300, trend Loss: 0.0348 | 0.0350
Epoch 284/300, trend Loss: 0.0348 | 0.0350
Epoch 285/300, trend Loss: 0.0347 | 0.0350
Epoch 286/300, trend Loss: 0.0347 | 0.0350
Epoch 287/300, trend Loss: 0.0347 | 0.0350
Epoch 288/300, trend Loss: 0.0347 | 0.0350
Epoch 289/300, trend Loss: 0.0346 | 0.0349
Epoch 290/300, trend Loss: 0.0346 | 0.0349
Epoch 291/300, trend Loss: 0.0346 | 0.0349
Epoch 292/300, trend Loss: 0.0346 | 0.0349
Epoch 293/300, trend Loss: 0.0345 | 0.0349
Epoch 294/300, trend Loss: 0.0345 | 0.0350
Epoch 295/300, trend Loss: 0.0345 | 0.0350
Epoch 296/300, trend Loss: 0.0345 | 0.0350
Epoch 297/300, trend Loss: 0.0344 | 0.0351
Epoch 298/300, trend Loss: 0.0344 | 0.0352
Epoch 299/300, trend Loss: 0.0344 | 0.0352
Epoch 300/300, trend Loss: 0.0345 | 0.0353
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.9826434517870474, 'learning_rate': 0.00016457774355828589, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9572259920018774}
Epoch 1/300, seasonal_0 Loss: 0.1513 | 0.0938
Epoch 2/300, seasonal_0 Loss: 0.1160 | 0.0756
Epoch 3/300, seasonal_0 Loss: 0.1085 | 0.0767
Epoch 4/300, seasonal_0 Loss: 0.1027 | 0.0780
Epoch 5/300, seasonal_0 Loss: 0.0969 | 0.0648
Epoch 6/300, seasonal_0 Loss: 0.0935 | 0.0663
Epoch 7/300, seasonal_0 Loss: 0.0922 | 0.0683
Epoch 8/300, seasonal_0 Loss: 0.0874 | 0.0657
Epoch 9/300, seasonal_0 Loss: 0.0842 | 0.0630
Epoch 10/300, seasonal_0 Loss: 0.0810 | 0.0580
Epoch 11/300, seasonal_0 Loss: 0.0784 | 0.0547
Epoch 12/300, seasonal_0 Loss: 0.0763 | 0.0526
Epoch 13/300, seasonal_0 Loss: 0.0745 | 0.0513
Epoch 14/300, seasonal_0 Loss: 0.0729 | 0.0511
Epoch 15/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 16/300, seasonal_0 Loss: 0.0705 | 0.0502
Epoch 17/300, seasonal_0 Loss: 0.0697 | 0.0479
Epoch 18/300, seasonal_0 Loss: 0.0694 | 0.0444
Epoch 19/300, seasonal_0 Loss: 0.0705 | 0.0443
Epoch 20/300, seasonal_0 Loss: 0.0687 | 0.0537
Epoch 21/300, seasonal_0 Loss: 0.0687 | 0.0683
Epoch 22/300, seasonal_0 Loss: 0.0680 | 0.0626
Epoch 23/300, seasonal_0 Loss: 0.0664 | 0.0517
Epoch 24/300, seasonal_0 Loss: 0.0646 | 0.0471
Epoch 25/300, seasonal_0 Loss: 0.0641 | 0.0458
Epoch 26/300, seasonal_0 Loss: 0.0632 | 0.0452
Epoch 27/300, seasonal_0 Loss: 0.0624 | 0.0447
Epoch 28/300, seasonal_0 Loss: 0.0622 | 0.0474
Epoch 29/300, seasonal_0 Loss: 0.0616 | 0.0490
Epoch 30/300, seasonal_0 Loss: 0.0619 | 0.0550
Epoch 31/300, seasonal_0 Loss: 0.0614 | 0.0533
Epoch 32/300, seasonal_0 Loss: 0.0606 | 0.0468
Epoch 33/300, seasonal_0 Loss: 0.0606 | 0.0446
Epoch 34/300, seasonal_0 Loss: 0.0595 | 0.0399
Epoch 35/300, seasonal_0 Loss: 0.0597 | 0.0404
Epoch 36/300, seasonal_0 Loss: 0.0586 | 0.0360
Epoch 37/300, seasonal_0 Loss: 0.0595 | 0.0438
Epoch 38/300, seasonal_0 Loss: 0.0588 | 0.0423
Epoch 39/300, seasonal_0 Loss: 0.0592 | 0.0526
Epoch 40/300, seasonal_0 Loss: 0.0584 | 0.0478
Epoch 41/300, seasonal_0 Loss: 0.0579 | 0.0452
Epoch 42/300, seasonal_0 Loss: 0.0574 | 0.0406
Epoch 43/300, seasonal_0 Loss: 0.0565 | 0.0396
Epoch 44/300, seasonal_0 Loss: 0.0569 | 0.0387
Epoch 45/300, seasonal_0 Loss: 0.0565 | 0.0357
Epoch 46/300, seasonal_0 Loss: 0.0559 | 0.0374
Epoch 47/300, seasonal_0 Loss: 0.0552 | 0.0380
Epoch 48/300, seasonal_0 Loss: 0.0552 | 0.0396
Epoch 49/300, seasonal_0 Loss: 0.0556 | 0.0412
Epoch 50/300, seasonal_0 Loss: 0.0544 | 0.0438
Epoch 51/300, seasonal_0 Loss: 0.0544 | 0.0454
Epoch 52/300, seasonal_0 Loss: 0.0541 | 0.0453
Epoch 53/300, seasonal_0 Loss: 0.0536 | 0.0436
Epoch 54/300, seasonal_0 Loss: 0.0532 | 0.0449
Epoch 55/300, seasonal_0 Loss: 0.0528 | 0.0420
Epoch 56/300, seasonal_0 Loss: 0.0524 | 0.0389
Epoch 57/300, seasonal_0 Loss: 0.0520 | 0.0400
Epoch 58/300, seasonal_0 Loss: 0.0532 | 0.0393
Epoch 59/300, seasonal_0 Loss: 0.0524 | 0.0396
Epoch 60/300, seasonal_0 Loss: 0.0515 | 0.0414
Epoch 61/300, seasonal_0 Loss: 0.0511 | 0.0422
Epoch 62/300, seasonal_0 Loss: 0.0507 | 0.0408
Epoch 63/300, seasonal_0 Loss: 0.0506 | 0.0391
Epoch 64/300, seasonal_0 Loss: 0.0502 | 0.0386
Epoch 65/300, seasonal_0 Loss: 0.0500 | 0.0466
Epoch 66/300, seasonal_0 Loss: 0.0504 | 0.0467
Epoch 67/300, seasonal_0 Loss: 0.0501 | 0.0455
Epoch 68/300, seasonal_0 Loss: 0.0496 | 0.0453
Epoch 69/300, seasonal_0 Loss: 0.0493 | 0.0480
Epoch 70/300, seasonal_0 Loss: 0.0492 | 0.0492
Epoch 71/300, seasonal_0 Loss: 0.0494 | 0.0483
Epoch 72/300, seasonal_0 Loss: 0.0507 | 0.0456
Epoch 73/300, seasonal_0 Loss: 0.0495 | 0.0338
Epoch 74/300, seasonal_0 Loss: 0.0481 | 0.0487
Epoch 75/300, seasonal_0 Loss: 0.0465 | 0.0439
Epoch 76/300, seasonal_0 Loss: 0.0499 | 0.0411
Epoch 77/300, seasonal_0 Loss: 0.0541 | 0.0356
Epoch 78/300, seasonal_0 Loss: 0.0484 | 0.0359
Epoch 79/300, seasonal_0 Loss: 0.0472 | 0.0391
Epoch 80/300, seasonal_0 Loss: 0.0460 | 0.0448
Epoch 81/300, seasonal_0 Loss: 0.0439 | 0.0721
Epoch 82/300, seasonal_0 Loss: 0.0427 | 0.0319
Epoch 83/300, seasonal_0 Loss: 0.0463 | 0.0615
Epoch 84/300, seasonal_0 Loss: 0.0416 | 0.0506
Epoch 85/300, seasonal_0 Loss: 0.0406 | 0.0327
Epoch 86/300, seasonal_0 Loss: 0.0454 | 0.0467
Epoch 87/300, seasonal_0 Loss: 0.0410 | 0.0826
Epoch 88/300, seasonal_0 Loss: 0.0406 | 0.0745
Epoch 89/300, seasonal_0 Loss: 0.0389 | 0.0331
Epoch 90/300, seasonal_0 Loss: 0.0380 | 0.0398
Epoch 91/300, seasonal_0 Loss: 0.0369 | 0.0336
Epoch 92/300, seasonal_0 Loss: 0.0369 | 0.0515
Epoch 93/300, seasonal_0 Loss: 0.0362 | 0.0337
Epoch 94/300, seasonal_0 Loss: 0.0359 | 0.0411
Epoch 95/300, seasonal_0 Loss: 0.0353 | 0.0370
Epoch 96/300, seasonal_0 Loss: 0.0351 | 0.0396
Epoch 97/300, seasonal_0 Loss: 0.0345 | 0.0367
Epoch 98/300, seasonal_0 Loss: 0.0344 | 0.0391
Epoch 99/300, seasonal_0 Loss: 0.0340 | 0.0381
Epoch 100/300, seasonal_0 Loss: 0.0340 | 0.0393
Epoch 101/300, seasonal_0 Loss: 0.0333 | 0.0375
Epoch 102/300, seasonal_0 Loss: 0.0330 | 0.0457
Epoch 103/300, seasonal_0 Loss: 0.0328 | 0.0393
Epoch 104/300, seasonal_0 Loss: 0.0325 | 0.0556
Epoch 105/300, seasonal_0 Loss: 0.0324 | 0.0410
Epoch 106/300, seasonal_0 Loss: 0.0323 | 0.0507
Epoch 107/300, seasonal_0 Loss: 0.0319 | 0.0429
Epoch 108/300, seasonal_0 Loss: 0.0317 | 0.0521
Epoch 109/300, seasonal_0 Loss: 0.0319 | 0.0447
Epoch 110/300, seasonal_0 Loss: 0.0311 | 0.0514
Epoch 111/300, seasonal_0 Loss: 0.0311 | 0.0570
Epoch 112/300, seasonal_0 Loss: 0.0312 | 0.0553
Epoch 113/300, seasonal_0 Loss: 0.0306 | 0.0633
Epoch 114/300, seasonal_0 Loss: 0.0308 | 0.0695
Epoch 115/300, seasonal_0 Loss: 0.0300 | 0.0803
Epoch 116/300, seasonal_0 Loss: 0.0302 | 0.0678
Epoch 117/300, seasonal_0 Loss: 0.0301 | 0.0747
Epoch 118/300, seasonal_0 Loss: 0.0298 | 0.0546
Epoch 119/300, seasonal_0 Loss: 0.0301 | 0.0699
Epoch 120/300, seasonal_0 Loss: 0.0295 | 0.0464
Epoch 121/300, seasonal_0 Loss: 0.0291 | 0.0601
Epoch 122/300, seasonal_0 Loss: 0.0289 | 0.0488
Epoch 123/300, seasonal_0 Loss: 0.0294 | 0.0494
Epoch 124/300, seasonal_0 Loss: 0.0287 | 0.0451
Epoch 125/300, seasonal_0 Loss: 0.0292 | 0.0527
Epoch 126/300, seasonal_0 Loss: 0.0283 | 0.0434
Epoch 127/300, seasonal_0 Loss: 0.0280 | 0.0481
Epoch 128/300, seasonal_0 Loss: 0.0276 | 0.0433
Epoch 129/300, seasonal_0 Loss: 0.0271 | 0.0472
Epoch 130/300, seasonal_0 Loss: 0.0270 | 0.0420
Epoch 131/300, seasonal_0 Loss: 0.0266 | 0.0435
Epoch 132/300, seasonal_0 Loss: 0.0265 | 0.0405
Epoch 133/300, seasonal_0 Loss: 0.0265 | 0.0432
Epoch 134/300, seasonal_0 Loss: 0.0263 | 0.0421
Epoch 135/300, seasonal_0 Loss: 0.0273 | 0.0416
Epoch 136/300, seasonal_0 Loss: 0.0263 | 0.0406
Epoch 137/300, seasonal_0 Loss: 0.0259 | 0.0428
Epoch 138/300, seasonal_0 Loss: 0.0258 | 0.0405
Epoch 139/300, seasonal_0 Loss: 0.0258 | 0.0491
Epoch 140/300, seasonal_0 Loss: 0.0256 | 0.0459
Epoch 141/300, seasonal_0 Loss: 0.0258 | 0.0459
Epoch 142/300, seasonal_0 Loss: 0.0257 | 0.0447
Epoch 143/300, seasonal_0 Loss: 0.0256 | 0.0493
Epoch 144/300, seasonal_0 Loss: 0.0256 | 0.0399
Epoch 145/300, seasonal_0 Loss: 0.0261 | 0.0448
Epoch 146/300, seasonal_0 Loss: 0.0274 | 0.0537
Epoch 147/300, seasonal_0 Loss: 0.0277 | 0.1137
Epoch 148/300, seasonal_0 Loss: 0.0270 | 0.0681
Epoch 149/300, seasonal_0 Loss: 0.0300 | 0.0405
Epoch 150/300, seasonal_0 Loss: 0.0279 | 0.0436
Epoch 151/300, seasonal_0 Loss: 0.0306 | 0.0391
Epoch 152/300, seasonal_0 Loss: 0.0267 | 0.0399
Epoch 153/300, seasonal_0 Loss: 0.0250 | 0.0414
Epoch 154/300, seasonal_0 Loss: 0.0242 | 0.0384
Epoch 155/300, seasonal_0 Loss: 0.0237 | 0.0388
Epoch 156/300, seasonal_0 Loss: 0.0234 | 0.0377
Epoch 157/300, seasonal_0 Loss: 0.0231 | 0.0382
Epoch 158/300, seasonal_0 Loss: 0.0230 | 0.0380
Epoch 159/300, seasonal_0 Loss: 0.0228 | 0.0379
Epoch 160/300, seasonal_0 Loss: 0.0227 | 0.0379
Epoch 161/300, seasonal_0 Loss: 0.0226 | 0.0385
Epoch 162/300, seasonal_0 Loss: 0.0225 | 0.0378
Epoch 163/300, seasonal_0 Loss: 0.0224 | 0.0383
Epoch 164/300, seasonal_0 Loss: 0.0222 | 0.0380
Epoch 165/300, seasonal_0 Loss: 0.0221 | 0.0375
Epoch 166/300, seasonal_0 Loss: 0.0220 | 0.0386
Epoch 167/300, seasonal_0 Loss: 0.0219 | 0.0374
Epoch 168/300, seasonal_0 Loss: 0.0218 | 0.0386
Epoch 169/300, seasonal_0 Loss: 0.0218 | 0.0384
Epoch 170/300, seasonal_0 Loss: 0.0217 | 0.0387
Epoch 171/300, seasonal_0 Loss: 0.0217 | 0.0394
Epoch 172/300, seasonal_0 Loss: 0.0216 | 0.0383
Epoch 173/300, seasonal_0 Loss: 0.0215 | 0.0404
Epoch 174/300, seasonal_0 Loss: 0.0214 | 0.0408
Epoch 175/300, seasonal_0 Loss: 0.0212 | 0.0385
Epoch 176/300, seasonal_0 Loss: 0.0211 | 0.0404
Epoch 177/300, seasonal_0 Loss: 0.0210 | 0.0405
Epoch 178/300, seasonal_0 Loss: 0.0211 | 0.0419
Epoch 179/300, seasonal_0 Loss: 0.0209 | 0.0415
Epoch 180/300, seasonal_0 Loss: 0.0206 | 0.0400
Epoch 181/300, seasonal_0 Loss: 0.0206 | 0.0393
Epoch 182/300, seasonal_0 Loss: 0.0205 | 0.0427
Epoch 183/300, seasonal_0 Loss: 0.0206 | 0.0400
Epoch 184/300, seasonal_0 Loss: 0.0209 | 0.0431
Epoch 185/300, seasonal_0 Loss: 0.0206 | 0.0433
Epoch 186/300, seasonal_0 Loss: 0.0207 | 0.0483
Epoch 187/300, seasonal_0 Loss: 0.0205 | 0.0433
Epoch 188/300, seasonal_0 Loss: 0.0203 | 0.0458
Epoch 189/300, seasonal_0 Loss: 0.0203 | 0.0435
Epoch 190/300, seasonal_0 Loss: 0.0206 | 0.0424
Epoch 191/300, seasonal_0 Loss: 0.0215 | 0.0468
Epoch 192/300, seasonal_0 Loss: 0.0230 | 0.0957
Epoch 193/300, seasonal_0 Loss: 0.0227 | 0.0969
Epoch 194/300, seasonal_0 Loss: 0.0228 | 0.0603
Epoch 195/300, seasonal_0 Loss: 0.0209 | 0.0446
Epoch 196/300, seasonal_0 Loss: 0.0216 | 0.0428
Epoch 197/300, seasonal_0 Loss: 0.0204 | 0.0440
Epoch 198/300, seasonal_0 Loss: 0.0207 | 0.0425
Epoch 199/300, seasonal_0 Loss: 0.0199 | 0.0431
Epoch 200/300, seasonal_0 Loss: 0.0196 | 0.0415
Epoch 201/300, seasonal_0 Loss: 0.0193 | 0.0432
Epoch 202/300, seasonal_0 Loss: 0.0191 | 0.0420
Epoch 203/300, seasonal_0 Loss: 0.0190 | 0.0439
Epoch 204/300, seasonal_0 Loss: 0.0189 | 0.0442
Epoch 205/300, seasonal_0 Loss: 0.0190 | 0.0460
Epoch 206/300, seasonal_0 Loss: 0.0190 | 0.0459
Epoch 207/300, seasonal_0 Loss: 0.0187 | 0.0454
Epoch 208/300, seasonal_0 Loss: 0.0186 | 0.0450
Epoch 209/300, seasonal_0 Loss: 0.0185 | 0.0454
Epoch 210/300, seasonal_0 Loss: 0.0184 | 0.0466
Epoch 211/300, seasonal_0 Loss: 0.0184 | 0.0475
Epoch 212/300, seasonal_0 Loss: 0.0183 | 0.0466
Epoch 213/300, seasonal_0 Loss: 0.0182 | 0.0477
Epoch 214/300, seasonal_0 Loss: 0.0183 | 0.0488
Epoch 215/300, seasonal_0 Loss: 0.0183 | 0.0471
Epoch 216/300, seasonal_0 Loss: 0.0188 | 0.0518
Epoch 217/300, seasonal_0 Loss: 0.0190 | 0.0510
Epoch 218/300, seasonal_0 Loss: 0.0185 | 0.0490
Epoch 219/300, seasonal_0 Loss: 0.0182 | 0.0495
Epoch 220/300, seasonal_0 Loss: 0.0179 | 0.0528
Epoch 221/300, seasonal_0 Loss: 0.0178 | 0.0515
Epoch 222/300, seasonal_0 Loss: 0.0177 | 0.0524
Epoch 223/300, seasonal_0 Loss: 0.0177 | 0.0506
Epoch 224/300, seasonal_0 Loss: 0.0175 | 0.0522
Epoch 225/300, seasonal_0 Loss: 0.0174 | 0.0493
Epoch 226/300, seasonal_0 Loss: 0.0173 | 0.0479
Epoch 227/300, seasonal_0 Loss: 0.0172 | 0.0484
Epoch 228/300, seasonal_0 Loss: 0.0171 | 0.0470
Epoch 229/300, seasonal_0 Loss: 0.0203 | 0.0593
Epoch 230/300, seasonal_0 Loss: 0.0237 | 0.0551
Epoch 231/300, seasonal_0 Loss: 0.0345 | 0.0609
Epoch 232/300, seasonal_0 Loss: 0.0206 | 0.0497
Epoch 233/300, seasonal_0 Loss: 0.0187 | 0.0432
Epoch 234/300, seasonal_0 Loss: 0.0180 | 0.0398
Epoch 235/300, seasonal_0 Loss: 0.0177 | 0.0401
Epoch 236/300, seasonal_0 Loss: 0.0176 | 0.0412
Epoch 237/300, seasonal_0 Loss: 0.0174 | 0.0416
Epoch 238/300, seasonal_0 Loss: 0.0172 | 0.0413
Epoch 239/300, seasonal_0 Loss: 0.0171 | 0.0417
Epoch 240/300, seasonal_0 Loss: 0.0169 | 0.0449
Epoch 241/300, seasonal_0 Loss: 0.0168 | 0.0494
Epoch 242/300, seasonal_0 Loss: 0.0167 | 0.0491
Epoch 243/300, seasonal_0 Loss: 0.0166 | 0.0455
Epoch 244/300, seasonal_0 Loss: 0.0165 | 0.0430
Epoch 245/300, seasonal_0 Loss: 0.0165 | 0.0423
Epoch 246/300, seasonal_0 Loss: 0.0164 | 0.0428
Epoch 247/300, seasonal_0 Loss: 0.0164 | 0.0440
Epoch 248/300, seasonal_0 Loss: 0.0164 | 0.0459
Epoch 249/300, seasonal_0 Loss: 0.0164 | 0.0504
Epoch 250/300, seasonal_0 Loss: 0.0163 | 0.0519
Epoch 251/300, seasonal_0 Loss: 0.0164 | 0.0468
Epoch 252/300, seasonal_0 Loss: 0.0168 | 0.0442
Epoch 253/300, seasonal_0 Loss: 0.0169 | 0.0642
Epoch 254/300, seasonal_0 Loss: 0.0169 | 0.0611
Epoch 255/300, seasonal_0 Loss: 0.0165 | 0.0422
Epoch 256/300, seasonal_0 Loss: 0.0166 | 0.0440
Epoch 257/300, seasonal_0 Loss: 0.0164 | 0.0480
Epoch 258/300, seasonal_0 Loss: 0.0164 | 0.0457
Epoch 259/300, seasonal_0 Loss: 0.0164 | 0.0439
Epoch 260/300, seasonal_0 Loss: 0.0161 | 0.0425
Epoch 261/300, seasonal_0 Loss: 0.0159 | 0.0428
Epoch 262/300, seasonal_0 Loss: 0.0162 | 0.0421
Epoch 263/300, seasonal_0 Loss: 0.0159 | 0.0425
Epoch 264/300, seasonal_0 Loss: 0.0158 | 0.0449
Epoch 265/300, seasonal_0 Loss: 0.0157 | 0.0463
Epoch 266/300, seasonal_0 Loss: 0.0156 | 0.0444
Epoch 267/300, seasonal_0 Loss: 0.0157 | 0.0433
Epoch 268/300, seasonal_0 Loss: 0.0159 | 0.0411
Epoch 269/300, seasonal_0 Loss: 0.0159 | 0.0422
Epoch 270/300, seasonal_0 Loss: 0.0159 | 0.0423
Epoch 271/300, seasonal_0 Loss: 0.0162 | 0.0418
Epoch 272/300, seasonal_0 Loss: 0.0159 | 0.0429
Epoch 273/300, seasonal_0 Loss: 0.0157 | 0.0419
Epoch 274/300, seasonal_0 Loss: 0.0156 | 0.0440
Epoch 275/300, seasonal_0 Loss: 0.0158 | 0.0449
Epoch 276/300, seasonal_0 Loss: 0.0155 | 0.0433
Epoch 277/300, seasonal_0 Loss: 0.0154 | 0.0439
Epoch 278/300, seasonal_0 Loss: 0.0153 | 0.0432
Epoch 279/300, seasonal_0 Loss: 0.0151 | 0.0450
Epoch 280/300, seasonal_0 Loss: 0.0150 | 0.0455
Epoch 281/300, seasonal_0 Loss: 0.0151 | 0.0452
Epoch 282/300, seasonal_0 Loss: 0.0150 | 0.0439
Epoch 283/300, seasonal_0 Loss: 0.0149 | 0.0449
Epoch 284/300, seasonal_0 Loss: 0.0155 | 0.0421
Epoch 285/300, seasonal_0 Loss: 0.0160 | 0.0428
Epoch 286/300, seasonal_0 Loss: 0.0157 | 0.0428
Epoch 287/300, seasonal_0 Loss: 0.0153 | 0.0424
Epoch 288/300, seasonal_0 Loss: 0.0151 | 0.0432
Epoch 289/300, seasonal_0 Loss: 0.0149 | 0.0439
Epoch 290/300, seasonal_0 Loss: 0.0148 | 0.0441
Epoch 291/300, seasonal_0 Loss: 0.0146 | 0.0447
Epoch 292/300, seasonal_0 Loss: 0.0144 | 0.0450
Epoch 293/300, seasonal_0 Loss: 0.0144 | 0.0454
Epoch 294/300, seasonal_0 Loss: 0.0144 | 0.0455
Epoch 295/300, seasonal_0 Loss: 0.0146 | 0.0455
Epoch 296/300, seasonal_0 Loss: 0.0145 | 0.0442
Epoch 297/300, seasonal_0 Loss: 0.0145 | 0.0454
Epoch 298/300, seasonal_0 Loss: 0.0144 | 0.0444
Epoch 299/300, seasonal_0 Loss: 0.0144 | 0.0450
Epoch 300/300, seasonal_0 Loss: 0.0143 | 0.0442
Training seasonal_1 component with params: {'observation_period_num': 18, 'train_rates': 0.891390838428539, 'learning_rate': 0.00022238016286435283, 'batch_size': 141, 'step_size': 5, 'gamma': 0.9615407837241159}
Epoch 1/300, seasonal_1 Loss: 0.4940 | 0.3080
Epoch 2/300, seasonal_1 Loss: 0.2414 | 0.4354
Epoch 3/300, seasonal_1 Loss: 0.2888 | 0.4363
Epoch 4/300, seasonal_1 Loss: 0.2258 | 0.2076
Epoch 5/300, seasonal_1 Loss: 0.1922 | 0.1761
Epoch 6/300, seasonal_1 Loss: 0.2209 | 0.1690
Epoch 7/300, seasonal_1 Loss: 0.1807 | 0.1537
Epoch 8/300, seasonal_1 Loss: 0.1360 | 0.1418
Epoch 9/300, seasonal_1 Loss: 0.1262 | 0.1250
Epoch 10/300, seasonal_1 Loss: 0.1295 | 0.1105
Epoch 11/300, seasonal_1 Loss: 0.1208 | 0.0963
Epoch 12/300, seasonal_1 Loss: 0.1174 | 0.0921
Epoch 13/300, seasonal_1 Loss: 0.1216 | 0.1009
Epoch 14/300, seasonal_1 Loss: 0.1211 | 0.1007
Epoch 15/300, seasonal_1 Loss: 0.1158 | 0.0884
Epoch 16/300, seasonal_1 Loss: 0.1095 | 0.0937
Epoch 17/300, seasonal_1 Loss: 0.1158 | 0.1244
Epoch 18/300, seasonal_1 Loss: 0.1279 | 0.2071
Epoch 19/300, seasonal_1 Loss: 0.1261 | 0.1587
Epoch 20/300, seasonal_1 Loss: 0.1139 | 0.0816
Epoch 21/300, seasonal_1 Loss: 0.1170 | 0.0962
Epoch 22/300, seasonal_1 Loss: 0.1273 | 0.0853
Epoch 23/300, seasonal_1 Loss: 0.1136 | 0.0910
Epoch 24/300, seasonal_1 Loss: 0.1054 | 0.0921
Epoch 25/300, seasonal_1 Loss: 0.1001 | 0.0769
Epoch 26/300, seasonal_1 Loss: 0.1070 | 0.0776
Epoch 27/300, seasonal_1 Loss: 0.1050 | 0.0781
Epoch 28/300, seasonal_1 Loss: 0.0966 | 0.0675
Epoch 29/300, seasonal_1 Loss: 0.0957 | 0.0674
Epoch 30/300, seasonal_1 Loss: 0.0935 | 0.0665
Epoch 31/300, seasonal_1 Loss: 0.0926 | 0.0656
Epoch 32/300, seasonal_1 Loss: 0.0925 | 0.0645
Epoch 33/300, seasonal_1 Loss: 0.0910 | 0.0632
Epoch 34/300, seasonal_1 Loss: 0.0897 | 0.0624
Epoch 35/300, seasonal_1 Loss: 0.0892 | 0.0615
Epoch 36/300, seasonal_1 Loss: 0.0887 | 0.0604
Epoch 37/300, seasonal_1 Loss: 0.0883 | 0.0602
Epoch 38/300, seasonal_1 Loss: 0.0886 | 0.0620
Epoch 39/300, seasonal_1 Loss: 0.0897 | 0.0685
Epoch 40/300, seasonal_1 Loss: 0.0909 | 0.0697
Epoch 41/300, seasonal_1 Loss: 0.0887 | 0.0603
Epoch 42/300, seasonal_1 Loss: 0.0876 | 0.0609
Epoch 43/300, seasonal_1 Loss: 0.0914 | 0.1024
Epoch 44/300, seasonal_1 Loss: 0.0965 | 0.1258
Epoch 45/300, seasonal_1 Loss: 0.0964 | 0.0671
Epoch 46/300, seasonal_1 Loss: 0.0866 | 0.0560
Epoch 47/300, seasonal_1 Loss: 0.0885 | 0.0647
Epoch 48/300, seasonal_1 Loss: 0.0913 | 0.0614
Epoch 49/300, seasonal_1 Loss: 0.0935 | 0.0574
Epoch 50/300, seasonal_1 Loss: 0.0932 | 0.0618
Epoch 51/300, seasonal_1 Loss: 0.0878 | 0.0707
Epoch 52/300, seasonal_1 Loss: 0.0893 | 0.0627
Epoch 53/300, seasonal_1 Loss: 0.0921 | 0.0565
Epoch 54/300, seasonal_1 Loss: 0.0854 | 0.0521
Epoch 55/300, seasonal_1 Loss: 0.0818 | 0.0522
Epoch 56/300, seasonal_1 Loss: 0.0817 | 0.0544
Epoch 57/300, seasonal_1 Loss: 0.0809 | 0.0545
Epoch 58/300, seasonal_1 Loss: 0.0804 | 0.0506
Epoch 59/300, seasonal_1 Loss: 0.0793 | 0.0485
Epoch 60/300, seasonal_1 Loss: 0.0787 | 0.0482
Epoch 61/300, seasonal_1 Loss: 0.0785 | 0.0483
Epoch 62/300, seasonal_1 Loss: 0.0783 | 0.0480
Epoch 63/300, seasonal_1 Loss: 0.0780 | 0.0473
Epoch 64/300, seasonal_1 Loss: 0.0777 | 0.0466
Epoch 65/300, seasonal_1 Loss: 0.0775 | 0.0460
Epoch 66/300, seasonal_1 Loss: 0.0772 | 0.0454
Epoch 67/300, seasonal_1 Loss: 0.0768 | 0.0449
Epoch 68/300, seasonal_1 Loss: 0.0763 | 0.0443
Epoch 69/300, seasonal_1 Loss: 0.0760 | 0.0438
Epoch 70/300, seasonal_1 Loss: 0.0758 | 0.0433
Epoch 71/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 72/300, seasonal_1 Loss: 0.0758 | 0.0431
Epoch 73/300, seasonal_1 Loss: 0.0759 | 0.0431
Epoch 74/300, seasonal_1 Loss: 0.0759 | 0.0430
Epoch 75/300, seasonal_1 Loss: 0.0756 | 0.0426
Epoch 76/300, seasonal_1 Loss: 0.0750 | 0.0420
Epoch 77/300, seasonal_1 Loss: 0.0745 | 0.0416
Epoch 78/300, seasonal_1 Loss: 0.0748 | 0.0417
Epoch 79/300, seasonal_1 Loss: 0.0760 | 0.0425
Epoch 80/300, seasonal_1 Loss: 0.0782 | 0.0433
Epoch 81/300, seasonal_1 Loss: 0.0802 | 0.0432
Epoch 82/300, seasonal_1 Loss: 0.0786 | 0.0433
Epoch 83/300, seasonal_1 Loss: 0.0788 | 0.0480
Epoch 84/300, seasonal_1 Loss: 0.0856 | 0.0538
Epoch 85/300, seasonal_1 Loss: 0.0818 | 0.0458
Epoch 86/300, seasonal_1 Loss: 0.0776 | 0.0460
Epoch 87/300, seasonal_1 Loss: 0.0748 | 0.0417
Epoch 88/300, seasonal_1 Loss: 0.0742 | 0.0406
Epoch 89/300, seasonal_1 Loss: 0.0738 | 0.0408
Epoch 90/300, seasonal_1 Loss: 0.0729 | 0.0397
Epoch 91/300, seasonal_1 Loss: 0.0726 | 0.0435
Epoch 92/300, seasonal_1 Loss: 0.0724 | 0.0433
Epoch 93/300, seasonal_1 Loss: 0.0721 | 0.0396
Epoch 94/300, seasonal_1 Loss: 0.0717 | 0.0387
Epoch 95/300, seasonal_1 Loss: 0.0716 | 0.0386
Epoch 96/300, seasonal_1 Loss: 0.0714 | 0.0390
Epoch 97/300, seasonal_1 Loss: 0.0712 | 0.0399
Epoch 98/300, seasonal_1 Loss: 0.0711 | 0.0396
Epoch 99/300, seasonal_1 Loss: 0.0709 | 0.0385
Epoch 100/300, seasonal_1 Loss: 0.0707 | 0.0379
Epoch 101/300, seasonal_1 Loss: 0.0705 | 0.0378
Epoch 102/300, seasonal_1 Loss: 0.0704 | 0.0380
Epoch 103/300, seasonal_1 Loss: 0.0702 | 0.0382
Epoch 104/300, seasonal_1 Loss: 0.0701 | 0.0381
Epoch 105/300, seasonal_1 Loss: 0.0700 | 0.0377
Epoch 106/300, seasonal_1 Loss: 0.0698 | 0.0375
Epoch 107/300, seasonal_1 Loss: 0.0697 | 0.0374
Epoch 108/300, seasonal_1 Loss: 0.0696 | 0.0374
Epoch 109/300, seasonal_1 Loss: 0.0695 | 0.0374
Epoch 110/300, seasonal_1 Loss: 0.0693 | 0.0372
Epoch 111/300, seasonal_1 Loss: 0.0692 | 0.0370
Epoch 112/300, seasonal_1 Loss: 0.0691 | 0.0368
Epoch 113/300, seasonal_1 Loss: 0.0689 | 0.0368
Epoch 114/300, seasonal_1 Loss: 0.0688 | 0.0368
Epoch 115/300, seasonal_1 Loss: 0.0688 | 0.0368
Epoch 116/300, seasonal_1 Loss: 0.0687 | 0.0368
Epoch 117/300, seasonal_1 Loss: 0.0685 | 0.0367
Epoch 118/300, seasonal_1 Loss: 0.0684 | 0.0366
Epoch 119/300, seasonal_1 Loss: 0.0683 | 0.0365
Epoch 120/300, seasonal_1 Loss: 0.0683 | 0.0363
Epoch 121/300, seasonal_1 Loss: 0.0682 | 0.0362
Epoch 122/300, seasonal_1 Loss: 0.0681 | 0.0360
Epoch 123/300, seasonal_1 Loss: 0.0679 | 0.0360
Epoch 124/300, seasonal_1 Loss: 0.0678 | 0.0360
Epoch 125/300, seasonal_1 Loss: 0.0678 | 0.0361
Epoch 126/300, seasonal_1 Loss: 0.0679 | 0.0362
Epoch 127/300, seasonal_1 Loss: 0.0678 | 0.0361
Epoch 128/300, seasonal_1 Loss: 0.0676 | 0.0360
Epoch 129/300, seasonal_1 Loss: 0.0675 | 0.0359
Epoch 130/300, seasonal_1 Loss: 0.0676 | 0.0359
Epoch 131/300, seasonal_1 Loss: 0.0677 | 0.0358
Epoch 132/300, seasonal_1 Loss: 0.0675 | 0.0355
Epoch 133/300, seasonal_1 Loss: 0.0671 | 0.0355
Epoch 134/300, seasonal_1 Loss: 0.0670 | 0.0356
Epoch 135/300, seasonal_1 Loss: 0.0670 | 0.0356
Epoch 136/300, seasonal_1 Loss: 0.0670 | 0.0353
Epoch 137/300, seasonal_1 Loss: 0.0667 | 0.0352
Epoch 138/300, seasonal_1 Loss: 0.0666 | 0.0352
Epoch 139/300, seasonal_1 Loss: 0.0666 | 0.0352
Epoch 140/300, seasonal_1 Loss: 0.0665 | 0.0349
Epoch 141/300, seasonal_1 Loss: 0.0662 | 0.0349
Epoch 142/300, seasonal_1 Loss: 0.0661 | 0.0349
Epoch 143/300, seasonal_1 Loss: 0.0660 | 0.0349
Epoch 144/300, seasonal_1 Loss: 0.0659 | 0.0347
Epoch 145/300, seasonal_1 Loss: 0.0658 | 0.0346
Epoch 146/300, seasonal_1 Loss: 0.0657 | 0.0346
Epoch 147/300, seasonal_1 Loss: 0.0657 | 0.0345
Epoch 148/300, seasonal_1 Loss: 0.0656 | 0.0344
Epoch 149/300, seasonal_1 Loss: 0.0655 | 0.0344
Epoch 150/300, seasonal_1 Loss: 0.0654 | 0.0344
Epoch 151/300, seasonal_1 Loss: 0.0653 | 0.0343
Epoch 152/300, seasonal_1 Loss: 0.0652 | 0.0342
Epoch 153/300, seasonal_1 Loss: 0.0652 | 0.0342
Epoch 154/300, seasonal_1 Loss: 0.0651 | 0.0341
Epoch 155/300, seasonal_1 Loss: 0.0650 | 0.0341
Epoch 156/300, seasonal_1 Loss: 0.0649 | 0.0340
Epoch 157/300, seasonal_1 Loss: 0.0649 | 0.0340
Epoch 158/300, seasonal_1 Loss: 0.0648 | 0.0340
Epoch 159/300, seasonal_1 Loss: 0.0647 | 0.0339
Epoch 160/300, seasonal_1 Loss: 0.0647 | 0.0339
Epoch 161/300, seasonal_1 Loss: 0.0646 | 0.0338
Epoch 162/300, seasonal_1 Loss: 0.0646 | 0.0338
Epoch 163/300, seasonal_1 Loss: 0.0645 | 0.0337
Epoch 164/300, seasonal_1 Loss: 0.0644 | 0.0337
Epoch 165/300, seasonal_1 Loss: 0.0644 | 0.0337
Epoch 166/300, seasonal_1 Loss: 0.0643 | 0.0336
Epoch 167/300, seasonal_1 Loss: 0.0643 | 0.0336
Epoch 168/300, seasonal_1 Loss: 0.0642 | 0.0335
Epoch 169/300, seasonal_1 Loss: 0.0641 | 0.0335
Epoch 170/300, seasonal_1 Loss: 0.0641 | 0.0335
Epoch 171/300, seasonal_1 Loss: 0.0640 | 0.0334
Epoch 172/300, seasonal_1 Loss: 0.0640 | 0.0334
Epoch 173/300, seasonal_1 Loss: 0.0639 | 0.0334
Epoch 174/300, seasonal_1 Loss: 0.0639 | 0.0333
Epoch 175/300, seasonal_1 Loss: 0.0638 | 0.0333
Epoch 176/300, seasonal_1 Loss: 0.0638 | 0.0333
Epoch 177/300, seasonal_1 Loss: 0.0637 | 0.0332
Epoch 178/300, seasonal_1 Loss: 0.0637 | 0.0332
Epoch 179/300, seasonal_1 Loss: 0.0636 | 0.0332
Epoch 180/300, seasonal_1 Loss: 0.0636 | 0.0331
Epoch 181/300, seasonal_1 Loss: 0.0635 | 0.0331
Epoch 182/300, seasonal_1 Loss: 0.0635 | 0.0331
Epoch 183/300, seasonal_1 Loss: 0.0634 | 0.0331
Epoch 184/300, seasonal_1 Loss: 0.0634 | 0.0330
Epoch 185/300, seasonal_1 Loss: 0.0633 | 0.0330
Epoch 186/300, seasonal_1 Loss: 0.0633 | 0.0330
Epoch 187/300, seasonal_1 Loss: 0.0633 | 0.0329
Epoch 188/300, seasonal_1 Loss: 0.0632 | 0.0329
Epoch 189/300, seasonal_1 Loss: 0.0632 | 0.0329
Epoch 190/300, seasonal_1 Loss: 0.0631 | 0.0329
Epoch 191/300, seasonal_1 Loss: 0.0631 | 0.0328
Epoch 192/300, seasonal_1 Loss: 0.0630 | 0.0328
Epoch 193/300, seasonal_1 Loss: 0.0630 | 0.0328
Epoch 194/300, seasonal_1 Loss: 0.0630 | 0.0328
Epoch 195/300, seasonal_1 Loss: 0.0629 | 0.0328
Epoch 196/300, seasonal_1 Loss: 0.0629 | 0.0327
Epoch 197/300, seasonal_1 Loss: 0.0628 | 0.0327
Epoch 198/300, seasonal_1 Loss: 0.0628 | 0.0327
Epoch 199/300, seasonal_1 Loss: 0.0628 | 0.0327
Epoch 200/300, seasonal_1 Loss: 0.0627 | 0.0326
Epoch 201/300, seasonal_1 Loss: 0.0627 | 0.0326
Epoch 202/300, seasonal_1 Loss: 0.0627 | 0.0326
Epoch 203/300, seasonal_1 Loss: 0.0626 | 0.0326
Epoch 204/300, seasonal_1 Loss: 0.0626 | 0.0326
Epoch 205/300, seasonal_1 Loss: 0.0626 | 0.0326
Epoch 206/300, seasonal_1 Loss: 0.0625 | 0.0325
Epoch 207/300, seasonal_1 Loss: 0.0625 | 0.0325
Epoch 208/300, seasonal_1 Loss: 0.0625 | 0.0325
Epoch 209/300, seasonal_1 Loss: 0.0624 | 0.0325
Epoch 210/300, seasonal_1 Loss: 0.0624 | 0.0325
Epoch 211/300, seasonal_1 Loss: 0.0624 | 0.0325
Epoch 212/300, seasonal_1 Loss: 0.0623 | 0.0324
Epoch 213/300, seasonal_1 Loss: 0.0623 | 0.0324
Epoch 214/300, seasonal_1 Loss: 0.0623 | 0.0324
Epoch 215/300, seasonal_1 Loss: 0.0622 | 0.0324
Epoch 216/300, seasonal_1 Loss: 0.0622 | 0.0324
Epoch 217/300, seasonal_1 Loss: 0.0622 | 0.0324
Epoch 218/300, seasonal_1 Loss: 0.0621 | 0.0324
Epoch 219/300, seasonal_1 Loss: 0.0621 | 0.0323
Epoch 220/300, seasonal_1 Loss: 0.0621 | 0.0323
Epoch 221/300, seasonal_1 Loss: 0.0621 | 0.0323
Epoch 222/300, seasonal_1 Loss: 0.0620 | 0.0323
Epoch 223/300, seasonal_1 Loss: 0.0620 | 0.0323
Epoch 224/300, seasonal_1 Loss: 0.0620 | 0.0323
Epoch 225/300, seasonal_1 Loss: 0.0619 | 0.0323
Epoch 226/300, seasonal_1 Loss: 0.0619 | 0.0322
Epoch 227/300, seasonal_1 Loss: 0.0619 | 0.0322
Epoch 228/300, seasonal_1 Loss: 0.0619 | 0.0322
Epoch 229/300, seasonal_1 Loss: 0.0618 | 0.0322
Epoch 230/300, seasonal_1 Loss: 0.0618 | 0.0322
Epoch 231/300, seasonal_1 Loss: 0.0618 | 0.0322
Epoch 232/300, seasonal_1 Loss: 0.0618 | 0.0322
Epoch 233/300, seasonal_1 Loss: 0.0618 | 0.0322
Epoch 234/300, seasonal_1 Loss: 0.0617 | 0.0322
Epoch 235/300, seasonal_1 Loss: 0.0617 | 0.0322
Epoch 236/300, seasonal_1 Loss: 0.0617 | 0.0321
Epoch 237/300, seasonal_1 Loss: 0.0617 | 0.0321
Epoch 238/300, seasonal_1 Loss: 0.0616 | 0.0321
Epoch 239/300, seasonal_1 Loss: 0.0616 | 0.0321
Epoch 240/300, seasonal_1 Loss: 0.0616 | 0.0321
Epoch 241/300, seasonal_1 Loss: 0.0616 | 0.0321
Epoch 242/300, seasonal_1 Loss: 0.0616 | 0.0321
Epoch 243/300, seasonal_1 Loss: 0.0615 | 0.0321
Epoch 244/300, seasonal_1 Loss: 0.0615 | 0.0321
Epoch 245/300, seasonal_1 Loss: 0.0615 | 0.0321
Epoch 246/300, seasonal_1 Loss: 0.0615 | 0.0321
Epoch 247/300, seasonal_1 Loss: 0.0615 | 0.0320
Epoch 248/300, seasonal_1 Loss: 0.0614 | 0.0320
Epoch 249/300, seasonal_1 Loss: 0.0614 | 0.0320
Epoch 250/300, seasonal_1 Loss: 0.0614 | 0.0320
Epoch 251/300, seasonal_1 Loss: 0.0614 | 0.0320
Epoch 252/300, seasonal_1 Loss: 0.0614 | 0.0320
Epoch 253/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 254/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 255/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 256/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 257/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 258/300, seasonal_1 Loss: 0.0613 | 0.0320
Epoch 259/300, seasonal_1 Loss: 0.0612 | 0.0320
Epoch 260/300, seasonal_1 Loss: 0.0612 | 0.0320
Epoch 261/300, seasonal_1 Loss: 0.0612 | 0.0320
Epoch 262/300, seasonal_1 Loss: 0.0612 | 0.0319
Epoch 263/300, seasonal_1 Loss: 0.0612 | 0.0319
Epoch 264/300, seasonal_1 Loss: 0.0612 | 0.0319
Epoch 265/300, seasonal_1 Loss: 0.0612 | 0.0319
Epoch 266/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 267/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 268/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 269/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 270/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 271/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 272/300, seasonal_1 Loss: 0.0611 | 0.0319
Epoch 273/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 274/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 275/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 276/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 277/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 278/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 279/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 280/300, seasonal_1 Loss: 0.0610 | 0.0319
Epoch 281/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 282/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 283/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 284/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 285/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 286/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 287/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 288/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 289/300, seasonal_1 Loss: 0.0609 | 0.0318
Epoch 290/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 291/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 292/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 293/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 294/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 295/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 296/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 297/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 298/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 299/300, seasonal_1 Loss: 0.0608 | 0.0318
Epoch 300/300, seasonal_1 Loss: 0.0607 | 0.0318
Training seasonal_2 component with params: {'observation_period_num': 21, 'train_rates': 0.9636580450164239, 'learning_rate': 0.000648628096492995, 'batch_size': 108, 'step_size': 3, 'gamma': 0.9474621713921687}
Epoch 1/300, seasonal_2 Loss: 0.3586 | 0.3025
Epoch 2/300, seasonal_2 Loss: 0.2289 | 0.2132
Epoch 3/300, seasonal_2 Loss: 0.1528 | 0.1582
Epoch 4/300, seasonal_2 Loss: 0.1374 | 0.1629
Epoch 5/300, seasonal_2 Loss: 0.1271 | 0.1613
Epoch 6/300, seasonal_2 Loss: 0.1215 | 0.1560
Epoch 7/300, seasonal_2 Loss: 0.1211 | 0.1535
Epoch 8/300, seasonal_2 Loss: 0.1335 | 0.1320
Epoch 9/300, seasonal_2 Loss: 0.1415 | 0.1547
Epoch 10/300, seasonal_2 Loss: 0.1331 | 0.1155
Epoch 11/300, seasonal_2 Loss: 0.1557 | 0.1495
Epoch 12/300, seasonal_2 Loss: 0.1658 | 0.1631
Epoch 13/300, seasonal_2 Loss: 0.1574 | 0.2401
Epoch 14/300, seasonal_2 Loss: 0.1360 | 0.2136
Epoch 15/300, seasonal_2 Loss: 0.1229 | 0.1219
Epoch 16/300, seasonal_2 Loss: 0.1038 | 0.1009
Epoch 17/300, seasonal_2 Loss: 0.0974 | 0.0882
Epoch 18/300, seasonal_2 Loss: 0.0932 | 0.0725
Epoch 19/300, seasonal_2 Loss: 0.1000 | 0.0836
Epoch 20/300, seasonal_2 Loss: 0.1000 | 0.0695
Epoch 21/300, seasonal_2 Loss: 0.0846 | 0.0618
Epoch 22/300, seasonal_2 Loss: 0.0883 | 0.0583
Epoch 23/300, seasonal_2 Loss: 0.0826 | 0.0530
Epoch 24/300, seasonal_2 Loss: 0.0795 | 0.0518
Epoch 25/300, seasonal_2 Loss: 0.0827 | 0.0603
Epoch 26/300, seasonal_2 Loss: 0.0864 | 0.0606
Epoch 27/300, seasonal_2 Loss: 0.0807 | 0.0567
Epoch 28/300, seasonal_2 Loss: 0.0758 | 0.0514
Epoch 29/300, seasonal_2 Loss: 0.0752 | 0.0484
Epoch 30/300, seasonal_2 Loss: 0.0772 | 0.0626
Epoch 31/300, seasonal_2 Loss: 0.0798 | 0.0807
Epoch 32/300, seasonal_2 Loss: 0.0780 | 0.0614
Epoch 33/300, seasonal_2 Loss: 0.0726 | 0.0463
Epoch 34/300, seasonal_2 Loss: 0.0713 | 0.0431
Epoch 35/300, seasonal_2 Loss: 0.0749 | 0.0492
Epoch 36/300, seasonal_2 Loss: 0.0738 | 0.0440
Epoch 37/300, seasonal_2 Loss: 0.0741 | 0.0463
Epoch 38/300, seasonal_2 Loss: 0.0732 | 0.0452
Epoch 39/300, seasonal_2 Loss: 0.0689 | 0.0422
Epoch 40/300, seasonal_2 Loss: 0.0676 | 0.0401
Epoch 41/300, seasonal_2 Loss: 0.0683 | 0.0390
Epoch 42/300, seasonal_2 Loss: 0.0689 | 0.0385
Epoch 43/300, seasonal_2 Loss: 0.0686 | 0.0380
Epoch 44/300, seasonal_2 Loss: 0.0681 | 0.0384
Epoch 45/300, seasonal_2 Loss: 0.0683 | 0.0393
Epoch 46/300, seasonal_2 Loss: 0.0684 | 0.0416
Epoch 47/300, seasonal_2 Loss: 0.0674 | 0.0404
Epoch 48/300, seasonal_2 Loss: 0.0662 | 0.0389
Epoch 49/300, seasonal_2 Loss: 0.0656 | 0.0370
Epoch 50/300, seasonal_2 Loss: 0.0669 | 0.0365
Epoch 51/300, seasonal_2 Loss: 0.0683 | 0.0359
Epoch 52/300, seasonal_2 Loss: 0.0665 | 0.0349
Epoch 53/300, seasonal_2 Loss: 0.0640 | 0.0348
Epoch 54/300, seasonal_2 Loss: 0.0632 | 0.0347
Epoch 55/300, seasonal_2 Loss: 0.0628 | 0.0350
Epoch 56/300, seasonal_2 Loss: 0.0626 | 0.0353
Epoch 57/300, seasonal_2 Loss: 0.0624 | 0.0354
Epoch 58/300, seasonal_2 Loss: 0.0620 | 0.0353
Epoch 59/300, seasonal_2 Loss: 0.0616 | 0.0352
Epoch 60/300, seasonal_2 Loss: 0.0613 | 0.0351
Epoch 61/300, seasonal_2 Loss: 0.0610 | 0.0347
Epoch 62/300, seasonal_2 Loss: 0.0605 | 0.0340
Epoch 63/300, seasonal_2 Loss: 0.0600 | 0.0333
Epoch 64/300, seasonal_2 Loss: 0.0596 | 0.0327
Epoch 65/300, seasonal_2 Loss: 0.0594 | 0.0323
Epoch 66/300, seasonal_2 Loss: 0.0591 | 0.0321
Epoch 67/300, seasonal_2 Loss: 0.0589 | 0.0320
Epoch 68/300, seasonal_2 Loss: 0.0587 | 0.0320
Epoch 69/300, seasonal_2 Loss: 0.0585 | 0.0320
Epoch 70/300, seasonal_2 Loss: 0.0584 | 0.0320
Epoch 71/300, seasonal_2 Loss: 0.0582 | 0.0320
Epoch 72/300, seasonal_2 Loss: 0.0581 | 0.0320
Epoch 73/300, seasonal_2 Loss: 0.0580 | 0.0321
Epoch 74/300, seasonal_2 Loss: 0.0579 | 0.0321
Epoch 75/300, seasonal_2 Loss: 0.0579 | 0.0321
Epoch 76/300, seasonal_2 Loss: 0.0579 | 0.0320
Epoch 77/300, seasonal_2 Loss: 0.0580 | 0.0319
Epoch 78/300, seasonal_2 Loss: 0.0583 | 0.0318
Epoch 79/300, seasonal_2 Loss: 0.0589 | 0.0319
Epoch 80/300, seasonal_2 Loss: 0.0598 | 0.0321
Epoch 81/300, seasonal_2 Loss: 0.0608 | 0.0316
Epoch 82/300, seasonal_2 Loss: 0.0601 | 0.0313
Epoch 83/300, seasonal_2 Loss: 0.0580 | 0.0313
Epoch 84/300, seasonal_2 Loss: 0.0572 | 0.0312
Epoch 85/300, seasonal_2 Loss: 0.0572 | 0.0309
Epoch 86/300, seasonal_2 Loss: 0.0570 | 0.0306
Epoch 87/300, seasonal_2 Loss: 0.0567 | 0.0304
Epoch 88/300, seasonal_2 Loss: 0.0565 | 0.0304
Epoch 89/300, seasonal_2 Loss: 0.0563 | 0.0304
Epoch 90/300, seasonal_2 Loss: 0.0562 | 0.0304
Epoch 91/300, seasonal_2 Loss: 0.0561 | 0.0303
Epoch 92/300, seasonal_2 Loss: 0.0560 | 0.0302
Epoch 93/300, seasonal_2 Loss: 0.0559 | 0.0302
Epoch 94/300, seasonal_2 Loss: 0.0559 | 0.0301
Epoch 95/300, seasonal_2 Loss: 0.0558 | 0.0301
Epoch 96/300, seasonal_2 Loss: 0.0557 | 0.0300
Epoch 97/300, seasonal_2 Loss: 0.0557 | 0.0300
Epoch 98/300, seasonal_2 Loss: 0.0556 | 0.0299
Epoch 99/300, seasonal_2 Loss: 0.0555 | 0.0299
Epoch 100/300, seasonal_2 Loss: 0.0555 | 0.0299
Epoch 101/300, seasonal_2 Loss: 0.0554 | 0.0298
Epoch 102/300, seasonal_2 Loss: 0.0554 | 0.0298
Epoch 103/300, seasonal_2 Loss: 0.0553 | 0.0298
Epoch 104/300, seasonal_2 Loss: 0.0553 | 0.0297
Epoch 105/300, seasonal_2 Loss: 0.0552 | 0.0297
Epoch 106/300, seasonal_2 Loss: 0.0552 | 0.0297
Epoch 107/300, seasonal_2 Loss: 0.0551 | 0.0296
Epoch 108/300, seasonal_2 Loss: 0.0551 | 0.0296
Epoch 109/300, seasonal_2 Loss: 0.0550 | 0.0296
Epoch 110/300, seasonal_2 Loss: 0.0550 | 0.0296
Epoch 111/300, seasonal_2 Loss: 0.0549 | 0.0295
Epoch 112/300, seasonal_2 Loss: 0.0549 | 0.0295
Epoch 113/300, seasonal_2 Loss: 0.0549 | 0.0295
Epoch 114/300, seasonal_2 Loss: 0.0548 | 0.0295
Epoch 115/300, seasonal_2 Loss: 0.0548 | 0.0295
Epoch 116/300, seasonal_2 Loss: 0.0547 | 0.0294
Epoch 117/300, seasonal_2 Loss: 0.0547 | 0.0294
Epoch 118/300, seasonal_2 Loss: 0.0547 | 0.0294
Epoch 119/300, seasonal_2 Loss: 0.0546 | 0.0294
Epoch 120/300, seasonal_2 Loss: 0.0546 | 0.0294
Epoch 121/300, seasonal_2 Loss: 0.0546 | 0.0294
Epoch 122/300, seasonal_2 Loss: 0.0546 | 0.0293
Epoch 123/300, seasonal_2 Loss: 0.0545 | 0.0293
Epoch 124/300, seasonal_2 Loss: 0.0545 | 0.0293
Epoch 125/300, seasonal_2 Loss: 0.0545 | 0.0293
Epoch 126/300, seasonal_2 Loss: 0.0544 | 0.0293
Epoch 127/300, seasonal_2 Loss: 0.0544 | 0.0293
Epoch 128/300, seasonal_2 Loss: 0.0544 | 0.0293
Epoch 129/300, seasonal_2 Loss: 0.0544 | 0.0292
Epoch 130/300, seasonal_2 Loss: 0.0544 | 0.0292
Epoch 131/300, seasonal_2 Loss: 0.0543 | 0.0292
Epoch 132/300, seasonal_2 Loss: 0.0543 | 0.0292
Epoch 133/300, seasonal_2 Loss: 0.0543 | 0.0292
Epoch 134/300, seasonal_2 Loss: 0.0543 | 0.0292
Epoch 135/300, seasonal_2 Loss: 0.0543 | 0.0292
Epoch 136/300, seasonal_2 Loss: 0.0542 | 0.0292
Epoch 137/300, seasonal_2 Loss: 0.0542 | 0.0292
Epoch 138/300, seasonal_2 Loss: 0.0542 | 0.0292
Epoch 139/300, seasonal_2 Loss: 0.0542 | 0.0292
Epoch 140/300, seasonal_2 Loss: 0.0542 | 0.0291
Epoch 141/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 142/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 143/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 144/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 145/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 146/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 147/300, seasonal_2 Loss: 0.0541 | 0.0291
Epoch 148/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 149/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 150/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 151/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 152/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 153/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 154/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 155/300, seasonal_2 Loss: 0.0540 | 0.0291
Epoch 156/300, seasonal_2 Loss: 0.0540 | 0.0290
Epoch 157/300, seasonal_2 Loss: 0.0540 | 0.0290
Epoch 158/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 159/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 160/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 161/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 162/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 163/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 164/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 165/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 166/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 167/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 168/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 169/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 170/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 171/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 172/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 173/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 174/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 175/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 176/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 177/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 178/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 179/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 180/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 181/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 182/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 183/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 184/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 185/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 186/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 187/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 188/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 189/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 190/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 191/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 192/300, seasonal_2 Loss: 0.0538 | 0.0290
Epoch 193/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 194/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 195/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 196/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 197/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 198/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 199/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 200/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 201/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 202/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 203/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 204/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 205/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 206/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 207/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 208/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 209/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 210/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 211/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 212/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 213/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 214/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 215/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 216/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 217/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 218/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 219/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 220/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 221/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 222/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 223/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 224/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 225/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 226/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 227/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 228/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 229/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 230/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 231/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 232/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 233/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 234/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 235/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 236/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 237/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 238/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 239/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 240/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 241/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 242/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 243/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 244/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 245/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 246/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 247/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 248/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 249/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 250/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 251/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 252/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 253/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 254/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 255/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 256/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 257/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 258/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 259/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 260/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 261/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 262/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 263/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 264/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 265/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 266/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 267/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 268/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 269/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 270/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 271/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 272/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 273/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 274/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 275/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 276/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 277/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 278/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 279/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 280/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 281/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 282/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 283/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 284/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 285/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 286/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 287/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 288/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 289/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 290/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 291/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 292/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 293/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 294/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 295/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 296/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 297/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 298/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 299/300, seasonal_2 Loss: 0.0537 | 0.0289
Epoch 300/300, seasonal_2 Loss: 0.0537 | 0.0289
Training seasonal_3 component with params: {'observation_period_num': 53, 'train_rates': 0.8641443636034551, 'learning_rate': 0.0003579179295962485, 'batch_size': 179, 'step_size': 3, 'gamma': 0.9543366816257685}
Epoch 1/300, seasonal_3 Loss: 0.6961 | 0.4943
Epoch 2/300, seasonal_3 Loss: 0.3684 | 0.4633
Epoch 3/300, seasonal_3 Loss: 0.2649 | 0.4329
Epoch 4/300, seasonal_3 Loss: 0.2091 | 0.2167
Epoch 5/300, seasonal_3 Loss: 0.2087 | 0.1975
Epoch 6/300, seasonal_3 Loss: 0.1848 | 0.2396
Epoch 7/300, seasonal_3 Loss: 0.1715 | 0.3072
Epoch 8/300, seasonal_3 Loss: 0.1520 | 0.1599
Epoch 9/300, seasonal_3 Loss: 0.1554 | 0.1537
Epoch 10/300, seasonal_3 Loss: 0.1549 | 0.1236
Epoch 11/300, seasonal_3 Loss: 0.1413 | 0.2183
Epoch 12/300, seasonal_3 Loss: 0.1297 | 0.1459
Epoch 13/300, seasonal_3 Loss: 0.1309 | 0.1109
Epoch 14/300, seasonal_3 Loss: 0.1279 | 0.1119
Epoch 15/300, seasonal_3 Loss: 0.1300 | 0.0964
Epoch 16/300, seasonal_3 Loss: 0.1130 | 0.1027
Epoch 17/300, seasonal_3 Loss: 0.1135 | 0.1313
Epoch 18/300, seasonal_3 Loss: 0.1152 | 0.1366
Epoch 19/300, seasonal_3 Loss: 0.1071 | 0.0959
Epoch 20/300, seasonal_3 Loss: 0.1117 | 0.1063
Epoch 21/300, seasonal_3 Loss: 0.1183 | 0.0911
Epoch 22/300, seasonal_3 Loss: 0.1144 | 0.2072
Epoch 23/300, seasonal_3 Loss: 0.1170 | 0.0963
Epoch 24/300, seasonal_3 Loss: 0.1002 | 0.0835
Epoch 25/300, seasonal_3 Loss: 0.0999 | 0.0780
Epoch 26/300, seasonal_3 Loss: 0.0962 | 0.0778
Epoch 27/300, seasonal_3 Loss: 0.0925 | 0.0850
Epoch 28/300, seasonal_3 Loss: 0.0923 | 0.0741
Epoch 29/300, seasonal_3 Loss: 0.0909 | 0.0689
Epoch 30/300, seasonal_3 Loss: 0.0912 | 0.0691
Epoch 31/300, seasonal_3 Loss: 0.0890 | 0.0691
Epoch 32/300, seasonal_3 Loss: 0.0881 | 0.0680
Epoch 33/300, seasonal_3 Loss: 0.0874 | 0.0742
Epoch 34/300, seasonal_3 Loss: 0.0864 | 0.0810
Epoch 35/300, seasonal_3 Loss: 0.0850 | 0.0654
Epoch 36/300, seasonal_3 Loss: 0.0864 | 0.0706
Epoch 37/300, seasonal_3 Loss: 0.0856 | 0.0642
Epoch 38/300, seasonal_3 Loss: 0.0865 | 0.0919
Epoch 39/300, seasonal_3 Loss: 0.0849 | 0.0689
Epoch 40/300, seasonal_3 Loss: 0.0854 | 0.0644
Epoch 41/300, seasonal_3 Loss: 0.0843 | 0.0621
Epoch 42/300, seasonal_3 Loss: 0.0834 | 0.0677
Epoch 43/300, seasonal_3 Loss: 0.0829 | 0.0710
Epoch 44/300, seasonal_3 Loss: 0.0821 | 0.0635
Epoch 45/300, seasonal_3 Loss: 0.0805 | 0.0608
Epoch 46/300, seasonal_3 Loss: 0.0804 | 0.0613
Epoch 47/300, seasonal_3 Loss: 0.0798 | 0.0606
Epoch 48/300, seasonal_3 Loss: 0.0793 | 0.0761
Epoch 49/300, seasonal_3 Loss: 0.0791 | 0.0632
Epoch 50/300, seasonal_3 Loss: 0.0783 | 0.0593
Epoch 51/300, seasonal_3 Loss: 0.0781 | 0.0588
Epoch 52/300, seasonal_3 Loss: 0.0778 | 0.0652
Epoch 53/300, seasonal_3 Loss: 0.0775 | 0.0649
Epoch 54/300, seasonal_3 Loss: 0.0767 | 0.0581
Epoch 55/300, seasonal_3 Loss: 0.0771 | 0.0584
Epoch 56/300, seasonal_3 Loss: 0.0764 | 0.0597
Epoch 57/300, seasonal_3 Loss: 0.0764 | 0.0677
Epoch 58/300, seasonal_3 Loss: 0.0758 | 0.0584
Epoch 59/300, seasonal_3 Loss: 0.0755 | 0.0579
Epoch 60/300, seasonal_3 Loss: 0.0758 | 0.0578
Epoch 61/300, seasonal_3 Loss: 0.0752 | 0.0658
Epoch 62/300, seasonal_3 Loss: 0.0751 | 0.0580
Epoch 63/300, seasonal_3 Loss: 0.0744 | 0.0569
Epoch 64/300, seasonal_3 Loss: 0.0747 | 0.0576
Epoch 65/300, seasonal_3 Loss: 0.0742 | 0.0616
Epoch 66/300, seasonal_3 Loss: 0.0740 | 0.0575
Epoch 67/300, seasonal_3 Loss: 0.0736 | 0.0565
Epoch 68/300, seasonal_3 Loss: 0.0737 | 0.0572
Epoch 69/300, seasonal_3 Loss: 0.0733 | 0.0593
Epoch 70/300, seasonal_3 Loss: 0.0732 | 0.0567
Epoch 71/300, seasonal_3 Loss: 0.0729 | 0.0564
Epoch 72/300, seasonal_3 Loss: 0.0729 | 0.0571
Epoch 73/300, seasonal_3 Loss: 0.0726 | 0.0575
Epoch 74/300, seasonal_3 Loss: 0.0725 | 0.0564
Epoch 75/300, seasonal_3 Loss: 0.0723 | 0.0563
Epoch 76/300, seasonal_3 Loss: 0.0723 | 0.0567
Epoch 77/300, seasonal_3 Loss: 0.0720 | 0.0567
Epoch 78/300, seasonal_3 Loss: 0.0719 | 0.0562
Epoch 79/300, seasonal_3 Loss: 0.0718 | 0.0562
Epoch 80/300, seasonal_3 Loss: 0.0717 | 0.0564
Epoch 81/300, seasonal_3 Loss: 0.0716 | 0.0563
Epoch 82/300, seasonal_3 Loss: 0.0714 | 0.0560
Epoch 83/300, seasonal_3 Loss: 0.0714 | 0.0560
Epoch 84/300, seasonal_3 Loss: 0.0712 | 0.0561
Epoch 85/300, seasonal_3 Loss: 0.0711 | 0.0559
Epoch 86/300, seasonal_3 Loss: 0.0710 | 0.0558
Epoch 87/300, seasonal_3 Loss: 0.0709 | 0.0559
Epoch 88/300, seasonal_3 Loss: 0.0708 | 0.0558
Epoch 89/300, seasonal_3 Loss: 0.0708 | 0.0557
Epoch 90/300, seasonal_3 Loss: 0.0707 | 0.0557
Epoch 91/300, seasonal_3 Loss: 0.0706 | 0.0557
Epoch 92/300, seasonal_3 Loss: 0.0705 | 0.0556
Epoch 93/300, seasonal_3 Loss: 0.0704 | 0.0556
Epoch 94/300, seasonal_3 Loss: 0.0704 | 0.0555
Epoch 95/300, seasonal_3 Loss: 0.0703 | 0.0555
Epoch 96/300, seasonal_3 Loss: 0.0702 | 0.0555
Epoch 97/300, seasonal_3 Loss: 0.0701 | 0.0554
Epoch 98/300, seasonal_3 Loss: 0.0701 | 0.0554
Epoch 99/300, seasonal_3 Loss: 0.0700 | 0.0554
Epoch 100/300, seasonal_3 Loss: 0.0699 | 0.0553
Epoch 101/300, seasonal_3 Loss: 0.0699 | 0.0553
Epoch 102/300, seasonal_3 Loss: 0.0698 | 0.0553
Epoch 103/300, seasonal_3 Loss: 0.0697 | 0.0552
Epoch 104/300, seasonal_3 Loss: 0.0697 | 0.0552
Epoch 105/300, seasonal_3 Loss: 0.0696 | 0.0552
Epoch 106/300, seasonal_3 Loss: 0.0696 | 0.0552
Epoch 107/300, seasonal_3 Loss: 0.0695 | 0.0551
Epoch 108/300, seasonal_3 Loss: 0.0695 | 0.0551
Epoch 109/300, seasonal_3 Loss: 0.0694 | 0.0551
Epoch 110/300, seasonal_3 Loss: 0.0694 | 0.0551
Epoch 111/300, seasonal_3 Loss: 0.0693 | 0.0550
Epoch 112/300, seasonal_3 Loss: 0.0693 | 0.0550
Epoch 113/300, seasonal_3 Loss: 0.0692 | 0.0550
Epoch 114/300, seasonal_3 Loss: 0.0692 | 0.0550
Epoch 115/300, seasonal_3 Loss: 0.0692 | 0.0550
Epoch 116/300, seasonal_3 Loss: 0.0691 | 0.0550
Epoch 117/300, seasonal_3 Loss: 0.0691 | 0.0549
Epoch 118/300, seasonal_3 Loss: 0.0690 | 0.0549
Epoch 119/300, seasonal_3 Loss: 0.0690 | 0.0549
Epoch 120/300, seasonal_3 Loss: 0.0690 | 0.0549
Epoch 121/300, seasonal_3 Loss: 0.0689 | 0.0549
Epoch 122/300, seasonal_3 Loss: 0.0689 | 0.0549
Epoch 123/300, seasonal_3 Loss: 0.0689 | 0.0548
Epoch 124/300, seasonal_3 Loss: 0.0688 | 0.0548
Epoch 125/300, seasonal_3 Loss: 0.0688 | 0.0548
Epoch 126/300, seasonal_3 Loss: 0.0688 | 0.0548
Epoch 127/300, seasonal_3 Loss: 0.0687 | 0.0548
Epoch 128/300, seasonal_3 Loss: 0.0687 | 0.0548
Epoch 129/300, seasonal_3 Loss: 0.0687 | 0.0548
Epoch 130/300, seasonal_3 Loss: 0.0686 | 0.0547
Epoch 131/300, seasonal_3 Loss: 0.0686 | 0.0547
Epoch 132/300, seasonal_3 Loss: 0.0686 | 0.0547
Epoch 133/300, seasonal_3 Loss: 0.0686 | 0.0547
Epoch 134/300, seasonal_3 Loss: 0.0685 | 0.0547
Epoch 135/300, seasonal_3 Loss: 0.0685 | 0.0547
Epoch 136/300, seasonal_3 Loss: 0.0685 | 0.0547
Epoch 137/300, seasonal_3 Loss: 0.0685 | 0.0547
Epoch 138/300, seasonal_3 Loss: 0.0684 | 0.0547
Epoch 139/300, seasonal_3 Loss: 0.0684 | 0.0547
Epoch 140/300, seasonal_3 Loss: 0.0684 | 0.0546
Epoch 141/300, seasonal_3 Loss: 0.0684 | 0.0546
Epoch 142/300, seasonal_3 Loss: 0.0684 | 0.0546
Epoch 143/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 144/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 145/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 146/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 147/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 148/300, seasonal_3 Loss: 0.0683 | 0.0546
Epoch 149/300, seasonal_3 Loss: 0.0682 | 0.0546
Epoch 150/300, seasonal_3 Loss: 0.0682 | 0.0546
Epoch 151/300, seasonal_3 Loss: 0.0682 | 0.0546
Epoch 152/300, seasonal_3 Loss: 0.0682 | 0.0546
Epoch 153/300, seasonal_3 Loss: 0.0682 | 0.0546
Epoch 154/300, seasonal_3 Loss: 0.0682 | 0.0545
Epoch 155/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 156/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 157/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 158/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 159/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 160/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 161/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 162/300, seasonal_3 Loss: 0.0681 | 0.0545
Epoch 163/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 164/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 165/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 166/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 167/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 168/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 169/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 170/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 171/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 172/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 173/300, seasonal_3 Loss: 0.0680 | 0.0545
Epoch 174/300, seasonal_3 Loss: 0.0679 | 0.0545
Epoch 175/300, seasonal_3 Loss: 0.0679 | 0.0545
Epoch 176/300, seasonal_3 Loss: 0.0679 | 0.0545
Epoch 177/300, seasonal_3 Loss: 0.0679 | 0.0545
Epoch 178/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 179/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 180/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 181/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 182/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 183/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 184/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 185/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 186/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 187/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 188/300, seasonal_3 Loss: 0.0679 | 0.0544
Epoch 189/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 190/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 191/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 192/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 193/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 194/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 195/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 196/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 197/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 198/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 199/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 200/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 201/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 202/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 203/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 204/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 205/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 206/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 207/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 208/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 209/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 210/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 211/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 212/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 213/300, seasonal_3 Loss: 0.0678 | 0.0544
Epoch 214/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 215/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 216/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 217/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 218/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 219/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 220/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 221/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 222/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 223/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 224/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 225/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 226/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 227/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 228/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 229/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 230/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 231/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 232/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 233/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 234/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 235/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 236/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 237/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 238/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 239/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 240/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 241/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 242/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 243/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 244/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 245/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 246/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 247/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 248/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 249/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 250/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 251/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 252/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 253/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 254/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 255/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 256/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 257/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 258/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 259/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 260/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 261/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 262/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 263/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 264/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 265/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 266/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 267/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 268/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 269/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 270/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 271/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 272/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 273/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 274/300, seasonal_3 Loss: 0.0677 | 0.0544
Epoch 275/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 276/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 277/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 278/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 279/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 280/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 281/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 282/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 283/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 284/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 285/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 286/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 287/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 288/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 289/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 290/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 291/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 292/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 293/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 294/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 295/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 296/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 297/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 298/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 299/300, seasonal_3 Loss: 0.0677 | 0.0543
Epoch 300/300, seasonal_3 Loss: 0.0677 | 0.0543
Training resid component with params: {'observation_period_num': 17, 'train_rates': 0.9470646080402849, 'learning_rate': 0.00045457222652129585, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8829388798391502}
Epoch 1/300, resid Loss: 0.2399 | 0.1919
Epoch 2/300, resid Loss: 0.1443 | 0.1329
Epoch 3/300, resid Loss: 0.1259 | 0.1675
Epoch 4/300, resid Loss: 0.1121 | 0.1271
Epoch 5/300, resid Loss: 0.1112 | 0.1486
Epoch 6/300, resid Loss: 0.1097 | 0.0970
Epoch 7/300, resid Loss: 0.1143 | 0.0975
Epoch 8/300, resid Loss: 0.1005 | 0.0814
Epoch 9/300, resid Loss: 0.0943 | 0.0806
Epoch 10/300, resid Loss: 0.0924 | 0.0739
Epoch 11/300, resid Loss: 0.0960 | 0.0715
Epoch 12/300, resid Loss: 0.1045 | 0.0868
Epoch 13/300, resid Loss: 0.0893 | 0.0655
Epoch 14/300, resid Loss: 0.0845 | 0.0579
Epoch 15/300, resid Loss: 0.0824 | 0.0519
Epoch 16/300, resid Loss: 0.0811 | 0.0486
Epoch 17/300, resid Loss: 0.0802 | 0.0494
Epoch 18/300, resid Loss: 0.0822 | 0.0516
Epoch 19/300, resid Loss: 0.0883 | 0.0531
Epoch 20/300, resid Loss: 0.0805 | 0.0513
Epoch 21/300, resid Loss: 0.0819 | 0.0586
Epoch 22/300, resid Loss: 0.0887 | 0.0595
Epoch 23/300, resid Loss: 0.0939 | 0.0557
Epoch 24/300, resid Loss: 0.0865 | 0.0518
Epoch 25/300, resid Loss: 0.0799 | 0.0474
Epoch 26/300, resid Loss: 0.0771 | 0.0463
Epoch 27/300, resid Loss: 0.0750 | 0.0453
Epoch 28/300, resid Loss: 0.0734 | 0.0444
Epoch 29/300, resid Loss: 0.0723 | 0.0436
Epoch 30/300, resid Loss: 0.0724 | 0.0440
Epoch 31/300, resid Loss: 0.0762 | 0.0449
Epoch 32/300, resid Loss: 0.0832 | 0.0498
Epoch 33/300, resid Loss: 0.0818 | 0.0437
Epoch 34/300, resid Loss: 0.0770 | 0.0418
Epoch 35/300, resid Loss: 0.0719 | 0.0421
Epoch 36/300, resid Loss: 0.0714 | 0.0438
Epoch 37/300, resid Loss: 0.0741 | 0.0477
Epoch 38/300, resid Loss: 0.0752 | 0.0501
Epoch 39/300, resid Loss: 0.0722 | 0.0496
Epoch 40/300, resid Loss: 0.0694 | 0.0441
Epoch 41/300, resid Loss: 0.0688 | 0.0433
Epoch 42/300, resid Loss: 0.0693 | 0.0439
Epoch 43/300, resid Loss: 0.0694 | 0.0431
Epoch 44/300, resid Loss: 0.0683 | 0.0449
Epoch 45/300, resid Loss: 0.0679 | 0.0466
Epoch 46/300, resid Loss: 0.0700 | 0.0403
Epoch 47/300, resid Loss: 0.0743 | 0.0421
Epoch 48/300, resid Loss: 0.0755 | 0.0426
Epoch 49/300, resid Loss: 0.0729 | 0.0508
Epoch 50/300, resid Loss: 0.0717 | 0.0546
Epoch 51/300, resid Loss: 0.0723 | 0.0504
Epoch 52/300, resid Loss: 0.0756 | 0.0607
Epoch 53/300, resid Loss: 0.0794 | 0.0699
Epoch 54/300, resid Loss: 0.0786 | 0.0691
Epoch 55/300, resid Loss: 0.0751 | 0.0579
Epoch 56/300, resid Loss: 0.0706 | 0.0447
Epoch 57/300, resid Loss: 0.0672 | 0.0410
Epoch 58/300, resid Loss: 0.0659 | 0.0398
Epoch 59/300, resid Loss: 0.0648 | 0.0394
Epoch 60/300, resid Loss: 0.0645 | 0.0396
Epoch 61/300, resid Loss: 0.0647 | 0.0400
Epoch 62/300, resid Loss: 0.0644 | 0.0399
Epoch 63/300, resid Loss: 0.0638 | 0.0398
Epoch 64/300, resid Loss: 0.0632 | 0.0395
Epoch 65/300, resid Loss: 0.0625 | 0.0392
Epoch 66/300, resid Loss: 0.0620 | 0.0387
Epoch 67/300, resid Loss: 0.0617 | 0.0381
Epoch 68/300, resid Loss: 0.0615 | 0.0376
Epoch 69/300, resid Loss: 0.0613 | 0.0372
Epoch 70/300, resid Loss: 0.0612 | 0.0369
Epoch 71/300, resid Loss: 0.0610 | 0.0366
Epoch 72/300, resid Loss: 0.0609 | 0.0364
Epoch 73/300, resid Loss: 0.0608 | 0.0362
Epoch 74/300, resid Loss: 0.0607 | 0.0361
Epoch 75/300, resid Loss: 0.0606 | 0.0360
Epoch 76/300, resid Loss: 0.0606 | 0.0359
Epoch 77/300, resid Loss: 0.0605 | 0.0357
Epoch 78/300, resid Loss: 0.0604 | 0.0356
Epoch 79/300, resid Loss: 0.0603 | 0.0355
Epoch 80/300, resid Loss: 0.0603 | 0.0354
Epoch 81/300, resid Loss: 0.0602 | 0.0354
Epoch 82/300, resid Loss: 0.0601 | 0.0353
Epoch 83/300, resid Loss: 0.0600 | 0.0352
Epoch 84/300, resid Loss: 0.0600 | 0.0351
Epoch 85/300, resid Loss: 0.0599 | 0.0351
Epoch 86/300, resid Loss: 0.0599 | 0.0350
Epoch 87/300, resid Loss: 0.0598 | 0.0349
Epoch 88/300, resid Loss: 0.0597 | 0.0349
Epoch 89/300, resid Loss: 0.0597 | 0.0348
Epoch 90/300, resid Loss: 0.0596 | 0.0348
Epoch 91/300, resid Loss: 0.0596 | 0.0347
Epoch 92/300, resid Loss: 0.0595 | 0.0347
Epoch 93/300, resid Loss: 0.0595 | 0.0346
Epoch 94/300, resid Loss: 0.0594 | 0.0346
Epoch 95/300, resid Loss: 0.0594 | 0.0345
Epoch 96/300, resid Loss: 0.0593 | 0.0345
Epoch 97/300, resid Loss: 0.0593 | 0.0344
Epoch 98/300, resid Loss: 0.0593 | 0.0344
Epoch 99/300, resid Loss: 0.0592 | 0.0344
Epoch 100/300, resid Loss: 0.0592 | 0.0343
Epoch 101/300, resid Loss: 0.0591 | 0.0343
Epoch 102/300, resid Loss: 0.0591 | 0.0343
Epoch 103/300, resid Loss: 0.0591 | 0.0342
Epoch 104/300, resid Loss: 0.0590 | 0.0342
Epoch 105/300, resid Loss: 0.0590 | 0.0342
Epoch 106/300, resid Loss: 0.0590 | 0.0341
Epoch 107/300, resid Loss: 0.0590 | 0.0341
Epoch 108/300, resid Loss: 0.0589 | 0.0341
Epoch 109/300, resid Loss: 0.0589 | 0.0341
Epoch 110/300, resid Loss: 0.0589 | 0.0340
Epoch 111/300, resid Loss: 0.0588 | 0.0340
Epoch 112/300, resid Loss: 0.0588 | 0.0340
Epoch 113/300, resid Loss: 0.0588 | 0.0340
Epoch 114/300, resid Loss: 0.0588 | 0.0339
Epoch 115/300, resid Loss: 0.0587 | 0.0339
Epoch 116/300, resid Loss: 0.0587 | 0.0339
Epoch 117/300, resid Loss: 0.0587 | 0.0339
Epoch 118/300, resid Loss: 0.0587 | 0.0339
Epoch 119/300, resid Loss: 0.0587 | 0.0339
Epoch 120/300, resid Loss: 0.0586 | 0.0338
Epoch 121/300, resid Loss: 0.0586 | 0.0338
Epoch 122/300, resid Loss: 0.0586 | 0.0338
Epoch 123/300, resid Loss: 0.0586 | 0.0338
Epoch 124/300, resid Loss: 0.0586 | 0.0338
Epoch 125/300, resid Loss: 0.0586 | 0.0338
Epoch 126/300, resid Loss: 0.0585 | 0.0337
Epoch 127/300, resid Loss: 0.0585 | 0.0337
Epoch 128/300, resid Loss: 0.0585 | 0.0337
Epoch 129/300, resid Loss: 0.0585 | 0.0337
Epoch 130/300, resid Loss: 0.0585 | 0.0337
Epoch 131/300, resid Loss: 0.0585 | 0.0337
Epoch 132/300, resid Loss: 0.0585 | 0.0337
Epoch 133/300, resid Loss: 0.0585 | 0.0337
Epoch 134/300, resid Loss: 0.0584 | 0.0337
Epoch 135/300, resid Loss: 0.0584 | 0.0336
Epoch 136/300, resid Loss: 0.0584 | 0.0336
Epoch 137/300, resid Loss: 0.0584 | 0.0336
Epoch 138/300, resid Loss: 0.0584 | 0.0336
Epoch 139/300, resid Loss: 0.0584 | 0.0336
Epoch 140/300, resid Loss: 0.0584 | 0.0336
Epoch 141/300, resid Loss: 0.0584 | 0.0336
Epoch 142/300, resid Loss: 0.0584 | 0.0336
Epoch 143/300, resid Loss: 0.0584 | 0.0336
Epoch 144/300, resid Loss: 0.0583 | 0.0336
Epoch 145/300, resid Loss: 0.0583 | 0.0336
Epoch 146/300, resid Loss: 0.0583 | 0.0336
Epoch 147/300, resid Loss: 0.0583 | 0.0336
Epoch 148/300, resid Loss: 0.0583 | 0.0335
Epoch 149/300, resid Loss: 0.0583 | 0.0335
Epoch 150/300, resid Loss: 0.0583 | 0.0335
Epoch 151/300, resid Loss: 0.0583 | 0.0335
Epoch 152/300, resid Loss: 0.0583 | 0.0335
Epoch 153/300, resid Loss: 0.0583 | 0.0335
Epoch 154/300, resid Loss: 0.0583 | 0.0335
Epoch 155/300, resid Loss: 0.0583 | 0.0335
Epoch 156/300, resid Loss: 0.0583 | 0.0335
Epoch 157/300, resid Loss: 0.0583 | 0.0335
Epoch 158/300, resid Loss: 0.0583 | 0.0335
Epoch 159/300, resid Loss: 0.0583 | 0.0335
Epoch 160/300, resid Loss: 0.0583 | 0.0335
Epoch 161/300, resid Loss: 0.0582 | 0.0335
Epoch 162/300, resid Loss: 0.0582 | 0.0335
Epoch 163/300, resid Loss: 0.0582 | 0.0335
Epoch 164/300, resid Loss: 0.0582 | 0.0335
Epoch 165/300, resid Loss: 0.0582 | 0.0335
Epoch 166/300, resid Loss: 0.0582 | 0.0335
Epoch 167/300, resid Loss: 0.0582 | 0.0335
Epoch 168/300, resid Loss: 0.0582 | 0.0335
Epoch 169/300, resid Loss: 0.0582 | 0.0335
Epoch 170/300, resid Loss: 0.0582 | 0.0335
Epoch 171/300, resid Loss: 0.0582 | 0.0335
Epoch 172/300, resid Loss: 0.0582 | 0.0335
Epoch 173/300, resid Loss: 0.0582 | 0.0334
Epoch 174/300, resid Loss: 0.0582 | 0.0334
Epoch 175/300, resid Loss: 0.0582 | 0.0334
Epoch 176/300, resid Loss: 0.0582 | 0.0334
Epoch 177/300, resid Loss: 0.0582 | 0.0334
Epoch 178/300, resid Loss: 0.0582 | 0.0334
Epoch 179/300, resid Loss: 0.0582 | 0.0334
Epoch 180/300, resid Loss: 0.0582 | 0.0334
Epoch 181/300, resid Loss: 0.0582 | 0.0334
Epoch 182/300, resid Loss: 0.0582 | 0.0334
Epoch 183/300, resid Loss: 0.0582 | 0.0334
Epoch 184/300, resid Loss: 0.0582 | 0.0334
Epoch 185/300, resid Loss: 0.0582 | 0.0334
Epoch 186/300, resid Loss: 0.0582 | 0.0334
Epoch 187/300, resid Loss: 0.0582 | 0.0334
Epoch 188/300, resid Loss: 0.0582 | 0.0334
Epoch 189/300, resid Loss: 0.0582 | 0.0334
Epoch 190/300, resid Loss: 0.0582 | 0.0334
Epoch 191/300, resid Loss: 0.0582 | 0.0334
Epoch 192/300, resid Loss: 0.0582 | 0.0334
Epoch 193/300, resid Loss: 0.0582 | 0.0334
Epoch 194/300, resid Loss: 0.0582 | 0.0334
Epoch 195/300, resid Loss: 0.0582 | 0.0334
Epoch 196/300, resid Loss: 0.0582 | 0.0334
Epoch 197/300, resid Loss: 0.0582 | 0.0334
Epoch 198/300, resid Loss: 0.0582 | 0.0334
Epoch 199/300, resid Loss: 0.0582 | 0.0334
Epoch 200/300, resid Loss: 0.0582 | 0.0334
Epoch 201/300, resid Loss: 0.0582 | 0.0334
Epoch 202/300, resid Loss: 0.0582 | 0.0334
Epoch 203/300, resid Loss: 0.0582 | 0.0334
Epoch 204/300, resid Loss: 0.0582 | 0.0334
Epoch 205/300, resid Loss: 0.0582 | 0.0334
Epoch 206/300, resid Loss: 0.0582 | 0.0334
Epoch 207/300, resid Loss: 0.0582 | 0.0334
Epoch 208/300, resid Loss: 0.0582 | 0.0334
Epoch 209/300, resid Loss: 0.0582 | 0.0334
Epoch 210/300, resid Loss: 0.0582 | 0.0334
Epoch 211/300, resid Loss: 0.0582 | 0.0334
Epoch 212/300, resid Loss: 0.0582 | 0.0334
Epoch 213/300, resid Loss: 0.0582 | 0.0334
Epoch 214/300, resid Loss: 0.0582 | 0.0334
Epoch 215/300, resid Loss: 0.0582 | 0.0334
Epoch 216/300, resid Loss: 0.0582 | 0.0334
Epoch 217/300, resid Loss: 0.0582 | 0.0334
Epoch 218/300, resid Loss: 0.0582 | 0.0334
Epoch 219/300, resid Loss: 0.0582 | 0.0334
Epoch 220/300, resid Loss: 0.0582 | 0.0334
Epoch 221/300, resid Loss: 0.0582 | 0.0334
Epoch 222/300, resid Loss: 0.0582 | 0.0334
Epoch 223/300, resid Loss: 0.0582 | 0.0334
Epoch 224/300, resid Loss: 0.0582 | 0.0334
Epoch 225/300, resid Loss: 0.0582 | 0.0334
Epoch 226/300, resid Loss: 0.0582 | 0.0334
Epoch 227/300, resid Loss: 0.0581 | 0.0334
Epoch 228/300, resid Loss: 0.0581 | 0.0334
Epoch 229/300, resid Loss: 0.0581 | 0.0334
Epoch 230/300, resid Loss: 0.0581 | 0.0334
Epoch 231/300, resid Loss: 0.0581 | 0.0334
Epoch 232/300, resid Loss: 0.0581 | 0.0334
Epoch 233/300, resid Loss: 0.0581 | 0.0334
Epoch 234/300, resid Loss: 0.0581 | 0.0334
Epoch 235/300, resid Loss: 0.0581 | 0.0334
Epoch 236/300, resid Loss: 0.0581 | 0.0334
Epoch 237/300, resid Loss: 0.0581 | 0.0334
Epoch 238/300, resid Loss: 0.0581 | 0.0334
Epoch 239/300, resid Loss: 0.0581 | 0.0334
Epoch 240/300, resid Loss: 0.0581 | 0.0334
Epoch 241/300, resid Loss: 0.0581 | 0.0334
Epoch 242/300, resid Loss: 0.0581 | 0.0334
Epoch 243/300, resid Loss: 0.0581 | 0.0334
Epoch 244/300, resid Loss: 0.0581 | 0.0334
Epoch 245/300, resid Loss: 0.0581 | 0.0334
Epoch 246/300, resid Loss: 0.0581 | 0.0334
Epoch 247/300, resid Loss: 0.0581 | 0.0334
Epoch 248/300, resid Loss: 0.0581 | 0.0334
Epoch 249/300, resid Loss: 0.0581 | 0.0334
Epoch 250/300, resid Loss: 0.0581 | 0.0334
Epoch 251/300, resid Loss: 0.0581 | 0.0334
Epoch 252/300, resid Loss: 0.0581 | 0.0334
Epoch 253/300, resid Loss: 0.0581 | 0.0334
Epoch 254/300, resid Loss: 0.0581 | 0.0334
Epoch 255/300, resid Loss: 0.0581 | 0.0334
Epoch 256/300, resid Loss: 0.0581 | 0.0334
Epoch 257/300, resid Loss: 0.0581 | 0.0334
Epoch 258/300, resid Loss: 0.0581 | 0.0334
Epoch 259/300, resid Loss: 0.0581 | 0.0334
Epoch 260/300, resid Loss: 0.0581 | 0.0334
Epoch 261/300, resid Loss: 0.0581 | 0.0334
Epoch 262/300, resid Loss: 0.0581 | 0.0334
Epoch 263/300, resid Loss: 0.0581 | 0.0334
Epoch 264/300, resid Loss: 0.0581 | 0.0334
Epoch 265/300, resid Loss: 0.0581 | 0.0334
Epoch 266/300, resid Loss: 0.0581 | 0.0334
Epoch 267/300, resid Loss: 0.0581 | 0.0334
Epoch 268/300, resid Loss: 0.0581 | 0.0334
Epoch 269/300, resid Loss: 0.0581 | 0.0334
Epoch 270/300, resid Loss: 0.0581 | 0.0334
Epoch 271/300, resid Loss: 0.0581 | 0.0334
Epoch 272/300, resid Loss: 0.0581 | 0.0334
Epoch 273/300, resid Loss: 0.0581 | 0.0334
Epoch 274/300, resid Loss: 0.0581 | 0.0334
Epoch 275/300, resid Loss: 0.0581 | 0.0334
Epoch 276/300, resid Loss: 0.0581 | 0.0334
Epoch 277/300, resid Loss: 0.0581 | 0.0334
Epoch 278/300, resid Loss: 0.0581 | 0.0334
Epoch 279/300, resid Loss: 0.0581 | 0.0334
Epoch 280/300, resid Loss: 0.0581 | 0.0334
Epoch 281/300, resid Loss: 0.0581 | 0.0334
Epoch 282/300, resid Loss: 0.0581 | 0.0334
Epoch 283/300, resid Loss: 0.0581 | 0.0334
Epoch 284/300, resid Loss: 0.0581 | 0.0334
Epoch 285/300, resid Loss: 0.0581 | 0.0334
Epoch 286/300, resid Loss: 0.0581 | 0.0334
Epoch 287/300, resid Loss: 0.0581 | 0.0334
Epoch 288/300, resid Loss: 0.0581 | 0.0334
Epoch 289/300, resid Loss: 0.0581 | 0.0334
Epoch 290/300, resid Loss: 0.0581 | 0.0334
Epoch 291/300, resid Loss: 0.0581 | 0.0334
Epoch 292/300, resid Loss: 0.0581 | 0.0334
Epoch 293/300, resid Loss: 0.0581 | 0.0334
Epoch 294/300, resid Loss: 0.0581 | 0.0334
Epoch 295/300, resid Loss: 0.0581 | 0.0334
Epoch 296/300, resid Loss: 0.0581 | 0.0334
Epoch 297/300, resid Loss: 0.0581 | 0.0334
Epoch 298/300, resid Loss: 0.0581 | 0.0334
Epoch 299/300, resid Loss: 0.0581 | 0.0334
Epoch 300/300, resid Loss: 0.0581 | 0.0334
Runtime (seconds): 1678.0060894489288
0.000116284026956772
[153.56584]
[-0.95628285]
[-3.7948413]
[10.996524]
[2.8081684]
[8.644946]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 233.04023838136345
RMSE: 15.265655517578125
MAE: 15.265655517578125
R-squared: nan
[171.26434]
