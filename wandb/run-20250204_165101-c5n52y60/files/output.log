ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-04 16:51:05,678][0m A new study created in memory with name: no-name-0aec5f52-dd01-481c-8f01-34e80fa33815[0m
[32m[I 2025-02-04 16:52:55,837][0m Trial 0 finished with value: 0.8023853302001953 and parameters: {'observation_period_num': 95, 'train_rates': 0.9862969998172089, 'learning_rate': 5.7331729195695646e-06, 'batch_size': 115, 'step_size': 13, 'gamma': 0.8905320548379873}. Best is trial 0 with value: 0.8023853302001953.[0m
[32m[I 2025-02-04 16:56:49,773][0m Trial 1 finished with value: 0.7022154200588868 and parameters: {'observation_period_num': 214, 'train_rates': 0.7959312418657112, 'learning_rate': 0.000638946089767044, 'batch_size': 191, 'step_size': 1, 'gamma': 0.8920118369537369}. Best is trial 1 with value: 0.7022154200588868.[0m
[32m[I 2025-02-04 17:01:45,082][0m Trial 2 finished with value: 1.6063897609710693 and parameters: {'observation_period_num': 230, 'train_rates': 0.975733247602385, 'learning_rate': 1.4239750004930132e-06, 'batch_size': 231, 'step_size': 3, 'gamma': 0.9833724418307078}. Best is trial 1 with value: 0.7022154200588868.[0m
[32m[I 2025-02-04 17:05:58,260][0m Trial 3 finished with value: 0.23535077273845673 and parameters: {'observation_period_num': 201, 'train_rates': 0.9882947977356431, 'learning_rate': 7.392777652574522e-05, 'batch_size': 184, 'step_size': 14, 'gamma': 0.8378828012835864}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:11:05,948][0m Trial 4 finished with value: 0.9272456580812151 and parameters: {'observation_period_num': 244, 'train_rates': 0.8769944527441073, 'learning_rate': 3.061106133148981e-06, 'batch_size': 62, 'step_size': 11, 'gamma': 0.9325605047049225}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:12:36,412][0m Trial 5 finished with value: 0.2657251322301922 and parameters: {'observation_period_num': 35, 'train_rates': 0.8743486188016183, 'learning_rate': 0.00021737516399444407, 'batch_size': 72, 'step_size': 9, 'gamma': 0.7904450141898691}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:17:02,852][0m Trial 6 finished with value: 1.4171913428010101 and parameters: {'observation_period_num': 212, 'train_rates': 0.6478669799530238, 'learning_rate': 0.0008196481807039943, 'batch_size': 117, 'step_size': 14, 'gamma': 0.820980723182848}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:17:54,802][0m Trial 7 finished with value: 1.9145012634544976 and parameters: {'observation_period_num': 64, 'train_rates': 0.6495987108963, 'learning_rate': 4.073339651246845e-06, 'batch_size': 150, 'step_size': 9, 'gamma': 0.7524089216257591}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:19:05,888][0m Trial 8 finished with value: 0.8386492282765392 and parameters: {'observation_period_num': 22, 'train_rates': 0.9171789808044528, 'learning_rate': 1.9503289056609116e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.761633621448796}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:23:25,007][0m Trial 9 finished with value: 0.24962595105171204 and parameters: {'observation_period_num': 208, 'train_rates': 0.9742920950430962, 'learning_rate': 4.7218313765624886e-05, 'batch_size': 140, 'step_size': 15, 'gamma': 0.9592472601479339}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:25:56,773][0m Trial 10 finished with value: 0.939319146256293 and parameters: {'observation_period_num': 153, 'train_rates': 0.7342866701002213, 'learning_rate': 9.340802382481031e-05, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8411819114028442}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:28:55,349][0m Trial 11 finished with value: 0.39496553726325745 and parameters: {'observation_period_num': 160, 'train_rates': 0.9210861661596321, 'learning_rate': 2.9740035885668226e-05, 'batch_size': 167, 'step_size': 15, 'gamma': 0.9894422866730551}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:32:14,226][0m Trial 12 finished with value: 0.5011527535713676 and parameters: {'observation_period_num': 181, 'train_rates': 0.8338319444478872, 'learning_rate': 0.00010722871501236905, 'batch_size': 201, 'step_size': 12, 'gamma': 0.9304193723467392}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:34:10,874][0m Trial 13 finished with value: 0.6948948510231511 and parameters: {'observation_period_num': 108, 'train_rates': 0.9457028251898423, 'learning_rate': 1.2207609442921752e-05, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8415144212076975}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:37:50,790][0m Trial 14 finished with value: 0.8857945051736201 and parameters: {'observation_period_num': 190, 'train_rates': 0.7152586072331025, 'learning_rate': 6.450700283312617e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.9403598431001844}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:40:19,104][0m Trial 15 finished with value: 0.30142565376192165 and parameters: {'observation_period_num': 138, 'train_rates': 0.8714551761148429, 'learning_rate': 0.00024825786978834377, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8604192221271696}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:43:54,113][0m Trial 16 finished with value: 0.4960288405418396 and parameters: {'observation_period_num': 184, 'train_rates': 0.9491753998020592, 'learning_rate': 6.255659399864793e-05, 'batch_size': 219, 'step_size': 13, 'gamma': 0.7963722781393794}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:48:38,594][0m Trial 17 finished with value: 0.9184387046263705 and parameters: {'observation_period_num': 251, 'train_rates': 0.7861639561595719, 'learning_rate': 1.3857562538343538e-05, 'batch_size': 149, 'step_size': 10, 'gamma': 0.898403293542078}. Best is trial 3 with value: 0.23535077273845673.[0m
[32m[I 2025-02-04 17:53:01,527][0m Trial 18 finished with value: 0.19068914651870728 and parameters: {'observation_period_num': 205, 'train_rates': 0.9842267665874311, 'learning_rate': 0.00022703013944175507, 'batch_size': 89, 'step_size': 15, 'gamma': 0.9562342232739526}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 17:55:07,818][0m Trial 19 finished with value: 0.23792864300134614 and parameters: {'observation_period_num': 119, 'train_rates': 0.9106093537803277, 'learning_rate': 0.0003516140001209461, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8700051226888235}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 17:58:38,103][0m Trial 20 finished with value: 0.41068055420806726 and parameters: {'observation_period_num': 165, 'train_rates': 0.8260489540590872, 'learning_rate': 0.00011610757863842506, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8114287516037921}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:00:44,854][0m Trial 21 finished with value: 0.23863362842025118 and parameters: {'observation_period_num': 116, 'train_rates': 0.9158653085437782, 'learning_rate': 0.00035402913185477055, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8656506085676591}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:02:28,011][0m Trial 22 finished with value: 0.22906063497066498 and parameters: {'observation_period_num': 90, 'train_rates': 0.9883114755315299, 'learning_rate': 0.0004038466053710367, 'batch_size': 106, 'step_size': 14, 'gamma': 0.840544653607271}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:04:26,410][0m Trial 23 finished with value: 0.2016932332599667 and parameters: {'observation_period_num': 80, 'train_rates': 0.9507184243996876, 'learning_rate': 0.00018259352503886047, 'batch_size': 41, 'step_size': 14, 'gamma': 0.8358905469684831}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:06:21,036][0m Trial 24 finished with value: 0.2519395994611933 and parameters: {'observation_period_num': 82, 'train_rates': 0.945034924797495, 'learning_rate': 0.00018427964217850563, 'batch_size': 42, 'step_size': 12, 'gamma': 0.9111284628878378}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:08:04,310][0m Trial 25 finished with value: 0.45585132588585503 and parameters: {'observation_period_num': 62, 'train_rates': 0.9520669437903257, 'learning_rate': 0.0005324647749202218, 'batch_size': 47, 'step_size': 15, 'gamma': 0.7856632343621953}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:09:08,887][0m Trial 26 finished with value: 0.2889558014988202 and parameters: {'observation_period_num': 57, 'train_rates': 0.8818581307816813, 'learning_rate': 0.00015260805171377225, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8225098038412315}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:10:20,754][0m Trial 27 finished with value: 1.4701174504522767 and parameters: {'observation_period_num': 84, 'train_rates': 0.6092581826524655, 'learning_rate': 0.0009809895643534633, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8768461287339372}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:13:18,823][0m Trial 28 finished with value: 0.44685955737766464 and parameters: {'observation_period_num': 142, 'train_rates': 0.9595064444475108, 'learning_rate': 0.00041540387760189513, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9663037987088472}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:15:07,556][0m Trial 29 finished with value: 0.2190442681312561 and parameters: {'observation_period_num': 92, 'train_rates': 0.9885862842620601, 'learning_rate': 0.0003034633776091629, 'batch_size': 83, 'step_size': 14, 'gamma': 0.8562709640106648}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:17:01,387][0m Trial 30 finished with value: 0.3428391330704397 and parameters: {'observation_period_num': 101, 'train_rates': 0.8971563426721465, 'learning_rate': 3.787559013558793e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.911808067686206}. Best is trial 18 with value: 0.19068914651870728.[0m
[32m[I 2025-02-04 18:18:38,943][0m Trial 31 finished with value: 0.18906888365745544 and parameters: {'observation_period_num': 86, 'train_rates': 0.9860545214079959, 'learning_rate': 0.0002544489786878751, 'batch_size': 106, 'step_size': 14, 'gamma': 0.8551068523152001}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:19:37,860][0m Trial 32 finished with value: 0.29149309479441315 and parameters: {'observation_period_num': 48, 'train_rates': 0.9344045420907474, 'learning_rate': 0.0002486987730962281, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8543270641371058}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:20:56,624][0m Trial 33 finished with value: 0.290010541677475 and parameters: {'observation_period_num': 72, 'train_rates': 0.9690324468117903, 'learning_rate': 0.00014359104678713137, 'batch_size': 127, 'step_size': 15, 'gamma': 0.8835135427545131}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:23:23,558][0m Trial 34 finished with value: 2.40834903717041 and parameters: {'observation_period_num': 10, 'train_rates': 0.9895182511162969, 'learning_rate': 0.0006631055657854056, 'batch_size': 34, 'step_size': 12, 'gamma': 0.8518311926590275}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:26:08,190][0m Trial 35 finished with value: 0.26154904428281284 and parameters: {'observation_period_num': 130, 'train_rates': 0.9664973817077526, 'learning_rate': 0.00026603142449065266, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8272627420983681}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:27:25,171][0m Trial 36 finished with value: 0.3413122074276793 and parameters: {'observation_period_num': 39, 'train_rates': 0.9299947712051013, 'learning_rate': 0.0005106309406069211, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8937237363066829}. Best is trial 31 with value: 0.18906888365745544.[0m
[32m[I 2025-02-04 18:29:24,599][0m Trial 37 finished with value: 0.18487614393234253 and parameters: {'observation_period_num': 98, 'train_rates': 0.9898653025571045, 'learning_rate': 0.00020188813682337284, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9082329479360141}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:34:38,178][0m Trial 38 finished with value: 0.2685639186874851 and parameters: {'observation_period_num': 231, 'train_rates': 0.9665843942650298, 'learning_rate': 7.887074391942496e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9710568467723302}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:36:52,892][0m Trial 39 finished with value: 0.3737473781649948 and parameters: {'observation_period_num': 73, 'train_rates': 0.853303226368219, 'learning_rate': 0.00017665558933416096, 'batch_size': 33, 'step_size': 10, 'gamma': 0.9137274448327622}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:38:51,041][0m Trial 40 finished with value: 0.8220601108983697 and parameters: {'observation_period_num': 124, 'train_rates': 0.7746587368097162, 'learning_rate': 0.00013205146157763695, 'batch_size': 114, 'step_size': 10, 'gamma': 0.9355980503673595}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:40:49,291][0m Trial 41 finished with value: 0.2554764449596405 and parameters: {'observation_period_num': 104, 'train_rates': 0.9843717272339372, 'learning_rate': 0.0002862929040051474, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8801501495531743}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:42:41,607][0m Trial 42 finished with value: 1.1588708973422492 and parameters: {'observation_period_num': 95, 'train_rates': 0.8981182723593931, 'learning_rate': 1.1138043194946892e-06, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9511767234226808}. Best is trial 37 with value: 0.18487614393234253.[0m
Early stopping at epoch 59
[32m[I 2025-02-04 18:43:38,735][0m Trial 43 finished with value: 0.9384238347411156 and parameters: {'observation_period_num': 78, 'train_rates': 0.9723667791812421, 'learning_rate': 0.000710797973478801, 'batch_size': 70, 'step_size': 1, 'gamma': 0.8042771411660438}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:44:35,025][0m Trial 44 finished with value: 0.2648732297950321 and parameters: {'observation_period_num': 52, 'train_rates': 0.9410352631911509, 'learning_rate': 0.0001815318854644286, 'batch_size': 133, 'step_size': 14, 'gamma': 0.8302280438388994}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:46:41,416][0m Trial 45 finished with value: 0.36998844146728516 and parameters: {'observation_period_num': 108, 'train_rates': 0.9887775333140734, 'learning_rate': 0.000535487744855579, 'batch_size': 75, 'step_size': 15, 'gamma': 0.8511978933123308}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:48:22,372][0m Trial 46 finished with value: 1.1489882225126733 and parameters: {'observation_period_num': 91, 'train_rates': 0.9557711150980303, 'learning_rate': 2.512997321638015e-06, 'batch_size': 102, 'step_size': 11, 'gamma': 0.9042313679700976}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:49:42,337][0m Trial 47 finished with value: 0.47395071179367776 and parameters: {'observation_period_num': 70, 'train_rates': 0.92559861267197, 'learning_rate': 8.803922811959097e-05, 'batch_size': 87, 'step_size': 4, 'gamma': 0.7793039926156551}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:52:28,841][0m Trial 48 finished with value: 0.20185184565143308 and parameters: {'observation_period_num': 133, 'train_rates': 0.9757233571384669, 'learning_rate': 0.00020079910010924523, 'batch_size': 53, 'step_size': 14, 'gamma': 0.9250463583216705}. Best is trial 37 with value: 0.18487614393234253.[0m
[32m[I 2025-02-04 18:56:27,324][0m Trial 49 finished with value: 0.8278316463538986 and parameters: {'observation_period_num': 226, 'train_rates': 0.6936684511520808, 'learning_rate': 5.162859346345985e-05, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9283785557857445}. Best is trial 37 with value: 0.18487614393234253.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-04 18:56:27,332][0m A new study created in memory with name: no-name-b6a83e1f-bc67-4827-a9f8-61af1f4e9a95[0m
[32m[I 2025-02-04 18:58:15,028][0m Trial 0 finished with value: 0.3487120201117801 and parameters: {'observation_period_num': 97, 'train_rates': 0.9523302874843754, 'learning_rate': 8.298924551542032e-05, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9199174232906021}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:00:02,432][0m Trial 1 finished with value: 0.46298012137413025 and parameters: {'observation_period_num': 105, 'train_rates': 0.9235440675552431, 'learning_rate': 5.051002404660332e-05, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8965371853387255}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:00:43,041][0m Trial 2 finished with value: 1.832369195743346 and parameters: {'observation_period_num': 50, 'train_rates': 0.6084146880805509, 'learning_rate': 5.113816016502668e-06, 'batch_size': 172, 'step_size': 10, 'gamma': 0.8715416250291961}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:01:02,019][0m Trial 3 finished with value: 1.7172849676237423 and parameters: {'observation_period_num': 11, 'train_rates': 0.6117324481269858, 'learning_rate': 8.641988251337676e-06, 'batch_size': 203, 'step_size': 2, 'gamma': 0.942551216850064}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:02:24,953][0m Trial 4 finished with value: 1.7666025255855762 and parameters: {'observation_period_num': 83, 'train_rates': 0.6622778111143036, 'learning_rate': 5.111825581023352e-06, 'batch_size': 51, 'step_size': 2, 'gamma': 0.9032433862931573}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:04:19,789][0m Trial 5 finished with value: 1.192300305297122 and parameters: {'observation_period_num': 118, 'train_rates': 0.8301924112303719, 'learning_rate': 8.395394945482107e-06, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8467078918739979}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:05:21,045][0m Trial 6 finished with value: 0.9655145870857551 and parameters: {'observation_period_num': 27, 'train_rates': 0.9165851621349338, 'learning_rate': 1.164561489878876e-05, 'batch_size': 81, 'step_size': 4, 'gamma': 0.8127788315037182}. Best is trial 0 with value: 0.3487120201117801.[0m
[32m[I 2025-02-04 19:09:32,961][0m Trial 7 finished with value: 0.19803692688865046 and parameters: {'observation_period_num': 143, 'train_rates': 0.9782150061392139, 'learning_rate': 7.4190298581322e-05, 'batch_size': 19, 'step_size': 14, 'gamma': 0.9730590022010237}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:09:50,707][0m Trial 8 finished with value: 0.6570974066111644 and parameters: {'observation_period_num': 5, 'train_rates': 0.7613204487653141, 'learning_rate': 0.00015273776387595, 'batch_size': 254, 'step_size': 11, 'gamma': 0.8644947570339617}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:12:15,918][0m Trial 9 finished with value: 1.4101081076983748 and parameters: {'observation_period_num': 150, 'train_rates': 0.7411472698411454, 'learning_rate': 1.0494813537055793e-05, 'batch_size': 236, 'step_size': 9, 'gamma': 0.8848024872558189}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:16:52,326][0m Trial 10 finished with value: 2.219308240231118 and parameters: {'observation_period_num': 214, 'train_rates': 0.8426426276020125, 'learning_rate': 0.0009674392366646903, 'batch_size': 24, 'step_size': 14, 'gamma': 0.9851538958732556}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:19:55,136][0m Trial 11 finished with value: 0.33499815409833733 and parameters: {'observation_period_num': 154, 'train_rates': 0.9606736798956153, 'learning_rate': 0.00011189064052215683, 'batch_size': 109, 'step_size': 6, 'gamma': 0.9841982623799094}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:23:28,371][0m Trial 12 finished with value: 0.21828994154930115 and parameters: {'observation_period_num': 173, 'train_rates': 0.9870018781560191, 'learning_rate': 0.000279445633386806, 'batch_size': 131, 'step_size': 7, 'gamma': 0.9813649613348806}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:27:33,999][0m Trial 13 finished with value: 2.006256103515625 and parameters: {'observation_period_num': 196, 'train_rates': 0.9859220041205968, 'learning_rate': 1.120066226733645e-06, 'batch_size': 170, 'step_size': 7, 'gamma': 0.7609239873152185}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:32:38,789][0m Trial 14 finished with value: 0.3840460195598832 and parameters: {'observation_period_num': 249, 'train_rates': 0.877379899969544, 'learning_rate': 0.00041226021857156993, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9496693217956659}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:36:33,659][0m Trial 15 finished with value: 0.7957505880877739 and parameters: {'observation_period_num': 173, 'train_rates': 0.8820123701368808, 'learning_rate': 0.000385922959297258, 'batch_size': 22, 'step_size': 8, 'gamma': 0.9509658593577914}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:40:52,755][0m Trial 16 finished with value: 0.20916534960269928 and parameters: {'observation_period_num': 201, 'train_rates': 0.9771552834904833, 'learning_rate': 3.466956287317573e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9886462708257073}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:45:48,230][0m Trial 17 finished with value: 0.300669913346342 and parameters: {'observation_period_num': 232, 'train_rates': 0.9115349453323109, 'learning_rate': 3.0076480375635518e-05, 'batch_size': 60, 'step_size': 12, 'gamma': 0.9271888632943948}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:49:38,777][0m Trial 18 finished with value: 0.461056375547367 and parameters: {'observation_period_num': 201, 'train_rates': 0.8024299541551587, 'learning_rate': 3.718656476389643e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9673903243333335}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:53:40,006][0m Trial 19 finished with value: 0.3687192093867522 and parameters: {'observation_period_num': 73, 'train_rates': 0.8647671632373158, 'learning_rate': 1.9277652925674602e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8245877911341926}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 19:56:04,089][0m Trial 20 finished with value: 0.7095137020666822 and parameters: {'observation_period_num': 137, 'train_rates': 0.7532464552479101, 'learning_rate': 6.383322857944107e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.7605301375591228}. Best is trial 7 with value: 0.19803692688865046.[0m
[32m[I 2025-02-04 20:00:04,700][0m Trial 21 finished with value: 0.16414666175842285 and parameters: {'observation_period_num': 182, 'train_rates': 0.9891594050356957, 'learning_rate': 0.00022693503580139684, 'batch_size': 42, 'step_size': 9, 'gamma': 0.9846544124217469}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:03:47,542][0m Trial 22 finished with value: 0.24719718098640442 and parameters: {'observation_period_num': 177, 'train_rates': 0.9424168961398205, 'learning_rate': 0.0001910684670408983, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9591409539445216}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:08:42,700][0m Trial 23 finished with value: 0.3559268116950989 and parameters: {'observation_period_num': 217, 'train_rates': 0.9890483088234279, 'learning_rate': 0.000647823893840077, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9339798195374263}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:11:27,005][0m Trial 24 finished with value: 0.3195571226475502 and parameters: {'observation_period_num': 134, 'train_rates': 0.9005380336927804, 'learning_rate': 2.3547382933482958e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.9666189945360856}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:15:17,934][0m Trial 25 finished with value: 0.35234780636456636 and parameters: {'observation_period_num': 191, 'train_rates': 0.9599347473543373, 'learning_rate': 0.00012232067369032052, 'batch_size': 100, 'step_size': 14, 'gamma': 0.9883478080869096}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:18:49,973][0m Trial 26 finished with value: 0.25685963762360947 and parameters: {'observation_period_num': 159, 'train_rates': 0.9385300410849859, 'learning_rate': 0.00020351434514086244, 'batch_size': 32, 'step_size': 11, 'gamma': 0.9193442954793272}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:23:24,720][0m Trial 27 finished with value: 0.8188087153586613 and parameters: {'observation_period_num': 252, 'train_rates': 0.7093545852066894, 'learning_rate': 7.511194638173544e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.966441084113485}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:28:11,929][0m Trial 28 finished with value: 0.2367042899131775 and parameters: {'observation_period_num': 223, 'train_rates': 0.9785485970239884, 'learning_rate': 4.1255430229360456e-05, 'batch_size': 118, 'step_size': 12, 'gamma': 0.9707037008474093}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:30:31,778][0m Trial 29 finished with value: 0.3316138605192556 and parameters: {'observation_period_num': 121, 'train_rates': 0.9583578742947666, 'learning_rate': 8.121475646263286e-05, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9125813115957773}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:34:24,726][0m Trial 30 finished with value: 0.37392491527966093 and parameters: {'observation_period_num': 184, 'train_rates': 0.9342333884279423, 'learning_rate': 1.7257271210934635e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9406648227449351}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:37:49,917][0m Trial 31 finished with value: 0.1976514309644699 and parameters: {'observation_period_num': 170, 'train_rates': 0.9890172639565215, 'learning_rate': 0.000309054726176176, 'batch_size': 143, 'step_size': 7, 'gamma': 0.988575885264158}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:40:51,209][0m Trial 32 finished with value: 0.2725715637207031 and parameters: {'observation_period_num': 155, 'train_rates': 0.9592767938451506, 'learning_rate': 0.0002890579468657679, 'batch_size': 154, 'step_size': 4, 'gamma': 0.9785403826783627}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:45:32,126][0m Trial 33 finished with value: 0.2695714641308439 and parameters: {'observation_period_num': 104, 'train_rates': 0.9030499361422905, 'learning_rate': 0.00010334005237146256, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9515968896436108}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:49:39,683][0m Trial 34 finished with value: 0.29938483519079795 and parameters: {'observation_period_num': 205, 'train_rates': 0.930584728351226, 'learning_rate': 0.0005582345654009342, 'batch_size': 161, 'step_size': 15, 'gamma': 0.9883343739646229}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:53:10,718][0m Trial 35 finished with value: 0.37830597162246704 and parameters: {'observation_period_num': 177, 'train_rates': 0.9617389496762221, 'learning_rate': 5.464904254462739e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.9628125677405324}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:55:58,495][0m Trial 36 finished with value: 0.2190648764371872 and parameters: {'observation_period_num': 140, 'train_rates': 0.9723036063186066, 'learning_rate': 0.00019670649168782142, 'batch_size': 91, 'step_size': 10, 'gamma': 0.932721531537208}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:58:25,861][0m Trial 37 finished with value: 1.051991257402632 and parameters: {'observation_period_num': 164, 'train_rates': 0.6641007510064862, 'learning_rate': 0.0009170213406891513, 'batch_size': 208, 'step_size': 1, 'gamma': 0.9763365907287136}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 20:59:59,416][0m Trial 38 finished with value: 1.2210870020919375 and parameters: {'observation_period_num': 89, 'train_rates': 0.9245236942997049, 'learning_rate': 4.08699827752201e-06, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9569475954328595}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 21:05:13,290][0m Trial 39 finished with value: 0.23816757850550316 and parameters: {'observation_period_num': 237, 'train_rates': 0.945620125442346, 'learning_rate': 0.00028392585578932436, 'batch_size': 50, 'step_size': 6, 'gamma': 0.9050313616903107}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 21:07:17,438][0m Trial 40 finished with value: 0.42998087552727243 and parameters: {'observation_period_num': 120, 'train_rates': 0.8275621178282692, 'learning_rate': 0.0001589969824082033, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7912116464756809}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 21:10:46,736][0m Trial 41 finished with value: 0.17815673351287842 and parameters: {'observation_period_num': 174, 'train_rates': 0.9893052934564982, 'learning_rate': 0.00029198994259538796, 'batch_size': 134, 'step_size': 7, 'gamma': 0.9755099265938639}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 21:13:35,206][0m Trial 42 finished with value: 0.2962099611759186 and parameters: {'observation_period_num': 144, 'train_rates': 0.9704681413625142, 'learning_rate': 0.0004938080396633112, 'batch_size': 116, 'step_size': 8, 'gamma': 0.975604683081138}. Best is trial 21 with value: 0.16414666175842285.[0m
[32m[I 2025-02-04 21:17:22,246][0m Trial 43 finished with value: 0.15871120989322662 and parameters: {'observation_period_num': 188, 'train_rates': 0.9886245555957569, 'learning_rate': 0.00033816019993174694, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9407271649332751}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:20:46,603][0m Trial 44 finished with value: 0.2273922562599182 and parameters: {'observation_period_num': 167, 'train_rates': 0.9886474782296378, 'learning_rate': 0.00031911378791014185, 'batch_size': 226, 'step_size': 3, 'gamma': 0.8880036066193031}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:23:38,412][0m Trial 45 finished with value: 0.26738643646240234 and parameters: {'observation_period_num': 149, 'train_rates': 0.9443270080412337, 'learning_rate': 0.0006894136944032073, 'batch_size': 199, 'step_size': 6, 'gamma': 0.9414360662649659}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:27:12,658][0m Trial 46 finished with value: 0.28211698339921293 and parameters: {'observation_period_num': 186, 'train_rates': 0.9227743647543512, 'learning_rate': 0.000224911124339694, 'batch_size': 189, 'step_size': 5, 'gamma': 0.9465521154070181}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:31:36,982][0m Trial 47 finished with value: 0.30447494983673096 and parameters: {'observation_period_num': 210, 'train_rates': 0.9698746164653753, 'learning_rate': 0.00011923434517194812, 'batch_size': 144, 'step_size': 7, 'gamma': 0.9747867102767016}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:35:22,516][0m Trial 48 finished with value: 0.2184077650308609 and parameters: {'observation_period_num': 186, 'train_rates': 0.9896308193051585, 'learning_rate': 0.00037235454624673397, 'batch_size': 176, 'step_size': 4, 'gamma': 0.8724344328398131}. Best is trial 43 with value: 0.15871120989322662.[0m
[32m[I 2025-02-04 21:37:30,246][0m Trial 49 finished with value: 0.38597854835088136 and parameters: {'observation_period_num': 126, 'train_rates': 0.8922400935872812, 'learning_rate': 0.00014486213125713753, 'batch_size': 223, 'step_size': 6, 'gamma': 0.8391390137898008}. Best is trial 43 with value: 0.15871120989322662.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-04 21:37:30,253][0m A new study created in memory with name: no-name-e32745f4-5a98-4ff0-b551-e8a7cbbc069b[0m
[32m[I 2025-02-04 21:40:04,267][0m Trial 0 finished with value: 1.4417348521096365 and parameters: {'observation_period_num': 141, 'train_rates': 0.8757019346088528, 'learning_rate': 3.1163678768294145e-06, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8479477624479117}. Best is trial 0 with value: 1.4417348521096365.[0m
[32m[I 2025-02-04 21:40:54,845][0m Trial 1 finished with value: 1.0060724049686263 and parameters: {'observation_period_num': 59, 'train_rates': 0.7430456717500272, 'learning_rate': 2.0077597146199656e-05, 'batch_size': 219, 'step_size': 10, 'gamma': 0.834132328716561}. Best is trial 1 with value: 1.0060724049686263.[0m
[32m[I 2025-02-04 21:43:08,052][0m Trial 2 finished with value: 0.9952465459759794 and parameters: {'observation_period_num': 144, 'train_rates': 0.7126027593828432, 'learning_rate': 0.0009814044430618057, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8052735479111107}. Best is trial 2 with value: 0.9952465459759794.[0m
[32m[I 2025-02-04 21:46:47,506][0m Trial 3 finished with value: 1.9378119707107544 and parameters: {'observation_period_num': 181, 'train_rates': 0.9694426042055863, 'learning_rate': 1.1588472872617655e-06, 'batch_size': 192, 'step_size': 13, 'gamma': 0.8858635706305942}. Best is trial 2 with value: 0.9952465459759794.[0m
[32m[I 2025-02-04 21:48:22,249][0m Trial 4 finished with value: 0.7767873606720909 and parameters: {'observation_period_num': 89, 'train_rates': 0.8294946808838897, 'learning_rate': 0.0008389215184207609, 'batch_size': 76, 'step_size': 7, 'gamma': 0.7564133807818441}. Best is trial 4 with value: 0.7767873606720909.[0m
[32m[I 2025-02-04 21:49:18,101][0m Trial 5 finished with value: 1.2432076524202478 and parameters: {'observation_period_num': 58, 'train_rates': 0.8835773082043764, 'learning_rate': 1.4903820285368738e-05, 'batch_size': 251, 'step_size': 3, 'gamma': 0.7645502152985277}. Best is trial 4 with value: 0.7767873606720909.[0m
[32m[I 2025-02-04 21:52:27,729][0m Trial 6 finished with value: 0.8243784308433533 and parameters: {'observation_period_num': 145, 'train_rates': 0.9818964437900966, 'learning_rate': 2.8198629829161005e-06, 'batch_size': 51, 'step_size': 4, 'gamma': 0.9664670560415533}. Best is trial 4 with value: 0.7767873606720909.[0m
[32m[I 2025-02-04 21:54:56,736][0m Trial 7 finished with value: 1.4233148241303657 and parameters: {'observation_period_num': 167, 'train_rates': 0.6055894985245436, 'learning_rate': 0.0001066631404312891, 'batch_size': 229, 'step_size': 2, 'gamma': 0.7951767454013832}. Best is trial 4 with value: 0.7767873606720909.[0m
[32m[I 2025-02-04 21:55:24,310][0m Trial 8 finished with value: 0.4172024428844452 and parameters: {'observation_period_num': 16, 'train_rates': 0.9811314705160158, 'learning_rate': 0.0008320233715939703, 'batch_size': 195, 'step_size': 15, 'gamma': 0.9482028839936065}. Best is trial 8 with value: 0.4172024428844452.[0m
[32m[I 2025-02-04 21:57:06,741][0m Trial 9 finished with value: 1.38653813373979 and parameters: {'observation_period_num': 127, 'train_rates': 0.6012348753767063, 'learning_rate': 3.0198953343243644e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8074944930987494}. Best is trial 8 with value: 0.4172024428844452.[0m
[32m[I 2025-02-04 21:57:44,869][0m Trial 10 finished with value: 0.2560415104323742 and parameters: {'observation_period_num': 11, 'train_rates': 0.9307025259326419, 'learning_rate': 0.00018009321279727054, 'batch_size': 134, 'step_size': 15, 'gamma': 0.9833829695958379}. Best is trial 10 with value: 0.2560415104323742.[0m
[32m[I 2025-02-04 21:58:17,549][0m Trial 11 finished with value: 0.28502569910956593 and parameters: {'observation_period_num': 5, 'train_rates': 0.9446047988332197, 'learning_rate': 0.00018911523874716929, 'batch_size': 157, 'step_size': 15, 'gamma': 0.9892405091587488}. Best is trial 10 with value: 0.2560415104323742.[0m
[32m[I 2025-02-04 21:58:58,734][0m Trial 12 finished with value: 0.23475082207730402 and parameters: {'observation_period_num': 5, 'train_rates': 0.9073023759676262, 'learning_rate': 0.00015818350484900564, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9171575376761518}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:03:49,561][0m Trial 13 finished with value: 0.26455314887257725 and parameters: {'observation_period_num': 235, 'train_rates': 0.90428761042814, 'learning_rate': 0.00016470009277511438, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9128051730892646}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:04:37,019][0m Trial 14 finished with value: 0.446350442224674 and parameters: {'observation_period_num': 43, 'train_rates': 0.8164305205869518, 'learning_rate': 7.451183678952898e-05, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9266141715736325}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:06:36,983][0m Trial 15 finished with value: 0.27794423382332983 and parameters: {'observation_period_num': 94, 'train_rates': 0.9180636987687957, 'learning_rate': 0.0003022627165237202, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8919636756087709}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:07:30,149][0m Trial 16 finished with value: 0.39213190600275993 and parameters: {'observation_period_num': 28, 'train_rates': 0.8523247681262487, 'learning_rate': 6.16196639077771e-05, 'batch_size': 87, 'step_size': 9, 'gamma': 0.9852597282895714}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:08:54,709][0m Trial 17 finished with value: 0.6400622649047211 and parameters: {'observation_period_num': 89, 'train_rates': 0.7711571927666337, 'learning_rate': 0.00036006279544246073, 'batch_size': 169, 'step_size': 13, 'gamma': 0.9463324251805921}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:14:17,116][0m Trial 18 finished with value: 0.31189459446304574 and parameters: {'observation_period_num': 245, 'train_rates': 0.9385105620789158, 'learning_rate': 4.494165264357517e-05, 'batch_size': 67, 'step_size': 13, 'gamma': 0.9207562730980073}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:15:03,988][0m Trial 19 finished with value: 0.9649532034486001 and parameters: {'observation_period_num': 41, 'train_rates': 0.7880247094726311, 'learning_rate': 1.1229300217494464e-05, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8643745416428089}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:15:27,596][0m Trial 20 finished with value: 0.9491139741794847 and parameters: {'observation_period_num': 5, 'train_rates': 0.6880230417707669, 'learning_rate': 0.00037111938349114845, 'batch_size': 181, 'step_size': 15, 'gamma': 0.9622874532140457}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:19:30,331][0m Trial 21 finished with value: 0.25915629267692564 and parameters: {'observation_period_num': 201, 'train_rates': 0.9074230906381032, 'learning_rate': 0.000129339674638465, 'batch_size': 102, 'step_size': 11, 'gamma': 0.9131856486566614}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:24:22,347][0m Trial 22 finished with value: 0.35115071612855664 and parameters: {'observation_period_num': 211, 'train_rates': 0.8741341346937805, 'learning_rate': 0.00015089051946041944, 'batch_size': 22, 'step_size': 11, 'gamma': 0.8996814073696687}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:28:55,817][0m Trial 23 finished with value: 0.31229880065299 and parameters: {'observation_period_num': 220, 'train_rates': 0.9241644911853574, 'learning_rate': 9.526935849443997e-05, 'batch_size': 133, 'step_size': 12, 'gamma': 0.9336794275272481}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:32:41,056][0m Trial 24 finished with value: 0.38620626764452975 and parameters: {'observation_period_num': 196, 'train_rates': 0.849861171733377, 'learning_rate': 0.0002690510864047632, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8704409382120966}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:34:39,244][0m Trial 25 finished with value: 0.2754408700825417 and parameters: {'observation_period_num': 114, 'train_rates': 0.8971760785778067, 'learning_rate': 0.00045359384838889957, 'batch_size': 151, 'step_size': 9, 'gamma': 0.9056604896591047}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:35:50,836][0m Trial 26 finished with value: 0.33064485331104226 and parameters: {'observation_period_num': 64, 'train_rates': 0.9494937783428351, 'learning_rate': 3.907973201553328e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9683700571464346}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:36:29,706][0m Trial 27 finished with value: 0.38582783491122274 and parameters: {'observation_period_num': 32, 'train_rates': 0.8547833429890789, 'learning_rate': 0.0001400407001490357, 'batch_size': 131, 'step_size': 14, 'gamma': 0.9357925150206771}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:39:35,491][0m Trial 28 finished with value: 0.6016933302993092 and parameters: {'observation_period_num': 171, 'train_rates': 0.8190754877905468, 'learning_rate': 0.0005334852217544466, 'batch_size': 94, 'step_size': 10, 'gamma': 0.883805396299656}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:41:23,033][0m Trial 29 finished with value: 0.8640330409417387 and parameters: {'observation_period_num': 106, 'train_rates': 0.8716338696251935, 'learning_rate': 7.98280812565269e-06, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9513804383175396}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:42:54,396][0m Trial 30 finished with value: 0.25707744738134014 and parameters: {'observation_period_num': 71, 'train_rates': 0.958856217786241, 'learning_rate': 0.00022037183813245918, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8480827814882417}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:44:30,360][0m Trial 31 finished with value: 0.25950230892718307 and parameters: {'observation_period_num': 76, 'train_rates': 0.955960873807056, 'learning_rate': 0.00021398989739552238, 'batch_size': 56, 'step_size': 5, 'gamma': 0.8253134848985149}. Best is trial 12 with value: 0.23475082207730402.[0m
[32m[I 2025-02-04 22:48:36,218][0m Trial 32 finished with value: 0.22149958365772623 and parameters: {'observation_period_num': 20, 'train_rates': 0.915237098231959, 'learning_rate': 7.378012325545223e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.8439421450256546}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 22:53:28,077][0m Trial 33 finished with value: 0.23305887086638089 and parameters: {'observation_period_num': 24, 'train_rates': 0.9208237832318978, 'learning_rate': 6.920183939238473e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8456413029765648}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 22:57:24,997][0m Trial 34 finished with value: 0.30515979275558935 and parameters: {'observation_period_num': 21, 'train_rates': 0.9324533386665171, 'learning_rate': 2.7648289255366286e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.8468362684276006}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 22:59:29,030][0m Trial 35 finished with value: 0.36204413614075653 and parameters: {'observation_period_num': 48, 'train_rates': 0.8918375377902219, 'learning_rate': 6.021978930778181e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.8242768739932987}. Best is trial 32 with value: 0.22149958365772623.[0m
Early stopping at epoch 81
[32m[I 2025-02-04 22:59:50,952][0m Trial 36 finished with value: 1.001459109936661 and parameters: {'observation_period_num': 17, 'train_rates': 0.9208760584334211, 'learning_rate': 8.477733258353853e-05, 'batch_size': 207, 'step_size': 1, 'gamma': 0.8563087734601139}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:02:22,389][0m Trial 37 finished with value: 0.2882148272850934 and parameters: {'observation_period_num': 34, 'train_rates': 0.9884158117981903, 'learning_rate': 2.282451879405223e-05, 'batch_size': 33, 'step_size': 6, 'gamma': 0.788138213847166}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:04:51,239][0m Trial 38 finished with value: 0.8434315269710096 and parameters: {'observation_period_num': 49, 'train_rates': 0.7191838589551982, 'learning_rate': 4.841064118793037e-05, 'batch_size': 26, 'step_size': 4, 'gamma': 0.8798285629447877}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:05:57,832][0m Trial 39 finished with value: 0.9577089096735982 and parameters: {'observation_period_num': 6, 'train_rates': 0.9650925610918637, 'learning_rate': 1.7278262525440412e-05, 'batch_size': 75, 'step_size': 2, 'gamma': 0.828495299172155}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:10:37,056][0m Trial 40 finished with value: 0.48927765636774845 and parameters: {'observation_period_num': 55, 'train_rates': 0.8807982159055463, 'learning_rate': 6.102154520848922e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.815077815318824}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:12:17,503][0m Trial 41 finished with value: 0.2729991717884938 and parameters: {'observation_period_num': 68, 'train_rates': 0.966822859751622, 'learning_rate': 0.00023520613930658006, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8421899781510209}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:13:38,569][0m Trial 42 finished with value: 0.2558262955821203 and parameters: {'observation_period_num': 23, 'train_rates': 0.9429467078453317, 'learning_rate': 0.00010444362461794598, 'batch_size': 61, 'step_size': 6, 'gamma': 0.8528376684306758}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:15:18,796][0m Trial 43 finished with value: 0.28199423241258115 and parameters: {'observation_period_num': 21, 'train_rates': 0.9090293788937944, 'learning_rate': 0.00010564967390597752, 'batch_size': 47, 'step_size': 3, 'gamma': 0.8684439213584799}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:16:00,233][0m Trial 44 finished with value: 0.41175973250721043 and parameters: {'observation_period_num': 33, 'train_rates': 0.9381624672969366, 'learning_rate': 6.896630614141231e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.836895901320588}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:17:03,556][0m Trial 45 finished with value: 1.9775538265705108 and parameters: {'observation_period_num': 18, 'train_rates': 0.8638605566854829, 'learning_rate': 1.0758711821565177e-06, 'batch_size': 73, 'step_size': 5, 'gamma': 0.7805707698834252}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:19:26,321][0m Trial 46 finished with value: 0.3619887654776456 and parameters: {'observation_period_num': 5, 'train_rates': 0.8344251067776995, 'learning_rate': 0.0001047631769173964, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8616695823435276}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:20:02,522][0m Trial 47 finished with value: 1.2640178316880848 and parameters: {'observation_period_num': 39, 'train_rates': 0.6675033960355834, 'learning_rate': 3.582483627030734e-05, 'batch_size': 161, 'step_size': 3, 'gamma': 0.8070240712186649}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:21:55,476][0m Trial 48 finished with value: 0.6613348722457886 and parameters: {'observation_period_num': 23, 'train_rates': 0.9729194836315825, 'learning_rate': 0.0006092687533687845, 'batch_size': 44, 'step_size': 4, 'gamma': 0.8543529359108631}. Best is trial 32 with value: 0.22149958365772623.[0m
[32m[I 2025-02-04 23:22:49,220][0m Trial 49 finished with value: 1.5918594273653897 and parameters: {'observation_period_num': 53, 'train_rates': 0.9205037409281855, 'learning_rate': 2.1192190301840994e-06, 'batch_size': 172, 'step_size': 6, 'gamma': 0.8748145686364616}. Best is trial 32 with value: 0.22149958365772623.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-04 23:22:49,227][0m A new study created in memory with name: no-name-942c627b-cc2d-4c74-a0c3-a916c817a6d3[0m
[32m[I 2025-02-04 23:25:36,034][0m Trial 0 finished with value: 1.0478930998041953 and parameters: {'observation_period_num': 185, 'train_rates': 0.6437454046867286, 'learning_rate': 0.0008877319276315515, 'batch_size': 240, 'step_size': 8, 'gamma': 0.7869433816659691}. Best is trial 0 with value: 1.0478930998041953.[0m
[32m[I 2025-02-04 23:27:48,089][0m Trial 1 finished with value: 1.9869774223922134 and parameters: {'observation_period_num': 144, 'train_rates': 0.6710383439128833, 'learning_rate': 1.2878802290874146e-06, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8759568409332286}. Best is trial 0 with value: 1.0478930998041953.[0m
[32m[I 2025-02-04 23:28:31,256][0m Trial 2 finished with value: 1.2007154510795583 and parameters: {'observation_period_num': 49, 'train_rates': 0.7040869285321651, 'learning_rate': 1.8437068248110152e-05, 'batch_size': 240, 'step_size': 10, 'gamma': 0.7678441471286213}. Best is trial 0 with value: 1.0478930998041953.[0m
[32m[I 2025-02-04 23:30:26,340][0m Trial 3 finished with value: 0.9489637154799241 and parameters: {'observation_period_num': 107, 'train_rates': 0.9271466361655359, 'learning_rate': 1.4096627995231875e-05, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8040406431340984}. Best is trial 3 with value: 0.9489637154799241.[0m
[32m[I 2025-02-04 23:34:20,277][0m Trial 4 finished with value: 1.0997166093969029 and parameters: {'observation_period_num': 239, 'train_rates': 0.6112371576311989, 'learning_rate': 8.49152691517653e-05, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8064428760241069}. Best is trial 3 with value: 0.9489637154799241.[0m
[32m[I 2025-02-04 23:37:11,109][0m Trial 5 finished with value: 1.9667737546936106 and parameters: {'observation_period_num': 184, 'train_rates': 0.6854301110652445, 'learning_rate': 1.02158753704533e-06, 'batch_size': 174, 'step_size': 10, 'gamma': 0.9261427389133473}. Best is trial 3 with value: 0.9489637154799241.[0m
[32m[I 2025-02-04 23:39:34,331][0m Trial 6 finished with value: 1.1349839564403004 and parameters: {'observation_period_num': 61, 'train_rates': 0.7378393002478654, 'learning_rate': 0.00047140142706747365, 'batch_size': 27, 'step_size': 7, 'gamma': 0.9392131517999014}. Best is trial 3 with value: 0.9489637154799241.[0m
[32m[I 2025-02-04 23:42:02,316][0m Trial 7 finished with value: 0.8838516164497034 and parameters: {'observation_period_num': 146, 'train_rates': 0.7309853983451502, 'learning_rate': 3.0572576911816616e-05, 'batch_size': 71, 'step_size': 15, 'gamma': 0.9034543674284116}. Best is trial 7 with value: 0.8838516164497034.[0m
Early stopping at epoch 34
[32m[I 2025-02-04 23:42:49,512][0m Trial 8 finished with value: 2.4884629249572754 and parameters: {'observation_period_num': 118, 'train_rates': 0.972239585762834, 'learning_rate': 1.1787806133861576e-06, 'batch_size': 87, 'step_size': 1, 'gamma': 0.7538709166250606}. Best is trial 7 with value: 0.8838516164497034.[0m
[32m[I 2025-02-04 23:44:46,094][0m Trial 9 finished with value: 1.3269537460705465 and parameters: {'observation_period_num': 120, 'train_rates': 0.8292784567441236, 'learning_rate': 2.4067575335356107e-06, 'batch_size': 213, 'step_size': 6, 'gamma': 0.9565824734622559}. Best is trial 7 with value: 0.8838516164497034.[0m
[32m[I 2025-02-04 23:48:18,764][0m Trial 10 finished with value: 0.46528773952503594 and parameters: {'observation_period_num': 8, 'train_rates': 0.8168221462420565, 'learning_rate': 0.00011219865845521537, 'batch_size': 20, 'step_size': 15, 'gamma': 0.879360500656778}. Best is trial 10 with value: 0.46528773952503594.[0m
[32m[I 2025-02-04 23:52:41,533][0m Trial 11 finished with value: 0.3949395058429347 and parameters: {'observation_period_num': 13, 'train_rates': 0.8078724978182761, 'learning_rate': 0.00010245582066967983, 'batch_size': 16, 'step_size': 15, 'gamma': 0.872318130049989}. Best is trial 11 with value: 0.3949395058429347.[0m
[32m[I 2025-02-04 23:56:53,888][0m Trial 12 finished with value: 0.5175853284086834 and parameters: {'observation_period_num': 9, 'train_rates': 0.8271056612367311, 'learning_rate': 0.00013917050428345173, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8441651586084022}. Best is trial 11 with value: 0.3949395058429347.[0m
[32m[I 2025-02-04 23:58:21,385][0m Trial 13 finished with value: 0.24671301599542825 and parameters: {'observation_period_num': 5, 'train_rates': 0.8716372730356945, 'learning_rate': 0.00015903375139800707, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8593481199158899}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-04 23:59:38,974][0m Trial 14 finished with value: 0.2930647100566737 and parameters: {'observation_period_num': 49, 'train_rates': 0.886118702421655, 'learning_rate': 0.00030048892204495223, 'batch_size': 60, 'step_size': 12, 'gamma': 0.8448916351942711}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:01:03,261][0m Trial 15 finished with value: 0.2676779343968346 and parameters: {'observation_period_num': 63, 'train_rates': 0.8984492034807904, 'learning_rate': 0.00029228297499515356, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8329040666003792}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:02:24,612][0m Trial 16 finished with value: 0.32844713034286893 and parameters: {'observation_period_num': 78, 'train_rates': 0.8986765361457447, 'learning_rate': 0.00025980346733241774, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9864298222635286}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:04:11,460][0m Trial 17 finished with value: 0.2512254543878414 and parameters: {'observation_period_num': 79, 'train_rates': 0.971968254974226, 'learning_rate': 5.209783586411144e-05, 'batch_size': 48, 'step_size': 12, 'gamma': 0.8378612199280155}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:05:59,986][0m Trial 18 finished with value: 0.7506729960441589 and parameters: {'observation_period_num': 93, 'train_rates': 0.9893847948681901, 'learning_rate': 6.700190019956166e-06, 'batch_size': 106, 'step_size': 12, 'gamma': 0.8221226722906938}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:06:36,165][0m Trial 19 finished with value: 0.40167495608329773 and parameters: {'observation_period_num': 30, 'train_rates': 0.9525161345098172, 'learning_rate': 4.886286080015422e-05, 'batch_size': 156, 'step_size': 13, 'gamma': 0.9014583016547718}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:08:15,761][0m Trial 20 finished with value: 0.5353105820004066 and parameters: {'observation_period_num': 35, 'train_rates': 0.863486674576298, 'learning_rate': 4.503078233178139e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8577780300311441}. Best is trial 13 with value: 0.24671301599542825.[0m
[32m[I 2025-02-05 00:09:50,515][0m Trial 21 finished with value: 0.2347235961881264 and parameters: {'observation_period_num': 76, 'train_rates': 0.9344033344822729, 'learning_rate': 0.0001968441563628783, 'batch_size': 53, 'step_size': 13, 'gamma': 0.8261998049123599}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:11:18,604][0m Trial 22 finished with value: 0.25632750641101254 and parameters: {'observation_period_num': 74, 'train_rates': 0.9346215906361898, 'learning_rate': 0.00020317255984960095, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8201554101596922}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:13:16,262][0m Trial 23 finished with value: 0.38322420489220393 and parameters: {'observation_period_num': 88, 'train_rates': 0.8608346742263349, 'learning_rate': 5.95987727050945e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8545731938627967}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:15:19,874][0m Trial 24 finished with value: 0.7149333737382841 and parameters: {'observation_period_num': 101, 'train_rates': 0.9294983186229013, 'learning_rate': 0.0005323775184636718, 'batch_size': 50, 'step_size': 10, 'gamma': 0.7841530099859737}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:17:54,574][0m Trial 25 finished with value: 0.7756658623803336 and parameters: {'observation_period_num': 145, 'train_rates': 0.7733873883325684, 'learning_rate': 2.5114260867951268e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8948380107085857}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:19:05,607][0m Trial 26 finished with value: 0.7141839882423138 and parameters: {'observation_period_num': 32, 'train_rates': 0.9703917510325832, 'learning_rate': 9.983684084189368e-06, 'batch_size': 71, 'step_size': 11, 'gamma': 0.8069953560734211}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:22:21,623][0m Trial 27 finished with value: 0.35722100165189874 and parameters: {'observation_period_num': 166, 'train_rates': 0.8629092732515526, 'learning_rate': 0.00016211878678035308, 'batch_size': 39, 'step_size': 14, 'gamma': 0.8339002812662172}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:27:24,990][0m Trial 28 finished with value: 0.3083104317592195 and parameters: {'observation_period_num': 242, 'train_rates': 0.910727560400255, 'learning_rate': 7.71776348117794e-05, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8581238718799481}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:28:44,151][0m Trial 29 finished with value: 0.6759237051010132 and parameters: {'observation_period_num': 53, 'train_rates': 0.9874771001770943, 'learning_rate': 0.0006955452467346774, 'batch_size': 64, 'step_size': 14, 'gamma': 0.773572183308511}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:30:15,663][0m Trial 30 finished with value: 1.0149447652075787 and parameters: {'observation_period_num': 80, 'train_rates': 0.9516472644270112, 'learning_rate': 0.0009421199991820263, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8147955107902315}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:31:43,354][0m Trial 31 finished with value: 0.24237884365850024 and parameters: {'observation_period_num': 74, 'train_rates': 0.9376753851184441, 'learning_rate': 0.00018506986681906344, 'batch_size': 77, 'step_size': 13, 'gamma': 0.7915671099746349}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:34:16,519][0m Trial 32 finished with value: 0.24469857476651669 and parameters: {'observation_period_num': 130, 'train_rates': 0.9517997175718823, 'learning_rate': 0.00017895575579970747, 'batch_size': 101, 'step_size': 11, 'gamma': 0.7918807796501088}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:36:45,886][0m Trial 33 finished with value: 0.2968489591564451 and parameters: {'observation_period_num': 131, 'train_rates': 0.8761640532742997, 'learning_rate': 0.00038226963661290136, 'batch_size': 106, 'step_size': 11, 'gamma': 0.7917571816469612}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:41:05,892][0m Trial 34 finished with value: 0.2497107730462001 and parameters: {'observation_period_num': 212, 'train_rates': 0.9241539586171972, 'learning_rate': 0.00017588552263278154, 'batch_size': 95, 'step_size': 13, 'gamma': 0.7953840873332187}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:44:11,112][0m Trial 35 finished with value: 0.3539125861063495 and parameters: {'observation_period_num': 172, 'train_rates': 0.8466105464774958, 'learning_rate': 0.0002139669760862278, 'batch_size': 121, 'step_size': 9, 'gamma': 0.7635352400845581}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:46:50,735][0m Trial 36 finished with value: 0.5781229258357704 and parameters: {'observation_period_num': 129, 'train_rates': 0.945424421201146, 'learning_rate': 0.0006059817189391437, 'batch_size': 73, 'step_size': 14, 'gamma': 0.7812740230707798}. Best is trial 21 with value: 0.2347235961881264.[0m
[32m[I 2025-02-05 00:49:20,317][0m Trial 37 finished with value: 0.2215273158760363 and parameters: {'observation_period_num': 110, 'train_rates': 0.9084141193572343, 'learning_rate': 0.00012839717656216377, 'batch_size': 31, 'step_size': 11, 'gamma': 0.7985272725664257}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 00:51:40,751][0m Trial 38 finished with value: 0.2816545412236569 and parameters: {'observation_period_num': 109, 'train_rates': 0.9103221010946141, 'learning_rate': 0.000355432871906431, 'batch_size': 37, 'step_size': 10, 'gamma': 0.7998029903854359}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 00:54:47,138][0m Trial 39 finished with value: 0.3659403091019923 and parameters: {'observation_period_num': 151, 'train_rates': 0.9640661605672174, 'learning_rate': 0.00011129325438722791, 'batch_size': 92, 'step_size': 7, 'gamma': 0.7539855807718058}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 00:57:08,814][0m Trial 40 finished with value: 0.6546077110012125 and parameters: {'observation_period_num': 100, 'train_rates': 0.7741158518979124, 'learning_rate': 8.722888208844585e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.7761032596719324}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 00:59:14,550][0m Trial 41 finished with value: 0.28587361929878113 and parameters: {'observation_period_num': 113, 'train_rates': 0.8908770654222928, 'learning_rate': 0.00014342060091343864, 'batch_size': 63, 'step_size': 11, 'gamma': 0.808640615051271}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:01:42,105][0m Trial 42 finished with value: 0.2884470820426941 and parameters: {'observation_period_num': 131, 'train_rates': 0.9170859422926104, 'learning_rate': 0.00023256142239049497, 'batch_size': 256, 'step_size': 13, 'gamma': 0.819398545091567}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:04:27,406][0m Trial 43 finished with value: 0.30711310411639076 and parameters: {'observation_period_num': 66, 'train_rates': 0.9529307811647815, 'learning_rate': 0.00038939176537830647, 'batch_size': 29, 'step_size': 11, 'gamma': 0.7887534177104797}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:06:01,410][0m Trial 44 finished with value: 0.25870711458356754 and parameters: {'observation_period_num': 44, 'train_rates': 0.9348117369718867, 'learning_rate': 6.788226161407025e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.7663902512929394}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:08:54,789][0m Trial 45 finished with value: 0.7416319573117841 and parameters: {'observation_period_num': 157, 'train_rates': 0.8448042646398062, 'learning_rate': 3.351417551782784e-05, 'batch_size': 81, 'step_size': 4, 'gamma': 0.8269016549124637}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:12:46,778][0m Trial 46 finished with value: 0.2685554902493258 and parameters: {'observation_period_num': 193, 'train_rates': 0.9068653473044104, 'learning_rate': 0.00010227779261358853, 'batch_size': 119, 'step_size': 10, 'gamma': 0.8812703008555127}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:13:56,637][0m Trial 47 finished with value: 0.28884934333332796 and parameters: {'observation_period_num': 21, 'train_rates': 0.883287682750787, 'learning_rate': 0.00013423374041443274, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8007630411863285}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:16:02,145][0m Trial 48 finished with value: 1.2432777846473886 and parameters: {'observation_period_num': 122, 'train_rates': 0.6433311455123131, 'learning_rate': 0.00028174404403362457, 'batch_size': 30, 'step_size': 12, 'gamma': 0.848143887066792}. Best is trial 37 with value: 0.2215273158760363.[0m
[32m[I 2025-02-05 01:17:53,922][0m Trial 49 finished with value: 1.18930561390034 and parameters: {'observation_period_num': 94, 'train_rates': 0.9400167354686495, 'learning_rate': 1.823231548456371e-06, 'batch_size': 55, 'step_size': 15, 'gamma': 0.8101518160433236}. Best is trial 37 with value: 0.2215273158760363.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-05 01:17:53,929][0m A new study created in memory with name: no-name-1f852b45-dc11-430c-9e3e-d0a675ba0bb2[0m
[32m[I 2025-02-05 01:20:40,195][0m Trial 0 finished with value: 0.8785332578153283 and parameters: {'observation_period_num': 165, 'train_rates': 0.7281757509229867, 'learning_rate': 0.00025321050411082223, 'batch_size': 132, 'step_size': 2, 'gamma': 0.8405415808158642}. Best is trial 0 with value: 0.8785332578153283.[0m
[32m[I 2025-02-05 01:24:20,767][0m Trial 1 finished with value: 0.48232484915139484 and parameters: {'observation_period_num': 185, 'train_rates': 0.8821183417747513, 'learning_rate': 0.00016645992437792605, 'batch_size': 53, 'step_size': 2, 'gamma': 0.7625068972312863}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:26:35,948][0m Trial 2 finished with value: 0.8382408918259259 and parameters: {'observation_period_num': 138, 'train_rates': 0.7573823217539181, 'learning_rate': 0.000114953410415043, 'batch_size': 241, 'step_size': 10, 'gamma': 0.7993697530111428}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:28:34,238][0m Trial 3 finished with value: 1.0611382995503573 and parameters: {'observation_period_num': 126, 'train_rates': 0.7880960428801309, 'learning_rate': 2.357657604425975e-05, 'batch_size': 189, 'step_size': 8, 'gamma': 0.8445561718129981}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:32:24,567][0m Trial 4 finished with value: 1.1493626570450926 and parameters: {'observation_period_num': 231, 'train_rates': 0.606631313866984, 'learning_rate': 0.0003009490113708092, 'batch_size': 65, 'step_size': 2, 'gamma': 0.8069621385137948}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:35:41,541][0m Trial 5 finished with value: 0.9229885612151293 and parameters: {'observation_period_num': 204, 'train_rates': 0.6610370999472683, 'learning_rate': 0.0001180082063194226, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9783359425278153}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:39:17,490][0m Trial 6 finished with value: 0.935908980185321 and parameters: {'observation_period_num': 202, 'train_rates': 0.7245684463899955, 'learning_rate': 0.00018553171734986864, 'batch_size': 50, 'step_size': 15, 'gamma': 0.7772599797419719}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:41:03,651][0m Trial 7 finished with value: 0.7085190590746533 and parameters: {'observation_period_num': 105, 'train_rates': 0.7815683023604933, 'learning_rate': 4.087529807800012e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.9085769400167686}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:42:06,759][0m Trial 8 finished with value: 1.1290586829185485 and parameters: {'observation_period_num': 57, 'train_rates': 0.9170931459628755, 'learning_rate': 3.6364500187245e-06, 'batch_size': 132, 'step_size': 7, 'gamma': 0.8742163973098325}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:45:56,325][0m Trial 9 finished with value: 1.020921435693036 and parameters: {'observation_period_num': 211, 'train_rates': 0.6725137027469069, 'learning_rate': 1.3208838415386987e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.890559971377037}. Best is trial 1 with value: 0.48232484915139484.[0m
[32m[I 2025-02-05 01:46:23,947][0m Trial 10 finished with value: 0.395946208120613 and parameters: {'observation_period_num': 8, 'train_rates': 0.9005935697170977, 'learning_rate': 0.0008398521817852934, 'batch_size': 184, 'step_size': 4, 'gamma': 0.9631783555399442}. Best is trial 10 with value: 0.395946208120613.[0m
[32m[I 2025-02-05 01:46:51,803][0m Trial 11 finished with value: 0.5428835399010602 and parameters: {'observation_period_num': 13, 'train_rates': 0.8960448303116326, 'learning_rate': 0.0008966690775595342, 'batch_size': 186, 'step_size': 4, 'gamma': 0.9737748915260567}. Best is trial 10 with value: 0.395946208120613.[0m
[32m[I 2025-02-05 01:48:09,316][0m Trial 12 finished with value: 0.38220465183258057 and parameters: {'observation_period_num': 72, 'train_rates': 0.9793400372507388, 'learning_rate': 0.0009477340093304372, 'batch_size': 200, 'step_size': 4, 'gamma': 0.7506983715038329}. Best is trial 12 with value: 0.38220465183258057.[0m
[32m[I 2025-02-05 01:48:36,740][0m Trial 13 finished with value: 0.2721875309944153 and parameters: {'observation_period_num': 6, 'train_rates': 0.9721955596552043, 'learning_rate': 0.0007643567529514562, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9379135042946304}. Best is trial 13 with value: 0.2721875309944153.[0m
[32m[I 2025-02-05 01:49:50,282][0m Trial 14 finished with value: 0.1751745641231537 and parameters: {'observation_period_num': 68, 'train_rates': 0.9868369619996316, 'learning_rate': 0.0006165439578096811, 'batch_size': 247, 'step_size': 5, 'gamma': 0.9299592399023255}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:50:42,622][0m Trial 15 finished with value: 1.5301300287246704 and parameters: {'observation_period_num': 50, 'train_rates': 0.9877340414126499, 'learning_rate': 1.379552626262894e-06, 'batch_size': 245, 'step_size': 11, 'gamma': 0.9332243637032195}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:51:16,241][0m Trial 16 finished with value: 0.33668629922223897 and parameters: {'observation_period_num': 32, 'train_rates': 0.8476638345548919, 'learning_rate': 0.0004116477407955079, 'batch_size': 219, 'step_size': 5, 'gamma': 0.9392995951514492}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:52:52,196][0m Trial 17 finished with value: 0.36278969049453735 and parameters: {'observation_period_num': 87, 'train_rates': 0.9594402724898734, 'learning_rate': 6.740481251616764e-05, 'batch_size': 162, 'step_size': 10, 'gamma': 0.9227267086084184}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:53:28,196][0m Trial 18 finished with value: 0.39415915680539076 and parameters: {'observation_period_num': 35, 'train_rates': 0.838691083547352, 'learning_rate': 0.0004670220880262538, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9503210268588022}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:55:04,263][0m Trial 19 finished with value: 0.8706240057945251 and parameters: {'observation_period_num': 91, 'train_rates': 0.9560892155607018, 'learning_rate': 1.240553073397028e-05, 'batch_size': 211, 'step_size': 13, 'gamma': 0.9022466136037474}. Best is trial 14 with value: 0.1751745641231537.[0m
Early stopping at epoch 78
[32m[I 2025-02-05 01:55:34,091][0m Trial 20 finished with value: 0.9155603314333773 and parameters: {'observation_period_num': 32, 'train_rates': 0.9358946411458411, 'learning_rate': 6.699089349862457e-05, 'batch_size': 167, 'step_size': 1, 'gamma': 0.8547044193358023}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:56:03,632][0m Trial 21 finished with value: 0.34955151796340944 and parameters: {'observation_period_num': 26, 'train_rates': 0.8546739551573698, 'learning_rate': 0.0004298738497385289, 'batch_size': 222, 'step_size': 5, 'gamma': 0.9415875154227982}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:56:55,227][0m Trial 22 finished with value: 0.5116061018866913 and parameters: {'observation_period_num': 56, 'train_rates': 0.8521855831529163, 'learning_rate': 0.000455554932730298, 'batch_size': 227, 'step_size': 6, 'gamma': 0.9859802212964554}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 01:57:34,821][0m Trial 23 finished with value: 0.43857369601169477 and parameters: {'observation_period_num': 42, 'train_rates': 0.8161679346057351, 'learning_rate': 0.0005560565159951699, 'batch_size': 224, 'step_size': 5, 'gamma': 0.9261319305920404}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:02:15,798][0m Trial 24 finished with value: 0.25888396557196075 and parameters: {'observation_period_num': 7, 'train_rates': 0.9437030262040974, 'learning_rate': 0.0002818415517305311, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9560592428341709}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:06:39,626][0m Trial 25 finished with value: 0.2564628587646799 and parameters: {'observation_period_num': 8, 'train_rates': 0.9382555643241896, 'learning_rate': 0.0002443024957219544, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9600602085215972}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:09:52,528][0m Trial 26 finished with value: 0.2187197588875814 and parameters: {'observation_period_num': 64, 'train_rates': 0.932068379449575, 'learning_rate': 7.645998525520124e-05, 'batch_size': 24, 'step_size': 1, 'gamma': 0.9661480468342165}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:11:51,293][0m Trial 27 finished with value: 0.3805059292147645 and parameters: {'observation_period_num': 111, 'train_rates': 0.9206948837525775, 'learning_rate': 8.89406432038852e-05, 'batch_size': 99, 'step_size': 1, 'gamma': 0.9611369362999546}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:16:10,961][0m Trial 28 finished with value: 0.2875064514700879 and parameters: {'observation_period_num': 77, 'train_rates': 0.8742262840195176, 'learning_rate': 3.649270898995966e-05, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9137519254984333}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:19:43,110][0m Trial 29 finished with value: 0.1802537590265274 and parameters: {'observation_period_num': 158, 'train_rates': 0.9887860408911298, 'learning_rate': 0.0002073994799676272, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9888744575196258}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:22:38,972][0m Trial 30 finished with value: 0.2100532054901123 and parameters: {'observation_period_num': 146, 'train_rates': 0.9815559197479576, 'learning_rate': 0.0001704434529153618, 'batch_size': 88, 'step_size': 3, 'gamma': 0.989226887994855}. Best is trial 14 with value: 0.1751745641231537.[0m
[32m[I 2025-02-05 02:25:57,812][0m Trial 31 finished with value: 0.16732759773731232 and parameters: {'observation_period_num': 159, 'train_rates': 0.9858663933448797, 'learning_rate': 0.00015840203763515197, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9863050265692794}. Best is trial 31 with value: 0.16732759773731232.[0m
[32m[I 2025-02-05 02:29:19,492][0m Trial 32 finished with value: 0.18420977890491486 and parameters: {'observation_period_num': 164, 'train_rates': 0.9881011599848187, 'learning_rate': 0.0001624049940835192, 'batch_size': 88, 'step_size': 3, 'gamma': 0.9874109113837333}. Best is trial 31 with value: 0.16732759773731232.[0m
[32m[I 2025-02-05 02:32:45,469][0m Trial 33 finished with value: 0.1484784185886383 and parameters: {'observation_period_num': 167, 'train_rates': 0.9866915906418945, 'learning_rate': 0.0001347443181048338, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9894113063456607}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:36:04,407][0m Trial 34 finished with value: 0.25115713477134705 and parameters: {'observation_period_num': 169, 'train_rates': 0.9613020225487487, 'learning_rate': 0.00011616980046610101, 'batch_size': 119, 'step_size': 2, 'gamma': 0.9756243043852982}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:38:58,794][0m Trial 35 finished with value: 0.3431639912966135 and parameters: {'observation_period_num': 155, 'train_rates': 0.9073568822898743, 'learning_rate': 0.00025038099537741035, 'batch_size': 147, 'step_size': 2, 'gamma': 0.8869914335421777}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:41:22,451][0m Trial 36 finished with value: 0.38372139183301773 and parameters: {'observation_period_num': 127, 'train_rates': 0.9555959098891345, 'learning_rate': 4.6401684902504745e-05, 'batch_size': 76, 'step_size': 2, 'gamma': 0.950641051027352}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:45:05,880][0m Trial 37 finished with value: 0.6880349516868591 and parameters: {'observation_period_num': 184, 'train_rates': 0.9897005905078426, 'learning_rate': 2.4793381743951816e-05, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8079755289238946}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:48:42,918][0m Trial 38 finished with value: 0.28471825478306734 and parameters: {'observation_period_num': 185, 'train_rates': 0.8800229590278806, 'learning_rate': 0.00012870700725330943, 'batch_size': 52, 'step_size': 3, 'gamma': 0.9736573877460666}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:53:01,175][0m Trial 39 finished with value: 1.1307290577135243 and parameters: {'observation_period_num': 233, 'train_rates': 0.7321517946274925, 'learning_rate': 0.0003488077750383917, 'batch_size': 129, 'step_size': 6, 'gamma': 0.9770539067602485}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:55:08,908][0m Trial 40 finished with value: 1.3857835515838708 and parameters: {'observation_period_num': 138, 'train_rates': 0.6146952139323772, 'learning_rate': 0.0006435369924860932, 'batch_size': 62, 'step_size': 1, 'gamma': 0.9881553780618116}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 02:58:36,133][0m Trial 41 finished with value: 0.2662264332175255 and parameters: {'observation_period_num': 172, 'train_rates': 0.9664771678723927, 'learning_rate': 0.00021408962868811024, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9894241181061183}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:01:48,246][0m Trial 42 finished with value: 0.17838260531425476 and parameters: {'observation_period_num': 155, 'train_rates': 0.9871830164801946, 'learning_rate': 0.00014317919232688581, 'batch_size': 93, 'step_size': 4, 'gamma': 0.9670465962241703}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:04:50,761][0m Trial 43 finished with value: 0.3539547422160841 and parameters: {'observation_period_num': 151, 'train_rates': 0.947973601632402, 'learning_rate': 4.98549006517979e-05, 'batch_size': 103, 'step_size': 2, 'gamma': 0.9703888831978297}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:09:44,570][0m Trial 44 finished with value: 0.19984212401840423 and parameters: {'observation_period_num': 217, 'train_rates': 0.9176837305845835, 'learning_rate': 0.0001400904189157055, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9476431146616014}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:13:34,984][0m Trial 45 finished with value: 0.2678215801715851 and parameters: {'observation_period_num': 192, 'train_rates': 0.9683499523781701, 'learning_rate': 9.342751854603331e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.9657985604657277}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:15:36,935][0m Trial 46 finished with value: 0.2526744928559646 and parameters: {'observation_period_num': 114, 'train_rates': 0.9286123199645047, 'learning_rate': 0.00019563420711799222, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8231347857351572}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:18:29,923][0m Trial 47 finished with value: 0.2809982859944723 and parameters: {'observation_period_num': 137, 'train_rates': 0.9705547090451561, 'learning_rate': 0.00032816296799697935, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9769574269154534}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:21:40,993][0m Trial 48 finished with value: 0.4414608777549706 and parameters: {'observation_period_num': 160, 'train_rates': 0.8910702413607188, 'learning_rate': 2.3743757338128394e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.931042110540212}. Best is trial 33 with value: 0.1484784185886383.[0m
[32m[I 2025-02-05 03:24:41,896][0m Trial 49 finished with value: 1.1160099668774062 and parameters: {'observation_period_num': 175, 'train_rates': 0.7594195170052511, 'learning_rate': 1.432585823148868e-05, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9173622414071227}. Best is trial 33 with value: 0.1484784185886383.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-05 03:24:42,011][0m A new study created in memory with name: no-name-5d9a8fd2-b504-44f8-96d6-b32040638cbf[0m
[32m[I 2025-02-05 03:27:03,643][0m Trial 0 finished with value: 1.361108475659281 and parameters: {'observation_period_num': 147, 'train_rates': 0.6381699978176678, 'learning_rate': 2.578761536183376e-05, 'batch_size': 51, 'step_size': 2, 'gamma': 0.7510904986303308}. Best is trial 0 with value: 1.361108475659281.[0m
[32m[I 2025-02-05 03:31:31,221][0m Trial 1 finished with value: 0.5172543398065546 and parameters: {'observation_period_num': 216, 'train_rates': 0.8344109260648138, 'learning_rate': 1.5433170428751145e-05, 'batch_size': 28, 'step_size': 6, 'gamma': 0.876562396575469}. Best is trial 1 with value: 0.5172543398065546.[0m
[32m[I 2025-02-05 03:33:23,858][0m Trial 2 finished with value: 0.5039741608189113 and parameters: {'observation_period_num': 112, 'train_rates': 0.858389264044969, 'learning_rate': 0.00038852548348593973, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8798910222057964}. Best is trial 2 with value: 0.5039741608189113.[0m
[32m[I 2025-02-05 03:36:21,585][0m Trial 3 finished with value: 0.39192813634872437 and parameters: {'observation_period_num': 139, 'train_rates': 0.984760062230741, 'learning_rate': 0.00025771315806025604, 'batch_size': 56, 'step_size': 2, 'gamma': 0.7900145250617789}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:40:11,634][0m Trial 4 finished with value: 0.7223994975572189 and parameters: {'observation_period_num': 186, 'train_rates': 0.8545462059795854, 'learning_rate': 0.00037052971903429166, 'batch_size': 29, 'step_size': 15, 'gamma': 0.85315122262145}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:40:51,202][0m Trial 5 finished with value: 0.7516525128558904 and parameters: {'observation_period_num': 25, 'train_rates': 0.837461560686407, 'learning_rate': 0.0009951355407806892, 'batch_size': 117, 'step_size': 4, 'gamma': 0.7886575842879491}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:42:50,214][0m Trial 6 finished with value: 0.4497502380854463 and parameters: {'observation_period_num': 114, 'train_rates': 0.9229681116443063, 'learning_rate': 0.0008754248947281381, 'batch_size': 164, 'step_size': 7, 'gamma': 0.8074797243962962}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:46:49,584][0m Trial 7 finished with value: 0.851023056028835 and parameters: {'observation_period_num': 209, 'train_rates': 0.7584189158179316, 'learning_rate': 0.000505216915447631, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8144619006532265}. Best is trial 3 with value: 0.39192813634872437.[0m
Early stopping at epoch 71
[32m[I 2025-02-05 03:47:49,140][0m Trial 8 finished with value: 1.825714320412481 and parameters: {'observation_period_num': 78, 'train_rates': 0.9336733688046122, 'learning_rate': 1.3877712849333023e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.854882574547911}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:51:12,243][0m Trial 9 finished with value: 0.6231875037170514 and parameters: {'observation_period_num': 174, 'train_rates': 0.8946316350769397, 'learning_rate': 8.833423922354273e-06, 'batch_size': 60, 'step_size': 12, 'gamma': 0.8920692874670035}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:52:09,300][0m Trial 10 finished with value: 1.2071129083633423 and parameters: {'observation_period_num': 56, 'train_rates': 0.9884760900487363, 'learning_rate': 1.9391178071245605e-06, 'batch_size': 250, 'step_size': 10, 'gamma': 0.9735582204482155}. Best is trial 3 with value: 0.39192813634872437.[0m
[32m[I 2025-02-05 03:54:25,798][0m Trial 11 finished with value: 0.32271310687065125 and parameters: {'observation_period_num': 121, 'train_rates': 0.9793839272105768, 'learning_rate': 9.778957347797852e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.7960402487635198}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 03:57:15,489][0m Trial 12 finished with value: 0.4778496325016022 and parameters: {'observation_period_num': 146, 'train_rates': 0.9710025148633256, 'learning_rate': 8.966838046918359e-05, 'batch_size': 202, 'step_size': 9, 'gamma': 0.7515354662935684}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:01:36,331][0m Trial 13 finished with value: 0.8655253827125949 and parameters: {'observation_period_num': 252, 'train_rates': 0.7030515256995884, 'learning_rate': 0.0001045998892009887, 'batch_size': 208, 'step_size': 12, 'gamma': 0.7938633342276381}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:03:01,521][0m Trial 14 finished with value: 0.7359376939178031 and parameters: {'observation_period_num': 88, 'train_rates': 0.7587438383871183, 'learning_rate': 0.0001047334581471278, 'batch_size': 89, 'step_size': 4, 'gamma': 0.9323044468049045}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:05:41,437][0m Trial 15 finished with value: 0.33944033223427145 and parameters: {'observation_period_num': 141, 'train_rates': 0.9420938116785135, 'learning_rate': 0.00017813171327644165, 'batch_size': 152, 'step_size': 10, 'gamma': 0.8252134755075047}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:06:13,289][0m Trial 16 finished with value: 0.5285840843795636 and parameters: {'observation_period_num': 25, 'train_rates': 0.9255196838304565, 'learning_rate': 5.157675360601145e-05, 'batch_size': 193, 'step_size': 11, 'gamma': 0.8176620458250313}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:09:25,337][0m Trial 17 finished with value: 0.33177397620802557 and parameters: {'observation_period_num': 172, 'train_rates': 0.8967229639528208, 'learning_rate': 0.0001889453561649444, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8398671139366243}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:12:39,672][0m Trial 18 finished with value: 1.240631687904382 and parameters: {'observation_period_num': 176, 'train_rates': 0.885781714936119, 'learning_rate': 3.346469183024259e-06, 'batch_size': 245, 'step_size': 15, 'gamma': 0.9093724586385076}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:17:26,977][0m Trial 19 finished with value: 0.7751284393858402 and parameters: {'observation_period_num': 250, 'train_rates': 0.7914764052451461, 'learning_rate': 3.908625330899907e-05, 'batch_size': 180, 'step_size': 13, 'gamma': 0.8403940843912763}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:18:40,776][0m Trial 20 finished with value: 1.003704389553934 and parameters: {'observation_period_num': 94, 'train_rates': 0.6059505998709753, 'learning_rate': 0.00014653893966639904, 'batch_size': 222, 'step_size': 13, 'gamma': 0.7670876962078008}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:21:42,986][0m Trial 21 finished with value: 0.37965354442596433 and parameters: {'observation_period_num': 153, 'train_rates': 0.9464872632648555, 'learning_rate': 0.00016865318538991297, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8324427160611431}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:23:47,935][0m Trial 22 finished with value: 0.4389692892630895 and parameters: {'observation_period_num': 119, 'train_rates': 0.8941913905981791, 'learning_rate': 6.200489546099563e-05, 'batch_size': 130, 'step_size': 9, 'gamma': 0.836763176451922}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:27:59,172][0m Trial 23 finished with value: 0.33107081055641174 and parameters: {'observation_period_num': 204, 'train_rates': 0.9558476421981329, 'learning_rate': 0.00020195776646843918, 'batch_size': 172, 'step_size': 10, 'gamma': 0.7801262184766967}. Best is trial 11 with value: 0.32271310687065125.[0m
[32m[I 2025-02-05 04:32:14,193][0m Trial 24 finished with value: 0.2701397240161896 and parameters: {'observation_period_num': 208, 'train_rates': 0.9595364293115122, 'learning_rate': 0.00025136971648295834, 'batch_size': 170, 'step_size': 14, 'gamma': 0.7711539229920282}. Best is trial 24 with value: 0.2701397240161896.[0m
[32m[I 2025-02-05 04:36:49,931][0m Trial 25 finished with value: 0.287264883518219 and parameters: {'observation_period_num': 220, 'train_rates': 0.9575989295503033, 'learning_rate': 0.0005226729843917394, 'batch_size': 172, 'step_size': 13, 'gamma': 0.7746110353019928}. Best is trial 24 with value: 0.2701397240161896.[0m
[32m[I 2025-02-05 04:41:45,077][0m Trial 26 finished with value: 0.18789514899253845 and parameters: {'observation_period_num': 230, 'train_rates': 0.9757038128875511, 'learning_rate': 0.0005787259491975776, 'batch_size': 220, 'step_size': 13, 'gamma': 0.7698209421788371}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 04:45:57,978][0m Trial 27 finished with value: 0.586863128809938 and parameters: {'observation_period_num': 227, 'train_rates': 0.8060734864059342, 'learning_rate': 0.00046113500471706864, 'batch_size': 223, 'step_size': 14, 'gamma': 0.7700954979975649}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 04:50:54,055][0m Trial 28 finished with value: 0.3539981245994568 and parameters: {'observation_period_num': 233, 'train_rates': 0.9236618222275054, 'learning_rate': 0.0006639913884983666, 'batch_size': 224, 'step_size': 13, 'gamma': 0.768172366689597}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 04:54:13,318][0m Trial 29 finished with value: 0.839848632702913 and parameters: {'observation_period_num': 197, 'train_rates': 0.6763224887186002, 'learning_rate': 0.0003070265397235855, 'batch_size': 194, 'step_size': 14, 'gamma': 0.7559050321990401}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 04:59:07,861][0m Trial 30 finished with value: 0.2529871165752411 and parameters: {'observation_period_num': 232, 'train_rates': 0.9662257635527317, 'learning_rate': 0.0006043529934596324, 'batch_size': 232, 'step_size': 12, 'gamma': 0.807800902810247}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 05:04:07,951][0m Trial 31 finished with value: 0.2814493477344513 and parameters: {'observation_period_num': 235, 'train_rates': 0.9574985507099638, 'learning_rate': 0.0006308270906351911, 'batch_size': 235, 'step_size': 12, 'gamma': 0.8030805771821724}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 05:09:01,361][0m Trial 32 finished with value: 0.32708337903022766 and parameters: {'observation_period_num': 237, 'train_rates': 0.9134733141107907, 'learning_rate': 0.0007135841972795862, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8014495948499968}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 05:14:23,711][0m Trial 33 finished with value: 0.28717824816703796 and parameters: {'observation_period_num': 241, 'train_rates': 0.9624017912016525, 'learning_rate': 0.0002912020855229723, 'batch_size': 234, 'step_size': 11, 'gamma': 0.7549850179237968}. Best is trial 26 with value: 0.18789514899253845.[0m
[32m[I 2025-02-05 05:18:28,215][0m Trial 34 finished with value: 0.1850302368402481 and parameters: {'observation_period_num': 193, 'train_rates': 0.9886554599946481, 'learning_rate': 0.0005595716136513092, 'batch_size': 212, 'step_size': 14, 'gamma': 0.7814172504435075}. Best is trial 34 with value: 0.1850302368402481.[0m
[32m[I 2025-02-05 05:22:18,091][0m Trial 35 finished with value: 0.1738891899585724 and parameters: {'observation_period_num': 190, 'train_rates': 0.9896776782373335, 'learning_rate': 0.00036197136259234313, 'batch_size': 211, 'step_size': 14, 'gamma': 0.7834862791342413}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:26:04,235][0m Trial 36 finished with value: 0.175641730427742 and parameters: {'observation_period_num': 186, 'train_rates': 0.986783265571744, 'learning_rate': 0.0003941856584770729, 'batch_size': 205, 'step_size': 14, 'gamma': 0.7851141514739084}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:29:58,586][0m Trial 37 finished with value: 0.1783193051815033 and parameters: {'observation_period_num': 191, 'train_rates': 0.9876788891032178, 'learning_rate': 0.0003483734216192437, 'batch_size': 209, 'step_size': 15, 'gamma': 0.7818865268156184}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:32:53,594][0m Trial 38 finished with value: 0.3632617833152894 and parameters: {'observation_period_num': 161, 'train_rates': 0.8667838109483434, 'learning_rate': 0.0003670721705655265, 'batch_size': 204, 'step_size': 15, 'gamma': 0.788742682096667}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:36:44,893][0m Trial 39 finished with value: 0.4886479377746582 and parameters: {'observation_period_num': 191, 'train_rates': 0.9847675015785793, 'learning_rate': 2.359917844107256e-05, 'batch_size': 194, 'step_size': 14, 'gamma': 0.8599504171084089}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:40:18,772][0m Trial 40 finished with value: 0.3542857585895446 and parameters: {'observation_period_num': 188, 'train_rates': 0.9104050905377632, 'learning_rate': 0.000865064580154736, 'batch_size': 210, 'step_size': 15, 'gamma': 0.7816136500421803}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:43:32,144][0m Trial 41 finished with value: 0.17421460151672363 and parameters: {'observation_period_num': 164, 'train_rates': 0.9839933413405779, 'learning_rate': 0.0003861280635834811, 'batch_size': 217, 'step_size': 14, 'gamma': 0.7634814464285421}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:46:56,107][0m Trial 42 finished with value: 0.1748470962047577 and parameters: {'observation_period_num': 167, 'train_rates': 0.9893015309330677, 'learning_rate': 0.00034437560223239997, 'batch_size': 185, 'step_size': 14, 'gamma': 0.7829506949899331}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:50:03,922][0m Trial 43 finished with value: 0.32527923583984375 and parameters: {'observation_period_num': 163, 'train_rates': 0.9387013845194834, 'learning_rate': 0.00038097115001935314, 'batch_size': 193, 'step_size': 14, 'gamma': 0.8159635605128724}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:53:35,027][0m Trial 44 finished with value: 0.3424917161464691 and parameters: {'observation_period_num': 182, 'train_rates': 0.9394832744467271, 'learning_rate': 0.00025906278329833083, 'batch_size': 183, 'step_size': 15, 'gamma': 0.7605468774216593}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:56:07,926][0m Trial 45 finished with value: 0.4136379063129425 and parameters: {'observation_period_num': 132, 'train_rates': 0.9842745875101866, 'learning_rate': 0.0009796567288853086, 'batch_size': 243, 'step_size': 11, 'gamma': 0.7886065366878703}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 05:59:20,930][0m Trial 46 finished with value: 0.20505137741565704 and parameters: {'observation_period_num': 162, 'train_rates': 0.9727872548416718, 'learning_rate': 0.0001374250758173298, 'batch_size': 107, 'step_size': 4, 'gamma': 0.9788781863291579}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 06:03:34,348][0m Trial 47 finished with value: 0.3241471472299761 and parameters: {'observation_period_num': 215, 'train_rates': 0.8685396951008627, 'learning_rate': 0.0004171935661476335, 'batch_size': 215, 'step_size': 14, 'gamma': 0.7505976641167542}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 06:06:57,356][0m Trial 48 finished with value: 0.9165016412734985 and parameters: {'observation_period_num': 170, 'train_rates': 0.9891203734959899, 'learning_rate': 6.832414848079673e-06, 'batch_size': 200, 'step_size': 13, 'gamma': 0.7938052226663616}. Best is trial 35 with value: 0.1738891899585724.[0m
[32m[I 2025-02-05 06:09:55,656][0m Trial 49 finished with value: 0.45557597279548645 and parameters: {'observation_period_num': 154, 'train_rates': 0.9468426972356297, 'learning_rate': 7.306764291583688e-05, 'batch_size': 161, 'step_size': 15, 'gamma': 0.9383437978979724}. Best is trial 35 with value: 0.1738891899585724.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 98, 'train_rates': 0.9898653025571045, 'learning_rate': 0.00020188813682337284, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9082329479360141}
Epoch 1/300, trend Loss: 0.8236 | 1.0852
Epoch 2/300, trend Loss: 0.7621 | 0.9309
Epoch 3/300, trend Loss: 0.6566 | 0.8218
Epoch 4/300, trend Loss: 0.5988 | 0.7933
Epoch 5/300, trend Loss: 0.5435 | 0.7478
Epoch 6/300, trend Loss: 0.5276 | 0.6362
Epoch 7/300, trend Loss: 0.4188 | 0.6240
Epoch 8/300, trend Loss: 0.4041 | 0.5651
Epoch 9/300, trend Loss: 0.4067 | 0.5013
Epoch 10/300, trend Loss: 0.3678 | 0.4841
Epoch 11/300, trend Loss: 0.4004 | 0.4944
Epoch 12/300, trend Loss: 0.4119 | 0.4717
Epoch 13/300, trend Loss: 0.3540 | 0.4862
Epoch 14/300, trend Loss: 0.3916 | 0.4492
Epoch 15/300, trend Loss: 0.3972 | 0.3917
Epoch 16/300, trend Loss: 0.3473 | 0.3910
Epoch 17/300, trend Loss: 0.3073 | 0.3749
Epoch 18/300, trend Loss: 0.2806 | 0.3532
Epoch 19/300, trend Loss: 0.2718 | 0.3532
Epoch 20/300, trend Loss: 0.2657 | 0.3241
Epoch 21/300, trend Loss: 0.2528 | 0.3212
Epoch 22/300, trend Loss: 0.2393 | 0.3268
Epoch 23/300, trend Loss: 0.2365 | 0.2977
Epoch 24/300, trend Loss: 0.2424 | 0.3153
Epoch 25/300, trend Loss: 0.2390 | 0.3002
Epoch 26/300, trend Loss: 0.2464 | 0.2694
Epoch 27/300, trend Loss: 0.2324 | 0.3072
Epoch 28/300, trend Loss: 0.2122 | 0.2714
Epoch 29/300, trend Loss: 0.2103 | 0.2636
Epoch 30/300, trend Loss: 0.2319 | 0.2837
Epoch 31/300, trend Loss: 0.2025 | 0.2572
Epoch 32/300, trend Loss: 0.2006 | 0.2521
Epoch 33/300, trend Loss: 0.2069 | 0.2367
Epoch 34/300, trend Loss: 0.2035 | 0.2628
Epoch 35/300, trend Loss: 0.1991 | 0.2315
Epoch 36/300, trend Loss: 0.2088 | 0.2669
Epoch 37/300, trend Loss: 0.2072 | 0.2504
Epoch 38/300, trend Loss: 0.2029 | 0.2242
Epoch 39/300, trend Loss: 0.2240 | 0.2640
Epoch 40/300, trend Loss: 0.2101 | 0.2265
Epoch 41/300, trend Loss: 0.2034 | 0.2517
Epoch 42/300, trend Loss: 0.1936 | 0.2408
Epoch 43/300, trend Loss: 0.1887 | 0.2313
Epoch 44/300, trend Loss: 0.1872 | 0.2338
Epoch 45/300, trend Loss: 0.1748 | 0.2200
Epoch 46/300, trend Loss: 0.1710 | 0.2193
Epoch 47/300, trend Loss: 0.1677 | 0.2098
Epoch 48/300, trend Loss: 0.1660 | 0.2166
Epoch 49/300, trend Loss: 0.1620 | 0.2052
Epoch 50/300, trend Loss: 0.1593 | 0.1999
Epoch 51/300, trend Loss: 0.1600 | 0.2076
Epoch 52/300, trend Loss: 0.1543 | 0.2026
Epoch 53/300, trend Loss: 0.1516 | 0.1917
Epoch 54/300, trend Loss: 0.1511 | 0.1976
Epoch 55/300, trend Loss: 0.1495 | 0.2009
Epoch 56/300, trend Loss: 0.1492 | 0.1818
Epoch 57/300, trend Loss: 0.1498 | 0.1957
Epoch 58/300, trend Loss: 0.1500 | 0.2142
Epoch 59/300, trend Loss: 0.1518 | 0.1828
Epoch 60/300, trend Loss: 0.1615 | 0.2048
Epoch 61/300, trend Loss: 0.1744 | 0.2175
Epoch 62/300, trend Loss: 0.1561 | 0.1872
Epoch 63/300, trend Loss: 0.1643 | 0.1970
Epoch 64/300, trend Loss: 0.1662 | 0.2215
Epoch 65/300, trend Loss: 0.1483 | 0.1852
Epoch 66/300, trend Loss: 0.1499 | 0.1835
Epoch 67/300, trend Loss: 0.1488 | 0.1956
Epoch 68/300, trend Loss: 0.1455 | 0.1766
Epoch 69/300, trend Loss: 0.1408 | 0.1766
Epoch 70/300, trend Loss: 0.1418 | 0.2065
Epoch 71/300, trend Loss: 0.1396 | 0.1694
Epoch 72/300, trend Loss: 0.1393 | 0.1700
Epoch 73/300, trend Loss: 0.1483 | 0.2251
Epoch 74/300, trend Loss: 0.1445 | 0.1672
Epoch 75/300, trend Loss: 0.1449 | 0.1743
Epoch 76/300, trend Loss: 0.1645 | 0.2128
Epoch 77/300, trend Loss: 0.1445 | 0.1643
Epoch 78/300, trend Loss: 0.1467 | 0.1842
Epoch 79/300, trend Loss: 0.1682 | 0.2008
Epoch 80/300, trend Loss: 0.1430 | 0.1611
Epoch 81/300, trend Loss: 0.1383 | 0.1813
Epoch 82/300, trend Loss: 0.1451 | 0.1780
Epoch 83/300, trend Loss: 0.1336 | 0.1678
Epoch 84/300, trend Loss: 0.1307 | 0.1792
Epoch 85/300, trend Loss: 0.1305 | 0.1670
Epoch 86/300, trend Loss: 0.1280 | 0.1652
Epoch 87/300, trend Loss: 0.1258 | 0.1692
Epoch 88/300, trend Loss: 0.1258 | 0.1618
Epoch 89/300, trend Loss: 0.1248 | 0.1667
Epoch 90/300, trend Loss: 0.1239 | 0.1639
Epoch 91/300, trend Loss: 0.1241 | 0.1565
Epoch 92/300, trend Loss: 0.1228 | 0.1678
Epoch 93/300, trend Loss: 0.1238 | 0.1574
Epoch 94/300, trend Loss: 0.1231 | 0.1556
Epoch 95/300, trend Loss: 0.1226 | 0.1715
Epoch 96/300, trend Loss: 0.1213 | 0.1498
Epoch 97/300, trend Loss: 0.1208 | 0.1554
Epoch 98/300, trend Loss: 0.1212 | 0.1599
Epoch 99/300, trend Loss: 0.1201 | 0.1500
Epoch 100/300, trend Loss: 0.1210 | 0.1568
Epoch 101/300, trend Loss: 0.1198 | 0.1533
Epoch 102/300, trend Loss: 0.1181 | 0.1479
Epoch 103/300, trend Loss: 0.1178 | 0.1598
Epoch 104/300, trend Loss: 0.1176 | 0.1480
Epoch 105/300, trend Loss: 0.1182 | 0.1485
Epoch 106/300, trend Loss: 0.1180 | 0.1511
Epoch 107/300, trend Loss: 0.1177 | 0.1437
Epoch 108/300, trend Loss: 0.1172 | 0.1544
Epoch 109/300, trend Loss: 0.1177 | 0.1463
Epoch 110/300, trend Loss: 0.1176 | 0.1436
Epoch 111/300, trend Loss: 0.1160 | 0.1557
Epoch 112/300, trend Loss: 0.1150 | 0.1473
Epoch 113/300, trend Loss: 0.1153 | 0.1466
Epoch 114/300, trend Loss: 0.1155 | 0.1465
Epoch 115/300, trend Loss: 0.1145 | 0.1452
Epoch 116/300, trend Loss: 0.1122 | 0.1429
Epoch 117/300, trend Loss: 0.1120 | 0.1515
Epoch 118/300, trend Loss: 0.1119 | 0.1423
Epoch 119/300, trend Loss: 0.1120 | 0.1425
Epoch 120/300, trend Loss: 0.1125 | 0.1461
Epoch 121/300, trend Loss: 0.1113 | 0.1455
Epoch 122/300, trend Loss: 0.1102 | 0.1429
Epoch 123/300, trend Loss: 0.1099 | 0.1416
Epoch 124/300, trend Loss: 0.1099 | 0.1453
Epoch 125/300, trend Loss: 0.1089 | 0.1392
Epoch 126/300, trend Loss: 0.1092 | 0.1461
Epoch 127/300, trend Loss: 0.1100 | 0.1384
Epoch 128/300, trend Loss: 0.1083 | 0.1417
Epoch 129/300, trend Loss: 0.1079 | 0.1443
Epoch 130/300, trend Loss: 0.1083 | 0.1363
Epoch 131/300, trend Loss: 0.1077 | 0.1427
Epoch 132/300, trend Loss: 0.1079 | 0.1409
Epoch 133/300, trend Loss: 0.1069 | 0.1398
Epoch 134/300, trend Loss: 0.1073 | 0.1416
Epoch 135/300, trend Loss: 0.1069 | 0.1364
Epoch 136/300, trend Loss: 0.1067 | 0.1420
Epoch 137/300, trend Loss: 0.1073 | 0.1378
Epoch 138/300, trend Loss: 0.1067 | 0.1431
Epoch 139/300, trend Loss: 0.1062 | 0.1427
Epoch 140/300, trend Loss: 0.1051 | 0.1392
Epoch 141/300, trend Loss: 0.1049 | 0.1392
Epoch 142/300, trend Loss: 0.1046 | 0.1347
Epoch 143/300, trend Loss: 0.1052 | 0.1409
Epoch 144/300, trend Loss: 0.1040 | 0.1411
Epoch 145/300, trend Loss: 0.1047 | 0.1379
Epoch 146/300, trend Loss: 0.1040 | 0.1330
Epoch 147/300, trend Loss: 0.1040 | 0.1364
Epoch 148/300, trend Loss: 0.1031 | 0.1354
Epoch 149/300, trend Loss: 0.1027 | 0.1374
Epoch 150/300, trend Loss: 0.1034 | 0.1348
Epoch 151/300, trend Loss: 0.1020 | 0.1337
Epoch 152/300, trend Loss: 0.1020 | 0.1351
Epoch 153/300, trend Loss: 0.1032 | 0.1351
Epoch 154/300, trend Loss: 0.1021 | 0.1342
Epoch 155/300, trend Loss: 0.1019 | 0.1365
Epoch 156/300, trend Loss: 0.1018 | 0.1318
Epoch 157/300, trend Loss: 0.1013 | 0.1345
Epoch 158/300, trend Loss: 0.1014 | 0.1326
Epoch 159/300, trend Loss: 0.1007 | 0.1331
Epoch 160/300, trend Loss: 0.1010 | 0.1359
Epoch 161/300, trend Loss: 0.1004 | 0.1311
Epoch 162/300, trend Loss: 0.1011 | 0.1346
Epoch 163/300, trend Loss: 0.1006 | 0.1339
Epoch 164/300, trend Loss: 0.1002 | 0.1322
Epoch 165/300, trend Loss: 0.1002 | 0.1335
Epoch 166/300, trend Loss: 0.0999 | 0.1338
Epoch 167/300, trend Loss: 0.0996 | 0.1310
Epoch 168/300, trend Loss: 0.0997 | 0.1326
Epoch 169/300, trend Loss: 0.0993 | 0.1337
Epoch 170/300, trend Loss: 0.0999 | 0.1324
Epoch 171/300, trend Loss: 0.0991 | 0.1306
Epoch 172/300, trend Loss: 0.0992 | 0.1335
Epoch 173/300, trend Loss: 0.0996 | 0.1327
Epoch 174/300, trend Loss: 0.0995 | 0.1329
Epoch 175/300, trend Loss: 0.0984 | 0.1326
Epoch 176/300, trend Loss: 0.0988 | 0.1314
Epoch 177/300, trend Loss: 0.0989 | 0.1345
Epoch 178/300, trend Loss: 0.0978 | 0.1331
Epoch 179/300, trend Loss: 0.0985 | 0.1319
Epoch 180/300, trend Loss: 0.0990 | 0.1345
Epoch 181/300, trend Loss: 0.0976 | 0.1308
Epoch 182/300, trend Loss: 0.0980 | 0.1326
Epoch 183/300, trend Loss: 0.0982 | 0.1301
Epoch 184/300, trend Loss: 0.0990 | 0.1346
Epoch 185/300, trend Loss: 0.0971 | 0.1313
Epoch 186/300, trend Loss: 0.0982 | 0.1332
Epoch 187/300, trend Loss: 0.0976 | 0.1306
Epoch 188/300, trend Loss: 0.0967 | 0.1311
Epoch 189/300, trend Loss: 0.0972 | 0.1293
Epoch 190/300, trend Loss: 0.0979 | 0.1338
Epoch 191/300, trend Loss: 0.0985 | 0.1324
Epoch 192/300, trend Loss: 0.0980 | 0.1313
Epoch 193/300, trend Loss: 0.0978 | 0.1338
Epoch 194/300, trend Loss: 0.0962 | 0.1330
Epoch 195/300, trend Loss: 0.0971 | 0.1354
Epoch 196/300, trend Loss: 0.0975 | 0.1349
Epoch 197/300, trend Loss: 0.0965 | 0.1314
Epoch 198/300, trend Loss: 0.0969 | 0.1320
Epoch 199/300, trend Loss: 0.0965 | 0.1331
Epoch 200/300, trend Loss: 0.0963 | 0.1331
Epoch 201/300, trend Loss: 0.0966 | 0.1311
Epoch 202/300, trend Loss: 0.0964 | 0.1325
Epoch 203/300, trend Loss: 0.0963 | 0.1345
Epoch 204/300, trend Loss: 0.0968 | 0.1337
Epoch 205/300, trend Loss: 0.0965 | 0.1344
Epoch 206/300, trend Loss: 0.0956 | 0.1340
Epoch 207/300, trend Loss: 0.0958 | 0.1323
Epoch 208/300, trend Loss: 0.0960 | 0.1339
Epoch 209/300, trend Loss: 0.0955 | 0.1333
Epoch 210/300, trend Loss: 0.0955 | 0.1326
Epoch 211/300, trend Loss: 0.0959 | 0.1331
Epoch 212/300, trend Loss: 0.0959 | 0.1337
Epoch 213/300, trend Loss: 0.0946 | 0.1330
Epoch 214/300, trend Loss: 0.0955 | 0.1320
Epoch 215/300, trend Loss: 0.0955 | 0.1327
Epoch 216/300, trend Loss: 0.0954 | 0.1319
Epoch 217/300, trend Loss: 0.0952 | 0.1314
Epoch 218/300, trend Loss: 0.0961 | 0.1307
Epoch 219/300, trend Loss: 0.0948 | 0.1323
Epoch 220/300, trend Loss: 0.0949 | 0.1314
Epoch 221/300, trend Loss: 0.0955 | 0.1321
Epoch 222/300, trend Loss: 0.0946 | 0.1319
Epoch 223/300, trend Loss: 0.0946 | 0.1314
Epoch 224/300, trend Loss: 0.0940 | 0.1321
Epoch 225/300, trend Loss: 0.0946 | 0.1315
Epoch 226/300, trend Loss: 0.0943 | 0.1325
Epoch 227/300, trend Loss: 0.0952 | 0.1328
Epoch 228/300, trend Loss: 0.0951 | 0.1321
Epoch 229/300, trend Loss: 0.0945 | 0.1334
Epoch 230/300, trend Loss: 0.0945 | 0.1314
Epoch 231/300, trend Loss: 0.0952 | 0.1320
Epoch 232/300, trend Loss: 0.0951 | 0.1327
Epoch 233/300, trend Loss: 0.0947 | 0.1321
Epoch 234/300, trend Loss: 0.0944 | 0.1315
Epoch 235/300, trend Loss: 0.0946 | 0.1316
Epoch 236/300, trend Loss: 0.0949 | 0.1296
Epoch 237/300, trend Loss: 0.0943 | 0.1297
Epoch 238/300, trend Loss: 0.0945 | 0.1306
Epoch 239/300, trend Loss: 0.0942 | 0.1311
Epoch 240/300, trend Loss: 0.0948 | 0.1295
Epoch 241/300, trend Loss: 0.0946 | 0.1307
Epoch 242/300, trend Loss: 0.0941 | 0.1307
Epoch 243/300, trend Loss: 0.0940 | 0.1314
Epoch 244/300, trend Loss: 0.0932 | 0.1313
Epoch 245/300, trend Loss: 0.0933 | 0.1317
Epoch 246/300, trend Loss: 0.0940 | 0.1315
Epoch 247/300, trend Loss: 0.0945 | 0.1319
Epoch 248/300, trend Loss: 0.0944 | 0.1311
Epoch 249/300, trend Loss: 0.0933 | 0.1309
Epoch 250/300, trend Loss: 0.0930 | 0.1316
Epoch 251/300, trend Loss: 0.0944 | 0.1317
Epoch 252/300, trend Loss: 0.0931 | 0.1309
Epoch 253/300, trend Loss: 0.0929 | 0.1308
Epoch 254/300, trend Loss: 0.0930 | 0.1301
Epoch 255/300, trend Loss: 0.0939 | 0.1298
Epoch 256/300, trend Loss: 0.0938 | 0.1304
Epoch 257/300, trend Loss: 0.0947 | 0.1293
Epoch 258/300, trend Loss: 0.0942 | 0.1307
Epoch 259/300, trend Loss: 0.0930 | 0.1306
Epoch 260/300, trend Loss: 0.0935 | 0.1306
Epoch 261/300, trend Loss: 0.0940 | 0.1300
Epoch 262/300, trend Loss: 0.0940 | 0.1301
Epoch 263/300, trend Loss: 0.0936 | 0.1308
Epoch 264/300, trend Loss: 0.0936 | 0.1307
Epoch 265/300, trend Loss: 0.0939 | 0.1311
Epoch 266/300, trend Loss: 0.0935 | 0.1307
Epoch 267/300, trend Loss: 0.0937 | 0.1308
Epoch 268/300, trend Loss: 0.0946 | 0.1312
Epoch 269/300, trend Loss: 0.0942 | 0.1305
Epoch 270/300, trend Loss: 0.0937 | 0.1296
Epoch 271/300, trend Loss: 0.0933 | 0.1301
Epoch 272/300, trend Loss: 0.0936 | 0.1310
Epoch 273/300, trend Loss: 0.0928 | 0.1300
Epoch 274/300, trend Loss: 0.0937 | 0.1303
Epoch 275/300, trend Loss: 0.0934 | 0.1304
Epoch 276/300, trend Loss: 0.0934 | 0.1302
Epoch 277/300, trend Loss: 0.0936 | 0.1300
Epoch 278/300, trend Loss: 0.0928 | 0.1300
Epoch 279/300, trend Loss: 0.0938 | 0.1300
Epoch 280/300, trend Loss: 0.0933 | 0.1290
Epoch 281/300, trend Loss: 0.0931 | 0.1291
Epoch 282/300, trend Loss: 0.0931 | 0.1301
Epoch 283/300, trend Loss: 0.0930 | 0.1297
Epoch 284/300, trend Loss: 0.0935 | 0.1295
Epoch 285/300, trend Loss: 0.0932 | 0.1294
Epoch 286/300, trend Loss: 0.0933 | 0.1297
Epoch 287/300, trend Loss: 0.0926 | 0.1304
Epoch 288/300, trend Loss: 0.0935 | 0.1301
Epoch 289/300, trend Loss: 0.0935 | 0.1303
Epoch 290/300, trend Loss: 0.0932 | 0.1299
Epoch 291/300, trend Loss: 0.0929 | 0.1302
Epoch 292/300, trend Loss: 0.0927 | 0.1306
Epoch 293/300, trend Loss: 0.0935 | 0.1307
Epoch 294/300, trend Loss: 0.0930 | 0.1301
Epoch 295/300, trend Loss: 0.0924 | 0.1299
Epoch 296/300, trend Loss: 0.0924 | 0.1298
Epoch 297/300, trend Loss: 0.0928 | 0.1309
Epoch 298/300, trend Loss: 0.0927 | 0.1308
Epoch 299/300, trend Loss: 0.0921 | 0.1300
Epoch 300/300, trend Loss: 0.0930 | 0.1297
Training seasonal_0 component with params: {'observation_period_num': 188, 'train_rates': 0.9886245555957569, 'learning_rate': 0.00033816019993174694, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9407271649332751}
Epoch 1/300, seasonal_0 Loss: 1.1120 | 1.6125
Epoch 2/300, seasonal_0 Loss: 0.7930 | 1.0278
Epoch 3/300, seasonal_0 Loss: 0.6567 | 0.9293
Epoch 4/300, seasonal_0 Loss: 0.5893 | 0.7715
Epoch 5/300, seasonal_0 Loss: 0.6277 | 0.7803
Epoch 6/300, seasonal_0 Loss: 0.5240 | 0.6945
Epoch 7/300, seasonal_0 Loss: 0.4933 | 0.6553
Epoch 8/300, seasonal_0 Loss: 0.4435 | 0.5689
Epoch 9/300, seasonal_0 Loss: 0.4238 | 0.5302
Epoch 10/300, seasonal_0 Loss: 0.4312 | 0.5171
Epoch 11/300, seasonal_0 Loss: 0.4536 | 0.4943
Epoch 12/300, seasonal_0 Loss: 0.4042 | 0.4681
Epoch 13/300, seasonal_0 Loss: 0.3696 | 0.4353
Epoch 14/300, seasonal_0 Loss: 0.4608 | 0.4480
Epoch 15/300, seasonal_0 Loss: 0.3624 | 0.4548
Epoch 16/300, seasonal_0 Loss: 0.3452 | 0.4363
Epoch 17/300, seasonal_0 Loss: 0.3512 | 0.4129
Epoch 18/300, seasonal_0 Loss: 0.3184 | 0.3933
Epoch 19/300, seasonal_0 Loss: 0.3241 | 0.3579
Epoch 20/300, seasonal_0 Loss: 0.2985 | 0.3544
Epoch 21/300, seasonal_0 Loss: 0.3086 | 0.3398
Epoch 22/300, seasonal_0 Loss: 0.2742 | 0.3483
Epoch 23/300, seasonal_0 Loss: 0.2899 | 0.3426
Epoch 24/300, seasonal_0 Loss: 0.3462 | 0.3368
Epoch 25/300, seasonal_0 Loss: 0.3209 | 0.3219
Epoch 26/300, seasonal_0 Loss: 0.3206 | 0.3559
Epoch 27/300, seasonal_0 Loss: 0.2771 | 0.3217
Epoch 28/300, seasonal_0 Loss: 0.2715 | 0.3128
Epoch 29/300, seasonal_0 Loss: 0.2576 | 0.3239
Epoch 30/300, seasonal_0 Loss: 0.2374 | 0.2892
Epoch 31/300, seasonal_0 Loss: 0.2288 | 0.2768
Epoch 32/300, seasonal_0 Loss: 0.2285 | 0.2769
Epoch 33/300, seasonal_0 Loss: 0.2319 | 0.2726
Epoch 34/300, seasonal_0 Loss: 0.2351 | 0.2639
Epoch 35/300, seasonal_0 Loss: 0.2277 | 0.2686
Epoch 36/300, seasonal_0 Loss: 0.2270 | 0.2580
Epoch 37/300, seasonal_0 Loss: 0.2224 | 0.2593
Epoch 38/300, seasonal_0 Loss: 0.2250 | 0.2589
Epoch 39/300, seasonal_0 Loss: 0.2253 | 0.2541
Epoch 40/300, seasonal_0 Loss: 0.2273 | 0.2626
Epoch 41/300, seasonal_0 Loss: 0.2167 | 0.2479
Epoch 42/300, seasonal_0 Loss: 0.2127 | 0.2543
Epoch 43/300, seasonal_0 Loss: 0.2041 | 0.2424
Epoch 44/300, seasonal_0 Loss: 0.2003 | 0.2422
Epoch 45/300, seasonal_0 Loss: 0.1972 | 0.2390
Epoch 46/300, seasonal_0 Loss: 0.1947 | 0.2367
Epoch 47/300, seasonal_0 Loss: 0.1931 | 0.2354
Epoch 48/300, seasonal_0 Loss: 0.1918 | 0.2331
Epoch 49/300, seasonal_0 Loss: 0.1900 | 0.2316
Epoch 50/300, seasonal_0 Loss: 0.1882 | 0.2274
Epoch 51/300, seasonal_0 Loss: 0.1859 | 0.2292
Epoch 52/300, seasonal_0 Loss: 0.1844 | 0.2232
Epoch 53/300, seasonal_0 Loss: 0.1834 | 0.2226
Epoch 54/300, seasonal_0 Loss: 0.1828 | 0.2183
Epoch 55/300, seasonal_0 Loss: 0.1813 | 0.2183
Epoch 56/300, seasonal_0 Loss: 0.1802 | 0.2157
Epoch 57/300, seasonal_0 Loss: 0.1781 | 0.2138
Epoch 58/300, seasonal_0 Loss: 0.1759 | 0.2099
Epoch 59/300, seasonal_0 Loss: 0.1736 | 0.2095
Epoch 60/300, seasonal_0 Loss: 0.1733 | 0.2059
Epoch 61/300, seasonal_0 Loss: 0.1728 | 0.2061
Epoch 62/300, seasonal_0 Loss: 0.1721 | 0.2026
Epoch 63/300, seasonal_0 Loss: 0.1711 | 0.2045
Epoch 64/300, seasonal_0 Loss: 0.1705 | 0.2021
Epoch 65/300, seasonal_0 Loss: 0.1690 | 0.2023
Epoch 66/300, seasonal_0 Loss: 0.1695 | 0.1984
Epoch 67/300, seasonal_0 Loss: 0.1690 | 0.1988
Epoch 68/300, seasonal_0 Loss: 0.1671 | 0.1974
Epoch 69/300, seasonal_0 Loss: 0.1663 | 0.1953
Epoch 70/300, seasonal_0 Loss: 0.1646 | 0.1941
Epoch 71/300, seasonal_0 Loss: 0.1638 | 0.1940
Epoch 72/300, seasonal_0 Loss: 0.1635 | 0.1896
Epoch 73/300, seasonal_0 Loss: 0.1619 | 0.1906
Epoch 74/300, seasonal_0 Loss: 0.1608 | 0.1885
Epoch 75/300, seasonal_0 Loss: 0.1592 | 0.1859
Epoch 76/300, seasonal_0 Loss: 0.1602 | 0.1879
Epoch 77/300, seasonal_0 Loss: 0.1595 | 0.1851
Epoch 78/300, seasonal_0 Loss: 0.1587 | 0.1859
Epoch 79/300, seasonal_0 Loss: 0.1591 | 0.1833
Epoch 80/300, seasonal_0 Loss: 0.1588 | 0.1832
Epoch 81/300, seasonal_0 Loss: 0.1597 | 0.1805
Epoch 82/300, seasonal_0 Loss: 0.1603 | 0.1790
Epoch 83/300, seasonal_0 Loss: 0.1600 | 0.1807
Epoch 84/300, seasonal_0 Loss: 0.1609 | 0.1772
Epoch 85/300, seasonal_0 Loss: 0.1597 | 0.1825
Epoch 86/300, seasonal_0 Loss: 0.1587 | 0.1766
Epoch 87/300, seasonal_0 Loss: 0.1578 | 0.1813
Epoch 88/300, seasonal_0 Loss: 0.1572 | 0.1745
Epoch 89/300, seasonal_0 Loss: 0.1560 | 0.1804
Epoch 90/300, seasonal_0 Loss: 0.1546 | 0.1736
Epoch 91/300, seasonal_0 Loss: 0.1527 | 0.1771
Epoch 92/300, seasonal_0 Loss: 0.1521 | 0.1720
Epoch 93/300, seasonal_0 Loss: 0.1508 | 0.1765
Epoch 94/300, seasonal_0 Loss: 0.1498 | 0.1704
Epoch 95/300, seasonal_0 Loss: 0.1499 | 0.1729
Epoch 96/300, seasonal_0 Loss: 0.1493 | 0.1698
Epoch 97/300, seasonal_0 Loss: 0.1482 | 0.1713
Epoch 98/300, seasonal_0 Loss: 0.1477 | 0.1680
Epoch 99/300, seasonal_0 Loss: 0.1478 | 0.1690
Epoch 100/300, seasonal_0 Loss: 0.1474 | 0.1676
Epoch 101/300, seasonal_0 Loss: 0.1472 | 0.1671
Epoch 102/300, seasonal_0 Loss: 0.1470 | 0.1657
Epoch 103/300, seasonal_0 Loss: 0.1462 | 0.1678
Epoch 104/300, seasonal_0 Loss: 0.1460 | 0.1644
Epoch 105/300, seasonal_0 Loss: 0.1459 | 0.1662
Epoch 106/300, seasonal_0 Loss: 0.1452 | 0.1650
Epoch 107/300, seasonal_0 Loss: 0.1454 | 0.1640
Epoch 108/300, seasonal_0 Loss: 0.1445 | 0.1624
Epoch 109/300, seasonal_0 Loss: 0.1445 | 0.1636
Epoch 110/300, seasonal_0 Loss: 0.1440 | 0.1640
Epoch 111/300, seasonal_0 Loss: 0.1443 | 0.1623
Epoch 112/300, seasonal_0 Loss: 0.1433 | 0.1609
Epoch 113/300, seasonal_0 Loss: 0.1431 | 0.1620
Epoch 114/300, seasonal_0 Loss: 0.1436 | 0.1598
Epoch 115/300, seasonal_0 Loss: 0.1425 | 0.1623
Epoch 116/300, seasonal_0 Loss: 0.1429 | 0.1602
Epoch 117/300, seasonal_0 Loss: 0.1419 | 0.1597
Epoch 118/300, seasonal_0 Loss: 0.1418 | 0.1593
Epoch 119/300, seasonal_0 Loss: 0.1423 | 0.1603
Epoch 120/300, seasonal_0 Loss: 0.1413 | 0.1581
Epoch 121/300, seasonal_0 Loss: 0.1406 | 0.1592
Epoch 122/300, seasonal_0 Loss: 0.1406 | 0.1596
Epoch 123/300, seasonal_0 Loss: 0.1407 | 0.1592
Epoch 124/300, seasonal_0 Loss: 0.1397 | 0.1566
Epoch 125/300, seasonal_0 Loss: 0.1396 | 0.1574
Epoch 126/300, seasonal_0 Loss: 0.1395 | 0.1584
Epoch 127/300, seasonal_0 Loss: 0.1393 | 0.1573
Epoch 128/300, seasonal_0 Loss: 0.1396 | 0.1569
Epoch 129/300, seasonal_0 Loss: 0.1393 | 0.1570
Epoch 130/300, seasonal_0 Loss: 0.1390 | 0.1566
Epoch 131/300, seasonal_0 Loss: 0.1388 | 0.1566
Epoch 132/300, seasonal_0 Loss: 0.1379 | 0.1557
Epoch 133/300, seasonal_0 Loss: 0.1379 | 0.1569
Epoch 134/300, seasonal_0 Loss: 0.1387 | 0.1572
Epoch 135/300, seasonal_0 Loss: 0.1377 | 0.1553
Epoch 136/300, seasonal_0 Loss: 0.1384 | 0.1569
Epoch 137/300, seasonal_0 Loss: 0.1376 | 0.1558
Epoch 138/300, seasonal_0 Loss: 0.1377 | 0.1548
Epoch 139/300, seasonal_0 Loss: 0.1381 | 0.1548
Epoch 140/300, seasonal_0 Loss: 0.1377 | 0.1539
Epoch 141/300, seasonal_0 Loss: 0.1375 | 0.1536
Epoch 142/300, seasonal_0 Loss: 0.1366 | 0.1545
Epoch 143/300, seasonal_0 Loss: 0.1371 | 0.1532
Epoch 144/300, seasonal_0 Loss: 0.1369 | 0.1535
Epoch 145/300, seasonal_0 Loss: 0.1364 | 0.1532
Epoch 146/300, seasonal_0 Loss: 0.1361 | 0.1528
Epoch 147/300, seasonal_0 Loss: 0.1362 | 0.1532
Epoch 148/300, seasonal_0 Loss: 0.1360 | 0.1540
Epoch 149/300, seasonal_0 Loss: 0.1363 | 0.1535
Epoch 150/300, seasonal_0 Loss: 0.1355 | 0.1533
Epoch 151/300, seasonal_0 Loss: 0.1364 | 0.1527
Epoch 152/300, seasonal_0 Loss: 0.1353 | 0.1527
Epoch 153/300, seasonal_0 Loss: 0.1361 | 0.1525
Epoch 154/300, seasonal_0 Loss: 0.1350 | 0.1524
Epoch 155/300, seasonal_0 Loss: 0.1346 | 0.1523
Epoch 156/300, seasonal_0 Loss: 0.1358 | 0.1517
Epoch 157/300, seasonal_0 Loss: 0.1352 | 0.1522
Epoch 158/300, seasonal_0 Loss: 0.1351 | 0.1515
Epoch 159/300, seasonal_0 Loss: 0.1352 | 0.1517
Epoch 160/300, seasonal_0 Loss: 0.1350 | 0.1507
Epoch 161/300, seasonal_0 Loss: 0.1342 | 0.1521
Epoch 162/300, seasonal_0 Loss: 0.1338 | 0.1515
Epoch 163/300, seasonal_0 Loss: 0.1338 | 0.1505
Epoch 164/300, seasonal_0 Loss: 0.1340 | 0.1508
Epoch 165/300, seasonal_0 Loss: 0.1340 | 0.1497
Epoch 166/300, seasonal_0 Loss: 0.1343 | 0.1505
Epoch 167/300, seasonal_0 Loss: 0.1339 | 0.1510
Epoch 168/300, seasonal_0 Loss: 0.1335 | 0.1506
Epoch 169/300, seasonal_0 Loss: 0.1341 | 0.1502
Epoch 170/300, seasonal_0 Loss: 0.1339 | 0.1500
Epoch 171/300, seasonal_0 Loss: 0.1331 | 0.1497
Epoch 172/300, seasonal_0 Loss: 0.1328 | 0.1500
Epoch 173/300, seasonal_0 Loss: 0.1332 | 0.1500
Epoch 174/300, seasonal_0 Loss: 0.1322 | 0.1506
Epoch 175/300, seasonal_0 Loss: 0.1332 | 0.1504
Epoch 176/300, seasonal_0 Loss: 0.1332 | 0.1503
Epoch 177/300, seasonal_0 Loss: 0.1330 | 0.1499
Epoch 178/300, seasonal_0 Loss: 0.1329 | 0.1496
Epoch 179/300, seasonal_0 Loss: 0.1324 | 0.1499
Epoch 180/300, seasonal_0 Loss: 0.1323 | 0.1500
Epoch 181/300, seasonal_0 Loss: 0.1327 | 0.1492
Epoch 182/300, seasonal_0 Loss: 0.1320 | 0.1496
Epoch 183/300, seasonal_0 Loss: 0.1327 | 0.1494
Epoch 184/300, seasonal_0 Loss: 0.1324 | 0.1493
Epoch 185/300, seasonal_0 Loss: 0.1326 | 0.1493
Epoch 186/300, seasonal_0 Loss: 0.1324 | 0.1496
Epoch 187/300, seasonal_0 Loss: 0.1319 | 0.1490
Epoch 188/300, seasonal_0 Loss: 0.1325 | 0.1487
Epoch 189/300, seasonal_0 Loss: 0.1329 | 0.1492
Epoch 190/300, seasonal_0 Loss: 0.1317 | 0.1490
Epoch 191/300, seasonal_0 Loss: 0.1315 | 0.1498
Epoch 192/300, seasonal_0 Loss: 0.1324 | 0.1495
Epoch 193/300, seasonal_0 Loss: 0.1321 | 0.1491
Epoch 194/300, seasonal_0 Loss: 0.1320 | 0.1482
Epoch 195/300, seasonal_0 Loss: 0.1325 | 0.1484
Epoch 196/300, seasonal_0 Loss: 0.1313 | 0.1489
Epoch 197/300, seasonal_0 Loss: 0.1317 | 0.1490
Epoch 198/300, seasonal_0 Loss: 0.1310 | 0.1488
Epoch 199/300, seasonal_0 Loss: 0.1312 | 0.1490
Epoch 200/300, seasonal_0 Loss: 0.1314 | 0.1485
Epoch 201/300, seasonal_0 Loss: 0.1314 | 0.1481
Epoch 202/300, seasonal_0 Loss: 0.1316 | 0.1486
Epoch 203/300, seasonal_0 Loss: 0.1319 | 0.1484
Epoch 204/300, seasonal_0 Loss: 0.1308 | 0.1480
Epoch 205/300, seasonal_0 Loss: 0.1311 | 0.1478
Epoch 206/300, seasonal_0 Loss: 0.1308 | 0.1479
Epoch 207/300, seasonal_0 Loss: 0.1308 | 0.1482
Epoch 208/300, seasonal_0 Loss: 0.1307 | 0.1486
Epoch 209/300, seasonal_0 Loss: 0.1325 | 0.1486
Epoch 210/300, seasonal_0 Loss: 0.1309 | 0.1484
Epoch 211/300, seasonal_0 Loss: 0.1314 | 0.1479
Epoch 212/300, seasonal_0 Loss: 0.1309 | 0.1479
Epoch 213/300, seasonal_0 Loss: 0.1306 | 0.1482
Epoch 214/300, seasonal_0 Loss: 0.1306 | 0.1478
Epoch 215/300, seasonal_0 Loss: 0.1300 | 0.1479
Epoch 216/300, seasonal_0 Loss: 0.1316 | 0.1480
Epoch 217/300, seasonal_0 Loss: 0.1302 | 0.1483
Epoch 218/300, seasonal_0 Loss: 0.1308 | 0.1482
Epoch 219/300, seasonal_0 Loss: 0.1312 | 0.1481
Epoch 220/300, seasonal_0 Loss: 0.1305 | 0.1484
Epoch 221/300, seasonal_0 Loss: 0.1304 | 0.1482
Epoch 222/300, seasonal_0 Loss: 0.1302 | 0.1481
Epoch 223/300, seasonal_0 Loss: 0.1309 | 0.1475
Epoch 224/300, seasonal_0 Loss: 0.1298 | 0.1477
Epoch 225/300, seasonal_0 Loss: 0.1307 | 0.1476
Epoch 226/300, seasonal_0 Loss: 0.1302 | 0.1474
Epoch 227/300, seasonal_0 Loss: 0.1300 | 0.1472
Epoch 228/300, seasonal_0 Loss: 0.1297 | 0.1475
Epoch 229/300, seasonal_0 Loss: 0.1302 | 0.1481
Epoch 230/300, seasonal_0 Loss: 0.1302 | 0.1482
Epoch 231/300, seasonal_0 Loss: 0.1298 | 0.1477
Epoch 232/300, seasonal_0 Loss: 0.1299 | 0.1475
Epoch 233/300, seasonal_0 Loss: 0.1296 | 0.1477
Epoch 234/300, seasonal_0 Loss: 0.1297 | 0.1478
Epoch 235/300, seasonal_0 Loss: 0.1295 | 0.1478
Epoch 236/300, seasonal_0 Loss: 0.1294 | 0.1480
Epoch 237/300, seasonal_0 Loss: 0.1303 | 0.1479
Epoch 238/300, seasonal_0 Loss: 0.1295 | 0.1478
Epoch 239/300, seasonal_0 Loss: 0.1292 | 0.1476
Epoch 240/300, seasonal_0 Loss: 0.1296 | 0.1475
Epoch 241/300, seasonal_0 Loss: 0.1295 | 0.1474
Epoch 242/300, seasonal_0 Loss: 0.1296 | 0.1477
Epoch 243/300, seasonal_0 Loss: 0.1297 | 0.1476
Epoch 244/300, seasonal_0 Loss: 0.1297 | 0.1475
Epoch 245/300, seasonal_0 Loss: 0.1291 | 0.1475
Epoch 246/300, seasonal_0 Loss: 0.1300 | 0.1474
Epoch 247/300, seasonal_0 Loss: 0.1297 | 0.1472
Epoch 248/300, seasonal_0 Loss: 0.1293 | 0.1472
Epoch 249/300, seasonal_0 Loss: 0.1293 | 0.1473
Epoch 250/300, seasonal_0 Loss: 0.1294 | 0.1473
Epoch 251/300, seasonal_0 Loss: 0.1286 | 0.1471
Epoch 252/300, seasonal_0 Loss: 0.1295 | 0.1469
Epoch 253/300, seasonal_0 Loss: 0.1284 | 0.1470
Epoch 254/300, seasonal_0 Loss: 0.1303 | 0.1470
Epoch 255/300, seasonal_0 Loss: 0.1293 | 0.1469
Epoch 256/300, seasonal_0 Loss: 0.1304 | 0.1466
Epoch 257/300, seasonal_0 Loss: 0.1299 | 0.1469
Epoch 258/300, seasonal_0 Loss: 0.1297 | 0.1471
Epoch 259/300, seasonal_0 Loss: 0.1295 | 0.1469
Epoch 260/300, seasonal_0 Loss: 0.1296 | 0.1470
Epoch 261/300, seasonal_0 Loss: 0.1287 | 0.1470
Epoch 262/300, seasonal_0 Loss: 0.1293 | 0.1469
Epoch 263/300, seasonal_0 Loss: 0.1298 | 0.1472
Epoch 264/300, seasonal_0 Loss: 0.1297 | 0.1473
Epoch 265/300, seasonal_0 Loss: 0.1292 | 0.1475
Epoch 266/300, seasonal_0 Loss: 0.1288 | 0.1473
Epoch 267/300, seasonal_0 Loss: 0.1289 | 0.1472
Epoch 268/300, seasonal_0 Loss: 0.1288 | 0.1472
Epoch 269/300, seasonal_0 Loss: 0.1285 | 0.1470
Epoch 270/300, seasonal_0 Loss: 0.1292 | 0.1470
Epoch 271/300, seasonal_0 Loss: 0.1290 | 0.1470
Epoch 272/300, seasonal_0 Loss: 0.1293 | 0.1470
Epoch 273/300, seasonal_0 Loss: 0.1290 | 0.1470
Epoch 274/300, seasonal_0 Loss: 0.1295 | 0.1470
Epoch 275/300, seasonal_0 Loss: 0.1282 | 0.1473
Epoch 276/300, seasonal_0 Loss: 0.1286 | 0.1473
Epoch 277/300, seasonal_0 Loss: 0.1281 | 0.1473
Epoch 278/300, seasonal_0 Loss: 0.1286 | 0.1472
Epoch 279/300, seasonal_0 Loss: 0.1291 | 0.1471
Epoch 280/300, seasonal_0 Loss: 0.1289 | 0.1472
Epoch 281/300, seasonal_0 Loss: 0.1278 | 0.1473
Epoch 282/300, seasonal_0 Loss: 0.1294 | 0.1472
Epoch 283/300, seasonal_0 Loss: 0.1286 | 0.1471
Epoch 284/300, seasonal_0 Loss: 0.1287 | 0.1473
Epoch 285/300, seasonal_0 Loss: 0.1287 | 0.1472
Epoch 286/300, seasonal_0 Loss: 0.1289 | 0.1472
Epoch 287/300, seasonal_0 Loss: 0.1285 | 0.1470
Epoch 288/300, seasonal_0 Loss: 0.1288 | 0.1469
Epoch 289/300, seasonal_0 Loss: 0.1288 | 0.1469
Epoch 290/300, seasonal_0 Loss: 0.1294 | 0.1470
Epoch 291/300, seasonal_0 Loss: 0.1288 | 0.1471
Epoch 292/300, seasonal_0 Loss: 0.1285 | 0.1471
Epoch 293/300, seasonal_0 Loss: 0.1286 | 0.1470
Epoch 294/300, seasonal_0 Loss: 0.1289 | 0.1468
Epoch 295/300, seasonal_0 Loss: 0.1287 | 0.1466
Epoch 296/300, seasonal_0 Loss: 0.1282 | 0.1468
Epoch 297/300, seasonal_0 Loss: 0.1286 | 0.1467
Epoch 298/300, seasonal_0 Loss: 0.1283 | 0.1467
Epoch 299/300, seasonal_0 Loss: 0.1290 | 0.1470
Epoch 300/300, seasonal_0 Loss: 0.1294 | 0.1470
Training seasonal_1 component with params: {'observation_period_num': 20, 'train_rates': 0.915237098231959, 'learning_rate': 7.378012325545223e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.8439421450256546}
Epoch 1/300, seasonal_1 Loss: 0.6184 | 0.9643
Epoch 2/300, seasonal_1 Loss: 0.4620 | 0.8454
Epoch 3/300, seasonal_1 Loss: 0.4016 | 0.7760
Epoch 4/300, seasonal_1 Loss: 0.3596 | 0.7763
Epoch 5/300, seasonal_1 Loss: 0.3396 | 0.6116
Epoch 6/300, seasonal_1 Loss: 0.3316 | 0.5970
Epoch 7/300, seasonal_1 Loss: 0.3002 | 0.5548
Epoch 8/300, seasonal_1 Loss: 0.3161 | 0.5246
Epoch 9/300, seasonal_1 Loss: 0.2996 | 0.4920
Epoch 10/300, seasonal_1 Loss: 0.2472 | 0.4622
Epoch 11/300, seasonal_1 Loss: 0.2383 | 0.4554
Epoch 12/300, seasonal_1 Loss: 0.2212 | 0.4228
Epoch 13/300, seasonal_1 Loss: 0.2142 | 0.4117
Epoch 14/300, seasonal_1 Loss: 0.2094 | 0.3888
Epoch 15/300, seasonal_1 Loss: 0.2039 | 0.3731
Epoch 16/300, seasonal_1 Loss: 0.1995 | 0.3711
Epoch 17/300, seasonal_1 Loss: 0.1954 | 0.3705
Epoch 18/300, seasonal_1 Loss: 0.1917 | 0.3519
Epoch 19/300, seasonal_1 Loss: 0.1891 | 0.3395
Epoch 20/300, seasonal_1 Loss: 0.1870 | 0.3376
Epoch 21/300, seasonal_1 Loss: 0.1843 | 0.3435
Epoch 22/300, seasonal_1 Loss: 0.1818 | 0.3281
Epoch 23/300, seasonal_1 Loss: 0.1797 | 0.3172
Epoch 24/300, seasonal_1 Loss: 0.1771 | 0.3195
Epoch 25/300, seasonal_1 Loss: 0.1761 | 0.3217
Epoch 26/300, seasonal_1 Loss: 0.1745 | 0.3140
Epoch 27/300, seasonal_1 Loss: 0.1723 | 0.3076
Epoch 28/300, seasonal_1 Loss: 0.1720 | 0.3066
Epoch 29/300, seasonal_1 Loss: 0.1707 | 0.3061
Epoch 30/300, seasonal_1 Loss: 0.1690 | 0.3022
Epoch 31/300, seasonal_1 Loss: 0.1674 | 0.3000
Epoch 32/300, seasonal_1 Loss: 0.1663 | 0.2999
Epoch 33/300, seasonal_1 Loss: 0.1653 | 0.2954
Epoch 34/300, seasonal_1 Loss: 0.1644 | 0.2928
Epoch 35/300, seasonal_1 Loss: 0.1642 | 0.2930
Epoch 36/300, seasonal_1 Loss: 0.1631 | 0.2920
Epoch 37/300, seasonal_1 Loss: 0.1630 | 0.2904
Epoch 38/300, seasonal_1 Loss: 0.1618 | 0.2891
Epoch 39/300, seasonal_1 Loss: 0.1609 | 0.2886
Epoch 40/300, seasonal_1 Loss: 0.1622 | 0.2859
Epoch 41/300, seasonal_1 Loss: 0.1602 | 0.2848
Epoch 42/300, seasonal_1 Loss: 0.1597 | 0.2846
Epoch 43/300, seasonal_1 Loss: 0.1595 | 0.2834
Epoch 44/300, seasonal_1 Loss: 0.1589 | 0.2839
Epoch 45/300, seasonal_1 Loss: 0.1592 | 0.2829
Epoch 46/300, seasonal_1 Loss: 0.1584 | 0.2819
Epoch 47/300, seasonal_1 Loss: 0.1583 | 0.2804
Epoch 48/300, seasonal_1 Loss: 0.1575 | 0.2813
Epoch 49/300, seasonal_1 Loss: 0.1577 | 0.2807
Epoch 50/300, seasonal_1 Loss: 0.1576 | 0.2798
Epoch 51/300, seasonal_1 Loss: 0.1565 | 0.2793
Epoch 52/300, seasonal_1 Loss: 0.1574 | 0.2789
Epoch 53/300, seasonal_1 Loss: 0.1563 | 0.2780
Epoch 54/300, seasonal_1 Loss: 0.1558 | 0.2776
Epoch 55/300, seasonal_1 Loss: 0.1565 | 0.2768
Epoch 56/300, seasonal_1 Loss: 0.1560 | 0.2766
Epoch 57/300, seasonal_1 Loss: 0.1556 | 0.2764
Epoch 58/300, seasonal_1 Loss: 0.1554 | 0.2761
Epoch 59/300, seasonal_1 Loss: 0.1548 | 0.2762
Epoch 60/300, seasonal_1 Loss: 0.1557 | 0.2758
Epoch 61/300, seasonal_1 Loss: 0.1542 | 0.2757
Epoch 62/300, seasonal_1 Loss: 0.1546 | 0.2747
Epoch 63/300, seasonal_1 Loss: 0.1548 | 0.2748
Epoch 64/300, seasonal_1 Loss: 0.1546 | 0.2753
Epoch 65/300, seasonal_1 Loss: 0.1545 | 0.2751
Epoch 66/300, seasonal_1 Loss: 0.1547 | 0.2751
Epoch 67/300, seasonal_1 Loss: 0.1541 | 0.2745
Epoch 68/300, seasonal_1 Loss: 0.1545 | 0.2744
Epoch 69/300, seasonal_1 Loss: 0.1543 | 0.2740
Epoch 70/300, seasonal_1 Loss: 0.1549 | 0.2739
Epoch 71/300, seasonal_1 Loss: 0.1535 | 0.2735
Epoch 72/300, seasonal_1 Loss: 0.1544 | 0.2734
Epoch 73/300, seasonal_1 Loss: 0.1541 | 0.2732
Epoch 74/300, seasonal_1 Loss: 0.1537 | 0.2730
Epoch 75/300, seasonal_1 Loss: 0.1539 | 0.2729
Epoch 76/300, seasonal_1 Loss: 0.1544 | 0.2730
Epoch 77/300, seasonal_1 Loss: 0.1542 | 0.2730
Epoch 78/300, seasonal_1 Loss: 0.1540 | 0.2729
Epoch 79/300, seasonal_1 Loss: 0.1540 | 0.2728
Epoch 80/300, seasonal_1 Loss: 0.1539 | 0.2727
Epoch 81/300, seasonal_1 Loss: 0.1535 | 0.2727
Epoch 82/300, seasonal_1 Loss: 0.1537 | 0.2727
Epoch 83/300, seasonal_1 Loss: 0.1536 | 0.2726
Epoch 84/300, seasonal_1 Loss: 0.1535 | 0.2726
Epoch 85/300, seasonal_1 Loss: 0.1537 | 0.2725
Epoch 86/300, seasonal_1 Loss: 0.1535 | 0.2725
Epoch 87/300, seasonal_1 Loss: 0.1534 | 0.2724
Epoch 88/300, seasonal_1 Loss: 0.1533 | 0.2723
Epoch 89/300, seasonal_1 Loss: 0.1541 | 0.2722
Epoch 90/300, seasonal_1 Loss: 0.1542 | 0.2722
Epoch 91/300, seasonal_1 Loss: 0.1532 | 0.2723
Epoch 92/300, seasonal_1 Loss: 0.1536 | 0.2723
Epoch 93/300, seasonal_1 Loss: 0.1536 | 0.2724
Epoch 94/300, seasonal_1 Loss: 0.1534 | 0.2724
Epoch 95/300, seasonal_1 Loss: 0.1535 | 0.2723
Epoch 96/300, seasonal_1 Loss: 0.1535 | 0.2723
Epoch 97/300, seasonal_1 Loss: 0.1539 | 0.2723
Epoch 98/300, seasonal_1 Loss: 0.1537 | 0.2723
Epoch 99/300, seasonal_1 Loss: 0.1535 | 0.2723
Epoch 100/300, seasonal_1 Loss: 0.1545 | 0.2722
Epoch 101/300, seasonal_1 Loss: 0.1531 | 0.2722
Epoch 102/300, seasonal_1 Loss: 0.1536 | 0.2722
Epoch 103/300, seasonal_1 Loss: 0.1543 | 0.2722
Epoch 104/300, seasonal_1 Loss: 0.1535 | 0.2721
Epoch 105/300, seasonal_1 Loss: 0.1538 | 0.2721
Epoch 106/300, seasonal_1 Loss: 0.1540 | 0.2721
Epoch 107/300, seasonal_1 Loss: 0.1531 | 0.2721
Epoch 108/300, seasonal_1 Loss: 0.1539 | 0.2721
Epoch 109/300, seasonal_1 Loss: 0.1541 | 0.2721
Epoch 110/300, seasonal_1 Loss: 0.1536 | 0.2721
Epoch 111/300, seasonal_1 Loss: 0.1533 | 0.2721
Epoch 112/300, seasonal_1 Loss: 0.1530 | 0.2721
Epoch 113/300, seasonal_1 Loss: 0.1532 | 0.2721
Epoch 114/300, seasonal_1 Loss: 0.1535 | 0.2721
Epoch 115/300, seasonal_1 Loss: 0.1536 | 0.2721
Epoch 116/300, seasonal_1 Loss: 0.1534 | 0.2721
Epoch 117/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 118/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 119/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 120/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 121/300, seasonal_1 Loss: 0.1543 | 0.2720
Epoch 122/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 123/300, seasonal_1 Loss: 0.1532 | 0.2720
Epoch 124/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 125/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 126/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 127/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 128/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 129/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 130/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 131/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 132/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 133/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 134/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 135/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 136/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 137/300, seasonal_1 Loss: 0.1547 | 0.2720
Epoch 138/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 139/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 140/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 141/300, seasonal_1 Loss: 0.1530 | 0.2720
Epoch 142/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 143/300, seasonal_1 Loss: 0.1529 | 0.2720
Epoch 144/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 145/300, seasonal_1 Loss: 0.1542 | 0.2720
Epoch 146/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 147/300, seasonal_1 Loss: 0.1532 | 0.2720
Epoch 148/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 149/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 150/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 151/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 152/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 153/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 154/300, seasonal_1 Loss: 0.1543 | 0.2720
Epoch 155/300, seasonal_1 Loss: 0.1529 | 0.2720
Epoch 156/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 157/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 158/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 159/300, seasonal_1 Loss: 0.1532 | 0.2720
Epoch 160/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 161/300, seasonal_1 Loss: 0.1529 | 0.2720
Epoch 162/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 163/300, seasonal_1 Loss: 0.1530 | 0.2720
Epoch 164/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 165/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 166/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 167/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 168/300, seasonal_1 Loss: 0.1522 | 0.2720
Epoch 169/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 170/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 171/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 172/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 173/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 174/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 175/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 176/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 177/300, seasonal_1 Loss: 0.1529 | 0.2720
Epoch 178/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 179/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 180/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 181/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 182/300, seasonal_1 Loss: 0.1532 | 0.2720
Epoch 183/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 184/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 185/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 186/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 187/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 188/300, seasonal_1 Loss: 0.1540 | 0.2720
Epoch 189/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 190/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 191/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 192/300, seasonal_1 Loss: 0.1543 | 0.2720
Epoch 193/300, seasonal_1 Loss: 0.1528 | 0.2720
Epoch 194/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 195/300, seasonal_1 Loss: 0.1530 | 0.2720
Epoch 196/300, seasonal_1 Loss: 0.1544 | 0.2720
Epoch 197/300, seasonal_1 Loss: 0.1539 | 0.2720
Epoch 198/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 199/300, seasonal_1 Loss: 0.1536 | 0.2720
Epoch 200/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 201/300, seasonal_1 Loss: 0.1527 | 0.2720
Epoch 202/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 203/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 204/300, seasonal_1 Loss: 0.1537 | 0.2720
Epoch 205/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 206/300, seasonal_1 Loss: 0.1544 | 0.2720
Epoch 207/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 208/300, seasonal_1 Loss: 0.1534 | 0.2720
Epoch 209/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 210/300, seasonal_1 Loss: 0.1532 | 0.2720
Epoch 211/300, seasonal_1 Loss: 0.1535 | 0.2720
Epoch 212/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 213/300, seasonal_1 Loss: 0.1527 | 0.2720
Epoch 214/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 215/300, seasonal_1 Loss: 0.1531 | 0.2720
Epoch 216/300, seasonal_1 Loss: 0.1527 | 0.2720
Epoch 217/300, seasonal_1 Loss: 0.1538 | 0.2720
Epoch 218/300, seasonal_1 Loss: 0.1533 | 0.2720
Epoch 219/300, seasonal_1 Loss: 0.1540 | 0.2720
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 110, 'train_rates': 0.9084141193572343, 'learning_rate': 0.00012839717656216377, 'batch_size': 31, 'step_size': 11, 'gamma': 0.7985272725664257}
Epoch 1/300, seasonal_2 Loss: 0.7596 | 1.2231
Epoch 2/300, seasonal_2 Loss: 0.5809 | 1.0690
Epoch 3/300, seasonal_2 Loss: 0.5035 | 0.9952
Epoch 4/300, seasonal_2 Loss: 0.5212 | 0.9198
Epoch 5/300, seasonal_2 Loss: 0.4781 | 0.8942
Epoch 6/300, seasonal_2 Loss: 0.4360 | 0.8900
Epoch 7/300, seasonal_2 Loss: 0.3932 | 0.7316
Epoch 8/300, seasonal_2 Loss: 0.3244 | 0.6542
Epoch 9/300, seasonal_2 Loss: 0.2975 | 0.6281
Epoch 10/300, seasonal_2 Loss: 0.2884 | 0.5925
Epoch 11/300, seasonal_2 Loss: 0.2708 | 0.5477
Epoch 12/300, seasonal_2 Loss: 0.2651 | 0.5409
Epoch 13/300, seasonal_2 Loss: 0.2541 | 0.5322
Epoch 14/300, seasonal_2 Loss: 0.2399 | 0.4823
Epoch 15/300, seasonal_2 Loss: 0.2366 | 0.4841
Epoch 16/300, seasonal_2 Loss: 0.2397 | 0.5249
Epoch 17/300, seasonal_2 Loss: 0.2528 | 0.5334
Epoch 18/300, seasonal_2 Loss: 0.2518 | 0.4536
Epoch 19/300, seasonal_2 Loss: 0.2536 | 0.4765
Epoch 20/300, seasonal_2 Loss: 0.2381 | 0.5079
Epoch 21/300, seasonal_2 Loss: 0.2216 | 0.4648
Epoch 22/300, seasonal_2 Loss: 0.2213 | 0.4176
Epoch 23/300, seasonal_2 Loss: 0.2093 | 0.4003
Epoch 24/300, seasonal_2 Loss: 0.2018 | 0.4002
Epoch 25/300, seasonal_2 Loss: 0.1975 | 0.3848
Epoch 26/300, seasonal_2 Loss: 0.1909 | 0.3646
Epoch 27/300, seasonal_2 Loss: 0.1851 | 0.3619
Epoch 28/300, seasonal_2 Loss: 0.1830 | 0.3518
Epoch 29/300, seasonal_2 Loss: 0.1804 | 0.3450
Epoch 30/300, seasonal_2 Loss: 0.1750 | 0.3376
Epoch 31/300, seasonal_2 Loss: 0.1743 | 0.3378
Epoch 32/300, seasonal_2 Loss: 0.1719 | 0.3286
Epoch 33/300, seasonal_2 Loss: 0.1707 | 0.3181
Epoch 34/300, seasonal_2 Loss: 0.1695 | 0.3235
Epoch 35/300, seasonal_2 Loss: 0.1678 | 0.3172
Epoch 36/300, seasonal_2 Loss: 0.1654 | 0.3099
Epoch 37/300, seasonal_2 Loss: 0.1635 | 0.3057
Epoch 38/300, seasonal_2 Loss: 0.1628 | 0.3063
Epoch 39/300, seasonal_2 Loss: 0.1614 | 0.2999
Epoch 40/300, seasonal_2 Loss: 0.1604 | 0.2955
Epoch 41/300, seasonal_2 Loss: 0.1590 | 0.2937
Epoch 42/300, seasonal_2 Loss: 0.1579 | 0.2912
Epoch 43/300, seasonal_2 Loss: 0.1565 | 0.2879
Epoch 44/300, seasonal_2 Loss: 0.1567 | 0.2877
Epoch 45/300, seasonal_2 Loss: 0.1553 | 0.2826
Epoch 46/300, seasonal_2 Loss: 0.1539 | 0.2814
Epoch 47/300, seasonal_2 Loss: 0.1532 | 0.2806
Epoch 48/300, seasonal_2 Loss: 0.1526 | 0.2764
Epoch 49/300, seasonal_2 Loss: 0.1507 | 0.2773
Epoch 50/300, seasonal_2 Loss: 0.1501 | 0.2754
Epoch 51/300, seasonal_2 Loss: 0.1498 | 0.2699
Epoch 52/300, seasonal_2 Loss: 0.1482 | 0.2687
Epoch 53/300, seasonal_2 Loss: 0.1489 | 0.2677
Epoch 54/300, seasonal_2 Loss: 0.1475 | 0.2649
Epoch 55/300, seasonal_2 Loss: 0.1468 | 0.2648
Epoch 56/300, seasonal_2 Loss: 0.1463 | 0.2645
Epoch 57/300, seasonal_2 Loss: 0.1447 | 0.2626
Epoch 58/300, seasonal_2 Loss: 0.1453 | 0.2607
Epoch 59/300, seasonal_2 Loss: 0.1444 | 0.2607
Epoch 60/300, seasonal_2 Loss: 0.1443 | 0.2593
Epoch 61/300, seasonal_2 Loss: 0.1430 | 0.2582
Epoch 62/300, seasonal_2 Loss: 0.1421 | 0.2562
Epoch 63/300, seasonal_2 Loss: 0.1418 | 0.2554
Epoch 64/300, seasonal_2 Loss: 0.1413 | 0.2546
Epoch 65/300, seasonal_2 Loss: 0.1411 | 0.2545
Epoch 66/300, seasonal_2 Loss: 0.1410 | 0.2528
Epoch 67/300, seasonal_2 Loss: 0.1400 | 0.2509
Epoch 68/300, seasonal_2 Loss: 0.1398 | 0.2512
Epoch 69/300, seasonal_2 Loss: 0.1396 | 0.2504
Epoch 70/300, seasonal_2 Loss: 0.1400 | 0.2496
Epoch 71/300, seasonal_2 Loss: 0.1391 | 0.2487
Epoch 72/300, seasonal_2 Loss: 0.1389 | 0.2486
Epoch 73/300, seasonal_2 Loss: 0.1384 | 0.2472
Epoch 74/300, seasonal_2 Loss: 0.1381 | 0.2467
Epoch 75/300, seasonal_2 Loss: 0.1380 | 0.2457
Epoch 76/300, seasonal_2 Loss: 0.1371 | 0.2455
Epoch 77/300, seasonal_2 Loss: 0.1377 | 0.2451
Epoch 78/300, seasonal_2 Loss: 0.1365 | 0.2449
Epoch 79/300, seasonal_2 Loss: 0.1368 | 0.2434
Epoch 80/300, seasonal_2 Loss: 0.1370 | 0.2433
Epoch 81/300, seasonal_2 Loss: 0.1366 | 0.2427
Epoch 82/300, seasonal_2 Loss: 0.1369 | 0.2427
Epoch 83/300, seasonal_2 Loss: 0.1363 | 0.2423
Epoch 84/300, seasonal_2 Loss: 0.1364 | 0.2417
Epoch 85/300, seasonal_2 Loss: 0.1356 | 0.2416
Epoch 86/300, seasonal_2 Loss: 0.1358 | 0.2415
Epoch 87/300, seasonal_2 Loss: 0.1358 | 0.2405
Epoch 88/300, seasonal_2 Loss: 0.1348 | 0.2399
Epoch 89/300, seasonal_2 Loss: 0.1351 | 0.2398
Epoch 90/300, seasonal_2 Loss: 0.1345 | 0.2396
Epoch 91/300, seasonal_2 Loss: 0.1344 | 0.2395
Epoch 92/300, seasonal_2 Loss: 0.1351 | 0.2392
Epoch 93/300, seasonal_2 Loss: 0.1352 | 0.2392
Epoch 94/300, seasonal_2 Loss: 0.1337 | 0.2381
Epoch 95/300, seasonal_2 Loss: 0.1343 | 0.2382
Epoch 96/300, seasonal_2 Loss: 0.1337 | 0.2385
Epoch 97/300, seasonal_2 Loss: 0.1344 | 0.2383
Epoch 98/300, seasonal_2 Loss: 0.1336 | 0.2382
Epoch 99/300, seasonal_2 Loss: 0.1339 | 0.2380
Epoch 100/300, seasonal_2 Loss: 0.1341 | 0.2377
Epoch 101/300, seasonal_2 Loss: 0.1337 | 0.2376
Epoch 102/300, seasonal_2 Loss: 0.1341 | 0.2372
Epoch 103/300, seasonal_2 Loss: 0.1337 | 0.2372
Epoch 104/300, seasonal_2 Loss: 0.1339 | 0.2371
Epoch 105/300, seasonal_2 Loss: 0.1341 | 0.2366
Epoch 106/300, seasonal_2 Loss: 0.1328 | 0.2366
Epoch 107/300, seasonal_2 Loss: 0.1337 | 0.2362
Epoch 108/300, seasonal_2 Loss: 0.1330 | 0.2358
Epoch 109/300, seasonal_2 Loss: 0.1331 | 0.2359
Epoch 110/300, seasonal_2 Loss: 0.1333 | 0.2362
Epoch 111/300, seasonal_2 Loss: 0.1336 | 0.2361
Epoch 112/300, seasonal_2 Loss: 0.1338 | 0.2356
Epoch 113/300, seasonal_2 Loss: 0.1322 | 0.2355
Epoch 114/300, seasonal_2 Loss: 0.1326 | 0.2352
Epoch 115/300, seasonal_2 Loss: 0.1337 | 0.2352
Epoch 116/300, seasonal_2 Loss: 0.1328 | 0.2353
Epoch 117/300, seasonal_2 Loss: 0.1324 | 0.2353
Epoch 118/300, seasonal_2 Loss: 0.1334 | 0.2349
Epoch 119/300, seasonal_2 Loss: 0.1325 | 0.2347
Epoch 120/300, seasonal_2 Loss: 0.1321 | 0.2349
Epoch 121/300, seasonal_2 Loss: 0.1323 | 0.2349
Epoch 122/300, seasonal_2 Loss: 0.1317 | 0.2346
Epoch 123/300, seasonal_2 Loss: 0.1327 | 0.2345
Epoch 124/300, seasonal_2 Loss: 0.1324 | 0.2343
Epoch 125/300, seasonal_2 Loss: 0.1335 | 0.2342
Epoch 126/300, seasonal_2 Loss: 0.1328 | 0.2342
Epoch 127/300, seasonal_2 Loss: 0.1322 | 0.2339
Epoch 128/300, seasonal_2 Loss: 0.1322 | 0.2337
Epoch 129/300, seasonal_2 Loss: 0.1320 | 0.2337
Epoch 130/300, seasonal_2 Loss: 0.1320 | 0.2337
Epoch 131/300, seasonal_2 Loss: 0.1319 | 0.2337
Epoch 132/300, seasonal_2 Loss: 0.1320 | 0.2338
Epoch 133/300, seasonal_2 Loss: 0.1322 | 0.2337
Epoch 134/300, seasonal_2 Loss: 0.1322 | 0.2337
Epoch 135/300, seasonal_2 Loss: 0.1318 | 0.2337
Epoch 136/300, seasonal_2 Loss: 0.1327 | 0.2337
Epoch 137/300, seasonal_2 Loss: 0.1320 | 0.2336
Epoch 138/300, seasonal_2 Loss: 0.1320 | 0.2336
Epoch 139/300, seasonal_2 Loss: 0.1319 | 0.2336
Epoch 140/300, seasonal_2 Loss: 0.1322 | 0.2335
Epoch 141/300, seasonal_2 Loss: 0.1317 | 0.2333
Epoch 142/300, seasonal_2 Loss: 0.1315 | 0.2332
Epoch 143/300, seasonal_2 Loss: 0.1318 | 0.2332
Epoch 144/300, seasonal_2 Loss: 0.1315 | 0.2333
Epoch 145/300, seasonal_2 Loss: 0.1320 | 0.2332
Epoch 146/300, seasonal_2 Loss: 0.1321 | 0.2332
Epoch 147/300, seasonal_2 Loss: 0.1319 | 0.2332
Epoch 148/300, seasonal_2 Loss: 0.1324 | 0.2332
Epoch 149/300, seasonal_2 Loss: 0.1317 | 0.2332
Epoch 150/300, seasonal_2 Loss: 0.1321 | 0.2331
Epoch 151/300, seasonal_2 Loss: 0.1321 | 0.2331
Epoch 152/300, seasonal_2 Loss: 0.1319 | 0.2330
Epoch 153/300, seasonal_2 Loss: 0.1314 | 0.2330
Epoch 154/300, seasonal_2 Loss: 0.1315 | 0.2330
Epoch 155/300, seasonal_2 Loss: 0.1322 | 0.2330
Epoch 156/300, seasonal_2 Loss: 0.1317 | 0.2329
Epoch 157/300, seasonal_2 Loss: 0.1314 | 0.2329
Epoch 158/300, seasonal_2 Loss: 0.1315 | 0.2329
Epoch 159/300, seasonal_2 Loss: 0.1317 | 0.2329
Epoch 160/300, seasonal_2 Loss: 0.1317 | 0.2329
Epoch 161/300, seasonal_2 Loss: 0.1320 | 0.2328
Epoch 162/300, seasonal_2 Loss: 0.1316 | 0.2328
Epoch 163/300, seasonal_2 Loss: 0.1313 | 0.2328
Epoch 164/300, seasonal_2 Loss: 0.1321 | 0.2328
Epoch 165/300, seasonal_2 Loss: 0.1315 | 0.2328
Epoch 166/300, seasonal_2 Loss: 0.1315 | 0.2328
Epoch 167/300, seasonal_2 Loss: 0.1323 | 0.2328
Epoch 168/300, seasonal_2 Loss: 0.1323 | 0.2328
Epoch 169/300, seasonal_2 Loss: 0.1316 | 0.2327
Epoch 170/300, seasonal_2 Loss: 0.1318 | 0.2327
Epoch 171/300, seasonal_2 Loss: 0.1318 | 0.2327
Epoch 172/300, seasonal_2 Loss: 0.1317 | 0.2327
Epoch 173/300, seasonal_2 Loss: 0.1315 | 0.2327
Epoch 174/300, seasonal_2 Loss: 0.1321 | 0.2327
Epoch 175/300, seasonal_2 Loss: 0.1317 | 0.2327
Epoch 176/300, seasonal_2 Loss: 0.1317 | 0.2327
Epoch 177/300, seasonal_2 Loss: 0.1319 | 0.2327
Epoch 178/300, seasonal_2 Loss: 0.1317 | 0.2327
Epoch 179/300, seasonal_2 Loss: 0.1311 | 0.2327
Epoch 180/300, seasonal_2 Loss: 0.1313 | 0.2327
Epoch 181/300, seasonal_2 Loss: 0.1321 | 0.2327
Epoch 182/300, seasonal_2 Loss: 0.1315 | 0.2327
Epoch 183/300, seasonal_2 Loss: 0.1319 | 0.2327
Epoch 184/300, seasonal_2 Loss: 0.1317 | 0.2327
Epoch 185/300, seasonal_2 Loss: 0.1320 | 0.2326
Epoch 186/300, seasonal_2 Loss: 0.1314 | 0.2326
Epoch 187/300, seasonal_2 Loss: 0.1318 | 0.2326
Epoch 188/300, seasonal_2 Loss: 0.1314 | 0.2326
Epoch 189/300, seasonal_2 Loss: 0.1315 | 0.2326
Epoch 190/300, seasonal_2 Loss: 0.1319 | 0.2326
Epoch 191/300, seasonal_2 Loss: 0.1311 | 0.2326
Epoch 192/300, seasonal_2 Loss: 0.1311 | 0.2326
Epoch 193/300, seasonal_2 Loss: 0.1314 | 0.2326
Epoch 194/300, seasonal_2 Loss: 0.1317 | 0.2326
Epoch 195/300, seasonal_2 Loss: 0.1319 | 0.2326
Epoch 196/300, seasonal_2 Loss: 0.1322 | 0.2326
Epoch 197/300, seasonal_2 Loss: 0.1321 | 0.2326
Epoch 198/300, seasonal_2 Loss: 0.1312 | 0.2326
Epoch 199/300, seasonal_2 Loss: 0.1316 | 0.2326
Epoch 200/300, seasonal_2 Loss: 0.1319 | 0.2326
Epoch 201/300, seasonal_2 Loss: 0.1324 | 0.2326
Epoch 202/300, seasonal_2 Loss: 0.1321 | 0.2325
Epoch 203/300, seasonal_2 Loss: 0.1313 | 0.2325
Epoch 204/300, seasonal_2 Loss: 0.1323 | 0.2325
Epoch 205/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 206/300, seasonal_2 Loss: 0.1321 | 0.2325
Epoch 207/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 208/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 209/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 210/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 211/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 212/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 213/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 214/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 215/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 216/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 217/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 218/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 219/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 220/300, seasonal_2 Loss: 0.1321 | 0.2325
Epoch 221/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 222/300, seasonal_2 Loss: 0.1313 | 0.2325
Epoch 223/300, seasonal_2 Loss: 0.1326 | 0.2325
Epoch 224/300, seasonal_2 Loss: 0.1307 | 0.2325
Epoch 225/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 226/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 227/300, seasonal_2 Loss: 0.1313 | 0.2325
Epoch 228/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 229/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 230/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 231/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 232/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 233/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 234/300, seasonal_2 Loss: 0.1312 | 0.2325
Epoch 235/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 236/300, seasonal_2 Loss: 0.1328 | 0.2325
Epoch 237/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 238/300, seasonal_2 Loss: 0.1313 | 0.2325
Epoch 239/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 240/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 241/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 242/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 243/300, seasonal_2 Loss: 0.1310 | 0.2325
Epoch 244/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 245/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 246/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 247/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 248/300, seasonal_2 Loss: 0.1311 | 0.2325
Epoch 249/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 250/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 251/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 252/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 253/300, seasonal_2 Loss: 0.1312 | 0.2325
Epoch 254/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 255/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 256/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 257/300, seasonal_2 Loss: 0.1309 | 0.2325
Epoch 258/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 259/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 260/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 261/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 262/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 263/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 264/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 265/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 266/300, seasonal_2 Loss: 0.1325 | 0.2325
Epoch 267/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 268/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 269/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 270/300, seasonal_2 Loss: 0.1314 | 0.2325
Epoch 271/300, seasonal_2 Loss: 0.1307 | 0.2325
Epoch 272/300, seasonal_2 Loss: 0.1307 | 0.2325
Epoch 273/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 274/300, seasonal_2 Loss: 0.1322 | 0.2325
Epoch 275/300, seasonal_2 Loss: 0.1312 | 0.2325
Epoch 276/300, seasonal_2 Loss: 0.1315 | 0.2325
Epoch 277/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 278/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 279/300, seasonal_2 Loss: 0.1321 | 0.2325
Epoch 280/300, seasonal_2 Loss: 0.1317 | 0.2325
Epoch 281/300, seasonal_2 Loss: 0.1313 | 0.2325
Epoch 282/300, seasonal_2 Loss: 0.1321 | 0.2325
Epoch 283/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 284/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 285/300, seasonal_2 Loss: 0.1322 | 0.2325
Epoch 286/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 287/300, seasonal_2 Loss: 0.1319 | 0.2325
Epoch 288/300, seasonal_2 Loss: 0.1310 | 0.2325
Epoch 289/300, seasonal_2 Loss: 0.1310 | 0.2325
Epoch 290/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 291/300, seasonal_2 Loss: 0.1318 | 0.2325
Epoch 292/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 293/300, seasonal_2 Loss: 0.1322 | 0.2325
Epoch 294/300, seasonal_2 Loss: 0.1310 | 0.2325
Epoch 295/300, seasonal_2 Loss: 0.1316 | 0.2325
Epoch 296/300, seasonal_2 Loss: 0.1320 | 0.2325
Epoch 297/300, seasonal_2 Loss: 0.1307 | 0.2325
Epoch 298/300, seasonal_2 Loss: 0.1308 | 0.2325
Epoch 299/300, seasonal_2 Loss: 0.1312 | 0.2325
Epoch 300/300, seasonal_2 Loss: 0.1310 | 0.2325
Training seasonal_3 component with params: {'observation_period_num': 167, 'train_rates': 0.9866915906418945, 'learning_rate': 0.0001347443181048338, 'batch_size': 116, 'step_size': 2, 'gamma': 0.9894113063456607}
Epoch 1/300, seasonal_3 Loss: 1.1914 | 2.1930
Epoch 2/300, seasonal_3 Loss: 0.8041 | 1.3051
Epoch 3/300, seasonal_3 Loss: 0.6303 | 1.0709
Epoch 4/300, seasonal_3 Loss: 0.5544 | 0.9291
Epoch 5/300, seasonal_3 Loss: 0.5275 | 0.8913
Epoch 6/300, seasonal_3 Loss: 0.4786 | 0.8191
Epoch 7/300, seasonal_3 Loss: 0.4685 | 0.7707
Epoch 8/300, seasonal_3 Loss: 0.4289 | 0.7214
Epoch 9/300, seasonal_3 Loss: 0.4843 | 0.7009
Epoch 10/300, seasonal_3 Loss: 0.4273 | 0.7006
Epoch 11/300, seasonal_3 Loss: 0.4138 | 0.6319
Epoch 12/300, seasonal_3 Loss: 0.3754 | 0.6033
Epoch 13/300, seasonal_3 Loss: 0.3592 | 0.5509
Epoch 14/300, seasonal_3 Loss: 0.3161 | 0.5487
Epoch 15/300, seasonal_3 Loss: 0.3176 | 0.5107
Epoch 16/300, seasonal_3 Loss: 0.3469 | 0.5075
Epoch 17/300, seasonal_3 Loss: 0.3440 | 0.4747
Epoch 18/300, seasonal_3 Loss: 0.3610 | 0.4590
Epoch 19/300, seasonal_3 Loss: 0.2907 | 0.4810
Epoch 20/300, seasonal_3 Loss: 0.2809 | 0.4269
Epoch 21/300, seasonal_3 Loss: 0.2925 | 0.4231
Epoch 22/300, seasonal_3 Loss: 0.2713 | 0.4175
Epoch 23/300, seasonal_3 Loss: 0.2578 | 0.3842
Epoch 24/300, seasonal_3 Loss: 0.2724 | 0.3740
Epoch 25/300, seasonal_3 Loss: 0.2518 | 0.3644
Epoch 26/300, seasonal_3 Loss: 0.2616 | 0.3504
Epoch 27/300, seasonal_3 Loss: 0.2506 | 0.3446
Epoch 28/300, seasonal_3 Loss: 0.2572 | 0.3328
Epoch 29/300, seasonal_3 Loss: 0.2712 | 0.3227
Epoch 30/300, seasonal_3 Loss: 0.2384 | 0.3300
Epoch 31/300, seasonal_3 Loss: 0.2374 | 0.3136
Epoch 32/300, seasonal_3 Loss: 0.2485 | 0.3093
Epoch 33/300, seasonal_3 Loss: 0.2368 | 0.2950
Epoch 34/300, seasonal_3 Loss: 0.2456 | 0.2924
Epoch 35/300, seasonal_3 Loss: 0.2335 | 0.2922
Epoch 36/300, seasonal_3 Loss: 0.2308 | 0.2798
Epoch 37/300, seasonal_3 Loss: 0.2394 | 0.2737
Epoch 38/300, seasonal_3 Loss: 0.2235 | 0.2783
Epoch 39/300, seasonal_3 Loss: 0.2272 | 0.2687
Epoch 40/300, seasonal_3 Loss: 0.2313 | 0.2683
Epoch 41/300, seasonal_3 Loss: 0.2204 | 0.2630
Epoch 42/300, seasonal_3 Loss: 0.2210 | 0.2544
Epoch 43/300, seasonal_3 Loss: 0.2071 | 0.2537
Epoch 44/300, seasonal_3 Loss: 0.2047 | 0.2474
Epoch 45/300, seasonal_3 Loss: 0.2117 | 0.2415
Epoch 46/300, seasonal_3 Loss: 0.2011 | 0.2440
Epoch 47/300, seasonal_3 Loss: 0.2019 | 0.2341
Epoch 48/300, seasonal_3 Loss: 0.2081 | 0.2384
Epoch 49/300, seasonal_3 Loss: 0.2063 | 0.2269
Epoch 50/300, seasonal_3 Loss: 0.2093 | 0.2268
Epoch 51/300, seasonal_3 Loss: 0.1913 | 0.2265
Epoch 52/300, seasonal_3 Loss: 0.1890 | 0.2215
Epoch 53/300, seasonal_3 Loss: 0.1972 | 0.2198
Epoch 54/300, seasonal_3 Loss: 0.1948 | 0.2175
Epoch 55/300, seasonal_3 Loss: 0.1931 | 0.2117
Epoch 56/300, seasonal_3 Loss: 0.1824 | 0.2129
Epoch 57/300, seasonal_3 Loss: 0.1790 | 0.2104
Epoch 58/300, seasonal_3 Loss: 0.1806 | 0.2068
Epoch 59/300, seasonal_3 Loss: 0.1814 | 0.2059
Epoch 60/300, seasonal_3 Loss: 0.1822 | 0.2043
Epoch 61/300, seasonal_3 Loss: 0.1778 | 0.2010
Epoch 62/300, seasonal_3 Loss: 0.1748 | 0.2025
Epoch 63/300, seasonal_3 Loss: 0.1737 | 0.1974
Epoch 64/300, seasonal_3 Loss: 0.1713 | 0.1992
Epoch 65/300, seasonal_3 Loss: 0.1716 | 0.1944
Epoch 66/300, seasonal_3 Loss: 0.1718 | 0.1969
Epoch 67/300, seasonal_3 Loss: 0.1714 | 0.1910
Epoch 68/300, seasonal_3 Loss: 0.1734 | 0.1919
Epoch 69/300, seasonal_3 Loss: 0.1673 | 0.1864
Epoch 70/300, seasonal_3 Loss: 0.1651 | 0.1892
Epoch 71/300, seasonal_3 Loss: 0.1663 | 0.1859
Epoch 72/300, seasonal_3 Loss: 0.1684 | 0.1857
Epoch 73/300, seasonal_3 Loss: 0.1681 | 0.1859
Epoch 74/300, seasonal_3 Loss: 0.1646 | 0.1824
Epoch 75/300, seasonal_3 Loss: 0.1658 | 0.1861
Epoch 76/300, seasonal_3 Loss: 0.1727 | 0.1795
Epoch 77/300, seasonal_3 Loss: 0.1662 | 0.1795
Epoch 78/300, seasonal_3 Loss: 0.1654 | 0.1807
Epoch 79/300, seasonal_3 Loss: 0.1694 | 0.1805
Epoch 80/300, seasonal_3 Loss: 0.1722 | 0.1766
Epoch 81/300, seasonal_3 Loss: 0.1749 | 0.1768
Epoch 82/300, seasonal_3 Loss: 0.1666 | 0.1773
Epoch 83/300, seasonal_3 Loss: 0.1675 | 0.1783
Epoch 84/300, seasonal_3 Loss: 0.1791 | 0.1767
Epoch 85/300, seasonal_3 Loss: 0.1725 | 0.1744
Epoch 86/300, seasonal_3 Loss: 0.1685 | 0.1784
Epoch 87/300, seasonal_3 Loss: 0.1568 | 0.1709
Epoch 88/300, seasonal_3 Loss: 0.1540 | 0.1777
Epoch 89/300, seasonal_3 Loss: 0.1540 | 0.1680
Epoch 90/300, seasonal_3 Loss: 0.1524 | 0.1735
Epoch 91/300, seasonal_3 Loss: 0.1532 | 0.1686
Epoch 92/300, seasonal_3 Loss: 0.1524 | 0.1719
Epoch 93/300, seasonal_3 Loss: 0.1513 | 0.1667
Epoch 94/300, seasonal_3 Loss: 0.1510 | 0.1703
Epoch 95/300, seasonal_3 Loss: 0.1494 | 0.1660
Epoch 96/300, seasonal_3 Loss: 0.1483 | 0.1687
Epoch 97/300, seasonal_3 Loss: 0.1472 | 0.1640
Epoch 98/300, seasonal_3 Loss: 0.1470 | 0.1658
Epoch 99/300, seasonal_3 Loss: 0.1478 | 0.1629
Epoch 100/300, seasonal_3 Loss: 0.1456 | 0.1655
Epoch 101/300, seasonal_3 Loss: 0.1452 | 0.1626
Epoch 102/300, seasonal_3 Loss: 0.1446 | 0.1639
Epoch 103/300, seasonal_3 Loss: 0.1436 | 0.1589
Epoch 104/300, seasonal_3 Loss: 0.1429 | 0.1632
Epoch 105/300, seasonal_3 Loss: 0.1436 | 0.1584
Epoch 106/300, seasonal_3 Loss: 0.1428 | 0.1623
Epoch 107/300, seasonal_3 Loss: 0.1438 | 0.1595
Epoch 108/300, seasonal_3 Loss: 0.1428 | 0.1609
Epoch 109/300, seasonal_3 Loss: 0.1415 | 0.1587
Epoch 110/300, seasonal_3 Loss: 0.1421 | 0.1603
Epoch 111/300, seasonal_3 Loss: 0.1419 | 0.1555
Epoch 112/300, seasonal_3 Loss: 0.1424 | 0.1626
Epoch 113/300, seasonal_3 Loss: 0.1451 | 0.1560
Epoch 114/300, seasonal_3 Loss: 0.1442 | 0.1570
Epoch 115/300, seasonal_3 Loss: 0.1423 | 0.1584
Epoch 116/300, seasonal_3 Loss: 0.1402 | 0.1565
Epoch 117/300, seasonal_3 Loss: 0.1414 | 0.1538
Epoch 118/300, seasonal_3 Loss: 0.1442 | 0.1561
Epoch 119/300, seasonal_3 Loss: 0.1428 | 0.1534
Epoch 120/300, seasonal_3 Loss: 0.1447 | 0.1577
Epoch 121/300, seasonal_3 Loss: 0.1528 | 0.1579
Epoch 122/300, seasonal_3 Loss: 0.1499 | 0.1550
Epoch 123/300, seasonal_3 Loss: 0.1458 | 0.1583
Epoch 124/300, seasonal_3 Loss: 0.1377 | 0.1502
Epoch 125/300, seasonal_3 Loss: 0.1356 | 0.1550
Epoch 126/300, seasonal_3 Loss: 0.1372 | 0.1522
Epoch 127/300, seasonal_3 Loss: 0.1359 | 0.1516
Epoch 128/300, seasonal_3 Loss: 0.1356 | 0.1534
Epoch 129/300, seasonal_3 Loss: 0.1362 | 0.1530
Epoch 130/300, seasonal_3 Loss: 0.1371 | 0.1531
Epoch 131/300, seasonal_3 Loss: 0.1384 | 0.1545
Epoch 132/300, seasonal_3 Loss: 0.1351 | 0.1505
Epoch 133/300, seasonal_3 Loss: 0.1341 | 0.1536
Epoch 134/300, seasonal_3 Loss: 0.1320 | 0.1514
Epoch 135/300, seasonal_3 Loss: 0.1311 | 0.1523
Epoch 136/300, seasonal_3 Loss: 0.1309 | 0.1518
Epoch 137/300, seasonal_3 Loss: 0.1316 | 0.1521
Epoch 138/300, seasonal_3 Loss: 0.1310 | 0.1504
Epoch 139/300, seasonal_3 Loss: 0.1308 | 0.1529
Epoch 140/300, seasonal_3 Loss: 0.1311 | 0.1520
Epoch 141/300, seasonal_3 Loss: 0.1304 | 0.1516
Epoch 142/300, seasonal_3 Loss: 0.1295 | 0.1485
Epoch 143/300, seasonal_3 Loss: 0.1293 | 0.1512
Epoch 144/300, seasonal_3 Loss: 0.1301 | 0.1482
Epoch 145/300, seasonal_3 Loss: 0.1301 | 0.1491
Epoch 146/300, seasonal_3 Loss: 0.1286 | 0.1490
Epoch 147/300, seasonal_3 Loss: 0.1273 | 0.1481
Epoch 148/300, seasonal_3 Loss: 0.1280 | 0.1482
Epoch 149/300, seasonal_3 Loss: 0.1271 | 0.1492
Epoch 150/300, seasonal_3 Loss: 0.1277 | 0.1460
Epoch 151/300, seasonal_3 Loss: 0.1274 | 0.1488
Epoch 152/300, seasonal_3 Loss: 0.1278 | 0.1481
Epoch 153/300, seasonal_3 Loss: 0.1296 | 0.1499
Epoch 154/300, seasonal_3 Loss: 0.1293 | 0.1470
Epoch 155/300, seasonal_3 Loss: 0.1278 | 0.1471
Epoch 156/300, seasonal_3 Loss: 0.1267 | 0.1460
Epoch 157/300, seasonal_3 Loss: 0.1250 | 0.1463
Epoch 158/300, seasonal_3 Loss: 0.1246 | 0.1471
Epoch 159/300, seasonal_3 Loss: 0.1241 | 0.1463
Epoch 160/300, seasonal_3 Loss: 0.1236 | 0.1460
Epoch 161/300, seasonal_3 Loss: 0.1240 | 0.1460
Epoch 162/300, seasonal_3 Loss: 0.1244 | 0.1469
Epoch 163/300, seasonal_3 Loss: 0.1253 | 0.1449
Epoch 164/300, seasonal_3 Loss: 0.1239 | 0.1476
Epoch 165/300, seasonal_3 Loss: 0.1233 | 0.1446
Epoch 166/300, seasonal_3 Loss: 0.1232 | 0.1467
Epoch 167/300, seasonal_3 Loss: 0.1234 | 0.1461
Epoch 168/300, seasonal_3 Loss: 0.1231 | 0.1461
Epoch 169/300, seasonal_3 Loss: 0.1233 | 0.1468
Epoch 170/300, seasonal_3 Loss: 0.1229 | 0.1454
Epoch 171/300, seasonal_3 Loss: 0.1226 | 0.1448
Epoch 172/300, seasonal_3 Loss: 0.1218 | 0.1462
Epoch 173/300, seasonal_3 Loss: 0.1208 | 0.1457
Epoch 174/300, seasonal_3 Loss: 0.1211 | 0.1474
Epoch 175/300, seasonal_3 Loss: 0.1215 | 0.1456
Epoch 176/300, seasonal_3 Loss: 0.1214 | 0.1448
Epoch 177/300, seasonal_3 Loss: 0.1211 | 0.1454
Epoch 178/300, seasonal_3 Loss: 0.1200 | 0.1446
Epoch 179/300, seasonal_3 Loss: 0.1206 | 0.1470
Epoch 180/300, seasonal_3 Loss: 0.1206 | 0.1445
Epoch 181/300, seasonal_3 Loss: 0.1199 | 0.1458
Epoch 182/300, seasonal_3 Loss: 0.1213 | 0.1452
Epoch 183/300, seasonal_3 Loss: 0.1211 | 0.1442
Epoch 184/300, seasonal_3 Loss: 0.1205 | 0.1455
Epoch 185/300, seasonal_3 Loss: 0.1193 | 0.1445
Epoch 186/300, seasonal_3 Loss: 0.1198 | 0.1441
Epoch 187/300, seasonal_3 Loss: 0.1195 | 0.1438
Epoch 188/300, seasonal_3 Loss: 0.1202 | 0.1446
Epoch 189/300, seasonal_3 Loss: 0.1200 | 0.1427
Epoch 190/300, seasonal_3 Loss: 0.1194 | 0.1464
Epoch 191/300, seasonal_3 Loss: 0.1191 | 0.1439
Epoch 192/300, seasonal_3 Loss: 0.1194 | 0.1465
Epoch 193/300, seasonal_3 Loss: 0.1185 | 0.1447
Epoch 194/300, seasonal_3 Loss: 0.1177 | 0.1463
Epoch 195/300, seasonal_3 Loss: 0.1168 | 0.1452
Epoch 196/300, seasonal_3 Loss: 0.1175 | 0.1448
Epoch 197/300, seasonal_3 Loss: 0.1175 | 0.1442
Epoch 198/300, seasonal_3 Loss: 0.1172 | 0.1457
Epoch 199/300, seasonal_3 Loss: 0.1168 | 0.1429
Epoch 200/300, seasonal_3 Loss: 0.1162 | 0.1448
Epoch 201/300, seasonal_3 Loss: 0.1160 | 0.1445
Epoch 202/300, seasonal_3 Loss: 0.1157 | 0.1448
Epoch 203/300, seasonal_3 Loss: 0.1153 | 0.1453
Epoch 204/300, seasonal_3 Loss: 0.1168 | 0.1472
Epoch 205/300, seasonal_3 Loss: 0.1159 | 0.1457
Epoch 206/300, seasonal_3 Loss: 0.1153 | 0.1454
Epoch 207/300, seasonal_3 Loss: 0.1156 | 0.1458
Epoch 208/300, seasonal_3 Loss: 0.1143 | 0.1466
Epoch 209/300, seasonal_3 Loss: 0.1151 | 0.1447
Epoch 210/300, seasonal_3 Loss: 0.1145 | 0.1455
Epoch 211/300, seasonal_3 Loss: 0.1149 | 0.1447
Epoch 212/300, seasonal_3 Loss: 0.1151 | 0.1458
Epoch 213/300, seasonal_3 Loss: 0.1145 | 0.1434
Epoch 214/300, seasonal_3 Loss: 0.1151 | 0.1438
Epoch 215/300, seasonal_3 Loss: 0.1137 | 0.1439
Epoch 216/300, seasonal_3 Loss: 0.1140 | 0.1447
Epoch 217/300, seasonal_3 Loss: 0.1141 | 0.1461
Epoch 218/300, seasonal_3 Loss: 0.1134 | 0.1462
Epoch 219/300, seasonal_3 Loss: 0.1141 | 0.1438
Epoch 220/300, seasonal_3 Loss: 0.1128 | 0.1459
Epoch 221/300, seasonal_3 Loss: 0.1131 | 0.1450
Epoch 222/300, seasonal_3 Loss: 0.1138 | 0.1455
Epoch 223/300, seasonal_3 Loss: 0.1135 | 0.1467
Epoch 224/300, seasonal_3 Loss: 0.1122 | 0.1444
Epoch 225/300, seasonal_3 Loss: 0.1131 | 0.1453
Epoch 226/300, seasonal_3 Loss: 0.1127 | 0.1449
Epoch 227/300, seasonal_3 Loss: 0.1127 | 0.1441
Epoch 228/300, seasonal_3 Loss: 0.1118 | 0.1445
Epoch 229/300, seasonal_3 Loss: 0.1121 | 0.1442
Epoch 230/300, seasonal_3 Loss: 0.1124 | 0.1423
Epoch 231/300, seasonal_3 Loss: 0.1117 | 0.1426
Epoch 232/300, seasonal_3 Loss: 0.1126 | 0.1446
Epoch 233/300, seasonal_3 Loss: 0.1120 | 0.1440
Epoch 234/300, seasonal_3 Loss: 0.1116 | 0.1444
Epoch 235/300, seasonal_3 Loss: 0.1117 | 0.1435
Epoch 236/300, seasonal_3 Loss: 0.1116 | 0.1456
Epoch 237/300, seasonal_3 Loss: 0.1121 | 0.1448
Epoch 238/300, seasonal_3 Loss: 0.1111 | 0.1448
Epoch 239/300, seasonal_3 Loss: 0.1107 | 0.1448
Epoch 240/300, seasonal_3 Loss: 0.1113 | 0.1454
Epoch 241/300, seasonal_3 Loss: 0.1101 | 0.1437
Epoch 242/300, seasonal_3 Loss: 0.1104 | 0.1450
Epoch 243/300, seasonal_3 Loss: 0.1103 | 0.1433
Epoch 244/300, seasonal_3 Loss: 0.1105 | 0.1441
Epoch 245/300, seasonal_3 Loss: 0.1110 | 0.1440
Epoch 246/300, seasonal_3 Loss: 0.1107 | 0.1437
Epoch 247/300, seasonal_3 Loss: 0.1103 | 0.1454
Epoch 248/300, seasonal_3 Loss: 0.1100 | 0.1449
Epoch 249/300, seasonal_3 Loss: 0.1101 | 0.1451
Epoch 250/300, seasonal_3 Loss: 0.1097 | 0.1441
Epoch 251/300, seasonal_3 Loss: 0.1098 | 0.1462
Epoch 252/300, seasonal_3 Loss: 0.1091 | 0.1441
Epoch 253/300, seasonal_3 Loss: 0.1094 | 0.1456
Epoch 254/300, seasonal_3 Loss: 0.1097 | 0.1445
Epoch 255/300, seasonal_3 Loss: 0.1099 | 0.1448
Epoch 256/300, seasonal_3 Loss: 0.1090 | 0.1436
Epoch 257/300, seasonal_3 Loss: 0.1085 | 0.1445
Epoch 258/300, seasonal_3 Loss: 0.1081 | 0.1443
Epoch 259/300, seasonal_3 Loss: 0.1088 | 0.1438
Epoch 260/300, seasonal_3 Loss: 0.1086 | 0.1455
Epoch 261/300, seasonal_3 Loss: 0.1088 | 0.1417
Epoch 262/300, seasonal_3 Loss: 0.1082 | 0.1454
Epoch 263/300, seasonal_3 Loss: 0.1074 | 0.1447
Epoch 264/300, seasonal_3 Loss: 0.1091 | 0.1437
Epoch 265/300, seasonal_3 Loss: 0.1085 | 0.1447
Epoch 266/300, seasonal_3 Loss: 0.1077 | 0.1455
Epoch 267/300, seasonal_3 Loss: 0.1077 | 0.1439
Epoch 268/300, seasonal_3 Loss: 0.1082 | 0.1448
Epoch 269/300, seasonal_3 Loss: 0.1076 | 0.1448
Epoch 270/300, seasonal_3 Loss: 0.1082 | 0.1433
Epoch 271/300, seasonal_3 Loss: 0.1071 | 0.1443
Epoch 272/300, seasonal_3 Loss: 0.1070 | 0.1450
Epoch 273/300, seasonal_3 Loss: 0.1072 | 0.1447
Epoch 274/300, seasonal_3 Loss: 0.1073 | 0.1446
Epoch 275/300, seasonal_3 Loss: 0.1075 | 0.1448
Epoch 276/300, seasonal_3 Loss: 0.1071 | 0.1450
Epoch 277/300, seasonal_3 Loss: 0.1066 | 0.1452
Epoch 278/300, seasonal_3 Loss: 0.1066 | 0.1444
Epoch 279/300, seasonal_3 Loss: 0.1062 | 0.1446
Epoch 280/300, seasonal_3 Loss: 0.1070 | 0.1454
Epoch 281/300, seasonal_3 Loss: 0.1070 | 0.1456
Epoch 282/300, seasonal_3 Loss: 0.1073 | 0.1433
Epoch 283/300, seasonal_3 Loss: 0.1062 | 0.1425
Epoch 284/300, seasonal_3 Loss: 0.1066 | 0.1443
Epoch 285/300, seasonal_3 Loss: 0.1062 | 0.1452
Epoch 286/300, seasonal_3 Loss: 0.1066 | 0.1441
Epoch 287/300, seasonal_3 Loss: 0.1063 | 0.1448
Epoch 288/300, seasonal_3 Loss: 0.1063 | 0.1456
Epoch 289/300, seasonal_3 Loss: 0.1060 | 0.1448
Epoch 290/300, seasonal_3 Loss: 0.1065 | 0.1439
Epoch 291/300, seasonal_3 Loss: 0.1059 | 0.1448
Epoch 292/300, seasonal_3 Loss: 0.1058 | 0.1437
Epoch 293/300, seasonal_3 Loss: 0.1058 | 0.1452
Epoch 294/300, seasonal_3 Loss: 0.1052 | 0.1453
Epoch 295/300, seasonal_3 Loss: 0.1060 | 0.1443
Epoch 296/300, seasonal_3 Loss: 0.1057 | 0.1452
Epoch 297/300, seasonal_3 Loss: 0.1052 | 0.1442
Epoch 298/300, seasonal_3 Loss: 0.1056 | 0.1456
Epoch 299/300, seasonal_3 Loss: 0.1049 | 0.1452
Epoch 300/300, seasonal_3 Loss: 0.1051 | 0.1446
Training resid component with params: {'observation_period_num': 190, 'train_rates': 0.9896776782373335, 'learning_rate': 0.00036197136259234313, 'batch_size': 211, 'step_size': 14, 'gamma': 0.7834862791342413}
Epoch 1/300, resid Loss: 1.1631 | 2.1145
Epoch 2/300, resid Loss: 0.7872 | 1.2118
Epoch 3/300, resid Loss: 0.6328 | 0.9921
Epoch 4/300, resid Loss: 0.5369 | 0.9294
Epoch 5/300, resid Loss: 0.7005 | 1.0088
Epoch 6/300, resid Loss: 0.5506 | 0.8332
Epoch 7/300, resid Loss: 0.5245 | 0.8385
Epoch 8/300, resid Loss: 0.4922 | 0.7611
Epoch 9/300, resid Loss: 0.4440 | 0.8484
Epoch 10/300, resid Loss: 0.4338 | 0.7907
Epoch 11/300, resid Loss: 0.4163 | 0.6944
Epoch 12/300, resid Loss: 0.4084 | 0.7104
Epoch 13/300, resid Loss: 0.3525 | 0.6149
Epoch 14/300, resid Loss: 0.3386 | 0.6290
Epoch 15/300, resid Loss: 0.3219 | 0.5862
Epoch 16/300, resid Loss: 0.3031 | 0.6027
Epoch 17/300, resid Loss: 0.2882 | 0.5590
Epoch 18/300, resid Loss: 0.2798 | 0.5497
Epoch 19/300, resid Loss: 0.2791 | 0.5251
Epoch 20/300, resid Loss: 0.2847 | 0.5052
Epoch 21/300, resid Loss: 0.3128 | 0.5033
Epoch 22/300, resid Loss: 0.2767 | 0.4785
Epoch 23/300, resid Loss: 0.2577 | 0.4752
Epoch 24/300, resid Loss: 0.2461 | 0.4539
Epoch 25/300, resid Loss: 0.2423 | 0.4434
Epoch 26/300, resid Loss: 0.2398 | 0.4275
Epoch 27/300, resid Loss: 0.2389 | 0.4240
Epoch 28/300, resid Loss: 0.2404 | 0.4047
Epoch 29/300, resid Loss: 0.2420 | 0.4125
Epoch 30/300, resid Loss: 0.2347 | 0.3852
Epoch 31/300, resid Loss: 0.2313 | 0.3930
Epoch 32/300, resid Loss: 0.2237 | 0.3736
Epoch 33/300, resid Loss: 0.2199 | 0.3758
Epoch 34/300, resid Loss: 0.2149 | 0.3623
Epoch 35/300, resid Loss: 0.2131 | 0.3614
Epoch 36/300, resid Loss: 0.2084 | 0.3541
Epoch 37/300, resid Loss: 0.2067 | 0.3517
Epoch 38/300, resid Loss: 0.2046 | 0.3456
Epoch 39/300, resid Loss: 0.2018 | 0.3395
Epoch 40/300, resid Loss: 0.2012 | 0.3360
Epoch 41/300, resid Loss: 0.1994 | 0.3302
Epoch 42/300, resid Loss: 0.1978 | 0.3279
Epoch 43/300, resid Loss: 0.1962 | 0.3229
Epoch 44/300, resid Loss: 0.1951 | 0.3198
Epoch 45/300, resid Loss: 0.1940 | 0.3178
Epoch 46/300, resid Loss: 0.1923 | 0.3137
Epoch 47/300, resid Loss: 0.1917 | 0.3102
Epoch 48/300, resid Loss: 0.1903 | 0.3071
Epoch 49/300, resid Loss: 0.1888 | 0.3024
Epoch 50/300, resid Loss: 0.1877 | 0.3007
Epoch 51/300, resid Loss: 0.1865 | 0.2988
Epoch 52/300, resid Loss: 0.1855 | 0.2961
Epoch 53/300, resid Loss: 0.1842 | 0.2947
Epoch 54/300, resid Loss: 0.1838 | 0.2914
Epoch 55/300, resid Loss: 0.1834 | 0.2891
Epoch 56/300, resid Loss: 0.1825 | 0.2875
Epoch 57/300, resid Loss: 0.1808 | 0.2852
Epoch 58/300, resid Loss: 0.1809 | 0.2833
Epoch 59/300, resid Loss: 0.1801 | 0.2822
Epoch 60/300, resid Loss: 0.1802 | 0.2789
Epoch 61/300, resid Loss: 0.1784 | 0.2783
Epoch 62/300, resid Loss: 0.1782 | 0.2768
Epoch 63/300, resid Loss: 0.1783 | 0.2742
Epoch 64/300, resid Loss: 0.1766 | 0.2742
Epoch 65/300, resid Loss: 0.1772 | 0.2721
Epoch 66/300, resid Loss: 0.1767 | 0.2702
Epoch 67/300, resid Loss: 0.1763 | 0.2698
Epoch 68/300, resid Loss: 0.1749 | 0.2691
Epoch 69/300, resid Loss: 0.1734 | 0.2674
Epoch 70/300, resid Loss: 0.1744 | 0.2657
Epoch 71/300, resid Loss: 0.1739 | 0.2649
Epoch 72/300, resid Loss: 0.1729 | 0.2645
Epoch 73/300, resid Loss: 0.1724 | 0.2633
Epoch 74/300, resid Loss: 0.1720 | 0.2617
Epoch 75/300, resid Loss: 0.1725 | 0.2614
Epoch 76/300, resid Loss: 0.1718 | 0.2594
Epoch 77/300, resid Loss: 0.1723 | 0.2585
Epoch 78/300, resid Loss: 0.1713 | 0.2576
Epoch 79/300, resid Loss: 0.1711 | 0.2571
Epoch 80/300, resid Loss: 0.1705 | 0.2567
Epoch 81/300, resid Loss: 0.1701 | 0.2555
Epoch 82/300, resid Loss: 0.1699 | 0.2546
Epoch 83/300, resid Loss: 0.1691 | 0.2540
Epoch 84/300, resid Loss: 0.1693 | 0.2532
Epoch 85/300, resid Loss: 0.1694 | 0.2524
Epoch 86/300, resid Loss: 0.1677 | 0.2521
Epoch 87/300, resid Loss: 0.1680 | 0.2510
Epoch 88/300, resid Loss: 0.1677 | 0.2505
Epoch 89/300, resid Loss: 0.1684 | 0.2502
Epoch 90/300, resid Loss: 0.1680 | 0.2493
Epoch 91/300, resid Loss: 0.1681 | 0.2483
Epoch 92/300, resid Loss: 0.1674 | 0.2477
Epoch 93/300, resid Loss: 0.1675 | 0.2477
Epoch 94/300, resid Loss: 0.1671 | 0.2472
Epoch 95/300, resid Loss: 0.1669 | 0.2470
Epoch 96/300, resid Loss: 0.1664 | 0.2469
Epoch 97/300, resid Loss: 0.1663 | 0.2462
Epoch 98/300, resid Loss: 0.1658 | 0.2455
Epoch 99/300, resid Loss: 0.1658 | 0.2452
Epoch 100/300, resid Loss: 0.1657 | 0.2450
Epoch 101/300, resid Loss: 0.1644 | 0.2444
Epoch 102/300, resid Loss: 0.1656 | 0.2436
Epoch 103/300, resid Loss: 0.1650 | 0.2432
Epoch 104/300, resid Loss: 0.1657 | 0.2428
Epoch 105/300, resid Loss: 0.1656 | 0.2423
Epoch 106/300, resid Loss: 0.1650 | 0.2420
Epoch 107/300, resid Loss: 0.1650 | 0.2417
Epoch 108/300, resid Loss: 0.1647 | 0.2418
Epoch 109/300, resid Loss: 0.1643 | 0.2415
Epoch 110/300, resid Loss: 0.1643 | 0.2412
Epoch 111/300, resid Loss: 0.1641 | 0.2408
Epoch 112/300, resid Loss: 0.1635 | 0.2405
Epoch 113/300, resid Loss: 0.1635 | 0.2401
Epoch 114/300, resid Loss: 0.1640 | 0.2399
Epoch 115/300, resid Loss: 0.1635 | 0.2397
Epoch 116/300, resid Loss: 0.1635 | 0.2394
Epoch 117/300, resid Loss: 0.1635 | 0.2393
Epoch 118/300, resid Loss: 0.1636 | 0.2392
Epoch 119/300, resid Loss: 0.1633 | 0.2389
Epoch 120/300, resid Loss: 0.1635 | 0.2388
Epoch 121/300, resid Loss: 0.1628 | 0.2386
Epoch 122/300, resid Loss: 0.1638 | 0.2384
Epoch 123/300, resid Loss: 0.1623 | 0.2381
Epoch 124/300, resid Loss: 0.1627 | 0.2378
Epoch 125/300, resid Loss: 0.1626 | 0.2377
Epoch 126/300, resid Loss: 0.1625 | 0.2374
Epoch 127/300, resid Loss: 0.1622 | 0.2372
Epoch 128/300, resid Loss: 0.1628 | 0.2370
Epoch 129/300, resid Loss: 0.1619 | 0.2368
Epoch 130/300, resid Loss: 0.1625 | 0.2369
Epoch 131/300, resid Loss: 0.1629 | 0.2369
Epoch 132/300, resid Loss: 0.1625 | 0.2367
Epoch 133/300, resid Loss: 0.1629 | 0.2365
Epoch 134/300, resid Loss: 0.1627 | 0.2364
Epoch 135/300, resid Loss: 0.1623 | 0.2362
Epoch 136/300, resid Loss: 0.1622 | 0.2360
Epoch 137/300, resid Loss: 0.1621 | 0.2358
Epoch 138/300, resid Loss: 0.1628 | 0.2358
Epoch 139/300, resid Loss: 0.1614 | 0.2358
Epoch 140/300, resid Loss: 0.1610 | 0.2355
Epoch 141/300, resid Loss: 0.1612 | 0.2354
Epoch 142/300, resid Loss: 0.1613 | 0.2354
Epoch 143/300, resid Loss: 0.1616 | 0.2354
Epoch 144/300, resid Loss: 0.1613 | 0.2353
Epoch 145/300, resid Loss: 0.1614 | 0.2352
Epoch 146/300, resid Loss: 0.1610 | 0.2350
Epoch 147/300, resid Loss: 0.1616 | 0.2349
Epoch 148/300, resid Loss: 0.1613 | 0.2348
Epoch 149/300, resid Loss: 0.1612 | 0.2347
Epoch 150/300, resid Loss: 0.1623 | 0.2345
Epoch 151/300, resid Loss: 0.1623 | 0.2344
Epoch 152/300, resid Loss: 0.1615 | 0.2344
Epoch 153/300, resid Loss: 0.1606 | 0.2343
Epoch 154/300, resid Loss: 0.1618 | 0.2343
Epoch 155/300, resid Loss: 0.1619 | 0.2341
Epoch 156/300, resid Loss: 0.1620 | 0.2341
Epoch 157/300, resid Loss: 0.1617 | 0.2341
Epoch 158/300, resid Loss: 0.1614 | 0.2340
Epoch 159/300, resid Loss: 0.1612 | 0.2339
Epoch 160/300, resid Loss: 0.1617 | 0.2338
Epoch 161/300, resid Loss: 0.1605 | 0.2337
Epoch 162/300, resid Loss: 0.1611 | 0.2337
Epoch 163/300, resid Loss: 0.1606 | 0.2337
Epoch 164/300, resid Loss: 0.1616 | 0.2336
Epoch 165/300, resid Loss: 0.1609 | 0.2335
Epoch 166/300, resid Loss: 0.1613 | 0.2335
Epoch 167/300, resid Loss: 0.1611 | 0.2335
Epoch 168/300, resid Loss: 0.1604 | 0.2334
Epoch 169/300, resid Loss: 0.1604 | 0.2333
Epoch 170/300, resid Loss: 0.1613 | 0.2333
Epoch 171/300, resid Loss: 0.1610 | 0.2332
Epoch 172/300, resid Loss: 0.1614 | 0.2332
Epoch 173/300, resid Loss: 0.1615 | 0.2332
Epoch 174/300, resid Loss: 0.1617 | 0.2331
Epoch 175/300, resid Loss: 0.1603 | 0.2331
Epoch 176/300, resid Loss: 0.1611 | 0.2331
Epoch 177/300, resid Loss: 0.1610 | 0.2330
Epoch 178/300, resid Loss: 0.1607 | 0.2330
Epoch 179/300, resid Loss: 0.1609 | 0.2329
Epoch 180/300, resid Loss: 0.1614 | 0.2329
Epoch 181/300, resid Loss: 0.1613 | 0.2328
Epoch 182/300, resid Loss: 0.1604 | 0.2327
Epoch 183/300, resid Loss: 0.1610 | 0.2327
Epoch 184/300, resid Loss: 0.1607 | 0.2327
Epoch 185/300, resid Loss: 0.1605 | 0.2326
Epoch 186/300, resid Loss: 0.1606 | 0.2326
Epoch 187/300, resid Loss: 0.1607 | 0.2326
Epoch 188/300, resid Loss: 0.1607 | 0.2325
Epoch 189/300, resid Loss: 0.1599 | 0.2325
Epoch 190/300, resid Loss: 0.1605 | 0.2325
Epoch 191/300, resid Loss: 0.1605 | 0.2325
Epoch 192/300, resid Loss: 0.1600 | 0.2325
Epoch 193/300, resid Loss: 0.1611 | 0.2324
Epoch 194/300, resid Loss: 0.1609 | 0.2324
Epoch 195/300, resid Loss: 0.1607 | 0.2324
Epoch 196/300, resid Loss: 0.1599 | 0.2323
Epoch 197/300, resid Loss: 0.1601 | 0.2323
Epoch 198/300, resid Loss: 0.1606 | 0.2323
Epoch 199/300, resid Loss: 0.1611 | 0.2323
Epoch 200/300, resid Loss: 0.1601 | 0.2323
Epoch 201/300, resid Loss: 0.1601 | 0.2323
Epoch 202/300, resid Loss: 0.1599 | 0.2322
Epoch 203/300, resid Loss: 0.1597 | 0.2322
Epoch 204/300, resid Loss: 0.1605 | 0.2322
Epoch 205/300, resid Loss: 0.1607 | 0.2322
Epoch 206/300, resid Loss: 0.1601 | 0.2322
Epoch 207/300, resid Loss: 0.1607 | 0.2322
Epoch 208/300, resid Loss: 0.1598 | 0.2322
Epoch 209/300, resid Loss: 0.1603 | 0.2322
Epoch 210/300, resid Loss: 0.1607 | 0.2322
Epoch 211/300, resid Loss: 0.1610 | 0.2322
Epoch 212/300, resid Loss: 0.1605 | 0.2322
Epoch 213/300, resid Loss: 0.1597 | 0.2322
Epoch 214/300, resid Loss: 0.1615 | 0.2321
Epoch 215/300, resid Loss: 0.1610 | 0.2321
Epoch 216/300, resid Loss: 0.1605 | 0.2321
Epoch 217/300, resid Loss: 0.1607 | 0.2321
Epoch 218/300, resid Loss: 0.1607 | 0.2321
Epoch 219/300, resid Loss: 0.1603 | 0.2321
Epoch 220/300, resid Loss: 0.1608 | 0.2321
Epoch 221/300, resid Loss: 0.1597 | 0.2321
Epoch 222/300, resid Loss: 0.1610 | 0.2321
Epoch 223/300, resid Loss: 0.1608 | 0.2321
Epoch 224/300, resid Loss: 0.1600 | 0.2321
Epoch 225/300, resid Loss: 0.1606 | 0.2321
Epoch 226/300, resid Loss: 0.1601 | 0.2321
Epoch 227/300, resid Loss: 0.1605 | 0.2321
Epoch 228/300, resid Loss: 0.1612 | 0.2321
Epoch 229/300, resid Loss: 0.1609 | 0.2321
Epoch 230/300, resid Loss: 0.1607 | 0.2321
Epoch 231/300, resid Loss: 0.1605 | 0.2320
Epoch 232/300, resid Loss: 0.1597 | 0.2320
Epoch 233/300, resid Loss: 0.1608 | 0.2320
Epoch 234/300, resid Loss: 0.1603 | 0.2320
Epoch 235/300, resid Loss: 0.1599 | 0.2320
Epoch 236/300, resid Loss: 0.1606 | 0.2320
Epoch 237/300, resid Loss: 0.1595 | 0.2320
Epoch 238/300, resid Loss: 0.1605 | 0.2320
Epoch 239/300, resid Loss: 0.1607 | 0.2320
Epoch 240/300, resid Loss: 0.1609 | 0.2320
Epoch 241/300, resid Loss: 0.1598 | 0.2320
Epoch 242/300, resid Loss: 0.1609 | 0.2320
Epoch 243/300, resid Loss: 0.1604 | 0.2320
Epoch 244/300, resid Loss: 0.1605 | 0.2320
Epoch 245/300, resid Loss: 0.1602 | 0.2320
Epoch 246/300, resid Loss: 0.1601 | 0.2320
Epoch 247/300, resid Loss: 0.1607 | 0.2320
Epoch 248/300, resid Loss: 0.1608 | 0.2320
Epoch 249/300, resid Loss: 0.1604 | 0.2320
Epoch 250/300, resid Loss: 0.1600 | 0.2320
Epoch 251/300, resid Loss: 0.1603 | 0.2320
Epoch 252/300, resid Loss: 0.1601 | 0.2320
Epoch 253/300, resid Loss: 0.1603 | 0.2320
Epoch 254/300, resid Loss: 0.1601 | 0.2320
Epoch 255/300, resid Loss: 0.1596 | 0.2320
Epoch 256/300, resid Loss: 0.1602 | 0.2319
Epoch 257/300, resid Loss: 0.1612 | 0.2319
Epoch 258/300, resid Loss: 0.1601 | 0.2319
Epoch 259/300, resid Loss: 0.1605 | 0.2319
Epoch 260/300, resid Loss: 0.1609 | 0.2319
Epoch 261/300, resid Loss: 0.1605 | 0.2319
Epoch 262/300, resid Loss: 0.1601 | 0.2319
Epoch 263/300, resid Loss: 0.1604 | 0.2319
Epoch 264/300, resid Loss: 0.1601 | 0.2319
Epoch 265/300, resid Loss: 0.1609 | 0.2319
Epoch 266/300, resid Loss: 0.1606 | 0.2319
Epoch 267/300, resid Loss: 0.1598 | 0.2319
Epoch 268/300, resid Loss: 0.1608 | 0.2319
Epoch 269/300, resid Loss: 0.1603 | 0.2319
Epoch 270/300, resid Loss: 0.1605 | 0.2319
Epoch 271/300, resid Loss: 0.1606 | 0.2319
Epoch 272/300, resid Loss: 0.1602 | 0.2319
Epoch 273/300, resid Loss: 0.1606 | 0.2319
Epoch 274/300, resid Loss: 0.1607 | 0.2319
Epoch 275/300, resid Loss: 0.1604 | 0.2319
Epoch 276/300, resid Loss: 0.1597 | 0.2319
Epoch 277/300, resid Loss: 0.1610 | 0.2319
Epoch 278/300, resid Loss: 0.1606 | 0.2319
Epoch 279/300, resid Loss: 0.1599 | 0.2319
Epoch 280/300, resid Loss: 0.1606 | 0.2319
Epoch 281/300, resid Loss: 0.1605 | 0.2319
Epoch 282/300, resid Loss: 0.1606 | 0.2319
Epoch 283/300, resid Loss: 0.1611 | 0.2319
Epoch 284/300, resid Loss: 0.1603 | 0.2319
Epoch 285/300, resid Loss: 0.1600 | 0.2319
Epoch 286/300, resid Loss: 0.1605 | 0.2319
Epoch 287/300, resid Loss: 0.1605 | 0.2319
Epoch 288/300, resid Loss: 0.1597 | 0.2319
Epoch 289/300, resid Loss: 0.1601 | 0.2319
Epoch 290/300, resid Loss: 0.1598 | 0.2319
Epoch 291/300, resid Loss: 0.1598 | 0.2319
Epoch 292/300, resid Loss: 0.1604 | 0.2319
Epoch 293/300, resid Loss: 0.1601 | 0.2319
Epoch 294/300, resid Loss: 0.1602 | 0.2319
Epoch 295/300, resid Loss: 0.1599 | 0.2319
Epoch 296/300, resid Loss: 0.1608 | 0.2319
Epoch 297/300, resid Loss: 0.1599 | 0.2319
Epoch 298/300, resid Loss: 0.1598 | 0.2319
Epoch 299/300, resid Loss: 0.1606 | 0.2319
Epoch 300/300, resid Loss: 0.1602 | 0.2319
Runtime (seconds): 3332.5018911361694
0.00020188813682337284
[147.49585]
[0.08365235]
[-2.1555758]
[8.056373]
[6.3815312]
[8.43735]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.678370002657175
RMSE: 1.91790771484375
MAE: 1.91790771484375
R-squared: nan
[168.29916]
