[32m[I 2025-02-06 17:28:44,986][0m A new study created in memory with name: no-name-2079916c-249e-4974-b9ee-c04f393e6240[0m
[32m[I 2025-02-06 17:29:12,732][0m Trial 0 finished with value: 0.16991106977329465 and parameters: {'observation_period_num': 37, 'train_rates': 0.6309579502810118, 'learning_rate': 0.00017517956581752952, 'batch_size': 184, 'step_size': 11, 'gamma': 0.7504404874724557}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:29:46,483][0m Trial 1 finished with value: 0.8154138326644897 and parameters: {'observation_period_num': 79, 'train_rates': 0.9879114849448891, 'learning_rate': 2.13841524947174e-06, 'batch_size': 193, 'step_size': 14, 'gamma': 0.9199566399629824}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:30:11,646][0m Trial 2 finished with value: 1.0140849735874395 and parameters: {'observation_period_num': 211, 'train_rates': 0.7010024971006209, 'learning_rate': 5.447154752003794e-06, 'batch_size': 201, 'step_size': 5, 'gamma': 0.759749723020079}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:30:39,660][0m Trial 3 finished with value: 0.9981846460025204 and parameters: {'observation_period_num': 144, 'train_rates': 0.67635542801784, 'learning_rate': 3.950671199640924e-06, 'batch_size': 177, 'step_size': 10, 'gamma': 0.8676723684068992}. Best is trial 0 with value: 0.16991106977329465.[0m
Early stopping at epoch 75
[32m[I 2025-02-06 17:30:58,614][0m Trial 4 finished with value: 0.7740346225994996 and parameters: {'observation_period_num': 171, 'train_rates': 0.7288440638344436, 'learning_rate': 3.5247223612979834e-05, 'batch_size': 226, 'step_size': 1, 'gamma': 0.8533285710713392}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:34:19,310][0m Trial 5 finished with value: 0.17890976306208947 and parameters: {'observation_period_num': 51, 'train_rates': 0.7275782804596981, 'learning_rate': 0.00013429374821493745, 'batch_size': 24, 'step_size': 6, 'gamma': 0.7628814193720005}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:34:55,785][0m Trial 6 finished with value: 0.39541064329396663 and parameters: {'observation_period_num': 104, 'train_rates': 0.8413927259579921, 'learning_rate': 3.4984116877852546e-05, 'batch_size': 154, 'step_size': 1, 'gamma': 0.9157094658754434}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:35:21,630][0m Trial 7 finished with value: 0.5716284078626103 and parameters: {'observation_period_num': 186, 'train_rates': 0.6734377240597583, 'learning_rate': 5.504537477199582e-06, 'batch_size': 209, 'step_size': 6, 'gamma': 0.9545807450766012}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:36:17,367][0m Trial 8 finished with value: 0.8475909066550872 and parameters: {'observation_period_num': 72, 'train_rates': 0.9534542612342203, 'learning_rate': 1.7582181660137234e-06, 'batch_size': 110, 'step_size': 8, 'gamma': 0.8591559509217617}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:37:05,526][0m Trial 9 finished with value: 0.26245608711242674 and parameters: {'observation_period_num': 218, 'train_rates': 0.9551842500288592, 'learning_rate': 0.00019328365124836008, 'batch_size': 121, 'step_size': 2, 'gamma': 0.8840262891991644}. Best is trial 0 with value: 0.16991106977329465.[0m
[32m[I 2025-02-06 17:38:29,072][0m Trial 10 finished with value: 0.07116511314548203 and parameters: {'observation_period_num': 15, 'train_rates': 0.8212999450806315, 'learning_rate': 0.0007205465478696568, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8042019761536797}. Best is trial 10 with value: 0.07116511314548203.[0m
[32m[I 2025-02-06 17:39:43,972][0m Trial 11 finished with value: 0.11724035510532685 and parameters: {'observation_period_num': 7, 'train_rates': 0.6075761723317409, 'learning_rate': 0.0009847290006266575, 'batch_size': 60, 'step_size': 13, 'gamma': 0.8077376956865668}. Best is trial 10 with value: 0.07116511314548203.[0m
[32m[I 2025-02-06 17:41:16,942][0m Trial 12 finished with value: 0.07452003468798953 and parameters: {'observation_period_num': 10, 'train_rates': 0.8188077913706389, 'learning_rate': 0.0009393507817122096, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8121583065432606}. Best is trial 10 with value: 0.07116511314548203.[0m
[32m[I 2025-02-06 17:42:29,902][0m Trial 13 finished with value: 0.04200322009049929 and parameters: {'observation_period_num': 14, 'train_rates': 0.8341769802210369, 'learning_rate': 0.0009681299235922148, 'batch_size': 76, 'step_size': 15, 'gamma': 0.8014984402475952}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:43:37,532][0m Trial 14 finished with value: 0.1705989131822865 and parameters: {'observation_period_num': 117, 'train_rates': 0.8695446117238309, 'learning_rate': 0.00034917871028662586, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8083688100184696}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:46:28,308][0m Trial 15 finished with value: 0.24627941686148738 and parameters: {'observation_period_num': 251, 'train_rates': 0.7771480084068066, 'learning_rate': 0.0004519687842692364, 'batch_size': 28, 'step_size': 15, 'gamma': 0.7909159992665789}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:47:31,563][0m Trial 16 finished with value: 0.050095098329262 and parameters: {'observation_period_num': 37, 'train_rates': 0.8799280361465363, 'learning_rate': 0.00010623730620387189, 'batch_size': 90, 'step_size': 9, 'gamma': 0.829283286094889}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:48:30,596][0m Trial 17 finished with value: 0.07140858842783382 and parameters: {'observation_period_num': 46, 'train_rates': 0.8891858389103644, 'learning_rate': 6.0598480480512505e-05, 'batch_size': 100, 'step_size': 9, 'gamma': 0.8349627908622229}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:49:12,423][0m Trial 18 finished with value: 0.17386214850167503 and parameters: {'observation_period_num': 86, 'train_rates': 0.9008445525191829, 'learning_rate': 8.280246137184265e-05, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8373707348815286}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:49:36,489][0m Trial 19 finished with value: 0.4242488911634759 and parameters: {'observation_period_num': 35, 'train_rates': 0.7864744434737146, 'learning_rate': 1.3223574135034432e-05, 'batch_size': 247, 'step_size': 8, 'gamma': 0.7801363564819366}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:50:47,109][0m Trial 20 finished with value: 0.17274541716750075 and parameters: {'observation_period_num': 141, 'train_rates': 0.9140743383760352, 'learning_rate': 0.00039578839303522985, 'batch_size': 81, 'step_size': 12, 'gamma': 0.9885298542512758}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:52:29,537][0m Trial 21 finished with value: 0.09184431401183951 and parameters: {'observation_period_num': 20, 'train_rates': 0.8476097752168821, 'learning_rate': 0.000571797266973909, 'batch_size': 54, 'step_size': 13, 'gamma': 0.8219874408775921}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:53:30,758][0m Trial 22 finished with value: 0.19116264635739827 and parameters: {'observation_period_num': 57, 'train_rates': 0.7630777464727593, 'learning_rate': 0.00023813057207943745, 'batch_size': 83, 'step_size': 15, 'gamma': 0.7914356502557691}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:55:43,133][0m Trial 23 finished with value: 0.06990129952013058 and parameters: {'observation_period_num': 23, 'train_rates': 0.8156850326032637, 'learning_rate': 1.4864029460068947e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8350788149345343}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:57:54,642][0m Trial 24 finished with value: 0.08511992845986341 and parameters: {'observation_period_num': 64, 'train_rates': 0.8108547525934879, 'learning_rate': 1.4306496593786674e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8820183155133512}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:58:39,792][0m Trial 25 finished with value: 0.13236781677873327 and parameters: {'observation_period_num': 32, 'train_rates': 0.8621638510881954, 'learning_rate': 2.2476763476876883e-05, 'batch_size': 131, 'step_size': 7, 'gamma': 0.8405089414089538}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 17:59:39,926][0m Trial 26 finished with value: 0.21622172025393466 and parameters: {'observation_period_num': 98, 'train_rates': 0.9155116824201184, 'learning_rate': 1.1954338994300176e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8966587335018268}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 18:02:08,117][0m Trial 27 finished with value: 0.17158803760625485 and parameters: {'observation_period_num': 28, 'train_rates': 0.7619666708453261, 'learning_rate': 7.378792294227907e-05, 'batch_size': 34, 'step_size': 12, 'gamma': 0.8288345600497317}. Best is trial 13 with value: 0.04200322009049929.[0m
[32m[I 2025-02-06 18:07:57,560][0m Trial 28 finished with value: 0.02815522721244229 and parameters: {'observation_period_num': 5, 'train_rates': 0.8798037120667941, 'learning_rate': 0.00010601552346714853, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7782783301631458}. Best is trial 28 with value: 0.02815522721244229.[0m
[32m[I 2025-02-06 18:08:37,277][0m Trial 29 finished with value: 0.1411494911598736 and parameters: {'observation_period_num': 47, 'train_rates': 0.9426716480579386, 'learning_rate': 0.0001649085090889531, 'batch_size': 163, 'step_size': 3, 'gamma': 0.7764027671813414}. Best is trial 28 with value: 0.02815522721244229.[0m
[32m[I 2025-02-06 18:13:00,584][0m Trial 30 finished with value: 0.026989411416638326 and parameters: {'observation_period_num': 6, 'train_rates': 0.86999894358015, 'learning_rate': 0.00010716167183258308, 'batch_size': 21, 'step_size': 11, 'gamma': 0.7703962472560427}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:17:24,965][0m Trial 31 finished with value: 0.02765283355888882 and parameters: {'observation_period_num': 9, 'train_rates': 0.879834356648608, 'learning_rate': 8.728240744266076e-05, 'batch_size': 21, 'step_size': 8, 'gamma': 0.7518209865386524}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:21:21,562][0m Trial 32 finished with value: 0.034891866778079546 and parameters: {'observation_period_num': 8, 'train_rates': 0.8497878378341351, 'learning_rate': 5.156787266175399e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.750875871498059}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:26:28,498][0m Trial 33 finished with value: 0.03469106043329696 and parameters: {'observation_period_num': 7, 'train_rates': 0.8636991594698264, 'learning_rate': 4.70959770885804e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.7518446153815537}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:32:06,596][0m Trial 34 finished with value: 0.04732415851223584 and parameters: {'observation_period_num': 30, 'train_rates': 0.9301420658963806, 'learning_rate': 4.810038783581587e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.7659015139718661}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:34:06,081][0m Trial 35 finished with value: 0.033520408010198954 and parameters: {'observation_period_num': 5, 'train_rates': 0.8948828495868653, 'learning_rate': 0.00011292900667629704, 'batch_size': 48, 'step_size': 11, 'gamma': 0.775335303438168}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:36:15,199][0m Trial 36 finished with value: 0.07326030899893562 and parameters: {'observation_period_num': 50, 'train_rates': 0.9821414158547332, 'learning_rate': 0.00011134982328334264, 'batch_size': 47, 'step_size': 11, 'gamma': 0.7774969672425278}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:38:44,350][0m Trial 37 finished with value: 0.12476607898668367 and parameters: {'observation_period_num': 74, 'train_rates': 0.8993476998540909, 'learning_rate': 0.00023063646257279435, 'batch_size': 38, 'step_size': 9, 'gamma': 0.7679434804903869}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:40:10,078][0m Trial 38 finished with value: 0.0915566172063081 and parameters: {'observation_period_num': 42, 'train_rates': 0.9254349715905038, 'learning_rate': 2.7053032754102385e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7915094388939303}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:42:12,826][0m Trial 39 finished with value: 0.7354551141078656 and parameters: {'observation_period_num': 86, 'train_rates': 0.9689463653979811, 'learning_rate': 1.141334581845516e-06, 'batch_size': 48, 'step_size': 13, 'gamma': 0.7589000580272066}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:45:17,105][0m Trial 40 finished with value: 0.06291063622889405 and parameters: {'observation_period_num': 60, 'train_rates': 0.8862285517595594, 'learning_rate': 0.00013290825134765398, 'batch_size': 30, 'step_size': 5, 'gamma': 0.7825211744499987}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:51:00,443][0m Trial 41 finished with value: 0.030850890085720492 and parameters: {'observation_period_num': 5, 'train_rates': 0.8634036216299515, 'learning_rate': 8.525853559247657e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7548606102326404}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:55:20,793][0m Trial 42 finished with value: 0.041066634185496984 and parameters: {'observation_period_num': 25, 'train_rates': 0.8694238406993997, 'learning_rate': 8.677085414760888e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.7680489994017862}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 18:57:18,559][0m Trial 43 finished with value: 0.040092709149914946 and parameters: {'observation_period_num': 5, 'train_rates': 0.8358825086086934, 'learning_rate': 0.00017207463005500617, 'batch_size': 47, 'step_size': 9, 'gamma': 0.7515524593893289}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:03:10,358][0m Trial 44 finished with value: 0.04729050604982927 and parameters: {'observation_period_num': 20, 'train_rates': 0.9071220562632106, 'learning_rate': 0.00027461228645390626, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7684903465529652}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:05:55,971][0m Trial 45 finished with value: 0.05324952246881026 and parameters: {'observation_period_num': 19, 'train_rates': 0.8868763222356862, 'learning_rate': 3.384647876406713e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.7879884724188865}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:07:25,123][0m Trial 46 finished with value: 0.20025462456620657 and parameters: {'observation_period_num': 156, 'train_rates': 0.9451255191456274, 'learning_rate': 0.00012672558123475136, 'batch_size': 67, 'step_size': 5, 'gamma': 0.7976945614835849}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:10:08,787][0m Trial 47 finished with value: 0.1407319786293166 and parameters: {'observation_period_num': 188, 'train_rates': 0.8003580040017984, 'learning_rate': 7.144605422751596e-05, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7712027847805271}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:10:42,067][0m Trial 48 finished with value: 0.0619897377555785 and parameters: {'observation_period_num': 5, 'train_rates': 0.8460882191298968, 'learning_rate': 9.026375940411599e-05, 'batch_size': 188, 'step_size': 8, 'gamma': 0.8151887700025497}. Best is trial 30 with value: 0.026989411416638326.[0m
[32m[I 2025-02-06 19:12:23,767][0m Trial 49 finished with value: 0.09648116130609902 and parameters: {'observation_period_num': 38, 'train_rates': 0.9337536645204724, 'learning_rate': 4.0881916036964677e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.7603769615169558}. Best is trial 30 with value: 0.026989411416638326.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2443 | 0.1480
Epoch 2/300, Loss: 0.1280 | 0.0891
Epoch 3/300, Loss: 0.1127 | 0.0740
Epoch 4/300, Loss: 0.1064 | 0.0705
Epoch 5/300, Loss: 0.1027 | 0.0677
Epoch 6/300, Loss: 0.0997 | 0.0644
Epoch 7/300, Loss: 0.0961 | 0.0610
Epoch 8/300, Loss: 0.0933 | 0.0572
Epoch 9/300, Loss: 0.0900 | 0.0543
Epoch 10/300, Loss: 0.0871 | 0.0523
Epoch 11/300, Loss: 0.0849 | 0.0509
Epoch 12/300, Loss: 0.0827 | 0.0484
Epoch 13/300, Loss: 0.0819 | 0.0478
Epoch 14/300, Loss: 0.0806 | 0.0472
Epoch 15/300, Loss: 0.0795 | 0.0463
Epoch 16/300, Loss: 0.0785 | 0.0454
Epoch 17/300, Loss: 0.0776 | 0.0444
Epoch 18/300, Loss: 0.0763 | 0.0426
Epoch 19/300, Loss: 0.0756 | 0.0415
Epoch 20/300, Loss: 0.0748 | 0.0400
Epoch 21/300, Loss: 0.0741 | 0.0389
Epoch 22/300, Loss: 0.0735 | 0.0382
Epoch 23/300, Loss: 0.0728 | 0.0360
Epoch 24/300, Loss: 0.0724 | 0.0358
Epoch 25/300, Loss: 0.0719 | 0.0357
Epoch 26/300, Loss: 0.0716 | 0.0356
Epoch 27/300, Loss: 0.0712 | 0.0354
Epoch 28/300, Loss: 0.0709 | 0.0353
Epoch 29/300, Loss: 0.0705 | 0.0351
Epoch 30/300, Loss: 0.0702 | 0.0349
Epoch 31/300, Loss: 0.0698 | 0.0347
Epoch 32/300, Loss: 0.0695 | 0.0345
Epoch 33/300, Loss: 0.0692 | 0.0344
Epoch 34/300, Loss: 0.0689 | 0.0343
Epoch 35/300, Loss: 0.0687 | 0.0341
Epoch 36/300, Loss: 0.0684 | 0.0339
Epoch 37/300, Loss: 0.0682 | 0.0338
Epoch 38/300, Loss: 0.0680 | 0.0337
Epoch 39/300, Loss: 0.0679 | 0.0336
Epoch 40/300, Loss: 0.0677 | 0.0335
Epoch 41/300, Loss: 0.0675 | 0.0333
Epoch 42/300, Loss: 0.0674 | 0.0332
Epoch 43/300, Loss: 0.0672 | 0.0330
Epoch 44/300, Loss: 0.0671 | 0.0329
Epoch 45/300, Loss: 0.0669 | 0.0330
Epoch 46/300, Loss: 0.0668 | 0.0329
Epoch 47/300, Loss: 0.0667 | 0.0327
Epoch 48/300, Loss: 0.0665 | 0.0326
Epoch 49/300, Loss: 0.0664 | 0.0325
Epoch 50/300, Loss: 0.0663 | 0.0324
Epoch 51/300, Loss: 0.0661 | 0.0323
Epoch 52/300, Loss: 0.0661 | 0.0323
Epoch 53/300, Loss: 0.0660 | 0.0322
Epoch 54/300, Loss: 0.0658 | 0.0321
Epoch 55/300, Loss: 0.0658 | 0.0321
Epoch 56/300, Loss: 0.0656 | 0.0320
Epoch 57/300, Loss: 0.0656 | 0.0320
Epoch 58/300, Loss: 0.0655 | 0.0320
Epoch 59/300, Loss: 0.0654 | 0.0319
Epoch 60/300, Loss: 0.0653 | 0.0319
Epoch 61/300, Loss: 0.0653 | 0.0319
Epoch 62/300, Loss: 0.0652 | 0.0319
Epoch 63/300, Loss: 0.0652 | 0.0318
Epoch 64/300, Loss: 0.0651 | 0.0318
Epoch 65/300, Loss: 0.0651 | 0.0317
Epoch 66/300, Loss: 0.0650 | 0.0317
Epoch 67/300, Loss: 0.0650 | 0.0315
Epoch 68/300, Loss: 0.0649 | 0.0314
Epoch 69/300, Loss: 0.0649 | 0.0314
Epoch 70/300, Loss: 0.0648 | 0.0314
Epoch 71/300, Loss: 0.0648 | 0.0314
Epoch 72/300, Loss: 0.0647 | 0.0313
Epoch 73/300, Loss: 0.0647 | 0.0313
Epoch 74/300, Loss: 0.0646 | 0.0313
Epoch 75/300, Loss: 0.0646 | 0.0312
Epoch 76/300, Loss: 0.0645 | 0.0312
Epoch 77/300, Loss: 0.0645 | 0.0312
Epoch 78/300, Loss: 0.0645 | 0.0311
Epoch 79/300, Loss: 0.0644 | 0.0311
Epoch 80/300, Loss: 0.0644 | 0.0310
Epoch 81/300, Loss: 0.0643 | 0.0310
Epoch 82/300, Loss: 0.0643 | 0.0310
Epoch 83/300, Loss: 0.0643 | 0.0310
Epoch 84/300, Loss: 0.0642 | 0.0309
Epoch 85/300, Loss: 0.0642 | 0.0309
Epoch 86/300, Loss: 0.0642 | 0.0309
Epoch 87/300, Loss: 0.0641 | 0.0309
Epoch 88/300, Loss: 0.0641 | 0.0309
Epoch 89/300, Loss: 0.0641 | 0.0309
Epoch 90/300, Loss: 0.0641 | 0.0309
Epoch 91/300, Loss: 0.0640 | 0.0309
Epoch 92/300, Loss: 0.0640 | 0.0309
Epoch 93/300, Loss: 0.0640 | 0.0308
Epoch 94/300, Loss: 0.0640 | 0.0308
Epoch 95/300, Loss: 0.0639 | 0.0308
Epoch 96/300, Loss: 0.0639 | 0.0308
Epoch 97/300, Loss: 0.0639 | 0.0308
Epoch 98/300, Loss: 0.0639 | 0.0308
Epoch 99/300, Loss: 0.0639 | 0.0308
Epoch 100/300, Loss: 0.0638 | 0.0308
Epoch 101/300, Loss: 0.0638 | 0.0308
Epoch 102/300, Loss: 0.0638 | 0.0308
Epoch 103/300, Loss: 0.0638 | 0.0308
Epoch 104/300, Loss: 0.0638 | 0.0308
Epoch 105/300, Loss: 0.0638 | 0.0307
Epoch 106/300, Loss: 0.0638 | 0.0307
Epoch 107/300, Loss: 0.0638 | 0.0307
Epoch 108/300, Loss: 0.0637 | 0.0307
Epoch 109/300, Loss: 0.0637 | 0.0307
Epoch 110/300, Loss: 0.0637 | 0.0307
Epoch 111/300, Loss: 0.0637 | 0.0307
Epoch 112/300, Loss: 0.0637 | 0.0307
Epoch 113/300, Loss: 0.0637 | 0.0307
Epoch 114/300, Loss: 0.0637 | 0.0307
Epoch 115/300, Loss: 0.0637 | 0.0307
Epoch 116/300, Loss: 0.0637 | 0.0307
Epoch 117/300, Loss: 0.0637 | 0.0307
Epoch 118/300, Loss: 0.0636 | 0.0307
Epoch 119/300, Loss: 0.0636 | 0.0307
Epoch 120/300, Loss: 0.0636 | 0.0307
Epoch 121/300, Loss: 0.0636 | 0.0307
Epoch 122/300, Loss: 0.0636 | 0.0307
Epoch 123/300, Loss: 0.0636 | 0.0307
Epoch 124/300, Loss: 0.0636 | 0.0306
Epoch 125/300, Loss: 0.0636 | 0.0306
Epoch 126/300, Loss: 0.0636 | 0.0306
Epoch 127/300, Loss: 0.0636 | 0.0306
Epoch 128/300, Loss: 0.0636 | 0.0306
Epoch 129/300, Loss: 0.0636 | 0.0306
Epoch 130/300, Loss: 0.0636 | 0.0306
Epoch 131/300, Loss: 0.0636 | 0.0306
Epoch 132/300, Loss: 0.0636 | 0.0306
Epoch 133/300, Loss: 0.0636 | 0.0306
Epoch 134/300, Loss: 0.0636 | 0.0306
Epoch 135/300, Loss: 0.0636 | 0.0306
Epoch 136/300, Loss: 0.0635 | 0.0306
Epoch 137/300, Loss: 0.0635 | 0.0306
Epoch 138/300, Loss: 0.0635 | 0.0306
Epoch 139/300, Loss: 0.0635 | 0.0306
Epoch 140/300, Loss: 0.0635 | 0.0306
Epoch 141/300, Loss: 0.0635 | 0.0306
Epoch 142/300, Loss: 0.0635 | 0.0306
Epoch 143/300, Loss: 0.0635 | 0.0306
Epoch 144/300, Loss: 0.0635 | 0.0306
Epoch 145/300, Loss: 0.0635 | 0.0306
Epoch 146/300, Loss: 0.0635 | 0.0306
Epoch 147/300, Loss: 0.0635 | 0.0306
Epoch 148/300, Loss: 0.0635 | 0.0306
Epoch 149/300, Loss: 0.0635 | 0.0306
Epoch 150/300, Loss: 0.0635 | 0.0306
Epoch 151/300, Loss: 0.0635 | 0.0306
Epoch 152/300, Loss: 0.0635 | 0.0306
Epoch 153/300, Loss: 0.0635 | 0.0306
Epoch 154/300, Loss: 0.0635 | 0.0306
Epoch 155/300, Loss: 0.0635 | 0.0306
Epoch 156/300, Loss: 0.0635 | 0.0306
Epoch 157/300, Loss: 0.0635 | 0.0306
Epoch 158/300, Loss: 0.0635 | 0.0306
Epoch 159/300, Loss: 0.0635 | 0.0306
Epoch 160/300, Loss: 0.0635 | 0.0306
Epoch 161/300, Loss: 0.0635 | 0.0306
Epoch 162/300, Loss: 0.0635 | 0.0306
Epoch 163/300, Loss: 0.0635 | 0.0306
Epoch 164/300, Loss: 0.0635 | 0.0306
Epoch 165/300, Loss: 0.0635 | 0.0306
Epoch 166/300, Loss: 0.0635 | 0.0306
Epoch 167/300, Loss: 0.0635 | 0.0306
Epoch 168/300, Loss: 0.0635 | 0.0306
Epoch 169/300, Loss: 0.0635 | 0.0306
Epoch 170/300, Loss: 0.0635 | 0.0306
Epoch 171/300, Loss: 0.0635 | 0.0306
Epoch 172/300, Loss: 0.0635 | 0.0306
Epoch 173/300, Loss: 0.0635 | 0.0306
Epoch 174/300, Loss: 0.0635 | 0.0306
Epoch 175/300, Loss: 0.0635 | 0.0306
Epoch 176/300, Loss: 0.0635 | 0.0306
Epoch 177/300, Loss: 0.0635 | 0.0306
Epoch 178/300, Loss: 0.0635 | 0.0306
Epoch 179/300, Loss: 0.0635 | 0.0306
Epoch 180/300, Loss: 0.0635 | 0.0306
Epoch 181/300, Loss: 0.0635 | 0.0306
Epoch 182/300, Loss: 0.0635 | 0.0306
Epoch 183/300, Loss: 0.0635 | 0.0306
Epoch 184/300, Loss: 0.0635 | 0.0306
Epoch 185/300, Loss: 0.0635 | 0.0306
Epoch 186/300, Loss: 0.0635 | 0.0306
Epoch 187/300, Loss: 0.0635 | 0.0306
Epoch 188/300, Loss: 0.0635 | 0.0306
Epoch 189/300, Loss: 0.0635 | 0.0306
Epoch 190/300, Loss: 0.0635 | 0.0306
Epoch 191/300, Loss: 0.0635 | 0.0306
Epoch 192/300, Loss: 0.0635 | 0.0306
Epoch 193/300, Loss: 0.0635 | 0.0306
Epoch 194/300, Loss: 0.0635 | 0.0306
Epoch 195/300, Loss: 0.0635 | 0.0306
Epoch 196/300, Loss: 0.0635 | 0.0306
Epoch 197/300, Loss: 0.0635 | 0.0306
Epoch 198/300, Loss: 0.0635 | 0.0306
Epoch 199/300, Loss: 0.0635 | 0.0306
Epoch 200/300, Loss: 0.0635 | 0.0306
Epoch 201/300, Loss: 0.0635 | 0.0306
Epoch 202/300, Loss: 0.0635 | 0.0306
Epoch 203/300, Loss: 0.0635 | 0.0306
Epoch 204/300, Loss: 0.0635 | 0.0306
Epoch 205/300, Loss: 0.0635 | 0.0306
Epoch 206/300, Loss: 0.0635 | 0.0306
Epoch 207/300, Loss: 0.0635 | 0.0306
Epoch 208/300, Loss: 0.0635 | 0.0306
Epoch 209/300, Loss: 0.0635 | 0.0306
Epoch 210/300, Loss: 0.0635 | 0.0306
Epoch 211/300, Loss: 0.0635 | 0.0306
Epoch 212/300, Loss: 0.0635 | 0.0306
Epoch 213/300, Loss: 0.0635 | 0.0306
Epoch 214/300, Loss: 0.0635 | 0.0306
Epoch 215/300, Loss: 0.0635 | 0.0306
Epoch 216/300, Loss: 0.0635 | 0.0306
Epoch 217/300, Loss: 0.0635 | 0.0306
Epoch 218/300, Loss: 0.0635 | 0.0306
Epoch 219/300, Loss: 0.0635 | 0.0306
Epoch 220/300, Loss: 0.0635 | 0.0306
Epoch 221/300, Loss: 0.0635 | 0.0306
Epoch 222/300, Loss: 0.0635 | 0.0306
Epoch 223/300, Loss: 0.0635 | 0.0306
Epoch 224/300, Loss: 0.0635 | 0.0306
Epoch 225/300, Loss: 0.0635 | 0.0306
Epoch 226/300, Loss: 0.0635 | 0.0306
Epoch 227/300, Loss: 0.0635 | 0.0306
Epoch 228/300, Loss: 0.0635 | 0.0306
Epoch 229/300, Loss: 0.0635 | 0.0306
Epoch 230/300, Loss: 0.0635 | 0.0306
Epoch 231/300, Loss: 0.0635 | 0.0306
Epoch 232/300, Loss: 0.0635 | 0.0306
Epoch 233/300, Loss: 0.0635 | 0.0306
Epoch 234/300, Loss: 0.0635 | 0.0306
Epoch 235/300, Loss: 0.0635 | 0.0306
Epoch 236/300, Loss: 0.0635 | 0.0306
Epoch 237/300, Loss: 0.0635 | 0.0306
Epoch 238/300, Loss: 0.0635 | 0.0306
Epoch 239/300, Loss: 0.0635 | 0.0306
Epoch 240/300, Loss: 0.0635 | 0.0306
Epoch 241/300, Loss: 0.0635 | 0.0306
Epoch 242/300, Loss: 0.0635 | 0.0306
Epoch 243/300, Loss: 0.0635 | 0.0306
Epoch 244/300, Loss: 0.0635 | 0.0306
Epoch 245/300, Loss: 0.0635 | 0.0306
Epoch 246/300, Loss: 0.0635 | 0.0306
Epoch 247/300, Loss: 0.0635 | 0.0306
Epoch 248/300, Loss: 0.0635 | 0.0306
Epoch 249/300, Loss: 0.0635 | 0.0306
Epoch 250/300, Loss: 0.0635 | 0.0306
Epoch 251/300, Loss: 0.0635 | 0.0306
Epoch 252/300, Loss: 0.0635 | 0.0306
Epoch 253/300, Loss: 0.0635 | 0.0306
Epoch 254/300, Loss: 0.0635 | 0.0306
Epoch 255/300, Loss: 0.0635 | 0.0306
Epoch 256/300, Loss: 0.0635 | 0.0306
Epoch 257/300, Loss: 0.0635 | 0.0306
Epoch 258/300, Loss: 0.0635 | 0.0306
Epoch 259/300, Loss: 0.0635 | 0.0306
Epoch 260/300, Loss: 0.0635 | 0.0306
Epoch 261/300, Loss: 0.0635 | 0.0306
Epoch 262/300, Loss: 0.0635 | 0.0306
Epoch 263/300, Loss: 0.0635 | 0.0306
Epoch 264/300, Loss: 0.0635 | 0.0306
Epoch 265/300, Loss: 0.0635 | 0.0306
Epoch 266/300, Loss: 0.0635 | 0.0306
Epoch 267/300, Loss: 0.0635 | 0.0306
Epoch 268/300, Loss: 0.0635 | 0.0306
Epoch 269/300, Loss: 0.0635 | 0.0306
Epoch 270/300, Loss: 0.0635 | 0.0306
Epoch 271/300, Loss: 0.0635 | 0.0306
Epoch 272/300, Loss: 0.0635 | 0.0306
Epoch 273/300, Loss: 0.0635 | 0.0306
Epoch 274/300, Loss: 0.0635 | 0.0306
Epoch 275/300, Loss: 0.0635 | 0.0306
Epoch 276/300, Loss: 0.0635 | 0.0306
Epoch 277/300, Loss: 0.0635 | 0.0306
Epoch 278/300, Loss: 0.0635 | 0.0306
Epoch 279/300, Loss: 0.0635 | 0.0306
Epoch 280/300, Loss: 0.0635 | 0.0306
Epoch 281/300, Loss: 0.0635 | 0.0306
Epoch 282/300, Loss: 0.0635 | 0.0306
Epoch 283/300, Loss: 0.0635 | 0.0306
Epoch 284/300, Loss: 0.0635 | 0.0306
Epoch 285/300, Loss: 0.0635 | 0.0306
Epoch 286/300, Loss: 0.0635 | 0.0306
Epoch 287/300, Loss: 0.0635 | 0.0306
Epoch 288/300, Loss: 0.0635 | 0.0306
Epoch 289/300, Loss: 0.0635 | 0.0306
Epoch 290/300, Loss: 0.0635 | 0.0306
Epoch 291/300, Loss: 0.0635 | 0.0306
Epoch 292/300, Loss: 0.0635 | 0.0306
Epoch 293/300, Loss: 0.0635 | 0.0306
Epoch 294/300, Loss: 0.0635 | 0.0306
Epoch 295/300, Loss: 0.0635 | 0.0306
Epoch 296/300, Loss: 0.0635 | 0.0306
Epoch 297/300, Loss: 0.0635 | 0.0306
Epoch 298/300, Loss: 0.0635 | 0.0306
Epoch 299/300, Loss: 0.0635 | 0.0306
Epoch 300/300, Loss: 0.0635 | 0.0306
Runtime (seconds): 794.037359714508
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.09920731279999018
RMSE: 0.314971923828125
MAE: 0.314971923828125
R-squared: nan
[197.49496]
