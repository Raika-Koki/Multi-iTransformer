ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-07 16:29:33,810][0m A new study created in memory with name: no-name-7585d10b-774a-4bbc-a39b-b3f92fbc0193[0m
[32m[I 2025-02-07 16:30:40,662][0m Trial 0 finished with value: 0.1619228006977784 and parameters: {'observation_period_num': 201, 'train_rates': 0.7935421996015991, 'learning_rate': 0.0004890713593419874, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8766961042989538}. Best is trial 0 with value: 0.1619228006977784.[0m
[32m[I 2025-02-07 16:31:12,768][0m Trial 1 finished with value: 0.11884887363104259 and parameters: {'observation_period_num': 148, 'train_rates': 0.8306803318376462, 'learning_rate': 0.0004900165196619036, 'batch_size': 173, 'step_size': 11, 'gamma': 0.7810759081539147}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:31:50,940][0m Trial 2 finished with value: 0.16665403950227842 and parameters: {'observation_period_num': 55, 'train_rates': 0.6950532179831688, 'learning_rate': 0.00012783798375233765, 'batch_size': 134, 'step_size': 7, 'gamma': 0.9709924379540826}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:32:20,508][0m Trial 3 finished with value: 0.2848028427863543 and parameters: {'observation_period_num': 190, 'train_rates': 0.8370224543839113, 'learning_rate': 5.3403903487093025e-05, 'batch_size': 182, 'step_size': 4, 'gamma': 0.8615028264510297}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:34:41,543][0m Trial 4 finished with value: 0.12256376094677869 and parameters: {'observation_period_num': 113, 'train_rates': 0.9405970664871506, 'learning_rate': 2.1165606448740934e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.8528089853724451}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:35:57,719][0m Trial 5 finished with value: 0.24218490239736196 and parameters: {'observation_period_num': 83, 'train_rates': 0.6784161972432875, 'learning_rate': 0.0002878788845972433, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8368730334053132}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:36:18,332][0m Trial 6 finished with value: 0.2637263201712583 and parameters: {'observation_period_num': 77, 'train_rates': 0.6622985607495493, 'learning_rate': 0.0001947411208726205, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8156041814082806}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:37:23,917][0m Trial 7 finished with value: 0.2233065355145289 and parameters: {'observation_period_num': 89, 'train_rates': 0.7675020901405883, 'learning_rate': 0.000285077742712471, 'batch_size': 76, 'step_size': 13, 'gamma': 0.9704386250207004}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:38:35,910][0m Trial 8 finished with value: 0.16381869111366631 and parameters: {'observation_period_num': 169, 'train_rates': 0.9147317125645948, 'learning_rate': 0.00012995496017281964, 'batch_size': 79, 'step_size': 6, 'gamma': 0.9508630016132669}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:39:08,246][0m Trial 9 finished with value: 0.1490758270520926 and parameters: {'observation_period_num': 166, 'train_rates': 0.8278740419612125, 'learning_rate': 0.0009575882643110379, 'batch_size': 170, 'step_size': 1, 'gamma': 0.8957693798734463}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:39:34,162][0m Trial 10 finished with value: 0.48572302883726426 and parameters: {'observation_period_num': 16, 'train_rates': 0.8819470191694345, 'learning_rate': 2.418168992902014e-06, 'batch_size': 247, 'step_size': 11, 'gamma': 0.7686069105256985}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:45:15,329][0m Trial 11 finished with value: 0.1991620429775171 and parameters: {'observation_period_num': 248, 'train_rates': 0.9790459645305553, 'learning_rate': 8.600514136487701e-06, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7601827261301052}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:46:04,799][0m Trial 12 finished with value: 0.2928204834461212 and parameters: {'observation_period_num': 143, 'train_rates': 0.9851508504703388, 'learning_rate': 1.6624267083189775e-05, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8036300814444812}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:46:34,485][0m Trial 13 finished with value: 0.17261048423575096 and parameters: {'observation_period_num': 119, 'train_rates': 0.8935488409450227, 'learning_rate': 3.9063190548446266e-05, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9173301006864578}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:47:16,525][0m Trial 14 finished with value: 0.5567144804141101 and parameters: {'observation_period_num': 118, 'train_rates': 0.7308512713194011, 'learning_rate': 6.419098279561745e-06, 'batch_size': 117, 'step_size': 15, 'gamma': 0.7933187850250253}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:52:50,752][0m Trial 15 finished with value: 0.16790243519280484 and parameters: {'observation_period_num': 228, 'train_rates': 0.9391755768593878, 'learning_rate': 6.519283883689119e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8428673562504525}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:53:18,307][0m Trial 16 finished with value: 1.1663627273218644 and parameters: {'observation_period_num': 134, 'train_rates': 0.8535300832590199, 'learning_rate': 1.302085748513957e-06, 'batch_size': 208, 'step_size': 9, 'gamma': 0.7505528315835729}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:53:59,704][0m Trial 17 finished with value: 0.17398446321487426 and parameters: {'observation_period_num': 24, 'train_rates': 0.9404294133732258, 'learning_rate': 1.7538100863772243e-05, 'batch_size': 161, 'step_size': 5, 'gamma': 0.9219023909896695}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:54:40,210][0m Trial 18 finished with value: 0.7464413243674473 and parameters: {'observation_period_num': 104, 'train_rates': 0.6040848599067132, 'learning_rate': 4.439516917309084e-06, 'batch_size': 109, 'step_size': 8, 'gamma': 0.8300076245608548}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:55:04,636][0m Trial 19 finished with value: 0.5187434639511931 and parameters: {'observation_period_num': 166, 'train_rates': 0.767670051816115, 'learning_rate': 2.0257943033370095e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.7829870787542512}. Best is trial 1 with value: 0.11884887363104259.[0m
[32m[I 2025-02-07 16:57:19,605][0m Trial 20 finished with value: 0.1143857091665268 and parameters: {'observation_period_num': 53, 'train_rates': 0.8691475361490288, 'learning_rate': 0.0007328233203624455, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8643189662718413}. Best is trial 20 with value: 0.1143857091665268.[0m
[32m[I 2025-02-07 16:59:10,015][0m Trial 21 finished with value: 0.169695460762318 and parameters: {'observation_period_num': 46, 'train_rates': 0.8649462554480242, 'learning_rate': 0.000854273491089385, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8663157785994758}. Best is trial 20 with value: 0.1143857091665268.[0m
[32m[I 2025-02-07 17:01:11,183][0m Trial 22 finished with value: 0.08982831133068155 and parameters: {'observation_period_num': 50, 'train_rates': 0.8128592947222782, 'learning_rate': 0.00041756131267873354, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8865715205913077}. Best is trial 22 with value: 0.08982831133068155.[0m
[32m[I 2025-02-07 17:02:02,984][0m Trial 23 finished with value: 0.06584995028169544 and parameters: {'observation_period_num': 54, 'train_rates': 0.8117580904691226, 'learning_rate': 0.00047988918827567077, 'batch_size': 107, 'step_size': 11, 'gamma': 0.8953260226260009}. Best is trial 23 with value: 0.06584995028169544.[0m
[32m[I 2025-02-07 17:02:56,914][0m Trial 24 finished with value: 0.0843717633420847 and parameters: {'observation_period_num': 50, 'train_rates': 0.8047000802416453, 'learning_rate': 0.0005107440308441107, 'batch_size': 98, 'step_size': 13, 'gamma': 0.8904955644330039}. Best is trial 23 with value: 0.06584995028169544.[0m
[32m[I 2025-02-07 17:03:51,379][0m Trial 25 finished with value: 0.051990319087298634 and parameters: {'observation_period_num': 37, 'train_rates': 0.7970304806564764, 'learning_rate': 9.46014900462305e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8884214219369923}. Best is trial 25 with value: 0.051990319087298634.[0m
[32m[I 2025-02-07 17:04:46,160][0m Trial 26 finished with value: 0.18068533848169482 and parameters: {'observation_period_num': 31, 'train_rates': 0.7844123111836675, 'learning_rate': 0.00011471678999372764, 'batch_size': 96, 'step_size': 15, 'gamma': 0.9089599052988666}. Best is trial 25 with value: 0.051990319087298634.[0m
[32m[I 2025-02-07 17:05:38,074][0m Trial 27 finished with value: 0.19415846490457037 and parameters: {'observation_period_num': 72, 'train_rates': 0.7414635995338641, 'learning_rate': 8.22053784677506e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.9394424617050556}. Best is trial 25 with value: 0.051990319087298634.[0m
[32m[I 2025-02-07 17:06:14,818][0m Trial 28 finished with value: 0.14885917895427153 and parameters: {'observation_period_num': 7, 'train_rates': 0.7348745346439041, 'learning_rate': 0.00021455219407661477, 'batch_size': 147, 'step_size': 14, 'gamma': 0.8988988590930557}. Best is trial 25 with value: 0.051990319087298634.[0m
[32m[I 2025-02-07 17:07:14,839][0m Trial 29 finished with value: 0.049040393576250604 and parameters: {'observation_period_num': 34, 'train_rates': 0.7959051555485636, 'learning_rate': 0.0004624841754962411, 'batch_size': 93, 'step_size': 13, 'gamma': 0.8827359675097759}. Best is trial 29 with value: 0.049040393576250604.[0m
[32m[I 2025-02-07 17:08:01,777][0m Trial 30 finished with value: 0.18230077504303657 and parameters: {'observation_period_num': 32, 'train_rates': 0.7858570686260848, 'learning_rate': 0.00031683872271194455, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8774078762504712}. Best is trial 29 with value: 0.049040393576250604.[0m
[32m[I 2025-02-07 17:09:02,916][0m Trial 31 finished with value: 0.08981283135199673 and parameters: {'observation_period_num': 61, 'train_rates': 0.8046365830715534, 'learning_rate': 0.0005578072068687812, 'batch_size': 88, 'step_size': 13, 'gamma': 0.9355254965801667}. Best is trial 29 with value: 0.049040393576250604.[0m
[32m[I 2025-02-07 17:10:22,558][0m Trial 32 finished with value: 0.05345687473242262 and parameters: {'observation_period_num': 35, 'train_rates': 0.8070711102463031, 'learning_rate': 0.00043502030242813396, 'batch_size': 67, 'step_size': 12, 'gamma': 0.8808307673888386}. Best is trial 29 with value: 0.049040393576250604.[0m
[32m[I 2025-02-07 17:11:43,973][0m Trial 33 finished with value: 0.17291382867398802 and parameters: {'observation_period_num': 36, 'train_rates': 0.7560920720181983, 'learning_rate': 0.00017467921354567324, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8835756841505574}. Best is trial 29 with value: 0.049040393576250604.[0m
[32m[I 2025-02-07 17:12:27,121][0m Trial 34 finished with value: 0.03164296977532407 and parameters: {'observation_period_num': 13, 'train_rates': 0.8372107469451779, 'learning_rate': 0.0003957799553701948, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9106259805066065}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:13:48,167][0m Trial 35 finished with value: 0.034698317098636695 and parameters: {'observation_period_num': 13, 'train_rates': 0.8414971536382716, 'learning_rate': 9.477150793436623e-05, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9110610543550982}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:14:28,099][0m Trial 36 finished with value: 0.058412648895616995 and parameters: {'observation_period_num': 5, 'train_rates': 0.8406119687285639, 'learning_rate': 0.00010233570401663076, 'batch_size': 146, 'step_size': 15, 'gamma': 0.9258848016872959}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:15:08,098][0m Trial 37 finished with value: 0.15949891758141618 and parameters: {'observation_period_num': 21, 'train_rates': 0.7069471526428185, 'learning_rate': 3.983718072200269e-05, 'batch_size': 125, 'step_size': 15, 'gamma': 0.9890277843214973}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:16:12,431][0m Trial 38 finished with value: 0.041944997742311245 and parameters: {'observation_period_num': 16, 'train_rates': 0.8354733203927728, 'learning_rate': 5.8842182742017085e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.9093825117848034}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:17:19,717][0m Trial 39 finished with value: 0.07294033478528585 and parameters: {'observation_period_num': 67, 'train_rates': 0.904564267800763, 'learning_rate': 5.790751877792852e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.9433646116770208}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:19:03,659][0m Trial 40 finished with value: 0.048967791759550815 and parameters: {'observation_period_num': 15, 'train_rates': 0.8349911274154059, 'learning_rate': 3.148764767536988e-05, 'batch_size': 52, 'step_size': 15, 'gamma': 0.9595142736042656}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:20:40,964][0m Trial 41 finished with value: 0.0445877657039091 and parameters: {'observation_period_num': 15, 'train_rates': 0.8369939560827551, 'learning_rate': 2.8290575553741906e-05, 'batch_size': 55, 'step_size': 15, 'gamma': 0.9589439845164693}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:22:23,849][0m Trial 42 finished with value: 0.05164125011086713 and parameters: {'observation_period_num': 17, 'train_rates': 0.8374978429460359, 'learning_rate': 2.8564130171598482e-05, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9658055193207583}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:25:26,444][0m Trial 43 finished with value: 0.04782910705951776 and parameters: {'observation_period_num': 5, 'train_rates': 0.8532164895578097, 'learning_rate': 1.2189725145910586e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.953732434960176}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:26:44,219][0m Trial 44 finished with value: 0.05502787902594754 and parameters: {'observation_period_num': 5, 'train_rates': 0.8587075339304576, 'learning_rate': 1.2410116330002204e-05, 'batch_size': 74, 'step_size': 14, 'gamma': 0.9506853686805359}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:29:49,586][0m Trial 45 finished with value: 0.08949339621138294 and parameters: {'observation_period_num': 97, 'train_rates': 0.8802075839942273, 'learning_rate': 1.237193273332234e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9811152930553243}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:32:46,866][0m Trial 46 finished with value: 0.03462809917330742 and parameters: {'observation_period_num': 22, 'train_rates': 0.9152049731710106, 'learning_rate': 4.685474532186764e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.9075305004060454}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:36:09,795][0m Trial 47 finished with value: 0.04098314907470787 and parameters: {'observation_period_num': 23, 'train_rates': 0.9633081974557134, 'learning_rate': 4.275597787373266e-05, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9093029346010907}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:39:31,179][0m Trial 48 finished with value: 0.05005556499931665 and parameters: {'observation_period_num': 25, 'train_rates': 0.9615397781515393, 'learning_rate': 4.826030802202718e-05, 'batch_size': 30, 'step_size': 3, 'gamma': 0.9085913608329866}. Best is trial 34 with value: 0.03164296977532407.[0m
[32m[I 2025-02-07 17:40:51,891][0m Trial 49 finished with value: 0.13736589635730884 and parameters: {'observation_period_num': 82, 'train_rates': 0.9245373945355085, 'learning_rate': 0.00015474105294480646, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9089323106174728}. Best is trial 34 with value: 0.03164296977532407.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-07 17:40:51,901][0m A new study created in memory with name: no-name-bf930a12-4e18-421f-969d-418cf9967b2d[0m
[32m[I 2025-02-07 17:41:38,267][0m Trial 0 finished with value: 0.16873380182502426 and parameters: {'observation_period_num': 35, 'train_rates': 0.8171531743272235, 'learning_rate': 7.374845568350977e-06, 'batch_size': 123, 'step_size': 15, 'gamma': 0.9406321312696582}. Best is trial 0 with value: 0.16873380182502426.[0m
[32m[I 2025-02-07 17:43:03,468][0m Trial 1 finished with value: 0.5653193877908078 and parameters: {'observation_period_num': 123, 'train_rates': 0.600274140147691, 'learning_rate': 1.5801594337261815e-06, 'batch_size': 50, 'step_size': 10, 'gamma': 0.9355281636816354}. Best is trial 0 with value: 0.16873380182502426.[0m
[32m[I 2025-02-07 17:43:42,830][0m Trial 2 finished with value: 0.21171602823821103 and parameters: {'observation_period_num': 198, 'train_rates': 0.870967706960742, 'learning_rate': 0.0003644548874516266, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9484526375405027}. Best is trial 0 with value: 0.16873380182502426.[0m
[32m[I 2025-02-07 17:44:08,873][0m Trial 3 finished with value: 0.2715032994747162 and parameters: {'observation_period_num': 237, 'train_rates': 0.9172528612273594, 'learning_rate': 2.9801220256427722e-05, 'batch_size': 228, 'step_size': 15, 'gamma': 0.8706433725064984}. Best is trial 0 with value: 0.16873380182502426.[0m
[32m[I 2025-02-07 17:47:26,952][0m Trial 4 finished with value: 0.14824646968141342 and parameters: {'observation_period_num': 170, 'train_rates': 0.8524919239785128, 'learning_rate': 0.0006150667148102028, 'batch_size': 26, 'step_size': 1, 'gamma': 0.8894042659804575}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:48:37,106][0m Trial 5 finished with value: 0.3232859045755668 and parameters: {'observation_period_num': 186, 'train_rates': 0.7803075943584029, 'learning_rate': 3.227011819857483e-05, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8064326761559681}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:49:09,939][0m Trial 6 finished with value: 0.16359919015272165 and parameters: {'observation_period_num': 111, 'train_rates': 0.82941142359064, 'learning_rate': 4.67491595064799e-05, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8575102658661699}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:50:13,068][0m Trial 7 finished with value: 0.7824563980102539 and parameters: {'observation_period_num': 153, 'train_rates': 0.9855277047316253, 'learning_rate': 1.0476343590908601e-06, 'batch_size': 88, 'step_size': 4, 'gamma': 0.8399628134355273}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:51:10,579][0m Trial 8 finished with value: 0.347272742630245 and parameters: {'observation_period_num': 122, 'train_rates': 0.7537510100400644, 'learning_rate': 1.6272608776897128e-05, 'batch_size': 83, 'step_size': 7, 'gamma': 0.890710155357494}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:51:31,545][0m Trial 9 finished with value: 0.705852389620822 and parameters: {'observation_period_num': 212, 'train_rates': 0.7717603378055069, 'learning_rate': 9.33153475258592e-06, 'batch_size': 246, 'step_size': 12, 'gamma': 0.8327879193911898}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:53:56,503][0m Trial 10 finished with value: 0.20822018230673853 and parameters: {'observation_period_num': 60, 'train_rates': 0.6769141587273804, 'learning_rate': 0.0009617395256228938, 'batch_size': 30, 'step_size': 2, 'gamma': 0.7551037625093682}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:54:28,289][0m Trial 11 finished with value: 0.27104101271168085 and parameters: {'observation_period_num': 95, 'train_rates': 0.8720616391563785, 'learning_rate': 0.00012783508589073994, 'batch_size': 181, 'step_size': 1, 'gamma': 0.9006254145211067}. Best is trial 4 with value: 0.14824646968141342.[0m
[32m[I 2025-02-07 17:54:57,485][0m Trial 12 finished with value: 0.12908235138334404 and parameters: {'observation_period_num': 161, 'train_rates': 0.8472951000367401, 'learning_rate': 0.00014868366398758221, 'batch_size': 183, 'step_size': 5, 'gamma': 0.976303784856648}. Best is trial 12 with value: 0.12908235138334404.[0m
[32m[I 2025-02-07 17:55:26,982][0m Trial 13 finished with value: 0.15947063267230988 and parameters: {'observation_period_num': 166, 'train_rates': 0.9403308703130364, 'learning_rate': 0.00019315943511914887, 'batch_size': 189, 'step_size': 3, 'gamma': 0.9795922411952159}. Best is trial 12 with value: 0.12908235138334404.[0m
[32m[I 2025-02-07 17:55:51,260][0m Trial 14 finished with value: 0.2803962373445722 and parameters: {'observation_period_num': 160, 'train_rates': 0.7191159487643729, 'learning_rate': 0.0007497056857798904, 'batch_size': 206, 'step_size': 5, 'gamma': 0.9852798124264402}. Best is trial 12 with value: 0.12908235138334404.[0m
[32m[I 2025-02-07 17:56:33,689][0m Trial 15 finished with value: 0.3126570279371805 and parameters: {'observation_period_num': 228, 'train_rates': 0.8690993270633561, 'learning_rate': 9.880860414658062e-05, 'batch_size': 120, 'step_size': 1, 'gamma': 0.9133176917987426}. Best is trial 12 with value: 0.12908235138334404.[0m
[32m[I 2025-02-07 17:57:25,333][0m Trial 16 finished with value: 0.08302051191616164 and parameters: {'observation_period_num': 77, 'train_rates': 0.9213311511174123, 'learning_rate': 0.00029123923329708206, 'batch_size': 107, 'step_size': 4, 'gamma': 0.8018339493088427}. Best is trial 16 with value: 0.08302051191616164.[0m
[32m[I 2025-02-07 17:58:20,724][0m Trial 17 finished with value: 0.07186312642362383 and parameters: {'observation_period_num': 72, 'train_rates': 0.9285741974245769, 'learning_rate': 0.0003757378457631264, 'batch_size': 97, 'step_size': 10, 'gamma': 0.7746402982531456}. Best is trial 17 with value: 0.07186312642362383.[0m
[32m[I 2025-02-07 17:59:15,622][0m Trial 18 finished with value: 0.0648738813198092 and parameters: {'observation_period_num': 71, 'train_rates': 0.9314156793785706, 'learning_rate': 0.00033112087020188685, 'batch_size': 103, 'step_size': 9, 'gamma': 0.7623416378410398}. Best is trial 18 with value: 0.0648738813198092.[0m
[32m[I 2025-02-07 18:00:16,624][0m Trial 19 finished with value: 0.05992282344959676 and parameters: {'observation_period_num': 14, 'train_rates': 0.9513086653625782, 'learning_rate': 6.573410008317709e-05, 'batch_size': 93, 'step_size': 10, 'gamma': 0.7503592572263122}. Best is trial 19 with value: 0.05992282344959676.[0m
[32m[I 2025-02-07 18:01:42,889][0m Trial 20 finished with value: 0.05999297508206524 and parameters: {'observation_period_num': 5, 'train_rates': 0.9588095522405303, 'learning_rate': 6.171341567593364e-05, 'batch_size': 64, 'step_size': 9, 'gamma': 0.7581144311330318}. Best is trial 19 with value: 0.05992282344959676.[0m
[32m[I 2025-02-07 18:03:12,478][0m Trial 21 finished with value: 0.049326258450746535 and parameters: {'observation_period_num': 12, 'train_rates': 0.9748102547365318, 'learning_rate': 6.883704512968068e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.7514317017206497}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:04:45,334][0m Trial 22 finished with value: 0.05713777244091034 and parameters: {'observation_period_num': 10, 'train_rates': 0.9889692038504095, 'learning_rate': 6.596070822785329e-05, 'batch_size': 61, 'step_size': 12, 'gamma': 0.7912263879923997}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:06:42,276][0m Trial 23 finished with value: 0.04978286102414131 and parameters: {'observation_period_num': 5, 'train_rates': 0.9894890692089678, 'learning_rate': 8.235543413778832e-05, 'batch_size': 48, 'step_size': 12, 'gamma': 0.787186971883805}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:08:54,294][0m Trial 24 finished with value: 0.07995688490852525 and parameters: {'observation_period_num': 35, 'train_rates': 0.9730018992853953, 'learning_rate': 1.8762260862824696e-05, 'batch_size': 41, 'step_size': 12, 'gamma': 0.7898063583498056}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:10:18,313][0m Trial 25 finished with value: 0.05844639241695404 and parameters: {'observation_period_num': 32, 'train_rates': 0.9881050403892563, 'learning_rate': 8.948699628640131e-05, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8211974439781409}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:15:42,054][0m Trial 26 finished with value: 0.08790659058397099 and parameters: {'observation_period_num': 49, 'train_rates': 0.8988592329198253, 'learning_rate': 5.403005369020263e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7824752865742739}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:17:24,399][0m Trial 27 finished with value: 0.06424108351625148 and parameters: {'observation_period_num': 21, 'train_rates': 0.9000023835829376, 'learning_rate': 3.009399706972308e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.8098560216929753}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:18:38,233][0m Trial 28 finished with value: 0.0607761412858963 and parameters: {'observation_period_num': 49, 'train_rates': 0.9622045830608872, 'learning_rate': 0.00019633555438072585, 'batch_size': 77, 'step_size': 13, 'gamma': 0.7761306760415141}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:19:23,549][0m Trial 29 finished with value: 0.125741627588067 and parameters: {'observation_period_num': 27, 'train_rates': 0.8097459608156303, 'learning_rate': 1.4306940682691928e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.7886646612513246}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:21:59,781][0m Trial 30 finished with value: 0.07828266623740394 and parameters: {'observation_period_num': 6, 'train_rates': 0.9839490302291867, 'learning_rate': 4.206722945284625e-06, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8485630553138234}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:23:26,504][0m Trial 31 finished with value: 0.0534299408624831 and parameters: {'observation_period_num': 39, 'train_rates': 0.9742051759512961, 'learning_rate': 8.15304178224959e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.8228648317559488}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:25:19,425][0m Trial 32 finished with value: 0.062032858660416815 and parameters: {'observation_period_num': 40, 'train_rates': 0.954103567212445, 'learning_rate': 5.229021043581551e-05, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8227419963602342}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:26:34,250][0m Trial 33 finished with value: 0.16525645343075812 and parameters: {'observation_period_num': 21, 'train_rates': 0.6154818883397725, 'learning_rate': 0.00011152948955075602, 'batch_size': 55, 'step_size': 11, 'gamma': 0.7968917450981243}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:27:19,033][0m Trial 34 finished with value: 0.08246585147523405 and parameters: {'observation_period_num': 49, 'train_rates': 0.8966072913346234, 'learning_rate': 3.9487950819846776e-05, 'batch_size': 132, 'step_size': 15, 'gamma': 0.76968786356047}. Best is trial 21 with value: 0.049326258450746535.[0m
[32m[I 2025-02-07 18:33:27,691][0m Trial 35 finished with value: 0.04019157913627855 and parameters: {'observation_period_num': 7, 'train_rates': 0.9615333834404514, 'learning_rate': 2.2948415170247652e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8226612940524967}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:38:12,567][0m Trial 36 finished with value: 0.07925053412117154 and parameters: {'observation_period_num': 94, 'train_rates': 0.9422216189837924, 'learning_rate': 2.240109985330829e-05, 'batch_size': 20, 'step_size': 14, 'gamma': 0.8185574307512111}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:40:17,429][0m Trial 37 finished with value: 0.07588480477051068 and parameters: {'observation_period_num': 23, 'train_rates': 0.9049870359443418, 'learning_rate': 9.753492554536682e-06, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8676764024054227}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:44:07,779][0m Trial 38 finished with value: 0.06754523940946695 and parameters: {'observation_period_num': 41, 'train_rates': 0.9637137008770137, 'learning_rate': 2.4271882409155346e-05, 'batch_size': 26, 'step_size': 9, 'gamma': 0.8427155228452925}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:46:47,724][0m Trial 39 finished with value: 0.09363035572107656 and parameters: {'observation_period_num': 141, 'train_rates': 0.8770417477246375, 'learning_rate': 3.8125155324797497e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8096374752166673}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:47:30,102][0m Trial 40 finished with value: 0.08216832720248377 and parameters: {'observation_period_num': 62, 'train_rates': 0.9186161439080693, 'learning_rate': 8.48807472468204e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8590555551736099}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:49:06,896][0m Trial 41 finished with value: 0.04642858728766441 and parameters: {'observation_period_num': 5, 'train_rates': 0.9859067869500348, 'learning_rate': 7.064007681270132e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7939537324404293}. Best is trial 35 with value: 0.04019157913627855.[0m
[32m[I 2025-02-07 18:50:26,300][0m Trial 42 finished with value: 0.03990304308483399 and parameters: {'observation_period_num': 14, 'train_rates': 0.9720680171735195, 'learning_rate': 0.00019855729819191845, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8283412631315773}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:51:44,739][0m Trial 43 finished with value: 0.04247587431491036 and parameters: {'observation_period_num': 16, 'train_rates': 0.9672855437905807, 'learning_rate': 0.000189508088340342, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8349012713021128}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:53:01,949][0m Trial 44 finished with value: 0.05072547493107391 and parameters: {'observation_period_num': 21, 'train_rates': 0.9438523947495779, 'learning_rate': 0.00021305111021116215, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8412384914288585}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:53:57,574][0m Trial 45 finished with value: 0.04067041352391243 and parameters: {'observation_period_num': 16, 'train_rates': 0.9660600638904501, 'learning_rate': 0.0004481692828018429, 'batch_size': 111, 'step_size': 7, 'gamma': 0.8349597137033299}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:54:37,401][0m Trial 46 finished with value: 0.05876203529212786 and parameters: {'observation_period_num': 57, 'train_rates': 0.9369293399106722, 'learning_rate': 0.000493301999385895, 'batch_size': 157, 'step_size': 7, 'gamma': 0.832066291485423}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:55:33,398][0m Trial 47 finished with value: 0.12821342632697338 and parameters: {'observation_period_num': 89, 'train_rates': 0.9606238627783299, 'learning_rate': 0.0004906230190846846, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8747940402187144}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:56:41,432][0m Trial 48 finished with value: 0.0823884016803435 and parameters: {'observation_period_num': 108, 'train_rates': 0.8833318745851387, 'learning_rate': 0.00015919837430710933, 'batch_size': 83, 'step_size': 14, 'gamma': 0.8579563686832465}. Best is trial 42 with value: 0.03990304308483399.[0m
[32m[I 2025-02-07 18:57:31,265][0m Trial 49 finished with value: 0.05052585220512222 and parameters: {'observation_period_num': 28, 'train_rates': 0.837694054838261, 'learning_rate': 0.0002441489368866088, 'batch_size': 116, 'step_size': 6, 'gamma': 0.8328978205925722}. Best is trial 42 with value: 0.03990304308483399.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-07 18:57:31,276][0m A new study created in memory with name: no-name-2a139c92-fb27-400d-9b11-b8c880c4de6e[0m
[32m[I 2025-02-07 18:58:08,907][0m Trial 0 finished with value: 0.2085176177516386 and parameters: {'observation_period_num': 112, 'train_rates': 0.8918390687886598, 'learning_rate': 1.7885462658469786e-05, 'batch_size': 155, 'step_size': 15, 'gamma': 0.8494424992426481}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:00:07,031][0m Trial 1 finished with value: 0.6925755889334018 and parameters: {'observation_period_num': 139, 'train_rates': 0.7622109512833835, 'learning_rate': 1.4593060781156905e-06, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9385010145486715}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:00:50,494][0m Trial 2 finished with value: 0.40214837069134063 and parameters: {'observation_period_num': 237, 'train_rates': 0.8469718428007245, 'learning_rate': 1.56465749772412e-05, 'batch_size': 128, 'step_size': 6, 'gamma': 0.7769492044154533}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:01:16,499][0m Trial 3 finished with value: 0.3780124467605543 and parameters: {'observation_period_num': 194, 'train_rates': 0.626685230154696, 'learning_rate': 0.00012560268149388112, 'batch_size': 190, 'step_size': 13, 'gamma': 0.909215495026324}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:01:41,290][0m Trial 4 finished with value: 0.8663326512395808 and parameters: {'observation_period_num': 147, 'train_rates': 0.6786006704695585, 'learning_rate': 1.183014614216614e-06, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8987325158749927}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:02:08,495][0m Trial 5 finished with value: 1.851572018985492 and parameters: {'observation_period_num': 162, 'train_rates': 0.7074685920009843, 'learning_rate': 2.6857140218521206e-06, 'batch_size': 184, 'step_size': 5, 'gamma': 0.7896665789766363}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:02:40,876][0m Trial 6 finished with value: 0.3339189013817927 and parameters: {'observation_period_num': 117, 'train_rates': 0.8995350517024745, 'learning_rate': 1.1550727214730948e-05, 'batch_size': 184, 'step_size': 4, 'gamma': 0.9648138585312673}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:03:22,442][0m Trial 7 finished with value: 0.7468251324571248 and parameters: {'observation_period_num': 203, 'train_rates': 0.8908363499717857, 'learning_rate': 2.6596633707302246e-06, 'batch_size': 138, 'step_size': 8, 'gamma': 0.821776871615912}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:04:02,323][0m Trial 8 finished with value: 0.5464411104443334 and parameters: {'observation_period_num': 35, 'train_rates': 0.7844221031882276, 'learning_rate': 2.283504507931571e-06, 'batch_size': 141, 'step_size': 15, 'gamma': 0.9346127670667115}. Best is trial 0 with value: 0.2085176177516386.[0m
[32m[I 2025-02-07 19:04:39,420][0m Trial 9 finished with value: 0.5865387916564941 and parameters: {'observation_period_num': 81, 'train_rates': 0.9549224823798097, 'learning_rate': 4.914853685631164e-06, 'batch_size': 165, 'step_size': 11, 'gamma': 0.9319696008410889}. Best is trial 0 with value: 0.2085176177516386.[0m
Early stopping at epoch 87
[32m[I 2025-02-07 19:05:04,616][0m Trial 10 finished with value: 0.093851238489151 and parameters: {'observation_period_num': 14, 'train_rates': 0.9390889889535126, 'learning_rate': 0.0009761193987273753, 'batch_size': 240, 'step_size': 1, 'gamma': 0.8531388832680693}. Best is trial 10 with value: 0.093851238489151.[0m
Early stopping at epoch 87
[32m[I 2025-02-07 19:05:29,111][0m Trial 11 finished with value: 0.09313253313302994 and parameters: {'observation_period_num': 12, 'train_rates': 0.9740177037323862, 'learning_rate': 0.000513783617006396, 'batch_size': 253, 'step_size': 1, 'gamma': 0.8583762264088873}. Best is trial 11 with value: 0.09313253313302994.[0m
Early stopping at epoch 86
[32m[I 2025-02-07 19:05:53,352][0m Trial 12 finished with value: 0.09622744470834732 and parameters: {'observation_period_num': 10, 'train_rates': 0.9649989124236762, 'learning_rate': 0.0008560912141043339, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8547444985681037}. Best is trial 11 with value: 0.09313253313302994.[0m
Early stopping at epoch 69
[32m[I 2025-02-07 19:06:13,090][0m Trial 13 finished with value: 0.21229282021522522 and parameters: {'observation_period_num': 63, 'train_rates': 0.9826938080031757, 'learning_rate': 0.0007059504560429584, 'batch_size': 255, 'step_size': 1, 'gamma': 0.8199239950972799}. Best is trial 11 with value: 0.09313253313302994.[0m
[32m[I 2025-02-07 19:06:39,050][0m Trial 14 finished with value: 0.06141954037656537 and parameters: {'observation_period_num': 5, 'train_rates': 0.8435196294967852, 'learning_rate': 0.00017542611602812825, 'batch_size': 228, 'step_size': 3, 'gamma': 0.8763329840679884}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:07:05,381][0m Trial 15 finished with value: 0.10998574833645196 and parameters: {'observation_period_num': 54, 'train_rates': 0.832272744640942, 'learning_rate': 0.00012806226142689873, 'batch_size': 222, 'step_size': 3, 'gamma': 0.887271328377578}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:08:03,775][0m Trial 16 finished with value: 0.0844104511752015 and parameters: {'observation_period_num': 88, 'train_rates': 0.8247687853728896, 'learning_rate': 9.58016055211407e-05, 'batch_size': 92, 'step_size': 7, 'gamma': 0.7514150411307279}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:09:04,468][0m Trial 17 finished with value: 0.08370708947430154 and parameters: {'observation_period_num': 90, 'train_rates': 0.8264053873251413, 'learning_rate': 8.963987450193999e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.7575372391727433}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:10:00,446][0m Trial 18 finished with value: 0.19703877907614786 and parameters: {'observation_period_num': 47, 'train_rates': 0.7500793213585369, 'learning_rate': 5.13951355819366e-05, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8167820658777254}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:13:16,139][0m Trial 19 finished with value: 0.2343770831049741 and parameters: {'observation_period_num': 93, 'train_rates': 0.7178873082326865, 'learning_rate': 0.00025333333312033764, 'batch_size': 24, 'step_size': 12, 'gamma': 0.975063742636749}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:14:20,511][0m Trial 20 finished with value: 0.29825777535031484 and parameters: {'observation_period_num': 170, 'train_rates': 0.853267830300053, 'learning_rate': 4.536181997332125e-05, 'batch_size': 83, 'step_size': 3, 'gamma': 0.7537547348900638}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:15:16,656][0m Trial 21 finished with value: 0.0904786330995275 and parameters: {'observation_period_num': 90, 'train_rates': 0.8134607186018825, 'learning_rate': 0.0001027236346756508, 'batch_size': 94, 'step_size': 7, 'gamma': 0.7665777473986438}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:16:38,315][0m Trial 22 finished with value: 0.06508229753865856 and parameters: {'observation_period_num': 75, 'train_rates': 0.7959190493235422, 'learning_rate': 0.0002815674500625246, 'batch_size': 65, 'step_size': 8, 'gamma': 0.7992345128751173}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:18:04,463][0m Trial 23 finished with value: 0.17752856032502995 and parameters: {'observation_period_num': 36, 'train_rates': 0.774181033825541, 'learning_rate': 0.0003262068237519106, 'batch_size': 60, 'step_size': 9, 'gamma': 0.8033953996216867}. Best is trial 14 with value: 0.06141954037656537.[0m
[32m[I 2025-02-07 19:18:54,359][0m Trial 24 finished with value: 0.05644754569996432 and parameters: {'observation_period_num': 63, 'train_rates': 0.8640797956765971, 'learning_rate': 0.0002545964504923249, 'batch_size': 115, 'step_size': 7, 'gamma': 0.7882661037348834}. Best is trial 24 with value: 0.05644754569996432.[0m
[32m[I 2025-02-07 19:19:43,982][0m Trial 25 finished with value: 0.06352167776630968 and parameters: {'observation_period_num': 70, 'train_rates': 0.8647122160999238, 'learning_rate': 0.0003126429961888307, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8334830036526206}. Best is trial 24 with value: 0.05644754569996432.[0m
[32m[I 2025-02-07 19:20:36,484][0m Trial 26 finished with value: 0.05795579397103243 and parameters: {'observation_period_num': 33, 'train_rates': 0.9223466152546559, 'learning_rate': 0.00020195110078265015, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8374729488520012}. Best is trial 24 with value: 0.05644754569996432.[0m
[32m[I 2025-02-07 19:21:31,387][0m Trial 27 finished with value: 0.046634231795197566 and parameters: {'observation_period_num': 32, 'train_rates': 0.908936622979061, 'learning_rate': 0.00018980633965732269, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8793809879930106}. Best is trial 27 with value: 0.046634231795197566.[0m
[32m[I 2025-02-07 19:22:25,150][0m Trial 28 finished with value: 0.04627021824991381 and parameters: {'observation_period_num': 37, 'train_rates': 0.924305449489864, 'learning_rate': 0.0004946238489257053, 'batch_size': 114, 'step_size': 5, 'gamma': 0.8309795694816836}. Best is trial 28 with value: 0.04627021824991381.[0m
[32m[I 2025-02-07 19:23:17,018][0m Trial 29 finished with value: 0.07809882819790959 and parameters: {'observation_period_num': 108, 'train_rates': 0.88692919834033, 'learning_rate': 0.00043161720657041114, 'batch_size': 111, 'step_size': 5, 'gamma': 0.8797111404377334}. Best is trial 28 with value: 0.04627021824991381.[0m
[32m[I 2025-02-07 19:23:57,737][0m Trial 30 finished with value: 0.14905842442869616 and parameters: {'observation_period_num': 32, 'train_rates': 0.9225106105697782, 'learning_rate': 2.794066411398626e-05, 'batch_size': 157, 'step_size': 6, 'gamma': 0.7835888376015844}. Best is trial 28 with value: 0.04627021824991381.[0m
[32m[I 2025-02-07 19:24:50,370][0m Trial 31 finished with value: 0.05452399333967036 and parameters: {'observation_period_num': 29, 'train_rates': 0.932274167121684, 'learning_rate': 0.00016637708597173765, 'batch_size': 115, 'step_size': 4, 'gamma': 0.8369960596366196}. Best is trial 28 with value: 0.04627021824991381.[0m
[32m[I 2025-02-07 19:25:44,064][0m Trial 32 finished with value: 0.052282471801334184 and parameters: {'observation_period_num': 46, 'train_rates': 0.9200278960492344, 'learning_rate': 0.0004760361411274871, 'batch_size': 112, 'step_size': 4, 'gamma': 0.8385543358452163}. Best is trial 28 with value: 0.04627021824991381.[0m
[32m[I 2025-02-07 19:27:04,387][0m Trial 33 finished with value: 0.03660177316555043 and parameters: {'observation_period_num': 22, 'train_rates': 0.934157405521328, 'learning_rate': 0.0005319950687052422, 'batch_size': 76, 'step_size': 4, 'gamma': 0.84137585022799}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:28:34,893][0m Trial 34 finished with value: 0.06477156116203828 and parameters: {'observation_period_num': 50, 'train_rates': 0.9057240253869827, 'learning_rate': 0.0005195998396756485, 'batch_size': 63, 'step_size': 4, 'gamma': 0.8687036410708601}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:29:54,685][0m Trial 35 finished with value: 0.07135110643674743 and parameters: {'observation_period_num': 50, 'train_rates': 0.945556442662535, 'learning_rate': 0.0005411527423397836, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9005503540037916}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:33:27,564][0m Trial 36 finished with value: 0.041972194852168895 and parameters: {'observation_period_num': 23, 'train_rates': 0.876568769990442, 'learning_rate': 0.0006318241441355193, 'batch_size': 26, 'step_size': 2, 'gamma': 0.9167223558169735}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:38:57,210][0m Trial 37 finished with value: 0.040663503400290645 and parameters: {'observation_period_num': 19, 'train_rates': 0.897419475981067, 'learning_rate': 0.0006387076645971306, 'batch_size': 17, 'step_size': 2, 'gamma': 0.9177813673310907}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:44:12,326][0m Trial 38 finished with value: 0.19279870883352057 and parameters: {'observation_period_num': 248, 'train_rates': 0.8706669154607498, 'learning_rate': 0.0006767069863887268, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9533362964971938}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:46:45,845][0m Trial 39 finished with value: 0.11933155359966414 and parameters: {'observation_period_num': 18, 'train_rates': 0.8812140516189093, 'learning_rate': 9.377085179858918e-06, 'batch_size': 37, 'step_size': 2, 'gamma': 0.9199898840916215}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:48:26,207][0m Trial 40 finished with value: 0.12847656751401815 and parameters: {'observation_period_num': 22, 'train_rates': 0.612643872760084, 'learning_rate': 0.0009743471250639379, 'batch_size': 44, 'step_size': 2, 'gamma': 0.9072327718909253}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:50:59,644][0m Trial 41 finished with value: 0.049111320849573405 and parameters: {'observation_period_num': 22, 'train_rates': 0.9032010094800647, 'learning_rate': 0.0003755637562140396, 'batch_size': 37, 'step_size': 5, 'gamma': 0.9194381459298179}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:53:19,694][0m Trial 42 finished with value: 0.054488129913806915 and parameters: {'observation_period_num': 42, 'train_rates': 0.9886029906562632, 'learning_rate': 0.0006540390829764103, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9446550217204877}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:54:04,785][0m Trial 43 finished with value: 0.1472533792257309 and parameters: {'observation_period_num': 131, 'train_rates': 0.960784934497382, 'learning_rate': 0.0004021702427606732, 'batch_size': 134, 'step_size': 2, 'gamma': 0.890998756067288}. Best is trial 33 with value: 0.03660177316555043.[0m
[32m[I 2025-02-07 19:58:35,250][0m Trial 44 finished with value: 0.03228132039614355 and parameters: {'observation_period_num': 6, 'train_rates': 0.9095179867530985, 'learning_rate': 0.0006913460416495062, 'batch_size': 21, 'step_size': 4, 'gamma': 0.8692771530677997}. Best is trial 44 with value: 0.03228132039614355.[0m
[32m[I 2025-02-07 20:02:06,810][0m Trial 45 finished with value: 0.02602727969105427 and parameters: {'observation_period_num': 6, 'train_rates': 0.9472320088785293, 'learning_rate': 0.000763690906660396, 'batch_size': 28, 'step_size': 2, 'gamma': 0.8511177795638218}. Best is trial 45 with value: 0.02602727969105427.[0m
[32m[I 2025-02-07 20:06:12,586][0m Trial 46 finished with value: 0.02398744752418075 and parameters: {'observation_period_num': 7, 'train_rates': 0.9496020212887621, 'learning_rate': 0.0008022948731597576, 'batch_size': 24, 'step_size': 2, 'gamma': 0.8684135626316811}. Best is trial 46 with value: 0.02398744752418075.[0m
[32m[I 2025-02-07 20:08:04,260][0m Trial 47 finished with value: 0.03795507366598492 and parameters: {'observation_period_num': 16, 'train_rates': 0.9517854353044396, 'learning_rate': 0.0009948209527091353, 'batch_size': 53, 'step_size': 1, 'gamma': 0.8631112481646315}. Best is trial 46 with value: 0.02398744752418075.[0m
[32m[I 2025-02-07 20:09:55,493][0m Trial 48 finished with value: 0.036398780893306344 and parameters: {'observation_period_num': 7, 'train_rates': 0.9503854856453898, 'learning_rate': 0.0009821281709886805, 'batch_size': 53, 'step_size': 1, 'gamma': 0.8629872426845342}. Best is trial 46 with value: 0.02398744752418075.[0m
[32m[I 2025-02-07 20:13:13,851][0m Trial 49 finished with value: 0.03239020962931522 and parameters: {'observation_period_num': 10, 'train_rates': 0.9755805113569207, 'learning_rate': 0.0008096133749000242, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8485972604241248}. Best is trial 46 with value: 0.02398744752418075.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-07 20:13:13,861][0m A new study created in memory with name: no-name-11735dcb-2de2-4a94-a236-508d464d309a[0m
[32m[I 2025-02-07 20:15:34,622][0m Trial 0 finished with value: 0.06990142893500445 and parameters: {'observation_period_num': 90, 'train_rates': 0.9287372120074266, 'learning_rate': 6.8676980589323e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.8422966763014219}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:16:40,427][0m Trial 1 finished with value: 0.23543574702113973 and parameters: {'observation_period_num': 108, 'train_rates': 0.6060502855127217, 'learning_rate': 0.0001639373651867144, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8190617873522953}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:17:05,654][0m Trial 2 finished with value: 0.5994830749432246 and parameters: {'observation_period_num': 118, 'train_rates': 0.894467043114298, 'learning_rate': 6.262135084957137e-06, 'batch_size': 239, 'step_size': 12, 'gamma': 0.7668048450192159}. Best is trial 0 with value: 0.06990142893500445.[0m
Early stopping at epoch 77
[32m[I 2025-02-07 20:17:31,586][0m Trial 3 finished with value: 3.191958624575318 and parameters: {'observation_period_num': 194, 'train_rates': 0.7849026055942391, 'learning_rate': 1.4206809398112744e-06, 'batch_size': 164, 'step_size': 2, 'gamma': 0.7635299895327416}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:17:59,124][0m Trial 4 finished with value: 0.3356786248863516 and parameters: {'observation_period_num': 147, 'train_rates': 0.6708727888389217, 'learning_rate': 0.0005164113109519062, 'batch_size': 183, 'step_size': 4, 'gamma': 0.9833474829027978}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:18:45,133][0m Trial 5 finished with value: 0.10436013797253513 and parameters: {'observation_period_num': 7, 'train_rates': 0.8585318156805661, 'learning_rate': 9.839257026445427e-06, 'batch_size': 128, 'step_size': 10, 'gamma': 0.8932912374152596}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:19:14,876][0m Trial 6 finished with value: 0.15491002798080444 and parameters: {'observation_period_num': 110, 'train_rates': 0.9853568350470434, 'learning_rate': 0.00016249023691179208, 'batch_size': 225, 'step_size': 7, 'gamma': 0.7738122216268544}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:20:09,912][0m Trial 7 finished with value: 0.42080534808127695 and parameters: {'observation_period_num': 187, 'train_rates': 0.6801658701567062, 'learning_rate': 6.535577325157358e-05, 'batch_size': 82, 'step_size': 3, 'gamma': 0.8173096797190272}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:20:38,359][0m Trial 8 finished with value: 0.4258745054746496 and parameters: {'observation_period_num': 125, 'train_rates': 0.7956050834898059, 'learning_rate': 1.1459259674404194e-05, 'batch_size': 202, 'step_size': 4, 'gamma': 0.8965342512120238}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:22:57,487][0m Trial 9 finished with value: 0.28129303293473873 and parameters: {'observation_period_num': 165, 'train_rates': 0.7433342338552982, 'learning_rate': 3.9508136396455696e-05, 'batch_size': 34, 'step_size': 4, 'gamma': 0.8161587540542196}. Best is trial 0 with value: 0.06990142893500445.[0m
[32m[I 2025-02-07 20:28:26,965][0m Trial 10 finished with value: 0.027296701557934285 and parameters: {'observation_period_num': 36, 'train_rates': 0.9831691369485885, 'learning_rate': 0.0007394913217755428, 'batch_size': 18, 'step_size': 15, 'gamma': 0.937016314585997}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:31:52,529][0m Trial 11 finished with value: 0.04085538835663881 and parameters: {'observation_period_num': 45, 'train_rates': 0.9808464533240622, 'learning_rate': 0.0007877315207543366, 'batch_size': 29, 'step_size': 15, 'gamma': 0.965247669212061}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:37:39,582][0m Trial 12 finished with value: 0.042503084960407105 and parameters: {'observation_period_num': 27, 'train_rates': 0.9765352958168864, 'learning_rate': 0.0009718795170388047, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9656700823061738}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:38:36,746][0m Trial 13 finished with value: 0.1072546258892702 and parameters: {'observation_period_num': 57, 'train_rates': 0.9208577936185385, 'learning_rate': 0.0003720540818714399, 'batch_size': 107, 'step_size': 12, 'gamma': 0.9331883944644174}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:39:48,747][0m Trial 14 finished with value: 0.36257315344280666 and parameters: {'observation_period_num': 250, 'train_rates': 0.8505817233533814, 'learning_rate': 0.0008770809806545535, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9413176685046062}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:41:47,991][0m Trial 15 finished with value: 0.05837581491639428 and parameters: {'observation_period_num': 58, 'train_rates': 0.9516367425819068, 'learning_rate': 0.0002157776274168862, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9282430906082083}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:47:26,398][0m Trial 16 finished with value: 0.1346579584222968 and parameters: {'observation_period_num': 56, 'train_rates': 0.8719864825541875, 'learning_rate': 0.00037093838101345773, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9890297490472023}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:48:28,058][0m Trial 17 finished with value: 0.05324774235486984 and parameters: {'observation_period_num': 30, 'train_rates': 0.9885912432213789, 'learning_rate': 0.00011226567991425059, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9023766035239607}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:49:02,944][0m Trial 18 finished with value: 0.8563732112429744 and parameters: {'observation_period_num': 72, 'train_rates': 0.8053366013962191, 'learning_rate': 1.1619933637929008e-06, 'batch_size': 155, 'step_size': 13, 'gamma': 0.953609252212058}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:50:01,901][0m Trial 19 finished with value: 0.060931805837548 and parameters: {'observation_period_num': 7, 'train_rates': 0.9282153647354395, 'learning_rate': 2.4570755301803567e-05, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9138798718723344}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:51:50,333][0m Trial 20 finished with value: 0.14424658351474337 and parameters: {'observation_period_num': 84, 'train_rates': 0.906163191705215, 'learning_rate': 0.0005882558318559604, 'batch_size': 52, 'step_size': 11, 'gamma': 0.8694652252484324}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 20:57:12,331][0m Trial 21 finished with value: 0.03954339865595102 and parameters: {'observation_period_num': 31, 'train_rates': 0.9509471301487322, 'learning_rate': 0.0007953275421234223, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9621158681376224}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:00:34,236][0m Trial 22 finished with value: 0.06432128381124116 and parameters: {'observation_period_num': 37, 'train_rates': 0.9545978095277634, 'learning_rate': 0.00030459063226066286, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9646841635770927}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:02:17,872][0m Trial 23 finished with value: 0.07427938886814647 and parameters: {'observation_period_num': 40, 'train_rates': 0.9540838179778981, 'learning_rate': 0.0009215149676595447, 'batch_size': 57, 'step_size': 14, 'gamma': 0.9692272363455114}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:03:29,783][0m Trial 24 finished with value: 0.09985776406986423 and parameters: {'observation_period_num': 22, 'train_rates': 0.8855122177278201, 'learning_rate': 0.0005009150336018472, 'batch_size': 79, 'step_size': 13, 'gamma': 0.9412668114602168}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:06:19,911][0m Trial 25 finished with value: 0.11707814561076842 and parameters: {'observation_period_num': 82, 'train_rates': 0.8361121200131367, 'learning_rate': 0.00029376381415116624, 'batch_size': 31, 'step_size': 14, 'gamma': 0.9174250372704146}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:12:19,963][0m Trial 26 finished with value: 0.060321295660906944 and parameters: {'observation_period_num': 48, 'train_rates': 0.9528648838388548, 'learning_rate': 0.00011078789727828238, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8727066470591922}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:14:34,498][0m Trial 27 finished with value: 0.12073046714067459 and parameters: {'observation_period_num': 70, 'train_rates': 0.9880396773069244, 'learning_rate': 0.0006341686571821458, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9581071417836796}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:15:41,521][0m Trial 28 finished with value: 0.22586050852021175 and parameters: {'observation_period_num': 15, 'train_rates': 0.9318486569479905, 'learning_rate': 2.1100899202048505e-06, 'batch_size': 90, 'step_size': 11, 'gamma': 0.9751549473608001}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:16:26,624][0m Trial 29 finished with value: 0.10061825212505128 and parameters: {'observation_period_num': 100, 'train_rates': 0.9213411526075315, 'learning_rate': 7.770013608394357e-05, 'batch_size': 131, 'step_size': 14, 'gamma': 0.8819795213696866}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:18:46,100][0m Trial 30 finished with value: 0.08218626656331295 and parameters: {'observation_period_num': 74, 'train_rates': 0.8213455094202738, 'learning_rate': 0.00022796740357458722, 'batch_size': 38, 'step_size': 8, 'gamma': 0.9499155633916757}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:23:52,868][0m Trial 31 finished with value: 0.04397996836181345 and parameters: {'observation_period_num': 43, 'train_rates': 0.9702240458146744, 'learning_rate': 0.0009303131918678956, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9674855448378906}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:25:29,152][0m Trial 32 finished with value: 0.05353631656826212 and parameters: {'observation_period_num': 25, 'train_rates': 0.9698909590177096, 'learning_rate': 0.000652997114533423, 'batch_size': 63, 'step_size': 15, 'gamma': 0.9882961411990947}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:27:55,628][0m Trial 33 finished with value: 0.058802768574319646 and parameters: {'observation_period_num': 30, 'train_rates': 0.9075362148187041, 'learning_rate': 0.0004293730086959067, 'batch_size': 39, 'step_size': 13, 'gamma': 0.8515601382592591}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:31:16,571][0m Trial 34 finished with value: 0.12222726680338383 and parameters: {'observation_period_num': 100, 'train_rates': 0.9442185845134177, 'learning_rate': 0.0009787539111555893, 'batch_size': 28, 'step_size': 14, 'gamma': 0.9211952285825471}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:32:47,434][0m Trial 35 finished with value: 0.16074357898730152 and parameters: {'observation_period_num': 17, 'train_rates': 0.7478628560729763, 'learning_rate': 0.00021081213157497788, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9427501756940129}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:38:40,719][0m Trial 36 finished with value: 0.08965943553825705 and parameters: {'observation_period_num': 143, 'train_rates': 0.9709561179505318, 'learning_rate': 0.0006609684456327341, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9755537881605905}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:39:45,768][0m Trial 37 finished with value: 0.224920227568903 and parameters: {'observation_period_num': 5, 'train_rates': 0.6090049209571138, 'learning_rate': 3.1194489772400646e-06, 'batch_size': 68, 'step_size': 14, 'gamma': 0.9542027916244726}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:41:54,453][0m Trial 38 finished with value: 0.19263524164038875 and parameters: {'observation_period_num': 247, 'train_rates': 0.8979799933568574, 'learning_rate': 0.00042461250702752944, 'batch_size': 41, 'step_size': 6, 'gamma': 0.9081815083768505}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:42:32,722][0m Trial 39 finished with value: 0.05626843501250822 and parameters: {'observation_period_num': 53, 'train_rates': 0.8749155619880206, 'learning_rate': 0.0001449524013821912, 'batch_size': 152, 'step_size': 11, 'gamma': 0.9284453309520447}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:43:05,401][0m Trial 40 finished with value: 0.05466979369521141 and parameters: {'observation_period_num': 34, 'train_rates': 0.9704174827916451, 'learning_rate': 0.00028519560028745735, 'batch_size': 192, 'step_size': 13, 'gamma': 0.9792972898333082}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:46:51,686][0m Trial 41 finished with value: 0.04166345942167588 and parameters: {'observation_period_num': 45, 'train_rates': 0.9723860231366181, 'learning_rate': 0.0009820612223143078, 'batch_size': 26, 'step_size': 15, 'gamma': 0.9640989780540753}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:50:39,969][0m Trial 42 finished with value: 0.08891645030063741 and parameters: {'observation_period_num': 44, 'train_rates': 0.9884314619516865, 'learning_rate': 0.0007361570036357035, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9634392741410627}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:51:07,757][0m Trial 43 finished with value: 0.0775008574128151 and parameters: {'observation_period_num': 69, 'train_rates': 0.937422211135802, 'learning_rate': 0.0005139794847555392, 'batch_size': 241, 'step_size': 15, 'gamma': 0.7886022029007953}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:53:17,609][0m Trial 44 finished with value: 0.09527923349364131 and parameters: {'observation_period_num': 61, 'train_rates': 0.9649625104504508, 'learning_rate': 0.0007278094769143049, 'batch_size': 46, 'step_size': 14, 'gamma': 0.9424866804148638}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:56:48,742][0m Trial 45 finished with value: 0.04766205880835122 and parameters: {'observation_period_num': 17, 'train_rates': 0.9141488143878521, 'learning_rate': 1.5499802094748257e-05, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9791642404064623}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:57:39,302][0m Trial 46 finished with value: 0.137113986604185 and parameters: {'observation_period_num': 93, 'train_rates': 0.9422309618729069, 'learning_rate': 0.0009541989595383285, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9488975192986436}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:58:08,386][0m Trial 47 finished with value: 0.14680266380310059 and parameters: {'observation_period_num': 196, 'train_rates': 0.9727051733046322, 'learning_rate': 0.00034861380616452766, 'batch_size': 213, 'step_size': 13, 'gamma': 0.750033282157377}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 21:59:13,827][0m Trial 48 finished with value: 0.2611770451945417 and parameters: {'observation_period_num': 112, 'train_rates': 0.7612858222315049, 'learning_rate': 0.0004737498196396283, 'batch_size': 76, 'step_size': 15, 'gamma': 0.9333322527445137}. Best is trial 10 with value: 0.027296701557934285.[0m
[32m[I 2025-02-07 22:00:33,730][0m Trial 49 finished with value: 0.17253157735779304 and parameters: {'observation_period_num': 60, 'train_rates': 0.7058311455159735, 'learning_rate': 0.00024690215075409043, 'batch_size': 60, 'step_size': 6, 'gamma': 0.829912672548597}. Best is trial 10 with value: 0.027296701557934285.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-07 22:00:33,741][0m A new study created in memory with name: no-name-e0ec6d91-d835-456e-b0e9-aecdbb9918ce[0m
[32m[I 2025-02-07 22:01:21,941][0m Trial 0 finished with value: 0.13947766912014134 and parameters: {'observation_period_num': 130, 'train_rates': 0.8627003963437838, 'learning_rate': 2.0549144224359898e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.884670798685351}. Best is trial 0 with value: 0.13947766912014134.[0m
[32m[I 2025-02-07 22:01:53,814][0m Trial 1 finished with value: 0.5490091136324475 and parameters: {'observation_period_num': 214, 'train_rates': 0.6888818219798274, 'learning_rate': 1.651797947864959e-05, 'batch_size': 149, 'step_size': 8, 'gamma': 0.7787037697114578}. Best is trial 0 with value: 0.13947766912014134.[0m
[32m[I 2025-02-07 22:02:29,039][0m Trial 2 finished with value: 0.38921524581596323 and parameters: {'observation_period_num': 85, 'train_rates': 0.6157374359661798, 'learning_rate': 4.6461775044447106e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9576465461336856}. Best is trial 0 with value: 0.13947766912014134.[0m
[32m[I 2025-02-07 22:03:11,905][0m Trial 3 finished with value: 0.6578030195287479 and parameters: {'observation_period_num': 240, 'train_rates': 0.7950133235491817, 'learning_rate': 1.3425324445152012e-06, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8986265216654368}. Best is trial 0 with value: 0.13947766912014134.[0m
[32m[I 2025-02-07 22:03:53,343][0m Trial 4 finished with value: 0.5317168831825256 and parameters: {'observation_period_num': 119, 'train_rates': 0.9530162645969238, 'learning_rate': 5.2457173550943374e-06, 'batch_size': 143, 'step_size': 9, 'gamma': 0.9172106067282892}. Best is trial 0 with value: 0.13947766912014134.[0m
[32m[I 2025-02-07 22:04:22,539][0m Trial 5 finished with value: 0.1275355327460501 and parameters: {'observation_period_num': 189, 'train_rates': 0.9025293101283258, 'learning_rate': 0.0005263521954189888, 'batch_size': 195, 'step_size': 11, 'gamma': 0.9059000575160514}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:05:09,334][0m Trial 6 finished with value: 0.3970795598329674 and parameters: {'observation_period_num': 222, 'train_rates': 0.9302380198325444, 'learning_rate': 1.8165621255821238e-05, 'batch_size': 126, 'step_size': 5, 'gamma': 0.8824867967095886}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:05:42,889][0m Trial 7 finished with value: 1.0672865957021713 and parameters: {'observation_period_num': 237, 'train_rates': 0.9354012128411897, 'learning_rate': 2.4661914511610614e-06, 'batch_size': 174, 'step_size': 6, 'gamma': 0.8067556629200187}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:06:07,387][0m Trial 8 finished with value: 0.30746201674143475 and parameters: {'observation_period_num': 240, 'train_rates': 0.7211135098979968, 'learning_rate': 0.00013042734711004732, 'batch_size': 209, 'step_size': 15, 'gamma': 0.8711234134851636}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:06:52,537][0m Trial 9 finished with value: 0.23154950254841855 and parameters: {'observation_period_num': 171, 'train_rates': 0.8296647389556577, 'learning_rate': 1.9035521277750213e-05, 'batch_size': 117, 'step_size': 5, 'gamma': 0.9815336363492767}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:08:58,752][0m Trial 10 finished with value: 0.1470612053181872 and parameters: {'observation_period_num': 164, 'train_rates': 0.8720792817895808, 'learning_rate': 0.0008879559312436578, 'batch_size': 42, 'step_size': 1, 'gamma': 0.821374711242812}. Best is trial 5 with value: 0.1275355327460501.[0m
[32m[I 2025-02-07 22:09:24,139][0m Trial 11 finished with value: 0.03920913770950089 and parameters: {'observation_period_num': 14, 'train_rates': 0.8699356672486097, 'learning_rate': 0.0004347990293136807, 'batch_size': 244, 'step_size': 14, 'gamma': 0.9306545472943095}. Best is trial 11 with value: 0.03920913770950089.[0m
[32m[I 2025-02-07 22:09:49,580][0m Trial 12 finished with value: 0.04621725464180396 and parameters: {'observation_period_num': 14, 'train_rates': 0.8935731110061853, 'learning_rate': 0.000994498650312888, 'batch_size': 255, 'step_size': 12, 'gamma': 0.9329758354938729}. Best is trial 11 with value: 0.03920913770950089.[0m
[32m[I 2025-02-07 22:10:16,974][0m Trial 13 finished with value: 0.03901101276278496 and parameters: {'observation_period_num': 13, 'train_rates': 0.9797337553742376, 'learning_rate': 0.00024406319381097653, 'batch_size': 255, 'step_size': 12, 'gamma': 0.9419616610042537}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:10:44,271][0m Trial 14 finished with value: 0.06466855108737946 and parameters: {'observation_period_num': 12, 'train_rates': 0.9845081257994294, 'learning_rate': 0.0001950073972602409, 'batch_size': 253, 'step_size': 12, 'gamma': 0.9551098176860884}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:11:08,668][0m Trial 15 finished with value: 0.18638434773756932 and parameters: {'observation_period_num': 50, 'train_rates': 0.7489485488585631, 'learning_rate': 0.00022200560759637813, 'batch_size': 220, 'step_size': 14, 'gamma': 0.8509410133290091}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:12:21,604][0m Trial 16 finished with value: 0.053447962433099745 and parameters: {'observation_period_num': 58, 'train_rates': 0.8277844257195258, 'learning_rate': 7.131105776739748e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.9428476797926056}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:12:50,902][0m Trial 17 finished with value: 0.0834735631942749 and parameters: {'observation_period_num': 46, 'train_rates': 0.9844529482490466, 'learning_rate': 0.00044315278565322064, 'batch_size': 233, 'step_size': 13, 'gamma': 0.9844312755852359}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:13:24,009][0m Trial 18 finished with value: 0.09834200103540679 and parameters: {'observation_period_num': 93, 'train_rates': 0.8192813036532005, 'learning_rate': 8.901662502834701e-05, 'batch_size': 180, 'step_size': 15, 'gamma': 0.9285928629978122}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:13:49,151][0m Trial 19 finished with value: 0.15840281484111043 and parameters: {'observation_period_num': 7, 'train_rates': 0.7478120506256162, 'learning_rate': 0.00034102104295161964, 'batch_size': 239, 'step_size': 10, 'gamma': 0.967287445408379}. Best is trial 13 with value: 0.03901101276278496.[0m
Early stopping at epoch 81
[32m[I 2025-02-07 22:14:13,914][0m Trial 20 finished with value: 0.18416704053289434 and parameters: {'observation_period_num': 34, 'train_rates': 0.9089180677382632, 'learning_rate': 0.0002128681208936359, 'batch_size': 210, 'step_size': 1, 'gamma': 0.8481746064657061}. Best is trial 13 with value: 0.03901101276278496.[0m
[32m[I 2025-02-07 22:14:40,383][0m Trial 21 finished with value: 0.034655753833552204 and parameters: {'observation_period_num': 19, 'train_rates': 0.8778111852501564, 'learning_rate': 0.0009568286359483312, 'batch_size': 255, 'step_size': 12, 'gamma': 0.9296920789071256}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:15:06,220][0m Trial 22 finished with value: 0.08808664018822429 and parameters: {'observation_period_num': 74, 'train_rates': 0.8610142501049391, 'learning_rate': 0.0005735756587836952, 'batch_size': 232, 'step_size': 13, 'gamma': 0.9198677369308332}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:15:33,821][0m Trial 23 finished with value: 0.10915955901145935 and parameters: {'observation_period_num': 31, 'train_rates': 0.9553540771579031, 'learning_rate': 0.0003172216136635195, 'batch_size': 254, 'step_size': 11, 'gamma': 0.9545358772836369}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:16:02,136][0m Trial 24 finished with value: 0.18132704571251737 and parameters: {'observation_period_num': 28, 'train_rates': 0.7898120841983175, 'learning_rate': 0.0006653260457481061, 'batch_size': 199, 'step_size': 13, 'gamma': 0.943277623246849}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:16:35,706][0m Trial 25 finished with value: 0.06370270834103098 and parameters: {'observation_period_num': 68, 'train_rates': 0.8800200938673644, 'learning_rate': 0.00012782290468687574, 'batch_size': 174, 'step_size': 11, 'gamma': 0.9001705035954595}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:17:03,307][0m Trial 26 finished with value: 0.10007590800523758 and parameters: {'observation_period_num': 118, 'train_rates': 0.9240364588876496, 'learning_rate': 0.00029111210161674654, 'batch_size': 228, 'step_size': 15, 'gamma': 0.7593309561274632}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:18:05,958][0m Trial 27 finished with value: 0.04416432514186674 and parameters: {'observation_period_num': 27, 'train_rates': 0.847833542419027, 'learning_rate': 0.0001324892026923703, 'batch_size': 89, 'step_size': 12, 'gamma': 0.9715445041992059}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:22:15,481][0m Trial 28 finished with value: 0.16255039912610014 and parameters: {'observation_period_num': 96, 'train_rates': 0.957994566270186, 'learning_rate': 8.341409666544737e-06, 'batch_size': 23, 'step_size': 9, 'gamma': 0.9173236783473077}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:22:40,123][0m Trial 29 finished with value: 0.06531070814644163 and parameters: {'observation_period_num': 6, 'train_rates': 0.7996864141413162, 'learning_rate': 3.691424533799068e-05, 'batch_size': 238, 'step_size': 13, 'gamma': 0.8862829530993707}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:23:10,243][0m Trial 30 finished with value: 0.1476053262888686 and parameters: {'observation_period_num': 42, 'train_rates': 0.6494442426724146, 'learning_rate': 0.0006647702705531587, 'batch_size': 159, 'step_size': 14, 'gamma': 0.8492437677496188}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:24:13,211][0m Trial 31 finished with value: 0.05403705518175331 and parameters: {'observation_period_num': 25, 'train_rates': 0.8527909814282446, 'learning_rate': 0.0001465514440923544, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9715466354873338}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:25:23,654][0m Trial 32 finished with value: 0.05455621978153988 and parameters: {'observation_period_num': 61, 'train_rates': 0.8480965356391766, 'learning_rate': 8.812608665153408e-05, 'batch_size': 78, 'step_size': 14, 'gamma': 0.9370110645799572}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:26:18,447][0m Trial 33 finished with value: 0.1763582860444712 and parameters: {'observation_period_num': 23, 'train_rates': 0.7688744848106653, 'learning_rate': 0.0003603305378874703, 'batch_size': 95, 'step_size': 10, 'gamma': 0.9741197343013746}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:27:14,661][0m Trial 34 finished with value: 0.05836421157407567 and parameters: {'observation_period_num': 44, 'train_rates': 0.8735014328992979, 'learning_rate': 7.28539670317354e-05, 'batch_size': 103, 'step_size': 12, 'gamma': 0.9596140367023503}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:28:47,762][0m Trial 35 finished with value: 0.0649874022819029 and parameters: {'observation_period_num': 77, 'train_rates': 0.8935729884680622, 'learning_rate': 4.5596043265926454e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.948247957511235}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:29:13,736][0m Trial 36 finished with value: 0.03841459735365116 and parameters: {'observation_period_num': 19, 'train_rates': 0.8476125706956541, 'learning_rate': 0.00042413338488297937, 'batch_size': 245, 'step_size': 14, 'gamma': 0.9860207167311861}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:29:40,164][0m Trial 37 finished with value: 0.205615428546744 and parameters: {'observation_period_num': 145, 'train_rates': 0.9121500585750174, 'learning_rate': 0.0007311516998322894, 'batch_size': 245, 'step_size': 14, 'gamma': 0.9231043431565299}. Best is trial 21 with value: 0.034655753833552204.[0m
[32m[I 2025-02-07 22:30:06,419][0m Trial 38 finished with value: 0.03453071880765592 and parameters: {'observation_period_num': 17, 'train_rates': 0.8022548914549998, 'learning_rate': 0.00048664304242939846, 'batch_size': 223, 'step_size': 15, 'gamma': 0.9035859332931515}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:30:30,584][0m Trial 39 finished with value: 0.18352552322381435 and parameters: {'observation_period_num': 103, 'train_rates': 0.6802008510479087, 'learning_rate': 0.0009508514231046851, 'batch_size': 221, 'step_size': 15, 'gamma': 0.8947472298983111}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:30:58,999][0m Trial 40 finished with value: 1.2343162838210406 and parameters: {'observation_period_num': 54, 'train_rates': 0.7986616129378069, 'learning_rate': 1.0384764560866598e-06, 'batch_size': 200, 'step_size': 7, 'gamma': 0.9099897424703818}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:31:27,369][0m Trial 41 finished with value: 0.03803897406096044 and parameters: {'observation_period_num': 15, 'train_rates': 0.828180306571302, 'learning_rate': 0.00045055847326145156, 'batch_size': 218, 'step_size': 15, 'gamma': 0.9890503418252242}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:31:56,105][0m Trial 42 finished with value: 0.05108614574364318 and parameters: {'observation_period_num': 39, 'train_rates': 0.818901126397287, 'learning_rate': 0.0004850304870050037, 'batch_size': 211, 'step_size': 15, 'gamma': 0.9825889614635542}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:32:22,249][0m Trial 43 finished with value: 0.167102394778406 and parameters: {'observation_period_num': 17, 'train_rates': 0.776353566094528, 'learning_rate': 0.0002587607627090866, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9897867843662055}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:32:51,478][0m Trial 44 finished with value: 0.152851720569198 and parameters: {'observation_period_num': 5, 'train_rates': 0.7175869355637897, 'learning_rate': 0.00048300147931501866, 'batch_size': 185, 'step_size': 3, 'gamma': 0.866751577730855}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:33:15,642][0m Trial 45 finished with value: 0.0502862168100326 and parameters: {'observation_period_num': 33, 'train_rates': 0.837552619922257, 'learning_rate': 0.0006158131680243853, 'batch_size': 247, 'step_size': 15, 'gamma': 0.9622269964150055}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:33:58,211][0m Trial 46 finished with value: 0.13290494831941896 and parameters: {'observation_period_num': 20, 'train_rates': 0.9400891959568423, 'learning_rate': 1.274202595843895e-05, 'batch_size': 153, 'step_size': 14, 'gamma': 0.9079049190630377}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:34:23,122][0m Trial 47 finished with value: 0.3014541834251571 and parameters: {'observation_period_num': 200, 'train_rates': 0.7626248159963789, 'learning_rate': 0.00017995046924679708, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8750267005629622}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:34:54,157][0m Trial 48 finished with value: 0.07377763013170514 and parameters: {'observation_period_num': 64, 'train_rates': 0.8087632722464555, 'learning_rate': 0.0008011860037325315, 'batch_size': 191, 'step_size': 11, 'gamma': 0.9489804326896062}. Best is trial 38 with value: 0.03453071880765592.[0m
[32m[I 2025-02-07 22:35:19,991][0m Trial 49 finished with value: 0.46528618697411506 and parameters: {'observation_period_num': 18, 'train_rates': 0.8904900451964419, 'learning_rate': 3.2981175319062935e-06, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9785647917998166}. Best is trial 38 with value: 0.03453071880765592.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-07 22:35:20,002][0m A new study created in memory with name: no-name-0f52a7ee-3940-4087-bcb6-3ad66cb96209[0m
[32m[I 2025-02-07 22:36:06,297][0m Trial 0 finished with value: 0.45157001536301894 and parameters: {'observation_period_num': 150, 'train_rates': 0.8239865968353861, 'learning_rate': 2.4612743154133484e-06, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9246425671701408}. Best is trial 0 with value: 0.45157001536301894.[0m
[32m[I 2025-02-07 22:36:58,391][0m Trial 1 finished with value: 0.4044109401423172 and parameters: {'observation_period_num': 227, 'train_rates': 0.8034730378659892, 'learning_rate': 5.356422255910463e-06, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8508702680967808}. Best is trial 1 with value: 0.4044109401423172.[0m
[32m[I 2025-02-07 22:37:36,967][0m Trial 2 finished with value: 0.2757593370772697 and parameters: {'observation_period_num': 12, 'train_rates': 0.8746770501288184, 'learning_rate': 1.2509402976294051e-06, 'batch_size': 156, 'step_size': 14, 'gamma': 0.9373403729015961}. Best is trial 2 with value: 0.2757593370772697.[0m
[32m[I 2025-02-07 22:39:55,805][0m Trial 3 finished with value: 0.15430901905581837 and parameters: {'observation_period_num': 144, 'train_rates': 0.8239745156811651, 'learning_rate': 1.5839036469215745e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8003036981926319}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:40:51,076][0m Trial 4 finished with value: 0.2654946447633531 and parameters: {'observation_period_num': 166, 'train_rates': 0.6707846704542952, 'learning_rate': 0.00021679045432565977, 'batch_size': 85, 'step_size': 14, 'gamma': 0.9366936428943364}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:41:22,393][0m Trial 5 finished with value: 0.41632959246635437 and parameters: {'observation_period_num': 181, 'train_rates': 0.9774157872387865, 'learning_rate': 5.263734715970001e-06, 'batch_size': 208, 'step_size': 10, 'gamma': 0.9844790458436851}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:41:52,071][0m Trial 6 finished with value: 1.4055510681461205 and parameters: {'observation_period_num': 78, 'train_rates': 0.6947195421874253, 'learning_rate': 1.0756564576320918e-06, 'batch_size': 175, 'step_size': 13, 'gamma': 0.7570170435976902}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:42:15,382][0m Trial 7 finished with value: 1.1619880352159324 and parameters: {'observation_period_num': 46, 'train_rates': 0.6145932391524174, 'learning_rate': 8.020596968599656e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.9341938090556576}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:44:04,015][0m Trial 8 finished with value: 0.3139951924748599 and parameters: {'observation_period_num': 243, 'train_rates': 0.7247433342174027, 'learning_rate': 0.0006342436945224321, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9776764827825026}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:46:09,117][0m Trial 9 finished with value: 0.5381750278174877 and parameters: {'observation_period_num': 204, 'train_rates': 0.654525961043372, 'learning_rate': 3.7557163327955696e-06, 'batch_size': 34, 'step_size': 3, 'gamma': 0.908992698135631}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:46:34,521][0m Trial 10 finished with value: 0.4697991907596588 and parameters: {'observation_period_num': 112, 'train_rates': 0.9190280021102879, 'learning_rate': 2.1221018415340544e-05, 'batch_size': 250, 'step_size': 5, 'gamma': 0.7898883866420761}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:47:40,452][0m Trial 11 finished with value: 0.22376819080428073 and parameters: {'observation_period_num': 146, 'train_rates': 0.7300372936834651, 'learning_rate': 0.00016417644211888063, 'batch_size': 74, 'step_size': 7, 'gamma': 0.8372526081186802}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:48:55,123][0m Trial 12 finished with value: 0.3236679849671383 and parameters: {'observation_period_num': 114, 'train_rates': 0.7312533529211795, 'learning_rate': 2.023054045419204e-05, 'batch_size': 65, 'step_size': 7, 'gamma': 0.83407703878025}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:53:39,683][0m Trial 13 finished with value: 0.24034398441251956 and parameters: {'observation_period_num': 133, 'train_rates': 0.7717180349425642, 'learning_rate': 7.861709436110602e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.822905691892641}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:55:04,944][0m Trial 14 finished with value: 0.156580128526219 and parameters: {'observation_period_num': 87, 'train_rates': 0.8585590082810146, 'learning_rate': 6.630698395746902e-05, 'batch_size': 64, 'step_size': 2, 'gamma': 0.7968505328138638}. Best is trial 3 with value: 0.15430901905581837.[0m
Early stopping at epoch 51
[32m[I 2025-02-07 22:55:29,310][0m Trial 15 finished with value: 0.6133418083190918 and parameters: {'observation_period_num': 86, 'train_rates': 0.8717563478724076, 'learning_rate': 1.5536990119372102e-05, 'batch_size': 123, 'step_size': 1, 'gamma': 0.7939526012440931}. Best is trial 3 with value: 0.15430901905581837.[0m
Early stopping at epoch 52
[32m[I 2025-02-07 22:56:28,705][0m Trial 16 finished with value: 0.26992643949516276 and parameters: {'observation_period_num': 75, 'train_rates': 0.8698935512284272, 'learning_rate': 6.434573741368294e-05, 'batch_size': 49, 'step_size': 1, 'gamma': 0.756295948217955}. Best is trial 3 with value: 0.15430901905581837.[0m
[32m[I 2025-02-07 22:57:24,694][0m Trial 17 finished with value: 0.04680039287091066 and parameters: {'observation_period_num': 41, 'train_rates': 0.9466953800809397, 'learning_rate': 0.0007149539066033908, 'batch_size': 107, 'step_size': 4, 'gamma': 0.8880051443610105}. Best is trial 17 with value: 0.04680039287091066.[0m
[32m[I 2025-02-07 22:58:24,161][0m Trial 18 finished with value: 0.029372701421380043 and parameters: {'observation_period_num': 7, 'train_rates': 0.9882060110018639, 'learning_rate': 0.0008887268356908011, 'batch_size': 112, 'step_size': 4, 'gamma': 0.8811588807012298}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 22:59:09,770][0m Trial 19 finished with value: 0.03598674386739731 and parameters: {'observation_period_num': 5, 'train_rates': 0.9819291033687609, 'learning_rate': 0.0007615586813568122, 'batch_size': 142, 'step_size': 4, 'gamma': 0.8844571948766393}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 22:59:53,160][0m Trial 20 finished with value: 0.040346261113882065 and parameters: {'observation_period_num': 6, 'train_rates': 0.9814723492425844, 'learning_rate': 0.0003417837216859212, 'batch_size': 152, 'step_size': 5, 'gamma': 0.8671494441597718}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:00:37,348][0m Trial 21 finished with value: 0.0414714440703392 and parameters: {'observation_period_num': 12, 'train_rates': 0.9845208364601776, 'learning_rate': 0.0003842047481086798, 'batch_size': 151, 'step_size': 5, 'gamma': 0.8683928766789306}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:01:21,583][0m Trial 22 finished with value: 0.04685318655365907 and parameters: {'observation_period_num': 34, 'train_rates': 0.9294381609180193, 'learning_rate': 0.0009537493425472362, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8843367026639876}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:01:56,934][0m Trial 23 finished with value: 0.04559575766324997 and parameters: {'observation_period_num': 6, 'train_rates': 0.9528714926132007, 'learning_rate': 0.0003459762666592755, 'batch_size': 178, 'step_size': 3, 'gamma': 0.8972714499183202}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:02:32,722][0m Trial 24 finished with value: 0.0724474925812817 and parameters: {'observation_period_num': 54, 'train_rates': 0.9177524036357265, 'learning_rate': 0.0004171662603760973, 'batch_size': 168, 'step_size': 4, 'gamma': 0.8582033615957868}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:03:05,777][0m Trial 25 finished with value: 0.05607489123940468 and parameters: {'observation_period_num': 26, 'train_rates': 0.9861051368818584, 'learning_rate': 0.00018241678846645423, 'batch_size': 202, 'step_size': 6, 'gamma': 0.8731441107396842}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:03:51,987][0m Trial 26 finished with value: 0.06678616298168195 and parameters: {'observation_period_num': 58, 'train_rates': 0.8978887641133615, 'learning_rate': 0.000998633687379679, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9110882328613913}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:04:51,781][0m Trial 27 finished with value: 0.041133051091391176 and parameters: {'observation_period_num': 22, 'train_rates': 0.951026470683239, 'learning_rate': 0.00026870960149955476, 'batch_size': 103, 'step_size': 8, 'gamma': 0.9643434170186032}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:05:33,286][0m Trial 28 finished with value: 0.0738903060555458 and parameters: {'observation_period_num': 63, 'train_rates': 0.9612730062879391, 'learning_rate': 0.0005010309400778567, 'batch_size': 147, 'step_size': 4, 'gamma': 0.8664711069509734}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:06:24,427][0m Trial 29 finished with value: 0.0670000428691605 and parameters: {'observation_period_num': 29, 'train_rates': 0.8970863881708175, 'learning_rate': 0.00013144397133926103, 'batch_size': 119, 'step_size': 2, 'gamma': 0.9161976457158233}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:06:54,769][0m Trial 30 finished with value: 0.06512143598923545 and parameters: {'observation_period_num': 10, 'train_rates': 0.8366303342082593, 'learning_rate': 0.00012917534387061675, 'batch_size': 192, 'step_size': 6, 'gamma': 0.8177962796332618}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:07:50,480][0m Trial 31 finished with value: 0.047383133947414204 and parameters: {'observation_period_num': 25, 'train_rates': 0.9535229904441744, 'learning_rate': 0.0002725330235300401, 'batch_size': 108, 'step_size': 9, 'gamma': 0.9453677256166764}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:08:58,044][0m Trial 32 finished with value: 0.07886599749326706 and parameters: {'observation_period_num': 23, 'train_rates': 0.9871521719738755, 'learning_rate': 0.0006529029536247702, 'batch_size': 91, 'step_size': 6, 'gamma': 0.9573622065195164}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:09:53,762][0m Trial 33 finished with value: 0.044414827928823584 and parameters: {'observation_period_num': 7, 'train_rates': 0.9369376543197546, 'learning_rate': 0.0002853314218397987, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9639045365461847}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:10:38,183][0m Trial 34 finished with value: 0.04593497489889463 and parameters: {'observation_period_num': 35, 'train_rates': 0.9025704500931135, 'learning_rate': 0.000530295037578185, 'batch_size': 133, 'step_size': 9, 'gamma': 0.8429403049796274}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:11:19,433][0m Trial 35 finished with value: 0.09366416186094284 and parameters: {'observation_period_num': 19, 'train_rates': 0.96850945433344, 'learning_rate': 3.634860230525755e-05, 'batch_size': 158, 'step_size': 5, 'gamma': 0.8936223928587795}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:12:19,388][0m Trial 36 finished with value: 0.18640237710284954 and parameters: {'observation_period_num': 46, 'train_rates': 0.7879474690820307, 'learning_rate': 0.000981908818284758, 'batch_size': 90, 'step_size': 2, 'gamma': 0.9247858359392641}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:12:46,464][0m Trial 37 finished with value: 0.09797503054141998 and parameters: {'observation_period_num': 63, 'train_rates': 0.9344446403866202, 'learning_rate': 0.00023392165866764267, 'batch_size': 231, 'step_size': 4, 'gamma': 0.8520802907660077}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:13:25,498][0m Trial 38 finished with value: 0.03802374005317688 and parameters: {'observation_period_num': 20, 'train_rates': 0.9654730577076878, 'learning_rate': 0.0004175962060940778, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8759170238772144}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:14:06,240][0m Trial 39 finished with value: 0.4527100622653961 and parameters: {'observation_period_num': 96, 'train_rates': 0.9668060595105581, 'learning_rate': 8.734841598865277e-06, 'batch_size': 162, 'step_size': 12, 'gamma': 0.877670800982016}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:14:41,450][0m Trial 40 finished with value: 0.18894237279891968 and parameters: {'observation_period_num': 178, 'train_rates': 0.9894291347935648, 'learning_rate': 0.0006868290193095471, 'batch_size': 183, 'step_size': 15, 'gamma': 0.8967591801552754}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:15:26,767][0m Trial 41 finished with value: 0.0390227735042572 and parameters: {'observation_period_num': 6, 'train_rates': 0.9658801206651801, 'learning_rate': 0.0003701472058317298, 'batch_size': 142, 'step_size': 8, 'gamma': 0.853004487579835}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:16:13,091][0m Trial 42 finished with value: 0.0328172892332077 and parameters: {'observation_period_num': 17, 'train_rates': 0.9694158340810223, 'learning_rate': 0.0004136402784918822, 'batch_size': 141, 'step_size': 9, 'gamma': 0.8603388474893576}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:16:57,469][0m Trial 43 finished with value: 0.6337667416291408 and parameters: {'observation_period_num': 35, 'train_rates': 0.9144270807973612, 'learning_rate': 1.6228512931341706e-06, 'batch_size': 138, 'step_size': 10, 'gamma': 0.8474658455433361}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:17:51,489][0m Trial 44 finished with value: 0.06003507971763611 and parameters: {'observation_period_num': 48, 'train_rates': 0.9669224716679062, 'learning_rate': 0.00043863913846647895, 'batch_size': 115, 'step_size': 9, 'gamma': 0.8581040638433014}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:18:36,329][0m Trial 45 finished with value: 0.046301823148809415 and parameters: {'observation_period_num': 18, 'train_rates': 0.9357529408993706, 'learning_rate': 0.0001290287936265152, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8285156565709911}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:19:11,734][0m Trial 46 finished with value: 0.05807004854563744 and parameters: {'observation_period_num': 40, 'train_rates': 0.8940529044206017, 'learning_rate': 0.0006924195791356758, 'batch_size': 168, 'step_size': 8, 'gamma': 0.8754669723195824}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:19:44,956][0m Trial 47 finished with value: 0.26811356459345137 and parameters: {'observation_period_num': 220, 'train_rates': 0.603994020217182, 'learning_rate': 0.0005098749610785525, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9042562291211912}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:20:14,071][0m Trial 48 finished with value: 0.18126961588859558 and parameters: {'observation_period_num': 71, 'train_rates': 0.9707870513944249, 'learning_rate': 4.252115491049263e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.8098560961503776}. Best is trial 18 with value: 0.029372701421380043.[0m
[32m[I 2025-02-07 23:20:48,308][0m Trial 49 finished with value: 0.11025821778082079 and parameters: {'observation_period_num': 158, 'train_rates': 0.8453424273279978, 'learning_rate': 0.0001946915096325403, 'batch_size': 166, 'step_size': 7, 'gamma': 0.8844808956152466}. Best is trial 18 with value: 0.029372701421380043.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 13, 'train_rates': 0.8372107469451779, 'learning_rate': 0.0003957799553701948, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9106259805066065}
Epoch 1/300, trend Loss: 0.3867 | 0.3073
Epoch 2/300, trend Loss: 0.1837 | 0.2121
Epoch 3/300, trend Loss: 0.1682 | 0.1526
Epoch 4/300, trend Loss: 0.1595 | 0.1440
Epoch 5/300, trend Loss: 0.1457 | 0.1243
Epoch 6/300, trend Loss: 0.1544 | 0.1349
Epoch 7/300, trend Loss: 0.1764 | 0.3987
Epoch 8/300, trend Loss: 0.1596 | 0.1259
Epoch 9/300, trend Loss: 0.1285 | 0.0923
Epoch 10/300, trend Loss: 0.1201 | 0.0836
Epoch 11/300, trend Loss: 0.1143 | 0.0785
Epoch 12/300, trend Loss: 0.1113 | 0.0728
Epoch 13/300, trend Loss: 0.1149 | 0.0783
Epoch 14/300, trend Loss: 0.1138 | 0.0680
Epoch 15/300, trend Loss: 0.1060 | 0.0691
Epoch 16/300, trend Loss: 0.1011 | 0.0795
Epoch 17/300, trend Loss: 0.1031 | 0.0865
Epoch 18/300, trend Loss: 0.0987 | 0.0673
Epoch 19/300, trend Loss: 0.1005 | 0.0667
Epoch 20/300, trend Loss: 0.1107 | 0.0649
Epoch 21/300, trend Loss: 0.1064 | 0.1053
Epoch 22/300, trend Loss: 0.1050 | 0.0574
Epoch 23/300, trend Loss: 0.0967 | 0.0574
Epoch 24/300, trend Loss: 0.0957 | 0.0542
Epoch 25/300, trend Loss: 0.0938 | 0.0550
Epoch 26/300, trend Loss: 0.1070 | 0.0955
Epoch 27/300, trend Loss: 0.1051 | 0.1155
Epoch 28/300, trend Loss: 0.1033 | 0.0634
Epoch 29/300, trend Loss: 0.1010 | 0.0570
Epoch 30/300, trend Loss: 0.0897 | 0.0472
Epoch 31/300, trend Loss: 0.0861 | 0.0499
Epoch 32/300, trend Loss: 0.0866 | 0.0656
Epoch 33/300, trend Loss: 0.0872 | 0.0617
Epoch 34/300, trend Loss: 0.0859 | 0.0454
Epoch 35/300, trend Loss: 0.0861 | 0.0545
Epoch 36/300, trend Loss: 0.0901 | 0.0493
Epoch 37/300, trend Loss: 0.0912 | 0.0578
Epoch 38/300, trend Loss: 0.0837 | 0.0753
Epoch 39/300, trend Loss: 0.0850 | 0.0448
Epoch 40/300, trend Loss: 0.0830 | 0.0518
Epoch 41/300, trend Loss: 0.0878 | 0.0448
Epoch 42/300, trend Loss: 0.0827 | 0.0649
Epoch 43/300, trend Loss: 0.0841 | 0.0593
Epoch 44/300, trend Loss: 0.0851 | 0.0437
Epoch 45/300, trend Loss: 0.0814 | 0.0474
Epoch 46/300, trend Loss: 0.0789 | 0.0492
Epoch 47/300, trend Loss: 0.0774 | 0.0404
Epoch 48/300, trend Loss: 0.0764 | 0.0404
Epoch 49/300, trend Loss: 0.0755 | 0.0442
Epoch 50/300, trend Loss: 0.0738 | 0.0407
Epoch 51/300, trend Loss: 0.0730 | 0.0385
Epoch 52/300, trend Loss: 0.0724 | 0.0390
Epoch 53/300, trend Loss: 0.0721 | 0.0401
Epoch 54/300, trend Loss: 0.0717 | 0.0390
Epoch 55/300, trend Loss: 0.0712 | 0.0378
Epoch 56/300, trend Loss: 0.0710 | 0.0393
Epoch 57/300, trend Loss: 0.0708 | 0.0416
Epoch 58/300, trend Loss: 0.0703 | 0.0391
Epoch 59/300, trend Loss: 0.0696 | 0.0366
Epoch 60/300, trend Loss: 0.0696 | 0.0366
Epoch 61/300, trend Loss: 0.0702 | 0.0376
Epoch 62/300, trend Loss: 0.0706 | 0.0362
Epoch 63/300, trend Loss: 0.0690 | 0.0367
Epoch 64/300, trend Loss: 0.0692 | 0.0391
Epoch 65/300, trend Loss: 0.0689 | 0.0388
Epoch 66/300, trend Loss: 0.0684 | 0.0341
Epoch 67/300, trend Loss: 0.0692 | 0.0387
Epoch 68/300, trend Loss: 0.0718 | 0.0386
Epoch 69/300, trend Loss: 0.0753 | 0.0396
Epoch 70/300, trend Loss: 0.0743 | 0.0653
Epoch 71/300, trend Loss: 0.0764 | 0.0377
Epoch 72/300, trend Loss: 0.0752 | 0.0380
Epoch 73/300, trend Loss: 0.0727 | 0.0350
Epoch 74/300, trend Loss: 0.0684 | 0.0344
Epoch 75/300, trend Loss: 0.0703 | 0.0400
Epoch 76/300, trend Loss: 0.0687 | 0.0386
Epoch 77/300, trend Loss: 0.0711 | 0.0339
Epoch 78/300, trend Loss: 0.0689 | 0.0341
Epoch 79/300, trend Loss: 0.0678 | 0.0382
Epoch 80/300, trend Loss: 0.0685 | 0.0353
Epoch 81/300, trend Loss: 0.0671 | 0.0340
Epoch 82/300, trend Loss: 0.0659 | 0.0330
Epoch 83/300, trend Loss: 0.0650 | 0.0320
Epoch 84/300, trend Loss: 0.0666 | 0.0322
Epoch 85/300, trend Loss: 0.0657 | 0.0349
Epoch 86/300, trend Loss: 0.0658 | 0.0336
Epoch 87/300, trend Loss: 0.0669 | 0.0328
Epoch 88/300, trend Loss: 0.0675 | 0.0335
Epoch 89/300, trend Loss: 0.0683 | 0.0403
Epoch 90/300, trend Loss: 0.0707 | 0.0398
Epoch 91/300, trend Loss: 0.0736 | 0.0364
Epoch 92/300, trend Loss: 0.0734 | 0.0339
Epoch 93/300, trend Loss: 0.0703 | 0.0375
Epoch 94/300, trend Loss: 0.0688 | 0.0328
Epoch 95/300, trend Loss: 0.0705 | 0.0341
Epoch 96/300, trend Loss: 0.0720 | 0.0360
Epoch 97/300, trend Loss: 0.0698 | 0.0364
Epoch 98/300, trend Loss: 0.0701 | 0.0337
Epoch 99/300, trend Loss: 0.0707 | 0.0363
Epoch 100/300, trend Loss: 0.0681 | 0.0407
Epoch 101/300, trend Loss: 0.0677 | 0.0316
Epoch 102/300, trend Loss: 0.0645 | 0.0315
Epoch 103/300, trend Loss: 0.0654 | 0.0322
Epoch 104/300, trend Loss: 0.0637 | 0.0317
Epoch 105/300, trend Loss: 0.0646 | 0.0309
Epoch 106/300, trend Loss: 0.0642 | 0.0297
Epoch 107/300, trend Loss: 0.0646 | 0.0305
Epoch 108/300, trend Loss: 0.0639 | 0.0310
Epoch 109/300, trend Loss: 0.0626 | 0.0300
Epoch 110/300, trend Loss: 0.0618 | 0.0301
Epoch 111/300, trend Loss: 0.0621 | 0.0326
Epoch 112/300, trend Loss: 0.0633 | 0.0314
Epoch 113/300, trend Loss: 0.0630 | 0.0304
Epoch 114/300, trend Loss: 0.0626 | 0.0298
Epoch 115/300, trend Loss: 0.0618 | 0.0307
Epoch 116/300, trend Loss: 0.0619 | 0.0302
Epoch 117/300, trend Loss: 0.0625 | 0.0320
Epoch 118/300, trend Loss: 0.0644 | 0.0303
Epoch 119/300, trend Loss: 0.0617 | 0.0311
Epoch 120/300, trend Loss: 0.0625 | 0.0312
Epoch 121/300, trend Loss: 0.0611 | 0.0311
Epoch 122/300, trend Loss: 0.0631 | 0.0304
Epoch 123/300, trend Loss: 0.0615 | 0.0307
Epoch 124/300, trend Loss: 0.0614 | 0.0304
Epoch 125/300, trend Loss: 0.0602 | 0.0291
Epoch 126/300, trend Loss: 0.0607 | 0.0297
Epoch 127/300, trend Loss: 0.0602 | 0.0297
Epoch 128/300, trend Loss: 0.0594 | 0.0292
Epoch 129/300, trend Loss: 0.0592 | 0.0302
Epoch 130/300, trend Loss: 0.0593 | 0.0307
Epoch 131/300, trend Loss: 0.0589 | 0.0296
Epoch 132/300, trend Loss: 0.0587 | 0.0294
Epoch 133/300, trend Loss: 0.0585 | 0.0305
Epoch 134/300, trend Loss: 0.0585 | 0.0294
Epoch 135/300, trend Loss: 0.0584 | 0.0293
Epoch 136/300, trend Loss: 0.0586 | 0.0296
Epoch 137/300, trend Loss: 0.0590 | 0.0297
Epoch 138/300, trend Loss: 0.0600 | 0.0306
Epoch 139/300, trend Loss: 0.0610 | 0.0302
Epoch 140/300, trend Loss: 0.0588 | 0.0291
Epoch 141/300, trend Loss: 0.0589 | 0.0293
Epoch 142/300, trend Loss: 0.0585 | 0.0291
Epoch 143/300, trend Loss: 0.0582 | 0.0295
Epoch 144/300, trend Loss: 0.0586 | 0.0298
Epoch 145/300, trend Loss: 0.0589 | 0.0308
Epoch 146/300, trend Loss: 0.0587 | 0.0290
Epoch 147/300, trend Loss: 0.0581 | 0.0290
Epoch 148/300, trend Loss: 0.0579 | 0.0310
Epoch 149/300, trend Loss: 0.0582 | 0.0296
Epoch 150/300, trend Loss: 0.0588 | 0.0300
Epoch 151/300, trend Loss: 0.0593 | 0.0296
Epoch 152/300, trend Loss: 0.0585 | 0.0298
Epoch 153/300, trend Loss: 0.0584 | 0.0296
Epoch 154/300, trend Loss: 0.0580 | 0.0296
Epoch 155/300, trend Loss: 0.0573 | 0.0290
Epoch 156/300, trend Loss: 0.0570 | 0.0295
Epoch 157/300, trend Loss: 0.0571 | 0.0299
Epoch 158/300, trend Loss: 0.0573 | 0.0289
Epoch 159/300, trend Loss: 0.0570 | 0.0286
Epoch 160/300, trend Loss: 0.0568 | 0.0293
Epoch 161/300, trend Loss: 0.0568 | 0.0291
Epoch 162/300, trend Loss: 0.0567 | 0.0292
Epoch 163/300, trend Loss: 0.0566 | 0.0291
Epoch 164/300, trend Loss: 0.0563 | 0.0288
Epoch 165/300, trend Loss: 0.0561 | 0.0290
Epoch 166/300, trend Loss: 0.0561 | 0.0288
Epoch 167/300, trend Loss: 0.0564 | 0.0289
Epoch 168/300, trend Loss: 0.0567 | 0.0286
Epoch 169/300, trend Loss: 0.0563 | 0.0287
Epoch 170/300, trend Loss: 0.0559 | 0.0286
Epoch 171/300, trend Loss: 0.0559 | 0.0290
Epoch 172/300, trend Loss: 0.0556 | 0.0288
Epoch 173/300, trend Loss: 0.0555 | 0.0285
Epoch 174/300, trend Loss: 0.0555 | 0.0289
Epoch 175/300, trend Loss: 0.0557 | 0.0290
Epoch 176/300, trend Loss: 0.0560 | 0.0283
Epoch 177/300, trend Loss: 0.0558 | 0.0284
Epoch 178/300, trend Loss: 0.0553 | 0.0287
Epoch 179/300, trend Loss: 0.0552 | 0.0287
Epoch 180/300, trend Loss: 0.0548 | 0.0285
Epoch 181/300, trend Loss: 0.0547 | 0.0284
Epoch 182/300, trend Loss: 0.0547 | 0.0292
Epoch 183/300, trend Loss: 0.0549 | 0.0288
Epoch 184/300, trend Loss: 0.0549 | 0.0281
Epoch 185/300, trend Loss: 0.0543 | 0.0285
Epoch 186/300, trend Loss: 0.0540 | 0.0290
Epoch 187/300, trend Loss: 0.0534 | 0.0288
Epoch 188/300, trend Loss: 0.0529 | 0.0287
Epoch 189/300, trend Loss: 0.0526 | 0.0293
Epoch 190/300, trend Loss: 0.0525 | 0.0290
Epoch 191/300, trend Loss: 0.0521 | 0.0285
Epoch 192/300, trend Loss: 0.0514 | 0.0285
Epoch 193/300, trend Loss: 0.0511 | 0.0296
Epoch 194/300, trend Loss: 0.0505 | 0.0290
Epoch 195/300, trend Loss: 0.0502 | 0.0288
Epoch 196/300, trend Loss: 0.0499 | 0.0295
Epoch 197/300, trend Loss: 0.0498 | 0.0289
Epoch 198/300, trend Loss: 0.0494 | 0.0286
Epoch 199/300, trend Loss: 0.0490 | 0.0288
Epoch 200/300, trend Loss: 0.0488 | 0.0292
Epoch 201/300, trend Loss: 0.0484 | 0.0288
Epoch 202/300, trend Loss: 0.0482 | 0.0286
Epoch 203/300, trend Loss: 0.0479 | 0.0291
Epoch 204/300, trend Loss: 0.0478 | 0.0289
Epoch 205/300, trend Loss: 0.0479 | 0.0286
Epoch 206/300, trend Loss: 0.0482 | 0.0289
Epoch 207/300, trend Loss: 0.0477 | 0.0295
Epoch 208/300, trend Loss: 0.0475 | 0.0291
Epoch 209/300, trend Loss: 0.0474 | 0.0289
Epoch 210/300, trend Loss: 0.0473 | 0.0292
Epoch 211/300, trend Loss: 0.0472 | 0.0288
Epoch 212/300, trend Loss: 0.0478 | 0.0287
Epoch 213/300, trend Loss: 0.0470 | 0.0298
Epoch 214/300, trend Loss: 0.0471 | 0.0290
Epoch 215/300, trend Loss: 0.0468 | 0.0290
Epoch 216/300, trend Loss: 0.0466 | 0.0293
Epoch 217/300, trend Loss: 0.0465 | 0.0287
Epoch 218/300, trend Loss: 0.0463 | 0.0289
Epoch 219/300, trend Loss: 0.0463 | 0.0286
Epoch 220/300, trend Loss: 0.0460 | 0.0287
Epoch 221/300, trend Loss: 0.0460 | 0.0287
Epoch 222/300, trend Loss: 0.0459 | 0.0287
Epoch 223/300, trend Loss: 0.0458 | 0.0287
Epoch 224/300, trend Loss: 0.0457 | 0.0286
Epoch 225/300, trend Loss: 0.0457 | 0.0286
Epoch 226/300, trend Loss: 0.0456 | 0.0287
Epoch 227/300, trend Loss: 0.0457 | 0.0284
Epoch 228/300, trend Loss: 0.0455 | 0.0288
Epoch 229/300, trend Loss: 0.0455 | 0.0286
Epoch 230/300, trend Loss: 0.0455 | 0.0286
Epoch 231/300, trend Loss: 0.0454 | 0.0288
Epoch 232/300, trend Loss: 0.0453 | 0.0288
Epoch 233/300, trend Loss: 0.0454 | 0.0287
Epoch 234/300, trend Loss: 0.0453 | 0.0286
Epoch 235/300, trend Loss: 0.0453 | 0.0285
Epoch 236/300, trend Loss: 0.0451 | 0.0287
Epoch 237/300, trend Loss: 0.0451 | 0.0286
Epoch 238/300, trend Loss: 0.0452 | 0.0287
Epoch 239/300, trend Loss: 0.0451 | 0.0287
Epoch 240/300, trend Loss: 0.0450 | 0.0288
Epoch 241/300, trend Loss: 0.0450 | 0.0287
Epoch 242/300, trend Loss: 0.0449 | 0.0285
Epoch 243/300, trend Loss: 0.0449 | 0.0287
Epoch 244/300, trend Loss: 0.0448 | 0.0286
Epoch 245/300, trend Loss: 0.0448 | 0.0287
Epoch 246/300, trend Loss: 0.0448 | 0.0288
Epoch 247/300, trend Loss: 0.0447 | 0.0287
Epoch 248/300, trend Loss: 0.0447 | 0.0287
Epoch 249/300, trend Loss: 0.0447 | 0.0286
Epoch 250/300, trend Loss: 0.0445 | 0.0286
Epoch 251/300, trend Loss: 0.0445 | 0.0287
Epoch 252/300, trend Loss: 0.0445 | 0.0287
Epoch 253/300, trend Loss: 0.0444 | 0.0288
Epoch 254/300, trend Loss: 0.0444 | 0.0287
Epoch 255/300, trend Loss: 0.0444 | 0.0287
Epoch 256/300, trend Loss: 0.0443 | 0.0287
Epoch 257/300, trend Loss: 0.0443 | 0.0286
Epoch 258/300, trend Loss: 0.0442 | 0.0287
Epoch 259/300, trend Loss: 0.0442 | 0.0287
Epoch 260/300, trend Loss: 0.0442 | 0.0288
Epoch 261/300, trend Loss: 0.0441 | 0.0288
Epoch 262/300, trend Loss: 0.0441 | 0.0287
Epoch 263/300, trend Loss: 0.0441 | 0.0287
Epoch 264/300, trend Loss: 0.0440 | 0.0287
Epoch 265/300, trend Loss: 0.0440 | 0.0287
Epoch 266/300, trend Loss: 0.0440 | 0.0288
Epoch 267/300, trend Loss: 0.0439 | 0.0288
Epoch 268/300, trend Loss: 0.0439 | 0.0288
Epoch 269/300, trend Loss: 0.0439 | 0.0287
Epoch 270/300, trend Loss: 0.0438 | 0.0287
Epoch 271/300, trend Loss: 0.0438 | 0.0287
Epoch 272/300, trend Loss: 0.0438 | 0.0287
Epoch 273/300, trend Loss: 0.0437 | 0.0288
Epoch 274/300, trend Loss: 0.0437 | 0.0288
Epoch 275/300, trend Loss: 0.0437 | 0.0288
Epoch 276/300, trend Loss: 0.0437 | 0.0288
Epoch 277/300, trend Loss: 0.0436 | 0.0288
Epoch 278/300, trend Loss: 0.0436 | 0.0288
Epoch 279/300, trend Loss: 0.0436 | 0.0288
Epoch 280/300, trend Loss: 0.0436 | 0.0288
Epoch 281/300, trend Loss: 0.0435 | 0.0288
Epoch 282/300, trend Loss: 0.0435 | 0.0288
Epoch 283/300, trend Loss: 0.0435 | 0.0288
Epoch 284/300, trend Loss: 0.0434 | 0.0288
Epoch 285/300, trend Loss: 0.0434 | 0.0288
Epoch 286/300, trend Loss: 0.0434 | 0.0288
Epoch 287/300, trend Loss: 0.0434 | 0.0289
Epoch 288/300, trend Loss: 0.0433 | 0.0289
Epoch 289/300, trend Loss: 0.0433 | 0.0289
Epoch 290/300, trend Loss: 0.0433 | 0.0288
Epoch 291/300, trend Loss: 0.0433 | 0.0288
Epoch 292/300, trend Loss: 0.0432 | 0.0288
Epoch 293/300, trend Loss: 0.0432 | 0.0289
Epoch 294/300, trend Loss: 0.0432 | 0.0289
Epoch 295/300, trend Loss: 0.0432 | 0.0289
Epoch 296/300, trend Loss: 0.0431 | 0.0289
Epoch 297/300, trend Loss: 0.0431 | 0.0289
Epoch 298/300, trend Loss: 0.0431 | 0.0288
Epoch 299/300, trend Loss: 0.0431 | 0.0289
Epoch 300/300, trend Loss: 0.0430 | 0.0289
Training seasonal_0 component with params: {'observation_period_num': 14, 'train_rates': 0.9720680171735195, 'learning_rate': 0.00019855729819191845, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8283412631315773}
Epoch 1/300, seasonal_0 Loss: 0.2459 | 0.1952
Epoch 2/300, seasonal_0 Loss: 0.1632 | 0.1328
Epoch 3/300, seasonal_0 Loss: 0.1411 | 0.1223
Epoch 4/300, seasonal_0 Loss: 0.1348 | 0.1027
Epoch 5/300, seasonal_0 Loss: 0.1331 | 0.1045
Epoch 6/300, seasonal_0 Loss: 0.1338 | 0.1011
Epoch 7/300, seasonal_0 Loss: 0.1272 | 0.0997
Epoch 8/300, seasonal_0 Loss: 0.1138 | 0.0840
Epoch 9/300, seasonal_0 Loss: 0.1070 | 0.0841
Epoch 10/300, seasonal_0 Loss: 0.1076 | 0.0823
Epoch 11/300, seasonal_0 Loss: 0.1033 | 0.0751
Epoch 12/300, seasonal_0 Loss: 0.1003 | 0.0736
Epoch 13/300, seasonal_0 Loss: 0.1004 | 0.0723
Epoch 14/300, seasonal_0 Loss: 0.0990 | 0.0737
Epoch 15/300, seasonal_0 Loss: 0.0979 | 0.0723
Epoch 16/300, seasonal_0 Loss: 0.0956 | 0.0715
Epoch 17/300, seasonal_0 Loss: 0.0933 | 0.0708
Epoch 18/300, seasonal_0 Loss: 0.0914 | 0.0705
Epoch 19/300, seasonal_0 Loss: 0.0898 | 0.0708
Epoch 20/300, seasonal_0 Loss: 0.0885 | 0.0720
Epoch 21/300, seasonal_0 Loss: 0.0873 | 0.0783
Epoch 22/300, seasonal_0 Loss: 0.0859 | 0.0700
Epoch 23/300, seasonal_0 Loss: 0.0849 | 0.0639
Epoch 24/300, seasonal_0 Loss: 0.0867 | 0.0577
Epoch 25/300, seasonal_0 Loss: 0.0870 | 0.0583
Epoch 26/300, seasonal_0 Loss: 0.0863 | 0.0565
Epoch 27/300, seasonal_0 Loss: 0.0856 | 0.0520
Epoch 28/300, seasonal_0 Loss: 0.0828 | 0.0539
Epoch 29/300, seasonal_0 Loss: 0.0826 | 0.0546
Epoch 30/300, seasonal_0 Loss: 0.0840 | 0.0553
Epoch 31/300, seasonal_0 Loss: 0.0852 | 0.0553
Epoch 32/300, seasonal_0 Loss: 0.0844 | 0.0545
Epoch 33/300, seasonal_0 Loss: 0.0821 | 0.0526
Epoch 34/300, seasonal_0 Loss: 0.0799 | 0.0511
Epoch 35/300, seasonal_0 Loss: 0.0778 | 0.0509
Epoch 36/300, seasonal_0 Loss: 0.0767 | 0.0502
Epoch 37/300, seasonal_0 Loss: 0.0763 | 0.0493
Epoch 38/300, seasonal_0 Loss: 0.0764 | 0.0485
Epoch 39/300, seasonal_0 Loss: 0.0768 | 0.0477
Epoch 40/300, seasonal_0 Loss: 0.0777 | 0.0475
Epoch 41/300, seasonal_0 Loss: 0.0782 | 0.0475
Epoch 42/300, seasonal_0 Loss: 0.0782 | 0.0474
Epoch 43/300, seasonal_0 Loss: 0.0775 | 0.0471
Epoch 44/300, seasonal_0 Loss: 0.0766 | 0.0467
Epoch 45/300, seasonal_0 Loss: 0.0758 | 0.0462
Epoch 46/300, seasonal_0 Loss: 0.0753 | 0.0457
Epoch 47/300, seasonal_0 Loss: 0.0758 | 0.0452
Epoch 48/300, seasonal_0 Loss: 0.0780 | 0.0465
Epoch 49/300, seasonal_0 Loss: 0.0807 | 0.0476
Epoch 50/300, seasonal_0 Loss: 0.0799 | 0.0476
Epoch 51/300, seasonal_0 Loss: 0.0773 | 0.0468
Epoch 52/300, seasonal_0 Loss: 0.0755 | 0.0464
Epoch 53/300, seasonal_0 Loss: 0.0748 | 0.0470
Epoch 54/300, seasonal_0 Loss: 0.0744 | 0.0464
Epoch 55/300, seasonal_0 Loss: 0.0740 | 0.0460
Epoch 56/300, seasonal_0 Loss: 0.0739 | 0.0459
Epoch 57/300, seasonal_0 Loss: 0.0738 | 0.0459
Epoch 58/300, seasonal_0 Loss: 0.0738 | 0.0459
Epoch 59/300, seasonal_0 Loss: 0.0741 | 0.0458
Epoch 60/300, seasonal_0 Loss: 0.0750 | 0.0437
Epoch 61/300, seasonal_0 Loss: 0.0768 | 0.0476
Epoch 62/300, seasonal_0 Loss: 0.0771 | 0.0528
Epoch 63/300, seasonal_0 Loss: 0.0754 | 0.0501
Epoch 64/300, seasonal_0 Loss: 0.0750 | 0.0459
Epoch 65/300, seasonal_0 Loss: 0.0759 | 0.0430
Epoch 66/300, seasonal_0 Loss: 0.0785 | 0.0443
Epoch 67/300, seasonal_0 Loss: 0.0836 | 0.0502
Epoch 68/300, seasonal_0 Loss: 0.0871 | 0.0654
Epoch 69/300, seasonal_0 Loss: 0.0858 | 0.0719
Epoch 70/300, seasonal_0 Loss: 0.0795 | 0.0572
Epoch 71/300, seasonal_0 Loss: 0.0744 | 0.0452
Epoch 72/300, seasonal_0 Loss: 0.0739 | 0.0427
Epoch 73/300, seasonal_0 Loss: 0.0764 | 0.0455
Epoch 74/300, seasonal_0 Loss: 0.0771 | 0.0428
Epoch 75/300, seasonal_0 Loss: 0.0747 | 0.0410
Epoch 76/300, seasonal_0 Loss: 0.0719 | 0.0401
Epoch 77/300, seasonal_0 Loss: 0.0702 | 0.0398
Epoch 78/300, seasonal_0 Loss: 0.0695 | 0.0398
Epoch 79/300, seasonal_0 Loss: 0.0693 | 0.0394
Epoch 80/300, seasonal_0 Loss: 0.0688 | 0.0396
Epoch 81/300, seasonal_0 Loss: 0.0685 | 0.0397
Epoch 82/300, seasonal_0 Loss: 0.0682 | 0.0398
Epoch 83/300, seasonal_0 Loss: 0.0679 | 0.0397
Epoch 84/300, seasonal_0 Loss: 0.0677 | 0.0395
Epoch 85/300, seasonal_0 Loss: 0.0675 | 0.0393
Epoch 86/300, seasonal_0 Loss: 0.0674 | 0.0392
Epoch 87/300, seasonal_0 Loss: 0.0672 | 0.0390
Epoch 88/300, seasonal_0 Loss: 0.0671 | 0.0387
Epoch 89/300, seasonal_0 Loss: 0.0670 | 0.0385
Epoch 90/300, seasonal_0 Loss: 0.0669 | 0.0384
Epoch 91/300, seasonal_0 Loss: 0.0668 | 0.0382
Epoch 92/300, seasonal_0 Loss: 0.0667 | 0.0381
Epoch 93/300, seasonal_0 Loss: 0.0666 | 0.0380
Epoch 94/300, seasonal_0 Loss: 0.0665 | 0.0379
Epoch 95/300, seasonal_0 Loss: 0.0664 | 0.0378
Epoch 96/300, seasonal_0 Loss: 0.0664 | 0.0377
Epoch 97/300, seasonal_0 Loss: 0.0663 | 0.0376
Epoch 98/300, seasonal_0 Loss: 0.0662 | 0.0376
Epoch 99/300, seasonal_0 Loss: 0.0662 | 0.0374
Epoch 100/300, seasonal_0 Loss: 0.0661 | 0.0374
Epoch 101/300, seasonal_0 Loss: 0.0661 | 0.0373
Epoch 102/300, seasonal_0 Loss: 0.0660 | 0.0373
Epoch 103/300, seasonal_0 Loss: 0.0659 | 0.0372
Epoch 104/300, seasonal_0 Loss: 0.0658 | 0.0371
Epoch 105/300, seasonal_0 Loss: 0.0658 | 0.0370
Epoch 106/300, seasonal_0 Loss: 0.0657 | 0.0370
Epoch 107/300, seasonal_0 Loss: 0.0656 | 0.0369
Epoch 108/300, seasonal_0 Loss: 0.0656 | 0.0369
Epoch 109/300, seasonal_0 Loss: 0.0655 | 0.0368
Epoch 110/300, seasonal_0 Loss: 0.0655 | 0.0368
Epoch 111/300, seasonal_0 Loss: 0.0654 | 0.0368
Epoch 112/300, seasonal_0 Loss: 0.0653 | 0.0367
Epoch 113/300, seasonal_0 Loss: 0.0653 | 0.0367
Epoch 114/300, seasonal_0 Loss: 0.0653 | 0.0367
Epoch 115/300, seasonal_0 Loss: 0.0652 | 0.0367
Epoch 116/300, seasonal_0 Loss: 0.0652 | 0.0366
Epoch 117/300, seasonal_0 Loss: 0.0651 | 0.0366
Epoch 118/300, seasonal_0 Loss: 0.0651 | 0.0366
Epoch 119/300, seasonal_0 Loss: 0.0650 | 0.0365
Epoch 120/300, seasonal_0 Loss: 0.0650 | 0.0365
Epoch 121/300, seasonal_0 Loss: 0.0649 | 0.0365
Epoch 122/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 123/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 124/300, seasonal_0 Loss: 0.0648 | 0.0364
Epoch 125/300, seasonal_0 Loss: 0.0648 | 0.0363
Epoch 126/300, seasonal_0 Loss: 0.0647 | 0.0363
Epoch 127/300, seasonal_0 Loss: 0.0647 | 0.0363
Epoch 128/300, seasonal_0 Loss: 0.0647 | 0.0363
Epoch 129/300, seasonal_0 Loss: 0.0646 | 0.0363
Epoch 130/300, seasonal_0 Loss: 0.0646 | 0.0362
Epoch 131/300, seasonal_0 Loss: 0.0646 | 0.0362
Epoch 132/300, seasonal_0 Loss: 0.0645 | 0.0362
Epoch 133/300, seasonal_0 Loss: 0.0645 | 0.0362
Epoch 134/300, seasonal_0 Loss: 0.0645 | 0.0362
Epoch 135/300, seasonal_0 Loss: 0.0645 | 0.0362
Epoch 136/300, seasonal_0 Loss: 0.0644 | 0.0361
Epoch 137/300, seasonal_0 Loss: 0.0644 | 0.0361
Epoch 138/300, seasonal_0 Loss: 0.0644 | 0.0361
Epoch 139/300, seasonal_0 Loss: 0.0643 | 0.0361
Epoch 140/300, seasonal_0 Loss: 0.0643 | 0.0361
Epoch 141/300, seasonal_0 Loss: 0.0643 | 0.0361
Epoch 142/300, seasonal_0 Loss: 0.0643 | 0.0361
Epoch 143/300, seasonal_0 Loss: 0.0642 | 0.0360
Epoch 144/300, seasonal_0 Loss: 0.0642 | 0.0360
Epoch 145/300, seasonal_0 Loss: 0.0642 | 0.0360
Epoch 146/300, seasonal_0 Loss: 0.0642 | 0.0360
Epoch 147/300, seasonal_0 Loss: 0.0642 | 0.0360
Epoch 148/300, seasonal_0 Loss: 0.0641 | 0.0360
Epoch 149/300, seasonal_0 Loss: 0.0641 | 0.0360
Epoch 150/300, seasonal_0 Loss: 0.0641 | 0.0360
Epoch 151/300, seasonal_0 Loss: 0.0641 | 0.0360
Epoch 152/300, seasonal_0 Loss: 0.0641 | 0.0359
Epoch 153/300, seasonal_0 Loss: 0.0640 | 0.0359
Epoch 154/300, seasonal_0 Loss: 0.0640 | 0.0359
Epoch 155/300, seasonal_0 Loss: 0.0640 | 0.0359
Epoch 156/300, seasonal_0 Loss: 0.0640 | 0.0359
Epoch 157/300, seasonal_0 Loss: 0.0640 | 0.0359
Epoch 158/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 159/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 160/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 161/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 162/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 163/300, seasonal_0 Loss: 0.0639 | 0.0359
Epoch 164/300, seasonal_0 Loss: 0.0639 | 0.0358
Epoch 165/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 166/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 167/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 168/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 169/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 170/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 171/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 172/300, seasonal_0 Loss: 0.0638 | 0.0358
Epoch 173/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 174/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 175/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 176/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 177/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 178/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 179/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 180/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 181/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 182/300, seasonal_0 Loss: 0.0637 | 0.0358
Epoch 183/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 184/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 185/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 186/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 187/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 188/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 189/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 190/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 191/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 192/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 193/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 194/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 195/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 196/300, seasonal_0 Loss: 0.0636 | 0.0357
Epoch 197/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 198/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 199/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 200/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 201/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 202/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 203/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 204/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 205/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 206/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 207/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 208/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 209/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 210/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 211/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 212/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 213/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 214/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 215/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 216/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 217/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 218/300, seasonal_0 Loss: 0.0635 | 0.0357
Epoch 219/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 220/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 221/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 222/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 223/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 224/300, seasonal_0 Loss: 0.0634 | 0.0357
Epoch 225/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 226/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 227/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 228/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 229/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 230/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 231/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 232/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 233/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 234/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 235/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 236/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 237/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 238/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 239/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 240/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 241/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 242/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 243/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 244/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 245/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 246/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 247/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 248/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 249/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 250/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 251/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 252/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 253/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 254/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 255/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 256/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 257/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 258/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 259/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 260/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 261/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 262/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 263/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 264/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 265/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 266/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 267/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 268/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 269/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 270/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 271/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 272/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 273/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 274/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 275/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 276/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 277/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 278/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 279/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 280/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 281/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 282/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 283/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 284/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 285/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 286/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 287/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 288/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 289/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 290/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 291/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 292/300, seasonal_0 Loss: 0.0634 | 0.0356
Epoch 293/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 294/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 295/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 296/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 297/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 298/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 299/300, seasonal_0 Loss: 0.0633 | 0.0356
Epoch 300/300, seasonal_0 Loss: 0.0633 | 0.0356
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.9496020212887621, 'learning_rate': 0.0008022948731597576, 'batch_size': 24, 'step_size': 2, 'gamma': 0.8684135626316811}
Epoch 1/300, seasonal_1 Loss: 0.2126 | 0.0939
Epoch 2/300, seasonal_1 Loss: 0.1064 | 0.0879
Epoch 3/300, seasonal_1 Loss: 0.0987 | 0.0631
Epoch 4/300, seasonal_1 Loss: 0.0883 | 0.0601
Epoch 5/300, seasonal_1 Loss: 0.0852 | 0.0545
Epoch 6/300, seasonal_1 Loss: 0.0815 | 0.0487
Epoch 7/300, seasonal_1 Loss: 0.0770 | 0.0425
Epoch 8/300, seasonal_1 Loss: 0.0735 | 0.0387
Epoch 9/300, seasonal_1 Loss: 0.0709 | 0.0377
Epoch 10/300, seasonal_1 Loss: 0.0691 | 0.0362
Epoch 11/300, seasonal_1 Loss: 0.0675 | 0.0349
Epoch 12/300, seasonal_1 Loss: 0.0660 | 0.0339
Epoch 13/300, seasonal_1 Loss: 0.0649 | 0.0323
Epoch 14/300, seasonal_1 Loss: 0.0640 | 0.0311
Epoch 15/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 16/300, seasonal_1 Loss: 0.0628 | 0.0305
Epoch 17/300, seasonal_1 Loss: 0.0621 | 0.0305
Epoch 18/300, seasonal_1 Loss: 0.0613 | 0.0296
Epoch 19/300, seasonal_1 Loss: 0.0609 | 0.0297
Epoch 20/300, seasonal_1 Loss: 0.0602 | 0.0288
Epoch 21/300, seasonal_1 Loss: 0.0601 | 0.0278
Epoch 22/300, seasonal_1 Loss: 0.0596 | 0.0274
Epoch 23/300, seasonal_1 Loss: 0.0591 | 0.0271
Epoch 24/300, seasonal_1 Loss: 0.0587 | 0.0270
Epoch 25/300, seasonal_1 Loss: 0.0584 | 0.0268
Epoch 26/300, seasonal_1 Loss: 0.0581 | 0.0267
Epoch 27/300, seasonal_1 Loss: 0.0579 | 0.0267
Epoch 28/300, seasonal_1 Loss: 0.0576 | 0.0267
Epoch 29/300, seasonal_1 Loss: 0.0574 | 0.0265
Epoch 30/300, seasonal_1 Loss: 0.0572 | 0.0264
Epoch 31/300, seasonal_1 Loss: 0.0571 | 0.0262
Epoch 32/300, seasonal_1 Loss: 0.0569 | 0.0261
Epoch 33/300, seasonal_1 Loss: 0.0568 | 0.0260
Epoch 34/300, seasonal_1 Loss: 0.0567 | 0.0259
Epoch 35/300, seasonal_1 Loss: 0.0567 | 0.0258
Epoch 36/300, seasonal_1 Loss: 0.0566 | 0.0258
Epoch 37/300, seasonal_1 Loss: 0.0565 | 0.0258
Epoch 38/300, seasonal_1 Loss: 0.0565 | 0.0258
Epoch 39/300, seasonal_1 Loss: 0.0565 | 0.0258
Epoch 40/300, seasonal_1 Loss: 0.0564 | 0.0257
Epoch 41/300, seasonal_1 Loss: 0.0564 | 0.0257
Epoch 42/300, seasonal_1 Loss: 0.0564 | 0.0257
Epoch 43/300, seasonal_1 Loss: 0.0564 | 0.0256
Epoch 44/300, seasonal_1 Loss: 0.0563 | 0.0256
Epoch 45/300, seasonal_1 Loss: 0.0563 | 0.0256
Epoch 46/300, seasonal_1 Loss: 0.0563 | 0.0256
Epoch 47/300, seasonal_1 Loss: 0.0563 | 0.0255
Epoch 48/300, seasonal_1 Loss: 0.0563 | 0.0255
Epoch 49/300, seasonal_1 Loss: 0.0562 | 0.0255
Epoch 50/300, seasonal_1 Loss: 0.0562 | 0.0255
Epoch 51/300, seasonal_1 Loss: 0.0562 | 0.0255
Epoch 52/300, seasonal_1 Loss: 0.0562 | 0.0255
Epoch 53/300, seasonal_1 Loss: 0.0562 | 0.0255
Epoch 54/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 55/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 56/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 57/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 58/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 59/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 60/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 61/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 62/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 63/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 64/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 65/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 66/300, seasonal_1 Loss: 0.0561 | 0.0255
Epoch 67/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 68/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 69/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 70/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 71/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 72/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 73/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 74/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 75/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 76/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 77/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 78/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 79/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 80/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 81/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 82/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 83/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 84/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 85/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 86/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 87/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 88/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 89/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 90/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 91/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 92/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 93/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 94/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 95/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 96/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 97/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 98/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 99/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 100/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 101/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 102/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 103/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 104/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 105/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 106/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 107/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 108/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 109/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 110/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 111/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 112/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 113/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 114/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 115/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 116/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 117/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 118/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 119/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 120/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 121/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 122/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 123/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 124/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 125/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 126/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 127/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 128/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 129/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 130/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 131/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 132/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 133/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 134/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 135/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 136/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 137/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 138/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 139/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 140/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 141/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 142/300, seasonal_1 Loss: 0.0560 | 0.0255
Epoch 143/300, seasonal_1 Loss: 0.0560 | 0.0255
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 36, 'train_rates': 0.9831691369485885, 'learning_rate': 0.0007394913217755428, 'batch_size': 18, 'step_size': 15, 'gamma': 0.937016314585997}
Epoch 1/300, seasonal_2 Loss: 0.1935 | 0.1017
Epoch 2/300, seasonal_2 Loss: 0.1216 | 0.1025
Epoch 3/300, seasonal_2 Loss: 0.1079 | 0.0742
Epoch 4/300, seasonal_2 Loss: 0.1020 | 0.0803
Epoch 5/300, seasonal_2 Loss: 0.0931 | 0.0917
Epoch 6/300, seasonal_2 Loss: 0.0905 | 0.1090
Epoch 7/300, seasonal_2 Loss: 0.0923 | 0.0712
Epoch 8/300, seasonal_2 Loss: 0.0908 | 0.0659
Epoch 9/300, seasonal_2 Loss: 0.0834 | 0.0607
Epoch 10/300, seasonal_2 Loss: 0.0789 | 0.0468
Epoch 11/300, seasonal_2 Loss: 0.0736 | 0.0470
Epoch 12/300, seasonal_2 Loss: 0.0724 | 0.0474
Epoch 13/300, seasonal_2 Loss: 0.0699 | 0.0488
Epoch 14/300, seasonal_2 Loss: 0.0658 | 0.0545
Epoch 15/300, seasonal_2 Loss: 0.0752 | 0.0719
Epoch 16/300, seasonal_2 Loss: 0.0745 | 0.0489
Epoch 17/300, seasonal_2 Loss: 0.0729 | 0.0593
Epoch 18/300, seasonal_2 Loss: 0.0691 | 0.0436
Epoch 19/300, seasonal_2 Loss: 0.0652 | 0.0430
Epoch 20/300, seasonal_2 Loss: 0.0602 | 0.0470
Epoch 21/300, seasonal_2 Loss: 0.0601 | 0.0417
Epoch 22/300, seasonal_2 Loss: 0.0570 | 0.0440
Epoch 23/300, seasonal_2 Loss: 0.0564 | 0.0481
Epoch 24/300, seasonal_2 Loss: 0.0554 | 0.0456
Epoch 25/300, seasonal_2 Loss: 0.0534 | 0.0419
Epoch 26/300, seasonal_2 Loss: 0.0526 | 0.0387
Epoch 27/300, seasonal_2 Loss: 0.0531 | 0.0430
Epoch 28/300, seasonal_2 Loss: 0.0521 | 0.0541
Epoch 29/300, seasonal_2 Loss: 0.0664 | 0.0789
Epoch 30/300, seasonal_2 Loss: 0.0575 | 0.0545
Epoch 31/300, seasonal_2 Loss: 0.0538 | 0.0463
Epoch 32/300, seasonal_2 Loss: 0.0518 | 0.0504
Epoch 33/300, seasonal_2 Loss: 0.0497 | 0.0429
Epoch 34/300, seasonal_2 Loss: 0.0492 | 0.0388
Epoch 35/300, seasonal_2 Loss: 0.0551 | 0.0538
Epoch 36/300, seasonal_2 Loss: 0.0496 | 0.0394
Epoch 37/300, seasonal_2 Loss: 0.0484 | 0.0344
Epoch 38/300, seasonal_2 Loss: 0.0494 | 0.0451
Epoch 39/300, seasonal_2 Loss: 0.0477 | 0.0299
Epoch 40/300, seasonal_2 Loss: 0.0461 | 0.0521
Epoch 41/300, seasonal_2 Loss: 0.0496 | 0.0615
Epoch 42/300, seasonal_2 Loss: 0.0513 | 0.0416
Epoch 43/300, seasonal_2 Loss: 0.0486 | 0.0456
Epoch 44/300, seasonal_2 Loss: 0.0486 | 0.0896
Epoch 45/300, seasonal_2 Loss: 0.0496 | 0.0580
Epoch 46/300, seasonal_2 Loss: 0.0458 | 0.0435
Epoch 47/300, seasonal_2 Loss: 0.0451 | 0.0628
Epoch 48/300, seasonal_2 Loss: 0.0463 | 0.0489
Epoch 49/300, seasonal_2 Loss: 0.0454 | 0.0384
Epoch 50/300, seasonal_2 Loss: 0.0414 | 0.0434
Epoch 51/300, seasonal_2 Loss: 0.0426 | 0.0386
Epoch 52/300, seasonal_2 Loss: 0.0483 | 0.0548
Epoch 53/300, seasonal_2 Loss: 0.0533 | 0.0465
Epoch 54/300, seasonal_2 Loss: 0.0489 | 0.0338
Epoch 55/300, seasonal_2 Loss: 0.0412 | 0.0295
Epoch 56/300, seasonal_2 Loss: 0.0456 | 0.0357
Epoch 57/300, seasonal_2 Loss: 0.0404 | 0.0322
Epoch 58/300, seasonal_2 Loss: 0.0431 | 0.0311
Epoch 59/300, seasonal_2 Loss: 0.0457 | 0.0352
Epoch 60/300, seasonal_2 Loss: 0.0412 | 0.0581
Epoch 61/300, seasonal_2 Loss: 0.0389 | 0.0630
Epoch 62/300, seasonal_2 Loss: 0.0409 | 0.0493
Epoch 63/300, seasonal_2 Loss: 0.0473 | 0.0455
Epoch 64/300, seasonal_2 Loss: 0.0455 | 0.0357
Epoch 65/300, seasonal_2 Loss: 0.0408 | 0.0342
Epoch 66/300, seasonal_2 Loss: 0.0394 | 0.0489
Epoch 67/300, seasonal_2 Loss: 0.0386 | 0.0701
Epoch 68/300, seasonal_2 Loss: 0.0376 | 0.0976
Epoch 69/300, seasonal_2 Loss: 0.0365 | 0.0369
Epoch 70/300, seasonal_2 Loss: 0.0369 | 0.0322
Epoch 71/300, seasonal_2 Loss: 0.0407 | 0.0466
Epoch 72/300, seasonal_2 Loss: 0.0473 | 0.0524
Epoch 73/300, seasonal_2 Loss: 0.0426 | 0.0413
Epoch 74/300, seasonal_2 Loss: 0.0409 | 0.0297
Epoch 75/300, seasonal_2 Loss: 0.0385 | 0.0415
Epoch 76/300, seasonal_2 Loss: 0.0444 | 0.0460
Epoch 77/300, seasonal_2 Loss: 0.0388 | 0.0407
Epoch 78/300, seasonal_2 Loss: 0.0398 | 0.0295
Epoch 79/300, seasonal_2 Loss: 0.0378 | 0.0680
Epoch 80/300, seasonal_2 Loss: 0.0322 | 0.0391
Epoch 81/300, seasonal_2 Loss: 0.0373 | 0.0274
Epoch 82/300, seasonal_2 Loss: 0.0371 | 0.0610
Epoch 83/300, seasonal_2 Loss: 0.0339 | 0.0283
Epoch 84/300, seasonal_2 Loss: 0.0412 | 0.0467
Epoch 85/300, seasonal_2 Loss: 0.0381 | 0.0304
Epoch 86/300, seasonal_2 Loss: 0.0338 | 0.0265
Epoch 87/300, seasonal_2 Loss: 0.0309 | 0.0306
Epoch 88/300, seasonal_2 Loss: 0.0357 | 0.0371
Epoch 89/300, seasonal_2 Loss: 0.0284 | 0.0544
Epoch 90/300, seasonal_2 Loss: 0.0367 | 0.0658
Epoch 91/300, seasonal_2 Loss: 0.0359 | 0.0853
Epoch 92/300, seasonal_2 Loss: 0.0281 | 0.0600
Epoch 93/300, seasonal_2 Loss: 0.0259 | 0.0281
Epoch 94/300, seasonal_2 Loss: 0.0246 | 0.0242
Epoch 95/300, seasonal_2 Loss: 0.0240 | 0.0221
Epoch 96/300, seasonal_2 Loss: 0.0239 | 0.0247
Epoch 97/300, seasonal_2 Loss: 0.0235 | 0.0263
Epoch 98/300, seasonal_2 Loss: 0.0304 | 0.0496
Epoch 99/300, seasonal_2 Loss: 0.0377 | 0.0395
Epoch 100/300, seasonal_2 Loss: 0.0417 | 0.2010
Epoch 101/300, seasonal_2 Loss: 0.0362 | 0.0442
Epoch 102/300, seasonal_2 Loss: 0.0405 | 0.0279
Epoch 103/300, seasonal_2 Loss: 0.0257 | 0.0281
Epoch 104/300, seasonal_2 Loss: 0.0243 | 0.0286
Epoch 105/300, seasonal_2 Loss: 0.0224 | 0.0295
Epoch 106/300, seasonal_2 Loss: 0.0221 | 0.0304
Epoch 107/300, seasonal_2 Loss: 0.0214 | 0.0299
Epoch 108/300, seasonal_2 Loss: 0.0209 | 0.0303
Epoch 109/300, seasonal_2 Loss: 0.0204 | 0.0287
Epoch 110/300, seasonal_2 Loss: 0.0200 | 0.0304
Epoch 111/300, seasonal_2 Loss: 0.0199 | 0.0280
Epoch 112/300, seasonal_2 Loss: 0.0195 | 0.0288
Epoch 113/300, seasonal_2 Loss: 0.0192 | 0.0281
Epoch 114/300, seasonal_2 Loss: 0.0193 | 0.0295
Epoch 115/300, seasonal_2 Loss: 0.0195 | 0.0291
Epoch 116/300, seasonal_2 Loss: 0.0193 | 0.0322
Epoch 117/300, seasonal_2 Loss: 0.0193 | 0.0291
Epoch 118/300, seasonal_2 Loss: 0.0192 | 0.0317
Epoch 119/300, seasonal_2 Loss: 0.0187 | 0.0322
Epoch 120/300, seasonal_2 Loss: 0.0181 | 0.0355
Epoch 121/300, seasonal_2 Loss: 0.0177 | 0.0316
Epoch 122/300, seasonal_2 Loss: 0.0174 | 0.0299
Epoch 123/300, seasonal_2 Loss: 0.0173 | 0.0262
Epoch 124/300, seasonal_2 Loss: 0.0172 | 0.0301
Epoch 125/300, seasonal_2 Loss: 0.0169 | 0.0330
Epoch 126/300, seasonal_2 Loss: 0.0169 | 0.0415
Epoch 127/300, seasonal_2 Loss: 0.0171 | 0.0302
Epoch 128/300, seasonal_2 Loss: 0.0177 | 0.0390
Epoch 129/300, seasonal_2 Loss: 0.0182 | 0.0296
Epoch 130/300, seasonal_2 Loss: 0.0180 | 0.0427
Epoch 131/300, seasonal_2 Loss: 0.0170 | 0.0274
Epoch 132/300, seasonal_2 Loss: 0.0160 | 0.0312
Epoch 133/300, seasonal_2 Loss: 0.0162 | 0.0375
Epoch 134/300, seasonal_2 Loss: 0.0157 | 0.0661
Epoch 135/300, seasonal_2 Loss: 0.0222 | 0.0503
Epoch 136/300, seasonal_2 Loss: 0.0391 | 0.0290
Epoch 137/300, seasonal_2 Loss: 0.0366 | 0.0253
Epoch 138/300, seasonal_2 Loss: 0.0353 | 0.0303
Epoch 139/300, seasonal_2 Loss: 0.0341 | 0.0347
Epoch 140/300, seasonal_2 Loss: 0.0165 | 0.0284
Epoch 141/300, seasonal_2 Loss: 0.0229 | 0.0505
Epoch 142/300, seasonal_2 Loss: 0.0413 | 0.0307
Epoch 143/300, seasonal_2 Loss: 0.0360 | 0.0300
Epoch 144/300, seasonal_2 Loss: 0.0338 | 0.0269
Epoch 145/300, seasonal_2 Loss: 0.0318 | 0.0281
Epoch 146/300, seasonal_2 Loss: 0.0317 | 0.0307
Epoch 147/300, seasonal_2 Loss: 0.0329 | 0.0299
Epoch 148/300, seasonal_2 Loss: 0.0287 | 0.0349
Epoch 149/300, seasonal_2 Loss: 0.0290 | 0.0316
Epoch 150/300, seasonal_2 Loss: 0.0303 | 0.0455
Epoch 151/300, seasonal_2 Loss: 0.0410 | 0.0370
Epoch 152/300, seasonal_2 Loss: 0.0329 | 0.0373
Epoch 153/300, seasonal_2 Loss: 0.0293 | 0.0299
Epoch 154/300, seasonal_2 Loss: 0.0216 | 0.0313
Epoch 155/300, seasonal_2 Loss: 0.0236 | 0.0337
Epoch 156/300, seasonal_2 Loss: 0.0218 | 0.0445
Epoch 157/300, seasonal_2 Loss: 0.0165 | 0.0691
Epoch 158/300, seasonal_2 Loss: 0.0149 | 0.0719
Epoch 159/300, seasonal_2 Loss: 0.0147 | 0.0289
Epoch 160/300, seasonal_2 Loss: 0.0141 | 0.0326
Epoch 161/300, seasonal_2 Loss: 0.0137 | 0.0340
Epoch 162/300, seasonal_2 Loss: 0.0135 | 0.0328
Epoch 163/300, seasonal_2 Loss: 0.0132 | 0.0335
Epoch 164/300, seasonal_2 Loss: 0.0130 | 0.0318
Epoch 165/300, seasonal_2 Loss: 0.0128 | 0.0325
Epoch 166/300, seasonal_2 Loss: 0.0126 | 0.0325
Epoch 167/300, seasonal_2 Loss: 0.0125 | 0.0320
Epoch 168/300, seasonal_2 Loss: 0.0123 | 0.0322
Epoch 169/300, seasonal_2 Loss: 0.0121 | 0.0311
Epoch 170/300, seasonal_2 Loss: 0.0119 | 0.0317
Epoch 171/300, seasonal_2 Loss: 0.0119 | 0.0311
Epoch 172/300, seasonal_2 Loss: 0.0118 | 0.0301
Epoch 173/300, seasonal_2 Loss: 0.0118 | 0.0319
Epoch 174/300, seasonal_2 Loss: 0.0119 | 0.0310
Epoch 175/300, seasonal_2 Loss: 0.0120 | 0.0326
Epoch 176/300, seasonal_2 Loss: 0.0124 | 0.0275
Epoch 177/300, seasonal_2 Loss: 0.0125 | 0.0300
Epoch 178/300, seasonal_2 Loss: 0.0127 | 0.0311
Epoch 179/300, seasonal_2 Loss: 0.0122 | 0.0332
Epoch 180/300, seasonal_2 Loss: 0.0123 | 0.0329
Epoch 181/300, seasonal_2 Loss: 0.0123 | 0.0393
Epoch 182/300, seasonal_2 Loss: 0.0123 | 0.0415
Epoch 183/300, seasonal_2 Loss: 0.0120 | 0.0498
Epoch 184/300, seasonal_2 Loss: 0.0118 | 0.0669
Epoch 185/300, seasonal_2 Loss: 0.0116 | 0.0317
Epoch 186/300, seasonal_2 Loss: 0.0115 | 0.0255
Epoch 187/300, seasonal_2 Loss: 0.0113 | 0.0323
Epoch 188/300, seasonal_2 Loss: 0.0113 | 0.0391
Epoch 189/300, seasonal_2 Loss: 0.0182 | 0.0414
Epoch 190/300, seasonal_2 Loss: 0.0223 | 0.0787
Epoch 191/300, seasonal_2 Loss: 0.0227 | 0.0489
Epoch 192/300, seasonal_2 Loss: 0.0129 | 0.0333
Epoch 193/300, seasonal_2 Loss: 0.0117 | 0.0331
Epoch 194/300, seasonal_2 Loss: 0.0109 | 0.0329
Epoch 195/300, seasonal_2 Loss: 0.0104 | 0.0327
Epoch 196/300, seasonal_2 Loss: 0.0101 | 0.0324
Epoch 197/300, seasonal_2 Loss: 0.0098 | 0.0319
Epoch 198/300, seasonal_2 Loss: 0.0096 | 0.0316
Epoch 199/300, seasonal_2 Loss: 0.0094 | 0.0312
Epoch 200/300, seasonal_2 Loss: 0.0092 | 0.0307
Epoch 201/300, seasonal_2 Loss: 0.0090 | 0.0301
Epoch 202/300, seasonal_2 Loss: 0.0088 | 0.0294
Epoch 203/300, seasonal_2 Loss: 0.0087 | 0.0287
Epoch 204/300, seasonal_2 Loss: 0.0085 | 0.0282
Epoch 205/300, seasonal_2 Loss: 0.0084 | 0.0277
Epoch 206/300, seasonal_2 Loss: 0.0083 | 0.0275
Epoch 207/300, seasonal_2 Loss: 0.0081 | 0.0272
Epoch 208/300, seasonal_2 Loss: 0.0080 | 0.0271
Epoch 209/300, seasonal_2 Loss: 0.0080 | 0.0271
Epoch 210/300, seasonal_2 Loss: 0.0079 | 0.0267
Epoch 211/300, seasonal_2 Loss: 0.0078 | 0.0263
Epoch 212/300, seasonal_2 Loss: 0.0076 | 0.0262
Epoch 213/300, seasonal_2 Loss: 0.0075 | 0.0254
Epoch 214/300, seasonal_2 Loss: 0.0074 | 0.0244
Epoch 215/300, seasonal_2 Loss: 0.0073 | 0.0245
Epoch 216/300, seasonal_2 Loss: 0.0073 | 0.0245
Epoch 217/300, seasonal_2 Loss: 0.0074 | 0.0239
Epoch 218/300, seasonal_2 Loss: 0.0074 | 0.0238
Epoch 219/300, seasonal_2 Loss: 0.0073 | 0.0251
Epoch 220/300, seasonal_2 Loss: 0.0073 | 0.0238
Epoch 221/300, seasonal_2 Loss: 0.0073 | 0.0242
Epoch 222/300, seasonal_2 Loss: 0.0073 | 0.0243
Epoch 223/300, seasonal_2 Loss: 0.0071 | 0.0245
Epoch 224/300, seasonal_2 Loss: 0.0070 | 0.0232
Epoch 225/300, seasonal_2 Loss: 0.0069 | 0.0243
Epoch 226/300, seasonal_2 Loss: 0.0070 | 0.0227
Epoch 227/300, seasonal_2 Loss: 0.0072 | 0.0232
Epoch 228/300, seasonal_2 Loss: 0.0073 | 0.0245
Epoch 229/300, seasonal_2 Loss: 0.0072 | 0.0223
Epoch 230/300, seasonal_2 Loss: 0.0071 | 0.0231
Epoch 231/300, seasonal_2 Loss: 0.0072 | 0.0302
Epoch 232/300, seasonal_2 Loss: 0.0071 | 0.0445
Epoch 233/300, seasonal_2 Loss: 0.0070 | 0.0397
Epoch 234/300, seasonal_2 Loss: 0.0070 | 0.0250
Epoch 235/300, seasonal_2 Loss: 0.0072 | 0.0247
Epoch 236/300, seasonal_2 Loss: 0.0069 | 0.0247
Epoch 237/300, seasonal_2 Loss: 0.0067 | 0.0256
Epoch 238/300, seasonal_2 Loss: 0.0065 | 0.0291
Epoch 239/300, seasonal_2 Loss: 0.0064 | 0.0254
Epoch 240/300, seasonal_2 Loss: 0.0063 | 0.0246
Epoch 241/300, seasonal_2 Loss: 0.0063 | 0.0256
Epoch 242/300, seasonal_2 Loss: 0.0062 | 0.0251
Epoch 243/300, seasonal_2 Loss: 0.0062 | 0.0260
Epoch 244/300, seasonal_2 Loss: 0.0061 | 0.0263
Epoch 245/300, seasonal_2 Loss: 0.0060 | 0.0249
Epoch 246/300, seasonal_2 Loss: 0.0059 | 0.0285
Epoch 247/300, seasonal_2 Loss: 0.0059 | 0.0250
Epoch 248/300, seasonal_2 Loss: 0.0058 | 0.0299
Epoch 249/300, seasonal_2 Loss: 0.0058 | 0.0273
Epoch 250/300, seasonal_2 Loss: 0.0058 | 0.0318
Epoch 251/300, seasonal_2 Loss: 0.0057 | 0.0303
Epoch 252/300, seasonal_2 Loss: 0.0056 | 0.0299
Epoch 253/300, seasonal_2 Loss: 0.0055 | 0.0308
Epoch 254/300, seasonal_2 Loss: 0.0056 | 0.0296
Epoch 255/300, seasonal_2 Loss: 0.0056 | 0.0303
Epoch 256/300, seasonal_2 Loss: 0.0058 | 0.0298
Epoch 257/300, seasonal_2 Loss: 0.0057 | 0.0297
Epoch 258/300, seasonal_2 Loss: 0.0057 | 0.0282
Epoch 259/300, seasonal_2 Loss: 0.0058 | 0.0288
Epoch 260/300, seasonal_2 Loss: 0.0061 | 0.0290
Epoch 261/300, seasonal_2 Loss: 0.0067 | 0.0643
Epoch 262/300, seasonal_2 Loss: 0.0073 | 0.1037
Epoch 263/300, seasonal_2 Loss: 0.0076 | 0.0273
Epoch 264/300, seasonal_2 Loss: 0.0062 | 0.0253
Epoch 265/300, seasonal_2 Loss: 0.0054 | 0.0248
Epoch 266/300, seasonal_2 Loss: 0.0051 | 0.0251
Epoch 267/300, seasonal_2 Loss: 0.0050 | 0.0253
Epoch 268/300, seasonal_2 Loss: 0.0049 | 0.0257
Epoch 269/300, seasonal_2 Loss: 0.0048 | 0.0259
Epoch 270/300, seasonal_2 Loss: 0.0047 | 0.0260
Epoch 271/300, seasonal_2 Loss: 0.0046 | 0.0263
Epoch 272/300, seasonal_2 Loss: 0.0045 | 0.0267
Epoch 273/300, seasonal_2 Loss: 0.0044 | 0.0271
Epoch 274/300, seasonal_2 Loss: 0.0043 | 0.0265
Epoch 275/300, seasonal_2 Loss: 0.0045 | 0.0266
Epoch 276/300, seasonal_2 Loss: 0.0045 | 0.0262
Epoch 277/300, seasonal_2 Loss: 0.0044 | 0.0258
Epoch 278/300, seasonal_2 Loss: 0.0042 | 0.0265
Epoch 279/300, seasonal_2 Loss: 0.0042 | 0.0260
Epoch 280/300, seasonal_2 Loss: 0.0042 | 0.0267
Epoch 281/300, seasonal_2 Loss: 0.0042 | 0.0261
Epoch 282/300, seasonal_2 Loss: 0.0042 | 0.0275
Epoch 283/300, seasonal_2 Loss: 0.0041 | 0.0263
Epoch 284/300, seasonal_2 Loss: 0.0041 | 0.0255
Epoch 285/300, seasonal_2 Loss: 0.0040 | 0.0266
Epoch 286/300, seasonal_2 Loss: 0.0040 | 0.0252
Epoch 287/300, seasonal_2 Loss: 0.0040 | 0.0267
Epoch 288/300, seasonal_2 Loss: 0.0039 | 0.0263
Epoch 289/300, seasonal_2 Loss: 0.0039 | 0.0267
Epoch 290/300, seasonal_2 Loss: 0.0039 | 0.0273
Epoch 291/300, seasonal_2 Loss: 0.0040 | 0.0275
Epoch 292/300, seasonal_2 Loss: 0.0040 | 0.0282
Epoch 293/300, seasonal_2 Loss: 0.0041 | 0.0287
Epoch 294/300, seasonal_2 Loss: 0.0042 | 0.0300
Epoch 295/300, seasonal_2 Loss: 0.0043 | 0.0288
Epoch 296/300, seasonal_2 Loss: 0.0044 | 0.0280
Epoch 297/300, seasonal_2 Loss: 0.0044 | 0.0292
Epoch 298/300, seasonal_2 Loss: 0.0043 | 0.0295
Epoch 299/300, seasonal_2 Loss: 0.0042 | 0.0290
Epoch 300/300, seasonal_2 Loss: 0.0043 | 0.0305
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.8022548914549998, 'learning_rate': 0.00048664304242939846, 'batch_size': 223, 'step_size': 15, 'gamma': 0.9035859332931515}
Epoch 1/300, seasonal_3 Loss: 0.4957 | 0.3023
Epoch 2/300, seasonal_3 Loss: 0.2769 | 0.4749
Epoch 3/300, seasonal_3 Loss: 0.3550 | 0.2659
Epoch 4/300, seasonal_3 Loss: 0.2408 | 0.2540
Epoch 5/300, seasonal_3 Loss: 0.2598 | 0.1476
Epoch 6/300, seasonal_3 Loss: 0.1626 | 0.1325
Epoch 7/300, seasonal_3 Loss: 0.1597 | 0.1298
Epoch 8/300, seasonal_3 Loss: 0.1420 | 0.1167
Epoch 9/300, seasonal_3 Loss: 0.1481 | 0.1083
Epoch 10/300, seasonal_3 Loss: 0.1276 | 0.1271
Epoch 11/300, seasonal_3 Loss: 0.1389 | 0.0936
Epoch 12/300, seasonal_3 Loss: 0.1233 | 0.1066
Epoch 13/300, seasonal_3 Loss: 0.1278 | 0.0921
Epoch 14/300, seasonal_3 Loss: 0.1188 | 0.0904
Epoch 15/300, seasonal_3 Loss: 0.1137 | 0.0783
Epoch 16/300, seasonal_3 Loss: 0.1066 | 0.0743
Epoch 17/300, seasonal_3 Loss: 0.1070 | 0.0720
Epoch 18/300, seasonal_3 Loss: 0.1037 | 0.0715
Epoch 19/300, seasonal_3 Loss: 0.1028 | 0.0699
Epoch 20/300, seasonal_3 Loss: 0.1015 | 0.0670
Epoch 21/300, seasonal_3 Loss: 0.1006 | 0.0740
Epoch 22/300, seasonal_3 Loss: 0.1054 | 0.0707
Epoch 23/300, seasonal_3 Loss: 0.1074 | 0.0710
Epoch 24/300, seasonal_3 Loss: 0.1115 | 0.1506
Epoch 25/300, seasonal_3 Loss: 0.1426 | 0.1358
Epoch 26/300, seasonal_3 Loss: 0.1289 | 0.0903
Epoch 27/300, seasonal_3 Loss: 0.1306 | 0.0820
Epoch 28/300, seasonal_3 Loss: 0.1271 | 0.0864
Epoch 29/300, seasonal_3 Loss: 0.1170 | 0.0759
Epoch 30/300, seasonal_3 Loss: 0.1141 | 0.0765
Epoch 31/300, seasonal_3 Loss: 0.1178 | 0.1028
Epoch 32/300, seasonal_3 Loss: 0.1109 | 0.0699
Epoch 33/300, seasonal_3 Loss: 0.1261 | 0.1289
Epoch 34/300, seasonal_3 Loss: 0.1227 | 0.0804
Epoch 35/300, seasonal_3 Loss: 0.1223 | 0.0945
Epoch 36/300, seasonal_3 Loss: 0.1128 | 0.0720
Epoch 37/300, seasonal_3 Loss: 0.1078 | 0.0743
Epoch 38/300, seasonal_3 Loss: 0.1011 | 0.0643
Epoch 39/300, seasonal_3 Loss: 0.1007 | 0.0641
Epoch 40/300, seasonal_3 Loss: 0.0969 | 0.0604
Epoch 41/300, seasonal_3 Loss: 0.0979 | 0.0612
Epoch 42/300, seasonal_3 Loss: 0.0953 | 0.0577
Epoch 43/300, seasonal_3 Loss: 0.0978 | 0.0627
Epoch 44/300, seasonal_3 Loss: 0.0964 | 0.0562
Epoch 45/300, seasonal_3 Loss: 0.1032 | 0.0750
Epoch 46/300, seasonal_3 Loss: 0.1006 | 0.0550
Epoch 47/300, seasonal_3 Loss: 0.1100 | 0.1062
Epoch 48/300, seasonal_3 Loss: 0.1079 | 0.0605
Epoch 49/300, seasonal_3 Loss: 0.1130 | 0.1110
Epoch 50/300, seasonal_3 Loss: 0.1059 | 0.0595
Epoch 51/300, seasonal_3 Loss: 0.1084 | 0.1147
Epoch 52/300, seasonal_3 Loss: 0.1013 | 0.0588
Epoch 53/300, seasonal_3 Loss: 0.1018 | 0.0978
Epoch 54/300, seasonal_3 Loss: 0.0930 | 0.0547
Epoch 55/300, seasonal_3 Loss: 0.0933 | 0.0827
Epoch 56/300, seasonal_3 Loss: 0.0888 | 0.0509
Epoch 57/300, seasonal_3 Loss: 0.0892 | 0.0702
Epoch 58/300, seasonal_3 Loss: 0.0857 | 0.0488
Epoch 59/300, seasonal_3 Loss: 0.0860 | 0.0605
Epoch 60/300, seasonal_3 Loss: 0.0836 | 0.0479
Epoch 61/300, seasonal_3 Loss: 0.0835 | 0.0585
Epoch 62/300, seasonal_3 Loss: 0.0821 | 0.0466
Epoch 63/300, seasonal_3 Loss: 0.0825 | 0.0564
Epoch 64/300, seasonal_3 Loss: 0.0809 | 0.0454
Epoch 65/300, seasonal_3 Loss: 0.0815 | 0.0547
Epoch 66/300, seasonal_3 Loss: 0.0801 | 0.0446
Epoch 67/300, seasonal_3 Loss: 0.0810 | 0.0538
Epoch 68/300, seasonal_3 Loss: 0.0800 | 0.0443
Epoch 69/300, seasonal_3 Loss: 0.0807 | 0.0551
Epoch 70/300, seasonal_3 Loss: 0.0798 | 0.0444
Epoch 71/300, seasonal_3 Loss: 0.0808 | 0.0548
Epoch 72/300, seasonal_3 Loss: 0.0811 | 0.0440
Epoch 73/300, seasonal_3 Loss: 0.0817 | 0.0525
Epoch 74/300, seasonal_3 Loss: 0.0803 | 0.0440
Epoch 75/300, seasonal_3 Loss: 0.0821 | 0.0534
Epoch 76/300, seasonal_3 Loss: 0.0822 | 0.0454
Epoch 77/300, seasonal_3 Loss: 0.0811 | 0.0523
Epoch 78/300, seasonal_3 Loss: 0.0819 | 0.0445
Epoch 79/300, seasonal_3 Loss: 0.0798 | 0.0524
Epoch 80/300, seasonal_3 Loss: 0.0786 | 0.0433
Epoch 81/300, seasonal_3 Loss: 0.0774 | 0.0493
Epoch 82/300, seasonal_3 Loss: 0.0771 | 0.0426
Epoch 83/300, seasonal_3 Loss: 0.0769 | 0.0477
Epoch 84/300, seasonal_3 Loss: 0.0760 | 0.0415
Epoch 85/300, seasonal_3 Loss: 0.0758 | 0.0473
Epoch 86/300, seasonal_3 Loss: 0.0753 | 0.0421
Epoch 87/300, seasonal_3 Loss: 0.0751 | 0.0460
Epoch 88/300, seasonal_3 Loss: 0.0744 | 0.0408
Epoch 89/300, seasonal_3 Loss: 0.0744 | 0.0447
Epoch 90/300, seasonal_3 Loss: 0.0740 | 0.0410
Epoch 91/300, seasonal_3 Loss: 0.0737 | 0.0447
Epoch 92/300, seasonal_3 Loss: 0.0734 | 0.0402
Epoch 93/300, seasonal_3 Loss: 0.0732 | 0.0435
Epoch 94/300, seasonal_3 Loss: 0.0729 | 0.0400
Epoch 95/300, seasonal_3 Loss: 0.0726 | 0.0428
Epoch 96/300, seasonal_3 Loss: 0.0725 | 0.0398
Epoch 97/300, seasonal_3 Loss: 0.0722 | 0.0424
Epoch 98/300, seasonal_3 Loss: 0.0721 | 0.0395
Epoch 99/300, seasonal_3 Loss: 0.0719 | 0.0422
Epoch 100/300, seasonal_3 Loss: 0.0716 | 0.0392
Epoch 101/300, seasonal_3 Loss: 0.0714 | 0.0416
Epoch 102/300, seasonal_3 Loss: 0.0712 | 0.0389
Epoch 103/300, seasonal_3 Loss: 0.0710 | 0.0410
Epoch 104/300, seasonal_3 Loss: 0.0708 | 0.0386
Epoch 105/300, seasonal_3 Loss: 0.0706 | 0.0406
Epoch 106/300, seasonal_3 Loss: 0.0704 | 0.0383
Epoch 107/300, seasonal_3 Loss: 0.0701 | 0.0403
Epoch 108/300, seasonal_3 Loss: 0.0699 | 0.0380
Epoch 109/300, seasonal_3 Loss: 0.0697 | 0.0396
Epoch 110/300, seasonal_3 Loss: 0.0695 | 0.0376
Epoch 111/300, seasonal_3 Loss: 0.0693 | 0.0391
Epoch 112/300, seasonal_3 Loss: 0.0692 | 0.0373
Epoch 113/300, seasonal_3 Loss: 0.0689 | 0.0387
Epoch 114/300, seasonal_3 Loss: 0.0688 | 0.0370
Epoch 115/300, seasonal_3 Loss: 0.0686 | 0.0384
Epoch 116/300, seasonal_3 Loss: 0.0685 | 0.0368
Epoch 117/300, seasonal_3 Loss: 0.0683 | 0.0380
Epoch 118/300, seasonal_3 Loss: 0.0681 | 0.0366
Epoch 119/300, seasonal_3 Loss: 0.0679 | 0.0376
Epoch 120/300, seasonal_3 Loss: 0.0678 | 0.0365
Epoch 121/300, seasonal_3 Loss: 0.0676 | 0.0375
Epoch 122/300, seasonal_3 Loss: 0.0677 | 0.0365
Epoch 123/300, seasonal_3 Loss: 0.0677 | 0.0377
Epoch 124/300, seasonal_3 Loss: 0.0680 | 0.0375
Epoch 125/300, seasonal_3 Loss: 0.0686 | 0.0388
Epoch 126/300, seasonal_3 Loss: 0.0698 | 0.0371
Epoch 127/300, seasonal_3 Loss: 0.0699 | 0.0375
Epoch 128/300, seasonal_3 Loss: 0.0691 | 0.0365
Epoch 129/300, seasonal_3 Loss: 0.0683 | 0.0379
Epoch 130/300, seasonal_3 Loss: 0.0688 | 0.0366
Epoch 131/300, seasonal_3 Loss: 0.0686 | 0.0362
Epoch 132/300, seasonal_3 Loss: 0.0672 | 0.0358
Epoch 133/300, seasonal_3 Loss: 0.0679 | 0.0361
Epoch 134/300, seasonal_3 Loss: 0.0677 | 0.0368
Epoch 135/300, seasonal_3 Loss: 0.0676 | 0.0371
Epoch 136/300, seasonal_3 Loss: 0.0676 | 0.0358
Epoch 137/300, seasonal_3 Loss: 0.0673 | 0.0358
Epoch 138/300, seasonal_3 Loss: 0.0669 | 0.0363
Epoch 139/300, seasonal_3 Loss: 0.0666 | 0.0370
Epoch 140/300, seasonal_3 Loss: 0.0663 | 0.0356
Epoch 141/300, seasonal_3 Loss: 0.0661 | 0.0354
Epoch 142/300, seasonal_3 Loss: 0.0659 | 0.0356
Epoch 143/300, seasonal_3 Loss: 0.0657 | 0.0360
Epoch 144/300, seasonal_3 Loss: 0.0655 | 0.0354
Epoch 145/300, seasonal_3 Loss: 0.0654 | 0.0352
Epoch 146/300, seasonal_3 Loss: 0.0653 | 0.0353
Epoch 147/300, seasonal_3 Loss: 0.0652 | 0.0355
Epoch 148/300, seasonal_3 Loss: 0.0651 | 0.0352
Epoch 149/300, seasonal_3 Loss: 0.0650 | 0.0352
Epoch 150/300, seasonal_3 Loss: 0.0650 | 0.0352
Epoch 151/300, seasonal_3 Loss: 0.0649 | 0.0352
Epoch 152/300, seasonal_3 Loss: 0.0648 | 0.0351
Epoch 153/300, seasonal_3 Loss: 0.0647 | 0.0351
Epoch 154/300, seasonal_3 Loss: 0.0647 | 0.0351
Epoch 155/300, seasonal_3 Loss: 0.0646 | 0.0350
Epoch 156/300, seasonal_3 Loss: 0.0646 | 0.0350
Epoch 157/300, seasonal_3 Loss: 0.0645 | 0.0350
Epoch 158/300, seasonal_3 Loss: 0.0644 | 0.0350
Epoch 159/300, seasonal_3 Loss: 0.0644 | 0.0350
Epoch 160/300, seasonal_3 Loss: 0.0643 | 0.0349
Epoch 161/300, seasonal_3 Loss: 0.0642 | 0.0349
Epoch 162/300, seasonal_3 Loss: 0.0642 | 0.0349
Epoch 163/300, seasonal_3 Loss: 0.0641 | 0.0349
Epoch 164/300, seasonal_3 Loss: 0.0641 | 0.0348
Epoch 165/300, seasonal_3 Loss: 0.0640 | 0.0348
Epoch 166/300, seasonal_3 Loss: 0.0639 | 0.0348
Epoch 167/300, seasonal_3 Loss: 0.0639 | 0.0348
Epoch 168/300, seasonal_3 Loss: 0.0638 | 0.0348
Epoch 169/300, seasonal_3 Loss: 0.0638 | 0.0347
Epoch 170/300, seasonal_3 Loss: 0.0637 | 0.0347
Epoch 171/300, seasonal_3 Loss: 0.0637 | 0.0347
Epoch 172/300, seasonal_3 Loss: 0.0636 | 0.0347
Epoch 173/300, seasonal_3 Loss: 0.0636 | 0.0346
Epoch 174/300, seasonal_3 Loss: 0.0635 | 0.0346
Epoch 175/300, seasonal_3 Loss: 0.0635 | 0.0346
Epoch 176/300, seasonal_3 Loss: 0.0634 | 0.0346
Epoch 177/300, seasonal_3 Loss: 0.0634 | 0.0346
Epoch 178/300, seasonal_3 Loss: 0.0633 | 0.0346
Epoch 179/300, seasonal_3 Loss: 0.0633 | 0.0346
Epoch 180/300, seasonal_3 Loss: 0.0632 | 0.0345
Epoch 181/300, seasonal_3 Loss: 0.0632 | 0.0345
Epoch 182/300, seasonal_3 Loss: 0.0631 | 0.0345
Epoch 183/300, seasonal_3 Loss: 0.0631 | 0.0345
Epoch 184/300, seasonal_3 Loss: 0.0630 | 0.0344
Epoch 185/300, seasonal_3 Loss: 0.0630 | 0.0344
Epoch 186/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 187/300, seasonal_3 Loss: 0.0629 | 0.0344
Epoch 188/300, seasonal_3 Loss: 0.0629 | 0.0343
Epoch 189/300, seasonal_3 Loss: 0.0628 | 0.0343
Epoch 190/300, seasonal_3 Loss: 0.0628 | 0.0343
Epoch 191/300, seasonal_3 Loss: 0.0627 | 0.0343
Epoch 192/300, seasonal_3 Loss: 0.0627 | 0.0343
Epoch 193/300, seasonal_3 Loss: 0.0626 | 0.0343
Epoch 194/300, seasonal_3 Loss: 0.0626 | 0.0342
Epoch 195/300, seasonal_3 Loss: 0.0625 | 0.0342
Epoch 196/300, seasonal_3 Loss: 0.0625 | 0.0342
Epoch 197/300, seasonal_3 Loss: 0.0625 | 0.0342
Epoch 198/300, seasonal_3 Loss: 0.0624 | 0.0341
Epoch 199/300, seasonal_3 Loss: 0.0624 | 0.0341
Epoch 200/300, seasonal_3 Loss: 0.0624 | 0.0341
Epoch 201/300, seasonal_3 Loss: 0.0623 | 0.0341
Epoch 202/300, seasonal_3 Loss: 0.0623 | 0.0341
Epoch 203/300, seasonal_3 Loss: 0.0622 | 0.0341
Epoch 204/300, seasonal_3 Loss: 0.0622 | 0.0340
Epoch 205/300, seasonal_3 Loss: 0.0622 | 0.0340
Epoch 206/300, seasonal_3 Loss: 0.0621 | 0.0340
Epoch 207/300, seasonal_3 Loss: 0.0621 | 0.0339
Epoch 208/300, seasonal_3 Loss: 0.0621 | 0.0339
Epoch 209/300, seasonal_3 Loss: 0.0620 | 0.0339
Epoch 210/300, seasonal_3 Loss: 0.0620 | 0.0339
Epoch 211/300, seasonal_3 Loss: 0.0619 | 0.0340
Epoch 212/300, seasonal_3 Loss: 0.0619 | 0.0339
Epoch 213/300, seasonal_3 Loss: 0.0619 | 0.0339
Epoch 214/300, seasonal_3 Loss: 0.0618 | 0.0338
Epoch 215/300, seasonal_3 Loss: 0.0618 | 0.0338
Epoch 216/300, seasonal_3 Loss: 0.0618 | 0.0338
Epoch 217/300, seasonal_3 Loss: 0.0618 | 0.0338
Epoch 218/300, seasonal_3 Loss: 0.0617 | 0.0338
Epoch 219/300, seasonal_3 Loss: 0.0617 | 0.0338
Epoch 220/300, seasonal_3 Loss: 0.0616 | 0.0338
Epoch 221/300, seasonal_3 Loss: 0.0616 | 0.0338
Epoch 222/300, seasonal_3 Loss: 0.0616 | 0.0337
Epoch 223/300, seasonal_3 Loss: 0.0615 | 0.0337
Epoch 224/300, seasonal_3 Loss: 0.0615 | 0.0337
Epoch 225/300, seasonal_3 Loss: 0.0615 | 0.0337
Epoch 226/300, seasonal_3 Loss: 0.0615 | 0.0337
Epoch 227/300, seasonal_3 Loss: 0.0614 | 0.0337
Epoch 228/300, seasonal_3 Loss: 0.0614 | 0.0337
Epoch 229/300, seasonal_3 Loss: 0.0614 | 0.0337
Epoch 230/300, seasonal_3 Loss: 0.0613 | 0.0336
Epoch 231/300, seasonal_3 Loss: 0.0613 | 0.0336
Epoch 232/300, seasonal_3 Loss: 0.0613 | 0.0336
Epoch 233/300, seasonal_3 Loss: 0.0612 | 0.0336
Epoch 234/300, seasonal_3 Loss: 0.0612 | 0.0336
Epoch 235/300, seasonal_3 Loss: 0.0612 | 0.0336
Epoch 236/300, seasonal_3 Loss: 0.0612 | 0.0336
Epoch 237/300, seasonal_3 Loss: 0.0611 | 0.0336
Epoch 238/300, seasonal_3 Loss: 0.0611 | 0.0335
Epoch 239/300, seasonal_3 Loss: 0.0611 | 0.0335
Epoch 240/300, seasonal_3 Loss: 0.0611 | 0.0335
Epoch 241/300, seasonal_3 Loss: 0.0610 | 0.0335
Epoch 242/300, seasonal_3 Loss: 0.0610 | 0.0335
Epoch 243/300, seasonal_3 Loss: 0.0610 | 0.0335
Epoch 244/300, seasonal_3 Loss: 0.0609 | 0.0335
Epoch 245/300, seasonal_3 Loss: 0.0609 | 0.0335
Epoch 246/300, seasonal_3 Loss: 0.0609 | 0.0335
Epoch 247/300, seasonal_3 Loss: 0.0609 | 0.0335
Epoch 248/300, seasonal_3 Loss: 0.0608 | 0.0335
Epoch 249/300, seasonal_3 Loss: 0.0608 | 0.0335
Epoch 250/300, seasonal_3 Loss: 0.0608 | 0.0335
Epoch 251/300, seasonal_3 Loss: 0.0608 | 0.0335
Epoch 252/300, seasonal_3 Loss: 0.0608 | 0.0335
Epoch 253/300, seasonal_3 Loss: 0.0607 | 0.0335
Epoch 254/300, seasonal_3 Loss: 0.0607 | 0.0334
Epoch 255/300, seasonal_3 Loss: 0.0607 | 0.0334
Epoch 256/300, seasonal_3 Loss: 0.0607 | 0.0335
Epoch 257/300, seasonal_3 Loss: 0.0606 | 0.0334
Epoch 258/300, seasonal_3 Loss: 0.0606 | 0.0334
Epoch 259/300, seasonal_3 Loss: 0.0606 | 0.0334
Epoch 260/300, seasonal_3 Loss: 0.0606 | 0.0334
Epoch 261/300, seasonal_3 Loss: 0.0605 | 0.0334
Epoch 262/300, seasonal_3 Loss: 0.0605 | 0.0334
Epoch 263/300, seasonal_3 Loss: 0.0605 | 0.0334
Epoch 264/300, seasonal_3 Loss: 0.0605 | 0.0334
Epoch 265/300, seasonal_3 Loss: 0.0605 | 0.0334
Epoch 266/300, seasonal_3 Loss: 0.0604 | 0.0334
Epoch 267/300, seasonal_3 Loss: 0.0604 | 0.0334
Epoch 268/300, seasonal_3 Loss: 0.0604 | 0.0334
Epoch 269/300, seasonal_3 Loss: 0.0604 | 0.0334
Epoch 270/300, seasonal_3 Loss: 0.0604 | 0.0334
Epoch 271/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 272/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 273/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 274/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 275/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 276/300, seasonal_3 Loss: 0.0603 | 0.0334
Epoch 277/300, seasonal_3 Loss: 0.0602 | 0.0334
Epoch 278/300, seasonal_3 Loss: 0.0602 | 0.0334
Epoch 279/300, seasonal_3 Loss: 0.0602 | 0.0334
Epoch 280/300, seasonal_3 Loss: 0.0602 | 0.0334
Epoch 281/300, seasonal_3 Loss: 0.0602 | 0.0334
Epoch 282/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 283/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 284/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 285/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 286/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 287/300, seasonal_3 Loss: 0.0601 | 0.0334
Epoch 288/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 289/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 290/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 291/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 292/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 293/300, seasonal_3 Loss: 0.0600 | 0.0334
Epoch 294/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 295/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 296/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 297/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 298/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 299/300, seasonal_3 Loss: 0.0599 | 0.0334
Epoch 300/300, seasonal_3 Loss: 0.0599 | 0.0334
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.9882060110018639, 'learning_rate': 0.0008887268356908011, 'batch_size': 112, 'step_size': 4, 'gamma': 0.8811588807012298}
Epoch 1/300, resid Loss: 0.4748 | 0.1788
Epoch 2/300, resid Loss: 0.1485 | 0.1323
Epoch 3/300, resid Loss: 0.1234 | 0.1064
Epoch 4/300, resid Loss: 0.1106 | 0.0817
Epoch 5/300, resid Loss: 0.1107 | 0.0799
Epoch 6/300, resid Loss: 0.1236 | 0.1146
Epoch 7/300, resid Loss: 0.1123 | 0.0779
Epoch 8/300, resid Loss: 0.1051 | 0.0767
Epoch 9/300, resid Loss: 0.1084 | 0.0836
Epoch 10/300, resid Loss: 0.1147 | 0.0890
Epoch 11/300, resid Loss: 0.1086 | 0.0837
Epoch 12/300, resid Loss: 0.1019 | 0.0894
Epoch 13/300, resid Loss: 0.0997 | 0.0823
Epoch 14/300, resid Loss: 0.0940 | 0.0800
Epoch 15/300, resid Loss: 0.0987 | 0.0731
Epoch 16/300, resid Loss: 0.1104 | 0.0730
Epoch 17/300, resid Loss: 0.1339 | 0.0791
Epoch 18/300, resid Loss: 0.1316 | 0.0763
Epoch 19/300, resid Loss: 0.1135 | 0.0722
Epoch 20/300, resid Loss: 0.1260 | 0.0876
Epoch 21/300, resid Loss: 0.1505 | 0.1074
Epoch 22/300, resid Loss: 0.1267 | 0.0832
Epoch 23/300, resid Loss: 0.1083 | 0.0693
Epoch 24/300, resid Loss: 0.1320 | 0.0938
Epoch 25/300, resid Loss: 0.1585 | 0.2778
Epoch 26/300, resid Loss: 0.1167 | 0.0843
Epoch 27/300, resid Loss: 0.0908 | 0.0729
Epoch 28/300, resid Loss: 0.0852 | 0.0718
Epoch 29/300, resid Loss: 0.0793 | 0.0631
Epoch 30/300, resid Loss: 0.0795 | 0.0620
Epoch 31/300, resid Loss: 0.0799 | 0.0619
Epoch 32/300, resid Loss: 0.0784 | 0.0596
Epoch 33/300, resid Loss: 0.0767 | 0.0594
Epoch 34/300, resid Loss: 0.0759 | 0.0591
Epoch 35/300, resid Loss: 0.0754 | 0.0582
Epoch 36/300, resid Loss: 0.0749 | 0.0572
Epoch 37/300, resid Loss: 0.0745 | 0.0563
Epoch 38/300, resid Loss: 0.0742 | 0.0555
Epoch 39/300, resid Loss: 0.0739 | 0.0549
Epoch 40/300, resid Loss: 0.0736 | 0.0544
Epoch 41/300, resid Loss: 0.0733 | 0.0539
Epoch 42/300, resid Loss: 0.0731 | 0.0536
Epoch 43/300, resid Loss: 0.0729 | 0.0532
Epoch 44/300, resid Loss: 0.0727 | 0.0530
Epoch 45/300, resid Loss: 0.0725 | 0.0528
Epoch 46/300, resid Loss: 0.0723 | 0.0526
Epoch 47/300, resid Loss: 0.0721 | 0.0526
Epoch 48/300, resid Loss: 0.0719 | 0.0525
Epoch 49/300, resid Loss: 0.0718 | 0.0524
Epoch 50/300, resid Loss: 0.0716 | 0.0524
Epoch 51/300, resid Loss: 0.0714 | 0.0524
Epoch 52/300, resid Loss: 0.0713 | 0.0524
Epoch 53/300, resid Loss: 0.0712 | 0.0524
Epoch 54/300, resid Loss: 0.0710 | 0.0523
Epoch 55/300, resid Loss: 0.0709 | 0.0523
Epoch 56/300, resid Loss: 0.0708 | 0.0523
Epoch 57/300, resid Loss: 0.0707 | 0.0523
Epoch 58/300, resid Loss: 0.0706 | 0.0522
Epoch 59/300, resid Loss: 0.0705 | 0.0522
Epoch 60/300, resid Loss: 0.0705 | 0.0522
Epoch 61/300, resid Loss: 0.0704 | 0.0522
Epoch 62/300, resid Loss: 0.0703 | 0.0522
Epoch 63/300, resid Loss: 0.0702 | 0.0521
Epoch 64/300, resid Loss: 0.0702 | 0.0521
Epoch 65/300, resid Loss: 0.0701 | 0.0521
Epoch 66/300, resid Loss: 0.0700 | 0.0521
Epoch 67/300, resid Loss: 0.0700 | 0.0521
Epoch 68/300, resid Loss: 0.0699 | 0.0521
Epoch 69/300, resid Loss: 0.0699 | 0.0521
Epoch 70/300, resid Loss: 0.0698 | 0.0520
Epoch 71/300, resid Loss: 0.0698 | 0.0520
Epoch 72/300, resid Loss: 0.0697 | 0.0520
Epoch 73/300, resid Loss: 0.0697 | 0.0520
Epoch 74/300, resid Loss: 0.0697 | 0.0520
Epoch 75/300, resid Loss: 0.0696 | 0.0520
Epoch 76/300, resid Loss: 0.0696 | 0.0520
Epoch 77/300, resid Loss: 0.0696 | 0.0520
Epoch 78/300, resid Loss: 0.0695 | 0.0519
Epoch 79/300, resid Loss: 0.0695 | 0.0519
Epoch 80/300, resid Loss: 0.0695 | 0.0519
Epoch 81/300, resid Loss: 0.0694 | 0.0519
Epoch 82/300, resid Loss: 0.0694 | 0.0519
Epoch 83/300, resid Loss: 0.0694 | 0.0519
Epoch 84/300, resid Loss: 0.0694 | 0.0519
Epoch 85/300, resid Loss: 0.0693 | 0.0519
Epoch 86/300, resid Loss: 0.0693 | 0.0519
Epoch 87/300, resid Loss: 0.0693 | 0.0519
Epoch 88/300, resid Loss: 0.0693 | 0.0519
Epoch 89/300, resid Loss: 0.0693 | 0.0518
Epoch 90/300, resid Loss: 0.0693 | 0.0518
Epoch 91/300, resid Loss: 0.0692 | 0.0518
Epoch 92/300, resid Loss: 0.0692 | 0.0518
Epoch 93/300, resid Loss: 0.0692 | 0.0518
Epoch 94/300, resid Loss: 0.0692 | 0.0518
Epoch 95/300, resid Loss: 0.0692 | 0.0518
Epoch 96/300, resid Loss: 0.0692 | 0.0518
Epoch 97/300, resid Loss: 0.0692 | 0.0518
Epoch 98/300, resid Loss: 0.0692 | 0.0518
Epoch 99/300, resid Loss: 0.0692 | 0.0518
Epoch 100/300, resid Loss: 0.0691 | 0.0518
Epoch 101/300, resid Loss: 0.0691 | 0.0518
Epoch 102/300, resid Loss: 0.0691 | 0.0518
Epoch 103/300, resid Loss: 0.0691 | 0.0518
Epoch 104/300, resid Loss: 0.0691 | 0.0518
Epoch 105/300, resid Loss: 0.0691 | 0.0518
Epoch 106/300, resid Loss: 0.0691 | 0.0518
Epoch 107/300, resid Loss: 0.0691 | 0.0518
Epoch 108/300, resid Loss: 0.0691 | 0.0518
Epoch 109/300, resid Loss: 0.0691 | 0.0518
Epoch 110/300, resid Loss: 0.0691 | 0.0518
Epoch 111/300, resid Loss: 0.0691 | 0.0518
Epoch 112/300, resid Loss: 0.0691 | 0.0518
Epoch 113/300, resid Loss: 0.0691 | 0.0518
Epoch 114/300, resid Loss: 0.0691 | 0.0518
Epoch 115/300, resid Loss: 0.0691 | 0.0517
Epoch 116/300, resid Loss: 0.0691 | 0.0517
Epoch 117/300, resid Loss: 0.0690 | 0.0517
Epoch 118/300, resid Loss: 0.0690 | 0.0517
Epoch 119/300, resid Loss: 0.0690 | 0.0517
Epoch 120/300, resid Loss: 0.0690 | 0.0517
Epoch 121/300, resid Loss: 0.0690 | 0.0517
Epoch 122/300, resid Loss: 0.0690 | 0.0517
Epoch 123/300, resid Loss: 0.0690 | 0.0517
Epoch 124/300, resid Loss: 0.0690 | 0.0517
Epoch 125/300, resid Loss: 0.0690 | 0.0517
Epoch 126/300, resid Loss: 0.0690 | 0.0517
Epoch 127/300, resid Loss: 0.0690 | 0.0517
Epoch 128/300, resid Loss: 0.0690 | 0.0517
Epoch 129/300, resid Loss: 0.0690 | 0.0517
Epoch 130/300, resid Loss: 0.0690 | 0.0517
Epoch 131/300, resid Loss: 0.0690 | 0.0517
Epoch 132/300, resid Loss: 0.0690 | 0.0517
Epoch 133/300, resid Loss: 0.0690 | 0.0517
Epoch 134/300, resid Loss: 0.0690 | 0.0517
Epoch 135/300, resid Loss: 0.0690 | 0.0517
Epoch 136/300, resid Loss: 0.0690 | 0.0517
Epoch 137/300, resid Loss: 0.0690 | 0.0517
Epoch 138/300, resid Loss: 0.0690 | 0.0517
Epoch 139/300, resid Loss: 0.0690 | 0.0517
Epoch 140/300, resid Loss: 0.0690 | 0.0517
Epoch 141/300, resid Loss: 0.0690 | 0.0517
Epoch 142/300, resid Loss: 0.0690 | 0.0517
Epoch 143/300, resid Loss: 0.0690 | 0.0517
Epoch 144/300, resid Loss: 0.0690 | 0.0517
Epoch 145/300, resid Loss: 0.0690 | 0.0517
Epoch 146/300, resid Loss: 0.0690 | 0.0517
Epoch 147/300, resid Loss: 0.0690 | 0.0517
Epoch 148/300, resid Loss: 0.0690 | 0.0517
Epoch 149/300, resid Loss: 0.0690 | 0.0517
Epoch 150/300, resid Loss: 0.0690 | 0.0517
Epoch 151/300, resid Loss: 0.0690 | 0.0517
Epoch 152/300, resid Loss: 0.0690 | 0.0517
Epoch 153/300, resid Loss: 0.0690 | 0.0517
Epoch 154/300, resid Loss: 0.0690 | 0.0517
Epoch 155/300, resid Loss: 0.0690 | 0.0517
Epoch 156/300, resid Loss: 0.0690 | 0.0517
Epoch 157/300, resid Loss: 0.0690 | 0.0517
Epoch 158/300, resid Loss: 0.0690 | 0.0517
Epoch 159/300, resid Loss: 0.0690 | 0.0517
Epoch 160/300, resid Loss: 0.0690 | 0.0517
Epoch 161/300, resid Loss: 0.0690 | 0.0517
Epoch 162/300, resid Loss: 0.0690 | 0.0517
Epoch 163/300, resid Loss: 0.0690 | 0.0517
Epoch 164/300, resid Loss: 0.0690 | 0.0517
Epoch 165/300, resid Loss: 0.0690 | 0.0517
Epoch 166/300, resid Loss: 0.0690 | 0.0517
Epoch 167/300, resid Loss: 0.0690 | 0.0517
Epoch 168/300, resid Loss: 0.0690 | 0.0517
Epoch 169/300, resid Loss: 0.0690 | 0.0517
Epoch 170/300, resid Loss: 0.0690 | 0.0517
Epoch 171/300, resid Loss: 0.0690 | 0.0517
Epoch 172/300, resid Loss: 0.0690 | 0.0517
Epoch 173/300, resid Loss: 0.0690 | 0.0517
Epoch 174/300, resid Loss: 0.0690 | 0.0517
Epoch 175/300, resid Loss: 0.0690 | 0.0517
Epoch 176/300, resid Loss: 0.0690 | 0.0517
Epoch 177/300, resid Loss: 0.0690 | 0.0517
Epoch 178/300, resid Loss: 0.0690 | 0.0517
Epoch 179/300, resid Loss: 0.0690 | 0.0517
Epoch 180/300, resid Loss: 0.0690 | 0.0517
Epoch 181/300, resid Loss: 0.0690 | 0.0517
Epoch 182/300, resid Loss: 0.0690 | 0.0517
Epoch 183/300, resid Loss: 0.0690 | 0.0517
Epoch 184/300, resid Loss: 0.0690 | 0.0517
Epoch 185/300, resid Loss: 0.0690 | 0.0517
Epoch 186/300, resid Loss: 0.0690 | 0.0517
Epoch 187/300, resid Loss: 0.0690 | 0.0517
Epoch 188/300, resid Loss: 0.0690 | 0.0517
Epoch 189/300, resid Loss: 0.0690 | 0.0517
Epoch 190/300, resid Loss: 0.0690 | 0.0517
Epoch 191/300, resid Loss: 0.0690 | 0.0517
Epoch 192/300, resid Loss: 0.0690 | 0.0517
Epoch 193/300, resid Loss: 0.0690 | 0.0517
Epoch 194/300, resid Loss: 0.0690 | 0.0517
Epoch 195/300, resid Loss: 0.0690 | 0.0517
Epoch 196/300, resid Loss: 0.0690 | 0.0517
Epoch 197/300, resid Loss: 0.0690 | 0.0517
Epoch 198/300, resid Loss: 0.0690 | 0.0517
Epoch 199/300, resid Loss: 0.0690 | 0.0517
Epoch 200/300, resid Loss: 0.0690 | 0.0517
Epoch 201/300, resid Loss: 0.0690 | 0.0517
Epoch 202/300, resid Loss: 0.0690 | 0.0517
Epoch 203/300, resid Loss: 0.0690 | 0.0517
Epoch 204/300, resid Loss: 0.0690 | 0.0517
Early stopping for resid
Runtime (seconds): 1891.0804488658905
0.0003957799553701948
[155.78702]
[-1.4985368]
[-3.8029208]
[10.193819]
[2.4658828]
[8.537048]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 21.230867342092097
RMSE: 4.607696533203125
MAE: 4.607696533203125
R-squared: nan
[171.68231]
