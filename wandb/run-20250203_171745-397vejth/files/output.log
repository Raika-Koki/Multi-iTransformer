ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 17:17:48,845][0m A new study created in memory with name: no-name-c1120009-8242-459c-8351-e6998b4acc14[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-03 17:18:17,522][0m Trial 0 finished with value: 0.28610815806491857 and parameters: {'observation_period_num': 181, 'train_rates': 0.7159252608000077, 'learning_rate': 2.1634244537883835e-05, 'batch_size': 188, 'step_size': 10, 'gamma': 0.8905013120177007}. Best is trial 0 with value: 0.28610815806491857.[0m
[32m[I 2025-02-03 17:18:41,888][0m Trial 1 finished with value: 0.17154356837272644 and parameters: {'observation_period_num': 99, 'train_rates': 0.9141458107720438, 'learning_rate': 1.675656483710311e-05, 'batch_size': 256, 'step_size': 7, 'gamma': 0.979654733548533}. Best is trial 1 with value: 0.17154356837272644.[0m
Early stopping at epoch 52
[32m[I 2025-02-03 17:19:18,319][0m Trial 2 finished with value: 0.18855114515983698 and parameters: {'observation_period_num': 176, 'train_rates': 0.94065156867035, 'learning_rate': 0.0007614988182321255, 'batch_size': 74, 'step_size': 1, 'gamma': 0.772210000272842}. Best is trial 1 with value: 0.17154356837272644.[0m
[32m[I 2025-02-03 17:20:05,332][0m Trial 3 finished with value: 0.12750164570949846 and parameters: {'observation_period_num': 193, 'train_rates': 0.8290965198150105, 'learning_rate': 0.0005021043020624506, 'batch_size': 106, 'step_size': 12, 'gamma': 0.7585377850771404}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:20:58,236][0m Trial 4 finished with value: 3.0629870099711503 and parameters: {'observation_period_num': 107, 'train_rates': 0.9028030280024748, 'learning_rate': 1.1041638567820036e-06, 'batch_size': 110, 'step_size': 1, 'gamma': 0.9245180294578391}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:21:23,358][0m Trial 5 finished with value: 0.21694508663465067 and parameters: {'observation_period_num': 205, 'train_rates': 0.7175549530473178, 'learning_rate': 0.00018633218561646343, 'batch_size': 213, 'step_size': 1, 'gamma': 0.9464482744700057}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:21:56,162][0m Trial 6 finished with value: 0.20834797620773315 and parameters: {'observation_period_num': 145, 'train_rates': 0.9509341065752868, 'learning_rate': 2.6483371663295426e-05, 'batch_size': 205, 'step_size': 2, 'gamma': 0.9740562201256391}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:22:57,095][0m Trial 7 finished with value: 0.3280583362997943 and parameters: {'observation_period_num': 246, 'train_rates': 0.6611442491306572, 'learning_rate': 4.3959960551577816e-05, 'batch_size': 72, 'step_size': 5, 'gamma': 0.7663603210458574}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:25:18,826][0m Trial 8 finished with value: 0.2519761047106448 and parameters: {'observation_period_num': 161, 'train_rates': 0.6031887088696966, 'learning_rate': 0.0008429370089170736, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9013347603427945}. Best is trial 3 with value: 0.12750164570949846.[0m
[32m[I 2025-02-03 17:26:15,621][0m Trial 9 finished with value: 0.07343304894086587 and parameters: {'observation_period_num': 78, 'train_rates': 0.826646442446757, 'learning_rate': 5.293888683687453e-05, 'batch_size': 97, 'step_size': 12, 'gamma': 0.8832087877368945}. Best is trial 9 with value: 0.07343304894086587.[0m
[32m[I 2025-02-03 17:26:53,388][0m Trial 10 finished with value: 0.3932016941986672 and parameters: {'observation_period_num': 20, 'train_rates': 0.8006873221527051, 'learning_rate': 3.7476209725681107e-06, 'batch_size': 154, 'step_size': 15, 'gamma': 0.8344829621233159}. Best is trial 9 with value: 0.07343304894086587.[0m
[32m[I 2025-02-03 17:27:44,404][0m Trial 11 finished with value: 0.056662249027705586 and parameters: {'observation_period_num': 52, 'train_rates': 0.8319249746712531, 'learning_rate': 0.0001563850898208954, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8396700890657978}. Best is trial 11 with value: 0.056662249027705586.[0m
[32m[I 2025-02-03 17:28:26,396][0m Trial 12 finished with value: 0.05607989858114638 and parameters: {'observation_period_num': 49, 'train_rates': 0.8487477458352357, 'learning_rate': 0.00011342557360929387, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8442472780492855}. Best is trial 12 with value: 0.05607989858114638.[0m
[32m[I 2025-02-03 17:29:05,264][0m Trial 13 finished with value: 0.045320262247091764 and parameters: {'observation_period_num': 34, 'train_rates': 0.8679879110974078, 'learning_rate': 0.00017656402306514592, 'batch_size': 151, 'step_size': 15, 'gamma': 0.8304590012840888}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:29:40,139][0m Trial 14 finished with value: 0.04707254101954848 and parameters: {'observation_period_num': 27, 'train_rates': 0.8749061295057028, 'learning_rate': 0.0001536759075341565, 'batch_size': 155, 'step_size': 15, 'gamma': 0.8163370183769969}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:30:13,329][0m Trial 15 finished with value: 0.045341125053297626 and parameters: {'observation_period_num': 13, 'train_rates': 0.87565485065755, 'learning_rate': 0.000246016473458666, 'batch_size': 178, 'step_size': 10, 'gamma': 0.8066885808603258}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:30:42,614][0m Trial 16 finished with value: 0.05369084246561561 and parameters: {'observation_period_num': 9, 'train_rates': 0.7586028702742529, 'learning_rate': 0.0003425631910520264, 'batch_size': 186, 'step_size': 9, 'gamma': 0.7967256895020548}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:31:10,447][0m Trial 17 finished with value: 0.500086784362793 and parameters: {'observation_period_num': 59, 'train_rates': 0.9738806860171784, 'learning_rate': 8.662542623338997e-06, 'batch_size': 241, 'step_size': 6, 'gamma': 0.7982670363667054}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:31:45,004][0m Trial 18 finished with value: 0.1334682710264954 and parameters: {'observation_period_num': 92, 'train_rates': 0.8851860706383465, 'learning_rate': 7.638568638393486e-05, 'batch_size': 172, 'step_size': 4, 'gamma': 0.8564536591225138}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:32:12,021][0m Trial 19 finished with value: 0.11952805158555234 and parameters: {'observation_period_num': 125, 'train_rates': 0.7690027750049286, 'learning_rate': 0.0003068040193129851, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8044506108342857}. Best is trial 13 with value: 0.045320262247091764.[0m
[32m[I 2025-02-03 17:33:02,606][0m Trial 20 finished with value: 0.04489435628056526 and parameters: {'observation_period_num': 37, 'train_rates': 0.9842555401213913, 'learning_rate': 0.00024341766580149142, 'batch_size': 129, 'step_size': 10, 'gamma': 0.8662279060803573}. Best is trial 20 with value: 0.04489435628056526.[0m
[32m[I 2025-02-03 17:33:51,271][0m Trial 21 finished with value: 0.04297444224357605 and parameters: {'observation_period_num': 32, 'train_rates': 0.9893333085168425, 'learning_rate': 0.0002953630590698783, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8638460653430693}. Best is trial 21 with value: 0.04297444224357605.[0m
[32m[I 2025-02-03 17:34:42,027][0m Trial 22 finished with value: 0.04073280468583107 and parameters: {'observation_period_num': 39, 'train_rates': 0.9857674156523927, 'learning_rate': 0.0003814769494676199, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8614216444739472}. Best is trial 22 with value: 0.04073280468583107.[0m
[32m[I 2025-02-03 17:35:34,747][0m Trial 23 finished with value: 0.08074051886796951 and parameters: {'observation_period_num': 63, 'train_rates': 0.9898293505251727, 'learning_rate': 0.0005262243733800393, 'batch_size': 120, 'step_size': 13, 'gamma': 0.870052640390548}. Best is trial 22 with value: 0.04073280468583107.[0m
[32m[I 2025-02-03 17:36:24,164][0m Trial 24 finished with value: 0.05403532642633357 and parameters: {'observation_period_num': 39, 'train_rates': 0.9518349368624514, 'learning_rate': 7.298803569674957e-05, 'batch_size': 126, 'step_size': 11, 'gamma': 0.9073113066918499}. Best is trial 22 with value: 0.04073280468583107.[0m
[32m[I 2025-02-03 17:37:36,605][0m Trial 25 finished with value: 0.09883370795586241 and parameters: {'observation_period_num': 76, 'train_rates': 0.924269079822768, 'learning_rate': 0.0009682453556666738, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8753974642248872}. Best is trial 22 with value: 0.04073280468583107.[0m
[32m[I 2025-02-03 17:39:44,391][0m Trial 26 finished with value: 0.021993175148963928 and parameters: {'observation_period_num': 5, 'train_rates': 0.9891957836612878, 'learning_rate': 0.0003950632010224091, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8537582930945804}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:42:24,837][0m Trial 27 finished with value: 0.045263363786879375 and parameters: {'observation_period_num': 5, 'train_rates': 0.9428725843930432, 'learning_rate': 0.00039969424097227547, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9300058456715418}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:44:33,601][0m Trial 28 finished with value: 0.07245334668211567 and parameters: {'observation_period_num': 71, 'train_rates': 0.96425852362303, 'learning_rate': 9.55925160517045e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8542861404174776}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:46:09,005][0m Trial 29 finished with value: 0.17822864098364816 and parameters: {'observation_period_num': 25, 'train_rates': 0.9071594063281254, 'learning_rate': 0.0006016183639097559, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8887754652202112}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:51:34,640][0m Trial 30 finished with value: 0.08884700573980808 and parameters: {'observation_period_num': 111, 'train_rates': 0.9887301963007893, 'learning_rate': 1.3140642268363911e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.903758734483019}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:52:24,121][0m Trial 31 finished with value: 0.04037988930940628 and parameters: {'observation_period_num': 42, 'train_rates': 0.9754856365623592, 'learning_rate': 0.0002790740549628919, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8591619915288367}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:53:34,582][0m Trial 32 finished with value: 0.05747847939873564 and parameters: {'observation_period_num': 43, 'train_rates': 0.9302564531388338, 'learning_rate': 0.00039677231765267997, 'batch_size': 85, 'step_size': 8, 'gamma': 0.8569320331535432}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:54:21,482][0m Trial 33 finished with value: 0.06740530580282211 and parameters: {'observation_period_num': 89, 'train_rates': 0.9633529700348906, 'learning_rate': 0.000269940240982007, 'batch_size': 137, 'step_size': 9, 'gamma': 0.8231860205327315}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:56:04,554][0m Trial 34 finished with value: 0.04880719252040821 and parameters: {'observation_period_num': 19, 'train_rates': 0.9308558930674944, 'learning_rate': 0.0006295215250223676, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8465896844419737}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:57:08,342][0m Trial 35 finished with value: 0.046197052587542614 and parameters: {'observation_period_num': 5, 'train_rates': 0.9616085361399388, 'learning_rate': 0.0001200108930572353, 'batch_size': 98, 'step_size': 7, 'gamma': 0.877936411665199}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:57:47,564][0m Trial 36 finished with value: 0.06682402483395887 and parameters: {'observation_period_num': 55, 'train_rates': 0.9182813458954049, 'learning_rate': 0.0004856626878746319, 'batch_size': 163, 'step_size': 14, 'gamma': 0.8642392577309674}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:58:52,995][0m Trial 37 finished with value: 0.04294968023896217 and parameters: {'observation_period_num': 28, 'train_rates': 0.9733649281804989, 'learning_rate': 0.0002120882101228635, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8958899167863456}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 17:59:56,736][0m Trial 38 finished with value: 0.18663036973221497 and parameters: {'observation_period_num': 120, 'train_rates': 0.9441122002463657, 'learning_rate': 3.255695299964674e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9306369989998966}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:01:49,934][0m Trial 39 finished with value: 0.598656523289565 and parameters: {'observation_period_num': 141, 'train_rates': 0.8973038619579782, 'learning_rate': 1.4172772650727082e-06, 'batch_size': 49, 'step_size': 14, 'gamma': 0.7804268402549606}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:02:35,967][0m Trial 40 finished with value: 0.31844183020662553 and parameters: {'observation_period_num': 220, 'train_rates': 0.7043237052160196, 'learning_rate': 0.0009863282812814494, 'batch_size': 104, 'step_size': 12, 'gamma': 0.9185132711544899}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:03:20,540][0m Trial 41 finished with value: 0.039224524050951004 and parameters: {'observation_period_num': 27, 'train_rates': 0.9701945983553704, 'learning_rate': 0.00020885245834727207, 'batch_size': 142, 'step_size': 11, 'gamma': 0.8946966435986081}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:04:13,488][0m Trial 42 finished with value: 0.0339948832988739 and parameters: {'observation_period_num': 19, 'train_rates': 0.9658922577279078, 'learning_rate': 0.00019441025945507958, 'batch_size': 118, 'step_size': 11, 'gamma': 0.8960121969180965}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:05:09,474][0m Trial 43 finished with value: 0.050172439567288576 and parameters: {'observation_period_num': 18, 'train_rates': 0.9499788225029842, 'learning_rate': 6.168159108578711e-05, 'batch_size': 115, 'step_size': 11, 'gamma': 0.969996860739708}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:05:52,048][0m Trial 44 finished with value: 0.16154591739177704 and parameters: {'observation_period_num': 67, 'train_rates': 0.9634264195309796, 'learning_rate': 0.00013370588321598713, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8867537330525762}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:06:30,221][0m Trial 45 finished with value: 0.051874419762974695 and parameters: {'observation_period_num': 44, 'train_rates': 0.9424705423650415, 'learning_rate': 0.0006847096502586114, 'batch_size': 164, 'step_size': 11, 'gamma': 0.9462880290592107}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:07:13,358][0m Trial 46 finished with value: 0.04910121390836164 and parameters: {'observation_period_num': 49, 'train_rates': 0.9013784979765935, 'learning_rate': 0.0001959578957115776, 'batch_size': 141, 'step_size': 14, 'gamma': 0.9171308531832858}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:07:52,197][0m Trial 47 finished with value: 0.13231556958414695 and parameters: {'observation_period_num': 85, 'train_rates': 0.6134347025579368, 'learning_rate': 0.0004592731551011296, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8934632900854288}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:09:00,483][0m Trial 48 finished with value: 0.16103437542915344 and parameters: {'observation_period_num': 180, 'train_rates': 0.9735673085015488, 'learning_rate': 3.9291924124818715e-05, 'batch_size': 86, 'step_size': 9, 'gamma': 0.9093607606667029}. Best is trial 26 with value: 0.021993175148963928.[0m
[32m[I 2025-02-03 18:09:31,208][0m Trial 49 finished with value: 0.06307700487335076 and parameters: {'observation_period_num': 18, 'train_rates': 0.8485165824118952, 'learning_rate': 0.00015842771416754936, 'batch_size': 201, 'step_size': 12, 'gamma': 0.8810530228992394}. Best is trial 26 with value: 0.021993175148963928.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 18:09:31,219][0m A new study created in memory with name: no-name-1d195c9c-b462-439f-b4e6-78bd207c36ab[0m
[32m[I 2025-02-03 18:10:15,418][0m Trial 0 finished with value: 0.06738900393247604 and parameters: {'observation_period_num': 67, 'train_rates': 0.9693711504702536, 'learning_rate': 0.00017847608799487485, 'batch_size': 146, 'step_size': 2, 'gamma': 0.9482983260279972}. Best is trial 0 with value: 0.06738900393247604.[0m
[32m[I 2025-02-03 18:10:42,385][0m Trial 1 finished with value: 0.49361680259697893 and parameters: {'observation_period_num': 27, 'train_rates': 0.7565419517099248, 'learning_rate': 1.469542902211241e-06, 'batch_size': 201, 'step_size': 13, 'gamma': 0.9755156884860341}. Best is trial 0 with value: 0.06738900393247604.[0m
[32m[I 2025-02-03 18:11:31,042][0m Trial 2 finished with value: 0.03982330962669018 and parameters: {'observation_period_num': 10, 'train_rates': 0.9416881005352238, 'learning_rate': 0.0003621249093550903, 'batch_size': 131, 'step_size': 4, 'gamma': 0.9219636687103316}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:12:36,529][0m Trial 3 finished with value: 0.4852138955152691 and parameters: {'observation_period_num': 223, 'train_rates': 0.7937872556984646, 'learning_rate': 1.322242877396126e-05, 'batch_size': 75, 'step_size': 3, 'gamma': 0.8057579613641705}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:13:45,114][0m Trial 4 finished with value: 0.5112597421595925 and parameters: {'observation_period_num': 52, 'train_rates': 0.8362931811508229, 'learning_rate': 1.852876905903749e-06, 'batch_size': 80, 'step_size': 4, 'gamma': 0.9249518046791297}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:14:47,762][0m Trial 5 finished with value: 0.11819384522318148 and parameters: {'observation_period_num': 164, 'train_rates': 0.8146324287647118, 'learning_rate': 0.0002503758572560933, 'batch_size': 86, 'step_size': 3, 'gamma': 0.8402199320801296}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:16:05,409][0m Trial 6 finished with value: 0.1521389264257709 and parameters: {'observation_period_num': 91, 'train_rates': 0.9303979335008737, 'learning_rate': 2.5017437677720697e-05, 'batch_size': 75, 'step_size': 5, 'gamma': 0.8104916378191698}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:16:57,174][0m Trial 7 finished with value: 0.16070109605789185 and parameters: {'observation_period_num': 210, 'train_rates': 0.7462559744348829, 'learning_rate': 7.277172056666959e-05, 'batch_size': 96, 'step_size': 12, 'gamma': 0.9328699136142066}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:17:30,596][0m Trial 8 finished with value: 0.055241516359695576 and parameters: {'observation_period_num': 48, 'train_rates': 0.8587817711721886, 'learning_rate': 0.0003928410034137379, 'batch_size': 185, 'step_size': 9, 'gamma': 0.8565854345538181}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:17:50,279][0m Trial 9 finished with value: 0.924152637170593 and parameters: {'observation_period_num': 182, 'train_rates': 0.6114466858900859, 'learning_rate': 2.655278378350638e-06, 'batch_size': 246, 'step_size': 1, 'gamma': 0.9219235606972516}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:21:48,813][0m Trial 10 finished with value: 0.09550187508246713 and parameters: {'observation_period_num': 117, 'train_rates': 0.9053701335426574, 'learning_rate': 0.0007671531562519899, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8854318056682641}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:22:25,485][0m Trial 11 finished with value: 0.040077614134654924 and parameters: {'observation_period_num': 11, 'train_rates': 0.864597540804099, 'learning_rate': 0.0009618144580714512, 'batch_size': 165, 'step_size': 8, 'gamma': 0.7524166404758192}. Best is trial 2 with value: 0.03982330962669018.[0m
[32m[I 2025-02-03 18:23:06,608][0m Trial 12 finished with value: 0.03534968893862132 and parameters: {'observation_period_num': 9, 'train_rates': 0.888116828105302, 'learning_rate': 0.0009883213953294518, 'batch_size': 146, 'step_size': 8, 'gamma': 0.7702215563019117}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:23:58,974][0m Trial 13 finished with value: 0.07135704904794693 and parameters: {'observation_period_num': 19, 'train_rates': 0.9778167845677148, 'learning_rate': 7.918886308633236e-05, 'batch_size': 122, 'step_size': 6, 'gamma': 0.7528506243457374}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:24:48,682][0m Trial 14 finished with value: 0.06746917133033276 and parameters: {'observation_period_num': 98, 'train_rates': 0.9123984496389571, 'learning_rate': 0.00010855991048466562, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8957032869994951}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:25:14,596][0m Trial 15 finished with value: 0.06863124489075395 and parameters: {'observation_period_num': 6, 'train_rates': 0.6718207591236862, 'learning_rate': 0.00046593107843467975, 'batch_size': 210, 'step_size': 6, 'gamma': 0.7933535216248702}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:25:52,877][0m Trial 16 finished with value: 0.28186028418333636 and parameters: {'observation_period_num': 145, 'train_rates': 0.9345429165050262, 'learning_rate': 7.893853067177743e-06, 'batch_size': 156, 'step_size': 15, 'gamma': 0.9760972286847855}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:28:29,620][0m Trial 17 finished with value: 0.1607863295429982 and parameters: {'observation_period_num': 250, 'train_rates': 0.880919025150219, 'learning_rate': 0.00040326709283876013, 'batch_size': 33, 'step_size': 7, 'gamma': 0.828417366737197}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:29:26,761][0m Trial 18 finished with value: 0.10998326539993286 and parameters: {'observation_period_num': 74, 'train_rates': 0.9790941960137635, 'learning_rate': 3.527368005875184e-05, 'batch_size': 112, 'step_size': 11, 'gamma': 0.7780907201876027}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:31:06,902][0m Trial 19 finished with value: 0.06530955300555316 and parameters: {'observation_period_num': 47, 'train_rates': 0.7748317413697884, 'learning_rate': 0.00017754288194467358, 'batch_size': 52, 'step_size': 4, 'gamma': 0.8888654946201173}. Best is trial 12 with value: 0.03534968893862132.[0m
Early stopping at epoch 95
[32m[I 2025-02-03 18:31:40,951][0m Trial 20 finished with value: 0.08849737793207169 and parameters: {'observation_period_num': 34, 'train_rates': 0.9442678221568268, 'learning_rate': 0.0008461290713120333, 'batch_size': 180, 'step_size': 1, 'gamma': 0.8659631979821087}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:32:19,461][0m Trial 21 finished with value: 0.04280153278956938 and parameters: {'observation_period_num': 7, 'train_rates': 0.8674006975469145, 'learning_rate': 0.0009566934394910247, 'batch_size': 159, 'step_size': 8, 'gamma': 0.7533077095746378}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:32:53,644][0m Trial 22 finished with value: 0.04560977683679478 and parameters: {'observation_period_num': 29, 'train_rates': 0.8861446264414002, 'learning_rate': 0.0004955365560035063, 'batch_size': 174, 'step_size': 9, 'gamma': 0.7810769519291487}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:33:35,796][0m Trial 23 finished with value: 0.06402765092609813 and parameters: {'observation_period_num': 73, 'train_rates': 0.8345334871565568, 'learning_rate': 0.0002455363323856552, 'batch_size': 138, 'step_size': 6, 'gamma': 0.7689972911451544}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:34:05,922][0m Trial 24 finished with value: 0.047280303753492695 and parameters: {'observation_period_num': 9, 'train_rates': 0.9035198052356951, 'learning_rate': 0.0006061551811450328, 'batch_size': 211, 'step_size': 10, 'gamma': 0.8321449483104038}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:34:56,333][0m Trial 25 finished with value: 0.05179578566284322 and parameters: {'observation_period_num': 42, 'train_rates': 0.8389108884224086, 'learning_rate': 0.0003106510120995213, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9080033151080537}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:35:24,338][0m Trial 26 finished with value: 0.14364762604236603 and parameters: {'observation_period_num': 112, 'train_rates': 0.9602809410393559, 'learning_rate': 0.0009901593601162803, 'batch_size': 242, 'step_size': 13, 'gamma': 0.9568286366736258}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:35:56,411][0m Trial 27 finished with value: 0.12499080984370688 and parameters: {'observation_period_num': 64, 'train_rates': 0.722719939696793, 'learning_rate': 0.0001274367163368239, 'batch_size': 163, 'step_size': 5, 'gamma': 0.7665689496478711}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:36:40,234][0m Trial 28 finished with value: 0.043427044431031764 and parameters: {'observation_period_num': 22, 'train_rates': 0.8589483159477914, 'learning_rate': 0.0005660857166526363, 'batch_size': 137, 'step_size': 9, 'gamma': 0.8048287022024023}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:37:22,278][0m Trial 29 finished with value: 0.059009164571762085 and parameters: {'observation_period_num': 60, 'train_rates': 0.9517315971461016, 'learning_rate': 0.00018153753479259203, 'batch_size': 147, 'step_size': 7, 'gamma': 0.9460077995881142}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:37:54,114][0m Trial 30 finished with value: 0.1896394024332685 and parameters: {'observation_period_num': 85, 'train_rates': 0.913731057447446, 'learning_rate': 4.381369071660632e-05, 'batch_size': 193, 'step_size': 3, 'gamma': 0.8542118873069496}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:38:30,881][0m Trial 31 finished with value: 0.04384867300319931 and parameters: {'observation_period_num': 7, 'train_rates': 0.8750932380150123, 'learning_rate': 0.000967973531319631, 'batch_size': 166, 'step_size': 8, 'gamma': 0.7506972482534989}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:39:12,949][0m Trial 32 finished with value: 0.04547490272670984 and parameters: {'observation_period_num': 23, 'train_rates': 0.8662546811717811, 'learning_rate': 0.0006484349585569844, 'batch_size': 149, 'step_size': 10, 'gamma': 0.7617383488567838}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:39:39,130][0m Trial 33 finished with value: 0.048750617294016155 and parameters: {'observation_period_num': 35, 'train_rates': 0.8076930261469887, 'learning_rate': 0.0009827665591034319, 'batch_size': 224, 'step_size': 8, 'gamma': 0.7880012972850446}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:40:17,324][0m Trial 34 finished with value: 0.04493186174129898 and parameters: {'observation_period_num': 18, 'train_rates': 0.8949564091393019, 'learning_rate': 0.000296333403336544, 'batch_size': 161, 'step_size': 11, 'gamma': 0.8167303098516715}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:41:03,413][0m Trial 35 finished with value: 0.05544773398838398 and parameters: {'observation_period_num': 6, 'train_rates': 0.8266880378575706, 'learning_rate': 0.00039136557827511396, 'batch_size': 128, 'step_size': 5, 'gamma': 0.794381997285047}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:41:54,234][0m Trial 36 finished with value: 0.2645798260958964 and parameters: {'observation_period_num': 54, 'train_rates': 0.7851009723395962, 'learning_rate': 5.5204162766371e-06, 'batch_size': 107, 'step_size': 2, 'gamma': 0.9879757694861644}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:42:26,954][0m Trial 37 finished with value: 0.26113497819835785 and parameters: {'observation_period_num': 39, 'train_rates': 0.9241116345541703, 'learning_rate': 1.9715608060215525e-05, 'batch_size': 196, 'step_size': 4, 'gamma': 0.7737880242940306}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:43:02,716][0m Trial 38 finished with value: 0.061117636616484894 and parameters: {'observation_period_num': 22, 'train_rates': 0.8468883170648377, 'learning_rate': 0.00021114366416716686, 'batch_size': 176, 'step_size': 7, 'gamma': 0.7600566299124323}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:43:59,813][0m Trial 39 finished with value: 0.15714940669209654 and parameters: {'observation_period_num': 145, 'train_rates': 0.8090553892626463, 'learning_rate': 0.0006497874372656589, 'batch_size': 93, 'step_size': 14, 'gamma': 0.7508753622222099}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:45:25,006][0m Trial 40 finished with value: 0.111029870562501 and parameters: {'observation_period_num': 188, 'train_rates': 0.9343319493580885, 'learning_rate': 0.00031895835258534584, 'batch_size': 67, 'step_size': 12, 'gamma': 0.8014428897601409}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:46:09,761][0m Trial 41 finished with value: 0.04423837926439435 and parameters: {'observation_period_num': 20, 'train_rates': 0.8544424375380468, 'learning_rate': 0.0005689118759283407, 'batch_size': 137, 'step_size': 9, 'gamma': 0.781884699940565}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:46:53,680][0m Trial 42 finished with value: 0.04674470887243093 and parameters: {'observation_period_num': 30, 'train_rates': 0.8756631508046668, 'learning_rate': 0.0006880080702841363, 'batch_size': 138, 'step_size': 9, 'gamma': 0.7713716565984692}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:47:33,415][0m Trial 43 finished with value: 0.04330466501414776 and parameters: {'observation_period_num': 15, 'train_rates': 0.8611328216265925, 'learning_rate': 0.0004459552086396754, 'batch_size': 154, 'step_size': 8, 'gamma': 0.8161968068424599}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:48:13,150][0m Trial 44 finished with value: 0.04891107653578122 and parameters: {'observation_period_num': 5, 'train_rates': 0.888109386668448, 'learning_rate': 0.00013942127656981976, 'batch_size': 154, 'step_size': 8, 'gamma': 0.8189959144219319}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:49:00,287][0m Trial 45 finished with value: 0.056595068546728446 and parameters: {'observation_period_num': 50, 'train_rates': 0.8207049074136835, 'learning_rate': 0.0004051499095937595, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8532591879110154}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:49:32,887][0m Trial 46 finished with value: 0.10426945156521267 and parameters: {'observation_period_num': 17, 'train_rates': 0.7547577209733755, 'learning_rate': 7.185292924451485e-05, 'batch_size': 172, 'step_size': 5, 'gamma': 0.761471152842822}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:50:08,909][0m Trial 47 finished with value: 0.0506662018597126 and parameters: {'observation_period_num': 39, 'train_rates': 0.9647613488189886, 'learning_rate': 0.000764526292589649, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8424562228875394}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:50:52,509][0m Trial 48 finished with value: 0.040210590531081614 and parameters: {'observation_period_num': 14, 'train_rates': 0.9222796544912083, 'learning_rate': 0.00046792982350532915, 'batch_size': 148, 'step_size': 11, 'gamma': 0.910372632572894}. Best is trial 12 with value: 0.03534968893862132.[0m
[32m[I 2025-02-03 18:51:55,533][0m Trial 49 finished with value: 0.5291476845741272 and parameters: {'observation_period_num': 31, 'train_rates': 0.9890046495625815, 'learning_rate': 1.179511960162029e-06, 'batch_size': 103, 'step_size': 12, 'gamma': 0.9152167781398738}. Best is trial 12 with value: 0.03534968893862132.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 18:51:55,543][0m A new study created in memory with name: no-name-aefb3fc8-5522-4946-a0b8-07276c439b09[0m
[32m[I 2025-02-03 18:52:24,192][0m Trial 0 finished with value: 0.10906465008431805 and parameters: {'observation_period_num': 85, 'train_rates': 0.6682866071759964, 'learning_rate': 0.000564909797411726, 'batch_size': 184, 'step_size': 8, 'gamma': 0.9519351814610616}. Best is trial 0 with value: 0.10906465008431805.[0m
[32m[I 2025-02-03 18:53:12,254][0m Trial 1 finished with value: 0.1822116274032846 and parameters: {'observation_period_num': 145, 'train_rates': 0.6174419823404731, 'learning_rate': 0.0002064077462426005, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8405563664439915}. Best is trial 0 with value: 0.10906465008431805.[0m
[32m[I 2025-02-03 18:57:23,583][0m Trial 2 finished with value: 0.24337913426584748 and parameters: {'observation_period_num': 59, 'train_rates': 0.7726425014813816, 'learning_rate': 1.2166774032134965e-05, 'batch_size': 20, 'step_size': 2, 'gamma': 0.833328423989753}. Best is trial 0 with value: 0.10906465008431805.[0m
[32m[I 2025-02-03 18:57:59,495][0m Trial 3 finished with value: 0.19144282599825488 and parameters: {'observation_period_num': 179, 'train_rates': 0.7725024101845452, 'learning_rate': 0.0003397482136820472, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9023214638426111}. Best is trial 0 with value: 0.10906465008431805.[0m
[32m[I 2025-02-03 18:58:48,031][0m Trial 4 finished with value: 0.13106131553649902 and parameters: {'observation_period_num': 166, 'train_rates': 0.9764223906169299, 'learning_rate': 3.8885180467955505e-05, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9278879091579786}. Best is trial 0 with value: 0.10906465008431805.[0m
[32m[I 2025-02-03 18:59:41,782][0m Trial 5 finished with value: 0.10189095776082431 and parameters: {'observation_period_num': 44, 'train_rates': 0.6951567671490806, 'learning_rate': 2.8920508274608736e-05, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8357072689126186}. Best is trial 5 with value: 0.10189095776082431.[0m
[32m[I 2025-02-03 19:00:11,853][0m Trial 6 finished with value: 0.08237681421591025 and parameters: {'observation_period_num': 40, 'train_rates': 0.7139841274687366, 'learning_rate': 0.0001258469890424586, 'batch_size': 183, 'step_size': 1, 'gamma': 0.9685070358110648}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:00:33,685][0m Trial 7 finished with value: 0.12016281491602653 and parameters: {'observation_period_num': 176, 'train_rates': 0.6447315421402621, 'learning_rate': 0.0009764810008323844, 'batch_size': 236, 'step_size': 7, 'gamma': 0.8044089578045716}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:01:07,523][0m Trial 8 finished with value: 0.09034672757027931 and parameters: {'observation_period_num': 117, 'train_rates': 0.816161565833531, 'learning_rate': 0.00014198710758007773, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8231452587227035}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:01:47,806][0m Trial 9 finished with value: 0.30480112910778207 and parameters: {'observation_period_num': 62, 'train_rates': 0.63420062049162, 'learning_rate': 1.9010840282985363e-05, 'batch_size': 118, 'step_size': 10, 'gamma': 0.9129477061564742}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:02:14,275][0m Trial 10 finished with value: 1.1036045787098643 and parameters: {'observation_period_num': 24, 'train_rates': 0.8758653213224886, 'learning_rate': 1.035256992450672e-06, 'batch_size': 250, 'step_size': 1, 'gamma': 0.9878319824291313}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:02:45,411][0m Trial 11 finished with value: 0.1679649916562167 and parameters: {'observation_period_num': 241, 'train_rates': 0.8581477199657709, 'learning_rate': 0.00011406896360986365, 'batch_size': 189, 'step_size': 13, 'gamma': 0.766146988632778}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:03:14,469][0m Trial 12 finished with value: 0.12446181011086727 and parameters: {'observation_period_num': 96, 'train_rates': 0.7418515427626426, 'learning_rate': 9.960753675520358e-05, 'batch_size': 191, 'step_size': 4, 'gamma': 0.876019725026646}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:03:51,810][0m Trial 13 finished with value: 0.22973061762050198 and parameters: {'observation_period_num': 9, 'train_rates': 0.8371898618146881, 'learning_rate': 5.088251391164767e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.7528759268941632}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:04:17,308][0m Trial 14 finished with value: 0.10662505684874544 and parameters: {'observation_period_num': 111, 'train_rates': 0.7170149144118628, 'learning_rate': 7.537051574350748e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.9694299280541704}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:06:22,160][0m Trial 15 finished with value: 0.0919891104929977 and parameters: {'observation_period_num': 245, 'train_rates': 0.9335333672675845, 'learning_rate': 0.00024124676458402855, 'batch_size': 44, 'step_size': 6, 'gamma': 0.7957419721910891}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:06:48,711][0m Trial 16 finished with value: 0.2567401799496671 and parameters: {'observation_period_num': 135, 'train_rates': 0.815021590381326, 'learning_rate': 6.293682082373944e-05, 'batch_size': 210, 'step_size': 3, 'gamma': 0.873437953184699}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:07:26,084][0m Trial 17 finished with value: 0.446516084692461 and parameters: {'observation_period_num': 82, 'train_rates': 0.9032501857162497, 'learning_rate': 6.962826928737629e-06, 'batch_size': 165, 'step_size': 6, 'gamma': 0.801119362095601}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:08:22,425][0m Trial 18 finished with value: 0.2056426165625453 and parameters: {'observation_period_num': 214, 'train_rates': 0.7956715586123387, 'learning_rate': 0.00016677054041228987, 'batch_size': 91, 'step_size': 15, 'gamma': 0.9356193958018385}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:09:03,536][0m Trial 19 finished with value: 0.1182070655574334 and parameters: {'observation_period_num': 112, 'train_rates': 0.7399514094880886, 'learning_rate': 0.00043895956167206785, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8946817893609265}. Best is trial 6 with value: 0.08237681421591025.[0m
Early stopping at epoch 82
[32m[I 2025-02-03 19:09:24,803][0m Trial 20 finished with value: 0.3562470112656387 and parameters: {'observation_period_num': 35, 'train_rates': 0.7041839295129252, 'learning_rate': 4.2742209178720506e-05, 'batch_size': 214, 'step_size': 1, 'gamma': 0.8541862712760582}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:11:03,730][0m Trial 21 finished with value: 0.08775526949992547 and parameters: {'observation_period_num': 208, 'train_rates': 0.9528631223828234, 'learning_rate': 0.0001630059434692427, 'batch_size': 57, 'step_size': 6, 'gamma': 0.7972487690179507}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:12:53,414][0m Trial 22 finished with value: 0.1060231201219977 and parameters: {'observation_period_num': 203, 'train_rates': 0.9795319972442227, 'learning_rate': 0.00013450473370200169, 'batch_size': 52, 'step_size': 4, 'gamma': 0.7804519569298325}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:13:29,597][0m Trial 23 finished with value: 0.1064905971288681 and parameters: {'observation_period_num': 155, 'train_rates': 0.9410499163504444, 'learning_rate': 0.0008603803275460707, 'batch_size': 170, 'step_size': 8, 'gamma': 0.8209809656286622}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:14:48,712][0m Trial 24 finished with value: 0.09700899797494908 and parameters: {'observation_period_num': 215, 'train_rates': 0.8931580154824903, 'learning_rate': 0.0003052591391908982, 'batch_size': 68, 'step_size': 3, 'gamma': 0.8146555970740947}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:15:26,660][0m Trial 25 finished with value: 0.19572213838069602 and parameters: {'observation_period_num': 200, 'train_rates': 0.8312989922290938, 'learning_rate': 5.412618605729952e-05, 'batch_size': 146, 'step_size': 6, 'gamma': 0.8538873366892571}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:16:11,344][0m Trial 26 finished with value: 0.13360552804141562 and parameters: {'observation_period_num': 120, 'train_rates': 0.6717797596442603, 'learning_rate': 0.00013864268256736864, 'batch_size': 107, 'step_size': 10, 'gamma': 0.7910763855585297}. Best is trial 6 with value: 0.08237681421591025.[0m
[32m[I 2025-02-03 19:17:32,618][0m Trial 27 finished with value: 0.08074303742761564 and parameters: {'observation_period_num': 63, 'train_rates': 0.9316116874837957, 'learning_rate': 2.5094503786616713e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.7814938790784821}. Best is trial 27 with value: 0.08074303742761564.[0m
[32m[I 2025-02-03 19:18:56,028][0m Trial 28 finished with value: 0.08727072951800983 and parameters: {'observation_period_num': 61, 'train_rates': 0.9437518115465086, 'learning_rate': 2.4215269912264256e-05, 'batch_size': 71, 'step_size': 13, 'gamma': 0.7776146407476235}. Best is trial 27 with value: 0.08074303742761564.[0m
[32m[I 2025-02-03 19:24:45,996][0m Trial 29 finished with value: 0.05515843800950849 and parameters: {'observation_period_num': 83, 'train_rates': 0.9274239995693055, 'learning_rate': 2.1585480508901962e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7509794148747403}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:29:55,459][0m Trial 30 finished with value: 0.06884591807337369 and parameters: {'observation_period_num': 87, 'train_rates': 0.911189103390988, 'learning_rate': 1.3056322075467286e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7572136477251601}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:35:42,978][0m Trial 31 finished with value: 0.07337808445038482 and parameters: {'observation_period_num': 79, 'train_rates': 0.9097916464025532, 'learning_rate': 1.2660131876015607e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7531677512767927}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:40:50,420][0m Trial 32 finished with value: 0.07330671586560016 and parameters: {'observation_period_num': 87, 'train_rates': 0.9158482763694205, 'learning_rate': 1.1879374123321977e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7567801136581969}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:45:40,478][0m Trial 33 finished with value: 0.08057090634911943 and parameters: {'observation_period_num': 85, 'train_rates': 0.8993079403553171, 'learning_rate': 1.02189769512844e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7502136420598009}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:48:26,321][0m Trial 34 finished with value: 0.24554253522172031 and parameters: {'observation_period_num': 95, 'train_rates': 0.9131213914197407, 'learning_rate': 3.0626723007886387e-06, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7659564894943576}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:51:16,487][0m Trial 35 finished with value: 0.08769704426264824 and parameters: {'observation_period_num': 79, 'train_rates': 0.8641459395273031, 'learning_rate': 1.3797049297043634e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.7654190008706815}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:56:35,847][0m Trial 36 finished with value: 0.1824937665134395 and parameters: {'observation_period_num': 98, 'train_rates': 0.9619556484359705, 'learning_rate': 3.380882652071247e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7505433349853338}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 19:59:29,502][0m Trial 37 finished with value: 0.134536282890856 and parameters: {'observation_period_num': 135, 'train_rates': 0.917394874828054, 'learning_rate': 8.998045488156376e-06, 'batch_size': 32, 'step_size': 13, 'gamma': 0.7667459867885168}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:01:39,363][0m Trial 38 finished with value: 0.08472778082366292 and parameters: {'observation_period_num': 73, 'train_rates': 0.8797113160227958, 'learning_rate': 1.5538732830522837e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.7786377983398469}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:07:39,029][0m Trial 39 finished with value: 0.13331185299229908 and parameters: {'observation_period_num': 102, 'train_rates': 0.9711753805542739, 'learning_rate': 6.426655882251118e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7609750413932899}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:08:51,922][0m Trial 40 finished with value: 0.32141411304473877 and parameters: {'observation_period_num': 48, 'train_rates': 0.9838879574268885, 'learning_rate': 3.824274357682419e-06, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8120751083221566}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:12:24,512][0m Trial 41 finished with value: 0.1007158385197583 and parameters: {'observation_period_num': 74, 'train_rates': 0.894502171361916, 'learning_rate': 8.372842808913114e-06, 'batch_size': 26, 'step_size': 15, 'gamma': 0.7600738646401899}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:14:40,856][0m Trial 42 finished with value: 0.13203464454686792 and parameters: {'observation_period_num': 84, 'train_rates': 0.9167722797244984, 'learning_rate': 1.0189662787129801e-05, 'batch_size': 42, 'step_size': 14, 'gamma': 0.7505165672975987}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:19:39,329][0m Trial 43 finished with value: 0.05870544151315149 and parameters: {'observation_period_num': 46, 'train_rates': 0.8540097942814282, 'learning_rate': 1.7818527480670966e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7877149613954784}. Best is trial 29 with value: 0.05515843800950849.[0m
[32m[I 2025-02-03 20:21:18,165][0m Trial 44 finished with value: 0.05492646684317284 and parameters: {'observation_period_num': 27, 'train_rates': 0.8502202136846639, 'learning_rate': 3.526408463392404e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.7874492033996604}. Best is trial 44 with value: 0.05492646684317284.[0m
[32m[I 2025-02-03 20:22:52,919][0m Trial 45 finished with value: 0.05538793218764923 and parameters: {'observation_period_num': 21, 'train_rates': 0.840216876850941, 'learning_rate': 3.9354392965750614e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.7910884643441376}. Best is trial 44 with value: 0.05492646684317284.[0m
[32m[I 2025-02-03 20:24:25,344][0m Trial 46 finished with value: 0.04914670769556039 and parameters: {'observation_period_num': 8, 'train_rates': 0.8501862837247752, 'learning_rate': 4.079393098551156e-05, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7880163628587862}. Best is trial 46 with value: 0.04914670769556039.[0m
[32m[I 2025-02-03 20:26:02,100][0m Trial 47 finished with value: 0.05084341416867184 and parameters: {'observation_period_num': 7, 'train_rates': 0.8478913835461831, 'learning_rate': 3.466036975823494e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8323458713598467}. Best is trial 46 with value: 0.04914670769556039.[0m
[32m[I 2025-02-03 20:27:28,570][0m Trial 48 finished with value: 0.05819123505088809 and parameters: {'observation_period_num': 7, 'train_rates': 0.7846512220365099, 'learning_rate': 3.411701919525513e-05, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8313625495303241}. Best is trial 46 with value: 0.04914670769556039.[0m
[32m[I 2025-02-03 20:28:21,215][0m Trial 49 finished with value: 0.05354426290241064 and parameters: {'observation_period_num': 21, 'train_rates': 0.8366249022947001, 'learning_rate': 7.95871975236308e-05, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8471674420882767}. Best is trial 46 with value: 0.04914670769556039.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 20:28:21,226][0m A new study created in memory with name: no-name-e786a798-1b96-4522-9310-6ec2efda00e8[0m
[32m[I 2025-02-03 20:29:52,409][0m Trial 0 finished with value: 0.12178620088826676 and parameters: {'observation_period_num': 135, 'train_rates': 0.7188916313846092, 'learning_rate': 0.0003502465164484445, 'batch_size': 53, 'step_size': 14, 'gamma': 0.7751685237967931}. Best is trial 0 with value: 0.12178620088826676.[0m
[32m[I 2025-02-03 20:30:20,288][0m Trial 1 finished with value: 0.14159228297688017 and parameters: {'observation_period_num': 179, 'train_rates': 0.8928076733205443, 'learning_rate': 0.00017607334490164848, 'batch_size': 216, 'step_size': 12, 'gamma': 0.796273417108078}. Best is trial 0 with value: 0.12178620088826676.[0m
[32m[I 2025-02-03 20:30:42,606][0m Trial 2 finished with value: 0.21336177989736305 and parameters: {'observation_period_num': 148, 'train_rates': 0.7450860318435718, 'learning_rate': 5.224957789368155e-05, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9769595707715496}. Best is trial 0 with value: 0.12178620088826676.[0m
[32m[I 2025-02-03 20:31:06,957][0m Trial 3 finished with value: 0.8355288918996822 and parameters: {'observation_period_num': 32, 'train_rates': 0.822315212956033, 'learning_rate': 1.1371508450978107e-06, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9486536867472386}. Best is trial 0 with value: 0.12178620088826676.[0m
[32m[I 2025-02-03 20:33:43,497][0m Trial 4 finished with value: 0.06630139494704645 and parameters: {'observation_period_num': 44, 'train_rates': 0.9704670298954816, 'learning_rate': 2.284577746561739e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.7633268147103622}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:34:38,458][0m Trial 5 finished with value: 0.07783882263528398 and parameters: {'observation_period_num': 99, 'train_rates': 0.823945004923273, 'learning_rate': 5.8522082846474804e-05, 'batch_size': 99, 'step_size': 7, 'gamma': 0.9152796330436409}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:35:03,268][0m Trial 6 finished with value: 0.9019522314270337 and parameters: {'observation_period_num': 252, 'train_rates': 0.7332173147982316, 'learning_rate': 1.648579241467336e-06, 'batch_size': 213, 'step_size': 8, 'gamma': 0.9336174395453274}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:35:39,373][0m Trial 7 finished with value: 0.1150805726647377 and parameters: {'observation_period_num': 169, 'train_rates': 0.982594527856018, 'learning_rate': 0.000823380620315242, 'batch_size': 174, 'step_size': 2, 'gamma': 0.8390267512939719}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:36:07,364][0m Trial 8 finished with value: 0.25943633941429806 and parameters: {'observation_period_num': 133, 'train_rates': 0.6916462540240015, 'learning_rate': 1.4211979148926608e-05, 'batch_size': 188, 'step_size': 8, 'gamma': 0.987204347686513}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:37:18,112][0m Trial 9 finished with value: 0.7136052317927268 and parameters: {'observation_period_num': 208, 'train_rates': 0.7263330979168474, 'learning_rate': 1.956841408408815e-06, 'batch_size': 67, 'step_size': 11, 'gamma': 0.8711575409211666}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:41:03,507][0m Trial 10 finished with value: 0.12410460715576754 and parameters: {'observation_period_num': 27, 'train_rates': 0.9800721062872955, 'learning_rate': 8.39271495738929e-06, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8210967396675878}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:41:48,886][0m Trial 11 finished with value: 0.25843220208142254 and parameters: {'observation_period_num': 73, 'train_rates': 0.6144660320184671, 'learning_rate': 4.983402980515282e-05, 'batch_size': 102, 'step_size': 5, 'gamma': 0.8908561286632355}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:42:38,790][0m Trial 12 finished with value: 0.19442453923133704 and parameters: {'observation_period_num': 79, 'train_rates': 0.8824104162880788, 'learning_rate': 1.5479305966583374e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.7512330509414171}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:43:44,063][0m Trial 13 finished with value: 0.07390884930888812 and parameters: {'observation_period_num': 76, 'train_rates': 0.9071576516655173, 'learning_rate': 0.00011207208947113754, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9140640884464686}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:47:04,369][0m Trial 14 finished with value: 0.08321415654637597 and parameters: {'observation_period_num': 52, 'train_rates': 0.9052073401808777, 'learning_rate': 0.0001684495503603061, 'batch_size': 28, 'step_size': 15, 'gamma': 0.836362604552481}. Best is trial 4 with value: 0.06630139494704645.[0m
[32m[I 2025-02-03 20:48:30,915][0m Trial 15 finished with value: 0.03233796765350483 and parameters: {'observation_period_num': 5, 'train_rates': 0.9253346846698467, 'learning_rate': 0.00013632621006793506, 'batch_size': 69, 'step_size': 13, 'gamma': 0.8977072052583156}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:50:12,442][0m Trial 16 finished with value: 0.1014524113931013 and parameters: {'observation_period_num': 9, 'train_rates': 0.9395139703927907, 'learning_rate': 5.031408393432135e-06, 'batch_size': 59, 'step_size': 12, 'gamma': 0.8602874079546383}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:50:58,633][0m Trial 17 finished with value: 0.06257691965861754 and parameters: {'observation_period_num': 5, 'train_rates': 0.8507048883504609, 'learning_rate': 2.690415300995701e-05, 'batch_size': 128, 'step_size': 11, 'gamma': 0.8848018199830957}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:51:40,087][0m Trial 18 finished with value: 0.036449507257695926 and parameters: {'observation_period_num': 5, 'train_rates': 0.8400442247488761, 'learning_rate': 0.0009334294381875313, 'batch_size': 141, 'step_size': 13, 'gamma': 0.8872951940652655}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:52:13,197][0m Trial 19 finished with value: 0.09465987764814365 and parameters: {'observation_period_num': 107, 'train_rates': 0.7765082332017043, 'learning_rate': 0.0009282297811071991, 'batch_size': 164, 'step_size': 13, 'gamma': 0.9061905291853389}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:52:49,849][0m Trial 20 finished with value: 0.06407366054145854 and parameters: {'observation_period_num': 54, 'train_rates': 0.8548511232194974, 'learning_rate': 0.0004398526087905787, 'batch_size': 156, 'step_size': 15, 'gamma': 0.948072736176178}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:53:34,135][0m Trial 21 finished with value: 0.0534801921707445 and parameters: {'observation_period_num': 11, 'train_rates': 0.8476891748272026, 'learning_rate': 0.00010136984420968166, 'batch_size': 137, 'step_size': 10, 'gamma': 0.878261659476068}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:54:14,099][0m Trial 22 finished with value: 0.038674425518463465 and parameters: {'observation_period_num': 5, 'train_rates': 0.7946522166627827, 'learning_rate': 0.0003787492498674201, 'batch_size': 140, 'step_size': 10, 'gamma': 0.8608811067445474}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:54:54,998][0m Trial 23 finished with value: 0.07206102467426971 and parameters: {'observation_period_num': 29, 'train_rates': 0.7753671696381866, 'learning_rate': 0.00040646146600586866, 'batch_size': 141, 'step_size': 13, 'gamma': 0.851190605785729}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:56:12,228][0m Trial 24 finished with value: 0.048647295905126106 and parameters: {'observation_period_num': 23, 'train_rates': 0.9370857423995995, 'learning_rate': 0.0002813544265426304, 'batch_size': 78, 'step_size': 10, 'gamma': 0.8146376038431813}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:56:58,459][0m Trial 25 finished with value: 0.08576313139283083 and parameters: {'observation_period_num': 60, 'train_rates': 0.8060365289958349, 'learning_rate': 0.000674357947336133, 'batch_size': 118, 'step_size': 13, 'gamma': 0.8896054588715557}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:57:26,844][0m Trial 26 finished with value: 0.1487781858393106 and parameters: {'observation_period_num': 99, 'train_rates': 0.6728090851704659, 'learning_rate': 0.00022196359005552646, 'batch_size': 184, 'step_size': 10, 'gamma': 0.9343262767489589}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:58:04,805][0m Trial 27 finished with value: 0.059271494562669495 and parameters: {'observation_period_num': 44, 'train_rates': 0.7744911469125004, 'learning_rate': 0.0005850511212578451, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8572842497793622}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:58:35,327][0m Trial 28 finished with value: 0.054062048471210024 and parameters: {'observation_period_num': 21, 'train_rates': 0.8622755311647482, 'learning_rate': 0.0001050111929864985, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9068183223435577}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 20:59:24,576][0m Trial 29 finished with value: 0.05121705998425131 and parameters: {'observation_period_num': 39, 'train_rates': 0.8054348888613974, 'learning_rate': 0.0003794106844396046, 'batch_size': 112, 'step_size': 14, 'gamma': 0.79193209454912}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 21:01:25,753][0m Trial 30 finished with value: 0.034523487746439596 and parameters: {'observation_period_num': 6, 'train_rates': 0.945237395903018, 'learning_rate': 0.0002948987006610546, 'batch_size': 49, 'step_size': 12, 'gamma': 0.8415204465549964}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 21:03:36,458][0m Trial 31 finished with value: 0.03553247649133915 and parameters: {'observation_period_num': 8, 'train_rates': 0.9429748496709194, 'learning_rate': 0.00028330726426010344, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8436175033993829}. Best is trial 15 with value: 0.03233796765350483.[0m
[32m[I 2025-02-03 21:05:50,005][0m Trial 32 finished with value: 0.03122996055361829 and parameters: {'observation_period_num': 18, 'train_rates': 0.9342397272162837, 'learning_rate': 0.0001857005882815361, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8272978339758787}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:08:13,378][0m Trial 33 finished with value: 0.03936883303172448 and parameters: {'observation_period_num': 22, 'train_rates': 0.9480795421233155, 'learning_rate': 0.00017766386232955073, 'batch_size': 42, 'step_size': 12, 'gamma': 0.8039262235097905}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:13:46,855][0m Trial 34 finished with value: 0.06246571288496818 and parameters: {'observation_period_num': 59, 'train_rates': 0.9269709964852291, 'learning_rate': 0.0001421473586926084, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8309925166746203}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:15:41,155][0m Trial 35 finished with value: 0.048081651361564455 and parameters: {'observation_period_num': 38, 'train_rates': 0.9597824716023887, 'learning_rate': 8.282714355759235e-05, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8455362082558754}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:17:02,507][0m Trial 36 finished with value: 0.039798119725369986 and parameters: {'observation_period_num': 21, 'train_rates': 0.8838352170921888, 'learning_rate': 0.0002554419584741594, 'batch_size': 71, 'step_size': 14, 'gamma': 0.782083158858753}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:19:16,016][0m Trial 37 finished with value: 0.08919867684487172 and parameters: {'observation_period_num': 154, 'train_rates': 0.9183841609709014, 'learning_rate': 6.817734876326865e-05, 'batch_size': 42, 'step_size': 13, 'gamma': 0.8216273856063048}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:20:21,742][0m Trial 38 finished with value: 0.13957888633012772 and parameters: {'observation_period_num': 206, 'train_rates': 0.895298505101144, 'learning_rate': 4.022626420096817e-05, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8018917698536682}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:21:57,475][0m Trial 39 finished with value: 0.06795659244060516 and parameters: {'observation_period_num': 38, 'train_rates': 0.9572269557703059, 'learning_rate': 0.0005791770603017771, 'batch_size': 64, 'step_size': 9, 'gamma': 0.8669456769276236}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:23:58,245][0m Trial 40 finished with value: 0.09275654848964734 and parameters: {'observation_period_num': 115, 'train_rates': 0.9694391256254105, 'learning_rate': 0.00023711143295610565, 'batch_size': 49, 'step_size': 2, 'gamma': 0.8292804421992824}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:27:13,235][0m Trial 41 finished with value: 0.03877400039413641 and parameters: {'observation_period_num': 15, 'train_rates': 0.9268245513439767, 'learning_rate': 0.0005119547432488524, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8959362227808566}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:27:42,908][0m Trial 42 finished with value: 0.03921741619706154 and parameters: {'observation_period_num': 5, 'train_rates': 0.9899373965432107, 'learning_rate': 0.0009956602860981118, 'batch_size': 239, 'step_size': 13, 'gamma': 0.8728457790585405}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:33:06,244][0m Trial 43 finished with value: 0.050681375625097884 and parameters: {'observation_period_num': 31, 'train_rates': 0.8740855490682805, 'learning_rate': 0.00030000527643364155, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8440862074177976}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:34:05,192][0m Trial 44 finished with value: 0.05649960371000426 and parameters: {'observation_period_num': 67, 'train_rates': 0.8303437413475082, 'learning_rate': 0.0001992218200792983, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8135752261414833}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:35:18,668][0m Trial 45 finished with value: 0.0497346350568391 and parameters: {'observation_period_num': 48, 'train_rates': 0.9133856319938836, 'learning_rate': 0.000130319702092116, 'batch_size': 79, 'step_size': 13, 'gamma': 0.923115599824479}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:37:05,541][0m Trial 46 finished with value: 0.10251967258313123 and parameters: {'observation_period_num': 87, 'train_rates': 0.9703916560693501, 'learning_rate': 0.0006563697199055407, 'batch_size': 55, 'step_size': 14, 'gamma': 0.8986718808369295}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:39:46,626][0m Trial 47 finished with value: 0.044199658049778504 and parameters: {'observation_period_num': 18, 'train_rates': 0.8949899754111706, 'learning_rate': 7.91792760464555e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8832458618929381}. Best is trial 32 with value: 0.03122996055361829.[0m
Early stopping at epoch 65
[32m[I 2025-02-03 21:40:43,796][0m Trial 48 finished with value: 0.22012463333301766 and parameters: {'observation_period_num': 33, 'train_rates': 0.9413264221879029, 'learning_rate': 4.258721061485694e-05, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8505118465034143}. Best is trial 32 with value: 0.03122996055361829.[0m
[32m[I 2025-02-03 21:41:39,598][0m Trial 49 finished with value: 0.039662007222625806 and parameters: {'observation_period_num': 15, 'train_rates': 0.8720941711250882, 'learning_rate': 0.00015379742000100124, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8366006780061327}. Best is trial 32 with value: 0.03122996055361829.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 21:41:39,608][0m A new study created in memory with name: no-name-9284497c-3714-4b0f-b082-5a098a25a479[0m
[32m[I 2025-02-03 21:42:07,628][0m Trial 0 finished with value: 0.12446513995528222 and parameters: {'observation_period_num': 173, 'train_rates': 0.8562181102065389, 'learning_rate': 0.0003365163939971431, 'batch_size': 205, 'step_size': 5, 'gamma': 0.7796176561139608}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:42:46,919][0m Trial 1 finished with value: 0.8313757838917898 and parameters: {'observation_period_num': 224, 'train_rates': 0.6564747063587435, 'learning_rate': 3.130906214418608e-06, 'batch_size': 112, 'step_size': 7, 'gamma': 0.7949863346336957}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:43:26,166][0m Trial 2 finished with value: 0.21135008667857902 and parameters: {'observation_period_num': 186, 'train_rates': 0.8139675852720982, 'learning_rate': 2.015499303751193e-05, 'batch_size': 140, 'step_size': 14, 'gamma': 0.9230156694344357}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:43:58,396][0m Trial 3 finished with value: 0.6139561390598952 and parameters: {'observation_period_num': 160, 'train_rates': 0.6618846234960701, 'learning_rate': 4.740838877255555e-06, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9753577059281966}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:45:01,691][0m Trial 4 finished with value: 0.16291918069022507 and parameters: {'observation_period_num': 196, 'train_rates': 0.7872429211266943, 'learning_rate': 5.268708082657435e-05, 'batch_size': 80, 'step_size': 6, 'gamma': 0.871924717318871}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:45:42,042][0m Trial 5 finished with value: 0.28958543983099005 and parameters: {'observation_period_num': 106, 'train_rates': 0.6312307507462156, 'learning_rate': 8.579777558745805e-06, 'batch_size': 119, 'step_size': 8, 'gamma': 0.9713512548817227}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:46:07,395][0m Trial 6 finished with value: 0.3443144303831187 and parameters: {'observation_period_num': 159, 'train_rates': 0.6847318348943737, 'learning_rate': 1.1502238213275892e-05, 'batch_size': 212, 'step_size': 15, 'gamma': 0.9016716623902761}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:46:30,766][0m Trial 7 finished with value: 0.4773659335600363 and parameters: {'observation_period_num': 132, 'train_rates': 0.6720864436003613, 'learning_rate': 9.704733964528161e-06, 'batch_size': 225, 'step_size': 10, 'gamma': 0.8633583913861643}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:47:15,916][0m Trial 8 finished with value: 0.159948824315655 and parameters: {'observation_period_num': 179, 'train_rates': 0.8942497916580814, 'learning_rate': 3.640669468076012e-05, 'batch_size': 129, 'step_size': 3, 'gamma': 0.9633306342928082}. Best is trial 0 with value: 0.12446513995528222.[0m
[32m[I 2025-02-03 21:48:09,454][0m Trial 9 finished with value: 0.47836116225446373 and parameters: {'observation_period_num': 236, 'train_rates': 0.6656674171522537, 'learning_rate': 4.602839048607111e-06, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8493620185462057}. Best is trial 0 with value: 0.12446513995528222.[0m
Early stopping at epoch 52
[32m[I 2025-02-03 21:49:51,891][0m Trial 10 finished with value: 0.04461910035938789 and parameters: {'observation_period_num': 17, 'train_rates': 0.9868353333135778, 'learning_rate': 0.000571783062034328, 'batch_size': 32, 'step_size': 1, 'gamma': 0.7546799114884005}. Best is trial 10 with value: 0.04461910035938789.[0m
Early stopping at epoch 68
[32m[I 2025-02-03 21:52:41,635][0m Trial 11 finished with value: 0.038568242685869336 and parameters: {'observation_period_num': 9, 'train_rates': 0.9866275251720715, 'learning_rate': 0.0006560203692934346, 'batch_size': 25, 'step_size': 1, 'gamma': 0.7572728345898556}. Best is trial 11 with value: 0.038568242685869336.[0m
Early stopping at epoch 75
[32m[I 2025-02-03 21:56:01,313][0m Trial 12 finished with value: 0.04188623494124628 and parameters: {'observation_period_num': 6, 'train_rates': 0.9626120553358659, 'learning_rate': 0.0009879743092466961, 'batch_size': 23, 'step_size': 1, 'gamma': 0.7609216291952051}. Best is trial 11 with value: 0.038568242685869336.[0m
Early stopping at epoch 58
[32m[I 2025-02-03 21:58:43,424][0m Trial 13 finished with value: 0.04293343433666797 and parameters: {'observation_period_num': 8, 'train_rates': 0.9857699990555697, 'learning_rate': 0.00016795468453429326, 'batch_size': 22, 'step_size': 1, 'gamma': 0.7970842665888958}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:00:07,442][0m Trial 14 finished with value: 0.0450609837743369 and parameters: {'observation_period_num': 52, 'train_rates': 0.9205205644413729, 'learning_rate': 0.0009629869124956102, 'batch_size': 70, 'step_size': 3, 'gamma': 0.828221250825042}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:02:14,048][0m Trial 15 finished with value: 0.08033442720770836 and parameters: {'observation_period_num': 64, 'train_rates': 0.9344968837591382, 'learning_rate': 0.00010389523914306137, 'batch_size': 47, 'step_size': 3, 'gamma': 0.7579811425751852}. Best is trial 11 with value: 0.038568242685869336.[0m
Early stopping at epoch 68
[32m[I 2025-02-03 22:03:18,137][0m Trial 16 finished with value: 1.8242554672776836 and parameters: {'observation_period_num': 68, 'train_rates': 0.7467623058824969, 'learning_rate': 1.0726131149523414e-06, 'batch_size': 55, 'step_size': 1, 'gamma': 0.8206415683147251}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:03:43,815][0m Trial 17 finished with value: 0.09689132869243622 and parameters: {'observation_period_num': 43, 'train_rates': 0.9560806645231226, 'learning_rate': 0.00022763414897658643, 'batch_size': 253, 'step_size': 4, 'gamma': 0.775781272274455}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:08:13,172][0m Trial 18 finished with value: 0.08269044688265574 and parameters: {'observation_period_num': 91, 'train_rates': 0.8671671864626105, 'learning_rate': 0.0009741616530344584, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8199833763620619}. Best is trial 11 with value: 0.038568242685869336.[0m
Early stopping at epoch 86
[32m[I 2025-02-03 22:08:42,581][0m Trial 19 finished with value: 0.16621092299150453 and parameters: {'observation_period_num': 29, 'train_rates': 0.8988009289495421, 'learning_rate': 8.392644574287005e-05, 'batch_size': 181, 'step_size': 2, 'gamma': 0.7526552957596133}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:09:43,479][0m Trial 20 finished with value: 0.07218085263521616 and parameters: {'observation_period_num': 92, 'train_rates': 0.9544061162657415, 'learning_rate': 0.0003539853004472304, 'batch_size': 99, 'step_size': 4, 'gamma': 0.7949471008974959}. Best is trial 11 with value: 0.038568242685869336.[0m
Early stopping at epoch 63
[32m[I 2025-02-03 22:13:18,276][0m Trial 21 finished with value: 0.04574247399488321 and parameters: {'observation_period_num': 6, 'train_rates': 0.9868319787944524, 'learning_rate': 0.00017546193207755782, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7869001884686587}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:15:30,647][0m Trial 22 finished with value: 0.0409046680563026 and parameters: {'observation_period_num': 27, 'train_rates': 0.9817782151171599, 'learning_rate': 0.0005334551115101508, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8075046420933697}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:17:34,568][0m Trial 23 finished with value: 0.04846363626420498 and parameters: {'observation_period_num': 36, 'train_rates': 0.94510940389997, 'learning_rate': 0.0005295885586413146, 'batch_size': 48, 'step_size': 2, 'gamma': 0.7664187063807}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:19:07,641][0m Trial 24 finished with value: 0.0412809393075231 and parameters: {'observation_period_num': 25, 'train_rates': 0.8504407492242956, 'learning_rate': 0.0005972181594361483, 'batch_size': 60, 'step_size': 4, 'gamma': 0.8077628030986512}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:20:39,594][0m Trial 25 finished with value: 0.059439375291604445 and parameters: {'observation_period_num': 68, 'train_rates': 0.8259472150501758, 'learning_rate': 0.0004676058154578294, 'batch_size': 59, 'step_size': 5, 'gamma': 0.8368037855307984}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:21:42,138][0m Trial 26 finished with value: 0.059618012061801506 and parameters: {'observation_period_num': 36, 'train_rates': 0.905405732966447, 'learning_rate': 0.00010618153549804064, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8068660896558553}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:23:48,753][0m Trial 27 finished with value: 0.0606656163296801 and parameters: {'observation_period_num': 26, 'train_rates': 0.7428766628539448, 'learning_rate': 0.0002719722741135166, 'batch_size': 40, 'step_size': 2, 'gamma': 0.8943919443395006}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:25:10,012][0m Trial 28 finished with value: 0.05083520758848686 and parameters: {'observation_period_num': 60, 'train_rates': 0.8669104595668407, 'learning_rate': 0.0006315831066235039, 'batch_size': 68, 'step_size': 5, 'gamma': 0.8138069474378856}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:26:10,256][0m Trial 29 finished with value: 0.08212757858935957 and parameters: {'observation_period_num': 115, 'train_rates': 0.8401439925290991, 'learning_rate': 0.000421501930249476, 'batch_size': 90, 'step_size': 3, 'gamma': 0.7766971950834521}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:26:42,410][0m Trial 30 finished with value: 0.08017189510513184 and parameters: {'observation_period_num': 84, 'train_rates': 0.759986362369491, 'learning_rate': 0.00024766982958034534, 'batch_size': 171, 'step_size': 6, 'gamma': 0.8442253565321298}. Best is trial 11 with value: 0.038568242685869336.[0m
[32m[I 2025-02-03 22:29:35,502][0m Trial 31 finished with value: 0.03072510173946035 and parameters: {'observation_period_num': 20, 'train_rates': 0.965481968982666, 'learning_rate': 0.0008291904163601667, 'batch_size': 35, 'step_size': 2, 'gamma': 0.7736538504506243}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:32:07,935][0m Trial 32 finished with value: 0.03353936472722204 and parameters: {'observation_period_num': 22, 'train_rates': 0.9213476918592236, 'learning_rate': 0.0007159679753187702, 'batch_size': 38, 'step_size': 2, 'gamma': 0.7753713974256757}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:34:58,409][0m Trial 33 finished with value: 0.048836015785733856 and parameters: {'observation_period_num': 46, 'train_rates': 0.9692039712762419, 'learning_rate': 0.0007330183080322314, 'batch_size': 35, 'step_size': 2, 'gamma': 0.7786697391512692}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:37:36,575][0m Trial 34 finished with value: 0.044277799294487524 and parameters: {'observation_period_num': 23, 'train_rates': 0.9253936202317412, 'learning_rate': 0.0003355690153873345, 'batch_size': 37, 'step_size': 2, 'gamma': 0.7886233065467508}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:39:04,012][0m Trial 35 finished with value: 0.0703347306584288 and parameters: {'observation_period_num': 50, 'train_rates': 0.8878652241242092, 'learning_rate': 0.00013579904574113436, 'batch_size': 65, 'step_size': 3, 'gamma': 0.7722647868993087}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:40:00,958][0m Trial 36 finished with value: 0.05325926511296045 and parameters: {'observation_period_num': 16, 'train_rates': 0.9270794441100729, 'learning_rate': 6.0172665894072304e-05, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8003501139661487}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:41:17,378][0m Trial 37 finished with value: 0.15097473408143544 and parameters: {'observation_period_num': 144, 'train_rates': 0.9656880249639953, 'learning_rate': 0.00036501370730684035, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9471543279284255}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:43:17,660][0m Trial 38 finished with value: 0.09924865084535936 and parameters: {'observation_period_num': 77, 'train_rates': 0.9408843157946875, 'learning_rate': 2.093251400231999e-05, 'batch_size': 48, 'step_size': 8, 'gamma': 0.7868289603173145}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:43:57,919][0m Trial 39 finished with value: 0.0805107516293623 and parameters: {'observation_period_num': 199, 'train_rates': 0.9110056675279143, 'learning_rate': 0.0007199911855438782, 'batch_size': 145, 'step_size': 7, 'gamma': 0.7513068605965568}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:47:07,138][0m Trial 40 finished with value: 0.055143450726481046 and parameters: {'observation_period_num': 32, 'train_rates': 0.8836131005616482, 'learning_rate': 0.00019316426328814024, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8688101966684272}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:48:48,290][0m Trial 41 finished with value: 0.03725889002556329 and parameters: {'observation_period_num': 15, 'train_rates': 0.8451769837571677, 'learning_rate': 0.0006695284103179426, 'batch_size': 54, 'step_size': 2, 'gamma': 0.8109200927429991}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:50:51,536][0m Trial 42 finished with value: 0.05547130075670766 and parameters: {'observation_period_num': 13, 'train_rates': 0.7915874429935037, 'learning_rate': 0.00031261517112732653, 'batch_size': 43, 'step_size': 2, 'gamma': 0.7689498753267078}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 22:56:41,500][0m Trial 43 finished with value: 0.036646581875781216 and parameters: {'observation_period_num': 39, 'train_rates': 0.9876802115537756, 'learning_rate': 0.000490621467026431, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8856333764969091}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 23:01:33,864][0m Trial 44 finished with value: 0.05757243335935315 and parameters: {'observation_period_num': 40, 'train_rates': 0.7026011603637722, 'learning_rate': 0.0007283935495053315, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9068631461472164}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 23:04:56,104][0m Trial 45 finished with value: 0.04396647290597039 and parameters: {'observation_period_num': 15, 'train_rates': 0.9472990376504267, 'learning_rate': 0.0009812094433060631, 'batch_size': 29, 'step_size': 3, 'gamma': 0.8772660905685193}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 23:05:52,452][0m Trial 46 finished with value: 0.0837756283811823 and parameters: {'observation_period_num': 55, 'train_rates': 0.609556458259029, 'learning_rate': 0.0004628452845508615, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9179849753448411}. Best is trial 31 with value: 0.03072510173946035.[0m
[32m[I 2025-02-03 23:09:13,978][0m Trial 47 finished with value: 0.030505120028536042 and parameters: {'observation_period_num': 5, 'train_rates': 0.9739588826782053, 'learning_rate': 0.0007074389761846369, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8889750367516519}. Best is trial 47 with value: 0.030505120028536042.[0m
[32m[I 2025-02-03 23:11:08,361][0m Trial 48 finished with value: 0.03514552509391701 and parameters: {'observation_period_num': 21, 'train_rates': 0.9690040187995225, 'learning_rate': 0.000784506723836477, 'batch_size': 54, 'step_size': 4, 'gamma': 0.880896332687234}. Best is trial 47 with value: 0.030505120028536042.[0m
[32m[I 2025-02-03 23:14:12,511][0m Trial 49 finished with value: 0.18767759390175343 and parameters: {'observation_period_num': 5, 'train_rates': 0.9728624012332552, 'learning_rate': 1.780689292451158e-06, 'batch_size': 33, 'step_size': 3, 'gamma': 0.8864299099556526}. Best is trial 47 with value: 0.030505120028536042.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 23:14:12,521][0m A new study created in memory with name: no-name-607da735-d581-4bf5-b03c-4c8f071378d1[0m
[32m[I 2025-02-03 23:14:44,775][0m Trial 0 finished with value: 0.08280106124834777 and parameters: {'observation_period_num': 70, 'train_rates': 0.923354385045788, 'learning_rate': 6.484727020225029e-05, 'batch_size': 190, 'step_size': 9, 'gamma': 0.883102339069234}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:16:52,848][0m Trial 1 finished with value: 0.14727562413377276 and parameters: {'observation_period_num': 249, 'train_rates': 0.9781839446378486, 'learning_rate': 1.8522045038636442e-05, 'batch_size': 44, 'step_size': 15, 'gamma': 0.8985924700066942}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:18:03,197][0m Trial 2 finished with value: 0.9685154104232788 and parameters: {'observation_period_num': 227, 'train_rates': 0.7248362614862215, 'learning_rate': 1.5054839190445728e-06, 'batch_size': 67, 'step_size': 2, 'gamma': 0.8519164220941458}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:18:26,132][0m Trial 3 finished with value: 0.5713468348239101 and parameters: {'observation_period_num': 133, 'train_rates': 0.6859120586873085, 'learning_rate': 2.432673907459303e-06, 'batch_size': 238, 'step_size': 9, 'gamma': 0.9584060019962508}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:19:25,193][0m Trial 4 finished with value: 0.09962622824764472 and parameters: {'observation_period_num': 117, 'train_rates': 0.9236640178214327, 'learning_rate': 7.458069627324637e-05, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9162836343220397}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:20:07,619][0m Trial 5 finished with value: 0.39260911756707717 and parameters: {'observation_period_num': 208, 'train_rates': 0.9059870800034243, 'learning_rate': 7.850094121862118e-06, 'batch_size': 136, 'step_size': 5, 'gamma': 0.7998473429510452}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:20:48,586][0m Trial 6 finished with value: 0.09352235237382493 and parameters: {'observation_period_num': 70, 'train_rates': 0.9080892737010333, 'learning_rate': 2.3238422057836217e-05, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9373012562394555}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:21:25,606][0m Trial 7 finished with value: 0.09800037502422246 and parameters: {'observation_period_num': 155, 'train_rates': 0.9223404980646421, 'learning_rate': 0.0008368655074213571, 'batch_size': 163, 'step_size': 6, 'gamma': 0.9104960283331339}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:24:20,903][0m Trial 8 finished with value: 0.25026661557395286 and parameters: {'observation_period_num': 120, 'train_rates': 0.8551450128955875, 'learning_rate': 8.792123212746348e-06, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9533613164643527}. Best is trial 0 with value: 0.08280106124834777.[0m
[32m[I 2025-02-03 23:24:59,500][0m Trial 9 finished with value: 0.05054891421967208 and parameters: {'observation_period_num': 51, 'train_rates': 0.8547371728029025, 'learning_rate': 0.0005358127925192453, 'batch_size': 150, 'step_size': 6, 'gamma': 0.7550088318704076}. Best is trial 9 with value: 0.05054891421967208.[0m
[32m[I 2025-02-03 23:25:24,550][0m Trial 10 finished with value: 0.04243018125553365 and parameters: {'observation_period_num': 7, 'train_rates': 0.7716229972013885, 'learning_rate': 0.0006491260175316564, 'batch_size': 248, 'step_size': 13, 'gamma': 0.7658810246743671}. Best is trial 10 with value: 0.04243018125553365.[0m
[32m[I 2025-02-03 23:25:46,940][0m Trial 11 finished with value: 0.04157986863310101 and parameters: {'observation_period_num': 9, 'train_rates': 0.7775302643617836, 'learning_rate': 0.0008633313930623283, 'batch_size': 254, 'step_size': 14, 'gamma': 0.7572739982062296}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:26:11,988][0m Trial 12 finished with value: 0.05305419354324882 and parameters: {'observation_period_num': 7, 'train_rates': 0.7636165399861511, 'learning_rate': 0.00021295350073947795, 'batch_size': 241, 'step_size': 14, 'gamma': 0.7522086341782231}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:26:35,479][0m Trial 13 finished with value: 0.05746935910669861 and parameters: {'observation_period_num': 7, 'train_rates': 0.626890870470936, 'learning_rate': 0.0002585544310278149, 'batch_size': 208, 'step_size': 12, 'gamma': 0.8129127706042537}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:26:59,762][0m Trial 14 finished with value: 0.05743617716951658 and parameters: {'observation_period_num': 40, 'train_rates': 0.7952167994146982, 'learning_rate': 0.0002715502342839938, 'batch_size': 248, 'step_size': 12, 'gamma': 0.8034762648692402}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:27:28,548][0m Trial 15 finished with value: 0.051276266986092965 and parameters: {'observation_period_num': 37, 'train_rates': 0.7885802533136098, 'learning_rate': 0.0008492711632507343, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8405518195742677}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:27:51,549][0m Trial 16 finished with value: 0.11844872742301167 and parameters: {'observation_period_num': 89, 'train_rates': 0.6823699429418433, 'learning_rate': 0.0001086657551926397, 'batch_size': 219, 'step_size': 11, 'gamma': 0.781328562799487}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:28:21,891][0m Trial 17 finished with value: 0.10444627472987542 and parameters: {'observation_period_num': 171, 'train_rates': 0.8317652829675226, 'learning_rate': 0.0003660669963261278, 'batch_size': 180, 'step_size': 15, 'gamma': 0.8319398662755529}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:29:09,701][0m Trial 18 finished with value: 0.07340840733484923 and parameters: {'observation_period_num': 5, 'train_rates': 0.7246969354048938, 'learning_rate': 0.00016137093401720365, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9884724922174339}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:29:31,572][0m Trial 19 finished with value: 0.18527256005788703 and parameters: {'observation_period_num': 93, 'train_rates': 0.7364896560023493, 'learning_rate': 4.621105403427968e-05, 'batch_size': 256, 'step_size': 11, 'gamma': 0.7718373994043211}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:29:54,771][0m Trial 20 finished with value: 0.07317449372065694 and parameters: {'observation_period_num': 40, 'train_rates': 0.6736426753741687, 'learning_rate': 0.000960690976364777, 'batch_size': 222, 'step_size': 8, 'gamma': 0.7810892689008841}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:31:03,316][0m Trial 21 finished with value: 0.048349039927943725 and parameters: {'observation_period_num': 32, 'train_rates': 0.8441632832514051, 'learning_rate': 0.0004666729684686517, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7561863745268136}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:32:04,109][0m Trial 22 finished with value: 0.05530209726949118 and parameters: {'observation_period_num': 22, 'train_rates': 0.8201827181022407, 'learning_rate': 0.0005215758028479765, 'batch_size': 92, 'step_size': 4, 'gamma': 0.750747064921521}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:33:21,355][0m Trial 23 finished with value: 0.06877207866840189 and parameters: {'observation_period_num': 59, 'train_rates': 0.7722484035269485, 'learning_rate': 0.00047719554750926945, 'batch_size': 66, 'step_size': 7, 'gamma': 0.7740161133405015}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:34:11,896][0m Trial 24 finished with value: 0.04696061123994369 and parameters: {'observation_period_num': 26, 'train_rates': 0.8593039756340991, 'learning_rate': 0.00013278917698738185, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8188621234983787}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:34:48,838][0m Trial 25 finished with value: 0.044023290308187094 and parameters: {'observation_period_num': 16, 'train_rates': 0.8798157058771278, 'learning_rate': 0.00014250334370004172, 'batch_size': 169, 'step_size': 14, 'gamma': 0.8148285047086061}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:35:15,780][0m Trial 26 finished with value: 0.07459412790510965 and parameters: {'observation_period_num': 87, 'train_rates': 0.8073781458267494, 'learning_rate': 0.0002878364644816118, 'batch_size': 229, 'step_size': 14, 'gamma': 0.8629492100700638}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:35:51,327][0m Trial 27 finished with value: 0.05053823068737984 and parameters: {'observation_period_num': 22, 'train_rates': 0.9792807878812056, 'learning_rate': 8.587627245758907e-05, 'batch_size': 182, 'step_size': 13, 'gamma': 0.7944621651142834}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:36:13,600][0m Trial 28 finished with value: 0.08396581967531795 and parameters: {'observation_period_num': 53, 'train_rates': 0.7549989621005655, 'learning_rate': 0.00019710166335244365, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8349805631648672}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:36:45,306][0m Trial 29 finished with value: 0.11506985977431324 and parameters: {'observation_period_num': 60, 'train_rates': 0.8891006271383122, 'learning_rate': 3.456995454348608e-05, 'batch_size': 195, 'step_size': 10, 'gamma': 0.8779347307584816}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:37:20,020][0m Trial 30 finished with value: 0.08654170665942447 and parameters: {'observation_period_num': 75, 'train_rates': 0.87682495114745, 'learning_rate': 5.650191433455546e-05, 'batch_size': 169, 'step_size': 12, 'gamma': 0.7893714246262111}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:38:12,924][0m Trial 31 finished with value: 0.046670401923176716 and parameters: {'observation_period_num': 22, 'train_rates': 0.9442767967507715, 'learning_rate': 0.00011786939017562323, 'batch_size': 118, 'step_size': 14, 'gamma': 0.8217201118174937}. Best is trial 11 with value: 0.04157986863310101.[0m
[32m[I 2025-02-03 23:38:44,587][0m Trial 32 finished with value: 0.035588204860687256 and parameters: {'observation_period_num': 15, 'train_rates': 0.9497212910664218, 'learning_rate': 0.000635215421587085, 'batch_size': 209, 'step_size': 15, 'gamma': 0.818141966628487}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:39:16,659][0m Trial 33 finished with value: 0.03820252791047096 and parameters: {'observation_period_num': 7, 'train_rates': 0.9606112312664858, 'learning_rate': 0.0006939966965470579, 'batch_size': 212, 'step_size': 15, 'gamma': 0.7665576400354811}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:39:46,388][0m Trial 34 finished with value: 0.038581304252147675 and parameters: {'observation_period_num': 5, 'train_rates': 0.950504338579367, 'learning_rate': 0.0006241970751803215, 'batch_size': 232, 'step_size': 15, 'gamma': 0.7688655180516991}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:40:18,186][0m Trial 35 finished with value: 0.0457717590034008 and parameters: {'observation_period_num': 32, 'train_rates': 0.9510552209606234, 'learning_rate': 0.0007185813553158425, 'batch_size': 208, 'step_size': 15, 'gamma': 0.7872118930610947}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:40:45,651][0m Trial 36 finished with value: 0.053967442363500595 and parameters: {'observation_period_num': 47, 'train_rates': 0.9582724263564247, 'learning_rate': 0.0004143958992500468, 'batch_size': 235, 'step_size': 15, 'gamma': 0.7664591631039197}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:41:15,405][0m Trial 37 finished with value: 0.15344201028347015 and parameters: {'observation_period_num': 198, 'train_rates': 0.9858358343889889, 'learning_rate': 0.000994619121163557, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8519669945095566}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:41:47,519][0m Trial 38 finished with value: 0.5729334507838334 and parameters: {'observation_period_num': 239, 'train_rates': 0.9256213973899153, 'learning_rate': 2.5344272516264407e-06, 'batch_size': 191, 'step_size': 11, 'gamma': 0.803994506927925}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:42:15,001][0m Trial 39 finished with value: 0.14181147515773773 and parameters: {'observation_period_num': 110, 'train_rates': 0.9552602882820929, 'learning_rate': 0.000320090483260236, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8897411970611171}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:42:44,587][0m Trial 40 finished with value: 0.2258478805693713 and parameters: {'observation_period_num': 69, 'train_rates': 0.9045890691391754, 'learning_rate': 1.5257021928526613e-05, 'batch_size': 213, 'step_size': 15, 'gamma': 0.7653007321137639}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:43:07,594][0m Trial 41 finished with value: 0.052727702599961145 and parameters: {'observation_period_num': 14, 'train_rates': 0.7119082017274715, 'learning_rate': 0.0006472823404641638, 'batch_size': 245, 'step_size': 14, 'gamma': 0.764413205118019}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:43:35,247][0m Trial 42 finished with value: 0.039467908442020416 and parameters: {'observation_period_num': 5, 'train_rates': 0.94354494402372, 'learning_rate': 0.0006438089597680114, 'batch_size': 234, 'step_size': 13, 'gamma': 0.7758128636161781}. Best is trial 32 with value: 0.035588204860687256.[0m
[32m[I 2025-02-03 23:44:03,110][0m Trial 43 finished with value: 0.030590631067752838 and parameters: {'observation_period_num': 17, 'train_rates': 0.934730906898141, 'learning_rate': 0.0006366548478941047, 'batch_size': 235, 'step_size': 14, 'gamma': 0.7957721371472752}. Best is trial 43 with value: 0.030590631067752838.[0m
[32m[I 2025-02-03 23:44:30,355][0m Trial 44 finished with value: 0.08147481828927994 and parameters: {'observation_period_num': 141, 'train_rates': 0.9365675057535525, 'learning_rate': 0.0005922676548607925, 'batch_size': 236, 'step_size': 12, 'gamma': 0.7978180724294255}. Best is trial 43 with value: 0.030590631067752838.[0m
[32m[I 2025-02-03 23:45:03,795][0m Trial 45 finished with value: 0.04254741221666336 and parameters: {'observation_period_num': 27, 'train_rates': 0.967001630528607, 'learning_rate': 0.00040866991867342207, 'batch_size': 201, 'step_size': 15, 'gamma': 0.778826995031297}. Best is trial 43 with value: 0.030590631067752838.[0m
[32m[I 2025-02-03 23:45:32,311][0m Trial 46 finished with value: 0.055227439579379055 and parameters: {'observation_period_num': 46, 'train_rates': 0.9101688635655965, 'learning_rate': 0.00019616441146376798, 'batch_size': 226, 'step_size': 13, 'gamma': 0.8066559399253344}. Best is trial 43 with value: 0.030590631067752838.[0m
[32m[I 2025-02-03 23:46:17,245][0m Trial 47 finished with value: 0.030257340520620346 and parameters: {'observation_period_num': 16, 'train_rates': 0.9692252531679102, 'learning_rate': 0.0006329125308394092, 'batch_size': 145, 'step_size': 9, 'gamma': 0.7899596170977622}. Best is trial 47 with value: 0.030257340520620346.[0m
[32m[I 2025-02-03 23:47:02,065][0m Trial 48 finished with value: 0.52696692943573 and parameters: {'observation_period_num': 17, 'train_rates': 0.9702925167916988, 'learning_rate': 1.2104044050780328e-06, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8241040155161001}. Best is trial 47 with value: 0.030257340520620346.[0m
[32m[I 2025-02-03 23:47:46,612][0m Trial 49 finished with value: 0.05498095989496999 and parameters: {'observation_period_num': 36, 'train_rates': 0.9244292218323779, 'learning_rate': 0.00033221086860092035, 'batch_size': 135, 'step_size': 3, 'gamma': 0.7893675278774088}. Best is trial 47 with value: 0.030257340520620346.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.9891957836612878, 'learning_rate': 0.0003950632010224091, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8537582930945804}
Epoch 1/300, trend Loss: 0.1805 | 0.1184
Epoch 2/300, trend Loss: 0.0959 | 0.0839
Epoch 3/300, trend Loss: 0.0782 | 0.0666
Epoch 4/300, trend Loss: 0.0722 | 0.0704
Epoch 5/300, trend Loss: 0.0653 | 0.0672
Epoch 6/300, trend Loss: 0.0588 | 0.0578
Epoch 7/300, trend Loss: 0.0545 | 0.0514
Epoch 8/300, trend Loss: 0.0507 | 0.0513
Epoch 9/300, trend Loss: 0.0477 | 0.0472
Epoch 10/300, trend Loss: 0.0456 | 0.0448
Epoch 11/300, trend Loss: 0.0437 | 0.0442
Epoch 12/300, trend Loss: 0.0421 | 0.0448
Epoch 13/300, trend Loss: 0.0408 | 0.0440
Epoch 14/300, trend Loss: 0.0391 | 0.0439
Epoch 15/300, trend Loss: 0.0377 | 0.0435
Epoch 16/300, trend Loss: 0.0370 | 0.0423
Epoch 17/300, trend Loss: 0.0361 | 0.0406
Epoch 18/300, trend Loss: 0.0356 | 0.0390
Epoch 19/300, trend Loss: 0.0351 | 0.0385
Epoch 20/300, trend Loss: 0.0345 | 0.0384
Epoch 21/300, trend Loss: 0.0333 | 0.0370
Epoch 22/300, trend Loss: 0.0332 | 0.0359
Epoch 23/300, trend Loss: 0.0334 | 0.0347
Epoch 24/300, trend Loss: 0.0329 | 0.0343
Epoch 25/300, trend Loss: 0.0366 | 0.0373
Epoch 26/300, trend Loss: 0.0329 | 0.0323
Epoch 27/300, trend Loss: 0.0311 | 0.0303
Epoch 28/300, trend Loss: 0.0307 | 0.0303
Epoch 29/300, trend Loss: 0.0304 | 0.0298
Epoch 30/300, trend Loss: 0.0306 | 0.0299
Epoch 31/300, trend Loss: 0.0311 | 0.0298
Epoch 32/300, trend Loss: 0.0296 | 0.0274
Epoch 33/300, trend Loss: 0.0310 | 0.0284
Epoch 34/300, trend Loss: 0.0290 | 0.0255
Epoch 35/300, trend Loss: 0.0279 | 0.0247
Epoch 36/300, trend Loss: 0.0284 | 0.0250
Epoch 37/300, trend Loss: 0.0292 | 0.0256
Epoch 38/300, trend Loss: 0.0283 | 0.0220
Epoch 39/300, trend Loss: 0.0342 | 0.0255
Epoch 40/300, trend Loss: 0.0271 | 0.0214
Epoch 41/300, trend Loss: 0.0280 | 0.0232
Epoch 42/300, trend Loss: 0.0260 | 0.0226
Epoch 43/300, trend Loss: 0.0254 | 0.0222
Epoch 44/300, trend Loss: 0.0252 | 0.0227
Epoch 45/300, trend Loss: 0.0250 | 0.0213
Epoch 46/300, trend Loss: 0.0247 | 0.0215
Epoch 47/300, trend Loss: 0.0255 | 0.0221
Epoch 48/300, trend Loss: 0.0246 | 0.0215
Epoch 49/300, trend Loss: 0.0244 | 0.0210
Epoch 50/300, trend Loss: 0.0238 | 0.0213
Epoch 51/300, trend Loss: 0.0236 | 0.0199
Epoch 52/300, trend Loss: 0.0234 | 0.0200
Epoch 53/300, trend Loss: 0.0234 | 0.0209
Epoch 54/300, trend Loss: 0.0233 | 0.0224
Epoch 55/300, trend Loss: 0.0234 | 0.0234
Epoch 56/300, trend Loss: 0.0232 | 0.0249
Epoch 57/300, trend Loss: 0.0234 | 0.0257
Epoch 58/300, trend Loss: 0.0234 | 0.0278
Epoch 59/300, trend Loss: 0.0237 | 0.0284
Epoch 60/300, trend Loss: 0.0239 | 0.0225
Epoch 61/300, trend Loss: 0.0247 | 0.0260
Epoch 62/300, trend Loss: 0.0250 | 0.0241
Epoch 63/300, trend Loss: 0.0261 | 0.0274
Epoch 64/300, trend Loss: 0.0248 | 0.0246
Epoch 65/300, trend Loss: 0.0263 | 0.0250
Epoch 66/300, trend Loss: 0.0232 | 0.0198
Epoch 67/300, trend Loss: 0.0235 | 0.0204
Epoch 68/300, trend Loss: 0.0225 | 0.0191
Epoch 69/300, trend Loss: 0.0221 | 0.0194
Epoch 70/300, trend Loss: 0.0218 | 0.0193
Epoch 71/300, trend Loss: 0.0216 | 0.0195
Epoch 72/300, trend Loss: 0.0215 | 0.0192
Epoch 73/300, trend Loss: 0.0214 | 0.0197
Epoch 74/300, trend Loss: 0.0213 | 0.0193
Epoch 75/300, trend Loss: 0.0212 | 0.0194
Epoch 76/300, trend Loss: 0.0211 | 0.0191
Epoch 77/300, trend Loss: 0.0211 | 0.0195
Epoch 78/300, trend Loss: 0.0211 | 0.0190
Epoch 79/300, trend Loss: 0.0211 | 0.0190
Epoch 80/300, trend Loss: 0.0212 | 0.0186
Epoch 81/300, trend Loss: 0.0213 | 0.0184
Epoch 82/300, trend Loss: 0.0217 | 0.0178
Epoch 83/300, trend Loss: 0.0214 | 0.0182
Epoch 84/300, trend Loss: 0.0212 | 0.0185
Epoch 85/300, trend Loss: 0.0211 | 0.0183
Epoch 86/300, trend Loss: 0.0209 | 0.0180
Epoch 87/300, trend Loss: 0.0211 | 0.0185
Epoch 88/300, trend Loss: 0.0207 | 0.0179
Epoch 89/300, trend Loss: 0.0206 | 0.0179
Epoch 90/300, trend Loss: 0.0205 | 0.0175
Epoch 91/300, trend Loss: 0.0204 | 0.0174
Epoch 92/300, trend Loss: 0.0204 | 0.0172
Epoch 93/300, trend Loss: 0.0203 | 0.0173
Epoch 94/300, trend Loss: 0.0201 | 0.0171
Epoch 95/300, trend Loss: 0.0199 | 0.0172
Epoch 96/300, trend Loss: 0.0198 | 0.0171
Epoch 97/300, trend Loss: 0.0197 | 0.0171
Epoch 98/300, trend Loss: 0.0197 | 0.0170
Epoch 99/300, trend Loss: 0.0195 | 0.0172
Epoch 100/300, trend Loss: 0.0195 | 0.0171
Epoch 101/300, trend Loss: 0.0194 | 0.0171
Epoch 102/300, trend Loss: 0.0194 | 0.0171
Epoch 103/300, trend Loss: 0.0193 | 0.0171
Epoch 104/300, trend Loss: 0.0193 | 0.0171
Epoch 105/300, trend Loss: 0.0192 | 0.0173
Epoch 106/300, trend Loss: 0.0192 | 0.0173
Epoch 107/300, trend Loss: 0.0191 | 0.0173
Epoch 108/300, trend Loss: 0.0191 | 0.0173
Epoch 109/300, trend Loss: 0.0190 | 0.0173
Epoch 110/300, trend Loss: 0.0190 | 0.0173
Epoch 111/300, trend Loss: 0.0190 | 0.0173
Epoch 112/300, trend Loss: 0.0190 | 0.0170
Epoch 113/300, trend Loss: 0.0190 | 0.0170
Epoch 114/300, trend Loss: 0.0190 | 0.0170
Epoch 115/300, trend Loss: 0.0190 | 0.0170
Epoch 116/300, trend Loss: 0.0190 | 0.0170
Epoch 117/300, trend Loss: 0.0189 | 0.0170
Epoch 118/300, trend Loss: 0.0189 | 0.0170
Epoch 119/300, trend Loss: 0.0189 | 0.0170
Epoch 120/300, trend Loss: 0.0189 | 0.0171
Epoch 121/300, trend Loss: 0.0189 | 0.0171
Epoch 122/300, trend Loss: 0.0189 | 0.0171
Epoch 123/300, trend Loss: 0.0190 | 0.0170
Epoch 124/300, trend Loss: 0.0190 | 0.0170
Epoch 125/300, trend Loss: 0.0191 | 0.0174
Epoch 126/300, trend Loss: 0.0192 | 0.0176
Epoch 127/300, trend Loss: 0.0191 | 0.0176
Epoch 128/300, trend Loss: 0.0190 | 0.0176
Epoch 129/300, trend Loss: 0.0189 | 0.0177
Epoch 130/300, trend Loss: 0.0187 | 0.0178
Epoch 131/300, trend Loss: 0.0186 | 0.0179
Epoch 132/300, trend Loss: 0.0186 | 0.0179
Epoch 133/300, trend Loss: 0.0186 | 0.0182
Epoch 134/300, trend Loss: 0.0185 | 0.0183
Epoch 135/300, trend Loss: 0.0185 | 0.0182
Epoch 136/300, trend Loss: 0.0185 | 0.0183
Epoch 137/300, trend Loss: 0.0186 | 0.0183
Epoch 138/300, trend Loss: 0.0187 | 0.0176
Epoch 139/300, trend Loss: 0.0187 | 0.0174
Epoch 140/300, trend Loss: 0.0190 | 0.0174
Epoch 141/300, trend Loss: 0.0190 | 0.0173
Epoch 142/300, trend Loss: 0.0192 | 0.0174
Epoch 143/300, trend Loss: 0.0191 | 0.0178
Epoch 144/300, trend Loss: 0.0193 | 0.0194
Epoch 145/300, trend Loss: 0.0189 | 0.0189
Epoch 146/300, trend Loss: 0.0186 | 0.0183
Epoch 147/300, trend Loss: 0.0184 | 0.0179
Epoch 148/300, trend Loss: 0.0183 | 0.0178
Epoch 149/300, trend Loss: 0.0184 | 0.0179
Epoch 150/300, trend Loss: 0.0184 | 0.0179
Epoch 151/300, trend Loss: 0.0185 | 0.0182
Epoch 152/300, trend Loss: 0.0185 | 0.0180
Epoch 153/300, trend Loss: 0.0184 | 0.0180
Epoch 154/300, trend Loss: 0.0183 | 0.0178
Epoch 155/300, trend Loss: 0.0182 | 0.0178
Epoch 156/300, trend Loss: 0.0181 | 0.0177
Epoch 157/300, trend Loss: 0.0181 | 0.0177
Epoch 158/300, trend Loss: 0.0180 | 0.0176
Epoch 159/300, trend Loss: 0.0180 | 0.0175
Epoch 160/300, trend Loss: 0.0179 | 0.0175
Epoch 161/300, trend Loss: 0.0179 | 0.0174
Epoch 162/300, trend Loss: 0.0178 | 0.0174
Epoch 163/300, trend Loss: 0.0178 | 0.0174
Epoch 164/300, trend Loss: 0.0178 | 0.0174
Epoch 165/300, trend Loss: 0.0178 | 0.0173
Epoch 166/300, trend Loss: 0.0177 | 0.0173
Epoch 167/300, trend Loss: 0.0177 | 0.0173
Epoch 168/300, trend Loss: 0.0177 | 0.0173
Epoch 169/300, trend Loss: 0.0177 | 0.0172
Epoch 170/300, trend Loss: 0.0176 | 0.0172
Epoch 171/300, trend Loss: 0.0176 | 0.0172
Epoch 172/300, trend Loss: 0.0176 | 0.0172
Epoch 173/300, trend Loss: 0.0176 | 0.0172
Epoch 174/300, trend Loss: 0.0176 | 0.0171
Epoch 175/300, trend Loss: 0.0176 | 0.0171
Epoch 176/300, trend Loss: 0.0175 | 0.0171
Epoch 177/300, trend Loss: 0.0175 | 0.0171
Epoch 178/300, trend Loss: 0.0175 | 0.0171
Epoch 179/300, trend Loss: 0.0175 | 0.0171
Epoch 180/300, trend Loss: 0.0175 | 0.0171
Epoch 181/300, trend Loss: 0.0175 | 0.0170
Epoch 182/300, trend Loss: 0.0175 | 0.0170
Epoch 183/300, trend Loss: 0.0175 | 0.0170
Epoch 184/300, trend Loss: 0.0175 | 0.0170
Epoch 185/300, trend Loss: 0.0175 | 0.0170
Epoch 186/300, trend Loss: 0.0174 | 0.0170
Epoch 187/300, trend Loss: 0.0174 | 0.0170
Epoch 188/300, trend Loss: 0.0174 | 0.0170
Epoch 189/300, trend Loss: 0.0174 | 0.0170
Epoch 190/300, trend Loss: 0.0174 | 0.0170
Epoch 191/300, trend Loss: 0.0174 | 0.0170
Epoch 192/300, trend Loss: 0.0174 | 0.0170
Epoch 193/300, trend Loss: 0.0174 | 0.0170
Epoch 194/300, trend Loss: 0.0174 | 0.0170
Epoch 195/300, trend Loss: 0.0174 | 0.0170
Epoch 196/300, trend Loss: 0.0174 | 0.0170
Epoch 197/300, trend Loss: 0.0174 | 0.0170
Epoch 198/300, trend Loss: 0.0174 | 0.0170
Epoch 199/300, trend Loss: 0.0174 | 0.0170
Epoch 200/300, trend Loss: 0.0174 | 0.0170
Epoch 201/300, trend Loss: 0.0174 | 0.0170
Epoch 202/300, trend Loss: 0.0173 | 0.0170
Epoch 203/300, trend Loss: 0.0173 | 0.0170
Epoch 204/300, trend Loss: 0.0173 | 0.0170
Epoch 205/300, trend Loss: 0.0173 | 0.0170
Epoch 206/300, trend Loss: 0.0173 | 0.0170
Epoch 207/300, trend Loss: 0.0173 | 0.0170
Epoch 208/300, trend Loss: 0.0173 | 0.0170
Epoch 209/300, trend Loss: 0.0173 | 0.0169
Epoch 210/300, trend Loss: 0.0173 | 0.0169
Epoch 211/300, trend Loss: 0.0173 | 0.0169
Epoch 212/300, trend Loss: 0.0173 | 0.0169
Epoch 213/300, trend Loss: 0.0173 | 0.0169
Epoch 214/300, trend Loss: 0.0173 | 0.0169
Epoch 215/300, trend Loss: 0.0173 | 0.0169
Epoch 216/300, trend Loss: 0.0173 | 0.0169
Epoch 217/300, trend Loss: 0.0173 | 0.0169
Epoch 218/300, trend Loss: 0.0172 | 0.0169
Epoch 219/300, trend Loss: 0.0172 | 0.0169
Epoch 220/300, trend Loss: 0.0172 | 0.0169
Epoch 221/300, trend Loss: 0.0172 | 0.0169
Epoch 222/300, trend Loss: 0.0172 | 0.0169
Epoch 223/300, trend Loss: 0.0172 | 0.0169
Epoch 224/300, trend Loss: 0.0172 | 0.0169
Epoch 225/300, trend Loss: 0.0172 | 0.0169
Epoch 226/300, trend Loss: 0.0172 | 0.0169
Epoch 227/300, trend Loss: 0.0172 | 0.0169
Epoch 228/300, trend Loss: 0.0172 | 0.0169
Epoch 229/300, trend Loss: 0.0172 | 0.0168
Epoch 230/300, trend Loss: 0.0172 | 0.0168
Epoch 231/300, trend Loss: 0.0172 | 0.0168
Epoch 232/300, trend Loss: 0.0172 | 0.0168
Epoch 233/300, trend Loss: 0.0172 | 0.0168
Epoch 234/300, trend Loss: 0.0172 | 0.0168
Epoch 235/300, trend Loss: 0.0171 | 0.0168
Epoch 236/300, trend Loss: 0.0171 | 0.0168
Epoch 237/300, trend Loss: 0.0171 | 0.0168
Epoch 238/300, trend Loss: 0.0171 | 0.0168
Epoch 239/300, trend Loss: 0.0171 | 0.0168
Epoch 240/300, trend Loss: 0.0171 | 0.0168
Epoch 241/300, trend Loss: 0.0171 | 0.0168
Epoch 242/300, trend Loss: 0.0171 | 0.0168
Epoch 243/300, trend Loss: 0.0171 | 0.0168
Epoch 244/300, trend Loss: 0.0171 | 0.0168
Epoch 245/300, trend Loss: 0.0171 | 0.0168
Epoch 246/300, trend Loss: 0.0171 | 0.0168
Epoch 247/300, trend Loss: 0.0171 | 0.0168
Epoch 248/300, trend Loss: 0.0171 | 0.0168
Epoch 249/300, trend Loss: 0.0171 | 0.0168
Epoch 250/300, trend Loss: 0.0171 | 0.0168
Epoch 251/300, trend Loss: 0.0171 | 0.0168
Epoch 252/300, trend Loss: 0.0171 | 0.0168
Epoch 253/300, trend Loss: 0.0171 | 0.0168
Epoch 254/300, trend Loss: 0.0171 | 0.0168
Epoch 255/300, trend Loss: 0.0171 | 0.0167
Epoch 256/300, trend Loss: 0.0171 | 0.0167
Epoch 257/300, trend Loss: 0.0171 | 0.0167
Epoch 258/300, trend Loss: 0.0171 | 0.0167
Epoch 259/300, trend Loss: 0.0171 | 0.0167
Epoch 260/300, trend Loss: 0.0171 | 0.0167
Epoch 261/300, trend Loss: 0.0171 | 0.0167
Epoch 262/300, trend Loss: 0.0171 | 0.0167
Epoch 263/300, trend Loss: 0.0171 | 0.0167
Epoch 264/300, trend Loss: 0.0171 | 0.0167
Epoch 265/300, trend Loss: 0.0171 | 0.0167
Epoch 266/300, trend Loss: 0.0171 | 0.0167
Epoch 267/300, trend Loss: 0.0171 | 0.0167
Epoch 268/300, trend Loss: 0.0171 | 0.0167
Epoch 269/300, trend Loss: 0.0170 | 0.0167
Epoch 270/300, trend Loss: 0.0170 | 0.0167
Epoch 271/300, trend Loss: 0.0170 | 0.0167
Epoch 272/300, trend Loss: 0.0170 | 0.0167
Epoch 273/300, trend Loss: 0.0170 | 0.0167
Epoch 274/300, trend Loss: 0.0170 | 0.0167
Epoch 275/300, trend Loss: 0.0170 | 0.0167
Epoch 276/300, trend Loss: 0.0170 | 0.0167
Epoch 277/300, trend Loss: 0.0170 | 0.0167
Epoch 278/300, trend Loss: 0.0170 | 0.0167
Epoch 279/300, trend Loss: 0.0170 | 0.0167
Epoch 280/300, trend Loss: 0.0170 | 0.0167
Epoch 281/300, trend Loss: 0.0170 | 0.0167
Epoch 282/300, trend Loss: 0.0170 | 0.0167
Epoch 283/300, trend Loss: 0.0170 | 0.0167
Epoch 284/300, trend Loss: 0.0170 | 0.0167
Epoch 285/300, trend Loss: 0.0170 | 0.0167
Epoch 286/300, trend Loss: 0.0170 | 0.0167
Epoch 287/300, trend Loss: 0.0170 | 0.0167
Epoch 288/300, trend Loss: 0.0170 | 0.0167
Epoch 289/300, trend Loss: 0.0170 | 0.0167
Epoch 290/300, trend Loss: 0.0170 | 0.0167
Epoch 291/300, trend Loss: 0.0170 | 0.0167
Epoch 292/300, trend Loss: 0.0170 | 0.0167
Epoch 293/300, trend Loss: 0.0170 | 0.0167
Epoch 294/300, trend Loss: 0.0170 | 0.0167
Epoch 295/300, trend Loss: 0.0170 | 0.0167
Epoch 296/300, trend Loss: 0.0170 | 0.0167
Epoch 297/300, trend Loss: 0.0170 | 0.0167
Epoch 298/300, trend Loss: 0.0170 | 0.0167
Epoch 299/300, trend Loss: 0.0170 | 0.0167
Epoch 300/300, trend Loss: 0.0170 | 0.0167
Training seasonal_0 component with params: {'observation_period_num': 9, 'train_rates': 0.888116828105302, 'learning_rate': 0.0009883213953294518, 'batch_size': 146, 'step_size': 8, 'gamma': 0.7702215563019117}
Epoch 1/300, seasonal_0 Loss: 0.4670 | 0.2156
Epoch 2/300, seasonal_0 Loss: 0.1514 | 0.1371
Epoch 3/300, seasonal_0 Loss: 0.1174 | 0.1916
Epoch 4/300, seasonal_0 Loss: 0.0852 | 0.1073
Epoch 5/300, seasonal_0 Loss: 0.0875 | 0.1466
Epoch 6/300, seasonal_0 Loss: 0.0984 | 0.0997
Epoch 7/300, seasonal_0 Loss: 0.1137 | 0.1090
Epoch 8/300, seasonal_0 Loss: 0.0863 | 0.1240
Epoch 9/300, seasonal_0 Loss: 0.0756 | 0.0833
Epoch 10/300, seasonal_0 Loss: 0.0738 | 0.0699
Epoch 11/300, seasonal_0 Loss: 0.0821 | 0.0855
Epoch 12/300, seasonal_0 Loss: 0.0876 | 0.0724
Epoch 13/300, seasonal_0 Loss: 0.0756 | 0.0708
Epoch 14/300, seasonal_0 Loss: 0.0824 | 0.0767
Epoch 15/300, seasonal_0 Loss: 0.0777 | 0.0667
Epoch 16/300, seasonal_0 Loss: 0.0628 | 0.0616
Epoch 17/300, seasonal_0 Loss: 0.0568 | 0.0672
Epoch 18/300, seasonal_0 Loss: 0.0566 | 0.0857
Epoch 19/300, seasonal_0 Loss: 0.0576 | 0.0803
Epoch 20/300, seasonal_0 Loss: 0.0570 | 0.0593
Epoch 21/300, seasonal_0 Loss: 0.0517 | 0.0626
Epoch 22/300, seasonal_0 Loss: 0.0515 | 0.0595
Epoch 23/300, seasonal_0 Loss: 0.0504 | 0.0575
Epoch 24/300, seasonal_0 Loss: 0.0501 | 0.0633
Epoch 25/300, seasonal_0 Loss: 0.0498 | 0.0566
Epoch 26/300, seasonal_0 Loss: 0.0495 | 0.0531
Epoch 27/300, seasonal_0 Loss: 0.0492 | 0.0639
Epoch 28/300, seasonal_0 Loss: 0.0495 | 0.0562
Epoch 29/300, seasonal_0 Loss: 0.0488 | 0.0515
Epoch 30/300, seasonal_0 Loss: 0.0485 | 0.0616
Epoch 31/300, seasonal_0 Loss: 0.0478 | 0.0521
Epoch 32/300, seasonal_0 Loss: 0.0463 | 0.0493
Epoch 33/300, seasonal_0 Loss: 0.0469 | 0.0570
Epoch 34/300, seasonal_0 Loss: 0.0461 | 0.0517
Epoch 35/300, seasonal_0 Loss: 0.0455 | 0.0505
Epoch 36/300, seasonal_0 Loss: 0.0454 | 0.0540
Epoch 37/300, seasonal_0 Loss: 0.0445 | 0.0510
Epoch 38/300, seasonal_0 Loss: 0.0438 | 0.0512
Epoch 39/300, seasonal_0 Loss: 0.0435 | 0.0521
Epoch 40/300, seasonal_0 Loss: 0.0432 | 0.0506
Epoch 41/300, seasonal_0 Loss: 0.0431 | 0.0510
Epoch 42/300, seasonal_0 Loss: 0.0428 | 0.0507
Epoch 43/300, seasonal_0 Loss: 0.0425 | 0.0505
Epoch 44/300, seasonal_0 Loss: 0.0424 | 0.0504
Epoch 45/300, seasonal_0 Loss: 0.0422 | 0.0502
Epoch 46/300, seasonal_0 Loss: 0.0421 | 0.0501
Epoch 47/300, seasonal_0 Loss: 0.0420 | 0.0501
Epoch 48/300, seasonal_0 Loss: 0.0419 | 0.0500
Epoch 49/300, seasonal_0 Loss: 0.0418 | 0.0499
Epoch 50/300, seasonal_0 Loss: 0.0417 | 0.0498
Epoch 51/300, seasonal_0 Loss: 0.0417 | 0.0497
Epoch 52/300, seasonal_0 Loss: 0.0416 | 0.0496
Epoch 53/300, seasonal_0 Loss: 0.0415 | 0.0495
Epoch 54/300, seasonal_0 Loss: 0.0413 | 0.0495
Epoch 55/300, seasonal_0 Loss: 0.0412 | 0.0494
Epoch 56/300, seasonal_0 Loss: 0.0412 | 0.0493
Epoch 57/300, seasonal_0 Loss: 0.0411 | 0.0492
Epoch 58/300, seasonal_0 Loss: 0.0410 | 0.0492
Epoch 59/300, seasonal_0 Loss: 0.0410 | 0.0491
Epoch 60/300, seasonal_0 Loss: 0.0409 | 0.0490
Epoch 61/300, seasonal_0 Loss: 0.0408 | 0.0490
Epoch 62/300, seasonal_0 Loss: 0.0408 | 0.0489
Epoch 63/300, seasonal_0 Loss: 0.0407 | 0.0489
Epoch 64/300, seasonal_0 Loss: 0.0407 | 0.0488
Epoch 65/300, seasonal_0 Loss: 0.0406 | 0.0488
Epoch 66/300, seasonal_0 Loss: 0.0406 | 0.0487
Epoch 67/300, seasonal_0 Loss: 0.0406 | 0.0487
Epoch 68/300, seasonal_0 Loss: 0.0405 | 0.0486
Epoch 69/300, seasonal_0 Loss: 0.0405 | 0.0486
Epoch 70/300, seasonal_0 Loss: 0.0404 | 0.0486
Epoch 71/300, seasonal_0 Loss: 0.0404 | 0.0485
Epoch 72/300, seasonal_0 Loss: 0.0404 | 0.0485
Epoch 73/300, seasonal_0 Loss: 0.0403 | 0.0485
Epoch 74/300, seasonal_0 Loss: 0.0403 | 0.0484
Epoch 75/300, seasonal_0 Loss: 0.0403 | 0.0484
Epoch 76/300, seasonal_0 Loss: 0.0403 | 0.0484
Epoch 77/300, seasonal_0 Loss: 0.0402 | 0.0484
Epoch 78/300, seasonal_0 Loss: 0.0402 | 0.0483
Epoch 79/300, seasonal_0 Loss: 0.0402 | 0.0483
Epoch 80/300, seasonal_0 Loss: 0.0402 | 0.0483
Epoch 81/300, seasonal_0 Loss: 0.0402 | 0.0483
Epoch 82/300, seasonal_0 Loss: 0.0401 | 0.0483
Epoch 83/300, seasonal_0 Loss: 0.0401 | 0.0482
Epoch 84/300, seasonal_0 Loss: 0.0401 | 0.0482
Epoch 85/300, seasonal_0 Loss: 0.0401 | 0.0482
Epoch 86/300, seasonal_0 Loss: 0.0401 | 0.0482
Epoch 87/300, seasonal_0 Loss: 0.0401 | 0.0482
Epoch 88/300, seasonal_0 Loss: 0.0400 | 0.0482
Epoch 89/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 90/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 91/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 92/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 93/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 94/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 95/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 96/300, seasonal_0 Loss: 0.0400 | 0.0481
Epoch 97/300, seasonal_0 Loss: 0.0399 | 0.0481
Epoch 98/300, seasonal_0 Loss: 0.0399 | 0.0481
Epoch 99/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 100/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 101/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 102/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 103/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 104/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 105/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 106/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 107/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 108/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 109/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 110/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 111/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 112/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 113/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 114/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 115/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 116/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 117/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 118/300, seasonal_0 Loss: 0.0399 | 0.0480
Epoch 119/300, seasonal_0 Loss: 0.0398 | 0.0480
Epoch 120/300, seasonal_0 Loss: 0.0398 | 0.0480
Epoch 121/300, seasonal_0 Loss: 0.0398 | 0.0480
Epoch 122/300, seasonal_0 Loss: 0.0398 | 0.0480
Epoch 123/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 124/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 125/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 126/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 127/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 128/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 129/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 130/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 131/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 132/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 133/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 134/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 135/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 136/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 137/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 138/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 139/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 140/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 141/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 142/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 143/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 144/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 145/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 146/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 147/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 148/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 149/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 150/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 151/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 152/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 153/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 154/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 155/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 156/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 157/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 158/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 159/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 160/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 161/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 162/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 163/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 164/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 165/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 166/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 167/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 168/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 169/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 170/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 171/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 172/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 173/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 174/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 175/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 176/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 177/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 178/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 179/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 180/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 181/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 182/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 183/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 184/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 185/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 186/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 187/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 188/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 189/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 190/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 191/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 192/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 193/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 194/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 195/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 196/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 197/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 198/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 199/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 200/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 201/300, seasonal_0 Loss: 0.0398 | 0.0479
Epoch 202/300, seasonal_0 Loss: 0.0398 | 0.0479
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 8, 'train_rates': 0.8501862837247752, 'learning_rate': 4.079393098551156e-05, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7880163628587862}
Epoch 1/300, seasonal_1 Loss: 0.4156 | 0.4496
Epoch 2/300, seasonal_1 Loss: 0.1722 | 0.2197
Epoch 3/300, seasonal_1 Loss: 0.1315 | 0.1845
Epoch 4/300, seasonal_1 Loss: 0.1144 | 0.1616
Epoch 5/300, seasonal_1 Loss: 0.1076 | 0.1454
Epoch 6/300, seasonal_1 Loss: 0.1008 | 0.1288
Epoch 7/300, seasonal_1 Loss: 0.0949 | 0.1139
Epoch 8/300, seasonal_1 Loss: 0.0903 | 0.1030
Epoch 9/300, seasonal_1 Loss: 0.0871 | 0.0944
Epoch 10/300, seasonal_1 Loss: 0.0846 | 0.0887
Epoch 11/300, seasonal_1 Loss: 0.0822 | 0.0843
Epoch 12/300, seasonal_1 Loss: 0.0802 | 0.0814
Epoch 13/300, seasonal_1 Loss: 0.0787 | 0.0798
Epoch 14/300, seasonal_1 Loss: 0.0775 | 0.0793
Epoch 15/300, seasonal_1 Loss: 0.0768 | 0.0790
Epoch 16/300, seasonal_1 Loss: 0.0761 | 0.0788
Epoch 17/300, seasonal_1 Loss: 0.0753 | 0.0787
Epoch 18/300, seasonal_1 Loss: 0.0746 | 0.0785
Epoch 19/300, seasonal_1 Loss: 0.0739 | 0.0784
Epoch 20/300, seasonal_1 Loss: 0.0731 | 0.0782
Epoch 21/300, seasonal_1 Loss: 0.0724 | 0.0782
Epoch 22/300, seasonal_1 Loss: 0.0718 | 0.0773
Epoch 23/300, seasonal_1 Loss: 0.0708 | 0.0769
Epoch 24/300, seasonal_1 Loss: 0.0701 | 0.0765
Epoch 25/300, seasonal_1 Loss: 0.0694 | 0.0761
Epoch 26/300, seasonal_1 Loss: 0.0688 | 0.0757
Epoch 27/300, seasonal_1 Loss: 0.0682 | 0.0749
Epoch 28/300, seasonal_1 Loss: 0.0678 | 0.0743
Epoch 29/300, seasonal_1 Loss: 0.0670 | 0.0738
Epoch 30/300, seasonal_1 Loss: 0.0665 | 0.0734
Epoch 31/300, seasonal_1 Loss: 0.0660 | 0.0731
Epoch 32/300, seasonal_1 Loss: 0.0655 | 0.0727
Epoch 33/300, seasonal_1 Loss: 0.0651 | 0.0723
Epoch 34/300, seasonal_1 Loss: 0.0646 | 0.0713
Epoch 35/300, seasonal_1 Loss: 0.0643 | 0.0709
Epoch 36/300, seasonal_1 Loss: 0.0638 | 0.0706
Epoch 37/300, seasonal_1 Loss: 0.0634 | 0.0703
Epoch 38/300, seasonal_1 Loss: 0.0631 | 0.0700
Epoch 39/300, seasonal_1 Loss: 0.0628 | 0.0697
Epoch 40/300, seasonal_1 Loss: 0.0624 | 0.0690
Epoch 41/300, seasonal_1 Loss: 0.0621 | 0.0688
Epoch 42/300, seasonal_1 Loss: 0.0618 | 0.0684
Epoch 43/300, seasonal_1 Loss: 0.0614 | 0.0682
Epoch 44/300, seasonal_1 Loss: 0.0612 | 0.0680
Epoch 45/300, seasonal_1 Loss: 0.0609 | 0.0677
Epoch 46/300, seasonal_1 Loss: 0.0606 | 0.0675
Epoch 47/300, seasonal_1 Loss: 0.0603 | 0.0669
Epoch 48/300, seasonal_1 Loss: 0.0601 | 0.0667
Epoch 49/300, seasonal_1 Loss: 0.0598 | 0.0665
Epoch 50/300, seasonal_1 Loss: 0.0595 | 0.0663
Epoch 51/300, seasonal_1 Loss: 0.0593 | 0.0661
Epoch 52/300, seasonal_1 Loss: 0.0591 | 0.0660
Epoch 53/300, seasonal_1 Loss: 0.0589 | 0.0655
Epoch 54/300, seasonal_1 Loss: 0.0587 | 0.0653
Epoch 55/300, seasonal_1 Loss: 0.0585 | 0.0652
Epoch 56/300, seasonal_1 Loss: 0.0583 | 0.0651
Epoch 57/300, seasonal_1 Loss: 0.0581 | 0.0649
Epoch 58/300, seasonal_1 Loss: 0.0580 | 0.0648
Epoch 59/300, seasonal_1 Loss: 0.0578 | 0.0647
Epoch 60/300, seasonal_1 Loss: 0.0576 | 0.0643
Epoch 61/300, seasonal_1 Loss: 0.0575 | 0.0642
Epoch 62/300, seasonal_1 Loss: 0.0573 | 0.0641
Epoch 63/300, seasonal_1 Loss: 0.0572 | 0.0640
Epoch 64/300, seasonal_1 Loss: 0.0570 | 0.0639
Epoch 65/300, seasonal_1 Loss: 0.0569 | 0.0638
Epoch 66/300, seasonal_1 Loss: 0.0568 | 0.0635
Epoch 67/300, seasonal_1 Loss: 0.0567 | 0.0634
Epoch 68/300, seasonal_1 Loss: 0.0565 | 0.0633
Epoch 69/300, seasonal_1 Loss: 0.0564 | 0.0633
Epoch 70/300, seasonal_1 Loss: 0.0563 | 0.0632
Epoch 71/300, seasonal_1 Loss: 0.0562 | 0.0631
Epoch 72/300, seasonal_1 Loss: 0.0561 | 0.0630
Epoch 73/300, seasonal_1 Loss: 0.0560 | 0.0628
Epoch 74/300, seasonal_1 Loss: 0.0559 | 0.0627
Epoch 75/300, seasonal_1 Loss: 0.0558 | 0.0626
Epoch 76/300, seasonal_1 Loss: 0.0557 | 0.0626
Epoch 77/300, seasonal_1 Loss: 0.0556 | 0.0625
Epoch 78/300, seasonal_1 Loss: 0.0555 | 0.0624
Epoch 79/300, seasonal_1 Loss: 0.0554 | 0.0623
Epoch 80/300, seasonal_1 Loss: 0.0553 | 0.0622
Epoch 81/300, seasonal_1 Loss: 0.0552 | 0.0621
Epoch 82/300, seasonal_1 Loss: 0.0552 | 0.0621
Epoch 83/300, seasonal_1 Loss: 0.0551 | 0.0620
Epoch 84/300, seasonal_1 Loss: 0.0550 | 0.0620
Epoch 85/300, seasonal_1 Loss: 0.0549 | 0.0619
Epoch 86/300, seasonal_1 Loss: 0.0548 | 0.0618
Epoch 87/300, seasonal_1 Loss: 0.0548 | 0.0617
Epoch 88/300, seasonal_1 Loss: 0.0547 | 0.0617
Epoch 89/300, seasonal_1 Loss: 0.0546 | 0.0616
Epoch 90/300, seasonal_1 Loss: 0.0546 | 0.0616
Epoch 91/300, seasonal_1 Loss: 0.0545 | 0.0615
Epoch 92/300, seasonal_1 Loss: 0.0544 | 0.0614
Epoch 93/300, seasonal_1 Loss: 0.0544 | 0.0614
Epoch 94/300, seasonal_1 Loss: 0.0543 | 0.0614
Epoch 95/300, seasonal_1 Loss: 0.0543 | 0.0613
Epoch 96/300, seasonal_1 Loss: 0.0542 | 0.0613
Epoch 97/300, seasonal_1 Loss: 0.0542 | 0.0612
Epoch 98/300, seasonal_1 Loss: 0.0541 | 0.0612
Epoch 99/300, seasonal_1 Loss: 0.0541 | 0.0611
Epoch 100/300, seasonal_1 Loss: 0.0540 | 0.0611
Epoch 101/300, seasonal_1 Loss: 0.0540 | 0.0611
Epoch 102/300, seasonal_1 Loss: 0.0540 | 0.0610
Epoch 103/300, seasonal_1 Loss: 0.0539 | 0.0610
Epoch 104/300, seasonal_1 Loss: 0.0539 | 0.0610
Epoch 105/300, seasonal_1 Loss: 0.0538 | 0.0609
Epoch 106/300, seasonal_1 Loss: 0.0538 | 0.0609
Epoch 107/300, seasonal_1 Loss: 0.0538 | 0.0608
Epoch 108/300, seasonal_1 Loss: 0.0537 | 0.0608
Epoch 109/300, seasonal_1 Loss: 0.0537 | 0.0608
Epoch 110/300, seasonal_1 Loss: 0.0537 | 0.0608
Epoch 111/300, seasonal_1 Loss: 0.0537 | 0.0607
Epoch 112/300, seasonal_1 Loss: 0.0536 | 0.0607
Epoch 113/300, seasonal_1 Loss: 0.0536 | 0.0607
Epoch 114/300, seasonal_1 Loss: 0.0536 | 0.0607
Epoch 115/300, seasonal_1 Loss: 0.0535 | 0.0606
Epoch 116/300, seasonal_1 Loss: 0.0535 | 0.0606
Epoch 117/300, seasonal_1 Loss: 0.0535 | 0.0606
Epoch 118/300, seasonal_1 Loss: 0.0535 | 0.0606
Epoch 119/300, seasonal_1 Loss: 0.0534 | 0.0605
Epoch 120/300, seasonal_1 Loss: 0.0534 | 0.0605
Epoch 121/300, seasonal_1 Loss: 0.0534 | 0.0605
Epoch 122/300, seasonal_1 Loss: 0.0534 | 0.0605
Epoch 123/300, seasonal_1 Loss: 0.0534 | 0.0605
Epoch 124/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 125/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 126/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 127/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 128/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 129/300, seasonal_1 Loss: 0.0533 | 0.0604
Epoch 130/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 131/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 132/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 133/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 134/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 135/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 136/300, seasonal_1 Loss: 0.0532 | 0.0603
Epoch 137/300, seasonal_1 Loss: 0.0531 | 0.0603
Epoch 138/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 139/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 140/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 141/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 142/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 143/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 144/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 145/300, seasonal_1 Loss: 0.0531 | 0.0602
Epoch 146/300, seasonal_1 Loss: 0.0530 | 0.0602
Epoch 147/300, seasonal_1 Loss: 0.0530 | 0.0602
Epoch 148/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 149/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 150/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 151/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 152/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 153/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 154/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 155/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 156/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 157/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 158/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 159/300, seasonal_1 Loss: 0.0530 | 0.0601
Epoch 160/300, seasonal_1 Loss: 0.0529 | 0.0601
Epoch 161/300, seasonal_1 Loss: 0.0529 | 0.0601
Epoch 162/300, seasonal_1 Loss: 0.0529 | 0.0601
Epoch 163/300, seasonal_1 Loss: 0.0529 | 0.0601
Epoch 164/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 165/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 166/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 167/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 168/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 169/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 170/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 171/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 172/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 173/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 174/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 175/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 176/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 177/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 178/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 179/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 180/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 181/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 182/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 183/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 184/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 185/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 186/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 187/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 188/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 189/300, seasonal_1 Loss: 0.0529 | 0.0600
Epoch 190/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 191/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 192/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 193/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 194/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 195/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 196/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 197/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 198/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 199/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 200/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 201/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 202/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 203/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 204/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 205/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 206/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 207/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 208/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 209/300, seasonal_1 Loss: 0.0528 | 0.0600
Epoch 210/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 211/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 212/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 213/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 214/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 215/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 216/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 217/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 218/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 219/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 220/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 221/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 222/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 223/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 224/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 225/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 226/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 227/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 228/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 229/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 230/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 231/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 232/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 233/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 234/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 235/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 236/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 237/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 238/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 239/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 240/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 241/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 242/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 243/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 244/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 245/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 246/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 247/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 248/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 249/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 250/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 251/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 252/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 253/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 254/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 255/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 256/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 257/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 258/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 259/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 260/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 261/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 262/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 263/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 264/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 265/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 266/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 267/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 268/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 269/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 270/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 271/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 272/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 273/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 274/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 275/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 276/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 277/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 278/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 279/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 280/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 281/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 282/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 283/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 284/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 285/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 286/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 287/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 288/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 289/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 290/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 291/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 292/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 293/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 294/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 295/300, seasonal_1 Loss: 0.0528 | 0.0599
Epoch 296/300, seasonal_1 Loss: 0.0528 | 0.0599
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 18, 'train_rates': 0.9342397272162837, 'learning_rate': 0.0001857005882815361, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8272978339758787}
Epoch 1/300, seasonal_2 Loss: 0.3468 | 0.1616
Epoch 2/300, seasonal_2 Loss: 0.1359 | 0.1288
Epoch 3/300, seasonal_2 Loss: 0.1089 | 0.1215
Epoch 4/300, seasonal_2 Loss: 0.0913 | 0.0830
Epoch 5/300, seasonal_2 Loss: 0.0812 | 0.0721
Epoch 6/300, seasonal_2 Loss: 0.0754 | 0.0730
Epoch 7/300, seasonal_2 Loss: 0.0703 | 0.0689
Epoch 8/300, seasonal_2 Loss: 0.0679 | 0.0680
Epoch 9/300, seasonal_2 Loss: 0.0646 | 0.0669
Epoch 10/300, seasonal_2 Loss: 0.0622 | 0.0659
Epoch 11/300, seasonal_2 Loss: 0.0600 | 0.0652
Epoch 12/300, seasonal_2 Loss: 0.0581 | 0.0645
Epoch 13/300, seasonal_2 Loss: 0.0568 | 0.0619
Epoch 14/300, seasonal_2 Loss: 0.0555 | 0.0617
Epoch 15/300, seasonal_2 Loss: 0.0536 | 0.0599
Epoch 16/300, seasonal_2 Loss: 0.0520 | 0.0583
Epoch 17/300, seasonal_2 Loss: 0.0508 | 0.0572
Epoch 18/300, seasonal_2 Loss: 0.0497 | 0.0566
Epoch 19/300, seasonal_2 Loss: 0.0476 | 0.0564
Epoch 20/300, seasonal_2 Loss: 0.0468 | 0.0510
Epoch 21/300, seasonal_2 Loss: 0.0456 | 0.0517
Epoch 22/300, seasonal_2 Loss: 0.0446 | 0.0502
Epoch 23/300, seasonal_2 Loss: 0.0439 | 0.0500
Epoch 24/300, seasonal_2 Loss: 0.0432 | 0.0498
Epoch 25/300, seasonal_2 Loss: 0.0426 | 0.0487
Epoch 26/300, seasonal_2 Loss: 0.0424 | 0.0485
Epoch 27/300, seasonal_2 Loss: 0.0415 | 0.0486
Epoch 28/300, seasonal_2 Loss: 0.0407 | 0.0482
Epoch 29/300, seasonal_2 Loss: 0.0400 | 0.0477
Epoch 30/300, seasonal_2 Loss: 0.0394 | 0.0469
Epoch 31/300, seasonal_2 Loss: 0.0388 | 0.0433
Epoch 32/300, seasonal_2 Loss: 0.0388 | 0.0444
Epoch 33/300, seasonal_2 Loss: 0.0377 | 0.0423
Epoch 34/300, seasonal_2 Loss: 0.0406 | 0.0507
Epoch 35/300, seasonal_2 Loss: 0.0380 | 0.0436
Epoch 36/300, seasonal_2 Loss: 0.0373 | 0.0408
Epoch 37/300, seasonal_2 Loss: 0.0358 | 0.0407
Epoch 38/300, seasonal_2 Loss: 0.0354 | 0.0406
Epoch 39/300, seasonal_2 Loss: 0.0347 | 0.0406
Epoch 40/300, seasonal_2 Loss: 0.0343 | 0.0405
Epoch 41/300, seasonal_2 Loss: 0.0339 | 0.0403
Epoch 42/300, seasonal_2 Loss: 0.0334 | 0.0400
Epoch 43/300, seasonal_2 Loss: 0.0332 | 0.0396
Epoch 44/300, seasonal_2 Loss: 0.0329 | 0.0401
Epoch 45/300, seasonal_2 Loss: 0.0326 | 0.0398
Epoch 46/300, seasonal_2 Loss: 0.0323 | 0.0398
Epoch 47/300, seasonal_2 Loss: 0.0320 | 0.0395
Epoch 48/300, seasonal_2 Loss: 0.0318 | 0.0392
Epoch 49/300, seasonal_2 Loss: 0.0316 | 0.0383
Epoch 50/300, seasonal_2 Loss: 0.0315 | 0.0384
Epoch 51/300, seasonal_2 Loss: 0.0313 | 0.0383
Epoch 52/300, seasonal_2 Loss: 0.0311 | 0.0382
Epoch 53/300, seasonal_2 Loss: 0.0309 | 0.0381
Epoch 54/300, seasonal_2 Loss: 0.0306 | 0.0379
Epoch 55/300, seasonal_2 Loss: 0.0304 | 0.0372
Epoch 56/300, seasonal_2 Loss: 0.0303 | 0.0373
Epoch 57/300, seasonal_2 Loss: 0.0301 | 0.0374
Epoch 58/300, seasonal_2 Loss: 0.0299 | 0.0374
Epoch 59/300, seasonal_2 Loss: 0.0297 | 0.0373
Epoch 60/300, seasonal_2 Loss: 0.0296 | 0.0372
Epoch 61/300, seasonal_2 Loss: 0.0295 | 0.0361
Epoch 62/300, seasonal_2 Loss: 0.0294 | 0.0360
Epoch 63/300, seasonal_2 Loss: 0.0293 | 0.0361
Epoch 64/300, seasonal_2 Loss: 0.0293 | 0.0360
Epoch 65/300, seasonal_2 Loss: 0.0292 | 0.0359
Epoch 66/300, seasonal_2 Loss: 0.0291 | 0.0358
Epoch 67/300, seasonal_2 Loss: 0.0291 | 0.0361
Epoch 68/300, seasonal_2 Loss: 0.0292 | 0.0365
Epoch 69/300, seasonal_2 Loss: 0.0292 | 0.0370
Epoch 70/300, seasonal_2 Loss: 0.0291 | 0.0378
Epoch 71/300, seasonal_2 Loss: 0.0290 | 0.0386
Epoch 72/300, seasonal_2 Loss: 0.0289 | 0.0397
Epoch 73/300, seasonal_2 Loss: 0.0289 | 0.0447
Epoch 74/300, seasonal_2 Loss: 0.0288 | 0.0457
Epoch 75/300, seasonal_2 Loss: 0.0285 | 0.0459
Epoch 76/300, seasonal_2 Loss: 0.0283 | 0.0457
Epoch 77/300, seasonal_2 Loss: 0.0282 | 0.0447
Epoch 78/300, seasonal_2 Loss: 0.0281 | 0.0439
Epoch 79/300, seasonal_2 Loss: 0.0283 | 0.0399
Epoch 80/300, seasonal_2 Loss: 0.0285 | 0.0389
Epoch 81/300, seasonal_2 Loss: 0.0287 | 0.0368
Epoch 82/300, seasonal_2 Loss: 0.0288 | 0.0360
Epoch 83/300, seasonal_2 Loss: 0.0289 | 0.0355
Epoch 84/300, seasonal_2 Loss: 0.0288 | 0.0354
Epoch 85/300, seasonal_2 Loss: 0.0289 | 0.0370
Epoch 86/300, seasonal_2 Loss: 0.0287 | 0.0362
Epoch 87/300, seasonal_2 Loss: 0.0282 | 0.0356
Epoch 88/300, seasonal_2 Loss: 0.0278 | 0.0350
Epoch 89/300, seasonal_2 Loss: 0.0275 | 0.0346
Epoch 90/300, seasonal_2 Loss: 0.0272 | 0.0344
Epoch 91/300, seasonal_2 Loss: 0.0272 | 0.0344
Epoch 92/300, seasonal_2 Loss: 0.0271 | 0.0342
Epoch 93/300, seasonal_2 Loss: 0.0270 | 0.0341
Epoch 94/300, seasonal_2 Loss: 0.0268 | 0.0340
Epoch 95/300, seasonal_2 Loss: 0.0267 | 0.0340
Epoch 96/300, seasonal_2 Loss: 0.0267 | 0.0339
Epoch 97/300, seasonal_2 Loss: 0.0267 | 0.0339
Epoch 98/300, seasonal_2 Loss: 0.0267 | 0.0338
Epoch 99/300, seasonal_2 Loss: 0.0266 | 0.0338
Epoch 100/300, seasonal_2 Loss: 0.0266 | 0.0337
Epoch 101/300, seasonal_2 Loss: 0.0265 | 0.0337
Epoch 102/300, seasonal_2 Loss: 0.0265 | 0.0337
Epoch 103/300, seasonal_2 Loss: 0.0265 | 0.0337
Epoch 104/300, seasonal_2 Loss: 0.0264 | 0.0336
Epoch 105/300, seasonal_2 Loss: 0.0263 | 0.0336
Epoch 106/300, seasonal_2 Loss: 0.0262 | 0.0336
Epoch 107/300, seasonal_2 Loss: 0.0262 | 0.0336
Epoch 108/300, seasonal_2 Loss: 0.0261 | 0.0336
Epoch 109/300, seasonal_2 Loss: 0.0260 | 0.0336
Epoch 110/300, seasonal_2 Loss: 0.0259 | 0.0336
Epoch 111/300, seasonal_2 Loss: 0.0258 | 0.0336
Epoch 112/300, seasonal_2 Loss: 0.0257 | 0.0336
Epoch 113/300, seasonal_2 Loss: 0.0257 | 0.0336
Epoch 114/300, seasonal_2 Loss: 0.0256 | 0.0336
Epoch 115/300, seasonal_2 Loss: 0.0255 | 0.0336
Epoch 116/300, seasonal_2 Loss: 0.0255 | 0.0336
Epoch 117/300, seasonal_2 Loss: 0.0254 | 0.0336
Epoch 118/300, seasonal_2 Loss: 0.0254 | 0.0336
Epoch 119/300, seasonal_2 Loss: 0.0253 | 0.0336
Epoch 120/300, seasonal_2 Loss: 0.0253 | 0.0336
Epoch 121/300, seasonal_2 Loss: 0.0253 | 0.0335
Epoch 122/300, seasonal_2 Loss: 0.0252 | 0.0336
Epoch 123/300, seasonal_2 Loss: 0.0252 | 0.0336
Epoch 124/300, seasonal_2 Loss: 0.0251 | 0.0336
Epoch 125/300, seasonal_2 Loss: 0.0251 | 0.0336
Epoch 126/300, seasonal_2 Loss: 0.0251 | 0.0335
Epoch 127/300, seasonal_2 Loss: 0.0251 | 0.0335
Epoch 128/300, seasonal_2 Loss: 0.0250 | 0.0335
Epoch 129/300, seasonal_2 Loss: 0.0250 | 0.0335
Epoch 130/300, seasonal_2 Loss: 0.0250 | 0.0335
Epoch 131/300, seasonal_2 Loss: 0.0250 | 0.0335
Epoch 132/300, seasonal_2 Loss: 0.0249 | 0.0335
Epoch 133/300, seasonal_2 Loss: 0.0249 | 0.0335
Epoch 134/300, seasonal_2 Loss: 0.0249 | 0.0335
Epoch 135/300, seasonal_2 Loss: 0.0249 | 0.0335
Epoch 136/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 137/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 138/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 139/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 140/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 141/300, seasonal_2 Loss: 0.0248 | 0.0335
Epoch 142/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 143/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 144/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 145/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 146/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 147/300, seasonal_2 Loss: 0.0247 | 0.0335
Epoch 148/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 149/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 150/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 151/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 152/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 153/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 154/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 155/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 156/300, seasonal_2 Loss: 0.0246 | 0.0335
Epoch 157/300, seasonal_2 Loss: 0.0245 | 0.0335
Epoch 158/300, seasonal_2 Loss: 0.0245 | 0.0335
Epoch 159/300, seasonal_2 Loss: 0.0245 | 0.0335
Epoch 160/300, seasonal_2 Loss: 0.0245 | 0.0335
Epoch 161/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 162/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 163/300, seasonal_2 Loss: 0.0245 | 0.0335
Epoch 164/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 165/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 166/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 167/300, seasonal_2 Loss: 0.0245 | 0.0334
Epoch 168/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 169/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 170/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 171/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 172/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 173/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 174/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 175/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 176/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 177/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 178/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 179/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 180/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 181/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 182/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 183/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 184/300, seasonal_2 Loss: 0.0244 | 0.0334
Epoch 185/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 186/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 187/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 188/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 189/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 190/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 191/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 192/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 193/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 194/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 195/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 196/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 197/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 198/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 199/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 200/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 201/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 202/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 203/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 204/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 205/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 206/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 207/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 208/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 209/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 210/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 211/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 212/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 213/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 214/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 215/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 216/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 217/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 218/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 219/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 220/300, seasonal_2 Loss: 0.0243 | 0.0334
Epoch 221/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 222/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 223/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 224/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 225/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 226/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 227/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 228/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 229/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 230/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 231/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 232/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 233/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 234/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 235/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 236/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 237/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 238/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 239/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 240/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 241/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 242/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 243/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 244/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 245/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 246/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 247/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 248/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 249/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 250/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 251/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 252/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 253/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 254/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 255/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 256/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 257/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 258/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 259/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 260/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 261/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 262/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 263/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 264/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 265/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 266/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 267/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 268/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 269/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 270/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 271/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 272/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 273/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 274/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 275/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 276/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 277/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 278/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 279/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 280/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 281/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 282/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 283/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 284/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 285/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 286/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 287/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 288/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 289/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 290/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 291/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 292/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 293/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 294/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 295/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 296/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 297/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 298/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 299/300, seasonal_2 Loss: 0.0242 | 0.0334
Epoch 300/300, seasonal_2 Loss: 0.0242 | 0.0334
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.9739588826782053, 'learning_rate': 0.0007074389761846369, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8889750367516519}
Epoch 1/300, seasonal_3 Loss: 0.1535 | 0.0718
Epoch 2/300, seasonal_3 Loss: 0.0762 | 0.0626
Epoch 3/300, seasonal_3 Loss: 0.0663 | 0.0590
Epoch 4/300, seasonal_3 Loss: 0.0579 | 0.0536
Epoch 5/300, seasonal_3 Loss: 0.0514 | 0.0494
Epoch 6/300, seasonal_3 Loss: 0.0479 | 0.0484
Epoch 7/300, seasonal_3 Loss: 0.0461 | 0.0464
Epoch 8/300, seasonal_3 Loss: 0.0437 | 0.0453
Epoch 9/300, seasonal_3 Loss: 0.0418 | 0.0448
Epoch 10/300, seasonal_3 Loss: 0.0407 | 0.0447
Epoch 11/300, seasonal_3 Loss: 0.0399 | 0.0446
Epoch 12/300, seasonal_3 Loss: 0.0393 | 0.0442
Epoch 13/300, seasonal_3 Loss: 0.0388 | 0.0436
Epoch 14/300, seasonal_3 Loss: 0.0384 | 0.0432
Epoch 15/300, seasonal_3 Loss: 0.0381 | 0.0429
Epoch 16/300, seasonal_3 Loss: 0.0378 | 0.0427
Epoch 17/300, seasonal_3 Loss: 0.0375 | 0.0425
Epoch 18/300, seasonal_3 Loss: 0.0374 | 0.0424
Epoch 19/300, seasonal_3 Loss: 0.0372 | 0.0423
Epoch 20/300, seasonal_3 Loss: 0.0371 | 0.0422
Epoch 21/300, seasonal_3 Loss: 0.0370 | 0.0422
Epoch 22/300, seasonal_3 Loss: 0.0369 | 0.0422
Epoch 23/300, seasonal_3 Loss: 0.0368 | 0.0422
Epoch 24/300, seasonal_3 Loss: 0.0367 | 0.0421
Epoch 25/300, seasonal_3 Loss: 0.0367 | 0.0421
Epoch 26/300, seasonal_3 Loss: 0.0366 | 0.0421
Epoch 27/300, seasonal_3 Loss: 0.0366 | 0.0420
Epoch 28/300, seasonal_3 Loss: 0.0365 | 0.0420
Epoch 29/300, seasonal_3 Loss: 0.0365 | 0.0420
Epoch 30/300, seasonal_3 Loss: 0.0365 | 0.0420
Epoch 31/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 32/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 33/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 34/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 35/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 36/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 37/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 38/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 39/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 40/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 41/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 42/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 43/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 44/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 45/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 46/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 47/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 48/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 49/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 50/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 51/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 52/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 53/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 54/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 55/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 56/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 57/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 58/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 59/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 60/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 61/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 62/300, seasonal_3 Loss: 0.0364 | 0.0420
Epoch 63/300, seasonal_3 Loss: 0.0364 | 0.0420
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 16, 'train_rates': 0.9692252531679102, 'learning_rate': 0.0006329125308394092, 'batch_size': 145, 'step_size': 9, 'gamma': 0.7899596170977622}
Epoch 1/300, resid Loss: 0.5494 | 0.2909
Epoch 2/300, resid Loss: 0.2018 | 0.1969
Epoch 3/300, resid Loss: 0.1664 | 0.1755
Epoch 4/300, resid Loss: 0.1624 | 0.1454
Epoch 5/300, resid Loss: 0.1475 | 0.1336
Epoch 6/300, resid Loss: 0.1425 | 0.1843
Epoch 7/300, resid Loss: 0.1568 | 0.2571
Epoch 8/300, resid Loss: 0.1901 | 0.6392
Epoch 9/300, resid Loss: 0.1566 | 0.1597
Epoch 10/300, resid Loss: 0.0982 | 0.1239
Epoch 11/300, resid Loss: 0.0857 | 0.0979
Epoch 12/300, resid Loss: 0.0748 | 0.0773
Epoch 13/300, resid Loss: 0.0721 | 0.0701
Epoch 14/300, resid Loss: 0.0685 | 0.0669
Epoch 15/300, resid Loss: 0.0657 | 0.0638
Epoch 16/300, resid Loss: 0.0645 | 0.0626
Epoch 17/300, resid Loss: 0.0627 | 0.0613
Epoch 18/300, resid Loss: 0.0618 | 0.0602
Epoch 19/300, resid Loss: 0.0616 | 0.0603
Epoch 20/300, resid Loss: 0.0603 | 0.0585
Epoch 21/300, resid Loss: 0.0568 | 0.0574
Epoch 22/300, resid Loss: 0.0551 | 0.0568
Epoch 23/300, resid Loss: 0.0552 | 0.0559
Epoch 24/300, resid Loss: 0.0562 | 0.0552
Epoch 25/300, resid Loss: 0.0556 | 0.0545
Epoch 26/300, resid Loss: 0.0525 | 0.0558
Epoch 27/300, resid Loss: 0.0513 | 0.0617
Epoch 28/300, resid Loss: 0.0516 | 0.0739
Epoch 29/300, resid Loss: 0.0515 | 0.0571
Epoch 30/300, resid Loss: 0.0484 | 0.0503
Epoch 31/300, resid Loss: 0.0472 | 0.0497
Epoch 32/300, resid Loss: 0.0463 | 0.0485
Epoch 33/300, resid Loss: 0.0454 | 0.0514
Epoch 34/300, resid Loss: 0.0451 | 0.0533
Epoch 35/300, resid Loss: 0.0448 | 0.0491
Epoch 36/300, resid Loss: 0.0439 | 0.0466
Epoch 37/300, resid Loss: 0.0437 | 0.0466
Epoch 38/300, resid Loss: 0.0438 | 0.0482
Epoch 39/300, resid Loss: 0.0436 | 0.0483
Epoch 40/300, resid Loss: 0.0432 | 0.0464
Epoch 41/300, resid Loss: 0.0426 | 0.0457
Epoch 42/300, resid Loss: 0.0422 | 0.0465
Epoch 43/300, resid Loss: 0.0417 | 0.0467
Epoch 44/300, resid Loss: 0.0413 | 0.0461
Epoch 45/300, resid Loss: 0.0409 | 0.0458
Epoch 46/300, resid Loss: 0.0406 | 0.0458
Epoch 47/300, resid Loss: 0.0404 | 0.0455
Epoch 48/300, resid Loss: 0.0402 | 0.0453
Epoch 49/300, resid Loss: 0.0400 | 0.0453
Epoch 50/300, resid Loss: 0.0399 | 0.0451
Epoch 51/300, resid Loss: 0.0397 | 0.0449
Epoch 52/300, resid Loss: 0.0395 | 0.0448
Epoch 53/300, resid Loss: 0.0394 | 0.0448
Epoch 54/300, resid Loss: 0.0393 | 0.0447
Epoch 55/300, resid Loss: 0.0391 | 0.0444
Epoch 56/300, resid Loss: 0.0390 | 0.0444
Epoch 57/300, resid Loss: 0.0389 | 0.0443
Epoch 58/300, resid Loss: 0.0388 | 0.0442
Epoch 59/300, resid Loss: 0.0387 | 0.0441
Epoch 60/300, resid Loss: 0.0386 | 0.0439
Epoch 61/300, resid Loss: 0.0385 | 0.0438
Epoch 62/300, resid Loss: 0.0384 | 0.0438
Epoch 63/300, resid Loss: 0.0383 | 0.0437
Epoch 64/300, resid Loss: 0.0382 | 0.0436
Epoch 65/300, resid Loss: 0.0382 | 0.0435
Epoch 66/300, resid Loss: 0.0381 | 0.0435
Epoch 67/300, resid Loss: 0.0380 | 0.0434
Epoch 68/300, resid Loss: 0.0380 | 0.0434
Epoch 69/300, resid Loss: 0.0379 | 0.0433
Epoch 70/300, resid Loss: 0.0379 | 0.0432
Epoch 71/300, resid Loss: 0.0378 | 0.0432
Epoch 72/300, resid Loss: 0.0378 | 0.0431
Epoch 73/300, resid Loss: 0.0377 | 0.0431
Epoch 74/300, resid Loss: 0.0377 | 0.0430
Epoch 75/300, resid Loss: 0.0376 | 0.0430
Epoch 76/300, resid Loss: 0.0376 | 0.0429
Epoch 77/300, resid Loss: 0.0375 | 0.0429
Epoch 78/300, resid Loss: 0.0375 | 0.0428
Epoch 79/300, resid Loss: 0.0375 | 0.0428
Epoch 80/300, resid Loss: 0.0374 | 0.0428
Epoch 81/300, resid Loss: 0.0374 | 0.0428
Epoch 82/300, resid Loss: 0.0374 | 0.0427
Epoch 83/300, resid Loss: 0.0373 | 0.0427
Epoch 84/300, resid Loss: 0.0373 | 0.0427
Epoch 85/300, resid Loss: 0.0373 | 0.0426
Epoch 86/300, resid Loss: 0.0373 | 0.0426
Epoch 87/300, resid Loss: 0.0372 | 0.0426
Epoch 88/300, resid Loss: 0.0372 | 0.0426
Epoch 89/300, resid Loss: 0.0372 | 0.0425
Epoch 90/300, resid Loss: 0.0372 | 0.0425
Epoch 91/300, resid Loss: 0.0371 | 0.0425
Epoch 92/300, resid Loss: 0.0371 | 0.0425
Epoch 93/300, resid Loss: 0.0371 | 0.0425
Epoch 94/300, resid Loss: 0.0371 | 0.0424
Epoch 95/300, resid Loss: 0.0371 | 0.0424
Epoch 96/300, resid Loss: 0.0371 | 0.0424
Epoch 97/300, resid Loss: 0.0370 | 0.0424
Epoch 98/300, resid Loss: 0.0370 | 0.0424
Epoch 99/300, resid Loss: 0.0370 | 0.0424
Epoch 100/300, resid Loss: 0.0370 | 0.0424
Epoch 101/300, resid Loss: 0.0370 | 0.0423
Epoch 102/300, resid Loss: 0.0370 | 0.0423
Epoch 103/300, resid Loss: 0.0370 | 0.0423
Epoch 104/300, resid Loss: 0.0370 | 0.0423
Epoch 105/300, resid Loss: 0.0369 | 0.0423
Epoch 106/300, resid Loss: 0.0369 | 0.0423
Epoch 107/300, resid Loss: 0.0369 | 0.0423
Epoch 108/300, resid Loss: 0.0369 | 0.0423
Epoch 109/300, resid Loss: 0.0369 | 0.0423
Epoch 110/300, resid Loss: 0.0369 | 0.0422
Epoch 111/300, resid Loss: 0.0369 | 0.0422
Epoch 112/300, resid Loss: 0.0369 | 0.0422
Epoch 113/300, resid Loss: 0.0369 | 0.0422
Epoch 114/300, resid Loss: 0.0369 | 0.0422
Epoch 115/300, resid Loss: 0.0369 | 0.0422
Epoch 116/300, resid Loss: 0.0369 | 0.0422
Epoch 117/300, resid Loss: 0.0368 | 0.0422
Epoch 118/300, resid Loss: 0.0368 | 0.0422
Epoch 119/300, resid Loss: 0.0368 | 0.0422
Epoch 120/300, resid Loss: 0.0368 | 0.0422
Epoch 121/300, resid Loss: 0.0368 | 0.0422
Epoch 122/300, resid Loss: 0.0368 | 0.0422
Epoch 123/300, resid Loss: 0.0368 | 0.0422
Epoch 124/300, resid Loss: 0.0368 | 0.0422
Epoch 125/300, resid Loss: 0.0368 | 0.0422
Epoch 126/300, resid Loss: 0.0368 | 0.0422
Epoch 127/300, resid Loss: 0.0368 | 0.0422
Epoch 128/300, resid Loss: 0.0368 | 0.0421
Epoch 129/300, resid Loss: 0.0368 | 0.0421
Epoch 130/300, resid Loss: 0.0368 | 0.0421
Epoch 131/300, resid Loss: 0.0368 | 0.0421
Epoch 132/300, resid Loss: 0.0368 | 0.0421
Epoch 133/300, resid Loss: 0.0368 | 0.0421
Epoch 134/300, resid Loss: 0.0368 | 0.0421
Epoch 135/300, resid Loss: 0.0368 | 0.0421
Epoch 136/300, resid Loss: 0.0368 | 0.0421
Epoch 137/300, resid Loss: 0.0368 | 0.0421
Epoch 138/300, resid Loss: 0.0368 | 0.0421
Epoch 139/300, resid Loss: 0.0368 | 0.0421
Epoch 140/300, resid Loss: 0.0368 | 0.0421
Epoch 141/300, resid Loss: 0.0368 | 0.0421
Epoch 142/300, resid Loss: 0.0368 | 0.0421
Epoch 143/300, resid Loss: 0.0368 | 0.0421
Epoch 144/300, resid Loss: 0.0368 | 0.0421
Epoch 145/300, resid Loss: 0.0368 | 0.0421
Epoch 146/300, resid Loss: 0.0368 | 0.0421
Epoch 147/300, resid Loss: 0.0368 | 0.0421
Epoch 148/300, resid Loss: 0.0368 | 0.0421
Epoch 149/300, resid Loss: 0.0368 | 0.0421
Epoch 150/300, resid Loss: 0.0368 | 0.0421
Epoch 151/300, resid Loss: 0.0368 | 0.0421
Epoch 152/300, resid Loss: 0.0368 | 0.0421
Epoch 153/300, resid Loss: 0.0367 | 0.0421
Epoch 154/300, resid Loss: 0.0367 | 0.0421
Epoch 155/300, resid Loss: 0.0367 | 0.0421
Epoch 156/300, resid Loss: 0.0367 | 0.0421
Epoch 157/300, resid Loss: 0.0367 | 0.0421
Epoch 158/300, resid Loss: 0.0367 | 0.0421
Epoch 159/300, resid Loss: 0.0367 | 0.0421
Epoch 160/300, resid Loss: 0.0367 | 0.0421
Epoch 161/300, resid Loss: 0.0367 | 0.0421
Epoch 162/300, resid Loss: 0.0367 | 0.0421
Epoch 163/300, resid Loss: 0.0367 | 0.0421
Epoch 164/300, resid Loss: 0.0367 | 0.0421
Epoch 165/300, resid Loss: 0.0367 | 0.0421
Epoch 166/300, resid Loss: 0.0367 | 0.0421
Epoch 167/300, resid Loss: 0.0367 | 0.0421
Epoch 168/300, resid Loss: 0.0367 | 0.0421
Epoch 169/300, resid Loss: 0.0367 | 0.0421
Epoch 170/300, resid Loss: 0.0367 | 0.0421
Epoch 171/300, resid Loss: 0.0367 | 0.0421
Epoch 172/300, resid Loss: 0.0367 | 0.0421
Epoch 173/300, resid Loss: 0.0367 | 0.0421
Epoch 174/300, resid Loss: 0.0367 | 0.0421
Epoch 175/300, resid Loss: 0.0367 | 0.0421
Epoch 176/300, resid Loss: 0.0367 | 0.0421
Epoch 177/300, resid Loss: 0.0367 | 0.0421
Epoch 178/300, resid Loss: 0.0367 | 0.0421
Epoch 179/300, resid Loss: 0.0367 | 0.0421
Epoch 180/300, resid Loss: 0.0367 | 0.0421
Epoch 181/300, resid Loss: 0.0367 | 0.0421
Epoch 182/300, resid Loss: 0.0367 | 0.0421
Epoch 183/300, resid Loss: 0.0367 | 0.0421
Epoch 184/300, resid Loss: 0.0367 | 0.0421
Epoch 185/300, resid Loss: 0.0367 | 0.0421
Epoch 186/300, resid Loss: 0.0367 | 0.0421
Epoch 187/300, resid Loss: 0.0367 | 0.0421
Epoch 188/300, resid Loss: 0.0367 | 0.0421
Epoch 189/300, resid Loss: 0.0367 | 0.0421
Epoch 190/300, resid Loss: 0.0367 | 0.0421
Epoch 191/300, resid Loss: 0.0367 | 0.0421
Epoch 192/300, resid Loss: 0.0367 | 0.0421
Epoch 193/300, resid Loss: 0.0367 | 0.0421
Epoch 194/300, resid Loss: 0.0367 | 0.0421
Epoch 195/300, resid Loss: 0.0367 | 0.0421
Epoch 196/300, resid Loss: 0.0367 | 0.0421
Epoch 197/300, resid Loss: 0.0367 | 0.0421
Epoch 198/300, resid Loss: 0.0367 | 0.0421
Epoch 199/300, resid Loss: 0.0367 | 0.0421
Epoch 200/300, resid Loss: 0.0367 | 0.0421
Epoch 201/300, resid Loss: 0.0367 | 0.0421
Epoch 202/300, resid Loss: 0.0367 | 0.0421
Epoch 203/300, resid Loss: 0.0367 | 0.0421
Epoch 204/300, resid Loss: 0.0367 | 0.0421
Epoch 205/300, resid Loss: 0.0367 | 0.0421
Epoch 206/300, resid Loss: 0.0367 | 0.0421
Epoch 207/300, resid Loss: 0.0367 | 0.0421
Epoch 208/300, resid Loss: 0.0367 | 0.0421
Epoch 209/300, resid Loss: 0.0367 | 0.0421
Epoch 210/300, resid Loss: 0.0367 | 0.0421
Epoch 211/300, resid Loss: 0.0367 | 0.0421
Epoch 212/300, resid Loss: 0.0367 | 0.0421
Epoch 213/300, resid Loss: 0.0367 | 0.0421
Epoch 214/300, resid Loss: 0.0367 | 0.0421
Epoch 215/300, resid Loss: 0.0367 | 0.0421
Epoch 216/300, resid Loss: 0.0367 | 0.0421
Epoch 217/300, resid Loss: 0.0367 | 0.0421
Epoch 218/300, resid Loss: 0.0367 | 0.0421
Epoch 219/300, resid Loss: 0.0367 | 0.0421
Epoch 220/300, resid Loss: 0.0367 | 0.0421
Epoch 221/300, resid Loss: 0.0367 | 0.0421
Epoch 222/300, resid Loss: 0.0367 | 0.0421
Epoch 223/300, resid Loss: 0.0367 | 0.0421
Epoch 224/300, resid Loss: 0.0367 | 0.0421
Epoch 225/300, resid Loss: 0.0367 | 0.0421
Epoch 226/300, resid Loss: 0.0367 | 0.0421
Epoch 227/300, resid Loss: 0.0367 | 0.0421
Epoch 228/300, resid Loss: 0.0367 | 0.0421
Epoch 229/300, resid Loss: 0.0367 | 0.0421
Epoch 230/300, resid Loss: 0.0367 | 0.0421
Epoch 231/300, resid Loss: 0.0367 | 0.0421
Epoch 232/300, resid Loss: 0.0367 | 0.0421
Early stopping for resid
Runtime (seconds): 1365.7730803489685
0.0003950632010224091
[109.663635]
[0.67504895]
[1.1876204]
[-0.43407807]
[5.9544396]
[0.7188592]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 16.196399811655283
RMSE: 4.02447509765625
MAE: 4.02447509765625
R-squared: nan
[117.765526]
