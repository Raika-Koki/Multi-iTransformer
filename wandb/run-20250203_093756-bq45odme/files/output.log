[32m[I 2025-02-03 09:38:02,509][0m A new study created in memory with name: no-name-f88ca17a-543a-4505-a226-f87a1d817cc1[0m
[32m[I 2025-02-03 09:38:40,429][0m Trial 0 finished with value: 0.26410044835913077 and parameters: {'observation_period_num': 229, 'train_rates': 0.6653309401218006, 'learning_rate': 4.4709471855192765e-05, 'batch_size': 127, 'step_size': 5, 'gamma': 0.8457252889704601}. Best is trial 0 with value: 0.26410044835913077.[0m
[32m[I 2025-02-03 09:39:05,960][0m Trial 1 finished with value: 0.5128806233406067 and parameters: {'observation_period_num': 95, 'train_rates': 0.9253203526287394, 'learning_rate': 2.7108665229736674e-06, 'batch_size': 246, 'step_size': 10, 'gamma': 0.8387166894177237}. Best is trial 0 with value: 0.26410044835913077.[0m
[32m[I 2025-02-03 09:39:27,326][0m Trial 2 finished with value: 0.4886700717223144 and parameters: {'observation_period_num': 198, 'train_rates': 0.6856169405132411, 'learning_rate': 2.0807541114466813e-06, 'batch_size': 253, 'step_size': 7, 'gamma': 0.8819493595287936}. Best is trial 0 with value: 0.26410044835913077.[0m
Early stopping at epoch 52
[32m[I 2025-02-03 09:39:39,020][0m Trial 3 finished with value: 0.38846358839279965 and parameters: {'observation_period_num': 191, 'train_rates': 0.6690594590608173, 'learning_rate': 6.175970962985315e-05, 'batch_size': 242, 'step_size': 1, 'gamma': 0.7879512484080866}. Best is trial 0 with value: 0.26410044835913077.[0m
[32m[I 2025-02-03 09:40:48,382][0m Trial 4 finished with value: 0.11544628441333771 and parameters: {'observation_period_num': 174, 'train_rates': 0.978172933410621, 'learning_rate': 1.878040878277777e-05, 'batch_size': 86, 'step_size': 4, 'gamma': 0.9216158134126164}. Best is trial 4 with value: 0.11544628441333771.[0m
[32m[I 2025-02-03 09:42:20,643][0m Trial 5 finished with value: 0.28130810231798226 and parameters: {'observation_period_num': 91, 'train_rates': 0.7991001261069725, 'learning_rate': 2.5622030519615183e-06, 'batch_size': 56, 'step_size': 13, 'gamma': 0.7675319543414907}. Best is trial 4 with value: 0.11544628441333771.[0m
[32m[I 2025-02-03 09:46:47,308][0m Trial 6 finished with value: 0.12795559044291333 and parameters: {'observation_period_num': 158, 'train_rates': 0.8829538381993407, 'learning_rate': 2.6836263738679503e-06, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9731147799046218}. Best is trial 4 with value: 0.11544628441333771.[0m
[32m[I 2025-02-03 09:47:29,342][0m Trial 7 finished with value: 0.06502108414560462 and parameters: {'observation_period_num': 213, 'train_rates': 0.884700500457126, 'learning_rate': 0.00021520774131000188, 'batch_size': 138, 'step_size': 4, 'gamma': 0.9203768988866056}. Best is trial 7 with value: 0.06502108414560462.[0m
[32m[I 2025-02-03 09:48:18,281][0m Trial 8 finished with value: 0.19560394078106075 and parameters: {'observation_period_num': 159, 'train_rates': 0.697212840967871, 'learning_rate': 1.474658170248692e-05, 'batch_size': 99, 'step_size': 6, 'gamma': 0.8225437523508111}. Best is trial 7 with value: 0.06502108414560462.[0m
[32m[I 2025-02-03 09:48:45,045][0m Trial 9 finished with value: 0.1443532345275725 and parameters: {'observation_period_num': 194, 'train_rates': 0.7756128860686391, 'learning_rate': 7.737227921818529e-05, 'batch_size': 211, 'step_size': 4, 'gamma': 0.8380983957747835}. Best is trial 7 with value: 0.06502108414560462.[0m
[32m[I 2025-02-03 09:49:18,166][0m Trial 10 finished with value: 0.027749420682445538 and parameters: {'observation_period_num': 9, 'train_rates': 0.8359559316762949, 'learning_rate': 0.000780381906120392, 'batch_size': 178, 'step_size': 1, 'gamma': 0.9882713410411984}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:49:51,102][0m Trial 11 finished with value: 0.033462315910693374 and parameters: {'observation_period_num': 11, 'train_rates': 0.8431751910903168, 'learning_rate': 0.0006608258473847773, 'batch_size': 182, 'step_size': 2, 'gamma': 0.9867598487366604}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:50:24,528][0m Trial 12 finished with value: 0.028649393536827782 and parameters: {'observation_period_num': 9, 'train_rates': 0.7945944954221755, 'learning_rate': 0.0007343667719404651, 'batch_size': 176, 'step_size': 1, 'gamma': 0.9872313016479896}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:50:57,981][0m Trial 13 finished with value: 0.03792082145810127 and parameters: {'observation_period_num': 12, 'train_rates': 0.7650715207963839, 'learning_rate': 0.0009781038075919784, 'batch_size': 173, 'step_size': 1, 'gamma': 0.9473380419203968}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:51:30,107][0m Trial 14 finished with value: 0.04818750483294328 and parameters: {'observation_period_num': 51, 'train_rates': 0.741956304624491, 'learning_rate': 0.0003132640939220327, 'batch_size': 175, 'step_size': 9, 'gamma': 0.9548074565094926}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:51:54,708][0m Trial 15 finished with value: 0.10555494879372418 and parameters: {'observation_period_num': 47, 'train_rates': 0.6037332873120608, 'learning_rate': 0.00020149324697677957, 'batch_size': 204, 'step_size': 2, 'gamma': 0.8960226329400997}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:52:37,170][0m Trial 16 finished with value: 0.05302159736053117 and parameters: {'observation_period_num': 45, 'train_rates': 0.8297387400880802, 'learning_rate': 0.0004171639287710905, 'batch_size': 141, 'step_size': 11, 'gamma': 0.985467910504731}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:53:06,569][0m Trial 17 finished with value: 0.06096739344848472 and parameters: {'observation_period_num': 103, 'train_rates': 0.8291506720970124, 'learning_rate': 0.00013281494625067084, 'batch_size': 207, 'step_size': 2, 'gamma': 0.9404883228063247}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:53:41,948][0m Trial 18 finished with value: 0.1486988827323433 and parameters: {'observation_period_num': 66, 'train_rates': 0.7250767261426235, 'learning_rate': 7.691663977906637e-06, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9058983340133995}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:54:38,894][0m Trial 19 finished with value: 0.032054389255963975 and parameters: {'observation_period_num': 27, 'train_rates': 0.88549921912053, 'learning_rate': 0.0005275459709301831, 'batch_size': 107, 'step_size': 3, 'gamma': 0.954636675523873}. Best is trial 10 with value: 0.027749420682445538.[0m
Early stopping at epoch 85
[32m[I 2025-02-03 09:55:12,447][0m Trial 20 finished with value: 0.1541469246149063 and parameters: {'observation_period_num': 125, 'train_rates': 0.958214910362968, 'learning_rate': 0.00011200881473317439, 'batch_size': 160, 'step_size': 1, 'gamma': 0.8668810299283989}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:56:07,101][0m Trial 21 finished with value: 0.03277341478637286 and parameters: {'observation_period_num': 8, 'train_rates': 0.8812412730330166, 'learning_rate': 0.0009928223164556345, 'batch_size': 111, 'step_size': 3, 'gamma': 0.964170436488681}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:57:33,817][0m Trial 22 finished with value: 0.03930453510365935 and parameters: {'observation_period_num': 28, 'train_rates': 0.9200606148979446, 'learning_rate': 0.00044922908891409656, 'batch_size': 70, 'step_size': 3, 'gamma': 0.9886147465851576}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:58:22,794][0m Trial 23 finished with value: 0.047685521798575575 and parameters: {'observation_period_num': 71, 'train_rates': 0.8541784065371872, 'learning_rate': 0.000507592992168257, 'batch_size': 123, 'step_size': 3, 'gamma': 0.9645304036098659}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:58:54,757][0m Trial 24 finished with value: 0.03331607126587413 and parameters: {'observation_period_num': 29, 'train_rates': 0.8020926530079554, 'learning_rate': 0.0002917494529954867, 'batch_size': 192, 'step_size': 5, 'gamma': 0.9428708653260575}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 09:59:25,036][0m Trial 25 finished with value: 0.039252009242773056 and parameters: {'observation_period_num': 28, 'train_rates': 0.9283038344136428, 'learning_rate': 0.00070236870017518, 'batch_size': 220, 'step_size': 1, 'gamma': 0.9290014350258144}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:00:04,527][0m Trial 26 finished with value: 0.044937168093607385 and parameters: {'observation_period_num': 60, 'train_rates': 0.8709355856563448, 'learning_rate': 0.00016785149850003322, 'batch_size': 156, 'step_size': 2, 'gamma': 0.9663973462491431}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:00:30,387][0m Trial 27 finished with value: 0.03331649805425752 and parameters: {'observation_period_num': 30, 'train_rates': 0.807179572032696, 'learning_rate': 0.0003598004784481444, 'batch_size': 230, 'step_size': 5, 'gamma': 0.9899257967868281}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:01:21,260][0m Trial 28 finished with value: 0.5764120011716276 and parameters: {'observation_period_num': 81, 'train_rates': 0.7682665489105232, 'learning_rate': 1.0070803970770821e-06, 'batch_size': 109, 'step_size': 3, 'gamma': 0.9719527480031883}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:02:04,641][0m Trial 29 finished with value: 0.11033713637340453 and parameters: {'observation_period_num': 250, 'train_rates': 0.9084110629103018, 'learning_rate': 4.020284264844073e-05, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9334655255182769}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:03:46,554][0m Trial 30 finished with value: 0.029291008906415488 and parameters: {'observation_period_num': 5, 'train_rates': 0.7308021708345742, 'learning_rate': 0.0006193570577527165, 'batch_size': 50, 'step_size': 6, 'gamma': 0.9538843045166734}. Best is trial 10 with value: 0.027749420682445538.[0m
[32m[I 2025-02-03 10:06:49,825][0m Trial 31 finished with value: 0.025596596404717873 and parameters: {'observation_period_num': 5, 'train_rates': 0.7241540040921328, 'learning_rate': 0.000641103987756869, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9529058384270008}. Best is trial 31 with value: 0.025596596404717873.[0m
[32m[I 2025-02-03 10:11:53,333][0m Trial 32 finished with value: 0.02512197972321713 and parameters: {'observation_period_num': 5, 'train_rates': 0.7209963610784927, 'learning_rate': 0.0009992608050628938, 'batch_size': 16, 'step_size': 7, 'gamma': 0.906894871149092}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:14:47,978][0m Trial 33 finished with value: 0.047148976025147185 and parameters: {'observation_period_num': 40, 'train_rates': 0.6463566409456895, 'learning_rate': 0.00091068714046088, 'batch_size': 26, 'step_size': 11, 'gamma': 0.9081926112799557}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:16:47,371][0m Trial 34 finished with value: 0.04968258086700886 and parameters: {'observation_period_num': 23, 'train_rates': 0.7084454108419002, 'learning_rate': 0.00027909147859161477, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8738646544211283}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:17:49,531][0m Trial 35 finished with value: 0.08573729850284688 and parameters: {'observation_period_num': 116, 'train_rates': 0.6480959566285044, 'learning_rate': 0.0006900896407303473, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8895805800047879}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:20:11,164][0m Trial 36 finished with value: 0.061337711985458385 and parameters: {'observation_period_num': 18, 'train_rates': 0.7521791813570367, 'learning_rate': 8.530019969238344e-05, 'batch_size': 36, 'step_size': 6, 'gamma': 0.9750929545550373}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:23:52,854][0m Trial 37 finished with value: 0.07888048332633814 and parameters: {'observation_period_num': 42, 'train_rates': 0.6785042447263273, 'learning_rate': 5.661713757585608e-06, 'batch_size': 21, 'step_size': 8, 'gamma': 0.9159453750332232}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:25:16,759][0m Trial 38 finished with value: 0.05626422542452614 and parameters: {'observation_period_num': 75, 'train_rates': 0.7915746786525071, 'learning_rate': 2.532950035250378e-05, 'batch_size': 62, 'step_size': 13, 'gamma': 0.8564162093496948}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:25:44,666][0m Trial 39 finished with value: 0.04796212325817433 and parameters: {'observation_period_num': 6, 'train_rates': 0.7192684420648474, 'learning_rate': 0.00022755314745885843, 'batch_size': 194, 'step_size': 10, 'gamma': 0.9753383423223084}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:26:48,039][0m Trial 40 finished with value: 0.04047621692395734 and parameters: {'observation_period_num': 57, 'train_rates': 0.8193524914264568, 'learning_rate': 0.00045050153362282807, 'batch_size': 85, 'step_size': 4, 'gamma': 0.8080488835100955}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:28:31,575][0m Trial 41 finished with value: 0.03126193601425609 and parameters: {'observation_period_num': 8, 'train_rates': 0.7323579325749832, 'learning_rate': 0.0006352425436707236, 'batch_size': 48, 'step_size': 6, 'gamma': 0.9524512616654676}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:30:55,897][0m Trial 42 finished with value: 0.029222592965635143 and parameters: {'observation_period_num': 18, 'train_rates': 0.6956372322688626, 'learning_rate': 0.000782238858762602, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9307546640800035}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:33:29,844][0m Trial 43 finished with value: 0.03273542560310217 and parameters: {'observation_period_num': 19, 'train_rates': 0.6894788894648833, 'learning_rate': 0.0008139988919528317, 'batch_size': 31, 'step_size': 8, 'gamma': 0.9299425387609469}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:37:58,548][0m Trial 44 finished with value: 0.034801187488276014 and parameters: {'observation_period_num': 37, 'train_rates': 0.6633430735558229, 'learning_rate': 0.0009935727374158594, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9365915864108952}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:39:57,487][0m Trial 45 finished with value: 0.0664587593957584 and parameters: {'observation_period_num': 155, 'train_rates': 0.7039073683743775, 'learning_rate': 0.0003699313437166337, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9193572332921919}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:40:21,839][0m Trial 46 finished with value: 0.038603343702149844 and parameters: {'observation_period_num': 19, 'train_rates': 0.783507793879672, 'learning_rate': 0.000542804393850601, 'batch_size': 252, 'step_size': 9, 'gamma': 0.7618485752943536}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:40:50,887][0m Trial 47 finished with value: 0.05347565689139603 and parameters: {'observation_period_num': 54, 'train_rates': 0.6257500662369845, 'learning_rate': 0.00024270113500694202, 'batch_size': 168, 'step_size': 1, 'gamma': 0.9793333587844094}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:42:14,283][0m Trial 48 finished with value: 0.034311055836743314 and parameters: {'observation_period_num': 37, 'train_rates': 0.7577424859824397, 'learning_rate': 0.0007433013058742713, 'batch_size': 62, 'step_size': 4, 'gamma': 0.9041899826647403}. Best is trial 32 with value: 0.02512197972321713.[0m
[32m[I 2025-02-03 10:42:44,019][0m Trial 49 finished with value: 0.04725223280139417 and parameters: {'observation_period_num': 16, 'train_rates': 0.7466615503220363, 'learning_rate': 0.00034962837355971386, 'batch_size': 187, 'step_size': 5, 'gamma': 0.9617606934508953}. Best is trial 32 with value: 0.02512197972321713.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1657 | 0.1066
Epoch 2/300, Loss: 0.0898 | 0.0943
Epoch 3/300, Loss: 0.0734 | 0.0586
Epoch 4/300, Loss: 0.0647 | 0.0562
Epoch 5/300, Loss: 0.0564 | 0.0600
Epoch 6/300, Loss: 0.0739 | 0.0506
Epoch 7/300, Loss: 0.0691 | 0.0600
Epoch 8/300, Loss: 0.0619 | 0.0587
Epoch 9/300, Loss: 0.0547 | 0.0532
Epoch 10/300, Loss: 0.0450 | 0.0476
Epoch 11/300, Loss: 0.0425 | 0.0499
Epoch 12/300, Loss: 0.0537 | 0.1138
Epoch 13/300, Loss: 0.0450 | 0.0689
Epoch 14/300, Loss: 0.0417 | 0.0542
Epoch 15/300, Loss: 0.0386 | 0.0366
Epoch 16/300, Loss: 0.0491 | 0.0674
Epoch 17/300, Loss: 0.0441 | 0.0423
Epoch 18/300, Loss: 0.0359 | 0.0346
Epoch 19/300, Loss: 0.0331 | 0.0389
Epoch 20/300, Loss: 0.0345 | 0.0416
Epoch 21/300, Loss: 0.0317 | 0.0346
Epoch 22/300, Loss: 0.0296 | 0.0366
Epoch 23/300, Loss: 0.0290 | 0.0367
Epoch 24/300, Loss: 0.0289 | 0.0387
Epoch 25/300, Loss: 0.0287 | 0.0395
Epoch 26/300, Loss: 0.0301 | 0.0454
Epoch 27/300, Loss: 0.0312 | 0.0367
Epoch 28/300, Loss: 0.0299 | 0.0393
Epoch 29/300, Loss: 0.0319 | 0.0474
Epoch 30/300, Loss: 0.0378 | 0.0411
Epoch 31/300, Loss: 0.0282 | 0.0319
Epoch 32/300, Loss: 0.0273 | 0.0335
Epoch 33/300, Loss: 0.0273 | 0.0325
Epoch 34/300, Loss: 0.0268 | 0.0326
Epoch 35/300, Loss: 0.0251 | 0.0286
Epoch 36/300, Loss: 0.0238 | 0.0302
Epoch 37/300, Loss: 0.0228 | 0.0301
Epoch 38/300, Loss: 0.0233 | 0.0288
Epoch 39/300, Loss: 0.0230 | 0.0301
Epoch 40/300, Loss: 0.0253 | 0.0377
Epoch 41/300, Loss: 0.0261 | 0.0309
Epoch 42/300, Loss: 0.0264 | 0.0336
Epoch 43/300, Loss: 0.0231 | 0.0300
Epoch 44/300, Loss: 0.0218 | 0.0313
Epoch 45/300, Loss: 0.0211 | 0.0303
Epoch 46/300, Loss: 0.0214 | 0.0303
Epoch 47/300, Loss: 0.0199 | 0.0291
Epoch 48/300, Loss: 0.0195 | 0.0289
Epoch 49/300, Loss: 0.0184 | 0.0266
Epoch 50/300, Loss: 0.0175 | 0.0274
Epoch 51/300, Loss: 0.0181 | 0.0249
Epoch 52/300, Loss: 0.0174 | 0.0263
Epoch 53/300, Loss: 0.0167 | 0.0257
Epoch 54/300, Loss: 0.0172 | 0.0264
Epoch 55/300, Loss: 0.0164 | 0.0259
Epoch 56/300, Loss: 0.0159 | 0.0263
Epoch 57/300, Loss: 0.0155 | 0.0252
Epoch 58/300, Loss: 0.0156 | 0.0259
Epoch 59/300, Loss: 0.0155 | 0.0256
Epoch 60/300, Loss: 0.0151 | 0.0253
Epoch 61/300, Loss: 0.0148 | 0.0241
Epoch 62/300, Loss: 0.0146 | 0.0261
Epoch 63/300, Loss: 0.0146 | 0.0246
Epoch 64/300, Loss: 0.0146 | 0.0256
Epoch 65/300, Loss: 0.0143 | 0.0234
Epoch 66/300, Loss: 0.0138 | 0.0247
Epoch 67/300, Loss: 0.0137 | 0.0241
Epoch 68/300, Loss: 0.0135 | 0.0248
Epoch 69/300, Loss: 0.0135 | 0.0235
Epoch 70/300, Loss: 0.0136 | 0.0251
Epoch 71/300, Loss: 0.0134 | 0.0234
Epoch 72/300, Loss: 0.0129 | 0.0241
Epoch 73/300, Loss: 0.0127 | 0.0242
Epoch 74/300, Loss: 0.0126 | 0.0239
Epoch 75/300, Loss: 0.0125 | 0.0234
Epoch 76/300, Loss: 0.0122 | 0.0241
Epoch 77/300, Loss: 0.0121 | 0.0233
Epoch 78/300, Loss: 0.0123 | 0.0245
Epoch 79/300, Loss: 0.0125 | 0.0234
Epoch 80/300, Loss: 0.0124 | 0.0247
Epoch 81/300, Loss: 0.0122 | 0.0240
Epoch 82/300, Loss: 0.0120 | 0.0244
Epoch 83/300, Loss: 0.0116 | 0.0246
Epoch 84/300, Loss: 0.0114 | 0.0240
Epoch 85/300, Loss: 0.0113 | 0.0246
Epoch 86/300, Loss: 0.0114 | 0.0243
Epoch 87/300, Loss: 0.0128 | 0.0244
Epoch 88/300, Loss: 0.0116 | 0.0241
Epoch 89/300, Loss: 0.0112 | 0.0241
Epoch 90/300, Loss: 0.0109 | 0.0238
Epoch 91/300, Loss: 0.0108 | 0.0241
Epoch 92/300, Loss: 0.0106 | 0.0242
Epoch 93/300, Loss: 0.0106 | 0.0244
Epoch 94/300, Loss: 0.0105 | 0.0247
Epoch 95/300, Loss: 0.0105 | 0.0246
Epoch 96/300, Loss: 0.0106 | 0.0251
Epoch 97/300, Loss: 0.0105 | 0.0244
Epoch 98/300, Loss: 0.0106 | 0.0254
Epoch 99/300, Loss: 0.0104 | 0.0247
Epoch 100/300, Loss: 0.0109 | 0.0244
Epoch 101/300, Loss: 0.0102 | 0.0246
Epoch 102/300, Loss: 0.0101 | 0.0247
Epoch 103/300, Loss: 0.0100 | 0.0249
Epoch 104/300, Loss: 0.0100 | 0.0248
Epoch 105/300, Loss: 0.0099 | 0.0247
Epoch 106/300, Loss: 0.0100 | 0.0246
Epoch 107/300, Loss: 0.0100 | 0.0247
Epoch 108/300, Loss: 0.0099 | 0.0246
Epoch 109/300, Loss: 0.0099 | 0.0249
Epoch 110/300, Loss: 0.0098 | 0.0246
Epoch 111/300, Loss: 0.0098 | 0.0249
Epoch 112/300, Loss: 0.0098 | 0.0246
Epoch 113/300, Loss: 0.0096 | 0.0247
Epoch 114/300, Loss: 0.0095 | 0.0249
Epoch 115/300, Loss: 0.0093 | 0.0247
Epoch 116/300, Loss: 0.0092 | 0.0249
Epoch 117/300, Loss: 0.0095 | 0.0247
Epoch 118/300, Loss: 0.0094 | 0.0248
Epoch 119/300, Loss: 0.0092 | 0.0248
Epoch 120/300, Loss: 0.0092 | 0.0252
Epoch 121/300, Loss: 0.0092 | 0.0253
Epoch 122/300, Loss: 0.0091 | 0.0254
Epoch 123/300, Loss: 0.0091 | 0.0253
Epoch 124/300, Loss: 0.0090 | 0.0256
Epoch 125/300, Loss: 0.0091 | 0.0254
Epoch 126/300, Loss: 0.0089 | 0.0251
Epoch 127/300, Loss: 0.0088 | 0.0250
Epoch 128/300, Loss: 0.0087 | 0.0252
Epoch 129/300, Loss: 0.0086 | 0.0251
Epoch 130/300, Loss: 0.0085 | 0.0252
Epoch 131/300, Loss: 0.0084 | 0.0253
Epoch 132/300, Loss: 0.0084 | 0.0254
Epoch 133/300, Loss: 0.0083 | 0.0253
Epoch 134/300, Loss: 0.0083 | 0.0254
Epoch 135/300, Loss: 0.0082 | 0.0254
Epoch 136/300, Loss: 0.0082 | 0.0254
Epoch 137/300, Loss: 0.0081 | 0.0255
Epoch 138/300, Loss: 0.0081 | 0.0254
Epoch 139/300, Loss: 0.0080 | 0.0255
Epoch 140/300, Loss: 0.0080 | 0.0255
Epoch 141/300, Loss: 0.0079 | 0.0255
Epoch 142/300, Loss: 0.0079 | 0.0256
Epoch 143/300, Loss: 0.0078 | 0.0256
Epoch 144/300, Loss: 0.0078 | 0.0257
Epoch 145/300, Loss: 0.0077 | 0.0256
Epoch 146/300, Loss: 0.0077 | 0.0257
Epoch 147/300, Loss: 0.0076 | 0.0257
Epoch 148/300, Loss: 0.0076 | 0.0258
Epoch 149/300, Loss: 0.0076 | 0.0258
Epoch 150/300, Loss: 0.0076 | 0.0259
Epoch 151/300, Loss: 0.0075 | 0.0259
Epoch 152/300, Loss: 0.0075 | 0.0260
Epoch 153/300, Loss: 0.0075 | 0.0260
Epoch 154/300, Loss: 0.0075 | 0.0261
Epoch 155/300, Loss: 0.0074 | 0.0261
Epoch 156/300, Loss: 0.0074 | 0.0261
Epoch 157/300, Loss: 0.0073 | 0.0262
Epoch 158/300, Loss: 0.0073 | 0.0262
Epoch 159/300, Loss: 0.0073 | 0.0263
Epoch 160/300, Loss: 0.0073 | 0.0263
Epoch 161/300, Loss: 0.0072 | 0.0264
Epoch 162/300, Loss: 0.0072 | 0.0264
Epoch 163/300, Loss: 0.0072 | 0.0265
Epoch 164/300, Loss: 0.0072 | 0.0265
Epoch 165/300, Loss: 0.0072 | 0.0265
Epoch 166/300, Loss: 0.0072 | 0.0266
Epoch 167/300, Loss: 0.0072 | 0.0267
Epoch 168/300, Loss: 0.0071 | 0.0267
Epoch 169/300, Loss: 0.0071 | 0.0269
Epoch 170/300, Loss: 0.0071 | 0.0270
Epoch 171/300, Loss: 0.0071 | 0.0270
Epoch 172/300, Loss: 0.0071 | 0.0271
Epoch 173/300, Loss: 0.0070 | 0.0271
Epoch 174/300, Loss: 0.0070 | 0.0272
Epoch 175/300, Loss: 0.0070 | 0.0271
Epoch 176/300, Loss: 0.0070 | 0.0272
Epoch 177/300, Loss: 0.0070 | 0.0272
Epoch 178/300, Loss: 0.0070 | 0.0272
Epoch 179/300, Loss: 0.0069 | 0.0272
Epoch 180/300, Loss: 0.0069 | 0.0272
Epoch 181/300, Loss: 0.0069 | 0.0272
Epoch 182/300, Loss: 0.0069 | 0.0272
Epoch 183/300, Loss: 0.0069 | 0.0272
Epoch 184/300, Loss: 0.0069 | 0.0272
Epoch 185/300, Loss: 0.0068 | 0.0272
Epoch 186/300, Loss: 0.0068 | 0.0272
Epoch 187/300, Loss: 0.0068 | 0.0272
Epoch 188/300, Loss: 0.0068 | 0.0272
Epoch 189/300, Loss: 0.0068 | 0.0271
Epoch 190/300, Loss: 0.0068 | 0.0271
Epoch 191/300, Loss: 0.0067 | 0.0271
Epoch 192/300, Loss: 0.0067 | 0.0270
Epoch 193/300, Loss: 0.0067 | 0.0270
Epoch 194/300, Loss: 0.0067 | 0.0270
Epoch 195/300, Loss: 0.0067 | 0.0269
Epoch 196/300, Loss: 0.0067 | 0.0269
Epoch 197/300, Loss: 0.0067 | 0.0269
Epoch 198/300, Loss: 0.0066 | 0.0269
Epoch 199/300, Loss: 0.0066 | 0.0269
Epoch 200/300, Loss: 0.0066 | 0.0269
Epoch 201/300, Loss: 0.0066 | 0.0268
Epoch 202/300, Loss: 0.0066 | 0.0268
Epoch 203/300, Loss: 0.0066 | 0.0268
Epoch 204/300, Loss: 0.0066 | 0.0268
Epoch 205/300, Loss: 0.0066 | 0.0268
Epoch 206/300, Loss: 0.0066 | 0.0268
Epoch 207/300, Loss: 0.0065 | 0.0268
Epoch 208/300, Loss: 0.0065 | 0.0268
Epoch 209/300, Loss: 0.0065 | 0.0268
Epoch 210/300, Loss: 0.0065 | 0.0268
Epoch 211/300, Loss: 0.0065 | 0.0268
Epoch 212/300, Loss: 0.0065 | 0.0268
Epoch 213/300, Loss: 0.0065 | 0.0268
Epoch 214/300, Loss: 0.0065 | 0.0268
Epoch 215/300, Loss: 0.0065 | 0.0268
Epoch 216/300, Loss: 0.0065 | 0.0268
Epoch 217/300, Loss: 0.0065 | 0.0268
Epoch 218/300, Loss: 0.0065 | 0.0268
Epoch 219/300, Loss: 0.0065 | 0.0268
Epoch 220/300, Loss: 0.0064 | 0.0268
Epoch 221/300, Loss: 0.0064 | 0.0268
Epoch 222/300, Loss: 0.0064 | 0.0268
Epoch 223/300, Loss: 0.0064 | 0.0268
Epoch 224/300, Loss: 0.0064 | 0.0268
Epoch 225/300, Loss: 0.0064 | 0.0268
Epoch 226/300, Loss: 0.0064 | 0.0268
Epoch 227/300, Loss: 0.0064 | 0.0268
Epoch 228/300, Loss: 0.0064 | 0.0268
Epoch 229/300, Loss: 0.0064 | 0.0268
Epoch 230/300, Loss: 0.0064 | 0.0268
Epoch 231/300, Loss: 0.0064 | 0.0268
Epoch 232/300, Loss: 0.0064 | 0.0268
Epoch 233/300, Loss: 0.0064 | 0.0268
Epoch 234/300, Loss: 0.0064 | 0.0268
Epoch 235/300, Loss: 0.0064 | 0.0268
Epoch 236/300, Loss: 0.0064 | 0.0268
Epoch 237/300, Loss: 0.0064 | 0.0268
Epoch 238/300, Loss: 0.0064 | 0.0268
Epoch 239/300, Loss: 0.0064 | 0.0268
Epoch 240/300, Loss: 0.0064 | 0.0268
Epoch 241/300, Loss: 0.0064 | 0.0268
Epoch 242/300, Loss: 0.0063 | 0.0268
Epoch 243/300, Loss: 0.0063 | 0.0268
Epoch 244/300, Loss: 0.0063 | 0.0268
Epoch 245/300, Loss: 0.0063 | 0.0268
Epoch 246/300, Loss: 0.0063 | 0.0268
Epoch 247/300, Loss: 0.0063 | 0.0268
Epoch 248/300, Loss: 0.0063 | 0.0268
Epoch 249/300, Loss: 0.0063 | 0.0268
Epoch 250/300, Loss: 0.0063 | 0.0268
Epoch 251/300, Loss: 0.0063 | 0.0268
Epoch 252/300, Loss: 0.0063 | 0.0268
Epoch 253/300, Loss: 0.0063 | 0.0268
Epoch 254/300, Loss: 0.0063 | 0.0268
Epoch 255/300, Loss: 0.0063 | 0.0268
Epoch 256/300, Loss: 0.0063 | 0.0268
Epoch 257/300, Loss: 0.0063 | 0.0268
Epoch 258/300, Loss: 0.0063 | 0.0268
Epoch 259/300, Loss: 0.0063 | 0.0268
Epoch 260/300, Loss: 0.0063 | 0.0268
Epoch 261/300, Loss: 0.0063 | 0.0268
Epoch 262/300, Loss: 0.0063 | 0.0268
Epoch 263/300, Loss: 0.0063 | 0.0269
Epoch 264/300, Loss: 0.0063 | 0.0268
Epoch 265/300, Loss: 0.0063 | 0.0269
Epoch 266/300, Loss: 0.0063 | 0.0269
Epoch 267/300, Loss: 0.0063 | 0.0269
Epoch 268/300, Loss: 0.0063 | 0.0269
Epoch 269/300, Loss: 0.0063 | 0.0269
Epoch 270/300, Loss: 0.0063 | 0.0269
Epoch 271/300, Loss: 0.0063 | 0.0269
Epoch 272/300, Loss: 0.0063 | 0.0269
Epoch 273/300, Loss: 0.0063 | 0.0269
Epoch 274/300, Loss: 0.0063 | 0.0269
Epoch 275/300, Loss: 0.0063 | 0.0269
Epoch 276/300, Loss: 0.0063 | 0.0269
Epoch 277/300, Loss: 0.0063 | 0.0269
Epoch 278/300, Loss: 0.0063 | 0.0269
Epoch 279/300, Loss: 0.0063 | 0.0269
Epoch 280/300, Loss: 0.0063 | 0.0269
Epoch 281/300, Loss: 0.0063 | 0.0269
Epoch 282/300, Loss: 0.0063 | 0.0269
Epoch 283/300, Loss: 0.0063 | 0.0269
Epoch 284/300, Loss: 0.0063 | 0.0269
Epoch 285/300, Loss: 0.0063 | 0.0269
Epoch 286/300, Loss: 0.0063 | 0.0269
Epoch 287/300, Loss: 0.0063 | 0.0269
Epoch 288/300, Loss: 0.0063 | 0.0269
Epoch 289/300, Loss: 0.0063 | 0.0269
Epoch 290/300, Loss: 0.0063 | 0.0269
Epoch 291/300, Loss: 0.0063 | 0.0269
Epoch 292/300, Loss: 0.0063 | 0.0269
Epoch 293/300, Loss: 0.0063 | 0.0269
Epoch 294/300, Loss: 0.0063 | 0.0269
Epoch 295/300, Loss: 0.0063 | 0.0269
Epoch 296/300, Loss: 0.0063 | 0.0269
Epoch 297/300, Loss: 0.0063 | 0.0269
Epoch 298/300, Loss: 0.0063 | 0.0269
Epoch 299/300, Loss: 0.0063 | 0.0269
Epoch 300/300, Loss: 0.0063 | 0.0269
Runtime (seconds): 912.3411154747009
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 5.1298075097147375
RMSE: 2.2649078369140625
MAE: 2.2649078369140625
R-squared: nan
[153.1751]
