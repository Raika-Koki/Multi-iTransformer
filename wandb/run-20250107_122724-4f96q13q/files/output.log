ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 12:27:27,688][0m A new study created in memory with name: no-name-57d57cc7-a632-4ca3-a7bd-8acb3628f7c9[0m
[32m[I 2025-01-07 12:32:12,446][0m Trial 0 finished with value: 0.24388621937721333 and parameters: {'observation_period_num': 228, 'train_rates': 0.9138010710362834, 'learning_rate': 1.7763133758593407e-06, 'batch_size': 19, 'step_size': 11, 'gamma': 0.773857497493709}. Best is trial 0 with value: 0.24388621937721333.[0m
[32m[I 2025-01-07 12:33:00,709][0m Trial 1 finished with value: 0.06526339054107666 and parameters: {'observation_period_num': 85, 'train_rates': 0.9858578893826205, 'learning_rate': 0.0005087776152647248, 'batch_size': 132, 'step_size': 5, 'gamma': 0.9390786619792244}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:34:03,656][0m Trial 2 finished with value: 0.7245466712894773 and parameters: {'observation_period_num': 224, 'train_rates': 0.6223773227402748, 'learning_rate': 1.4756543607319878e-06, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9475280020366029}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:34:35,849][0m Trial 3 finished with value: 0.20660020998700618 and parameters: {'observation_period_num': 20, 'train_rates': 0.690261178598398, 'learning_rate': 3.266777369204612e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.7552213844115503}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:35:41,422][0m Trial 4 finished with value: 0.3282667509001321 and parameters: {'observation_period_num': 43, 'train_rates': 0.6086412961425683, 'learning_rate': 5.169530084045816e-05, 'batch_size': 69, 'step_size': 1, 'gamma': 0.964175618941975}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:36:48,994][0m Trial 5 finished with value: 0.24922539700161328 and parameters: {'observation_period_num': 156, 'train_rates': 0.6777591173519436, 'learning_rate': 3.841826055163538e-05, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9049602710968915}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:37:31,417][0m Trial 6 finished with value: 0.3882327665031226 and parameters: {'observation_period_num': 196, 'train_rates': 0.7597135740064684, 'learning_rate': 3.32334034957152e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.915120817651288}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:38:06,390][0m Trial 7 finished with value: 0.235895975293676 and parameters: {'observation_period_num': 170, 'train_rates': 0.6590166581979635, 'learning_rate': 0.0003638388249099378, 'batch_size': 229, 'step_size': 8, 'gamma': 0.9262394519937478}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:38:40,969][0m Trial 8 finished with value: 0.2815934598106748 and parameters: {'observation_period_num': 146, 'train_rates': 0.7496325112318386, 'learning_rate': 0.00024736706859928133, 'batch_size': 202, 'step_size': 10, 'gamma': 0.8697773583302163}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:39:50,564][0m Trial 9 finished with value: 0.2497315655527158 and parameters: {'observation_period_num': 187, 'train_rates': 0.6412046488211867, 'learning_rate': 0.0005687841207754368, 'batch_size': 63, 'step_size': 11, 'gamma': 0.7918759401874461}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:40:35,706][0m Trial 10 finished with value: 0.18667517602443695 and parameters: {'observation_period_num': 101, 'train_rates': 0.9776075095033901, 'learning_rate': 8.414029034509574e-06, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9897438574210121}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:41:21,251][0m Trial 11 finished with value: 0.26731643080711365 and parameters: {'observation_period_num': 83, 'train_rates': 0.9635643846279507, 'learning_rate': 4.3019269747027935e-06, 'batch_size': 146, 'step_size': 4, 'gamma': 0.9878578607649868}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:42:22,803][0m Trial 12 finished with value: 0.370869113358459 and parameters: {'observation_period_num': 104, 'train_rates': 0.878583319237179, 'learning_rate': 7.939210424522172e-06, 'batch_size': 125, 'step_size': 4, 'gamma': 0.8663552205415725}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:43:05,417][0m Trial 13 finished with value: 0.09314662218093872 and parameters: {'observation_period_num': 68, 'train_rates': 0.9784338987375463, 'learning_rate': 0.00014348806793044308, 'batch_size': 171, 'step_size': 4, 'gamma': 0.9868157879553828}. Best is trial 1 with value: 0.06526339054107666.[0m
[32m[I 2025-01-07 12:43:42,529][0m Trial 14 finished with value: 0.05891405554144707 and parameters: {'observation_period_num': 55, 'train_rates': 0.8580893799296209, 'learning_rate': 0.00017821105971163934, 'batch_size': 182, 'step_size': 6, 'gamma': 0.831539174779009}. Best is trial 14 with value: 0.05891405554144707.[0m
[32m[I 2025-01-07 12:44:19,322][0m Trial 15 finished with value: 0.031375392384472345 and parameters: {'observation_period_num': 7, 'train_rates': 0.8431960850449925, 'learning_rate': 0.0008818955334531115, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8278894819173495}. Best is trial 15 with value: 0.031375392384472345.[0m
[32m[I 2025-01-07 12:44:56,966][0m Trial 16 finished with value: 0.04436836336645631 and parameters: {'observation_period_num': 7, 'train_rates': 0.8418156858958146, 'learning_rate': 0.0001328770423953814, 'batch_size': 247, 'step_size': 14, 'gamma': 0.8231777022257338}. Best is trial 15 with value: 0.031375392384472345.[0m
[32m[I 2025-01-07 12:45:30,580][0m Trial 17 finished with value: 0.0356673494358239 and parameters: {'observation_period_num': 10, 'train_rates': 0.8255501503503696, 'learning_rate': 0.0009739368106818899, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8094616269290634}. Best is trial 15 with value: 0.031375392384472345.[0m
[32m[I 2025-01-07 12:46:19,215][0m Trial 18 finished with value: 0.07970790911804546 and parameters: {'observation_period_num': 39, 'train_rates': 0.8113868615010008, 'learning_rate': 0.0008237580452348467, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8240565203688955}. Best is trial 15 with value: 0.031375392384472345.[0m
[32m[I 2025-01-07 12:47:09,774][0m Trial 19 finished with value: 0.02610269343030864 and parameters: {'observation_period_num': 5, 'train_rates': 0.9117286556118189, 'learning_rate': 0.0009876273611041503, 'batch_size': 222, 'step_size': 13, 'gamma': 0.7995218455363069}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:48:08,495][0m Trial 20 finished with value: 0.09149473348335058 and parameters: {'observation_period_num': 121, 'train_rates': 0.9162039598869971, 'learning_rate': 9.195470534975222e-05, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8541003553711287}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:49:04,656][0m Trial 21 finished with value: 0.038930123671889305 and parameters: {'observation_period_num': 23, 'train_rates': 0.8094440444026505, 'learning_rate': 0.000989520529099169, 'batch_size': 221, 'step_size': 13, 'gamma': 0.7988440581903148}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:49:58,447][0m Trial 22 finished with value: 0.0354433753249938 and parameters: {'observation_period_num': 8, 'train_rates': 0.9034454854985917, 'learning_rate': 0.000323856536700471, 'batch_size': 242, 'step_size': 15, 'gamma': 0.8021218756312107}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:50:42,774][0m Trial 23 finished with value: 0.05233479520237004 and parameters: {'observation_period_num': 35, 'train_rates': 0.9077318822317069, 'learning_rate': 0.0003710493852208186, 'batch_size': 206, 'step_size': 13, 'gamma': 0.8471452043621823}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:51:25,474][0m Trial 24 finished with value: 0.06613046072166541 and parameters: {'observation_period_num': 61, 'train_rates': 0.8799684946561221, 'learning_rate': 0.0003101422475989964, 'batch_size': 234, 'step_size': 14, 'gamma': 0.7784956679199988}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:52:08,586][0m Trial 25 finished with value: 0.04685111716389656 and parameters: {'observation_period_num': 30, 'train_rates': 0.9375306003605436, 'learning_rate': 0.0005234622316719051, 'batch_size': 193, 'step_size': 15, 'gamma': 0.8870435741190593}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:52:54,795][0m Trial 26 finished with value: 0.1972661621113252 and parameters: {'observation_period_num': 5, 'train_rates': 0.7793373398325952, 'learning_rate': 8.476778773521378e-05, 'batch_size': 237, 'step_size': 12, 'gamma': 0.7502491180894278}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:53:31,249][0m Trial 27 finished with value: 0.05043803633501132 and parameters: {'observation_period_num': 48, 'train_rates': 0.8761125977532951, 'learning_rate': 0.0002285779203436699, 'batch_size': 219, 'step_size': 14, 'gamma': 0.8403195539888848}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:54:01,811][0m Trial 28 finished with value: 0.26587343215942383 and parameters: {'observation_period_num': 249, 'train_rates': 0.927723418451906, 'learning_rate': 1.4069633497619665e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8101365043923335}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:54:39,729][0m Trial 29 finished with value: 0.0677633062005043 and parameters: {'observation_period_num': 71, 'train_rates': 0.94398932708963, 'learning_rate': 0.0006273653862365584, 'batch_size': 168, 'step_size': 12, 'gamma': 0.774786763480453}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 12:59:26,614][0m Trial 30 finished with value: 0.03434766932356595 and parameters: {'observation_period_num': 20, 'train_rates': 0.8974290262643443, 'learning_rate': 6.878578754980228e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7856770617414236}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:01:55,869][0m Trial 31 finished with value: 0.048288954021055486 and parameters: {'observation_period_num': 21, 'train_rates': 0.8923245613571245, 'learning_rate': 1.8310593274526825e-05, 'batch_size': 37, 'step_size': 15, 'gamma': 0.7894126854586876}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:02:48,686][0m Trial 32 finished with value: 0.04727561650030753 and parameters: {'observation_period_num': 22, 'train_rates': 0.8552870876723505, 'learning_rate': 8.552512657451445e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.7639437724226841}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:07:55,951][0m Trial 33 finished with value: 0.037455661182424854 and parameters: {'observation_period_num': 6, 'train_rates': 0.9050519113271268, 'learning_rate': 0.0004346278532225818, 'batch_size': 18, 'step_size': 13, 'gamma': 0.809263783295696}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:08:32,889][0m Trial 34 finished with value: 0.040459711446324716 and parameters: {'observation_period_num': 36, 'train_rates': 0.8467888519668948, 'learning_rate': 0.0006213022547057998, 'batch_size': 157, 'step_size': 15, 'gamma': 0.7795281165385625}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:11:03,152][0m Trial 35 finished with value: 0.05987516138702631 and parameters: {'observation_period_num': 50, 'train_rates': 0.9479116207807476, 'learning_rate': 6.253264897119e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7989685199445329}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:11:50,577][0m Trial 36 finished with value: 0.15499137362049786 and parameters: {'observation_period_num': 18, 'train_rates': 0.7283877232761864, 'learning_rate': 0.0002007692527338109, 'batch_size': 109, 'step_size': 14, 'gamma': 0.7638482584912716}. Best is trial 19 with value: 0.02610269343030864.[0m
Early stopping at epoch 68
[32m[I 2025-01-07 13:12:17,423][0m Trial 37 finished with value: 0.14359632574143957 and parameters: {'observation_period_num': 82, 'train_rates': 0.8937759888030122, 'learning_rate': 0.0007174516380824164, 'batch_size': 191, 'step_size': 1, 'gamma': 0.8191147778363291}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:12:47,696][0m Trial 38 finished with value: 1.2369836836445087 and parameters: {'observation_period_num': 30, 'train_rates': 0.8659511227984251, 'learning_rate': 1.0157787880453737e-06, 'batch_size': 240, 'step_size': 10, 'gamma': 0.8352435581364681}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:13:49,611][0m Trial 39 finished with value: 0.05816827942013242 and parameters: {'observation_period_num': 45, 'train_rates': 0.8359512508520661, 'learning_rate': 0.000282612630976868, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8569481816451767}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:14:22,361][0m Trial 40 finished with value: 0.07696898591748634 and parameters: {'observation_period_num': 18, 'train_rates': 0.9206239585429625, 'learning_rate': 2.3864996211424267e-05, 'batch_size': 222, 'step_size': 7, 'gamma': 0.7880883384443342}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:14:50,678][0m Trial 41 finished with value: 0.032028633207632606 and parameters: {'observation_period_num': 11, 'train_rates': 0.8159613861885553, 'learning_rate': 0.0009695683554601878, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8118745135802743}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:15:18,325][0m Trial 42 finished with value: 0.04056801822748077 and parameters: {'observation_period_num': 5, 'train_rates': 0.7889997706034222, 'learning_rate': 0.00044465921098781226, 'batch_size': 254, 'step_size': 15, 'gamma': 0.8057688010583816}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:15:58,586][0m Trial 43 finished with value: 0.04050978205795658 and parameters: {'observation_period_num': 26, 'train_rates': 0.8151539525583549, 'learning_rate': 0.0007639038145876039, 'batch_size': 230, 'step_size': 14, 'gamma': 0.7657179637927843}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:16:25,328][0m Trial 44 finished with value: 0.17359292764777398 and parameters: {'observation_period_num': 17, 'train_rates': 0.7717466810725205, 'learning_rate': 0.0004807861615837465, 'batch_size': 243, 'step_size': 13, 'gamma': 0.8914597684610963}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:16:51,412][0m Trial 45 finished with value: 0.34730370926723986 and parameters: {'observation_period_num': 208, 'train_rates': 0.7389644671422315, 'learning_rate': 4.325787559865679e-05, 'batch_size': 214, 'step_size': 15, 'gamma': 0.7855338848152232}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:17:21,880][0m Trial 46 finished with value: 0.05491537121972677 and parameters: {'observation_period_num': 61, 'train_rates': 0.8996086177900144, 'learning_rate': 0.0009645930406872295, 'batch_size': 229, 'step_size': 11, 'gamma': 0.817842060351829}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:17:53,743][0m Trial 47 finished with value: 0.10045859962701797 and parameters: {'observation_period_num': 140, 'train_rates': 0.9597192409880407, 'learning_rate': 0.00013878984715857108, 'batch_size': 199, 'step_size': 14, 'gamma': 0.7986726413784679}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:18:23,886][0m Trial 48 finished with value: 0.16890264532492186 and parameters: {'observation_period_num': 39, 'train_rates': 0.7124041145112214, 'learning_rate': 0.0003129965492611739, 'batch_size': 180, 'step_size': 13, 'gamma': 0.832307628645725}. Best is trial 19 with value: 0.02610269343030864.[0m
[32m[I 2025-01-07 13:18:55,862][0m Trial 49 finished with value: 0.646443021712733 and parameters: {'observation_period_num': 15, 'train_rates': 0.8342350178775267, 'learning_rate': 2.335157955615108e-06, 'batch_size': 247, 'step_size': 10, 'gamma': 0.842989483150022}. Best is trial 19 with value: 0.02610269343030864.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 13:18:55,873][0m A new study created in memory with name: no-name-c470c89a-28fa-403a-9135-09da762d323c[0m
[32m[I 2025-01-07 13:19:42,827][0m Trial 0 finished with value: 0.09730452885744216 and parameters: {'observation_period_num': 125, 'train_rates': 0.8156249019402783, 'learning_rate': 0.00010183459800909995, 'batch_size': 123, 'step_size': 15, 'gamma': 0.8955672174070282}. Best is trial 0 with value: 0.09730452885744216.[0m
[32m[I 2025-01-07 13:24:28,729][0m Trial 1 finished with value: 0.1478267082523915 and parameters: {'observation_period_num': 252, 'train_rates': 0.8396744082369156, 'learning_rate': 7.742064394575778e-06, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8975057489141895}. Best is trial 0 with value: 0.09730452885744216.[0m
[32m[I 2025-01-07 13:25:17,477][0m Trial 2 finished with value: 0.3747509963654929 and parameters: {'observation_period_num': 78, 'train_rates': 0.6335322074214059, 'learning_rate': 1.0042340320067184e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.7971282856674511}. Best is trial 0 with value: 0.09730452885744216.[0m
[32m[I 2025-01-07 13:26:08,260][0m Trial 3 finished with value: 0.2478787899017334 and parameters: {'observation_period_num': 35, 'train_rates': 0.9847062488755645, 'learning_rate': 9.636980606049692e-06, 'batch_size': 243, 'step_size': 10, 'gamma': 0.7959336476199315}. Best is trial 0 with value: 0.09730452885744216.[0m
[32m[I 2025-01-07 13:26:53,469][0m Trial 4 finished with value: 0.47769823451303683 and parameters: {'observation_period_num': 184, 'train_rates': 0.6905622050235516, 'learning_rate': 5.9355534395382204e-05, 'batch_size': 234, 'step_size': 3, 'gamma': 0.7721014882426198}. Best is trial 0 with value: 0.09730452885744216.[0m
[32m[I 2025-01-07 13:28:21,710][0m Trial 5 finished with value: 0.05745496535673737 and parameters: {'observation_period_num': 24, 'train_rates': 0.9181412186248694, 'learning_rate': 0.00030342328184025836, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8498580070882691}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:28:56,622][0m Trial 6 finished with value: 0.27346690006987756 and parameters: {'observation_period_num': 27, 'train_rates': 0.6125892389194451, 'learning_rate': 2.2811292377806372e-05, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8155410420385256}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:29:40,570][0m Trial 7 finished with value: 0.08353690001762139 and parameters: {'observation_period_num': 42, 'train_rates': 0.9090754303502555, 'learning_rate': 0.00014018137167555746, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8959105702988096}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:32:11,581][0m Trial 8 finished with value: 0.1475509381719998 and parameters: {'observation_period_num': 225, 'train_rates': 0.7949595648312433, 'learning_rate': 0.0004565569028029355, 'batch_size': 36, 'step_size': 4, 'gamma': 0.8429459054633077}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:32:46,504][0m Trial 9 finished with value: 0.2084124558974826 and parameters: {'observation_period_num': 75, 'train_rates': 0.6063158375361508, 'learning_rate': 0.00037168599090219923, 'batch_size': 234, 'step_size': 10, 'gamma': 0.8705357463073147}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:34:09,032][0m Trial 10 finished with value: 0.6835862398147583 and parameters: {'observation_period_num': 135, 'train_rates': 0.9885453112249809, 'learning_rate': 1.0335542446940111e-06, 'batch_size': 72, 'step_size': 1, 'gamma': 0.9740708824676232}. Best is trial 5 with value: 0.05745496535673737.[0m
[32m[I 2025-01-07 13:34:48,768][0m Trial 11 finished with value: 0.05710732150566942 and parameters: {'observation_period_num': 26, 'train_rates': 0.9106445596769744, 'learning_rate': 0.0009820702665347021, 'batch_size': 169, 'step_size': 10, 'gamma': 0.9556938261149684}. Best is trial 11 with value: 0.05710732150566942.[0m
[32m[I 2025-01-07 13:35:27,707][0m Trial 12 finished with value: 0.05599323015321385 and parameters: {'observation_period_num': 13, 'train_rates': 0.9066254067747425, 'learning_rate': 0.0008232748042868174, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9803041483417463}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:36:05,017][0m Trial 13 finished with value: 0.10294198343350042 and parameters: {'observation_period_num': 80, 'train_rates': 0.8923083099127496, 'learning_rate': 0.0009745249335535266, 'batch_size': 163, 'step_size': 12, 'gamma': 0.984446477000298}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:36:41,618][0m Trial 14 finished with value: 0.208157825943916 and parameters: {'observation_period_num': 8, 'train_rates': 0.7482319737679166, 'learning_rate': 0.0009208181341830463, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9516830020260318}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:37:23,854][0m Trial 15 finished with value: 0.09129517751833352 and parameters: {'observation_period_num': 123, 'train_rates': 0.8723342955521104, 'learning_rate': 0.00017486964037505466, 'batch_size': 148, 'step_size': 15, 'gamma': 0.9351292264469324}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:38:09,022][0m Trial 16 finished with value: 0.6544761657714844 and parameters: {'observation_period_num': 64, 'train_rates': 0.948280509793392, 'learning_rate': 1.3641862617536637e-06, 'batch_size': 192, 'step_size': 7, 'gamma': 0.9386095847167615}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:38:46,076][0m Trial 17 finished with value: 0.2876973956823349 and parameters: {'observation_period_num': 105, 'train_rates': 0.7613368293621438, 'learning_rate': 5.309845475491329e-05, 'batch_size': 207, 'step_size': 12, 'gamma': 0.9615968689850155}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:39:52,450][0m Trial 18 finished with value: 0.11869967305780056 and parameters: {'observation_period_num': 163, 'train_rates': 0.8574728151839706, 'learning_rate': 0.0005277150883950011, 'batch_size': 85, 'step_size': 11, 'gamma': 0.918372439094748}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:40:34,735][0m Trial 19 finished with value: 0.1699216915294528 and parameters: {'observation_period_num': 6, 'train_rates': 0.940318765382968, 'learning_rate': 2.973653733403878e-06, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9884054872788663}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:41:13,165][0m Trial 20 finished with value: 0.06561577320098877 and parameters: {'observation_period_num': 57, 'train_rates': 0.9572926884957127, 'learning_rate': 0.00023622228295479882, 'batch_size': 170, 'step_size': 14, 'gamma': 0.9199422642597292}. Best is trial 12 with value: 0.05599323015321385.[0m
[32m[I 2025-01-07 13:42:12,448][0m Trial 21 finished with value: 0.05427751243683218 and parameters: {'observation_period_num': 23, 'train_rates': 0.9106586202755962, 'learning_rate': 0.00033523935081972145, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8560525231148622}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:43:06,587][0m Trial 22 finished with value: 0.09941262975335122 and parameters: {'observation_period_num': 48, 'train_rates': 0.890117001252383, 'learning_rate': 0.0007024092642441374, 'batch_size': 112, 'step_size': 11, 'gamma': 0.8660213644924623}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:44:46,380][0m Trial 23 finished with value: 0.07028620770413005 and parameters: {'observation_period_num': 13, 'train_rates': 0.8297571715356659, 'learning_rate': 0.00044629422935047917, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9589922083977601}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:45:28,052][0m Trial 24 finished with value: 0.08293154249312701 and parameters: {'observation_period_num': 96, 'train_rates': 0.924421090605365, 'learning_rate': 0.0009467648249094634, 'batch_size': 143, 'step_size': 10, 'gamma': 0.8267636004890236}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:46:01,325][0m Trial 25 finished with value: 0.07510680320330328 and parameters: {'observation_period_num': 38, 'train_rates': 0.8667719415913273, 'learning_rate': 9.600116692472539e-05, 'batch_size': 189, 'step_size': 12, 'gamma': 0.8765630500168652}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:46:58,353][0m Trial 26 finished with value: 0.09196994947877961 and parameters: {'observation_period_num': 59, 'train_rates': 0.7939400687012713, 'learning_rate': 0.0002578016434489873, 'batch_size': 93, 'step_size': 7, 'gamma': 0.9179679111261523}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:47:46,606][0m Trial 27 finished with value: 0.061724308878183365 and parameters: {'observation_period_num': 23, 'train_rates': 0.9680161377392091, 'learning_rate': 0.0001780229328434809, 'batch_size': 132, 'step_size': 9, 'gamma': 0.9701406819911224}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:49:10,396][0m Trial 28 finished with value: 0.0918086642732486 and parameters: {'observation_period_num': 98, 'train_rates': 0.8943156867838746, 'learning_rate': 0.0005179792675855884, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9395941018085046}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:50:00,999][0m Trial 29 finished with value: 0.09078330904896793 and parameters: {'observation_period_num': 5, 'train_rates': 0.8138226152113796, 'learning_rate': 2.5829796012862457e-05, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8872365659121269}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:50:36,285][0m Trial 30 finished with value: 0.12693668046755158 and parameters: {'observation_period_num': 165, 'train_rates': 0.8543886371352956, 'learning_rate': 9.700683297324488e-05, 'batch_size': 162, 'step_size': 5, 'gamma': 0.8538216528213404}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:51:32,351][0m Trial 31 finished with value: 0.05541579090598701 and parameters: {'observation_period_num': 29, 'train_rates': 0.929540752641447, 'learning_rate': 0.0003135705485048919, 'batch_size': 107, 'step_size': 9, 'gamma': 0.8413743839573888}. Best is trial 21 with value: 0.05427751243683218.[0m
[32m[I 2025-01-07 13:52:20,760][0m Trial 32 finished with value: 0.053562555275857446 and parameters: {'observation_period_num': 47, 'train_rates': 0.9314165097839823, 'learning_rate': 0.0006060031271112746, 'batch_size': 126, 'step_size': 8, 'gamma': 0.8343184256979604}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:53:09,159][0m Trial 33 finished with value: 0.056146619280660025 and parameters: {'observation_period_num': 51, 'train_rates': 0.935931732596786, 'learning_rate': 0.0006084710707697068, 'batch_size': 125, 'step_size': 8, 'gamma': 0.8328931859554436}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:54:06,754][0m Trial 34 finished with value: 0.07337822020053864 and parameters: {'observation_period_num': 70, 'train_rates': 0.9700368816418744, 'learning_rate': 0.0003192826118120843, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8034859233095857}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:54:53,696][0m Trial 35 finished with value: 0.09907564228363229 and parameters: {'observation_period_num': 43, 'train_rates': 0.8802570448885682, 'learning_rate': 4.588376299727111e-05, 'batch_size': 127, 'step_size': 8, 'gamma': 0.7744208956519736}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:55:59,728][0m Trial 36 finished with value: 0.07206401299787626 and parameters: {'observation_period_num': 19, 'train_rates': 0.8387989270181768, 'learning_rate': 0.00012464728633152717, 'batch_size': 83, 'step_size': 9, 'gamma': 0.757986684016517}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:58:07,483][0m Trial 37 finished with value: 0.06926246394508893 and parameters: {'observation_period_num': 87, 'train_rates': 0.9323282514897926, 'learning_rate': 0.00022162083193831905, 'batch_size': 44, 'step_size': 6, 'gamma': 0.8133239358756943}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:58:56,501][0m Trial 38 finished with value: 0.26000406583613256 and parameters: {'observation_period_num': 33, 'train_rates': 0.6768514985745105, 'learning_rate': 1.503025952679337e-05, 'batch_size': 100, 'step_size': 8, 'gamma': 0.8605460194481152}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 13:59:42,839][0m Trial 39 finished with value: 0.07785503143694864 and parameters: {'observation_period_num': 33, 'train_rates': 0.9083327303974603, 'learning_rate': 7.597147922891208e-05, 'batch_size': 136, 'step_size': 5, 'gamma': 0.8385771418773306}. Best is trial 32 with value: 0.053562555275857446.[0m
[32m[I 2025-01-07 14:04:49,812][0m Trial 40 finished with value: 0.042260418976506876 and parameters: {'observation_period_num': 21, 'train_rates': 0.9684373263335981, 'learning_rate': 0.00033531135729907805, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8818183263617073}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:07:50,850][0m Trial 41 finished with value: 0.05568252984120185 and parameters: {'observation_period_num': 17, 'train_rates': 0.9720130848740935, 'learning_rate': 0.00035157061054603284, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8263223832740496}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:12:40,984][0m Trial 42 finished with value: 0.14751754238687712 and parameters: {'observation_period_num': 221, 'train_rates': 0.9896183345693595, 'learning_rate': 0.0003436469235171931, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8189365994989153}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:17:30,738][0m Trial 43 finished with value: 0.04694971794723182 and parameters: {'observation_period_num': 23, 'train_rates': 0.9683717807907842, 'learning_rate': 0.0001590348080898546, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8810508293700139}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:22:46,114][0m Trial 44 finished with value: 0.0561970140399604 and parameters: {'observation_period_num': 47, 'train_rates': 0.9501770868757357, 'learning_rate': 0.00016141949505808857, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8825595321805063}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:24:28,249][0m Trial 45 finished with value: 0.055795845698635535 and parameters: {'observation_period_num': 30, 'train_rates': 0.9639449820970352, 'learning_rate': 0.000232842240752005, 'batch_size': 58, 'step_size': 11, 'gamma': 0.9014816361107761}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:27:29,837][0m Trial 46 finished with value: 0.056974773098706105 and parameters: {'observation_period_num': 38, 'train_rates': 0.9255223175119094, 'learning_rate': 0.0004631577182953875, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8503014462298769}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:28:51,744][0m Trial 47 finished with value: 0.050123151391744614 and parameters: {'observation_period_num': 23, 'train_rates': 0.9797691553823352, 'learning_rate': 0.00012082693017978312, 'batch_size': 74, 'step_size': 10, 'gamma': 0.906351723551745}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:30:18,403][0m Trial 48 finished with value: 0.07788604497909546 and parameters: {'observation_period_num': 68, 'train_rates': 0.9772225091642178, 'learning_rate': 3.974570512271552e-05, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9047754769204811}. Best is trial 40 with value: 0.042260418976506876.[0m
[32m[I 2025-01-07 14:32:20,352][0m Trial 49 finished with value: 0.05500524491071701 and parameters: {'observation_period_num': 20, 'train_rates': 0.9899258231607119, 'learning_rate': 6.90754275203324e-05, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8906898079805298}. Best is trial 40 with value: 0.042260418976506876.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 14:32:20,363][0m A new study created in memory with name: no-name-48dacdc5-34e1-4b59-9303-7c77dbb85a09[0m
[32m[I 2025-01-07 14:32:48,072][0m Trial 0 finished with value: 0.6869972643852233 and parameters: {'observation_period_num': 185, 'train_rates': 0.6389119501274325, 'learning_rate': 4.820042435074802e-06, 'batch_size': 232, 'step_size': 3, 'gamma': 0.9643422801523109}. Best is trial 0 with value: 0.6869972643852233.[0m
[32m[I 2025-01-07 14:33:31,023][0m Trial 1 finished with value: 0.4581754784430227 and parameters: {'observation_period_num': 218, 'train_rates': 0.6147709746949129, 'learning_rate': 4.4874278084101215e-05, 'batch_size': 103, 'step_size': 1, 'gamma': 0.9377312235719446}. Best is trial 1 with value: 0.4581754784430227.[0m
[32m[I 2025-01-07 14:35:44,512][0m Trial 2 finished with value: 0.09995166761310477 and parameters: {'observation_period_num': 48, 'train_rates': 0.8562498342682147, 'learning_rate': 7.45945353832598e-06, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9083043318380926}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:36:37,592][0m Trial 3 finished with value: 0.3278508994014973 and parameters: {'observation_period_num': 154, 'train_rates': 0.6644977696059828, 'learning_rate': 3.8898429451527954e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.887059678535269}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:38:05,427][0m Trial 4 finished with value: 0.2776261046386765 and parameters: {'observation_period_num': 16, 'train_rates': 0.6430636180315652, 'learning_rate': 2.0327913523643904e-06, 'batch_size': 57, 'step_size': 6, 'gamma': 0.9258045146966816}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:39:51,708][0m Trial 5 finished with value: 0.8591374623399751 and parameters: {'observation_period_num': 45, 'train_rates': 0.7601911967204003, 'learning_rate': 1.0071467423176077e-06, 'batch_size': 52, 'step_size': 4, 'gamma': 0.9041960902185158}. Best is trial 2 with value: 0.09995166761310477.[0m
Early stopping at epoch 62
[32m[I 2025-01-07 14:40:52,235][0m Trial 6 finished with value: 0.1538329492141674 and parameters: {'observation_period_num': 220, 'train_rates': 0.915757926738525, 'learning_rate': 0.0003752457137175057, 'batch_size': 60, 'step_size': 1, 'gamma': 0.7967240670062019}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:41:58,047][0m Trial 7 finished with value: 0.13809173795822505 and parameters: {'observation_period_num': 225, 'train_rates': 0.796813494507407, 'learning_rate': 4.2853868806903906e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9218302116903286}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:42:41,015][0m Trial 8 finished with value: 0.5508259001699458 and parameters: {'observation_period_num': 118, 'train_rates': 0.8030634580144561, 'learning_rate': 2.500868781911649e-06, 'batch_size': 202, 'step_size': 5, 'gamma': 0.927612600964136}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:44:25,997][0m Trial 9 finished with value: 0.2710201034673246 and parameters: {'observation_period_num': 150, 'train_rates': 0.717882169761846, 'learning_rate': 1.9803905844097157e-05, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8661042213556303}. Best is trial 2 with value: 0.09995166761310477.[0m
[32m[I 2025-01-07 14:45:22,748][0m Trial 10 finished with value: 0.06702616065740585 and parameters: {'observation_period_num': 75, 'train_rates': 0.9567619234569531, 'learning_rate': 0.0003773478552893038, 'batch_size': 159, 'step_size': 10, 'gamma': 0.8278855380671587}. Best is trial 10 with value: 0.06702616065740585.[0m
[32m[I 2025-01-07 14:46:19,560][0m Trial 11 finished with value: 0.0891743078827858 and parameters: {'observation_period_num': 78, 'train_rates': 0.9891153172107638, 'learning_rate': 0.0006018943240059287, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8337838046877903}. Best is trial 10 with value: 0.06702616065740585.[0m
[32m[I 2025-01-07 14:47:16,739][0m Trial 12 finished with value: 0.09288403391838074 and parameters: {'observation_period_num': 93, 'train_rates': 0.9796086588061809, 'learning_rate': 0.000973881588845484, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8263861790300554}. Best is trial 10 with value: 0.06702616065740585.[0m
[32m[I 2025-01-07 14:48:07,242][0m Trial 13 finished with value: 0.08635766804218292 and parameters: {'observation_period_num': 78, 'train_rates': 0.9880477769687209, 'learning_rate': 0.00019621099647267637, 'batch_size': 155, 'step_size': 15, 'gamma': 0.7552382674432279}. Best is trial 10 with value: 0.06702616065740585.[0m
[32m[I 2025-01-07 14:48:53,503][0m Trial 14 finished with value: 0.05876330088211011 and parameters: {'observation_period_num': 88, 'train_rates': 0.9174259172550561, 'learning_rate': 0.00017016778053827988, 'batch_size': 136, 'step_size': 15, 'gamma': 0.7624557469590297}. Best is trial 14 with value: 0.05876330088211011.[0m
[32m[I 2025-01-07 14:49:43,379][0m Trial 15 finished with value: 0.07013071896064849 and parameters: {'observation_period_num': 113, 'train_rates': 0.9038913127787352, 'learning_rate': 0.00014745187636464515, 'batch_size': 122, 'step_size': 15, 'gamma': 0.7653984856444727}. Best is trial 14 with value: 0.05876330088211011.[0m
[32m[I 2025-01-07 14:50:22,347][0m Trial 16 finished with value: 0.06179565587988148 and parameters: {'observation_period_num': 47, 'train_rates': 0.9206499647144644, 'learning_rate': 0.00011939516614711403, 'batch_size': 192, 'step_size': 12, 'gamma': 0.7924843040050836}. Best is trial 14 with value: 0.05876330088211011.[0m
[32m[I 2025-01-07 14:51:00,462][0m Trial 17 finished with value: 0.05541178876278447 and parameters: {'observation_period_num': 9, 'train_rates': 0.8616169098714386, 'learning_rate': 0.00010255066699609998, 'batch_size': 200, 'step_size': 13, 'gamma': 0.7836734596815758}. Best is trial 17 with value: 0.05541178876278447.[0m
[32m[I 2025-01-07 14:51:37,356][0m Trial 18 finished with value: 0.0676657582008386 and parameters: {'observation_period_num': 7, 'train_rates': 0.8662256393752874, 'learning_rate': 7.778984110128694e-05, 'batch_size': 245, 'step_size': 13, 'gamma': 0.7851674208307802}. Best is trial 17 with value: 0.05541178876278447.[0m
[32m[I 2025-01-07 14:56:54,096][0m Trial 19 finished with value: 0.048428794753045096 and parameters: {'observation_period_num': 33, 'train_rates': 0.8772876043338285, 'learning_rate': 1.826870916302428e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.8509520322335029}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:00:35,995][0m Trial 20 finished with value: 0.05319575550935126 and parameters: {'observation_period_num': 23, 'train_rates': 0.8454817707058907, 'learning_rate': 1.5688458007440347e-05, 'batch_size': 24, 'step_size': 13, 'gamma': 0.8510590026713826}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:05:49,367][0m Trial 21 finished with value: 0.04960408541638258 and parameters: {'observation_period_num': 28, 'train_rates': 0.8435887497343622, 'learning_rate': 1.5038918482645516e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8584750313361397}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:11:18,496][0m Trial 22 finished with value: 0.05451570028864493 and parameters: {'observation_period_num': 34, 'train_rates': 0.8259773616647319, 'learning_rate': 1.2223246737097992e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8580938721928344}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:14:48,457][0m Trial 23 finished with value: 0.19023826207734665 and parameters: {'observation_period_num': 29, 'train_rates': 0.7464928574330394, 'learning_rate': 1.8153060769450897e-05, 'batch_size': 23, 'step_size': 14, 'gamma': 0.8398606670012342}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:17:51,998][0m Trial 24 finished with value: 0.08821715064618268 and parameters: {'observation_period_num': 60, 'train_rates': 0.8292431532160397, 'learning_rate': 9.00828199710019e-06, 'batch_size': 28, 'step_size': 8, 'gamma': 0.87913056183461}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:19:07,169][0m Trial 25 finished with value: 0.07101208811652833 and parameters: {'observation_period_num': 19, 'train_rates': 0.8827176396380865, 'learning_rate': 2.4681866271797178e-05, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8131168104508716}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:21:38,792][0m Trial 26 finished with value: 0.09025512297713074 and parameters: {'observation_period_num': 29, 'train_rates': 0.8267070095554684, 'learning_rate': 4.742830003314103e-06, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8511953686654772}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:22:48,609][0m Trial 27 finished with value: 0.25288517809843725 and parameters: {'observation_period_num': 59, 'train_rates': 0.7476578638594935, 'learning_rate': 1.4991081487917857e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8914591948118284}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:27:09,414][0m Trial 28 finished with value: 0.10761670327999374 and parameters: {'observation_period_num': 106, 'train_rates': 0.8456746529712551, 'learning_rate': 5.381631777099894e-06, 'batch_size': 20, 'step_size': 14, 'gamma': 0.810855278771846}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:29:46,989][0m Trial 29 finished with value: 0.1248404938905012 and parameters: {'observation_period_num': 135, 'train_rates': 0.9478450037614988, 'learning_rate': 5.957442698968789e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9734460130034208}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:31:04,802][0m Trial 30 finished with value: 0.514417712667763 and parameters: {'observation_period_num': 186, 'train_rates': 0.7710915751191607, 'learning_rate': 3.2482453609424994e-06, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8529232442785843}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:36:08,489][0m Trial 31 finished with value: 0.05660602319270536 and parameters: {'observation_period_num': 24, 'train_rates': 0.81352099321784, 'learning_rate': 1.1636214177513678e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8662747119901772}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:42:01,174][0m Trial 32 finished with value: 0.051146987441015104 and parameters: {'observation_period_num': 37, 'train_rates': 0.8837943634210547, 'learning_rate': 2.7966129360543832e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8514548485705437}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:44:39,525][0m Trial 33 finished with value: 0.06043317899157438 and parameters: {'observation_period_num': 60, 'train_rates': 0.8960051103161693, 'learning_rate': 2.7048028112408773e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.8773795438060488}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:45:46,643][0m Trial 34 finished with value: 0.12452596094873217 and parameters: {'observation_period_num': 244, 'train_rates': 0.8871415820180056, 'learning_rate': 3.1603685275725856e-05, 'batch_size': 98, 'step_size': 8, 'gamma': 0.8508026278595463}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:47:55,864][0m Trial 35 finished with value: 0.09068381728642622 and parameters: {'observation_period_num': 40, 'train_rates': 0.8493663237462337, 'learning_rate': 6.867554592794454e-06, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8946539252584805}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:50:54,778][0m Trial 36 finished with value: 0.06634399703104202 and parameters: {'observation_period_num': 59, 'train_rates': 0.874125990935764, 'learning_rate': 5.5282465253824836e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8166035413418873}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:52:21,963][0m Trial 37 finished with value: 0.07034451881367149 and parameters: {'observation_period_num': 15, 'train_rates': 0.9326480678409298, 'learning_rate': 1.1227136694218496e-05, 'batch_size': 67, 'step_size': 13, 'gamma': 0.9064843665347344}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:53:24,324][0m Trial 38 finished with value: 0.21614801984319917 and parameters: {'observation_period_num': 41, 'train_rates': 0.7740127668663782, 'learning_rate': 3.3648788122203106e-05, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9516728124752741}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:54:28,038][0m Trial 39 finished with value: 0.3730304746116911 and parameters: {'observation_period_num': 191, 'train_rates': 0.6833049297341379, 'learning_rate': 2.0252049042002517e-05, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8776672617292736}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 15:56:30,930][0m Trial 40 finished with value: 0.07162270921867815 and parameters: {'observation_period_num': 8, 'train_rates': 0.8428785068169373, 'learning_rate': 7.544275081991207e-06, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8431789237345622}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 16:01:31,403][0m Trial 41 finished with value: 0.05773165138353678 and parameters: {'observation_period_num': 32, 'train_rates': 0.8193682335985225, 'learning_rate': 1.360632381621616e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8643960025308208}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 16:03:18,098][0m Trial 42 finished with value: 0.23430664557881986 and parameters: {'observation_period_num': 36, 'train_rates': 0.6089311687930994, 'learning_rate': 9.399141062740538e-06, 'batch_size': 40, 'step_size': 12, 'gamma': 0.8580003211198179}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 16:04:55,584][0m Trial 43 finished with value: 0.10497182734055387 and parameters: {'observation_period_num': 53, 'train_rates': 0.7887724882670277, 'learning_rate': 1.6581910215320978e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8414343126021919}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 16:10:09,893][0m Trial 44 finished with value: 0.23705265667789427 and parameters: {'observation_period_num': 67, 'train_rates': 0.8401310549803839, 'learning_rate': 4.265850198855534e-06, 'batch_size': 17, 'step_size': 2, 'gamma': 0.8253406512076381}. Best is trial 19 with value: 0.048428794753045096.[0m
[32m[I 2025-01-07 16:13:09,248][0m Trial 45 finished with value: 0.04248192802331691 and parameters: {'observation_period_num': 22, 'train_rates': 0.8633278888216829, 'learning_rate': 4.185349153393916e-05, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8840162721829478}. Best is trial 45 with value: 0.04248192802331691.[0m
[32m[I 2025-01-07 16:16:49,250][0m Trial 46 finished with value: 0.0398456900002202 and parameters: {'observation_period_num': 19, 'train_rates': 0.8693226928773488, 'learning_rate': 4.162179117134796e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.9172364180298105}. Best is trial 46 with value: 0.0398456900002202.[0m
[32m[I 2025-01-07 16:18:29,695][0m Trial 47 finished with value: 0.05929116747223291 and parameters: {'observation_period_num': 48, 'train_rates': 0.8974826796457149, 'learning_rate': 4.406360312998192e-05, 'batch_size': 66, 'step_size': 15, 'gamma': 0.9147611701579343}. Best is trial 46 with value: 0.0398456900002202.[0m
[32m[I 2025-01-07 16:20:16,292][0m Trial 48 finished with value: 0.08042249035017973 and parameters: {'observation_period_num': 168, 'train_rates': 0.8707315062602624, 'learning_rate': 6.537343117164014e-05, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8964079978783014}. Best is trial 46 with value: 0.0398456900002202.[0m
[32m[I 2025-01-07 16:23:21,999][0m Trial 49 finished with value: 0.13658015678326288 and parameters: {'observation_period_num': 18, 'train_rates': 0.9428149656686426, 'learning_rate': 1.4887155219274444e-06, 'batch_size': 31, 'step_size': 14, 'gamma': 0.942228017527317}. Best is trial 46 with value: 0.0398456900002202.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 16:23:22,009][0m A new study created in memory with name: no-name-939e216f-7958-484e-ace2-9741b142a27c[0m
[32m[I 2025-01-07 16:24:01,433][0m Trial 0 finished with value: 0.8236025014807461 and parameters: {'observation_period_num': 158, 'train_rates': 0.6778450760018125, 'learning_rate': 1.8768417210562784e-06, 'batch_size': 171, 'step_size': 8, 'gamma': 0.870855097331738}. Best is trial 0 with value: 0.8236025014807461.[0m
[32m[I 2025-01-07 16:25:15,753][0m Trial 1 finished with value: 0.238123578564176 and parameters: {'observation_period_num': 139, 'train_rates': 0.9626434361875882, 'learning_rate': 5.197311443882761e-06, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8864245297547326}. Best is trial 1 with value: 0.238123578564176.[0m
Early stopping at epoch 59
[32m[I 2025-01-07 16:27:20,942][0m Trial 2 finished with value: 0.13494599011598848 and parameters: {'observation_period_num': 108, 'train_rates': 0.9821257994454196, 'learning_rate': 7.185723601542456e-05, 'batch_size': 29, 'step_size': 1, 'gamma': 0.7882888616099636}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:28:03,931][0m Trial 3 finished with value: 1.2046080830571748 and parameters: {'observation_period_num': 121, 'train_rates': 0.7055610625724285, 'learning_rate': 1.2707941106885465e-06, 'batch_size': 194, 'step_size': 14, 'gamma': 0.8851526053436771}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:28:48,896][0m Trial 4 finished with value: 0.7175913980834925 and parameters: {'observation_period_num': 107, 'train_rates': 0.7382180529771701, 'learning_rate': 4.490215079261918e-06, 'batch_size': 161, 'step_size': 11, 'gamma': 0.8473432602464555}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:29:33,100][0m Trial 5 finished with value: 0.44133134136412666 and parameters: {'observation_period_num': 241, 'train_rates': 0.7024403030915765, 'learning_rate': 1.602296861693809e-05, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8264351366630798}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:30:25,298][0m Trial 6 finished with value: 0.3332279394829392 and parameters: {'observation_period_num': 248, 'train_rates': 0.6663729535877868, 'learning_rate': 2.7830600540733207e-05, 'batch_size': 158, 'step_size': 9, 'gamma': 0.9758772085745213}. Best is trial 2 with value: 0.13494599011598848.[0m
Early stopping at epoch 40
[32m[I 2025-01-07 16:30:48,907][0m Trial 7 finished with value: 0.760268779595693 and parameters: {'observation_period_num': 202, 'train_rates': 0.8637834511647992, 'learning_rate': 4.851616984400686e-06, 'batch_size': 110, 'step_size': 1, 'gamma': 0.7680256684774159}. Best is trial 2 with value: 0.13494599011598848.[0m
Early stopping at epoch 92
[32m[I 2025-01-07 16:32:45,801][0m Trial 8 finished with value: 0.1592757999370726 and parameters: {'observation_period_num': 10, 'train_rates': 0.6633522543859923, 'learning_rate': 0.00036329781510454734, 'batch_size': 38, 'step_size': 1, 'gamma': 0.815130093275941}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:33:38,788][0m Trial 9 finished with value: 0.5215423430570147 and parameters: {'observation_period_num': 119, 'train_rates': 0.9381931257184501, 'learning_rate': 1.4530248298095963e-06, 'batch_size': 141, 'step_size': 6, 'gamma': 0.9753201718452414}. Best is trial 2 with value: 0.13494599011598848.[0m
[32m[I 2025-01-07 16:38:53,610][0m Trial 10 finished with value: 0.0585183390804256 and parameters: {'observation_period_num': 47, 'train_rates': 0.8349061829483879, 'learning_rate': 0.0002011434261699995, 'batch_size': 17, 'step_size': 4, 'gamma': 0.7595542714490247}. Best is trial 10 with value: 0.0585183390804256.[0m
[32m[I 2025-01-07 16:41:53,937][0m Trial 11 finished with value: 0.05642677672147988 and parameters: {'observation_period_num': 48, 'train_rates': 0.827404245934431, 'learning_rate': 0.0002333355645978622, 'batch_size': 30, 'step_size': 4, 'gamma': 0.7544755816356339}. Best is trial 11 with value: 0.05642677672147988.[0m
[32m[I 2025-01-07 16:43:14,648][0m Trial 12 finished with value: 0.04156746847412324 and parameters: {'observation_period_num': 34, 'train_rates': 0.8259660092352188, 'learning_rate': 0.0006173355281357591, 'batch_size': 67, 'step_size': 5, 'gamma': 0.7506441767579574}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:44:26,352][0m Trial 13 finished with value: 0.0619142060607963 and parameters: {'observation_period_num': 57, 'train_rates': 0.7882136074614887, 'learning_rate': 0.0007440426975372658, 'batch_size': 72, 'step_size': 4, 'gamma': 0.7964689153438808}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:46:01,221][0m Trial 14 finished with value: 0.05971547749543947 and parameters: {'observation_period_num': 64, 'train_rates': 0.8912235879575953, 'learning_rate': 0.0001325083001185968, 'batch_size': 59, 'step_size': 4, 'gamma': 0.7516285160773791}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:46:52,816][0m Trial 15 finished with value: 0.1718021656019925 and parameters: {'observation_period_num': 20, 'train_rates': 0.7742439244157998, 'learning_rate': 0.000946110545463796, 'batch_size': 111, 'step_size': 6, 'gamma': 0.7959062809311385}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:48:29,694][0m Trial 16 finished with value: 0.08480972605870991 and parameters: {'observation_period_num': 77, 'train_rates': 0.8290320680599886, 'learning_rate': 0.00037540790959063065, 'batch_size': 57, 'step_size': 6, 'gamma': 0.9186548349026076}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:49:18,144][0m Trial 17 finished with value: 0.2501784492902157 and parameters: {'observation_period_num': 33, 'train_rates': 0.6182578316703298, 'learning_rate': 6.899637355384297e-05, 'batch_size': 98, 'step_size': 3, 'gamma': 0.933087035998748}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:51:19,743][0m Trial 18 finished with value: 0.06307844034447445 and parameters: {'observation_period_num': 79, 'train_rates': 0.911859695396188, 'learning_rate': 0.0003837011716861705, 'batch_size': 46, 'step_size': 8, 'gamma': 0.7747530849004141}. Best is trial 12 with value: 0.04156746847412324.[0m
[32m[I 2025-01-07 16:56:32,374][0m Trial 19 finished with value: 0.032449400482269436 and parameters: {'observation_period_num': 6, 'train_rates': 0.823812191625443, 'learning_rate': 0.00017516285036875043, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8330089250187226}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 16:57:40,737][0m Trial 20 finished with value: 0.16723968765906042 and parameters: {'observation_period_num': 15, 'train_rates': 0.7514144229561291, 'learning_rate': 8.22511545515001e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.839251587657827}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:03:05,939][0m Trial 21 finished with value: 0.0714744995155278 and parameters: {'observation_period_num': 48, 'train_rates': 0.8255084162921665, 'learning_rate': 0.00023259556043155562, 'batch_size': 16, 'step_size': 10, 'gamma': 0.810186690681827}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:05:23,083][0m Trial 22 finished with value: 0.04606363231838489 and parameters: {'observation_period_num': 32, 'train_rates': 0.8620531063541106, 'learning_rate': 0.0005886310334228715, 'batch_size': 42, 'step_size': 7, 'gamma': 0.7509960750630187}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:07:02,365][0m Trial 23 finished with value: 0.07980512659996748 and parameters: {'observation_period_num': 86, 'train_rates': 0.8746642388916203, 'learning_rate': 0.000682889207319936, 'batch_size': 54, 'step_size': 7, 'gamma': 0.7782573570389073}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:08:23,126][0m Trial 24 finished with value: 0.03470295041604875 and parameters: {'observation_period_num': 5, 'train_rates': 0.8572146093727667, 'learning_rate': 0.0005804058296584657, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8509834504520366}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:09:07,167][0m Trial 25 finished with value: 0.050871768402621934 and parameters: {'observation_period_num': 6, 'train_rates': 0.7938323648416435, 'learning_rate': 0.00012612846128938658, 'batch_size': 129, 'step_size': 12, 'gamma': 0.8534546533528}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:10:27,980][0m Trial 26 finished with value: 0.04530838410918114 and parameters: {'observation_period_num': 27, 'train_rates': 0.9171148404976701, 'learning_rate': 4.009888703354167e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.9086318654487195}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:11:24,943][0m Trial 27 finished with value: 0.15574288274191145 and parameters: {'observation_period_num': 5, 'train_rates': 0.7549679560959778, 'learning_rate': 0.0004762037162042364, 'batch_size': 91, 'step_size': 13, 'gamma': 0.8610329385771424}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:12:01,215][0m Trial 28 finished with value: 0.055010350970096655 and parameters: {'observation_period_num': 38, 'train_rates': 0.8103425940465473, 'learning_rate': 0.0009392825477107812, 'batch_size': 236, 'step_size': 10, 'gamma': 0.8351241036189714}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:12:41,747][0m Trial 29 finished with value: 0.060899491920027624 and parameters: {'observation_period_num': 70, 'train_rates': 0.8511181512208419, 'learning_rate': 0.00012317851230155563, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8660640846420113}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:13:29,543][0m Trial 30 finished with value: 0.08033338401998792 and parameters: {'observation_period_num': 90, 'train_rates': 0.8925589988908207, 'learning_rate': 0.00019550030330995442, 'batch_size': 125, 'step_size': 12, 'gamma': 0.9534900516211195}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:14:59,146][0m Trial 31 finished with value: 0.0446109476824993 and parameters: {'observation_period_num': 24, 'train_rates': 0.9309789299103333, 'learning_rate': 3.6954893968901786e-05, 'batch_size': 68, 'step_size': 13, 'gamma': 0.912710690781596}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:16:07,798][0m Trial 32 finished with value: 0.06493876548483968 and parameters: {'observation_period_num': 26, 'train_rates': 0.9345745501776689, 'learning_rate': 1.4761391173317386e-05, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8894644446290211}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:17:26,667][0m Trial 33 finished with value: 0.09009084850549698 and parameters: {'observation_period_num': 167, 'train_rates': 0.9769820267205888, 'learning_rate': 3.942110794171986e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.8811595453798878}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:19:03,327][0m Trial 34 finished with value: 0.06828157315680454 and parameters: {'observation_period_num': 23, 'train_rates': 0.9414696881484964, 'learning_rate': 8.475900974166954e-06, 'batch_size': 63, 'step_size': 11, 'gamma': 0.9449883483684547}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:22:14,685][0m Trial 35 finished with value: 0.12533954890264618 and parameters: {'observation_period_num': 176, 'train_rates': 0.8886892443721651, 'learning_rate': 0.00031314888022003186, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9063387960268452}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:23:23,221][0m Trial 36 finished with value: 0.05092866672202945 and parameters: {'observation_period_num': 42, 'train_rates': 0.8076366714069964, 'learning_rate': 6.73323533393765e-05, 'batch_size': 90, 'step_size': 13, 'gamma': 0.8986913074215077}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:25:34,642][0m Trial 37 finished with value: 0.0908965907313607 and parameters: {'observation_period_num': 98, 'train_rates': 0.9614713704483859, 'learning_rate': 2.426548335357694e-05, 'batch_size': 46, 'step_size': 10, 'gamma': 0.8178929565798851}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:26:31,616][0m Trial 38 finished with value: 0.060310457833111286 and parameters: {'observation_period_num': 64, 'train_rates': 0.8451288501845059, 'learning_rate': 0.0005732679307062012, 'batch_size': 112, 'step_size': 9, 'gamma': 0.8720833962813268}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:27:30,129][0m Trial 39 finished with value: 0.3423963178433645 and parameters: {'observation_period_num': 15, 'train_rates': 0.7725299715785042, 'learning_rate': 3.617874249634901e-06, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9253088401058935}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:30:51,541][0m Trial 40 finished with value: 0.20478405027468669 and parameters: {'observation_period_num': 137, 'train_rates': 0.9145028389404304, 'learning_rate': 2.36797679899105e-06, 'batch_size': 28, 'step_size': 15, 'gamma': 0.8472698309749993}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:32:13,844][0m Trial 41 finished with value: 0.054841348704526095 and parameters: {'observation_period_num': 26, 'train_rates': 0.916126733578486, 'learning_rate': 1.858855856084891e-05, 'batch_size': 70, 'step_size': 12, 'gamma': 0.9052600581299745}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:32:48,063][0m Trial 42 finished with value: 0.06520802275929853 and parameters: {'observation_period_num': 57, 'train_rates': 0.8736126533130182, 'learning_rate': 4.3124414235793794e-05, 'batch_size': 179, 'step_size': 11, 'gamma': 0.9592809008098413}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:33:43,076][0m Trial 43 finished with value: 0.3853301038900184 and parameters: {'observation_period_num': 224, 'train_rates': 0.7185521601639268, 'learning_rate': 8.926796942303806e-06, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9896112633843475}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:35:11,931][0m Trial 44 finished with value: 0.04333746090934083 and parameters: {'observation_period_num': 7, 'train_rates': 0.9625427249399654, 'learning_rate': 4.797989118936632e-05, 'batch_size': 67, 'step_size': 13, 'gamma': 0.8833809654327719}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:37:08,055][0m Trial 45 finished with value: 0.042166199535131454 and parameters: {'observation_period_num': 13, 'train_rates': 0.987160075168559, 'learning_rate': 5.503289494645504e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8806022018631954}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:39:45,360][0m Trial 46 finished with value: 0.05650420697486919 and parameters: {'observation_period_num': 6, 'train_rates': 0.984610734087427, 'learning_rate': 5.9139443897158076e-05, 'batch_size': 38, 'step_size': 2, 'gamma': 0.876701161366316}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:43:46,676][0m Trial 47 finished with value: 0.036479318725026175 and parameters: {'observation_period_num': 14, 'train_rates': 0.9609546379437283, 'learning_rate': 9.883915991778494e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.8278370511680977}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:48:27,288][0m Trial 48 finished with value: 0.035230247653089464 and parameters: {'observation_period_num': 38, 'train_rates': 0.9893425666527292, 'learning_rate': 0.00010558795921920437, 'batch_size': 21, 'step_size': 14, 'gamma': 0.8233584139721398}. Best is trial 19 with value: 0.032449400482269436.[0m
[32m[I 2025-01-07 17:52:35,650][0m Trial 49 finished with value: 0.04921297836852701 and parameters: {'observation_period_num': 52, 'train_rates': 0.9479120688009423, 'learning_rate': 9.022397569582141e-05, 'batch_size': 23, 'step_size': 15, 'gamma': 0.8256727851028995}. Best is trial 19 with value: 0.032449400482269436.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 17:52:35,661][0m A new study created in memory with name: no-name-54ae4f76-40d1-485c-8c4b-c967c06243be[0m
[32m[I 2025-01-07 17:53:16,675][0m Trial 0 finished with value: 0.15976531655591397 and parameters: {'observation_period_num': 177, 'train_rates': 0.7844307023882998, 'learning_rate': 2.7699129740638924e-05, 'batch_size': 177, 'step_size': 13, 'gamma': 0.9710380325183245}. Best is trial 0 with value: 0.15976531655591397.[0m
[32m[I 2025-01-07 17:54:32,345][0m Trial 1 finished with value: 0.325457487608971 and parameters: {'observation_period_num': 234, 'train_rates': 0.7172413862688743, 'learning_rate': 1.8271991319034007e-05, 'batch_size': 64, 'step_size': 8, 'gamma': 0.9193093077094329}. Best is trial 0 with value: 0.15976531655591397.[0m
[32m[I 2025-01-07 17:55:08,974][0m Trial 2 finished with value: 0.096072408788617 and parameters: {'observation_period_num': 156, 'train_rates': 0.8990867431087173, 'learning_rate': 8.35883152991813e-05, 'batch_size': 230, 'step_size': 12, 'gamma': 0.7583735830357344}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 17:56:04,927][0m Trial 3 finished with value: 0.2532038713684519 and parameters: {'observation_period_num': 97, 'train_rates': 0.9541417722919251, 'learning_rate': 1.69929044783062e-05, 'batch_size': 110, 'step_size': 5, 'gamma': 0.7806038204620107}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 17:56:47,212][0m Trial 4 finished with value: 0.20249552715411803 and parameters: {'observation_period_num': 25, 'train_rates': 0.726105584578685, 'learning_rate': 5.856737578544571e-05, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8466696388337435}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 17:57:20,913][0m Trial 5 finished with value: 0.3573652702564269 and parameters: {'observation_period_num': 54, 'train_rates': 0.6388699224845478, 'learning_rate': 1.840112545107232e-05, 'batch_size': 218, 'step_size': 7, 'gamma': 0.7832296037676763}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 17:58:04,598][0m Trial 6 finished with value: 0.2517663391598736 and parameters: {'observation_period_num': 90, 'train_rates': 0.710200240176959, 'learning_rate': 3.089220572740208e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.9478653522665212}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 17:58:43,711][0m Trial 7 finished with value: 0.782977283000946 and parameters: {'observation_period_num': 197, 'train_rates': 0.963268466066033, 'learning_rate': 1.5777623620220866e-06, 'batch_size': 242, 'step_size': 14, 'gamma': 0.856086323998758}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 18:01:21,918][0m Trial 8 finished with value: 0.15720396169594356 and parameters: {'observation_period_num': 67, 'train_rates': 0.8060586467124276, 'learning_rate': 3.0565421333529495e-06, 'batch_size': 32, 'step_size': 2, 'gamma': 0.977733810238073}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 18:01:54,029][0m Trial 9 finished with value: 0.23033464654100255 and parameters: {'observation_period_num': 240, 'train_rates': 0.8744158903516113, 'learning_rate': 2.6333106340608313e-05, 'batch_size': 240, 'step_size': 6, 'gamma': 0.8808322721260978}. Best is trial 2 with value: 0.096072408788617.[0m
[32m[I 2025-01-07 18:02:29,843][0m Trial 10 finished with value: 0.083356461276452 and parameters: {'observation_period_num': 148, 'train_rates': 0.8885513324542015, 'learning_rate': 0.000650492165305009, 'batch_size': 171, 'step_size': 11, 'gamma': 0.7618505400410343}. Best is trial 10 with value: 0.083356461276452.[0m
[32m[I 2025-01-07 18:03:03,403][0m Trial 11 finished with value: 0.0693646880487601 and parameters: {'observation_period_num': 147, 'train_rates': 0.8848360176001201, 'learning_rate': 0.0006752524137606171, 'batch_size': 189, 'step_size': 11, 'gamma': 0.7597112663259039}. Best is trial 11 with value: 0.0693646880487601.[0m
[32m[I 2025-01-07 18:03:36,890][0m Trial 12 finished with value: 0.0723681608336557 and parameters: {'observation_period_num': 132, 'train_rates': 0.864291446223515, 'learning_rate': 0.0008351959093738028, 'batch_size': 187, 'step_size': 11, 'gamma': 0.8040237211247859}. Best is trial 11 with value: 0.0693646880487601.[0m
[32m[I 2025-01-07 18:04:09,560][0m Trial 13 finished with value: 0.06456477431883995 and parameters: {'observation_period_num': 119, 'train_rates': 0.8246896086454135, 'learning_rate': 0.000964474954952828, 'batch_size': 192, 'step_size': 10, 'gamma': 0.8117748287828033}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:04:40,449][0m Trial 14 finished with value: 0.08723114130477752 and parameters: {'observation_period_num': 110, 'train_rates': 0.8257223233099259, 'learning_rate': 0.00027104153004721946, 'batch_size': 200, 'step_size': 15, 'gamma': 0.8060837987549659}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:05:18,607][0m Trial 15 finished with value: 0.08650350315790427 and parameters: {'observation_period_num': 193, 'train_rates': 0.9313307936233828, 'learning_rate': 0.0002109895977368504, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8251684327889209}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:06:07,868][0m Trial 16 finished with value: 0.23091019939932023 and parameters: {'observation_period_num': 123, 'train_rates': 0.7727252756798779, 'learning_rate': 0.00031837466993934056, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8878138926704766}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:06:38,741][0m Trial 17 finished with value: 0.10391676517460367 and parameters: {'observation_period_num': 168, 'train_rates': 0.8334054762847222, 'learning_rate': 0.000525693718172561, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8259036461509427}. Best is trial 13 with value: 0.06456477431883995.[0m
Early stopping at epoch 55
[32m[I 2025-01-07 18:07:19,558][0m Trial 18 finished with value: 0.1514015141072456 and parameters: {'observation_period_num': 72, 'train_rates': 0.9277326822252319, 'learning_rate': 0.00011082278763760124, 'batch_size': 83, 'step_size': 1, 'gamma': 0.788571285604707}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:07:48,082][0m Trial 19 finished with value: 0.6901494863119768 and parameters: {'observation_period_num': 216, 'train_rates': 0.6474803785083829, 'learning_rate': 5.955518840803509e-06, 'batch_size': 166, 'step_size': 10, 'gamma': 0.7592427428991573}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:08:15,162][0m Trial 20 finished with value: 0.1670104265947577 and parameters: {'observation_period_num': 12, 'train_rates': 0.758881090889996, 'learning_rate': 0.0009831984386727219, 'batch_size': 251, 'step_size': 7, 'gamma': 0.8255057457931695}. Best is trial 13 with value: 0.06456477431883995.[0m
[32m[I 2025-01-07 18:08:47,521][0m Trial 21 finished with value: 0.06348172445428116 and parameters: {'observation_period_num': 138, 'train_rates': 0.8514422348300189, 'learning_rate': 0.000941393886804453, 'batch_size': 193, 'step_size': 11, 'gamma': 0.8035280395382328}. Best is trial 21 with value: 0.06348172445428116.[0m
[32m[I 2025-01-07 18:09:20,338][0m Trial 22 finished with value: 0.06094221445356178 and parameters: {'observation_period_num': 138, 'train_rates': 0.8513351814340249, 'learning_rate': 0.0004070512238695089, 'batch_size': 196, 'step_size': 12, 'gamma': 0.7894800222137822}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:09:50,889][0m Trial 23 finished with value: 0.08957449823349446 and parameters: {'observation_period_num': 129, 'train_rates': 0.8430486073390044, 'learning_rate': 0.00018309545249839697, 'batch_size': 211, 'step_size': 13, 'gamma': 0.8043897857233221}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:10:34,018][0m Trial 24 finished with value: 0.07232132493192246 and parameters: {'observation_period_num': 103, 'train_rates': 0.8047304131247255, 'learning_rate': 0.0003952769538821062, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8448888769349391}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:11:05,953][0m Trial 25 finished with value: 0.08747942000627518 and parameters: {'observation_period_num': 122, 'train_rates': 0.8452011330410028, 'learning_rate': 0.00044540087607123133, 'batch_size': 224, 'step_size': 12, 'gamma': 0.897743996906236}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:11:41,915][0m Trial 26 finished with value: 0.10229969024658203 and parameters: {'observation_period_num': 172, 'train_rates': 0.9839076421557154, 'learning_rate': 0.00014992203375471528, 'batch_size': 193, 'step_size': 9, 'gamma': 0.7933342741860971}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:12:20,064][0m Trial 27 finished with value: 0.20486841290894953 and parameters: {'observation_period_num': 82, 'train_rates': 0.7532027008941364, 'learning_rate': 0.0009985707995864264, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8223544767165852}. Best is trial 22 with value: 0.06094221445356178.[0m
[32m[I 2025-01-07 18:12:58,291][0m Trial 28 finished with value: 0.06049739666064913 and parameters: {'observation_period_num': 46, 'train_rates': 0.9179473453802447, 'learning_rate': 5.4089673683555816e-05, 'batch_size': 160, 'step_size': 10, 'gamma': 0.8643258709145601}. Best is trial 28 with value: 0.06049739666064913.[0m
[32m[I 2025-01-07 18:13:34,820][0m Trial 29 finished with value: 0.17740256601655988 and parameters: {'observation_period_num': 56, 'train_rates': 0.9267723714921839, 'learning_rate': 7.588512496189016e-06, 'batch_size': 168, 'step_size': 14, 'gamma': 0.9157909872732002}. Best is trial 28 with value: 0.06049739666064913.[0m
[32m[I 2025-01-07 18:14:09,741][0m Trial 30 finished with value: 0.06318658097184819 and parameters: {'observation_period_num': 39, 'train_rates': 0.9047185525423413, 'learning_rate': 5.145351269160545e-05, 'batch_size': 177, 'step_size': 8, 'gamma': 0.8592877207782145}. Best is trial 28 with value: 0.06049739666064913.[0m
[32m[I 2025-01-07 18:14:45,489][0m Trial 31 finished with value: 0.06616108085749284 and parameters: {'observation_period_num': 37, 'train_rates': 0.9092217127723392, 'learning_rate': 3.9884441364357395e-05, 'batch_size': 179, 'step_size': 8, 'gamma': 0.869866044084589}. Best is trial 28 with value: 0.06049739666064913.[0m
[32m[I 2025-01-07 18:15:30,085][0m Trial 32 finished with value: 0.05641589464699205 and parameters: {'observation_period_num': 36, 'train_rates': 0.8554151106767424, 'learning_rate': 6.934724129758479e-05, 'batch_size': 157, 'step_size': 8, 'gamma': 0.8579550640587359}. Best is trial 32 with value: 0.05641589464699205.[0m
[32m[I 2025-01-07 18:16:11,711][0m Trial 33 finished with value: 0.05611506123095751 and parameters: {'observation_period_num': 7, 'train_rates': 0.9049849481795289, 'learning_rate': 6.731638723581116e-05, 'batch_size': 154, 'step_size': 7, 'gamma': 0.8628674556962658}. Best is trial 33 with value: 0.05611506123095751.[0m
[32m[I 2025-01-07 18:17:08,189][0m Trial 34 finished with value: 0.04690032642572484 and parameters: {'observation_period_num': 9, 'train_rates': 0.9524185980163853, 'learning_rate': 9.508014683504368e-05, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9076320896986778}. Best is trial 34 with value: 0.04690032642572484.[0m
[32m[I 2025-01-07 18:18:06,581][0m Trial 35 finished with value: 0.04618876075295553 and parameters: {'observation_period_num': 6, 'train_rates': 0.9506431421409031, 'learning_rate': 8.171668464425619e-05, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9126787872021261}. Best is trial 35 with value: 0.04618876075295553.[0m
[32m[I 2025-01-07 18:19:06,451][0m Trial 36 finished with value: 0.04343931955265832 and parameters: {'observation_period_num': 7, 'train_rates': 0.9517303842177492, 'learning_rate': 7.945154585608708e-05, 'batch_size': 101, 'step_size': 4, 'gamma': 0.9490665932002382}. Best is trial 36 with value: 0.04343931955265832.[0m
[32m[I 2025-01-07 18:20:18,494][0m Trial 37 finished with value: 0.05626881495118141 and parameters: {'observation_period_num': 8, 'train_rates': 0.9899119935374813, 'learning_rate': 9.281454662431962e-05, 'batch_size': 87, 'step_size': 4, 'gamma': 0.9496249333046892}. Best is trial 36 with value: 0.04343931955265832.[0m
[32m[I 2025-01-07 18:21:16,158][0m Trial 38 finished with value: 0.04676285997853367 and parameters: {'observation_period_num': 22, 'train_rates': 0.9629024933027761, 'learning_rate': 0.00011648485201696857, 'batch_size': 105, 'step_size': 3, 'gamma': 0.9399752662598532}. Best is trial 36 with value: 0.04343931955265832.[0m
[32m[I 2025-01-07 18:23:16,383][0m Trial 39 finished with value: 0.04231715472760024 and parameters: {'observation_period_num': 22, 'train_rates': 0.9542115174132776, 'learning_rate': 0.00012555056434260654, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9505645472501556}. Best is trial 39 with value: 0.04231715472760024.[0m
[32m[I 2025-01-07 18:26:17,665][0m Trial 40 finished with value: 0.03633798972189983 and parameters: {'observation_period_num': 24, 'train_rates': 0.9635697043241712, 'learning_rate': 0.00014168565272108758, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9462813239969009}. Best is trial 40 with value: 0.03633798972189983.[0m
[32m[I 2025-01-07 18:29:03,565][0m Trial 41 finished with value: 0.04146428253840317 and parameters: {'observation_period_num': 24, 'train_rates': 0.9627148690572994, 'learning_rate': 0.00013313487493411746, 'batch_size': 35, 'step_size': 3, 'gamma': 0.9461291837244341}. Best is trial 40 with value: 0.03633798972189983.[0m
[32m[I 2025-01-07 18:34:01,158][0m Trial 42 finished with value: 0.036726470730161366 and parameters: {'observation_period_num': 25, 'train_rates': 0.9462094389889487, 'learning_rate': 0.0001404177599602716, 'batch_size': 19, 'step_size': 3, 'gamma': 0.961679588415459}. Best is trial 40 with value: 0.03633798972189983.[0m
[32m[I 2025-01-07 18:38:42,729][0m Trial 43 finished with value: 0.04378480554140847 and parameters: {'observation_period_num': 25, 'train_rates': 0.940620095811842, 'learning_rate': 0.00015136398819917156, 'batch_size': 20, 'step_size': 2, 'gamma': 0.9615983336754116}. Best is trial 40 with value: 0.03633798972189983.[0m
[32m[I 2025-01-07 18:40:46,814][0m Trial 44 finished with value: 0.02939761523157358 and parameters: {'observation_period_num': 23, 'train_rates': 0.9782856662554168, 'learning_rate': 0.0002170887499126454, 'batch_size': 48, 'step_size': 5, 'gamma': 0.9826425207806204}. Best is trial 44 with value: 0.02939761523157358.[0m
[32m[I 2025-01-07 18:42:50,083][0m Trial 45 finished with value: 0.06432792792717616 and parameters: {'observation_period_num': 57, 'train_rates': 0.9751811722547731, 'learning_rate': 0.00023061542844113374, 'batch_size': 48, 'step_size': 5, 'gamma': 0.9895247211561548}. Best is trial 44 with value: 0.02939761523157358.[0m
[32m[I 2025-01-07 18:44:59,640][0m Trial 46 finished with value: 0.046839382666885185 and parameters: {'observation_period_num': 25, 'train_rates': 0.9697507172343475, 'learning_rate': 0.00013276840650869776, 'batch_size': 46, 'step_size': 2, 'gamma': 0.9302409233125926}. Best is trial 44 with value: 0.02939761523157358.[0m
[32m[I 2025-01-07 18:51:01,466][0m Trial 47 finished with value: 0.06894927823992625 and parameters: {'observation_period_num': 68, 'train_rates': 0.9749264762099695, 'learning_rate': 2.3409987753786436e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9689693032764873}. Best is trial 44 with value: 0.02939761523157358.[0m
[32m[I 2025-01-07 18:52:31,121][0m Trial 48 finished with value: 0.15983268151948885 and parameters: {'observation_period_num': 44, 'train_rates': 0.6054252983678439, 'learning_rate': 0.0003003249861237597, 'batch_size': 58, 'step_size': 1, 'gamma': 0.9813260510912378}. Best is trial 44 with value: 0.02939761523157358.[0m
[32m[I 2025-01-07 18:55:43,663][0m Trial 49 finished with value: 0.04792055762246296 and parameters: {'observation_period_num': 20, 'train_rates': 0.8874561334032365, 'learning_rate': 0.0001847921687038193, 'batch_size': 32, 'step_size': 5, 'gamma': 0.9604910354456726}. Best is trial 44 with value: 0.02939761523157358.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 18:55:43,672][0m A new study created in memory with name: no-name-321fce8b-52cb-4bc8-a991-a5aa2a2bd73b[0m
[32m[I 2025-01-07 18:57:41,041][0m Trial 0 finished with value: 0.1206826613111011 and parameters: {'observation_period_num': 174, 'train_rates': 0.9788102921391798, 'learning_rate': 6.85350124770712e-05, 'batch_size': 53, 'step_size': 14, 'gamma': 0.9135526998767305}. Best is trial 0 with value: 0.1206826613111011.[0m
[32m[I 2025-01-07 18:59:54,410][0m Trial 1 finished with value: 0.036517390563171735 and parameters: {'observation_period_num': 17, 'train_rates': 0.8999554067909559, 'learning_rate': 0.00036956755387620294, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9185838704257898}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:00:48,150][0m Trial 2 finished with value: 0.1730118456274964 and parameters: {'observation_period_num': 62, 'train_rates': 0.8068091115105893, 'learning_rate': 1.5904170771069868e-05, 'batch_size': 247, 'step_size': 14, 'gamma': 0.8310793213440123}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:02:23,945][0m Trial 3 finished with value: 0.20935466446854867 and parameters: {'observation_period_num': 57, 'train_rates': 0.7436990277950017, 'learning_rate': 0.0008227075838084467, 'batch_size': 63, 'step_size': 15, 'gamma': 0.879362547291479}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:03:35,972][0m Trial 4 finished with value: 0.05604981113334789 and parameters: {'observation_period_num': 53, 'train_rates': 0.8126809852234399, 'learning_rate': 8.82360591299531e-05, 'batch_size': 107, 'step_size': 5, 'gamma': 0.9329892056574636}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:04:41,559][0m Trial 5 finished with value: 0.1019757262139178 and parameters: {'observation_period_num': 247, 'train_rates': 0.8517843584973805, 'learning_rate': 0.00015392268325688295, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8659043838046119}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:05:50,704][0m Trial 6 finished with value: 0.16158081063379845 and parameters: {'observation_period_num': 15, 'train_rates': 0.6365522265826841, 'learning_rate': 2.6520432790440306e-05, 'batch_size': 103, 'step_size': 6, 'gamma': 0.9885313985524473}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:06:57,181][0m Trial 7 finished with value: 0.3321835939362456 and parameters: {'observation_period_num': 124, 'train_rates': 0.8611717185631238, 'learning_rate': 3.4554555312641256e-06, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8056212728493432}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:08:14,139][0m Trial 8 finished with value: 0.06833124028800598 and parameters: {'observation_period_num': 89, 'train_rates': 0.8107714073317096, 'learning_rate': 6.173264455694085e-05, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9526112702944115}. Best is trial 1 with value: 0.036517390563171735.[0m
[32m[I 2025-01-07 19:09:10,885][0m Trial 9 finished with value: 0.13045994523631627 and parameters: {'observation_period_num': 144, 'train_rates': 0.7782510229392622, 'learning_rate': 4.980836142838678e-05, 'batch_size': 149, 'step_size': 14, 'gamma': 0.9096278055752064}. Best is trial 1 with value: 0.036517390563171735.[0m
Early stopping at epoch 64
[32m[I 2025-01-07 19:11:53,106][0m Trial 10 finished with value: 0.04653956092894077 and parameters: {'observation_period_num': 14, 'train_rates': 0.9661790351984781, 'learning_rate': 0.0005452884120768443, 'batch_size': 26, 'step_size': 1, 'gamma': 0.7501409243522977}. Best is trial 1 with value: 0.036517390563171735.[0m
Early stopping at epoch 74
[32m[I 2025-01-07 19:16:14,368][0m Trial 11 finished with value: 0.03388796428718218 and parameters: {'observation_period_num': 15, 'train_rates': 0.9583361773603969, 'learning_rate': 0.0009653282271988024, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7789105802293776}. Best is trial 11 with value: 0.03388796428718218.[0m
Early stopping at epoch 55
[32m[I 2025-01-07 19:19:07,254][0m Trial 12 finished with value: 0.044672529455425776 and parameters: {'observation_period_num': 7, 'train_rates': 0.9166888386629709, 'learning_rate': 0.0003485684935812183, 'batch_size': 19, 'step_size': 1, 'gamma': 0.7501495479297519}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:19:52,216][0m Trial 13 finished with value: 0.06569054918136538 and parameters: {'observation_period_num': 109, 'train_rates': 0.9137844166538266, 'learning_rate': 0.0009860311214457483, 'batch_size': 193, 'step_size': 4, 'gamma': 0.7982048300766919}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:21:21,819][0m Trial 14 finished with value: 0.11779631210468974 and parameters: {'observation_period_num': 181, 'train_rates': 0.9090355339218537, 'learning_rate': 0.0003558949670961733, 'batch_size': 61, 'step_size': 10, 'gamma': 0.8650374227808814}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:25:59,354][0m Trial 15 finished with value: 0.16614233510512294 and parameters: {'observation_period_num': 34, 'train_rates': 0.7063422854755984, 'learning_rate': 0.0001911315007127899, 'batch_size': 17, 'step_size': 3, 'gamma': 0.8220421925307165}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:27:53,585][0m Trial 16 finished with value: 0.2897107941763742 and parameters: {'observation_period_num': 83, 'train_rates': 0.9513124688512069, 'learning_rate': 1.5016197810806975e-06, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9692482567569758}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:28:34,391][0m Trial 17 finished with value: 0.5118634899457296 and parameters: {'observation_period_num': 227, 'train_rates': 0.8734762116091008, 'learning_rate': 1.226578627966877e-05, 'batch_size': 184, 'step_size': 2, 'gamma': 0.901263064037079}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:29:54,372][0m Trial 18 finished with value: 0.04542749375104904 and parameters: {'observation_period_num': 38, 'train_rates': 0.9861284752721272, 'learning_rate': 0.00019516306812562164, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8430467611902345}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:32:09,956][0m Trial 19 finished with value: 0.08436707958069622 and parameters: {'observation_period_num': 87, 'train_rates': 0.9186805474318616, 'learning_rate': 0.0004296937661018944, 'batch_size': 41, 'step_size': 10, 'gamma': 0.780642585120173}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:33:03,596][0m Trial 20 finished with value: 0.2392323595538818 and parameters: {'observation_period_num': 153, 'train_rates': 0.6138145052300346, 'learning_rate': 0.0009705351067323114, 'batch_size': 80, 'step_size': 3, 'gamma': 0.8872670397563973}. Best is trial 11 with value: 0.03388796428718218.[0m
Early stopping at epoch 70
[32m[I 2025-01-07 19:36:36,306][0m Trial 21 finished with value: 0.053241083878524526 and parameters: {'observation_period_num': 20, 'train_rates': 0.9340146648078859, 'learning_rate': 0.00031274316401001454, 'batch_size': 19, 'step_size': 1, 'gamma': 0.7526000535407922}. Best is trial 11 with value: 0.03388796428718218.[0m
Early stopping at epoch 69
[32m[I 2025-01-07 19:38:26,831][0m Trial 22 finished with value: 0.06669365300711255 and parameters: {'observation_period_num': 35, 'train_rates': 0.8845219649433631, 'learning_rate': 0.00015227019785026975, 'batch_size': 36, 'step_size': 1, 'gamma': 0.7745951481389164}. Best is trial 11 with value: 0.03388796428718218.[0m
[32m[I 2025-01-07 19:44:25,824][0m Trial 23 finished with value: 0.02977675926394579 and parameters: {'observation_period_num': 9, 'train_rates': 0.9444472463248235, 'learning_rate': 0.0005027129455611631, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7765408970098743}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:45:50,510][0m Trial 24 finished with value: 0.03824012898481809 and parameters: {'observation_period_num': 6, 'train_rates': 0.9472238486646066, 'learning_rate': 0.0006259160799899691, 'batch_size': 72, 'step_size': 3, 'gamma': 0.7886649963111999}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:48:15,280][0m Trial 25 finished with value: 0.0956126595639689 and parameters: {'observation_period_num': 63, 'train_rates': 0.8913509307359679, 'learning_rate': 0.00023001333407123166, 'batch_size': 39, 'step_size': 5, 'gamma': 0.9375596518998748}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:49:05,795][0m Trial 26 finished with value: 0.0674926591262823 and parameters: {'observation_period_num': 38, 'train_rates': 0.8556089673318094, 'learning_rate': 0.00012302678009184327, 'batch_size': 123, 'step_size': 4, 'gamma': 0.7711670536422846}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:51:40,690][0m Trial 27 finished with value: 0.04411510782132686 and parameters: {'observation_period_num': 28, 'train_rates': 0.9515583923094617, 'learning_rate': 0.0005649969572198911, 'batch_size': 38, 'step_size': 2, 'gamma': 0.8477940820479837}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:52:26,204][0m Trial 28 finished with value: 0.7002224280864378 and parameters: {'observation_period_num': 51, 'train_rates': 0.8367429340244761, 'learning_rate': 6.6127705626738335e-06, 'batch_size': 162, 'step_size': 2, 'gamma': 0.8136133981664038}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:54:07,704][0m Trial 29 finished with value: 0.09987707436084747 and parameters: {'observation_period_num': 191, 'train_rates': 0.9846741955816076, 'learning_rate': 9.654400790879594e-05, 'batch_size': 57, 'step_size': 4, 'gamma': 0.934616405542397}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:55:15,205][0m Trial 30 finished with value: 0.09882868081331253 and parameters: {'observation_period_num': 76, 'train_rates': 0.9891945321174661, 'learning_rate': 4.1505684681917834e-05, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8948478330027897}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:56:41,247][0m Trial 31 finished with value: 0.035089724074453994 and parameters: {'observation_period_num': 9, 'train_rates': 0.9427550260003242, 'learning_rate': 0.0006773081008106985, 'batch_size': 70, 'step_size': 3, 'gamma': 0.7928239500668747}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 19:59:48,967][0m Trial 32 finished with value: 0.04076555371284485 and parameters: {'observation_period_num': 24, 'train_rates': 0.9367520931913681, 'learning_rate': 0.0006453201069538738, 'batch_size': 31, 'step_size': 2, 'gamma': 0.7704408752160835}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 20:01:34,275][0m Trial 33 finished with value: 0.04873525918714258 and parameters: {'observation_period_num': 46, 'train_rates': 0.8963921300056223, 'learning_rate': 0.0002888354536123616, 'batch_size': 57, 'step_size': 5, 'gamma': 0.7889961175495104}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 20:03:50,638][0m Trial 34 finished with value: 0.03304667253461149 and parameters: {'observation_period_num': 6, 'train_rates': 0.9666721581620716, 'learning_rate': 0.0004744676997747434, 'batch_size': 44, 'step_size': 3, 'gamma': 0.8289743893765271}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 20:05:58,510][0m Trial 35 finished with value: 0.08403422338988191 and parameters: {'observation_period_num': 72, 'train_rates': 0.967895164340841, 'learning_rate': 0.000503328470498273, 'batch_size': 46, 'step_size': 6, 'gamma': 0.8152444904437607}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 20:06:32,227][0m Trial 36 finished with value: 0.05748302489519119 and parameters: {'observation_period_num': 21, 'train_rates': 0.9635018655978328, 'learning_rate': 0.0007360275296730729, 'batch_size': 237, 'step_size': 3, 'gamma': 0.8332725640998295}. Best is trial 23 with value: 0.02977675926394579.[0m
[32m[I 2025-01-07 20:07:57,617][0m Trial 37 finished with value: 0.02664862187376237 and parameters: {'observation_period_num': 6, 'train_rates': 0.9287370973886402, 'learning_rate': 0.0009989030292173794, 'batch_size': 69, 'step_size': 6, 'gamma': 0.7993059181398754}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:10:40,526][0m Trial 38 finished with value: 0.1979590897164111 and parameters: {'observation_period_num': 60, 'train_rates': 0.7215818329113772, 'learning_rate': 0.0004074167990438434, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8296381578947204}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:16:15,929][0m Trial 39 finished with value: 0.09743439715395692 and parameters: {'observation_period_num': 99, 'train_rates': 0.8373196908855344, 'learning_rate': 0.0009366748981260139, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8068996101214321}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:17:24,289][0m Trial 40 finished with value: 0.15953955752550117 and parameters: {'observation_period_num': 47, 'train_rates': 0.7863192794182089, 'learning_rate': 2.5170114903935045e-05, 'batch_size': 95, 'step_size': 5, 'gamma': 0.8540139292869108}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:19:04,834][0m Trial 41 finished with value: 0.027339981088937797 and parameters: {'observation_period_num': 6, 'train_rates': 0.9312091409298331, 'learning_rate': 0.000719160842902578, 'batch_size': 64, 'step_size': 4, 'gamma': 0.7960199415155385}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:21:09,506][0m Trial 42 finished with value: 0.04179669405505838 and parameters: {'observation_period_num': 25, 'train_rates': 0.9281481524189433, 'learning_rate': 0.0002559646558772305, 'batch_size': 49, 'step_size': 4, 'gamma': 0.7632788226229505}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:22:44,027][0m Trial 43 finished with value: 0.03275524380993336 and parameters: {'observation_period_num': 5, 'train_rates': 0.9681869990209384, 'learning_rate': 0.000450837975285635, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8015917568460356}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:23:47,581][0m Trial 44 finished with value: 0.04208991676568985 and parameters: {'observation_period_num': 5, 'train_rates': 0.9725411655564702, 'learning_rate': 0.0004560312799119876, 'batch_size': 117, 'step_size': 7, 'gamma': 0.8008541944931077}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:24:44,214][0m Trial 45 finished with value: 0.16035358922667162 and parameters: {'observation_period_num': 18, 'train_rates': 0.6631442473378909, 'learning_rate': 8.68790800117886e-05, 'batch_size': 102, 'step_size': 6, 'gamma': 0.8213338014673405}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:25:53,506][0m Trial 46 finished with value: 0.04799729984785829 and parameters: {'observation_period_num': 29, 'train_rates': 0.8709113852404543, 'learning_rate': 0.0007106191309849316, 'batch_size': 83, 'step_size': 5, 'gamma': 0.8360742895391208}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:27:24,714][0m Trial 47 finished with value: 0.05116558535847553 and parameters: {'observation_period_num': 43, 'train_rates': 0.92617333654196, 'learning_rate': 0.0003581651195789009, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8091900866964644}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:28:23,813][0m Trial 48 finished with value: 0.05117122828960419 and parameters: {'observation_period_num': 6, 'train_rates': 0.9719844523752439, 'learning_rate': 0.0001660480760672641, 'batch_size': 128, 'step_size': 8, 'gamma': 0.7843044460842228}. Best is trial 37 with value: 0.02664862187376237.[0m
[32m[I 2025-01-07 20:29:58,522][0m Trial 49 finished with value: 0.032617757529754876 and parameters: {'observation_period_num': 16, 'train_rates': 0.9034380488532228, 'learning_rate': 0.00024235208678083148, 'batch_size': 62, 'step_size': 5, 'gamma': 0.7980371775772506}. Best is trial 37 with value: 0.02664862187376237.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.9117286556118189, 'learning_rate': 0.0009876273611041503, 'batch_size': 222, 'step_size': 13, 'gamma': 0.7995218455363069}
Epoch 1/300, trend Loss: 0.8555 | 0.2851
Epoch 2/300, trend Loss: 0.2265 | 0.1279
Epoch 3/300, trend Loss: 0.1512 | 0.1146
Epoch 4/300, trend Loss: 0.1476 | 0.1128
Epoch 5/300, trend Loss: 0.1498 | 0.1684
Epoch 6/300, trend Loss: 0.1477 | 0.0832
Epoch 7/300, trend Loss: 0.1557 | 0.0819
Epoch 8/300, trend Loss: 0.2077 | 0.1195
Epoch 9/300, trend Loss: 0.1991 | 0.1096
Epoch 10/300, trend Loss: 0.1808 | 0.1996
Epoch 11/300, trend Loss: 0.2279 | 0.0971
Epoch 12/300, trend Loss: 0.1940 | 0.1489
Epoch 13/300, trend Loss: 0.1755 | 0.1052
Epoch 14/300, trend Loss: 0.1879 | 0.0967
Epoch 15/300, trend Loss: 0.1541 | 0.0920
Epoch 16/300, trend Loss: 0.1439 | 0.0672
Epoch 17/300, trend Loss: 0.1422 | 0.0704
Epoch 18/300, trend Loss: 0.1465 | 0.0906
Epoch 19/300, trend Loss: 0.1570 | 0.1368
Epoch 20/300, trend Loss: 0.1453 | 0.0785
Epoch 21/300, trend Loss: 0.1283 | 0.0873
Epoch 22/300, trend Loss: 0.1176 | 0.0662
Epoch 23/300, trend Loss: 0.1168 | 0.0576
Epoch 24/300, trend Loss: 0.1066 | 0.0529
Epoch 25/300, trend Loss: 0.1022 | 0.0486
Epoch 26/300, trend Loss: 0.0973 | 0.0479
Epoch 27/300, trend Loss: 0.0962 | 0.0486
Epoch 28/300, trend Loss: 0.0936 | 0.0468
Epoch 29/300, trend Loss: 0.0930 | 0.0460
Epoch 30/300, trend Loss: 0.0896 | 0.0432
Epoch 31/300, trend Loss: 0.0889 | 0.0428
Epoch 32/300, trend Loss: 0.0873 | 0.0412
Epoch 33/300, trend Loss: 0.0873 | 0.0434
Epoch 34/300, trend Loss: 0.0860 | 0.0411
Epoch 35/300, trend Loss: 0.0861 | 0.0436
Epoch 36/300, trend Loss: 0.0860 | 0.0433
Epoch 37/300, trend Loss: 0.0875 | 0.0430
Epoch 38/300, trend Loss: 0.0844 | 0.0424
Epoch 39/300, trend Loss: 0.0827 | 0.0409
Epoch 40/300, trend Loss: 0.0819 | 0.0392
Epoch 41/300, trend Loss: 0.0811 | 0.0398
Epoch 42/300, trend Loss: 0.0806 | 0.0387
Epoch 43/300, trend Loss: 0.0801 | 0.0390
Epoch 44/300, trend Loss: 0.0797 | 0.0384
Epoch 45/300, trend Loss: 0.0794 | 0.0384
Epoch 46/300, trend Loss: 0.0790 | 0.0379
Epoch 47/300, trend Loss: 0.0786 | 0.0376
Epoch 48/300, trend Loss: 0.0782 | 0.0371
Epoch 49/300, trend Loss: 0.0778 | 0.0369
Epoch 50/300, trend Loss: 0.0774 | 0.0367
Epoch 51/300, trend Loss: 0.0772 | 0.0365
Epoch 52/300, trend Loss: 0.0769 | 0.0364
Epoch 53/300, trend Loss: 0.0767 | 0.0361
Epoch 54/300, trend Loss: 0.0766 | 0.0360
Epoch 55/300, trend Loss: 0.0764 | 0.0358
Epoch 56/300, trend Loss: 0.0762 | 0.0357
Epoch 57/300, trend Loss: 0.0759 | 0.0355
Epoch 58/300, trend Loss: 0.0757 | 0.0352
Epoch 59/300, trend Loss: 0.0754 | 0.0350
Epoch 60/300, trend Loss: 0.0753 | 0.0348
Epoch 61/300, trend Loss: 0.0755 | 0.0352
Epoch 62/300, trend Loss: 0.0759 | 0.0359
Epoch 63/300, trend Loss: 0.0765 | 0.0353
Epoch 64/300, trend Loss: 0.0760 | 0.0344
Epoch 65/300, trend Loss: 0.0747 | 0.0341
Epoch 66/300, trend Loss: 0.0740 | 0.0340
Epoch 67/300, trend Loss: 0.0738 | 0.0338
Epoch 68/300, trend Loss: 0.0736 | 0.0337
Epoch 69/300, trend Loss: 0.0734 | 0.0335
Epoch 70/300, trend Loss: 0.0732 | 0.0333
Epoch 71/300, trend Loss: 0.0730 | 0.0332
Epoch 72/300, trend Loss: 0.0728 | 0.0330
Epoch 73/300, trend Loss: 0.0726 | 0.0329
Epoch 74/300, trend Loss: 0.0725 | 0.0328
Epoch 75/300, trend Loss: 0.0723 | 0.0327
Epoch 76/300, trend Loss: 0.0722 | 0.0326
Epoch 77/300, trend Loss: 0.0720 | 0.0325
Epoch 78/300, trend Loss: 0.0719 | 0.0324
Epoch 79/300, trend Loss: 0.0718 | 0.0323
Epoch 80/300, trend Loss: 0.0717 | 0.0322
Epoch 81/300, trend Loss: 0.0716 | 0.0321
Epoch 82/300, trend Loss: 0.0715 | 0.0321
Epoch 83/300, trend Loss: 0.0713 | 0.0321
Epoch 84/300, trend Loss: 0.0711 | 0.0320
Epoch 85/300, trend Loss: 0.0710 | 0.0320
Epoch 86/300, trend Loss: 0.0709 | 0.0319
Epoch 87/300, trend Loss: 0.0708 | 0.0318
Epoch 88/300, trend Loss: 0.0707 | 0.0317
Epoch 89/300, trend Loss: 0.0706 | 0.0317
Epoch 90/300, trend Loss: 0.0705 | 0.0317
Epoch 91/300, trend Loss: 0.0704 | 0.0317
Epoch 92/300, trend Loss: 0.0703 | 0.0316
Epoch 93/300, trend Loss: 0.0703 | 0.0315
Epoch 94/300, trend Loss: 0.0702 | 0.0315
Epoch 95/300, trend Loss: 0.0701 | 0.0315
Epoch 96/300, trend Loss: 0.0700 | 0.0315
Epoch 97/300, trend Loss: 0.0700 | 0.0314
Epoch 98/300, trend Loss: 0.0699 | 0.0314
Epoch 99/300, trend Loss: 0.0698 | 0.0314
Epoch 100/300, trend Loss: 0.0698 | 0.0314
Epoch 101/300, trend Loss: 0.0698 | 0.0313
Epoch 102/300, trend Loss: 0.0697 | 0.0313
Epoch 103/300, trend Loss: 0.0696 | 0.0312
Epoch 104/300, trend Loss: 0.0696 | 0.0311
Epoch 105/300, trend Loss: 0.0695 | 0.0310
Epoch 106/300, trend Loss: 0.0695 | 0.0309
Epoch 107/300, trend Loss: 0.0694 | 0.0308
Epoch 108/300, trend Loss: 0.0694 | 0.0308
Epoch 109/300, trend Loss: 0.0693 | 0.0307
Epoch 110/300, trend Loss: 0.0693 | 0.0307
Epoch 111/300, trend Loss: 0.0692 | 0.0307
Epoch 112/300, trend Loss: 0.0692 | 0.0307
Epoch 113/300, trend Loss: 0.0692 | 0.0306
Epoch 114/300, trend Loss: 0.0691 | 0.0306
Epoch 115/300, trend Loss: 0.0691 | 0.0305
Epoch 116/300, trend Loss: 0.0691 | 0.0304
Epoch 117/300, trend Loss: 0.0690 | 0.0304
Epoch 118/300, trend Loss: 0.0690 | 0.0304
Epoch 119/300, trend Loss: 0.0690 | 0.0303
Epoch 120/300, trend Loss: 0.0689 | 0.0303
Epoch 121/300, trend Loss: 0.0689 | 0.0303
Epoch 122/300, trend Loss: 0.0689 | 0.0302
Epoch 123/300, trend Loss: 0.0688 | 0.0302
Epoch 124/300, trend Loss: 0.0688 | 0.0302
Epoch 125/300, trend Loss: 0.0688 | 0.0301
Epoch 126/300, trend Loss: 0.0688 | 0.0301
Epoch 127/300, trend Loss: 0.0687 | 0.0301
Epoch 128/300, trend Loss: 0.0687 | 0.0301
Epoch 129/300, trend Loss: 0.0687 | 0.0300
Epoch 130/300, trend Loss: 0.0687 | 0.0300
Epoch 131/300, trend Loss: 0.0686 | 0.0300
Epoch 132/300, trend Loss: 0.0686 | 0.0299
Epoch 133/300, trend Loss: 0.0686 | 0.0299
Epoch 134/300, trend Loss: 0.0686 | 0.0299
Epoch 135/300, trend Loss: 0.0686 | 0.0299
Epoch 136/300, trend Loss: 0.0686 | 0.0299
Epoch 137/300, trend Loss: 0.0685 | 0.0298
Epoch 138/300, trend Loss: 0.0685 | 0.0298
Epoch 139/300, trend Loss: 0.0685 | 0.0298
Epoch 140/300, trend Loss: 0.0685 | 0.0298
Epoch 141/300, trend Loss: 0.0685 | 0.0298
Epoch 142/300, trend Loss: 0.0684 | 0.0297
Epoch 143/300, trend Loss: 0.0684 | 0.0297
Epoch 144/300, trend Loss: 0.0684 | 0.0297
Epoch 145/300, trend Loss: 0.0684 | 0.0297
Epoch 146/300, trend Loss: 0.0684 | 0.0297
Epoch 147/300, trend Loss: 0.0684 | 0.0297
Epoch 148/300, trend Loss: 0.0684 | 0.0297
Epoch 149/300, trend Loss: 0.0684 | 0.0296
Epoch 150/300, trend Loss: 0.0683 | 0.0296
Epoch 151/300, trend Loss: 0.0683 | 0.0296
Epoch 152/300, trend Loss: 0.0683 | 0.0296
Epoch 153/300, trend Loss: 0.0683 | 0.0296
Epoch 154/300, trend Loss: 0.0683 | 0.0296
Epoch 155/300, trend Loss: 0.0683 | 0.0296
Epoch 156/300, trend Loss: 0.0683 | 0.0295
Epoch 157/300, trend Loss: 0.0683 | 0.0295
Epoch 158/300, trend Loss: 0.0683 | 0.0295
Epoch 159/300, trend Loss: 0.0682 | 0.0295
Epoch 160/300, trend Loss: 0.0682 | 0.0295
Epoch 161/300, trend Loss: 0.0682 | 0.0295
Epoch 162/300, trend Loss: 0.0682 | 0.0295
Epoch 163/300, trend Loss: 0.0682 | 0.0295
Epoch 164/300, trend Loss: 0.0682 | 0.0295
Epoch 165/300, trend Loss: 0.0682 | 0.0295
Epoch 166/300, trend Loss: 0.0682 | 0.0295
Epoch 167/300, trend Loss: 0.0682 | 0.0294
Epoch 168/300, trend Loss: 0.0682 | 0.0294
Epoch 169/300, trend Loss: 0.0682 | 0.0294
Epoch 170/300, trend Loss: 0.0682 | 0.0294
Epoch 171/300, trend Loss: 0.0682 | 0.0294
Epoch 172/300, trend Loss: 0.0681 | 0.0294
Epoch 173/300, trend Loss: 0.0681 | 0.0294
Epoch 174/300, trend Loss: 0.0681 | 0.0294
Epoch 175/300, trend Loss: 0.0681 | 0.0294
Epoch 176/300, trend Loss: 0.0681 | 0.0294
Epoch 177/300, trend Loss: 0.0681 | 0.0294
Epoch 178/300, trend Loss: 0.0681 | 0.0294
Epoch 179/300, trend Loss: 0.0681 | 0.0294
Epoch 180/300, trend Loss: 0.0681 | 0.0294
Epoch 181/300, trend Loss: 0.0681 | 0.0294
Epoch 182/300, trend Loss: 0.0681 | 0.0293
Epoch 183/300, trend Loss: 0.0681 | 0.0293
Epoch 184/300, trend Loss: 0.0681 | 0.0293
Epoch 185/300, trend Loss: 0.0681 | 0.0293
Epoch 186/300, trend Loss: 0.0681 | 0.0293
Epoch 187/300, trend Loss: 0.0681 | 0.0293
Epoch 188/300, trend Loss: 0.0681 | 0.0293
Epoch 189/300, trend Loss: 0.0681 | 0.0293
Epoch 190/300, trend Loss: 0.0681 | 0.0293
Epoch 191/300, trend Loss: 0.0681 | 0.0293
Epoch 192/300, trend Loss: 0.0681 | 0.0293
Epoch 193/300, trend Loss: 0.0680 | 0.0293
Epoch 194/300, trend Loss: 0.0680 | 0.0293
Epoch 195/300, trend Loss: 0.0680 | 0.0293
Epoch 196/300, trend Loss: 0.0680 | 0.0293
Epoch 197/300, trend Loss: 0.0680 | 0.0293
Epoch 198/300, trend Loss: 0.0680 | 0.0293
Epoch 199/300, trend Loss: 0.0680 | 0.0293
Epoch 200/300, trend Loss: 0.0680 | 0.0293
Epoch 201/300, trend Loss: 0.0680 | 0.0293
Epoch 202/300, trend Loss: 0.0680 | 0.0293
Epoch 203/300, trend Loss: 0.0680 | 0.0293
Epoch 204/300, trend Loss: 0.0680 | 0.0293
Epoch 205/300, trend Loss: 0.0680 | 0.0293
Epoch 206/300, trend Loss: 0.0680 | 0.0293
Epoch 207/300, trend Loss: 0.0680 | 0.0293
Epoch 208/300, trend Loss: 0.0680 | 0.0293
Epoch 209/300, trend Loss: 0.0680 | 0.0293
Epoch 210/300, trend Loss: 0.0680 | 0.0293
Epoch 211/300, trend Loss: 0.0680 | 0.0292
Epoch 212/300, trend Loss: 0.0680 | 0.0292
Epoch 213/300, trend Loss: 0.0680 | 0.0292
Epoch 214/300, trend Loss: 0.0680 | 0.0292
Epoch 215/300, trend Loss: 0.0680 | 0.0292
Epoch 216/300, trend Loss: 0.0680 | 0.0292
Epoch 217/300, trend Loss: 0.0680 | 0.0292
Epoch 218/300, trend Loss: 0.0680 | 0.0292
Epoch 219/300, trend Loss: 0.0680 | 0.0292
Epoch 220/300, trend Loss: 0.0680 | 0.0292
Epoch 221/300, trend Loss: 0.0680 | 0.0292
Epoch 222/300, trend Loss: 0.0680 | 0.0292
Epoch 223/300, trend Loss: 0.0680 | 0.0292
Epoch 224/300, trend Loss: 0.0680 | 0.0292
Epoch 225/300, trend Loss: 0.0680 | 0.0292
Epoch 226/300, trend Loss: 0.0680 | 0.0292
Epoch 227/300, trend Loss: 0.0680 | 0.0292
Epoch 228/300, trend Loss: 0.0680 | 0.0292
Epoch 229/300, trend Loss: 0.0680 | 0.0292
Epoch 230/300, trend Loss: 0.0680 | 0.0292
Epoch 231/300, trend Loss: 0.0680 | 0.0292
Epoch 232/300, trend Loss: 0.0680 | 0.0292
Epoch 233/300, trend Loss: 0.0680 | 0.0292
Epoch 234/300, trend Loss: 0.0680 | 0.0292
Epoch 235/300, trend Loss: 0.0680 | 0.0292
Epoch 236/300, trend Loss: 0.0680 | 0.0292
Epoch 237/300, trend Loss: 0.0680 | 0.0292
Epoch 238/300, trend Loss: 0.0680 | 0.0292
Epoch 239/300, trend Loss: 0.0680 | 0.0292
Epoch 240/300, trend Loss: 0.0680 | 0.0292
Epoch 241/300, trend Loss: 0.0680 | 0.0292
Epoch 242/300, trend Loss: 0.0680 | 0.0292
Epoch 243/300, trend Loss: 0.0680 | 0.0292
Epoch 244/300, trend Loss: 0.0680 | 0.0292
Epoch 245/300, trend Loss: 0.0680 | 0.0292
Epoch 246/300, trend Loss: 0.0680 | 0.0292
Epoch 247/300, trend Loss: 0.0680 | 0.0292
Epoch 248/300, trend Loss: 0.0680 | 0.0292
Epoch 249/300, trend Loss: 0.0680 | 0.0292
Epoch 250/300, trend Loss: 0.0680 | 0.0292
Epoch 251/300, trend Loss: 0.0680 | 0.0292
Epoch 252/300, trend Loss: 0.0680 | 0.0292
Epoch 253/300, trend Loss: 0.0680 | 0.0292
Epoch 254/300, trend Loss: 0.0680 | 0.0292
Epoch 255/300, trend Loss: 0.0680 | 0.0292
Epoch 256/300, trend Loss: 0.0680 | 0.0292
Epoch 257/300, trend Loss: 0.0680 | 0.0292
Epoch 258/300, trend Loss: 0.0680 | 0.0292
Epoch 259/300, trend Loss: 0.0680 | 0.0292
Epoch 260/300, trend Loss: 0.0680 | 0.0292
Epoch 261/300, trend Loss: 0.0680 | 0.0292
Epoch 262/300, trend Loss: 0.0680 | 0.0292
Epoch 263/300, trend Loss: 0.0680 | 0.0292
Epoch 264/300, trend Loss: 0.0680 | 0.0292
Epoch 265/300, trend Loss: 0.0680 | 0.0292
Epoch 266/300, trend Loss: 0.0680 | 0.0292
Epoch 267/300, trend Loss: 0.0680 | 0.0292
Epoch 268/300, trend Loss: 0.0680 | 0.0292
Epoch 269/300, trend Loss: 0.0680 | 0.0292
Epoch 270/300, trend Loss: 0.0680 | 0.0292
Epoch 271/300, trend Loss: 0.0680 | 0.0292
Epoch 272/300, trend Loss: 0.0680 | 0.0292
Epoch 273/300, trend Loss: 0.0680 | 0.0292
Epoch 274/300, trend Loss: 0.0680 | 0.0292
Epoch 275/300, trend Loss: 0.0680 | 0.0292
Epoch 276/300, trend Loss: 0.0680 | 0.0292
Epoch 277/300, trend Loss: 0.0680 | 0.0292
Epoch 278/300, trend Loss: 0.0680 | 0.0292
Epoch 279/300, trend Loss: 0.0679 | 0.0292
Epoch 280/300, trend Loss: 0.0679 | 0.0292
Epoch 281/300, trend Loss: 0.0679 | 0.0292
Epoch 282/300, trend Loss: 0.0679 | 0.0292
Epoch 283/300, trend Loss: 0.0679 | 0.0292
Epoch 284/300, trend Loss: 0.0679 | 0.0292
Epoch 285/300, trend Loss: 0.0679 | 0.0292
Epoch 286/300, trend Loss: 0.0679 | 0.0292
Epoch 287/300, trend Loss: 0.0679 | 0.0292
Epoch 288/300, trend Loss: 0.0679 | 0.0292
Epoch 289/300, trend Loss: 0.0679 | 0.0292
Epoch 290/300, trend Loss: 0.0679 | 0.0292
Epoch 291/300, trend Loss: 0.0679 | 0.0292
Epoch 292/300, trend Loss: 0.0679 | 0.0292
Epoch 293/300, trend Loss: 0.0679 | 0.0292
Epoch 294/300, trend Loss: 0.0679 | 0.0292
Epoch 295/300, trend Loss: 0.0679 | 0.0292
Epoch 296/300, trend Loss: 0.0679 | 0.0292
Epoch 297/300, trend Loss: 0.0679 | 0.0292
Epoch 298/300, trend Loss: 0.0679 | 0.0292
Epoch 299/300, trend Loss: 0.0679 | 0.0292
Epoch 300/300, trend Loss: 0.0679 | 0.0292
Training seasonal_0 component with params: {'observation_period_num': 21, 'train_rates': 0.9684373263335981, 'learning_rate': 0.00033531135729907805, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8818183263617073}
Epoch 1/300, seasonal_0 Loss: 0.1908 | 0.1121
Epoch 2/300, seasonal_0 Loss: 0.1268 | 0.1011
Epoch 3/300, seasonal_0 Loss: 0.1155 | 0.0963
Epoch 4/300, seasonal_0 Loss: 0.1103 | 0.0888
Epoch 5/300, seasonal_0 Loss: 0.1069 | 0.0863
Epoch 6/300, seasonal_0 Loss: 0.1045 | 0.0846
Epoch 7/300, seasonal_0 Loss: 0.1008 | 0.0817
Epoch 8/300, seasonal_0 Loss: 0.0958 | 0.0765
Epoch 9/300, seasonal_0 Loss: 0.0925 | 0.0737
Epoch 10/300, seasonal_0 Loss: 0.0876 | 0.0693
Epoch 11/300, seasonal_0 Loss: 0.0828 | 0.0643
Epoch 12/300, seasonal_0 Loss: 0.0790 | 0.0599
Epoch 13/300, seasonal_0 Loss: 0.0759 | 0.0567
Epoch 14/300, seasonal_0 Loss: 0.0728 | 0.0556
Epoch 15/300, seasonal_0 Loss: 0.0697 | 0.0543
Epoch 16/300, seasonal_0 Loss: 0.0666 | 0.0544
Epoch 17/300, seasonal_0 Loss: 0.0644 | 0.0533
Epoch 18/300, seasonal_0 Loss: 0.0620 | 0.0514
Epoch 19/300, seasonal_0 Loss: 0.0607 | 0.0509
Epoch 20/300, seasonal_0 Loss: 0.0582 | 0.0485
Epoch 21/300, seasonal_0 Loss: 0.0577 | 0.0493
Epoch 22/300, seasonal_0 Loss: 0.0555 | 0.0500
Epoch 23/300, seasonal_0 Loss: 0.0553 | 0.0504
Epoch 24/300, seasonal_0 Loss: 0.0527 | 0.0514
Epoch 25/300, seasonal_0 Loss: 0.0519 | 0.0511
Epoch 26/300, seasonal_0 Loss: 0.0508 | 0.0511
Epoch 27/300, seasonal_0 Loss: 0.0488 | 0.0476
Epoch 28/300, seasonal_0 Loss: 0.0486 | 0.0491
Epoch 29/300, seasonal_0 Loss: 0.0472 | 0.0507
Epoch 30/300, seasonal_0 Loss: 0.0473 | 0.0503
Epoch 31/300, seasonal_0 Loss: 0.0440 | 0.0462
Epoch 32/300, seasonal_0 Loss: 0.0410 | 0.0470
Epoch 33/300, seasonal_0 Loss: 0.0446 | 0.0469
Epoch 34/300, seasonal_0 Loss: 0.0406 | 0.0447
Epoch 35/300, seasonal_0 Loss: 0.0386 | 0.0491
Epoch 36/300, seasonal_0 Loss: 0.0385 | 0.0472
Epoch 37/300, seasonal_0 Loss: 0.0362 | 0.0473
Epoch 38/300, seasonal_0 Loss: 0.0370 | 0.0470
Epoch 39/300, seasonal_0 Loss: 0.0364 | 0.0472
Epoch 40/300, seasonal_0 Loss: 0.0343 | 0.0480
Epoch 41/300, seasonal_0 Loss: 0.0343 | 0.0461
Epoch 42/300, seasonal_0 Loss: 0.0328 | 0.0472
Epoch 43/300, seasonal_0 Loss: 0.0320 | 0.0484
Epoch 44/300, seasonal_0 Loss: 0.0316 | 0.0484
Epoch 45/300, seasonal_0 Loss: 0.0311 | 0.0487
Epoch 46/300, seasonal_0 Loss: 0.0305 | 0.0486
Epoch 47/300, seasonal_0 Loss: 0.0299 | 0.0490
Epoch 48/300, seasonal_0 Loss: 0.0295 | 0.0490
Epoch 49/300, seasonal_0 Loss: 0.0292 | 0.0491
Epoch 50/300, seasonal_0 Loss: 0.0288 | 0.0498
Epoch 51/300, seasonal_0 Loss: 0.0285 | 0.0501
Epoch 52/300, seasonal_0 Loss: 0.0282 | 0.0504
Epoch 53/300, seasonal_0 Loss: 0.0281 | 0.0503
Epoch 54/300, seasonal_0 Loss: 0.0277 | 0.0520
Epoch 55/300, seasonal_0 Loss: 0.0279 | 0.0510
Epoch 56/300, seasonal_0 Loss: 0.0276 | 0.0521
Epoch 57/300, seasonal_0 Loss: 0.0272 | 0.0519
Epoch 58/300, seasonal_0 Loss: 0.0271 | 0.0514
Epoch 59/300, seasonal_0 Loss: 0.0270 | 0.0520
Epoch 60/300, seasonal_0 Loss: 0.0268 | 0.0495
Epoch 61/300, seasonal_0 Loss: 0.0268 | 0.0504
Epoch 62/300, seasonal_0 Loss: 0.0269 | 0.0502
Epoch 63/300, seasonal_0 Loss: 0.0262 | 0.0500
Epoch 64/300, seasonal_0 Loss: 0.0260 | 0.0500
Epoch 65/300, seasonal_0 Loss: 0.0257 | 0.0504
Epoch 66/300, seasonal_0 Loss: 0.0254 | 0.0506
Epoch 67/300, seasonal_0 Loss: 0.0252 | 0.0505
Epoch 68/300, seasonal_0 Loss: 0.0249 | 0.0506
Epoch 69/300, seasonal_0 Loss: 0.0248 | 0.0506
Epoch 70/300, seasonal_0 Loss: 0.0246 | 0.0501
Epoch 71/300, seasonal_0 Loss: 0.0244 | 0.0507
Epoch 72/300, seasonal_0 Loss: 0.0243 | 0.0506
Epoch 73/300, seasonal_0 Loss: 0.0242 | 0.0509
Epoch 74/300, seasonal_0 Loss: 0.0240 | 0.0510
Epoch 75/300, seasonal_0 Loss: 0.0239 | 0.0508
Epoch 76/300, seasonal_0 Loss: 0.0238 | 0.0513
Epoch 77/300, seasonal_0 Loss: 0.0237 | 0.0510
Epoch 78/300, seasonal_0 Loss: 0.0235 | 0.0511
Epoch 79/300, seasonal_0 Loss: 0.0234 | 0.0518
Epoch 80/300, seasonal_0 Loss: 0.0233 | 0.0515
Epoch 81/300, seasonal_0 Loss: 0.0232 | 0.0524
Epoch 82/300, seasonal_0 Loss: 0.0232 | 0.0516
Epoch 83/300, seasonal_0 Loss: 0.0230 | 0.0522
Epoch 84/300, seasonal_0 Loss: 0.0230 | 0.0515
Epoch 85/300, seasonal_0 Loss: 0.0228 | 0.0518
Epoch 86/300, seasonal_0 Loss: 0.0227 | 0.0514
Epoch 87/300, seasonal_0 Loss: 0.0227 | 0.0521
Epoch 88/300, seasonal_0 Loss: 0.0226 | 0.0517
Epoch 89/300, seasonal_0 Loss: 0.0225 | 0.0520
Epoch 90/300, seasonal_0 Loss: 0.0224 | 0.0516
Epoch 91/300, seasonal_0 Loss: 0.0224 | 0.0521
Epoch 92/300, seasonal_0 Loss: 0.0224 | 0.0517
Epoch 93/300, seasonal_0 Loss: 0.0223 | 0.0517
Epoch 94/300, seasonal_0 Loss: 0.0222 | 0.0514
Epoch 95/300, seasonal_0 Loss: 0.0221 | 0.0518
Epoch 96/300, seasonal_0 Loss: 0.0221 | 0.0512
Epoch 97/300, seasonal_0 Loss: 0.0221 | 0.0516
Epoch 98/300, seasonal_0 Loss: 0.0220 | 0.0516
Epoch 99/300, seasonal_0 Loss: 0.0219 | 0.0517
Epoch 100/300, seasonal_0 Loss: 0.0219 | 0.0520
Epoch 101/300, seasonal_0 Loss: 0.0218 | 0.0520
Epoch 102/300, seasonal_0 Loss: 0.0217 | 0.0525
Epoch 103/300, seasonal_0 Loss: 0.0217 | 0.0521
Epoch 104/300, seasonal_0 Loss: 0.0216 | 0.0524
Epoch 105/300, seasonal_0 Loss: 0.0216 | 0.0522
Epoch 106/300, seasonal_0 Loss: 0.0215 | 0.0528
Epoch 107/300, seasonal_0 Loss: 0.0215 | 0.0522
Epoch 108/300, seasonal_0 Loss: 0.0214 | 0.0524
Epoch 109/300, seasonal_0 Loss: 0.0214 | 0.0520
Epoch 110/300, seasonal_0 Loss: 0.0213 | 0.0525
Epoch 111/300, seasonal_0 Loss: 0.0213 | 0.0520
Epoch 112/300, seasonal_0 Loss: 0.0212 | 0.0523
Epoch 113/300, seasonal_0 Loss: 0.0212 | 0.0520
Epoch 114/300, seasonal_0 Loss: 0.0211 | 0.0524
Epoch 115/300, seasonal_0 Loss: 0.0211 | 0.0521
Epoch 116/300, seasonal_0 Loss: 0.0210 | 0.0521
Epoch 117/300, seasonal_0 Loss: 0.0210 | 0.0523
Epoch 118/300, seasonal_0 Loss: 0.0210 | 0.0521
Epoch 119/300, seasonal_0 Loss: 0.0209 | 0.0525
Epoch 120/300, seasonal_0 Loss: 0.0209 | 0.0522
Epoch 121/300, seasonal_0 Loss: 0.0208 | 0.0522
Epoch 122/300, seasonal_0 Loss: 0.0208 | 0.0524
Epoch 123/300, seasonal_0 Loss: 0.0208 | 0.0521
Epoch 124/300, seasonal_0 Loss: 0.0207 | 0.0524
Epoch 125/300, seasonal_0 Loss: 0.0207 | 0.0522
Epoch 126/300, seasonal_0 Loss: 0.0207 | 0.0523
Epoch 127/300, seasonal_0 Loss: 0.0207 | 0.0524
Epoch 128/300, seasonal_0 Loss: 0.0206 | 0.0522
Epoch 129/300, seasonal_0 Loss: 0.0206 | 0.0524
Epoch 130/300, seasonal_0 Loss: 0.0206 | 0.0523
Epoch 131/300, seasonal_0 Loss: 0.0205 | 0.0523
Epoch 132/300, seasonal_0 Loss: 0.0205 | 0.0525
Epoch 133/300, seasonal_0 Loss: 0.0205 | 0.0523
Epoch 134/300, seasonal_0 Loss: 0.0205 | 0.0524
Epoch 135/300, seasonal_0 Loss: 0.0204 | 0.0525
Epoch 136/300, seasonal_0 Loss: 0.0204 | 0.0524
Epoch 137/300, seasonal_0 Loss: 0.0204 | 0.0525
Epoch 138/300, seasonal_0 Loss: 0.0204 | 0.0525
Epoch 139/300, seasonal_0 Loss: 0.0204 | 0.0524
Epoch 140/300, seasonal_0 Loss: 0.0203 | 0.0526
Epoch 141/300, seasonal_0 Loss: 0.0203 | 0.0526
Epoch 142/300, seasonal_0 Loss: 0.0203 | 0.0526
Epoch 143/300, seasonal_0 Loss: 0.0203 | 0.0528
Epoch 144/300, seasonal_0 Loss: 0.0203 | 0.0527
Epoch 145/300, seasonal_0 Loss: 0.0203 | 0.0528
Epoch 146/300, seasonal_0 Loss: 0.0202 | 0.0529
Epoch 147/300, seasonal_0 Loss: 0.0202 | 0.0528
Epoch 148/300, seasonal_0 Loss: 0.0202 | 0.0529
Epoch 149/300, seasonal_0 Loss: 0.0202 | 0.0529
Epoch 150/300, seasonal_0 Loss: 0.0202 | 0.0529
Epoch 151/300, seasonal_0 Loss: 0.0202 | 0.0530
Epoch 152/300, seasonal_0 Loss: 0.0202 | 0.0530
Epoch 153/300, seasonal_0 Loss: 0.0202 | 0.0529
Epoch 154/300, seasonal_0 Loss: 0.0202 | 0.0530
Epoch 155/300, seasonal_0 Loss: 0.0202 | 0.0530
Epoch 156/300, seasonal_0 Loss: 0.0201 | 0.0530
Epoch 157/300, seasonal_0 Loss: 0.0201 | 0.0531
Epoch 158/300, seasonal_0 Loss: 0.0201 | 0.0530
Epoch 159/300, seasonal_0 Loss: 0.0201 | 0.0533
Epoch 160/300, seasonal_0 Loss: 0.0201 | 0.0534
Epoch 161/300, seasonal_0 Loss: 0.0201 | 0.0533
Epoch 162/300, seasonal_0 Loss: 0.0201 | 0.0534
Epoch 163/300, seasonal_0 Loss: 0.0201 | 0.0534
Epoch 164/300, seasonal_0 Loss: 0.0201 | 0.0534
Epoch 165/300, seasonal_0 Loss: 0.0201 | 0.0534
Epoch 166/300, seasonal_0 Loss: 0.0200 | 0.0534
Epoch 167/300, seasonal_0 Loss: 0.0200 | 0.0534
Epoch 168/300, seasonal_0 Loss: 0.0200 | 0.0534
Epoch 169/300, seasonal_0 Loss: 0.0200 | 0.0533
Epoch 170/300, seasonal_0 Loss: 0.0200 | 0.0533
Epoch 171/300, seasonal_0 Loss: 0.0200 | 0.0533
Epoch 172/300, seasonal_0 Loss: 0.0200 | 0.0532
Epoch 173/300, seasonal_0 Loss: 0.0200 | 0.0532
Epoch 174/300, seasonal_0 Loss: 0.0200 | 0.0532
Epoch 175/300, seasonal_0 Loss: 0.0200 | 0.0531
Epoch 176/300, seasonal_0 Loss: 0.0200 | 0.0531
Epoch 177/300, seasonal_0 Loss: 0.0199 | 0.0531
Epoch 178/300, seasonal_0 Loss: 0.0199 | 0.0531
Epoch 179/300, seasonal_0 Loss: 0.0199 | 0.0531
Epoch 180/300, seasonal_0 Loss: 0.0199 | 0.0531
Epoch 181/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 182/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 183/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 184/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 185/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 186/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 187/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 188/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 189/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 190/300, seasonal_0 Loss: 0.0199 | 0.0530
Epoch 191/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 192/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 193/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 194/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 195/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 196/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 197/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 198/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 199/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 200/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 201/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 202/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 203/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 204/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 205/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 206/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 207/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 208/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 209/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 210/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 211/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 212/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 213/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 214/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 215/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 216/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 217/300, seasonal_0 Loss: 0.0198 | 0.0530
Epoch 218/300, seasonal_0 Loss: 0.0197 | 0.0530
Epoch 219/300, seasonal_0 Loss: 0.0197 | 0.0530
Epoch 220/300, seasonal_0 Loss: 0.0197 | 0.0530
Epoch 221/300, seasonal_0 Loss: 0.0197 | 0.0530
Epoch 222/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 223/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 224/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 225/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 226/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 227/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 228/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 229/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 230/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 231/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 232/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 233/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 234/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 235/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 236/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 237/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 238/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 239/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 240/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 241/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 242/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 243/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 244/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 245/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 246/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 247/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 248/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 249/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 250/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 251/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 252/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 253/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 254/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 255/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 256/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 257/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 258/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 259/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 260/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 261/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 262/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 263/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 264/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 265/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 266/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 267/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 268/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 269/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 270/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 271/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 272/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 273/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 274/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 275/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 276/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 277/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 278/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 279/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 280/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 281/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 282/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 283/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 284/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 285/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 286/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 287/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 288/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 289/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 290/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 291/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 292/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 293/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 294/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 295/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 296/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 297/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 298/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 299/300, seasonal_0 Loss: 0.0197 | 0.0531
Epoch 300/300, seasonal_0 Loss: 0.0197 | 0.0531
Training seasonal_1 component with params: {'observation_period_num': 19, 'train_rates': 0.8693226928773488, 'learning_rate': 4.162179117134796e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.9172364180298105}
Epoch 1/300, seasonal_1 Loss: 0.3723 | 0.1839
Epoch 2/300, seasonal_1 Loss: 0.2025 | 0.1402
Epoch 3/300, seasonal_1 Loss: 0.1667 | 0.1182
Epoch 4/300, seasonal_1 Loss: 0.1502 | 0.1035
Epoch 5/300, seasonal_1 Loss: 0.1406 | 0.0943
Epoch 6/300, seasonal_1 Loss: 0.1341 | 0.0897
Epoch 7/300, seasonal_1 Loss: 0.1293 | 0.0895
Epoch 8/300, seasonal_1 Loss: 0.1256 | 0.0990
Epoch 9/300, seasonal_1 Loss: 0.1229 | 0.1054
Epoch 10/300, seasonal_1 Loss: 0.1206 | 0.1130
Epoch 11/300, seasonal_1 Loss: 0.1187 | 0.1206
Epoch 12/300, seasonal_1 Loss: 0.1171 | 0.1259
Epoch 13/300, seasonal_1 Loss: 0.1159 | 0.1265
Epoch 14/300, seasonal_1 Loss: 0.1150 | 0.1055
Epoch 15/300, seasonal_1 Loss: 0.1147 | 0.0904
Epoch 16/300, seasonal_1 Loss: 0.1144 | 0.0774
Epoch 17/300, seasonal_1 Loss: 0.1136 | 0.0710
Epoch 18/300, seasonal_1 Loss: 0.1120 | 0.0689
Epoch 19/300, seasonal_1 Loss: 0.1097 | 0.0679
Epoch 20/300, seasonal_1 Loss: 0.1072 | 0.0671
Epoch 21/300, seasonal_1 Loss: 0.1048 | 0.0661
Epoch 22/300, seasonal_1 Loss: 0.1031 | 0.0655
Epoch 23/300, seasonal_1 Loss: 0.1013 | 0.0651
Epoch 24/300, seasonal_1 Loss: 0.0998 | 0.0647
Epoch 25/300, seasonal_1 Loss: 0.0986 | 0.0644
Epoch 26/300, seasonal_1 Loss: 0.0974 | 0.0641
Epoch 27/300, seasonal_1 Loss: 0.0962 | 0.0635
Epoch 28/300, seasonal_1 Loss: 0.0953 | 0.0632
Epoch 29/300, seasonal_1 Loss: 0.0943 | 0.0627
Epoch 30/300, seasonal_1 Loss: 0.0933 | 0.0622
Epoch 31/300, seasonal_1 Loss: 0.0925 | 0.0615
Epoch 32/300, seasonal_1 Loss: 0.0916 | 0.0608
Epoch 33/300, seasonal_1 Loss: 0.0907 | 0.0601
Epoch 34/300, seasonal_1 Loss: 0.0898 | 0.0590
Epoch 35/300, seasonal_1 Loss: 0.0890 | 0.0582
Epoch 36/300, seasonal_1 Loss: 0.0882 | 0.0574
Epoch 37/300, seasonal_1 Loss: 0.0875 | 0.0565
Epoch 38/300, seasonal_1 Loss: 0.0868 | 0.0557
Epoch 39/300, seasonal_1 Loss: 0.0861 | 0.0549
Epoch 40/300, seasonal_1 Loss: 0.0854 | 0.0544
Epoch 41/300, seasonal_1 Loss: 0.0848 | 0.0537
Epoch 42/300, seasonal_1 Loss: 0.0843 | 0.0532
Epoch 43/300, seasonal_1 Loss: 0.0837 | 0.0527
Epoch 44/300, seasonal_1 Loss: 0.0832 | 0.0523
Epoch 45/300, seasonal_1 Loss: 0.0826 | 0.0518
Epoch 46/300, seasonal_1 Loss: 0.0821 | 0.0514
Epoch 47/300, seasonal_1 Loss: 0.0816 | 0.0515
Epoch 48/300, seasonal_1 Loss: 0.0811 | 0.0511
Epoch 49/300, seasonal_1 Loss: 0.0807 | 0.0509
Epoch 50/300, seasonal_1 Loss: 0.0803 | 0.0506
Epoch 51/300, seasonal_1 Loss: 0.0798 | 0.0503
Epoch 52/300, seasonal_1 Loss: 0.0794 | 0.0501
Epoch 53/300, seasonal_1 Loss: 0.0790 | 0.0501
Epoch 54/300, seasonal_1 Loss: 0.0786 | 0.0499
Epoch 55/300, seasonal_1 Loss: 0.0783 | 0.0497
Epoch 56/300, seasonal_1 Loss: 0.0779 | 0.0496
Epoch 57/300, seasonal_1 Loss: 0.0776 | 0.0494
Epoch 58/300, seasonal_1 Loss: 0.0772 | 0.0492
Epoch 59/300, seasonal_1 Loss: 0.0769 | 0.0491
Epoch 60/300, seasonal_1 Loss: 0.0765 | 0.0491
Epoch 61/300, seasonal_1 Loss: 0.0763 | 0.0489
Epoch 62/300, seasonal_1 Loss: 0.0760 | 0.0489
Epoch 63/300, seasonal_1 Loss: 0.0757 | 0.0488
Epoch 64/300, seasonal_1 Loss: 0.0754 | 0.0488
Epoch 65/300, seasonal_1 Loss: 0.0751 | 0.0487
Epoch 66/300, seasonal_1 Loss: 0.0748 | 0.0480
Epoch 67/300, seasonal_1 Loss: 0.0746 | 0.0477
Epoch 68/300, seasonal_1 Loss: 0.0743 | 0.0475
Epoch 69/300, seasonal_1 Loss: 0.0741 | 0.0472
Epoch 70/300, seasonal_1 Loss: 0.0738 | 0.0469
Epoch 71/300, seasonal_1 Loss: 0.0736 | 0.0465
Epoch 72/300, seasonal_1 Loss: 0.0733 | 0.0461
Epoch 73/300, seasonal_1 Loss: 0.0731 | 0.0443
Epoch 74/300, seasonal_1 Loss: 0.0729 | 0.0437
Epoch 75/300, seasonal_1 Loss: 0.0727 | 0.0433
Epoch 76/300, seasonal_1 Loss: 0.0724 | 0.0428
Epoch 77/300, seasonal_1 Loss: 0.0722 | 0.0424
Epoch 78/300, seasonal_1 Loss: 0.0719 | 0.0421
Epoch 79/300, seasonal_1 Loss: 0.0717 | 0.0412
Epoch 80/300, seasonal_1 Loss: 0.0714 | 0.0410
Epoch 81/300, seasonal_1 Loss: 0.0712 | 0.0408
Epoch 82/300, seasonal_1 Loss: 0.0709 | 0.0406
Epoch 83/300, seasonal_1 Loss: 0.0707 | 0.0404
Epoch 84/300, seasonal_1 Loss: 0.0705 | 0.0402
Epoch 85/300, seasonal_1 Loss: 0.0703 | 0.0401
Epoch 86/300, seasonal_1 Loss: 0.0701 | 0.0399
Epoch 87/300, seasonal_1 Loss: 0.0699 | 0.0399
Epoch 88/300, seasonal_1 Loss: 0.0697 | 0.0398
Epoch 89/300, seasonal_1 Loss: 0.0695 | 0.0397
Epoch 90/300, seasonal_1 Loss: 0.0693 | 0.0396
Epoch 91/300, seasonal_1 Loss: 0.0691 | 0.0395
Epoch 92/300, seasonal_1 Loss: 0.0690 | 0.0395
Epoch 93/300, seasonal_1 Loss: 0.0688 | 0.0394
Epoch 94/300, seasonal_1 Loss: 0.0686 | 0.0393
Epoch 95/300, seasonal_1 Loss: 0.0685 | 0.0393
Epoch 96/300, seasonal_1 Loss: 0.0683 | 0.0392
Epoch 97/300, seasonal_1 Loss: 0.0682 | 0.0391
Epoch 98/300, seasonal_1 Loss: 0.0680 | 0.0391
Epoch 99/300, seasonal_1 Loss: 0.0678 | 0.0391
Epoch 100/300, seasonal_1 Loss: 0.0677 | 0.0390
Epoch 101/300, seasonal_1 Loss: 0.0676 | 0.0389
Epoch 102/300, seasonal_1 Loss: 0.0674 | 0.0389
Epoch 103/300, seasonal_1 Loss: 0.0673 | 0.0388
Epoch 104/300, seasonal_1 Loss: 0.0671 | 0.0388
Epoch 105/300, seasonal_1 Loss: 0.0670 | 0.0388
Epoch 106/300, seasonal_1 Loss: 0.0669 | 0.0387
Epoch 107/300, seasonal_1 Loss: 0.0667 | 0.0387
Epoch 108/300, seasonal_1 Loss: 0.0666 | 0.0386
Epoch 109/300, seasonal_1 Loss: 0.0665 | 0.0386
Epoch 110/300, seasonal_1 Loss: 0.0664 | 0.0385
Epoch 111/300, seasonal_1 Loss: 0.0663 | 0.0385
Epoch 112/300, seasonal_1 Loss: 0.0661 | 0.0384
Epoch 113/300, seasonal_1 Loss: 0.0660 | 0.0384
Epoch 114/300, seasonal_1 Loss: 0.0659 | 0.0384
Epoch 115/300, seasonal_1 Loss: 0.0658 | 0.0383
Epoch 116/300, seasonal_1 Loss: 0.0657 | 0.0383
Epoch 117/300, seasonal_1 Loss: 0.0656 | 0.0382
Epoch 118/300, seasonal_1 Loss: 0.0655 | 0.0382
Epoch 119/300, seasonal_1 Loss: 0.0654 | 0.0382
Epoch 120/300, seasonal_1 Loss: 0.0653 | 0.0381
Epoch 121/300, seasonal_1 Loss: 0.0652 | 0.0381
Epoch 122/300, seasonal_1 Loss: 0.0651 | 0.0381
Epoch 123/300, seasonal_1 Loss: 0.0650 | 0.0380
Epoch 124/300, seasonal_1 Loss: 0.0649 | 0.0380
Epoch 125/300, seasonal_1 Loss: 0.0648 | 0.0380
Epoch 126/300, seasonal_1 Loss: 0.0647 | 0.0380
Epoch 127/300, seasonal_1 Loss: 0.0646 | 0.0379
Epoch 128/300, seasonal_1 Loss: 0.0645 | 0.0379
Epoch 129/300, seasonal_1 Loss: 0.0644 | 0.0379
Epoch 130/300, seasonal_1 Loss: 0.0643 | 0.0379
Epoch 131/300, seasonal_1 Loss: 0.0642 | 0.0379
Epoch 132/300, seasonal_1 Loss: 0.0642 | 0.0378
Epoch 133/300, seasonal_1 Loss: 0.0641 | 0.0378
Epoch 134/300, seasonal_1 Loss: 0.0640 | 0.0378
Epoch 135/300, seasonal_1 Loss: 0.0639 | 0.0378
Epoch 136/300, seasonal_1 Loss: 0.0639 | 0.0378
Epoch 137/300, seasonal_1 Loss: 0.0638 | 0.0377
Epoch 138/300, seasonal_1 Loss: 0.0637 | 0.0377
Epoch 139/300, seasonal_1 Loss: 0.0636 | 0.0377
Epoch 140/300, seasonal_1 Loss: 0.0636 | 0.0377
Epoch 141/300, seasonal_1 Loss: 0.0635 | 0.0377
Epoch 142/300, seasonal_1 Loss: 0.0634 | 0.0377
Epoch 143/300, seasonal_1 Loss: 0.0633 | 0.0376
Epoch 144/300, seasonal_1 Loss: 0.0633 | 0.0376
Epoch 145/300, seasonal_1 Loss: 0.0632 | 0.0376
Epoch 146/300, seasonal_1 Loss: 0.0631 | 0.0376
Epoch 147/300, seasonal_1 Loss: 0.0631 | 0.0376
Epoch 148/300, seasonal_1 Loss: 0.0630 | 0.0376
Epoch 149/300, seasonal_1 Loss: 0.0630 | 0.0375
Epoch 150/300, seasonal_1 Loss: 0.0629 | 0.0375
Epoch 151/300, seasonal_1 Loss: 0.0628 | 0.0375
Epoch 152/300, seasonal_1 Loss: 0.0628 | 0.0375
Epoch 153/300, seasonal_1 Loss: 0.0627 | 0.0375
Epoch 154/300, seasonal_1 Loss: 0.0627 | 0.0375
Epoch 155/300, seasonal_1 Loss: 0.0626 | 0.0375
Epoch 156/300, seasonal_1 Loss: 0.0625 | 0.0375
Epoch 157/300, seasonal_1 Loss: 0.0625 | 0.0374
Epoch 158/300, seasonal_1 Loss: 0.0624 | 0.0374
Epoch 159/300, seasonal_1 Loss: 0.0624 | 0.0374
Epoch 160/300, seasonal_1 Loss: 0.0623 | 0.0374
Epoch 161/300, seasonal_1 Loss: 0.0623 | 0.0374
Epoch 162/300, seasonal_1 Loss: 0.0622 | 0.0374
Epoch 163/300, seasonal_1 Loss: 0.0622 | 0.0374
Epoch 164/300, seasonal_1 Loss: 0.0621 | 0.0374
Epoch 165/300, seasonal_1 Loss: 0.0621 | 0.0374
Epoch 166/300, seasonal_1 Loss: 0.0620 | 0.0374
Epoch 167/300, seasonal_1 Loss: 0.0620 | 0.0374
Epoch 168/300, seasonal_1 Loss: 0.0619 | 0.0374
Epoch 169/300, seasonal_1 Loss: 0.0619 | 0.0374
Epoch 170/300, seasonal_1 Loss: 0.0618 | 0.0374
Epoch 171/300, seasonal_1 Loss: 0.0618 | 0.0374
Epoch 172/300, seasonal_1 Loss: 0.0617 | 0.0374
Epoch 173/300, seasonal_1 Loss: 0.0617 | 0.0374
Epoch 174/300, seasonal_1 Loss: 0.0616 | 0.0374
Epoch 175/300, seasonal_1 Loss: 0.0616 | 0.0374
Epoch 176/300, seasonal_1 Loss: 0.0615 | 0.0374
Epoch 177/300, seasonal_1 Loss: 0.0615 | 0.0374
Epoch 178/300, seasonal_1 Loss: 0.0615 | 0.0374
Epoch 179/300, seasonal_1 Loss: 0.0614 | 0.0374
Epoch 180/300, seasonal_1 Loss: 0.0614 | 0.0374
Epoch 181/300, seasonal_1 Loss: 0.0613 | 0.0373
Epoch 182/300, seasonal_1 Loss: 0.0613 | 0.0373
Epoch 183/300, seasonal_1 Loss: 0.0612 | 0.0373
Epoch 184/300, seasonal_1 Loss: 0.0612 | 0.0373
Epoch 185/300, seasonal_1 Loss: 0.0612 | 0.0373
Epoch 186/300, seasonal_1 Loss: 0.0611 | 0.0373
Epoch 187/300, seasonal_1 Loss: 0.0611 | 0.0373
Epoch 188/300, seasonal_1 Loss: 0.0611 | 0.0373
Epoch 189/300, seasonal_1 Loss: 0.0610 | 0.0373
Epoch 190/300, seasonal_1 Loss: 0.0610 | 0.0373
Epoch 191/300, seasonal_1 Loss: 0.0610 | 0.0373
Epoch 192/300, seasonal_1 Loss: 0.0609 | 0.0373
Epoch 193/300, seasonal_1 Loss: 0.0609 | 0.0373
Epoch 194/300, seasonal_1 Loss: 0.0609 | 0.0373
Epoch 195/300, seasonal_1 Loss: 0.0608 | 0.0373
Epoch 196/300, seasonal_1 Loss: 0.0608 | 0.0373
Epoch 197/300, seasonal_1 Loss: 0.0608 | 0.0373
Epoch 198/300, seasonal_1 Loss: 0.0607 | 0.0373
Epoch 199/300, seasonal_1 Loss: 0.0607 | 0.0373
Epoch 200/300, seasonal_1 Loss: 0.0607 | 0.0373
Epoch 201/300, seasonal_1 Loss: 0.0606 | 0.0373
Epoch 202/300, seasonal_1 Loss: 0.0606 | 0.0373
Epoch 203/300, seasonal_1 Loss: 0.0606 | 0.0373
Epoch 204/300, seasonal_1 Loss: 0.0605 | 0.0373
Epoch 205/300, seasonal_1 Loss: 0.0605 | 0.0373
Epoch 206/300, seasonal_1 Loss: 0.0605 | 0.0373
Epoch 207/300, seasonal_1 Loss: 0.0605 | 0.0373
Epoch 208/300, seasonal_1 Loss: 0.0604 | 0.0373
Epoch 209/300, seasonal_1 Loss: 0.0604 | 0.0373
Epoch 210/300, seasonal_1 Loss: 0.0604 | 0.0373
Epoch 211/300, seasonal_1 Loss: 0.0603 | 0.0373
Epoch 212/300, seasonal_1 Loss: 0.0603 | 0.0373
Epoch 213/300, seasonal_1 Loss: 0.0603 | 0.0373
Epoch 214/300, seasonal_1 Loss: 0.0603 | 0.0373
Epoch 215/300, seasonal_1 Loss: 0.0602 | 0.0373
Epoch 216/300, seasonal_1 Loss: 0.0602 | 0.0373
Epoch 217/300, seasonal_1 Loss: 0.0602 | 0.0373
Epoch 218/300, seasonal_1 Loss: 0.0601 | 0.0373
Epoch 219/300, seasonal_1 Loss: 0.0601 | 0.0372
Epoch 220/300, seasonal_1 Loss: 0.0601 | 0.0372
Epoch 221/300, seasonal_1 Loss: 0.0601 | 0.0372
Epoch 222/300, seasonal_1 Loss: 0.0600 | 0.0372
Epoch 223/300, seasonal_1 Loss: 0.0600 | 0.0372
Epoch 224/300, seasonal_1 Loss: 0.0600 | 0.0372
Epoch 225/300, seasonal_1 Loss: 0.0600 | 0.0372
Epoch 226/300, seasonal_1 Loss: 0.0599 | 0.0372
Epoch 227/300, seasonal_1 Loss: 0.0599 | 0.0372
Epoch 228/300, seasonal_1 Loss: 0.0599 | 0.0372
Epoch 229/300, seasonal_1 Loss: 0.0599 | 0.0372
Epoch 230/300, seasonal_1 Loss: 0.0598 | 0.0372
Epoch 231/300, seasonal_1 Loss: 0.0598 | 0.0372
Epoch 232/300, seasonal_1 Loss: 0.0598 | 0.0372
Epoch 233/300, seasonal_1 Loss: 0.0598 | 0.0372
Epoch 234/300, seasonal_1 Loss: 0.0598 | 0.0372
Epoch 235/300, seasonal_1 Loss: 0.0597 | 0.0372
Epoch 236/300, seasonal_1 Loss: 0.0597 | 0.0372
Epoch 237/300, seasonal_1 Loss: 0.0597 | 0.0372
Epoch 238/300, seasonal_1 Loss: 0.0597 | 0.0372
Epoch 239/300, seasonal_1 Loss: 0.0596 | 0.0372
Epoch 240/300, seasonal_1 Loss: 0.0596 | 0.0372
Epoch 241/300, seasonal_1 Loss: 0.0596 | 0.0372
Epoch 242/300, seasonal_1 Loss: 0.0596 | 0.0372
Epoch 243/300, seasonal_1 Loss: 0.0595 | 0.0371
Epoch 244/300, seasonal_1 Loss: 0.0595 | 0.0371
Epoch 245/300, seasonal_1 Loss: 0.0595 | 0.0371
Epoch 246/300, seasonal_1 Loss: 0.0595 | 0.0371
Epoch 247/300, seasonal_1 Loss: 0.0595 | 0.0371
Epoch 248/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 249/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 250/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 251/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 252/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 253/300, seasonal_1 Loss: 0.0594 | 0.0371
Epoch 254/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 255/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 256/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 257/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 258/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 259/300, seasonal_1 Loss: 0.0593 | 0.0371
Epoch 260/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 261/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 262/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 263/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 264/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 265/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 266/300, seasonal_1 Loss: 0.0592 | 0.0371
Epoch 267/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 268/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 269/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 270/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 271/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 272/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 273/300, seasonal_1 Loss: 0.0591 | 0.0371
Epoch 274/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 275/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 276/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 277/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 278/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 279/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 280/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 281/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 282/300, seasonal_1 Loss: 0.0590 | 0.0371
Epoch 283/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 284/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 285/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 286/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 287/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 288/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 289/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 290/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 291/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 292/300, seasonal_1 Loss: 0.0589 | 0.0371
Epoch 293/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 294/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 295/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 296/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 297/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 298/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 299/300, seasonal_1 Loss: 0.0588 | 0.0371
Epoch 300/300, seasonal_1 Loss: 0.0588 | 0.0371
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.823812191625443, 'learning_rate': 0.00017516285036875043, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8330089250187226}
Epoch 1/300, seasonal_2 Loss: 0.2384 | 0.0900
Epoch 2/300, seasonal_2 Loss: 0.1336 | 0.0954
Epoch 3/300, seasonal_2 Loss: 0.1240 | 0.0944
Epoch 4/300, seasonal_2 Loss: 0.1200 | 0.0903
Epoch 5/300, seasonal_2 Loss: 0.1155 | 0.0839
Epoch 6/300, seasonal_2 Loss: 0.1077 | 0.0742
Epoch 7/300, seasonal_2 Loss: 0.1013 | 0.0736
Epoch 8/300, seasonal_2 Loss: 0.0979 | 0.0716
Epoch 9/300, seasonal_2 Loss: 0.0956 | 0.0695
Epoch 10/300, seasonal_2 Loss: 0.0934 | 0.0666
Epoch 11/300, seasonal_2 Loss: 0.0908 | 0.0611
Epoch 12/300, seasonal_2 Loss: 0.0876 | 0.0569
Epoch 13/300, seasonal_2 Loss: 0.0852 | 0.0534
Epoch 14/300, seasonal_2 Loss: 0.0836 | 0.0516
Epoch 15/300, seasonal_2 Loss: 0.0824 | 0.0504
Epoch 16/300, seasonal_2 Loss: 0.0806 | 0.0460
Epoch 17/300, seasonal_2 Loss: 0.0790 | 0.0452
Epoch 18/300, seasonal_2 Loss: 0.0776 | 0.0447
Epoch 19/300, seasonal_2 Loss: 0.0768 | 0.0443
Epoch 20/300, seasonal_2 Loss: 0.0761 | 0.0439
Epoch 21/300, seasonal_2 Loss: 0.0750 | 0.0400
Epoch 22/300, seasonal_2 Loss: 0.0744 | 0.0385
Epoch 23/300, seasonal_2 Loss: 0.0736 | 0.0374
Epoch 24/300, seasonal_2 Loss: 0.0730 | 0.0367
Epoch 25/300, seasonal_2 Loss: 0.0723 | 0.0364
Epoch 26/300, seasonal_2 Loss: 0.0714 | 0.0353
Epoch 27/300, seasonal_2 Loss: 0.0709 | 0.0347
Epoch 28/300, seasonal_2 Loss: 0.0704 | 0.0342
Epoch 29/300, seasonal_2 Loss: 0.0699 | 0.0339
Epoch 30/300, seasonal_2 Loss: 0.0693 | 0.0338
Epoch 31/300, seasonal_2 Loss: 0.0685 | 0.0330
Epoch 32/300, seasonal_2 Loss: 0.0680 | 0.0324
Epoch 33/300, seasonal_2 Loss: 0.0675 | 0.0319
Epoch 34/300, seasonal_2 Loss: 0.0670 | 0.0317
Epoch 35/300, seasonal_2 Loss: 0.0666 | 0.0316
Epoch 36/300, seasonal_2 Loss: 0.0660 | 0.0314
Epoch 37/300, seasonal_2 Loss: 0.0658 | 0.0312
Epoch 38/300, seasonal_2 Loss: 0.0655 | 0.0310
Epoch 39/300, seasonal_2 Loss: 0.0652 | 0.0310
Epoch 40/300, seasonal_2 Loss: 0.0650 | 0.0309
Epoch 41/300, seasonal_2 Loss: 0.0648 | 0.0303
Epoch 42/300, seasonal_2 Loss: 0.0650 | 0.0304
Epoch 43/300, seasonal_2 Loss: 0.0649 | 0.0305
Epoch 44/300, seasonal_2 Loss: 0.0647 | 0.0307
Epoch 45/300, seasonal_2 Loss: 0.0642 | 0.0307
Epoch 46/300, seasonal_2 Loss: 0.0636 | 0.0315
Epoch 47/300, seasonal_2 Loss: 0.0632 | 0.0310
Epoch 48/300, seasonal_2 Loss: 0.0628 | 0.0307
Epoch 49/300, seasonal_2 Loss: 0.0626 | 0.0305
Epoch 50/300, seasonal_2 Loss: 0.0624 | 0.0304
Epoch 51/300, seasonal_2 Loss: 0.0621 | 0.0309
Epoch 52/300, seasonal_2 Loss: 0.0620 | 0.0306
Epoch 53/300, seasonal_2 Loss: 0.0618 | 0.0305
Epoch 54/300, seasonal_2 Loss: 0.0616 | 0.0304
Epoch 55/300, seasonal_2 Loss: 0.0615 | 0.0304
Epoch 56/300, seasonal_2 Loss: 0.0613 | 0.0303
Epoch 57/300, seasonal_2 Loss: 0.0611 | 0.0302
Epoch 58/300, seasonal_2 Loss: 0.0610 | 0.0302
Epoch 59/300, seasonal_2 Loss: 0.0609 | 0.0302
Epoch 60/300, seasonal_2 Loss: 0.0607 | 0.0301
Epoch 61/300, seasonal_2 Loss: 0.0605 | 0.0297
Epoch 62/300, seasonal_2 Loss: 0.0604 | 0.0298
Epoch 63/300, seasonal_2 Loss: 0.0603 | 0.0298
Epoch 64/300, seasonal_2 Loss: 0.0602 | 0.0297
Epoch 65/300, seasonal_2 Loss: 0.0601 | 0.0297
Epoch 66/300, seasonal_2 Loss: 0.0600 | 0.0297
Epoch 67/300, seasonal_2 Loss: 0.0600 | 0.0297
Epoch 68/300, seasonal_2 Loss: 0.0599 | 0.0296
Epoch 69/300, seasonal_2 Loss: 0.0598 | 0.0296
Epoch 70/300, seasonal_2 Loss: 0.0597 | 0.0296
Epoch 71/300, seasonal_2 Loss: 0.0596 | 0.0294
Epoch 72/300, seasonal_2 Loss: 0.0595 | 0.0294
Epoch 73/300, seasonal_2 Loss: 0.0595 | 0.0294
Epoch 74/300, seasonal_2 Loss: 0.0594 | 0.0293
Epoch 75/300, seasonal_2 Loss: 0.0593 | 0.0293
Epoch 76/300, seasonal_2 Loss: 0.0592 | 0.0292
Epoch 77/300, seasonal_2 Loss: 0.0592 | 0.0291
Epoch 78/300, seasonal_2 Loss: 0.0591 | 0.0291
Epoch 79/300, seasonal_2 Loss: 0.0590 | 0.0291
Epoch 80/300, seasonal_2 Loss: 0.0590 | 0.0290
Epoch 81/300, seasonal_2 Loss: 0.0589 | 0.0289
Epoch 82/300, seasonal_2 Loss: 0.0588 | 0.0289
Epoch 83/300, seasonal_2 Loss: 0.0588 | 0.0289
Epoch 84/300, seasonal_2 Loss: 0.0587 | 0.0289
Epoch 85/300, seasonal_2 Loss: 0.0587 | 0.0289
Epoch 86/300, seasonal_2 Loss: 0.0586 | 0.0288
Epoch 87/300, seasonal_2 Loss: 0.0586 | 0.0288
Epoch 88/300, seasonal_2 Loss: 0.0586 | 0.0287
Epoch 89/300, seasonal_2 Loss: 0.0585 | 0.0287
Epoch 90/300, seasonal_2 Loss: 0.0585 | 0.0287
Epoch 91/300, seasonal_2 Loss: 0.0584 | 0.0287
Epoch 92/300, seasonal_2 Loss: 0.0584 | 0.0287
Epoch 93/300, seasonal_2 Loss: 0.0583 | 0.0286
Epoch 94/300, seasonal_2 Loss: 0.0583 | 0.0286
Epoch 95/300, seasonal_2 Loss: 0.0583 | 0.0286
Epoch 96/300, seasonal_2 Loss: 0.0582 | 0.0286
Epoch 97/300, seasonal_2 Loss: 0.0582 | 0.0286
Epoch 98/300, seasonal_2 Loss: 0.0582 | 0.0286
Epoch 99/300, seasonal_2 Loss: 0.0581 | 0.0286
Epoch 100/300, seasonal_2 Loss: 0.0581 | 0.0286
Epoch 101/300, seasonal_2 Loss: 0.0581 | 0.0286
Epoch 102/300, seasonal_2 Loss: 0.0580 | 0.0286
Epoch 103/300, seasonal_2 Loss: 0.0580 | 0.0286
Epoch 104/300, seasonal_2 Loss: 0.0580 | 0.0286
Epoch 105/300, seasonal_2 Loss: 0.0580 | 0.0286
Epoch 106/300, seasonal_2 Loss: 0.0579 | 0.0286
Epoch 107/300, seasonal_2 Loss: 0.0579 | 0.0286
Epoch 108/300, seasonal_2 Loss: 0.0579 | 0.0286
Epoch 109/300, seasonal_2 Loss: 0.0579 | 0.0286
Epoch 110/300, seasonal_2 Loss: 0.0578 | 0.0286
Epoch 111/300, seasonal_2 Loss: 0.0578 | 0.0286
Epoch 112/300, seasonal_2 Loss: 0.0578 | 0.0286
Epoch 113/300, seasonal_2 Loss: 0.0578 | 0.0286
Epoch 114/300, seasonal_2 Loss: 0.0577 | 0.0286
Epoch 115/300, seasonal_2 Loss: 0.0577 | 0.0286
Epoch 116/300, seasonal_2 Loss: 0.0577 | 0.0287
Epoch 117/300, seasonal_2 Loss: 0.0577 | 0.0287
Epoch 118/300, seasonal_2 Loss: 0.0577 | 0.0287
Epoch 119/300, seasonal_2 Loss: 0.0577 | 0.0287
Epoch 120/300, seasonal_2 Loss: 0.0576 | 0.0287
Epoch 121/300, seasonal_2 Loss: 0.0576 | 0.0288
Epoch 122/300, seasonal_2 Loss: 0.0576 | 0.0288
Epoch 123/300, seasonal_2 Loss: 0.0576 | 0.0288
Epoch 124/300, seasonal_2 Loss: 0.0576 | 0.0288
Epoch 125/300, seasonal_2 Loss: 0.0576 | 0.0288
Epoch 126/300, seasonal_2 Loss: 0.0575 | 0.0288
Epoch 127/300, seasonal_2 Loss: 0.0575 | 0.0289
Epoch 128/300, seasonal_2 Loss: 0.0575 | 0.0289
Epoch 129/300, seasonal_2 Loss: 0.0575 | 0.0288
Epoch 130/300, seasonal_2 Loss: 0.0575 | 0.0288
Epoch 131/300, seasonal_2 Loss: 0.0575 | 0.0289
Epoch 132/300, seasonal_2 Loss: 0.0575 | 0.0289
Epoch 133/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 134/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 135/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 136/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 137/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 138/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 139/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 140/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 141/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 142/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 143/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 144/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 145/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 146/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 147/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 148/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 149/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 150/300, seasonal_2 Loss: 0.0573 | 0.0289
Epoch 151/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 152/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 153/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 154/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 155/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 156/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 157/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 158/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 159/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 160/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 161/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 162/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 163/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 164/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 165/300, seasonal_2 Loss: 0.0572 | 0.0289
Epoch 166/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 167/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 168/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 169/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 170/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 171/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 172/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 173/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 174/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 175/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 176/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 177/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 178/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 179/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 180/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 181/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 182/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 183/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 184/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 185/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 186/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 187/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 188/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 189/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 190/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 191/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 192/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 193/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 194/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 195/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 196/300, seasonal_2 Loss: 0.0571 | 0.0289
Epoch 197/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 198/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 199/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 200/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 201/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 202/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 203/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 204/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 205/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 206/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 207/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 208/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 209/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 210/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 211/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 212/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 213/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 214/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 215/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 216/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 217/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 218/300, seasonal_2 Loss: 0.0570 | 0.0289
Epoch 219/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 220/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 221/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 222/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 223/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 224/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 225/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 226/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 227/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 228/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 229/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 230/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 231/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 232/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 233/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 234/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 235/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 236/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 237/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 238/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 239/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 240/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 241/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 242/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 243/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 244/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 245/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 246/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 247/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 248/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 249/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 250/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 251/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 252/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 253/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 254/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 255/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 256/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 257/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 258/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 259/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 260/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 261/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 262/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 263/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 264/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 265/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 266/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 267/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 268/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 269/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 270/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 271/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 272/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 273/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 274/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 275/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 276/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 277/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 278/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 279/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 280/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 281/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 282/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 283/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 284/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 285/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 286/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 287/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 288/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 289/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 290/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 291/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 292/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 293/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 294/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 295/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 296/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 297/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 298/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 299/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 300/300, seasonal_2 Loss: 0.0570 | 0.0288
Training seasonal_3 component with params: {'observation_period_num': 23, 'train_rates': 0.9782856662554168, 'learning_rate': 0.0002170887499126454, 'batch_size': 48, 'step_size': 5, 'gamma': 0.9826425207806204}
Epoch 1/300, seasonal_3 Loss: 0.3049 | 0.1405
Epoch 2/300, seasonal_3 Loss: 0.1568 | 0.0876
Epoch 3/300, seasonal_3 Loss: 0.1285 | 0.0730
Epoch 4/300, seasonal_3 Loss: 0.1143 | 0.0673
Epoch 5/300, seasonal_3 Loss: 0.1067 | 0.0628
Epoch 6/300, seasonal_3 Loss: 0.1018 | 0.0587
Epoch 7/300, seasonal_3 Loss: 0.0988 | 0.0581
Epoch 8/300, seasonal_3 Loss: 0.0963 | 0.0663
Epoch 9/300, seasonal_3 Loss: 0.0957 | 0.0892
Epoch 10/300, seasonal_3 Loss: 0.1030 | 0.0749
Epoch 11/300, seasonal_3 Loss: 0.1049 | 0.0780
Epoch 12/300, seasonal_3 Loss: 0.0955 | 0.0607
Epoch 13/300, seasonal_3 Loss: 0.0915 | 0.0575
Epoch 14/300, seasonal_3 Loss: 0.0904 | 0.0668
Epoch 15/300, seasonal_3 Loss: 0.0891 | 0.0629
Epoch 16/300, seasonal_3 Loss: 0.0852 | 0.0585
Epoch 17/300, seasonal_3 Loss: 0.0825 | 0.0561
Epoch 18/300, seasonal_3 Loss: 0.0810 | 0.0556
Epoch 19/300, seasonal_3 Loss: 0.0795 | 0.0548
Epoch 20/300, seasonal_3 Loss: 0.0776 | 0.0522
Epoch 21/300, seasonal_3 Loss: 0.0756 | 0.0500
Epoch 22/300, seasonal_3 Loss: 0.0740 | 0.0482
Epoch 23/300, seasonal_3 Loss: 0.0728 | 0.0470
Epoch 24/300, seasonal_3 Loss: 0.0719 | 0.0467
Epoch 25/300, seasonal_3 Loss: 0.0709 | 0.0457
Epoch 26/300, seasonal_3 Loss: 0.0701 | 0.0448
Epoch 27/300, seasonal_3 Loss: 0.0691 | 0.0434
Epoch 28/300, seasonal_3 Loss: 0.0683 | 0.0421
Epoch 29/300, seasonal_3 Loss: 0.0676 | 0.0413
Epoch 30/300, seasonal_3 Loss: 0.0672 | 0.0417
Epoch 31/300, seasonal_3 Loss: 0.0672 | 0.0476
Epoch 32/300, seasonal_3 Loss: 0.0670 | 0.0496
Epoch 33/300, seasonal_3 Loss: 0.0656 | 0.0477
Epoch 34/300, seasonal_3 Loss: 0.0641 | 0.0491
Epoch 35/300, seasonal_3 Loss: 0.0632 | 0.0471
Epoch 36/300, seasonal_3 Loss: 0.0625 | 0.0465
Epoch 37/300, seasonal_3 Loss: 0.0626 | 0.0471
Epoch 38/300, seasonal_3 Loss: 0.0623 | 0.0438
Epoch 39/300, seasonal_3 Loss: 0.0618 | 0.0391
Epoch 40/300, seasonal_3 Loss: 0.0613 | 0.0353
Epoch 41/300, seasonal_3 Loss: 0.0607 | 0.0314
Epoch 42/300, seasonal_3 Loss: 0.0600 | 0.0296
Epoch 43/300, seasonal_3 Loss: 0.0601 | 0.0288
Epoch 44/300, seasonal_3 Loss: 0.0598 | 0.0350
Epoch 45/300, seasonal_3 Loss: 0.0581 | 0.0397
Epoch 46/300, seasonal_3 Loss: 0.0568 | 0.0331
Epoch 47/300, seasonal_3 Loss: 0.0562 | 0.0306
Epoch 48/300, seasonal_3 Loss: 0.0558 | 0.0303
Epoch 49/300, seasonal_3 Loss: 0.0556 | 0.0309
Epoch 50/300, seasonal_3 Loss: 0.0555 | 0.0312
Epoch 51/300, seasonal_3 Loss: 0.0556 | 0.0372
Epoch 52/300, seasonal_3 Loss: 0.0549 | 0.0359
Epoch 53/300, seasonal_3 Loss: 0.0551 | 0.0345
Epoch 54/300, seasonal_3 Loss: 0.0546 | 0.0340
Epoch 55/300, seasonal_3 Loss: 0.0538 | 0.0336
Epoch 56/300, seasonal_3 Loss: 0.0536 | 0.0366
Epoch 57/300, seasonal_3 Loss: 0.0527 | 0.0342
Epoch 58/300, seasonal_3 Loss: 0.0531 | 0.0379
Epoch 59/300, seasonal_3 Loss: 0.0520 | 0.0355
Epoch 60/300, seasonal_3 Loss: 0.0551 | 0.0354
Epoch 61/300, seasonal_3 Loss: 0.0523 | 0.0336
Epoch 62/300, seasonal_3 Loss: 0.0521 | 0.0353
Epoch 63/300, seasonal_3 Loss: 0.0533 | 0.0340
Epoch 64/300, seasonal_3 Loss: 0.0516 | 0.0370
Epoch 65/300, seasonal_3 Loss: 0.0561 | 0.0347
Epoch 66/300, seasonal_3 Loss: 0.0531 | 0.0342
Epoch 67/300, seasonal_3 Loss: 0.0573 | 0.0343
Epoch 68/300, seasonal_3 Loss: 0.0538 | 0.0323
Epoch 69/300, seasonal_3 Loss: 0.0576 | 0.0411
Epoch 70/300, seasonal_3 Loss: 0.0575 | 0.0379
Epoch 71/300, seasonal_3 Loss: 0.0574 | 0.0696
Epoch 72/300, seasonal_3 Loss: 0.0562 | 0.0626
Epoch 73/300, seasonal_3 Loss: 0.0607 | 0.0456
Epoch 74/300, seasonal_3 Loss: 0.0590 | 0.0468
Epoch 75/300, seasonal_3 Loss: 0.0552 | 0.0419
Epoch 76/300, seasonal_3 Loss: 0.0532 | 0.0362
Epoch 77/300, seasonal_3 Loss: 0.0504 | 0.0334
Epoch 78/300, seasonal_3 Loss: 0.0485 | 0.0318
Epoch 79/300, seasonal_3 Loss: 0.0471 | 0.0320
Epoch 80/300, seasonal_3 Loss: 0.0463 | 0.0329
Epoch 81/300, seasonal_3 Loss: 0.0458 | 0.0335
Epoch 82/300, seasonal_3 Loss: 0.0456 | 0.0327
Epoch 83/300, seasonal_3 Loss: 0.0453 | 0.0322
Epoch 84/300, seasonal_3 Loss: 0.0451 | 0.0317
Epoch 85/300, seasonal_3 Loss: 0.0448 | 0.0312
Epoch 86/300, seasonal_3 Loss: 0.0443 | 0.0307
Epoch 87/300, seasonal_3 Loss: 0.0430 | 0.0307
Epoch 88/300, seasonal_3 Loss: 0.0458 | 0.0428
Epoch 89/300, seasonal_3 Loss: 0.0469 | 0.0309
Epoch 90/300, seasonal_3 Loss: 0.0424 | 0.0303
Epoch 91/300, seasonal_3 Loss: 0.0411 | 0.0361
Epoch 92/300, seasonal_3 Loss: 0.0414 | 0.0316
Epoch 93/300, seasonal_3 Loss: 0.0396 | 0.0351
Epoch 94/300, seasonal_3 Loss: 0.0401 | 0.0321
Epoch 95/300, seasonal_3 Loss: 0.0384 | 0.0319
Epoch 96/300, seasonal_3 Loss: 0.0374 | 0.0329
Epoch 97/300, seasonal_3 Loss: 0.0376 | 0.0327
Epoch 98/300, seasonal_3 Loss: 0.0378 | 0.0334
Epoch 99/300, seasonal_3 Loss: 0.0370 | 0.0340
Epoch 100/300, seasonal_3 Loss: 0.0364 | 0.0343
Epoch 101/300, seasonal_3 Loss: 0.0359 | 0.0336
Epoch 102/300, seasonal_3 Loss: 0.0354 | 0.0335
Epoch 103/300, seasonal_3 Loss: 0.0349 | 0.0332
Epoch 104/300, seasonal_3 Loss: 0.0344 | 0.0350
Epoch 105/300, seasonal_3 Loss: 0.0344 | 0.0352
Epoch 106/300, seasonal_3 Loss: 0.0343 | 0.0380
Epoch 107/300, seasonal_3 Loss: 0.0344 | 0.0363
Epoch 108/300, seasonal_3 Loss: 0.0338 | 0.0363
Epoch 109/300, seasonal_3 Loss: 0.0336 | 0.0369
Epoch 110/300, seasonal_3 Loss: 0.0333 | 0.0367
Epoch 111/300, seasonal_3 Loss: 0.0331 | 0.0378
Epoch 112/300, seasonal_3 Loss: 0.0327 | 0.0371
Epoch 113/300, seasonal_3 Loss: 0.0325 | 0.0378
Epoch 114/300, seasonal_3 Loss: 0.0322 | 0.0374
Epoch 115/300, seasonal_3 Loss: 0.0320 | 0.0373
Epoch 116/300, seasonal_3 Loss: 0.0318 | 0.0369
Epoch 117/300, seasonal_3 Loss: 0.0317 | 0.0365
Epoch 118/300, seasonal_3 Loss: 0.0315 | 0.0360
Epoch 119/300, seasonal_3 Loss: 0.0313 | 0.0357
Epoch 120/300, seasonal_3 Loss: 0.0311 | 0.0360
Epoch 121/300, seasonal_3 Loss: 0.0309 | 0.0357
Epoch 122/300, seasonal_3 Loss: 0.0308 | 0.0365
Epoch 123/300, seasonal_3 Loss: 0.0306 | 0.0359
Epoch 124/300, seasonal_3 Loss: 0.0304 | 0.0364
Epoch 125/300, seasonal_3 Loss: 0.0302 | 0.0361
Epoch 126/300, seasonal_3 Loss: 0.0303 | 0.0371
Epoch 127/300, seasonal_3 Loss: 0.0302 | 0.0362
Epoch 128/300, seasonal_3 Loss: 0.0300 | 0.0376
Epoch 129/300, seasonal_3 Loss: 0.0299 | 0.0365
Epoch 130/300, seasonal_3 Loss: 0.0297 | 0.0383
Epoch 131/300, seasonal_3 Loss: 0.0301 | 0.0414
Epoch 132/300, seasonal_3 Loss: 0.0304 | 0.0461
Epoch 133/300, seasonal_3 Loss: 0.0307 | 0.0465
Epoch 134/300, seasonal_3 Loss: 0.0338 | 0.0379
Epoch 135/300, seasonal_3 Loss: 0.0362 | 0.0339
Epoch 136/300, seasonal_3 Loss: 0.0367 | 0.0391
Epoch 137/300, seasonal_3 Loss: 0.0348 | 0.0400
Epoch 138/300, seasonal_3 Loss: 0.0332 | 0.0356
Epoch 139/300, seasonal_3 Loss: 0.0312 | 0.0360
Epoch 140/300, seasonal_3 Loss: 0.0305 | 0.0363
Epoch 141/300, seasonal_3 Loss: 0.0292 | 0.0351
Epoch 142/300, seasonal_3 Loss: 0.0290 | 0.0368
Epoch 143/300, seasonal_3 Loss: 0.0288 | 0.0366
Epoch 144/300, seasonal_3 Loss: 0.0286 | 0.0374
Epoch 145/300, seasonal_3 Loss: 0.0284 | 0.0368
Epoch 146/300, seasonal_3 Loss: 0.0283 | 0.0371
Epoch 147/300, seasonal_3 Loss: 0.0282 | 0.0365
Epoch 148/300, seasonal_3 Loss: 0.0279 | 0.0363
Epoch 149/300, seasonal_3 Loss: 0.0279 | 0.0356
Epoch 150/300, seasonal_3 Loss: 0.0277 | 0.0348
Epoch 151/300, seasonal_3 Loss: 0.0276 | 0.0341
Epoch 152/300, seasonal_3 Loss: 0.0274 | 0.0341
Epoch 153/300, seasonal_3 Loss: 0.0272 | 0.0342
Epoch 154/300, seasonal_3 Loss: 0.0270 | 0.0340
Epoch 155/300, seasonal_3 Loss: 0.0269 | 0.0343
Epoch 156/300, seasonal_3 Loss: 0.0268 | 0.0344
Epoch 157/300, seasonal_3 Loss: 0.0267 | 0.0345
Epoch 158/300, seasonal_3 Loss: 0.0265 | 0.0346
Epoch 159/300, seasonal_3 Loss: 0.0264 | 0.0346
Epoch 160/300, seasonal_3 Loss: 0.0262 | 0.0351
Epoch 161/300, seasonal_3 Loss: 0.0261 | 0.0355
Epoch 162/300, seasonal_3 Loss: 0.0260 | 0.0364
Epoch 163/300, seasonal_3 Loss: 0.0260 | 0.0366
Epoch 164/300, seasonal_3 Loss: 0.0260 | 0.0380
Epoch 165/300, seasonal_3 Loss: 0.0261 | 0.0393
Epoch 166/300, seasonal_3 Loss: 0.0260 | 0.0397
Epoch 167/300, seasonal_3 Loss: 0.0260 | 0.0399
Epoch 168/300, seasonal_3 Loss: 0.0258 | 0.0405
Epoch 169/300, seasonal_3 Loss: 0.0256 | 0.0404
Epoch 170/300, seasonal_3 Loss: 0.0258 | 0.0394
Epoch 171/300, seasonal_3 Loss: 0.0257 | 0.0388
Epoch 172/300, seasonal_3 Loss: 0.0261 | 0.0380
Epoch 173/300, seasonal_3 Loss: 0.0263 | 0.0367
Epoch 174/300, seasonal_3 Loss: 0.0261 | 0.0368
Epoch 175/300, seasonal_3 Loss: 0.0261 | 0.0359
Epoch 176/300, seasonal_3 Loss: 0.0254 | 0.0372
Epoch 177/300, seasonal_3 Loss: 0.0251 | 0.0376
Epoch 178/300, seasonal_3 Loss: 0.0250 | 0.0395
Epoch 179/300, seasonal_3 Loss: 0.0248 | 0.0404
Epoch 180/300, seasonal_3 Loss: 0.0247 | 0.0407
Epoch 181/300, seasonal_3 Loss: 0.0247 | 0.0405
Epoch 182/300, seasonal_3 Loss: 0.0249 | 0.0393
Epoch 183/300, seasonal_3 Loss: 0.0249 | 0.0395
Epoch 184/300, seasonal_3 Loss: 0.0250 | 0.0394
Epoch 185/300, seasonal_3 Loss: 0.0250 | 0.0423
Epoch 186/300, seasonal_3 Loss: 0.0248 | 0.0406
Epoch 187/300, seasonal_3 Loss: 0.0273 | 0.0547
Epoch 188/300, seasonal_3 Loss: 0.0316 | 0.0429
Epoch 189/300, seasonal_3 Loss: 0.0281 | 0.0443
Epoch 190/300, seasonal_3 Loss: 0.0281 | 0.0385
Epoch 191/300, seasonal_3 Loss: 0.0280 | 0.0475
Epoch 192/300, seasonal_3 Loss: 0.0294 | 0.0400
Epoch 193/300, seasonal_3 Loss: 0.0262 | 0.0393
Epoch 194/300, seasonal_3 Loss: 0.0254 | 0.0380
Epoch 195/300, seasonal_3 Loss: 0.0250 | 0.0372
Epoch 196/300, seasonal_3 Loss: 0.0248 | 0.0371
Epoch 197/300, seasonal_3 Loss: 0.0244 | 0.0370
Epoch 198/300, seasonal_3 Loss: 0.0243 | 0.0371
Epoch 199/300, seasonal_3 Loss: 0.0241 | 0.0373
Epoch 200/300, seasonal_3 Loss: 0.0239 | 0.0376
Epoch 201/300, seasonal_3 Loss: 0.0237 | 0.0379
Epoch 202/300, seasonal_3 Loss: 0.0236 | 0.0383
Epoch 203/300, seasonal_3 Loss: 0.0235 | 0.0386
Epoch 204/300, seasonal_3 Loss: 0.0234 | 0.0390
Epoch 205/300, seasonal_3 Loss: 0.0233 | 0.0393
Epoch 206/300, seasonal_3 Loss: 0.0232 | 0.0396
Epoch 207/300, seasonal_3 Loss: 0.0231 | 0.0399
Epoch 208/300, seasonal_3 Loss: 0.0230 | 0.0400
Epoch 209/300, seasonal_3 Loss: 0.0229 | 0.0403
Epoch 210/300, seasonal_3 Loss: 0.0229 | 0.0404
Epoch 211/300, seasonal_3 Loss: 0.0228 | 0.0405
Epoch 212/300, seasonal_3 Loss: 0.0227 | 0.0406
Epoch 213/300, seasonal_3 Loss: 0.0227 | 0.0407
Epoch 214/300, seasonal_3 Loss: 0.0226 | 0.0408
Epoch 215/300, seasonal_3 Loss: 0.0226 | 0.0409
Epoch 216/300, seasonal_3 Loss: 0.0225 | 0.0410
Epoch 217/300, seasonal_3 Loss: 0.0225 | 0.0410
Epoch 218/300, seasonal_3 Loss: 0.0224 | 0.0410
Epoch 219/300, seasonal_3 Loss: 0.0224 | 0.0410
Epoch 220/300, seasonal_3 Loss: 0.0223 | 0.0410
Epoch 221/300, seasonal_3 Loss: 0.0223 | 0.0409
Epoch 222/300, seasonal_3 Loss: 0.0223 | 0.0409
Epoch 223/300, seasonal_3 Loss: 0.0223 | 0.0408
Epoch 224/300, seasonal_3 Loss: 0.0223 | 0.0408
Epoch 225/300, seasonal_3 Loss: 0.0223 | 0.0407
Epoch 226/300, seasonal_3 Loss: 0.0223 | 0.0410
Epoch 227/300, seasonal_3 Loss: 0.0223 | 0.0412
Epoch 228/300, seasonal_3 Loss: 0.0222 | 0.0416
Epoch 229/300, seasonal_3 Loss: 0.0221 | 0.0417
Epoch 230/300, seasonal_3 Loss: 0.0221 | 0.0413
Epoch 231/300, seasonal_3 Loss: 0.0221 | 0.0404
Epoch 232/300, seasonal_3 Loss: 0.0223 | 0.0403
Epoch 233/300, seasonal_3 Loss: 0.0223 | 0.0414
Epoch 234/300, seasonal_3 Loss: 0.0224 | 0.0420
Epoch 235/300, seasonal_3 Loss: 0.0225 | 0.0423
Epoch 236/300, seasonal_3 Loss: 0.0226 | 0.0418
Epoch 237/300, seasonal_3 Loss: 0.0225 | 0.0418
Epoch 238/300, seasonal_3 Loss: 0.0226 | 0.0406
Epoch 239/300, seasonal_3 Loss: 0.0222 | 0.0424
Epoch 240/300, seasonal_3 Loss: 0.0220 | 0.0419
Epoch 241/300, seasonal_3 Loss: 0.0220 | 0.0427
Epoch 242/300, seasonal_3 Loss: 0.0221 | 0.0419
Epoch 243/300, seasonal_3 Loss: 0.0221 | 0.0417
Epoch 244/300, seasonal_3 Loss: 0.0221 | 0.0412
Epoch 245/300, seasonal_3 Loss: 0.0222 | 0.0409
Epoch 246/300, seasonal_3 Loss: 0.0223 | 0.0408
Epoch 247/300, seasonal_3 Loss: 0.0224 | 0.0413
Epoch 248/300, seasonal_3 Loss: 0.0225 | 0.0423
Epoch 249/300, seasonal_3 Loss: 0.0227 | 0.0440
Epoch 250/300, seasonal_3 Loss: 0.0229 | 0.0473
Epoch 251/300, seasonal_3 Loss: 0.0232 | 0.0518
Epoch 252/300, seasonal_3 Loss: 0.0232 | 0.0514
Epoch 253/300, seasonal_3 Loss: 0.0235 | 0.0471
Epoch 254/300, seasonal_3 Loss: 0.0236 | 0.0423
Epoch 255/300, seasonal_3 Loss: 0.0243 | 0.0436
Epoch 256/300, seasonal_3 Loss: 0.0242 | 0.0466
Epoch 257/300, seasonal_3 Loss: 0.0240 | 0.0454
Epoch 258/300, seasonal_3 Loss: 0.0233 | 0.0441
Epoch 259/300, seasonal_3 Loss: 0.0228 | 0.0436
Epoch 260/300, seasonal_3 Loss: 0.0226 | 0.0429
Epoch 261/300, seasonal_3 Loss: 0.0222 | 0.0420
Epoch 262/300, seasonal_3 Loss: 0.0221 | 0.0417
Epoch 263/300, seasonal_3 Loss: 0.0219 | 0.0412
Epoch 264/300, seasonal_3 Loss: 0.0218 | 0.0412
Epoch 265/300, seasonal_3 Loss: 0.0216 | 0.0411
Epoch 266/300, seasonal_3 Loss: 0.0215 | 0.0411
Epoch 267/300, seasonal_3 Loss: 0.0214 | 0.0412
Epoch 268/300, seasonal_3 Loss: 0.0213 | 0.0413
Epoch 269/300, seasonal_3 Loss: 0.0212 | 0.0414
Epoch 270/300, seasonal_3 Loss: 0.0211 | 0.0416
Epoch 271/300, seasonal_3 Loss: 0.0211 | 0.0418
Epoch 272/300, seasonal_3 Loss: 0.0210 | 0.0421
Epoch 273/300, seasonal_3 Loss: 0.0210 | 0.0424
Epoch 274/300, seasonal_3 Loss: 0.0209 | 0.0426
Epoch 275/300, seasonal_3 Loss: 0.0209 | 0.0428
Epoch 276/300, seasonal_3 Loss: 0.0208 | 0.0428
Epoch 277/300, seasonal_3 Loss: 0.0208 | 0.0428
Epoch 278/300, seasonal_3 Loss: 0.0207 | 0.0428
Epoch 279/300, seasonal_3 Loss: 0.0207 | 0.0426
Epoch 280/300, seasonal_3 Loss: 0.0206 | 0.0425
Epoch 281/300, seasonal_3 Loss: 0.0206 | 0.0424
Epoch 282/300, seasonal_3 Loss: 0.0205 | 0.0423
Epoch 283/300, seasonal_3 Loss: 0.0205 | 0.0423
Epoch 284/300, seasonal_3 Loss: 0.0204 | 0.0422
Epoch 285/300, seasonal_3 Loss: 0.0204 | 0.0422
Epoch 286/300, seasonal_3 Loss: 0.0204 | 0.0422
Epoch 287/300, seasonal_3 Loss: 0.0203 | 0.0422
Epoch 288/300, seasonal_3 Loss: 0.0203 | 0.0423
Epoch 289/300, seasonal_3 Loss: 0.0203 | 0.0423
Epoch 290/300, seasonal_3 Loss: 0.0202 | 0.0424
Epoch 291/300, seasonal_3 Loss: 0.0202 | 0.0425
Epoch 292/300, seasonal_3 Loss: 0.0202 | 0.0426
Epoch 293/300, seasonal_3 Loss: 0.0202 | 0.0425
Epoch 294/300, seasonal_3 Loss: 0.0201 | 0.0428
Epoch 295/300, seasonal_3 Loss: 0.0201 | 0.0426
Epoch 296/300, seasonal_3 Loss: 0.0201 | 0.0431
Epoch 297/300, seasonal_3 Loss: 0.0201 | 0.0428
Epoch 298/300, seasonal_3 Loss: 0.0201 | 0.0432
Epoch 299/300, seasonal_3 Loss: 0.0201 | 0.0428
Epoch 300/300, seasonal_3 Loss: 0.0201 | 0.0435
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9287370973886402, 'learning_rate': 0.0009989030292173794, 'batch_size': 69, 'step_size': 6, 'gamma': 0.7993059181398754}
Epoch 1/300, resid Loss: 0.3450 | 0.1040
Epoch 2/300, resid Loss: 0.1393 | 0.0842
Epoch 3/300, resid Loss: 0.1218 | 0.0952
Epoch 4/300, resid Loss: 0.1082 | 0.0791
Epoch 5/300, resid Loss: 0.1021 | 0.0676
Epoch 6/300, resid Loss: 0.1071 | 0.0726
Epoch 7/300, resid Loss: 0.1023 | 0.0585
Epoch 8/300, resid Loss: 0.1062 | 0.0584
Epoch 9/300, resid Loss: 0.1052 | 0.0579
Epoch 10/300, resid Loss: 0.0929 | 0.0500
Epoch 11/300, resid Loss: 0.0948 | 0.0558
Epoch 12/300, resid Loss: 0.0890 | 0.0542
Epoch 13/300, resid Loss: 0.0890 | 0.0550
Epoch 14/300, resid Loss: 0.0871 | 0.0494
Epoch 15/300, resid Loss: 0.0804 | 0.0426
Epoch 16/300, resid Loss: 0.0782 | 0.0418
Epoch 17/300, resid Loss: 0.0769 | 0.0406
Epoch 18/300, resid Loss: 0.0754 | 0.0407
Epoch 19/300, resid Loss: 0.0744 | 0.0403
Epoch 20/300, resid Loss: 0.0744 | 0.0434
Epoch 21/300, resid Loss: 0.0763 | 0.0462
Epoch 22/300, resid Loss: 0.0752 | 0.0399
Epoch 23/300, resid Loss: 0.0732 | 0.0373
Epoch 24/300, resid Loss: 0.0728 | 0.0369
Epoch 25/300, resid Loss: 0.0720 | 0.0369
Epoch 26/300, resid Loss: 0.0709 | 0.0366
Epoch 27/300, resid Loss: 0.0699 | 0.0363
Epoch 28/300, resid Loss: 0.0692 | 0.0362
Epoch 29/300, resid Loss: 0.0688 | 0.0359
Epoch 30/300, resid Loss: 0.0683 | 0.0354
Epoch 31/300, resid Loss: 0.0680 | 0.0356
Epoch 32/300, resid Loss: 0.0677 | 0.0352
Epoch 33/300, resid Loss: 0.0675 | 0.0347
Epoch 34/300, resid Loss: 0.0672 | 0.0353
Epoch 35/300, resid Loss: 0.0670 | 0.0350
Epoch 36/300, resid Loss: 0.0668 | 0.0348
Epoch 37/300, resid Loss: 0.0666 | 0.0359
Epoch 38/300, resid Loss: 0.0666 | 0.0361
Epoch 39/300, resid Loss: 0.0666 | 0.0362
Epoch 40/300, resid Loss: 0.0667 | 0.0357
Epoch 41/300, resid Loss: 0.0667 | 0.0355
Epoch 42/300, resid Loss: 0.0666 | 0.0353
Epoch 43/300, resid Loss: 0.0666 | 0.0337
Epoch 44/300, resid Loss: 0.0665 | 0.0334
Epoch 45/300, resid Loss: 0.0662 | 0.0332
Epoch 46/300, resid Loss: 0.0661 | 0.0329
Epoch 47/300, resid Loss: 0.0659 | 0.0329
Epoch 48/300, resid Loss: 0.0657 | 0.0329
Epoch 49/300, resid Loss: 0.0657 | 0.0320
Epoch 50/300, resid Loss: 0.0655 | 0.0320
Epoch 51/300, resid Loss: 0.0653 | 0.0320
Epoch 52/300, resid Loss: 0.0651 | 0.0320
Epoch 53/300, resid Loss: 0.0650 | 0.0319
Epoch 54/300, resid Loss: 0.0648 | 0.0319
Epoch 55/300, resid Loss: 0.0647 | 0.0317
Epoch 56/300, resid Loss: 0.0645 | 0.0316
Epoch 57/300, resid Loss: 0.0643 | 0.0315
Epoch 58/300, resid Loss: 0.0641 | 0.0313
Epoch 59/300, resid Loss: 0.0640 | 0.0313
Epoch 60/300, resid Loss: 0.0638 | 0.0312
Epoch 61/300, resid Loss: 0.0637 | 0.0312
Epoch 62/300, resid Loss: 0.0636 | 0.0312
Epoch 63/300, resid Loss: 0.0636 | 0.0312
Epoch 64/300, resid Loss: 0.0635 | 0.0312
Epoch 65/300, resid Loss: 0.0635 | 0.0312
Epoch 66/300, resid Loss: 0.0634 | 0.0312
Epoch 67/300, resid Loss: 0.0634 | 0.0313
Epoch 68/300, resid Loss: 0.0635 | 0.0313
Epoch 69/300, resid Loss: 0.0634 | 0.0313
Epoch 70/300, resid Loss: 0.0634 | 0.0314
Epoch 71/300, resid Loss: 0.0635 | 0.0314
Epoch 72/300, resid Loss: 0.0634 | 0.0314
Epoch 73/300, resid Loss: 0.0633 | 0.0312
Epoch 74/300, resid Loss: 0.0632 | 0.0312
Epoch 75/300, resid Loss: 0.0631 | 0.0312
Epoch 76/300, resid Loss: 0.0631 | 0.0311
Epoch 77/300, resid Loss: 0.0630 | 0.0311
Epoch 78/300, resid Loss: 0.0630 | 0.0311
Epoch 79/300, resid Loss: 0.0630 | 0.0311
Epoch 80/300, resid Loss: 0.0629 | 0.0311
Epoch 81/300, resid Loss: 0.0629 | 0.0311
Epoch 82/300, resid Loss: 0.0629 | 0.0311
Epoch 83/300, resid Loss: 0.0629 | 0.0310
Epoch 84/300, resid Loss: 0.0629 | 0.0310
Epoch 85/300, resid Loss: 0.0628 | 0.0310
Epoch 86/300, resid Loss: 0.0628 | 0.0310
Epoch 87/300, resid Loss: 0.0628 | 0.0310
Epoch 88/300, resid Loss: 0.0628 | 0.0310
Epoch 89/300, resid Loss: 0.0628 | 0.0310
Epoch 90/300, resid Loss: 0.0628 | 0.0310
Epoch 91/300, resid Loss: 0.0628 | 0.0310
Epoch 92/300, resid Loss: 0.0627 | 0.0310
Epoch 93/300, resid Loss: 0.0627 | 0.0310
Epoch 94/300, resid Loss: 0.0627 | 0.0310
Epoch 95/300, resid Loss: 0.0627 | 0.0310
Epoch 96/300, resid Loss: 0.0627 | 0.0309
Epoch 97/300, resid Loss: 0.0627 | 0.0309
Epoch 98/300, resid Loss: 0.0627 | 0.0309
Epoch 99/300, resid Loss: 0.0627 | 0.0309
Epoch 100/300, resid Loss: 0.0627 | 0.0309
Epoch 101/300, resid Loss: 0.0627 | 0.0309
Epoch 102/300, resid Loss: 0.0627 | 0.0309
Epoch 103/300, resid Loss: 0.0627 | 0.0309
Epoch 104/300, resid Loss: 0.0627 | 0.0309
Epoch 105/300, resid Loss: 0.0627 | 0.0309
Epoch 106/300, resid Loss: 0.0627 | 0.0309
Epoch 107/300, resid Loss: 0.0627 | 0.0309
Epoch 108/300, resid Loss: 0.0627 | 0.0309
Epoch 109/300, resid Loss: 0.0627 | 0.0309
Epoch 110/300, resid Loss: 0.0627 | 0.0309
Epoch 111/300, resid Loss: 0.0627 | 0.0309
Epoch 112/300, resid Loss: 0.0627 | 0.0309
Epoch 113/300, resid Loss: 0.0627 | 0.0309
Epoch 114/300, resid Loss: 0.0627 | 0.0309
Epoch 115/300, resid Loss: 0.0627 | 0.0309
Epoch 116/300, resid Loss: 0.0626 | 0.0309
Epoch 117/300, resid Loss: 0.0626 | 0.0309
Epoch 118/300, resid Loss: 0.0626 | 0.0309
Epoch 119/300, resid Loss: 0.0626 | 0.0309
Epoch 120/300, resid Loss: 0.0626 | 0.0309
Epoch 121/300, resid Loss: 0.0626 | 0.0309
Epoch 122/300, resid Loss: 0.0626 | 0.0309
Epoch 123/300, resid Loss: 0.0626 | 0.0309
Epoch 124/300, resid Loss: 0.0626 | 0.0309
Epoch 125/300, resid Loss: 0.0626 | 0.0309
Epoch 126/300, resid Loss: 0.0626 | 0.0309
Epoch 127/300, resid Loss: 0.0626 | 0.0309
Epoch 128/300, resid Loss: 0.0626 | 0.0309
Epoch 129/300, resid Loss: 0.0626 | 0.0309
Epoch 130/300, resid Loss: 0.0626 | 0.0309
Epoch 131/300, resid Loss: 0.0626 | 0.0309
Epoch 132/300, resid Loss: 0.0626 | 0.0309
Epoch 133/300, resid Loss: 0.0626 | 0.0309
Epoch 134/300, resid Loss: 0.0626 | 0.0309
Epoch 135/300, resid Loss: 0.0626 | 0.0309
Epoch 136/300, resid Loss: 0.0626 | 0.0309
Epoch 137/300, resid Loss: 0.0626 | 0.0309
Epoch 138/300, resid Loss: 0.0626 | 0.0309
Epoch 139/300, resid Loss: 0.0626 | 0.0309
Epoch 140/300, resid Loss: 0.0626 | 0.0309
Epoch 141/300, resid Loss: 0.0626 | 0.0309
Epoch 142/300, resid Loss: 0.0626 | 0.0309
Epoch 143/300, resid Loss: 0.0626 | 0.0309
Epoch 144/300, resid Loss: 0.0626 | 0.0309
Epoch 145/300, resid Loss: 0.0626 | 0.0309
Epoch 146/300, resid Loss: 0.0626 | 0.0309
Epoch 147/300, resid Loss: 0.0626 | 0.0309
Epoch 148/300, resid Loss: 0.0626 | 0.0309
Epoch 149/300, resid Loss: 0.0626 | 0.0309
Epoch 150/300, resid Loss: 0.0626 | 0.0309
Epoch 151/300, resid Loss: 0.0626 | 0.0309
Epoch 152/300, resid Loss: 0.0626 | 0.0309
Epoch 153/300, resid Loss: 0.0626 | 0.0309
Epoch 154/300, resid Loss: 0.0626 | 0.0309
Epoch 155/300, resid Loss: 0.0626 | 0.0309
Epoch 156/300, resid Loss: 0.0626 | 0.0309
Epoch 157/300, resid Loss: 0.0626 | 0.0309
Epoch 158/300, resid Loss: 0.0626 | 0.0309
Epoch 159/300, resid Loss: 0.0626 | 0.0309
Epoch 160/300, resid Loss: 0.0626 | 0.0309
Epoch 161/300, resid Loss: 0.0626 | 0.0309
Epoch 162/300, resid Loss: 0.0626 | 0.0309
Epoch 163/300, resid Loss: 0.0626 | 0.0309
Epoch 164/300, resid Loss: 0.0626 | 0.0309
Epoch 165/300, resid Loss: 0.0626 | 0.0309
Epoch 166/300, resid Loss: 0.0626 | 0.0309
Epoch 167/300, resid Loss: 0.0626 | 0.0309
Epoch 168/300, resid Loss: 0.0626 | 0.0309
Epoch 169/300, resid Loss: 0.0626 | 0.0309
Epoch 170/300, resid Loss: 0.0626 | 0.0309
Epoch 171/300, resid Loss: 0.0626 | 0.0309
Epoch 172/300, resid Loss: 0.0626 | 0.0309
Epoch 173/300, resid Loss: 0.0626 | 0.0309
Epoch 174/300, resid Loss: 0.0626 | 0.0309
Epoch 175/300, resid Loss: 0.0626 | 0.0309
Epoch 176/300, resid Loss: 0.0626 | 0.0309
Epoch 177/300, resid Loss: 0.0626 | 0.0309
Epoch 178/300, resid Loss: 0.0626 | 0.0309
Epoch 179/300, resid Loss: 0.0626 | 0.0309
Epoch 180/300, resid Loss: 0.0626 | 0.0309
Epoch 181/300, resid Loss: 0.0626 | 0.0309
Epoch 182/300, resid Loss: 0.0626 | 0.0309
Epoch 183/300, resid Loss: 0.0626 | 0.0309
Epoch 184/300, resid Loss: 0.0626 | 0.0309
Epoch 185/300, resid Loss: 0.0626 | 0.0309
Epoch 186/300, resid Loss: 0.0626 | 0.0309
Epoch 187/300, resid Loss: 0.0626 | 0.0309
Epoch 188/300, resid Loss: 0.0626 | 0.0309
Epoch 189/300, resid Loss: 0.0626 | 0.0309
Epoch 190/300, resid Loss: 0.0626 | 0.0309
Epoch 191/300, resid Loss: 0.0626 | 0.0309
Epoch 192/300, resid Loss: 0.0626 | 0.0309
Epoch 193/300, resid Loss: 0.0626 | 0.0309
Epoch 194/300, resid Loss: 0.0626 | 0.0309
Epoch 195/300, resid Loss: 0.0626 | 0.0309
Epoch 196/300, resid Loss: 0.0626 | 0.0309
Epoch 197/300, resid Loss: 0.0626 | 0.0309
Early stopping for resid
Runtime (seconds): 3070.398477077484
0.0009876273611041503
[189.80182]
[-1.7365587]
[-1.6967777]
[0.34852874]
[-1.0011073]
[8.64174]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 129.55834670574404
RMSE: 11.382369995117188
MAE: 11.382369995117188
R-squared: nan
[194.35764]
