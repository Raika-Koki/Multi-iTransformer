[32m[I 2025-02-05 01:46:07,582][0m A new study created in memory with name: no-name-69c14b63-82b4-48ce-84a3-b5822e12bcf0[0m
[32m[I 2025-02-05 01:46:46,092][0m Trial 0 finished with value: 0.3702876122252455 and parameters: {'observation_period_num': 212, 'train_rates': 0.8876877883414336, 'learning_rate': 1.3700841642880712e-05, 'batch_size': 147, 'step_size': 9, 'gamma': 0.8378197329168209}. Best is trial 0 with value: 0.3702876122252455.[0m
[32m[I 2025-02-05 01:47:37,486][0m Trial 1 finished with value: 0.05819559134374377 and parameters: {'observation_period_num': 71, 'train_rates': 0.8471985956171395, 'learning_rate': 0.0003998975617321439, 'batch_size': 110, 'step_size': 6, 'gamma': 0.842345329817771}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:50:53,449][0m Trial 2 finished with value: 0.19651558772814506 and parameters: {'observation_period_num': 66, 'train_rates': 0.7151535723797529, 'learning_rate': 0.0005982082658776594, 'batch_size': 24, 'step_size': 10, 'gamma': 0.9545693525413073}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:51:14,012][0m Trial 3 finished with value: 0.16682456869179021 and parameters: {'observation_period_num': 23, 'train_rates': 0.6245963434623878, 'learning_rate': 0.0002993387164876516, 'batch_size': 240, 'step_size': 15, 'gamma': 0.7560000155533795}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:51:56,650][0m Trial 4 finished with value: 0.23507113754749298 and parameters: {'observation_period_num': 182, 'train_rates': 0.9790581811740835, 'learning_rate': 0.0005596829290571399, 'batch_size': 139, 'step_size': 2, 'gamma': 0.7837556947270996}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:52:40,350][0m Trial 5 finished with value: 0.17059479653835297 and parameters: {'observation_period_num': 155, 'train_rates': 0.960000236249581, 'learning_rate': 4.524653588358404e-05, 'batch_size': 139, 'step_size': 15, 'gamma': 0.956274830860547}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:53:02,993][0m Trial 6 finished with value: 0.31953576305410364 and parameters: {'observation_period_num': 166, 'train_rates': 0.6741945930520902, 'learning_rate': 3.8982863170251055e-05, 'batch_size': 225, 'step_size': 12, 'gamma': 0.9838901595288579}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:53:42,207][0m Trial 7 finished with value: 0.39378214975471026 and parameters: {'observation_period_num': 168, 'train_rates': 0.7390895035840813, 'learning_rate': 2.3203457143794293e-05, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8086197029850535}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:54:18,946][0m Trial 8 finished with value: 0.13702995950489563 and parameters: {'observation_period_num': 14, 'train_rates': 0.6540514056491082, 'learning_rate': 0.00018584623541882783, 'batch_size': 133, 'step_size': 9, 'gamma': 0.8933232714400907}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:55:16,165][0m Trial 9 finished with value: 0.41566550115059164 and parameters: {'observation_period_num': 185, 'train_rates': 0.9478568507018683, 'learning_rate': 1.8333268557887106e-05, 'batch_size': 105, 'step_size': 3, 'gamma': 0.8846102046081832}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:56:45,745][0m Trial 10 finished with value: 0.4625522852434543 and parameters: {'observation_period_num': 101, 'train_rates': 0.8302357604347415, 'learning_rate': 5.05344337086406e-06, 'batch_size': 59, 'step_size': 5, 'gamma': 0.8467914096438239}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:57:15,564][0m Trial 11 finished with value: 0.06454194618721401 and parameters: {'observation_period_num': 18, 'train_rates': 0.7981277641982023, 'learning_rate': 0.0001276347653819108, 'batch_size': 193, 'step_size': 6, 'gamma': 0.904777520102329}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:57:43,854][0m Trial 12 finished with value: 0.07952096929707503 and parameters: {'observation_period_num': 67, 'train_rates': 0.7956783900980323, 'learning_rate': 0.00011411681519460566, 'batch_size': 192, 'step_size': 6, 'gamma': 0.9133031651486249}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:58:15,979][0m Trial 13 finished with value: 0.9893023856986123 and parameters: {'observation_period_num': 52, 'train_rates': 0.8595436351026701, 'learning_rate': 1.3163546365380923e-06, 'batch_size': 182, 'step_size': 6, 'gamma': 0.8476579608713051}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 01:59:17,643][0m Trial 14 finished with value: 0.22615092339507048 and parameters: {'observation_period_num': 107, 'train_rates': 0.7580150556801775, 'learning_rate': 8.832268774749474e-05, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9227224026887746}. Best is trial 1 with value: 0.05819559134374377.[0m
Early stopping at epoch 96
[32m[I 2025-02-05 01:59:48,160][0m Trial 15 finished with value: 0.1264605581705012 and parameters: {'observation_period_num': 47, 'train_rates': 0.9077475875802153, 'learning_rate': 0.0008881473791144453, 'batch_size': 189, 'step_size': 1, 'gamma': 0.8611426031091939}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 02:00:22,638][0m Trial 16 finished with value: 0.09840344546205831 and parameters: {'observation_period_num': 104, 'train_rates': 0.8086907603307939, 'learning_rate': 0.0002744446712568498, 'batch_size': 163, 'step_size': 7, 'gamma': 0.8147142717634179}. Best is trial 1 with value: 0.05819559134374377.[0m
[32m[I 2025-02-05 02:00:50,185][0m Trial 17 finished with value: 0.04792470263949992 and parameters: {'observation_period_num': 8, 'train_rates': 0.8585046538884792, 'learning_rate': 0.00010193347508596195, 'batch_size': 216, 'step_size': 12, 'gamma': 0.9132753054681308}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:01:15,333][0m Trial 18 finished with value: 0.1270174953020195 and parameters: {'observation_period_num': 84, 'train_rates': 0.9066593615793668, 'learning_rate': 7.769644026796321e-05, 'batch_size': 252, 'step_size': 12, 'gamma': 0.8768432444807104}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:02:20,668][0m Trial 19 finished with value: 0.2079926333805122 and parameters: {'observation_period_num': 133, 'train_rates': 0.8572245739519918, 'learning_rate': 9.711903692105021e-06, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9366235033025833}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:02:45,038][0m Trial 20 finished with value: 0.1850053627506993 and parameters: {'observation_period_num': 38, 'train_rates': 0.7576988303357536, 'learning_rate': 0.00038368420108169434, 'batch_size': 227, 'step_size': 8, 'gamma': 0.8226880753579783}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:03:12,670][0m Trial 21 finished with value: 0.05012044053143649 and parameters: {'observation_period_num': 6, 'train_rates': 0.8379686022620659, 'learning_rate': 0.00014678415705077626, 'batch_size': 212, 'step_size': 5, 'gamma': 0.908339494690187}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:03:42,569][0m Trial 22 finished with value: 0.04841238234240936 and parameters: {'observation_period_num': 6, 'train_rates': 0.8451706297274776, 'learning_rate': 0.0001862838476643398, 'batch_size': 206, 'step_size': 3, 'gamma': 0.933112543548219}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:04:09,663][0m Trial 23 finished with value: 0.0607917376777582 and parameters: {'observation_period_num': 7, 'train_rates': 0.8843800128492353, 'learning_rate': 7.168548182500814e-05, 'batch_size': 219, 'step_size': 4, 'gamma': 0.9421508802283901}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:04:40,886][0m Trial 24 finished with value: 0.07620704919099808 and parameters: {'observation_period_num': 33, 'train_rates': 0.9305591899906303, 'learning_rate': 0.00018580230077664818, 'batch_size': 209, 'step_size': 1, 'gamma': 0.9857035755105202}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:05:12,718][0m Trial 25 finished with value: 0.2521542375542454 and parameters: {'observation_period_num': 228, 'train_rates': 0.822530575822637, 'learning_rate': 5.498440047502179e-05, 'batch_size': 169, 'step_size': 3, 'gamma': 0.9338548804858265}. Best is trial 17 with value: 0.04792470263949992.[0m
[32m[I 2025-02-05 02:05:37,893][0m Trial 26 finished with value: 0.04776595193873007 and parameters: {'observation_period_num': 5, 'train_rates': 0.8722627306883742, 'learning_rate': 0.0001648689973796673, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9657671115917065}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:06:01,141][0m Trial 27 finished with value: 0.0655605409855712 and parameters: {'observation_period_num': 38, 'train_rates': 0.8751350033821984, 'learning_rate': 0.00019006142156674052, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9636644290846775}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:06:28,901][0m Trial 28 finished with value: 0.1126248851792583 and parameters: {'observation_period_num': 26, 'train_rates': 0.9187764595966801, 'learning_rate': 2.926054162066914e-05, 'batch_size': 237, 'step_size': 2, 'gamma': 0.9667460417726103}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:06:54,904][0m Trial 29 finished with value: 0.4625652557584697 and parameters: {'observation_period_num': 54, 'train_rates': 0.770150883143627, 'learning_rate': 7.672282157212364e-06, 'batch_size': 207, 'step_size': 8, 'gamma': 0.9297787020814525}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:07:30,757][0m Trial 30 finished with value: 0.6697110486803232 and parameters: {'observation_period_num': 90, 'train_rates': 0.8873449517807034, 'learning_rate': 2.5332490989845955e-06, 'batch_size': 161, 'step_size': 11, 'gamma': 0.8968374817471646}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:07:58,777][0m Trial 31 finished with value: 0.049995059747960285 and parameters: {'observation_period_num': 8, 'train_rates': 0.8350716881206649, 'learning_rate': 0.00014105662258720553, 'batch_size': 214, 'step_size': 4, 'gamma': 0.913836445549695}. Best is trial 26 with value: 0.04776595193873007.[0m
[32m[I 2025-02-05 02:08:24,031][0m Trial 32 finished with value: 0.04739701811382944 and parameters: {'observation_period_num': 25, 'train_rates': 0.8641543874569252, 'learning_rate': 0.000217388737320949, 'batch_size': 239, 'step_size': 3, 'gamma': 0.9478909475363734}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:08:49,061][0m Trial 33 finished with value: 0.05514743348414248 and parameters: {'observation_period_num': 27, 'train_rates': 0.8576624752199058, 'learning_rate': 0.0002492145095547076, 'batch_size': 237, 'step_size': 2, 'gamma': 0.9460512424040395}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:09:13,951][0m Trial 34 finished with value: 0.10576047889949862 and parameters: {'observation_period_num': 60, 'train_rates': 0.8750450923116784, 'learning_rate': 0.0004916036460326413, 'batch_size': 244, 'step_size': 3, 'gamma': 0.9714417871187255}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:09:41,636][0m Trial 35 finished with value: 0.07628807948844243 and parameters: {'observation_period_num': 41, 'train_rates': 0.9009897184622887, 'learning_rate': 0.00034218572375050083, 'batch_size': 231, 'step_size': 1, 'gamma': 0.952256062443663}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:10:06,435][0m Trial 36 finished with value: 0.1200336366891861 and parameters: {'observation_period_num': 78, 'train_rates': 0.9370343163905089, 'learning_rate': 0.0009326209794955941, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9240952296457905}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:10:39,694][0m Trial 37 finished with value: 0.07056894153356552 and parameters: {'observation_period_num': 19, 'train_rates': 0.9833070386921898, 'learning_rate': 5.1452779968605695e-05, 'batch_size': 203, 'step_size': 3, 'gamma': 0.9749174955446894}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:11:03,585][0m Trial 38 finished with value: 0.14353251755237578 and parameters: {'observation_period_num': 129, 'train_rates': 0.8146123835180251, 'learning_rate': 0.0006157290127912651, 'batch_size': 240, 'step_size': 7, 'gamma': 0.9557715960516688}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:11:30,832][0m Trial 39 finished with value: 0.062332415268787564 and parameters: {'observation_period_num': 5, 'train_rates': 0.8481459698889028, 'learning_rate': 8.901150950049542e-05, 'batch_size': 227, 'step_size': 14, 'gamma': 0.7519450358682362}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:12:03,578][0m Trial 40 finished with value: 0.19208361579807376 and parameters: {'observation_period_num': 26, 'train_rates': 0.7812776786229602, 'learning_rate': 0.00019824601339311893, 'batch_size': 170, 'step_size': 10, 'gamma': 0.9894598258332453}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:12:29,304][0m Trial 41 finished with value: 0.05908569316679274 and parameters: {'observation_period_num': 16, 'train_rates': 0.8256364061557524, 'learning_rate': 0.0001335986619387331, 'batch_size': 221, 'step_size': 4, 'gamma': 0.9182250781138969}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:12:58,271][0m Trial 42 finished with value: 0.06552884944619772 and parameters: {'observation_period_num': 5, 'train_rates': 0.8705154912193489, 'learning_rate': 0.00023016501943270144, 'batch_size': 199, 'step_size': 2, 'gamma': 0.8899851118319883}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:13:26,134][0m Trial 43 finished with value: 0.05690201901291546 and parameters: {'observation_period_num': 30, 'train_rates': 0.8443790940301414, 'learning_rate': 0.00010059535524924026, 'batch_size': 216, 'step_size': 4, 'gamma': 0.9429495066804532}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:13:51,869][0m Trial 44 finished with value: 0.17284204238845455 and parameters: {'observation_period_num': 18, 'train_rates': 0.8948560095066662, 'learning_rate': 3.5077040654627034e-05, 'batch_size': 246, 'step_size': 3, 'gamma': 0.8670471531037334}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:18:34,258][0m Trial 45 finished with value: 0.2596844049589582 and parameters: {'observation_period_num': 200, 'train_rates': 0.7325065987358524, 'learning_rate': 6.49861504889898e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9034505965256383}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:19:03,869][0m Trial 46 finished with value: 0.20936379260980287 and parameters: {'observation_period_num': 47, 'train_rates': 0.6866979615387818, 'learning_rate': 0.00014753091849264595, 'batch_size': 180, 'step_size': 5, 'gamma': 0.769015304315912}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:19:38,018][0m Trial 47 finished with value: 0.19373176541853276 and parameters: {'observation_period_num': 244, 'train_rates': 0.7843864445669968, 'learning_rate': 0.00033104639449760547, 'batch_size': 149, 'step_size': 4, 'gamma': 0.9577551859559448}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:20:09,860][0m Trial 48 finished with value: 0.09207013088260524 and parameters: {'observation_period_num': 67, 'train_rates': 0.8386326864403687, 'learning_rate': 0.0005613697621165431, 'batch_size': 182, 'step_size': 9, 'gamma': 0.975759798679397}. Best is trial 32 with value: 0.04739701811382944.[0m
[32m[I 2025-02-05 02:22:04,182][0m Trial 49 finished with value: 0.06181326341933348 and parameters: {'observation_period_num': 16, 'train_rates': 0.8070053926769134, 'learning_rate': 1.9293406023582123e-05, 'batch_size': 46, 'step_size': 5, 'gamma': 0.9152793818318592}. Best is trial 32 with value: 0.04739701811382944.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.9298 | 0.6464
Epoch 2/300, Loss: 0.3286 | 0.5188
Epoch 3/300, Loss: 0.3113 | 0.3453
Epoch 4/300, Loss: 0.3453 | 0.2464
Epoch 5/300, Loss: 0.2984 | 0.6483
Epoch 6/300, Loss: 0.2285 | 0.2607
Epoch 7/300, Loss: 0.2100 | 0.3023
Epoch 8/300, Loss: 0.1975 | 0.3155
Epoch 9/300, Loss: 0.1774 | 0.2175
Epoch 10/300, Loss: 0.1636 | 0.2515
Epoch 11/300, Loss: 0.1559 | 0.1958
Epoch 12/300, Loss: 0.1508 | 0.2104
Epoch 13/300, Loss: 0.1459 | 0.1781
Epoch 14/300, Loss: 0.1419 | 0.1854
Epoch 15/300, Loss: 0.1379 | 0.1608
Epoch 16/300, Loss: 0.1347 | 0.1669
Epoch 17/300, Loss: 0.1317 | 0.1457
Epoch 18/300, Loss: 0.1290 | 0.1510
Epoch 19/300, Loss: 0.1268 | 0.1330
Epoch 20/300, Loss: 0.1249 | 0.1383
Epoch 21/300, Loss: 0.1235 | 0.1226
Epoch 22/300, Loss: 0.1229 | 0.1322
Epoch 23/300, Loss: 0.1227 | 0.1145
Epoch 24/300, Loss: 0.1251 | 0.1376
Epoch 25/300, Loss: 0.1253 | 0.1095
Epoch 26/300, Loss: 0.1288 | 0.1551
Epoch 27/300, Loss: 0.1258 | 0.1097
Epoch 28/300, Loss: 0.1244 | 0.1427
Epoch 29/300, Loss: 0.1202 | 0.1095
Epoch 30/300, Loss: 0.1190 | 0.1118
Epoch 31/300, Loss: 0.1163 | 0.1070
Epoch 32/300, Loss: 0.1146 | 0.0965
Epoch 33/300, Loss: 0.1127 | 0.1044
Epoch 34/300, Loss: 0.1111 | 0.0903
Epoch 35/300, Loss: 0.1102 | 0.1025
Epoch 36/300, Loss: 0.1089 | 0.0874
Epoch 37/300, Loss: 0.1084 | 0.0993
Epoch 38/300, Loss: 0.1072 | 0.0857
Epoch 39/300, Loss: 0.1068 | 0.0933
Epoch 40/300, Loss: 0.1057 | 0.0842
Epoch 41/300, Loss: 0.1053 | 0.0876
Epoch 42/300, Loss: 0.1045 | 0.0829
Epoch 43/300, Loss: 0.1041 | 0.0836
Epoch 44/300, Loss: 0.1035 | 0.0815
Epoch 45/300, Loss: 0.1031 | 0.0807
Epoch 46/300, Loss: 0.1026 | 0.0800
Epoch 47/300, Loss: 0.1022 | 0.0787
Epoch 48/300, Loss: 0.1018 | 0.0784
Epoch 49/300, Loss: 0.1014 | 0.0772
Epoch 50/300, Loss: 0.1011 | 0.0769
Epoch 51/300, Loss: 0.1007 | 0.0759
Epoch 52/300, Loss: 0.1004 | 0.0755
Epoch 53/300, Loss: 0.1000 | 0.0747
Epoch 54/300, Loss: 0.0997 | 0.0742
Epoch 55/300, Loss: 0.0994 | 0.0737
Epoch 56/300, Loss: 0.0991 | 0.0732
Epoch 57/300, Loss: 0.0988 | 0.0727
Epoch 58/300, Loss: 0.0985 | 0.0722
Epoch 59/300, Loss: 0.0983 | 0.0718
Epoch 60/300, Loss: 0.0980 | 0.0713
Epoch 61/300, Loss: 0.0978 | 0.0710
Epoch 62/300, Loss: 0.0975 | 0.0706
Epoch 63/300, Loss: 0.0973 | 0.0702
Epoch 64/300, Loss: 0.0971 | 0.0699
Epoch 65/300, Loss: 0.0969 | 0.0695
Epoch 66/300, Loss: 0.0966 | 0.0692
Epoch 67/300, Loss: 0.0964 | 0.0689
Epoch 68/300, Loss: 0.0963 | 0.0686
Epoch 69/300, Loss: 0.0961 | 0.0683
Epoch 70/300, Loss: 0.0959 | 0.0681
Epoch 71/300, Loss: 0.0957 | 0.0678
Epoch 72/300, Loss: 0.0955 | 0.0676
Epoch 73/300, Loss: 0.0954 | 0.0673
Epoch 74/300, Loss: 0.0952 | 0.0671
Epoch 75/300, Loss: 0.0951 | 0.0669
Epoch 76/300, Loss: 0.0949 | 0.0667
Epoch 77/300, Loss: 0.0948 | 0.0665
Epoch 78/300, Loss: 0.0947 | 0.0663
Epoch 79/300, Loss: 0.0945 | 0.0661
Epoch 80/300, Loss: 0.0944 | 0.0660
Epoch 81/300, Loss: 0.0943 | 0.0658
Epoch 82/300, Loss: 0.0942 | 0.0656
Epoch 83/300, Loss: 0.0940 | 0.0655
Epoch 84/300, Loss: 0.0939 | 0.0653
Epoch 85/300, Loss: 0.0938 | 0.0652
Epoch 86/300, Loss: 0.0937 | 0.0650
Epoch 87/300, Loss: 0.0936 | 0.0649
Epoch 88/300, Loss: 0.0935 | 0.0648
Epoch 89/300, Loss: 0.0934 | 0.0647
Epoch 90/300, Loss: 0.0933 | 0.0645
Epoch 91/300, Loss: 0.0933 | 0.0644
Epoch 92/300, Loss: 0.0932 | 0.0643
Epoch 93/300, Loss: 0.0931 | 0.0642
Epoch 94/300, Loss: 0.0930 | 0.0641
Epoch 95/300, Loss: 0.0929 | 0.0640
Epoch 96/300, Loss: 0.0929 | 0.0639
Epoch 97/300, Loss: 0.0928 | 0.0638
Epoch 98/300, Loss: 0.0927 | 0.0638
Epoch 99/300, Loss: 0.0927 | 0.0637
Epoch 100/300, Loss: 0.0926 | 0.0636
Epoch 101/300, Loss: 0.0925 | 0.0635
Epoch 102/300, Loss: 0.0925 | 0.0634
Epoch 103/300, Loss: 0.0924 | 0.0634
Epoch 104/300, Loss: 0.0924 | 0.0633
Epoch 105/300, Loss: 0.0923 | 0.0632
Epoch 106/300, Loss: 0.0923 | 0.0632
Epoch 107/300, Loss: 0.0922 | 0.0631
Epoch 108/300, Loss: 0.0922 | 0.0630
Epoch 109/300, Loss: 0.0921 | 0.0630
Epoch 110/300, Loss: 0.0921 | 0.0629
Epoch 111/300, Loss: 0.0920 | 0.0629
Epoch 112/300, Loss: 0.0920 | 0.0628
Epoch 113/300, Loss: 0.0919 | 0.0628
Epoch 114/300, Loss: 0.0919 | 0.0627
Epoch 115/300, Loss: 0.0919 | 0.0627
Epoch 116/300, Loss: 0.0918 | 0.0626
Epoch 117/300, Loss: 0.0918 | 0.0626
Epoch 118/300, Loss: 0.0918 | 0.0625
Epoch 119/300, Loss: 0.0917 | 0.0625
Epoch 120/300, Loss: 0.0917 | 0.0625
Epoch 121/300, Loss: 0.0917 | 0.0624
Epoch 122/300, Loss: 0.0916 | 0.0624
Epoch 123/300, Loss: 0.0916 | 0.0624
Epoch 124/300, Loss: 0.0916 | 0.0623
Epoch 125/300, Loss: 0.0916 | 0.0623
Epoch 126/300, Loss: 0.0915 | 0.0622
Epoch 127/300, Loss: 0.0915 | 0.0622
Epoch 128/300, Loss: 0.0915 | 0.0622
Epoch 129/300, Loss: 0.0915 | 0.0622
Epoch 130/300, Loss: 0.0914 | 0.0621
Epoch 131/300, Loss: 0.0914 | 0.0621
Epoch 132/300, Loss: 0.0914 | 0.0621
Epoch 133/300, Loss: 0.0914 | 0.0621
Epoch 134/300, Loss: 0.0913 | 0.0620
Epoch 135/300, Loss: 0.0913 | 0.0620
Epoch 136/300, Loss: 0.0913 | 0.0620
Epoch 137/300, Loss: 0.0913 | 0.0620
Epoch 138/300, Loss: 0.0913 | 0.0619
Epoch 139/300, Loss: 0.0912 | 0.0619
Epoch 140/300, Loss: 0.0912 | 0.0619
Epoch 141/300, Loss: 0.0912 | 0.0619
Epoch 142/300, Loss: 0.0912 | 0.0619
Epoch 143/300, Loss: 0.0912 | 0.0618
Epoch 144/300, Loss: 0.0912 | 0.0618
Epoch 145/300, Loss: 0.0912 | 0.0618
Epoch 146/300, Loss: 0.0911 | 0.0618
Epoch 147/300, Loss: 0.0911 | 0.0618
Epoch 148/300, Loss: 0.0911 | 0.0618
Epoch 149/300, Loss: 0.0911 | 0.0617
Epoch 150/300, Loss: 0.0911 | 0.0617
Epoch 151/300, Loss: 0.0911 | 0.0617
Epoch 152/300, Loss: 0.0911 | 0.0617
Epoch 153/300, Loss: 0.0911 | 0.0617
Epoch 154/300, Loss: 0.0910 | 0.0617
Epoch 155/300, Loss: 0.0910 | 0.0617
Epoch 156/300, Loss: 0.0910 | 0.0616
Epoch 157/300, Loss: 0.0910 | 0.0616
Epoch 158/300, Loss: 0.0910 | 0.0616
Epoch 159/300, Loss: 0.0910 | 0.0616
Epoch 160/300, Loss: 0.0910 | 0.0616
Epoch 161/300, Loss: 0.0910 | 0.0616
Epoch 162/300, Loss: 0.0910 | 0.0616
Epoch 163/300, Loss: 0.0910 | 0.0616
Epoch 164/300, Loss: 0.0910 | 0.0616
Epoch 165/300, Loss: 0.0910 | 0.0616
Epoch 166/300, Loss: 0.0909 | 0.0616
Epoch 167/300, Loss: 0.0909 | 0.0615
Epoch 168/300, Loss: 0.0909 | 0.0615
Epoch 169/300, Loss: 0.0909 | 0.0615
Epoch 170/300, Loss: 0.0909 | 0.0615
Epoch 171/300, Loss: 0.0909 | 0.0615
Epoch 172/300, Loss: 0.0909 | 0.0615
Epoch 173/300, Loss: 0.0909 | 0.0615
Epoch 174/300, Loss: 0.0909 | 0.0615
Epoch 175/300, Loss: 0.0909 | 0.0615
Epoch 176/300, Loss: 0.0909 | 0.0615
Epoch 177/300, Loss: 0.0909 | 0.0615
Epoch 178/300, Loss: 0.0909 | 0.0615
Epoch 179/300, Loss: 0.0909 | 0.0615
Epoch 180/300, Loss: 0.0909 | 0.0615
Epoch 181/300, Loss: 0.0909 | 0.0615
Epoch 182/300, Loss: 0.0909 | 0.0614
Epoch 183/300, Loss: 0.0909 | 0.0614
Epoch 184/300, Loss: 0.0909 | 0.0614
Epoch 185/300, Loss: 0.0908 | 0.0614
Epoch 186/300, Loss: 0.0908 | 0.0614
Epoch 187/300, Loss: 0.0908 | 0.0614
Epoch 188/300, Loss: 0.0908 | 0.0614
Epoch 189/300, Loss: 0.0908 | 0.0614
Epoch 190/300, Loss: 0.0908 | 0.0614
Epoch 191/300, Loss: 0.0908 | 0.0614
Epoch 192/300, Loss: 0.0908 | 0.0614
Epoch 193/300, Loss: 0.0908 | 0.0614
Epoch 194/300, Loss: 0.0908 | 0.0614
Epoch 195/300, Loss: 0.0908 | 0.0614
Epoch 196/300, Loss: 0.0908 | 0.0614
Epoch 197/300, Loss: 0.0908 | 0.0614
Epoch 198/300, Loss: 0.0908 | 0.0614
Epoch 199/300, Loss: 0.0908 | 0.0614
Epoch 200/300, Loss: 0.0908 | 0.0614
Epoch 201/300, Loss: 0.0908 | 0.0614
Epoch 202/300, Loss: 0.0908 | 0.0614
Epoch 203/300, Loss: 0.0908 | 0.0614
Epoch 204/300, Loss: 0.0908 | 0.0614
Epoch 205/300, Loss: 0.0908 | 0.0614
Epoch 206/300, Loss: 0.0908 | 0.0614
Epoch 207/300, Loss: 0.0908 | 0.0614
Epoch 208/300, Loss: 0.0908 | 0.0614
Epoch 209/300, Loss: 0.0908 | 0.0614
Epoch 210/300, Loss: 0.0908 | 0.0614
Epoch 211/300, Loss: 0.0908 | 0.0614
Epoch 212/300, Loss: 0.0908 | 0.0614
Epoch 213/300, Loss: 0.0908 | 0.0614
Epoch 214/300, Loss: 0.0908 | 0.0613
Epoch 215/300, Loss: 0.0908 | 0.0613
Epoch 216/300, Loss: 0.0908 | 0.0613
Epoch 217/300, Loss: 0.0908 | 0.0613
Epoch 218/300, Loss: 0.0908 | 0.0613
Epoch 219/300, Loss: 0.0908 | 0.0613
Epoch 220/300, Loss: 0.0908 | 0.0613
Epoch 221/300, Loss: 0.0908 | 0.0613
Epoch 222/300, Loss: 0.0908 | 0.0613
Epoch 223/300, Loss: 0.0908 | 0.0613
Epoch 224/300, Loss: 0.0908 | 0.0613
Epoch 225/300, Loss: 0.0908 | 0.0613
Epoch 226/300, Loss: 0.0908 | 0.0613
Epoch 227/300, Loss: 0.0908 | 0.0613
Epoch 228/300, Loss: 0.0908 | 0.0613
Epoch 229/300, Loss: 0.0908 | 0.0613
Epoch 230/300, Loss: 0.0908 | 0.0613
Epoch 231/300, Loss: 0.0908 | 0.0613
Epoch 232/300, Loss: 0.0908 | 0.0613
Epoch 233/300, Loss: 0.0908 | 0.0613
Epoch 234/300, Loss: 0.0908 | 0.0613
Epoch 235/300, Loss: 0.0908 | 0.0613
Epoch 236/300, Loss: 0.0908 | 0.0613
Epoch 237/300, Loss: 0.0908 | 0.0613
Epoch 238/300, Loss: 0.0908 | 0.0613
Epoch 239/300, Loss: 0.0908 | 0.0613
Epoch 240/300, Loss: 0.0908 | 0.0613
Epoch 241/300, Loss: 0.0908 | 0.0613
Epoch 242/300, Loss: 0.0908 | 0.0613
Epoch 243/300, Loss: 0.0908 | 0.0613
Epoch 244/300, Loss: 0.0908 | 0.0613
Epoch 245/300, Loss: 0.0907 | 0.0613
Epoch 246/300, Loss: 0.0907 | 0.0613
Epoch 247/300, Loss: 0.0907 | 0.0613
Epoch 248/300, Loss: 0.0907 | 0.0613
Epoch 249/300, Loss: 0.0907 | 0.0613
Epoch 250/300, Loss: 0.0907 | 0.0613
Epoch 251/300, Loss: 0.0907 | 0.0613
Epoch 252/300, Loss: 0.0907 | 0.0613
Epoch 253/300, Loss: 0.0907 | 0.0613
Epoch 254/300, Loss: 0.0907 | 0.0613
Epoch 255/300, Loss: 0.0907 | 0.0613
Epoch 256/300, Loss: 0.0907 | 0.0613
Epoch 257/300, Loss: 0.0907 | 0.0613
Epoch 258/300, Loss: 0.0907 | 0.0613
Epoch 259/300, Loss: 0.0907 | 0.0613
Epoch 260/300, Loss: 0.0907 | 0.0613
Epoch 261/300, Loss: 0.0907 | 0.0613
Epoch 262/300, Loss: 0.0907 | 0.0613
Epoch 263/300, Loss: 0.0907 | 0.0613
Epoch 264/300, Loss: 0.0907 | 0.0613
Epoch 265/300, Loss: 0.0907 | 0.0613
Epoch 266/300, Loss: 0.0907 | 0.0613
Epoch 267/300, Loss: 0.0907 | 0.0613
Epoch 268/300, Loss: 0.0907 | 0.0613
Epoch 269/300, Loss: 0.0907 | 0.0613
Epoch 270/300, Loss: 0.0907 | 0.0613
Epoch 271/300, Loss: 0.0907 | 0.0613
Epoch 272/300, Loss: 0.0907 | 0.0613
Epoch 273/300, Loss: 0.0907 | 0.0613
Epoch 274/300, Loss: 0.0907 | 0.0613
Epoch 275/300, Loss: 0.0907 | 0.0613
Epoch 276/300, Loss: 0.0907 | 0.0613
Epoch 277/300, Loss: 0.0907 | 0.0613
Epoch 278/300, Loss: 0.0907 | 0.0613
Epoch 279/300, Loss: 0.0907 | 0.0613
Epoch 280/300, Loss: 0.0907 | 0.0613
Epoch 281/300, Loss: 0.0907 | 0.0613
Epoch 282/300, Loss: 0.0907 | 0.0613
Epoch 283/300, Loss: 0.0907 | 0.0613
Epoch 284/300, Loss: 0.0907 | 0.0613
Epoch 285/300, Loss: 0.0907 | 0.0613
Epoch 286/300, Loss: 0.0907 | 0.0613
Epoch 287/300, Loss: 0.0907 | 0.0613
Epoch 288/300, Loss: 0.0907 | 0.0613
Epoch 289/300, Loss: 0.0907 | 0.0613
Epoch 290/300, Loss: 0.0907 | 0.0613
Epoch 291/300, Loss: 0.0907 | 0.0613
Epoch 292/300, Loss: 0.0907 | 0.0613
Epoch 293/300, Loss: 0.0907 | 0.0613
Epoch 294/300, Loss: 0.0907 | 0.0613
Epoch 295/300, Loss: 0.0907 | 0.0613
Epoch 296/300, Loss: 0.0907 | 0.0613
Epoch 297/300, Loss: 0.0907 | 0.0613
Epoch 298/300, Loss: 0.0907 | 0.0613
Epoch 299/300, Loss: 0.0907 | 0.0613
Epoch 300/300, Loss: 0.0907 | 0.0613
Runtime (seconds): 74.60703873634338
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 40.34215235733427
RMSE: 6.3515472412109375
MAE: 6.3515472412109375
R-squared: nan
[167.76093]
