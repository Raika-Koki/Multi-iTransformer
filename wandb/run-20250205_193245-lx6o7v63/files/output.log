[32m[I 2025-02-05 19:32:50,560][0m A new study created in memory with name: no-name-55e1c7bf-48f0-4cef-a3b1-e2b6557e7a20[0m
[32m[I 2025-02-05 19:34:11,532][0m Trial 0 finished with value: 0.17558964843399968 and parameters: {'observation_period_num': 247, 'train_rates': 0.8676492212393997, 'learning_rate': 8.755174278042876e-05, 'batch_size': 65, 'step_size': 1, 'gamma': 0.9661493037385335}. Best is trial 0 with value: 0.17558964843399968.[0m
[32m[I 2025-02-05 19:36:00,031][0m Trial 1 finished with value: 0.13718642021568728 and parameters: {'observation_period_num': 21, 'train_rates': 0.975994178004425, 'learning_rate': 7.391494295095199e-06, 'batch_size': 56, 'step_size': 12, 'gamma': 0.7933838956814198}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:37:20,801][0m Trial 2 finished with value: 0.2974071931963909 and parameters: {'observation_period_num': 117, 'train_rates': 0.7077646301195061, 'learning_rate': 3.256948247368973e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.7653236963601412}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:39:09,115][0m Trial 3 finished with value: 0.32574240291737633 and parameters: {'observation_period_num': 164, 'train_rates': 0.7876404174383947, 'learning_rate': 3.384036128698992e-06, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9673976146274385}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:41:31,441][0m Trial 4 finished with value: 0.788580771185394 and parameters: {'observation_period_num': 46, 'train_rates': 0.9203911628642574, 'learning_rate': 2.3842707473193798e-06, 'batch_size': 40, 'step_size': 2, 'gamma': 0.8302351249739289}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:41:58,273][0m Trial 5 finished with value: 0.26550808548927307 and parameters: {'observation_period_num': 237, 'train_rates': 0.9331811045833628, 'learning_rate': 0.0001285368479437357, 'batch_size': 215, 'step_size': 5, 'gamma': 0.8702369534450013}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:42:27,535][0m Trial 6 finished with value: 0.6233252882957458 and parameters: {'observation_period_num': 184, 'train_rates': 0.9889141301415004, 'learning_rate': 6.454337440947464e-05, 'batch_size': 223, 'step_size': 15, 'gamma': 0.9553853666083094}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:43:00,577][0m Trial 7 finished with value: 0.7668491294877825 and parameters: {'observation_period_num': 152, 'train_rates': 0.7313429313320617, 'learning_rate': 1.4488185868138262e-06, 'batch_size': 155, 'step_size': 12, 'gamma': 0.835059855657156}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:44:00,376][0m Trial 8 finished with value: 0.3770396760419795 and parameters: {'observation_period_num': 155, 'train_rates': 0.9463195812473615, 'learning_rate': 1.2914775160964816e-05, 'batch_size': 98, 'step_size': 8, 'gamma': 0.90478902533044}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:44:21,954][0m Trial 9 finished with value: 0.2704965094071984 and parameters: {'observation_period_num': 240, 'train_rates': 0.8109648895072606, 'learning_rate': 0.0002983892666950247, 'batch_size': 250, 'step_size': 15, 'gamma': 0.8229749693113853}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:45:05,299][0m Trial 10 finished with value: 0.2860096852694239 and parameters: {'observation_period_num': 16, 'train_rates': 0.8485234256202269, 'learning_rate': 4.868505781529666e-06, 'batch_size': 136, 'step_size': 10, 'gamma': 0.7652285449774479}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:45:55,310][0m Trial 11 finished with value: 0.17221203776902722 and parameters: {'observation_period_num': 67, 'train_rates': 0.63388096698982, 'learning_rate': 0.0006703221805207983, 'batch_size': 94, 'step_size': 1, 'gamma': 0.9295994825785264}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:46:37,744][0m Trial 12 finished with value: 0.19088817963092 and parameters: {'observation_period_num': 76, 'train_rates': 0.6140448969490259, 'learning_rate': 0.0009739588728823652, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9168079778482756}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:47:25,204][0m Trial 13 finished with value: 0.387258818328584 and parameters: {'observation_period_num': 8, 'train_rates': 0.6047529798831536, 'learning_rate': 2.1099691503740346e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9095954739524226}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:47:56,689][0m Trial 14 finished with value: 0.882492472629736 and parameters: {'observation_period_num': 82, 'train_rates': 0.6858987351333081, 'learning_rate': 8.009134802725032e-06, 'batch_size': 153, 'step_size': 3, 'gamma': 0.8002688996293633}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:50:47,873][0m Trial 15 finished with value: 0.20240950717378114 and parameters: {'observation_period_num': 52, 'train_rates': 0.6567468020065906, 'learning_rate': 0.0008011338839372077, 'batch_size': 26, 'step_size': 11, 'gamma': 0.8822034744541579}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:51:52,943][0m Trial 16 finished with value: 0.23597203681518317 and parameters: {'observation_period_num': 105, 'train_rates': 0.7813695713318685, 'learning_rate': 0.00021455995912838516, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9252703513705154}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:56:08,204][0m Trial 17 finished with value: 0.17914493803923226 and parameters: {'observation_period_num': 37, 'train_rates': 0.7496965694417715, 'learning_rate': 3.617312774477935e-05, 'batch_size': 19, 'step_size': 14, 'gamma': 0.7963645560619308}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:56:55,755][0m Trial 18 finished with value: 0.1599697571476934 and parameters: {'observation_period_num': 78, 'train_rates': 0.8484725824335706, 'learning_rate': 0.0004066391944525182, 'batch_size': 122, 'step_size': 13, 'gamma': 0.9411010153125712}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:57:29,135][0m Trial 19 finished with value: 0.3622888352859903 and parameters: {'observation_period_num': 102, 'train_rates': 0.8872944742857048, 'learning_rate': 8.42963979820891e-06, 'batch_size': 177, 'step_size': 13, 'gamma': 0.9851823122738664}. Best is trial 1 with value: 0.13718642021568728.[0m
[32m[I 2025-02-05 19:58:20,798][0m Trial 20 finished with value: 0.06292542815208435 and parameters: {'observation_period_num': 27, 'train_rates': 0.9807975157433093, 'learning_rate': 0.0003548681397637752, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8578796406910614}. Best is trial 20 with value: 0.06292542815208435.[0m
[32m[I 2025-02-05 19:59:08,168][0m Trial 21 finished with value: 0.049428541213274 and parameters: {'observation_period_num': 27, 'train_rates': 0.9666882027075907, 'learning_rate': 0.0003023298383126885, 'batch_size': 131, 'step_size': 10, 'gamma': 0.8888172673684466}. Best is trial 21 with value: 0.049428541213274.[0m
[32m[I 2025-02-05 19:59:46,419][0m Trial 22 finished with value: 0.09104441106319427 and parameters: {'observation_period_num': 31, 'train_rates': 0.9873536166511684, 'learning_rate': 0.0001238260643153462, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8525424210380885}. Best is trial 21 with value: 0.049428541213274.[0m
[32m[I 2025-02-05 20:00:21,358][0m Trial 23 finished with value: 0.0733112320303917 and parameters: {'observation_period_num': 32, 'train_rates': 0.9595130303633341, 'learning_rate': 0.00016748011908120992, 'batch_size': 184, 'step_size': 9, 'gamma': 0.8536343042071705}. Best is trial 21 with value: 0.049428541213274.[0m
[32m[I 2025-02-05 20:00:54,454][0m Trial 24 finished with value: 0.05723962617608217 and parameters: {'observation_period_num': 51, 'train_rates': 0.9112429949233464, 'learning_rate': 0.0004113524951167249, 'batch_size': 180, 'step_size': 8, 'gamma': 0.8953287024242503}. Best is trial 21 with value: 0.049428541213274.[0m
[32m[I 2025-02-05 20:01:43,927][0m Trial 25 finished with value: 0.07641835087468188 and parameters: {'observation_period_num': 57, 'train_rates': 0.9119054905361488, 'learning_rate': 0.0004422224054201113, 'batch_size': 124, 'step_size': 8, 'gamma': 0.8898069602998256}. Best is trial 21 with value: 0.049428541213274.[0m
[32m[I 2025-02-05 20:02:22,784][0m Trial 26 finished with value: 0.034821350029424616 and parameters: {'observation_period_num': 5, 'train_rates': 0.8975798037416279, 'learning_rate': 0.00032424121956023746, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8924090072091727}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:02:59,941][0m Trial 27 finished with value: 0.0644083880486979 and parameters: {'observation_period_num': 13, 'train_rates': 0.8950087320944141, 'learning_rate': 6.694433683146676e-05, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8883560824641328}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:03:31,094][0m Trial 28 finished with value: 0.04811370030778353 and parameters: {'observation_period_num': 5, 'train_rates': 0.8768323054349239, 'learning_rate': 0.00018833059961497956, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9013804991116597}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:04:00,002][0m Trial 29 finished with value: 0.05882375742550249 and parameters: {'observation_period_num': 7, 'train_rates': 0.8636038412466466, 'learning_rate': 8.03523666010265e-05, 'batch_size': 206, 'step_size': 11, 'gamma': 0.9401381403219174}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:04:24,067][0m Trial 30 finished with value: 0.19680638589540453 and parameters: {'observation_period_num': 217, 'train_rates': 0.8264139218923516, 'learning_rate': 0.00023868555176343874, 'batch_size': 236, 'step_size': 9, 'gamma': 0.8731383477306218}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:04:55,680][0m Trial 31 finished with value: 0.038688743767331675 and parameters: {'observation_period_num': 5, 'train_rates': 0.8994540567291343, 'learning_rate': 0.0006216781276017657, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9028221374928859}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:05:26,751][0m Trial 32 finished with value: 0.051184330900306226 and parameters: {'observation_period_num': 23, 'train_rates': 0.8845489247473952, 'learning_rate': 0.0005729372520083296, 'batch_size': 195, 'step_size': 6, 'gamma': 0.9013792708739321}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:06:05,894][0m Trial 33 finished with value: 0.04636919125914574 and parameters: {'observation_period_num': 8, 'train_rates': 0.9546006390146624, 'learning_rate': 0.00018174793349416693, 'batch_size': 165, 'step_size': 11, 'gamma': 0.8752417957990918}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:06:43,955][0m Trial 34 finished with value: 0.08074228444745421 and parameters: {'observation_period_num': 41, 'train_rates': 0.9390983557960626, 'learning_rate': 0.0001231560546367446, 'batch_size': 165, 'step_size': 11, 'gamma': 0.9163458340268692}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:07:14,519][0m Trial 35 finished with value: 0.0722111141859154 and parameters: {'observation_period_num': 5, 'train_rates': 0.8644151908318796, 'learning_rate': 4.928118366445212e-05, 'batch_size': 200, 'step_size': 4, 'gamma': 0.864719238172604}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:07:42,344][0m Trial 36 finished with value: 0.0898415759909569 and parameters: {'observation_period_num': 65, 'train_rates': 0.8327908415802691, 'learning_rate': 0.00016568483229532072, 'batch_size': 220, 'step_size': 9, 'gamma': 0.8394655884557441}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:08:24,458][0m Trial 37 finished with value: 0.03533059184620571 and parameters: {'observation_period_num': 5, 'train_rates': 0.9030431575817162, 'learning_rate': 0.0005578294247434079, 'batch_size': 143, 'step_size': 7, 'gamma': 0.8738507789035364}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:09:06,850][0m Trial 38 finished with value: 0.03887124465066282 and parameters: {'observation_period_num': 21, 'train_rates': 0.9080740890355302, 'learning_rate': 0.0005382890774179327, 'batch_size': 145, 'step_size': 7, 'gamma': 0.8729792083881378}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:09:52,200][0m Trial 39 finished with value: 0.05129725970156182 and parameters: {'observation_period_num': 42, 'train_rates': 0.9262079856339323, 'learning_rate': 0.0009986042513791097, 'batch_size': 141, 'step_size': 7, 'gamma': 0.8119579075800253}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:10:29,491][0m Trial 40 finished with value: 0.1307113324696163 and parameters: {'observation_period_num': 192, 'train_rates': 0.9047413459191822, 'learning_rate': 0.0006028220538380459, 'batch_size': 148, 'step_size': 6, 'gamma': 0.84514534915068}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:11:07,366][0m Trial 41 finished with value: 0.04007937380491377 and parameters: {'observation_period_num': 19, 'train_rates': 0.9385391075307645, 'learning_rate': 0.0005519358944020442, 'batch_size': 167, 'step_size': 5, 'gamma': 0.8793794135961642}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:11:46,558][0m Trial 42 finished with value: 0.043954955243127314 and parameters: {'observation_period_num': 23, 'train_rates': 0.9225182575491547, 'learning_rate': 0.0004931209297361415, 'batch_size': 160, 'step_size': 5, 'gamma': 0.8672186154233222}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:12:30,517][0m Trial 43 finished with value: 0.041862584241143946 and parameters: {'observation_period_num': 19, 'train_rates': 0.938680190351537, 'learning_rate': 0.0007391861712106719, 'batch_size': 144, 'step_size': 4, 'gamma': 0.8812156796710982}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:13:23,614][0m Trial 44 finished with value: 0.10830583965697256 and parameters: {'observation_period_num': 128, 'train_rates': 0.8969655797471869, 'learning_rate': 0.00028112507382568424, 'batch_size': 110, 'step_size': 7, 'gamma': 0.8202531172448178}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:13:56,060][0m Trial 45 finished with value: 0.04031906135148243 and parameters: {'observation_period_num': 18, 'train_rates': 0.8047262038991031, 'learning_rate': 0.0007908924196287045, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8609693609255145}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:14:26,014][0m Trial 46 finished with value: 0.08282858095235295 and parameters: {'observation_period_num': 40, 'train_rates': 0.8466678658081027, 'learning_rate': 0.000530448406901632, 'batch_size': 191, 'step_size': 8, 'gamma': 0.9267923080208063}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:15:00,165][0m Trial 47 finished with value: 0.09657804387815709 and parameters: {'observation_period_num': 88, 'train_rates': 0.8686386674222272, 'learning_rate': 0.00033780328899640506, 'batch_size': 169, 'step_size': 4, 'gamma': 0.9110474381026993}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:15:40,886][0m Trial 48 finished with value: 1.7510942717838716 and parameters: {'observation_period_num': 61, 'train_rates': 0.7705002922484389, 'learning_rate': 1.2065545093197448e-06, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8792871371336498}. Best is trial 26 with value: 0.034821350029424616.[0m
[32m[I 2025-02-05 20:16:37,970][0m Trial 49 finished with value: 0.061405095688410495 and parameters: {'observation_period_num': 50, 'train_rates': 0.9481305786866049, 'learning_rate': 0.0006594289923255287, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8984990874720018}. Best is trial 26 with value: 0.034821350029424616.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.2923 | 0.2530
Epoch 2/300, Loss: 0.1653 | 0.2589
Epoch 3/300, Loss: 0.1910 | 0.1499
Epoch 4/300, Loss: 0.1439 | 0.1139
Epoch 5/300, Loss: 0.1358 | 0.1040
Epoch 6/300, Loss: 0.1193 | 0.1048
Epoch 7/300, Loss: 0.1329 | 0.1031
Epoch 8/300, Loss: 0.1703 | 0.1868
Epoch 9/300, Loss: 0.1666 | 0.0936
Epoch 10/300, Loss: 0.1296 | 0.2020
Epoch 11/300, Loss: 0.1384 | 0.1783
Epoch 12/300, Loss: 0.1535 | 0.0998
Epoch 13/300, Loss: 0.1416 | 0.1029
Epoch 14/300, Loss: 0.1362 | 0.1130
Epoch 15/300, Loss: 0.1251 | 0.1167
Epoch 16/300, Loss: 0.1055 | 0.0741
Epoch 17/300, Loss: 0.1140 | 0.0878
Epoch 18/300, Loss: 0.1315 | 0.0854
Epoch 19/300, Loss: 0.1095 | 0.0734
Epoch 20/300, Loss: 0.1022 | 0.0769
Epoch 21/300, Loss: 0.1106 | 0.0769
Epoch 22/300, Loss: 0.0982 | 0.0738
Epoch 23/300, Loss: 0.0981 | 0.0660
Epoch 24/300, Loss: 0.0956 | 0.0681
Epoch 25/300, Loss: 0.0964 | 0.0662
Epoch 26/300, Loss: 0.0950 | 0.0697
Epoch 27/300, Loss: 0.0918 | 0.0741
Epoch 28/300, Loss: 0.0917 | 0.0609
Epoch 29/300, Loss: 0.0898 | 0.0624
Epoch 30/300, Loss: 0.0894 | 0.0579
Epoch 31/300, Loss: 0.0883 | 0.0637
Epoch 32/300, Loss: 0.0885 | 0.0688
Epoch 33/300, Loss: 0.0889 | 0.0570
Epoch 34/300, Loss: 0.0872 | 0.0551
Epoch 35/300, Loss: 0.0853 | 0.0529
Epoch 36/300, Loss: 0.0835 | 0.0540
Epoch 37/300, Loss: 0.0827 | 0.0554
Epoch 38/300, Loss: 0.0822 | 0.0521
Epoch 39/300, Loss: 0.0821 | 0.0505
Epoch 40/300, Loss: 0.0823 | 0.0502
Epoch 41/300, Loss: 0.0824 | 0.0510
Epoch 42/300, Loss: 0.0825 | 0.0521
Epoch 43/300, Loss: 0.0819 | 0.0508
Epoch 44/300, Loss: 0.0817 | 0.0496
Epoch 45/300, Loss: 0.0814 | 0.0507
Epoch 46/300, Loss: 0.0815 | 0.0532
Epoch 47/300, Loss: 0.0827 | 0.0522
Epoch 48/300, Loss: 0.0827 | 0.0506
Epoch 49/300, Loss: 0.0807 | 0.0496
Epoch 50/300, Loss: 0.0795 | 0.0477
Epoch 51/300, Loss: 0.0798 | 0.0477
Epoch 52/300, Loss: 0.0800 | 0.0472
Epoch 53/300, Loss: 0.0790 | 0.0479
Epoch 54/300, Loss: 0.0781 | 0.0488
Epoch 55/300, Loss: 0.0779 | 0.0469
Epoch 56/300, Loss: 0.0779 | 0.0459
Epoch 57/300, Loss: 0.0777 | 0.0460
Epoch 58/300, Loss: 0.0774 | 0.0452
Epoch 59/300, Loss: 0.0769 | 0.0440
Epoch 60/300, Loss: 0.0766 | 0.0435
Epoch 61/300, Loss: 0.0764 | 0.0434
Epoch 62/300, Loss: 0.0763 | 0.0436
Epoch 63/300, Loss: 0.0759 | 0.0432
Epoch 64/300, Loss: 0.0756 | 0.0428
Epoch 65/300, Loss: 0.0754 | 0.0428
Epoch 66/300, Loss: 0.0752 | 0.0430
Epoch 67/300, Loss: 0.0750 | 0.0429
Epoch 68/300, Loss: 0.0749 | 0.0428
Epoch 69/300, Loss: 0.0749 | 0.0428
Epoch 70/300, Loss: 0.0751 | 0.0432
Epoch 71/300, Loss: 0.0756 | 0.0438
Epoch 72/300, Loss: 0.0764 | 0.0437
Epoch 73/300, Loss: 0.0759 | 0.0428
Epoch 74/300, Loss: 0.0751 | 0.0420
Epoch 75/300, Loss: 0.0754 | 0.0422
Epoch 76/300, Loss: 0.0762 | 0.0424
Epoch 77/300, Loss: 0.0757 | 0.0423
Epoch 78/300, Loss: 0.0742 | 0.0420
Epoch 79/300, Loss: 0.0738 | 0.0412
Epoch 80/300, Loss: 0.0735 | 0.0408
Epoch 81/300, Loss: 0.0732 | 0.0408
Epoch 82/300, Loss: 0.0731 | 0.0407
Epoch 83/300, Loss: 0.0730 | 0.0402
Epoch 84/300, Loss: 0.0728 | 0.0403
Epoch 85/300, Loss: 0.0726 | 0.0403
Epoch 86/300, Loss: 0.0724 | 0.0401
Epoch 87/300, Loss: 0.0723 | 0.0401
Epoch 88/300, Loss: 0.0722 | 0.0400
Epoch 89/300, Loss: 0.0720 | 0.0399
Epoch 90/300, Loss: 0.0719 | 0.0398
Epoch 91/300, Loss: 0.0718 | 0.0398
Epoch 92/300, Loss: 0.0717 | 0.0397
Epoch 93/300, Loss: 0.0716 | 0.0396
Epoch 94/300, Loss: 0.0715 | 0.0395
Epoch 95/300, Loss: 0.0714 | 0.0394
Epoch 96/300, Loss: 0.0713 | 0.0393
Epoch 97/300, Loss: 0.0711 | 0.0392
Epoch 98/300, Loss: 0.0710 | 0.0391
Epoch 99/300, Loss: 0.0708 | 0.0391
Epoch 100/300, Loss: 0.0707 | 0.0390
Epoch 101/300, Loss: 0.0706 | 0.0390
Epoch 102/300, Loss: 0.0705 | 0.0390
Epoch 103/300, Loss: 0.0704 | 0.0389
Epoch 104/300, Loss: 0.0703 | 0.0388
Epoch 105/300, Loss: 0.0702 | 0.0387
Epoch 106/300, Loss: 0.0701 | 0.0386
Epoch 107/300, Loss: 0.0700 | 0.0385
Epoch 108/300, Loss: 0.0700 | 0.0383
Epoch 109/300, Loss: 0.0699 | 0.0382
Epoch 110/300, Loss: 0.0698 | 0.0382
Epoch 111/300, Loss: 0.0697 | 0.0381
Epoch 112/300, Loss: 0.0696 | 0.0381
Epoch 113/300, Loss: 0.0695 | 0.0381
Epoch 114/300, Loss: 0.0694 | 0.0381
Epoch 115/300, Loss: 0.0693 | 0.0382
Epoch 116/300, Loss: 0.0692 | 0.0382
Epoch 117/300, Loss: 0.0691 | 0.0381
Epoch 118/300, Loss: 0.0690 | 0.0379
Epoch 119/300, Loss: 0.0689 | 0.0378
Epoch 120/300, Loss: 0.0689 | 0.0377
Epoch 121/300, Loss: 0.0688 | 0.0376
Epoch 122/300, Loss: 0.0688 | 0.0375
Epoch 123/300, Loss: 0.0687 | 0.0375
Epoch 124/300, Loss: 0.0686 | 0.0375
Epoch 125/300, Loss: 0.0685 | 0.0375
Epoch 126/300, Loss: 0.0684 | 0.0376
Epoch 127/300, Loss: 0.0683 | 0.0376
Epoch 128/300, Loss: 0.0683 | 0.0375
Epoch 129/300, Loss: 0.0682 | 0.0374
Epoch 130/300, Loss: 0.0681 | 0.0374
Epoch 131/300, Loss: 0.0681 | 0.0373
Epoch 132/300, Loss: 0.0680 | 0.0373
Epoch 133/300, Loss: 0.0680 | 0.0373
Epoch 134/300, Loss: 0.0679 | 0.0373
Epoch 135/300, Loss: 0.0678 | 0.0373
Epoch 136/300, Loss: 0.0678 | 0.0373
Epoch 137/300, Loss: 0.0677 | 0.0372
Epoch 138/300, Loss: 0.0677 | 0.0372
Epoch 139/300, Loss: 0.0676 | 0.0372
Epoch 140/300, Loss: 0.0676 | 0.0371
Epoch 141/300, Loss: 0.0675 | 0.0371
Epoch 142/300, Loss: 0.0675 | 0.0371
Epoch 143/300, Loss: 0.0674 | 0.0371
Epoch 144/300, Loss: 0.0674 | 0.0371
Epoch 145/300, Loss: 0.0674 | 0.0371
Epoch 146/300, Loss: 0.0673 | 0.0371
Epoch 147/300, Loss: 0.0673 | 0.0371
Epoch 148/300, Loss: 0.0672 | 0.0370
Epoch 149/300, Loss: 0.0672 | 0.0370
Epoch 150/300, Loss: 0.0671 | 0.0370
Epoch 151/300, Loss: 0.0671 | 0.0370
Epoch 152/300, Loss: 0.0671 | 0.0370
Epoch 153/300, Loss: 0.0670 | 0.0370
Epoch 154/300, Loss: 0.0670 | 0.0370
Epoch 155/300, Loss: 0.0670 | 0.0369
Epoch 156/300, Loss: 0.0669 | 0.0369
Epoch 157/300, Loss: 0.0669 | 0.0369
Epoch 158/300, Loss: 0.0669 | 0.0369
Epoch 159/300, Loss: 0.0668 | 0.0369
Epoch 160/300, Loss: 0.0668 | 0.0369
Epoch 161/300, Loss: 0.0668 | 0.0369
Epoch 162/300, Loss: 0.0667 | 0.0369
Epoch 163/300, Loss: 0.0667 | 0.0368
Epoch 164/300, Loss: 0.0667 | 0.0368
Epoch 165/300, Loss: 0.0667 | 0.0368
Epoch 166/300, Loss: 0.0666 | 0.0368
Epoch 167/300, Loss: 0.0666 | 0.0368
Epoch 168/300, Loss: 0.0666 | 0.0368
Epoch 169/300, Loss: 0.0665 | 0.0367
Epoch 170/300, Loss: 0.0665 | 0.0367
Epoch 171/300, Loss: 0.0665 | 0.0367
Epoch 172/300, Loss: 0.0665 | 0.0367
Epoch 173/300, Loss: 0.0664 | 0.0367
Epoch 174/300, Loss: 0.0664 | 0.0367
Epoch 175/300, Loss: 0.0664 | 0.0367
Epoch 176/300, Loss: 0.0664 | 0.0367
Epoch 177/300, Loss: 0.0664 | 0.0366
Epoch 178/300, Loss: 0.0663 | 0.0366
Epoch 179/300, Loss: 0.0663 | 0.0366
Epoch 180/300, Loss: 0.0663 | 0.0366
Epoch 181/300, Loss: 0.0663 | 0.0366
Epoch 182/300, Loss: 0.0663 | 0.0366
Epoch 183/300, Loss: 0.0662 | 0.0366
Epoch 184/300, Loss: 0.0662 | 0.0366
Epoch 185/300, Loss: 0.0662 | 0.0365
Epoch 186/300, Loss: 0.0662 | 0.0365
Epoch 187/300, Loss: 0.0662 | 0.0365
Epoch 188/300, Loss: 0.0661 | 0.0365
Epoch 189/300, Loss: 0.0661 | 0.0365
Epoch 190/300, Loss: 0.0661 | 0.0365
Epoch 191/300, Loss: 0.0661 | 0.0365
Epoch 192/300, Loss: 0.0661 | 0.0365
Epoch 193/300, Loss: 0.0661 | 0.0365
Epoch 194/300, Loss: 0.0661 | 0.0364
Epoch 195/300, Loss: 0.0660 | 0.0364
Epoch 196/300, Loss: 0.0660 | 0.0364
Epoch 197/300, Loss: 0.0660 | 0.0364
Epoch 198/300, Loss: 0.0660 | 0.0364
Epoch 199/300, Loss: 0.0660 | 0.0364
Epoch 200/300, Loss: 0.0660 | 0.0364
Epoch 201/300, Loss: 0.0660 | 0.0364
Epoch 202/300, Loss: 0.0659 | 0.0364
Epoch 203/300, Loss: 0.0659 | 0.0364
Epoch 204/300, Loss: 0.0659 | 0.0363
Epoch 205/300, Loss: 0.0659 | 0.0363
Epoch 206/300, Loss: 0.0659 | 0.0363
Epoch 207/300, Loss: 0.0659 | 0.0363
Epoch 208/300, Loss: 0.0659 | 0.0363
Epoch 209/300, Loss: 0.0659 | 0.0363
Epoch 210/300, Loss: 0.0659 | 0.0363
Epoch 211/300, Loss: 0.0658 | 0.0363
Epoch 212/300, Loss: 0.0658 | 0.0363
Epoch 213/300, Loss: 0.0658 | 0.0363
Epoch 214/300, Loss: 0.0658 | 0.0363
Epoch 215/300, Loss: 0.0658 | 0.0362
Epoch 216/300, Loss: 0.0658 | 0.0362
Epoch 217/300, Loss: 0.0658 | 0.0362
Epoch 218/300, Loss: 0.0658 | 0.0362
Epoch 219/300, Loss: 0.0658 | 0.0362
Epoch 220/300, Loss: 0.0658 | 0.0362
Epoch 221/300, Loss: 0.0657 | 0.0362
Epoch 222/300, Loss: 0.0657 | 0.0362
Epoch 223/300, Loss: 0.0657 | 0.0362
Epoch 224/300, Loss: 0.0657 | 0.0362
Epoch 225/300, Loss: 0.0657 | 0.0362
Epoch 226/300, Loss: 0.0657 | 0.0362
Epoch 227/300, Loss: 0.0657 | 0.0362
Epoch 228/300, Loss: 0.0657 | 0.0362
Epoch 229/300, Loss: 0.0657 | 0.0361
Epoch 230/300, Loss: 0.0657 | 0.0361
Epoch 231/300, Loss: 0.0657 | 0.0361
Epoch 232/300, Loss: 0.0657 | 0.0361
Epoch 233/300, Loss: 0.0657 | 0.0361
Epoch 234/300, Loss: 0.0657 | 0.0361
Epoch 235/300, Loss: 0.0656 | 0.0361
Epoch 236/300, Loss: 0.0656 | 0.0361
Epoch 237/300, Loss: 0.0656 | 0.0361
Epoch 238/300, Loss: 0.0656 | 0.0361
Epoch 239/300, Loss: 0.0656 | 0.0361
Epoch 240/300, Loss: 0.0656 | 0.0361
Epoch 241/300, Loss: 0.0656 | 0.0361
Epoch 242/300, Loss: 0.0656 | 0.0361
Epoch 243/300, Loss: 0.0656 | 0.0361
Epoch 244/300, Loss: 0.0656 | 0.0361
Epoch 245/300, Loss: 0.0656 | 0.0361
Epoch 246/300, Loss: 0.0656 | 0.0361
Epoch 247/300, Loss: 0.0656 | 0.0361
Epoch 248/300, Loss: 0.0656 | 0.0360
Epoch 249/300, Loss: 0.0656 | 0.0360
Epoch 250/300, Loss: 0.0656 | 0.0360
Epoch 251/300, Loss: 0.0656 | 0.0360
Epoch 252/300, Loss: 0.0656 | 0.0360
Epoch 253/300, Loss: 0.0655 | 0.0360
Epoch 254/300, Loss: 0.0655 | 0.0360
Epoch 255/300, Loss: 0.0655 | 0.0360
Epoch 256/300, Loss: 0.0655 | 0.0360
Epoch 257/300, Loss: 0.0655 | 0.0360
Epoch 258/300, Loss: 0.0655 | 0.0360
Epoch 259/300, Loss: 0.0655 | 0.0360
Epoch 260/300, Loss: 0.0655 | 0.0360
Epoch 261/300, Loss: 0.0655 | 0.0360
Epoch 262/300, Loss: 0.0655 | 0.0360
Epoch 263/300, Loss: 0.0655 | 0.0360
Epoch 264/300, Loss: 0.0655 | 0.0360
Epoch 265/300, Loss: 0.0655 | 0.0360
Epoch 266/300, Loss: 0.0655 | 0.0360
Epoch 267/300, Loss: 0.0655 | 0.0360
Epoch 268/300, Loss: 0.0655 | 0.0360
Epoch 269/300, Loss: 0.0655 | 0.0360
Epoch 270/300, Loss: 0.0655 | 0.0360
Epoch 271/300, Loss: 0.0655 | 0.0360
Epoch 272/300, Loss: 0.0655 | 0.0360
Epoch 273/300, Loss: 0.0655 | 0.0360
Epoch 274/300, Loss: 0.0655 | 0.0360
Epoch 275/300, Loss: 0.0655 | 0.0360
Epoch 276/300, Loss: 0.0655 | 0.0359
Epoch 277/300, Loss: 0.0655 | 0.0359
Epoch 278/300, Loss: 0.0655 | 0.0359
Epoch 279/300, Loss: 0.0655 | 0.0359
Epoch 280/300, Loss: 0.0655 | 0.0359
Epoch 281/300, Loss: 0.0655 | 0.0359
Epoch 282/300, Loss: 0.0655 | 0.0359
Epoch 283/300, Loss: 0.0655 | 0.0359
Epoch 284/300, Loss: 0.0654 | 0.0359
Epoch 285/300, Loss: 0.0654 | 0.0359
Epoch 286/300, Loss: 0.0654 | 0.0359
Epoch 287/300, Loss: 0.0654 | 0.0359
Epoch 288/300, Loss: 0.0654 | 0.0359
Epoch 289/300, Loss: 0.0654 | 0.0359
Epoch 290/300, Loss: 0.0654 | 0.0359
Epoch 291/300, Loss: 0.0654 | 0.0359
Epoch 292/300, Loss: 0.0654 | 0.0359
Epoch 293/300, Loss: 0.0654 | 0.0359
Epoch 294/300, Loss: 0.0654 | 0.0359
Epoch 295/300, Loss: 0.0654 | 0.0359
Epoch 296/300, Loss: 0.0654 | 0.0359
Epoch 297/300, Loss: 0.0654 | 0.0359
Epoch 298/300, Loss: 0.0654 | 0.0359
Epoch 299/300, Loss: 0.0654 | 0.0359
Epoch 300/300, Loss: 0.0654 | 0.0359
Runtime (seconds): 118.28572916984558
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 41.371300807455555
RMSE: 6.4320526123046875
MAE: 6.4320526123046875
R-squared: nan
[187.60794]
