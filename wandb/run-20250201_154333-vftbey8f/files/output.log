ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-01 15:43:38,813][0m A new study created in memory with name: no-name-869fcb73-4f35-4086-94ad-1dfc144a9697[0m
[32m[I 2025-02-01 15:44:08,182][0m Trial 0 finished with value: 0.11481781551801482 and parameters: {'observation_period_num': 43, 'train_rates': 0.6071294987894675, 'learning_rate': 0.00048434467394903736, 'batch_size': 156, 'step_size': 15, 'gamma': 0.784560260041608}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:44:38,719][0m Trial 1 finished with value: 0.16110785194607669 and parameters: {'observation_period_num': 125, 'train_rates': 0.6963814936580912, 'learning_rate': 0.00029052363890131764, 'batch_size': 152, 'step_size': 4, 'gamma': 0.7503308968339779}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:45:03,005][0m Trial 2 finished with value: 0.453699108839035 and parameters: {'observation_period_num': 161, 'train_rates': 0.6423034289503988, 'learning_rate': 5.830073481380973e-06, 'batch_size': 186, 'step_size': 11, 'gamma': 0.8846586251009003}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:46:41,513][0m Trial 3 finished with value: 0.24880611077411413 and parameters: {'observation_period_num': 238, 'train_rates': 0.6427915717472201, 'learning_rate': 0.0003385262609130172, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9108459356329022}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:47:08,844][0m Trial 4 finished with value: 0.9668675830157903 and parameters: {'observation_period_num': 198, 'train_rates': 0.849439892533927, 'learning_rate': 1.075520671047022e-06, 'batch_size': 197, 'step_size': 9, 'gamma': 0.9321433246326921}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:48:11,427][0m Trial 5 finished with value: 0.26420979512234527 and parameters: {'observation_period_num': 129, 'train_rates': 0.9407458335178712, 'learning_rate': 4.1280353912168815e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.9258695800037786}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:48:36,186][0m Trial 6 finished with value: 0.1633870005607605 and parameters: {'observation_period_num': 26, 'train_rates': 0.9262952187076399, 'learning_rate': 0.0007459714709078435, 'batch_size': 248, 'step_size': 7, 'gamma': 0.9030292671165041}. Best is trial 0 with value: 0.11481781551801482.[0m
Early stopping at epoch 74
[32m[I 2025-02-01 15:48:57,674][0m Trial 7 finished with value: 1.1007481814174913 and parameters: {'observation_period_num': 17, 'train_rates': 0.72759722254574, 'learning_rate': 8.282062685173795e-06, 'batch_size': 179, 'step_size': 1, 'gamma': 0.8451622644024055}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:49:36,078][0m Trial 8 finished with value: 0.1646740499458511 and parameters: {'observation_period_num': 126, 'train_rates': 0.8980206127853988, 'learning_rate': 0.00021048173935128963, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8406348625677373}. Best is trial 0 with value: 0.11481781551801482.[0m
[32m[I 2025-02-01 15:49:56,185][0m Trial 9 finished with value: 0.09531271484921595 and parameters: {'observation_period_num': 18, 'train_rates': 0.6465676288837658, 'learning_rate': 0.0002655827754381814, 'batch_size': 250, 'step_size': 10, 'gamma': 0.8508513891771998}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:50:19,442][0m Trial 10 finished with value: 0.13322151518080555 and parameters: {'observation_period_num': 80, 'train_rates': 0.7952303916135521, 'learning_rate': 6.458363586687589e-05, 'batch_size': 243, 'step_size': 11, 'gamma': 0.9760867238575193}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:51:09,856][0m Trial 11 finished with value: 0.11955307135867328 and parameters: {'observation_period_num': 61, 'train_rates': 0.601780220558898, 'learning_rate': 0.0009774251319590956, 'batch_size': 86, 'step_size': 13, 'gamma': 0.7815173893594206}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:51:33,808][0m Trial 12 finished with value: 0.1125216845333046 and parameters: {'observation_period_num': 61, 'train_rates': 0.7284104661786726, 'learning_rate': 0.00011438625947813966, 'batch_size': 214, 'step_size': 12, 'gamma': 0.8109795750822123}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:51:57,378][0m Trial 13 finished with value: 0.12796938309276645 and parameters: {'observation_period_num': 86, 'train_rates': 0.7242124478043853, 'learning_rate': 0.00013141611321136718, 'batch_size': 219, 'step_size': 11, 'gamma': 0.8276997201847065}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:52:20,523][0m Trial 14 finished with value: 0.11773558913968331 and parameters: {'observation_period_num': 12, 'train_rates': 0.7809278551952125, 'learning_rate': 9.595246229600247e-05, 'batch_size': 255, 'step_size': 6, 'gamma': 0.8154417628251337}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:52:42,712][0m Trial 15 finished with value: 0.21465456114430082 and parameters: {'observation_period_num': 56, 'train_rates': 0.681758527475442, 'learning_rate': 1.8034721955889423e-05, 'batch_size': 224, 'step_size': 12, 'gamma': 0.8665856371473721}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:53:27,367][0m Trial 16 finished with value: 0.18287918595003302 and parameters: {'observation_period_num': 97, 'train_rates': 0.7752280445387144, 'learning_rate': 2.6572170630876093e-05, 'batch_size': 114, 'step_size': 9, 'gamma': 0.7892557032568285}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:53:51,913][0m Trial 17 finished with value: 0.10403611615115071 and parameters: {'observation_period_num': 46, 'train_rates': 0.6682949857911854, 'learning_rate': 0.00014788875326195367, 'batch_size': 214, 'step_size': 10, 'gamma': 0.8706053507979706}. Best is trial 9 with value: 0.09531271484921595.[0m
[32m[I 2025-02-01 15:54:16,537][0m Trial 18 finished with value: 0.08789549396914456 and parameters: {'observation_period_num': 8, 'train_rates': 0.6570259315131902, 'learning_rate': 0.0001760397763962606, 'batch_size': 197, 'step_size': 5, 'gamma': 0.8613015324584462}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:54:47,873][0m Trial 19 finished with value: 0.11123242499767147 and parameters: {'observation_period_num': 11, 'train_rates': 0.8214992938253459, 'learning_rate': 4.837130476742561e-05, 'batch_size': 178, 'step_size': 4, 'gamma': 0.9522388472367022}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:57:10,769][0m Trial 20 finished with value: 0.144603517509394 and parameters: {'observation_period_num': 175, 'train_rates': 0.6401834077802369, 'learning_rate': 0.0004898819777995747, 'batch_size': 29, 'step_size': 5, 'gamma': 0.861070350693319}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:57:32,890][0m Trial 21 finished with value: 0.1275274131947489 and parameters: {'observation_period_num': 38, 'train_rates': 0.6788817371117783, 'learning_rate': 0.00016705279533167985, 'batch_size': 230, 'step_size': 2, 'gamma': 0.8759737599794767}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:57:57,152][0m Trial 22 finished with value: 0.15477407434372584 and parameters: {'observation_period_num': 34, 'train_rates': 0.655687429650921, 'learning_rate': 8.303316173446469e-05, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8916446392759716}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:58:20,605][0m Trial 23 finished with value: 0.09277157006374172 and parameters: {'observation_period_num': 12, 'train_rates': 0.7509378453238963, 'learning_rate': 0.00020308304836634785, 'batch_size': 240, 'step_size': 7, 'gamma': 0.8549056631629048}. Best is trial 18 with value: 0.08789549396914456.[0m
[32m[I 2025-02-01 15:58:42,565][0m Trial 24 finished with value: 0.08015087315563782 and parameters: {'observation_period_num': 5, 'train_rates': 0.7061854111320305, 'learning_rate': 0.0003013278996634321, 'batch_size': 237, 'step_size': 7, 'gamma': 0.8444228951539271}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 15:59:05,996][0m Trial 25 finished with value: 0.08089488999624002 and parameters: {'observation_period_num': 5, 'train_rates': 0.7489855735050504, 'learning_rate': 0.0004471674627410808, 'batch_size': 234, 'step_size': 6, 'gamma': 0.8318164156749354}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 15:59:36,343][0m Trial 26 finished with value: 0.08047737527643092 and parameters: {'observation_period_num': 5, 'train_rates': 0.7143421984461065, 'learning_rate': 0.0005371063978253063, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8223299809033907}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:00:07,471][0m Trial 27 finished with value: 0.127091004884201 and parameters: {'observation_period_num': 74, 'train_rates': 0.7048814099730857, 'learning_rate': 0.0006165938107519067, 'batch_size': 167, 'step_size': 3, 'gamma': 0.8035533857105881}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:00:47,835][0m Trial 28 finished with value: 0.12540653358552267 and parameters: {'observation_period_num': 108, 'train_rates': 0.7530375451262441, 'learning_rate': 0.00038687645141638694, 'batch_size': 126, 'step_size': 6, 'gamma': 0.8289671014181373}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:01:21,174][0m Trial 29 finished with value: 0.1073185819429609 and parameters: {'observation_period_num': 37, 'train_rates': 0.8187426868310521, 'learning_rate': 0.0009243124044069315, 'batch_size': 167, 'step_size': 3, 'gamma': 0.7602717770381022}. Best is trial 24 with value: 0.08015087315563782.[0m
Early stopping at epoch 57
[32m[I 2025-02-01 16:01:46,390][0m Trial 30 finished with value: 0.2053985744714737 and parameters: {'observation_period_num': 244, 'train_rates': 0.9835620961458303, 'learning_rate': 0.0006058856291616189, 'batch_size': 136, 'step_size': 1, 'gamma': 0.7915733790858531}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:02:12,580][0m Trial 31 finished with value: 0.08843367144294169 and parameters: {'observation_period_num': 28, 'train_rates': 0.7030417035859009, 'learning_rate': 0.00039752512728102147, 'batch_size': 200, 'step_size': 5, 'gamma': 0.8306426026872322}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:02:32,815][0m Trial 32 finished with value: 0.10560989890704146 and parameters: {'observation_period_num': 7, 'train_rates': 0.6182925503116323, 'learning_rate': 0.0002765408180710221, 'batch_size': 232, 'step_size': 5, 'gamma': 0.8194740696617594}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:03:00,096][0m Trial 33 finished with value: 0.08762057188760729 and parameters: {'observation_period_num': 5, 'train_rates': 0.7524749875225608, 'learning_rate': 0.0004605336111209188, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8400080778124701}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:03:27,723][0m Trial 34 finished with value: 0.12658700791940297 and parameters: {'observation_period_num': 49, 'train_rates': 0.7489028742783279, 'learning_rate': 0.000494819976896396, 'batch_size': 189, 'step_size': 3, 'gamma': 0.7699831831833803}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:03:58,214][0m Trial 35 finished with value: 0.518993087104169 and parameters: {'observation_period_num': 29, 'train_rates': 0.7086972528426145, 'learning_rate': 1.598143815073081e-06, 'batch_size': 161, 'step_size': 7, 'gamma': 0.7981927019090305}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:04:24,010][0m Trial 36 finished with value: 0.20850824991058062 and parameters: {'observation_period_num': 213, 'train_rates': 0.8108653580417655, 'learning_rate': 0.00025923956220066707, 'batch_size': 219, 'step_size': 4, 'gamma': 0.8395674009618617}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:04:55,580][0m Trial 37 finished with value: 0.09732842872316917 and parameters: {'observation_period_num': 5, 'train_rates': 0.8495172966641146, 'learning_rate': 0.0003662436054781296, 'batch_size': 185, 'step_size': 6, 'gamma': 0.8945150061042177}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:05:21,134][0m Trial 38 finished with value: 0.15075668159010108 and parameters: {'observation_period_num': 70, 'train_rates': 0.7620356338473014, 'learning_rate': 0.000714033197540816, 'batch_size': 208, 'step_size': 2, 'gamma': 0.8098714533474276}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:05:42,613][0m Trial 39 finished with value: 0.3762259456244382 and parameters: {'observation_period_num': 144, 'train_rates': 0.7340930862604509, 'learning_rate': 1.1002937995160603e-05, 'batch_size': 230, 'step_size': 8, 'gamma': 0.837253034188099}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:06:55,088][0m Trial 40 finished with value: 0.08812132372960939 and parameters: {'observation_period_num': 25, 'train_rates': 0.6895447083119904, 'learning_rate': 0.0005122062205323048, 'batch_size': 64, 'step_size': 4, 'gamma': 0.8757412666355251}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:07:19,683][0m Trial 41 finished with value: 0.11870040121175572 and parameters: {'observation_period_num': 21, 'train_rates': 0.6256446534626887, 'learning_rate': 0.00020317587993823346, 'batch_size': 193, 'step_size': 5, 'gamma': 0.8520964388828665}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:07:52,595][0m Trial 42 finished with value: 0.09728749583750994 and parameters: {'observation_period_num': 5, 'train_rates': 0.6669877045648885, 'learning_rate': 0.00029001027655005987, 'batch_size': 151, 'step_size': 2, 'gamma': 0.82160769666912}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:08:20,817][0m Trial 43 finished with value: 0.09139155924718068 and parameters: {'observation_period_num': 47, 'train_rates': 0.7167440410557133, 'learning_rate': 0.0007138171181468222, 'batch_size': 181, 'step_size': 7, 'gamma': 0.9090060161926836}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:08:44,223][0m Trial 44 finished with value: 0.08089649517205544 and parameters: {'observation_period_num': 20, 'train_rates': 0.738647578396302, 'learning_rate': 0.0009608701983205663, 'batch_size': 238, 'step_size': 6, 'gamma': 0.8445590707533215}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:09:07,558][0m Trial 45 finished with value: 0.08136036079732856 and parameters: {'observation_period_num': 21, 'train_rates': 0.7862724451479992, 'learning_rate': 0.000990046885764376, 'batch_size': 242, 'step_size': 8, 'gamma': 0.8447264482184247}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:09:31,047][0m Trial 46 finished with value: 0.0861547769520615 and parameters: {'observation_period_num': 22, 'train_rates': 0.7922229013467788, 'learning_rate': 0.0009932910913584662, 'batch_size': 241, 'step_size': 8, 'gamma': 0.8846950877526586}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:09:53,213][0m Trial 47 finished with value: 0.11944220438599587 and parameters: {'observation_period_num': 40, 'train_rates': 0.8630872669705, 'learning_rate': 0.0007924777571674295, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8535592099369171}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:10:16,676][0m Trial 48 finished with value: 0.10839015681844923 and parameters: {'observation_period_num': 57, 'train_rates': 0.7727008627998005, 'learning_rate': 0.0006211591638949604, 'batch_size': 238, 'step_size': 6, 'gamma': 0.845753829830832}. Best is trial 24 with value: 0.08015087315563782.[0m
[32m[I 2025-02-01 16:10:38,142][0m Trial 49 finished with value: 0.8365696881227432 and parameters: {'observation_period_num': 18, 'train_rates': 0.7328804083493882, 'learning_rate': 4.009044840489101e-06, 'batch_size': 248, 'step_size': 7, 'gamma': 0.8030108850566817}. Best is trial 24 with value: 0.08015087315563782.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-01 16:10:38,153][0m A new study created in memory with name: no-name-1904fc84-8dd9-4a5b-b7af-c8a665969038[0m
Early stopping at epoch 34
[32m[I 2025-02-01 16:10:47,672][0m Trial 0 finished with value: 1.5332796243911093 and parameters: {'observation_period_num': 194, 'train_rates': 0.8980928691872125, 'learning_rate': 1.8936237684698684e-06, 'batch_size': 234, 'step_size': 1, 'gamma': 0.7529519418932042}. Best is trial 0 with value: 1.5332796243911093.[0m
[32m[I 2025-02-01 16:11:12,987][0m Trial 1 finished with value: 0.6426294108431817 and parameters: {'observation_period_num': 213, 'train_rates': 0.6015556651765379, 'learning_rate': 5.915423458150801e-06, 'batch_size': 174, 'step_size': 12, 'gamma': 0.8260610864349276}. Best is trial 1 with value: 0.6426294108431817.[0m
[32m[I 2025-02-01 16:11:43,431][0m Trial 2 finished with value: 0.27356039762027623 and parameters: {'observation_period_num': 182, 'train_rates': 0.9086419803385578, 'learning_rate': 5.259456900633704e-05, 'batch_size': 192, 'step_size': 11, 'gamma': 0.7520170150397385}. Best is trial 2 with value: 0.27356039762027623.[0m
[32m[I 2025-02-01 16:12:06,861][0m Trial 3 finished with value: 0.7074736672581817 and parameters: {'observation_period_num': 71, 'train_rates': 0.7507964452002801, 'learning_rate': 3.3462451652171022e-06, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8401003266287321}. Best is trial 2 with value: 0.27356039762027623.[0m
Early stopping at epoch 71
[32m[I 2025-02-01 16:12:54,626][0m Trial 4 finished with value: 0.46775674661663846 and parameters: {'observation_period_num': 70, 'train_rates': 0.7716950360589251, 'learning_rate': 2.1023312375674757e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.8392642220980325}. Best is trial 2 with value: 0.27356039762027623.[0m
[32m[I 2025-02-01 16:15:25,546][0m Trial 5 finished with value: 0.14004463067566844 and parameters: {'observation_period_num': 5, 'train_rates': 0.8318898440486566, 'learning_rate': 1.9977684565627307e-06, 'batch_size': 35, 'step_size': 10, 'gamma': 0.9779947030358737}. Best is trial 5 with value: 0.14004463067566844.[0m
[32m[I 2025-02-01 16:17:54,574][0m Trial 6 finished with value: 0.3332137314058979 and parameters: {'observation_period_num': 221, 'train_rates': 0.7059844368759076, 'learning_rate': 2.635738542532334e-06, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9494399952324477}. Best is trial 5 with value: 0.14004463067566844.[0m
[32m[I 2025-02-01 16:18:18,754][0m Trial 7 finished with value: 0.29548317193984985 and parameters: {'observation_period_num': 102, 'train_rates': 0.9579709742179678, 'learning_rate': 0.00047681280874901054, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9453480315928309}. Best is trial 5 with value: 0.14004463067566844.[0m
[32m[I 2025-02-01 16:18:45,975][0m Trial 8 finished with value: 0.5728365011320783 and parameters: {'observation_period_num': 91, 'train_rates': 0.8111189935889512, 'learning_rate': 3.364526711413353e-06, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8408067965907635}. Best is trial 5 with value: 0.14004463067566844.[0m
[32m[I 2025-02-01 16:19:56,652][0m Trial 9 finished with value: 0.15396048038573212 and parameters: {'observation_period_num': 158, 'train_rates': 0.7104787986603248, 'learning_rate': 0.0005580366867044399, 'batch_size': 66, 'step_size': 5, 'gamma': 0.9055619082726668}. Best is trial 5 with value: 0.14004463067566844.[0m
[32m[I 2025-02-01 16:20:42,610][0m Trial 10 finished with value: 0.1283641169557657 and parameters: {'observation_period_num': 6, 'train_rates': 0.8488181269870474, 'learning_rate': 2.0654016389604787e-05, 'batch_size': 122, 'step_size': 6, 'gamma': 0.9835729337380945}. Best is trial 10 with value: 0.1283641169557657.[0m
[32m[I 2025-02-01 16:21:28,060][0m Trial 11 finished with value: 0.11496088736587101 and parameters: {'observation_period_num': 6, 'train_rates': 0.844656915374102, 'learning_rate': 3.4721303981326854e-05, 'batch_size': 119, 'step_size': 6, 'gamma': 0.989702647680512}. Best is trial 11 with value: 0.11496088736587101.[0m
[32m[I 2025-02-01 16:22:13,940][0m Trial 12 finished with value: 0.11707485889890545 and parameters: {'observation_period_num': 5, 'train_rates': 0.8644436738465691, 'learning_rate': 6.098175482532243e-05, 'batch_size': 126, 'step_size': 6, 'gamma': 0.9884858120289699}. Best is trial 11 with value: 0.11496088736587101.[0m
[32m[I 2025-02-01 16:23:04,443][0m Trial 13 finished with value: 0.04943442344665527 and parameters: {'observation_period_num': 40, 'train_rates': 0.9817583756279298, 'learning_rate': 0.00010045968631504395, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9130452153192891}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:23:44,620][0m Trial 14 finished with value: 0.08064062893390656 and parameters: {'observation_period_num': 46, 'train_rates': 0.9899497613747412, 'learning_rate': 0.00014948961188320783, 'batch_size': 157, 'step_size': 4, 'gamma': 0.8941334866085376}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:24:26,833][0m Trial 15 finished with value: 0.0686337798833847 and parameters: {'observation_period_num': 56, 'train_rates': 0.9860190469956831, 'learning_rate': 0.0001710963605102243, 'batch_size': 149, 'step_size': 3, 'gamma': 0.8907631795149414}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:25:29,357][0m Trial 16 finished with value: 0.23863539990309057 and parameters: {'observation_period_num': 44, 'train_rates': 0.9483541430434843, 'learning_rate': 0.00018211418302941653, 'batch_size': 94, 'step_size': 3, 'gamma': 0.8864634279342353}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:26:07,722][0m Trial 17 finished with value: 0.1092153936624527 and parameters: {'observation_period_num': 131, 'train_rates': 0.9849388413794393, 'learning_rate': 0.00014090173434369417, 'batch_size': 157, 'step_size': 3, 'gamma': 0.9235719455621467}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:26:48,378][0m Trial 18 finished with value: 0.17938017381720567 and parameters: {'observation_period_num': 46, 'train_rates': 0.9284202972701046, 'learning_rate': 0.00024346352409020523, 'batch_size': 149, 'step_size': 7, 'gamma': 0.8713471543278981}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:27:43,826][0m Trial 19 finished with value: 0.1718107416193084 and parameters: {'observation_period_num': 113, 'train_rates': 0.8937082701382312, 'learning_rate': 0.0009495592451030079, 'batch_size': 99, 'step_size': 3, 'gamma': 0.7928189406626532}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:28:08,397][0m Trial 20 finished with value: 0.1824477307248914 and parameters: {'observation_period_num': 142, 'train_rates': 0.6288347385910203, 'learning_rate': 7.348787333181959e-05, 'batch_size': 194, 'step_size': 8, 'gamma': 0.9236873625613216}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:28:46,700][0m Trial 21 finished with value: 0.061997149139642715 and parameters: {'observation_period_num': 40, 'train_rates': 0.9827536457587744, 'learning_rate': 0.00011685799672784696, 'batch_size': 160, 'step_size': 4, 'gamma': 0.8889080486032553}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:29:31,466][0m Trial 22 finished with value: 0.21301420359578851 and parameters: {'observation_period_num': 31, 'train_rates': 0.9500980620581173, 'learning_rate': 0.00033620796516276545, 'batch_size': 138, 'step_size': 4, 'gamma': 0.8697187215486041}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:30:32,070][0m Trial 23 finished with value: 0.11042109876871109 and parameters: {'observation_period_num': 68, 'train_rates': 0.9862132277276945, 'learning_rate': 0.00010764799658319572, 'batch_size': 101, 'step_size': 2, 'gamma': 0.8668460885780566}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:31:06,307][0m Trial 24 finished with value: 0.26245194502864116 and parameters: {'observation_period_num': 30, 'train_rates': 0.9226803957409627, 'learning_rate': 9.540373169324396e-06, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9210924865738308}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:31:40,550][0m Trial 25 finished with value: 0.3114269971847534 and parameters: {'observation_period_num': 88, 'train_rates': 0.957581606701601, 'learning_rate': 0.00010682673698226125, 'batch_size': 176, 'step_size': 5, 'gamma': 0.9550363282482384}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:32:19,998][0m Trial 26 finished with value: 0.1866050055547125 and parameters: {'observation_period_num': 244, 'train_rates': 0.8844503764882758, 'learning_rate': 0.000273743614243194, 'batch_size': 141, 'step_size': 4, 'gamma': 0.9035091766617172}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:33:47,030][0m Trial 27 finished with value: 0.2898736141744207 and parameters: {'observation_period_num': 60, 'train_rates': 0.9370652712702553, 'learning_rate': 3.668526298135241e-05, 'batch_size': 65, 'step_size': 2, 'gamma': 0.8567700988499484}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:34:17,350][0m Trial 28 finished with value: 0.31541258096694946 and parameters: {'observation_period_num': 26, 'train_rates': 0.9677494767627282, 'learning_rate': 8.57681655287663e-05, 'batch_size': 212, 'step_size': 7, 'gamma': 0.8844221744113843}. Best is trial 13 with value: 0.04943442344665527.[0m
Early stopping at epoch 55
[32m[I 2025-02-01 16:34:45,459][0m Trial 29 finished with value: 0.6523535052935282 and parameters: {'observation_period_num': 85, 'train_rates': 0.884099894079206, 'learning_rate': 1.6786824273162388e-05, 'batch_size': 112, 'step_size': 1, 'gamma': 0.8049033481723213}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:35:22,034][0m Trial 30 finished with value: 0.7405973633941338 and parameters: {'observation_period_num': 112, 'train_rates': 0.9140800503853806, 'learning_rate': 1.1140030334193274e-06, 'batch_size': 156, 'step_size': 5, 'gamma': 0.9331886984630445}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:36:00,349][0m Trial 31 finished with value: 0.07300955802202225 and parameters: {'observation_period_num': 49, 'train_rates': 0.9894556706794932, 'learning_rate': 0.0001661901259393449, 'batch_size': 164, 'step_size': 4, 'gamma': 0.9010186154365837}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:36:36,629][0m Trial 32 finished with value: 0.3264862895011902 and parameters: {'observation_period_num': 52, 'train_rates': 0.9666384119111173, 'learning_rate': 0.00018237021226923895, 'batch_size': 167, 'step_size': 2, 'gamma': 0.9078622066554641}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:37:10,959][0m Trial 33 finished with value: 0.08273376524448395 and parameters: {'observation_period_num': 28, 'train_rates': 0.9868461818494927, 'learning_rate': 5.024320057780318e-05, 'batch_size': 183, 'step_size': 4, 'gamma': 0.8811024639486403}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:37:55,292][0m Trial 34 finished with value: 0.19783094617986927 and parameters: {'observation_period_num': 82, 'train_rates': 0.9328479173459401, 'learning_rate': 0.0006952110293758423, 'batch_size': 133, 'step_size': 3, 'gamma': 0.8989855606009935}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:38:23,963][0m Trial 35 finished with value: 0.1423574045766145 and parameters: {'observation_period_num': 20, 'train_rates': 0.9075871833126524, 'learning_rate': 0.0003597194750946795, 'batch_size': 221, 'step_size': 5, 'gamma': 0.9631758166058654}. Best is trial 13 with value: 0.04943442344665527.[0m
Early stopping at epoch 74
[32m[I 2025-02-01 16:38:51,558][0m Trial 36 finished with value: 0.48135724663734436 and parameters: {'observation_period_num': 61, 'train_rates': 0.9696310760139664, 'learning_rate': 0.00021192226881810896, 'batch_size': 167, 'step_size': 1, 'gamma': 0.8539311154875869}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:39:32,134][0m Trial 37 finished with value: 0.20783235299187666 and parameters: {'observation_period_num': 38, 'train_rates': 0.9388318832985248, 'learning_rate': 0.00010300401783047889, 'batch_size': 145, 'step_size': 7, 'gamma': 0.9120230490746529}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:40:29,377][0m Trial 38 finished with value: 0.1554570742280913 and parameters: {'observation_period_num': 179, 'train_rates': 0.7885122524839102, 'learning_rate': 4.586824682326908e-05, 'batch_size': 85, 'step_size': 4, 'gamma': 0.9367654333266711}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:40:54,408][0m Trial 39 finished with value: 0.17113899088360632 and parameters: {'observation_period_num': 68, 'train_rates': 0.6187723392207151, 'learning_rate': 0.00037883183057494784, 'batch_size': 187, 'step_size': 2, 'gamma': 0.7653394804432407}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:41:40,208][0m Trial 40 finished with value: 0.08504248814882008 and parameters: {'observation_period_num': 18, 'train_rates': 0.7379122392369147, 'learning_rate': 0.00013537540949443288, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9693508026619362}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:42:20,457][0m Trial 41 finished with value: 0.06763172894716263 and parameters: {'observation_period_num': 50, 'train_rates': 0.979053624564766, 'learning_rate': 0.0001418563046404767, 'batch_size': 155, 'step_size': 4, 'gamma': 0.8939098231518638}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:42:56,975][0m Trial 42 finished with value: 0.406920462846756 and parameters: {'observation_period_num': 76, 'train_rates': 0.9678606000825095, 'learning_rate': 7.09988780285883e-05, 'batch_size': 164, 'step_size': 3, 'gamma': 0.8942791149080066}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:43:43,686][0m Trial 43 finished with value: 0.05320098623633385 and parameters: {'observation_period_num': 55, 'train_rates': 0.9743665857137717, 'learning_rate': 0.0002587775863391444, 'batch_size': 132, 'step_size': 6, 'gamma': 0.8757810747029094}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:44:30,266][0m Trial 44 finished with value: 0.22982946038246155 and parameters: {'observation_period_num': 57, 'train_rates': 0.9548444876413625, 'learning_rate': 0.000528211915980486, 'batch_size': 131, 'step_size': 6, 'gamma': 0.8501243698290276}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:45:10,854][0m Trial 45 finished with value: 0.11077812034347805 and parameters: {'observation_period_num': 94, 'train_rates': 0.6622132993444269, 'learning_rate': 0.00027112060332517673, 'batch_size': 114, 'step_size': 8, 'gamma': 0.8742894593848362}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:45:50,237][0m Trial 46 finished with value: 0.16512643144080777 and parameters: {'observation_period_num': 17, 'train_rates': 0.8702854210470494, 'learning_rate': 2.6357332253345566e-05, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8169265920589703}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:47:52,600][0m Trial 47 finished with value: 0.17454992780703624 and parameters: {'observation_period_num': 102, 'train_rates': 0.9084209534017982, 'learning_rate': 0.00011840620823528853, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8298313904034302}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:48:19,926][0m Trial 48 finished with value: 0.11942387382114317 and parameters: {'observation_period_num': 41, 'train_rates': 0.8185802634859295, 'learning_rate': 8.2655526293641e-05, 'batch_size': 201, 'step_size': 8, 'gamma': 0.9140117668362215}. Best is trial 13 with value: 0.04943442344665527.[0m
[32m[I 2025-02-01 16:49:08,235][0m Trial 49 finished with value: 0.09185514599084854 and parameters: {'observation_period_num': 71, 'train_rates': 0.973513519885891, 'learning_rate': 0.00021823085339555992, 'batch_size': 125, 'step_size': 7, 'gamma': 0.8876006198204268}. Best is trial 13 with value: 0.04943442344665527.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-01 16:49:08,245][0m A new study created in memory with name: no-name-5fd8e4f0-9eea-44e0-8fc3-3b3e20a6534e[0m
[32m[I 2025-02-01 16:50:12,606][0m Trial 0 finished with value: 0.4287033677101135 and parameters: {'observation_period_num': 9, 'train_rates': 0.9721668984985137, 'learning_rate': 2.9976141322521934e-06, 'batch_size': 93, 'step_size': 8, 'gamma': 0.979364526796394}. Best is trial 0 with value: 0.4287033677101135.[0m
[32m[I 2025-02-01 16:50:53,898][0m Trial 1 finished with value: 1.0859809481864713 and parameters: {'observation_period_num': 123, 'train_rates': 0.6028049823873022, 'learning_rate': 1.0624710390187e-06, 'batch_size': 102, 'step_size': 4, 'gamma': 0.8872145513203387}. Best is trial 0 with value: 0.4287033677101135.[0m
Early stopping at epoch 52
[32m[I 2025-02-01 16:51:20,450][0m Trial 2 finished with value: 0.37980127193386715 and parameters: {'observation_period_num': 119, 'train_rates': 0.8588270213049618, 'learning_rate': 6.321201222995257e-05, 'batch_size': 106, 'step_size': 1, 'gamma': 0.790296261437753}. Best is trial 2 with value: 0.37980127193386715.[0m
[32m[I 2025-02-01 16:52:52,173][0m Trial 3 finished with value: 0.1226209586197482 and parameters: {'observation_period_num': 123, 'train_rates': 0.7679515917126869, 'learning_rate': 5.7767201024140304e-05, 'batch_size': 52, 'step_size': 2, 'gamma': 0.9649935082192602}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:53:29,697][0m Trial 4 finished with value: 0.13755932134209256 and parameters: {'observation_period_num': 157, 'train_rates': 0.7054418230333239, 'learning_rate': 8.00650291397552e-05, 'batch_size': 125, 'step_size': 14, 'gamma': 0.947801025262627}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:55:05,198][0m Trial 5 finished with value: 0.2526575082540512 and parameters: {'observation_period_num': 228, 'train_rates': 0.9268481350475868, 'learning_rate': 1.9800914507158426e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9338173760270037}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:55:25,483][0m Trial 6 finished with value: 0.40249295933888507 and parameters: {'observation_period_num': 194, 'train_rates': 0.6048987728506725, 'learning_rate': 7.179378743437624e-06, 'batch_size': 231, 'step_size': 13, 'gamma': 0.9666876360077515}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:55:59,124][0m Trial 7 finished with value: 0.133344940907721 and parameters: {'observation_period_num': 133, 'train_rates': 0.6432054350580866, 'learning_rate': 0.0007276825218523743, 'batch_size': 138, 'step_size': 15, 'gamma': 0.7961038602951918}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:57:48,252][0m Trial 8 finished with value: 0.16115991582635592 and parameters: {'observation_period_num': 116, 'train_rates': 0.8871854291663341, 'learning_rate': 0.00015870301101470155, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8620263134383945}. Best is trial 3 with value: 0.1226209586197482.[0m
[32m[I 2025-02-01 16:59:59,580][0m Trial 9 finished with value: 0.09624858483770825 and parameters: {'observation_period_num': 41, 'train_rates': 0.741078832514085, 'learning_rate': 2.1767717229053347e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.964019125286165}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:00:27,067][0m Trial 10 finished with value: 0.2002520241599152 and parameters: {'observation_period_num': 33, 'train_rates': 0.7875610537188952, 'learning_rate': 1.3527692807901674e-05, 'batch_size': 192, 'step_size': 7, 'gamma': 0.8890732032681651}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:04:03,830][0m Trial 11 finished with value: 0.10820331728016888 and parameters: {'observation_period_num': 61, 'train_rates': 0.7578034057000985, 'learning_rate': 0.0004012675910659942, 'batch_size': 22, 'step_size': 5, 'gamma': 0.9222484232508569}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:07:31,338][0m Trial 12 finished with value: 0.09730504587346336 and parameters: {'observation_period_num': 57, 'train_rates': 0.7128224288786913, 'learning_rate': 0.000832432953401429, 'batch_size': 22, 'step_size': 6, 'gamma': 0.9143245120359602}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:10:58,723][0m Trial 13 finished with value: 0.12193251085902398 and parameters: {'observation_period_num': 67, 'train_rates': 0.702677651024113, 'learning_rate': 0.0002642698778174568, 'batch_size': 22, 'step_size': 11, 'gamma': 0.8392068323084257}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:11:29,075][0m Trial 14 finished with value: 0.26357645364034743 and parameters: {'observation_period_num': 71, 'train_rates': 0.7016656664060583, 'learning_rate': 6.739655986126868e-06, 'batch_size': 164, 'step_size': 10, 'gamma': 0.912754953606512}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:12:44,008][0m Trial 15 finished with value: 0.10244794575962177 and parameters: {'observation_period_num': 9, 'train_rates': 0.8409514606693338, 'learning_rate': 2.4456723422051417e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.989336057597703}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:16:23,371][0m Trial 16 finished with value: 0.12479429736791194 and parameters: {'observation_period_num': 86, 'train_rates': 0.7303330738607606, 'learning_rate': 0.0008583366323208973, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8338470682899531}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:17:23,721][0m Trial 17 finished with value: 0.11791246200370933 and parameters: {'observation_period_num': 42, 'train_rates': 0.6597258518655891, 'learning_rate': 0.00011775862295849095, 'batch_size': 75, 'step_size': 4, 'gamma': 0.7589735200472878}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:17:47,076][0m Trial 18 finished with value: 0.1775539449670098 and parameters: {'observation_period_num': 97, 'train_rates': 0.8195126088984854, 'learning_rate': 4.0239692428750614e-05, 'batch_size': 235, 'step_size': 8, 'gamma': 0.9059944975518475}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:19:43,580][0m Trial 19 finished with value: 0.11567035227066005 and parameters: {'observation_period_num': 33, 'train_rates': 0.6558894253701959, 'learning_rate': 9.971128420863456e-06, 'batch_size': 38, 'step_size': 3, 'gamma': 0.9454862685437352}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:20:11,975][0m Trial 20 finished with value: 0.7749648599789061 and parameters: {'observation_period_num': 151, 'train_rates': 0.7416599813471028, 'learning_rate': 3.1196368053767143e-06, 'batch_size': 179, 'step_size': 6, 'gamma': 0.8621898076235313}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:21:23,256][0m Trial 21 finished with value: 0.0985113787105178 and parameters: {'observation_period_num': 10, 'train_rates': 0.8286972864484893, 'learning_rate': 2.4053108366356692e-05, 'batch_size': 76, 'step_size': 6, 'gamma': 0.9815364039565221}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:22:32,490][0m Trial 22 finished with value: 0.1140568512062305 and parameters: {'observation_period_num': 47, 'train_rates': 0.8137045530355134, 'learning_rate': 3.2013510703605375e-05, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9532552663171951}. Best is trial 9 with value: 0.09624858483770825.[0m
[32m[I 2025-02-01 17:24:39,473][0m Trial 23 finished with value: 0.08717581632733346 and parameters: {'observation_period_num': 8, 'train_rates': 0.7882899965619038, 'learning_rate': 1.478000451538773e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9714485829381044}. Best is trial 23 with value: 0.08717581632733346.[0m
[32m[I 2025-02-01 17:26:55,603][0m Trial 24 finished with value: 0.13680445807703973 and parameters: {'observation_period_num': 30, 'train_rates': 0.7915022824108502, 'learning_rate': 3.7767464656955492e-06, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9358002541294526}. Best is trial 23 with value: 0.08717581632733346.[0m
[32m[I 2025-02-01 17:29:08,182][0m Trial 25 finished with value: 0.12577489723673377 and parameters: {'observation_period_num': 83, 'train_rates': 0.6876781635858834, 'learning_rate': 1.3844367670022103e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.8995141774564003}. Best is trial 23 with value: 0.08717581632733346.[0m
[32m[I 2025-02-01 17:30:30,071][0m Trial 26 finished with value: 0.39383884514417483 and parameters: {'observation_period_num': 54, 'train_rates': 0.7393994905928267, 'learning_rate': 1.4011319047119159e-06, 'batch_size': 59, 'step_size': 12, 'gamma': 0.9665184776868684}. Best is trial 23 with value: 0.08717581632733346.[0m
[32m[I 2025-02-01 17:35:19,832][0m Trial 27 finished with value: 0.08319155666041592 and parameters: {'observation_period_num': 16, 'train_rates': 0.7765916685040163, 'learning_rate': 0.0001290349427495362, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9277080437405427}. Best is trial 27 with value: 0.08319155666041592.[0m
[32m[I 2025-02-01 17:36:23,352][0m Trial 28 finished with value: 0.11809681805370506 and parameters: {'observation_period_num': 25, 'train_rates': 0.8829706298343428, 'learning_rate': 0.000171858444191688, 'batch_size': 87, 'step_size': 9, 'gamma': 0.9296910216905999}. Best is trial 27 with value: 0.08319155666041592.[0m
[32m[I 2025-02-01 17:36:49,721][0m Trial 29 finished with value: 0.05964331328868866 and parameters: {'observation_period_num': 8, 'train_rates': 0.9857392660941815, 'learning_rate': 4.3885582160670965e-05, 'batch_size': 255, 'step_size': 10, 'gamma': 0.972202331640702}. Best is trial 29 with value: 0.05964331328868866.[0m
[32m[I 2025-02-01 17:37:20,022][0m Trial 30 finished with value: 0.05136840045452118 and parameters: {'observation_period_num': 5, 'train_rates': 0.9834904522772281, 'learning_rate': 4.930514041297924e-05, 'batch_size': 222, 'step_size': 11, 'gamma': 0.9781569118949897}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:37:46,471][0m Trial 31 finished with value: 0.2672424912452698 and parameters: {'observation_period_num': 8, 'train_rates': 0.9622168441754252, 'learning_rate': 4.5525389919936564e-05, 'batch_size': 247, 'step_size': 11, 'gamma': 0.9884031665346853}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:38:16,557][0m Trial 32 finished with value: 0.32612642645835876 and parameters: {'observation_period_num': 21, 'train_rates': 0.9715528576268794, 'learning_rate': 0.00013056446193771915, 'batch_size': 219, 'step_size': 10, 'gamma': 0.9735785810937905}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:38:42,799][0m Trial 33 finished with value: 0.06338392943143845 and parameters: {'observation_period_num': 7, 'train_rates': 0.9898255485706613, 'learning_rate': 8.835363775867753e-05, 'batch_size': 255, 'step_size': 12, 'gamma': 0.9521398458431766}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:39:07,428][0m Trial 34 finished with value: 0.20916303992271423 and parameters: {'observation_period_num': 19, 'train_rates': 0.9358544828646641, 'learning_rate': 8.664437387857814e-05, 'batch_size': 252, 'step_size': 12, 'gamma': 0.9501755047126053}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:39:37,656][0m Trial 35 finished with value: 0.22002056241035461 and parameters: {'observation_period_num': 24, 'train_rates': 0.937780424212344, 'learning_rate': 0.00027496461424073986, 'batch_size': 211, 'step_size': 12, 'gamma': 0.9516507062967453}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:40:04,481][0m Trial 36 finished with value: 0.061996474862098694 and parameters: {'observation_period_num': 6, 'train_rates': 0.9761139015625094, 'learning_rate': 6.706520409257484e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.9391518135873854}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:40:30,888][0m Trial 37 finished with value: 0.1487487554550171 and parameters: {'observation_period_num': 250, 'train_rates': 0.9864352855049433, 'learning_rate': 6.071435338812878e-05, 'batch_size': 238, 'step_size': 15, 'gamma': 0.9551645605591573}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:40:59,374][0m Trial 38 finished with value: 0.17347005263713128 and parameters: {'observation_period_num': 43, 'train_rates': 0.9104176844473371, 'learning_rate': 7.934317151811566e-05, 'batch_size': 208, 'step_size': 14, 'gamma': 0.9766757631876659}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:41:25,759][0m Trial 39 finished with value: 0.33627885580062866 and parameters: {'observation_period_num': 167, 'train_rates': 0.9550798840244057, 'learning_rate': 4.145690873571937e-05, 'batch_size': 227, 'step_size': 13, 'gamma': 0.9428819974727782}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:41:50,220][0m Trial 40 finished with value: 0.1402542144060135 and parameters: {'observation_period_num': 179, 'train_rates': 0.987943361334822, 'learning_rate': 8.033565076209372e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.9618098975061874}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:42:16,595][0m Trial 41 finished with value: 0.1457600692635054 and parameters: {'observation_period_num': 7, 'train_rates': 0.907495669366379, 'learning_rate': 0.00010247917770138538, 'batch_size': 243, 'step_size': 11, 'gamma': 0.9319109770058025}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:43:07,524][0m Trial 42 finished with value: 0.21894739450592743 and parameters: {'observation_period_num': 22, 'train_rates': 0.9483560658558821, 'learning_rate': 0.0002398829148808186, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8878230554847237}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:43:39,882][0m Trial 43 finished with value: 0.06181715428829193 and parameters: {'observation_period_num': 5, 'train_rates': 0.9874659521196089, 'learning_rate': 5.2882713910863246e-05, 'batch_size': 195, 'step_size': 12, 'gamma': 0.9203985355635368}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:44:12,319][0m Trial 44 finished with value: 0.07150427997112274 and parameters: {'observation_period_num': 30, 'train_rates': 0.9745676278537715, 'learning_rate': 5.480405603727443e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.9391353151327192}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:44:41,584][0m Trial 45 finished with value: 0.1676394365342015 and parameters: {'observation_period_num': 5, 'train_rates': 0.915223733443812, 'learning_rate': 3.170164855284749e-05, 'batch_size': 223, 'step_size': 14, 'gamma': 0.8727649631506412}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:45:18,868][0m Trial 46 finished with value: 0.14054375062268282 and parameters: {'observation_period_num': 47, 'train_rates': 0.8844317233132934, 'learning_rate': 6.200374300576588e-05, 'batch_size': 152, 'step_size': 15, 'gamma': 0.9218320538679318}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:45:51,128][0m Trial 47 finished with value: 0.3070286214351654 and parameters: {'observation_period_num': 36, 'train_rates': 0.9691375712382119, 'learning_rate': 0.00020474849072078552, 'batch_size': 197, 'step_size': 13, 'gamma': 0.958255026900685}. Best is trial 30 with value: 0.05136840045452118.[0m
[32m[I 2025-02-01 17:46:21,758][0m Trial 48 finished with value: 0.02337191253900528 and parameters: {'observation_period_num': 17, 'train_rates': 0.9842228395349396, 'learning_rate': 0.0003899080586656311, 'batch_size': 211, 'step_size': 11, 'gamma': 0.9847343976297569}. Best is trial 48 with value: 0.02337191253900528.[0m
[32m[I 2025-02-01 17:46:50,208][0m Trial 49 finished with value: 0.36145833134651184 and parameters: {'observation_period_num': 201, 'train_rates': 0.9449423020049443, 'learning_rate': 0.00046536067853495934, 'batch_size': 214, 'step_size': 11, 'gamma': 0.9819364952510954}. Best is trial 48 with value: 0.02337191253900528.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-01 17:46:50,218][0m A new study created in memory with name: no-name-48f72f6e-3f94-4566-b250-add31eea02b8[0m
[32m[I 2025-02-01 17:47:19,510][0m Trial 0 finished with value: 0.18349666327815434 and parameters: {'observation_period_num': 21, 'train_rates': 0.6309470373319434, 'learning_rate': 4.9810699664427516e-05, 'batch_size': 157, 'step_size': 10, 'gamma': 0.851763482495449}. Best is trial 0 with value: 0.18349666327815434.[0m
[32m[I 2025-02-01 17:47:48,717][0m Trial 1 finished with value: 1.105688565763934 and parameters: {'observation_period_num': 128, 'train_rates': 0.8362127465285738, 'learning_rate': 1.2961165643879054e-06, 'batch_size': 184, 'step_size': 4, 'gamma': 0.9226882748699528}. Best is trial 0 with value: 0.18349666327815434.[0m
[32m[I 2025-02-01 17:48:35,112][0m Trial 2 finished with value: 0.14069370094997188 and parameters: {'observation_period_num': 154, 'train_rates': 0.7946432048571423, 'learning_rate': 0.0002631266568269417, 'batch_size': 110, 'step_size': 15, 'gamma': 0.794444399353088}. Best is trial 2 with value: 0.14069370094997188.[0m
[32m[I 2025-02-01 17:49:15,226][0m Trial 3 finished with value: 0.32920669353430426 and parameters: {'observation_period_num': 216, 'train_rates': 0.7837540905669168, 'learning_rate': 5.263834662125036e-05, 'batch_size': 125, 'step_size': 3, 'gamma': 0.7563941281541556}. Best is trial 2 with value: 0.14069370094997188.[0m
[32m[I 2025-02-01 17:49:41,771][0m Trial 4 finished with value: 0.31792996033574594 and parameters: {'observation_period_num': 167, 'train_rates': 0.8819635895729643, 'learning_rate': 1.5430456547766534e-05, 'batch_size': 205, 'step_size': 9, 'gamma': 0.8797830788766516}. Best is trial 2 with value: 0.14069370094997188.[0m
[32m[I 2025-02-01 17:50:07,895][0m Trial 5 finished with value: 0.2790505861161185 and parameters: {'observation_period_num': 22, 'train_rates': 0.6883951607308834, 'learning_rate': 1.0459573580309111e-05, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8114866941804326}. Best is trial 2 with value: 0.14069370094997188.[0m
[32m[I 2025-02-01 17:50:37,250][0m Trial 6 finished with value: 0.14910392917610787 and parameters: {'observation_period_num': 149, 'train_rates': 0.611072365927371, 'learning_rate': 0.00010530892753251972, 'batch_size': 153, 'step_size': 13, 'gamma': 0.8689014340556481}. Best is trial 2 with value: 0.14069370094997188.[0m
[32m[I 2025-02-01 17:51:12,669][0m Trial 7 finished with value: 0.13431167028931268 and parameters: {'observation_period_num': 113, 'train_rates': 0.6683970989224046, 'learning_rate': 0.00040997258804397163, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8288652719618566}. Best is trial 7 with value: 0.13431167028931268.[0m
[32m[I 2025-02-01 17:51:39,191][0m Trial 8 finished with value: 0.647721002603832 and parameters: {'observation_period_num': 201, 'train_rates': 0.8622141864138582, 'learning_rate': 2.661893759663837e-06, 'batch_size': 208, 'step_size': 6, 'gamma': 0.757230014059999}. Best is trial 7 with value: 0.13431167028931268.[0m
[32m[I 2025-02-01 17:53:05,525][0m Trial 9 finished with value: 0.09411757439374924 and parameters: {'observation_period_num': 179, 'train_rates': 0.9784085860071238, 'learning_rate': 0.00023115960000513977, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9648649720281331}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 17:56:01,664][0m Trial 10 finished with value: 0.1317472904920578 and parameters: {'observation_period_num': 244, 'train_rates': 0.9887629902287485, 'learning_rate': 0.0008216497778234377, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9641808800837907}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 17:58:41,938][0m Trial 11 finished with value: 0.09441725792069185 and parameters: {'observation_period_num': 247, 'train_rates': 0.9861775553485814, 'learning_rate': 0.0007854415516368064, 'batch_size': 34, 'step_size': 1, 'gamma': 0.9825217486310655}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:02:02,648][0m Trial 12 finished with value: 0.09873316869139671 and parameters: {'observation_period_num': 246, 'train_rates': 0.9815758161907951, 'learning_rate': 0.0009813689047107304, 'batch_size': 27, 'step_size': 1, 'gamma': 0.9861248453624236}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:03:18,973][0m Trial 13 finished with value: 0.18490272692658685 and parameters: {'observation_period_num': 78, 'train_rates': 0.9237575734401424, 'learning_rate': 0.00016897824196235458, 'batch_size': 75, 'step_size': 6, 'gamma': 0.9427694304330678}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:04:38,591][0m Trial 14 finished with value: 0.2735800036144527 and parameters: {'observation_period_num': 200, 'train_rates': 0.936227519468213, 'learning_rate': 0.0004155971229728866, 'batch_size': 69, 'step_size': 3, 'gamma': 0.9140777431989371}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:06:01,939][0m Trial 15 finished with value: 0.24143129687543977 and parameters: {'observation_period_num': 184, 'train_rates': 0.9306171688189622, 'learning_rate': 0.00011379885547956684, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9894512713178808}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:06:21,200][0m Trial 16 finished with value: 0.18045989646870866 and parameters: {'observation_period_num': 230, 'train_rates': 0.7426538446954344, 'learning_rate': 0.0005885481421995165, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9493799117233869}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:11:57,504][0m Trial 17 finished with value: 0.17340998325835574 and parameters: {'observation_period_num': 75, 'train_rates': 0.9048455166162919, 'learning_rate': 0.00024062684985877778, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8988878616018625}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:13:49,248][0m Trial 18 finished with value: 0.3577451595914698 and parameters: {'observation_period_num': 179, 'train_rates': 0.9616030434167028, 'learning_rate': 1.3927245066637184e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9639230795741068}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:14:39,549][0m Trial 19 finished with value: 0.1772667821372272 and parameters: {'observation_period_num': 207, 'train_rates': 0.836833683321271, 'learning_rate': 8.204650112203981e-05, 'batch_size': 104, 'step_size': 2, 'gamma': 0.9455658665754215}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:15:26,882][0m Trial 20 finished with value: 0.15484149045186488 and parameters: {'observation_period_num': 102, 'train_rates': 0.740585693377001, 'learning_rate': 3.4101093512100716e-05, 'batch_size': 101, 'step_size': 12, 'gamma': 0.9133860383122535}. Best is trial 9 with value: 0.09411757439374924.[0m
[32m[I 2025-02-01 18:17:43,062][0m Trial 21 finished with value: 0.08111744413250371 and parameters: {'observation_period_num': 252, 'train_rates': 0.9790259432084709, 'learning_rate': 0.0009680149414439754, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9880950520331362}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:19:40,437][0m Trial 22 finished with value: 0.43821092306946713 and parameters: {'observation_period_num': 252, 'train_rates': 0.9645918064645617, 'learning_rate': 0.00044839412279545584, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9733872737902516}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:21:28,001][0m Trial 23 finished with value: 0.32134701035275365 and parameters: {'observation_period_num': 227, 'train_rates': 0.9450326159200539, 'learning_rate': 0.0009124795805611551, 'batch_size': 50, 'step_size': 2, 'gamma': 0.931204843423521}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:22:30,211][0m Trial 24 finished with value: 0.21632052357598539 and parameters: {'observation_period_num': 227, 'train_rates': 0.9003025313771804, 'learning_rate': 0.00023453124522195465, 'batch_size': 85, 'step_size': 1, 'gamma': 0.970801006308339}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:24:50,575][0m Trial 25 finished with value: 0.1024242490530014 and parameters: {'observation_period_num': 188, 'train_rates': 0.9881432152198166, 'learning_rate': 0.00048512384994509056, 'batch_size': 40, 'step_size': 5, 'gamma': 0.9897463509242992}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:25:55,180][0m Trial 26 finished with value: 0.30236162691135104 and parameters: {'observation_period_num': 155, 'train_rates': 0.9547062291027023, 'learning_rate': 0.00016008251650990565, 'batch_size': 89, 'step_size': 2, 'gamma': 0.9538395674853073}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:27:21,369][0m Trial 27 finished with value: 0.21419617662678903 and parameters: {'observation_period_num': 220, 'train_rates': 0.8776653996857724, 'learning_rate': 0.00028692289291830554, 'batch_size': 59, 'step_size': 8, 'gamma': 0.896552617054271}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:28:24,890][0m Trial 28 finished with value: 0.23216373173601598 and parameters: {'observation_period_num': 242, 'train_rates': 0.9075768429711148, 'learning_rate': 0.0006656523503890358, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9745432582792127}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:33:40,625][0m Trial 29 finished with value: 0.15477451897011354 and parameters: {'observation_period_num': 136, 'train_rates': 0.8528324440973011, 'learning_rate': 7.009857646054059e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8445934221027793}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:34:21,213][0m Trial 30 finished with value: 0.27742335200309753 and parameters: {'observation_period_num': 30, 'train_rates': 0.9648398069366965, 'learning_rate': 0.0009982138341618645, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9340347587352817}. Best is trial 21 with value: 0.08111744413250371.[0m
[32m[I 2025-02-01 18:37:35,707][0m Trial 31 finished with value: 0.07899689805858276 and parameters: {'observation_period_num': 250, 'train_rates': 0.9874565052873338, 'learning_rate': 0.0006461200811464487, 'batch_size': 28, 'step_size': 1, 'gamma': 0.9807884850061753}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:39:54,087][0m Trial 32 finished with value: 0.10739261656999588 and parameters: {'observation_period_num': 236, 'train_rates': 0.9868499245116324, 'learning_rate': 0.00038055672957511895, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9587408888153185}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:42:55,045][0m Trial 33 finished with value: 0.2908747506941237 and parameters: {'observation_period_num': 211, 'train_rates': 0.9256375988440632, 'learning_rate': 0.0006174173308410126, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9771232528263062}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:44:26,519][0m Trial 34 finished with value: 0.3683419745592844 and parameters: {'observation_period_num': 195, 'train_rates': 0.9547257406272701, 'learning_rate': 0.0001747312902062605, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9796135768862292}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:49:59,727][0m Trial 35 finished with value: 0.11840003175394875 and parameters: {'observation_period_num': 251, 'train_rates': 0.9743857591945468, 'learning_rate': 0.0003132286178111557, 'batch_size': 16, 'step_size': 3, 'gamma': 0.933529182486811}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:52:22,841][0m Trial 36 finished with value: 0.20267464154540832 and parameters: {'observation_period_num': 166, 'train_rates': 0.8060030029913414, 'learning_rate': 6.000059682625916e-06, 'batch_size': 34, 'step_size': 5, 'gamma': 0.9573715289960743}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:53:10,387][0m Trial 37 finished with value: 0.18588854905407307 and parameters: {'observation_period_num': 214, 'train_rates': 0.8927296545588064, 'learning_rate': 0.0006499955942888721, 'batch_size': 113, 'step_size': 5, 'gamma': 0.7793901396944316}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:54:46,071][0m Trial 38 finished with value: 0.2679723538983036 and parameters: {'observation_period_num': 223, 'train_rates': 0.9223693229899654, 'learning_rate': 4.7735784133769685e-05, 'batch_size': 55, 'step_size': 2, 'gamma': 0.919279986347572}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:55:59,246][0m Trial 39 finished with value: 0.3450878789417105 and parameters: {'observation_period_num': 174, 'train_rates': 0.9452860054087336, 'learning_rate': 0.0006169872913747948, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9659677493073042}. Best is trial 31 with value: 0.07899689805858276.[0m
Early stopping at epoch 90
[32m[I 2025-02-01 18:56:23,573][0m Trial 40 finished with value: 0.35949019724678494 and parameters: {'observation_period_num': 235, 'train_rates': 0.6684296284404861, 'learning_rate': 0.0003282324175446617, 'batch_size': 169, 'step_size': 1, 'gamma': 0.8678191944508493}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 18:59:51,415][0m Trial 41 finished with value: 0.08245982741937041 and parameters: {'observation_period_num': 248, 'train_rates': 0.9764299850242631, 'learning_rate': 0.0009684864914781812, 'batch_size': 26, 'step_size': 1, 'gamma': 0.9884731601275194}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:03:10,652][0m Trial 42 finished with value: 0.1027154536867464 and parameters: {'observation_period_num': 252, 'train_rates': 0.9729652627192529, 'learning_rate': 0.0007670485522467452, 'batch_size': 27, 'step_size': 1, 'gamma': 0.9820256953572878}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:05:21,143][0m Trial 43 finished with value: 0.14474347233772278 and parameters: {'observation_period_num': 239, 'train_rates': 0.9889867323311166, 'learning_rate': 0.000485640382483702, 'batch_size': 42, 'step_size': 2, 'gamma': 0.9890022004977099}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:08:13,143][0m Trial 44 finished with value: 0.5908612636725108 and parameters: {'observation_period_num': 219, 'train_rates': 0.9452033929356964, 'learning_rate': 1.309487248249874e-06, 'batch_size': 31, 'step_size': 3, 'gamma': 0.9692330987605786}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:11:48,456][0m Trial 45 finished with value: 0.4655970454562542 and parameters: {'observation_period_num': 237, 'train_rates': 0.9686359584015605, 'learning_rate': 0.000993304410420775, 'batch_size': 25, 'step_size': 1, 'gamma': 0.9423051293064172}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:13:08,045][0m Trial 46 finished with value: 0.22020081751819315 and parameters: {'observation_period_num': 204, 'train_rates': 0.9159094508345008, 'learning_rate': 0.00022186814141269938, 'batch_size': 68, 'step_size': 13, 'gamma': 0.954381363203047}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:15:01,246][0m Trial 47 finished with value: 0.27061192099418907 and parameters: {'observation_period_num': 192, 'train_rates': 0.9350001421740873, 'learning_rate': 0.0005159165375905619, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9794087815800561}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:15:26,215][0m Trial 48 finished with value: 0.16459749351359076 and parameters: {'observation_period_num': 129, 'train_rates': 0.8729981978822926, 'learning_rate': 0.00013105689099142032, 'batch_size': 239, 'step_size': 4, 'gamma': 0.9642700142812888}. Best is trial 31 with value: 0.07899689805858276.[0m
[32m[I 2025-02-01 19:16:02,388][0m Trial 49 finished with value: 0.18893759078542952 and parameters: {'observation_period_num': 232, 'train_rates': 0.7657506096076275, 'learning_rate': 0.000782206593419661, 'batch_size': 135, 'step_size': 2, 'gamma': 0.8092583862502883}. Best is trial 31 with value: 0.07899689805858276.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-01 19:16:02,399][0m A new study created in memory with name: no-name-0f548af8-b0f9-498d-a459-be868c1502d2[0m
[32m[I 2025-02-01 19:16:24,841][0m Trial 0 finished with value: 0.731506062604313 and parameters: {'observation_period_num': 116, 'train_rates': 0.6684834434118216, 'learning_rate': 1.6694917090960192e-06, 'batch_size': 235, 'step_size': 15, 'gamma': 0.8272250092678333}. Best is trial 0 with value: 0.731506062604313.[0m
[32m[I 2025-02-01 19:16:50,325][0m Trial 1 finished with value: 0.14265459845317344 and parameters: {'observation_period_num': 5, 'train_rates': 0.7237605335597441, 'learning_rate': 9.459940063731855e-06, 'batch_size': 206, 'step_size': 8, 'gamma': 0.9596578799926616}. Best is trial 1 with value: 0.14265459845317344.[0m
[32m[I 2025-02-01 19:17:12,755][0m Trial 2 finished with value: 0.24798338162701047 and parameters: {'observation_period_num': 150, 'train_rates': 0.6033424931076304, 'learning_rate': 0.0007051158502551079, 'batch_size': 207, 'step_size': 3, 'gamma': 0.8890031284635828}. Best is trial 1 with value: 0.14265459845317344.[0m
[32m[I 2025-02-01 19:18:41,707][0m Trial 3 finished with value: 0.3279240641480165 and parameters: {'observation_period_num': 34, 'train_rates': 0.6704246089777677, 'learning_rate': 1.0646479647787916e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8895508164332042}. Best is trial 1 with value: 0.14265459845317344.[0m
[32m[I 2025-02-01 19:19:19,010][0m Trial 4 finished with value: 0.1568005290237211 and parameters: {'observation_period_num': 141, 'train_rates': 0.8508343002701958, 'learning_rate': 0.0006540433836324765, 'batch_size': 143, 'step_size': 12, 'gamma': 0.945075738232136}. Best is trial 1 with value: 0.14265459845317344.[0m
[32m[I 2025-02-01 19:19:59,635][0m Trial 5 finished with value: 0.21025693309148755 and parameters: {'observation_period_num': 170, 'train_rates': 0.6218094929466694, 'learning_rate': 8.456866412824926e-05, 'batch_size': 107, 'step_size': 12, 'gamma': 0.8821163797678709}. Best is trial 1 with value: 0.14265459845317344.[0m
[32m[I 2025-02-01 19:20:51,992][0m Trial 6 finished with value: 0.127304137451298 and parameters: {'observation_period_num': 147, 'train_rates': 0.7001227337942181, 'learning_rate': 0.0001359694314580391, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7817539822076993}. Best is trial 6 with value: 0.127304137451298.[0m
[32m[I 2025-02-01 19:21:16,492][0m Trial 7 finished with value: 0.34645901213992725 and parameters: {'observation_period_num': 207, 'train_rates': 0.9120647883714335, 'learning_rate': 1.5392151317622912e-05, 'batch_size': 235, 'step_size': 14, 'gamma': 0.9616957466388281}. Best is trial 6 with value: 0.127304137451298.[0m
[32m[I 2025-02-01 19:21:47,924][0m Trial 8 finished with value: 0.12081717715936033 and parameters: {'observation_period_num': 100, 'train_rates': 0.7904882883766586, 'learning_rate': 0.0007860027966220026, 'batch_size': 170, 'step_size': 4, 'gamma': 0.7804073878639335}. Best is trial 8 with value: 0.12081717715936033.[0m
[32m[I 2025-02-01 19:22:46,807][0m Trial 9 finished with value: 0.11844632969175467 and parameters: {'observation_period_num': 113, 'train_rates': 0.7167431004188956, 'learning_rate': 0.00047102323274595596, 'batch_size': 79, 'step_size': 4, 'gamma': 0.7618800605172151}. Best is trial 9 with value: 0.11844632969175467.[0m
Early stopping at epoch 49
[32m[I 2025-02-01 19:25:44,817][0m Trial 10 finished with value: 0.4117332972586155 and parameters: {'observation_period_num': 73, 'train_rates': 0.9722678017618533, 'learning_rate': 9.896750146156718e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.7541056185270282}. Best is trial 9 with value: 0.11844632969175467.[0m
[32m[I 2025-02-01 19:26:23,286][0m Trial 11 finished with value: 0.12097961211414468 and parameters: {'observation_period_num': 94, 'train_rates': 0.7721757768931002, 'learning_rate': 0.00034636611599370875, 'batch_size': 131, 'step_size': 5, 'gamma': 0.8106645672675715}. Best is trial 9 with value: 0.11844632969175467.[0m
[32m[I 2025-02-01 19:26:57,422][0m Trial 12 finished with value: 0.1158080181365248 and parameters: {'observation_period_num': 65, 'train_rates': 0.8105046862957667, 'learning_rate': 0.0002769993409160718, 'batch_size': 161, 'step_size': 6, 'gamma': 0.7504921173373057}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:28:12,666][0m Trial 13 finished with value: 0.11760089984711479 and parameters: {'observation_period_num': 60, 'train_rates': 0.8476546367831357, 'learning_rate': 0.00022653406874437217, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8381886775731566}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:28:46,177][0m Trial 14 finished with value: 0.1573069595465078 and parameters: {'observation_period_num': 48, 'train_rates': 0.8535619041314136, 'learning_rate': 3.488797629525162e-05, 'batch_size': 166, 'step_size': 8, 'gamma': 0.8385809720415465}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:30:45,930][0m Trial 15 finished with value: 0.13258490570873585 and parameters: {'observation_period_num': 61, 'train_rates': 0.8538687870988029, 'learning_rate': 0.00021817385433339756, 'batch_size': 44, 'step_size': 10, 'gamma': 0.9218901666544945}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:31:35,157][0m Trial 16 finished with value: 0.16805612328615818 and parameters: {'observation_period_num': 13, 'train_rates': 0.907243788796502, 'learning_rate': 4.339736119060376e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8481732302935954}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:32:08,762][0m Trial 17 finished with value: 0.11591125979546958 and parameters: {'observation_period_num': 75, 'train_rates': 0.8141697010244416, 'learning_rate': 0.00023930024745384947, 'batch_size': 167, 'step_size': 7, 'gamma': 0.8023279560575768}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:32:38,569][0m Trial 18 finished with value: 0.32092019393158383 and parameters: {'observation_period_num': 83, 'train_rates': 0.7648141870196593, 'learning_rate': 9.359744670360396e-06, 'batch_size': 179, 'step_size': 10, 'gamma': 0.795975681729994}. Best is trial 12 with value: 0.1158080181365248.[0m
Early stopping at epoch 56
[32m[I 2025-02-01 19:32:57,560][0m Trial 19 finished with value: 0.47077338085203113 and parameters: {'observation_period_num': 232, 'train_rates': 0.8164163507910867, 'learning_rate': 5.711656242621807e-05, 'batch_size': 153, 'step_size': 1, 'gamma': 0.8093801796449139}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:33:27,971][0m Trial 20 finished with value: 0.15204936217177997 and parameters: {'observation_period_num': 32, 'train_rates': 0.9060212351582912, 'learning_rate': 0.00021954991759544942, 'batch_size': 197, 'step_size': 9, 'gamma': 0.7513650090444272}. Best is trial 12 with value: 0.1158080181365248.[0m
[32m[I 2025-02-01 19:34:20,335][0m Trial 21 finished with value: 0.10895605437878159 and parameters: {'observation_period_num': 56, 'train_rates': 0.829003721316412, 'learning_rate': 0.00026025915323713987, 'batch_size': 103, 'step_size': 6, 'gamma': 0.8610126009702858}. Best is trial 21 with value: 0.10895605437878159.[0m
[32m[I 2025-02-01 19:35:10,890][0m Trial 22 finished with value: 0.09981136420320202 and parameters: {'observation_period_num': 45, 'train_rates': 0.8119580308405208, 'learning_rate': 0.00034539252951514216, 'batch_size': 105, 'step_size': 6, 'gamma': 0.8612140203854729}. Best is trial 22 with value: 0.09981136420320202.[0m
[32m[I 2025-02-01 19:36:02,132][0m Trial 23 finished with value: 0.08696785013627455 and parameters: {'observation_period_num': 32, 'train_rates': 0.7635663363910425, 'learning_rate': 0.00038828480891713724, 'batch_size': 100, 'step_size': 6, 'gamma': 0.8617218656747896}. Best is trial 23 with value: 0.08696785013627455.[0m
[32m[I 2025-02-01 19:36:53,959][0m Trial 24 finished with value: 0.08058887320022061 and parameters: {'observation_period_num': 31, 'train_rates': 0.7506091193389596, 'learning_rate': 0.0009912239113884496, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8621733541916572}. Best is trial 24 with value: 0.08058887320022061.[0m
[32m[I 2025-02-01 19:38:13,702][0m Trial 25 finished with value: 0.0846853699603396 and parameters: {'observation_period_num': 29, 'train_rates': 0.7390552628316069, 'learning_rate': 0.0008660727274988208, 'batch_size': 61, 'step_size': 3, 'gamma': 0.918023469889643}. Best is trial 24 with value: 0.08058887320022061.[0m
[32m[I 2025-02-01 19:39:32,986][0m Trial 26 finished with value: 0.08298556009404434 and parameters: {'observation_period_num': 27, 'train_rates': 0.7573514370165969, 'learning_rate': 0.0009889291211505117, 'batch_size': 62, 'step_size': 2, 'gamma': 0.9128395045802633}. Best is trial 24 with value: 0.08058887320022061.[0m
[32m[I 2025-02-01 19:41:00,500][0m Trial 27 finished with value: 0.07012562480910371 and parameters: {'observation_period_num': 12, 'train_rates': 0.7556222318002644, 'learning_rate': 0.0009501848803819557, 'batch_size': 57, 'step_size': 2, 'gamma': 0.9179995441415254}. Best is trial 27 with value: 0.07012562480910371.[0m
[32m[I 2025-02-01 19:43:43,584][0m Trial 28 finished with value: 0.07166842666144173 and parameters: {'observation_period_num': 15, 'train_rates': 0.6899088641466359, 'learning_rate': 0.0005786874869877585, 'batch_size': 28, 'step_size': 2, 'gamma': 0.9263135267500134}. Best is trial 27 with value: 0.07012562480910371.[0m
[32m[I 2025-02-01 19:46:05,097][0m Trial 29 finished with value: 0.11480684007207552 and parameters: {'observation_period_num': 9, 'train_rates': 0.6743891752490716, 'learning_rate': 4.052838599584682e-06, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9813805500832207}. Best is trial 27 with value: 0.07012562480910371.[0m
[32m[I 2025-02-01 19:50:06,811][0m Trial 30 finished with value: 0.0739077086654006 and parameters: {'observation_period_num': 18, 'train_rates': 0.6446523760977731, 'learning_rate': 0.0004837377537153127, 'batch_size': 18, 'step_size': 2, 'gamma': 0.9353221145356193}. Best is trial 27 with value: 0.07012562480910371.[0m
[32m[I 2025-02-01 19:54:18,670][0m Trial 31 finished with value: 0.07286109521038948 and parameters: {'observation_period_num': 15, 'train_rates': 0.635904487713278, 'learning_rate': 0.0005226847248856466, 'batch_size': 17, 'step_size': 2, 'gamma': 0.9389161472624201}. Best is trial 27 with value: 0.07012562480910371.[0m
[32m[I 2025-02-01 19:57:27,233][0m Trial 32 finished with value: 0.06426666915812139 and parameters: {'observation_period_num': 9, 'train_rates': 0.6355800205984784, 'learning_rate': 0.0005215479963514043, 'batch_size': 23, 'step_size': 2, 'gamma': 0.9414463114955876}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 19:59:31,883][0m Trial 33 finished with value: 0.07592046615399058 and parameters: {'observation_period_num': 18, 'train_rates': 0.6368168431883495, 'learning_rate': 0.00013411455190793644, 'batch_size': 35, 'step_size': 1, 'gamma': 0.986931260742623}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:02:28,843][0m Trial 34 finished with value: 0.07040762984720261 and parameters: {'observation_period_num': 5, 'train_rates': 0.6882509333808485, 'learning_rate': 0.0004822843225356338, 'batch_size': 26, 'step_size': 4, 'gamma': 0.9025602422643226}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:04:26,353][0m Trial 35 finished with value: 0.09819281755819174 and parameters: {'observation_period_num': 45, 'train_rates': 0.67607831893762, 'learning_rate': 0.0005759991234872647, 'batch_size': 38, 'step_size': 4, 'gamma': 0.8959804935704239}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:05:52,576][0m Trial 36 finished with value: 0.10772183385271464 and parameters: {'observation_period_num': 7, 'train_rates': 0.6923806423463481, 'learning_rate': 2.241202475112172e-05, 'batch_size': 55, 'step_size': 3, 'gamma': 0.9055750364821733}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:08:05,606][0m Trial 37 finished with value: 0.10962451844887376 and parameters: {'observation_period_num': 44, 'train_rates': 0.606797123487865, 'learning_rate': 0.00013644399274125336, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9604828383113319}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:09:11,179][0m Trial 38 finished with value: 0.13814146755644993 and parameters: {'observation_period_num': 131, 'train_rates': 0.7153212741691912, 'learning_rate': 0.0006157831068879063, 'batch_size': 72, 'step_size': 1, 'gamma': 0.9483851383397962}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:10:45,544][0m Trial 39 finished with value: 0.15618843104069433 and parameters: {'observation_period_num': 178, 'train_rates': 0.6890934551086764, 'learning_rate': 0.0006579490961381708, 'batch_size': 46, 'step_size': 5, 'gamma': 0.8775585390889922}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:13:24,760][0m Trial 40 finished with value: 0.1412157666417632 and parameters: {'observation_period_num': 5, 'train_rates': 0.6579310895498651, 'learning_rate': 3.4034412198942847e-06, 'batch_size': 28, 'step_size': 2, 'gamma': 0.9286188609490929}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:17:22,555][0m Trial 41 finished with value: 0.0750924406707452 and parameters: {'observation_period_num': 19, 'train_rates': 0.631682017109213, 'learning_rate': 0.0004581164221954878, 'batch_size': 18, 'step_size': 2, 'gamma': 0.9416790116153273}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:18:47,694][0m Trial 42 finished with value: 0.08727806023964586 and parameters: {'observation_period_num': 20, 'train_rates': 0.6046811864751022, 'learning_rate': 0.0006851298136990599, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9031235130893979}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:21:45,432][0m Trial 43 finished with value: 0.07332128242942541 and parameters: {'observation_period_num': 6, 'train_rates': 0.6540058409048773, 'learning_rate': 0.00035754486853040044, 'batch_size': 25, 'step_size': 3, 'gamma': 0.953447761604153}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:23:48,906][0m Trial 44 finished with value: 0.09738126736819294 and parameters: {'observation_period_num': 42, 'train_rates': 0.7276036881736977, 'learning_rate': 0.00016436559840622453, 'batch_size': 38, 'step_size': 1, 'gamma': 0.9352685670335089}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:28:20,582][0m Trial 45 finished with value: 0.15019997394228557 and parameters: {'observation_period_num': 162, 'train_rates': 0.7032909950216307, 'learning_rate': 0.0004910216049373643, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9692908883455573}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:29:12,458][0m Trial 46 finished with value: 0.07596261262920004 and parameters: {'observation_period_num': 20, 'train_rates': 0.6145997629678784, 'learning_rate': 0.0007281050450735295, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9693874475716053}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:30:26,177][0m Trial 47 finished with value: 0.1552231787274982 and parameters: {'observation_period_num': 106, 'train_rates': 0.6618309159760621, 'learning_rate': 7.428643364142297e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8959924408318599}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:32:01,967][0m Trial 48 finished with value: 0.10060246115889997 and parameters: {'observation_period_num': 38, 'train_rates': 0.6280975461632285, 'learning_rate': 0.00032697014978488626, 'batch_size': 45, 'step_size': 1, 'gamma': 0.9269999775584025}. Best is trial 32 with value: 0.06426666915812139.[0m
[32m[I 2025-02-01 20:32:23,312][0m Trial 49 finished with value: 0.11161629566850588 and parameters: {'observation_period_num': 54, 'train_rates': 0.6889761631036462, 'learning_rate': 0.0001706897141328253, 'batch_size': 238, 'step_size': 4, 'gamma': 0.8843608698115775}. Best is trial 32 with value: 0.06426666915812139.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-01 20:32:23,333][0m A new study created in memory with name: no-name-eac803a0-b620-4067-b89c-346540bceb77[0m
[32m[I 2025-02-01 20:32:46,587][0m Trial 0 finished with value: 0.5123435098055171 and parameters: {'observation_period_num': 167, 'train_rates': 0.7557520945332925, 'learning_rate': 4.977007491624571e-06, 'batch_size': 228, 'step_size': 11, 'gamma': 0.8101606110048306}. Best is trial 0 with value: 0.5123435098055171.[0m
[32m[I 2025-02-01 20:35:25,291][0m Trial 1 finished with value: 0.12675901407083845 and parameters: {'observation_period_num': 90, 'train_rates': 0.7287833556405584, 'learning_rate': 7.800286273971717e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9764451094856064}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:36:10,760][0m Trial 2 finished with value: 0.41432092235471213 and parameters: {'observation_period_num': 220, 'train_rates': 0.8999517694973405, 'learning_rate': 9.955429652041545e-06, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9659019783942557}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:36:54,186][0m Trial 3 finished with value: 0.12822226038045972 and parameters: {'observation_period_num': 147, 'train_rates': 0.7335318750767789, 'learning_rate': 0.0004912232329878782, 'batch_size': 112, 'step_size': 12, 'gamma': 0.7663124808756854}. Best is trial 1 with value: 0.12675901407083845.[0m
Early stopping at epoch 62
[32m[I 2025-02-01 20:37:45,918][0m Trial 4 finished with value: 0.21823763103861557 and parameters: {'observation_period_num': 168, 'train_rates': 0.8296837106580953, 'learning_rate': 0.00016266985328507516, 'batch_size': 62, 'step_size': 1, 'gamma': 0.8162739581959972}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:38:45,419][0m Trial 5 finished with value: 1.0812159571351048 and parameters: {'observation_period_num': 200, 'train_rates': 0.6970159042373413, 'learning_rate': 2.142134792789346e-06, 'batch_size': 74, 'step_size': 2, 'gamma': 0.8496464451541157}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:39:12,125][0m Trial 6 finished with value: 0.3646669387817383 and parameters: {'observation_period_num': 231, 'train_rates': 0.9593079629006384, 'learning_rate': 0.0003575236867683075, 'batch_size': 222, 'step_size': 10, 'gamma': 0.9635628475563727}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:39:38,385][0m Trial 7 finished with value: 0.738314321407905 and parameters: {'observation_period_num': 75, 'train_rates': 0.7158480416378357, 'learning_rate': 1.9479651782920937e-06, 'batch_size': 189, 'step_size': 4, 'gamma': 0.9159338090935263}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:40:22,614][0m Trial 8 finished with value: 0.3888019621372223 and parameters: {'observation_period_num': 233, 'train_rates': 0.9567158647972496, 'learning_rate': 3.548264127208006e-05, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9788109126794864}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:40:42,856][0m Trial 9 finished with value: 0.23012542614982304 and parameters: {'observation_period_num': 92, 'train_rates': 0.6158813113369317, 'learning_rate': 5.484999552376029e-05, 'batch_size': 241, 'step_size': 12, 'gamma': 0.7655229240596514}. Best is trial 1 with value: 0.12675901407083845.[0m
[32m[I 2025-02-01 20:44:13,401][0m Trial 10 finished with value: 0.06184046451101829 and parameters: {'observation_period_num': 8, 'train_rates': 0.606289375412508, 'learning_rate': 0.0001041801855429689, 'batch_size': 20, 'step_size': 7, 'gamma': 0.9082389833386236}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:47:55,948][0m Trial 11 finished with value: 0.07365129064693195 and parameters: {'observation_period_num': 5, 'train_rates': 0.6148901372842596, 'learning_rate': 0.00016216882848044928, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9024210688484379}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:51:46,705][0m Trial 12 finished with value: 0.09501781035056543 and parameters: {'observation_period_num': 10, 'train_rates': 0.6002446549458678, 'learning_rate': 0.0009725054854671522, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9066905538850996}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:52:56,323][0m Trial 13 finished with value: 0.07424347046762705 and parameters: {'observation_period_num': 19, 'train_rates': 0.6596738826869044, 'learning_rate': 0.00017068020447184436, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8993745357552997}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:53:24,696][0m Trial 14 finished with value: 0.18108874446940873 and parameters: {'observation_period_num': 46, 'train_rates': 0.6541025422775978, 'learning_rate': 1.4653038377129155e-05, 'batch_size': 171, 'step_size': 15, 'gamma': 0.8707412316266144}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:55:17,910][0m Trial 15 finished with value: 0.10838073554362893 and parameters: {'observation_period_num': 46, 'train_rates': 0.8100596898019375, 'learning_rate': 9.745339537133421e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.939455756893271}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:56:07,198][0m Trial 16 finished with value: 0.15396333185594474 and parameters: {'observation_period_num': 50, 'train_rates': 0.6542165879179912, 'learning_rate': 1.9697457760026524e-05, 'batch_size': 94, 'step_size': 8, 'gamma': 0.8636852224506505}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 20:56:42,590][0m Trial 17 finished with value: 0.11472026353820841 and parameters: {'observation_period_num': 5, 'train_rates': 0.8728041888109315, 'learning_rate': 0.0002760426860321795, 'batch_size': 168, 'step_size': 8, 'gamma': 0.9345458120328667}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:01:38,322][0m Trial 18 finished with value: 0.14085768362504328 and parameters: {'observation_period_num': 115, 'train_rates': 0.7738490273825244, 'learning_rate': 0.000877090373429808, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8903793590604632}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:03:17,874][0m Trial 19 finished with value: 0.10409928967544807 and parameters: {'observation_period_num': 34, 'train_rates': 0.6779873472919025, 'learning_rate': 3.8041137751442086e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.836475263255276}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:04:03,132][0m Trial 20 finished with value: 0.09584307712602592 and parameters: {'observation_period_num': 61, 'train_rates': 0.6295754529520491, 'learning_rate': 0.00014147397213518375, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9319749519784855}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:05:10,312][0m Trial 21 finished with value: 0.07439583200048148 and parameters: {'observation_period_num': 22, 'train_rates': 0.6499082298688647, 'learning_rate': 0.00017299014300110294, 'batch_size': 67, 'step_size': 7, 'gamma': 0.8942460507670876}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:06:38,116][0m Trial 22 finished with value: 0.1304747407669074 and parameters: {'observation_period_num': 28, 'train_rates': 0.601178576615246, 'learning_rate': 0.0002814611981043169, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8769471054607851}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:08:49,021][0m Trial 23 finished with value: 0.06880282292044476 and parameters: {'observation_period_num': 6, 'train_rates': 0.6783032438694888, 'learning_rate': 7.527982565847536e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.911824756153664}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:10:57,261][0m Trial 24 finished with value: 0.13425928070095838 and parameters: {'observation_period_num': 70, 'train_rates': 0.6890399941968464, 'learning_rate': 7.021830671317923e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9507893265902129}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:11:48,744][0m Trial 25 finished with value: 0.14616006210267157 and parameters: {'observation_period_num': 5, 'train_rates': 0.633874875372516, 'learning_rate': 2.1089316140916347e-05, 'batch_size': 89, 'step_size': 3, 'gamma': 0.9085940499275323}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:16:21,588][0m Trial 26 finished with value: 0.153884243488841 and parameters: {'observation_period_num': 110, 'train_rates': 0.6834451160294853, 'learning_rate': 5.0216320776474896e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9218689980247362}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:18:26,738][0m Trial 27 finished with value: 0.09847192065178478 and parameters: {'observation_period_num': 36, 'train_rates': 0.6248719867437712, 'learning_rate': 0.00010796439125359575, 'batch_size': 34, 'step_size': 5, 'gamma': 0.8841935802495157}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:19:49,926][0m Trial 28 finished with value: 0.1348581233372291 and parameters: {'observation_period_num': 89, 'train_rates': 0.7574325098992336, 'learning_rate': 0.00036909306476079875, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9521860572433626}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:20:44,389][0m Trial 29 finished with value: 0.12407078755337118 and parameters: {'observation_period_num': 141, 'train_rates': 0.7130461958968775, 'learning_rate': 0.000557989269893143, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8536204381537776}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:22:51,211][0m Trial 30 finished with value: 0.1566339171123366 and parameters: {'observation_period_num': 60, 'train_rates': 0.6735524597046147, 'learning_rate': 6.268126158719191e-06, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8329659056597618}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:24:22,598][0m Trial 31 finished with value: 0.07692513873211942 and parameters: {'observation_period_num': 21, 'train_rates': 0.6583212739894994, 'learning_rate': 0.00020792467520625333, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9041746424074875}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:27:01,726][0m Trial 32 finished with value: 0.07678183767684671 and parameters: {'observation_period_num': 20, 'train_rates': 0.6341526686781755, 'learning_rate': 9.633736431443968e-05, 'batch_size': 27, 'step_size': 8, 'gamma': 0.895224920647818}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:28:06,031][0m Trial 33 finished with value: 0.09097096219776803 and parameters: {'observation_period_num': 37, 'train_rates': 0.7403280759939218, 'learning_rate': 0.0001284027780079589, 'batch_size': 76, 'step_size': 10, 'gamma': 0.9223766201968022}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:30:24,108][0m Trial 34 finished with value: 0.06925229970102491 and parameters: {'observation_period_num': 15, 'train_rates': 0.6153906555677184, 'learning_rate': 6.9944791987908e-05, 'batch_size': 31, 'step_size': 9, 'gamma': 0.8760715602932763}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:33:15,701][0m Trial 35 finished with value: 0.06321125230038757 and parameters: {'observation_period_num': 5, 'train_rates': 0.6241018543478298, 'learning_rate': 6.416420970708867e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.8025252411941766}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:33:54,293][0m Trial 36 finished with value: 0.2280022929544034 and parameters: {'observation_period_num': 176, 'train_rates': 0.6028988352771651, 'learning_rate': 5.499926208551903e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.7889372587774517}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:35:17,413][0m Trial 37 finished with value: 0.10946419339540393 and parameters: {'observation_period_num': 53, 'train_rates': 0.7039341142917735, 'learning_rate': 2.554450326008921e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.7940305060331885}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:38:09,588][0m Trial 38 finished with value: 0.07733936607837677 and parameters: {'observation_period_num': 35, 'train_rates': 0.9896662999612224, 'learning_rate': 1.015939614219458e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8079167774405996}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:38:43,245][0m Trial 39 finished with value: 0.1707218934329595 and parameters: {'observation_period_num': 77, 'train_rates': 0.6381463100920614, 'learning_rate': 7.205988253318649e-05, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8392919629244956}. Best is trial 10 with value: 0.06184046451101829.[0m
Early stopping at epoch 94
[32m[I 2025-02-01 21:41:25,950][0m Trial 40 finished with value: 0.3179001377144856 and parameters: {'observation_period_num': 251, 'train_rates': 0.8762790754807286, 'learning_rate': 4.205180485428274e-05, 'batch_size': 29, 'step_size': 2, 'gamma': 0.7517147990360695}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:44:09,022][0m Trial 41 finished with value: 0.06245761028795352 and parameters: {'observation_period_num': 13, 'train_rates': 0.6166535618183558, 'learning_rate': 8.12649865719865e-05, 'batch_size': 26, 'step_size': 9, 'gamma': 0.8812772096739231}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:45:49,303][0m Trial 42 finished with value: 0.13479877194369816 and parameters: {'observation_period_num': 15, 'train_rates': 0.6179544437025277, 'learning_rate': 7.327625261636192e-05, 'batch_size': 43, 'step_size': 1, 'gamma': 0.860213926256122}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:48:34,729][0m Trial 43 finished with value: 0.10936357885538066 and parameters: {'observation_period_num': 26, 'train_rates': 0.6672317199582019, 'learning_rate': 2.938177084214427e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8796185905443168}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:49:35,581][0m Trial 44 finished with value: 0.07891117604846638 and parameters: {'observation_period_num': 12, 'train_rates': 0.6418657084829048, 'learning_rate': 5.563041206495528e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8252579182091336}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:49:57,949][0m Trial 45 finished with value: 0.32739633491276193 and parameters: {'observation_period_num': 41, 'train_rates': 0.6188324490215363, 'learning_rate': 0.00010329741821195652, 'batch_size': 209, 'step_size': 9, 'gamma': 0.8536115314141546}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:50:16,084][0m Trial 46 finished with value: 1.1490128545957237 and parameters: {'observation_period_num': 14, 'train_rates': 0.6033707654344612, 'learning_rate': 3.252878647567874e-06, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8728363995014173}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:51:32,710][0m Trial 47 finished with value: 0.15373830632968738 and parameters: {'observation_period_num': 190, 'train_rates': 0.6926254039219252, 'learning_rate': 0.00021078784953457885, 'batch_size': 58, 'step_size': 10, 'gamma': 0.985625166353876}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:54:42,185][0m Trial 48 finished with value: 0.09060458862807733 and parameters: {'observation_period_num': 29, 'train_rates': 0.7283850841755719, 'learning_rate': 7.880513595757169e-05, 'batch_size': 25, 'step_size': 8, 'gamma': 0.9176000637517124}. Best is trial 10 with value: 0.06184046451101829.[0m
[32m[I 2025-02-01 21:56:30,772][0m Trial 49 finished with value: 0.2288206240877538 and parameters: {'observation_period_num': 60, 'train_rates': 0.6458106398087682, 'learning_rate': 1.354263193555068e-05, 'batch_size': 40, 'step_size': 3, 'gamma': 0.8863114613016317}. Best is trial 10 with value: 0.06184046451101829.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.7061854111320305, 'learning_rate': 0.0003013278996634321, 'batch_size': 237, 'step_size': 7, 'gamma': 0.8444228951539271}
Epoch 1/300, trend Loss: 0.6075 | 0.6055
Epoch 2/300, trend Loss: 0.2410 | 0.2227
Epoch 3/300, trend Loss: 0.3005 | 0.2379
Epoch 4/300, trend Loss: 0.2734 | 0.2381
Epoch 5/300, trend Loss: 0.1915 | 0.1787
Epoch 6/300, trend Loss: 0.1542 | 0.1692
Epoch 7/300, trend Loss: 0.1483 | 0.1461
Epoch 8/300, trend Loss: 0.1428 | 0.1495
Epoch 9/300, trend Loss: 0.1438 | 0.1357
Epoch 10/300, trend Loss: 0.1396 | 0.1446
Epoch 11/300, trend Loss: 0.1403 | 0.1286
Epoch 12/300, trend Loss: 0.1359 | 0.1420
Epoch 13/300, trend Loss: 0.1346 | 0.1233
Epoch 14/300, trend Loss: 0.1308 | 0.1316
Epoch 15/300, trend Loss: 0.1290 | 0.1239
Epoch 16/300, trend Loss: 0.1267 | 0.1220
Epoch 17/300, trend Loss: 0.1247 | 0.1259
Epoch 18/300, trend Loss: 0.1237 | 0.1164
Epoch 19/300, trend Loss: 0.1217 | 0.1261
Epoch 20/300, trend Loss: 0.1213 | 0.1138
Epoch 21/300, trend Loss: 0.1196 | 0.1238
Epoch 22/300, trend Loss: 0.1191 | 0.1131
Epoch 23/300, trend Loss: 0.1178 | 0.1194
Epoch 24/300, trend Loss: 0.1172 | 0.1142
Epoch 25/300, trend Loss: 0.1163 | 0.1159
Epoch 26/300, trend Loss: 0.1156 | 0.1140
Epoch 27/300, trend Loss: 0.1150 | 0.1135
Epoch 28/300, trend Loss: 0.1144 | 0.1135
Epoch 29/300, trend Loss: 0.1139 | 0.1120
Epoch 30/300, trend Loss: 0.1135 | 0.1121
Epoch 31/300, trend Loss: 0.1131 | 0.1113
Epoch 32/300, trend Loss: 0.1127 | 0.1108
Epoch 33/300, trend Loss: 0.1123 | 0.1103
Epoch 34/300, trend Loss: 0.1119 | 0.1097
Epoch 35/300, trend Loss: 0.1116 | 0.1095
Epoch 36/300, trend Loss: 0.1113 | 0.1089
Epoch 37/300, trend Loss: 0.1111 | 0.1085
Epoch 38/300, trend Loss: 0.1108 | 0.1082
Epoch 39/300, trend Loss: 0.1106 | 0.1077
Epoch 40/300, trend Loss: 0.1104 | 0.1074
Epoch 41/300, trend Loss: 0.1102 | 0.1070
Epoch 42/300, trend Loss: 0.1100 | 0.1067
Epoch 43/300, trend Loss: 0.1098 | 0.1064
Epoch 44/300, trend Loss: 0.1096 | 0.1061
Epoch 45/300, trend Loss: 0.1094 | 0.1058
Epoch 46/300, trend Loss: 0.1093 | 0.1056
Epoch 47/300, trend Loss: 0.1091 | 0.1053
Epoch 48/300, trend Loss: 0.1090 | 0.1050
Epoch 49/300, trend Loss: 0.1089 | 0.1048
Epoch 50/300, trend Loss: 0.1087 | 0.1046
Epoch 51/300, trend Loss: 0.1086 | 0.1044
Epoch 52/300, trend Loss: 0.1085 | 0.1042
Epoch 53/300, trend Loss: 0.1084 | 0.1040
Epoch 54/300, trend Loss: 0.1083 | 0.1038
Epoch 55/300, trend Loss: 0.1082 | 0.1036
Epoch 56/300, trend Loss: 0.1081 | 0.1035
Epoch 57/300, trend Loss: 0.1080 | 0.1033
Epoch 58/300, trend Loss: 0.1079 | 0.1032
Epoch 59/300, trend Loss: 0.1078 | 0.1030
Epoch 60/300, trend Loss: 0.1078 | 0.1029
Epoch 61/300, trend Loss: 0.1077 | 0.1028
Epoch 62/300, trend Loss: 0.1076 | 0.1026
Epoch 63/300, trend Loss: 0.1076 | 0.1025
Epoch 64/300, trend Loss: 0.1075 | 0.1024
Epoch 65/300, trend Loss: 0.1074 | 0.1023
Epoch 66/300, trend Loss: 0.1074 | 0.1022
Epoch 67/300, trend Loss: 0.1073 | 0.1021
Epoch 68/300, trend Loss: 0.1073 | 0.1020
Epoch 69/300, trend Loss: 0.1072 | 0.1019
Epoch 70/300, trend Loss: 0.1072 | 0.1018
Epoch 71/300, trend Loss: 0.1071 | 0.1017
Epoch 72/300, trend Loss: 0.1071 | 0.1017
Epoch 73/300, trend Loss: 0.1070 | 0.1016
Epoch 74/300, trend Loss: 0.1070 | 0.1015
Epoch 75/300, trend Loss: 0.1069 | 0.1014
Epoch 76/300, trend Loss: 0.1069 | 0.1014
Epoch 77/300, trend Loss: 0.1069 | 0.1013
Epoch 78/300, trend Loss: 0.1068 | 0.1012
Epoch 79/300, trend Loss: 0.1068 | 0.1012
Epoch 80/300, trend Loss: 0.1068 | 0.1011
Epoch 81/300, trend Loss: 0.1067 | 0.1011
Epoch 82/300, trend Loss: 0.1067 | 0.1010
Epoch 83/300, trend Loss: 0.1067 | 0.1010
Epoch 84/300, trend Loss: 0.1067 | 0.1009
Epoch 85/300, trend Loss: 0.1066 | 0.1009
Epoch 86/300, trend Loss: 0.1066 | 0.1009
Epoch 87/300, trend Loss: 0.1066 | 0.1008
Epoch 88/300, trend Loss: 0.1066 | 0.1008
Epoch 89/300, trend Loss: 0.1065 | 0.1007
Epoch 90/300, trend Loss: 0.1065 | 0.1007
Epoch 91/300, trend Loss: 0.1065 | 0.1007
Epoch 92/300, trend Loss: 0.1065 | 0.1006
Epoch 93/300, trend Loss: 0.1065 | 0.1006
Epoch 94/300, trend Loss: 0.1065 | 0.1006
Epoch 95/300, trend Loss: 0.1064 | 0.1005
Epoch 96/300, trend Loss: 0.1064 | 0.1005
Epoch 97/300, trend Loss: 0.1064 | 0.1005
Epoch 98/300, trend Loss: 0.1064 | 0.1005
Epoch 99/300, trend Loss: 0.1064 | 0.1004
Epoch 100/300, trend Loss: 0.1064 | 0.1004
Epoch 101/300, trend Loss: 0.1064 | 0.1004
Epoch 102/300, trend Loss: 0.1063 | 0.1004
Epoch 103/300, trend Loss: 0.1063 | 0.1004
Epoch 104/300, trend Loss: 0.1063 | 0.1003
Epoch 105/300, trend Loss: 0.1063 | 0.1003
Epoch 106/300, trend Loss: 0.1063 | 0.1003
Epoch 107/300, trend Loss: 0.1063 | 0.1003
Epoch 108/300, trend Loss: 0.1063 | 0.1003
Epoch 109/300, trend Loss: 0.1063 | 0.1003
Epoch 110/300, trend Loss: 0.1063 | 0.1002
Epoch 111/300, trend Loss: 0.1063 | 0.1002
Epoch 112/300, trend Loss: 0.1063 | 0.1002
Epoch 113/300, trend Loss: 0.1062 | 0.1002
Epoch 114/300, trend Loss: 0.1062 | 0.1002
Epoch 115/300, trend Loss: 0.1062 | 0.1002
Epoch 116/300, trend Loss: 0.1062 | 0.1002
Epoch 117/300, trend Loss: 0.1062 | 0.1002
Epoch 118/300, trend Loss: 0.1062 | 0.1001
Epoch 119/300, trend Loss: 0.1062 | 0.1001
Epoch 120/300, trend Loss: 0.1062 | 0.1001
Epoch 121/300, trend Loss: 0.1062 | 0.1001
Epoch 122/300, trend Loss: 0.1062 | 0.1001
Epoch 123/300, trend Loss: 0.1062 | 0.1001
Epoch 124/300, trend Loss: 0.1062 | 0.1001
Epoch 125/300, trend Loss: 0.1062 | 0.1001
Epoch 126/300, trend Loss: 0.1062 | 0.1001
Epoch 127/300, trend Loss: 0.1062 | 0.1001
Epoch 128/300, trend Loss: 0.1062 | 0.1001
Epoch 129/300, trend Loss: 0.1062 | 0.1001
Epoch 130/300, trend Loss: 0.1062 | 0.1001
Epoch 131/300, trend Loss: 0.1062 | 0.1000
Epoch 132/300, trend Loss: 0.1062 | 0.1000
Epoch 133/300, trend Loss: 0.1061 | 0.1000
Epoch 134/300, trend Loss: 0.1061 | 0.1000
Epoch 135/300, trend Loss: 0.1061 | 0.1000
Epoch 136/300, trend Loss: 0.1061 | 0.1000
Epoch 137/300, trend Loss: 0.1061 | 0.1000
Epoch 138/300, trend Loss: 0.1061 | 0.1000
Epoch 139/300, trend Loss: 0.1061 | 0.1000
Epoch 140/300, trend Loss: 0.1061 | 0.1000
Epoch 141/300, trend Loss: 0.1061 | 0.1000
Epoch 142/300, trend Loss: 0.1061 | 0.1000
Epoch 143/300, trend Loss: 0.1061 | 0.1000
Epoch 144/300, trend Loss: 0.1061 | 0.1000
Epoch 145/300, trend Loss: 0.1061 | 0.1000
Epoch 146/300, trend Loss: 0.1061 | 0.1000
Epoch 147/300, trend Loss: 0.1061 | 0.1000
Epoch 148/300, trend Loss: 0.1061 | 0.1000
Epoch 149/300, trend Loss: 0.1061 | 0.1000
Epoch 150/300, trend Loss: 0.1061 | 0.1000
Epoch 151/300, trend Loss: 0.1061 | 0.1000
Epoch 152/300, trend Loss: 0.1061 | 0.1000
Epoch 153/300, trend Loss: 0.1061 | 0.1000
Epoch 154/300, trend Loss: 0.1061 | 0.1000
Epoch 155/300, trend Loss: 0.1061 | 0.1000
Epoch 156/300, trend Loss: 0.1061 | 0.1000
Epoch 157/300, trend Loss: 0.1061 | 0.1000
Epoch 158/300, trend Loss: 0.1061 | 0.1000
Epoch 159/300, trend Loss: 0.1061 | 0.1000
Epoch 160/300, trend Loss: 0.1061 | 0.1000
Epoch 161/300, trend Loss: 0.1061 | 0.1000
Epoch 162/300, trend Loss: 0.1061 | 0.1000
Epoch 163/300, trend Loss: 0.1061 | 0.1000
Epoch 164/300, trend Loss: 0.1061 | 0.1000
Epoch 165/300, trend Loss: 0.1061 | 0.1000
Epoch 166/300, trend Loss: 0.1061 | 0.0999
Epoch 167/300, trend Loss: 0.1061 | 0.0999
Epoch 168/300, trend Loss: 0.1061 | 0.0999
Epoch 169/300, trend Loss: 0.1061 | 0.0999
Epoch 170/300, trend Loss: 0.1061 | 0.0999
Epoch 171/300, trend Loss: 0.1061 | 0.0999
Epoch 172/300, trend Loss: 0.1061 | 0.0999
Epoch 173/300, trend Loss: 0.1061 | 0.0999
Epoch 174/300, trend Loss: 0.1061 | 0.0999
Epoch 175/300, trend Loss: 0.1061 | 0.0999
Epoch 176/300, trend Loss: 0.1061 | 0.0999
Epoch 177/300, trend Loss: 0.1061 | 0.0999
Epoch 178/300, trend Loss: 0.1061 | 0.0999
Epoch 179/300, trend Loss: 0.1061 | 0.0999
Epoch 180/300, trend Loss: 0.1061 | 0.0999
Epoch 181/300, trend Loss: 0.1061 | 0.0999
Epoch 182/300, trend Loss: 0.1061 | 0.0999
Epoch 183/300, trend Loss: 0.1061 | 0.0999
Epoch 184/300, trend Loss: 0.1061 | 0.0999
Epoch 185/300, trend Loss: 0.1061 | 0.0999
Epoch 186/300, trend Loss: 0.1061 | 0.0999
Epoch 187/300, trend Loss: 0.1061 | 0.0999
Epoch 188/300, trend Loss: 0.1061 | 0.0999
Epoch 189/300, trend Loss: 0.1061 | 0.0999
Epoch 190/300, trend Loss: 0.1061 | 0.0999
Epoch 191/300, trend Loss: 0.1061 | 0.0999
Epoch 192/300, trend Loss: 0.1061 | 0.0999
Epoch 193/300, trend Loss: 0.1061 | 0.0999
Epoch 194/300, trend Loss: 0.1061 | 0.0999
Epoch 195/300, trend Loss: 0.1061 | 0.0999
Epoch 196/300, trend Loss: 0.1061 | 0.0999
Epoch 197/300, trend Loss: 0.1061 | 0.0999
Epoch 198/300, trend Loss: 0.1061 | 0.0999
Epoch 199/300, trend Loss: 0.1061 | 0.0999
Epoch 200/300, trend Loss: 0.1061 | 0.0999
Epoch 201/300, trend Loss: 0.1061 | 0.0999
Epoch 202/300, trend Loss: 0.1061 | 0.0999
Epoch 203/300, trend Loss: 0.1061 | 0.0999
Epoch 204/300, trend Loss: 0.1061 | 0.0999
Epoch 205/300, trend Loss: 0.1061 | 0.0999
Epoch 206/300, trend Loss: 0.1061 | 0.0999
Epoch 207/300, trend Loss: 0.1061 | 0.0999
Epoch 208/300, trend Loss: 0.1061 | 0.0999
Epoch 209/300, trend Loss: 0.1061 | 0.0999
Epoch 210/300, trend Loss: 0.1061 | 0.0999
Epoch 211/300, trend Loss: 0.1061 | 0.0999
Epoch 212/300, trend Loss: 0.1061 | 0.0999
Epoch 213/300, trend Loss: 0.1061 | 0.0999
Epoch 214/300, trend Loss: 0.1061 | 0.0999
Epoch 215/300, trend Loss: 0.1061 | 0.0999
Epoch 216/300, trend Loss: 0.1061 | 0.0999
Epoch 217/300, trend Loss: 0.1061 | 0.0999
Epoch 218/300, trend Loss: 0.1061 | 0.0999
Epoch 219/300, trend Loss: 0.1061 | 0.0999
Epoch 220/300, trend Loss: 0.1061 | 0.0999
Epoch 221/300, trend Loss: 0.1061 | 0.0999
Epoch 222/300, trend Loss: 0.1061 | 0.0999
Epoch 223/300, trend Loss: 0.1061 | 0.0999
Epoch 224/300, trend Loss: 0.1061 | 0.0999
Epoch 225/300, trend Loss: 0.1061 | 0.0999
Epoch 226/300, trend Loss: 0.1061 | 0.0999
Epoch 227/300, trend Loss: 0.1061 | 0.0999
Epoch 228/300, trend Loss: 0.1061 | 0.0999
Epoch 229/300, trend Loss: 0.1061 | 0.0999
Epoch 230/300, trend Loss: 0.1061 | 0.0999
Epoch 231/300, trend Loss: 0.1061 | 0.0999
Epoch 232/300, trend Loss: 0.1061 | 0.0999
Epoch 233/300, trend Loss: 0.1061 | 0.0999
Epoch 234/300, trend Loss: 0.1061 | 0.0999
Epoch 235/300, trend Loss: 0.1061 | 0.0999
Epoch 236/300, trend Loss: 0.1061 | 0.0999
Epoch 237/300, trend Loss: 0.1061 | 0.0999
Epoch 238/300, trend Loss: 0.1061 | 0.0999
Epoch 239/300, trend Loss: 0.1061 | 0.0999
Epoch 240/300, trend Loss: 0.1061 | 0.0999
Epoch 241/300, trend Loss: 0.1061 | 0.0999
Epoch 242/300, trend Loss: 0.1061 | 0.0999
Epoch 243/300, trend Loss: 0.1061 | 0.0999
Epoch 244/300, trend Loss: 0.1061 | 0.0999
Epoch 245/300, trend Loss: 0.1061 | 0.0999
Epoch 246/300, trend Loss: 0.1061 | 0.0999
Epoch 247/300, trend Loss: 0.1061 | 0.0999
Epoch 248/300, trend Loss: 0.1061 | 0.0999
Epoch 249/300, trend Loss: 0.1061 | 0.0999
Epoch 250/300, trend Loss: 0.1061 | 0.0999
Epoch 251/300, trend Loss: 0.1061 | 0.0999
Epoch 252/300, trend Loss: 0.1061 | 0.0999
Epoch 253/300, trend Loss: 0.1061 | 0.0999
Epoch 254/300, trend Loss: 0.1061 | 0.0999
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 40, 'train_rates': 0.9817583756279298, 'learning_rate': 0.00010045968631504395, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9130452153192891}
Epoch 1/300, seasonal_0 Loss: 1.0693 | 0.5476
Epoch 2/300, seasonal_0 Loss: 0.5469 | 0.5020
Epoch 3/300, seasonal_0 Loss: 0.3338 | 0.2956
Epoch 4/300, seasonal_0 Loss: 0.2643 | 0.2383
Epoch 5/300, seasonal_0 Loss: 0.2384 | 0.2350
Epoch 6/300, seasonal_0 Loss: 0.2112 | 0.2112
Epoch 7/300, seasonal_0 Loss: 0.1926 | 0.1906
Epoch 8/300, seasonal_0 Loss: 0.1807 | 0.1828
Epoch 9/300, seasonal_0 Loss: 0.1715 | 0.1797
Epoch 10/300, seasonal_0 Loss: 0.1638 | 0.1732
Epoch 11/300, seasonal_0 Loss: 0.1570 | 0.1651
Epoch 12/300, seasonal_0 Loss: 0.1525 | 0.1557
Epoch 13/300, seasonal_0 Loss: 0.1500 | 0.1479
Epoch 14/300, seasonal_0 Loss: 0.1493 | 0.1384
Epoch 15/300, seasonal_0 Loss: 0.1470 | 0.1305
Epoch 16/300, seasonal_0 Loss: 0.1423 | 0.1192
Epoch 17/300, seasonal_0 Loss: 0.1408 | 0.1123
Epoch 18/300, seasonal_0 Loss: 0.1427 | 0.1065
Epoch 19/300, seasonal_0 Loss: 0.1428 | 0.1023
Epoch 20/300, seasonal_0 Loss: 0.1380 | 0.1003
Epoch 21/300, seasonal_0 Loss: 0.1304 | 0.0981
Epoch 22/300, seasonal_0 Loss: 0.1272 | 0.0979
Epoch 23/300, seasonal_0 Loss: 0.1312 | 0.0992
Epoch 24/300, seasonal_0 Loss: 0.1342 | 0.0993
Epoch 25/300, seasonal_0 Loss: 0.1269 | 0.0975
Epoch 26/300, seasonal_0 Loss: 0.1244 | 0.0943
Epoch 27/300, seasonal_0 Loss: 0.1245 | 0.0883
Epoch 28/300, seasonal_0 Loss: 0.1220 | 0.0848
Epoch 29/300, seasonal_0 Loss: 0.1212 | 0.0834
Epoch 30/300, seasonal_0 Loss: 0.1200 | 0.0824
Epoch 31/300, seasonal_0 Loss: 0.1182 | 0.0819
Epoch 32/300, seasonal_0 Loss: 0.1170 | 0.0817
Epoch 33/300, seasonal_0 Loss: 0.1162 | 0.0808
Epoch 34/300, seasonal_0 Loss: 0.1155 | 0.0795
Epoch 35/300, seasonal_0 Loss: 0.1149 | 0.0785
Epoch 36/300, seasonal_0 Loss: 0.1143 | 0.0774
Epoch 37/300, seasonal_0 Loss: 0.1138 | 0.0768
Epoch 38/300, seasonal_0 Loss: 0.1132 | 0.0762
Epoch 39/300, seasonal_0 Loss: 0.1126 | 0.0755
Epoch 40/300, seasonal_0 Loss: 0.1121 | 0.0751
Epoch 41/300, seasonal_0 Loss: 0.1116 | 0.0746
Epoch 42/300, seasonal_0 Loss: 0.1111 | 0.0742
Epoch 43/300, seasonal_0 Loss: 0.1107 | 0.0736
Epoch 44/300, seasonal_0 Loss: 0.1102 | 0.0731
Epoch 45/300, seasonal_0 Loss: 0.1098 | 0.0728
Epoch 46/300, seasonal_0 Loss: 0.1094 | 0.0724
Epoch 47/300, seasonal_0 Loss: 0.1090 | 0.0720
Epoch 48/300, seasonal_0 Loss: 0.1087 | 0.0717
Epoch 49/300, seasonal_0 Loss: 0.1083 | 0.0713
Epoch 50/300, seasonal_0 Loss: 0.1080 | 0.0710
Epoch 51/300, seasonal_0 Loss: 0.1076 | 0.0707
Epoch 52/300, seasonal_0 Loss: 0.1073 | 0.0705
Epoch 53/300, seasonal_0 Loss: 0.1070 | 0.0702
Epoch 54/300, seasonal_0 Loss: 0.1067 | 0.0699
Epoch 55/300, seasonal_0 Loss: 0.1064 | 0.0697
Epoch 56/300, seasonal_0 Loss: 0.1061 | 0.0694
Epoch 57/300, seasonal_0 Loss: 0.1059 | 0.0692
Epoch 58/300, seasonal_0 Loss: 0.1057 | 0.0690
Epoch 59/300, seasonal_0 Loss: 0.1054 | 0.0688
Epoch 60/300, seasonal_0 Loss: 0.1052 | 0.0686
Epoch 61/300, seasonal_0 Loss: 0.1049 | 0.0684
Epoch 62/300, seasonal_0 Loss: 0.1047 | 0.0683
Epoch 63/300, seasonal_0 Loss: 0.1045 | 0.0681
Epoch 64/300, seasonal_0 Loss: 0.1043 | 0.0679
Epoch 65/300, seasonal_0 Loss: 0.1041 | 0.0678
Epoch 66/300, seasonal_0 Loss: 0.1039 | 0.0676
Epoch 67/300, seasonal_0 Loss: 0.1037 | 0.0675
Epoch 68/300, seasonal_0 Loss: 0.1036 | 0.0673
Epoch 69/300, seasonal_0 Loss: 0.1034 | 0.0672
Epoch 70/300, seasonal_0 Loss: 0.1032 | 0.0671
Epoch 71/300, seasonal_0 Loss: 0.1031 | 0.0670
Epoch 72/300, seasonal_0 Loss: 0.1029 | 0.0668
Epoch 73/300, seasonal_0 Loss: 0.1028 | 0.0667
Epoch 74/300, seasonal_0 Loss: 0.1026 | 0.0666
Epoch 75/300, seasonal_0 Loss: 0.1025 | 0.0665
Epoch 76/300, seasonal_0 Loss: 0.1023 | 0.0664
Epoch 77/300, seasonal_0 Loss: 0.1022 | 0.0663
Epoch 78/300, seasonal_0 Loss: 0.1021 | 0.0662
Epoch 79/300, seasonal_0 Loss: 0.1020 | 0.0661
Epoch 80/300, seasonal_0 Loss: 0.1019 | 0.0660
Epoch 81/300, seasonal_0 Loss: 0.1017 | 0.0660
Epoch 82/300, seasonal_0 Loss: 0.1016 | 0.0659
Epoch 83/300, seasonal_0 Loss: 0.1015 | 0.0658
Epoch 84/300, seasonal_0 Loss: 0.1014 | 0.0657
Epoch 85/300, seasonal_0 Loss: 0.1013 | 0.0657
Epoch 86/300, seasonal_0 Loss: 0.1012 | 0.0656
Epoch 87/300, seasonal_0 Loss: 0.1011 | 0.0655
Epoch 88/300, seasonal_0 Loss: 0.1011 | 0.0655
Epoch 89/300, seasonal_0 Loss: 0.1010 | 0.0654
Epoch 90/300, seasonal_0 Loss: 0.1009 | 0.0653
Epoch 91/300, seasonal_0 Loss: 0.1008 | 0.0653
Epoch 92/300, seasonal_0 Loss: 0.1007 | 0.0652
Epoch 93/300, seasonal_0 Loss: 0.1007 | 0.0652
Epoch 94/300, seasonal_0 Loss: 0.1006 | 0.0651
Epoch 95/300, seasonal_0 Loss: 0.1005 | 0.0651
Epoch 96/300, seasonal_0 Loss: 0.1004 | 0.0650
Epoch 97/300, seasonal_0 Loss: 0.1004 | 0.0650
Epoch 98/300, seasonal_0 Loss: 0.1003 | 0.0649
Epoch 99/300, seasonal_0 Loss: 0.1003 | 0.0649
Epoch 100/300, seasonal_0 Loss: 0.1002 | 0.0648
Epoch 101/300, seasonal_0 Loss: 0.1001 | 0.0648
Epoch 102/300, seasonal_0 Loss: 0.1001 | 0.0648
Epoch 103/300, seasonal_0 Loss: 0.1000 | 0.0647
Epoch 104/300, seasonal_0 Loss: 0.1000 | 0.0647
Epoch 105/300, seasonal_0 Loss: 0.0999 | 0.0646
Epoch 106/300, seasonal_0 Loss: 0.0999 | 0.0646
Epoch 107/300, seasonal_0 Loss: 0.0998 | 0.0646
Epoch 108/300, seasonal_0 Loss: 0.0998 | 0.0645
Epoch 109/300, seasonal_0 Loss: 0.0997 | 0.0645
Epoch 110/300, seasonal_0 Loss: 0.0997 | 0.0645
Epoch 111/300, seasonal_0 Loss: 0.0997 | 0.0644
Epoch 112/300, seasonal_0 Loss: 0.0996 | 0.0644
Epoch 113/300, seasonal_0 Loss: 0.0996 | 0.0644
Epoch 114/300, seasonal_0 Loss: 0.0995 | 0.0644
Epoch 115/300, seasonal_0 Loss: 0.0995 | 0.0643
Epoch 116/300, seasonal_0 Loss: 0.0995 | 0.0643
Epoch 117/300, seasonal_0 Loss: 0.0994 | 0.0643
Epoch 118/300, seasonal_0 Loss: 0.0994 | 0.0643
Epoch 119/300, seasonal_0 Loss: 0.0994 | 0.0642
Epoch 120/300, seasonal_0 Loss: 0.0993 | 0.0642
Epoch 121/300, seasonal_0 Loss: 0.0993 | 0.0642
Epoch 122/300, seasonal_0 Loss: 0.0993 | 0.0642
Epoch 123/300, seasonal_0 Loss: 0.0993 | 0.0642
Epoch 124/300, seasonal_0 Loss: 0.0992 | 0.0641
Epoch 125/300, seasonal_0 Loss: 0.0992 | 0.0641
Epoch 126/300, seasonal_0 Loss: 0.0992 | 0.0641
Epoch 127/300, seasonal_0 Loss: 0.0992 | 0.0641
Epoch 128/300, seasonal_0 Loss: 0.0991 | 0.0641
Epoch 129/300, seasonal_0 Loss: 0.0991 | 0.0641
Epoch 130/300, seasonal_0 Loss: 0.0991 | 0.0640
Epoch 131/300, seasonal_0 Loss: 0.0991 | 0.0640
Epoch 132/300, seasonal_0 Loss: 0.0990 | 0.0640
Epoch 133/300, seasonal_0 Loss: 0.0990 | 0.0640
Epoch 134/300, seasonal_0 Loss: 0.0990 | 0.0640
Epoch 135/300, seasonal_0 Loss: 0.0990 | 0.0640
Epoch 136/300, seasonal_0 Loss: 0.0990 | 0.0640
Epoch 137/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 138/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 139/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 140/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 141/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 142/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 143/300, seasonal_0 Loss: 0.0989 | 0.0639
Epoch 144/300, seasonal_0 Loss: 0.0988 | 0.0639
Epoch 145/300, seasonal_0 Loss: 0.0988 | 0.0639
Epoch 146/300, seasonal_0 Loss: 0.0988 | 0.0639
Epoch 147/300, seasonal_0 Loss: 0.0988 | 0.0638
Epoch 148/300, seasonal_0 Loss: 0.0988 | 0.0638
Epoch 149/300, seasonal_0 Loss: 0.0988 | 0.0638
Epoch 150/300, seasonal_0 Loss: 0.0988 | 0.0638
Epoch 151/300, seasonal_0 Loss: 0.0988 | 0.0638
Epoch 152/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 153/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 154/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 155/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 156/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 157/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 158/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 159/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 160/300, seasonal_0 Loss: 0.0987 | 0.0638
Epoch 161/300, seasonal_0 Loss: 0.0987 | 0.0637
Epoch 162/300, seasonal_0 Loss: 0.0987 | 0.0637
Epoch 163/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 164/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 165/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 166/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 167/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 168/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 169/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 170/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 171/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 172/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 173/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 174/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 175/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 176/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 177/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 178/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 179/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 180/300, seasonal_0 Loss: 0.0986 | 0.0637
Epoch 181/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 182/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 183/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 184/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 185/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 186/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 187/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 188/300, seasonal_0 Loss: 0.0985 | 0.0637
Epoch 189/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 190/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 191/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 192/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 193/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 194/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 195/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 196/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 197/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 198/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 199/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 200/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 201/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 202/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 203/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 204/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 205/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 206/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 207/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 208/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 209/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 210/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 211/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 212/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 213/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 214/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 215/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 216/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 217/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 218/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 219/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 220/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 221/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 222/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 223/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 224/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 225/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 226/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 227/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 228/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 229/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 230/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 231/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 232/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 233/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 234/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 235/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 236/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 237/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 238/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 239/300, seasonal_0 Loss: 0.0985 | 0.0636
Epoch 240/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 241/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 242/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 243/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 244/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 245/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 246/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 247/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 248/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 249/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 250/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 251/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 252/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 253/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 254/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 255/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 256/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 257/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 258/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 259/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 260/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 261/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 262/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 263/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 264/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 265/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 266/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 267/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 268/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 269/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 270/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 271/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 272/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 273/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 274/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 275/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 276/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 277/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 278/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 279/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 280/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 281/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 282/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 283/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 284/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 285/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 286/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 287/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 288/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 289/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 290/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 291/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 292/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 293/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 294/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 295/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 296/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 297/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 298/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 299/300, seasonal_0 Loss: 0.0984 | 0.0636
Epoch 300/300, seasonal_0 Loss: 0.0984 | 0.0636
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.9842228395349396, 'learning_rate': 0.0003899080586656311, 'batch_size': 211, 'step_size': 11, 'gamma': 0.9847343976297569}
Epoch 1/300, seasonal_1 Loss: 0.3770 | 0.1641
Epoch 2/300, seasonal_1 Loss: 0.1991 | 0.1250
Epoch 3/300, seasonal_1 Loss: 0.2043 | 0.1154
Epoch 4/300, seasonal_1 Loss: 0.3254 | 0.6030
Epoch 5/300, seasonal_1 Loss: 0.3458 | 0.1500
Epoch 6/300, seasonal_1 Loss: 0.2241 | 0.1035
Epoch 7/300, seasonal_1 Loss: 0.2601 | 0.1002
Epoch 8/300, seasonal_1 Loss: 0.1673 | 0.0882
Epoch 9/300, seasonal_1 Loss: 0.1588 | 0.0784
Epoch 10/300, seasonal_1 Loss: 0.1429 | 0.1027
Epoch 11/300, seasonal_1 Loss: 0.1272 | 0.0731
Epoch 12/300, seasonal_1 Loss: 0.1221 | 0.0626
Epoch 13/300, seasonal_1 Loss: 0.1141 | 0.0615
Epoch 14/300, seasonal_1 Loss: 0.1120 | 0.0624
Epoch 15/300, seasonal_1 Loss: 0.1130 | 0.0592
Epoch 16/300, seasonal_1 Loss: 0.1141 | 0.0654
Epoch 17/300, seasonal_1 Loss: 0.1074 | 0.0572
Epoch 18/300, seasonal_1 Loss: 0.1043 | 0.0534
Epoch 19/300, seasonal_1 Loss: 0.1016 | 0.0535
Epoch 20/300, seasonal_1 Loss: 0.1042 | 0.0562
Epoch 21/300, seasonal_1 Loss: 0.1089 | 0.0637
Epoch 22/300, seasonal_1 Loss: 0.1114 | 0.1143
Epoch 23/300, seasonal_1 Loss: 0.1108 | 0.0846
Epoch 24/300, seasonal_1 Loss: 0.1093 | 0.0541
Epoch 25/300, seasonal_1 Loss: 0.1108 | 0.0730
Epoch 26/300, seasonal_1 Loss: 0.1042 | 0.0509
Epoch 27/300, seasonal_1 Loss: 0.0934 | 0.0586
Epoch 28/300, seasonal_1 Loss: 0.0910 | 0.0477
Epoch 29/300, seasonal_1 Loss: 0.0899 | 0.0466
Epoch 30/300, seasonal_1 Loss: 0.0937 | 0.0636
Epoch 31/300, seasonal_1 Loss: 0.0883 | 0.0436
Epoch 32/300, seasonal_1 Loss: 0.0871 | 0.0481
Epoch 33/300, seasonal_1 Loss: 0.0875 | 0.0564
Epoch 34/300, seasonal_1 Loss: 0.0855 | 0.0487
Epoch 35/300, seasonal_1 Loss: 0.0836 | 0.0428
Epoch 36/300, seasonal_1 Loss: 0.0865 | 0.0423
Epoch 37/300, seasonal_1 Loss: 0.0907 | 0.0557
Epoch 38/300, seasonal_1 Loss: 0.0882 | 0.0465
Epoch 39/300, seasonal_1 Loss: 0.0917 | 0.0469
Epoch 40/300, seasonal_1 Loss: 0.0927 | 0.0515
Epoch 41/300, seasonal_1 Loss: 0.0933 | 0.0472
Epoch 42/300, seasonal_1 Loss: 0.0927 | 0.0558
Epoch 43/300, seasonal_1 Loss: 0.0948 | 0.0642
Epoch 44/300, seasonal_1 Loss: 0.1083 | 0.0694
Epoch 45/300, seasonal_1 Loss: 0.1104 | 0.0542
Epoch 46/300, seasonal_1 Loss: 0.0903 | 0.0443
Epoch 47/300, seasonal_1 Loss: 0.0894 | 0.0436
Epoch 48/300, seasonal_1 Loss: 0.0980 | 0.0441
Epoch 49/300, seasonal_1 Loss: 0.0899 | 0.0466
Epoch 50/300, seasonal_1 Loss: 0.0898 | 0.0483
Epoch 51/300, seasonal_1 Loss: 0.1030 | 0.0720
Epoch 52/300, seasonal_1 Loss: 0.0975 | 0.0698
Epoch 53/300, seasonal_1 Loss: 0.0927 | 0.0471
Epoch 54/300, seasonal_1 Loss: 0.0892 | 0.0416
Epoch 55/300, seasonal_1 Loss: 0.0960 | 0.0558
Epoch 56/300, seasonal_1 Loss: 0.0943 | 0.0596
Epoch 57/300, seasonal_1 Loss: 0.0965 | 0.0819
Epoch 58/300, seasonal_1 Loss: 0.0939 | 0.0418
Epoch 59/300, seasonal_1 Loss: 0.0840 | 0.0382
Epoch 60/300, seasonal_1 Loss: 0.0838 | 0.0403
Epoch 61/300, seasonal_1 Loss: 0.0782 | 0.0346
Epoch 62/300, seasonal_1 Loss: 0.0807 | 0.0497
Epoch 63/300, seasonal_1 Loss: 0.0814 | 0.0372
Epoch 64/300, seasonal_1 Loss: 0.0773 | 0.0343
Epoch 65/300, seasonal_1 Loss: 0.0781 | 0.0385
Epoch 66/300, seasonal_1 Loss: 0.0775 | 0.0346
Epoch 67/300, seasonal_1 Loss: 0.0786 | 0.0461
Epoch 68/300, seasonal_1 Loss: 0.0795 | 0.0475
Epoch 69/300, seasonal_1 Loss: 0.0799 | 0.0337
Epoch 70/300, seasonal_1 Loss: 0.0730 | 0.0338
Epoch 71/300, seasonal_1 Loss: 0.0734 | 0.0324
Epoch 72/300, seasonal_1 Loss: 0.0740 | 0.0340
Epoch 73/300, seasonal_1 Loss: 0.0733 | 0.0418
Epoch 74/300, seasonal_1 Loss: 0.0724 | 0.0317
Epoch 75/300, seasonal_1 Loss: 0.0711 | 0.0302
Epoch 76/300, seasonal_1 Loss: 0.0699 | 0.0302
Epoch 77/300, seasonal_1 Loss: 0.0703 | 0.0308
Epoch 78/300, seasonal_1 Loss: 0.0700 | 0.0325
Epoch 79/300, seasonal_1 Loss: 0.0692 | 0.0303
Epoch 80/300, seasonal_1 Loss: 0.0689 | 0.0294
Epoch 81/300, seasonal_1 Loss: 0.0691 | 0.0302
Epoch 82/300, seasonal_1 Loss: 0.0708 | 0.0320
Epoch 83/300, seasonal_1 Loss: 0.0715 | 0.0316
Epoch 84/300, seasonal_1 Loss: 0.0700 | 0.0291
Epoch 85/300, seasonal_1 Loss: 0.0703 | 0.0292
Epoch 86/300, seasonal_1 Loss: 0.0736 | 0.0313
Epoch 87/300, seasonal_1 Loss: 0.0721 | 0.0350
Epoch 88/300, seasonal_1 Loss: 0.0737 | 0.0294
Epoch 89/300, seasonal_1 Loss: 0.0714 | 0.0305
Epoch 90/300, seasonal_1 Loss: 0.0697 | 0.0288
Epoch 91/300, seasonal_1 Loss: 0.0689 | 0.0264
Epoch 92/300, seasonal_1 Loss: 0.0663 | 0.0259
Epoch 93/300, seasonal_1 Loss: 0.0669 | 0.0279
Epoch 94/300, seasonal_1 Loss: 0.0688 | 0.0301
Epoch 95/300, seasonal_1 Loss: 0.0675 | 0.0299
Epoch 96/300, seasonal_1 Loss: 0.0671 | 0.0253
Epoch 97/300, seasonal_1 Loss: 0.0655 | 0.0260
Epoch 98/300, seasonal_1 Loss: 0.0674 | 0.0267
Epoch 99/300, seasonal_1 Loss: 0.0694 | 0.0272
Epoch 100/300, seasonal_1 Loss: 0.0686 | 0.0322
Epoch 101/300, seasonal_1 Loss: 0.0701 | 0.0337
Epoch 102/300, seasonal_1 Loss: 0.0757 | 0.0362
Epoch 103/300, seasonal_1 Loss: 0.0706 | 0.0363
Epoch 104/300, seasonal_1 Loss: 0.0726 | 0.0314
Epoch 105/300, seasonal_1 Loss: 0.0670 | 0.0269
Epoch 106/300, seasonal_1 Loss: 0.0648 | 0.0250
Epoch 107/300, seasonal_1 Loss: 0.0633 | 0.0226
Epoch 108/300, seasonal_1 Loss: 0.0631 | 0.0242
Epoch 109/300, seasonal_1 Loss: 0.0622 | 0.0248
Epoch 110/300, seasonal_1 Loss: 0.0646 | 0.0257
Epoch 111/300, seasonal_1 Loss: 0.0677 | 0.0379
Epoch 112/300, seasonal_1 Loss: 0.0662 | 0.0301
Epoch 113/300, seasonal_1 Loss: 0.0638 | 0.0228
Epoch 114/300, seasonal_1 Loss: 0.0627 | 0.0256
Epoch 115/300, seasonal_1 Loss: 0.0645 | 0.0244
Epoch 116/300, seasonal_1 Loss: 0.0633 | 0.0264
Epoch 117/300, seasonal_1 Loss: 0.0622 | 0.0244
Epoch 118/300, seasonal_1 Loss: 0.0633 | 0.0261
Epoch 119/300, seasonal_1 Loss: 0.0660 | 0.0262
Epoch 120/300, seasonal_1 Loss: 0.0649 | 0.0224
Epoch 121/300, seasonal_1 Loss: 0.0609 | 0.0225
Epoch 122/300, seasonal_1 Loss: 0.0618 | 0.0224
Epoch 123/300, seasonal_1 Loss: 0.0612 | 0.0234
Epoch 124/300, seasonal_1 Loss: 0.0634 | 0.0269
Epoch 125/300, seasonal_1 Loss: 0.0622 | 0.0251
Epoch 126/300, seasonal_1 Loss: 0.0601 | 0.0224
Epoch 127/300, seasonal_1 Loss: 0.0585 | 0.0219
Epoch 128/300, seasonal_1 Loss: 0.0594 | 0.0223
Epoch 129/300, seasonal_1 Loss: 0.0610 | 0.0222
Epoch 130/300, seasonal_1 Loss: 0.0617 | 0.0269
Epoch 131/300, seasonal_1 Loss: 0.0623 | 0.0258
Epoch 132/300, seasonal_1 Loss: 0.0627 | 0.0254
Epoch 133/300, seasonal_1 Loss: 0.0603 | 0.0270
Epoch 134/300, seasonal_1 Loss: 0.0593 | 0.0235
Epoch 135/300, seasonal_1 Loss: 0.0605 | 0.0242
Epoch 136/300, seasonal_1 Loss: 0.0664 | 0.0291
Epoch 137/300, seasonal_1 Loss: 0.0696 | 0.0382
Epoch 138/300, seasonal_1 Loss: 0.0728 | 0.0390
Epoch 139/300, seasonal_1 Loss: 0.0746 | 0.0397
Epoch 140/300, seasonal_1 Loss: 0.0631 | 0.0251
Epoch 141/300, seasonal_1 Loss: 0.0627 | 0.0248
Epoch 142/300, seasonal_1 Loss: 0.0731 | 0.0297
Epoch 143/300, seasonal_1 Loss: 0.0696 | 0.0279
Epoch 144/300, seasonal_1 Loss: 0.0673 | 0.0305
Epoch 145/300, seasonal_1 Loss: 0.0640 | 0.0222
Epoch 146/300, seasonal_1 Loss: 0.0610 | 0.0230
Epoch 147/300, seasonal_1 Loss: 0.0579 | 0.0230
Epoch 148/300, seasonal_1 Loss: 0.0589 | 0.0244
Epoch 149/300, seasonal_1 Loss: 0.0591 | 0.0231
Epoch 150/300, seasonal_1 Loss: 0.0575 | 0.0227
Epoch 151/300, seasonal_1 Loss: 0.0567 | 0.0206
Epoch 152/300, seasonal_1 Loss: 0.0560 | 0.0218
Epoch 153/300, seasonal_1 Loss: 0.0570 | 0.0225
Epoch 154/300, seasonal_1 Loss: 0.0560 | 0.0221
Epoch 155/300, seasonal_1 Loss: 0.0545 | 0.0207
Epoch 156/300, seasonal_1 Loss: 0.0552 | 0.0209
Epoch 157/300, seasonal_1 Loss: 0.0551 | 0.0206
Epoch 158/300, seasonal_1 Loss: 0.0546 | 0.0209
Epoch 159/300, seasonal_1 Loss: 0.0551 | 0.0223
Epoch 160/300, seasonal_1 Loss: 0.0552 | 0.0210
Epoch 161/300, seasonal_1 Loss: 0.0552 | 0.0212
Epoch 162/300, seasonal_1 Loss: 0.0547 | 0.0210
Epoch 163/300, seasonal_1 Loss: 0.0550 | 0.0230
Epoch 164/300, seasonal_1 Loss: 0.0551 | 0.0250
Epoch 165/300, seasonal_1 Loss: 0.0569 | 0.0247
Epoch 166/300, seasonal_1 Loss: 0.0588 | 0.0263
Epoch 167/300, seasonal_1 Loss: 0.0571 | 0.0255
Epoch 168/300, seasonal_1 Loss: 0.0562 | 0.0237
Epoch 169/300, seasonal_1 Loss: 0.0571 | 0.0254
Epoch 170/300, seasonal_1 Loss: 0.0579 | 0.0279
Epoch 171/300, seasonal_1 Loss: 0.0558 | 0.0271
Epoch 172/300, seasonal_1 Loss: 0.0549 | 0.0220
Epoch 173/300, seasonal_1 Loss: 0.0528 | 0.0208
Epoch 174/300, seasonal_1 Loss: 0.0529 | 0.0212
Epoch 175/300, seasonal_1 Loss: 0.0530 | 0.0218
Epoch 176/300, seasonal_1 Loss: 0.0527 | 0.0222
Epoch 177/300, seasonal_1 Loss: 0.0527 | 0.0214
Epoch 178/300, seasonal_1 Loss: 0.0545 | 0.0257
Epoch 179/300, seasonal_1 Loss: 0.0554 | 0.0246
Epoch 180/300, seasonal_1 Loss: 0.0553 | 0.0272
Epoch 181/300, seasonal_1 Loss: 0.0570 | 0.0234
Epoch 182/300, seasonal_1 Loss: 0.0565 | 0.0234
Epoch 183/300, seasonal_1 Loss: 0.0534 | 0.0207
Epoch 184/300, seasonal_1 Loss: 0.0545 | 0.0240
Epoch 185/300, seasonal_1 Loss: 0.0601 | 0.0288
Epoch 186/300, seasonal_1 Loss: 0.0629 | 0.0289
Epoch 187/300, seasonal_1 Loss: 0.0618 | 0.0275
Epoch 188/300, seasonal_1 Loss: 0.0580 | 0.0223
Epoch 189/300, seasonal_1 Loss: 0.0565 | 0.0234
Epoch 190/300, seasonal_1 Loss: 0.0569 | 0.0239
Epoch 191/300, seasonal_1 Loss: 0.0549 | 0.0216
Epoch 192/300, seasonal_1 Loss: 0.0547 | 0.0261
Epoch 193/300, seasonal_1 Loss: 0.0555 | 0.0251
Epoch 194/300, seasonal_1 Loss: 0.0532 | 0.0253
Epoch 195/300, seasonal_1 Loss: 0.0534 | 0.0237
Epoch 196/300, seasonal_1 Loss: 0.0526 | 0.0230
Epoch 197/300, seasonal_1 Loss: 0.0519 | 0.0212
Epoch 198/300, seasonal_1 Loss: 0.0515 | 0.0210
Epoch 199/300, seasonal_1 Loss: 0.0509 | 0.0209
Epoch 200/300, seasonal_1 Loss: 0.0505 | 0.0206
Epoch 201/300, seasonal_1 Loss: 0.0511 | 0.0219
Epoch 202/300, seasonal_1 Loss: 0.0512 | 0.0242
Epoch 203/300, seasonal_1 Loss: 0.0504 | 0.0238
Epoch 204/300, seasonal_1 Loss: 0.0504 | 0.0210
Epoch 205/300, seasonal_1 Loss: 0.0507 | 0.0218
Epoch 206/300, seasonal_1 Loss: 0.0501 | 0.0228
Epoch 207/300, seasonal_1 Loss: 0.0532 | 0.0250
Epoch 208/300, seasonal_1 Loss: 0.0532 | 0.0246
Epoch 209/300, seasonal_1 Loss: 0.0516 | 0.0229
Epoch 210/300, seasonal_1 Loss: 0.0512 | 0.0251
Epoch 211/300, seasonal_1 Loss: 0.0509 | 0.0255
Epoch 212/300, seasonal_1 Loss: 0.0497 | 0.0229
Epoch 213/300, seasonal_1 Loss: 0.0494 | 0.0211
Epoch 214/300, seasonal_1 Loss: 0.0513 | 0.0212
Epoch 215/300, seasonal_1 Loss: 0.0539 | 0.0275
Epoch 216/300, seasonal_1 Loss: 0.0545 | 0.0332
Epoch 217/300, seasonal_1 Loss: 0.0568 | 0.0241
Epoch 218/300, seasonal_1 Loss: 0.0548 | 0.0312
Epoch 219/300, seasonal_1 Loss: 0.0574 | 0.0321
Epoch 220/300, seasonal_1 Loss: 0.0542 | 0.0243
Epoch 221/300, seasonal_1 Loss: 0.0502 | 0.0247
Epoch 222/300, seasonal_1 Loss: 0.0498 | 0.0259
Epoch 223/300, seasonal_1 Loss: 0.0495 | 0.0255
Epoch 224/300, seasonal_1 Loss: 0.0499 | 0.0257
Epoch 225/300, seasonal_1 Loss: 0.0503 | 0.0277
Epoch 226/300, seasonal_1 Loss: 0.0502 | 0.0324
Epoch 227/300, seasonal_1 Loss: 0.0497 | 0.0296
Epoch 228/300, seasonal_1 Loss: 0.0505 | 0.0230
Epoch 229/300, seasonal_1 Loss: 0.0500 | 0.0229
Epoch 230/300, seasonal_1 Loss: 0.0499 | 0.0214
Epoch 231/300, seasonal_1 Loss: 0.0514 | 0.0239
Epoch 232/300, seasonal_1 Loss: 0.0506 | 0.0239
Epoch 233/300, seasonal_1 Loss: 0.0492 | 0.0232
Epoch 234/300, seasonal_1 Loss: 0.0475 | 0.0209
Epoch 235/300, seasonal_1 Loss: 0.0469 | 0.0220
Epoch 236/300, seasonal_1 Loss: 0.0480 | 0.0251
Epoch 237/300, seasonal_1 Loss: 0.0511 | 0.0270
Epoch 238/300, seasonal_1 Loss: 0.0510 | 0.0284
Epoch 239/300, seasonal_1 Loss: 0.0517 | 0.0256
Epoch 240/300, seasonal_1 Loss: 0.0484 | 0.0233
Epoch 241/300, seasonal_1 Loss: 0.0492 | 0.0228
Epoch 242/300, seasonal_1 Loss: 0.0523 | 0.0266
Epoch 243/300, seasonal_1 Loss: 0.0494 | 0.0215
Epoch 244/300, seasonal_1 Loss: 0.0493 | 0.0274
Epoch 245/300, seasonal_1 Loss: 0.0488 | 0.0315
Epoch 246/300, seasonal_1 Loss: 0.0505 | 0.0293
Epoch 247/300, seasonal_1 Loss: 0.0461 | 0.0278
Epoch 248/300, seasonal_1 Loss: 0.0435 | 0.0237
Epoch 249/300, seasonal_1 Loss: 0.0480 | 0.0294
Epoch 250/300, seasonal_1 Loss: 0.0463 | 0.0207
Epoch 251/300, seasonal_1 Loss: 0.0466 | 0.0241
Epoch 252/300, seasonal_1 Loss: 0.0432 | 0.0249
Epoch 253/300, seasonal_1 Loss: 0.0454 | 0.0216
Epoch 254/300, seasonal_1 Loss: 0.0426 | 0.0225
Epoch 255/300, seasonal_1 Loss: 0.0449 | 0.0227
Epoch 256/300, seasonal_1 Loss: 0.0395 | 0.0206
Epoch 257/300, seasonal_1 Loss: 0.0390 | 0.0200
Epoch 258/300, seasonal_1 Loss: 0.0384 | 0.0204
Epoch 259/300, seasonal_1 Loss: 0.0381 | 0.0204
Epoch 260/300, seasonal_1 Loss: 0.0379 | 0.0205
Epoch 261/300, seasonal_1 Loss: 0.0379 | 0.0217
Epoch 262/300, seasonal_1 Loss: 0.0381 | 0.0231
Epoch 263/300, seasonal_1 Loss: 0.0386 | 0.0236
Epoch 264/300, seasonal_1 Loss: 0.0392 | 0.0250
Epoch 265/300, seasonal_1 Loss: 0.0400 | 0.0303
Epoch 266/300, seasonal_1 Loss: 0.0438 | 0.0276
Epoch 267/300, seasonal_1 Loss: 0.0427 | 0.0257
Epoch 268/300, seasonal_1 Loss: 0.0436 | 0.0252
Epoch 269/300, seasonal_1 Loss: 0.0446 | 0.0278
Epoch 270/300, seasonal_1 Loss: 0.0450 | 0.0231
Epoch 271/300, seasonal_1 Loss: 0.0414 | 0.0247
Epoch 272/300, seasonal_1 Loss: 0.0397 | 0.0293
Epoch 273/300, seasonal_1 Loss: 0.0420 | 0.0342
Epoch 274/300, seasonal_1 Loss: 0.0402 | 0.0264
Epoch 275/300, seasonal_1 Loss: 0.0383 | 0.0228
Epoch 276/300, seasonal_1 Loss: 0.0411 | 0.0224
Epoch 277/300, seasonal_1 Loss: 0.0398 | 0.0259
Epoch 278/300, seasonal_1 Loss: 0.0401 | 0.0271
Epoch 279/300, seasonal_1 Loss: 0.0426 | 0.0289
Epoch 280/300, seasonal_1 Loss: 0.0415 | 0.0255
Epoch 281/300, seasonal_1 Loss: 0.0400 | 0.0224
Epoch 282/300, seasonal_1 Loss: 0.0399 | 0.0254
Epoch 283/300, seasonal_1 Loss: 0.0377 | 0.0233
Epoch 284/300, seasonal_1 Loss: 0.0376 | 0.0262
Epoch 285/300, seasonal_1 Loss: 0.0369 | 0.0256
Epoch 286/300, seasonal_1 Loss: 0.0362 | 0.0231
Epoch 287/300, seasonal_1 Loss: 0.0366 | 0.0216
Epoch 288/300, seasonal_1 Loss: 0.0363 | 0.0213
Epoch 289/300, seasonal_1 Loss: 0.0365 | 0.0267
Epoch 290/300, seasonal_1 Loss: 0.0362 | 0.0266
Epoch 291/300, seasonal_1 Loss: 0.0357 | 0.0243
Epoch 292/300, seasonal_1 Loss: 0.0354 | 0.0224
Epoch 293/300, seasonal_1 Loss: 0.0353 | 0.0220
Epoch 294/300, seasonal_1 Loss: 0.0353 | 0.0244
Epoch 295/300, seasonal_1 Loss: 0.0351 | 0.0250
Epoch 296/300, seasonal_1 Loss: 0.0349 | 0.0240
Epoch 297/300, seasonal_1 Loss: 0.0343 | 0.0228
Epoch 298/300, seasonal_1 Loss: 0.0347 | 0.0231
Epoch 299/300, seasonal_1 Loss: 0.0342 | 0.0235
Epoch 300/300, seasonal_1 Loss: 0.0341 | 0.0239
Training seasonal_2 component with params: {'observation_period_num': 250, 'train_rates': 0.9874565052873338, 'learning_rate': 0.0006461200811464487, 'batch_size': 28, 'step_size': 1, 'gamma': 0.9807884850061753}
Epoch 1/300, seasonal_2 Loss: 0.4020 | 0.2814
Epoch 2/300, seasonal_2 Loss: 0.2389 | 0.2513
Epoch 3/300, seasonal_2 Loss: 0.1950 | 0.2367
Epoch 4/300, seasonal_2 Loss: 0.1616 | 0.1989
Epoch 5/300, seasonal_2 Loss: 0.1429 | 0.1697
Epoch 6/300, seasonal_2 Loss: 0.1289 | 0.1360
Epoch 7/300, seasonal_2 Loss: 0.1237 | 0.1340
Epoch 8/300, seasonal_2 Loss: 0.1205 | 0.1813
Epoch 9/300, seasonal_2 Loss: 0.1152 | 0.1757
Epoch 10/300, seasonal_2 Loss: 0.1076 | 0.1416
Epoch 11/300, seasonal_2 Loss: 0.0975 | 0.1147
Epoch 12/300, seasonal_2 Loss: 0.0890 | 0.1226
Epoch 13/300, seasonal_2 Loss: 0.0791 | 0.1126
Epoch 14/300, seasonal_2 Loss: 0.0760 | 0.1182
Epoch 15/300, seasonal_2 Loss: 0.0753 | 0.0933
Epoch 16/300, seasonal_2 Loss: 0.0786 | 0.1461
Epoch 17/300, seasonal_2 Loss: 0.0778 | 0.1191
Epoch 18/300, seasonal_2 Loss: 0.0739 | 0.1032
Epoch 19/300, seasonal_2 Loss: 0.0770 | 0.0961
Epoch 20/300, seasonal_2 Loss: 0.0782 | 0.1037
Epoch 21/300, seasonal_2 Loss: 0.0773 | 0.1478
Epoch 22/300, seasonal_2 Loss: 0.0757 | 0.1050
Epoch 23/300, seasonal_2 Loss: 0.0765 | 0.1113
Epoch 24/300, seasonal_2 Loss: 0.0709 | 0.1072
Epoch 25/300, seasonal_2 Loss: 0.0731 | 0.1050
Epoch 26/300, seasonal_2 Loss: 0.0727 | 0.1062
Epoch 27/300, seasonal_2 Loss: 0.0670 | 0.1097
Epoch 28/300, seasonal_2 Loss: 0.0641 | 0.0829
Epoch 29/300, seasonal_2 Loss: 0.0661 | 0.0879
Epoch 30/300, seasonal_2 Loss: 0.0635 | 0.0883
Epoch 31/300, seasonal_2 Loss: 0.0664 | 0.0920
Epoch 32/300, seasonal_2 Loss: 0.0646 | 0.0941
Epoch 33/300, seasonal_2 Loss: 0.0637 | 0.0890
Epoch 34/300, seasonal_2 Loss: 0.0616 | 0.0818
Epoch 35/300, seasonal_2 Loss: 0.0677 | 0.0896
Epoch 36/300, seasonal_2 Loss: 0.0665 | 0.0811
Epoch 37/300, seasonal_2 Loss: 0.0752 | 0.0974
Epoch 38/300, seasonal_2 Loss: 0.0688 | 0.1338
Epoch 39/300, seasonal_2 Loss: 0.0773 | 0.1247
Epoch 40/300, seasonal_2 Loss: 0.0696 | 0.0910
Epoch 41/300, seasonal_2 Loss: 0.0652 | 0.0879
Epoch 42/300, seasonal_2 Loss: 0.0638 | 0.0917
Epoch 43/300, seasonal_2 Loss: 0.0650 | 0.0994
Epoch 44/300, seasonal_2 Loss: 0.0591 | 0.1014
Epoch 45/300, seasonal_2 Loss: 0.0584 | 0.1090
Epoch 46/300, seasonal_2 Loss: 0.0549 | 0.0996
Epoch 47/300, seasonal_2 Loss: 0.0529 | 0.0983
Epoch 48/300, seasonal_2 Loss: 0.0517 | 0.1030
Epoch 49/300, seasonal_2 Loss: 0.0505 | 0.0986
Epoch 50/300, seasonal_2 Loss: 0.0503 | 0.1202
Epoch 51/300, seasonal_2 Loss: 0.0487 | 0.1025
Epoch 52/300, seasonal_2 Loss: 0.0483 | 0.0980
Epoch 53/300, seasonal_2 Loss: 0.0481 | 0.0974
Epoch 54/300, seasonal_2 Loss: 0.0481 | 0.0934
Epoch 55/300, seasonal_2 Loss: 0.0478 | 0.0895
Epoch 56/300, seasonal_2 Loss: 0.0468 | 0.0913
Epoch 57/300, seasonal_2 Loss: 0.0463 | 0.0961
Epoch 58/300, seasonal_2 Loss: 0.0461 | 0.0971
Epoch 59/300, seasonal_2 Loss: 0.0460 | 0.0962
Epoch 60/300, seasonal_2 Loss: 0.0463 | 0.0967
Epoch 61/300, seasonal_2 Loss: 0.0462 | 0.0974
Epoch 62/300, seasonal_2 Loss: 0.0459 | 0.0987
Epoch 63/300, seasonal_2 Loss: 0.0457 | 0.0948
Epoch 64/300, seasonal_2 Loss: 0.0455 | 0.0950
Epoch 65/300, seasonal_2 Loss: 0.0460 | 0.0969
Epoch 66/300, seasonal_2 Loss: 0.0462 | 0.0989
Epoch 67/300, seasonal_2 Loss: 0.0454 | 0.0984
Epoch 68/300, seasonal_2 Loss: 0.0456 | 0.1010
Epoch 69/300, seasonal_2 Loss: 0.0446 | 0.0987
Epoch 70/300, seasonal_2 Loss: 0.0449 | 0.0954
Epoch 71/300, seasonal_2 Loss: 0.0446 | 0.0966
Epoch 72/300, seasonal_2 Loss: 0.0441 | 0.0980
Epoch 73/300, seasonal_2 Loss: 0.0436 | 0.0960
Epoch 74/300, seasonal_2 Loss: 0.0435 | 0.0944
Epoch 75/300, seasonal_2 Loss: 0.0434 | 0.0966
Epoch 76/300, seasonal_2 Loss: 0.0432 | 0.0955
Epoch 77/300, seasonal_2 Loss: 0.0428 | 0.0935
Epoch 78/300, seasonal_2 Loss: 0.0427 | 0.0919
Epoch 79/300, seasonal_2 Loss: 0.0425 | 0.0934
Epoch 80/300, seasonal_2 Loss: 0.0421 | 0.0920
Epoch 81/300, seasonal_2 Loss: 0.0415 | 0.0901
Epoch 82/300, seasonal_2 Loss: 0.0406 | 0.0900
Epoch 83/300, seasonal_2 Loss: 0.0391 | 0.0927
Epoch 84/300, seasonal_2 Loss: 0.0377 | 0.0937
Epoch 85/300, seasonal_2 Loss: 0.0369 | 0.0935
Epoch 86/300, seasonal_2 Loss: 0.0363 | 0.0931
Epoch 87/300, seasonal_2 Loss: 0.0358 | 0.0933
Epoch 88/300, seasonal_2 Loss: 0.0355 | 0.0933
Epoch 89/300, seasonal_2 Loss: 0.0352 | 0.0934
Epoch 90/300, seasonal_2 Loss: 0.0350 | 0.0940
Epoch 91/300, seasonal_2 Loss: 0.0347 | 0.0949
Epoch 92/300, seasonal_2 Loss: 0.0345 | 0.0956
Epoch 93/300, seasonal_2 Loss: 0.0344 | 0.0958
Epoch 94/300, seasonal_2 Loss: 0.0342 | 0.0954
Epoch 95/300, seasonal_2 Loss: 0.0341 | 0.0947
Epoch 96/300, seasonal_2 Loss: 0.0339 | 0.0941
Epoch 97/300, seasonal_2 Loss: 0.0338 | 0.0941
Epoch 98/300, seasonal_2 Loss: 0.0337 | 0.0942
Epoch 99/300, seasonal_2 Loss: 0.0336 | 0.0949
Epoch 100/300, seasonal_2 Loss: 0.0335 | 0.0952
Epoch 101/300, seasonal_2 Loss: 0.0334 | 0.0956
Epoch 102/300, seasonal_2 Loss: 0.0333 | 0.0952
Epoch 103/300, seasonal_2 Loss: 0.0332 | 0.0952
Epoch 104/300, seasonal_2 Loss: 0.0331 | 0.0947
Epoch 105/300, seasonal_2 Loss: 0.0331 | 0.0948
Epoch 106/300, seasonal_2 Loss: 0.0330 | 0.0946
Epoch 107/300, seasonal_2 Loss: 0.0329 | 0.0951
Epoch 108/300, seasonal_2 Loss: 0.0328 | 0.0951
Epoch 109/300, seasonal_2 Loss: 0.0328 | 0.0954
Epoch 110/300, seasonal_2 Loss: 0.0327 | 0.0952
Epoch 111/300, seasonal_2 Loss: 0.0326 | 0.0953
Epoch 112/300, seasonal_2 Loss: 0.0326 | 0.0951
Epoch 113/300, seasonal_2 Loss: 0.0325 | 0.0951
Epoch 114/300, seasonal_2 Loss: 0.0325 | 0.0950
Epoch 115/300, seasonal_2 Loss: 0.0324 | 0.0951
Epoch 116/300, seasonal_2 Loss: 0.0324 | 0.0951
Epoch 117/300, seasonal_2 Loss: 0.0323 | 0.0953
Epoch 118/300, seasonal_2 Loss: 0.0323 | 0.0953
Epoch 119/300, seasonal_2 Loss: 0.0322 | 0.0954
Epoch 120/300, seasonal_2 Loss: 0.0322 | 0.0954
Epoch 121/300, seasonal_2 Loss: 0.0321 | 0.0955
Epoch 122/300, seasonal_2 Loss: 0.0321 | 0.0954
Epoch 123/300, seasonal_2 Loss: 0.0321 | 0.0955
Epoch 124/300, seasonal_2 Loss: 0.0320 | 0.0955
Epoch 125/300, seasonal_2 Loss: 0.0320 | 0.0956
Epoch 126/300, seasonal_2 Loss: 0.0319 | 0.0956
Epoch 127/300, seasonal_2 Loss: 0.0319 | 0.0956
Epoch 128/300, seasonal_2 Loss: 0.0319 | 0.0956
Epoch 129/300, seasonal_2 Loss: 0.0318 | 0.0957
Epoch 130/300, seasonal_2 Loss: 0.0318 | 0.0957
Epoch 131/300, seasonal_2 Loss: 0.0318 | 0.0957
Epoch 132/300, seasonal_2 Loss: 0.0318 | 0.0957
Epoch 133/300, seasonal_2 Loss: 0.0317 | 0.0957
Epoch 134/300, seasonal_2 Loss: 0.0317 | 0.0957
Epoch 135/300, seasonal_2 Loss: 0.0317 | 0.0957
Epoch 136/300, seasonal_2 Loss: 0.0316 | 0.0957
Epoch 137/300, seasonal_2 Loss: 0.0316 | 0.0958
Epoch 138/300, seasonal_2 Loss: 0.0316 | 0.0958
Epoch 139/300, seasonal_2 Loss: 0.0316 | 0.0958
Epoch 140/300, seasonal_2 Loss: 0.0316 | 0.0958
Epoch 141/300, seasonal_2 Loss: 0.0315 | 0.0959
Epoch 142/300, seasonal_2 Loss: 0.0315 | 0.0959
Epoch 143/300, seasonal_2 Loss: 0.0315 | 0.0959
Epoch 144/300, seasonal_2 Loss: 0.0315 | 0.0960
Epoch 145/300, seasonal_2 Loss: 0.0315 | 0.0960
Epoch 146/300, seasonal_2 Loss: 0.0314 | 0.0961
Epoch 147/300, seasonal_2 Loss: 0.0314 | 0.0961
Epoch 148/300, seasonal_2 Loss: 0.0314 | 0.0961
Epoch 149/300, seasonal_2 Loss: 0.0314 | 0.0961
Epoch 150/300, seasonal_2 Loss: 0.0314 | 0.0962
Epoch 151/300, seasonal_2 Loss: 0.0314 | 0.0962
Epoch 152/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 153/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 154/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 155/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 156/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 157/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 158/300, seasonal_2 Loss: 0.0313 | 0.0962
Epoch 159/300, seasonal_2 Loss: 0.0313 | 0.0963
Epoch 160/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 161/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 162/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 163/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 164/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 165/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 166/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 167/300, seasonal_2 Loss: 0.0312 | 0.0963
Epoch 168/300, seasonal_2 Loss: 0.0312 | 0.0964
Epoch 169/300, seasonal_2 Loss: 0.0312 | 0.0964
Epoch 170/300, seasonal_2 Loss: 0.0312 | 0.0964
Epoch 171/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 172/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 173/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 174/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 175/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 176/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 177/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 178/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 179/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 180/300, seasonal_2 Loss: 0.0311 | 0.0964
Epoch 181/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 182/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 183/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 184/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 185/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 186/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 187/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 188/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 189/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 190/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 191/300, seasonal_2 Loss: 0.0311 | 0.0965
Epoch 192/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 193/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 194/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 195/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 196/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 197/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 198/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 199/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 200/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 201/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 202/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 203/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 204/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 205/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 206/300, seasonal_2 Loss: 0.0310 | 0.0965
Epoch 207/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 208/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 209/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 210/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 211/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 212/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 213/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 214/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 215/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 216/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 217/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 218/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 219/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 220/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 221/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 222/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 223/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 224/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 225/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 226/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 227/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 228/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 229/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 230/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 231/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 232/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 233/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 234/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 235/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 236/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 237/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 238/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 239/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 240/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 241/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 242/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 243/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 244/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 245/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 246/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 247/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 248/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 249/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 250/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 251/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 252/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 253/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 254/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 255/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 256/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 257/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 258/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 259/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 260/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 261/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 262/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 263/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 264/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 265/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 266/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 267/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 268/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 269/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 270/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 271/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 272/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 273/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 274/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 275/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 276/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 277/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 278/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 279/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 280/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 281/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 282/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 283/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 284/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 285/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 286/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 287/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 288/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 289/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 290/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 291/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 292/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 293/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 294/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 295/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 296/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 297/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 298/300, seasonal_2 Loss: 0.0310 | 0.0966
Epoch 299/300, seasonal_2 Loss: 0.0310 | 0.0966
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 9, 'train_rates': 0.6355800205984784, 'learning_rate': 0.0005215479963514043, 'batch_size': 23, 'step_size': 2, 'gamma': 0.9414463114955876}
Epoch 1/300, seasonal_3 Loss: 0.2124 | 0.3629
Epoch 2/300, seasonal_3 Loss: 0.1378 | 0.4006
Epoch 3/300, seasonal_3 Loss: 0.1263 | 0.2907
Epoch 4/300, seasonal_3 Loss: 0.1176 | 0.1750
Epoch 5/300, seasonal_3 Loss: 0.1105 | 0.1214
Epoch 6/300, seasonal_3 Loss: 0.1052 | 0.0946
Epoch 7/300, seasonal_3 Loss: 0.1015 | 0.0879
Epoch 8/300, seasonal_3 Loss: 0.0972 | 0.0887
Epoch 9/300, seasonal_3 Loss: 0.0931 | 0.0843
Epoch 10/300, seasonal_3 Loss: 0.0891 | 0.0748
Epoch 11/300, seasonal_3 Loss: 0.0868 | 0.0687
Epoch 12/300, seasonal_3 Loss: 0.0854 | 0.0667
Epoch 13/300, seasonal_3 Loss: 0.0837 | 0.0665
Epoch 14/300, seasonal_3 Loss: 0.0826 | 0.0650
Epoch 15/300, seasonal_3 Loss: 0.0808 | 0.0653
Epoch 16/300, seasonal_3 Loss: 0.0799 | 0.0634
Epoch 17/300, seasonal_3 Loss: 0.0785 | 0.0629
Epoch 18/300, seasonal_3 Loss: 0.0773 | 0.0631
Epoch 19/300, seasonal_3 Loss: 0.0762 | 0.0625
Epoch 20/300, seasonal_3 Loss: 0.0752 | 0.0630
Epoch 21/300, seasonal_3 Loss: 0.0744 | 0.0629
Epoch 22/300, seasonal_3 Loss: 0.0738 | 0.0636
Epoch 23/300, seasonal_3 Loss: 0.0734 | 0.0635
Epoch 24/300, seasonal_3 Loss: 0.0726 | 0.0627
Epoch 25/300, seasonal_3 Loss: 0.0721 | 0.0631
Epoch 26/300, seasonal_3 Loss: 0.0714 | 0.0632
Epoch 27/300, seasonal_3 Loss: 0.0709 | 0.0625
Epoch 28/300, seasonal_3 Loss: 0.0706 | 0.0631
Epoch 29/300, seasonal_3 Loss: 0.0700 | 0.0625
Epoch 30/300, seasonal_3 Loss: 0.0695 | 0.0634
Epoch 31/300, seasonal_3 Loss: 0.0693 | 0.0638
Epoch 32/300, seasonal_3 Loss: 0.0690 | 0.0633
Epoch 33/300, seasonal_3 Loss: 0.0685 | 0.0618
Epoch 34/300, seasonal_3 Loss: 0.0684 | 0.0621
Epoch 35/300, seasonal_3 Loss: 0.0682 | 0.0624
Epoch 36/300, seasonal_3 Loss: 0.0680 | 0.0621
Epoch 37/300, seasonal_3 Loss: 0.0677 | 0.0625
Epoch 38/300, seasonal_3 Loss: 0.0672 | 0.0623
Epoch 39/300, seasonal_3 Loss: 0.0664 | 0.0626
Epoch 40/300, seasonal_3 Loss: 0.0660 | 0.0629
Epoch 41/300, seasonal_3 Loss: 0.0657 | 0.0628
Epoch 42/300, seasonal_3 Loss: 0.0646 | 0.0633
Epoch 43/300, seasonal_3 Loss: 0.0640 | 0.0643
Epoch 44/300, seasonal_3 Loss: 0.0628 | 0.0651
Epoch 45/300, seasonal_3 Loss: 0.0617 | 0.0649
Epoch 46/300, seasonal_3 Loss: 0.0596 | 0.0656
Epoch 47/300, seasonal_3 Loss: 0.0586 | 0.0654
Epoch 48/300, seasonal_3 Loss: 0.0578 | 0.0662
Epoch 49/300, seasonal_3 Loss: 0.0572 | 0.0661
Epoch 50/300, seasonal_3 Loss: 0.0566 | 0.0664
Epoch 51/300, seasonal_3 Loss: 0.0561 | 0.0659
Epoch 52/300, seasonal_3 Loss: 0.0558 | 0.0656
Epoch 53/300, seasonal_3 Loss: 0.0555 | 0.0651
Epoch 54/300, seasonal_3 Loss: 0.0552 | 0.0649
Epoch 55/300, seasonal_3 Loss: 0.0550 | 0.0644
Epoch 56/300, seasonal_3 Loss: 0.0547 | 0.0642
Epoch 57/300, seasonal_3 Loss: 0.0546 | 0.0637
Epoch 58/300, seasonal_3 Loss: 0.0544 | 0.0636
Epoch 59/300, seasonal_3 Loss: 0.0542 | 0.0632
Epoch 60/300, seasonal_3 Loss: 0.0541 | 0.0632
Epoch 61/300, seasonal_3 Loss: 0.0540 | 0.0627
Epoch 62/300, seasonal_3 Loss: 0.0539 | 0.0628
Epoch 63/300, seasonal_3 Loss: 0.0537 | 0.0624
Epoch 64/300, seasonal_3 Loss: 0.0536 | 0.0625
Epoch 65/300, seasonal_3 Loss: 0.0535 | 0.0622
Epoch 66/300, seasonal_3 Loss: 0.0535 | 0.0623
Epoch 67/300, seasonal_3 Loss: 0.0534 | 0.0620
Epoch 68/300, seasonal_3 Loss: 0.0533 | 0.0622
Epoch 69/300, seasonal_3 Loss: 0.0532 | 0.0620
Epoch 70/300, seasonal_3 Loss: 0.0531 | 0.0621
Epoch 71/300, seasonal_3 Loss: 0.0531 | 0.0620
Epoch 72/300, seasonal_3 Loss: 0.0530 | 0.0621
Epoch 73/300, seasonal_3 Loss: 0.0530 | 0.0620
Epoch 74/300, seasonal_3 Loss: 0.0529 | 0.0621
Epoch 75/300, seasonal_3 Loss: 0.0528 | 0.0621
Epoch 76/300, seasonal_3 Loss: 0.0528 | 0.0621
Epoch 77/300, seasonal_3 Loss: 0.0527 | 0.0621
Epoch 78/300, seasonal_3 Loss: 0.0527 | 0.0622
Epoch 79/300, seasonal_3 Loss: 0.0527 | 0.0622
Epoch 80/300, seasonal_3 Loss: 0.0526 | 0.0623
Epoch 81/300, seasonal_3 Loss: 0.0526 | 0.0623
Epoch 82/300, seasonal_3 Loss: 0.0526 | 0.0624
Epoch 83/300, seasonal_3 Loss: 0.0525 | 0.0624
Epoch 84/300, seasonal_3 Loss: 0.0525 | 0.0624
Epoch 85/300, seasonal_3 Loss: 0.0525 | 0.0625
Epoch 86/300, seasonal_3 Loss: 0.0525 | 0.0625
Epoch 87/300, seasonal_3 Loss: 0.0524 | 0.0625
Epoch 88/300, seasonal_3 Loss: 0.0524 | 0.0625
Epoch 89/300, seasonal_3 Loss: 0.0524 | 0.0625
Epoch 90/300, seasonal_3 Loss: 0.0524 | 0.0624
Epoch 91/300, seasonal_3 Loss: 0.0524 | 0.0624
Epoch 92/300, seasonal_3 Loss: 0.0523 | 0.0624
Epoch 93/300, seasonal_3 Loss: 0.0523 | 0.0624
Epoch 94/300, seasonal_3 Loss: 0.0523 | 0.0624
Epoch 95/300, seasonal_3 Loss: 0.0523 | 0.0624
Epoch 96/300, seasonal_3 Loss: 0.0523 | 0.0623
Epoch 97/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 98/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 99/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 100/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 101/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 102/300, seasonal_3 Loss: 0.0522 | 0.0623
Epoch 103/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 104/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 105/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 106/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 107/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 108/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 109/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 110/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 111/300, seasonal_3 Loss: 0.0521 | 0.0623
Epoch 112/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 113/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 114/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 115/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 116/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 117/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 118/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 119/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 120/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 121/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 122/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 123/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 124/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 125/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 126/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 127/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 128/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 129/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 130/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 131/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 132/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 133/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 134/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 135/300, seasonal_3 Loss: 0.0520 | 0.0623
Epoch 136/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 137/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 138/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 139/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 140/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 141/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 142/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 143/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 144/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 145/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 146/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 147/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 148/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 149/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 150/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 151/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 152/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 153/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 154/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 155/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 156/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 157/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 158/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 159/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 160/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 161/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 162/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 163/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 164/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 165/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 166/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 167/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 168/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 169/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 170/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 171/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 172/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 173/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 174/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 175/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 176/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 177/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 178/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 179/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 180/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 181/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 182/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 183/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 184/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 185/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 186/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 187/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 188/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 189/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 190/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 191/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 192/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 193/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 194/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 195/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 196/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 197/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 198/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 199/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 200/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 201/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 202/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 203/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 204/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 205/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 206/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 207/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 208/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 209/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 210/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 211/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 212/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 213/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 214/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 215/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 216/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 217/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 218/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 219/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 220/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 221/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 222/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 223/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 224/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 225/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 226/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 227/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 228/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 229/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 230/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 231/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 232/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 233/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 234/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 235/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 236/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 237/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 238/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 239/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 240/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 241/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 242/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 243/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 244/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 245/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 246/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 247/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 248/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 249/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 250/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 251/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 252/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 253/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 254/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 255/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 256/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 257/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 258/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 259/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 260/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 261/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 262/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 263/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 264/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 265/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 266/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 267/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 268/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 269/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 270/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 271/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 272/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 273/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 274/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 275/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 276/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 277/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 278/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 279/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 280/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 281/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 282/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 283/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 284/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 285/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 286/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 287/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 288/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 289/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 290/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 291/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 292/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 293/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 294/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 295/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 296/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 297/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 298/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 299/300, seasonal_3 Loss: 0.0519 | 0.0623
Epoch 300/300, seasonal_3 Loss: 0.0519 | 0.0623
Training resid component with params: {'observation_period_num': 8, 'train_rates': 0.606289375412508, 'learning_rate': 0.0001041801855429689, 'batch_size': 20, 'step_size': 7, 'gamma': 0.9082389833386236}
Epoch 1/300, resid Loss: 0.2264 | 0.1850
Epoch 2/300, resid Loss: 0.1452 | 0.1351
Epoch 3/300, resid Loss: 0.1341 | 0.1363
Epoch 4/300, resid Loss: 0.1275 | 0.1708
Epoch 5/300, resid Loss: 0.1235 | 0.1904
Epoch 6/300, resid Loss: 0.1214 | 0.2013
Epoch 7/300, resid Loss: 0.1200 | 0.1967
Epoch 8/300, resid Loss: 0.1182 | 0.1740
Epoch 9/300, resid Loss: 0.1170 | 0.1611
Epoch 10/300, resid Loss: 0.1155 | 0.1472
Epoch 11/300, resid Loss: 0.1140 | 0.1347
Epoch 12/300, resid Loss: 0.1130 | 0.1147
Epoch 13/300, resid Loss: 0.1111 | 0.1082
Epoch 14/300, resid Loss: 0.1081 | 0.1041
Epoch 15/300, resid Loss: 0.1056 | 0.0996
Epoch 16/300, resid Loss: 0.1028 | 0.0982
Epoch 17/300, resid Loss: 0.1000 | 0.0923
Epoch 18/300, resid Loss: 0.0988 | 0.0855
Epoch 19/300, resid Loss: 0.0966 | 0.0884
Epoch 20/300, resid Loss: 0.0970 | 0.0806
Epoch 21/300, resid Loss: 0.0949 | 0.0875
Epoch 22/300, resid Loss: 0.0960 | 0.0778
Epoch 23/300, resid Loss: 0.0940 | 0.0850
Epoch 24/300, resid Loss: 0.0947 | 0.0765
Epoch 25/300, resid Loss: 0.0928 | 0.0822
Epoch 26/300, resid Loss: 0.0930 | 0.0740
Epoch 27/300, resid Loss: 0.0914 | 0.0792
Epoch 28/300, resid Loss: 0.0918 | 0.0728
Epoch 29/300, resid Loss: 0.0901 | 0.0765
Epoch 30/300, resid Loss: 0.0903 | 0.0705
Epoch 31/300, resid Loss: 0.0890 | 0.0746
Epoch 32/300, resid Loss: 0.0892 | 0.0692
Epoch 33/300, resid Loss: 0.0879 | 0.0727
Epoch 34/300, resid Loss: 0.0880 | 0.0674
Epoch 35/300, resid Loss: 0.0869 | 0.0711
Epoch 36/300, resid Loss: 0.0868 | 0.0660
Epoch 37/300, resid Loss: 0.0859 | 0.0693
Epoch 38/300, resid Loss: 0.0860 | 0.0651
Epoch 39/300, resid Loss: 0.0852 | 0.0685
Epoch 40/300, resid Loss: 0.0852 | 0.0643
Epoch 41/300, resid Loss: 0.0845 | 0.0674
Epoch 42/300, resid Loss: 0.0846 | 0.0638
Epoch 43/300, resid Loss: 0.0839 | 0.0664
Epoch 44/300, resid Loss: 0.0839 | 0.0632
Epoch 45/300, resid Loss: 0.0833 | 0.0658
Epoch 46/300, resid Loss: 0.0834 | 0.0628
Epoch 47/300, resid Loss: 0.0828 | 0.0652
Epoch 48/300, resid Loss: 0.0828 | 0.0624
Epoch 49/300, resid Loss: 0.0824 | 0.0648
Epoch 50/300, resid Loss: 0.0822 | 0.0620
Epoch 51/300, resid Loss: 0.0819 | 0.0641
Epoch 52/300, resid Loss: 0.0818 | 0.0617
Epoch 53/300, resid Loss: 0.0815 | 0.0640
Epoch 54/300, resid Loss: 0.0813 | 0.0614
Epoch 55/300, resid Loss: 0.0810 | 0.0634
Epoch 56/300, resid Loss: 0.0809 | 0.0612
Epoch 57/300, resid Loss: 0.0805 | 0.0630
Epoch 58/300, resid Loss: 0.0804 | 0.0610
Epoch 59/300, resid Loss: 0.0802 | 0.0627
Epoch 60/300, resid Loss: 0.0801 | 0.0608
Epoch 61/300, resid Loss: 0.0798 | 0.0624
Epoch 62/300, resid Loss: 0.0797 | 0.0606
Epoch 63/300, resid Loss: 0.0795 | 0.0622
Epoch 64/300, resid Loss: 0.0793 | 0.0606
Epoch 65/300, resid Loss: 0.0791 | 0.0618
Epoch 66/300, resid Loss: 0.0790 | 0.0604
Epoch 67/300, resid Loss: 0.0788 | 0.0617
Epoch 68/300, resid Loss: 0.0786 | 0.0604
Epoch 69/300, resid Loss: 0.0785 | 0.0614
Epoch 70/300, resid Loss: 0.0784 | 0.0602
Epoch 71/300, resid Loss: 0.0782 | 0.0612
Epoch 72/300, resid Loss: 0.0780 | 0.0602
Epoch 73/300, resid Loss: 0.0779 | 0.0611
Epoch 74/300, resid Loss: 0.0778 | 0.0602
Epoch 75/300, resid Loss: 0.0776 | 0.0609
Epoch 76/300, resid Loss: 0.0775 | 0.0601
Epoch 77/300, resid Loss: 0.0774 | 0.0608
Epoch 78/300, resid Loss: 0.0773 | 0.0601
Epoch 79/300, resid Loss: 0.0771 | 0.0607
Epoch 80/300, resid Loss: 0.0770 | 0.0601
Epoch 81/300, resid Loss: 0.0769 | 0.0606
Epoch 82/300, resid Loss: 0.0768 | 0.0601
Epoch 83/300, resid Loss: 0.0767 | 0.0605
Epoch 84/300, resid Loss: 0.0766 | 0.0600
Epoch 85/300, resid Loss: 0.0765 | 0.0603
Epoch 86/300, resid Loss: 0.0764 | 0.0600
Epoch 87/300, resid Loss: 0.0764 | 0.0603
Epoch 88/300, resid Loss: 0.0763 | 0.0599
Epoch 89/300, resid Loss: 0.0762 | 0.0602
Epoch 90/300, resid Loss: 0.0761 | 0.0599
Epoch 91/300, resid Loss: 0.0760 | 0.0601
Epoch 92/300, resid Loss: 0.0759 | 0.0599
Epoch 93/300, resid Loss: 0.0759 | 0.0601
Epoch 94/300, resid Loss: 0.0758 | 0.0599
Epoch 95/300, resid Loss: 0.0757 | 0.0600
Epoch 96/300, resid Loss: 0.0756 | 0.0599
Epoch 97/300, resid Loss: 0.0756 | 0.0600
Epoch 98/300, resid Loss: 0.0755 | 0.0599
Epoch 99/300, resid Loss: 0.0754 | 0.0600
Epoch 100/300, resid Loss: 0.0754 | 0.0599
Epoch 101/300, resid Loss: 0.0753 | 0.0599
Epoch 102/300, resid Loss: 0.0753 | 0.0599
Epoch 103/300, resid Loss: 0.0752 | 0.0599
Epoch 104/300, resid Loss: 0.0752 | 0.0599
Epoch 105/300, resid Loss: 0.0751 | 0.0599
Epoch 106/300, resid Loss: 0.0751 | 0.0599
Epoch 107/300, resid Loss: 0.0750 | 0.0599
Epoch 108/300, resid Loss: 0.0750 | 0.0599
Epoch 109/300, resid Loss: 0.0749 | 0.0599
Epoch 110/300, resid Loss: 0.0749 | 0.0599
Epoch 111/300, resid Loss: 0.0749 | 0.0599
Epoch 112/300, resid Loss: 0.0748 | 0.0599
Epoch 113/300, resid Loss: 0.0748 | 0.0600
Epoch 114/300, resid Loss: 0.0747 | 0.0600
Epoch 115/300, resid Loss: 0.0747 | 0.0600
Epoch 116/300, resid Loss: 0.0747 | 0.0600
Epoch 117/300, resid Loss: 0.0746 | 0.0600
Epoch 118/300, resid Loss: 0.0746 | 0.0600
Epoch 119/300, resid Loss: 0.0745 | 0.0600
Epoch 120/300, resid Loss: 0.0745 | 0.0600
Epoch 121/300, resid Loss: 0.0745 | 0.0600
Epoch 122/300, resid Loss: 0.0744 | 0.0600
Epoch 123/300, resid Loss: 0.0744 | 0.0600
Epoch 124/300, resid Loss: 0.0744 | 0.0600
Epoch 125/300, resid Loss: 0.0743 | 0.0599
Epoch 126/300, resid Loss: 0.0743 | 0.0599
Epoch 127/300, resid Loss: 0.0743 | 0.0599
Epoch 128/300, resid Loss: 0.0743 | 0.0599
Epoch 129/300, resid Loss: 0.0742 | 0.0599
Epoch 130/300, resid Loss: 0.0742 | 0.0599
Epoch 131/300, resid Loss: 0.0742 | 0.0598
Epoch 132/300, resid Loss: 0.0742 | 0.0598
Epoch 133/300, resid Loss: 0.0741 | 0.0598
Epoch 134/300, resid Loss: 0.0741 | 0.0598
Epoch 135/300, resid Loss: 0.0741 | 0.0598
Epoch 136/300, resid Loss: 0.0741 | 0.0598
Epoch 137/300, resid Loss: 0.0741 | 0.0598
Epoch 138/300, resid Loss: 0.0740 | 0.0597
Epoch 139/300, resid Loss: 0.0740 | 0.0597
Epoch 140/300, resid Loss: 0.0740 | 0.0597
Epoch 141/300, resid Loss: 0.0740 | 0.0597
Epoch 142/300, resid Loss: 0.0740 | 0.0597
Epoch 143/300, resid Loss: 0.0739 | 0.0597
Epoch 144/300, resid Loss: 0.0739 | 0.0597
Epoch 145/300, resid Loss: 0.0739 | 0.0596
Epoch 146/300, resid Loss: 0.0739 | 0.0596
Epoch 147/300, resid Loss: 0.0739 | 0.0596
Epoch 148/300, resid Loss: 0.0739 | 0.0596
Epoch 149/300, resid Loss: 0.0738 | 0.0596
Epoch 150/300, resid Loss: 0.0738 | 0.0596
Epoch 151/300, resid Loss: 0.0738 | 0.0596
Epoch 152/300, resid Loss: 0.0738 | 0.0596
Epoch 153/300, resid Loss: 0.0738 | 0.0596
Epoch 154/300, resid Loss: 0.0738 | 0.0596
Epoch 155/300, resid Loss: 0.0737 | 0.0596
Epoch 156/300, resid Loss: 0.0737 | 0.0596
Epoch 157/300, resid Loss: 0.0737 | 0.0596
Epoch 158/300, resid Loss: 0.0737 | 0.0596
Epoch 159/300, resid Loss: 0.0737 | 0.0596
Epoch 160/300, resid Loss: 0.0737 | 0.0596
Epoch 161/300, resid Loss: 0.0737 | 0.0596
Epoch 162/300, resid Loss: 0.0736 | 0.0596
Epoch 163/300, resid Loss: 0.0736 | 0.0596
Epoch 164/300, resid Loss: 0.0736 | 0.0596
Epoch 165/300, resid Loss: 0.0736 | 0.0596
Epoch 166/300, resid Loss: 0.0736 | 0.0596
Epoch 167/300, resid Loss: 0.0736 | 0.0596
Epoch 168/300, resid Loss: 0.0736 | 0.0596
Epoch 169/300, resid Loss: 0.0736 | 0.0596
Epoch 170/300, resid Loss: 0.0735 | 0.0596
Epoch 171/300, resid Loss: 0.0735 | 0.0596
Epoch 172/300, resid Loss: 0.0735 | 0.0596
Epoch 173/300, resid Loss: 0.0735 | 0.0596
Epoch 174/300, resid Loss: 0.0735 | 0.0596
Epoch 175/300, resid Loss: 0.0735 | 0.0596
Epoch 176/300, resid Loss: 0.0735 | 0.0596
Epoch 177/300, resid Loss: 0.0735 | 0.0596
Epoch 178/300, resid Loss: 0.0735 | 0.0596
Epoch 179/300, resid Loss: 0.0735 | 0.0596
Epoch 180/300, resid Loss: 0.0735 | 0.0596
Epoch 181/300, resid Loss: 0.0735 | 0.0596
Epoch 182/300, resid Loss: 0.0735 | 0.0596
Epoch 183/300, resid Loss: 0.0734 | 0.0596
Epoch 184/300, resid Loss: 0.0734 | 0.0596
Epoch 185/300, resid Loss: 0.0734 | 0.0596
Epoch 186/300, resid Loss: 0.0734 | 0.0596
Epoch 187/300, resid Loss: 0.0734 | 0.0596
Epoch 188/300, resid Loss: 0.0734 | 0.0596
Epoch 189/300, resid Loss: 0.0734 | 0.0596
Epoch 190/300, resid Loss: 0.0734 | 0.0596
Epoch 191/300, resid Loss: 0.0734 | 0.0596
Epoch 192/300, resid Loss: 0.0734 | 0.0596
Epoch 193/300, resid Loss: 0.0734 | 0.0596
Epoch 194/300, resid Loss: 0.0734 | 0.0596
Epoch 195/300, resid Loss: 0.0734 | 0.0596
Epoch 196/300, resid Loss: 0.0734 | 0.0596
Epoch 197/300, resid Loss: 0.0734 | 0.0596
Epoch 198/300, resid Loss: 0.0734 | 0.0596
Epoch 199/300, resid Loss: 0.0734 | 0.0596
Epoch 200/300, resid Loss: 0.0734 | 0.0596
Epoch 201/300, resid Loss: 0.0734 | 0.0596
Epoch 202/300, resid Loss: 0.0734 | 0.0596
Epoch 203/300, resid Loss: 0.0734 | 0.0596
Epoch 204/300, resid Loss: 0.0733 | 0.0596
Epoch 205/300, resid Loss: 0.0733 | 0.0596
Epoch 206/300, resid Loss: 0.0733 | 0.0596
Epoch 207/300, resid Loss: 0.0733 | 0.0596
Epoch 208/300, resid Loss: 0.0733 | 0.0596
Epoch 209/300, resid Loss: 0.0733 | 0.0596
Epoch 210/300, resid Loss: 0.0733 | 0.0596
Epoch 211/300, resid Loss: 0.0733 | 0.0596
Epoch 212/300, resid Loss: 0.0733 | 0.0596
Epoch 213/300, resid Loss: 0.0733 | 0.0596
Epoch 214/300, resid Loss: 0.0733 | 0.0596
Epoch 215/300, resid Loss: 0.0733 | 0.0596
Epoch 216/300, resid Loss: 0.0733 | 0.0596
Epoch 217/300, resid Loss: 0.0733 | 0.0596
Epoch 218/300, resid Loss: 0.0733 | 0.0596
Epoch 219/300, resid Loss: 0.0733 | 0.0596
Epoch 220/300, resid Loss: 0.0733 | 0.0596
Epoch 221/300, resid Loss: 0.0733 | 0.0596
Epoch 222/300, resid Loss: 0.0733 | 0.0596
Epoch 223/300, resid Loss: 0.0733 | 0.0596
Epoch 224/300, resid Loss: 0.0733 | 0.0596
Epoch 225/300, resid Loss: 0.0733 | 0.0596
Epoch 226/300, resid Loss: 0.0733 | 0.0596
Epoch 227/300, resid Loss: 0.0733 | 0.0596
Epoch 228/300, resid Loss: 0.0733 | 0.0596
Epoch 229/300, resid Loss: 0.0733 | 0.0596
Epoch 230/300, resid Loss: 0.0733 | 0.0596
Epoch 231/300, resid Loss: 0.0733 | 0.0596
Epoch 232/300, resid Loss: 0.0733 | 0.0596
Epoch 233/300, resid Loss: 0.0733 | 0.0596
Epoch 234/300, resid Loss: 0.0733 | 0.0596
Epoch 235/300, resid Loss: 0.0733 | 0.0596
Epoch 236/300, resid Loss: 0.0733 | 0.0596
Epoch 237/300, resid Loss: 0.0733 | 0.0596
Epoch 238/300, resid Loss: 0.0733 | 0.0596
Epoch 239/300, resid Loss: 0.0733 | 0.0596
Epoch 240/300, resid Loss: 0.0733 | 0.0596
Epoch 241/300, resid Loss: 0.0733 | 0.0596
Epoch 242/300, resid Loss: 0.0733 | 0.0596
Epoch 243/300, resid Loss: 0.0733 | 0.0596
Epoch 244/300, resid Loss: 0.0733 | 0.0596
Epoch 245/300, resid Loss: 0.0733 | 0.0596
Epoch 246/300, resid Loss: 0.0733 | 0.0596
Epoch 247/300, resid Loss: 0.0733 | 0.0596
Epoch 248/300, resid Loss: 0.0733 | 0.0596
Epoch 249/300, resid Loss: 0.0733 | 0.0596
Epoch 250/300, resid Loss: 0.0733 | 0.0596
Epoch 251/300, resid Loss: 0.0733 | 0.0596
Epoch 252/300, resid Loss: 0.0733 | 0.0596
Epoch 253/300, resid Loss: 0.0733 | 0.0596
Epoch 254/300, resid Loss: 0.0733 | 0.0596
Epoch 255/300, resid Loss: 0.0733 | 0.0596
Epoch 256/300, resid Loss: 0.0733 | 0.0596
Epoch 257/300, resid Loss: 0.0733 | 0.0596
Epoch 258/300, resid Loss: 0.0732 | 0.0596
Epoch 259/300, resid Loss: 0.0732 | 0.0596
Epoch 260/300, resid Loss: 0.0732 | 0.0596
Epoch 261/300, resid Loss: 0.0732 | 0.0596
Epoch 262/300, resid Loss: 0.0732 | 0.0596
Epoch 263/300, resid Loss: 0.0732 | 0.0596
Epoch 264/300, resid Loss: 0.0732 | 0.0596
Epoch 265/300, resid Loss: 0.0732 | 0.0596
Epoch 266/300, resid Loss: 0.0732 | 0.0596
Epoch 267/300, resid Loss: 0.0732 | 0.0596
Epoch 268/300, resid Loss: 0.0732 | 0.0596
Epoch 269/300, resid Loss: 0.0732 | 0.0596
Epoch 270/300, resid Loss: 0.0732 | 0.0596
Epoch 271/300, resid Loss: 0.0732 | 0.0596
Epoch 272/300, resid Loss: 0.0732 | 0.0596
Epoch 273/300, resid Loss: 0.0732 | 0.0596
Epoch 274/300, resid Loss: 0.0732 | 0.0596
Epoch 275/300, resid Loss: 0.0732 | 0.0596
Epoch 276/300, resid Loss: 0.0732 | 0.0596
Epoch 277/300, resid Loss: 0.0732 | 0.0596
Epoch 278/300, resid Loss: 0.0732 | 0.0596
Epoch 279/300, resid Loss: 0.0732 | 0.0596
Epoch 280/300, resid Loss: 0.0732 | 0.0596
Epoch 281/300, resid Loss: 0.0732 | 0.0596
Epoch 282/300, resid Loss: 0.0732 | 0.0596
Epoch 283/300, resid Loss: 0.0732 | 0.0596
Epoch 284/300, resid Loss: 0.0732 | 0.0596
Epoch 285/300, resid Loss: 0.0732 | 0.0596
Epoch 286/300, resid Loss: 0.0732 | 0.0596
Epoch 287/300, resid Loss: 0.0732 | 0.0596
Epoch 288/300, resid Loss: 0.0732 | 0.0596
Epoch 289/300, resid Loss: 0.0732 | 0.0596
Epoch 290/300, resid Loss: 0.0732 | 0.0596
Epoch 291/300, resid Loss: 0.0732 | 0.0596
Epoch 292/300, resid Loss: 0.0732 | 0.0596
Epoch 293/300, resid Loss: 0.0732 | 0.0596
Epoch 294/300, resid Loss: 0.0732 | 0.0596
Epoch 295/300, resid Loss: 0.0732 | 0.0596
Epoch 296/300, resid Loss: 0.0732 | 0.0596
Epoch 297/300, resid Loss: 0.0732 | 0.0596
Epoch 298/300, resid Loss: 0.0732 | 0.0596
Epoch 299/300, resid Loss: 0.0732 | 0.0596
Epoch 300/300, resid Loss: 0.0732 | 0.0596
Runtime (seconds): 2063.925815820694
0.0003013278996634321
[103.20438]
[5.281662]
[-0.615773]
[0.53118855]
[-3.6691215]
[-1.9047341]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1.5815549353137612
RMSE: 1.257598876953125
MAE: 1.257598876953125
R-squared: nan
[102.8276]
