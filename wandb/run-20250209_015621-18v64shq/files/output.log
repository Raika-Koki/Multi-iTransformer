ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-09 01:56:26,156][0m A new study created in memory with name: no-name-d0ceb7e7-88e1-451b-96a1-8ad0b5b2f12a[0m
[32m[I 2025-02-09 01:56:47,169][0m Trial 0 finished with value: 1.1310927296842697 and parameters: {'observation_period_num': 5, 'train_rates': 0.8466896547629706, 'learning_rate': 4.323039007615716e-06, 'batch_size': 235, 'step_size': 11, 'gamma': 0.9749473556762327}. Best is trial 0 with value: 1.1310927296842697.[0m
[32m[I 2025-02-09 01:59:27,064][0m Trial 1 finished with value: 1.0504261778481903 and parameters: {'observation_period_num': 89, 'train_rates': 0.7832706491060261, 'learning_rate': 0.0006589192090831499, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8785838546409506}. Best is trial 1 with value: 1.0504261778481903.[0m
[32m[I 2025-02-09 02:01:13,216][0m Trial 2 finished with value: 0.9817639650350594 and parameters: {'observation_period_num': 119, 'train_rates': 0.6492714774233428, 'learning_rate': 1.1648363075150863e-05, 'batch_size': 71, 'step_size': 11, 'gamma': 0.9718641045311068}. Best is trial 2 with value: 0.9817639650350594.[0m
[32m[I 2025-02-09 02:01:36,930][0m Trial 3 finished with value: 0.960579973096433 and parameters: {'observation_period_num': 19, 'train_rates': 0.6087066402728609, 'learning_rate': 0.00010109506805322945, 'batch_size': 176, 'step_size': 8, 'gamma': 0.960117599183969}. Best is trial 3 with value: 0.960579973096433.[0m
[32m[I 2025-02-09 02:02:07,004][0m Trial 4 finished with value: 0.46683448456689675 and parameters: {'observation_period_num': 25, 'train_rates': 0.8735278514083257, 'learning_rate': 0.0008343751587875162, 'batch_size': 190, 'step_size': 14, 'gamma': 0.9353763542122302}. Best is trial 4 with value: 0.46683448456689675.[0m
[32m[I 2025-02-09 02:06:41,988][0m Trial 5 finished with value: 0.4382174015045166 and parameters: {'observation_period_num': 225, 'train_rates': 0.9222280310237281, 'learning_rate': 0.0003104265794957499, 'batch_size': 250, 'step_size': 2, 'gamma': 0.8721310114426518}. Best is trial 5 with value: 0.4382174015045166.[0m
[32m[I 2025-02-09 02:08:52,701][0m Trial 6 finished with value: 0.9711316470017723 and parameters: {'observation_period_num': 152, 'train_rates': 0.660076999602915, 'learning_rate': 0.0001114661368304315, 'batch_size': 161, 'step_size': 8, 'gamma': 0.7873819050765638}. Best is trial 5 with value: 0.4382174015045166.[0m
[32m[I 2025-02-09 02:09:34,568][0m Trial 7 finished with value: 0.4302339310522471 and parameters: {'observation_period_num': 42, 'train_rates': 0.8412165365117381, 'learning_rate': 5.844898425111651e-05, 'batch_size': 160, 'step_size': 12, 'gamma': 0.8144862973497351}. Best is trial 7 with value: 0.4302339310522471.[0m
Early stopping at epoch 57
[32m[I 2025-02-09 02:10:30,118][0m Trial 8 finished with value: 2.6014110675824234 and parameters: {'observation_period_num': 84, 'train_rates': 0.893578579119565, 'learning_rate': 1.0616861900234412e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.8330829976448385}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:13:37,810][0m Trial 9 finished with value: 0.828753356264109 and parameters: {'observation_period_num': 173, 'train_rates': 0.8749429871323089, 'learning_rate': 1.5077934164961741e-05, 'batch_size': 222, 'step_size': 10, 'gamma': 0.8865957794806418}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:14:47,024][0m Trial 10 finished with value: 0.4803754687309265 and parameters: {'observation_period_num': 62, 'train_rates': 0.9881060131887813, 'learning_rate': 6.67984631304387e-05, 'batch_size': 118, 'step_size': 5, 'gamma': 0.7536259971134983}. Best is trial 7 with value: 0.4302339310522471.[0m
Early stopping at epoch 66
[32m[I 2025-02-09 02:18:27,429][0m Trial 11 finished with value: 0.8235514760017395 and parameters: {'observation_period_num': 251, 'train_rates': 0.9714457174339786, 'learning_rate': 0.0002939463965875058, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8275404333775181}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:22:35,063][0m Trial 12 finished with value: 0.7545181566251898 and parameters: {'observation_period_num': 227, 'train_rates': 0.7670646136324604, 'learning_rate': 0.0001966218129949811, 'batch_size': 126, 'step_size': 4, 'gamma': 0.8356656102807257}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:26:27,220][0m Trial 13 finished with value: 0.6568971872329712 and parameters: {'observation_period_num': 195, 'train_rates': 0.9322442917939485, 'learning_rate': 3.949232941255462e-05, 'batch_size': 206, 'step_size': 4, 'gamma': 0.9128045775159683}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:28:37,065][0m Trial 14 finished with value: 0.4841232404921406 and parameters: {'observation_period_num': 130, 'train_rates': 0.8175793817671643, 'learning_rate': 0.0003090637811876394, 'batch_size': 145, 'step_size': 6, 'gamma': 0.7979271989812787}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:32:04,246][0m Trial 15 finished with value: 0.8942549441659879 and parameters: {'observation_period_num': 197, 'train_rates': 0.7387163031806924, 'learning_rate': 2.3886889769514955e-05, 'batch_size': 92, 'step_size': 13, 'gamma': 0.8570913488549309}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:32:57,690][0m Trial 16 finished with value: 1.4080123901367188 and parameters: {'observation_period_num': 55, 'train_rates': 0.9228358407101056, 'learning_rate': 4.690721457054121e-06, 'batch_size': 254, 'step_size': 12, 'gamma': 0.7962825065902441}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:34:36,153][0m Trial 17 finished with value: 0.8817147407607128 and parameters: {'observation_period_num': 112, 'train_rates': 0.7334583486255514, 'learning_rate': 4.209884099271011e-05, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8992564024217867}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:37:16,518][0m Trial 18 finished with value: 0.523346995934844 and parameters: {'observation_period_num': 155, 'train_rates': 0.8173425601050005, 'learning_rate': 0.0004186846999050967, 'batch_size': 161, 'step_size': 6, 'gamma': 0.8539847408512967}. Best is trial 7 with value: 0.4302339310522471.[0m
Early stopping at epoch 95
[32m[I 2025-02-09 02:41:35,003][0m Trial 19 finished with value: 0.6144787322033892 and parameters: {'observation_period_num': 224, 'train_rates': 0.933489952814172, 'learning_rate': 0.00017699896078263608, 'batch_size': 102, 'step_size': 2, 'gamma': 0.7730275658683935}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:43:08,069][0m Trial 20 finished with value: 0.5180334500920798 and parameters: {'observation_period_num': 51, 'train_rates': 0.8388919023688082, 'learning_rate': 9.274604907213852e-05, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8090200399207018}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:43:43,321][0m Trial 21 finished with value: 0.5390191645198702 and parameters: {'observation_period_num': 34, 'train_rates': 0.8809368309419321, 'learning_rate': 0.0009693289955673104, 'batch_size': 186, 'step_size': 15, 'gamma': 0.9340999662828441}. Best is trial 7 with value: 0.4302339310522471.[0m
[32m[I 2025-02-09 02:45:09,174][0m Trial 22 finished with value: 0.2924305712063007 and parameters: {'observation_period_num': 84, 'train_rates': 0.8997130220730127, 'learning_rate': 0.0005397175433040943, 'batch_size': 196, 'step_size': 13, 'gamma': 0.9346827790174236}. Best is trial 22 with value: 0.2924305712063007.[0m
[32m[I 2025-02-09 02:46:36,837][0m Trial 23 finished with value: 0.2551841735839844 and parameters: {'observation_period_num': 83, 'train_rates': 0.9533508888782088, 'learning_rate': 0.0005714968833790498, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9225869176115977}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:48:11,415][0m Trial 24 finished with value: 0.32371556758880615 and parameters: {'observation_period_num': 89, 'train_rates': 0.9650640539864584, 'learning_rate': 0.000558187507666346, 'batch_size': 221, 'step_size': 13, 'gamma': 0.9239259275250513}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:49:39,629][0m Trial 25 finished with value: 0.3803543150424957 and parameters: {'observation_period_num': 84, 'train_rates': 0.9633779235002472, 'learning_rate': 0.0004935711012741537, 'batch_size': 222, 'step_size': 13, 'gamma': 0.9383316496999269}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:51:21,615][0m Trial 26 finished with value: 0.264804482460022 and parameters: {'observation_period_num': 97, 'train_rates': 0.9610193418069433, 'learning_rate': 0.00020740311924136368, 'batch_size': 222, 'step_size': 14, 'gamma': 0.9141330526948003}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:53:05,899][0m Trial 27 finished with value: 0.2872139834931919 and parameters: {'observation_period_num': 104, 'train_rates': 0.9019528317172648, 'learning_rate': 0.00014872766975769475, 'batch_size': 198, 'step_size': 15, 'gamma': 0.9044121458422383}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:54:55,609][0m Trial 28 finished with value: 0.27923622727394104 and parameters: {'observation_period_num': 106, 'train_rates': 0.9528115046860991, 'learning_rate': 0.00016913940095341926, 'batch_size': 236, 'step_size': 15, 'gamma': 0.9010115323524339}. Best is trial 23 with value: 0.2551841735839844.[0m
[32m[I 2025-02-09 02:57:37,965][0m Trial 29 finished with value: 0.18948552012443542 and parameters: {'observation_period_num': 140, 'train_rates': 0.9894055225151823, 'learning_rate': 0.00023173969253406835, 'batch_size': 239, 'step_size': 11, 'gamma': 0.8921133323296396}. Best is trial 29 with value: 0.18948552012443542.[0m
[32m[I 2025-02-09 03:00:08,564][0m Trial 30 finished with value: 0.19807420670986176 and parameters: {'observation_period_num': 136, 'train_rates': 0.9862629865244779, 'learning_rate': 0.00024141214550074246, 'batch_size': 239, 'step_size': 11, 'gamma': 0.9528162648022881}. Best is trial 29 with value: 0.18948552012443542.[0m
[32m[I 2025-02-09 03:02:42,627][0m Trial 31 finished with value: 0.17354221642017365 and parameters: {'observation_period_num': 134, 'train_rates': 0.9881337077688087, 'learning_rate': 0.00025639651373930055, 'batch_size': 235, 'step_size': 11, 'gamma': 0.987920255495902}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:05:24,433][0m Trial 32 finished with value: 0.2264750897884369 and parameters: {'observation_period_num': 140, 'train_rates': 0.985892366212356, 'learning_rate': 0.00035711647588665, 'batch_size': 236, 'step_size': 10, 'gamma': 0.9557393519845198}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:08:07,231][0m Trial 33 finished with value: 0.19491839408874512 and parameters: {'observation_period_num': 142, 'train_rates': 0.9821785815400152, 'learning_rate': 0.00031511201697767284, 'batch_size': 240, 'step_size': 10, 'gamma': 0.9887328123396324}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:11:22,496][0m Trial 34 finished with value: 0.21591544151306152 and parameters: {'observation_period_num': 167, 'train_rates': 0.9842262662108554, 'learning_rate': 7.669636274287469e-05, 'batch_size': 241, 'step_size': 11, 'gamma': 0.9884862723608427}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:13:32,924][0m Trial 35 finished with value: 0.29352980852127075 and parameters: {'observation_period_num': 125, 'train_rates': 0.934284739491127, 'learning_rate': 0.000124572080223651, 'batch_size': 211, 'step_size': 10, 'gamma': 0.9871437629972347}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:16:25,418][0m Trial 36 finished with value: 0.1912584900856018 and parameters: {'observation_period_num': 147, 'train_rates': 0.9886661838573235, 'learning_rate': 0.00025651258612053587, 'batch_size': 180, 'step_size': 11, 'gamma': 0.9698079700698103}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:19:49,468][0m Trial 37 finished with value: 0.443036288022995 and parameters: {'observation_period_num': 180, 'train_rates': 0.9445692973497518, 'learning_rate': 0.0007647264534965201, 'batch_size': 182, 'step_size': 9, 'gamma': 0.9710389365328917}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:22:37,718][0m Trial 38 finished with value: 0.27404505239119603 and parameters: {'observation_period_num': 151, 'train_rates': 0.9102417970696403, 'learning_rate': 0.00012194178956972197, 'batch_size': 211, 'step_size': 9, 'gamma': 0.9783935042963273}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:24:44,956][0m Trial 39 finished with value: 0.24822895228862762 and parameters: {'observation_period_num': 120, 'train_rates': 0.9687371064787355, 'learning_rate': 0.00025710006988382335, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9667777024924306}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:26:58,572][0m Trial 40 finished with value: 0.9566986503205996 and parameters: {'observation_period_num': 145, 'train_rates': 0.691044341648928, 'learning_rate': 0.000402887668960873, 'batch_size': 246, 'step_size': 12, 'gamma': 0.9514313698402719}. Best is trial 31 with value: 0.17354221642017365.[0m
[32m[I 2025-02-09 03:29:34,604][0m Trial 41 finished with value: 0.15908633172512054 and parameters: {'observation_period_num': 134, 'train_rates': 0.9882769596559033, 'learning_rate': 0.00024035196656608756, 'batch_size': 232, 'step_size': 11, 'gamma': 0.9519385147072839}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:32:41,012][0m Trial 42 finished with value: 1.2287836074829102 and parameters: {'observation_period_num': 159, 'train_rates': 0.9887440628190372, 'learning_rate': 1.2903451256472444e-06, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9791023078140589}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:36:12,976][0m Trial 43 finished with value: 0.4205362796783447 and parameters: {'observation_period_num': 184, 'train_rates': 0.9434342817171706, 'learning_rate': 5.271319092871503e-05, 'batch_size': 247, 'step_size': 10, 'gamma': 0.9640157784266301}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:38:38,511][0m Trial 44 finished with value: 0.43648457527160645 and parameters: {'observation_period_num': 134, 'train_rates': 0.9142663026999659, 'learning_rate': 0.0007495481218844041, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9762408646601355}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:42:52,666][0m Trial 45 finished with value: 0.21524226998581605 and parameters: {'observation_period_num': 165, 'train_rates': 0.9698846361960317, 'learning_rate': 0.000250757743787912, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9449040477925077}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:44:30,603][0m Trial 46 finished with value: 0.8881096362264405 and parameters: {'observation_period_num': 120, 'train_rates': 0.621041256253407, 'learning_rate': 7.91553732803301e-05, 'batch_size': 175, 'step_size': 9, 'gamma': 0.9887450895282283}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:46:55,759][0m Trial 47 finished with value: 0.38127667729626125 and parameters: {'observation_period_num': 144, 'train_rates': 0.8554250866926238, 'learning_rate': 0.00013623207587867532, 'batch_size': 211, 'step_size': 11, 'gamma': 0.9628870584507548}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:50:52,489][0m Trial 48 finished with value: 0.5207267144640673 and parameters: {'observation_period_num': 197, 'train_rates': 0.9255939245659818, 'learning_rate': 2.2776964108860367e-05, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8839482667488229}. Best is trial 41 with value: 0.15908633172512054.[0m
[32m[I 2025-02-09 03:53:21,129][0m Trial 49 finished with value: 0.9363130927085876 and parameters: {'observation_period_num': 129, 'train_rates': 0.9739994539190195, 'learning_rate': 7.2889132268558236e-06, 'batch_size': 234, 'step_size': 10, 'gamma': 0.9475275799896046}. Best is trial 41 with value: 0.15908633172512054.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-09 03:53:21,136][0m A new study created in memory with name: no-name-0737abd0-90de-475b-b420-f01305f8eb27[0m
[32m[I 2025-02-09 03:54:18,231][0m Trial 0 finished with value: 1.7814512271855707 and parameters: {'observation_period_num': 34, 'train_rates': 0.7431952436061819, 'learning_rate': 4.468086724536427e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.9367742290534774}. Best is trial 0 with value: 1.7814512271855707.[0m
[32m[I 2025-02-09 03:55:08,888][0m Trial 1 finished with value: 1.077997593388605 and parameters: {'observation_period_num': 54, 'train_rates': 0.7929601804817572, 'learning_rate': 1.747277593058299e-05, 'batch_size': 174, 'step_size': 11, 'gamma': 0.7754514457960905}. Best is trial 1 with value: 1.077997593388605.[0m
[32m[I 2025-02-09 03:55:51,705][0m Trial 2 finished with value: 1.738855755606363 and parameters: {'observation_period_num': 53, 'train_rates': 0.629858179534554, 'learning_rate': 1.500214599116402e-06, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8664226522182075}. Best is trial 1 with value: 1.077997593388605.[0m
[32m[I 2025-02-09 03:58:09,004][0m Trial 3 finished with value: 0.2588804727015288 and parameters: {'observation_period_num': 82, 'train_rates': 0.9440445399618386, 'learning_rate': 5.331955069790449e-05, 'batch_size': 35, 'step_size': 13, 'gamma': 0.9270615735919465}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 03:59:21,772][0m Trial 4 finished with value: 1.2376240661083642 and parameters: {'observation_period_num': 58, 'train_rates': 0.6340049348347321, 'learning_rate': 7.0493114519545305e-06, 'batch_size': 51, 'step_size': 15, 'gamma': 0.7630272098458677}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:00:10,593][0m Trial 5 finished with value: 2.123006718261804 and parameters: {'observation_period_num': 6, 'train_rates': 0.7571472640247902, 'learning_rate': 1.8143422850382705e-06, 'batch_size': 86, 'step_size': 13, 'gamma': 0.779093851181965}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:04:15,516][0m Trial 6 finished with value: 0.9580642612356889 and parameters: {'observation_period_num': 242, 'train_rates': 0.6854457408289849, 'learning_rate': 6.740466526107769e-05, 'batch_size': 240, 'step_size': 7, 'gamma': 0.8174598119297569}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:07:30,749][0m Trial 7 finished with value: 0.7608796965800636 and parameters: {'observation_period_num': 189, 'train_rates': 0.796605067500243, 'learning_rate': 0.0001192046013461762, 'batch_size': 208, 'step_size': 3, 'gamma': 0.8708940925391327}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:11:28,570][0m Trial 8 finished with value: 0.9176178026101728 and parameters: {'observation_period_num': 243, 'train_rates': 0.6402285678715406, 'learning_rate': 0.0001888700806256484, 'batch_size': 186, 'step_size': 2, 'gamma': 0.9341082848303837}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:12:21,913][0m Trial 9 finished with value: 1.321157527335903 and parameters: {'observation_period_num': 56, 'train_rates': 0.842935697723546, 'learning_rate': 5.465246699633674e-06, 'batch_size': 137, 'step_size': 9, 'gamma': 0.7532187563724027}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:17:12,242][0m Trial 10 finished with value: 2.078096728171072 and parameters: {'observation_period_num': 137, 'train_rates': 0.9891084971784234, 'learning_rate': 0.0007274555383350676, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9893176273774807}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:20:04,487][0m Trial 11 finished with value: 0.4464778263592026 and parameters: {'observation_period_num': 159, 'train_rates': 0.8897677744071406, 'learning_rate': 0.00010162886911745003, 'batch_size': 250, 'step_size': 6, 'gamma': 0.8862335746501792}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:22:04,282][0m Trial 12 finished with value: 0.4824924549359954 and parameters: {'observation_period_num': 116, 'train_rates': 0.9375127663697616, 'learning_rate': 3.604913695124682e-05, 'batch_size': 118, 'step_size': 7, 'gamma': 0.9013044499105615}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:23:58,127][0m Trial 13 finished with value: 0.27102497682319615 and parameters: {'observation_period_num': 113, 'train_rates': 0.8936199909716951, 'learning_rate': 0.00033973958617303007, 'batch_size': 253, 'step_size': 5, 'gamma': 0.9537561755635998}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:25:40,305][0m Trial 14 finished with value: 0.2953937943019564 and parameters: {'observation_period_num': 102, 'train_rates': 0.9120069971862431, 'learning_rate': 0.0005771219179154224, 'batch_size': 148, 'step_size': 10, 'gamma': 0.980907087860061}. Best is trial 3 with value: 0.2588804727015288.[0m
[32m[I 2025-02-09 04:29:33,553][0m Trial 15 finished with value: 0.19684833139181138 and parameters: {'observation_period_num': 94, 'train_rates': 0.9791529710415884, 'learning_rate': 0.0003009839014056854, 'batch_size': 21, 'step_size': 5, 'gamma': 0.9476707846185006}. Best is trial 15 with value: 0.19684833139181138.[0m
[32m[I 2025-02-09 04:34:06,342][0m Trial 16 finished with value: 0.15959751725196838 and parameters: {'observation_period_num': 96, 'train_rates': 0.9828693362809758, 'learning_rate': 2.9992459587197963e-05, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9159834108686529}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:37:32,143][0m Trial 17 finished with value: 0.4037306010723114 and parameters: {'observation_period_num': 169, 'train_rates': 0.987415867966105, 'learning_rate': 1.5195173853129507e-05, 'batch_size': 98, 'step_size': 12, 'gamma': 0.9092947167689951}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:39:10,172][0m Trial 18 finished with value: 0.37231726197751014 and parameters: {'observation_period_num': 88, 'train_rates': 0.8415330686247713, 'learning_rate': 0.00026330755638343493, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8441913481920388}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:44:07,951][0m Trial 19 finished with value: 0.2601118672352571 and parameters: {'observation_period_num': 151, 'train_rates': 0.9630686466588536, 'learning_rate': 2.9701747987769856e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9655225909797942}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:48:10,611][0m Trial 20 finished with value: 0.5377397933018901 and parameters: {'observation_period_num': 205, 'train_rates': 0.8628009639912357, 'learning_rate': 1.5289816762035323e-05, 'batch_size': 62, 'step_size': 5, 'gamma': 0.9568203139148119}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:50:26,351][0m Trial 21 finished with value: 0.2785203390651279 and parameters: {'observation_period_num': 93, 'train_rates': 0.9437718564743666, 'learning_rate': 4.383950575538568e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.9236178906275345}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:52:40,507][0m Trial 22 finished with value: 0.25336150446934486 and parameters: {'observation_period_num': 81, 'train_rates': 0.9304325260156245, 'learning_rate': 7.19250132628618e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.9105731811432388}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:54:48,884][0m Trial 23 finished with value: 0.3064323487218286 and parameters: {'observation_period_num': 74, 'train_rates': 0.9155627326745336, 'learning_rate': 0.00013320337893104925, 'batch_size': 37, 'step_size': 11, 'gamma': 0.9002393099219278}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:57:18,067][0m Trial 24 finished with value: 0.29940828680992126 and parameters: {'observation_period_num': 130, 'train_rates': 0.9687842507035612, 'learning_rate': 0.0003550808925910512, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8794491430133814}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 04:58:24,519][0m Trial 25 finished with value: 0.2758803197926882 and parameters: {'observation_period_num': 23, 'train_rates': 0.9288670615143897, 'learning_rate': 7.040172595603391e-05, 'batch_size': 74, 'step_size': 14, 'gamma': 0.8443861817424072}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:02:44,221][0m Trial 26 finished with value: 0.3184152820338942 and parameters: {'observation_period_num': 112, 'train_rates': 0.8717181345164099, 'learning_rate': 2.1582467438389788e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9454193572365465}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:04:42,739][0m Trial 27 finished with value: 2.3051808986467184 and parameters: {'observation_period_num': 72, 'train_rates': 0.9665840254012923, 'learning_rate': 0.0009599467548766619, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9106687228780372}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:07:23,998][0m Trial 28 finished with value: 0.46525904536247253 and parameters: {'observation_period_num': 136, 'train_rates': 0.989963833285077, 'learning_rate': 9.939785242183963e-06, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9659014344671216}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:08:16,345][0m Trial 29 finished with value: 0.8015691385967351 and parameters: {'observation_period_num': 32, 'train_rates': 0.7087891504236749, 'learning_rate': 0.00019923263911557218, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9220109308661859}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:10:48,581][0m Trial 30 finished with value: 0.400563202121041 and parameters: {'observation_period_num': 69, 'train_rates': 0.8134828019514487, 'learning_rate': 8.384776022496703e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9388265686934445}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:12:39,625][0m Trial 31 finished with value: 0.2867539094327247 and parameters: {'observation_period_num': 89, 'train_rates': 0.9492160191047558, 'learning_rate': 5.0354691750013356e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9330135357575187}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:15:11,369][0m Trial 32 finished with value: 0.24313758019032133 and parameters: {'observation_period_num': 80, 'train_rates': 0.9086550203981215, 'learning_rate': 5.2875157724981694e-05, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8949066109495104}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:18:11,026][0m Trial 33 finished with value: 0.24888314590567634 and parameters: {'observation_period_num': 46, 'train_rates': 0.9064778779598042, 'learning_rate': 3.050797792669138e-05, 'batch_size': 26, 'step_size': 10, 'gamma': 0.9002085540905742}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:19:29,461][0m Trial 34 finished with value: 0.38023121920007985 and parameters: {'observation_period_num': 47, 'train_rates': 0.8978020896391258, 'learning_rate': 2.477842105940496e-05, 'batch_size': 61, 'step_size': 10, 'gamma': 0.8875948327958013}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:22:10,487][0m Trial 35 finished with value: 0.4842619436711938 and parameters: {'observation_period_num': 34, 'train_rates': 0.8706256996415317, 'learning_rate': 1.1894508714341255e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8585346944470554}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:23:11,911][0m Trial 36 finished with value: 1.3122238868322127 and parameters: {'observation_period_num': 47, 'train_rates': 0.9600894055139397, 'learning_rate': 2.6700075117941096e-06, 'batch_size': 87, 'step_size': 12, 'gamma': 0.8954889852160297}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:24:31,338][0m Trial 37 finished with value: 0.929887210776922 and parameters: {'observation_period_num': 6, 'train_rates': 0.7708056144871257, 'learning_rate': 7.745444750932056e-06, 'batch_size': 52, 'step_size': 11, 'gamma': 0.821765606638217}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:26:19,752][0m Trial 38 finished with value: 0.6601085641469614 and parameters: {'observation_period_num': 105, 'train_rates': 0.9122556299484061, 'learning_rate': 3.3520077131669464e-05, 'batch_size': 163, 'step_size': 8, 'gamma': 0.8634072789859044}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:28:50,776][0m Trial 39 finished with value: 0.6631005869836223 and parameters: {'observation_period_num': 59, 'train_rates': 0.8309569402455466, 'learning_rate': 4.104055335499666e-06, 'batch_size': 29, 'step_size': 14, 'gamma': 0.918910017129681}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:30:47,160][0m Trial 40 finished with value: 0.3626700051385781 and parameters: {'observation_period_num': 96, 'train_rates': 0.9192131234566469, 'learning_rate': 2.1539155363706094e-05, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9448538447520785}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:33:51,606][0m Trial 41 finished with value: 0.3169602018527009 and parameters: {'observation_period_num': 80, 'train_rates': 0.9389096989846891, 'learning_rate': 5.843027079993134e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9159916840043456}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:36:22,826][0m Trial 42 finished with value: 0.1875917897042301 and parameters: {'observation_period_num': 122, 'train_rates': 0.9746606518577903, 'learning_rate': 4.256747533624717e-05, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8776636765584198}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:40:53,663][0m Trial 43 finished with value: 0.17892797020348636 and parameters: {'observation_period_num': 124, 'train_rates': 0.97695407431511, 'learning_rate': 0.00016250036421257565, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8715988747864107}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:44:58,610][0m Trial 44 finished with value: 0.16540202655290304 and parameters: {'observation_period_num': 119, 'train_rates': 0.9802474389035889, 'learning_rate': 0.0001363234030457629, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8496404370571303}. Best is trial 16 with value: 0.15959751725196838.[0m
[32m[I 2025-02-09 05:49:13,791][0m Trial 45 finished with value: 0.1493869139938741 and parameters: {'observation_period_num': 122, 'train_rates': 0.9740328603729431, 'learning_rate': 0.00014018728210616471, 'batch_size': 19, 'step_size': 14, 'gamma': 0.8237019071778787}. Best is trial 45 with value: 0.1493869139938741.[0m
[32m[I 2025-02-09 05:51:18,138][0m Trial 46 finished with value: 0.34280723333358765 and parameters: {'observation_period_num': 122, 'train_rates': 0.9551130271571673, 'learning_rate': 0.00013488040440275847, 'batch_size': 213, 'step_size': 14, 'gamma': 0.7997213802749861}. Best is trial 45 with value: 0.1493869139938741.[0m
[32m[I 2025-02-09 05:56:16,801][0m Trial 47 finished with value: 0.24016284562793433 and parameters: {'observation_period_num': 145, 'train_rates': 0.9640161373683759, 'learning_rate': 0.00018076539863303315, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8268527986216647}. Best is trial 45 with value: 0.1493869139938741.[0m
[32m[I 2025-02-09 05:58:49,432][0m Trial 48 finished with value: 0.2747692362943166 and parameters: {'observation_period_num': 123, 'train_rates': 0.9751831026753947, 'learning_rate': 0.0004013930430678728, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8483243717567143}. Best is trial 45 with value: 0.1493869139938741.[0m
[32m[I 2025-02-09 06:02:17,551][0m Trial 49 finished with value: 0.2543615436553955 and parameters: {'observation_period_num': 170, 'train_rates': 0.9463881334848179, 'learning_rate': 9.88987865110029e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8041534108161056}. Best is trial 45 with value: 0.1493869139938741.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-09 06:02:17,558][0m A new study created in memory with name: no-name-61a313b3-31d5-455f-a84a-1b34ad0fa9fc[0m
[32m[I 2025-02-09 06:04:14,051][0m Trial 0 finished with value: 1.0080533027648926 and parameters: {'observation_period_num': 108, 'train_rates': 0.9804414065277123, 'learning_rate': 1.480397768021008e-05, 'batch_size': 102, 'step_size': 2, 'gamma': 0.8622654935904533}. Best is trial 0 with value: 1.0080533027648926.[0m
[32m[I 2025-02-09 06:06:04,730][0m Trial 1 finished with value: 1.084146474641666 and parameters: {'observation_period_num': 122, 'train_rates': 0.7990641665837976, 'learning_rate': 1.1151568641313523e-05, 'batch_size': 191, 'step_size': 10, 'gamma': 0.9228258976937747}. Best is trial 0 with value: 1.0080533027648926.[0m
Early stopping at epoch 97
[32m[I 2025-02-09 06:07:24,187][0m Trial 2 finished with value: 0.7931343968075113 and parameters: {'observation_period_num': 86, 'train_rates': 0.8605274328744386, 'learning_rate': 0.00019721357796232235, 'batch_size': 211, 'step_size': 1, 'gamma': 0.875865230704854}. Best is trial 2 with value: 0.7931343968075113.[0m
[32m[I 2025-02-09 06:10:11,758][0m Trial 3 finished with value: 0.25159741560025 and parameters: {'observation_period_num': 133, 'train_rates': 0.9765947714545944, 'learning_rate': 0.00042045885323601, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8250093118183026}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:12:34,780][0m Trial 4 finished with value: 1.4078963573352532 and parameters: {'observation_period_num': 153, 'train_rates': 0.6905383168912849, 'learning_rate': 3.910532241265628e-06, 'batch_size': 143, 'step_size': 8, 'gamma': 0.8794874692204666}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:15:39,455][0m Trial 5 finished with value: 0.5202615857124329 and parameters: {'observation_period_num': 157, 'train_rates': 0.9806036248241543, 'learning_rate': 2.103096499387353e-05, 'batch_size': 144, 'step_size': 6, 'gamma': 0.9258005023176975}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:17:37,194][0m Trial 6 finished with value: 0.9048914926990784 and parameters: {'observation_period_num': 103, 'train_rates': 0.8104910138094881, 'learning_rate': 3.6601840868309556e-06, 'batch_size': 42, 'step_size': 12, 'gamma': 0.8690709672771669}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:22:14,626][0m Trial 7 finished with value: 1.2554911375045776 and parameters: {'observation_period_num': 215, 'train_rates': 0.987701904288431, 'learning_rate': 1.5527624052647288e-06, 'batch_size': 95, 'step_size': 8, 'gamma': 0.9491780800804683}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:23:30,800][0m Trial 8 finished with value: 1.4377154764139428 and parameters: {'observation_period_num': 77, 'train_rates': 0.6872473313847166, 'learning_rate': 5.5418458305732e-06, 'batch_size': 68, 'step_size': 5, 'gamma': 0.8788255554441506}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:27:06,757][0m Trial 9 finished with value: 0.9986194088677449 and parameters: {'observation_period_num': 204, 'train_rates': 0.8138010054955195, 'learning_rate': 4.3447023679300314e-05, 'batch_size': 176, 'step_size': 3, 'gamma': 0.7670201357796248}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:30:37,471][0m Trial 10 finished with value: 1.4031238703257025 and parameters: {'observation_period_num': 18, 'train_rates': 0.603625880797017, 'learning_rate': 0.0003976156473163913, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7761088759053051}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:33:43,587][0m Trial 11 finished with value: 0.5234513786662535 and parameters: {'observation_period_num': 167, 'train_rates': 0.9110295060284358, 'learning_rate': 7.146373311922147e-05, 'batch_size': 139, 'step_size': 6, 'gamma': 0.8141204580217651}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:36:49,792][0m Trial 12 finished with value: 0.40161001682281494 and parameters: {'observation_period_num': 172, 'train_rates': 0.9214737440279032, 'learning_rate': 0.0007440681765688705, 'batch_size': 236, 'step_size': 5, 'gamma': 0.9894389330275882}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:41:44,843][0m Trial 13 finished with value: 1.0310691800438057 and parameters: {'observation_period_num': 235, 'train_rates': 0.9126178130238449, 'learning_rate': 0.0009105673323514647, 'batch_size': 94, 'step_size': 4, 'gamma': 0.9808238907427402}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:45:14,012][0m Trial 14 finished with value: 0.5731085538864136 and parameters: {'observation_period_num': 185, 'train_rates': 0.9112896714297112, 'learning_rate': 0.000980213706125189, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8235540400553244}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:46:04,061][0m Trial 15 finished with value: 0.4615824546674835 and parameters: {'observation_period_num': 56, 'train_rates': 0.8703605654903921, 'learning_rate': 0.0001607508976506053, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8181576127926113}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:48:42,307][0m Trial 16 finished with value: 0.2531644105911255 and parameters: {'observation_period_num': 139, 'train_rates': 0.9498631488260421, 'learning_rate': 0.00033367457696259945, 'batch_size': 217, 'step_size': 11, 'gamma': 0.9848391975801275}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:50:55,239][0m Trial 17 finished with value: 0.745091067581642 and parameters: {'observation_period_num': 137, 'train_rates': 0.7383898320410969, 'learning_rate': 0.00012756009639897355, 'batch_size': 180, 'step_size': 13, 'gamma': 0.8437991541844704}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:52:23,429][0m Trial 18 finished with value: 0.30784660166707534 and parameters: {'observation_period_num': 37, 'train_rates': 0.94072692973259, 'learning_rate': 0.000371514112579683, 'batch_size': 57, 'step_size': 10, 'gamma': 0.7880855123900045}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:54:42,623][0m Trial 19 finished with value: 0.42041299714670555 and parameters: {'observation_period_num': 130, 'train_rates': 0.8641333138902417, 'learning_rate': 0.0003681677056335192, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9041957328101374}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 06:59:58,186][0m Trial 20 finished with value: 0.43716639280319214 and parameters: {'observation_period_num': 250, 'train_rates': 0.9491213070644983, 'learning_rate': 8.258235360483037e-05, 'batch_size': 220, 'step_size': 15, 'gamma': 0.7519784054321086}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:01:22,620][0m Trial 21 finished with value: 0.2607561629265547 and parameters: {'observation_period_num': 18, 'train_rates': 0.9482907548900809, 'learning_rate': 0.00033469365355405094, 'batch_size': 57, 'step_size': 10, 'gamma': 0.793605105776456}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:04:23,604][0m Trial 22 finished with value: 0.29723600298166275 and parameters: {'observation_period_num': 81, 'train_rates': 0.9541216863769325, 'learning_rate': 0.00029987993962484567, 'batch_size': 27, 'step_size': 9, 'gamma': 0.7976574235144483}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:05:32,633][0m Trial 23 finished with value: 0.45881201928122 and parameters: {'observation_period_num': 15, 'train_rates': 0.88395034800703, 'learning_rate': 0.0005730430829880412, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8462371644810822}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:07:38,319][0m Trial 24 finished with value: 0.4631304816230313 and parameters: {'observation_period_num': 113, 'train_rates': 0.8400183661977498, 'learning_rate': 0.00020012386602701323, 'batch_size': 46, 'step_size': 11, 'gamma': 0.8373549855848555}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:10:25,513][0m Trial 25 finished with value: 0.32945173089183977 and parameters: {'observation_period_num': 142, 'train_rates': 0.9525201425771704, 'learning_rate': 7.448620363050066e-05, 'batch_size': 77, 'step_size': 9, 'gamma': 0.8001106285904108}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:11:29,548][0m Trial 26 finished with value: 0.7841268975683984 and parameters: {'observation_period_num': 65, 'train_rates': 0.756501005616617, 'learning_rate': 3.595273248234328e-05, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9604023046564873}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:15:15,333][0m Trial 27 finished with value: 0.30695860766035854 and parameters: {'observation_period_num': 193, 'train_rates': 0.8834585158863792, 'learning_rate': 0.000527381092461477, 'batch_size': 161, 'step_size': 7, 'gamma': 0.9079282981757129}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:17:04,061][0m Trial 28 finished with value: 0.25405868712593527 and parameters: {'observation_period_num': 95, 'train_rates': 0.9645277936042523, 'learning_rate': 0.0002618456380878509, 'batch_size': 83, 'step_size': 9, 'gamma': 0.7791008555550811}. Best is trial 3 with value: 0.25159741560025.[0m
[32m[I 2025-02-09 07:18:58,561][0m Trial 29 finished with value: 0.24011263251304626 and parameters: {'observation_period_num': 99, 'train_rates': 0.988725974469351, 'learning_rate': 0.00012395781292640944, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7500030453569219}. Best is trial 29 with value: 0.24011263251304626.[0m
[32m[I 2025-02-09 07:21:00,820][0m Trial 30 finished with value: 0.4650537669658661 and parameters: {'observation_period_num': 116, 'train_rates': 0.9701753570652688, 'learning_rate': 7.528391271815105e-05, 'batch_size': 112, 'step_size': 7, 'gamma': 0.7525461566069549}. Best is trial 29 with value: 0.24011263251304626.[0m
[32m[I 2025-02-09 07:22:57,682][0m Trial 31 finished with value: 0.19522660970687866 and parameters: {'observation_period_num': 101, 'train_rates': 0.9805903289855735, 'learning_rate': 0.00011924954637469712, 'batch_size': 79, 'step_size': 9, 'gamma': 0.7636506515049294}. Best is trial 31 with value: 0.19522660970687866.[0m
[32m[I 2025-02-09 07:25:26,631][0m Trial 32 finished with value: 0.16358520090579987 and parameters: {'observation_period_num': 101, 'train_rates': 0.9896361259514996, 'learning_rate': 0.00012137513841261055, 'batch_size': 34, 'step_size': 8, 'gamma': 0.7653040738242892}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:27:54,673][0m Trial 33 finished with value: 0.1750941276550293 and parameters: {'observation_period_num': 96, 'train_rates': 0.9885097639046797, 'learning_rate': 0.00012566121207446327, 'batch_size': 34, 'step_size': 6, 'gamma': 0.7663503863416062}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:30:54,166][0m Trial 34 finished with value: 0.1920639102129226 and parameters: {'observation_period_num': 51, 'train_rates': 0.9839218870254307, 'learning_rate': 4.668935291348999e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.7651913869203175}. Best is trial 32 with value: 0.16358520090579987.[0m
Early stopping at epoch 91
[32m[I 2025-02-09 07:33:22,153][0m Trial 35 finished with value: 0.8071824312210083 and parameters: {'observation_period_num': 49, 'train_rates': 0.9347563915366728, 'learning_rate': 1.897055708222341e-05, 'batch_size': 30, 'step_size': 2, 'gamma': 0.7656936050231975}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:38:14,796][0m Trial 36 finished with value: 0.19533462077379227 and parameters: {'observation_period_num': 70, 'train_rates': 0.9884647604614383, 'learning_rate': 2.9797926868186635e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7681110106997112}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:40:09,617][0m Trial 37 finished with value: 0.309091251367837 and parameters: {'observation_period_num': 38, 'train_rates': 0.8952131024310994, 'learning_rate': 5.086052826470041e-05, 'batch_size': 41, 'step_size': 8, 'gamma': 0.7847695502133781}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:42:38,706][0m Trial 38 finished with value: 0.18546459930283682 and parameters: {'observation_period_num': 85, 'train_rates': 0.973288647442241, 'learning_rate': 0.00012206432366871451, 'batch_size': 33, 'step_size': 8, 'gamma': 0.7612747720340375}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:45:16,647][0m Trial 39 finished with value: 0.6166193929429238 and parameters: {'observation_period_num': 88, 'train_rates': 0.9275694848185871, 'learning_rate': 1.0971659140138313e-05, 'batch_size': 30, 'step_size': 5, 'gamma': 0.8010939046770092}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:47:17,427][0m Trial 40 finished with value: 0.4731597281757154 and parameters: {'observation_period_num': 35, 'train_rates': 0.8377722781709678, 'learning_rate': 2.5784067799576384e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8110053113638684}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:49:25,004][0m Trial 41 finished with value: 0.22693640301967488 and parameters: {'observation_period_num': 108, 'train_rates': 0.9694838254497828, 'learning_rate': 0.00010810137531252483, 'batch_size': 51, 'step_size': 8, 'gamma': 0.764009639124062}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:53:30,639][0m Trial 42 finished with value: 0.22790021613492803 and parameters: {'observation_period_num': 91, 'train_rates': 0.9795551943029779, 'learning_rate': 0.00021148628579594628, 'batch_size': 20, 'step_size': 9, 'gamma': 0.7750321568964147}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:54:55,888][0m Trial 43 finished with value: 0.4376030849085914 and parameters: {'observation_period_num': 68, 'train_rates': 0.9691362085368109, 'learning_rate': 5.674643784237244e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.7606681954419551}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:56:46,859][0m Trial 44 finished with value: 1.0347931688083403 and parameters: {'observation_period_num': 122, 'train_rates': 0.602418963865057, 'learning_rate': 0.00010566840408356106, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7804114031884144}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:58:03,578][0m Trial 45 finished with value: 0.8585961913656989 and parameters: {'observation_period_num': 54, 'train_rates': 0.6304448812568406, 'learning_rate': 0.00014852451294100575, 'batch_size': 47, 'step_size': 8, 'gamma': 0.7716469757991175}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 07:59:35,292][0m Trial 46 finished with value: 0.6720141003716666 and parameters: {'observation_period_num': 78, 'train_rates': 0.9633752260719168, 'learning_rate': 1.3782517965079378e-05, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8054531083795913}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 08:01:26,056][0m Trial 47 finished with value: 0.4733518064022064 and parameters: {'observation_period_num': 102, 'train_rates': 0.9887709608486959, 'learning_rate': 4.2309709152202724e-05, 'batch_size': 99, 'step_size': 7, 'gamma': 0.7585136382840897}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 08:04:04,717][0m Trial 48 finished with value: 0.32386952954711334 and parameters: {'observation_period_num': 121, 'train_rates': 0.9303698810769898, 'learning_rate': 5.9400609926030757e-05, 'batch_size': 31, 'step_size': 5, 'gamma': 0.7891041470675205}. Best is trial 32 with value: 0.16358520090579987.[0m
[32m[I 2025-02-09 08:07:33,423][0m Trial 49 finished with value: 0.8927300724669964 and parameters: {'observation_period_num': 155, 'train_rates': 0.9024630395998207, 'learning_rate': 1.368667059787777e-06, 'batch_size': 24, 'step_size': 11, 'gamma': 0.8319900920350191}. Best is trial 32 with value: 0.16358520090579987.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-09 08:07:33,431][0m A new study created in memory with name: no-name-cae2d3dc-6da5-4d63-b4dd-58d3fd18e3db[0m
[32m[I 2025-02-09 08:11:50,068][0m Trial 0 finished with value: 0.9105374470533625 and parameters: {'observation_period_num': 242, 'train_rates': 0.7265894437401441, 'learning_rate': 0.00012846957217618868, 'batch_size': 182, 'step_size': 9, 'gamma': 0.774494144965191}. Best is trial 0 with value: 0.9105374470533625.[0m
[32m[I 2025-02-09 08:14:29,389][0m Trial 1 finished with value: 1.3361806841875543 and parameters: {'observation_period_num': 167, 'train_rates': 0.6213296506000177, 'learning_rate': 1.146659900957208e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8127006086366788}. Best is trial 0 with value: 0.9105374470533625.[0m
[32m[I 2025-02-09 08:16:46,774][0m Trial 2 finished with value: 1.0315718811291914 and parameters: {'observation_period_num': 140, 'train_rates': 0.7416297369894442, 'learning_rate': 2.5765748087162344e-05, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9837899547451067}. Best is trial 0 with value: 0.9105374470533625.[0m
[32m[I 2025-02-09 08:20:50,021][0m Trial 3 finished with value: 0.5464096610826221 and parameters: {'observation_period_num': 184, 'train_rates': 0.8682981691863658, 'learning_rate': 0.00029170059566617084, 'batch_size': 22, 'step_size': 7, 'gamma': 0.8934585947618361}. Best is trial 3 with value: 0.5464096610826221.[0m
[32m[I 2025-02-09 08:22:39,201][0m Trial 4 finished with value: 1.0413832369591647 and parameters: {'observation_period_num': 121, 'train_rates': 0.6302473549506904, 'learning_rate': 0.00027476312863288925, 'batch_size': 61, 'step_size': 12, 'gamma': 0.8190462804362801}. Best is trial 3 with value: 0.5464096610826221.[0m
[32m[I 2025-02-09 08:24:16,984][0m Trial 5 finished with value: 0.8850520312481996 and parameters: {'observation_period_num': 101, 'train_rates': 0.8629383418159866, 'learning_rate': 8.48186641551329e-06, 'batch_size': 255, 'step_size': 14, 'gamma': 0.9156376192256482}. Best is trial 3 with value: 0.5464096610826221.[0m
[32m[I 2025-02-09 08:24:48,943][0m Trial 6 finished with value: 0.3635921776294708 and parameters: {'observation_period_num': 27, 'train_rates': 0.9700588927538664, 'learning_rate': 6.052007539851529e-05, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9824606879634805}. Best is trial 6 with value: 0.3635921776294708.[0m
[32m[I 2025-02-09 08:29:02,289][0m Trial 7 finished with value: 0.7828129391921194 and parameters: {'observation_period_num': 223, 'train_rates': 0.8262983936831718, 'learning_rate': 1.1559649847105795e-05, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8730957929205034}. Best is trial 6 with value: 0.3635921776294708.[0m
[32m[I 2025-02-09 08:31:45,867][0m Trial 8 finished with value: 0.32311715020073783 and parameters: {'observation_period_num': 25, 'train_rates': 0.9110984369488957, 'learning_rate': 1.5667990130696112e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8425216938665192}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:33:08,872][0m Trial 9 finished with value: 1.269434037708467 and parameters: {'observation_period_num': 98, 'train_rates': 0.674970570189266, 'learning_rate': 5.065883262685543e-05, 'batch_size': 219, 'step_size': 3, 'gamma': 0.7964304306639461}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:33:55,440][0m Trial 10 finished with value: 1.4014388879140218 and parameters: {'observation_period_num': 5, 'train_rates': 0.9595486923994678, 'learning_rate': 2.839783538298465e-06, 'batch_size': 113, 'step_size': 12, 'gamma': 0.8481349993496882}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:34:28,600][0m Trial 11 finished with value: 0.41147512197494507 and parameters: {'observation_period_num': 13, 'train_rates': 0.9823128986367492, 'learning_rate': 0.0008493101411724235, 'batch_size': 167, 'step_size': 10, 'gamma': 0.9873651470898754}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:35:27,521][0m Trial 12 finished with value: 1.3202189630932277 and parameters: {'observation_period_num': 45, 'train_rates': 0.9382802246422333, 'learning_rate': 2.00229738869673e-06, 'batch_size': 86, 'step_size': 5, 'gamma': 0.9371696788518605}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:36:29,256][0m Trial 13 finished with value: 0.3506681070327759 and parameters: {'observation_period_num': 60, 'train_rates': 0.9138423332326481, 'learning_rate': 4.928128260757195e-05, 'batch_size': 164, 'step_size': 11, 'gamma': 0.9444404729339632}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:37:37,790][0m Trial 14 finished with value: 0.5398391115573026 and parameters: {'observation_period_num': 65, 'train_rates': 0.911033979249242, 'learning_rate': 2.7954170922169254e-05, 'batch_size': 160, 'step_size': 12, 'gamma': 0.849234305520187}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:38:48,960][0m Trial 15 finished with value: 0.8122439361461873 and parameters: {'observation_period_num': 67, 'train_rates': 0.8926241641012529, 'learning_rate': 6.90571107879051e-06, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9374093420609175}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:39:52,428][0m Trial 16 finished with value: 0.5749199749179768 and parameters: {'observation_period_num': 61, 'train_rates': 0.7924821040840435, 'learning_rate': 0.00010166635319731105, 'batch_size': 81, 'step_size': 14, 'gamma': 0.9473737400322554}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:41:18,510][0m Trial 17 finished with value: 1.9601214161625615 and parameters: {'observation_period_num': 89, 'train_rates': 0.8119332995334136, 'learning_rate': 1.019773753411313e-06, 'batch_size': 188, 'step_size': 6, 'gamma': 0.8908093721929954}. Best is trial 8 with value: 0.32311715020073783.[0m
[32m[I 2025-02-09 08:44:52,483][0m Trial 18 finished with value: 0.25861631567219656 and parameters: {'observation_period_num': 37, 'train_rates': 0.9148870184879945, 'learning_rate': 2.1316626990070283e-05, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8414193768823252}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:49:06,346][0m Trial 19 finished with value: 0.9242875427889401 and parameters: {'observation_period_num': 31, 'train_rates': 0.769210861290752, 'learning_rate': 5.249084588156481e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7515006633452797}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:51:48,816][0m Trial 20 finished with value: 0.5664231973113837 and parameters: {'observation_period_num': 137, 'train_rates': 0.846897650408456, 'learning_rate': 1.7034059263593676e-05, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8444570736384216}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:52:49,460][0m Trial 21 finished with value: 0.36296247050795755 and parameters: {'observation_period_num': 44, 'train_rates': 0.9027247007768368, 'learning_rate': 4.680497818640559e-05, 'batch_size': 80, 'step_size': 10, 'gamma': 0.8274682371809766}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:55:12,653][0m Trial 22 finished with value: 0.30526823611533055 and parameters: {'observation_period_num': 77, 'train_rates': 0.936617663345056, 'learning_rate': 2.1538612311118536e-05, 'batch_size': 33, 'step_size': 13, 'gamma': 0.870763041524918}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:57:35,563][0m Trial 23 finished with value: 0.31259835966995786 and parameters: {'observation_period_num': 83, 'train_rates': 0.9391801937575692, 'learning_rate': 1.7438644061138865e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8700856944149754}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 08:59:09,715][0m Trial 24 finished with value: 0.33732413990717186 and parameters: {'observation_period_num': 80, 'train_rates': 0.947190504341693, 'learning_rate': 2.762746815402027e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.8696531526561705}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:01:48,028][0m Trial 25 finished with value: 0.6590366959571838 and parameters: {'observation_period_num': 125, 'train_rates': 0.9898778364446705, 'learning_rate': 3.52734188693347e-06, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8717625093444826}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:03:48,123][0m Trial 26 finished with value: 0.29442352825595486 and parameters: {'observation_period_num': 112, 'train_rates': 0.9349248278618821, 'learning_rate': 9.880626956394662e-05, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9037956191779506}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:06:51,904][0m Trial 27 finished with value: 0.31965547553180945 and parameters: {'observation_period_num': 162, 'train_rates': 0.8791384917667772, 'learning_rate': 0.0002638842228454267, 'batch_size': 94, 'step_size': 13, 'gamma': 0.90938509293484}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:08:50,960][0m Trial 28 finished with value: 0.4933829158222701 and parameters: {'observation_period_num': 116, 'train_rates': 0.8373638055916937, 'learning_rate': 0.0001084143916528061, 'batch_size': 62, 'step_size': 14, 'gamma': 0.9113774956332168}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:10:42,310][0m Trial 29 finished with value: 0.2771820545811014 and parameters: {'observation_period_num': 108, 'train_rates': 0.9320988767563956, 'learning_rate': 0.00017686907349733603, 'batch_size': 108, 'step_size': 9, 'gamma': 0.7906560825517336}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:14:08,067][0m Trial 30 finished with value: 0.7415061002740493 and parameters: {'observation_period_num': 212, 'train_rates': 0.6969781584147331, 'learning_rate': 0.0001603096610091712, 'batch_size': 136, 'step_size': 9, 'gamma': 0.7945404728591366}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:16:03,221][0m Trial 31 finished with value: 0.39406424820423125 and parameters: {'observation_period_num': 107, 'train_rates': 0.92982783095111, 'learning_rate': 8.39882092654812e-05, 'batch_size': 102, 'step_size': 9, 'gamma': 0.7743591632695248}. Best is trial 18 with value: 0.25861631567219656.[0m
[32m[I 2025-02-09 09:19:01,791][0m Trial 32 finished with value: 0.24673925325422003 and parameters: {'observation_period_num': 149, 'train_rates': 0.9641076724265201, 'learning_rate': 0.00015594226948442434, 'batch_size': 74, 'step_size': 10, 'gamma': 0.7949854271537281}. Best is trial 32 with value: 0.24673925325422003.[0m
[32m[I 2025-02-09 09:22:10,179][0m Trial 33 finished with value: 0.5630663439350309 and parameters: {'observation_period_num': 157, 'train_rates': 0.9621671142775751, 'learning_rate': 0.0006178031504047648, 'batch_size': 73, 'step_size': 8, 'gamma': 0.794340841830945}. Best is trial 32 with value: 0.24673925325422003.[0m
[32m[I 2025-02-09 09:25:43,127][0m Trial 34 finished with value: 0.30090830557876164 and parameters: {'observation_period_num': 189, 'train_rates': 0.8897829250409567, 'learning_rate': 0.0003922829565552458, 'batch_size': 130, 'step_size': 10, 'gamma': 0.7638092210809158}. Best is trial 32 with value: 0.24673925325422003.[0m
[32m[I 2025-02-09 09:28:56,032][0m Trial 35 finished with value: 0.3595512933455981 and parameters: {'observation_period_num': 180, 'train_rates': 0.8642492402028938, 'learning_rate': 0.0001780915555833721, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8080328383480909}. Best is trial 32 with value: 0.24673925325422003.[0m
[32m[I 2025-02-09 09:31:53,561][0m Trial 36 finished with value: 0.2134908089543333 and parameters: {'observation_period_num': 142, 'train_rates': 0.9644274437459438, 'learning_rate': 0.00017978104353792118, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8297602737135564}. Best is trial 36 with value: 0.2134908089543333.[0m
[32m[I 2025-02-09 09:34:40,517][0m Trial 37 finished with value: 0.2700307888261388 and parameters: {'observation_period_num': 138, 'train_rates': 0.9687581785378729, 'learning_rate': 0.0003904819281827011, 'batch_size': 54, 'step_size': 9, 'gamma': 0.8326370720963211}. Best is trial 36 with value: 0.2134908089543333.[0m
[32m[I 2025-02-09 09:37:38,680][0m Trial 38 finished with value: 0.3256796279374291 and parameters: {'observation_period_num': 143, 'train_rates': 0.9701151977626562, 'learning_rate': 0.000448061141497777, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8273508296959206}. Best is trial 36 with value: 0.2134908089543333.[0m
[32m[I 2025-02-09 09:41:41,006][0m Trial 39 finished with value: 0.15066426279752151 and parameters: {'observation_period_num': 148, 'train_rates': 0.9838912685837625, 'learning_rate': 0.0002359805738088743, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8126929224658732}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 09:46:27,217][0m Trial 40 finished with value: 0.16183585152029992 and parameters: {'observation_period_num': 154, 'train_rates': 0.9858690008082938, 'learning_rate': 0.00022743541188002884, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8061983865930593}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 09:50:03,425][0m Trial 41 finished with value: 0.15545080499402408 and parameters: {'observation_period_num': 152, 'train_rates': 0.989769766608637, 'learning_rate': 0.00022320432160444906, 'batch_size': 23, 'step_size': 10, 'gamma': 0.8104830641974596}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 09:53:31,097][0m Trial 42 finished with value: 0.16276856029734893 and parameters: {'observation_period_num': 152, 'train_rates': 0.9881044281944789, 'learning_rate': 0.00022313809749851832, 'batch_size': 26, 'step_size': 11, 'gamma': 0.8095451453984801}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 09:58:16,004][0m Trial 43 finished with value: 0.17806147229164204 and parameters: {'observation_period_num': 181, 'train_rates': 0.9833182720609863, 'learning_rate': 0.00023852353250941062, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8175134803321186}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:01:12,869][0m Trial 44 finished with value: 1.4194220138185152 and parameters: {'observation_period_num': 171, 'train_rates': 0.6044282377444582, 'learning_rate': 0.000260243789020717, 'batch_size': 23, 'step_size': 11, 'gamma': 0.8107094919146286}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:06:16,372][0m Trial 45 finished with value: 2.1664082407951355 and parameters: {'observation_period_num': 199, 'train_rates': 0.9799419177112213, 'learning_rate': 0.0008856607616297723, 'batch_size': 17, 'step_size': 12, 'gamma': 0.7747059179791874}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:09:52,654][0m Trial 46 finished with value: 0.7356552481651306 and parameters: {'observation_period_num': 172, 'train_rates': 0.9855207716193427, 'learning_rate': 0.0005778917862170562, 'batch_size': 41, 'step_size': 11, 'gamma': 0.8118408078631151}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:15:33,902][0m Trial 47 finished with value: 0.162695185043091 and parameters: {'observation_period_num': 239, 'train_rates': 0.9842712700639866, 'learning_rate': 6.672158230723227e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8189376936663395}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:21:26,608][0m Trial 48 finished with value: 0.20712257126220188 and parameters: {'observation_period_num': 251, 'train_rates': 0.9509399364741025, 'learning_rate': 7.695817817331011e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7827511700989653}. Best is trial 39 with value: 0.15066426279752151.[0m
[32m[I 2025-02-09 10:26:32,400][0m Trial 49 finished with value: 0.2209161182641983 and parameters: {'observation_period_num': 220, 'train_rates': 0.954390086513135, 'learning_rate': 6.868777996467838e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.7786677077017516}. Best is trial 39 with value: 0.15066426279752151.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-09 10:26:32,408][0m A new study created in memory with name: no-name-35f54f4f-aef4-4647-8fbc-d303884c5794[0m
[32m[I 2025-02-09 10:28:58,743][0m Trial 0 finished with value: 2.481566847495313 and parameters: {'observation_period_num': 49, 'train_rates': 0.9638754485763741, 'learning_rate': 0.0005673794405238137, 'batch_size': 34, 'step_size': 7, 'gamma': 0.923817837474818}. Best is trial 0 with value: 2.481566847495313.[0m
[32m[I 2025-02-09 10:30:05,990][0m Trial 1 finished with value: 0.8917550445631816 and parameters: {'observation_period_num': 73, 'train_rates': 0.6986961721152158, 'learning_rate': 0.00023211333427298396, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9060182838590998}. Best is trial 1 with value: 0.8917550445631816.[0m
[32m[I 2025-02-09 10:30:40,029][0m Trial 2 finished with value: 0.3838633813344649 and parameters: {'observation_period_num': 15, 'train_rates': 0.8137995062203655, 'learning_rate': 0.00013816054485036772, 'batch_size': 139, 'step_size': 8, 'gamma': 0.9320312185197928}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:32:01,252][0m Trial 3 finished with value: 1.6770253727784685 and parameters: {'observation_period_num': 97, 'train_rates': 0.6440227931820373, 'learning_rate': 2.783757951579152e-06, 'batch_size': 147, 'step_size': 3, 'gamma': 0.9694634813315224}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:33:00,820][0m Trial 4 finished with value: 1.0840529203414917 and parameters: {'observation_period_num': 64, 'train_rates': 0.9389464766041319, 'learning_rate': 4.8083622695535325e-06, 'batch_size': 221, 'step_size': 9, 'gamma': 0.8616725735454921}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:36:28,158][0m Trial 5 finished with value: 1.4368299228456902 and parameters: {'observation_period_num': 187, 'train_rates': 0.8764951084474688, 'learning_rate': 1.8764683381598164e-06, 'batch_size': 148, 'step_size': 7, 'gamma': 0.7564541390904497}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:36:49,673][0m Trial 6 finished with value: 0.7340179445179364 and parameters: {'observation_period_num': 17, 'train_rates': 0.7326717011513363, 'learning_rate': 0.00027413113265014737, 'batch_size': 234, 'step_size': 6, 'gamma': 0.8706882203973652}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:38:13,995][0m Trial 7 finished with value: 1.6702006884983607 and parameters: {'observation_period_num': 76, 'train_rates': 0.9490364500475077, 'learning_rate': 1.8177374638251999e-06, 'batch_size': 136, 'step_size': 12, 'gamma': 0.7756236604457162}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:40:03,225][0m Trial 8 finished with value: 1.243038212593365 and parameters: {'observation_period_num': 108, 'train_rates': 0.8227404881487884, 'learning_rate': 2.7700087564963695e-06, 'batch_size': 62, 'step_size': 14, 'gamma': 0.7785785005384509}. Best is trial 2 with value: 0.3838633813344649.[0m
Early stopping at epoch 65
[32m[I 2025-02-09 10:40:42,109][0m Trial 9 finished with value: 1.886482526827892 and parameters: {'observation_period_num': 64, 'train_rates': 0.6652310656329039, 'learning_rate': 4.1318082928761395e-06, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8318061450092571}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:45:20,070][0m Trial 10 finished with value: 0.7995058040120708 and parameters: {'observation_period_num': 249, 'train_rates': 0.7774467200487357, 'learning_rate': 4.30130673790973e-05, 'batch_size': 198, 'step_size': 15, 'gamma': 0.9804933545815738}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:45:39,116][0m Trial 11 finished with value: 0.7585505103212178 and parameters: {'observation_period_num': 15, 'train_rates': 0.7720792654409908, 'learning_rate': 9.890416370879844e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9146822045272863}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:46:03,713][0m Trial 12 finished with value: 0.975086515933383 and parameters: {'observation_period_num': 18, 'train_rates': 0.7181424817445993, 'learning_rate': 0.000892334830236037, 'batch_size': 191, 'step_size': 5, 'gamma': 0.8408798968722444}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:48:45,329][0m Trial 13 finished with value: 0.7572315373551954 and parameters: {'observation_period_num': 158, 'train_rates': 0.8444192469943458, 'learning_rate': 1.823196900331892e-05, 'batch_size': 242, 'step_size': 9, 'gamma': 0.9406425296296279}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:49:28,655][0m Trial 14 finished with value: 0.7029751673601518 and parameters: {'observation_period_num': 6, 'train_rates': 0.7483895848983834, 'learning_rate': 0.00013563759951357155, 'batch_size': 99, 'step_size': 5, 'gamma': 0.87389209264144}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:50:16,150][0m Trial 15 finished with value: 0.4065259671713932 and parameters: {'observation_period_num': 6, 'train_rates': 0.8877837380168463, 'learning_rate': 6.102203974548381e-05, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8859820733388029}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:52:51,141][0m Trial 16 finished with value: 0.8154049703949376 and parameters: {'observation_period_num': 144, 'train_rates': 0.8921853835687729, 'learning_rate': 2.14114687322142e-05, 'batch_size': 104, 'step_size': 1, 'gamma': 0.9490978227553329}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:53:32,332][0m Trial 17 finished with value: 0.4188916170597076 and parameters: {'observation_period_num': 37, 'train_rates': 0.8974541783330807, 'learning_rate': 5.0120311246300984e-05, 'batch_size': 172, 'step_size': 11, 'gamma': 0.8155082744670632}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:57:24,658][0m Trial 18 finished with value: 1.152909787564442 and parameters: {'observation_period_num': 205, 'train_rates': 0.8315496858505518, 'learning_rate': 9.066265285264838e-06, 'batch_size': 98, 'step_size': 3, 'gamma': 0.8937442005688668}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 10:59:18,731][0m Trial 19 finished with value: 0.39857752377336675 and parameters: {'observation_period_num': 109, 'train_rates': 0.8070978834284853, 'learning_rate': 8.284491840590758e-05, 'batch_size': 58, 'step_size': 3, 'gamma': 0.9505488435731982}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 11:02:24,853][0m Trial 20 finished with value: 1.5996592851673685 and parameters: {'observation_period_num': 124, 'train_rates': 0.6094215663867648, 'learning_rate': 0.0003048740836264096, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9531117743488454}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 11:03:26,249][0m Trial 21 finished with value: 0.42815316907939804 and parameters: {'observation_period_num': 42, 'train_rates': 0.8111975327208203, 'learning_rate': 8.306320862023605e-05, 'batch_size': 70, 'step_size': 3, 'gamma': 0.9897878408641774}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 11:05:03,304][0m Trial 22 finished with value: 0.508649266615112 and parameters: {'observation_period_num': 96, 'train_rates': 0.882498939020535, 'learning_rate': 5.954966929565523e-05, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9277974887081947}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 11:06:02,830][0m Trial 23 finished with value: 0.39162146414701754 and parameters: {'observation_period_num': 37, 'train_rates': 0.8575827113508376, 'learning_rate': 0.00013637412166364545, 'batch_size': 80, 'step_size': 4, 'gamma': 0.8888271324230894}. Best is trial 2 with value: 0.3838633813344649.[0m
[32m[I 2025-02-09 11:09:20,374][0m Trial 24 finished with value: 0.3440554674339529 and parameters: {'observation_period_num': 171, 'train_rates': 0.8541707680562469, 'learning_rate': 0.00013565631700195008, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9628732072739012}. Best is trial 24 with value: 0.3440554674339529.[0m
[32m[I 2025-02-09 11:12:43,790][0m Trial 25 finished with value: 0.37223400044611116 and parameters: {'observation_period_num': 176, 'train_rates': 0.8486373440624737, 'learning_rate': 0.00015833463766679094, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9661398181223264}. Best is trial 24 with value: 0.3440554674339529.[0m
[32m[I 2025-02-09 11:16:35,960][0m Trial 26 finished with value: 0.48052708158450846 and parameters: {'observation_period_num': 181, 'train_rates': 0.9188599371872926, 'learning_rate': 0.00044893223893897763, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9695591157912307}. Best is trial 24 with value: 0.3440554674339529.[0m
[32m[I 2025-02-09 11:20:43,062][0m Trial 27 finished with value: 0.7908313482306725 and parameters: {'observation_period_num': 224, 'train_rates': 0.7798738044725394, 'learning_rate': 0.0001793779620604645, 'batch_size': 42, 'step_size': 8, 'gamma': 0.9630639810731176}. Best is trial 24 with value: 0.3440554674339529.[0m
[32m[I 2025-02-09 11:23:37,246][0m Trial 28 finished with value: 0.604986838775106 and parameters: {'observation_period_num': 164, 'train_rates': 0.855472535941012, 'learning_rate': 2.5919121234136784e-05, 'batch_size': 168, 'step_size': 6, 'gamma': 0.9343222913427639}. Best is trial 24 with value: 0.3440554674339529.[0m
[32m[I 2025-02-09 11:28:41,296][0m Trial 29 finished with value: 0.3075855913070532 and parameters: {'observation_period_num': 138, 'train_rates': 0.9817797308087237, 'learning_rate': 0.00043848386601675346, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9294717400497599}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:31:58,120][0m Trial 30 finished with value: 0.6375408371289571 and parameters: {'observation_period_num': 136, 'train_rates': 0.9779880661358069, 'learning_rate': 0.0006060368921320518, 'batch_size': 25, 'step_size': 6, 'gamma': 0.9100044937339223}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:35:21,374][0m Trial 31 finished with value: 0.40650455798723 and parameters: {'observation_period_num': 172, 'train_rates': 0.9189461534882052, 'learning_rate': 0.000523575644248222, 'batch_size': 48, 'step_size': 8, 'gamma': 0.9229733890548382}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:39:50,651][0m Trial 32 finished with value: 0.6726140190254558 and parameters: {'observation_period_num': 152, 'train_rates': 0.976661222055724, 'learning_rate': 0.0003322926251065884, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9828810727341303}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:43:31,329][0m Trial 33 finished with value: 1.038888710994451 and parameters: {'observation_period_num': 197, 'train_rates': 0.7566332482596402, 'learning_rate': 0.0001708159817549242, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9630513662804537}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:45:28,232][0m Trial 34 finished with value: 0.5450508214402617 and parameters: {'observation_period_num': 122, 'train_rates': 0.799025205466001, 'learning_rate': 0.00037625272534497114, 'batch_size': 82, 'step_size': 4, 'gamma': 0.9400175069780292}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:50:11,487][0m Trial 35 finished with value: 0.8432324313535923 and parameters: {'observation_period_num': 214, 'train_rates': 0.925385120636943, 'learning_rate': 0.0007547115700181736, 'batch_size': 33, 'step_size': 4, 'gamma': 0.9030565499477939}. Best is trial 29 with value: 0.3075855913070532.[0m
[32m[I 2025-02-09 11:53:38,256][0m Trial 36 finished with value: 0.2246009111404419 and parameters: {'observation_period_num': 177, 'train_rates': 0.9578872998986909, 'learning_rate': 0.00019722384963223935, 'batch_size': 152, 'step_size': 6, 'gamma': 0.9738155261211282}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 11:57:09,408][0m Trial 37 finished with value: 0.22593318331241607 and parameters: {'observation_period_num': 179, 'train_rates': 0.9552491533032953, 'learning_rate': 0.0002404329534350309, 'batch_size': 123, 'step_size': 5, 'gamma': 0.97404600566072}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:00:48,725][0m Trial 38 finished with value: 0.25181302428245544 and parameters: {'observation_period_num': 190, 'train_rates': 0.9518046297347169, 'learning_rate': 0.00022627751698675634, 'batch_size': 156, 'step_size': 6, 'gamma': 0.9759217810244158}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:05:37,537][0m Trial 39 finished with value: 0.22711822390556335 and parameters: {'observation_period_num': 228, 'train_rates': 0.9577555871257385, 'learning_rate': 0.0002340452909751495, 'batch_size': 160, 'step_size': 6, 'gamma': 0.9778293656862851}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:10:54,551][0m Trial 40 finished with value: 0.2254149466753006 and parameters: {'observation_period_num': 247, 'train_rates': 0.9541039026466779, 'learning_rate': 0.00026295477010863244, 'batch_size': 154, 'step_size': 6, 'gamma': 0.9763104756222583}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:16:11,570][0m Trial 41 finished with value: 0.23432761430740356 and parameters: {'observation_period_num': 247, 'train_rates': 0.9536780195506022, 'learning_rate': 0.0002438586668790569, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9779031482452988}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:21:24,744][0m Trial 42 finished with value: 0.22704045474529266 and parameters: {'observation_period_num': 248, 'train_rates': 0.9614962297665217, 'learning_rate': 0.00022868517257912225, 'batch_size': 127, 'step_size': 6, 'gamma': 0.988987060389595}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:26:18,166][0m Trial 43 finished with value: 0.24540849551979432 and parameters: {'observation_period_num': 238, 'train_rates': 0.9350606361917145, 'learning_rate': 0.0002325079151523206, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9894868375622031}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:31:17,475][0m Trial 44 finished with value: 0.2911960780620575 and parameters: {'observation_period_num': 233, 'train_rates': 0.9648510737735526, 'learning_rate': 9.303473468347186e-05, 'batch_size': 137, 'step_size': 5, 'gamma': 0.9506165118965261}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:35:42,494][0m Trial 45 finished with value: 0.26133572832951624 and parameters: {'observation_period_num': 219, 'train_rates': 0.9096425350661649, 'learning_rate': 0.0007275391895779957, 'batch_size': 187, 'step_size': 8, 'gamma': 0.9742384886518927}. Best is trial 36 with value: 0.2246009111404419.[0m
[32m[I 2025-02-09 12:41:20,194][0m Trial 46 finished with value: 0.1779703050851822 and parameters: {'observation_period_num': 252, 'train_rates': 0.9891452580023616, 'learning_rate': 0.00031904116605453573, 'batch_size': 119, 'step_size': 6, 'gamma': 0.9577795577572629}. Best is trial 46 with value: 0.1779703050851822.[0m
[32m[I 2025-02-09 12:46:56,551][0m Trial 47 finished with value: 0.16370609402656555 and parameters: {'observation_period_num': 252, 'train_rates': 0.9894217129224654, 'learning_rate': 0.0003584428495512724, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9604566413830802}. Best is trial 47 with value: 0.16370609402656555.[0m
[32m[I 2025-02-09 12:51:19,644][0m Trial 48 finished with value: 0.6254987716674805 and parameters: {'observation_period_num': 212, 'train_rates': 0.9882661056179193, 'learning_rate': 0.0009574818025142941, 'batch_size': 109, 'step_size': 5, 'gamma': 0.956002856426068}. Best is trial 47 with value: 0.16370609402656555.[0m
[32m[I 2025-02-09 12:56:24,300][0m Trial 49 finished with value: 2.2030952721834183 and parameters: {'observation_period_num': 239, 'train_rates': 0.9413056455663629, 'learning_rate': 1.2587434844776439e-06, 'batch_size': 146, 'step_size': 2, 'gamma': 0.9436748808627716}. Best is trial 47 with value: 0.16370609402656555.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-09 12:56:24,307][0m A new study created in memory with name: no-name-14dd11a8-c00d-4d78-a787-016ecb08bf97[0m
[32m[I 2025-02-09 13:00:12,599][0m Trial 0 finished with value: 2.107985941331778 and parameters: {'observation_period_num': 231, 'train_rates': 0.6561954976918651, 'learning_rate': 1.6187928586801456e-06, 'batch_size': 252, 'step_size': 7, 'gamma': 0.9581503801661577}. Best is trial 0 with value: 2.107985941331778.[0m
[32m[I 2025-02-09 13:04:30,866][0m Trial 1 finished with value: 1.1107206934214153 and parameters: {'observation_period_num': 232, 'train_rates': 0.830271292263879, 'learning_rate': 0.000999949374174885, 'batch_size': 109, 'step_size': 7, 'gamma': 0.8156375398516134}. Best is trial 1 with value: 1.1107206934214153.[0m
[32m[I 2025-02-09 13:05:38,103][0m Trial 2 finished with value: 0.8706315462018402 and parameters: {'observation_period_num': 72, 'train_rates': 0.7293863720521888, 'learning_rate': 3.5309698537730335e-05, 'batch_size': 99, 'step_size': 4, 'gamma': 0.9743636466364343}. Best is trial 2 with value: 0.8706315462018402.[0m
[32m[I 2025-02-09 13:08:26,385][0m Trial 3 finished with value: 1.7119983181469423 and parameters: {'observation_period_num': 191, 'train_rates': 0.6153298675821, 'learning_rate': 5.069530543466483e-06, 'batch_size': 220, 'step_size': 4, 'gamma': 0.9502781712039692}. Best is trial 2 with value: 0.8706315462018402.[0m
[32m[I 2025-02-09 13:08:53,238][0m Trial 4 finished with value: 0.44840726256370544 and parameters: {'observation_period_num': 17, 'train_rates': 0.9689057652023599, 'learning_rate': 8.740102141405247e-05, 'batch_size': 208, 'step_size': 9, 'gamma': 0.8004598519293317}. Best is trial 4 with value: 0.44840726256370544.[0m
[32m[I 2025-02-09 13:12:33,909][0m Trial 5 finished with value: 2.216127634048462 and parameters: {'observation_period_num': 189, 'train_rates': 0.9416234830916725, 'learning_rate': 1.5484417459056228e-06, 'batch_size': 236, 'step_size': 9, 'gamma': 0.8402374103413995}. Best is trial 4 with value: 0.44840726256370544.[0m
[32m[I 2025-02-09 13:15:59,933][0m Trial 6 finished with value: 0.3018478370946029 and parameters: {'observation_period_num': 177, 'train_rates': 0.9061623599329023, 'learning_rate': 9.244136639439512e-05, 'batch_size': 84, 'step_size': 15, 'gamma': 0.8947755196077829}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:17:17,974][0m Trial 7 finished with value: 0.35020772408355366 and parameters: {'observation_period_num': 76, 'train_rates': 0.8856467282498541, 'learning_rate': 0.0007620286869219204, 'batch_size': 166, 'step_size': 10, 'gamma': 0.8877829136402642}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:21:07,228][0m Trial 8 finished with value: 0.4380520284175873 and parameters: {'observation_period_num': 191, 'train_rates': 0.9716719548585245, 'learning_rate': 2.3406751973617464e-05, 'batch_size': 123, 'step_size': 9, 'gamma': 0.9549831439150824}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:21:52,311][0m Trial 9 finished with value: 0.6407403672784271 and parameters: {'observation_period_num': 26, 'train_rates': 0.8286625550701852, 'learning_rate': 0.0004977128519182063, 'batch_size': 105, 'step_size': 2, 'gamma': 0.8270383631636412}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:24:40,803][0m Trial 10 finished with value: 0.8322956321661364 and parameters: {'observation_period_num': 129, 'train_rates': 0.7422773114711152, 'learning_rate': 0.00010448565598159088, 'batch_size': 24, 'step_size': 14, 'gamma': 0.895091706501635}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:26:21,520][0m Trial 11 finished with value: 0.34395534738346384 and parameters: {'observation_period_num': 99, 'train_rates': 0.8772202386735323, 'learning_rate': 0.00023156239373893484, 'batch_size': 169, 'step_size': 15, 'gamma': 0.8881208242944297}. Best is trial 6 with value: 0.3018478370946029.[0m
[32m[I 2025-02-09 13:29:00,886][0m Trial 12 finished with value: 0.2856792320245467 and parameters: {'observation_period_num': 135, 'train_rates': 0.8874232138910859, 'learning_rate': 0.00020916732036351765, 'batch_size': 52, 'step_size': 15, 'gamma': 0.7552150754828924}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:31:59,317][0m Trial 13 finished with value: 0.4311973137082532 and parameters: {'observation_period_num': 147, 'train_rates': 0.909131140163284, 'learning_rate': 2.6125067051358875e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.7607231863362255}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:34:40,811][0m Trial 14 finished with value: 0.6212219231087586 and parameters: {'observation_period_num': 153, 'train_rates': 0.7831174539442105, 'learning_rate': 0.00015182585166510285, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7546819874026567}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:37:47,841][0m Trial 15 finished with value: 0.511676438463231 and parameters: {'observation_period_num': 172, 'train_rates': 0.8621564154582781, 'learning_rate': 0.0002859616520749796, 'batch_size': 68, 'step_size': 12, 'gamma': 0.9292625685037443}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:42:36,781][0m Trial 16 finished with value: 0.34586134284351006 and parameters: {'observation_period_num': 105, 'train_rates': 0.9184899193145389, 'learning_rate': 5.948243060048119e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8560095841260374}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:46:36,228][0m Trial 17 finished with value: 0.6812623229409968 and parameters: {'observation_period_num': 213, 'train_rates': 0.7959266113489621, 'learning_rate': 0.00034255560881318257, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9151457443930724}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:48:42,686][0m Trial 18 finished with value: 1.0375587940216064 and parameters: {'observation_period_num': 120, 'train_rates': 0.9341226575245619, 'learning_rate': 9.880924070488518e-06, 'batch_size': 143, 'step_size': 13, 'gamma': 0.7837414234028954}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:53:54,580][0m Trial 19 finished with value: 0.5923955968562389 and parameters: {'observation_period_num': 251, 'train_rates': 0.8349439427910075, 'learning_rate': 1.1180833325423399e-05, 'batch_size': 39, 'step_size': 15, 'gamma': 0.8663716089106062}. Best is trial 12 with value: 0.2856792320245467.[0m
[32m[I 2025-02-09 13:57:06,951][0m Trial 20 finished with value: 0.19904975593090057 and parameters: {'observation_period_num': 159, 'train_rates': 0.98852108298268, 'learning_rate': 5.045749788151578e-05, 'batch_size': 86, 'step_size': 13, 'gamma': 0.9121003538837361}. Best is trial 20 with value: 0.19904975593090057.[0m
[32m[I 2025-02-09 14:00:20,106][0m Trial 21 finished with value: 0.18776361644268036 and parameters: {'observation_period_num': 159, 'train_rates': 0.9883195151805108, 'learning_rate': 5.235476086066342e-05, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9158338300666733}. Best is trial 21 with value: 0.18776361644268036.[0m
[32m[I 2025-02-09 14:03:26,623][0m Trial 22 finished with value: 0.1691017895936966 and parameters: {'observation_period_num': 150, 'train_rates': 0.9883498624514254, 'learning_rate': 4.467668679268026e-05, 'batch_size': 44, 'step_size': 13, 'gamma': 0.9137444588805481}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:06:33,820][0m Trial 23 finished with value: 0.21267813444137573 and parameters: {'observation_period_num': 155, 'train_rates': 0.9890613928506424, 'learning_rate': 4.5969108865648335e-05, 'batch_size': 89, 'step_size': 11, 'gamma': 0.9254674421457731}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:09:48,608][0m Trial 24 finished with value: 0.5541221499443054 and parameters: {'observation_period_num': 164, 'train_rates': 0.9853570687871892, 'learning_rate': 1.3802608514957937e-05, 'batch_size': 130, 'step_size': 13, 'gamma': 0.908065725311893}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:14:20,461][0m Trial 25 finished with value: 0.6652224997679392 and parameters: {'observation_period_num': 206, 'train_rates': 0.9456357603637165, 'learning_rate': 4.5950811575065795e-06, 'batch_size': 35, 'step_size': 11, 'gamma': 0.9379543440886844}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:15:53,966][0m Trial 26 finished with value: 0.4942206798061248 and parameters: {'observation_period_num': 81, 'train_rates': 0.9463283796408112, 'learning_rate': 1.754385922658569e-05, 'batch_size': 80, 'step_size': 13, 'gamma': 0.9879069067219507}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:16:42,526][0m Trial 27 finished with value: 0.4762698709964752 and parameters: {'observation_period_num': 48, 'train_rates': 0.9624786730056017, 'learning_rate': 4.142169841794585e-05, 'batch_size': 156, 'step_size': 12, 'gamma': 0.8787315623454381}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:18:57,043][0m Trial 28 finished with value: 0.18132030963897705 and parameters: {'observation_period_num': 113, 'train_rates': 0.988198576248804, 'learning_rate': 6.606025278336339e-05, 'batch_size': 59, 'step_size': 7, 'gamma': 0.9098291817077192}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:20:49,079][0m Trial 29 finished with value: 1.170765791993824 and parameters: {'observation_period_num': 113, 'train_rates': 0.6833531242522357, 'learning_rate': 6.032723047618814e-06, 'batch_size': 46, 'step_size': 6, 'gamma': 0.9427384941902753}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:23:37,507][0m Trial 30 finished with value: 1.2661791870379335 and parameters: {'observation_period_num': 140, 'train_rates': 0.925242985065143, 'learning_rate': 2.970214540068299e-06, 'batch_size': 62, 'step_size': 7, 'gamma': 0.8565169784104063}. Best is trial 22 with value: 0.1691017895936966.[0m
[32m[I 2025-02-09 14:26:14,823][0m Trial 31 finished with value: 0.1429359427520207 and parameters: {'observation_period_num': 97, 'train_rates': 0.9878362310663918, 'learning_rate': 6.577677867567627e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9122149417847534}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:29:08,056][0m Trial 32 finished with value: 0.24081725985046445 and parameters: {'observation_period_num': 89, 'train_rates': 0.9557456760003648, 'learning_rate': 6.51988959340503e-05, 'batch_size': 28, 'step_size': 5, 'gamma': 0.9040449563931324}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:30:39,272][0m Trial 33 finished with value: 0.25032716531019944 and parameters: {'observation_period_num': 52, 'train_rates': 0.9643384058794654, 'learning_rate': 0.0001079398896811598, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9665622986337865}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:35:25,284][0m Trial 34 finished with value: 0.15485030023211782 and parameters: {'observation_period_num': 120, 'train_rates': 0.9847033142144639, 'learning_rate': 2.8080158834528055e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9229352674991073}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:39:11,218][0m Trial 35 finished with value: 0.28799449693072926 and parameters: {'observation_period_num': 62, 'train_rates': 0.9242488912031624, 'learning_rate': 3.0464221077238854e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.93239106107788}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:41:07,939][0m Trial 36 finished with value: 0.9956999366689452 and parameters: {'observation_period_num': 120, 'train_rates': 0.6285592553355799, 'learning_rate': 0.00015177640594455087, 'batch_size': 37, 'step_size': 3, 'gamma': 0.9506045053510511}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:46:08,603][0m Trial 37 finished with value: 0.2607648279688774 and parameters: {'observation_period_num': 96, 'train_rates': 0.9622436565620621, 'learning_rate': 1.9189702697869082e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8741072256384739}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:47:43,119][0m Trial 38 finished with value: 0.8098544755647349 and parameters: {'observation_period_num': 110, 'train_rates': 0.6980000957742635, 'learning_rate': 7.39603719065922e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.9652470654574701}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:50:30,428][0m Trial 39 finished with value: 0.29375914178735063 and parameters: {'observation_period_num': 132, 'train_rates': 0.9018671578098565, 'learning_rate': 3.6264624022818374e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9258580215164441}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:51:56,404][0m Trial 40 finished with value: 1.05475914478302 and parameters: {'observation_period_num': 84, 'train_rates': 0.9406683595816632, 'learning_rate': 0.00013155911825993383, 'batch_size': 250, 'step_size': 1, 'gamma': 0.8959280809500273}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:55:47,021][0m Trial 41 finished with value: 0.14548349471108332 and parameters: {'observation_period_num': 178, 'train_rates': 0.9735983024017074, 'learning_rate': 7.476853472592193e-05, 'batch_size': 45, 'step_size': 6, 'gamma': 0.9203768084709089}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 14:59:39,139][0m Trial 42 finished with value: 0.1970896517688578 and parameters: {'observation_period_num': 179, 'train_rates': 0.972554848602043, 'learning_rate': 7.89399583523113e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.943018919260877}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:03:02,083][0m Trial 43 finished with value: 0.25293929436627555 and parameters: {'observation_period_num': 145, 'train_rates': 0.969994841725965, 'learning_rate': 2.7001791364385468e-05, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9205403896367466}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:05:11,427][0m Trial 44 finished with value: 0.5435670669987905 and parameters: {'observation_period_num': 120, 'train_rates': 0.9512464258159443, 'learning_rate': 3.727125674739993e-05, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9046104116407516}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:09:03,417][0m Trial 45 finished with value: 0.17328771436586976 and parameters: {'observation_period_num': 185, 'train_rates': 0.9771538145481791, 'learning_rate': 9.092045849596405e-05, 'batch_size': 57, 'step_size': 5, 'gamma': 0.8916043136480186}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:12:37,836][0m Trial 46 finished with value: 0.3471065777624753 and parameters: {'observation_period_num': 187, 'train_rates': 0.8984610533543036, 'learning_rate': 0.00010586563088653158, 'batch_size': 97, 'step_size': 5, 'gamma': 0.8833334247484175}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:17:17,015][0m Trial 47 finished with value: 0.3456944025614682 and parameters: {'observation_period_num': 225, 'train_rates': 0.8694404497555082, 'learning_rate': 0.00020359033408833894, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8949162518843373}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:20:29,132][0m Trial 48 finished with value: 2.2302918051190943 and parameters: {'observation_period_num': 166, 'train_rates': 0.931039577781449, 'learning_rate': 1.0087779658997025e-06, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8358342368024456}. Best is trial 31 with value: 0.1429359427520207.[0m
[32m[I 2025-02-09 15:24:23,231][0m Trial 49 finished with value: 0.897434413433075 and parameters: {'observation_period_num': 198, 'train_rates': 0.8451001873738989, 'learning_rate': 0.0004635229133394653, 'batch_size': 51, 'step_size': 9, 'gamma': 0.8661039803890954}. Best is trial 31 with value: 0.1429359427520207.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 134, 'train_rates': 0.9882769596559033, 'learning_rate': 0.00024035196656608756, 'batch_size': 232, 'step_size': 11, 'gamma': 0.9519385147072839}
Epoch 1/300, trend Loss: 1.1047 | 1.7617
Epoch 2/300, trend Loss: 0.7233 | 1.1344
Epoch 3/300, trend Loss: 0.6262 | 0.9869
Epoch 4/300, trend Loss: 0.5654 | 0.8655
Epoch 5/300, trend Loss: 0.5307 | 0.8238
Epoch 6/300, trend Loss: 0.5900 | 0.7462
Epoch 7/300, trend Loss: 0.5007 | 0.7425
Epoch 8/300, trend Loss: 0.4609 | 0.7167
Epoch 9/300, trend Loss: 0.4466 | 0.6539
Epoch 10/300, trend Loss: 0.4621 | 0.6339
Epoch 11/300, trend Loss: 0.4359 | 0.6632
Epoch 12/300, trend Loss: 0.3770 | 0.5943
Epoch 13/300, trend Loss: 0.3432 | 0.5278
Epoch 14/300, trend Loss: 0.3227 | 0.4999
Epoch 15/300, trend Loss: 0.3211 | 0.4751
Epoch 16/300, trend Loss: 0.3210 | 0.4647
Epoch 17/300, trend Loss: 0.3258 | 0.4446
Epoch 18/300, trend Loss: 0.3648 | 0.4502
Epoch 19/300, trend Loss: 0.3192 | 0.4116
Epoch 20/300, trend Loss: 0.3474 | 0.3947
Epoch 21/300, trend Loss: 0.3038 | 0.3902
Epoch 22/300, trend Loss: 0.2958 | 0.3807
Epoch 23/300, trend Loss: 0.2826 | 0.3666
Epoch 24/300, trend Loss: 0.3017 | 0.3551
Epoch 25/300, trend Loss: 0.2998 | 0.3445
Epoch 26/300, trend Loss: 0.3165 | 0.3351
Epoch 27/300, trend Loss: 0.3136 | 0.3535
Epoch 28/300, trend Loss: 0.2716 | 0.3314
Epoch 29/300, trend Loss: 0.2558 | 0.3193
Epoch 30/300, trend Loss: 0.2583 | 0.3324
Epoch 31/300, trend Loss: 0.2418 | 0.3149
Epoch 32/300, trend Loss: 0.2375 | 0.2944
Epoch 33/300, trend Loss: 0.2366 | 0.2955
Epoch 34/300, trend Loss: 0.2346 | 0.2818
Epoch 35/300, trend Loss: 0.2261 | 0.2730
Epoch 36/300, trend Loss: 0.2140 | 0.2563
Epoch 37/300, trend Loss: 0.2113 | 0.2517
Epoch 38/300, trend Loss: 0.2115 | 0.2453
Epoch 39/300, trend Loss: 0.2111 | 0.2410
Epoch 40/300, trend Loss: 0.2181 | 0.2391
Epoch 41/300, trend Loss: 0.2150 | 0.2336
Epoch 42/300, trend Loss: 0.2156 | 0.2312
Epoch 43/300, trend Loss: 0.2058 | 0.2299
Epoch 44/300, trend Loss: 0.2040 | 0.2241
Epoch 45/300, trend Loss: 0.1979 | 0.2264
Epoch 46/300, trend Loss: 0.1951 | 0.2149
Epoch 47/300, trend Loss: 0.1956 | 0.2197
Epoch 48/300, trend Loss: 0.2005 | 0.2136
Epoch 49/300, trend Loss: 0.2275 | 0.2123
Epoch 50/300, trend Loss: 0.2376 | 0.2129
Epoch 51/300, trend Loss: 0.2918 | 0.2225
Epoch 52/300, trend Loss: 0.2703 | 0.2390
Epoch 53/300, trend Loss: 0.2280 | 0.2260
Epoch 54/300, trend Loss: 0.2102 | 0.2213
Epoch 55/300, trend Loss: 0.1994 | 0.2143
Epoch 56/300, trend Loss: 0.2010 | 0.2188
Epoch 57/300, trend Loss: 0.1954 | 0.2239
Epoch 58/300, trend Loss: 0.1921 | 0.2062
Epoch 59/300, trend Loss: 0.1916 | 0.2162
Epoch 60/300, trend Loss: 0.1873 | 0.2033
Epoch 61/300, trend Loss: 0.1814 | 0.1907
Epoch 62/300, trend Loss: 0.1773 | 0.1930
Epoch 63/300, trend Loss: 0.1741 | 0.1861
Epoch 64/300, trend Loss: 0.1727 | 0.1869
Epoch 65/300, trend Loss: 0.1723 | 0.1852
Epoch 66/300, trend Loss: 0.1737 | 0.1826
Epoch 67/300, trend Loss: 0.1749 | 0.1838
Epoch 68/300, trend Loss: 0.1739 | 0.1794
Epoch 69/300, trend Loss: 0.1744 | 0.1795
Epoch 70/300, trend Loss: 0.1746 | 0.1806
Epoch 71/300, trend Loss: 0.1781 | 0.1797
Epoch 72/300, trend Loss: 0.1749 | 0.1789
Epoch 73/300, trend Loss: 0.1740 | 0.1783
Epoch 74/300, trend Loss: 0.1674 | 0.1736
Epoch 75/300, trend Loss: 0.1651 | 0.1725
Epoch 76/300, trend Loss: 0.1649 | 0.1708
Epoch 77/300, trend Loss: 0.1646 | 0.1705
Epoch 78/300, trend Loss: 0.1632 | 0.1706
Epoch 79/300, trend Loss: 0.1627 | 0.1693
Epoch 80/300, trend Loss: 0.1604 | 0.1705
Epoch 81/300, trend Loss: 0.1591 | 0.1658
Epoch 82/300, trend Loss: 0.1591 | 0.1676
Epoch 83/300, trend Loss: 0.1588 | 0.1623
Epoch 84/300, trend Loss: 0.1588 | 0.1646
Epoch 85/300, trend Loss: 0.1598 | 0.1602
Epoch 86/300, trend Loss: 0.1626 | 0.1607
Epoch 87/300, trend Loss: 0.1683 | 0.1606
Epoch 88/300, trend Loss: 0.1698 | 0.1604
Epoch 89/300, trend Loss: 0.1725 | 0.1615
Epoch 90/300, trend Loss: 0.1651 | 0.1645
Epoch 91/300, trend Loss: 0.1625 | 0.1614
Epoch 92/300, trend Loss: 0.1595 | 0.1629
Epoch 93/300, trend Loss: 0.1573 | 0.1578
Epoch 94/300, trend Loss: 0.1549 | 0.1606
Epoch 95/300, trend Loss: 0.1538 | 0.1574
Epoch 96/300, trend Loss: 0.1548 | 0.1575
Epoch 97/300, trend Loss: 0.1605 | 0.1595
Epoch 98/300, trend Loss: 0.1707 | 0.1554
Epoch 99/300, trend Loss: 0.1755 | 0.1644
Epoch 100/300, trend Loss: 0.1952 | 0.1607
Epoch 101/300, trend Loss: 0.1951 | 0.1631
Epoch 102/300, trend Loss: 0.1932 | 0.1733
Epoch 103/300, trend Loss: 0.1873 | 0.1576
Epoch 104/300, trend Loss: 0.1744 | 0.1750
Epoch 105/300, trend Loss: 0.1627 | 0.1521
Epoch 106/300, trend Loss: 0.1540 | 0.1600
Epoch 107/300, trend Loss: 0.1521 | 0.1577
Epoch 108/300, trend Loss: 0.1506 | 0.1556
Epoch 109/300, trend Loss: 0.1487 | 0.1517
Epoch 110/300, trend Loss: 0.1466 | 0.1557
Epoch 111/300, trend Loss: 0.1465 | 0.1517
Epoch 112/300, trend Loss: 0.1459 | 0.1527
Epoch 113/300, trend Loss: 0.1444 | 0.1485
Epoch 114/300, trend Loss: 0.1452 | 0.1512
Epoch 115/300, trend Loss: 0.1446 | 0.1498
Epoch 116/300, trend Loss: 0.1442 | 0.1496
Epoch 117/300, trend Loss: 0.1457 | 0.1487
Epoch 118/300, trend Loss: 0.1435 | 0.1483
Epoch 119/300, trend Loss: 0.1431 | 0.1473
Epoch 120/300, trend Loss: 0.1435 | 0.1443
Epoch 121/300, trend Loss: 0.1439 | 0.1487
Epoch 122/300, trend Loss: 0.1427 | 0.1419
Epoch 123/300, trend Loss: 0.1407 | 0.1447
Epoch 124/300, trend Loss: 0.1390 | 0.1418
Epoch 125/300, trend Loss: 0.1376 | 0.1434
Epoch 126/300, trend Loss: 0.1363 | 0.1420
Epoch 127/300, trend Loss: 0.1365 | 0.1409
Epoch 128/300, trend Loss: 0.1358 | 0.1427
Epoch 129/300, trend Loss: 0.1356 | 0.1442
Epoch 130/300, trend Loss: 0.1367 | 0.1405
Epoch 131/300, trend Loss: 0.1359 | 0.1462
Epoch 132/300, trend Loss: 0.1372 | 0.1374
Epoch 133/300, trend Loss: 0.1378 | 0.1435
Epoch 134/300, trend Loss: 0.1364 | 0.1387
Epoch 135/300, trend Loss: 0.1355 | 0.1418
Epoch 136/300, trend Loss: 0.1339 | 0.1373
Epoch 137/300, trend Loss: 0.1337 | 0.1388
Epoch 138/300, trend Loss: 0.1330 | 0.1367
Epoch 139/300, trend Loss: 0.1334 | 0.1369
Epoch 140/300, trend Loss: 0.1325 | 0.1377
Epoch 141/300, trend Loss: 0.1320 | 0.1351
Epoch 142/300, trend Loss: 0.1323 | 0.1357
Epoch 143/300, trend Loss: 0.1315 | 0.1358
Epoch 144/300, trend Loss: 0.1320 | 0.1354
Epoch 145/300, trend Loss: 0.1316 | 0.1353
Epoch 146/300, trend Loss: 0.1320 | 0.1347
Epoch 147/300, trend Loss: 0.1322 | 0.1337
Epoch 148/300, trend Loss: 0.1324 | 0.1342
Epoch 149/300, trend Loss: 0.1330 | 0.1358
Epoch 150/300, trend Loss: 0.1338 | 0.1328
Epoch 151/300, trend Loss: 0.1341 | 0.1348
Epoch 152/300, trend Loss: 0.1347 | 0.1330
Epoch 153/300, trend Loss: 0.1340 | 0.1349
Epoch 154/300, trend Loss: 0.1345 | 0.1324
Epoch 155/300, trend Loss: 0.1368 | 0.1367
Epoch 156/300, trend Loss: 0.1356 | 0.1333
Epoch 157/300, trend Loss: 0.1349 | 0.1325
Epoch 158/300, trend Loss: 0.1353 | 0.1342
Epoch 159/300, trend Loss: 0.1354 | 0.1311
Epoch 160/300, trend Loss: 0.1356 | 0.1356
Epoch 161/300, trend Loss: 0.1367 | 0.1314
Epoch 162/300, trend Loss: 0.1340 | 0.1340
Epoch 163/300, trend Loss: 0.1320 | 0.1312
Epoch 164/300, trend Loss: 0.1308 | 0.1332
Epoch 165/300, trend Loss: 0.1295 | 0.1306
Epoch 166/300, trend Loss: 0.1279 | 0.1322
Epoch 167/300, trend Loss: 0.1276 | 0.1320
Epoch 168/300, trend Loss: 0.1257 | 0.1328
Epoch 169/300, trend Loss: 0.1252 | 0.1321
Epoch 170/300, trend Loss: 0.1251 | 0.1312
Epoch 171/300, trend Loss: 0.1246 | 0.1308
Epoch 172/300, trend Loss: 0.1251 | 0.1290
Epoch 173/300, trend Loss: 0.1244 | 0.1296
Epoch 174/300, trend Loss: 0.1242 | 0.1301
Epoch 175/300, trend Loss: 0.1234 | 0.1285
Epoch 176/300, trend Loss: 0.1233 | 0.1295
Epoch 177/300, trend Loss: 0.1242 | 0.1287
Epoch 178/300, trend Loss: 0.1238 | 0.1291
Epoch 179/300, trend Loss: 0.1227 | 0.1285
Epoch 180/300, trend Loss: 0.1234 | 0.1295
Epoch 181/300, trend Loss: 0.1226 | 0.1275
Epoch 182/300, trend Loss: 0.1231 | 0.1293
Epoch 183/300, trend Loss: 0.1225 | 0.1262
Epoch 184/300, trend Loss: 0.1229 | 0.1311
Epoch 185/300, trend Loss: 0.1226 | 0.1277
Epoch 186/300, trend Loss: 0.1227 | 0.1278
Epoch 187/300, trend Loss: 0.1220 | 0.1263
Epoch 188/300, trend Loss: 0.1222 | 0.1280
Epoch 189/300, trend Loss: 0.1222 | 0.1277
Epoch 190/300, trend Loss: 0.1216 | 0.1261
Epoch 191/300, trend Loss: 0.1204 | 0.1267
Epoch 192/300, trend Loss: 0.1220 | 0.1260
Epoch 193/300, trend Loss: 0.1224 | 0.1251
Epoch 194/300, trend Loss: 0.1212 | 0.1263
Epoch 195/300, trend Loss: 0.1206 | 0.1256
Epoch 196/300, trend Loss: 0.1202 | 0.1277
Epoch 197/300, trend Loss: 0.1196 | 0.1248
Epoch 198/300, trend Loss: 0.1194 | 0.1271
Epoch 199/300, trend Loss: 0.1201 | 0.1251
Epoch 200/300, trend Loss: 0.1203 | 0.1266
Epoch 201/300, trend Loss: 0.1203 | 0.1251
Epoch 202/300, trend Loss: 0.1202 | 0.1260
Epoch 203/300, trend Loss: 0.1192 | 0.1253
Epoch 204/300, trend Loss: 0.1184 | 0.1241
Epoch 205/300, trend Loss: 0.1171 | 0.1250
Epoch 206/300, trend Loss: 0.1177 | 0.1243
Epoch 207/300, trend Loss: 0.1173 | 0.1255
Epoch 208/300, trend Loss: 0.1165 | 0.1234
Epoch 209/300, trend Loss: 0.1165 | 0.1248
Epoch 210/300, trend Loss: 0.1174 | 0.1229
Epoch 211/300, trend Loss: 0.1174 | 0.1246
Epoch 212/300, trend Loss: 0.1165 | 0.1232
Epoch 213/300, trend Loss: 0.1163 | 0.1225
Epoch 214/300, trend Loss: 0.1161 | 0.1232
Epoch 215/300, trend Loss: 0.1162 | 0.1248
Epoch 216/300, trend Loss: 0.1156 | 0.1231
Epoch 217/300, trend Loss: 0.1169 | 0.1237
Epoch 218/300, trend Loss: 0.1154 | 0.1245
Epoch 219/300, trend Loss: 0.1149 | 0.1236
Epoch 220/300, trend Loss: 0.1151 | 0.1249
Epoch 221/300, trend Loss: 0.1149 | 0.1227
Epoch 222/300, trend Loss: 0.1150 | 0.1223
Epoch 223/300, trend Loss: 0.1138 | 0.1227
Epoch 224/300, trend Loss: 0.1141 | 0.1218
Epoch 225/300, trend Loss: 0.1141 | 0.1212
Epoch 226/300, trend Loss: 0.1147 | 0.1224
Epoch 227/300, trend Loss: 0.1144 | 0.1225
Epoch 228/300, trend Loss: 0.1144 | 0.1207
Epoch 229/300, trend Loss: 0.1138 | 0.1223
Epoch 230/300, trend Loss: 0.1139 | 0.1202
Epoch 231/300, trend Loss: 0.1139 | 0.1210
Epoch 232/300, trend Loss: 0.1144 | 0.1204
Epoch 233/300, trend Loss: 0.1140 | 0.1206
Epoch 234/300, trend Loss: 0.1136 | 0.1221
Epoch 235/300, trend Loss: 0.1134 | 0.1221
Epoch 236/300, trend Loss: 0.1145 | 0.1212
Epoch 237/300, trend Loss: 0.1133 | 0.1219
Epoch 238/300, trend Loss: 0.1128 | 0.1203
Epoch 239/300, trend Loss: 0.1117 | 0.1208
Epoch 240/300, trend Loss: 0.1126 | 0.1196
Epoch 241/300, trend Loss: 0.1113 | 0.1204
Epoch 242/300, trend Loss: 0.1118 | 0.1195
Epoch 243/300, trend Loss: 0.1116 | 0.1205
Epoch 244/300, trend Loss: 0.1118 | 0.1201
Epoch 245/300, trend Loss: 0.1115 | 0.1199
Epoch 246/300, trend Loss: 0.1109 | 0.1190
Epoch 247/300, trend Loss: 0.1111 | 0.1188
Epoch 248/300, trend Loss: 0.1111 | 0.1191
Epoch 249/300, trend Loss: 0.1113 | 0.1183
Epoch 250/300, trend Loss: 0.1108 | 0.1192
Epoch 251/300, trend Loss: 0.1105 | 0.1183
Epoch 252/300, trend Loss: 0.1104 | 0.1200
Epoch 253/300, trend Loss: 0.1100 | 0.1190
Epoch 254/300, trend Loss: 0.1100 | 0.1193
Epoch 255/300, trend Loss: 0.1098 | 0.1195
Epoch 256/300, trend Loss: 0.1111 | 0.1194
Epoch 257/300, trend Loss: 0.1103 | 0.1185
Epoch 258/300, trend Loss: 0.1099 | 0.1192
Epoch 259/300, trend Loss: 0.1103 | 0.1177
Epoch 260/300, trend Loss: 0.1100 | 0.1190
Epoch 261/300, trend Loss: 0.1098 | 0.1181
Epoch 262/300, trend Loss: 0.1093 | 0.1184
Epoch 263/300, trend Loss: 0.1087 | 0.1171
Epoch 264/300, trend Loss: 0.1082 | 0.1178
Epoch 265/300, trend Loss: 0.1092 | 0.1176
Epoch 266/300, trend Loss: 0.1080 | 0.1183
Epoch 267/300, trend Loss: 0.1086 | 0.1175
Epoch 268/300, trend Loss: 0.1082 | 0.1170
Epoch 269/300, trend Loss: 0.1084 | 0.1169
Epoch 270/300, trend Loss: 0.1079 | 0.1177
Epoch 271/300, trend Loss: 0.1077 | 0.1174
Epoch 272/300, trend Loss: 0.1082 | 0.1175
Epoch 273/300, trend Loss: 0.1079 | 0.1164
Epoch 274/300, trend Loss: 0.1076 | 0.1176
Epoch 275/300, trend Loss: 0.1075 | 0.1173
Epoch 276/300, trend Loss: 0.1073 | 0.1168
Epoch 277/300, trend Loss: 0.1077 | 0.1175
Epoch 278/300, trend Loss: 0.1067 | 0.1171
Epoch 279/300, trend Loss: 0.1088 | 0.1173
Epoch 280/300, trend Loss: 0.1074 | 0.1180
Epoch 281/300, trend Loss: 0.1069 | 0.1167
Epoch 282/300, trend Loss: 0.1071 | 0.1160
Epoch 283/300, trend Loss: 0.1069 | 0.1172
Epoch 284/300, trend Loss: 0.1064 | 0.1176
Epoch 285/300, trend Loss: 0.1063 | 0.1168
Epoch 286/300, trend Loss: 0.1067 | 0.1164
Epoch 287/300, trend Loss: 0.1066 | 0.1163
Epoch 288/300, trend Loss: 0.1067 | 0.1150
Epoch 289/300, trend Loss: 0.1070 | 0.1172
Epoch 290/300, trend Loss: 0.1062 | 0.1160
Epoch 291/300, trend Loss: 0.1064 | 0.1161
Epoch 292/300, trend Loss: 0.1062 | 0.1164
Epoch 293/300, trend Loss: 0.1062 | 0.1170
Epoch 294/300, trend Loss: 0.1060 | 0.1159
Epoch 295/300, trend Loss: 0.1058 | 0.1162
Epoch 296/300, trend Loss: 0.1054 | 0.1159
Epoch 297/300, trend Loss: 0.1052 | 0.1146
Epoch 298/300, trend Loss: 0.1052 | 0.1158
Epoch 299/300, trend Loss: 0.1053 | 0.1161
Epoch 300/300, trend Loss: 0.1053 | 0.1150
Training seasonal_0 component with params: {'observation_period_num': 122, 'train_rates': 0.9740328603729431, 'learning_rate': 0.00014018728210616471, 'batch_size': 19, 'step_size': 14, 'gamma': 0.8237019071778787}
Epoch 1/300, seasonal_0 Loss: 0.5626 | 0.8335
Epoch 2/300, seasonal_0 Loss: 0.4538 | 0.7333
Epoch 3/300, seasonal_0 Loss: 0.3918 | 0.5908
Epoch 4/300, seasonal_0 Loss: 0.3535 | 0.5831
Epoch 5/300, seasonal_0 Loss: 0.3346 | 0.5229
Epoch 6/300, seasonal_0 Loss: 0.3318 | 0.4653
Epoch 7/300, seasonal_0 Loss: 0.3033 | 0.3960
Epoch 8/300, seasonal_0 Loss: 0.3053 | 0.4913
Epoch 9/300, seasonal_0 Loss: 0.2923 | 0.4333
Epoch 10/300, seasonal_0 Loss: 0.2734 | 0.3783
Epoch 11/300, seasonal_0 Loss: 0.2643 | 0.3556
Epoch 12/300, seasonal_0 Loss: 0.2533 | 0.3587
Epoch 13/300, seasonal_0 Loss: 0.2585 | 0.3071
Epoch 14/300, seasonal_0 Loss: 0.2462 | 0.3299
Epoch 15/300, seasonal_0 Loss: 0.2461 | 0.3655
Epoch 16/300, seasonal_0 Loss: 0.2376 | 0.3033
Epoch 17/300, seasonal_0 Loss: 0.2432 | 0.3183
Epoch 18/300, seasonal_0 Loss: 0.2396 | 0.3262
Epoch 19/300, seasonal_0 Loss: 0.2203 | 0.3235
Epoch 20/300, seasonal_0 Loss: 0.2317 | 0.2841
Epoch 21/300, seasonal_0 Loss: 0.2124 | 0.2685
Epoch 22/300, seasonal_0 Loss: 0.2152 | 0.2687
Epoch 23/300, seasonal_0 Loss: 0.2055 | 0.2953
Epoch 24/300, seasonal_0 Loss: 0.2037 | 0.2554
Epoch 25/300, seasonal_0 Loss: 0.1991 | 0.2280
Epoch 26/300, seasonal_0 Loss: 0.1963 | 0.2362
Epoch 27/300, seasonal_0 Loss: 0.1951 | 0.2902
Epoch 28/300, seasonal_0 Loss: 0.1911 | 0.2347
Epoch 29/300, seasonal_0 Loss: 0.1964 | 0.2338
Epoch 30/300, seasonal_0 Loss: 0.1896 | 0.2547
Epoch 31/300, seasonal_0 Loss: 0.1708 | 0.2427
Epoch 32/300, seasonal_0 Loss: 0.1696 | 0.2409
Epoch 33/300, seasonal_0 Loss: 0.1637 | 0.2169
Epoch 34/300, seasonal_0 Loss: 0.1612 | 0.2124
Epoch 35/300, seasonal_0 Loss: 0.1584 | 0.2263
Epoch 36/300, seasonal_0 Loss: 0.1617 | 0.2098
Epoch 37/300, seasonal_0 Loss: 0.1599 | 0.2140
Epoch 38/300, seasonal_0 Loss: 0.1600 | 0.2217
Epoch 39/300, seasonal_0 Loss: 0.1532 | 0.2155
Epoch 40/300, seasonal_0 Loss: 0.1524 | 0.1948
Epoch 41/300, seasonal_0 Loss: 0.1513 | 0.1861
Epoch 42/300, seasonal_0 Loss: 0.1519 | 0.1944
Epoch 43/300, seasonal_0 Loss: 0.1499 | 0.2065
Epoch 44/300, seasonal_0 Loss: 0.1462 | 0.1860
Epoch 45/300, seasonal_0 Loss: 0.1430 | 0.1749
Epoch 46/300, seasonal_0 Loss: 0.1438 | 0.1789
Epoch 47/300, seasonal_0 Loss: 0.1407 | 0.1834
Epoch 48/300, seasonal_0 Loss: 0.1395 | 0.1776
Epoch 49/300, seasonal_0 Loss: 0.1406 | 0.1738
Epoch 50/300, seasonal_0 Loss: 0.1431 | 0.1744
Epoch 51/300, seasonal_0 Loss: 0.1437 | 0.2100
Epoch 52/300, seasonal_0 Loss: 0.1393 | 0.2034
Epoch 53/300, seasonal_0 Loss: 0.1364 | 0.1627
Epoch 54/300, seasonal_0 Loss: 0.1371 | 0.1564
Epoch 55/300, seasonal_0 Loss: 0.1378 | 0.1913
Epoch 56/300, seasonal_0 Loss: 0.1359 | 0.2116
Epoch 57/300, seasonal_0 Loss: 0.1346 | 0.1746
Epoch 58/300, seasonal_0 Loss: 0.1344 | 0.1552
Epoch 59/300, seasonal_0 Loss: 0.1341 | 0.1627
Epoch 60/300, seasonal_0 Loss: 0.1298 | 0.1951
Epoch 61/300, seasonal_0 Loss: 0.1302 | 0.1963
Epoch 62/300, seasonal_0 Loss: 0.1265 | 0.1712
Epoch 63/300, seasonal_0 Loss: 0.1279 | 0.1549
Epoch 64/300, seasonal_0 Loss: 0.1267 | 0.1616
Epoch 65/300, seasonal_0 Loss: 0.1257 | 0.1905
Epoch 66/300, seasonal_0 Loss: 0.1253 | 0.1886
Epoch 67/300, seasonal_0 Loss: 0.1217 | 0.1652
Epoch 68/300, seasonal_0 Loss: 0.1204 | 0.1519
Epoch 69/300, seasonal_0 Loss: 0.1205 | 0.1607
Epoch 70/300, seasonal_0 Loss: 0.1195 | 0.1860
Epoch 71/300, seasonal_0 Loss: 0.1193 | 0.1722
Epoch 72/300, seasonal_0 Loss: 0.1166 | 0.1553
Epoch 73/300, seasonal_0 Loss: 0.1160 | 0.1502
Epoch 74/300, seasonal_0 Loss: 0.1156 | 0.1646
Epoch 75/300, seasonal_0 Loss: 0.1145 | 0.1754
Epoch 76/300, seasonal_0 Loss: 0.1142 | 0.1688
Epoch 77/300, seasonal_0 Loss: 0.1136 | 0.1555
Epoch 78/300, seasonal_0 Loss: 0.1131 | 0.1487
Epoch 79/300, seasonal_0 Loss: 0.1120 | 0.1643
Epoch 80/300, seasonal_0 Loss: 0.1121 | 0.1729
Epoch 81/300, seasonal_0 Loss: 0.1112 | 0.1599
Epoch 82/300, seasonal_0 Loss: 0.1101 | 0.1525
Epoch 83/300, seasonal_0 Loss: 0.1102 | 0.1536
Epoch 84/300, seasonal_0 Loss: 0.1097 | 0.1625
Epoch 85/300, seasonal_0 Loss: 0.1090 | 0.1677
Epoch 86/300, seasonal_0 Loss: 0.1087 | 0.1574
Epoch 87/300, seasonal_0 Loss: 0.1085 | 0.1521
Epoch 88/300, seasonal_0 Loss: 0.1070 | 0.1570
Epoch 89/300, seasonal_0 Loss: 0.1070 | 0.1574
Epoch 90/300, seasonal_0 Loss: 0.1075 | 0.1550
Epoch 91/300, seasonal_0 Loss: 0.1065 | 0.1539
Epoch 92/300, seasonal_0 Loss: 0.1057 | 0.1526
Epoch 93/300, seasonal_0 Loss: 0.1050 | 0.1535
Epoch 94/300, seasonal_0 Loss: 0.1050 | 0.1536
Epoch 95/300, seasonal_0 Loss: 0.1042 | 0.1551
Epoch 96/300, seasonal_0 Loss: 0.1048 | 0.1505
Epoch 97/300, seasonal_0 Loss: 0.1053 | 0.1489
Epoch 98/300, seasonal_0 Loss: 0.1032 | 0.1506
Epoch 99/300, seasonal_0 Loss: 0.1035 | 0.1545
Epoch 100/300, seasonal_0 Loss: 0.1028 | 0.1528
Epoch 101/300, seasonal_0 Loss: 0.1029 | 0.1541
Epoch 102/300, seasonal_0 Loss: 0.1033 | 0.1538
Epoch 103/300, seasonal_0 Loss: 0.1030 | 0.1526
Epoch 104/300, seasonal_0 Loss: 0.1024 | 0.1537
Epoch 105/300, seasonal_0 Loss: 0.1026 | 0.1528
Epoch 106/300, seasonal_0 Loss: 0.1014 | 0.1519
Epoch 107/300, seasonal_0 Loss: 0.1016 | 0.1506
Epoch 108/300, seasonal_0 Loss: 0.1017 | 0.1523
Epoch 109/300, seasonal_0 Loss: 0.1009 | 0.1511
Epoch 110/300, seasonal_0 Loss: 0.1014 | 0.1538
Epoch 111/300, seasonal_0 Loss: 0.1009 | 0.1510
Epoch 112/300, seasonal_0 Loss: 0.1000 | 0.1504
Epoch 113/300, seasonal_0 Loss: 0.0995 | 0.1510
Epoch 114/300, seasonal_0 Loss: 0.0995 | 0.1499
Epoch 115/300, seasonal_0 Loss: 0.1002 | 0.1536
Epoch 116/300, seasonal_0 Loss: 0.0990 | 0.1522
Epoch 117/300, seasonal_0 Loss: 0.0994 | 0.1501
Epoch 118/300, seasonal_0 Loss: 0.0994 | 0.1507
Epoch 119/300, seasonal_0 Loss: 0.0989 | 0.1497
Epoch 120/300, seasonal_0 Loss: 0.0996 | 0.1505
Epoch 121/300, seasonal_0 Loss: 0.0983 | 0.1508
Epoch 122/300, seasonal_0 Loss: 0.0991 | 0.1480
Epoch 123/300, seasonal_0 Loss: 0.0985 | 0.1495
Epoch 124/300, seasonal_0 Loss: 0.0977 | 0.1507
Epoch 125/300, seasonal_0 Loss: 0.0980 | 0.1510
Epoch 126/300, seasonal_0 Loss: 0.0984 | 0.1511
Epoch 127/300, seasonal_0 Loss: 0.0987 | 0.1511
Epoch 128/300, seasonal_0 Loss: 0.0985 | 0.1504
Epoch 129/300, seasonal_0 Loss: 0.0976 | 0.1494
Epoch 130/300, seasonal_0 Loss: 0.0978 | 0.1514
Epoch 131/300, seasonal_0 Loss: 0.0979 | 0.1509
Epoch 132/300, seasonal_0 Loss: 0.0974 | 0.1505
Epoch 133/300, seasonal_0 Loss: 0.0980 | 0.1513
Epoch 134/300, seasonal_0 Loss: 0.0968 | 0.1501
Epoch 135/300, seasonal_0 Loss: 0.0966 | 0.1512
Epoch 136/300, seasonal_0 Loss: 0.0963 | 0.1504
Epoch 137/300, seasonal_0 Loss: 0.0969 | 0.1514
Epoch 138/300, seasonal_0 Loss: 0.0967 | 0.1503
Epoch 139/300, seasonal_0 Loss: 0.0962 | 0.1515
Epoch 140/300, seasonal_0 Loss: 0.0967 | 0.1498
Epoch 141/300, seasonal_0 Loss: 0.0972 | 0.1503
Epoch 142/300, seasonal_0 Loss: 0.0958 | 0.1498
Epoch 143/300, seasonal_0 Loss: 0.0957 | 0.1500
Epoch 144/300, seasonal_0 Loss: 0.0963 | 0.1497
Epoch 145/300, seasonal_0 Loss: 0.0957 | 0.1492
Epoch 146/300, seasonal_0 Loss: 0.0957 | 0.1492
Epoch 147/300, seasonal_0 Loss: 0.0956 | 0.1495
Epoch 148/300, seasonal_0 Loss: 0.0958 | 0.1494
Epoch 149/300, seasonal_0 Loss: 0.0966 | 0.1488
Epoch 150/300, seasonal_0 Loss: 0.0953 | 0.1497
Epoch 151/300, seasonal_0 Loss: 0.0963 | 0.1503
Epoch 152/300, seasonal_0 Loss: 0.0952 | 0.1496
Epoch 153/300, seasonal_0 Loss: 0.0951 | 0.1498
Epoch 154/300, seasonal_0 Loss: 0.0957 | 0.1494
Epoch 155/300, seasonal_0 Loss: 0.0955 | 0.1498
Epoch 156/300, seasonal_0 Loss: 0.0952 | 0.1500
Epoch 157/300, seasonal_0 Loss: 0.0954 | 0.1493
Epoch 158/300, seasonal_0 Loss: 0.0950 | 0.1494
Epoch 159/300, seasonal_0 Loss: 0.0942 | 0.1491
Epoch 160/300, seasonal_0 Loss: 0.0961 | 0.1494
Epoch 161/300, seasonal_0 Loss: 0.0952 | 0.1490
Epoch 162/300, seasonal_0 Loss: 0.0961 | 0.1488
Epoch 163/300, seasonal_0 Loss: 0.0947 | 0.1490
Epoch 164/300, seasonal_0 Loss: 0.0952 | 0.1489
Epoch 165/300, seasonal_0 Loss: 0.0946 | 0.1492
Epoch 166/300, seasonal_0 Loss: 0.0948 | 0.1492
Epoch 167/300, seasonal_0 Loss: 0.0946 | 0.1485
Epoch 168/300, seasonal_0 Loss: 0.0948 | 0.1488
Epoch 169/300, seasonal_0 Loss: 0.0948 | 0.1492
Epoch 170/300, seasonal_0 Loss: 0.0943 | 0.1494
Epoch 171/300, seasonal_0 Loss: 0.0953 | 0.1498
Epoch 172/300, seasonal_0 Loss: 0.0947 | 0.1495
Epoch 173/300, seasonal_0 Loss: 0.0944 | 0.1498
Epoch 174/300, seasonal_0 Loss: 0.0954 | 0.1502
Epoch 175/300, seasonal_0 Loss: 0.0954 | 0.1498
Epoch 176/300, seasonal_0 Loss: 0.0938 | 0.1504
Epoch 177/300, seasonal_0 Loss: 0.0946 | 0.1500
Epoch 178/300, seasonal_0 Loss: 0.0941 | 0.1496
Epoch 179/300, seasonal_0 Loss: 0.0951 | 0.1492
Epoch 180/300, seasonal_0 Loss: 0.0941 | 0.1495
Epoch 181/300, seasonal_0 Loss: 0.0940 | 0.1499
Epoch 182/300, seasonal_0 Loss: 0.0949 | 0.1504
Epoch 183/300, seasonal_0 Loss: 0.0955 | 0.1502
Epoch 184/300, seasonal_0 Loss: 0.0946 | 0.1498
Epoch 185/300, seasonal_0 Loss: 0.0950 | 0.1495
Epoch 186/300, seasonal_0 Loss: 0.0942 | 0.1495
Epoch 187/300, seasonal_0 Loss: 0.0944 | 0.1495
Epoch 188/300, seasonal_0 Loss: 0.0943 | 0.1496
Epoch 189/300, seasonal_0 Loss: 0.0942 | 0.1498
Epoch 190/300, seasonal_0 Loss: 0.0938 | 0.1496
Epoch 191/300, seasonal_0 Loss: 0.0942 | 0.1492
Epoch 192/300, seasonal_0 Loss: 0.0942 | 0.1492
Epoch 193/300, seasonal_0 Loss: 0.0943 | 0.1489
Epoch 194/300, seasonal_0 Loss: 0.0949 | 0.1490
Epoch 195/300, seasonal_0 Loss: 0.0945 | 0.1487
Epoch 196/300, seasonal_0 Loss: 0.0943 | 0.1488
Epoch 197/300, seasonal_0 Loss: 0.0939 | 0.1488
Epoch 198/300, seasonal_0 Loss: 0.0939 | 0.1483
Epoch 199/300, seasonal_0 Loss: 0.0942 | 0.1485
Epoch 200/300, seasonal_0 Loss: 0.0932 | 0.1487
Epoch 201/300, seasonal_0 Loss: 0.0943 | 0.1488
Epoch 202/300, seasonal_0 Loss: 0.0938 | 0.1492
Epoch 203/300, seasonal_0 Loss: 0.0945 | 0.1491
Epoch 204/300, seasonal_0 Loss: 0.0945 | 0.1490
Epoch 205/300, seasonal_0 Loss: 0.0949 | 0.1491
Epoch 206/300, seasonal_0 Loss: 0.0944 | 0.1492
Epoch 207/300, seasonal_0 Loss: 0.0951 | 0.1490
Epoch 208/300, seasonal_0 Loss: 0.0933 | 0.1491
Epoch 209/300, seasonal_0 Loss: 0.0935 | 0.1491
Epoch 210/300, seasonal_0 Loss: 0.0943 | 0.1492
Epoch 211/300, seasonal_0 Loss: 0.0933 | 0.1492
Epoch 212/300, seasonal_0 Loss: 0.0938 | 0.1492
Epoch 213/300, seasonal_0 Loss: 0.0949 | 0.1491
Epoch 214/300, seasonal_0 Loss: 0.0937 | 0.1489
Epoch 215/300, seasonal_0 Loss: 0.0938 | 0.1488
Epoch 216/300, seasonal_0 Loss: 0.0945 | 0.1488
Epoch 217/300, seasonal_0 Loss: 0.0931 | 0.1487
Epoch 218/300, seasonal_0 Loss: 0.0938 | 0.1486
Epoch 219/300, seasonal_0 Loss: 0.0942 | 0.1487
Epoch 220/300, seasonal_0 Loss: 0.0933 | 0.1485
Epoch 221/300, seasonal_0 Loss: 0.0938 | 0.1485
Epoch 222/300, seasonal_0 Loss: 0.0938 | 0.1485
Epoch 223/300, seasonal_0 Loss: 0.0946 | 0.1482
Epoch 224/300, seasonal_0 Loss: 0.0942 | 0.1482
Epoch 225/300, seasonal_0 Loss: 0.0942 | 0.1481
Epoch 226/300, seasonal_0 Loss: 0.0938 | 0.1482
Epoch 227/300, seasonal_0 Loss: 0.0935 | 0.1484
Epoch 228/300, seasonal_0 Loss: 0.0935 | 0.1482
Epoch 229/300, seasonal_0 Loss: 0.0931 | 0.1483
Epoch 230/300, seasonal_0 Loss: 0.0931 | 0.1483
Epoch 231/300, seasonal_0 Loss: 0.0936 | 0.1482
Epoch 232/300, seasonal_0 Loss: 0.0940 | 0.1483
Epoch 233/300, seasonal_0 Loss: 0.0939 | 0.1484
Epoch 234/300, seasonal_0 Loss: 0.0944 | 0.1484
Epoch 235/300, seasonal_0 Loss: 0.0946 | 0.1485
Epoch 236/300, seasonal_0 Loss: 0.0936 | 0.1483
Epoch 237/300, seasonal_0 Loss: 0.0939 | 0.1483
Epoch 238/300, seasonal_0 Loss: 0.0932 | 0.1485
Epoch 239/300, seasonal_0 Loss: 0.0936 | 0.1486
Epoch 240/300, seasonal_0 Loss: 0.0940 | 0.1487
Epoch 241/300, seasonal_0 Loss: 0.0942 | 0.1486
Epoch 242/300, seasonal_0 Loss: 0.0938 | 0.1486
Epoch 243/300, seasonal_0 Loss: 0.0938 | 0.1487
Epoch 244/300, seasonal_0 Loss: 0.0937 | 0.1487
Epoch 245/300, seasonal_0 Loss: 0.0933 | 0.1485
Epoch 246/300, seasonal_0 Loss: 0.0934 | 0.1487
Epoch 247/300, seasonal_0 Loss: 0.0937 | 0.1488
Epoch 248/300, seasonal_0 Loss: 0.0934 | 0.1488
Epoch 249/300, seasonal_0 Loss: 0.0931 | 0.1487
Epoch 250/300, seasonal_0 Loss: 0.0928 | 0.1487
Epoch 251/300, seasonal_0 Loss: 0.0943 | 0.1488
Epoch 252/300, seasonal_0 Loss: 0.0934 | 0.1488
Epoch 253/300, seasonal_0 Loss: 0.0944 | 0.1488
Epoch 254/300, seasonal_0 Loss: 0.0934 | 0.1488
Epoch 255/300, seasonal_0 Loss: 0.0931 | 0.1489
Epoch 256/300, seasonal_0 Loss: 0.0933 | 0.1488
Epoch 257/300, seasonal_0 Loss: 0.0939 | 0.1487
Epoch 258/300, seasonal_0 Loss: 0.0934 | 0.1487
Epoch 259/300, seasonal_0 Loss: 0.0944 | 0.1487
Epoch 260/300, seasonal_0 Loss: 0.0933 | 0.1487
Epoch 261/300, seasonal_0 Loss: 0.0938 | 0.1487
Epoch 262/300, seasonal_0 Loss: 0.0934 | 0.1487
Epoch 263/300, seasonal_0 Loss: 0.0939 | 0.1488
Epoch 264/300, seasonal_0 Loss: 0.0938 | 0.1487
Epoch 265/300, seasonal_0 Loss: 0.0935 | 0.1487
Epoch 266/300, seasonal_0 Loss: 0.0931 | 0.1487
Epoch 267/300, seasonal_0 Loss: 0.0932 | 0.1487
Epoch 268/300, seasonal_0 Loss: 0.0935 | 0.1486
Epoch 269/300, seasonal_0 Loss: 0.0937 | 0.1486
Epoch 270/300, seasonal_0 Loss: 0.0937 | 0.1486
Epoch 271/300, seasonal_0 Loss: 0.0939 | 0.1486
Epoch 272/300, seasonal_0 Loss: 0.0940 | 0.1486
Epoch 273/300, seasonal_0 Loss: 0.0937 | 0.1486
Epoch 274/300, seasonal_0 Loss: 0.0935 | 0.1486
Epoch 275/300, seasonal_0 Loss: 0.0935 | 0.1485
Epoch 276/300, seasonal_0 Loss: 0.0933 | 0.1485
Epoch 277/300, seasonal_0 Loss: 0.0937 | 0.1485
Epoch 278/300, seasonal_0 Loss: 0.0935 | 0.1485
Epoch 279/300, seasonal_0 Loss: 0.0933 | 0.1485
Epoch 280/300, seasonal_0 Loss: 0.0936 | 0.1485
Epoch 281/300, seasonal_0 Loss: 0.0934 | 0.1485
Epoch 282/300, seasonal_0 Loss: 0.0943 | 0.1485
Epoch 283/300, seasonal_0 Loss: 0.0934 | 0.1485
Epoch 284/300, seasonal_0 Loss: 0.0937 | 0.1485
Epoch 285/300, seasonal_0 Loss: 0.0936 | 0.1485
Epoch 286/300, seasonal_0 Loss: 0.0944 | 0.1485
Epoch 287/300, seasonal_0 Loss: 0.0943 | 0.1485
Epoch 288/300, seasonal_0 Loss: 0.0935 | 0.1485
Epoch 289/300, seasonal_0 Loss: 0.0933 | 0.1485
Epoch 290/300, seasonal_0 Loss: 0.0938 | 0.1485
Epoch 291/300, seasonal_0 Loss: 0.0946 | 0.1485
Epoch 292/300, seasonal_0 Loss: 0.0937 | 0.1486
Epoch 293/300, seasonal_0 Loss: 0.0942 | 0.1486
Epoch 294/300, seasonal_0 Loss: 0.0935 | 0.1486
Epoch 295/300, seasonal_0 Loss: 0.0933 | 0.1486
Epoch 296/300, seasonal_0 Loss: 0.0930 | 0.1486
Epoch 297/300, seasonal_0 Loss: 0.0939 | 0.1486
Epoch 298/300, seasonal_0 Loss: 0.0930 | 0.1486
Epoch 299/300, seasonal_0 Loss: 0.0930 | 0.1486
Epoch 300/300, seasonal_0 Loss: 0.0934 | 0.1486
Training seasonal_1 component with params: {'observation_period_num': 101, 'train_rates': 0.9896361259514996, 'learning_rate': 0.00012137513841261055, 'batch_size': 34, 'step_size': 8, 'gamma': 0.7653040738242892}
Epoch 1/300, seasonal_1 Loss: 0.8135 | 1.1641
Epoch 2/300, seasonal_1 Loss: 0.6715 | 1.0029
Epoch 3/300, seasonal_1 Loss: 0.5604 | 0.8802
Epoch 4/300, seasonal_1 Loss: 0.5057 | 0.7363
Epoch 5/300, seasonal_1 Loss: 0.4903 | 0.7237
Epoch 6/300, seasonal_1 Loss: 0.4190 | 0.6499
Epoch 7/300, seasonal_1 Loss: 0.3389 | 0.5832
Epoch 8/300, seasonal_1 Loss: 0.3074 | 0.5675
Epoch 9/300, seasonal_1 Loss: 0.2829 | 0.5110
Epoch 10/300, seasonal_1 Loss: 0.2779 | 0.4699
Epoch 11/300, seasonal_1 Loss: 0.2656 | 0.4802
Epoch 12/300, seasonal_1 Loss: 0.2512 | 0.4470
Epoch 13/300, seasonal_1 Loss: 0.2446 | 0.4088
Epoch 14/300, seasonal_1 Loss: 0.2354 | 0.4008
Epoch 15/300, seasonal_1 Loss: 0.2337 | 0.3968
Epoch 16/300, seasonal_1 Loss: 0.2185 | 0.3788
Epoch 17/300, seasonal_1 Loss: 0.2154 | 0.3540
Epoch 18/300, seasonal_1 Loss: 0.2130 | 0.3831
Epoch 19/300, seasonal_1 Loss: 0.2065 | 0.3595
Epoch 20/300, seasonal_1 Loss: 0.2055 | 0.3225
Epoch 21/300, seasonal_1 Loss: 0.2022 | 0.3276
Epoch 22/300, seasonal_1 Loss: 0.2016 | 0.3537
Epoch 23/300, seasonal_1 Loss: 0.1955 | 0.3094
Epoch 24/300, seasonal_1 Loss: 0.1926 | 0.2997
Epoch 25/300, seasonal_1 Loss: 0.1925 | 0.3156
Epoch 26/300, seasonal_1 Loss: 0.1882 | 0.3014
Epoch 27/300, seasonal_1 Loss: 0.1858 | 0.2937
Epoch 28/300, seasonal_1 Loss: 0.1841 | 0.2911
Epoch 29/300, seasonal_1 Loss: 0.1826 | 0.2900
Epoch 30/300, seasonal_1 Loss: 0.1803 | 0.2794
Epoch 31/300, seasonal_1 Loss: 0.1801 | 0.2775
Epoch 32/300, seasonal_1 Loss: 0.1787 | 0.2785
Epoch 33/300, seasonal_1 Loss: 0.1777 | 0.2730
Epoch 34/300, seasonal_1 Loss: 0.1761 | 0.2676
Epoch 35/300, seasonal_1 Loss: 0.1755 | 0.2654
Epoch 36/300, seasonal_1 Loss: 0.1746 | 0.2664
Epoch 37/300, seasonal_1 Loss: 0.1737 | 0.2610
Epoch 38/300, seasonal_1 Loss: 0.1725 | 0.2599
Epoch 39/300, seasonal_1 Loss: 0.1726 | 0.2575
Epoch 40/300, seasonal_1 Loss: 0.1713 | 0.2539
Epoch 41/300, seasonal_1 Loss: 0.1708 | 0.2539
Epoch 42/300, seasonal_1 Loss: 0.1701 | 0.2536
Epoch 43/300, seasonal_1 Loss: 0.1700 | 0.2481
Epoch 44/300, seasonal_1 Loss: 0.1695 | 0.2488
Epoch 45/300, seasonal_1 Loss: 0.1688 | 0.2450
Epoch 46/300, seasonal_1 Loss: 0.1686 | 0.2445
Epoch 47/300, seasonal_1 Loss: 0.1689 | 0.2443
Epoch 48/300, seasonal_1 Loss: 0.1675 | 0.2435
Epoch 49/300, seasonal_1 Loss: 0.1673 | 0.2417
Epoch 50/300, seasonal_1 Loss: 0.1672 | 0.2412
Epoch 51/300, seasonal_1 Loss: 0.1663 | 0.2407
Epoch 52/300, seasonal_1 Loss: 0.1662 | 0.2387
Epoch 53/300, seasonal_1 Loss: 0.1667 | 0.2383
Epoch 54/300, seasonal_1 Loss: 0.1653 | 0.2379
Epoch 55/300, seasonal_1 Loss: 0.1654 | 0.2370
Epoch 56/300, seasonal_1 Loss: 0.1663 | 0.2360
Epoch 57/300, seasonal_1 Loss: 0.1656 | 0.2353
Epoch 58/300, seasonal_1 Loss: 0.1650 | 0.2347
Epoch 59/300, seasonal_1 Loss: 0.1641 | 0.2351
Epoch 60/300, seasonal_1 Loss: 0.1640 | 0.2344
Epoch 61/300, seasonal_1 Loss: 0.1638 | 0.2339
Epoch 62/300, seasonal_1 Loss: 0.1643 | 0.2331
Epoch 63/300, seasonal_1 Loss: 0.1633 | 0.2339
Epoch 64/300, seasonal_1 Loss: 0.1640 | 0.2323
Epoch 65/300, seasonal_1 Loss: 0.1641 | 0.2321
Epoch 66/300, seasonal_1 Loss: 0.1630 | 0.2321
Epoch 67/300, seasonal_1 Loss: 0.1632 | 0.2311
Epoch 68/300, seasonal_1 Loss: 0.1634 | 0.2305
Epoch 69/300, seasonal_1 Loss: 0.1629 | 0.2309
Epoch 70/300, seasonal_1 Loss: 0.1634 | 0.2306
Epoch 71/300, seasonal_1 Loss: 0.1623 | 0.2305
Epoch 72/300, seasonal_1 Loss: 0.1625 | 0.2299
Epoch 73/300, seasonal_1 Loss: 0.1636 | 0.2295
Epoch 74/300, seasonal_1 Loss: 0.1626 | 0.2294
Epoch 75/300, seasonal_1 Loss: 0.1626 | 0.2293
Epoch 76/300, seasonal_1 Loss: 0.1620 | 0.2293
Epoch 77/300, seasonal_1 Loss: 0.1623 | 0.2295
Epoch 78/300, seasonal_1 Loss: 0.1620 | 0.2291
Epoch 79/300, seasonal_1 Loss: 0.1622 | 0.2291
Epoch 80/300, seasonal_1 Loss: 0.1629 | 0.2289
Epoch 81/300, seasonal_1 Loss: 0.1627 | 0.2284
Epoch 82/300, seasonal_1 Loss: 0.1624 | 0.2283
Epoch 83/300, seasonal_1 Loss: 0.1624 | 0.2283
Epoch 84/300, seasonal_1 Loss: 0.1617 | 0.2282
Epoch 85/300, seasonal_1 Loss: 0.1621 | 0.2281
Epoch 86/300, seasonal_1 Loss: 0.1620 | 0.2281
Epoch 87/300, seasonal_1 Loss: 0.1620 | 0.2280
Epoch 88/300, seasonal_1 Loss: 0.1619 | 0.2280
Epoch 89/300, seasonal_1 Loss: 0.1621 | 0.2282
Epoch 90/300, seasonal_1 Loss: 0.1627 | 0.2283
Epoch 91/300, seasonal_1 Loss: 0.1616 | 0.2282
Epoch 92/300, seasonal_1 Loss: 0.1618 | 0.2281
Epoch 93/300, seasonal_1 Loss: 0.1618 | 0.2280
Epoch 94/300, seasonal_1 Loss: 0.1617 | 0.2279
Epoch 95/300, seasonal_1 Loss: 0.1617 | 0.2277
Epoch 96/300, seasonal_1 Loss: 0.1623 | 0.2276
Epoch 97/300, seasonal_1 Loss: 0.1621 | 0.2276
Epoch 98/300, seasonal_1 Loss: 0.1611 | 0.2274
Epoch 99/300, seasonal_1 Loss: 0.1620 | 0.2273
Epoch 100/300, seasonal_1 Loss: 0.1617 | 0.2272
Epoch 101/300, seasonal_1 Loss: 0.1620 | 0.2272
Epoch 102/300, seasonal_1 Loss: 0.1619 | 0.2272
Epoch 103/300, seasonal_1 Loss: 0.1611 | 0.2272
Epoch 104/300, seasonal_1 Loss: 0.1622 | 0.2272
Epoch 105/300, seasonal_1 Loss: 0.1619 | 0.2271
Epoch 106/300, seasonal_1 Loss: 0.1623 | 0.2271
Epoch 107/300, seasonal_1 Loss: 0.1614 | 0.2271
Epoch 108/300, seasonal_1 Loss: 0.1619 | 0.2271
Epoch 109/300, seasonal_1 Loss: 0.1621 | 0.2271
Epoch 110/300, seasonal_1 Loss: 0.1614 | 0.2270
Epoch 111/300, seasonal_1 Loss: 0.1618 | 0.2270
Epoch 112/300, seasonal_1 Loss: 0.1621 | 0.2269
Epoch 113/300, seasonal_1 Loss: 0.1616 | 0.2270
Epoch 114/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 115/300, seasonal_1 Loss: 0.1619 | 0.2270
Epoch 116/300, seasonal_1 Loss: 0.1619 | 0.2270
Epoch 117/300, seasonal_1 Loss: 0.1621 | 0.2270
Epoch 118/300, seasonal_1 Loss: 0.1613 | 0.2270
Epoch 119/300, seasonal_1 Loss: 0.1613 | 0.2270
Epoch 120/300, seasonal_1 Loss: 0.1618 | 0.2270
Epoch 121/300, seasonal_1 Loss: 0.1617 | 0.2270
Epoch 122/300, seasonal_1 Loss: 0.1609 | 0.2270
Epoch 123/300, seasonal_1 Loss: 0.1621 | 0.2270
Epoch 124/300, seasonal_1 Loss: 0.1620 | 0.2270
Epoch 125/300, seasonal_1 Loss: 0.1611 | 0.2270
Epoch 126/300, seasonal_1 Loss: 0.1619 | 0.2270
Epoch 127/300, seasonal_1 Loss: 0.1622 | 0.2270
Epoch 128/300, seasonal_1 Loss: 0.1625 | 0.2270
Epoch 129/300, seasonal_1 Loss: 0.1616 | 0.2270
Epoch 130/300, seasonal_1 Loss: 0.1619 | 0.2270
Epoch 131/300, seasonal_1 Loss: 0.1614 | 0.2270
Epoch 132/300, seasonal_1 Loss: 0.1617 | 0.2270
Epoch 133/300, seasonal_1 Loss: 0.1612 | 0.2269
Epoch 134/300, seasonal_1 Loss: 0.1611 | 0.2269
Epoch 135/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 136/300, seasonal_1 Loss: 0.1621 | 0.2269
Epoch 137/300, seasonal_1 Loss: 0.1611 | 0.2269
Epoch 138/300, seasonal_1 Loss: 0.1612 | 0.2269
Epoch 139/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 140/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 141/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 142/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 143/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 144/300, seasonal_1 Loss: 0.1621 | 0.2269
Epoch 145/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 146/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 147/300, seasonal_1 Loss: 0.1612 | 0.2269
Epoch 148/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 149/300, seasonal_1 Loss: 0.1611 | 0.2269
Epoch 150/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 151/300, seasonal_1 Loss: 0.1624 | 0.2269
Epoch 152/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 153/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 154/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 155/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 156/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 157/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 158/300, seasonal_1 Loss: 0.1611 | 0.2269
Epoch 159/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 160/300, seasonal_1 Loss: 0.1620 | 0.2269
Epoch 161/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 162/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 163/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 164/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 165/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 166/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 167/300, seasonal_1 Loss: 0.1623 | 0.2269
Epoch 168/300, seasonal_1 Loss: 0.1621 | 0.2269
Epoch 169/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 170/300, seasonal_1 Loss: 0.1609 | 0.2269
Epoch 171/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 172/300, seasonal_1 Loss: 0.1622 | 0.2269
Epoch 173/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 174/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 175/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 176/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 177/300, seasonal_1 Loss: 0.1622 | 0.2269
Epoch 178/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 179/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 180/300, seasonal_1 Loss: 0.1608 | 0.2269
Epoch 181/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 182/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 183/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 184/300, seasonal_1 Loss: 0.1614 | 0.2269
Epoch 185/300, seasonal_1 Loss: 0.1613 | 0.2269
Epoch 186/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 187/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 188/300, seasonal_1 Loss: 0.1617 | 0.2269
Epoch 189/300, seasonal_1 Loss: 0.1619 | 0.2269
Epoch 190/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 191/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 192/300, seasonal_1 Loss: 0.1616 | 0.2269
Epoch 193/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 194/300, seasonal_1 Loss: 0.1618 | 0.2269
Epoch 195/300, seasonal_1 Loss: 0.1615 | 0.2269
Epoch 196/300, seasonal_1 Loss: 0.1621 | 0.2269
Epoch 197/300, seasonal_1 Loss: 0.1622 | 0.2269
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 148, 'train_rates': 0.9838912685837625, 'learning_rate': 0.0002359805738088743, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8126929224658732}
Epoch 1/300, seasonal_2 Loss: 0.5987 | 0.6379
Epoch 2/300, seasonal_2 Loss: 0.5084 | 0.5276
Epoch 3/300, seasonal_2 Loss: 0.4514 | 0.5323
Epoch 4/300, seasonal_2 Loss: 0.4947 | 0.4511
Epoch 5/300, seasonal_2 Loss: 0.4997 | 0.5161
Epoch 6/300, seasonal_2 Loss: 0.6424 | 0.5135
Epoch 7/300, seasonal_2 Loss: 0.5802 | 0.4726
Epoch 8/300, seasonal_2 Loss: 0.5220 | 0.4651
Epoch 9/300, seasonal_2 Loss: 0.4684 | 0.4379
Epoch 10/300, seasonal_2 Loss: 0.4580 | 0.4486
Epoch 11/300, seasonal_2 Loss: 0.5054 | 0.4291
Epoch 12/300, seasonal_2 Loss: 0.4641 | 0.3803
Epoch 13/300, seasonal_2 Loss: 0.4702 | 0.3652
Epoch 14/300, seasonal_2 Loss: 0.3843 | 0.3462
Epoch 15/300, seasonal_2 Loss: 0.3565 | 0.3274
Epoch 16/300, seasonal_2 Loss: 0.3276 | 0.3268
Epoch 17/300, seasonal_2 Loss: 0.3033 | 0.3266
Epoch 18/300, seasonal_2 Loss: 0.2980 | 0.3269
Epoch 19/300, seasonal_2 Loss: 0.2929 | 0.3205
Epoch 20/300, seasonal_2 Loss: 0.2748 | 0.3004
Epoch 21/300, seasonal_2 Loss: 0.2819 | 0.2944
Epoch 22/300, seasonal_2 Loss: 0.2617 | 0.2882
Epoch 23/300, seasonal_2 Loss: 0.2480 | 0.2884
Epoch 24/300, seasonal_2 Loss: 0.2470 | 0.3179
Epoch 25/300, seasonal_2 Loss: 0.2381 | 0.2781
Epoch 26/300, seasonal_2 Loss: 0.2359 | 0.2720
Epoch 27/300, seasonal_2 Loss: 0.2438 | 0.2890
Epoch 28/300, seasonal_2 Loss: 0.2356 | 0.3050
Epoch 29/300, seasonal_2 Loss: 0.2341 | 0.2908
Epoch 30/300, seasonal_2 Loss: 0.2430 | 0.2757
Epoch 31/300, seasonal_2 Loss: 0.2540 | 0.2725
Epoch 32/300, seasonal_2 Loss: 0.2461 | 0.2975
Epoch 33/300, seasonal_2 Loss: 0.2491 | 0.2730
Epoch 34/300, seasonal_2 Loss: 0.2285 | 0.2579
Epoch 35/300, seasonal_2 Loss: 0.2290 | 0.2667
Epoch 36/300, seasonal_2 Loss: 0.2172 | 0.2792
Epoch 37/300, seasonal_2 Loss: 0.2063 | 0.2620
Epoch 38/300, seasonal_2 Loss: 0.1984 | 0.2493
Epoch 39/300, seasonal_2 Loss: 0.1967 | 0.2506
Epoch 40/300, seasonal_2 Loss: 0.1934 | 0.2647
Epoch 41/300, seasonal_2 Loss: 0.1944 | 0.2539
Epoch 42/300, seasonal_2 Loss: 0.1868 | 0.2408
Epoch 43/300, seasonal_2 Loss: 0.1868 | 0.2386
Epoch 44/300, seasonal_2 Loss: 0.1844 | 0.2535
Epoch 45/300, seasonal_2 Loss: 0.1840 | 0.2554
Epoch 46/300, seasonal_2 Loss: 0.1820 | 0.2343
Epoch 47/300, seasonal_2 Loss: 0.1818 | 0.2292
Epoch 48/300, seasonal_2 Loss: 0.1804 | 0.2491
Epoch 49/300, seasonal_2 Loss: 0.1788 | 0.2415
Epoch 50/300, seasonal_2 Loss: 0.1745 | 0.2383
Epoch 51/300, seasonal_2 Loss: 0.1724 | 0.2355
Epoch 52/300, seasonal_2 Loss: 0.1726 | 0.2377
Epoch 53/300, seasonal_2 Loss: 0.1695 | 0.2265
Epoch 54/300, seasonal_2 Loss: 0.1674 | 0.2289
Epoch 55/300, seasonal_2 Loss: 0.1664 | 0.2292
Epoch 56/300, seasonal_2 Loss: 0.1650 | 0.2234
Epoch 57/300, seasonal_2 Loss: 0.1639 | 0.2218
Epoch 58/300, seasonal_2 Loss: 0.1635 | 0.2248
Epoch 59/300, seasonal_2 Loss: 0.1614 | 0.2261
Epoch 60/300, seasonal_2 Loss: 0.1590 | 0.2155
Epoch 61/300, seasonal_2 Loss: 0.1583 | 0.2180
Epoch 62/300, seasonal_2 Loss: 0.1584 | 0.2156
Epoch 63/300, seasonal_2 Loss: 0.1568 | 0.2154
Epoch 64/300, seasonal_2 Loss: 0.1556 | 0.2133
Epoch 65/300, seasonal_2 Loss: 0.1549 | 0.2143
Epoch 66/300, seasonal_2 Loss: 0.1555 | 0.2096
Epoch 67/300, seasonal_2 Loss: 0.1540 | 0.2117
Epoch 68/300, seasonal_2 Loss: 0.1533 | 0.2161
Epoch 69/300, seasonal_2 Loss: 0.1511 | 0.2123
Epoch 70/300, seasonal_2 Loss: 0.1511 | 0.2083
Epoch 71/300, seasonal_2 Loss: 0.1514 | 0.2088
Epoch 72/300, seasonal_2 Loss: 0.1507 | 0.2092
Epoch 73/300, seasonal_2 Loss: 0.1506 | 0.2089
Epoch 74/300, seasonal_2 Loss: 0.1487 | 0.2076
Epoch 75/300, seasonal_2 Loss: 0.1487 | 0.2091
Epoch 76/300, seasonal_2 Loss: 0.1476 | 0.2069
Epoch 77/300, seasonal_2 Loss: 0.1475 | 0.2068
Epoch 78/300, seasonal_2 Loss: 0.1471 | 0.2062
Epoch 79/300, seasonal_2 Loss: 0.1469 | 0.2067
Epoch 80/300, seasonal_2 Loss: 0.1463 | 0.2050
Epoch 81/300, seasonal_2 Loss: 0.1446 | 0.2080
Epoch 82/300, seasonal_2 Loss: 0.1450 | 0.2049
Epoch 83/300, seasonal_2 Loss: 0.1435 | 0.2075
Epoch 84/300, seasonal_2 Loss: 0.1451 | 0.2032
Epoch 85/300, seasonal_2 Loss: 0.1449 | 0.2042
Epoch 86/300, seasonal_2 Loss: 0.1444 | 0.2034
Epoch 87/300, seasonal_2 Loss: 0.1436 | 0.2031
Epoch 88/300, seasonal_2 Loss: 0.1431 | 0.2009
Epoch 89/300, seasonal_2 Loss: 0.1442 | 0.2036
Epoch 90/300, seasonal_2 Loss: 0.1423 | 0.2015
Epoch 91/300, seasonal_2 Loss: 0.1413 | 0.2008
Epoch 92/300, seasonal_2 Loss: 0.1426 | 0.2016
Epoch 93/300, seasonal_2 Loss: 0.1416 | 0.2037
Epoch 94/300, seasonal_2 Loss: 0.1424 | 0.2026
Epoch 95/300, seasonal_2 Loss: 0.1411 | 0.2020
Epoch 96/300, seasonal_2 Loss: 0.1406 | 0.1999
Epoch 97/300, seasonal_2 Loss: 0.1407 | 0.2003
Epoch 98/300, seasonal_2 Loss: 0.1410 | 0.2001
Epoch 99/300, seasonal_2 Loss: 0.1410 | 0.2012
Epoch 100/300, seasonal_2 Loss: 0.1403 | 0.2017
Epoch 101/300, seasonal_2 Loss: 0.1401 | 0.2016
Epoch 102/300, seasonal_2 Loss: 0.1385 | 0.1998
Epoch 103/300, seasonal_2 Loss: 0.1400 | 0.2003
Epoch 104/300, seasonal_2 Loss: 0.1404 | 0.1993
Epoch 105/300, seasonal_2 Loss: 0.1399 | 0.1991
Epoch 106/300, seasonal_2 Loss: 0.1396 | 0.1989
Epoch 107/300, seasonal_2 Loss: 0.1399 | 0.1985
Epoch 108/300, seasonal_2 Loss: 0.1390 | 0.1991
Epoch 109/300, seasonal_2 Loss: 0.1390 | 0.1983
Epoch 110/300, seasonal_2 Loss: 0.1395 | 0.1986
Epoch 111/300, seasonal_2 Loss: 0.1398 | 0.1987
Epoch 112/300, seasonal_2 Loss: 0.1383 | 0.1994
Epoch 113/300, seasonal_2 Loss: 0.1396 | 0.1980
Epoch 114/300, seasonal_2 Loss: 0.1393 | 0.1985
Epoch 115/300, seasonal_2 Loss: 0.1378 | 0.1996
Epoch 116/300, seasonal_2 Loss: 0.1390 | 0.1988
Epoch 117/300, seasonal_2 Loss: 0.1391 | 0.1984
Epoch 118/300, seasonal_2 Loss: 0.1381 | 0.1985
Epoch 119/300, seasonal_2 Loss: 0.1382 | 0.1993
Epoch 120/300, seasonal_2 Loss: 0.1385 | 0.1983
Epoch 121/300, seasonal_2 Loss: 0.1391 | 0.1982
Epoch 122/300, seasonal_2 Loss: 0.1377 | 0.1992
Epoch 123/300, seasonal_2 Loss: 0.1382 | 0.1987
Epoch 124/300, seasonal_2 Loss: 0.1380 | 0.1991
Epoch 125/300, seasonal_2 Loss: 0.1373 | 0.1986
Epoch 126/300, seasonal_2 Loss: 0.1385 | 0.1984
Epoch 127/300, seasonal_2 Loss: 0.1388 | 0.1981
Epoch 128/300, seasonal_2 Loss: 0.1391 | 0.1982
Epoch 129/300, seasonal_2 Loss: 0.1374 | 0.1982
Epoch 130/300, seasonal_2 Loss: 0.1382 | 0.1980
Epoch 131/300, seasonal_2 Loss: 0.1373 | 0.1989
Epoch 132/300, seasonal_2 Loss: 0.1381 | 0.1987
Epoch 133/300, seasonal_2 Loss: 0.1370 | 0.1986
Epoch 134/300, seasonal_2 Loss: 0.1370 | 0.1980
Epoch 135/300, seasonal_2 Loss: 0.1380 | 0.1979
Epoch 136/300, seasonal_2 Loss: 0.1373 | 0.1981
Epoch 137/300, seasonal_2 Loss: 0.1371 | 0.1985
Epoch 138/300, seasonal_2 Loss: 0.1377 | 0.1985
Epoch 139/300, seasonal_2 Loss: 0.1373 | 0.1983
Epoch 140/300, seasonal_2 Loss: 0.1369 | 0.1983
Epoch 141/300, seasonal_2 Loss: 0.1369 | 0.1983
Epoch 142/300, seasonal_2 Loss: 0.1382 | 0.1982
Epoch 143/300, seasonal_2 Loss: 0.1370 | 0.1983
Epoch 144/300, seasonal_2 Loss: 0.1377 | 0.1982
Epoch 145/300, seasonal_2 Loss: 0.1373 | 0.1979
Epoch 146/300, seasonal_2 Loss: 0.1372 | 0.1981
Epoch 147/300, seasonal_2 Loss: 0.1367 | 0.1980
Epoch 148/300, seasonal_2 Loss: 0.1372 | 0.1979
Epoch 149/300, seasonal_2 Loss: 0.1365 | 0.1979
Epoch 150/300, seasonal_2 Loss: 0.1375 | 0.1980
Epoch 151/300, seasonal_2 Loss: 0.1376 | 0.1978
Epoch 152/300, seasonal_2 Loss: 0.1364 | 0.1979
Epoch 153/300, seasonal_2 Loss: 0.1365 | 0.1979
Epoch 154/300, seasonal_2 Loss: 0.1378 | 0.1982
Epoch 155/300, seasonal_2 Loss: 0.1367 | 0.1980
Epoch 156/300, seasonal_2 Loss: 0.1365 | 0.1980
Epoch 157/300, seasonal_2 Loss: 0.1367 | 0.1978
Epoch 158/300, seasonal_2 Loss: 0.1369 | 0.1975
Epoch 159/300, seasonal_2 Loss: 0.1365 | 0.1974
Epoch 160/300, seasonal_2 Loss: 0.1381 | 0.1972
Epoch 161/300, seasonal_2 Loss: 0.1366 | 0.1972
Epoch 162/300, seasonal_2 Loss: 0.1383 | 0.1972
Epoch 163/300, seasonal_2 Loss: 0.1367 | 0.1973
Epoch 164/300, seasonal_2 Loss: 0.1362 | 0.1973
Epoch 165/300, seasonal_2 Loss: 0.1368 | 0.1975
Epoch 166/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 167/300, seasonal_2 Loss: 0.1366 | 0.1977
Epoch 168/300, seasonal_2 Loss: 0.1378 | 0.1977
Epoch 169/300, seasonal_2 Loss: 0.1365 | 0.1978
Epoch 170/300, seasonal_2 Loss: 0.1369 | 0.1978
Epoch 171/300, seasonal_2 Loss: 0.1364 | 0.1977
Epoch 172/300, seasonal_2 Loss: 0.1369 | 0.1977
Epoch 173/300, seasonal_2 Loss: 0.1370 | 0.1977
Epoch 174/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 175/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 176/300, seasonal_2 Loss: 0.1365 | 0.1977
Epoch 177/300, seasonal_2 Loss: 0.1374 | 0.1976
Epoch 178/300, seasonal_2 Loss: 0.1374 | 0.1975
Epoch 179/300, seasonal_2 Loss: 0.1362 | 0.1976
Epoch 180/300, seasonal_2 Loss: 0.1367 | 0.1975
Epoch 181/300, seasonal_2 Loss: 0.1365 | 0.1975
Epoch 182/300, seasonal_2 Loss: 0.1373 | 0.1975
Epoch 183/300, seasonal_2 Loss: 0.1367 | 0.1975
Epoch 184/300, seasonal_2 Loss: 0.1364 | 0.1973
Epoch 185/300, seasonal_2 Loss: 0.1357 | 0.1973
Epoch 186/300, seasonal_2 Loss: 0.1373 | 0.1973
Epoch 187/300, seasonal_2 Loss: 0.1367 | 0.1973
Epoch 188/300, seasonal_2 Loss: 0.1373 | 0.1973
Epoch 189/300, seasonal_2 Loss: 0.1361 | 0.1973
Epoch 190/300, seasonal_2 Loss: 0.1373 | 0.1974
Epoch 191/300, seasonal_2 Loss: 0.1372 | 0.1974
Epoch 192/300, seasonal_2 Loss: 0.1365 | 0.1974
Epoch 193/300, seasonal_2 Loss: 0.1362 | 0.1975
Epoch 194/300, seasonal_2 Loss: 0.1367 | 0.1975
Epoch 195/300, seasonal_2 Loss: 0.1367 | 0.1975
Epoch 196/300, seasonal_2 Loss: 0.1365 | 0.1975
Epoch 197/300, seasonal_2 Loss: 0.1369 | 0.1975
Epoch 198/300, seasonal_2 Loss: 0.1371 | 0.1975
Epoch 199/300, seasonal_2 Loss: 0.1373 | 0.1975
Epoch 200/300, seasonal_2 Loss: 0.1374 | 0.1975
Epoch 201/300, seasonal_2 Loss: 0.1371 | 0.1975
Epoch 202/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 203/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 204/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 205/300, seasonal_2 Loss: 0.1374 | 0.1976
Epoch 206/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 207/300, seasonal_2 Loss: 0.1367 | 0.1976
Epoch 208/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 209/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 210/300, seasonal_2 Loss: 0.1371 | 0.1975
Epoch 211/300, seasonal_2 Loss: 0.1366 | 0.1975
Epoch 212/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 213/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 214/300, seasonal_2 Loss: 0.1369 | 0.1976
Epoch 215/300, seasonal_2 Loss: 0.1366 | 0.1976
Epoch 216/300, seasonal_2 Loss: 0.1379 | 0.1976
Epoch 217/300, seasonal_2 Loss: 0.1362 | 0.1976
Epoch 218/300, seasonal_2 Loss: 0.1387 | 0.1976
Epoch 219/300, seasonal_2 Loss: 0.1369 | 0.1976
Epoch 220/300, seasonal_2 Loss: 0.1359 | 0.1976
Epoch 221/300, seasonal_2 Loss: 0.1360 | 0.1976
Epoch 222/300, seasonal_2 Loss: 0.1359 | 0.1976
Epoch 223/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 224/300, seasonal_2 Loss: 0.1366 | 0.1976
Epoch 225/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 226/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 227/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 228/300, seasonal_2 Loss: 0.1373 | 0.1976
Epoch 229/300, seasonal_2 Loss: 0.1367 | 0.1976
Epoch 230/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 231/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 232/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 233/300, seasonal_2 Loss: 0.1361 | 0.1976
Epoch 234/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 235/300, seasonal_2 Loss: 0.1374 | 0.1976
Epoch 236/300, seasonal_2 Loss: 0.1366 | 0.1976
Epoch 237/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 238/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 239/300, seasonal_2 Loss: 0.1367 | 0.1976
Epoch 240/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 241/300, seasonal_2 Loss: 0.1366 | 0.1976
Epoch 242/300, seasonal_2 Loss: 0.1359 | 0.1976
Epoch 243/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 244/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 245/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 246/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 247/300, seasonal_2 Loss: 0.1385 | 0.1976
Epoch 248/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 249/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 250/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 251/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 252/300, seasonal_2 Loss: 0.1369 | 0.1976
Epoch 253/300, seasonal_2 Loss: 0.1369 | 0.1976
Epoch 254/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 255/300, seasonal_2 Loss: 0.1379 | 0.1976
Epoch 256/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 257/300, seasonal_2 Loss: 0.1367 | 0.1976
Epoch 258/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 259/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 260/300, seasonal_2 Loss: 0.1373 | 0.1976
Epoch 261/300, seasonal_2 Loss: 0.1369 | 0.1976
Epoch 262/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 263/300, seasonal_2 Loss: 0.1375 | 0.1976
Epoch 264/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 265/300, seasonal_2 Loss: 0.1375 | 0.1976
Epoch 266/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 267/300, seasonal_2 Loss: 0.1375 | 0.1976
Epoch 268/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 269/300, seasonal_2 Loss: 0.1371 | 0.1976
Epoch 270/300, seasonal_2 Loss: 0.1375 | 0.1976
Epoch 271/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 272/300, seasonal_2 Loss: 0.1362 | 0.1976
Epoch 273/300, seasonal_2 Loss: 0.1357 | 0.1976
Epoch 274/300, seasonal_2 Loss: 0.1374 | 0.1976
Epoch 275/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 276/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 277/300, seasonal_2 Loss: 0.1373 | 0.1976
Epoch 278/300, seasonal_2 Loss: 0.1357 | 0.1976
Epoch 279/300, seasonal_2 Loss: 0.1362 | 0.1976
Epoch 280/300, seasonal_2 Loss: 0.1372 | 0.1976
Epoch 281/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 282/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 283/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 284/300, seasonal_2 Loss: 0.1360 | 0.1976
Epoch 285/300, seasonal_2 Loss: 0.1359 | 0.1976
Epoch 286/300, seasonal_2 Loss: 0.1374 | 0.1976
Epoch 287/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 288/300, seasonal_2 Loss: 0.1366 | 0.1976
Epoch 289/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 290/300, seasonal_2 Loss: 0.1378 | 0.1976
Epoch 291/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 292/300, seasonal_2 Loss: 0.1368 | 0.1976
Epoch 293/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 294/300, seasonal_2 Loss: 0.1363 | 0.1976
Epoch 295/300, seasonal_2 Loss: 0.1376 | 0.1976
Epoch 296/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 297/300, seasonal_2 Loss: 0.1364 | 0.1976
Epoch 298/300, seasonal_2 Loss: 0.1365 | 0.1976
Epoch 299/300, seasonal_2 Loss: 0.1370 | 0.1976
Epoch 300/300, seasonal_2 Loss: 0.1360 | 0.1976
Training seasonal_3 component with params: {'observation_period_num': 252, 'train_rates': 0.9894217129224654, 'learning_rate': 0.0003584428495512724, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9604566413830802}
Epoch 1/300, seasonal_3 Loss: 0.9549 | 1.2312
Epoch 2/300, seasonal_3 Loss: 0.7915 | 1.0312
Epoch 3/300, seasonal_3 Loss: 0.6207 | 0.8799
Epoch 4/300, seasonal_3 Loss: 0.5585 | 0.7526
Epoch 5/300, seasonal_3 Loss: 0.6141 | 0.7436
Epoch 6/300, seasonal_3 Loss: 0.5870 | 0.7177
Epoch 7/300, seasonal_3 Loss: 0.5345 | 0.7230
Epoch 8/300, seasonal_3 Loss: 0.4783 | 0.5811
Epoch 9/300, seasonal_3 Loss: 0.4982 | 0.5955
Epoch 10/300, seasonal_3 Loss: 0.5146 | 0.5509
Epoch 11/300, seasonal_3 Loss: 0.4504 | 0.5059
Epoch 12/300, seasonal_3 Loss: 0.4102 | 0.5156
Epoch 13/300, seasonal_3 Loss: 0.3633 | 0.4893
Epoch 14/300, seasonal_3 Loss: 0.3535 | 0.4515
Epoch 15/300, seasonal_3 Loss: 0.3717 | 0.4127
Epoch 16/300, seasonal_3 Loss: 0.3767 | 0.4150
Epoch 17/300, seasonal_3 Loss: 0.3645 | 0.4550
Epoch 18/300, seasonal_3 Loss: 0.3438 | 0.4167
Epoch 19/300, seasonal_3 Loss: 0.3341 | 0.4144
Epoch 20/300, seasonal_3 Loss: 0.3267 | 0.4075
Epoch 21/300, seasonal_3 Loss: 0.3424 | 0.3886
Epoch 22/300, seasonal_3 Loss: 0.3503 | 0.3970
Epoch 23/300, seasonal_3 Loss: 0.3106 | 0.3446
Epoch 24/300, seasonal_3 Loss: 0.2968 | 0.3597
Epoch 25/300, seasonal_3 Loss: 0.2723 | 0.3348
Epoch 26/300, seasonal_3 Loss: 0.2528 | 0.3240
Epoch 27/300, seasonal_3 Loss: 0.2415 | 0.3167
Epoch 28/300, seasonal_3 Loss: 0.2362 | 0.2991
Epoch 29/300, seasonal_3 Loss: 0.2397 | 0.2975
Epoch 30/300, seasonal_3 Loss: 0.2653 | 0.3007
Epoch 31/300, seasonal_3 Loss: 0.2767 | 0.2890
Epoch 32/300, seasonal_3 Loss: 0.2809 | 0.3083
Epoch 33/300, seasonal_3 Loss: 0.3128 | 0.2977
Epoch 34/300, seasonal_3 Loss: 0.2837 | 0.3029
Epoch 35/300, seasonal_3 Loss: 0.3122 | 0.2934
Epoch 36/300, seasonal_3 Loss: 0.2692 | 0.3422
Epoch 37/300, seasonal_3 Loss: 0.2424 | 0.2814
Epoch 38/300, seasonal_3 Loss: 0.2619 | 0.2921
Epoch 39/300, seasonal_3 Loss: 0.2326 | 0.2726
Epoch 40/300, seasonal_3 Loss: 0.2311 | 0.2907
Epoch 41/300, seasonal_3 Loss: 0.2244 | 0.2756
Epoch 42/300, seasonal_3 Loss: 0.2263 | 0.2900
Epoch 43/300, seasonal_3 Loss: 0.2401 | 0.2580
Epoch 44/300, seasonal_3 Loss: 0.2253 | 0.2659
Epoch 45/300, seasonal_3 Loss: 0.2232 | 0.2492
Epoch 46/300, seasonal_3 Loss: 0.2127 | 0.2503
Epoch 47/300, seasonal_3 Loss: 0.2096 | 0.2495
Epoch 48/300, seasonal_3 Loss: 0.2013 | 0.2413
Epoch 49/300, seasonal_3 Loss: 0.1996 | 0.2444
Epoch 50/300, seasonal_3 Loss: 0.2088 | 0.2350
Epoch 51/300, seasonal_3 Loss: 0.2196 | 0.2358
Epoch 52/300, seasonal_3 Loss: 0.2487 | 0.2439
Epoch 53/300, seasonal_3 Loss: 0.2329 | 0.2345
Epoch 54/300, seasonal_3 Loss: 0.2237 | 0.2520
Epoch 55/300, seasonal_3 Loss: 0.2100 | 0.2240
Epoch 56/300, seasonal_3 Loss: 0.2024 | 0.2527
Epoch 57/300, seasonal_3 Loss: 0.1956 | 0.2186
Epoch 58/300, seasonal_3 Loss: 0.1866 | 0.2458
Epoch 59/300, seasonal_3 Loss: 0.1837 | 0.2154
Epoch 60/300, seasonal_3 Loss: 0.1791 | 0.2354
Epoch 61/300, seasonal_3 Loss: 0.1753 | 0.2132
Epoch 62/300, seasonal_3 Loss: 0.1708 | 0.2159
Epoch 63/300, seasonal_3 Loss: 0.1678 | 0.2086
Epoch 64/300, seasonal_3 Loss: 0.1660 | 0.2087
Epoch 65/300, seasonal_3 Loss: 0.1647 | 0.2052
Epoch 66/300, seasonal_3 Loss: 0.1643 | 0.2017
Epoch 67/300, seasonal_3 Loss: 0.1642 | 0.2046
Epoch 68/300, seasonal_3 Loss: 0.1623 | 0.1969
Epoch 69/300, seasonal_3 Loss: 0.1647 | 0.1973
Epoch 70/300, seasonal_3 Loss: 0.1651 | 0.1939
Epoch 71/300, seasonal_3 Loss: 0.1663 | 0.1944
Epoch 72/300, seasonal_3 Loss: 0.1653 | 0.1911
Epoch 73/300, seasonal_3 Loss: 0.1623 | 0.1911
Epoch 74/300, seasonal_3 Loss: 0.1613 | 0.1887
Epoch 75/300, seasonal_3 Loss: 0.1579 | 0.1882
Epoch 76/300, seasonal_3 Loss: 0.1564 | 0.1875
Epoch 77/300, seasonal_3 Loss: 0.1549 | 0.1861
Epoch 78/300, seasonal_3 Loss: 0.1561 | 0.1865
Epoch 79/300, seasonal_3 Loss: 0.1585 | 0.1824
Epoch 80/300, seasonal_3 Loss: 0.1578 | 0.1855
Epoch 81/300, seasonal_3 Loss: 0.1568 | 0.1794
Epoch 82/300, seasonal_3 Loss: 0.1536 | 0.1783
Epoch 83/300, seasonal_3 Loss: 0.1528 | 0.1783
Epoch 84/300, seasonal_3 Loss: 0.1542 | 0.1764
Epoch 85/300, seasonal_3 Loss: 0.1602 | 0.1874
Epoch 86/300, seasonal_3 Loss: 0.1641 | 0.1804
Epoch 87/300, seasonal_3 Loss: 0.1571 | 0.1794
Epoch 88/300, seasonal_3 Loss: 0.1544 | 0.1713
Epoch 89/300, seasonal_3 Loss: 0.1481 | 0.1733
Epoch 90/300, seasonal_3 Loss: 0.1452 | 0.1689
Epoch 91/300, seasonal_3 Loss: 0.1438 | 0.1709
Epoch 92/300, seasonal_3 Loss: 0.1419 | 0.1688
Epoch 93/300, seasonal_3 Loss: 0.1434 | 0.1665
Epoch 94/300, seasonal_3 Loss: 0.1441 | 0.1675
Epoch 95/300, seasonal_3 Loss: 0.1423 | 0.1648
Epoch 96/300, seasonal_3 Loss: 0.1404 | 0.1655
Epoch 97/300, seasonal_3 Loss: 0.1386 | 0.1629
Epoch 98/300, seasonal_3 Loss: 0.1374 | 0.1617
Epoch 99/300, seasonal_3 Loss: 0.1357 | 0.1622
Epoch 100/300, seasonal_3 Loss: 0.1355 | 0.1598
Epoch 101/300, seasonal_3 Loss: 0.1344 | 0.1620
Epoch 102/300, seasonal_3 Loss: 0.1344 | 0.1578
Epoch 103/300, seasonal_3 Loss: 0.1339 | 0.1582
Epoch 104/300, seasonal_3 Loss: 0.1326 | 0.1551
Epoch 105/300, seasonal_3 Loss: 0.1319 | 0.1579
Epoch 106/300, seasonal_3 Loss: 0.1318 | 0.1542
Epoch 107/300, seasonal_3 Loss: 0.1312 | 0.1543
Epoch 108/300, seasonal_3 Loss: 0.1308 | 0.1554
Epoch 109/300, seasonal_3 Loss: 0.1309 | 0.1542
Epoch 110/300, seasonal_3 Loss: 0.1305 | 0.1545
Epoch 111/300, seasonal_3 Loss: 0.1306 | 0.1527
Epoch 112/300, seasonal_3 Loss: 0.1297 | 0.1524
Epoch 113/300, seasonal_3 Loss: 0.1300 | 0.1515
Epoch 114/300, seasonal_3 Loss: 0.1300 | 0.1517
Epoch 115/300, seasonal_3 Loss: 0.1301 | 0.1518
Epoch 116/300, seasonal_3 Loss: 0.1311 | 0.1515
Epoch 117/300, seasonal_3 Loss: 0.1302 | 0.1511
Epoch 118/300, seasonal_3 Loss: 0.1296 | 0.1488
Epoch 119/300, seasonal_3 Loss: 0.1292 | 0.1536
Epoch 120/300, seasonal_3 Loss: 0.1283 | 0.1482
Epoch 121/300, seasonal_3 Loss: 0.1275 | 0.1508
Epoch 122/300, seasonal_3 Loss: 0.1263 | 0.1455
Epoch 123/300, seasonal_3 Loss: 0.1258 | 0.1492
Epoch 124/300, seasonal_3 Loss: 0.1253 | 0.1462
Epoch 125/300, seasonal_3 Loss: 0.1246 | 0.1468
Epoch 126/300, seasonal_3 Loss: 0.1247 | 0.1489
Epoch 127/300, seasonal_3 Loss: 0.1250 | 0.1434
Epoch 128/300, seasonal_3 Loss: 0.1256 | 0.1510
Epoch 129/300, seasonal_3 Loss: 0.1246 | 0.1435
Epoch 130/300, seasonal_3 Loss: 0.1245 | 0.1494
Epoch 131/300, seasonal_3 Loss: 0.1241 | 0.1416
Epoch 132/300, seasonal_3 Loss: 0.1237 | 0.1483
Epoch 133/300, seasonal_3 Loss: 0.1230 | 0.1415
Epoch 134/300, seasonal_3 Loss: 0.1232 | 0.1470
Epoch 135/300, seasonal_3 Loss: 0.1229 | 0.1407
Epoch 136/300, seasonal_3 Loss: 0.1220 | 0.1463
Epoch 137/300, seasonal_3 Loss: 0.1230 | 0.1397
Epoch 138/300, seasonal_3 Loss: 0.1233 | 0.1471
Epoch 139/300, seasonal_3 Loss: 0.1222 | 0.1402
Epoch 140/300, seasonal_3 Loss: 0.1220 | 0.1454
Epoch 141/300, seasonal_3 Loss: 0.1219 | 0.1383
Epoch 142/300, seasonal_3 Loss: 0.1210 | 0.1453
Epoch 143/300, seasonal_3 Loss: 0.1208 | 0.1391
Epoch 144/300, seasonal_3 Loss: 0.1197 | 0.1433
Epoch 145/300, seasonal_3 Loss: 0.1195 | 0.1391
Epoch 146/300, seasonal_3 Loss: 0.1189 | 0.1412
Epoch 147/300, seasonal_3 Loss: 0.1188 | 0.1397
Epoch 148/300, seasonal_3 Loss: 0.1184 | 0.1416
Epoch 149/300, seasonal_3 Loss: 0.1179 | 0.1368
Epoch 150/300, seasonal_3 Loss: 0.1177 | 0.1411
Epoch 151/300, seasonal_3 Loss: 0.1172 | 0.1369
Epoch 152/300, seasonal_3 Loss: 0.1172 | 0.1392
Epoch 153/300, seasonal_3 Loss: 0.1163 | 0.1368
Epoch 154/300, seasonal_3 Loss: 0.1158 | 0.1386
Epoch 155/300, seasonal_3 Loss: 0.1163 | 0.1360
Epoch 156/300, seasonal_3 Loss: 0.1157 | 0.1371
Epoch 157/300, seasonal_3 Loss: 0.1156 | 0.1356
Epoch 158/300, seasonal_3 Loss: 0.1163 | 0.1377
Epoch 159/300, seasonal_3 Loss: 0.1165 | 0.1347
Epoch 160/300, seasonal_3 Loss: 0.1173 | 0.1367
Epoch 161/300, seasonal_3 Loss: 0.1185 | 0.1342
Epoch 162/300, seasonal_3 Loss: 0.1167 | 0.1362
Epoch 163/300, seasonal_3 Loss: 0.1158 | 0.1344
Epoch 164/300, seasonal_3 Loss: 0.1149 | 0.1358
Epoch 165/300, seasonal_3 Loss: 0.1157 | 0.1349
Epoch 166/300, seasonal_3 Loss: 0.1170 | 0.1354
Epoch 167/300, seasonal_3 Loss: 0.1160 | 0.1352
Epoch 168/300, seasonal_3 Loss: 0.1150 | 0.1343
Epoch 169/300, seasonal_3 Loss: 0.1143 | 0.1342
Epoch 170/300, seasonal_3 Loss: 0.1150 | 0.1331
Epoch 171/300, seasonal_3 Loss: 0.1132 | 0.1353
Epoch 172/300, seasonal_3 Loss: 0.1133 | 0.1322
Epoch 173/300, seasonal_3 Loss: 0.1137 | 0.1345
Epoch 174/300, seasonal_3 Loss: 0.1122 | 0.1331
Epoch 175/300, seasonal_3 Loss: 0.1132 | 0.1337
Epoch 176/300, seasonal_3 Loss: 0.1129 | 0.1316
Epoch 177/300, seasonal_3 Loss: 0.1124 | 0.1334
Epoch 178/300, seasonal_3 Loss: 0.1119 | 0.1311
Epoch 179/300, seasonal_3 Loss: 0.1112 | 0.1324
Epoch 180/300, seasonal_3 Loss: 0.1116 | 0.1307
Epoch 181/300, seasonal_3 Loss: 0.1108 | 0.1309
Epoch 182/300, seasonal_3 Loss: 0.1105 | 0.1317
Epoch 183/300, seasonal_3 Loss: 0.1102 | 0.1312
Epoch 184/300, seasonal_3 Loss: 0.1096 | 0.1298
Epoch 185/300, seasonal_3 Loss: 0.1092 | 0.1303
Epoch 186/300, seasonal_3 Loss: 0.1097 | 0.1303
Epoch 187/300, seasonal_3 Loss: 0.1095 | 0.1298
Epoch 188/300, seasonal_3 Loss: 0.1088 | 0.1298
Epoch 189/300, seasonal_3 Loss: 0.1091 | 0.1292
Epoch 190/300, seasonal_3 Loss: 0.1085 | 0.1291
Epoch 191/300, seasonal_3 Loss: 0.1085 | 0.1291
Epoch 192/300, seasonal_3 Loss: 0.1090 | 0.1290
Epoch 193/300, seasonal_3 Loss: 0.1081 | 0.1308
Epoch 194/300, seasonal_3 Loss: 0.1092 | 0.1280
Epoch 195/300, seasonal_3 Loss: 0.1078 | 0.1300
Epoch 196/300, seasonal_3 Loss: 0.1084 | 0.1285
Epoch 197/300, seasonal_3 Loss: 0.1081 | 0.1290
Epoch 198/300, seasonal_3 Loss: 0.1082 | 0.1289
Epoch 199/300, seasonal_3 Loss: 0.1082 | 0.1283
Epoch 200/300, seasonal_3 Loss: 0.1077 | 0.1279
Epoch 201/300, seasonal_3 Loss: 0.1064 | 0.1278
Epoch 202/300, seasonal_3 Loss: 0.1082 | 0.1275
Epoch 203/300, seasonal_3 Loss: 0.1079 | 0.1267
Epoch 204/300, seasonal_3 Loss: 0.1071 | 0.1281
Epoch 205/300, seasonal_3 Loss: 0.1067 | 0.1262
Epoch 206/300, seasonal_3 Loss: 0.1070 | 0.1267
Epoch 207/300, seasonal_3 Loss: 0.1064 | 0.1270
Epoch 208/300, seasonal_3 Loss: 0.1073 | 0.1260
Epoch 209/300, seasonal_3 Loss: 0.1070 | 0.1263
Epoch 210/300, seasonal_3 Loss: 0.1055 | 0.1267
Epoch 211/300, seasonal_3 Loss: 0.1060 | 0.1269
Epoch 212/300, seasonal_3 Loss: 0.1066 | 0.1262
Epoch 213/300, seasonal_3 Loss: 0.1061 | 0.1259
Epoch 214/300, seasonal_3 Loss: 0.1054 | 0.1246
Epoch 215/300, seasonal_3 Loss: 0.1054 | 0.1252
Epoch 216/300, seasonal_3 Loss: 0.1061 | 0.1255
Epoch 217/300, seasonal_3 Loss: 0.1056 | 0.1248
Epoch 218/300, seasonal_3 Loss: 0.1057 | 0.1247
Epoch 219/300, seasonal_3 Loss: 0.1057 | 0.1251
Epoch 220/300, seasonal_3 Loss: 0.1057 | 0.1244
Epoch 221/300, seasonal_3 Loss: 0.1052 | 0.1252
Epoch 222/300, seasonal_3 Loss: 0.1046 | 0.1251
Epoch 223/300, seasonal_3 Loss: 0.1052 | 0.1253
Epoch 224/300, seasonal_3 Loss: 0.1049 | 0.1247
Epoch 225/300, seasonal_3 Loss: 0.1059 | 0.1249
Epoch 226/300, seasonal_3 Loss: 0.1046 | 0.1251
Epoch 227/300, seasonal_3 Loss: 0.1039 | 0.1244
Epoch 228/300, seasonal_3 Loss: 0.1054 | 0.1250
Epoch 229/300, seasonal_3 Loss: 0.1051 | 0.1241
Epoch 230/300, seasonal_3 Loss: 0.1050 | 0.1241
Epoch 231/300, seasonal_3 Loss: 0.1051 | 0.1248
Epoch 232/300, seasonal_3 Loss: 0.1043 | 0.1242
Epoch 233/300, seasonal_3 Loss: 0.1049 | 0.1249
Epoch 234/300, seasonal_3 Loss: 0.1041 | 0.1237
Epoch 235/300, seasonal_3 Loss: 0.1038 | 0.1239
Epoch 236/300, seasonal_3 Loss: 0.1040 | 0.1242
Epoch 237/300, seasonal_3 Loss: 0.1037 | 0.1245
Epoch 238/300, seasonal_3 Loss: 0.1035 | 0.1231
Epoch 239/300, seasonal_3 Loss: 0.1040 | 0.1235
Epoch 240/300, seasonal_3 Loss: 0.1034 | 0.1231
Epoch 241/300, seasonal_3 Loss: 0.1042 | 0.1232
Epoch 242/300, seasonal_3 Loss: 0.1035 | 0.1227
Epoch 243/300, seasonal_3 Loss: 0.1038 | 0.1235
Epoch 244/300, seasonal_3 Loss: 0.1038 | 0.1228
Epoch 245/300, seasonal_3 Loss: 0.1037 | 0.1225
Epoch 246/300, seasonal_3 Loss: 0.1039 | 0.1227
Epoch 247/300, seasonal_3 Loss: 0.1036 | 0.1229
Epoch 248/300, seasonal_3 Loss: 0.1026 | 0.1227
Epoch 249/300, seasonal_3 Loss: 0.1027 | 0.1222
Epoch 250/300, seasonal_3 Loss: 0.1026 | 0.1221
Epoch 251/300, seasonal_3 Loss: 0.1034 | 0.1220
Epoch 252/300, seasonal_3 Loss: 0.1027 | 0.1224
Epoch 253/300, seasonal_3 Loss: 0.1029 | 0.1222
Epoch 254/300, seasonal_3 Loss: 0.1026 | 0.1220
Epoch 255/300, seasonal_3 Loss: 0.1025 | 0.1220
Epoch 256/300, seasonal_3 Loss: 0.1030 | 0.1219
Epoch 257/300, seasonal_3 Loss: 0.1038 | 0.1215
Epoch 258/300, seasonal_3 Loss: 0.1024 | 0.1221
Epoch 259/300, seasonal_3 Loss: 0.1025 | 0.1212
Epoch 260/300, seasonal_3 Loss: 0.1036 | 0.1217
Epoch 261/300, seasonal_3 Loss: 0.1030 | 0.1221
Epoch 262/300, seasonal_3 Loss: 0.1015 | 0.1221
Epoch 263/300, seasonal_3 Loss: 0.1026 | 0.1219
Epoch 264/300, seasonal_3 Loss: 0.1027 | 0.1213
Epoch 265/300, seasonal_3 Loss: 0.1021 | 0.1215
Epoch 266/300, seasonal_3 Loss: 0.1018 | 0.1214
Epoch 267/300, seasonal_3 Loss: 0.1017 | 0.1211
Epoch 268/300, seasonal_3 Loss: 0.1018 | 0.1212
Epoch 269/300, seasonal_3 Loss: 0.1019 | 0.1209
Epoch 270/300, seasonal_3 Loss: 0.1015 | 0.1216
Epoch 271/300, seasonal_3 Loss: 0.1017 | 0.1211
Epoch 272/300, seasonal_3 Loss: 0.1026 | 0.1210
Epoch 273/300, seasonal_3 Loss: 0.1019 | 0.1212
Epoch 274/300, seasonal_3 Loss: 0.1016 | 0.1212
Epoch 275/300, seasonal_3 Loss: 0.1011 | 0.1205
Epoch 276/300, seasonal_3 Loss: 0.1013 | 0.1207
Epoch 277/300, seasonal_3 Loss: 0.1017 | 0.1207
Epoch 278/300, seasonal_3 Loss: 0.1010 | 0.1206
Epoch 279/300, seasonal_3 Loss: 0.1022 | 0.1208
Epoch 280/300, seasonal_3 Loss: 0.1019 | 0.1208
Epoch 281/300, seasonal_3 Loss: 0.1013 | 0.1202
Epoch 282/300, seasonal_3 Loss: 0.1006 | 0.1206
Epoch 283/300, seasonal_3 Loss: 0.1011 | 0.1206
Epoch 284/300, seasonal_3 Loss: 0.1017 | 0.1205
Epoch 285/300, seasonal_3 Loss: 0.1011 | 0.1203
Epoch 286/300, seasonal_3 Loss: 0.1009 | 0.1201
Epoch 287/300, seasonal_3 Loss: 0.1013 | 0.1203
Epoch 288/300, seasonal_3 Loss: 0.1009 | 0.1199
Epoch 289/300, seasonal_3 Loss: 0.1012 | 0.1201
Epoch 290/300, seasonal_3 Loss: 0.1010 | 0.1201
Epoch 291/300, seasonal_3 Loss: 0.1007 | 0.1200
Epoch 292/300, seasonal_3 Loss: 0.1015 | 0.1200
Epoch 293/300, seasonal_3 Loss: 0.1002 | 0.1196
Epoch 294/300, seasonal_3 Loss: 0.1011 | 0.1196
Epoch 295/300, seasonal_3 Loss: 0.1008 | 0.1192
Epoch 296/300, seasonal_3 Loss: 0.1011 | 0.1191
Epoch 297/300, seasonal_3 Loss: 0.1017 | 0.1191
Epoch 298/300, seasonal_3 Loss: 0.1004 | 0.1198
Epoch 299/300, seasonal_3 Loss: 0.1008 | 0.1199
Epoch 300/300, seasonal_3 Loss: 0.1003 | 0.1195
Training resid component with params: {'observation_period_num': 97, 'train_rates': 0.9878362310663918, 'learning_rate': 6.577677867567627e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9122149417847534}
Epoch 1/300, resid Loss: 0.8377 | 1.1894
Epoch 2/300, resid Loss: 0.6381 | 1.0117
Epoch 3/300, resid Loss: 0.5188 | 0.9078
Epoch 4/300, resid Loss: 0.4374 | 0.7949
Epoch 5/300, resid Loss: 0.3797 | 0.7087
Epoch 6/300, resid Loss: 0.3605 | 0.6680
Epoch 7/300, resid Loss: 0.3221 | 0.5971
Epoch 8/300, resid Loss: 0.3015 | 0.5586
Epoch 9/300, resid Loss: 0.3017 | 0.5465
Epoch 10/300, resid Loss: 0.2823 | 0.4887
Epoch 11/300, resid Loss: 0.2782 | 0.4663
Epoch 12/300, resid Loss: 0.2843 | 0.4463
Epoch 13/300, resid Loss: 0.2906 | 0.4482
Epoch 14/300, resid Loss: 0.2798 | 0.4117
Epoch 15/300, resid Loss: 0.2512 | 0.3979
Epoch 16/300, resid Loss: 0.2447 | 0.3657
Epoch 17/300, resid Loss: 0.2334 | 0.3766
Epoch 18/300, resid Loss: 0.2164 | 0.3432
Epoch 19/300, resid Loss: 0.2112 | 0.3265
Epoch 20/300, resid Loss: 0.2097 | 0.3199
Epoch 21/300, resid Loss: 0.2053 | 0.3154
Epoch 22/300, resid Loss: 0.2005 | 0.3043
Epoch 23/300, resid Loss: 0.1966 | 0.2957
Epoch 24/300, resid Loss: 0.1959 | 0.2884
Epoch 25/300, resid Loss: 0.1946 | 0.2906
Epoch 26/300, resid Loss: 0.1923 | 0.2707
Epoch 27/300, resid Loss: 0.1937 | 0.2734
Epoch 28/300, resid Loss: 0.1919 | 0.2753
Epoch 29/300, resid Loss: 0.1917 | 0.2650
Epoch 30/300, resid Loss: 0.1875 | 0.2535
Epoch 31/300, resid Loss: 0.1899 | 0.2515
Epoch 32/300, resid Loss: 0.1897 | 0.2585
Epoch 33/300, resid Loss: 0.1818 | 0.2585
Epoch 34/300, resid Loss: 0.1808 | 0.2371
Epoch 35/300, resid Loss: 0.1795 | 0.2285
Epoch 36/300, resid Loss: 0.1777 | 0.2364
Epoch 37/300, resid Loss: 0.1715 | 0.2339
Epoch 38/300, resid Loss: 0.1695 | 0.2214
Epoch 39/300, resid Loss: 0.1688 | 0.2175
Epoch 40/300, resid Loss: 0.1688 | 0.2192
Epoch 41/300, resid Loss: 0.1665 | 0.2189
Epoch 42/300, resid Loss: 0.1647 | 0.2138
Epoch 43/300, resid Loss: 0.1641 | 0.2081
Epoch 44/300, resid Loss: 0.1636 | 0.2072
Epoch 45/300, resid Loss: 0.1623 | 0.2082
Epoch 46/300, resid Loss: 0.1609 | 0.2056
Epoch 47/300, resid Loss: 0.1596 | 0.2021
Epoch 48/300, resid Loss: 0.1600 | 0.2002
Epoch 49/300, resid Loss: 0.1587 | 0.1995
Epoch 50/300, resid Loss: 0.1575 | 0.1959
Epoch 51/300, resid Loss: 0.1567 | 0.1951
Epoch 52/300, resid Loss: 0.1562 | 0.1975
Epoch 53/300, resid Loss: 0.1551 | 0.1966
Epoch 54/300, resid Loss: 0.1540 | 0.1878
Epoch 55/300, resid Loss: 0.1537 | 0.1870
Epoch 56/300, resid Loss: 0.1530 | 0.1897
Epoch 57/300, resid Loss: 0.1518 | 0.1895
Epoch 58/300, resid Loss: 0.1509 | 0.1848
Epoch 59/300, resid Loss: 0.1501 | 0.1818
Epoch 60/300, resid Loss: 0.1499 | 0.1811
Epoch 61/300, resid Loss: 0.1495 | 0.1830
Epoch 62/300, resid Loss: 0.1486 | 0.1808
Epoch 63/300, resid Loss: 0.1478 | 0.1791
Epoch 64/300, resid Loss: 0.1483 | 0.1773
Epoch 65/300, resid Loss: 0.1470 | 0.1783
Epoch 66/300, resid Loss: 0.1467 | 0.1743
Epoch 67/300, resid Loss: 0.1468 | 0.1738
Epoch 68/300, resid Loss: 0.1455 | 0.1736
Epoch 69/300, resid Loss: 0.1450 | 0.1741
Epoch 70/300, resid Loss: 0.1441 | 0.1729
Epoch 71/300, resid Loss: 0.1440 | 0.1704
Epoch 72/300, resid Loss: 0.1439 | 0.1699
Epoch 73/300, resid Loss: 0.1434 | 0.1700
Epoch 74/300, resid Loss: 0.1426 | 0.1700
Epoch 75/300, resid Loss: 0.1425 | 0.1680
Epoch 76/300, resid Loss: 0.1422 | 0.1680
Epoch 77/300, resid Loss: 0.1414 | 0.1671
Epoch 78/300, resid Loss: 0.1415 | 0.1672
Epoch 79/300, resid Loss: 0.1405 | 0.1665
Epoch 80/300, resid Loss: 0.1416 | 0.1653
Epoch 81/300, resid Loss: 0.1413 | 0.1650
Epoch 82/300, resid Loss: 0.1404 | 0.1647
Epoch 83/300, resid Loss: 0.1409 | 0.1644
Epoch 84/300, resid Loss: 0.1399 | 0.1633
Epoch 85/300, resid Loss: 0.1389 | 0.1646
Epoch 86/300, resid Loss: 0.1392 | 0.1645
Epoch 87/300, resid Loss: 0.1388 | 0.1632
Epoch 88/300, resid Loss: 0.1390 | 0.1634
Epoch 89/300, resid Loss: 0.1381 | 0.1630
Epoch 90/300, resid Loss: 0.1388 | 0.1620
Epoch 91/300, resid Loss: 0.1383 | 0.1619
Epoch 92/300, resid Loss: 0.1383 | 0.1615
Epoch 93/300, resid Loss: 0.1376 | 0.1603
Epoch 94/300, resid Loss: 0.1381 | 0.1604
Epoch 95/300, resid Loss: 0.1378 | 0.1602
Epoch 96/300, resid Loss: 0.1375 | 0.1604
Epoch 97/300, resid Loss: 0.1372 | 0.1600
Epoch 98/300, resid Loss: 0.1375 | 0.1600
Epoch 99/300, resid Loss: 0.1367 | 0.1590
Epoch 100/300, resid Loss: 0.1365 | 0.1585
Epoch 101/300, resid Loss: 0.1366 | 0.1585
Epoch 102/300, resid Loss: 0.1365 | 0.1582
Epoch 103/300, resid Loss: 0.1360 | 0.1574
Epoch 104/300, resid Loss: 0.1359 | 0.1580
Epoch 105/300, resid Loss: 0.1354 | 0.1573
Epoch 106/300, resid Loss: 0.1352 | 0.1571
Epoch 107/300, resid Loss: 0.1352 | 0.1574
Epoch 108/300, resid Loss: 0.1355 | 0.1567
Epoch 109/300, resid Loss: 0.1350 | 0.1572
Epoch 110/300, resid Loss: 0.1351 | 0.1565
Epoch 111/300, resid Loss: 0.1348 | 0.1563
Epoch 112/300, resid Loss: 0.1350 | 0.1563
Epoch 113/300, resid Loss: 0.1352 | 0.1568
Epoch 114/300, resid Loss: 0.1346 | 0.1562
Epoch 115/300, resid Loss: 0.1341 | 0.1559
Epoch 116/300, resid Loss: 0.1349 | 0.1563
Epoch 117/300, resid Loss: 0.1343 | 0.1562
Epoch 118/300, resid Loss: 0.1340 | 0.1562
Epoch 119/300, resid Loss: 0.1342 | 0.1561
Epoch 120/300, resid Loss: 0.1344 | 0.1561
Epoch 121/300, resid Loss: 0.1341 | 0.1558
Epoch 122/300, resid Loss: 0.1337 | 0.1551
Epoch 123/300, resid Loss: 0.1336 | 0.1552
Epoch 124/300, resid Loss: 0.1338 | 0.1546
Epoch 125/300, resid Loss: 0.1342 | 0.1547
Epoch 126/300, resid Loss: 0.1333 | 0.1551
Epoch 127/300, resid Loss: 0.1344 | 0.1547
Epoch 128/300, resid Loss: 0.1333 | 0.1544
Epoch 129/300, resid Loss: 0.1336 | 0.1546
Epoch 130/300, resid Loss: 0.1327 | 0.1543
Epoch 131/300, resid Loss: 0.1329 | 0.1544
Epoch 132/300, resid Loss: 0.1331 | 0.1541
Epoch 133/300, resid Loss: 0.1333 | 0.1541
Epoch 134/300, resid Loss: 0.1334 | 0.1543
Epoch 135/300, resid Loss: 0.1328 | 0.1541
Epoch 136/300, resid Loss: 0.1331 | 0.1541
Epoch 137/300, resid Loss: 0.1331 | 0.1538
Epoch 138/300, resid Loss: 0.1327 | 0.1536
Epoch 139/300, resid Loss: 0.1335 | 0.1536
Epoch 140/300, resid Loss: 0.1322 | 0.1536
Epoch 141/300, resid Loss: 0.1333 | 0.1536
Epoch 142/300, resid Loss: 0.1328 | 0.1534
Epoch 143/300, resid Loss: 0.1325 | 0.1535
Epoch 144/300, resid Loss: 0.1323 | 0.1534
Epoch 145/300, resid Loss: 0.1324 | 0.1534
Epoch 146/300, resid Loss: 0.1325 | 0.1533
Epoch 147/300, resid Loss: 0.1327 | 0.1533
Epoch 148/300, resid Loss: 0.1330 | 0.1531
Epoch 149/300, resid Loss: 0.1319 | 0.1532
Epoch 150/300, resid Loss: 0.1325 | 0.1531
Epoch 151/300, resid Loss: 0.1322 | 0.1527
Epoch 152/300, resid Loss: 0.1325 | 0.1528
Epoch 153/300, resid Loss: 0.1323 | 0.1528
Epoch 154/300, resid Loss: 0.1328 | 0.1526
Epoch 155/300, resid Loss: 0.1323 | 0.1525
Epoch 156/300, resid Loss: 0.1326 | 0.1524
Epoch 157/300, resid Loss: 0.1327 | 0.1524
Epoch 158/300, resid Loss: 0.1321 | 0.1523
Epoch 159/300, resid Loss: 0.1321 | 0.1524
Epoch 160/300, resid Loss: 0.1323 | 0.1523
Epoch 161/300, resid Loss: 0.1322 | 0.1522
Epoch 162/300, resid Loss: 0.1323 | 0.1523
Epoch 163/300, resid Loss: 0.1328 | 0.1523
Epoch 164/300, resid Loss: 0.1321 | 0.1522
Epoch 165/300, resid Loss: 0.1324 | 0.1522
Epoch 166/300, resid Loss: 0.1327 | 0.1522
Epoch 167/300, resid Loss: 0.1317 | 0.1524
Epoch 168/300, resid Loss: 0.1324 | 0.1525
Epoch 169/300, resid Loss: 0.1327 | 0.1524
Epoch 170/300, resid Loss: 0.1318 | 0.1524
Epoch 171/300, resid Loss: 0.1317 | 0.1523
Epoch 172/300, resid Loss: 0.1320 | 0.1521
Epoch 173/300, resid Loss: 0.1322 | 0.1522
Epoch 174/300, resid Loss: 0.1321 | 0.1522
Epoch 175/300, resid Loss: 0.1324 | 0.1523
Epoch 176/300, resid Loss: 0.1320 | 0.1522
Epoch 177/300, resid Loss: 0.1322 | 0.1522
Epoch 178/300, resid Loss: 0.1318 | 0.1520
Epoch 179/300, resid Loss: 0.1318 | 0.1518
Epoch 180/300, resid Loss: 0.1319 | 0.1518
Epoch 181/300, resid Loss: 0.1318 | 0.1518
Epoch 182/300, resid Loss: 0.1310 | 0.1518
Epoch 183/300, resid Loss: 0.1326 | 0.1518
Epoch 184/300, resid Loss: 0.1314 | 0.1518
Epoch 185/300, resid Loss: 0.1319 | 0.1518
Epoch 186/300, resid Loss: 0.1316 | 0.1518
Epoch 187/300, resid Loss: 0.1322 | 0.1518
Epoch 188/300, resid Loss: 0.1322 | 0.1518
Epoch 189/300, resid Loss: 0.1320 | 0.1517
Epoch 190/300, resid Loss: 0.1318 | 0.1517
Epoch 191/300, resid Loss: 0.1317 | 0.1518
Epoch 192/300, resid Loss: 0.1321 | 0.1518
Epoch 193/300, resid Loss: 0.1315 | 0.1517
Epoch 194/300, resid Loss: 0.1316 | 0.1517
Epoch 195/300, resid Loss: 0.1314 | 0.1517
Epoch 196/300, resid Loss: 0.1321 | 0.1516
Epoch 197/300, resid Loss: 0.1314 | 0.1517
Epoch 198/300, resid Loss: 0.1318 | 0.1517
Epoch 199/300, resid Loss: 0.1316 | 0.1516
Epoch 200/300, resid Loss: 0.1310 | 0.1516
Epoch 201/300, resid Loss: 0.1316 | 0.1517
Epoch 202/300, resid Loss: 0.1314 | 0.1517
Epoch 203/300, resid Loss: 0.1318 | 0.1517
Epoch 204/300, resid Loss: 0.1315 | 0.1516
Epoch 205/300, resid Loss: 0.1313 | 0.1516
Epoch 206/300, resid Loss: 0.1318 | 0.1516
Epoch 207/300, resid Loss: 0.1314 | 0.1516
Epoch 208/300, resid Loss: 0.1321 | 0.1516
Epoch 209/300, resid Loss: 0.1319 | 0.1516
Epoch 210/300, resid Loss: 0.1320 | 0.1517
Epoch 211/300, resid Loss: 0.1318 | 0.1517
Epoch 212/300, resid Loss: 0.1319 | 0.1517
Epoch 213/300, resid Loss: 0.1316 | 0.1517
Epoch 214/300, resid Loss: 0.1318 | 0.1517
Epoch 215/300, resid Loss: 0.1321 | 0.1517
Epoch 216/300, resid Loss: 0.1314 | 0.1517
Epoch 217/300, resid Loss: 0.1317 | 0.1517
Epoch 218/300, resid Loss: 0.1319 | 0.1517
Epoch 219/300, resid Loss: 0.1317 | 0.1516
Epoch 220/300, resid Loss: 0.1313 | 0.1517
Epoch 221/300, resid Loss: 0.1320 | 0.1516
Epoch 222/300, resid Loss: 0.1319 | 0.1516
Epoch 223/300, resid Loss: 0.1316 | 0.1516
Epoch 224/300, resid Loss: 0.1318 | 0.1516
Epoch 225/300, resid Loss: 0.1316 | 0.1516
Epoch 226/300, resid Loss: 0.1317 | 0.1515
Epoch 227/300, resid Loss: 0.1315 | 0.1515
Epoch 228/300, resid Loss: 0.1315 | 0.1515
Epoch 229/300, resid Loss: 0.1307 | 0.1515
Epoch 230/300, resid Loss: 0.1316 | 0.1515
Epoch 231/300, resid Loss: 0.1308 | 0.1515
Epoch 232/300, resid Loss: 0.1315 | 0.1515
Epoch 233/300, resid Loss: 0.1315 | 0.1515
Epoch 234/300, resid Loss: 0.1317 | 0.1515
Epoch 235/300, resid Loss: 0.1320 | 0.1515
Epoch 236/300, resid Loss: 0.1311 | 0.1515
Epoch 237/300, resid Loss: 0.1322 | 0.1515
Epoch 238/300, resid Loss: 0.1316 | 0.1515
Epoch 239/300, resid Loss: 0.1314 | 0.1515
Epoch 240/300, resid Loss: 0.1324 | 0.1514
Epoch 241/300, resid Loss: 0.1316 | 0.1514
Epoch 242/300, resid Loss: 0.1318 | 0.1515
Epoch 243/300, resid Loss: 0.1316 | 0.1515
Epoch 244/300, resid Loss: 0.1309 | 0.1515
Epoch 245/300, resid Loss: 0.1318 | 0.1515
Epoch 246/300, resid Loss: 0.1314 | 0.1515
Epoch 247/300, resid Loss: 0.1319 | 0.1515
Epoch 248/300, resid Loss: 0.1318 | 0.1515
Epoch 249/300, resid Loss: 0.1313 | 0.1515
Epoch 250/300, resid Loss: 0.1313 | 0.1515
Epoch 251/300, resid Loss: 0.1315 | 0.1515
Epoch 252/300, resid Loss: 0.1321 | 0.1514
Epoch 253/300, resid Loss: 0.1312 | 0.1514
Epoch 254/300, resid Loss: 0.1313 | 0.1514
Epoch 255/300, resid Loss: 0.1313 | 0.1514
Epoch 256/300, resid Loss: 0.1313 | 0.1514
Epoch 257/300, resid Loss: 0.1307 | 0.1515
Epoch 258/300, resid Loss: 0.1318 | 0.1514
Epoch 259/300, resid Loss: 0.1317 | 0.1514
Epoch 260/300, resid Loss: 0.1318 | 0.1514
Epoch 261/300, resid Loss: 0.1314 | 0.1514
Epoch 262/300, resid Loss: 0.1317 | 0.1514
Epoch 263/300, resid Loss: 0.1315 | 0.1514
Epoch 264/300, resid Loss: 0.1317 | 0.1514
Epoch 265/300, resid Loss: 0.1312 | 0.1514
Epoch 266/300, resid Loss: 0.1321 | 0.1514
Epoch 267/300, resid Loss: 0.1324 | 0.1514
Epoch 268/300, resid Loss: 0.1313 | 0.1514
Epoch 269/300, resid Loss: 0.1316 | 0.1514
Epoch 270/300, resid Loss: 0.1321 | 0.1514
Epoch 271/300, resid Loss: 0.1313 | 0.1514
Epoch 272/300, resid Loss: 0.1309 | 0.1514
Epoch 273/300, resid Loss: 0.1315 | 0.1514
Epoch 274/300, resid Loss: 0.1315 | 0.1514
Epoch 275/300, resid Loss: 0.1313 | 0.1514
Epoch 276/300, resid Loss: 0.1320 | 0.1514
Epoch 277/300, resid Loss: 0.1315 | 0.1514
Epoch 278/300, resid Loss: 0.1320 | 0.1514
Epoch 279/300, resid Loss: 0.1309 | 0.1514
Epoch 280/300, resid Loss: 0.1321 | 0.1514
Epoch 281/300, resid Loss: 0.1315 | 0.1514
Epoch 282/300, resid Loss: 0.1312 | 0.1514
Epoch 283/300, resid Loss: 0.1314 | 0.1514
Epoch 284/300, resid Loss: 0.1309 | 0.1514
Epoch 285/300, resid Loss: 0.1312 | 0.1514
Epoch 286/300, resid Loss: 0.1315 | 0.1514
Epoch 287/300, resid Loss: 0.1309 | 0.1514
Epoch 288/300, resid Loss: 0.1313 | 0.1514
Epoch 289/300, resid Loss: 0.1313 | 0.1514
Epoch 290/300, resid Loss: 0.1315 | 0.1514
Epoch 291/300, resid Loss: 0.1318 | 0.1514
Epoch 292/300, resid Loss: 0.1310 | 0.1514
Epoch 293/300, resid Loss: 0.1313 | 0.1514
Epoch 294/300, resid Loss: 0.1312 | 0.1514
Epoch 295/300, resid Loss: 0.1313 | 0.1514
Epoch 296/300, resid Loss: 0.1313 | 0.1514
Epoch 297/300, resid Loss: 0.1311 | 0.1514
Epoch 298/300, resid Loss: 0.1314 | 0.1514
Epoch 299/300, resid Loss: 0.1321 | 0.1514
Epoch 300/300, resid Loss: 0.1320 | 0.1514
Runtime (seconds): 3694.0098779201508
0.00024035196656608756
[150.63493]
[-1.9963185]
[-3.9069638]
[9.026614]
[2.8249712]
[6.069645]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 175.23391091288067
RMSE: 13.237594604492188
MAE: 13.237594604492188
R-squared: nan
[162.65286]
