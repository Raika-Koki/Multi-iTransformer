[32m[I 2025-01-06 07:58:48,355][0m A new study created in memory with name: no-name-7b2ed1a9-2949-4f90-8f44-1136ff2f8295[0m
Early stopping at epoch 47
[32m[I 2025-01-06 07:59:54,091][0m Trial 0 finished with value: 2.2622437319912754 and parameters: {'observation_period_num': 112, 'train_rates': 0.8401040025677017, 'learning_rate': 1.4552467436682627e-06, 'batch_size': 226, 'step_size': 1, 'gamma': 0.8028883865230605}. Best is trial 0 with value: 2.2622437319912754.[0m
[32m[I 2025-01-06 08:01:12,796][0m Trial 1 finished with value: 0.6555738004048666 and parameters: {'observation_period_num': 67, 'train_rates': 0.7404568830572625, 'learning_rate': 2.324718857996033e-05, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9824871032214412}. Best is trial 1 with value: 0.6555738004048666.[0m
[32m[I 2025-01-06 08:04:17,988][0m Trial 2 finished with value: 0.1951617349025815 and parameters: {'observation_period_num': 118, 'train_rates': 0.9546483816142114, 'learning_rate': 0.00013840320479709426, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9871173774696267}. Best is trial 2 with value: 0.1951617349025815.[0m
[32m[I 2025-01-06 08:09:02,532][0m Trial 3 finished with value: 0.48435310431835404 and parameters: {'observation_period_num': 197, 'train_rates': 0.8851702281776063, 'learning_rate': 8.318915246850089e-06, 'batch_size': 95, 'step_size': 9, 'gamma': 0.7930017415953495}. Best is trial 2 with value: 0.1951617349025815.[0m
[32m[I 2025-01-06 08:10:51,872][0m Trial 4 finished with value: 0.7097852437032593 and parameters: {'observation_period_num': 93, 'train_rates': 0.7736604781684572, 'learning_rate': 8.440195518959146e-06, 'batch_size': 234, 'step_size': 11, 'gamma': 0.9655624149124501}. Best is trial 2 with value: 0.1951617349025815.[0m
[32m[I 2025-01-06 08:15:07,288][0m Trial 5 finished with value: 0.25877970253879373 and parameters: {'observation_period_num': 178, 'train_rates': 0.936746294486519, 'learning_rate': 2.75794881657285e-05, 'batch_size': 170, 'step_size': 15, 'gamma': 0.8832569788063164}. Best is trial 2 with value: 0.1951617349025815.[0m
[32m[I 2025-01-06 08:18:36,993][0m Trial 6 finished with value: 0.12017274647951126 and parameters: {'observation_period_num': 143, 'train_rates': 0.9860925884474925, 'learning_rate': 8.792871693127005e-05, 'batch_size': 182, 'step_size': 3, 'gamma': 0.9759286878215627}. Best is trial 6 with value: 0.12017274647951126.[0m
[32m[I 2025-01-06 08:19:53,592][0m Trial 7 finished with value: 0.11454509943723679 and parameters: {'observation_period_num': 57, 'train_rates': 0.9837908588895652, 'learning_rate': 0.0002500992440187324, 'batch_size': 253, 'step_size': 11, 'gamma': 0.9548817701696343}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:21:12,653][0m Trial 8 finished with value: 0.3557032294929984 and parameters: {'observation_period_num': 64, 'train_rates': 0.8708697429679683, 'learning_rate': 0.0005092413398589104, 'batch_size': 135, 'step_size': 3, 'gamma': 0.7604339013623762}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:24:21,996][0m Trial 9 finished with value: 2.3781751788966714 and parameters: {'observation_period_num': 166, 'train_rates': 0.6265290943847803, 'learning_rate': 0.0009401080969606811, 'batch_size': 63, 'step_size': 8, 'gamma': 0.9704146428861293}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:24:37,189][0m Trial 10 finished with value: 0.6794738364460364 and parameters: {'observation_period_num': 6, 'train_rates': 0.6809234897337797, 'learning_rate': 0.0002851520870263163, 'batch_size': 253, 'step_size': 14, 'gamma': 0.9028017594940436}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:30:46,819][0m Trial 11 finished with value: 0.16547998785972595 and parameters: {'observation_period_num': 235, 'train_rates': 0.9660290486032841, 'learning_rate': 0.00010716381111725553, 'batch_size': 196, 'step_size': 12, 'gamma': 0.9255556765219654}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:31:15,431][0m Trial 12 finished with value: 0.12925933301448822 and parameters: {'observation_period_num': 17, 'train_rates': 0.989992737810337, 'learning_rate': 9.178771510747656e-05, 'batch_size': 194, 'step_size': 6, 'gamma': 0.933950581730731}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:34:46,169][0m Trial 13 finished with value: 0.17710351166771907 and parameters: {'observation_period_num': 151, 'train_rates': 0.9092056290275167, 'learning_rate': 0.00021907611589867468, 'batch_size': 197, 'step_size': 11, 'gamma': 0.8458004583817905}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:35:47,723][0m Trial 14 finished with value: 0.3199436105787754 and parameters: {'observation_period_num': 49, 'train_rates': 0.8102822151523341, 'learning_rate': 5.352887177262695e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.9480318906407331}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:38:53,963][0m Trial 15 finished with value: 0.19472508132457733 and parameters: {'observation_period_num': 133, 'train_rates': 0.9170213543961164, 'learning_rate': 0.0004586708288156925, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8554239469189668}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:44:22,215][0m Trial 16 finished with value: 0.4121396839618683 and parameters: {'observation_period_num': 210, 'train_rates': 0.9895897359470205, 'learning_rate': 5.2651096787384315e-05, 'batch_size': 170, 'step_size': 1, 'gamma': 0.910900398061186}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:46:16,750][0m Trial 17 finished with value: 0.4761633594830831 and parameters: {'observation_period_num': 93, 'train_rates': 0.8532897627207137, 'learning_rate': 1.228915531414541e-05, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9492529379445546}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:47:11,546][0m Trial 18 finished with value: 1.0541765328203694 and parameters: {'observation_period_num': 47, 'train_rates': 0.6939442217155948, 'learning_rate': 3.4286980793182597e-06, 'batch_size': 104, 'step_size': 13, 'gamma': 0.8848600642177722}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:53:36,025][0m Trial 19 finished with value: 0.8447368340200733 and parameters: {'observation_period_num': 252, 'train_rates': 0.9156085389043054, 'learning_rate': 0.0009335482762312151, 'batch_size': 169, 'step_size': 10, 'gamma': 0.9504884007855691}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:56:39,467][0m Trial 20 finished with value: 0.39725505976417985 and parameters: {'observation_period_num': 144, 'train_rates': 0.8101125170371491, 'learning_rate': 0.0002140942101590194, 'batch_size': 214, 'step_size': 5, 'gamma': 0.8240899745375865}. Best is trial 7 with value: 0.11454509943723679.[0m
[32m[I 2025-01-06 08:57:05,401][0m Trial 21 finished with value: 0.11419647186994553 and parameters: {'observation_period_num': 9, 'train_rates': 0.9877424241338936, 'learning_rate': 8.286275593084115e-05, 'batch_size': 194, 'step_size': 6, 'gamma': 0.926546983825023}. Best is trial 21 with value: 0.11419647186994553.[0m
[32m[I 2025-01-06 08:57:41,895][0m Trial 22 finished with value: 0.25669223070144653 and parameters: {'observation_period_num': 23, 'train_rates': 0.9540853691163681, 'learning_rate': 5.970233178106304e-05, 'batch_size': 182, 'step_size': 3, 'gamma': 0.9222608659007518}. Best is trial 21 with value: 0.11419647186994553.[0m
[32m[I 2025-01-06 08:58:32,019][0m Trial 23 finished with value: 0.09044138342142105 and parameters: {'observation_period_num': 33, 'train_rates': 0.9853580372837696, 'learning_rate': 0.0001508439887145705, 'batch_size': 149, 'step_size': 5, 'gamma': 0.9644579245059043}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 08:59:19,141][0m Trial 24 finished with value: 0.14013812367509051 and parameters: {'observation_period_num': 33, 'train_rates': 0.9301357322503707, 'learning_rate': 0.00043298612946354866, 'batch_size': 154, 'step_size': 7, 'gamma': 0.9048161019129216}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:01:02,821][0m Trial 25 finished with value: 0.18271660248735058 and parameters: {'observation_period_num': 78, 'train_rates': 0.8836921606138243, 'learning_rate': 0.00017438373216247235, 'batch_size': 115, 'step_size': 5, 'gamma': 0.9557357915684194}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:01:57,589][0m Trial 26 finished with value: 0.1969028264284134 and parameters: {'observation_period_num': 41, 'train_rates': 0.9566440269517893, 'learning_rate': 0.000310500838494581, 'batch_size': 238, 'step_size': 8, 'gamma': 0.9379628869232468}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:02:19,394][0m Trial 27 finished with value: 0.23208318933592004 and parameters: {'observation_period_num': 7, 'train_rates': 0.900016805491881, 'learning_rate': 3.4257253523574104e-05, 'batch_size': 207, 'step_size': 10, 'gamma': 0.8788418318713959}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:03:03,446][0m Trial 28 finished with value: 0.18447749839200603 and parameters: {'observation_period_num': 28, 'train_rates': 0.9476279497380596, 'learning_rate': 0.00013369645292673758, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9640626715598731}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:05:13,770][0m Trial 29 finished with value: 1.4355313509246193 and parameters: {'observation_period_num': 99, 'train_rates': 0.840247277194962, 'learning_rate': 1.009338998470391e-06, 'batch_size': 68, 'step_size': 1, 'gamma': 0.9178564058180868}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:06:33,323][0m Trial 30 finished with value: 0.28867948055267334 and parameters: {'observation_period_num': 60, 'train_rates': 0.968066198554378, 'learning_rate': 1.5911060471090223e-05, 'batch_size': 152, 'step_size': 7, 'gamma': 0.9898617778967033}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:09:07,320][0m Trial 31 finished with value: 0.16338130831718445 and parameters: {'observation_period_num': 111, 'train_rates': 0.9695445721898466, 'learning_rate': 8.401791751228813e-05, 'batch_size': 183, 'step_size': 2, 'gamma': 0.9727893476850593}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:11:01,345][0m Trial 32 finished with value: 0.14414101839065552 and parameters: {'observation_period_num': 82, 'train_rates': 0.9892092673211725, 'learning_rate': 6.812600660743948e-05, 'batch_size': 217, 'step_size': 4, 'gamma': 0.9404393372600149}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:12:14,527][0m Trial 33 finished with value: 0.18869915096969395 and parameters: {'observation_period_num': 56, 'train_rates': 0.9372512614678238, 'learning_rate': 0.00013575846180649667, 'batch_size': 160, 'step_size': 6, 'gamma': 0.9763980492129797}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:15:07,235][0m Trial 34 finished with value: 0.17617200314998627 and parameters: {'observation_period_num': 124, 'train_rates': 0.9899375952827509, 'learning_rate': 4.198890618695885e-05, 'batch_size': 244, 'step_size': 2, 'gamma': 0.989767102366598}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:15:52,225][0m Trial 35 finished with value: 0.4122259397984243 and parameters: {'observation_period_num': 37, 'train_rates': 0.764046697617478, 'learning_rate': 0.0002799016214271631, 'batch_size': 206, 'step_size': 4, 'gamma': 0.9603749818259878}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:17:31,139][0m Trial 36 finished with value: 0.33021920687794065 and parameters: {'observation_period_num': 73, 'train_rates': 0.9330519524611816, 'learning_rate': 2.266364137909847e-05, 'batch_size': 184, 'step_size': 5, 'gamma': 0.8948095823263871}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:21:50,396][0m Trial 37 finished with value: 0.39960119128227234 and parameters: {'observation_period_num': 175, 'train_rates': 0.9700940868951579, 'learning_rate': 0.000642904418609487, 'batch_size': 126, 'step_size': 6, 'gamma': 0.976861576394022}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:24:59,158][0m Trial 38 finished with value: 0.15875135947551047 and parameters: {'observation_period_num': 110, 'train_rates': 0.8918784313728445, 'learning_rate': 0.00016369905893135435, 'batch_size': 22, 'step_size': 3, 'gamma': 0.9333682620369695}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:25:22,925][0m Trial 39 finished with value: 0.26091163176588894 and parameters: {'observation_period_num': 16, 'train_rates': 0.8606989004948029, 'learning_rate': 0.00010099556597188916, 'batch_size': 231, 'step_size': 9, 'gamma': 0.762349639397554}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:30:13,194][0m Trial 40 finished with value: 0.1727890162338764 and parameters: {'observation_period_num': 194, 'train_rates': 0.9267696542753507, 'learning_rate': 0.00032670993037225564, 'batch_size': 143, 'step_size': 12, 'gamma': 0.9629293396805721}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:30:42,285][0m Trial 41 finished with value: 0.1146453469991684 and parameters: {'observation_period_num': 17, 'train_rates': 0.9895842373367219, 'learning_rate': 8.630890587094208e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9331721880112396}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:31:09,639][0m Trial 42 finished with value: 0.12563584744930267 and parameters: {'observation_period_num': 7, 'train_rates': 0.9686784723308575, 'learning_rate': 8.097268743508552e-05, 'batch_size': 177, 'step_size': 8, 'gamma': 0.9437414273035049}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:31:45,474][0m Trial 43 finished with value: 0.2554849684238434 and parameters: {'observation_period_num': 24, 'train_rates': 0.9475265682528891, 'learning_rate': 3.863483973028115e-05, 'batch_size': 195, 'step_size': 7, 'gamma': 0.9315443210308526}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:34:29,723][0m Trial 44 finished with value: 0.9065558432660186 and parameters: {'observation_period_num': 153, 'train_rates': 0.6098524915958728, 'learning_rate': 0.00012149337812324923, 'batch_size': 160, 'step_size': 4, 'gamma': 0.9782160293525307}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:35:45,152][0m Trial 45 finished with value: 0.1367276906967163 and parameters: {'observation_period_num': 52, 'train_rates': 0.9760688247608, 'learning_rate': 0.00020562568294672228, 'batch_size': 82, 'step_size': 15, 'gamma': 0.9159576536519941}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:36:06,338][0m Trial 46 finished with value: 0.5061769102300916 and parameters: {'observation_period_num': 16, 'train_rates': 0.7072228465237038, 'learning_rate': 7.090175059588046e-05, 'batch_size': 243, 'step_size': 6, 'gamma': 0.9568271383560008}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:36:56,984][0m Trial 47 finished with value: 0.45236438512802124 and parameters: {'observation_period_num': 37, 'train_rates': 0.9469924573470512, 'learning_rate': 4.8609131436825414e-05, 'batch_size': 223, 'step_size': 2, 'gamma': 0.8954927022172465}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:38:30,875][0m Trial 48 finished with value: 0.3164624571800232 and parameters: {'observation_period_num': 66, 'train_rates': 0.9847806625347522, 'learning_rate': 2.30381645268836e-05, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8610268345020742}. Best is trial 23 with value: 0.09044138342142105.[0m
[32m[I 2025-01-06 09:41:40,653][0m Trial 49 finished with value: 0.17757288163358514 and parameters: {'observation_period_num': 137, 'train_rates': 0.9066352421516656, 'learning_rate': 0.00016589787939153523, 'batch_size': 144, 'step_size': 14, 'gamma': 0.9680863287737663}. Best is trial 23 with value: 0.09044138342142105.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.9145 | 1.2676
Epoch 2/300, Loss: 0.6180 | 0.8004
Epoch 3/300, Loss: 0.4973 | 0.7520
Epoch 4/300, Loss: 0.4424 | 0.5984
Epoch 5/300, Loss: 0.4466 | 0.5926
Epoch 6/300, Loss: 0.4050 | 0.5439
Epoch 7/300, Loss: 0.3432 | 0.4556
Epoch 8/300, Loss: 0.3503 | 0.4603
Epoch 9/300, Loss: 0.3271 | 0.3857
Epoch 10/300, Loss: 0.3439 | 0.3914
Epoch 11/300, Loss: 0.3380 | 0.3409
Epoch 12/300, Loss: 0.3088 | 0.3282
Epoch 13/300, Loss: 0.4081 | 0.4105
Epoch 14/300, Loss: 0.3356 | 0.3151
Epoch 15/300, Loss: 0.3062 | 0.3260
Epoch 16/300, Loss: 0.2473 | 0.2823
Epoch 17/300, Loss: 0.2268 | 0.2629
Epoch 18/300, Loss: 0.2177 | 0.2550
Epoch 19/300, Loss: 0.2186 | 0.2360
Epoch 20/300, Loss: 0.2309 | 0.2338
Epoch 21/300, Loss: 0.2276 | 0.2276
Epoch 22/300, Loss: 0.2728 | 0.2406
Epoch 23/300, Loss: 0.2235 | 0.2306
Epoch 24/300, Loss: 0.2059 | 0.2211
Epoch 25/300, Loss: 0.1911 | 0.2089
Epoch 26/300, Loss: 0.1908 | 0.2115
Epoch 27/300, Loss: 0.1923 | 0.1986
Epoch 28/300, Loss: 0.2006 | 0.2526
Epoch 29/300, Loss: 0.1969 | 0.1916
Epoch 30/300, Loss: 0.1838 | 0.1916
Epoch 31/300, Loss: 0.1798 | 0.1847
Epoch 32/300, Loss: 0.1786 | 0.1751
Epoch 33/300, Loss: 0.1863 | 0.1824
Epoch 34/300, Loss: 0.1757 | 0.1693
Epoch 35/300, Loss: 0.1743 | 0.1661
Epoch 36/300, Loss: 0.1898 | 0.1683
Epoch 37/300, Loss: 0.2015 | 0.1594
Epoch 38/300, Loss: 0.2458 | 0.1859
Epoch 39/300, Loss: 0.1912 | 0.1664
Epoch 40/300, Loss: 0.1788 | 0.1738
Epoch 41/300, Loss: 0.1635 | 0.1570
Epoch 42/300, Loss: 0.1577 | 0.1688
Epoch 43/300, Loss: 0.1532 | 0.1523
Epoch 44/300, Loss: 0.1527 | 0.1553
Epoch 45/300, Loss: 0.1505 | 0.1479
Epoch 46/300, Loss: 0.1537 | 0.1450
Epoch 47/300, Loss: 0.1570 | 0.1434
Epoch 48/300, Loss: 0.1652 | 0.1473
Epoch 49/300, Loss: 0.1795 | 0.1500
Epoch 50/300, Loss: 0.1767 | 0.1560
Epoch 51/300, Loss: 0.1701 | 0.1652
Epoch 52/300, Loss: 0.1527 | 0.1462
Epoch 53/300, Loss: 0.1436 | 0.1402
Epoch 54/300, Loss: 0.1387 | 0.1374
Epoch 55/300, Loss: 0.1385 | 0.1311
Epoch 56/300, Loss: 0.1393 | 0.1324
Epoch 57/300, Loss: 0.1412 | 0.1286
Epoch 58/300, Loss: 0.1452 | 0.1284
Epoch 59/300, Loss: 0.1453 | 0.1269
Epoch 60/300, Loss: 0.1484 | 0.1290
Epoch 61/300, Loss: 0.1464 | 0.1244
Epoch 62/300, Loss: 0.1476 | 0.1276
Epoch 63/300, Loss: 0.1442 | 0.1213
Epoch 64/300, Loss: 0.1451 | 0.1246
Epoch 65/300, Loss: 0.1396 | 0.1191
Epoch 66/300, Loss: 0.1378 | 0.1231
Epoch 67/300, Loss: 0.1353 | 0.1184
Epoch 68/300, Loss: 0.1343 | 0.1214
Epoch 69/300, Loss: 0.1326 | 0.1198
Epoch 70/300, Loss: 0.1320 | 0.1210
Epoch 71/300, Loss: 0.1294 | 0.1172
Epoch 72/300, Loss: 0.1281 | 0.1180
Epoch 73/300, Loss: 0.1266 | 0.1148
Epoch 74/300, Loss: 0.1256 | 0.1164
Epoch 75/300, Loss: 0.1236 | 0.1108
Epoch 76/300, Loss: 0.1221 | 0.1115
Epoch 77/300, Loss: 0.1204 | 0.1082
Epoch 78/300, Loss: 0.1193 | 0.1067
Epoch 79/300, Loss: 0.1182 | 0.1052
Epoch 80/300, Loss: 0.1186 | 0.1060
Epoch 81/300, Loss: 0.1182 | 0.1042
Epoch 82/300, Loss: 0.1186 | 0.1065
Epoch 83/300, Loss: 0.1179 | 0.1019
Epoch 84/300, Loss: 0.1180 | 0.1050
Epoch 85/300, Loss: 0.1170 | 0.1011
Epoch 86/300, Loss: 0.1165 | 0.1041
Epoch 87/300, Loss: 0.1154 | 0.0999
Epoch 88/300, Loss: 0.1149 | 0.1033
Epoch 89/300, Loss: 0.1139 | 0.0983
Epoch 90/300, Loss: 0.1128 | 0.0993
Epoch 91/300, Loss: 0.1116 | 0.0984
Epoch 92/300, Loss: 0.1114 | 0.0967
Epoch 93/300, Loss: 0.1125 | 0.1023
Epoch 94/300, Loss: 0.1176 | 0.0995
Epoch 95/300, Loss: 0.1241 | 0.1140
Epoch 96/300, Loss: 0.1238 | 0.0985
Epoch 97/300, Loss: 0.1172 | 0.0998
Epoch 98/300, Loss: 0.1099 | 0.0946
Epoch 99/300, Loss: 0.1075 | 0.0952
Epoch 100/300, Loss: 0.1067 | 0.0929
Epoch 101/300, Loss: 0.1058 | 0.0929
Epoch 102/300, Loss: 0.1065 | 0.0933
Epoch 103/300, Loss: 0.1055 | 0.0920
Epoch 104/300, Loss: 0.1052 | 0.0927
Epoch 105/300, Loss: 0.1048 | 0.0910
Epoch 106/300, Loss: 0.1030 | 0.0909
Epoch 107/300, Loss: 0.1035 | 0.0899
Epoch 108/300, Loss: 0.1021 | 0.0892
Epoch 109/300, Loss: 0.1027 | 0.0891
Epoch 110/300, Loss: 0.1021 | 0.0898
Epoch 111/300, Loss: 0.1020 | 0.0878
Epoch 112/300, Loss: 0.1019 | 0.0902
Epoch 113/300, Loss: 0.1025 | 0.0866
Epoch 114/300, Loss: 0.1009 | 0.0898
Epoch 115/300, Loss: 0.1013 | 0.0868
Epoch 116/300, Loss: 0.0997 | 0.0878
Epoch 117/300, Loss: 0.0993 | 0.0872
Epoch 118/300, Loss: 0.0999 | 0.0870
Epoch 119/300, Loss: 0.0995 | 0.0864
Epoch 120/300, Loss: 0.0997 | 0.0869
Epoch 121/300, Loss: 0.1006 | 0.0843
Epoch 122/300, Loss: 0.1005 | 0.0879
Epoch 123/300, Loss: 0.1001 | 0.0838
Epoch 124/300, Loss: 0.0991 | 0.0858
Epoch 125/300, Loss: 0.0983 | 0.0850
Epoch 126/300, Loss: 0.0986 | 0.0842
Epoch 127/300, Loss: 0.0985 | 0.0853
Epoch 128/300, Loss: 0.0984 | 0.0847
Epoch 129/300, Loss: 0.0975 | 0.0838
Epoch 130/300, Loss: 0.0973 | 0.0837
Epoch 131/300, Loss: 0.0965 | 0.0836
Epoch 132/300, Loss: 0.0958 | 0.0836
Epoch 133/300, Loss: 0.0958 | 0.0824
Epoch 134/300, Loss: 0.0962 | 0.0845
Epoch 135/300, Loss: 0.0962 | 0.0815
Epoch 136/300, Loss: 0.0964 | 0.0853
Epoch 137/300, Loss: 0.0959 | 0.0808
Epoch 138/300, Loss: 0.0951 | 0.0844
Epoch 139/300, Loss: 0.0953 | 0.0817
Epoch 140/300, Loss: 0.0956 | 0.0831
Epoch 141/300, Loss: 0.0953 | 0.0810
Epoch 142/300, Loss: 0.0959 | 0.0844
Epoch 143/300, Loss: 0.0957 | 0.0801
Epoch 144/300, Loss: 0.0943 | 0.0835
Epoch 145/300, Loss: 0.0939 | 0.0814
Epoch 146/300, Loss: 0.0925 | 0.0829
Epoch 147/300, Loss: 0.0929 | 0.0799
Epoch 148/300, Loss: 0.0925 | 0.0809
Epoch 149/300, Loss: 0.0921 | 0.0809
Epoch 150/300, Loss: 0.0920 | 0.0808
Epoch 151/300, Loss: 0.0922 | 0.0804
Epoch 152/300, Loss: 0.0930 | 0.0808
Epoch 153/300, Loss: 0.0924 | 0.0798
Epoch 154/300, Loss: 0.0932 | 0.0821
Epoch 155/300, Loss: 0.0915 | 0.0803
Epoch 156/300, Loss: 0.0911 | 0.0806
Epoch 157/300, Loss: 0.0915 | 0.0801
Epoch 158/300, Loss: 0.0910 | 0.0809
Epoch 159/300, Loss: 0.0904 | 0.0814
Epoch 160/300, Loss: 0.0907 | 0.0794
Epoch 161/300, Loss: 0.0904 | 0.0798
Epoch 162/300, Loss: 0.0903 | 0.0787
Epoch 163/300, Loss: 0.0905 | 0.0803
Epoch 164/300, Loss: 0.0905 | 0.0787
Epoch 165/300, Loss: 0.0900 | 0.0801
Epoch 166/300, Loss: 0.0896 | 0.0790
Epoch 167/300, Loss: 0.0895 | 0.0795
Epoch 168/300, Loss: 0.0894 | 0.0784
Epoch 169/300, Loss: 0.0886 | 0.0792
Epoch 170/300, Loss: 0.0887 | 0.0786
Epoch 171/300, Loss: 0.0886 | 0.0794
Epoch 172/300, Loss: 0.0887 | 0.0783
Epoch 173/300, Loss: 0.0887 | 0.0798
Epoch 174/300, Loss: 0.0887 | 0.0772
Epoch 175/300, Loss: 0.0886 | 0.0798
Epoch 176/300, Loss: 0.0885 | 0.0775
Epoch 177/300, Loss: 0.0880 | 0.0788
Epoch 178/300, Loss: 0.0875 | 0.0775
Epoch 179/300, Loss: 0.0876 | 0.0775
Epoch 180/300, Loss: 0.0877 | 0.0770
Epoch 181/300, Loss: 0.0875 | 0.0776
Epoch 182/300, Loss: 0.0875 | 0.0780
Epoch 183/300, Loss: 0.0871 | 0.0770
Epoch 184/300, Loss: 0.0870 | 0.0781
Epoch 185/300, Loss: 0.0877 | 0.0776
Epoch 186/300, Loss: 0.0869 | 0.0785
Epoch 187/300, Loss: 0.0868 | 0.0782
Epoch 188/300, Loss: 0.0869 | 0.0785
Epoch 189/300, Loss: 0.0866 | 0.0775
Epoch 190/300, Loss: 0.0867 | 0.0775
Epoch 191/300, Loss: 0.0864 | 0.0770
Epoch 192/300, Loss: 0.0859 | 0.0765
Epoch 193/300, Loss: 0.0864 | 0.0773
Epoch 194/300, Loss: 0.0860 | 0.0762
Epoch 195/300, Loss: 0.0862 | 0.0772
Epoch 196/300, Loss: 0.0857 | 0.0760
Epoch 197/300, Loss: 0.0857 | 0.0764
Epoch 198/300, Loss: 0.0859 | 0.0762
Epoch 199/300, Loss: 0.0854 | 0.0757
Epoch 200/300, Loss: 0.0857 | 0.0768
Epoch 201/300, Loss: 0.0853 | 0.0757
Epoch 202/300, Loss: 0.0849 | 0.0779
Epoch 203/300, Loss: 0.0853 | 0.0761
Epoch 204/300, Loss: 0.0852 | 0.0768
Epoch 205/300, Loss: 0.0850 | 0.0759
Epoch 206/300, Loss: 0.0843 | 0.0758
Epoch 207/300, Loss: 0.0851 | 0.0757
Epoch 208/300, Loss: 0.0846 | 0.0762
Epoch 209/300, Loss: 0.0844 | 0.0758
Epoch 210/300, Loss: 0.0843 | 0.0764
Epoch 211/300, Loss: 0.0850 | 0.0761
Epoch 212/300, Loss: 0.0845 | 0.0747
Epoch 213/300, Loss: 0.0843 | 0.0751
Epoch 214/300, Loss: 0.0837 | 0.0755
Epoch 215/300, Loss: 0.0837 | 0.0760
Epoch 216/300, Loss: 0.0839 | 0.0759
Epoch 217/300, Loss: 0.0838 | 0.0753
Epoch 218/300, Loss: 0.0838 | 0.0753
Epoch 219/300, Loss: 0.0834 | 0.0748
Epoch 220/300, Loss: 0.0838 | 0.0756
Epoch 221/300, Loss: 0.0838 | 0.0759
Epoch 222/300, Loss: 0.0836 | 0.0752
Epoch 223/300, Loss: 0.0830 | 0.0757
Epoch 224/300, Loss: 0.0840 | 0.0754
Epoch 225/300, Loss: 0.0833 | 0.0751
Epoch 226/300, Loss: 0.0822 | 0.0751
Epoch 227/300, Loss: 0.0830 | 0.0749
Epoch 228/300, Loss: 0.0830 | 0.0750
Epoch 229/300, Loss: 0.0825 | 0.0749
Epoch 230/300, Loss: 0.0831 | 0.0753
Epoch 231/300, Loss: 0.0831 | 0.0746
Epoch 232/300, Loss: 0.0826 | 0.0751
Epoch 233/300, Loss: 0.0822 | 0.0743
Epoch 234/300, Loss: 0.0825 | 0.0746
Epoch 235/300, Loss: 0.0823 | 0.0742
Epoch 236/300, Loss: 0.0828 | 0.0737
Epoch 237/300, Loss: 0.0822 | 0.0739
Epoch 238/300, Loss: 0.0823 | 0.0740
Epoch 239/300, Loss: 0.0822 | 0.0746
Epoch 240/300, Loss: 0.0824 | 0.0752
Epoch 241/300, Loss: 0.0820 | 0.0753
Epoch 242/300, Loss: 0.0820 | 0.0753
Epoch 243/300, Loss: 0.0829 | 0.0753
Epoch 244/300, Loss: 0.0817 | 0.0750
Epoch 245/300, Loss: 0.0823 | 0.0751
Epoch 246/300, Loss: 0.0818 | 0.0745
Epoch 247/300, Loss: 0.0816 | 0.0744
Epoch 248/300, Loss: 0.0823 | 0.0745
Epoch 249/300, Loss: 0.0812 | 0.0753
Epoch 250/300, Loss: 0.0813 | 0.0745
Epoch 251/300, Loss: 0.0814 | 0.0742
Epoch 252/300, Loss: 0.0819 | 0.0747
Epoch 253/300, Loss: 0.0814 | 0.0742
Epoch 254/300, Loss: 0.0813 | 0.0742
Epoch 255/300, Loss: 0.0805 | 0.0744
Epoch 256/300, Loss: 0.0809 | 0.0742
Epoch 257/300, Loss: 0.0812 | 0.0739
Epoch 258/300, Loss: 0.0810 | 0.0742
Epoch 259/300, Loss: 0.0811 | 0.0736
Epoch 260/300, Loss: 0.0806 | 0.0737
Epoch 261/300, Loss: 0.0808 | 0.0740
Epoch 262/300, Loss: 0.0804 | 0.0738
Epoch 263/300, Loss: 0.0813 | 0.0739
Epoch 264/300, Loss: 0.0808 | 0.0739
Epoch 265/300, Loss: 0.0806 | 0.0747
Epoch 266/300, Loss: 0.0806 | 0.0743
Epoch 267/300, Loss: 0.0803 | 0.0745
Epoch 268/300, Loss: 0.0809 | 0.0730
Epoch 269/300, Loss: 0.0806 | 0.0739
Epoch 270/300, Loss: 0.0803 | 0.0730
Epoch 271/300, Loss: 0.0807 | 0.0732
Epoch 272/300, Loss: 0.0806 | 0.0734
Epoch 273/300, Loss: 0.0804 | 0.0739
Epoch 274/300, Loss: 0.0800 | 0.0733
Epoch 275/300, Loss: 0.0804 | 0.0728
Epoch 276/300, Loss: 0.0800 | 0.0725
Epoch 277/300, Loss: 0.0803 | 0.0728
Epoch 278/300, Loss: 0.0798 | 0.0734
Epoch 279/300, Loss: 0.0803 | 0.0735
Epoch 280/300, Loss: 0.0806 | 0.0732
Epoch 281/300, Loss: 0.0794 | 0.0733
Epoch 282/300, Loss: 0.0804 | 0.0730
Epoch 283/300, Loss: 0.0799 | 0.0725
Epoch 284/300, Loss: 0.0801 | 0.0729
Epoch 285/300, Loss: 0.0797 | 0.0730
Epoch 286/300, Loss: 0.0798 | 0.0735
Epoch 287/300, Loss: 0.0802 | 0.0728
Epoch 288/300, Loss: 0.0802 | 0.0723
Epoch 289/300, Loss: 0.0799 | 0.0725
Epoch 290/300, Loss: 0.0794 | 0.0723
Epoch 291/300, Loss: 0.0798 | 0.0725
Epoch 292/300, Loss: 0.0793 | 0.0729
Epoch 293/300, Loss: 0.0800 | 0.0731
Epoch 294/300, Loss: 0.0792 | 0.0731
Epoch 295/300, Loss: 0.0798 | 0.0728
Epoch 296/300, Loss: 0.0805 | 0.0726
Epoch 297/300, Loss: 0.0790 | 0.0729
Epoch 298/300, Loss: 0.0796 | 0.0730
Epoch 299/300, Loss: 0.0798 | 0.0730
Epoch 300/300, Loss: 0.0794 | 0.0731
Runtime (seconds): 146.19999766349792
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 856.7528017146979
RMSE: 29.270339965820312
MAE: 29.270339965820312
R-squared: nan
[208.05966]
