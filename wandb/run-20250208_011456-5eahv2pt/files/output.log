ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-08 01:15:01,238][0m A new study created in memory with name: no-name-5da437a8-24a3-45b4-a765-96400d8defdf[0m
[32m[I 2025-02-08 01:17:44,746][0m Trial 0 finished with value: 0.5759215384721756 and parameters: {'observation_period_num': 151, 'train_rates': 0.9144963298085889, 'learning_rate': 1.6993661296483933e-05, 'batch_size': 118, 'step_size': 11, 'gamma': 0.7925767161723288}. Best is trial 0 with value: 0.5759215384721756.[0m
[32m[I 2025-02-08 01:19:22,008][0m Trial 1 finished with value: 1.0360592069439074 and parameters: {'observation_period_num': 116, 'train_rates': 0.617928031234484, 'learning_rate': 0.00026285715108681067, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8601702129155564}. Best is trial 0 with value: 0.5759215384721756.[0m
[32m[I 2025-02-08 01:21:40,699][0m Trial 2 finished with value: 0.4803681826174129 and parameters: {'observation_period_num': 135, 'train_rates': 0.8380649104774731, 'learning_rate': 0.00016865258769773465, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9884943093612739}. Best is trial 2 with value: 0.4803681826174129.[0m
[32m[I 2025-02-08 01:23:39,753][0m Trial 3 finished with value: 0.38047936145758243 and parameters: {'observation_period_num': 119, 'train_rates': 0.8474901058213364, 'learning_rate': 0.00013140892652929782, 'batch_size': 78, 'step_size': 4, 'gamma': 0.9096727285336902}. Best is trial 3 with value: 0.38047936145758243.[0m
[32m[I 2025-02-08 01:28:30,891][0m Trial 4 finished with value: 0.5413251718942155 and parameters: {'observation_period_num': 235, 'train_rates': 0.9311096961535326, 'learning_rate': 1.9127794906943378e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.8752505693093999}. Best is trial 3 with value: 0.38047936145758243.[0m
[32m[I 2025-02-08 01:30:37,368][0m Trial 5 finished with value: 0.22956110537052155 and parameters: {'observation_period_num': 112, 'train_rates': 0.9840814851252946, 'learning_rate': 2.1606611536336062e-05, 'batch_size': 61, 'step_size': 10, 'gamma': 0.9056888140819123}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:30:59,070][0m Trial 6 finished with value: 0.7076266598766708 and parameters: {'observation_period_num': 9, 'train_rates': 0.7519077724086112, 'learning_rate': 0.0006853241010508691, 'batch_size': 220, 'step_size': 15, 'gamma': 0.8459578117035969}. Best is trial 5 with value: 0.22956110537052155.[0m
Early stopping at epoch 67
[32m[I 2025-02-08 01:33:45,465][0m Trial 7 finished with value: 2.3495179578292467 and parameters: {'observation_period_num': 227, 'train_rates': 0.7784869297817656, 'learning_rate': 1.9081758618960565e-06, 'batch_size': 104, 'step_size': 1, 'gamma': 0.8606069712787955}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:36:53,857][0m Trial 8 finished with value: 2.238003188742349 and parameters: {'observation_period_num': 202, 'train_rates': 0.6321656266920487, 'learning_rate': 1.9447110326167835e-06, 'batch_size': 205, 'step_size': 5, 'gamma': 0.8339208026390492}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:37:12,188][0m Trial 9 finished with value: 0.7763682655791682 and parameters: {'observation_period_num': 5, 'train_rates': 0.7202587213265095, 'learning_rate': 4.7250807028066285e-05, 'batch_size': 237, 'step_size': 11, 'gamma': 0.8975436775500423}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:41:26,532][0m Trial 10 finished with value: 0.2816609133567129 and parameters: {'observation_period_num': 66, 'train_rates': 0.966359631187177, 'learning_rate': 8.042783949531976e-06, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9592250204112944}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:43:35,867][0m Trial 11 finished with value: 0.34563597147144487 and parameters: {'observation_period_num': 68, 'train_rates': 0.9770569661349958, 'learning_rate': 7.757327886491067e-06, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9638062878039743}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:48:42,116][0m Trial 12 finished with value: 0.2426816515530212 and parameters: {'observation_period_num': 64, 'train_rates': 0.9729315260733201, 'learning_rate': 6.749837477575686e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9444692326066144}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:49:41,077][0m Trial 13 finished with value: 0.8864054341925267 and parameters: {'observation_period_num': 62, 'train_rates': 0.8891803133309302, 'learning_rate': 5.42919088683475e-06, 'batch_size': 161, 'step_size': 12, 'gamma': 0.9252047758567424}. Best is trial 5 with value: 0.22956110537052155.[0m
[32m[I 2025-02-08 01:51:30,348][0m Trial 14 finished with value: 0.15859118103981018 and parameters: {'observation_period_num': 85, 'train_rates': 0.9884395873706885, 'learning_rate': 4.66069714817907e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9401246206272881}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 01:54:48,115][0m Trial 15 finished with value: 0.42365814349078157 and parameters: {'observation_period_num': 172, 'train_rates': 0.8596958740612919, 'learning_rate': 5.777069381371186e-05, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8085280045218879}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 01:56:31,497][0m Trial 16 finished with value: 0.2781645357608795 and parameters: {'observation_period_num': 98, 'train_rates': 0.9898710127301524, 'learning_rate': 7.24397607512302e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8914127159308283}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 01:58:18,415][0m Trial 17 finished with value: 0.44794236928858655 and parameters: {'observation_period_num': 95, 'train_rates': 0.9180644255992492, 'learning_rate': 2.8877897086761746e-05, 'batch_size': 56, 'step_size': 9, 'gamma': 0.7678531566513711}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 01:58:50,187][0m Trial 18 finished with value: 1.0019499699736751 and parameters: {'observation_period_num': 32, 'train_rates': 0.706092222874146, 'learning_rate': 0.00042242667415678253, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9258941508715637}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:01:57,831][0m Trial 19 finished with value: 1.0288745713011127 and parameters: {'observation_period_num': 171, 'train_rates': 0.8080145647917915, 'learning_rate': 1.0223265223532329e-06, 'batch_size': 43, 'step_size': 13, 'gamma': 0.9354417181846179}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:03:40,567][0m Trial 20 finished with value: 0.29276426073276635 and parameters: {'observation_period_num': 99, 'train_rates': 0.9424683199024404, 'learning_rate': 0.00011622038378868452, 'batch_size': 127, 'step_size': 3, 'gamma': 0.9869221271323327}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:07:26,046][0m Trial 21 finished with value: 0.17248752117156982 and parameters: {'observation_period_num': 43, 'train_rates': 0.9899680491206693, 'learning_rate': 1.4829042020678834e-05, 'batch_size': 22, 'step_size': 13, 'gamma': 0.9463871317541271}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:08:32,122][0m Trial 22 finished with value: 0.4352863362968661 and parameters: {'observation_period_num': 42, 'train_rates': 0.8821993409160667, 'learning_rate': 1.7488625989105664e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.9097678797747645}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:10:56,091][0m Trial 23 finished with value: 0.25609675706840873 and parameters: {'observation_period_num': 85, 'train_rates': 0.9560293000671305, 'learning_rate': 3.0434881284371366e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9553028791857145}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:13:43,554][0m Trial 24 finished with value: 0.2932573354469155 and parameters: {'observation_period_num': 36, 'train_rates': 0.9093185842220969, 'learning_rate': 1.4734127989554866e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8865637563699562}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:16:29,065][0m Trial 25 finished with value: 0.9605613162643031 and parameters: {'observation_period_num': 138, 'train_rates': 0.9395534811581527, 'learning_rate': 3.7067682586444397e-06, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9693913215027751}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:17:30,619][0m Trial 26 finished with value: 0.20249967277050018 and parameters: {'observation_period_num': 47, 'train_rates': 0.9883523665803421, 'learning_rate': 4.3536894812189024e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9158680026264507}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:18:26,409][0m Trial 27 finished with value: 0.3026187078297487 and parameters: {'observation_period_num': 44, 'train_rates': 0.8827784816804043, 'learning_rate': 8.122137801828291e-05, 'batch_size': 88, 'step_size': 14, 'gamma': 0.9239038563466065}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:19:55,288][0m Trial 28 finished with value: 0.8129153948194114 and parameters: {'observation_period_num': 20, 'train_rates': 0.6642458439374817, 'learning_rate': 4.479616032063876e-05, 'batch_size': 42, 'step_size': 12, 'gamma': 0.9511639354055703}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:21:19,792][0m Trial 29 finished with value: 0.5995921800497274 and parameters: {'observation_period_num': 82, 'train_rates': 0.9057245243120688, 'learning_rate': 9.746743627740046e-06, 'batch_size': 111, 'step_size': 14, 'gamma': 0.938144510314141}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:22:08,839][0m Trial 30 finished with value: 0.7866472598141844 and parameters: {'observation_period_num': 46, 'train_rates': 0.9456185967659356, 'learning_rate': 1.2220722805879165e-05, 'batch_size': 140, 'step_size': 11, 'gamma': 0.9183894101655901}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:25:16,848][0m Trial 31 finished with value: 0.21855682134628296 and parameters: {'observation_period_num': 156, 'train_rates': 0.9886306340158233, 'learning_rate': 2.782418227905528e-05, 'batch_size': 61, 'step_size': 10, 'gamma': 0.9070756221047276}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:28:44,854][0m Trial 32 finished with value: 0.3892739746305678 and parameters: {'observation_period_num': 172, 'train_rates': 0.9612485958446984, 'learning_rate': 3.298607926146407e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8777658922424038}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:31:43,990][0m Trial 33 finished with value: 0.1631862372159958 and parameters: {'observation_period_num': 145, 'train_rates': 0.9890592171392836, 'learning_rate': 9.12451431135529e-05, 'batch_size': 51, 'step_size': 8, 'gamma': 0.97337332688027}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:34:19,393][0m Trial 34 finished with value: 0.30032455650872963 and parameters: {'observation_period_num': 123, 'train_rates': 0.9244131987023226, 'learning_rate': 0.000254087754526431, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9737418167097148}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:37:09,016][0m Trial 35 finished with value: 0.24762110092810222 and parameters: {'observation_period_num': 148, 'train_rates': 0.9502859637265682, 'learning_rate': 0.00010825877875755561, 'batch_size': 95, 'step_size': 8, 'gamma': 0.9793435993142778}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:38:55,620][0m Trial 36 finished with value: 0.27930637686810594 and parameters: {'observation_period_num': 24, 'train_rates': 0.9682684738441035, 'learning_rate': 0.00022853222506875426, 'batch_size': 45, 'step_size': 8, 'gamma': 0.9393328061690618}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:40:00,784][0m Trial 37 finished with value: 0.34436173930144837 and parameters: {'observation_period_num': 54, 'train_rates': 0.8591850933047058, 'learning_rate': 6.409203286046968e-05, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9513915116182007}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:43:14,382][0m Trial 38 finished with value: 0.5074009209040151 and parameters: {'observation_period_num': 113, 'train_rates': 0.8236593617816154, 'learning_rate': 0.00017889797599990822, 'batch_size': 22, 'step_size': 5, 'gamma': 0.983092788405308}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:44:46,877][0m Trial 39 finished with value: 0.265200373215891 and parameters: {'observation_period_num': 82, 'train_rates': 0.9308270481992782, 'learning_rate': 8.574302951107823e-05, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9676318344101232}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:49:15,808][0m Trial 40 finished with value: 0.198318213224411 and parameters: {'observation_period_num': 212, 'train_rates': 0.9891873978951016, 'learning_rate': 0.0003618375269865532, 'batch_size': 120, 'step_size': 12, 'gamma': 0.9353803982235787}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:54:37,924][0m Trial 41 finished with value: 0.20902752876281738 and parameters: {'observation_period_num': 249, 'train_rates': 0.9829105863697621, 'learning_rate': 0.00043397233775144576, 'batch_size': 125, 'step_size': 12, 'gamma': 0.9337488853879541}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 02:58:32,945][0m Trial 42 finished with value: 0.9222865174482534 and parameters: {'observation_period_num': 195, 'train_rates': 0.9599233269131774, 'learning_rate': 0.000960183100656628, 'batch_size': 100, 'step_size': 14, 'gamma': 0.9160211517961094}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:03:05,021][0m Trial 43 finished with value: 0.2648506760597229 and parameters: {'observation_period_num': 212, 'train_rates': 0.9850383453308449, 'learning_rate': 0.0004441619335377218, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9496520271504634}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:07:46,862][0m Trial 44 finished with value: 0.2556019723415375 and parameters: {'observation_period_num': 228, 'train_rates': 0.9606801932593992, 'learning_rate': 0.00016299283946676638, 'batch_size': 156, 'step_size': 11, 'gamma': 0.9892350808657983}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:08:40,705][0m Trial 45 finished with value: 0.6307203592732549 and parameters: {'observation_period_num': 53, 'train_rates': 0.9340713299022634, 'learning_rate': 2.199476377464949e-05, 'batch_size': 179, 'step_size': 15, 'gamma': 0.8994108583923767}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:12:39,476][0m Trial 46 finished with value: 0.28224797649817035 and parameters: {'observation_period_num': 194, 'train_rates': 0.9004547191884753, 'learning_rate': 4.2377882958570466e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8597920629295841}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:15:13,475][0m Trial 47 finished with value: 0.2901700437068939 and parameters: {'observation_period_num': 130, 'train_rates': 0.9734603348965505, 'learning_rate': 0.00033169665543254486, 'batch_size': 107, 'step_size': 6, 'gamma': 0.9604398090381597}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:18:59,809][0m Trial 48 finished with value: 0.674180890881009 and parameters: {'observation_period_num': 211, 'train_rates': 0.7756782510275447, 'learning_rate': 4.892382884673721e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.9311628637326301}. Best is trial 14 with value: 0.15859118103981018.[0m
[32m[I 2025-02-08 03:19:54,923][0m Trial 49 finished with value: 1.3548268952739615 and parameters: {'observation_period_num': 17, 'train_rates': 0.6010826391871824, 'learning_rate': 0.00010005168017389368, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9445520889569153}. Best is trial 14 with value: 0.15859118103981018.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 03:19:54,930][0m A new study created in memory with name: no-name-0afcabc5-4bef-4fd2-a518-0cdc54f4baa2[0m
[32m[I 2025-02-08 03:22:00,670][0m Trial 0 finished with value: 0.9823395418934524 and parameters: {'observation_period_num': 135, 'train_rates': 0.6372072634172324, 'learning_rate': 1.1938765337856615e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9077970473712763}. Best is trial 0 with value: 0.9823395418934524.[0m
[32m[I 2025-02-08 03:23:05,374][0m Trial 1 finished with value: 0.9611870015948272 and parameters: {'observation_period_num': 68, 'train_rates': 0.832670706060678, 'learning_rate': 1.3101783747520866e-05, 'batch_size': 212, 'step_size': 4, 'gamma': 0.9098554318467047}. Best is trial 1 with value: 0.9611870015948272.[0m
[32m[I 2025-02-08 03:26:00,167][0m Trial 2 finished with value: 1.058335554343147 and parameters: {'observation_period_num': 173, 'train_rates': 0.8166381095852331, 'learning_rate': 1.9133082241921377e-05, 'batch_size': 178, 'step_size': 4, 'gamma': 0.8743831886385857}. Best is trial 1 with value: 0.9611870015948272.[0m
[32m[I 2025-02-08 03:26:28,825][0m Trial 3 finished with value: 0.9947803782932649 and parameters: {'observation_period_num': 32, 'train_rates': 0.7305621960875865, 'learning_rate': 2.9692400356149586e-05, 'batch_size': 248, 'step_size': 10, 'gamma': 0.7920322820855867}. Best is trial 1 with value: 0.9611870015948272.[0m
[32m[I 2025-02-08 03:31:07,214][0m Trial 4 finished with value: 1.149938724513324 and parameters: {'observation_period_num': 243, 'train_rates': 0.8440058732495878, 'learning_rate': 1.1642400905014586e-05, 'batch_size': 189, 'step_size': 9, 'gamma': 0.7737627627653214}. Best is trial 1 with value: 0.9611870015948272.[0m
[32m[I 2025-02-08 03:34:33,003][0m Trial 5 finished with value: 0.9057337662760755 and parameters: {'observation_period_num': 201, 'train_rates': 0.6338318846805276, 'learning_rate': 2.0530296636932494e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8595366367106587}. Best is trial 5 with value: 0.9057337662760755.[0m
[32m[I 2025-02-08 03:36:05,390][0m Trial 6 finished with value: 0.43378574694140576 and parameters: {'observation_period_num': 92, 'train_rates': 0.8135833906180473, 'learning_rate': 8.174433483077366e-05, 'batch_size': 96, 'step_size': 14, 'gamma': 0.9285694648370674}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:39:32,224][0m Trial 7 finished with value: 0.7391396462058472 and parameters: {'observation_period_num': 203, 'train_rates': 0.747915382706587, 'learning_rate': 0.000703830401976986, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8721215589904281}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:42:48,236][0m Trial 8 finished with value: 0.98552957932714 and parameters: {'observation_period_num': 206, 'train_rates': 0.6386973266136474, 'learning_rate': 9.43957316732382e-05, 'batch_size': 70, 'step_size': 4, 'gamma': 0.8895792214836439}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:45:12,562][0m Trial 9 finished with value: 0.9934304014938634 and parameters: {'observation_period_num': 167, 'train_rates': 0.6472588467374359, 'learning_rate': 0.00019885400932156266, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8950443388021169}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:46:31,554][0m Trial 10 finished with value: 0.7886144518852234 and parameters: {'observation_period_num': 71, 'train_rates': 0.9819017072964266, 'learning_rate': 2.4292296977188748e-06, 'batch_size': 107, 'step_size': 15, 'gamma': 0.9814097469896197}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:48:02,343][0m Trial 11 finished with value: 1.0756516604783148 and parameters: {'observation_period_num': 103, 'train_rates': 0.749167780364686, 'learning_rate': 0.0007397088089961898, 'batch_size': 139, 'step_size': 15, 'gamma': 0.9667496263184168}. Best is trial 6 with value: 0.43378574694140576.[0m
Early stopping at epoch 73
[32m[I 2025-02-08 03:49:32,835][0m Trial 12 finished with value: 0.8256292060373586 and parameters: {'observation_period_num': 116, 'train_rates': 0.8969391546954264, 'learning_rate': 0.0009352557283692374, 'batch_size': 131, 'step_size': 1, 'gamma': 0.8287316848860058}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:53:58,001][0m Trial 13 finished with value: 0.8514386167822603 and parameters: {'observation_period_num': 251, 'train_rates': 0.7352309035672253, 'learning_rate': 0.0001904596581670549, 'batch_size': 91, 'step_size': 13, 'gamma': 0.9383559449285005}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:56:39,033][0m Trial 14 finished with value: 0.4390354312739326 and parameters: {'observation_period_num': 146, 'train_rates': 0.8903187670181798, 'learning_rate': 8.91305707735236e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8392376672019505}. Best is trial 6 with value: 0.43378574694140576.[0m
[32m[I 2025-02-08 03:57:14,491][0m Trial 15 finished with value: 0.39288383638355096 and parameters: {'observation_period_num': 12, 'train_rates': 0.9035184758645054, 'learning_rate': 8.153392528269979e-05, 'batch_size': 140, 'step_size': 6, 'gamma': 0.8239994004938348}. Best is trial 15 with value: 0.39288383638355096.[0m
[32m[I 2025-02-08 03:58:08,193][0m Trial 16 finished with value: 0.386347234249115 and parameters: {'observation_period_num': 12, 'train_rates': 0.9725316128780273, 'learning_rate': 6.246990976493045e-05, 'batch_size': 96, 'step_size': 7, 'gamma': 0.8101342803150985}. Best is trial 16 with value: 0.386347234249115.[0m
[32m[I 2025-02-08 03:58:53,178][0m Trial 17 finished with value: 1.3222520351409912 and parameters: {'observation_period_num': 5, 'train_rates': 0.9742916931442542, 'learning_rate': 4.578207697952711e-06, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8054869546521106}. Best is trial 16 with value: 0.386347234249115.[0m
[32m[I 2025-02-08 04:00:03,077][0m Trial 18 finished with value: 0.44499868845594104 and parameters: {'observation_period_num': 31, 'train_rates': 0.9294424767121727, 'learning_rate': 5.2840135067431024e-05, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8175567750457661}. Best is trial 16 with value: 0.386347234249115.[0m
[32m[I 2025-02-08 04:00:34,728][0m Trial 19 finished with value: 0.5685455250691195 and parameters: {'observation_period_num': 8, 'train_rates': 0.9173004239658253, 'learning_rate': 0.00029295610940410544, 'batch_size': 159, 'step_size': 3, 'gamma': 0.7573812785184321}. Best is trial 16 with value: 0.386347234249115.[0m
[32m[I 2025-02-08 04:01:25,083][0m Trial 20 finished with value: 2.703075885772705 and parameters: {'observation_period_num': 50, 'train_rates': 0.9497323728327517, 'learning_rate': 1.2697454079602844e-06, 'batch_size': 233, 'step_size': 6, 'gamma': 0.8485648394842769}. Best is trial 16 with value: 0.386347234249115.[0m
[32m[I 2025-02-08 04:02:41,057][0m Trial 21 finished with value: 0.336836524247154 and parameters: {'observation_period_num': 71, 'train_rates': 0.8720806276216887, 'learning_rate': 5.9258739986207876e-05, 'batch_size': 89, 'step_size': 12, 'gamma': 0.9500231953191441}. Best is trial 21 with value: 0.336836524247154.[0m
[32m[I 2025-02-08 04:03:42,009][0m Trial 22 finished with value: 0.4192073936634015 and parameters: {'observation_period_num': 36, 'train_rates': 0.8670774946741003, 'learning_rate': 4.812646248619634e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8062623954007051}. Best is trial 21 with value: 0.336836524247154.[0m
[32m[I 2025-02-08 04:04:55,828][0m Trial 23 finished with value: 0.3876630506318086 and parameters: {'observation_period_num': 69, 'train_rates': 0.9456962967250641, 'learning_rate': 0.0001472468468246187, 'batch_size': 120, 'step_size': 7, 'gamma': 0.7803708091586055}. Best is trial 21 with value: 0.336836524247154.[0m
[32m[I 2025-02-08 04:06:35,546][0m Trial 24 finished with value: 0.2850082052284724 and parameters: {'observation_period_num': 76, 'train_rates': 0.9494744046465549, 'learning_rate': 0.00036776748821293147, 'batch_size': 49, 'step_size': 8, 'gamma': 0.7864841365653487}. Best is trial 24 with value: 0.2850082052284724.[0m
[32m[I 2025-02-08 04:08:32,973][0m Trial 25 finished with value: 0.20294970273971558 and parameters: {'observation_period_num': 90, 'train_rates': 0.9874876705547574, 'learning_rate': 0.0003272169527660895, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7648341673697985}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:12:03,758][0m Trial 26 finished with value: 0.5736134192234353 and parameters: {'observation_period_num': 91, 'train_rates': 0.8622742459808566, 'learning_rate': 0.00040599134884844234, 'batch_size': 21, 'step_size': 11, 'gamma': 0.7596275959667363}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:14:22,390][0m Trial 27 finished with value: 0.2724285525935037 and parameters: {'observation_period_num': 114, 'train_rates': 0.9509785404887761, 'learning_rate': 0.00035069581120652886, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9561772460300251}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:16:36,821][0m Trial 28 finished with value: 0.28812673730815913 and parameters: {'observation_period_num': 115, 'train_rates': 0.9512674952967853, 'learning_rate': 0.00042732737723259696, 'batch_size': 46, 'step_size': 11, 'gamma': 0.750346086652159}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:19:35,434][0m Trial 29 finished with value: 0.2271728217601776 and parameters: {'observation_period_num': 141, 'train_rates': 0.9890937708396951, 'learning_rate': 0.00041955229773172416, 'batch_size': 41, 'step_size': 10, 'gamma': 0.7815933386174541}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:22:33,821][0m Trial 30 finished with value: 0.4453974664211273 and parameters: {'observation_period_num': 140, 'train_rates': 0.9893529825593285, 'learning_rate': 0.0005563750158279437, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9099243202713959}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:24:54,745][0m Trial 31 finished with value: 0.25980098521898665 and parameters: {'observation_period_num': 127, 'train_rates': 0.9334191948228497, 'learning_rate': 0.0003003013201109442, 'batch_size': 59, 'step_size': 8, 'gamma': 0.7861669505405581}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:28:02,855][0m Trial 32 finished with value: 0.24041823290434539 and parameters: {'observation_period_num': 158, 'train_rates': 0.9273918698474286, 'learning_rate': 0.00023810534736767608, 'batch_size': 59, 'step_size': 11, 'gamma': 0.7721326238851804}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:30:59,665][0m Trial 33 finished with value: 0.2843982247174333 and parameters: {'observation_period_num': 155, 'train_rates': 0.9222519433076869, 'learning_rate': 0.00021252744718371414, 'batch_size': 63, 'step_size': 8, 'gamma': 0.7700912176861081}. Best is trial 25 with value: 0.20294970273971558.[0m
[32m[I 2025-02-08 04:33:57,940][0m Trial 34 finished with value: 0.15406344830989838 and parameters: {'observation_period_num': 130, 'train_rates': 0.9897768091602662, 'learning_rate': 0.0001440099845024324, 'batch_size': 33, 'step_size': 13, 'gamma': 0.7982001698660561}. Best is trial 34 with value: 0.15406344830989838.[0m
[32m[I 2025-02-08 04:37:54,349][0m Trial 35 finished with value: 0.22204769835915678 and parameters: {'observation_period_num': 185, 'train_rates': 0.9691325288010368, 'learning_rate': 0.0001328009936690519, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7966088574317994}. Best is trial 34 with value: 0.15406344830989838.[0m
[32m[I 2025-02-08 04:42:54,278][0m Trial 36 finished with value: 0.12370998039841652 and parameters: {'observation_period_num': 185, 'train_rates': 0.9885532667148774, 'learning_rate': 0.00012043003084372219, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7962720967447972}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 04:47:05,514][0m Trial 37 finished with value: 0.6720204359273182 and parameters: {'observation_period_num': 175, 'train_rates': 0.7837948068502313, 'learning_rate': 0.0001435854021614594, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7957884654303449}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 04:50:46,755][0m Trial 38 finished with value: 1.0402347754656114 and parameters: {'observation_period_num': 225, 'train_rates': 0.6033007464011534, 'learning_rate': 3.3118974896296886e-05, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8403407506405678}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 04:55:03,528][0m Trial 39 finished with value: 0.23675568369419678 and parameters: {'observation_period_num': 195, 'train_rates': 0.9670132051550264, 'learning_rate': 0.00016165971671650253, 'batch_size': 33, 'step_size': 14, 'gamma': 0.7973983885959369}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 04:58:53,224][0m Trial 40 finished with value: 0.9604853125061931 and parameters: {'observation_period_num': 188, 'train_rates': 0.7038275602229415, 'learning_rate': 0.00011624135306241994, 'batch_size': 17, 'step_size': 13, 'gamma': 0.7649680533249849}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:03:47,897][0m Trial 41 finished with value: 0.27408477663993835 and parameters: {'observation_period_num': 220, 'train_rates': 0.98914236795013, 'learning_rate': 0.0005552071214353826, 'batch_size': 38, 'step_size': 13, 'gamma': 0.7800824304466681}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:07:30,879][0m Trial 42 finished with value: 0.22294764001579845 and parameters: {'observation_period_num': 176, 'train_rates': 0.9635669842530831, 'learning_rate': 0.00011851904997761258, 'batch_size': 35, 'step_size': 15, 'gamma': 0.7969491802359374}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:11:32,825][0m Trial 43 finished with value: 0.23084642631667002 and parameters: {'observation_period_num': 179, 'train_rates': 0.9622397705145211, 'learning_rate': 0.00011732392765694767, 'batch_size': 25, 'step_size': 15, 'gamma': 0.7999957039611876}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:16:01,992][0m Trial 44 finished with value: 0.5769630983108427 and parameters: {'observation_period_num': 215, 'train_rates': 0.970369842931845, 'learning_rate': 1.3752577604514323e-05, 'batch_size': 77, 'step_size': 14, 'gamma': 0.8193847234289133}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:20:06,548][0m Trial 45 finished with value: 0.26092979311943054 and parameters: {'observation_period_num': 188, 'train_rates': 0.9053397217338661, 'learning_rate': 0.00011308334014969013, 'batch_size': 29, 'step_size': 15, 'gamma': 0.8607333987542618}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:23:21,929][0m Trial 46 finished with value: 0.31255742584197027 and parameters: {'observation_period_num': 162, 'train_rates': 0.9622734518415655, 'learning_rate': 4.13642948990756e-05, 'batch_size': 55, 'step_size': 14, 'gamma': 0.7918963930449394}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:27:52,647][0m Trial 47 finished with value: 0.64821013615563 and parameters: {'observation_period_num': 236, 'train_rates': 0.8294730745022166, 'learning_rate': 2.5857445072195143e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.8368874003870154}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:30:32,756][0m Trial 48 finished with value: 0.2532144675307043 and parameters: {'observation_period_num': 129, 'train_rates': 0.8829326348928181, 'learning_rate': 8.006833984853443e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8145850997526665}. Best is trial 36 with value: 0.12370998039841652.[0m
[32m[I 2025-02-08 05:33:29,948][0m Trial 49 finished with value: 0.25803489051759243 and parameters: {'observation_period_num': 96, 'train_rates': 0.935834195622417, 'learning_rate': 0.0002331973997013109, 'batch_size': 27, 'step_size': 14, 'gamma': 0.7529108732684925}. Best is trial 36 with value: 0.12370998039841652.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-08 05:33:29,956][0m A new study created in memory with name: no-name-2d1458fb-ad07-4d58-8637-5fbb44d73ffb[0m
[32m[I 2025-02-08 05:35:56,604][0m Trial 0 finished with value: 1.5799308856499055 and parameters: {'observation_period_num': 162, 'train_rates': 0.6669422932673488, 'learning_rate': 5.230890035276081e-06, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9212965220829403}. Best is trial 0 with value: 1.5799308856499055.[0m
[32m[I 2025-02-08 05:39:55,402][0m Trial 1 finished with value: 1.873301735924333 and parameters: {'observation_period_num': 93, 'train_rates': 0.7269263756160311, 'learning_rate': 0.0005961700047799614, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9324152927904629}. Best is trial 0 with value: 1.5799308856499055.[0m
[32m[I 2025-02-08 05:42:07,393][0m Trial 2 finished with value: 1.8890605419316095 and parameters: {'observation_period_num': 159, 'train_rates': 0.6182844676915711, 'learning_rate': 1.2682959630262695e-06, 'batch_size': 149, 'step_size': 5, 'gamma': 0.867180072492263}. Best is trial 0 with value: 1.5799308856499055.[0m
[32m[I 2025-02-08 05:45:45,501][0m Trial 3 finished with value: 1.4868988990783691 and parameters: {'observation_period_num': 189, 'train_rates': 0.9667479844566683, 'learning_rate': 9.032640459056738e-06, 'batch_size': 214, 'step_size': 4, 'gamma': 0.7567734764926334}. Best is trial 3 with value: 1.4868988990783691.[0m
[32m[I 2025-02-08 05:46:47,453][0m Trial 4 finished with value: 1.69933949874135 and parameters: {'observation_period_num': 45, 'train_rates': 0.8165842328037378, 'learning_rate': 2.278502905796941e-06, 'batch_size': 73, 'step_size': 3, 'gamma': 0.8616807215615768}. Best is trial 3 with value: 1.4868988990783691.[0m
[32m[I 2025-02-08 05:51:48,266][0m Trial 5 finished with value: 0.6519765495284786 and parameters: {'observation_period_num': 245, 'train_rates': 0.9095858706371891, 'learning_rate': 0.0006463035039232192, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9541536416266775}. Best is trial 5 with value: 0.6519765495284786.[0m
[32m[I 2025-02-08 05:55:52,865][0m Trial 6 finished with value: 0.7843075440480158 and parameters: {'observation_period_num': 191, 'train_rates': 0.9765684345779518, 'learning_rate': 0.0005080968493845722, 'batch_size': 40, 'step_size': 15, 'gamma': 0.9718587333046871}. Best is trial 5 with value: 0.6519765495284786.[0m
[32m[I 2025-02-08 06:00:20,192][0m Trial 7 finished with value: 0.43301965296268463 and parameters: {'observation_period_num': 222, 'train_rates': 0.9371906900533375, 'learning_rate': 5.351395297706369e-05, 'batch_size': 139, 'step_size': 10, 'gamma': 0.8905016192667222}. Best is trial 7 with value: 0.43301965296268463.[0m
[32m[I 2025-02-08 06:00:58,773][0m Trial 8 finished with value: 0.4721431247455137 and parameters: {'observation_period_num': 36, 'train_rates': 0.9120511516208671, 'learning_rate': 0.00010717838806675731, 'batch_size': 196, 'step_size': 3, 'gamma': 0.8836139081299376}. Best is trial 7 with value: 0.43301965296268463.[0m
[32m[I 2025-02-08 06:01:43,124][0m Trial 9 finished with value: 0.5576833023669872 and parameters: {'observation_period_num': 42, 'train_rates': 0.8389753275180294, 'learning_rate': 3.193632548834673e-05, 'batch_size': 137, 'step_size': 12, 'gamma': 0.9683576340206166}. Best is trial 7 with value: 0.43301965296268463.[0m
[32m[I 2025-02-08 06:06:08,128][0m Trial 10 finished with value: 0.8444527482270171 and parameters: {'observation_period_num': 246, 'train_rates': 0.7422843426876797, 'learning_rate': 8.173054575538745e-05, 'batch_size': 252, 'step_size': 10, 'gamma': 0.8062251361649461}. Best is trial 7 with value: 0.43301965296268463.[0m
[32m[I 2025-02-08 06:06:34,341][0m Trial 11 finished with value: 0.389835469314834 and parameters: {'observation_period_num': 5, 'train_rates': 0.8851176340892739, 'learning_rate': 9.155591308405943e-05, 'batch_size': 187, 'step_size': 7, 'gamma': 0.8821713000479089}. Best is trial 11 with value: 0.389835469314834.[0m
[32m[I 2025-02-08 06:08:12,029][0m Trial 12 finished with value: 0.3642900724085573 and parameters: {'observation_period_num': 101, 'train_rates': 0.8820080396244004, 'learning_rate': 0.00012295608047342704, 'batch_size': 174, 'step_size': 7, 'gamma': 0.8216966587646404}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:09:47,853][0m Trial 13 finished with value: 0.3994498886461333 and parameters: {'observation_period_num': 101, 'train_rates': 0.8665572657173115, 'learning_rate': 0.0002033660387652346, 'batch_size': 184, 'step_size': 7, 'gamma': 0.8209951848598801}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:10:14,763][0m Trial 14 finished with value: 1.1145585769275477 and parameters: {'observation_period_num': 5, 'train_rates': 0.7683558105164529, 'learning_rate': 1.6513827383543486e-05, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8248233445147422}. Best is trial 12 with value: 0.3642900724085573.[0m
Early stopping at epoch 50
[32m[I 2025-02-08 06:10:59,418][0m Trial 15 finished with value: 0.9280040922164917 and parameters: {'observation_period_num': 91, 'train_rates': 0.8693679826147196, 'learning_rate': 0.00020203894176313764, 'batch_size': 236, 'step_size': 1, 'gamma': 0.7675092537034253}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:13:16,504][0m Trial 16 finished with value: 0.3898086280221642 and parameters: {'observation_period_num': 129, 'train_rates': 0.8753141710609479, 'learning_rate': 0.00015329434070138822, 'batch_size': 109, 'step_size': 6, 'gamma': 0.8414961631902237}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:15:14,479][0m Trial 17 finished with value: 0.4881241387561943 and parameters: {'observation_period_num': 127, 'train_rates': 0.7959439944345726, 'learning_rate': 0.00023319553442583248, 'batch_size': 104, 'step_size': 10, 'gamma': 0.7929161948039636}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:17:34,118][0m Trial 18 finished with value: 0.7026556170716578 and parameters: {'observation_period_num': 134, 'train_rates': 0.8437303306264496, 'learning_rate': 3.113039415208274e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.8448606624588013}. Best is trial 12 with value: 0.3642900724085573.[0m
Early stopping at epoch 57
[32m[I 2025-02-08 06:18:48,674][0m Trial 19 finished with value: 0.9046748659827493 and parameters: {'observation_period_num': 126, 'train_rates': 0.9379353617699464, 'learning_rate': 0.0003204177197931975, 'batch_size': 160, 'step_size': 1, 'gamma': 0.7909952240728424}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:19:49,899][0m Trial 20 finished with value: 1.5093770788129244 and parameters: {'observation_period_num': 66, 'train_rates': 0.6985988208401753, 'learning_rate': 0.000874042852294735, 'batch_size': 112, 'step_size': 9, 'gamma': 0.8416854087053983}. Best is trial 12 with value: 0.3642900724085573.[0m
[32m[I 2025-02-08 06:20:14,880][0m Trial 21 finished with value: 0.3209327570870965 and parameters: {'observation_period_num': 16, 'train_rates': 0.8866095464393317, 'learning_rate': 9.68816595750631e-05, 'batch_size': 206, 'step_size': 7, 'gamma': 0.9023078861839472}. Best is trial 21 with value: 0.3209327570870965.[0m
[32m[I 2025-02-08 06:21:21,206][0m Trial 22 finished with value: 0.35292642312034533 and parameters: {'observation_period_num': 67, 'train_rates': 0.8926243007045591, 'learning_rate': 0.00013471373497689107, 'batch_size': 220, 'step_size': 6, 'gamma': 0.9117473069609672}. Best is trial 21 with value: 0.3209327570870965.[0m
[32m[I 2025-02-08 06:22:38,564][0m Trial 23 finished with value: 0.43920493936538696 and parameters: {'observation_period_num': 77, 'train_rates': 0.9131818351196715, 'learning_rate': 4.481860898973539e-05, 'batch_size': 216, 'step_size': 8, 'gamma': 0.9144103411699691}. Best is trial 21 with value: 0.3209327570870965.[0m
[32m[I 2025-02-08 06:23:31,746][0m Trial 24 finished with value: 0.9214365670368785 and parameters: {'observation_period_num': 60, 'train_rates': 0.8260256980628173, 'learning_rate': 1.611290645962898e-05, 'batch_size': 219, 'step_size': 6, 'gamma': 0.9029161891579907}. Best is trial 21 with value: 0.3209327570870965.[0m
[32m[I 2025-02-08 06:24:01,379][0m Trial 25 finished with value: 0.269272118806839 and parameters: {'observation_period_num': 26, 'train_rates': 0.9482701643359167, 'learning_rate': 0.0003961442088780366, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9444989133663603}. Best is trial 25 with value: 0.269272118806839.[0m
[32m[I 2025-02-08 06:24:32,381][0m Trial 26 finished with value: 0.17223714292049408 and parameters: {'observation_period_num': 27, 'train_rates': 0.9838613872863935, 'learning_rate': 0.00033681638647322054, 'batch_size': 248, 'step_size': 11, 'gamma': 0.9899068918842432}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:25:00,604][0m Trial 27 finished with value: 0.17229114472866058 and parameters: {'observation_period_num': 23, 'train_rates': 0.9894946421646122, 'learning_rate': 0.0002935472577584965, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9466421190918446}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:25:34,168][0m Trial 28 finished with value: 0.29839566349983215 and parameters: {'observation_period_num': 30, 'train_rates': 0.9569698351501345, 'learning_rate': 0.00035675007907828095, 'batch_size': 251, 'step_size': 12, 'gamma': 0.9886031199139466}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:26:06,721][0m Trial 29 finished with value: 0.39597809314727783 and parameters: {'observation_period_num': 27, 'train_rates': 0.9851683859040115, 'learning_rate': 0.0008576745498512349, 'batch_size': 237, 'step_size': 12, 'gamma': 0.934646437389432}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:26:58,872][0m Trial 30 finished with value: 0.2006712555885315 and parameters: {'observation_period_num': 52, 'train_rates': 0.989640035399378, 'learning_rate': 0.00038692505012933144, 'batch_size': 234, 'step_size': 13, 'gamma': 0.9896700935725912}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:27:48,564][0m Trial 31 finished with value: 0.2838843762874603 and parameters: {'observation_period_num': 50, 'train_rates': 0.9497681908479632, 'learning_rate': 0.0003770360891393319, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9896321771801666}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:28:16,025][0m Trial 32 finished with value: 0.2329748570919037 and parameters: {'observation_period_num': 20, 'train_rates': 0.9840931983657056, 'learning_rate': 0.0004807978614023012, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9416577060076247}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:28:40,621][0m Trial 33 finished with value: 0.20539648830890656 and parameters: {'observation_period_num': 17, 'train_rates': 0.9888220949016915, 'learning_rate': 0.0006027242107463892, 'batch_size': 233, 'step_size': 14, 'gamma': 0.9643670704247228}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:29:20,333][0m Trial 34 finished with value: 0.9156687475259571 and parameters: {'observation_period_num': 53, 'train_rates': 0.6213642345671688, 'learning_rate': 0.00026033921840736896, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9635765532398403}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:30:44,812][0m Trial 35 finished with value: 0.5086145401000977 and parameters: {'observation_period_num': 82, 'train_rates': 0.9859375462657525, 'learning_rate': 0.0006205630008250924, 'batch_size': 230, 'step_size': 14, 'gamma': 0.9783003363190156}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:31:10,112][0m Trial 36 finished with value: 0.6209066475138945 and parameters: {'observation_period_num': 12, 'train_rates': 0.9307914036399647, 'learning_rate': 0.0009778036507080512, 'batch_size': 202, 'step_size': 13, 'gamma': 0.9572321371154496}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:31:48,761][0m Trial 37 finished with value: 0.9613597393035889 and parameters: {'observation_period_num': 36, 'train_rates': 0.9652716051206264, 'learning_rate': 5.798924984230934e-06, 'batch_size': 246, 'step_size': 14, 'gamma': 0.9273138355958479}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:33:02,681][0m Trial 38 finished with value: 0.5461031907313579 and parameters: {'observation_period_num': 42, 'train_rates': 0.9621160515985643, 'learning_rate': 0.0005935909939042193, 'batch_size': 70, 'step_size': 13, 'gamma': 0.9794555641420942}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:36:08,485][0m Trial 39 finished with value: 0.2729179859161377 and parameters: {'observation_period_num': 170, 'train_rates': 0.9296446201811818, 'learning_rate': 0.000185612848268999, 'batch_size': 229, 'step_size': 11, 'gamma': 0.9501970880738608}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:40:44,389][0m Trial 40 finished with value: 0.7568806648254395 and parameters: {'observation_period_num': 55, 'train_rates': 0.9899728288865455, 'learning_rate': 1.3563968381161935e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9648239694249982}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:41:07,921][0m Trial 41 finished with value: 0.28720909357070923 and parameters: {'observation_period_num': 17, 'train_rates': 0.9655537743178899, 'learning_rate': 0.00047842476882794854, 'batch_size': 242, 'step_size': 11, 'gamma': 0.9411008748482002}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:41:38,018][0m Trial 42 finished with value: 0.259547621011734 and parameters: {'observation_period_num': 25, 'train_rates': 0.9738611358579062, 'learning_rate': 0.0003102653003593221, 'batch_size': 209, 'step_size': 11, 'gamma': 0.9766425726759739}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:42:03,172][0m Trial 43 finished with value: 0.27252277731895447 and parameters: {'observation_period_num': 18, 'train_rates': 0.9895531469110372, 'learning_rate': 0.00047044078906503594, 'batch_size': 226, 'step_size': 13, 'gamma': 0.9541437341371475}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:42:47,827][0m Trial 44 finished with value: 0.2633917040559424 and parameters: {'observation_period_num': 45, 'train_rates': 0.9183032022187305, 'learning_rate': 0.000709918315052928, 'batch_size': 195, 'step_size': 9, 'gamma': 0.9337746308164977}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:43:24,483][0m Trial 45 finished with value: 0.297135591506958 and parameters: {'observation_period_num': 35, 'train_rates': 0.9459326475660328, 'learning_rate': 0.000518542464086256, 'batch_size': 242, 'step_size': 14, 'gamma': 0.9885650462575322}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:43:48,811][0m Trial 46 finished with value: 0.2894696295261383 and parameters: {'observation_period_num': 5, 'train_rates': 0.9756199541799359, 'learning_rate': 6.340115963506576e-05, 'batch_size': 222, 'step_size': 11, 'gamma': 0.9713940056449858}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:44:14,193][0m Trial 47 finished with value: 0.2779456430547674 and parameters: {'observation_period_num': 22, 'train_rates': 0.9034983712976637, 'learning_rate': 0.00025841096019404227, 'batch_size': 245, 'step_size': 9, 'gamma': 0.9586758230019318}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:48:36,581][0m Trial 48 finished with value: 0.3168913424015045 and parameters: {'observation_period_num': 221, 'train_rates': 0.9308405139521755, 'learning_rate': 0.00015757902790042286, 'batch_size': 212, 'step_size': 10, 'gamma': 0.9230880262493024}. Best is trial 26 with value: 0.17223714292049408.[0m
[32m[I 2025-02-08 06:50:37,755][0m Trial 49 finished with value: 0.5504377484321594 and parameters: {'observation_period_num': 114, 'train_rates': 0.9686187304397427, 'learning_rate': 0.0007172296628403397, 'batch_size': 198, 'step_size': 12, 'gamma': 0.9456668628561713}. Best is trial 26 with value: 0.17223714292049408.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-08 06:50:37,762][0m A new study created in memory with name: no-name-aaa980e8-ce68-4247-bf12-64ce830d1334[0m
[32m[I 2025-02-08 06:53:38,219][0m Trial 0 finished with value: 0.7725307941436768 and parameters: {'observation_period_num': 164, 'train_rates': 0.9437754914508706, 'learning_rate': 5.2675344296787945e-05, 'batch_size': 225, 'step_size': 2, 'gamma': 0.8970078160336054}. Best is trial 0 with value: 0.7725307941436768.[0m
[32m[I 2025-02-08 06:54:08,303][0m Trial 1 finished with value: 1.0610777364772501 and parameters: {'observation_period_num': 35, 'train_rates': 0.6475362584036273, 'learning_rate': 7.086120904366668e-05, 'batch_size': 223, 'step_size': 5, 'gamma': 0.7677536242779177}. Best is trial 0 with value: 0.7725307941436768.[0m
[32m[I 2025-02-08 06:57:24,056][0m Trial 2 finished with value: 2.1910671684964416 and parameters: {'observation_period_num': 198, 'train_rates': 0.724076736140547, 'learning_rate': 1.9551651266829947e-06, 'batch_size': 143, 'step_size': 3, 'gamma': 0.8260052467027599}. Best is trial 0 with value: 0.7725307941436768.[0m
[32m[I 2025-02-08 07:00:00,940][0m Trial 3 finished with value: 1.9713407846044007 and parameters: {'observation_period_num': 174, 'train_rates': 0.6426882957026707, 'learning_rate': 1.0120806221271575e-06, 'batch_size': 171, 'step_size': 7, 'gamma': 0.7615500829597458}. Best is trial 0 with value: 0.7725307941436768.[0m
[32m[I 2025-02-08 07:03:33,701][0m Trial 4 finished with value: 0.6304907114713784 and parameters: {'observation_period_num': 196, 'train_rates': 0.7841387072100331, 'learning_rate': 9.652270998793808e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8366738603327308}. Best is trial 4 with value: 0.6304907114713784.[0m
Early stopping at epoch 35
[32m[I 2025-02-08 07:03:45,337][0m Trial 5 finished with value: 2.148550037924013 and parameters: {'observation_period_num': 27, 'train_rates': 0.7759419017163672, 'learning_rate': 2.2836287716944572e-06, 'batch_size': 154, 'step_size': 1, 'gamma': 0.757379379622731}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:04:25,665][0m Trial 6 finished with value: 1.1950550079345703 and parameters: {'observation_period_num': 40, 'train_rates': 0.9337402602303477, 'learning_rate': 5.705363480153029e-06, 'batch_size': 242, 'step_size': 14, 'gamma': 0.7987464654156533}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:05:02,978][0m Trial 7 finished with value: 0.9912637276793005 and parameters: {'observation_period_num': 43, 'train_rates': 0.6922215273099398, 'learning_rate': 2.230789035477529e-05, 'batch_size': 174, 'step_size': 11, 'gamma': 0.7907871545148577}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:08:04,119][0m Trial 8 finished with value: 1.5591337718689495 and parameters: {'observation_period_num': 180, 'train_rates': 0.7620549522022602, 'learning_rate': 3.6093900022993292e-06, 'batch_size': 129, 'step_size': 9, 'gamma': 0.8310022248862919}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:09:37,768][0m Trial 9 finished with value: 0.9108323059145503 and parameters: {'observation_period_num': 45, 'train_rates': 0.793732731655233, 'learning_rate': 2.096194299149577e-05, 'batch_size': 46, 'step_size': 1, 'gamma': 0.9559361801910499}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:14:35,710][0m Trial 10 finished with value: 0.6478754634206946 and parameters: {'observation_period_num': 247, 'train_rates': 0.878439270613788, 'learning_rate': 0.0007323625220209223, 'batch_size': 69, 'step_size': 6, 'gamma': 0.891012238735995}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:19:22,812][0m Trial 11 finished with value: 0.8300880283446602 and parameters: {'observation_period_num': 234, 'train_rates': 0.8670990287849373, 'learning_rate': 0.00085512386104523, 'batch_size': 69, 'step_size': 6, 'gamma': 0.8950245955187778}. Best is trial 4 with value: 0.6304907114713784.[0m
[32m[I 2025-02-08 07:24:16,541][0m Trial 12 finished with value: 0.5548790749513878 and parameters: {'observation_period_num': 250, 'train_rates': 0.8630868121340509, 'learning_rate': 0.0006083066093023378, 'batch_size': 100, 'step_size': 9, 'gamma': 0.8620690487061169}. Best is trial 12 with value: 0.5548790749513878.[0m
[32m[I 2025-02-08 07:25:48,205][0m Trial 13 finished with value: 0.3456138770948808 and parameters: {'observation_period_num': 96, 'train_rates': 0.8509267904068772, 'learning_rate': 0.0002266561885952149, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8533116182532932}. Best is trial 13 with value: 0.3456138770948808.[0m
[32m[I 2025-02-08 07:27:24,621][0m Trial 14 finished with value: 0.35161456822330117 and parameters: {'observation_period_num': 97, 'train_rates': 0.8772449105914302, 'learning_rate': 0.00024373149848876731, 'batch_size': 102, 'step_size': 11, 'gamma': 0.9379833807349115}. Best is trial 13 with value: 0.3456138770948808.[0m
[32m[I 2025-02-08 07:29:57,809][0m Trial 15 finished with value: 0.5509614218909369 and parameters: {'observation_period_num': 94, 'train_rates': 0.845290591581747, 'learning_rate': 0.0002310491453126087, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9568015633825923}. Best is trial 13 with value: 0.3456138770948808.[0m
[32m[I 2025-02-08 07:31:57,780][0m Trial 16 finished with value: 0.201640784740448 and parameters: {'observation_period_num': 109, 'train_rates': 0.9897354805908076, 'learning_rate': 0.0002560597433527761, 'batch_size': 89, 'step_size': 15, 'gamma': 0.9310790775930988}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:34:09,971][0m Trial 17 finished with value: 0.32458893259366356 and parameters: {'observation_period_num': 120, 'train_rates': 0.9685492913350507, 'learning_rate': 0.00022566663684327106, 'batch_size': 76, 'step_size': 15, 'gamma': 0.9836415352243476}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:36:46,946][0m Trial 18 finished with value: 0.21844002604484558 and parameters: {'observation_period_num': 132, 'train_rates': 0.9860370724002121, 'learning_rate': 0.00014575401646818017, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9897485470226148}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:40:38,203][0m Trial 19 finished with value: 0.2096164503267833 and parameters: {'observation_period_num': 135, 'train_rates': 0.9827415775302207, 'learning_rate': 8.831795931865674e-06, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9256104870091356}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:44:12,287][0m Trial 20 finished with value: 0.3117143832720243 and parameters: {'observation_period_num': 69, 'train_rates': 0.918956919092587, 'learning_rate': 9.626418662136332e-06, 'batch_size': 22, 'step_size': 14, 'gamma': 0.9181059835010894}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:47:06,862][0m Trial 21 finished with value: 0.26917901635169983 and parameters: {'observation_period_num': 141, 'train_rates': 0.9868598984546935, 'learning_rate': 1.3175290525723191e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9887676939628856}. Best is trial 16 with value: 0.201640784740448.[0m
[32m[I 2025-02-08 07:49:51,277][0m Trial 22 finished with value: 0.15500348806381226 and parameters: {'observation_period_num': 134, 'train_rates': 0.9878391651928513, 'learning_rate': 0.00013051584474052695, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9244233365364108}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 07:54:36,719][0m Trial 23 finished with value: 0.33317966063817345 and parameters: {'observation_period_num': 115, 'train_rates': 0.9157880587086423, 'learning_rate': 4.286286062821107e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9209507401516531}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 07:57:43,910][0m Trial 24 finished with value: 0.20791328605264425 and parameters: {'observation_period_num': 153, 'train_rates': 0.9544590747000858, 'learning_rate': 0.0001080781716446032, 'batch_size': 44, 'step_size': 15, 'gamma': 0.9240522383167218}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:00:48,415][0m Trial 25 finished with value: 0.35806383290430055 and parameters: {'observation_period_num': 153, 'train_rates': 0.9511134818864873, 'learning_rate': 0.0004079802644944759, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9491265866612737}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:02:05,264][0m Trial 26 finished with value: 0.25835173169887343 and parameters: {'observation_period_num': 68, 'train_rates': 0.9108058198309288, 'learning_rate': 0.00012085454678050311, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8838037940750176}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:03:50,840][0m Trial 27 finished with value: 0.7189250007243165 and parameters: {'observation_period_num': 74, 'train_rates': 0.8191756870831635, 'learning_rate': 0.00044868284004955373, 'batch_size': 42, 'step_size': 12, 'gamma': 0.9107300326332718}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:06:48,903][0m Trial 28 finished with value: 0.239204959012568 and parameters: {'observation_period_num': 152, 'train_rates': 0.9544087367360616, 'learning_rate': 9.20560546111252e-05, 'batch_size': 85, 'step_size': 15, 'gamma': 0.9713699351471895}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:07:45,259][0m Trial 29 finished with value: 0.29090084932571236 and parameters: {'observation_period_num': 11, 'train_rates': 0.9417252182009116, 'learning_rate': 4.934338345900783e-05, 'batch_size': 88, 'step_size': 12, 'gamma': 0.8784790288225051}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:09:50,572][0m Trial 30 finished with value: 0.2894443142414093 and parameters: {'observation_period_num': 113, 'train_rates': 0.9120712331382204, 'learning_rate': 3.260907145883749e-05, 'batch_size': 60, 'step_size': 14, 'gamma': 0.9336680116659157}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:12:54,368][0m Trial 31 finished with value: 0.16807438004761935 and parameters: {'observation_period_num': 137, 'train_rates': 0.9718486570339944, 'learning_rate': 0.00013737856060572066, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9029044952597447}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:16:20,578][0m Trial 32 finished with value: 0.22418906471946023 and parameters: {'observation_period_num': 162, 'train_rates': 0.9606701864983237, 'learning_rate': 0.0001557329924251269, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9067136397413587}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:20:29,276][0m Trial 33 finished with value: 0.24963299159345956 and parameters: {'observation_period_num': 194, 'train_rates': 0.9686665658534347, 'learning_rate': 7.360930946030529e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.906049086515472}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:23:24,637][0m Trial 34 finished with value: 0.3888168602001549 and parameters: {'observation_period_num': 147, 'train_rates': 0.8923812522482062, 'learning_rate': 0.0003745293070750036, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9457274466248942}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:26:42,708][0m Trial 35 finished with value: 0.3295776944428204 and parameters: {'observation_period_num': 172, 'train_rates': 0.9328229221497208, 'learning_rate': 6.97930369146034e-05, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8718193060521201}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:28:09,231][0m Trial 36 finished with value: 0.9797007809056034 and parameters: {'observation_period_num': 108, 'train_rates': 0.6071975058969873, 'learning_rate': 0.0001525683665213925, 'batch_size': 189, 'step_size': 11, 'gamma': 0.9347961003770532}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:32:25,341][0m Trial 37 finished with value: 0.19456177949905396 and parameters: {'observation_period_num': 208, 'train_rates': 0.9869598840932973, 'learning_rate': 0.0003470941814736085, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9688027299420057}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:36:57,467][0m Trial 38 finished with value: 0.19201405346393585 and parameters: {'observation_period_num': 217, 'train_rates': 0.9859399177854321, 'learning_rate': 0.00037453233462794104, 'batch_size': 139, 'step_size': 13, 'gamma': 0.9723501371584317}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:40:37,648][0m Trial 39 finished with value: 1.1393082460674193 and parameters: {'observation_period_num': 216, 'train_rates': 0.7120033636373362, 'learning_rate': 0.0005054471895011526, 'batch_size': 140, 'step_size': 8, 'gamma': 0.9725296050926832}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:45:05,053][0m Trial 40 finished with value: 0.25630663633346557 and parameters: {'observation_period_num': 222, 'train_rates': 0.9341503002690466, 'learning_rate': 0.00032363036181020375, 'batch_size': 166, 'step_size': 10, 'gamma': 0.9675911277133378}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:49:27,366][0m Trial 41 finished with value: 0.8295480608940125 and parameters: {'observation_period_num': 214, 'train_rates': 0.9730590153606083, 'learning_rate': 0.000993964027051426, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9617737265815963}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:53:08,195][0m Trial 42 finished with value: 0.18647728860378265 and parameters: {'observation_period_num': 185, 'train_rates': 0.9895543709328166, 'learning_rate': 0.0002932243624990505, 'batch_size': 119, 'step_size': 13, 'gamma': 0.97655678241256}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 08:56:50,815][0m Trial 43 finished with value: 0.25461873412132263 and parameters: {'observation_period_num': 191, 'train_rates': 0.9651968911133171, 'learning_rate': 0.0003094282691483891, 'batch_size': 114, 'step_size': 12, 'gamma': 0.9773556649525593}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:00:29,604][0m Trial 44 finished with value: 0.19557930529117584 and parameters: {'observation_period_num': 182, 'train_rates': 0.9899922350584016, 'learning_rate': 0.00019226581552382493, 'batch_size': 186, 'step_size': 13, 'gamma': 0.9526368764686168}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:04:27,230][0m Trial 45 finished with value: 0.4956076250609938 and parameters: {'observation_period_num': 205, 'train_rates': 0.8996088408294572, 'learning_rate': 0.0006445155401478299, 'batch_size': 130, 'step_size': 10, 'gamma': 0.963018863914701}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:09:01,644][0m Trial 46 finished with value: 1.534865158467205 and parameters: {'observation_period_num': 225, 'train_rates': 0.9406313687867618, 'learning_rate': 1.0112519424518511e-06, 'batch_size': 118, 'step_size': 13, 'gamma': 0.9439233063520935}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:14:09,601][0m Trial 47 finished with value: 0.2494678646326065 and parameters: {'observation_period_num': 240, 'train_rates': 0.972874846273677, 'learning_rate': 0.0005218047250593393, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9760310477303028}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:17:38,036][0m Trial 48 finished with value: 0.7437802672386169 and parameters: {'observation_period_num': 203, 'train_rates': 0.763845484054871, 'learning_rate': 7.50664182416987e-05, 'batch_size': 208, 'step_size': 11, 'gamma': 0.8415761820560559}. Best is trial 22 with value: 0.15500348806381226.[0m
[32m[I 2025-02-08 09:20:45,233][0m Trial 49 finished with value: 0.2321520037419423 and parameters: {'observation_period_num': 164, 'train_rates': 0.9245081187895267, 'learning_rate': 0.00017890471940498794, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8134339499344037}. Best is trial 22 with value: 0.15500348806381226.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-08 09:20:45,240][0m A new study created in memory with name: no-name-2e72a6ce-a026-489f-99fe-f72e8e6406c7[0m
Early stopping at epoch 91
[32m[I 2025-02-08 09:23:11,630][0m Trial 0 finished with value: 0.8177828034748037 and parameters: {'observation_period_num': 149, 'train_rates': 0.8993286724164475, 'learning_rate': 3.489515155364075e-05, 'batch_size': 134, 'step_size': 2, 'gamma': 0.7697039216692815}. Best is trial 0 with value: 0.8177828034748037.[0m
[32m[I 2025-02-08 09:26:04,081][0m Trial 1 finished with value: 0.2042473405599594 and parameters: {'observation_period_num': 146, 'train_rates': 0.9745483318805235, 'learning_rate': 0.00037251476528212235, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9744810626504684}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:29:06,305][0m Trial 2 finished with value: 1.0063813924789429 and parameters: {'observation_period_num': 162, 'train_rates': 0.9715887924097106, 'learning_rate': 5.260295343125629e-06, 'batch_size': 246, 'step_size': 8, 'gamma': 0.959169081610439}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:29:31,610][0m Trial 3 finished with value: 0.9169616488620853 and parameters: {'observation_period_num': 5, 'train_rates': 0.7401049460331156, 'learning_rate': 2.494679701823442e-05, 'batch_size': 175, 'step_size': 4, 'gamma': 0.9467734644916366}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:30:41,429][0m Trial 4 finished with value: 1.9205638726069698 and parameters: {'observation_period_num': 43, 'train_rates': 0.7457592510833418, 'learning_rate': 1.1529013419625874e-06, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8598232107401311}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:32:21,597][0m Trial 5 finished with value: 0.24739889800548553 and parameters: {'observation_period_num': 95, 'train_rates': 0.972624125434802, 'learning_rate': 0.0003249579203186766, 'batch_size': 124, 'step_size': 5, 'gamma': 0.8327092396343682}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:32:48,216][0m Trial 6 finished with value: 0.2334240936072527 and parameters: {'observation_period_num': 15, 'train_rates': 0.9231702859414557, 'learning_rate': 0.0002188148909254042, 'batch_size': 194, 'step_size': 10, 'gamma': 0.9003089454650268}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:35:11,456][0m Trial 7 finished with value: 2.3790703742734847 and parameters: {'observation_period_num': 81, 'train_rates': 0.7089050201855767, 'learning_rate': 0.000976810121278235, 'batch_size': 27, 'step_size': 6, 'gamma': 0.7788194400999529}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:35:58,419][0m Trial 8 finished with value: 1.2360520021624217 and parameters: {'observation_period_num': 46, 'train_rates': 0.6989064974081216, 'learning_rate': 0.000728077089863757, 'batch_size': 91, 'step_size': 2, 'gamma': 0.8248063634576174}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:40:33,679][0m Trial 9 finished with value: 0.3631589710712433 and parameters: {'observation_period_num': 218, 'train_rates': 0.9618961489134371, 'learning_rate': 0.0003693001938410019, 'batch_size': 155, 'step_size': 4, 'gamma': 0.8146156695111072}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:45:02,973][0m Trial 10 finished with value: 0.4479625446921481 and parameters: {'observation_period_num': 236, 'train_rates': 0.8353992663446984, 'learning_rate': 7.562395667023029e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.9869383414112342}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:48:29,787][0m Trial 11 finished with value: 0.3551514006816269 and parameters: {'observation_period_num': 188, 'train_rates': 0.8714534384248559, 'learning_rate': 0.00012913570182613839, 'batch_size': 207, 'step_size': 12, 'gamma': 0.9043236526280523}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:49:56,175][0m Trial 12 finished with value: 1.032736312039723 and parameters: {'observation_period_num': 110, 'train_rates': 0.6271560842734398, 'learning_rate': 0.00018876494204324155, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9059294793362654}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:52:19,142][0m Trial 13 finished with value: 0.3874645531177521 and parameters: {'observation_period_num': 133, 'train_rates': 0.9193220341612031, 'learning_rate': 6.482340258087552e-05, 'batch_size': 250, 'step_size': 10, 'gamma': 0.9088657162669815}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:55:25,778][0m Trial 14 finished with value: 0.6485520208108753 and parameters: {'observation_period_num': 178, 'train_rates': 0.8297244635815078, 'learning_rate': 2.016603942176933e-05, 'batch_size': 108, 'step_size': 15, 'gamma': 0.9415984542352583}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:56:33,012][0m Trial 15 finished with value: 0.24960506225333495 and parameters: {'observation_period_num': 66, 'train_rates': 0.9178090330959864, 'learning_rate': 0.0003781269018612854, 'batch_size': 167, 'step_size': 9, 'gamma': 0.9882032738315422}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:56:59,577][0m Trial 16 finished with value: 1.0049021244049072 and parameters: {'observation_period_num': 18, 'train_rates': 0.9897809774917586, 'learning_rate': 8.53105307684641e-06, 'batch_size': 213, 'step_size': 6, 'gamma': 0.8752685675434022}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 09:59:01,077][0m Trial 17 finished with value: 0.3636383668181315 and parameters: {'observation_period_num': 115, 'train_rates': 0.8588022659057276, 'learning_rate': 0.00013604840626954797, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9276018528142241}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:02:39,487][0m Trial 18 finished with value: 0.4738434779856886 and parameters: {'observation_period_num': 207, 'train_rates': 0.7963627066905, 'learning_rate': 0.0005415108435774864, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8783660916529661}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:05:03,242][0m Trial 19 finished with value: 0.3047377844834657 and parameters: {'observation_period_num': 135, 'train_rates': 0.9230609156525694, 'learning_rate': 6.812729925510199e-05, 'batch_size': 183, 'step_size': 10, 'gamma': 0.9683594835662628}. Best is trial 1 with value: 0.2042473405599594.[0m
Early stopping at epoch 78
[32m[I 2025-02-08 10:05:49,736][0m Trial 20 finished with value: 0.8118256330490112 and parameters: {'observation_period_num': 61, 'train_rates': 0.9407857425729111, 'learning_rate': 0.00020576935978328526, 'batch_size': 224, 'step_size': 1, 'gamma': 0.8525525714027004}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:07:24,789][0m Trial 21 finished with value: 0.2165193110704422 and parameters: {'observation_period_num': 89, 'train_rates': 0.9830837548309926, 'learning_rate': 0.00030972490679205013, 'batch_size': 123, 'step_size': 5, 'gamma': 0.8270103085714285}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:08:07,449][0m Trial 22 finished with value: 0.3188722041414304 and parameters: {'observation_period_num': 31, 'train_rates': 0.8952123121841247, 'learning_rate': 0.00032614737147265043, 'batch_size': 116, 'step_size': 4, 'gamma': 0.806030892624442}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:09:44,769][0m Trial 23 finished with value: 0.5962875377048146 and parameters: {'observation_period_num': 89, 'train_rates': 0.9578690706005415, 'learning_rate': 0.000986322256834012, 'batch_size': 93, 'step_size': 7, 'gamma': 0.7866723758891978}. Best is trial 1 with value: 0.2042473405599594.[0m
[32m[I 2025-02-08 10:12:52,239][0m Trial 24 finished with value: 0.1859656423330307 and parameters: {'observation_period_num': 163, 'train_rates': 0.9874310069766136, 'learning_rate': 0.00020938713513469332, 'batch_size': 142, 'step_size': 9, 'gamma': 0.7538709856824515}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:15:59,700][0m Trial 25 finished with value: 0.3481084406375885 and parameters: {'observation_period_num': 164, 'train_rates': 0.9851112519020921, 'learning_rate': 0.00011097952739832576, 'batch_size': 143, 'step_size': 6, 'gamma': 0.7528188554383688}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:18:13,378][0m Trial 26 finished with value: 0.4115937511632397 and parameters: {'observation_period_num': 116, 'train_rates': 0.944909131053956, 'learning_rate': 0.0005385191166037759, 'batch_size': 66, 'step_size': 3, 'gamma': 0.8424776390796651}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:20:48,962][0m Trial 27 finished with value: 1.147178335457432 and parameters: {'observation_period_num': 183, 'train_rates': 0.6113959996283242, 'learning_rate': 4.4316584106652295e-05, 'batch_size': 110, 'step_size': 7, 'gamma': 0.8007411056057012}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:23:23,842][0m Trial 28 finished with value: 0.42179921308121127 and parameters: {'observation_period_num': 148, 'train_rates': 0.8629169849580126, 'learning_rate': 0.00023216804926827955, 'batch_size': 130, 'step_size': 5, 'gamma': 0.794762528225093}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:26:06,543][0m Trial 29 finished with value: 0.8990613520145416 and parameters: {'observation_period_num': 153, 'train_rates': 0.8874737931923916, 'learning_rate': 1.3321850376903056e-05, 'batch_size': 158, 'step_size': 9, 'gamma': 0.7635145337926804}. Best is trial 24 with value: 0.1859656423330307.[0m
Early stopping at epoch 79
[32m[I 2025-02-08 10:29:23,008][0m Trial 30 finished with value: 1.9166016578674316 and parameters: {'observation_period_num': 200, 'train_rates': 0.9897281248693586, 'learning_rate': 2.534264411653662e-06, 'batch_size': 133, 'step_size': 2, 'gamma': 0.7708325454386266}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:31:55,857][0m Trial 31 finished with value: 0.23538371920585632 and parameters: {'observation_period_num': 141, 'train_rates': 0.9441255981188195, 'learning_rate': 0.00020526011858840225, 'batch_size': 188, 'step_size': 11, 'gamma': 0.8831215820016887}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:35:01,105][0m Trial 32 finished with value: 0.2363123893737793 and parameters: {'observation_period_num': 170, 'train_rates': 0.9226084443060169, 'learning_rate': 0.0005358896538867273, 'batch_size': 231, 'step_size': 9, 'gamma': 0.9224548500229092}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:36:59,584][0m Trial 33 finished with value: 0.3398913337425752 and parameters: {'observation_period_num': 102, 'train_rates': 0.961676101518744, 'learning_rate': 3.668567251201711e-05, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8935302261460196}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:38:14,494][0m Trial 34 finished with value: 0.2734156491729499 and parameters: {'observation_period_num': 67, 'train_rates': 0.9388781919555933, 'learning_rate': 0.0001237266781424028, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9658916308968674}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:38:47,745][0m Trial 35 finished with value: 0.2173003852367401 and parameters: {'observation_period_num': 10, 'train_rates': 0.9690721558209103, 'learning_rate': 0.0002950250769140063, 'batch_size': 162, 'step_size': 8, 'gamma': 0.8555569209969335}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:40:53,446][0m Trial 36 finished with value: 0.2977602779865265 and parameters: {'observation_period_num': 121, 'train_rates': 0.9669486255456468, 'learning_rate': 0.0004264505841058908, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8605119030829388}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:43:41,636][0m Trial 37 finished with value: 0.3445586880114565 and parameters: {'observation_period_num': 157, 'train_rates': 0.8932775064659378, 'learning_rate': 0.0006931588979131357, 'batch_size': 122, 'step_size': 8, 'gamma': 0.8138930115594825}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:44:27,919][0m Trial 38 finished with value: 0.273078978061676 and parameters: {'observation_period_num': 41, 'train_rates': 0.9757222681922113, 'learning_rate': 9.426130230259852e-05, 'batch_size': 142, 'step_size': 7, 'gamma': 0.842153655575313}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:46:15,181][0m Trial 39 finished with value: 0.6778372276315516 and parameters: {'observation_period_num': 81, 'train_rates': 0.7699062328970978, 'learning_rate': 0.0002759074111192916, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8271155814313134}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:46:50,094][0m Trial 40 finished with value: 0.35190221667289734 and parameters: {'observation_period_num': 8, 'train_rates': 0.9547390031982174, 'learning_rate': 0.0007480035551647779, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8664388526184593}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:47:20,699][0m Trial 41 finished with value: 0.24587799608707428 and parameters: {'observation_period_num': 23, 'train_rates': 0.9686380191042842, 'learning_rate': 0.00017166615614625004, 'batch_size': 182, 'step_size': 8, 'gamma': 0.8868165467123096}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:48:11,656][0m Trial 42 finished with value: 0.2354657136476957 and parameters: {'observation_period_num': 50, 'train_rates': 0.9107233174578214, 'learning_rate': 0.00027269496980860845, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9485307514776762}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:48:49,794][0m Trial 43 finished with value: 0.21033776276989988 and parameters: {'observation_period_num': 6, 'train_rates': 0.9294532878403908, 'learning_rate': 0.0004128466022024338, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8475589028065867}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:49:31,354][0m Trial 44 finished with value: 0.2494916901473076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9371416028586544, 'learning_rate': 0.00044805983743419024, 'batch_size': 123, 'step_size': 9, 'gamma': 0.8476680991999482}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:50:24,186][0m Trial 45 finished with value: 0.2494318187236786 and parameters: {'observation_period_num': 33, 'train_rates': 0.9734117873020481, 'learning_rate': 0.0001614318493746625, 'batch_size': 99, 'step_size': 6, 'gamma': 0.826796796338102}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:55:08,981][0m Trial 46 finished with value: 0.22588345408439636 and parameters: {'observation_period_num': 230, 'train_rates': 0.9529000665372724, 'learning_rate': 0.0003074700632963878, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8354329590926195}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 10:59:15,613][0m Trial 47 finished with value: 0.34428438544273376 and parameters: {'observation_period_num': 196, 'train_rates': 0.989632468611794, 'learning_rate': 5.3781660269400256e-05, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8157102767151021}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 11:01:53,525][0m Trial 48 finished with value: 1.1648737264328732 and parameters: {'observation_period_num': 170, 'train_rates': 0.6904713316785671, 'learning_rate': 0.0007080003591707263, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8538952114229921}. Best is trial 24 with value: 0.1859656423330307.[0m
[32m[I 2025-02-08 11:02:44,376][0m Trial 49 finished with value: 0.5189087266431135 and parameters: {'observation_period_num': 54, 'train_rates': 0.8242437472377936, 'learning_rate': 8.249791261800017e-05, 'batch_size': 149, 'step_size': 6, 'gamma': 0.8643857956894103}. Best is trial 24 with value: 0.1859656423330307.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-08 11:02:44,384][0m A new study created in memory with name: no-name-e1138c9f-27ff-48c6-9233-df7852a28936[0m
[32m[I 2025-02-08 11:06:41,558][0m Trial 0 finished with value: 1.065881171468961 and parameters: {'observation_period_num': 216, 'train_rates': 0.8063324433349076, 'learning_rate': 6.022628731504422e-06, 'batch_size': 178, 'step_size': 6, 'gamma': 0.9766434611319788}. Best is trial 0 with value: 1.065881171468961.[0m
[32m[I 2025-02-08 11:08:47,372][0m Trial 1 finished with value: 0.2991640441345446 and parameters: {'observation_period_num': 127, 'train_rates': 0.9186518191864284, 'learning_rate': 0.0002547535659533029, 'batch_size': 203, 'step_size': 14, 'gamma': 0.8929669984782143}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:09:49,799][0m Trial 2 finished with value: 1.214365713306934 and parameters: {'observation_period_num': 75, 'train_rates': 0.6999488745405016, 'learning_rate': 8.708282013136332e-06, 'batch_size': 254, 'step_size': 13, 'gamma': 0.9603582863142733}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:12:33,350][0m Trial 3 finished with value: 0.487935739823363 and parameters: {'observation_period_num': 144, 'train_rates': 0.8904688484227239, 'learning_rate': 0.0005836529413926339, 'batch_size': 65, 'step_size': 3, 'gamma': 0.9287077391676817}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:13:15,773][0m Trial 4 finished with value: 1.7697321956750016 and parameters: {'observation_period_num': 52, 'train_rates': 0.6133931487189013, 'learning_rate': 4.282843346897356e-06, 'batch_size': 112, 'step_size': 4, 'gamma': 0.862326289001162}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:14:49,023][0m Trial 5 finished with value: 0.41793957270827947 and parameters: {'observation_period_num': 23, 'train_rates': 0.9133206367680456, 'learning_rate': 0.0004410162963248428, 'batch_size': 52, 'step_size': 11, 'gamma': 0.803599935836433}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:17:27,495][0m Trial 6 finished with value: 1.385866072319007 and parameters: {'observation_period_num': 165, 'train_rates': 0.7702292026509001, 'learning_rate': 3.9737941513824e-05, 'batch_size': 200, 'step_size': 2, 'gamma': 0.7923394218658224}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:19:38,955][0m Trial 7 finished with value: 0.9475633695677842 and parameters: {'observation_period_num': 153, 'train_rates': 0.6358819567841548, 'learning_rate': 0.00010152310307537714, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9708574963229898}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:23:34,039][0m Trial 8 finished with value: 1.0332465969642128 and parameters: {'observation_period_num': 219, 'train_rates': 0.7659646417957653, 'learning_rate': 9.447869209230245e-06, 'batch_size': 62, 'step_size': 15, 'gamma': 0.7894117845902384}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:25:12,979][0m Trial 9 finished with value: 1.3177773043260737 and parameters: {'observation_period_num': 95, 'train_rates': 0.855748239882463, 'learning_rate': 1.5469442850336427e-06, 'batch_size': 74, 'step_size': 6, 'gamma': 0.9592900043275095}. Best is trial 1 with value: 0.2991640441345446.[0m
[32m[I 2025-02-08 11:27:05,060][0m Trial 10 finished with value: 0.19182232022285461 and parameters: {'observation_period_num': 109, 'train_rates': 0.9751642602349396, 'learning_rate': 0.00019403379320575274, 'batch_size': 244, 'step_size': 9, 'gamma': 0.8805657127201864}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:28:47,048][0m Trial 11 finished with value: 0.22038599848747253 and parameters: {'observation_period_num': 99, 'train_rates': 0.9860740337175056, 'learning_rate': 0.00016404607809472423, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8787347611611436}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:30:42,448][0m Trial 12 finished with value: 0.3307582139968872 and parameters: {'observation_period_num': 108, 'train_rates': 0.9894912993211322, 'learning_rate': 0.00010141487141833219, 'batch_size': 249, 'step_size': 9, 'gamma': 0.857810280547178}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:31:40,879][0m Trial 13 finished with value: 0.2168169468641281 and parameters: {'observation_period_num': 59, 'train_rates': 0.975018215463735, 'learning_rate': 0.00012063552130824891, 'batch_size': 229, 'step_size': 9, 'gamma': 0.8992004362361361}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:32:13,736][0m Trial 14 finished with value: 0.6604809165000916 and parameters: {'observation_period_num': 6, 'train_rates': 0.9558277012202202, 'learning_rate': 3.8787700406195234e-05, 'batch_size': 163, 'step_size': 7, 'gamma': 0.832530793319184}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:33:03,429][0m Trial 15 finished with value: 0.426370319392946 and parameters: {'observation_period_num': 55, 'train_rates': 0.8449994228352538, 'learning_rate': 0.000907516418799208, 'batch_size': 217, 'step_size': 8, 'gamma': 0.9028788684504495}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:36:28,877][0m Trial 16 finished with value: 0.6594083905220032 and parameters: {'observation_period_num': 183, 'train_rates': 0.9407705627699424, 'learning_rate': 6.0533991494844504e-05, 'batch_size': 221, 'step_size': 12, 'gamma': 0.7518781758831058}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:37:12,092][0m Trial 17 finished with value: 0.7274017035961151 and parameters: {'observation_period_num': 43, 'train_rates': 0.8727587947178568, 'learning_rate': 1.754381077591133e-05, 'batch_size': 159, 'step_size': 5, 'gamma': 0.9202184896832295}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:38:16,299][0m Trial 18 finished with value: 0.8147869774664955 and parameters: {'observation_period_num': 77, 'train_rates': 0.6980029336724143, 'learning_rate': 0.00026183709297566786, 'batch_size': 230, 'step_size': 9, 'gamma': 0.8277674333926798}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:40:18,701][0m Trial 19 finished with value: 0.8685079216957092 and parameters: {'observation_period_num': 119, 'train_rates': 0.9537467246705892, 'learning_rate': 9.408733903312316e-05, 'batch_size': 180, 'step_size': 1, 'gamma': 0.9260025738136958}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:44:59,633][0m Trial 20 finished with value: 0.8176735028530817 and parameters: {'observation_period_num': 245, 'train_rates': 0.8241649354909002, 'learning_rate': 1.9973810545220545e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8357947918111619}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:46:27,491][0m Trial 21 finished with value: 0.21604982018470764 and parameters: {'observation_period_num': 87, 'train_rates': 0.976945522874992, 'learning_rate': 0.00020556960880893828, 'batch_size': 240, 'step_size': 10, 'gamma': 0.883269309282366}. Best is trial 10 with value: 0.19182232022285461.[0m
[32m[I 2025-02-08 11:47:51,239][0m Trial 22 finished with value: 0.17597709596157074 and parameters: {'observation_period_num': 78, 'train_rates': 0.9845217102426269, 'learning_rate': 0.00021346384760085146, 'batch_size': 234, 'step_size': 10, 'gamma': 0.8942020870492958}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 11:51:35,374][0m Trial 23 finished with value: 0.44868859371473624 and parameters: {'observation_period_num': 86, 'train_rates': 0.9253870395413785, 'learning_rate': 0.00026231211195408045, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8738981614426659}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 11:53:54,961][0m Trial 24 finished with value: 0.366609342546271 and parameters: {'observation_period_num': 133, 'train_rates': 0.8946130875026346, 'learning_rate': 0.0005827063518736059, 'batch_size': 199, 'step_size': 13, 'gamma': 0.940452777136917}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 11:54:30,185][0m Trial 25 finished with value: 0.31037458777427673 and parameters: {'observation_period_num': 32, 'train_rates': 0.9571788170351017, 'learning_rate': 0.0001877875973325136, 'batch_size': 232, 'step_size': 10, 'gamma': 0.8491934901494042}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 11:56:23,204][0m Trial 26 finished with value: 0.323344990159526 and parameters: {'observation_period_num': 111, 'train_rates': 0.9304921104894107, 'learning_rate': 0.000381518229232078, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8892999818315285}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 11:57:40,855][0m Trial 27 finished with value: 0.2828388214111328 and parameters: {'observation_period_num': 74, 'train_rates': 0.9885361552176677, 'learning_rate': 5.869704386986889e-05, 'batch_size': 185, 'step_size': 12, 'gamma': 0.9135878260671226}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:00:36,317][0m Trial 28 finished with value: 0.458410203140953 and parameters: {'observation_period_num': 165, 'train_rates': 0.8987541261015634, 'learning_rate': 5.808930092763443e-05, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8752666422949333}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:04:06,722][0m Trial 29 finished with value: 0.3066685199737549 and parameters: {'observation_period_num': 185, 'train_rates': 0.9606126378056373, 'learning_rate': 0.00071179701042796, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9088312501078568}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:05:07,488][0m Trial 30 finished with value: 0.5988465477494994 and parameters: {'observation_period_num': 66, 'train_rates': 0.7850806081385981, 'learning_rate': 0.00038804235690116163, 'batch_size': 238, 'step_size': 8, 'gamma': 0.940027174196849}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:06:44,496][0m Trial 31 finished with value: 0.307513952255249 and parameters: {'observation_period_num': 95, 'train_rates': 0.9670402143906215, 'learning_rate': 0.0001312084171839487, 'batch_size': 226, 'step_size': 9, 'gamma': 0.8916868167544442}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:07:41,137][0m Trial 32 finished with value: 0.23990678787231445 and parameters: {'observation_period_num': 57, 'train_rates': 0.9714697026212248, 'learning_rate': 0.00018637473538455374, 'batch_size': 209, 'step_size': 9, 'gamma': 0.9011813960335153}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:10:06,850][0m Trial 33 finished with value: 0.3407284617424011 and parameters: {'observation_period_num': 130, 'train_rates': 0.9327780304246273, 'learning_rate': 0.00026314089392250924, 'batch_size': 190, 'step_size': 7, 'gamma': 0.8819241315727946}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:11:22,584][0m Trial 34 finished with value: 0.8881997421248569 and parameters: {'observation_period_num': 86, 'train_rates': 0.731590664283355, 'learning_rate': 7.626709224861246e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.8434213673222434}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:12:00,544][0m Trial 35 finished with value: 0.313927816308063 and parameters: {'observation_period_num': 37, 'train_rates': 0.913382151869999, 'learning_rate': 0.0001628369269883887, 'batch_size': 231, 'step_size': 10, 'gamma': 0.863897714947548}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:13:05,241][0m Trial 36 finished with value: 0.49529034352268686 and parameters: {'observation_period_num': 65, 'train_rates': 0.8784145636334609, 'learning_rate': 0.0004176117446143851, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9896999875907815}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:15:04,138][0m Trial 37 finished with value: 0.7388894557952881 and parameters: {'observation_period_num': 114, 'train_rates': 0.9455224989666434, 'learning_rate': 2.8329915617770065e-05, 'batch_size': 242, 'step_size': 11, 'gamma': 0.9409119268565028}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:15:30,447][0m Trial 38 finished with value: 0.34897914655910905 and parameters: {'observation_period_num': 6, 'train_rates': 0.9125732080858128, 'learning_rate': 0.00013249493363553277, 'batch_size': 199, 'step_size': 5, 'gamma': 0.892630754368181}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:15:58,058][0m Trial 39 finished with value: 0.18071173131465912 and parameters: {'observation_period_num': 20, 'train_rates': 0.977559132665295, 'learning_rate': 0.0003314605198438321, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8573159807162902}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:16:28,924][0m Trial 40 finished with value: 0.998863435551232 and parameters: {'observation_period_num': 25, 'train_rates': 0.6694948868203494, 'learning_rate': 0.0005708958641802643, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8162702326400529}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:17:19,257][0m Trial 41 finished with value: 0.1862623691558838 and parameters: {'observation_period_num': 48, 'train_rates': 0.9749874526024184, 'learning_rate': 0.00032706970083806053, 'batch_size': 211, 'step_size': 9, 'gamma': 0.8549997214889886}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:18:06,091][0m Trial 42 finished with value: 0.2596491575241089 and parameters: {'observation_period_num': 46, 'train_rates': 0.9711594952541618, 'learning_rate': 0.0003498722265559425, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8538042921946716}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:18:30,185][0m Trial 43 finished with value: 0.3337284326553345 and parameters: {'observation_period_num': 18, 'train_rates': 0.9412400357467349, 'learning_rate': 0.0002663264770450093, 'batch_size': 245, 'step_size': 10, 'gamma': 0.8666944103783053}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:19:54,431][0m Trial 44 finished with value: 2.273610830307007 and parameters: {'observation_period_num': 82, 'train_rates': 0.9821337568210661, 'learning_rate': 1.1286597639625487e-06, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8128832047800559}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:21:39,211][0m Trial 45 finished with value: 0.44060516357421875 and parameters: {'observation_period_num': 101, 'train_rates': 0.9887738383463547, 'learning_rate': 0.0008985759330513527, 'batch_size': 219, 'step_size': 8, 'gamma': 0.8818213130348184}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:22:06,036][0m Trial 46 finished with value: 0.2954830550805642 and parameters: {'observation_period_num': 19, 'train_rates': 0.905211230875852, 'learning_rate': 0.0002142961142568536, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8588167560915728}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:23:19,082][0m Trial 47 finished with value: 1.4976038932800293 and parameters: {'observation_period_num': 71, 'train_rates': 0.9504166980774466, 'learning_rate': 3.763673079717876e-06, 'batch_size': 207, 'step_size': 14, 'gamma': 0.8421508903526381}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:26:02,284][0m Trial 48 finished with value: 0.3055965923081895 and parameters: {'observation_period_num': 144, 'train_rates': 0.9231384333769637, 'learning_rate': 0.0003141735709090145, 'batch_size': 111, 'step_size': 10, 'gamma': 0.8677417539657198}. Best is trial 22 with value: 0.17597709596157074.[0m
[32m[I 2025-02-08 12:26:47,974][0m Trial 49 finished with value: 0.352918423733725 and parameters: {'observation_period_num': 49, 'train_rates': 0.8806557332856887, 'learning_rate': 0.0006212411308116883, 'batch_size': 238, 'step_size': 6, 'gamma': 0.8244218090373674}. Best is trial 22 with value: 0.17597709596157074.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 85, 'train_rates': 0.9884395873706885, 'learning_rate': 4.66069714817907e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9401246206272881}
Epoch 1/300, trend Loss: 1.0986 | 1.7346
Epoch 2/300, trend Loss: 0.6755 | 1.1470
Epoch 3/300, trend Loss: 0.5310 | 1.0247
Epoch 4/300, trend Loss: 0.4513 | 0.9259
Epoch 5/300, trend Loss: 0.4020 | 0.8330
Epoch 6/300, trend Loss: 0.3732 | 0.7908
Epoch 7/300, trend Loss: 0.3534 | 0.7335
Epoch 8/300, trend Loss: 0.3345 | 0.7308
Epoch 9/300, trend Loss: 0.3087 | 0.6425
Epoch 10/300, trend Loss: 0.2990 | 0.6101
Epoch 11/300, trend Loss: 0.2850 | 0.5853
Epoch 12/300, trend Loss: 0.2745 | 0.5650
Epoch 13/300, trend Loss: 0.2657 | 0.5342
Epoch 14/300, trend Loss: 0.2631 | 0.5097
Epoch 15/300, trend Loss: 0.2635 | 0.4787
Epoch 16/300, trend Loss: 0.2619 | 0.4579
Epoch 17/300, trend Loss: 0.2614 | 0.4798
Epoch 18/300, trend Loss: 0.2822 | 0.4562
Epoch 19/300, trend Loss: 0.2669 | 0.4219
Epoch 20/300, trend Loss: 0.2508 | 0.4192
Epoch 21/300, trend Loss: 0.2663 | 0.3992
Epoch 22/300, trend Loss: 0.2456 | 0.3823
Epoch 23/300, trend Loss: 0.2308 | 0.3699
Epoch 24/300, trend Loss: 0.2193 | 0.3741
Epoch 25/300, trend Loss: 0.2152 | 0.3492
Epoch 26/300, trend Loss: 0.2115 | 0.3306
Epoch 27/300, trend Loss: 0.2060 | 0.3285
Epoch 28/300, trend Loss: 0.2056 | 0.3260
Epoch 29/300, trend Loss: 0.2002 | 0.3115
Epoch 30/300, trend Loss: 0.1973 | 0.3045
Epoch 31/300, trend Loss: 0.1960 | 0.3023
Epoch 32/300, trend Loss: 0.1923 | 0.2828
Epoch 33/300, trend Loss: 0.1913 | 0.2949
Epoch 34/300, trend Loss: 0.1881 | 0.2764
Epoch 35/300, trend Loss: 0.1878 | 0.2684
Epoch 36/300, trend Loss: 0.1907 | 0.2793
Epoch 37/300, trend Loss: 0.1890 | 0.2659
Epoch 38/300, trend Loss: 0.1856 | 0.2613
Epoch 39/300, trend Loss: 0.1957 | 0.2815
Epoch 40/300, trend Loss: 0.1957 | 0.2510
Epoch 41/300, trend Loss: 0.1966 | 0.2501
Epoch 42/300, trend Loss: 0.2222 | 0.2834
Epoch 43/300, trend Loss: 0.2125 | 0.2383
Epoch 44/300, trend Loss: 0.2024 | 0.2481
Epoch 45/300, trend Loss: 0.2261 | 0.2827
Epoch 46/300, trend Loss: 0.1996 | 0.2256
Epoch 47/300, trend Loss: 0.1849 | 0.2427
Epoch 48/300, trend Loss: 0.1812 | 0.2443
Epoch 49/300, trend Loss: 0.1746 | 0.2219
Epoch 50/300, trend Loss: 0.1739 | 0.2314
Epoch 51/300, trend Loss: 0.1690 | 0.2208
Epoch 52/300, trend Loss: 0.1666 | 0.2196
Epoch 53/300, trend Loss: 0.1655 | 0.2153
Epoch 54/300, trend Loss: 0.1647 | 0.2151
Epoch 55/300, trend Loss: 0.1633 | 0.2143
Epoch 56/300, trend Loss: 0.1611 | 0.2078
Epoch 57/300, trend Loss: 0.1603 | 0.2052
Epoch 58/300, trend Loss: 0.1596 | 0.2075
Epoch 59/300, trend Loss: 0.1577 | 0.2008
Epoch 60/300, trend Loss: 0.1561 | 0.1962
Epoch 61/300, trend Loss: 0.1560 | 0.1962
Epoch 62/300, trend Loss: 0.1559 | 0.1996
Epoch 63/300, trend Loss: 0.1546 | 0.1879
Epoch 64/300, trend Loss: 0.1533 | 0.1870
Epoch 65/300, trend Loss: 0.1541 | 0.1913
Epoch 66/300, trend Loss: 0.1530 | 0.1896
Epoch 67/300, trend Loss: 0.1519 | 0.1815
Epoch 68/300, trend Loss: 0.1520 | 0.1891
Epoch 69/300, trend Loss: 0.1526 | 0.2014
Epoch 70/300, trend Loss: 0.1527 | 0.1825
Epoch 71/300, trend Loss: 0.1542 | 0.1747
Epoch 72/300, trend Loss: 0.1571 | 0.1877
Epoch 73/300, trend Loss: 0.1609 | 0.2333
Epoch 74/300, trend Loss: 0.1594 | 0.1785
Epoch 75/300, trend Loss: 0.1678 | 0.1732
Epoch 76/300, trend Loss: 0.1803 | 0.2033
Epoch 77/300, trend Loss: 0.1546 | 0.1779
Epoch 78/300, trend Loss: 0.1516 | 0.1716
Epoch 79/300, trend Loss: 0.1489 | 0.1782
Epoch 80/300, trend Loss: 0.1469 | 0.1726
Epoch 81/300, trend Loss: 0.1452 | 0.1671
Epoch 82/300, trend Loss: 0.1442 | 0.1648
Epoch 83/300, trend Loss: 0.1452 | 0.1788
Epoch 84/300, trend Loss: 0.1423 | 0.1671
Epoch 85/300, trend Loss: 0.1433 | 0.1651
Epoch 86/300, trend Loss: 0.1432 | 0.1698
Epoch 87/300, trend Loss: 0.1450 | 0.1771
Epoch 88/300, trend Loss: 0.1438 | 0.1594
Epoch 89/300, trend Loss: 0.1417 | 0.1647
Epoch 90/300, trend Loss: 0.1461 | 0.1770
Epoch 91/300, trend Loss: 0.1416 | 0.1612
Epoch 92/300, trend Loss: 0.1416 | 0.1600
Epoch 93/300, trend Loss: 0.1455 | 0.1740
Epoch 94/300, trend Loss: 0.1386 | 0.1572
Epoch 95/300, trend Loss: 0.1397 | 0.1546
Epoch 96/300, trend Loss: 0.1409 | 0.1680
Epoch 97/300, trend Loss: 0.1373 | 0.1634
Epoch 98/300, trend Loss: 0.1365 | 0.1544
Epoch 99/300, trend Loss: 0.1366 | 0.1600
Epoch 100/300, trend Loss: 0.1348 | 0.1600
Epoch 101/300, trend Loss: 0.1347 | 0.1520
Epoch 102/300, trend Loss: 0.1338 | 0.1535
Epoch 103/300, trend Loss: 0.1334 | 0.1578
Epoch 104/300, trend Loss: 0.1343 | 0.1616
Epoch 105/300, trend Loss: 0.1350 | 0.1464
Epoch 106/300, trend Loss: 0.1336 | 0.1541
Epoch 107/300, trend Loss: 0.1367 | 0.1766
Epoch 108/300, trend Loss: 0.1355 | 0.1467
Epoch 109/300, trend Loss: 0.1352 | 0.1501
Epoch 110/300, trend Loss: 0.1393 | 0.1697
Epoch 111/300, trend Loss: 0.1331 | 0.1506
Epoch 112/300, trend Loss: 0.1363 | 0.1471
Epoch 113/300, trend Loss: 0.1353 | 0.1548
Epoch 114/300, trend Loss: 0.1305 | 0.1523
Epoch 115/300, trend Loss: 0.1316 | 0.1442
Epoch 116/300, trend Loss: 0.1297 | 0.1447
Epoch 117/300, trend Loss: 0.1299 | 0.1504
Epoch 118/300, trend Loss: 0.1286 | 0.1461
Epoch 119/300, trend Loss: 0.1278 | 0.1436
Epoch 120/300, trend Loss: 0.1283 | 0.1464
Epoch 121/300, trend Loss: 0.1269 | 0.1496
Epoch 122/300, trend Loss: 0.1272 | 0.1458
Epoch 123/300, trend Loss: 0.1274 | 0.1415
Epoch 124/300, trend Loss: 0.1274 | 0.1543
Epoch 125/300, trend Loss: 0.1258 | 0.1459
Epoch 126/300, trend Loss: 0.1256 | 0.1431
Epoch 127/300, trend Loss: 0.1262 | 0.1438
Epoch 128/300, trend Loss: 0.1260 | 0.1514
Epoch 129/300, trend Loss: 0.1250 | 0.1429
Epoch 130/300, trend Loss: 0.1268 | 0.1353
Epoch 131/300, trend Loss: 0.1261 | 0.1484
Epoch 132/300, trend Loss: 0.1280 | 0.1601
Epoch 133/300, trend Loss: 0.1271 | 0.1332
Epoch 134/300, trend Loss: 0.1291 | 0.1378
Epoch 135/300, trend Loss: 0.1311 | 0.1683
Epoch 136/300, trend Loss: 0.1252 | 0.1397
Epoch 137/300, trend Loss: 0.1261 | 0.1372
Epoch 138/300, trend Loss: 0.1242 | 0.1478
Epoch 139/300, trend Loss: 0.1227 | 0.1411
Epoch 140/300, trend Loss: 0.1223 | 0.1365
Epoch 141/300, trend Loss: 0.1223 | 0.1394
Epoch 142/300, trend Loss: 0.1214 | 0.1422
Epoch 143/300, trend Loss: 0.1210 | 0.1396
Epoch 144/300, trend Loss: 0.1203 | 0.1357
Epoch 145/300, trend Loss: 0.1208 | 0.1403
Epoch 146/300, trend Loss: 0.1207 | 0.1424
Epoch 147/300, trend Loss: 0.1208 | 0.1374
Epoch 148/300, trend Loss: 0.1209 | 0.1342
Epoch 149/300, trend Loss: 0.1212 | 0.1410
Epoch 150/300, trend Loss: 0.1193 | 0.1349
Epoch 151/300, trend Loss: 0.1201 | 0.1385
Epoch 152/300, trend Loss: 0.1195 | 0.1343
Epoch 153/300, trend Loss: 0.1202 | 0.1411
Epoch 154/300, trend Loss: 0.1185 | 0.1315
Epoch 155/300, trend Loss: 0.1184 | 0.1337
Epoch 156/300, trend Loss: 0.1181 | 0.1404
Epoch 157/300, trend Loss: 0.1182 | 0.1315
Epoch 158/300, trend Loss: 0.1177 | 0.1336
Epoch 159/300, trend Loss: 0.1188 | 0.1420
Epoch 160/300, trend Loss: 0.1175 | 0.1358
Epoch 161/300, trend Loss: 0.1177 | 0.1309
Epoch 162/300, trend Loss: 0.1179 | 0.1390
Epoch 163/300, trend Loss: 0.1191 | 0.1430
Epoch 164/300, trend Loss: 0.1184 | 0.1284
Epoch 165/300, trend Loss: 0.1182 | 0.1331
Epoch 166/300, trend Loss: 0.1197 | 0.1419
Epoch 167/300, trend Loss: 0.1169 | 0.1316
Epoch 168/300, trend Loss: 0.1168 | 0.1336
Epoch 169/300, trend Loss: 0.1170 | 0.1387
Epoch 170/300, trend Loss: 0.1150 | 0.1309
Epoch 171/300, trend Loss: 0.1154 | 0.1321
Epoch 172/300, trend Loss: 0.1149 | 0.1337
Epoch 173/300, trend Loss: 0.1153 | 0.1328
Epoch 174/300, trend Loss: 0.1147 | 0.1296
Epoch 175/300, trend Loss: 0.1143 | 0.1325
Epoch 176/300, trend Loss: 0.1144 | 0.1344
Epoch 177/300, trend Loss: 0.1144 | 0.1305
Epoch 178/300, trend Loss: 0.1137 | 0.1343
Epoch 179/300, trend Loss: 0.1142 | 0.1333
Epoch 180/300, trend Loss: 0.1140 | 0.1300
Epoch 181/300, trend Loss: 0.1139 | 0.1316
Epoch 182/300, trend Loss: 0.1138 | 0.1307
Epoch 183/300, trend Loss: 0.1130 | 0.1319
Epoch 184/300, trend Loss: 0.1134 | 0.1329
Epoch 185/300, trend Loss: 0.1126 | 0.1322
Epoch 186/300, trend Loss: 0.1126 | 0.1281
Epoch 187/300, trend Loss: 0.1121 | 0.1323
Epoch 188/300, trend Loss: 0.1130 | 0.1308
Epoch 189/300, trend Loss: 0.1125 | 0.1271
Epoch 190/300, trend Loss: 0.1131 | 0.1336
Epoch 191/300, trend Loss: 0.1123 | 0.1305
Epoch 192/300, trend Loss: 0.1120 | 0.1291
Epoch 193/300, trend Loss: 0.1114 | 0.1304
Epoch 194/300, trend Loss: 0.1114 | 0.1301
Epoch 195/300, trend Loss: 0.1113 | 0.1292
Epoch 196/300, trend Loss: 0.1114 | 0.1279
Epoch 197/300, trend Loss: 0.1113 | 0.1271
Epoch 198/300, trend Loss: 0.1109 | 0.1311
Epoch 199/300, trend Loss: 0.1107 | 0.1273
Epoch 200/300, trend Loss: 0.1102 | 0.1279
Epoch 201/300, trend Loss: 0.1112 | 0.1294
Epoch 202/300, trend Loss: 0.1103 | 0.1278
Epoch 203/300, trend Loss: 0.1096 | 0.1263
Epoch 204/300, trend Loss: 0.1096 | 0.1298
Epoch 205/300, trend Loss: 0.1105 | 0.1278
Epoch 206/300, trend Loss: 0.1099 | 0.1283
Epoch 207/300, trend Loss: 0.1101 | 0.1277
Epoch 208/300, trend Loss: 0.1100 | 0.1270
Epoch 209/300, trend Loss: 0.1101 | 0.1278
Epoch 210/300, trend Loss: 0.1094 | 0.1263
Epoch 211/300, trend Loss: 0.1090 | 0.1254
Epoch 212/300, trend Loss: 0.1087 | 0.1302
Epoch 213/300, trend Loss: 0.1088 | 0.1248
Epoch 214/300, trend Loss: 0.1093 | 0.1268
Epoch 215/300, trend Loss: 0.1090 | 0.1294
Epoch 216/300, trend Loss: 0.1083 | 0.1269
Epoch 217/300, trend Loss: 0.1082 | 0.1246
Epoch 218/300, trend Loss: 0.1083 | 0.1307
Epoch 219/300, trend Loss: 0.1086 | 0.1244
Epoch 220/300, trend Loss: 0.1083 | 0.1265
Epoch 221/300, trend Loss: 0.1080 | 0.1299
Epoch 222/300, trend Loss: 0.1082 | 0.1263
Epoch 223/300, trend Loss: 0.1073 | 0.1261
Epoch 224/300, trend Loss: 0.1078 | 0.1241
Epoch 225/300, trend Loss: 0.1070 | 0.1287
Epoch 226/300, trend Loss: 0.1068 | 0.1248
Epoch 227/300, trend Loss: 0.1074 | 0.1256
Epoch 228/300, trend Loss: 0.1071 | 0.1269
Epoch 229/300, trend Loss: 0.1078 | 0.1261
Epoch 230/300, trend Loss: 0.1068 | 0.1252
Epoch 231/300, trend Loss: 0.1061 | 0.1260
Epoch 232/300, trend Loss: 0.1075 | 0.1271
Epoch 233/300, trend Loss: 0.1077 | 0.1236
Epoch 234/300, trend Loss: 0.1067 | 0.1260
Epoch 235/300, trend Loss: 0.1069 | 0.1238
Epoch 236/300, trend Loss: 0.1064 | 0.1256
Epoch 237/300, trend Loss: 0.1062 | 0.1248
Epoch 238/300, trend Loss: 0.1063 | 0.1237
Epoch 239/300, trend Loss: 0.1066 | 0.1246
Epoch 240/300, trend Loss: 0.1062 | 0.1249
Epoch 241/300, trend Loss: 0.1056 | 0.1241
Epoch 242/300, trend Loss: 0.1054 | 0.1248
Epoch 243/300, trend Loss: 0.1057 | 0.1239
Epoch 244/300, trend Loss: 0.1052 | 0.1245
Epoch 245/300, trend Loss: 0.1057 | 0.1244
Epoch 246/300, trend Loss: 0.1056 | 0.1255
Epoch 247/300, trend Loss: 0.1059 | 0.1252
Epoch 248/300, trend Loss: 0.1047 | 0.1243
Epoch 249/300, trend Loss: 0.1053 | 0.1237
Epoch 250/300, trend Loss: 0.1050 | 0.1244
Epoch 251/300, trend Loss: 0.1043 | 0.1234
Epoch 252/300, trend Loss: 0.1052 | 0.1248
Epoch 253/300, trend Loss: 0.1045 | 0.1236
Epoch 254/300, trend Loss: 0.1048 | 0.1239
Epoch 255/300, trend Loss: 0.1046 | 0.1248
Epoch 256/300, trend Loss: 0.1042 | 0.1245
Epoch 257/300, trend Loss: 0.1042 | 0.1244
Epoch 258/300, trend Loss: 0.1046 | 0.1243
Epoch 259/300, trend Loss: 0.1042 | 0.1249
Epoch 260/300, trend Loss: 0.1051 | 0.1240
Epoch 261/300, trend Loss: 0.1045 | 0.1237
Epoch 262/300, trend Loss: 0.1043 | 0.1249
Epoch 263/300, trend Loss: 0.1039 | 0.1251
Epoch 264/300, trend Loss: 0.1042 | 0.1245
Epoch 265/300, trend Loss: 0.1045 | 0.1241
Epoch 266/300, trend Loss: 0.1041 | 0.1224
Epoch 267/300, trend Loss: 0.1042 | 0.1220
Epoch 268/300, trend Loss: 0.1046 | 0.1239
Epoch 269/300, trend Loss: 0.1035 | 0.1245
Epoch 270/300, trend Loss: 0.1038 | 0.1233
Epoch 271/300, trend Loss: 0.1024 | 0.1228
Epoch 272/300, trend Loss: 0.1040 | 0.1244
Epoch 273/300, trend Loss: 0.1040 | 0.1242
Epoch 274/300, trend Loss: 0.1033 | 0.1240
Epoch 275/300, trend Loss: 0.1037 | 0.1247
Epoch 276/300, trend Loss: 0.1027 | 0.1226
Epoch 277/300, trend Loss: 0.1040 | 0.1237
Epoch 278/300, trend Loss: 0.1039 | 0.1246
Epoch 279/300, trend Loss: 0.1033 | 0.1239
Epoch 280/300, trend Loss: 0.1031 | 0.1229
Epoch 281/300, trend Loss: 0.1024 | 0.1228
Epoch 282/300, trend Loss: 0.1022 | 0.1238
Epoch 283/300, trend Loss: 0.1028 | 0.1246
Epoch 284/300, trend Loss: 0.1026 | 0.1224
Epoch 285/300, trend Loss: 0.1028 | 0.1236
Epoch 286/300, trend Loss: 0.1032 | 0.1241
Epoch 287/300, trend Loss: 0.1020 | 0.1226
Epoch 288/300, trend Loss: 0.1030 | 0.1226
Epoch 289/300, trend Loss: 0.1026 | 0.1232
Epoch 290/300, trend Loss: 0.1035 | 0.1225
Epoch 291/300, trend Loss: 0.1026 | 0.1233
Epoch 292/300, trend Loss: 0.1030 | 0.1233
Epoch 293/300, trend Loss: 0.1030 | 0.1203
Epoch 294/300, trend Loss: 0.1025 | 0.1225
Epoch 295/300, trend Loss: 0.1027 | 0.1216
Epoch 296/300, trend Loss: 0.1034 | 0.1213
Epoch 297/300, trend Loss: 0.1028 | 0.1209
Epoch 298/300, trend Loss: 0.1022 | 0.1222
Epoch 299/300, trend Loss: 0.1020 | 0.1225
Epoch 300/300, trend Loss: 0.1017 | 0.1217
Training seasonal_0 component with params: {'observation_period_num': 185, 'train_rates': 0.9885532667148774, 'learning_rate': 0.00012043003084372219, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7962720967447972}
Epoch 1/300, seasonal_0 Loss: 0.5330 | 0.8151
Epoch 2/300, seasonal_0 Loss: 0.4033 | 0.5967
Epoch 3/300, seasonal_0 Loss: 0.3560 | 0.5163
Epoch 4/300, seasonal_0 Loss: 0.3094 | 0.4396
Epoch 5/300, seasonal_0 Loss: 0.3115 | 0.4444
Epoch 6/300, seasonal_0 Loss: 0.3114 | 0.3838
Epoch 7/300, seasonal_0 Loss: 0.3084 | 0.3414
Epoch 8/300, seasonal_0 Loss: 0.2864 | 0.3691
Epoch 9/300, seasonal_0 Loss: 0.2583 | 0.3204
Epoch 10/300, seasonal_0 Loss: 0.2747 | 0.3679
Epoch 11/300, seasonal_0 Loss: 0.2361 | 0.2876
Epoch 12/300, seasonal_0 Loss: 0.2457 | 0.3260
Epoch 13/300, seasonal_0 Loss: 0.2155 | 0.2652
Epoch 14/300, seasonal_0 Loss: 0.2278 | 0.2617
Epoch 15/300, seasonal_0 Loss: 0.2177 | 0.3142
Epoch 16/300, seasonal_0 Loss: 0.2132 | 0.2611
Epoch 17/300, seasonal_0 Loss: 0.2125 | 0.2680
Epoch 18/300, seasonal_0 Loss: 0.2236 | 0.2998
Epoch 19/300, seasonal_0 Loss: 0.2248 | 0.3066
Epoch 20/300, seasonal_0 Loss: 0.2309 | 0.2428
Epoch 21/300, seasonal_0 Loss: 0.2177 | 0.2509
Epoch 22/300, seasonal_0 Loss: 0.2201 | 0.2454
Epoch 23/300, seasonal_0 Loss: 0.1986 | 0.2206
Epoch 24/300, seasonal_0 Loss: 0.1852 | 0.2216
Epoch 25/300, seasonal_0 Loss: 0.1828 | 0.2141
Epoch 26/300, seasonal_0 Loss: 0.1781 | 0.1967
Epoch 27/300, seasonal_0 Loss: 0.1749 | 0.1986
Epoch 28/300, seasonal_0 Loss: 0.1737 | 0.2031
Epoch 29/300, seasonal_0 Loss: 0.1699 | 0.2014
Epoch 30/300, seasonal_0 Loss: 0.1658 | 0.1853
Epoch 31/300, seasonal_0 Loss: 0.1638 | 0.1818
Epoch 32/300, seasonal_0 Loss: 0.1577 | 0.1816
Epoch 33/300, seasonal_0 Loss: 0.1564 | 0.1751
Epoch 34/300, seasonal_0 Loss: 0.1541 | 0.1740
Epoch 35/300, seasonal_0 Loss: 0.1518 | 0.1792
Epoch 36/300, seasonal_0 Loss: 0.1517 | 0.1734
Epoch 37/300, seasonal_0 Loss: 0.1491 | 0.1639
Epoch 38/300, seasonal_0 Loss: 0.1493 | 0.1597
Epoch 39/300, seasonal_0 Loss: 0.1455 | 0.1688
Epoch 40/300, seasonal_0 Loss: 0.1448 | 0.1642
Epoch 41/300, seasonal_0 Loss: 0.1443 | 0.1609
Epoch 42/300, seasonal_0 Loss: 0.1412 | 0.1576
Epoch 43/300, seasonal_0 Loss: 0.1405 | 0.1670
Epoch 44/300, seasonal_0 Loss: 0.1396 | 0.1582
Epoch 45/300, seasonal_0 Loss: 0.1372 | 0.1536
Epoch 46/300, seasonal_0 Loss: 0.1365 | 0.1565
Epoch 47/300, seasonal_0 Loss: 0.1366 | 0.1584
Epoch 48/300, seasonal_0 Loss: 0.1362 | 0.1582
Epoch 49/300, seasonal_0 Loss: 0.1369 | 0.1588
Epoch 50/300, seasonal_0 Loss: 0.1384 | 0.1483
Epoch 51/300, seasonal_0 Loss: 0.1376 | 0.1498
Epoch 52/300, seasonal_0 Loss: 0.1344 | 0.1655
Epoch 53/300, seasonal_0 Loss: 0.1329 | 0.1637
Epoch 54/300, seasonal_0 Loss: 0.1305 | 0.1512
Epoch 55/300, seasonal_0 Loss: 0.1290 | 0.1417
Epoch 56/300, seasonal_0 Loss: 0.1290 | 0.1436
Epoch 57/300, seasonal_0 Loss: 0.1299 | 0.1561
Epoch 58/300, seasonal_0 Loss: 0.1274 | 0.1606
Epoch 59/300, seasonal_0 Loss: 0.1263 | 0.1488
Epoch 60/300, seasonal_0 Loss: 0.1259 | 0.1399
Epoch 61/300, seasonal_0 Loss: 0.1247 | 0.1399
Epoch 62/300, seasonal_0 Loss: 0.1229 | 0.1423
Epoch 63/300, seasonal_0 Loss: 0.1248 | 0.1505
Epoch 64/300, seasonal_0 Loss: 0.1232 | 0.1520
Epoch 65/300, seasonal_0 Loss: 0.1216 | 0.1406
Epoch 66/300, seasonal_0 Loss: 0.1219 | 0.1370
Epoch 67/300, seasonal_0 Loss: 0.1204 | 0.1391
Epoch 68/300, seasonal_0 Loss: 0.1196 | 0.1448
Epoch 69/300, seasonal_0 Loss: 0.1202 | 0.1453
Epoch 70/300, seasonal_0 Loss: 0.1195 | 0.1457
Epoch 71/300, seasonal_0 Loss: 0.1193 | 0.1372
Epoch 72/300, seasonal_0 Loss: 0.1171 | 0.1372
Epoch 73/300, seasonal_0 Loss: 0.1167 | 0.1395
Epoch 74/300, seasonal_0 Loss: 0.1174 | 0.1434
Epoch 75/300, seasonal_0 Loss: 0.1150 | 0.1422
Epoch 76/300, seasonal_0 Loss: 0.1161 | 0.1394
Epoch 77/300, seasonal_0 Loss: 0.1141 | 0.1369
Epoch 78/300, seasonal_0 Loss: 0.1149 | 0.1380
Epoch 79/300, seasonal_0 Loss: 0.1137 | 0.1414
Epoch 80/300, seasonal_0 Loss: 0.1137 | 0.1399
Epoch 81/300, seasonal_0 Loss: 0.1137 | 0.1393
Epoch 82/300, seasonal_0 Loss: 0.1129 | 0.1377
Epoch 83/300, seasonal_0 Loss: 0.1125 | 0.1368
Epoch 84/300, seasonal_0 Loss: 0.1117 | 0.1371
Epoch 85/300, seasonal_0 Loss: 0.1111 | 0.1388
Epoch 86/300, seasonal_0 Loss: 0.1118 | 0.1376
Epoch 87/300, seasonal_0 Loss: 0.1114 | 0.1357
Epoch 88/300, seasonal_0 Loss: 0.1102 | 0.1353
Epoch 89/300, seasonal_0 Loss: 0.1115 | 0.1375
Epoch 90/300, seasonal_0 Loss: 0.1113 | 0.1382
Epoch 91/300, seasonal_0 Loss: 0.1111 | 0.1371
Epoch 92/300, seasonal_0 Loss: 0.1104 | 0.1373
Epoch 93/300, seasonal_0 Loss: 0.1101 | 0.1367
Epoch 94/300, seasonal_0 Loss: 0.1105 | 0.1359
Epoch 95/300, seasonal_0 Loss: 0.1090 | 0.1371
Epoch 96/300, seasonal_0 Loss: 0.1087 | 0.1357
Epoch 97/300, seasonal_0 Loss: 0.1087 | 0.1349
Epoch 98/300, seasonal_0 Loss: 0.1091 | 0.1361
Epoch 99/300, seasonal_0 Loss: 0.1085 | 0.1365
Epoch 100/300, seasonal_0 Loss: 0.1085 | 0.1372
Epoch 101/300, seasonal_0 Loss: 0.1080 | 0.1357
Epoch 102/300, seasonal_0 Loss: 0.1080 | 0.1349
Epoch 103/300, seasonal_0 Loss: 0.1073 | 0.1346
Epoch 104/300, seasonal_0 Loss: 0.1077 | 0.1353
Epoch 105/300, seasonal_0 Loss: 0.1072 | 0.1360
Epoch 106/300, seasonal_0 Loss: 0.1063 | 0.1358
Epoch 107/300, seasonal_0 Loss: 0.1063 | 0.1353
Epoch 108/300, seasonal_0 Loss: 0.1071 | 0.1348
Epoch 109/300, seasonal_0 Loss: 0.1069 | 0.1351
Epoch 110/300, seasonal_0 Loss: 0.1069 | 0.1349
Epoch 111/300, seasonal_0 Loss: 0.1073 | 0.1347
Epoch 112/300, seasonal_0 Loss: 0.1058 | 0.1344
Epoch 113/300, seasonal_0 Loss: 0.1070 | 0.1347
Epoch 114/300, seasonal_0 Loss: 0.1075 | 0.1347
Epoch 115/300, seasonal_0 Loss: 0.1063 | 0.1346
Epoch 116/300, seasonal_0 Loss: 0.1058 | 0.1347
Epoch 117/300, seasonal_0 Loss: 0.1059 | 0.1346
Epoch 118/300, seasonal_0 Loss: 0.1056 | 0.1337
Epoch 119/300, seasonal_0 Loss: 0.1054 | 0.1335
Epoch 120/300, seasonal_0 Loss: 0.1066 | 0.1346
Epoch 121/300, seasonal_0 Loss: 0.1058 | 0.1359
Epoch 122/300, seasonal_0 Loss: 0.1054 | 0.1346
Epoch 123/300, seasonal_0 Loss: 0.1060 | 0.1349
Epoch 124/300, seasonal_0 Loss: 0.1051 | 0.1351
Epoch 125/300, seasonal_0 Loss: 0.1057 | 0.1344
Epoch 126/300, seasonal_0 Loss: 0.1051 | 0.1340
Epoch 127/300, seasonal_0 Loss: 0.1049 | 0.1339
Epoch 128/300, seasonal_0 Loss: 0.1056 | 0.1340
Epoch 129/300, seasonal_0 Loss: 0.1052 | 0.1348
Epoch 130/300, seasonal_0 Loss: 0.1046 | 0.1342
Epoch 131/300, seasonal_0 Loss: 0.1057 | 0.1339
Epoch 132/300, seasonal_0 Loss: 0.1049 | 0.1344
Epoch 133/300, seasonal_0 Loss: 0.1055 | 0.1340
Epoch 134/300, seasonal_0 Loss: 0.1053 | 0.1339
Epoch 135/300, seasonal_0 Loss: 0.1048 | 0.1340
Epoch 136/300, seasonal_0 Loss: 0.1046 | 0.1340
Epoch 137/300, seasonal_0 Loss: 0.1037 | 0.1340
Epoch 138/300, seasonal_0 Loss: 0.1042 | 0.1337
Epoch 139/300, seasonal_0 Loss: 0.1047 | 0.1341
Epoch 140/300, seasonal_0 Loss: 0.1044 | 0.1342
Epoch 141/300, seasonal_0 Loss: 0.1044 | 0.1339
Epoch 142/300, seasonal_0 Loss: 0.1045 | 0.1340
Epoch 143/300, seasonal_0 Loss: 0.1043 | 0.1340
Epoch 144/300, seasonal_0 Loss: 0.1048 | 0.1340
Epoch 145/300, seasonal_0 Loss: 0.1045 | 0.1342
Epoch 146/300, seasonal_0 Loss: 0.1042 | 0.1343
Epoch 147/300, seasonal_0 Loss: 0.1054 | 0.1342
Epoch 148/300, seasonal_0 Loss: 0.1040 | 0.1345
Epoch 149/300, seasonal_0 Loss: 0.1043 | 0.1344
Epoch 150/300, seasonal_0 Loss: 0.1035 | 0.1341
Epoch 151/300, seasonal_0 Loss: 0.1045 | 0.1340
Epoch 152/300, seasonal_0 Loss: 0.1035 | 0.1340
Epoch 153/300, seasonal_0 Loss: 0.1044 | 0.1338
Epoch 154/300, seasonal_0 Loss: 0.1035 | 0.1338
Epoch 155/300, seasonal_0 Loss: 0.1038 | 0.1335
Epoch 156/300, seasonal_0 Loss: 0.1033 | 0.1334
Epoch 157/300, seasonal_0 Loss: 0.1036 | 0.1335
Epoch 158/300, seasonal_0 Loss: 0.1034 | 0.1334
Epoch 159/300, seasonal_0 Loss: 0.1035 | 0.1337
Epoch 160/300, seasonal_0 Loss: 0.1036 | 0.1337
Epoch 161/300, seasonal_0 Loss: 0.1036 | 0.1336
Epoch 162/300, seasonal_0 Loss: 0.1040 | 0.1337
Epoch 163/300, seasonal_0 Loss: 0.1037 | 0.1339
Epoch 164/300, seasonal_0 Loss: 0.1043 | 0.1340
Epoch 165/300, seasonal_0 Loss: 0.1036 | 0.1344
Epoch 166/300, seasonal_0 Loss: 0.1026 | 0.1344
Epoch 167/300, seasonal_0 Loss: 0.1049 | 0.1341
Epoch 168/300, seasonal_0 Loss: 0.1043 | 0.1341
Epoch 169/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 170/300, seasonal_0 Loss: 0.1031 | 0.1339
Epoch 171/300, seasonal_0 Loss: 0.1031 | 0.1344
Epoch 172/300, seasonal_0 Loss: 0.1035 | 0.1343
Epoch 173/300, seasonal_0 Loss: 0.1033 | 0.1341
Epoch 174/300, seasonal_0 Loss: 0.1033 | 0.1341
Epoch 175/300, seasonal_0 Loss: 0.1042 | 0.1342
Epoch 176/300, seasonal_0 Loss: 0.1039 | 0.1343
Epoch 177/300, seasonal_0 Loss: 0.1034 | 0.1343
Epoch 178/300, seasonal_0 Loss: 0.1033 | 0.1342
Epoch 179/300, seasonal_0 Loss: 0.1041 | 0.1344
Epoch 180/300, seasonal_0 Loss: 0.1034 | 0.1342
Epoch 181/300, seasonal_0 Loss: 0.1030 | 0.1345
Epoch 182/300, seasonal_0 Loss: 0.1037 | 0.1346
Epoch 183/300, seasonal_0 Loss: 0.1043 | 0.1345
Epoch 184/300, seasonal_0 Loss: 0.1030 | 0.1346
Epoch 185/300, seasonal_0 Loss: 0.1031 | 0.1345
Epoch 186/300, seasonal_0 Loss: 0.1036 | 0.1345
Epoch 187/300, seasonal_0 Loss: 0.1031 | 0.1345
Epoch 188/300, seasonal_0 Loss: 0.1041 | 0.1344
Epoch 189/300, seasonal_0 Loss: 0.1033 | 0.1343
Epoch 190/300, seasonal_0 Loss: 0.1029 | 0.1343
Epoch 191/300, seasonal_0 Loss: 0.1032 | 0.1343
Epoch 192/300, seasonal_0 Loss: 0.1034 | 0.1342
Epoch 193/300, seasonal_0 Loss: 0.1040 | 0.1343
Epoch 194/300, seasonal_0 Loss: 0.1032 | 0.1343
Epoch 195/300, seasonal_0 Loss: 0.1038 | 0.1343
Epoch 196/300, seasonal_0 Loss: 0.1041 | 0.1342
Epoch 197/300, seasonal_0 Loss: 0.1036 | 0.1342
Epoch 198/300, seasonal_0 Loss: 0.1042 | 0.1341
Epoch 199/300, seasonal_0 Loss: 0.1036 | 0.1340
Epoch 200/300, seasonal_0 Loss: 0.1036 | 0.1340
Epoch 201/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 202/300, seasonal_0 Loss: 0.1025 | 0.1339
Epoch 203/300, seasonal_0 Loss: 0.1034 | 0.1338
Epoch 204/300, seasonal_0 Loss: 0.1033 | 0.1338
Epoch 205/300, seasonal_0 Loss: 0.1033 | 0.1338
Epoch 206/300, seasonal_0 Loss: 0.1030 | 0.1338
Epoch 207/300, seasonal_0 Loss: 0.1029 | 0.1338
Epoch 208/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 209/300, seasonal_0 Loss: 0.1031 | 0.1338
Epoch 210/300, seasonal_0 Loss: 0.1037 | 0.1338
Epoch 211/300, seasonal_0 Loss: 0.1041 | 0.1338
Epoch 212/300, seasonal_0 Loss: 0.1024 | 0.1338
Epoch 213/300, seasonal_0 Loss: 0.1033 | 0.1338
Epoch 214/300, seasonal_0 Loss: 0.1040 | 0.1338
Epoch 215/300, seasonal_0 Loss: 0.1030 | 0.1338
Epoch 216/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 217/300, seasonal_0 Loss: 0.1033 | 0.1339
Epoch 218/300, seasonal_0 Loss: 0.1031 | 0.1338
Epoch 219/300, seasonal_0 Loss: 0.1039 | 0.1338
Epoch 220/300, seasonal_0 Loss: 0.1024 | 0.1338
Epoch 221/300, seasonal_0 Loss: 0.1032 | 0.1338
Epoch 222/300, seasonal_0 Loss: 0.1027 | 0.1338
Epoch 223/300, seasonal_0 Loss: 0.1031 | 0.1338
Epoch 224/300, seasonal_0 Loss: 0.1028 | 0.1338
Epoch 225/300, seasonal_0 Loss: 0.1026 | 0.1338
Epoch 226/300, seasonal_0 Loss: 0.1038 | 0.1338
Epoch 227/300, seasonal_0 Loss: 0.1027 | 0.1338
Epoch 228/300, seasonal_0 Loss: 0.1030 | 0.1338
Epoch 229/300, seasonal_0 Loss: 0.1021 | 0.1339
Epoch 230/300, seasonal_0 Loss: 0.1023 | 0.1339
Epoch 231/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 232/300, seasonal_0 Loss: 0.1037 | 0.1339
Epoch 233/300, seasonal_0 Loss: 0.1024 | 0.1339
Epoch 234/300, seasonal_0 Loss: 0.1035 | 0.1339
Epoch 235/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 236/300, seasonal_0 Loss: 0.1037 | 0.1339
Epoch 237/300, seasonal_0 Loss: 0.1038 | 0.1339
Epoch 238/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 239/300, seasonal_0 Loss: 0.1030 | 0.1339
Epoch 240/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 241/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 242/300, seasonal_0 Loss: 0.1027 | 0.1339
Epoch 243/300, seasonal_0 Loss: 0.1033 | 0.1339
Epoch 244/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 245/300, seasonal_0 Loss: 0.1040 | 0.1339
Epoch 246/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 247/300, seasonal_0 Loss: 0.1031 | 0.1339
Epoch 248/300, seasonal_0 Loss: 0.1043 | 0.1339
Epoch 249/300, seasonal_0 Loss: 0.1041 | 0.1339
Epoch 250/300, seasonal_0 Loss: 0.1039 | 0.1339
Epoch 251/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 252/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 253/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 254/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 255/300, seasonal_0 Loss: 0.1037 | 0.1339
Epoch 256/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 257/300, seasonal_0 Loss: 0.1039 | 0.1339
Epoch 258/300, seasonal_0 Loss: 0.1039 | 0.1339
Epoch 259/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 260/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 261/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 262/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 263/300, seasonal_0 Loss: 0.1027 | 0.1339
Epoch 264/300, seasonal_0 Loss: 0.1035 | 0.1339
Epoch 265/300, seasonal_0 Loss: 0.1026 | 0.1339
Epoch 266/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 267/300, seasonal_0 Loss: 0.1035 | 0.1339
Epoch 268/300, seasonal_0 Loss: 0.1031 | 0.1339
Epoch 269/300, seasonal_0 Loss: 0.1033 | 0.1339
Epoch 270/300, seasonal_0 Loss: 0.1036 | 0.1339
Epoch 271/300, seasonal_0 Loss: 0.1037 | 0.1339
Epoch 272/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 273/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 274/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 275/300, seasonal_0 Loss: 0.1033 | 0.1339
Epoch 276/300, seasonal_0 Loss: 0.1034 | 0.1339
Epoch 277/300, seasonal_0 Loss: 0.1039 | 0.1339
Epoch 278/300, seasonal_0 Loss: 0.1025 | 0.1339
Epoch 279/300, seasonal_0 Loss: 0.1039 | 0.1339
Epoch 280/300, seasonal_0 Loss: 0.1030 | 0.1339
Epoch 281/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 282/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 283/300, seasonal_0 Loss: 0.1026 | 0.1339
Epoch 284/300, seasonal_0 Loss: 0.1033 | 0.1339
Epoch 285/300, seasonal_0 Loss: 0.1039 | 0.1338
Epoch 286/300, seasonal_0 Loss: 0.1032 | 0.1338
Epoch 287/300, seasonal_0 Loss: 0.1037 | 0.1338
Epoch 288/300, seasonal_0 Loss: 0.1035 | 0.1338
Epoch 289/300, seasonal_0 Loss: 0.1028 | 0.1338
Epoch 290/300, seasonal_0 Loss: 0.1028 | 0.1338
Epoch 291/300, seasonal_0 Loss: 0.1037 | 0.1338
Epoch 292/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 293/300, seasonal_0 Loss: 0.1032 | 0.1338
Epoch 294/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 295/300, seasonal_0 Loss: 0.1028 | 0.1339
Epoch 296/300, seasonal_0 Loss: 0.1032 | 0.1339
Epoch 297/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 298/300, seasonal_0 Loss: 0.1030 | 0.1339
Epoch 299/300, seasonal_0 Loss: 0.1029 | 0.1339
Epoch 300/300, seasonal_0 Loss: 0.1024 | 0.1339
Training seasonal_1 component with params: {'observation_period_num': 27, 'train_rates': 0.9838613872863935, 'learning_rate': 0.00033681638647322054, 'batch_size': 248, 'step_size': 11, 'gamma': 0.9899068918842432}
Epoch 1/300, seasonal_1 Loss: 1.1461 | 2.0567
Epoch 2/300, seasonal_1 Loss: 0.7724 | 1.3055
Epoch 3/300, seasonal_1 Loss: 0.6395 | 1.0203
Epoch 4/300, seasonal_1 Loss: 0.5901 | 0.8761
Epoch 5/300, seasonal_1 Loss: 0.5509 | 0.8645
Epoch 6/300, seasonal_1 Loss: 0.5949 | 0.8057
Epoch 7/300, seasonal_1 Loss: 0.5625 | 0.8129
Epoch 8/300, seasonal_1 Loss: 0.4849 | 0.7841
Epoch 9/300, seasonal_1 Loss: 0.4287 | 0.6601
Epoch 10/300, seasonal_1 Loss: 0.3845 | 0.5965
Epoch 11/300, seasonal_1 Loss: 0.4309 | 0.6256
Epoch 12/300, seasonal_1 Loss: 0.4418 | 0.5686
Epoch 13/300, seasonal_1 Loss: 0.4530 | 0.5748
Epoch 14/300, seasonal_1 Loss: 0.3551 | 0.5388
Epoch 15/300, seasonal_1 Loss: 0.3607 | 0.5123
Epoch 16/300, seasonal_1 Loss: 0.3165 | 0.5211
Epoch 17/300, seasonal_1 Loss: 0.3251 | 0.4973
Epoch 18/300, seasonal_1 Loss: 0.3152 | 0.4820
Epoch 19/300, seasonal_1 Loss: 0.3189 | 0.4991
Epoch 20/300, seasonal_1 Loss: 0.2991 | 0.4223
Epoch 21/300, seasonal_1 Loss: 0.2657 | 0.4052
Epoch 22/300, seasonal_1 Loss: 0.2725 | 0.3986
Epoch 23/300, seasonal_1 Loss: 0.2819 | 0.3970
Epoch 24/300, seasonal_1 Loss: 0.3131 | 0.3951
Epoch 25/300, seasonal_1 Loss: 0.2790 | 0.3899
Epoch 26/300, seasonal_1 Loss: 0.2832 | 0.3904
Epoch 27/300, seasonal_1 Loss: 0.2658 | 0.3408
Epoch 28/300, seasonal_1 Loss: 0.2428 | 0.3585
Epoch 29/300, seasonal_1 Loss: 0.2515 | 0.3200
Epoch 30/300, seasonal_1 Loss: 0.2797 | 0.3539
Epoch 31/300, seasonal_1 Loss: 0.2850 | 0.3173
Epoch 32/300, seasonal_1 Loss: 0.3352 | 0.3517
Epoch 33/300, seasonal_1 Loss: 0.2714 | 0.3263
Epoch 34/300, seasonal_1 Loss: 0.2889 | 0.3423
Epoch 35/300, seasonal_1 Loss: 0.2819 | 0.3564
Epoch 36/300, seasonal_1 Loss: 0.2809 | 0.3490
Epoch 37/300, seasonal_1 Loss: 0.2834 | 0.3330
Epoch 38/300, seasonal_1 Loss: 0.2713 | 0.3116
Epoch 39/300, seasonal_1 Loss: 0.2425 | 0.3225
Epoch 40/300, seasonal_1 Loss: 0.2260 | 0.2907
Epoch 41/300, seasonal_1 Loss: 0.2131 | 0.2964
Epoch 42/300, seasonal_1 Loss: 0.2125 | 0.2738
Epoch 43/300, seasonal_1 Loss: 0.2366 | 0.2847
Epoch 44/300, seasonal_1 Loss: 0.2461 | 0.2647
Epoch 45/300, seasonal_1 Loss: 0.2725 | 0.3289
Epoch 46/300, seasonal_1 Loss: 0.2292 | 0.2817
Epoch 47/300, seasonal_1 Loss: 0.2123 | 0.2849
Epoch 48/300, seasonal_1 Loss: 0.2149 | 0.2748
Epoch 49/300, seasonal_1 Loss: 0.2301 | 0.2697
Epoch 50/300, seasonal_1 Loss: 0.2243 | 0.2753
Epoch 51/300, seasonal_1 Loss: 0.2097 | 0.2807
Epoch 52/300, seasonal_1 Loss: 0.1972 | 0.2408
Epoch 53/300, seasonal_1 Loss: 0.2033 | 0.2660
Epoch 54/300, seasonal_1 Loss: 0.2140 | 0.2424
Epoch 55/300, seasonal_1 Loss: 0.2469 | 0.2804
Epoch 56/300, seasonal_1 Loss: 0.2872 | 0.3245
Epoch 57/300, seasonal_1 Loss: 0.2810 | 0.3035
Epoch 58/300, seasonal_1 Loss: 0.2743 | 0.3280
Epoch 59/300, seasonal_1 Loss: 0.2415 | 0.2902
Epoch 60/300, seasonal_1 Loss: 0.2331 | 0.2796
Epoch 61/300, seasonal_1 Loss: 0.2205 | 0.2635
Epoch 62/300, seasonal_1 Loss: 0.2137 | 0.2697
Epoch 63/300, seasonal_1 Loss: 0.1994 | 0.2314
Epoch 64/300, seasonal_1 Loss: 0.1848 | 0.2543
Epoch 65/300, seasonal_1 Loss: 0.1772 | 0.2439
Epoch 66/300, seasonal_1 Loss: 0.1690 | 0.2424
Epoch 67/300, seasonal_1 Loss: 0.1688 | 0.2365
Epoch 68/300, seasonal_1 Loss: 0.1705 | 0.2407
Epoch 69/300, seasonal_1 Loss: 0.1716 | 0.2219
Epoch 70/300, seasonal_1 Loss: 0.1674 | 0.2342
Epoch 71/300, seasonal_1 Loss: 0.1639 | 0.2102
Epoch 72/300, seasonal_1 Loss: 0.1601 | 0.2257
Epoch 73/300, seasonal_1 Loss: 0.1601 | 0.2044
Epoch 74/300, seasonal_1 Loss: 0.1562 | 0.2171
Epoch 75/300, seasonal_1 Loss: 0.1566 | 0.2005
Epoch 76/300, seasonal_1 Loss: 0.1548 | 0.2078
Epoch 77/300, seasonal_1 Loss: 0.1593 | 0.1980
Epoch 78/300, seasonal_1 Loss: 0.1693 | 0.2000
Epoch 79/300, seasonal_1 Loss: 0.1705 | 0.1915
Epoch 80/300, seasonal_1 Loss: 0.1866 | 0.2040
Epoch 81/300, seasonal_1 Loss: 0.1868 | 0.1892
Epoch 82/300, seasonal_1 Loss: 0.1981 | 0.2417
Epoch 83/300, seasonal_1 Loss: 0.1980 | 0.1996
Epoch 84/300, seasonal_1 Loss: 0.2005 | 0.2681
Epoch 85/300, seasonal_1 Loss: 0.1868 | 0.2386
Epoch 86/300, seasonal_1 Loss: 0.1753 | 0.2228
Epoch 87/300, seasonal_1 Loss: 0.1661 | 0.2187
Epoch 88/300, seasonal_1 Loss: 0.1613 | 0.2084
Epoch 89/300, seasonal_1 Loss: 0.1634 | 0.2181
Epoch 90/300, seasonal_1 Loss: 0.1686 | 0.2122
Epoch 91/300, seasonal_1 Loss: 0.1666 | 0.2212
Epoch 92/300, seasonal_1 Loss: 0.1528 | 0.2069
Epoch 93/300, seasonal_1 Loss: 0.1466 | 0.1931
Epoch 94/300, seasonal_1 Loss: 0.1411 | 0.1889
Epoch 95/300, seasonal_1 Loss: 0.1380 | 0.1873
Epoch 96/300, seasonal_1 Loss: 0.1398 | 0.1844
Epoch 97/300, seasonal_1 Loss: 0.1480 | 0.1801
Epoch 98/300, seasonal_1 Loss: 0.1579 | 0.1883
Epoch 99/300, seasonal_1 Loss: 0.1773 | 0.1902
Epoch 100/300, seasonal_1 Loss: 0.1777 | 0.1844
Epoch 101/300, seasonal_1 Loss: 0.1782 | 0.2136
Epoch 102/300, seasonal_1 Loss: 0.1754 | 0.2022
Epoch 103/300, seasonal_1 Loss: 0.1745 | 0.2105
Epoch 104/300, seasonal_1 Loss: 0.1639 | 0.1938
Epoch 105/300, seasonal_1 Loss: 0.1550 | 0.2343
Epoch 106/300, seasonal_1 Loss: 0.1503 | 0.2101
Epoch 107/300, seasonal_1 Loss: 0.1418 | 0.1946
Epoch 108/300, seasonal_1 Loss: 0.1419 | 0.1971
Epoch 109/300, seasonal_1 Loss: 0.1420 | 0.1708
Epoch 110/300, seasonal_1 Loss: 0.1435 | 0.1897
Epoch 111/300, seasonal_1 Loss: 0.1520 | 0.1695
Epoch 112/300, seasonal_1 Loss: 0.1595 | 0.1781
Epoch 113/300, seasonal_1 Loss: 0.1718 | 0.2202
Epoch 114/300, seasonal_1 Loss: 0.1656 | 0.1708
Epoch 115/300, seasonal_1 Loss: 0.1720 | 0.2074
Epoch 116/300, seasonal_1 Loss: 0.1547 | 0.1981
Epoch 117/300, seasonal_1 Loss: 0.1375 | 0.1905
Epoch 118/300, seasonal_1 Loss: 0.1331 | 0.1774
Epoch 119/300, seasonal_1 Loss: 0.1344 | 0.1648
Epoch 120/300, seasonal_1 Loss: 0.1309 | 0.1677
Epoch 121/300, seasonal_1 Loss: 0.1242 | 0.1569
Epoch 122/300, seasonal_1 Loss: 0.1209 | 0.1636
Epoch 123/300, seasonal_1 Loss: 0.1215 | 0.1527
Epoch 124/300, seasonal_1 Loss: 0.1226 | 0.1696
Epoch 125/300, seasonal_1 Loss: 0.1258 | 0.1493
Epoch 126/300, seasonal_1 Loss: 0.1254 | 0.1776
Epoch 127/300, seasonal_1 Loss: 0.1270 | 0.1461
Epoch 128/300, seasonal_1 Loss: 0.1292 | 0.1856
Epoch 129/300, seasonal_1 Loss: 0.1402 | 0.1547
Epoch 130/300, seasonal_1 Loss: 0.1440 | 0.1788
Epoch 131/300, seasonal_1 Loss: 0.1276 | 0.1473
Epoch 132/300, seasonal_1 Loss: 0.1205 | 0.1623
Epoch 133/300, seasonal_1 Loss: 0.1189 | 0.1434
Epoch 134/300, seasonal_1 Loss: 0.1162 | 0.1501
Epoch 135/300, seasonal_1 Loss: 0.1139 | 0.1393
Epoch 136/300, seasonal_1 Loss: 0.1111 | 0.1423
Epoch 137/300, seasonal_1 Loss: 0.1106 | 0.1360
Epoch 138/300, seasonal_1 Loss: 0.1100 | 0.1467
Epoch 139/300, seasonal_1 Loss: 0.1119 | 0.1365
Epoch 140/300, seasonal_1 Loss: 0.1114 | 0.1513
Epoch 141/300, seasonal_1 Loss: 0.1115 | 0.1408
Epoch 142/300, seasonal_1 Loss: 0.1124 | 0.1536
Epoch 143/300, seasonal_1 Loss: 0.1122 | 0.1345
Epoch 144/300, seasonal_1 Loss: 0.1116 | 0.1590
Epoch 145/300, seasonal_1 Loss: 0.1085 | 0.1297
Epoch 146/300, seasonal_1 Loss: 0.1066 | 0.1514
Epoch 147/300, seasonal_1 Loss: 0.1078 | 0.1343
Epoch 148/300, seasonal_1 Loss: 0.1089 | 0.1408
Epoch 149/300, seasonal_1 Loss: 0.1159 | 0.1385
Epoch 150/300, seasonal_1 Loss: 0.1178 | 0.1368
Epoch 151/300, seasonal_1 Loss: 0.1171 | 0.1415
Epoch 152/300, seasonal_1 Loss: 0.1161 | 0.1363
Epoch 153/300, seasonal_1 Loss: 0.1182 | 0.1529
Epoch 154/300, seasonal_1 Loss: 0.1262 | 0.1411
Epoch 155/300, seasonal_1 Loss: 0.1351 | 0.1740
Epoch 156/300, seasonal_1 Loss: 0.1347 | 0.1428
Epoch 157/300, seasonal_1 Loss: 0.1353 | 0.1569
Epoch 158/300, seasonal_1 Loss: 0.1419 | 0.1824
Epoch 159/300, seasonal_1 Loss: 0.1450 | 0.1589
Epoch 160/300, seasonal_1 Loss: 0.1295 | 0.1399
Epoch 161/300, seasonal_1 Loss: 0.1195 | 0.1868
Epoch 162/300, seasonal_1 Loss: 0.1155 | 0.1409
Epoch 163/300, seasonal_1 Loss: 0.1115 | 0.1460
Epoch 164/300, seasonal_1 Loss: 0.1094 | 0.1422
Epoch 165/300, seasonal_1 Loss: 0.1085 | 0.1390
Epoch 166/300, seasonal_1 Loss: 0.1085 | 0.1415
Epoch 167/300, seasonal_1 Loss: 0.1085 | 0.1295
Epoch 168/300, seasonal_1 Loss: 0.1101 | 0.1372
Epoch 169/300, seasonal_1 Loss: 0.1125 | 0.1318
Epoch 170/300, seasonal_1 Loss: 0.1141 | 0.1370
Epoch 171/300, seasonal_1 Loss: 0.1151 | 0.1414
Epoch 172/300, seasonal_1 Loss: 0.1087 | 0.1365
Epoch 173/300, seasonal_1 Loss: 0.1058 | 0.1312
Epoch 174/300, seasonal_1 Loss: 0.1006 | 0.1356
Epoch 175/300, seasonal_1 Loss: 0.1007 | 0.1216
Epoch 176/300, seasonal_1 Loss: 0.1085 | 0.1366
Epoch 177/300, seasonal_1 Loss: 0.1219 | 0.1347
Epoch 178/300, seasonal_1 Loss: 0.1403 | 0.1629
Epoch 179/300, seasonal_1 Loss: 0.1426 | 0.1334
Epoch 180/300, seasonal_1 Loss: 0.1365 | 0.2166
Epoch 181/300, seasonal_1 Loss: 0.1275 | 0.1347
Epoch 182/300, seasonal_1 Loss: 0.1118 | 0.1756
Epoch 183/300, seasonal_1 Loss: 0.1018 | 0.1510
Epoch 184/300, seasonal_1 Loss: 0.1014 | 0.1436
Epoch 185/300, seasonal_1 Loss: 0.1003 | 0.1405
Epoch 186/300, seasonal_1 Loss: 0.1001 | 0.1474
Epoch 187/300, seasonal_1 Loss: 0.0968 | 0.1347
Epoch 188/300, seasonal_1 Loss: 0.0943 | 0.1425
Epoch 189/300, seasonal_1 Loss: 0.0914 | 0.1281
Epoch 190/300, seasonal_1 Loss: 0.0901 | 0.1394
Epoch 191/300, seasonal_1 Loss: 0.0895 | 0.1256
Epoch 192/300, seasonal_1 Loss: 0.0887 | 0.1341
Epoch 193/300, seasonal_1 Loss: 0.0873 | 0.1259
Epoch 194/300, seasonal_1 Loss: 0.0856 | 0.1286
Epoch 195/300, seasonal_1 Loss: 0.0859 | 0.1255
Epoch 196/300, seasonal_1 Loss: 0.0852 | 0.1258
Epoch 197/300, seasonal_1 Loss: 0.0854 | 0.1258
Epoch 198/300, seasonal_1 Loss: 0.0862 | 0.1233
Epoch 199/300, seasonal_1 Loss: 0.0886 | 0.1281
Epoch 200/300, seasonal_1 Loss: 0.0903 | 0.1232
Epoch 201/300, seasonal_1 Loss: 0.0939 | 0.1317
Epoch 202/300, seasonal_1 Loss: 0.0940 | 0.1266
Epoch 203/300, seasonal_1 Loss: 0.0953 | 0.1304
Epoch 204/300, seasonal_1 Loss: 0.0946 | 0.1241
Epoch 205/300, seasonal_1 Loss: 0.0952 | 0.1277
Epoch 206/300, seasonal_1 Loss: 0.0967 | 0.1241
Epoch 207/300, seasonal_1 Loss: 0.0991 | 0.1304
Epoch 208/300, seasonal_1 Loss: 0.0968 | 0.1287
Epoch 209/300, seasonal_1 Loss: 0.0965 | 0.1419
Epoch 210/300, seasonal_1 Loss: 0.0981 | 0.1262
Epoch 211/300, seasonal_1 Loss: 0.1057 | 0.1771
Epoch 212/300, seasonal_1 Loss: 0.1095 | 0.1167
Epoch 213/300, seasonal_1 Loss: 0.1055 | 0.1628
Epoch 214/300, seasonal_1 Loss: 0.1038 | 0.1160
Epoch 215/300, seasonal_1 Loss: 0.1027 | 0.1566
Epoch 216/300, seasonal_1 Loss: 0.0924 | 0.1107
Epoch 217/300, seasonal_1 Loss: 0.0882 | 0.1433
Epoch 218/300, seasonal_1 Loss: 0.0853 | 0.1219
Epoch 219/300, seasonal_1 Loss: 0.0844 | 0.1489
Epoch 220/300, seasonal_1 Loss: 0.0839 | 0.1195
Epoch 221/300, seasonal_1 Loss: 0.0828 | 0.1422
Epoch 222/300, seasonal_1 Loss: 0.0819 | 0.1191
Epoch 223/300, seasonal_1 Loss: 0.0805 | 0.1322
Epoch 224/300, seasonal_1 Loss: 0.0803 | 0.1200
Epoch 225/300, seasonal_1 Loss: 0.0804 | 0.1291
Epoch 226/300, seasonal_1 Loss: 0.0800 | 0.1242
Epoch 227/300, seasonal_1 Loss: 0.0800 | 0.1187
Epoch 228/300, seasonal_1 Loss: 0.0802 | 0.1204
Epoch 229/300, seasonal_1 Loss: 0.0804 | 0.1185
Epoch 230/300, seasonal_1 Loss: 0.0833 | 0.1192
Epoch 231/300, seasonal_1 Loss: 0.0857 | 0.1203
Epoch 232/300, seasonal_1 Loss: 0.0872 | 0.1150
Epoch 233/300, seasonal_1 Loss: 0.0868 | 0.1238
Epoch 234/300, seasonal_1 Loss: 0.0883 | 0.1159
Epoch 235/300, seasonal_1 Loss: 0.0878 | 0.1265
Epoch 236/300, seasonal_1 Loss: 0.0883 | 0.1262
Epoch 237/300, seasonal_1 Loss: 0.0842 | 0.1286
Epoch 238/300, seasonal_1 Loss: 0.0816 | 0.1202
Epoch 239/300, seasonal_1 Loss: 0.0809 | 0.1215
Epoch 240/300, seasonal_1 Loss: 0.0817 | 0.1088
Epoch 241/300, seasonal_1 Loss: 0.0838 | 0.1315
Epoch 242/300, seasonal_1 Loss: 0.0838 | 0.1109
Epoch 243/300, seasonal_1 Loss: 0.0859 | 0.1368
Epoch 244/300, seasonal_1 Loss: 0.0894 | 0.1149
Epoch 245/300, seasonal_1 Loss: 0.0965 | 0.1259
Epoch 246/300, seasonal_1 Loss: 0.0942 | 0.1205
Epoch 247/300, seasonal_1 Loss: 0.0934 | 0.1151
Epoch 248/300, seasonal_1 Loss: 0.0893 | 0.1126
Epoch 249/300, seasonal_1 Loss: 0.0895 | 0.1340
Epoch 250/300, seasonal_1 Loss: 0.0879 | 0.1203
Epoch 251/300, seasonal_1 Loss: 0.0837 | 0.1539
Epoch 252/300, seasonal_1 Loss: 0.0803 | 0.1194
Epoch 253/300, seasonal_1 Loss: 0.0775 | 0.1422
Epoch 254/300, seasonal_1 Loss: 0.0748 | 0.1158
Epoch 255/300, seasonal_1 Loss: 0.0751 | 0.1305
Epoch 256/300, seasonal_1 Loss: 0.0766 | 0.1094
Epoch 257/300, seasonal_1 Loss: 0.0784 | 0.1362
Epoch 258/300, seasonal_1 Loss: 0.0785 | 0.1107
Epoch 259/300, seasonal_1 Loss: 0.0807 | 0.1327
Epoch 260/300, seasonal_1 Loss: 0.0775 | 0.1097
Epoch 261/300, seasonal_1 Loss: 0.0741 | 0.1287
Epoch 262/300, seasonal_1 Loss: 0.0726 | 0.1116
Epoch 263/300, seasonal_1 Loss: 0.0716 | 0.1217
Epoch 264/300, seasonal_1 Loss: 0.0724 | 0.1117
Epoch 265/300, seasonal_1 Loss: 0.0721 | 0.1179
Epoch 266/300, seasonal_1 Loss: 0.0713 | 0.1081
Epoch 267/300, seasonal_1 Loss: 0.0711 | 0.1159
Epoch 268/300, seasonal_1 Loss: 0.0707 | 0.1078
Epoch 269/300, seasonal_1 Loss: 0.0699 | 0.1157
Epoch 270/300, seasonal_1 Loss: 0.0697 | 0.1096
Epoch 271/300, seasonal_1 Loss: 0.0707 | 0.1175
Epoch 272/300, seasonal_1 Loss: 0.0695 | 0.1082
Epoch 273/300, seasonal_1 Loss: 0.0711 | 0.1196
Epoch 274/300, seasonal_1 Loss: 0.0720 | 0.1093
Epoch 275/300, seasonal_1 Loss: 0.0735 | 0.1207
Epoch 276/300, seasonal_1 Loss: 0.0748 | 0.1080
Epoch 277/300, seasonal_1 Loss: 0.0757 | 0.1188
Epoch 278/300, seasonal_1 Loss: 0.0750 | 0.1078
Epoch 279/300, seasonal_1 Loss: 0.0745 | 0.1095
Epoch 280/300, seasonal_1 Loss: 0.0732 | 0.1091
Epoch 281/300, seasonal_1 Loss: 0.0707 | 0.1127
Epoch 282/300, seasonal_1 Loss: 0.0706 | 0.1070
Epoch 283/300, seasonal_1 Loss: 0.0720 | 0.1257
Epoch 284/300, seasonal_1 Loss: 0.0727 | 0.1078
Epoch 285/300, seasonal_1 Loss: 0.0712 | 0.1336
Epoch 286/300, seasonal_1 Loss: 0.0705 | 0.1134
Epoch 287/300, seasonal_1 Loss: 0.0711 | 0.1247
Epoch 288/300, seasonal_1 Loss: 0.0698 | 0.1130
Epoch 289/300, seasonal_1 Loss: 0.0690 | 0.1238
Epoch 290/300, seasonal_1 Loss: 0.0688 | 0.1115
Epoch 291/300, seasonal_1 Loss: 0.0701 | 0.1271
Epoch 292/300, seasonal_1 Loss: 0.0706 | 0.1097
Epoch 293/300, seasonal_1 Loss: 0.0751 | 0.1378
Epoch 294/300, seasonal_1 Loss: 0.0750 | 0.1102
Epoch 295/300, seasonal_1 Loss: 0.0732 | 0.1405
Epoch 296/300, seasonal_1 Loss: 0.0703 | 0.1110
Epoch 297/300, seasonal_1 Loss: 0.0687 | 0.1261
Epoch 298/300, seasonal_1 Loss: 0.0685 | 0.1087
Epoch 299/300, seasonal_1 Loss: 0.0707 | 0.1258
Epoch 300/300, seasonal_1 Loss: 0.0732 | 0.1100
Training seasonal_2 component with params: {'observation_period_num': 134, 'train_rates': 0.9878391651928513, 'learning_rate': 0.00013051584474052695, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9244233365364108}
Epoch 1/300, seasonal_2 Loss: 0.9353 | 1.3413
Epoch 2/300, seasonal_2 Loss: 0.7724 | 1.1212
Epoch 3/300, seasonal_2 Loss: 0.6342 | 0.9771
Epoch 4/300, seasonal_2 Loss: 0.5195 | 0.8179
Epoch 5/300, seasonal_2 Loss: 0.4523 | 0.7935
Epoch 6/300, seasonal_2 Loss: 0.4704 | 0.7296
Epoch 7/300, seasonal_2 Loss: 0.4844 | 0.7743
Epoch 8/300, seasonal_2 Loss: 0.4670 | 0.6617
Epoch 9/300, seasonal_2 Loss: 0.4097 | 0.5678
Epoch 10/300, seasonal_2 Loss: 0.3500 | 0.5242
Epoch 11/300, seasonal_2 Loss: 0.3313 | 0.5054
Epoch 12/300, seasonal_2 Loss: 0.3245 | 0.4737
Epoch 13/300, seasonal_2 Loss: 0.2935 | 0.4590
Epoch 14/300, seasonal_2 Loss: 0.2817 | 0.4559
Epoch 15/300, seasonal_2 Loss: 0.2699 | 0.4037
Epoch 16/300, seasonal_2 Loss: 0.2612 | 0.4224
Epoch 17/300, seasonal_2 Loss: 0.2648 | 0.3773
Epoch 18/300, seasonal_2 Loss: 0.2505 | 0.3710
Epoch 19/300, seasonal_2 Loss: 0.2454 | 0.3845
Epoch 20/300, seasonal_2 Loss: 0.2500 | 0.3332
Epoch 21/300, seasonal_2 Loss: 0.2347 | 0.3404
Epoch 22/300, seasonal_2 Loss: 0.2573 | 0.3563
Epoch 23/300, seasonal_2 Loss: 0.2777 | 0.3871
Epoch 24/300, seasonal_2 Loss: 0.2610 | 0.3462
Epoch 25/300, seasonal_2 Loss: 0.2660 | 0.3645
Epoch 26/300, seasonal_2 Loss: 0.2589 | 0.3505
Epoch 27/300, seasonal_2 Loss: 0.2650 | 0.3237
Epoch 28/300, seasonal_2 Loss: 0.2676 | 0.2930
Epoch 29/300, seasonal_2 Loss: 0.3030 | 0.3280
Epoch 30/300, seasonal_2 Loss: 0.3227 | 0.3309
Epoch 31/300, seasonal_2 Loss: 0.2616 | 0.3209
Epoch 32/300, seasonal_2 Loss: 0.2566 | 0.3230
Epoch 33/300, seasonal_2 Loss: 0.2411 | 0.2943
Epoch 34/300, seasonal_2 Loss: 0.2318 | 0.2923
Epoch 35/300, seasonal_2 Loss: 0.2244 | 0.2902
Epoch 36/300, seasonal_2 Loss: 0.2148 | 0.2783
Epoch 37/300, seasonal_2 Loss: 0.1939 | 0.2717
Epoch 38/300, seasonal_2 Loss: 0.1872 | 0.2717
Epoch 39/300, seasonal_2 Loss: 0.1828 | 0.2594
Epoch 40/300, seasonal_2 Loss: 0.1796 | 0.2567
Epoch 41/300, seasonal_2 Loss: 0.1749 | 0.2493
Epoch 42/300, seasonal_2 Loss: 0.1753 | 0.2439
Epoch 43/300, seasonal_2 Loss: 0.1736 | 0.2386
Epoch 44/300, seasonal_2 Loss: 0.1728 | 0.2324
Epoch 45/300, seasonal_2 Loss: 0.1716 | 0.2378
Epoch 46/300, seasonal_2 Loss: 0.1712 | 0.2234
Epoch 47/300, seasonal_2 Loss: 0.1696 | 0.2236
Epoch 48/300, seasonal_2 Loss: 0.1707 | 0.2360
Epoch 49/300, seasonal_2 Loss: 0.1762 | 0.2220
Epoch 50/300, seasonal_2 Loss: 0.1749 | 0.2074
Epoch 51/300, seasonal_2 Loss: 0.1771 | 0.2151
Epoch 52/300, seasonal_2 Loss: 0.1807 | 0.2495
Epoch 53/300, seasonal_2 Loss: 0.1834 | 0.2173
Epoch 54/300, seasonal_2 Loss: 0.1868 | 0.2003
Epoch 55/300, seasonal_2 Loss: 0.1918 | 0.2293
Epoch 56/300, seasonal_2 Loss: 0.1964 | 0.2373
Epoch 57/300, seasonal_2 Loss: 0.2049 | 0.2010
Epoch 58/300, seasonal_2 Loss: 0.1989 | 0.2167
Epoch 59/300, seasonal_2 Loss: 0.2115 | 0.2474
Epoch 60/300, seasonal_2 Loss: 0.1910 | 0.2296
Epoch 61/300, seasonal_2 Loss: 0.1790 | 0.2272
Epoch 62/300, seasonal_2 Loss: 0.1725 | 0.2061
Epoch 63/300, seasonal_2 Loss: 0.1648 | 0.2096
Epoch 64/300, seasonal_2 Loss: 0.1575 | 0.1989
Epoch 65/300, seasonal_2 Loss: 0.1536 | 0.1967
Epoch 66/300, seasonal_2 Loss: 0.1523 | 0.1932
Epoch 67/300, seasonal_2 Loss: 0.1506 | 0.1879
Epoch 68/300, seasonal_2 Loss: 0.1514 | 0.1992
Epoch 69/300, seasonal_2 Loss: 0.1493 | 0.1855
Epoch 70/300, seasonal_2 Loss: 0.1466 | 0.1805
Epoch 71/300, seasonal_2 Loss: 0.1496 | 0.1895
Epoch 72/300, seasonal_2 Loss: 0.1479 | 0.1806
Epoch 73/300, seasonal_2 Loss: 0.1442 | 0.1791
Epoch 74/300, seasonal_2 Loss: 0.1436 | 0.1730
Epoch 75/300, seasonal_2 Loss: 0.1430 | 0.1748
Epoch 76/300, seasonal_2 Loss: 0.1457 | 0.1829
Epoch 77/300, seasonal_2 Loss: 0.1441 | 0.1749
Epoch 78/300, seasonal_2 Loss: 0.1462 | 0.1711
Epoch 79/300, seasonal_2 Loss: 0.1463 | 0.1689
Epoch 80/300, seasonal_2 Loss: 0.1524 | 0.1935
Epoch 81/300, seasonal_2 Loss: 0.1455 | 0.1671
Epoch 82/300, seasonal_2 Loss: 0.1479 | 0.1690
Epoch 83/300, seasonal_2 Loss: 0.1563 | 0.2223
Epoch 84/300, seasonal_2 Loss: 0.1502 | 0.1763
Epoch 85/300, seasonal_2 Loss: 0.1585 | 0.1723
Epoch 86/300, seasonal_2 Loss: 0.1608 | 0.1686
Epoch 87/300, seasonal_2 Loss: 0.1588 | 0.1773
Epoch 88/300, seasonal_2 Loss: 0.1641 | 0.1874
Epoch 89/300, seasonal_2 Loss: 0.1580 | 0.1628
Epoch 90/300, seasonal_2 Loss: 0.1529 | 0.1858
Epoch 91/300, seasonal_2 Loss: 0.1665 | 0.1883
Epoch 92/300, seasonal_2 Loss: 0.1505 | 0.1668
Epoch 93/300, seasonal_2 Loss: 0.1492 | 0.1729
Epoch 94/300, seasonal_2 Loss: 0.1419 | 0.1580
Epoch 95/300, seasonal_2 Loss: 0.1378 | 0.1599
Epoch 96/300, seasonal_2 Loss: 0.1342 | 0.1723
Epoch 97/300, seasonal_2 Loss: 0.1304 | 0.1565
Epoch 98/300, seasonal_2 Loss: 0.1315 | 0.1584
Epoch 99/300, seasonal_2 Loss: 0.1288 | 0.1482
Epoch 100/300, seasonal_2 Loss: 0.1281 | 0.1582
Epoch 101/300, seasonal_2 Loss: 0.1276 | 0.1608
Epoch 102/300, seasonal_2 Loss: 0.1263 | 0.1551
Epoch 103/300, seasonal_2 Loss: 0.1269 | 0.1494
Epoch 104/300, seasonal_2 Loss: 0.1271 | 0.1558
Epoch 105/300, seasonal_2 Loss: 0.1306 | 0.1534
Epoch 106/300, seasonal_2 Loss: 0.1289 | 0.1614
Epoch 107/300, seasonal_2 Loss: 0.1226 | 0.1562
Epoch 108/300, seasonal_2 Loss: 0.1242 | 0.1495
Epoch 109/300, seasonal_2 Loss: 0.1236 | 0.1518
Epoch 110/300, seasonal_2 Loss: 0.1233 | 0.1522
Epoch 111/300, seasonal_2 Loss: 0.1205 | 0.1464
Epoch 112/300, seasonal_2 Loss: 0.1231 | 0.1556
Epoch 113/300, seasonal_2 Loss: 0.1226 | 0.1659
Epoch 114/300, seasonal_2 Loss: 0.1226 | 0.1456
Epoch 115/300, seasonal_2 Loss: 0.1243 | 0.1450
Epoch 116/300, seasonal_2 Loss: 0.1246 | 0.1538
Epoch 117/300, seasonal_2 Loss: 0.1239 | 0.1474
Epoch 118/300, seasonal_2 Loss: 0.1200 | 0.1419
Epoch 119/300, seasonal_2 Loss: 0.1249 | 0.1746
Epoch 120/300, seasonal_2 Loss: 0.1246 | 0.1441
Epoch 121/300, seasonal_2 Loss: 0.1234 | 0.1423
Epoch 122/300, seasonal_2 Loss: 0.1263 | 0.1713
Epoch 123/300, seasonal_2 Loss: 0.1220 | 0.1427
Epoch 124/300, seasonal_2 Loss: 0.1210 | 0.1488
Epoch 125/300, seasonal_2 Loss: 0.1171 | 0.1472
Epoch 126/300, seasonal_2 Loss: 0.1153 | 0.1432
Epoch 127/300, seasonal_2 Loss: 0.1156 | 0.1464
Epoch 128/300, seasonal_2 Loss: 0.1165 | 0.1447
Epoch 129/300, seasonal_2 Loss: 0.1141 | 0.1472
Epoch 130/300, seasonal_2 Loss: 0.1139 | 0.1446
Epoch 131/300, seasonal_2 Loss: 0.1129 | 0.1397
Epoch 132/300, seasonal_2 Loss: 0.1113 | 0.1422
Epoch 133/300, seasonal_2 Loss: 0.1111 | 0.1466
Epoch 134/300, seasonal_2 Loss: 0.1106 | 0.1399
Epoch 135/300, seasonal_2 Loss: 0.1122 | 0.1453
Epoch 136/300, seasonal_2 Loss: 0.1133 | 0.1360
Epoch 137/300, seasonal_2 Loss: 0.1110 | 0.1407
Epoch 138/300, seasonal_2 Loss: 0.1101 | 0.1471
Epoch 139/300, seasonal_2 Loss: 0.1111 | 0.1360
Epoch 140/300, seasonal_2 Loss: 0.1111 | 0.1406
Epoch 141/300, seasonal_2 Loss: 0.1111 | 0.1472
Epoch 142/300, seasonal_2 Loss: 0.1108 | 0.1342
Epoch 143/300, seasonal_2 Loss: 0.1104 | 0.1388
Epoch 144/300, seasonal_2 Loss: 0.1132 | 0.1572
Epoch 145/300, seasonal_2 Loss: 0.1113 | 0.1336
Epoch 146/300, seasonal_2 Loss: 0.1095 | 0.1407
Epoch 147/300, seasonal_2 Loss: 0.1075 | 0.1410
Epoch 148/300, seasonal_2 Loss: 0.1065 | 0.1328
Epoch 149/300, seasonal_2 Loss: 0.1078 | 0.1416
Epoch 150/300, seasonal_2 Loss: 0.1061 | 0.1388
Epoch 151/300, seasonal_2 Loss: 0.1060 | 0.1369
Epoch 152/300, seasonal_2 Loss: 0.1053 | 0.1421
Epoch 153/300, seasonal_2 Loss: 0.1061 | 0.1357
Epoch 154/300, seasonal_2 Loss: 0.1040 | 0.1339
Epoch 155/300, seasonal_2 Loss: 0.1033 | 0.1369
Epoch 156/300, seasonal_2 Loss: 0.1044 | 0.1371
Epoch 157/300, seasonal_2 Loss: 0.1048 | 0.1350
Epoch 158/300, seasonal_2 Loss: 0.1043 | 0.1362
Epoch 159/300, seasonal_2 Loss: 0.1033 | 0.1354
Epoch 160/300, seasonal_2 Loss: 0.1026 | 0.1299
Epoch 161/300, seasonal_2 Loss: 0.1013 | 0.1367
Epoch 162/300, seasonal_2 Loss: 0.1007 | 0.1329
Epoch 163/300, seasonal_2 Loss: 0.0990 | 0.1329
Epoch 164/300, seasonal_2 Loss: 0.1001 | 0.1310
Epoch 165/300, seasonal_2 Loss: 0.0999 | 0.1312
Epoch 166/300, seasonal_2 Loss: 0.0994 | 0.1337
Epoch 167/300, seasonal_2 Loss: 0.0994 | 0.1309
Epoch 168/300, seasonal_2 Loss: 0.0987 | 0.1335
Epoch 169/300, seasonal_2 Loss: 0.0990 | 0.1289
Epoch 170/300, seasonal_2 Loss: 0.0974 | 0.1297
Epoch 171/300, seasonal_2 Loss: 0.0971 | 0.1279
Epoch 172/300, seasonal_2 Loss: 0.0971 | 0.1291
Epoch 173/300, seasonal_2 Loss: 0.0977 | 0.1359
Epoch 174/300, seasonal_2 Loss: 0.0965 | 0.1277
Epoch 175/300, seasonal_2 Loss: 0.0980 | 0.1250
Epoch 176/300, seasonal_2 Loss: 0.0962 | 0.1275
Epoch 177/300, seasonal_2 Loss: 0.0982 | 0.1285
Epoch 178/300, seasonal_2 Loss: 0.0998 | 0.1305
Epoch 179/300, seasonal_2 Loss: 0.1000 | 0.1345
Epoch 180/300, seasonal_2 Loss: 0.1004 | 0.1277
Epoch 181/300, seasonal_2 Loss: 0.0993 | 0.1268
Epoch 182/300, seasonal_2 Loss: 0.0979 | 0.1278
Epoch 183/300, seasonal_2 Loss: 0.0963 | 0.1261
Epoch 184/300, seasonal_2 Loss: 0.0959 | 0.1315
Epoch 185/300, seasonal_2 Loss: 0.0954 | 0.1299
Epoch 186/300, seasonal_2 Loss: 0.0958 | 0.1248
Epoch 187/300, seasonal_2 Loss: 0.0948 | 0.1213
Epoch 188/300, seasonal_2 Loss: 0.0954 | 0.1272
Epoch 189/300, seasonal_2 Loss: 0.0959 | 0.1307
Epoch 190/300, seasonal_2 Loss: 0.0947 | 0.1281
Epoch 191/300, seasonal_2 Loss: 0.0954 | 0.1243
Epoch 192/300, seasonal_2 Loss: 0.0952 | 0.1232
Epoch 193/300, seasonal_2 Loss: 0.0963 | 0.1282
Epoch 194/300, seasonal_2 Loss: 0.0943 | 0.1307
Epoch 195/300, seasonal_2 Loss: 0.0948 | 0.1230
Epoch 196/300, seasonal_2 Loss: 0.0959 | 0.1236
Epoch 197/300, seasonal_2 Loss: 0.0937 | 0.1234
Epoch 198/300, seasonal_2 Loss: 0.0937 | 0.1273
Epoch 199/300, seasonal_2 Loss: 0.0923 | 0.1226
Epoch 200/300, seasonal_2 Loss: 0.0938 | 0.1195
Epoch 201/300, seasonal_2 Loss: 0.0931 | 0.1236
Epoch 202/300, seasonal_2 Loss: 0.0922 | 0.1225
Epoch 203/300, seasonal_2 Loss: 0.0927 | 0.1235
Epoch 204/300, seasonal_2 Loss: 0.0910 | 0.1243
Epoch 205/300, seasonal_2 Loss: 0.0919 | 0.1212
Epoch 206/300, seasonal_2 Loss: 0.0919 | 0.1226
Epoch 207/300, seasonal_2 Loss: 0.0926 | 0.1217
Epoch 208/300, seasonal_2 Loss: 0.0906 | 0.1227
Epoch 209/300, seasonal_2 Loss: 0.0908 | 0.1202
Epoch 210/300, seasonal_2 Loss: 0.0903 | 0.1212
Epoch 211/300, seasonal_2 Loss: 0.0899 | 0.1213
Epoch 212/300, seasonal_2 Loss: 0.0918 | 0.1232
Epoch 213/300, seasonal_2 Loss: 0.0900 | 0.1170
Epoch 214/300, seasonal_2 Loss: 0.0893 | 0.1211
Epoch 215/300, seasonal_2 Loss: 0.0896 | 0.1209
Epoch 216/300, seasonal_2 Loss: 0.0885 | 0.1196
Epoch 217/300, seasonal_2 Loss: 0.0882 | 0.1182
Epoch 218/300, seasonal_2 Loss: 0.0884 | 0.1182
Epoch 219/300, seasonal_2 Loss: 0.0885 | 0.1191
Epoch 220/300, seasonal_2 Loss: 0.0887 | 0.1196
Epoch 221/300, seasonal_2 Loss: 0.0879 | 0.1188
Epoch 222/300, seasonal_2 Loss: 0.0889 | 0.1209
Epoch 223/300, seasonal_2 Loss: 0.0880 | 0.1203
Epoch 224/300, seasonal_2 Loss: 0.0877 | 0.1174
Epoch 225/300, seasonal_2 Loss: 0.0880 | 0.1206
Epoch 226/300, seasonal_2 Loss: 0.0880 | 0.1190
Epoch 227/300, seasonal_2 Loss: 0.0878 | 0.1196
Epoch 228/300, seasonal_2 Loss: 0.0868 | 0.1193
Epoch 229/300, seasonal_2 Loss: 0.0860 | 0.1180
Epoch 230/300, seasonal_2 Loss: 0.0858 | 0.1181
Epoch 231/300, seasonal_2 Loss: 0.0863 | 0.1183
Epoch 232/300, seasonal_2 Loss: 0.0866 | 0.1192
Epoch 233/300, seasonal_2 Loss: 0.0860 | 0.1170
Epoch 234/300, seasonal_2 Loss: 0.0863 | 0.1211
Epoch 235/300, seasonal_2 Loss: 0.0867 | 0.1179
Epoch 236/300, seasonal_2 Loss: 0.0863 | 0.1166
Epoch 237/300, seasonal_2 Loss: 0.0863 | 0.1171
Epoch 238/300, seasonal_2 Loss: 0.0856 | 0.1188
Epoch 239/300, seasonal_2 Loss: 0.0865 | 0.1178
Epoch 240/300, seasonal_2 Loss: 0.0863 | 0.1177
Epoch 241/300, seasonal_2 Loss: 0.0858 | 0.1155
Epoch 242/300, seasonal_2 Loss: 0.0849 | 0.1166
Epoch 243/300, seasonal_2 Loss: 0.0849 | 0.1148
Epoch 244/300, seasonal_2 Loss: 0.0845 | 0.1166
Epoch 245/300, seasonal_2 Loss: 0.0854 | 0.1155
Epoch 246/300, seasonal_2 Loss: 0.0847 | 0.1166
Epoch 247/300, seasonal_2 Loss: 0.0849 | 0.1161
Epoch 248/300, seasonal_2 Loss: 0.0852 | 0.1163
Epoch 249/300, seasonal_2 Loss: 0.0856 | 0.1159
Epoch 250/300, seasonal_2 Loss: 0.0850 | 0.1164
Epoch 251/300, seasonal_2 Loss: 0.0848 | 0.1159
Epoch 252/300, seasonal_2 Loss: 0.0847 | 0.1164
Epoch 253/300, seasonal_2 Loss: 0.0839 | 0.1163
Epoch 254/300, seasonal_2 Loss: 0.0843 | 0.1161
Epoch 255/300, seasonal_2 Loss: 0.0834 | 0.1143
Epoch 256/300, seasonal_2 Loss: 0.0838 | 0.1137
Epoch 257/300, seasonal_2 Loss: 0.0834 | 0.1166
Epoch 258/300, seasonal_2 Loss: 0.0826 | 0.1153
Epoch 259/300, seasonal_2 Loss: 0.0832 | 0.1150
Epoch 260/300, seasonal_2 Loss: 0.0834 | 0.1156
Epoch 261/300, seasonal_2 Loss: 0.0832 | 0.1141
Epoch 262/300, seasonal_2 Loss: 0.0848 | 0.1136
Epoch 263/300, seasonal_2 Loss: 0.0838 | 0.1139
Epoch 264/300, seasonal_2 Loss: 0.0826 | 0.1140
Epoch 265/300, seasonal_2 Loss: 0.0829 | 0.1143
Epoch 266/300, seasonal_2 Loss: 0.0819 | 0.1138
Epoch 267/300, seasonal_2 Loss: 0.0824 | 0.1144
Epoch 268/300, seasonal_2 Loss: 0.0828 | 0.1138
Epoch 269/300, seasonal_2 Loss: 0.0828 | 0.1145
Epoch 270/300, seasonal_2 Loss: 0.0826 | 0.1132
Epoch 271/300, seasonal_2 Loss: 0.0816 | 0.1140
Epoch 272/300, seasonal_2 Loss: 0.0831 | 0.1132
Epoch 273/300, seasonal_2 Loss: 0.0824 | 0.1139
Epoch 274/300, seasonal_2 Loss: 0.0827 | 0.1143
Epoch 275/300, seasonal_2 Loss: 0.0819 | 0.1141
Epoch 276/300, seasonal_2 Loss: 0.0815 | 0.1127
Epoch 277/300, seasonal_2 Loss: 0.0809 | 0.1122
Epoch 278/300, seasonal_2 Loss: 0.0823 | 0.1131
Epoch 279/300, seasonal_2 Loss: 0.0821 | 0.1146
Epoch 280/300, seasonal_2 Loss: 0.0817 | 0.1118
Epoch 281/300, seasonal_2 Loss: 0.0825 | 0.1124
Epoch 282/300, seasonal_2 Loss: 0.0818 | 0.1141
Epoch 283/300, seasonal_2 Loss: 0.0824 | 0.1117
Epoch 284/300, seasonal_2 Loss: 0.0818 | 0.1137
Epoch 285/300, seasonal_2 Loss: 0.0814 | 0.1118
Epoch 286/300, seasonal_2 Loss: 0.0823 | 0.1153
Epoch 287/300, seasonal_2 Loss: 0.0812 | 0.1140
Epoch 288/300, seasonal_2 Loss: 0.0812 | 0.1132
Epoch 289/300, seasonal_2 Loss: 0.0822 | 0.1121
Epoch 290/300, seasonal_2 Loss: 0.0809 | 0.1125
Epoch 291/300, seasonal_2 Loss: 0.0808 | 0.1134
Epoch 292/300, seasonal_2 Loss: 0.0808 | 0.1133
Epoch 293/300, seasonal_2 Loss: 0.0812 | 0.1126
Epoch 294/300, seasonal_2 Loss: 0.0809 | 0.1129
Epoch 295/300, seasonal_2 Loss: 0.0808 | 0.1119
Epoch 296/300, seasonal_2 Loss: 0.0806 | 0.1130
Epoch 297/300, seasonal_2 Loss: 0.0805 | 0.1128
Epoch 298/300, seasonal_2 Loss: 0.0802 | 0.1123
Epoch 299/300, seasonal_2 Loss: 0.0797 | 0.1143
Epoch 300/300, seasonal_2 Loss: 0.0804 | 0.1139
Training seasonal_3 component with params: {'observation_period_num': 163, 'train_rates': 0.9874310069766136, 'learning_rate': 0.00020938713513469332, 'batch_size': 142, 'step_size': 9, 'gamma': 0.7538709856824515}
Epoch 1/300, seasonal_3 Loss: 1.0620 | 1.5733
Epoch 2/300, seasonal_3 Loss: 0.7643 | 1.0238
Epoch 3/300, seasonal_3 Loss: 0.6186 | 0.8820
Epoch 4/300, seasonal_3 Loss: 0.5521 | 0.7873
Epoch 5/300, seasonal_3 Loss: 0.5057 | 0.7097
Epoch 6/300, seasonal_3 Loss: 0.4657 | 0.6661
Epoch 7/300, seasonal_3 Loss: 0.4329 | 0.6462
Epoch 8/300, seasonal_3 Loss: 0.5100 | 0.6962
Epoch 9/300, seasonal_3 Loss: 0.4341 | 0.5914
Epoch 10/300, seasonal_3 Loss: 0.3976 | 0.5652
Epoch 11/300, seasonal_3 Loss: 0.3720 | 0.5799
Epoch 12/300, seasonal_3 Loss: 0.3613 | 0.5093
Epoch 13/300, seasonal_3 Loss: 0.3225 | 0.5060
Epoch 14/300, seasonal_3 Loss: 0.3194 | 0.4819
Epoch 15/300, seasonal_3 Loss: 0.3059 | 0.4725
Epoch 16/300, seasonal_3 Loss: 0.2877 | 0.4514
Epoch 17/300, seasonal_3 Loss: 0.2857 | 0.4376
Epoch 18/300, seasonal_3 Loss: 0.2711 | 0.4307
Epoch 19/300, seasonal_3 Loss: 0.2694 | 0.4127
Epoch 20/300, seasonal_3 Loss: 0.2574 | 0.4119
Epoch 21/300, seasonal_3 Loss: 0.2515 | 0.4007
Epoch 22/300, seasonal_3 Loss: 0.2478 | 0.3901
Epoch 23/300, seasonal_3 Loss: 0.2432 | 0.3872
Epoch 24/300, seasonal_3 Loss: 0.2397 | 0.3747
Epoch 25/300, seasonal_3 Loss: 0.2361 | 0.3752
Epoch 26/300, seasonal_3 Loss: 0.2331 | 0.3646
Epoch 27/300, seasonal_3 Loss: 0.2297 | 0.3612
Epoch 28/300, seasonal_3 Loss: 0.2277 | 0.3555
Epoch 29/300, seasonal_3 Loss: 0.2268 | 0.3528
Epoch 30/300, seasonal_3 Loss: 0.2248 | 0.3490
Epoch 31/300, seasonal_3 Loss: 0.2235 | 0.3449
Epoch 32/300, seasonal_3 Loss: 0.2212 | 0.3409
Epoch 33/300, seasonal_3 Loss: 0.2196 | 0.3385
Epoch 34/300, seasonal_3 Loss: 0.2192 | 0.3347
Epoch 35/300, seasonal_3 Loss: 0.2174 | 0.3322
Epoch 36/300, seasonal_3 Loss: 0.2168 | 0.3302
Epoch 37/300, seasonal_3 Loss: 0.2158 | 0.3279
Epoch 38/300, seasonal_3 Loss: 0.2144 | 0.3261
Epoch 39/300, seasonal_3 Loss: 0.2133 | 0.3239
Epoch 40/300, seasonal_3 Loss: 0.2134 | 0.3218
Epoch 41/300, seasonal_3 Loss: 0.2115 | 0.3203
Epoch 42/300, seasonal_3 Loss: 0.2111 | 0.3187
Epoch 43/300, seasonal_3 Loss: 0.2106 | 0.3173
Epoch 44/300, seasonal_3 Loss: 0.2103 | 0.3157
Epoch 45/300, seasonal_3 Loss: 0.2094 | 0.3146
Epoch 46/300, seasonal_3 Loss: 0.2088 | 0.3132
Epoch 47/300, seasonal_3 Loss: 0.2078 | 0.3116
Epoch 48/300, seasonal_3 Loss: 0.2074 | 0.3103
Epoch 49/300, seasonal_3 Loss: 0.2072 | 0.3095
Epoch 50/300, seasonal_3 Loss: 0.2066 | 0.3083
Epoch 51/300, seasonal_3 Loss: 0.2063 | 0.3075
Epoch 52/300, seasonal_3 Loss: 0.2061 | 0.3067
Epoch 53/300, seasonal_3 Loss: 0.2058 | 0.3059
Epoch 54/300, seasonal_3 Loss: 0.2052 | 0.3050
Epoch 55/300, seasonal_3 Loss: 0.2050 | 0.3045
Epoch 56/300, seasonal_3 Loss: 0.2050 | 0.3038
Epoch 57/300, seasonal_3 Loss: 0.2046 | 0.3034
Epoch 58/300, seasonal_3 Loss: 0.2044 | 0.3027
Epoch 59/300, seasonal_3 Loss: 0.2038 | 0.3019
Epoch 60/300, seasonal_3 Loss: 0.2034 | 0.3014
Epoch 61/300, seasonal_3 Loss: 0.2038 | 0.3012
Epoch 62/300, seasonal_3 Loss: 0.2031 | 0.3009
Epoch 63/300, seasonal_3 Loss: 0.2028 | 0.3004
Epoch 64/300, seasonal_3 Loss: 0.2021 | 0.3002
Epoch 65/300, seasonal_3 Loss: 0.2028 | 0.2997
Epoch 66/300, seasonal_3 Loss: 0.2025 | 0.2990
Epoch 67/300, seasonal_3 Loss: 0.2024 | 0.2985
Epoch 68/300, seasonal_3 Loss: 0.2018 | 0.2980
Epoch 69/300, seasonal_3 Loss: 0.2022 | 0.2977
Epoch 70/300, seasonal_3 Loss: 0.2019 | 0.2974
Epoch 71/300, seasonal_3 Loss: 0.2015 | 0.2971
Epoch 72/300, seasonal_3 Loss: 0.2017 | 0.2967
Epoch 73/300, seasonal_3 Loss: 0.2009 | 0.2965
Epoch 74/300, seasonal_3 Loss: 0.2012 | 0.2963
Epoch 75/300, seasonal_3 Loss: 0.2006 | 0.2962
Epoch 76/300, seasonal_3 Loss: 0.2013 | 0.2961
Epoch 77/300, seasonal_3 Loss: 0.2015 | 0.2959
Epoch 78/300, seasonal_3 Loss: 0.2008 | 0.2958
Epoch 79/300, seasonal_3 Loss: 0.2003 | 0.2956
Epoch 80/300, seasonal_3 Loss: 0.2003 | 0.2955
Epoch 81/300, seasonal_3 Loss: 0.2004 | 0.2954
Epoch 82/300, seasonal_3 Loss: 0.2000 | 0.2953
Epoch 83/300, seasonal_3 Loss: 0.2005 | 0.2951
Epoch 84/300, seasonal_3 Loss: 0.2003 | 0.2949
Epoch 85/300, seasonal_3 Loss: 0.2009 | 0.2947
Epoch 86/300, seasonal_3 Loss: 0.2004 | 0.2946
Epoch 87/300, seasonal_3 Loss: 0.2009 | 0.2945
Epoch 88/300, seasonal_3 Loss: 0.2001 | 0.2944
Epoch 89/300, seasonal_3 Loss: 0.2002 | 0.2943
Epoch 90/300, seasonal_3 Loss: 0.2002 | 0.2942
Epoch 91/300, seasonal_3 Loss: 0.1994 | 0.2941
Epoch 92/300, seasonal_3 Loss: 0.2000 | 0.2941
Epoch 93/300, seasonal_3 Loss: 0.2000 | 0.2940
Epoch 94/300, seasonal_3 Loss: 0.2006 | 0.2939
Epoch 95/300, seasonal_3 Loss: 0.1999 | 0.2939
Epoch 96/300, seasonal_3 Loss: 0.2003 | 0.2938
Epoch 97/300, seasonal_3 Loss: 0.1996 | 0.2937
Epoch 98/300, seasonal_3 Loss: 0.2000 | 0.2936
Epoch 99/300, seasonal_3 Loss: 0.2001 | 0.2935
Epoch 100/300, seasonal_3 Loss: 0.2006 | 0.2934
Epoch 101/300, seasonal_3 Loss: 0.1999 | 0.2934
Epoch 102/300, seasonal_3 Loss: 0.1999 | 0.2934
Epoch 103/300, seasonal_3 Loss: 0.1995 | 0.2933
Epoch 104/300, seasonal_3 Loss: 0.2002 | 0.2933
Epoch 105/300, seasonal_3 Loss: 0.2000 | 0.2932
Epoch 106/300, seasonal_3 Loss: 0.2004 | 0.2932
Epoch 107/300, seasonal_3 Loss: 0.1995 | 0.2931
Epoch 108/300, seasonal_3 Loss: 0.1996 | 0.2931
Epoch 109/300, seasonal_3 Loss: 0.1998 | 0.2931
Epoch 110/300, seasonal_3 Loss: 0.1996 | 0.2931
Epoch 111/300, seasonal_3 Loss: 0.1993 | 0.2931
Epoch 112/300, seasonal_3 Loss: 0.2007 | 0.2931
Epoch 113/300, seasonal_3 Loss: 0.1997 | 0.2930
Epoch 114/300, seasonal_3 Loss: 0.1998 | 0.2930
Epoch 115/300, seasonal_3 Loss: 0.1996 | 0.2930
Epoch 116/300, seasonal_3 Loss: 0.2002 | 0.2930
Epoch 117/300, seasonal_3 Loss: 0.1995 | 0.2929
Epoch 118/300, seasonal_3 Loss: 0.1998 | 0.2929
Epoch 119/300, seasonal_3 Loss: 0.2003 | 0.2929
Epoch 120/300, seasonal_3 Loss: 0.1993 | 0.2929
Epoch 121/300, seasonal_3 Loss: 0.1993 | 0.2928
Epoch 122/300, seasonal_3 Loss: 0.1998 | 0.2928
Epoch 123/300, seasonal_3 Loss: 0.1997 | 0.2928
Epoch 124/300, seasonal_3 Loss: 0.1999 | 0.2928
Epoch 125/300, seasonal_3 Loss: 0.1995 | 0.2928
Epoch 126/300, seasonal_3 Loss: 0.1997 | 0.2928
Epoch 127/300, seasonal_3 Loss: 0.1998 | 0.2928
Epoch 128/300, seasonal_3 Loss: 0.1994 | 0.2928
Epoch 129/300, seasonal_3 Loss: 0.1996 | 0.2927
Epoch 130/300, seasonal_3 Loss: 0.1990 | 0.2927
Epoch 131/300, seasonal_3 Loss: 0.1992 | 0.2927
Epoch 132/300, seasonal_3 Loss: 0.1993 | 0.2927
Epoch 133/300, seasonal_3 Loss: 0.1992 | 0.2927
Epoch 134/300, seasonal_3 Loss: 0.1998 | 0.2927
Epoch 135/300, seasonal_3 Loss: 0.1994 | 0.2927
Epoch 136/300, seasonal_3 Loss: 0.1994 | 0.2927
Epoch 137/300, seasonal_3 Loss: 0.1997 | 0.2927
Epoch 138/300, seasonal_3 Loss: 0.1993 | 0.2927
Epoch 139/300, seasonal_3 Loss: 0.1997 | 0.2927
Epoch 140/300, seasonal_3 Loss: 0.2000 | 0.2927
Epoch 141/300, seasonal_3 Loss: 0.1991 | 0.2927
Epoch 142/300, seasonal_3 Loss: 0.1998 | 0.2927
Epoch 143/300, seasonal_3 Loss: 0.2004 | 0.2927
Epoch 144/300, seasonal_3 Loss: 0.1995 | 0.2927
Epoch 145/300, seasonal_3 Loss: 0.1996 | 0.2927
Epoch 146/300, seasonal_3 Loss: 0.1999 | 0.2927
Epoch 147/300, seasonal_3 Loss: 0.1997 | 0.2927
Epoch 148/300, seasonal_3 Loss: 0.1996 | 0.2927
Epoch 149/300, seasonal_3 Loss: 0.1994 | 0.2927
Epoch 150/300, seasonal_3 Loss: 0.1991 | 0.2927
Epoch 151/300, seasonal_3 Loss: 0.1995 | 0.2927
Epoch 152/300, seasonal_3 Loss: 0.2002 | 0.2927
Epoch 153/300, seasonal_3 Loss: 0.1994 | 0.2927
Epoch 154/300, seasonal_3 Loss: 0.1994 | 0.2927
Epoch 155/300, seasonal_3 Loss: 0.1996 | 0.2926
Epoch 156/300, seasonal_3 Loss: 0.1999 | 0.2926
Epoch 157/300, seasonal_3 Loss: 0.1991 | 0.2926
Epoch 158/300, seasonal_3 Loss: 0.1996 | 0.2926
Epoch 159/300, seasonal_3 Loss: 0.1998 | 0.2926
Epoch 160/300, seasonal_3 Loss: 0.1996 | 0.2926
Epoch 161/300, seasonal_3 Loss: 0.1992 | 0.2926
Epoch 162/300, seasonal_3 Loss: 0.1998 | 0.2926
Epoch 163/300, seasonal_3 Loss: 0.1990 | 0.2926
Epoch 164/300, seasonal_3 Loss: 0.1995 | 0.2926
Epoch 165/300, seasonal_3 Loss: 0.1998 | 0.2926
Epoch 166/300, seasonal_3 Loss: 0.1998 | 0.2926
Epoch 167/300, seasonal_3 Loss: 0.1995 | 0.2926
Epoch 168/300, seasonal_3 Loss: 0.1998 | 0.2926
Epoch 169/300, seasonal_3 Loss: 0.1990 | 0.2926
Epoch 170/300, seasonal_3 Loss: 0.2001 | 0.2926
Epoch 171/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 172/300, seasonal_3 Loss: 0.1992 | 0.2926
Epoch 173/300, seasonal_3 Loss: 0.1994 | 0.2926
Epoch 174/300, seasonal_3 Loss: 0.1987 | 0.2926
Epoch 175/300, seasonal_3 Loss: 0.1995 | 0.2926
Epoch 176/300, seasonal_3 Loss: 0.1993 | 0.2926
Epoch 177/300, seasonal_3 Loss: 0.1994 | 0.2926
Epoch 178/300, seasonal_3 Loss: 0.1991 | 0.2926
Epoch 179/300, seasonal_3 Loss: 0.1999 | 0.2926
Epoch 180/300, seasonal_3 Loss: 0.1996 | 0.2926
Epoch 181/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 182/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 183/300, seasonal_3 Loss: 0.1996 | 0.2926
Epoch 184/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 185/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 186/300, seasonal_3 Loss: 0.1994 | 0.2926
Epoch 187/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 188/300, seasonal_3 Loss: 0.1997 | 0.2926
Epoch 189/300, seasonal_3 Loss: 0.1993 | 0.2926
Epoch 190/300, seasonal_3 Loss: 0.2002 | 0.2926
Epoch 191/300, seasonal_3 Loss: 0.1996 | 0.2926
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 78, 'train_rates': 0.9845217102426269, 'learning_rate': 0.00021346384760085146, 'batch_size': 234, 'step_size': 10, 'gamma': 0.8942020870492958}
Epoch 1/300, resid Loss: 1.2155 | 1.9738
Epoch 2/300, resid Loss: 0.7434 | 1.2665
Epoch 3/300, resid Loss: 0.6105 | 1.0688
Epoch 4/300, resid Loss: 0.5414 | 0.9590
Epoch 5/300, resid Loss: 0.4945 | 0.8458
Epoch 6/300, resid Loss: 0.4697 | 0.8199
Epoch 7/300, resid Loss: 0.4799 | 0.7733
Epoch 8/300, resid Loss: 0.4438 | 0.7793
Epoch 9/300, resid Loss: 0.3964 | 0.6980
Epoch 10/300, resid Loss: 0.3840 | 0.6716
Epoch 11/300, resid Loss: 0.3572 | 0.6334
Epoch 12/300, resid Loss: 0.3832 | 0.6159
Epoch 13/300, resid Loss: 0.3552 | 0.6007
Epoch 14/300, resid Loss: 0.3603 | 0.5729
Epoch 15/300, resid Loss: 0.3639 | 0.5930
Epoch 16/300, resid Loss: 0.3400 | 0.5476
Epoch 17/300, resid Loss: 0.3616 | 0.5312
Epoch 18/300, resid Loss: 0.3006 | 0.5311
Epoch 19/300, resid Loss: 0.2937 | 0.5051
Epoch 20/300, resid Loss: 0.2696 | 0.4893
Epoch 21/300, resid Loss: 0.2607 | 0.4748
Epoch 22/300, resid Loss: 0.2525 | 0.4632
Epoch 23/300, resid Loss: 0.2482 | 0.4510
Epoch 24/300, resid Loss: 0.2441 | 0.4379
Epoch 25/300, resid Loss: 0.2394 | 0.4289
Epoch 26/300, resid Loss: 0.2369 | 0.4190
Epoch 27/300, resid Loss: 0.2328 | 0.4095
Epoch 28/300, resid Loss: 0.2311 | 0.4000
Epoch 29/300, resid Loss: 0.2306 | 0.3935
Epoch 30/300, resid Loss: 0.2353 | 0.3827
Epoch 31/300, resid Loss: 0.2358 | 0.3803
Epoch 32/300, resid Loss: 0.2398 | 0.3680
Epoch 33/300, resid Loss: 0.2266 | 0.3695
Epoch 34/300, resid Loss: 0.2247 | 0.3586
Epoch 35/300, resid Loss: 0.2278 | 0.3619
Epoch 36/300, resid Loss: 0.2327 | 0.3581
Epoch 37/300, resid Loss: 0.2442 | 0.3578
Epoch 38/300, resid Loss: 0.2427 | 0.3537
Epoch 39/300, resid Loss: 0.2502 | 0.3587
Epoch 40/300, resid Loss: 0.2396 | 0.3497
Epoch 41/300, resid Loss: 0.2226 | 0.3419
Epoch 42/300, resid Loss: 0.2107 | 0.3340
Epoch 43/300, resid Loss: 0.2052 | 0.3250
Epoch 44/300, resid Loss: 0.2037 | 0.3278
Epoch 45/300, resid Loss: 0.2015 | 0.3152
Epoch 46/300, resid Loss: 0.2008 | 0.3231
Epoch 47/300, resid Loss: 0.1988 | 0.3068
Epoch 48/300, resid Loss: 0.1977 | 0.3177
Epoch 49/300, resid Loss: 0.1969 | 0.3006
Epoch 50/300, resid Loss: 0.1950 | 0.3080
Epoch 51/300, resid Loss: 0.1934 | 0.2952
Epoch 52/300, resid Loss: 0.1921 | 0.2998
Epoch 53/300, resid Loss: 0.1903 | 0.2922
Epoch 54/300, resid Loss: 0.1904 | 0.2932
Epoch 55/300, resid Loss: 0.1893 | 0.2907
Epoch 56/300, resid Loss: 0.1882 | 0.2905
Epoch 57/300, resid Loss: 0.1872 | 0.2834
Epoch 58/300, resid Loss: 0.1864 | 0.2829
Epoch 59/300, resid Loss: 0.1855 | 0.2837
Epoch 60/300, resid Loss: 0.1845 | 0.2802
Epoch 61/300, resid Loss: 0.1839 | 0.2782
Epoch 62/300, resid Loss: 0.1835 | 0.2765
Epoch 63/300, resid Loss: 0.1823 | 0.2759
Epoch 64/300, resid Loss: 0.1820 | 0.2736
Epoch 65/300, resid Loss: 0.1806 | 0.2732
Epoch 66/300, resid Loss: 0.1804 | 0.2708
Epoch 67/300, resid Loss: 0.1810 | 0.2684
Epoch 68/300, resid Loss: 0.1789 | 0.2696
Epoch 69/300, resid Loss: 0.1787 | 0.2661
Epoch 70/300, resid Loss: 0.1782 | 0.2636
Epoch 71/300, resid Loss: 0.1774 | 0.2646
Epoch 72/300, resid Loss: 0.1773 | 0.2618
Epoch 73/300, resid Loss: 0.1768 | 0.2599
Epoch 74/300, resid Loss: 0.1766 | 0.2601
Epoch 75/300, resid Loss: 0.1756 | 0.2568
Epoch 76/300, resid Loss: 0.1749 | 0.2584
Epoch 77/300, resid Loss: 0.1748 | 0.2561
Epoch 78/300, resid Loss: 0.1744 | 0.2566
Epoch 79/300, resid Loss: 0.1733 | 0.2536
Epoch 80/300, resid Loss: 0.1731 | 0.2529
Epoch 81/300, resid Loss: 0.1727 | 0.2528
Epoch 82/300, resid Loss: 0.1728 | 0.2516
Epoch 83/300, resid Loss: 0.1722 | 0.2495
Epoch 84/300, resid Loss: 0.1718 | 0.2504
Epoch 85/300, resid Loss: 0.1715 | 0.2468
Epoch 86/300, resid Loss: 0.1712 | 0.2477
Epoch 87/300, resid Loss: 0.1702 | 0.2450
Epoch 88/300, resid Loss: 0.1698 | 0.2452
Epoch 89/300, resid Loss: 0.1702 | 0.2449
Epoch 90/300, resid Loss: 0.1700 | 0.2431
Epoch 91/300, resid Loss: 0.1697 | 0.2429
Epoch 92/300, resid Loss: 0.1682 | 0.2420
Epoch 93/300, resid Loss: 0.1690 | 0.2430
Epoch 94/300, resid Loss: 0.1693 | 0.2407
Epoch 95/300, resid Loss: 0.1682 | 0.2399
Epoch 96/300, resid Loss: 0.1672 | 0.2394
Epoch 97/300, resid Loss: 0.1682 | 0.2393
Epoch 98/300, resid Loss: 0.1671 | 0.2380
Epoch 99/300, resid Loss: 0.1672 | 0.2385
Epoch 100/300, resid Loss: 0.1674 | 0.2364
Epoch 101/300, resid Loss: 0.1675 | 0.2356
Epoch 102/300, resid Loss: 0.1660 | 0.2356
Epoch 103/300, resid Loss: 0.1658 | 0.2354
Epoch 104/300, resid Loss: 0.1658 | 0.2339
Epoch 105/300, resid Loss: 0.1654 | 0.2337
Epoch 106/300, resid Loss: 0.1652 | 0.2334
Epoch 107/300, resid Loss: 0.1653 | 0.2331
Epoch 108/300, resid Loss: 0.1654 | 0.2329
Epoch 109/300, resid Loss: 0.1648 | 0.2321
Epoch 110/300, resid Loss: 0.1649 | 0.2318
Epoch 111/300, resid Loss: 0.1648 | 0.2307
Epoch 112/300, resid Loss: 0.1638 | 0.2304
Epoch 113/300, resid Loss: 0.1632 | 0.2314
Epoch 114/300, resid Loss: 0.1640 | 0.2294
Epoch 115/300, resid Loss: 0.1641 | 0.2293
Epoch 116/300, resid Loss: 0.1633 | 0.2306
Epoch 117/300, resid Loss: 0.1624 | 0.2287
Epoch 118/300, resid Loss: 0.1629 | 0.2294
Epoch 119/300, resid Loss: 0.1617 | 0.2292
Epoch 120/300, resid Loss: 0.1626 | 0.2274
Epoch 121/300, resid Loss: 0.1624 | 0.2283
Epoch 122/300, resid Loss: 0.1625 | 0.2274
Epoch 123/300, resid Loss: 0.1622 | 0.2280
Epoch 124/300, resid Loss: 0.1625 | 0.2267
Epoch 125/300, resid Loss: 0.1614 | 0.2268
Epoch 126/300, resid Loss: 0.1619 | 0.2266
Epoch 127/300, resid Loss: 0.1612 | 0.2247
Epoch 128/300, resid Loss: 0.1607 | 0.2254
Epoch 129/300, resid Loss: 0.1609 | 0.2262
Epoch 130/300, resid Loss: 0.1613 | 0.2247
Epoch 131/300, resid Loss: 0.1610 | 0.2233
Epoch 132/300, resid Loss: 0.1608 | 0.2230
Epoch 133/300, resid Loss: 0.1605 | 0.2224
Epoch 134/300, resid Loss: 0.1604 | 0.2224
Epoch 135/300, resid Loss: 0.1603 | 0.2229
Epoch 136/300, resid Loss: 0.1598 | 0.2230
Epoch 137/300, resid Loss: 0.1601 | 0.2220
Epoch 138/300, resid Loss: 0.1595 | 0.2218
Epoch 139/300, resid Loss: 0.1601 | 0.2212
Epoch 140/300, resid Loss: 0.1591 | 0.2211
Epoch 141/300, resid Loss: 0.1599 | 0.2222
Epoch 142/300, resid Loss: 0.1595 | 0.2213
Epoch 143/300, resid Loss: 0.1589 | 0.2209
Epoch 144/300, resid Loss: 0.1590 | 0.2211
Epoch 145/300, resid Loss: 0.1594 | 0.2214
Epoch 146/300, resid Loss: 0.1592 | 0.2205
Epoch 147/300, resid Loss: 0.1588 | 0.2199
Epoch 148/300, resid Loss: 0.1586 | 0.2198
Epoch 149/300, resid Loss: 0.1585 | 0.2195
Epoch 150/300, resid Loss: 0.1592 | 0.2184
Epoch 151/300, resid Loss: 0.1585 | 0.2185
Epoch 152/300, resid Loss: 0.1588 | 0.2180
Epoch 153/300, resid Loss: 0.1583 | 0.2188
Epoch 154/300, resid Loss: 0.1581 | 0.2186
Epoch 155/300, resid Loss: 0.1581 | 0.2184
Epoch 156/300, resid Loss: 0.1580 | 0.2180
Epoch 157/300, resid Loss: 0.1575 | 0.2178
Epoch 158/300, resid Loss: 0.1584 | 0.2173
Epoch 159/300, resid Loss: 0.1575 | 0.2172
Epoch 160/300, resid Loss: 0.1583 | 0.2166
Epoch 161/300, resid Loss: 0.1581 | 0.2166
Epoch 162/300, resid Loss: 0.1568 | 0.2167
Epoch 163/300, resid Loss: 0.1574 | 0.2167
Epoch 164/300, resid Loss: 0.1577 | 0.2165
Epoch 165/300, resid Loss: 0.1570 | 0.2164
Epoch 166/300, resid Loss: 0.1569 | 0.2168
Epoch 167/300, resid Loss: 0.1578 | 0.2166
Epoch 168/300, resid Loss: 0.1572 | 0.2160
Epoch 169/300, resid Loss: 0.1573 | 0.2159
Epoch 170/300, resid Loss: 0.1567 | 0.2160
Epoch 171/300, resid Loss: 0.1573 | 0.2159
Epoch 172/300, resid Loss: 0.1565 | 0.2157
Epoch 173/300, resid Loss: 0.1570 | 0.2156
Epoch 174/300, resid Loss: 0.1567 | 0.2156
Epoch 175/300, resid Loss: 0.1571 | 0.2152
Epoch 176/300, resid Loss: 0.1566 | 0.2149
Epoch 177/300, resid Loss: 0.1565 | 0.2145
Epoch 178/300, resid Loss: 0.1555 | 0.2149
Epoch 179/300, resid Loss: 0.1561 | 0.2153
Epoch 180/300, resid Loss: 0.1568 | 0.2149
Epoch 181/300, resid Loss: 0.1560 | 0.2146
Epoch 182/300, resid Loss: 0.1564 | 0.2144
Epoch 183/300, resid Loss: 0.1566 | 0.2144
Epoch 184/300, resid Loss: 0.1561 | 0.2143
Epoch 185/300, resid Loss: 0.1565 | 0.2142
Epoch 186/300, resid Loss: 0.1558 | 0.2142
Epoch 187/300, resid Loss: 0.1561 | 0.2141
Epoch 188/300, resid Loss: 0.1566 | 0.2138
Epoch 189/300, resid Loss: 0.1557 | 0.2136
Epoch 190/300, resid Loss: 0.1558 | 0.2134
Epoch 191/300, resid Loss: 0.1564 | 0.2131
Epoch 192/300, resid Loss: 0.1553 | 0.2132
Epoch 193/300, resid Loss: 0.1555 | 0.2133
Epoch 194/300, resid Loss: 0.1560 | 0.2129
Epoch 195/300, resid Loss: 0.1554 | 0.2123
Epoch 196/300, resid Loss: 0.1551 | 0.2125
Epoch 197/300, resid Loss: 0.1545 | 0.2125
Epoch 198/300, resid Loss: 0.1554 | 0.2126
Epoch 199/300, resid Loss: 0.1558 | 0.2126
Epoch 200/300, resid Loss: 0.1552 | 0.2127
Epoch 201/300, resid Loss: 0.1558 | 0.2127
Epoch 202/300, resid Loss: 0.1557 | 0.2126
Epoch 203/300, resid Loss: 0.1559 | 0.2122
Epoch 204/300, resid Loss: 0.1552 | 0.2124
Epoch 205/300, resid Loss: 0.1556 | 0.2126
Epoch 206/300, resid Loss: 0.1551 | 0.2125
Epoch 207/300, resid Loss: 0.1554 | 0.2125
Epoch 208/300, resid Loss: 0.1553 | 0.2125
Epoch 209/300, resid Loss: 0.1556 | 0.2123
Epoch 210/300, resid Loss: 0.1552 | 0.2123
Epoch 211/300, resid Loss: 0.1556 | 0.2120
Epoch 212/300, resid Loss: 0.1549 | 0.2120
Epoch 213/300, resid Loss: 0.1549 | 0.2121
Epoch 214/300, resid Loss: 0.1557 | 0.2122
Epoch 215/300, resid Loss: 0.1550 | 0.2120
Epoch 216/300, resid Loss: 0.1546 | 0.2118
Epoch 217/300, resid Loss: 0.1548 | 0.2118
Epoch 218/300, resid Loss: 0.1546 | 0.2118
Epoch 219/300, resid Loss: 0.1547 | 0.2121
Epoch 220/300, resid Loss: 0.1547 | 0.2123
Epoch 221/300, resid Loss: 0.1546 | 0.2122
Epoch 222/300, resid Loss: 0.1547 | 0.2120
Epoch 223/300, resid Loss: 0.1546 | 0.2119
Epoch 224/300, resid Loss: 0.1546 | 0.2119
Epoch 225/300, resid Loss: 0.1552 | 0.2118
Epoch 226/300, resid Loss: 0.1548 | 0.2115
Epoch 227/300, resid Loss: 0.1545 | 0.2112
Epoch 228/300, resid Loss: 0.1547 | 0.2115
Epoch 229/300, resid Loss: 0.1547 | 0.2116
Epoch 230/300, resid Loss: 0.1541 | 0.2116
Epoch 231/300, resid Loss: 0.1545 | 0.2116
Epoch 232/300, resid Loss: 0.1548 | 0.2116
Epoch 233/300, resid Loss: 0.1545 | 0.2113
Epoch 234/300, resid Loss: 0.1547 | 0.2109
Epoch 235/300, resid Loss: 0.1553 | 0.2108
Epoch 236/300, resid Loss: 0.1545 | 0.2107
Epoch 237/300, resid Loss: 0.1537 | 0.2108
Epoch 238/300, resid Loss: 0.1548 | 0.2108
Epoch 239/300, resid Loss: 0.1549 | 0.2109
Epoch 240/300, resid Loss: 0.1540 | 0.2110
Epoch 241/300, resid Loss: 0.1545 | 0.2111
Epoch 242/300, resid Loss: 0.1550 | 0.2109
Epoch 243/300, resid Loss: 0.1543 | 0.2109
Epoch 244/300, resid Loss: 0.1541 | 0.2108
Epoch 245/300, resid Loss: 0.1544 | 0.2108
Epoch 246/300, resid Loss: 0.1544 | 0.2107
Epoch 247/300, resid Loss: 0.1543 | 0.2105
Epoch 248/300, resid Loss: 0.1544 | 0.2105
Epoch 249/300, resid Loss: 0.1542 | 0.2104
Epoch 250/300, resid Loss: 0.1553 | 0.2105
Epoch 251/300, resid Loss: 0.1550 | 0.2106
Epoch 252/300, resid Loss: 0.1541 | 0.2107
Epoch 253/300, resid Loss: 0.1543 | 0.2106
Epoch 254/300, resid Loss: 0.1537 | 0.2106
Epoch 255/300, resid Loss: 0.1554 | 0.2105
Epoch 256/300, resid Loss: 0.1545 | 0.2104
Epoch 257/300, resid Loss: 0.1539 | 0.2103
Epoch 258/300, resid Loss: 0.1542 | 0.2103
Epoch 259/300, resid Loss: 0.1545 | 0.2102
Epoch 260/300, resid Loss: 0.1542 | 0.2101
Epoch 261/300, resid Loss: 0.1542 | 0.2100
Epoch 262/300, resid Loss: 0.1536 | 0.2099
Epoch 263/300, resid Loss: 0.1541 | 0.2099
Epoch 264/300, resid Loss: 0.1545 | 0.2101
Epoch 265/300, resid Loss: 0.1545 | 0.2100
Epoch 266/300, resid Loss: 0.1543 | 0.2100
Epoch 267/300, resid Loss: 0.1539 | 0.2101
Epoch 268/300, resid Loss: 0.1543 | 0.2101
Epoch 269/300, resid Loss: 0.1548 | 0.2101
Epoch 270/300, resid Loss: 0.1539 | 0.2100
Epoch 271/300, resid Loss: 0.1542 | 0.2099
Epoch 272/300, resid Loss: 0.1540 | 0.2099
Epoch 273/300, resid Loss: 0.1538 | 0.2099
Epoch 274/300, resid Loss: 0.1544 | 0.2098
Epoch 275/300, resid Loss: 0.1539 | 0.2098
Epoch 276/300, resid Loss: 0.1534 | 0.2098
Epoch 277/300, resid Loss: 0.1534 | 0.2098
Epoch 278/300, resid Loss: 0.1542 | 0.2098
Epoch 279/300, resid Loss: 0.1534 | 0.2098
Epoch 280/300, resid Loss: 0.1545 | 0.2097
Epoch 281/300, resid Loss: 0.1546 | 0.2097
Epoch 282/300, resid Loss: 0.1541 | 0.2097
Epoch 283/300, resid Loss: 0.1534 | 0.2098
Epoch 284/300, resid Loss: 0.1539 | 0.2098
Epoch 285/300, resid Loss: 0.1544 | 0.2098
Epoch 286/300, resid Loss: 0.1543 | 0.2097
Epoch 287/300, resid Loss: 0.1543 | 0.2097
Epoch 288/300, resid Loss: 0.1539 | 0.2097
Epoch 289/300, resid Loss: 0.1541 | 0.2097
Epoch 290/300, resid Loss: 0.1537 | 0.2097
Epoch 291/300, resid Loss: 0.1537 | 0.2097
Epoch 292/300, resid Loss: 0.1536 | 0.2098
Epoch 293/300, resid Loss: 0.1541 | 0.2098
Epoch 294/300, resid Loss: 0.1538 | 0.2097
Epoch 295/300, resid Loss: 0.1526 | 0.2097
Epoch 296/300, resid Loss: 0.1539 | 0.2097
Epoch 297/300, resid Loss: 0.1537 | 0.2097
Epoch 298/300, resid Loss: 0.1539 | 0.2097
Epoch 299/300, resid Loss: 0.1538 | 0.2096
Epoch 300/300, resid Loss: 0.1534 | 0.2096
Runtime (seconds): 2426.5556104183197
4.66069714817907e-05
[143.06335]
[-1.2343184]
[-5.757511]
[10.531801]
[1.0192488]
[5.030734]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 405.2441270686686
RMSE: 20.13067626953125
MAE: 20.13067626953125
R-squared: nan
[152.6533]
