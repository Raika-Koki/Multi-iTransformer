ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 02:43:54,439][0m A new study created in memory with name: no-name-311197ff-9fb9-496f-ba28-4dac3b0527e7[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-03 02:44:19,984][0m Trial 0 finished with value: 0.21032116793083017 and parameters: {'observation_period_num': 183, 'train_rates': 0.7938803966100396, 'learning_rate': 2.312010618561586e-05, 'batch_size': 222, 'step_size': 4, 'gamma': 0.9642194469803012}. Best is trial 0 with value: 0.21032116793083017.[0m
[32m[I 2025-01-03 02:45:03,865][0m Trial 1 finished with value: 0.06317059700362779 and parameters: {'observation_period_num': 114, 'train_rates': 0.8868672803755167, 'learning_rate': 0.00017311149125374476, 'batch_size': 127, 'step_size': 14, 'gamma': 0.826898172426274}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:45:34,952][0m Trial 2 finished with value: 0.371770845706387 and parameters: {'observation_period_num': 13, 'train_rates': 0.6147482989885472, 'learning_rate': 4.363465593667764e-06, 'batch_size': 152, 'step_size': 13, 'gamma': 0.7667065512648343}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:46:03,498][0m Trial 3 finished with value: 0.7941872281634548 and parameters: {'observation_period_num': 125, 'train_rates': 0.773021291486405, 'learning_rate': 5.48107991622808e-06, 'batch_size': 182, 'step_size': 4, 'gamma': 0.794630311285549}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:46:41,181][0m Trial 4 finished with value: 0.6776317853632857 and parameters: {'observation_period_num': 57, 'train_rates': 0.653317699318492, 'learning_rate': 1.9702190589467894e-06, 'batch_size': 126, 'step_size': 11, 'gamma': 0.9454072244714691}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:47:00,042][0m Trial 5 finished with value: 0.6695923449739688 and parameters: {'observation_period_num': 208, 'train_rates': 0.6124927709322591, 'learning_rate': 7.445001876083235e-06, 'batch_size': 243, 'step_size': 3, 'gamma': 0.9161105578330857}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:48:17,596][0m Trial 6 finished with value: 0.07011302672326565 and parameters: {'observation_period_num': 138, 'train_rates': 0.9076833089575918, 'learning_rate': 0.0001416058016089083, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8622104262515415}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:48:57,965][0m Trial 7 finished with value: 0.07990754190948068 and parameters: {'observation_period_num': 99, 'train_rates': 0.8871792937810068, 'learning_rate': 0.00038825994069111816, 'batch_size': 145, 'step_size': 15, 'gamma': 0.8781879284059655}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:50:54,244][0m Trial 8 finished with value: 0.2246588649258258 and parameters: {'observation_period_num': 79, 'train_rates': 0.6546197985929536, 'learning_rate': 1.2804477589885116e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9474993901826962}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:51:31,274][0m Trial 9 finished with value: 0.6465755520909545 and parameters: {'observation_period_num': 211, 'train_rates': 0.8866354120139821, 'learning_rate': 1.8681373374462654e-06, 'batch_size': 155, 'step_size': 2, 'gamma': 0.9404705575203639}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:52:35,052][0m Trial 10 finished with value: 0.09294984489679337 and parameters: {'observation_period_num': 251, 'train_rates': 0.9815793482516578, 'learning_rate': 8.712773880593782e-05, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8257739193764893}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:53:56,396][0m Trial 11 finished with value: 0.0691944238452511 and parameters: {'observation_period_num': 153, 'train_rates': 0.9064982492083092, 'learning_rate': 0.00012149399561860252, 'batch_size': 67, 'step_size': 8, 'gamma': 0.8491653028591508}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:57:39,422][0m Trial 12 finished with value: 0.12636384375176382 and parameters: {'observation_period_num': 150, 'train_rates': 0.9633418030474918, 'learning_rate': 0.000918928912775633, 'batch_size': 25, 'step_size': 8, 'gamma': 0.8393316434211794}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:58:36,991][0m Trial 13 finished with value: 0.08378371865385109 and parameters: {'observation_period_num': 167, 'train_rates': 0.8448091707398393, 'learning_rate': 5.354325957005685e-05, 'batch_size': 92, 'step_size': 10, 'gamma': 0.7972896838284049}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 02:59:23,650][0m Trial 14 finished with value: 0.22354716700423696 and parameters: {'observation_period_num': 108, 'train_rates': 0.7481009741501488, 'learning_rate': 0.00021438984959991608, 'batch_size': 109, 'step_size': 7, 'gamma': 0.8923072070565436}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:00:59,297][0m Trial 15 finished with value: 0.07914831586614517 and parameters: {'observation_period_num': 43, 'train_rates': 0.8458935280227646, 'learning_rate': 0.00036037712117928925, 'batch_size': 57, 'step_size': 12, 'gamma': 0.8363216254958582}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:01:32,508][0m Trial 16 finished with value: 0.10002627223730087 and parameters: {'observation_period_num': 82, 'train_rates': 0.9435976981523698, 'learning_rate': 3.035828408950076e-05, 'batch_size': 186, 'step_size': 15, 'gamma': 0.7588158991643034}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:03:02,342][0m Trial 17 finished with value: 0.10296416852047773 and parameters: {'observation_period_num': 182, 'train_rates': 0.8223513301410124, 'learning_rate': 0.0008223332787675262, 'batch_size': 56, 'step_size': 6, 'gamma': 0.8053642926273553}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:03:54,664][0m Trial 18 finished with value: 0.08338854803678093 and parameters: {'observation_period_num': 126, 'train_rates': 0.9230990016368394, 'learning_rate': 6.671655257860436e-05, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9882969541932487}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:04:23,586][0m Trial 19 finished with value: 0.2701250620634924 and parameters: {'observation_period_num': 157, 'train_rates': 0.726758159768609, 'learning_rate': 0.0001728762195190862, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8621866103975958}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:09:21,348][0m Trial 20 finished with value: 0.07951953803194302 and parameters: {'observation_period_num': 212, 'train_rates': 0.8696092193122725, 'learning_rate': 0.0002954373160373774, 'batch_size': 17, 'step_size': 1, 'gamma': 0.8979476215271263}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:10:42,209][0m Trial 21 finished with value: 0.07302526921726936 and parameters: {'observation_period_num': 141, 'train_rates': 0.9235066991186317, 'learning_rate': 0.00010226786986749046, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8614265804538397}. Best is trial 1 with value: 0.06317059700362779.[0m
[32m[I 2025-01-03 03:11:53,191][0m Trial 22 finished with value: 0.06015927644939762 and parameters: {'observation_period_num': 115, 'train_rates': 0.9108735367486148, 'learning_rate': 0.00014042401095462989, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8516375920902725}. Best is trial 22 with value: 0.06015927644939762.[0m
[32m[I 2025-01-03 03:12:59,575][0m Trial 23 finished with value: 0.11086701601743698 and parameters: {'observation_period_num': 107, 'train_rates': 0.9894683045301185, 'learning_rate': 0.0005226052678590838, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8178085878248935}. Best is trial 22 with value: 0.06015927644939762.[0m
[32m[I 2025-01-03 03:13:49,790][0m Trial 24 finished with value: 0.075824903396817 and parameters: {'observation_period_num': 69, 'train_rates': 0.946769633427864, 'learning_rate': 4.710983508467268e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8499227635626416}. Best is trial 22 with value: 0.06015927644939762.[0m
[32m[I 2025-01-03 03:15:51,006][0m Trial 25 finished with value: 0.0707606222918805 and parameters: {'observation_period_num': 97, 'train_rates': 0.8572097487766243, 'learning_rate': 0.00012926077866258, 'batch_size': 44, 'step_size': 10, 'gamma': 0.7835096169187749}. Best is trial 22 with value: 0.06015927644939762.[0m
[32m[I 2025-01-03 03:16:59,897][0m Trial 26 finished with value: 0.038013290114905854 and parameters: {'observation_period_num': 31, 'train_rates': 0.813395532284418, 'learning_rate': 0.00024036974089930647, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8170662167102746}. Best is trial 26 with value: 0.038013290114905854.[0m
[32m[I 2025-01-03 03:17:53,842][0m Trial 27 finished with value: 0.04170140458557468 and parameters: {'observation_period_num': 32, 'train_rates': 0.8219323786495841, 'learning_rate': 0.00025818896582871525, 'batch_size': 103, 'step_size': 14, 'gamma': 0.8159541442280817}. Best is trial 26 with value: 0.038013290114905854.[0m
[32m[I 2025-01-03 03:18:48,674][0m Trial 28 finished with value: 0.030980236489664425 and parameters: {'observation_period_num': 15, 'train_rates': 0.8091529666117765, 'learning_rate': 0.0006419719121738479, 'batch_size': 102, 'step_size': 11, 'gamma': 0.7731199416715583}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:19:40,543][0m Trial 29 finished with value: 0.03492200490389321 and parameters: {'observation_period_num': 10, 'train_rates': 0.7989178365060802, 'learning_rate': 0.0005236937655830456, 'batch_size': 104, 'step_size': 12, 'gamma': 0.7732835695729481}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:20:15,053][0m Trial 30 finished with value: 0.169088500435836 and parameters: {'observation_period_num': 7, 'train_rates': 0.7875803407270512, 'learning_rate': 0.0005926700924794401, 'batch_size': 163, 'step_size': 11, 'gamma': 0.7773570149263133}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:21:07,398][0m Trial 31 finished with value: 0.040429253000800344 and parameters: {'observation_period_num': 30, 'train_rates': 0.8142372349777623, 'learning_rate': 0.0002689095332599889, 'batch_size': 106, 'step_size': 13, 'gamma': 0.7548481091120101}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:21:53,768][0m Trial 32 finished with value: 0.16518220092423475 and parameters: {'observation_period_num': 25, 'train_rates': 0.7504031962571532, 'learning_rate': 0.0005802943761719913, 'batch_size': 111, 'step_size': 12, 'gamma': 0.7502843962916131}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:22:36,364][0m Trial 33 finished with value: 0.05374991848404997 and parameters: {'observation_period_num': 49, 'train_rates': 0.8134420106862439, 'learning_rate': 0.0009708291582589793, 'batch_size': 134, 'step_size': 13, 'gamma': 0.7727432700949031}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:23:26,455][0m Trial 34 finished with value: 0.1531815610601027 and parameters: {'observation_period_num': 23, 'train_rates': 0.7060342319940119, 'learning_rate': 0.00042768744178980905, 'batch_size': 101, 'step_size': 12, 'gamma': 0.7860409762155287}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:24:30,215][0m Trial 35 finished with value: 0.04615818346674378 and parameters: {'observation_period_num': 36, 'train_rates': 0.7970252291057335, 'learning_rate': 0.0002537450545097691, 'batch_size': 84, 'step_size': 11, 'gamma': 0.7628864569477855}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:25:15,103][0m Trial 36 finished with value: 0.16408783612937172 and parameters: {'observation_period_num': 7, 'train_rates': 0.7691947316034213, 'learning_rate': 0.0005913001070731606, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8040367666663045}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:25:54,179][0m Trial 37 finished with value: 0.058310366804185124 and parameters: {'observation_period_num': 56, 'train_rates': 0.8013655595698865, 'learning_rate': 0.00019539426621227701, 'batch_size': 144, 'step_size': 13, 'gamma': 0.752115989188366}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:26:37,751][0m Trial 38 finished with value: 0.14264387800115422 and parameters: {'observation_period_num': 14, 'train_rates': 0.7062011610325671, 'learning_rate': 0.0003507969327139474, 'batch_size': 119, 'step_size': 10, 'gamma': 0.7899077754087386}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:26:59,865][0m Trial 39 finished with value: 0.37997850726123295 and parameters: {'observation_period_num': 70, 'train_rates': 0.7707672070951576, 'learning_rate': 1.6547448428080373e-05, 'batch_size': 250, 'step_size': 12, 'gamma': 0.763351144513773}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:27:43,642][0m Trial 40 finished with value: 0.031710109294728374 and parameters: {'observation_period_num': 22, 'train_rates': 0.8356849639356, 'learning_rate': 0.0007161110278545574, 'batch_size': 133, 'step_size': 11, 'gamma': 0.772805791746901}. Best is trial 28 with value: 0.030980236489664425.[0m
[32m[I 2025-01-03 03:28:24,489][0m Trial 41 finished with value: 0.030444969553500414 and parameters: {'observation_period_num': 20, 'train_rates': 0.8294365016459776, 'learning_rate': 0.0006990606550326469, 'batch_size': 139, 'step_size': 11, 'gamma': 0.7758365335704978}. Best is trial 41 with value: 0.030444969553500414.[0m
[32m[I 2025-01-03 03:28:58,490][0m Trial 42 finished with value: 0.030121251924942107 and parameters: {'observation_period_num': 19, 'train_rates': 0.8390281360342446, 'learning_rate': 0.0006221488770577347, 'batch_size': 169, 'step_size': 11, 'gamma': 0.7807568611860005}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:29:32,691][0m Trial 43 finished with value: 0.032830277967309536 and parameters: {'observation_period_num': 16, 'train_rates': 0.8444118728387892, 'learning_rate': 0.0007261193798954294, 'batch_size': 166, 'step_size': 9, 'gamma': 0.7729089317351326}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:30:02,429][0m Trial 44 finished with value: 0.03236190987598596 and parameters: {'observation_period_num': 19, 'train_rates': 0.8375143288905863, 'learning_rate': 0.0007236009194192306, 'batch_size': 202, 'step_size': 9, 'gamma': 0.797698571780495}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:30:29,566][0m Trial 45 finished with value: 1.1707005696442292 and parameters: {'observation_period_num': 43, 'train_rates': 0.864919563925945, 'learning_rate': 1.0278079201819711e-06, 'batch_size': 213, 'step_size': 10, 'gamma': 0.7966625997115152}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:30:59,006][0m Trial 46 finished with value: 0.053567141090353876 and parameters: {'observation_period_num': 59, 'train_rates': 0.8738131297598092, 'learning_rate': 0.0007406723087611745, 'batch_size': 195, 'step_size': 9, 'gamma': 0.7816102345918213}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:31:24,850][0m Trial 47 finished with value: 0.03786806149273803 and parameters: {'observation_period_num': 21, 'train_rates': 0.8344714063153202, 'learning_rate': 0.0004061958158673928, 'batch_size': 233, 'step_size': 11, 'gamma': 0.809426720361378}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:31:53,834][0m Trial 48 finished with value: 0.052952435367354535 and parameters: {'observation_period_num': 43, 'train_rates': 0.887261240524866, 'learning_rate': 0.0009956485866488775, 'batch_size': 214, 'step_size': 10, 'gamma': 0.7921546167209598}. Best is trial 42 with value: 0.030121251924942107.[0m
[32m[I 2025-01-03 03:32:28,179][0m Trial 49 finished with value: 0.1302058577352429 and parameters: {'observation_period_num': 5, 'train_rates': 0.8362869119572468, 'learning_rate': 4.398082002388428e-06, 'batch_size': 173, 'step_size': 9, 'gamma': 0.8309987555447842}. Best is trial 42 with value: 0.030121251924942107.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 03:32:28,189][0m A new study created in memory with name: no-name-0d1c9269-638e-4ce5-be65-f58c6bfecbf5[0m
[32m[I 2025-01-03 03:33:59,895][0m Trial 0 finished with value: 0.2602951258039949 and parameters: {'observation_period_num': 69, 'train_rates': 0.6335716399300945, 'learning_rate': 6.761913884281209e-06, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9836321282639295}. Best is trial 0 with value: 0.2602951258039949.[0m
[32m[I 2025-01-03 03:34:19,610][0m Trial 1 finished with value: 0.30980767311418755 and parameters: {'observation_period_num': 165, 'train_rates': 0.6949565687754264, 'learning_rate': 0.0003789839072382517, 'batch_size': 253, 'step_size': 2, 'gamma': 0.847459391853769}. Best is trial 0 with value: 0.2602951258039949.[0m
[32m[I 2025-01-03 03:35:01,653][0m Trial 2 finished with value: 0.2597081680660662 and parameters: {'observation_period_num': 96, 'train_rates': 0.927660191845992, 'learning_rate': 5.844236774368747e-06, 'batch_size': 145, 'step_size': 9, 'gamma': 0.9153032220572264}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:35:33,763][0m Trial 3 finished with value: 1.0138617975767268 and parameters: {'observation_period_num': 218, 'train_rates': 0.7957336041508319, 'learning_rate': 1.0916198390809276e-06, 'batch_size': 156, 'step_size': 9, 'gamma': 0.85689563480579}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:36:41,572][0m Trial 4 finished with value: 0.35864434411345153 and parameters: {'observation_period_num': 199, 'train_rates': 0.7170000355100549, 'learning_rate': 0.00012050797467202158, 'batch_size': 67, 'step_size': 2, 'gamma': 0.8172386217165605}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:37:12,356][0m Trial 5 finished with value: 0.2875807583332062 and parameters: {'observation_period_num': 247, 'train_rates': 0.9837959793445007, 'learning_rate': 1.0243009416947316e-05, 'batch_size': 202, 'step_size': 9, 'gamma': 0.9326206025025932}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:37:45,199][0m Trial 6 finished with value: 0.3020923300763177 and parameters: {'observation_period_num': 218, 'train_rates': 0.7524493837546861, 'learning_rate': 0.0001911031998835419, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9769833509846066}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:38:38,406][0m Trial 7 finished with value: 0.3710560032245626 and parameters: {'observation_period_num': 134, 'train_rates': 0.7201444359259949, 'learning_rate': 8.257667740172482e-06, 'batch_size': 92, 'step_size': 5, 'gamma': 0.9857044638153563}. Best is trial 2 with value: 0.2597081680660662.[0m
[32m[I 2025-01-03 03:39:30,072][0m Trial 8 finished with value: 0.1076255710491168 and parameters: {'observation_period_num': 103, 'train_rates': 0.9463949248733902, 'learning_rate': 0.0006552846618819995, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8179419894380523}. Best is trial 8 with value: 0.1076255710491168.[0m
[32m[I 2025-01-03 03:39:54,677][0m Trial 9 finished with value: 0.08577731251716614 and parameters: {'observation_period_num': 109, 'train_rates': 0.9695879319055012, 'learning_rate': 0.00014149214107995014, 'batch_size': 252, 'step_size': 13, 'gamma': 0.9460839806447232}. Best is trial 9 with value: 0.08577731251716614.[0m
[32m[I 2025-01-03 03:40:19,892][0m Trial 10 finished with value: 0.06285765628997467 and parameters: {'observation_period_num': 9, 'train_rates': 0.8516802524382219, 'learning_rate': 4.7036396914654676e-05, 'batch_size': 242, 'step_size': 12, 'gamma': 0.7552691284958017}. Best is trial 10 with value: 0.06285765628997467.[0m
[32m[I 2025-01-03 03:40:43,298][0m Trial 11 finished with value: 0.06394560242352902 and parameters: {'observation_period_num': 29, 'train_rates': 0.8548511290091644, 'learning_rate': 5.109135529903264e-05, 'batch_size': 251, 'step_size': 12, 'gamma': 0.7563097746342566}. Best is trial 10 with value: 0.06285765628997467.[0m
[32m[I 2025-01-03 03:41:13,034][0m Trial 12 finished with value: 0.061657105417664775 and parameters: {'observation_period_num': 12, 'train_rates': 0.8549182506835016, 'learning_rate': 4.734985713645979e-05, 'batch_size': 205, 'step_size': 11, 'gamma': 0.7545190069284657}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:41:43,068][0m Trial 13 finished with value: 0.06417101187613404 and parameters: {'observation_period_num': 5, 'train_rates': 0.8602444907104813, 'learning_rate': 4.2023550367533103e-05, 'batch_size': 200, 'step_size': 11, 'gamma': 0.7524771717343441}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:42:12,473][0m Trial 14 finished with value: 0.2037533316268492 and parameters: {'observation_period_num': 47, 'train_rates': 0.8700121336103981, 'learning_rate': 1.8941673940139854e-05, 'batch_size': 199, 'step_size': 6, 'gamma': 0.7827634815009121}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:42:38,545][0m Trial 15 finished with value: 0.0661678284406662 and parameters: {'observation_period_num': 7, 'train_rates': 0.8148880401394618, 'learning_rate': 5.5049857401209175e-05, 'batch_size': 218, 'step_size': 11, 'gamma': 0.7914615351476916}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:43:11,791][0m Trial 16 finished with value: 0.10215765109560529 and parameters: {'observation_period_num': 68, 'train_rates': 0.9153828766769585, 'learning_rate': 2.3320515781347395e-05, 'batch_size': 177, 'step_size': 15, 'gamma': 0.88983693889613}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:43:38,940][0m Trial 17 finished with value: 0.5487630113241184 and parameters: {'observation_period_num': 40, 'train_rates': 0.8860436872999746, 'learning_rate': 2.635274155017169e-06, 'batch_size': 225, 'step_size': 7, 'gamma': 0.78506308311519}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:44:11,390][0m Trial 18 finished with value: 0.0731670458156329 and parameters: {'observation_period_num': 70, 'train_rates': 0.8196609766299396, 'learning_rate': 7.756349612004055e-05, 'batch_size': 170, 'step_size': 11, 'gamma': 0.8209282167534014}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:44:53,292][0m Trial 19 finished with value: 0.25711481240526984 and parameters: {'observation_period_num': 154, 'train_rates': 0.7715427436485258, 'learning_rate': 0.00028017919684357245, 'batch_size': 124, 'step_size': 4, 'gamma': 0.7515370873740006}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:45:14,467][0m Trial 20 finished with value: 0.27211643680164666 and parameters: {'observation_period_num': 27, 'train_rates': 0.6007319095846115, 'learning_rate': 1.788598581614294e-05, 'batch_size': 229, 'step_size': 10, 'gamma': 0.8854882632624778}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:45:37,599][0m Trial 21 finished with value: 0.06468958061960367 and parameters: {'observation_period_num': 28, 'train_rates': 0.8482025465133801, 'learning_rate': 7.396424280111602e-05, 'batch_size': 254, 'step_size': 12, 'gamma': 0.7687544661280369}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:46:04,668][0m Trial 22 finished with value: 0.0866367875360975 and parameters: {'observation_period_num': 54, 'train_rates': 0.8943851595673723, 'learning_rate': 3.529874861943198e-05, 'batch_size': 233, 'step_size': 12, 'gamma': 0.7984583461178344}. Best is trial 12 with value: 0.061657105417664775.[0m
[32m[I 2025-01-03 03:46:36,868][0m Trial 23 finished with value: 0.05714358826023821 and parameters: {'observation_period_num': 19, 'train_rates': 0.8434659087601374, 'learning_rate': 7.737709119709128e-05, 'batch_size': 184, 'step_size': 15, 'gamma': 0.7678255518246604}. Best is trial 23 with value: 0.05714358826023821.[0m
[32m[I 2025-01-03 03:47:09,458][0m Trial 24 finished with value: 0.0543997864048165 and parameters: {'observation_period_num': 8, 'train_rates': 0.8308294898601657, 'learning_rate': 0.00010481329719373199, 'batch_size': 178, 'step_size': 15, 'gamma': 0.7724986128926669}. Best is trial 24 with value: 0.0543997864048165.[0m
[32m[I 2025-01-03 03:47:41,414][0m Trial 25 finished with value: 0.06712224943301648 and parameters: {'observation_period_num': 86, 'train_rates': 0.8312133584469936, 'learning_rate': 0.00010201612235806862, 'batch_size': 175, 'step_size': 15, 'gamma': 0.8389962511973184}. Best is trial 24 with value: 0.0543997864048165.[0m
[32m[I 2025-01-03 03:52:36,893][0m Trial 26 finished with value: 0.1843487497695284 and parameters: {'observation_period_num': 19, 'train_rates': 0.7837436592050149, 'learning_rate': 0.0002618236323643053, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7741767663085373}. Best is trial 24 with value: 0.0543997864048165.[0m
[32m[I 2025-01-03 03:53:08,738][0m Trial 27 finished with value: 0.0626172716685274 and parameters: {'observation_period_num': 49, 'train_rates': 0.890977611278215, 'learning_rate': 0.0008298015359122881, 'batch_size': 187, 'step_size': 14, 'gamma': 0.8138364976706285}. Best is trial 24 with value: 0.0543997864048165.[0m
[32m[I 2025-01-03 03:53:52,422][0m Trial 28 finished with value: 0.0463013803847007 and parameters: {'observation_period_num': 34, 'train_rates': 0.7993267289735965, 'learning_rate': 0.0004036401312088971, 'batch_size': 128, 'step_size': 13, 'gamma': 0.802404959975063}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:54:33,737][0m Trial 29 finished with value: 0.2142499356460951 and parameters: {'observation_period_num': 74, 'train_rates': 0.749220142180276, 'learning_rate': 0.0005636758153239725, 'batch_size': 125, 'step_size': 13, 'gamma': 0.7966859381937201}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:55:23,317][0m Trial 30 finished with value: 0.16025037211015394 and parameters: {'observation_period_num': 38, 'train_rates': 0.6762797125203743, 'learning_rate': 0.0004372601750568385, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8329694401128963}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:55:57,506][0m Trial 31 finished with value: 0.04901705980684295 and parameters: {'observation_period_num': 22, 'train_rates': 0.8011949037440066, 'learning_rate': 0.00018329752372556494, 'batch_size': 161, 'step_size': 15, 'gamma': 0.7702402536921031}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:56:31,907][0m Trial 32 finished with value: 0.0593041204124117 and parameters: {'observation_period_num': 59, 'train_rates': 0.8020421829606934, 'learning_rate': 0.00019427885174596422, 'batch_size': 163, 'step_size': 15, 'gamma': 0.7708167492436977}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:57:11,676][0m Trial 33 finished with value: 0.17268051952123642 and parameters: {'observation_period_num': 29, 'train_rates': 0.7604031420582281, 'learning_rate': 0.00035752556769497526, 'batch_size': 133, 'step_size': 13, 'gamma': 0.8068391864492003}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:57:49,865][0m Trial 34 finished with value: 0.07738874196188479 and parameters: {'observation_period_num': 124, 'train_rates': 0.8262233610237295, 'learning_rate': 0.00015832784550316213, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7773291469615863}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:58:20,405][0m Trial 35 finished with value: 0.1095855419112052 and parameters: {'observation_period_num': 87, 'train_rates': 0.7971905869889784, 'learning_rate': 0.0009737999566469644, 'batch_size': 185, 'step_size': 14, 'gamma': 0.8713674243302023}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:59:12,332][0m Trial 36 finished with value: 0.19266518806865054 and parameters: {'observation_period_num': 38, 'train_rates': 0.7828632715358845, 'learning_rate': 9.831289010997729e-05, 'batch_size': 103, 'step_size': 13, 'gamma': 0.801448097425871}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 03:59:43,531][0m Trial 37 finished with value: 0.27928896812568926 and parameters: {'observation_period_num': 186, 'train_rates': 0.727405608956528, 'learning_rate': 0.0002379028269999546, 'batch_size': 157, 'step_size': 15, 'gamma': 0.851522943154813}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 04:00:39,590][0m Trial 38 finished with value: 0.16723429901687892 and parameters: {'observation_period_num': 58, 'train_rates': 0.6911192082201028, 'learning_rate': 0.00040736033724696056, 'batch_size': 85, 'step_size': 14, 'gamma': 0.7708538992395432}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 04:01:21,718][0m Trial 39 finished with value: 0.06813017816520205 and parameters: {'observation_period_num': 18, 'train_rates': 0.9133337516387356, 'learning_rate': 7.271685153742589e-05, 'batch_size': 142, 'step_size': 3, 'gamma': 0.8276299151974219}. Best is trial 28 with value: 0.0463013803847007.[0m
Early stopping at epoch 71
[32m[I 2025-01-03 04:02:14,484][0m Trial 40 finished with value: 0.25095368404897095 and parameters: {'observation_period_num': 25, 'train_rates': 0.7389517902932325, 'learning_rate': 0.00013431170405817674, 'batch_size': 69, 'step_size': 1, 'gamma': 0.763913498706569}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 04:02:47,082][0m Trial 41 finished with value: 0.06220018821618249 and parameters: {'observation_period_num': 61, 'train_rates': 0.8071394488239272, 'learning_rate': 0.00017793910275153114, 'batch_size': 167, 'step_size': 15, 'gamma': 0.7848228309823498}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 04:03:23,033][0m Trial 42 finished with value: 0.051942482483362466 and parameters: {'observation_period_num': 41, 'train_rates': 0.8331331925402274, 'learning_rate': 0.0002190105602207359, 'batch_size': 157, 'step_size': 14, 'gamma': 0.7720082640237027}. Best is trial 28 with value: 0.0463013803847007.[0m
[32m[I 2025-01-03 04:03:59,085][0m Trial 43 finished with value: 0.04556808852937431 and parameters: {'observation_period_num': 41, 'train_rates': 0.8358426241331443, 'learning_rate': 0.00032899134313946833, 'batch_size': 154, 'step_size': 14, 'gamma': 0.7921857477415819}. Best is trial 43 with value: 0.04556808852937431.[0m
[32m[I 2025-01-03 04:04:36,919][0m Trial 44 finished with value: 0.04660554952249768 and parameters: {'observation_period_num': 38, 'train_rates': 0.8780281585533652, 'learning_rate': 0.000549417302054961, 'batch_size': 154, 'step_size': 13, 'gamma': 0.8099725904680228}. Best is trial 43 with value: 0.04556808852937431.[0m
[32m[I 2025-01-03 04:05:14,996][0m Trial 45 finished with value: 0.04402399796941509 and parameters: {'observation_period_num': 44, 'train_rates': 0.8746103296204295, 'learning_rate': 0.000494603421847704, 'batch_size': 151, 'step_size': 10, 'gamma': 0.8083422607629602}. Best is trial 45 with value: 0.04402399796941509.[0m
[32m[I 2025-01-03 04:06:05,149][0m Trial 46 finished with value: 0.07090826246251421 and parameters: {'observation_period_num': 92, 'train_rates': 0.8853874546853977, 'learning_rate': 0.0005947134850278658, 'batch_size': 113, 'step_size': 9, 'gamma': 0.8095096565900962}. Best is trial 45 with value: 0.04402399796941509.[0m
[32m[I 2025-01-03 04:06:51,462][0m Trial 47 finished with value: 0.08648854696831736 and parameters: {'observation_period_num': 122, 'train_rates': 0.9482025288033168, 'learning_rate': 0.0007491972773922049, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8451271619348951}. Best is trial 45 with value: 0.04402399796941509.[0m
[32m[I 2025-01-03 04:07:27,268][0m Trial 48 finished with value: 0.10577160017243747 and parameters: {'observation_period_num': 244, 'train_rates': 0.8715248192711644, 'learning_rate': 0.000490549667528947, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8258671200880608}. Best is trial 45 with value: 0.04402399796941509.[0m
[32m[I 2025-01-03 04:08:20,641][0m Trial 49 finished with value: 0.06962698728086487 and parameters: {'observation_period_num': 75, 'train_rates': 0.9355682779224301, 'learning_rate': 0.00036791329919723506, 'batch_size': 111, 'step_size': 12, 'gamma': 0.8578487587824092}. Best is trial 45 with value: 0.04402399796941509.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 04:08:20,651][0m A new study created in memory with name: no-name-f6852abe-f98b-4ffd-b894-1612db3e0790[0m
[32m[I 2025-01-03 04:09:05,732][0m Trial 0 finished with value: 0.17247654404491186 and parameters: {'observation_period_num': 87, 'train_rates': 0.6479520040156711, 'learning_rate': 0.00013324544656154156, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9711282732815913}. Best is trial 0 with value: 0.17247654404491186.[0m
[32m[I 2025-01-03 04:10:19,498][0m Trial 1 finished with value: 0.1667253258508011 and parameters: {'observation_period_num': 195, 'train_rates': 0.9351338074522444, 'learning_rate': 1.386153894082047e-05, 'batch_size': 74, 'step_size': 4, 'gamma': 0.925293355781644}. Best is trial 1 with value: 0.1667253258508011.[0m
[32m[I 2025-01-03 04:10:40,404][0m Trial 2 finished with value: 0.16882617336787362 and parameters: {'observation_period_num': 70, 'train_rates': 0.6098798771276204, 'learning_rate': 0.00046994852372014677, 'batch_size': 221, 'step_size': 9, 'gamma': 0.9746213882563276}. Best is trial 1 with value: 0.1667253258508011.[0m
[32m[I 2025-01-03 04:11:15,108][0m Trial 3 finished with value: 0.1822180404966132 and parameters: {'observation_period_num': 16, 'train_rates': 0.7880010352501671, 'learning_rate': 0.00013262114007463484, 'batch_size': 155, 'step_size': 7, 'gamma': 0.916147736375771}. Best is trial 1 with value: 0.1667253258508011.[0m
[32m[I 2025-01-03 04:12:04,165][0m Trial 4 finished with value: 0.26089027523994446 and parameters: {'observation_period_num': 185, 'train_rates': 0.9782585119584173, 'learning_rate': 6.221924056743894e-06, 'batch_size': 121, 'step_size': 13, 'gamma': 0.7990903787278975}. Best is trial 1 with value: 0.1667253258508011.[0m
[32m[I 2025-01-03 04:12:29,966][0m Trial 5 finished with value: 0.07777381977759117 and parameters: {'observation_period_num': 110, 'train_rates': 0.8298263901779159, 'learning_rate': 0.00013888279787750644, 'batch_size': 216, 'step_size': 14, 'gamma': 0.7661892456261274}. Best is trial 5 with value: 0.07777381977759117.[0m
[32m[I 2025-01-03 04:17:54,672][0m Trial 6 finished with value: 0.0949652015135206 and parameters: {'observation_period_num': 16, 'train_rates': 0.9802681925190773, 'learning_rate': 1.3048648454245846e-06, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8393832969452228}. Best is trial 5 with value: 0.07777381977759117.[0m
Early stopping at epoch 93
[32m[I 2025-01-03 04:18:25,472][0m Trial 7 finished with value: 1.5773840771463816 and parameters: {'observation_period_num': 149, 'train_rates': 0.7617845429185592, 'learning_rate': 5.571461836391453e-06, 'batch_size': 160, 'step_size': 2, 'gamma': 0.7659530203454447}. Best is trial 5 with value: 0.07777381977759117.[0m
Early stopping at epoch 70
[32m[I 2025-01-03 04:19:23,869][0m Trial 8 finished with value: 1.8143314836181212 and parameters: {'observation_period_num': 246, 'train_rates': 0.6043344705648984, 'learning_rate': 1.962527789083105e-06, 'batch_size': 49, 'step_size': 1, 'gamma': 0.7883353576136962}. Best is trial 5 with value: 0.07777381977759117.[0m
[32m[I 2025-01-03 04:20:09,560][0m Trial 9 finished with value: 0.21994599083967628 and parameters: {'observation_period_num': 165, 'train_rates': 0.844554020013583, 'learning_rate': 5.6318218715330135e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.9198072058086972}. Best is trial 5 with value: 0.07777381977759117.[0m
[32m[I 2025-01-03 04:20:29,629][0m Trial 10 finished with value: 0.282851642268954 and parameters: {'observation_period_num': 106, 'train_rates': 0.7105770029657734, 'learning_rate': 6.359220141461006e-05, 'batch_size': 253, 'step_size': 10, 'gamma': 0.8448853935731603}. Best is trial 5 with value: 0.07777381977759117.[0m
[32m[I 2025-01-03 04:26:04,611][0m Trial 11 finished with value: 0.07053056927685886 and parameters: {'observation_period_num': 9, 'train_rates': 0.8682530216853778, 'learning_rate': 1.0262008949873728e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8351359760258268}. Best is trial 11 with value: 0.07053056927685886.[0m
[32m[I 2025-01-03 04:26:36,632][0m Trial 12 finished with value: 0.053983406292704435 and parameters: {'observation_period_num': 57, 'train_rates': 0.8653948748906845, 'learning_rate': 0.00036992675491986313, 'batch_size': 183, 'step_size': 15, 'gamma': 0.7582305013343279}. Best is trial 12 with value: 0.053983406292704435.[0m
[32m[I 2025-01-03 04:27:09,597][0m Trial 13 finished with value: 0.051059581541943046 and parameters: {'observation_period_num': 52, 'train_rates': 0.8859514883786991, 'learning_rate': 0.0005753816829154077, 'batch_size': 183, 'step_size': 11, 'gamma': 0.8215310232785243}. Best is trial 13 with value: 0.051059581541943046.[0m
[32m[I 2025-01-03 04:27:40,730][0m Trial 14 finished with value: 0.045701183834854435 and parameters: {'observation_period_num': 54, 'train_rates': 0.8986724514251933, 'learning_rate': 0.0008139631563476336, 'batch_size': 192, 'step_size': 11, 'gamma': 0.8041327454422983}. Best is trial 14 with value: 0.045701183834854435.[0m
[32m[I 2025-01-03 04:28:13,301][0m Trial 15 finished with value: 0.042778972536325455 and parameters: {'observation_period_num': 42, 'train_rates': 0.9353321299757272, 'learning_rate': 0.0009927851547830832, 'batch_size': 194, 'step_size': 11, 'gamma': 0.8089579084393337}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:28:42,276][0m Trial 16 finished with value: 0.05011757310747094 and parameters: {'observation_period_num': 40, 'train_rates': 0.9186372995930651, 'learning_rate': 0.0008636419108752961, 'batch_size': 215, 'step_size': 12, 'gamma': 0.8756042551159292}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:29:07,007][0m Trial 17 finished with value: 0.05305452272295952 and parameters: {'observation_period_num': 84, 'train_rates': 0.9213849540348255, 'learning_rate': 0.0002448252473210326, 'batch_size': 252, 'step_size': 8, 'gamma': 0.8686203230201002}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:29:39,602][0m Trial 18 finished with value: 0.08314590901136398 and parameters: {'observation_period_num': 128, 'train_rates': 0.9453241478325403, 'learning_rate': 0.0009888610651729273, 'batch_size': 181, 'step_size': 11, 'gamma': 0.7984681641041512}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:30:15,685][0m Trial 19 finished with value: 0.07903526569873782 and parameters: {'observation_period_num': 37, 'train_rates': 0.8070622391075777, 'learning_rate': 4.6199655094049816e-05, 'batch_size': 154, 'step_size': 4, 'gamma': 0.8665195999655853}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:30:45,226][0m Trial 20 finished with value: 0.15831663767755613 and parameters: {'observation_period_num': 81, 'train_rates': 0.8931013861118642, 'learning_rate': 2.3101098278013658e-05, 'batch_size': 201, 'step_size': 9, 'gamma': 0.8115122694529998}. Best is trial 15 with value: 0.042778972536325455.[0m
[32m[I 2025-01-03 04:31:12,839][0m Trial 21 finished with value: 0.038017829589080065 and parameters: {'observation_period_num': 35, 'train_rates': 0.9122591074124782, 'learning_rate': 0.0009060871359966369, 'batch_size': 225, 'step_size': 12, 'gamma': 0.893588418934904}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:31:40,111][0m Trial 22 finished with value: 0.050641849637031555 and parameters: {'observation_period_num': 33, 'train_rates': 0.9590092689213184, 'learning_rate': 0.00024921427211089663, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9061886035842025}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:32:10,775][0m Trial 23 finished with value: 0.05606518570791211 and parameters: {'observation_period_num': 58, 'train_rates': 0.9016471939493702, 'learning_rate': 0.0009474008443813866, 'batch_size': 193, 'step_size': 10, 'gamma': 0.8859656476568928}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:32:39,411][0m Trial 24 finished with value: 0.04693285748362541 and parameters: {'observation_period_num': 26, 'train_rates': 0.953680844702639, 'learning_rate': 0.00028405047956290425, 'batch_size': 231, 'step_size': 12, 'gamma': 0.9390140450048341}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:33:16,290][0m Trial 25 finished with value: 0.08742444217205048 and parameters: {'observation_period_num': 103, 'train_rates': 0.9899237065906665, 'learning_rate': 0.000665342242609339, 'batch_size': 169, 'step_size': 12, 'gamma': 0.7802027925297764}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:33:55,650][0m Trial 26 finished with value: 0.16058248258132377 and parameters: {'observation_period_num': 5, 'train_rates': 0.7508195560013644, 'learning_rate': 0.000476074961502743, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8507963920128641}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:34:20,895][0m Trial 27 finished with value: 0.07699114788274408 and parameters: {'observation_period_num': 66, 'train_rates': 0.8426817982298096, 'learning_rate': 9.044321050842872e-05, 'batch_size': 235, 'step_size': 10, 'gamma': 0.8170510712170809}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:34:52,545][0m Trial 28 finished with value: 0.06134184991218606 and parameters: {'observation_period_num': 45, 'train_rates': 0.9157146942373313, 'learning_rate': 0.00021636529428833805, 'batch_size': 204, 'step_size': 13, 'gamma': 0.9001418326489895}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:35:27,548][0m Trial 29 finished with value: 0.1854198222550062 and parameters: {'observation_period_num': 70, 'train_rates': 0.7067190674335284, 'learning_rate': 0.00013900282121121946, 'batch_size': 143, 'step_size': 7, 'gamma': 0.9476424900012694}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:36:01,329][0m Trial 30 finished with value: 0.06705539641501028 and parameters: {'observation_period_num': 93, 'train_rates': 0.8619414255112725, 'learning_rate': 0.0004007301662383878, 'batch_size': 172, 'step_size': 9, 'gamma': 0.9511187101916133}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:36:28,129][0m Trial 31 finished with value: 0.04714314639568329 and parameters: {'observation_period_num': 27, 'train_rates': 0.9546394997460025, 'learning_rate': 0.00031620523534341187, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8917461200927084}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:36:57,199][0m Trial 32 finished with value: 0.0414968878030777 and parameters: {'observation_period_num': 26, 'train_rates': 0.9409135679293464, 'learning_rate': 0.0006840299093319673, 'batch_size': 220, 'step_size': 12, 'gamma': 0.9403154159273273}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:37:28,409][0m Trial 33 finished with value: 0.06472457200288773 and parameters: {'observation_period_num': 47, 'train_rates': 0.9350593336567589, 'learning_rate': 0.0007291543271344406, 'batch_size': 204, 'step_size': 14, 'gamma': 0.9737306827693017}. Best is trial 21 with value: 0.038017829589080065.[0m
[32m[I 2025-01-03 04:37:58,317][0m Trial 34 finished with value: 0.033195746276760474 and parameters: {'observation_period_num': 24, 'train_rates': 0.8908090484110834, 'learning_rate': 0.0005674414483444585, 'batch_size': 215, 'step_size': 11, 'gamma': 0.9833993042011113}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:38:28,018][0m Trial 35 finished with value: 0.03770722448825836 and parameters: {'observation_period_num': 24, 'train_rates': 0.9314141206117077, 'learning_rate': 0.0005446745062928567, 'batch_size': 221, 'step_size': 8, 'gamma': 0.9879061385104818}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:38:54,339][0m Trial 36 finished with value: 0.04546764130011583 and parameters: {'observation_period_num': 18, 'train_rates': 0.8138186486290128, 'learning_rate': 0.00017467109473377721, 'batch_size': 224, 'step_size': 6, 'gamma': 0.9849817260733194}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:39:17,883][0m Trial 37 finished with value: 0.13001998433147569 and parameters: {'observation_period_num': 241, 'train_rates': 0.8777948807320493, 'learning_rate': 0.0004950577825267025, 'batch_size': 248, 'step_size': 8, 'gamma': 0.9888939715435476}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:39:46,999][0m Trial 38 finished with value: 0.10953254997730255 and parameters: {'observation_period_num': 215, 'train_rates': 0.9701270683114753, 'learning_rate': 0.0005543006680261165, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9563383649806804}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:41:02,048][0m Trial 39 finished with value: 0.05521705182998077 and parameters: {'observation_period_num': 24, 'train_rates': 0.9216876205911693, 'learning_rate': 2.400400810779587e-05, 'batch_size': 79, 'step_size': 9, 'gamma': 0.966919369816572}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:41:30,308][0m Trial 40 finished with value: 0.04352671198671582 and parameters: {'observation_period_num': 5, 'train_rates': 0.8379641178400538, 'learning_rate': 9.828849147982845e-05, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9326399160990942}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:42:01,645][0m Trial 41 finished with value: 0.0485430508852005 and parameters: {'observation_period_num': 37, 'train_rates': 0.938794585928118, 'learning_rate': 0.0004144651203626502, 'batch_size': 208, 'step_size': 10, 'gamma': 0.9692774534799692}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:42:28,556][0m Trial 42 finished with value: 0.10648608207702637 and parameters: {'observation_period_num': 68, 'train_rates': 0.9691784296273801, 'learning_rate': 0.000641765543026779, 'batch_size': 239, 'step_size': 14, 'gamma': 0.961516066115369}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:42:59,469][0m Trial 43 finished with value: 0.09612436908004927 and parameters: {'observation_period_num': 16, 'train_rates': 0.9066521462561038, 'learning_rate': 1.0508300128389741e-05, 'batch_size': 195, 'step_size': 13, 'gamma': 0.9231206567325883}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:43:28,407][0m Trial 44 finished with value: 0.04410473257303238 and parameters: {'observation_period_num': 29, 'train_rates': 0.9319699657095372, 'learning_rate': 0.00035728537785896584, 'batch_size': 219, 'step_size': 7, 'gamma': 0.9842884966526948}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:43:53,532][0m Trial 45 finished with value: 0.04276425911218016 and parameters: {'observation_period_num': 15, 'train_rates': 0.8846788343022656, 'learning_rate': 0.00018359078679134014, 'batch_size': 245, 'step_size': 11, 'gamma': 0.9440195929519065}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:44:14,286][0m Trial 46 finished with value: 0.13811625719411683 and parameters: {'observation_period_num': 19, 'train_rates': 0.6424857610847554, 'learning_rate': 0.0001833933231342459, 'batch_size': 249, 'step_size': 9, 'gamma': 0.9404227519471382}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:44:37,322][0m Trial 47 finished with value: 0.1032945242964833 and parameters: {'observation_period_num': 158, 'train_rates': 0.8572787575721568, 'learning_rate': 0.0005497902224980632, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9089024005653707}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:45:30,100][0m Trial 48 finished with value: 0.044879715174956376 and parameters: {'observation_period_num': 10, 'train_rates': 0.8832404835439442, 'learning_rate': 0.00011544513531191936, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9310584040339243}. Best is trial 34 with value: 0.033195746276760474.[0m
[32m[I 2025-01-03 04:45:53,727][0m Trial 49 finished with value: 0.4632546491236838 and parameters: {'observation_period_num': 127, 'train_rates': 0.7878531980530862, 'learning_rate': 3.5980113162277554e-06, 'batch_size': 243, 'step_size': 10, 'gamma': 0.9780217552585814}. Best is trial 34 with value: 0.033195746276760474.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 04:45:53,737][0m A new study created in memory with name: no-name-a743e3e7-0dad-4136-9f9f-c26f225a8d30[0m
[32m[I 2025-01-03 04:47:10,980][0m Trial 0 finished with value: 0.8627462017056126 and parameters: {'observation_period_num': 96, 'train_rates': 0.7913771738815223, 'learning_rate': 1.9670208466800836e-06, 'batch_size': 65, 'step_size': 2, 'gamma': 0.8045496209365381}. Best is trial 0 with value: 0.8627462017056126.[0m
[32m[I 2025-01-03 04:47:38,851][0m Trial 1 finished with value: 0.2595457492830002 and parameters: {'observation_period_num': 167, 'train_rates': 0.737833310400958, 'learning_rate': 0.00032665184822299224, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9419735261975095}. Best is trial 1 with value: 0.2595457492830002.[0m
[32m[I 2025-01-03 04:50:51,180][0m Trial 2 finished with value: 0.09843946948297833 and parameters: {'observation_period_num': 186, 'train_rates': 0.7998528570667731, 'learning_rate': 6.316226192302545e-05, 'batch_size': 25, 'step_size': 6, 'gamma': 0.7748760006439054}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:51:24,335][0m Trial 3 finished with value: 0.6554227182420633 and parameters: {'observation_period_num': 206, 'train_rates': 0.8925826831227088, 'learning_rate': 1.2652120139158815e-06, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9244063443813046}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:51:47,382][0m Trial 4 finished with value: 0.6856927006926162 and parameters: {'observation_period_num': 102, 'train_rates': 0.6107007812466847, 'learning_rate': 1.6987367172870989e-06, 'batch_size': 199, 'step_size': 12, 'gamma': 0.8556797314970355}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:52:09,395][0m Trial 5 finished with value: 0.2268994188158767 and parameters: {'observation_period_num': 110, 'train_rates': 0.7761249398963754, 'learning_rate': 0.0005573860355336937, 'batch_size': 250, 'step_size': 6, 'gamma': 0.8046385399737793}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:53:01,815][0m Trial 6 finished with value: 0.28289763305498206 and parameters: {'observation_period_num': 47, 'train_rates': 0.7466061703288297, 'learning_rate': 1.057395143114645e-05, 'batch_size': 96, 'step_size': 11, 'gamma': 0.7724609177842715}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:53:31,609][0m Trial 7 finished with value: 0.2846114760089348 and parameters: {'observation_period_num': 189, 'train_rates': 0.7579613557929119, 'learning_rate': 0.0001887826678972827, 'batch_size': 169, 'step_size': 14, 'gamma': 0.9566179853642651}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:53:53,656][0m Trial 8 finished with value: 0.5890844062522606 and parameters: {'observation_period_num': 217, 'train_rates': 0.7530586781963619, 'learning_rate': 4.19509677777715e-05, 'batch_size': 256, 'step_size': 1, 'gamma': 0.8926296386026294}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:54:19,608][0m Trial 9 finished with value: 0.12362740933895111 and parameters: {'observation_period_num': 211, 'train_rates': 0.9647755660709088, 'learning_rate': 0.000324414406401142, 'batch_size': 243, 'step_size': 10, 'gamma': 0.8097211823593545}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:58:39,294][0m Trial 10 finished with value: 0.12934845019889765 and parameters: {'observation_period_num': 154, 'train_rates': 0.8752125829717424, 'learning_rate': 4.805578634992692e-05, 'batch_size': 20, 'step_size': 7, 'gamma': 0.8583310399533322}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 04:59:28,810][0m Trial 11 finished with value: 0.09939251840114594 and parameters: {'observation_period_num': 245, 'train_rates': 0.9771723222554278, 'learning_rate': 0.0001213392462242058, 'batch_size': 116, 'step_size': 10, 'gamma': 0.7501643618525422}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 05:00:21,911][0m Trial 12 finished with value: 0.11693385988473892 and parameters: {'observation_period_num': 248, 'train_rates': 0.9889589785385211, 'learning_rate': 9.768337415116888e-05, 'batch_size': 111, 'step_size': 9, 'gamma': 0.7501808981398662}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 05:02:57,982][0m Trial 13 finished with value: 0.33378372986444693 and parameters: {'observation_period_num': 245, 'train_rates': 0.669748971598506, 'learning_rate': 1.115067792598833e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.7506430057217548}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 05:04:15,089][0m Trial 14 finished with value: 0.12209936483852887 and parameters: {'observation_period_num': 168, 'train_rates': 0.8920632978140114, 'learning_rate': 1.069034877391274e-05, 'batch_size': 69, 'step_size': 3, 'gamma': 0.9849097491590044}. Best is trial 2 with value: 0.09843946948297833.[0m
[32m[I 2025-01-03 05:04:57,696][0m Trial 15 finished with value: 0.04703881915460909 and parameters: {'observation_period_num': 19, 'train_rates': 0.8458858417011965, 'learning_rate': 9.58330277636375e-05, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8309908646918815}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:05:37,625][0m Trial 16 finished with value: 0.06171770664366457 and parameters: {'observation_period_num': 9, 'train_rates': 0.832429512475595, 'learning_rate': 2.41288260826794e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.8245353700519426}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:06:18,325][0m Trial 17 finished with value: 0.09147709296851218 and parameters: {'observation_period_num': 13, 'train_rates': 0.8388613089115362, 'learning_rate': 1.8014716148236816e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.8333879755060668}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:06:46,154][0m Trial 18 finished with value: 0.13807730428438125 and parameters: {'observation_period_num': 6, 'train_rates': 0.8435303964607583, 'learning_rate': 3.3550191703681713e-06, 'batch_size': 216, 'step_size': 8, 'gamma': 0.902727783198373}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:07:29,524][0m Trial 19 finished with value: 0.07067743649191045 and parameters: {'observation_period_num': 52, 'train_rates': 0.9352820223351974, 'learning_rate': 2.5122894149074106e-05, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8331467877097666}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:08:36,992][0m Trial 20 finished with value: 0.18266948216027262 and parameters: {'observation_period_num': 47, 'train_rates': 0.8377969983806101, 'learning_rate': 5.254272860085893e-06, 'batch_size': 80, 'step_size': 4, 'gamma': 0.8789024184510588}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:09:20,473][0m Trial 21 finished with value: 0.06411377422579932 and parameters: {'observation_period_num': 40, 'train_rates': 0.9253158386723962, 'learning_rate': 2.4879037828851485e-05, 'batch_size': 140, 'step_size': 12, 'gamma': 0.8313024539547625}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:10:05,965][0m Trial 22 finished with value: 0.06559475239204324 and parameters: {'observation_period_num': 27, 'train_rates': 0.9214280892107876, 'learning_rate': 2.369533725936412e-05, 'batch_size': 129, 'step_size': 9, 'gamma': 0.8330436907027722}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:10:45,014][0m Trial 23 finished with value: 0.057584898870679274 and parameters: {'observation_period_num': 64, 'train_rates': 0.9303632585009081, 'learning_rate': 8.492213577870861e-05, 'batch_size': 154, 'step_size': 12, 'gamma': 0.8189870729942768}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:11:20,698][0m Trial 24 finished with value: 0.05941478925569873 and parameters: {'observation_period_num': 73, 'train_rates': 0.8670666407530648, 'learning_rate': 8.948847785110346e-05, 'batch_size': 161, 'step_size': 15, 'gamma': 0.795930057440858}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:11:56,529][0m Trial 25 finished with value: 0.05491554031728721 and parameters: {'observation_period_num': 71, 'train_rates': 0.872999936539547, 'learning_rate': 8.835480942189353e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.7862671656060418}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:12:25,590][0m Trial 26 finished with value: 0.06333572417497635 and parameters: {'observation_period_num': 69, 'train_rates': 0.938901879700319, 'learning_rate': 0.00015999828249956772, 'batch_size': 210, 'step_size': 15, 'gamma': 0.7795740235870933}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:12:58,602][0m Trial 27 finished with value: 0.059055210961090336 and parameters: {'observation_period_num': 74, 'train_rates': 0.9052186005079695, 'learning_rate': 0.0002464221738580589, 'batch_size': 183, 'step_size': 13, 'gamma': 0.8541306943186702}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:13:42,918][0m Trial 28 finished with value: 0.20123646408319473 and parameters: {'observation_period_num': 85, 'train_rates': 0.7068217571230571, 'learning_rate': 0.0009098716747051564, 'batch_size': 112, 'step_size': 13, 'gamma': 0.7934488323833104}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:14:42,882][0m Trial 29 finished with value: 0.07684263767121913 and parameters: {'observation_period_num': 133, 'train_rates': 0.8062787373425566, 'learning_rate': 6.951862491706625e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8163168880119493}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:15:14,387][0m Trial 30 finished with value: 0.0777773845408644 and parameters: {'observation_period_num': 116, 'train_rates': 0.8643522822665711, 'learning_rate': 0.00013547251391984983, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8447695997551269}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:15:52,418][0m Trial 31 finished with value: 0.07046196343982296 and parameters: {'observation_period_num': 72, 'train_rates': 0.8912165369352005, 'learning_rate': 0.00026799760551360374, 'batch_size': 155, 'step_size': 13, 'gamma': 0.875792650036559}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:16:25,446][0m Trial 32 finished with value: 0.07375996559858322 and parameters: {'observation_period_num': 87, 'train_rates': 0.9588790086098208, 'learning_rate': 0.0005456079811362372, 'batch_size': 191, 'step_size': 15, 'gamma': 0.8502618360968008}. Best is trial 15 with value: 0.04703881915460909.[0m
[32m[I 2025-01-03 05:16:54,644][0m Trial 33 finished with value: 0.04047548208090899 and parameters: {'observation_period_num': 27, 'train_rates': 0.9049263998649159, 'learning_rate': 0.00019842865628052532, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7869888691000124}. Best is trial 33 with value: 0.04047548208090899.[0m
[32m[I 2025-01-03 05:17:20,407][0m Trial 34 finished with value: 0.06586346486565747 and parameters: {'observation_period_num': 30, 'train_rates': 0.8189601372321339, 'learning_rate': 5.980727644455092e-05, 'batch_size': 226, 'step_size': 11, 'gamma': 0.7776150197879556}. Best is trial 33 with value: 0.04047548208090899.[0m
[32m[I 2025-01-03 05:17:45,751][0m Trial 35 finished with value: 0.05048225097247024 and parameters: {'observation_period_num': 59, 'train_rates': 0.8610955299604259, 'learning_rate': 0.0004392978173095162, 'batch_size': 231, 'step_size': 14, 'gamma': 0.7890435712093754}. Best is trial 33 with value: 0.04047548208090899.[0m
[32m[I 2025-01-03 05:18:09,628][0m Trial 36 finished with value: 0.1799825004449016 and parameters: {'observation_period_num': 28, 'train_rates': 0.7790430749305789, 'learning_rate': 0.0005306363830152843, 'batch_size': 235, 'step_size': 14, 'gamma': 0.7877474974060779}. Best is trial 33 with value: 0.04047548208090899.[0m
[32m[I 2025-01-03 05:18:40,018][0m Trial 37 finished with value: 0.03953737446240017 and parameters: {'observation_period_num': 23, 'train_rates': 0.8568013084235129, 'learning_rate': 0.000833111017076163, 'batch_size': 205, 'step_size': 6, 'gamma': 0.7633620395913743}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:19:08,275][0m Trial 38 finished with value: 0.044386361305987214 and parameters: {'observation_period_num': 23, 'train_rates': 0.7961360712718188, 'learning_rate': 0.0009214707506746443, 'batch_size': 203, 'step_size': 6, 'gamma': 0.7659288439647531}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:19:34,918][0m Trial 39 finished with value: 0.14945808851038825 and parameters: {'observation_period_num': 19, 'train_rates': 0.7144810650568033, 'learning_rate': 0.0009982989153019678, 'batch_size': 202, 'step_size': 6, 'gamma': 0.7720583556601873}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:20:00,704][0m Trial 40 finished with value: 0.054831571877002716 and parameters: {'observation_period_num': 34, 'train_rates': 0.7883121757412377, 'learning_rate': 0.0007490467677613072, 'batch_size': 216, 'step_size': 4, 'gamma': 0.8032242848534372}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:20:25,495][0m Trial 41 finished with value: 0.052904723250838406 and parameters: {'observation_period_num': 53, 'train_rates': 0.8508042067260632, 'learning_rate': 0.0003894696474852235, 'batch_size': 231, 'step_size': 7, 'gamma': 0.7687193276642303}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:20:55,207][0m Trial 42 finished with value: 0.049479002325699246 and parameters: {'observation_period_num': 18, 'train_rates': 0.8222849438308638, 'learning_rate': 0.00038756384759671325, 'batch_size': 196, 'step_size': 5, 'gamma': 0.768928574451681}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:21:25,126][0m Trial 43 finished with value: 0.05806854281801477 and parameters: {'observation_period_num': 18, 'train_rates': 0.8098653412352711, 'learning_rate': 0.00022892914738555526, 'batch_size': 196, 'step_size': 5, 'gamma': 0.7638740006486712}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:21:55,878][0m Trial 44 finished with value: 0.1911425785453462 and parameters: {'observation_period_num': 39, 'train_rates': 0.7698508744995014, 'learning_rate': 0.0006928771476427313, 'batch_size': 177, 'step_size': 5, 'gamma': 0.7592093821605118}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:22:23,496][0m Trial 45 finished with value: 0.05274720733339463 and parameters: {'observation_period_num': 24, 'train_rates': 0.7961631032293228, 'learning_rate': 0.0003408001124028128, 'batch_size': 210, 'step_size': 7, 'gamma': 0.7611682301213606}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:22:47,685][0m Trial 46 finished with value: 0.05920513378511561 and parameters: {'observation_period_num': 42, 'train_rates': 0.8265713454002519, 'learning_rate': 0.00019637277948108227, 'batch_size': 247, 'step_size': 6, 'gamma': 0.8050535090212049}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:23:16,868][0m Trial 47 finished with value: 0.1591152807875494 and parameters: {'observation_period_num': 8, 'train_rates': 0.7297466862177774, 'learning_rate': 0.0007193057098445775, 'batch_size': 188, 'step_size': 4, 'gamma': 0.7794505602951277}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:24:02,406][0m Trial 48 finished with value: 0.05300238950938923 and parameters: {'observation_period_num': 5, 'train_rates': 0.8165427001543991, 'learning_rate': 0.0004834099521233652, 'batch_size': 125, 'step_size': 2, 'gamma': 0.8002017615535469}. Best is trial 37 with value: 0.03953737446240017.[0m
[32m[I 2025-01-03 05:26:26,648][0m Trial 49 finished with value: 0.030298777061857674 and parameters: {'observation_period_num': 20, 'train_rates': 0.9027765814986409, 'learning_rate': 0.0003012990802565954, 'batch_size': 39, 'step_size': 7, 'gamma': 0.7595410828182823}. Best is trial 49 with value: 0.030298777061857674.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 05:26:26,658][0m A new study created in memory with name: no-name-015f9b73-8bae-4b3d-bd82-d7996ff28ceb[0m
[32m[I 2025-01-03 05:26:55,349][0m Trial 0 finished with value: 0.861177488735744 and parameters: {'observation_period_num': 199, 'train_rates': 0.7457975651474518, 'learning_rate': 1.1628805790827133e-06, 'batch_size': 180, 'step_size': 10, 'gamma': 0.9891545655897489}. Best is trial 0 with value: 0.861177488735744.[0m
[32m[I 2025-01-03 05:27:37,373][0m Trial 1 finished with value: 0.17390831770568058 and parameters: {'observation_period_num': 63, 'train_rates': 0.8996065727844208, 'learning_rate': 1.0519040215139526e-05, 'batch_size': 144, 'step_size': 3, 'gamma': 0.9467930744202417}. Best is trial 1 with value: 0.17390831770568058.[0m
[32m[I 2025-01-03 05:28:35,798][0m Trial 2 finished with value: 0.08981212665294779 and parameters: {'observation_period_num': 185, 'train_rates': 0.9164129703658151, 'learning_rate': 0.0003782014022975029, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9321409592270046}. Best is trial 2 with value: 0.08981212665294779.[0m
[32m[I 2025-01-03 05:29:12,838][0m Trial 3 finished with value: 0.10558106861314419 and parameters: {'observation_period_num': 202, 'train_rates': 0.9414875674768904, 'learning_rate': 0.00016793678466275452, 'batch_size': 158, 'step_size': 3, 'gamma': 0.8848455796853135}. Best is trial 2 with value: 0.08981212665294779.[0m
[32m[I 2025-01-03 05:29:44,144][0m Trial 4 finished with value: 0.40313704096381886 and parameters: {'observation_period_num': 178, 'train_rates': 0.7630254326121952, 'learning_rate': 1.59399235790338e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.7691415906447083}. Best is trial 2 with value: 0.08981212665294779.[0m
[32m[I 2025-01-03 05:30:34,476][0m Trial 5 finished with value: 0.07700334847504024 and parameters: {'observation_period_num': 218, 'train_rates': 0.8766484208558598, 'learning_rate': 0.0003090790625972563, 'batch_size': 105, 'step_size': 8, 'gamma': 0.7564840180190889}. Best is trial 5 with value: 0.07700334847504024.[0m
[32m[I 2025-01-03 05:31:33,337][0m Trial 6 finished with value: 0.10974955591170685 and parameters: {'observation_period_num': 240, 'train_rates': 0.957818617969473, 'learning_rate': 0.0006319384034315029, 'batch_size': 95, 'step_size': 14, 'gamma': 0.896479030945732}. Best is trial 5 with value: 0.07700334847504024.[0m
[32m[I 2025-01-03 05:31:55,418][0m Trial 7 finished with value: 0.22482908689058745 and parameters: {'observation_period_num': 144, 'train_rates': 0.6991913989954961, 'learning_rate': 0.000786785209014771, 'batch_size': 225, 'step_size': 4, 'gamma': 0.7815834198388971}. Best is trial 5 with value: 0.07700334847504024.[0m
[32m[I 2025-01-03 05:32:19,202][0m Trial 8 finished with value: 0.6180230253246201 and parameters: {'observation_period_num': 232, 'train_rates': 0.8153372802891126, 'learning_rate': 1.2014995331822617e-06, 'batch_size': 237, 'step_size': 13, 'gamma': 0.768193516262786}. Best is trial 5 with value: 0.07700334847504024.[0m
[32m[I 2025-01-03 05:34:25,269][0m Trial 9 finished with value: 0.33804622014586844 and parameters: {'observation_period_num': 205, 'train_rates': 0.6758277218990748, 'learning_rate': 0.00019256858542918194, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9739879545747747}. Best is trial 5 with value: 0.07700334847504024.[0m
[32m[I 2025-01-03 05:38:26,496][0m Trial 10 finished with value: 0.03150041765196526 and parameters: {'observation_period_num': 7, 'train_rates': 0.8402991056701384, 'learning_rate': 5.823433020504899e-05, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8260259558754263}. Best is trial 10 with value: 0.03150041765196526.[0m
[32m[I 2025-01-03 05:43:55,378][0m Trial 11 finished with value: 0.03141620288120175 and parameters: {'observation_period_num': 20, 'train_rates': 0.8428972070345593, 'learning_rate': 6.736563877632127e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8272457959423247}. Best is trial 11 with value: 0.03141620288120175.[0m
[32m[I 2025-01-03 05:49:21,722][0m Trial 12 finished with value: 0.029512436573350717 and parameters: {'observation_period_num': 11, 'train_rates': 0.830326421367511, 'learning_rate': 6.576198100849827e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8261428244099536}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 05:50:59,816][0m Trial 13 finished with value: 0.04197524856723105 and parameters: {'observation_period_num': 17, 'train_rates': 0.8322366604067968, 'learning_rate': 5.628002576147508e-05, 'batch_size': 55, 'step_size': 6, 'gamma': 0.8298394550074597}. Best is trial 12 with value: 0.029512436573350717.[0m
Early stopping at epoch 85
[32m[I 2025-01-03 05:52:04,693][0m Trial 14 finished with value: 0.7637555692398174 and parameters: {'observation_period_num': 68, 'train_rates': 0.6360236545495033, 'learning_rate': 5.398577048916098e-06, 'batch_size': 59, 'step_size': 1, 'gamma': 0.8358752241196966}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 05:56:22,942][0m Trial 15 finished with value: 0.20413231282655145 and parameters: {'observation_period_num': 59, 'train_rates': 0.7727898804670281, 'learning_rate': 7.60877171079907e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8564932894795735}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 05:57:45,955][0m Trial 16 finished with value: 0.0785778176627661 and parameters: {'observation_period_num': 39, 'train_rates': 0.8567054963533626, 'learning_rate': 2.686457736827769e-05, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8114489074559291}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 05:58:28,283][0m Trial 17 finished with value: 0.21267856812686514 and parameters: {'observation_period_num': 101, 'train_rates': 0.7204279351051817, 'learning_rate': 0.00013141973061204279, 'batch_size': 118, 'step_size': 11, 'gamma': 0.8039261869710809}. Best is trial 12 with value: 0.029512436573350717.[0m
Early stopping at epoch 74
[32m[I 2025-01-03 05:59:19,820][0m Trial 18 finished with value: 0.6375817126869537 and parameters: {'observation_period_num': 108, 'train_rates': 0.7965317756778619, 'learning_rate': 4.677676230218968e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.858135968862391}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:01:42,176][0m Trial 19 finished with value: 0.054867187416867205 and parameters: {'observation_period_num': 36, 'train_rates': 0.9805036647761286, 'learning_rate': 2.9689154967742892e-05, 'batch_size': 42, 'step_size': 8, 'gamma': 0.7936329692977887}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:02:11,128][0m Trial 20 finished with value: 0.07828376336644093 and parameters: {'observation_period_num': 86, 'train_rates': 0.8744670108576181, 'learning_rate': 9.613827791626111e-05, 'batch_size': 197, 'step_size': 4, 'gamma': 0.9004034369526465}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:07:16,444][0m Trial 21 finished with value: 0.04765328182450806 and parameters: {'observation_period_num': 27, 'train_rates': 0.8359771733191235, 'learning_rate': 5.0339187794185395e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8332153837115384}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:09:15,675][0m Trial 22 finished with value: 0.051032119216162616 and parameters: {'observation_period_num': 11, 'train_rates': 0.7977457859363946, 'learning_rate': 3.790401836442008e-05, 'batch_size': 43, 'step_size': 6, 'gamma': 0.8197849040364279}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:12:48,879][0m Trial 23 finished with value: 0.041145299384024 and parameters: {'observation_period_num': 5, 'train_rates': 0.8449617454198177, 'learning_rate': 1.485137147022026e-05, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8537712955270699}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:13:59,004][0m Trial 24 finished with value: 0.05145392417907715 and parameters: {'observation_period_num': 48, 'train_rates': 0.9071570368360592, 'learning_rate': 8.258458271103434e-05, 'batch_size': 81, 'step_size': 5, 'gamma': 0.8781690907671154}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:15:55,942][0m Trial 25 finished with value: 0.0758869166467704 and parameters: {'observation_period_num': 138, 'train_rates': 0.8188323924473884, 'learning_rate': 0.0003021002593851803, 'batch_size': 43, 'step_size': 8, 'gamma': 0.7964178601242555}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:16:37,066][0m Trial 26 finished with value: 0.23655606668677415 and parameters: {'observation_period_num': 5, 'train_rates': 0.7741318480638955, 'learning_rate': 2.4870177123384913e-05, 'batch_size': 127, 'step_size': 3, 'gamma': 0.8450919172815392}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:21:31,834][0m Trial 27 finished with value: 0.09715717790805432 and parameters: {'observation_period_num': 84, 'train_rates': 0.8844225807766065, 'learning_rate': 0.000136849256941892, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8193153205981462}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:23:25,295][0m Trial 28 finished with value: 0.1119869988359196 and parameters: {'observation_period_num': 28, 'train_rates': 0.9273196965669848, 'learning_rate': 7.25788384813083e-06, 'batch_size': 51, 'step_size': 7, 'gamma': 0.783995012251618}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:24:31,625][0m Trial 29 finished with value: 0.1866224196081588 and parameters: {'observation_period_num': 49, 'train_rates': 0.7372627044736009, 'learning_rate': 5.145972284988235e-05, 'batch_size': 76, 'step_size': 10, 'gamma': 0.8680327469373196}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:26:40,118][0m Trial 30 finished with value: 0.26446445239866756 and parameters: {'observation_period_num': 22, 'train_rates': 0.6000528237868219, 'learning_rate': 2.419351743373081e-06, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9160612943868902}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:29:45,231][0m Trial 31 finished with value: 0.04984074475413019 and parameters: {'observation_period_num': 12, 'train_rates': 0.8504196418403812, 'learning_rate': 1.2842958108068803e-05, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8548645883603627}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:33:14,727][0m Trial 32 finished with value: 0.05262423512924995 and parameters: {'observation_period_num': 7, 'train_rates': 0.8668112710552474, 'learning_rate': 1.9910372629910287e-05, 'batch_size': 26, 'step_size': 4, 'gamma': 0.8428437034866044}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:38:40,148][0m Trial 33 finished with value: 0.06443719598262207 and parameters: {'observation_period_num': 41, 'train_rates': 0.8420367621545337, 'learning_rate': 8.390114221251121e-06, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8205532458014931}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:40:27,539][0m Trial 34 finished with value: 0.05392403814655084 and parameters: {'observation_period_num': 65, 'train_rates': 0.8965909596093881, 'learning_rate': 3.855304405584498e-05, 'batch_size': 51, 'step_size': 9, 'gamma': 0.8675576988192588}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:42:54,235][0m Trial 35 finished with value: 0.0874829337604948 and parameters: {'observation_period_num': 25, 'train_rates': 0.810449778203832, 'learning_rate': 1.67493449838201e-05, 'batch_size': 35, 'step_size': 2, 'gamma': 0.846999845689281}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:43:31,531][0m Trial 36 finished with value: 0.11097960378851322 and parameters: {'observation_period_num': 175, 'train_rates': 0.7826730117513888, 'learning_rate': 0.0002287158835140035, 'batch_size': 142, 'step_size': 7, 'gamma': 0.8088753573186686}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:44:28,590][0m Trial 37 finished with value: 0.21221339066843847 and parameters: {'observation_period_num': 79, 'train_rates': 0.7473984019861617, 'learning_rate': 9.239915300263361e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.886882797698309}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:45:21,944][0m Trial 38 finished with value: 0.051033483708606046 and parameters: {'observation_period_num': 54, 'train_rates': 0.935486838288122, 'learning_rate': 0.00046663425567600435, 'batch_size': 110, 'step_size': 10, 'gamma': 0.7513430426099474}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:45:51,084][0m Trial 39 finished with value: 0.40164631427518577 and parameters: {'observation_period_num': 159, 'train_rates': 0.8917989416414049, 'learning_rate': 1.2227522298918164e-05, 'batch_size': 201, 'step_size': 4, 'gamma': 0.7893367326716195}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:47:13,594][0m Trial 40 finished with value: 0.06415883777570534 and parameters: {'observation_period_num': 33, 'train_rates': 0.8277665566243054, 'learning_rate': 6.526358923080285e-05, 'batch_size': 64, 'step_size': 3, 'gamma': 0.7716708286505203}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:48:46,647][0m Trial 41 finished with value: 0.04882818978700426 and parameters: {'observation_period_num': 16, 'train_rates': 0.8311338501951915, 'learning_rate': 5.0768836714048144e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.8264861011412064}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:51:40,533][0m Trial 42 finished with value: 0.03111818632734871 and parameters: {'observation_period_num': 19, 'train_rates': 0.8605812087311592, 'learning_rate': 0.00011176045451226508, 'batch_size': 31, 'step_size': 6, 'gamma': 0.8347205816379623}. Best is trial 12 with value: 0.029512436573350717.[0m
[32m[I 2025-01-03 06:54:54,901][0m Trial 43 finished with value: 0.02787755703186745 and parameters: {'observation_period_num': 5, 'train_rates': 0.8674334329829317, 'learning_rate': 0.0001725589922002092, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8391125274501056}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 06:55:33,297][0m Trial 44 finished with value: 0.04593292790754088 and parameters: {'observation_period_num': 21, 'train_rates': 0.8616582705096985, 'learning_rate': 0.00012532535192618003, 'batch_size': 157, 'step_size': 8, 'gamma': 0.8386521074736384}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 06:57:58,085][0m Trial 45 finished with value: 0.040289146602461655 and parameters: {'observation_period_num': 34, 'train_rates': 0.9172204092981793, 'learning_rate': 0.00018549370699300085, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8009902097486948}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 07:00:48,680][0m Trial 46 finished with value: 0.11714847393909092 and parameters: {'observation_period_num': 43, 'train_rates': 0.8085343297628728, 'learning_rate': 0.00023912695891664302, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9599420251831853}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 07:02:42,416][0m Trial 47 finished with value: 0.10195745990194123 and parameters: {'observation_period_num': 121, 'train_rates': 0.9591792045123414, 'learning_rate': 0.0004953757350668599, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8284709758469306}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 07:08:15,955][0m Trial 48 finished with value: 0.04501899275878799 and parameters: {'observation_period_num': 20, 'train_rates': 0.8729300012978246, 'learning_rate': 0.0001124098660159235, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8125467266641978}. Best is trial 43 with value: 0.02787755703186745.[0m
[32m[I 2025-01-03 07:09:36,831][0m Trial 49 finished with value: 0.051540237368948486 and parameters: {'observation_period_num': 59, 'train_rates': 0.8854897066616589, 'learning_rate': 7.343748245694821e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8646845561191212}. Best is trial 43 with value: 0.02787755703186745.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-03 07:09:36,841][0m A new study created in memory with name: no-name-e8cee86c-6804-482b-95c7-62b01f976914[0m
[32m[I 2025-01-03 07:10:19,884][0m Trial 0 finished with value: 0.41890502941440527 and parameters: {'observation_period_num': 50, 'train_rates': 0.6575745701141849, 'learning_rate': 3.3043537676084104e-06, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9063144591721743}. Best is trial 0 with value: 0.41890502941440527.[0m
[32m[I 2025-01-03 07:11:38,205][0m Trial 1 finished with value: 0.4818126997812007 and parameters: {'observation_period_num': 128, 'train_rates': 0.912887380581084, 'learning_rate': 1.919472223145136e-06, 'batch_size': 70, 'step_size': 4, 'gamma': 0.8800689632257739}. Best is trial 0 with value: 0.41890502941440527.[0m
[32m[I 2025-01-03 07:12:28,857][0m Trial 2 finished with value: 0.44008386104793873 and parameters: {'observation_period_num': 125, 'train_rates': 0.6870247589070275, 'learning_rate': 1.893723749634953e-05, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8115775727761485}. Best is trial 0 with value: 0.41890502941440527.[0m
[32m[I 2025-01-03 07:12:55,610][0m Trial 3 finished with value: 0.12995493412017822 and parameters: {'observation_period_num': 212, 'train_rates': 0.9421926292266712, 'learning_rate': 0.00025443487051952885, 'batch_size': 215, 'step_size': 6, 'gamma': 0.9656107352555621}. Best is trial 3 with value: 0.12995493412017822.[0m
[32m[I 2025-01-03 07:13:18,351][0m Trial 4 finished with value: 0.2796993815481908 and parameters: {'observation_period_num': 216, 'train_rates': 0.6446399931723744, 'learning_rate': 0.00016563803515473865, 'batch_size': 208, 'step_size': 14, 'gamma': 0.813752681764195}. Best is trial 3 with value: 0.12995493412017822.[0m
[32m[I 2025-01-03 07:13:44,926][0m Trial 5 finished with value: 0.6087485819340791 and parameters: {'observation_period_num': 234, 'train_rates': 0.6620595611811402, 'learning_rate': 3.344654972537703e-06, 'batch_size': 179, 'step_size': 12, 'gamma': 0.9565853762945733}. Best is trial 3 with value: 0.12995493412017822.[0m
[32m[I 2025-01-03 07:14:33,021][0m Trial 6 finished with value: 1.0257962705760166 and parameters: {'observation_period_num': 76, 'train_rates': 0.7579827244461974, 'learning_rate': 1.1174760341479946e-06, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8981653590878041}. Best is trial 3 with value: 0.12995493412017822.[0m
[32m[I 2025-01-03 07:15:15,002][0m Trial 7 finished with value: 0.15458624068083185 and parameters: {'observation_period_num': 22, 'train_rates': 0.7183165284278381, 'learning_rate': 0.00035847942042885756, 'batch_size': 118, 'step_size': 5, 'gamma': 0.8534282820772301}. Best is trial 3 with value: 0.12995493412017822.[0m
[32m[I 2025-01-03 07:17:00,794][0m Trial 8 finished with value: 0.09413168260029384 and parameters: {'observation_period_num': 166, 'train_rates': 0.9750020930564192, 'learning_rate': 7.385933266473418e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.8673194547763997}. Best is trial 8 with value: 0.09413168260029384.[0m
[32m[I 2025-01-03 07:19:38,854][0m Trial 9 finished with value: 0.23505247453537795 and parameters: {'observation_period_num': 224, 'train_rates': 0.6013695647144816, 'learning_rate': 0.00014362581429613086, 'batch_size': 25, 'step_size': 12, 'gamma': 0.7804033395787592}. Best is trial 8 with value: 0.09413168260029384.[0m
[32m[I 2025-01-03 07:22:41,796][0m Trial 10 finished with value: 0.07853774511282212 and parameters: {'observation_period_num': 173, 'train_rates': 0.8630669745803249, 'learning_rate': 2.9459736877785187e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.7522755697370012}. Best is trial 10 with value: 0.07853774511282212.[0m
[32m[I 2025-01-03 07:27:36,901][0m Trial 11 finished with value: 0.0666365111517964 and parameters: {'observation_period_num': 173, 'train_rates': 0.8517039147776649, 'learning_rate': 3.599593319461594e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7519323739089804}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:30:09,864][0m Trial 12 finished with value: 0.112590100198344 and parameters: {'observation_period_num': 168, 'train_rates': 0.8494262188749072, 'learning_rate': 1.4046340996185475e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.7584733034280748}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:34:29,292][0m Trial 13 finished with value: 0.0700125825335222 and parameters: {'observation_period_num': 170, 'train_rates': 0.8315627728756997, 'learning_rate': 3.9231888980563124e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.7530516002962021}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:35:00,930][0m Trial 14 finished with value: 0.3307310360967562 and parameters: {'observation_period_num': 95, 'train_rates': 0.8107496849444358, 'learning_rate': 8.889697163720934e-06, 'batch_size': 170, 'step_size': 7, 'gamma': 0.8031035208116812}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:35:22,346][0m Trial 15 finished with value: 0.17932696938514708 and parameters: {'observation_period_num': 190, 'train_rates': 0.7882711880653351, 'learning_rate': 5.4841477717148506e-05, 'batch_size': 252, 'step_size': 11, 'gamma': 0.7816553650944252}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:36:42,514][0m Trial 16 finished with value: 0.07395721521348722 and parameters: {'observation_period_num': 153, 'train_rates': 0.8673449969442253, 'learning_rate': 6.548623487487805e-05, 'batch_size': 66, 'step_size': 7, 'gamma': 0.8406311555669032}. Best is trial 11 with value: 0.0666365111517964.[0m
Early stopping at epoch 46
[32m[I 2025-01-03 07:37:00,339][0m Trial 17 finished with value: 1.332645923023942 and parameters: {'observation_period_num': 250, 'train_rates': 0.8231695330838277, 'learning_rate': 5.747835989274432e-06, 'batch_size': 143, 'step_size': 1, 'gamma': 0.7790829748882065}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:38:47,017][0m Trial 18 finished with value: 0.09406955303816959 and parameters: {'observation_period_num': 107, 'train_rates': 0.8980310797915152, 'learning_rate': 0.0008168358774066097, 'batch_size': 52, 'step_size': 10, 'gamma': 0.8319990028296033}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:39:45,139][0m Trial 19 finished with value: 0.25623756446875634 and parameters: {'observation_period_num': 144, 'train_rates': 0.771969154776703, 'learning_rate': 3.27274713628699e-05, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9348643391894781}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:41:28,888][0m Trial 20 finished with value: 0.430465476699377 and parameters: {'observation_period_num': 198, 'train_rates': 0.7343104812808054, 'learning_rate': 1.2917286751877626e-05, 'batch_size': 41, 'step_size': 7, 'gamma': 0.7502576280538308}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:45:22,013][0m Trial 21 finished with value: 0.08132348687581296 and parameters: {'observation_period_num': 153, 'train_rates': 0.8674338954290004, 'learning_rate': 7.258624413892834e-05, 'batch_size': 21, 'step_size': 7, 'gamma': 0.8403748850682345}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:46:40,127][0m Trial 22 finished with value: 0.10227782155076663 and parameters: {'observation_period_num': 144, 'train_rates': 0.8335538339655458, 'learning_rate': 3.612267692449062e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.7964954250799875}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:51:59,519][0m Trial 23 finished with value: 0.08320558674160054 and parameters: {'observation_period_num': 190, 'train_rates': 0.8918170877100848, 'learning_rate': 0.00010735482844010859, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7743108342268145}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:53:54,071][0m Trial 24 finished with value: 0.069769923522901 and parameters: {'observation_period_num': 115, 'train_rates': 0.937435713350281, 'learning_rate': 4.85175041585325e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.8244839184680002}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:55:49,623][0m Trial 25 finished with value: 0.09765735268592834 and parameters: {'observation_period_num': 101, 'train_rates': 0.988905862372802, 'learning_rate': 1.9712579075072833e-05, 'batch_size': 51, 'step_size': 11, 'gamma': 0.8232235157248764}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:57:00,199][0m Trial 26 finished with value: 0.0718331522373266 and parameters: {'observation_period_num': 76, 'train_rates': 0.9402986318104901, 'learning_rate': 4.2162204809443595e-05, 'batch_size': 84, 'step_size': 8, 'gamma': 0.7969858235388195}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 07:59:11,842][0m Trial 27 finished with value: 0.14202687785956278 and parameters: {'observation_period_num': 119, 'train_rates': 0.9263936666561138, 'learning_rate': 7.171589893070202e-06, 'batch_size': 42, 'step_size': 10, 'gamma': 0.7666849714571586}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 08:04:26,529][0m Trial 28 finished with value: 0.07509464894731839 and parameters: {'observation_period_num': 73, 'train_rates': 0.9625341395827613, 'learning_rate': 2.2064293859835392e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.791195376347396}. Best is trial 11 with value: 0.0666365111517964.[0m
[32m[I 2025-01-03 08:05:07,227][0m Trial 29 finished with value: 0.04575221377176855 and parameters: {'observation_period_num': 23, 'train_rates': 0.7963469620001971, 'learning_rate': 0.00010485724787718224, 'batch_size': 132, 'step_size': 10, 'gamma': 0.9332445661524655}. Best is trial 29 with value: 0.04575221377176855.[0m
[32m[I 2025-01-03 08:05:48,106][0m Trial 30 finished with value: 0.03266228027641773 and parameters: {'observation_period_num': 12, 'train_rates': 0.8043770546979335, 'learning_rate': 0.0003508854496808343, 'batch_size': 132, 'step_size': 13, 'gamma': 0.9231027365150155}. Best is trial 30 with value: 0.03266228027641773.[0m
[32m[I 2025-01-03 08:06:24,994][0m Trial 31 finished with value: 0.1724625483885245 and parameters: {'observation_period_num': 19, 'train_rates': 0.7808675209474158, 'learning_rate': 0.0005720214846424009, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9214669045318792}. Best is trial 30 with value: 0.03266228027641773.[0m
[32m[I 2025-01-03 08:07:12,908][0m Trial 32 finished with value: 0.0451720032384092 and parameters: {'observation_period_num': 38, 'train_rates': 0.8910044458233622, 'learning_rate': 0.0002741610298353051, 'batch_size': 119, 'step_size': 10, 'gamma': 0.887672889973069}. Best is trial 30 with value: 0.03266228027641773.[0m
[32m[I 2025-01-03 08:07:59,734][0m Trial 33 finished with value: 0.03131525322470976 and parameters: {'observation_period_num': 5, 'train_rates': 0.8049383460804654, 'learning_rate': 0.0003513072544073003, 'batch_size': 117, 'step_size': 13, 'gamma': 0.9018413476621423}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:08:41,363][0m Trial 34 finished with value: 0.15025352548028148 and parameters: {'observation_period_num': 6, 'train_rates': 0.7372220850945477, 'learning_rate': 0.00029574560221615025, 'batch_size': 126, 'step_size': 14, 'gamma': 0.8953653192325784}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:09:15,450][0m Trial 35 finished with value: 0.049840563486985096 and parameters: {'observation_period_num': 45, 'train_rates': 0.80531453912209, 'learning_rate': 0.0005113310272445335, 'batch_size': 161, 'step_size': 12, 'gamma': 0.9854030005049647}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:10:01,984][0m Trial 36 finished with value: 0.1568183166601912 and parameters: {'observation_period_num': 39, 'train_rates': 0.7038167289572016, 'learning_rate': 0.0001924582034291556, 'batch_size': 107, 'step_size': 15, 'gamma': 0.9266158673172284}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:10:42,878][0m Trial 37 finished with value: 0.18665370106914617 and parameters: {'observation_period_num': 30, 'train_rates': 0.7655869642322274, 'learning_rate': 0.0003925118392333318, 'batch_size': 128, 'step_size': 13, 'gamma': 0.9468237582969159}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:11:42,815][0m Trial 38 finished with value: 0.03640605799548258 and parameters: {'observation_period_num': 9, 'train_rates': 0.8925534027793097, 'learning_rate': 0.000891663457111718, 'batch_size': 97, 'step_size': 14, 'gamma': 0.882980814678868}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:12:43,594][0m Trial 39 finished with value: 0.06028646617284361 and parameters: {'observation_period_num': 57, 'train_rates': 0.9084267447691659, 'learning_rate': 0.000971199981422622, 'batch_size': 95, 'step_size': 14, 'gamma': 0.8820195979855675}. Best is trial 33 with value: 0.03131525322470976.[0m
[32m[I 2025-01-03 08:13:41,259][0m Trial 40 finished with value: 0.030430668204493777 and parameters: {'observation_period_num': 6, 'train_rates': 0.8853292084440536, 'learning_rate': 0.000609398953927698, 'batch_size': 100, 'step_size': 14, 'gamma': 0.911119954547612}. Best is trial 40 with value: 0.030430668204493777.[0m
[32m[I 2025-01-03 08:14:38,085][0m Trial 41 finished with value: 0.030428317240066272 and parameters: {'observation_period_num': 5, 'train_rates': 0.8857117097861374, 'learning_rate': 0.0006429126970534857, 'batch_size': 102, 'step_size': 14, 'gamma': 0.9132654733444099}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:15:32,448][0m Trial 42 finished with value: 0.0449128819056038 and parameters: {'observation_period_num': 7, 'train_rates': 0.8753892115893329, 'learning_rate': 0.0006652772599641464, 'batch_size': 105, 'step_size': 14, 'gamma': 0.9143344514527941}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:16:34,442][0m Trial 43 finished with value: 0.08220383341091846 and parameters: {'observation_period_num': 58, 'train_rates': 0.9216696132934021, 'learning_rate': 0.0004225113953439118, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9012806342143084}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:17:41,466][0m Trial 44 finished with value: 0.03635360214479445 and parameters: {'observation_period_num': 12, 'train_rates': 0.8434078676139676, 'learning_rate': 0.0006845571599950154, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8617262042846666}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:18:19,052][0m Trial 45 finished with value: 0.03542742560979581 and parameters: {'observation_period_num': 18, 'train_rates': 0.843795911987513, 'learning_rate': 0.00019024741152991488, 'batch_size': 153, 'step_size': 15, 'gamma': 0.8622743223444898}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:18:48,702][0m Trial 46 finished with value: 0.04529326060291533 and parameters: {'observation_period_num': 31, 'train_rates': 0.8174009109550366, 'learning_rate': 0.0001761475266690239, 'batch_size': 184, 'step_size': 12, 'gamma': 0.9055369090947913}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:19:18,442][0m Trial 47 finished with value: 0.03887054479795776 and parameters: {'observation_period_num': 18, 'train_rates': 0.8508638812588757, 'learning_rate': 0.00021074732496328, 'batch_size': 197, 'step_size': 15, 'gamma': 0.9503171771834225}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:19:50,882][0m Trial 48 finished with value: 0.2035485368928951 and parameters: {'observation_period_num': 54, 'train_rates': 0.743481259416645, 'learning_rate': 0.00032972264259418924, 'batch_size': 157, 'step_size': 13, 'gamma': 0.8715138311087826}. Best is trial 41 with value: 0.030428317240066272.[0m
[32m[I 2025-01-03 08:20:32,919][0m Trial 49 finished with value: 0.04324089404380783 and parameters: {'observation_period_num': 29, 'train_rates': 0.8761450523858965, 'learning_rate': 0.0001211254198785676, 'batch_size': 142, 'step_size': 12, 'gamma': 0.9132721386698398}. Best is trial 41 with value: 0.030428317240066272.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 19, 'train_rates': 0.8390281360342446, 'learning_rate': 0.0006221488770577347, 'batch_size': 169, 'step_size': 11, 'gamma': 0.7807568611860005}
Epoch 1/300, trend Loss: 0.3714 | 0.3122
Epoch 2/300, trend Loss: 0.1973 | 0.1474
Epoch 3/300, trend Loss: 0.1712 | 0.1257
Epoch 4/300, trend Loss: 0.1894 | 0.1285
Epoch 5/300, trend Loss: 0.1928 | 0.1836
Epoch 6/300, trend Loss: 0.1564 | 0.1086
Epoch 7/300, trend Loss: 0.1521 | 0.1282
Epoch 8/300, trend Loss: 0.1621 | 0.2406
Epoch 9/300, trend Loss: 0.1581 | 0.0952
Epoch 10/300, trend Loss: 0.1351 | 0.0829
Epoch 11/300, trend Loss: 0.1712 | 0.1058
Epoch 12/300, trend Loss: 0.2592 | 0.1271
Epoch 13/300, trend Loss: 0.1722 | 0.0810
Epoch 14/300, trend Loss: 0.1315 | 0.0641
Epoch 15/300, trend Loss: 0.1269 | 0.0663
Epoch 16/300, trend Loss: 0.1201 | 0.0617
Epoch 17/300, trend Loss: 0.1217 | 0.0615
Epoch 18/300, trend Loss: 0.1125 | 0.0640
Epoch 19/300, trend Loss: 0.1130 | 0.0569
Epoch 20/300, trend Loss: 0.1058 | 0.0621
Epoch 21/300, trend Loss: 0.1019 | 0.0577
Epoch 22/300, trend Loss: 0.1001 | 0.0537
Epoch 23/300, trend Loss: 0.0974 | 0.0589
Epoch 24/300, trend Loss: 0.0956 | 0.0570
Epoch 25/300, trend Loss: 0.0946 | 0.0534
Epoch 26/300, trend Loss: 0.0934 | 0.0536
Epoch 27/300, trend Loss: 0.0924 | 0.0510
Epoch 28/300, trend Loss: 0.0914 | 0.0502
Epoch 29/300, trend Loss: 0.0904 | 0.0542
Epoch 30/300, trend Loss: 0.0896 | 0.0513
Epoch 31/300, trend Loss: 0.0887 | 0.0489
Epoch 32/300, trend Loss: 0.0878 | 0.0483
Epoch 33/300, trend Loss: 0.0872 | 0.0477
Epoch 34/300, trend Loss: 0.0864 | 0.0498
Epoch 35/300, trend Loss: 0.0857 | 0.0473
Epoch 36/300, trend Loss: 0.0851 | 0.0453
Epoch 37/300, trend Loss: 0.0844 | 0.0462
Epoch 38/300, trend Loss: 0.0839 | 0.0472
Epoch 39/300, trend Loss: 0.0832 | 0.0453
Epoch 40/300, trend Loss: 0.0827 | 0.0438
Epoch 41/300, trend Loss: 0.0826 | 0.0438
Epoch 42/300, trend Loss: 0.0823 | 0.0443
Epoch 43/300, trend Loss: 0.0817 | 0.0434
Epoch 44/300, trend Loss: 0.0812 | 0.0421
Epoch 45/300, trend Loss: 0.0811 | 0.0426
Epoch 46/300, trend Loss: 0.0812 | 0.0432
Epoch 47/300, trend Loss: 0.0805 | 0.0420
Epoch 48/300, trend Loss: 0.0797 | 0.0423
Epoch 49/300, trend Loss: 0.0792 | 0.0426
Epoch 50/300, trend Loss: 0.0789 | 0.0417
Epoch 51/300, trend Loss: 0.0786 | 0.0413
Epoch 52/300, trend Loss: 0.0784 | 0.0415
Epoch 53/300, trend Loss: 0.0782 | 0.0412
Epoch 54/300, trend Loss: 0.0779 | 0.0411
Epoch 55/300, trend Loss: 0.0777 | 0.0410
Epoch 56/300, trend Loss: 0.0775 | 0.0407
Epoch 57/300, trend Loss: 0.0773 | 0.0406
Epoch 58/300, trend Loss: 0.0771 | 0.0406
Epoch 59/300, trend Loss: 0.0769 | 0.0404
Epoch 60/300, trend Loss: 0.0768 | 0.0403
Epoch 61/300, trend Loss: 0.0766 | 0.0402
Epoch 62/300, trend Loss: 0.0764 | 0.0400
Epoch 63/300, trend Loss: 0.0763 | 0.0399
Epoch 64/300, trend Loss: 0.0761 | 0.0399
Epoch 65/300, trend Loss: 0.0760 | 0.0397
Epoch 66/300, trend Loss: 0.0759 | 0.0397
Epoch 67/300, trend Loss: 0.0757 | 0.0395
Epoch 68/300, trend Loss: 0.0756 | 0.0395
Epoch 69/300, trend Loss: 0.0755 | 0.0394
Epoch 70/300, trend Loss: 0.0754 | 0.0394
Epoch 71/300, trend Loss: 0.0753 | 0.0392
Epoch 72/300, trend Loss: 0.0752 | 0.0393
Epoch 73/300, trend Loss: 0.0751 | 0.0391
Epoch 74/300, trend Loss: 0.0750 | 0.0391
Epoch 75/300, trend Loss: 0.0749 | 0.0389
Epoch 76/300, trend Loss: 0.0748 | 0.0391
Epoch 77/300, trend Loss: 0.0748 | 0.0387
Epoch 78/300, trend Loss: 0.0747 | 0.0388
Epoch 79/300, trend Loss: 0.0746 | 0.0389
Epoch 80/300, trend Loss: 0.0747 | 0.0387
Epoch 81/300, trend Loss: 0.0746 | 0.0389
Epoch 82/300, trend Loss: 0.0747 | 0.0385
Epoch 83/300, trend Loss: 0.0748 | 0.0393
Epoch 84/300, trend Loss: 0.0748 | 0.0377
Epoch 85/300, trend Loss: 0.0746 | 0.0406
Epoch 86/300, trend Loss: 0.0745 | 0.0376
Epoch 87/300, trend Loss: 0.0743 | 0.0400
Epoch 88/300, trend Loss: 0.0742 | 0.0377
Epoch 89/300, trend Loss: 0.0741 | 0.0394
Epoch 90/300, trend Loss: 0.0740 | 0.0380
Epoch 91/300, trend Loss: 0.0739 | 0.0386
Epoch 92/300, trend Loss: 0.0739 | 0.0383
Epoch 93/300, trend Loss: 0.0738 | 0.0383
Epoch 94/300, trend Loss: 0.0738 | 0.0383
Epoch 95/300, trend Loss: 0.0738 | 0.0383
Epoch 96/300, trend Loss: 0.0737 | 0.0382
Epoch 97/300, trend Loss: 0.0737 | 0.0382
Epoch 98/300, trend Loss: 0.0737 | 0.0382
Epoch 99/300, trend Loss: 0.0736 | 0.0382
Epoch 100/300, trend Loss: 0.0736 | 0.0382
Epoch 101/300, trend Loss: 0.0736 | 0.0381
Epoch 102/300, trend Loss: 0.0735 | 0.0381
Epoch 103/300, trend Loss: 0.0735 | 0.0381
Epoch 104/300, trend Loss: 0.0735 | 0.0381
Epoch 105/300, trend Loss: 0.0735 | 0.0381
Epoch 106/300, trend Loss: 0.0734 | 0.0381
Epoch 107/300, trend Loss: 0.0734 | 0.0380
Epoch 108/300, trend Loss: 0.0734 | 0.0380
Epoch 109/300, trend Loss: 0.0734 | 0.0380
Epoch 110/300, trend Loss: 0.0734 | 0.0380
Epoch 111/300, trend Loss: 0.0733 | 0.0380
Epoch 112/300, trend Loss: 0.0733 | 0.0380
Epoch 113/300, trend Loss: 0.0733 | 0.0380
Epoch 114/300, trend Loss: 0.0733 | 0.0380
Epoch 115/300, trend Loss: 0.0733 | 0.0379
Epoch 116/300, trend Loss: 0.0733 | 0.0379
Epoch 117/300, trend Loss: 0.0732 | 0.0379
Epoch 118/300, trend Loss: 0.0732 | 0.0379
Epoch 119/300, trend Loss: 0.0732 | 0.0379
Epoch 120/300, trend Loss: 0.0732 | 0.0379
Epoch 121/300, trend Loss: 0.0732 | 0.0379
Epoch 122/300, trend Loss: 0.0732 | 0.0379
Epoch 123/300, trend Loss: 0.0732 | 0.0379
Epoch 124/300, trend Loss: 0.0731 | 0.0379
Epoch 125/300, trend Loss: 0.0731 | 0.0379
Epoch 126/300, trend Loss: 0.0731 | 0.0379
Epoch 127/300, trend Loss: 0.0731 | 0.0378
Epoch 128/300, trend Loss: 0.0731 | 0.0378
Epoch 129/300, trend Loss: 0.0731 | 0.0378
Epoch 130/300, trend Loss: 0.0731 | 0.0378
Epoch 131/300, trend Loss: 0.0731 | 0.0378
Epoch 132/300, trend Loss: 0.0731 | 0.0378
Epoch 133/300, trend Loss: 0.0731 | 0.0378
Epoch 134/300, trend Loss: 0.0731 | 0.0378
Epoch 135/300, trend Loss: 0.0731 | 0.0378
Epoch 136/300, trend Loss: 0.0730 | 0.0378
Epoch 137/300, trend Loss: 0.0730 | 0.0378
Epoch 138/300, trend Loss: 0.0730 | 0.0378
Epoch 139/300, trend Loss: 0.0730 | 0.0378
Epoch 140/300, trend Loss: 0.0730 | 0.0378
Epoch 141/300, trend Loss: 0.0730 | 0.0378
Epoch 142/300, trend Loss: 0.0730 | 0.0378
Epoch 143/300, trend Loss: 0.0730 | 0.0378
Epoch 144/300, trend Loss: 0.0730 | 0.0378
Epoch 145/300, trend Loss: 0.0730 | 0.0378
Epoch 146/300, trend Loss: 0.0730 | 0.0378
Epoch 147/300, trend Loss: 0.0730 | 0.0378
Epoch 148/300, trend Loss: 0.0730 | 0.0378
Epoch 149/300, trend Loss: 0.0730 | 0.0378
Epoch 150/300, trend Loss: 0.0730 | 0.0378
Epoch 151/300, trend Loss: 0.0730 | 0.0377
Epoch 152/300, trend Loss: 0.0730 | 0.0377
Epoch 153/300, trend Loss: 0.0730 | 0.0377
Epoch 154/300, trend Loss: 0.0730 | 0.0377
Epoch 155/300, trend Loss: 0.0730 | 0.0377
Epoch 156/300, trend Loss: 0.0730 | 0.0377
Epoch 157/300, trend Loss: 0.0730 | 0.0377
Epoch 158/300, trend Loss: 0.0730 | 0.0377
Epoch 159/300, trend Loss: 0.0730 | 0.0377
Epoch 160/300, trend Loss: 0.0729 | 0.0377
Epoch 161/300, trend Loss: 0.0729 | 0.0377
Epoch 162/300, trend Loss: 0.0729 | 0.0377
Epoch 163/300, trend Loss: 0.0729 | 0.0377
Epoch 164/300, trend Loss: 0.0729 | 0.0377
Epoch 165/300, trend Loss: 0.0729 | 0.0377
Epoch 166/300, trend Loss: 0.0729 | 0.0377
Epoch 167/300, trend Loss: 0.0729 | 0.0377
Epoch 168/300, trend Loss: 0.0729 | 0.0377
Epoch 169/300, trend Loss: 0.0729 | 0.0377
Epoch 170/300, trend Loss: 0.0729 | 0.0377
Epoch 171/300, trend Loss: 0.0729 | 0.0377
Epoch 172/300, trend Loss: 0.0729 | 0.0377
Epoch 173/300, trend Loss: 0.0729 | 0.0377
Epoch 174/300, trend Loss: 0.0729 | 0.0377
Epoch 175/300, trend Loss: 0.0729 | 0.0377
Epoch 176/300, trend Loss: 0.0729 | 0.0377
Epoch 177/300, trend Loss: 0.0729 | 0.0377
Epoch 178/300, trend Loss: 0.0729 | 0.0377
Epoch 179/300, trend Loss: 0.0729 | 0.0377
Epoch 180/300, trend Loss: 0.0729 | 0.0377
Epoch 181/300, trend Loss: 0.0729 | 0.0377
Epoch 182/300, trend Loss: 0.0729 | 0.0377
Epoch 183/300, trend Loss: 0.0729 | 0.0377
Epoch 184/300, trend Loss: 0.0729 | 0.0377
Epoch 185/300, trend Loss: 0.0729 | 0.0377
Epoch 186/300, trend Loss: 0.0729 | 0.0377
Epoch 187/300, trend Loss: 0.0729 | 0.0377
Epoch 188/300, trend Loss: 0.0729 | 0.0377
Epoch 189/300, trend Loss: 0.0729 | 0.0377
Epoch 190/300, trend Loss: 0.0729 | 0.0377
Epoch 191/300, trend Loss: 0.0729 | 0.0377
Epoch 192/300, trend Loss: 0.0729 | 0.0377
Epoch 193/300, trend Loss: 0.0729 | 0.0377
Epoch 194/300, trend Loss: 0.0729 | 0.0377
Epoch 195/300, trend Loss: 0.0729 | 0.0377
Epoch 196/300, trend Loss: 0.0729 | 0.0377
Epoch 197/300, trend Loss: 0.0729 | 0.0377
Epoch 198/300, trend Loss: 0.0729 | 0.0377
Epoch 199/300, trend Loss: 0.0729 | 0.0377
Epoch 200/300, trend Loss: 0.0729 | 0.0377
Epoch 201/300, trend Loss: 0.0729 | 0.0377
Epoch 202/300, trend Loss: 0.0729 | 0.0377
Epoch 203/300, trend Loss: 0.0729 | 0.0377
Epoch 204/300, trend Loss: 0.0729 | 0.0377
Epoch 205/300, trend Loss: 0.0729 | 0.0377
Epoch 206/300, trend Loss: 0.0729 | 0.0377
Epoch 207/300, trend Loss: 0.0729 | 0.0377
Epoch 208/300, trend Loss: 0.0729 | 0.0377
Epoch 209/300, trend Loss: 0.0729 | 0.0377
Epoch 210/300, trend Loss: 0.0729 | 0.0377
Epoch 211/300, trend Loss: 0.0729 | 0.0377
Epoch 212/300, trend Loss: 0.0729 | 0.0377
Epoch 213/300, trend Loss: 0.0729 | 0.0377
Epoch 214/300, trend Loss: 0.0729 | 0.0377
Epoch 215/300, trend Loss: 0.0729 | 0.0377
Epoch 216/300, trend Loss: 0.0729 | 0.0377
Epoch 217/300, trend Loss: 0.0729 | 0.0377
Epoch 218/300, trend Loss: 0.0729 | 0.0377
Epoch 219/300, trend Loss: 0.0729 | 0.0377
Epoch 220/300, trend Loss: 0.0729 | 0.0377
Epoch 221/300, trend Loss: 0.0729 | 0.0377
Epoch 222/300, trend Loss: 0.0729 | 0.0377
Epoch 223/300, trend Loss: 0.0729 | 0.0377
Epoch 224/300, trend Loss: 0.0729 | 0.0377
Epoch 225/300, trend Loss: 0.0729 | 0.0377
Epoch 226/300, trend Loss: 0.0729 | 0.0377
Epoch 227/300, trend Loss: 0.0729 | 0.0377
Epoch 228/300, trend Loss: 0.0729 | 0.0377
Epoch 229/300, trend Loss: 0.0729 | 0.0377
Epoch 230/300, trend Loss: 0.0729 | 0.0377
Epoch 231/300, trend Loss: 0.0729 | 0.0377
Epoch 232/300, trend Loss: 0.0729 | 0.0377
Epoch 233/300, trend Loss: 0.0729 | 0.0377
Epoch 234/300, trend Loss: 0.0729 | 0.0377
Epoch 235/300, trend Loss: 0.0729 | 0.0377
Epoch 236/300, trend Loss: 0.0729 | 0.0377
Epoch 237/300, trend Loss: 0.0729 | 0.0377
Epoch 238/300, trend Loss: 0.0729 | 0.0377
Epoch 239/300, trend Loss: 0.0729 | 0.0377
Epoch 240/300, trend Loss: 0.0729 | 0.0377
Epoch 241/300, trend Loss: 0.0729 | 0.0377
Epoch 242/300, trend Loss: 0.0729 | 0.0377
Epoch 243/300, trend Loss: 0.0729 | 0.0377
Epoch 244/300, trend Loss: 0.0729 | 0.0377
Epoch 245/300, trend Loss: 0.0729 | 0.0377
Epoch 246/300, trend Loss: 0.0729 | 0.0377
Epoch 247/300, trend Loss: 0.0729 | 0.0377
Epoch 248/300, trend Loss: 0.0729 | 0.0377
Epoch 249/300, trend Loss: 0.0729 | 0.0377
Epoch 250/300, trend Loss: 0.0729 | 0.0377
Epoch 251/300, trend Loss: 0.0729 | 0.0377
Epoch 252/300, trend Loss: 0.0729 | 0.0377
Epoch 253/300, trend Loss: 0.0729 | 0.0377
Epoch 254/300, trend Loss: 0.0729 | 0.0377
Epoch 255/300, trend Loss: 0.0729 | 0.0377
Epoch 256/300, trend Loss: 0.0729 | 0.0377
Epoch 257/300, trend Loss: 0.0729 | 0.0377
Epoch 258/300, trend Loss: 0.0729 | 0.0377
Epoch 259/300, trend Loss: 0.0729 | 0.0377
Epoch 260/300, trend Loss: 0.0729 | 0.0377
Epoch 261/300, trend Loss: 0.0729 | 0.0377
Epoch 262/300, trend Loss: 0.0729 | 0.0377
Epoch 263/300, trend Loss: 0.0729 | 0.0377
Epoch 264/300, trend Loss: 0.0729 | 0.0377
Epoch 265/300, trend Loss: 0.0729 | 0.0377
Epoch 266/300, trend Loss: 0.0729 | 0.0377
Epoch 267/300, trend Loss: 0.0729 | 0.0377
Epoch 268/300, trend Loss: 0.0729 | 0.0377
Epoch 269/300, trend Loss: 0.0729 | 0.0377
Epoch 270/300, trend Loss: 0.0729 | 0.0377
Epoch 271/300, trend Loss: 0.0729 | 0.0377
Epoch 272/300, trend Loss: 0.0729 | 0.0377
Epoch 273/300, trend Loss: 0.0729 | 0.0377
Epoch 274/300, trend Loss: 0.0729 | 0.0377
Epoch 275/300, trend Loss: 0.0729 | 0.0377
Epoch 276/300, trend Loss: 0.0729 | 0.0377
Epoch 277/300, trend Loss: 0.0729 | 0.0377
Epoch 278/300, trend Loss: 0.0729 | 0.0377
Epoch 279/300, trend Loss: 0.0729 | 0.0377
Epoch 280/300, trend Loss: 0.0729 | 0.0377
Epoch 281/300, trend Loss: 0.0729 | 0.0377
Epoch 282/300, trend Loss: 0.0729 | 0.0377
Epoch 283/300, trend Loss: 0.0729 | 0.0377
Epoch 284/300, trend Loss: 0.0729 | 0.0377
Epoch 285/300, trend Loss: 0.0729 | 0.0377
Epoch 286/300, trend Loss: 0.0729 | 0.0377
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 44, 'train_rates': 0.8746103296204295, 'learning_rate': 0.000494603421847704, 'batch_size': 151, 'step_size': 10, 'gamma': 0.8083422607629602}
Epoch 1/300, seasonal_0 Loss: 0.4715 | 0.3339
Epoch 2/300, seasonal_0 Loss: 0.2660 | 0.2092
Epoch 3/300, seasonal_0 Loss: 0.2516 | 0.1570
Epoch 4/300, seasonal_0 Loss: 0.2934 | 0.1703
Epoch 5/300, seasonal_0 Loss: 0.2785 | 0.1182
Epoch 6/300, seasonal_0 Loss: 0.2733 | 0.3388
Epoch 7/300, seasonal_0 Loss: 0.1792 | 0.1532
Epoch 8/300, seasonal_0 Loss: 0.1712 | 0.1657
Epoch 9/300, seasonal_0 Loss: 0.1469 | 0.1023
Epoch 10/300, seasonal_0 Loss: 0.1460 | 0.0915
Epoch 11/300, seasonal_0 Loss: 0.1310 | 0.0864
Epoch 12/300, seasonal_0 Loss: 0.1164 | 0.0979
Epoch 13/300, seasonal_0 Loss: 0.1156 | 0.1228
Epoch 14/300, seasonal_0 Loss: 0.1158 | 0.1333
Epoch 15/300, seasonal_0 Loss: 0.1104 | 0.1160
Epoch 16/300, seasonal_0 Loss: 0.1047 | 0.0761
Epoch 17/300, seasonal_0 Loss: 0.1105 | 0.0844
Epoch 18/300, seasonal_0 Loss: 0.1155 | 0.0839
Epoch 19/300, seasonal_0 Loss: 0.1056 | 0.0826
Epoch 20/300, seasonal_0 Loss: 0.1021 | 0.1125
Epoch 21/300, seasonal_0 Loss: 0.1014 | 0.0836
Epoch 22/300, seasonal_0 Loss: 0.0969 | 0.0691
Epoch 23/300, seasonal_0 Loss: 0.0943 | 0.0651
Epoch 24/300, seasonal_0 Loss: 0.0923 | 0.0635
Epoch 25/300, seasonal_0 Loss: 0.0903 | 0.0604
Epoch 26/300, seasonal_0 Loss: 0.0880 | 0.0637
Epoch 27/300, seasonal_0 Loss: 0.0889 | 0.0630
Epoch 28/300, seasonal_0 Loss: 0.0883 | 0.0789
Epoch 29/300, seasonal_0 Loss: 0.0881 | 0.0878
Epoch 30/300, seasonal_0 Loss: 0.0868 | 0.0609
Epoch 31/300, seasonal_0 Loss: 0.0867 | 0.0583
Epoch 32/300, seasonal_0 Loss: 0.0854 | 0.0577
Epoch 33/300, seasonal_0 Loss: 0.0846 | 0.0571
Epoch 34/300, seasonal_0 Loss: 0.0823 | 0.0576
Epoch 35/300, seasonal_0 Loss: 0.0840 | 0.0585
Epoch 36/300, seasonal_0 Loss: 0.0823 | 0.0645
Epoch 37/300, seasonal_0 Loss: 0.0821 | 0.0572
Epoch 38/300, seasonal_0 Loss: 0.0807 | 0.0567
Epoch 39/300, seasonal_0 Loss: 0.0809 | 0.0559
Epoch 40/300, seasonal_0 Loss: 0.0797 | 0.0606
Epoch 41/300, seasonal_0 Loss: 0.0792 | 0.0562
Epoch 42/300, seasonal_0 Loss: 0.0789 | 0.0540
Epoch 43/300, seasonal_0 Loss: 0.0780 | 0.0557
Epoch 44/300, seasonal_0 Loss: 0.0777 | 0.0541
Epoch 45/300, seasonal_0 Loss: 0.0769 | 0.0535
Epoch 46/300, seasonal_0 Loss: 0.0773 | 0.0556
Epoch 47/300, seasonal_0 Loss: 0.0762 | 0.0539
Epoch 48/300, seasonal_0 Loss: 0.0759 | 0.0528
Epoch 49/300, seasonal_0 Loss: 0.0759 | 0.0556
Epoch 50/300, seasonal_0 Loss: 0.0750 | 0.0539
Epoch 51/300, seasonal_0 Loss: 0.0746 | 0.0523
Epoch 52/300, seasonal_0 Loss: 0.0744 | 0.0526
Epoch 53/300, seasonal_0 Loss: 0.0741 | 0.0535
Epoch 54/300, seasonal_0 Loss: 0.0738 | 0.0516
Epoch 55/300, seasonal_0 Loss: 0.0735 | 0.0526
Epoch 56/300, seasonal_0 Loss: 0.0733 | 0.0518
Epoch 57/300, seasonal_0 Loss: 0.0730 | 0.0518
Epoch 58/300, seasonal_0 Loss: 0.0729 | 0.0515
Epoch 59/300, seasonal_0 Loss: 0.0727 | 0.0519
Epoch 60/300, seasonal_0 Loss: 0.0725 | 0.0511
Epoch 61/300, seasonal_0 Loss: 0.0723 | 0.0514
Epoch 62/300, seasonal_0 Loss: 0.0721 | 0.0509
Epoch 63/300, seasonal_0 Loss: 0.0720 | 0.0512
Epoch 64/300, seasonal_0 Loss: 0.0718 | 0.0507
Epoch 65/300, seasonal_0 Loss: 0.0717 | 0.0510
Epoch 66/300, seasonal_0 Loss: 0.0715 | 0.0505
Epoch 67/300, seasonal_0 Loss: 0.0714 | 0.0507
Epoch 68/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 69/300, seasonal_0 Loss: 0.0712 | 0.0506
Epoch 70/300, seasonal_0 Loss: 0.0710 | 0.0502
Epoch 71/300, seasonal_0 Loss: 0.0709 | 0.0503
Epoch 72/300, seasonal_0 Loss: 0.0708 | 0.0500
Epoch 73/300, seasonal_0 Loss: 0.0707 | 0.0503
Epoch 74/300, seasonal_0 Loss: 0.0706 | 0.0498
Epoch 75/300, seasonal_0 Loss: 0.0705 | 0.0502
Epoch 76/300, seasonal_0 Loss: 0.0704 | 0.0496
Epoch 77/300, seasonal_0 Loss: 0.0704 | 0.0500
Epoch 78/300, seasonal_0 Loss: 0.0703 | 0.0495
Epoch 79/300, seasonal_0 Loss: 0.0702 | 0.0501
Epoch 80/300, seasonal_0 Loss: 0.0702 | 0.0492
Epoch 81/300, seasonal_0 Loss: 0.0701 | 0.0499
Epoch 82/300, seasonal_0 Loss: 0.0701 | 0.0494
Epoch 83/300, seasonal_0 Loss: 0.0701 | 0.0498
Epoch 84/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 85/300, seasonal_0 Loss: 0.0701 | 0.0499
Epoch 86/300, seasonal_0 Loss: 0.0701 | 0.0499
Epoch 87/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 88/300, seasonal_0 Loss: 0.0700 | 0.0500
Epoch 89/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 90/300, seasonal_0 Loss: 0.0699 | 0.0501
Epoch 91/300, seasonal_0 Loss: 0.0697 | 0.0488
Epoch 92/300, seasonal_0 Loss: 0.0696 | 0.0503
Epoch 93/300, seasonal_0 Loss: 0.0695 | 0.0487
Epoch 94/300, seasonal_0 Loss: 0.0694 | 0.0498
Epoch 95/300, seasonal_0 Loss: 0.0693 | 0.0487
Epoch 96/300, seasonal_0 Loss: 0.0692 | 0.0495
Epoch 97/300, seasonal_0 Loss: 0.0692 | 0.0488
Epoch 98/300, seasonal_0 Loss: 0.0691 | 0.0492
Epoch 99/300, seasonal_0 Loss: 0.0691 | 0.0489
Epoch 100/300, seasonal_0 Loss: 0.0691 | 0.0490
Epoch 101/300, seasonal_0 Loss: 0.0690 | 0.0489
Epoch 102/300, seasonal_0 Loss: 0.0690 | 0.0489
Epoch 103/300, seasonal_0 Loss: 0.0690 | 0.0489
Epoch 104/300, seasonal_0 Loss: 0.0689 | 0.0489
Epoch 105/300, seasonal_0 Loss: 0.0689 | 0.0488
Epoch 106/300, seasonal_0 Loss: 0.0689 | 0.0488
Epoch 107/300, seasonal_0 Loss: 0.0689 | 0.0488
Epoch 108/300, seasonal_0 Loss: 0.0688 | 0.0488
Epoch 109/300, seasonal_0 Loss: 0.0688 | 0.0488
Epoch 110/300, seasonal_0 Loss: 0.0688 | 0.0487
Epoch 111/300, seasonal_0 Loss: 0.0688 | 0.0487
Epoch 112/300, seasonal_0 Loss: 0.0687 | 0.0487
Epoch 113/300, seasonal_0 Loss: 0.0687 | 0.0487
Epoch 114/300, seasonal_0 Loss: 0.0687 | 0.0487
Epoch 115/300, seasonal_0 Loss: 0.0687 | 0.0487
Epoch 116/300, seasonal_0 Loss: 0.0687 | 0.0487
Epoch 117/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 118/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 119/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 120/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 121/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 122/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 123/300, seasonal_0 Loss: 0.0686 | 0.0486
Epoch 124/300, seasonal_0 Loss: 0.0685 | 0.0486
Epoch 125/300, seasonal_0 Loss: 0.0685 | 0.0486
Epoch 126/300, seasonal_0 Loss: 0.0685 | 0.0486
Epoch 127/300, seasonal_0 Loss: 0.0685 | 0.0485
Epoch 128/300, seasonal_0 Loss: 0.0685 | 0.0485
Epoch 129/300, seasonal_0 Loss: 0.0685 | 0.0485
Epoch 130/300, seasonal_0 Loss: 0.0685 | 0.0485
Epoch 131/300, seasonal_0 Loss: 0.0685 | 0.0485
Epoch 132/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 133/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 134/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 135/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 136/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 137/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 138/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 139/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 140/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 141/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 142/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 143/300, seasonal_0 Loss: 0.0684 | 0.0485
Epoch 144/300, seasonal_0 Loss: 0.0684 | 0.0484
Epoch 145/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 146/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 147/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 148/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 149/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 150/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 151/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 152/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 153/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 154/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 155/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 156/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 157/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 158/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 159/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 160/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 161/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 162/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 163/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 164/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 165/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 166/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 167/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 168/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 169/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 170/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 171/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 172/300, seasonal_0 Loss: 0.0683 | 0.0484
Epoch 173/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 174/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 175/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 176/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 177/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 178/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 179/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 180/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 181/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 182/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 183/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 184/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 185/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 186/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 187/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 188/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 189/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 190/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 191/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 192/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 193/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 194/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 195/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 196/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 197/300, seasonal_0 Loss: 0.0682 | 0.0484
Epoch 198/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 199/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 200/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 201/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 202/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 203/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 204/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 205/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 206/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 207/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 208/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 209/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 210/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 211/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 212/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 213/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 214/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 215/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 216/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 217/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 218/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 219/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 220/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 221/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 222/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 223/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 224/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 225/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 226/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 227/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 228/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 229/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 230/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 231/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 232/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 233/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 234/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 235/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 236/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 237/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 238/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 239/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 240/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 241/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 242/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 243/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 244/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 245/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 246/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 247/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 248/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 249/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 250/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 251/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 252/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 253/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 254/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 255/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 256/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 257/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 258/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 259/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 260/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 261/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 262/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 263/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 264/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 265/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 266/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 267/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 268/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 269/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 270/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 271/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 272/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 273/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 274/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 275/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 276/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 277/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 278/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 279/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 280/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 281/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 282/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 283/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 284/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 285/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 286/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 287/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 288/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 289/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 290/300, seasonal_0 Loss: 0.0682 | 0.0483
Epoch 291/300, seasonal_0 Loss: 0.0682 | 0.0483
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 24, 'train_rates': 0.8908090484110834, 'learning_rate': 0.0005674414483444585, 'batch_size': 215, 'step_size': 11, 'gamma': 0.9833993042011113}
Epoch 1/300, seasonal_1 Loss: 0.4899 | 0.2876
Epoch 2/300, seasonal_1 Loss: 0.2639 | 0.1990
Epoch 3/300, seasonal_1 Loss: 0.2464 | 0.1471
Epoch 4/300, seasonal_1 Loss: 0.2350 | 0.3995
Epoch 5/300, seasonal_1 Loss: 0.2210 | 0.1452
Epoch 6/300, seasonal_1 Loss: 0.1790 | 0.1242
Epoch 7/300, seasonal_1 Loss: 0.2155 | 0.1138
Epoch 8/300, seasonal_1 Loss: 0.2047 | 0.0962
Epoch 9/300, seasonal_1 Loss: 0.1440 | 0.1173
Epoch 10/300, seasonal_1 Loss: 0.1557 | 0.1137
Epoch 11/300, seasonal_1 Loss: 0.1622 | 0.0795
Epoch 12/300, seasonal_1 Loss: 0.1639 | 0.1165
Epoch 13/300, seasonal_1 Loss: 0.1772 | 0.0812
Epoch 14/300, seasonal_1 Loss: 0.1451 | 0.1803
Epoch 15/300, seasonal_1 Loss: 0.1881 | 0.1333
Epoch 16/300, seasonal_1 Loss: 0.1591 | 0.0877
Epoch 17/300, seasonal_1 Loss: 0.1712 | 0.0931
Epoch 18/300, seasonal_1 Loss: 0.1418 | 0.1005
Epoch 19/300, seasonal_1 Loss: 0.1306 | 0.0633
Epoch 20/300, seasonal_1 Loss: 0.1133 | 0.0725
Epoch 21/300, seasonal_1 Loss: 0.1121 | 0.0629
Epoch 22/300, seasonal_1 Loss: 0.1074 | 0.0558
Epoch 23/300, seasonal_1 Loss: 0.0973 | 0.0511
Epoch 24/300, seasonal_1 Loss: 0.0990 | 0.0721
Epoch 25/300, seasonal_1 Loss: 0.1015 | 0.1102
Epoch 26/300, seasonal_1 Loss: 0.0998 | 0.0791
Epoch 27/300, seasonal_1 Loss: 0.0981 | 0.0514
Epoch 28/300, seasonal_1 Loss: 0.1020 | 0.0552
Epoch 29/300, seasonal_1 Loss: 0.1123 | 0.0639
Epoch 30/300, seasonal_1 Loss: 0.1072 | 0.0960
Epoch 31/300, seasonal_1 Loss: 0.1051 | 0.1036
Epoch 32/300, seasonal_1 Loss: 0.1056 | 0.0497
Epoch 33/300, seasonal_1 Loss: 0.0922 | 0.0580
Epoch 34/300, seasonal_1 Loss: 0.0935 | 0.0615
Epoch 35/300, seasonal_1 Loss: 0.0859 | 0.0531
Epoch 36/300, seasonal_1 Loss: 0.0846 | 0.0554
Epoch 37/300, seasonal_1 Loss: 0.0862 | 0.0499
Epoch 38/300, seasonal_1 Loss: 0.0831 | 0.0450
Epoch 39/300, seasonal_1 Loss: 0.0908 | 0.0545
Epoch 40/300, seasonal_1 Loss: 0.0949 | 0.0504
Epoch 41/300, seasonal_1 Loss: 0.0887 | 0.0495
Epoch 42/300, seasonal_1 Loss: 0.0861 | 0.0494
Epoch 43/300, seasonal_1 Loss: 0.0918 | 0.0898
Epoch 44/300, seasonal_1 Loss: 0.1047 | 0.1036
Epoch 45/300, seasonal_1 Loss: 0.1055 | 0.0761
Epoch 46/300, seasonal_1 Loss: 0.1051 | 0.0525
Epoch 47/300, seasonal_1 Loss: 0.0926 | 0.0487
Epoch 48/300, seasonal_1 Loss: 0.0922 | 0.0423
Epoch 49/300, seasonal_1 Loss: 0.0812 | 0.0528
Epoch 50/300, seasonal_1 Loss: 0.0813 | 0.0649
Epoch 51/300, seasonal_1 Loss: 0.0839 | 0.0457
Epoch 52/300, seasonal_1 Loss: 0.0807 | 0.0426
Epoch 53/300, seasonal_1 Loss: 0.0828 | 0.0432
Epoch 54/300, seasonal_1 Loss: 0.0855 | 0.0398
Epoch 55/300, seasonal_1 Loss: 0.0781 | 0.0471
Epoch 56/300, seasonal_1 Loss: 0.0796 | 0.0526
Epoch 57/300, seasonal_1 Loss: 0.0836 | 0.0456
Epoch 58/300, seasonal_1 Loss: 0.0784 | 0.0445
Epoch 59/300, seasonal_1 Loss: 0.0764 | 0.0497
Epoch 60/300, seasonal_1 Loss: 0.0746 | 0.0424
Epoch 61/300, seasonal_1 Loss: 0.0733 | 0.0416
Epoch 62/300, seasonal_1 Loss: 0.0734 | 0.0395
Epoch 63/300, seasonal_1 Loss: 0.0719 | 0.0360
Epoch 64/300, seasonal_1 Loss: 0.0734 | 0.0370
Epoch 65/300, seasonal_1 Loss: 0.0728 | 0.0443
Epoch 66/300, seasonal_1 Loss: 0.0723 | 0.0471
Epoch 67/300, seasonal_1 Loss: 0.0739 | 0.0639
Epoch 68/300, seasonal_1 Loss: 0.0726 | 0.0461
Epoch 69/300, seasonal_1 Loss: 0.0719 | 0.0440
Epoch 70/300, seasonal_1 Loss: 0.0709 | 0.0396
Epoch 71/300, seasonal_1 Loss: 0.0705 | 0.0408
Epoch 72/300, seasonal_1 Loss: 0.0723 | 0.0369
Epoch 73/300, seasonal_1 Loss: 0.0702 | 0.0402
Epoch 74/300, seasonal_1 Loss: 0.0711 | 0.0370
Epoch 75/300, seasonal_1 Loss: 0.0696 | 0.0379
Epoch 76/300, seasonal_1 Loss: 0.0683 | 0.0383
Epoch 77/300, seasonal_1 Loss: 0.0714 | 0.0517
Epoch 78/300, seasonal_1 Loss: 0.0733 | 0.0521
Epoch 79/300, seasonal_1 Loss: 0.0747 | 0.0517
Epoch 80/300, seasonal_1 Loss: 0.0728 | 0.0415
Epoch 81/300, seasonal_1 Loss: 0.0689 | 0.0493
Epoch 82/300, seasonal_1 Loss: 0.0691 | 0.0432
Epoch 83/300, seasonal_1 Loss: 0.0683 | 0.0422
Epoch 84/300, seasonal_1 Loss: 0.0709 | 0.0360
Epoch 85/300, seasonal_1 Loss: 0.0697 | 0.0390
Epoch 86/300, seasonal_1 Loss: 0.0694 | 0.0389
Epoch 87/300, seasonal_1 Loss: 0.0700 | 0.0532
Epoch 88/300, seasonal_1 Loss: 0.0719 | 0.0522
Epoch 89/300, seasonal_1 Loss: 0.0697 | 0.0544
Epoch 90/300, seasonal_1 Loss: 0.0679 | 0.0485
Epoch 91/300, seasonal_1 Loss: 0.0681 | 0.0498
Epoch 92/300, seasonal_1 Loss: 0.0659 | 0.0412
Epoch 93/300, seasonal_1 Loss: 0.0634 | 0.0343
Epoch 94/300, seasonal_1 Loss: 0.0655 | 0.0347
Epoch 95/300, seasonal_1 Loss: 0.0652 | 0.0352
Epoch 96/300, seasonal_1 Loss: 0.0669 | 0.0352
Epoch 97/300, seasonal_1 Loss: 0.0657 | 0.0372
Epoch 98/300, seasonal_1 Loss: 0.0637 | 0.0388
Epoch 99/300, seasonal_1 Loss: 0.0632 | 0.0416
Epoch 100/300, seasonal_1 Loss: 0.0673 | 0.0522
Epoch 101/300, seasonal_1 Loss: 0.0691 | 0.0473
Epoch 102/300, seasonal_1 Loss: 0.0681 | 0.0399
Epoch 103/300, seasonal_1 Loss: 0.0653 | 0.0457
Epoch 104/300, seasonal_1 Loss: 0.0626 | 0.0495
Epoch 105/300, seasonal_1 Loss: 0.0619 | 0.0458
Epoch 106/300, seasonal_1 Loss: 0.0628 | 0.0355
Epoch 107/300, seasonal_1 Loss: 0.0609 | 0.0338
Epoch 108/300, seasonal_1 Loss: 0.0613 | 0.0363
Epoch 109/300, seasonal_1 Loss: 0.0609 | 0.0348
Epoch 110/300, seasonal_1 Loss: 0.0593 | 0.0379
Epoch 111/300, seasonal_1 Loss: 0.0613 | 0.0529
Epoch 112/300, seasonal_1 Loss: 0.0619 | 0.0538
Epoch 113/300, seasonal_1 Loss: 0.0623 | 0.0612
Epoch 114/300, seasonal_1 Loss: 0.0600 | 0.0399
Epoch 115/300, seasonal_1 Loss: 0.0585 | 0.0338
Epoch 116/300, seasonal_1 Loss: 0.0593 | 0.0315
Epoch 117/300, seasonal_1 Loss: 0.0585 | 0.0352
Epoch 118/300, seasonal_1 Loss: 0.0603 | 0.0367
Epoch 119/300, seasonal_1 Loss: 0.0589 | 0.0485
Epoch 120/300, seasonal_1 Loss: 0.0580 | 0.0440
Epoch 121/300, seasonal_1 Loss: 0.0579 | 0.0390
Epoch 122/300, seasonal_1 Loss: 0.0572 | 0.0346
Epoch 123/300, seasonal_1 Loss: 0.0575 | 0.0358
Epoch 124/300, seasonal_1 Loss: 0.0581 | 0.0371
Epoch 125/300, seasonal_1 Loss: 0.0569 | 0.0350
Epoch 126/300, seasonal_1 Loss: 0.0553 | 0.0356
Epoch 127/300, seasonal_1 Loss: 0.0553 | 0.0382
Epoch 128/300, seasonal_1 Loss: 0.0552 | 0.0453
Epoch 129/300, seasonal_1 Loss: 0.0569 | 0.0608
Epoch 130/300, seasonal_1 Loss: 0.0571 | 0.0522
Epoch 131/300, seasonal_1 Loss: 0.0565 | 0.0380
Epoch 132/300, seasonal_1 Loss: 0.0574 | 0.0360
Epoch 133/300, seasonal_1 Loss: 0.0604 | 0.0371
Epoch 134/300, seasonal_1 Loss: 0.0605 | 0.0345
Epoch 135/300, seasonal_1 Loss: 0.0623 | 0.0478
Epoch 136/300, seasonal_1 Loss: 0.0637 | 0.0536
Epoch 137/300, seasonal_1 Loss: 0.0620 | 0.0559
Epoch 138/300, seasonal_1 Loss: 0.0607 | 0.0379
Epoch 139/300, seasonal_1 Loss: 0.0613 | 0.0367
Epoch 140/300, seasonal_1 Loss: 0.0556 | 0.0341
Epoch 141/300, seasonal_1 Loss: 0.0547 | 0.0407
Epoch 142/300, seasonal_1 Loss: 0.0539 | 0.0546
Epoch 143/300, seasonal_1 Loss: 0.0531 | 0.0389
Epoch 144/300, seasonal_1 Loss: 0.0519 | 0.0311
Epoch 145/300, seasonal_1 Loss: 0.0511 | 0.0310
Epoch 146/300, seasonal_1 Loss: 0.0509 | 0.0381
Epoch 147/300, seasonal_1 Loss: 0.0516 | 0.0441
Epoch 148/300, seasonal_1 Loss: 0.0512 | 0.0360
Epoch 149/300, seasonal_1 Loss: 0.0518 | 0.0324
Epoch 150/300, seasonal_1 Loss: 0.0534 | 0.0318
Epoch 151/300, seasonal_1 Loss: 0.0544 | 0.0344
Epoch 152/300, seasonal_1 Loss: 0.0540 | 0.0401
Epoch 153/300, seasonal_1 Loss: 0.0518 | 0.0408
Epoch 154/300, seasonal_1 Loss: 0.0531 | 0.0392
Epoch 155/300, seasonal_1 Loss: 0.0555 | 0.0367
Epoch 156/300, seasonal_1 Loss: 0.0521 | 0.0344
Epoch 157/300, seasonal_1 Loss: 0.0538 | 0.0372
Epoch 158/300, seasonal_1 Loss: 0.0526 | 0.0530
Epoch 159/300, seasonal_1 Loss: 0.0530 | 0.0447
Epoch 160/300, seasonal_1 Loss: 0.0522 | 0.0322
Epoch 161/300, seasonal_1 Loss: 0.0526 | 0.0327
Epoch 162/300, seasonal_1 Loss: 0.0522 | 0.0336
Epoch 163/300, seasonal_1 Loss: 0.0531 | 0.0421
Epoch 164/300, seasonal_1 Loss: 0.0512 | 0.0416
Epoch 165/300, seasonal_1 Loss: 0.0499 | 0.0335
Epoch 166/300, seasonal_1 Loss: 0.0514 | 0.0371
Epoch 167/300, seasonal_1 Loss: 0.0517 | 0.0360
Epoch 168/300, seasonal_1 Loss: 0.0497 | 0.0356
Epoch 169/300, seasonal_1 Loss: 0.0514 | 0.0374
Epoch 170/300, seasonal_1 Loss: 0.0488 | 0.0484
Epoch 171/300, seasonal_1 Loss: 0.0503 | 0.0337
Epoch 172/300, seasonal_1 Loss: 0.0494 | 0.0317
Epoch 173/300, seasonal_1 Loss: 0.0516 | 0.0346
Epoch 174/300, seasonal_1 Loss: 0.0512 | 0.0425
Epoch 175/300, seasonal_1 Loss: 0.0509 | 0.0422
Epoch 176/300, seasonal_1 Loss: 0.0499 | 0.0429
Epoch 177/300, seasonal_1 Loss: 0.0494 | 0.0352
Epoch 178/300, seasonal_1 Loss: 0.0494 | 0.0318
Epoch 179/300, seasonal_1 Loss: 0.0482 | 0.0321
Epoch 180/300, seasonal_1 Loss: 0.0486 | 0.0417
Epoch 181/300, seasonal_1 Loss: 0.0472 | 0.0416
Epoch 182/300, seasonal_1 Loss: 0.0470 | 0.0348
Epoch 183/300, seasonal_1 Loss: 0.0474 | 0.0311
Epoch 184/300, seasonal_1 Loss: 0.0467 | 0.0317
Epoch 185/300, seasonal_1 Loss: 0.0467 | 0.0397
Epoch 186/300, seasonal_1 Loss: 0.0462 | 0.0446
Epoch 187/300, seasonal_1 Loss: 0.0456 | 0.0337
Epoch 188/300, seasonal_1 Loss: 0.0457 | 0.0327
Epoch 189/300, seasonal_1 Loss: 0.0457 | 0.0328
Epoch 190/300, seasonal_1 Loss: 0.0451 | 0.0402
Epoch 191/300, seasonal_1 Loss: 0.0458 | 0.0415
Epoch 192/300, seasonal_1 Loss: 0.0448 | 0.0391
Epoch 193/300, seasonal_1 Loss: 0.0448 | 0.0357
Epoch 194/300, seasonal_1 Loss: 0.0448 | 0.0315
Epoch 195/300, seasonal_1 Loss: 0.0463 | 0.0328
Epoch 196/300, seasonal_1 Loss: 0.0477 | 0.0377
Epoch 197/300, seasonal_1 Loss: 0.0477 | 0.0452
Epoch 198/300, seasonal_1 Loss: 0.0472 | 0.0369
Epoch 199/300, seasonal_1 Loss: 0.0467 | 0.0345
Epoch 200/300, seasonal_1 Loss: 0.0480 | 0.0339
Epoch 201/300, seasonal_1 Loss: 0.0465 | 0.0353
Epoch 202/300, seasonal_1 Loss: 0.0476 | 0.0531
Epoch 203/300, seasonal_1 Loss: 0.0487 | 0.0541
Epoch 204/300, seasonal_1 Loss: 0.0495 | 0.0394
Epoch 205/300, seasonal_1 Loss: 0.0473 | 0.0330
Epoch 206/300, seasonal_1 Loss: 0.0476 | 0.0349
Epoch 207/300, seasonal_1 Loss: 0.0473 | 0.0421
Epoch 208/300, seasonal_1 Loss: 0.0463 | 0.0362
Epoch 209/300, seasonal_1 Loss: 0.0463 | 0.0358
Epoch 210/300, seasonal_1 Loss: 0.0463 | 0.0323
Epoch 211/300, seasonal_1 Loss: 0.0459 | 0.0441
Epoch 212/300, seasonal_1 Loss: 0.0476 | 0.0607
Epoch 213/300, seasonal_1 Loss: 0.0477 | 0.0382
Epoch 214/300, seasonal_1 Loss: 0.0452 | 0.0344
Epoch 215/300, seasonal_1 Loss: 0.0436 | 0.0375
Epoch 216/300, seasonal_1 Loss: 0.0461 | 0.0350
Epoch 217/300, seasonal_1 Loss: 0.0456 | 0.0330
Epoch 218/300, seasonal_1 Loss: 0.0446 | 0.0337
Epoch 219/300, seasonal_1 Loss: 0.0449 | 0.0383
Epoch 220/300, seasonal_1 Loss: 0.0443 | 0.0463
Epoch 221/300, seasonal_1 Loss: 0.0449 | 0.0418
Epoch 222/300, seasonal_1 Loss: 0.0427 | 0.0359
Epoch 223/300, seasonal_1 Loss: 0.0425 | 0.0358
Epoch 224/300, seasonal_1 Loss: 0.0458 | 0.0364
Epoch 225/300, seasonal_1 Loss: 0.0449 | 0.0365
Epoch 226/300, seasonal_1 Loss: 0.0442 | 0.0440
Epoch 227/300, seasonal_1 Loss: 0.0481 | 0.0400
Epoch 228/300, seasonal_1 Loss: 0.0476 | 0.0349
Epoch 229/300, seasonal_1 Loss: 0.0452 | 0.0438
Epoch 230/300, seasonal_1 Loss: 0.0448 | 0.0416
Epoch 231/300, seasonal_1 Loss: 0.0446 | 0.0332
Epoch 232/300, seasonal_1 Loss: 0.0445 | 0.0371
Epoch 233/300, seasonal_1 Loss: 0.0451 | 0.0399
Epoch 234/300, seasonal_1 Loss: 0.0448 | 0.0593
Epoch 235/300, seasonal_1 Loss: 0.0438 | 0.0425
Epoch 236/300, seasonal_1 Loss: 0.0433 | 0.0347
Epoch 237/300, seasonal_1 Loss: 0.0423 | 0.0323
Epoch 238/300, seasonal_1 Loss: 0.0438 | 0.0389
Epoch 239/300, seasonal_1 Loss: 0.0431 | 0.0382
Epoch 240/300, seasonal_1 Loss: 0.0425 | 0.0358
Epoch 241/300, seasonal_1 Loss: 0.0428 | 0.0344
Epoch 242/300, seasonal_1 Loss: 0.0418 | 0.0383
Epoch 243/300, seasonal_1 Loss: 0.0419 | 0.0409
Epoch 244/300, seasonal_1 Loss: 0.0413 | 0.0350
Epoch 245/300, seasonal_1 Loss: 0.0413 | 0.0329
Epoch 246/300, seasonal_1 Loss: 0.0411 | 0.0357
Epoch 247/300, seasonal_1 Loss: 0.0409 | 0.0443
Epoch 248/300, seasonal_1 Loss: 0.0413 | 0.0379
Epoch 249/300, seasonal_1 Loss: 0.0414 | 0.0334
Epoch 250/300, seasonal_1 Loss: 0.0409 | 0.0360
Epoch 251/300, seasonal_1 Loss: 0.0413 | 0.0413
Epoch 252/300, seasonal_1 Loss: 0.0410 | 0.0354
Epoch 253/300, seasonal_1 Loss: 0.0411 | 0.0349
Epoch 254/300, seasonal_1 Loss: 0.0409 | 0.0363
Epoch 255/300, seasonal_1 Loss: 0.0406 | 0.0451
Epoch 256/300, seasonal_1 Loss: 0.0408 | 0.0381
Epoch 257/300, seasonal_1 Loss: 0.0409 | 0.0353
Epoch 258/300, seasonal_1 Loss: 0.0402 | 0.0355
Epoch 259/300, seasonal_1 Loss: 0.0410 | 0.0398
Epoch 260/300, seasonal_1 Loss: 0.0407 | 0.0355
Epoch 261/300, seasonal_1 Loss: 0.0406 | 0.0351
Epoch 262/300, seasonal_1 Loss: 0.0402 | 0.0349
Epoch 263/300, seasonal_1 Loss: 0.0400 | 0.0423
Epoch 264/300, seasonal_1 Loss: 0.0404 | 0.0454
Epoch 265/300, seasonal_1 Loss: 0.0405 | 0.0376
Epoch 266/300, seasonal_1 Loss: 0.0397 | 0.0347
Epoch 267/300, seasonal_1 Loss: 0.0404 | 0.0370
Epoch 268/300, seasonal_1 Loss: 0.0411 | 0.0373
Epoch 269/300, seasonal_1 Loss: 0.0404 | 0.0391
Epoch 270/300, seasonal_1 Loss: 0.0421 | 0.0371
Epoch 271/300, seasonal_1 Loss: 0.0414 | 0.0374
Epoch 272/300, seasonal_1 Loss: 0.0416 | 0.0427
Epoch 273/300, seasonal_1 Loss: 0.0410 | 0.0472
Epoch 274/300, seasonal_1 Loss: 0.0401 | 0.0382
Epoch 275/300, seasonal_1 Loss: 0.0406 | 0.0376
Epoch 276/300, seasonal_1 Loss: 0.0415 | 0.0351
Epoch 277/300, seasonal_1 Loss: 0.0405 | 0.0507
Epoch 278/300, seasonal_1 Loss: 0.0402 | 0.0509
Epoch 279/300, seasonal_1 Loss: 0.0415 | 0.0485
Epoch 280/300, seasonal_1 Loss: 0.0462 | 0.0418
Epoch 281/300, seasonal_1 Loss: 0.0424 | 0.0382
Epoch 282/300, seasonal_1 Loss: 0.0386 | 0.0438
Epoch 283/300, seasonal_1 Loss: 0.0367 | 0.0472
Epoch 284/300, seasonal_1 Loss: 0.0361 | 0.0412
Epoch 285/300, seasonal_1 Loss: 0.0351 | 0.0375
Epoch 286/300, seasonal_1 Loss: 0.0395 | 0.0397
Epoch 287/300, seasonal_1 Loss: 0.0363 | 0.0455
Epoch 288/300, seasonal_1 Loss: 0.0345 | 0.0416
Epoch 289/300, seasonal_1 Loss: 0.0353 | 0.0403
Epoch 290/300, seasonal_1 Loss: 0.0342 | 0.0372
Epoch 291/300, seasonal_1 Loss: 0.0345 | 0.0379
Epoch 292/300, seasonal_1 Loss: 0.0331 | 0.0465
Epoch 293/300, seasonal_1 Loss: 0.0332 | 0.0423
Epoch 294/300, seasonal_1 Loss: 0.0323 | 0.0366
Epoch 295/300, seasonal_1 Loss: 0.0318 | 0.0346
Epoch 296/300, seasonal_1 Loss: 0.0323 | 0.0434
Epoch 297/300, seasonal_1 Loss: 0.0334 | 0.0454
Epoch 298/300, seasonal_1 Loss: 0.0337 | 0.0374
Epoch 299/300, seasonal_1 Loss: 0.0322 | 0.0373
Epoch 300/300, seasonal_1 Loss: 0.0312 | 0.0410
Training seasonal_2 component with params: {'observation_period_num': 20, 'train_rates': 0.9027765814986409, 'learning_rate': 0.0003012990802565954, 'batch_size': 39, 'step_size': 7, 'gamma': 0.7595410828182823}
Epoch 1/300, seasonal_2 Loss: 0.2890 | 0.1193
Epoch 2/300, seasonal_2 Loss: 0.1451 | 0.1033
Epoch 3/300, seasonal_2 Loss: 0.1226 | 0.0795
Epoch 4/300, seasonal_2 Loss: 0.1119 | 0.0726
Epoch 5/300, seasonal_2 Loss: 0.1045 | 0.0615
Epoch 6/300, seasonal_2 Loss: 0.1022 | 0.0601
Epoch 7/300, seasonal_2 Loss: 0.0994 | 0.0586
Epoch 8/300, seasonal_2 Loss: 0.0977 | 0.0757
Epoch 9/300, seasonal_2 Loss: 0.0942 | 0.0784
Epoch 10/300, seasonal_2 Loss: 0.0909 | 0.0798
Epoch 11/300, seasonal_2 Loss: 0.0887 | 0.0813
Epoch 12/300, seasonal_2 Loss: 0.0873 | 0.0832
Epoch 13/300, seasonal_2 Loss: 0.0867 | 0.0693
Epoch 14/300, seasonal_2 Loss: 0.0863 | 0.0638
Epoch 15/300, seasonal_2 Loss: 0.0848 | 0.0440
Epoch 16/300, seasonal_2 Loss: 0.0857 | 0.0426
Epoch 17/300, seasonal_2 Loss: 0.0824 | 0.0431
Epoch 18/300, seasonal_2 Loss: 0.0788 | 0.0438
Epoch 19/300, seasonal_2 Loss: 0.0762 | 0.0395
Epoch 20/300, seasonal_2 Loss: 0.0751 | 0.0378
Epoch 21/300, seasonal_2 Loss: 0.0741 | 0.0371
Epoch 22/300, seasonal_2 Loss: 0.0733 | 0.0372
Epoch 23/300, seasonal_2 Loss: 0.0726 | 0.0365
Epoch 24/300, seasonal_2 Loss: 0.0721 | 0.0360
Epoch 25/300, seasonal_2 Loss: 0.0715 | 0.0355
Epoch 26/300, seasonal_2 Loss: 0.0710 | 0.0358
Epoch 27/300, seasonal_2 Loss: 0.0706 | 0.0353
Epoch 28/300, seasonal_2 Loss: 0.0702 | 0.0350
Epoch 29/300, seasonal_2 Loss: 0.0699 | 0.0355
Epoch 30/300, seasonal_2 Loss: 0.0694 | 0.0351
Epoch 31/300, seasonal_2 Loss: 0.0690 | 0.0348
Epoch 32/300, seasonal_2 Loss: 0.0687 | 0.0345
Epoch 33/300, seasonal_2 Loss: 0.0685 | 0.0350
Epoch 34/300, seasonal_2 Loss: 0.0681 | 0.0348
Epoch 35/300, seasonal_2 Loss: 0.0678 | 0.0346
Epoch 36/300, seasonal_2 Loss: 0.0676 | 0.0350
Epoch 37/300, seasonal_2 Loss: 0.0675 | 0.0349
Epoch 38/300, seasonal_2 Loss: 0.0672 | 0.0347
Epoch 39/300, seasonal_2 Loss: 0.0670 | 0.0345
Epoch 40/300, seasonal_2 Loss: 0.0669 | 0.0348
Epoch 41/300, seasonal_2 Loss: 0.0668 | 0.0347
Epoch 42/300, seasonal_2 Loss: 0.0666 | 0.0346
Epoch 43/300, seasonal_2 Loss: 0.0664 | 0.0346
Epoch 44/300, seasonal_2 Loss: 0.0664 | 0.0346
Epoch 45/300, seasonal_2 Loss: 0.0662 | 0.0345
Epoch 46/300, seasonal_2 Loss: 0.0660 | 0.0344
Epoch 47/300, seasonal_2 Loss: 0.0658 | 0.0339
Epoch 48/300, seasonal_2 Loss: 0.0657 | 0.0338
Epoch 49/300, seasonal_2 Loss: 0.0656 | 0.0337
Epoch 50/300, seasonal_2 Loss: 0.0654 | 0.0333
Epoch 51/300, seasonal_2 Loss: 0.0653 | 0.0332
Epoch 52/300, seasonal_2 Loss: 0.0651 | 0.0332
Epoch 53/300, seasonal_2 Loss: 0.0650 | 0.0331
Epoch 54/300, seasonal_2 Loss: 0.0649 | 0.0329
Epoch 55/300, seasonal_2 Loss: 0.0648 | 0.0328
Epoch 56/300, seasonal_2 Loss: 0.0648 | 0.0328
Epoch 57/300, seasonal_2 Loss: 0.0647 | 0.0326
Epoch 58/300, seasonal_2 Loss: 0.0646 | 0.0326
Epoch 59/300, seasonal_2 Loss: 0.0646 | 0.0326
Epoch 60/300, seasonal_2 Loss: 0.0645 | 0.0325
Epoch 61/300, seasonal_2 Loss: 0.0645 | 0.0324
Epoch 62/300, seasonal_2 Loss: 0.0644 | 0.0324
Epoch 63/300, seasonal_2 Loss: 0.0644 | 0.0324
Epoch 64/300, seasonal_2 Loss: 0.0643 | 0.0323
Epoch 65/300, seasonal_2 Loss: 0.0643 | 0.0323
Epoch 66/300, seasonal_2 Loss: 0.0643 | 0.0322
Epoch 67/300, seasonal_2 Loss: 0.0642 | 0.0322
Epoch 68/300, seasonal_2 Loss: 0.0642 | 0.0322
Epoch 69/300, seasonal_2 Loss: 0.0642 | 0.0321
Epoch 70/300, seasonal_2 Loss: 0.0641 | 0.0321
Epoch 71/300, seasonal_2 Loss: 0.0641 | 0.0321
Epoch 72/300, seasonal_2 Loss: 0.0641 | 0.0321
Epoch 73/300, seasonal_2 Loss: 0.0641 | 0.0321
Epoch 74/300, seasonal_2 Loss: 0.0641 | 0.0320
Epoch 75/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 76/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 77/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 78/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 79/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 80/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 81/300, seasonal_2 Loss: 0.0640 | 0.0320
Epoch 82/300, seasonal_2 Loss: 0.0640 | 0.0319
Epoch 83/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 84/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 85/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 86/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 87/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 88/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 89/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 90/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 91/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 92/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 93/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 94/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 95/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 96/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 97/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 98/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 99/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 100/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 101/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 102/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 103/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 104/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 105/300, seasonal_2 Loss: 0.0639 | 0.0319
Epoch 106/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 107/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 108/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 109/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 110/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 111/300, seasonal_2 Loss: 0.0638 | 0.0319
Epoch 112/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 113/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 114/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 115/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 116/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 117/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 118/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 119/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 120/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 121/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 122/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 123/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 124/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 125/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 126/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 127/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 128/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 129/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 130/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 131/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 132/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 133/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 134/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 135/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 136/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 137/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 138/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 139/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 140/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 141/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 142/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 143/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 144/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 145/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 146/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 147/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 148/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 149/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 150/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 151/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 152/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 153/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 154/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 155/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 156/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 157/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 158/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 159/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 160/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 161/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 162/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 163/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 164/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 165/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 166/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 167/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 168/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 169/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 170/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 171/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 172/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 173/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 174/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 175/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 176/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 177/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 178/300, seasonal_2 Loss: 0.0638 | 0.0318
Epoch 179/300, seasonal_2 Loss: 0.0638 | 0.0318
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.8674334329829317, 'learning_rate': 0.0001725589922002092, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8391125274501056}
Epoch 1/300, seasonal_3 Loss: 0.2671 | 0.0899
Epoch 2/300, seasonal_3 Loss: 0.1323 | 0.0890
Epoch 3/300, seasonal_3 Loss: 0.1177 | 0.0791
Epoch 4/300, seasonal_3 Loss: 0.1116 | 0.0764
Epoch 5/300, seasonal_3 Loss: 0.1078 | 0.0721
Epoch 6/300, seasonal_3 Loss: 0.1054 | 0.0731
Epoch 7/300, seasonal_3 Loss: 0.1030 | 0.0733
Epoch 8/300, seasonal_3 Loss: 0.0991 | 0.0639
Epoch 9/300, seasonal_3 Loss: 0.0959 | 0.0635
Epoch 10/300, seasonal_3 Loss: 0.0934 | 0.0632
Epoch 11/300, seasonal_3 Loss: 0.0919 | 0.0630
Epoch 12/300, seasonal_3 Loss: 0.0902 | 0.0634
Epoch 13/300, seasonal_3 Loss: 0.0886 | 0.0620
Epoch 14/300, seasonal_3 Loss: 0.0870 | 0.0593
Epoch 15/300, seasonal_3 Loss: 0.0849 | 0.0593
Epoch 16/300, seasonal_3 Loss: 0.0835 | 0.0553
Epoch 17/300, seasonal_3 Loss: 0.0823 | 0.0513
Epoch 18/300, seasonal_3 Loss: 0.0811 | 0.0469
Epoch 19/300, seasonal_3 Loss: 0.0798 | 0.0458
Epoch 20/300, seasonal_3 Loss: 0.0788 | 0.0435
Epoch 21/300, seasonal_3 Loss: 0.0778 | 0.0410
Epoch 22/300, seasonal_3 Loss: 0.0767 | 0.0422
Epoch 23/300, seasonal_3 Loss: 0.0759 | 0.0422
Epoch 24/300, seasonal_3 Loss: 0.0751 | 0.0418
Epoch 25/300, seasonal_3 Loss: 0.0745 | 0.0417
Epoch 26/300, seasonal_3 Loss: 0.0740 | 0.0445
Epoch 27/300, seasonal_3 Loss: 0.0737 | 0.0463
Epoch 28/300, seasonal_3 Loss: 0.0734 | 0.0475
Epoch 29/300, seasonal_3 Loss: 0.0731 | 0.0462
Epoch 30/300, seasonal_3 Loss: 0.0730 | 0.0480
Epoch 31/300, seasonal_3 Loss: 0.0728 | 0.0493
Epoch 32/300, seasonal_3 Loss: 0.0726 | 0.0501
Epoch 33/300, seasonal_3 Loss: 0.0723 | 0.0419
Epoch 34/300, seasonal_3 Loss: 0.0722 | 0.0416
Epoch 35/300, seasonal_3 Loss: 0.0721 | 0.0416
Epoch 36/300, seasonal_3 Loss: 0.0718 | 0.0359
Epoch 37/300, seasonal_3 Loss: 0.0715 | 0.0356
Epoch 38/300, seasonal_3 Loss: 0.0713 | 0.0356
Epoch 39/300, seasonal_3 Loss: 0.0710 | 0.0357
Epoch 40/300, seasonal_3 Loss: 0.0708 | 0.0334
Epoch 41/300, seasonal_3 Loss: 0.0707 | 0.0331
Epoch 42/300, seasonal_3 Loss: 0.0704 | 0.0330
Epoch 43/300, seasonal_3 Loss: 0.0702 | 0.0322
Epoch 44/300, seasonal_3 Loss: 0.0701 | 0.0320
Epoch 45/300, seasonal_3 Loss: 0.0699 | 0.0318
Epoch 46/300, seasonal_3 Loss: 0.0698 | 0.0317
Epoch 47/300, seasonal_3 Loss: 0.0696 | 0.0313
Epoch 48/300, seasonal_3 Loss: 0.0695 | 0.0311
Epoch 49/300, seasonal_3 Loss: 0.0694 | 0.0309
Epoch 50/300, seasonal_3 Loss: 0.0692 | 0.0310
Epoch 51/300, seasonal_3 Loss: 0.0691 | 0.0308
Epoch 52/300, seasonal_3 Loss: 0.0690 | 0.0307
Epoch 53/300, seasonal_3 Loss: 0.0689 | 0.0306
Epoch 54/300, seasonal_3 Loss: 0.0688 | 0.0305
Epoch 55/300, seasonal_3 Loss: 0.0688 | 0.0302
Epoch 56/300, seasonal_3 Loss: 0.0687 | 0.0299
Epoch 57/300, seasonal_3 Loss: 0.0685 | 0.0290
Epoch 58/300, seasonal_3 Loss: 0.0684 | 0.0289
Epoch 59/300, seasonal_3 Loss: 0.0683 | 0.0288
Epoch 60/300, seasonal_3 Loss: 0.0682 | 0.0287
Epoch 61/300, seasonal_3 Loss: 0.0680 | 0.0285
Epoch 62/300, seasonal_3 Loss: 0.0680 | 0.0285
Epoch 63/300, seasonal_3 Loss: 0.0678 | 0.0284
Epoch 64/300, seasonal_3 Loss: 0.0677 | 0.0283
Epoch 65/300, seasonal_3 Loss: 0.0677 | 0.0283
Epoch 66/300, seasonal_3 Loss: 0.0676 | 0.0282
Epoch 67/300, seasonal_3 Loss: 0.0675 | 0.0282
Epoch 68/300, seasonal_3 Loss: 0.0674 | 0.0281
Epoch 69/300, seasonal_3 Loss: 0.0673 | 0.0281
Epoch 70/300, seasonal_3 Loss: 0.0672 | 0.0280
Epoch 71/300, seasonal_3 Loss: 0.0672 | 0.0280
Epoch 72/300, seasonal_3 Loss: 0.0671 | 0.0280
Epoch 73/300, seasonal_3 Loss: 0.0671 | 0.0279
Epoch 74/300, seasonal_3 Loss: 0.0670 | 0.0279
Epoch 75/300, seasonal_3 Loss: 0.0669 | 0.0279
Epoch 76/300, seasonal_3 Loss: 0.0669 | 0.0279
Epoch 77/300, seasonal_3 Loss: 0.0668 | 0.0279
Epoch 78/300, seasonal_3 Loss: 0.0668 | 0.0280
Epoch 79/300, seasonal_3 Loss: 0.0667 | 0.0279
Epoch 80/300, seasonal_3 Loss: 0.0667 | 0.0279
Epoch 81/300, seasonal_3 Loss: 0.0666 | 0.0279
Epoch 82/300, seasonal_3 Loss: 0.0666 | 0.0280
Epoch 83/300, seasonal_3 Loss: 0.0666 | 0.0280
Epoch 84/300, seasonal_3 Loss: 0.0665 | 0.0280
Epoch 85/300, seasonal_3 Loss: 0.0665 | 0.0281
Epoch 86/300, seasonal_3 Loss: 0.0664 | 0.0281
Epoch 87/300, seasonal_3 Loss: 0.0664 | 0.0281
Epoch 88/300, seasonal_3 Loss: 0.0664 | 0.0281
Epoch 89/300, seasonal_3 Loss: 0.0663 | 0.0282
Epoch 90/300, seasonal_3 Loss: 0.0663 | 0.0282
Epoch 91/300, seasonal_3 Loss: 0.0663 | 0.0282
Epoch 92/300, seasonal_3 Loss: 0.0662 | 0.0283
Epoch 93/300, seasonal_3 Loss: 0.0662 | 0.0283
Epoch 94/300, seasonal_3 Loss: 0.0662 | 0.0283
Epoch 95/300, seasonal_3 Loss: 0.0662 | 0.0282
Epoch 96/300, seasonal_3 Loss: 0.0661 | 0.0283
Epoch 97/300, seasonal_3 Loss: 0.0661 | 0.0283
Epoch 98/300, seasonal_3 Loss: 0.0661 | 0.0283
Epoch 99/300, seasonal_3 Loss: 0.0661 | 0.0283
Epoch 100/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 101/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 102/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 103/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 104/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 105/300, seasonal_3 Loss: 0.0660 | 0.0283
Epoch 106/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 107/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 108/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 109/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 110/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 111/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 112/300, seasonal_3 Loss: 0.0659 | 0.0283
Epoch 113/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 114/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 115/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 116/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 117/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 118/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 119/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 120/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 121/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 122/300, seasonal_3 Loss: 0.0658 | 0.0284
Epoch 123/300, seasonal_3 Loss: 0.0658 | 0.0283
Epoch 124/300, seasonal_3 Loss: 0.0658 | 0.0284
Epoch 125/300, seasonal_3 Loss: 0.0658 | 0.0284
Epoch 126/300, seasonal_3 Loss: 0.0658 | 0.0284
Epoch 127/300, seasonal_3 Loss: 0.0658 | 0.0284
Epoch 128/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 129/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 130/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 131/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 132/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 133/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 134/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 135/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 136/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 137/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 138/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 139/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 140/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 141/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 142/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 143/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 144/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 145/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 146/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 147/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 148/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 149/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 150/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 151/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 152/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 153/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 154/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 155/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 156/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 157/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 158/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 159/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 160/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 161/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 162/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 163/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 164/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 165/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 166/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 167/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 168/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 169/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 170/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 171/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 172/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 173/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 174/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 175/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 176/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 177/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 178/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 179/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 180/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 181/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 182/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 183/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 184/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 185/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 186/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 187/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 188/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 189/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 190/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 191/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 192/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 193/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 194/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 195/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 196/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 197/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 198/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 199/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 200/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 201/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 202/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 203/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 204/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 205/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 206/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 207/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 208/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 209/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 210/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 211/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 212/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 213/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 214/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 215/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 216/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 217/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 218/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 219/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 220/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 221/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 222/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 223/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 224/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 225/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 226/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 227/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 228/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 229/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 230/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 231/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 232/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 233/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 234/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 235/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 236/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 237/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 238/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 239/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 240/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 241/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 242/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 243/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 244/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 245/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 246/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 247/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 248/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 249/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 250/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 251/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 252/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 253/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 254/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 255/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 256/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 257/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 258/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 259/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 260/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 261/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 262/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 263/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 264/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 265/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 266/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 267/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 268/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 269/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 270/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 271/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 272/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 273/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 274/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 275/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 276/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 277/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 278/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 279/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 280/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 281/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 282/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 283/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 284/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 285/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 286/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 287/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 288/300, seasonal_3 Loss: 0.0657 | 0.0284
Epoch 289/300, seasonal_3 Loss: 0.0657 | 0.0284
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8857117097861374, 'learning_rate': 0.0006429126970534857, 'batch_size': 102, 'step_size': 14, 'gamma': 0.9132654733444099}
Epoch 1/300, resid Loss: 0.2762 | 0.0970
Epoch 2/300, resid Loss: 0.1349 | 0.0730
Epoch 3/300, resid Loss: 0.1200 | 0.0671
Epoch 4/300, resid Loss: 0.1179 | 0.0744
Epoch 5/300, resid Loss: 0.1306 | 0.0912
Epoch 6/300, resid Loss: 0.1147 | 0.0713
Epoch 7/300, resid Loss: 0.1083 | 0.0612
Epoch 8/300, resid Loss: 0.1051 | 0.0710
Epoch 9/300, resid Loss: 0.1012 | 0.0551
Epoch 10/300, resid Loss: 0.0952 | 0.0507
Epoch 11/300, resid Loss: 0.0920 | 0.0484
Epoch 12/300, resid Loss: 0.0916 | 0.0554
Epoch 13/300, resid Loss: 0.0949 | 0.0499
Epoch 14/300, resid Loss: 0.0956 | 0.0500
Epoch 15/300, resid Loss: 0.0963 | 0.0472
Epoch 16/300, resid Loss: 0.0996 | 0.0458
Epoch 17/300, resid Loss: 0.0983 | 0.0416
Epoch 18/300, resid Loss: 0.0915 | 0.0515
Epoch 19/300, resid Loss: 0.1003 | 0.0536
Epoch 20/300, resid Loss: 0.0968 | 0.0423
Epoch 21/300, resid Loss: 0.0972 | 0.0498
Epoch 22/300, resid Loss: 0.0999 | 0.0609
Epoch 23/300, resid Loss: 0.0912 | 0.0483
Epoch 24/300, resid Loss: 0.0950 | 0.0605
Epoch 25/300, resid Loss: 0.0899 | 0.0488
Epoch 26/300, resid Loss: 0.0941 | 0.0454
Epoch 27/300, resid Loss: 0.0902 | 0.0436
Epoch 28/300, resid Loss: 0.0952 | 0.0417
Epoch 29/300, resid Loss: 0.0936 | 0.0524
Epoch 30/300, resid Loss: 0.0950 | 0.0505
Epoch 31/300, resid Loss: 0.0931 | 0.0612
Epoch 32/300, resid Loss: 0.0963 | 0.0592
Epoch 33/300, resid Loss: 0.0901 | 0.0483
Epoch 34/300, resid Loss: 0.0840 | 0.0500
Epoch 35/300, resid Loss: 0.0802 | 0.0399
Epoch 36/300, resid Loss: 0.0756 | 0.0369
Epoch 37/300, resid Loss: 0.0734 | 0.0329
Epoch 38/300, resid Loss: 0.0732 | 0.0337
Epoch 39/300, resid Loss: 0.0748 | 0.0335
Epoch 40/300, resid Loss: 0.0752 | 0.0362
Epoch 41/300, resid Loss: 0.0770 | 0.0357
Epoch 42/300, resid Loss: 0.0766 | 0.0363
Epoch 43/300, resid Loss: 0.0741 | 0.0350
Epoch 44/300, resid Loss: 0.0723 | 0.0341
Epoch 45/300, resid Loss: 0.0704 | 0.0320
Epoch 46/300, resid Loss: 0.0689 | 0.0308
Epoch 47/300, resid Loss: 0.0685 | 0.0299
Epoch 48/300, resid Loss: 0.0687 | 0.0304
Epoch 49/300, resid Loss: 0.0687 | 0.0302
Epoch 50/300, resid Loss: 0.0685 | 0.0313
Epoch 51/300, resid Loss: 0.0679 | 0.0319
Epoch 52/300, resid Loss: 0.0674 | 0.0353
Epoch 53/300, resid Loss: 0.0668 | 0.0362
Epoch 54/300, resid Loss: 0.0667 | 0.0305
Epoch 55/300, resid Loss: 0.0671 | 0.0314
Epoch 56/300, resid Loss: 0.0691 | 0.0366
Epoch 57/300, resid Loss: 0.0721 | 0.0371
Epoch 58/300, resid Loss: 0.0731 | 0.0342
Epoch 59/300, resid Loss: 0.0733 | 0.0333
Epoch 60/300, resid Loss: 0.0751 | 0.0342
Epoch 61/300, resid Loss: 0.0745 | 0.0358
Epoch 62/300, resid Loss: 0.0745 | 0.0345
Epoch 63/300, resid Loss: 0.0717 | 0.0318
Epoch 64/300, resid Loss: 0.0729 | 0.0312
Epoch 65/300, resid Loss: 0.0693 | 0.0292
Epoch 66/300, resid Loss: 0.0707 | 0.0326
Epoch 67/300, resid Loss: 0.0698 | 0.0294
Epoch 68/300, resid Loss: 0.0721 | 0.0297
Epoch 69/300, resid Loss: 0.0714 | 0.0305
Epoch 70/300, resid Loss: 0.0702 | 0.0290
Epoch 71/300, resid Loss: 0.0693 | 0.0326
Epoch 72/300, resid Loss: 0.0665 | 0.0288
Epoch 73/300, resid Loss: 0.0642 | 0.0267
Epoch 74/300, resid Loss: 0.0634 | 0.0293
Epoch 75/300, resid Loss: 0.0651 | 0.0276
Epoch 76/300, resid Loss: 0.0636 | 0.0323
Epoch 77/300, resid Loss: 0.0667 | 0.0290
Epoch 78/300, resid Loss: 0.0642 | 0.0347
Epoch 79/300, resid Loss: 0.0664 | 0.0302
Epoch 80/300, resid Loss: 0.0626 | 0.0292
Epoch 81/300, resid Loss: 0.0619 | 0.0276
Epoch 82/300, resid Loss: 0.0619 | 0.0280
Epoch 83/300, resid Loss: 0.0615 | 0.0272
Epoch 84/300, resid Loss: 0.0616 | 0.0276
Epoch 85/300, resid Loss: 0.0616 | 0.0277
Epoch 86/300, resid Loss: 0.0615 | 0.0278
Epoch 87/300, resid Loss: 0.0604 | 0.0270
Epoch 88/300, resid Loss: 0.0595 | 0.0267
Epoch 89/300, resid Loss: 0.0591 | 0.0267
Epoch 90/300, resid Loss: 0.0589 | 0.0272
Epoch 91/300, resid Loss: 0.0594 | 0.0278
Epoch 92/300, resid Loss: 0.0611 | 0.0291
Epoch 93/300, resid Loss: 0.0605 | 0.0283
Epoch 94/300, resid Loss: 0.0597 | 0.0267
Epoch 95/300, resid Loss: 0.0588 | 0.0262
Epoch 96/300, resid Loss: 0.0579 | 0.0257
Epoch 97/300, resid Loss: 0.0575 | 0.0256
Epoch 98/300, resid Loss: 0.0572 | 0.0257
Epoch 99/300, resid Loss: 0.0575 | 0.0259
Epoch 100/300, resid Loss: 0.0573 | 0.0257
Epoch 101/300, resid Loss: 0.0576 | 0.0260
Epoch 102/300, resid Loss: 0.0577 | 0.0261
Epoch 103/300, resid Loss: 0.0589 | 0.0279
Epoch 104/300, resid Loss: 0.0600 | 0.0285
Epoch 105/300, resid Loss: 0.0610 | 0.0313
Epoch 106/300, resid Loss: 0.0625 | 0.0326
Epoch 107/300, resid Loss: 0.0623 | 0.0300
Epoch 108/300, resid Loss: 0.0629 | 0.0296
Epoch 109/300, resid Loss: 0.0613 | 0.0301
Epoch 110/300, resid Loss: 0.0601 | 0.0288
Epoch 111/300, resid Loss: 0.0589 | 0.0271
Epoch 112/300, resid Loss: 0.0585 | 0.0267
Epoch 113/300, resid Loss: 0.0602 | 0.0276
Epoch 114/300, resid Loss: 0.0601 | 0.0285
Epoch 115/300, resid Loss: 0.0593 | 0.0282
Epoch 116/300, resid Loss: 0.0589 | 0.0287
Epoch 117/300, resid Loss: 0.0593 | 0.0284
Epoch 118/300, resid Loss: 0.0598 | 0.0284
Epoch 119/300, resid Loss: 0.0603 | 0.0272
Epoch 120/300, resid Loss: 0.0584 | 0.0274
Epoch 121/300, resid Loss: 0.0575 | 0.0270
Epoch 122/300, resid Loss: 0.0565 | 0.0265
Epoch 123/300, resid Loss: 0.0562 | 0.0260
Epoch 124/300, resid Loss: 0.0567 | 0.0266
Epoch 125/300, resid Loss: 0.0579 | 0.0262
Epoch 126/300, resid Loss: 0.0585 | 0.0277
Epoch 127/300, resid Loss: 0.0583 | 0.0267
Epoch 128/300, resid Loss: 0.0578 | 0.0273
Epoch 129/300, resid Loss: 0.0577 | 0.0285
Epoch 130/300, resid Loss: 0.0583 | 0.0275
Epoch 131/300, resid Loss: 0.0602 | 0.0280
Epoch 132/300, resid Loss: 0.0628 | 0.0277
Epoch 133/300, resid Loss: 0.0636 | 0.0298
Epoch 134/300, resid Loss: 0.0619 | 0.0301
Epoch 135/300, resid Loss: 0.0604 | 0.0289
Epoch 136/300, resid Loss: 0.0614 | 0.0286
Epoch 137/300, resid Loss: 0.0636 | 0.0300
Epoch 138/300, resid Loss: 0.0620 | 0.0308
Epoch 139/300, resid Loss: 0.0576 | 0.0313
Epoch 140/300, resid Loss: 0.0588 | 0.0280
Epoch 141/300, resid Loss: 0.0622 | 0.0284
Epoch 142/300, resid Loss: 0.0614 | 0.0279
Epoch 143/300, resid Loss: 0.0572 | 0.0283
Epoch 144/300, resid Loss: 0.0592 | 0.0289
Epoch 145/300, resid Loss: 0.0618 | 0.0316
Epoch 146/300, resid Loss: 0.0567 | 0.0295
Epoch 147/300, resid Loss: 0.0559 | 0.0277
Epoch 148/300, resid Loss: 0.0570 | 0.0273
Epoch 149/300, resid Loss: 0.0552 | 0.0268
Epoch 150/300, resid Loss: 0.0536 | 0.0266
Epoch 151/300, resid Loss: 0.0534 | 0.0264
Epoch 152/300, resid Loss: 0.0529 | 0.0259
Epoch 153/300, resid Loss: 0.0526 | 0.0259
Epoch 154/300, resid Loss: 0.0524 | 0.0261
Epoch 155/300, resid Loss: 0.0525 | 0.0263
Epoch 156/300, resid Loss: 0.0523 | 0.0262
Epoch 157/300, resid Loss: 0.0520 | 0.0261
Epoch 158/300, resid Loss: 0.0518 | 0.0258
Epoch 159/300, resid Loss: 0.0517 | 0.0260
Epoch 160/300, resid Loss: 0.0515 | 0.0259
Epoch 161/300, resid Loss: 0.0515 | 0.0263
Epoch 162/300, resid Loss: 0.0517 | 0.0262
Epoch 163/300, resid Loss: 0.0517 | 0.0266
Epoch 164/300, resid Loss: 0.0514 | 0.0263
Epoch 165/300, resid Loss: 0.0512 | 0.0263
Epoch 166/300, resid Loss: 0.0511 | 0.0261
Epoch 167/300, resid Loss: 0.0510 | 0.0263
Epoch 168/300, resid Loss: 0.0509 | 0.0262
Epoch 169/300, resid Loss: 0.0509 | 0.0266
Epoch 170/300, resid Loss: 0.0511 | 0.0265
Epoch 171/300, resid Loss: 0.0512 | 0.0270
Epoch 172/300, resid Loss: 0.0509 | 0.0267
Epoch 173/300, resid Loss: 0.0507 | 0.0267
Epoch 174/300, resid Loss: 0.0506 | 0.0265
Epoch 175/300, resid Loss: 0.0504 | 0.0267
Epoch 176/300, resid Loss: 0.0503 | 0.0268
Epoch 177/300, resid Loss: 0.0503 | 0.0271
Epoch 178/300, resid Loss: 0.0505 | 0.0268
Epoch 179/300, resid Loss: 0.0505 | 0.0274
Epoch 180/300, resid Loss: 0.0503 | 0.0271
Epoch 181/300, resid Loss: 0.0501 | 0.0271
Epoch 182/300, resid Loss: 0.0500 | 0.0270
Epoch 183/300, resid Loss: 0.0500 | 0.0272
Epoch 184/300, resid Loss: 0.0498 | 0.0272
Epoch 185/300, resid Loss: 0.0497 | 0.0276
Epoch 186/300, resid Loss: 0.0499 | 0.0273
Epoch 187/300, resid Loss: 0.0499 | 0.0277
Epoch 188/300, resid Loss: 0.0499 | 0.0277
Epoch 189/300, resid Loss: 0.0496 | 0.0279
Epoch 190/300, resid Loss: 0.0496 | 0.0274
Epoch 191/300, resid Loss: 0.0496 | 0.0278
Epoch 192/300, resid Loss: 0.0494 | 0.0275
Epoch 193/300, resid Loss: 0.0493 | 0.0281
Epoch 194/300, resid Loss: 0.0493 | 0.0281
Epoch 195/300, resid Loss: 0.0491 | 0.0282
Epoch 196/300, resid Loss: 0.0490 | 0.0285
Epoch 197/300, resid Loss: 0.0490 | 0.0283
Epoch 198/300, resid Loss: 0.0490 | 0.0285
Epoch 199/300, resid Loss: 0.0485 | 0.0288
Epoch 200/300, resid Loss: 0.0482 | 0.0284
Epoch 201/300, resid Loss: 0.0480 | 0.0295
Epoch 202/300, resid Loss: 0.0479 | 0.0292
Epoch 203/300, resid Loss: 0.0500 | 0.0304
Epoch 204/300, resid Loss: 0.0477 | 0.0294
Epoch 205/300, resid Loss: 0.0472 | 0.0306
Epoch 206/300, resid Loss: 0.0469 | 0.0304
Epoch 207/300, resid Loss: 0.0458 | 0.0294
Epoch 208/300, resid Loss: 0.0467 | 0.0298
Epoch 209/300, resid Loss: 0.0554 | 0.0307
Epoch 210/300, resid Loss: 0.0557 | 0.0292
Epoch 211/300, resid Loss: 0.0508 | 0.0272
Epoch 212/300, resid Loss: 0.0473 | 0.0287
Epoch 213/300, resid Loss: 0.0451 | 0.0293
Epoch 214/300, resid Loss: 0.0437 | 0.0279
Epoch 215/300, resid Loss: 0.0431 | 0.0278
Epoch 216/300, resid Loss: 0.0426 | 0.0276
Epoch 217/300, resid Loss: 0.0424 | 0.0276
Epoch 218/300, resid Loss: 0.0422 | 0.0276
Epoch 219/300, resid Loss: 0.0420 | 0.0277
Epoch 220/300, resid Loss: 0.0419 | 0.0277
Epoch 221/300, resid Loss: 0.0418 | 0.0276
Epoch 222/300, resid Loss: 0.0417 | 0.0276
Epoch 223/300, resid Loss: 0.0416 | 0.0276
Epoch 224/300, resid Loss: 0.0415 | 0.0276
Epoch 225/300, resid Loss: 0.0414 | 0.0276
Epoch 226/300, resid Loss: 0.0413 | 0.0276
Epoch 227/300, resid Loss: 0.0413 | 0.0276
Epoch 228/300, resid Loss: 0.0412 | 0.0276
Epoch 229/300, resid Loss: 0.0411 | 0.0276
Epoch 230/300, resid Loss: 0.0411 | 0.0276
Epoch 231/300, resid Loss: 0.0410 | 0.0276
Epoch 232/300, resid Loss: 0.0409 | 0.0276
Epoch 233/300, resid Loss: 0.0409 | 0.0276
Epoch 234/300, resid Loss: 0.0408 | 0.0276
Epoch 235/300, resid Loss: 0.0408 | 0.0276
Epoch 236/300, resid Loss: 0.0407 | 0.0276
Epoch 237/300, resid Loss: 0.0407 | 0.0276
Epoch 238/300, resid Loss: 0.0406 | 0.0277
Epoch 239/300, resid Loss: 0.0406 | 0.0277
Epoch 240/300, resid Loss: 0.0405 | 0.0277
Epoch 241/300, resid Loss: 0.0405 | 0.0277
Epoch 242/300, resid Loss: 0.0404 | 0.0277
Epoch 243/300, resid Loss: 0.0404 | 0.0277
Epoch 244/300, resid Loss: 0.0404 | 0.0277
Epoch 245/300, resid Loss: 0.0403 | 0.0277
Epoch 246/300, resid Loss: 0.0403 | 0.0277
Epoch 247/300, resid Loss: 0.0402 | 0.0277
Epoch 248/300, resid Loss: 0.0402 | 0.0277
Epoch 249/300, resid Loss: 0.0401 | 0.0278
Epoch 250/300, resid Loss: 0.0401 | 0.0278
Epoch 251/300, resid Loss: 0.0401 | 0.0278
Epoch 252/300, resid Loss: 0.0400 | 0.0279
Epoch 253/300, resid Loss: 0.0400 | 0.0279
Epoch 254/300, resid Loss: 0.0400 | 0.0279
Epoch 255/300, resid Loss: 0.0399 | 0.0279
Epoch 256/300, resid Loss: 0.0399 | 0.0278
Epoch 257/300, resid Loss: 0.0398 | 0.0278
Epoch 258/300, resid Loss: 0.0398 | 0.0277
Epoch 259/300, resid Loss: 0.0399 | 0.0278
Epoch 260/300, resid Loss: 0.0399 | 0.0279
Epoch 261/300, resid Loss: 0.0398 | 0.0280
Epoch 262/300, resid Loss: 0.0397 | 0.0282
Epoch 263/300, resid Loss: 0.0397 | 0.0283
Epoch 264/300, resid Loss: 0.0398 | 0.0282
Epoch 265/300, resid Loss: 0.0398 | 0.0282
Epoch 266/300, resid Loss: 0.0396 | 0.0281
Epoch 267/300, resid Loss: 0.0396 | 0.0279
Epoch 268/300, resid Loss: 0.0396 | 0.0280
Epoch 269/300, resid Loss: 0.0395 | 0.0281
Epoch 270/300, resid Loss: 0.0394 | 0.0282
Epoch 271/300, resid Loss: 0.0393 | 0.0283
Epoch 272/300, resid Loss: 0.0394 | 0.0283
Epoch 273/300, resid Loss: 0.0393 | 0.0283
Epoch 274/300, resid Loss: 0.0393 | 0.0282
Epoch 275/300, resid Loss: 0.0392 | 0.0282
Epoch 276/300, resid Loss: 0.0392 | 0.0282
Epoch 277/300, resid Loss: 0.0392 | 0.0282
Epoch 278/300, resid Loss: 0.0391 | 0.0283
Epoch 279/300, resid Loss: 0.0391 | 0.0284
Epoch 280/300, resid Loss: 0.0391 | 0.0284
Epoch 281/300, resid Loss: 0.0391 | 0.0284
Epoch 282/300, resid Loss: 0.0390 | 0.0284
Epoch 283/300, resid Loss: 0.0390 | 0.0283
Epoch 284/300, resid Loss: 0.0389 | 0.0283
Epoch 285/300, resid Loss: 0.0389 | 0.0283
Epoch 286/300, resid Loss: 0.0389 | 0.0284
Epoch 287/300, resid Loss: 0.0388 | 0.0285
Epoch 288/300, resid Loss: 0.0388 | 0.0286
Epoch 289/300, resid Loss: 0.0388 | 0.0286
Epoch 290/300, resid Loss: 0.0388 | 0.0285
Epoch 291/300, resid Loss: 0.0387 | 0.0285
Epoch 292/300, resid Loss: 0.0387 | 0.0285
Epoch 293/300, resid Loss: 0.0387 | 0.0284
Epoch 294/300, resid Loss: 0.0387 | 0.0285
Epoch 295/300, resid Loss: 0.0386 | 0.0286
Epoch 296/300, resid Loss: 0.0386 | 0.0286
Epoch 297/300, resid Loss: 0.0386 | 0.0287
Epoch 298/300, resid Loss: 0.0386 | 0.0287
Epoch 299/300, resid Loss: 0.0385 | 0.0286
Epoch 300/300, resid Loss: 0.0385 | 0.0286
Runtime (seconds): 1277.6227324008942
0.0006221488770577347
[157.03119]
[-4.0142016]
[3.2174883]
[15.319838]
[2.651454]
[23.048439]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.01801401749253273
RMSE: 0.13421630859375
MAE: 0.13421630859375
R-squared: nan
[197.25421]
File amzn_stock_price_prediction_by_iTransformer.png exists. Logging to WandB.
