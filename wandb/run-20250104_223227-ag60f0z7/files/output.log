[32m[I 2025-01-04 22:32:29,396][0m A new study created in memory with name: no-name-97cb27b4-5d74-4986-9e83-a9e41f52dd20[0m
[32m[I 2025-01-04 22:35:26,777][0m Trial 0 finished with value: 0.28411676324143703 and parameters: {'observation_period_num': 236, 'train_rates': 0.9820440033230331, 'learning_rate': 1.1883479737638973e-06, 'batch_size': 25, 'step_size': 10, 'gamma': 0.9714578120300147}. Best is trial 0 with value: 0.28411676324143703.[0m
[32m[I 2025-01-04 22:36:02,751][0m Trial 1 finished with value: 0.3132585482843266 and parameters: {'observation_period_num': 187, 'train_rates': 0.7645564937027236, 'learning_rate': 0.0004052627808905491, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8398189347321348}. Best is trial 0 with value: 0.28411676324143703.[0m
[32m[I 2025-01-04 22:36:37,199][0m Trial 2 finished with value: 0.22071948280251275 and parameters: {'observation_period_num': 137, 'train_rates': 0.6745454791237375, 'learning_rate': 0.0007035516877927867, 'batch_size': 227, 'step_size': 8, 'gamma': 0.8862407060444603}. Best is trial 2 with value: 0.22071948280251275.[0m
[32m[I 2025-01-04 22:37:15,230][0m Trial 3 finished with value: 0.08620610398054122 and parameters: {'observation_period_num': 193, 'train_rates': 0.864276619010959, 'learning_rate': 0.00016318411084067987, 'batch_size': 171, 'step_size': 12, 'gamma': 0.8516059183629094}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:37:50,603][0m Trial 4 finished with value: 0.3810931947608452 and parameters: {'observation_period_num': 222, 'train_rates': 0.7410296617915584, 'learning_rate': 3.742370719538772e-05, 'batch_size': 250, 'step_size': 9, 'gamma': 0.8764961161156932}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:38:27,432][0m Trial 5 finished with value: 0.7061724536499735 and parameters: {'observation_period_num': 136, 'train_rates': 0.6653600254251664, 'learning_rate': 5.578470441874883e-06, 'batch_size': 184, 'step_size': 5, 'gamma': 0.7937810639445374}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:40:53,392][0m Trial 6 finished with value: 0.2559855474243439 and parameters: {'observation_period_num': 189, 'train_rates': 0.7363982389404419, 'learning_rate': 0.00023415918131201415, 'batch_size': 24, 'step_size': 2, 'gamma': 0.7512139545344629}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:41:30,986][0m Trial 7 finished with value: 0.42706305082933405 and parameters: {'observation_period_num': 243, 'train_rates': 0.9104614297520263, 'learning_rate': 2.7470435037861716e-06, 'batch_size': 182, 'step_size': 14, 'gamma': 0.8922892549878576}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:42:05,680][0m Trial 8 finished with value: 0.2095099808161996 and parameters: {'observation_period_num': 87, 'train_rates': 0.7395016728396604, 'learning_rate': 0.00035960560457494234, 'batch_size': 222, 'step_size': 15, 'gamma': 0.9722627094834921}. Best is trial 3 with value: 0.08620610398054122.[0m
Early stopping at epoch 67
[32m[I 2025-01-04 22:42:51,162][0m Trial 9 finished with value: 0.26397130345031034 and parameters: {'observation_period_num': 16, 'train_rates': 0.8274028157612046, 'learning_rate': 5.812769146132831e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.82486647920427}. Best is trial 3 with value: 0.08620610398054122.[0m
[32m[I 2025-01-04 22:43:35,846][0m Trial 10 finished with value: 0.07386619679848938 and parameters: {'observation_period_num': 73, 'train_rates': 0.8585957055647535, 'learning_rate': 7.256702738041666e-05, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9341873852737339}. Best is trial 10 with value: 0.07386619679848938.[0m
[32m[I 2025-01-04 22:44:21,551][0m Trial 11 finished with value: 0.06953692567440072 and parameters: {'observation_period_num': 67, 'train_rates': 0.8535193824134794, 'learning_rate': 6.421242047076796e-05, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9227908434007241}. Best is trial 11 with value: 0.06953692567440072.[0m
[32m[I 2025-01-04 22:45:10,305][0m Trial 12 finished with value: 0.06229033721120734 and parameters: {'observation_period_num': 67, 'train_rates': 0.9014147999426305, 'learning_rate': 4.262709860772364e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.9301808606426865}. Best is trial 12 with value: 0.06229033721120734.[0m
[32m[I 2025-01-04 22:45:59,073][0m Trial 13 finished with value: 0.06540476453009543 and parameters: {'observation_period_num': 32, 'train_rates': 0.9378454619095945, 'learning_rate': 1.751314320999518e-05, 'batch_size': 107, 'step_size': 7, 'gamma': 0.9275128689647338}. Best is trial 12 with value: 0.06229033721120734.[0m
[32m[I 2025-01-04 22:47:04,824][0m Trial 14 finished with value: 0.06488775631243532 and parameters: {'observation_period_num': 10, 'train_rates': 0.9666504440790772, 'learning_rate': 1.4681044582571264e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.9293665198141936}. Best is trial 12 with value: 0.06229033721120734.[0m
[32m[I 2025-01-04 22:48:11,894][0m Trial 15 finished with value: 0.05756980553269386 and parameters: {'observation_period_num': 5, 'train_rates': 0.9764591090948149, 'learning_rate': 1.7986048995971732e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.9513919464015411}. Best is trial 15 with value: 0.05756980553269386.[0m
[32m[I 2025-01-04 22:49:16,126][0m Trial 16 finished with value: 0.06152026255636714 and parameters: {'observation_period_num': 46, 'train_rates': 0.9114127282045574, 'learning_rate': 1.6563206527978297e-05, 'batch_size': 71, 'step_size': 4, 'gamma': 0.9830086474776234}. Best is trial 15 with value: 0.05756980553269386.[0m
[32m[I 2025-01-04 22:50:08,325][0m Trial 17 finished with value: 0.42770338856607754 and parameters: {'observation_period_num': 33, 'train_rates': 0.6108956051554248, 'learning_rate': 1.570311164637356e-05, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9831852797855901}. Best is trial 15 with value: 0.05756980553269386.[0m
[32m[I 2025-01-04 22:51:52,472][0m Trial 18 finished with value: 0.1044281228984657 and parameters: {'observation_period_num': 106, 'train_rates': 0.9334989029879432, 'learning_rate': 6.924474376861433e-06, 'batch_size': 45, 'step_size': 5, 'gamma': 0.9571671862089567}. Best is trial 15 with value: 0.05756980553269386.[0m
[32m[I 2025-01-04 22:52:53,696][0m Trial 19 finished with value: 0.053076530767368596 and parameters: {'observation_period_num': 44, 'train_rates': 0.80496897994912, 'learning_rate': 9.213544255054547e-05, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9554669023254009}. Best is trial 19 with value: 0.053076530767368596.[0m
[32m[I 2025-01-04 22:53:37,460][0m Trial 20 finished with value: 0.10054558456794127 and parameters: {'observation_period_num': 114, 'train_rates': 0.7903106125153269, 'learning_rate': 0.0001014222396232824, 'batch_size': 153, 'step_size': 7, 'gamma': 0.9041650500544489}. Best is trial 19 with value: 0.053076530767368596.[0m
[32m[I 2025-01-04 22:54:54,636][0m Trial 21 finished with value: 0.07031258195638657 and parameters: {'observation_period_num': 50, 'train_rates': 0.9833776764193088, 'learning_rate': 2.495088217518283e-05, 'batch_size': 85, 'step_size': 4, 'gamma': 0.9549010593736247}. Best is trial 19 with value: 0.053076530767368596.[0m
[32m[I 2025-01-04 22:55:51,688][0m Trial 22 finished with value: 0.07622601388438234 and parameters: {'observation_period_num': 40, 'train_rates': 0.8942934991625742, 'learning_rate': 1.1990040308058774e-05, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9877874260970443}. Best is trial 19 with value: 0.053076530767368596.[0m
[32m[I 2025-01-04 22:57:47,839][0m Trial 23 finished with value: 0.030160294676368885 and parameters: {'observation_period_num': 8, 'train_rates': 0.9441911121278512, 'learning_rate': 0.0001323302573872996, 'batch_size': 39, 'step_size': 8, 'gamma': 0.9559756815793978}. Best is trial 23 with value: 0.030160294676368885.[0m
[32m[I 2025-01-04 22:59:33,851][0m Trial 24 finished with value: 0.026484874801503286 and parameters: {'observation_period_num': 7, 'train_rates': 0.9422363089442014, 'learning_rate': 0.00015098541157199712, 'batch_size': 43, 'step_size': 9, 'gamma': 0.9514464171389687}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:01:03,952][0m Trial 25 finished with value: 0.04789065795260158 and parameters: {'observation_period_num': 22, 'train_rates': 0.8129025639878327, 'learning_rate': 0.00014098021555275264, 'batch_size': 43, 'step_size': 9, 'gamma': 0.9113331911709343}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:02:54,363][0m Trial 26 finished with value: 0.04086827061006001 and parameters: {'observation_period_num': 22, 'train_rates': 0.9476512902188411, 'learning_rate': 0.00014474274735896319, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9106209902340163}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:04:32,252][0m Trial 27 finished with value: 0.13609734066124038 and parameters: {'observation_period_num': 95, 'train_rates': 0.9555891143327924, 'learning_rate': 0.0006960634067781151, 'batch_size': 45, 'step_size': 10, 'gamma': 0.9071076558456452}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:08:05,475][0m Trial 28 finished with value: 0.06088116568293464 and parameters: {'observation_period_num': 59, 'train_rates': 0.9385592395230363, 'learning_rate': 0.0002180737217428604, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8630796062683862}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:10:03,509][0m Trial 29 finished with value: 0.055609461250995154 and parameters: {'observation_period_num': 5, 'train_rates': 0.8818747719655609, 'learning_rate': 0.00037096791412405964, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9399246646305353}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:10:49,538][0m Trial 30 finished with value: 0.07209193706512451 and parameters: {'observation_period_num': 30, 'train_rates': 0.9883708707666685, 'learning_rate': 5.5066573244065305e-05, 'batch_size': 141, 'step_size': 8, 'gamma': 0.9732873026713879}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:13:08,198][0m Trial 31 finished with value: 0.038489986377212924 and parameters: {'observation_period_num': 22, 'train_rates': 0.9276787997975064, 'learning_rate': 0.00014651833422377747, 'batch_size': 37, 'step_size': 9, 'gamma': 0.9134952953593851}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:14:53,514][0m Trial 32 finished with value: 0.03457695425034844 and parameters: {'observation_period_num': 20, 'train_rates': 0.9363914211092231, 'learning_rate': 0.00013235618510661935, 'batch_size': 52, 'step_size': 11, 'gamma': 0.9153843807603093}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:16:35,069][0m Trial 33 finished with value: 0.03594466895496441 and parameters: {'observation_period_num': 20, 'train_rates': 0.9197218460002707, 'learning_rate': 0.00026987552057561917, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8915838843034647}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:18:40,090][0m Trial 34 finished with value: 0.14348459875405725 and parameters: {'observation_period_num': 159, 'train_rates': 0.8803229094409091, 'learning_rate': 0.0005522701398626304, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8781556075583277}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:22:08,296][0m Trial 35 finished with value: 0.08546997280791402 and parameters: {'observation_period_num': 158, 'train_rates': 0.9201686383973715, 'learning_rate': 0.0002536298460549322, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9440150390519049}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:24:22,810][0m Trial 36 finished with value: 0.08723033855444398 and parameters: {'observation_period_num': 81, 'train_rates': 0.9597061771520804, 'learning_rate': 0.0008691973133427704, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8921528837500464}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:25:50,187][0m Trial 37 finished with value: 0.07021235493700499 and parameters: {'observation_period_num': 56, 'train_rates': 0.8437344605974805, 'learning_rate': 0.00048797439925968864, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8476158682626108}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:30:15,694][0m Trial 38 finished with value: 0.029940030842769045 and parameters: {'observation_period_num': 5, 'train_rates': 0.878965161704169, 'learning_rate': 0.00028806003449234467, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8270551574060854}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:32:48,629][0m Trial 39 finished with value: 0.08618083904992353 and parameters: {'observation_period_num': 216, 'train_rates': 0.8785039024448775, 'learning_rate': 0.00010162841285114641, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8201360763514448}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:37:02,292][0m Trial 40 finished with value: 0.17175181545726545 and parameters: {'observation_period_num': 6, 'train_rates': 0.7842511843073856, 'learning_rate': 2.8678037064795405e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7788315777460773}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:39:11,227][0m Trial 41 finished with value: 0.04729103547856463 and parameters: {'observation_period_num': 21, 'train_rates': 0.8988936112939676, 'learning_rate': 0.0002876653932376601, 'batch_size': 56, 'step_size': 10, 'gamma': 0.8343211504130655}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:42:23,562][0m Trial 42 finished with value: 0.05416294212456997 and parameters: {'observation_period_num': 36, 'train_rates': 0.9188034948336791, 'learning_rate': 0.0001947426795319125, 'batch_size': 28, 'step_size': 11, 'gamma': 0.8662382806720241}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:44:41,493][0m Trial 43 finished with value: 0.028889694533339054 and parameters: {'observation_period_num': 17, 'train_rates': 0.9552323541119001, 'learning_rate': 0.000294686273053302, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8070257055746392}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:47:54,748][0m Trial 44 finished with value: 0.19684576821104388 and parameters: {'observation_period_num': 14, 'train_rates': 0.9637358598166023, 'learning_rate': 1.4081207862706526e-06, 'batch_size': 32, 'step_size': 10, 'gamma': 0.770351576979937}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:48:48,348][0m Trial 45 finished with value: 0.047571681439876556 and parameters: {'observation_period_num': 28, 'train_rates': 0.947293476988929, 'learning_rate': 0.0003856559106583873, 'batch_size': 215, 'step_size': 7, 'gamma': 0.8139667637742141}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:49:55,239][0m Trial 46 finished with value: 0.05412902261391594 and parameters: {'observation_period_num': 42, 'train_rates': 0.8361347115644499, 'learning_rate': 0.0001755473538607696, 'batch_size': 61, 'step_size': 13, 'gamma': 0.8090374606478437}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:54:13,020][0m Trial 47 finished with value: 0.06617671666960967 and parameters: {'observation_period_num': 59, 'train_rates': 0.9672484919535668, 'learning_rate': 7.920334057342433e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7872642060129779}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:55:15,436][0m Trial 48 finished with value: 0.14352610116077807 and parameters: {'observation_period_num': 12, 'train_rates': 0.691454977284369, 'learning_rate': 0.00011987189014395492, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8032256220026058}. Best is trial 24 with value: 0.026484874801503286.[0m
[32m[I 2025-01-04 23:56:42,549][0m Trial 49 finished with value: 0.05360377687156644 and parameters: {'observation_period_num': 29, 'train_rates': 0.8624123853310466, 'learning_rate': 5.6642344277948154e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.7532716240621666}. Best is trial 24 with value: 0.026484874801503286.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AMZN_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2329 | 0.1187
Epoch 2/300, Loss: 0.1298 | 0.0825
Epoch 3/300, Loss: 0.1175 | 0.0697
Epoch 4/300, Loss: 0.1115 | 0.0661
Epoch 5/300, Loss: 0.1127 | 0.1015
Epoch 6/300, Loss: 0.1121 | 0.0599
Epoch 7/300, Loss: 0.1047 | 0.0614
Epoch 8/300, Loss: 0.0989 | 0.0593
Epoch 9/300, Loss: 0.0940 | 0.0572
Epoch 10/300, Loss: 0.0904 | 0.0542
Epoch 11/300, Loss: 0.0872 | 0.0517
Epoch 12/300, Loss: 0.0847 | 0.0495
Epoch 13/300, Loss: 0.0831 | 0.0481
Epoch 14/300, Loss: 0.0818 | 0.0468
Epoch 15/300, Loss: 0.0805 | 0.0448
Epoch 16/300, Loss: 0.0794 | 0.0436
Epoch 17/300, Loss: 0.0783 | 0.0425
Epoch 18/300, Loss: 0.0773 | 0.0416
Epoch 19/300, Loss: 0.0763 | 0.0405
Epoch 20/300, Loss: 0.0757 | 0.0402
Epoch 21/300, Loss: 0.0748 | 0.0399
Epoch 22/300, Loss: 0.0737 | 0.0402
Epoch 23/300, Loss: 0.0727 | 0.0404
Epoch 24/300, Loss: 0.0718 | 0.0402
Epoch 25/300, Loss: 0.0710 | 0.0397
Epoch 26/300, Loss: 0.0705 | 0.0386
Epoch 27/300, Loss: 0.0701 | 0.0375
Epoch 28/300, Loss: 0.0696 | 0.0362
Epoch 29/300, Loss: 0.0691 | 0.0356
Epoch 30/300, Loss: 0.0687 | 0.0352
Epoch 31/300, Loss: 0.0682 | 0.0349
Epoch 32/300, Loss: 0.0679 | 0.0347
Epoch 33/300, Loss: 0.0674 | 0.0343
Epoch 34/300, Loss: 0.0670 | 0.0341
Epoch 35/300, Loss: 0.0667 | 0.0340
Epoch 36/300, Loss: 0.0663 | 0.0337
Epoch 37/300, Loss: 0.0658 | 0.0332
Epoch 38/300, Loss: 0.0654 | 0.0331
Epoch 39/300, Loss: 0.0651 | 0.0329
Epoch 40/300, Loss: 0.0648 | 0.0328
Epoch 41/300, Loss: 0.0646 | 0.0328
Epoch 42/300, Loss: 0.0644 | 0.0331
Epoch 43/300, Loss: 0.0644 | 0.0334
Epoch 44/300, Loss: 0.0644 | 0.0337
Epoch 45/300, Loss: 0.0641 | 0.0335
Epoch 46/300, Loss: 0.0636 | 0.0325
Epoch 47/300, Loss: 0.0631 | 0.0317
Epoch 48/300, Loss: 0.0627 | 0.0310
Epoch 49/300, Loss: 0.0623 | 0.0300
Epoch 50/300, Loss: 0.0619 | 0.0296
Epoch 51/300, Loss: 0.0616 | 0.0287
Epoch 52/300, Loss: 0.0616 | 0.0295
Epoch 53/300, Loss: 0.0611 | 0.0282
Epoch 54/300, Loss: 0.0617 | 0.0305
Epoch 55/300, Loss: 0.0608 | 0.0297
Epoch 56/300, Loss: 0.0605 | 0.0289
Epoch 57/300, Loss: 0.0610 | 0.0308
Epoch 58/300, Loss: 0.0604 | 0.0295
Epoch 59/300, Loss: 0.0611 | 0.0308
Epoch 60/300, Loss: 0.0604 | 0.0288
Epoch 61/300, Loss: 0.0608 | 0.0286
Epoch 62/300, Loss: 0.0605 | 0.0276
Epoch 63/300, Loss: 0.0637 | 0.0290
Epoch 64/300, Loss: 0.0603 | 0.0283
Epoch 65/300, Loss: 0.0602 | 0.0279
Epoch 66/300, Loss: 0.0593 | 0.0279
Epoch 67/300, Loss: 0.0603 | 0.0280
Epoch 68/300, Loss: 0.0583 | 0.0275
Epoch 69/300, Loss: 0.0593 | 0.0278
Epoch 70/300, Loss: 0.0575 | 0.0271
Epoch 71/300, Loss: 0.0579 | 0.0272
Epoch 72/300, Loss: 0.0569 | 0.0267
Epoch 73/300, Loss: 0.0570 | 0.0265
Epoch 74/300, Loss: 0.0564 | 0.0263
Epoch 75/300, Loss: 0.0563 | 0.0262
Epoch 76/300, Loss: 0.0559 | 0.0262
Epoch 77/300, Loss: 0.0558 | 0.0260
Epoch 78/300, Loss: 0.0554 | 0.0259
Epoch 79/300, Loss: 0.0554 | 0.0258
Epoch 80/300, Loss: 0.0551 | 0.0257
Epoch 81/300, Loss: 0.0551 | 0.0258
Epoch 82/300, Loss: 0.0548 | 0.0257
Epoch 83/300, Loss: 0.0549 | 0.0259
Epoch 84/300, Loss: 0.0546 | 0.0255
Epoch 85/300, Loss: 0.0547 | 0.0255
Epoch 86/300, Loss: 0.0545 | 0.0250
Epoch 87/300, Loss: 0.0546 | 0.0253
Epoch 88/300, Loss: 0.0542 | 0.0252
Epoch 89/300, Loss: 0.0542 | 0.0254
Epoch 90/300, Loss: 0.0539 | 0.0252
Epoch 91/300, Loss: 0.0541 | 0.0256
Epoch 92/300, Loss: 0.0537 | 0.0255
Epoch 93/300, Loss: 0.0537 | 0.0260
Epoch 94/300, Loss: 0.0534 | 0.0258
Epoch 95/300, Loss: 0.0536 | 0.0266
Epoch 96/300, Loss: 0.0532 | 0.0260
Epoch 97/300, Loss: 0.0535 | 0.0267
Epoch 98/300, Loss: 0.0529 | 0.0262
Epoch 99/300, Loss: 0.0532 | 0.0269
Epoch 100/300, Loss: 0.0527 | 0.0264
Epoch 101/300, Loss: 0.0529 | 0.0269
Epoch 102/300, Loss: 0.0524 | 0.0265
Epoch 103/300, Loss: 0.0526 | 0.0270
Epoch 104/300, Loss: 0.0522 | 0.0265
Epoch 105/300, Loss: 0.0524 | 0.0270
Epoch 106/300, Loss: 0.0519 | 0.0266
Epoch 107/300, Loss: 0.0520 | 0.0268
Epoch 108/300, Loss: 0.0517 | 0.0267
Epoch 109/300, Loss: 0.0518 | 0.0268
Epoch 110/300, Loss: 0.0515 | 0.0267
Epoch 111/300, Loss: 0.0515 | 0.0268
Epoch 112/300, Loss: 0.0513 | 0.0266
Epoch 113/300, Loss: 0.0513 | 0.0266
Epoch 114/300, Loss: 0.0511 | 0.0267
Epoch 115/300, Loss: 0.0511 | 0.0267
Epoch 116/300, Loss: 0.0509 | 0.0268
Epoch 117/300, Loss: 0.0508 | 0.0268
Epoch 118/300, Loss: 0.0507 | 0.0270
Epoch 119/300, Loss: 0.0507 | 0.0272
Epoch 120/300, Loss: 0.0506 | 0.0272
Epoch 121/300, Loss: 0.0505 | 0.0273
Epoch 122/300, Loss: 0.0504 | 0.0273
Epoch 123/300, Loss: 0.0504 | 0.0276
Epoch 124/300, Loss: 0.0503 | 0.0276
Epoch 125/300, Loss: 0.0503 | 0.0276
Epoch 126/300, Loss: 0.0502 | 0.0274
Epoch 127/300, Loss: 0.0501 | 0.0274
Epoch 128/300, Loss: 0.0500 | 0.0271
Epoch 129/300, Loss: 0.0500 | 0.0269
Epoch 130/300, Loss: 0.0499 | 0.0266
Epoch 131/300, Loss: 0.0499 | 0.0265
Epoch 132/300, Loss: 0.0499 | 0.0266
Epoch 133/300, Loss: 0.0500 | 0.0269
Epoch 134/300, Loss: 0.0500 | 0.0273
Epoch 135/300, Loss: 0.0498 | 0.0272
Epoch 136/300, Loss: 0.0496 | 0.0271
Epoch 137/300, Loss: 0.0496 | 0.0269
Epoch 138/300, Loss: 0.0495 | 0.0266
Epoch 139/300, Loss: 0.0494 | 0.0264
Epoch 140/300, Loss: 0.0493 | 0.0264
Epoch 141/300, Loss: 0.0492 | 0.0264
Epoch 142/300, Loss: 0.0490 | 0.0263
Epoch 143/300, Loss: 0.0488 | 0.0263
Epoch 144/300, Loss: 0.0486 | 0.0263
Epoch 145/300, Loss: 0.0484 | 0.0264
Epoch 146/300, Loss: 0.0482 | 0.0265
Epoch 147/300, Loss: 0.0481 | 0.0268
Epoch 148/300, Loss: 0.0479 | 0.0277
Epoch 149/300, Loss: 0.0474 | 0.0284
Epoch 150/300, Loss: 0.0465 | 0.0283
Epoch 151/300, Loss: 0.0456 | 0.0285
Epoch 152/300, Loss: 0.0451 | 0.0312
Epoch 153/300, Loss: 0.0455 | 0.0398
Epoch 154/300, Loss: 0.0470 | 0.0399
Epoch 155/300, Loss: 0.0482 | 0.0314
Epoch 156/300, Loss: 0.0489 | 0.0367
Epoch 157/300, Loss: 0.0509 | 0.0306
Epoch 158/300, Loss: 0.0460 | 0.0272
Epoch 159/300, Loss: 0.0452 | 0.0265
Epoch 160/300, Loss: 0.0448 | 0.0298
Epoch 161/300, Loss: 0.0445 | 0.0268
Epoch 162/300, Loss: 0.0427 | 0.0272
Epoch 163/300, Loss: 0.0420 | 0.0267
Epoch 164/300, Loss: 0.0418 | 0.0267
Epoch 165/300, Loss: 0.0416 | 0.0267
Epoch 166/300, Loss: 0.0414 | 0.0267
Epoch 167/300, Loss: 0.0413 | 0.0267
Epoch 168/300, Loss: 0.0412 | 0.0267
Epoch 169/300, Loss: 0.0411 | 0.0267
Epoch 170/300, Loss: 0.0410 | 0.0267
Epoch 171/300, Loss: 0.0409 | 0.0268
Epoch 172/300, Loss: 0.0408 | 0.0267
Epoch 173/300, Loss: 0.0407 | 0.0267
Epoch 174/300, Loss: 0.0407 | 0.0268
Epoch 175/300, Loss: 0.0406 | 0.0267
Epoch 176/300, Loss: 0.0405 | 0.0268
Epoch 177/300, Loss: 0.0404 | 0.0267
Epoch 178/300, Loss: 0.0404 | 0.0269
Epoch 179/300, Loss: 0.0403 | 0.0267
Epoch 180/300, Loss: 0.0402 | 0.0269
Epoch 181/300, Loss: 0.0402 | 0.0267
Epoch 182/300, Loss: 0.0401 | 0.0269
Epoch 183/300, Loss: 0.0400 | 0.0267
Epoch 184/300, Loss: 0.0400 | 0.0269
Epoch 185/300, Loss: 0.0399 | 0.0267
Epoch 186/300, Loss: 0.0399 | 0.0269
Epoch 187/300, Loss: 0.0398 | 0.0267
Epoch 188/300, Loss: 0.0398 | 0.0270
Epoch 189/300, Loss: 0.0397 | 0.0267
Epoch 190/300, Loss: 0.0396 | 0.0270
Epoch 191/300, Loss: 0.0396 | 0.0267
Epoch 192/300, Loss: 0.0395 | 0.0270
Epoch 193/300, Loss: 0.0395 | 0.0268
Epoch 194/300, Loss: 0.0394 | 0.0270
Epoch 195/300, Loss: 0.0394 | 0.0268
Epoch 196/300, Loss: 0.0393 | 0.0270
Epoch 197/300, Loss: 0.0393 | 0.0268
Epoch 198/300, Loss: 0.0392 | 0.0270
Epoch 199/300, Loss: 0.0392 | 0.0268
Epoch 200/300, Loss: 0.0391 | 0.0270
Epoch 201/300, Loss: 0.0391 | 0.0268
Epoch 202/300, Loss: 0.0390 | 0.0270
Epoch 203/300, Loss: 0.0390 | 0.0269
Epoch 204/300, Loss: 0.0389 | 0.0270
Epoch 205/300, Loss: 0.0389 | 0.0269
Epoch 206/300, Loss: 0.0388 | 0.0270
Epoch 207/300, Loss: 0.0388 | 0.0269
Epoch 208/300, Loss: 0.0387 | 0.0271
Epoch 209/300, Loss: 0.0387 | 0.0269
Epoch 210/300, Loss: 0.0386 | 0.0271
Epoch 211/300, Loss: 0.0386 | 0.0269
Epoch 212/300, Loss: 0.0386 | 0.0271
Epoch 213/300, Loss: 0.0385 | 0.0270
Epoch 214/300, Loss: 0.0385 | 0.0271
Epoch 215/300, Loss: 0.0384 | 0.0270
Epoch 216/300, Loss: 0.0384 | 0.0272
Epoch 217/300, Loss: 0.0384 | 0.0270
Epoch 218/300, Loss: 0.0383 | 0.0272
Epoch 219/300, Loss: 0.0383 | 0.0271
Epoch 220/300, Loss: 0.0382 | 0.0272
Epoch 221/300, Loss: 0.0382 | 0.0271
Epoch 222/300, Loss: 0.0381 | 0.0273
Epoch 223/300, Loss: 0.0381 | 0.0272
Epoch 224/300, Loss: 0.0381 | 0.0274
Epoch 225/300, Loss: 0.0380 | 0.0272
Epoch 226/300, Loss: 0.0380 | 0.0275
Epoch 227/300, Loss: 0.0380 | 0.0273
Epoch 228/300, Loss: 0.0379 | 0.0275
Epoch 229/300, Loss: 0.0379 | 0.0273
Epoch 230/300, Loss: 0.0378 | 0.0276
Epoch 231/300, Loss: 0.0378 | 0.0274
Epoch 232/300, Loss: 0.0378 | 0.0277
Epoch 233/300, Loss: 0.0378 | 0.0275
Epoch 234/300, Loss: 0.0377 | 0.0277
Epoch 235/300, Loss: 0.0377 | 0.0275
Epoch 236/300, Loss: 0.0376 | 0.0278
Epoch 237/300, Loss: 0.0376 | 0.0275
Epoch 238/300, Loss: 0.0375 | 0.0278
Epoch 239/300, Loss: 0.0375 | 0.0276
Epoch 240/300, Loss: 0.0375 | 0.0278
Epoch 241/300, Loss: 0.0374 | 0.0276
Epoch 242/300, Loss: 0.0374 | 0.0278
Epoch 243/300, Loss: 0.0374 | 0.0276
Epoch 244/300, Loss: 0.0373 | 0.0277
Epoch 245/300, Loss: 0.0373 | 0.0275
Epoch 246/300, Loss: 0.0373 | 0.0277
Epoch 247/300, Loss: 0.0372 | 0.0276
Epoch 248/300, Loss: 0.0372 | 0.0277
Epoch 249/300, Loss: 0.0372 | 0.0275
Epoch 250/300, Loss: 0.0372 | 0.0277
Epoch 251/300, Loss: 0.0371 | 0.0275
Epoch 252/300, Loss: 0.0371 | 0.0276
Epoch 253/300, Loss: 0.0370 | 0.0275
Epoch 254/300, Loss: 0.0370 | 0.0276
Epoch 255/300, Loss: 0.0370 | 0.0275
Epoch 256/300, Loss: 0.0370 | 0.0276
Epoch 257/300, Loss: 0.0369 | 0.0275
Epoch 258/300, Loss: 0.0369 | 0.0276
Epoch 259/300, Loss: 0.0369 | 0.0275
Epoch 260/300, Loss: 0.0369 | 0.0276
Epoch 261/300, Loss: 0.0368 | 0.0275
Epoch 262/300, Loss: 0.0368 | 0.0276
Epoch 263/300, Loss: 0.0367 | 0.0275
Epoch 264/300, Loss: 0.0367 | 0.0276
Epoch 265/300, Loss: 0.0367 | 0.0276
Epoch 266/300, Loss: 0.0366 | 0.0276
Epoch 267/300, Loss: 0.0366 | 0.0276
Epoch 268/300, Loss: 0.0366 | 0.0277
Epoch 269/300, Loss: 0.0365 | 0.0276
Epoch 270/300, Loss: 0.0365 | 0.0277
Epoch 271/300, Loss: 0.0365 | 0.0277
Epoch 272/300, Loss: 0.0365 | 0.0277
Epoch 273/300, Loss: 0.0364 | 0.0277
Epoch 274/300, Loss: 0.0364 | 0.0278
Epoch 275/300, Loss: 0.0364 | 0.0277
Epoch 276/300, Loss: 0.0364 | 0.0278
Epoch 277/300, Loss: 0.0363 | 0.0278
Epoch 278/300, Loss: 0.0363 | 0.0278
Epoch 279/300, Loss: 0.0363 | 0.0278
Epoch 280/300, Loss: 0.0363 | 0.0278
Epoch 281/300, Loss: 0.0363 | 0.0278
Epoch 282/300, Loss: 0.0363 | 0.0279
Epoch 283/300, Loss: 0.0362 | 0.0278
Epoch 284/300, Loss: 0.0362 | 0.0279
Epoch 285/300, Loss: 0.0362 | 0.0279
Epoch 286/300, Loss: 0.0362 | 0.0279
Epoch 287/300, Loss: 0.0361 | 0.0279
Epoch 288/300, Loss: 0.0361 | 0.0279
Epoch 289/300, Loss: 0.0361 | 0.0279
Epoch 290/300, Loss: 0.0361 | 0.0279
Epoch 291/300, Loss: 0.0361 | 0.0279
Epoch 292/300, Loss: 0.0361 | 0.0280
Epoch 293/300, Loss: 0.0360 | 0.0279
Epoch 294/300, Loss: 0.0360 | 0.0280
Epoch 295/300, Loss: 0.0360 | 0.0280
Epoch 296/300, Loss: 0.0360 | 0.0280
Epoch 297/300, Loss: 0.0360 | 0.0280
Epoch 298/300, Loss: 0.0360 | 0.0280
Epoch 299/300, Loss: 0.0359 | 0.0280
Epoch 300/300, Loss: 0.0359 | 0.0280
Runtime (seconds): 316.75463676452637
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 13.04422575631179
RMSE: 3.6116790771484375
MAE: 3.6116790771484375
R-squared: nan
[204.27832]
