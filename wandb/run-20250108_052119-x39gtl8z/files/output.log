ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-08 05:21:22,413][0m A new study created in memory with name: no-name-73753608-244c-4f4a-9855-3ee0209ff1d7[0m
[32m[I 2025-01-08 05:22:27,735][0m Trial 0 finished with value: 0.2582511793491509 and parameters: {'observation_period_num': 191, 'train_rates': 0.6981701980830769, 'learning_rate': 0.0007613488610566987, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8195305846964697}. Best is trial 0 with value: 0.2582511793491509.[0m
[32m[I 2025-01-08 05:23:11,956][0m Trial 1 finished with value: 0.3964297701703741 and parameters: {'observation_period_num': 189, 'train_rates': 0.847316549768736, 'learning_rate': 3.292986585334401e-06, 'batch_size': 180, 'step_size': 12, 'gamma': 0.758748917615017}. Best is trial 0 with value: 0.2582511793491509.[0m
[32m[I 2025-01-08 05:23:48,679][0m Trial 2 finished with value: 0.2630121028886901 and parameters: {'observation_period_num': 92, 'train_rates': 0.6071544401707007, 'learning_rate': 0.00011485432437661291, 'batch_size': 161, 'step_size': 9, 'gamma': 0.9197325230564182}. Best is trial 0 with value: 0.2582511793491509.[0m
[32m[I 2025-01-08 05:24:51,286][0m Trial 3 finished with value: 0.6581555337741457 and parameters: {'observation_period_num': 190, 'train_rates': 0.6435966300158932, 'learning_rate': 2.0760214109642946e-06, 'batch_size': 103, 'step_size': 12, 'gamma': 0.8489046300921925}. Best is trial 0 with value: 0.2582511793491509.[0m
[32m[I 2025-01-08 05:25:37,632][0m Trial 4 finished with value: 0.36634782986029196 and parameters: {'observation_period_num': 241, 'train_rates': 0.7199529583895199, 'learning_rate': 5.672737318788764e-05, 'batch_size': 173, 'step_size': 8, 'gamma': 0.8299128671891476}. Best is trial 0 with value: 0.2582511793491509.[0m
[32m[I 2025-01-08 05:26:20,107][0m Trial 5 finished with value: 0.21789142244142903 and parameters: {'observation_period_num': 95, 'train_rates': 0.7166715283129867, 'learning_rate': 0.00036741681129774455, 'batch_size': 216, 'step_size': 7, 'gamma': 0.9635860234616891}. Best is trial 5 with value: 0.21789142244142903.[0m
[32m[I 2025-01-08 05:29:03,309][0m Trial 6 finished with value: 0.1804468189243371 and parameters: {'observation_period_num': 36, 'train_rates': 0.831133716825254, 'learning_rate': 1.3325405712905726e-06, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9093050744660883}. Best is trial 6 with value: 0.1804468189243371.[0m
[32m[I 2025-01-08 05:29:46,588][0m Trial 7 finished with value: 0.07722418755292892 and parameters: {'observation_period_num': 107, 'train_rates': 0.9240795133840358, 'learning_rate': 0.00025615486480136336, 'batch_size': 235, 'step_size': 4, 'gamma': 0.8594091708822247}. Best is trial 7 with value: 0.07722418755292892.[0m
[32m[I 2025-01-08 05:30:34,681][0m Trial 8 finished with value: 0.6875539575276417 and parameters: {'observation_period_num': 109, 'train_rates': 0.6781805392771705, 'learning_rate': 2.433449543103384e-06, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8789106844491212}. Best is trial 7 with value: 0.07722418755292892.[0m
[32m[I 2025-01-08 05:31:11,909][0m Trial 9 finished with value: 0.24356742171258886 and parameters: {'observation_period_num': 135, 'train_rates': 0.8347706722739715, 'learning_rate': 5.28692843766147e-05, 'batch_size': 208, 'step_size': 3, 'gamma': 0.8011158970511715}. Best is trial 7 with value: 0.07722418755292892.[0m
[32m[I 2025-01-08 05:32:08,024][0m Trial 10 finished with value: 0.21609559655189514 and parameters: {'observation_period_num': 12, 'train_rates': 0.9701480007858314, 'learning_rate': 1.0593937716079168e-05, 'batch_size': 256, 'step_size': 1, 'gamma': 0.9568489022467287}. Best is trial 7 with value: 0.07722418755292892.[0m
[32m[I 2025-01-08 05:36:55,681][0m Trial 11 finished with value: 0.05122555933288626 and parameters: {'observation_period_num': 27, 'train_rates': 0.9369935173045625, 'learning_rate': 1.2166262135142459e-05, 'batch_size': 20, 'step_size': 4, 'gamma': 0.900159469788859}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:41:05,737][0m Trial 12 finished with value: 0.08709902888120608 and parameters: {'observation_period_num': 54, 'train_rates': 0.9705957593430402, 'learning_rate': 1.1578095431248468e-05, 'batch_size': 23, 'step_size': 4, 'gamma': 0.887186242288959}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:42:35,245][0m Trial 13 finished with value: 0.06114533527579624 and parameters: {'observation_period_num': 61, 'train_rates': 0.9116367015500841, 'learning_rate': 0.00019986349321646972, 'batch_size': 69, 'step_size': 4, 'gamma': 0.928239820949198}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:44:07,723][0m Trial 14 finished with value: 0.1745535440486053 and parameters: {'observation_period_num': 61, 'train_rates': 0.8998543455309965, 'learning_rate': 1.8370937397775114e-05, 'batch_size': 62, 'step_size': 1, 'gamma': 0.9332684722004573}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:46:00,720][0m Trial 15 finished with value: 0.0565524329073154 and parameters: {'observation_period_num': 7, 'train_rates': 0.9030838820881093, 'learning_rate': 6.953721663984586e-06, 'batch_size': 55, 'step_size': 5, 'gamma': 0.9883979624033805}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:51:02,996][0m Trial 16 finished with value: 0.18643110084462627 and parameters: {'observation_period_num': 16, 'train_rates': 0.7717464379363729, 'learning_rate': 6.200260830220347e-06, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9852413829984061}. Best is trial 11 with value: 0.05122555933288626.[0m
[32m[I 2025-01-08 05:52:04,896][0m Trial 17 finished with value: 0.049168786713671885 and parameters: {'observation_period_num': 6, 'train_rates': 0.8783901030161293, 'learning_rate': 2.7192759788194884e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9872966022287496}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 05:53:05,363][0m Trial 18 finished with value: 0.21039261043165317 and parameters: {'observation_period_num': 42, 'train_rates': 0.7813637024726481, 'learning_rate': 2.913671807739794e-05, 'batch_size': 108, 'step_size': 10, 'gamma': 0.8973502961277159}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 05:54:07,787][0m Trial 19 finished with value: 0.0768015970478482 and parameters: {'observation_period_num': 145, 'train_rates': 0.8680724610583171, 'learning_rate': 5.6030479062076443e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.9556436052820895}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 05:55:04,634][0m Trial 20 finished with value: 0.09391713674579348 and parameters: {'observation_period_num': 76, 'train_rates': 0.9441107940808421, 'learning_rate': 1.7861128671900067e-05, 'batch_size': 141, 'step_size': 15, 'gamma': 0.9433428309676899}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 05:57:17,769][0m Trial 21 finished with value: 0.05744488585806457 and parameters: {'observation_period_num': 11, 'train_rates': 0.8853692702197135, 'learning_rate': 5.5630091147949054e-06, 'batch_size': 46, 'step_size': 6, 'gamma': 0.9885937094660169}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 05:59:32,362][0m Trial 22 finished with value: 0.08821570128202438 and parameters: {'observation_period_num': 28, 'train_rates': 0.9839716775181309, 'learning_rate': 5.556256322003244e-06, 'batch_size': 48, 'step_size': 3, 'gamma': 0.9764303740093768}. Best is trial 17 with value: 0.049168786713671885.[0m
[32m[I 2025-01-08 06:00:57,784][0m Trial 23 finished with value: 0.04395564335376717 and parameters: {'observation_period_num': 9, 'train_rates': 0.9272439946163211, 'learning_rate': 2.8035101372246482e-05, 'batch_size': 84, 'step_size': 9, 'gamma': 0.9675753776258882}. Best is trial 23 with value: 0.04395564335376717.[0m
[32m[I 2025-01-08 06:02:16,954][0m Trial 24 finished with value: 0.05713442527810059 and parameters: {'observation_period_num': 35, 'train_rates': 0.9393814967725803, 'learning_rate': 3.2662975894070605e-05, 'batch_size': 82, 'step_size': 9, 'gamma': 0.9438425576808481}. Best is trial 23 with value: 0.04395564335376717.[0m
[32m[I 2025-01-08 06:03:09,663][0m Trial 25 finished with value: 0.06631504827663699 and parameters: {'observation_period_num': 50, 'train_rates': 0.8062626550648082, 'learning_rate': 0.00010405236643483053, 'batch_size': 123, 'step_size': 10, 'gamma': 0.9647185839468424}. Best is trial 23 with value: 0.04395564335376717.[0m
[32m[I 2025-01-08 06:04:21,687][0m Trial 26 finished with value: 0.04004918293422948 and parameters: {'observation_period_num': 5, 'train_rates': 0.8715499186951456, 'learning_rate': 2.9141312910622815e-05, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9088076013659144}. Best is trial 26 with value: 0.04004918293422948.[0m
[32m[I 2025-01-08 06:05:30,246][0m Trial 27 finished with value: 0.07111794174651474 and parameters: {'observation_period_num': 75, 'train_rates': 0.8783381064083686, 'learning_rate': 2.691932999477246e-05, 'batch_size': 86, 'step_size': 13, 'gamma': 0.9680698408581292}. Best is trial 26 with value: 0.04004918293422948.[0m
[32m[I 2025-01-08 06:06:28,283][0m Trial 28 finished with value: 0.037155278258774266 and parameters: {'observation_period_num': 5, 'train_rates': 0.8583505564798121, 'learning_rate': 0.0001025314954255456, 'batch_size': 120, 'step_size': 13, 'gamma': 0.9169972181876022}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:07:25,844][0m Trial 29 finished with value: 0.16190966296989967 and parameters: {'observation_period_num': 250, 'train_rates': 0.8029874294187973, 'learning_rate': 0.0008993423084325083, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9184726662421094}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:08:33,916][0m Trial 30 finished with value: 0.2712019821036758 and parameters: {'observation_period_num': 154, 'train_rates': 0.7531016391363169, 'learning_rate': 0.00012595280156225017, 'batch_size': 86, 'step_size': 15, 'gamma': 0.9406282969818365}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:09:38,214][0m Trial 31 finished with value: 0.04121894747950137 and parameters: {'observation_period_num': 6, 'train_rates': 0.8646150128285234, 'learning_rate': 7.54328538154897e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8752942638847989}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:10:32,550][0m Trial 32 finished with value: 0.04527360064879296 and parameters: {'observation_period_num': 25, 'train_rates': 0.8494813510524354, 'learning_rate': 7.996968055050176e-05, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8613848650068923}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:11:31,137][0m Trial 33 finished with value: 0.052940886271627324 and parameters: {'observation_period_num': 22, 'train_rates': 0.857683984387694, 'learning_rate': 4.107671100685733e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.8387636532960601}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:12:46,955][0m Trial 34 finished with value: 0.0986355828585001 and parameters: {'observation_period_num': 219, 'train_rates': 0.8269022711859376, 'learning_rate': 0.00017534645579431917, 'batch_size': 74, 'step_size': 13, 'gamma': 0.8058845407060939}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:13:27,451][0m Trial 35 finished with value: 0.14107398433344706 and parameters: {'observation_period_num': 167, 'train_rates': 0.8117969581187592, 'learning_rate': 0.0005028452339529818, 'batch_size': 170, 'step_size': 14, 'gamma': 0.8739800570889736}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:14:13,834][0m Trial 36 finished with value: 0.05558844932820648 and parameters: {'observation_period_num': 44, 'train_rates': 0.8902898462250163, 'learning_rate': 8.19214045157591e-05, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8914777106279915}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:15:18,824][0m Trial 37 finished with value: 0.049700702472430906 and parameters: {'observation_period_num': 68, 'train_rates': 0.858173181050739, 'learning_rate': 0.00014440657100949481, 'batch_size': 96, 'step_size': 12, 'gamma': 0.761253777406203}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:16:18,364][0m Trial 38 finished with value: 0.06548291336124142 and parameters: {'observation_period_num': 87, 'train_rates': 0.9247694602393912, 'learning_rate': 7.244075469183304e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9115697639972472}. Best is trial 28 with value: 0.037155278258774266.[0m
[32m[I 2025-01-08 06:17:19,149][0m Trial 39 finished with value: 0.032771663932020176 and parameters: {'observation_period_num': 5, 'train_rates': 0.95401046812605, 'learning_rate': 0.0003204139231581046, 'batch_size': 130, 'step_size': 9, 'gamma': 0.8488686699678654}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:18:05,796][0m Trial 40 finished with value: 0.18077212664323325 and parameters: {'observation_period_num': 37, 'train_rates': 0.7535799287009988, 'learning_rate': 0.00042063974187392654, 'batch_size': 190, 'step_size': 15, 'gamma': 0.8512546938634296}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:19:06,009][0m Trial 41 finished with value: 0.03677181154489517 and parameters: {'observation_period_num': 5, 'train_rates': 0.9583963154204052, 'learning_rate': 0.0003087948610333431, 'batch_size': 131, 'step_size': 9, 'gamma': 0.8243131825071869}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:20:02,224][0m Trial 42 finished with value: 0.03462310046791197 and parameters: {'observation_period_num': 22, 'train_rates': 0.9514804117177716, 'learning_rate': 0.00029472650354616335, 'batch_size': 128, 'step_size': 8, 'gamma': 0.8258051906010403}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:20:59,746][0m Trial 43 finished with value: 0.03304029256105423 and parameters: {'observation_period_num': 22, 'train_rates': 0.9578511535715297, 'learning_rate': 0.0005958499458979141, 'batch_size': 136, 'step_size': 8, 'gamma': 0.8191540981407059}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:21:59,947][0m Trial 44 finished with value: 0.03514164509318715 and parameters: {'observation_period_num': 22, 'train_rates': 0.952927934086147, 'learning_rate': 0.0006179908586855361, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8217065173598844}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:22:51,575][0m Trial 45 finished with value: 0.03296458348631859 and parameters: {'observation_period_num': 23, 'train_rates': 0.9581864072208136, 'learning_rate': 0.0005627906058180921, 'batch_size': 136, 'step_size': 8, 'gamma': 0.8180239584454742}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:23:42,083][0m Trial 46 finished with value: 0.10298527032136917 and parameters: {'observation_period_num': 119, 'train_rates': 0.9782790274268437, 'learning_rate': 0.0006186795653678052, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8060017822484246}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:24:39,474][0m Trial 47 finished with value: 0.04029099643230438 and parameters: {'observation_period_num': 30, 'train_rates': 0.9505591093719818, 'learning_rate': 0.000688091817070451, 'batch_size': 160, 'step_size': 8, 'gamma': 0.7922112117570475}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:25:24,573][0m Trial 48 finished with value: 0.18023371943116495 and parameters: {'observation_period_num': 48, 'train_rates': 0.600126021200632, 'learning_rate': 0.0003082180532573546, 'batch_size': 183, 'step_size': 7, 'gamma': 0.7891317389401806}. Best is trial 39 with value: 0.032771663932020176.[0m
[32m[I 2025-01-08 06:26:21,838][0m Trial 49 finished with value: 0.04308817908167839 and parameters: {'observation_period_num': 20, 'train_rates': 0.9884577252062801, 'learning_rate': 0.00023888321028167327, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8347905837669808}. Best is trial 39 with value: 0.032771663932020176.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-08 06:26:21,849][0m A new study created in memory with name: no-name-0a3949d9-acdd-4e0d-9ca7-8ccc9d5aab84[0m
[32m[I 2025-01-08 06:27:25,318][0m Trial 0 finished with value: 0.15698905698545687 and parameters: {'observation_period_num': 38, 'train_rates': 0.9378763062456141, 'learning_rate': 6.2569484051639325e-06, 'batch_size': 129, 'step_size': 14, 'gamma': 0.845422318502953}. Best is trial 0 with value: 0.15698905698545687.[0m
[32m[I 2025-01-08 06:28:24,391][0m Trial 1 finished with value: 0.6769840121269226 and parameters: {'observation_period_num': 249, 'train_rates': 0.9868935359195761, 'learning_rate': 1.651725321338713e-06, 'batch_size': 238, 'step_size': 7, 'gamma': 0.9266014736904693}. Best is trial 0 with value: 0.15698905698545687.[0m
[32m[I 2025-01-08 06:29:17,563][0m Trial 2 finished with value: 0.10564670734470938 and parameters: {'observation_period_num': 68, 'train_rates': 0.8296602227598613, 'learning_rate': 6.133500739676324e-05, 'batch_size': 243, 'step_size': 5, 'gamma': 0.929447582344513}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:30:22,568][0m Trial 3 finished with value: 0.3751021550005473 and parameters: {'observation_period_num': 159, 'train_rates': 0.703831802727017, 'learning_rate': 1.338039532116403e-05, 'batch_size': 85, 'step_size': 9, 'gamma': 0.9416889944117127}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:31:28,394][0m Trial 4 finished with value: 0.6563973454944452 and parameters: {'observation_period_num': 160, 'train_rates': 0.8029832398175746, 'learning_rate': 3.5474014643375138e-06, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8768558957745175}. Best is trial 2 with value: 0.10564670734470938.[0m
Early stopping at epoch 98
[32m[I 2025-01-08 06:32:26,545][0m Trial 5 finished with value: 0.11628056121499915 and parameters: {'observation_period_num': 53, 'train_rates': 0.8690658750351808, 'learning_rate': 0.0001559184525665334, 'batch_size': 105, 'step_size': 2, 'gamma': 0.7702659399148531}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:33:07,201][0m Trial 6 finished with value: 0.1750425891608608 and parameters: {'observation_period_num': 200, 'train_rates': 0.9111804635550673, 'learning_rate': 3.330437703065128e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.7788210692412295}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:33:47,349][0m Trial 7 finished with value: 0.20813905694287457 and parameters: {'observation_period_num': 162, 'train_rates': 0.9005914423023608, 'learning_rate': 2.924302660578215e-05, 'batch_size': 187, 'step_size': 5, 'gamma': 0.8416325638559405}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:34:29,419][0m Trial 8 finished with value: 0.2827236219625206 and parameters: {'observation_period_num': 153, 'train_rates': 0.637211757311065, 'learning_rate': 0.00022839206812212765, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9637502972770897}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:35:27,270][0m Trial 9 finished with value: 0.1156478300690651 and parameters: {'observation_period_num': 153, 'train_rates': 0.972366339905568, 'learning_rate': 0.0002737776743057959, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9412400114829826}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:40:36,812][0m Trial 10 finished with value: 0.248464983109742 and parameters: {'observation_period_num': 89, 'train_rates': 0.7709114637803146, 'learning_rate': 8.817545165874796e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9012952842769107}. Best is trial 2 with value: 0.10564670734470938.[0m
[32m[I 2025-01-08 06:42:19,884][0m Trial 11 finished with value: 0.08914560789226993 and parameters: {'observation_period_num': 99, 'train_rates': 0.8307661168581499, 'learning_rate': 0.000669957497016635, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9881264304471361}. Best is trial 11 with value: 0.08914560789226993.[0m
[32m[I 2025-01-08 06:44:18,445][0m Trial 12 finished with value: 0.09774186587288179 and parameters: {'observation_period_num': 91, 'train_rates': 0.817013316379295, 'learning_rate': 0.0009748010882133695, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9847239749209838}. Best is trial 11 with value: 0.08914560789226993.[0m
[32m[I 2025-01-08 06:46:38,145][0m Trial 13 finished with value: 0.25483716973635595 and parameters: {'observation_period_num': 105, 'train_rates': 0.7267943557377804, 'learning_rate': 0.0009923782202095784, 'batch_size': 35, 'step_size': 11, 'gamma': 0.9864295609884826}. Best is trial 11 with value: 0.08914560789226993.[0m
[32m[I 2025-01-08 06:48:15,541][0m Trial 14 finished with value: 0.07603645773793934 and parameters: {'observation_period_num': 13, 'train_rates': 0.839587497216312, 'learning_rate': 0.000807322917249979, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9791282329042403}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:49:48,508][0m Trial 15 finished with value: 0.11094642519117236 and parameters: {'observation_period_num': 9, 'train_rates': 0.8546391557979105, 'learning_rate': 0.000436876791467047, 'batch_size': 61, 'step_size': 11, 'gamma': 0.9896100637622453}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:51:11,881][0m Trial 16 finished with value: 0.20465068746884557 and parameters: {'observation_period_num': 6, 'train_rates': 0.7505959006350222, 'learning_rate': 0.00050814506205358, 'batch_size': 66, 'step_size': 8, 'gamma': 0.895756388178147}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:51:42,837][0m Trial 17 finished with value: 0.2456164437904954 and parameters: {'observation_period_num': 117, 'train_rates': 0.6475946035326317, 'learning_rate': 0.00010475648870444455, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8105998768958154}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:52:46,436][0m Trial 18 finished with value: 0.1979829866865213 and parameters: {'observation_period_num': 34, 'train_rates': 0.697716632734671, 'learning_rate': 0.0004898403234060311, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9524889159285356}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:58:05,367][0m Trial 19 finished with value: 0.12152744299626049 and parameters: {'observation_period_num': 71, 'train_rates': 0.863319714562396, 'learning_rate': 4.388452652995111e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9120261901231772}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 06:59:58,258][0m Trial 20 finished with value: 0.17059447755885435 and parameters: {'observation_period_num': 205, 'train_rates': 0.7768242444154905, 'learning_rate': 1.6897814572292916e-05, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9668998939963719}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 07:01:52,747][0m Trial 21 finished with value: 0.13139111983388505 and parameters: {'observation_period_num': 103, 'train_rates': 0.8004278496076909, 'learning_rate': 0.000961676696104778, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9736915842382489}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 07:03:04,597][0m Trial 22 finished with value: 0.1050348151861632 and parameters: {'observation_period_num': 129, 'train_rates': 0.8294083572538726, 'learning_rate': 0.0006709057069153952, 'batch_size': 89, 'step_size': 7, 'gamma': 0.9866854291104126}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 07:04:42,299][0m Trial 23 finished with value: 0.09934050625426358 and parameters: {'observation_period_num': 80, 'train_rates': 0.8328032088333862, 'learning_rate': 0.00030346891089412715, 'batch_size': 59, 'step_size': 9, 'gamma': 0.9622163100576447}. Best is trial 14 with value: 0.07603645773793934.[0m
[32m[I 2025-01-08 07:07:35,857][0m Trial 24 finished with value: 0.07348903441342755 and parameters: {'observation_period_num': 31, 'train_rates': 0.9058631318205219, 'learning_rate': 0.00015857573657937068, 'batch_size': 34, 'step_size': 5, 'gamma': 0.9453869017854338}. Best is trial 24 with value: 0.07348903441342755.[0m
[32m[I 2025-01-08 07:11:03,181][0m Trial 25 finished with value: 0.053657818737686894 and parameters: {'observation_period_num': 26, 'train_rates': 0.9165745011858901, 'learning_rate': 0.00015941954549301052, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9515696473158344}. Best is trial 25 with value: 0.053657818737686894.[0m
[32m[I 2025-01-08 07:14:25,804][0m Trial 26 finished with value: 0.046253350111301876 and parameters: {'observation_period_num': 30, 'train_rates': 0.9415348518106073, 'learning_rate': 0.0001530608378949736, 'batch_size': 29, 'step_size': 4, 'gamma': 0.9199537995553148}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:18:34,654][0m Trial 27 finished with value: 0.05454049804588643 and parameters: {'observation_period_num': 34, 'train_rates': 0.9438891104764601, 'learning_rate': 0.00014634521882469177, 'batch_size': 24, 'step_size': 3, 'gamma': 0.8728977578933871}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:22:49,680][0m Trial 28 finished with value: 0.06979373647343545 and parameters: {'observation_period_num': 52, 'train_rates': 0.9424473887756741, 'learning_rate': 0.00010161268806032247, 'batch_size': 23, 'step_size': 3, 'gamma': 0.8706039033840729}. Best is trial 26 with value: 0.046253350111301876.[0m
Early stopping at epoch 77
[32m[I 2025-01-08 07:23:51,889][0m Trial 29 finished with value: 0.1444411389529705 and parameters: {'observation_period_num': 51, 'train_rates': 0.9477155546209401, 'learning_rate': 0.00016589926114701423, 'batch_size': 134, 'step_size': 1, 'gamma': 0.8382309029671974}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:25:37,015][0m Trial 30 finished with value: 0.07752552178797421 and parameters: {'observation_period_num': 27, 'train_rates': 0.9624126183479177, 'learning_rate': 6.495682467406277e-05, 'batch_size': 73, 'step_size': 3, 'gamma': 0.8888990733257343}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:29:16,009][0m Trial 31 finished with value: 0.07468138313503518 and parameters: {'observation_period_num': 51, 'train_rates': 0.9220721419345332, 'learning_rate': 0.00010501913143274455, 'batch_size': 28, 'step_size': 3, 'gamma': 0.8650800171321591}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:35:27,541][0m Trial 32 finished with value: 0.0551585890352726 and parameters: {'observation_period_num': 26, 'train_rates': 0.945732861371461, 'learning_rate': 0.00019765138865967226, 'batch_size': 16, 'step_size': 2, 'gamma': 0.859305328768722}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:38:15,176][0m Trial 33 finished with value: 0.057453326391061876 and parameters: {'observation_period_num': 21, 'train_rates': 0.888622776594705, 'learning_rate': 0.0002342533136042617, 'batch_size': 35, 'step_size': 2, 'gamma': 0.9156084971739207}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:44:36,364][0m Trial 34 finished with value: 0.06707672343442314 and parameters: {'observation_period_num': 38, 'train_rates': 0.987203163346321, 'learning_rate': 6.620086957189149e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.815316922273912}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:45:28,061][0m Trial 35 finished with value: 0.7848909181707046 and parameters: {'observation_period_num': 67, 'train_rates': 0.923708865101303, 'learning_rate': 1.3094827576506392e-06, 'batch_size': 208, 'step_size': 2, 'gamma': 0.8568980550047945}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:48:35,998][0m Trial 36 finished with value: 0.07449893694784906 and parameters: {'observation_period_num': 62, 'train_rates': 0.962842867853721, 'learning_rate': 0.0003551093365931735, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9251229386623481}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:49:47,368][0m Trial 37 finished with value: 0.12804182428722918 and parameters: {'observation_period_num': 20, 'train_rates': 0.885153884370323, 'learning_rate': 1.7615471388538948e-05, 'batch_size': 87, 'step_size': 2, 'gamma': 0.8850325048834853}. Best is trial 26 with value: 0.046253350111301876.[0m
Early stopping at epoch 75
[32m[I 2025-01-08 07:51:24,206][0m Trial 38 finished with value: 0.573744393762995 and parameters: {'observation_period_num': 43, 'train_rates': 0.9374047128185257, 'learning_rate': 5.209529295459257e-06, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8556535157120116}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:52:26,359][0m Trial 39 finished with value: 0.07502217590808868 and parameters: {'observation_period_num': 5, 'train_rates': 0.9860883742556218, 'learning_rate': 0.00017802643877657973, 'batch_size': 112, 'step_size': 5, 'gamma': 0.8231499136515164}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:53:29,173][0m Trial 40 finished with value: 0.19302193075418472 and parameters: {'observation_period_num': 246, 'train_rates': 0.9292962726457811, 'learning_rate': 4.548990175224132e-05, 'batch_size': 96, 'step_size': 4, 'gamma': 0.7941514218125749}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:56:13,272][0m Trial 41 finished with value: 0.06564983593610425 and parameters: {'observation_period_num': 22, 'train_rates': 0.8855751141008172, 'learning_rate': 0.0002383555040766018, 'batch_size': 35, 'step_size': 2, 'gamma': 0.9162788575909457}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 07:56:56,736][0m Trial 42 finished with value: 0.09242901216690724 and parameters: {'observation_period_num': 21, 'train_rates': 0.8812447326785621, 'learning_rate': 0.000133148718896439, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9084449122847557}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:00:23,283][0m Trial 43 finished with value: 0.07496039605849272 and parameters: {'observation_period_num': 43, 'train_rates': 0.901869352232028, 'learning_rate': 0.00022699903905541794, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9278581096411419}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:01:39,498][0m Trial 44 finished with value: 0.6003735214471817 and parameters: {'observation_period_num': 60, 'train_rates': 0.9558757011776733, 'learning_rate': 2.4931667247294695e-06, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9390437480454176}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:03:50,700][0m Trial 45 finished with value: 0.06746754504959374 and parameters: {'observation_period_num': 35, 'train_rates': 0.907150993799228, 'learning_rate': 0.0003706159976002698, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8780763278423891}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:04:32,033][0m Trial 46 finished with value: 0.057070542126894 and parameters: {'observation_period_num': 15, 'train_rates': 0.9790452553106415, 'learning_rate': 0.0002103978595205489, 'batch_size': 149, 'step_size': 5, 'gamma': 0.898696717215735}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:05:08,322][0m Trial 47 finished with value: 0.09849123656749725 and parameters: {'observation_period_num': 76, 'train_rates': 0.9737123212030055, 'learning_rate': 7.390461913814027e-05, 'batch_size': 177, 'step_size': 5, 'gamma': 0.898338411037151}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:05:50,557][0m Trial 48 finished with value: 0.06746553629636765 and parameters: {'observation_period_num': 13, 'train_rates': 0.9728379784855867, 'learning_rate': 0.00012949640286410218, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8360398228216381}. Best is trial 26 with value: 0.046253350111301876.[0m
[32m[I 2025-01-08 08:06:39,267][0m Trial 49 finished with value: 0.09796599074397036 and parameters: {'observation_period_num': 41, 'train_rates': 0.9361971968230461, 'learning_rate': 3.0240110029149605e-05, 'batch_size': 119, 'step_size': 5, 'gamma': 0.880004131109817}. Best is trial 26 with value: 0.046253350111301876.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-08 08:06:39,277][0m A new study created in memory with name: no-name-5820f69a-4171-4523-a8e5-9b6554925740[0m
[32m[I 2025-01-08 08:07:12,119][0m Trial 0 finished with value: 0.8257031097906326 and parameters: {'observation_period_num': 37, 'train_rates': 0.6991986442867726, 'learning_rate': 2.675801394494071e-06, 'batch_size': 147, 'step_size': 4, 'gamma': 0.8711337044771775}. Best is trial 0 with value: 0.8257031097906326.[0m
[32m[I 2025-01-08 08:07:43,866][0m Trial 1 finished with value: 0.27483107686042785 and parameters: {'observation_period_num': 107, 'train_rates': 0.8157804432043786, 'learning_rate': 1.0946548789941418e-05, 'batch_size': 168, 'step_size': 11, 'gamma': 0.7637641113983541}. Best is trial 1 with value: 0.27483107686042785.[0m
[32m[I 2025-01-08 08:08:36,516][0m Trial 2 finished with value: 0.045895815674362885 and parameters: {'observation_period_num': 29, 'train_rates': 0.9396053340983417, 'learning_rate': 0.0009655702826129826, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8361034647414947}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:09:00,141][0m Trial 3 finished with value: 0.20709586381001244 and parameters: {'observation_period_num': 5, 'train_rates': 0.7670632696326878, 'learning_rate': 3.0033430874486113e-05, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9247719183138856}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:11:58,345][0m Trial 4 finished with value: 0.2769307584967464 and parameters: {'observation_period_num': 168, 'train_rates': 0.7417798736493286, 'learning_rate': 0.0003057147658046083, 'batch_size': 25, 'step_size': 5, 'gamma': 0.938346248535221}. Best is trial 2 with value: 0.045895815674362885.[0m
Early stopping at epoch 55
[32m[I 2025-01-08 08:12:33,672][0m Trial 5 finished with value: 0.5215077759919466 and parameters: {'observation_period_num': 216, 'train_rates': 0.7389690241083813, 'learning_rate': 6.995701593077301e-05, 'batch_size': 73, 'step_size': 1, 'gamma': 0.7927955183538301}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:14:27,228][0m Trial 6 finished with value: 0.06761877131009218 and parameters: {'observation_period_num': 79, 'train_rates': 0.9293867152253426, 'learning_rate': 0.0003238689232748163, 'batch_size': 49, 'step_size': 2, 'gamma': 0.8808139897020767}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:14:58,320][0m Trial 7 finished with value: 0.5975796580314636 and parameters: {'observation_period_num': 203, 'train_rates': 0.934557261915876, 'learning_rate': 3.723185066859076e-06, 'batch_size': 253, 'step_size': 9, 'gamma': 0.835283242104671}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:16:02,456][0m Trial 8 finished with value: 0.19551998636834264 and parameters: {'observation_period_num': 242, 'train_rates': 0.7720472683185773, 'learning_rate': 0.0002569665850468588, 'batch_size': 73, 'step_size': 12, 'gamma': 0.9152852103994111}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:17:05,323][0m Trial 9 finished with value: 0.43216670066505286 and parameters: {'observation_period_num': 147, 'train_rates': 0.7258264508431422, 'learning_rate': 4.453565775795361e-06, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9307423951393647}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:17:40,377][0m Trial 10 finished with value: 0.09817037731409073 and parameters: {'observation_period_num': 65, 'train_rates': 0.9880352638159098, 'learning_rate': 0.00099633511934046, 'batch_size': 196, 'step_size': 6, 'gamma': 0.9859330085410085}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:22:30,671][0m Trial 11 finished with value: 0.05957965049950331 and parameters: {'observation_period_num': 87, 'train_rates': 0.8873483312280288, 'learning_rate': 0.0009309705379734438, 'batch_size': 18, 'step_size': 1, 'gamma': 0.8556372588185849}. Best is trial 2 with value: 0.045895815674362885.[0m
[32m[I 2025-01-08 08:23:17,872][0m Trial 12 finished with value: 0.04243474445221099 and parameters: {'observation_period_num': 6, 'train_rates': 0.8658378013698125, 'learning_rate': 0.0006908826457700714, 'batch_size': 119, 'step_size': 3, 'gamma': 0.8281970479817794}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:24:09,677][0m Trial 13 finished with value: 0.04856563702257869 and parameters: {'observation_period_num': 5, 'train_rates': 0.8432913456864817, 'learning_rate': 0.00015760507521417233, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8147216221491668}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:24:47,960][0m Trial 14 finished with value: 0.3350208061785737 and parameters: {'observation_period_num': 41, 'train_rates': 0.6262733316515539, 'learning_rate': 8.272252694210311e-05, 'batch_size': 117, 'step_size': 3, 'gamma': 0.793704190278903}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:25:19,156][0m Trial 15 finished with value: 0.05434057392710746 and parameters: {'observation_period_num': 34, 'train_rates': 0.8704581435485901, 'learning_rate': 0.0005598723045694969, 'batch_size': 183, 'step_size': 4, 'gamma': 0.8315934262284363}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:26:07,306][0m Trial 16 finished with value: 0.18931913375854492 and parameters: {'observation_period_num': 117, 'train_rates': 0.9812287877589065, 'learning_rate': 2.5146490430518187e-05, 'batch_size': 124, 'step_size': 9, 'gamma': 0.7539547593571556}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:27:03,255][0m Trial 17 finished with value: 0.0638768742127078 and parameters: {'observation_period_num': 55, 'train_rates': 0.9206406302209447, 'learning_rate': 7.759768353688239e-05, 'batch_size': 99, 'step_size': 6, 'gamma': 0.8920776950869196}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:27:38,130][0m Trial 18 finished with value: 1.934899869572702 and parameters: {'observation_period_num': 24, 'train_rates': 0.8232431336308187, 'learning_rate': 1.2848777952335363e-06, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8486996978420462}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:28:08,850][0m Trial 19 finished with value: 0.06903717783351801 and parameters: {'observation_period_num': 90, 'train_rates': 0.8872528465645134, 'learning_rate': 0.000480540599526995, 'batch_size': 195, 'step_size': 8, 'gamma': 0.7876819755773543}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:28:50,645][0m Trial 20 finished with value: 0.09745478630065918 and parameters: {'observation_period_num': 164, 'train_rates': 0.9554391259536812, 'learning_rate': 0.00013890812252855502, 'batch_size': 134, 'step_size': 5, 'gamma': 0.8174900419274166}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:29:47,625][0m Trial 21 finished with value: 0.04764050632616542 and parameters: {'observation_period_num': 5, 'train_rates': 0.8466268630590709, 'learning_rate': 0.0001640846735950254, 'batch_size': 95, 'step_size': 7, 'gamma': 0.8132593060571022}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:30:45,040][0m Trial 22 finished with value: 0.04478833766109278 and parameters: {'observation_period_num': 10, 'train_rates': 0.8505266326051835, 'learning_rate': 0.0005831145961378284, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8100704976528631}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:31:50,420][0m Trial 23 finished with value: 0.05990802943706512 and parameters: {'observation_period_num': 53, 'train_rates': 0.9003676983872697, 'learning_rate': 0.000625322854387442, 'batch_size': 87, 'step_size': 3, 'gamma': 0.7802745486339491}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:33:49,316][0m Trial 24 finished with value: 0.056952965220534 and parameters: {'observation_period_num': 26, 'train_rates': 0.8631347735779962, 'learning_rate': 0.0005628575282057533, 'batch_size': 45, 'step_size': 2, 'gamma': 0.849996999965304}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:34:30,303][0m Trial 25 finished with value: 0.07211723795445979 and parameters: {'observation_period_num': 63, 'train_rates': 0.799255707566946, 'learning_rate': 0.00030109331398651414, 'batch_size': 135, 'step_size': 4, 'gamma': 0.8235065152383458}. Best is trial 12 with value: 0.04243474445221099.[0m
[32m[I 2025-01-08 08:36:10,645][0m Trial 26 finished with value: 0.04139223125659757 and parameters: {'observation_period_num': 22, 'train_rates': 0.9510057802195173, 'learning_rate': 0.0008244664052321718, 'batch_size': 58, 'step_size': 2, 'gamma': 0.8937591655273168}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:38:04,572][0m Trial 27 finished with value: 0.08990816488466431 and parameters: {'observation_period_num': 18, 'train_rates': 0.9615207499163818, 'learning_rate': 4.750856688328283e-05, 'batch_size': 51, 'step_size': 1, 'gamma': 0.8971288632303736}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:39:17,933][0m Trial 28 finished with value: 0.2628207342191176 and parameters: {'observation_period_num': 49, 'train_rates': 0.6594086279158347, 'learning_rate': 1.6303219171500877e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9497323087502703}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:40:25,205][0m Trial 29 finished with value: 0.05189717813592517 and parameters: {'observation_period_num': 72, 'train_rates': 0.9085611161201123, 'learning_rate': 0.0004438961654302797, 'batch_size': 83, 'step_size': 5, 'gamma': 0.8729415434224178}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:42:50,370][0m Trial 30 finished with value: 0.0756182406465383 and parameters: {'observation_period_num': 102, 'train_rates': 0.8368728582603322, 'learning_rate': 0.0002108759795454625, 'batch_size': 35, 'step_size': 3, 'gamma': 0.904058100258729}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:43:30,491][0m Trial 31 finished with value: 0.048714395612478256 and parameters: {'observation_period_num': 32, 'train_rates': 0.9501991094139108, 'learning_rate': 0.0009509329430860043, 'batch_size': 149, 'step_size': 4, 'gamma': 0.8618207047096746}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:44:18,425][0m Trial 32 finished with value: 0.05874954550354569 and parameters: {'observation_period_num': 21, 'train_rates': 0.8622267812978633, 'learning_rate': 0.0007010113362526316, 'batch_size': 118, 'step_size': 2, 'gamma': 0.8015473279609547}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:45:06,927][0m Trial 33 finished with value: 0.055934414723566896 and parameters: {'observation_period_num': 38, 'train_rates': 0.8011273182086319, 'learning_rate': 0.00037717613414845695, 'batch_size': 109, 'step_size': 3, 'gamma': 0.8389652390510964}. Best is trial 26 with value: 0.04139223125659757.[0m
[32m[I 2025-01-08 08:45:41,659][0m Trial 34 finished with value: 0.03808680274886286 and parameters: {'observation_period_num': 19, 'train_rates': 0.8863613947948532, 'learning_rate': 0.0007535469872884809, 'batch_size': 165, 'step_size': 4, 'gamma': 0.868323857214635}. Best is trial 34 with value: 0.03808680274886286.[0m
[32m[I 2025-01-08 08:46:16,060][0m Trial 35 finished with value: 0.035861398772162906 and parameters: {'observation_period_num': 13, 'train_rates': 0.8799383423601663, 'learning_rate': 0.0007433390577525729, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8818531309531288}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:46:42,775][0m Trial 36 finished with value: 0.2126941553308817 and parameters: {'observation_period_num': 20, 'train_rates': 0.8841073294469314, 'learning_rate': 1.040362025586962e-05, 'batch_size': 226, 'step_size': 5, 'gamma': 0.8844793210101944}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:47:18,788][0m Trial 37 finished with value: 0.051621122806057754 and parameters: {'observation_period_num': 45, 'train_rates': 0.9151846266564948, 'learning_rate': 0.00019358001094562358, 'batch_size': 163, 'step_size': 6, 'gamma': 0.8689540944175003}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:47:49,779][0m Trial 38 finished with value: 0.04926627912985844 and parameters: {'observation_period_num': 17, 'train_rates': 0.8226511340522916, 'learning_rate': 0.00010989739843221255, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9119700901363077}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:48:15,018][0m Trial 39 finished with value: 0.26987240657755224 and parameters: {'observation_period_num': 136, 'train_rates': 0.7770061559257361, 'learning_rate': 0.00036496666972659443, 'batch_size': 217, 'step_size': 1, 'gamma': 0.9430637446409259}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:48:55,043][0m Trial 40 finished with value: 0.05853483929197387 and parameters: {'observation_period_num': 36, 'train_rates': 0.9308291724309142, 'learning_rate': 0.0007555448883573677, 'batch_size': 146, 'step_size': 4, 'gamma': 0.958339213149383}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:49:29,338][0m Trial 41 finished with value: 0.07387414358692443 and parameters: {'observation_period_num': 5, 'train_rates': 0.8703649246882955, 'learning_rate': 0.00046492147610295364, 'batch_size': 170, 'step_size': 2, 'gamma': 0.7751226003198383}. Best is trial 35 with value: 0.035861398772162906.[0m
[32m[I 2025-01-08 08:50:14,350][0m Trial 42 finished with value: 0.032746313929052674 and parameters: {'observation_period_num': 16, 'train_rates': 0.8998138978719316, 'learning_rate': 0.0006448861412147662, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8740297935643873}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:50:45,044][0m Trial 43 finished with value: 0.04821258994741011 and parameters: {'observation_period_num': 60, 'train_rates': 0.9001560572268072, 'learning_rate': 0.0007599829358697025, 'batch_size': 209, 'step_size': 6, 'gamma': 0.8810533592954894}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:51:17,418][0m Trial 44 finished with value: 0.12050995975732803 and parameters: {'observation_period_num': 200, 'train_rates': 0.9702067061493942, 'learning_rate': 0.0002509872017853776, 'batch_size': 188, 'step_size': 7, 'gamma': 0.9171388889436486}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:51:58,073][0m Trial 45 finished with value: 0.04434941661766338 and parameters: {'observation_period_num': 14, 'train_rates': 0.9330134049343823, 'learning_rate': 0.00034980925740255513, 'batch_size': 150, 'step_size': 4, 'gamma': 0.8674926410651957}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:52:42,768][0m Trial 46 finished with value: 0.042739380128591895 and parameters: {'observation_period_num': 26, 'train_rates': 0.8866404317444706, 'learning_rate': 0.0009850616839239146, 'batch_size': 126, 'step_size': 5, 'gamma': 0.8905349386711034}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:53:21,222][0m Trial 47 finished with value: 0.05830767221579498 and parameters: {'observation_period_num': 74, 'train_rates': 0.9392330983134407, 'learning_rate': 0.00028245081702792243, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8436210860887274}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:54:01,702][0m Trial 48 finished with value: 0.1326563098498816 and parameters: {'observation_period_num': 46, 'train_rates': 0.9069731691638505, 'learning_rate': 0.0007233444718250089, 'batch_size': 141, 'step_size': 14, 'gamma': 0.903412921672361}. Best is trial 42 with value: 0.032746313929052674.[0m
[32m[I 2025-01-08 08:54:33,186][0m Trial 49 finished with value: 0.06353433939142966 and parameters: {'observation_period_num': 35, 'train_rates': 0.8298996919260047, 'learning_rate': 0.00010889727325319054, 'batch_size': 177, 'step_size': 5, 'gamma': 0.8579777077413886}. Best is trial 42 with value: 0.032746313929052674.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-08 08:54:33,196][0m A new study created in memory with name: no-name-f1e59e0f-67be-4708-a0f3-4b027f479fab[0m
[32m[I 2025-01-08 08:56:54,308][0m Trial 0 finished with value: 0.3755181970084525 and parameters: {'observation_period_num': 110, 'train_rates': 0.9328519207256933, 'learning_rate': 1.018888274012394e-06, 'batch_size': 39, 'step_size': 11, 'gamma': 0.8786596223031007}. Best is trial 0 with value: 0.3755181970084525.[0m
[32m[I 2025-01-08 08:57:22,011][0m Trial 1 finished with value: 0.22555017132539137 and parameters: {'observation_period_num': 106, 'train_rates': 0.7871840166498504, 'learning_rate': 3.688966381283548e-05, 'batch_size': 201, 'step_size': 6, 'gamma': 0.7764179510569356}. Best is trial 1 with value: 0.22555017132539137.[0m
[32m[I 2025-01-08 08:58:02,435][0m Trial 2 finished with value: 0.1810330824057261 and parameters: {'observation_period_num': 30, 'train_rates': 0.7181611230887983, 'learning_rate': 0.00013839771985232094, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9230925385867093}. Best is trial 2 with value: 0.1810330824057261.[0m
Early stopping at epoch 74
[32m[I 2025-01-08 08:58:18,817][0m Trial 3 finished with value: 0.7630007391011704 and parameters: {'observation_period_num': 107, 'train_rates': 0.6252491428570359, 'learning_rate': 2.2110557233925225e-05, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8572689068247377}. Best is trial 2 with value: 0.1810330824057261.[0m
[32m[I 2025-01-08 08:58:44,955][0m Trial 4 finished with value: 0.44594594504857477 and parameters: {'observation_period_num': 30, 'train_rates': 0.764810026166088, 'learning_rate': 8.586408883727085e-06, 'batch_size': 205, 'step_size': 8, 'gamma': 0.7848549715414131}. Best is trial 2 with value: 0.1810330824057261.[0m
[32m[I 2025-01-08 08:59:16,691][0m Trial 5 finished with value: 0.06720977236513595 and parameters: {'observation_period_num': 60, 'train_rates': 0.7975143904276658, 'learning_rate': 0.0005336554700253941, 'batch_size': 175, 'step_size': 5, 'gamma': 0.986444104059951}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 08:59:39,066][0m Trial 6 finished with value: 0.6667575712693043 and parameters: {'observation_period_num': 233, 'train_rates': 0.6417450706397833, 'learning_rate': 7.092149594505972e-06, 'batch_size': 196, 'step_size': 2, 'gamma': 0.9447345423343934}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:00:05,024][0m Trial 7 finished with value: 0.21587114171136323 and parameters: {'observation_period_num': 91, 'train_rates': 0.7131971153427417, 'learning_rate': 0.00028401545065588336, 'batch_size': 203, 'step_size': 14, 'gamma': 0.8891822987046907}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:00:26,805][0m Trial 8 finished with value: 0.5071280018968897 and parameters: {'observation_period_num': 136, 'train_rates': 0.6448470951354773, 'learning_rate': 9.351934308901864e-06, 'batch_size': 227, 'step_size': 13, 'gamma': 0.9343538714044295}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:01:06,384][0m Trial 9 finished with value: 0.2540935814985769 and parameters: {'observation_period_num': 216, 'train_rates': 0.7843916762886007, 'learning_rate': 2.646469194611533e-05, 'batch_size': 121, 'step_size': 14, 'gamma': 0.9253367498826783}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:02:25,623][0m Trial 10 finished with value: 0.17739790182175308 and parameters: {'observation_period_num': 177, 'train_rates': 0.8959051722693333, 'learning_rate': 0.0008241400178837247, 'batch_size': 67, 'step_size': 5, 'gamma': 0.981217746259144}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:03:50,802][0m Trial 11 finished with value: 0.11338552843765373 and parameters: {'observation_period_num': 172, 'train_rates': 0.9043582973452511, 'learning_rate': 0.0009769017070400936, 'batch_size': 62, 'step_size': 5, 'gamma': 0.9731185172692606}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:04:27,834][0m Trial 12 finished with value: 0.18502987925232894 and parameters: {'observation_period_num': 172, 'train_rates': 0.8660619118663683, 'learning_rate': 0.0008632175108348285, 'batch_size': 150, 'step_size': 5, 'gamma': 0.9887705572278127}. Best is trial 5 with value: 0.06720977236513595.[0m
[32m[I 2025-01-08 09:05:36,117][0m Trial 13 finished with value: 0.05119160509535244 and parameters: {'observation_period_num': 67, 'train_rates': 0.8547767266999244, 'learning_rate': 0.00024249419323606484, 'batch_size': 78, 'step_size': 8, 'gamma': 0.8301734342010161}. Best is trial 13 with value: 0.05119160509535244.[0m
[32m[I 2025-01-08 09:06:16,024][0m Trial 14 finished with value: 0.06411051750183105 and parameters: {'observation_period_num': 54, 'train_rates': 0.987434482298637, 'learning_rate': 0.00014548996172633613, 'batch_size': 153, 'step_size': 9, 'gamma': 0.8280312640465446}. Best is trial 13 with value: 0.05119160509535244.[0m
[32m[I 2025-01-08 09:07:21,883][0m Trial 15 finished with value: 0.0867689996957779 and parameters: {'observation_period_num': 68, 'train_rates': 0.9895085075259893, 'learning_rate': 0.00011991985425207835, 'batch_size': 92, 'step_size': 10, 'gamma': 0.8225792347902209}. Best is trial 13 with value: 0.05119160509535244.[0m
[32m[I 2025-01-08 09:11:26,858][0m Trial 16 finished with value: 0.03656788670590946 and parameters: {'observation_period_num': 5, 'train_rates': 0.9883907766371864, 'learning_rate': 0.00010271428109523045, 'batch_size': 24, 'step_size': 9, 'gamma': 0.8169319993519532}. Best is trial 16 with value: 0.03656788670590946.[0m
[32m[I 2025-01-08 09:16:15,134][0m Trial 17 finished with value: 0.03099128835941706 and parameters: {'observation_period_num': 5, 'train_rates': 0.8469236748545818, 'learning_rate': 6.95012068121626e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8141398861230108}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:18:28,975][0m Trial 18 finished with value: 0.03391153042464227 and parameters: {'observation_period_num': 6, 'train_rates': 0.8346006054335473, 'learning_rate': 5.5738060278664744e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.7588221438522632}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:22:45,880][0m Trial 19 finished with value: 0.04240801293572944 and parameters: {'observation_period_num': 26, 'train_rates': 0.835466407388939, 'learning_rate': 4.989024201758614e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7512630923569391}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:24:38,127][0m Trial 20 finished with value: 0.1014936306522888 and parameters: {'observation_period_num': 16, 'train_rates': 0.824999424455736, 'learning_rate': 3.608016115140566e-06, 'batch_size': 46, 'step_size': 11, 'gamma': 0.7879722662071149}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:28:13,815][0m Trial 21 finished with value: 0.03210386176194464 and parameters: {'observation_period_num': 7, 'train_rates': 0.9288417550101598, 'learning_rate': 6.21321444363307e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8091514972825293}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:33:58,654][0m Trial 22 finished with value: 0.04401148735515533 and parameters: {'observation_period_num': 42, 'train_rates': 0.9363587597013683, 'learning_rate': 6.65581459281881e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7515794603637466}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:35:50,832][0m Trial 23 finished with value: 0.04761197100262026 and parameters: {'observation_period_num': 11, 'train_rates': 0.8871302865775348, 'learning_rate': 1.8317516172406647e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8021311657905584}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:36:54,396][0m Trial 24 finished with value: 0.04105654566198992 and parameters: {'observation_period_num': 6, 'train_rates': 0.9253287123304552, 'learning_rate': 6.258888979621938e-05, 'batch_size': 90, 'step_size': 7, 'gamma': 0.858223257852566}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:38:56,336][0m Trial 25 finished with value: 0.21638419936677727 and parameters: {'observation_period_num': 80, 'train_rates': 0.7487673167326423, 'learning_rate': 0.00030397167574297795, 'batch_size': 39, 'step_size': 3, 'gamma': 0.7720634803449296}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:39:45,953][0m Trial 26 finished with value: 0.12007595070884458 and parameters: {'observation_period_num': 45, 'train_rates': 0.8326759976554312, 'learning_rate': 1.2524210364756573e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8014953397238508}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:42:42,246][0m Trial 27 finished with value: 0.08716947324573994 and parameters: {'observation_period_num': 142, 'train_rates': 0.9574215401790592, 'learning_rate': 3.759008619241751e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8456253721854551}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:44:09,949][0m Trial 28 finished with value: 0.061291572504809926 and parameters: {'observation_period_num': 34, 'train_rates': 0.8706791027416872, 'learning_rate': 7.649541591028011e-05, 'batch_size': 61, 'step_size': 4, 'gamma': 0.7646758700742369}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:46:11,101][0m Trial 29 finished with value: 0.16922287191201119 and parameters: {'observation_period_num': 22, 'train_rates': 0.9247999094595867, 'learning_rate': 1.5037381949124166e-06, 'batch_size': 46, 'step_size': 11, 'gamma': 0.8881583649828915}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:47:17,710][0m Trial 30 finished with value: 0.05766046166823496 and parameters: {'observation_period_num': 83, 'train_rates': 0.8201489632154503, 'learning_rate': 0.00017135954171268791, 'batch_size': 78, 'step_size': 9, 'gamma': 0.8044946020250884}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:50:58,230][0m Trial 31 finished with value: 0.03184604475168678 and parameters: {'observation_period_num': 5, 'train_rates': 0.9585807153664458, 'learning_rate': 9.084823171754106e-05, 'batch_size': 26, 'step_size': 8, 'gamma': 0.8145897338918653}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:53:42,410][0m Trial 32 finished with value: 0.06009259168058634 and parameters: {'observation_period_num': 47, 'train_rates': 0.9425375361835623, 'learning_rate': 4.166364374648432e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.8413906124821635}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 09:59:40,189][0m Trial 33 finished with value: 0.03187038849752683 and parameters: {'observation_period_num': 5, 'train_rates': 0.9649432955350667, 'learning_rate': 8.824580579513378e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7902095138480103}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:04:23,747][0m Trial 34 finished with value: 0.03715959427560248 and parameters: {'observation_period_num': 22, 'train_rates': 0.9581917017315433, 'learning_rate': 8.514653277285313e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.7845598739878072}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:06:08,335][0m Trial 35 finished with value: 0.05068347432951289 and parameters: {'observation_period_num': 37, 'train_rates': 0.9132155245771015, 'learning_rate': 0.00019511571083571163, 'batch_size': 54, 'step_size': 7, 'gamma': 0.8076946614782808}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:09:12,139][0m Trial 36 finished with value: 0.03953759310146173 and parameters: {'observation_period_num': 22, 'train_rates': 0.9591976972585717, 'learning_rate': 0.0004576408587737046, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8723942796358315}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:10:26,740][0m Trial 37 finished with value: 0.2052029030254254 and parameters: {'observation_period_num': 120, 'train_rates': 0.963526854594899, 'learning_rate': 1.92835907793125e-05, 'batch_size': 77, 'step_size': 4, 'gamma': 0.7914201783843167}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:15:18,667][0m Trial 38 finished with value: 0.06335597328562557 and parameters: {'observation_period_num': 55, 'train_rates': 0.8806027288157238, 'learning_rate': 3.038989038363014e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.8407204904639366}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:16:17,519][0m Trial 39 finished with value: 0.05412431817218574 and parameters: {'observation_period_num': 33, 'train_rates': 0.9415592753625903, 'learning_rate': 0.00010205613443226073, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8165325105858142}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:17:36,195][0m Trial 40 finished with value: 0.20480162393340665 and parameters: {'observation_period_num': 100, 'train_rates': 0.698423681717451, 'learning_rate': 0.0004374276705656654, 'batch_size': 58, 'step_size': 6, 'gamma': 0.858543732135182}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:20:03,077][0m Trial 41 finished with value: 0.038742127745184256 and parameters: {'observation_period_num': 5, 'train_rates': 0.8521138941408342, 'learning_rate': 5.725841439961178e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7671522018066866}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:21:56,381][0m Trial 42 finished with value: 0.18252291547644084 and parameters: {'observation_period_num': 15, 'train_rates': 0.7800435257118219, 'learning_rate': 4.4349345872695276e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.7735279828311322}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:24:44,395][0m Trial 43 finished with value: 0.10975721618923985 and parameters: {'observation_period_num': 252, 'train_rates': 0.9098628990385041, 'learning_rate': 0.000132322677846423, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7858988531228505}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:25:57,283][0m Trial 44 finished with value: 0.0546070770138786 and parameters: {'observation_period_num': 29, 'train_rates': 0.7992724156155137, 'learning_rate': 8.103623437671392e-05, 'batch_size': 70, 'step_size': 8, 'gamma': 0.7622781000704363}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:28:39,318][0m Trial 45 finished with value: 0.16965106009593 and parameters: {'observation_period_num': 18, 'train_rates': 0.7385347449616179, 'learning_rate': 2.71501243797634e-05, 'batch_size': 29, 'step_size': 12, 'gamma': 0.7984703388348412}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:34:32,016][0m Trial 46 finished with value: 0.06685894788527975 and parameters: {'observation_period_num': 66, 'train_rates': 0.9662702191930509, 'learning_rate': 0.00019697953001435937, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7789978291762903}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:34:53,808][0m Trial 47 finished with value: 0.16884644576033256 and parameters: {'observation_period_num': 204, 'train_rates': 0.8067109606940043, 'learning_rate': 5.056673644096938e-05, 'batch_size': 248, 'step_size': 5, 'gamma': 0.9058296740942918}. Best is trial 17 with value: 0.03099128835941706.[0m
Early stopping at epoch 68
[32m[I 2025-01-08 10:35:22,954][0m Trial 48 finished with value: 0.24460293278098105 and parameters: {'observation_period_num': 151, 'train_rates': 0.8932473633169312, 'learning_rate': 0.00010003117213996216, 'batch_size': 135, 'step_size': 1, 'gamma': 0.8303549838253372}. Best is trial 17 with value: 0.03099128835941706.[0m
[32m[I 2025-01-08 10:37:39,577][0m Trial 49 finished with value: 0.06543085662604764 and parameters: {'observation_period_num': 5, 'train_rates': 0.8544070813563458, 'learning_rate': 1.269395236174785e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.794111582659662}. Best is trial 17 with value: 0.03099128835941706.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-08 10:37:39,588][0m A new study created in memory with name: no-name-4a21490a-ca95-41f5-9dc7-aa687cd4096b[0m
[32m[I 2025-01-08 10:38:14,261][0m Trial 0 finished with value: 0.25622010404753576 and parameters: {'observation_period_num': 35, 'train_rates': 0.923764939997516, 'learning_rate': 2.538345499599325e-06, 'batch_size': 178, 'step_size': 12, 'gamma': 0.9773115485960349}. Best is trial 0 with value: 0.25622010404753576.[0m
[32m[I 2025-01-08 10:39:04,080][0m Trial 1 finished with value: 0.0893803084392128 and parameters: {'observation_period_num': 214, 'train_rates': 0.8509002365493908, 'learning_rate': 0.0008898944575981938, 'batch_size': 104, 'step_size': 1, 'gamma': 0.950034568232606}. Best is trial 1 with value: 0.0893803084392128.[0m
[32m[I 2025-01-08 10:39:28,931][0m Trial 2 finished with value: 0.04721776352209203 and parameters: {'observation_period_num': 36, 'train_rates': 0.9068269295975829, 'learning_rate': 0.00021956014338489073, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9566141830125505}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:40:20,127][0m Trial 3 finished with value: 0.053044488018861524 and parameters: {'observation_period_num': 81, 'train_rates': 0.9062752500671143, 'learning_rate': 0.0001795783773693137, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8744493659184321}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:40:44,111][0m Trial 4 finished with value: 0.20692157137470887 and parameters: {'observation_period_num': 92, 'train_rates': 0.7429565342346751, 'learning_rate': 0.0004606668292065424, 'batch_size': 227, 'step_size': 7, 'gamma': 0.8915487542512106}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:41:42,331][0m Trial 5 finished with value: 0.21638989126166472 and parameters: {'observation_period_num': 111, 'train_rates': 0.7074550078863283, 'learning_rate': 0.00022906565629225148, 'batch_size': 80, 'step_size': 4, 'gamma': 0.8023235576650045}. Best is trial 2 with value: 0.04721776352209203.[0m
Early stopping at epoch 57
[32m[I 2025-01-08 10:42:01,935][0m Trial 6 finished with value: 1.3195031881332397 and parameters: {'observation_period_num': 71, 'train_rates': 0.9718620331816865, 'learning_rate': 3.5899831583493224e-06, 'batch_size': 188, 'step_size': 1, 'gamma': 0.8281440738352933}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:42:42,594][0m Trial 7 finished with value: 0.10210800656052522 and parameters: {'observation_period_num': 195, 'train_rates': 0.8754231169857761, 'learning_rate': 7.63169313412843e-05, 'batch_size': 133, 'step_size': 14, 'gamma': 0.960184361692907}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:43:09,529][0m Trial 8 finished with value: 0.4760041021736416 and parameters: {'observation_period_num': 249, 'train_rates': 0.8753193952185021, 'learning_rate': 1.1677301344742937e-06, 'batch_size': 212, 'step_size': 7, 'gamma': 0.9766174316000626}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:43:50,148][0m Trial 9 finished with value: 0.25917510081147066 and parameters: {'observation_period_num': 9, 'train_rates': 0.7451042867105784, 'learning_rate': 4.718874111138574e-06, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9404665334402369}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:44:13,733][0m Trial 10 finished with value: 0.20907117591963875 and parameters: {'observation_period_num': 150, 'train_rates': 0.8042247785935166, 'learning_rate': 2.39035623874308e-05, 'batch_size': 244, 'step_size': 11, 'gamma': 0.75435795362744}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:47:09,698][0m Trial 11 finished with value: 0.05682322662323713 and parameters: {'observation_period_num': 52, 'train_rates': 0.9851303297698963, 'learning_rate': 7.688904012620851e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8954641634173657}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:48:22,379][0m Trial 12 finished with value: 0.19131506195738981 and parameters: {'observation_period_num': 148, 'train_rates': 0.6156409810655903, 'learning_rate': 0.00010831476109377614, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8487475327327526}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:48:58,778][0m Trial 13 finished with value: 0.060093195021020625 and parameters: {'observation_period_num': 11, 'train_rates': 0.9233833256614999, 'learning_rate': 2.43673265855014e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.9161142211450735}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:49:57,756][0m Trial 14 finished with value: 0.07589552465040864 and parameters: {'observation_period_num': 79, 'train_rates': 0.820518007824372, 'learning_rate': 0.0002807535093890323, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8709178712086536}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:50:36,206][0m Trial 15 finished with value: 0.06880484671541015 and parameters: {'observation_period_num': 120, 'train_rates': 0.9309002555567318, 'learning_rate': 0.0008452477183059019, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8019242997663697}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:51:05,036][0m Trial 16 finished with value: 0.15741955240567526 and parameters: {'observation_period_num': 43, 'train_rates': 0.8930600646367305, 'learning_rate': 1.0983698638836022e-05, 'batch_size': 208, 'step_size': 15, 'gamma': 0.9204498449167217}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:51:29,759][0m Trial 17 finished with value: 0.059490948523345744 and parameters: {'observation_period_num': 61, 'train_rates': 0.8360132765786634, 'learning_rate': 0.0001825572143988844, 'batch_size': 250, 'step_size': 13, 'gamma': 0.7505608159513241}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:52:14,727][0m Trial 18 finished with value: 0.24488230251888343 and parameters: {'observation_period_num': 98, 'train_rates': 0.768171957426778, 'learning_rate': 4.975512719132651e-05, 'batch_size': 113, 'step_size': 9, 'gamma': 0.8607918425768183}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:53:23,124][0m Trial 19 finished with value: 0.19332723817996267 and parameters: {'observation_period_num': 144, 'train_rates': 0.6738443553445634, 'learning_rate': 0.0004750877534228203, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9249877083410345}. Best is trial 2 with value: 0.04721776352209203.[0m
[32m[I 2025-01-08 10:55:52,078][0m Trial 20 finished with value: 0.04118053987622261 and parameters: {'observation_period_num': 28, 'train_rates': 0.9384875173174226, 'learning_rate': 0.0001403871944882851, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8900137081121209}. Best is trial 20 with value: 0.04118053987622261.[0m
[32m[I 2025-01-08 11:01:23,670][0m Trial 21 finished with value: 0.05750385234414628 and parameters: {'observation_period_num': 33, 'train_rates': 0.9550092076745902, 'learning_rate': 0.00014361870869753662, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8967283031122382}. Best is trial 20 with value: 0.04118053987622261.[0m
[32m[I 2025-01-08 11:03:19,556][0m Trial 22 finished with value: 0.03832464755202333 and parameters: {'observation_period_num': 24, 'train_rates': 0.8977048835239347, 'learning_rate': 0.0003290705830793794, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8421988383573827}. Best is trial 22 with value: 0.03832464755202333.[0m
[32m[I 2025-01-08 11:05:31,602][0m Trial 23 finished with value: 0.04194257678525732 and parameters: {'observation_period_num': 21, 'train_rates': 0.9487255342038764, 'learning_rate': 0.00039170964896330834, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8318868163516829}. Best is trial 22 with value: 0.03832464755202333.[0m
[32m[I 2025-01-08 11:07:53,801][0m Trial 24 finished with value: 0.03444091446038817 and parameters: {'observation_period_num': 9, 'train_rates': 0.9501518574478275, 'learning_rate': 0.00041106134625214366, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8345217167808987}. Best is trial 24 with value: 0.03444091446038817.[0m
[32m[I 2025-01-08 11:12:45,657][0m Trial 25 finished with value: 0.0348589438945055 and parameters: {'observation_period_num': 7, 'train_rates': 0.8598033791178261, 'learning_rate': 0.0005631800441003893, 'batch_size': 18, 'step_size': 11, 'gamma': 0.8004190626028973}. Best is trial 24 with value: 0.03444091446038817.[0m
[32m[I 2025-01-08 11:17:55,628][0m Trial 26 finished with value: 0.03377953039773419 and parameters: {'observation_period_num': 10, 'train_rates': 0.8633216578205954, 'learning_rate': 0.0006095356170655949, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7778913697906622}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:21:52,944][0m Trial 27 finished with value: 0.03398225016784496 and parameters: {'observation_period_num': 9, 'train_rates': 0.8588746307101951, 'learning_rate': 0.0006871345483251992, 'batch_size': 22, 'step_size': 9, 'gamma': 0.7775465440375163}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:23:00,100][0m Trial 28 finished with value: 0.19697660832711042 and parameters: {'observation_period_num': 58, 'train_rates': 0.7793979079397207, 'learning_rate': 0.000924194044534423, 'batch_size': 74, 'step_size': 5, 'gamma': 0.7712805523320275}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:26:07,519][0m Trial 29 finished with value: 0.07209324896886422 and parameters: {'observation_period_num': 48, 'train_rates': 0.823380893627329, 'learning_rate': 0.0006886594691682977, 'batch_size': 27, 'step_size': 9, 'gamma': 0.7780716002770696}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:27:46,618][0m Trial 30 finished with value: 0.05753474670186268 and parameters: {'observation_period_num': 19, 'train_rates': 0.8771799784995584, 'learning_rate': 4.603027956886539e-05, 'batch_size': 55, 'step_size': 6, 'gamma': 0.7809571178746763}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:32:46,997][0m Trial 31 finished with value: 0.0383012345129693 and parameters: {'observation_period_num': 10, 'train_rates': 0.8441507123268501, 'learning_rate': 0.0005562996121554152, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8070734856157661}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:37:56,893][0m Trial 32 finished with value: 0.040478877623938966 and parameters: {'observation_period_num': 6, 'train_rates': 0.8666707546954641, 'learning_rate': 0.0009839589719593956, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8179534261144065}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:40:41,297][0m Trial 33 finished with value: 0.047536084552605946 and parameters: {'observation_period_num': 34, 'train_rates': 0.8562327111593282, 'learning_rate': 0.0005499000834336458, 'batch_size': 32, 'step_size': 9, 'gamma': 0.7921577752040709}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:42:09,352][0m Trial 34 finished with value: 0.11655232334590476 and parameters: {'observation_period_num': 189, 'train_rates': 0.800598043857614, 'learning_rate': 0.0003310404813036561, 'batch_size': 54, 'step_size': 10, 'gamma': 0.7659197013894092}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:43:12,030][0m Trial 35 finished with value: 0.04517794718240736 and parameters: {'observation_period_num': 38, 'train_rates': 0.910440549011215, 'learning_rate': 0.0007071478699965529, 'batch_size': 89, 'step_size': 8, 'gamma': 0.7898928411022859}. Best is trial 26 with value: 0.03377953039773419.[0m
[32m[I 2025-01-08 11:44:29,059][0m Trial 36 finished with value: 0.029196024568105838 and parameters: {'observation_period_num': 5, 'train_rates': 0.8301117959609203, 'learning_rate': 0.0002367299444144094, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8204687213801174}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:45:42,157][0m Trial 37 finished with value: 0.051372977740624375 and parameters: {'observation_period_num': 67, 'train_rates': 0.8236599457583414, 'learning_rate': 0.00025209444261423554, 'batch_size': 70, 'step_size': 12, 'gamma': 0.8198599959988642}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:46:36,782][0m Trial 38 finished with value: 0.17526323621895928 and parameters: {'observation_period_num': 24, 'train_rates': 0.7803122743840436, 'learning_rate': 0.0003872696921316406, 'batch_size': 94, 'step_size': 8, 'gamma': 0.8450407285305004}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:48:04,839][0m Trial 39 finished with value: 0.07992898414329606 and parameters: {'observation_period_num': 91, 'train_rates': 0.966073086768507, 'learning_rate': 0.00010278689151108972, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8143547701137723}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:49:48,746][0m Trial 40 finished with value: 0.17766479932861365 and parameters: {'observation_period_num': 46, 'train_rates': 0.7392099965140673, 'learning_rate': 0.00020524974865610457, 'batch_size': 45, 'step_size': 7, 'gamma': 0.8344798695728384}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:53:09,201][0m Trial 41 finished with value: 0.03940358040445671 and parameters: {'observation_period_num': 7, 'train_rates': 0.8533882434654578, 'learning_rate': 0.0006453112490049279, 'batch_size': 26, 'step_size': 10, 'gamma': 0.7949392390067564}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:55:51,630][0m Trial 42 finished with value: 0.031471937196329236 and parameters: {'observation_period_num': 5, 'train_rates': 0.880857458107092, 'learning_rate': 0.00040253236469252134, 'batch_size': 33, 'step_size': 11, 'gamma': 0.7658011990600699}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 11:58:27,120][0m Trial 43 finished with value: 0.04242342532061421 and parameters: {'observation_period_num': 21, 'train_rates': 0.8864940927384636, 'learning_rate': 0.00042542297875236405, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7663197154641214}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:00:02,212][0m Trial 44 finished with value: 0.03818566957488656 and parameters: {'observation_period_num': 34, 'train_rates': 0.9194078324057984, 'learning_rate': 0.00026581733945673513, 'batch_size': 59, 'step_size': 9, 'gamma': 0.7831401365264938}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:01:52,345][0m Trial 45 finished with value: 0.07039723234872024 and parameters: {'observation_period_num': 18, 'train_rates': 0.8325536262149988, 'learning_rate': 9.858634528364553e-06, 'batch_size': 47, 'step_size': 12, 'gamma': 0.7607897393291837}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:02:55,298][0m Trial 46 finished with value: 0.18033390457139295 and parameters: {'observation_period_num': 243, 'train_rates': 0.8122440799688153, 'learning_rate': 0.00015145099233162377, 'batch_size': 77, 'step_size': 2, 'gamma': 0.7748267687098627}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:06:15,234][0m Trial 47 finished with value: 0.06239739111323151 and parameters: {'observation_period_num': 50, 'train_rates': 0.9044732694236725, 'learning_rate': 0.0007587970461966949, 'batch_size': 27, 'step_size': 11, 'gamma': 0.8552809335885242}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:08:49,867][0m Trial 48 finished with value: 0.4094431698322296 and parameters: {'observation_period_num': 173, 'train_rates': 0.9890744585776963, 'learning_rate': 1.5369764671741577e-06, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8243113060253674}. Best is trial 36 with value: 0.029196024568105838.[0m
[32m[I 2025-01-08 12:09:37,198][0m Trial 49 finished with value: 0.02993878278021629 and parameters: {'observation_period_num': 5, 'train_rates': 0.8855139171042664, 'learning_rate': 0.00031924299425119864, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7594350283676775}. Best is trial 36 with value: 0.029196024568105838.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-08 12:09:37,208][0m A new study created in memory with name: no-name-14aab70e-d6fd-4989-af6e-7ae02bd525fa[0m
[32m[I 2025-01-08 12:10:05,607][0m Trial 0 finished with value: 0.23571220675100057 and parameters: {'observation_period_num': 123, 'train_rates': 0.7067240805377533, 'learning_rate': 0.00021927569098630298, 'batch_size': 174, 'step_size': 8, 'gamma': 0.7937604451864329}. Best is trial 0 with value: 0.23571220675100057.[0m
[32m[I 2025-01-08 12:10:30,552][0m Trial 1 finished with value: 0.06450247237186753 and parameters: {'observation_period_num': 90, 'train_rates': 0.906899726318553, 'learning_rate': 0.0006091451037570241, 'batch_size': 240, 'step_size': 14, 'gamma': 0.9799825541523342}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:11:32,892][0m Trial 2 finished with value: 0.4179284763900009 and parameters: {'observation_period_num': 40, 'train_rates': 0.7208785731451653, 'learning_rate': 8.659943056130003e-06, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8058598072509315}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:11:58,461][0m Trial 3 finished with value: 0.4876755426016558 and parameters: {'observation_period_num': 69, 'train_rates': 0.7231127338845383, 'learning_rate': 5.1392512120923305e-06, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9412759322625552}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:12:27,397][0m Trial 4 finished with value: 0.11096381052019913 and parameters: {'observation_period_num': 104, 'train_rates': 0.8764669866114543, 'learning_rate': 3.9618401944177946e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.7748056853976939}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:12:51,976][0m Trial 5 finished with value: 0.17781831324100494 and parameters: {'observation_period_num': 232, 'train_rates': 0.9247711217504835, 'learning_rate': 5.9966741129274014e-05, 'batch_size': 232, 'step_size': 6, 'gamma': 0.8065082759476391}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:13:33,799][0m Trial 6 finished with value: 0.2968586350250923 and parameters: {'observation_period_num': 103, 'train_rates': 0.7043741101802503, 'learning_rate': 2.23020071537901e-05, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8289125425051191}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:13:59,336][0m Trial 7 finished with value: 0.22665096028965911 and parameters: {'observation_period_num': 234, 'train_rates': 0.8131030230376733, 'learning_rate': 7.018278169868559e-05, 'batch_size': 207, 'step_size': 4, 'gamma': 0.8258372364144392}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:15:06,886][0m Trial 8 finished with value: 0.2672884166240692 and parameters: {'observation_period_num': 239, 'train_rates': 0.9713072443209928, 'learning_rate': 7.832130022178632e-06, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9019240428727432}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:17:29,980][0m Trial 9 finished with value: 0.07389699642908548 and parameters: {'observation_period_num': 27, 'train_rates': 0.8360036381806057, 'learning_rate': 5.572874520898201e-06, 'batch_size': 36, 'step_size': 15, 'gamma': 0.8237385176932055}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:18:12,094][0m Trial 10 finished with value: 0.18644092977046967 and parameters: {'observation_period_num': 172, 'train_rates': 0.9666704654488838, 'learning_rate': 0.0009495095641949173, 'batch_size': 139, 'step_size': 11, 'gamma': 0.980251316591594}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:20:36,571][0m Trial 11 finished with value: 1.0571683796969327 and parameters: {'observation_period_num': 10, 'train_rates': 0.8318784320258046, 'learning_rate': 1.1706576235677757e-06, 'batch_size': 36, 'step_size': 1, 'gamma': 0.8756702567274842}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:20:59,300][0m Trial 12 finished with value: 0.07459156215190887 and parameters: {'observation_period_num': 60, 'train_rates': 0.8774029468231841, 'learning_rate': 0.0009550706404187315, 'batch_size': 255, 'step_size': 12, 'gamma': 0.7529981062004634}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:23:29,693][0m Trial 13 finished with value: 0.4863804724233545 and parameters: {'observation_period_num': 164, 'train_rates': 0.7752074051454134, 'learning_rate': 1.7237589636817355e-06, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8617162245590022}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:24:01,298][0m Trial 14 finished with value: 0.13692485147143063 and parameters: {'observation_period_num': 18, 'train_rates': 0.6273028048735209, 'learning_rate': 0.00019016925547583, 'batch_size': 153, 'step_size': 9, 'gamma': 0.9394701719919115}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:24:56,616][0m Trial 15 finished with value: 0.2731439923017011 and parameters: {'observation_period_num': 81, 'train_rates': 0.8796673693454187, 'learning_rate': 2.7570647559423106e-06, 'batch_size': 101, 'step_size': 14, 'gamma': 0.9810858417296934}. Best is trial 1 with value: 0.06450247237186753.[0m
[32m[I 2025-01-08 12:26:37,061][0m Trial 16 finished with value: 0.06100716731614537 and parameters: {'observation_period_num': 41, 'train_rates': 0.9165875199123205, 'learning_rate': 1.9351086251281197e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.8575677626293409}. Best is trial 16 with value: 0.06100716731614537.[0m
[32m[I 2025-01-08 12:27:09,768][0m Trial 17 finished with value: 0.1695463181410021 and parameters: {'observation_period_num': 153, 'train_rates': 0.9268104238431674, 'learning_rate': 1.8143251981479213e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.9053099624209902}. Best is trial 16 with value: 0.06100716731614537.[0m
[32m[I 2025-01-08 12:28:30,891][0m Trial 18 finished with value: 0.068124244870936 and parameters: {'observation_period_num': 50, 'train_rates': 0.9294958148549439, 'learning_rate': 0.0003762489118426415, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8617545131124729}. Best is trial 16 with value: 0.06100716731614537.[0m
[32m[I 2025-01-08 12:30:14,261][0m Trial 19 finished with value: 0.10251671075820923 and parameters: {'observation_period_num': 97, 'train_rates': 0.9882287678483312, 'learning_rate': 9.842013385553956e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9449013304211046}. Best is trial 16 with value: 0.06100716731614537.[0m
[32m[I 2025-01-08 12:31:00,877][0m Trial 20 finished with value: 0.18858813721200693 and parameters: {'observation_period_num': 136, 'train_rates': 0.7796472460266131, 'learning_rate': 1.6605094415125408e-05, 'batch_size': 108, 'step_size': 13, 'gamma': 0.8914467059136626}. Best is trial 16 with value: 0.06100716731614537.[0m
[32m[I 2025-01-08 12:32:15,761][0m Trial 21 finished with value: 0.060396211221814156 and parameters: {'observation_period_num': 47, 'train_rates': 0.9237591919083327, 'learning_rate': 0.0004078594627885673, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8539644651818298}. Best is trial 21 with value: 0.060396211221814156.[0m
[32m[I 2025-01-08 12:34:00,513][0m Trial 22 finished with value: 0.06361355551665551 and parameters: {'observation_period_num': 40, 'train_rates': 0.8957023215135296, 'learning_rate': 0.00043820975820542884, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8476478264780136}. Best is trial 21 with value: 0.060396211221814156.[0m
[32m[I 2025-01-08 12:35:46,986][0m Trial 23 finished with value: 0.047604615056274724 and parameters: {'observation_period_num': 40, 'train_rates': 0.9491828294155104, 'learning_rate': 0.00022283283772222374, 'batch_size': 54, 'step_size': 7, 'gamma': 0.8463898188676889}. Best is trial 23 with value: 0.047604615056274724.[0m
[32m[I 2025-01-08 12:40:16,937][0m Trial 24 finished with value: 0.026026369647142736 and parameters: {'observation_period_num': 6, 'train_rates': 0.9524756977260238, 'learning_rate': 0.00024507610243632564, 'batch_size': 21, 'step_size': 3, 'gamma': 0.8422050181926662}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 12:45:32,476][0m Trial 25 finished with value: 0.03530308118442426 and parameters: {'observation_period_num': 6, 'train_rates': 0.9500033468567598, 'learning_rate': 0.0001473929445807876, 'batch_size': 18, 'step_size': 2, 'gamma': 0.8398050036210043}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 12:49:40,413][0m Trial 26 finished with value: 0.03952881994728859 and parameters: {'observation_period_num': 9, 'train_rates': 0.9515817918669781, 'learning_rate': 0.00012549272256236343, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8381330884943486}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 12:54:31,663][0m Trial 27 finished with value: 0.04875622130930424 and parameters: {'observation_period_num': 16, 'train_rates': 0.9862557815273054, 'learning_rate': 0.00013342753390826594, 'batch_size': 20, 'step_size': 1, 'gamma': 0.8853826334578421}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 12:58:51,832][0m Trial 28 finished with value: 0.043851258734998094 and parameters: {'observation_period_num': 5, 'train_rates': 0.8516198771094385, 'learning_rate': 0.0001157503037669343, 'batch_size': 20, 'step_size': 2, 'gamma': 0.7825685165715052}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:04:19,288][0m Trial 29 finished with value: 0.08967818726192821 and parameters: {'observation_period_num': 198, 'train_rates': 0.9521156536614311, 'learning_rate': 0.00023672193507596649, 'batch_size': 16, 'step_size': 3, 'gamma': 0.835613795102873}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:06:13,011][0m Trial 30 finished with value: 0.3886428251071314 and parameters: {'observation_period_num': 25, 'train_rates': 0.6144066640722383, 'learning_rate': 5.2274810694285e-05, 'batch_size': 37, 'step_size': 3, 'gamma': 0.8080912733399299}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:11:25,180][0m Trial 31 finished with value: 0.04237619228208962 and parameters: {'observation_period_num': 5, 'train_rates': 0.8604982929461177, 'learning_rate': 0.00011659120025657134, 'batch_size': 17, 'step_size': 2, 'gamma': 0.7841383507430499}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:17:21,918][0m Trial 32 finished with value: 0.04723812983586238 and parameters: {'observation_period_num': 8, 'train_rates': 0.9560006764570439, 'learning_rate': 9.403544231145904e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7647169513963744}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:19:35,558][0m Trial 33 finished with value: 0.04608213492822878 and parameters: {'observation_period_num': 30, 'train_rates': 0.8945660734155971, 'learning_rate': 0.00014435581449121054, 'batch_size': 41, 'step_size': 3, 'gamma': 0.78776381420497}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:21:26,904][0m Trial 34 finished with value: 0.060149001023973254 and parameters: {'observation_period_num': 72, 'train_rates': 0.854033428812846, 'learning_rate': 0.0002959459155113309, 'batch_size': 47, 'step_size': 4, 'gamma': 0.7989629388854187}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:22:46,253][0m Trial 35 finished with value: 0.10915889020462136 and parameters: {'observation_period_num': 27, 'train_rates': 0.9024567946729747, 'learning_rate': 3.4722744037575214e-05, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8157544021471279}. Best is trial 24 with value: 0.026026369647142736.[0m
Early stopping at epoch 90
[32m[I 2025-01-08 13:25:48,436][0m Trial 36 finished with value: 0.07216588342014481 and parameters: {'observation_period_num': 62, 'train_rates': 0.9414329821690823, 'learning_rate': 0.00017826032893940037, 'batch_size': 28, 'step_size': 1, 'gamma': 0.8428521479046265}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:26:49,154][0m Trial 37 finished with value: 0.07156857532878366 and parameters: {'observation_period_num': 117, 'train_rates': 0.8601590344371106, 'learning_rate': 0.0006124312879836113, 'batch_size': 90, 'step_size': 4, 'gamma': 0.8171911068407879}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:29:41,309][0m Trial 38 finished with value: 0.15905332020502316 and parameters: {'observation_period_num': 6, 'train_rates': 0.7425984520292515, 'learning_rate': 7.56502004457805e-05, 'batch_size': 28, 'step_size': 5, 'gamma': 0.7943887821237222}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:31:11,128][0m Trial 39 finished with value: 0.0760887865794868 and parameters: {'observation_period_num': 21, 'train_rates': 0.9721982029402968, 'learning_rate': 4.728889059096645e-05, 'batch_size': 65, 'step_size': 2, 'gamma': 0.8724107799817121}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:32:12,733][0m Trial 40 finished with value: 0.06135832857001911 and parameters: {'observation_period_num': 56, 'train_rates': 0.8064706040275559, 'learning_rate': 0.0006216227913848019, 'batch_size': 85, 'step_size': 3, 'gamma': 0.7743979374215897}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:35:50,922][0m Trial 41 finished with value: 0.05082594365295437 and parameters: {'observation_period_num': 10, 'train_rates': 0.8535092952871202, 'learning_rate': 9.87884554645917e-05, 'batch_size': 24, 'step_size': 2, 'gamma': 0.7794023494537069}. Best is trial 24 with value: 0.026026369647142736.[0m
Early stopping at epoch 56
[32m[I 2025-01-08 13:37:03,253][0m Trial 42 finished with value: 0.06487849188515865 and parameters: {'observation_period_num': 5, 'train_rates': 0.8299812384325307, 'learning_rate': 0.00012104271692714357, 'batch_size': 42, 'step_size': 1, 'gamma': 0.752922866620156}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:41:48,659][0m Trial 43 finished with value: 0.04933038403496089 and parameters: {'observation_period_num': 32, 'train_rates': 0.9101575836119795, 'learning_rate': 2.8679700059256124e-05, 'batch_size': 19, 'step_size': 4, 'gamma': 0.8342917891122796}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:43:20,548][0m Trial 44 finished with value: 0.0517262798934681 and parameters: {'observation_period_num': 21, 'train_rates': 0.938410135281194, 'learning_rate': 0.00027258686173755735, 'batch_size': 62, 'step_size': 2, 'gamma': 0.788186749483192}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:46:09,429][0m Trial 45 finished with value: 0.03519342716964746 and parameters: {'observation_period_num': 5, 'train_rates': 0.8816667626474585, 'learning_rate': 6.850721175337155e-05, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8068407511433638}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:48:52,062][0m Trial 46 finished with value: 0.04692797254967062 and parameters: {'observation_period_num': 34, 'train_rates': 0.8831229008089815, 'learning_rate': 7.28553314480033e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.8040868514659025}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:51:02,287][0m Trial 47 finished with value: 0.03759160783374682 and parameters: {'observation_period_num': 18, 'train_rates': 0.9673220261178135, 'learning_rate': 0.00015277839622162748, 'batch_size': 45, 'step_size': 6, 'gamma': 0.8186456097980259}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:53:03,802][0m Trial 48 finished with value: 0.03787211737325115 and parameters: {'observation_period_num': 17, 'train_rates': 0.9683444980794349, 'learning_rate': 0.00018050406987797274, 'batch_size': 48, 'step_size': 6, 'gamma': 0.8223856787511017}. Best is trial 24 with value: 0.026026369647142736.[0m
[32m[I 2025-01-08 13:54:05,381][0m Trial 49 finished with value: 0.0753302052617073 and parameters: {'observation_period_num': 82, 'train_rates': 0.9729643393133236, 'learning_rate': 0.00017137485315619065, 'batch_size': 94, 'step_size': 6, 'gamma': 0.8225499740263775}. Best is trial 24 with value: 0.026026369647142736.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.95401046812605, 'learning_rate': 0.0003204139231581046, 'batch_size': 130, 'step_size': 9, 'gamma': 0.8488686699678654}
Epoch 1/300, trend Loss: 0.5222 | 0.1758
Epoch 2/300, trend Loss: 0.1867 | 0.1277
Epoch 3/300, trend Loss: 0.1450 | 0.1220
Epoch 4/300, trend Loss: 0.1363 | 0.0970
Epoch 5/300, trend Loss: 0.1289 | 0.0803
Epoch 6/300, trend Loss: 0.1317 | 0.0817
Epoch 7/300, trend Loss: 0.1542 | 0.0814
Epoch 8/300, trend Loss: 0.2011 | 0.2338
Epoch 9/300, trend Loss: 0.1893 | 0.2256
Epoch 10/300, trend Loss: 0.1618 | 0.0850
Epoch 11/300, trend Loss: 0.1364 | 0.0832
Epoch 12/300, trend Loss: 0.1253 | 0.0795
Epoch 13/300, trend Loss: 0.1132 | 0.0693
Epoch 14/300, trend Loss: 0.1111 | 0.0662
Epoch 15/300, trend Loss: 0.1156 | 0.0681
Epoch 16/300, trend Loss: 0.1230 | 0.0694
Epoch 17/300, trend Loss: 0.1319 | 0.0724
Epoch 18/300, trend Loss: 0.1362 | 0.0760
Epoch 19/300, trend Loss: 0.1339 | 0.0741
Epoch 20/300, trend Loss: 0.1125 | 0.0645
Epoch 21/300, trend Loss: 0.1204 | 0.0750
Epoch 22/300, trend Loss: 0.1178 | 0.0716
Epoch 23/300, trend Loss: 0.1052 | 0.0679
Epoch 24/300, trend Loss: 0.0962 | 0.0638
Epoch 25/300, trend Loss: 0.0957 | 0.0602
Epoch 26/300, trend Loss: 0.0944 | 0.0593
Epoch 27/300, trend Loss: 0.0935 | 0.0586
Epoch 28/300, trend Loss: 0.0926 | 0.0600
Epoch 29/300, trend Loss: 0.0914 | 0.0591
Epoch 30/300, trend Loss: 0.0903 | 0.0572
Epoch 31/300, trend Loss: 0.0893 | 0.0561
Epoch 32/300, trend Loss: 0.0884 | 0.0557
Epoch 33/300, trend Loss: 0.0875 | 0.0551
Epoch 34/300, trend Loss: 0.0866 | 0.0539
Epoch 35/300, trend Loss: 0.0857 | 0.0530
Epoch 36/300, trend Loss: 0.0848 | 0.0523
Epoch 37/300, trend Loss: 0.0839 | 0.0512
Epoch 38/300, trend Loss: 0.0830 | 0.0499
Epoch 39/300, trend Loss: 0.0822 | 0.0492
Epoch 40/300, trend Loss: 0.0817 | 0.0488
Epoch 41/300, trend Loss: 0.0813 | 0.0482
Epoch 42/300, trend Loss: 0.0809 | 0.0476
Epoch 43/300, trend Loss: 0.0806 | 0.0471
Epoch 44/300, trend Loss: 0.0804 | 0.0470
Epoch 45/300, trend Loss: 0.0803 | 0.0468
Epoch 46/300, trend Loss: 0.0804 | 0.0470
Epoch 47/300, trend Loss: 0.0810 | 0.0477
Epoch 48/300, trend Loss: 0.0820 | 0.0483
Epoch 49/300, trend Loss: 0.0825 | 0.0478
Epoch 50/300, trend Loss: 0.0816 | 0.0469
Epoch 51/300, trend Loss: 0.0801 | 0.0460
Epoch 52/300, trend Loss: 0.0790 | 0.0454
Epoch 53/300, trend Loss: 0.0787 | 0.0453
Epoch 54/300, trend Loss: 0.0790 | 0.0454
Epoch 55/300, trend Loss: 0.0794 | 0.0452
Epoch 56/300, trend Loss: 0.0791 | 0.0445
Epoch 57/300, trend Loss: 0.0781 | 0.0442
Epoch 58/300, trend Loss: 0.0775 | 0.0441
Epoch 59/300, trend Loss: 0.0772 | 0.0440
Epoch 60/300, trend Loss: 0.0770 | 0.0439
Epoch 61/300, trend Loss: 0.0768 | 0.0437
Epoch 62/300, trend Loss: 0.0767 | 0.0435
Epoch 63/300, trend Loss: 0.0766 | 0.0434
Epoch 64/300, trend Loss: 0.0764 | 0.0432
Epoch 65/300, trend Loss: 0.0763 | 0.0431
Epoch 66/300, trend Loss: 0.0762 | 0.0429
Epoch 67/300, trend Loss: 0.0760 | 0.0428
Epoch 68/300, trend Loss: 0.0759 | 0.0427
Epoch 69/300, trend Loss: 0.0758 | 0.0426
Epoch 70/300, trend Loss: 0.0757 | 0.0425
Epoch 71/300, trend Loss: 0.0756 | 0.0424
Epoch 72/300, trend Loss: 0.0755 | 0.0423
Epoch 73/300, trend Loss: 0.0754 | 0.0422
Epoch 74/300, trend Loss: 0.0753 | 0.0421
Epoch 75/300, trend Loss: 0.0752 | 0.0420
Epoch 76/300, trend Loss: 0.0751 | 0.0420
Epoch 77/300, trend Loss: 0.0750 | 0.0419
Epoch 78/300, trend Loss: 0.0749 | 0.0418
Epoch 79/300, trend Loss: 0.0749 | 0.0417
Epoch 80/300, trend Loss: 0.0748 | 0.0417
Epoch 81/300, trend Loss: 0.0747 | 0.0416
Epoch 82/300, trend Loss: 0.0747 | 0.0416
Epoch 83/300, trend Loss: 0.0746 | 0.0415
Epoch 84/300, trend Loss: 0.0745 | 0.0414
Epoch 85/300, trend Loss: 0.0745 | 0.0414
Epoch 86/300, trend Loss: 0.0744 | 0.0413
Epoch 87/300, trend Loss: 0.0743 | 0.0413
Epoch 88/300, trend Loss: 0.0743 | 0.0412
Epoch 89/300, trend Loss: 0.0742 | 0.0412
Epoch 90/300, trend Loss: 0.0742 | 0.0411
Epoch 91/300, trend Loss: 0.0741 | 0.0411
Epoch 92/300, trend Loss: 0.0741 | 0.0410
Epoch 93/300, trend Loss: 0.0741 | 0.0410
Epoch 94/300, trend Loss: 0.0740 | 0.0410
Epoch 95/300, trend Loss: 0.0740 | 0.0409
Epoch 96/300, trend Loss: 0.0739 | 0.0409
Epoch 97/300, trend Loss: 0.0739 | 0.0408
Epoch 98/300, trend Loss: 0.0738 | 0.0408
Epoch 99/300, trend Loss: 0.0738 | 0.0408
Epoch 100/300, trend Loss: 0.0738 | 0.0407
Epoch 101/300, trend Loss: 0.0737 | 0.0407
Epoch 102/300, trend Loss: 0.0737 | 0.0407
Epoch 103/300, trend Loss: 0.0737 | 0.0407
Epoch 104/300, trend Loss: 0.0736 | 0.0406
Epoch 105/300, trend Loss: 0.0736 | 0.0406
Epoch 106/300, trend Loss: 0.0736 | 0.0406
Epoch 107/300, trend Loss: 0.0735 | 0.0405
Epoch 108/300, trend Loss: 0.0735 | 0.0405
Epoch 109/300, trend Loss: 0.0735 | 0.0405
Epoch 110/300, trend Loss: 0.0735 | 0.0405
Epoch 111/300, trend Loss: 0.0734 | 0.0404
Epoch 112/300, trend Loss: 0.0734 | 0.0404
Epoch 113/300, trend Loss: 0.0734 | 0.0404
Epoch 114/300, trend Loss: 0.0733 | 0.0404
Epoch 115/300, trend Loss: 0.0733 | 0.0404
Epoch 116/300, trend Loss: 0.0733 | 0.0403
Epoch 117/300, trend Loss: 0.0733 | 0.0403
Epoch 118/300, trend Loss: 0.0733 | 0.0403
Epoch 119/300, trend Loss: 0.0732 | 0.0403
Epoch 120/300, trend Loss: 0.0732 | 0.0403
Epoch 121/300, trend Loss: 0.0732 | 0.0402
Epoch 122/300, trend Loss: 0.0732 | 0.0402
Epoch 123/300, trend Loss: 0.0732 | 0.0402
Epoch 124/300, trend Loss: 0.0731 | 0.0402
Epoch 125/300, trend Loss: 0.0731 | 0.0402
Epoch 126/300, trend Loss: 0.0731 | 0.0402
Epoch 127/300, trend Loss: 0.0731 | 0.0401
Epoch 128/300, trend Loss: 0.0731 | 0.0401
Epoch 129/300, trend Loss: 0.0731 | 0.0401
Epoch 130/300, trend Loss: 0.0731 | 0.0401
Epoch 131/300, trend Loss: 0.0730 | 0.0401
Epoch 132/300, trend Loss: 0.0730 | 0.0401
Epoch 133/300, trend Loss: 0.0730 | 0.0401
Epoch 134/300, trend Loss: 0.0730 | 0.0401
Epoch 135/300, trend Loss: 0.0730 | 0.0400
Epoch 136/300, trend Loss: 0.0730 | 0.0400
Epoch 137/300, trend Loss: 0.0730 | 0.0400
Epoch 138/300, trend Loss: 0.0729 | 0.0400
Epoch 139/300, trend Loss: 0.0729 | 0.0400
Epoch 140/300, trend Loss: 0.0729 | 0.0400
Epoch 141/300, trend Loss: 0.0729 | 0.0400
Epoch 142/300, trend Loss: 0.0729 | 0.0400
Epoch 143/300, trend Loss: 0.0729 | 0.0400
Epoch 144/300, trend Loss: 0.0729 | 0.0400
Epoch 145/300, trend Loss: 0.0729 | 0.0399
Epoch 146/300, trend Loss: 0.0729 | 0.0399
Epoch 147/300, trend Loss: 0.0729 | 0.0399
Epoch 148/300, trend Loss: 0.0729 | 0.0399
Epoch 149/300, trend Loss: 0.0728 | 0.0399
Epoch 150/300, trend Loss: 0.0728 | 0.0399
Epoch 151/300, trend Loss: 0.0728 | 0.0399
Epoch 152/300, trend Loss: 0.0728 | 0.0399
Epoch 153/300, trend Loss: 0.0728 | 0.0399
Epoch 154/300, trend Loss: 0.0728 | 0.0399
Epoch 155/300, trend Loss: 0.0728 | 0.0399
Epoch 156/300, trend Loss: 0.0728 | 0.0399
Epoch 157/300, trend Loss: 0.0728 | 0.0399
Epoch 158/300, trend Loss: 0.0728 | 0.0399
Epoch 159/300, trend Loss: 0.0728 | 0.0399
Epoch 160/300, trend Loss: 0.0728 | 0.0399
Epoch 161/300, trend Loss: 0.0728 | 0.0398
Epoch 162/300, trend Loss: 0.0728 | 0.0398
Epoch 163/300, trend Loss: 0.0727 | 0.0398
Epoch 164/300, trend Loss: 0.0727 | 0.0398
Epoch 165/300, trend Loss: 0.0727 | 0.0398
Epoch 166/300, trend Loss: 0.0727 | 0.0398
Epoch 167/300, trend Loss: 0.0727 | 0.0398
Epoch 168/300, trend Loss: 0.0727 | 0.0398
Epoch 169/300, trend Loss: 0.0727 | 0.0398
Epoch 170/300, trend Loss: 0.0727 | 0.0398
Epoch 171/300, trend Loss: 0.0727 | 0.0398
Epoch 172/300, trend Loss: 0.0727 | 0.0398
Epoch 173/300, trend Loss: 0.0727 | 0.0398
Epoch 174/300, trend Loss: 0.0727 | 0.0398
Epoch 175/300, trend Loss: 0.0727 | 0.0398
Epoch 176/300, trend Loss: 0.0727 | 0.0398
Epoch 177/300, trend Loss: 0.0727 | 0.0398
Epoch 178/300, trend Loss: 0.0727 | 0.0398
Epoch 179/300, trend Loss: 0.0727 | 0.0398
Epoch 180/300, trend Loss: 0.0727 | 0.0398
Epoch 181/300, trend Loss: 0.0727 | 0.0398
Epoch 182/300, trend Loss: 0.0727 | 0.0398
Epoch 183/300, trend Loss: 0.0727 | 0.0398
Epoch 184/300, trend Loss: 0.0727 | 0.0398
Epoch 185/300, trend Loss: 0.0727 | 0.0398
Epoch 186/300, trend Loss: 0.0727 | 0.0398
Epoch 187/300, trend Loss: 0.0727 | 0.0398
Epoch 188/300, trend Loss: 0.0727 | 0.0398
Epoch 189/300, trend Loss: 0.0727 | 0.0398
Epoch 190/300, trend Loss: 0.0727 | 0.0398
Epoch 191/300, trend Loss: 0.0727 | 0.0397
Epoch 192/300, trend Loss: 0.0727 | 0.0397
Epoch 193/300, trend Loss: 0.0726 | 0.0397
Epoch 194/300, trend Loss: 0.0726 | 0.0397
Epoch 195/300, trend Loss: 0.0726 | 0.0397
Epoch 196/300, trend Loss: 0.0726 | 0.0397
Epoch 197/300, trend Loss: 0.0726 | 0.0397
Epoch 198/300, trend Loss: 0.0726 | 0.0397
Epoch 199/300, trend Loss: 0.0726 | 0.0397
Epoch 200/300, trend Loss: 0.0726 | 0.0397
Epoch 201/300, trend Loss: 0.0726 | 0.0397
Epoch 202/300, trend Loss: 0.0726 | 0.0397
Epoch 203/300, trend Loss: 0.0726 | 0.0397
Epoch 204/300, trend Loss: 0.0726 | 0.0397
Epoch 205/300, trend Loss: 0.0726 | 0.0397
Epoch 206/300, trend Loss: 0.0726 | 0.0397
Epoch 207/300, trend Loss: 0.0726 | 0.0397
Epoch 208/300, trend Loss: 0.0726 | 0.0397
Epoch 209/300, trend Loss: 0.0726 | 0.0397
Epoch 210/300, trend Loss: 0.0726 | 0.0397
Epoch 211/300, trend Loss: 0.0726 | 0.0397
Epoch 212/300, trend Loss: 0.0726 | 0.0397
Epoch 213/300, trend Loss: 0.0726 | 0.0397
Epoch 214/300, trend Loss: 0.0726 | 0.0397
Epoch 215/300, trend Loss: 0.0726 | 0.0397
Epoch 216/300, trend Loss: 0.0726 | 0.0397
Epoch 217/300, trend Loss: 0.0726 | 0.0397
Epoch 218/300, trend Loss: 0.0726 | 0.0397
Epoch 219/300, trend Loss: 0.0726 | 0.0397
Epoch 220/300, trend Loss: 0.0726 | 0.0397
Epoch 221/300, trend Loss: 0.0726 | 0.0397
Epoch 222/300, trend Loss: 0.0726 | 0.0397
Epoch 223/300, trend Loss: 0.0726 | 0.0397
Epoch 224/300, trend Loss: 0.0726 | 0.0397
Epoch 225/300, trend Loss: 0.0726 | 0.0397
Epoch 226/300, trend Loss: 0.0726 | 0.0397
Epoch 227/300, trend Loss: 0.0726 | 0.0397
Epoch 228/300, trend Loss: 0.0726 | 0.0397
Epoch 229/300, trend Loss: 0.0726 | 0.0397
Epoch 230/300, trend Loss: 0.0726 | 0.0397
Epoch 231/300, trend Loss: 0.0726 | 0.0397
Epoch 232/300, trend Loss: 0.0726 | 0.0397
Epoch 233/300, trend Loss: 0.0726 | 0.0397
Epoch 234/300, trend Loss: 0.0726 | 0.0397
Epoch 235/300, trend Loss: 0.0726 | 0.0397
Epoch 236/300, trend Loss: 0.0726 | 0.0397
Epoch 237/300, trend Loss: 0.0726 | 0.0397
Epoch 238/300, trend Loss: 0.0726 | 0.0397
Epoch 239/300, trend Loss: 0.0726 | 0.0397
Epoch 240/300, trend Loss: 0.0726 | 0.0397
Epoch 241/300, trend Loss: 0.0726 | 0.0397
Epoch 242/300, trend Loss: 0.0726 | 0.0397
Epoch 243/300, trend Loss: 0.0726 | 0.0397
Epoch 244/300, trend Loss: 0.0726 | 0.0397
Epoch 245/300, trend Loss: 0.0726 | 0.0397
Epoch 246/300, trend Loss: 0.0726 | 0.0397
Epoch 247/300, trend Loss: 0.0726 | 0.0397
Epoch 248/300, trend Loss: 0.0726 | 0.0397
Epoch 249/300, trend Loss: 0.0726 | 0.0397
Epoch 250/300, trend Loss: 0.0726 | 0.0397
Epoch 251/300, trend Loss: 0.0726 | 0.0397
Epoch 252/300, trend Loss: 0.0726 | 0.0397
Epoch 253/300, trend Loss: 0.0726 | 0.0397
Epoch 254/300, trend Loss: 0.0726 | 0.0397
Epoch 255/300, trend Loss: 0.0726 | 0.0397
Epoch 256/300, trend Loss: 0.0726 | 0.0397
Epoch 257/300, trend Loss: 0.0726 | 0.0397
Epoch 258/300, trend Loss: 0.0726 | 0.0397
Epoch 259/300, trend Loss: 0.0726 | 0.0397
Epoch 260/300, trend Loss: 0.0726 | 0.0397
Epoch 261/300, trend Loss: 0.0726 | 0.0397
Epoch 262/300, trend Loss: 0.0726 | 0.0397
Epoch 263/300, trend Loss: 0.0726 | 0.0397
Epoch 264/300, trend Loss: 0.0726 | 0.0397
Epoch 265/300, trend Loss: 0.0726 | 0.0397
Epoch 266/300, trend Loss: 0.0726 | 0.0397
Epoch 267/300, trend Loss: 0.0726 | 0.0397
Epoch 268/300, trend Loss: 0.0726 | 0.0397
Epoch 269/300, trend Loss: 0.0726 | 0.0397
Epoch 270/300, trend Loss: 0.0726 | 0.0397
Epoch 271/300, trend Loss: 0.0726 | 0.0397
Epoch 272/300, trend Loss: 0.0726 | 0.0397
Epoch 273/300, trend Loss: 0.0726 | 0.0397
Epoch 274/300, trend Loss: 0.0726 | 0.0397
Epoch 275/300, trend Loss: 0.0726 | 0.0397
Epoch 276/300, trend Loss: 0.0726 | 0.0397
Epoch 277/300, trend Loss: 0.0726 | 0.0397
Epoch 278/300, trend Loss: 0.0726 | 0.0397
Epoch 279/300, trend Loss: 0.0726 | 0.0397
Epoch 280/300, trend Loss: 0.0726 | 0.0397
Epoch 281/300, trend Loss: 0.0726 | 0.0397
Epoch 282/300, trend Loss: 0.0726 | 0.0397
Epoch 283/300, trend Loss: 0.0726 | 0.0397
Epoch 284/300, trend Loss: 0.0726 | 0.0397
Epoch 285/300, trend Loss: 0.0726 | 0.0397
Epoch 286/300, trend Loss: 0.0726 | 0.0397
Epoch 287/300, trend Loss: 0.0726 | 0.0397
Epoch 288/300, trend Loss: 0.0726 | 0.0397
Epoch 289/300, trend Loss: 0.0726 | 0.0397
Epoch 290/300, trend Loss: 0.0726 | 0.0397
Epoch 291/300, trend Loss: 0.0726 | 0.0397
Epoch 292/300, trend Loss: 0.0726 | 0.0397
Epoch 293/300, trend Loss: 0.0726 | 0.0397
Epoch 294/300, trend Loss: 0.0726 | 0.0397
Epoch 295/300, trend Loss: 0.0726 | 0.0397
Epoch 296/300, trend Loss: 0.0726 | 0.0397
Epoch 297/300, trend Loss: 0.0726 | 0.0397
Epoch 298/300, trend Loss: 0.0726 | 0.0397
Epoch 299/300, trend Loss: 0.0726 | 0.0397
Epoch 300/300, trend Loss: 0.0726 | 0.0397
Training seasonal_0 component with params: {'observation_period_num': 30, 'train_rates': 0.9415348518106073, 'learning_rate': 0.0001530608378949736, 'batch_size': 29, 'step_size': 4, 'gamma': 0.9199537995553148}
Epoch 1/300, seasonal_0 Loss: 0.2657 | 0.1487
Epoch 2/300, seasonal_0 Loss: 0.1673 | 0.1066
Epoch 3/300, seasonal_0 Loss: 0.1406 | 0.0951
Epoch 4/300, seasonal_0 Loss: 0.1296 | 0.0896
Epoch 5/300, seasonal_0 Loss: 0.1208 | 0.0854
Epoch 6/300, seasonal_0 Loss: 0.1145 | 0.0828
Epoch 7/300, seasonal_0 Loss: 0.1100 | 0.0805
Epoch 8/300, seasonal_0 Loss: 0.1072 | 0.0789
Epoch 9/300, seasonal_0 Loss: 0.1063 | 0.0787
Epoch 10/300, seasonal_0 Loss: 0.1076 | 0.1010
Epoch 11/300, seasonal_0 Loss: 0.1097 | 0.0859
Epoch 12/300, seasonal_0 Loss: 0.1059 | 0.0807
Epoch 13/300, seasonal_0 Loss: 0.0981 | 0.0788
Epoch 14/300, seasonal_0 Loss: 0.0941 | 0.0795
Epoch 15/300, seasonal_0 Loss: 0.0913 | 0.0759
Epoch 16/300, seasonal_0 Loss: 0.0895 | 0.0753
Epoch 17/300, seasonal_0 Loss: 0.0880 | 0.0732
Epoch 18/300, seasonal_0 Loss: 0.0869 | 0.0724
Epoch 19/300, seasonal_0 Loss: 0.0857 | 0.0710
Epoch 20/300, seasonal_0 Loss: 0.0849 | 0.0703
Epoch 21/300, seasonal_0 Loss: 0.0839 | 0.0693
Epoch 22/300, seasonal_0 Loss: 0.0832 | 0.0687
Epoch 23/300, seasonal_0 Loss: 0.0821 | 0.0678
Epoch 24/300, seasonal_0 Loss: 0.0812 | 0.0672
Epoch 25/300, seasonal_0 Loss: 0.0800 | 0.0665
Epoch 26/300, seasonal_0 Loss: 0.0789 | 0.0659
Epoch 27/300, seasonal_0 Loss: 0.0780 | 0.0654
Epoch 28/300, seasonal_0 Loss: 0.0770 | 0.0649
Epoch 29/300, seasonal_0 Loss: 0.0762 | 0.0647
Epoch 30/300, seasonal_0 Loss: 0.0754 | 0.0645
Epoch 31/300, seasonal_0 Loss: 0.0747 | 0.0646
Epoch 32/300, seasonal_0 Loss: 0.0739 | 0.0646
Epoch 33/300, seasonal_0 Loss: 0.0733 | 0.0651
Epoch 34/300, seasonal_0 Loss: 0.0727 | 0.0654
Epoch 35/300, seasonal_0 Loss: 0.0721 | 0.0661
Epoch 36/300, seasonal_0 Loss: 0.0716 | 0.0667
Epoch 37/300, seasonal_0 Loss: 0.0712 | 0.0675
Epoch 38/300, seasonal_0 Loss: 0.0708 | 0.0683
Epoch 39/300, seasonal_0 Loss: 0.0705 | 0.0687
Epoch 40/300, seasonal_0 Loss: 0.0703 | 0.0693
Epoch 41/300, seasonal_0 Loss: 0.0702 | 0.0680
Epoch 42/300, seasonal_0 Loss: 0.0702 | 0.0673
Epoch 43/300, seasonal_0 Loss: 0.0701 | 0.0639
Epoch 44/300, seasonal_0 Loss: 0.0701 | 0.0626
Epoch 45/300, seasonal_0 Loss: 0.0697 | 0.0612
Epoch 46/300, seasonal_0 Loss: 0.0691 | 0.0604
Epoch 47/300, seasonal_0 Loss: 0.0680 | 0.0595
Epoch 48/300, seasonal_0 Loss: 0.0671 | 0.0588
Epoch 49/300, seasonal_0 Loss: 0.0664 | 0.0585
Epoch 50/300, seasonal_0 Loss: 0.0660 | 0.0583
Epoch 51/300, seasonal_0 Loss: 0.0656 | 0.0582
Epoch 52/300, seasonal_0 Loss: 0.0653 | 0.0581
Epoch 53/300, seasonal_0 Loss: 0.0651 | 0.0580
Epoch 54/300, seasonal_0 Loss: 0.0648 | 0.0579
Epoch 55/300, seasonal_0 Loss: 0.0646 | 0.0579
Epoch 56/300, seasonal_0 Loss: 0.0643 | 0.0577
Epoch 57/300, seasonal_0 Loss: 0.0641 | 0.0577
Epoch 58/300, seasonal_0 Loss: 0.0639 | 0.0576
Epoch 59/300, seasonal_0 Loss: 0.0637 | 0.0577
Epoch 60/300, seasonal_0 Loss: 0.0635 | 0.0575
Epoch 61/300, seasonal_0 Loss: 0.0634 | 0.0577
Epoch 62/300, seasonal_0 Loss: 0.0632 | 0.0575
Epoch 63/300, seasonal_0 Loss: 0.0631 | 0.0578
Epoch 64/300, seasonal_0 Loss: 0.0629 | 0.0576
Epoch 65/300, seasonal_0 Loss: 0.0628 | 0.0579
Epoch 66/300, seasonal_0 Loss: 0.0626 | 0.0578
Epoch 67/300, seasonal_0 Loss: 0.0625 | 0.0581
Epoch 68/300, seasonal_0 Loss: 0.0624 | 0.0580
Epoch 69/300, seasonal_0 Loss: 0.0623 | 0.0582
Epoch 70/300, seasonal_0 Loss: 0.0622 | 0.0581
Epoch 71/300, seasonal_0 Loss: 0.0621 | 0.0583
Epoch 72/300, seasonal_0 Loss: 0.0620 | 0.0581
Epoch 73/300, seasonal_0 Loss: 0.0620 | 0.0584
Epoch 74/300, seasonal_0 Loss: 0.0619 | 0.0583
Epoch 75/300, seasonal_0 Loss: 0.0618 | 0.0587
Epoch 76/300, seasonal_0 Loss: 0.0617 | 0.0587
Epoch 77/300, seasonal_0 Loss: 0.0616 | 0.0592
Epoch 78/300, seasonal_0 Loss: 0.0615 | 0.0592
Epoch 79/300, seasonal_0 Loss: 0.0614 | 0.0598
Epoch 80/300, seasonal_0 Loss: 0.0612 | 0.0598
Epoch 81/300, seasonal_0 Loss: 0.0611 | 0.0601
Epoch 82/300, seasonal_0 Loss: 0.0610 | 0.0601
Epoch 83/300, seasonal_0 Loss: 0.0608 | 0.0602
Epoch 84/300, seasonal_0 Loss: 0.0607 | 0.0601
Epoch 85/300, seasonal_0 Loss: 0.0606 | 0.0600
Epoch 86/300, seasonal_0 Loss: 0.0605 | 0.0599
Epoch 87/300, seasonal_0 Loss: 0.0604 | 0.0597
Epoch 88/300, seasonal_0 Loss: 0.0603 | 0.0596
Epoch 89/300, seasonal_0 Loss: 0.0602 | 0.0593
Epoch 90/300, seasonal_0 Loss: 0.0601 | 0.0593
Epoch 91/300, seasonal_0 Loss: 0.0600 | 0.0590
Epoch 92/300, seasonal_0 Loss: 0.0599 | 0.0590
Epoch 93/300, seasonal_0 Loss: 0.0599 | 0.0587
Epoch 94/300, seasonal_0 Loss: 0.0598 | 0.0587
Epoch 95/300, seasonal_0 Loss: 0.0597 | 0.0585
Epoch 96/300, seasonal_0 Loss: 0.0597 | 0.0585
Epoch 97/300, seasonal_0 Loss: 0.0596 | 0.0583
Epoch 98/300, seasonal_0 Loss: 0.0595 | 0.0583
Epoch 99/300, seasonal_0 Loss: 0.0595 | 0.0581
Epoch 100/300, seasonal_0 Loss: 0.0594 | 0.0581
Epoch 101/300, seasonal_0 Loss: 0.0594 | 0.0580
Epoch 102/300, seasonal_0 Loss: 0.0593 | 0.0580
Epoch 103/300, seasonal_0 Loss: 0.0593 | 0.0578
Epoch 104/300, seasonal_0 Loss: 0.0593 | 0.0578
Epoch 105/300, seasonal_0 Loss: 0.0592 | 0.0577
Epoch 106/300, seasonal_0 Loss: 0.0592 | 0.0577
Epoch 107/300, seasonal_0 Loss: 0.0591 | 0.0576
Epoch 108/300, seasonal_0 Loss: 0.0591 | 0.0576
Epoch 109/300, seasonal_0 Loss: 0.0591 | 0.0575
Epoch 110/300, seasonal_0 Loss: 0.0590 | 0.0575
Epoch 111/300, seasonal_0 Loss: 0.0590 | 0.0574
Epoch 112/300, seasonal_0 Loss: 0.0590 | 0.0574
Epoch 113/300, seasonal_0 Loss: 0.0589 | 0.0573
Epoch 114/300, seasonal_0 Loss: 0.0589 | 0.0573
Epoch 115/300, seasonal_0 Loss: 0.0589 | 0.0572
Epoch 116/300, seasonal_0 Loss: 0.0589 | 0.0572
Epoch 117/300, seasonal_0 Loss: 0.0588 | 0.0571
Epoch 118/300, seasonal_0 Loss: 0.0588 | 0.0571
Epoch 119/300, seasonal_0 Loss: 0.0588 | 0.0571
Epoch 120/300, seasonal_0 Loss: 0.0588 | 0.0570
Epoch 121/300, seasonal_0 Loss: 0.0587 | 0.0570
Epoch 122/300, seasonal_0 Loss: 0.0587 | 0.0570
Epoch 123/300, seasonal_0 Loss: 0.0587 | 0.0569
Epoch 124/300, seasonal_0 Loss: 0.0587 | 0.0569
Epoch 125/300, seasonal_0 Loss: 0.0587 | 0.0569
Epoch 126/300, seasonal_0 Loss: 0.0587 | 0.0569
Epoch 127/300, seasonal_0 Loss: 0.0586 | 0.0568
Epoch 128/300, seasonal_0 Loss: 0.0586 | 0.0568
Epoch 129/300, seasonal_0 Loss: 0.0586 | 0.0568
Epoch 130/300, seasonal_0 Loss: 0.0586 | 0.0568
Epoch 131/300, seasonal_0 Loss: 0.0586 | 0.0567
Epoch 132/300, seasonal_0 Loss: 0.0586 | 0.0567
Epoch 133/300, seasonal_0 Loss: 0.0586 | 0.0567
Epoch 134/300, seasonal_0 Loss: 0.0586 | 0.0567
Epoch 135/300, seasonal_0 Loss: 0.0585 | 0.0567
Epoch 136/300, seasonal_0 Loss: 0.0585 | 0.0567
Epoch 137/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 138/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 139/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 140/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 141/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 142/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 143/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 144/300, seasonal_0 Loss: 0.0585 | 0.0566
Epoch 145/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 146/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 147/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 148/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 149/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 150/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 151/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 152/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 153/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 154/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 155/300, seasonal_0 Loss: 0.0584 | 0.0565
Epoch 156/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 157/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 158/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 159/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 160/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 161/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 162/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 163/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 164/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 165/300, seasonal_0 Loss: 0.0584 | 0.0564
Epoch 166/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 167/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 168/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 169/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 170/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 171/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 172/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 173/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 174/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 175/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 176/300, seasonal_0 Loss: 0.0583 | 0.0564
Epoch 177/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 178/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 179/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 180/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 181/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 182/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 183/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 184/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 185/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 186/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 187/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 188/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 189/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 190/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 191/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 192/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 193/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 194/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 195/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 196/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 197/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 198/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 199/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 200/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 201/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 202/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 203/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 204/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 205/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 206/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 207/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 208/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 209/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 210/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 211/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 212/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 213/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 214/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 215/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 216/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 217/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 218/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 219/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 220/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 221/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 222/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 223/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 224/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 225/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 226/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 227/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 228/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 229/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 230/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 231/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 232/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 233/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 234/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 235/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 236/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 237/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 238/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 239/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 240/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 241/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 242/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 243/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 244/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 245/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 246/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 247/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 248/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 249/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 250/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 251/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 252/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 253/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 254/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 255/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 256/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 257/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 258/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 259/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 260/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 261/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 262/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 263/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 264/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 265/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 266/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 267/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 268/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 269/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 270/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 271/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 272/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 273/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 274/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 275/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 276/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 277/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 278/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 279/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 280/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 281/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 282/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 283/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 284/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 285/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 286/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 287/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 288/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 289/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 290/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 291/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 292/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 293/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 294/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 295/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 296/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 297/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 298/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 299/300, seasonal_0 Loss: 0.0583 | 0.0563
Epoch 300/300, seasonal_0 Loss: 0.0583 | 0.0563
Training seasonal_1 component with params: {'observation_period_num': 16, 'train_rates': 0.8998138978719316, 'learning_rate': 0.0006448861412147662, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8740297935643873}
Epoch 1/300, seasonal_1 Loss: 0.4174 | 0.2304
Epoch 2/300, seasonal_1 Loss: 0.1880 | 0.1554
Epoch 3/300, seasonal_1 Loss: 0.1903 | 0.1255
Epoch 4/300, seasonal_1 Loss: 0.1525 | 0.1025
Epoch 5/300, seasonal_1 Loss: 0.1424 | 0.1065
Epoch 6/300, seasonal_1 Loss: 0.1541 | 0.1318
Epoch 7/300, seasonal_1 Loss: 0.1704 | 0.2757
Epoch 8/300, seasonal_1 Loss: 0.1605 | 0.1572
Epoch 9/300, seasonal_1 Loss: 0.1478 | 0.0894
Epoch 10/300, seasonal_1 Loss: 0.1362 | 0.0792
Epoch 11/300, seasonal_1 Loss: 0.1302 | 0.1105
Epoch 12/300, seasonal_1 Loss: 0.1327 | 0.0844
Epoch 13/300, seasonal_1 Loss: 0.1139 | 0.0692
Epoch 14/300, seasonal_1 Loss: 0.1218 | 0.0775
Epoch 15/300, seasonal_1 Loss: 0.1087 | 0.0651
Epoch 16/300, seasonal_1 Loss: 0.1049 | 0.0664
Epoch 17/300, seasonal_1 Loss: 0.1003 | 0.0598
Epoch 18/300, seasonal_1 Loss: 0.0992 | 0.0599
Epoch 19/300, seasonal_1 Loss: 0.1005 | 0.0642
Epoch 20/300, seasonal_1 Loss: 0.1017 | 0.0622
Epoch 21/300, seasonal_1 Loss: 0.0993 | 0.0556
Epoch 22/300, seasonal_1 Loss: 0.0978 | 0.0572
Epoch 23/300, seasonal_1 Loss: 0.0980 | 0.0679
Epoch 24/300, seasonal_1 Loss: 0.0980 | 0.0795
Epoch 25/300, seasonal_1 Loss: 0.0949 | 0.0565
Epoch 26/300, seasonal_1 Loss: 0.0906 | 0.0514
Epoch 27/300, seasonal_1 Loss: 0.0915 | 0.0509
Epoch 28/300, seasonal_1 Loss: 0.0902 | 0.0499
Epoch 29/300, seasonal_1 Loss: 0.0887 | 0.0512
Epoch 30/300, seasonal_1 Loss: 0.0869 | 0.0542
Epoch 31/300, seasonal_1 Loss: 0.0873 | 0.0576
Epoch 32/300, seasonal_1 Loss: 0.0892 | 0.0549
Epoch 33/300, seasonal_1 Loss: 0.0883 | 0.0520
Epoch 34/300, seasonal_1 Loss: 0.0872 | 0.0545
Epoch 35/300, seasonal_1 Loss: 0.0870 | 0.0535
Epoch 36/300, seasonal_1 Loss: 0.0848 | 0.0489
Epoch 37/300, seasonal_1 Loss: 0.0829 | 0.0472
Epoch 38/300, seasonal_1 Loss: 0.0827 | 0.0481
Epoch 39/300, seasonal_1 Loss: 0.0823 | 0.0486
Epoch 40/300, seasonal_1 Loss: 0.0823 | 0.0472
Epoch 41/300, seasonal_1 Loss: 0.0816 | 0.0464
Epoch 42/300, seasonal_1 Loss: 0.0806 | 0.0455
Epoch 43/300, seasonal_1 Loss: 0.0801 | 0.0454
Epoch 44/300, seasonal_1 Loss: 0.0801 | 0.0455
Epoch 45/300, seasonal_1 Loss: 0.0800 | 0.0453
Epoch 46/300, seasonal_1 Loss: 0.0797 | 0.0453
Epoch 47/300, seasonal_1 Loss: 0.0794 | 0.0450
Epoch 48/300, seasonal_1 Loss: 0.0790 | 0.0447
Epoch 49/300, seasonal_1 Loss: 0.0787 | 0.0444
Epoch 50/300, seasonal_1 Loss: 0.0785 | 0.0441
Epoch 51/300, seasonal_1 Loss: 0.0783 | 0.0437
Epoch 52/300, seasonal_1 Loss: 0.0780 | 0.0436
Epoch 53/300, seasonal_1 Loss: 0.0777 | 0.0435
Epoch 54/300, seasonal_1 Loss: 0.0775 | 0.0432
Epoch 55/300, seasonal_1 Loss: 0.0773 | 0.0431
Epoch 56/300, seasonal_1 Loss: 0.0772 | 0.0429
Epoch 57/300, seasonal_1 Loss: 0.0770 | 0.0428
Epoch 58/300, seasonal_1 Loss: 0.0768 | 0.0427
Epoch 59/300, seasonal_1 Loss: 0.0767 | 0.0425
Epoch 60/300, seasonal_1 Loss: 0.0765 | 0.0423
Epoch 61/300, seasonal_1 Loss: 0.0764 | 0.0421
Epoch 62/300, seasonal_1 Loss: 0.0762 | 0.0420
Epoch 63/300, seasonal_1 Loss: 0.0761 | 0.0418
Epoch 64/300, seasonal_1 Loss: 0.0759 | 0.0417
Epoch 65/300, seasonal_1 Loss: 0.0758 | 0.0415
Epoch 66/300, seasonal_1 Loss: 0.0757 | 0.0414
Epoch 67/300, seasonal_1 Loss: 0.0756 | 0.0413
Epoch 68/300, seasonal_1 Loss: 0.0755 | 0.0412
Epoch 69/300, seasonal_1 Loss: 0.0754 | 0.0411
Epoch 70/300, seasonal_1 Loss: 0.0753 | 0.0410
Epoch 71/300, seasonal_1 Loss: 0.0752 | 0.0409
Epoch 72/300, seasonal_1 Loss: 0.0751 | 0.0409
Epoch 73/300, seasonal_1 Loss: 0.0750 | 0.0408
Epoch 74/300, seasonal_1 Loss: 0.0750 | 0.0407
Epoch 75/300, seasonal_1 Loss: 0.0749 | 0.0407
Epoch 76/300, seasonal_1 Loss: 0.0748 | 0.0406
Epoch 77/300, seasonal_1 Loss: 0.0748 | 0.0405
Epoch 78/300, seasonal_1 Loss: 0.0747 | 0.0405
Epoch 79/300, seasonal_1 Loss: 0.0746 | 0.0404
Epoch 80/300, seasonal_1 Loss: 0.0746 | 0.0404
Epoch 81/300, seasonal_1 Loss: 0.0745 | 0.0404
Epoch 82/300, seasonal_1 Loss: 0.0745 | 0.0403
Epoch 83/300, seasonal_1 Loss: 0.0745 | 0.0403
Epoch 84/300, seasonal_1 Loss: 0.0744 | 0.0402
Epoch 85/300, seasonal_1 Loss: 0.0744 | 0.0402
Epoch 86/300, seasonal_1 Loss: 0.0743 | 0.0402
Epoch 87/300, seasonal_1 Loss: 0.0743 | 0.0401
Epoch 88/300, seasonal_1 Loss: 0.0743 | 0.0401
Epoch 89/300, seasonal_1 Loss: 0.0742 | 0.0401
Epoch 90/300, seasonal_1 Loss: 0.0742 | 0.0401
Epoch 91/300, seasonal_1 Loss: 0.0742 | 0.0400
Epoch 92/300, seasonal_1 Loss: 0.0741 | 0.0400
Epoch 93/300, seasonal_1 Loss: 0.0741 | 0.0400
Epoch 94/300, seasonal_1 Loss: 0.0741 | 0.0400
Epoch 95/300, seasonal_1 Loss: 0.0741 | 0.0400
Epoch 96/300, seasonal_1 Loss: 0.0740 | 0.0399
Epoch 97/300, seasonal_1 Loss: 0.0740 | 0.0399
Epoch 98/300, seasonal_1 Loss: 0.0740 | 0.0399
Epoch 99/300, seasonal_1 Loss: 0.0740 | 0.0399
Epoch 100/300, seasonal_1 Loss: 0.0740 | 0.0399
Epoch 101/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 102/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 103/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 104/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 105/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 106/300, seasonal_1 Loss: 0.0739 | 0.0398
Epoch 107/300, seasonal_1 Loss: 0.0738 | 0.0398
Epoch 108/300, seasonal_1 Loss: 0.0738 | 0.0398
Epoch 109/300, seasonal_1 Loss: 0.0738 | 0.0398
Epoch 110/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 111/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 112/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 113/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 114/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 115/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 116/300, seasonal_1 Loss: 0.0738 | 0.0397
Epoch 117/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 118/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 119/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 120/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 121/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 122/300, seasonal_1 Loss: 0.0737 | 0.0397
Epoch 123/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 124/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 125/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 126/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 127/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 128/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 129/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 130/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 131/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 132/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 133/300, seasonal_1 Loss: 0.0737 | 0.0396
Epoch 134/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 135/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 136/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 137/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 138/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 139/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 140/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 141/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 142/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 143/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 144/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 145/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 146/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 147/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 148/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 149/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 150/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 151/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 152/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 153/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 154/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 155/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 156/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 157/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 158/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 159/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 160/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 161/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 162/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 163/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 164/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 165/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 166/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 167/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 168/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 169/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 170/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 171/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 172/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 173/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 174/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 175/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 176/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 177/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 178/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 179/300, seasonal_1 Loss: 0.0736 | 0.0396
Epoch 180/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 181/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 182/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 183/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 184/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 185/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 186/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 187/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 188/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 189/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 190/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 191/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 192/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 193/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 194/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 195/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 196/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 197/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 198/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 199/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 200/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 201/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 202/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 203/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 204/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 205/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 206/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 207/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 208/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 209/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 210/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 211/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 212/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 213/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 214/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 215/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 216/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 217/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 218/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 219/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 220/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 221/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 222/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 223/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 224/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 225/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 226/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 227/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 228/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 229/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 230/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 231/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 232/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 233/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 234/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 235/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 236/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 237/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 238/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 239/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 240/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 241/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 242/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 243/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 244/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 245/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 246/300, seasonal_1 Loss: 0.0736 | 0.0395
Epoch 247/300, seasonal_1 Loss: 0.0736 | 0.0395
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8469236748545818, 'learning_rate': 6.95012068121626e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8141398861230108}
Epoch 1/300, seasonal_2 Loss: 0.1711 | 0.1228
Epoch 2/300, seasonal_2 Loss: 0.1347 | 0.1206
Epoch 3/300, seasonal_2 Loss: 0.1264 | 0.1136
Epoch 4/300, seasonal_2 Loss: 0.1196 | 0.0898
Epoch 5/300, seasonal_2 Loss: 0.1134 | 0.0645
Epoch 6/300, seasonal_2 Loss: 0.1090 | 0.0643
Epoch 7/300, seasonal_2 Loss: 0.1057 | 0.0631
Epoch 8/300, seasonal_2 Loss: 0.1019 | 0.0590
Epoch 9/300, seasonal_2 Loss: 0.0992 | 0.0585
Epoch 10/300, seasonal_2 Loss: 0.0966 | 0.0583
Epoch 11/300, seasonal_2 Loss: 0.0945 | 0.0583
Epoch 12/300, seasonal_2 Loss: 0.0924 | 0.0587
Epoch 13/300, seasonal_2 Loss: 0.0909 | 0.0581
Epoch 14/300, seasonal_2 Loss: 0.0895 | 0.0578
Epoch 15/300, seasonal_2 Loss: 0.0883 | 0.0578
Epoch 16/300, seasonal_2 Loss: 0.0874 | 0.0571
Epoch 17/300, seasonal_2 Loss: 0.0865 | 0.0564
Epoch 18/300, seasonal_2 Loss: 0.0857 | 0.0558
Epoch 19/300, seasonal_2 Loss: 0.0849 | 0.0531
Epoch 20/300, seasonal_2 Loss: 0.0843 | 0.0525
Epoch 21/300, seasonal_2 Loss: 0.0837 | 0.0520
Epoch 22/300, seasonal_2 Loss: 0.0830 | 0.0481
Epoch 23/300, seasonal_2 Loss: 0.0826 | 0.0477
Epoch 24/300, seasonal_2 Loss: 0.0821 | 0.0474
Epoch 25/300, seasonal_2 Loss: 0.0817 | 0.0471
Epoch 26/300, seasonal_2 Loss: 0.0811 | 0.0440
Epoch 27/300, seasonal_2 Loss: 0.0808 | 0.0437
Epoch 28/300, seasonal_2 Loss: 0.0804 | 0.0435
Epoch 29/300, seasonal_2 Loss: 0.0799 | 0.0413
Epoch 30/300, seasonal_2 Loss: 0.0796 | 0.0411
Epoch 31/300, seasonal_2 Loss: 0.0793 | 0.0409
Epoch 32/300, seasonal_2 Loss: 0.0790 | 0.0407
Epoch 33/300, seasonal_2 Loss: 0.0786 | 0.0394
Epoch 34/300, seasonal_2 Loss: 0.0784 | 0.0392
Epoch 35/300, seasonal_2 Loss: 0.0781 | 0.0391
Epoch 36/300, seasonal_2 Loss: 0.0778 | 0.0383
Epoch 37/300, seasonal_2 Loss: 0.0776 | 0.0381
Epoch 38/300, seasonal_2 Loss: 0.0774 | 0.0380
Epoch 39/300, seasonal_2 Loss: 0.0772 | 0.0379
Epoch 40/300, seasonal_2 Loss: 0.0770 | 0.0373
Epoch 41/300, seasonal_2 Loss: 0.0768 | 0.0372
Epoch 42/300, seasonal_2 Loss: 0.0766 | 0.0371
Epoch 43/300, seasonal_2 Loss: 0.0764 | 0.0368
Epoch 44/300, seasonal_2 Loss: 0.0763 | 0.0367
Epoch 45/300, seasonal_2 Loss: 0.0761 | 0.0366
Epoch 46/300, seasonal_2 Loss: 0.0760 | 0.0364
Epoch 47/300, seasonal_2 Loss: 0.0758 | 0.0364
Epoch 48/300, seasonal_2 Loss: 0.0757 | 0.0363
Epoch 49/300, seasonal_2 Loss: 0.0756 | 0.0362
Epoch 50/300, seasonal_2 Loss: 0.0755 | 0.0362
Epoch 51/300, seasonal_2 Loss: 0.0754 | 0.0361
Epoch 52/300, seasonal_2 Loss: 0.0753 | 0.0360
Epoch 53/300, seasonal_2 Loss: 0.0752 | 0.0359
Epoch 54/300, seasonal_2 Loss: 0.0751 | 0.0360
Epoch 55/300, seasonal_2 Loss: 0.0750 | 0.0359
Epoch 56/300, seasonal_2 Loss: 0.0749 | 0.0358
Epoch 57/300, seasonal_2 Loss: 0.0749 | 0.0358
Epoch 58/300, seasonal_2 Loss: 0.0748 | 0.0358
Epoch 59/300, seasonal_2 Loss: 0.0748 | 0.0357
Epoch 60/300, seasonal_2 Loss: 0.0747 | 0.0356
Epoch 61/300, seasonal_2 Loss: 0.0746 | 0.0357
Epoch 62/300, seasonal_2 Loss: 0.0746 | 0.0357
Epoch 63/300, seasonal_2 Loss: 0.0745 | 0.0356
Epoch 64/300, seasonal_2 Loss: 0.0745 | 0.0357
Epoch 65/300, seasonal_2 Loss: 0.0745 | 0.0357
Epoch 66/300, seasonal_2 Loss: 0.0744 | 0.0357
Epoch 67/300, seasonal_2 Loss: 0.0744 | 0.0356
Epoch 68/300, seasonal_2 Loss: 0.0743 | 0.0358
Epoch 69/300, seasonal_2 Loss: 0.0743 | 0.0357
Epoch 70/300, seasonal_2 Loss: 0.0743 | 0.0357
Epoch 71/300, seasonal_2 Loss: 0.0743 | 0.0358
Epoch 72/300, seasonal_2 Loss: 0.0742 | 0.0358
Epoch 73/300, seasonal_2 Loss: 0.0742 | 0.0358
Epoch 74/300, seasonal_2 Loss: 0.0742 | 0.0357
Epoch 75/300, seasonal_2 Loss: 0.0741 | 0.0358
Epoch 76/300, seasonal_2 Loss: 0.0741 | 0.0358
Epoch 77/300, seasonal_2 Loss: 0.0741 | 0.0358
Epoch 78/300, seasonal_2 Loss: 0.0741 | 0.0359
Epoch 79/300, seasonal_2 Loss: 0.0740 | 0.0359
Epoch 80/300, seasonal_2 Loss: 0.0740 | 0.0358
Epoch 81/300, seasonal_2 Loss: 0.0740 | 0.0358
Epoch 82/300, seasonal_2 Loss: 0.0740 | 0.0359
Epoch 83/300, seasonal_2 Loss: 0.0740 | 0.0359
Epoch 84/300, seasonal_2 Loss: 0.0740 | 0.0359
Epoch 85/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 86/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 87/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 88/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 89/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 90/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 91/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 92/300, seasonal_2 Loss: 0.0739 | 0.0359
Epoch 93/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 94/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 95/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 96/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 97/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 98/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 99/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 100/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 101/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 102/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 103/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 104/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 105/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 106/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 107/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 108/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 109/300, seasonal_2 Loss: 0.0738 | 0.0359
Epoch 110/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 111/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 112/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 113/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 114/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 115/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 116/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 117/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 118/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 119/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 120/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 121/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 122/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 123/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 124/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 125/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 126/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 127/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 128/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 129/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 130/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 131/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 132/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 133/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 134/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 135/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 136/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 137/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 138/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 139/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 140/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 141/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 142/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 143/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 144/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 145/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 146/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 147/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 148/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 149/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 150/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 151/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 152/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 153/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 154/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 155/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 156/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 157/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 158/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 159/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 160/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 161/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 162/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 163/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 164/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 165/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 166/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 167/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 168/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 169/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 170/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 171/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 172/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 173/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 174/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 175/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 176/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 177/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 178/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 179/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 180/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 181/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 182/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 183/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 184/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 185/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 186/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 187/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 188/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 189/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 190/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 191/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 192/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 193/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 194/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 195/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 196/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 197/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 198/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 199/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 200/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 201/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 202/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 203/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 204/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 205/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 206/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 207/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 208/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 209/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 210/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 211/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 212/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 213/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 214/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 215/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 216/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 217/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 218/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 219/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 220/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 221/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 222/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 223/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 224/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 225/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 226/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 227/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 228/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 229/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 230/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 231/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 232/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 233/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 234/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 235/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 236/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 237/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 238/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 239/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 240/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 241/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 242/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 243/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 244/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 245/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 246/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 247/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 248/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 249/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 250/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 251/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 252/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 253/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 254/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 255/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 256/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 257/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 258/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 259/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 260/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 261/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 262/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 263/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 264/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 265/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 266/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 267/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 268/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 269/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 270/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 271/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 272/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 273/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 274/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 275/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 276/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 277/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 278/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 279/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 280/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 281/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 282/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 283/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 284/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 285/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 286/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 287/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 288/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 289/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 290/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 291/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 292/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 293/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 294/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 295/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 296/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 297/300, seasonal_2 Loss: 0.0737 | 0.0359
Epoch 298/300, seasonal_2 Loss: 0.0737 | 0.0359
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.8301117959609203, 'learning_rate': 0.0002367299444144094, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8204687213801174}
Epoch 1/300, seasonal_3 Loss: 0.4520 | 0.2094
Epoch 2/300, seasonal_3 Loss: 0.1655 | 0.0966
Epoch 3/300, seasonal_3 Loss: 0.1523 | 0.0789
Epoch 4/300, seasonal_3 Loss: 0.1354 | 0.0743
Epoch 5/300, seasonal_3 Loss: 0.1247 | 0.0768
Epoch 6/300, seasonal_3 Loss: 0.1190 | 0.0774
Epoch 7/300, seasonal_3 Loss: 0.1160 | 0.0664
Epoch 8/300, seasonal_3 Loss: 0.1159 | 0.0618
Epoch 9/300, seasonal_3 Loss: 0.1141 | 0.0593
Epoch 10/300, seasonal_3 Loss: 0.1118 | 0.0577
Epoch 11/300, seasonal_3 Loss: 0.1097 | 0.0569
Epoch 12/300, seasonal_3 Loss: 0.1079 | 0.0566
Epoch 13/300, seasonal_3 Loss: 0.1062 | 0.0545
Epoch 14/300, seasonal_3 Loss: 0.1052 | 0.0541
Epoch 15/300, seasonal_3 Loss: 0.1037 | 0.0553
Epoch 16/300, seasonal_3 Loss: 0.1016 | 0.0557
Epoch 17/300, seasonal_3 Loss: 0.0992 | 0.0528
Epoch 18/300, seasonal_3 Loss: 0.0969 | 0.0508
Epoch 19/300, seasonal_3 Loss: 0.0946 | 0.0541
Epoch 20/300, seasonal_3 Loss: 0.0938 | 0.0525
Epoch 21/300, seasonal_3 Loss: 0.0923 | 0.0535
Epoch 22/300, seasonal_3 Loss: 0.0917 | 0.0562
Epoch 23/300, seasonal_3 Loss: 0.0919 | 0.0597
Epoch 24/300, seasonal_3 Loss: 0.0926 | 0.0619
Epoch 25/300, seasonal_3 Loss: 0.0922 | 0.0557
Epoch 26/300, seasonal_3 Loss: 0.0923 | 0.0504
Epoch 27/300, seasonal_3 Loss: 0.0909 | 0.0496
Epoch 28/300, seasonal_3 Loss: 0.0909 | 0.0493
Epoch 29/300, seasonal_3 Loss: 0.0913 | 0.0483
Epoch 30/300, seasonal_3 Loss: 0.0915 | 0.0464
Epoch 31/300, seasonal_3 Loss: 0.0905 | 0.0424
Epoch 32/300, seasonal_3 Loss: 0.0908 | 0.0429
Epoch 33/300, seasonal_3 Loss: 0.0907 | 0.0430
Epoch 34/300, seasonal_3 Loss: 0.0898 | 0.0417
Epoch 35/300, seasonal_3 Loss: 0.0885 | 0.0411
Epoch 36/300, seasonal_3 Loss: 0.0874 | 0.0404
Epoch 37/300, seasonal_3 Loss: 0.0866 | 0.0398
Epoch 38/300, seasonal_3 Loss: 0.0859 | 0.0394
Epoch 39/300, seasonal_3 Loss: 0.0849 | 0.0392
Epoch 40/300, seasonal_3 Loss: 0.0842 | 0.0390
Epoch 41/300, seasonal_3 Loss: 0.0838 | 0.0388
Epoch 42/300, seasonal_3 Loss: 0.0835 | 0.0386
Epoch 43/300, seasonal_3 Loss: 0.0830 | 0.0389
Epoch 44/300, seasonal_3 Loss: 0.0826 | 0.0383
Epoch 45/300, seasonal_3 Loss: 0.0820 | 0.0378
Epoch 46/300, seasonal_3 Loss: 0.0816 | 0.0374
Epoch 47/300, seasonal_3 Loss: 0.0812 | 0.0371
Epoch 48/300, seasonal_3 Loss: 0.0809 | 0.0369
Epoch 49/300, seasonal_3 Loss: 0.0805 | 0.0368
Epoch 50/300, seasonal_3 Loss: 0.0802 | 0.0364
Epoch 51/300, seasonal_3 Loss: 0.0799 | 0.0361
Epoch 52/300, seasonal_3 Loss: 0.0796 | 0.0358
Epoch 53/300, seasonal_3 Loss: 0.0793 | 0.0356
Epoch 54/300, seasonal_3 Loss: 0.0790 | 0.0354
Epoch 55/300, seasonal_3 Loss: 0.0786 | 0.0352
Epoch 56/300, seasonal_3 Loss: 0.0784 | 0.0350
Epoch 57/300, seasonal_3 Loss: 0.0781 | 0.0348
Epoch 58/300, seasonal_3 Loss: 0.0779 | 0.0346
Epoch 59/300, seasonal_3 Loss: 0.0776 | 0.0344
Epoch 60/300, seasonal_3 Loss: 0.0774 | 0.0343
Epoch 61/300, seasonal_3 Loss: 0.0771 | 0.0342
Epoch 62/300, seasonal_3 Loss: 0.0770 | 0.0340
Epoch 63/300, seasonal_3 Loss: 0.0768 | 0.0339
Epoch 64/300, seasonal_3 Loss: 0.0766 | 0.0338
Epoch 65/300, seasonal_3 Loss: 0.0765 | 0.0337
Epoch 66/300, seasonal_3 Loss: 0.0763 | 0.0336
Epoch 67/300, seasonal_3 Loss: 0.0761 | 0.0335
Epoch 68/300, seasonal_3 Loss: 0.0761 | 0.0334
Epoch 69/300, seasonal_3 Loss: 0.0759 | 0.0333
Epoch 70/300, seasonal_3 Loss: 0.0758 | 0.0332
Epoch 71/300, seasonal_3 Loss: 0.0757 | 0.0330
Epoch 72/300, seasonal_3 Loss: 0.0756 | 0.0329
Epoch 73/300, seasonal_3 Loss: 0.0754 | 0.0329
Epoch 74/300, seasonal_3 Loss: 0.0753 | 0.0328
Epoch 75/300, seasonal_3 Loss: 0.0752 | 0.0327
Epoch 76/300, seasonal_3 Loss: 0.0751 | 0.0326
Epoch 77/300, seasonal_3 Loss: 0.0750 | 0.0325
Epoch 78/300, seasonal_3 Loss: 0.0749 | 0.0324
Epoch 79/300, seasonal_3 Loss: 0.0748 | 0.0324
Epoch 80/300, seasonal_3 Loss: 0.0747 | 0.0323
Epoch 81/300, seasonal_3 Loss: 0.0746 | 0.0322
Epoch 82/300, seasonal_3 Loss: 0.0745 | 0.0321
Epoch 83/300, seasonal_3 Loss: 0.0744 | 0.0320
Epoch 84/300, seasonal_3 Loss: 0.0743 | 0.0320
Epoch 85/300, seasonal_3 Loss: 0.0742 | 0.0320
Epoch 86/300, seasonal_3 Loss: 0.0741 | 0.0319
Epoch 87/300, seasonal_3 Loss: 0.0741 | 0.0319
Epoch 88/300, seasonal_3 Loss: 0.0740 | 0.0318
Epoch 89/300, seasonal_3 Loss: 0.0739 | 0.0317
Epoch 90/300, seasonal_3 Loss: 0.0738 | 0.0317
Epoch 91/300, seasonal_3 Loss: 0.0737 | 0.0317
Epoch 92/300, seasonal_3 Loss: 0.0737 | 0.0316
Epoch 93/300, seasonal_3 Loss: 0.0737 | 0.0316
Epoch 94/300, seasonal_3 Loss: 0.0737 | 0.0315
Epoch 95/300, seasonal_3 Loss: 0.0736 | 0.0315
Epoch 96/300, seasonal_3 Loss: 0.0736 | 0.0314
Epoch 97/300, seasonal_3 Loss: 0.0735 | 0.0314
Epoch 98/300, seasonal_3 Loss: 0.0737 | 0.0315
Epoch 99/300, seasonal_3 Loss: 0.0737 | 0.0315
Epoch 100/300, seasonal_3 Loss: 0.0736 | 0.0315
Epoch 101/300, seasonal_3 Loss: 0.0736 | 0.0315
Epoch 102/300, seasonal_3 Loss: 0.0734 | 0.0315
Epoch 103/300, seasonal_3 Loss: 0.0732 | 0.0314
Epoch 104/300, seasonal_3 Loss: 0.0731 | 0.0313
Epoch 105/300, seasonal_3 Loss: 0.0730 | 0.0313
Epoch 106/300, seasonal_3 Loss: 0.0729 | 0.0313
Epoch 107/300, seasonal_3 Loss: 0.0728 | 0.0313
Epoch 108/300, seasonal_3 Loss: 0.0727 | 0.0312
Epoch 109/300, seasonal_3 Loss: 0.0726 | 0.0311
Epoch 110/300, seasonal_3 Loss: 0.0725 | 0.0311
Epoch 111/300, seasonal_3 Loss: 0.0725 | 0.0311
Epoch 112/300, seasonal_3 Loss: 0.0724 | 0.0311
Epoch 113/300, seasonal_3 Loss: 0.0724 | 0.0311
Epoch 114/300, seasonal_3 Loss: 0.0723 | 0.0310
Epoch 115/300, seasonal_3 Loss: 0.0723 | 0.0310
Epoch 116/300, seasonal_3 Loss: 0.0722 | 0.0310
Epoch 117/300, seasonal_3 Loss: 0.0722 | 0.0310
Epoch 118/300, seasonal_3 Loss: 0.0722 | 0.0309
Epoch 119/300, seasonal_3 Loss: 0.0721 | 0.0309
Epoch 120/300, seasonal_3 Loss: 0.0721 | 0.0309
Epoch 121/300, seasonal_3 Loss: 0.0721 | 0.0309
Epoch 122/300, seasonal_3 Loss: 0.0720 | 0.0309
Epoch 123/300, seasonal_3 Loss: 0.0720 | 0.0309
Epoch 124/300, seasonal_3 Loss: 0.0720 | 0.0308
Epoch 125/300, seasonal_3 Loss: 0.0720 | 0.0308
Epoch 126/300, seasonal_3 Loss: 0.0720 | 0.0308
Epoch 127/300, seasonal_3 Loss: 0.0719 | 0.0308
Epoch 128/300, seasonal_3 Loss: 0.0719 | 0.0308
Epoch 129/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 130/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 131/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 132/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 133/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 134/300, seasonal_3 Loss: 0.0719 | 0.0307
Epoch 135/300, seasonal_3 Loss: 0.0718 | 0.0307
Epoch 136/300, seasonal_3 Loss: 0.0718 | 0.0306
Epoch 137/300, seasonal_3 Loss: 0.0718 | 0.0306
Epoch 138/300, seasonal_3 Loss: 0.0717 | 0.0306
Epoch 139/300, seasonal_3 Loss: 0.0717 | 0.0306
Epoch 140/300, seasonal_3 Loss: 0.0717 | 0.0306
Epoch 141/300, seasonal_3 Loss: 0.0717 | 0.0306
Epoch 142/300, seasonal_3 Loss: 0.0716 | 0.0306
Epoch 143/300, seasonal_3 Loss: 0.0716 | 0.0306
Epoch 144/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 145/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 146/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 147/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 148/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 149/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 150/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 151/300, seasonal_3 Loss: 0.0716 | 0.0305
Epoch 152/300, seasonal_3 Loss: 0.0715 | 0.0305
Epoch 153/300, seasonal_3 Loss: 0.0715 | 0.0305
Epoch 154/300, seasonal_3 Loss: 0.0715 | 0.0305
Epoch 155/300, seasonal_3 Loss: 0.0715 | 0.0305
Epoch 156/300, seasonal_3 Loss: 0.0715 | 0.0305
Epoch 157/300, seasonal_3 Loss: 0.0714 | 0.0305
Epoch 158/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 159/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 160/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 161/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 162/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 163/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 164/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 165/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 166/300, seasonal_3 Loss: 0.0714 | 0.0304
Epoch 167/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 168/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 169/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 170/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 171/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 172/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 173/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 174/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 175/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 176/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 177/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 178/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 179/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 180/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 181/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 182/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 183/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 184/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 185/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 186/300, seasonal_3 Loss: 0.0713 | 0.0304
Epoch 187/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 188/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 189/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 190/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 191/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 192/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 193/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 194/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 195/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 196/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 197/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 198/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 199/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 200/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 201/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 202/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 203/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 204/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 205/300, seasonal_3 Loss: 0.0712 | 0.0304
Epoch 206/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 207/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 208/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 209/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 210/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 211/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 212/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 213/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 214/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 215/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 216/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 217/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 218/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 219/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 220/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 221/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 222/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 223/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 224/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 225/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 226/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 227/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 228/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 229/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 230/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 231/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 232/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 233/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 234/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 235/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 236/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 237/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 238/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 239/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 240/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 241/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 242/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 243/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 244/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 245/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 246/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 247/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 248/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 249/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 250/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 251/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 252/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 253/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 254/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 255/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 256/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 257/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 258/300, seasonal_3 Loss: 0.0712 | 0.0303
Epoch 259/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 260/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 261/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 262/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 263/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 264/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 265/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 266/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 267/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 268/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 269/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 270/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 271/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 272/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 273/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 274/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 275/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 276/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 277/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 278/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 279/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 280/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 281/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 282/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 283/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 284/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 285/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 286/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 287/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 288/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 289/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 290/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 291/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 292/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 293/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 294/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 295/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 296/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 297/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 298/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 299/300, seasonal_3 Loss: 0.0711 | 0.0303
Epoch 300/300, seasonal_3 Loss: 0.0711 | 0.0303
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9524756977260238, 'learning_rate': 0.00024507610243632564, 'batch_size': 21, 'step_size': 3, 'gamma': 0.8422050181926662}
Epoch 1/300, resid Loss: 0.1627 | 0.0869
Epoch 2/300, resid Loss: 0.1177 | 0.0806
Epoch 3/300, resid Loss: 0.1081 | 0.0676
Epoch 4/300, resid Loss: 0.0996 | 0.0598
Epoch 5/300, resid Loss: 0.0939 | 0.0554
Epoch 6/300, resid Loss: 0.0889 | 0.0504
Epoch 7/300, resid Loss: 0.0854 | 0.0481
Epoch 8/300, resid Loss: 0.0832 | 0.0469
Epoch 9/300, resid Loss: 0.0810 | 0.0447
Epoch 10/300, resid Loss: 0.0787 | 0.0432
Epoch 11/300, resid Loss: 0.0768 | 0.0417
Epoch 12/300, resid Loss: 0.0752 | 0.0410
Epoch 13/300, resid Loss: 0.0738 | 0.0404
Epoch 14/300, resid Loss: 0.0729 | 0.0397
Epoch 15/300, resid Loss: 0.0720 | 0.0397
Epoch 16/300, resid Loss: 0.0711 | 0.0396
Epoch 17/300, resid Loss: 0.0705 | 0.0391
Epoch 18/300, resid Loss: 0.0699 | 0.0386
Epoch 19/300, resid Loss: 0.0695 | 0.0379
Epoch 20/300, resid Loss: 0.0691 | 0.0378
Epoch 21/300, resid Loss: 0.0688 | 0.0374
Epoch 22/300, resid Loss: 0.0687 | 0.0373
Epoch 23/300, resid Loss: 0.0684 | 0.0373
Epoch 24/300, resid Loss: 0.0682 | 0.0371
Epoch 25/300, resid Loss: 0.0680 | 0.0371
Epoch 26/300, resid Loss: 0.0678 | 0.0370
Epoch 27/300, resid Loss: 0.0676 | 0.0372
Epoch 28/300, resid Loss: 0.0674 | 0.0373
Epoch 29/300, resid Loss: 0.0673 | 0.0371
Epoch 30/300, resid Loss: 0.0671 | 0.0370
Epoch 31/300, resid Loss: 0.0670 | 0.0369
Epoch 32/300, resid Loss: 0.0669 | 0.0368
Epoch 33/300, resid Loss: 0.0667 | 0.0366
Epoch 34/300, resid Loss: 0.0666 | 0.0365
Epoch 35/300, resid Loss: 0.0665 | 0.0365
Epoch 36/300, resid Loss: 0.0665 | 0.0364
Epoch 37/300, resid Loss: 0.0664 | 0.0363
Epoch 38/300, resid Loss: 0.0664 | 0.0362
Epoch 39/300, resid Loss: 0.0663 | 0.0362
Epoch 40/300, resid Loss: 0.0663 | 0.0362
Epoch 41/300, resid Loss: 0.0662 | 0.0362
Epoch 42/300, resid Loss: 0.0662 | 0.0362
Epoch 43/300, resid Loss: 0.0662 | 0.0362
Epoch 44/300, resid Loss: 0.0662 | 0.0362
Epoch 45/300, resid Loss: 0.0661 | 0.0362
Epoch 46/300, resid Loss: 0.0661 | 0.0363
Epoch 47/300, resid Loss: 0.0661 | 0.0362
Epoch 48/300, resid Loss: 0.0660 | 0.0362
Epoch 49/300, resid Loss: 0.0660 | 0.0362
Epoch 50/300, resid Loss: 0.0660 | 0.0362
Epoch 51/300, resid Loss: 0.0660 | 0.0362
Epoch 52/300, resid Loss: 0.0659 | 0.0362
Epoch 53/300, resid Loss: 0.0659 | 0.0362
Epoch 54/300, resid Loss: 0.0659 | 0.0362
Epoch 55/300, resid Loss: 0.0659 | 0.0362
Epoch 56/300, resid Loss: 0.0659 | 0.0362
Epoch 57/300, resid Loss: 0.0658 | 0.0362
Epoch 58/300, resid Loss: 0.0658 | 0.0362
Epoch 59/300, resid Loss: 0.0658 | 0.0362
Epoch 60/300, resid Loss: 0.0658 | 0.0362
Epoch 61/300, resid Loss: 0.0658 | 0.0362
Epoch 62/300, resid Loss: 0.0658 | 0.0362
Epoch 63/300, resid Loss: 0.0658 | 0.0362
Epoch 64/300, resid Loss: 0.0658 | 0.0362
Epoch 65/300, resid Loss: 0.0658 | 0.0362
Epoch 66/300, resid Loss: 0.0658 | 0.0362
Epoch 67/300, resid Loss: 0.0658 | 0.0362
Epoch 68/300, resid Loss: 0.0658 | 0.0362
Epoch 69/300, resid Loss: 0.0658 | 0.0362
Epoch 70/300, resid Loss: 0.0658 | 0.0362
Epoch 71/300, resid Loss: 0.0657 | 0.0362
Epoch 72/300, resid Loss: 0.0657 | 0.0362
Epoch 73/300, resid Loss: 0.0657 | 0.0362
Epoch 74/300, resid Loss: 0.0657 | 0.0362
Epoch 75/300, resid Loss: 0.0657 | 0.0362
Epoch 76/300, resid Loss: 0.0657 | 0.0362
Epoch 77/300, resid Loss: 0.0657 | 0.0362
Epoch 78/300, resid Loss: 0.0657 | 0.0362
Epoch 79/300, resid Loss: 0.0657 | 0.0362
Epoch 80/300, resid Loss: 0.0657 | 0.0362
Epoch 81/300, resid Loss: 0.0657 | 0.0362
Epoch 82/300, resid Loss: 0.0657 | 0.0362
Epoch 83/300, resid Loss: 0.0657 | 0.0362
Epoch 84/300, resid Loss: 0.0657 | 0.0362
Epoch 85/300, resid Loss: 0.0657 | 0.0362
Epoch 86/300, resid Loss: 0.0657 | 0.0362
Epoch 87/300, resid Loss: 0.0657 | 0.0362
Epoch 88/300, resid Loss: 0.0657 | 0.0362
Epoch 89/300, resid Loss: 0.0657 | 0.0362
Epoch 90/300, resid Loss: 0.0657 | 0.0362
Epoch 91/300, resid Loss: 0.0657 | 0.0362
Epoch 92/300, resid Loss: 0.0657 | 0.0362
Epoch 93/300, resid Loss: 0.0657 | 0.0362
Epoch 94/300, resid Loss: 0.0657 | 0.0362
Epoch 95/300, resid Loss: 0.0657 | 0.0362
Epoch 96/300, resid Loss: 0.0657 | 0.0362
Epoch 97/300, resid Loss: 0.0657 | 0.0362
Epoch 98/300, resid Loss: 0.0657 | 0.0362
Epoch 99/300, resid Loss: 0.0657 | 0.0362
Epoch 100/300, resid Loss: 0.0657 | 0.0362
Epoch 101/300, resid Loss: 0.0657 | 0.0362
Epoch 102/300, resid Loss: 0.0657 | 0.0362
Epoch 103/300, resid Loss: 0.0657 | 0.0362
Epoch 104/300, resid Loss: 0.0657 | 0.0362
Epoch 105/300, resid Loss: 0.0657 | 0.0362
Epoch 106/300, resid Loss: 0.0657 | 0.0362
Epoch 107/300, resid Loss: 0.0657 | 0.0362
Epoch 108/300, resid Loss: 0.0657 | 0.0362
Epoch 109/300, resid Loss: 0.0657 | 0.0362
Epoch 110/300, resid Loss: 0.0657 | 0.0362
Epoch 111/300, resid Loss: 0.0657 | 0.0362
Epoch 112/300, resid Loss: 0.0657 | 0.0362
Epoch 113/300, resid Loss: 0.0657 | 0.0362
Epoch 114/300, resid Loss: 0.0657 | 0.0362
Epoch 115/300, resid Loss: 0.0657 | 0.0362
Epoch 116/300, resid Loss: 0.0657 | 0.0362
Epoch 117/300, resid Loss: 0.0657 | 0.0362
Epoch 118/300, resid Loss: 0.0657 | 0.0362
Epoch 119/300, resid Loss: 0.0657 | 0.0362
Epoch 120/300, resid Loss: 0.0657 | 0.0362
Epoch 121/300, resid Loss: 0.0657 | 0.0362
Epoch 122/300, resid Loss: 0.0657 | 0.0362
Epoch 123/300, resid Loss: 0.0657 | 0.0362
Epoch 124/300, resid Loss: 0.0657 | 0.0362
Epoch 125/300, resid Loss: 0.0657 | 0.0362
Epoch 126/300, resid Loss: 0.0657 | 0.0362
Epoch 127/300, resid Loss: 0.0657 | 0.0362
Epoch 128/300, resid Loss: 0.0657 | 0.0362
Epoch 129/300, resid Loss: 0.0657 | 0.0362
Epoch 130/300, resid Loss: 0.0657 | 0.0362
Epoch 131/300, resid Loss: 0.0657 | 0.0362
Epoch 132/300, resid Loss: 0.0657 | 0.0362
Epoch 133/300, resid Loss: 0.0657 | 0.0362
Epoch 134/300, resid Loss: 0.0657 | 0.0362
Epoch 135/300, resid Loss: 0.0657 | 0.0362
Epoch 136/300, resid Loss: 0.0657 | 0.0362
Epoch 137/300, resid Loss: 0.0657 | 0.0362
Epoch 138/300, resid Loss: 0.0657 | 0.0362
Epoch 139/300, resid Loss: 0.0657 | 0.0362
Epoch 140/300, resid Loss: 0.0657 | 0.0362
Epoch 141/300, resid Loss: 0.0657 | 0.0362
Epoch 142/300, resid Loss: 0.0657 | 0.0362
Epoch 143/300, resid Loss: 0.0657 | 0.0362
Epoch 144/300, resid Loss: 0.0657 | 0.0362
Epoch 145/300, resid Loss: 0.0657 | 0.0362
Epoch 146/300, resid Loss: 0.0657 | 0.0362
Epoch 147/300, resid Loss: 0.0657 | 0.0362
Epoch 148/300, resid Loss: 0.0657 | 0.0362
Epoch 149/300, resid Loss: 0.0657 | 0.0362
Epoch 150/300, resid Loss: 0.0657 | 0.0362
Epoch 151/300, resid Loss: 0.0657 | 0.0362
Epoch 152/300, resid Loss: 0.0657 | 0.0362
Epoch 153/300, resid Loss: 0.0657 | 0.0362
Epoch 154/300, resid Loss: 0.0657 | 0.0362
Epoch 155/300, resid Loss: 0.0657 | 0.0362
Epoch 156/300, resid Loss: 0.0657 | 0.0362
Epoch 157/300, resid Loss: 0.0657 | 0.0362
Early stopping for resid
Runtime (seconds): 2332.189976453781
0.0003204139231581046
[189.09103]
[-1.3212879]
[-0.93137026]
[0.53103733]
[-1.0481905]
[11.729374]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 96.81381686241366
RMSE: 9.839401245117188
MAE: 9.839401245117188
R-squared: nan
[198.0506]
