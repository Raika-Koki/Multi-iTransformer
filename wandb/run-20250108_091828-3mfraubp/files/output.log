[32m[I 2025-01-08 09:18:29,966][0m A new study created in memory with name: no-name-1a984729-75d6-49a5-9883-2b3734c02391[0m
[32m[I 2025-01-08 09:22:50,636][0m Trial 0 finished with value: 0.33490724796202126 and parameters: {'observation_period_num': 168, 'train_rates': 0.8538930067137223, 'learning_rate': 2.342349209778947e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8407709305339187}. Best is trial 0 with value: 0.33490724796202126.[0m
[32m[I 2025-01-08 09:24:37,990][0m Trial 1 finished with value: 0.16555579219545638 and parameters: {'observation_period_num': 76, 'train_rates': 0.9468323513692398, 'learning_rate': 0.00018019842687727234, 'batch_size': 106, 'step_size': 9, 'gamma': 0.975041245107649}. Best is trial 1 with value: 0.16555579219545638.[0m
Early stopping at epoch 86
[32m[I 2025-01-08 09:26:19,160][0m Trial 2 finished with value: 0.5195699452065435 and parameters: {'observation_period_num': 95, 'train_rates': 0.8151659390182031, 'learning_rate': 0.000539534755446171, 'batch_size': 153, 'step_size': 1, 'gamma': 0.8487352435960621}. Best is trial 1 with value: 0.16555579219545638.[0m
[32m[I 2025-01-08 09:27:14,431][0m Trial 3 finished with value: 0.15730328857898712 and parameters: {'observation_period_num': 39, 'train_rates': 0.9880124602581107, 'learning_rate': 0.0001641553673922329, 'batch_size': 200, 'step_size': 13, 'gamma': 0.9325642065828778}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:30:59,152][0m Trial 4 finished with value: 0.8485254549816863 and parameters: {'observation_period_num': 182, 'train_rates': 0.7383701045126932, 'learning_rate': 5.9159818167038995e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.8424665292343514}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:31:29,848][0m Trial 5 finished with value: 2.1223342118950903 and parameters: {'observation_period_num': 25, 'train_rates': 0.6424518014593646, 'learning_rate': 1.1547482536226316e-06, 'batch_size': 165, 'step_size': 4, 'gamma': 0.8600645550254944}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:34:32,168][0m Trial 6 finished with value: 0.9183643675977405 and parameters: {'observation_period_num': 148, 'train_rates': 0.7604736620142594, 'learning_rate': 0.0009069504576332162, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9646003530661089}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:38:09,814][0m Trial 7 finished with value: 1.2378275394439697 and parameters: {'observation_period_num': 150, 'train_rates': 0.9637355625486728, 'learning_rate': 1.051036197547377e-06, 'batch_size': 196, 'step_size': 5, 'gamma': 0.954086427712612}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:41:37,081][0m Trial 8 finished with value: 0.5070864426347014 and parameters: {'observation_period_num': 165, 'train_rates': 0.7770688972147085, 'learning_rate': 0.0002723083893093149, 'batch_size': 251, 'step_size': 8, 'gamma': 0.8671120041925033}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:42:09,650][0m Trial 9 finished with value: 0.7285805965578833 and parameters: {'observation_period_num': 11, 'train_rates': 0.7094944061362826, 'learning_rate': 1.7433152603771434e-05, 'batch_size': 112, 'step_size': 9, 'gamma': 0.9683876653945952}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:48:18,262][0m Trial 10 finished with value: 0.5795367670059204 and parameters: {'observation_period_num': 248, 'train_rates': 0.8900883005225271, 'learning_rate': 6.986033408137158e-06, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7555937140587073}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:50:00,336][0m Trial 11 finished with value: 0.19950810074806213 and parameters: {'observation_period_num': 72, 'train_rates': 0.9824654271674174, 'learning_rate': 0.00013594214421663116, 'batch_size': 100, 'step_size': 12, 'gamma': 0.9205933000724503}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:51:46,234][0m Trial 12 finished with value: 0.16898660695082263 and parameters: {'observation_period_num': 72, 'train_rates': 0.9213647542776319, 'learning_rate': 0.00014805041330371874, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9210042709255583}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:52:44,857][0m Trial 13 finished with value: 0.18118526538213095 and parameters: {'observation_period_num': 43, 'train_rates': 0.9262357008494576, 'learning_rate': 6.724512839817955e-05, 'batch_size': 132, 'step_size': 15, 'gamma': 0.918111641191785}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:55:16,354][0m Trial 14 finished with value: 0.17137086391448975 and parameters: {'observation_period_num': 109, 'train_rates': 0.9891954544366175, 'learning_rate': 0.00030504797005826567, 'batch_size': 201, 'step_size': 11, 'gamma': 0.9863565108151894}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:56:20,952][0m Trial 15 finished with value: 0.28753864721870886 and parameters: {'observation_period_num': 51, 'train_rates': 0.8587809104632649, 'learning_rate': 8.762713676459639e-05, 'batch_size': 160, 'step_size': 10, 'gamma': 0.9037689196483655}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 09:58:55,455][0m Trial 16 finished with value: 0.17527897514795002 and parameters: {'observation_period_num': 111, 'train_rates': 0.9336532219820649, 'learning_rate': 0.00027931987481957793, 'batch_size': 66, 'step_size': 7, 'gamma': 0.9442270054902122}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:00:28,723][0m Trial 17 finished with value: 0.4615625711212381 and parameters: {'observation_period_num': 74, 'train_rates': 0.8230982869511572, 'learning_rate': 8.536332880968981e-06, 'batch_size': 181, 'step_size': 13, 'gamma': 0.989090867002661}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:01:21,360][0m Trial 18 finished with value: 0.33237826783914826 and parameters: {'observation_period_num': 41, 'train_rates': 0.8993164078085034, 'learning_rate': 3.97663996070464e-05, 'batch_size': 226, 'step_size': 3, 'gamma': 0.8899521093176876}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:01:56,189][0m Trial 19 finished with value: 1.032039371852217 and parameters: {'observation_period_num': 5, 'train_rates': 0.9513272837410136, 'learning_rate': 0.0009232075080195707, 'batch_size': 123, 'step_size': 14, 'gamma': 0.9405892801341977}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:04:05,568][0m Trial 20 finished with value: 0.8462780739353821 and parameters: {'observation_period_num': 127, 'train_rates': 0.6028186530820021, 'learning_rate': 0.00014095345038657504, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8075517503225628}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:05:54,870][0m Trial 21 finished with value: 0.1698567073441407 and parameters: {'observation_period_num': 73, 'train_rates': 0.9230570255039983, 'learning_rate': 0.00011673208601973435, 'batch_size': 56, 'step_size': 12, 'gamma': 0.926949489854922}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:08:14,021][0m Trial 22 finished with value: 0.27163695950399747 and parameters: {'observation_period_num': 85, 'train_rates': 0.8782820303567432, 'learning_rate': 0.0001983421795836314, 'batch_size': 32, 'step_size': 11, 'gamma': 0.8909574317324683}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:09:52,108][0m Trial 23 finished with value: 0.8540135513652455 and parameters: {'observation_period_num': 56, 'train_rates': 0.9587779997236381, 'learning_rate': 0.00045119852107762976, 'batch_size': 48, 'step_size': 13, 'gamma': 0.9698068386488972}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:10:51,705][0m Trial 24 finished with value: 0.15916190573768893 and parameters: {'observation_period_num': 37, 'train_rates': 0.9067337037525864, 'learning_rate': 5.0797304261805267e-05, 'batch_size': 88, 'step_size': 11, 'gamma': 0.9341450162894882}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:11:51,443][0m Trial 25 finished with value: 0.17888876795768738 and parameters: {'observation_period_num': 28, 'train_rates': 0.98462510155786, 'learning_rate': 4.0370017869642535e-05, 'batch_size': 87, 'step_size': 8, 'gamma': 0.939940375026197}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:12:36,846][0m Trial 26 finished with value: 0.4061919283121824 and parameters: {'observation_period_num': 33, 'train_rates': 0.8423944645618248, 'learning_rate': 1.5188682201716314e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.8923948036062875}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:13:54,765][0m Trial 27 finished with value: 0.541044007628838 and parameters: {'observation_period_num': 59, 'train_rates': 0.9038103279507131, 'learning_rate': 2.557492323059738e-06, 'batch_size': 106, 'step_size': 7, 'gamma': 0.9743250196301088}. Best is trial 3 with value: 0.15730328857898712.[0m
[32m[I 2025-01-08 10:16:09,545][0m Trial 28 finished with value: 0.15314627373995987 and parameters: {'observation_period_num': 99, 'train_rates': 0.9521871880886221, 'learning_rate': 7.977457492927796e-05, 'batch_size': 123, 'step_size': 11, 'gamma': 0.9492784358303653}. Best is trial 28 with value: 0.15314627373995987.[0m
[32m[I 2025-01-08 10:20:47,654][0m Trial 29 finished with value: 0.3610964272041423 and parameters: {'observation_period_num': 197, 'train_rates': 0.8658121141147721, 'learning_rate': 3.149568357606396e-05, 'batch_size': 173, 'step_size': 14, 'gamma': 0.8109371079458539}. Best is trial 28 with value: 0.15314627373995987.[0m
[32m[I 2025-01-08 10:23:07,361][0m Trial 30 finished with value: 0.16637787222862244 and parameters: {'observation_period_num': 102, 'train_rates': 0.9647834943974172, 'learning_rate': 6.455974082531222e-05, 'batch_size': 128, 'step_size': 11, 'gamma': 0.907532544577087}. Best is trial 28 with value: 0.15314627373995987.[0m
[32m[I 2025-01-08 10:25:55,268][0m Trial 31 finished with value: 0.14380208320087856 and parameters: {'observation_period_num': 123, 'train_rates': 0.9465418385539695, 'learning_rate': 9.801642955036768e-05, 'batch_size': 119, 'step_size': 9, 'gamma': 0.955200902848204}. Best is trial 31 with value: 0.14380208320087856.[0m
[32m[I 2025-01-08 10:28:42,415][0m Trial 32 finished with value: 0.2198332963204475 and parameters: {'observation_period_num': 127, 'train_rates': 0.9083711769571698, 'learning_rate': 2.306049440871674e-05, 'batch_size': 143, 'step_size': 12, 'gamma': 0.9457176837086672}. Best is trial 31 with value: 0.14380208320087856.[0m
[32m[I 2025-01-08 10:32:08,332][0m Trial 33 finished with value: 0.1792985364343181 and parameters: {'observation_period_num': 143, 'train_rates': 0.9417375992667165, 'learning_rate': 9.865629738472551e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.958733069727528}. Best is trial 31 with value: 0.14380208320087856.[0m
[32m[I 2025-01-08 10:34:16,007][0m Trial 34 finished with value: 0.15065438364903758 and parameters: {'observation_period_num': 91, 'train_rates': 0.9560425629364236, 'learning_rate': 3.971585699953872e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9336589748748931}. Best is trial 31 with value: 0.14380208320087856.[0m
[32m[I 2025-01-08 10:36:22,287][0m Trial 35 finished with value: 0.2018515368860759 and parameters: {'observation_period_num': 89, 'train_rates': 0.9694471571753605, 'learning_rate': 0.00020252457791310253, 'batch_size': 79, 'step_size': 8, 'gamma': 0.9537033759879541}. Best is trial 31 with value: 0.14380208320087856.[0m
[32m[I 2025-01-08 10:39:48,498][0m Trial 36 finished with value: 0.1358593266795982 and parameters: {'observation_period_num': 117, 'train_rates': 0.942224010675303, 'learning_rate': 7.520183315893919e-05, 'batch_size': 21, 'step_size': 9, 'gamma': 0.9073365485907854}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 10:42:35,391][0m Trial 37 finished with value: 0.4391050044891994 and parameters: {'observation_period_num': 119, 'train_rates': 0.8122886733051977, 'learning_rate': 1.0967938226063618e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8778473021550052}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 10:46:31,963][0m Trial 38 finished with value: 0.7596399110871679 and parameters: {'observation_period_num': 140, 'train_rates': 0.7137142104808292, 'learning_rate': 2.579498975887067e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9093588336082042}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 10:50:26,852][0m Trial 39 finished with value: 0.2263049007188983 and parameters: {'observation_period_num': 161, 'train_rates': 0.8836019069693233, 'learning_rate': 7.90085244299963e-05, 'batch_size': 40, 'step_size': 6, 'gamma': 0.8774950619039683}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 10:55:09,988][0m Trial 40 finished with value: 0.1980784046649933 and parameters: {'observation_period_num': 191, 'train_rates': 0.9462092281228484, 'learning_rate': 4.682380425264342e-05, 'batch_size': 64, 'step_size': 1, 'gamma': 0.9529199424084601}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 10:57:25,934][0m Trial 41 finished with value: 0.2359260469675064 and parameters: {'observation_period_num': 98, 'train_rates': 0.970122092776043, 'learning_rate': 0.0004608067791617527, 'batch_size': 147, 'step_size': 10, 'gamma': 0.9303207859903084}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 11:00:07,628][0m Trial 42 finished with value: 0.14429264136080472 and parameters: {'observation_period_num': 114, 'train_rates': 0.9445397196068016, 'learning_rate': 0.00010420279260615973, 'batch_size': 75, 'step_size': 9, 'gamma': 0.9124829703884271}. Best is trial 36 with value: 0.1358593266795982.[0m
[32m[I 2025-01-08 11:03:20,688][0m Trial 43 finished with value: 0.13471137256258064 and parameters: {'observation_period_num': 133, 'train_rates': 0.9367954120685003, 'learning_rate': 0.00010663334791298593, 'batch_size': 73, 'step_size': 8, 'gamma': 0.9011192917160059}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:06:35,416][0m Trial 44 finished with value: 0.13661155209959489 and parameters: {'observation_period_num': 133, 'train_rates': 0.9327799890557269, 'learning_rate': 0.00010766088972993419, 'batch_size': 71, 'step_size': 9, 'gamma': 0.8986127289649651}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:10:52,566][0m Trial 45 finished with value: 0.1615913627144781 and parameters: {'observation_period_num': 175, 'train_rates': 0.9265133653843297, 'learning_rate': 0.0002163783263866475, 'batch_size': 67, 'step_size': 9, 'gamma': 0.901193769289096}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:14:09,035][0m Trial 46 finished with value: 0.2066726326609457 and parameters: {'observation_period_num': 137, 'train_rates': 0.8738870238303201, 'learning_rate': 0.0001094578881095256, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8548683823956431}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:18:13,268][0m Trial 47 finished with value: 0.158234581624975 and parameters: {'observation_period_num': 155, 'train_rates': 0.9130359480170257, 'learning_rate': 0.00016214840125070463, 'batch_size': 26, 'step_size': 5, 'gamma': 0.8711859633096923}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:20:49,018][0m Trial 48 finished with value: 0.16413566699394813 and parameters: {'observation_period_num': 114, 'train_rates': 0.9364010953156885, 'learning_rate': 6.069953730850143e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.9107711202235517}. Best is trial 43 with value: 0.13471137256258064.[0m
[32m[I 2025-01-08 11:23:49,862][0m Trial 49 finished with value: 1.0736309897820682 and parameters: {'observation_period_num': 131, 'train_rates': 0.8408302130213099, 'learning_rate': 0.0006752850380282736, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8370421726779029}. Best is trial 43 with value: 0.13471137256258064.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.7934 | 0.9592
Epoch 2/300, Loss: 0.7264 | 0.6787
Epoch 3/300, Loss: 0.5632 | 0.7358
Epoch 4/300, Loss: 0.5283 | 0.5836
Epoch 5/300, Loss: 0.3887 | 0.4699
Epoch 6/300, Loss: 0.3557 | 0.4226
Epoch 7/300, Loss: 0.3930 | 0.3866
Epoch 8/300, Loss: 0.4009 | 0.3744
Epoch 9/300, Loss: 0.4151 | 0.5850
Epoch 10/300, Loss: 0.3303 | 0.3932
Epoch 11/300, Loss: 0.3158 | 0.3184
Epoch 12/300, Loss: 0.3124 | 0.3271
Epoch 13/300, Loss: 0.3419 | 0.3809
Epoch 14/300, Loss: 0.2726 | 0.3168
Epoch 15/300, Loss: 0.2567 | 0.3113
Epoch 16/300, Loss: 0.2511 | 0.2993
Epoch 17/300, Loss: 0.2612 | 0.3131
Epoch 18/300, Loss: 0.2822 | 0.3217
Epoch 19/300, Loss: 0.2633 | 0.3093
Epoch 20/300, Loss: 0.2279 | 0.2665
Epoch 21/300, Loss: 0.2158 | 0.2983
Epoch 22/300, Loss: 0.1985 | 0.2580
Epoch 23/300, Loss: 0.1983 | 0.2681
Epoch 24/300, Loss: 0.1998 | 0.2616
Epoch 25/300, Loss: 0.1929 | 0.2463
Epoch 26/300, Loss: 0.1825 | 0.2561
Epoch 27/300, Loss: 0.1794 | 0.2425
Epoch 28/300, Loss: 0.1757 | 0.2399
Epoch 29/300, Loss: 0.1763 | 0.2423
Epoch 30/300, Loss: 0.1685 | 0.2344
Epoch 31/300, Loss: 0.1673 | 0.2344
Epoch 32/300, Loss: 0.1655 | 0.2280
Epoch 33/300, Loss: 0.1642 | 0.2246
Epoch 34/300, Loss: 0.1629 | 0.2248
Epoch 35/300, Loss: 0.1612 | 0.2197
Epoch 36/300, Loss: 0.1600 | 0.2169
Epoch 37/300, Loss: 0.1588 | 0.2154
Epoch 38/300, Loss: 0.1578 | 0.2109
Epoch 39/300, Loss: 0.1569 | 0.2111
Epoch 40/300, Loss: 0.1558 | 0.2071
Epoch 41/300, Loss: 0.1554 | 0.2052
Epoch 42/300, Loss: 0.1545 | 0.2047
Epoch 43/300, Loss: 0.1536 | 0.2017
Epoch 44/300, Loss: 0.1528 | 0.1971
Epoch 45/300, Loss: 0.1518 | 0.1997
Epoch 46/300, Loss: 0.1517 | 0.1953
Epoch 47/300, Loss: 0.1502 | 0.1923
Epoch 48/300, Loss: 0.1503 | 0.1949
Epoch 49/300, Loss: 0.1495 | 0.1900
Epoch 50/300, Loss: 0.1487 | 0.1883
Epoch 51/300, Loss: 0.1476 | 0.1881
Epoch 52/300, Loss: 0.1469 | 0.1876
Epoch 53/300, Loss: 0.1472 | 0.1829
Epoch 54/300, Loss: 0.1465 | 0.1826
Epoch 55/300, Loss: 0.1452 | 0.1836
Epoch 56/300, Loss: 0.1450 | 0.1784
Epoch 57/300, Loss: 0.1456 | 0.1772
Epoch 58/300, Loss: 0.1457 | 0.1804
Epoch 59/300, Loss: 0.1438 | 0.1756
Epoch 60/300, Loss: 0.1426 | 0.1759
Epoch 61/300, Loss: 0.1426 | 0.1755
Epoch 62/300, Loss: 0.1419 | 0.1733
Epoch 63/300, Loss: 0.1410 | 0.1709
Epoch 64/300, Loss: 0.1407 | 0.1705
Epoch 65/300, Loss: 0.1401 | 0.1699
Epoch 66/300, Loss: 0.1393 | 0.1681
Epoch 67/300, Loss: 0.1390 | 0.1673
Epoch 68/300, Loss: 0.1389 | 0.1674
Epoch 69/300, Loss: 0.1377 | 0.1652
Epoch 70/300, Loss: 0.1372 | 0.1642
Epoch 71/300, Loss: 0.1366 | 0.1648
Epoch 72/300, Loss: 0.1362 | 0.1634
Epoch 73/300, Loss: 0.1364 | 0.1612
Epoch 74/300, Loss: 0.1361 | 0.1632
Epoch 75/300, Loss: 0.1360 | 0.1601
Epoch 76/300, Loss: 0.1355 | 0.1601
Epoch 77/300, Loss: 0.1346 | 0.1600
Epoch 78/300, Loss: 0.1337 | 0.1585
Epoch 79/300, Loss: 0.1336 | 0.1570
Epoch 80/300, Loss: 0.1341 | 0.1584
Epoch 81/300, Loss: 0.1327 | 0.1549
Epoch 82/300, Loss: 0.1329 | 0.1555
Epoch 83/300, Loss: 0.1323 | 0.1546
Epoch 84/300, Loss: 0.1324 | 0.1540
Epoch 85/300, Loss: 0.1317 | 0.1540
Epoch 86/300, Loss: 0.1317 | 0.1549
Epoch 87/300, Loss: 0.1317 | 0.1517
Epoch 88/300, Loss: 0.1313 | 0.1527
Epoch 89/300, Loss: 0.1306 | 0.1519
Epoch 90/300, Loss: 0.1299 | 0.1513
Epoch 91/300, Loss: 0.1294 | 0.1508
Epoch 92/300, Loss: 0.1287 | 0.1506
Epoch 93/300, Loss: 0.1286 | 0.1492
Epoch 94/300, Loss: 0.1284 | 0.1494
Epoch 95/300, Loss: 0.1288 | 0.1478
Epoch 96/300, Loss: 0.1273 | 0.1484
Epoch 97/300, Loss: 0.1279 | 0.1473
Epoch 98/300, Loss: 0.1276 | 0.1471
Epoch 99/300, Loss: 0.1273 | 0.1470
Epoch 100/300, Loss: 0.1261 | 0.1468
Epoch 101/300, Loss: 0.1269 | 0.1458
Epoch 102/300, Loss: 0.1269 | 0.1464
Epoch 103/300, Loss: 0.1255 | 0.1458
Epoch 104/300, Loss: 0.1263 | 0.1454
Epoch 105/300, Loss: 0.1255 | 0.1453
Epoch 106/300, Loss: 0.1251 | 0.1446
Epoch 107/300, Loss: 0.1258 | 0.1441
Epoch 108/300, Loss: 0.1255 | 0.1441
Epoch 109/300, Loss: 0.1256 | 0.1439
Epoch 110/300, Loss: 0.1243 | 0.1435
Epoch 111/300, Loss: 0.1243 | 0.1433
Epoch 112/300, Loss: 0.1247 | 0.1428
Epoch 113/300, Loss: 0.1237 | 0.1426
Epoch 114/300, Loss: 0.1239 | 0.1420
Epoch 115/300, Loss: 0.1240 | 0.1421
Epoch 116/300, Loss: 0.1237 | 0.1415
Epoch 117/300, Loss: 0.1229 | 0.1416
Epoch 118/300, Loss: 0.1233 | 0.1408
Epoch 119/300, Loss: 0.1233 | 0.1405
Epoch 120/300, Loss: 0.1236 | 0.1405
Epoch 121/300, Loss: 0.1233 | 0.1407
Epoch 122/300, Loss: 0.1228 | 0.1404
Epoch 123/300, Loss: 0.1224 | 0.1405
Epoch 124/300, Loss: 0.1217 | 0.1397
Epoch 125/300, Loss: 0.1220 | 0.1396
Epoch 126/300, Loss: 0.1225 | 0.1394
Epoch 127/300, Loss: 0.1221 | 0.1394
Epoch 128/300, Loss: 0.1223 | 0.1390
Epoch 129/300, Loss: 0.1221 | 0.1389
Epoch 130/300, Loss: 0.1219 | 0.1390
Epoch 131/300, Loss: 0.1214 | 0.1392
Epoch 132/300, Loss: 0.1217 | 0.1388
Epoch 133/300, Loss: 0.1213 | 0.1385
Epoch 134/300, Loss: 0.1215 | 0.1389
Epoch 135/300, Loss: 0.1210 | 0.1377
Epoch 136/300, Loss: 0.1205 | 0.1379
Epoch 137/300, Loss: 0.1213 | 0.1380
Epoch 138/300, Loss: 0.1206 | 0.1375
Epoch 139/300, Loss: 0.1212 | 0.1379
Epoch 140/300, Loss: 0.1205 | 0.1378
Epoch 141/300, Loss: 0.1203 | 0.1378
Epoch 142/300, Loss: 0.1206 | 0.1378
Epoch 143/300, Loss: 0.1204 | 0.1377
Epoch 144/300, Loss: 0.1202 | 0.1376
Epoch 145/300, Loss: 0.1205 | 0.1370
Epoch 146/300, Loss: 0.1206 | 0.1369
Epoch 147/300, Loss: 0.1201 | 0.1367
Epoch 148/300, Loss: 0.1205 | 0.1365
Epoch 149/300, Loss: 0.1197 | 0.1366
Epoch 150/300, Loss: 0.1193 | 0.1366
Epoch 151/300, Loss: 0.1191 | 0.1363
Epoch 152/300, Loss: 0.1206 | 0.1363
Epoch 153/300, Loss: 0.1201 | 0.1359
Epoch 154/300, Loss: 0.1195 | 0.1360
Epoch 155/300, Loss: 0.1194 | 0.1361
Epoch 156/300, Loss: 0.1191 | 0.1357
Epoch 157/300, Loss: 0.1194 | 0.1356
Epoch 158/300, Loss: 0.1188 | 0.1357
Epoch 159/300, Loss: 0.1190 | 0.1357
Epoch 160/300, Loss: 0.1194 | 0.1357
Epoch 161/300, Loss: 0.1189 | 0.1355
Epoch 162/300, Loss: 0.1186 | 0.1349
Epoch 163/300, Loss: 0.1194 | 0.1346
Epoch 164/300, Loss: 0.1189 | 0.1345
Epoch 165/300, Loss: 0.1187 | 0.1342
Epoch 166/300, Loss: 0.1193 | 0.1346
Epoch 167/300, Loss: 0.1189 | 0.1347
Epoch 168/300, Loss: 0.1190 | 0.1344
Epoch 169/300, Loss: 0.1192 | 0.1342
Epoch 170/300, Loss: 0.1193 | 0.1342
Epoch 171/300, Loss: 0.1182 | 0.1344
Epoch 172/300, Loss: 0.1185 | 0.1345
Epoch 173/300, Loss: 0.1182 | 0.1344
Epoch 174/300, Loss: 0.1186 | 0.1342
Epoch 175/300, Loss: 0.1188 | 0.1341
Epoch 176/300, Loss: 0.1188 | 0.1341
Epoch 177/300, Loss: 0.1190 | 0.1341
Epoch 178/300, Loss: 0.1185 | 0.1339
Epoch 179/300, Loss: 0.1182 | 0.1335
Epoch 180/300, Loss: 0.1186 | 0.1338
Epoch 181/300, Loss: 0.1182 | 0.1340
Epoch 182/300, Loss: 0.1185 | 0.1342
Epoch 183/300, Loss: 0.1178 | 0.1340
Epoch 184/300, Loss: 0.1184 | 0.1339
Epoch 185/300, Loss: 0.1180 | 0.1338
Epoch 186/300, Loss: 0.1180 | 0.1337
Epoch 187/300, Loss: 0.1182 | 0.1335
Epoch 188/300, Loss: 0.1187 | 0.1336
Epoch 189/300, Loss: 0.1172 | 0.1336
Epoch 190/300, Loss: 0.1179 | 0.1334
Epoch 191/300, Loss: 0.1185 | 0.1332
Epoch 192/300, Loss: 0.1176 | 0.1335
Epoch 193/300, Loss: 0.1173 | 0.1336
Epoch 194/300, Loss: 0.1178 | 0.1335
Epoch 195/300, Loss: 0.1175 | 0.1333
Epoch 196/300, Loss: 0.1180 | 0.1332
Epoch 197/300, Loss: 0.1177 | 0.1332
Epoch 198/300, Loss: 0.1177 | 0.1331
Epoch 199/300, Loss: 0.1182 | 0.1331
Epoch 200/300, Loss: 0.1173 | 0.1330
Epoch 201/300, Loss: 0.1181 | 0.1330
Epoch 202/300, Loss: 0.1177 | 0.1329
Epoch 203/300, Loss: 0.1178 | 0.1327
Epoch 204/300, Loss: 0.1177 | 0.1325
Epoch 205/300, Loss: 0.1176 | 0.1325
Epoch 206/300, Loss: 0.1181 | 0.1325
Epoch 207/300, Loss: 0.1170 | 0.1325
Epoch 208/300, Loss: 0.1178 | 0.1325
Epoch 209/300, Loss: 0.1179 | 0.1326
Epoch 210/300, Loss: 0.1175 | 0.1327
Epoch 211/300, Loss: 0.1183 | 0.1326
Epoch 212/300, Loss: 0.1181 | 0.1327
Epoch 213/300, Loss: 0.1178 | 0.1327
Epoch 214/300, Loss: 0.1180 | 0.1327
Epoch 215/300, Loss: 0.1171 | 0.1327
Epoch 216/300, Loss: 0.1176 | 0.1327
Epoch 217/300, Loss: 0.1172 | 0.1327
Epoch 218/300, Loss: 0.1171 | 0.1325
Epoch 219/300, Loss: 0.1169 | 0.1326
Epoch 220/300, Loss: 0.1174 | 0.1326
Epoch 221/300, Loss: 0.1181 | 0.1326
Epoch 222/300, Loss: 0.1166 | 0.1326
Epoch 223/300, Loss: 0.1180 | 0.1326
Epoch 224/300, Loss: 0.1173 | 0.1325
Epoch 225/300, Loss: 0.1176 | 0.1326
Epoch 226/300, Loss: 0.1173 | 0.1327
Epoch 227/300, Loss: 0.1177 | 0.1326
Epoch 228/300, Loss: 0.1171 | 0.1325
Epoch 229/300, Loss: 0.1179 | 0.1325
Epoch 230/300, Loss: 0.1172 | 0.1324
Epoch 231/300, Loss: 0.1172 | 0.1324
Epoch 232/300, Loss: 0.1175 | 0.1324
Epoch 233/300, Loss: 0.1171 | 0.1324
Epoch 234/300, Loss: 0.1170 | 0.1323
Epoch 235/300, Loss: 0.1168 | 0.1323
Epoch 236/300, Loss: 0.1167 | 0.1323
Epoch 237/300, Loss: 0.1170 | 0.1323
Epoch 238/300, Loss: 0.1173 | 0.1322
Epoch 239/300, Loss: 0.1172 | 0.1323
Epoch 240/300, Loss: 0.1172 | 0.1323
Epoch 241/300, Loss: 0.1166 | 0.1324
Epoch 242/300, Loss: 0.1173 | 0.1323
Epoch 243/300, Loss: 0.1174 | 0.1323
Epoch 244/300, Loss: 0.1170 | 0.1322
Epoch 245/300, Loss: 0.1171 | 0.1322
Epoch 246/300, Loss: 0.1174 | 0.1323
Epoch 247/300, Loss: 0.1175 | 0.1323
Epoch 248/300, Loss: 0.1170 | 0.1322
Epoch 249/300, Loss: 0.1174 | 0.1322
Epoch 250/300, Loss: 0.1167 | 0.1322
Epoch 251/300, Loss: 0.1170 | 0.1321
Epoch 252/300, Loss: 0.1175 | 0.1321
Epoch 253/300, Loss: 0.1178 | 0.1320
Epoch 254/300, Loss: 0.1172 | 0.1320
Epoch 255/300, Loss: 0.1165 | 0.1320
Epoch 256/300, Loss: 0.1167 | 0.1320
Epoch 257/300, Loss: 0.1174 | 0.1320
Epoch 258/300, Loss: 0.1174 | 0.1320
Epoch 259/300, Loss: 0.1168 | 0.1320
Epoch 260/300, Loss: 0.1173 | 0.1320
Epoch 261/300, Loss: 0.1171 | 0.1320
Epoch 262/300, Loss: 0.1171 | 0.1319
Epoch 263/300, Loss: 0.1167 | 0.1319
Epoch 264/300, Loss: 0.1171 | 0.1319
Epoch 265/300, Loss: 0.1177 | 0.1319
Epoch 266/300, Loss: 0.1163 | 0.1319
Epoch 267/300, Loss: 0.1170 | 0.1319
Epoch 268/300, Loss: 0.1171 | 0.1319
Epoch 269/300, Loss: 0.1170 | 0.1319
Epoch 270/300, Loss: 0.1173 | 0.1319
Epoch 271/300, Loss: 0.1176 | 0.1319
Epoch 272/300, Loss: 0.1175 | 0.1320
Epoch 273/300, Loss: 0.1168 | 0.1320
Epoch 274/300, Loss: 0.1167 | 0.1319
Epoch 275/300, Loss: 0.1173 | 0.1319
Epoch 276/300, Loss: 0.1164 | 0.1319
Epoch 277/300, Loss: 0.1164 | 0.1319
Epoch 278/300, Loss: 0.1171 | 0.1319
Epoch 279/300, Loss: 0.1171 | 0.1319
Epoch 280/300, Loss: 0.1168 | 0.1319
Epoch 281/300, Loss: 0.1178 | 0.1319
Epoch 282/300, Loss: 0.1169 | 0.1319
Epoch 283/300, Loss: 0.1170 | 0.1319
Epoch 284/300, Loss: 0.1175 | 0.1319
Epoch 285/300, Loss: 0.1169 | 0.1319
Epoch 286/300, Loss: 0.1171 | 0.1319
Epoch 287/300, Loss: 0.1165 | 0.1319
Epoch 288/300, Loss: 0.1170 | 0.1319
Epoch 289/300, Loss: 0.1173 | 0.1319
Epoch 290/300, Loss: 0.1171 | 0.1319
Epoch 291/300, Loss: 0.1167 | 0.1319
Epoch 292/300, Loss: 0.1172 | 0.1319
Epoch 293/300, Loss: 0.1175 | 0.1319
Epoch 294/300, Loss: 0.1174 | 0.1319
Epoch 295/300, Loss: 0.1170 | 0.1319
Epoch 296/300, Loss: 0.1172 | 0.1319
Epoch 297/300, Loss: 0.1171 | 0.1319
Epoch 298/300, Loss: 0.1168 | 0.1319
Epoch 299/300, Loss: 0.1164 | 0.1319
Epoch 300/300, Loss: 0.1176 | 0.1319
Runtime (seconds): 582.5153820514679
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1511.4471451903228
RMSE: 38.87733459472656
MAE: 38.87733459472656
R-squared: nan
[190.17267]
