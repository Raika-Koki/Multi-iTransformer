[32m[I 2025-02-06 04:33:15,757][0m A new study created in memory with name: no-name-52df4b67-8441-45d4-81c7-74b9f48c7063[0m
[32m[I 2025-02-06 04:34:15,685][0m Trial 0 finished with value: 0.26150412510417265 and parameters: {'observation_period_num': 45, 'train_rates': 0.7953749814772488, 'learning_rate': 5.7109966964060125e-06, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8126270835817179}. Best is trial 0 with value: 0.26150412510417265.[0m
[32m[I 2025-02-06 04:34:52,861][0m Trial 1 finished with value: 0.05594167113304138 and parameters: {'observation_period_num': 56, 'train_rates': 0.9447656382071742, 'learning_rate': 0.0008560369006169248, 'batch_size': 170, 'step_size': 10, 'gamma': 0.7703116038366031}. Best is trial 1 with value: 0.05594167113304138.[0m
Early stopping at epoch 84
[32m[I 2025-02-06 04:35:13,128][0m Trial 2 finished with value: 0.8730396479238972 and parameters: {'observation_period_num': 82, 'train_rates': 0.6023031408119206, 'learning_rate': 1.7473811255535585e-05, 'batch_size': 205, 'step_size': 1, 'gamma': 0.8690870517808474}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:35:52,762][0m Trial 3 finished with value: 0.20190784010020169 and parameters: {'observation_period_num': 136, 'train_rates': 0.884237902007406, 'learning_rate': 4.238500688309721e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.8226455409184514}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:37:17,076][0m Trial 4 finished with value: 0.23141194844102286 and parameters: {'observation_period_num': 123, 'train_rates': 0.767718250472736, 'learning_rate': 0.00012666246392636324, 'batch_size': 58, 'step_size': 2, 'gamma': 0.9324479407108086}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:37:37,800][0m Trial 5 finished with value: 0.5989800624736734 and parameters: {'observation_period_num': 196, 'train_rates': 0.6288924737511483, 'learning_rate': 4.976139669916911e-06, 'batch_size': 230, 'step_size': 10, 'gamma': 0.9890960007958473}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:39:27,747][0m Trial 6 finished with value: 0.2806369120806036 and parameters: {'observation_period_num': 14, 'train_rates': 0.6170466154424995, 'learning_rate': 4.363257054467281e-06, 'batch_size': 40, 'step_size': 8, 'gamma': 0.7703549157394389}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:40:11,985][0m Trial 7 finished with value: 0.6259566592925694 and parameters: {'observation_period_num': 252, 'train_rates': 0.6246554520812414, 'learning_rate': 3.9711844104989144e-06, 'batch_size': 95, 'step_size': 14, 'gamma': 0.7929599420553906}. Best is trial 1 with value: 0.05594167113304138.[0m
Early stopping at epoch 68
[32m[I 2025-02-06 04:40:33,318][0m Trial 8 finished with value: 1.1142321980162844 and parameters: {'observation_period_num': 239, 'train_rates': 0.8404700646314804, 'learning_rate': 7.104360437768996e-06, 'batch_size': 186, 'step_size': 1, 'gamma': 0.846208422893701}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:41:12,035][0m Trial 9 finished with value: 0.30005875279743704 and parameters: {'observation_period_num': 252, 'train_rates': 0.8470472268180819, 'learning_rate': 3.767389126433805e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.7570268490041174}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:41:39,833][0m Trial 10 finished with value: 0.23753486573696136 and parameters: {'observation_period_num': 96, 'train_rates': 0.9856040241724834, 'learning_rate': 0.0009023147915546018, 'batch_size': 250, 'step_size': 5, 'gamma': 0.9100461331732245}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:42:21,489][0m Trial 11 finished with value: 0.17010754346847534 and parameters: {'observation_period_num': 164, 'train_rates': 0.9758824834213276, 'learning_rate': 0.0009971764019964173, 'batch_size': 151, 'step_size': 6, 'gamma': 0.8189334014666336}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:42:57,758][0m Trial 12 finished with value: 0.2013818472623825 and parameters: {'observation_period_num': 172, 'train_rates': 0.9681053890137561, 'learning_rate': 0.0009340236301620263, 'batch_size': 170, 'step_size': 5, 'gamma': 0.7846033639436402}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:43:51,545][0m Trial 13 finished with value: 0.12377970166770476 and parameters: {'observation_period_num': 172, 'train_rates': 0.920491415233798, 'learning_rate': 0.0002515472606788387, 'batch_size': 109, 'step_size': 5, 'gamma': 0.8342005021056925}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:44:56,330][0m Trial 14 finished with value: 0.08396294235947575 and parameters: {'observation_period_num': 60, 'train_rates': 0.9135371627175751, 'learning_rate': 0.0002430067949249444, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8919489047897013}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:45:47,568][0m Trial 15 finished with value: 0.7034575066396168 and parameters: {'observation_period_num': 54, 'train_rates': 0.9139937606603554, 'learning_rate': 1.028932896154544e-06, 'batch_size': 117, 'step_size': 14, 'gamma': 0.9088600676597639}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:49:35,442][0m Trial 16 finished with value: 0.16125048178038157 and parameters: {'observation_period_num': 25, 'train_rates': 0.7074931921423581, 'learning_rate': 0.00029779212955001244, 'batch_size': 21, 'step_size': 12, 'gamma': 0.878154158114191}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:51:04,282][0m Trial 17 finished with value: 0.16398341579640166 and parameters: {'observation_period_num': 78, 'train_rates': 0.9193297528078577, 'learning_rate': 0.0003012983284483146, 'batch_size': 64, 'step_size': 11, 'gamma': 0.952624628786697}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:51:34,591][0m Trial 18 finished with value: 0.15191702646725552 and parameters: {'observation_period_num': 115, 'train_rates': 0.8650982040480213, 'learning_rate': 0.00010468281447918, 'batch_size': 205, 'step_size': 15, 'gamma': 0.8696055363620728}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:52:09,450][0m Trial 19 finished with value: 0.11343262092815798 and parameters: {'observation_period_num': 54, 'train_rates': 0.8145186252616441, 'learning_rate': 0.00046349765116756057, 'batch_size': 169, 'step_size': 13, 'gamma': 0.8957789153579935}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:53:13,590][0m Trial 20 finished with value: 0.14844447403523692 and parameters: {'observation_period_num': 7, 'train_rates': 0.74240431271752, 'learning_rate': 0.0001081392003120962, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9644241468003358}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:53:49,058][0m Trial 21 finished with value: 0.09042528585383766 and parameters: {'observation_period_num': 53, 'train_rates': 0.8053219667340245, 'learning_rate': 0.0005502807280656269, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8999038655884022}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:54:31,063][0m Trial 22 finished with value: 0.1570410226782163 and parameters: {'observation_period_num': 38, 'train_rates': 0.6892222803702704, 'learning_rate': 0.0005227732084541062, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8529416922605048}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:55:08,765][0m Trial 23 finished with value: 0.14545407891273499 and parameters: {'observation_period_num': 71, 'train_rates': 0.9459952911683807, 'learning_rate': 0.00018836191693049907, 'batch_size': 165, 'step_size': 10, 'gamma': 0.9354428914430141}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:55:39,398][0m Trial 24 finished with value: 0.20707798323460988 and parameters: {'observation_period_num': 98, 'train_rates': 0.8933366801847771, 'learning_rate': 0.0004365165219889705, 'batch_size': 198, 'step_size': 15, 'gamma': 0.8951543042439813}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:56:27,877][0m Trial 25 finished with value: 0.21468816068820182 and parameters: {'observation_period_num': 68, 'train_rates': 0.9407237666193594, 'learning_rate': 0.0005801850359370053, 'batch_size': 128, 'step_size': 13, 'gamma': 0.9297730465876288}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:56:58,778][0m Trial 26 finished with value: 0.06204218904886927 and parameters: {'observation_period_num': 29, 'train_rates': 0.8223171429424367, 'learning_rate': 6.886054458092555e-05, 'batch_size': 187, 'step_size': 11, 'gamma': 0.8784635892819568}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:57:27,080][0m Trial 27 finished with value: 0.08501010262267791 and parameters: {'observation_period_num': 26, 'train_rates': 0.8759109574937217, 'learning_rate': 5.931215621182993e-05, 'batch_size': 224, 'step_size': 7, 'gamma': 0.852306043143849}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:57:58,943][0m Trial 28 finished with value: 0.14951064729429855 and parameters: {'observation_period_num': 32, 'train_rates': 0.8294129629396756, 'learning_rate': 1.7442431354745207e-05, 'batch_size': 185, 'step_size': 11, 'gamma': 0.8835488100697988}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:58:55,662][0m Trial 29 finished with value: 0.2103793013772768 and parameters: {'observation_period_num': 97, 'train_rates': 0.7726917925424441, 'learning_rate': 0.0001670048205087974, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8112542476798578}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:59:23,204][0m Trial 30 finished with value: 0.23924846947193146 and parameters: {'observation_period_num': 142, 'train_rates': 0.9375542947640082, 'learning_rate': 8.054151372214536e-05, 'batch_size': 223, 'step_size': 9, 'gamma': 0.7944968844351465}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 04:59:50,431][0m Trial 31 finished with value: 0.08965144524315619 and parameters: {'observation_period_num': 19, 'train_rates': 0.892614048739267, 'learning_rate': 6.479417440986644e-05, 'batch_size': 231, 'step_size': 7, 'gamma': 0.8580404262895522}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:00:17,731][0m Trial 32 finished with value: 0.2756389567558778 and parameters: {'observation_period_num': 43, 'train_rates': 0.8729610695248826, 'learning_rate': 2.4370831566949647e-05, 'batch_size': 214, 'step_size': 7, 'gamma': 0.8426457974089752}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:00:50,202][0m Trial 33 finished with value: 0.09106888979812325 and parameters: {'observation_period_num': 29, 'train_rates': 0.8548895240285457, 'learning_rate': 5.2159577855002916e-05, 'batch_size': 191, 'step_size': 11, 'gamma': 0.7501540369579233}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:01:16,199][0m Trial 34 finished with value: 0.19037138227361755 and parameters: {'observation_period_num': 5, 'train_rates': 0.8921561740175927, 'learning_rate': 1.2762692843012094e-05, 'batch_size': 251, 'step_size': 8, 'gamma': 0.8646090776487662}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:01:50,925][0m Trial 35 finished with value: 0.2950011444174581 and parameters: {'observation_period_num': 47, 'train_rates': 0.7790788245729697, 'learning_rate': 2.803724721627646e-05, 'batch_size': 153, 'step_size': 3, 'gamma': 0.9214742815685513}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:02:19,127][0m Trial 36 finished with value: 0.07831415642495357 and parameters: {'observation_period_num': 69, 'train_rates': 0.8209845258791186, 'learning_rate': 0.0001656646055218793, 'batch_size': 214, 'step_size': 9, 'gamma': 0.8806319186652619}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:02:49,854][0m Trial 37 finished with value: 0.2117864136221716 and parameters: {'observation_period_num': 63, 'train_rates': 0.7421426968668224, 'learning_rate': 0.0001776913283628381, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8798273543518518}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:03:17,995][0m Trial 38 finished with value: 0.0986739469975635 and parameters: {'observation_period_num': 91, 'train_rates': 0.820449085929331, 'learning_rate': 0.00013913323816338433, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8863410644516396}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:04:16,200][0m Trial 39 finished with value: 0.14797223904940088 and parameters: {'observation_period_num': 113, 'train_rates': 0.9558530389258335, 'learning_rate': 0.00023564151342891034, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8345087546290753}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:05:27,097][0m Trial 40 finished with value: 0.20541070221412566 and parameters: {'observation_period_num': 88, 'train_rates': 0.7429475645245263, 'learning_rate': 0.00035928218314795074, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8678357935241859}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:05:50,804][0m Trial 41 finished with value: 0.07232382116379034 and parameters: {'observation_period_num': 32, 'train_rates': 0.7957611508527953, 'learning_rate': 8.317582013662025e-05, 'batch_size': 236, 'step_size': 7, 'gamma': 0.9094934675725557}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:06:14,444][0m Trial 42 finished with value: 0.23042865328051065 and parameters: {'observation_period_num': 65, 'train_rates': 0.7852441787819945, 'learning_rate': 8.119813702669308e-05, 'batch_size': 239, 'step_size': 8, 'gamma': 0.9148128029933353}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:06:42,582][0m Trial 43 finished with value: 0.09028589384751108 and parameters: {'observation_period_num': 39, 'train_rates': 0.800810034459305, 'learning_rate': 4.0473768886005595e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.8979194183736748}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:07:05,767][0m Trial 44 finished with value: 0.27589929210797454 and parameters: {'observation_period_num': 221, 'train_rates': 0.8488343654771107, 'learning_rate': 0.0007122366370170186, 'batch_size': 237, 'step_size': 9, 'gamma': 0.9401502510918283}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:07:27,563][0m Trial 45 finished with value: 0.22799772981633531 and parameters: {'observation_period_num': 77, 'train_rates': 0.7574280578692367, 'learning_rate': 8.909153973687594e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9219949225130637}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:09:06,285][0m Trial 46 finished with value: 0.13181862392532165 and parameters: {'observation_period_num': 14, 'train_rates': 0.7015491861433536, 'learning_rate': 0.00021128424082279024, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9059877054409492}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:09:38,526][0m Trial 47 finished with value: 0.20372634563381217 and parameters: {'observation_period_num': 58, 'train_rates': 0.6603567308218248, 'learning_rate': 0.0001260709284011789, 'batch_size': 149, 'step_size': 4, 'gamma': 0.9753858092714003}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:10:06,562][0m Trial 48 finished with value: 0.08584112166145151 and parameters: {'observation_period_num': 45, 'train_rates': 0.8369791303726638, 'learning_rate': 4.542079907569355e-05, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8869500575716089}. Best is trial 1 with value: 0.05594167113304138.[0m
[32m[I 2025-02-06 05:10:40,753][0m Trial 49 finished with value: 0.0967372579977858 and parameters: {'observation_period_num': 79, 'train_rates': 0.9076132536186341, 'learning_rate': 0.00036505074488696635, 'batch_size': 180, 'step_size': 6, 'gamma': 0.7674153409402196}. Best is trial 1 with value: 0.05594167113304138.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6262 | 0.5121
Epoch 2/300, Loss: 0.2715 | 0.2930
Epoch 3/300, Loss: 0.2480 | 0.3171
Epoch 4/300, Loss: 0.3123 | 0.2524
Epoch 5/300, Loss: 0.4521 | 0.8578
Epoch 6/300, Loss: 0.2321 | 0.4646
Epoch 7/300, Loss: 0.2458 | 0.6056
Epoch 8/300, Loss: 0.2321 | 0.3284
Epoch 9/300, Loss: 0.2584 | 0.2993
Epoch 10/300, Loss: 0.1598 | 0.3298
Epoch 11/300, Loss: 0.1448 | 0.3634
Epoch 12/300, Loss: 0.1345 | 0.1604
Epoch 13/300, Loss: 0.1473 | 0.4293
Epoch 14/300, Loss: 0.1310 | 0.1810
Epoch 15/300, Loss: 0.1477 | 0.3800
Epoch 16/300, Loss: 0.1456 | 0.2279
Epoch 17/300, Loss: 0.1667 | 0.2899
Epoch 18/300, Loss: 0.1662 | 0.3570
Epoch 19/300, Loss: 0.1873 | 0.2412
Epoch 20/300, Loss: 0.1510 | 0.3278
Epoch 21/300, Loss: 0.1357 | 0.2163
Epoch 22/300, Loss: 0.1106 | 0.2472
Epoch 23/300, Loss: 0.1119 | 0.2080
Epoch 24/300, Loss: 0.1075 | 0.1932
Epoch 25/300, Loss: 0.1142 | 0.2228
Epoch 26/300, Loss: 0.1080 | 0.2198
Epoch 27/300, Loss: 0.1128 | 0.1708
Epoch 28/300, Loss: 0.1035 | 0.2236
Epoch 29/300, Loss: 0.1050 | 0.1660
Epoch 30/300, Loss: 0.0961 | 0.1787
Epoch 31/300, Loss: 0.0954 | 0.1476
Epoch 32/300, Loss: 0.0899 | 0.1576
Epoch 33/300, Loss: 0.0912 | 0.1426
Epoch 34/300, Loss: 0.0883 | 0.1326
Epoch 35/300, Loss: 0.0901 | 0.1435
Epoch 36/300, Loss: 0.0869 | 0.1274
Epoch 37/300, Loss: 0.0881 | 0.1260
Epoch 38/300, Loss: 0.0862 | 0.1217
Epoch 39/300, Loss: 0.0877 | 0.1242
Epoch 40/300, Loss: 0.0853 | 0.1138
Epoch 41/300, Loss: 0.0845 | 0.1094
Epoch 42/300, Loss: 0.0812 | 0.1080
Epoch 43/300, Loss: 0.0809 | 0.1028
Epoch 44/300, Loss: 0.0790 | 0.0987
Epoch 45/300, Loss: 0.0785 | 0.0942
Epoch 46/300, Loss: 0.0775 | 0.0946
Epoch 47/300, Loss: 0.0774 | 0.0918
Epoch 48/300, Loss: 0.0776 | 0.0939
Epoch 49/300, Loss: 0.0778 | 0.0910
Epoch 50/300, Loss: 0.0784 | 0.0952
Epoch 51/300, Loss: 0.0777 | 0.0941
Epoch 52/300, Loss: 0.0771 | 0.0887
Epoch 53/300, Loss: 0.0762 | 0.0898
Epoch 54/300, Loss: 0.0759 | 0.0872
Epoch 55/300, Loss: 0.0753 | 0.0867
Epoch 56/300, Loss: 0.0750 | 0.0852
Epoch 57/300, Loss: 0.0746 | 0.0853
Epoch 58/300, Loss: 0.0744 | 0.0843
Epoch 59/300, Loss: 0.0742 | 0.0837
Epoch 60/300, Loss: 0.0741 | 0.0833
Epoch 61/300, Loss: 0.0739 | 0.0827
Epoch 62/300, Loss: 0.0738 | 0.0828
Epoch 63/300, Loss: 0.0736 | 0.0818
Epoch 64/300, Loss: 0.0735 | 0.0821
Epoch 65/300, Loss: 0.0734 | 0.0810
Epoch 66/300, Loss: 0.0733 | 0.0818
Epoch 67/300, Loss: 0.0731 | 0.0803
Epoch 68/300, Loss: 0.0731 | 0.0813
Epoch 69/300, Loss: 0.0729 | 0.0796
Epoch 70/300, Loss: 0.0729 | 0.0809
Epoch 71/300, Loss: 0.0727 | 0.0791
Epoch 72/300, Loss: 0.0727 | 0.0806
Epoch 73/300, Loss: 0.0726 | 0.0788
Epoch 74/300, Loss: 0.0725 | 0.0800
Epoch 75/300, Loss: 0.0724 | 0.0785
Epoch 76/300, Loss: 0.0724 | 0.0795
Epoch 77/300, Loss: 0.0723 | 0.0785
Epoch 78/300, Loss: 0.0722 | 0.0788
Epoch 79/300, Loss: 0.0721 | 0.0784
Epoch 80/300, Loss: 0.0721 | 0.0784
Epoch 81/300, Loss: 0.0720 | 0.0782
Epoch 82/300, Loss: 0.0720 | 0.0781
Epoch 83/300, Loss: 0.0719 | 0.0780
Epoch 84/300, Loss: 0.0719 | 0.0779
Epoch 85/300, Loss: 0.0718 | 0.0777
Epoch 86/300, Loss: 0.0718 | 0.0777
Epoch 87/300, Loss: 0.0717 | 0.0776
Epoch 88/300, Loss: 0.0717 | 0.0775
Epoch 89/300, Loss: 0.0717 | 0.0774
Epoch 90/300, Loss: 0.0716 | 0.0773
Epoch 91/300, Loss: 0.0716 | 0.0772
Epoch 92/300, Loss: 0.0716 | 0.0772
Epoch 93/300, Loss: 0.0715 | 0.0771
Epoch 94/300, Loss: 0.0715 | 0.0770
Epoch 95/300, Loss: 0.0715 | 0.0769
Epoch 96/300, Loss: 0.0714 | 0.0769
Epoch 97/300, Loss: 0.0714 | 0.0768
Epoch 98/300, Loss: 0.0714 | 0.0768
Epoch 99/300, Loss: 0.0714 | 0.0767
Epoch 100/300, Loss: 0.0713 | 0.0767
Epoch 101/300, Loss: 0.0713 | 0.0766
Epoch 102/300, Loss: 0.0713 | 0.0766
Epoch 103/300, Loss: 0.0713 | 0.0765
Epoch 104/300, Loss: 0.0713 | 0.0765
Epoch 105/300, Loss: 0.0712 | 0.0765
Epoch 106/300, Loss: 0.0712 | 0.0764
Epoch 107/300, Loss: 0.0712 | 0.0764
Epoch 108/300, Loss: 0.0712 | 0.0764
Epoch 109/300, Loss: 0.0712 | 0.0763
Epoch 110/300, Loss: 0.0712 | 0.0763
Epoch 111/300, Loss: 0.0711 | 0.0763
Epoch 112/300, Loss: 0.0711 | 0.0762
Epoch 113/300, Loss: 0.0711 | 0.0762
Epoch 114/300, Loss: 0.0711 | 0.0762
Epoch 115/300, Loss: 0.0711 | 0.0762
Epoch 116/300, Loss: 0.0711 | 0.0761
Epoch 117/300, Loss: 0.0711 | 0.0761
Epoch 118/300, Loss: 0.0711 | 0.0761
Epoch 119/300, Loss: 0.0711 | 0.0761
Epoch 120/300, Loss: 0.0710 | 0.0760
Epoch 121/300, Loss: 0.0710 | 0.0760
Epoch 122/300, Loss: 0.0710 | 0.0760
Epoch 123/300, Loss: 0.0710 | 0.0760
Epoch 124/300, Loss: 0.0710 | 0.0760
Epoch 125/300, Loss: 0.0710 | 0.0760
Epoch 126/300, Loss: 0.0710 | 0.0759
Epoch 127/300, Loss: 0.0710 | 0.0759
Epoch 128/300, Loss: 0.0710 | 0.0759
Epoch 129/300, Loss: 0.0710 | 0.0759
Epoch 130/300, Loss: 0.0710 | 0.0759
Epoch 131/300, Loss: 0.0710 | 0.0759
Epoch 132/300, Loss: 0.0710 | 0.0759
Epoch 133/300, Loss: 0.0710 | 0.0759
Epoch 134/300, Loss: 0.0710 | 0.0758
Epoch 135/300, Loss: 0.0709 | 0.0758
Epoch 136/300, Loss: 0.0709 | 0.0758
Epoch 137/300, Loss: 0.0709 | 0.0758
Epoch 138/300, Loss: 0.0709 | 0.0758
Epoch 139/300, Loss: 0.0709 | 0.0758
Epoch 140/300, Loss: 0.0709 | 0.0758
Epoch 141/300, Loss: 0.0709 | 0.0758
Epoch 142/300, Loss: 0.0709 | 0.0758
Epoch 143/300, Loss: 0.0709 | 0.0758
Epoch 144/300, Loss: 0.0709 | 0.0758
Epoch 145/300, Loss: 0.0709 | 0.0758
Epoch 146/300, Loss: 0.0709 | 0.0758
Epoch 147/300, Loss: 0.0709 | 0.0757
Epoch 148/300, Loss: 0.0709 | 0.0757
Epoch 149/300, Loss: 0.0709 | 0.0757
Epoch 150/300, Loss: 0.0709 | 0.0757
Epoch 151/300, Loss: 0.0709 | 0.0757
Epoch 152/300, Loss: 0.0709 | 0.0757
Epoch 153/300, Loss: 0.0709 | 0.0757
Epoch 154/300, Loss: 0.0709 | 0.0757
Epoch 155/300, Loss: 0.0709 | 0.0757
Epoch 156/300, Loss: 0.0709 | 0.0757
Epoch 157/300, Loss: 0.0709 | 0.0757
Epoch 158/300, Loss: 0.0709 | 0.0757
Epoch 159/300, Loss: 0.0709 | 0.0757
Epoch 160/300, Loss: 0.0709 | 0.0757
Epoch 161/300, Loss: 0.0709 | 0.0757
Epoch 162/300, Loss: 0.0709 | 0.0757
Epoch 163/300, Loss: 0.0709 | 0.0757
Epoch 164/300, Loss: 0.0709 | 0.0757
Epoch 165/300, Loss: 0.0709 | 0.0757
Epoch 166/300, Loss: 0.0709 | 0.0757
Epoch 167/300, Loss: 0.0709 | 0.0757
Epoch 168/300, Loss: 0.0709 | 0.0757
Epoch 169/300, Loss: 0.0709 | 0.0757
Epoch 170/300, Loss: 0.0709 | 0.0757
Epoch 171/300, Loss: 0.0709 | 0.0757
Epoch 172/300, Loss: 0.0709 | 0.0757
Epoch 173/300, Loss: 0.0709 | 0.0757
Epoch 174/300, Loss: 0.0709 | 0.0757
Epoch 175/300, Loss: 0.0709 | 0.0757
Epoch 176/300, Loss: 0.0709 | 0.0757
Epoch 177/300, Loss: 0.0709 | 0.0757
Epoch 178/300, Loss: 0.0709 | 0.0757
Epoch 179/300, Loss: 0.0709 | 0.0757
Epoch 180/300, Loss: 0.0709 | 0.0757
Epoch 181/300, Loss: 0.0709 | 0.0757
Epoch 182/300, Loss: 0.0709 | 0.0757
Epoch 183/300, Loss: 0.0709 | 0.0757
Epoch 184/300, Loss: 0.0709 | 0.0757
Epoch 185/300, Loss: 0.0709 | 0.0757
Epoch 186/300, Loss: 0.0709 | 0.0757
Epoch 187/300, Loss: 0.0709 | 0.0756
Epoch 188/300, Loss: 0.0709 | 0.0756
Epoch 189/300, Loss: 0.0709 | 0.0756
Epoch 190/300, Loss: 0.0709 | 0.0756
Epoch 191/300, Loss: 0.0709 | 0.0756
Epoch 192/300, Loss: 0.0709 | 0.0756
Epoch 193/300, Loss: 0.0709 | 0.0756
Epoch 194/300, Loss: 0.0709 | 0.0756
Epoch 195/300, Loss: 0.0709 | 0.0756
Epoch 196/300, Loss: 0.0709 | 0.0756
Epoch 197/300, Loss: 0.0709 | 0.0756
Epoch 198/300, Loss: 0.0709 | 0.0756
Epoch 199/300, Loss: 0.0709 | 0.0756
Epoch 200/300, Loss: 0.0709 | 0.0756
Epoch 201/300, Loss: 0.0709 | 0.0756
Epoch 202/300, Loss: 0.0709 | 0.0756
Epoch 203/300, Loss: 0.0709 | 0.0756
Epoch 204/300, Loss: 0.0709 | 0.0756
Epoch 205/300, Loss: 0.0709 | 0.0756
Epoch 206/300, Loss: 0.0709 | 0.0756
Epoch 207/300, Loss: 0.0709 | 0.0756
Epoch 208/300, Loss: 0.0709 | 0.0756
Epoch 209/300, Loss: 0.0709 | 0.0756
Epoch 210/300, Loss: 0.0709 | 0.0756
Epoch 211/300, Loss: 0.0709 | 0.0756
Epoch 212/300, Loss: 0.0709 | 0.0756
Epoch 213/300, Loss: 0.0709 | 0.0756
Epoch 214/300, Loss: 0.0709 | 0.0756
Epoch 215/300, Loss: 0.0709 | 0.0756
Epoch 216/300, Loss: 0.0709 | 0.0756
Epoch 217/300, Loss: 0.0709 | 0.0756
Epoch 218/300, Loss: 0.0709 | 0.0756
Epoch 219/300, Loss: 0.0709 | 0.0756
Epoch 220/300, Loss: 0.0709 | 0.0756
Epoch 221/300, Loss: 0.0709 | 0.0756
Epoch 222/300, Loss: 0.0709 | 0.0756
Epoch 223/300, Loss: 0.0709 | 0.0756
Epoch 224/300, Loss: 0.0709 | 0.0756
Epoch 225/300, Loss: 0.0709 | 0.0756
Epoch 226/300, Loss: 0.0709 | 0.0756
Epoch 227/300, Loss: 0.0709 | 0.0756
Epoch 228/300, Loss: 0.0709 | 0.0756
Epoch 229/300, Loss: 0.0709 | 0.0756
Epoch 230/300, Loss: 0.0709 | 0.0756
Epoch 231/300, Loss: 0.0709 | 0.0756
Epoch 232/300, Loss: 0.0709 | 0.0756
Epoch 233/300, Loss: 0.0709 | 0.0756
Epoch 234/300, Loss: 0.0709 | 0.0756
Epoch 235/300, Loss: 0.0709 | 0.0756
Epoch 236/300, Loss: 0.0709 | 0.0756
Epoch 237/300, Loss: 0.0709 | 0.0756
Epoch 238/300, Loss: 0.0709 | 0.0756
Epoch 239/300, Loss: 0.0709 | 0.0756
Epoch 240/300, Loss: 0.0709 | 0.0756
Epoch 241/300, Loss: 0.0709 | 0.0756
Epoch 242/300, Loss: 0.0709 | 0.0756
Epoch 243/300, Loss: 0.0709 | 0.0756
Epoch 244/300, Loss: 0.0709 | 0.0756
Epoch 245/300, Loss: 0.0709 | 0.0756
Epoch 246/300, Loss: 0.0709 | 0.0756
Epoch 247/300, Loss: 0.0709 | 0.0756
Epoch 248/300, Loss: 0.0709 | 0.0756
Epoch 249/300, Loss: 0.0709 | 0.0756
Early stopping
Runtime (seconds): 94.20455574989319
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 299.45379376620986
RMSE: 17.304733276367188
MAE: 17.304733276367188
R-squared: nan
[178.08527]
